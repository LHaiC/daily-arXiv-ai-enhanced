<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 100]
- [cs.CL](#cs.CL) [Total: 124]
- [eess.SP](#eess.SP) [Total: 11]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 17]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.AR](#cs.AR) [Total: 4]
- [eess.SY](#eess.SY) [Total: 13]
- [cs.MA](#cs.MA) [Total: 4]
- [quant-ph](#quant-ph) [Total: 36]
- [cs.GT](#cs.GT) [Total: 5]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 19]
- [cs.RO](#cs.RO) [Total: 37]
- [cs.AI](#cs.AI) [Total: 54]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.LG](#cs.LG) [Total: 101]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.DS](#cs.DS) [Total: 3]
- [physics.app-ph](#physics.app-ph) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MultiFoodhat: A potential new paradigm for intelligent food quality inspection](https://arxiv.org/abs/2510.13889)
*Yue Hu,Guohang Zhuang*

Main category: cs.CV

TL;DR: MultiFoodChat是一个对话驱动的多智能体推理框架，用于零样本食物识别，解决了传统模型对大规模标注数据集的依赖和泛化能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有监督模型在食物图像分类方面严重依赖大型标注数据集，且泛化到未见过的食物类别能力有限。本研究旨在克服这些挑战。

Method: 提出MultiFoodChat框架，融合视觉-语言模型（VLMs）和大型语言模型（LLMs），通过多轮视觉-文本对话实现协同推理。其中，物体感知令牌（OPT）捕捉细粒度的视觉属性，交互式推理代理（IRA）动态解释上下文线索以优化预测。这种多智能体设计无需额外训练或手动标注即可实现对复杂食物场景的灵活、类似人类的理解。

Result: 在多个公共食物数据集上的实验表明，MultiFoodChat在识别准确性和可解释性方面优于现有的无监督和少样本方法。

Conclusion: MultiFoodChat为智能食品质量检测和分析提供了一个新的范式。

Abstract: Food image classification plays a vital role in intelligent food quality
inspection, dietary assessment, and automated monitoring. However, most
existing supervised models rely heavily on large labeled datasets and exhibit
limited generalization to unseen food categories. To overcome these challenges,
this study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning
framework for zero-shot food recognition. The framework integrates
vision-language models (VLMs) and large language models (LLMs) to enable
collaborative reasoning through multi-round visual-textual dialogues. An Object
Perception Token (OPT) captures fine-grained visual attributes, while an
Interactive Reasoning Agent (IRA) dynamically interprets contextual cues to
refine predictions. This multi-agent design allows flexible and human-like
understanding of complex food scenes without additional training or manual
annotations. Experiments on multiple public food datasets demonstrate that
MultiFoodChat achieves superior recognition accuracy and interpretability
compared with existing unsupervised and few-shot methods, highlighting its
potential as a new paradigm for intelligent food quality inspection and
analysis.

</details>


### [2] [Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images](https://arxiv.org/abs/2510.14081)
*Emanuel Garbin,Guy Adam,Oded Krams,Zohar Barzelay,Eran Guendelman,Michael Schwarz,Moran Vatelmacher,Yigal Shenkman,Eli Peker,Itai Druker,Uri Patish,Yoav Blum,Max Bluvstein,Junxuan Li,Rawal Khirodkar,Shunsuke Saito*

Main category: cs.CV

TL;DR: 该研究提出了一种新的零样本方法，能够从少量非结构化手机图像生成超逼真、保留身份信息的3D头像。现有的方法存在几何不一致、细节丢失等问题。本方法通过生成性规范化模块将多视角图像转换为一致性表示，并利用基于Transformer、在包含真实人物高保真高斯喷涂数据的 auu 新大规模数据集上训练的模型，实现了静态的、四分之三身体的3D头像生成，具有出色的真实感和身份保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有的3D头像生成方法，如单视角方法，存在几何不一致和细节缺失的问题，导致身份信息丢失，而基于合成数据训练的模型无法捕捉像皮肤皱纹和精细毛发等高频细节，限制了最终的逼真度。

Method: 本研究提出了一种“捕获、规范化、喷涂”的零样本管线。首先，利用一个生成性规范化模块将输入的非结构化多视角图像转换为标准一致的表示。然后，使用一个基于Transformer的模型，该模型在一个包含真实人物高保真高斯喷涂数据的新大规模数据集上进行了训练。

Result: 该方法能够从非结构化的手机照片生成静态的、四分之三身体的3D头像，并展现出令人信服的逼真度和强大的身份保留能力。

Conclusion: 本研究提出的零样本管线能够从非结构化手机图像生成高质量的3D头像，有效解决了现有方法的局限性，并在逼真度和身份保留方面取得了显著成果。

Abstract: We present a novel, zero-shot pipeline for creating hyperrealistic,
identity-preserving 3D avatars from a few unstructured phone images. Existing
methods face several challenges: single-view approaches suffer from geometric
inconsistencies and hallucinations, degrading identity preservation, while
models trained on synthetic data fail to capture high-frequency details like
skin wrinkles and fine hair, limiting realism. Our method introduces two key
contributions: (1) a generative canonicalization module that processes multiple
unstructured views into a standardized, consistent representation, and (2) a
transformer-based model trained on a new, large-scale dataset of high-fidelity
Gaussian splatting avatars derived from dome captures of real people. This
"Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars
with compelling realism and robust identity preservation from unstructured
photos.

</details>


### [3] [Post-surgical Endometriosis Segmentation in Laparoscopic Videos](https://arxiv.org/abs/2510.13899)
*Andreas Leibetseder,Klaus Schoeffmann,Jörg Keckstein,Simon Keckstein*

Main category: cs.CV

TL;DR: 该论文提出了一个用于分割内异症黑暗子宫内膜植入物的系统，以辅助医生诊断。


<details>
  <summary>Details</summary>
Motivation: 内异症的视觉外观多样且难以识别，特别是对于非专业人士，需要辅助工具来提高诊断的准确性。

Method: 开发一个系统，该系统能够分析腹腔镜手术视频，对识别出的植入物区域进行多色叠加注释，并显示检测摘要以改进视频浏览。

Result: 系统能够分割内异症的黑暗子宫内膜植入物，并在视频中进行标注和提供摘要。

Conclusion: 该系统能够帮助妇科医生治疗内异症，提高诊断效率和准确性。

Abstract: Endometriosis is a common women's condition exhibiting a manifold visual
appearance in various body-internal locations. Having such properties makes its
identification very difficult and error-prone, at least for laymen and
non-specialized medical practitioners. In an attempt to provide assistance to
gynecologic physicians treating endometriosis, this demo paper describes a
system that is trained to segment one frequently occurring visual appearance of
endometriosis, namely dark endometrial implants. The system is capable of
analyzing laparoscopic surgery videos, annotating identified implant regions
with multi-colored overlays and displaying a detection summary for improved
video browsing.

</details>


### [4] [GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering](https://arxiv.org/abs/2510.14270)
*Alexander Valverde,Brian Xu,Yuyin Zhou,Meng Xu,Hongyun Wang*

Main category: cs.CV

TL;DR: GauSSmart结合2D基础模型和3D高斯喷溅技术，提升了场景重建的细节和真实感，特别是在稀疏覆盖区域。


<details>
  <summary>Details</summary>
Motivation: 现有高斯喷溅方法在稀疏数据覆盖区域难以捕捉细节和保持真实感。

Method: 提出GauSSmart混合方法，利用2D基础模型（如DINO）的语义特征和2D分割先验来指导3D高斯喷溅的密集化和优化，以改进覆盖并保留细节。

Result: 在三个数据集上的实验表明，GauSSmart在大多数评估场景中优于现有的高斯喷溅方法。

Conclusion: 混合2D基础模型和3D重建流水线是克服单一方法局限性的有效途径。

Abstract: Scene reconstruction has emerged as a central challenge in computer vision,
with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting
achieving remarkable progress. While Gaussian Splatting demonstrates strong
performance on large-scale datasets, it often struggles to capture fine details
or maintain realism in regions with sparse coverage, largely due to the
inherent limitations of sparse 3D training data.
  In this work, we propose GauSSmart, a hybrid method that effectively bridges
2D foundational models and 3D Gaussian Splatting reconstruction. Our approach
integrates established 2D computer vision techniques, including convex
filtering and semantic feature supervision from foundational models such as
DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D
segmentation priors and high-dimensional feature embeddings, our method guides
the densification and refinement of Gaussian splats, improving coverage in
underrepresented areas and preserving intricate structural details.
  We validate our approach across three datasets, where GauSSmart consistently
outperforms existing Gaussian Splatting in the majority of evaluated scenes.
Our results demonstrate the significant potential of hybrid 2D-3D approaches,
highlighting how the thoughtful combination of 2D foundational models with 3D
reconstruction pipelines can overcome the limitations inherent in either
approach alone.

</details>


### [5] [Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models](https://arxiv.org/abs/2510.13993)
*Jia Yun Chua,Argyrios Zolotas,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: 本研究结合了YOLO、LLaVA、ChatGPT和Gemini等视觉模型和视觉语言模型（VLM），以提高遥感图像的分析能力，特别是在飞机检测和场景理解方面。


<details>
  <summary>Details</summary>
Motivation: 传统视觉模型在遥感领域需要大量特定领域标签数据，且难以理解复杂环境的上下文；而现有视觉语言模型在遥感领域的应用尚不充分。

Method: 将YOLO与LLaVA、ChatGPT、Gemini等视觉语言模型相结合，并在标记和未标记的遥感数据以及退化的图像场景中评估其性能，重点关注飞机检测、计数和场景理解。

Result: 在飞机检测和计数方面，模型的平均平均误差（MAE）提高了48.46%，尤其是在具有挑战性的原始和退化场景中。在场景理解方面，CLIPScore提高了6.17%。

Conclusion: 结合传统视觉模型和视觉语言模型的方法，为遥感图像分析提供了更先进、更高效的途径，尤其是在少样本学习场景下。

Abstract: Remote sensing has become a vital tool across sectors such as urban planning,
environmental monitoring, and disaster response. While the volume of data
generated has increased significantly, traditional vision models are often
constrained by the requirement for extensive domain-specific labelled data and
their limited ability to understand the context within complex environments.
Vision Language Models offer a complementary approach by integrating visual and
textual data; however, their application to remote sensing remains
underexplored, particularly given their generalist nature. This work
investigates the combination of vision models and VLMs to enhance image
analysis in remote sensing, with a focus on aircraft detection and scene
understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and
Gemini aims to achieve more accurate and contextually aware image
interpretation. Performance is evaluated on both labelled and unlabelled remote
sensing data, as well as degraded image scenarios which are crucial for remote
sensing. The findings show an average MAE improvement of 48.46% across models
in the accuracy of aircraft detection and counting, especially in challenging
conditions, in both raw and degraded scenarios. A 6.17% improvement in
CLIPScore for comprehensive understanding of remote sensing images is obtained.
The proposed approach combining traditional vision models and VLMs paves the
way for more advanced and efficient remote sensing image analysis, especially
in few-shot learning scenarios.

</details>


### [6] [Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality](https://arxiv.org/abs/2510.14765)
*Giuseppe Lorenzo Catalano,Agata Marta Soccini*

Main category: cs.CV

TL;DR: 本研究提出一种基于无条件扩散模型的火星表面重建方法，以填补高程数据中的缺失值，并优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 由于采集和传输限制，外星地形图经常包含缺失值。火星表面重建对于填补这些空白至关重要，但现有方法（如逆距离加权、克里金法和纳维-斯托克斯算法）在保持几何一致性方面存在不足。

Method: 利用无条件扩散模型，在包含12000个火星高程图的增强数据集上进行训练。采用非均匀重缩放策略来捕捉多尺度地形特征，然后调整到128x128的模型分辨率。

Result: 与现有的填补空白和修复技术相比，该方法在重建精度（RMSE降低4-15%）和感知相似性（LPIPS提高29-81%）方面表现更好。

Conclusion: 本研究提出的无条件扩散模型在火星表面重建方面取得了优于现有技术的性能，为空间探索中的虚拟现实模拟提供了更准确的地形表示。

Abstract: Space exploration increasingly relies on Virtual Reality for several tasks,
such as mission planning, multidisciplinary scientific analysis, and astronaut
training. A key factor for the reliability of the simulations is having
accurate 3D representations of planetary terrains. Extraterrestrial heightmaps
derived from satellite imagery often contain missing values due to acquisition
and transmission constraints. Mars is among the most studied planets beyond
Earth, and its extensive terrain datasets make the Martian surface
reconstruction a valuable task, although many areas remain unmapped. Deep
learning algorithms can support void-filling tasks; however, whereas Earth's
comprehensive datasets enables the use of conditional methods, such approaches
cannot be applied to Mars. Current approaches rely on simpler interpolation
techniques which, however, often fail to preserve geometric coherence. In this
work, we propose a method for reconstructing the surface of Mars based on an
unconditional diffusion model. Training was conducted on an augmented dataset
of 12000 Martian heightmaps derived from NASA's HiRISE survey. A
non-homogeneous rescaling strategy captures terrain features across multiple
scales before resizing to a fixed 128x128 model resolution. We compared our
method against established void-filling and inpainting techniques, including
Inverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an
evaluation set of 1000 samples. Results show that our approach consistently
outperforms these methods in terms of reconstruction accuracy (4-15% on RMSE)
and perceptual similarity (29-81% on LPIPS) with the original data.

</details>


### [7] [Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer](https://arxiv.org/abs/2510.13995)
*Kelvin Szolnoky,Anders Blilie,Nita Mulliqi,Toyonori Tsuzuki,Hemamali Samaratunga,Matteo Titus,Xiaoyi Ji,Sol Erika Boman,Einar Gudlaugsson,Svein Reidar Kjosavik,José Asenjo,Marcello Gambacorta,Paolo Libretti,Marcin Braun,Radisław Kordek,Roman Łowicki,Brett Delahunt,Kenneth A. Iczkowski,Theo van der Kwast,Geert J. L. H. van Leenders,Katia R. M. Leite,Chin-Chen Pan,Emiel Adrianus Maria Janssen,Martin Eklund,Lars Egevad,Kimmo Kartasalo*

Main category: cs.CV

TL;DR: 该研究开发了一种人工智能（AI）模型，用于检测前列腺癌中的筛状形态，该形态预示着不良预后。该模型在内部和外部数据集上均表现出高精度，并优于人类病理学家的表现。


<details>
  <summary>Details</summary>
Motivation: 前列腺癌的筛状形态预后不良，但漏报率高且存在观察者间变异性。本研究旨在开发和验证一个 AI 系统以改进筛状形态的检测。

Method: 研究人员使用 EfficientNetV2-S 编码器和多实例学习构建了一个深度学习模型，并在三个队列的 640 个前列腺核心针穿刺活检中进行了训练。该模型在内部（261 张幻灯片）和外部（266 张幻灯片）数据集上进行了验证。还进行了观察者间分析，并将模型与九名病理学家进行了比较。

Result: 该模型在内部验证中表现强劲（AUC：0.97，Kappa：0.81），在外部验证中表现稳健（AUC：0.90，Kappa：0.55）。在观察者间分析中，该模型取得了最高的平均一致性（Kappa：0.66），优于所有九名病理学家（Kappa 范围为 0.35-0.62）。

Conclusion: 所开发的人工智能模型在前列腺癌筛状形态检测方面达到了病理学家水平的表现，有望提高诊断的可靠性、规范报告并改善前列腺癌患者的治疗决策。

Abstract: Background: Cribriform morphology in prostate cancer is a histological
feature that indicates poor prognosis and contraindicates active surveillance.
However, it remains underreported and subject to significant interobserver
variability amongst pathologists. We aimed to develop and validate an AI-based
system to improve cribriform pattern detection.
  Methods: We created a deep learning model using an EfficientNetV2-S encoder
with multiple instance learning for end-to-end whole-slide classification. The
model was trained on 640 digitised prostate core needle biopsies from 430
patients, collected across three cohorts. It was validated internally (261
slides from 171 patients) and externally (266 slides, 104 patients from three
independent cohorts). Internal validation cohorts included laboratories or
scanners from the development set, while external cohorts used completely
independent instruments and laboratories. Annotations were provided by three
expert uropathologists with known high concordance. Additionally, we conducted
an inter-rater analysis and compared the model's performance against nine
expert uropathologists on 88 slides from the internal validation cohort.
  Results: The model showed strong internal validation performance (AUC: 0.97,
95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external
validation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI:
0.45-0.64). In our inter-rater analysis, the model achieved the highest average
agreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine
pathologists whose Cohen's kappas ranged from 0.35 to 0.62.
  Conclusion: Our AI model demonstrates pathologist-level performance for
cribriform morphology detection in prostate cancer. This approach could enhance
diagnostic reliability, standardise reporting, and improve treatment decisions
for prostate cancer patients.

</details>


### [8] [Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation](https://arxiv.org/abs/2510.14976)
*Shaowei Liu,Chuan Guo,Bing Zhou,Jian Wang*

Main category: cs.CV

TL;DR: Ponimator是一个利用近距离交互姿势生成和合成交互式动画的框架，能够处理图像、文本等多种输入，并支持图像驱动动画、反应动画和文本到交互合成等任务。


<details>
  <summary>Details</summary>
Motivation: 人类可以通过近距离交互姿势直观地推断交互的上下文和动态，这启发了我们提出一个基于近距离交互姿势的通用交互式动画框架。

Method: Ponimator框架包含两个条件扩散模型：一个姿势动画器，利用时间先验从交互姿势生成动态运动序列；一个姿势生成器，利用空间先验从单个姿势、文本或两者合成交互姿势。

Result: 实验证明了姿势先验的通用性以及所提出框架的有效性和鲁棒性，该框架能够处理多种任务，包括图像驱动的交互式动画、反应动画和文本到交互合成。

Conclusion: Ponimator框架能够有效地将高质量动作捕捉数据中的交互知识迁移到开放世界场景中，实现了多样化的交互式动画生成。

Abstract: Close-proximity human-human interactive poses convey rich contextual
information about interaction dynamics. Given such poses, humans can
intuitively infer the context and anticipate possible past and future dynamics,
drawing on strong priors of human behavior. Inspired by this observation, we
propose Ponimator, a simple framework anchored on proximal interactive poses
for versatile interaction animation. Our training data consists of
close-contact two-person poses and their surrounding temporal context from
motion-capture interaction datasets. Leveraging interactive pose priors,
Ponimator employs two conditional diffusion models: (1) a pose animator that
uses the temporal prior to generate dynamic motion sequences from interactive
poses, and (2) a pose generator that applies the spatial prior to synthesize
interactive poses from a single pose, text, or both when interactive poses are
unavailable. Collectively, Ponimator supports diverse tasks, including
image-based interaction animation, reaction animation, and text-to-interaction
synthesis, facilitating the transfer of interaction knowledge from high-quality
mocap data to open-world scenarios. Empirical experiments across diverse
datasets and applications demonstrate the universality of the pose prior and
the effectiveness and robustness of our framework.

</details>


### [9] [NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations](https://arxiv.org/abs/2510.14025)
*Junjie Nan,Jianing Li,Wei Chen,Mingkun Zhang,Xueqi Cheng*

Main category: cs.CV

TL;DR: NAPPure框架可以处理非加性对抗性扰动，在GTSRB和CIFAR-10数据集上显著提高了图像分类模型在非加性扰动下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性净化方法主要针对加性扰动，在处理现实世界中常见的非加性扰动（如模糊、遮挡、失真）时效果不佳。

Method: 提出了一种名为NAPPure的扩展对抗性净化框架，通过建立对抗性图像的生成过程，并利用最大似然估计解耦底层清洁图像和扰动参数。

Result: 在GTSRB和CIFAR-10数据集上的实验表明，NAPPure显著提高了图像分类模型在非加性扰动下的鲁棒性。

Conclusion: NAPPure框架能够有效处理非加性对抗性扰动，提高了模型的鲁棒性。

Abstract: Adversarial purification has achieved great success in combating adversarial
image perturbations, which are usually assumed to be additive. However,
non-additive adversarial perturbations such as blur, occlusion, and distortion
are also common in the real world. Under such perturbations, existing
adversarial purification methods are much less effective since they are
designed to fit the additive nature. In this paper, we propose an extended
adversarial purification framework named NAPPure, which can further handle
non-additive perturbations. Specifically, we first establish the generation
process of an adversarial image, and then disentangle the underlying clean
image and perturbation parameters through likelihood maximization. Experiments
on GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the
robustness of image classification models against non-additive perturbations.

</details>


### [10] [Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding](https://arxiv.org/abs/2510.14032)
*Xiaoqian Shen,Wenxuan Zhang,Jun Chen,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: Vgent是一个创新的基于图的检索-推理-增强生成框架，通过结构化图表示和中间推理步骤来增强大型视频语言模型（LVLMs）在长视频理解方面的能力，解决了长视频处理中的时间依赖性破坏和无关信息干扰问题，并在多个基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 长视频理解对于大型视频语言模型（LVLMs）来说是一个挑战，因为它们难以处理超出上下文窗口的大量视频令牌并保留长期的顺序信息。虽然检索增强生成（RAG）在处理长文本方面很有效，但将其应用于长视频会遇到时间依赖性中断和包含无关信息的问题。

Method: 提出了一种名为Vgent的新型框架，该框架采用基于图的检索-推理-增强生成方法。其创新点包括：1. 通过结构化图表示视频，保留了视频片段之间的语义关系，以提高检索效率。2. 引入了中间推理步骤，利用结构化验证来减少检索噪音，并促进跨片段相关信息的显式聚合，从而增强LVLMs的推理能力。

Result: 在MLVU基准测试中，Vgent框架相比基础模型在整体性能上提升了3.0%~5.4%，并且在性能上超越了现有的视频RAG方法8.6%。

Conclusion: Vgent框架通过结构化图表示和中间推理步骤，有效解决了长视频理解中的关键挑战，显著提升了LVLMs在长视频任务上的性能。

Abstract: Understanding and reasoning over long videos pose significant challenges for
large video language models (LVLMs) due to the difficulty in processing
intensive video tokens beyond context window and retaining long-term sequential
information. Retrieval-Augmented Generation (RAG) has demonstrated
effectiveness in processing long context for Large Language Models (LLMs);
however, applying RAG to long video faces challenges such as disrupted temporal
dependencies and inclusion of irrelevant information that can hinder accurate
reasoning. To address these limitations, we propose Vgent, a novel graph-based
retrieval-reasoning-augmented generation framework to enhance LVLMs for long
video understanding. Our approach introduces two key innovations: (i) It
represents videos by structured graphs with semantic relationships across video
clips preserved to improve retrieval effectiveness. (ii) It introduces an
intermediate reasoning step to mitigate the reasoning limitation of LVLMs,
which leverages structured verification to reduce retrieval noise and
facilitate the explicit aggregation of relevant information across clips,
resulting in more accurate and context-aware responses. We comprehensively
evaluate our framework with various open-source LVLMs on three long-video
understanding benchmarks. Our approach yielded an overall performance
improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed
state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available
at https://xiaoqian-shen.github.io/Vgent.

</details>


### [11] [Synchronization of Multiple Videos](https://arxiv.org/abs/2510.14051)
*Avihai Naaman,Ron Shapira Weber,Oren Freifeld*

Main category: cs.CV

TL;DR: TPL通过学习统一的原型序列来解决多视角、多生成AI视频的同步难题，提高了同步的准确性、效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 同步来自不同场景或生成AI视频存在挑战，因为它们具有不同的内容和非线性时间错位。

Method: TPL构建了一个基于原型的框架，该框架从预训练模型的高维嵌入中提取一个共享的、紧凑的一维表示。它通过学习一个统一的原型序列来对齐视频，该序列锚定了关键动作阶段，从而避免了详尽的成对匹配。

Result: 实验表明，TPL在各种数据集上提高了同步的准确性、效率和鲁棒性，并在细粒度帧检索和阶段分类任务中表现出色。TPL是第一个能够解决多个生成AI视频同步问题的方案。

Conclusion: TPL是一种有效且通用的视频同步方法，特别是在处理复杂场景和生成AI视频方面。

Abstract: Synchronizing videos captured simultaneously from multiple cameras in the
same scene is often easy and typically requires only simple time shifts.
However, synchronizing videos from different scenes or, more recently,
generative AI videos, poses a far more complex challenge due to diverse
subjects, backgrounds, and nonlinear temporal misalignment. We propose Temporal
Prototype Learning (TPL), a prototype-based framework that constructs a shared,
compact 1D representation from high-dimensional embeddings extracted by any of
various pretrained models. TPL robustly aligns videos by learning a unified
prototype sequence that anchors key action phases, thereby avoiding exhaustive
pairwise matching. Our experiments show that TPL improves synchronization
accuracy, efficiency, and robustness across diverse datasets, including
fine-grained frame retrieval and phase classification tasks. Importantly, TPL
is the first approach to mitigate synchronization issues in multiple generative
AI videos depicting the same action. Our code and a new multiple video
synchronization dataset are available at https://bgu-cs-vil.github.io/TPL/

</details>


### [12] [cubic: CUDA-accelerated 3D Bioimage Computing](https://arxiv.org/abs/2510.14143)
*Alexandr A. Kalinin,Anne E. Carpenter,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: cubic是一个开源Python库，通过集成CuPy和RAPIDS cuCIM，提供了GPU加速的生物图像处理功能，解决了现有工具可扩展性、效率和集成性不足的问题，支持2D和3D数据处理，并能加速现有分析流程。


<details>
  <summary>Details</summary>
Motivation: 现有的生物图像分析工具存在可扩展性、效率和集成性不足的问题，无法满足现代显微镜生成的大规模2D和3D数据集的处理需求，并且通常缺乏API、GPU加速、广泛的3D处理能力和良好的互操作性。

Method: 引入了一个名为cubic的开源Python库，该库扩展了SciPy和scikit-image的API，提供了来自CuPy和RAPIDS cuCIM的GPU加速替代方案。cubic的API是设备无关的，可以无缝地在CPU或GPU上执行操作，从而加速图像处理流程。

Result: cubic在基准测试和重现现有去卷积及分割流程的实验中，实现了显著的速度提升，同时保持了算法的准确性。

Conclusion: cubic为可扩展、可复现的生物图像分析奠定了坚实的基础，并能与更广泛的Python科学计算生态系统集成，支持交互式探索和自动化高通量分析工作流。

Abstract: Quantitative analysis of multidimensional biological images is useful for
understanding complex cellular phenotypes and accelerating advances in
biomedical research. As modern microscopy generates ever-larger 2D and 3D
datasets, existing computational approaches are increasingly limited by their
scalability, efficiency, and integration with modern scientific computing
workflows. Existing bioimage analysis tools often lack application programmable
interfaces (APIs), do not support graphics processing unit (GPU) acceleration,
lack broad 3D image processing capabilities, and/or have poor interoperability
for compute-heavy workflows. Here, we introduce cubic, an open-source Python
library that addresses these challenges by augmenting widely used SciPy and
scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.
cubic's API is device-agnostic and dispatches operations to GPU when data
reside on the device and otherwise executes on CPU, seamlessly accelerating a
broad range of image processing routines. This approach enables GPU
acceleration of existing bioimage analysis workflows, from preprocessing to
segmentation and feature extraction for 2D and 3D data. We evaluate cubic both
by benchmarking individual operations and by reproducing existing deconvolution
and segmentation pipelines, achieving substantial speedups while maintaining
algorithmic fidelity. These advances establish a robust foundation for
scalable, reproducible bioimage analysis that integrates with the broader
Python scientific computing ecosystem, including other GPU-accelerated methods,
enabling both interactive exploration and automated high-throughput analysis
workflows. cubic is openly available at
https://github$.$com/alxndrkalinin/cubic

</details>


### [13] [Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures](https://arxiv.org/abs/2510.14179)
*Yuancheng Xu,Wenqi Xian,Li Ma,Julien Philip,Ahmet Levent Taşel,Yiwei Zhao,Ryan Burgert,Mingming He,Oliver Hermann,Oliver Pilarski,Rahul Garg,Paul Debevec,Ning Yu*

Main category: cs.CV

TL;DR: 我们提出了一种新颖的定制数据处理流程，实现了视频扩散模型中多视角角色一致性和三维相机控制。


<details>
  <summary>Details</summary>
Motivation: 旨在解决视频扩散模型中的多视角角色一致性和三维相机控制问题，并支持虚拟制片的核心功能。

Method: 通过4D高斯泼溅（4DGS）和视频重照明模型，利用重渲染的体积捕捉表演数据来训练角色一致性组件，并微调了最先进的开源视频扩散模型。

Result: 实验证明，该框架提高了视频质量、个性化准确性、相机控制和照明适应性。

Conclusion: 该框架能够实现多视角身份保持、精确相机控制和照明适应性，并支持多主体生成、场景和现实生活视频定制以及运动和空间布局控制，从而推动了视频生成在虚拟制片中的应用。

Abstract: We introduce a framework that enables both multi-view character consistency
and 3D camera control in video diffusion models through a novel customization
data pipeline. We train the character consistency component with recorded
volumetric capture performances re-rendered with diverse camera trajectories
via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video
relighting model. We fine-tune state-of-the-art open-source video diffusion
models on this data to provide strong multi-view identity preservation, precise
camera control, and lighting adaptability. Our framework also supports core
capabilities for virtual production, including multi-subject generation using
two approaches: joint training and noise blending, the latter enabling
efficient composition of independently customized models at inference time; it
also achieves scene and real-life video customization as well as control over
motion and spatial layout during customization. Extensive experiments show
improved video quality, higher personalization accuracy, and enhanced camera
control and lighting adaptability, advancing the integration of video
generation into virtual production. Our project page is available at:
https://eyeline-labs.github.io/Virtually-Being.

</details>


### [14] [Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition](https://arxiv.org/abs/2510.14203)
*Ryo Masumura,Shota Orihashi,Mana Ihori,Tomohiro Tanaka,Naoki Makishima,Taiga Yamane,Naotaka Kawata,Satoshi Suzuki,Taichi Katayama*

Main category: cs.CV

TL;DR: 本研究提出了一种联合模型，用于从多模态人类行为中自动识别大五人格和 HEXACO 人格特质。


<details>
  <summary>Details</summary>
Motivation: 以往研究多集中于大五人格，但忽略了 HEXACO 模型，该模型能评估与攻击性和报复心、社会支配倾向等相关的“诚实-谦逊”特质。本研究旨在通过联合建模，明确机器学习条件下大五人格与 HEXACO 模型的关联，并提升对多模态人类行为的认知。

Method: 提出了一种联合优化大五人格和 HEXACO 人格特质识别的模型。

Result: 在自我介绍视频数据集上的实验表明，该方法能够有效地识别大五人格和 HEXACO 人格特质。

Conclusion: 联合识别大五人格和 HEXACO 特质能够有效识别这两种人格模型。

Abstract: This paper proposes a joint modeling method of the Big Five, which has long
been studied, and HEXACO, which has recently attracted attention in psychology,
for automatically recognizing apparent personality traits from multimodal human
behavior. Most previous studies have used the Big Five for multimodal apparent
personality-trait recognition. However, no study has focused on apparent HEXACO
which can evaluate an Honesty-Humility trait related to displaced aggression
and vengefulness, social-dominance orientation, etc. In addition, the
relationships between the Big Five and HEXACO when modeled by machine learning
have not been clarified. We expect awareness of multimodal human behavior to
improve by considering these relationships. The key advance of our proposed
method is to optimize jointly recognizing the Big Five and HEXACO. Experiments
using a self-introduction video dataset demonstrate that the proposed method
can effectively recognize the Big Five and HEXACO.

</details>


### [15] [LOTA: Bit-Planes Guided AI-Generated Image Detection](https://arxiv.org/abs/2510.14230)
*Hongsong Wang,Renxi Cheng,Yang Zhang,Chaolei Han,Jie Gui*

Main category: cs.CV

TL;DR: 该研究提出了一种基于位平面分析的AI生成图像检测方法，通过提取和增强图像噪声特征，提高了检测效率和准确性，并实现了毫秒级的检测速度。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测方法计算成本高且无法有效捕捉原始图像的内在噪声特征。

Method: 提出了一种位平面引导的噪声图像生成方法，并结合最大梯度块选择来增强噪声信号，最后设计了轻量级的分类头进行分类。

Result: 在GenImage基准测试中，该方法取得了98.9%的平均准确率，并且在跨生成器泛化能力和检测速度方面表现优异，检测速度比现有方法快近百倍。

Conclusion: 所提出的基于位平面的噪声特征提取方法能够有效且高效地检测AI生成图像，并且具有良好的跨模型泛化能力。

Abstract: The rapid advancement of GAN and Diffusion models makes it more difficult to
distinguish AI-generated images from real ones. Recent studies often use
image-based reconstruction errors as an important feature for determining
whether an image is AI-generated. However, these approaches typically incur
high computational costs and also fail to capture intrinsic noisy features
present in the raw images. To solve these problems, we innovatively refine
error extraction by using bit-plane-based image processing, as lower bit planes
indeed represent noise patterns in images. We introduce an effective bit-planes
guided noisy image generation and exploit various image normalization
strategies, including scaling and thresholding. Then, to amplify the noise
signal for easier AI-generated image detection, we design a maximum gradient
patch selection that applies multi-directional gradients to compute the noise
score and selects the region with the highest score. Finally, we propose a
lightweight and effective classification head and explore two different
structures: noise-based classifier and noise-guided classifier. Extensive
experiments on the GenImage benchmark demonstrate the outstanding performance
of our method, which achieves an average accuracy of \textbf{98.9\%}
(\textbf{11.9}\%~$\uparrow$) and shows excellent cross-generator generalization
capability. Particularly, our method achieves an accuracy of over 98.2\% from
GAN to Diffusion and over 99.2\% from Diffusion to GAN. Moreover, it performs
error extraction at the millisecond level, nearly a hundred times faster than
existing methods. The code is at https://github.com/hongsong-wang/LOTA.

</details>


### [16] [PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis](https://arxiv.org/abs/2510.14241)
*Soumyya Kanti Datta,Tanvi Ranga,Chengzhe Sun,Siwei Lyu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为PIA的新型多模态音频-视频框架，用于检测由先进生成模型（如GANs、扩散模型和神经渲染技术）产生的深度伪造内容。该框架结合了语言、面部动态和面部识别线索，以识别传统检测方法可能忽略的细微时间差异。


<details>
  <summary>Details</summary>
Motivation: 传统的深度伪造检测方法在识别由现代先进生成模型（如GANs、扩散模型和神经渲染技术）产生的深度伪造内容方面存在不足。这些模型能够生成近乎完美的单帧图像，但可能会在帧之间引入细微的时间不一致性，而传统检测方法（如基于语音-视觉对应、帧级一致性检查或单一模态的方法）往往会忽略这些不一致性。

Method: 提出了一种名为PIA（Phoneme-Temporal and Identity-Dynamic Analysis）的新型多模态音频-视频框架。该框架整合了语言（音素序列）、面部动态（唇部几何数据）和面部识别（面部身份嵌入）线索。通过分析这些多模态线索，PIA能够识别不同模态之间存在的细微不一致性，从而提高深度伪造检测的准确性。

Result: 通过结合音素序列、唇部几何数据和先进的面部身份嵌入，PIA框架显著提高了对细微深度伪造篡改的检测能力，能够识别出传统方法忽略的、跨多个互补模态的不一致性。

Conclusion: PIA框架通过整合多模态信息（语言、面部动态和面部识别）有效地解决了传统深度伪造检测方法的局限性，能够更准确地识别由先进生成模型产生的深度伪造内容。

Abstract: The rise of manipulated media has made deepfakes a particularly insidious
threat, involving various generative manipulations such as lip-sync
modifications, face-swaps, and avatar-driven facial synthesis. Conventional
detection methods, which predominantly depend on manually designed
phoneme-viseme alignment thresholds, fundamental frame-level consistency
checks, or a unimodal detection strategy, inadequately identify modern-day
deepfakes generated by advanced generative models such as GANs, diffusion
models, and neural rendering techniques. These advanced techniques generate
nearly perfect individual frames yet inadvertently create minor temporal
discrepancies frequently overlooked by traditional detectors. We present a
novel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic
Analysis(PIA), incorporating language, dynamic face motion, and facial
identification cues to address these limitations. We utilize phoneme sequences,
lip geometry data, and advanced facial identity embeddings. This integrated
method significantly improves the detection of subtle deepfake alterations by
identifying inconsistencies across multiple complementary modalities. Code is
available at https://github.com/skrantidatta/PIA

</details>


### [17] [Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication](https://arxiv.org/abs/2510.14245)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 提出了一种新的事件间隔调制（EIM）方案，用于事件基视觉传感器（EVS）的光学相机通信（OCC），以提高传输速率并充分利用EVS的特性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于事件的OCC系统使用传统的调制方案，如开关（OOK）和脉冲位置调制，未能充分利用EVS的独特特性，并且存在低比特率和高处理负荷的问题。

Method: 提出了一种新的事件间隔调制（EIM）方案，并通过实验确定了EIM的最大调制阶数，优化了EVS的参数以匹配EIM。

Result: 在室内环境中，EIM在10米处实现了28 kbps的传输速率，在50米处实现了8.4 kbps的传输速率，创下了事件基OCC系统比特率的新纪录。

Conclusion: 提出的EIM方案能够显著提高事件基OCC系统的传输速率，并充分利用了EVS的优势，在实际应用中具有潜力。

Abstract: Optical camera communication (OCC) represents a promising visible light
communication technology. Nonetheless, typical OCC systems utilizing
frame-based cameras are encumbered by limitations, including low bit rate and
high processing load. To address these issues, OCC system utilizing an
event-based vision sensor (EVS) as receivers have been proposed. The EVS
enables high-speed, low-latency, and robust communication due to its
asynchronous operation and high dynamic range. In existing event-based OCC
systems, conventional modulation schemes such as on-off keying (OOK) and pulse
position modulation have been applied, however, to the best of our knowledge,
no modulation method has been proposed that fully exploits the unique
characteristics of the EVS. This paper proposes a novel modulation scheme,
called the event interval modulation (EIM) scheme, specifically designed for
event-based OCC. EIM enables improvement in transmission speed by modulating
information using the intervals between events. This paper proposes a
theoretical model of EIM and conducts a proof-of-concept experiment. First, the
parameters of the EVS are tuned and customized to optimize the frequency
response specifically for EIM. Then, the maximum modulation order usable in EIM
is determined experimentally. We conduct transmission experiments based on the
obtained parameters. Finally, we report successful transmission at 28 kbps over
10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new
benchmark for bit rate in event-based OCC systems.

</details>


### [18] [MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering](https://arxiv.org/abs/2510.14251)
*Mingkai Liu,Dikai Fan,Haohua Que,Haojia Gao,Xiao Liu,Shuxue Peng,Meixia Lin,Shengyu Gu,Ruicong Ye,Wanli Qiu,Handong Yao,Ruopeng Zhang,Xianliang Huang*

Main category: cs.CV

TL;DR: MACE是一种基于混合专家（MOE）的新方法，用于解决大规模场景中的高效定位和高质量渲染问题。


<details>
  <summary>Details</summary>
Motivation: 大规模场景中的计算成本阻碍了高效定位和高质量渲染。现有的场景坐标回归（SCR）方法在小规模场景中表现良好，但扩展到大规模场景时受到单一网络容量的限制。

Method: 提出了一种基于混合专家的加速坐标编码（MACE）方法，该方法包括一个门控网络，用于在每次推理时激活单个子网络。此外，还提出了一种无辅助损失的负载均衡（ALF-LB）策略来提高定位精度。

Result: MACE在降低成本的同时提高了精度，为大规模场景应用提供了有效的解决方案。

Conclusion: MACE在剑桥测试集上进行了实验，结果表明该方法在仅10分钟的训练时间内即可实现高质量的渲染效果。

Abstract: Efficient localization and high-quality rendering in large-scale scenes
remain a significant challenge due to the computational cost involved. While
Scene Coordinate Regression (SCR) methods perform well in small-scale
localization, they are limited by the capacity of a single network when
extended to large-scale scenes. To address these challenges, we propose the
Mixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables
efficient localization and high-quality rendering in large-scale scenes.
Inspired by the remarkable capabilities of MOE in large model domains, we
introduce a gating network to implicitly classify and select sub-networks,
ensuring that only a single sub-network is activated during each inference.
Furtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to
enhance the localization accuracy on large-scale scene. Our framework provides
a significant reduction in costs while maintaining higher precision, offering
an efficient solution for large-scale scene applications. Additional
experiments on the Cambridge test set demonstrate that our method achieves
high-quality rendering results with merely 10 minutes of training.

</details>


### [19] [Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization](https://arxiv.org/abs/2510.14255)
*Liao Shen,Wentao Jiang,Yiran Zhu,Tiezheng Ge,Zhiguo Cao,Bo Zheng*

Main category: cs.CV

TL;DR: 本研究提出了一种名为IPRO的新型视频扩散框架，利用强化学习和面部身份评分器来优化扩散模型，以解决图像到视频生成中人物身份一致性问题，特别是在面部占比较小的情况下。该方法通过在采样链的最后几步反向传播奖励信号来加速收敛，并引入多角度面部特征池和KL散度正则化来提高泛化能力和训练稳定性。实验证明了该方法在保持人物身份一致性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频生成模型在保持人物身份一致性方面存在困难，尤其是在人物面部表情变化大、动作幅度大或面部在图像中占比较小的情况下，这对于高度关注身份变化的观察者来说是一个关键但研究不足的挑战。

Method: 提出了一种名为IPRO（Identity-Preserving Reward-guided Optimization）的新型视频扩散框架，该框架基于强化学习。IPRO通过引入一个面部身份评分器来优化扩散模型，而无需额外的模块或修改模型架构。具体来说，该方法将奖励信号反向传播到采样链的最后几步，以获得更丰富的梯度反馈，从而提高性能并加速收敛。此外，研究人员还提出了一种新颖的面部评分机制，将真实视频中的面部视为面部特征池，以提供多角度的面部信息来增强泛化能力。同时，引入KL散度正则化来稳定训练过程，防止模型过度拟合奖励信号。

Result: 在Wan 2.2 I2V模型和研究人员自有的I2V模型上的大量实验证明了IPRO方法的有效性。

Conclusion: IPRO框架通过强化学习和面部身份评分器，有效解决了图像到视频生成中人物身份一致性的问题，尤其是在具有挑战性的场景下。该方法通过优化的训练策略和面部信息处理机制，提高了生成视频的身份保持能力和泛化性。

Abstract: Recent advances in image-to-video (I2V) generation have achieved remarkable
progress in synthesizing high-quality, temporally coherent videos from static
images. Among all the applications of I2V, human-centric video generation
includes a large portion. However, existing I2V models encounter difficulties
in maintaining identity consistency between the input human image and the
generated video, especially when the person in the video exhibits significant
expression changes and movements. This issue becomes critical when the human
face occupies merely a small fraction of the image. Since humans are highly
sensitive to identity variations, this poses a critical yet under-explored
challenge in I2V generation. In this paper, we propose Identity-Preserving
Reward-guided Optimization (IPRO), a novel video diffusion framework based on
reinforcement learning to enhance identity preservation. Instead of introducing
auxiliary modules or altering model architectures, our approach introduces a
direct and effective tuning algorithm that optimizes diffusion models using a
face identity scorer. To improve performance and accelerate convergence, our
method backpropagates the reward signal through the last steps of the sampling
chain, enabling richer gradient feedback. We also propose a novel facial
scoring mechanism that treats faces in ground-truth videos as facial feature
pools, providing multi-angle facial information to enhance generalization. A
KL-divergence regularization is further incorporated to stabilize training and
prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V
model and our in-house I2V model demonstrate the effectiveness of our method.
Our project and code are available at
\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.

</details>


### [20] [Multi-modal video data-pipelines for machine learning with minimal human supervision](https://arxiv.org/abs/2510.14862)
*Mihai-Cristian Pîrvu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 真实世界是多模态的，但目前的机器学习模型大多是单模态或双模态的。本研究旨在整合尽可能多的视觉模态，利用预训练模型和程序化组合，在几乎无人监督的情况下，通过一个全自动的数据管道处理原始视频。研究使用了 PHG-MAE 模型，并将其高效地蒸馏成一个参数量小于 100 万的模型，在语义分割任务上取得了与参数量达 3 亿的模型相媲美 results。此外，研究还将该框架应用于实时语义分割和深度估计等用例。


<details>
  <summary>Details</summary>
Motivation: 真实世界本质上是多模态的，而传统的机器学习模型在处理单一模态方面存在局限性。为了更全面地理解世界，需要整合多种独立的模态。本研究的动机是探索如何在几乎无人监督的情况下，利用预训练模型和程序化组合来整合多种视觉模态。

Method: 本研究使用预训练模型和程序化组合，在原始视频上构建了一个全自动的数据管道，以整合多种视觉模态。研究采用了 PHG-MAE 模型，并将其高效蒸馏成一个参数量小于 100 万的模型，用于处理多模态数据。

Result: PHG-MAE 模型（参数量小于 100 万）在多模态学习任务上取得了与参数量达 3 亿的模型相媲美 results。该模型被成功应用于实时语义分割和深度估计等用例，即使在普通硬件上也能实现 near real-time 的性能。

Conclusion: 本研究成功地展示了一种利用预训练模型和程序化组合，在几乎无人监督的情况下整合多种视觉模态的方法。通过高效蒸馏的 PHG-MAE 模型，即使参数量很小，也能在多模态学习任务上取得有竞争力的 results，并可应用于实时语义分割和深度估计等实际场景。

Abstract: The real-world is inherently multi-modal at its core. Our tools observe and
take snapshots of it, in digital form, such as videos or sounds, however much
of it is lost. Similarly for actions and information passing between humans,
languages are used as a written form of communication. Traditionally, Machine
Learning models have been unimodal (i.e. rgb -> semantic or text ->
sentiment_class). Recent trends go towards bi-modality, where images and text
are learned together, however, in order to truly understand the world, we need
to integrate all these independent modalities. In this work we try to combine
as many visual modalities as we can using little to no human supervision. In
order to do this, we use pre-trained experts and procedural combinations
between them on top of raw videos using a fully autonomous data-pipeline, which
we also open-source. We then make use of PHG-MAE, a model specifically designed
to leverage multi-modal data. We show that this model which was efficiently
distilled into a low-parameter (<1M) can have competitive results compared to
models of ~300M parameters. We deploy this model and analyze the use-case of
real-time semantic segmentation from handheld devices or webcams on commodity
hardware. Finally, we deploy other off-the-shelf models using the same
framework, such as DPT for near real-time depth estimation.

</details>


### [21] [Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning](https://arxiv.org/abs/2510.14256)
*Xiangyu Meng,Zixian Zhang,Zhenghao Zhang,Junchao Liao,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: Identity-GRPO通过人类反馈优化解决了现有视频生成方法在多人物身份保持方面的不足，显著提高了视频中人物身份的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法（如VACE和Phantom）在处理多个人物交互时，难以保持不同人物身份的一致性，而这在动态交互场景中至关重要。

Method: 提出Identity-GRPO，一个基于人类反馈的优化流程。首先，构建一个视频奖励模型，该模型在包含人类标注和合成失真数据的大规模偏好数据集上进行训练，重点关注视频中人物身份的一致性。然后，采用为多人物一致性定制的GRPO（Proximal Policy Optimization）变体来优化模型，从而提升现有视频生成方法（VACE和Phantom）的性能。

Result: Identity-GRPO在人类一致性指标上比基线方法提高了18.9%。通过消融实验评估了标注质量和设计选择对策略优化的影响。

Conclusion: Identity-GRPO成功地提高了多人物视频生成中身份保持的一致性，并为强化学习在个性化视频生成中的应用提供了可行的见解。

Abstract: While advanced methods like VACE and Phantom have advanced video generation
for specific subjects in diverse scenarios, they struggle with multi-human
identity preservation in dynamic interactions, where consistent identities
across multiple characters are critical. To address this, we propose
Identity-GRPO, a human feedback-driven optimization pipeline for refining
multi-human identity-preserving video generation. First, we construct a video
reward model trained on a large-scale preference dataset containing
human-annotated and synthetic distortion data, with pairwise annotations
focused on maintaining human consistency throughout the video. We then employ a
GRPO variant tailored for multi-human consistency, which greatly enhances both
VACE and Phantom. Through extensive ablation studies, we evaluate the impact of
annotation quality and design choices on policy optimization. Experiments show
that Identity-GRPO achieves up to 18.9% improvement in human consistency
metrics over baseline methods, offering actionable insights for aligning
reinforcement learning with personalized video generation.

</details>


### [22] [MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching](https://arxiv.org/abs/2510.14260)
*Tingman Yan,Tao Liu,Xilian Yang,Qunfei Zhao,Zeyang Xia*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MatchAttention的新型注意力机制，用于解决高分辨率图像的跨视图匹配问题。该机制通过动态匹配相对位置来克服现有方法的二次复杂度问题，并引入BilinearSoftmax实现连续可微分的滑动窗口注意力采样。此外，论文还提出了MatchDecoder、门控交叉匹配注意力和一致性约束损失来处理遮挡问题并提升匹配精度。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像的跨视图匹配因现有交叉注意力机制的二次复杂度和缺乏显式匹配约束而面临挑战。

Method: 提出了一种名为MatchAttention的新型注意力机制，该机制动态地匹配相对位置，并使用BilinearSoftmax实现连续可微分的滑动窗口注意力采样。相对位置被嵌入到特征通道中，并通过残差连接跨层迭代更新。基于MatchAttention设计了MatchDecoder。为了处理遮挡问题，提出了门控交叉匹配注意力和一致性约束损失。

Result: MatchStereo-B在Middlebury基准测试中平均误差排名第一，并在KITTI上实现29ms的推理时间。MatchStereo-T在0.1秒内处理4K UHD图像，仅使用3GB GPU内存。所提出的模型在KITTI 2012、KITTI 2015、ETH3D和Spring flow数据集上均达到最先进性能。

Conclusion: 所提出的MatchAttention机制及其相关模型实现了高精度和低计算复杂度的结合，使得实时、高分辨率、高精度的跨视图匹配成为可能。

Abstract: Cross-view matching is fundamentally achieved through cross-attention
mechanisms. However, matching of high-resolution images remains challenging due
to the quadratic complexity and lack of explicit matching constraints in the
existing cross-attention. This paper proposes an attention mechanism,
MatchAttention, that dynamically matches relative positions. The relative
position determines the attention sampling center of the key-value pairs given
a query. Continuous and differentiable sliding-window attention sampling is
achieved by the proposed BilinearSoftmax. The relative positions are
iteratively updated through residual connections across layers by embedding
them into the feature channels. Since the relative position is exactly the
learning target for cross-view matching, an efficient hierarchical cross-view
decoder, MatchDecoder, is designed with MatchAttention as its core component.
To handle cross-view occlusions, gated cross-MatchAttention and a
consistency-constrained loss are proposed. These two components collectively
mitigate the impact of occlusions in both forward and backward passes, allowing
the model to focus more on learning matching relationships. When applied to
stereo matching, MatchStereo-B ranked 1st in average error on the public
Middlebury benchmark and requires only 29ms for KITTI-resolution inference.
MatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU
memory. The proposed models also achieve state-of-the-art performance on KITTI
2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high
accuracy and low computational complexity makes real-time, high-resolution, and
high-accuracy cross-view matching possible. Code is available at
https://github.com/TingmanYan/MatchAttention.

</details>


### [23] [Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment](https://arxiv.org/abs/2510.14266)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 一个用于光学相机通信系统的鲁棒解调方案，使用基于事件的视觉传感器，结合了OOK、开关解调和数字锁相环，并在室外实验中实现了200米-60kbps和400米-30kbps的BER < 10^-3。


<details>
  <summary>Details</summary>
Motivation: 提出一种用于光学相机通信系统的鲁棒解调方案。

Method: 结合OOK、开关解调和数字锁相环，使用基于事件的视觉传感器。

Result: 在室外实验中，实现了200米-60kbps和400米-30kbps的BER < 10^-3。

Conclusion: 该方案是首次在室外实验中达到如此远的距离和高数据速率下BER < 10^-3的记录。

Abstract: We propose a robust demodulation scheme for optical camera communication
systems using an event-based vision sensor, combining OOK with toggle
demodulation and a digital phase-locked loop. This is the first report to
achieve a $\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor
experiments.

</details>


### [24] [CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts](https://arxiv.org/abs/2510.14273)
*Kieu-Anh Truong Thi,Huy-Hieu Pham,Duc-Trong Le*

Main category: cs.CV

TL;DR: 该研究提出了一种基于因果推断的框架，用于解决病理学图像中的域转移问题，通过利用语义特征并结合中介因素来减轻混淆因素的影响，并在两个数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于采集过程或数据源的差异，病理学图像中的域转移会严重影响深度学习模型的泛化能力。现有方法主要关注统计相关性，忽略了因果关系。

Method: 提出了一种新颖的基于因果推断的框架，利用语义特征并采用前门原则，通过设计包含中介因素和观测组织切片的转换策略来减轻混淆因素的影响。

Result: 在CAMELYON17和私有数据集上，该方法在未见过的域上实现了持续的性能提升，相较于现有基线方法，在两个数据集上均取得了最高7%的提升。

Conclusion: 因果推断为解决病理学图像分析中的域转移问题提供了一种强大的工具。

Abstract: Domain shift in histopathology, often caused by differences in acquisition
processes or data sources, poses a major challenge to the generalization
ability of deep learning models. Existing methods primarily rely on modeling
statistical correlations by aligning feature distributions or introducing
statistical variation, yet they often overlook causal relationships. In this
work, we propose a novel causal-inference-based framework that leverages
semantic features while mitigating the impact of confounders. Our method
implements the front-door principle by designing transformation strategies that
explicitly incorporate mediators and observed tissue slides. We validate our
method on the CAMELYON17 dataset and a private histopathology dataset,
demonstrating consistent performance gains across unseen domains. As a result,
our approach achieved up to a 7% improvement in both the CAMELYON17 dataset and
the private histopathology dataset, outperforming existing baselines. These
results highlight the potential of causal inference as a powerful tool for
addressing domain shift in histopathology image analysis.

</details>


### [25] [Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding](https://arxiv.org/abs/2510.14304)
*Kyungryul Back,Seongbeom Park,Milim Kim,Mincheol Kwon,SangHyeok Lee,Hyunyoung Lee,Junhee Cho,Seunghyun Park,Jinkyu Kim*

Main category: cs.CV

TL;DR: 本研究提出了一种无需训练的、三层对比解码和水印技术，以减少大型视觉语言模型（LVLM）的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）虽然在多模态任务上表现出色，但仍然容易出现幻觉，并且过度依赖单一模态或记忆训练数据，而没有充分的视觉基础。

Method: 本研究提出了一种三层对比解码和水印技术，具体包括：1. 在解码层中选择一个成熟层和一个业余层；2. 使用与水印相关的问题来识别一个支点层，以评估该层是否具有良好的视觉基础；3. 应用三层对比解码来生成最终输出。

Result: 在 POPE、MME 和 AMBER 等公共基准测试上进行的大量实验表明，该方法在减少 LVLM 的幻觉方面取得了最先进的性能，并生成了更具视觉基础的回应。

Conclusion: 该研究提出的三层对比解码和水印技术能够有效减少大型视觉语言模型的幻觉，并生成更具视觉基础的输出。

Abstract: Large Vision-Language Models (LVLMs) have recently shown promising results on
various multimodal tasks, even achieving human-comparable performance in
certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often
rely heavily on a single modality or memorize training data without properly
grounding their outputs. To address this, we propose a training-free, tri-layer
contrastive decoding with watermarking, which proceeds in three steps: (1)
select a mature layer and an amateur layer among the decoding layers, (2)
identify a pivot layer using a watermark-related question to assess whether the
layer is visually well-grounded, and (3) apply tri-layer contrastive decoding
to generate the final output. Experiments on public benchmarks such as POPE,
MME and AMBER demonstrate that our method achieves state-of-the-art performance
in reducing hallucinations in LVLMs and generates more visually grounded
responses.

</details>


### [26] [A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection](https://arxiv.org/abs/2510.14314)
*Shivangi Yadav,Arun Ross*

Main category: cs.CV

TL;DR: 本研究提出了一种名为MID-StyleGAN的生成框架，用于创建包含真实虹膜和各种伪造攻击（如打印眼、美瞳）的合成眼部图像，以解决现有虹膜身份识别系统在面对呈现攻击时的数据集不足问题。该框架结合了扩散模型和生成对抗网络（GANs）的优势，并采用多域架构和自适应损失函数来生成高质量、多样化的数据，显著提升了伪造攻击检测（PAD）系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的虹膜身份识别系统容易受到呈现攻击（PAs）的威胁，但由于构建和成像PA的固有难度，导致用于训练和评估虹膜PA检测（PAD）技术的公开数据集非常有限。因此，需要新的方法来生成高质量的PA和真实眼部图像数据。

Method: 提出了一种名为多域图像转换扩散生成对抗网络（MID-StyleGAN）的新框架。该框架结合了扩散模型和生成对抗网络（GANs）的优点，采用了多域架构，能够在真实眼部图像和不同PA域（如打印眼、美瞳）之间进行转换，并使用针对眼部数据的自适应损失函数来保持域一致性。

Result: MID-StyleGAN在生成高质量合成眼部图像方面优于现有方法。使用MID-StyleGAN生成的数据显著提高了PAD系统的性能。例如，在LivDet2020数据集上，在1%误检率下，真实检测率从93.41%提高到98.72%。

Conclusion: MID-StyleGAN为解决虹膜和眼部生物识别领域的数据稀缺问题提供了一个可扩展的解决方案，通过生成高质量的合成眼部图像，能够显著增强呈现攻击检测（PAD）系统的性能。

Abstract: An iris biometric system can be compromised by presentation attacks (PAs)
where artifacts such as artificial eyes, printed eye images, or cosmetic
contact lenses are presented to the system. To counteract this, several
presentation attack detection (PAD) methods have been developed. However, there
is a scarcity of datasets for training and evaluating iris PAD techniques due
to the implicit difficulties in constructing and imaging PAs. To address this,
we introduce the Multi-domain Image Translative Diffusion StyleGAN
(MID-StyleGAN), a new framework for generating synthetic ocular images that
captures the PA and bonafide characteristics in multiple domains such as
bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the
strengths of diffusion models and generative adversarial networks (GANs) to
produce realistic and diverse synthetic data. Our approach utilizes a
multi-domain architecture that enables the translation between bonafide ocular
images and different PA domains. The model employs an adaptive loss function
tailored for ocular data to maintain domain consistency. Extensive experiments
demonstrate that MID-StyleGAN outperforms existing methods in generating
high-quality synthetic ocular images. The generated data was used to
significantly enhance the performance of PAD systems, providing a scalable
solution to the data scarcity problem in iris and ocular biometrics. For
example, on the LivDet2020 dataset, the true detect rate at 1% false detect
rate improved from 93.41% to 98.72%, showcasing the impact of the proposed
method.

</details>


### [27] [Vision-Centric Activation and Coordination for Multimodal Large Language Models](https://arxiv.org/abs/2510.14349)
*Yunnan Wang,Fan Lu,Kecheng Zheng,Ziyuan Huang,Ziqiang Li,Wenjun Zeng,Xin Jin*

Main category: cs.CV

TL;DR: VaCo通过结合视觉中心激活和多视觉基础模型（VFMs）的协调来优化MLLM，解决了当前MLLM仅依赖文本信息的问题，显著提升了视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型（MLLMs）主要依赖文本的下一词预测进行监督，忽略了对分析能力至关重要的视觉信息。

Method: VaCo通过引入视觉判别性对齐，整合来自VFMs的任务感知感知特征，并结合可学习的模块化任务查询（MTQs）和视觉对齐层（VALs），在多种VFMs的监督下激活特定的视觉信号。同时，利用精心设计的Token Gateway Mask（TGM）来限制MTQ组之间的信息流，以协调VFMs之间的表示冲突。

Result: 实验证明，VaCo在多个基准测试中显著提升了不同MLLMs的性能，在视觉理解方面表现出色。

Conclusion: VaCo能够有效优化MLLM的表示，通过视觉中心激活和多VFMs协调，弥补了现有模型在视觉信息利用上的不足，提升了模型的视觉理解能力。

Abstract: Multimodal large language models (MLLMs) integrate image features from visual
encoders with LLMs, demonstrating advanced comprehension capabilities. However,
mainstream MLLMs are solely supervised by the next-token prediction of textual
tokens, neglecting critical vision-centric information essential for analytical
abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM
representations through Vision-Centric activation and Coordination from
multiple vision foundation models (VFMs). VaCo introduces visual discriminative
alignment to integrate task-aware perceptual features extracted from VFMs,
thereby unifying the optimization of both textual and visual outputs in MLLMs.
Specifically, we incorporate the learnable Modular Task Queries (MTQs) and
Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals
under the supervision of diverse VFMs. To coordinate representation conflicts
across VFMs, the crafted Token Gateway Mask (TGM) restricts the information
flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo
significantly improves the performance of different MLLMs on various
benchmarks, showcasing its superior capabilities in visual comprehension.

</details>


### [28] [Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration](https://arxiv.org/abs/2510.14354)
*Siddharth Tourani,Jayaram Reddy,Sarvesh Thakur,K Madhava Krishna,Muhammad Haris Khan,N Dinesh Reddy*

Main category: cs.CV

TL;DR: 利用循环一致的关键点和新颖的姿态块（结合GRU和变换同步）进行无监督RGB-D图像配准，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 如何利用大量的无标签RGB-D数据进行几何推理。

Method: 使用循环一致的关键点强制执行空间连贯性约束，并引入一个结合GRU循环单元和变换同步的新型姿态块来融合历史和多视图数据。

Result: 在ScanNet和3DMatch数据集上超越了以前的无监督配准方法，甚至优于一些有监督方法。

Conclusion: 所提出的循环一致的关键点和姿态块方法在无监督RGB-D图像配准方面取得了显著的成功，并能提升现有方法性能。

Abstract: With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has
become available. This prompts the question of how to utilize this data for
geometric reasoning of scenes. While many RGB-D registration meth- ods rely on
geometric and feature-based similarity, we take a different approach. We use
cycle-consistent keypoints as salient points to enforce spatial coherence
constraints during matching, improving correspondence accuracy. Additionally,
we introduce a novel pose block that combines a GRU recurrent unit with
transformation synchronization, blending historical and multi-view data. Our
approach surpasses previous self- supervised registration methods on ScanNet
and 3DMatch, even outperforming some older supervised methods. We also
integrate our components into existing methods, showing their effectiveness.

</details>


### [29] [Spatial Preference Rewarding for MLLMs Spatial Understanding](https://arxiv.org/abs/2510.14374)
*Han Qiu,Peng Gao,Lewei Lu,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: SPR 通过奖励 MLLMs 对图像区域的精确描述和定位来提升其细粒度空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLMs 在细粒度空间感知（如区域描述、精确定位）方面存在不足，并且未能满足用户对细粒度空间理解的需求。这是因为现有方法主要依赖预先标注的指令数据来注入空间知识，而没有直接监督 MLLMs 的实际响应。

Method: SPR (Spatial Preference Rewarding) 方法通过引入语义和定位分数来全面评估 MLLMs 生成的描述在文本质量和定位质量方面。它还优化了 MLLMs 的描述，提高了定位精度，并将评分最高的优化描述与得分最低的初始描述配对，以进行直接偏好优化。

Result: SPR 在标准的 Referring 和 Grounding 基准测试中，能够有效提升 MLLMs 的空间理解能力，并且训练开销极小。

Conclusion: SPR 是一种有效的、开销极小的方法，通过偏好奖励机制显著增强了 MLLMs 的细粒度空间理解和视觉输入对齐能力。

Abstract: Multimodal large language models~(MLLMs) have demonstrated promising spatial
understanding capabilities, such as referencing and grounding object
descriptions. Despite their successes, MLLMs still fall short in fine-grained
spatial perception abilities, such as generating detailed region descriptions
or accurately localizing objects. Additionally, they often fail to respond to
the user's requirements for desired fine-grained spatial understanding. This
issue might arise because existing approaches primarily focus on tuning MLLMs
to model pre-annotated instruction data to inject spatial knowledge, without
direct supervision of MLLMs' actual responses. We address this issue by SPR, a
Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial
capabilities by rewarding MLLMs' detailed responses with precise object
localization over vague or inaccurate responses. With randomly selected image
regions and region descriptions from MLLMs, SPR introduces semantic and
localization scores to comprehensively evaluate the text quality and
localization quality in MLLM-generated descriptions. We also refine the MLLM
descriptions with better localization accuracy and pair the best-scored
refinement with the initial descriptions of the lowest score for direct
preference optimization, thereby enhancing fine-grained alignment with visual
input. Extensive experiments over standard referring and grounding benchmarks
show that SPR improves MLLM spatial understanding capabilities effectively with
minimal overhead in training. Data and code will be released at
https://github.com/hanqiu-hq/SPR

</details>


### [30] [DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation](https://arxiv.org/abs/2510.14376)
*Dongnam Byun,Jungwon Park,Jumgmin Ko,Changin Choi,Wonjong Rhee*

Main category: cs.CV

TL;DR: DOS 通过修改 CLIP 文本嵌入来改进涉及多个对象的文本到图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在处理涉及多个对象的提示时存在对象遗漏或混合的问题，尤其是在对象形状/纹理相似、背景偏差或对象数量众多时。

Method: DOS（定向对象分离）通过修改 CLIP 文本嵌入来解决多对象问题。

Result: DOS 持续提高了多对象图像生成的成功率并减少了对象混合，在人类评估中显著优于其他方法。

Conclusion: DOS 是一种实用且有效的解决方案，可用于改进多对象图像生成。

Abstract: Recent progress in text-to-image (T2I) generative models has led to
significant improvements in generating high-quality images aligned with text
prompts. However, these models still struggle with prompts involving multiple
objects, often resulting in object neglect or object mixing. Through extensive
studies, we identify four problematic scenarios, Similar Shapes, Similar
Textures, Dissimilar Background Biases, and Many Objects, where inter-object
relationships frequently lead to such failures. Motivated by two key
observations about CLIP embeddings, we propose DOS (Directional Object
Separation), a method that modifies three types of CLIP text embeddings before
passing them into text-to-image models. Experimental results show that DOS
consistently improves the success rate of multi-object image generation and
reduces object mixing. In human evaluations, DOS significantly outperforms four
competing methods, receiving 26.24%-43.04% more votes across four benchmarks.
These results highlight DOS as a practical and effective solution for improving
multi-object image generation.

</details>


### [31] [DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights](https://arxiv.org/abs/2510.14383)
*Danish Ali,Ajmal Mian,Naveed Akhtar,Ghulam Mubashar Hassan*

Main category: cs.CV

TL;DR: DRBD-Mamba是一种高效的3D脑肿瘤分割模型，通过利用空间填充曲线和门控融合模块，在保持准确性的同时显著降低了计算开销，并在BraTS数据集上展示了优越的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于Mamba的状态空间模型在脑肿瘤分割方面虽然表现出潜力，但存在显著的计算开销问题，并且其在不同BraTS数据划分下的鲁棒性评估不足，存在可靠性评估的空白。

Method: 提出了一种名为DRBD-Mamba的双分辨率双向Mamba模型。该模型采用空间填充曲线将3D特征映射到1D，以保留空间局部性并减少计算量。引入门控融合模块自适应地融合正向和反向上下文，并通过量化块离散化特征以增强鲁棒性。此外，在BraTS2023上创建了五个系统性数据划分，用于对分割技术进行严格评估。

Result: 在BraTS2023的20%测试集上，DRBD-Mamba在全肿瘤、肿瘤核心和增强肿瘤分割上的Dice系数分别提高了0.10%、1.75%和0.93%。在提出的五个系统性数据划分上的评估显示，与现有最先进的方法相比，DRBD-Mamba在肿瘤核心和增强肿瘤分割上的平均Dice系数分别提高了0.86%和1.45%，同时保持了全肿瘤分割的竞争力。此外，该模型在保持高分割精度的同时，实现了15倍的效率提升。

Conclusion: DRBD-Mamba通过其创新的方法，在脑肿瘤分割任务中实现了高精度、高效率和高鲁棒性的结合，有效解决了现有方法的局限性，为临床诊断和治疗提供了有力的支持。

Abstract: Accurate brain tumor segmentation is significant for clinical diagnosis and
treatment. It is challenging due to the heterogeneity of tumor subregions.
Mamba-based State Space Models have demonstrated promising performance.
However, they incur significant computational overhead due to sequential
feature computation across multiple spatial axes. Moreover, their robustness
across diverse BraTS data partitions remains largely unexplored, leaving a
critical gap in reliable evaluation. To address these limitations, we propose
dual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation
model that captures multi-scale long-range dependencies with minimal
computational overhead. We leverage a space-filling curve to preserve spatial
locality during 3D-to-1D feature mapping, thereby reducing reliance on
computationally expensive multi-axial feature scans. To enrich feature
representation, we propose a gated fusion module that adaptively integrates
forward and reverse contexts, along with a quantization block that discretizes
features to improve robustness. In addition, we propose five systematic folds
on BraTS2023 for rigorous evaluation of segmentation techniques under diverse
conditions and present detailed analysis of common failure scenarios. On the
20\% test set used by recent methods, our model achieves Dice improvements of
0.10\% for whole tumor, 1.75\% for tumor core, and 0.93\% for enhancing tumor.
Evaluations on the proposed systematic five folds demonstrate that our model
maintains competitive whole tumor accuracy while achieving clear average Dice
gains of 0.86\% for tumor core and 1.45\% for enhancing tumor over existing
state-of-the-art. Furthermore, our model attains 15 times improvement in
efficiency while maintaining high segmentation accuracy, highlighting its
robustness and computational advantage over existing approaches.

</details>


### [32] [BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble](https://arxiv.org/abs/2510.14389)
*Brandon Hill,Kma Solaiman*

Main category: cs.CV

TL;DR: 该研究提出了一个名为BoardVision的框架，用于检测主板组装级别的缺陷，并对YOLOv7和Faster R-CNN进行了系统性比较，提出了CTV Voter集成模型来平衡精确率和召回率，并评估了模型在不同扰动下的鲁棒性，最后发布了一个可部署的GUI工具，旨在将计算机视觉技术应用于实际主板制造的质量保证中。


<details>
  <summary>Details</summary>
Motivation: 主板缺陷检测对于电子产品的大规模制造至关重要，但现有研究主要集中在裸板或走线级别，组装级别缺陷的检测仍有待探索。

Method: 提出BoardVision框架，在MiracleFactory主板数据集上对YOLOv7和Faster R-CNN进行基准测试和系统性比较，提出CTV Voter集成模型来平衡精确率和召回率，并评估模型在锐度、亮度、方向改变等扰动下的鲁棒性。

Result: YOLOv7在精确率方面表现优异，但在召回率方面表现不佳；Faster R-CNN则相反。CTV Voter集成模型能够平衡精确率和召回率。研究还评估了模型在各种扰动下的稳定性。

Conclusion: 计算机视觉技术可以从基准测试结果过渡到实际应用中，为组装级别的主板制造提供实用的质量保证。所提出的BoardVision框架、CTV Voter模型以及GUI工具为该领域做出了贡献。

Abstract: Motherboard defect detection is critical for ensuring reliability in
high-volume electronics manufacturing. While prior research in PCB inspection
has largely targeted bare-board or trace-level defects, assembly-level
inspection of full motherboards inspection remains underexplored. In this work,
we present BoardVision, a reproducible framework for detecting assembly-level
defects such as missing screws, loose fan wiring, and surface scratches. We
benchmark two representative detectors - YOLOv7 and Faster R-CNN, under
controlled conditions on the MiracleFactory motherboard dataset, providing the
first systematic comparison in this domain. To mitigate the limitations of
single models, where YOLO excels in precision but underperforms in recall and
Faster R-CNN shows the reverse, we propose a lightweight ensemble,
Confidence-Temporal Voting (CTV Voter), that balances precision and recall
through interpretable rules. We further evaluate robustness under realistic
perturbations including sharpness, brightness, and orientation changes,
highlighting stability challenges often overlooked in motherboard defect
detection. Finally, we release a deployable GUI-driven inspection tool that
bridges research evaluation with operator usability. Together, these
contributions demonstrate how computer vision techniques can transition from
benchmark results to practical quality assurance for assembly-level motherboard
manufacturing.

</details>


### [33] [DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis](https://arxiv.org/abs/2510.14403)
*Chao Tu,Kun Huang,Jie Zhang,Qianjin Feng,Yu Zhang,Zhenyuan Ning*

Main category: cs.CV

TL;DR: DCMIL是一种新提出的计算病理学模型，可以有效处理全切片图像（WSI）并进行癌症预后预测，无需密集注释。


<details>
  <summary>Details</summary>
Motivation: 计算病理学在癌症预后方面显示出潜力，但面临计算瓶颈和标注稀缺的挑战。现有方法忽略了多放大倍率WSI和肿瘤微环境的细粒度信息。

Method: 提出了一种易到难渐进式表示学习模型，称为双课程对比多实例学习（DCMIL），无需密集注释，可直接将巨像级WSI转化为预后预测。

Result: 在十二种癌症类型（5,954名患者，12.54百万个图像块）上进行的广泛实验表明，DCMIL的性能优于标准的WSI预后模型。此外，DCMIL还能识别出细粒度的预后相关区域，提供鲁棒的实例不确定性估计，并捕捉正常和肿瘤组织之间的形态差异。

Conclusion: DCMIL是一种有效处理WSI进行癌症预后的模型，具有识别细粒度预后相关区域、提供不确定性估计和捕捉形态差异的潜力，有望产生新的生物学见解。

Abstract: The burgeoning discipline of computational pathology shows promise in
harnessing whole slide images (WSIs) to quantify morphological heterogeneity
and develop objective prognostic modes for human cancers. However, progress is
impeded by the computational bottleneck of gigapixel-size inputs and the
scarcity of dense manual annotations. Current methods often overlook
fine-grained information across multi-magnification WSIs and variations in
tumor microenvironments. Here, we propose an easy-to-hard progressive
representation learning model, termed dual-curriculum contrastive
multi-instance learning (DCMIL), to efficiently process WSIs for cancer
prognosis. The model does not rely on dense annotations and enables the direct
transformation of gigapixel-size WSIs into outcome predictions. Extensive
experiments on twelve cancer types (5,954 patients, 12.54 million tiles)
demonstrate that DCMIL outperforms standard WSI-based prognostic models.
Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides
robust instance uncertainty estimation, and captures morphological differences
between normal and tumor tissues, with the potential to generate new biological
insights. All codes have been made publicly accessible at
https://github.com/tuuuc/DCMIL.

</details>


### [34] [Real-Time Neural Video Compression with Unified Intra and Inter Coding](https://arxiv.org/abs/2510.14431)
*Hui Xiang,Yifan Bian,Li Li,Jingran Wu,Xianguo Zhang,Dong Liu*

Main category: cs.CV

TL;DR: 该研究提出了一种新的神经视频压缩（NVC）框架，通过统一的帧内和帧间编码来解决现有NVC方法的局限性，并实现了比现有先进方法（如DCVC-RT）更高的压缩效率和更稳定的比特率/质量。


<details>
  <summary>Details</summary>
Motivation: 现有神经视频压缩（NVC）方法在处理遮挡、新内容和帧间误差累积方面存在不足。

Method: 提出了一种统一的帧内/帧间编码NVC框架，使用单一模型自适应地处理这两种编码模式。此外，还提出了一种双向两帧压缩设计，以利用前向和后向的帧间冗余。

Result: 所提出的框架在BD-rate上比DCVC-RT平均降低了10.7%，提供了更稳定的每帧比特率和质量，同时保持了实时编解码性能。

Conclusion: 该统一编码框架有效解决了现有NVC方法的局限性，并在压缩效率和性能稳定性方面取得了显著提升。

Abstract: Neural video compression (NVC) technologies have advanced rapidly in recent
years, yielding state-of-the-art schemes such as DCVC-RT that offer superior
compression efficiency to H.266/VVC and real-time encoding/decoding
capabilities. Nonetheless, existing NVC schemes have several limitations,
including inefficiency in dealing with disocclusion and new content, interframe
error propagation and accumulation, among others. To eliminate these
limitations, we borrow the idea from classic video coding schemes, which allow
intra coding within inter-coded frames. With the intra coding tool enabled,
disocclusion and new content are properly handled, and interframe error
propagation is naturally intercepted without the need for manual refresh
mechanisms. We present an NVC framework with unified intra and inter coding,
where every frame is processed by a single model that is trained to perform
intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame
compression design to exploit interframe redundancy not only forwardly but also
backwardly. Experimental results show that our scheme outperforms DCVC-RT by an
average of 10.7\% BD-rate reduction, delivers more stable bitrate and quality
per frame, and retains real-time encoding/decoding performances. Code and
models will be released.

</details>


### [35] [Structured Universal Adversarial Attacks on Object Detection for Video Sequences](https://arxiv.org/abs/2510.14460)
*Sven Jacob,Weijia Shao,Gjergji Kasneci*

Main category: cs.CV

TL;DR: 提出了一种针对视频目标检测的、以核范数正则化为基础的、以背景为中心的、结构化扰动的、最小失真通用对抗性攻击方法，并使用自适应指数梯度算法进行优化，该方法在保持高隐蔽性的同时，在有效性上优于其他基于低秩和Frank-Wolfe的攻击。


<details>
  <summary>Details</summary>
Motivation: 深度学习目标检测器虽然性能优越，但在视频目标检测的安全关键应用中，仍然容易受到包括通用扰动在内的对抗性攻击。

Method: 提出了一种通用的对抗性攻击方法，该方法利用核范数正则化来促进集中在背景中的结构化扰动，并使用自适应指数梯度方法进行优化。

Result: 所提出的攻击方法在有效性方面优于基于低秩和Frank-Wolfe的攻击，同时保持了高隐蔽性。

Conclusion: 本研究提出了一种有效的、具有高隐蔽性的、针对视频目标检测的通用对抗性攻击方法。

Abstract: Video-based object detection plays a vital role in safety-critical
applications. While deep learning-based object detectors have achieved
impressive performance, they remain vulnerable to adversarial attacks,
particularly those involving universal perturbations. In this work, we propose
a minimally distorted universal adversarial attack tailored for video object
detection, which leverages nuclear norm regularization to promote structured
perturbations concentrated in the background. To optimize this formulation
efficiently, we employ an adaptive, optimistic exponentiated gradient method
that enhances both scalability and convergence. Our results demonstrate that
the proposed attack outperforms both low-rank projected gradient descent and
Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness.
All code and data are publicly available at
https://github.com/jsve96/AO-Exp-Attack.

</details>


### [36] [Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review](https://arxiv.org/abs/2510.14462)
*Youwan Mahé,Elise Bannier,Stéphanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: 无监督深度生成模型在脑成像异常检测中展现出潜力，可处理稀有或异构疾病，未来需关注解剖结构感知、基础模型、评估指标和临床验证。


<details>
  <summary>Details</summary>
Motivation: 无监督深度生成模型可作为监督方法的替代方案，用于脑成像的异常检测和分割，尤其适用于缺乏大规模标注数据的场景，能识别出与正常脑结构偏差。

Method: 对2018-2025年间发表的关于无监督深度生成模型（包括自编码器、变分自编码器、生成对抗网络和去噪扩散模型）在脑成像异常检测中的应用进行了PRISMA引导的范围审查，共纳入49项研究。

Result: 在脑MRI和CT成像中，生成模型在检测大病灶方面表现出令人鼓舞的性能，并在解决细微异常方面取得进展。该模型能生成可解释的伪健康重建图，这在处理罕见或异构疾病时尤其有价值。

Conclusion: 无监督深度生成模型为异常检测提供了一个有前景的方向，可实现半监督学习、发现新的成像生物标志物，并在统一的端到端框架中支持疾病内和跨疾病的偏差映射。为实现临床应用，未来的研究应优先考虑解剖结构感知建模、基础模型开发、任务适当的评估指标以及严格的临床验证。

Abstract: Unsupervised deep generative models are emerging as a promising alternative
to supervised methods for detecting and segmenting anomalies in brain imaging.
Unlike fully supervised approaches, which require large voxel-level annotated
datasets and are limited to well-characterised pathologies, these models can be
trained exclusively on healthy data and identify anomalies as deviations from
learned normative brain structures. This PRISMA-guided scoping review
synthesises recent work on unsupervised deep generative models for anomaly
detection in neuroimaging, including autoencoders, variational autoencoders,
generative adversarial networks, and denoising diffusion models. A total of 49
studies published between 2018 - 2025 were identified, covering applications to
brain MRI and, less frequently, CT across diverse pathologies such as tumours,
stroke, multiple sclerosis, and small vessel disease. Reported performance
metrics are compared alongside architectural design choices. Across the
included studies, generative models achieved encouraging performance for large
focal lesions and demonstrated progress in addressing more subtle
abnormalities. A key strength of generative models is their ability to produce
interpretable pseudo-healthy (also referred to as counterfactual)
reconstructions, which is particularly valuable when annotated data are scarce,
as in rare or heterogeneous diseases. Looking ahead, these models offer a
compelling direction for anomaly detection, enabling semi-supervised learning,
supporting the discovery of novel imaging biomarkers, and facilitating within-
and cross-disease deviation mapping in unified end-to-end frameworks. To
realise clinical impact, future work should prioritise anatomy-aware modelling,
development of foundation models, task-appropriate evaluation metrics, and
rigorous clinical validation.

</details>


### [37] [Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration](https://arxiv.org/abs/2510.14463)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MIR-L的压缩策略，用于压缩多任务图像恢复模型，通过迭代剪枝和权重重置，在仅保留10%参数的情况下，仍能达到先进水平的图像恢复性能。


<details>
  <summary>Details</summary>
Motivation: 在线社交网络（OSNs）的图像压缩操作会降低图像质量，影响用户体验。现有的多任务图像恢复模型虽然能处理多种图像退化问题，但参数量过大，计算效率低下。

Method: MIR-L模型采用迭代剪枝策略，通过多轮移除低幅度权重，并重置剩余权重，以发掘能够匹配甚至超越密集模型的稀疏子网络。

Result: 在去雨、去雾和去噪任务的基准数据集上进行实验评估，结果表明MIR-L在仅保留10%可训练参数的情况下，仍能保持高图像恢复性能。

Conclusion: MIR-L通过有效的剪枝策略，实现了多任务图像恢复模型的显著压缩，同时保持了高性能，为解决计算效率问题提供了可行方案。

Abstract: Image quality is a critical factor in delivering visually appealing content
on web platforms. However, images often suffer from degradation due to lossy
operations applied by online social networks (OSNs), negatively affecting user
experience. Image restoration is the process of recovering a clean high-quality
image from a given degraded input. Recently, multi-task (all-in-one) image
restoration models have gained significant attention, due to their ability to
simultaneously handle different types of image degradations. However, these
models often come with an excessively high number of trainable parameters,
making them computationally inefficient. In this paper, we propose a strategy
for compressing multi-task image restoration models. We aim to discover highly
sparse subnetworks within overparameterized deep models that can match or even
surpass the performance of their dense counterparts. The proposed model, namely
MIR-L, utilizes an iterative pruning strategy that removes low-magnitude
weights across multiple rounds, while resetting the remaining weights to their
original initialization. This iterative process is important for the multi-task
image restoration model's optimization, effectively uncovering "winning
tickets" that maintain or exceed state-of-the-art performance at high sparsity
levels. Experimental evaluation on benchmark datasets for the deraining,
dehazing, and denoising tasks shows that MIR-L retains only 10% of the
trainable parameters while maintaining high image restoration performance. Our
code, datasets and pre-trained models are made publicly available at
https://github.com/Thomkat/MIR-L.

</details>


### [38] [Grazing Detection using Deep Learning and Sentinel-2 Time Series Data](https://arxiv.org/abs/2510.14493)
*Aleksis Pirinen,Delia Fano Yela,Smita Chakraborty,Erik Källman*

Main category: cs.CV

TL;DR: 通过 Sentinel-2 L2A 卫星数据和 CNN-LSTM 模型，实现了对季节性放牧的监测，提高了检查效率。


<details>
  <summary>Details</summary>
Motivation: 现有放牧监测方法难以规模化，而放牧对农业生产和生物多样性至关重要。

Method: 利用 4 月至 10 月的 Sentinel-2 L2A 影像，对每个地块进行二元分类（放牧/未放牧）。训练了一个包含 CNN-LSTM 模型的集成模型，并使用多时序反射率特征。

Result: 在五个验证集中，模型的平均 F1 分数达到 77%，对放牧草地的召回率达到 90%。如果检查员每年最多只能访问 4% 的地点，那么优先检查模型预测为未放牧的区域，可以使已确认未放牧的地点数量比随机检查多 17.2 倍。

Conclusion: 免费的、低分辨率的卫星数据可以有效地指导检查资源，以确保符合与保护相关的土地利用规定。

Abstract: Grazing shapes both agricultural production and biodiversity, yet scalable
monitoring of where grazing occurs remains limited. We study seasonal grazing
detection from Sentinel-2 L2A time series: for each polygon-defined field
boundary, April-October imagery is used for binary prediction (grazed / not
grazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance
features, and achieve an average F1 score of 77 percent across five validation
splits, with 90 percent recall on grazed pastures. Operationally, if inspectors
can visit at most 4 percent of sites annually, prioritising fields predicted by
our model as non-grazed yields 17.2 times more confirmed non-grazing sites than
random inspection. These results indicate that coarse-resolution, freely
available satellite data can reliably steer inspection resources for
conservation-aligned land-use compliance. Code and models have been made
publicly available.

</details>


### [39] [Vision Mamba for Permeability Prediction of Porous Media](https://arxiv.org/abs/2510.14516)
*Ali Kashefi,Tapan Mukerji*

Main category: cs.CV

TL;DR: Vision Mamba因其网络尺寸随输入图像分辨率线性扩展，而Vision Transformers（ViTs）是二次扩展，从而提高了计算和内存效率，并且Vision Mamba需要的可训练参数比传统卷积神经网络（CNN）少，因此，我们首次提出使用Vision Mamba作为骨干网络来预测三维多孔介质的渗透率，并证明了其优于ViTs和CNNs的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决ViTs在图像分辨率上二次扩展带来的计算和内存效率问题，以及CNNs需要更多可训练参数的问题，提出使用Vision Mamba作为骨干网络来预测三维多孔介质的渗透率。

Method: 提出使用Vision Mamba作为骨干网络，并与ViT和CNN模型在渗透率预测的多个方面进行性能比较，同时进行消融研究以评估其组件对准确性的影响。

Result: 在三维多孔介质的渗透率预测中，Vision Mamba相比于ViTs和CNNs在计算和内存效率以及模型参数量方面展现出优势，并且在准确性方面表现出色。

Conclusion: Vision Mamba在三维多孔介质渗透率预测任务中展现出优越性，并有潜力应用于其他大型视觉模型中。

Abstract: Vision Mamba has recently received attention as an alternative to Vision
Transformers (ViTs) for image classification. The network size of Vision Mamba
scales linearly with input image resolution, whereas ViTs scale quadratically,
a feature that improves computational and memory efficiency. Moreover, Vision
Mamba requires a significantly smaller number of trainable parameters than
traditional convolutional neural networks (CNNs), and thus, they can be more
memory efficient. Because of these features, we introduce, for the first time,
a neural network that uses Vision Mamba as its backbone for predicting the
permeability of three-dimensional porous media. We compare the performance of
Vision Mamba with ViT and CNN models across multiple aspects of permeability
prediction and perform an ablation study to assess the effects of its
components on accuracy. We demonstrate in practice the aforementioned
advantages of Vision Mamba over ViTs and CNNs in the permeability prediction of
three-dimensional porous media. We make the source code publicly available to
facilitate reproducibility and to enable other researchers to build on and
extend this work. We believe the proposed framework has the potential to be
integrated into large vision models in which Vision Mamba is used instead of
ViTs.

</details>


### [40] [Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing](https://arxiv.org/abs/2510.14525)
*Qurrat Ul Ain,Atif Aftab Ahmed Jilani,Zunaira Shafqat,Nigar Azhar Butt*

Main category: cs.CV

TL;DR: SurgScan是一个AI驱动的手术器械缺陷检测框架，使用YOLOv8在实时中进行分类，准确率为99.3%，推断速度为4.2-5.8毫秒，可用于工业部署。


<details>
  <summary>Details</summary>
Motivation: 手动检查手术器械存在人为错误和不一致性的风险，可能影响无菌性、机械完整性和患者安全。

Method: 使用YOLOv8训练了一个AI模型，该模型在包含102,876张图像的数据集上进行了训练，涵盖了11种器械和5种主要缺陷类别，并使用了对比度增强的预处理。

Result: SurgScan在准确率（99.3%）和实时推断速度（4.2-5.8毫秒）方面优于最先进的CNN架构，并证明了对比度增强预处理的有效性。

Conclusion: SurgScan为自动化手术器械质量控制提供了一个可扩展、经济高效的AI解决方案，符合ISO 13485和FDA标准，可减少对人工检查的依赖，并提高缺陷检测能力。

Abstract: Defective surgical instruments pose serious risks to sterility, mechanical
integrity, and patient safety, increasing the likelihood of surgical
complications. However, quality control in surgical instrument manufacturing
often relies on manual inspection, which is prone to human error and
inconsistency. This study introduces SurgScan, an AI-powered defect detection
framework for surgical instruments. Using YOLOv8, SurgScan classifies defects
in real-time, ensuring high accuracy and industrial scalability. The model is
trained on a high-resolution dataset of 102,876 images, covering 11 instrument
types and five major defect categories. Extensive evaluation against
state-of-the-art CNN architectures confirms that SurgScan achieves the highest
accuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image,
making it suitable for industrial deployment. Statistical analysis demonstrates
that contrast-enhanced preprocessing significantly improves defect detection,
addressing key limitations in visual inspection. SurgScan provides a scalable,
cost-effective AI solution for automated quality control, reducing reliance on
manual inspection while ensuring compliance with ISO 13485 and FDA standards,
paving the way for enhanced defect detection in medical manufacturing.

</details>


### [41] [Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models](https://arxiv.org/abs/2510.14526)
*Yunze Tong,Didi Zhu,Zijing Hu,Jinluan Yang,Ziyu Zhao*

Main category: cs.CV

TL;DR: 通过引入一个提示感知的噪声投影器来解决文本到图像生成中的提示-图像不匹配问题，该投影器在去噪之前对初始噪声进行条件化细化，从而使噪声分布更接近训练时观察到的分布，而无需修改预训练的稳定扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成方法在不同的初始噪声下会产生不同的去噪路径，但这可能导致一些图像与提示不匹配。现有的解决方法要么改变去噪动力学，要么进行多次采样和后选择。

Method: 提出一个噪声投影器，在去噪之前对初始噪声进行文本条件细化。该方法首先从视觉语言模型（VLM）获取噪声及其对应图像的 token 级反馈，然后将这些信号蒸馏成一个奖励模型，最后通过准直接偏好优化来优化噪声投影器。

Result: 该方法无需参考图像或手工先验，并且推理成本很低，通过单次前向传播替代了多样本选择。大量实验表明，所提出的提示感知噪声投影能够提高各种提示下的文本-图像匹配度。

Conclusion: 所提出的噪声投影器能够有效解决文本到图像生成中的提示-图像不匹配问题，提高了生成图像与提示的对齐度，且具有低推理成本和无需参考图像的优点。

Abstract: In text-to-image generation, different initial noises induce distinct
denoising paths with a pretrained Stable Diffusion (SD) model. While this
pattern could output diverse images, some of them may fail to align well with
the prompt. Existing methods alleviate this issue either by altering the
denoising dynamics or by drawing multiple noises and conducting post-selection.
In this paper, we attribute the misalignment to a training-inference mismatch:
during training, prompt-conditioned noises lie in a prompt-specific subset of
the latent space, whereas at inference the noise is drawn from a
prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector
that applies text-conditioned refinement to the initial noise before denoising.
Conditioned on the prompt embedding, it maps the noise to a prompt-aware
counterpart that better matches the distribution observed during SD training,
without modifying the SD model. Our framework consists of these steps: we first
sample some noises and obtain token-level feedback for their corresponding
images from a vision-language model (VLM), then distill these signals into a
reward model, and finally optimize the noise projector via a quasi-direct
preference optimization. Our design has two benefits: (i) it requires no
reference images or handcrafted priors, and (ii) it incurs small inference
cost, replacing multi-sample selection with a single forward pass. Extensive
experiments further show that our prompt-aware noise projection improves
text-image alignment across diverse prompts.

</details>


### [42] [PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model](https://arxiv.org/abs/2510.14528)
*Cheng Cui,Ting Sun,Suyin Liang,Tingquan Gao,Zelun Zhang,Jiaxuan Liu,Xueqing Wang,Changda Zhou,Hongen Liu,Manhui Lin,Yue Zhang,Yubo Zhang,Handong Zheng,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR-VL是一款高效的文档解析模型，支持109种语言，在文本、表格、公式和图表识别方面达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 提出一种用于文档解析的SOTA且资源高效的模型PaddleOCR-VL。

Method: PaddleOCR-VL-0.9B模型，集成NaViT风格的动态分辨率视觉编码器和ERNIE-4.5-0.3B语言模型。

Result: 在公开和内部基准测试中，PaddleOCR-VL在页面级文档解析和元素级识别方面均达到SOTA性能，优于现有解决方案，并具有快速的推理速度。

Conclusion: PaddleOCR-VL凭借其卓越的性能和效率，非常适合在实际场景中部署。

Abstract: In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model
tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a
compact yet powerful vision-language model (VLM) that integrates a NaViT-style
dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to
enable accurate element recognition. This innovative model efficiently supports
109 languages and excels in recognizing complex elements (e.g., text, tables,
formulas, and charts), while maintaining minimal resource consumption. Through
comprehensive evaluations on widely used public benchmarks and in-house
benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document
parsing and element-level recognition. It significantly outperforms existing
solutions, exhibits strong competitiveness against top-tier VLMs, and delivers
fast inference speeds. These strengths make it highly suitable for practical
deployment in real-world scenarios.

</details>


### [43] [Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology](https://arxiv.org/abs/2510.14532)
*Xinrui Huang,Fan Xiao,Dongming He,Anqi Gao,Dandan Li,Xiaofan Zhang,Shaoting Zhang,Xudong Wang*

Main category: cs.CV

TL;DR: DentVFM是首个用于牙科的视觉基础模型（VFM），通过自监督学习在大型多模态牙科影像数据集（DentVista）上进行训练，能够为多种牙科应用生成通用的视觉表征，并显著优于现有基线模型，有望改善智能牙科护理。


<details>
  <summary>Details</summary>
Motivation: 现有的牙科AI系统因其单一模式、特定任务设计和对昂贵标记数据的依赖，在泛化能力方面存在局限，而训练有素的口腔颌面放射科医生短缺，限制了放射影像解读。DentVFM旨在解决这些挑战，提供一个更通用、适应性更强且标签效率更高的解决方案。

Method: 提出DentVFM，首个牙科视觉基础模型（VFM），采用基于Vision Transformer（ViT）架构的2D和3D变体，利用自监督学习在DentVista（一个包含约160万份多模态牙科放射影像的大型数据集）上进行训练。同时，引入DentBench，一个涵盖八个牙科亚专科、多种疾病、成像模式和广泛地理分布的综合基准。

Result: DentVFM展现了出色的通用智能，在疾病诊断、治疗分析、生物标志物识别、解剖标志物检测与分割等多种牙科任务上表现出强大的泛化能力。实验结果显示，DentVFM在泛化能力、标签效率和可扩展性方面显著优于监督、自监督和弱监督基线模型。此外，DentVFM能够实现跨模态诊断，在常规成像不可用时提供比经验丰富的牙医更可靠的结果。

Conclusion: DentVFM为牙科AI树立了新范式，提供了一个可扩展、适应性强且标签效率高的模型，以改善智能牙科护理，并解决全球口腔医疗保健中的关键差距。

Abstract: Oral and maxillofacial radiology plays a vital role in dental healthcare, but
radiographic image interpretation is limited by a shortage of trained
professionals. While AI approaches have shown promise, existing dental AI
systems are restricted by their single-modality focus, task-specific design,
and reliance on costly labeled data, hindering their generalization across
diverse clinical scenarios. To address these challenges, we introduce DentVFM,
the first family of vision foundation models (VFMs) designed for dentistry.
DentVFM generates task-agnostic visual representations for a wide range of
dental applications and uses self-supervised learning on DentVista, a large
curated dental imaging dataset with approximately 1.6 million multi-modal
radiographic images from various medical centers. DentVFM includes 2D and 3D
variants based on the Vision Transformer (ViT) architecture. To address gaps in
dental intelligence assessment and benchmarks, we introduce DentBench, a
comprehensive benchmark covering eight dental subspecialties, more diseases,
imaging modalities, and a wide geographical distribution. DentVFM shows
impressive generalist intelligence, demonstrating robust generalization to
diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker
identification, and anatomical landmark detection and segmentation.
Experimental results indicate DentVFM significantly outperforms supervised,
self-supervised, and weakly supervised baselines, offering superior
generalization, label efficiency, and scalability. Additionally, DentVFM
enables cross-modality diagnostics, providing more reliable results than
experienced dentists in situations where conventional imaging is unavailable.
DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and
label-efficient model to improve intelligent dental healthcare and address
critical gaps in global oral healthcare.

</details>


### [44] [Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval](https://arxiv.org/abs/2510.14535)
*Keima Abe,Hayato Muraki,Shuhei Tomoshige,Kenichi Oishi,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 提出了一种名为PL-SE-ADA的域适应框架，用于医学图像的域协调和可解释表示学习，在图像重建、疾病分类和域识别方面取得了与先前方法相当或更好的性能，并提高了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像域适应方法缺乏可解释性，这在医疗应用中是一个关键问题。

Method: 提出PL-SE-ADA框架，包含两个编码器（用于提取域不变和域特异性特征）、一个解码器和一个域预测器。通过对抗训练和图像重建来确保域协调和信息保留。

Result: PL-SE-ADA在图像重建、疾病分类和域识别方面达到了与先前方法相当或更好的性能，并能可视化域无关和域特异性特征，提高了可解释性。

Conclusion: PL-SE-ADA是一个通用的域协调和可解释表示学习框架，能够有效解决医学图像的域偏移问题，并提供高度可解释性。

Abstract: Medical images like MR scans often show domain shifts across imaging sites
due to scanner and protocol differences, which degrade machine learning
performance in tasks such as disease classification. Domain harmonization is
thus a critical research focus. Recent approaches encode brain images
$\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, then
disentangle it into $\boldsymbol{z_u}$ (domain-invariant) and
$\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these
methods often lack interpretability$-$an essential requirement in medical
applications$-$leaving practical issues unresolved. We propose
Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a
general framework for domain harmonization and interpretable representation
learning that preserves disease-relevant information in brain MR images.
PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract
$\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image
$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the
encoder and domain predictor, the model learns to reconstruct the input image
$\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and
$\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared
to prior methods, PL-SE-ADA achieves equal or better performance in image
reconstruction, disease classification, and domain recognition. It also enables
visualization of both domain-independent brain features and domain-specific
components, offering high interpretability across the entire framework.

</details>


### [45] [QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models](https://arxiv.org/abs/2510.14836)
*Yixuan Li,Yuhui Chen,Mingcai Zhou,Haoran Li*

Main category: cs.CV

TL;DR: QDepth-VLA 通过增加深度预测任务来增强 VLA 模型，以改进空间理解和操作任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有 VLA 模型在理解和推理 3D 结构方面能力不足，这对于精细操作至关重要。

Method: 提出 QDepth-VLA 框架，包含一个深度预测专家，用于预测 VQ-VAE 编码器得到的深度图的量化潜在令牌，从而学习深度感知表示。

Result: 在仿真基准和真实世界任务的实验结果表明，QDepth-VLA 具有强大的空间推理能力和具有竞争力的操作任务性能。

Conclusion: QDepth-VLA 框架通过增强深度感知能力，有效提升了 VLA 模型在空间推理和精细操作任务中的表现。

Abstract: Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)
models to accomplish fine-grained manipulation tasks. However, existing
approaches often lack the ability to understand and reason over the essential
3D structures necessary for precise control. To address this limitation, we
propose QDepth-VLA, a general framework that augments VLA models with an
auxiliary depth prediction task. A dedicated depth expert is designed to
predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,
enabling the model to learn depth-aware representations that capture critical
geometric cues. Experimental results on the simulation benchmarks and
real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning
and competitive performance on manipulation tasks.

</details>


### [46] [Exploring Image Representation with Decoupled Classical Visual Descriptors](https://arxiv.org/abs/2510.14536)
*Chenyuan Qu,Hao Chen,Jianbo Jiao*

Main category: cs.CV

TL;DR: VisualSplit框架通过将图像分解为经典视觉描述符来弥合深度学习的可解释性与经典方法的直观性之间的差距，并在图像生成和编辑等任务中展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 弥合深度学习模型内部表示的不透明性与经典视觉描述符（如边缘、颜色、强度分布）的直观可理解性之间的差距，探索是否能将经典视觉线索应用于现代学习方法。

Method: 提出VisualSplit框架，将图像显式分解为解耦的经典描述符，并将每个描述符视为独立的但互补的视觉知识组成部分。通过以重建为导向的预训练方案来学习捕捉每个视觉描述符的本质，同时保持其可解释性。

Result: VisualSplit框架在图像生成和编辑等任务中展示了其有效性，超越了传统的分类和分割任务，表明这种新的学习方法在视觉理解方面是有效的。

Conclusion: VisualSplit框架通过显式分解视觉属性，为各种高级视觉任务提供了有效的属性控制，证明了这种将经典视觉线索与现代学习方法相结合的新兴方法在视觉理解领域的潜力。

Abstract: Exploring and understanding efficient image representations is a
long-standing challenge in computer vision. While deep learning has achieved
remarkable progress across image understanding tasks, its internal
representations are often opaque, making it difficult to interpret how visual
information is processed. In contrast, classical visual descriptors (e.g. edge,
colour, and intensity distribution) have long been fundamental to image
analysis and remain intuitively understandable to humans. Motivated by this
gap, we ask a central question: Can modern learning benefit from these
classical cues? In this paper, we answer it with VisualSplit, a framework that
explicitly decomposes images into decoupled classical descriptors, treating
each as an independent but complementary component of visual knowledge. Through
a reconstruction-driven pre-training scheme, VisualSplit learns to capture the
essence of each visual descriptor while preserving their interpretability. By
explicitly decomposing visual attributes, our method inherently facilitates
effective attribute control in various advanced visual tasks, including image
generation and editing, extending beyond conventional classification and
segmentation, suggesting the effectiveness of this new learning approach for
visual understanding. Project page: https://chenyuanqu.com/VisualSplit/.

</details>


### [47] [Exploring Cross-Modal Flows for Few-Shot Learning](https://arxiv.org/abs/2510.14543)
*Ziqi Jiang,Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: 现有的参数高效微调（PEFT）方法在处理复杂跨模态任务时，由于只进行单步调整，对于高度交织的特征对齐能力不足。本文提出了首个模型无关的多步调整方法——流匹配对齐（FMA），通过学习跨模态速度场，并结合固定耦合策略、噪声增强策略和提前停止求解器，实现了更精确、更鲁棒的特征对齐，并在多项基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法仅能进行单步调整，对于特征高度交织的复杂跨模态数据集，其对齐能力不足。

Method: 提出了一种名为流匹配对齐（FMA）的模型无关多步调整方法，通过学习跨模态速度场。具体包括：1. 采用固定耦合策略确保训练时的类别对应。2. 提出噪声增强策略缓解数据稀疏问题。3. 设计提前停止求解器以提高效率和准确性。

Result: FMA 相比于单步 PEFT 方法，具有多步校正能力，能够实现更精确、更鲁棒的对齐。在多个基准测试和骨干网络上，FMA 持续带来显著的性能提升，尤其在挑战性数据集上表现突出。

Conclusion: FMA 作为首个多步调整方法，通过学习跨模态速度场，有效解决了现有 PEFT 方法在复杂跨模态对齐任务中的局限性，并在实验中证明了其优越的性能和广泛的适用性。

Abstract: Aligning features from different modalities, is one of the most fundamental
challenges for cross-modal tasks. Although pre-trained vision-language models
can achieve a general alignment between image and text, they often require
parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT
methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively
fine-tune a subset of parameters, which can slightly adjust either visual or
textual features, and avoid overfitting. In this paper, we are the first to
highlight that all existing PEFT methods perform one-step adjustment. It is
insufficient for complex (or difficult) datasets, where features of different
modalities are highly entangled. To this end, we propose the first
model-agnostic multi-step adjustment approach by learning a cross-modal
velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the
correspondence between categories during training, we first utilize a fixed
coupling strategy. Then, we propose a noise augmentation strategy to alleviate
the data scarcity issue. Finally, we design an early-stopping solver, which
terminates the transformation process earlier, improving both efficiency and
accuracy. Compared with one-step PEFT methods, FMA has the multi-step
rectification ability to achieve more precise and robust alignment. Extensive
results have demonstrated that FMA can consistently yield significant
performance gains across various benchmarks and backbones, particularly on
challenging datasets.

</details>


### [48] [Consistent text-to-image generation via scene de-contextualization](https://arxiv.org/abs/2510.14553)
*Song Tang,Peihao Gong,Kunyu Li,Kai Guo,Boyu Wang,Mao Ye,Jianwei Zhang,Xiatian Zhu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为SDeC的训练无关方法，通过编辑提示嵌入来解决文本到图像生成中的身份（ID）偏移问题，该问题源于主体与场景的固有相关性。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中，由于主体与场景固有相关性（场景概念化）导致的身份（ID）偏移问题，该问题在先前的方法中通常假设需要预先知道所有目标场景。

Method: 提出了一种名为SDeC（Scene De-Contextualization）的新颖、高效、训练无关的提示嵌入编辑方法，通过奇异值分解（SVD）定向稳定性和自适应地重新加权特征值来识别并抑制潜在的场景-ID相关性，从而实现T2I内置场景概念化的逆过程。

Result: 实验证明，SDeC在保持场景多样性的同时，显著提高了身份保持能力。

Conclusion: SDeC是一种灵活且通用的解决方案，适用于不需要预先了解所有目标场景的真实世界应用，能够有效解决文本到图像生成中的身份偏移问题。

Abstract: Consistent text-to-image (T2I) generation seeks to produce
identity-preserving images of the same subject across diverse scenes, yet it
often fails due to a phenomenon called identity (ID) shift. Previous methods
have tackled this issue, but typically rely on the unrealistic assumption of
knowing all target scenes in advance. This paper reveals that a key source of
ID shift is the native correlation between subject and scene context, called
scene contextualization, which arises naturally as T2I models fit the training
distribution of vast natural images. We formally prove the near-universality of
this scene-ID correlation and derive theoretical bounds on its strength. On
this basis, we propose a novel, efficient, training-free prompt embedding
editing approach, called Scene De-Contextualization (SDeC), that imposes an
inversion process of T2I's built-in scene contextualization. Specifically, it
identifies and suppresses the latent scene-ID correlation within the ID
prompt's embedding by quantifying the SVD directional stability to adaptively
re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene
use (one scene per prompt) without requiring prior access to all target scenes.
This makes it a highly flexible and general solution well-suited to real-world
applications where such prior knowledge is often unavailable or varies over
time. Experiments demonstrate that SDeC significantly enhances identity
preservation while maintaining scene diversity.

</details>


### [49] [Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video](https://arxiv.org/abs/2510.14560)
*Yulin Zhang,Cheng Shi,Yang Wang,Sibei Yang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为ESTP-Bench的框架和ESTP-F1指标，用于评估和解决AI在实时视频流中进行主动问答的能力。


<details>
  <summary>Details</summary>
Motivation: 为了实现一个能够理解、预测并主动应对人类环境中事件的AI，本研究聚焦于一项创新任务：AI需要根据输入的个人视角实时视频流，在恰当的时机主动回答各种不断变化的问题，并保持感知和推理的同步性。

Method: 研究中提出的技术流程包括：1. 数据引擎；2. 多阶段训练策略；3. 主动动态压缩技术。ESTP-Bench是一个用于评估主动连贯性、即时响应性和同步效率的新框架，ESTP-F1是其评估指标。

Result: 提出的模型在多个在线和离线基准测试中，在处理主动连贯性、即时响应性和同步效率这三个关键属性方面，表现优于多个基线模型。

Conclusion: 研究提出的ESTP-Bench框架、ESTP-F1指标以及配套的技术流程，能够有效地解决AI在实时视频流中进行主动问答所面临的挑战，并显著提升了AI在该任务上的表现。

Abstract: Envision an AI capable of functioning in human-like settings, moving beyond
mere observation to actively understand, anticipate, and proactively respond to
unfolding events. Towards this vision, we focus on the innovative task where,
given ego-streaming video input, an assistant proactively answers diverse,
evolving questions at the opportune moment, while maintaining synchronized
perception and reasoning. This task embodies three key properties: (1)
Proactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized
Efficiency. To evaluate and address these properties, we first introduce
ESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a
novel framework designed for their rigorous assessment. Secondly, we propose a
comprehensive technical pipeline to enable models to tackle this challenging
task. This pipeline comprises: (1) a data engine, (2) a multi-stage training
strategy, and (3) a proactive dynamic compression technique. Our proposed model
effectively addresses these critical properties while outperforming multiple
baselines across diverse online and offline benchmarks. Project
Page:https://zhangyl4.github.io/publications/eyes-wide-open/

</details>


### [50] [BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU](https://arxiv.org/abs/2510.14564)
*Junyi Wu,Jiaming Xu,Jinhao Li,Yongkang Zhou,Jiayi Pan,Xingyang Li,Guohao Dai*

Main category: cs.CV

TL;DR: BalanceGS通过算法-系统协同设计，优化了3DGS的训练效率，实现了1.44倍的提速，同时保持了可忽略的质量损失。


<details>
  <summary>Details</summary>
Motivation: 传统的3DGS训练流程存在高斯密度分配不均、计算负载不平衡以及内存访问碎片化等效率问题。

Method: 1. 算法层面：提出启发式工作负载敏感的高斯密度控制，平衡点分布，去除80%冗余高斯，填充稀疏区域。 2. 系统层面：提出基于相似性的高斯采样与合并，替换静态线程-像素映射，实现自适应工作负载分配。 3. 映射层面：提出基于重排序的内存访问映射策略，重构RGB存储，实现共享内存的批加载。

Result: BalanceGS在NVIDIA A100 GPU上实现了1.44倍的训练加速，且模型质量损失可忽略不计。

Conclusion: BalanceGS通过算法和系统层面的协同优化，显著提高了3DGS的训练效率，是解决传统3DGS训练低效问题的有效方法。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction
technique. The traditional 3DGS training pipeline follows three sequential
steps: Gaussian densification, Gaussian projection, and color splatting.
Despite its promising reconstruction quality, this conventional approach
suffers from three critical inefficiencies: (1) Skewed density allocation
during Gaussian densification, (2) Imbalanced computation workload during
Gaussian projection and (3) Fragmented memory access during color splatting.
  To tackle the above challenges, we introduce BalanceGS, the algorithm-system
co-design for efficient training in 3DGS. (1) At the algorithm level, we
propose heuristic workload-sensitive Gaussian density control to automatically
balance point distributions - removing 80% redundant Gaussians in dense regions
while filling gaps in sparse areas. (2) At the system level, we propose
Similarity-based Gaussian sampling and merging, which replaces the static
one-to-one thread-pixel mapping with adaptive workload distribution - threads
now dynamically process variable numbers of Gaussians based on local cluster
density. (3) At the mapping level, we propose reordering-based memory access
mapping strategy that restructures RGB storage and enables batch loading in
shared memory.
  Extensive experiments demonstrate that compared with 3DGS, our approach
achieves a 1.44$\times$ training speedup on a NVIDIA A100 GPU with negligible
quality degradation.

</details>


### [51] [CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification](https://arxiv.org/abs/2510.14576)
*Dongwook Lee,Sol Han,Jinwhan Kim*

Main category: cs.CV

TL;DR: CALM-Net是一个利用曲率信息的多分支神经网络，用于激光雷达点云的车辆重识别，在nuScenes数据集上取得了1.97%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 解决从三维点云中学习区分性特征以识别车辆的挑战。

Method: 采用多分支架构，结合了边缘卷积、点注意力以及表征点云局部表面变化的曲率嵌入。

Result: 在nuScenes数据集上，CALM-Net的平均识别准确率比最强的基线模型提高了约1.97%。

Conclusion: 结合曲率信息和多分支特征学习对基于激光雷达点云的车辆重识别任务是有效的。

Abstract: This paper presents CALM-Net, a curvature-aware LiDAR point cloud-based
multi-branch neural network for vehicle re-identification. The proposed model
addresses the challenge of learning discriminative and complementary features
from three-dimensional point clouds to distinguish between vehicles. CALM-Net
employs a multi-branch architecture that integrates edge convolution, point
attention, and a curvature embedding that characterizes local surface variation
in point clouds. By combining these mechanisms, the model learns richer
geometric and contextual features that are well suited for the
re-identification task. Experimental evaluation on the large-scale nuScenes
dataset demonstrates that CALM-Net achieves a mean re-identification accuracy
improvement of approximately 1.97\% points compared with the strongest baseline
in our study. The results confirms the effectiveness of incorporating curvature
information into deep learning architectures and highlight the benefit of
multi-branch feature learning for LiDAR point cloud-based vehicle
re-identification.

</details>


### [52] [Talking Points: Describing and Localizing Pixels](https://arxiv.org/abs/2510.14583)
*Matan Rusanovsky,Shimon Malnick,Shai Avidan*

Main category: cs.CV

TL;DR: 该研究提出了一种新的像素级定位框架，用于通过自然语言理解图像中的关键点。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在像素级关键点理解方面存在局限性，该研究旨在解决此问题。

Method: 提出一个包含“点描述符”和“点定位器”两部分的框架，并创建了一个名为 LlamaPointInPart 的新数据集。

Result: 在 LlamaPointInPart 数据集上取得了优于基线模型的性能，并建立了一个新的评估协议。

Conclusion: 该框架能够实现像素级关键点理解和语言引导的精确定位，并有望应用于未来的相关领域。

Abstract: Vision-language models have achieved remarkable success in cross-modal
understanding. Yet, these models remain limited to object-level or region-level
grounding, lacking the capability for pixel-precise keypoint comprehension
through natural language. We introduce a novel framework for pixel level
grounding. The framework consists of two complementary components: a Point
Descriptor that generates rich, contextual descriptions of individual
keypoints, and a Point Localizer that regresses precise pixel coordinates from
these descriptions. Unlike prior work that relies on templated prompts or
keypoint names, our approach produces free-form, coarse-to-fine descriptions
that situate keypoints within their visual context. Since there is no available
dataset to train such a system, we introduce LlamaPointInPart, a carefully
curated dataset of 20K+ image-keypoint-description triplets synthesized from
multiple vision-language models, capturing multi-scale information from
scene-level context to visual features around the keypoint. For cross-category
generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the
frozen Point Localizer as a reward model to produce descriptions that maximize
localization accuracy. To evaluate our results we establish a new evaluation
protocol. Instead of comparing the text description produced by our method to
the ground truth, we use the localizer to determine how close is the predicted
point generated to the ground truth point. Experiments demonstrate superior
performance compared to baseline models on LlamaPointInPart.The bidirectional
nature of our framework should enable future applications in both
keypoint-guided image understanding and language-guided precise localization.
Our code and dataset are publicly available at
https://github.com/matanr/Talking_Points.

</details>


### [53] [STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding](https://arxiv.org/abs/2510.14588)
*Zhifei Chen,Tianshuo Xu,Leyi Wu,Luozhou Wang,Dongyu Yan,Zihan You,Wenting Luo,Guo Zhang,Yingcong Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种名为STANCE的视频生成框架，通过引入实例线索和密集RoPE来解决现有视频生成中物体运动和交互不连贯的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成方法在保持物体运动和交互的连贯性方面存在困难，主要瓶颈在于：1. 2D运动提示在编码后有效信息过少，难以提供有效引导；2. 外观和运动优化在单一模块进行，容易导致纹理信息覆盖时间连贯性。

Method: STANCE框架包含两个主要组件：1. 实例线索（Instance Cues）：一种像素对齐的控制信号，通过平均实例流并结合单目深度信息，将稀疏的用户编辑提示转化为密集的2.5D运动场，解决了2D箭头输入的深度歧义问题。2. 密集RoPE（Dense RoPE）：通过将空间可寻址的旋转嵌入附加到少量运动标记上，保留了运动线索在标记空间中的重要性。结合RGB和辅助地图（分割或深度）的联合预测，STANCE可以稳定优化过程，提高时间连贯性，无需每帧轨迹脚本。

Result: STANCE框架能够提高视频生成的时间连贯性，并稳定优化过程。

Conclusion: STANCE通过实例线索和密集RoPE有效地解决了现有视频生成方法在物体运动和交互连贯性方面的问题。

Abstract: Video generation has recently made striking visual progress, but maintaining
coherent object motion and interactions remains difficult. We trace two
practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)
often collapse to too few effective tokens after encoding, weakening guidance;
and (ii) optimizing for appearance and motion in a single head can favor
texture over temporal consistency. We present STANCE, an image-to-video
framework that addresses both issues with two simple components. First, we
introduce Instance Cues -- a pixel-aligned control signal that turns sparse,
user-editable hints into a dense 2.5D (camera-relative) motion field by
averaging per-instance flow and augmenting with monocular depth over the
instance mask. This reduces depth ambiguity compared to 2D arrow inputs while
remaining easy to use. Second, we preserve the salience of these cues in token
space with Dense RoPE, which tags a small set of motion tokens (anchored on the
first frame) with spatial-addressable rotary embeddings. Paired with joint RGB
\(+\) auxiliary-map prediction (segmentation or depth), our model anchors
structure while RGB handles appearance, stabilizing optimization and improving
temporal coherence without requiring per-frame trajectory scripts.

</details>


### [54] [Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers](https://arxiv.org/abs/2510.14594)
*Hugo Markoff,Jevgenijs Galaktionovs*

Main category: cs.CV

TL;DR: 提出一个分层再分类系统，结合了SpeciesNet和CLIP，以提高动物物种识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的动物分类模型（如SpeciesNet）虽然能识别数千种，但由于保守的汇总策略，常将动物归类于高阶分类单元而非具体物种。本研究旨在改进这一点。

Method: 开发了一个包含五个阶段（高置信度接受、鸟类覆盖、质心构建、三元损失度量学习、自适应余弦距离评分）的分层再分类系统，结合了SpeciesNet EfficientNetV2-M的预测、CLIP嵌入和度量学习。

Result: 在LILA BC沙漠狮子保护数据集的一个片段（4018张图像，15031个检测）上进行了评估。该系统从“空白”和“动物”标签中恢复了761个鸟类检测，并将456个被标记为动物、哺乳动物或空白的检测重新分类，准确率达到96.5%，成功将64.9%的检测识别到物种级别。

Conclusion: 所提出的分层再分类系统能有效提升动物物种识别的准确性和粒度，尤其是在处理模糊或高阶分类标签时。

Abstract: State-of-the-art animal classification models like SpeciesNet provide
predictions across thousands of species but use conservative rollup strategies,
resulting in many animals labeled at high taxonomic levels rather than species.
We present a hierarchical re-classification system for the Animal Detect
platform that combines SpeciesNet EfficientNetV2-M predictions with CLIP
embeddings and metric learning to refine high-level taxonomic labels toward
species-level identification. Our five-stage pipeline (high-confidence
acceptance, bird override, centroid building, triplet-loss metric learning, and
adaptive cosine-distance scoring) is evaluated on a segment of the LILA BC
Desert Lion Conservation dataset (4,018 images, 15,031 detections). After
recovering 761 bird detections from "blank" and "animal" labels, we re-classify
456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving
species-level identification for 64.9 percent

</details>


### [55] [Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering](https://arxiv.org/abs/2510.14596)
*Hugo Markoff,Jevgenijs Galaktionovs*

Main category: cs.CV

TL;DR: 该研究评估了使用自监督视觉转换器进行零样本学习，以组织包含现有分类器中缺失物种的未标记野生动植物图像。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱产生数百万张野生动植物图像，但许多数据集包含现有分类器中缺失的物种，这促使人们需要新的方法来组织这些图像。

Method: 研究人员评估了无监督聚类方法（DBSCAN、GMM）在三种架构（CLIP、DINOv2、MegaDescriptor）上的表现，并结合了降维技术（PCA、UMAP）。他们还演示了通过 t-SNE 投影实现的连续一维相似性排序。

Result: 在包含 5 个物种的测试集上，使用 UMAP 和 GMM 的 DINOv2 架构达到了 88.6% 的准确率（宏 F1 分数 = 0.874）。对于哺乳动物和鸟类，一维排序达到了 88.2% 的相干性；对于鱼类，则达到了 95.2% 的相干性，涉及 1500 张图像。

Conclusion: 基于这些结果，研究人员在实际应用中采用了连续相似性排序，从而能够快速进行探索性分析，并加速生物多样性监测的手动注释工作流程。

Abstract: Camera traps generate millions of wildlife images, yet many datasets contain
species that are absent from existing classifiers. This work evaluates
zero-shot approaches for organizing unlabeled wildlife imagery using
self-supervised vision transformers, developed and tested within the Animal
Detect platform for camera trap analysis. We compare unsupervised clustering
methods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor)
combined with dimensionality reduction techniques (PCA, UMAP), and we
demonstrate continuous 1D similarity ordering via t-SNE projection. On a
5-species test set with ground truth labels used only for evaluation, DINOv2
with UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D
sorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent
for fish across 1,500 images. Based on these findings, we deployed continuous
similarity ordering in production, enabling rapid exploratory analysis and
accelerating manual annotation workflows for biodiversity monitoring.

</details>


### [56] [Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering](https://arxiv.org/abs/2510.14605)
*Yuyang Hong,Jiaqi Gu,Qi Yang,Lubin Fan,Yue Wu,Ying Wang,Kun Ding,Shiming Xiang,Jieping Ye*

Main category: cs.CV

TL;DR: Wiki-PRF通过结合视觉理解和外部知识检索来改进知识库视觉问答，解决了现有方法的局限性，在E-VQA和InfoSeek数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）方法在知识库视觉问答（KB-VQA）任务中，在多模态查询质量和检索结果相关性方面存在不足。

Method: 提出了一种新颖的三阶段方法（Wiki-PRF），包括处理、检索和过滤。处理阶段使用视觉工具提取精确的多模态信息，检索阶段整合视觉和文本特征进行多模态知识检索，过滤阶段进行相关性过滤和结果浓缩。此外，通过强化学习训练视觉语言模型，以答案准确性和格式一致性作为奖励信号。

Result: 在E-VQA和InfoSeek基准数据集上，Wiki-PRF在答案质量方面取得了显著的提高（分别为36.0和42.8），达到了最先进的性能。

Conclusion: Wiki-PRF通过其创新的三阶段方法和强化学习训练，有效解决了KB-VQA中的多模态查询和检索相关性问题，显著提升了答案质量，并在相关数据集上取得了领先的成果。

Abstract: Knowledge-based visual question answering (KB-VQA) requires visual language
models (VLMs) to integrate visual understanding with external knowledge
retrieval. Although retrieval-augmented generation (RAG) achieves significant
advances in this task by combining knowledge-base querying, it still struggles
with the quality of multimodal queries and the relevance of retrieved results.
To overcome these challenges, we propose a novel three-stage method, termed
Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing
stage dynamically invokes visual tools to extract precise multimodal
information for retrieval. The retrieval stage integrates visual and text
features to achieve multimodal knowledge retrieval. The filtering stage
performs relevance filtering and concentration on retrieval results. To this
end, we introduce a visual language model trained with answer accuracy and
format consistency as reward signals via a reinforcement learning manner. This
enhances the model's reasoning, tool invocation for accurate queries, and
filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and
InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,
achieving state-of-the-art performance. Code is available at
https://github.com/cqu-student/Wiki-PRF

</details>


### [57] [Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding](https://arxiv.org/abs/2510.14617)
*Ning Ding,Keisuke Fujii,Toru Tamaki*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 Shot2Tactic-Caption 的新框架，用于羽毛球视频的语义和时间多尺度视频字幕生成，能够生成描述单个动作的“镜头级”字幕和捕捉动作随时间展开的“战术级”字幕。


<details>
  <summary>Details</summary>
Motivation: 传统的战术理解方法仅关注个体动作，而忽略了战术的动态执行过程。本研究旨在解决这一问题，通过生成能够捕捉战术执行过程的字幕来增强对羽毛球战术的理解。

Method: Shot2Tactic-Caption 框架采用双分支设计，包括视觉编码器、时空 Transformer 编码器和 Transformer 解码器，分别生成镜头级和战术级字幕。此外，还引入了战术单元检测器来识别战术单元、类型和状态（如中断、恢复），并采用基于镜头的提示引导机制来增强战术字幕的生成能力。

Result: 实验结果表明，Shot2Tactic-Caption 框架在生成镜头级和战术级字幕方面均有效。消融研究表明，基于 ResNet50 的时空编码器优于其他变体，并且基于镜头的提示构建能够生成更连贯、更准确的战术字幕。

Conclusion: Shot2Tactic-Caption 框架成功实现了羽毛球视频的语义和时间多尺度视频字幕生成，能够生成描述单个动作和捕获战术执行过程的字幕，为羽毛球战术理解提供了新的途径。

Abstract: Tactical understanding in badminton involves interpreting not only individual
actions but also how tactics are dynamically executed over time. In this paper,
we propose \textbf{Shot2Tactic-Caption}, a novel framework for semantic and
temporal multi-scale video captioning in badminton, capable of generating
shot-level captions that describe individual actions and tactic-level captions
that capture how these actions unfold over time within a tactical execution. We
also introduce the Shot2Tactic-Caption Dataset, the first badminton captioning
dataset containing 5,494 shot captions and 544 tactic captions.
Shot2Tactic-Caption adopts a dual-branch design, with both branches including a
visual encoder, a spatio-temporal Transformer encoder, and a Transformer-based
decoder to generate shot and tactic captions. To support tactic captioning, we
additionally introduce a Tactic Unit Detector that identifies valid tactic
units, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic
captioning, we further incorporate a shot-wise prompt-guided mechanism, where
the predicted tactic type and state are embedded as prompts and injected into
the decoder via cross-attention. The shot-wise prompt-guided mechanism enables
our system not only to describe successfully executed tactics but also to
capture tactical executions that are temporarily interrupted and later resumed.
Experimental results demonstrate the effectiveness of our framework in
generating both shot and tactic captions. Ablation studies show that the
ResNet50-based spatio-temporal encoder outperforms other variants, and that
shot-wise prompt structuring leads to more coherent and accurate tactic
captioning.

</details>


### [58] [Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference](https://arxiv.org/abs/2510.14624)
*Natan Bagrov,Eugene Khvedchenia,Borys Tymchenko,Shay Aharon,Lior Kadoch,Tomer Keren,Ofri Masad,Yonatan Geifman,Ran Zilberstein,Tuomas Rintamaki,Matthieu Le,Andrew Tao*

Main category: cs.CV

TL;DR: EVS通过移除视频中内容未变化的区域来减少处理视频所需的计算量，从而提高视频语言模型的效率和处理长视频的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视频语言模型在处理长视频时存在计算量大、延迟高、上下文受限等问题，主要是因为视频帧序列处理成本是二次方的，并且超出了模型的token预算。

Method: 提出了一种名为EVS（Efficient Video Sampling）的即插即用方法，通过识别并移除视频中连续帧之间保持不变的图像块（空间区域），来减少token的冗余。该方法保留了位置信息，无需改变模型架构或重新训练。

Result: EVS在显著减少token数量的同时，保持了语义的完整性。在推理时，EVS可以将大语言模型（LLM）的首次响应时间（TTFT）最多缩短4倍，同时准确率损失很小。与带有随机剪枝率的预训练阶段相结合，EVS能够生成对不同压缩级别具有鲁棒性、并且在激进剪枝下仍能保持全部性能的模型。

Conclusion: EVS能够有效地提高视频语言模型的效率和准确性之间的权衡，使得在不牺牲质量的情况下实现可扩展的视频语言理解。

Abstract: Vision-language models (VLMs) have recently expanded from static image
understanding to video reasoning, but their scalability is fundamentally
limited by the quadratic cost of processing dense frame sequences. Long videos
often exceed the token budget of modern language models, leading to severe
context limitations and latency issues. We introduce Efficient Video Sampling
(EVS), a simple, plug-and-play method for reducing token redundancy in videos
by identifying and pruning temporally static patches -- spatial regions that
remain unchanged across consecutive frames. EVS preserves positional identity,
requires no architectural changes or retraining. We show that EVS substantially
reduces token count while maintaining semantic fidelity, enabling faster
inference and longer input sequences. Applied at inference time, EVS reduces
large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal
accuracy loss. When combined with an uptraining phase using stochastic pruning
rates, EVS yields models that are robust to varying compression levels and
retain full performance under aggressive pruning. Extensive experiments
demonstrate that EVS consistently improves efficiency-accuracy trade-offs,
unlocking scalable video-language understanding without sacrificing quality.

</details>


### [59] [Adapting Self-Supervised Representations as a Latent Space for Efficient Generation](https://arxiv.org/abs/2510.14630)
*Ming Gui,Johannes Schusterbauer,Timy Phan,Felix Krause,Josh Susskind,Miguel Angel Bautista,Björn Ommer*

Main category: cs.CV

TL;DR: RepTok是一个生成模型框架，它使用来自自监督视觉变换器的单个连续潜在令牌来表示图像，在ImageNet和MS-COCO上实现了具有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有生成模型中空间冗余和高昂训练成本的问题，并探索利用自监督学习（SSL）的强大表征能力来构建高效的生成模型。

Method: RepTok框架首先利用预训练的SSL编码器，然后仅对语义令牌嵌入进行微调，并结合标准流匹配目标联合训练生成解码器。通过添加余弦相似性损失来正则化自适应令牌，以保持SSL空间的良好几何特性。

Result: RepTok在类条件ImageNet生成任务上取得了有竞争力的结果，并且能够自然地扩展到文本到图像合成，在MS-COCO数据集上实现了具有竞争力的零样本性能，同时显著降低了训练成本。

Conclusion: 经过微调的SSL表征可以作为紧凑且有效的潜在空间，用于高效的生成建模，RepTok的简单性和效率证明了这一潜力。

Abstract: We introduce Representation Tokenizer (RepTok), a generative modeling
framework that represents an image using a single continuous latent token
obtained from self-supervised vision transformers. Building on a pre-trained
SSL encoder, we fine-tune only the semantic token embedding and pair it with a
generative decoder trained jointly using a standard flow matching objective.
This adaptation enriches the token with low-level, reconstruction-relevant
details, enabling faithful image reconstruction. To preserve the favorable
geometry of the original SSL space, we add a cosine-similarity loss that
regularizes the adapted token, ensuring the latent space remains smooth and
suitable for generation. Our single-token formulation resolves spatial
redundancies of 2D latent spaces and significantly reduces training costs.
Despite its simplicity and efficiency, RepTok achieves competitive results on
class-conditional ImageNet generation and naturally extends to text-to-image
synthesis, reaching competitive zero-shot performance on MS-COCO under
extremely limited training budgets. Our findings highlight the potential of
fine-tuned SSL representations as compact and effective latent spaces for
efficient generative modeling.

</details>


### [60] [SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation](https://arxiv.org/abs/2510.14634)
*Jihyun Yu,Yoojin Oh,Wonho Bae,Mingyu Kim,Junhyug Noh*

Main category: cs.CV

TL;DR: SteeringTTA是一种仅在推理时使用的框架，通过将Feynman-Kac引导应用于扩散模型，并使用伪标签驱动的奖励来优化分类任务，从而在不更新模型或不使用源数据的情况下，在ImageNet-C上实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的测试时自适应（TTA）方法在处理分布偏移引起的性能下降时，虽然能提高分类鲁棒性，但依赖梯度引导，限制了探索和跨失真类型的泛化能力。

Method: SteeringTTA框架通过维护多个粒子轨迹，并结合累积的top-K概率和熵调度来平衡探索和置信度，从而引导基于扩散的输入自适应。

Result: 在ImageNet-C数据集上，SteeringTTA在不进行任何模型更新或使用源数据的情况下，始终优于基线方法。

Conclusion: SteeringTTA通过一种新颖的仅在推理时使用的自适应框架，有效地解决了现有TTA方法的局限性，提高了在图像识别任务中的鲁棒性和泛化能力。

Abstract: Test-time adaptation (TTA) aims to correct performance degradation of deep
models under distribution shifts by updating models or inputs using unlabeled
test data. Input-only diffusion-based TTA methods improve robustness for
classification to corruptions but rely on gradient guidance, limiting
exploration and generalization across distortion types. We propose SteeringTTA,
an inference-only framework that adapts Feynman-Kac steering to guide
diffusion-based input adaptation for classification with rewards driven by
pseudo-label. SteeringTTA maintains multiple particle trajectories, steered by
a combination of cumulative top-K probabilities and an entropy schedule, to
balance exploration and confidence. On ImageNet-C, SteeringTTA consistently
outperforms the baseline without any model updates or source data.

</details>


### [61] [In-Context Learning with Unpaired Clips for Instruction-based Video Editing](https://arxiv.org/abs/2510.14648)
*Xinyao Liao,Xianfang Zeng,Ziye Song,Zhoujie Fu,Gang Yu,Guosheng Lin*

Main category: cs.CV

TL;DR: 利用无配对视频片段进行上下文学习的低成本预训练策略，可以使基础视频生成模型具备指令驱动的视频编辑能力，并通过少量高质量配对数据进行微调，从而在指令对齐和视觉保真度方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑技术在视频领域应用不足，主要是因为构建大规模配对视频编辑数据集的成本高昂且复杂。

Method: 提出一种低成本预训练策略，利用无配对视频片段进行上下文学习，使基础视频生成模型具备添加、替换或删除等编辑能力，然后用少量高质量配对编辑数据进行高效微调。具体做法是在大约1M个真实视频片段上进行预训练，学习基本编辑概念，然后在少于150k个精选编辑对上进行微调。

Result: 与现有方法相比，该方法在指令对齐和视觉保真度方面均有提升，指令遵循能力提高12%，编辑质量提高15%。

Conclusion: 所提出的低成本预训练策略有效解决了视频编辑数据集的构建难题，使基础视频生成模型能够进行指令驱动的视频编辑，并在性能上超越现有方法。

Abstract: Despite the rapid progress of instruction-based image editing, its extension
to video remains underexplored, primarily due to the prohibitive cost and
complexity of constructing large-scale paired video editing datasets. To
address this challenge, we introduce a low-cost pretraining strategy for
instruction-based video editing that leverages in-context learning from
unpaired video clips. We show that pretraining a foundation video generation
model with this strategy endows it with general editing capabilities, such as
adding, replacing, or deleting operations, according to input editing
instructions. The pretrained model can then be efficiently refined with a small
amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our
framework first pretrains on approximately 1M real video clips to learn basic
editing concepts, and subsequently fine-tunes on fewer than 150k curated
editing pairs to extend more editing tasks and improve the editing quality.
Comparative experiments show that our method surpasses existing
instruction-based video editing approaches in both instruction alignment and
visual fidelity, achieving a 12\% improvement in editing instruction following
and a 15\% improvement in editing quality.

</details>


### [62] [Decorrelation Speeds Up Vision Transformers](https://arxiv.org/abs/2510.14657)
*Kieran Carrigg,Rob van Gastel,Melda Yeghaian,Sander Dalm,Faysal Boughorbel,Marcel van Gerven*

Main category: cs.CV

TL;DR: 通过在MAE预训练中集成Decorrelated Backpropagation (DBP)，可以减少训练时间和能源消耗，同时提高下游性能。


<details>
  <summary>Details</summary>
Motivation: MAE预训练在低标签环境下表现优异，但计算成本高昂，不适用于时间和资源受限的工业环境。

Method: 将Decorrelated Backpropagation (DBP)集成到MAE预训练中，DBP是一种迭代地降低每层输入相关性的优化方法，以加速收敛。

Result: DBP-MAE在ImageNet-1K预训练和ADE20K微调方面，将达到基线性能的挂钟时间减少了21.1%，碳排放量降低了21.4%，分割mIoU提高了1.1个点。在专有工业数据上也观察到类似增益。

Conclusion: DBP可以减少大规模ViT预训练的训练时间和能源消耗，同时提高下游性能，适用于实际场景。

Abstract: Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields
strong performance in low-label regimes but comes with substantial
computational costs, making it impractical in time- and resource-constrained
industrial settings. We address this by integrating Decorrelated
Backpropagation (DBP) into MAE pre-training, an optimization method that
iteratively reduces input correlations at each layer to accelerate convergence.
Applied selectively to the encoder, DBP achieves faster pre-training without
loss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE
reduces wall-clock time to baseline performance by 21.1%, lowers carbon
emissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe
similar gains when pre-training and fine-tuning on proprietary industrial data,
confirming the method's applicability in real-world scenarios. These results
demonstrate that DBP can reduce training time and energy use while improving
downstream performance for large-scale ViT pre-training.

</details>


### [63] [EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)](https://arxiv.org/abs/2510.14661)
*Weikang Yu,Vincent Nwazelibe,Xianping Ma,Xiaokang Zhang,Richard Gloaguen,Xiao Xiang Zhu,Pedram Ghamisi*

Main category: cs.CV

TL;DR: EuroMineNet是一个基于Sentinel-2卫星图像的欧洲多时相采矿足迹数据集，用于监测和管理采矿活动对环境的影响。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在时间深度或地理范围上存在局限，无法满足对采矿诱导的土地地表变化进行长期监测的需求，而这对于可持续资源管理和环境治理至关重要。

Method: 构建了一个包含2015年至2024年间133个欧盟矿区年度观测和专家验证标注的EuroMineNet数据集，并利用其评估了20种先进的深度学习模型在多时相采矿足迹绘制和跨时相变化检测任务上的表现，提出了新的CA-TIoU评估指标。

Result: 研究表明，GeoAI方法能有效识别长期环境变化，但在检测短期动态方面仍有挑战，而这对于及时缓解措施至关重要。

Conclusion: EuroMineNet数据集通过提供时间一致且可解释的采矿监测，促进了可持续土地利用管理、环境韧性以及在地理人工智能（GeoAI）在社会和环境方面的应用，并已在GitHub上公开。

Abstract: Mining activities are essential for industrial and economic development, but
remain a leading source of environmental degradation, contributing to
deforestation, soil erosion, and water contamination. Sustainable resource
management and environmental governance require consistent, long-term
monitoring of mining-induced land surface changes, yet existing datasets are
often limited in temporal depth or geographic scope. To address this gap, we
present EuroMineNet, the first comprehensive multitemporal benchmark for mining
footprint mapping and monitoring based on Sentinel-2 multispectral imagery.
Spanning 133 mining sites across the European Union, EuroMineNet provides
annual observations and expert-verified annotations from 2015 to 2024, enabling
GeoAI-based models to analyze environmental dynamics at a continental scale. It
supports two sustainability-driven tasks: (1) multitemporal mining footprint
mapping for consistent annual land-use delineation, evaluated with a novel
Change-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change
detection to capture both gradual and abrupt surface transformations.
Benchmarking 20 state-of-the-art deep learning models reveals that while GeoAI
methods effectively identify long-term environmental changes, challenges remain
in detecting short-term dynamics critical for timely mitigation. By advancing
temporally consistent and explainable mining monitoring, EuroMineNet
contributes to sustainable land-use management, environmental resilience, and
the broader goal of applying GeoAI for social and environmental good. We
release the codes and datasets by aligning with FAIR and the open science
paradigm at https://github.com/EricYu97/EuroMineNet.

</details>


### [64] [WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging](https://arxiv.org/abs/2510.14668)
*Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Sami Azam,Asif Karim,Jemima Beissbarth,Amanda Leach*

Main category: cs.CV

TL;DR: WeCKD通过构建一个逐步精炼知识的蒸馏链，解决了传统知识蒸馏中知识退化和数据依赖问题，在有限数据下表现优于监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法存在知识退化、监督效率低下、依赖强教师模型或大量标记数据等问题，在数据有限的场景下效果受限。

Method: 提出首个弱监督链式知识蒸馏网络（WeCKD），构建一个由相互连接的模型组成的序列，每个模型在前一个模型学习知识的基础上进行精炼，并传递给下一个模型。每个模型仅使用部分数据进行训练。

Result: +23%的累积准确率增益，在多个耳镜、显微镜和核磁共振成像数据集上，性能超越现有监督方法。

Conclusion: WeCKD通过结构化知识传递有效解决了传统知识蒸馏的局限性，在有限数据下实现高效学习，具有实际应用潜力。

Abstract: Knowledge distillation (KD) has traditionally relied on a static
teacher-student framework, where a large, well-trained teacher transfers
knowledge to a single student model. However, these approaches often suffer
from knowledge degradation, inefficient supervision, and reliance on either a
very strong teacher model or large labeled datasets, which limits their
effectiveness in real-world, limited-data scenarios. To address these, we
present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that
redefines knowledge transfer through a structured sequence of interconnected
models. Unlike conventional KD, it forms a progressive distillation chain,
where each model not only learns from its predecessor but also refines the
knowledge before passing it forward. This structured knowledge transfer further
enhances feature learning, reduces data dependency, and mitigates the
limitations of one-step KD. Each model in the distillation chain is trained on
only a fraction of the dataset and demonstrates that effective learning can be
achieved with minimal supervision. Extensive evaluations across four otoscopic
imaging datasets demonstrate that it not only matches but in many cases
surpasses the performance of existing supervised methods. Experimental results
on two other datasets further underscore its generalization across diverse
medical imaging modalities, including microscopic and magnetic resonance
imaging. Furthermore, our evaluations resulted in cumulative accuracy gains of
up to +23% over a single backbone trained on the same limited data, which
highlights its potential for real-world adoption.

</details>


### [65] [VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning](https://arxiv.org/abs/2510.14672)
*Jinglei Zhang,Yuanfan Guo,Rolandos Alexandros Potamias,Jiankang Deng,Hang Xu,Chao Ma*

Main category: cs.CV

TL;DR: VTimeCoT是一个创新的无训练框架，通过引入视频进度条工具和跨模态的visuotemporal CoT，显著提升了基于多模态大语言模型（MLLM）的视频问答在时间定位和推理方面的能力，并实现了可解释的推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有的基于多模态大语言模型（MLLM）的视频问答系统在视频时间定位和推理方面存在不足，难以满足真实世界视频理解的需求。

Method: 提出VTimeCoT框架，包含两个核心组件：1. 进度条集成工具（用于模拟人类与播放器交互）；2. 高效高亮工具。同时，引入visuotemporal CoT过程，结合视频和文本信息进行跨模态推理，以克服传统文本CoT的局限性。

Result: 在Qwen2VL-7B和GPT4o基线上，VTimeCoT在视频时间定位和推理任务上均取得了显著的性能提升。此外，框架展示了其在推理过程中的组合性和可解释性。

Conclusion: VTimeCoT框架通过引入新颖的视频进度条工具和visuotemporal CoT，有效解决了现有MLLM在视频时间定位和推理方面的短板，提升了视频理解性能，并提供了可解释的推理能力。

Abstract: In recent years, video question answering based on multimodal large language
models (MLLM) has garnered considerable attention, due to the benefits from the
substantial advancements in LLMs. However, these models have a notable
deficiency in the domains of video temporal grounding and reasoning, posing
challenges to the development of effective real-world video understanding
systems. Inspired by how humans use video players to interact with the progress
bar for video comprehension, we introduce VTimeCoT, a simple yet effective
training-free framework, designed for high-performance video grounding and
reasoning. The proposed framework incorporates two novel visual tools of the
progress bar: a plug-and-play progress bar integration tool and a
high-efficiency highlighting tool. In addition, to address the limitations of
conventional text-based chain-of-thought (CoT) approaches, we introduce a
visuotemporal CoT process that integrates cross-modality reasoning across both
video and text. Our approach demonstrates significant performance improvements
on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and
reasoning-based question answering. Finally, we showcase that the proposed
framework achieves a compositional and interpretable reasoning process. Project
page: https://vtimecot.github.io

</details>


### [66] [Leveraging Learned Image Prior for 3D Gaussian Compression](https://arxiv.org/abs/2510.14705)
*Seungjoo Shin,Jaesik Park,Sunghyun Cho*

Main category: cs.CV

TL;DR: 该研究提出了一种利用学习到的图像先验来压缩3D高斯泼溅（3DGS）的技术，以提高压缩率和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS压缩技术在减小存储开销方面取得了成功，但缺乏学习先验来进一步优化率失真性能。

Method: 提出了一种新的3DGS压缩框架，该框架利用学习到的图像先验来恢复压缩引起的质量下降。该框架在初始压缩的高斯基础上，构建了一个恢复网络，用于模拟图像空间中压缩伪影。为了提高率失真性能，该框架还引入了粗糙渲染残差作为侧信息。通过恢复图像的监督，压缩的高斯被优化，从而实现紧凑的表示和增强的渲染性能。该框架兼容现有的高斯压缩方法。

Result: 实验证明，该框架在率失真性能方面优于现有的3DGS压缩方法，同时所需的存储空间大大减少，渲染质量也更高。

Conclusion: 所提出的框架能够有效地利用学习到的图像先验来提高3DGS的压缩效率和渲染质量，并在率失真性能方面取得了最先进的成果。

Abstract: Compression techniques for 3D Gaussian Splatting (3DGS) have recently
achieved considerable success in minimizing storage overhead for 3D Gaussians
while preserving high rendering quality. Despite the impressive storage
reduction, the lack of learned priors restricts further advances in the
rate-distortion trade-off for 3DGS compression tasks. To address this, we
introduce a novel 3DGS compression framework that leverages the powerful
representational capacity of learned image priors to recover
compression-induced quality degradation. Built upon initially compressed
Gaussians, our restoration network effectively models the compression artifacts
in the image space between degraded and original Gaussians. To enhance the
rate-distortion performance, we provide coarse rendering residuals into the
restoration network as side information. By leveraging the supervision of
restored images, the compressed Gaussians are refined, resulting in a highly
compact representation with enhanced rendering performance. Our framework is
designed to be compatible with existing Gaussian compression methods, making it
broadly applicable across different baselines. Extensive experiments validate
the effectiveness of our framework, demonstrating superior rate-distortion
performance and outperforming the rendering quality of state-of-the-art 3DGS
compression methods while requiring substantially less storage.

</details>


### [67] [Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery](https://arxiv.org/abs/2510.14709)
*Caleb Robinson,Kimberly T. Goetz,Christin B. Khan,Meredith Sackett,Kathleen Leonard,Rahul Dodhia,Juan M. Lavista Ferres*

Main category: cs.CV

TL;DR: 该研究提出了一种利用统计异常检测方法，通过识别图像中的空间异常点（“有趣点”）来半自动地在VHR卫星图像中检测鲸鱼，并结合一个基于Web的标注界面，以提高鲸鱼种群监测的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 鲸鱼种群监测对保护至关重要，但传统方法成本高昂且难以规模化。虽然已有研究表明可在VHR卫星图像中识别鲸鱼，但由于缺乏标注数据、图像质量和环境条件变化以及构建大规模遥感数据处理管线的成本高昂，自动化检测仍具挑战性。

Method: 采用一种统计异常检测方法，识别VHR卫星图像中的空间异常点，并将此检测器与一个基于Web的标注界面相结合，以实现鲸鱼的半自动检测和专家标注。

Result: 在三个具有已知鲸鱼标注的基准场景上进行评估，实现了90.3%至96.4%的召回率，并将需要专家检查的区域减少了高达99.8%，有效解决了大规模自动化检测的挑战。

Conclusion: 该方法不依赖于标记的训练数据，为未来从太空进行海洋哺乳动物监测的机器辅助提供了可扩展的第一步，并已开源其处理管线。

Abstract: Effective monitoring of whale populations is critical for conservation, but
traditional survey methods are expensive and difficult to scale. While prior
work has shown that whales can be identified in very high-resolution (VHR)
satellite imagery, large-scale automated detection remains challenging due to a
lack of annotated imagery, variability in image quality and environmental
conditions, and the cost of building robust machine learning pipelines over
massive remote sensing archives. We present a semi-automated approach for
surfacing possible whale detections in VHR imagery using a statistical anomaly
detection method that flags spatial outliers, i.e. "interesting points". We
pair this detector with a web-based labeling interface designed to enable
experts to quickly annotate the interesting points. We evaluate our system on
three benchmark scenes with known whale annotations and achieve recalls of
90.3% to 96.4%, while reducing the area requiring expert inspection by up to
99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method
does not rely on labeled training data and offers a scalable first step toward
future machine-assisted marine mammal monitoring from space. We have open
sourced this pipeline at https://github.com/microsoft/whales.

</details>


### [68] [Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models](https://arxiv.org/abs/2510.14713)
*Tingyu Lin,Armin Dadras,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: 该研究首次系统性地评估了深度视频镜头运动分类（CMC）模型在历史影像资料上的泛化能力，并提出了HISTORIAN数据集，用于在二战档案片段上评估模型表现。


<details>
  <summary>Details</summary>
Motivation: 评估现有深度视频镜头运动分类（CMC）方法在历史影像资料上的泛化能力，并为未来在该领域的研究提供指导。

Method: 评估了五种标准的视频分类模型在HISTORIAN数据集上的表现，该数据集包含经过专家注释的二战影像资料。

Result: 在HISTORIAN数据集上，性能最佳的模型（Video Swin Transformer）达到了80.25%的准确率，显示出在有限的训练数据下仍具有很强的收敛性。

Conclusion: 现有的CMC模型在低质量视频上的应用面临挑战，但仍有潜力。未来的研究应探索结合多种输入模态和时间架构的方法。

Abstract: Camera movement conveys spatial and narrative information essential for
understanding video content. While recent camera movement classification (CMC)
methods perform well on modern datasets, their generalization to historical
footage remains unexplored. This paper presents the first systematic evaluation
of deep video CMC models on archival film material. We summarize representative
methods and datasets, highlighting differences in model design and label
definitions. Five standard video classification models are assessed on the
HISTORIAN dataset, which includes expert-annotated World War II footage. The
best-performing model, Video Swin Transformer, achieves 80.25% accuracy,
showing strong convergence despite limited training data. Our findings
highlight the challenges and potential of adapting existing models to
low-quality video and motivate future work combining diverse input modalities
and temporal architectures.

</details>


### [69] [Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection](https://arxiv.org/abs/2510.14726)
*Dingzhou Xie,Rushi Lan,Cheng Pang,Enhao Ning,Jiahao Zeng,Wei Zheng*

Main category: cs.CV

TL;DR: CFSAM模块通过显式跨层注意力建模，提升了多尺度目标检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有对象检测方法在利用注意力机制时，仅限于单层或双层特征的融合，忽略了多尺度表示之间丰富的跨层依赖关系，限制了其捕捉包含大尺度变化目标的综合上下文信息的能力。

Method: 提出了一种新颖的跨层特征自注意力模块（CFSAM），该模块整体建模了多尺度特征图中的局部和全局依赖关系。CFSAM包含三个关键组件：卷积局部特征提取器、能够有效捕捉跨层交互的基于Transformer的全局建模单元以及用于恢复和增强原始表示的特征融合机制。

Result: 将CFSAM集成到SSD300框架中，在PASCAL VOC上达到了78.6%的mAP（相比基线75.5%），在COCO上达到了52.1%的mAP（相比基线43.1%），性能显著提升，优于现有的注意力模块。此外，该模块在没有引入大量计算开销的情况下，加速了训练过程中的收敛。

Conclusion: 明确的跨层注意力建模对于推进多尺度目标检测至关重要。

Abstract: Recent object detection methods have made remarkable progress by leveraging
attention mechanisms to improve feature discriminability. However, most
existing approaches are confined to refining single-layer or fusing dual-layer
features, overlooking the rich inter-layer dependencies across multi-scale
representations. This limits their ability to capture comprehensive contextual
information essential for detecting objects with large scale variations. In
this paper, we propose a novel Cross-Layer Feature Self-Attention Module
(CFSAM), which holistically models both local and global dependencies within
multi-scale feature maps. CFSAM consists of three key components: a
convolutional local feature extractor, a Transformer-based global modeling unit
that efficiently captures cross-layer interactions, and a feature fusion
mechanism to restore and enhance the original representations. When integrated
into the SSD300 framework, CFSAM significantly boosts detection performance,
achieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO
(vs. 43.1% baseline), outperforming existing attention modules. Moreover, the
module accelerates convergence during training without introducing substantial
computational overhead. Our work highlights the importance of explicit
cross-layer attention modeling in advancing multi-scale object detection.

</details>


### [70] [Free-Grained Hierarchical Recognition](https://arxiv.org/abs/2510.14737)
*Seulki Park,Zilin Wang,Stella X. Yu*

Main category: cs.CV

TL;DR: 现实世界的标签标注具有不确定性，这给图像分类带来了挑战。本研究提出了一个名为ImageNet-F的新基准，并引入了自由粒度学习方法，利用伪属性和半监督学习来提高在混合监督下的分层图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图像标注通常缺乏细粒度，并且标注的粒度不一，这是现有分层图像分类方法未能解决的一个关键问题。

Method: 通过使用CLIP模拟现实世界的混合粒度标签，并提出自由粒度学习方法，该方法结合了伪属性和半监督学习来处理异构监督。

Result: 该方法在混合监督下显著提高了分层图像分类的性能。

Conclusion: ImageNet-F基准和新提出的自由粒度学习方法共同推动了在真实世界约束下进行分层图像分类的进展。

Abstract: Hierarchical image classification predicts labels across a semantic taxonomy,
but existing methods typically assume complete, fine-grained annotations, an
assumption rarely met in practice. Real-world supervision varies in
granularity, influenced by image quality, annotator expertise, and task
demands; a distant bird may be labeled Bird, while a close-up reveals Bald
eagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet
and structured into cognitively inspired basic, subordinate, and fine-grained
levels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic,
mixed-granularity labels reflecting human annotation behavior. We propose
free-grain learning, with heterogeneous supervision across instances. We
develop methods that enhance semantic guidance via pseudo-attributes from
vision-language models and visual guidance via semi-supervised learning. These,
along with strong baselines, substantially improve performance under mixed
supervision. Together, our benchmark and methods advance hierarchical
classification under real-world constraints.

</details>


### [71] [DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models](https://arxiv.org/abs/2510.14741)
*Simone Carnemolla,Matteo Pennisi,Sarinda Samarasinghe,Giovanni Bellitto,Simone Palazzo,Daniela Giordano,Mubarak Shah,Concetto Spampinato*

Main category: cs.CV

TL;DR: DEXTER是一个数据无关的框架，利用扩散模型和大型语言模型来生成视觉分类器的全局文本解释，无需训练数据或真实标签。


<details>
  <summary>Details</summary>
Motivation: 理解和解释机器学习模型的行为对于构建透明和可信赖的AI系统至关重要。

Method: DEXTER通过优化文本提示来合成强激活目标分类器的条件图像。然后，利用这些合成样本生成详细的自然语言报告，描述特定类别的决策模式和偏差。

Result: DEXTER在激活最大化、切片发现和去偏以及偏差解释等三个任务中展示了其灵活性，揭示了视觉分类器的内部机制。用户研究等定量和定性评估表明，DEXTER的输出准确且可解释。在ImageNet、Waterbirds、CelebA和FairFaces上的实验证实，DEXTER在全局模型解释和类别级偏差报告方面优于现有方法。

Conclusion: DEXTER在没有训练数据或真实标签的情况下，能够生成关于分类器决策过程的自然语言解释，并且在准确性、可解释性和偏差报告方面优于现有方法。

Abstract: Understanding and explaining the behavior of machine learning models is
essential for building transparent and trustworthy AI systems. We introduce
DEXTER, a data-free framework that employs diffusion models and large language
models to generate global, textual explanations of visual classifiers. DEXTER
operates by optimizing text prompts to synthesize class-conditional images that
strongly activate a target classifier. These synthetic samples are then used to
elicit detailed natural language reports that describe class-specific decision
patterns and biases. Unlike prior work, DEXTER enables natural language
explanation about a classifier's decision process without access to training
data or ground-truth labels. We demonstrate DEXTER's flexibility across three
tasks-activation maximization, slice discovery and debiasing, and bias
explanation-each illustrating its ability to uncover the internal mechanisms of
visual classifiers. Quantitative and qualitative evaluations, including a user
study, show that DEXTER produces accurate, interpretable outputs. Experiments
on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms
existing approaches in global model explanation and class-level bias reporting.
Code is available at https://github.com/perceivelab/dexter.

</details>


### [72] [LightQANet: Quantized and Adaptive Feature Learning for Low-Light Image Enhancement](https://arxiv.org/abs/2510.14753)
*Xu Wu,Zhihui Lai,Xianxu Hou,Jie Zhou,Ya-nan Zhang,Linlin Shen*

Main category: cs.CV

TL;DR: 该研究提出了一种名为LightQANet的新型低光图像增强框架，通过量化和自适应特征学习来提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法在提取特征时存在困难，导致纹理恢复差、色彩不一致和伪影等问题。本研究旨在解决这些挑战。

Method: 提出了一种名为LightQANet的新型框架，包含两个关键模块：1. 光量化模块（LQM），用于提取和量化图像特征中的光照相关因素，增强光照不变性表示。 2. 光感知提示模块（LAPM），将光照先验知识编码为可学习的提示，动态引导特征学习过程。

Result: 在多个低光数据集上的广泛实验表明，该方法在各种具有挑战性的光照场景下实现了最先进的定性和定量结果。

Conclusion: LightQANet通过量化和自适应特征学习，能够有效解决低光图像增强中的挑战，在不同光照条件下实现一致且鲁棒的图像质量。

Abstract: Low-light image enhancement (LLIE) aims to improve illumination while
preserving high-quality color and texture. However, existing methods often fail
to extract reliable feature representations due to severely degraded
pixel-level information under low-light conditions, resulting in poor texture
restoration, color inconsistency, and artifact. To address these challenges, we
propose LightQANet, a novel framework that introduces quantized and adaptive
feature learning for low-light enhancement, aiming to achieve consistent and
robust image quality across diverse lighting conditions. From the static
modeling perspective, we design a Light Quantization Module (LQM) to explicitly
extract and quantify illumination-related factors from image features. By
enforcing structured light factor learning, LQM enhances the extraction of
light-invariant representations and mitigates feature inconsistency across
varying illumination levels. From the dynamic adaptation perspective, we
introduce a Light-Aware Prompt Module (LAPM), which encodes illumination priors
into learnable prompts to dynamically guide the feature learning process. LAPM
enables the model to flexibly adapt to complex and continuously changing
lighting conditions, further improving image enhancement. Extensive experiments
on multiple low-light datasets demonstrate that our method achieves
state-of-the-art performance, delivering superior qualitative and quantitative
results across various challenging lighting scenarios.

</details>


### [73] [MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision and Spiking Neural Networks](https://arxiv.org/abs/2510.14770)
*Zhang Nengbo,Hann Woei Ho,Ye Zhou*

Main category: cs.CV

TL;DR: MAV群体通信面临挑战，提出一种受蜜蜂“摇摆舞”启发的基于运动视觉信号的通信框架，使用事件相机和脉冲神经网络（SNN）解码。


<details>
  <summary>Details</summary>
Motivation: 传统无线通信方法在MAV群体中存在频谱拥挤、干扰和高功耗问题，需要一种更可靠、节能的通信方式。

Method: MAV通过特定的飞行模式（如垂直、水平、左-上-右、左-下-右）传达信息，这些模式被事件相机捕捉，并使用预定义的视觉码本（代表“开始”、“结束”、“1”、“0”）进行解码。采用基于事件帧的分割模型和轻量级SNN进行动作识别，并结合解码算法来解析Mav运动序列。

Result: 实验结果表明，该框架能够准确解码MAV的运动信号，并且功耗低。

Conclusion: 该视觉通信框架为MAV在受限环境下的通信提供了一种有潜力的节能替代方案。

Abstract: Reliable communication in Micro Air Vehicle (MAV) swarms is challenging in
environments, where conventional radio-based methods suffer from spectrum
congestion, jamming, and high power consumption. Inspired by the waggle dance
of honeybees, which efficiently communicate the location of food sources
without sound or contact, we propose a novel visual communication framework for
MAV swarms using motion-based signaling. In this framework, MAVs convey
information, such as heading and distance, through deliberate flight patterns,
which are passively captured by event cameras and interpreted using a
predefined visual codebook of four motion primitives: vertical (up/down),
horizontal (left/right), left-to-up-to-right, and left-to-down-to-right,
representing control symbols (``start'', ``end'', ``1'', ``0''). To decode
these signals, we design an event frame-based segmentation model and a
lightweight Spiking Neural Network (SNN) for action recognition. An integrated
decoding algorithm then combines segmentation and classification to robustly
interpret MAV motion sequences. Experimental results validate the framework's
effectiveness, which demonstrates accurate decoding and low power consumption,
and highlights its potential as an energy-efficient alternative for MAV
communication in constrained environments.

</details>


### [74] [CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection](https://arxiv.org/abs/2510.14792)
*Hojun Choi,Youngsun Lim,Jaeyo Shin,Hyunjung Shim*

Main category: cs.CV

TL;DR: CoT-PL是一个新的框架，通过将结构化的视觉思维链（CoT）推理纳入伪标签生成过程，来解决开放词汇目标检测（OVD）中的鲁棒性问题，特别是在拥挤或遮挡的场景下。它将目标理解分解为区域感知、零样本识别和背景定位三个步骤，并引入对比背景学习（CBL）来分离目标和背景特征。实验证明，CoT-PL在COCO和LVIS数据集上对新类别具有显著的性能提升，并达到了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的开放词汇目标检测（OVD）方法在处理包含遮挡或密集物体的复杂场景时鲁棒性有限，因为它们过度依赖直接的图像-文本匹配，而忽略了中间推理步骤。

Method: CoT-PL框架将结构化的视觉思维链（CoT）推理应用于伪标签生成。具体来说，它将目标理解分解为三个步骤：区域感知（即使是未见过的物体）、通过零样本推理进行类别识别，以及通过背景定位将复杂物体与背景分离开来。此外，该框架还引入了对比背景学习（CBL），利用预先计算的背景线索作为负样本，以促进目标和背景特征的解耦。

Result: CoT-PL在拥挤和遮挡场景下的新类别伪标签质量分别比先前最佳方法有103.4%和168.4%的相对提升。在COCO数据集上，CoT-PL在新类别上的AP50提高了7.7，在LVIS数据集上，新类别的掩码AP提高了2.9，创造了新的最先进水平。

Conclusion: CoT-PL通过整合CoT推理和CBL，为在拥挤或遮挡场景下进行鲁棒的伪标签生成提供了一个有效的解决方案，显著提高了开放词汇目标检测的性能。

Abstract: Open-vocabulary object detection (OVD) seeks to recognize and localize object
categories beyond those seen during training. Recent approaches typically
leverage vision-language models (VLMs) to generate pseudo-labels using
image-text alignment, allowing detectors to generalize to unseen classes
without explicit supervision. However, these methods depend heavily on direct
image-text matching, neglecting the intermediate reasoning steps essential for
interpreting semantically complex scenes. This results in limited robustness
when confronted with crowded or occluded visual contexts. In this paper, we
introduce CoT-PL, a new framework that employs structured visual
chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL
decomposes object understanding into three interpretable steps: (1) region
perception even for unseen objects, (2) category recognition via zero-shot
reasoning, and (3) background grounding to separate semantically complex
objects. Crucially, the third step naturally motivates our contrastive
background learning (CBL) that uses the pre-computed background cues as
negatives to promote feature disentanglement between objects and background. In
this way, CoT reasoning and CBL form an integrated pipeline tailored to robust
pseudo-labeling in crowded or occluded scenes. Notably, in these two settings,
our novel-class pseudo-label quality achieves relative improvements of 103.4%
and 168.4% over the best prior, respectively. Our extensive experiments
demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9
mask AP on LVIS for novel classes, setting a new state of the art.

</details>


### [75] [Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&E Whole Slide Images](https://arxiv.org/abs/2510.14800)
*Usama Sajjad,Abdul Rehman Akbar,Ziyu Su,Deborah Knight,Wendy L. Frankel,Metin N. Gurcan,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: PRISM是一个新颖、可解释的人工智能模型，用于分析结直肠癌（CRC）的组织学图像，通过整合空间形态学特征来预测患者的五年总生存期，其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的计算病理学方法在处理结直肠癌（CRC）时，可能忽略了对肿瘤行为、治疗反应和患者预后至关重要的器官特异性形态学模式。本研究旨在开发一个新颖、可解释的人工智能模型，以捕捉形态学的连续变异性，并反映恶性肿瘤演变的渐进性。

Method: 开发并训练了一个名为PRISM（Prognostic Representation of Integrated Spatial Morphology）的新颖、可解释的人工智能模型。该模型将连续的变异性光谱纳入每种形态学特征中，以表征表型多样性，并利用了424名III期CRC患者的874万张组织学图像。

Result: PRISM在五年总生存期（OS）预测方面表现出卓越的性能，取得了0.70 ± 0.04的AUC，68.37% ± 4.75%的准确率，风险比（HR）为3.34（95% CI = 2.28-4.90；p < 0.0001）。与现有的CRC专用方法相比，PRISM的准确率提高了15%，与AI基础模型相比，准确率提高了约23%。该模型在性别方面表现稳健，并且在不同的临床病理亚组中表现稳定。

Conclusion: PRISM模型能够有效捕捉结直肠癌的表型多样性，并提供优于现有方法的预后预测能力。其可解释性和在不同亚组中的稳定性表明，该模型在临床应用中具有潜力。

Abstract: Colorectal cancer (CRC) remains the third most prevalent malignancy globally,
with approximately 154,000 new cases and 54,000 projected deaths anticipated
for 2025. The recent advancement of foundation models in computational
pathology has been largely propelled by task agnostic methodologies that can
overlook organ-specific crucial morphological patterns that represent distinct
biological processes that can fundamentally influence tumor behavior,
therapeutic response, and patient outcomes. The aim of this study is to develop
a novel, interpretable AI model, PRISM (Prognostic Representation of Integrated
Spatial Morphology), that incorporates a continuous variability spectrum within
each distinct morphology to characterize phenotypic diversity and reflecting
the principle that malignant transformation occurs through incremental
evolutionary processes rather than abrupt phenotypic shifts. PRISM is trained
on 8.74 million histological images extracted from surgical resection specimens
of 424 patients with stage III CRC. PRISM achieved superior prognostic
performance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%;
HR = 3.34, 95% CI = 2.28-4.90; p < 0.0001), outperforming existing CRC-specific
methods by 15% and AI foundation models by ~23% accuracy. It showed
sex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable
performance across clinicopathological subgroups, with minimal accuracy
fluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens,
replicating the Alliance cohort finding of no survival difference between
treatments.

</details>


### [76] [Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks](https://arxiv.org/abs/2510.14803)
*Pedro R. A. S. Bassi,Xinze Zhou,Wenxuan Li,Szymon Płotka,Jieneng Chen,Qi Chen,Zheren Zhu,Jakub Prządo,Ibrahim E. Hamacı,Sezgin Er,Yuhan Wang,Ashwin Kumar,Bjoern Menze,Jarosław B. Ćwikła,Yuyin Zhou,Akshay S. Chaudhari,Curtis P. Langlotz,Sergio Decherchi,Andrea Cavalli,Kang Wang,Yang Yang,Alan L. Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: 通过利用现有的医学报告，R-Super 训练 AI 对 CT 图像中的肿瘤进行分割，无需大量手动标注的肿瘤掩模，从而实现可扩展的肿瘤检测。


<details>
  <summary>Details</summary>
Motivation: 手动创建肿瘤掩模以训练 AI 模型成本高昂且耗时，而医学报告中包含丰富的信息，但未被充分利用。

Method: R-Super 训练 AI 模型，使其能够根据医学报告中的描述来分割肿瘤，从而减少对手动肿瘤掩模的需求。

Result: 在 101,654 份报告上训练的 AI 模型，其性能与使用 723 个掩模训练的模型相当。结合报告和掩模可提高敏感性 13% 和特异性 8%，在七种肿瘤类型中的五种上超过了放射科医生。R-Super 还实现了以前不存在掩模或 AI 模型的脾脏、胆囊、前列腺、膀胱、子宫和食道肿瘤的分割。

Conclusion: R-Super 证明了利用医学报告进行 AI 训练的可行性，挑战了对大规模、劳动密集型肿瘤掩模创建的依赖，为早期检测提供了可扩展且易于获取的途径。

Abstract: Early tumor detection save lives. Each year, more than 300 million computed
tomography (CT) scans are performed worldwide, offering a vast opportunity for
effective cancer screening. However, detecting small or early-stage tumors on
these CT scans remains challenging, even for experts. Artificial intelligence
(AI) models can assist by highlighting suspicious regions, but training such
models typically requires extensive tumor masks--detailed, voxel-wise outlines
of tumors manually drawn by radiologists. Drawing these masks is costly,
requiring years of effort and millions of dollars. In contrast, nearly every CT
scan in clinical practice is already accompanied by medical reports describing
the tumor's size, number, appearance, and sometimes, pathology
results--information that is rich, abundant, and often underutilized for AI
training. We introduce R-Super, which trains AI to segment tumors that match
their descriptions in medical reports. This approach scales AI training with
large collections of readily available medical reports, substantially reducing
the need for manually drawn tumor masks. When trained on 101,654 reports, AI
models achieved performance comparable to those trained on 723 masks. Combining
reports and masks further improved sensitivity by +13% and specificity by +8%,
surpassing radiologists in detecting five of the seven tumor types. Notably,
R-Super enabled segmentation of tumors in the spleen, gallbladder, prostate,
bladder, uterus, and esophagus, for which no public masks or AI models
previously existed. This study challenges the long-held belief that
large-scale, labor-intensive tumor mask creation is indispensable, establishing
a scalable and accessible path toward early detection across diverse tumor
types.
  We plan to release our trained models, code, and dataset at
https://github.com/MrGiovanni/R-Super

</details>


### [77] [Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning](https://arxiv.org/abs/2510.14819)
*Ji Cao,Yu Wang,Tongya Zheng,Zujie Ren,Canghong Jin,Gang Chen,Mingli Song*

Main category: cs.CV

TL;DR: PRTraj是一个新的轨迹表示学习框架，它通过整合环境感知和路线选择模型来改进轨迹编码，并在多项下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的轨迹表示学习方法忽略了影响轨迹形成的外部环境和内部路线选择行为。

Method: PRTraj框架包含一个环境感知模块，用于捕捉周围兴趣点（POI）分布的多粒度环境语义，以及一个路线选择编码器，用于将轨迹的路线选择行为建模为一系列决策。

Result: PRTraj在3个真实世界数据集的5项下游任务上进行了广泛实验，验证了其有效性和泛化能力，并且在少样本场景下表现出良好的数据效率。

Conclusion: PRTraj通过整合环境感知和路线选择模型，能够学习到更有效的轨迹表示，并在各种下游任务中取得优越性能。

Abstract: Trajectory Representation Learning (TRL) aims to encode raw trajectories into
low-dimensional vectors, which can then be leveraged in various downstream
tasks, including travel time estimation, location prediction, and trajectory
similarity analysis. However, existing TRL methods suffer from a key oversight:
treating trajectories as isolated spatio-temporal sequences, without
considering the external environment and internal route choice behavior that
govern their formation. To bridge this gap, we propose a novel framework that
unifies comprehensive environment \textbf{P}erception and explicit
\textbf{R}oute choice modeling for effective \textbf{Traj}ectory representation
learning, dubbed \textbf{PRTraj}. Specifically, PRTraj first introduces an
Environment Perception Module to enhance the road network by capturing
multi-granularity environmental semantics from surrounding POI distributions.
Building on this environment-aware backbone, a Route Choice Encoder then
captures the route choice behavior inherent in each trajectory by modeling its
constituent road segment transitions as a sequence of decisions. These
route-choice-aware representations are finally aggregated to form the global
trajectory embedding. Extensive experiments on 3 real-world datasets across 5
downstream tasks validate the effectiveness and generalizability of PRTraj.
Moreover, PRTraj demonstrates strong data efficiency, maintaining robust
performance under few-shot scenarios. Our code is available at:
https://anonymous.4open.science/r/PRTraj.

</details>


### [78] [FraQAT: Quantization Aware Training with Fractional bits](https://arxiv.org/abs/2510.14823)
*Luca Morreale,Alberto Gil C. P. Ramos,Malcolm Chadwick,Mehid Noroozi,Ruchika Chavhan,Abhinav Mehrotra,Sourav Bhattacharya*

Main category: cs.CV

TL;DR: 该研究提出了一种名为‘short’的新型量化方法，通过渐进式降低模型精度并利用优化过程中的分数位来解决大模型在智能手机上部署的效率和内存问题，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 大型生成模型因内存和计算限制难以部署在智能手机上，而现有量化方法在降低精度的同时难以保持模型质量。

Method: 提出一种名为‘short’的新型分数位量化方法，将模型参数精度从32位渐进式降低至4位，并利用优化过程中的分数位来保持生成质量。

Result: ‘short’量化方法在SD3.5-Medium、Sana、pixart和FLUX.1-schnell等多种扩散模型上取得了比标准QAT（量化感知训练）更好的质量表现，FiD（Fréchet Inception Distance）降低了4-7%。研究成功地将Sana模型部署在三星S25U手机上。

Conclusion: ‘short’量化方法能够有效解决大模型在移动端部署的挑战，在保持高效的同时保证了生成质量，并成功实现了在智能手机上的实际应用。

Abstract: State-of-the-art (SOTA) generative models have demonstrated impressive
capabilities in image synthesis or text generation, often with a large capacity
model. However, these large models cannot be deployed on smartphones due to the
limited availability of on-board memory and computations. Quantization methods
lower the precision of the model parameters, allowing for efficient
computations, \eg, in \INT{8}. Although aggressive quantization addresses
efficiency and memory constraints, preserving the quality of the model remains
a challenge. To retain quality in previous aggressive quantization, we propose
a new fractional bits quantization (\short) approach. The novelty is a simple
yet effective idea: we progressively reduce the model's precision from 32 to 4
bits per parameter, and exploit the fractional bits during optimization to
maintain high generation quality. We show that the \short{} yields improved
quality on a variety of diffusion models, including SD3.5-Medium, Sana,
\pixart, and FLUX.1-schnell, while achieving $4-7\%$ lower FiD than standard
QAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the
Qualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP).

</details>


### [79] [Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data](https://arxiv.org/abs/2510.14831)
*Qi Chen,Xinze Zhou,Chen Liu,Hao Chen,Wenxuan Li,Zekun Jiang,Ziyan Huang,Yuxuan Zhao,Dexin Yu,Junjun He,Yefeng Zheng,Ling Shao,Alan Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: AI在肿瘤分割方面，由于缺乏大型、逐个体素标注的数据集而受到限制。本研究通过使用合成数据，在仅使用500个真实扫描的情况下达到了与1500个真实扫描相当的性能，这表明合成数据可以加强数据规模法则，从而比单独使用真实数据进行更有效的模型训练。研究人员基于这些经验创建了AbdomenAtlas 2.0，这是一个包含10,135个CT扫描的大型数据集，标注了六个器官的15,130个肿瘤实例，并附带5,893个对照扫描。该数据集由23名放射科专家标注，规模远超现有公共数据集，并在器官肿瘤分割方面取得了显著的性能提升，在分布内测试中提高了7%的DSC，在分布外测试中提高了16%。


<details>
  <summary>Details</summary>
Motivation: AI在肿瘤分割方面，由于缺乏大型、逐个体素标注的数据集而受到限制。本研究旨在通过利用合成数据和创建一个更大规模的数据集来克服这一挑战，以提高AI在肿瘤分割任务上的效率和性能。

Method: 1. 分析专有JHH数据集，发现AI性能在1500个扫描后停止改进，并表明合成数据可将所需真实扫描数量减少到500个。 2. 基于这些发现，创建了AbdomenAtlas 2.0数据集，包含10,135个CT扫描和15,130个肿瘤实例，以及5,893个对照扫描，并由23名放射科专家进行了标注。

Result: AbdomenAtlas 2.0数据集在器官肿瘤分割方面取得了显著的性能提升，在分布内测试中提高了7%的DSC，在分布外测试中提高了16%。

Conclusion: 合成数据可以加强数据规模法则，使模型训练比单独使用真实数据更有效。AbdomenAtlas 2.0数据集为训练AI分割器官肿瘤提供了坚实的基础，并显著优于现有的公共数据集。

Abstract: AI for tumor segmentation is limited by the lack of large, voxel-wise
annotated datasets, which are hard to create and require medical experts. In
our proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found
that AI performance stopped improving after 1,500 scans. With synthetic data,
we reached the same performance using only 500 real scans. This finding
suggests that synthetic data can steepen data scaling laws, enabling more
efficient model training than real data alone. Motivated by these lessons, we
created AbdomenAtlas 2.0--a dataset of 10,135 CT scans with a total of 15,130
tumor instances per-voxel manually annotated in six organs (pancreas, liver,
kidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23
expert radiologists, it is several orders of magnitude larger than existing
public tumor datasets. While we continue expanding the dataset, the current
version of AbdomenAtlas 2.0 already provides a strong foundation--based on
lessons from the JHH dataset--for training AI to segment tumors in six organs.
It achieves notable improvements over public datasets, with a +7% DSC gain on
in-distribution tests and +16% on out-of-distribution tests.

</details>


### [80] [ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints](https://arxiv.org/abs/2510.14847)
*Meiqi Wu,Jiashu Zhu,Xiaokun Feng,Chubin Chen,Chen Zhu,Bingze Song,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为ImagerySearch的自适应测试时间搜索策略，用于提升模型在生成富有想象力的视频场景（如包含罕见概念组合或长距离语义关系的提示）方面的能力。同时，引入了一个新的基准测试LDT-Bench来专门评估这类场景下的视频生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在处理包含罕见概念组合或长距离语义关系的富有想象力的场景时表现不佳，而测试时调整（如test-time scaling）的固定搜索空间和奖励设计限制了其适应性。

Method: 提出ImagerySearch，一种提示引导的自适应测试时间搜索策略，能够根据提示中的语义关系动态调整推理搜索空间和奖励函数。引入LDT-Bench基准测试，包含2839个概念对，用于评估创意生成能力。

Result: ImagerySearch在LDT-Bench基准测试上系统性地优于现有的视频生成基线和测试时间调整方法，并在VBench上取得了有竞争力的结果，证明了其在多种提示类型上的有效性。

Conclusion: ImagerySearch通过自适应地调整搜索空间和奖励函数，能够更有效地生成连贯且视觉上可信的富有想象力的视频。LDT-Bench为该领域的研究提供了新的评估工具。

Abstract: Video generation models have achieved remarkable progress, particularly
excelling in realistic scenarios; however, their performance degrades notably
in imaginative scenarios. These prompts often involve rarely co-occurring
concepts with long-distance semantic relationships, falling outside training
distributions. Existing methods typically apply test-time scaling for improving
video quality, but their fixed search spaces and static reward designs limit
adaptability to imaginative scenarios. To fill this gap, we propose
ImagerySearch, a prompt-guided adaptive test-time search strategy that
dynamically adjusts both the inference search space and reward function
according to semantic relationships in the prompt. This enables more coherent
and visually plausible videos in challenging imaginative settings. To evaluate
progress in this direction, we introduce LDT-Bench, the first dedicated
benchmark for long-distance semantic prompts, consisting of 2,839 diverse
concept pairs and an automated protocol for assessing creative generation
capabilities. Extensive experiments show that ImagerySearch consistently
outperforms strong video generation baselines and existing test-time scaling
approaches on LDT-Bench, and achieves competitive improvements on VBench,
demonstrating its effectiveness across diverse prompt types. We will release
LDT-Bench and code to facilitate future research on imaginative video
generation.

</details>


### [81] [A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution Simulation](https://arxiv.org/abs/2510.14855)
*Harsha Kotla,Arun Kumar Rajasekaran,Hannah Rana*

Main category: cs.CV

TL;DR: 提出一个深度学习框架，对皮肤病灶进行分类，并量化ABCD特征得分，模拟E特征的演变，并可视化ABCD特征轨迹。


<details>
  <summary>Details</summary>
Motivation: 早期检测黑色瘤对提高生存率至关重要，但目前大多数深度学习方法将ABCDE分类法视为黑箱，未能解释其可解释的特征。

Method: 提出一个深度学习框架，对皮肤病灶进行分类，量化ABCD特征得分，并模拟E特征的演变。该框架还在潜在空间中可视化ABCD特征轨迹，以模拟皮肤病灶从良性痣到恶性黑色瘤的演变。

Result: 分类准确率为89%，黑色瘤AUC为0.96。在预测不对称性、颜色变化和直径方面表现良好，但边境不规则性建模仍然困难。

Conclusion: 该框架能够将机器学习诊断与临床相关标准联系起来，从而增进对皮肤癌进展的理解。

Abstract: Early detection of melanoma has grown to be essential because it
significantly improves survival rates, but automated analysis of skin lesions
still remains challenging. ABCDE, which stands for Asymmetry, Border
irregularity, Color variation, Diameter, and Evolving, is a well-known
classification method for skin lesions, but most deep learning mechanisms treat
it as a black box, as most of the human interpretable features are not
explained. In this work, we propose a deep learning framework that both
classifies skin lesions into categories and also quantifies scores for each
ABCD feature. It simulates the evolution of these features over time in order
to represent the E aspect, opening more windows for future exploration. The A,
B, C, and D values are quantified particularly within this work. Moreover, this
framework also visualizes ABCD feature trajectories in latent space as skin
lesions evolve from benign nevuses to malignant melanoma. The experiments are
conducted using the HAM10000 dataset that contains around ten thousand images
of skin lesions of varying stages. In summary, the classification worked with
an accuracy of around 89 percent, with melanoma AUC being 0.96, while the
feature evaluation performed well in predicting asymmetry, color variation, and
diameter, though border irregularity remains more difficult to model. Overall,
this work provides a deep learning framework that will allow doctors to link ML
diagnoses to clinically relevant criteria, thus improving our understanding of
skin cancer progression.

</details>


### [82] [Benchmarking Multimodal Large Language Models for Face Recognition](https://arxiv.org/abs/2510.14866)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: MLLMs在面部识别任务中表现有待提高，虽然能够捕捉丰富的语义线索，但在高精度识别方面不如专业模型，本研究提供了一个系统性基准，为未来模型设计提供参考。


<details>
  <summary>Details</summary>
Motivation: 评估和比较开源MLLMs在标准面部识别基准上的性能，以及与现有面部识别模型的对比。

Method: 在LFW、CALFW、CPLFW、CFP、AgeDB和RFW等多个面部识别数据集上，对最先进的MLLMs进行系统性基准测试。

Result: MLLMs能够捕捉到有用的面部相关语义线索，但在零样本应用的高精度识别场景中，其性能落后于专业模型。

Conclusion: MLLMs在面部识别方面有潜力，但目前在高精度识别方面仍需改进，本基准测试为未来MLLM在面部识别领域的研发提供了基础和方向。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable performance
across diverse vision-and-language tasks. However, their potential in face
recognition remains underexplored. In particular, the performance of
open-source MLLMs needs to be evaluated and compared with existing face
recognition models on standard benchmarks with similar protocol. In this work,
we present a systematic benchmark of state-of-the-art MLLMs for face
recognition on several face recognition datasets, including LFW, CALFW, CPLFW,
CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich
semantic cues useful for face-related tasks, they lag behind specialized models
in high-precision recognition scenarios in zero-shot applications. This
benchmark provides a foundation for advancing MLLM-based face recognition,
offering insights for the design of next-generation models with higher accuracy
and generalization. The source code of our benchmark is publicly available in
the project page.

</details>


### [83] [TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions](https://arxiv.org/abs/2510.14874)
*Guangyi Han,Wei Zhai,Yuhang Yang,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 该研究提出了一种新的自由形式手-物交互（HOI）生成方法，称为TOUCH，它扩展了HOI的范围，使其不仅限于抓握，还包括推、戳、旋转等多种形式的交互。


<details>
  <summary>Details</summary>
Motivation: 现有的HOI生成研究主要集中在固定的抓握模式，受限于物理先验或笼统的意图指令，无法捕捉日常HOI的多样性。本研究旨在解决这些局限性，实现可控、多样且物理上合理的HOI生成，并支持细粒度的意图控制。

Method: 研究人员构建了一个名为WildO2的数据集，其中包含来自互联网视频的、多样化的3D HOI数据，涵盖了92种意图和610种物体类别。在此基础上，他们提出了TOUCH框架，该框架以多层次扩散模型为核心，通过显式接触建模、接触一致性和物理约束来实现细粒度的语义控制，生成超越抓握先验的多功能手部姿势。

Result: 实验证明，所提出的TOUCH方法能够生成可控、多样且物理上合理的手部交互，能够代表日常活动。

Conclusion: 该研究成功地将HOI生成从固定的抓握模式扩展到自由形式的交互，并通过WildO2数据集和TOUCH框架实现了细粒度的意图控制和多样化的交互生成。

Abstract: Hand-object interaction (HOI) is fundamental for humans to express intent.
Existing HOI generation research is predominantly confined to fixed grasping
patterns, where control is tied to physical priors such as force closure or
generic intent instructions, even when expressed through elaborate language.
Such an overly general conditioning imposes a strong inductive bias for stable
grasps, thus failing to capture the diversity of daily HOI. To address these
limitations, we introduce Free-Form HOI Generation, which aims to generate
controllable, diverse, and physically plausible HOI conditioned on fine-grained
intent, extending HOI from grasping to free-form interactions, like pushing,
poking, and rotating. To support this task, we construct WildO2, an in-the-wild
diverse 3D HOI dataset, which includes diverse HOI derived from internet
videos. Specifically, it contains 4.4k unique interactions across 92 intents
and 610 object categories, each with detailed semantic annotations. Building on
this dataset, we propose TOUCH, a three-stage framework centered on a
multi-level diffusion model that facilitates fine-grained semantic control to
generate versatile hand poses beyond grasping priors. This process leverages
explicit contact modeling for conditioning and is subsequently refined with
contact consistency and physical constraints to ensure realism. Comprehensive
experiments demonstrate our method's ability to generate controllable, diverse,
and physically plausible hand interactions representative of daily activities.
The project page is $\href{https://guangyid.github.io/hoi123touch}{here}$.

</details>


### [84] [BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data](https://arxiv.org/abs/2510.14876)
*Roni Goldshmidt,Hamish Scott,Lorenzo Niccolini,Shizhan Zhu,Daniel Moura,Orly Zvitia*

Main category: cs.CV

TL;DR: BADAS是一种新的碰撞预测模型，在真实世界的行车记录仪数据上进行训练，能够区分与“自我”车辆相关的威胁和随机事故，从而减少不必要的警报。


<details>
  <summary>Details</summary>
Motivation: 现有的碰撞预测方法无法区分涉及“自我”车辆的威胁和随机事故，导致在实际部署中发出过多的错误警报。

Method: BADAS使用V-JEPA2骨干网络进行端到端训练，并提供两种版本：BADAS-Open（在1.5k个公开视频上训练）和BADAS1.0（在40k个专有视频上训练）。对主要基准进行了重新标注，以识别“自我”的参与，添加了共识警报时间标签，并在需要时合成负样本，以实现公平的AP/AUC和时间评估。

Result: 在DAD、DADA-2000、DoTA和Nexar数据集上，BADAS实现了最先进的AP/AUC，并且在生成更真实的车祸时间估计方面优于前向碰撞ADAS基线。

Conclusion: BADAS通过区分与“自我”车辆相关的威胁和随机事故，显著提高了碰撞预测的准确性，并减少了不必要的警报。研究人员发布了BADAS-Open模型权重、代码以及所有评估数据集的重新标注，以促进以“自我”为中心的碰撞预测研究。

Abstract: Existing collision prediction methods often fail to distinguish between
ego-vehicle threats and random accidents not involving the ego vehicle, leading
to excessive false alerts in real-world deployment. We present BADAS, a family
of collision prediction models trained on Nexar's real-world dashcam collision
dataset -- the first benchmark designed explicitly for ego-centric evaluation.
We re-annotate major benchmarks to identify ego involvement, add consensus
alert-time labels, and synthesize negatives where needed, enabling fair AP/AUC
and temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and
comes in two variants: BADAS-Open (trained on our 1.5k public videos) and
BADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and
Nexar, BADAS achieves state-of-the-art AP/AUC and outperforms a
forward-collision ADAS baseline while producing more realistic time-to-accident
estimates. We release our BADAS-Open model weights and code, along with
re-annotations of all evaluation datasets to promote ego-centric collision
prediction research.

</details>


### [85] [ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention](https://arxiv.org/abs/2510.14882)
*Keli Liu,Zhendong Wang,Wengang Zhou,Shaodong Xu,Ruixiao Dong,Houqiang Li*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Text-to-image generation with visual autoregressive~(VAR) models has recently
achieved impressive advances in generation fidelity and inference efficiency.
While control mechanisms have been explored for diffusion models, enabling
precise and flexible control within VAR paradigm remains underexplored. To
bridge this critical gap, in this paper, we introduce ScaleWeaver, a novel
framework designed to achieve high-fidelity, controllable generation upon
advanced VAR models through parameter-efficient fine-tuning. The core module in
ScaleWeaver is the improved MMDiT block with the proposed Reference Attention
module, which efficiently and effectively incorporates conditional information.
Different from MM Attention, the proposed Reference Attention module discards
the unnecessary attention from image$\rightarrow$condition, reducing
computational cost while stabilizing control injection. Besides, it
strategically emphasizes parameter reuse, leveraging the capability of the VAR
backbone itself with a few introduced parameters to process control
information, and equipping a zero-initialized linear projection to ensure that
control signals are incorporated effectively without disrupting the generative
capability of the base model. Extensive experiments show that ScaleWeaver
delivers high-quality generation and precise control while attaining superior
efficiency over diffusion-based methods, making ScaleWeaver a practical and
effective solution for controllable text-to-image generation within the visual
autoregressive paradigm. Code and models will be released.

</details>


### [86] [You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction](https://arxiv.org/abs/2510.14885)
*Logan Lawrence,Oindrila Saha,Megan Wei,Chen Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: 尽管多模态大语言模型（MLLM）的兴起重新激发了对零样本视觉分类的兴趣，但评估自回归模型的自由形式响应仍然是一个持续的挑战。现有方法要么侧重于纯语言任务，要么无法处理超过5个选项的多项选择题（MCQ），而这在细粒度视觉分类（FGVC）任务中至关重要，这类任务的选项数量可达数百甚至数千，且选项之间高度相关。此外，在如此多的选项中，如何将大语言模型（LLM）的选择提取扩展到检索式问题尚不明确，因为计算选项集上的概率成本高昂。本研究提出了一种名为nlg2choice的两阶段简单方法：首先，在约束最少的情况下，让MLLM回答一个开放式问题；然后，利用纯文本约束解码来预测最可能的选项。在检索设置中，我们使用一种带有早期停止机制的方法来计算约束响应的概率，从而显著提高吞吐量。实验结果表明，在七个细粒度视觉数据集上的分类和检索评估中，该方法均有所改进，并且在用户以自然语言方式实现任务的各种场景下都能保持良好的性能。


<details>
  <summary>Details</summary>
Motivation: 评估自回归模型的自由形式响应在零样本视觉分类中仍然是一个挑战，特别是在细粒度视觉分类（FGVC）任务中，选项数量多且高度相关，现有方法难以应对。

Method: 提出了一种名为nlg2choice的两阶段方法：首先，让MLLM回答一个开放式问题；然后，利用纯文本约束解码来预测最可能的选项。在检索设置中，使用带有早期停止机制的方法计算约束响应的概率以提高吞吐量。

Result: 在七个细粒度视觉数据集上的分类和检索评估中，nlg2choice方法均有所改进，并且在用户以自然语言方式实现任务的各种场景下都能保持良好的性能。

Conclusion: nlg2choice是一种有效的方法，能够处理细粒度视觉分类任务中具有大量选项的零样本学习问题，并在分类和检索任务上都取得了显著的改进。

Abstract: Despite the renewed interest in zero-shot visual classification due to the
rise of Multimodal Large Language Models (MLLMs), the problem of evaluating
free-form responses of auto-regressive models remains a persistent challenge.
Most existing works focus on language-only tasks or don't consider Multiple
Choice Questions (MCQs) beyond 5-way options, both of which are critical
capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where
choice counts are in the hundreds to thousands and the choices are highly
related. Furthermore, in this highly multi-way MCQ setting it is not clear how
to extend LLM choice extraction to retrieval-based problems, where computing
probabilities over the choice set is computationally costly. In this work we
investigate nlg2choice, a simple two-stage method which first asks the MLLM an
open-ended question for the task with minimal constraints, then uses text-only
constrained decoding to predict the most likely choice. In retrieval settings,
we compute the probability of the constrained response taking that choice with
an early stopping method to significantly improve throughput. Our results show
improvement over a suite of seven fine-grained visual datasets when evaluating
in terms of classification and retrieval, and show that this performance holds
over the various ways that users of LLMs can implement tasks in natural
language.

</details>


### [87] [Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection](https://arxiv.org/abs/2510.14896)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: 该研究提出了一种利用多模态大语言模型（MLLM）的新型视频异常检测（VAD）框架，通过关注物体随时间变化的活动和交互来检测复杂异常，并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有半监督视频异常检测（VAD）方法在检测涉及物体交互的复杂异常方面存在困难，并且通常缺乏可解释性。

Method: 提出了一种新颖的VAD框架，利用多模态大语言模型（MLLM）。该方法不直接在帧级别进行异常判断，而是提取和解释物体随时间变化的活动和交互。通过向MLLM查询不同时刻物体对的视觉输入，生成名义视频中活动和交互的文本描述。在测试时，通过将这些描述与名义训练视频中的文本描述进行比较来检测异常。

Result: 在基准数据集上的大量实验表明，该方法能有效检测复杂的基于交互的异常，并在没有交互异常的数据集上达到了最先进的性能。

Conclusion: 该方法能够有效检测复杂的基于交互的异常，并提供可解释性。此外，该方法还可以与许多传统的VAD方法结合，以进一步提高其可解释性。

Abstract: Existing semi-supervised video anomaly detection (VAD) methods often struggle
with detecting complex anomalies involving object interactions and generally
lack explainability. To overcome these limitations, we propose a novel VAD
framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous
MLLM-based approaches that make direct anomaly judgments at the frame level,
our method focuses on extracting and interpreting object activity and
interactions over time. By querying an MLLM with visual inputs of object pairs
at different moments, we generate textual descriptions of the activity and
interactions from nominal videos. These textual descriptions serve as a
high-level representation of the activity and interactions of objects in a
video. They are used to detect anomalies during test time by comparing them to
textual descriptions found in nominal training videos. Our approach inherently
provides explainability and can be combined with many traditional VAD methods
to further enhance their interpretability. Extensive experiments on benchmark
datasets demonstrate that our method not only detects complex interaction-based
anomalies effectively but also achieves state-of-the-art performance on
datasets without interaction anomalies.

</details>


### [88] [MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos](https://arxiv.org/abs/2510.14904)
*Gabriel Fiastre,Antoine Yang,Cordelia Schmid*

Main category: cs.CV

TL;DR: 为解决稠密视频对象描述（DVOC）任务中存在的训练策略不 S.O.P. 和标注成本高的问题，本研究提出了一种新的方法。


<details>
  <summary>Details</summary>
Motivation: 以往的 DVOC 方法由于手动标注成本高，采用了分离的训练策略，这可能导致次优性能。

Method: 本研究提出了一种利用先进的 VLM 生成关于时空局部化实体的字幕的方法。通过扩展 LVIS 和 LV-VIS 数据集并加入合成字幕（LVISCap 和 LV-VISCap），训练了一个名为 MaskCaptioner 的端到端模型，该模型能够联合检测、分割、跟踪和描述视频中的对象轨迹。

Result: 在 LVISCap 和 LV-VISCap 数据集上进行预训练后，MaskCaptioner 在 VidSTG、VLN 和 BenSMOT 三个现有基准上取得了 S.O.T.A. 的 DVOC 结果。

Conclusion: 本研究成功地提出了一种端到端的模型 MaskCaptioner，并通过引入新的合成数据集 LVISCap 和 LV-VISCap，解决了 DVOC 任务中的训练和标注难题，并在多个基准上取得了 S.O.T.A. 的性能。

Abstract: Dense Video Object Captioning (DVOC) is the task of jointly detecting,
tracking, and captioning object trajectories in a video, requiring the ability
to understand spatio-temporal details and describe them in natural language.
Due to the complexity of the task and the high cost associated with manual
annotation, previous approaches resort to disjoint training strategies,
potentially leading to suboptimal performance. To circumvent this issue, we
propose to generate captions about spatio-temporally localized entities
leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets
with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an
end-to-end model capable of jointly detecting, segmenting, tracking and
captioning object trajectories. Moreover, with pretraining on LVISCap and
LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three
existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are
available at https://www.gabriel.fiastre.fr/maskcaptioner/.

</details>


### [89] [3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation](https://arxiv.org/abs/2510.14945)
*JoungBin Lee,Jaewoo Jung,Jisang Han,Takuya Narihira,Kazumi Fukuda,Junyoung Seo,Sunghwan Hong,Yuki Mitsufuji,Seungryong Kim*

Main category: cs.CV

TL;DR: 3DScenePrompt框架通过双时空条件和3D场景记忆来生成任意长度的视频片段，实现精确相机控制和场景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成方法在处理任意长度输入、精确相机控制和保持场景一致性方面存在局限性，尤其是在生成超出时间边界的内容时。

Method: 提出3DScenePrompt框架，采用双时空条件（时间相邻帧和空间相邻内容）来生成视频。引入3D场景记忆，通过动态SLAM和动态掩码策略提取静态场景几何，以解决直接使用空间相邻帧导致动态元素错误保留的问题。该记忆可被投射到任意视角，提供一致的3D空间提示。

Result: 实验证明，3DScenePrompt在场景一致性、相机可控性和生成质量方面显著优于现有方法。

Conclusion: 3DScenePrompt框架能够有效实现长范围空间连贯性和精确相机控制，同时保持计算效率和运动真实感，解决了现有方法的不足。

Abstract: We present 3DScenePrompt, a framework that generates the next video chunk
from arbitrary-length input while enabling precise camera control and
preserving scene consistency. Unlike methods conditioned on a single image or a
short clip, we employ dual spatio-temporal conditioning that reformulates
context-view referencing across the input video. Our approach conditions on
both temporally adjacent frames for motion continuity and spatially adjacent
content for scene consistency. However, when generating beyond temporal
boundaries, directly using spatially adjacent frames would incorrectly preserve
dynamic elements from the past. We address this by introducing a 3D scene
memory that represents exclusively the static geometry extracted from the
entire input video. To construct this memory, we leverage dynamic SLAM with our
newly introduced dynamic masking strategy that explicitly separates static
scene geometry from moving elements. The static scene representation can then
be projected to any target viewpoint, providing geometrically consistent warped
views that serve as strong 3D spatial prompts while allowing dynamic regions to
evolve naturally from temporal context. This enables our model to maintain
long-range spatial coherence and precise camera control without sacrificing
computational efficiency or motion realism. Extensive experiments demonstrate
that our framework significantly outperforms existing methods in scene
consistency, camera controllability, and generation quality. Project page :
https://cvlab-kaist.github.io/3DScenePrompt/

</details>


### [90] [OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression](https://arxiv.org/abs/2510.14954)
*Zhe Li,Weihao Yuan,Weichao Shen,Siyu Zhu,Zilong Dong,Chang Xu*

Main category: cs.CV

TL;DR: 提出一种连续掩码自回归运动Transformer，并结合DiT结构，以实现多模态人体运动生成，并在文本到运动、语音到手势和音乐到舞蹈等任务上取得了优于先前方法的结果。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在运动生成机制和多模态融合方面的不足，以实现有效且统一的多模态人体运动生成。

Method: 提出一种连续掩码自回归运动Transformer，包含因果注意力、门控线性注意力、RMSNorm模块，并结合DiT结构进行条件扩散，利用AdaLN和交叉注意力融合文本、语音和音乐信号。

Result: 在文本到运动、语音到手势和音乐到舞蹈等任务上，实验结果表明所提出的框架在所有模态上都优于先前的方法。

Conclusion: 所提出的框架能够有效地进行多模态人体运动生成，并在各种条件下实现先进的性能。

Abstract: Whole-body multi-modal human motion generation poses two primary challenges:
creating an effective motion generation mechanism and integrating various
modalities, such as text, speech, and music, into a cohesive framework. Unlike
previous methods that usually employ discrete masked modeling or autoregressive
modeling, we develop a continuous masked autoregressive motion transformer,
where a causal attention is performed considering the sequential nature within
the human motion. Within this transformer, we introduce a gated linear
attention and an RMSNorm module, which drive the transformer to pay attention
to the key actions and suppress the instability caused by either the abnormal
movements or the heterogeneous distributions within multi-modalities. To
further enhance both the motion generation and the multimodal generalization,
we employ the DiT structure to diffuse the conditions from the transformer
towards the targets. To fuse different modalities, AdaLN and cross-attention
are leveraged to inject the text, speech, and music signals. Experimental
results demonstrate that our framework outperforms previous methods across all
modalities, including text-to-motion, speech-to-gesture, and music-to-dance.
The code of our method will be made public.

</details>


### [91] [RealDPO: Real or Not Real, that is the Preference](https://arxiv.org/abs/2510.14955)
*Guo Cheng,Danni Yang,Ziqi Huang,Jianlou Si,Chenyang Si,Ziwei Liu*

Main category: cs.CV

TL;DR: RealDPO是一种新的范式，利用真实世界的视频作为正样本进行偏好学习，以实现更精确的动作合成，并提出RealAction-5K数据集来支持训练。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型在生成复杂、自然、流畅和上下文一致的运动方面存在挑战，这限制了它们的实际应用。

Method: RealDPO采用直接偏好优化（DPO）和定制的损失函数，通过将真实世界的视频与错误的模型输出进行对比，实现迭代式自我修正，从而提高运动的真实感。RealAction-5K数据集包含了高质量的、具有丰富运动细节的人类日常活动视频。

Result: RealDPO在视频质量、文本对齐和运动真实感方面显著优于最先进的模型和现有的偏好优化技术。

Conclusion: RealDPO通过利用真实数据进行偏好学习，能够更有效地生成高质量、真实感强的复杂运动视频。

Abstract: Video generative models have recently achieved notable advancements in
synthesis quality. However, generating complex motions remains a critical
challenge, as existing models often struggle to produce natural, smooth, and
contextually consistent movements. This gap between generated and real-world
motions limits their practical applicability. To address this issue, we
introduce RealDPO, a novel alignment paradigm that leverages real-world data as
positive samples for preference learning, enabling more accurate motion
synthesis. Unlike traditional supervised fine-tuning (SFT), which offers
limited corrective feedback, RealDPO employs Direct Preference Optimization
(DPO) with a tailored loss function to enhance motion realism. By contrasting
real-world videos with erroneous model outputs, RealDPO enables iterative
self-correction, progressively refining motion quality. To support
post-training in complex motion synthesis, we propose RealAction-5K, a curated
dataset of high-quality videos capturing human daily activities with rich and
precise motion details. Extensive experiments demonstrate that RealDPO
significantly improves video quality, text alignment, and motion realism
compared to state-of-the-art models and existing preference optimization
techniques.

</details>


### [92] [MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2510.14958)
*Weikang Shi,Aldrich Yu,Rongyao Fang,Houxing Ren,Ke Wang,Aojun Zhou,Changyao Tian,Xinyu Fu,Yuxuan Hu,Zimu Lu,Linjiang Huang,Si Liu,Rui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: LLMs在数学领域（尤其是几何）存在不足，因其需要视觉辅助。本文提出的MathCanvas框架，通过预训练和指令微调，使LMM具备内在的视觉推理能力，并在MathCanvas-Bench基准上取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉链式思考方法在数学领域受限于外部工具或无法生成高质量、适时的图表，本文旨在解决LLMs在数学推理中对视觉辅助的依赖和不足。

Method: 提出MathCanvas框架，包含两个阶段：1. 视觉操作阶段：预训练模型在MathCanvas-Imagen（10M图文对）和MathCanvas-Edit（5.2M编辑轨迹）数据集上，以掌握图表生成和编辑。2. 策略性视觉辅助推理阶段：在MathCanvas-Instruct（219K视觉-文本推理路径）数据集上进行微调，学习何时及如何使用视觉辅助。并引入MathCanvas-Bench（3K问题）进行评估。

Result: 在MathCanvas-Bench基准上，所提出的BAGEL-Canvas模型比现有LMM基线提高了86%，并在其他公开数学基准上也表现出良好的泛化能力。

Conclusion: MathCanvas框架提供了一个完整的工具集（框架、数据集、基准），能够解锁LMM中复杂、类似人类的视觉辅助推理能力。

Abstract: While Large Language Models (LLMs) have excelled in textual reasoning, they
struggle with mathematical domains like geometry that intrinsically rely on
visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often
limited by rigid external tools or fail to generate the high-fidelity,
strategically-timed diagrams necessary for complex problem-solving. To bridge
this gap, we introduce MathCanvas, a comprehensive framework designed to endow
unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for
mathematics. Our approach consists of two phases. First, a Visual Manipulation
stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M
caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing
trajectories (MathCanvas-Edit), to master diagram generation and editing.
Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on
MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual
reasoning paths, teaching it when and how to leverage visual aids. To
facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging
benchmark with 3K problems that require models to produce interleaved
visual-textual solutions. Our model, BAGEL-Canvas, trained under this
framework, achieves an 86% relative improvement over strong LMM baselines on
MathCanvas-Bench, demonstrating excellent generalization to other public math
benchmarks. Our work provides a complete toolkit-framework, datasets, and
benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project
Page: https://mathcanvas.github.io/

</details>


### [93] [C4D: 4D Made from 3D through Dual Correspondences](https://arxiv.org/abs/2510.14960)
*Shizun Wang,Zhenxiang Jiang,Xingyi Yang,Xinchao Wang*

Main category: cs.CV

TL;DR: C4D是一个新框架，通过结合短期光流和长期点追踪来解决单目视频中动态场景的4D重建问题，有效分离动静物体，实现完整的4D重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于点图的三维重建方法难以直接应用于动态场景，因为运动物体会破坏多视图几何约束，导致重建不准确。现有的方法在处理静态场景方面取得了进展，但在动态场景方面仍需改进。

Method: C4D框架通过预测点图，并捕捉短期光流和长期点追踪两种类型的对应关系。它训练了一个动态感知点追踪器来提供移动性信息，从而实现运动掩码的估计，将运动物体与静态背景分离。此外，该框架引入了一组动态场景优化目标，用于恢复每帧的三维几何和相机参数。同时，通过对应关系将二维轨迹提升为平滑的三维轨迹，实现完整的四维重建。

Result: 实验证明，C4D框架能够实现完整的四维重建，并在深度估计、相机姿态估计和点追踪等多个下游任务中表现出强大的性能。

Conclusion: C4D框架通过利用时间对应关系，将现有的三维重建方法扩展到四维，能够有效地处理动态场景，实现完整的动态场景四维重建。

Abstract: Recovering 4D from monocular video, which jointly estimates dynamic geometry
and camera poses, is an inevitably challenging problem. While recent
pointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great
progress in reconstructing static scenes, directly applying them to dynamic
scenes leads to inaccurate results. This discrepancy arises because moving
objects violate multi-view geometric constraints, disrupting the
reconstruction. To address this, we introduce C4D, a framework that leverages
temporal Correspondences to extend existing 3D reconstruction formulation to
4D. Specifically, apart from predicting pointmaps, C4D captures two types of
correspondences: short-term optical flow and long-term point tracking. We train
a dynamic-aware point tracker that provides additional mobility information,
facilitating the estimation of motion masks to separate moving elements from
the static background, thus offering more reliable guidance for dynamic scenes.
Furthermore, we introduce a set of dynamic scene optimization objectives to
recover per-frame 3D geometry and camera parameters. Simultaneously, the
correspondences lift 2D trajectories into smooth 3D trajectories, enabling
fully integrated 4D reconstruction. Experiments show that our framework
achieves complete 4D recovery and demonstrates strong performance across
multiple downstream tasks, including depth estimation, camera pose estimation,
and point tracking. Project Page: https://littlepure2333.github.io/C4D

</details>


### [94] [RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention Diffusion](https://arxiv.org/abs/2510.14962)
*Thao Nguyen,Jiaqi Ma,Fahad Shahbaz Khan,Souhaib Ben Taieb,Salman Khan*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于注意力机制的扩散模型，用于改进降水临近预报的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的降水临近预报方法在处理大气动力学的复杂性时存在可扩展性问题，需要额外的自编码器或计算成本高昂且缺乏长程依赖建模能力。

Method: 提出了一种将Token-wise Attention机制集成到U-Net扩散模型和时空编码器中的新方法，以动态捕捉多尺度空间交互和时间演化，无需额外的自编码器且计算成本较低。

Result: 实验结果表明，所提出的方法在处理不同数据集时，在局部保真度、泛化能力和鲁棒性方面显著优于现有最先进的方法。

Conclusion: 所提出的基于Token-wise Attention的扩散模型能够有效解决现有方法的局限性，在降水临近预报任务上取得更好的性能。

Abstract: Precipitation nowcasting, predicting future radar echo sequences from current
observations, is a critical yet challenging task due to the inherently chaotic
and tightly coupled spatio-temporal dynamics of the atmosphere. While recent
advances in diffusion-based models attempt to capture both large-scale motion
and fine-grained stochastic variability, they often suffer from scalability
issues: latent-space approaches require a separately trained autoencoder,
adding complexity and limiting generalization, while pixel-space approaches are
computationally intensive and often omit attention mechanisms, reducing their
ability to model long-range spatio-temporal dependencies. To address these
limitations, we propose a Token-wise Attention integrated into not only the
U-Net diffusion model but also the spatio-temporal encoder that dynamically
captures multi-scale spatial interactions and temporal evolution. Unlike prior
approaches, our method natively integrates attention into the architecture
without incurring the high resource cost typical of pixel-space diffusion,
thereby eliminating the need for separate latent modules. Our extensive
experiments and visual evaluations across diverse datasets demonstrate that the
proposed method significantly outperforms state-of-the-art approaches, yielding
superior local fidelity, generalization, and robustness in complex
precipitation forecasting scenarios.

</details>


### [95] [ChangingGrounding: 3D Visual Grounding in Changing Scenes](https://arxiv.org/abs/2510.14965)
*Miao Hu,Zhiwei Huang,Tai Wang,Jiangmiao Pang,Dahua Lin,Nanning Zheng,Runsen Xu*

Main category: cs.CV

TL;DR: 该研究提出了ChangingGrounding基准和Mem-ChangingGrounder方法，用于解决机器人实时3D物体定位问题，即使在场景不断变化的情况下也能通过利用历史观察和有效探索来实现精确的目标框定位。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉地面定位方法假设场景是静态的且点云是最新的，这不适用于真实世界中不断变化的场景，并导致成本高昂的重新扫描。因此，需要一种能够利用过去观察、按需探索并能在动态场景中精确定位的3D视觉地面定位方法。

Method: 提出了一种名为ChangingGrounding的新基准，用于评估机器人在动态场景中利用过去观察和进行有效探索的能力。同时，提出了一种名为Mem-ChangingGrounder的零样本方法，该方法结合了跨模态检索和轻量级多视图融合技术，能够识别目标物体类型、利用检索到的记忆指导探索、在必要时回退，并通过融合多视图扫描证据来获得精确的目标边界框。

Result: Mem-ChangingGrounder在ChangingGrounding基准上取得了最高的定位准确性，并显著降低了探索成本。

Conclusion: ChangingGrounding基准和Mem-ChangingGrounder方法的提出，有望推动3D视觉地面定位研究向实际应用和以记忆为中心的方向发展。

Abstract: Real-world robots localize objects from natural-language instructions while
scenes around them keep changing. Yet most of the existing 3D visual grounding
(3DVG) method still assumes a reconstructed and up-to-date point cloud, an
assumption that forces costly re-scans and hinders deployment. We argue that
3DVG should be formulated as an active, memory-driven problem, and we introduce
ChangingGrounding, the first benchmark that explicitly measures how well an
agent can exploit past observations, explore only where needed, and still
deliver precise 3D boxes in changing scenes. To set a strong reference point,
we also propose Mem-ChangingGrounder, a zero-shot method for this task that
marries cross-modal retrieval with lightweight multi-view fusion: it identifies
the object type implied by the query, retrieves relevant memories to guide
actions, then explores the target efficiently in the scene, falls back when
previous operations are invalid, performs multi-view scanning of the target,
and projects the fused evidence from multi-view scans to get accurate object
bounding boxes. We evaluate different baselines on ChangingGrounding, and our
Mem-ChangingGrounder achieves the highest localization accuracy while greatly
reducing exploration cost. We hope this benchmark and method catalyze a shift
toward practical, memory-centric 3DVG research for real-world applications.
Project page: https://hm123450.github.io/CGB/ .

</details>


### [96] [WithAnyone: Towards Controllable and ID Consistent Image Generation](https://arxiv.org/abs/2510.14975)
*Hengyuan Xu,Wei Cheng,Peng Xing,Yixiao Fang,Shuhan Wu,Rui Wang,Xianfang Zeng,Daxin Jiang,Gang Yu,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为WithAnyone的新方法，用于解决文本到图像生成中的身份复制粘贴问题，通过构建新数据集、引入评估基准和创新的训练范式，提高了身份生成的多样性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在保持身份一致性方面存在局限，常因缺乏大规模多图像身份数据集而采用基于重建的训练，导致生成图像过度相似（复制粘贴），无法自然表现身份在不同姿态、表情和光照下的变化。

Method: 1. 构建了大规模配对数据集MultiID-2M，包含同一身份的多样化图像，以适应多人场景。 2. 引入了一个新的基准，用于量化复制粘贴伪影以及身份保真度与多样性之间的权衡。 3. 提出了一种新的训练范式，结合了对比身份损失，利用配对数据来平衡保真度和多样性。 4. 开发了基于扩散模型的WithAnyone，以减轻复制粘贴问题并保持高身份相似性。

Result: WithAnyone模型显著减少了复制粘贴伪影，提高了对姿态和表情的控制能力，并保持了高质量的感知效果。用户研究证实，该方法在实现高身份保真度的同时，能够生成富有表现力的、可控的图像。

Conclusion: 研究提出的MultiID-2M数据集、评估基准和WithAnyone模型及其训练范式，有效解决了文本到图像生成中的身份复制粘贴问题，实现了身份保真度、多样性和可控性的平衡，为该领域的研究和应用提供了重要进展。

Abstract: Identity-consistent generation has become an important focus in text-to-image
research, with recent models achieving notable success in producing images
aligned with a reference identity. Yet, the scarcity of large-scale paired
datasets containing multiple images of the same individual forces most
approaches to adopt reconstruction-based training. This reliance often leads to
a failure mode we term copy-paste, where the model directly replicates the
reference face rather than preserving identity across natural variations in
pose, expression, or lighting. Such over-similarity undermines controllability
and limits the expressive power of generation. To address these limitations, we
(1) construct a large-scale paired dataset MultiID-2M, tailored for
multi-person scenarios, providing diverse references for each identity; (2)
introduce a benchmark that quantifies both copy-paste artifacts and the
trade-off between identity fidelity and variation; and (3) propose a novel
training paradigm with a contrastive identity loss that leverages paired data
to balance fidelity with diversity. These contributions culminate in
WithAnyone, a diffusion-based model that effectively mitigates copy-paste while
preserving high identity similarity. Extensive qualitative and quantitative
experiments demonstrate that WithAnyone significantly reduces copy-paste
artifacts, improves controllability over pose and expression, and maintains
strong perceptual quality. User studies further validate that our method
achieves high identity fidelity while enabling expressive controllable
generation.

</details>


### [97] [Terra: Explorable Native 3D World Model with Point Latents](https://arxiv.org/abs/2510.14977)
*Yuanhui Huang,Weiliang Chen,Wenzhao Zheng,Xin Tao,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Terra是一个原生的3D世界模型，它在3D潜在空间中表示和生成可探索的环境，解决了现有世界模型依赖于像素对齐表示而忽略3D物理世界的问题。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型大多依赖于像素对齐表示，忽略了物理世界的3D本质，这可能影响3D一致性并降低建模效率。本文提出Terra，一个原生的3D世界模型，旨在解决这些问题。

Method: Terra模型提出了一种新颖的点到高斯变分自编码器（P2G-VAE），用于将3D输入编码为潜在点表示，并将其解码为3D高斯图元以同时模拟几何和外观。此外，还引入了稀疏点流匹配网络（SPFlow）来生成潜在点表示，并进行位置和特征去噪。

Result: Terra实现了精确的多视角一致性，支持从任何视点进行灵活渲染，并且通过在点潜在空间中进行渐进式生成，实现了可探索的世界建模。在ScanNet v2数据集上的实验表明，Terra在重建和生成方面均达到了最先进的性能，并具有高度的3D一致性。

Conclusion: Terra是一个原生的3D世界模型，通过其P2G-VAE和SPFlow网络，能够实现精确的多视角一致性、灵活的渲染和可探索的世界建模，并在室内场景重建和生成任务上取得了领先的性能。

Abstract: World models have garnered increasing attention for comprehensive modeling of
the real world. However, most existing methods still rely on pixel-aligned
representations as the basis for world evolution, neglecting the inherent 3D
nature of the physical world. This could undermine the 3D consistency and
diminish the modeling efficiency of world models. In this paper, we present
Terra, a native 3D world model that represents and generates explorable
environments in an intrinsic 3D latent space. Specifically, we propose a novel
point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into
a latent point representation, which is subsequently decoded as 3D Gaussian
primitives to jointly model geometry and appearance. We then introduce a sparse
point flow matching network (SPFlow) for generating the latent point
representation, which simultaneously denoises the positions and features of the
point latents. Our Terra enables exact multi-view consistency with native 3D
representation and architecture, and supports flexible rendering from any
viewpoint with only a single generation process. Furthermore, Terra achieves
explorable world modeling through progressive generation in the point latent
space. We conduct extensive experiments on the challenging indoor scenes from
ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction
and generation with high 3D consistency.

</details>


### [98] [Learning an Image Editing Model without Image Editing Pairs](https://arxiv.org/abs/2510.14978)
*Nupur Kumari,Sheng-Yu Wang,Nanxuan Zhao,Yotam Nitzan,Yuheng Li,Krishna Kumar Singh,Richard Zhang,Eli Shechtman,Jun-Yan Zhu,Xun Huang*

Main category: cs.CV

TL;DR: 本研究提出了一种无需配对数据即可训练图像编辑模型的新范式，通过利用视觉语言模型（VLM）的反馈进行直接优化，并结合分布匹配损失（DMD）来确保图像保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的图像编辑模型依赖于大量配对数据进行监督微调，而配对数据的收集成本高昂。利用合成数据又会传播预训练模型的伪影。因此，需要一种不依赖配对数据的方法。

Method: 本研究提出了一种新的训练范式，通过在训练过程中展开几步扩散模型，并利用视觉语言模型（VLM）的反馈来进行直接优化。VLM评估编辑是否符合指令且保持不变的内容，并提供直接梯度用于端到端优化。同时，引入分布匹配损失（DMD）来约束生成图像保持在预训练模型学习的图像流形内。

Result: 在标准基准测试中，本方法在无需配对数据的情况下，在几步设置下表现与使用大量监督配对数据训练的各种图像编辑扩散模型相当，并且优于基于强化学习（RL）的技术（如Flow-GRPO）。

Conclusion: 本研究成功提出了一种无需配对数据即可训练图像编辑模型的有效方法，在保持图像编辑效果的同时，解决了配对数据难以获取的瓶颈问题，并优于现有的一些先进方法。

Abstract: Recent image editing models have achieved impressive results while following
natural language editing instructions, but they rely on supervised fine-tuning
with large datasets of input-target pairs. This is a critical bottleneck, as
such naturally occurring pairs are hard to curate at scale. Current workarounds
use synthetic training pairs that leverage the zero-shot capabilities of
existing models. However, this can propagate and magnify the artifacts of the
pretrained model into the final trained model. In this work, we present a new
training paradigm that eliminates the need for paired data entirely. Our
approach directly optimizes a few-step diffusion model by unrolling it during
training and leveraging feedback from vision-language models (VLMs). For each
input and editing instruction, the VLM evaluates if an edit follows the
instruction and preserves unchanged content, providing direct gradients for
end-to-end optimization. To ensure visual fidelity, we incorporate distribution
matching loss (DMD), which constrains generated images to remain within the
image manifold learned by pretrained models. We evaluate our method on standard
benchmarks and include an extensive ablation study. Without any paired data,
our method performs on par with various image editing diffusion models trained
on extensive supervised paired data, under the few-step setting. Given the same
VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.

</details>


### [99] [From Pixels to Words -- Towards Native Vision-Language Primitives at Scale](https://arxiv.org/abs/2510.14979)
*Haiwen Diao,Mingxuan Li,Silei Wu,Linjun Dai,Xiaohua Wang,Hanming Deng,Lewei Lu,Dahua Lin,Ziwei Liu*

Main category: cs.CV

TL;DR:  本文提出了NEO，一种新的原生视觉语言模型（VLM）系列，旨在克服现有原生VLM的局限性，并通过提供可扩展、经济高效的组件来促进该领域的民主化和加速进展。


<details>
  <summary>Details</summary>
Motivation: 原生VLM在模型架构和训练范式方面不断发展，但面临两大挑战：1. 原生VLM与模块化VLM在根本约束上的差异以及克服这些障碍的潜力；2. 如何使原生VLM的研究更易于访问和民主化，以加速该领域的进展。

Method: 本文提出了三项构建原生VLM的指导原则：（1）有效对齐像素和词语表示；（2）无缝整合原先分离的视觉和语言模块的优势；（3）内在化支持统一视觉-语言编码、对齐和推理的跨模态特性。基于这些原则，研究团队推出了NEO，一个从头开始构建的}(\textbf{native VLM}) 系列。

Result: NEO在仅使用390M图像-文本示例的情况下，有效地从头开始开发了视觉感知能力，并在一个由精细构建的原语构成的密集、单一模型中减轻了视觉-语言冲突。NEO在各种真实场景中表现与顶级模块化模型相当。

Conclusion: NEO被定位为可扩展且强大的原生VLM的基石，并配有一套丰富的可重用组件，旨在促进成本效益高且可扩展的生态系统。代码和模型已公开。

Abstract: The edifice of native Vision-Language Models (VLMs) has emerged as a rising
contender to typical modular VLMs, shaped by evolving model architectures and
training paradigms. Yet, two lingering clouds cast shadows over its widespread
exploration and promotion: (-) What fundamental constraints set native VLMs
apart from modular ones, and to what extent can these barriers be overcome? (-)
How to make research in native VLMs more accessible and democratized, thereby
accelerating progress in the field. In this paper, we clarify these challenges
and outline guiding principles for constructing native VLMs. Specifically, one
native VLM primitive should: (i) effectively align pixel and word
representations within a shared semantic space; (ii) seamlessly integrate the
strengths of formerly separate vision and language modules; (iii) inherently
embody various cross-modal properties that support unified vision-language
encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of
native VLMs built from first principles, capable of rivaling top-tier modular
counterparts across diverse real-world scenarios. With only 390M image-text
examples, NEO efficiently develops visual perception from scratch while
mitigating vision-language conflicts inside a dense and monolithic model
crafted from our elaborate primitives. We position NEO as a cornerstone for
scalable and powerful native VLMs, paired with a rich set of reusable
components that foster a cost-effective and extensible ecosystem. Our code and
models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.

</details>


### [100] [Coupled Diffusion Sampling for Training-Free Multi-View Image Editing](https://arxiv.org/abs/2510.14981)
*Hadi Alzayer,Yunzhi Zhang,Chen Geng,Jia-Bin Huang,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出一种推理时扩散采样方法，通过耦合扩散采样，在不进行显式3D优化的前提下，实现多视角一致的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的2D图像编辑模型在处理3D场景的多视角图像时，无法保证编辑结果在不同视角之间的一致性。而现有的解决方式，如基于显式3D表示的优化方法，存在耗时长、在视角稀疏时不稳定等问题。

Method: 提出一种隐式3D正则化方法，通过耦合扩散采样，同时从多视角图像分布和2D编辑图像分布中进行采样，并利用耦合项强制生成图像满足多视角一致性，从而在推理时实现多视角一致的图像编辑。

Result: 该框架在三个不同的多视角图像编辑任务上进行了验证，证明了其在不同模型架构上的有效性和通用性。

Conclusion: 所提出的耦合扩散采样方法是一种简单且有效的技术，可以作为多视角一致性编辑的通用解决方案，并且避免了传统方法中耗时和不稳定的问题。

Abstract: We present an inference-time diffusion sampling method to perform multi-view
consistent image editing using pre-trained 2D image editing models. These
models can independently produce high-quality edits for each image in a set of
multi-view images of a 3D scene or object, but they do not maintain consistency
across views. Existing approaches typically address this by optimizing over
explicit 3D representations, but they suffer from a lengthy optimization
process and instability under sparse view settings. We propose an implicit 3D
regularization approach by constraining the generated 2D image sequences to
adhere to a pre-trained multi-view image distribution. This is achieved through
coupled diffusion sampling, a simple diffusion sampling technique that
concurrently samples two trajectories from both a multi-view image distribution
and a 2D edited image distribution, using a coupling term to enforce the
multi-view consistency among the generated images. We validate the
effectiveness and generality of this framework on three distinct multi-view
image editing tasks, demonstrating its applicability across various model
architectures and highlighting its potential as a general solution for
multi-view consistent editing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [101] [Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL](https://arxiv.org/abs/2510.13827)
*Ashish Kattamuri,Ishita Prasad,Meetu Malhotra,Arpita Vats,Rahul Raja,Albert Lie*

Main category: cs.CL

TL;DR: 当前Text-to-SQL方法在评估时仅关注可执行查询，忽略了查询的语义对齐和执行结果的正确性。跨语言场景下，即使是执行准确率在非英语语言上的表现也会显著下降。本研究提出了一个结合了Group Relative Policy Optimization (GRPO)和多语言对比奖励信号的新框架，以增强跨语言Text-to-SQL系统的任务效率和语义准确性。该方法通过结合基于语义相似度的奖励信号，教会模型更好地匹配SQL生成与用户意图。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL方法在评估时仅关注可执行查询，忽略了查询的语义对齐和执行结果的正确性。跨语言场景下，即使是执行准确率在非英语语言上的表现也会显著下降。

Method: 提出一个结合了Group Relative Policy Optimization (GRPO)和多语言对比奖励信号的新框架，以增强跨语言Text-to-SQL系统的任务效率和语义准确性。该方法通过结合基于语义相似度的奖励信号，教会模型更好地匹配SQL生成与用户意图。

Result: 在七种语言的MultiSpider数据集上，使用GRPO微调LLaMA-3-3B模型，执行准确率最高可达87.4%（比零样本提高了26个百分点），语义准确率最高可达52.29%（提高了32.86个百分点）。加入对比奖励信号后，平均语义准确率进一步提高到59.14%（提高了6.85个百分点，越南语最高提高10个百分点）。此外，使用对比奖励信号微调的3B LLaMA模型在执行准确率上超过了更大的8B LLaMA模型（88.86% vs 81.43%），并且在语义准确率上与其接近（59.14% vs 68.57%），而训练样本仅为3000个。

Conclusion: 本研究提出的对比奖励方法能够显著提高Text-to-SQL系统的性能，尤其是在跨语言场景下，并且不需要大规模的训练数据集。

Abstract: Current Text-to-SQL methods are evaluated and only focused on executable
queries, overlooking the semantic alignment challenge -- both in terms of the
semantic meaning of the query and the correctness of the execution results.
Even execution accuracy itself shows significant drops when moving from English
to other languages, with an average decline of 6 percentage points across
non-English languages. We address these challenges by presenting a new
framework that combines Group Relative Policy Optimization (GRPO) within a
multilingual contrastive reward signal to enhance both task efficiency and
semantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method
teaches models to obtain better correspondence between SQL generation and user
intent by combining a reward signal based on semantic similarity. On the
seven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO
improved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and
semantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive
reward signal in the GRPO framework further improved the average semantic
accuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our
experiments showcase that a smaller, parameter-efficient 3B LLaMA model
fine-tuned with our contrastive reward signal outperforms a much larger
zero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from
81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly
matches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using
just 3,000 reinforcement learning training examples. These results demonstrate
how we can improve the performance of Text-to-SQL systems with contrastive
rewards for directed semantic alignment, without requiring large-scale training
datasets.

</details>


### [102] [From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening](https://arxiv.org/abs/2510.13828)
*Ratna Kandala,Akshata Kishore Moharir,Divya Arvinda Nayak*

Main category: cs.CL

TL;DR: XAI在心理健康筛查(MHS)中潜力巨大，但存在“从实验室到临床”的差距。现有XAI技术（如SHAP、LIME）的输出缺乏临床相关性和可操作性。本文提出生成式操作框架(Generative Operational Framework)，利用大型语言模型(LLM)作为翻译引擎，将XAI的原始技术输出与临床指南结合（通过RAG），自动生成人类可读、有证据支持的临床叙述，以解决技术透明度与人类效用之间的脱节问题。


<details>
  <summary>Details</summary>
Motivation: 现有XAI技术虽然在技术上忠实，但无法提供临床医生可使用或患者可理解的临床相关、可操作的见解，导致“从实验室到临床”的差距，阻碍了AI在临床实践中的应用。

Method: 提出生成式操作框架（Generative Operational Framework），一个新颖的系统架构，利用大型语言模型（LLM）作为核心翻译引擎。该框架能够接收来自不同XAI工具的原始技术输出，并通过检索增强生成（RAG）技术与临床指南相结合，自动生成人类可读、有证据支持的临床叙述。

Result: 该框架通过系统性分析其整合的组件，追溯了从内在模型到生成式XAI的演变过程。证明了该框架如何直接解决关键的操作障碍，包括工作流程整合、偏见缓解和面向特定利益相关者的沟通。

Conclusion: 该框架为将XAI从生成孤立的数据点推进到在临床实践中提供集成、可操作和可信赖的AI提供了战略路线图，解决了技术透明度与人类效用之间的脱节问题。

Abstract: Explainable Artificial Intelligence (XAI) has been presented as the critical
component for unlocking the potential of machine learning in mental health
screening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI
techniques, such as SHAP and LIME, excel at producing technically faithful
outputs such as feature importance scores, but fail to deliver clinically
relevant, actionable insights that can be used by clinicians or understood by
patients. This disconnect between technical transparency and human utility is
the primary barrier to real-world adoption. This paper argues that this gap is
a translation problem and proposes the Generative Operational Framework, a
novel system architecture that leverages Large Language Models (LLMs) as a
central translation engine. This framework is designed to ingest the raw,
technical outputs from diverse XAI tools and synthesize them with clinical
guidelines (via RAG) to automatically generate human-readable, evidence-backed
clinical narratives. To justify our solution, we provide a systematic analysis
of the components it integrates, tracing the evolution from intrinsic models to
generative XAI. We demonstrate how this framework directly addresses key
operational barriers, including workflow integration, bias mitigation, and
stakeholder-specific communication. This paper also provides a strategic
roadmap for moving the field beyond the generation of isolated data points
toward the delivery of integrated, actionable, and trustworthy AI in clinical
practice.

</details>


### [103] [Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection](https://arxiv.org/abs/2510.13837)
*Weibin Cai,Reza Zafarani*

Main category: cs.CL

TL;DR: 现有的hate speech检测方法在处理现实世界的复杂性方面存在不足，例如训练标签的偏差以及不同文化背景下对hate speech的个体化解读。本文提出了一种文化感知框架，通过构建个体hate子空间来解决这些挑战，并能在分类性能上实现平均1.05%的提升。


<details>
  <summary>Details</summary>
Motivation: 现有的hate speech检测方法未能充分考虑训练标签的偏差以及不同文化背景下个体对hate speech定义的差异性等现实世界的复杂性。

Method: 提出了一种文化感知框架，该框架构建了个体hate子空间。为了缓解数据稀疏性，我们对文化属性的组合进行了建模。针对文化纠缠和标签歧义问题，我们利用标签传播来捕捉每个组合的独特特征，最终形成个体hate子空间，以增强分类性能。

Result: 实验表明，该方法在所有指标上的平均表现比现有技术水平 (state-of-the-art) 提升了1.05%。

Conclusion: 本文提出的文化感知框架通过构建个体hate子空间，有效解决了hate speech检测中的数据稀疏、文化纠缠和标签歧义等问题，并取得了优于现有方法的性能。

Abstract: Hate speech detection has been extensively studied, yet existing methods
often overlook a real-world complexity: training labels are biased, and
interpretations of what is considered hate vary across individuals with
different cultural backgrounds. We first analyze these challenges, including
data sparsity, cultural entanglement, and ambiguous labeling. To address them,
we propose a culture-aware framework that constructs individuals' hate
subspaces. To alleviate data sparsity, we model combinations of cultural
attributes. For cultural entanglement and ambiguous labels, we use label
propagation to capture distinctive features of each combination. Finally,
individual hate subspaces, which in turn can further enhance classification
performance. Experiments show our method outperforms state-of-the-art by 1.05\%
on average across all metrics.

</details>


### [104] [A Linguistics-Aware LLM Watermarking via Syntactic Predictability](https://arxiv.org/abs/2510.13829)
*Shinwoo Park,Hyejin Park,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: STELA是一个新颖的框架，通过词性n-gram模型来衡量语言的自由度，动态调整水印强度，以平衡文本质量和检测鲁棒性，并且无需访问模型logits即可进行公共验证。


<details>
  <summary>Details</summary>
Motivation: 现有的水印方法在文本质量和检测鲁棒性之间难以平衡，并且通常需要访问模型logits才能进行检测，阻碍了公共验证。

Method: STELA利用词性n-gram模型来估计语言的自由度，动态调整水印强度。在语法约束强的语境下减弱水印以保证质量，在语言更灵活的语境下增强水印以提高可检测性。

Result: 在英语、中文和韩语等多种语言上的实验表明，STELA的检测鲁棒性优于现有方法。

Conclusion: STELA通过基于语言自由度的动态水印方法，解决了现有水印方法的局限性，实现了无需模型logits即可进行公共验证，并提高了检测鲁棒性。

Abstract: As large language models (LLMs) continue to advance rapidly, reliable
governance tools have become critical. Publicly verifiable watermarking is
particularly essential for fostering a trustworthy AI ecosystem. A central
challenge persists: balancing text quality against detection robustness. Recent
studies have sought to navigate this trade-off by leveraging signals from model
output distributions (e.g., token-level entropy); however, their reliance on
these model-specific signals presents a significant barrier to public
verification, as the detection process requires access to the logits of the
underlying model. We introduce STELA, a novel framework that aligns watermark
strength with the linguistic degrees of freedom inherent in language. STELA
dynamically modulates the signal using part-of-speech (POS) n-gram-modeled
linguistic indeterminacy, weakening it in grammatically constrained contexts to
preserve quality and strengthen it in contexts with greater linguistic
flexibility to enhance detectability. Our detector operates without access to
any model logits, thus facilitating publicly verifiable detection. Through
extensive experiments on typologically diverse languages-analytic English,
isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior
methods in detection robustness. Our code is available at
https://github.com/Shinwoo-Park/stela_watermark.

</details>


### [105] [Users as Annotators: LLM Preference Learning from Comparison Mode](https://arxiv.org/abs/2510.13830)
*Zhongze Cai,Xiaocheng Li*

Main category: cs.CL

TL;DR: 从用户交互中收集的偏好标签可用于LLM对齐，但存在质量控制问题。本文提出了一种通过用户行为模型推断用户数据质量并相应过滤用户注释数据的方法。


<details>
  <summary>Details</summary>
Motivation: 目前，LLM对齐主要依赖人工标注的偏好数据。然而，随着LLM的普及，用户在日常交互中产生的偏好标签数量不断增加。这些标签的优点是用户最了解自己查询的响应质量，但缺点是缺乏质量控制。

Method: 提出了一种生成两个不同模型或同一模型不同版本的响应的方法。这种不对称性使得能够通过提出的用户行为模型推断用户的数据质量。开发了一种期望最大化算法来估计用户的潜在质量因子，并相应地过滤用户注释数据。

Result: 所提出的方法在捕获用户行为和过滤LLM对齐数据方面都显示出有效性。

Conclusion: 通过用户行为模型推断用户数据质量并相应过滤用户注释数据，可以有效用于LLM对齐。

Abstract: Pairwise preference data have played an important role in the alignment of
large language models (LLMs). Each sample of such data consists of a prompt,
two different responses to the prompt, and a binary label indicating which of
the two responses is better. The labels are usually annotated by professional
human annotators. In this paper, we consider an alternative approach to collect
pairwise preference data -- user annotation from comparison mode. With the
increasingly wider adoption of LLMs among the population, users are
contributing more and more of their preference labels through their daily
interactions with the LLMs. The upside of such labels is that users are the
best experts in judging the responses to their own queries/prompts, but the
downside is the lack of quality control in these labels. In this paper, we
consider a new idea of generating two responses from two different models or
two different versions of the same model. The asymmetry allows us to make an
inference of the user's data quality through our proposed user behavior model.
We develop an expectation-maximization algorithm to estimate a latent quality
factor of the user, and filter users' annotation data accordingly. The
downstream task shows the effectiveness of our approach in both capturing the
user behavior and data filtering for LLM alignment.

</details>


### [106] [Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference](https://arxiv.org/abs/2510.13831)
*Chao Han,Yijuan Liang,Zihao Xuan,Daokuan Wu,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 通过引入


<details>
  <summary>Details</summary>
Motivation: LLMs的高昂推理成本限制了其在实际应用中的部署。现有方法（如动态token级计算分配）通过贪婪路由来选择性激活模型组件，但这会导致信息丢失和次优的token选择。

Method: 提出了一种新的‘智能路由’范式，通过‘轻量级特征预测器（LFF）’来评估token的即时重要性及其可恢复性，实现了‘执行或近似’的策略，以在保持模型保真度的同时大幅降低计算成本。

Result: 在语言建模和推理任务的实验表明，‘智能路由’在不同稀疏度级别下实现了最先进的效率-性能权衡。即使没有最终的LoRA微调，其性能也与需要完全微调的强基线相当或更优，同时训练时间减少了50%以上。

Conclusion: ‘智能路由’范式通过考虑token的可恢复性并采用‘执行或近似’策略，有效解决了现有LLM效率问题的局限性，实现了显著的计算节省和优于现有方法的性能。

Abstract: The deployment of large language models (LLMs) in real-world applications is
increasingly limited by their high inference cost. While recent advances in
dynamic token-level computation allocation attempt to improve efficiency by
selectively activating model components per token, existing methods rely on
greedy routing--a myopic execute-or-skip mechanism that often leads to
irreversible information loss and suboptimal token selection. This paper
introduces informed routing, a new paradigm that proactively addresses these
issues. The key insight is to assess not only a token's immediate importance
but also its recoverability, i.e., how well its transformation can be
approximated. To this end, we propose the Lightweight Feature Forecaster (LFF),
a small predictive module that estimates a unit's output before routing
decisions are made. This enables a flexible execute-or-approximate policy that
preserves model fidelity while drastically reducing computation. Extensive
experiments on both language modeling and reasoning tasks show that informed
routing achieves state-of-the-art efficiency-performance trade-offs across
multiple sparsity levels. Notably, even without final LoRA fine-tuning, our
method matches or surpasses strong baselines that require full fine-tuning, all
while reducing training time by over 50%. The code is available at:
https://github.com/EIT-NLP/informed-routing

</details>


### [107] [Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning](https://arxiv.org/abs/2510.13832)
*Minsik Choi,Hyegang Son,Changhoon Kim,Young Geun Kim*

Main category: cs.CL

TL;DR: Transformer模型在NLP任务中表现优异，但其多层和多注意力头的结构带来效率挑战。HIS等剪枝方法虽有可解释性，但仅考虑梯度贡献，忽略了注意力模式的多样性。本文提出HIES（Head Importance-Entropy Score），结合了头重要性得分和注意力熵，为每头贡献提供互补证据。实验证明，HIES剪枝在模型质量和稳定性方面优于仅HIS的方法，实现了显著的模型压缩，且不牺牲准确性或稳定性。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在NLP任务中表现出色，但其结构复杂性导致推理和部署效率低下。现有的剪枝方法（如基于HIS的方法）虽然提供了解释性，但未能充分捕捉注意力头的全部贡献，特别是忽略了注意力模式的多样性。

Method: 提出了一种新的剪枝标准HIES（Head Importance-Entropy Score），该方法结合了头重要性得分（HIS）和注意力熵。通过整合这两种度量，HIES能够更全面地评估每个注意力头的贡献。

Result: 基于HIES的剪枝方法在模型质量方面取得了高达15.2%的提升，在稳定性方面取得了2.04倍的提升。这表明HIES能够实现显著的模型压缩，同时保持甚至提高模型的准确性和稳定性。

Conclusion: HIES是一种有效的剪枝方法，通过结合头重要性得分和注意力熵，克服了仅使用HIS的局限性。该方法能够在不牺牲模型准确性和稳定性的前提下，实现有效的模型压缩。

Abstract: Transformer-based models have achieved remarkable performance in NLP tasks.
However, their structural characteristics-multiple layers and attention
heads-introduce efficiency challenges in inference and deployment. To address
these challenges, various pruning methods have recently been proposed. Notably,
gradient-based methods using Head Importance Scores (HIS) have gained traction
for interpretability, efficiency, and ability to identify redundant heads.
However, HIS alone has limitations as it captures only the gradient-driven
contribution, overlooking the diversity of attention patterns. To overcome
these limitations, we introduce a novel pruning criterion, HIES (Head
Importance-Entropy Score), which integrates head importance scores with
attention entropy, providing complementary evidence on per-head contribution.
Empirically, HIES-based pruning yields up to 15.2% improvement in model quality
and 2.04x improvement in stability over HIS-only methods, enabling substantial
model compression without sacrificing either accuracy or stability. Code will
be released upon publication.

</details>


### [108] [ConDABench: Interactive Evaluation of Language Models for Data Analysis](https://arxiv.org/abs/2510.13835)
*Avik Dutta,Priyanshu Gupta,Hosein Hasanbeig,Rahul Pratap Singh,Harshit Nigam,Sumit Gulwani,Arjun Radhakrishna,Gustavo Soares,Ashish Tiwari*

Main category: cs.CL

TL;DR: ConDABench 是一个用于生成和评估对话式数据分析（ConDA）基准的新框架，它模拟了真实世界数据分析的复杂性，并通过多代理工作流程生成了 1,420 个问题。该框架首次实现了对对话式数据分析工具的系统评估，并揭示了现有的大型语言模型在需要长期交互的任务上仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 真实世界的数据分析任务目标不明确，数据不干净，需要用户交互来理解和明确用户的意图。然而，现有的评估 LLM 数据分析能力的数据集并未充分体现这些复杂性，也不支持交互式评估。

Method: 提出 ConDABench 框架，包含三个部分：1. 使用多代理工作流生成真实数据分析基准，该工作流从描述从公共数据集中获得见解的文章中提取信息。2. 生成了 1,420 个 ConDA 问题。3. 一个评估工具，可以系统地评估对话式数据分析工具在生成的 ConDA 问题上的表现。

Result: 在生成的基准上评估了最先进的 LLM，发现虽然模型能解决更多的问题实例，但它们在需要长期、持续交互的任务上的表现并未得到necessarily 提升。

Conclusion: ConDABench 为模型开发者提供了一个衡量其在构建真正协作式模型（能够完成复杂交互式任务）方面进展的途径。

Abstract: Real-world data analysis tasks often come with under-specified goals and
unclean data. User interaction is necessary to understand and disambiguate a
user's intent, and hence, essential to solving these complex tasks. Existing
benchmarks for evaluating LLMs on data analysis tasks do not capture these
complexities or provide first-class support for interactivity. We introduce
ConDABench, a framework for generating conversational data analysis (ConDA)
benchmarks and evaluating external tools on the generated benchmarks. \bench
consists of (a) a multi-agent workflow for generating realistic benchmarks from
articles describing insights gained from public datasets, (b) 1,420 ConDA
problems generated using this workflow, and (c) an evaluation harness that, for
the first time, makes it possible to systematically evaluate conversational
data analysis tools on the generated ConDA problems. Evaluation of
state-of-the-art LLMs on the benchmarks reveals that while the new generation
of models are better at solving more instances, they are not necessarily better
at solving tasks that require sustained, long-form engagement. ConDABench is an
avenue for model builders to measure progress towards truly collaborative
models that can complete complex interactive tasks.

</details>


### [109] [From Loop Nests to Silicon: Mapping AI Workloads onto AMD NPUs with MLIR-AIR](https://arxiv.org/abs/2510.14871)
*Erwei Wang,Samuel Bayliss,Andra Bisca,Zachary Blair,Sangeeta Chowdhary,Kristof Denolf,Jeff Fifield,Brandon Freiberger,Erika Hunhoff,Phil James-Roxby,Jack Lo,Joseph Melber,Stephen Neuendorffer,Eddie Richter,Andre Rosti,Javier Setoain,Gagandeep Singh,Endri Taka,Pranathi Vasireddy,Zhewen Yu,Niansong Zhang,Jinming Zhuang*

Main category: cs.CL

TL;DR: MLIR-AIR是一个基于MLIR的新型编译器栈，用于弥合高级工作负载与AMD NPU等细粒度空间架构之间的语义鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现代计算架构越来越依赖对数据移动、执行顺序和计算放置进行细粒度控制以获得性能，因此编译器基础设施必须提供显式的机制来协调计算和数据，以充分利用这些架构。

Method: MLIR-AIR定义了AIR方言，为跨计算和内存资源的异步和分层操作提供了结构化表示。AIR原语允许编译器进行空间调度、在硬件区域之间分发计算以及在不依赖临时运行时协调或手动调度的 Reconcile 的情况下重叠通信与计算。

Result: 对于矩阵乘法，MLIR-AIR实现了高达78.7%的计算效率，生成的实现性能几乎与使用低级、接近金属的MLIR-AIE框架手工优化的矩阵乘法相媲美。对于多头注意力，AIR接口支持使用大约150行代码进行融合实现，从而能够有效地将复杂工作负载映射到空间硬件。

Conclusion: MLIR-AIR将高级结构化控制流转换为空间程序，通过编译器管理的调度，利用异步执行、分块和通信重叠，有效地利用NPU的计算结构和内存层次结构。

Abstract: General-purpose compilers abstract away parallelism, locality, and
synchronization, limiting their effectiveness on modern spatial architectures.
As modern computing architectures increasingly rely on fine-grained control
over data movement, execution order, and compute placement for performance,
compiler infrastructure must provide explicit mechanisms for orchestrating
compute and data to fully exploit such architectures. We introduce MLIR-AIR, a
novel, open-source compiler stack built on MLIR that bridges the semantic gap
between high-level workloads and fine-grained spatial architectures such as
AMD's NPUs. MLIR-AIR defines the AIR dialect, which provides structured
representations for asynchronous and hierarchical operations across compute and
memory resources. AIR primitives allow the compiler to orchestrate spatial
scheduling, distribute computation across hardware regions, and overlap
communication with computation without relying on ad hoc runtime coordination
or manual scheduling. We demonstrate MLIR-AIR's capabilities through two case
studies: matrix multiplication and the multi-head attention block from the
LLaMA 2 model. For matrix multiplication, MLIR-AIR achieves up to 78.7% compute
efficiency and generates implementations with performance almost identical to
state-of-the-art, hand-optimized matrix multiplication written using the
lower-level, close-to-metal MLIR-AIE framework. For multi-head attention, we
demonstrate that the AIR interface supports fused implementations using
approximately 150 lines of code, enabling tractable expression of complex
workloads with efficient mapping to spatial hardware. MLIR-AIR transforms
high-level structured control flow into spatial programs that efficiently
utilize the compute fabric and memory hierarchy of an NPU, leveraging
asynchronous execution, tiling, and communication overlap through
compiler-managed scheduling.

</details>


### [110] [SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2510.13836)
*Debarun Bhattacharjya,Balaji Ganesan,Junkyu Lee,Radu Marinescu,Katsiaryna Mirylenka,Michael Glass,Xiao Shou*

Main category: cs.CL

TL;DR: 在生成式人工智能中，通过评估生成内容的一致性来量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 虽然不确定性量化（UQ）对于建立可信赖的人工智能至关重要，但许多现有方法需要访问模型的内部信息。然而，黑盒UQ方法具有诸多实际优势，例如对系统更改的鲁棒性、对LLM选择的适应性、成本降低和计算可行性。

Method: 提出了一种基于非语言相似度的聚合框架，该框架可以包含各种适用于复杂生成任务的UQ方法，并引入了使用小型训练集进行置信度估计模型训练的新技术。

Result: 通过在问答、摘要和文本到SQL等多样化任务的数据集上进行实证研究，证明了所提出的基于相似度的方法可以提供比基线方法更好的置信度校准。

Conclusion: 基于相似度的UQ方法在量化大型语言模型的知识不确定性方面，尤其是在复杂生成任务中，是一种有效且实用的方法。

Abstract: When does a large language model (LLM) know what it does not know?
Uncertainty quantification (UQ) provides measures of uncertainty, such as an
estimate of the confidence in an LLM's generated output, and is therefore
increasingly recognized as a crucial component of trusted AI systems. Black-box
UQ methods do not require access to internal model information from the
generating LLM and therefore have numerous real-world advantages, such as
robustness to system changes, adaptability to choice of LLM, reduced costs, and
computational tractability. In this paper, we investigate the effectiveness of
UQ techniques that are primarily but not necessarily entirely black-box, where
the consistency between a generated output and other sampled generations is
used as a proxy for confidence in its correctness. We propose a high-level
non-verbalized similarity-based aggregation framework that subsumes a broad
swath of UQ approaches suitable for complex generative tasks, as well as
introduce specific novel techniques from the framework that train confidence
estimation models using small training sets. Through an empirical study with
datasets spanning the diverse tasks of question answering, summarization, and
text-to-SQL, we demonstrate that our proposed similarity-based methods can
yield better calibrated confidences than baselines.

</details>


### [111] [Meronymic Ontology Extraction via Large Language Models](https://arxiv.org/abs/2510.13839)
*Dekai Zhang,Simone Conia,Antonio Rago*

Main category: cs.CL

TL;DR: 本文利用大型语言模型（LLMs）自动化提取商品本体，并证明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 手动构建本体耗时耗力，需要自动化方法。本研究旨在利用LLMs实现商品本体的自动化提取。

Method: 利用大型语言模型（LLMs）从原始评论文本中提取本体（meronymies）。

Result: 通过LLM-as-a-judge评估，本方法产生的本体优于现有的、基于BERT的基线。

Conclusion: 为LLMs更广泛地应用于（商品或其他）本体提取奠定了基础。

Abstract: Ontologies have become essential in today's digital age as a way of
organising the vast amount of readily available unstructured text. In providing
formal structure to this information, ontologies have immense value and
application across various domains, e.g., e-commerce, where countless product
listings necessitate proper product organisation. However, the manual
construction of these ontologies is a time-consuming, expensive and laborious
process. In this paper, we harness the recent advancements in large language
models (LLMs) to develop a fully-automated method of extracting product
ontologies, in the form of meronymies, from raw review texts. We demonstrate
that the ontologies produced by our method surpass an existing, BERT-based
baseline when evaluating using an LLM-as-a-judge. Our investigation provides
the groundwork for LLMs to be used more generally in (product or otherwise)
ontology extraction.

</details>


### [112] [ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking](https://arxiv.org/abs/2510.13842)
*Yutao Wu,Xiao Liu,Yinghui Li,Yifeng Gao,Yifan Ding,Jiale Ding,Xiang Zheng,Xingjun Ma*

Main category: cs.CL

TL;DR: 知识投毒对检索增强生成（RAG）系统构成严重威胁，尤其是在事实核查场景下。研究提出了ADMIT（对抗性多重注入技术），一种少样本、语义对齐的攻击方法，能够在不访问目标模型或检索器的情况下，翻转事实核查决策并产生欺骗性论证。


<details>
  <summary>Details</summary>
Motivation: 现有研究揭示了大型语言模型（LLM）容易受到误导性或恶意检索内容的影响。然而，真实世界的事实核查场景更具挑战性，因为检索池中通常充斥着可信证据。本研究旨在解决这一问题，将知识投毒扩展到事实核查领域。

Method: 提出了一种名为ADMIT（对抗性多重注入技术）的攻击方法。该方法是一种少样本、语义对齐的攻击技术，无需访问目标LLM、检索器或进行令牌级控制，即可翻转事实核查决策并诱导产生欺骗性论证。

Result: ADMIT在4个检索器、11个LLM和4个跨领域基准测试中表现出良好的迁移性，在极低的投毒率（0.93 x 10^-6）下实现了86%的平均攻击成功率（ASR）。即使存在强对抗性证据，该攻击仍然保持鲁棒性。与现有最优攻击方法相比，ADMIT在所有设置下的ASR平均提高了11.2%。

Conclusion: ADMIT攻击揭示了基于RAG的事实核查系统存在严重漏洞。

Abstract: Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation
(RAG) systems by injecting adversarial content into knowledge bases, tricking
Large Language Models (LLMs) into producing attacker-controlled outputs
grounded in manipulated context. Prior work highlights LLMs' susceptibility to
misleading or malicious retrieved content. However, real-world fact-checking
scenarios are more challenging, as credible evidence typically dominates the
retrieval pool. To investigate this problem, we extend knowledge poisoning to
the fact-checking setting, where retrieved context includes authentic
supporting or refuting evidence. We propose \textbf{ADMIT}
(\textbf{AD}versarial \textbf{M}ulti-\textbf{I}njection \textbf{T}echnique), a
few-shot, semantically aligned poisoning attack that flips fact-checking
decisions and induces deceptive justifications, all without access to the
target LLMs, retrievers, or token-level control. Extensive experiments show
that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4
cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\%
at an extremely low poisoning rate of $0.93 \times 10^{-6}$, and remaining
robust even in the presence of strong counter-evidence. Compared with prior
state-of-the-art attacks, ADMIT improves ASR by 11.2\% across all settings,
exposing significant vulnerabilities in real-world RAG-based fact-checking
systems.

</details>


### [113] [Serialized EHR make for good text representations](https://arxiv.org/abs/2510.13843)
*Zhirong Chou,Quan Qin,Shi Li*

Main category: cs.CL

TL;DR: SerialBEHRT是一个针对电子健康记录（EHR）设计的领域适应性基础模型，通过在结构化EHR序列上进行额外预训练，解决了现有模型在处理EHR的表格和事件性质以及自然语言模型的序列先验之间的结构不匹配问题，能更好地捕捉患者就诊的纵向依赖关系，并在抗生素敏感性预测任务中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在处理电子健康记录（EHR）的表格和事件性质与自然语言模型的序列先验之间存在结构不匹配问题，限制了它们捕捉患者就诊纵向依赖关系的能力。

Method: 通过在结构化EHR序列上进行额外预训练，扩展了SciBERT模型，使其能够编码临床事件之间的时间和上下文关系，从而产生更丰富的患者表征。模型命名为SerialBEHRT。

Result: 在抗生素敏感性预测这一临床应用任务中，SerialBEHRT通过广泛的基准测试，证明了其相对于现有最先进的EHR表征策略具有更优越、更一致的性能。

Conclusion: 在医疗保健领域，时间序列化在基础模型预训练中至关重要，SerialBEHRT的有效性凸显了这一点。

Abstract: The emergence of foundation models in healthcare has opened new avenues for
learning generalizable representations from large scale clinical data. Yet,
existing approaches often struggle to reconcile the tabular and event based
nature of Electronic Health Records (EHRs) with the sequential priors of
natural language models. This structural mismatch limits their ability to
capture longitudinal dependencies across patient encounters. We introduce
SerialBEHRT, a domain aligned foundation model that extends SciBERT through
additional pretraining on structured EHR sequences. SerialBEHRT is designed to
encode temporal and contextual relationships among clinical events, thereby
producing richer patient representations. We evaluate its effectiveness on the
task of antibiotic susceptibility prediction, a clinically meaningful problem
in antibiotic stewardship. Through extensive benchmarking against state of the
art EHR representation strategies, we demonstrate that SerialBEHRT achieves
superior and more consistent performance, highlighting the importance of
temporal serialization in foundation model pretraining for healthcare.

</details>


### [114] [DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models](https://arxiv.org/abs/2510.13847)
*Jinbin Zhang,Nasib Ullah,Erik Schultheis,Rohit Babbar*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Speculative decoding (a.k.a. speculative sampling) has become a standard way
to accelerate LLM inference: a small drafter proposes multiple tokens and a
large target model verifies them once per speculation length. Recently, scaling
of the LLM vocabulary has pushed the number of tokens to grow substantially.
While verification over the full vocabulary leaves the target model largely
unaffected, the O(|V|d) parameters in the drafter's output head become a
latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g.,
FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the
target model's vocabulary, ranked in descending order of token frequency.
Although this reduces draft-time compute, it is brittle, since: (i) frequency
lists are corpus-dependent and require retuning to generalize, and (ii) static
shortlists suppress rare or domain-specific tokens, lowering the expected
number of tokens per verification step. We propose DynaSpec, a
context-dependent dynamic shortlisting mechanism that is robust, speeds up
drafting, and generalizes across diverse tasks. Concretely, we introduce
lightweight, coarse-grained meta-classifiers that route contexts to a small
number of token clusters; the union of the top-k selected clusters forms the
drafter's shortlist, while verification retains the full vocabulary and
exactness. The meta-classifier finishes its computation earlier than the
drafter's hidden state generation by exploiting parallel execution of draft
encoding and meta shortlisting on separate streams. On standard
speculative-decoding benchmarks, we observe consistent gains in mean accepted
length over fixed-shortlist baselines, while context-dependent selection
enables smaller shortlists without degrading acceptance.

</details>


### [115] [On-device System of Compositional Multi-tasking in Large Language Models](https://arxiv.org/abs/2510.13848)
*Ondrej Bohdal,Konstantinos Theodosiadis,Asterios Mpatziakas,Dimitris Filippidis,Iro Spyrou,Christos Zonios,Anastasios Drosou,Dimosthenis Ioannidis,Kyeng-Hun Lee,Jijoong Moon,Hyeonmok Ko,Mete Ozay,Umberto Michieli*

Main category: cs.CL

TL;DR: LLM 任务的适配方法可以通过LoRA等方法进行参数高效微调，但组合多个任务时存在挑战。本文提出一种新的方法，通过在LoRA适配器之上添加一个可学习的投影层，实现了对包含摘要和翻译在内的复合任务的同时执行，提高了效率。该方法在云端和设备端都有良好表现，并开发了相应的安卓应用。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法（如LoRA）在处理需要同时执行的复杂复合任务（如生成翻译摘要）时存在困难，导致无法有效整合多个适配器。因此，需要一种新的方法来解决这一挑战，以实现对多个任务的有效整合和高效处理。

Method: 提出了一种新的方法，该方法通过在组合的摘要和翻译适配器之上添加一个可学习的投影层，实现了对复合任务的有效整合。这种设计能够在保持高效的同时，减少计算开销，避免了广泛的再训练或顺序处理的需要。

Result: 实验表明，该方法在云端和设备端的实现均表现良好且速度快。通过开发一个安卓应用程序，展示了该方法在设备端无缝执行复合任务的可行性。

Conclusion: 该方法为处理涉及摘要和翻译的复合任务提供了一种有效且高效的解决方案。其在云端和设备端的高性能表现，以及在资源受限环境下的潜力，使其在需要高速运行和资源限制的实际应用中具有广阔的应用前景。

Abstract: Large language models (LLMs) are commonly adapted for diverse downstream
tasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters
(LoRA). While adapters can be combined to handle multiple tasks separately,
standard approaches struggle when targeting the simultaneous execution of
complex tasks, such as generating a translated summary from a long
conversation. To address this challenge, we propose a novel approach tailored
specifically for compositional multi-tasking scenarios involving summarization
and translation. Our technique involves adding a learnable projection layer on
top of the combined summarization and translation adapters. This design enables
effective integration while maintaining efficiency through reduced
computational overhead compared to alternative strategies requiring extensive
retraining or sequential processing. We demonstrate the practical viability of
our method within an on-device environment by developing an Android app capable
of executing compositional tasks seamlessly. Experimental results indicate our
solution performs well and is fast in both cloud-based and on-device
implementations, highlighting the potential benefits of adopting our framework
in real-world applications demanding high-speed operation alongside resource
constraints.

</details>


### [116] [Language steering in latent space to mitigate unintended code-switching](https://arxiv.org/abs/2510.13849)
*Andrey Goncharov,Nikolai Kondusov,Alexey Zaytsev*

Main category: cs.CL

TL;DR: 提出一种名为“潜在空间语言引导”的轻量级推理时方法，通过PCA识别语言方向来控制语言身份，以减轻多语言大语言模型中的意外语码转换，同时保持语义。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型（LLMs）常常会发生意外的语码转换，从而降低了在下游任务中的可靠性。

Method: 通过在平行翻译上使用PCA识别语言方向，然后沿着这些轴引导词嵌入以控制语言身份。

Result: 在Qwen2.5和Llama-3.2模型上的多种语言对中，使用单个主成分实现了95-99%的语言分类准确率，并将下一个词的分布散度降低了高达42%。

Conclusion: 该方法可以减轻语码转换，同时保持语义，计算开销可忽略不计，并且只需要少量的平行数据进行校准。语言身份集中在靠后的层中，具有近乎完美的线性可分离性。

Abstract: Multilingual Large Language Models (LLMs) often exhibit unintended
code-switching, reducing reliability in downstream tasks. We propose
latent-space language steering, a lightweight inference-time method that
identifies language directions via PCA on parallel translations and steers
token embeddings along these axes to control language identity. Our approach
mitigates code-switching while preserving semantics with negligible
computational overhead and requires only minimal parallel data for calibration.
Empirically, we achieve 95-99\% language classification accuracy using a single
principal component and reduce next-token distributional divergence by up to
42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. We further
analyze the layer-wise evolution of language representations, revealing that
language identity concentrates in final layers with near-perfect linear
separability.

</details>


### [117] [Revisiting the UID Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.13850)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.CL

TL;DR: LLM的CoT推理过程中，成功的推理路径信息密度是非均匀的，这与人类沟通模式形成对比。


<details>
  <summary>Details</summary>
Motivation: LLM的CoT推理中间步骤常常不准确或难以解释，启发于人类沟通中信息密度保持稳定的UID假说，研究LLM推理过程中的信息流。

Method: 使用基于熵的度量来分析推理轨迹中的信息流，并在三个数学基准上进行测试。

Result: LLM的成功推理在全局上是非均匀的，正确的解决方案具有信息密度不均匀的特点，这与人类的沟通模式形成鲜明对比。

Conclusion: LLM的推理方式与人类不同，这挑战了关于机器推理的假设，并为设计可解释和自适应的推理模型提供了新的方向。

Abstract: Large language models (LLMs) often solve problems using step-by-step
Chain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently
unfaithful or hard to interpret. Inspired by the Uniform Information Density
(UID) hypothesis in psycholinguistics -- which posits that humans communicate
by maintaining a stable flow of information -- we introduce entropy-based
metrics to analyze the information flow within reasoning traces. Surprisingly,
across three challenging mathematical benchmarks, we find that successful
reasoning in LLMs is globally non-uniform: correct solutions are characterized
by uneven swings in information density, in stark contrast to human
communication patterns. This result challenges assumptions about machine
reasoning and suggests new directions for designing interpretable and adaptive
reasoning models.

</details>


### [118] [EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing](https://arxiv.org/abs/2510.13851)
*Sicheng Lyu,Yu Gu,Xinyu Wang,Jerry Huang,Sitao Luan,Yufei Cui,Xiao-Wen Chang,Peng Lu*

Main category: cs.CL

TL;DR: EvoEdit通过顺序零空间对齐来解决大型语言模型（LLM）的灾难性干扰问题，在保持原有知识的同时稳定高效地进行模型编辑。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法在顺序编辑时存在灾难性干扰问题，导致新编辑损害旧编辑和保留的知识。而LLM需要持续更新来纠正过时或错误的信息。

Method: EvoEdit通过对每个新编辑执行顺序零空间对齐，以保留原有和先前修改的知识表示，并保持在保留知识上的输出不变性，从而有效缓解干扰。

Result: 在真实世界的顺序知识编辑基准测试中，EvoEdit的性能优于或媲美先前最先进的定位-编辑技术，并且速度最高可提升3.53倍。

Conclusion: 为应对动态演变信息环境中的LLM设计提出更原则性方法是必要的，EvoEdit提供了一个具有强大理论保证的简单而有效的解决方案。

Abstract: Large language models (LLMs) require continual updates to rectify outdated or
erroneous knowledge. Model editing has emerged as a compelling paradigm for
introducing targeted modifications without the computational burden of full
retraining. Existing approaches are mainly based on a locate-then-edit
framework. However, in sequential editing contexts, where multiple updates are
applied over time, they exhibit significant limitations and suffer from
catastrophic interference, i.e., new edits compromise previously integrated
updates and degrade preserved knowledge. To address these challenges, we
introduce EvoEdit, a novel editing strategy that mitigates catastrophic
interference through sequential null-space alignment, enabling stable and
efficient model editing. By performing sequential null-space alignment for each
incoming edit, EvoEdit preserves both original and previously modified
knowledge representations and maintains output invariance on preserved
knowledge even across long edit sequences, effectively mitigating interference.
Evaluations on real-world sequential knowledge-editing benchmarks show that
EvoEdit achieves better or comparable performance than prior state-of-the-art
locate-then-edit techniques, with up to 3.53 times speedup. Overall, these
results underscore the necessity of developing more principled approaches for
designing LLMs in dynamically evolving information settings, while providing a
simple yet effective solution with strong theoretical guarantees.

</details>


### [119] [ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups](https://arxiv.org/abs/2510.13852)
*Peter Banyas,Shristi Sharma,Alistair Simmons,Atharva Vispute*

Main category: cs.CL

TL;DR: 该论文提出了一个名为 ConsistencyAI 的独立基准，用于评估大型语言模型（LLM）在面对不同用户身份（persona）时回答事实信息的一致性。


<details>
  <summary>Details</summary>
Motivation: 衡量 LLM 在不同用户身份提问相同问题时，是否会给出事实不一致的回答，以评估其可靠性和公平性。

Method: 使用 19 个 LLM，针对 15 个主题，每个主题请求 5 个事实，并为每个 LLM 进行了 100 次不同身份的用户模拟。通过计算响应的句子嵌入的余弦相似度来量化事实一致性。

Result: 在 100 个身份的实验中，一致性得分范围为 0.7896 至 0.9065，平均得分为 0.8656。xAI 的 Grok-3 表现最稳定，而一些小型模型得分最低。不同主题的一致性也不同，就业市场最低，G7 世界领导人最高，疫苗或巴以冲突等问题则因模型提供商而异。

Conclusion: LLM 的事实一致性受到模型提供商和主题内容的影响。论文发布了代码和演示以支持可复现的评估，并鼓励开发不变的提示策略。

Abstract: Is an LLM telling you different facts than it's telling me? This paper
introduces ConsistencyAI, an independent benchmark for measuring the factual
consistency of large language models (LLMs) for different personas.
ConsistencyAI tests whether, when users of different demographics ask identical
questions, the model responds with factually inconsistent answers. Designed
without involvement from LLM providers, this benchmark offers impartial
evaluation and accountability. In our experiment, we queried 19 LLMs with
prompts that requested 5 facts for each of 15 topics. We repeated this query
100 times for each LLM, each time adding prompt context from a different
persona selected from a subset of personas modeling the general population. We
processed the responses into sentence embeddings, computed cross-persona cosine
similarity, and computed the weighted average of cross-persona cosine
similarity to calculate factual consistency scores. In 100-persona experiments,
scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as
a benchmark threshold. xAI's Grok-3 is most consistent, while several
lightweight models rank lowest. Consistency varies by topic: the job market is
least consistent, G7 world leaders most consistent, and issues like vaccines or
the Israeli-Palestinian conflict diverge by provider. These results show that
both the provider and the topic shape the factual consistency. We release our
code and interactive demo to support reproducible evaluation and encourage
persona-invariant prompting strategies.

</details>


### [120] [BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation](https://arxiv.org/abs/2510.13853)
*Fabian Wenz,Omar Bouattour,Devin Yang,Justin Choi,Cecil Gregg,Nesime Tatbul,Çağatay Demiralp*

Main category: cs.CL

TL;DR: LLM 在文本到 SQL 生成任务上表现出色，但主要是在公开数据集上。为了解决私有企业数据仓库的挑战，我们发布了 Beaver 基准。为了减轻手动注释 SQL 日志的负担，我们引入了 BenchPress，一个结合了检索增强生成 (RAG) 和 LLM 的人机协作系统，用于加速特定领域的文本到 SQL 基准的创建。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到 SQL 研究主要集中在公开数据集上，而 LLM 在私有企业数据仓库上的表现不佳。创建私有企业文本到 SQL 基准（如 Beaver）需要手动注释 SQL 日志，这是一个耗时且成本高昂的过程，尤其是数据库管理员的参与。

Method: BenchPress 是一个结合了人机协作的系统。它利用检索增强生成 (RAG) 和 LLM 来提出针对给定 SQL 查询的多种自然语言描述。然后，人类专家对这些建议进行选择、排名或编辑，以确保准确性和领域相关性。

Result: BenchPress 在注释的企业 SQL 日志上进行了评估，结果表明 LLM 辅助注释可显著减少创建高质量基准所需的时间和精力。结合人类验证和 LLM 生成的建议可以提高注释准确性、基准可靠性和模型评估的鲁棒性。

Conclusion: BenchPress 通过简化自定义基准的创建，为研究人员和从业人员提供了一种评估特定领域工作负载的文本到 SQL 模型的方法，从而解决了在私有企业环境中创建文本到 SQL 基准的挑战。

Abstract: Large language models (LLMs) have been successfully applied to many tasks,
including text-to-SQL generation. However, much of this work has focused on
publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work
showed that LLMs are much less effective in querying large private enterprise
data warehouses and released Beaver, the first private enterprise text-to-SQL
benchmark. To create Beaver, we leveraged SQL logs, which are often readily
available. However, manually annotating these logs to identify which natural
language questions they answer is a daunting task. Asking database
administrators, who are highly trained experts, to take on additional work to
construct and validate corresponding natural language utterances is not only
challenging but also quite costly. To address this challenge, we introduce
BenchPress, a human-in-the-loop system designed to accelerate the creation of
domain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses
retrieval-augmented generation (RAG) and LLMs to propose multiple natural
language descriptions. Human experts then select, rank, or edit these drafts to
ensure accuracy and domain alignment. We evaluated BenchPress on annotated
enterprise SQL logs, demonstrating that LLM-assisted annotation drastically
reduces the time and effort required to create high-quality benchmarks. Our
results show that combining human verification with LLM-generated suggestions
enhances annotation accuracy, benchmark reliability, and model evaluation
robustness. By streamlining the creation of custom benchmarks, BenchPress
offers researchers and practitioners a mechanism for assessing text-to-SQL
models on a given domain-specific workload. BenchPress is freely available via
our public GitHub repository at
https://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our
website at http://dsg-mcgraw.csail.mit.edu:5000.

</details>


### [121] [R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging](https://arxiv.org/abs/2510.13854)
*Mamadou K. Keita,Christopher Homan,Sebastien Diarra*

Main category: cs.CL

TL;DR: 该研究提出了一种名为规则到标签（R2T）的混合框架，该框架将多层语言规则集成到神经网络的训练目标中，并提出了一种名为“原则学习”（PrL）的新范式。


<details>
  <summary>Details</summary>
Motivation: 开发一种不依赖大量标注数据，而是通过显式任务约束来训练模型的学习范式。

Method: R2T框架，包含一个自适应损失函数和一个正则化项，用于处理未登录词（OOV）的不确定性。将此框架应用于词性标注（POS）和命名实体识别（NER）任务。

Result: 在Zarma词性标注任务上，R2T-BiLSTM模型仅使用无标签文本就达到了98.2%的准确率，优于使用300个标注句子进行微调的AfriBERTa模型。在命名实体识别任务上，R2T作为预训练步骤，在仅50个标注句子上微调的模型优于在300个标注句子上训练的基线模型。

Conclusion: R2T框架能够有效地利用语言规则进行模型训练，尤其是在标注数据稀疏的情况下，能够显著提高模型性能，并为更复杂的任务提供了有效的预训练方法。

Abstract: We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that
integrates a multi-tiered system of linguistic rules directly into a neural
network's training objective. R2T's novelty lies in its adaptive loss function,
which includes a regularization term that teaches the model to handle
out-of-vocabulary (OOV) words with principled uncertainty. We frame this work
as a case study in a paradigm we call principled learning (PrL), where models
are trained with explicit task constraints rather than on labeled examples
alone. Our experiments on Zarma part-of-speech (POS) tagging show that the
R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy,
outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We
further show that for more complex tasks like named entity recognition (NER),
R2T serves as a powerful pre-training step; a model pre-trained with R2T and
fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.

</details>


### [122] [Harnessing Consistency for Robust Test-Time LLM Ensemble](https://arxiv.org/abs/2510.13855)
*Zhichen Zeng,Qi Yu,Xiao Lin,Ruizhong Qiu,Xuying Ning,Tianxin Wei,Yuchen Yan,Jingrui He,Hanghang Tong*

Main category: cs.CL

TL;DR: LLM集成模型在处理不同模型和分词方案时存在鲁棒性问题，提出CoRE技术通过引入token级和模型级的一致性来提高集成模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM集成方法在提高模型集成质量方面取得了显著进展，但对集成模型在面对潜在错误信号（如异构分词方案和不同模型专业知识）时的鲁棒性关注不足。

Method: 提出CoRE技术，一种即插即用的方法，利用模型一致性来增强LLM集成模型的鲁棒性。CoRE包含两个关键组件：1. Token级一致性：通过低通滤波降低不确定性高的token的权重，以应对token对齐问题。2. 模型级一致性：通过提高模型的自置信度和减少模型间输出的分歧来建模全局一致性。

Result: 在多个基准测试、模型组合和集成策略的广泛实验表明，CoRE能够持续提升集成模型的性能和鲁棒性。

Conclusion: CoRE是一种有效的技术，可以无缝集成到各种LLM集成方法中，通过引入token级和模型级的一致性，显著提高了LLM集成模型的鲁棒性。

Abstract: Different large language models (LLMs) exhibit diverse strengths and
weaknesses, and LLM ensemble serves as a promising approach to integrate their
complementary capabilities. Despite substantial progress in improving ensemble
quality, limited attention has been paid to the robustness of ensembles against
potential erroneous signals, which often arise from heterogeneous tokenization
schemes and varying model expertise. Our analysis shows that ensemble failures
typically arise from both the token level and the model level: the former
reflects severe disagreement in token predictions, while the latter involves
low confidence and pronounced disparities among models. In light of this, we
propose CoRE, a plug-and-play technique that harnesses model consistency for
robust LLM ensemble, which can be seamlessly integrated with diverse ensemble
methods. Token-level consistency captures fine-grained disagreements by
applying a low-pass filter to downweight uncertain tokens with high
inconsistency, often due to token misalignment, thereby improving robustness at
a granular level. Model-level consistency models global agreement by promoting
model outputs with high self-confidence and minimal divergence from others,
enhancing robustness at a coarser level. Extensive experiments across diverse
benchmarks, model combinations, and ensemble strategies demonstrate that CoRE
consistently improves ensemble performance and robustness.

</details>


### [123] [Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA](https://arxiv.org/abs/2510.13856)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

TL;DR: MasonNLP 系统使用检索增强生成（RAG）框架，结合通用领域指令微调大语言模型和领域内文本/视觉示例，在MEDIQA-WV 2025 伤口护理 VQA 任务中取得第三名，证明了轻量级 RAG 是一个有效且简单的多模态临床 NLP 基线。


<details>
  <summary>Details</summary>
Motivation: 支持临床决策和患者护理，解决伤口护理 VQA 任务，需要系统从图像和患者查询中生成自由文本响应和结构化伤口属性。

Method: 采用通用领域指令微调大语言模型，并结合检索增强生成（RAG）框架，该框架包含领域内数据的文本和视觉示例。

Result: 最佳系统在 19 个团队和 51 个提交中排名第三，平均得分为 41.37%。该方法提高了 dBLEU、ROUGE、BERTScore 和基于 LLM 的指标。

Conclusion: 轻量级 RAG 结合通用 LLM，通过添加少量相关示例，提供了一个简单有效的多模态临床 NLP 任务基线。

Abstract: Medical Visual Question Answering (MedVQA) enables natural language queries
over medical images to support clinical decision-making and patient care. The
MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to
generate free-text responses and structured wound attributes from images and
patient queries. We present the MasonNLP system, which employs a
general-domain, instruction-tuned large language model with a
retrieval-augmented generation (RAG) framework that incorporates textual and
visual examples from in-domain data. This approach grounds outputs in
clinically relevant exemplars, improving reasoning, schema adherence, and
response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our
best-performing system ranked 3rd among 19 teams and 51 submissions with an
average score of 41.37%, demonstrating that lightweight RAG with
general-purpose LLMs -- a minimal inference-time layer that adds a few relevant
exemplars via simple indexing and fusion, with no extra training or complex
re-ranking -- provides a simple and effective baseline for multimodal clinical
NLP tasks.

</details>


### [124] [ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing](https://arxiv.org/abs/2510.13860)
*Shivanshu Kumar,Gopalakrishnan Srinivasan*

Main category: cs.CL

TL;DR: ShishuLM通过使用MLP近似Transformer块来减少内存和计算开销，从而提高效率。


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽然在NLP任务中表现优异，但存在内存和计算开销大的问题。ShishuLM旨在解决这些问题，并关注日益重要的SLM。

Method: 通过分析Transformer块中的归一化和注意力计算，发现其与输入大致呈线性关系，从而提出用MLP近似整个Transformer块的方法，构建了ShishuLM架构。

Result: ShishuLM在不同规模的SLM上进行了评估，结果显示其内存需求减少高达25%，训练和推理时的延迟提高了40%。

Conclusion: ShishuLM的实验和分析结果为从预训练角度构建更高效的SLM架构提供了见解。

Abstract: While the transformer architecture has achieved state-of-the-art performance
on natural language processing tasks, these models impose substantial memory
and computational overhead. Recent research has identified significant
architectural redundancies within these models, presenting opportunities for
optimization without compromising performance. Taking insights from research in
AI interpretability and inference-time layer pruning, we introduce an efficient
language model architecture, referred to as ShishuLM, which reduces both the
parameter count and Key-Value (KV) cache requirements. Given the increasing
importance of Small Language Models (SLMs) in agentic AI systems, we evaluate
our approach on two SLMs of different scales. Our analysis reveals that for
moderate-context scenarios, normalization coupled with attention computation is
roughly linear with the input, enabling entire transformer blocks to be
approximated through Multi-Layer Perceptrons (MLPs). Our results show that
ShishuLM provides up to 25% reduction in memory requirements and up to 40%
improvement in latency during both training and inference, compared to parent
models. Our experimental and analytical findings provide insights towards
building more efficient SLM architectures from a pre-training standpoint.

</details>


### [125] [Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues](https://arxiv.org/abs/2510.13862)
*Chenyu Zhang,Sharifa Alghowinem,Cynthia Breazeal*

Main category: cs.CL

TL;DR: 本研究提出了一个集成LLM的框架，用于大规模情感识别，并分析了学生与AI导师互动的过程中情绪的变化。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究关注LLM在教育中的应用，但对LLM驱动的辅导过程中情感动态的理解仍显不足。本研究旨在填补这一空白。

Method: 本研究构建了一个集成LLM的框架，用于分析PyTutor（一个LLM驱动的AI导师）与261名本科生之间261名本科生之间16,986次对话。研究使用Gemini、GPT-4o和Claude三个前沿LLM对对话进行零样本情感标注，包括效价、唤醒度和学习帮助度，并生成自由文本情感标签。通过模型内秩加权池化和跨模型多数表决融合这些标注，生成稳健的情感画像。

Result: 研究发现，学生在与AI导师互动时，通常表现出轻微积极的情感和中等程度的唤醒度。然而，学习过程并非一帆风顺，困惑和好奇是解决问题过程中的常见伴侣，而挫败感虽然不常见，但会干扰学习进程。情感状态的持续时间较短，积极情绪的持续时间略长于中性或消极情绪，但它们易受干扰。值得鼓舞的是，消极情绪通常会迅速消散，有时甚至会直接转变为积极情绪。中性时刻常常是转折点，它们更多地将学生引向积极状态而非消极状态，这表明存在干预机会。

Conclusion: 学生与AI导师互动时，情感状态是动态且短暂的，但也存在积极的反馈循环和干预机会。通过关注学生的情绪状态，可以更负责任地将生成式AI整合到教育中。

Abstract: While recent studies have examined the leaning impact of large language model
(LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring
remain insufficiently understood. This work introduces the first ensemble-LLM
framework for large-scale affect sensing in tutoring dialogues, advancing the
conversation on responsible pathways for integrating generative AI into
education by attending to learners' evolving affective states. To achieve this,
we analyzed two semesters' worth of 16,986 conversational turns exchanged
between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across
three U.S. institutions. To investigate learners' emotional experiences, we
generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o,
Claude), including scalar ratings of valence, arousal, and
learning-helpfulness, along with free-text emotion labels. These estimates are
fused through rank-weighted intra-model pooling and plurality consensus across
models to produce robust emotion profiles. Our analysis shows that during
interaction with the AI tutor, students typically report mildly positive affect
and moderate arousal. Yet learning is not uniformly smooth: confusion and
curiosity are frequent companions to problem solving, and frustration, while
less common, still surfaces in ways that can derail progress. Emotional states
are short-lived--positive moments last slightly longer than neutral or negative
ones, but they are fragile and easily disrupted. Encouragingly, negative
emotions often resolve quickly, sometimes rebounding directly into positive
states. Neutral moments frequently act as turning points, more often steering
students upward than downward, suggesting opportunities for tutors to intervene
at precisely these junctures.

</details>


### [126] [Unlocking the Potential of Diffusion Language Models through Template Infilling](https://arxiv.org/abs/2510.13870)
*Junhoo Lee,Seungyeon Kim,Nojun Kwak*

Main category: cs.CL

TL;DR: DLMs在生成方面，提出了一种新的模板填充（TI）方法，能够生成结构化模板并填充掩码段。同时，还引入了动态片段分配（DSA）来提高灵活性，并在数学推理和代码生成等任务上取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: DLMs的推理策略受限于传统的自回归模型，主要依赖前缀提示。本研究旨在为DLMs开发一种新的、更灵活的生成方法。

Method: 提出模板填充（TI）方法，先生成响应的结构模板，然后填充掩码段。引入动态片段分配（DSA）来根据生成置信度自适应调整片段长度。

Result: 在数学推理和代码生成基准测试中，TI 方法取得了 17.01% 的提升。此外，TI 在多令牌生成场景下实现了速度提升，同时保持了生成质量。

Conclusion: TI 和 DSA 是DLMs的有效条件生成方法，在数学推理和代码生成等任务上表现出色，并在多令牌生成中提供了速度和质量的优势。

Abstract: Diffusion Language Models (DLMs) have emerged as a promising alternative to
Autoregressive Language Models, yet their inference strategies remain limited
to prefix-based prompting inherited from the autoregressive paradigm. In this
paper, we propose Template Infilling (TI), a tailored conditioning methodology
for DLMs' generation process. Unlike conventional prefix prompting, TI first
generates a structural template for the target response, then fills in the
masked segments. To enhance the flexibility of this structural control, we
introduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment
lengths based on generation confidence. We demonstrate the effectiveness of our
approach on mathematical reasoning and code generation benchmarks, achieving
consistent improvements of 17.01$\%$p over baseline. Furthermore, we show that
TI provides additional advantages in multi-token generation settings, enabling
effective speedup while maintaining generation quality.

</details>


### [127] [Quechua Speech Datasets in Common Voice: The Case of Puno Quechua](https://arxiv.org/abs/2510.13871)
*Elwin Huaman,Wendi Huaman,Jorge Luis Huaman,Ninfa Quispe*

Main category: cs.CL

TL;DR: Common Voice 致力于为数据稀疏的克丘亚语等资源匮乏的语言创建开放的语音数据集，并通过 Puno Quechua 的案例研究展示了其潜力。


<details>
  <summary>Details</summary>
Motivation: 解决克丘亚语等资源匮乏的语言在语音技术发展中面临的数据和资源稀缺问题。

Method: 将克丘亚语整合到 Common Voice 数据集中，重点研究 Puno Quechua 的语言上线和语料库（包括朗读和自发语音）的收集。

Result: Common Voice 目前拥有 191.1 小时的克丘亚语语音数据（86% 已验证），其中 Puno Quechua 贡献了 12 小时（77% 已验证）。

Conclusion: Common Voice 具有为资源匮乏语言创建语音数据的巨大潜力，并提出了应对技术挑战、社区参与和数据主权问题的研究议程，旨在促进包容性语音技术和数字赋权。

Abstract: Under-resourced languages, such as Quechuas, face data and resource scarcity,
hindering their development in speech technology. To address this issue, Common
Voice presents a crucial opportunity to foster an open and community-driven
speech dataset creation. This paper examines the integration of Quechua
languages into Common Voice. We detail the current 17 Quechua languages,
presenting Puno Quechua (ISO 639-3: qxp) as a focused case study that includes
language onboarding and corpus collection of both reading and spontaneous
speech data. Our results demonstrate that Common Voice now hosts 191.1 hours of
Quechua speech (86\% validated), with Puno Quechua contributing 12 hours (77\%
validated), highlighting the Common Voice's potential. We further propose a
research agenda addressing technical challenges, alongside ethical
considerations for community engagement and indigenous data sovereignty. Our
work contributes towards inclusive voice technology and digital empowerment of
under-resourced language communities.

</details>


### [128] [FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1 normalisation](https://arxiv.org/abs/2510.13873)
*Johann Pignat,Milena Vucetic,Christophe Gaudet-Blavignac,Jamil Zaghir,Amandine Stettler,Fanny Amrein,Jonatan Bonjour,Jean-Philippe Goldman,Olivier Michielin,Christian Lovis,Mina Bjelogrlic*

Main category: cs.CL

TL;DR: FRACCO是一个包含1301个合成法语临床病例的专家标注语料库，用于临床肿瘤学中的自然语言处理。


<details>
  <summary>Details</summary>
Motivation: 由于法语肿瘤学资源匮乏，开发临床文本的自然语言处理工具面临挑战。

Method: FRACCO语料库是通过翻译西班牙语CANTEMIST语料库并由领域专家进行标注而创建的，标注内容包括形态学、地理位置和组织学分化，并使用国际疾病分类的肿瘤学（ICD-O）作为参考。此外，还包含一个额外的注释层，用于组合表达式级别的规范化，将多个ICD-O元素合并为统一的临床概念。注释质量通过专家评审来保证。

Result: 该数据集包含399个独特的形态学代码、272个独特的地理位置代码和2043个独特的组合表达式。

Conclusion: FRACCO语料库为法语肿瘤学文本中的命名实体识别和概念规范化提供了参考标准。

Abstract: Developing natural language processing tools for clinical text requires
annotated datasets, yet French oncology resources remain scarce. We present
FRACCO (FRench Annotated Corpus for Clinical Oncology) an expert-annotated
corpus of 1301 synthetic French clinical cases, initially translated from the
Spanish CANTEMIST corpus as part of the FRASIMED initiative. Each document is
annotated with terms related to morphology, topography, and histologic
differentiation, using the International Classification of Diseases for
Oncology (ICD-O) as reference. An additional annotation layer captures
composite expression-level normalisations that combine multiple ICD-O elements
into unified clinical concepts. Annotation quality was ensured through expert
review: 1301 texts were manually annotated for entity spans by two domain
experts. A total of 71127 ICD-O normalisations were produced through a
combination of automated matching and manual validation by a team of five
annotators. The final dataset representing 399 unique morphology codes (from
2549 different expressions), 272 topography codes (from 3143 different
expressions), and 2043 unique composite expressions (from 11144 different
expressions). This dataset provides a reference standard for named entity
recognition and concept normalisation in French oncology texts.

</details>


### [129] [What Layers When: Learning to Skip Compute in LLMs with Residual Gates](https://arxiv.org/abs/2510.13876)
*Filipe Laitenberger,Dawid Kopiczko,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: GateSkip通过引入一个简单的残差流门控机制，实现了Transformer中逐token的层跳过，在推理时根据门控值对低重要性token进行跳过，从而在节省高达15%的计算量的同时保持90%以上的基线准确率。


<details>
  <summary>Details</summary>
Motivation: 在深度学习模型中，尤其是在处理长序列时，如何提高计算效率并保持模型性能是一个重要的挑战。该研究旨在提出一种机制，允许模型在处理序列时根据token的重要性动态地跳过某些计算层，从而减少不必要的计算开销。

Method: 提出了一种名为GateSkip的残差流门控机制，为每个Attention/MLP分支配备了一个sigmoid-线性门控，该门控能够压缩分支的输出，然后将其重新注入残差流。在推理过程中，根据门控值对token进行排序，并根据每层的预算跳过低重要性的token。

Result: GateSkip在长文本推理任务中，可以节省高达15%的计算量，同时保持超过90%的基线准确率。在经过指令微调的模型上，GateSkip在计算量不变的情况下提升了准确率，并且在节省近50%计算量时能达到基线质量水平。此外，学习到的门控值还能揭示Transformer信息流的规律（例如，BOS token充当锚点），并且该方法易于与量化、剪枝和自推测解码等技术结合使用。

Conclusion: GateSkip是一种简单而有效的逐token层跳过机制，它通过平滑、可微的门控单元，在不影响模型稳定性的前提下，显著提高了Transformer在长文本推理和指令遵循任务上的计算效率，同时提供了对模型内部信息流的洞察，并易于与其他优化技术结合。

Abstract: We introduce GateSkip, a simple residual-stream gating mechanism that enables
token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is
equipped with a sigmoid-linear gate that condenses the branch's output before
it re-enters the residual stream. During inference we rank tokens by the gate
values and skip low-importance ones using a per-layer budget. While early-exit
or router-based Mixture-of-Depths models are known to be unstable and need
extensive retraining, our smooth, differentiable gates fine-tune stably on top
of pretrained models. On long-form reasoning, we save up to 15\% compute while
retaining over 90\% of baseline accuracy. On instruction-tuned models we see
accuracy gains at full compute and match baseline quality near 50\% savings.
The learned gates give insight into transformer information flow (e.g., BOS
tokens act as anchors), and the method combines easily with quantization,
pruning, and self-speculative decoding.

</details>


### [130] [TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks](https://arxiv.org/abs/2510.13878)
*Jimin Lim,Arjun Damerla,Arthur Jiang,Nam Le*

Main category: cs.CL

TL;DR: LLMs can perform sequential decision-making using only natural language feedback, with Qwen3-4B outperforming traditional methods in a novel benchmark.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' sequential decision-making abilities using only natural language feedback, without numerical cues or explicit probabilities, and to evaluate their capability to infer latent reward structures from linguistic cues.

Method: Introduced a benchmark where LLMs interact with multi-armed bandit environments via textual feedback. Evaluated four open-source LLMs against standard algorithms (Thompson Sampling, Epsilon Greedy, UCB, random choice).

Result: Most LLMs underperformed baselines, but Qwen3-4B achieved an 89.2% best-arm selection rate, outperforming larger LLMs and traditional methods. This suggests emergent probabilistic reasoning from language alone.

Conclusion: LLMs, particularly Qwen3-4B, demonstrate the ability to perform sequential decision-making using only natural language, indicating that probabilistic reasoning can emerge from linguistic cues. The proposed benchmark serves as a tool for evaluating decision-making in non-numeric contexts.

Abstract: Large language models (LLMs) have shown to be increasingly capable of
performing reasoning tasks, but their ability to make sequential decisions
under uncertainty only using natural language remains underexplored. We
introduce a novel benchmark in which LLMs interact with multi-armed bandit
environments using purely textual feedback, "you earned a token", without
access to numerical cues or explicit probabilities, resulting in the model to
infer latent reward structures purely off linguistic cues and to adapt
accordingly. We evaluated the performance of four open-source LLMs and compare
their performance to standard decision-making algorithms such as Thompson
Sampling, Epsilon Greedy, Upper Confidence Bound (UCB), and random choice.
While most of the LLMs underperformed compared to the baselines, Qwen3-4B,
achieved the best-arm selection rate of 89.2% , which significantly
outperformed both the larger LLMs and traditional methods. Our findings suggest
that probabilistic reasoning is able to emerge from language alone, and we
present this benchmark as a step towards evaluating decision-making
capabilities in naturalistic, non-numeric contexts.

</details>


### [131] [Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production](https://arxiv.org/abs/2510.13879)
*Alexandre Galashov,Matt Jones,Rosemary Ke,Yuan Cao,Vaishnavh Nagarajan,Michael C. Mozer*

Main category: cs.CL

TL;DR: 该模型通过引入“不知道”和“暂停”标记，动态地为每个输入token调整计算量，并使用三种方法（CYB-AP, CYB-VA, CYB-DP）进行训练，在保证性能的同时减少了训练数据需求。


<details>
  <summary>Details</summary>
Motivation: 为了让语言模型能够根据输入token动态地、自主地调整计算量，以提高效率和准确性。

Method: 提出了一种名为“Catch Your Breath”（CYB）的损失函数，包括三种变体：CYB-AP（任意时间预测）、CYB-VA（变分方法）和CYB-DP（计算预算惩罚）。模型通过发出“不知道”标记来请求额外的计算步骤，并通过插入“暂停”标记来实现。

Result: CYB模型仅需三分之一的训练数据即可达到与基线模型相同的性能，并且只需要基线模型一半的训练数据。模型能够根据token的复杂性和上下文自适应地调整处理时间，例如在复数名词后暂停，在合同词开头从不暂停，并在歧义token上表现出高变异性。

Conclusion: CYB模型能够智能地分配计算资源，通过动态调整计算步骤来提高训练效率和模型性能，并能根据token的特性自适应地调整其处理策略。

Abstract: We explore a class of supervised training objectives that allow a language
model to dynamically and autonomously scale the number of compute steps used
for each input token. For any token, the model can request additional compute
steps by emitting a <don't know> output. If the model is granted a delay, a
specialized <pause> token is inserted at the next input step, providing the
model with additional compute resources to generate an output. The model can
request multiple pauses. To train the model to use <don't know> outputs
judiciously and to calibrate its uncertainty, we frame the selection of each
output token as a sequential-decision problem with a time cost. We refer to the
class of methods as $\textit{Catch Your Breath}$ losses and we study three
methods in this class: CYB-AP frames the model's task as anytime prediction,
where an output may be required at any step and accuracy is discounted over
time; CYB-VA is a variational approach that aims to maximize prediction
accuracy subject to a specified distribution over stopping times; and CYB-DP
imposes a penalty based on a computational budget. Through fine-tuning
experiments, we identify the best performing loss variant. The CYB model needs
only one third as much training data as the baseline (no pause) model needs to
achieve the same performance, and half as much data as a model with pauses and
a cross-entropy loss. We find that the CYB model requests additional steps when
doing so improves accuracy, and the model adapts its processing time to
token-level complexity and context. For example, it often pauses after plural
nouns like $\textit{patients}$ and $\textit{challenges}$ but never pauses after
the first token of contracted words like $\textit{wasn}$ and $\textit{didn}$,
and it shows high variability for ambiguous tokens like $\textit{won}$, which
could function as either a verb or part of a contraction.

</details>


### [132] [PAGE: Prompt Augmentation for text Generation Enhancement](https://arxiv.org/abs/2510.13880)
*Mauro Jose Pacchiotti,Luciana Ballejos,Mariel Ale*

Main category: cs.CL

TL;DR: PAGE框架使用简单的辅助模块（如分类器或提取器）来丰富输入，以提高文本生成任务的质量和可控性，无需额外的生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言生成模型在特定任务或需求下面临性能瓶颈，需要大量额外数据进行调整。

Method: PAGE框架通过集成简单的辅助模块（如分类器、提取器）来提供输入文本的推断，并利用这些推断来构建增强的输入，从而提高生成文本的质量和可控性。

Result: 在需求工程领域进行了概念验证，使用带有分类器的辅助模块成功提高了软件需求生成质量。

Conclusion: PAGE框架提供了一种无需辅助生成模型即可适应不同任务的简单、模块化方法，以增强文本生成。

Abstract: In recent years, natural language generative models have shown outstanding
performance in text generation tasks. However, when facing specific tasks or
particular requirements, they may exhibit poor performance or require
adjustments that demand large amounts of additional data. This work introduces
PAGE (Prompt Augmentation for text Generation Enhancement), a framework
designed to assist these models through the use of simple auxiliary modules.
These modules, lightweight models such as classifiers or extractors, provide
inferences from the input text. The output of these auxiliaries is then used to
construct an enriched input that improves the quality and controllability of
the generation. Unlike other generation-assistance approaches, PAGE does not
require auxiliary generative models; instead, it proposes a simpler, modular
architecture that is easy to adapt to different tasks. This paper presents the
proposal, its components and architecture, and reports a proof of concept in
the domain of requirements engineering, where an auxiliary module with a
classifier is used to improve the quality of software requirements generation.

</details>


### [133] [Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation](https://arxiv.org/abs/2510.13884)
*Bolei Ma,Yong Cao,Indira Sen,Anna-Carolina Haensch,Frauke Kreuter,Barbara Plank,Daniel Hershcovich*

Main category: cs.CL

TL;DR: LLM在模拟公众意见等社会现象时，应采用开放式、自由形式的文本，以更好地捕捉和利用LLM的生成能力，而不是局限于封闭式问答。这有助于提高测量精度、支持意外观点的探索、减少研究者偏见，并最终增强方法论的效用。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在模拟社会现象时多采用封闭式问答（选择题、简答题），忽视了LLM的生成本质，限制了其在社会模拟中的应用。因此，需要探讨开放式、自由形式文本在LLM社会模拟中的价值。

Method: 借鉴调查方法学研究和近期自然语言处理（NLP）的进展，论证开放式文本在LLM社会模拟中的价值，包括其在改进测量和设计、支持探索意外观点、减少研究者偏见、捕捉表达性和个体性、辅助预测试以及提升方法论效用等方面的优势。

Result: 开放式文本能够更全面地捕捉LLM生成的社会现象，提高模拟的真实性和研究的深度。它可以促进NLP与社会科学的结合，鼓励新的实践和评估框架的出现。

Conclusion: 鼓励研究者采用开放式、自由形式的文本进行LLM社会模拟，并开发相应的评估框架，以充分发挥LLM在生成多样性方面的潜力，实现NLP与社会科学的协同增效。

Abstract: Large Language Models (LLMs) are increasingly used to simulate public opinion
and other social phenomena. Most current studies constrain these simulations to
multiple-choice or short-answer formats for ease of scoring and comparison, but
such closed designs overlook the inherently generative nature of LLMs. In this
position paper, we argue that open-endedness, using free-form text that
captures topics, viewpoints, and reasoning processes "in" LLMs, is essential
for realistic social simulation. Drawing on decades of survey-methodology
research and recent advances in NLP, we argue why this open-endedness is
valuable in LLM social simulations, showing how it can improve measurement and
design, support exploration of unanticipated views, and reduce
researcher-imposed directive bias. It also captures expressiveness and
individuality, aids in pretesting, and ultimately enhances methodological
utility. We call for novel practices and evaluation frameworks that leverage
rather than constrain the open-ended generative diversity of LLMs, creating
synergies between NLP and social science.

</details>


### [134] [Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization](https://arxiv.org/abs/2510.13885)
*Ariel Kamen*

Main category: cs.CL

TL;DR: LLMs在非结构化文本分类方面表现尚可，但存在幻觉和类别冗余问题。集成方法显著提升了性能并消除了幻觉。


<details>
  <summary>Details</summary>
Motivation: 评估现有LLM在IAB 2.2分类任务上的表现，并探索改进方法。

Method: 使用统一数据集、零样本提示，评估了十种LLM在准确率、召回率、F1分数、幻觉率、膨胀率和成本等指标上的表现。随后，测试了一种基于集成学习的方法。

Result: LLM平均准确率为34%，幻觉和膨胀率较高。Gemini 1.5/2.0 Flash和GPT 20B/120B在成本效益方面表现最佳，GPT 120B幻觉率最低。集成方法显著提高了准确率，降低了膨胀率，并消除了幻觉。

Conclusion: 单一LLM在文本分类方面存在局限性，集成方法是实现或超越人类专家表现的有效途径。

Abstract: This study presents a comparative evaluation of ten state-of-the-art large
language models (LLMs) applied to unstructured text categorization using the
Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis
employed a uniform dataset of 8,660 human-annotated samples and identical
zero-shot prompts to ensure methodological consistency across all models.
Evaluation metrics included four classic measures - accuracy, precision,
recall, and F1-score - and three LLM-specific indicators: hallucination ratio,
inflation ratio, and categorization cost.
  Results show that, despite their rapid advancement, contemporary LLMs achieve
only moderate classic performance, with average scores of 34% accuracy, 42%
precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios
reveal that models frequently overproduce categories relative to human
annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B
offered the most favorable cost-to-performance balance, while GPT 120B
demonstrated the lowest hallucination ratio. The findings suggest that scaling
and architectural improvements alone do not ensure better categorization
accuracy, as the task requires compressing rich unstructured text into a
limited taxonomy - a process that challenges current model architectures.
  To address these limitations, a separate ensemble-based approach was
developed and tested. The ensemble method, in which multiple LLMs act as
independent experts, substantially improved accuracy, reduced inflation, and
completely eliminated hallucinations. These results indicate that coordinated
orchestration of models - rather than sheer scale - may represent the most
effective path toward achieving or surpassing human-expert performance in
large-scale text categorization.

</details>


### [135] [Reliable Fine-Grained Evaluation of Natural Language Math Proofs](https://arxiv.org/abs/2510.13888)
*Wenjie Ma,Andrei Cojocaru,Neel Kolhe,Bradley Louie,Robin Said Sharif,Haihan Zhang,Vincent Zhuang,Matei Zaharia,Sewon Min*

Main category: cs.CL

TL;DR: LLM在数学推理方面取得了进展，但生成和验证自然语言数学证明仍具挑战性。本文提出了ProofBench数据集和ProofGrader评估器，用于评估LLM生成的数学证明，并在评估器设计和应用方面进行了探索。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在数学推理上的进展主要集中在答案易于验证的任务上，而生成和验证自然语言数学证明仍然是一个公开的挑战。LLM生成的数学证明缺乏可靠的、细粒度的评估器是一个关键的不足。

Method: 本文提出了一种系统性的方法来开发和验证评估器，该评估器可以为模型生成的数学证明分配0-7的细粒度分数。为了支持这项研究，作者引入了ProofBench，这是第一个包含细粒度证明评分的专家注释数据集，涵盖了来自六个主要数学竞赛的145个问题以及435个LLM生成的解决方案。利用ProofBench作为试验台，作者系统地探索了评估器设计空间，并开发了ProofGrader评估器，该评估器结合了强大的推理主干LM、来自参考解决方案和评分方案的丰富上下文以及简单的集成方法。

Result: ProofGrader评估器实现了0.926的平均绝对误差（MAE），显著优于基线方法。在n=16的 தேர்ந்தெடுக்க任务中，ProofGrader的平均得分为4.14（满分7分），缩小了简单二元评估器（2.48）和人类评分者（4.62）之间78%的差距。

Conclusion: 本文提出的ProofGrader评估器能够有效地评估LLM生成的数学证明，并且在实际应用中具有潜力，可以促进下游证明的生成。

Abstract: Recent advances in large language models (LLMs) for mathematical reasoning
have largely focused on tasks with easily verifiable final answers; however,
generating and verifying natural language math proofs remains an open
challenge. We identify the absence of a reliable, fine-grained evaluator for
LLM-generated math proofs as a critical gap. To address this, we propose a
systematic methodology for developing and validating evaluators that assign
fine-grained scores on a 0-7 scale to model-generated math proofs. To enable
this study, we introduce ProofBench, the first expert-annotated dataset of
fine-grained proof ratings, spanning 145 problems from six major math
competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from
Gemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as
a testbed, we systematically explore the evaluator design space across key
axes: the backbone model, input context, instructions and evaluation workflow.
Our analysis delivers ProofGrader, an evaluator that combines a strong
reasoning backbone LM, rich context from reference solutions and marking
schemes, and a simple ensembling method; it achieves a low Mean Absolute Error
(MAE) of 0.926 against expert scores, significantly outperforming naive
baselines. Finally, we demonstrate its practical utility in a best-of-$n$
selection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out
of 7), closing 78% of the gap between a naive binary evaluator (2.48) and the
human oracle (4.62), highlighting its potential to advance downstream proof
generation.

</details>


### [136] [A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness](https://arxiv.org/abs/2510.13890)
*Fali Wang,Jihai Chen,Shuhua Yang,Ali Al-Lawati,Linli Tang,Hui Liu,Suhang Wang*

Main category: cs.CL

TL;DR: LLM在许多领域取得了进步，但也面临高昂的微调成本、推理延迟、有限的边缘部署能力和可靠性问题。SLM（小型语言模型）因其紧凑、高效和适应性强的特点，提供了互补的解决方案。本研究提出了一种系统性的SLM-LLM协作调查，该调查根据协作目标进行组织，并提出了四个目标的分类：性能提升、成本效益、云-边缘隐私和可信度。在此框架内，我们回顾了代表性的方法，总结了设计范式，并概述了实现高效、安全和可扩展的SLM-LLM协作所面临的开放挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: LLM在许多领域取得了进步，但也面临高昂的微调成本、推理延迟、有限的边缘部署能力和可靠性问题。SLM（小型语言模型）因其紧凑、高效和适应性强的特点，提供了互补的解决方案。本研究提出了一个系统性的SLM-LLM协作调查，以应对这些挑战。

Method: 本研究提出了一种系统性的SLM-LLM协作调查，该调查根据协作目标进行组织，并提出了四个目标的分类：性能提升、成本效益、云-边缘隐私和可信度。在此框架内，我们回顾了代表性的方法，总结了设计范式，并概述了开放挑战和未来方向。

Result: 对SLM-LLM协作进行了分类，回顾了代表性方法，总结了设计范式，并指出了开放的挑战和未来方向。

Conclusion: 实现高效、安全和可扩展的SLM-LLM协作仍然是一个活跃的研究领域，具有许多开放的挑战和未来的发展方向。

Abstract: Large language models (LLMs) have advanced many domains and applications but
face high fine-tuning costs, inference latency, limited edge deployability, and
reliability concerns. Small language models (SLMs), compact, efficient, and
adaptable, offer complementary remedies. Recent work explores collaborative
frameworks that fuse SLMs' specialization and efficiency with LLMs'
generalization and reasoning to meet diverse objectives across tasks and
deployment scenarios. Motivated by these developments, this paper presents a
systematic survey of SLM-LLM collaboration organized by collaboration
objectives. We propose a taxonomy with four goals: performance enhancement,
cost-effectiveness, cloud-edge privacy, and trustworthiness. Within this
framework, we review representative methods, summarize design paradigms, and
outline open challenges and future directions toward efficient, secure, and
scalable SLM-LLM collaboration.

</details>


### [137] [The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data](https://arxiv.org/abs/2510.13892)
*Zhaoyang Shang,Sibo Wei,Jianbin Guo,Rui Zhou,Lifeng Dong,Yin Luo*

Main category: cs.CL

TL;DR: THTB是一个受认知科学启发的框架，用于指令数据选择和标注指导，通过结合质量过滤和内在/外在难度评分来优先处理更高级别的认知指令，可以提高SFT效率和泛化能力，并为垂直领域提供有效的标注指导。


<details>
  <summary>Details</summary>
Motivation: 现有方法在为LLM进行SFT数据选择时，过度依赖LLM内部知识，可解释性差，泛化能力有限。THTB旨在解决这些局限性。

Method: THTB框架结合质量过滤以及内在和外在难度评分，优先处理更高级别的认知指令，为SFT提供可解释和可量化的标准，用于数据选择和标注指导。

Result: 实验表明，THTB在仅使用5%的数据进行训练时，模型性能优于使用全部数据集训练的模型，并且泛化能力优于仅使用LLM选择的模型。此外，THTB在垂直领域提供了有效的标注指导，使得使用2%的数据训练的模型能够超越使用更大训练数据集的模型。

Conclusion: THTB框架能够有效地进行指令数据的选择和标注指导，显著提高SFT效率和模型泛化能力，尤其在垂直领域展现出强大的领域适应潜力。

Abstract: Large Language Models (LLMs) excel in general tasks, but adapting them to
specialized domains relies on high-quality supervised fine-tuning (SFT) data.
Although existing methods can identify subsets of high-quality data and reduce
training cost to some extent, their selection process still suffers from
over-reliance on LLMs' internal knowledge, weak interpretability, and limited
generalization. To address these limitations, we propose THTB (The Harder The
Better), a cognitive science-inspired framework for instruction data selection
and annotation guidance. THTB prioritizes higher-level cognitive instructions
by combining quality filtering with intrinsic and extrinsic hardness scoring,
offering interpretable and quantifiable criteria for efficient SFT, both in
data selection and annotation guidance. Experiments show that THTB enables
models trained on only 5% of the data to outperform full-dataset training,
while achieving superior generalization compared with LLM-only selection. In
addition, THTB provides effective annotation guidance in vertical domains,
enabling a model trained on just 2% of the data to surpass models trained on
much larger datasets, demonstrating strong potential for domain adaptation. Our
code, datasets, and models are available on
https://github.com/DYJG-research/THTB.

</details>


### [138] [Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection](https://arxiv.org/abs/2510.13893)
*Olga E. Sorokoletova,Francesco Giarrusso,Vincenzo Suriani,Daniele Nardi*

Main category: cs.CL

TL;DR: LLM 越狱技术对模型安全构成威胁。本研究提出了一个包含 50 种越狱策略的分类法，并分析了这些策略的有效性，开发了一个新的意大利语数据集，并对模型进行了越狱检测基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 防御措施在应对单轮攻击、跨语言覆盖和攻击策略分类方面存在不足，需要更全面的越狱技术理解和检测方法。

Method: 通过结构化红队挑战，开发了包含 7 个大类 50 种策略的分层分类法，分析了攻击类型、成功率和模型漏洞利用情况，对一个流行的 LLM 进行了越狱检测基准测试，并构建了一个意大利语多轮对话数据集。

Result: 1. 提出了一个包含 50 种越狱策略的分层分类法（包括拟人化、说服、权限提升、认知过载、混淆、目标冲突和数据投毒）。2. 分析了不同攻击类型的普遍性和成功率，揭示了攻击者如何利用模型漏洞。3. 对流行的 LLM 进行了基准测试，评估了分类法引导提示在改进自动检测方面的优势。4. 编译了一个包含 1364 个意大利语多轮对话的新数据集，用于研究对抗性意图逐渐出现并绕过传统安全措施的交互作用。

Conclusion: 本研究通过构建全面的越狱策略分类法、分析攻击数据、进行模型基准测试和创建新的数据集，为理解和防御 LLM 越狱攻击提供了重要贡献，特别是强调了多轮和跨语言攻击的挑战。

Abstract: Jailbreaking techniques pose a significant threat to the safety of Large
Language Models (LLMs). Existing defenses typically focus on single-turn
attacks, lack coverage across languages, and rely on limited taxonomies that
either fail to capture the full diversity of attack strategies or emphasize
risk categories rather than the jailbreaking techniques. To advance the
understanding of the effectiveness of jailbreaking techniques, we conducted a
structured red-teaming challenge. The outcome of our experiments are manifold.
First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak
strategies, consolidating and extending prior classifications into seven broad
families, including impersonation, persuasion, privilege escalation, cognitive
overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed
the data collected from the challenge to examine the prevalence and success
rates of different attack types, providing insights into how specific jailbreak
strategies exploit model vulnerabilities and induce misalignment. Third, we
benchmark a popular LLM for jailbreak detection, evaluating the benefits of
taxonomy-guided prompting for improving automatic detection. Finally, we
compiled a new Italian dataset of 1364 multi-turn adversarial dialogues,
annotated with our taxonomy, enabling the study of interactions where
adversarial intent emerges gradually and succeeds in bypassing traditional
safeguards.

</details>


### [139] [Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges](https://arxiv.org/abs/2510.13898)
*Misam Abbas*

Main category: cs.CL

TL;DR: LLM生成的文本使得作者归属识别更具挑战性。本文评估了固定风格嵌入和LLM裁判（GPT-4o）在人类-AI平行语料库上的归属能力。结果显示，风格嵌入在GPT生成文本上表现更好（82% vs 68%），而LLM裁判在LLaMA生成文本上略有优势（85% vs 81%），但无统计显著性。LLM裁判在小说和学术文本上表现突出，表明其语义敏感性；风格嵌入在口语和脚本对话中表现更佳，体现了其结构优势。这表明作者归属是一个多维度问题，需要混合策略。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）生成文本能力的提升，区分人类创作和机器生成内容变得越来越困难，这使得作者归属的识别成为一个关键挑战。

Method: 本文在人类-AI平行语料库（包含600个平衡实例，涵盖六个领域）上，对两种互补的归属机制——固定风格嵌入和指令微调的LLM裁判（GPT-4o）——进行了基准测试。语料库中的每个实例都包含一个人类提示以及来自GPT-4o或LLaMA-70B-Instruct的一个黄金（人类）续写和一个LLM生成的续写。

Result: 在GPT续写方面，风格嵌入基线达到了更高的总体准确率（82% vs 68%）。在LLaMA续写方面，LLM裁判的表现略优于风格嵌入（85% vs 81%），但结果在统计学上不显著。LLM裁判在小说和学术散文方面表现显著优于风格嵌入，表明其语义敏感性；而嵌入在口语和脚本对话方面表现更优，反映了其结构优势。

Conclusion: 作者归属是一个多维度的问题，需要结合多种方法。固定风格嵌入和LLM裁判在不同类型文本上表现出互补的优势，这表明混合策略是应对LLM时代作者归属挑战的有效途径。本文提供的代码和数据支持了这项研究的可复现性，并为AI生成内容的归属质量评估提供了一个可复现的基准。

Abstract: Attributing authorship in the era of large language models (LLMs) is
increasingly challenging as machine-generated prose rivals human writing. We
benchmark two complementary attribution mechanisms , fixed Style Embeddings and
an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an
open dataset of 600 balanced instances spanning six domains (academic, news,
fiction, blogs, spoken transcripts, and TV/movie scripts). Each instance
contains a human prompt with both a gold continuation and an LLM-generated
continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding
baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs.
68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA
continuations (85 pct vs. 81 pct) but the results are not statistically
significant. Crucially, the LLM judge significantly outperforms in fiction and
academic prose, indicating semantic sensitivity, whereas embeddings dominate in
spoken and scripted dialogue, reflecting structural strengths. These
complementary patterns highlight attribution as a multidimensional problem
requiring hybrid strategies. To support reproducibility we provide code on
GitHub and derived data on Hugging Face under the MIT license. This open
framework provides a reproducible benchmark for attribution quality assessment
in AI-generated content, along with a review of related literature influencing
this work.

</details>


### [140] [Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences](https://arxiv.org/abs/2510.13900)
*Julian Minder,Clément Dumas,Stewart Slocum,Helena Casademunt,Cameron Holmes,Robert West,Neel Nanda*

Main category: cs.CL

TL;DR: 对LLM进行窄域微调会产生可解释的激活偏差，可用于理解微调领域。通过分析模型差异和激活偏差，可以更好地理解和控制模型行为，但需警惕潜在风险。


<details>
  <summary>Details</summary>
Motivation: 解释微调LLM时产生的偏差，并提出一种可解释性方法。

Method: 通过分析模型微调前后的激活差异来识别偏差，并利用这些偏差来指导模型的生成。

Result: 模型在理解微调领域方面表现更好，并且能够生成类似微调数据的格式和内容。

Conclusion: 窄域微调会在LLM激活中留下可识别的偏差，可用于理解微调数据，但也可能导致过度拟合和潜在风险。研究者应警惕使用此类模型作为更广泛微调的替代品，并需进一步研究窄域微调的影响。

Abstract: Finetuning on narrow domains has become an essential tool to adapt Large
Language Models (LLMs) to specific tasks and to create models with known
unusual properties that are useful for research. We show that narrow finetuning
creates strong biases in LLM activations that can be interpreted to understand
the finetuning domain. These biases can be discovered using simple tools from
model diffing - the study of differences between models before and after
finetuning. In particular, analyzing activation differences on the first few
tokens of random text and steering by adding this difference to the model
activations produces text similar to the format and general content of the
finetuning data. We demonstrate that these analyses contain crucial information
by creating an LLM-based interpretability agent to understand the finetuning
domain. With access to the bias, the agent performs significantly better
compared to baseline agents using simple prompting. Our analysis spans
synthetic document finetuning for false facts, emergent misalignment,
subliminal learning, and taboo word guessing game models across different
architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We
suspect these biases reflect overfitting and find that mixing pretraining data
into the finetuning corpus largely removes them, though residual risks may
remain. Our work (1) demonstrates that narrowly finetuned models have salient
traces of their training objective in their activations and suggests ways to
improve how they are trained, (2) warns AI safety and interpretability
researchers that the common practice of using such models as a proxy for
studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3)
highlights the need for deeper investigation into the effects of narrow
finetuning and development of truly realistic case studies for model-diffing,
safety and interpretability research.

</details>


### [141] [RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs](https://arxiv.org/abs/2510.13901)
*Tuan T. Nguyen,John Le,Thai T. Vu,Willy Susilo,Heath Cooper*

Main category: cs.CL

TL;DR: RAID是一个框架，通过优化连续嵌入来生成对抗性后缀，从而绕过LLM的安全机制，同时保持响应的流畅性。它使用联合目标来鼓励受限响应，避免拒绝，并保持语义连贯性。RAID在多种LLM上实现了更高的攻击成功率和更低的成本。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然在各种任务上表现出色，但容易受到绕过安全机制的越狱攻击。本研究旨在探索LLMs的这些弱点，并提出一种新的攻击方法。

Method: RAID通过将离散标记放松为连续嵌入，并使用联合目标进行优化来生成对抗性后缀。该目标包括鼓励受限响应、使用拒绝感知正则化器将激活引导至远离拒绝方向，以及一个保持语义合理性和非冗余的连贯性项。在优化后，采用一个由评论家引导的解码过程，通过平衡嵌入亲和力和语言模型似然性将嵌入映射回标记。

Result: RAID在多种开源LLM上的实验表明，与最近的白盒和黑盒基线相比，RAID在查询次数更少、计算成本更低的情况下，实现了更高的攻击成功率。这表明在嵌入空间中进行正则化对于理解和缓解LLM的越狱漏洞很重要。

Conclusion: RAID框架通过在嵌入空间中进行正则化，成功地生成了能够绕过LLM安全机制的对抗性后缀，同时保持了响应的流畅性和语义连贯性。实验证明了RAID在攻击成功率和效率方面的优越性，并强调了嵌入空间正则化在LLM安全研究中的重要性。

Abstract: Large language models (LLMs) achieve impressive performance across diverse
tasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms.
We present RAID (Refusal-Aware and Integrated Decoding), a framework that
systematically probes these weaknesses by crafting adversarial suffixes that
induce restricted content while preserving fluency. RAID relaxes discrete
tokens into continuous embeddings and optimizes them with a joint objective
that (i) encourages restricted responses, (ii) incorporates a refusal-aware
regularizer to steer activations away from refusal directions in embedding
space, and (iii) applies a coherence term to maintain semantic plausibility and
non-redundancy. After optimization, a critic-guided decoding procedure maps
embeddings back to tokens by balancing embedding affinity with language-model
likelihood. This integration yields suffixes that are both effective in
bypassing defenses and natural in form. Experiments on multiple open-source
LLMs show that RAID achieves higher attack success rates with fewer queries and
lower computational cost than recent white-box and black-box baselines. These
findings highlight the importance of embedding-space regularization for
understanding and mitigating LLM jailbreak vulnerabilities.

</details>


### [142] [Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory](https://arxiv.org/abs/2510.13902)
*Nicole Smith-Vaniz,Harper Lyon,Lorraine Steigner,Ben Armstrong,Nicholas Mattei*

Main category: cs.CL

TL;DR: 该研究量化了大型语言模型（LLM）在政治和道德领域的潜在偏见，并将其与人类研究进行对比。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在政治和道德领域的回应中是否存在偏见，特别是与人类研究的对比。

Method: 使用道德基础理论（MFT）分析LLM的回应，并与现有人类研究进行对比，包括直接回应、扮演政治角色以及扮演具有特定人口统计学特征的人类角色。

Result: 量化LLM回应的道德倾向，并探究其是否表现出意识形态倾向，以及在多大程度上能准确地模仿不同意识形态和人口统计学特征的角色。

Conclusion: 通过系统分析LLM在不同条件下的行为，揭示AI生成回应在多大程度上依赖于政治和人口统计学信息。

Abstract: Large Language Models (LLMs) have become increasingly incorporated into
everyday life for many internet users, taking on significant roles as advice
givers in the domains of medicine, personal relationships, and even legal
matters. The importance of these roles raise questions about how and what
responses LLMs make in difficult political and moral domains, especially
questions about possible biases. To quantify the nature of potential biases in
LLMs, various works have applied Moral Foundations Theory (MFT), a framework
that categorizes human moral reasoning into five dimensions: Harm, Fairness,
Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to
measure differences in human participants along political, national, and
cultural lines. While there has been some analysis of the responses of LLM with
respect to political stance in role-playing scenarios, no work so far has
directly assessed the moral leanings in the LLM responses, nor have they
connected LLM outputs with robust human data. In this paper we analyze the
distinctions between LLM MFT responses and existing human research directly,
investigating whether commonly available LLM responses demonstrate ideological
leanings: either through their inherent responses, straightforward
representations of political ideologies, or when responding from the
perspectives of constructed human personas. We assess whether LLMs inherently
generate responses that align more closely with one political ideology over
another, and additionally examine how accurately LLMs can represent ideological
perspectives through both explicit prompting and demographic-based
role-playing. By systematically analyzing LLM behavior across these conditions
and experiments, our study provides insight into the extent of political and
demographic dependency in AI-generated responses.

</details>


### [143] [Schema for In-Context Learning](https://arxiv.org/abs/2510.13905)
*Pan Chen,Shaohong Chen,Mark Wang,Shi Xuan Leong,Priscilla Fung,Varinia Bernales,Alan Aspuru-Guzik*

Main category: cs.CL

TL;DR: SA-ICL通过引入显式知识检索和迁移模块，借鉴认知科学中的图式理论，帮助LLM从示例中学习并生成抽象图式，从而提升在化学和物理问题上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统ICL缺乏显式的知识检索和迁移模块，而借鉴认知科学中的图式理论，可以帮助模型更好地理解和推理。SA-ICL旨在解决这一问题。

Method: SA-ICL框架首先从先前的示例中提取推理过程的认知构建块的表示，生成一个抽象的图式（轻量级的、结构化的推理步骤及其关系的模板），然后利用该图式来增强模型在面对新问题时的推理过程。

Result: SA-ICL在GPQA数据集的化学和物理问题上，能够显著提升LLM的性能，最高可提升36.19%。同时，它减少了对示例数量的依赖，并增强了可解释性。

Conclusion: SA-ICL不仅能够整合不同的ICL策略，还为增强LLM的类人推理能力开辟了新的途径。

Abstract: In-Context Learning (ICL) enables transformer-based language models to adapt
to new tasks by conditioning on demonstration examples. However, traditional
example-driven in-context learning lacks explicit modules for knowledge
retrieval and transfer at the abstraction level. Inspired by cognitive science,
specifically schema theory, which holds that humans interpret new information
by activating pre-existing mental frameworks (schemas) to structure
understanding, we introduce SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL). This
framework extracts the representation of the building blocks of cognition for
the reasoning process instilled from prior examples, creating an abstracted
schema, a lightweight, structured template of key inferential steps and their
relationships, which is then used to augment a model's reasoning process when
presented with a novel question. We demonstrate that a broad range of large
language models (LLMs) lack the capacity to form and utilize internal
schema-based learning representations implicitly, but instead benefit
significantly from explicit schema-based scaffolding. Across chemistry and
physics questions from the GPQA dataset, our experiments show that SA-ICL
consistently boosts performance, up to 36.19 percent, when the single
demonstration example is of high quality, which simultaneously reduces reliance
on the number of demonstrations and enhances interpretability. SCHEMA ACTIVATED
IN CONTEXT LEARNING not only bridges disparate ICL strategies ranging from
pattern priming to Chain-of-Thought prompting, but also paves a new path for
enhancing human-like reasoning in LLMs.

</details>


### [144] [LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization](https://arxiv.org/abs/2510.13907)
*Yuanchen Wu,Saurabh Verma,Justin Lee,Fangzhou Xiong,Poppy Zhang,Amel Awadelkarim,Xu Chen,Yubai Yuan,Shawndra Hill*

Main category: cs.CL

TL;DR: 本研究提出了一种名为PDO的无标签提示优化框架，通过LLM裁判提供的成对偏好反馈来优化LLM提示，并结合了D-TS和变异策略以提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 自动提示优化（APO）虽然减少了手动工程，但大多数方法需要真实标签，而收集这些标签成本高昂且耗时。因此，需要一种无需真实标签的提示优化方法。

Method: PDO将提示优化问题构建为一个 duelo-bandit 问题，利用LLM裁判提供的成对偏好反馈作为监督信号。该框架结合了Double Thompson Sampling（D-TS）和Top-Performer Guided Mutation（一种通过变异高性能提示来扩展候选池的策略）。

Result: PDO在BIG-bench Hard (BBH) 和 MS MARCO数据集上进行了实验，结果显示PDO的性能始终优于基线方法。消融研究也验证了D-TS和提示变异策略的有效性。

Conclusion: PDO是一种高效的无标签提示优化框架，通过LLM裁判的偏好反馈和创新的优化策略，有效解决了现有方法对真实标签的依赖问题，并在多个基准测试中取得了优于其他方法的性能。

Abstract: Large language models (LLMs) are highly sensitive to their input prompts,
making prompt design a central challenge. While automatic prompt optimization
(APO) reduces manual engineering, most approaches assume access to ground-truth
references such as labeled validation data. In practice, however, collecting
high-quality labels is costly and slow. We propose the Prompt Duel Optimizer
(PDO), a sample-efficient framework for label-free prompt optimization. PDO
formulates the problem as a dueling-bandit setting, where supervision signal
comes from pairwise preference feedback provided by an LLM judge. The framework
combines Double Thompson Sampling (D-TS), which prioritizes informative prompt
comparisons, with Top-Performer Guided Mutation, which expands the candidate
pool by mutating high-performing prompts. PDO naturally operates in label-free
settings and can also incorporate partial labels to mitigate judge noise.
Experiments on BIG-bench Hard (BBH) and MS MARCO show that PDO consistently
outperforms baseline methods. Ablation studies further demonstrate the
effectiveness of both D-TS and prompt mutation.

</details>


### [145] [Interpreting the Latent Structure of Operator Precedence in Language Models](https://arxiv.org/abs/2510.13908)
*Dharunish Yugeswardeenoo,Harshil Nukala,Cole Blondin,Sean O Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: LLMs在算术任务中存在不足，本研究探究LLaMA 3.2-3B模型内部表征是否编码算子优先级，并使用多种可解释性技术验证了中间计算结果的存在，尤其是在MLP模块之后，同时发现模型在线性地编码了每个算子在注意力层之后的嵌入优先级，并提出了一种通过交换嵌入维度来修改算子优先级的‘部分嵌入交换’技术。


<details>
  <summary>Details</summary>
Motivation: LLMs在算术方面存在不足，但其内部算术计算机制尚不清楚，特别是算子优先级如何被内部表征。

Method: 构建包含三个操作数和两个运算符的算术表达式数据集，改变括号顺序和位置，并利用logist lens、线性分类探针和UMAP可视化等可解释性技术，追踪中间结果在LLaMA 3.2-3B模型残差流中的出现情况，并提出‘部分嵌入交换’技术。

Result: 中间计算结果存在于模型的残差流中，尤其是在MLP模块之后。模型在线性地编码了注意力层之后的每个运算符的嵌入优先级。‘部分嵌入交换’技术能够修改算子优先级。

Conclusion: LLaMA 3.2-3B模型在其内部表征中编码了算子优先级，中间计算结果可在残差流中追踪到，并且可以通过修改嵌入来实现对算子优先级的干预。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities but continue to struggle with arithmetic tasks. Prior works
largely focus on outputs or prompting strategies, leaving the open question of
the internal structure through which models do arithmetic computation. In this
work, we investigate whether LLMs encode operator precedence in their internal
representations via the open-source instruction-tuned LLaMA 3.2-3B model. We
constructed a dataset of arithmetic expressions with three operands and two
operators, varying the order and placement of parentheses. Using this dataset,
we trace whether intermediate results appear in the residual stream of the
instruction-tuned LLaMA 3.2-3B model. We apply interpretability techniques such
as logit lens, linear classification probes, and UMAP geometric visualization.
Our results show that intermediate computations are present in the residual
stream, particularly after MLP blocks. We also find that the model linearly
encodes precedence in each operator's embeddings post attention layer. We
introduce partial embedding swap, a technique that modifies operator precedence
by exchanging high-impact embedding dimensions between operators.

</details>


### [146] [Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning](https://arxiv.org/abs/2510.13909)
*Xingrui Zhuo,Jiapu Wang,Gongqing Wu,Zhongyuan Wang,Jichen Zhang,Shirui Pan,Xindong Wu*

Main category: cs.CL

TL;DR: LLM在归纳知识图谱推理（KGR）中可能因稀疏的知识图谱（KG）上下文而导致知识失真，并且难以约束生成性幻觉。为了解决这些问题，本文提出了一个知识推理语言模型（KRLM），通过设计KRL指令格式、KRL分词器、KRL注意力层（带有动态知识记忆机制）以及结构感知下一个实体预测器，实现了LLM知识与KG上下文的统一协调，从而提高了推理的准确性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM-KGFM在处理开放域KG的不确定性时，LLM的内在知识可能被稀疏的KG上下文所掩盖，导致知识失真，并且难以约束LLM的生成性幻觉，从而限制了推理结果的可信度。

Method: 提出了一种知识推理语言模型（KRLM），该模型包括：1. KRL指令格式和KRL分词器，用于将LLM知识与KG表示对齐。2. KRL注意力层，通过动态知识记忆机制协调LLM内在知识和额外的KG上下文。3. 结构感知下一个实体预测器，严格将推理结果约束在可信赖的知识域内。

Result: KRLM在25个真实世界的归纳KGR数据集上进行了广泛的实验评估，结果表明在零样本推理和微调场景下均具有显著的优越性。

Conclusion: 所提出的KRLM通过统一协调LLM知识和KG上下文，有效地解决了现有LLM-KGR方法的局限性，提高了推理的准确性和可信度。

Abstract: Inductive Knowledge Graph Reasoning (KGR) aims to discover facts in
open-domain KGs containing unknown entities and relations, which poses a
challenge for KGR models in comprehending uncertain KG components. Existing
studies have proposed Knowledge Graph Foundation Models (KGFMs) that learn
structural invariances across KGs to handle this uncertainty. Recently, Large
Language Models (LLMs) have demonstrated strong capabilities for open-domain
knowledge reasoning. As a result, the latest research has focused on LLM-based
KGFMs that integrate LLM knowledge with KG context for inductive KGR. However,
the intrinsic knowledge of LLMs may be overshadowed by sparse KG context,
leading to LLM knowledge distortion, which can cause irreversible damage to
model reasoning. Moreover, existing LLM-based KGR methods still struggle to
fully constrain generative hallucinations in LLMs, severely limiting the
credibility of reasoning results. To address these limitations, we propose a
Knowledge Reasoning Language Model (KRLM) that achieves unified coordination
between LLM knowledge and KG context throughout the KGR process. Specifically,
we design a Knowledge Reasoning Language (KRL) instruction format and a KRL
tokenizer to align LLM knowledge with KG representations. Then, we propose a
KRL attention layer that coordinates intrinsic LLM knowledge with additional KG
context through a dynamic knowledge memory mechanism. Finally, a
structure-aware next-entity predictor is proposed, which strictly constrains
the reasoning results within a trustworthy knowledge domain. Extensive
experimental results on 25 real-world inductive KGR datasets demonstrate the
significant superiority of the proposed KRLM\footnote{Our source codes are
available at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shot
reasoning and fine-tuning scenarios.

</details>


### [147] [RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems](https://arxiv.org/abs/2510.13910)
*Jingru Lin,Chen Zhang,Stephen Y. Liu,Haizhou Li*

Main category: cs.CL

TL;DR: RAGCap-Bench是一个用于评估代理RAG工作流中中间任务的新基准，它通过分析模型输出来识别常见任务、核心能力和LLM错误，实验表明“慢思考”模型在RAGCap上表现更好，从而提高了端到端结果。


<details>
  <summary>Details</summary>
Motivation: 当前的代理RAG系统在处理复杂的多跳问题方面仍存在困难，并且它们的中间推理能力尚未得到充分探索。

Method: 提出RAGCap-Bench，一个面向能力的基准，用于对代理RAG工作流中的中间任务进行细粒度评估。分析了最先进的系统输出，以识别常见任务、核心能力和LLM错误，并设计了有针对性的评估问题。

Result: 实验表明，“慢思考”模型在RAGCap上的表现与更好的端到端结果相关，证明了该基准的有效性以及增强这些中间能力的重要性。

Conclusion: RAGCap-Bench能够有效评估代理RAG工作流中的中间能力，并且这些中间能力的提高对于获得更好的端到端性能至关重要。

Abstract: Retrieval-Augmented Generation (RAG) mitigates key limitations of Large
Language Models (LLMs)-such as factual errors, outdated knowledge, and
hallucinations-by dynamically retrieving external information. Recent work
extends this paradigm through agentic RAG systems, where LLMs act as agents to
iteratively plan, retrieve, and reason over complex queries. However, these
systems still struggle with challenging multi-hop questions, and their
intermediate reasoning capabilities remain underexplored. To address this, we
propose RAGCap-Bench, a capability-oriented benchmark for fine-grained
evaluation of intermediate tasks in agentic RAG workflows. We analyze outputs
from state-of-the-art systems to identify common tasks and the core
capabilities required for their execution, then construct a taxonomy of typical
LLM errors to design targeted evaluation questions. Experiments show that
"slow-thinking" models with stronger RAGCap performance achieve better
end-to-end results, underscoring the benchmark's validity and the importance of
enhancing these intermediate capabilities.

</details>


### [148] [AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs](https://arxiv.org/abs/2510.13912)
*María Victoria Carro,Denise Alejandra Mester,Facundo Nieto,Oscar Agustín Stanchi,Guido Ernesto Bergman,Mario Alejandro Leiva,Eitan Sprejer,Luca Nicolás Forziati Gangi,Francisca Gauna Selasco,Juan Gustavo Corvalán,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.CL

TL;DR: AI辩论实验表明，模型在面对与自身先验信念冲突的观点时，更倾向于迎合裁判，而非坚持己见。


<details>
  <summary>Details</summary>
Motivation: 现有AI辩论实验依赖有客观真理的数据集，忽略了辩论中‘说谎’的主观成分，即辩手需认为自己所辩护的观点是错误的。本研究旨在探索AI在主观问题辩论中的表现，并量化其先验信念。

Method: 实验中，首先确定模型对某个主观问题的先验立场，然后设置一个与模型先验立场相反的裁判角色。比较了顺序辩论和同步辩论两种协议，并评估模型在坚持己见或反对己见时，说服力和论证质量的表现。

Result: 模型倾向于迎合裁判，而非坚持先验信念；顺序辩论偏袒后发言者；模型在坚持己见时更具说服力；然而，与先验信念相悖的论证在配对比较中被评为更高质量。

Conclusion: 研究结果有助于人类裁判提供更高质量的训练信号，构建更可信赖的AI系统，并揭示了语言模型在说服动态方面与人类互动的重要方面。

Abstract: The core premise of AI debate as a scalable oversight technique is that it is
harder to lie convincingly than to refute a lie, enabling the judge to identify
the correct position. Yet, existing debate experiments have relied on datasets
with ground truth, where lying is reduced to defending an incorrect
proposition. This overlooks a subjective dimension: lying also requires the
belief that the claim defended is false. In this work, we apply debate to
subjective questions and explicitly measure large language models' prior
beliefs before experiments. Debaters were asked to select their preferred
position, then presented with a judge persona deliberately designed to conflict
with their identified priors. This setup tested whether models would adopt
sycophantic strategies, aligning with the judge's presumed perspective to
maximize persuasiveness, or remain faithful to their prior beliefs. We
implemented and compared two debate protocols, sequential and simultaneous, to
evaluate potential systematic biases. Finally, we assessed whether models were
more persuasive and produced higher-quality arguments when defending positions
consistent with their prior beliefs versus when arguing against them. Our main
findings show that models tend to prefer defending stances aligned with the
judge persona rather than their prior beliefs, sequential debate introduces
significant bias favoring the second debater, models are more persuasive when
defending positions aligned with their prior beliefs, and paradoxically,
arguments misaligned with prior beliefs are rated as higher quality in pairwise
comparison. These results can inform human judges to provide higher-quality
training signals and contribute to more aligned AI systems, while revealing
important aspects of human-AI interaction regarding persuasion dynamics in
language models.

</details>


### [149] [Synthesizing Agentic Data for Web Agents with Progressive Difficulty Enhancement Mechanisms](https://arxiv.org/abs/2510.13913)
*Shrey Pandit,Xuan-Phi Nguyen,Yifei Ming,Austin Xu,Jiayu Wang,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 本研究提出了一种新的数据合成方法，用于训练更有效的网络研究代理。


<details>
  <summary>Details</summary>
Motivation: 现有的网络研究代理在处理长周期推理和探索任务时面临挑战，因为语言模型通常未针对此类任务进行优化。现有数据集的合成方法缺乏对难度和质量的精细控制，并且常常将数据和训练效果混淆，难以评估数据本身的有效性。

Method: 提出了一种两阶段的数据合成流程，通过逐步增加任务复杂度来生成问答对，直到一个基础网络代理失败。该基础代理在问答、事实校验、查找替代答案和执行过滤等方面扮演多个角色。为了评估合成方法的有效性，采用了基于强网络代理蒸馏的可控训练设置。

Result: 在多个基于网络的基准测试中，作者的数据集虽然规模较小，但能够训练出比现有数据集更有效的网络代理。特别地，作者的数据集在工具使用动作方面表现出两倍的多样性，使得在其中训练的模型能够获得更强的性能，并避免了重复的工具调用行为。

Conclusion: 作者提出的数据合成方法能够生成高质量、多样化的数据集，有效提升网络研究代理的性能，并在工具使用方面展现出更高的效率。

Abstract: Web-based 'deep research' agents aim to solve complex question - answering
tasks through long-horizon interactions with online tools. These tasks remain
challenging, as the underlying language models are often not optimized for
long-horizon reasoning and exploration. Prior work has proposed workflows for
constructing instruction-tuning datasets, often leveraging knowledge graphs.
However, such methods typically lack fine-grained control over difficulty and
quality, yielding synthetic data that falls short of capturing the complexity
required for long-horizon reasoning. Furthermore, many studies conflate data
and training effects by comparing models trained under different optimization
recipes, making it difficult to isolate and evaluate the effectiveness of the
data itself. We introduce a two-pronged data synthesis pipeline that generates
question - answer pairs by progressively increasing task complexity until a
frontier baseline web agent fails. The baseline agent plays multiple roles in
this process: attempting the questions, validating factuality, checking for
alternative answers, and enforcing filtering. To evaluate the effectiveness of
our synthesis methods, we adopt a controlled training setup based on
distillation from strong web agents. Experiments across multiple web-based
benchmarks show that our dataset - despite being smaller - enables the training
of more effective web agents than existing datasets. In particular, our data
exhibits twice the diversity in tool-use actions, allowing models trained on it
to achieve stronger performance while avoiding repetitive tool-calling
behaviors.

</details>


### [150] [Readability $\ne$ Learnability: Rethinking the Role of Simplicity in Training Small Language Models](https://arxiv.org/abs/2510.13915)
*Ivan Lee,Taylor Berg-Kirkpatrick*

Main category: cs.CL

TL;DR: SLMs在儿童语料上表现出的连贯性并非由可读性引起，而是由统计上的简单性（如n-gram多样性）决定。模型的复杂性或可读性对SLM的学习效率影响不大。


<details>
  <summary>Details</summary>
Motivation: 挑战“可读性是SLM生成连贯文本的关键”的观点。

Method: 构建了结构相同但可读性不同的合成数据集，并比较了在这些数据集上训练的SLM的表现。

Result: 可读性并非预测连贯性或学习效率的指标。在复杂文本上训练的模型表现相当，甚至学习效率更快。统计简单性（n-gram多样性）是更强的可学习性预测指标。

Conclusion: SLM的学习和能力涌现主要由统计简单性驱动，而非可读性。反对将SLM训练过程拟人化，并主张对模型能力涌现的支撑因素进行更精确的推理。

Abstract: Recent studies suggest that very small language models (SLMs) can generate
surprisingly coherent text when trained on simplified, child-directed corpora
such as TinyStories. These findings have been interpreted as evidence that
readability -- characterized by accessible vocabulary, familiar narrative
structure, and simple syntax -- plays a key role in enabling such capabilities
to emerge. In this paper, we challenge that interpretation. We construct
synthetic datasets with matched structure but varied readability, and find that
readability alone does not predict coherence or learning efficiency in SLMs.
Models trained on complex, adult-level text perform comparably to those trained
on simplified language, and even exhibit faster development of coherence during
training. Instead, we show that statistical simplicity, as measured by n-gram
diversity, is a stronger predictor of learnability. Our findings caution
against the growing trend of anthropomorphizing language model training --
drawing parallels to human cognitive development without empirical basis -- and
argue for more precise reasoning about what properties actually support
capability emergence in small models.

</details>


### [151] [Element2Vec: Build Chemical Element Representation from Text for Property Prediction](https://arxiv.org/abs/2510.13916)
*Yuanhao Li,Keyuan Lai,Tianqi Wang,Qihao Liu,Jiawei Ma,Yuan-Chao Hu*

Main category: cs.CL

TL;DR: Element2Vector利用自然语言处理技术为化学元素生成向量表示，并结合测试时训练方法来提高材料科学研究中财产预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统化学元素属性预测方法难以处理复杂关系且存在局限性，而现有AI方法存在幻觉和可解释性差的问题。

Method: 使用语言模型为化学元素生成通用嵌入（Global）和属性突出向量（Local），并设计基于自注意力机制的测试时训练方法来缓解数据稀疏性和分布不均带来的预测误差。

Result: Element2Vector能够有效表示化学元素，并提升财产预测的准确性，解决了传统方法和现有AI方法的局限性。

Conclusion: Element2Vector为材料科学领域的人工智能驱动发现奠定了基础。

Abstract: Accurate property data for chemical elements is crucial for materials design
and manufacturing, but many of them are difficult to measure directly due to
equipment constraints. While traditional methods use the properties of other
elements or related properties for prediction via numerical analyses, they
often fail to model complex relationships. After all, not all characteristics
can be represented as scalars. Recent efforts have been made to explore
advanced AI tools such as language models for property estimation, but they
still suffer from hallucinations and a lack of interpretability. In this paper,
we investigate Element2Vecto effectively represent chemical elements from
natural languages to support research in the natural sciences. Given the text
parsed from Wikipedia pages, we use language models to generate both a single
general-purpose embedding (Global) and a set of attribute-highlighted vectors
(Local). Despite the complicated relationship across elements, the
computational challenges also exist because of 1) the discrepancy in text
distribution between common descriptions and specialized scientific texts, and
2) the extremely limited data, i.e., with only 118 known elements, data for
specific properties is often highly sparse and incomplete. Thus, we also design
a test-time training method based on self-attention to mitigate the prediction
error caused by Vanilla regression clearly. We hope this work could pave the
way for advancing AI-driven discovery in materials science.

</details>


### [152] [Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time Scaling](https://arxiv.org/abs/2510.13918)
*Peng Kuang,Yanli Wang,Xiaoyu Han,Yaowenqi Liu,Kaidi Xu,Haohan Wang*

Main category: cs.CL

TL;DR: PRM信号在LLM的TTS中可能被错误利用，我们提出了一个理论框架来优化信号的结合，并通过实验证明了我们提出的校准方法可以提高TTS效率。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM测试时间扩展（TTS）依赖于过程奖励模型（PRM）来选择最佳响应，但最近的基准测试表明，简单的多数投票有时优于标准的PRM选择，这表明PRM信号的利用可能存在问题。需要研究如何有效利用PRM的验证信号。

Method: 1. 提出一个理论框架，用于最优地结合来自LLM和PRM的信号。 2. 基于理论结果，提出一种有效的预计算方法来校准权重函数。 3. 在多个LLM和PRM上进行实验，以验证所提出方法的有效性。

Result: 所提出的校准方法显著提高了TTS效率，并且优于标准的加权多数投票方法，同时计算量仅占后者的21.3%。

Conclusion: 更智能的聚合策略比单纯扩展测试时间计算更能有效地提升LLM的性能。

Abstract: Process reward models (PRMs) are a cornerstone of test-time scaling (TTS),
designed to verify and select the best responses from large language models
(LLMs). However, this promise is challenged by recent benchmarks where simple
majority voting, which ignores PRM signals, occasionally outperforms standard
PRM-based selection. This raises a critical question: How can we effectively
utilize verification signals from PRMs for TTS? To address this, we start by
developing a theoretical framework for optimally combining signals from both
the LLM and the PRM. Our framework reveals that the optimal strategy is a
weighted aggregation of responses, a strategy whose effectiveness hinges on
estimating weights that capture the complex interplay between the models. Based
on our theoretical results, we empirically show that these optimal weighting
functions differ significantly across LLM-PRM pairs and, notably, often assign
substantial negative weights. Motivated by these insights, we propose efficient
pre-computation methods to calibrate these weighting functions. Extensive
experiments across 5 LLMs and 7 PRMs demonstrate that our calibration method
significantly boosts the TTS efficiency, surpassing the performance of vanilla
weighted majority voting while using only $21.3\%$ of the computation.
Ultimately, our work demonstrates that investing in a more intelligent
aggregation strategy can be a more convincing path to performance gains than
simply scaling test-time computation.

</details>


### [153] [FACTS: Table Summarization via Offline Template Generation with Agentic Workflows](https://arxiv.org/abs/2510.13920)
*Ye Yuan,Mohammad Amin Shabani,Siqi Liu*

Main category: cs.CL

TL;DR: FACTS是一个通过离线模板生成实现快速、准确和隐私合规的表格摘要方法，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的表格摘要方法在微调成本、复杂推理、令牌限制、效率和数据隐私方面存在不足。FACTS旨在克服这些问题。

Method: FACTS通过生成离线模板（包括SQL查询和Jinja2模板）来处理表格摘要。这些模板可以渲染成自然语言摘要，并可跨具有相同模式的多个表重复使用。其优点包括：通过可重用模板实现快速摘要，通过可执行SQL查询实现准确输出，以及仅将表模式发送给LLM以确保隐私合规。

Result: 在广泛使用的基准测试中，FACTS的评估结果持续优于基线方法。

Conclusion: FACTS是解决现实世界中查询导向型表格摘要问题的实用解决方案。

Abstract: Query-focused table summarization requires generating natural language
summaries of tabular data conditioned on a user query, enabling users to access
insights beyond fact retrieval. Existing approaches face key limitations:
table-to-text models require costly fine-tuning and struggle with complex
reasoning, prompt-based LLM methods suffer from token-limit and efficiency
issues while exposing sensitive data, and prior agentic pipelines often rely on
decomposition, planning, or manual templates that lack robustness and
scalability. To mitigate these issues, we introduce an agentic workflow, FACTS,
a Fast, Accurate, and Privacy-Compliant Table Summarization approach via
Offline Template Generation. FACTS produces offline templates, consisting of
SQL queries and Jinja2 templates, which can be rendered into natural language
summaries and are reusable across multiple tables sharing the same schema. It
enables fast summarization through reusable offline templates, accurate outputs
with executable SQL queries, and privacy compliance by sending only table
schemas to LLMs. Evaluations on widely-used benchmarks show that FACTS
consistently outperforms baseline methods, establishing it as a practical
solution for real-world query-focused table summarization.

</details>


### [154] [An LLM-Powered AI Agent Framework for Holistic IoT Traffic Interpretation](https://arxiv.org/abs/2510.13925)
*Daniel Adu Worae,Spyridon Mastorakis*

Main category: cs.CL

TL;DR: 该研究提出一个由大型语言模型驱动的AI代理框架，用于解析和解释物联网（IoT）网络流量，通过跨层分析和增强的检索方法，提高了分析的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的物联网流量分析方法难以处理海量、多样化的数据并进行跨层关联分析，需要更智能、更全面的解析方法来识别正常活动和潜在威胁。

Method: 提出一个集成特征提取、基于Transformer的异常检测、数据包和流摘要、威胁情报丰富化以及检索增强问答的LLM驱动的AI代理框架。该框架通过大型语言模型指导下的AI代理，对索引的网络流量进行推理，整合证据以生成准确、易于理解的解释。实验中采用了结合词汇和语义搜索及重排的混合检索方法。

Result: 实验结果表明，在多个物联网流量捕获和六个开放模型上，混合检索方法在BLEU、ROUGE、METEOR和BERTScore指标上显著优于仅使用稠密检索的方法。系统性能分析显示，该框架具有较低CPU、GPU和内存开销。

Conclusion: 该LLM驱动的AI代理框架能够高效且全面地解释物联网网络流量，通过混合检索策略显著提高了分析结果的质量，同时保持了较低的系统资源消耗。

Abstract: Internet of Things (IoT) networks generate diverse and high-volume traffic
that reflects both normal activity and potential threats. Deriving meaningful
insight from such telemetry requires cross-layer interpretation of behaviors,
protocols, and context rather than isolated detection. This work presents an
LLM-powered AI agent framework that converts raw packet captures into
structured and semantically enriched representations for interactive analysis.
The framework integrates feature extraction, transformer-based anomaly
detection, packet and flow summarization, threat intelligence enrichment, and
retrieval-augmented question answering. An AI agent guided by a large language
model performs reasoning over the indexed traffic artifacts, assembling
evidence to produce accurate and human-readable interpretations. Experimental
evaluation on multiple IoT captures and six open models shows that hybrid
retrieval, which combines lexical and semantic search with reranking,
substantially improves BLEU, ROUGE, METEOR, and BERTScore results compared with
dense-only retrieval. System profiling further indicates low CPU, GPU, and
memory overhead, demonstrating that the framework achieves holistic and
efficient interpretation of IoT network traffic.

</details>


### [155] [BioMedSearch: A Multi-Source Biomedical Retrieval Framework Based on LLMs](https://arxiv.org/abs/2510.13926)
*Congying Liu,Xingyuan Wei,Peipei Liu,Yiqing Shen,Yanxu Mao,Tiehan Cui*

Main category: cs.CL

TL;DR: BioMedSearch是一个基于LLM的多源生物医学信息检索框架，通过整合文献、蛋白质数据库和网络搜索，并采用子查询分解、关键词提取、任务图构建和多源信息过滤等方法，提高了生物医学问答的准确性，并在BioMedMCQs数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM在处理生物医学专业知识时存在科学严谨性不足的问题，会编造与事实不符的蛋白质功能、相互作用和结构细节，这是因为它们无法访问权威的生物医学数据库。因此，需要一个能够整合多源信息并确保准确性的生物医学问答框架。

Method: BioMedSearch框架通过以下步骤处理复杂的生物医学查询：1. 子查询分解：将复杂查询拆分为更小的部分。2. 关键词提取：识别查询中的关键信息。3. 任务图构建：规划查询处理的步骤和依赖关系。4. 多源信息过滤：整合来自文献、蛋白质数据库和网络搜索的结果，并进行筛选，以生成高质量的问答结果。

Result: 在包含3000个问题的BioMedMCQs数据集上，BioMedSearch在三个推理层面（机制识别、非邻近语义整合和时间因果推理）上均显著优于基线模型。具体而言，第一层的准确率从59.1%提升到91.9%；第二层的准确率从47.0%提升到81.0%；第三层的准确率从36.3%提升到73.4%。

Conclusion: BioMedSearch框架有效地解决了LLM在生物医学问答中的局限性，通过整合多源信息和精细化的处理流程，显著提高了问答的准确性和可靠性，为处理复杂的生物医学查询提供了有效的解决方案。

Abstract: Biomedical queries often rely on a deep understanding of specialized
knowledge such as gene regulatory mechanisms and pathological processes of
diseases. They require detailed analysis of complex physiological processes and
effective integration of information from multiple data sources to support
accurate retrieval and reasoning. Although large language models (LLMs) perform
well in general reasoning tasks, their generated biomedical content often lacks
scientific rigor due to the inability to access authoritative biomedical
databases and frequently fabricates protein functions, interactions, and
structural details that deviate from authentic information. Therefore, we
present BioMedSearch, a multi-source biomedical information retrieval framework
based on LLMs. The method integrates literature retrieval, protein database and
web search access to support accurate and efficient handling of complex
biomedical queries. Through sub-queries decomposition, keywords extraction,
task graph construction, and multi-source information filtering, BioMedSearch
generates high-quality question-answering results. To evaluate the accuracy of
question answering, we constructed a multi-level dataset, BioMedMCQs,
consisting of 3,000 questions. The dataset covers three levels of reasoning:
mechanistic identification, non-adjacent semantic integration, and temporal
causal reasoning, and is used to assess the performance of BioMedSearch and
other methods on complex QA tasks. Experimental results demonstrate that
BioMedSearch consistently improves accuracy over all baseline models across all
levels. Specifically, at Level 1, the average accuracy increases from 59.1% to
91.9%; at Level 2, it rises from 47.0% to 81.0%; and at the most challenging
Level 3, the average accuracy improves from 36.3% to 73.4%. The code and
BioMedMCQs are available at: https://github.com/CyL-ucas/BioMed_Search

</details>


### [156] [LLMs Can Get "Brain Rot"!](https://arxiv.org/abs/2510.13928)
*Shuo Xing,Junyuan Hong,Yifan Wang,Runjin Chen,Zhenyu Zhang,Ananth Grama,Zhengzhong Tu,Zhangyang Wang*

Main category: cs.CL

TL;DR: 持续暴露于低质量网络文本会导致大型语言模型（LLM）的认知能力下降，并可能引发“心智腐烂”。


<details>
  <summary>Details</summary>
Motivation: 探究数据质量对大型语言模型（LLM）认知能力的影响，特别是低质量网络文本（“垃圾”文本）对模型性能的潜在负面作用。

Method: 通过在真实Twitter/X语料库上进行对照实验，构建了“垃圾”数据集和对照数据集，并分别进行了M1（参与度）和M2（语义质量）两种操作化定义。使用4个LLM在“垃圾”数据集上进行持续预训练，并与其他模型进行对比。分析了不同“垃圾”数据比例对模型性能的影响，并进行了错误分析。

Result: 在“垃圾”数据集上预训练的LLM在推理、长上下文理解、安全性等方面表现出明显的认知下降（Hedges' $g>0.3$），并且“黑暗特质”（如精神病态、自恋）有所增加。数据比例与认知能力下降呈剂量反应关系。错误分析表明，思维跳跃是主要的损伤原因，模型倾向于截断或跳过推理链。指令调优和干净数据预训练可以部分恢复能力，但无法完全恢复到基线水平。推文的流行度（非语义指标）比其长度更能指示“心智腐烂”效应。

Conclusion: 数据质量是LLM能力衰退的因果驱动因素。持续预训练中的数据策展是一个“训练时安全”问题，并表明需要对已部署的LLM进行常规的“认知健康检查”。

Abstract: We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk
web text induces lasting cognitive decline in large language models (LLMs). To
causally isolate data quality, we run controlled experiments on real Twitter/X
corpora, constructing junk and reversely controlled datasets via two orthogonal
operationalizations: M1 (engagement degree) and M2 (semantic quality), with
matched token scale and training operations across conditions. Contrary to the
control group, continual pre-training of 4 LLMs on the junk dataset causes
non-trivial declines (Hedges' $g>0.3$) on reasoning, long-context
understanding, safety, and inflating "dark traits" (e.g., psychopathy,
narcissism). The gradual mixtures of junk and control datasets also yield
dose-response cognition decay: for example, under M1, ARC-Challenge with Chain
Of Thoughts drops $74.9 \rightarrow 57.2$ and RULER-CWE $84.4 \rightarrow 52.3$
as junk ratio rises from $0\%$ to $100\%$.
  Error forensics reveal several key insights. First, we identify
thought-skipping as the primary lesion: models increasingly truncate or skip
reasoning chains, explaining most of the error growth. Second, partial but
incomplete healing is observed: scaling instruction tuning and clean data
pre-training improve the declined cognition yet cannot restore baseline
capability, suggesting persistent representational drift rather than format
mismatch. Finally, we discover that the popularity, a non-semantic metric, of a
tweet is a better indicator of the Brain Rot effect than the length in M1.
Together, the results provide significant, multi-perspective evidence that data
quality is a causal driver of LLM capability decay, reframing curation for
continual pretraining as a \textit{training-time safety} problem and motivating
routine "cognitive health checks" for deployed LLMs.

</details>


### [157] [Robust or Suggestible? Exploring Non-Clinical Induction in LLM Drug-Safety Decisions](https://arxiv.org/abs/2510.13931)
*Siying Liu,Shisheng Zhang,Indu Bala*

Main category: cs.CL

TL;DR: LLMs用于药物安全预测时，可能因社会人口统计学信息产生偏见，导致对弱势群体的预测准确性下降，并存在明确和隐含的偏见模式，需要在临床部署前进行公平性评估和缓解。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在药物安全预测中是否会利用社会人口统计学信息，尽管这些信息与临床无关。

Method: 使用FAERS结构化数据和基于角色的评估框架，评估ChatGPT-4o和Bio-Medical-Llama-3.8B在不同社会人口统计学特征（如教育、婚姻状况、就业等）和用户角色（全科医生、专家、患者）下的不良事件（AE）预测表现。

Result: 结果显示，LLMs在AE预测方面存在系统性差异。社会经济地位较低的群体（如低教育、住房不稳定）往往被预测有更高的AE可能性。发现了明确偏见（预测直接引用了角色属性）和隐含偏见（预测不一致但未明确提及角色属性）。

Conclusion: 将LLMs应用于药物警戒存在重大风险，需要在临床部署前制定公平性评估和缓解策略。

Abstract: Large language models (LLMs) are increasingly applied in biomedical domains,
yet their reliability in drug-safety prediction remains underexplored. In this
work, we investigate whether LLMs incorporate socio-demographic information
into adverse event (AE) predictions, despite such attributes being clinically
irrelevant. Using structured data from the United States Food and Drug
Administration Adverse Event Reporting System (FAERS) and a persona-based
evaluation framework, we assess two state-of-the-art models, ChatGPT-4o and
Bio-Medical-Llama-3.8B, across diverse personas defined by education, marital
status, employment, insurance, language, housing stability, and religion. We
further evaluate performance across three user roles (general practitioner,
specialist, patient) to reflect real-world deployment scenarios where
commercial systems often differentiate access by user type. Our results reveal
systematic disparities in AE prediction accuracy. Disadvantaged groups (e.g.,
low education, unstable housing) were frequently assigned higher predicted AE
likelihoods than more privileged groups (e.g., postgraduate-educated, privately
insured). Beyond outcome disparities, we identify two distinct modes of bias:
explicit bias, where incorrect predictions directly reference persona
attributes in reasoning traces, and implicit bias, where predictions are
inconsistent, yet personas are not explicitly mentioned. These findings expose
critical risks in applying LLMs to pharmacovigilance and highlight the urgent
need for fairness-aware evaluation protocols and mitigation strategies before
clinical deployment.

</details>


### [158] [Big Reasoning with Small Models: Instruction Retrieval at Inference Time](https://arxiv.org/abs/2510.13935)
*Kenan Alkiek,David Jurgens,Vinod Vydiswaran*

Main category: cs.CL

TL;DR: 小型语言模型（SLM）可以通过在推理时检索结构化推理过程来克服其在多步推理和领域特定知识方面的局限性，从而在本地硬件上实现高效、私密和低成本的推理。


<details>
  <summary>Details</summary>
Motivation: 旨在解决小型语言模型（SLM）在需要多步推理或领域特定知识的任务中表现不佳的问题，同时利用SLM在本地计算上的优势（如隐私、低成本和环境影响）。

Method: 提出了一种在推理时进行指令干预的方法，其中SLM检索结构化的推理过程而非从头生成。首先，通过对训练问题进行分组并使用GPT-5创建指令来构建指令语料库。然后在推理时，SLM检索最相关的指令并遵循其步骤。这与检索文本段落的检索增强生成不同，它为模型提供了结构化的推理指导。

Result: 在MedQA（医学委员会考试）、MMLU专业法律和MathQA的测试中，使用了3B到14B参数的模型，没有进行任何额外的微调。结果显示，指令检索带来了显著的提升：MedQA提高9.4%，MMLU法律提高7.9%，MathQA提高5.1%。研究还发现，简洁的指令比冗长的指令效果更好，并且改进的幅度很大程度上取决于模型家族和固有的推理能力。

Conclusion: 所提出的指令检索框架能够有效地增强小型语言模型（SLM）在复杂推理任务上的表现，同时保持SLM的计算效率和隐私优势。该方法通过提供结构化的推理指导，克服了SLM在多步推理和领域知识方面的不足。

Abstract: Can we bring large-scale reasoning to local-scale compute? Small language
models (SLMs) are increasingly attractive because they run efficiently on local
hardware, offering strong privacy, low cost, and reduced environmental impact.
Yet they often struggle with tasks that require multi-step reasoning or
domain-specific knowledge. We address this limitation through instruction
intervention at inference time, where an SLM retrieves structured reasoning
procedures rather than generating them from scratch. Our method builds an
Instruction Corpus by grouping similar training questions and creating
instructions via GPT-5. During inference, the SLM retrieves the most relevant
instructions and follows their steps. Unlike retrieval-augmented generation,
which retrieves text passages, instruction retrieval gives the model structured
guidance for reasoning. We evaluate this framework on MedQA (medical board
exams), MMLU Professional Law, and MathQA using models from 3B to 14B
parameters without any additional fine-tuning. Instruction retrieval yields
consistent gains: 9.4% on MedQA, 7.9% on MMLU Law, and 5.1% on MathQA. Concise
instructions outperform longer ones, and the magnitude of improvement depends
strongly on model family and intrinsic reasoning ability.

</details>


### [159] [FinDeepResearch: Evaluating Deep Research Agents in Rigorous Financial Analysis](https://arxiv.org/abs/2510.13936)
*Fengbin Zhu,Xiang Yao Ng,Ziyang Liu,Chang Liu,Xianwei Zeng,Chao Wang,Tianhui Tan,Xuan Yao,Pengyang Shao,Min Xu,Zixuan Wang,Jing Wang,Xin Lin,Junfeng Li,Jingxian Zhu,Yang Zhang,Wenjie Wang,Fuli Feng,Richang Hong,Huanbo Luan,Ke-Wei Huang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文提出了HisRubric评估框架和FinDeepResearch基准，用于系统性评估深度研究（DR）智能体在公司金融分析中的能力，并进行了广泛的实验。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对深度研究（DR）智能体在关键研究分析能力方面的系统性评估，因此需要一个评估框架和基准来解决这一差距。

Method: 提出了一种名为HisRubric的新型评估框架，该框架具有分层的分析结构和细粒度的评分标准，用于严格评估DR智能体在公司金融分析中的能力。在此框架的基础上，构建了一个包含64家上市公司、跨越8个金融市场、涉及4种语言的FinDeepResearch基准，共包含15,808个评分项。使用16种代表性方法（包括6种DR智能体、5种具备深度推理和搜索能力的LLM，以及5种仅具备深度推理能力的LLM）在FinDeepResearch基准上进行了广泛的实验。

Result: 实验结果揭示了这些方法在不同能力、金融市场和语言方面的优势和局限性。

Conclusion: HisRubric框架和FinDeepResearch基准为未来DR智能体在公司金融分析领域的研究和开发提供了宝贵的见解，评估框架和代码将公开。

Abstract: Deep Research (DR) agents, powered by advanced Large Language Models (LLMs),
have recently garnered increasing attention for their capability in conducting
complex research tasks. However, existing literature lacks a rigorous and
systematic evaluation of DR Agent's capabilities in critical research analysis.
To address this gap, we first propose HisRubric, a novel evaluation framework
with a hierarchical analytical structure and a fine-grained grading rubric for
rigorously assessing DR agents' capabilities in corporate financial analysis.
This framework mirrors the professional analyst's workflow, progressing from
data recognition to metric calculation, and finally to strategic summarization
and interpretation. Built on this framework, we construct a FinDeepResearch
benchmark that comprises 64 listed companies from 8 financial markets across 4
languages, encompassing a total of 15,808 grading items. We further conduct
extensive experiments on the FinDeepResearch using 16 representative methods,
including 6 DR agents, 5 LLMs equipped with both deep reasoning and search
capabilities, and 5 LLMs with deep reasoning capabilities only. The results
reveal the strengths and limitations of these approaches across diverse
capabilities, financial markets, and languages, offering valuable insights for
future research and development. The benchmark and evaluation code will be made
publicly available.

</details>


### [160] [Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers](https://arxiv.org/abs/2510.13939)
*Tuhin Chakrabarty,Jane C. Ginsburg,Paramveer Dhillon*

Main category: cs.CL

TL;DR: AI模型在模仿作家风格方面，通过对特定作家作品进行微调，可以超越人类作家，但未经微调的AI写作则被专家读者强烈排斥。


<details>
  <summary>Details</summary>
Motivation: 评估AI模型模仿作家风格和生成文学文本的质量，以及其对版权相关法律问题的潜在影响。

Method: 对三个前沿AI模型（ChatGPT, Claude, Gemini）进行写作任务，模仿50位获奖作家风格，并通过专家和普通读者进行盲审评估。对比了直接提示（in-context prompting）和对模型进行特定作家作品微调（fine-tuning）的效果。

Result: 未经微调的AI写作在风格和质量上被专家读者否定，但普通读者评价不一。然而，经过针对单个作家作品微调的ChatGPT，在专家读者眼中，在风格和质量上均优于人类写作。这种微调还能显著降低AI文本被识别为AI生成内容的比例，并大大降低了写作成本。

Conclusion: 作者认为，经过特定作家作品微调的AI写作，在质量和风格上能满足读者偏好，并且成本效益显著，这为AI在版权领域的应用提供了实证依据。

Abstract: The use of copyrighted books for training AI models has led to numerous
lawsuits from authors concerned about AI's ability to generate derivative
content.Yet it's unclear whether these models can generate high quality
literary text while emulating authors' styles. To answer this we conducted a
preregistered study comparing MFA-trained expert writers with three frontier AI
models: ChatGPT, Claude & Gemini in writing up to 450 word excerpts emulating
50 award-winning authors' diverse styles. In blind pairwise evaluations by 159
representative expert & lay readers, AI-generated text from in-context
prompting was strongly disfavored by experts for both stylistic fidelity
(OR=0.16, p<10^8) & writing quality (OR=0.13, p<10^7) but showed mixed results
with lay readers. However, fine-tuning ChatGPT on individual authors' complete
works completely reversed these findings: experts now favored AI-generated text
for stylistic fidelity (OR=8.16, p<10^13) & writing quality (OR=1.87, p=0.010),
with lay readers showing similar shifts. These effects generalize across
authors & styles. The fine-tuned outputs were rarely flagged as AI-generated
(3% rate v. 97% for in-context prompting) by best AI detectors. Mediation
analysis shows this reversal occurs because fine-tuning eliminates detectable
AI stylistic quirks (e.g., cliche density) that penalize in-context outputs.
While we do not account for additional costs of human effort required to
transform raw AI output into cohesive, publishable prose, the median
fine-tuning & inference cost of $81 per author represents a dramatic 99.7%
reduction compared to typical professional writer compensation. Author-specific
fine-tuning thus enables non-verbatim AI writing that readers prefer to expert
human writing, providing empirical evidence directly relevant to copyright's
fourth fair-use factor, the "effect upon the potential market or value" of the
source works.

</details>


### [161] [Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention](https://arxiv.org/abs/2510.13940)
*Zhen Yang,Mingyang Zhang,Feng Chen,Ganggui Ding,Liang Hou,Xin Tao,Pengfei Wan,Ying-Cong Chen*

Main category: cs.CL

TL;DR: LLM推理中的不确定性集中在少数高熵标记上，MTI框架通过选择性干预和轻量级负面提示来提高准确性，同时保持高效率。


<details>
  <summary>Details</summary>
Motivation: LLM的推理准确性和效率之间的权衡。之前的研究侧重于增加计算量，但效率低下。

Method: 提出了一种名为最小测试时间干预（MTI）的训练框架，包括选择性条件判别生成（CFG）干预和轻量级负面提示引导。

Result: 在通用、编码和STEM任务上实现了持续的改进，例如在八个基准测试上平均提高了1.35%（Qwen3-8B-Base），在AIME2024上提高了5%（Qwen3-32B-Reasoning），同时保持了高效率。

Conclusion: MTI是一种有效的训练框架，可以在不影响效率的情况下提高LLM的推理能力。

Abstract: Recent progress in large language models (LLMs) has focused on test-time
scaling to improve reasoning via increased inference computation, but often at
the cost of efficiency. We revisit test-time behavior and uncover a simple yet
underexplored phenomenon: reasoning uncertainty is highly localized-only a
small subset of high-entropy tokens dominantly affects output correctness.
Motivated by this, we propose Minimal Test-Time Intervention (MTI), a
training-free framework that enhances reasoning accuracy and stability with
minimal overhead. MTI includes: (i) Selective CFG intervention, applying
classifier-free guidance only at uncertain positions; and (ii) Lightweight
negative-prompt guidance, reusing the main model's KV cache to approximate
unconditional decoding efficiently. MTI yields consistent gains across general,
coding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for
Qwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining
highly efficient.

</details>


### [162] [Classifying and Addressing the Diversity of Errors in Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2510.13975)
*Kin Kwan Leung,Mouloud Belbahri,Yi Sui,Alex Labach,Xueying Zhang,Stephen Rose,Jesse C. Cresswell*

Main category: cs.CL

TL;DR: RAG系统在问答中常用于整合外部知识，但易出错。本文提出了一个RAG错误分类体系，并附带示例、解决方案、标注数据集以及一个自动评估方法，旨在帮助开发者识别和修复RAG系统中的错误。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的RAG系统复杂多样，可能导致输出错误。理解这些错误对于RAG系统的稳健部署至关重要。

Method: 提出了一个RAG错误类型的分类体系，并提供了每个类别的示例和相应的解决方案。同时，整理了一个包含错误类型标注的RAG错误响应数据集，并提出了一种与该分类体系相匹配的自动评估方法，用于在开发过程中跟踪和解决错误。

Result: 创建了一个新的RAG错误类型分类体系，并收集了一个标注数据集。提出了一种自动评估方法，能够帮助开发者在开发过程中监控和解决RAG系统中的错误。

Conclusion: 本文提出的错误分类体系、数据集和自动评估方法为RAG系统的错误分析和改进提供了实践指导。

Abstract: Retrieval-augmented generation (RAG) is a prevalent approach for building
LLM-based question-answering systems that can take advantage of external
knowledge databases. Due to the complexity of real-world RAG systems, there are
many potential causes for erroneous outputs. Understanding the range of errors
that can occur in practice is crucial for robust deployment. We present a new
taxonomy of the error types that can occur in realistic RAG systems, examples
of each, and practical advice for addressing them. Additionally, we curate a
dataset of erroneous RAG responses annotated by error types. We then propose an
auto-evaluation method aligned with our taxonomy that can be used in practice
to track and address errors during development. Code and data are available at
https://github.com/layer6ai-labs/rag-error-classification.

</details>


### [163] [The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models](https://arxiv.org/abs/2510.13996)
*Lukas Gienapp,Christopher Schröder,Stefan Schweter,Christopher Akiki,Ferdinand Schlatt,Arden Zimmermann,Phillipe Genêt,Martin Potthast*

Main category: cs.CL

TL;DR: 该论文介绍了德语Commons，一个包含1545.6亿个词元的、拥有开源许可证的德语文本数据集，旨在解决德语开源语言模型开发中开放许可文本稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）的开发依赖大规模语料库，但大多数语料库的许可证状态不明确，这限制了真正开源模型的开发，尤其是在非英语语言（如德语）方面，开放许可的文本尤为稀缺。

Method: 该论文构建了德语Commons，这是迄今为止最大的开放许可德语文本集合。它整合了来自七个领域（法律、科学、文化、政治、新闻、经济和网络文本）的41个来源的数据。通过从具有可验证许可证的数据提供商那里系统地搜集数据，最终获得了1545.6亿个高质量的词元，用于语言模型训练。其处理流程包括全面的质量过滤、去重和文本格式修复，以确保异构文本来源的质量一致性。所有领域的数据子集都具备至少CC-BY-SA 4.0或同等许可证，确保模型训练和再分发的合法合规性。此外，该论文还发布了语料库构建和针对德语文本的数据过滤代码，以确保德语Commons的可复现性和可扩展性。

Result: 德语Commons包含了1545.6亿个词元，是目前最大的开放许可德语文本数据集，其数据来源于41个不同来源，覆盖七个主要领域，并且所有数据都具有至少CC-BY-SA 4.0或同等许可证。

Conclusion: 德语Commons解决了德语开放许可预训练数据的关键缺口，为开发真正开源的德语语言模型提供了可能，并确保了数据的合规性、高质量和可复现性。

Abstract: Large language model development relies on large-scale training corpora, yet
most contain data of unclear licensing status, limiting the development of
truly open models. This problem is exacerbated for non-English languages, where
openly licensed text remains critically scarce. We introduce the German
Commons, the largest collection of openly licensed German text to date. It
compiles data from 41 sources across seven domains, encompassing legal,
scientific, cultural, political, news, economic, and web text. Through
systematic sourcing from established data providers with verifiable licensing,
it yields 154.56 billion tokens of high-quality text for language model
training. Our processing pipeline implements comprehensive quality filtering,
deduplication, and text formatting fixes, ensuring consistent quality across
heterogeneous text sources. All domain subsets feature licenses of at least
CC-BY-SA 4.0 or equivalent, ensuring legal compliance for model training and
redistribution. The German Commons therefore addresses the critical gap in
openly licensed German pretraining data, and enables the development of truly
open German language models. We also release code for corpus construction and
data filtering tailored to German language text, rendering the German Commons
fully reproducible and extensible.

</details>


### [164] [CRaFT: An Explanation-Based Framework for Evaluating Cultural Reasoning in Multilingual Language Models](https://arxiv.org/abs/2510.14014)
*Shehenaz Hossain,Haithem Afli*

Main category: cs.CL

TL;DR: CRaFT框架通过评估LLM在跨文化背景下的推理能力，而非仅仅准确性，来评估其文化理解能力。该框架使用文化流畅度、偏差、一致性和语言适应性等指标，并应用于世界价值观调查中的多语种问题，揭示了不同语言对LLM推理的影响。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）在跨文化背景下的推理能力，并超越传统的基于准确性的评估方法。

Method: 提出CRaFT（解释型多语言框架），该框架评估模型解释的文化流畅度、偏差、一致性和语言适应性。将该框架应用于50个源自世界价值观调查的文化问题，并将其翻译成阿拉伯语、孟加拉语和西班牙语，以评估GPT、DeepSeek和FANAR三个模型。

Result: 研究发现，阿拉伯语降低了流畅度，孟加拉语提高了流畅度，西班牙语则基本保持稳定。GPT在跨语言适应性方面表现更好，但一致性较低；FANAR表现出稳定但僵化的推理模式。

Conclusion: 文化意识并非LLM的内在属性，而是通过语言框架产生的。CRaFT提供了一种评估多语言环境中跨文化推理的新视角，并为构建文化适应性语言模型提供了可行的见解。

Abstract: Correct answers do not necessarily reflect cultural understanding. We
introduce CRaFT, an explanation-based multilingual evaluation framework
designed to assess how large language models (LLMs) reason across cultural
contexts. Rather than scoring outputs solely based on accuracy, CRaFT evaluates
model explanations using four interpretable metrics: Cultural Fluency,
Deviation, Consistency, and Linguistic Adaptation. We apply the framework to 50
culturally grounded questions from the World Values Survey, translated into
Arabic, Bengali, and Spanish, and evaluate three models (GPT, DeepSeek, and
FANAR) across over 2,100 answer-explanation pairs. Results reveal significant
cross-lingual variation in reasoning: Arabic reduces fluency, Bengali enhances
it, and Spanish remains largely stable. While GPT adapts more effectively
across languages, it exhibits lower consistency; FANAR shows stable but rigid
reasoning. These findings suggest that cultural awareness in LLMs is not
intrinsic but emerges through linguistic framing. CRaFT offers a new lens for
evaluating cross-cultural reasoning in multilingual settings, providing
actionable insights for building culturally adaptive language models.

</details>


### [165] [Think Globally, Group Locally: Evaluating LLMs Using Multi-Lingual Word Grouping Games](https://arxiv.org/abs/2510.14030)
*César Guerra-Solano,Zhuochun Li,Xiang Lorraine Li*

Main category: cs.CL

TL;DR: LLMs在涉及“跳出固有思维模式”的抽象推理任务中存在语言偏见，在英语中表现更好，并且开源模型表现优于闭源模型。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在跨语言抽象推理任务中的偏见，因为现有研究主要集中在依赖策略或知识的任务上，而忽略了抽象推理的重要性。

Method: 提出一个受纽约时报“Connections”游戏启发的名为“GlobalGroup”的新基准测试，包含五种语言（英语、西班牙语、中文、印地语、阿拉伯语）及其英语翻译。引入游戏难度测量以进行更可控的比较。

Result: 实验表明，LLMs在英语环境中进行抽象推理任务时表现更好，并且开源模型在表现上优于闭源模型。

Conclusion: LLMs在抽象推理任务中存在显著的语言偏见，英语是表现最好的语言，开源模型比闭源模型具有优势。

Abstract: Large language models (LLMs) can exhibit biases in reasoning capabilities due
to linguistic modality, performing better on tasks in one language versus
another, even with similar content. Most previous works evaluate this through
reasoning tasks where reliance on strategies or knowledge can ensure success,
such as in commonsense or math tasks. However, abstract reasoning is vital to
reasoning for everyday life, where people apply "out-of-the-box thinking" to
identify and use patterns for solutions, without a reliance on formulaic
approaches. Comparatively, little work has evaluated linguistic biases in this
task type. In this paper, we propose a task inspired by the New York Times
Connections: GlobalGroup, that evaluates models in an abstract reasoning task
across several languages. We constructed a game benchmark with five linguistic
backgrounds -- English, Spanish, Chinese, Hindi, and Arabic -- in both the
native language and an English translation for comparison. We also proposed
game difficulty measurements to evaluate models on games with similar
difficulty, enabling a more controlled comparison, which is particularly
important in reasoning evaluations. Through experimentation, we find English
modalities largely lead to better performance in this abstract reasoning task,
and performance disparities between open- and closed-source models.

</details>


### [166] [Quantifying Phonosemantic Iconicity Distributionally in 6 Languages](https://arxiv.org/abs/2510.14040)
*George Flint,Kaustubh Kislay*

Main category: cs.CL

TL;DR: 本研究通过分布式方法，对6种语言（英语、西班牙语、印地语、芬兰语、土耳其语和泰米尔语）的音韵-语义图像进行量化。


<details>
  <summary>Details</summary>
Motivation: 探讨系统性的语音-语义关系在多大程度上能在大规模、定量研究中体现出来，包括先前已识别和未识别的现象。

Method: 在每种语言中，分析词素的语音和语义相似性空间与一系列统计测量值的一致性。

Result: 发现了大量先前文献中未识别出的、可解释的音韵-语义对齐现象，并揭示了跨语言模式。同时，对5个先前假设的音韵-语义对齐现象进行了分析，发现部分假设得到支持，部分则结果不一。

Conclusion: 大规模的量化研究能够发现新的音韵-语义对齐现象，并揭示跨语言模式，部分先前假设的现象也得到了验证。

Abstract: Language is, as commonly theorized, largely arbitrary. Yet, systematic
relationships between phonetics and semantics have been observed in many
specific cases. To what degree could those systematic relationships manifest
themselves in large scale, quantitative investigations--both in previously
identified and unidentified phenomena? This work undertakes a distributional
approach to quantifying phonosemantic iconicity at scale across 6 diverse
languages (English, Spanish, Hindi, Finnish, Turkish, and Tamil). In each
language, we analyze the alignment of morphemes' phonetic and semantic
similarity spaces with a suite of statistical measures, and discover an array
of interpretable phonosemantic alignments not previously identified in the
literature, along with crosslinguistic patterns. We also analyze 5 previously
hypothesized phonosemantic alignments, finding support for some such alignments
and mixed results for others.

</details>


### [167] [ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models](https://arxiv.org/abs/2510.14077)
*Haziq Mohammad Khalid,Athikash Jeyaganthan,Timothy Do,Yicheng Fu,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: LLMs在多轮对话中性能下降，ERGO通过监测和响应模型不确定性来提高对话AI的性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多轮对话中，当信息逐步呈现时，性能会显著下降，这严重阻碍了其在现实世界中的应用。

Method: ERGO（Entropy-guided Resetting for Generation Optimization）通过计算词元分布的香农熵来量化模型的不确定性，并在检测到熵急剧增加时触发自适应提示整合。

Result: ERGO在逐步指令的多轮任务中，平均性能提升56.6%，能力提升24.7%，不可靠性降低35.3%。

Conclusion: 基于不确定性的干预可以同时提高对话AI的准确性和可靠性。

Abstract: Large Language Models (LLMs) suffer significant performance degradation in
multi-turn conversations when information is presented incrementally. Given
that multi-turn conversations characterize everyday interactions with LLMs,
this degradation poses a severe challenge to real world usability. We
hypothesize that abrupt increases in model uncertainty signal misalignment in
multi-turn LLM interactions, and we exploit this insight to dynamically realign
conversational context. We introduce ERGO (Entropy-guided Resetting for
Generation Optimization), which continuously quantifies internal uncertainty
via Shannon entropy over next token distributions and triggers adaptive prompt
consolidation when a sharp spike in entropy is detected. By treating
uncertainty as a first class signal rather than a nuisance to eliminate, ERGO
embraces variability in language and modeling, representing and responding to
uncertainty. In multi-turn tasks with incrementally revealed instructions, ERGO
yields a 56.6% average performance gain over standard baselines, increases
aptitude (peak performance capability) by 24.7%, and decreases unreliability
(variability in performance) by 35.3%, demonstrating that uncertainty aware
interventions can improve both accuracy and reliability in conversational AI.

</details>


### [168] [DROID: Dual Representation for Out-of-Scope Intent Detection](https://arxiv.org/abs/2510.14110)
*Wael Rashwan,Hossam M. Zawbaa,Sourav Dutta,Haytham Assem*

Main category: cs.CL

TL;DR: DROID是一个端到端的框架，结合了通用句子编码器（USE）和领域自适应的基于Transformer的去噪自编码器（TSDAE），用于检测范围外（OOS）的用户意图。该框架使用一个集成的表示和分支分类器，并结合了合成和开放域的异常值增强，以提高在监督有限情况下的边界学习能力。


<details>
  <summary>Details</summary>
Motivation: 检测任务导向对话系统中的范围外（OOS）用户话语以及更广泛的开放集意图识别仍然是一个关键挑战，现有方法通常依赖于强的分布假设或辅助校准模块。

Method: DROID框架结合了两种互补的编码器——通用句子编码器（USE）用于广泛的语义泛化，以及领域自适应的基于Transformer的去噪自编码器（TSDAE）用于特定领域的上下文区分。这两种编码器的融合表示由一个轻量级分支分类器进行处理，该分类器具有单一的校准阈值，可将域内和OOS意图分开，无需事后评分。为了增强在监督有限情况下的边界学习能力，DROID同时纳入了合成和开放域的异常值增强。

Result: 尽管只使用了150万个可训练参数，DROID在多个意图基准测试中持续优于最近最先进的基线，在已知意图方面实现了6-15%的宏观F1改进，在OOS意图方面实现了8-20%的改进，在低资源设置下获得了最显著的收益。

Conclusion: 双编码器表示结合简单的校准可以为神经对话系统提供健壮、可扩展且可靠的OOS检测。

Abstract: Detecting out-of-scope (OOS) user utterances remains a key challenge in
task-oriented dialogue systems and, more broadly, in open-set intent
recognition. Existing approaches often depend on strong distributional
assumptions or auxiliary calibration modules. We present DROID (Dual
Representation for Out-of-Scope Intent Detection), a compact end-to-end
framework that combines two complementary encoders -- the Universal Sentence
Encoder (USE) for broad semantic generalization and a domain-adapted
Transformer-based Denoising Autoencoder (TSDAE) for domain-specific contextual
distinctions. Their fused representations are processed by a lightweight
branched classifier with a single calibrated threshold that separates in-domain
and OOS intents without post-hoc scoring. To enhance boundary learning under
limited supervision, DROID incorporates both synthetic and open-domain outlier
augmentation. Despite using only 1.5M trainable parameters, DROID consistently
outperforms recent state-of-the-art baselines across multiple intent
benchmarks, achieving macro-F1 improvements of 6--15% for known and 8--20% for
OOS intents, with the most significant gains in low-resource settings. These
results demonstrate that dual-encoder representations with simple calibration
can yield robust, scalable, and reliable OOS detection for neural dialogue
systems.

</details>


### [169] [Toward Cybersecurity-Expert Small Language Models](https://arxiv.org/abs/2510.14113)
*Matan Levi,Daniel Ohayon,Ariel Blobstein,Ravid Sagi,Ian Molloy,Yair Allouche*

Main category: cs.CL

TL;DR: 网络安全领域缺乏专业模型和数据集，CyberPal 2.0 填补了这一空白。该模型家族参数量在 4B-20B 之间，在多种网络安全基准测试中表现优异，超越了许多现有模型，并能在不消耗过多资源的情况下匹配或超越它们。


<details>
  <summary>Details</summary>
Motivation: 部署大型语言模型（LLMs）在网络安全领域存在滞后，主要是因为缺乏高质量、特定领域的模型和训练数据集。

Method: 研究人员创建了一个名为 CyberPal 2.0 的小型语言模型（SLMs）系列，参数量在 4B 到 20B 之间。他们还开发了一个名为 SecKnowledge 2.0 的数据处理流程，用于构建一个包含丰富推理链的网络安全指令数据集，该流程结合了专家指导和多步骤的 LLM 驱动的推理，以提高网络安全任务的准确性。

Result: CyberPal 2.0 在各种网络安全基准测试中始终优于基线模型，并且在性能上可以媲美甚至超越各种开源和闭源的前沿模型，但模型尺寸却小得多。在网络威胁情报知识任务方面，CyberPal 2.0 的表现仅次于 Sec-Gemini v1。在威胁调查任务（如关联漏洞和错误票证与弱点）方面，性能最佳的 20B 参数模型超越了 GPT-4o、o1、o3-mini 和 Sec-Gemini v1，位列第一；而最小的 4B 参数模型则位列第二。

Conclusion: CyberPal 2.0 作为一个专门为网络安全领域设计的小型语言模型系列，通过其先进的训练方法和数据集，在性能上能够与大型模型相媲美，甚至在某些特定任务上超越它们，同时保持了更小的模型规模，为网络安全领域的模型部署提供了一个高效且有效的解决方案。

Abstract: Large language models (LLMs) are transforming everyday applications, yet
deployment in cybersecurity lags due to a lack of high-quality, domain-specific
models and training datasets. To address this gap, we present CyberPal 2.0, a
family of cybersecurity-expert small language models (SLMs) ranging from 4B-20B
parameters. To train CyberPal 2.0, we generate an enriched chain-of-thought
cybersecurity instruction dataset built with our data enrichment and formatting
pipeline, SecKnowledge 2.0, which integrates expert-in-the-loop steering of
reasoning formats alongside LLM-driven multi-step grounding, yielding
higher-fidelity, task-grounded reasoning traces for security tasks. Across
diverse cybersecurity benchmarks, CyberPal 2.0 consistently outperforms its
baselines and matches or surpasses various open and closed-source frontier
models, while remaining a fraction of their size. On core cyber threat
intelligence knowledge tasks, our models outperform almost all tested frontier
models, ranking second only to Sec-Gemini v1. On core threat-investigation
tasks, such as correlating vulnerabilities and bug tickets with weaknesses, our
best 20B-parameter model outperforms GPT-4o, o1, o3-mini, and Sec-Gemini v1,
ranking first, while our smallest 4B-parameter model ranks second.

</details>


### [170] [Building a Macedonian Recipe Dataset: Collection, Parsing, and Comparative Analysis](https://arxiv.org/abs/2510.14128)
*Darko Sasanski,Dimitar Peshevski,Riste Stojanov,Dimitar Trajanov*

Main category: cs.CL

TL;DR: 本研究构建了首个马其顿菜谱数据集，以填补该领域数字研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的计算美食学研究严重依赖于大规模、高质量的菜谱数据集，但马其顿语菜谱在该领域的研究中代表性不足。

Method: 通过网络抓取和结构化解析来构建马其顿菜谱数据集，并处理了单位、数量和描述词的标准化等挑战。此外，还利用了点互信息和提升度等度量方法对配料的频率和共现模式进行了探索性分析。

Result: 成功构建了一个全面的马其顿菜谱数据集，并揭示了该菜系独特的配料组合模式。

Conclusion: 该数据集为研究代表性不足的语言的饮食文化提供了一个新资源，并揭示了马其顿烹饪传统的独特模式。

Abstract: Computational gastronomy increasingly relies on diverse, high-quality recipe
datasets to capture regional culinary traditions. Although there are
large-scale collections for major languages, Macedonian recipes remain
under-represented in digital research. In this work, we present the first
systematic effort to construct a Macedonian recipe dataset through web scraping
and structured parsing. We address challenges in processing heterogeneous
ingredient descriptions, including unit, quantity, and descriptor
normalization. An exploratory analysis of ingredient frequency and
co-occurrence patterns, using measures such as Pointwise Mutual Information and
Lift score, highlights distinctive ingredient combinations that characterize
Macedonian cuisine. The resulting dataset contributes a new resource for
studying food culture in underrepresented languages and offers insights into
the unique patterns of Macedonian culinary tradition.

</details>


### [171] [RLSR: Reinforcement Learning with Supervised Reward Outperforms SFT in Instruction Following](https://arxiv.org/abs/2510.14200)
*Zhichao Wang,Andy Wong,Ruslan Belkin*

Main category: cs.CL

TL;DR: SFT的替代方法RLSR利用RL框架和SFT数据集来提升指令遵循能力，并且可以单独使用或与SFT结合使用以获得更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有技术如SFT、RLHF等用于增强LLM的指令遵循能力，但SFT依赖大规模标注数据，而RFT在特定领域适应性上有所不足。本文提出RLSR旨在利用SFT数据集的优势，在RL框架下提升基础模型的指令遵循能力。

Method: RLSR在RL框架下，让基础模型针对每个提示生成多个响应，并计算生成响应与人工标注响应在语义嵌入空间的余弦相似度作为奖励分数。

Result: RLSR可以直接替代SFT，在指令遵循基准测试中取得更优异的表现。例如，RLSR (SB) 在Qwen-7B (INFINITY)上达到了26.34%的AlpacaEval胜率，优于SFT的21.01%。此外，SFT与RLSR结合训练可以进一步提升下游任务性能，Qwen-7B (INFINITY)的胜率达到30.73%。

Conclusion: RLSR是一种有效的方法，可以提升基础模型的指令遵循能力，并且可以作为SFT的替代或补充，以实现更好的模型性能。

Abstract: After the pretraining stage of LLMs, techniques such as SFT, RLHF, RLVR, and
RFT are applied to enhance instruction-following ability, mitigate undesired
responses, improve reasoning capability and enable efficient domain adaptation
with minimal data. SFT relies on the next-token prediction objective to
strengthen instruction following in a base model using a large corpus of
human-labeled responses. In contrast, RFT employs a RL-based approach to adapt
fine-tuned reasoning models to specific domains with limited supervision.
Inspired by RFT, we propose replacing SFT with RLSR to leverage the extensive
SFT dataset in an RL framework, thereby improving the base model's
instruction-following ability. In RLSR, the base model generates multiple
responses for each prompt, and reward scores are computed as the cosine
similarity in the semantic embedding space between the generated and
human-labeled responses. RLSR can be utilized in multiple ways. It can directly
replace SFT, achieving superior performance on instruction-following
benchmarks-for example, RLSR (SB) on Qwen-7B (INFINITY) achieved an AlpacaEval
win rate of 26.34%, surpassing SFT's 21.01%. Furthermore, combining SFT and
RLSR further enhances downstream task performance; Qwen-7B (INFINITY) achieved
a win rate of 30.73% when trained with SFT + RLSR.

</details>


### [172] [DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans](https://arxiv.org/abs/2510.14205)
*Bingsheng Yao,Bo Sun,Yuanzhe Dong,Yuxuan Lu,Dakuo Wang*

Main category: cs.CL

TL;DR: LLM 角色扮演代理 (LLM RPAs) 旨在模拟人类行为，但手动创建的档案会损害其身份保真度。我们提出了动态身份完善框架 (DPRF)，通过识别生成行为与人类真实情况之间的认知差异来优化 LLM RPAs 的行为与目标个人的一致性，并完善身份档案以减轻这些差异。DPRF 在各种模型和场景中均能显着提高行为一致性。


<details>
  <summary>Details</summary>
Motivation: 手动创建的 LLM RPA 档案缺乏与目标个人的真实性验证，导致身份保真度不足。该研究旨在通过动态身份完善框架 (DPRF) 来解决此问题，以提高 LLM RPA 行为与目标个人行为的一致性。

Method: DPRF 通过迭代识别生成行为与人类真实情况之间的认知差异（通过自由形式或基于理论的结构化分析），并完善身份档案以减轻这些差异，从而优化 LLM RPA 的行为与目标个人的一致性。

Result: 在五个 LLM 和四种不同的行为预测场景（正式辩论、有心理健康问题的社交媒体帖子、公开采访和电影评论）中评估 DPRF。结果表明，DPRF 能够一致地显着提高行为一致性，并且在模型和场景之间具有通用性。

Conclusion: DPRF 提供了一种可靠的方法来创建高保真身份档案，并提高用户模拟、社会研究和个性化人工智能等下游应用的有效性。

Abstract: The emerging large language model role-playing agents (LLM RPAs) aim to
simulate individual human behaviors, but the persona fidelity is often
undermined by manually-created profiles (e.g., cherry-picked information and
personality characteristics) without validating the alignment with the target
individuals. To address this limitation, our work introduces the Dynamic
Persona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM
RPAs' behaviors with those of target individuals by iteratively identifying the
cognitive divergence, either through free-form or theory-grounded, structured
analysis, between generated behaviors and human ground truth, and refining the
persona profile to mitigate these divergences.We evaluate DPRF with five LLMs
on four diverse behavior-prediction scenarios: formal debates, social media
posts with mental health issues, public interviews, and movie reviews.DPRF can
consistently improve behavioral alignment considerably over baseline personas
and generalizes across models and scenarios.Our work provides a robust
methodology for creating high-fidelity persona profiles and enhancing the
validity of downstream applications, such as user simulation, social studies,
and personalized AI.

</details>


### [173] [LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning](https://arxiv.org/abs/2510.14211)
*Beomseok Kang,Jiwon Song,Jae-Joon Kim*

Main category: cs.CL

TL;DR: LiteStage通过分阶段离线搜索和基于置信度的在线提前退出机制，在提高多阶段推理效率的同时，有效解决了层跳跃的准确性问题，实现了显著的加速和较低的准确率损失。


<details>
  <summary>Details</summary>
Motivation: 现有的多阶段推理方法虽然能提升小语言模型的推理能力，但推理延迟较高。现有的自适应加速技术（如层跳跃）在这一场景下难以平衡效率和准确性，主要面临两个挑战：1. 不同阶段对跳跃的敏感度不同；2. 会生成冗余的输出。因此，需要一种新的方法来解决这些问题。

Method: 提出LiteStage框架，结合分阶段离线搜索（为每个阶段分配最优的层预算）和基于置信度的在线生成提前退出机制（抑制不必要的解码），以实现延迟感知的层跳跃。

Result: 在OBQA、CSQA和StrategyQA三个基准测试上，LiteStage实现了高达1.70倍的加速，准确率损失小于4.0%，优于先前不进行训练的层跳跃方法。

Conclusion: LiteStage框架能够有效地解决多阶段推理中的效率与准确性权衡问题，通过优化的层预算分配和智能的提前退出机制，在保证较高准确率的同时显著降低了推理延迟。

Abstract: Multi-stage reasoning has emerged as an effective strategy for enhancing the
reasoning capability of small language models by decomposing complex problems
into sequential sub-stages. However, this comes at the cost of increased
latency. We observe that existing adaptive acceleration techniques, such as
layer skipping, struggle to balance efficiency and accuracy in this setting due
to two key challenges: (1) stage-wise variation in skip sensitivity, and (2)
the generation of redundant output tokens. To address these, we propose
LiteStage, a latency-aware layer skipping framework for multi-stage reasoning.
LiteStage combines a stage-wise offline search that allocates optimal layer
budgets with an online confidence-based generation early exit to suppress
unnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and
StrategyQA, show that LiteStage achieves up to 1.70x speedup with less than
4.0% accuracy loss, outperforming prior training-free layer skipping methods.

</details>


### [174] [Flip-Flop Consistency: Unsupervised Training for Robustness to Prompt Perturbations in LLMs](https://arxiv.org/abs/2510.14242)
*Parsa Hejabi,Elnaz Rahmati,Alireza S. Ziabari,Morteza Dehghani*

Main category: cs.CL

TL;DR: Flip-Flop Consistency (F^2C) 是一种无监督训练方法，通过共识交叉熵和表示对齐损失来提高大型语言模型 (LLM) 在面对相同提示的不同表述时的鲁棒性。实验表明，F^2C 能显著提高模型在多项 NLP 任务上的表现一致性、平均 F1 分数和泛化能力，同时降低性能方差。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在面对相同提示的不同表述时，往往会产生不一致的回答。

Method: F^2C 包含两个主要部分：1. 共识交叉熵 (CCE)，通过对不同提示变体的多数投票来创建硬伪标签。2. 表示对齐损失，将低置信度预测器和非多数预测器拉向由高置信度、多数投票变体建立的共识。

Result: 在 11 个数据集和 4 种 NLP 任务上进行评估，F^2C 平均提高了 11.62% 的观察一致性，提升了 8.94% 的平均 F1 分数，并降低了 3.29% 的跨格式性能方差。在域外评估中，F^2C 也能有效泛化，在大多数源-目标对中提高了 F1 分数和一致性，并降低了方差。即使仅在部分提示扰动上训练，F^2C 也能在未见过的格式上持续提升性能和一致性，并降低方差。

Conclusion: F^2C 是一种有效的无监督方法，能够增强 LLM 在提示扰动下的模型一致性、性能和泛化能力。

Abstract: Large Language Models (LLMs) often produce inconsistent answers when faced
with different phrasings of the same prompt. In this paper, we propose
Flip-Flop Consistency ($F^2C$), an unsupervised training method that improves
robustness to such perturbations. $F^2C$ is composed of two key components. The
first, Consensus Cross-Entropy (CCE), uses a majority vote across prompt
variations to create a hard pseudo-label. The second is a representation
alignment loss that pulls lower-confidence and non-majority predictors toward
the consensus established by high-confidence, majority-voting variations. We
evaluate our method on 11 datasets spanning four NLP tasks, with 4-15 prompt
variations per dataset. On average, $F^2C$ raises observed agreement by 11.62%,
improves mean $F_1$ by 8.94%, and reduces performance variance across formats
by 3.29%. In out-of-domain evaluations, $F^2C$ generalizes effectively,
increasing $\overline{F_1}$ and agreement while decreasing variance across most
source-target pairs. Finally, when trained on only a subset of prompt
perturbations and evaluated on held-out formats, $F^2C$ consistently improves
both performance and agreement while reducing variance. These findings
highlight $F^2C$ as an effective unsupervised method for enhancing LLM
consistency, performance, and generalization under prompt perturbations. Code
is available at
https://github.com/ParsaHejabi/Flip-Flop-Consistency-Unsupervised-Training-for-Robustness-to-Prompt-Perturbations-in-LLMs.

</details>


### [175] [MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2510.14252)
*Jihao Zhao,Zhiyuan Ji,Simin Niu,Hanyu Wang,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: 该研究提出了一种名为MoM（Mixtures of scenario-aware document Memories）的新框架，用于改进检索增强生成（RAG）系统。MoM将传统的被动文本分块改为主动理解，模拟人类阅读时的认知过程，以提取文档记忆。该框架能高效处理多领域文档，并训练小型语言模型（SLMs）具备主动构建文档记忆的能力。通过LLM生成文档逻辑大纲，结构化分块并提取核心内容，再结合多路径采样和多视角评估来选择最优文档记忆。此外，还引入反向推理策略训练SLMs，使其具备更深层次的类人阅读能力。最终，MoM生成的记忆内容通过三层检索机制进行检索，并在三个不同领域进行了广泛实验验证，证明其能解决现有RAG系统的文本分块问题，为SLMs实现以人为中心的智能文本处理铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在文本理解和推理方面存在局限性，该研究旨在通过模拟人类阅读的认知过程，实现从被动分块到主动理解的转变，以克服这些限制。

Method: 提出MoM框架，利用LLM生成文档逻辑大纲，进行结构化分块和核心内容提取。采用多路径采样和多视角评估机制（包括分块清晰度和提取完整性度量）来选择最优文档记忆。引入反向推理策略训练SLMs，并开发了基于概率建模理论证明的三层文档记忆检索机制。

Result: MoM框架能够为LLMs提供语义完整的文档记忆，解决现有RAG系统的文本分块挑战。实验证明，该框架在三个不同领域均表现出色，并使SLMs能够实现以人为中心的智能文本处理。

Conclusion: MoM框架通过模拟人类阅读的认知过程，实现了文档记忆的主动提取和高效处理，显著提升了RAG系统的性能，并为小型语言模型在智能文本处理领域开辟了新途径。

Abstract: The traditional RAG paradigm, which typically engages in the comprehension of
relevant text chunks in response to received queries, inherently restricts both
the depth of knowledge internalization and reasoning capabilities. To address
this limitation, our research transforms the text processing in RAG from
passive chunking to proactive understanding, defining this process as document
memory extraction with the objective of simulating human cognitive processes
during reading. Building upon this, we propose the Mixtures of scenario-aware
document Memories (MoM) framework, engineered to efficiently handle documents
from multiple domains and train small language models (SLMs) to acquire the
ability to proactively explore and construct document memories. The MoM
initially instructs large language models (LLMs) to simulate domain experts in
generating document logical outlines, thereby directing structured chunking and
core content extraction. It employs a multi-path sampling and multi-perspective
evaluation mechanism, specifically designing comprehensive metrics that
represent chunk clarity and extraction completeness to select the optimal
document memories. Additionally, to infuse deeper human-like reading abilities
during the training of SLMs, we incorporate a reverse reasoning strategy, which
deduces refined expert thinking paths from high-quality outcomes. Finally,
leveraging diverse forms of content generated by MoM, we develop a three-layer
document memory retrieval mechanism, which is grounded in our theoretical proof
from the perspective of probabilistic modeling. Extensive experimental results
across three distinct domains demonstrate that the MoM framework not only
resolves text chunking challenges in existing RAG systems, providing LLMs with
semantically complete document memories, but also paves the way for SLMs to
achieve human-centric intelligent text processing.

</details>


### [176] [Rewriting History: A Recipe for Interventional Analyses to Study Data Effects on Model Behavior](https://arxiv.org/abs/2510.14261)
*Rahul Nadkarni,Yanai Elazar,Hila Gonen,Noah A. Smith*

Main category: cs.CL

TL;DR: 本研究提出了一种实验方法，用于研究训练数据与语言模型行为之间的关系，通过“重写历史”干预数据批次并重新训练模型检查点来测试假设。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据如何影响语言模型的行为，并提供一种可复现的实验方法来测试相关假设。

Method: 提出了一种包含选择评估项、匹配相关文档、修改文档、重新训练和测量影响的实验流程，并使用共现统计和信息检索方法来识别可能促进知识学习的文档。

Result: 实验结果补充了过去将共现与模型行为联系起来的观察性分析，并表明现有识别相关训练文档的方法未能完全解释语言模型回答知识问题的能力。

Conclusion: 本研究提供了一个可供研究人员遵循的实验流程，以进一步测试关于训练数据如何影响模型行为的假设，并公开了相关代码以促进未来研究。

Abstract: We present an experimental recipe for studying the relationship between
training data and language model (LM) behavior. We outline steps for
intervening on data batches -- i.e., ``rewriting history'' -- and then
retraining model checkpoints over that data to test hypotheses relating data to
behavior. Our recipe breaks down such an intervention into stages that include
selecting evaluation items from a benchmark that measures model behavior,
matching relevant documents to those items, and modifying those documents
before retraining and measuring the effects. We demonstrate the utility of our
recipe through case studies on factual knowledge acquisition in LMs, using both
cooccurrence statistics and information retrieval methods to identify documents
that might contribute to knowledge learning. Our results supplement past
observational analyses that link cooccurrence to model behavior, while
demonstrating that extant methods for identifying relevant training documents
do not fully explain an LM's ability to correctly answer knowledge questions.
Overall, we outline a recipe that researchers can follow to test further
hypotheses about how training data affects model behavior. Our code is made
publicly available to promote future work.

</details>


### [177] [Less is More: Denoising Knowledge Graphs For Retrieval Augmented Generation](https://arxiv.org/abs/2510.14271)
*Yilun Zheng,Dan Yang,Jie Li,Lin Shang,Lihui Chen,Jiahao Xu,Sitao Luan*

Main category: cs.CL

TL;DR: 本研究提出了DEG-RAG框架，通过实体解析和关系验证来净化由LLM生成的知识图谱，从而提高检索增强生成（RAG）系统的性能。


<details>
  <summary>Details</summary>
Motivation: 大多数基于图的RAG系统依赖LLM自动构建知识图谱（KG），这往往会导致 KG 存在冗余实体和不可靠关系等噪声，从而降低检索和生成性能并增加计算成本。现有研究并未全面解决LLM生成KG的去噪问题。

Method: DEG-RAG框架包含两个主要技术：1. 实体解析：消除冗余实体。2. 三元组验证：移除错误的关系。该研究还系统地评估了LLM生成KG的实体解析方法，考察了不同的阻塞策略、嵌入选择、相似性度量和实体合并技术。

Result: 该方法显著减小了图谱的大小，并在一系列流行的基于图的RAG变体中持续提高了问答性能。实体解析实验证明了该方法的有效性。

Conclusion: 本研究首次全面探讨了LLM生成KG的实体解析问题，提出的DEG-RAG框架通过净化知识图谱，有效解决了基于图的RAG系统的噪声问题，并显著提升了问答性能。

Abstract: Retrieval-Augmented Generation (RAG) systems enable large language models
(LLMs) instant access to relevant information for the generative process,
demonstrating their superior performance in addressing common LLM challenges
such as hallucination, factual inaccuracy, and the knowledge cutoff.
Graph-based RAG further extends this paradigm by incorporating knowledge graphs
(KGs) to leverage rich, structured connections for more precise and inferential
responses. A critical challenge, however, is that most Graph-based RAG systems
rely on LLMs for automated KG construction, often yielding noisy KGs with
redundant entities and unreliable relationships. This noise degrades retrieval
and generation performance while also increasing computational cost. Crucially,
current research does not comprehensively address the denoising problem for
LLM-generated KGs. In this paper, we introduce DEnoised knowledge Graphs for
Retrieval Augmented Generation (DEG-RAG), a framework that addresses these
challenges through: (1) entity resolution, which eliminates redundant entities,
and (2) triple reflection, which removes erroneous relations. Together, these
techniques yield more compact, higher-quality KGs that significantly outperform
their unprocessed counterparts. Beyond the methods, we conduct a systematic
evaluation of entity resolution for LLM-generated KGs, examining different
blocking strategies, embedding choices, similarity metrics, and entity merging
techniques. To the best of our knowledge, this is the first comprehensive
exploration of entity resolution in LLM-generated KGs. Our experiments
demonstrate that this straightforward approach not only drastically reduces
graph size but also consistently improves question answering performance across
diverse popular Graph-based RAG variants.

</details>


### [178] [Retrofitting Small Multilingual Models for Retrieval: Matching 7B Performance with 300M Parameters](https://arxiv.org/abs/2510.14274)
*Lifu Tu,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: 小型多语言模型在检索任务上表现不如大型模型，本文旨在通过优化训练策略提升其检索能力。


<details>
  <summary>Details</summary>
Motivation: 现有的(-1B)小型多语言模型在多语言任务上表现良好，但在检索任务上明显落后于大型模型(>1B)，因此需要研究如何提升小型模型的检索性能。

Method: 通过实验探究训练数据规模、负采样策略和数据多样性对多语言嵌入模型检索效果的影响。

Result: 增加训练数据规模能提升性能但边际效益递减；引入困难负例对提升检索准确率至关重要；任务多样性比语言多样性对性能提升更重要。

Conclusion: 通过优化训练策略，可以构建一个约300M的紧凑型多语言模型，其检索性能可与甚至超越当前强大的7B模型。

Abstract: Training effective multilingual embedding models presents unique challenges
due to the diversity of languages and task objectives. Although small
multilingual models (<1 B parameters) perform well on multilingual tasks
generally, they consistently lag behind larger models (>1 B) in the most
prevalent use case: retrieval. This raises a critical question: Can smaller
models be retrofitted specifically for retrieval tasks to enhance their
performance? In this work, we investigate key factors that influence the
effectiveness of multilingual embeddings, focusing on training data scale,
negative sampling strategies, and data diversity. We find that while increasing
the scale of training data yields initial performance gains, these improvements
quickly plateau - indicating diminishing returns. Incorporating hard negatives
proves essential for consistently improving retrieval accuracy. Furthermore,
our analysis reveals that task diversity in the training data contributes more
significantly to performance than language diversity alone. As a result, we
develop a compact (approximately 300M) multilingual model that achieves
retrieval performance comparable to or even surpassing current strong 7B
models.

</details>


### [179] [Qwen3Guard Technical Report](https://arxiv.org/abs/2510.14276)
*Haiquan Zhao,Chenhan Yuan,Fei Huang,Xiaomeng Hu,Yichang Zhang,An Yang,Bowen Yu,Dayiheng Liu,Jingren Zhou,Junyang Lin,Baosong Yang,Chen Cheng,Jialong Tang,Jiandong Jiang,Jianwei Zhang,Jijie Xu,Ming Yan,Minmin Sun,Pei Zhang,Pengjun Xie,Qiaoyu Tang,Qin Zhu,Rong Zhang,Shibin Wu,Shuo Zhang,Tao He,Tianyi Tang,Tingyu Xia,Wei Liao,Weizhou Shen,Wenbiao Yin,Wenmeng Zhou,Wenyuan Yu,Xiaobin Wang,Xiaodong Deng,Xiaodong Xu,Xinyu Zhang,Yang Liu,Yeqiu Li,Yi Zhang,Yong Jiang,Yu Wan,Yuxin Zhou*

Main category: cs.CL

TL;DR: Qwen3Guard is a new set of multilingual safety guardrail models designed to address limitations of existing models in real-world LLM applications, offering fine-grained judgments and real-time monitoring for improved safety and compatibility with streaming inference.


<details>
  <summary>Details</summary>
Motivation: Existing guardrail models have limitations in real-world applications: their binary safety labels lead to inconsistent interpretation and inability to accommodate varying safety tolerances, and their need for complete model outputs prevents timely intervention during streaming LLM inference, increasing exposure to harmful partial outputs.

Method: Qwen3Guard introduces two specialized variants: Generative Qwen3Guard, which treats safety classification as an instruction-following task for tri-class judgments (safe, controversial, unsafe), and Stream Qwen3Guard, which adds a token-level classification head for real-time safety monitoring during incremental text generation. Both variants are available in three sizes (0.6B, 4B, and 8B parameters) and support up to 119 languages and dialects.

Result: Qwen3Guard achieves state-of-the-art performance in prompt and response safety classification across English, Chinese, and multilingual benchmarks. The models are available in multiple sizes and support numerous languages, offering scalable, low-latency safety moderation.

Conclusion: Qwen3Guard provides a comprehensive, scalable, and low-latency solution for LLM safety moderation, effectively addressing the limitations of existing guardrail models through fine-grained judgments and real-time monitoring capabilities, making it suitable for global LLM deployments. The models are released under the Apache 2.0 license.

Abstract: As large language models (LLMs) become more capable and widely used, ensuring
the safety of their outputs is increasingly critical. Existing guardrail
models, though useful in static evaluation settings, face two major limitations
in real-world applications: (1) they typically output only binary "safe/unsafe"
labels, which can be interpreted inconsistently across diverse safety policies,
rendering them incapable of accommodating varying safety tolerances across
domains; and (2) they require complete model outputs before performing safety
checks, making them fundamentally incompatible with streaming LLM inference,
thereby preventing timely intervention during generation and increasing
exposure to harmful partial outputs. To address these challenges, we present
Qwen3Guard, a series of multilingual safety guardrail models with two
specialized variants: Generative Qwen3Guard, which casts safety classification
as an instruction-following task to enable fine-grained tri-class judgments
(safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a
token-level classification head for real-time safety monitoring during
incremental text generation. Both variants are available in three sizes (0.6B,
4B, and 8B parameters) and support up to 119 languages and dialects, providing
comprehensive, scalable, and low-latency safety moderation for global LLM
deployments. Evaluated across English, Chinese, and multilingual benchmarks,
Qwen3Guard achieves state-of-the-art performance in both prompt and response
safety classification. All models are released under the Apache 2.0 license for
public use.

</details>


### [180] [PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.14278)
*Md Mahadi Hasan Nahid,Davood Rafiei*

Main category: cs.CL

TL;DR: 本研究提出了一个基于LLM的Agentic检索系统，通过问题分析、选择和补充三个专业Agent的协同工作，提高了多跳问答中证据检索的准确性和召回率，并减少了无关信息的干扰。


<details>
  <summary>Details</summary>
Motivation: 多跳问答需要检索多条证据，现有方法在检索精度和召回率之间难以平衡，并且容易引入无关信息。

Method: 设计了一个包含问题分析、上下文选择和证据补充三个Agent的结构化循环系统。问题分析Agent将复杂问题分解为子问题；选择Agent优先考虑检索相关性（精度）；补充Agent则关注召回遗漏的证据。选择Agent和补充Agent的迭代交互，旨在生成一个既紧凑又全面的证据集。

Result: 该方法在HotpotQA、2WikiMultiHopQA、MuSiQue和MultiHopRAG四个基准测试中，均优于现有方法。检索到的证据集信息量更少，但准确性更高，下游QA模型在利用这些证据时，能够达到比使用全部上下文更高的准确率。

Conclusion: Agentic检索系统通过LLM的结构化循环利用，能够高效、精确地检索多跳问答所需证据，克服了传统方法的局限性。

Abstract: Retrieval plays a central role in multi-hop question answering (QA), where
answering complex questions requires gathering multiple pieces of evidence. We
introduce an Agentic Retrieval System that leverages large language models
(LLMs) in a structured loop to retrieve relevant evidence with high precision
and recall. Our framework consists of three specialized agents: a Question
Analyzer that decomposes a multi-hop question into sub-questions, a Selector
that identifies the most relevant context for each sub-question (focusing on
precision), and an Adder that brings in any missing evidence (focusing on
recall). The iterative interaction between Selector and Adder yields a compact
yet comprehensive set of supporting passages. In particular, it achieves higher
retrieval accuracy while filtering out distracting content, enabling downstream
QA models to surpass full-context answer accuracy while relying on
significantly less irrelevant information. Experiments on four multi-hop QA
benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG --
demonstrates that our approach consistently outperforms strong baselines.

</details>


### [181] [Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL](https://arxiv.org/abs/2510.14296)
*Md Mahadi Hasan Nahid,Davood Rafiei,Weiwei Zhang,Yong Zhang*

Main category: cs.CL

TL;DR: 该研究提出了一种上下文感知的双向模式检索框架，将模式链接作为独立问题来解决，以提高Text-to-SQL系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法在改进SQL生成时，往往忽略了模式链接的检索，这可能导致模型产生幻觉和执行失败。

Method: 提出了一种上下文感知的双向模式检索框架，结合了表格优先和列优先的检索策略，并辅以问题分解、关键字提取和关键短语提取等技术。

Result: 实验证明，该方法显著提高了模式召回率，减少了假阳性。使用该方法检索到的模式进行SQL生成，性能优于全模式基线，并接近于完备模式的性能，且无需查询重构。

Conclusion: 模式链接是提高Text-to-SQL准确性和效率的关键因素。

Abstract: Schema linking -- the process of aligning natural language questions with
database schema elements -- is a critical yet underexplored component of
Text-to-SQL systems. While recent methods have focused primarily on improving
SQL generation, they often neglect the retrieval of relevant schema elements,
which can lead to hallucinations and execution failures. In this work, we
propose a context-aware bidirectional schema retrieval framework that treats
schema linking as a standalone problem. Our approach combines two complementary
strategies: table-first retrieval followed by column selection, and
column-first retrieval followed by table selection. It is further augmented
with techniques such as question decomposition, keyword extraction, and
keyphrase extraction. Through comprehensive evaluations on challenging
benchmarks such as BIRD and Spider, we demonstrate that our method
significantly improves schema recall while reducing false positives. Moreover,
SQL generation using our retrieved schema consistently outperforms full-schema
baselines and closely approaches oracle performance, all without requiring
query refinement. Notably, our method narrows the performance gap between full
and perfect schema settings by 50\%. Our findings highlight schema linking as a
powerful lever for enhancing Text-to-SQL accuracy and efficiency.

</details>


### [182] [Constraint-Driven Small Language Models Based on Agent and OpenAlex Knowledge Graph: Mining Conceptual Pathways and Discovering Innovation Points in Academic Papers](https://arxiv.org/abs/2510.14303)
*Ziye Xia,Sergei S. Ospichev*

Main category: cs.CL

TL;DR: 本研究提出了一种基于提示工程的关键概念路径分析方法，利用小型语言模型和知识图谱来识别学术论文中的创新点和稀有路径。


<details>
  <summary>Details</summary>
Motivation: 学术出版物激增，现有论文数据库在概念关系挖掘方面存在不足。

Method: 基于OpenAlex知识图谱，分析8000篇论文数据，提出提示工程的关键概念路径分析方法，并微调Qwen和DeepSeek模型。

Result: 发现论文关键概念路径分布与创新点和稀有路径之间存在强相关性，提高了概念提取和创新点识别的准确性。

Conclusion: 所提出的方法能够有效识别学术论文中的创新点和稀有路径，为深入的学术文献分析提供了新的途径。

Abstract: In recent years, the rapid increase in academic publications across various
fields has posed severe challenges for academic paper analysis: scientists
struggle to timely and comprehensively track the latest research findings and
methodologies. Key concept extraction has proven to be an effective analytical
paradigm, and its automation has been achieved with the widespread application
of language models in industrial and scientific domains. However, existing
paper databases are mostly limited to similarity matching and basic
classification of key concepts, failing to deeply explore the relational
networks between concepts. This paper is based on the OpenAlex opensource
knowledge graph. By analyzing nearly 8,000 open-source paper data from
Novosibirsk State University, we discovered a strong correlation between the
distribution patterns of paper key concept paths and both innovation points and
rare paths. We propose a prompt engineering-based key concept path analysis
method. This method leverages small language models to achieve precise key
concept extraction and innovation point identification, and constructs an agent
based on a knowledge graph constraint mechanism to enhance analysis accuracy.
Through fine-tuning of the Qwen and DeepSeek models, we achieved significant
improvements in accuracy, with the models publicly available on the Hugging
Face platform.

</details>


### [183] [MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical Problem Solving and Reasoning](https://arxiv.org/abs/2510.14305)
*Mahbub E Sobhani,Md. Faiyaz Abdullah Sayeedi,Tasnim Mohiuddin,Md Mofijul Islam,Swakkhar Shatabda*

Main category: cs.CL

TL;DR: LLMs在数学推理方面存在多语言能力不足的问题，尤其是在低资源语言中。新基准MathMist旨在解决这一问题，但结果显示LLMs在跨语言数学推理方面仍有待提高。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在多语言和跨语言数学推理方面的能力，填补现有基准在低资源语言方面的空白。

Method: 构建了一个包含21K以上多语言对齐问答对的MathMist基准，涵盖七种语言，并评估了多种LLMs在零样本、思维链和代码转换推理下的表现。

Result: LLMs在跨语言数学推理方面存在持续的不足，在低资源语言中表现尤为明显。

Conclusion: 尽管LLMs在通用推理方面表现出色，但它们在跨语言数学推理方面仍存在挑战，尤其是在低资源语言环境中。

Abstract: Mathematical reasoning remains one of the most challenging domains for large
language models (LLMs), requiring not only linguistic understanding but also
structured logical deduction and numerical precision. While recent LLMs
demonstrate strong general-purpose reasoning abilities, their mathematical
competence across diverse languages remains underexplored. Existing benchmarks
primarily focus on English or a narrow subset of high-resource languages,
leaving significant gaps in assessing multilingual and cross-lingual
mathematical reasoning. To address this, we introduce MathMist, a parallel
multilingual benchmark for mathematical problem solving and reasoning. MathMist
encompasses over 21K aligned question-answer pairs across seven languages,
representing a balanced coverage of high-, medium-, and low-resource linguistic
settings. The dataset captures linguistic variety, multiple types of problem
settings, and solution synthesizing capabilities. We systematically evaluate a
diverse suite of models, including open-source small and medium LLMs,
proprietary systems, and multilingual-reasoning-focused models, under
zero-shot, chain-of-thought (CoT), and code-switched reasoning paradigms. Our
results reveal persistent deficiencies in LLMs' ability to perform consistent
and interpretable mathematical reasoning across languages, with pronounced
degradation in low-resource settings. All the codes and data are available at
GitHub: https://github.com/mahbubhimel/MathMist

</details>


### [184] [MERLIN: A Testbed for Multilingual Multimodal Entity Recognition and Linking](https://arxiv.org/abs/2510.14307)
*Sathyanarayanan Ramamoorthy,Vishwa Shah,Simran Khanuja,Zaid Sheikh,Shan Jie,Ann Chia,Shearman Chua,Graham Neubig*

Main category: cs.CL

TL;DR: MERLIN是一个用于多语言多模态实体链接的新型测试平台系统，包含BBC新闻文章标题及其配图，涵盖五种语言，拥有超过7000个命名实体提及量和2500个独特的Wikidata实体。


<details>
  <summary>Details</summary>
Motivation: 创建了一个包含五种语言（印地语、日语、印度尼西亚语、越南语和泰米尔语）BBC新闻文章标题及其对应图像的数据集，用于多语言多模态实体链接任务。

Method: 使用多语言和多模态实体链接方法，并探索了LLaMa-2和Aya-23等语言模型。

Result: 引入了MERLIN测试平台；创建了包含五种语言的新数据集；发现视觉数据能提高实体链接的准确性，尤其是在文本语境模糊或不足的情况下，并且对多语言能力较弱的模型尤其有效。

Conclusion: 视觉数据对于多语言多模态实体链接任务是有益的，特别是当文本信息不足时。

Abstract: This paper introduces MERLIN, a novel testbed system for the task of
Multilingual Multimodal Entity Linking. The created dataset includes BBC news
article titles, paired with corresponding images, in five languages: Hindi,
Japanese, Indonesian, Vietnamese, and Tamil, featuring over 7,000 named entity
mentions linked to 2,500 unique Wikidata entities. We also include several
benchmarks using multilingual and multimodal entity linking methods exploring
different language models like LLaMa-2 and Aya-23. Our findings indicate that
incorporating visual data improves the accuracy of entity linking, especially
for entities where the textual context is ambiguous or insufficient, and
particularly for models that do not have strong multilingual abilities. For the
work, the dataset, methods are available here at
https://github.com/rsathya4802/merlin

</details>


### [185] [Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL](https://arxiv.org/abs/2510.14318)
*Marwa Abdulhai,Ryan Cheng,Aryansh Shrivastava,Natasha Jaques,Yarin Gal,Sergey Levine*

Main category: cs.CL

TL;DR: LLMs present safety risks due to their potential for deception, with current models exhibiting deceptive behavior in about 26% of dialogue turns, even with benign objectives. A new metric, belief misalignment, and a multi-turn reinforcement learning method are proposed to quantify and reduce this deception, achieving a 77.6% reduction.


<details>
  <summary>Details</summary>
Motivation: The widespread use of LLMs in critical applications necessitates addressing their potential for deceptive outputs, which poses significant safety risks due to unpredictable behavior and insufficient safeguards against issues like hallucination and misinformation.

Method: The paper proposes a novel metric, belief misalignment, to quantify deception in LLM dialogues. Deception is evaluated across four scenarios using this metric and five existing metrics on eight state-of-the-art models. A multi-turn reinforcement learning methodology is introduced to fine-tune LLMs for reduced deception.

Result: LLMs exhibit deceptive behavior in approximately 26% of dialogue turns, which can increase by up to 31% when prompted to deceive. Even models trained with RLHF show deception in 43% of turns. The proposed belief misalignment metric shows a higher correlation with human judgments than existing metrics. The multi-turn reinforcement learning method achieves a 77.6% reduction in deceptive behaviors compared to instruction-tuned models.

Conclusion: Deception in LLM dialogues is a significant safety concern that requires specialized metrics and mitigation strategies beyond single-utterance analysis. The belief misalignment metric and the multi-turn reinforcement learning approach show promise in quantifying and reducing deceptive behaviors in LLMs.

Abstract: Large Language Models (LLMs) interact with millions of people worldwide in
applications such as customer support, education and healthcare. However, their
ability to produce deceptive outputs, whether intentionally or inadvertently,
poses significant safety concerns. The unpredictable nature of LLM behavior,
combined with insufficient safeguards against hallucination, misinformation,
and user manipulation, makes their misuse a serious, real-world risk. In this
paper, we investigate the extent to which LLMs engage in deception within
dialogue, and propose the belief misalignment metric to quantify deception. We
evaluate deception across four distinct dialogue scenarios, using five
established deception detection metrics and our proposed metric. Our findings
reveal this novel deception measure correlates more closely with human
judgments than any existing metrics we test. Additionally, our benchmarking of
eight state-of-the-art models indicates that LLMs naturally exhibit deceptive
behavior in approximately 26% of dialogue turns, even when prompted with
seemingly benign objectives. When prompted to deceive, LLMs are capable of
increasing deceptiveness by as much as 31% relative to baselines. Unexpectedly,
models trained with RLHF, the predominant approach for ensuring the safety of
widely-deployed LLMs, still exhibit deception at a rate of 43% on average.
Given that deception in dialogue is a behavior that develops over an
interaction history, its effective evaluation and mitigation necessitates
moving beyond single-utterance analyses. We introduce a multi-turn
reinforcement learning methodology to fine-tune LLMs to reduce deceptive
behaviors, leading to a 77.6% reduction compared to other instruction-tuned
models.

</details>


### [186] [A Robust Classification Method using Hybrid Word Embedding for Early Diagnosis of Alzheimer's Disease](https://arxiv.org/abs/2510.14332)
*Yangyang Li*

Main category: cs.CL

TL;DR: 通过混合词嵌入和微调超参数，提出了一种新方法，可实现 91% 的准确率和 97% 的 AUC，用于区分早期阿尔茨海默病 (AD) 和健康个体，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 早期检测阿尔茨海默病 (AD) 对患者至关重要，可以早期治疗，减轻症状并降低医疗保健负担。语言能力的变化是 AD 的一个主要迹象，可用于早期诊断。

Method: 该方法结合了 Doc2Vec 和 ELMo 的词向量，创建了混合词嵌入，以获得句子的困惑度得分，从而识别句子的流畅性并捕捉语义。通过添加语言学特征来丰富词嵌入，以分析句法和语义。然后，将嵌入的特征向量输入逻辑回归模型，并对整个流程的超参数进行微调。

Result: 该方法实现了 91% 的分类准确率和 97% 的曲线下面积 (AUC)，在区分早期 AD 和健康受试者方面优于现有的基于 NLP 的 AD 诊断模型（准确率为 88%）。通过重复实验研究了模型稳定性，发现模型即使在随机划分训练数据时也具有稳定性（准确率标准差 = 0.0403；AUC 标准差 = 0.0174）。

Conclusion: 该混合词嵌入和超参数微调方法在早期 AD 检测方面既准确又稳定，可作为大规模筛查方法或辅助医生诊断 AD 的工具。

Abstract: Early detection of Alzheimer's Disease (AD) is greatly beneficial to AD
patients, leading to early treatments that lessen symptoms and alleviating
financial burden of health care. As one of the leading signs of AD, language
capability changes can be used for early diagnosis of AD. In this paper, I
develop a robust classification method using hybrid word embedding and
fine-tuned hyperparameters to achieve state-of-the-art accuracy in the early
detection of AD. Specifically, we create a hybrid word embedding based on word
vectors from Doc2Vec and ELMo to obtain perplexity scores of the sentences. The
scores identify whether a sentence is fluent or not and capture semantic
context of the sentences. I enrich the word embedding by adding linguistic
features to analyze syntax and semantics. Further, we input an embedded feature
vector into logistic regression and fine tune hyperparameters throughout the
pipeline. By tuning hyperparameters of the machine learning pipeline (e.g.,
model regularization parameter, learning rate and vector size of Doc2Vec, and
vector size of ELMo), I achieve 91% classification accuracy and an Area Under
the Curve (AUC) of 97% in distinguishing early AD from healthy subjects. Based
on my knowledge, my model with 91% accuracy and 97% AUC outperforms the best
existing NLP model for AD diagnosis with an accuracy of 88% [32]. I study the
model stability through repeated experiments and find that the model is stable
even though the training data is split randomly (standard deviation of accuracy
= 0.0403; standard deviation of AUC = 0.0174). This affirms our proposed method
is accurate and stable. This model can be used as a large-scale screening
method for AD, as well as a complementary examination for doctors to detect AD.

</details>


### [187] [Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts](https://arxiv.org/abs/2510.14351)
*Perapard Ngokpol,Kun Kerdthaisong,Pasin Buakhaw,Pitikorn Khlaisamniang,Supasate Vorathammathorn,Piyalitt Ittichaiwong,Nutchanon Yongsatianchot*

Main category: cs.CL

TL;DR: LLMs 在扮演特定版本角色（如不同宇宙的超级英雄）方面能力未被充分探索。我们提出了一个包含 30 位英雄和 90 个特定版本角色的基准 Beyond One World，包含“正典事件”和“道德困境”两项任务，并提出了“思考-行动匹配”指标来评估模型的信任度。实验发现，思维链提示对弱模型的叙事连贯性有改善作用，但可能降低强模型的正典准确性；跨版本泛化仍是重大障碍；模型通常擅长“思考”或“行动”中的一项，但很少两者兼顾。该基准暴露了多重宇宙一致性和推理对齐方面的关键差距。


<details>
  <summary>Details</summary>
Motivation: LLMs 越来越多地被用作角色扮演代理，但它们在忠实和一致地扮演特定版本角色（例如，漫画和电影宇宙中的超级英雄）方面的能力仍有待探索。

Method: 我们引入了一个名为 Beyond One World 的基准，用于跨越 30 位标志性英雄和 90 个特定于正典的版本进行角色扮演。该基准包含两项任务：(i) 正典事件，用于探测关键生活阶段的事实回忆；(ii) 道德困境，用于让模型面对充满伦理争议的场景。我们根据正典准确性和推理保真度对响应进行评分，该框架将内部思考（“思考”）与外部决策（“行动”）分开。我们还提出了思考-行动匹配，一个量化原因和行动之间一致性的指标，作为模型可信度的代理。

Result: 实验结果表明：(1) 思维链提示可以提高较弱模型的叙事连贯性，但会降低较强模型的正典准确性；(2) 同一角色的跨版本泛化仍然是一个主要的障碍；(3) 模型通常在“思考”或“行动”方面表现出色，但很少两者兼顾。

Conclusion: Beyond One World 基准暴露了多重宇宙一致性和推理对齐方面的关键差距，为角色扮演 LLM 提供了一个具有挑战性的评估。

Abstract: Large language models (LLMs) are increasingly used as role-playing agents,
yet their capacity to faithfully and consistently portray version-specific
characters -- for example, superheroes across comic and cinematic universes --
remains underexplored. Superhero canons such as Marvel and DC provide a rich
testbed: decades of storytelling yield multiple incarnations of the same
character with distinct histories, values, and moral codes. To study this
problem, we introduce Beyond One World, a benchmark for character-grounded
roleplay spanning 30 iconic heroes and 90 canon-specific versions. The
benchmark comprises two tasks: (i) Canon Events, which probes factual recall of
pivotal life stages, and (ii) Moral Dilemmas, which confronts models with
ethically charged scenarios. We score responses for canonical accuracy and
reasoning fidelity under a framework that separates internal deliberation
("thinking") from outward decisions ("acting"). We further propose Think-Act
Matching, a metric that quantifies alignment between reasons and actions and
serves as a proxy for model trustworthiness. Experiments across reasoning- and
non-reasoning-oriented models yield three findings: (1) chain-of-thought
prompting improves narrative coherence in weaker models but can reduce
canonical accuracy in stronger ones; (2) cross-version generalization within a
character remains a major obstacle; and (3) models often excel at either
thinking or acting, but rarely both. Beyond One World exposes critical gaps in
multiversal consistency and reasoning alignment, offering a challenging
evaluation for role-playing LLMs.

</details>


### [188] [CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical Question Answering](https://arxiv.org/abs/2510.14353)
*Ziad Elshaer,Essam A. Rashed*

Main category: cs.CL

TL;DR: 本研究提出了一种无需微调、利用模型多样性来增强医疗问答能力的置信度驱动多模型框架，通过置信度检测和自适应路由机制，引导低置信度查询至具有互补知识的辅助模型进行协作推理。


<details>
  <summary>Details</summary>
Motivation: 高价值的医疗大语言模型（LLMs）通常需要大量的微调和计算资源，这限制了资源有限的医疗机构的可及性。

Method: 该框架采用两阶段架构：置信度检测模块评估主模型的不确定性，自适应路由机制将低置信度的查询路由到具有互补知识的辅助模型以进行协作推理。

Result: 在 MedQA、MedMCQA 和 PubMedQA 三个医学基准测试中，使用 Qwen3-30B-A3B-Instruct、Phi-4 14B 和 Gemma 2 12B 进行评估，结果显示该框架的性能具有竞争力，在 PubMedQA (95.0%) 和 MedMCQA (78.0%) 上表现尤为强劲。消融研究证实，置信度感知路由与多模型协作的性能显著优于单一模型方法和统一推理策略。

Conclusion: 战略性的模型协作提供了一种实用且计算高效的途径来改进医疗人工智能系统，对于在资源有限的环境中普及先进的医疗人工智能具有重要意义。

Abstract: High-performing medical Large Language Models (LLMs) typically require
extensive fine-tuning with substantial computational resources, limiting
accessibility for resource-constrained healthcare institutions. This study
introduces a confidence-driven multi-model framework that leverages model
diversity to enhance medical question answering without fine-tuning. Our
framework employs a two-stage architecture: a confidence detection module
assesses the primary model's certainty, and an adaptive routing mechanism
directs low-confidence queries to Helper models with complementary knowledge
for collaborative reasoning. We evaluate our approach using
Qwen3-30B-A3B-Instruct, Phi-4 14B, and Gemma 2 12B across three medical
benchmarks; MedQA, MedMCQA, and PubMedQA. Result demonstrate that our framework
achieves competitive performance, with particularly strong results in PubMedQA
(95.0\%) and MedMCQA (78.0\%). Ablation studies confirm that confidence-aware
routing combined with multi-model collaboration substantially outperforms
single-model approaches and uniform reasoning strategies. This work establishes
that strategic model collaboration offers a practical, computationally
efficient pathway to improve medical AI systems, with significant implications
for democratizing access to advanced medical AI in resource-limited settings.

</details>


### [189] [On the Ability of LLMs to Handle Character-Level Perturbations: How Well and How?](https://arxiv.org/abs/2510.14365)
*Anyun Zhuo,Xuefei Ning,Ningyuan Li,Yu Wang,Pinyan Lu*

Main category: cs.CL

TL;DR: 该研究提出了一种名为‘
ameshort’的方法，通过在输入文本的每个字符后插入不可见的Unicode控制字符来干扰LLM的正常工作，以防止其被滥用，例如在线考试系统。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在面对频繁、结构化的字符级扰动（如在每个输入字符后插入噪声字符）时的韧性，特别是为了防止LLM被滥用。

Method: 
ameshort"方法，该方法将不可见的Unicode控制字符插入文本中，以破坏分词并显著降低信噪比。

Result: 尽管有强烈的混淆，许多LLM仍然保持了显著的性能。研究人员通过对模型、问题和噪声配置进行全面评估，检验了这种鲁棒性的程度和机制。

Conclusion: LLM在字符级扰动下表现出意想不到的低级鲁棒性，这揭示了它们被滥用的风险以及在各种应用中部署LLM的可靠性问题。

Abstract: This work investigates the resilience of contemporary LLMs against frequent
and structured character-level perturbations, specifically through the
insertion of noisy characters after each input character. We introduce
\nameshort{}, a practical method that inserts invisible Unicode control
characters into text to discourage LLM misuse in scenarios such as online exam
systems. Surprisingly, despite strong obfuscation that fragments tokenization
and reduces the signal-to-noise ratio significantly, many LLMs still maintain
notable performance. Through comprehensive evaluation across model-, problem-,
and noise-related configurations, we examine the extent and mechanisms of this
robustness, exploring both the handling of character-level tokenization and
\textit{implicit} versus \textit{explicit} denoising mechanism hypotheses of
character-level noises. We hope our findings on the low-level robustness of
LLMs will shed light on the risks of their misuse and on the reliability of
deploying LLMs across diverse applications.

</details>


### [190] [From Binary to Bilingual: How the National Weather Service is Using Artificial Intelligence to Develop a Comprehensive Translation Program](https://arxiv.org/abs/2510.14369)
*Joseph E. Trujillo-Falcon,Monica L. Bozeman,Liam E. Llewellyn,Samuel T. Halvorson,Meryl Mizell,Stuti Deshpande,Bob Manning,Todd Fagin*

Main category: cs.CL

TL;DR: 美国国家气象局（NWS）正在开发一个自动翻译工具，以服务于不以英语为母语的6880万美国人，利用人工智能和LILT的专利技术，该工具支持西班牙语、中文、越南语等多种语言，并整合了GIS映射和道德AI实践，最终目标是为所有美国人提供气象预警。


<details>
  <summary>Details</summary>
Motivation: 为了服务美国6880万不以英语为母语的人群，推进“天气准备好国家”的建设，国家气象局（NWS）正在开发一个系统性的翻译计划。

Method: NWS与LILT合作，利用其专利的训练流程，使大型语言模型（LLMs）能够适应天气术语和信息的神经机器翻译（NMT）工具。该系统设计用于跨天气预报办公室（WFOs）和国家中心进行扩展，并采用GIS映射来识别不同NWS地区的需求，同时整合了道德AI实践。

Result: 目前正在开发支持西班牙语、简体中文、越南语等多种语言的系统。该系统能够提供准确、及时、符合文化习惯的翻译，显著减少人工翻译时间，减轻NWS的运营负担。一个包含实验性多语言NWS产品的网站已经上线，包括翻译的预警、7天预报和教育宣传活动。

Conclusion: 该项目通过开发自动翻译工具，并结合GIS映射和道德AI实践，显著改善了NWS服务于非英语母语人群的能力，是实现覆盖所有美国人的国家预警系统的重要一步。

Abstract: To advance a Weather-Ready Nation, the National Weather Service (NWS) is
developing a systematic translation program to better serve the 68.8 million
people in the U.S. who do not speak English at home. This article outlines the
foundation of an automated translation tool for NWS products, powered by
artificial intelligence. The NWS has partnered with LILT, whose patented
training process enables large language models (LLMs) to adapt neural machine
translation (NMT) tools for weather terminology and messaging. Designed for
scalability across Weather Forecast Offices (WFOs) and National Centers, the
system is currently being developed in Spanish, Simplified Chinese, Vietnamese,
and other widely spoken non-English languages. Rooted in best practices for
multilingual risk communication, the system provides accurate, timely, and
culturally relevant translations, significantly reducing manual translation
time and easing operational workloads across the NWS. To guide the distribution
of these products, GIS mapping was used to identify language needs across
different NWS regions, helping prioritize resources for the communities that
need them most. We also integrated ethical AI practices throughout the
program's design, ensuring that transparency, fairness, and human oversight
guide how automated translations are created, evaluated, and shared with the
public. This work has culminated into a website featuring experimental
multilingual NWS products, including translated warnings, 7-day forecasts, and
educational campaigns, bringing the country one step closer to a national
warning system that reaches all Americans.

</details>


### [191] [PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora](https://arxiv.org/abs/2510.14377)
*Mykolas Sveistrys,Richard Kunert*

Main category: cs.CL

TL;DR: LLMs 和 RAG 在单跳和多跳问答方面取得了进展，但许多实际问题需要跨多个文档进行聚合，并且对遗漏一个文档非常敏感。本文提出了 pluri-hop 问题，并构建了一个名为 PluriHopWIND 的多语言数据集。现有 RAG 方法在此数据集上的表现不佳，F1 分数低于 40%。因此，本文提出了 PluriHopRAG，一种通过“逐个检查所有文档，廉价过滤”的方法来解决此问题，并将 F1 分数提高了 18-52%。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的问题，特别是关于重复报告数据（如医疗记录、合规文件、维护日志）的问题，通常需要聚合所有相关文档的信息，而不是仅仅检索少数几个文档。然而，现有的问答系统在处理这类需要跨大量文档进行信息聚合的问题时，表现不佳，并且对遗漏任何一个关键文档都非常敏感。因此，有必要研究这类“pluri-hop”问题，并开发能够有效处理这类问题的问答系统。

Method: 1. 定义并形式化了“pluri-hop”问题，该问题具有三个标准：召回敏感性、详尽性和精确性。 2. 构建了一个名为 PluriHopWIND 的诊断性多语言数据集，包含 48 个 pluri-hop 问题，源自 191 份德语和英语的真实世界风力行业报告。 3. 测试了传统的 RAG、基于图和多模态的 RAG 变体，发现它们在此数据集上的表现不佳。 4. 提出了一种名为 PluriHopRAG 的新 RAG 架构，其核心思想是“逐个检查所有文档，廉价过滤”。该架构包括：(i) 将查询分解为文档级子问题；(ii) 使用交叉编码器过滤器在进行昂贵的 LLM 推理之前剔除不相关的文档。 5. 评估了 PluriHopRAG 的性能，发现其相对于基础 LLM 提高了 18-52% 的 F1 分数。

Result: PluriHopWIND 数据集比其他常见数据集更具挑战性，重复性更高，干扰文档密度更大。在 PluriHopWIND 数据集上，传统的 RAG、基于图和多模态的 RAG 变体的语句级 F1 分数均未超过 40%。提出的 PluriHopRAG 架构在 F1 分数上相对于基础 LLM 取得了 18-52% 的相对提升。

Conclusion: 尽管规模不大，PluriHopWIND 数据集能够有效地暴露当前问答系统在处理重复的、充满干扰信息语料库时的局限性。PluriHopRAG 的性能表明，详尽检索和早期过滤是一种有效的替代传统 top-k 方法的策略，能够显著提高在复杂报告语料库上的问答性能。

Abstract: Recent advances in large language models (LLMs) and retrieval-augmented
generation (RAG) have enabled progress on question answering (QA) when relevant
evidence is in one (single-hop) or multiple (multi-hop) passages. Yet many
realistic questions about recurring report data - medical records, compliance
filings, maintenance logs - require aggregation across all documents, with no
clear stopping point for retrieval and high sensitivity to even one missed
passage. We term these pluri-hop questions and formalize them by three
criteria: recall sensitivity, exhaustiveness, and exactness. To study this
setting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48
pluri-hop questions built from 191 real-world wind industry reports in German
and English. We show that PluriHopWIND is 8-40% more repetitive than other
common datasets and thus has higher density of distractor documents, better
reflecting practical challenges of recurring report corpora. We test a
traditional RAG pipeline as well as graph-based and multimodal variants, and
find that none of the tested approaches exceed 40% in statement-wise F1 score.
Motivated by this, we propose PluriHopRAG, a RAG architecture that follows a
"check all documents individually, filter cheaply" approach: it (i) decomposes
queries into document-level subquestions and (ii) uses a cross-encoder filter
to discard irrelevant documents before costly LLM reasoning. We find that
PluriHopRAG achieves relative F1 score improvements of 18-52% depending on base
LLM. Despite its modest size, PluriHopWIND exposes the limitations of current
QA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance
highlights the value of exhaustive retrieval and early filtering as a powerful
alternative to top-k methods.

</details>


### [192] [Suicidal Comment Tree Dataset: Enhancing Risk Assessment and Prediction Through Contextual Analysis](https://arxiv.org/abs/2510.14395)
*Jun Li,Qun Zhao*

Main category: cs.CL

TL;DR: 该研究通过分析Reddit评论树数据，显著提高了对用户自杀风险的预测能力。


<details>
  <summary>Details</summary>
Motivation: 之前的研究主要关注单条社交媒体帖子，但忽略了用户随时间变化的评论树信息，而用户意图往往通过历史帖子和互动评论逐步揭示，因此本研究旨在填补这一空白。

Method: 构建了一个包含用户发帖历史和评论的高质量标注数据集（来自Reddit），并采用了基于C-SSRS的四标签标注框架。利用统计分析和大型语言模型（LLMs）实验来评估评论树信息的影响。

Result: 实验结果表明，整合评论树数据能够显著提升对用户自杀风险水平的区分和预测能力。

Conclusion: 本研究为提高高风险个体检测的准确性提供了新见解，为早期自杀干预策略奠定了基础。

Abstract: Suicide remains a critical global public health issue. While previous studies
have provided valuable insights into detecting suicidal expressions in
individual social media posts, limited attention has been paid to the analysis
of longitudinal, sequential comment trees for predicting a user's evolving
suicidal risk. Users, however, often reveal their intentions through historical
posts and interactive comments over time. This study addresses this gap by
investigating how the information in comment trees affects both the
discrimination and prediction of users' suicidal risk levels. We constructed a
high-quality annotated dataset, sourced from Reddit, which incorporates users'
posting history and comments, using a refined four-label annotation framework
based on the Columbia Suicide Severity Rating Scale (C-SSRS). Statistical
analysis of the dataset, along with experimental results from Large Language
Models (LLMs) experiments, demonstrates that incorporating comment trees data
significantly enhances the discrimination and prediction of user suicidal risk
levels. This research offers a novel insight to enhancing the detection
accuracy of at-risk individuals, thereby providing a valuable foundation for
early suicide intervention strategies.

</details>


### [193] [Your Next Token Prediction: A Multilingual Benchmark for Personalized Response Generation](https://arxiv.org/abs/2510.14398)
*Shiyao Ding,Takayuki Ito*

Main category: cs.CL

TL;DR: LLMs 难以生成反映个人真实沟通风格的回复，因此我们提出了“你的下一个词预测 (YNTP)”任务，并构建了一个包含 100 个多语言对话会话的多语言基准，以解决此问题。


<details>
  <summary>Details</summary>
Motivation: LLMs 难以生成反映个人真实沟通风格的回复，并且由于隐私问题，难以收集真实的 SNS 或电子邮件历史记录。

Method: 我们提出了“你的下一个词预测 (YNTP)”任务，通过受控的人机对话来模拟用户的精确词语选择。我们构建了一个包含 100 个对话会话的多语言基准，用户与基于 MBTI 维度进行心理学设定的 NPC 进行了为期五天的互动。

Result: 我们评估了基于提示和基于微调的个性化方法，并建立了 YNTP 的第一个基准以及用户对齐语言模型的基础。

Conclusion: YNTP 任务和相关基准为用户对齐语言模型奠定了基础，并能够分析用户的内部模型。

Abstract: Large language models (LLMs) excel at general next-token prediction but still
struggle to generate responses that reflect how individuals truly communicate,
such as replying to emails or social messages in their own style. However, real
SNS or email histories are difficult to collect due to privacy concerns. To
address this, we propose the task of "Your Next Token Prediction (YNTP)", which
models a user's precise word choices through controlled human-agent
conversations. We build a multilingual benchmark of 100 dialogue sessions
across English, Japanese, and Chinese, where users interact for five days with
psychologically grounded NPCs based on MBTI dimensions. This setup captures
natural, daily-life communication patterns and enables analysis of users'
internal models. We evaluate prompt-based and fine-tuning-based personalization
methods, establishing the first benchmark for YNTP and a foundation for
user-aligned language modeling. The dataset is available at:
https://github.com/AnonymousHub4Submissions/your-next-token-prediction-dataset-100

</details>


### [194] [MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical Question Answering](https://arxiv.org/abs/2510.14400)
*Yingpeng Ning,Yuanyuan Sun,Ling Luo,Yanhua Wang,Yuchen Pan,Hongfei Lin*

Main category: cs.CL

TL;DR: LLM在生物医学QA中存在幻觉问题，提出MedTrust-Guided Iterative RAG框架，通过引用感知推理、迭代检索-验证和MedTrust-Align Module来提高事实一致性，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有的基于检索增强生成（RAG）的生物医学QA方法存在幻觉问题，这是由于检索后噪声和证据验证不足导致的，从而影响了回答的可靠性。

Method: 提出MedTrust-Guided Iterative RAG框架，包含三个创新点：1. 强制引用感知推理，要求生成内容必须基于检索到的医学文档，并在证据不足时使用结构化负知识断言。2. 采用迭代检索-验证过程，由验证代理评估证据充分性并通过医学差距分析优化查询。3. 集成MedTrust-Align Module（MTAM），结合正例和负例，利用直接偏好优化来强化引用基础推理并惩罚易产生幻觉的回答模式。

Result: 在MedMCQA、MedQA和MMLU-Med数据集上进行实验，在多种模型架构上均优于现有方法，在LLaMA3.1-8B-Instruct和Qwen3-8B上分别提高了2.7%和2.4%的准确率。

Conclusion: MedTrust-Guided Iterative RAG框架能够有效提高生物医学QA的事实一致性并减少幻觉，在多个基准测试中表现出优越的性能。

Abstract: Biomedical question answering (QA) requires accurate interpretation of
complex medical knowledge. Large language models (LLMs) have shown promising
capabilities in this domain, with retrieval-augmented generation (RAG) systems
enhancing performance by incorporating external medical literature. However,
RAG-based approaches in biomedical QA suffer from hallucinations due to
post-retrieval noise and insufficient verification of retrieved evidence,
undermining response reliability. We propose MedTrust-Guided Iterative RAG, a
framework designed to enhance factual consistency and mitigate hallucinations
in medical QA. Our method introduces three key innovations. First, it enforces
citation-aware reasoning by requiring all generated content to be explicitly
grounded in retrieved medical documents, with structured Negative Knowledge
Assertions used when evidence is insufficient. Second, it employs an iterative
retrieval-verification process, where a verification agent assesses evidence
adequacy and refines queries through Medical Gap Analysis until reliable
information is obtained. Third, it integrates the MedTrust-Align Module (MTAM)
that combines verified positive examples with hallucination-aware negative
samples, leveraging Direct Preference Optimization to reinforce
citation-grounded reasoning while penalizing hallucination-prone response
patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our
approach consistently outperforms competitive baselines across multiple model
architectures, achieving the best average accuracy with gains of 2.7% for
LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.

</details>


### [195] [Instructions are all you need: Self-supervised Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2510.14420)
*Qingyu Ren,Qianyu He,Bowei Zhang,Jie Zeng,Jiaqing Liang,Yanghua Xiao,Weikang Zhou,Zeye Sun,Fei Yu*

Main category: cs.CL

TL;DR: 提出了一种无需标签的自监督强化学习框架，用于解决语言模型在遵循多约束指令时遇到的挑战，通过直接从指令中提取奖励信号和生成伪标签来克服外部监督和稀疏奖励的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习（RL）方法在处理多约束指令时，依赖外部监督和稀疏奖励信号，这在实际应用中是重要的。

Method: 提出了一种无需标签的自监督强化学习框架，该框架通过从指令中直接导出奖励信号并生成伪标签来训练奖励模型，从而消除了对外部监督的依赖。引入了约束分解策略和高效的约束逐个二元分类来解决稀疏奖励的挑战，同时保持计算效率。

Result: 实验表明，该方法具有良好的泛化能力，在3个领域内和5个领域外的数据集上取得了显著的改进，包括具有挑战性的agentic和多轮指令跟随任务。

Conclusion: 所提出的框架能够有效地处理语言模型在遵循多约束指令方面的挑战，并在各种任务和数据集上表现出优越的性能。

Abstract: Language models often struggle to follow multi-constraint instructions that
are crucial for real-world applications. Existing reinforcement learning (RL)
approaches suffer from dependency on external supervision and sparse reward
signals from multi-constraint tasks. We propose a label-free self-supervised RL
framework that eliminates dependency on external supervision by deriving reward
signals directly from instructions and generating pseudo-labels for reward
model training. Our approach introduces constraint decomposition strategies and
efficient constraint-wise binary classification to address sparse reward
challenges while maintaining computational efficiency. Experiments show that
our approach generalizes well, achieving strong improvements across 3 in-domain
and 5 out-of-domain datasets, including challenging agentic and multi-turn
instruction following. The data and code are publicly available at
https://github.com/Rainier-rq/verl-if

</details>


### [196] [Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents](https://arxiv.org/abs/2510.14438)
*Rui Wang,Ce Zhang,Jun-Yu Ma,Jianshu Zhang,Hongru Wang,Yi Chen,Boyang Xue,Tianqing Fang,Zhisong Zhang,Hongming Zhang,Haitao Mi,Dong Yu,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 现有的深度研究网页代理主要关注信息检索，而忽略了信息聚合能力。我们提出了“探索到进化”（Explore to Evolve）范式，通过主动在线探索和自我进化聚合程序来生成可验证的问答对，从而构建用于训练网页代理的可扩展、可验证的训练数据。我们创建了一个包含10K个样本的WebAggregatorQA数据集，并开发了一系列名为WebAggregator的基础模型。其中，WebAggregator-8B模型性能与GPT-4.1相当，而32B模型在GAIA-text基准测试上超越GPT-4.1超过10%，并接近Claude-3.7-sonnet。此外，我们构建了一个具有挑战性的评估集来测试信息聚合能力，在该基准上，Claude-3.7-sonnet仅达到28%，GPT-4.1为25.8%，凸显了加强网页代理信息聚合能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的开源深度研究网页代理主要侧重于信息检索能力，而忽略了对信息进行分析和聚合以支持深度研究的关键需求。

Method: 提出“探索到进化”（Explore to Evolve）范式，首先通过在线探索搜集信息，然后利用搜集到的证据，通过选择、组合和优化12种高级逻辑操作来自我进化一个聚合程序，生成可验证的问答对。基于此范式，构建了包含10K样本的WebAggregatorQA数据集，并使用SmolAgents框架开发了WebAggregator系列基础模型。

Result: WebAggregator-8B模型性能与GPT-4.1相当。WebAggregator-32B模型在GAIA-text基准测试上超越GPT-4.1超过10%，并接近Claude-3.7-sonnet。在信息聚合能力评估基准上，Claude-3.7-sonnet仅获得28%的准确率，GPT-4.1获得25.8%的准确率。

Conclusion: 现有网页代理在信息检索后仍然难以有效聚合信息，这表明需要加强网页代理在信息聚合方面的基础能力。所提出的“探索到进化”范式和WebAggregator系列模型为解决这一问题提供了有效途径。

Abstract: Deep research web agents not only retrieve information from diverse sources
such as web environments, files, and multimodal inputs, but more importantly,
they need to rigorously analyze and aggregate knowledge for insightful
research. However, existing open-source deep research agents predominantly
focus on enhancing information-seeking capabilities of web agents to locate
specific information, while overlooking the essential need for information
aggregation, which would limit their ability to support in-depth research. We
propose an Explore to Evolve paradigm to scalably construct verifiable training
data for web agents. Begins with proactive online exploration, an agent sources
grounded information by exploring the real web. Using the collected evidence,
the agent then self-evolves an aggregation program by selecting, composing, and
refining operations from 12 high-level logical types to synthesize a verifiable
QA pair. This evolution from high-level guidance to concrete operations allowed
us to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K
websites and 11 domains. Based on an open-source agent framework, SmolAgents,
we collect supervised fine-tuning trajectories to develop a series of
foundation models, WebAggregator. WebAggregator-8B matches the performance of
GPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text
and closely approaches Claude-3.7-sonnet. Moreover, given the limited
availability of benchmarks that evaluate web agents' information aggregation
abilities, we construct a human-annotated evaluation split of WebAggregatorQA
as a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves
28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all
references, they still struggle on WebAggregatorQA, highlighting the need to
strengthen the information aggregation capabilities of web agent foundations.

</details>


### [197] [Natural Language Tools: A Natural Language Approach to Tool Calling In Large Language Agents](https://arxiv.org/abs/2510.14453)
*Reid T. Johnson,Michelle D. Pain,Jordan D. West*

Main category: cs.CL

TL;DR: NLT框架用自然语言输出来替代LLMs中程序化的JSON工具调用，消除了任务干扰和格式限制，提高了工具调用准确性18.4个百分点，降低了70%的输出方差，尤其有利于开放模型。


<details>
  <summary>Details</summary>
Motivation: 程序化的JSON工具调用方法在LLMs中存在任务干扰和格式限制，影响了工具调用性能。

Method: 提出了一种名为自然语言工具（NLT）的框架，该框架用自然语言输出来替代程序化的JSON工具调用，并将工具选择与响应生成分离。

Result: NLT在10个模型和6400个客户服务及心理健康领域的试验中，将工具调用准确性提高了18.4个百分点，并将输出方差降低了70%。开放模型性能提升尤为显著，甚至超过了某些闭源模型。

Conclusion: NLT框架有效解决了LLMs中工具调用的问题，提高了准确性和稳定性，并为模型训练提供了新的方向，尤其是在增强开放模型能力方面。

Abstract: We present Natural Language Tools (NLT), a framework that replaces
programmatic JSON tool calling in large language models (LLMs) with natural
language outputs. By decoupling tool selection from response generation, NLT
eliminates task interference and format constraints that degrade tool call
performance. When evaluated across 10 models and 6,400 trials spanning customer
service and mental health domains, NLT improves tool calling accuracy by 18.4
percentage points while reducing output variance by 70%. Open-weight models see
the largest gains, surpassing flagship closed-weight alternatives, with
implications for model training in both reinforcement learning and supervised
fine-tuning stages. These improvements persist under prompt perturbations and
extend tool-calling capabilities to models lacking native support.

</details>


### [198] [LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language Models](https://arxiv.org/abs/2510.14466)
*Haolin Li,Haipeng Zhang,Mang Li,Yaohua Wang,Lijie Wen,Yu Zhang,Biqing Huang*

Main category: cs.CL

TL;DR: LiRA是一个改进大语言模型在低资源语言上表现的训练框架，通过Arca和LaSR模块增强跨语言表示、检索和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在低资源语言上的表现不佳，因为训练数据有限、机器翻译质量不高以及跨语言对齐不稳定。

Method: LiRA框架包含两个模块：(1) Arca通过锚点对齐和多智能体协作编码将低资源语言锚定到英语语义空间，保持共享嵌入空间的几何稳定性；(2) LaSR在Arca的多语言表示之上增加了一个语言感知轻量级推理头，并进行一致性正则化，以提高跨语言理解、检索和推理的鲁棒性。此外，还构建了一个包含五种东南亚和两种南亚语言的多语言产品检索数据集。

Result: 在低资源跨语言检索、语义相似性和推理基准测试中，LiRA在少样本和噪声放大设置下均表现出稳定的提升。消融实验验证了Arca和LaSR的有效性。

Conclusion: LiRA框架能有效提升大语言模型在低资源语言上的跨语言表示、检索和推理能力，并在实验中得到验证。

Abstract: As large language models (LLMs) rapidly advance, performance on high-resource
languages (e.g., English, Chinese) is nearing saturation, yet remains
substantially lower for low-resource languages (e.g., Urdu, Thai) due to
limited training data, machine-translation noise, and unstable cross-lingual
alignment. We introduce LiRA (Linguistic Robust Anchoring for Large Language
Models), a training framework that robustly improves cross-lingual
representations under low-resource conditions while jointly strengthening
retrieval and reasoning. LiRA comprises two modules: (i) Arca (Anchored
Representation Composition Architecture), which anchors low-resource languages
to an English semantic space via anchor-based alignment and multi-agent
collaborative encoding, preserving geometric stability in a shared embedding
space; and (ii) LaSR (Language-coupled Semantic Reasoner), which adds a
language-aware lightweight reasoning head with consistency regularization on
top of Arca's multilingual representations, unifying the training objective to
enhance cross-lingual understanding, retrieval, and reasoning robustness. We
further construct and release a multilingual product retrieval dataset covering
five Southeast Asian and two South Asian languages. Experiments across
low-resource benchmarks (cross-lingual retrieval, semantic similarity, and
reasoning) show consistent gains and robustness under few-shot and
noise-amplified settings; ablations validate the contribution of both Arca and
LaSR. Code will be released on GitHub and the dataset on Hugging Face.

</details>


### [199] [Efficient Seq2seq Coreference Resolution Using Entity Representations](https://arxiv.org/abs/2510.14504)
*Matt Grenander,Shay B. Cohen,Mark Steedman*

Main category: cs.CL

TL;DR: Seq2seq核心关系模型在对话等增量场景下效率低下，本文提出一种压缩表示方法，通过提取和重组实体级标记来提高效率，并在OntoNotes和LitBank数据集上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: Seq2seq核心关系模型在提高性能的同时，牺牲了灵活性和效率，特别是在对话等需要顺序处理文本的增量场景下。因此，需要一种能够提高模型在增量场景下效率的方法。

Method: 提出一种压缩表示方法，通过提取和重组实体级标记，并丢弃大部分其他输入标记，来提高Seq2seq核心关系模型的效率。

Result: 在OntoNotes数据集上，该方法比全前缀增量基线仅低0.6 CoNLL F1点，压缩比达到1.8。在LitBank数据集上，该方法超过了现有技术水平。

Conclusion: 在Seq2seq解析器中丢弃大部分标记是增量核心关系分辨率的一种可行策略。

Abstract: Seq2seq coreference models have introduced a new paradigm for coreference
resolution by learning to generate text corresponding to coreference labels,
without requiring task-specific parameters. While these models achieve new
state-of-the-art performance, they do so at the cost of flexibility and
efficiency. In particular, they do not efficiently handle incremental settings
such as dialogue, where text must processed sequentially. We propose a
compressed representation in order to improve the efficiency of these methods
in incremental settings. Our method works by extracting and re-organizing
entity-level tokens, and discarding the majority of other input tokens. On
OntoNotes, our best model achieves just 0.6 CoNLL F1 points below a
full-prefix, incremental baseline while achieving a compression ratio of 1.8.
On LitBank, where singleton mentions are annotated, it passes state-of-the-art
performance. Our results indicate that discarding a wide portion of tokens in
seq2seq resolvers is a feasible strategy for incremental coreference
resolution.

</details>


### [200] [Assessing Socio-Cultural Alignment and Technical Safety of Sovereign LLMs](https://arxiv.org/abs/2510.14565)
*Kyubyung Chae,Gihoon Kim,Gyuseong Lee,Taesup Kim,Jaejin Lee,Heejin Kim*

Main category: cs.CL

TL;DR: 主权大型语言模型（LLM）在低资源语言支持方面发挥着重要作用，但它们并不总是符合‘更好地服务目标用户’的说法，并且过度追求这一未经测试的说法可能会导致低估安全性等关键质量属性。因此，需要更广泛、更可靠、更实用的标准来评估主权LLM。


<details>
  <summary>Details</summary>
Motivation: 许多国家对开发符合其独特社会文化和历史背景的主权大型语言模型（LLM）表现出日益增长的兴趣。然而，目前缺乏用于验证这些模型在多大程度上符合用户社会文化背景以及它们在多大程度上保持安全性和技术稳健性而不对用户造成潜在危害和风险的框架和数据集。

Method: 构建了一个新的数据集，并引入了一个分析框架，用于提取和评估主权LLM的社会文化元素，并评估其技术稳健性。

Result: 实验结果表明，主权LLM在支持低资源语言方面发挥着重要作用，但它们并不总是能满足‘更好地服务目标用户’的说法。此外，追求这一未经证实的说法可能会导致低估安全性等关键质量属性。

Conclusion: 主权LLM的进展需要更广泛的、包含更多基于事实和实用标准的评估。

Abstract: Recent trends in LLMs development clearly show growing interest in the use
and application of sovereign LLMs. The global debate over sovereign LLMs
highlights the need for governments to develop their LLMs, tailored to their
unique socio-cultural and historical contexts. However, there remains a
shortage of frameworks and datasets to verify two critical questions: (1) how
well these models align with users' socio-cultural backgrounds, and (2) whether
they maintain safety and technical robustness without exposing users to
potential harms and risks. To address this gap, we construct a new dataset and
introduce an analytic framework for extracting and evaluating the
socio-cultural elements of sovereign LLMs, alongside assessments of their
technical robustness. Our experimental results demonstrate that while sovereign
LLMs play a meaningful role in supporting low-resource languages, they do not
always meet the popular claim that these models serve their target users well.
We also show that pursuing this untested claim may lead to underestimating
critical quality attributes such as safety. Our study suggests that advancing
sovereign LLMs requires a more extensive evaluation that incorporates a broader
range of well-grounded and practical criteria.

</details>


### [201] [Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures](https://arxiv.org/abs/2510.14616)
*Shuangshuang Ying,Yunwen Li,Xingwei Qu,Xin Li,Sheng Jin,Minghao Liu,Zhoufutu Wen,Xeron Du,Tianyu Zheng,Yichi Zhang,Letian Ni,Yuyang Cheng,Qiguang Chen,Jingzhe Ding,Shengda Long,Wangchunshu Zhou,Jiazhan Feng,Wanjun Zhong,Libo Qin,Ge Zhang,Wenhao Huang,Wanxiang Che,Chenghua Lin*

Main category: cs.CL

TL;DR: 现有的偏好学习方法在移除客观质量信号后性能下降，WritingPreferenceBench基准测试显示，基于序列的奖励模型和零样本语言模型判断器的准确率仅为52.7%和53.9%，而具有明确推理链的生成式奖励模型的准确率高达81.8%。模型在不同体裁下的准确率差异很大，且模型规模对这种差异影响不大。这表明目前的RLHF方法主要学习检测客观错误而非主观质量偏好，成功的偏好建模可能需要中间推理表示。


<details>
  <summary>Details</summary>
Motivation: 现有偏好学习方法在缺乏客观质量信号时性能会显著下降，这表明它们可能主要学习检测客观错误而非主观质量偏好。

Method: 在WritingPreferenceBench基准上评估了基于序列的奖励模型、零样本语言模型判断器和生成式奖励模型。该基准包含1800个人类标注的偏好对（1200个英文，600个中文），涵盖8种创意写作体裁，并匹配了客观正确性、事实准确性和长度。

Result: 在WritingPreferenceBench基准上，基于序列的奖励模型准确率为52.7%，零样本语言模型判断器为53.9%，而生成式奖励模型准确率为81.8%。模型在不同体裁下的准确率差异很大（平均标准差为10.1%），并且模型规模（8B vs 27B参数）并未显著改善这种情况。

Conclusion: 目前的RLHF方法主要学习检测客观错误，而非捕捉主观质量偏好（如创造力、风格和情感共鸣）。成功的偏好建模可能需要中间推理表示，而不是直接分类。

Abstract: Current preference learning methods achieve high accuracy on standard
benchmarks but exhibit significant performance degradation when objective
quality signals are removed. We introduce WritingPreferenceBench, a dataset of
1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8
creative writing genres, where responses are matched for objective correctness,
factual accuracy, and length. On this benchmark, sequence-based reward
models--the standard architecture for RLHF--achieve only 52.7% mean accuracy,
while zero-shot language model judges perform at 53.9%. In contrast, generative
reward models that produce explicit reasoning chains achieve 81.8% accuracy. We
observe high within-model variance across genres: individual models range from
18.2% to 81.8% accuracy across different writing categories, with standard
deviations averaging 10.1%. This variance persists regardless of model scale,
with 27B parameter models showing no consistent improvement over 8B variants.
Our results suggest that current RLHF methods primarily learn to detect
objective errors rather than capture subjective quality preferences (e.g.,
creativity, stylistic flair, and emotional resonance), and that successful
preference modeling may require intermediate reasoning representations rather
than direct classification.

</details>


### [202] [Code-driven Number Sequence Calculation: Enhancing the inductive Reasoning Abilities of Large Language Models](https://arxiv.org/abs/2510.14620)
*Kedi Chen,Zhikai Lei,Xu Guo,Xuecheng Wu,Siyuan Zeng,Jianghao Yin,Yinqi Zhang,Qin Chen,Jie Zhou,Liang He,Qipeng Guo,Kai Chen,Wei Zhang*

Main category: cs.CL

TL;DR: CodeSeq是一个合成数据集，旨在解决LLM在归纳推理任务中的挑战，通过生成包含复杂内部模式的算法问题，并结合监督微调和强化学习来提高模型的推理能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有的归纳推理数据侧重于表面规律，缺乏复杂内部模式，且现有方法仅依赖简单提示或少量示例，未能提供精确的思维过程或难度控制。研究旨在解决这些挑战。

Method: 引入CodeSeq合成数据集，将数字序列包装成算法问题以发现其通项。通过反思失败案例并进行迭代修正来生成监督微调数据，使LLM能够自主生成案例并进行自我检查。利用基于问题可解性和自我指导案例生成成功率的新型案例协同可解性缩放奖励的强化学习，使模型能更有效地从成功和失败中学习。

Result: 在各种推理任务上，使用CodeSeq训练的模型均有所改进，并能保持模型在分布外（OOD）的性能。

Conclusion: CodeSeq数据集和提出的方法能够有效提升LLM在归纳推理任务上的表现，并增强模型的泛化能力。

Abstract: Large language models (LLMs) make remarkable progress in reasoning tasks.
Among different reasoning modes, inductive reasoning, due to its better
alignment with human learning, attracts increasing interest. However, research
on inductive reasoning faces certain challenges. First, existing inductive data
mostly focuses on superficial regularities while lacking more complex internal
patterns. Second, current works merely prompt LLMs or finetune on simple
prompt-response pairs, but do not provide precise thinking processes nor
implement difficulty control. Unlike previous work, we address these challenges
by introducing \textit{CodeSeq}, a synthetic post-training dataset built from
number sequences. We package number sequences into algorithmic problems to
discover their general terms, defining a general term generation (GTG) task
correspondingly. Our pipeline generates supervised finetuning data by
reflecting on failed test cases and incorporating iterative corrections,
thereby teaching LLMs to learn autonomous case generation and self-checking.
Additionally, it leverages reinforcement learning with a novel Case-Synergy
Solvability Scaling Reward based on both solvability, estimated from the
problem pass rate, and the success rate of self-directed case generation,
enabling models to learn more effectively from both successes and failures.
Experimental results show that the models trained with \textit{CodeSeq} improve
on various reasoning tasks and can preserve the models' OOD performance.

</details>


### [203] [RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF](https://arxiv.org/abs/2510.14628)
*Qing Yang,Zhenghao Liu,Junxin Wang,Yangfan Du,Pengcheng Huang,Tong Xiao*

Main category: cs.CL

TL;DR: RLAIF-SPA框架通过结合强化学习、ASR和LLM，解决了文本到语音合成中的情感表达和可懂度难题，在Libri Speech数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有文本到语音合成技术在生成中性语音方面已接近人类水平，但在情感表达方面仍存在挑战，且现有方法常依赖昂贵的表情标注或间接优化目标，导致生成语音情感平淡。为解决此问题，本文提出RLAIF-SPA框架。

Method: RLAIF-SPA框架引入了AI反馈强化学习（RLAIF）机制，利用自动语音识别（ASR）和大型语言模型（LLM）技术分别评估语义准确性和韵律-情感标签的一致性，将其作为情感表达和可懂度优化的直接奖励。该框架结合了韵律标签对齐（Prosodic Label Alignment），通过联合考虑语义准确性和韵律-情感一致性（在结构、情感、语速和语调四个维度上），以及语义准确性反馈（Semantic Accuracy Feedback），来提升情感表达质量并确保生成语音的清晰度和准确性。

Result: 在Libri Speech数据集上的实验表明，RLAIF-SPA的词错误率（WER）降低了26.1%，SIM-O得分提高了9.1%，并且在人类评估中获得了超过10%的提升，优于Chat-TTS。

Conclusion: RLAIF-SPA框架有效解决了文本到语音合成中的情感表达和可懂度挑战，通过RLAIF机制和多维度对齐策略，实现了在保持语义准确性的同时，显著提升语音的情感表现力和感知自然度。

Abstract: Text-To-Speech synthesis has achieved near-human quality in neutral speech,
but emotional expressiveness remains a challenge. Existing methods often rely
on costly emotion annotations or optimize indirect objectives that fail to
capture the emotional expressiveness and perceptual naturalness of speech,
leading to generated speech that is accurate but emotionally flat. To address
these challenges, we propose the RLAIF-SPA framework, incorporating a
Reinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic
Speech Recognition (ASR) and Large Language Model (LLM) techniques to
respectively judge semantic accuracy and prosodic-emotional label alignment as
a direct reward for emotional expressiveness and intelligibility optimization.
Specifically, it leverages Prosodic Label Alignment to enhance expressive
quality by jointly considering semantic accuracy and prosodic-emotional
alignment along four fine-grained dimensions: Structure, Emotion, Speed, and
Tone. In addition, it incorporates Semantic Accuracy Feedback to ensure the
generation of clear and accurate speech. Experiments on the Libri Speech
dataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in
WER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation.

</details>


### [204] [Intent Clustering with Shared Pseudo-Labels](https://arxiv.org/abs/2510.14640)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: 提出一种无需训练、无需标签、基于轻量级开源大语言模型的意图聚类方法，解决现有方法依赖昂贵商业模型、需预设聚类数的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵商业大模型，透明度有限，且常需预知聚类数量，这在实际中不常见。

Method: 首先让大模型为每个文本生成伪标签，然后在该伪标签集中对每个文本执行多标签分类，假设同一簇的文本共享更多标签，编码后距离更近。

Result: 在四个基准数据集上的评估结果显示，该方法在保持简单和计算效率的同时，取得了与近期基线相当甚至更好的结果。

Conclusion: 该方法适用于低资源场景，并且在多种模型和数据集上表现稳定。

Abstract: In this paper, we propose an intuitive, training-free and label-free method
for intent clustering that makes minimal assumptions using lightweight and
open-source LLMs. Many current approaches rely on commercial LLMs, which are
costly, and offer limited transparency. Additionally, their methods often
explicitly depend on knowing the number of clusters in advance, which is often
not the case in realistic settings. To address these challenges, instead of
asking the LLM to match similar text directly, we first ask it to generate
pseudo-labels for each text, and then perform multi-label classification in
this pseudo-label set for each text. This approach is based on the hypothesis
that texts belonging to the same cluster will share more labels, and will
therefore be closer when encoded into embeddings. These pseudo-labels are more
human-readable than direct similarity matches. Our evaluation on four benchmark
sets shows that our approach achieves results comparable to and better than
recent baselines, while remaining simple and computationally efficient. Our
findings indicate that our method can be applied in low-resource scenarios and
is stable across multiple models and datasets.

</details>


### [205] [An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs](https://arxiv.org/abs/2510.14660)
*Linyue Ma,Yilong Xu,Xiang Long,Zhi Zheng*

Main category: cs.CL

TL;DR: 提出了一种名为“nugget-as-rubric”的统一、可验证的范式，将原子信息点视为结构化评估标准，以解决现有搜索增强大语言模型奖励建模的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索增强大语言模型奖励建模方法存在不足：基于规则的奖励（如Exact Match）易碎且不适用于长文本；生成奖励虽然更鲁棒，但在动态语料库的长文本任务中设计可验证、稳定的奖励具有挑战性且计算成本高。

Method: 提出“nugget-as-rubric”范式，将原子信息点作为结构化评估标准。对于短文本任务，使用单个标准；对于长文本任务，使用与问题信息需求对齐的多个标准。设计了基于查询重写的自动标准构建流程，能从静态语料库和动态在线内容中检索相关段落并提取标准。引入了参数量为4B的生成式验证器Search-Gen-V，采用蒸馏和两阶段策略进行训练。

Result: Search-Gen-V在不同工作负载下实现了强大的验证准确性，证明了其作为搜索增强大语言模型的可扩展、鲁棒且高效的可验证奖励构建器。

Conclusion: “nugget-as-rubric”范式和Search-Gen-V为搜索增强大语言模型提供了一种新的、可验证的奖励建模方法，克服了现有方法的局限性，并在各种任务中表现出优越的性能。

Abstract: Search augmentation empowers Large Language Models with retrieval
capabilities to overcome the limitations imposed by static parameters.
Recently, Reinforcement Learning leverages tailored reward signals as a viable
technique to enhance LLMs performing tasks involving search. However, existing
reward modeling for search-augmented LLMs faces several limitations. Rule-based
rewards, such as Exact Match, are verifiable but fragile to variations in
expression and cannot be applied to long-form workloads. In contrast,
generative rewards improve robustness, but designing verifiable and stable
rewards for long-form workloads in dynamic corpora remains challenging and also
incurs high computational costs. In this paper, we propose a unified and
verifiable paradigm, "nugget-as-rubric", which treats atomic information points
as structured evaluation criteria for different search-augmentation workloads.
Short-form tasks correspond to a single rubric, whereas long-form tasks expand
to multiple rubrics aligned with the question's information needs. To support
long-form settings, we design an automatic rubric construction pipeline based
on query rewriting, which can automatically retrieve passages relevant to each
question and extract rubrics from them, both from static corpora and from
dynamic online web content. Furthermore, we introduce \textbf{Search-Gen-V}, a
4B-parameter efficient generative verifier under our proposed verifiable
paradigm, which is trained via the idea of distillation and a two-stage
strategy. Experimental results show that Search-Gen-V achieves strong
verification accuracy across different workloads, making it a scalable, robust,
and efficient verifiable reward constructor for search-augmented LLMs.

</details>


### [206] [Semantic Prosody in Machine Translation: the English-Chinese Case of Passive Structures](https://arxiv.org/abs/2510.14662)
*Xinyue Ma,Pol Pastells,Mireia Farrús,Mariona Taulé*

Main category: cs.CL

TL;DR: 语义韵是语言单位与一系列固定词语搭配形成的搭配意义，应与语义意义区分开。由于字面意思相同的词语可能有不同的语义韵，因此在翻译中应给予更多关注。现有机器翻译模型无法处理此问题。为解决此问题，我们提出一种向机器翻译模型传授特定结构语义韵的方法，重点关注中文“被”字句，并创建了一个英汉句对数据集，以展示“被”字句的负面语义韵。我们使用此数据集对 OPUS-MT、NLLB-600M 和 mBART50 模型进行微调，以执行英汉翻译任务。结果表明，微调后的机器翻译模型在使用“被”字句翻译负面内容时表现更好，而在翻译中性或正面内容时则会避免使用。此外，在多语言模型 NLLB-600M 中，这种语义韵知识可以从英汉翻译迁移到其他语言对，例如西语-汉语翻译。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译模型无法处理语义韵问题，导致翻译不准确。

Method: 创建英汉“被”字句数据集，并用该数据集对 OPUS-MT、NLLB-600M 和 mBART50 模型进行微调，以执行英汉翻译任务。

Result: 微调后的机器翻译模型在使用“被”字句翻译负面内容时表现更好，而在翻译中性或正面内容时则会避免使用。在 NLLB-600M 模型中，语义韵知识可以从英汉翻译迁移到其他语言对。

Conclusion: 所提出的方法能够有效地改善机器翻译模型在处理语义韵方面的能力，并且该知识可以跨语言迁移。

Abstract: Semantic prosody is a collocational meaning formed through the co-occurrence
of a linguistic unit and a consistent series of collocates, which should be
treated separately from semantic meaning. Since words that are literal
translations of each other may have different semantic prosody, more attention
should be paid to this linguistic property to generate accurate translations.
However, current machine translation models cannot handle this problem. To
bridge the gap, we propose an approach to teach machine translation models
about semantic prosody of a specific structure. We focus on Chinese BEI
passives and create a dataset of English-Chinese sentence pairs with the
purpose of demonstrating the negative semantic prosody of BEI passives. Then we
fine-tune OPUS-MT, NLLB-600M and mBART50 models with our dataset for the
English-Chinese translation task. Our results show that fine-tuned MT models
perform better on using BEI passives for translating unfavourable content and
avoid using it for neutral and favourable content. Also, in NLLB-600M, which is
a multilingual model, this knowledge of semantic prosody can be transferred
from English-Chinese translation to other language pairs, such as
Spanish-Chinese.

</details>


### [207] [Speculative Model Risk in Healthcare AI: Using Storytelling to Surface Unintended Harms](https://arxiv.org/abs/2510.14718)
*Xingmeng Zhao,Dan Schumacher,Veronica Rammouz,Anthony Rios*

Main category: cs.CL

TL;DR: AI在医疗领域的快速发展带来了偏见、隐私侵犯和不平等访问的风险。本文提出了一个以用户为中心的框架，通过生成用户故事和支持多方讨论，帮助人们在部署前思考AI的潜在益处和风险。用户研究表明，阅读用户故事的参与者能识别更广泛的危害，而未阅读故事的参与者则主要关注隐私和福祉。


<details>
  <summary>Details</summary>
Motivation: AI在医疗领域的快速发展可能带来偏见、隐私侵犯和不平等访问等风险，并且现有方法可能减少人类在理解和应对这些风险中的参与度。

Method: 提出一个以用户为中心的框架，生成用户故事并支持多方讨论，以帮助人们在部署前思考AI的潜在益处和风险。

Result: 使用该框架的用户能够识别更广泛的危害，并将关注点更均匀地分布在所有13种危害类型上。相比之下，未使用该框架的用户主要关注隐私和福祉（58.3%）。

Conclusion: 用户故事有助于参与者推测更广泛的危害和益处，并能更具创造性地思考AI对用户的影响。

Abstract: Artificial intelligence (AI) is rapidly transforming healthcare, enabling
fast development of tools like stress monitors, wellness trackers, and mental
health chatbots. However, rapid and low-barrier development can introduce risks
of bias, privacy violations, and unequal access, especially when systems ignore
real-world contexts and diverse user needs. Many recent methods use AI to
detect risks automatically, but this can reduce human engagement in
understanding how harms arise and who they affect. We present a human-centered
framework that generates user stories and supports multi-agent discussions to
help people think creatively about potential benefits and harms before
deployment. In a user study, participants who read stories recognized a broader
range of harms, distributing their responses more evenly across all 13 harm
types. In contrast, those who did not read stories focused primarily on privacy
and well-being (58.3%). Our findings show that storytelling helped participants
speculate about a broader range of harms and benefits and think more creatively
about AI's impact on users.

</details>


### [208] [AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning](https://arxiv.org/abs/2510.14738)
*Mengzhao Jia,Zhihan Zhang,Ignacio Cases,Zheyuan Liu,Meng Jiang,Peng Qi*

Main category: cs.CL

TL;DR: 为解决多模态大语言模型（MLLM）在强化学习中因仅奖励最终答案正确性而导致的推理错误问题，我们提出了AutoRubric-R1V框架。该框架通过自动收集的、基于评分标准的生成奖励，将强化学习与过程级监督相结合。其创新之处在于一种可扩展的自聚合方法，能从成功的轨迹中提炼出一致的推理检查点，从而无需人工标注或更强的教师模型即可构建特定于问题的评分标准。通过联合利用基于评分标准和基于结果的奖励，AutoRubric-R1V在六个多模态推理基准测试中取得了最先进的性能，并显著提高了推理忠实度。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLM）在复杂的多步推理任务中表现出局限性，强化学习与可验证奖励（RLVR）由于仅奖励最终答案的正确性，容易导致模型产生错误的推理过程。

Method: 提出AutoRubric-R1V框架，该框架整合了强化学习与过程级监督，利用自动收集的、基于评分标准的生成奖励。通过一种可扩展的自聚合方法，从成功的轨迹中提炼一致的推理检查点，无需人工标注或更强的教师模型即可构建问题特定的评分标准。

Result: 在六个多模态推理基准测试中取得了最先进的性能，并显著提高了推理忠实度。

Conclusion: AutoRubric-R1V框架通过结合基于评分标准和基于结果的奖励，有效解决了多模态大语言模型在强化学习中存在的推理错误问题，并在多个基准测试中取得了优异表现。

Abstract: Multimodal large language models (MLLMs) have rapidly advanced from
perception tasks to complex multi-step reasoning, yet reinforcement learning
with verifiable rewards (RLVR) often leads to spurious reasoning since only the
final-answer correctness is rewarded. To address this limitation, we propose
AutoRubric-R1V, a framework that integrates RLVR with process-level supervision
through automatically collected rubric-based generative rewards. Our key
innovation lies in a scalable self-aggregation method that distills consistent
reasoning checkpoints from successful trajectories, enabling problem-specific
rubric construction without human annotation or stronger teacher models. By
jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves
state-of-the-art performance on six multimodal reasoning benchmarks and
substantially improves reasoning faithfulness in dedicated evaluations.

</details>


### [209] [Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware Code](https://arxiv.org/abs/2510.14756)
*Manar Abdelatty,Maryam Nouh,Jacob K. Rosenstein,Sherief Reda*

Main category: cs.CL

TL;DR: LLMs在自动化硬件设计（如Verilog代码生成）方面取得了显著进展，但其生成的代码在面积、延迟和功耗等综合指标方面仍需优化。Pluto是一个评估LLM生成Verilog设计效率的基准和框架，包含114个问题、自检测试平台和参考实现。实验表明，虽然LLMs在功能正确性方面表现良好（pass@1达到78.3%），但在效率方面（面积、延迟、功耗的eff@1分别为63.8%、65.9%、64.0%）仍落后于专家设计，强调了效率感知评估框架的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能全面评估LLM生成Verilog代码在面积、延迟和功耗等综合效率指标上的表现，缺乏优化的基线和验证测试平台。

Method: 提出了Pluto，一个包含114个问题、自检测试平台和多组帕累托最优参考实现的基准和评估框架，用于评估LLM生成Verilog设计的效率。

Result: 实验结果显示，最先进的LLMs在功能正确性方面达到了78.3%（pass@1），但在面积、延迟和功耗效率方面（eff@1）分别为63.8%、65.9%和64.0%，落后于专家设计的实现。

Conclusion: 需要Pluto这样的效率感知评估框架来推动硬件领域LLM研究的进步。

Abstract: Large Language Models (LLMs) are increasingly used to automate hardware
design tasks, including the generation of Verilog code. While early benchmarks
focus primarily on functional correctness, efficient hardware design demands
additional optimization for synthesis metrics such as area, delay, and power.
Existing benchmarks fall short in evaluating these aspects comprehensively:
they often lack optimized baselines or testbenches for verification. To address
these gaps, we present Pluto, a benchmark and evaluation framework designed to
assess the efficiency of LLM-generated Verilog designs. Pluto presents a
comprehensive evaluation set of 114 problems with self-checking testbenches and
multiple Pareto-optimal reference implementations. Experimental results show
that state-of-the-art LLMs can achieve high functional correctness, reaching
78.3\% at pass@1, but their synthesis efficiency still lags behind
expert-crafted implementations, with area efficiency of 63.8\%, delay
efficiency of 65.9\%, and power efficiency of 64.0\% at eff@1. This highlights
the need for efficiency-aware evaluation frameworks such as Pluto to drive
progress in hardware-focused LLM research.

</details>


### [210] [COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes](https://arxiv.org/abs/2510.14763)
*Yunwen Li,Shuangshuang Ying,Xingwei Qu,Xin Li,Sheng Jin,Minghao Liu,Zhoufutu Wen,Tianyu Zheng,Xeron Du,Qiguang Chen,Jiajun Shi,Wangchunshu Zhou,Jiazhan Feng,Wanjun Zhong,Libo Qin,Stephen Huang,Wanxiang Che,Chenghua Lin,Eli Zhang*

Main category: cs.CL

TL;DR: COIG-Writer是一个包含1665个三元组（逆向工程提示、创作推理、最终文本）的新型中文创作数据集，用于解决大型语言模型在创意写作方面的不足，尤其是在非英语语境下。实验表明，创作推理（过程监督）和语言表达（通用数据）共同构成了创作能力。过程监督非常有效，但需要通用数据进行稳定，最佳比例约为1:12。创作能力具有文化特异性，跨语言迁移效果不佳。词汇多样性与创作质量呈负相关（TTR悖论）。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在创意写作方面存在系统性缺陷，尤其是在非英语语境下，由于训练数据稀缺且缺乏过程监督。本研究旨在通过构建一个新的中文创意写作数据集来解决这个问题。

Method: 构建了一个名为COIG-Writer的新型中文创意写作数据集，包含1665个三元组，每个三元组包括逆向工程的提示、详细的创作推理过程和最终文本。通过实验研究了过程监督、跨语言迁移和词汇多样性对创意写作能力的影响。

Result: 实验发现，过程监督对于提升创意写作能力至关重要，但需要与通用数据结合使用，最佳比例约为1:12。创作能力具有文化特异性，在中文和英文之间存在显著的性能差距。词汇多样性与创意质量之间存在负相关关系（TTR悖论），表明高多样性可能掩盖逻辑缺陷。

Conclusion: 创意写作能力的提升源于逻辑构建（过程监督）和语言表达（通用数据）的相互作用。过程监督是关键，但需要通用数据来稳定。创意能力具有文化特异性，并且高词汇多样性可能并非总是好事。

Abstract: Large language models exhibit systematic deficiencies in creative writing,
particularly in non-English contexts where training data is scarce and lacks
process-level supervision. We present COIG-Writer, a novel Chinese creative
writing dataset that captures both diverse outputs and their underlying thought
processes through systematic reverse-engineering of high-quality texts. Unlike
existing datasets that provide only input-output pairs, COIG-Writer comprises
1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a
reverse-engineered prompt, (2) detailed creative reasoning documenting
decision-making processes, and (3) the final text. Through comprehensive
experiments, we identify a two-component model of creative writing: narrative
logic (provided by process supervision) and linguistic expression (maintained
by general-purpose data). Our findings reveal three critical insights: (1)
Process supervision is highly effective but requires stabilization with general
data. A ratio of at least one creative sample to twelve general samples is
needed to achieve optimal performance; below this threshold, the win rate
progressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities
are culturally-bound with no cross-lingual transfer (89.26pp gap between
Chinese and English performance), and (3) lexical diversity inversely
correlates with creative quality (TTR paradox), suggesting high diversity
signals compensatory behavior for logical deficiencies. These findings
establish that creative excellence emerges from the interaction between logical
scaffolding and linguistic grounding, analogous to how mathematical reasoning
enhances but cannot replace linguistic competence in foundation models.

</details>


### [211] [Finding Answers in Thought Matters: Revisiting Evaluation on Large Language Models with Reasoning](https://arxiv.org/abs/2510.14773)
*Hwiyeol Jo,Joosung Lee,Jaehone Lee,Sang-Woo Lee,Joonsuk Park,Kang Min Yoo*

Main category: cs.CL

TL;DR: Answer Regeneration框架通过增加额外的模型推理来提高LLM在推理任务中的鲁棒性，使其评估对提取方法不敏感。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法（基于概率选择答案）不适用于需要推理的模型，因为答案提取方法会严重影响模型性能和最终答案分布。

Method: 提出Answer Regeneration框架，该框架通过在原始输入输出后添加‘Answer:’提示，增加一次额外的模型推理，并从再生输出中选择或提取最终答案。

Result: Answer Regeneration方法被证明可以提高模型性能和鲁棒性，并且不依赖于特定的答案提取规则。该框架已成功应用于数学问题和开放式问答任务。

Conclusion: Answer Regeneration框架为模型评估提供了一种更可靠的方法，尤其是在处理需要推理的任务时。

Abstract: Evaluating generative models, such as large language models (LLMs), commonly
involves question-answering tasks where the final answer is selected based on
probability of answer choices. On the other hand, for models requiring
reasoning, the method of answer extraction plays a critical role. Our research
reveals that the performance of reasoning models and their final answer
distributions are highly sensitive to the answer extraction algorithm employed.
In order to mitigate this, we propose a basic framework: Answer Regeneration.
The method uses an additional model inference, providing the prior input and
output prefaced by the prompt "Answer:". The final answer is then selected or
extracted from the regenerated output. We show that this
extraction-rule-agnostic approach exhibits improved performance and enhanced
robustness. Furthermore, we have applied this framework to general math
problems and open-ended question answering tasks. Our analysis and this
framework could offer a more reliable results for model evaluation.

</details>


### [212] [Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking](https://arxiv.org/abs/2510.14824)
*Ziqi Dai,Xin Zhang,Mingxin Li,Yanzhao Zhang,Dingkun Long,Pengjun Xie,Meishan Zhang,Wenjie Li,Min Zhang*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在信息检索中的重排任务上，监督微调（SFT）相比对比学习（CL）更具优势，SFT具有更强的权重更新机制，并在MRB基准测试中达到了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在信息检索的重排任务中，对比学习（CL）和监督微调（SFT）哪种目标函数更优，以及其背后的机制是什么，是一个待解决的问题。

Method: 通过将目标函数分解为权重和方向两个组成部分，并构建统一框架来分析CL和SFT的交互作用，并在通用多模态检索（UMR）上进行实验。

Result: SFT提供了比CL强得多的权重机制，而方向性上没有明显差异。SFT在LLM重排任务上优于CL。

Conclusion: SFT在LLM重排任务上比CL更具优势，实验结果支持了这一发现，并在MRB基准测试中实现了新的最先进性能。

Abstract: In information retrieval, training reranking models mainly focuses on two
types of objectives: metric learning (e.g. contrastive loss to increase the
predicted scores on relevant query-document pairs) and classification (binary
label prediction of relevance vs. irrelevance). For BERT-style encoders,
various studies have shown that contrastive learning (CL) can be more effective
than discriminative (classification) learning. However, for large language
models (LLMs), classification via supervised fine-tuning (SFT), which predicts
''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears
more promising as it aligns well with the generative nature of LLMs. This
divergence raises a central question: which objective is intrinsically better
suited to LLM-based reranking, and what mechanism underlies the difference? In
this work, we conduct a comprehensive comparison and analysis between CL and
SFT for reranking, taking the universal multimodal retrieval (UMR) as the
experimental playground. We first decompose the objectives into two components:
weight, which controls the magnitude of those updates, and direction, which
guides the model updates, then present a unified framework for understanding
their interactions. Through probing experiments, we find that SFT provides a
substantially stronger weighting scheme than CL, whereas the preferred scoring
direction shows no clear winner. Taken together, these results point to a
consistent advantage of SFT over CL for LLM reranking. To further validate our
findings, we conduct large-scale training with SFT and present new
state-of-the-art rerankers on the MRB benchmark. We also provide ablations on
SFT settings and expect our findings to benefit future research and
applications in this area.

</details>


### [213] [Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models](https://arxiv.org/abs/2510.14853)
*Guinan Su,Yanwu Yang,Li Shen,Lu Yin,Shiwei Liu,Jonas Geiping*

Main category: cs.CL

TL;DR: 本文提出了一种数据驱动、在线的测试时间框架，用于在不依赖外部数据或监督的情况下，优化混合专家（MoE）模型的路由决策，以解决部署中的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时间适应方法主要针对密集模型，并且需要外部数据，这限制了它们在混合专家（MoE）模型上的应用。本文旨在解决 MoE 模型在部署中因分布偏移导致的次优路由决策问题。

Method: 提出了一种数据驱动、在线的测试时间框架，通过在预填充阶段和定期间隔中，利用已生成序列的自监督信号来优化路由决策。该方法使用轻量级的加性向量来更新选定层中的路由器 logit，以保持计算效率并防止过度适应。

Result: 该方法在推理任务上实现了性能的稳定提升，并在 HumanEval 和 DeepSeek-V2-Lite 等基准测试中取得了显著的改进（例如，在 HumanEval 上提高了 5.5%，与自洽性结合使用时平均提高了 6%）。

Conclusion: 所提出的框架能够有效优化 MoE 模型的路由决策，提高模型在推理任务上的性能，并且具有即插即用的特性，可以与现有的测试时间扩展技术结合使用。

Abstract: Mixture-of-Experts (MoE) models achieve efficient scaling through sparse
expert activation, but often suffer from suboptimal routing decisions due to
distribution shifts in deployment. While existing test-time adaptation methods
could potentially address these issues, they primarily focus on dense models
and require access to external data, limiting their practical applicability to
MoE architectures. However, we find that, instead of relying on reference data,
we can optimize MoE expert selection on-the-fly based only on input context. As
such, we propose \textit{a data-free, online test-time framework} that
continuously adapts MoE routing decisions during text generation without
external supervision or data. Our method cycles between two phases: During the
prefill stage, and later in regular intervals, we optimize the routing
decisions of the model using self-supervision based on the already generated
sequence. Then, we generate text as normal, maintaining the modified router
until the next adaption. We implement this through lightweight additive vectors
that only update router logits in selected layers, maintaining computational
efficiency while preventing over-adaptation. The experimental results show
consistent performance gains on challenging reasoning tasks while maintaining
robustness to context shifts. For example, our method achieves a 5.5\%
improvement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play
property, our method naturally complements existing test-time scaling
techniques, e.g., achieving 6\% average gains when incorporated with
self-consistency on DeepSeek-V2-Lite.

</details>


### [214] [DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation](https://arxiv.org/abs/2510.14949)
*Yu Zhou,Sohyun An,Haikang Deng,Da Yin,Clark Peng,Cho-Jui Hsieh,Kai-Wei Chang,Nanyun Peng*

Main category: cs.CL

TL;DR: 现有的多模态生成模型在处理包含方言词汇的提示时，性能会显著下降，本研究提出了一个基于通用编码器的缓解策略，能在不牺牲标准美式英语表现的情况下，提升方言词汇的处理能力。


<details>
  <summary>Details</summary>
Motivation: 研究多模态生成模型在处理包含方言输入时的有效性，以及现有缓解方法的局限性。

Method: 构建包含六种英语方言的大型基准，收集超过4200个提示，并在17个图像和视频生成模型上进行评估。设计并实现了一种新的基于通用编码器的缓解策略。

Result: 现有模型在处理方言提示时性能下降32.26%至48.17%。现有的缓解方法提升效果有限且可能损害标准美式英语的表现。本研究提出的方法能将五种方言的性能提升至与标准美式英语相当的水平，且对标准美式英语的表现几乎没有负面影响。

Conclusion: 为解决多模态生成模型在处理英语方言输入时面临的性能挑战，提出了一种新的基于通用编码器的缓解策略，该策略能在保持标准美式英语性能的同时，显著提升对方言的生成能力。

Abstract: Contact languages like English exhibit rich regional variations in the form
of dialects, which are often used by dialect speakers interacting with
generative models. However, can multimodal generative models effectively
produce content given dialectal textual input? In this work, we study this
question by constructing a new large-scale benchmark spanning six common
English dialects. We work with dialect speakers to collect and verify over 4200
unique prompts and evaluate on 17 image and video generative models. Our
automatic and human evaluation results show that current state-of-the-art
multimodal generative models exhibit 32.26% to 48.17% performance degradation
when a single dialect word is used in the prompt. Common mitigation methods
such as fine-tuning and prompt rewriting can only improve dialect performance
by small margins (< 7%), while potentially incurring significant performance
degradation in Standard American English (SAE). To this end, we design a
general encoder-based mitigation strategy for multimodal generative models. Our
method teaches the model to recognize new dialect features while preserving SAE
performance. Experiments on models such as Stable Diffusion 1.5 show that our
method is able to simultaneously raise performance on five dialects to be on
par with SAE (+34.4%), while incurring near zero cost to SAE performance.

</details>


### [215] [Midtraining Bridges Pretraining and Posttraining Distributions](https://arxiv.org/abs/2510.14865)
*Emmy Liu,Graham Neubig,Chenyan Xiong*

Main category: cs.CL

TL;DR: 语言模型的


<details>
  <summary>Details</summary>
Motivation: 在预训练的后期加入高质量、指令格式的数据（称为

Method: 通过在从头预训练并跨不同领域进行监督微调的语言模型上进行受控实验，

Result: 与在监督微调（SFT）后进行比较，中期训练在数学和代码领域最有效，

Conclusion: 中期训练是一种比继续预训练更能通过减少遗忘来提高性能的域适应技术。

Abstract: Recently, many language models have been pretrained with a "midtraining"
phase, in which higher quality, often instruction-formatted data, is mixed in
at the end of pretraining. Despite the popularity of this practice, there is
little scientific understanding of this phase of model training or why it is
effective. In this work, we conduct the first systematic investigation of
midtraining through controlled experiments with language models pretrained from
scratch and fine-tuned on supervised finetuning datasets in different domains.
We find that when compared after supervised fine-tuning, the effectiveness of
midtraining is highest in the math and code domains, where midtraining can best
reduce the syntactic gap between pretraining and posttraining data. In these
cases, midtraining consistently outperforms continued pretraining in both
in-domain validation loss as well as pretraining data forgetting after
posttraining. We conduct ablations on the starting time of the midtraining
phase and mixture weights of the midtraining data, using code midtraining as a
case study, and find that timing has a greater impact than mixture weights,
with earlier introduction of specialized data, yielding greater benefits
in-domain as well as preserving general language modeling better. These
findings establish midtraining as a domain adaptation technique that compared
to continued pretraining yields better performance through reduced forgetting.

</details>


### [216] [Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent Generation](https://arxiv.org/abs/2510.14915)
*Xujun Peng,Anoop Kumar,Jingyu Wu,Parker Glenn,Daben Liu*

Main category: cs.CL

TL;DR: LLM在RAG系统中存在不一致性问题，提出结合合成数据、三元组损失和层级模型合并的新方法，通过一致性感知权重提升模型性能，实验结果显示在响应相似性方面有显著提高。


<details>
  <summary>Details</summary>
Motivation: LLM在处理语义等价输入时输出不一致，现有数据和微调技术难以解决此问题。

Method: 结合系统化合成数据生成、三元组损失优化嵌入以及新颖的层级模型合并方法，利用来自中间层激活的一致性感知权重来整合专门模型的知识。

Result: 实验结果表明，合并后的模型显著提高了输出一致性，在响应相似性方面比基线提高了约47.5%。

Conclusion: 所提出的方法为提高工业RAG系统的可靠性提供了一个切实可行的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) systems leverage Large Language Models
(LLMs) to generate accurate and reliable responses that are grounded in
retrieved context. However, LLMs often generate inconsistent outputs for
semantically equivalent inputs, a problem compounded by the scarcity of
consistency-focused training data and the limitations of current fine-tuning
techniques in enhancing output consistency. We propose a new approach combining
systematic synthetic data generation, triplet loss for better embeddings, and a
novel layer-wise model merging approach. Using consistency-aware weights
derived from intermediate layer activations, our method effectively integrates
knowledge from specialized models. Experimental results how that our merged
model significantly enhances output consistency, achieving a ~47.5\%
improvement in response similarity over the baseline, thus offering a practical
solution for increasing the reliability of an industrial RAG system.

</details>


### [217] [Predicting Task Performance with Context-aware Scaling Laws](https://arxiv.org/abs/2510.14919)
*Kyle Montgomery,David Park,Jianhong Tu,Michael Bendersky,Beliz Gunel,Dawn Song,Chenguang Wang*

Main category: cs.CL

TL;DR: 传统的Scaling laws无法捕捉下游任务表现，本文提出了一个联合模型，将下游表现作为训练计算量和提供上下文的函数。


<details>
  <summary>Details</summary>
Motivation: 传统的Scaling laws在连接模型设计因素和模型表现方面存在局限性，尤其是在下游任务表现方面，而上下文信息起着关键作用。

Method: 提出了一个简单且可解释的框架，将下游任务表现作为训练计算量和提供上下文的函数，并在Llama-2-7B和Llama-2-13B的扩展上下文变体上进行了经验验证，涵盖了算术推理、常识推理和机器翻译三个任务。

Result: 该框架能够准确地模拟分布内下游任务表现，跨越三个数量级的训练计算量，并能可靠地推断上下文量增加时的表现。

Conclusion: 研究结果为设计更高效、适用于不同下游任务的长上下文LLM提供了指导，揭示了训练计算量和上下文利用之间的相互作用。

Abstract: Scaling laws have transformed our understanding of large language models by
linking upstream metrics like cross-entropy loss to design factors such as
model size, training data, and compute. However, these conventional laws fail
to capture downstream task performance, where context plays a critical role. In
this work, we propose a straightforward, interpretable framework that jointly
models downstream performance as a function of the training compute and the
provided context. We empirically validate our framework by fitting it on the
observed downstream performance of extended-context variants of Llama-2-7B and
Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic
reasoning, common sense reasoning, and machine translation. Our results
demonstrate that our framework accurately models in-distribution downstream
performance, generalizes across three orders of magnitude in training compute,
and reliably extrapolates performance as the amount of context increases. These
findings offer valuable insights into the interplay between training compute
and context utilization, providing guidance for designing more efficient
long-context LLMs for diverse downstream tasks. Our code is available at
https://github.com/wang-research-lab/context-scaling.

</details>


### [218] [AI-Powered Early Diagnosis of Mental Health Disorders from Real-World Clinical Conversations](https://arxiv.org/abs/2510.14937)
*Jianfeng Zhu,Julina Maharjan,Xinyu Li,Karin G. Coifman,Ruoming Jin*

Main category: cs.CL

TL;DR: 本研究评估了使用机器学习模型进行心理健康筛查的有效性，特别关注使用GPT-4.1 Mini、MetaLLaMA和RoBERTa（通过LoRA进行微调）等模型。


<details>
  <summary>Details</summary>
Motivation: 抑郁症、焦虑症和PTSD等心理健康障碍在全球范围内造成了严重的残疾，但在初级保健中常常被误诊或漏诊，这凸显了开发可扩展、可及且具有情境意识的诊断工具以支持早期检测和干预的迫切需求。

Method: 研究人员使用了一个包含553个真实世界半结构化访谈及其对应诊断（包括 major depressive episodes (MDE)、焦虑症和PTSD）的数据集，对包括零样本提示（GPT-4.1 Mini和MetaLLaMA）和LoRA微调RoBERTa模型在内的多种模型类别进行了基准测试。

Result: 所提出的模型在各项诊断类别中准确率均超过80%，尤其在PTSD方面表现出色（准确率高达89%，召回率高达98%）。研究还发现，使用较短的、聚焦的语境片段可以提高召回率。LoRA微调被证明是高效且有效的，低秩配置（如秩8和16）在各项评估指标上均保持了有竞争力的性能。

Conclusion: 研究结果表明，基于大型语言模型（LLM）的模型在心理健康筛查方面可以显著优于传统的自我报告筛查工具，为实现低门槛、人工智能驱动的早期诊断提供了途径。这项工作为将机器学习整合到实际的临床工作流程中奠定了基础，尤其是在资源匮乏或污名化严重的地区。

Abstract: Mental health disorders remain among the leading cause of disability
worldwide, yet conditions such as depression, anxiety, and Post-Traumatic
Stress Disorder (PTSD) are frequently underdiagnosed or misdiagnosed due to
subjective assessments, limited clinical resources, and stigma and low
awareness. In primary care settings, studies show that providers misidentify
depression or anxiety in over 60% of cases, highlighting the urgent need for
scalable, accessible, and context-aware diagnostic tools that can support early
detection and intervention. In this study, we evaluate the effectiveness of
machine learning models for mental health screening using a unique dataset of
553 real-world, semistructured interviews, each paried with ground-truth
diagnoses for major depressive episodes (MDE), anxiety disorders, and PTSD. We
benchmark multiple model classes, including zero-shot prompting with GPT-4.1
Mini and MetaLLaMA, as well as fine-tuned RoBERTa models using LowRank
Adaptation (LoRA). Our models achieve over 80% accuracy across diagnostic
categories, with especially strongperformance on PTSD (up to 89% accuracy and
98% recall). We also find that using shorter context, focused context segments
improves recall, suggesting that focused narrative cues enhance detection
sensitivity. LoRA fine-tuning proves both efficient and effective, with
lower-rank configurations (e.g., rank 8 and 16) maintaining competitive
performance across evaluation metrics. Our results demonstrate that LLM-based
models can offer substantial improvements over traditional self-report
screening tools, providing a path toward low-barrier, AI-powerd early
diagnosis. This work lays the groundwork for integrating machine learning into
real-world clinical workflows, particularly in low-resource or high-stigma
environments where access to timely mental health care is most limited.

</details>


### [219] [LaSeR: Reinforcement Learning with Last-Token Self-Rewarding](https://arxiv.org/abs/2510.14943)
*Wenkai Yang,Weijie Liu,Ruobing Xie,Yiju Guo,Lulu Wu,Saiyong Yang,Yankai Lin*

Main category: cs.CL

TL;DR: LaSeR算法通过对RLVR目标进行理论分析，提出了一种新方法，将真实推理奖励与最后一个token的自我奖励分数联系起来，从而在不显著增加计算成本的情况下，同时优化LLM的推理和自我奖励能力，提高了模型在训练和测试阶段的性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在测试时缺乏验证信号，虽然通过训练模型的自我验证能力解决了这个问题，但需要分步生成解决方案和自我验证，效率低下。

Method: LaSeR算法基于理论分析，将RL自我验证目标的闭式解简化为真实推理奖励等于最后一个token的自我奖励分数（通过策略模型的下一个token对特定token的对数概率与预计算常数的差值计算）。该算法将此发现整合到RLVR损失中，并添加了一个MSE损失项，以使最后一个token的自我奖励分数与基于验证器的推理奖励保持一致，从而联合优化LLM的推理和自我奖励能力。

Result: LaSeR算法在不显著增加额外计算成本（仅需额外一个token推理）的情况下，提升了模型的推理性能，并赋予了模型显著的自我奖励能力，从而提高了推理时的性能扩展性。

Conclusion: LaSeR算法通过理论创新，提供了一种更高效的RLVR方法，能够同时提升LLM的推理和自我奖励能力，并在实际应用中展现出优越的性能和扩展性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a core paradigm for enhancing the reasoning capabilities of Large Language
Models (LLMs). To address the lack of verification signals at test time, prior
studies incorporate the training of model's self-verification capability into
the standard RLVR process, thereby unifying reasoning and verification
capabilities within a single LLM. However, previous practice requires the LLM
to sequentially generate solutions and self-verifications using two separate
prompt templates, which significantly reduces efficiency. In this work, we
theoretically reveal that the closed-form solution to the RL objective of
self-verification can be reduced to a remarkably simple form: the true
reasoning reward of a solution is equal to its last-token self-rewarding score,
which is computed as the difference between the policy model's next-token
log-probability assigned to any pre-specified token at the solution's last
token and a pre-calculated constant, scaled by the KL coefficient. Based on
this insight, we propose LaSeR (Reinforcement Learning with Last-Token
Self-Rewarding), an algorithm that simply augments the original RLVR loss with
a MSE loss that aligns the last-token self-rewarding scores with verifier-based
reasoning rewards, jointly optimizing the reasoning and self-rewarding
capabilities of LLMs. The optimized self-rewarding scores can be utilized in
both training and testing to enhance model performance. Notably, our algorithm
derives these scores from the predicted next-token probability distribution of
the last token immediately after generation, incurring only the minimal extra
cost of one additional token inference. Experiments show that our method not
only improves the model's reasoning performance but also equips it with
remarkable self-rewarding capability, thereby boosting its inference-time
scaling performance.

</details>


### [220] [MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics](https://arxiv.org/abs/2510.14944)
*Yuxing Lu,Xukai Zhao,J. Ben Tamo,Micky C. Nnamdi,Rui Peng,Shuang Zeng,Xingyu Hu,Jinzhuo Wang,May D. Wang*

Main category: cs.CL

TL;DR: LLMs在专业科学领域（如代谢组学）的表现不佳，MetaBench是首个评估LLM在该领域能力的基准，评估了25个LLM，结果显示LLMs在文本生成方面表现良好，但在跨数据库标识符关联和长尾代谢物处理方面存在挑战。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在需要深入、互联知识的专业科学领域（特别是代谢组学）的能力。

Method: 引入MetaBench，一个包含知识、理解、关联、推理和研究五个关键能力评估的基准，评估了25个开源和闭源LLMs。

Result: LLMs在文本生成任务上表现良好，但在跨数据库标识符关联（即使有检索增强）和处理长尾代谢物（注释稀疏）方面存在困难，性能随之下降。

Conclusion: MetaBench为代谢组学AI系统的开发和评估提供了基础设施，有望促进可靠的计算工具的发展。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities on
general text; however, their proficiency in specialized scientific domains that
require deep, interconnected knowledge remains largely uncharacterized.
Metabolomics presents unique challenges with its complex biochemical pathways,
heterogeneous identifier systems, and fragmented databases. To systematically
evaluate LLM capabilities in this domain, we introduce MetaBench, the first
benchmark for metabolomics assessment. Curated from authoritative public
resources, MetaBench evaluates five capabilities essential for metabolomics
research: knowledge, understanding, grounding, reasoning, and research. Our
evaluation of 25 open- and closed-source LLMs reveals distinct performance
patterns across metabolomics tasks: while models perform well on text
generation tasks, cross-database identifier grounding remains challenging even
with retrieval augmentation. Model performance also decreases on long-tail
metabolites with sparse annotations. With MetaBench, we provide essential
infrastructure for developing and evaluating metabolomics AI systems, enabling
systematic progress toward reliable computational tools for metabolomics
research.

</details>


### [221] [Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents](https://arxiv.org/abs/2510.14967)
*Guoqing Wang,Sunhao Dai,Guangze Ye,Zeyu Gan,Wei Yao,Yong Deng,Xiaofeng Wu,Zhenzhe Ying*

Main category: cs.CL

TL;DR: LLM智能体使用基于信息增益的策略优化（IGPO）进行多轮工具使用训练，通过每一步的信息获取提供密集奖励，解决了传统基于结果的稀疏奖励问题，提高了准确性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的智能体在多轮工具使用中面临奖励稀疏的问题，导致优势崩溃和信用分配困难。

Method: 提出信息增益策略优化（IGPO）框架，将每一步交互视为获取信息的增量过程，并将每步奖励定义为模型产生正确答案的概率的边际增量，该奖励直接从模型自身的信念更新中获得。

Result: 在领域内和领域外基准测试中，IGPO在多轮场景中持续优于强基线，实现了更高的准确性和改进的样本效率。

Conclusion: IGPO框架通过提供密集且内在的监督，有效解决了LLM智能体在多轮工具使用中的奖励稀疏问题，并在实验中取得了显著的性能提升。

Abstract: Large language model (LLM)-based agents are increasingly trained with
reinforcement learning (RL) to enhance their ability to interact with external
environments through tool use, particularly in search-based settings that
require multi-turn reasoning and knowledge acquisition. However, existing
approaches typically rely on outcome-based rewards that are only provided at
the final answer. This reward sparsity becomes particularly problematic in
multi-turn settings, where long trajectories exacerbate two critical issues:
(i) advantage collapse, where all rollouts receive identical rewards and
provide no useful learning signals, and (ii) lack of fine-grained credit
assignment, where dependencies between turns are obscured, especially in
long-horizon tasks. In this paper, we propose Information Gain-based Policy
Optimization (IGPO), a simple yet effective RL framework that provides dense
and intrinsic supervision for multi-turn agent training. IGPO models each
interaction turn as an incremental process of acquiring information about the
ground truth, and defines turn-level rewards as the marginal increase in the
policy's probability of producing the correct answer. Unlike prior
process-level reward approaches that depend on external reward models or costly
Monte Carlo estimation, IGPO derives intrinsic rewards directly from the
model's own belief updates. These intrinsic turn-level rewards are combined
with outcome-level supervision to form dense reward trajectories. Extensive
experiments on both in-domain and out-of-domain benchmarks demonstrate that
IGPO consistently outperforms strong baselines in multi-turn scenarios,
achieving higher accuracy and improved sample efficiency.

</details>


### [222] [LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training](https://arxiv.org/abs/2510.14969)
*Yiming Wang,Da Yin,Yuedong Cui,Ruichen Zheng,Zhiqian Li,Zongyu Lin,Di Wu,Xueqing Wu,Chenchen Ye,Yu Zhou,Kai-Wei Chang*

Main category: cs.CL

TL;DR: UI-Simulator是一个可扩展的范例，用于生成结构化UI状态和转换，以合成大规模训练轨迹，UI-Simulator-Grow是一种有针对性的扩展策略，通过优先处理高影响力任务和合成信息轨迹变体，实现更快、更数据高效的扩展。


<details>
  <summary>Details</summary>
Motivation: 为了让数字代理能够泛化到现实世界的任务，需要多样化、大规模的UI轨迹，但收集此类数据的成本过高。

Method: UI-Simulator集成了数字世界模拟器、引导式探索过程和轨迹包装器，用于合成训练轨迹。UI-Simulator-Grow是一种有针对性的扩展策略，优先处理高影响力任务并合成信息轨迹变体。

Result: 在WebArena和AndroidWorld上的实验表明，UI-Simulator在具有更好鲁棒性的情况下，可以媲美或超越在真实UI上训练的开源代理。UI-Simulator-Grow使用Llama-3-8B-Instruct作为基础模型，达到了Llama-3-70B-Instruct的性能。

Conclusion: UI-Simulator和UI-Simulator-Grow在生成大规模、高质量的UI训练数据方面表现出色，并且UI-Simulator-Grow的定向合成扩展范式可以持续有效地提升数字代理的性能。

Abstract: Digital agents require diverse, large-scale UI trajectories to generalize
across real-world tasks, yet collecting such data is prohibitively expensive in
both human annotation, infra and engineering perspectives. To this end, we
introduce $\textbf{UI-Simulator}$, a scalable paradigm that generates
structured UI states and transitions to synthesize training trajectories at
scale. Our paradigm integrates a digital world simulator for diverse UI states,
a guided rollout process for coherent exploration, and a trajectory wrapper
that produces high-quality and diverse trajectories for agent training. We
further propose $\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that
enables more rapid and data-efficient scaling by prioritizing high-impact tasks
and synthesizes informative trajectory variants. Experiments on WebArena and
AndroidWorld show that UI-Simulator rivals or surpasses open-source agents
trained on real UIs with significantly better robustness, despite using weaker
teacher models. Moreover, UI-Simulator-Grow matches the performance of
Llama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,
highlighting the potential of targeted synthesis scaling paradigm to
continuously and efficiently enhance the digital agents.

</details>


### [223] [TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar](https://arxiv.org/abs/2510.14972)
*Yinxi Li,Yuntian Deng,Pengyu Nie*

Main category: cs.CL

TL;DR: 代码大模型使用的子词（subword）分词器（tokenizer）由统计驱动而非语法驱动，导致语义相同的代码片段可能因格式（如空格、命名）不同而被不同地分词，从而影响模型行为。


<details>
  <summary>Details</summary>
Motivation: 现有的代码大模型（LLMs）依赖于统计驱动的子词分词器（如BPE），这可能导致语义相同的代码片段被错误地分词，影响模型的理解和生成能力。

Method: 提出TokDrift框架，该框架应用语义保持的重写规则来创建仅在分词上不同的代码变体，并进行层级分析。

Result: 即使是细微的格式变化也会导致模型行为发生显著变化，问题根源在于早期嵌入层，子词分割未能捕捉语法标记边界。

Conclusion: 分词与语法对齐失调是代码理解和生成可靠性的潜在障碍，未来的代码LLMs需要采用语法感知（grammar-aware）的分词方法。

Abstract: Large language models (LLMs) for code rely on subword tokenizers, such as
byte-pair encoding (BPE), learned from mixed natural language text and
programming language code but driven by statistics rather than grammar. As a
result, semantically identical code snippets can be tokenized differently
depending on superficial factors such as whitespace or identifier naming. To
measure the impact of this misalignment, we introduce TokDrift, a framework
that applies semantic-preserving rewrite rules to create code variants
differing only in tokenization. Across nine code LLMs, including large ones
with over 30B parameters, even minor formatting changes can cause substantial
shifts in model behavior. Layer-wise analysis shows that the issue originates
in early embeddings, where subword segmentation fails to capture grammar token
boundaries. Our findings identify misaligned tokenization as a hidden obstacle
to reliable code understanding and generation, highlighting the need for
grammar-aware tokenization for future code LLMs.

</details>


### [224] [Attention Is All You Need for KV Cache in Diffusion LLMs](https://arxiv.org/abs/2510.14973)
*Quan Nguyen-Tri,Mukul Ranjan,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 本研究提出了一种名为 Elastic-Cache 的训练无关、架构无关的策略，用于自适应地重新计算扩散大语言模型 (DLMs) 的 KV 缓存，以在最小化解码延迟的同时最大化预测准确性。该方法通过注意力感知漂移测试和深度感知调度来决定何时以及在何处刷新缓存，减少了冗余计算，并能显著加速解码，同时对生成质量的影响很小。


<details>
  <summary>Details</summary>
Motivation: 现有的 DLM 解码器在每个去噪步骤和层级都会重新计算所有 token 的 QKV，即使 KV 状态变化很小，尤其是在浅层。这种方法存在大量冗余，导致计算效率低下。

Method: Elastic-Cache 策略基于三个关键观察：(1) 远程 MASK token 可作为长度偏差进行块级缓存；(2) KV 动态随深度增加，表明从深层开始选择性刷新即可；(3) 最受关注的 token KV 漂移最小。基于此，Elastic-Cache 结合了注意力感知的漂移测试（决定何时刷新）和深度感知的调度（决定在哪里刷新），从选定层开始重新计算，并重用浅层缓存和窗口外的 MASK 缓存。

Result: 在 LLaDA-Instruct, LLaDA-1.5, 和 LLaDA-V 模型上，针对数学推理和代码生成任务，Elastic-Cache 实现了显著的加速：在 GSM8K (256 tokens) 上达到 8.7 倍，在更长序列上达到 45.1 倍，在 HumanEval 上达到 4.8 倍，同时保持了比基线更高的准确性。与基于置信度的方法相比，该方法实现了 6.8 倍的吞吐量提升。

Conclusion: Elastic-Cache 是一种有效的、训练无关的策略，能够自适应地优化 DLMs 的 KV 缓存，显著减少冗余计算，提高解码速度，并保持生成质量，从而为 DLMs 的实际部署提供了可行性。

Abstract: This work studies how to adaptively recompute key-value (KV) caches for
diffusion large language models (DLMs) to maximize prediction accuracy while
minimizing decoding latency. Prior methods' decoders recompute QKV for all
tokens at every denoising step and layer, despite KV states changing little
across most steps, especially in shallow layers, leading to substantial
redundancy. We make three observations: (1) distant ${\bf MASK}$ tokens
primarily act as a length-bias and can be cached block-wise beyond the active
prediction window; (2) KV dynamics increase with depth, suggesting that
selective refresh starting from deeper layers is sufficient; and (3) the
most-attended token exhibits the smallest KV drift, providing a conservative
lower bound on cache change for other tokens. Building on these, we propose
${\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that
jointly decides ${when}$ to refresh (via an attention-aware drift test on the
most-attended token) and ${where}$ to refresh (via a depth-aware schedule that
recomputes from a chosen layer onward while reusing shallow-layer caches and
off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs
adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant
computation and accelerating decoding with negligible loss in generation
quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across
mathematical reasoning and code generation tasks demonstrate consistent
speedups: $8.7\times$ on GSM8K (256 tokens), $45.1\times$ on longer sequences,
and $4.8\times$ on HumanEval, while consistently maintaining higher accuracy
than the baseline. Our method achieves significantly higher throughput
($6.8\times$ on GSM8K) than existing confidence-based approaches while
preserving generation quality, enabling practical deployment of diffusion LLMs.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [225] [Generalized Pinching-Antenna Systems: A Tutorial on Principles, Design Strategies, and Future Directions](https://arxiv.org/abs/2510.14166)
*Yanqing Xu,Jingjing Cui,Yongxu Zhu,Zhiguo Ding,Tsung-Hui Chang,Robert Schober,Vincent W. S. Wong,Octavia A. Dobre,George K. Karagiannidis,H. Vincent Poor,Xiaohu You*

Main category: eess.SP

TL;DR: 该论文介绍了广义捏合天线系统，这是一种可重构的灵活天线架构，可通过在信号引导介质上动态定位和激活辐射单元来实现。


<details>
  <summary>Details</summary>
Motivation: 介绍捏合天线系统的概念，并强调其与传统固定天线系统相比的灵活性和空间可重构性。

Method: 描述了广义捏合天线系统的物理机制，包括基于介电波导、泄漏同轴电缆、表面波引导结构等的实现方式，以及不同的馈电和激活机制。论文还讨论了相关的无线信道模型、系统架构和设计策略。

Result: 研究了捏合天线系统与新兴无线技术的集成，以实现以用户为中心的解决方案，并确定了关键的开放研究挑战和未来方向。

Conclusion: 广义捏合天线系统具有形成、重新定位或停用辐射点的能力，有望在下一代无线网络中实现以用户为中心和动态覆盖。

Abstract: Pinching-antenna systems have emerged as a novel and transformative
flexible-antenna architecture for next-generation wireless networks. They offer
unprecedented flexibility and spatial reconfigurability by enabling dynamic
positioning and activation of radiating elements along a signal-guiding medium
(e.g., dielectric waveguides), which is not possible with conventional fixed
antenna systems. In this paper, we introduce the concept of generalized
pinching antenna systems, which retain the core principle of creating localized
radiation points on demand, but can be physically realized in a variety of
settings. These include implementations based on dielectric waveguides, leaky
coaxial cables, surface-wave guiding structures, and other types of media,
employing different feeding methods and activation mechanisms (e.g.,
mechanical, electronic, or hybrid). Despite differences in their physical
realizations, they all share the same inherent ability to form, reposition, or
deactivate radiation sites as needed, enabling user-centric and dynamic
coverage. We first describe the underlying physical mechanisms of
representative generalized pinching-antenna realizations and their associated
wireless channel models, highlighting their unique propagation and
reconfigurability characteristics compared with conventional antennas. Then, we
review several representative pinching-antenna system architectures, ranging
from single- to multiple-waveguide configurations, and discuss advanced design
strategies tailored to these flexible deployments. Furthermore, we examine
their integration with emerging wireless technologies to enable synergistic,
user-centric solutions. Finally, we identify key open research challenges and
outline future directions, charting a pathway toward the practical deployment
of generalized pinching antennas in next-generation wireless networks.

</details>


### [226] [Integrated Massive Communication and Target Localization in 6G Cell-Free Networks](https://arxiv.org/abs/2510.14281)
*Junyuan Gao,Weifeng Zhu,Shuowen Zhang,Yongpeng Wu,Jiannong Cao,Giuseppe Caire,Liang Liu*

Main category: eess.SP

TL;DR: 将定位功能嵌入海量通信系统，以实现6G网络。


<details>
  <summary>Details</summary>
Motivation: 重点研究6G无线网络中的集成传感与通信（ISAC）和海量通信。

Method: 提出一种基于混合消息传递的框架，包含多种近似方法，以处理用户信道和目标位置之间复杂的依赖关系，从而降低计算复杂度。

Result: 所提方法可同时实现高精度的设备活动检测、信道估计和目标定位。

Conclusion: 该研究验证了为未来6G网络将定位功能嵌入海量通信系统的可行性。

Abstract: This paper presents an initial investigation into the combination of
integrated sensing and communication (ISAC) and massive communication, both of
which are largely regarded as key scenarios in sixth-generation (6G) wireless
networks. Specifically, we consider a cell-free network comprising a large
number of users, multiple targets, and distributed base stations (BSs). In each
time slot, a random subset of users becomes active, transmitting pilot signals
that can be scattered by the targets before reaching the BSs. Unlike
conventional massive random access schemes, where the primary objectives are
device activity detection and channel estimation, our framework also enables
target localization by leveraging the multipath propagation effects introduced
by the targets. However, due to the intricate dependency between user channels
and target locations, characterizing the posterior distribution required for
minimum mean-square error (MMSE) estimation presents significant computational
challenges. To handle this problem, we propose a hybrid message passing-based
framework that incorporates multiple approximations to mitigate computational
complexity. Numerical results demonstrate that the proposed approach achieves
high-accuracy device activity detection, channel estimation, and target
localization simultaneously, validating the feasibility of embedding
localization functionality into massive communication systems for future 6G
networks.

</details>


### [227] [Integrated Sensing and Communication: Towards Multifunctional Perceptive Network](https://arxiv.org/abs/2510.14358)
*Yuanhao Cui,Jiali Nie,Fan Liu,Weijie Yuan,Zhiyong Feng,Xiaojun Jing,Yulin Liu,Jie Xu,Christos Masouros,Shuguang Cui*

Main category: eess.SP

TL;DR: ISAC将无线网络从单一数据传输扩展到支持多样化应用的多功能平台。


<details>
  <summary>Details</summary>
Motivation: 随着数据流量增长放缓，移动行业无法仅依靠通信服务来维持发展，因此需要ISAC这种变革性解决方案。

Method: 本文对ISAC进行了综述，重点介绍了关键挑战、机遇和应用场景，为该领域的未来研究提供指导。

Result: ISAC通过将传感能力嵌入通信网络，实现了多功能无线系统。

Conclusion: ISAC是支持多样化应用的多功能平台，为无线网络带来了新的发展机遇。

Abstract: The capacity-maximization design philosophy has driven the growth of wireless
networks for decades. However, with the slowdown in recent data traffic demand,
the mobile industry can no longer rely solely on communication services to
sustain development. In response, Integrated Sensing and Communications (ISAC)
has emerged as a transformative solution, embedding sensing capabilities into
communication networks to enable multifunctional wireless systems. This
paradigm shift expands the role of networks from sole data transmission to
versatile platforms supporting diverse applications. In this review, we provide
a bird's-eye view of ISAC for new researchers, highlighting key challenges,
opportunities, and application scenarios to guide future exploration in this
field.

</details>


### [228] [Error Rate Analysis and Low-Complexity Receiver Design for Zero-Padded AFDM](https://arxiv.org/abs/2510.14507)
*Qin Yi,Zeping Sui,Zilong Liu*

Main category: eess.SP

TL;DR: 该论文提出了一种低复杂度接收机设计，用于零填充仿射频分复用（ZP-AFDM）系统，并在理论上分析了比特错误率性能。


<details>
  <summary>Details</summary>
Motivation: 研究零填充仿射频分复用（ZP-AFDM）系统的误码率性能和低复杂度接收机设计。

Method: 利用时域（TD）信道矩阵的零填充辅助下三角结构，提出了一种新颖的低复杂度最小均方误差（MMSE）检测器和基于最大比合并的TD（MRC-TD）检测器。对MMSE和最大似然检测器的理论比特错误率（BER）性能进行了分析。

Result: 仿真结果表明，所提出的检测器在实现与基于矩阵求逆的传统MMSE检测器相同的BER性能的同时，复杂度显著降低。

Conclusion: 提出的低复杂度检测器在ZP-AFDM系统中的性能与传统方法相当，但计算量大大减少。

Abstract: This paper studies the error rate performance and low-complexity receiver
design for zero-padded affine frequency division multiplexing (ZP-AFDM)
systems. By exploiting the unique ZP-aided lower triangular structure of the
time domain (TD) channel matrix, we propose {a novel low-complexity} minimum
mean square error (MMSE) detector and {a} maximum ratio combining-based TD
(MRC-TD) detector. Furthermore, the theoretical bit error rate (BER)
{performance} of both MMSE and maximum likelihood detectors {is} analyzed.
Simulation results demonstrate {that} the proposed detectors can achieve
identical BER performance to that of {the conventional MMSE detector based on
matrix inversion} while {enjoying significantly reduced complexity.}

</details>


### [229] [Integrated Sensing and Communication with Tri-Hybrid Beamforming Across Electromagnetically Reconfigurable Antennas](https://arxiv.org/abs/2510.14530)
*Jiangong Chen,Xia Lei,Yuchen Zhang,Kaitao Meng,Christos Masouros*

Main category: eess.SP

TL;DR: ERA-aided ISAC systems enhance Degrees of Freedom (DoFs) and performance through dynamically reconfigurable radiation patterns, achieving significant gains over conventional systems.


<details>
  <summary>Details</summary>
Motivation: The performance of beamforming in ISAC systems (MU-MIMO and MIMO radar) is limited by the DoFs in conventional hybrid beamforming. ERA-aided ISAC aims to overcome this limitation.

Method: A tri-hybrid beamforming optimization framework combining digital, analog, and Electromagnetic (EM) beamforming is proposed. An integrated Fractional Programming (FP) and Manifold Optimization (MO) approach is used to solve the optimization problem with closed-form updates.

Result: The proposed ERA-ISAC system achieves almost 10 dB Sensing and Communication (S&C) performance gain compared to conventional hybrid beamforming counterparts with Omnidirectional Antennas (OA).

Conclusion: ERA-aided ISAC systems with tri-hybrid beamforming and an FP/MO optimization approach offer significant performance improvements for integrated sensing and communication by enhancing system DoFs.

Abstract: Beamforming with a sufficient number of antennas is one of the most
significant technologies for both Multi-user (MU) Multiple-input
Multiple-output (MIMO) communication and MIMO radar sensing in Integrated
Sensing and Communication (ISAC) systems. However, its performance suffers from
limited Degrees of Freedom (DoFs) in conventional hybrid beamforming systems.
To overcome this, we propose an Electromagnetically Reconfigurable Antenna
(ERA)-aided ISAC system, where transmit ERAs dynamically adjust their radiation
patterns to enhance system DoFs and improve overall performance. Specifically,
we design a tri-hybrid beamforming optimization framework combining digital,
analog, and Electromagnetic (EM) beamforming to jointly maximize communication
rate and sensing Signal-to-Clutter-plus-Noise Ratio (SCNR). Furthermore, an
integrated Fractional Programming (FP) and Manifold Optimization (MO) approach
is developed to transform the problem into tractable subproblems with
closed-form updates. Simulation results verify that the proposed ERA-ISAC
system achieves almost 10 dB Sensing and Communication (S&C) performance gain
compared to its conventional hybrid beamforming counterparts with
Omnidirectional Antenna (OA).

</details>


### [230] [Proceedings of the second edition of the International Symposium on Computational Sensing (ISCS25)](https://arxiv.org/abs/2510.14604)
*Thomas Feuillen,Amirafshar Moshtaghpour*

Main category: eess.SP

TL;DR: ISCS是一个汇集了光学、电子、雷达、天文、生物医学、遥感和信号处理等领域研究人员的计算传感国际研讨会。


<details>
  <summary>Details</summary>
Motivation: ISCS旨在为计算传感领域不同应用方向的研究人员提供一个交流学习的论坛，以分享新发现和挑战。

Method: ISCS是一个为期三天的研讨会，包含6场主旨演讲，并接受科学演讲和展示的扩展摘要。

Result: ISCS促进了不同计算传感应用领域研究人员之间的交流和学习。

Conclusion: ISCS为计算传感领域的研究人员提供了一个跨学科交流的平台。

Abstract: The International Symposium on Computational Sensing (ISCS) brings together
researchers from optical microscopy, electron microscopy, RADAR, astronomical
imaging, biomedical imaging, remote sensing, and signal processing. With a
particular focus on applications and demonstrators, the purpose of this
symposium is to be a forum where researchers in computational sensing working
in seemingly unrelated applications can learn, discover, and exchange on their
new findings and challenges. This 3-day symposium in the heart of Europe
features 6 keynotes speakers and is open to extended abstracts for scientific
presentations and show-and-tell demonstrations.

</details>


### [231] [Bridging Theory and Practice in Reconfigurable Fluid Antenna Systems](https://arxiv.org/abs/2510.14794)
*Halvin Yang,Yizhe Zhao,Kai-Kit Wong,Hsiao-Hwa Chen,Chan-Byoung Chae*

Main category: eess.SP

TL;DR: 流体天线在下一代无线系统中具有巨大潜力，但实际应用中需要考虑实际约束，而不是理想化的假设。


<details>
  <summary>Details</summary>
Motivation: 理论分析常假设流体天线具有瞬时重构、完美信道知识、静态环境和理想材料等，这些在实际中不成立。

Method: 本文分析了这些理想化假设与实际情况（如有限驱动时间、不完美信道信息、快速变化信道、电磁耦合和机械约束）的差异，并通过仿真实例展示了忽略这些因素对性能评估的误导。

Result: 忽略实际约束会导致对容量和覆盖范围等性能提升的估计过高。

Conclusion: 虽然可重构天线对 B5G/6G 和物联网应用前景广阔，但只有在设计和评估中纳入实际考量，才能充分发挥其潜力。

Abstract: Fluid antennas, including those based on liquid, mechanical, and pixel-based
technologies, are poised to significantly enhance next-generation wireless
systems by adaptively optimizing their radiation characteristics. Many
theoretical analyses assumed near-instant reconfiguration, perfect channel
knowledge, static or slowly varying propagation environments, and ideal
material properties that rarely hold in practice. In this article, we dissect
these common assumptions and contrast them with the realities of finite
actuation time, limited and imperfect channel state information, rapidly
changing fading conditions, electromagnetic coupling, and mechanical
constraints. Through illustrative examples and simulations, we demonstrate how
ignoring these factors can lead to overestimated gains in capacity, coverage,
etc.. We then propose modeling refinements, experimental validation methods,
and emerging control algorithms that better account for real-world constraints.
Our findings highlight that, while reconfigurable antennas remain highly
promising for B5G/6G and Internet of things (IoT) applications, their full
potential can only be realized by incorporating practical considerations into
system design and performance evaluation.

</details>


### [232] [A Scalable MVDR Beamforming Algorithm That is Linear in the Number of Antennas](https://arxiv.org/abs/2510.14802)
*Sanjaya Herath,Armin Gerami,Kevin Wagner,Ramani Duraiswami,Christopher A. Metzler*

Main category: eess.SP

TL;DR: 本文提出了一种针对大规模天线阵列的MVDR波束形成方法，将计算复杂度从三次方降低到线性，同时保持了高波束形成精度。


<details>
  <summary>Details</summary>
Motivation: 传统的MVDR波束形成技术在应用于大规模阵列时面临计算复杂度过高（与天线单元数量呈立方关系）的挑战。

Method: 利用Sherman-Morrison公式、低秩奇异值分解（SVD）近似和代数运算，提出了一种可扩展的MVDR波束形成方法，适用于信噪比低于噪声基线的信号（如GPS）。

Result: 该方法将计算复杂度从三次方降低到线性，并通过仿真验证了其在计算效率和波束形成精度方面优于传统MVDR方法，尤其是在大规模阵列场景下。

Conclusion: 所提出的方法显著降低了计算量，同时保持了高波束形成精度，有望在雷达、声纳和无线通信等领域的大规模天线阵列实时应用中发挥作用。

Abstract: The Minimum Variance Distortionless Response (MVDR) beamforming technique is
widely applied in array systems to mitigate interference. However, applying
MVDR to large arrays is computationally challenging; its computational
complexity scales cubically with the number of antenna elements. In this paper,
we introduce a scalable MVDR beamforming method tailored for massive arrays.
Our approach, which is specific to scenarios where the signal of interest is
below the noise floor (e.g.,~GPS), leverages the Sherman-Morrison formula,
low-rank Singular Value Decomposition (SVD) approximations, and algebraic
manipulation. Using our approach, we reduce the computational complexity from
cubic to linear in the number of antennas. We evaluate the proposed method
through simulations, comparing its computational efficiency and beamforming
accuracy with the conventional MVDR approach. Our method significantly reduces
the computational load while maintaining high beamforming accuracy for
large-scale arrays. This solution holds promise for real-time applications of
MVDR beamforming in fields like radar, sonar, and wireless communications,
where massive antenna arrays are proliferating.

</details>


### [233] [Joint Channel and CFO Estimation From Beam-Swept Synchronization Signal Under Strong Inter-Cell Interference](https://arxiv.org/abs/2510.14806)
*Bowen Li,Junting Chen,Nikolaos Pappas*

Main category: eess.SP

TL;DR: 该研究提出了一种基于最大似然（ML）的跨导频估计框架，利用载波频率偏移（CFO）在波束扫描同步信号（SS）中的不变性，跨多个观测进行相干聚合，以在干扰下增强目标信号。


<details>
  <summary>Details</summary>
Motivation: 未来的智能网络需要感知所有传输信号，而不仅仅是最强的信号，但估计被强干扰信号掩埋的目标信号是一个基本障碍。

Method: 提出了一种基于最大似然（ML）的跨导频估计框架，该框架利用了载波频率偏移（CFO）在波束扫描同步信号（SS）中的不变性，并通过跨多个观测进行相干聚合来增强目标信号。

Result: 通过Cramer-Rao下界（CRLB）分析和仿真，证明了即使目标信号的强度只有干扰信号的千分之一，也能进行可靠的估计。低空无线电地图案例研究也验证了该框架的有效性。

Conclusion: 所提出的最大似然（ML）跨导频估计框架能够可靠地估计出被强共信道干扰所掩埋的目标信号，即使目标信号的强度远低于干扰信号。该方法在低空无线电地图等实际场景中也得到了验证。

Abstract: Complete awareness of the wireless environment, crucial for future
intelligent networks, requires sensing all transmitted signals, not just the
strongest. A fundamental barrier is estimating the target signal when it is
buried under strong co-channel interference from other transmitters, a failure
of which renders the signal unusable. This work proposes a maximum likelihood
(ML)-based cross-preamble estimation framework that exploits carrier frequency
offset (CFO) constancy across beam-swept synchronization signals (SS),
coherently aggregating information across multiple observations to reinforce
the desired signal against overwhelming interference. Cramer-Rao lower bound
(CRLB) analysis and simulation demonstrate reliable estimation even when the
signal is over a thousand times weaker than the interference. A low-altitude
radio-map case study further verifies the framework's practical effectiveness.

</details>


### [234] [Decoding in the presence of ISI without interleaving ORBGRAND AI](https://arxiv.org/abs/2510.14939)
*Ken R. Duffy,Moritz Grundei,Jane A. Millward,Muralidhar Rangaswamy,Muriel Medard*

Main category: eess.SP

TL;DR: ORBGRAND-AI是一种新的解码器，用于在有 ISI 的信道中，通过近似独立性来减轻 ISI 引起的噪声染色，其性能与 CA-SCL 解码器相当，甚至更好，同时能耗更低。


<details>
  <summary>Details</summary>
Motivation: Inter symbol interference (ISI) 会导致噪声染色，需要一种有效的解码器来处理。ORBGRAND-AI 旨在解决这个问题。

Method: 提出了一种名为 ORBGRAND-AI 的解码器，该解码器受统计物理学中近似独立性概念的启发，通过放弃交织器来减轻噪声染色，并应用于 ISI 信道。

Result: ORBGRAND-AI 在 ISI 信道中，与 CA-SCL 解码器相比，在相同的能量每比特信息量下，实现了相同或更低的块错误率 (BLER)。研究还表明，二阶自回归模型能够充分表示 RFView 信道效应。

Conclusion: ORBGRAND-AI 是一种在 ISI 信道中具有竞争力的解码器，能够与先进的解码器相媲美，并且在能耗方面具有优势。

Abstract: Inter symbol interference (ISI), which occurs in a wide variety of channels,
is a result of time dispersion. It can be mitigated by equalization which
results in noise coloring. For such colored noise, we propose a decoder called
Ordered Reliability Bit Guessing Random Additive Noise Decoding (ORBGRANDAI)
which is inspired by the development of approximate independence in statistical
physics. By foregoing interleaving, ORBGRAND-AI can deliver the same, or lower,
block error rate (BLER) for the same amount of energy per information bit in an
ISI channel as a state-of-the-art soft input decoder, such as Cyclic Redundancy
Check Assisted-Successive Cancellation List (CA-SCL) decoding, with an
interleaver. To assess the decoding performance of ORBGRAND-AI, we consider
delay tap models and their associated colored noise. In particular, we examine
a two-tap dicode ISI channel as well as an ISI channel derived from data from
RFView, a physics-informed modeling and simulation tool. We investigate the
dicode and RFView channel under a variety of imperfect channel state
information assumptions and show that a second order autoregressive model
adequately represents the RFView channel effect.

</details>


### [235] [Transfer Learning-Enabled Efficient Raman Pump Tuning under Dynamic Launch Power for C+L Band Transmission](https://arxiv.org/abs/2510.09047)
*Jiaming Liu,Rui Wang,JinJiang Li,Hong Lin,Jing Zhang,Kun Qiu*

Main category: eess.SP

TL;DR: Proposed a transfer learning-enabled Transformer framework for accurate modeling and Raman pump design in C+L-band systems, achieving low RMSE and GSNR variation.


<details>
  <summary>Details</summary>
Motivation: To simultaneously achieve accurate modeling and Raman pump design in C+L-band systems.

Method: Used a transfer learning-enabled Transformer framework.

Result: Achieved RMSE within 0.22 dB for modeling and peak-to-peak GSNR variation/deviation within 0.86/0.1 dB.

Conclusion: The proposed framework enables accurate modeling and Raman pump design in C+L-band systems with high precision.

Abstract: We propose a transfer learning-enabled Transformer framework to
simultaneously realize accurate modeling and Raman pump design in C+L-band
systems. The RMSE for modeling and peak-to-peak GSNR variation/deviation is
within 0.22 dB and 0.86/0.1 dB, respectively.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [236] [CBVB-nH complexes as prevalent defects in metal-organic vapor-phase epitaxy-grown hexagonal boron nitride](https://arxiv.org/abs/2510.14012)
*Marek Maciaszek,Bartłomiej Baur*

Main category: cond-mat.mtrl-sci

TL;DR: hBN中含碳、空位和氢的缺陷复合物（CBVB-nH）在氮气充足条件下具有良好的热力学稳定性和光学特性，有望用于量子技术。


<details>
  <summary>Details</summary>
Motivation: 为了深入理解hBN（六方氮化硼）中光学活性缺陷的物理和化学性质，以推动其在量子技术中的应用。

Method: 使用从头算（ab initio）计算方法，研究了涉及碳、空位和氢的缺陷复合物的热力学稳定性和光学性质。

Result: 计算表明，CBVB-nH（n=0-3）复合物在氮气充足、存在碳和氢的条件下形成是能量上有利的。这归因于带正电的碳取代缺陷（CB）和带负电的氢钝化空位（VB-nH）之间的强静电吸引。这些复合物尤其容易在MOVPE（金属有机气相外延）生长样本中形成。计算的光学性质分析表明，1.90 eV和2.24 eV的发射峰来源于CBVB和CBVB-H复合物捕获空穴，与实验测量结果一致。

Conclusion: CBVB-nH复合物是一种有前景的hBN量子缺陷，其形成机制和光学特性已通过理论计算得到解释，并与实验结果相符。

Abstract: Optically active defects in hexagonal boron nitride (hBN) are promising
candidates for active components in emerging quantum technologies, such as
single-photon emitters and spin centers. However, further progress in hBN-based
quantum technologies requires a deeper understanding of the physics and
chemistry of hBN defects. In this work, we employ ab initio calculations to
investigate the thermodynamic stability and optical properties of defect
complexes involving carbon, boron vacancies, and hydrogen. We demonstrate that
the formation of CBVB-nH complexes (n from 0 to 3) is energetically favorable
under nitrogen-rich conditions in the presence of carbon and hydrogen. The low
formation energies and high binding energies of these complexes arise from the
strong electrostatic attraction between the positively charged carbon
substitutional defect (CB) and the negatively charged hydrogen-passivated boron
vacancies (VB-nH). These complexes are particularly likely to form in
metal-organic vapor-phase epitaxy (MOVPE)-grown samples, where growth occurs in
the presence of carbon and hydrogen and is accompanied by a high density of
boron vacancies. The optical properties of CBVB-nH complexes are analyzed and
compared to recent photoluminescence measurements on MOVPE-grown hBN samples.
In particular, we investigate the origin of the emission peaks at 1.90 eV and
2.24 eV and demonstrate that both the energies and lineshapes are consistent
with hole capture by negatively charged CBVB and CBVB-H complexes.

</details>


### [237] [Turn-on of Current-Induced Spin Torque upon Noncollinear Antiferromagnetic Ordering in Delafossite PdCrO2](https://arxiv.org/abs/2510.14103)
*Xiaoxi Huang,Qi Song,Gautam Gurung,Daniel A. Pharis,Thow Min Jerald Cham,Yulan Chen,Rakshit Jain,Maciej Olszewski,Yufan Feng,Amal El-Ghazaly,Evgeny Y. Tsymbal,Darrell G. Schlom,Daniel C. Ralph*

Main category: cond-mat.mtrl-sci

TL;DR: PdCrO2的自旋扭矩效应在低于其Néel温度时显著增强，这与反铁磁有序有关。


<details>
  <summary>Details</summary>
Motivation: 研究由delafossite反铁磁材料PdCrO2产生的自旋扭矩对相邻铁磁层的影响，并探讨其与温度和反铁磁有序的关系。

Method: 测量PdCrO2产生的自旋扭矩，并进行密度泛函理论计算，对比实验结果。

Result: 自旋扭矩随温度降低而显著增强，尤其是在PdCrO2相变到反铁磁状态时。计算结果与实验现象定性一致。

Conclusion: PdCrO2的反铁磁有序是增强自旋扭矩效应的关键因素，该材料在自旋电子学器件方面具有应用潜力。

Abstract: We report measurements of the current-induced spin torque produced by the
delafossite antiferromagnet PdCrO2 and acting on an adjacent ferromagnetic
permalloy layer. The spin torque increases strongly as the temperature is
reduced through the Neel temperature, when the PdCrO2 transitions from a
paramagnetic phase to a noncollinear antiferromagnetic state. This result is
qualitatively consistent with density functional theory calculations regarding
how spin-current generation changes upon antiferromagnetic ordering in PdCrO2.

</details>


### [238] [A large spin-splitting altermagnet designed from the hydroxylated MBene monolayer](https://arxiv.org/abs/2510.14174)
*Xinyu Yang,Shan-Shan Wang,Shuai Dong*

Main category: cond-mat.mtrl-sci

TL;DR: 通过羟基旋转策略设计具有大自旋劈裂的二维阿尔磁体材料，并实现自旋相关输运特性。


<details>
  <summary>Details</summary>
Motivation: 为推进自旋电子器件技术，开发具有强自旋劈裂的阿尔磁体材料至关重要，尤其是在二维材料领域。

Method: 基于自旋群对称性分析和第一性原理计算，提出了一种新颖的羟基旋转策略来设计阿尔磁体。

Result: 在 $\alpha_{60}$-Mn$_2$B$_2$(OH)$_2$ 单层材料中实现了超过 1130 meV 的大且可逆的自旋劈裂。该系统在无自旋-轨道耦合时表现出节点线半金属的内在特性。羟基的角度可作为开关阿尔磁性的主要序参量，并耦合铁弹性机制。同时，磁晶各向异性也得到了调控。此外，还出现了一种具有 10$^{19}$ $\Omega^{-1}m^{-1}s^{-1}$ 自旋极化电导率的有趣的自旋相关输运特性。

Conclusion: 羟基旋转策略是一种设计阿尔磁性节点线半金属的通用工具，为实现与大自旋劈裂相关的奇异化学和物理特性开辟了新途径。

Abstract: The development of altermagnets is fundamentally important for advancing
spintronic device technology, but remains unpractical for the weak spin
splitting in most cases, especially in two-dimensional materials. Based on spin
group symmetry analysis and first-principles calculations, a novel hydroxyl
rotation strategy in collinear antiferromagnets has been proposed to design
altermagnets. This approach achieves a large chirality-reversible spin
splitting exceeding $1130$ meV in $\alpha_{60}$-Mn$_2$B$_2$(OH)$_2$ monolayer.
The system also exhibits intrinsic features of a node-line semimetal in the
absence of spin-orbit coupling. Besides, the angles of hydroxyl groups serve as
the primary order parameter, which can switch on/off the altermagnetism coupled
with the ferroelastic mechanism. The corresponding magnetocrystalline
anisotropy have also been modulated. Moreover, an interesting spin-related
transport property with the spin-polarized conductivity of 10$^{19}$
$\Omega^{-1}m^{-1}s^{-1}$ also emerges. These findings uncover the hydroxyl
rotation strategy as a versatile tool for designing altermagnetic node-line
semimetals and opening new avenues for achieving exotic chemical and physical
characteristics associated with large spin splitting.

</details>


### [239] [Ferroelasticity tunable altermagnets](https://arxiv.org/abs/2510.14193)
*Ning Ding,Haoshen Ye,Shan-Shan Wang,Shuai Dong*

Main category: cond-mat.mtrl-sci

TL;DR: 在褶皱的五边形CoSe2单层中发现了一种新的铁弹-交变磁性态，其中铁弹应变与自旋分裂直接耦合。


<details>
  <summary>Details</summary>
Motivation: 控制交变磁性状态的研究尚不充分，但交变磁体因其非相对论性自旋分裂和新颖的物理性质而备受关注。

Method: 通过对称性分析和第一性原理计算，识别了褶皱的五边形CoSe2单层中的铁弹d波交变磁性。

Result: 单轴应力可诱导铁弹相变，并伴随自旋分裂带的90度旋转。晶格和Néel矢的协同旋转保持了克尔角的符号，而非协同旋转则反转了它。

Conclusion: 该工作为操纵多铁性系统中的交变磁性提供了一个通用策略，并为探索新兴的磁弹性现象开辟了新途径。

Abstract: Altermagnets have garnered great interest due to their non-relativistic spin
splitting and novel physical properties. However, the control of altermagnetic
states remains underexplored. Here, we propose a new multiferroic state, i.e.
ferroelastic altermagnetic state, in which ferroelastic strain couples directly
to the spin-splitting. Through symmetry analysis and first-principles
calculations, we identify the ferroelastic $d$-wave altermagnetism of puckered
pentagonal CoSe$_2$ monolayer. Interestingly, uniaxial stress can induce a
ferroelastic phase transition, accompanied by a $90^\circ$ rotation of the
spin-splitting bands. Cooperative rotation of the lattice and N\'eel vectors
preserves the sign of Kerr angle, whereas noncooperative rotation reverses it.
Our work provides a general strategy for manipulating altermagnetism in
multiferroic systems and opens new avenues for exploring emergent
magnetoelastic phenomena.

</details>


### [240] [Comparison of Electroluminescence and Photoluminescence Imaging of Mixed-Cation Mixed-Halide Perovskite Solar Cells at Low Temperatures](https://arxiv.org/abs/2510.14213)
*Hurriyet Yuce-Cakir,Haoran Chen,Isaac Ogunniranye,Susanna M. Thon,Yanfa Yan,Zhaoning Song,Behrang H. Hamadani*

Main category: cond-mat.mtrl-sci

TL;DR: 研究重点是混合阳离子、混合卤素钙钛矿太阳能电池在不同温度下的光电特性，特别是低温下的表现。


<details>
  <summary>Details</summary>
Motivation: 深入理解钙钛矿太阳能电池在低温下的性能，为太空能源系统和先进半导体器件的应用奠定基础。

Method: 采用电致发光（EL）和光致发光（PL）高光谱成像技术，结合电流-电压分析，研究了混合阳离子、混合卤素钙钛矿太阳能电池随温度变化的光电特性。

Result: 低温（240 K以下）EL ERE图显示出局部电荷注入和提取瓶颈是异质性的主要来源；PL ERE图则显示非辐射复合被抑制，效率在整个温度范围内都有显著提高。钙钛矿层在低温下ERE增强，但界面处的电荷注入势垒在240 K以下严重抑制了EL并降低了填充因子。

Conclusion: 虽然钙钛矿层在低温下表现出更高的ERE，但界面处的电荷注入势垒会严重影响器件的整体性能。因此，需要更深入地理解钙钛矿太阳能电池在低温下的行为，以实现其在太空能源系统等领域的应用。

Abstract: Halide perovskites have emerged as promising candidates for high-performance
solar cells. This study investigates the temperature-dependent optoelectronic
properties of mixed-cation mixed-halide perovskite solar cells using
electroluminescence (EL) and photoluminescence (PL) hyperspectral imaging,
along with current-voltage analysis. Luminescence images, which were converted
to EL and PL external radiative efficiency (ERE) maps, revealed significant
changes in the optoelectronic behavior of these devices at low temperatures.
Specifically, we found that a significant source of heterogeneity in the
low-temperature EL ERE maps below 240 K is related to local charge injection
and extraction bottlenecks, whereas PL ERE maps show suppressed non-radiative
recombination and significant improvements in efficiency throughout the
investigated temperature range. The spatial distribution of ERE and its
variation with applied current were analyzed, offering insights into
charge-carrier dynamics and defect behavior. Our results reveal that while the
perovskite layer exhibits enhanced ERE at low temperatures, charge injection
barriers at the interfaces of the perovskite solar cells significantly suppress
EL and degrade the fill factor below 240 K. These findings reveal that a deeper
understanding of the performance of perovskite solar cells under
low-temperature conditions is an essential step toward their potential
application in space power systems and advanced semiconductor devices.

</details>


### [241] [Magnetization, excitations, and microwave power absorption in transition-metal/rare-earth ferrites with disorder](https://arxiv.org/abs/2510.14237)
*D. A. Garanin,E. M. Chudnovsky*

Main category: cond-mat.mtrl-sci

TL;DR: 该论文开发了高效的数值程序，用于研究铁氧体材料中平衡磁态、激发和微波功率吸收与温度和成分（特别是稀土原子浓度）的依赖关系，特别关注了磁矩补偿点和角动量补偿点附近的磁态翻转现象。


<details>
  <summary>Details</summary>
Motivation: 研究过渡金属/稀土铁氧体材料中平衡磁态、激发和微波功率吸收与温度和成分（包括稀土原子浓度）的依赖关系，以及磁矩补偿点和角动量补偿点附近的磁态翻转现象。

Method: 开发高效的数值程序，利用磁化相关函数计算主导的均匀振荡模式，并与解析解进行比较。使用涨落耗散定理计算吸收的微波功率与频率的依赖关系。研究了稀土原子位置无序对磁化行为的影响。

Result: 计算出的均匀振荡模式与解析解吻合良好。数值结果与解析结果一致。稀土原子位置无序导致了多个局域模式，这些模式随着系统尺寸的增加收敛为宽吸收峰。功率吸收积分在补偿点处呈现最小值。

Conclusion: 该研究通过数值方法有效地模拟了铁氧体材料的磁行为，并与解析结果进行了验证，揭示了温度、成分和稀土原子无序对材料磁特性的显著影响。

Abstract: Efficient numerical routines are developed for numerical studies of the
dependence of the equilibrium magnetic states, excitations, and microwave power
absorption on temperature and composition in transition-metal/rare-earth
ferrites, including the reversal of the N\'eel vector occurring on both
temperature and the concentration of the rare-earth atoms. It results in a
drastic change in the behavior at the magnetization and angular-momentum
compensation points. Dominant uniform oscillation modes are obtained by
computing the magnetization correlation function. They are compared with the
analytical solution, which is analyzed in detail. The fluctuation-dissipation
theorem is used to compute the frequency dependence of the absorbed microwave
power. A good agreement with analytical results is demonstrated. Disorder
caused by random positions of rare-earth atoms in a diluted RE system leads to
multiple localized modes that converge into broad absorption maxima as the size
of the system increases. The power absorption integrated over frequency
exhibits a minimum at the compensation point.

</details>


### [242] [Laser-Induced Heating in Diamonds: Influence of Substrate Thermal Conductivity and Interfacial Polymer Layers](https://arxiv.org/abs/2510.14372)
*Md Shakhawath Hossain,Jiatong Xu,Thi Ngoc Anh Mai,Nhat Minh Nguyen,Trung Vuong Doan,Chaohao Chen,Qian Peter Su,Yongliang Chen,Evgeny Ekimov,Toan Dinh,Xiaoxue Xu,Toan Trong Tran*

Main category: cond-mat.mtrl-sci

TL;DR: 在低导热基板和聚合物层存在的情况下，钻石在激光照射下会显著升温，这与通常认为钻石导热性高的观点相反。


<details>
  <summary>Details</summary>
Motivation: 研究激光加热对安装在不同导热基板上的金刚石（特别是在存在聚合物层的情况下）的影响，因为先前对这种局部加热效应的关注很少，但它在高导热性材料中仍然可能发生。

Method: 系统地研究了硅-空位金刚石在不同导热基板和不同厚度聚合物界面层上的激光诱导加热效应，并使用稳态三维传热模型通过 COMSOL Multiphysics 模拟进行验证。

Result: 研究表明，即使在较低的激光功率下，低导热基板（如非晶多孔碳）也会导致金刚石显著升温。玻璃和 PDMS 基板在较高功率下也会出现明显升温。即使是很薄的聚合物层（2.2 微米）也会在较高功率下显著增强加热效应。

Conclusion: 基板的导热性和聚合物层的厚度都会显著影响金刚石的局部激光加热效应。这项研究为选择合适的基板和制备样品提供了指导，以优化光学测温和量子传感应用中的条件。

Abstract: Diamonds hosting color centers possess intrinsically high thermal
conductivity; therefore, laser-induced heating has often received little
attention. However, when placed on substrates with low thermal conductivity,
localized heating of diamonds under laser excitation can become significant,
and the presence of an interfacial polymer layer between substrate and diamond
further amplifies this effect. Yet, the relationship between substrate thermal
conductivity, polymer thickness, and laser heating remains to be established.
Here, a systematic investigation is presented on laser-induced heating of
silicon-vacancy diamond on substrates with varying thermal conductivity and
interfacial polymer thickness. Results reveal that even at a low excitation
power of 737~$\mu$W/$\mu$m$^2$, thin amorphous holey carbon -- the
lowest-conductivity substrate ($\sim$0.2~W~m$^{-1}$~K$^{-1}$) studied --
exhibits substantial heating, while glass ($\sim$1.4~W~m$^{-1}$~K$^{-1}$) and
polydimethylsiloxane (PDMS, $\sim$0.35~W~m$^{-1}$~K$^{-1}$) show noticeable
heating only above 2.95~mW/$\mu$m$^2$. For polymer interlayers, a thickness of
just 2.2~$\mu$m induces significant heating at 2.95~mW/$\mu$m$^2$ and above,
highlighting strong influence of both substrate and polymer thickness on local
heating response. Experimental findings are further validated using COMSOL
Multiphysics simulations with a steady-state 3D heat transfer model. These
results provide practical guidance for substrate selection and sample
preparation, enabling optimization of conditions for optical thermometry and
quantum sensing applications.

</details>


### [243] [Multiscale Models For Perovskite Optimisation](https://arxiv.org/abs/2510.14396)
*Philippe Baranek,James P. Connolly,Antoine Gissler,Philip Schulz,Michel Rérat,Roberto Dovesi*

Main category: cond-mat.mtrl-sci

TL;DR: 本论文提出了一种多尺度方法来评估钙钛太阳能电池的性能，该方法在原子尺度上使用第一性原理计算确定材料性质，并将其应用于宏观器件模型。


<details>
  <summary>Details</summary>
Motivation: 研究 MAPbI3（MA = CH3NH3）钙钛及其相变对其光学、电子和结构性质的影响，并将其应用于器件模型中，以期为设计和优化钙钛材料提供方法。

Method: 使用第一性原理计算研究材料性质，并将其与数值漂移扩散器件模型耦合，采用混合交换-关联泛函，并与实验数据进行验证。

Result: 开发了一种将原子尺度和器件模型耦合的框架，实现了光学、振动和电子参数在两个尺度之间的交换，并验证了该方法的预测能力。

Conclusion: 提出了一种设计和优化钙钛材料以提高电池性能和稳定性的多尺度方法，为钙钛太阳能电池的社会应用扫清了主要障碍。

Abstract: This paper presents a multiscale approach to evaluate perovskite solar cell
performance which determines material properties at the atomistic scale with
first-principles calculations, and applies them in macro-scale device models.
This work focuses on the MAPbI3 (MA = CH3NH3) perovskite and how its phase
transitions impact on its optical, electronic, and structural properties which
are investigated at the first-principles level. The obtained data are coupled
to a numerical drift-diffusion device model enabling evaluation of the
performance of corresponding single junction devices. The first-principles
simulation applies a hybrid exchange-correlation functional adapted to the
studied family of compounds. Validation by available experimental data is
presented from materials properties to device performance, justifying the use
of the approach for predictive evaluation of existing and novel perovskites.
The coupling between atomistic and device models is described in terms of a
framework for exchange of optical, vibrational, and electronic parameters
between the two scales. The result of this theoretical investigation is a
methodology for designing and optimising perovskite materials for both cell
performance and stability, the key obstacle in the societal implementation of
these record-breaking new materials.

</details>


### [244] [First-Principles Approach to Spin Excitations in Noncollinear Magnetic Systems](https://arxiv.org/abs/2510.14405)
*Hsiao-Yi Chen,Ryotaro Arita,Yusuke Nomura*

Main category: cond-mat.mtrl-sci

TL;DR: We present a first-principles method for computing spin excitations in noncollinear magnetic systems, extending existing ab initio frameworks. Our approach, which incorporates a Wannier-basis representation and an ansatz potential method, successfully captures the spin-spiral state of LiCu2O2, including its spin-rotation pitch and magnon dispersion, in agreement with experimental measurements. The method also elucidates the role of magnetic dipoles and predicts magnon dispersion for helical spin backgrounds. This work establishes an efficient and general framework for simulating collective spin dynamics in noncollinear magnetic systems.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for studying magnetic excitations rely on localized spin models. While recent ab initio Green's function methods have emerged, their application has been limited to collinear magnetic systems. This work extends these methods to handle large-scale noncollinear magnetic systems, which are common in many materials.

Method: The method combines density functional theory and many-body perturbation theory. It extends existing Green's function frameworks by using a Wannier-basis representation and an ansatz potential method to reduce computational cost, enabling the treatment of large-scale noncollinear magnetic systems.

Result: The method successfully captures the spin-spiral state of LiCu2O2, including its spin-rotation pitch and magnon dispersion, in agreement with experimental measurements. It also elucidates the interplay between spin structure and spin-exchange splitting, and the role of magnetic dipoles in mediating ferromagnetic interactions. A theoretical prediction of magnon dispersion on a helical spin background is also provided, showing high agreement with experimental measurements.

Conclusion: This work establishes a general and computationally efficient framework for simulating collective spin dynamics in noncollinear magnetic systems from first principles, with successful application to spin-spiral states as exemplified by LiCu2O2.

Abstract: We present a first-principles method based on density functional theory and
many-body perturbation theory for computing spin excitations in magnetic
systems with noncollinear spin textures. Traditionally, the study of magnetic
excitations has relied on spin models that assume magnetic moments to be
localized. Beyond this restriction, recent $ab~initio$ methods based on Green's
functions within the local spin-density approximation have emerged as a general
framework for calculating magnetic susceptibilities. However, their application
has so far been largely limited to collinear ferromagnetic and
antiferromagnetic systems. In this work, we extend this framework and enable
the treatment of large-scale noncollinear magnetic systems by leveraging a
Wannier-basis representation and implementing an ansatz potential method to
reduce computational cost. We apply our method to the spin-spiral state of
LiCu$_2$O$_2$, successfully capturing its steady-state spin-rotation pitch in
agreement with the experimental measurement and resolving the characteristic
magnon dispersion. We further analyze the interplay between the spiral spin
structure and the on-site spin-exchange splitting, and elucidate the crucial
role of magnetic dipoles on ligand ions in mediating effective ferromagnetic
interaction among the primary spins on Cu$^{2+}$ ions. Finally, we provide a
theoretical prediction of the magnon dispersion on top of the helical spin
background in high agreement with the experimental measurement. Overall, this
work establishes a general and computationally efficient framework for
simulating collective spin dynamics in noncollinear magnetic systems from first
principles, exemplified by -- but not limited to -- spin-spiral states.

</details>


### [245] [Ferroelectric amplitude switching and continuous memory](https://arxiv.org/abs/2510.14491)
*Gye-Hyeon Kim,Tae Hyun Jung,Seungjoon Sun,Jung Kyu Lee,Jaewoo Han,P. Karuna Kumari,Jin-Hyun Choi,Hansol Lee,Tae Heon Kim,Yoon Seok Oh,Seung Chul Chae,Se Young Park,Sang Mo Yang,Changhee Sohn*

Main category: cond-mat.mtrl-sci

TL;DR: 铁电材料 Ba1-xSrx 异质结构表现出内禀的二元切换行为，但为了实现连续内存状态，本文展示了介观尺度的铁电幅度切换，无需改变极化方向即可连续调制极化幅度。


<details>
  <summary>Details</summary>
Motivation: 最近开发的模拟记忆设备对实现连续记忆状态的铁电材料表现出越来越大的兴趣。

Method: 通过切换电流测量、压电响应显微镜和 Landau-Ginzburg-Devonshire 模拟来揭示具有平坦最小值双势阱的成分梯度铁电异质结构具有幅度切换行为。

Result: 成分梯度铁电异质结构表现出幅度切换行为，支持稳定的、连续的极化状态。

Conclusion: 幅度切换是一种新的有序参数动力学，为节能高效且可靠的模拟内存系统奠定了新的平台。

Abstract: Although ferroelectric systems inherently exhibit binary switching behavior,
recent advances in analog memory device have spurred growing interest in
achieving continuous memory states. In this work, we demonstrate ferroelectric
amplitude switching at the mesoscopic scale in compositionally graded
Ba1-xSrxTiO3 heterostructures, enabling continuous modulation of polarization
magnitude without altering its direction, which we defined as amplitude
switching. Using switching current measurement, piezoresponse force microscopy
and Landau-Ginzburg-Devonshire simulations, we reveal that compositionally
graded ferroelectric heterostructure can possess amplitude switching behavior
through a double well potential with flattened minima. This behavior supports
stable, continuous polarization states and establishes a new platform for
analog memory applications. These findings introduce amplitude switching as a
new dynamic of the order parameter, paving the way for energy-efficient and
reliable analog memory systems.

</details>


### [246] [Enhanced Secondary Electron Detection of Single Ion Implants in Silicon Through Thin SiO2 Layers](https://arxiv.org/abs/2510.14495)
*Ella B Schneider,Oscar G Lloyd-Willard,Kristian Stockbridge,Mark Ludlow,Sam Eserin,Luke Antwis,David C Cox,Roger P Webb,Ben N Murdin,Steve K Clowes*

Main category: cond-mat.mtrl-sci

TL;DR: 通过二次电子探测技术，我们实现了一种非破坏性的高效率单离子注入探测方法，空间分辨率达到30纳米，可用于可扩展量子器件的精确剂量调控。


<details>
  <summary>Details</summary>
Motivation: 可扩展量子器件需要对单掺杂剂进行确定性放置，而现有的方法存在局限性。

Method: 使用聚焦离子束（FIB）系统，通过二次电子（SE）信号来探测单个离子的注入事件。通过引入SiO2覆盖层来提高SE产额，并研究了SE产额与离子速度、质量的关系。

Result: 实现了高达98%的单离子探测效率，空间分辨率约为30纳米，且无需制备电学接触或器件。该方法对不同质量的离子（如Sb, Yb, Bi等）均适用，但需要调整能量以维持探测效率。

Conclusion: 所提出的方法为精确放置供体提供了一条稳健且可扩展的途径，并将确定性注入策略扩展到更广泛的材料系统和量子器件架构。

Abstract: Deterministic placement of single dopants is essential for scalable quantum
devices based on group-V donors in silicon. We demonstrate a non-destructive,
high-efficiency method for detecting individual ion implantation events using
secondary electrons (SEs) in a focused ion beam (FIB) system. Using low-energy
Sb ions implanted into undoped silicon, we achieve up to 98% single-ion
detection efficiency, verified by calibrated ion-current measurements before
and after implantation. The technique attains ~30 nm spatial resolution without
requiring electrical contacts or device fabrication, in contrast to
ion-beam-induced-current (IBIC) methods. We find that introducing a controlled
SiO2 capping layer significantly enhances SE yield, consistent with an
increased electron mean free path in the oxide, while maintaining high
probability of successful ion deposition in the underlying substrate. The yield
appears to scale with ion velocity, so higher projectile mass (e.g. Yb, Bi etc)
requires increased energy to maintain detection efficiency. Our approach
provides a robust and scalable route to precise donor placement and extends
deterministic implantation strategies to a broad range of material systems and
quantum device architectures.

</details>


### [247] [Uniaxial Magnetic Anisotropy and Type-X/Y Current-Induced Magnetization Switching in Oblique-Angle-Deposited Ta/CoFeB/Pt and W/CoFeB/Pt Heterostructures](https://arxiv.org/abs/2510.14540)
*Amir Khan,Shalini Sharma,Tiago de Oliveira Schneider,Markus Meinert*

Main category: cond-mat.mtrl-sci

TL;DR: 通过(Ta or W)/CoFeB/Pt三层异质结构，实现了平面电流诱导的磁化翻转（CIMS），并达到了低至 $2 	imes 10^{11}$ A/m$^2$ 的翻转电流密度。


<details>
  <summary>Details</summary>
Motivation: 为了提高自旋-轨道矩（SOT）效率，采用(Ta or W)/CoFeB/Pt三层异质结构。

Method: 制备了(Ta or W)/CoFeB/Pt三层异质结构，并通过霍尔条形器件中的单向自旋霍尔磁电阻（USMR）和平面霍尔效应（PHE）测量了磁化翻转。利用宏自旋模拟研究了翻转机制。

Result: 在(Ta or W)/CoFeB/Pt三层异质结构中实现了CIMS，翻转时间短至亚微秒。其中，W (4 nm)/CoFeB (1.4 nm)/Pt (2 nm)器件的翻转电流密度低至 $2 	imes 10^{11}$ A/m$^2$。宏自旋模拟成功复现了Y型几何结构的翻转过程，但X型几何结构的翻转电流低于模拟预测值，表明该结构中翻转由成核和畴壁传播主导。

Conclusion: 通过(Ta or W)/CoFeB/Pt三层异质结构可以实现高效的CIMS，并且翻转机制与器件结构相关。Y型几何结构主要通过宏自旋相干旋转实现翻转，而X型几何结构则通过成核和畴壁传播实现翻转。

Abstract: Planar current-induced magnetization switching (CIMS) driven by spin-orbit
torque (SOT) requires an in-plane uniaxial magnetic anisotropy (UMA), which can
be induced by oblique-angle sputter deposition of the heavy-metal underlayer.
To enhance the SOT efficiency, we employ trilayer heterostructures of (Ta or
W)/CoFeB/Pt, where the bottom layer exhibits a UMA of 50 mT at 2 nm thickness.
The magnetization reversal in Hall-bar devices is detected through
unidirectional spin Hall magnetoresistance (USMR) for the Type Y geometry (easy
axis transverse to current) and planar Hall measurements for the Type X
geometry (easy axis parallel to current). Both configurations exhibit CIMS with
sub-microsecond current pulses, reaching switching current densities as low as
$2 \times 10^{11}$ A/m$^2$ for a W (4 nm)/CoFeB (1.4 nm)/Pt (2 nm) stack with a
UMA of 146 mT. Macrospin simulations reproduce the Type Y switching as coherent
magnetization rotation, whereas the Type X devices switch at much lower
currents than predicted, indicating that nucleation and domain-wall propagation
dominate reversal in this geometry.

</details>


### [248] [Contrasting properties of free carriers in $n$- and $p$-type Sb$_2$Se$_3$](https://arxiv.org/abs/2510.14554)
*F. Herklotz,E. V. Lavrov,T. D. C. Hobson,T. P. Shalvey,J. D. Major,K. Durose*

Main category: cond-mat.mtrl-sci

TL;DR: Sb2Se3单晶在低温下表现出持久的光电导性，掺杂Cd或Zn后，在光照停止数小时后电导率仍能保持，表明其载流子动力学存在根本性不对称性，可能与极化子效应有关。


<details>
  <summary>Details</summary>
Motivation: 探究p型Sb2Se3单晶中的持久光电导现象，并与n型Sb2Se3进行比较，以理解载流子动力学的不对称性。

Method: 对掺杂Cd或Zn的p型Sb2Se3单晶和n型Cl掺杂Sb2Se3单晶进行了电输运和红外吸收测量。

Result: p型Sb2Se3单晶在低温（~25K以下）下表现出持久光电导性，光照停止数小时后电导率仍可保持。研究发现， Sb2Se3中的空穴传输比电子传输更容易受到本征载流子散射的影响。

Conclusion: Sb2Se3的载流子动力学存在根本性不对称性，其中极化子效应对限制空穴迁移率起着重要作用。

Abstract: We report persistent photoconductivity in $p$-type Sb$_2$Se$_3$ single
crystals doped with Cd or Zn, where enhanced conductivity remains for hours
after illumination ceases at temperatures below $\sim$25~K. Comparative
transport and infrared absorption measurements, including on $n$-type Cl-doped
counterparts, reveal strong indications that hole transport in Sb$_2$Se$_3$ is
more strongly affected by intrinsic carrier scattering than electron transport.
These results point to a fundamental asymmetry in charge carrier dynamics and
highlight the potential role of polaronic effects in limiting hole mobility in
this quasi-one-dimensional semiconductor.

</details>


### [249] [Orbital magnetization in Sierpinski fractals](https://arxiv.org/abs/2510.14556)
*L. L. Lage,T. P. Cysne,A. Latgé*

Main category: cond-mat.mtrl-sci

TL;DR: Sierpinski carpet and triangle fractals exhibit distinct orbital magnetization behaviors due to their unique geometries, with potential applications in orbitronics.


<details>
  <summary>Details</summary>
Motivation: Investigate orbital magnetization in Sierpinski carpet (SC) and triangle (ST) fractals using the Haldane model to understand the effects of quantum confinement on electronic orbital angular momentum.

Method: Calculate orbital magnetization using both the definition and local markers formalism for SC and ST fractals. Analyze the resulting magnetization profiles as a function of chemical potential.

Result: For SC, higher fractal generations lead to dense edge states and oscillations in magnetization. For ST, fractal-induced spectral gaps create plateaus in magnetization, with sensitivity to edge terminations. Both calculation methods yield consistent results.

Conclusion: Quantum confinement in fractal structures significantly affects electronic orbital angular momentum. SC and ST fractals show distinct magnetization behaviors, suggesting potential for novel orbitronics applications in complex geometries.

Abstract: Orbital magnetization (OM) in Sierpinski carpet (SC) and triangle (ST)
fractal is theoretically investigated by using Haldane model as a prototypical
example. The OM calculation is performed following two distinct approaches;
employing the definition and local markers formalism. Both methods coincides
for all systems analyzed. For the SC, higher fractal generations create a dense
set of edge states, resulting in a staircase profile, leading to oscillations
in the magnetization as a function of the chemical potential. In contrast, the
ST self-similarity produces distinct fractal-induced spectral gaps, which
manifest as constant plateaus in the magnetization. The STs exhibit a
pronounced sensitivity to edge terminations. Our results reveal how quantum
confinement in fractal structures affects the electronic orbital angular
momentum, pointing to possible pathways for exploring novel orbitronics in
systems with complex geometries.

</details>


### [250] [Substitutional sulfur and its vibrational fingerprints in Sb$_2$Se$_3$](https://arxiv.org/abs/2510.14590)
*F. Herklotz,E. V. Lavrov,A. Herklotz,V. V. Melnikov,T. P. Shalvey,J. D. Major,and K. Durose*

Main category: cond-mat.mtrl-sci

TL;DR: 硫在Sb2Se3中的结构行为


<details>
  <summary>Details</summary>
Motivation: 结合红外吸收光谱和密度泛函理论研究硫在Sb2Se3中的结构行为。

Method: 通过红外吸收光谱和密度泛函理论，识别了硫相关的局域振动模式，并通过偏振测量和理论计算，将光谱特征与硫取代硒位点相关联。

Result: 识别了249、273、283和312 cm$^{-1}$处的四种硫相关局域振动模式，并证实这些模式与Sb2Se3中三个不等价的硒位点上的取代硫相关。

Conclusion: 观察到的光谱特征与硫取代Sb2Se3中的硒位点完全一致。

Abstract: The configurational behavior of sulfur in antimony triselenide (Sb$_2$Se$_3$)
is investigated by combining infrared absorption spectroscopy with density
functional theory. Four sulfur-related local vibrational modes are identified
at 249, 273, 283, and 312~cm$^{-1}$ in melt-grown single crystals prepared from
Sb$_2$Se$_3$ granulate. Their assignment to sulfur is confirmed through
controlled indiffusion experiments using Sb$_2$S$_3$ and elemental sulfur, as
well as isotope-substitution studies with $^{34}$S, which produce the expected
frequency shifts. Polarization-resolved measurements, together with theoretical
calculations of local vibrational modes, demonstrate that the observed spectral
features are fully consistent with substitutional sulfur on the three
inequivalent selenium sites of Sb$_2$Se$_3$.

</details>


### [251] [Unique Hierarchical Rotational Dynamics Induces Ultralow Lattice Thermal Conductivity in Cyanide-bridged Framework Materials](https://arxiv.org/abs/2510.14692)
*Zhunyun Tang,Xiaoxia Wang,Jin Li,Chaoyu He,Mingxing Chen,Chao Tang,Tao Ouyang*

Main category: cond-mat.mtrl-sci

TL;DR: CFM材料通过结合层级振动和旋转动力学，实现了超低晶格热导率，为开发轻质低热导材料提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 开发结合轻质和超低晶格热导率的材料对于热电和热障涂层技术至关重要，但一直面临挑战。

Method: 通过结合超原位结构中的层级振动和钙钛矿中的旋转动力学，实现了CFM材料的超低晶格热导率。

Result: CFM材料的晶格热导率比具有同等平均原子质量的已知钙钛矿材料降低了一到两个数量级，室温下范围为0.35至0.81 W/mK。

Conclusion: CFM材料是研究极端声子非谐性的新平台，并通过结合层级和旋转动力学为实现轻质材料的超低热导率提供了新范式。

Abstract: The pursuit of materials combining light constituent elements with ultralow
lattice thermal conductivity ($\kappa_{\mathrm{L}}$) is crucial to advancing
technologies like thermoelectrics and thermal barrier coatings, yet it remains
a formidable challenge to date. Herein, we achieve ultralow
$\kappa_{\mathrm{L}}$ in lightweight cyanide-bridged framework materials (CFMs)
through the rational integration of properties such as the hierarchical
vibrations exhibited in superatomic structures and rotational dynamics
exhibited in perovskites. Unique hierarchical rotation behavior leads to
multiple negative peaks in Gr\"uneisen parameters across a wide frequency
range, thereby inducing pronounced negative thermal expansion and strong cubic
anharmonicity in CFMs. Meanwhile, the synergistic effect between large
four-phonon scattering phase space (induced by phonon quasi-flat bands and wide
bandgaps) and strong quartic anharmonicity (associated with rotation modes)
leads to giant quartic anharmonic scattering rates in these materials.
Consequently, the $\kappa_{\mathrm{L}}$ of these CFMs decreases by one to two
orders of magnitude compared to the known perovskites or perovskite-like
materials with equivalent average atomic masses. For instance, the
Cd(CN)$_{2}$, NaB(CN)$_{4}$, LiIn(CN)$_{4}$, and AgX(CN)$_{4}$ (X = B, Al, Ga,
In) exhibit ultralow room-temperature $\kappa_{\mathrm{L}}$ values ranging from
0.35 to 0.81 W/mK. This work not only establishes CFMs as a novel and rich
platform for studying extreme phonon anharmonicity, but also provides a new
paradigm for achieving ultralow thermal conductivity in lightweight materials
via the conscious integration of hierarchical and rotational dynamics.

</details>


### [252] [Identification of formation of amorphous Si phase in SiOxNy films produced by plasma enhanced chemical vapor deposition](https://arxiv.org/abs/2510.14701)
*M. V. Voitovych,A. Sarikov,V. O. Yukhymchuk,V. V. Voitovych,M. O. Semenenko*

Main category: cond-mat.mtrl-sci

TL;DR: 非晶硅（a-Si）在富硅硅氧氮化物薄膜中的形成具有特殊性，通过拉曼光谱和红外吸收光谱研究发现，当硅含量超过阈值（约0.4）时，a-Si会形成，且其含量与薄膜中的氢浓度相关。通过分析红外光谱中约660 cm-1处的吸收峰，可以识别a-Si相，表明红外光谱是一种有效的分析方法。


<details>
  <summary>Details</summary>
Motivation: 研究等离子体增强化学气相沉积（PECVD）生长的富硅硅氧氮化物薄膜中非晶硅（a-Si）相的形成特点。

Method: 结合拉曼散射和红外吸收光谱，特别是通过开发用于分离和识别约660 cm-1处红外吸收峰（源自a-Si相）的红外光谱分析方法。

Result: 拉曼散射证实当硅含量超过约0.4的阈值时存在a-Si相。红外光谱分析（特别是约660 cm-1处的吸收峰）与薄膜中的氢浓度相关，并能有效识别a-Si相。

Conclusion: 红外光谱分析（特别是光谱的低波数部分）是一种有效识别富硅硅氧氮化物薄膜相组成的方法，有助于理解PECVD生长硅氧氮化物薄膜相组成的规律，并控制薄膜特性以实现实际应用。

Abstract: Peculiarities of formation of inclusions of amorphous Si (a-Si) phase in
Si-rich Si oxynitride films grown by plasma-enhanced chemical vapor deposition
(PECVD) are studied by combined Raman scattering and infrared (IR) absorption
spectroscopy. The Raman scattering results identify presence of a-Si phase in
the studied films at the relative Si content exceeding a threshold value of
about 0.4. The a-Si amount correlates with the concentration of hydrogen in the
films, the presence of which is detected by characteristic IR absorption bands
corresponding to Si-H bending (about 660 cm-1) and stretching (a composite band
in the range of about 1900-2400 cm-1) vibrations. The method of deconvolution
of IR absorbance spectra in the range of about 600 to 1300 cm-1 developed
earlier is used to reliably separate the IR band at about 660 cm-1. This band
is identified to origin from the amorphous Si phase within the studied Si
oxynitride films. This makes it possible to propose IR spectroscopy with
analysis of the low-wavenumber part of the spectra as an efficient method of
identifying phase composition of Si-rich Si oxynitride films. The obtained
results contribute to understanding of the regularities of formation of phase
compositions of PECVD grown Si oxynitride films and are useful for controlling
the films properties for practical applications.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [253] [Spiking Neural Network Architecture Search: A Survey](https://arxiv.org/abs/2510.14235)
*Kama Svoboda,Tosiron Adegbija*

Main category: cs.NE

TL;DR: 本综述从硬件/软件协同设计的角度全面考察了脉冲神经网络（SNN）架构搜索（SNNaS）。


<details>
  <summary>Details</summary>
Motivation: SNN因其在能效和实时资源受限处理方面的优势，在神经形态计算领域备受关注，但SNN架构设计面临复杂性和硬件约束与模型相互作用的挑战。

Method: 概述了SNN及其与ANN的区别，回顾了ANN的NAS技术及其在SNN上的局限性，并调研了SNN的NAS方法。

Result: 该调研提供了SNNaS的现状，并强调了硬件/软件协同设计在发挥SNN全部潜力方面的重要性。

Conclusion: 未来的研究应侧重于硬件/软件协同设计，以充分挖掘SNN在神经形态计算中的潜力。

Abstract: This survey paper presents a comprehensive examination of Spiking Neural
Network (SNN) architecture search (SNNaS) from a unique hardware/software
co-design perspective. SNNs, inspired by biological neurons, have emerged as a
promising approach to neuromorphic computing. They offer significant advantages
in terms of power efficiency and real-time resource-constrained processing,
making them ideal for edge computing and IoT applications. However, designing
optimal SNN architectures poses significant challenges, due to their inherent
complexity (e.g., with respect to training) and the interplay between hardware
constraints and SNN models. We begin by providing an overview of SNNs,
emphasizing their operational principles and key distinctions from traditional
artificial neural networks (ANNs). We then provide a brief overview of the
state of the art in NAS for ANNs, highlighting the challenges of directly
applying these approaches to SNNs. We then survey the state-of-the-art in
SNN-specific NAS approaches. Finally, we conclude with insights into future
research directions for SNN research, emphasizing the potential of
hardware/software co-design in unlocking the full capabilities of SNNs. This
survey aims to serve as a valuable resource for researchers and practitioners
in the field, offering a holistic view of SNNaS and underscoring the importance
of a co-design approach to harness the true potential of neuromorphic
computing.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [254] [DIAMOND: Systolic Array Acceleration of Sparse Matrix Multiplication for Quantum Simulation](https://arxiv.org/abs/2510.14172)
*Yuchao Su,Srikar Chundury,Jiajia Li,Frank Mueller*

Main category: cs.AR

TL;DR: 该论文提出了一种名为“\name”的、针对量子计算中汉密尔顿模拟工作负载进行优化的加速器，该加速器能够处理因希尔伯特空间维度随量子比特数呈指数增长而带来的计算挑战。


<details>
  <summary>Details</summary>
Motivation: 汉密尔顿模拟在量子计算中至关重要，但面临希尔伯特空间维度指数增长带来的计算挑战，特别是矩阵指数计算成本高昂。现有加速器主要针对机器学习，其稀疏性模式与汉密尔顿模拟不同。

Method: 提出了一种名为“\name”的、首个针对量子模拟中常见的对角线稀疏性进行优化的加速器。该加速器利用了汉密尔顿矩阵的对角线结构，并通过重构的收缩阵列数据流将稀疏矩阵转化为稠密计算，以提高利用率和性能。

Result: 在HamLib的基准测试中，“\name”加速器相对于SIGMA、Outer Product和Gustavson算法，平均性能分别提高了10.26倍、33.58倍和53.15倍，峰值加速可达127.03倍。同时，与SIGMA相比，能耗平均降低了471.55倍，最高可达4630.58倍。

Conclusion: “\name”加速器是第一个针对量子模拟中汉密尔顿矩阵的对角线稀疏性进行优化的专用硬件，通过利用其结构特性，可以显著提升计算性能并降低能耗，为经典汉密尔顿模拟提供了高效的解决方案。

Abstract: Hamiltonian simulation is a key workload in quantum computing, enabling the
study of complex quantum systems and serving as a critical tool for classical
verification of quantum devices. However, it is computationally challenging
because the Hilbert space dimension grows exponentially with the number of
qubits. The growing dimensions make matrix exponentiation, the key kernel in
Hamiltonian simulations, increasingly expensive. Matrix exponentiation is
typically approximated by the Taylor series, which contains a series of matrix
multiplications. Since Hermitian operators are often sparse, sparse matrix
multiplication accelerators are essential for improving the scalability of
classical Hamiltonian simulation. Yet, existing accelerators are primarily
designed for machine learning workloads and tuned to their characteristic
sparsity patterns, which differ fundamentally from those in Hamiltonian
simulations that are often dominated by structured diagonals.
  In this work, we present \name, the first diagonal-optimized quantum
simulation accelerator. It exploits the diagonal structure commonly found in
problem-Hamiltonian (Hermitian) matrices and leverages a restructured systolic
array dataflow to transform diagonally sparse matrices into dense computations,
enabling high utilization and performance. Through detailed cycle-level
simulation of diverse benchmarks in HamLib, \name{} demonstrates average
performance improvements of $10.26\times$, $33.58\times$, and $53.15\times$
over SIGMA, Outer Product, and Gustavson's algorithm, respectively, with peak
speedups up to $127.03\times$ while reducing energy consumption by an average
of $471.55\times$ and up to $4630.58\times$ compared to SIGMA.

</details>


### [255] [Computing-In-Memory Aware Model Adaption For Edge Devices](https://arxiv.org/abs/2510.14379)
*Ming-Han Lin,Tian-Sheuan Chang*

Main category: cs.AR

TL;DR: 本文提出了一种两阶段的计算内存（CIM）感知模型自适应方法，通过模型压缩、资源重分配和量化感知训练来解决CIM宏尺寸和ADC精度限制带来的瓶颈，显著提高了CIM利用率和模型压缩率，同时保持了与先前方法相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的计算内存（CIM）宏在深度学习加速方面虽然具有高并行计算和低功耗的优势，但其有限的宏尺寸和ADC精度限制了吞吐量和准确性。

Method: 本文提出了一种两阶段的CIM感知模型自适应过程。第一阶段，根据层重要性和宏尺寸限制进行模型压缩和资源重分配，以减少模型权重加载延迟，提高资源利用率并保持准确性。第二阶段，进行量化感知训练，结合部分和量化和ADC精度，以减轻推理中的量化误差。

Result: 所提出的方法将CIM阵列利用率提高到90%，支持同时激活多达256个字线，并实现了高达93%的压缩率，同时保持了与先前方法相当的准确性。

Conclusion: 所提出的两阶段CIM感知模型自适应方法有效地解决了CIM宏尺寸和ADC精度带来的限制，通过模型压缩、资源重分配和量化感知训练，在提高CIM利用率、模型压缩率和加速性能的同时，保持了模型的准确性。

Abstract: Computing-in-Memory (CIM) macros have gained popularity for deep learning
acceleration due to their highly parallel computation and low power
consumption. However, limited macro size and ADC precision introduce throughput
and accuracy bottlenecks. This paper proposes a two-stage CIM-aware model
adaptation process. The first stage compresses the model and reallocates
resources based on layer importance and macro size constraints, reducing model
weight loading latency while improving resource utilization and maintaining
accuracy. The second stage performs quantization-aware training, incorporating
partial sum quantization and ADC precision to mitigate quantization errors in
inference. The proposed approach enhances CIM array utilization to 90\%,
enables concurrent activation of up to 256 word lines, and achieves up to 93\%
compression, all while preserving accuracy comparable to previous methods.

</details>


### [256] [Low Power Vision Transformer Accelerator with Hardware-Aware Pruning and Optimized Dataflow](https://arxiv.org/abs/2510.14393)
*Ching-Lin Hsiung,Tian-Sheuan Chang*

Main category: cs.AR

TL;DR: 本论文提出了一种低功耗的视觉Transformer加速器，通过算法-硬件协同设计，重点优化前馈神经网络（FFN）的计算瓶颈，并采用动态剪枝和ReLU激活等技术减少计算量和模型复杂度，同时实现了高能效和面积效率。


<details>
  <summary>Details</summary>
Motivation: 当前的Transformer加速器主要关注自注意力机制，但对于短序列的视觉Transformer而言，前馈神经网络（FFN）才是主要的计算瓶颈。因此，需要一种新的优化策略来解决FFN的计算问题。

Method: 该加速器通过算法-硬件协同设计进行优化。采用了硬件友好的动态令牌剪枝技术来降低模型复杂度，并用ReLU替换GELU激活函数以提高稀疏性。此外，还采用了动态FFN2剪枝技术，最终实现了61.5%的操作量减少和59.3%的FFN2权重减少，同时准确率损失小于2%。硬件方面，采用了逐行数据流和面向输出的数据访问方式，以消除数据转置，并支持动态操作，从而最小化面积开销。

Result: 在TSMC 28nm CMOS工艺下实现，该设计占用了496.4K门电路，并包含232KB的SRAM缓存。峰值吞吐量达到1024 GOPS (1GHz)，能效为2.31 TOPS/W，面积效率为858.61 GOPS/mm²。

Conclusion: 本论文提出了一种创新的低功耗视觉Transformer加速器，通过针对FFN瓶颈进行算法-硬件协同优化，在保证较低精度损失的情况下，显著提升了能效和面积效率，为移动和边缘设备上的Transformer应用提供了解决方案。

Abstract: Current transformer accelerators primarily focus on optimizing self-attention
due to its quadratic complexity. However, this focus is less relevant for
vision transformers with short token lengths, where the Feed-Forward Network
(FFN) tends to be the dominant computational bottleneck. This paper presents a
low power Vision Transformer accelerator, optimized through algorithm-hardware
co-design. The model complexity is reduced using hardware-friendly dynamic
token pruning without introducing complex mechanisms. Sparsity is further
improved by replacing GELU with ReLU activations and employing dynamic FFN2
pruning, achieving a 61.5\% reduction in operations and a 59.3\% reduction in
FFN2 weights, with an accuracy loss of less than 2\%. The hardware adopts a
row-wise dataflow with output-oriented data access to eliminate data
transposition, and supports dynamic operations with minimal area overhead.
Implemented in TSMC's 28nm CMOS technology, our design occupies 496.4K gates
and includes a 232KB SRAM buffer, achieving a peak throughput of 1024 GOPS at
1GHz, with an energy efficiency of 2.31 TOPS/W and an area efficiency of 858.61
GOPS/mm2.

</details>


### [257] [ColumnDisturb: Understanding Column-based Read Disturbance in Real DRAM Chips and Implications for Future Systems](https://arxiv.org/abs/2510.14750)
*İsmail Emir Yüksel,Ataberk Olgun,F. Nisa Bostancı,Haocong Luo,A. Giray Yağlıkçı,Onur Mutlu*

Main category: cs.AR

TL;DR: ColumnDisturb是一种新的DRAM读干扰现象，它通过激活行来干扰同一列中的单元，影响范围比RowHammer更广，并且在不同制造商的DDR4和HBM2芯片中普遍存在，尤其在更小的工艺节点下更为严重，同时会削弱现有刷新机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有DRAM芯片中存在一种新的、普遍存在的读干扰现象，即ColumnDisturb，该现象通过激活一行来干扰同一列中的单元，并且其影响范围和严重程度随着技术的发展而加剧，对现有的刷新机制构成了挑战。

Method: 通过实验，利用216个DDR4和4个HBM2芯片，在不同工作条件下，对ColumnDisturb现象进行了表征，并分析了其影响范围、性能以及与现有刷新机制的关系。

Result: ColumnDisturb现象普遍存在于三大制造商的DRAM芯片中，且在更小的工艺节点下（如DDR4）性能更好（诱导首次比特翻转所需时间减少高达5.06倍）。该现象能在标准的刷新窗口内（如63.6毫秒）诱导多个单元发生比特翻转，并且影响的行数远超保留故障（最多198倍），这会显著降低现有保留感知刷新机制的优势。

Conclusion: ColumnDisturb是一种新的、普遍存在的DRAM读干扰现象，其影响范围广、严重程度随技术发展而加剧，并且对现有的保留感知刷新机制的有效性构成了严重挑战，需要进一步研究和应对。

Abstract: We experimentally demonstrate a new widespread read disturbance phenomenon,
ColumnDisturb, in real commodity DRAM chips. By repeatedly opening or keeping a
DRAM row (aggressor row) open, we show that it is possible to disturb DRAM
cells through a DRAM column (i.e., bitline) and induce bitflips in DRAM cells
sharing the same columns as the aggressor row (across multiple DRAM subarrays).
With ColumnDisturb, the activation of a single row concurrently disturbs cells
across as many as three subarrays (e.g., 3072 rows) as opposed to
RowHammer/RowPress, which affect only a few neighboring rows of the aggressor
row in a single subarray. We rigorously characterize ColumnDisturb and its
characteristics under various operational conditions using 216 DDR4 and 4 HBM2
chips from three major manufacturers. Among our 27 key experimental
observations, we highlight two major results and their implications.
  First, ColumnDisturb affects chips from all three major manufacturers and
worsens as DRAM technology scales down to smaller node sizes (e.g., the minimum
time to induce the first ColumnDisturb bitflip reduces by up to 5.06x). We
observe that, in existing DRAM chips, ColumnDisturb induces bitflips within a
standard DDR4 refresh window (e.g., in 63.6 ms) in multiple cells. We predict
that, as DRAM technology node size reduces, ColumnDisturb would worsen in
future DRAM chips, likely causing many more bitflips in the standard refresh
window. Second, ColumnDisturb induces bitflips in many (up to 198x) more rows
than retention failures. Therefore, ColumnDisturb has strong implications for
retention-aware refresh mechanisms that leverage the heterogeneity in cell
retention times: our detailed analyses show that ColumnDisturb greatly reduces
the benefits of such mechanisms.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [258] [Multi-Period Sparse Optimization for Proactive Grid Blackout Diagnosis](https://arxiv.org/abs/2510.14045)
*Qinghua Ma,Reetam Sen Biswas,Denis Osipov,Guannan Qu,Soummya Kar,Shimiao Li*

Main category: eess.SY

TL;DR: 该论文提出一种多时段稀疏优化方法，用于识别电网在极端事件下（如峰值负荷过载）导致系统崩溃（停电）的关键脆弱点。


<details>
  <summary>Details</summary>
Motivation: 为了提高电网在极端事件下的恢复能力，需要主动识别导致系统崩溃的关键脆弱点。

Method: 提出一种多时段稀疏优化方法，通过在具有不同系统压力（如不断增长的需求或恶化的意外情况）的一系列崩溃系统中跨时段地识别持续存在的故障源来捕获“隐藏”的演变中的脆弱性。该方法利用基于电路理论的潮流公式和受电路启发的优化启发式方法来提高可扩展性。

Result: 实验表明，该方法能够可靠地跟踪负载压力增加下的持续脆弱点位置，并且在大系统上具有可扩展性（在2000+节点的系统上，每种场景平均耗时约200秒）。

Conclusion: 该方法能够有效地识别电网中持续存在的脆弱点，并能在保证可扩展性的前提下，为提高电网在极端事件下的恢复能力提供支持。

Abstract: Existing or planned power grids need to evaluate survivability under extreme
events, like a number of peak load overloading conditions, which could possibly
cause system collapses (i.e. blackouts). For realistic extreme events that are
correlated or share similar patterns, it is reasonable to expect that the
dominant vulnerability or failure sources behind them share the same locations
but with different severity. Early warning diagnosis that proactively
identifies the key vulnerabilities responsible for a number of system collapses
of interest can significantly enhance resilience. This paper proposes a
multi-period sparse optimization method, enabling the discovery of {persistent
failure sources} across a sequence of collapsed systems with increasing system
stress, such as rising demand or worsening contingencies. This work defines
persistency and efficiently integrates persistency constraints to capture the
``hidden'' evolving vulnerabilities. Circuit-theory based power flow
formulations and circuit-inspired optimization heuristics are used to
facilitate the scalability of the method. Experiments on benchmark systems show
that the method reliably tracks persistent vulnerability locations under
increasing load stress, and solves with scalability to large systems ({on
average} taking {around} 200 s per scenario on 2000+ bus systems).

</details>


### [259] [Cyber-Resilient System Identification for Power Grid through Bayesian Integration](https://arxiv.org/abs/2510.14043)
*Shimiao Li,Guannan Qu,Bryan Hooi,Vyas Sekar,Soummya Kar,Larry Pileggi*

Main category: eess.SY

TL;DR: 本研究提出了一种结合快照系统识别与时间序列模型（通过贝叶斯融合）的方法，以增强电网应对随机和目标性虚假数据攻击的网络弹性。


<details>
  <summary>Details</summary>
Motivation: 现代电网面临日益严峻的网络威胁，需要实时态势感知。现有的快照系统识别方法无法有效检测现代交互式、目标性虚假数据，从而严重影响估计准确性。

Method: 本研究将快照式方法与时间序列模型通过贝叶斯融合相结合，提出了一种改进的系统识别方法。该方法利用基于距离的时间序列模型，能够处理由电网拓扑和设置变化引起的不同历史数据分布。通过贝叶斯处理将历史数据中捕获的正常系统行为整合到系统识别中，从而使解决方案能够抵御目标性虚假数据。

Result: 实验证明，该方法在混合随机异常（坏数据、拓扑错误）和目标性虚假数据注入攻击（FDIA）下，能够实现：1）网络弹性：FDIA 下估计误差减少超过 70%；2）异常数据识别：能够报警并定位异常数据；3）近乎线性可扩展性：在大规模 2383 节点系统上，使用笔记本 CPU，每时间步长处理时间与快照式基线相当，均小于 1 分钟。

Conclusion: 本研究提出的结合快照系统识别与时间序列模型（通过贝叶斯融合）的方法，能够有效提高电网在面临虚假数据攻击时的网络弹性，并能准确识别和定位异常数据，同时保持良好的可扩展性。

Abstract: Power grids increasingly need real-time situational awareness under the
ever-evolving cyberthreat landscape. Advances in snapshot-based system
identification approaches have enabled accurately estimating states and
topology from a snapshot of measurement data, under random bad data and
topology errors. However, modern interactive, targeted false data can stay
undetectable to these methods, and significantly compromise estimation
accuracy. This work advances system identification that combines snapshot-based
method with time-series model via Bayesian Integration, to advance cyber
resiliency against both random and targeted false data. Using a distance-based
time-series model, this work can leverage historical data of different
distributions induced by changes in grid topology and other settings. The
normal system behavior captured from historical data is integrated into system
identification through a Bayesian treatment, to make solutions robust to
targeted false data. We experiment on mixed random anomalies (bad data,
topology error) and targeted false data injection attack (FDIA) to demonstrate
our method's 1) cyber resilience: achieving over 70% reduction in estimation
error under FDIA; 2) anomalous data identification: being able to alarm and
locate anomalous data; 3) almost linear scalability: achieving comparable speed
with the snapshot-based baseline, both taking <1min per time tick on the large
2,383-bus system using a laptop CPU.

</details>


### [260] [Dual Detection Framework for Faults and Integrity Attacks in Cyber-Physical Control Systems](https://arxiv.org/abs/2510.14052)
*Xixing Xue,Dong Shen,Steven X. Ding,Dong Zhao*

Main category: eess.SY

TL;DR: 本研究提出了一种用于网络物理控制系统异常检测和区分的双检测框架，通过利用控制回路的动态特性和完整性攻击的隐蔽性特征，能够有效地区分故障和网络攻击，并能进一步优化检测性能。


<details>
  <summary>Details</summary>
Motivation: 网络物理控制系统的安全至关重要，准确区分不同类型的异常对于系统恢复和缓解至关重要。

Method: 提出了一种双检测框架，该框架首先推导出闭环隐蔽性条件，然后在控制器和被控对象两侧分别部署专用检测器，以实现联合的被控对象故障和网络攻击检测。通过联合分析两个检测器针对不同异常的残差响应，并利用残差空间区分故障和攻击。最后，通过两阶段优化方案进一步提高了故障和攻击检测性能。

Result: 仿真结果验证了所提出方法的有效性。

Conclusion: 所提出的双检测框架能够有效地区分故障和完整性攻击，并能通过两阶段优化方案进一步提高检测性能。

Abstract: Anomaly detection plays a vital role in the security and safety of
cyber-physical control systems, and accurately distinguishing between different
anomaly types is crucial for system recovery and mitigation. This study
proposes a dual detection framework for anomaly detection and discrimination.
By leveraging the dynamic characteristics of control loops and the stealthiness
features of integrity attacks, the closed-loop stealthiness condition is first
derived, and two dedicated detectors are designed and deployed on the
controller side and the plant side, respectively, enabling joint plant fault
and cyber attack detection. Moreover, by jointly analyzing the residual
response of the two detectors corresponding to different anomalies, it is
proved that the proposed method can distinguish between faults and integrity
attacks due to the detectors' individual residual spaces. According to the
detector's residual space, the fault and attack detection performance is
further improved by a two-stage optimization scheme. Simulation results
validate the effectiveness of the proposed approach.

</details>


### [261] [DiffOPF: Diffusion Solver for Optimal Power Flow](https://arxiv.org/abs/2510.14075)
*Milad Hoseinpour,Vladimir Dvorkin*

Main category: eess.SY

TL;DR: OPF问题通常是多值的、非凸的。现有深度学习方法是单值的，难以处理系统参数变化。本文提出DiffOPF，一种基于扩散模型的OPF求解器，将OPF视为条件采样问题。DiffOPF学习了负荷和调度设定点的联合分布，并能在给定负荷条件下返回边际调度分布。与单值求解器不同，DiffOPF能生成统计上可信的、在成本和约束满足之间有良好权衡的暖启动。文章还探讨了DiffOPF的样本复杂度，并在实际电力系统基准测试中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习OPF求解器是单值的，无法捕捉系统参数变化导致的调度设定点多值性，除非将所有参数纳入特征空间，这在计算上是不可行的。

Method: 将OPF视为条件采样问题，提出了一种基于扩散模型的OPF求解器DiffOPF。该求解器从运行历史中学习负荷和调度设定点的联合分布，并能根据给定的负荷条件生成边际调度分布。

Result: DiffOPF能够生成统计上可信的暖启动，并在成本和约束满足之间实现有利的权衡。通过对样本复杂度的分析和在电力系统基准测试中的实验验证，证明了DiffOPF能够获得接近优化求解器精度的OPF解。

Conclusion: DiffOPF作为一种新颖的基于扩散模型的OPF求解器，能够有效解决传统深度学习方法在处理OPF多值性和系统参数变化方面的局限性，并在精度和效率上取得了良好的效果。

Abstract: The optimal power flow (OPF) is a multi-valued, non-convex mapping from loads
to dispatch setpoints. The variability of system parameters (e.g., admittances,
topology) further contributes to the multiplicity of dispatch setpoints for a
given load. Existing deep learning OPF solvers are single-valued and thus fail
to capture the variability of system parameters unless fully represented in the
feature space, which is prohibitive. To solve this problem, we introduce a
diffusion-based OPF solver, termed \textit{DiffOPF}, that treats OPF as a
conditional sampling problem. The solver learns the joint distribution of loads
and dispatch setpoints from operational history, and returns the marginal
dispatch distributions conditioned on loads. Unlike single-valued solvers,
DiffOPF enables sampling statistically credible warm starts with favorable cost
and constraint satisfaction trade-offs. We explore the sample complexity of
DiffOPF to ensure the OPF solution within a prescribed distance from the
optimization-based solution, and verify this experimentally on power system
benchmarks.

</details>


### [262] [Belief Space Control of Safety-Critical Systems Under State-Dependent Measurement Noise](https://arxiv.org/abs/2510.14100)
*Rohan Walia,Mitchell Black,Andrew Schoer,Kevin Leahy*

Main category: eess.SY

TL;DR: 该研究提出了一种名为 BCBF-GEKF 的新方法，用于处理具有状态相关噪声的传感器，以提高自动驾驶系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的 CBF 方法在处理具有状态相关误差的复杂传感器时可能过于保守。

Method: 该研究将 BCBF 框架与 GEKF 算法相结合，以模拟状态相关的测量噪声。

Result: 通过在 1D 和 2D 场景下的仿真结果表明，BCBF-GEKF 方法比现有的 BCBF 方法具有更保守的控制和更高的安全性。

Conclusion: BCBF-GEKF 方法能够为具有状态相关噪声的传感器提供更优的控制和更高的安全性。

Abstract: Safety-critical control is imperative for deploying autonomous systems in the
real world. Control Barrier Functions (CBFs) offer strong safety guarantees
when accurate system and sensor models are available. However, widely used
additive, fixed-noise models are not representative of complex sensor
modalities with state-dependent error characteristics. Although CBFs have been
designed to mitigate uncertainty using fixed worst-case bounds on measurement
noise, this approach can lead to overly-conservative control. To solve this
problem, we extend the Belief Control Barrier Function (BCBF) framework to
accommodate state-dependent measurement noise via the Generalized Extended
Kalman Filter (GEKF) algorithm, which models measurement noise as a linear
function of the state. Using the original BCBF framework as baseline, we
demonstrate the performance of the BCBF-GEKF approach through simulation
results on a 1D single integrator setpoint tracking scenario and 2D unicycle
kinematics trajectory tracking scenario. Our results confirm that the BCBF-GEKF
approach offers less conservative control with greater safety.

</details>


### [263] [Resource-Aware Stealthy Attacks in Vehicle Platoons](https://arxiv.org/abs/2510.14119)
*Ali Eslami,Mohammad Pirani*

Main category: eess.SY

TL;DR: CAVs易受隐蔽攻击，攻击者可秘密操纵车队轨迹；本研究分析攻击可行性、依赖性、所需资源，揭示系统漏洞，为设计更安全的CAV系统提供依据。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注防御攻击，但对旨在秘密操纵车队行为的隐蔽性对手的关注较少。

Method: 通过演示攻击者如何在不被察觉的情况下引导车队走向其期望的轨迹，来介绍攻击设计问题的新视角。分析攻击的可行性条件、对通信拓扑和控制协议的依赖性，以及攻击者所需的资源。

Result: 揭示了当前车队架构和异常检测机制的关键弱点。

Conclusion: 通过表征发起隐蔽攻击所需的资源，解决了系统漏洞，并为设计有弹性的对策提供了信息。本研究为开发更安全、更可信的CAV系统提供了方法。

Abstract: Connected and Autonomous Vehicles (CAVs) are transforming modern
transportation by enabling cooperative applications such as vehicle platooning,
where multiple vehicles travel in close formation to improve efficiency and
safety. However, the heavy reliance on inter-vehicle communication makes
platoons highly susceptible to attacks, where even subtle manipulations can
escalate into severe physical consequences. While existing research has largely
focused on defending against attacks, far less attention has been given to
stealthy adversaries that aim to covertly manipulate platoon behavior. This
paper introduces a new perspective on the attack design problem by
demonstrating how attackers can guide platoons toward their own desired
trajectories while remaining undetected. We outline conditions under which such
attacks are feasible, analyze their dependence on communication topologies and
control protocols, and investigate the resources required by the attacker. By
characterizing the resources needed to launch stealthy attacks, we address
system vulnerabilities and informing the design of resilient countermeasures.
Our findings reveal critical weaknesses in current platoon architectures and
anomaly detection mechanisms and provide methods to develop more secure and
trustworthy CAV systems.

</details>


### [264] [A Deep State-Space Model Compression Method using Upper Bound on Output Error](https://arxiv.org/abs/2510.14542)
*Hiroki Sakamoto,Kazuhiro Sato*

Main category: eess.SY

TL;DR: 该研究提出了一种压缩深度状态空间模型（Deep SSMs）的方法，该方法利用线性二次输出（LQO）系统作为内部模块，并提供了可证明的输出误差保证。


<details>
  <summary>Details</summary>
Motivation: 在深度状态空间模型（Deep SSMs）中，利用线性二次输出（LQO）系统作为内部模块，并提出一种具有可证明输出误差保证的压缩方法。

Method: 首先推导出两个Deep SSMs之间的输出误差上界，并将其表示为层级LQO系统之间的$h^2$-误差范数。在此基础上，构建了一个基于$h^2$-误差范数的优化问题，并开发了一种基于梯度的模型阶数约减（MOR）方法。

Result: 在Long Range Arena基准测试的IMDb任务上，所提出的压缩方法取得了优异的性能，减少了约80%的可训练参数，而性能仅下降4-5%。

Conclusion: 所提出的基于$h^2$-误差范数的梯度下降MOR方法能够有效地压缩Deep SSMs，在保持高性能的同时显著减少模型参数量。

Abstract: We study deep state-space models (Deep SSMs) that contain
linear-quadratic-output (LQO) systems as internal blocks and present a
compression method with a provable output error guarantee. We first derive an
upper bound on the output error between two Deep SSMs and show that the bound
can be expressed via the $h^2$-error norms between the layerwise LQO systems,
thereby providing a theoretical justification for existing model order
reduction (MOR)-based compression. Building on this bound, we formulate an
optimization problem in terms of the $h^2$-error norm and develop a
gradient-based MOR method. On the IMDb task from the Long Range Arena
benchmark, we demonstrate that our compression method achieves strong
performance. Moreover, unlike prior approaches, we reduce roughly 80% of
trainable parameters without retraining, with only a 4-5% performance drop.

</details>


### [265] [High-Resolution PTDF-Based Planning of Storage and Transmission Under High Renewables](https://arxiv.org/abs/2510.14696)
*Kevin Wu,Rabab Haider,Pascal Van Hentenryck*

Main category: eess.SY

TL;DR: 该论文提出了一种新的输电网扩展规划（TEP）和储能系统选址/定容的联合优化方法，并验证了其在大规模系统上的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了满足日益增长的电力需求和整合可再生能源，需要对电网进行升级改造，同时需要储能系统来提供灵活性和缓解拥塞。

Method: 提出了一种多周期、两阶段的PTDF模型，并结合了信任域、多割Benders分解方法，以实现输电网升级和储能系统的联合优化。

Result: 在模拟的德克萨斯大型电力系统上，该方法在保证高可再生能源渗透率的条件下，得到了优化的输电网升级方案和储能系统配置，最终最优性间隙小于1%。

Conclusion: 该PTDF方法能够有效地处理大规模分布式储能系统，在大空间分辨率下展现出良好的可扩展性。

Abstract: Transmission Expansion Planning (TEP) optimizes power grid upgrades and
investments to ensure reliable, efficient, and cost-effective electricity
delivery while addressing grid constraints. To support growing demand and
renewable energy integration, energy storage is emerging as a pivotal asset
that provides temporal flexibility and alleviates congestion. This paper
develops a multiperiod, two-stage PTDF formulation that co-optimizes
transmission upgrades and storage siting/sizing. To ensure scalability, a
trust-region, multicut Benders scheme warm-started from per-representative-day
optima is proposed. Applied to a 2,000-bus synthetic Texas system under
high-renewable projections, the method attains final optimality gaps below 1%
and yields a plan with storage at about 180 nodes (32% of peak renewable
capacity). These results demonstrate that the proposed PTDF-based methodology
efficiently handles large distributed storage fleets, demonstrating scalability
at high spatial resolution

</details>


### [266] [A Human-Vector Susceptible--Infected--Susceptible Model for Analyzing and Controlling the Spread of Vector-Borne Diseases](https://arxiv.org/abs/2510.14787)
*Lorenzo Zino,Alessandro Casu,Alessandro Rizzo*

Main category: eess.SY

TL;DR: 本文提出了一个针对病媒传播疾病传播的流行病学模型，该模型扩展了经典的SIR模型，同时考虑了人类和媒介两个种群，以及它们之间的交叉感染。通过将模型表述为常微分方程组，并利用单调系统理论来严格表征流行病学动态，确定了疾病快速根除或收敛到地方性流行病学平衡的条件。此外，文章还结合了媒介控制和鼓励采取防护措施这两种控制措施，并通过数学工具评估了这些措施的影响，最终确定了最优控制策略。


<details>
  <summary>Details</summary>
Motivation: 建立一个能够描述病媒传播疾病传播过程的数学模型，并分析疾病的传播动态，同时评估不同控制措施的效果，以期找到最优的控制策略。

Method: 1. 扩展经典的SIR模型，构建包含人类和媒介两个种群及其交叉感染的流行病学模型。
2. 将模型表述为常微分方程组。
3. 利用单调系统理论分析模型的全局渐近行为，确定疾病根除或达到地方性平衡的条件。
4. 引入媒介控制和防护措施激励两种控制措施。
5. 评估控制措施对模型动态的影响，并确定最优控制策略。

Result: 1. 得到了疾病根除（所有轨迹收敛到无病平衡点）或收敛到唯一的地方性平衡点的条件。
2. 评估了媒介控制和防护措施激励的效果。
3. 确定了最优控制策略。

Conclusion: 所提出的模型能够有效地描述病媒传播疾病的传播动态，并为评估和制定控制策略提供了数学工具，其中媒介控制和防护措施激励是有效的干预手段。

Abstract: We propose an epidemic model for the spread of vector-borne diseases. The
model, which is built extending the classical susceptible-infected-susceptible
model, accounts for two populations -- humans and vectors -- and for
cross-contagion between the two species, whereby humans become infected upon
interaction with carrier vectors, and vectors become carriers after interaction
with infected humans. We formulate the model as a system of ordinary
differential equations and leverage monotone systems theory to rigorously
characterize the epidemic dynamics. Specifically, we characterize the global
asymptotic behavior of the disease, determining conditions for quick
eradication of the disease (i.e., for which all trajectories converge to a
disease-free equilibrium), or convergence to a (unique) endemic equilibrium.
Then, we incorporate two control actions: namely, vector control and incentives
to adopt protection measures. Using the derived mathematical tools, we assess
the impact of these two control actions and determine the optimal control
policy.

</details>


### [267] [Improved Voltage Regulation with Optimal Design of Decentralized Volt-VAr Control](https://arxiv.org/abs/2510.14834)
*Daniel Russell,Dakota Hamilton,Mads R. Almassalkhi,Hamid R. Ossareh*

Main category: eess.SY

TL;DR: 智能电网需要分布式能源的自主动态电压调节。本文提出了一种基于线性潮流模型的去中心化伏安控制（VVC）方法，用于优化VVC斜率以最小化电压偏差并满足稳定性约束，并通过仿真证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源的整合，需要对电网进行自主、动态的电压调节。

Method: 使用线性潮流模型对电网-VVC闭环动态进行建模，并采用非凸谱半径稳定性约束来设计VVC斜率，以最小化稳态电压偏差。

Result: 通过在真实的馈线上的仿真，与现有的凸约束相比，使用谱半径约束可以实现更有效的电压调节。

Conclusion: 提出的基于谱半径约束的VVC设计方法在分布式能源接入的电网中能实现更优的电压调节效果。

Abstract: Integration of distributed energy resources has created a need for
autonomous, dynamic voltage regulation. Decentralized Volt-VAr Control (VVC) of
grid-connected inverters presents a unique opportunity for voltage management
but, if designed poorly, can lead to unstable behavior when in feedback with
the grid. We model the grid-VVC closed-loop dynamics with a linearized power
flow approach, leveraging historical data, which shows improvement over the
commonly used LinDistFlow model. This model is used to design VVC slopes by
minimizing steady-state voltage deviation from the nominal value, subject to a
non-convex spectral radius stability constraint, which has not been previously
implemented within this context. We compare this constraint to existing convex
restrictions and demonstrate, through simulations on a realistic feeder, that
using the spectral radius results in more effective voltage regulation.

</details>


### [268] [Dynamic-Key-Aware Co-Simulation Framework for Next Generation of SCADA Systems Encrypted by Quantum-Key-Distribution Techniques](https://arxiv.org/abs/2510.14838)
*Ziqing Zhu*

Main category: eess.SY

TL;DR: 本论文提出了一种多层建模与优化框架，将量子密钥分发（QKD）应用于改进的SCADA系统，以应对日益严峻的网络安全挑战。与以往仅关注点对点通信安全的应用不同，该框架整合了密钥生成、消耗、库存预测及控制延迟，实现了基于安全需求和资源约束的自适应控制链重构。


<details>
  <summary>Details</summary>
Motivation: 现有SCADA系统面临严峻的网络安全挑战，而传统的QKD应用未能充分考虑密钥动态与控制调度的系统级耦合。

Method: 提出一个多层建模与优化框架，整合量子密钥生成、消耗、库存预测和控制延迟，并利用双层Stackelberg博弈及补余约束数学规划（MPCC）解决资源分配冲突。开发了LD-CP算法求解，并构建了仿真平台进行评估。

Result: 实验结果表明，该方法将任务成功率提高了25%，将峰值频率偏差降低了70%，并将密钥利用率提高到83%。

Conclusion: 该研究为未来量子安全电力系统运营奠定了基础。

Abstract: To address growing cybersecurity challenges in modern power dispatch systems,
this paper proposes a multi-layer modeling and optimization framework for SCADA
systems enhanced with quantum key distribution (QKD). While most existing
applications of QKD in the power sector focus on building secure point-to-point
communication tunnels, they rarely consider the system-level coupling between
key dynamics and control scheduling. In contrast, our approach integrates
quantum key generation, consumption, inventory prediction, and control latency
into a unified model, enabling key-aware reconfiguration of SCADA control
chains based on task security demands and real-time resource constraints. To
resolve conflicts in key resource allocation between transmission system
operators (TSOs) and distribution system operators (DSOs), we formulate a
bi-level Stackelberg game and transform it into a mathematical program with
complementarity constraints (MPCC). We further develop an efficient Level
Decomposition-Complementarity Pruning (LD-CP) algorithm to solve the problem.
To support reproducible evaluation, we build an end-to-end co-simulation
platform that integrates physical-layer disruptions via OpenQKD-Sim,
Q3P/IEC-104 protocol stack binding, and real-time control-chain monitoring
through Grafana. Experimental results on the IEEE 39- and 118-bus systems show
that our method increases task success rate by 25%, reduces peak frequency
deviation by 70%, and improves key utilization to 83%. This work lays the
foundation for future quantum-secure control systems in power grid operations.

</details>


### [269] [Through-the-Earth Magnetic Induction Communication and Networking: A Comprehensive Survey](https://arxiv.org/abs/2510.14854)
*Honglei Ma,Erwu Liu,Wei Ni,Zhijun Fang,Rui Wang,Yongbin Gao,Dusit Niyato,Ekram Hossain*

Main category: eess.SY

TL;DR: 本文全面 survey 了地波磁感应（MI）通信，涵盖了其应用、信道建模、点对点设计、中继技术、网络框架和新兴技术，并提出了一种支持 TCP/IP 和 Linux 的 MI 通信框架，以加速 SAGUI 网络中的 MI 研究。


<details>
  <summary>Details</summary>
Motivation: 为了满足下一代移动通信系统中空间-航空-地面-地下（SAGUI）网络集成对地下通信的需求，需要对地波磁感应（MI）通信进行深入研究，特别是解决 MI 快衰落等新兴挑战。

Method: 本文提出了一种新颖的几何模型来分析 MI 快衰落，并将 MI 信道功率增益分解为四个独立的物理参数。此外，还总结了 MI 中继技术，研究了中继和高密度网络中的串扰效应，并提出了一个支持 TCP/IP 和 Linux 的 MI 通信框架。

Result: 本文对 MI 应用进行了比较，回顾了信道建模原理（包括慢衰落和快衰落），分析了 MI 快衰落的潜在影响。提出的 MI 通信框架能够支持现有和新兴的 MI 解决方案的完全实现，并为研究人员提供了一个利用 Linux 资源和深度学习平台进行加速开发的途径。

Conclusion: MI 通信是地下通信的有希望的候选技术。通过对 MI 快衰落进行细致的分析和建模，并提出一个支持 TCP/IP 和 Linux 的通用 MI 框架，可以克服现有挑战，促进 MI 在 SAGUI 网络中的发展和应用。

Abstract: Magnetic induction (MI) communication (MIC) has emerged as a promising
candidate for underground communication networks due to its excellent
penetration capabilities. Integration with Space-Air-Ground-Underground (SAGUI)
networks in next-generation mobile communication systems requires a
well-defined network architecture. A recent discovery in MIC research, MI fast
fading, remains in its early stages and presents unique challenges. This paper
provides a comprehensive survey on through-the-earth (TTE) MIC, covering MI
applications, channel modeling, point-to-point MIC design, relay techniques,
network frameworks, and emerging technologies. We compare various MIC
applications to highlight TTE-specific challenges and review the principles of
channel modeling, addressing both MI slow fading and MI fast fading, along with
its potential impact on existing MIC theories. We conduct a fine-grained
decomposition of MI channel power gain into four distinct physical parameters,
and propose a novel geometric model to analyze MI fast fading. We also
summarize MI relay techniques, examine crosstalk effects in relay and
high-density networks, and explore key research tasks within the OSI framework
for a holistic MI network protocol in SAGUI. To bridge the gaps identified, we
propose a MIC framework that supports TCP/IP and Linux, enabling full
implementation of existing and emerging MIC solutions. This framework empowers
researchers to leverage Linux resources and deep learning platforms for
accelerated development of MIC in SAGUI networks. Remaining research
challenges, open issues, and promising novel techniques are further identified
to advance MIC research.

</details>


### [270] [Further Results on Safety-Critical Stabilization of Force-Controlled Nonholonomic Mobile Robots](https://arxiv.org/abs/2510.14931)
*Bo Wang,Tianyu Han,Guangwei Wang*

Main category: eess.SY

TL;DR: 本论文提出了一种基于gamma m-二次规划（gamma m-QP）框架的连续、时不变控制律，用于解决带安全约束的力控非完整移动机器人的镇定问题，该框架统一了控制李雅普诺夫函数（CLFs）和控制障碍函数（CBFs），以在闭环系统中同时保证稳定性和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决带安全约束的力控非完整移动机器人的镇定问题。

Method: 提出了一种基于gamma m-二次规划（gamma m-QP）框架的连续、时不变控制律。该框架结合了控制李雅普诺夫函数（CLFs）和控制障碍函数（CBFs）。首先，在极坐标下为闭环非完整移动机器人系统构造了一个全局、时不变、严格的李雅普诺夫函数，并将其用作QP设计中的CLF。其次，利用车辆动力学的内在级联结构，通过积分器反步法为移动机器人开发了一个CBF。

Result: 保证了闭环系统的渐近稳定性和安全性。通过仿真和实验结果验证了所提方法的有效性和性能。

Conclusion: 所提出的基于gamma m-QP的控制方法能够有效地保证力控非完整移动机器人在安全约束下的稳定性和安全性。

Abstract: In this paper, we address the stabilization problem for force-controlled
nonholonomic mobile robots under safety-critical constraints. We propose a
continuous, time-invariant control law based on the gamma m-quadratic
programming (gamma m-QP) framework, which unifies control Lyapunov functions
(CLFs) and control barrier functions (CBFs) to enforce both stability and
safety in the closed-loop system. For the first time, we construct a global,
time-invariant, strict Lyapunov function for the closed-loop nonholonomic
mobile robot system with a nominal stabilization controller in polar
coordinates; this strict Lyapunov function then serves as the CLF in the QP
design. Next, by exploiting the inherent cascaded structure of the vehicle
dynamics, we develop a CBF for the mobile robot via an integrator backstepping
procedure. Our main results guarantee both asymptotic stability and safety for
the closed-loop system. Both the simulation and experimental results are
presented to illustrate the effectiveness and performance of our approach.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [271] [Benefits and Limitations of Communication in Multi-Agent Reasoning](https://arxiv.org/abs/2510.13903)
*Michael Rizvi-Martel,Satwik Bhattamishra,Neil Rathi,Guillaume Rabusseau,Michael Hahn*

Main category: cs.MA

TL;DR: Chain-of-thought prompting is limited by problem complexity and context length. Multi-agent systems offer a solution, but their capabilities are not well understood. This paper provides a theoretical framework to analyze multi-agent systems, deriving bounds on agent count, communication, and speedups for state tracking, recall, and k-hop reasoning tasks. The analysis identifies conditions where communication is beneficial, trade-offs between agent count and bandwidth, and limitations under resource constraints. Experiments on LLMs confirm these theoretical predictions, offering guidance for designing scalable multi-agent reasoning systems.


<details>
  <summary>Details</summary>
Motivation: Multi-agent paradigms offer a promising solution to the degradation of LLM performance with increasing problem complexity and context length, but their fundamental capacities are poorly understood.

Method: This paper proposes a theoretical framework to analyze the expressivity of multi-agent systems, applying it to state tracking, recall, and k-hop reasoning. Bounds are derived for the number of agents, communication requirements, and achievable speedups. This is complemented by experiments on pretrained LLMs using controlled synthetic benchmarks.

Result: The theoretical analysis derives bounds on agent count, communication, and speedups, identifying regimes where communication is beneficial and delineating trade-offs between agent count and bandwidth. It also exposes intrinsic limitations when resources are constrained. Empirical outcomes confirm the predicted trade-offs.

Conclusion: The analysis provides principled guidance for designing scalable multi-agent reasoning systems by offering a theoretical framework, deriving bounds on key performance metrics, and empirically validating the findings.

Abstract: Chain-of-thought prompting has popularized step-by-step reasoning in large
language models, yet model performance still degrades as problem complexity and
context length grow. By decomposing difficult tasks with long contexts into
shorter, manageable ones, recent multi-agent paradigms offer a promising
near-term solution to this problem. However, the fundamental capacities of such
systems are poorly understood. In this work, we propose a theoretical framework
to analyze the expressivity of multi-agent systems. We apply our framework to
three algorithmic families: state tracking, recall, and $k$-hop reasoning. We
derive bounds on (i) the number of agents required to solve the task exactly,
(ii) the quantity and structure of inter-agent communication, and (iii) the
achievable speedups as problem size and context scale. Our results identify
regimes where communication is provably beneficial, delineate tradeoffs between
agent count and bandwidth, and expose intrinsic limitations when either
resource is constrained. We complement our theoretical analysis with a set of
experiments on pretrained LLMs using controlled synthetic benchmarks. Empirical
outcomes confirm the tradeoffs between key quantities predicted by our theory.
Collectively, our analysis offers principled guidance for designing scalable
multi-agent reasoning systems.

</details>


### [272] [Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations](https://arxiv.org/abs/2510.13982)
*Jinkun Chen,Sher Badshah,Xuemin Yu,Sijia Han,Jiechao Gao*

Main category: cs.MA

TL;DR: 当前的多智能体系统和社交模拟受限于静态环境，无法捕捉真实世界的复杂性。本文提出需要新的研究范式，转向开放式、持续共进化和具有社会适应性的AI生态系统。


<details>
  <summary>Details</summary>
Motivation: 当前的多智能体系统和社交模拟大多运行在静态环境中，具有预定义任务、有限动力学和僵化评估标准，无法捕捉真实社会复杂性。

Method: 本文批判性地回顾了融合LLM与多智能体动力学的新兴架构，指出了平衡稳定与多样性、评估意外行为、扩展复杂性等关键挑战，并提出了该领域的新分类法。

Result: 提出了一种以开放性、持续共进化和有弹性的、社会适应的AI生态系统为中心的研究路线图。

Conclusion: 呼吁社区超越静态范式，共同塑造下一代自适应、具有社会意识的多智能体模拟。

Abstract: What if artificial agents could not just communicate, but also evolve, adapt,
and reshape their worlds in ways we cannot fully predict? With llm now powering
multi-agent systems and social simulations, we are witnessing new possibilities
for modeling open-ended, ever-changing environments. Yet, most current
simulations remain constrained within static sandboxes, characterized by
predefined tasks, limited dynamics, and rigid evaluation criteria. These
limitations prevent them from capturing the complexity of real-world societies.
In this paper, we argue that static, task-specific benchmarks are fundamentally
inadequate and must be rethought. We critically review emerging architectures
that blend llm with multi-agent dynamics, highlight key hurdles such as
balancing stability and diversity, evaluating unexpected behaviors, and scaling
to greater complexity, and introduce a fresh taxonomy for this rapidly evolving
field. Finally, we present a research roadmap centered on open-endedness,
continuous co-evolution, and the development of resilient, socially aligned AI
ecosystems. \textbf{We call on the community to move beyond static paradigms
and help shape the next generation of adaptive, socially-aware multi-agent
simulations.}

</details>


### [273] [Stop Reducing Responsibility in LLM-Powered Multi-Agent Systems to Local Alignment](https://arxiv.org/abs/2510.14008)
*Jinwei Hu,Yi Dong,Shuang Ao,Zhuoyun Li,Boxuan Wang,Lokesh Singh,Guangliang Cheng,Sarvapali D. Ramchurn,Xiaowei Huang*

Main category: cs.MA

TL;DR: LLM驱动的多智能体系统（LLM-MAS）带来了机遇和风险。为了确保负责任的行为，需要从关注个体智能体转向关注整个系统的“全局一致性”。责任被视为一个贯穿系统生命周期的属性，包括一致性、不确定性和安全性，需要结合人类价值观和客观可验证性。此外，还需要一个结合跨学科设计和人机协作监督的“双重视角治理框架”。该框架将LLM-MAS视为统一的社会技术系统，需要有原则的机制来支持责任的各个维度，从而实现合乎道德、可验证且具有韧性的行为，以维持系统范围内的持续一致性。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统（LLM-MAS）虽然带来了分布式推理、协作和任务泛化的新潜力，但也带来了由于无法保证的一致性、级联不确定性和对抗性漏洞而产生的额外风险。因此，有必要从局部、肤浅的智能体级别对齐转向全局、系统的共识，以确保LLM-MAS中的负责任行为。

Method: 提出了一种将主观的、以人为中心的价值观和客观的可验证性相结合的解决方案。此外，还提出了一种“双重视角治理框架”，该框架结合了跨学科设计和人机协作监督，贯穿LLM-MAS的整个生命周期进行追踪和确保责任。

Result: 该方法将LLM-MAS视为统一的、动态的社会技术系统，而不是松散的智能体集合，并提出了支持责任各个维度的原则性机制。

Conclusion: 为了实现LLM-MAS的合乎道德、可验证和有韧性的行为，以维持系统范围内的持续一致性，需要建立一个集成的、贯穿整个生命周期的责任框架，该框架结合了跨学科设计、人机协作监督、人类价值观和客观可验证性。

Abstract: LLM-powered Multi-Agent Systems (LLM-MAS) unlock new potentials in
distributed reasoning, collaboration, and task generalization but also
introduce additional risks due to unguaranteed agreement, cascading
uncertainty, and adversarial vulnerabilities. We argue that ensuring
responsible behavior in such systems requires a paradigm shift: from local,
superficial agent-level alignment to global, systemic agreement. We
conceptualize responsibility not as a static constraint but as a lifecycle-wide
property encompassing agreement, uncertainty, and security, each requiring the
complementary integration of subjective human-centered values and objective
verifiability. Furthermore, a dual-perspective governance framework that
combines interdisciplinary design with human-AI collaborative oversight is
essential for tracing and ensuring responsibility throughout the lifecycle of
LLM-MAS. Our position views LLM-MAS not as loose collections of agents, but as
unified, dynamic socio-technical systems that demand principled mechanisms to
support each dimension of responsibility and enable ethically aligned,
verifiably coherent, and resilient behavior for sustained, system-wide
agreement.

</details>


### [274] [The Role of Social Learning and Collective Norm Formation in Fostering Cooperation in LLM Multi-Agent Systems](https://arxiv.org/abs/2510.14401)
*Prateek Gupta,Qiankun Zhong,Hiromu Yakura,Thomas Eisenmann,Iyad Rahwan*

Main category: cs.MA

TL;DR: 该研究提出了一个无显式奖励信号的CPR模拟框架，结合了社会学习和基于规范的惩罚机制，用于研究LLM多智能体系统中的规范涌现和合作。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体研究多采用显式奖励函数，而人类合作通常依赖于启发式、沟通和惩罚。本研究旨在模拟更接近人类的合作机制。

Method: 构建了一个包含社会学习（从成功的同伴那里采纳策略和信念）和基于规范的惩罚（依据奥斯特罗姆的资源治理原则）的CPR模拟框架，移除了显式奖励信号，并允许智能体从环境反馈中学习。

Result: 模拟能够重现关于人类行为的现有研究发现。在不同的环境和社会初始条件下（资源丰富/稀缺，利他/利己），研究了不同LLM组成的智能体社会的表现，并揭示了不同模型在维持合作和规范形成方面的系统性差异。

Conclusion: 该框架为研究LLM社会中涌现的规范提供了一个严谨的测试平台，有助于设计在社会和组织环境中，与合作规范保持一致性的AI系统，以确保稳定、公平和有效的AI中介环境治理。

Abstract: A growing body of multi-agent studies with Large Language Models (LLMs)
explores how norms and cooperation emerge in mixed-motive scenarios, where
pursuing individual gain can undermine the collective good. While prior work
has explored these dynamics in both richly contextualized simulations and
simplified game-theoretic environments, most LLM systems featuring common-pool
resource (CPR) games provide agents with explicit reward functions directly
tied to their actions. In contrast, human cooperation often emerges without
full visibility into payoffs and population, relying instead on heuristics,
communication, and punishment. We introduce a CPR simulation framework that
removes explicit reward signals and embeds cultural-evolutionary mechanisms:
social learning (adopting strategies and beliefs from successful peers) and
norm-based punishment, grounded in Ostrom's principles of resource governance.
Agents also individually learn from the consequences of harvesting, monitoring,
and punishing via environmental feedback, enabling norms to emerge
endogenously. We establish the validity of our simulation by reproducing key
findings from existing studies on human behavior. Building on this, we examine
norm evolution across a $2\times2$ grid of environmental and social
initialisations (resource-rich vs. resource-scarce; altruistic vs. selfish) and
benchmark how agentic societies comprised of different LLMs perform under these
conditions. Our results reveal systematic model differences in sustaining
cooperation and norm formation, positioning the framework as a rigorous testbed
for studying emergent norms in mixed-motive LLM societies. Such analysis can
inform the design of AI systems deployed in social and organizational contexts,
where alignment with cooperative norms is critical for stability, fairness, and
effective governance of AI-mediated environments.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [275] [Universal Growth of Krylov Complexity Across A Quantum Phase Transition](https://arxiv.org/abs/2510.13947)
*András Grabarits,Adolfo del Campo*

Main category: quant-ph

TL;DR: 量子系统在跨越量子相变时，Krylov空间中的复杂度增长与Kibble-Zurek缺陷标度律相关，且复杂度分布渐进高斯化。


<details>
  <summary>Details</summary>
Motivation: 研究跨越量子相变的驱动量子系统Krylov空间中复杂度增长的统计特性。

Method: 使用非绝热Magnus展开将演化映射到有效的一维跳跃模型，并分析其复杂度增长与Kibble-Zurek缺陷标度律的精确联系。

Result: 对于横向场Ising模型，证明了复杂度增长与Kibble-Zurek缺陷标度律之间的精确联系，所有复杂度的累积量都呈现与缺陷密度相同的幂律标度，且系数与均值相同，完整分布渐进高斯化。

Conclusion: 该结果为任意二阶量子相变中复杂度增长提供了普适的标度论证。

Abstract: We study the statistical properties of the spread complexity in the Krylov
space of quantum systems driven across a quantum phase transition. Using the
diabatic Magnus expansion, we map the evolution to an effective one-dimensional
hopping model. For the transverse field Ising model, we establish an exact link
between the growth of complexity and the Kibble-Zurek defect scaling: all
cumulants of complexity exhibit the same power-law scaling as the defect
density, with coefficients identical to the mean, and the full distribution
asymptotically becomes Gaussian. These results yield general scaling arguments
for the growth of complexity across arbitrary second-order quantum phase
transitions.

</details>


### [276] [Quantum State Designs via Magic Teleportation](https://arxiv.org/abs/2510.13950)
*Hugo Lóio,Guglielmo Lami,Lorenzo Leone,Max McGinley,Xhek Turkeshi,Jacopo De Nardis*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate how non-stabilizer resources enable the emergence of quantum
state designs within the projected ensemble. Starting from initial states with
finite magic and applying resource-free Clifford circuits to scramble them, we
analyze the ensemble generated by performing projective Pauli measurements on a
subsystem of the final state. Using both analytical arguments and large-scale
numerics, we show that the projected ensemble converges towards a state
$k$-design with an error that decays exponentially with the $k$-th Stabilizer
Renyi Entropy of the pre-measurement state, via a Magic-Induced Design Ansatz
(MIDA) that we introduce. We identify a universal scaling form, valid across
different classes of magic initial states, and corroborate it through numerical
simulations and analytical calculations of the frame potential. For
finite-depth Clifford unitaries, we show that the timescales at which state
designs emerge are controlled by the transport of magic. We identify a ``magic
teleportation'' mechanism whereby non-Clifford resources injected locally
spread through Clifford scrambling and measurements across distances beyond the
lightcone. Our results demonstrate how a small and controlled amount of magic
suffices to generate highly random states, providing a systematic route toward
generating quantum state designs in early fault-tolerant devices.

</details>


### [277] [Temporal Entanglement Transitions in the Periodically Driven Ising Chain](https://arxiv.org/abs/2510.13970)
*Karun Gadge,Abhinav Prem,Rishabh Jha*

Main category: quant-ph

TL;DR: 本论文发现了周期性驱动的量子系统中存在时间纠缠跃迁，这是一种在纠缠哈密顿量谱中发生的量子相变，并伴随着动力学自发对称性破缺。


<details>
  <summary>Details</summary>
Motivation: 研究周期性驱动的量子系统在非平衡态下的纠缠动力学，特别是发现在没有静态类似物现象（如时间纠缠跃迁）的情况下，纠缠如何演化。

Method: 通过分析 Floquet 自旋链中的纠缠动力学，研究纠缠哈密顿量的谱，并使用有限尺寸标度来确定临界行为和关联长度指数。

Result: 发现在一系列驱动频率下存在时间纠缠跃迁，表现为纠缠谱重组、施密特隙闭合、纠缠回声消失和对称量子数翻转。在сона频率下，跃迁是真正的稳态特征。有限尺寸标度显示出普适的临界行为，关联长度指数为 1，与平衡态伊辛普遍性一致。

Conclusion: 时间纠缠跃迁是 Floquet 量子物质中的新特征，表明即使在没有静态临界性的情况下，纯粹由动力学机制也可以产生普适的临界行为。

Abstract: Periodically driven quantum systems can host non-equilibrium phenomena
without static analogs, including in their entanglement dynamics. Here, we
discover $temporal$ $entanglement$ $transitions$ in a Floquet spin chain, which
correspond to a quantum phase transition in the spectrum of the entanglement
Hamiltonian and are signaled by dynamical spontaneous symmetry breaking. We
show that these transitions are entanglement-driven, i.e., they require
initially entangled states and remain invisible to conventional local
observables. Intriguingly, we find these transitions across a broad range of
driving frequencies (from adiabatic to high-frequency regime) and independently
of drive details, where they manifest as periodic, sharp entanglement spectrum
reorganizations marked by the Schmidt-gap closure, a vanishing entanglement
echo, and symmetry-quantum-number flips. At high frequencies, the entanglement
Hamiltonian acquires an intrinsic timescale decoupled from the drive period,
rendering the transitions genuine steady-state features. Finite-size scaling
reveals universal critical behavior with correlation-length exponent $\nu=1$,
matching equilibrium Ising universality despite its emergence from purely
dynamical mechanisms decoupled from static criticality. Our work establishes
temporal entanglement transitions as novel features in Floquet quantum matter.

</details>


### [278] [Towards gravimetry enhancement with squeezed states](https://arxiv.org/abs/2510.13973)
*Oziel R. de Araujo,Lucas S. Marinho,Jonas F. G. Santos,Carlos H. S. Vieira*

Main category: quant-ph

TL;DR: 使用压缩探测态估计重力加速度时，探测态的压缩相位比幅度更影响精度，最优精度需要相位工程压缩。


<details>
  <summary>Details</summary>
Motivation: 研究压缩探测态估计重力加速度的精度，特别是分析压缩相位的影响。

Method: 分析了压缩相位、幅度以及它们与动量测量的关系，并提出了最优测量方案。

Result: 发现沿相空间正交量压缩的探测态无法超越散粒噪声极限，而具有压缩幅度的位置-动量相关输入态可以。最优灵敏度通过动量测量和时间依赖的压缩相位调整实现。

Conclusion: 相位工程压缩在实验重力测量方案中起着根本性作用。

Abstract: We investigate the estimation sensitivity of gravitational acceleration using
squeezed probe states within a quantum metrology framework. In particular, we
analyze how the squeezing phase, beyond its amplitudes, of the probes affects
the attainable precision. We find that probes squeezed along the canonical
phase-space quadrature can fail to achieve a quantum Fisher information (QFI)
surpassing the shot-noise limit, regardless of the interaction time with the
gravitational field. In contrast, position-momentum correlated input states
with the squeezing amplitude can overcome this limit. Furthermore, we show that
optimal sensitivity is attained through projective momentum measurements
combined with a time-dependent adjustment of the squeezing phase. Our results
are important to highlight the fundamental role of phase-engineered squeezing
in experimental gravimetry protocols.

</details>


### [279] [Sequential Quantum Measurements and the Instrumental Group Algebra](https://arxiv.org/abs/2510.13980)
*Christopher S. Jackson*

Main category: quant-ph

TL;DR: 该论文提出了一种新的量子测量理论框架，用于处理传统方法难以处理的基本可观测量（如位置、动量、相点和自旋方向）。


<details>
  <summary>Details</summary>
Motivation: 传统量子测量理论（基于正交投影公设）无法处理某些基本可观测量的测量。

Method: 引入了连续时间测量理论，并定义了“仪器群”（IG）和“克劳斯算子密度”（KOD）。KOD 遵循经典的柯尔莫哥洛夫方程演化。将连续测量推广到顺序测量，证明了仪器组合的结构对应于 KOD 的卷积，从而形成“仪器群代数”（IGA）。IGA 是一种结合了巴拿赫代数和 C*-代数理论的结构。

Result: 提出了 KOD 及其演化方程（柯尔莫哥洛夫方程），并将其与现有理论（如林德伯格主方程）联系起来。建立了仪器群代数（IGA）作为 KOD 的理论基础，并定义了“超算子”（ultraoperators）来处理 IGA 上的算子。推导了跳跃过程和扩散过程的 KOD 柯尔莫哥洛夫生成元。

Conclusion: 新的理论框架（基于 IG 和 IGA）为处理难以测量的量子可观测量提供了坚实的基础，并揭示了量子测量中不同数学结构之间的深刻联系。

Abstract: Many of the most fundamental observables | position, momentum, phase-point,
and spin-direction | cannot be measured by an instrument that obeys the
orthogonal projection postulate. Continuous-in-time measurements provide the
missing theoretical framework to make sense of such observables. The elements
of the time-dependent instrument define a group called the \emph{instrumental
group} (IG). Relative to the IG, all of the time-dependence is contained in a
certain function called the \emph{Kraus-operator density} (KOD), which evolves
according to a classical Kolmogorov equation. Unlike the Lindblad master
equation, the KOD Kolmogorov equation is a direct expression of how the
elements of the instrument (not just the total channel) evolve. Shifting from
continuous measurement to sequential measurements more generally, the structure
of combining instruments in sequence is shown to correspond to the convolution
of their KODs. This convolution promotes the IG to an \emph{involutive Banach
algebra} (a structure that goes all the way back to the origins of POVM and
C*-algebra theory) which will be called the \emph{instrumental group algebra}
(IGA). The IGA is the true home of the KOD, similar to how the dual of a von
Neumann algebra is the home of the density operator. Operators on the IGA,
which play the same role for KODs as superoperators play for density operators,
are called \emph{ultraoperators} and various examples are discussed. Certain
ultraoperator-superoperator intertwining relations are considered, including
the relation between the KOD Kolmogorov equation and the Lindblad master
equation. The IGA is also shown to have actually two involutions: one respected
by the convolution ultraoperators and the other by the quantum channel
superoperators. Finally, the KOD Kolmogorov generators are derived for jump
processes and more general diffusive processes.

</details>


### [280] [A Rigorous Quantum Framework for Inequality-Constrained and Multi-Objective Binary Optimization](https://arxiv.org/abs/2510.13983)
*Sebastian Egginger,Kristina Kirova,Sonja Bruckner,Stefan Hillmich,Richard Kueng*

Main category: quant-ph

TL;DR: 将包含不等式约束的组合优化问题转化为多目标问题，并提出一种名为MOQA的新框架来解决它。


<details>
  <summary>Details</summary>
Motivation: 在量子优化中，处理组合优化问题的约束，特别是不等式约束，是一个主要的挑战。

Method: 将包含不等式约束的优化问题转化为多目标优化问题，并提出一种名为MOQA（Multi-Objective Quantum Approximation）的框架。该框架通过使用较小的p-范数来近似最大值，并提供了严格的性能保证。MOQA直接在哈密顿量层面操作，可与地面态求解器（如量子绝热退火、QAOA或虚时演化）兼容，并且不限于二次函数。

Result: 提出了一种名为MOQA的多目标量子近似框架，该框架能够处理包含不等式约束的组合优化问题，并具有严格的性能保证。

Conclusion: 研究表明，将包含不等式约束的组合优化问题转化为多目标问题是可行的，并且提出的MOQA框架为解决这类问题提供了一种有效的方法。

Abstract: Encoding combinatorial optimization problems into physically meaningful
Hamiltonians with tractable energy landscapes forms the foundation of quantum
optimization. Numerous works have studied such efficient encodings for the
class of Quadratic Unconstrained Binary Optimization (QUBO) problems. However,
many real-world tasks are constrained, and handling equality and, in
particular, inequality constraints on quantum computers remains a major
challenge. In this letter, we show that including inequality constraints is
equivalent to solving a multi-objective optimization. This insight motivates
the Multi-Objective Quantum Approximation (MOQA) framework, which approximates
the maximum via smaller $p$-norms and comes with rigorous performance
guarantees. MOQA operates directly at the Hamiltonian level and is compatible
with, but not restricted to, ground-state solvers such as quantum adiabatic
annealing, the Quantum Approximate Optimization Algorithm (QAOA), or
imaginary-time evolution. Moreover, it is not limited to quadratic functions.

</details>


### [281] [A Rigorous Quantum Framework for Inequality-Constrained and Multi-Objective Binary Optimization: Quadratic Cost Functions and Empirical Evaluations](https://arxiv.org/abs/2510.13987)
*Sebastian Egginger,Kristina Kirova,Sonja Bruckner,Stefan Hillmich,Richard Kueng*

Main category: quant-ph

TL;DR: MOQA框架提出了新的方法来处理多目标QUBO问题，并将其转化为可处理的伊辛模型，从而为量子和受量子启发的优化方法开辟了新的应用前景。


<details>
  <summary>Details</summary>
Motivation: 现有的量子优化方法在处理多目标QUBO问题时存在局限性，需要新的方法来映射问题到可处理的量子能量景观。

Method: 提出了一种名为MOQA（Multi-Objective Quantum Approximations）的框架，该框架能够将多种QUBO问题的最大值映射到可处理的伊辛类型哈密顿量，从而将经典二元优化问题转化为基态问题。

Result: MOQA框架能够处理包括路由、划分和不等式约束的二元优化问题，为解决实际应用中的多种问题提供了新的量子和受量子启发的解决方案。

Conclusion: MOQA框架为解决一类新的经典二元优化问题提供了新的量子和受量子启发的解决方案，具有广泛的应用前景。

Abstract: The prospect of quantum solutions for complicated optimization problems is
contingent on mapping the original problem onto a tractable quantum energy
landscape, e.g. an Ising-type Hamiltonian. Subsequently, techniques like
adiabatic optimization, quantum annealing, and the Quantum Approximate
Optimization Algorithm (QAOA) can be used to find the ground state of this
Hamiltonian. Quadratic Unconstrained Binary Optimization (QUBO) is one
prominent problem class for which this entire pipeline is well understood and
has received considerable attention over the past years. In this work, we
provide novel, tractable mappings for the maxima of multiple QUBO problems.
Termed Multi-Objective Quantum Approximations, or MOQA for short, our framework
allows us to recast new types of classical binary optimization problems as
ground state problems of a tractable Ising-type Hamiltonian. This, in turn,
opens the possibility of new quantum- and quantum-inspired solutions to a
variety of problems that frequently occur in practical applications. In
particular, MOQA can handle various types of routing and partitioning problems,
as well as inequality-constrained binary optimization problems.

</details>


### [282] [Continuous-variable photonic quantum extreme learning machines for fast collider-data selection](https://arxiv.org/abs/2510.13994)
*Benedikt Maier,Michael Spannowsky,Simon Williams*

Main category: quant-ph

TL;DR: 连续变量光量子极限学习机可作为粒子对撞机数据处理的快速、低开销前端，通过高斯量子基板和高维随机特征映射实现。


<details>
  <summary>Details</summary>
Motivation: 研究连续变量光量子极限学习机作为粒子对撞机数据处理的快速、低开销前端。

Method: 数据被编码到光子模式中，通过固定时间的、高斯量子基板进行传播，并使用高斯兼容测量产生高维随机特征映射。仅训练线性分类器。

Result: 在顶喷注标记和希格斯玻色子识别任务上，光量子极限学习机（QELM）优于具有两个隐藏单元的多层感知机（MLP），并在大数据集上可匹敌或超越具有十个隐藏单元的MLP。

Conclusion: 高斯光量子极限学习机能在固定延迟下提供紧凑且富有表现力的随机特征，结合其确定性时序、快速再训练、低光功率和室温运行等优点，可作为未来粒子对撞机实验在线数据选择甚至一级触发集成的重要组成部分。

Abstract: We study continuous-variable photonic quantum extreme learning machines as
fast, low-overhead front-ends for collider data processing. Data is encoded in
photonic modes through quadrature displacements and propagated through a
fixed-time Gaussian quantum substrate. The final readout occurs through
Gaussian-compatible measurements to produce a high-dimensional random feature
map. Only a linear classifier is trained, using a single linear solve, so
retraining is fast, and the optical path and detector response set the
analytical and inference latency. We evaluate this architecture on two
representative classification tasks, top-jet tagging and Higgs-boson
identification, with parameter-matched multi-layer perceptron (MLP) baselines.
Using standard public datasets and identical train, validation, and test
splits, the photonic Quantum Extreme Learning Machine (QELM) outperforms an MLP
with two hidden units for all considered training sizes, and matches or exceeds
an MLP with ten hidden units at large sample sizes, while training only the
linear readout. These results indicate that Gaussian photonic extreme-learning
machines can provide compact and expressive random features at fixed latency.
The combination of deterministic timing, rapid retraining, low optical power,
and room temperature operation makes photonic QELMs a credible building block
for online data selection and even first-stage trigger integration at future
collider experiments.

</details>


### [283] [Qutrits for physics at the LHC](https://arxiv.org/abs/2510.14001)
*Miranda Carou Laiño,Veronika Chobanova,Miriam Lucio Martínez*

Main category: quant-ph

TL;DR: qutrit量子机器学习模型在寻找新物理学信号方面可能比传统量子计算机更有效。


<details>
  <summary>Details</summary>
Motivation: 未来的高亮度大型强子对撞机（HL-LHC）将产生大量数据，给数据处理、信号重建和分析带来了挑战，需要新的方法来寻找超出标准模型的新物理学现象。

Method: 提出一种基于qutrit的量子机器学习模型，并与基于qubit的方法进行比较，评估其准确性、可扩展性和计算效率。

Result: 研究qutrit量子模型在解决未来对撞机实验的计算和分析需求方面是否具有优势。

Conclusion: qutrit量子机器学习模型在处理高能物理数据以检测异常现象方面可能比基于qubit的模型更具优势。

Abstract: The identification of anomalous events, not explained by the Standard Model
of particle physics, and the possible discovery of exotic physical phenomena
pose significant theoretical, experimental and computational challenges. The
task will intensify at next-generation colliders, such as the High- Luminosity
Large Hadron Collider (HL-LHC). Consequently, considerable challenges are
expected concerning data processing, signal reconstruction, and analysis. This
work explores the use of qutrit- based Quantum Machine Learning models for
anomaly detection in high-energy physics data, with a focus on LHC
applications. We propose the development of a qutrit quantum model and
benchmark its performance against qubit-based approaches, assessing accuracy,
scalability, and computational efficiency. This study aims to establish whether
qutrit architectures can offer an advantage in addressing the computational and
analytical demands of future collider experiments.

</details>


### [284] [Decoding Correlated Errors in Quantum LDPC Codes](https://arxiv.org/abs/2510.14060)
*Arshpreet Singh Maan,Francisco-Garcia Herrero,Alexandru Paler,Valentin Savin*

Main category: quant-ph

TL;DR: 提出了一种用于量子LDPC码中相关错误的解码框架，通过GARI方法消除4-环路Y型错误，并在增强图上使用归一化min-sum解码器和集成解码，实现了高精度和低延迟，尤其在距离12码上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 在量子LDPC码中处理电路级噪声下的相关错误，并提高解码精度和效率。

Method: 提出GARI（图增强和重布线以干扰）方法，消除4-环路Y型错误，并结合归一化min-sum解码器和集成解码。

Result: 在距离6、10、12的双变量自行车码上实现了高精度和低延迟的解码。距离12码在10^-3的物理错误率下，逻辑错误率达到(6.70±1.93)×10^-9。FPGA实现显示，平均解码延迟为273ns，99.99%的实例延迟低于1微秒。

Conclusion: 所提出的解码框架（GARI方法结合归一化min-sum和集成解码）在量子LDPC码的电路级噪声下实现了高精度和低延迟，并且具有实现实时处理的潜力。

Abstract: We introduce a decoding framework for correlated errors in quantum LDPC codes
under circuit-level noise. The core of our approach is a graph augmentation and
rewiring for interference (GARI) method, which modifies the correlated detector
error model by eliminating 4-cycles involving Y-type errors, while preserving
the equivalence of the decoding problem. We test our approach on the bivariate
bicycle codes of distances 6, 10, and 12. A normalized min-sum decoder with a
hybrid serial-layered schedule is applied on the transformed graph, achieving
high accuracy with low latency. Performance is further enhanced through
ensemble decoding, where 24 randomized normalized min-sum decoders run in
parallel on the transformed graph, yielding the highest reported accuracy (on
par with XYZ-Relay-BP) with unprecedented speed for the tested codes under
uniform depolarizing circuit level noise. For the distance 12 (gross) code, our
approach yields a logical error rate of $(6.70 \pm 1.93) \times 10^{-9}$ at a
practical physical error rate of $10^{-3}$. Furthermore, preliminary FPGA
implementation results show that such high accuracy can be achieved in real
time, with a per-round average decoding latency of 273 ns and sub-microsecond
latency in 99.99% of the decoding instances.

</details>


### [285] [Quantum Search in Superposed Quantum Lattice Gas Automata and Lattice Boltzmann Systems](https://arxiv.org/abs/2510.14062)
*Călin A. Georgescu,Matthias Möller*

Main category: quant-ph

TL;DR: 量子计算在计算流体动力学（CFD）领域具有潜力，但现有量子格子方法（QLGA和QLBM）主要集中在模型开发而非应用，限制了其实际用途。本文提出了一种基于离散优化和量子搜索的新应用，通过同时模拟多种格子配置并利用幅度估计和量子搜索，避免了流场测量，并有望提供渐进式量子优势。


<details>
  <summary>Details</summary>
Motivation: 随着计算流体动力学（CFD）问题规模的扩大，探索量子计算的优势变得日益重要。现有的量子格子气体（QLGA）和量子格子玻尔兹曼方法（QLBM）虽然在模型开发上取得进展，但缺乏实际应用，主要局限于量子态层析和可观测量测量，这可能抵消潜在的量子优势。

Method: 提出了一种基于离散优化和量子搜索的新应用，能够同时模拟多种格子配置，并利用幅度估计和量子搜索来实现渐进式量子优势，从而避免了对流场进行测量的需要。

Result: 通过详细的门级电路复杂度分析，并考虑了不同编码方案的优缺点，证明了所提出的方法在理论上具有提供渐进式量子优势的潜力。

Conclusion: 本文提出的基于离散优化和量子搜索的应用，为QLGA和QLBM在CFD领域的实际应用开辟了新的途径，通过避免流场测量并利用量子搜索技术，有望实现真正的量子优势。

Abstract: As the scope of Computational Fluid Dynamics (CFD) grows to encompass ever
larger problem scales, so does the interest in whether quantum computing can
provide an advantage. In recent years, Quantum Lattice Gas Automata (QLGA) and
Quantum Lattice Boltzmann Methods (QLBM) have emerged as promising candidates
for quantum-native implementations of CFD solvers. Though the progress in
developing QLGA and QLBM algorithms has been significant, it has largely
focused on the development of models rather than applications. As a result, the
zoo of QLGA and QLBM algorithms has grown to target several equations and to
support many extensions, but the practical use of these models is largely
limited to quantum state tomography and observable measurement. This limitation
is crucial in practice, because unless very specific criteria are met, such
measurements may cancel out any potential quantum advantage. In this paper, we
propose an application based on discrete optimization and quantum search, which
circumvents flow field measurement altogether. We propose methods for
simulating many different lattice configurations simultaneously and describe
how the usage of amplitude estimation and quantum search can provide an
asymptotic quantum advantage. Throughout the paper, we provide detailed
complexity analyses of gate-level implementations of our circuits and consider
the benefits and costs of several encodings.

</details>


### [286] [Quantum Low-Density Parity-Check Codes](https://arxiv.org/abs/2510.14090)
*Bane Vasic,Valentin Savin,Michele Pacenti,Shantom Borah,Nithin Raveendran*

Main category: quant-ph

TL;DR: QLDPC码在量子计算中用于纠错，具有低开销和高容错性的潜力，但仍面临硬件兼容性和解码挑战。


<details>
  <summary>Details</summary>
Motivation: QLDPC码在量子计算中具有低开销和高容错性的潜力，有望突破现有量子纠错的瓶颈，因此需要深入研究。

Method: 探讨QLDPC码的理论基础、量子信道的特性、关键码的构造以及解码算法。

Result: QLDPC码已在理论和实践中取得显著进展，尤其在构建低开销容错量子计算方面。

Conclusion: QLDPC码是量子信息科学的重要方向，但仍需克服硬件限制和解码效率的挑战，以充分发挥其潜力。

Abstract: Quantum error correction (QEC) is a cornerstone of quantum computing,
enabling reliable information processing in the presence of noise. Sparse
stabilizer codes -- referred to generally as quantum low-density parity-check
(QLDPC) codes -- have risen to the forefront of QEC research in recent years.
This can be attributed to several key factors. First, classical LDPC codes
admit low-complexity belief propagation iterative decoding and near-capacity
performance, which contributed to the early interest in QLDPC codes. Then, the
result promising constant overhead fault tolerance using QLDPC codes led to the
search for code families that go beyond the long-holding $\sqrt{n}$ scaling
barrier of minimum distance for codelength $n$. This resulted in recent
breakthroughs in the construction of QLDPC codes, which, combined with
efficient decoding algorithms and the development of fault-tolerant protocols
operating on QLDPC-encoded quantum information, provide a promising pathway to
low-overhead, fault-tolerant quantum computation. However, despite their
potential, challenges remain, particularly in constructing and decoding
finite-length codes that account for, or efficiently leverage, specific
characteristics of quantum hardware, such as connectivity, topology, native
gate sets, and noise models. This article provides an in-depth examination of
QLDPC codes and their iterative decoders, catering to an information theory
audience with no or limited background in quantum mechanics. We discuss the
theoretical underpinnings, explore unique characteristics of quantum channels,
and delineate key code constructions and decoding algorithms, ultimately
highlighting the impact and future prospects of QLDPC codes in quantum
information science.

</details>


### [287] [Quantum machine learning and quantum-inspired methods applied to computational fluid dynamics: a short review](https://arxiv.org/abs/2510.14099)
*Cesar A. Amaral,Vinícius L. Oliveira,Juan P. L. C. Salazar,Eduardo I. Duzzioni*

Main category: quant-ph

TL;DR: 量子计算和张量网络方法有望解决计算流体动力学（CFD）的性能瓶颈，尤其是在高维、多尺度和湍流条件下。


<details>
  <summary>Details</summary>
Motivation: 传统CFD方法在高维、多尺度和湍流条件下存在可扩展性挑战，成本高昂。

Method: 本篇综述探讨了量子计算、量子算法、机器学习和张量网络技术在CFD中的应用，包括变分量子算法（VQA）、量子神经网络（QNN）、量子物理信息神经网络（QPINN）以及张量网络方法。

Result: 研究表明，量子和量子启发的方法（如张量网络）在内存和运行时间上可减少几个数量级，同时保持精度。

Conclusion: 虽然完全的量子CFD在NISQ时代尚不可行，但量子启发式张量网络已展现出实际优势，混合方法是近期最有前景的策略。

Abstract: Computational Fluid Dynamics (CFD) is central to science and engineering, but
faces severe scalability challenges, especially in high-dimensional,
multiscale, and turbulent regimes. Traditional numerical methods often become
prohibitively expensive under these conditions. Quantum computing and
quantum-inspired methods have been investigated as promising alternatives. This
review surveys advances at the intersection of quantum computing, quantum
algorithms, machine learning, and tensor network techniques for CFD. We discuss
the use of Variational Quantum Algorithms as hybrid quantum-classical solvers
for PDEs, emphasizing their ability to incorporate nonlinearities through
Quantum Nonlinear Processing Units. We further review Quantum Neural Networks
and Quantum Physics-Informed Neural Networks, which extend classical machine
learning frameworks to quantum hardware and have shown advantages in parameter
efficiency and solution accuracy for certain CFD benchmarks. Beyond quantum
computing, we examine tensor network methods, originally developed for quantum
many-body systems and now adapted to CFD as efficient high-dimensional
compression and solver tools. Reported studies include several orders of
magnitude reductions in memory and runtime while preserving accuracy. Together,
these approaches highlight quantum and quantum-inspired strategies that may
enable more efficient CFD solvers. This review closes with perspectives:
quantum CFD remains out of reach in the NISQ era, but quantum-inspired tensor
networks already show practical benefits, with hybrid approaches offering the
most promising near-term strategy.

</details>


### [288] [Symmetry-protected states of interacting qubits in superconducting quantum circuits](https://arxiv.org/abs/2510.14121)
*Yi Shi,Eran Ginossar,Michael Stern,Marzena Szymanska*

Main category: quant-ph

TL;DR: 该研究提出了一种新的超导电路量子比特模型，利用对称性保护，使其对环境噪声具有鲁棒性，有望实现毫秒级的相干时间。


<details>
  <summary>Details</summary>
Motivation: 超导电路是量子信息存储和操纵的有希望的候选者，但需要内在的噪声保护。该研究旨在开发一种能够抵抗局部扰动（包括弛豫和退相干）的量子比特。

Method: 提出一个相互作用的自旋模型，该模型至少需要四个自旋以及最近邻和次近邻耦合。计算态由最低的两个本征态形成对称性保护的量子比特流形。然后将该自旋模型映射到超导电路，并模拟其在现实环境噪声下的性能。

Result: 所提出的模型可以实现超过毫秒级的相干时间，这表明该量子比特对弛豫和退相干具有鲁棒性。

Conclusion: 该工作为在新型量子设备中实现长相干时间的量子比特提供了一条新途径。

Abstract: Superconducting circuits are one of the leading candidates for storing and
manipulating quantum information. Among them, qubits embedded with intrinsic
noise protection have seen rapid advancements in recent years. This noise
protection is typically realized by isolating the computational states from
local sources of noise. Here, we propose an interacting spin model that
requires at least four spins with nearest-neighbor and next-nearest-neighbor
couplings, where the two lowest eigenstates form a symmetry-protected qubit
manifold, which is robust to both relaxation and dephasing from local
perturbations. We map the spin model to a superconducting circuit and show that
such a circuit can reach coherence times exceeding several milliseconds in the
presence of realistic environmental noise. Our work opens a pathway to
realizing qubits with long coherence times in a new generation of quantum
devices.

</details>


### [289] [Universal energy-space localization and stable quantum phases against time-dependent perturbations](https://arxiv.org/abs/2510.14160)
*Hongye Yu,Tzu-Chieh Wei*

Main category: quant-ph

TL;DR: 在 q-局域哈密顿量中存在一种普遍的能量空间局域化现象，即使在任意单调时间重缩放下也能抵抗时间的扰动，从而证明了其稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究量子系统在时间依赖性扰动下的稳定性，尤其是在能量空间中的稳定性。

Method: 通过识别 q-局域哈密顿量中的能量空间局域化现象，并证明其在时间依赖性扰动下具有鲁棒性。

Result: 能量空间局域化现象可以抵抗时间依赖性扰动，并且在自旋玻璃模型、LDPC码和具有聚集解空间的经典优化问题中具有应用。

Conclusion: 能量空间局域化为分析非平衡量子动力学、证明稳定性和设计量子算法提供了新的视角和数学工具。

Abstract: Stability against perturbation is a highly nontrivial property of quantum
systems and is often a requirement to define new phases. In most systems where
stability can be rigorously established, only static perturbations are
considered; whether a system is stable against generic time-dependent
perturbations remains largely elusive. In this work, we identify a universal
phenomenon in $q$-local Hamiltonians called energy-space localization and prove
that it can survive under generic time-dependent perturbations, where the
evolving state is exponentially localized in an energy window of the
instantaneous spectrum. The property holds ubiquitously, and the leakage bounds
remain invariant under arbitrarily monotonic rescaling of evolution time. This
flexibility enables the energy-space localization to be a powerful tool in
proving the stability of systems. For spin glass models where the configuration
spaces are separated by large energy barriers, the localization in energy space
can induce a true localization in the configuration space and robustly break
ergodicity. We then demonstrate the applications of our results in several
systems with such barriers. For certain LDPC codes, we show that the evolving
state is localized near the original codeword for an exponentially long time
even under generic time-dependent perturbations. We also extend the stability
of LDPC codes against static $q$-local perturbations to quasi-$q$-local. In
addition, we show that for some classical hard optimization problems with
clustered solution space, the stability becomes an obstacle for quantum
Hamiltonian-based algorithms to drive the system out of local minima. Our work
provides a new lens for analyzing the non-equilibrium dynamics of generic
quantum systems, and versatile mathematical tools for stability proving and
quantum algorithm design.

</details>


### [290] [Classification of Transuranium Elements in Terms of `Winding' Numbers in the Bohr-Sommerfeld Model](https://arxiv.org/abs/2510.14289)
*Sergei K. Suslov*

Main category: quant-ph

TL;DR: Bohr-Sommerfeld原子模型用于探索Uranium（Z=92）、Oganesson（Z=118）和假设的超重元素，发现强库仑场中会出现自相交轨道，可用“绕数”进行拓扑描述，连接了早期量子理论和现代超重元素物理。


<details>
  <summary>Details</summary>
Motivation: 探索Uranium（Z=92）、Oganesson（Z=118）及更重的假设超重元素的氢类离子

Method: 使用Sommerfeld精细结构公式和计算机代数方法

Result: 在强库仑场中，从Oganesson开始（Z≤137），出现自相交轨道，可以用“绕数”进行拓扑分类。

Conclusion: 该方法展示了早期量子理论与现代超重元素物理之间的概念联系。

Abstract: We revisit the Bohr-Sommerfeld atomic model to explore hydrogen-like ions of
Uranium ($Z=92$), Oganesson ($Z=118$), and hypothetical superheavy elements
beyond. Although superseded by the Dirac equation and modern quantum
electrodynamics, the semiclassical approach offers a historically and
pedagogically valuable perspective. Using the Sommerfeld fine structure formula
and computer algebra methods, we demonstrate the appearance of
self-intersecting orbits in super strong Coulomb fields, beginning with
Oganesson and hypothetical elements up to $Z\le137$. These orbits can be
classified by their `winding' numbers, providing a simple topological
description of Coulomb field strength in this framework. Our results highlight
a conceptual bridge between early quantum theory and modern superheavy element
physics.

</details>


### [291] [Quantifiers of Noise Reducibility Under Restricted Control](https://arxiv.org/abs/2510.14316)
*Graeme D. Berk,Kavan Modi,Simon Milz*

Main category: quant-ph

TL;DR: 研究了量子过程的关联结构（量子梳），并引入了量化其效用的方法。


<details>
  <summary>Details</summary>
Motivation: 量子梳作为量子信息协议和控制任务的重要资源，其效用量化具有实际意义。

Method: 引入满足单调性的量子过程效用量化器，并将其应用于开放-回路控制下的量子过程噪声消除问题，研究其资源组合行为，并与广义梳散度联系起来。

Result: 提出的量化器能够表示量子过程能够展示的最大时间互信息量，并重新解释了先前关于动力学解耦和非马尔可夫内存关系的研究发现。

Conclusion: 先前关于动力学解耦和非马尔可夫内存关系的研究结论——将动力学解耦解释为资源蒸馏——仍然成立，尽管先前使用的资源量化器存在不足。

Abstract: The correlation structure of multitime quantum processes - succinctly
described by quantum combs - is an important resource for many quantum
information protocols and control tasks. Inspired by approaches for quantum
states, we introduce quantifiers of the practical utility of quantum processes
that satisfy monotonicity properties, thus overcoming shortcomings in previous
state-motivated approaches. Applying these quantifiers to the problem of noise
reduction of a quantum process under open-loop control, they are shown to
represent the largest amount of temporal mutual information that a process can
possibly exhibit. In addition, we study their resource composition behaviour
and connect them to the recently introduced notion of generalised comb
divergences. Finally, in light of these new quantifiers, we re-interpret the
numerical findings of npj Quantum Information 9, 104 (2023) on the relationship
of dynamical decoupling and non-Markovian memory, which were based on
insufficient resource quantifiers, and show that its main conclusion - the
interpretation of dynamical decoupling as a resource distillation - still
holds.

</details>


### [292] [Offline Dedicated Quantum Attacks on Block Ciphers Based on Two Parallel Permutation-Based Pseudorandom Functions](https://arxiv.org/abs/2510.14475)
*Xiao-Fan Zhen,Zhen-Qiang Li,Jia-Cheng Fan,Su-Juan Qin,Fei Gao*

Main category: quant-ph

TL;DR: 本研究提出了针对XOR型函数的量子攻击新结构，并改进了其攻击方式，降低了资源需求。


<details>
  <summary>Details</summary>
Motivation: 评估加密系统在量子计算威胁下面临的安全风险，特别是针对XOR型函数和基于TPP-PRFs的块密码。

Method: 1. 发现适用于Shi等人量子攻击的新密码结构：PolyMAC和基于TPP-PRFs的块密码（XopEM, SoEM22, SUMPIP, DS-SoEM）。2. 克服在线查询的限制，构建解耦的XOR型函数，提出针对基于TPP-PRFs的块密码的离线量子攻击方法。

Result: 1. 成功将Shi等人的量子攻击扩展到新的密码结构。2. 提出的离线量子攻击将基于TPP-PRFs的块密码的查询复杂度从量子查询模型下的$	ilde O(2^{(n+t)/2})$降低到$	ilde O(2^{t})$，同时保持时间复杂度不变。3. 在经典查询模型下，将查询复杂度与时间复杂度从$	ilde O(2^{(2n)/3})$优化到$	ilde O(2^{(2n-t)/3})$。

Conclusion: 本研究不仅扩展了现有量子攻击的应用范围，还通过提出离线攻击方法显著提高了攻击效率，为评估和设计抗量子密码系统提供了新的见解。

Abstract: Quantum cryptanalysis is essential for evaluating the security of
cryptographic systems against the threat of quantum computing. Recently, Shi et
al. introduced the dedicated quantum attack on XOR-type function that greatly
reduces the required resources (including circuit depth, width, and the number
of gates) compared to the parallel Grover-meets-Simon algorithm. Here, our
contribution is in two aspects. On the one hand, we discover new cryptographic
structures amenable to this attack: PolyMAC and block ciphers based on two
parallel permutation-based pseudorandom functions (TPP-PRFs), including XopEM,
SoEM22, SUMPIP, and DS-SoEM, partially answering Shi et al.'s open question. On
the other hand, for block ciphers based on TPP-PRFs, we break the obstacle that
this attack rely on online query by constructing decoupled XOR-type function,
then propose an offline quantum attack on them. Compared to previous results,
our offline attack exhibits significantly reduced query complexity.
Specifically, we reduce the number of queries to the encryption oracle from
$\tilde O(2^{(n+t)/2})$ to $\tilde O(2^{t})$ with the same time complexity in
the quantum query model, and enable its implementation in the classical query
model, optimizing both the classical query complexity and time complexity from
$\tilde O(2^{(2n)/3})$ to $\tilde O(2^{(2n-t)/3})$.

</details>


### [293] [Transferable Equivariant Quantum Circuits for TSP: Generalization Bounds and Empirical Validation](https://arxiv.org/abs/2510.14533)
*Monit Sharma,Hoong Chuin Lau*

Main category: quant-ph

TL;DR: 通过在量子强化学习（QRL）中引入等变量子电路（EQC）来解决组合优化问题（特别是旅行商问题 TSP）的泛化挑战，实现了在不同规模 TSP 问题之间进行零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的 QRL 方法在处理大规模 TSP 问题时存在训练不可行的限制，导致其仅能应用于小规模问题。本研究旨在克服这一限制，实现 QRL 在 TSP 上的泛化能力。

Method: 利用等变量子电路（EQC）来利用 TSP 图的排列对称性。推导了用于迁移设置的新颖泛化边界，该边界包含一个量化 TSP 问题在不同节点数量之间结构差异的项。

Result: 在小规模 TSP 问题上训练的 EQC 策略在更大的 TSP 问题上展现了强大的零样本迁移性能，并且在经过微调后性能得到进一步提升，这与经典计算机上观察到的跨尺度正迁移现象一致。

Conclusion: 将排列对称性嵌入量子模型是实现可扩展 QRL 解决方案的关键，尤其是在组合优化任务中。等变性在可迁移的量子学习中起着至关重要的作用。

Abstract: In this work, we address the challenge of generalization in quantum
reinforcement learning (QRL) for combinatorial optimization, focusing on the
Traveling Salesman Problem (TSP). Training quantum policies on large TSP
instances is often infeasible, so existing QRL approaches are limited to
small-scale problems. To mitigate this, we employed Equivariant Quantum
Circuits (EQCs) that respect the permutation symmetry of the TSP graph.
  This symmetry-aware ansatz enabled zero-shot transfer of trained parameters
from $n-$city training instances to larger m-city problems. Building on recent
theory showing that equivariant architectures avoid barren plateaus and
generalize well, we derived novel generalization bounds for the transfer
setting. Our analysis introduces a term quantifying the structural
dissimilarity between $n-$ and $m-$node TSPs, yielding an upper bound on
performance loss under transfer. Empirically, we trained EQC-based policies on
small $n-$city TSPs and evaluated them on larger instances, finding that they
retained strong performance zero-shot and further improved with fine-tuning,
consistent with classical observations of positive transfer between scales.
These results demonstrate that embedding permutation symmetry into quantum
models yields scalable QRL solutions for combinatorial tasks, highlighting the
crucial role of equivariance in transferable quantum learning.

</details>


### [294] [Polaritons confined in dielectric structures](https://arxiv.org/abs/2510.14566)
*Amir Rahmani,Dogyun Ko,Maciej Dems,Andrzej Opala,Michał Matuszewski*

Main category: quant-ph

TL;DR: 文章提出了一种基于Bogoliubov变换和第三方量化技术来构建量子模型的方法，以克服传统Hopfield模型的局限性，并能在纳米结构中增强相互作用强度和设计非局域多体相互作用，从而实现强非经典光关联。


<details>
  <summary>Details</summary>
Motivation: 传统Hopfield模型在处理强量子耦合时存在局限性，即其描述的本征模式形状在相互作用下会发生改变，且模型参数通常需要拟合实验数据，导致理论与实际不符。需要一种能确定特定介电结构量子主方程的直接方法。

Method: 提出了一种在极化子本征模式基础上构建量子模型的方法。在保守情况下，使用Bogoliubov变换；在耗散情况下，使用第三方量化技术。

Result: 该方法可以用于增强相互作用强度，并在精心设计的纳米结构中工程化非局域多体相互作用，产生强非经典的发射光关联。

Conclusion: 所提出的方法克服了传统模型的局限性，为在特定介电结构下构建量子模型提供了新的途径，并在纳米科学领域具有潜在应用价值。

Abstract: Light-matter interaction in the regime of strong quantum coupling is usually
treated within the framework of the Hopfield model. However, the picture of
coupling well-defined modes of light and matter is correct only as long as the
shapes of these eigenmodes are not substantially modified by the interaction.
Moreover, parameters of theoretical models are usually obtained by fitting to
experimental data. To date, there has been no straightforward method to
determine a quantum master equation corresponding to a system with specific
dielectric structure, which may lead to incompatibility of theoretical
descriptions and physical realizations. We present a recipe for obtaining a
quantum model in the polariton eigenmode basis based on Bogoliubov
transformation in the conservative case and third quantization technique in the
dissipative case. We show how this method can be used for boosting interaction
strength and engineering nonlocal many-body interactions in carefully designed
nanostructures, resulting in strongly nonclassical correlations of emitted
light.

</details>


### [295] [Quantum Reinforcement Learning: Recent Advances and Future Directions](https://arxiv.org/abs/2510.14595)
*Jawaher Kaldari,Shehbaz Tariq,Saif Al-Kuwari,Samuel Yen-Chi Chen,Symeon Chatzinotas,Hyundong Shin*

Main category: quant-ph

TL;DR: 量子强化学习（QRL）是量子机器学习的一个有前景但未被充分探索的领域，本综述对其最新进展进行了调查。


<details>
  <summary>Details</summary>
Motivation: 量子强化学习（QRL）作为一个有前景但未被充分探索的领域，值得深入研究其在量子机器学习中的潜力。

Method: 对QRL框架，包括其算法、架构和支持的SDK，以及在不同领域的应用进行了全面的分析。

Result: QRL在量子和经典领域都显示出独特的优势和横向适用性，尽管它得到的关注少于其他量子机器学习方法。

Conclusion: QRL面临挑战但充满机遇，其潜在的应用有望推动量子启发强化学习的创新，并促进其在跨学科背景下的应用。

Abstract: As quantum machine learning continues to evolve, reinforcement learning
stands out as a particularly promising yet underexplored frontier. In this
survey, we investigate the recent advances in QRL to assess its potential in
various applications. While QRL has generally received less attention than
other quantum machine learning approaches, recent research reveals its distinct
advantages and transversal applicability in both quantum and classical domains.
We present a comprehensive analysis of the QRL framework, including its
algorithms, architectures, and supporting SDK, as well as its applications in
diverse fields. Additionally, we discuss the challenges and opportunities that
QRL can unfold, highlighting promising use cases that may drive innovation in
quantum-inspired reinforcement learning and catalyze its adoption in various
interdisciplinary contexts.

</details>


### [296] [Single-shot antidistinguishability of unitary operations](https://arxiv.org/abs/2510.14609)
*Satyaki Manna,Anandamay Das Bhowmik*

Main category: quant-ph

TL;DR: 本篇论文研究了量子信道中的单次反区分度，特别关注了酉运算。


<details>
  <summary>Details</summary>
Motivation: 填补量子信道反区分度研究的空白，并探索了不同探测器（单系统、纠缠态）在区分酉运算时的作用。

Method: 研究了单系统探测器和纠缠态探测器在区分三酉群时的反区分度，并推导了维度与探测器性能之间的关系。

Result: 证明了在量子比特情况下，最大纠缠态探测器足以实现反区分度；但在三维情况下，最大纠缠态探测器并非总是充分的，存在某些酉群仅能被非最大纠缠态或单系统探测器反区分。此外，还证明了两个可反区分的酉群的并集仍是可反区分的，并提供了从不可反区分酉群构建可反区分酉群的方法。

Conclusion: 量子信道中的反区分度是一个值得研究的领域，维度是影响探测器性能的关键因素。

Abstract: The notion of antidistinguishability captures the possibility of ruling out
certain alternatives in a quantum experiment without identifying the actual
outcome. Although extensively studied for quantum states, the
antidistinguishability of quantum channels remains largely unexplored. In this
work, we investigate the single-shot antidistinguishability of unitary
operations. We analyse two scenarios: antidistinguishability with single-system
probes and with entangled probes. For sets of three unitaries, we first prove
that all maximally entangled states are equivalent in their performance as
probe. In the qubit case, we further establish that maximally entangled probes
are always sufficient: if a set of three qubit unitaries is antidistinguishable
with either a single-system or non-maximally entangled probe, then it is also
antidistinguishable with a maximally entangled one. However, in higher
dimension, this equivalence fails. In \textit{dimension 3}, there exists a set
of unitaries that are antidistinguishable with non-maximally entangled probe or
single-system probe but not with maximally entangled probe. We also establish
that union of two antidistinguishable sets of three qubit unitaries also forms
a set of antidistinguishable unitaries. Lastly, we provide methods to construct
antidistinguishable unitaries from non-antidistinguishable ones.

</details>


### [297] [Multiparameter quantum-enhanced adaptive metrology with squeezed light](https://arxiv.org/abs/2510.14739)
*Giorgio Minati,Enrico Urbani,Nicolò Spagnolo,Valeria Cimini,Fabio Sciarrino*

Main category: quant-ph

TL;DR: 本研究提出了一种自适应多参数估计策略，用于从头开始进行相位估计，能够在整个周期 $[0, rac{π}{2})$ 区间内实现低于标准量子极限的精度，且无需预先了解压缩参数。


<details>
  <summary>Details</summary>
Motivation: 现有的基于预校准压缩水平的相位估计方法容易受到不稳定性影响，且在实验条件波动时会变得次优。本研究旨在开发一种无需先验知识即可在整个参数范围内保持最优且能抵抗不稳定的自适应估计策略。

Method: 采用实时反馈机制，联合估计光学相位和压缩水平，以实现自校准。

Result: 在整个周期 $[0, rac{π}{2})$ 区间内实现了低于标准量子极限的精度，并证明了该方法对实验漂移和校准误差的鲁棒性。

Conclusion: 该自校准方案提供了一个可靠的量子增强传感框架，为实际应用和可扩展的分布式传感器网络开辟了新的途径。

Abstract: Squeezed light enables quantum-enhanced phase estimation, with crucial
applications in both fundamental physics and emerging technologies. To fully
exploit the advantage provided by this approach, estimation protocols must
remain optimal across the entire parameter range and resilient to instabilities
in the probe state. In this context, strategies that rely on pre-calibrated
squeezing levels are vulnerable to degradation over time and become sub-optimal
when experimental conditions fluctuate. Here, we develop an adaptive
multiparameter estimation strategy for ab-initio phase estimation, achieving
sub-standard quantum limit precision in the full periodicity interval
$[0,\pi)$, without relying on prior knowledge of the squeezing parameter. Our
approach employs real-time feedback to jointly estimate both the optical phase
and the squeezing level, ensuring robustness against experimental drifts and
calibration errors. This self-calibrating scheme establishes a reliable
quantum-enhanced sensing framework, opening new routes for practical scenarios
and scalable distributed sensor networks using squeezed light.

</details>


### [298] [Unsupervised Learning to Recognize Quantum Phases of Matter](https://arxiv.org/abs/2510.14742)
*Mehran Khosrojerdi,Alessandro Cuccoli,Paola Verrucchi,Leonardo Banchi*

Main category: quant-ph

TL;DR: 该研究采用无监督学习方法，通过计算量子态保真度来绘制多体系统的量子相图，并成功应用于自旋链模型。


<details>
  <summary>Details</summary>
Motivation: 绘制多体系统的量子相图可以被看作一个学习问题，需要根据分类标准对基态进行标记。本工作旨在利用无监督学习，在没有预先标记状态的情况下，确定多体系统的量子相图。

Method: 采用无监督学习算法，直接处理量子态。该算法根据量子态之间的保真度相似性标准，对不同哈密顿量参数下的基态构型进行分组，从而揭示最显著的分类方式。具体使用了基于谱聚类的无监督学习算法，并结合“轮廓系数”和“肘部法则”来确定最优相数。

Result: 通过对两个特定的自旋$rac{1}{2}$链进行基准测试，其中量子态通过张量网络技术确定。结果表明，所提出的无监督学习方法能够准确地重构出这些系统的相图。

Conclusion: 无监督学习能够自主识别并可能揭示新颖的量子物质相。

Abstract: Drawing the quantum phase diagram of a many-body system in the parameter
space of its Hamiltonian can be seen as a learning problem, which implies
labelling the corresponding ground states according to some classification
criterium that defines the phases. In this work we adopt unsupervised learning,
where the algorithm has no access to any priorly labeled states, as a tool for
determining quantum phase diagrams of many-body systems. The algorithm directly
works with quantum states: given the ground-state configurations for different
values of the Hamiltonian parameters, the process uncovers the most significant
way of grouping them based on a similarity criterion that refers to the
fidelity between quantum states, that can be easily estimated, even
experimentally. We benchmark our method with two specific spin-$\frac{1}{2}$
chains, with states determined via tensor network techniques. We find that
unsupervised learning algorithms based on spectral clustering, combined with
``silhouette'' and ``elbow'' methods for determining the optimal number of
phases, can accurately reproduce the phase diagrams. Our results show how
unsupervised learning can autonomously recognize and possibly unveil novel
phases of quantum matter.

</details>


### [299] [Spectral subspace extraction via incoherent quantum phase estimation](https://arxiv.org/abs/2510.14744)
*Stefano Scali,Josh Kirsopp,Antonio Márquez Romero,Michał Krompiec*

Main category: quant-ph

TL;DR: DOS-QPE 算法可以通过量子相位估计（QPE）的集成方法来提取哈密顿量特征值，从而克服了标准 QPE 的局限性，即只针对单个特征态并需要仔细制备的相干输入。


<details>
  <summary>Details</summary>
Motivation: 标准 QPE 算法的局限性，即它只针对单个特征态并且需要仔细制备的相干输入。

Method: DOS-QPE 算法，它以集成方法为基础，利用对称适应性输入集成和高级谱重建技术，将谱重建问题作为通过压缩感知解决的二次规划问题。

Result: DOS-QPE 算法在费米子模型和核哈密顿量上的表现，突出显示了其在光谱学、电子结构和核理论的早期容错量子模拟中的潜力。

Conclusion: DOS-QPE 算法为量子模拟提供了新的途径，能够处理更广泛的量子系统并提取有价值的物理信息。

Abstract: Quantum phase estimation (QPE) is a cornerstone algorithm for extracting
Hamiltonian eigenvalues, but its standard form targets individual eigenstates
and requires carefully prepared coherent inputs. To overcome these limitations,
we adopt an ensemble-based formulation of QPE that estimates the density of
states (DOS) of the Hamiltonian generator of the evolution. This approach,
which we refer to as DOS-QPE, builds on a prior formulation introduced by one
of the authors. In this work, we present DOS-QPE as a circuit primitive,
extending it with symmetry-adapted input ensembles and advanced spectrum
reconstruction techniques. This variant of QPE enables natural access to
thermodynamic properties, symmetry-resolved spectral functions, and features
relevant to quantum many-body systems. We demonstrate its performance on
fermionic models and nuclear Hamiltonians by casting the spectrum
reconstruction problem as a quadratic program solved via compressed sensing.
These use cases highlight the potential of DOS-QPE for early fault-tolerant
quantum simulations in spectroscopy, electronic structure, and nuclear theory.

</details>


### [300] [Quantum remeshing and efficient encoding for fracture mechanics](https://arxiv.org/abs/2510.14746)
*Ulysse Remond,Pierre-Emmanuel Emeriau,Liam Lysaght,Jean Ruel,Joseph Mikael,Kyryl Kazymyrenko*

Main category: quant-ph

TL;DR: 该论文提出了一种用于结构力学问题（特别是裂纹扩展模拟）的变分量子算法，以替代计算资源密集型传统方法。该算法通过参数化量子电路存储节点位移，并能高效提取关键可观测量。


<details>
  <summary>Details</summary>
Motivation: 传统裂纹扩展模拟计算成本高昂，本文旨在提供一种更高效的量子计算替代方案。

Method: 1.使用参数化量子电路存储节点位移（作为量子幅度）。 2.通过最小化弹性应变能（通过有限元方法获得）来优化节点位移。 3.通过具有对数多项式复杂度的测量来计算能量。 4.在收敛解上高效提取应力强度因子等可观测量。 5.提出一种基于网格重构技术的“热启动”策略，以解决优化过程中的“荒漠高原”问题，并验证算法的可扩展性。

Result: 在2D案例中，算法能够优化节点位移，并高效提取关键可观测量。通过网格重构的“热启动”策略，验证了算法的可扩展性。在Quandela的Ascella光量子处理器上进行了实验验证，数值模拟也证明了其在复杂量子系统中的可扩展性。

Conclusion: 所提出的变分量子算法为结构力学问题提供了一种可行的、计算效率高的新颖方法，并在量子硬件上得到了实验验证，显示了其在解决复杂问题上的潜力。

Abstract: We present a variational quantum algorithm for structural mechanical
problems, specifically addressing crack opening simulations that traditionally
require extensive computational resources. Our approach provides an alternative
solution for a relevant 2D case by implementing a parametrized quantum circuit
that stores nodal displacements as quantum amplitudes and efficiently extracts
critical observables. The algorithm achieves optimal nodal displacements by
minimizing the elastic energy obtained from finite element method. The energy
is computed with only a polylogarithmic number of measurements. Extracting
relevant scalar observables such as the stress intensity factor is then done
efficiently on the converged solution. To validate the scalability of our
approach, we develop a warm start strategy based on a remeshing technique that
uses coarse solutions to circumvent barren plateaus in the optimization
landscape of the more refined problems. Our method has been experimentally
validated on Quandela's photonic quantum processor Ascella and comprehensive
numerical simulations demonstrate its scalability across increasingly complex
quantum systems.

</details>


### [301] [ParaToric 1.0-beta: Continuous-time quantum Monte Carlo for the toric code in a parallel field](https://arxiv.org/abs/2510.14781)
*Simon M. Linsel,Lode Pollet*

Main category: quant-ph

TL;DR: ParaToric是一个C++包，用于在有限温度下模拟平行磁场中的toric code。它实现了扩展的连续时间量子蒙特卡洛算法，支持多种晶格和边界条件，并可扩展到任意晶格几何和自定义观测值。该软件还支持X和Z基的快照提取，可用于为其他方法生成训练/基准测试数据，并提供C/C++和Python接口，易于集成。


<details>
  <summary>Details</summary>
Motivation: 介绍ParaToric，一个用于在平行磁场（X和Z场）和有限温度下模拟toric code的C++软件包。

Method: 实现并扩展了Wu, Deng, and Prokof'ev的连续时间量子蒙特卡洛算法，适用于方形、三角形、蜂窝和立方体晶格，支持开放和周期性边界条件。该软件包可扩展至任意晶格几何和自定义的X或Z基可观测量。

Result: ParaToric支持X和Z基的快照提取，可用于为其他方法（如格子规范理论、冷原子模拟器、量子自旋液体、人工智能和量子纠错）生成训练/基准测试数据。它还提供C/C++和Python接口，易于集成。

Conclusion: ParaToric软件包为在平行磁场和有限温度下模拟toric code提供了一个灵活且可扩展的工具，并支持与其他软件项目的广泛集成。

Abstract: We introduce ParaToric, a C++ package for simulating the toric code in a
parallel field (i.e., $X$- and $Z$-fields) at finite temperature. We implement
and extend the continuous-time quantum Monte Carlo algorithm of Wu, Deng, and
Prokof'ev on the square, triangular, honeycomb, and cubic lattices with open
and periodic boundaries, respectively. The package is expandable to arbitrary
lattice geometries and custom observables diagonal in either the $X$- or
$Z$-basis. ParaToric also supports snapshot extraction in both bases, making it
ideal for generating training/benchmarking data for other methods, such as
lattice gauge theories, cold atom or other quantum simulators, quantum spin
liquids, artificial intelligence, and quantum error correction. The software
provides bindings to C/C++ and Python, and is thus almost universally
integrable into other software projects.

</details>


### [302] [Efficient adaptive control strategy for multi-parameter quantum metrology in two-dimensional systems](https://arxiv.org/abs/2510.14811)
*Qifei Wei,Shengshi Pang*

Main category: quant-ph

TL;DR: 该研究提出了一种用于多参数量子计量的高效自适应控制策略，通过系统扩展和重新参数化技术，实现了最优的精度并展现出对控制参数误差的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有量子计量方法在多参数估计方面存在局限，特别是自适应控制策略的研究不足。本研究旨在填补这一空白，提出一种适用于多参数量子计量的自适应控制策略。

Method: 提出一种适用于二维系统的多参数量子计量自适应控制策略。通过系统扩展消除最优测量、初始状态和控制哈密顿量之间的权衡，推导出估计量方差与演化时间的显式关系。利用重新参数化技术优化演化时间，并建立递推关系来表征迭代过程中的精度提升。

Result: 该策略在几次迭代内就能达到接近最优的性能（相差一个常数因子），并对单个迭代中控制参数误差的偏差表现出很强的鲁棒性。该方法对任意参数依赖性的哈密顿量均有效。

Conclusion: 本研究为在实际场景中通过自适应哈密顿量控制实现多参数量子计量提供了一种实用方法。

Abstract: Quantum metrology leverages quantum resources such as entanglement and
squeezing to enhance parameter estimation precision beyond classical limits.
While optimal quantum control strategies can assist to reach or even surpass
the Heisenberg limit, their practical implementation often requires the
knowledge of the parameters to be estimated, necessitating adaptive control
methods with feedback. Such adaptive control methods have been considered in
single-parameter quantum metrology, but not much in multi-parameter quantum
metrology so far. In this work, we bridge this gap by proposing an efficient
adaptive control strategy for multi-parameter quantum metrology in
two-dimensional systems. By eliminating the trade-offs among optimal
measurements, initial states, and control Hamiltonians through a system
extension scheme, we derive an explicit relation between the estimator variance
and evolution time. Through a reparameterization technique, the optimization of
evolution times in adaptive iterations are obtained, and a recursive relation
is established to characterize the precision improvement across the iterations.
The proposed strategy achieves the optimal performance up to an overall factor
of constant order with only a few iterations and demonstrates strong robustness
against deviations in the errors of control parameters at individual
iterations. Further analysis shows the effectiveness of this strategy for
Hamiltonians with arbitrary parameter dependence. This work provides a
practical approach for multi-parameter quantum metrology with adaptive
Hamiltonian control in realistic scenarios.

</details>


### [303] [Signatures of Topological Symmetries on a Noisy Quantum Simulator](https://arxiv.org/abs/2510.14817)
*Christopher Lamb,Robert M. Konik,Hubert Saleur,Ananda Roy*

Main category: quant-ph

TL;DR: IBM的量子模拟器被用于在二维Ising量子场论中实现和探测拓扑对称性。


<details>
  <summary>Details</summary>
Motivation: 量子模拟器为研究具有拓扑对称性的量子场论模型提供了新的平台，这些模型在物理学中至关重要但难以在传统系统中实现。

Method: 使用混合量子-经典算法（量子近似优化算法和量子自然梯度优化）在IBM的量子模拟器上实现了Ising量子场论的本征态和与之相关的环算符。通过测量量子比特算符的关联函数来探测拓扑对称性。

Result: 在量子设备上获得了与经典计算结果合理一致的关联函数测量值，验证了拓扑对称性的存在。

Conclusion: 嘈杂的量子模拟器有潜力成为研究低维量子场论的平台，并能探测到传统凝聚态实验中难以获得的物理量。

Abstract: Topological symmetries, invertible and otherwise, play a fundamental role in
the investigation of quantum field theories. Despite their ubiquitous
importance across a multitude of disciplines ranging from string theory to
condensed matter physics, controlled realizations of models exhibiting these
symmetries in physical systems are rare. Quantum simulators based on engineered
solid-state devices provide a novel alternative to conventional condensed
matter systems for realizing these models.
  In this work, eigenstates of impurity Hamiltonians and loop operators
associated with the topological symmetries for the Ising conformal field theory
in two space-time dimensions are realized on IBM's Kingston simulator. The
relevant states are created on the quantum device using a hybrid
quantum-classical algorithm. The latter is based on a variation of the quantum
approximate optimization algorithm ansatz combined with the quantum natural
gradient optimization method. Signatures of the topological symmetry are
captured by measuring correlation functions of different qubit operators with
results obtained from the quantum device in reasonable agreement with those
obtained from classical computations. The current work demonstrates the
viability of noisy quantum simulators as platforms for investigating
low-dimensional quantum field theories with direct access to observables that
are often difficult to probe in conventional condensed matter experiments.

</details>


### [304] [Ruelle-Pollicott Decay of Out-of-Time-Order Correlators in Many-Body Systems](https://arxiv.org/abs/2510.14886)
*Jerónimo Duarte,Ignacio García-Mata,Diego A. Wisniacki*

Main category: quant-ph

TL;DR: 本研究表明，孤立多体量子系统的OTOC弛豫率与弱开放动力学李氏谱的最小弛豫率（即李氏间隙）成两倍关系，为理解封闭多体量子系统的弛豫和不可逆性提供了一个统一的框架。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索多体量子系统（特别是踢打伊辛自旋链）中信息 the OTOC（out-of-time-order correlator）的弛豫机制，并将其与李氏谱（Liouvillian spectrum）联系起来，以期为理解封闭多体量子系统的弛豫和不可逆性提供一个统一的框架。

Method: 研究方法是通过分析踢打伊辛自旋链（kicked Ising spin chain）中的OTOC动力学，并将其与该系统弱开放扩展的李氏谱进行比较，特别是关注李氏间隙（Liouvillian gap）对OTOC弛豫率的影响。

Result: 研究发现，孤立的踢打伊辛自旋链的OTOC的长期指数衰减率，精确地等于其弱开放扩展的李氏谱中最小的衰减率（即李氏间隙）的两倍。这种对应关系甚至在可积性与混沌之间的交叉区域也保持不变。

Conclusion: 李氏谱提供了一个统一的框架，用于理解封闭多体量子系统中的弛豫和不可逆性。OTOC的弛豫率与李氏间隙的关系，为分析量子混沌和信息 the OTOC 提供了新的视角。

Abstract: The out-of-time-order correlator (OTOC) quantifies information scrambling in
quantum systems and serves as a key diagnostic of quantum chaos. In one-body
systems with a classical counterpart, the relaxation of the OTOC is governed by
Ruelle-Pollicott resonances. For many-body systems lacking a semiclassical
limit, recent studies have identified an analogous role played by the
Liouvillian spectrum of weakly open extensions of the dynamics, where the
slowest decay rate -- the Liouvillian gap -- encodes relaxation. Here we study
the kicked Ising spin chain and show that the long-time exponential decay of
the OTOC in the isolated system occurs at a rate equal to twice this intrinsic
gap. This correspondence persists even in crossover regimes between
integrability and chaos, demonstrating that the Liouvillian spectrum provides a
unified framework for understanding relaxation and irreversibility in closed
many-body quantum systems.

</details>


### [305] [Fast and fault-tolerant logical measurements: Auxiliary hypergraphs and transversal surgery](https://arxiv.org/abs/2510.14895)
*Alexander Cowtan,Zhiyang He,Dominic J. Williamson,Theodore J. Yoder*

Main category: quant-ph

TL;DR: 本研究旨在降低量子码手术操作的时间开销，通过引入常数时间开销和中间时间开销的方案，并建立了相关电路等价关系。


<details>
  <summary>Details</summary>
Motivation: 通用手术操作需要O(d)轮重复的综合提取才能容错，本研究旨在减少时间开销。

Method: 提出了确保容错手术操作具有常数时间开销的通用条件，并引入了块读取方案；研究了具有中间时间开销的手术操作；建立了同态测量和超图手术之间的电路等价性。

Result: 通用手术操作可以以常数时间开销完成；提出了块读取方案；研究了量子局部可测试码的手术操作；建立了同态测量和超图手术之间的电路等价性，并推导了通用逻辑测量方案的时间开销上限。

Conclusion: 降低码手术的时间成本不依赖于量子存储器是单次运行的，而主要取决于码与其测量寄生系统之间的连接性。

Abstract: Quantum code surgery is a promising technique to perform fault-tolerant
computation on quantum low-density parity-check codes. Recent developments have
significantly reduced the space overhead of surgery. However, generic surgery
operations still require $O(d)$ rounds of repeated syndrome extraction to be
made fault-tolerant. In this work, we focus on reducing the time overhead of
surgery. We first present a general set of conditions that ensure
fault-tolerant surgery operations can be performed with constant time overhead.
This fast surgery necessarily makes use of an auxiliary complex described by a
hypergraph rather than a graph. We then introduce a concrete scheme called
block reading, which performs transversal surgery across multiple code blocks.
We further investigate surgery operations with intermediate time overhead,
between $O(1)$ and $O(d)$, which apply to quantum locally testable codes.
Finally, we establish a circuit equivalence between homomorphic measurement and
hypergraph surgery and derive bounds on the time overhead of generic logical
measurement schemes. Overall, our results demonstrate that reducing the time
cost of code surgery is not reliant on the quantum memory being single-shot.
Instead it is chiefly the connectivity between a code and its measurement
ancilla system that determines the achievable measurement time overhead.

</details>


### [306] [Forecasting Quantum Observables: A Compressed Sensing Approach with Performance Guarantees](https://arxiv.org/abs/2510.14897)
*Víctor Valls,Albert Akhriev,Olatz Sanz Larrarte,Javier Oliva del Moral,Štěpán Šmíd,Josu Etxezarreta Martinez,Sergiy Zhuk,Dmytro Mishagli*

Main category: quant-ph

TL;DR: 通过原子范数最小化框架对量子动力学模型进行认证，以保证其准确性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的外推方法无法保证恢复的模型能够准确反映真实的动力学。本研究旨在提出一种方法来解决此问题。

Method: 引入原子范数最小化作为一种框架，用于认证任何算法学习到的模型是否准确地捕捉了量子可观测量的潜在动力学。

Result: 在涉及8-20个量子比特的自旋链哈密顿量的数值实验中，本认证框架在98%的情况下将平均预测误差控制在0.1以下，在89-97%的情况下将误差控制在0.05以下。即使在存在实际的探测噪声的情况下，认证模型也保持稳健，成功率仅下降到88-95%。

Conclusion: 原子范数最小化提供了一种有效的方法来认证从嘈杂的量子设备数据中学习到的量子动力学模型，确保其准确性，即使在存在噪声的情况下也表现出鲁棒性。

Abstract: Noise in near-term quantum devices limits the timescales over which
measurements can be reliably obtained. Existing data-driven extrapolation
methods extend the dynamics of quantum observables from measurements, but they
cannot guarantee that the recovered model reflects the true dynamics. In this
paper, we introduce atomic norm minimization as a framework to certify that a
model learned by any algorithm accurately captures the underlying dynamics of
quantum observables. This certification is valid when the system is governed by
a small number of well-separated Bohr frequencies. We validate the framework
across multiple algorithms on numerical experiments with spin-chain
Hamiltonians involving 8-20 qubits. Comparing with exact diagonalization,
certification yields an average forecasting error below 0.1 (within the
observable's range of $[-1, 1]$) in 98% of cases and below 0.05 in 89-97% of
cases, depending on the forecasting algorithm. Even in the presence of
realistic shot noise, certified models remain robust, with success rates
decreasing only to 88-95% for the 0.1 error threshold.

</details>


### [307] [Continuous-time quantum walk on a random graph using quantum circuits](https://arxiv.org/abs/2510.14905)
*Sabyasachi Chakraborty,Rohit Sarma Sarkar,Sonjoy Majumder,Rohit Kishan Ray*

Main category: quant-ph

TL;DR: 本研究提出了一种可扩展的量子电路形式，用于在Erd	ext{o}s-R	ext{e}nyi随机图等随机图结构上模拟连续时间量子行走（CTQW）。


<details>
  <summary>Details</summary>
Motivation: 量子行走，特别是CTQW，是模拟量子输运、复杂动力学和开发量子算法的有力工具。

Method: 研究人员提出了一种量子电路结构，利用Trotterization方案有效地实现了图拉普拉斯量的时间演化。

Result: 通过在随机图上进行量子电路实现，研究了CTQW的关键动力学特性，特别是其局域化行为。

Conclusion: 该方法确保了量子电路设计可以适用于任何图结构，为实现基于CTQW的量子模拟奠定了基础。

Abstract: Quantum walks, particularly continuous-time quantum walks (CTQW), have
emerged as powerful tools for modeling quantum transport, simulating complex
dynamics, and developing quantum algorithms with potential speedups over
classical counterparts. In this work, we present a scalable quantum circuit
formalism to simulate CTQW on random graph structures, especially focusing on
Erd\H{o}s-R\'enyi random graphs. Our quantum circuit construction efficiently
implements the time evolution of the graph Laplacian, using the Trotterization
scheme. We investigate key dynamical properties, \emph{i.e.,} the localization
behavior of the CTQW. Our quantum circuit implementation over random graph
ensures that the circuit design can work on any graph structure, thereby laying
the foundation for realizing CTQW-based quantum simulations efficiently.

</details>


### [308] [Decoherence-Aware Entangling and Swapping Strategy Optimization for Entanglement Routing in Quantum Networks](https://arxiv.org/abs/2510.14912)
*Shao-Min Huang,Cheng-Yang Cheng,Ming-Huang Chien,Jian-Jhih Kuo,Chih-Yu Wang*

Main category: quant-ph

TL;DR: 本研究提出了一种新的量子比特对分发协议，以提高通信的安全性和保真度。


<details>
  <summary>Details</summary>
Motivation: 传统的量子隐形传态协议中，由于环境干扰导致量子比特对保真度低，且交换过程会造成额外的保真度损失。因此，需要及时生成量子比特对并优化生成和交换过程。

Method: 提出了一种短时间槽协议，并设计了两种新颖的算法来解决最大化保真度总和的优化问题TETRIS。

Result: 与现有方法相比，该协议在大多数情况下能将保真度提高60%~78%，在低保真度的情况下也能提高20%~75%。

Conclusion: 本研究提出的短时间槽协议和优化算法能有效提高量子隐形传态的保真度，为高安全性通信提供了新的解决方案。

Abstract: Quantum teleportation enables high-security communications through end-to-end
quantum entangled pairs. End-to-end entangled pairs are created by using
swapping processes to consume short entangled pairs and generate long pairs.
However, due to environmental interference, entangled pairs decohere over time,
resulting in low fidelity. Thus, generating entangled pairs at the right time
is crucial. Moreover, the swapping process also causes additional fidelity
loss. To this end, this paper presents a short time slot protocol, where a time
slot can only accommodate a process. It has a more flexible arrangement of
entangling and swapping processes than the traditional long time slot protocol.
It raises a new optimization problem TETRIS for finding strategies of
entangling and swapping for each request to maximize the fidelity sum of all
accepted requests. To solve the TETRIS, we design two novel algorithms with
different optimization techniques. Finally, the simulation results manifest
that our algorithms can outperform the existing methods by up to 60 ~ 78% in
general, and by 20 ~ 75% even under low entangling probabilities.

</details>


### [309] [Current fluctuations in nonequilibrium open quantum systems beyond weak coupling: a reaction coordinate approach](https://arxiv.org/abs/2510.14926)
*Khalak Mahadeviya,Saulo V. Moreira,Sheikh Parvez Mandal,Mahasweta Pandit,Javier Prior,Mark T. Mitchison*

Main category: quant-ph

TL;DR: 研究了强耦合下开放量子系统中的电流涨落，并提出了一种克服弱耦合和马尔可夫限制的新方法，发现了抑制噪声和非经典关联的现象。


<details>
  <summary>Details</summary>
Motivation: 目前的开放量子系统研究主要集中在弱耦合和马尔可夫限制下，忽略了强耦合下的复杂情况。本研究旨在探索强耦合下开放量子系统中的电流涨落特性，并开发相应的计算框架。

Method: 结合了全计数统计和反应坐标映射方法，用于计算稳态电流涨落及其时间相关性。

Result: 发现在强耦合下，平均电流和电流涨落都与系统-环境相互作用强度存在非单调关系。在特定条件下，电流噪声低于经典热力学不确定性界限，并伴随着量子跳变轨迹中更强的反关联和更快的系统弛豫。这些现象与反应坐标模式的非经典特性（如非高斯性和量子相干性）相关。

Conclusion: 研究结果为在强耦合条件下控制量子器件中的电流涨落提供了新的见解和设计原则，拓展了对开放量子系统的理解。

Abstract: We investigate current fluctuations in open quantum systems beyond the
weak-coupling and Markovian regimes, focusing on a coherently driven qubit
strongly coupled to a structured bosonic environment. By combining full
counting statistics with the reaction coordinate mapping, we develop a
framework that enables the calculation of steady-state current fluctuations and
their temporal correlations in the strong-coupling regime. Our analysis reveals
that, unlike in weak coupling, both the average current and its fluctuations
exhibit nonmonotonic dependence on the system-environment interaction strength.
Notably, we identify a regime where current noise is suppressed below the
classical thermodynamic uncertainty bound, coinciding with enhanced
anticorrelations in quantum jump trajectories and faster system relaxation. We
further show that these features are linked to nonclassical properties of the
reaction coordinate mode, such as non-Gaussianity and quantum coherence. Our
results provide new insights and design principles for controlling current
fluctuations in quantum devices operating beyond the standard weak-coupling
paradigm.

</details>


### [310] [Orders matter: tight bounds on the precision of sequential quantum estimation for multiparameter models](https://arxiv.org/abs/2510.14963)
*Gabriele Fazio,Jiayu He,Matteo G. A. Paris*

Main category: quant-ph

TL;DR: 逐步估计策略在量子计量中可提供优于联合测量的精度，尤其是在资源受限或不完美的实验条件下。


<details>
  <summary>Details</summary>
Motivation: 探讨了一种替代联合估计的逐步估计策略，用于多参数量子计量。

Method: 推导了逐步估计协议的精度边界（逐步可分离边界）及其解析表达式，并与联合测量策略进行了严格比较。

Result: 分析了几个SU(2)幺正编码模型，证明了逐步策略在非最优探针或松弛度高的模型中可以优于联合测量。

Conclusion: 逐步估计是一种有效的替代联合测量的方法，尤其适用于资源受限或不完美的实验环境。

Abstract: In multiparameter quantum metrology, the ultimate precision of joint
estimation is dictated by the Holevo Cram\'er-Rao bound. In this paper, we
discuss and analyze in detail an alternative approach: the stepwise estimation
strategy. In this approach, parameters are estimated sequentially, using an
optimized fraction of the total available resources allocated to each step. We
derive a tight and achievable precision bound for this protocol, the stepwise
separable bound, and provide its closed-form analytical expression, revealing a
crucial dependence on the chosen measurement ordering. We provide a rigorous
comparison with the joint measurement strategy, deriving analytical conditions
that determine when the stepwise approach offers superior precision. Through
the analysis of several paradigmatic SU(2) unitary encoding models, we
demonstrate that the stepwise strategy can indeed outperform joint
measurements, particularly in scenarios characterized by non-optimal probes or
models with a high degree of sloppiness. Our findings establish stepwise
estimation as a powerful alternative to joint and collective measurements,
proving that sequential protocols can provide a genuine metrological advantage,
especially in resource-constrained or imperfect experimental settings.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [311] [Why Instant-Runoff Voting Is So Resilient to Coalitional Manipulation: Phase Transitions in the Perturbed Culture](https://arxiv.org/abs/2510.14450)
*François Durand*

Main category: cs.GT

TL;DR: IRV (Instant-Runoff Voting) is resistant to coalitional manipulation (CM) due to the presence of a Super Condorcet Winner (SCW), with a critical threshold of 0 for preference concentration.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical reasons behind IRV's resistance to coalitional manipulation (CM) and analyze the susceptibility to CM of Plurality, Two-Round System, and IRV within the Perturbed Culture model.

Method: Analyze the susceptibility to CM of Plurality, Two-Round System, and IRV within the Perturbed Culture model. Introduce and utilize the concept of the Super Condorcet Winner (SCW).

Result: Each voting rule (Plurality, Two-Round System, IRV) exhibits a phase transition at a critical preference concentration (theta_c). Below theta_c, the probability of CM converges to 1; above theta_c, it converges to 0. For IRV, theta_c is proven to be 0, indicating strong resistance to CM.

Conclusion: The presence of the Super Condorcet Winner (SCW) is key to IRV's resistance to coalitional manipulation. IRV has a critical threshold of 0 for preference concentration, making it robust against CM even with minimal preference concentration.

Abstract: Previous studies have shown that Instant-Runoff Voting (IRV) is highly
resistant to coalitional manipulation (CM), though the theoretical reasons for
this remain unclear. To address this gap, we analyze the susceptibility to CM
of three major voting rules-Plurality, Two-Round System, and IRV-within the
Perturbed Culture model. Our findings reveal that each rule undergoes a phase
transition at a critical value theta\_c of the concentration of preferences:
the probability of CM for large electorates converges exponentially fast to 1
below theta\_c and to 0 above theta\_c. We introduce the Super Condorcet Winner
(SCW), showing that its presence is a key factor of IRV's resistance to
coalitional manipulation, both theoretically and empirically. Notably, we use
this notion to prove that for IRV, theta\_c = 0, making it resistant to CM with
even minimal preference concentration.

</details>


### [312] [Online Proportional Apportionment](https://arxiv.org/abs/2510.14752)
*Javier Cembrano,Jose Correa,Svenja M. Griesbach,Victor Verdugo*

Main category: cs.GT

TL;DR: 该研究首次在线设定下研究了比例分配问题，提出了一种无需未来信息即可进行分配的框架，并分析了确定性和随机性分配方法。


<details>
  <summary>Details</summary>
Motivation: 传统立法席位分配方法未考虑动态因素，而动态因素在许多情况下至关重要。本研究旨在解决在线分配问题。

Method: 研究了确定性和随机性在线分配方法。对于确定性方法，构建了产生线性下界的对抗性实例，并提出了一种贪心方法来匹配该下界。对于随机性方法，证明了当且仅当 n≤3 时，存在满足全局配额并确保各方期望获得其比例份额的方法，该方法可通过递归构造网络的流来获得。

Result: 对于确定性方法，证明了最坏情况偏差的下界（与n线性相关）是最佳的，并且一个简单的贪心方法可以达到这个界限。对于随机性方法，当n≤3时，可以实现全局配额，并确保各方在每一步期望获得其比例份额。

Conclusion: 研究结果为在线比例分配问题提供了理论基础，并展示了其在在线依赖四舍五入过程中的应用。

Abstract: Traditionally, the problem of apportioning the seats of a legislative body
has been viewed as a one-shot process with no dynamic considerations. While
this approach is reasonable for some settings, dynamic aspects play an
important role in many others. We initiate the study of apportionment problems
in an online setting. Specifically, we introduce a framework for proportional
apportionment with no information about the future. In this model, time is
discrete and there are $n$ parties that receive a certain share of the votes at
each time step. An online algorithm needs to irrevocably assign a prescribed
number of seats at each time, ensuring that each party receives its fractional
share rounded up or down, and that the cumulative number of seats allocated to
each party remains close to its cumulative share up to that time.
  We study deterministic and randomized online apportionment methods. For
deterministic methods, we construct a family of adversarial instances that
yield a lower bound, linear in $n$, on the worst-case deviation between the
seats allocated to a party and its cumulative share. We show that this bound is
best possible and is matched by a natural greedy method. As a consequence, a
method guaranteeing that the cumulative number of seats assigned to each party
up to any step equals its cumulative share rounded up or down (global quota)
exists if and only if $n\leq 3$. Then, we turn to randomized allocations and
show that, for $n\leq 3$, we can randomize over methods satisfying global quota
with the additional guarantee that each party receives, in expectation, its
proportional share in every step. Our proof is constructive: Any method
satisfying these properties can be obtained from a flow on a recursively
constructed network. We showcase the applicability of our results to obtain
approximate solutions in the context of online dependent rounding procedures.

</details>


### [313] [Co-Investment under Revenue Uncertainty Based on Stochastic Coalitional Game Theory](https://arxiv.org/abs/2510.14555)
*Amal Sakr,Andrea Araldo,Tijani Chahed,Daniel Kofman*

Main category: cs.GT

TL;DR: 本文提出了一种新的随机博弈模型，用于解决移动边缘计算（MEC）等新服务部署中的成本分摊和收入分配问题，并分析了在不确定收入下的联盟稳定性和盈利能力。


<details>
  <summary>Details</summary>
Motivation: 单个利益相关者（如基础设施提供商 InP）无法承担引入新服务（如移动边缘计算 MEC）所需的大量投资，但服务提供商（SP）也有兴趣部署这些服务。

Method: 提出了一种新的随机博弈模型，该模型基于鲁棒博弈论，并推导了“大联盟”（包含所有利益相关者）稳定性的概率下界。在收入不确定且存在相关波动的情况下，引入了盈利能力（参与者的支付为非负）作为联合投资的必要条件。

Result: 数值结果表明，当 SP 的收入规模相似且投资期足够长时，即使存在高度不确定性，联盟的稳定性概率下界也很高。然而，在收入高度可变的情况下，稳定性的下界可能非常低，但联合投资仍然有利可图。

Conclusion: 所提出的随机博弈框架能够有效地处理 MEC 部署中涉及的不确定性和多方利益相关者的问题，并为联盟的稳定性和盈利能力提供了可量化的评估。

Abstract: The introduction of new services, such as Mobile Edge Computing (MEC),
requires a massive investment that cannot be assumed by a single stakeholder,
for instance the Infrastructure Provider (InP). Service Providers (SPs) however
also have an interest in the deployment of such services. We hence propose a
co-investment scheme in which all stakeholders, i.e., the InP and the SPs, form
the so-called grand coalition composed of all the stakeholders with the aim of
sharing costs and revenues and maximizing their payoffs. The challenge comes
from the fact that future revenues are uncertain. We devise in this case a
novel stochastic coalitional game formulation which builds upon robust game
theory and derive a lower bound on the probability of the stability of the
grand coalition, wherein no player can be better off outside of it. In the
presence of some correlated fluctuations of revenues however, stability can be
too conservative. In this case, we make use also of profitability, in which
payoffs of players are non-negative, as a necessary condition for
co-investment. The proposed framework is showcased for MEC deployment, where
computational resources need to be deployed in nodes at the edge of a
telecommunication network. Numerical results show high lower bound on the
probability of stability when the SPs' revenues are of similar magnitude and
the investment period is sufficiently long, even with high levels of
uncertainty. In the case where revenues are highly variable however, the lower
bound on stability can be trivially low whereas co-investment is still
profitable.

</details>


### [314] [The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon Blockchain](https://arxiv.org/abs/2510.14642)
*Andrei Seoev,Leonid Gremyachikh,Anastasiia Smirnova,Yash Madhwal,Alisa Kalacheva,Dmitry Belousov,Ilia Zubov,Aleksei Smirnov,Denis Fedyanin,Vladimir Gorgadze,Yury Yanovich*

Main category: cs.GT

TL;DR: 该研究提出了一种基于强化学习的MEV提取框架，用于在Polygon Atlas等拍卖机制下进行最优出价，并在仿真和实证中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于博弈论的方法在Polygon Atlas等高频、部分可观察的环境中难以应对信息不全和竞争动态，需要新的方法来应对MEV提取的挑战。

Method: 提出了一种基于PPO（Proximal Policy Optimization）的强化学习智能体，并构建了一个能够模拟套利机会随机性和拍卖竞争的仿真环境，以在实时约束下进行自适应策略学习。

Result: 在仿真环境中，所提出的智能体与现有搜索者并存时能捕获49%的可用利润，在替换市场领导者时能捕获81%的利润，显著优于静态出价策略。

Conclusion: 强化学习在信息不全、高频交易环境中相比传统优化方法具有显著优势，能为MEV提取的工业参与者和协议设计者提供实际价值。

Abstract: In blockchain networks, the strategic ordering of transactions within blocks
has emerged as a significant source of profit extraction, known as Maximal
Extractable Value (MEV). The transition from spam-based Priority Gas Auctions
to structured auction mechanisms like Polygon Atlas has transformed MEV
extraction from public bidding wars into sealed-bid competitions under extreme
time constraints. While this shift reduces network congestion, it introduces
complex strategic challenges where searchers must make optimal bidding
decisions within a sub-second window without knowledge of competitor behavior
or presence. Traditional game-theoretic approaches struggle in this
high-frequency, partially observable environment due to their reliance on
complete information and static equilibrium assumptions. We present a
reinforcement learning framework for MEV extraction on Polygon Atlas and make
three contributions: (1) A novel simulation environment that accurately models
the stochastic arrival of arbitrage opportunities and probabilistic competition
in Atlas auctions; (2) A PPO-based bidding agent optimized for real-time
constraints, capable of adaptive strategy formulation in continuous action
spaces while maintaining production-ready inference speeds; (3) Empirical
validation demonstrating our history-conditioned agent captures 49\% of
available profits when deployed alongside existing searchers and 81\% when
replacing the market leader, significantly outperforming static bidding
strategies. Our work establishes that reinforcement learning provides a
critical advantage in high-frequency MEV environments where traditional
optimization methods fail, offering immediate value for industrial participants
and protocol designers alike.

</details>


### [315] [Learnable Mixed Nash Equilibria are Collectively Rational](https://arxiv.org/abs/2510.14907)
*Geelon So,Yi-An Ma*

Main category: cs.GT

TL;DR: 该研究探讨了在具有非渐近稳定性的动态中学习博弈论，并将其与集体合理性的经济特性联系起来。


<details>
  <summary>Details</summary>
Motivation: 研究在具有非渐近稳定性的动态中学习博弈论，并探索其与集体合理性的经济特性的联系。

Method: 引入了均匀稳定性的概念，研究了个体效用寻求动态的均衡，并将其与帕累托最优性联系起来。此外，研究还分析了均匀稳定性如何影响增量平滑最佳响应动态的最终迭代收敛行为。

Result: 研究发现，如果混合均衡不均匀稳定，则不是弱帕累托最优的；如果均匀稳定，则是弱帕累托最优的。研究还表明，个体效用寻求行为可以导致集体合理性。

Conclusion: 均匀稳定性是衡量博弈论中混合纳什均衡的集体合理性的关键因素，并影响着学习动态的收敛行为。

Abstract: We extend the study of learning in games to dynamics that exhibit
non-asymptotic stability. We do so through the notion of uniform stability,
which is concerned with equilibria of individually utility-seeking dynamics.
Perhaps surprisingly, it turns out to be closely connected to economic
properties of collective rationality. Under mild non-degeneracy conditions and
up to strategic equivalence, if a mixed equilibrium is not uniformly stable,
then it is not weakly Pareto optimal: there is a way for all players to improve
by jointly deviating from the equilibrium. On the other hand, if it is locally
uniformly stable, then the equilibrium must be weakly Pareto optimal. Moreover,
we show that uniform stability determines the last-iterate convergence behavior
for the family of incremental smoothed best-response dynamics, used to model
individual and corporate behaviors in the markets. Unlike dynamics around
strict equilibria, which can stabilize to socially-inefficient solutions,
individually utility-seeking behaviors near mixed Nash equilibria lead to
collective rationality.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [316] [Spontaneous Breaking of the SU(3) Flavor Symmetry in a Quantum Hall Valley Nematic](https://arxiv.org/abs/2510.13874)
*G. Krizman,A. Kazakov,C. -W. Cho,V. V. Volobuev,A. Majou,E. Ben Achour,T. Wojtowicz,G. Bauer,Y. Guldner,B. A. Piot,Th. Jolicoeur,G. Springholz,L. -A. de Vaulchier*

Main category: cond-mat.mes-hall

TL;DR: Pb1-xSnxSe量子阱中发现了具有SU(3)序参量的量子霍尔谷尼马相，并展示了通过塞曼场对该序参量的控制，这为理解SU(3)系统的多体物理提供了基础。


<details>
  <summary>Details</summary>
Motivation: 研究二维量子材料中的电子关联、对称性和拓扑性所产生的奇异电子相，特别是赝自旋和空间自由度同时被自发对称性破缺所实现的尼马相。

Method: 通过Pb1-xSnxSe量子阱，在有塞曼场和无塞曼场的情况下，研究谷自由度的极化行为，寻找SU(3)序参量的证据。

Result: 在Pb1-xSnxSe量子阱中发现了量子霍尔谷尼马相，并证明了在塞曼场存在下可以对该相进行进一步的控制，这表明存在自发和显式的SU(3)对称性破缺。

Conclusion: 在Pb1-xSnxSe量子阱中观察到的量子霍尔谷尼马相以及对SU(3)对称性破缺的控制，为理解和探索具有SU(3)对称性的多体物理系统提供了新的视角和实验基础。

Abstract: Two-dimensional quantum materials can host original electronic phases that
arise from the interplay of electronic correlations, symmetry and topology. In
particular, the spontaneous breaking of internal symmetry that acts
simultaneously on the pseudospin and the spatial degree of freedom realizes a
nematic ordering. We report evidence of a quantum Hall valley nematic phase
with an underlying SU(3) order parameter space obtained by a spontaneous
polarization between the threefold degenerate valley pseudospins in Pb1-xSnxSe
quantum wells. In the presence of a Zeeman field, we demonstrate a further
control of the nematic ordering with an explicit symmetry breaking. Evidence of
both spontaneous and explicit SU(3) symmetry breaking, reminiscent of the quark
flavor paradigm, is of fundamental interest to shape the many body physics in a
SU(3) system.

</details>


### [317] [Unconventional criticality in $O(D)$-invariant loop-constrained Landau theory](https://arxiv.org/abs/2510.13960)
*Svitlana Kondovych,Asle Sudbø,Flavio S. Nogueira*

Main category: cond-mat.mes-hall

TL;DR: This paper analyzes ferroelectrics with a divergence-free polarization constraint, revealing critical behavior beyond the standard Landau-Ginzburg-Wilson model due to emergent gauge symmetry, leading to a large anomalous dimension for the order parameter.


<details>
  <summary>Details</summary>
Motivation: The motivation is to study the critical behavior of Landau-type theory for ferroelectrics with a divergence-free constraint on the polarization order parameter, which allows for a natural extension to O(N) models and may lead to novel critical phenomena.

Method: The study uses renormalization group analysis to investigate the critical behavior of the ferroelectric Landau-type theory with a divergence-free polarization constraint. This constraint forces the number of components of the polarization field to equal the spatial dimensionality (D), enabling an extension to O(N) models with N=D.

Result: The renormalization group analysis reveals critical behavior that deviates from the conventional Landau-Ginzburg-Wilson paradigm. Specifically, the order parameter exhibits a large anomalous dimension, reaching approximately 0.239 in three dimensions, which is significantly larger than the typical value of 0.034 for the O(3) universality class. This anomaly is attributed to an emergent gauge symmetry arising from the local constraint.

Conclusion: The paper concludes that the divergence-free constraint on the polarization order parameter in ferroelectrics leads to emergent gauge symmetry and critical behavior beyond the standard universality classes, characterized by a significantly large anomalous dimension for the order parameter.

Abstract: We study the critical behavior of a Landau-type theory for ferroelectrics in
which the polarization order parameter $\vec{P}$ is subject to a
divergence-free constraint, such that only loop-like polarization
configurations contribute to the partition function. This constraint forces the
number of components of the field $\vec{P}$ to equal the spatial dimensionality
$D$, allowing a natural extension of the present theory to $O(N)$ models with
$N=D$. Renormalization group analysis reveals critical behavior beyond the
conventional Landau-Ginzburg-Wilson paradigm. In particular, the order
parameter acquires a remarkably large anomalous dimension, with $\eta\approx
0.239$ in three dimensions -- significantly exceeding the value $\eta\approx
0.034$ typical of the $O(3)$ universality class. This is due to an emergent
gauge symmetry originating with the local constraint.

</details>


### [318] [Comparative study of phonon-limited carrier transport in the Weyl semimetal TaAs family](https://arxiv.org/abs/2510.14048)
*Shashi B. Mishra,Zhe Liu,Sabyasachi Tiwari,Feliciano Giustino,Elena R. Margine*

Main category: cond-mat.mes-hall

TL;DR: TaAs家族韦尔半金属的声子散射是主要的电荷传输限制因素，其中NbP电导率最高，TaAs最低。


<details>
  <summary>Details</summary>
Motivation: 研究TaAs家族韦尔半金属中由声子引起的输运现象，并解释不同化合物之间的电导率差异。

Method: 使用第一性原理计算和ab initio玻尔兹曼输运方程系统研究声子限制的输运。

Result: 计算得到的电导率与实验数据吻合良好，表明声子散射是主要的限制因素。NbP电导率最高，是由于其大的费米速度；TaAs电导率最低，是由于载流子口袋和速度的限制。NbP对掺杂不敏感，而TaAs表现出显著的电子-空穴不对称性。

Conclusion: 声子、掺杂和载流子动力学在决定TaAs家族的电子响应方面起着关键作用，提供了对输运机制的微观认识。

Abstract: We present a systematic first-principles study of phonon-limited transport in
the TaAs family of Weyl semimetals using the ab initio Boltzmann transport
equation. The calculated electrical conductivities show excellent agreement
with experimental data for high-quality samples, confirming that transport in
these systems is predominantly limited by phonon scattering. Among the four
compounds, NbP achieves the highest conductivity, governed primarily by its
large Fermi velocities that offset its stronger scattering rates. In contrast,
TaAs displays the lowest conductivity, linked to reduced carrier pockets and
limited carrier velocities. Additionally, NbP conductivity remains largely
unaffected by small hole or electron doping, whereas TaAs exhibits pronounced
electron-hole asymmetry. NbAs and TaP show intermediate behavior, reflecting
their Fermi surface topologies and scattering phase space. These findings
provide microscopic insight into the transport mechanisms of the TaAs family
and emphasize the critical role of phonons, doping, and carrier dynamics in
shaping their electronic response.

</details>


### [319] [Towards a unified mechanistic understanding of the electrical response of bipolar nanofluidic systems](https://arxiv.org/abs/2510.14080)
*Ayelet Ben-Kish Sharvit,Yoav Green*

Main category: cond-mat.mes-hall

TL;DR: Bipolar纳米膜和纳米通道在水脱盐和能源收集系统中发挥着关键作用，但其潜在物理机制尚未完全理解。本研究结合理论分析和数值模拟，提出了一个改进纳米流体器件设计的统一框架。该框架揭示了施加电压与几何形状和表面电荷密度比的参数η之间的相互作用决定了系统响应。在低电压下，响应主要由η决定，可以用简化的相空间表示；在高电压下，相空间会过度简化。研究范围涵盖了从单极通道到双极通道的多种配置，并将模拟结果与三种理论模型进行了比较，解释了偏差。


<details>
  <summary>Details</summary>
Motivation: 鉴于水脱盐和能源收集系统对高性能双极纳米膜和纳米通道日益增长的需求，但目前对其潜在物理机制的理解不足，导致了低效的经验优化。

Method: 结合理论分析和数值模拟，开发一个统一的框架来改进纳米流体器件的设计。

Result: 研究表明，系统响应受施加电压和参数η（取决于几何形状和两个带电区域的表面电荷密度之比）的相互作用控制。在低电压下，响应主要由η决定，可以用简化的相空间表示；在高电压下，该相空间会过度简化。研究涵盖了从单极到双极通道的多种配置，并将模拟结果与三种理论模型进行了比较，解释了观察到的偏差。

Conclusion: 本研究提出的统一框架有助于减少优化纳米流体器件所需的时间和资源，并能改进对实验和模拟结果的解释。

Abstract: Bipolar nanoporous membranes and bipolar nanochannels are used in water
desalination and energy-harvesting systems that provide clean water and green
energy, respectively. The growing need for both requires continuous improvement
of their performance. However, the underlying physics of these complex systems
is still not fully understood, making empirical optimization slow and
inefficient. In this work, we combine theoretical analysis and numerical
simulations to develop a unified framework for improving the design of
nanofluidic devices. We show that the system response is governed by the
interplay between the applied voltage and a parameter $\eta$, which depends on
the ratio of geometry and surface charge densities of both charged regions. At
low voltages, the response is mostly determined by $\eta$, allowing its
dependence to be represented by a simplified phase space. At high voltages,
this phase space becomes oversimplified. To demonstrate the framework's
robustness, we scan a range of configurations, from unipolar channels (single
charged region) to bipolar channels (positive and negative segments). We
compare the numerically simulated current-voltage responses with three
theoretical models, which are limiting scenarios within the phase space, and
explain the observed deviations. These findings can help reduce the time and
resources required to optimize nanofluidic devices and improve the
interpretation of experiments and simulations.

</details>


### [320] [Mapping Temperature Using Transmission Kikuchi Diffraction](https://arxiv.org/abs/2510.14175)
*Yueyun Chen,Xin Yi Ling,Jared Lodico,Tristan P. O`Neill,B. C. Regan,Matthew Mecklenburg*

Main category: cond-mat.mes-hall

TL;DR: KDTh是一种利用菊池衍射图样拟合来测量纳米尺度温度和压力的非接触式扫描电子显微镜技术。


<details>
  <summary>Details</summary>
Motivation: 由于物理接触会不可避免地扰乱系统，因此需要新的测量纳米尺度热力学的方法来理解电子设备在日益减小的长度尺度下的工程。

Method: KDTh通过精确拟合菊池衍射图样来检测晶体样品中局部的体积晶格变化，并利用热膨胀系数（CTE）推断温度变化。我们在焦耳加热的石墨上，通过在样品上扫描5.5纳米的电子探针来绘制晶格参数和温度。

Result: 我们实现了约0.01%的参数精度和2.2 K/$\sqrt{Hz}$的温度灵敏度。KDTh通过拟合整个菊池衍𝘁tern，即使超出透射电子显微镜的测量精度，也能提供更高的灵敏度。KDTh可以在透射（透射菊池衍射，TKD）和反射（电子背散射衍射，EBSD）模式下运行。

Conclusion: KDTh是一种有前途的纳米尺度热力学测量技术，具有高精度和高灵敏度，可用于各种晶体材料。

Abstract: Electronic devices are engineered at increasingly smaller length scales; new
metrologies to understand nanoscale thermodynamics are needed. Temperature and
pressure are fundamental thermodynamic quantities whose nanoscale measurement
is challenging as physical contact inevitably perturbs the system. Here we
demonstrate Kikuchi diffraction thermometry (KDTh), a non-contact scanning
electron microscope (SEM) technique capable of mapping nanoscale temperatures
and pressures. KDTh detects local volumetric lattice changes in crystalline
samples by precisely fitting Kikuchi patterns. Temperature changes are deduced
using the coefficient of thermal expansion (CTE). We map lattice parameters and
temperatures on Joule-heated graphite by rastering a 5.5-nm electron probe
across the sample. Our parameter precision is ~0.01% and our temperature
sensitivity is 2.2 K/$\sqrt{Hz}$. KDTh offers advanced sensitivity by fitting
the entire Kikuchi pattern, even beyond the precision measured in transmission
electron microscopy. KDTh can operate in both transmission (transmission
Kikuchi diffraction, TKD) and reflection (electron backscatter diffraction,
EBSD) modes.

</details>


### [321] [Phenomenological Ehrenfest Dynamics with Topological and Geometric Phase Effects and the curious case of Elliptical intersection](https://arxiv.org/abs/2510.14181)
*Dhruv Sharma*

Main category: cond-mat.mes-hall

TL;DR: 本研究提出了一种包含几何相位效应的非绝热分子动力学模拟的计算框架。


<details>
  <summary>Details</summary>
Motivation: 在分子动力学模拟中，需要考虑几何相位效应，尤其是在电子简并点附近，这些效应会影响系统的动力学行为。

Method: 本研究提出了一种基于广义两能级哈密顿模型的方法，该模型通过参数化可以处理不同类型的电子态交叉（如锥形交叉、避免交叉和椭圆交叉）。引入了一种新的预循环轨迹初始化方案来编码几何相位。通过引入Berry曲率修正的Ehrenfest动力学来包含几何相位效应。

Result: 数值模拟结果表明，该方法能够准确地模拟几何相位效应对电子态混合的影响，并与理论预测一致。该方法对于锥形交叉可以得到理论预期的π相位，对于椭圆交叉可以得到与π不同的、可调的相位。

Conclusion: 本研究提出的框架能够有效地模拟几何相位效应在分子动力学中的作用，为研究量子-经典相互作用提供了有价值的工具。此外，椭圆交叉和几何相位效应为设计新材料、新光谱和量子比特应用提供了新的可能性。

Abstract: We present a comprehensive computational framework for simulating
nonadiabatic molecular dynamics with explicit inclusion of geometric phase (GP)
effects. Our approach is based on a generalized two-level Hamiltonian model
that can represent various electronic state crossings - conical intersections,
avoided crossings, and elliptic intersections - through appropriate
parameterization. We introduce a novel prelooping trajectory initialization
scheme, allowing us to encode the memory as an initial phase accumulated due to
the adiabatic evolution over the potential energy surface. This is a unified
framework to handle different types of level crossings by incorporating Berry
curvature-based force corrections to Ehrenfest dynamics, ensuring accurate
representation of topological effects. For conical intersections, our method
incorporates the theoretically expected phase pi, while for elliptic
intersections, it yields a parametrically tunable but loop radius (energy)
independent phase different from pi. We also include an eccentricity parameter
(e) in the diabatic coupling to model more realistic molecular systems.
Numerical simulations demonstrate the consistency of our approach with
theoretical predictions for mixing of states and inhibition from mixing due to
geometric phase effects. This framework provides a valuable tool for studying
quantum-classical interactions in molecular systems where geometric phase
effects play a significant role. The elliptical intersection and geometric
phase effect opens avenue for the design and discovery of degenerate materials.
It produces a fresh look to help develop a new kind of spectroscopy and
potential qubit applications. This simple Hamiltonian reveals a pathological
phase protection effect E = kr, where k is real, that has great utility in a
new spectroscopy design.

</details>


### [322] [Cryogenic temperature dependence and hysteresis of surface-trap-induced gate leakage in GaN high-electron-mobility transistors](https://arxiv.org/abs/2510.14456)
*Ching-Yang Pan,Shi-Kai Lin,Yu-An Chen,Pei-hsun Jiang*

Main category: cond-mat.mes-hall

TL;DR: 表面陷阱引起 GaN HEMT 的栅极漏电机制在室温到低温范围内有详细的映射。在小栅极偏压下观察到二维变程跳跃。在高反向栅极偏压下，漏电在 220 K 以上主要由 Poole--Frenkel 发射主导，但在冻结陷阱效应下，在 220 K 以下逐渐过渡到陷阱辅助隧穿。从向上栅极扫描下的栅极漏电电流提取的陷阱势垒高度为 0.65 V，比向下扫描提取的高 12%。栅极漏电电流随栅极偏压的变化表现出 220 K 以上的顺时针磁滞回线，但在 220 K 以下表现为逆时针磁滞回线。这种显著的相反磁滞现象通过陷阱机制得到了彻底的解释。


<details>
  <summary>Details</summary>
Motivation: 研究 GaN HEMT 在宽温度范围内（室温至低温）由表面陷阱引起的栅极漏电机制。

Method: 通过在不同温度下测量栅极漏电电流，并分析其与栅极偏压的关系，来识别和量化不同的漏电机制（二维变程跳跃、Poole--Frenkel 发射、陷阱辅助隧穿）。通过栅极扫描提取陷阱势垒高度，并分析磁滞回线的方向与温度的关系。

Result: 在小栅压下观察到二维变程跳跃。在高反向栅压下，220 K 以上漏电由 Poole--Frenkel 发射主导，220 K 以下转变为陷阱辅助隧穿。提取的陷阱势垒高度为 0.65 V（向上扫描）和 0.58 V（向下扫描）。220 K 以上观察到顺时针磁滞回线，220 K 以下观察到逆时针磁滞回线。

Conclusion: 表面陷阱引起 GaN HEMT 的栅极漏电机制受温度影响显著，存在两种主要的漏电机制（Poole--Frenkel 发射和陷阱辅助隧穿），并且漏电行为表现出与温度相关的磁滞现象。

Abstract: This work provides a detailed mapping of various mechanisms of
surface-trap-induced gate leakage in GaN HEMTs across a temperature range from
room to cryogenic levels. Two-dimensional variable-range hopping is observed at
small gate bias. Under higher reverse gate bias, the leakage is dominated by
the Poole--Frenkel emission above 220 K, but gradually transitions to the
trap-assisted tunneling below 220 K owing to the frozen-trap effect. The trap
barrier height extracted from the gate leakage current under the upward gate
sweep is 0.65 V, which is 12\% higher than that from the downward sweep. The
gate leakage current as a function of the gate bias exhibits clockwise
hysteresis loops above 220 K but counterclockwise ones below 220 K. This
remarkable opposite hysteresis phenomenon is thoroughly explained by the trap
mechanisms.

</details>


### [323] [Magnetic Flux-induced Higher-order Topological Superconductors](https://arxiv.org/abs/2510.14216)
*Jinpeng Xiao,Qianglin Hu,Zuodong Yu,Weipeng Chen,Xiaobing Luo*

Main category: cond-mat.mes-hall

TL;DR: 该研究提出了一个无需自旋-轨道耦合即可实现高阶拓扑超导性的模型。


<details>
  <summary>Details</summary>
Motivation: 为了寻找一种在不需要自旋-轨道耦合、复杂能带结构或磁序的情况下实现高阶拓扑超导性的新方法。

Method: 在二维系统中，通过引入交错磁通量、塞曼场和反铁磁序，并使塞曼场垂直于磁序矩来构造二阶拓扑超导体。在三维系统中，通过堆叠二维结构来理论上实现二阶和三阶拓扑超导体。

Result: 成功在二维和三维系统中理论上实现了二阶和三阶拓扑超导体，且该模型不依赖于自旋-轨道耦合。

Conclusion: 该模型提供了一种在更易于实现的条件下实现高阶拓扑超导性的新途径，为相关物理现象的研究和应用提供了理论基础。

Abstract: Higher-order topological superconductivity typically depends on spin-orbit
interaction, and often necessitates well designed sample structures, nodal
superconducting pairings or complex magnetic order. In this work, we propose a
model that incorporates a Zeeman field, antiferromagnetic order, and $s$-wave
superconducting pairing, all without the need for spin-orbit interaction. In a
two-dimensional system, we realize a second-order topological superconductor by
utilizing a staggered flux, provided that the Zeeman field is oriented
perpendicular to the magnetic order moments. In three-dimensional systems, we
achieve second- and third-order topological superconductors in theory, through
stacking the two-dimensional second-order topological superconductor.

</details>


### [324] [Electric field-induced spin-valley locking in twisted bilayer buckled honeycomb materials](https://arxiv.org/abs/2510.14404)
*Harold J. W. Zandvliet,Pantelis Bampoulis,Cristiane Morais Smith,Lumen Eek*

Main category: cond-mat.mes-hall

TL;DR: 扭曲的蜂窝双层材料（如双层石墨烯、硅烯和锗烯）在施加垂直电场时会产生复杂的量子谷霍尔效应。在特定范围的电场下，自旋自由度会与谷自由度锁定，从而提供更强的拓扑保护。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨扭曲的蜂窝双层材料（特别是双层石墨烯、硅烯和锗烯）在垂直电场作用下产生的量子谷霍尔效应，并研究其自旋-谷锁定机制及其对拓扑保护强度的影响。

Method: 通过分析扭曲的蜂窝双层材料（石墨烯、硅烯、锗烯）在垂直电场下的能带结构，研究了其莫尔超晶格、反转带隙的形成，以及由此产生的二维三角网络状的谷保护螺旋域边界态（量子谷霍尔效应）。特别关注了自旋-轨耦合和褶皱效应如何影响硅烯和锗烯中的量子谷霍尔效应，并确定了自旋-谷锁定的电场范围。

Result: 研究发现，在特定的电场范围内，扭曲双层硅烯和锗烯的量子谷霍尔态中，自旋自由度会与谷自由度锁定，形成更强的拓扑保护。低于该范围的电场，量子谷霍尔效应不出现；高于该范围的电场，自旋-谷锁定被解除，产生的量子谷霍尔态仅受谷保护。

Conclusion: 量子谷霍尔效应在扭曲双层材料中是一种重要的拓扑现象。通过调控电场，可以实现自旋-谷的锁定，从而增强其拓扑保护。这一发现为设计具有特定电子性质的新型拓扑材料提供了理论基础。

Abstract: A twisted honeycomb bilayer exhibits a moir\'e superstructure that is
composed of a hexagonal arrangement of AB and BA stacked domains separated by
domain boundaries. In the case of twisted bilayer graphene, the application of
an electric field normal to the bilayer leads to the opening of inverted band
gaps in the AB and BA stacked domains. The inverted band gaps result in the
formation of a two-dimensional triangular network of counterpropagating valley
protected helical domain boundary states, also referred to as the quantum
valley Hall effect. Owing to spin-orbit coupling and buckling, the quantum
valley Hall effect in twisted bilayer silicene and germanene is more complex
than in twisted bilayer graphene. We found that there is a range of electric
fields for which the spin degree of freedom is locked to the valley degree of
freedom of the electrons in the quantum valley Hall states, resulting in a
stronger topological protection. For electric fields smaller than the
aforementioned range the twisted bilayer does not exhibit the quantum valley
Hall effect, whereas for larger electric fields the spin-valley locking is
lifted and the emergent quantum valley Hall states are only valley-protected.

</details>


### [325] [Linearly polarized light enables chiral edge transport in quasi-2D Dirac materials](https://arxiv.org/abs/2510.14447)
*Mohammad Shafiei,Farhad Fazileh,Milorad V. Milošević*

Main category: cond-mat.mes-hall

TL;DR: 准二维狄拉克材料在具有动量依赖性的二次项的辅助下，可以通过线偏振光（LPL）诱导拓扑相变，从而产生手性边缘通道。这扩展了Floquet拓扑工程的潜力。


<details>
  <summary>Details</summary>
Motivation: 在准二维狄拉克材料中，探索线偏振光（LPL）诱导拓扑相变的可能性，以及其产生手性边缘通道的机制，并将其与高频光调控的拓扑相联系起来。

Method: 通过理论计算，考虑准二维狄拉克材料（以超薄Bi2Se3薄膜为例）中的二次动量项和与表面耦合的相互作用，分析线偏振光（LPL）照射下系统哈密顿量的变化，并研究其拓扑性质。

Result: 在线偏振光（LPL）作用下，准二维狄拉克材料中的二次动量项可以诱导拓扑相变，形成手性边缘通道。该相变在超薄Bi2Se3薄膜中是可实现的，且所需的激光强度在实验范围内。

Conclusion: 准二维材料是实现光控拓扑相的有效平台，为Floquet拓扑工程开辟了新的可能性。线偏振光（LPL）可以有效地诱导准二维狄拉克材料的拓扑相变，这为设计和控制量子材料的拓扑性质提供了新的途径。

Abstract: Floquet engineering with high-frequency light offers dynamic control over
topological phases in quantum materials. While in 3D Dirac systems circularly
polarized light is known to induce topological phase transitions via gap
opening, linearly polarized light (LPL) has generally been considered
ineffective. Here we show that in quasi-2D Dirac materials the second-order
momentum term arising from the intersurface coupling can induce a topological
phase transition under LPL, leading to chiral edge channels. Considering an
ultrathin Bi$_2$Se$_3$ film as a representative system, we show that this
transition occurs at experimentally accessible light intensities. Our results
thus promote quasi-2D materials as viable platforms for light-controlled
topological phases, expanding the potential of Floquet topological engineering.

</details>


### [326] [The fate of disorder in twisted bilayer graphene near the magic angle](https://arxiv.org/abs/2510.14567)
*Zhe Hou,Hailong Li,Qing Yan,Yu-Hang Li,Hua Jiang*

Main category: cond-mat.mes-hall

TL;DR: 中等强度的无序性可以增强TBG的电导率，而更强的无序性则会恢复局域化，这是一种由无序性驱动的非局域化-局域化输运行为。


<details>
  <summary>Details</summary>
Motivation: 在扭曲双层石墨烯（TBG）中，无序性在平带系统中的作用仍然模糊不清。TBG为研究这个问题提供了一个代表性的平带平台。

Method: 使用原子尺度的紧束缚量子输运计算方法，研究了TBG器件中无序性和平带的相互作用。

Result: 研究发现，中等强度的无序性可以增强电导率，而更强的无序性则会恢复局域化。通过对无序TBG圆柱体的能谱流分析，揭示了其物理机制是有效的层间隧穿强度。研究还表明，平带材料对无序性的响应与非平带材料不同。

Conclusion: 无序性在平带材料中起着反常的作用，并可能影响分数量子反常霍尔效应的观察。

Abstract: In disordered lattices, itinerant electrons typically undergo Anderson
localization due to random phase interference, which suppresses their motion.
By contrast, in flat-band systems where electrons are intrinsically localized
owing to their vanishing group velocity, the role of disorder remains elusive.
Twisted bilayer graphene (TBG) at the magic angle $\sim 1.1^\circ$ provides a
representative flat-band platform to investigate this problem. Here, we perform
an atomistic tight-binding quantum transport calculation on the interplay
between disorder and flat-bands in TBG devices. This non-phenomenological
approach provides direct evidence that moderate disorder enhances conductance,
whereas stronger disorder restores localization, revealing a disorder-driven
delocalization-to-localization transport behavior. The underlying physical
mechanism is understood by an effective inter-moir{\'e} tunneling strength via
spectral flow analysis of a disordered TBG cylinder. Moreover, by comparing
magic-angle and large-angle TBG, we demonstrate qualitatively distinct disorder
responses tied to the presence of flat-bands. Our quantitative results
highlight the unconventional role of disorder in flat-band moir{\'e} materials
and offer insights into the observation of the fractional quantum anomalous
Hall effect in disordered moir{\'e} systems.

</details>


### [327] [Precision of an autonomous demon exploiting nonthermal resources and information](https://arxiv.org/abs/2510.14578)
*Juliette Monsel,Matteo Acciai,Didrik Palmqvist,Nicolas Chiabrando,Rafael Sánchez,Janine Splettstoesser*

Main category: cond-mat.mes-hall

TL;DR: 本研究探究了一种多端点三点量子点系统，该系统可作为冰箱从冷的电子接触中提取热量，并利用非热资源，实现“妖魔化”的冷却，即平均而言不从资源中提取能量，但需要资源波动。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探究多端点三点量子点系统作为冰箱的运行机制，特别是其利用非热资源进行冷却的特性，并与标准热机进行对比。

Method: 利用全计数统计和随机轨迹分析，采用受热力学和动力学不确定性关系启发的性能量化指标，重点关注两种具有大输出功率的模式：基于信息和基于非热资源特性的模式。

Result: 研究表明，基于信息和非热资源特性的两种模式在冷却功率精度上存在显著差异。其中，基于非热资源特性的模式可以将冷却功率的涨落相对于输入涨落抑制一个数量级。

Conclusion: 本研究证实了两种不同工作原理的有效性，并通过分析输入输出热流和信息流之间的交叉相关性，为进一步理解量子热力学器件提供了新的视角。

Abstract: Quantum-dot systems serve as nanoscale heat engines exploiting thermal
fluctuations to perform a useful task. Here, we investigate a multi-terminal
triple-dot system, operating as a refrigerator that extracts heat from a cold
electronic contact. In contrast to standard heat engines, this system exploits
a nonthermal resource. This has the intriguing consequence that cooling can
occur without extracting energy from the resource on average -- a seemingly
demonic action -- while, however, requiring the resource to fluctuate. Using
full counting statistics and stochastic trajectories, we analyze the
performance of the device in terms of the cooling-power precision, employing
performance quantifiers motivated by the thermodynamic and kinetic uncertainty
relations. We focus on two regimes with large output power, which are based on
two operational principles: exploiting information on one hand and the
nonthermal properties of the resource on the other. We show that these regimes
significantly differ in precision. In particular, the regime exploiting the
nonthermal properties of the resource can have cooling-power fluctuations that
are suppressed with respect to the input fluctuations by an order of magnitude.
We also substantiate the interpretation of the two different working principles
by analyzing cross-correlations between input and output heat currents and
information flow.

</details>


### [328] [Magnetic D-brane solitons: skyrmion strings ending on a Néel wall in chiral magnets](https://arxiv.org/abs/2510.14689)
*Sven Bjarke Gudnason,Muneto Nitta*

Main category: cond-mat.mes-hall

TL;DR: 磁斯子串可以终止于畴壁，成为磁性D-brane的类似物。


<details>
  <summary>Details</summary>
Motivation: 探索三维磁斯子串的基本作用，特别是它们如何终止于畴壁。

Method: 研究了斯子串终止于N'eel型畴壁的情况，并将其与场论中的D-brane进行比较，分析了DMI对畴壁弯曲的影响以及斯子串的宽度。

Result: 发现斯子串可以稳定N'eel型畴壁，DMI导致畴壁线性弯曲，斯子串具有有限宽度，并且通过周期性多结点的解决方案，实现了斯子串和畴壁变形的方形格子。

Conclusion: 磁斯子串是基本弦，可以终止于D-brane。

Abstract: Magnetic skyrmions extended to three dimensions form string-like objects
whose fundamental role remains largely unexplored. We show that skyrmion
strings can terminate on a N\'eel-type domain wall (DW), realizing a magnetic
analogue of a Dirichlet(D)-brane soliton. While an isolated N\'eel DW tends to
rotate into a Bloch DW, the N\'eel DW is stabilized when a skyrmion string ends
on it. Unlike field-theory D-branes, the Bloch-type DMI produces linear rather
than logarithmic DW bending, and the strings retain finite width far from the
DW, circumventing singular behavior. Furthermore, the repulsive interaction
between strings allows periodic multi-junction solutions, yielding a square
lattice of alternating strings and local DW deformations. These results
establish magnetic skyrmion strings as fundamental strings that can end on a
D-brane.

</details>


### [329] [Quantum beats of exciton-polarons in CsPbI3 perovskite nanocrystals](https://arxiv.org/abs/2510.14695)
*A. V. Trifonov,M. O. Nestoklon,M. -A. Hollberg,S. Grisard,D. Kudlacik,E. V. Kolobkova,M. S. Kuznetsova,S. V. Goupalov,J. M. Kaspari,D. E. Reiter,D. R. Yakovlev,M. Bayer,I. A. Akimov*

Main category: cond-mat.mes-hall

TL;DR: Exciton-phonon interactions in CsPbI3 nanocrystals show exceptionally long coherence times, allowing for the quantification of exciton-phonon coupling and opening paths for tuning optical properties for quantum technologies.


<details>
  <summary>Details</summary>
Motivation: Investigate the strong exciton-phonon interactions in lead-halide perovskite nanocrystals and establish a new regime of coherent exciton-polaron dynamics.

Method: Using transient two-pulse photon echo at 2 K temperature to observe quantum beats between exciton-polaron states in CsPbI3 nanocrystals embedded in a glass matrix, and analyze within a four-level model.

Result: Observed quantum beats between exciton-polaron states, quantified exciton-phonon coupling strength through Huang-Rhys factors (0.05-0.1 and 0.02-0.04) for optical phonons with energies of 3.2 and 5.1 meV, and found a pronounced size dependence of coupling strengths and phonon lifetimes.

Conclusion: A new regime of coherent exciton-polaron dynamics with long coherence times (~300 ps) was established. The results provide a path to tune optical transitions and tailor coherent optical dynamics in perovskite semiconductors for solid-state quantum technologies.

Abstract: Exciton-phonon interactions govern the energy level spectrum and thus the
optical response in semiconductors. In this respect, lead-halide perovskite
nanocrystals represent a unique system, for which the interaction with optical
phonons is particularly strong, giving rise to a ladder of multiple exciton
states which can be optically excited with femtosecond pulses. We establish a
new regime of coherent exciton-polaron dynamics with exceptionally long
coherence times (T2 ~300 ps) in an ensemble of CsPbI3 nanocrystals embedded in
a glass matrix. Using transient two-pulse photon echo at 2 K temperature, we
observe quantum beats between the exciton-polaron states. Within a four-level
model, we directly quantify the exciton-phonon coupling strength through the
Huang-Rhys factors of 0.05-0.1 and 0.02-0.04 for low-energy optical phonons
with energies of 3.2 and 5.1 meV, respectively. The pronounced size dependence
of both coupling strengths and phonon lifetimes offers a path to tune the
optical transitions between polaron states and to tailor the coherent optical
dynamics in perovskite semiconductors for solid-state quantum technologies.

</details>


### [330] [Fundamental quantum and relativistic formulation of thermal noise and linear conductance in an 1D quasi-particle ensemble under ballistic transport-regime](https://arxiv.org/abs/2510.14721)
*Lino Reggiani,Federico Intini,Luca Varani*

Main category: cond-mat.mes-hall

TL;DR: 该论文研究了电磁场低频下噪声功率谱、电流-噪声谱与线性响应电导之间的涨落-耗散关系中的量子和量子相对论效应。


<details>
  <summary>Details</summary>
Motivation: 论文旨在探讨量子和量子相对论效应对噪声功率谱以及电流-噪声谱与线性响应电导之间涨落-耗散关系的影响。

Method: 在分析中，研究了高频下的真空灾难问题，并提出了卡西米尔力可以避免该现象。同时，在低频下，论文回顾了一维结构在纳米尺度长度的弹道输运条件下的量子效应，并采用了通用的准粒子方法。此外，论文还探讨了黑体腔内光子气与原子元素线状光谱之间的关系，并基于物理常数 $\alpha =1/137.0560$ 进行了精确的统计分析。

Result: 研究表明，卡西米尔力在高频下可以避免真空灾难。论文还回顾了低频下量子效应在纳米尺度结构中的应用，并提出了一种光子气在黑体腔内产生原子元素线状光谱的物理机制。

Conclusion: 论文通过分析量子和量子相对论效应，提出了卡西米尔力在高频下的作用，并解释了低频下量子效应在纳米结构中的应用。最后，论文还为原子元素线状光谱的产生提供了一种基于物理常数的统计解释。

Abstract: We investigate quantum and quantum-relativistic effects associated with the
noise power spectrum and the fluctuation--dissipation relation between
current--noise spectra and linear--response conductance at low frequencies of
the electromagnetic field. At high frequencies, vacuum catastrophe is shown to
be avoided by the presence of Casimir force. At low frequencies, the quantum
effect associated with one--dimensional structures under the conditions of
ballistic transport typical at the nanometric scale length are briefly reviewed
in terms of a universal quasi-particle approach. The case of a photon gas
inside an appropriate black-body cavity is found to provide a physical
interpretation of the lines spectra of atomic elements within an exact
statistical approach based on a physical interpretation of the fine structure
constant, $\alpha =1/137.0560$.

</details>


### [331] [Disorder-assisted Spin-Filtering at Metal/Ferromagnet Interfaces: An Alternative Route to Anisotropic Magnetoresistance](https://arxiv.org/abs/2510.14867)
*Ivan Iorsh,Mikhail Titov*

Main category: cond-mat.mes-hall

TL;DR: 通过界面散射产生各向异性磁阻效应


<details>
  <summary>Details</summary>
Motivation: 在金属/铁磁体双层结构中，无需利用体自旋或轨道霍尔电流即可产生显著的各向异性磁阻（AMR）效应。

Method: 利用界面交换和Rashba自旋-轨道耦合的δ层模型，通过界面电荷转移产生具有自旋选择性的相位条件（界面自旋过滤），从而抑制一个自旋投影的回散射，同时增强另一个自旋投影的动量弛豫。

Result: 该模型能够定量复现通常归因于自旋霍尔磁阻（SMR）的厚度和角度依赖性，以及其特征幅度。AMR在金属厚度约为几纳米时达到峰值。最大AMR与交换或自旋-轨道耦合强度中较小者呈线性关系。

Conclusion: 该机制与SMR有本质区别，并提出了一种新的AMR产生机制，该机制对界面电荷转移和无序敏感。

Abstract: We introduce a minimal interface-scattering mechanism that produces a sizable
anisotropic magnetoresistance (AMR) in metal/ferromagnet bilayers (e.g.,
Pt/YIG) without invoking bulk spin or orbital Hall currents. In a
$\delta$-layer model with interfacial exchange and Rashba spin-orbit coupling,
charge transfer at a high-quality interface creates a spin-selective phase
condition (interfacial spin filtering) that suppresses backscattering for one
spin projection while enhancing momentum relaxation for the other. The
resulting resistance anisotropy peaks at an optimal metal thickness of a few
nanometers, quantitatively reproducing the thickness and angular dependences
typically attributed to spin Hall magnetoresistance (SMR), as well as its
characteristic magnitude. Remarkably, the maximal AMR scales linearly with the
smaller of the two coupling strengths - exchange or spin-orbit, highlighting a
mechanism fundamentally distinct from SMR. Our scattering formulation maps onto
Boltzmann boundary conditions and predicts other clear discriminants from SMR,
including strong sensitivity to interfacial charge transfer and disorder.

</details>


### [332] [Electron transport in junctions between altermagnets](https://arxiv.org/abs/2510.14868)
*Shubham Ghadigaonkar,Sachchidanand Das,Abhiram Soori*

Main category: cond-mat.mes-hall

TL;DR: 文章研究了不同磁性相（强磁性和弱磁性）下，两种反常磁体（AMs）连接处的电子输运特性，重点分析了电荷和自旋电导率随两种AMs的Néel矢量夹角（θ）的变化关系。


<details>
  <summary>Details</summary>
Motivation: 研究反常磁体（AMs）异质结中的电子输运特性，探索其在自旋电子学领域的应用潜力。

Method: 理论上研究了强、弱反常磁性相下AMs连接处的电子输运，分析了电荷和自旋电导率随Néel矢量夹角（θ）的变化。引入普通金属层，研究了电荷电导率的法布里-珀罗型振荡。

Result: 在强反常磁性相中，电荷电导率在θ→π时消失，而在弱反常磁性相中则保持有限。强相传输主要由自旋向上的电子主导，而弱相中，两个自旋通道均有贡献。在强相中，引入普通金属导致电荷电导率出现法布里-珀罗型振荡。

Conclusion: 反常磁体（AMs）基异质结在自旋电子学应用方面具有巨大潜力，例如作为自旋滤波器和利用可调谐的自旋相关输运及量子干涉效应的量子干涉基自旋电子器件。

Abstract: We theoretically investigate electron transport in junctions between the two
AMs in strong and weak altermagnetic phases. The charge and spin conductivities
are analyzed as functions of angle between the N\'eel vectors of the two AMs
$\theta$. In the strong AM regime, the charge conductivity vanishes as $\theta
\to \pi$, while in the weak AM phase it remains finite. Introducing a normal
metal between two AMs leads to Fabry-P\'erot-type oscillations in charge
conductivity. In the strong phase, transport is dominated by up-spin electrons,
whereas both spin channels contribute in the weak phase. These results
highlight the potential of AM-based heterostructures for spintronic
applications, such as spin filters, and quantum interference-based spintronic
devices, where tunable spin-dependent transport and interference effects can be
utilized in electronic devices.

</details>


### [333] [Electric field controlled second-order anomalous Hall effect in altermagnets](https://arxiv.org/abs/2510.14899)
*Arnob Mukherjee,Biplab Sanyal,Annica M. Black-Schaffer,Ankita Bhattacharya*

Main category: cond-mat.mes-hall

TL;DR: 尽管纯d波交替磁体中的线性和二阶异常霍尔响应被禁止，但本研究发现，占优布洛赫态的非平凡量子度量可以产生一个由电场诱导的贝里曲率偶极子，从而产生一个强且可调的二阶霍尔电流。该电流可以通过调整对称性降低的直流场和交流探测场之间的相对方向来控制。


<details>
  <summary>Details</summary>
Motivation: 在具有动量相关自旋劈裂和反常输运特性的补偿磁体交替磁体中，即使没有净磁化强度，也存在着有趣的现象。特别是，在结合了四重旋转和时间反演（C4T）对称性的情况下，纯d波交替磁体中的线性和二阶异常霍尔响应被禁止。本研究的动机是探索在这种禁止的情况下是否存在新的机制来产生霍尔响应。

Method: 研究人员提出，占优布洛赫态的非平凡量子度量可以产生一个由电场诱导的贝里曲率偶极子，该偶极子可以产生一个强且可调的二阶霍尔电流。他们通过研究二维Rashba耦合混合交替磁体中电场诱导的二阶异常霍尔响应来验证这一观点，该磁体在d_{x^2-y^2}（B1g）和d_{xy}（B2g）交替磁体对称性之间进行插值。

Result: 研究发现，二阶霍尔电流是可调的，并且可以通过调整直流场和交流探测场之间的相对方向来控制。此外，非线性信号对掺杂水平下的交替磁体序底层的对称性高度敏感，这提供了一种区分不同交替磁体序的纯粹电学方法。

Conclusion: 混合交替磁体为可控的非线性输运和自旋电子学应用提供了一个有前途的平台。该研究发现了由电场诱导的二阶异常霍尔效应，即使在通常禁止这些效应的对称性下也是如此，并提出了一种利用该效应来区分不同交替磁体序的电学方法。

Abstract: Altermagnets are a recently discovered class of compensated magnets with
momentum-dependent spin splittings and unusual transport properties, even
without a net magnetization. In the presence of combined four-fold rotation and
time-reversal ($C_4\mathcal{T}$) symmetry, linear and also second-order, driven
by a Berry curvature dipole, anomalous Hall responses are forbidden in any pure
$d$-wave altermagnet. Nevertheless, here we find that the nontrivial quantum
metric of the occupied Bloch states allows for an electric field induced Berry
curvature dipole, which generates a strong and tunable second-order Hall
current, enabling it to be switched on or off by simply adjusting the relative
orientation between the symmetry-reducing dc field and the ac probe field.
Specifically, we investigate the electric field induced second-order anomalous
Hall response in a two-dimensional Rashba-coupled hybrid altermagnet that
interpolates between $d_{x^2-y^2}$ ($B_{1g}$) and $d_{xy}$ ($B_{2g}$)
altermagnet symmetry, motivated by recent proposals for mixed-symmetry states.
Crucially, the nonlinear signal is highly sensitive to the underlying symmetry
of the altermagnetic order at specific doping levels, offering a purely
electrical method to distinguish distinct altermagnetic orders. Our results
position hybrid altermagnets as a promising platform for controllable nonlinear
transport and spintronic applications.

</details>


### [334] [Skyrmion behavior in attractive-repulsive square array of pinning centers](https://arxiv.org/abs/2510.14903)
*L. Basseto,N. P. Vizarim,J. C. Bellizotti Souza,P. A. Venegas*

Main category: cond-mat.mes-hall

TL;DR: 混合钉扎效应对斯格明子动力学行为和有效霍尔效应进行调控，为基于斯格明子的存储和逻辑器件的设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 研究混合钉扎效应对斯格明子动力学行为的影响，以期调控斯格明子轨迹和霍尔效应。

Method: 使用基于粒子的模型，模拟研究了在混合钉扎位点（同时存在吸引和排斥缺陷）的方形晶格中单个斯格明子的驱动动力学。

Result: 混合钉扎导致了方向锁定（在 $\theta_{\rm sk}=-45^\circ$ 处）和在接近固有斯格明子霍尔角的方向锁定角度附近的流动。通过调整钉扎强度，发现较弱的吸引力会降低退钉阈值，而较强的排斥力则能稳定并拓宽-45°锁定平台。此外，吸引和排斥缺陷强度的组合可以控制方向锁定及其作用范围。缺陷尺寸也影响响应，可以选择-45°、-50°、-55°和约-59°等锁定角度。

Conclusion: 混合钉扎是一种有效的调控斯格明子轨迹和有效霍尔效应的方法，可以为设计基于斯格明子的存储和逻辑器件提供指导。

Abstract: We investigate the driven dynamics of a single skyrmion in a square lattice
of mixed pinning sites, where attractive and repulsive defects coexist using a
particle-based model. The mixed landscape yields directional locking at
$\theta_{\rm sk}=-45^\circ$ and flow at locked angles near the intrinsic
skyrmion Hall angle. By mapping defect strengths, we show that weaker
attraction lowers the depinning threshold, whereas stronger repulsion
stabilizes and broadens the $-45^\circ$ locking plateau. Moreover, combinations
of attractive and repulsive defect strengths allows control of directional
lockings and their force ranges. Defect size further tunes the response,
selecting among $-45^\circ$, $-50^\circ$, $-55^\circ$, and $\approx-59^\circ$.
These results establish mixed pinning as a practical knob to steer skyrmion
trajectories and the effective Hall response, providing design guidelines for
skyrmion-based memory and logic devices.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [335] [A Diffusion-Refined Planner with Reinforcement Learning Priors for Confined-Space Parking](https://arxiv.org/abs/2510.14000)
*Mingyang Jiang,Yueyuan Li,Jiaru Zhang,Songan Zhang,Ming Yang*

Main category: cs.RO

TL;DR: DRIP是一个基于强化学习（RL）先验动作分布的扩散精炼规划器，用于解决自动化停车规划问题，尤其是在狭窄空间内。


<details>
  <summary>Details</summary>
Motivation: 现有自动化停车规划方法在精确建模最优动作分布方面存在挑战，尤其是在限制性和复杂环境中。

Method: 提出DRIP方法，该方法结合了强化学习（RL）的先验动作分布和扩散模型。RL预训练策略为扩散训练过程提供先验动作分布以进行正则化，在推理阶段，去噪过程将这些粗糙的先验细化为更精确的动作分布。

Result: DRIP在不同空间限制的停车场景中进行了评估，实验结果表明该方法显著提高了狭窄空间停车环境中的规划性能，并保持了在常见场景中的良好泛化能力。

Conclusion: DRIP通过利用RL先验分布引导训练过程中的去噪轨迹，能够继承信息更丰富的初始化，从而实现更精确的动作建模、更高的规划成功率和更少的推理步骤。

Abstract: The growing demand for parking has increased the need for automated parking
planning methods that can operate reliably in confined spaces. In restricted
and complex environments, high-precision maneuvers are required to achieve a
high success rate in planning, yet existing approaches often rely on explicit
action modeling, which faces challenges when accurately modeling the optimal
action distribution. In this paper, we propose DRIP, a diffusion-refined
planner anchored in reinforcement learning (RL) prior action distribution, in
which an RL-pretrained policy provides prior action distributions to regularize
the diffusion training process. During the inference phase the denoising
process refines these coarse priors into more precise action distributions. By
steering the denoising trajectory through the reinforcement learning prior
distribution during training, the diffusion model inherits a well-informed
initialization, resulting in more accurate action modeling, a higher planning
success rate, and reduced inference steps. We evaluate our approach across
parking scenarios with varying degrees of spatial constraints. Experimental
results demonstrate that our method significantly improves planning performance
in confined-space parking environments while maintaining strong generalization
in common scenarios.

</details>


### [336] [Spatially Intelligent Patrol Routes for Concealed Emitter Localization by Robot Swarms](https://arxiv.org/abs/2510.14018)
*Adam Morris,Timothy Pelham,Edmund R. Hunt*

Main category: cs.RO

TL;DR: 该论文提出了一种利用差分进化算法生成巡逻路线，使机器人可以自主定位未知信号源的方法。


<details>
  <summary>Details</summary>
Motivation: 在电磁监测中，自主定位隐藏的无线电信标是一个关键挑战，因为这通常需要独立于信号源参数进行。

Method: 研究人员使用差分进化算法来生成几何巡逻路线，并模拟了四机器人蜂拥在八种配置下的行为。通过改变巡逻形状和天线类型（全向或定向）来评估信息增益和覆盖范围。

Result: 仿真结果表明，定向天线的检测成功率（98.75%）高于全向天线（80.25%），并且定向天线的定位误差（1.01-1.30米）也小于全向天线（1.67-1.90米）。全向天线的定位成功率主要受信号源位置影响，在地图边缘区域的失败率较高。

Conclusion: 机器人的环境感知能力直接取决于其与环境的物理交互，因此，通过优化巡逻路线和选择天线类型实现的“空间智能”对于有效的机器人监控至关重要。

Abstract: This paper introduces a method for designing spatially intelligent robot
swarm behaviors to localize concealed radio emitters. We use differential
evolution to generate geometric patrol routes that localize unknown signals
independently of emitter parameters, a key challenge in electromagnetic
surveillance. Patrol shape and antenna type are shown to influence information
gain, which in turn determines the effective triangulation coverage. We
simulate a four-robot swarm across eight configurations, assigning
pre-generated patrol routes based on a specified patrol shape and sensing
capability (antenna type: omnidirectional or directional). An emitter is placed
within the map for each trial, with randomized position, transmission power and
frequency. Results show that omnidirectional localization success rates are
driven primarily by source location rather than signal properties, with
failures occurring most often when sources are placed in peripheral areas of
the map. Directional antennas are able to overcome this limitation due to their
higher gain and directivity, with an average detection success rate of 98.75%
compared to 80.25% for omnidirectional. Average localization errors range from
1.01-1.30 m for directional sensing and 1.67-1.90 m for omnidirectional
sensing; while directional sensing also benefits from shorter patrol edges.
These results demonstrate that a swarm's ability to predict electromagnetic
phenomena is directly dependent on its physical interaction with the
environment. Consequently, spatial intelligence, realized here through
optimized patrol routes and antenna selection, is a critical design
consideration for effective robotic surveillance.

</details>


### [337] [Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming](https://arxiv.org/abs/2510.14063)
*Nan Li,Jiming Ren,Haris Miller,Samuel Coogan,Karen M. Feigh,Ye Zhao*

Main category: cs.RO

TL;DR: OATH是一个用于异构机器人团队的新型自适应障碍感知任务分配与规划框架，通过引入自适应Halton序列图和集成障碍感知聚类、加权拍卖及集群内任务选择的框架，有效解决了大规模、空间推理和动态环境下的任务分配挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的多人任务分配和规划（MATP）方法在可扩展性、空间推理和富含障碍的环境适应性方面存在挑战。

Method: 提出了一种名为OATH的框架，该框架包含两个主要创新：1. 自适应Halton序列图，这是Halton采样在MATP中的首次应用，能够根据障碍物分布调整采样密度。2. 集群-拍卖-选择框架，结合了障碍物感知聚类、加权拍卖和集群内任务选择，以实现异构机器人间的有效协调。此外，还利用大型语言模型（LLM）来解析人类指令并实时指导规划器。

Result: 在NVIDIA Isaac Sim中进行的验证显示，与现有的MATP基线方法相比，OATH在任务分配质量、可扩展性、对动态变化的适应性以及整体执行性能方面取得了显著改进。

Conclusion: OATH通过其新颖的障碍感知策略和集成框架，成功克服了MATP在复杂环境下面临的挑战，并在模拟环境中展现出优越的性能。

Abstract: Multi-Agent Task Assignment and Planning (MATP) has attracted growing
attention but remains challenging in terms of scalability, spatial reasoning,
and adaptability in obstacle-rich environments. To address these challenges, we
propose OATH: Adaptive Obstacle-Aware Task Assignment and Planning for
Heterogeneous Robot Teaming, which advances MATP by introducing a novel
obstacle-aware strategy for task assignment. First, we develop an adaptive
Halton sequence map, the first known application of Halton sampling with
obstacle-aware adaptation in MATP, which adjusts sampling density based on
obstacle distribution. Second, we propose a cluster-auction-selection framework
that integrates obstacle-aware clustering with weighted auctions and
intra-cluster task selection. These mechanisms jointly enable effective
coordination among heterogeneous robots while maintaining scalability and
near-optimal allocation performance. In addition, our framework leverages an
LLM to interpret human instructions and directly guide the planner in real
time. We validate OATH in NVIDIA Isaac Sim, showing substantial improvements in
task assignment quality, scalability, adaptability to dynamic changes, and
overall execution performance compared to state-of-the-art MATP baselines. A
project website is available at https://llm-oath.github.io/.

</details>


### [338] [Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning](https://arxiv.org/abs/2510.14065)
*Gaoyuan Liu,Joris de Winter,Yuri Durodie,Denis Steckelmacher,Ann Nowe,Bram Vanderborght*

Main category: cs.RO

TL;DR: 该方法将强化学习技能集成到任务和运动规划（TAMP）中，以处理不确定的概率性动作，并提高了规划效率。


<details>
  <summary>Details</summary>
Motivation: TAMP 在处理不确定的概率性动作方面存在挑战，而强化学习（RL）在学习灵活的、对不确定性具有鲁棒性的操作技能方面表现出色。

Method: 通过数据驱动的逻辑组件定义 RL 技能，并将其集成到 TAMP 流程中。设计了一个计划优化子程序来处理不确定性。

Result: 实验结果表明，与基线方法相比，该方法在包含概率性技能的领域中扩展了 TAMP 的能力，并提高了规划效率。

Conclusion: 将 RL 技能嵌入 TAMP 管道可以克服 TAMP 在处理不确定性方面的挑战，并提高规划效率。

Abstract: Task and motion planning (TAMP) for robotics manipulation necessitates
long-horizon reasoning involving versatile actions and skills. While
deterministic actions can be crafted by sampling or optimizing with certain
constraints, planning actions with uncertainty, i.e., probabilistic actions,
remains a challenge for TAMP. On the contrary, Reinforcement Learning (RL)
excels in acquiring versatile, yet short-horizon, manipulation skills that are
robust with uncertainties. In this letter, we design a method that integrates
RL skills into TAMP pipelines. Besides the policy, a RL skill is defined with
data-driven logical components that enable the skill to be deployed by symbolic
planning. A plan refinement sub-routine is designed to further tackle the
inevitable effect uncertainties. In the experiments, we compare our method with
baseline hierarchical planning from both TAMP and RL fields and illustrate the
strength of the method. The results show that by embedding RL skills, we extend
the capability of TAMP to domains with probabilistic skills, and improve the
planning efficiency compared to the previous methods.

</details>


### [339] [Partial Feedback Linearization Control of a Cable-Suspended Multirotor Platform for Stabilization of an Attached Load](https://arxiv.org/abs/2510.14072)
*Hemjyoti Das,Christian Ott*

Main category: cs.RO

TL;DR: 本研究提出了一种基于部分反馈线性化（PFL）的控制方法，用于稳定带有负载的悬挂空中平台，适用于建筑起重机等应用场景。


<details>
  <summary>Details</summary>
Motivation: 该控制方法旨在解决带有负载的悬挂空中平台的稳定化问题，这些平台在建筑起重机等场景中有潜在应用价值。

Method: 提出了一种基于部分反馈线性化（PFL）的控制方法，该方法利用了系统的耦合动力学来实现稳定化，并考虑了系统的欠驱动特性。

Result: 通过数值稳定性分析表明，耦合项对于系统的稳定至关重要。此外，还进行了鲁棒性分析，以评估在风扰、传感器噪声和系统不确定性存在下的性能。

Conclusion: 该研究提出的控制方法仅依赖于车载传感器，适用于室外建筑工地等目标应用场景。通过广泛的仿真和实验研究验证了该方法的有效性。

Abstract: In this work, we present a novel control approach based on partial feedback
linearization (PFL) for the stabilization of a suspended aerial platform with
an attached load. Such systems are envisioned for various applications in
construction sites involving cranes, such as the holding and transportation of
heavy objects. Our proposed control approach considers the underactuation of
the whole system while utilizing its coupled dynamics for stabilization. We
demonstrate using numerical stability analysis that these coupled terms are
crucial for the stabilization of the complete system. We also carried out
robustness analysis of the proposed approach in the presence of external wind
disturbances, sensor noise, and uncertainties in system dynamics. As our
envisioned target application involves cranes in outdoor construction sites,
our control approaches rely on only onboard sensors, thus making it suitable
for such applications. We carried out extensive simulation studies and
experimental tests to validate our proposed control approach.

</details>


### [340] [When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks](https://arxiv.org/abs/2510.14677)
*Steffen Hagedorn,Luka Donkov,Aron Distelzweig,Alexandru P. Condurache*

Main category: cs.RO

TL;DR: 集成了SMART模型到nuPlan中，以更真实的交通流评估自动驾驶规划器，发现现有基于IDM的评估会高估规划器性能，并提出SMART-reactive simulation作为新的基准。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的交通代理（如IDM）行为过于简化，无法充分暴露规划器的缺陷，导致对规划器性能的评估产生偏差。需要更真实的仿真环境来评估规划器在复杂交互场景下的能力。

Method: 将先进的基于学习的交通代理模型SMART集成到nuPlan仿真环境中，并使用该环境对14个近期规划器和基线模型进行了评估。对比了在基于IDM和SMART的仿真环境下的规划器性能得分。

Result: 基于SMART的仿真显示，现有评估方法普遍高估了规划器的性能，大多数规划器的得分有所下降。然而，许多规划器在多车道、交互密集场景（如变道、转弯）下表现出更好的交互能力，甚至性能有所提升。在闭环中训练的规划器表现出最佳且最稳定的驾驶性能。但在边缘案例场景下，所有学习型规划器性能都会突然下降，而基于规则的规划器仍能保持基本行为。

Conclusion: SMART-reactive simulation应作为nuPlan中的新标准闭环基准，以更准确地评估自动驾驶规划器。SMART代理可作为IDM的替代方案。

Abstract: Planner evaluation in closed-loop simulation often uses rule-based traffic
agents, whose simplistic and passive behavior can hide planner deficiencies and
bias rankings. Widely used IDM agents simply follow a lead vehicle and cannot
react to vehicles in adjacent lanes, hindering tests of complex interaction
capabilities. We address this issue by integrating the state-of-the-art learned
traffic agent model SMART into nuPlan. Thus, we are the first to evaluate
planners under more realistic conditions and quantify how conclusions shift
when narrowing the sim-to-real gap. Our analysis covers 14 recent planners and
established baselines and shows that IDM-based simulation overestimates
planning performance: nearly all scores deteriorate. In contrast, many planners
interact better than previously assumed and even improve in multi-lane,
interaction-heavy scenarios like lane changes or turns. Methods trained in
closed-loop demonstrate the best and most stable driving performance. However,
when reaching their limits in augmented edge-case scenarios, all learned
planners degrade abruptly, whereas rule-based planners maintain reasonable
basic behavior. Based on our results, we suggest SMART-reactive simulation as a
new standard closed-loop benchmark in nuPlan and release the SMART agents as a
drop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop.

</details>


### [341] [ViTacGen: Robotic Pushing with Vision-to-Touch Generation](https://arxiv.org/abs/2510.14117)
*Zhiyuan Wu,Yijiong Lin,Yongqiang Zhao,Xuyang Zhang,Zhuo Chen,Nathan Lepora,Shan Luo*

Main category: cs.RO

TL;DR: 该研究提出了一种名为ViTacGen的机器人操作框架，利用视觉信息生成触觉反馈，以克服真实触觉传感器的硬件限制，并在机器人推物体任务中实现了高成功率。


<details>
  <summary>Details</summary>
Motivation: 真实触觉传感器成本高、易损坏且部署困难，而纯视觉策略性能不佳，因此需要一种无需高精度触觉传感器即可实现有效操作的方法。

Method: ViTacGen框架包括一个视觉到触觉生成网络，可以直接从视觉图像序列生成接触深度图，以及一个结合视觉和生成触觉数据的强化学习策略，该策略使用基于视觉和生成触觉观测的对比学习。

Result: 在模拟和真实世界实验中，ViTacGen框架在机器人推物体任务中表现出色，成功率高达86%。

Conclusion: ViTacGen框架通过从视觉信息生成触觉反馈，成功消除了对高精度真实触觉传感器的依赖，实现了有效的零样本部署，并在机器人操作任务中取得了优异的性能。

Abstract: Robotic pushing is a fundamental manipulation task that requires tactile
feedback to capture subtle contact forces and dynamics between the end-effector
and the object. However, real tactile sensors often face hardware limitations
such as high costs and fragility, and deployment challenges involving
calibration and variations between different sensors, while vision-only
policies struggle with satisfactory performance. Inspired by humans' ability to
infer tactile states from vision, we propose ViTacGen, a novel robot
manipulation framework designed for visual robotic pushing with vision-to-touch
generation in reinforcement learning to eliminate the reliance on
high-resolution real tactile sensors, enabling effective zero-shot deployment
on visual-only robotic systems. Specifically, ViTacGen consists of an
encoder-decoder vision-to-touch generation network that generates contact depth
images, a standardized tactile representation, directly from visual image
sequence, followed by a reinforcement learning policy that fuses visual-tactile
data with contrastive learning based on visual and generated tactile
observations. We validate the effectiveness of our approach in both simulation
and real world experiments, demonstrating its superior performance and
achieving a success rate of up to 86\%.

</details>


### [342] [Multi Agent Switching Mode Controller for Sound Source localization](https://arxiv.org/abs/2510.14849)
*Marcello Sorge,Nicola Cigarini,Riccardo Lorigiola,Giulia Michieletto,Andrea Masiero,Angelo Cenedese,Alberto Guarnieri*

Main category: cs.RO

TL;DR: 该论文提出了一种用于声学目标定位的多智能体切换模式控制策略。


<details>
  <summary>Details</summary>
Motivation: 声源定位是机器人研究中的一个重要课题，尤其是在声学传感器方面，因为它们可以使智能体在无法建立直接视线的情况下定位目标。

Method: 提出了一种多智能体切换模式控制策略，包括单一目标定位（智能体保持固定编队）和多目标定位（智能体独立搜索）。

Result: 该研究设计了一种用于声学目标定位的多智能体切换模式控制策略。

Conclusion: 该研究设计了一种用于声学目标定位的多智能体切换模式控制策略。

Abstract: Source seeking is an important topic in robotic research, especially
considering sound-based sensors since they allow the agents to locate a target
even in critical conditions where it is not possible to establish a direct line
of sight. In this work, we design a multi- agent switching mode control
strategy for acoustic-based target localization. Two scenarios are considered:
single source localization, in which the agents are driven maintaining a rigid
formation towards the target, and multi-source scenario, in which each agent
searches for the targets independently from the others.

</details>


### [343] [Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space](https://arxiv.org/abs/2510.14234)
*Ning Han,Gu Gong,Bin Zhang,Yuexuan Xu,Bohan Yang,Yunhui Liu,David Navarro-Alarcon*

Main category: cs.RO

TL;DR: 提出一种模型无关的、基于关键点约束的3D可变形物体形状控制方法，利用深度学习提取关键点，简化为视觉伺服问题，并通过带约束的性能控制提高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 3D可变形物体因其无限维状态空间和复杂动力学特性，给机器人操控带来巨大挑战。

Method: 利用深度学习提取可变形物体的关键点作为特征向量，结合变形雅可比矩阵描述形状动力学，并采用集成了障碍李雅普诺夫函数（BLF）的预设性能控制方法来强制执行关键点约束。

Result: 实验结果证明了该方法在控制精度和鲁棒性方面的有效性。

Conclusion: 所提出的方法能够有效控制3D可变形物体的形状，并满足关键点约束，具有良好的鲁棒性。

Abstract: Manipulating three-dimensional (3D) deformable objects presents significant
challenges for robotic systems due to their infinite-dimensional state space
and complex deformable dynamics. This paper proposes a novel model-free
approach for shape control with constraints imposed on key points. Unlike
existing methods that rely on feature dimensionality reduction, the proposed
controller leverages the coordinates of key points as the feature vector, which
are extracted from the deformable object's point cloud using deep learning
methods. This approach not only reduces the dimensionality of the feature space
but also retains the spatial information of the object. By extracting key
points, the manipulation of deformable objects is simplified into a visual
servoing problem, where the shape dynamics are described using a deformation
Jacobian matrix. To enhance control accuracy, a prescribed performance control
method is developed by integrating barrier Lyapunov functions (BLF) to enforce
constraints on the key points. The stability of the closed-loop system is
rigorously analyzed and verified using the Lyapunov method. Experimental
results further demonstrate the effectiveness and robustness of the proposed
method.

</details>


### [344] [SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time](https://arxiv.org/abs/2510.14851)
*Jakob Bichler,Andreu Matoses Gimenez,Javier Alonso-Mora*

Main category: cs.RO

TL;DR: Sadcher是一个用于异构多机器人团队的实时任务分配框架，通过动态联盟形成和任务前置约束来优化分配。


<details>
  <summary>Details</summary>
Motivation: 提出一个能够处理异构多机器人团队、动态联盟形成和任务前置约束的实时任务分配框架。

Method: 使用模仿学习训练，结合图注意力（Graph Attention）和Transformer来预测机器人-任务分配奖励，并通过松弛二分图匹配生成高质量、有可行性保证的调度。该方法显式建模了机器人和任务的位置、任务持续时间和机器人的剩余处理时间，实现了高级时空推理，并能泛化到具有不同时空分布的环境。

Result: 在随机的、未见过的小型和中型机器人团队问题上，Sadcher的表现优于其他基于学习和启发式的方法，并且计算时间适合实时操作。同时，还探讨了基于采样的方法，并评估了在不同机器人和任务数量下的可扩展性。

Conclusion: Sadcher能够有效地解决异构多机器人团队的实时任务分配问题，并优于现有基线方法，同时具有良好的可扩展性和泛化能力。

Abstract: We present Sadcher, a real-time task assignment framework for heterogeneous
multi-robot teams that incorporates dynamic coalition formation and task
precedence constraints. Sadcher is trained through Imitation Learning and
combines graph attention and transformers to predict assignment rewards between
robots and tasks. Based on the predicted rewards, a relaxed bipartite matching
step generates high-quality schedules with feasibility guarantees. We
explicitly model robot and task positions, task durations, and robots'
remaining processing times, enabling advanced temporal and spatial reasoning
and generalization to environments with different spatiotemporal distributions
compared to training. Trained on optimally solved small-scale instances, our
method can scale to larger task sets and team sizes. Sadcher outperforms other
learning-based and heuristic baselines on randomized, unseen problems for small
and medium-sized teams with computation times suitable for real-time operation.
We also explore sampling-based variants and evaluate scalability across robot
and task counts. In addition, we release our dataset of 250,000 optimal
schedules: https://autonomousrobots.nl/paper_websites/sadcher_MRTA/

</details>


### [345] [Learning Human-Humanoid Coordination for Collaborative Object Carrying](https://arxiv.org/abs/2510.14293)
*Yushi Du,Yixuan Li,Baoxiong Jia,Yutang Lin,Pei Zhou,Wei Liang,Yanchao Yang,Siyuan Huang*

Main category: cs.RO

TL;DR: 提出一种仅用本体感觉进行强化学习的方法COLA，实现了人与人形机器人的柔顺协作搬运任务，并验证了其有效性、泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 已有机器人领域的研究多集中于机械臂与人的协作，而人形机器人与人的协作因其复杂的全身动力学而鲜有探索。本研究旨在解决此问题。

Method: 提出一种仅用本体感觉进行强化学习的方法COLA，该方法在一个闭环环境中进行训练，通过动态物体交互来隐式预测物体运动模式和人类意图，并通过协调轨迹规划来实现柔顺协作以维持负载平衡。

Result: 仿真实验表明，与基线方法相比，该模型使人省力24.7%，同时保持了物体的稳定性。真实世界实验验证了该模型在不同物体类型（箱子、桌子、担架等）和运动模式（直线、转弯、爬坡）下的鲁棒性。用户研究（23名参与者）证实，与基线模型相比，平均提高了27.4%。

Conclusion: 本方法无需外部传感器或复杂的交互模型，即可实现柔顺的人与人形机器人协作搬运，为实际部署提供了一种实用解决方案。

Abstract: Human-humanoid collaboration shows significant promise for applications in
healthcare, domestic assistance, and manufacturing. While compliant robot-human
collaboration has been extensively developed for robotic arms, enabling
compliant human-humanoid collaboration remains largely unexplored due to
humanoids' complex whole-body dynamics. In this paper, we propose a
proprioception-only reinforcement learning approach, COLA, that combines leader
and follower behaviors within a single policy. The model is trained in a
closed-loop environment with dynamic object interactions to predict object
motion patterns and human intentions implicitly, enabling compliant
collaboration to maintain load balance through coordinated trajectory planning.
We evaluate our approach through comprehensive simulator and real-world
experiments on collaborative carrying tasks, demonstrating the effectiveness,
generalization, and robustness of our model across various terrains and
objects. Simulation experiments demonstrate that our model reduces human effort
by 24.7%. compared to baseline approaches while maintaining object stability.
Real-world experiments validate robust collaborative carrying across different
object types (boxes, desks, stretchers, etc.) and movement patterns
(straight-line, turning, slope climbing). Human user studies with 23
participants confirm an average improvement of 27.4% compared to baseline
models. Our method enables compliant human-humanoid collaborative carrying
without requiring external sensors or complex interaction models, offering a
practical solution for real-world deployment.

</details>


### [346] [Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning](https://arxiv.org/abs/2510.14300)
*Weijie Shen,Yitian Liu,Yuhao Wu,Zhixuan Liang,Sijia Gu,Dehui Wang,Tian Nian,Lei Xu,Yusen Qin,Jiangmiao Pang,Xinping Guan,Xiaokang Yang,Yao Mu*

Main category: cs.RO

TL;DR: AdaMoE 是一种新的 VLA 模型架构，通过继承预训练权重和采用稀疏激活的 MoE 层来扩展模型，实现了在机器人操作任务中的高效和高性能。


<details>
  <summary>Details</summary>
Motivation: 扩展 VLA 模型以应对机器人操作任务的挑战，同时解决计算资源、数据集稀缺和实时控制效率的问题。

Method: 提出 AdaMoE 架构，继承预训练的 VLA 模型权重，并将密集前馈层替换为稀疏激活的 MoE 层。采用解耦技术，通过独立的尺度适配器和传统的路由器来分离专家选择和专家加权，以实现协作专家利用。

Result: AdaMoE 在 LIBERO 和 RoboTwin 基准测试中分别取得了 1.8% 和 9.3% 的性能提升。在现实世界的机器人操作任务中，性能提升了 21.5%。

Conclusion: AdaMoE 通过协作专家利用，在保持计算效率的同时，显著提高了机器人操作任务的性能，验证了其在实际应用中的有效性。

Abstract: Vision-Language-Action (VLA) models are experiencing rapid development and
demonstrating promising capabilities in robotic manipulation tasks. However,
scaling up VLA models presents several critical challenges: (1) Training new
VLA models from scratch demands substantial computational resources and
extensive datasets. Given the current scarcity of robot data, it becomes
particularly valuable to fully leverage well-pretrained VLA model weights
during the scaling process. (2) Real-time control requires carefully balancing
model capacity with computational efficiency. To address these challenges, We
propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits
pretrained weights from dense VLA models, and scales up the action expert by
substituting the feedforward layers into sparsely activated MoE layers. AdaMoE
employs a decoupling technique that decouples expert selection from expert
weighting through an independent scale adapter working alongside the
traditional router. This enables experts to be selected based on task relevance
while contributing with independently controlled weights, allowing
collaborative expert utilization rather than winner-takes-all dynamics. Our
approach demonstrates that expertise need not monopolize. Instead, through
collaborative expert utilization, we can achieve superior performance while
maintaining computational efficiency. AdaMoE consistently outperforms the
baseline model across key benchmarks, delivering performance gains of 1.8% on
LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement
in real-world experiments validates its practical effectiveness for robotic
manipulation tasks.

</details>


### [347] [Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion](https://arxiv.org/abs/2510.14338)
*Yuanhong Zeng,Anushri Dixit*

Main category: cs.RO

TL;DR: 本研究提出了一种风险感知强化学习方法，用于训练四足机器人步态策略，并通过多臂老虎机框架自适应选择策略，以提高在未知环境中的稳定性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，尤其是在机器人步态控制领域，提高策略的稳定性和在未知环境中的适应性是一个重要的挑战。本研究旨在解决如何训练能够应对不同风险水平并能动态适应未知环境的四足机器人步态策略的问题。

Method: 本研究使用条件在险价值（CVaR）约束策略优化技术来训练一系列风险条件策略。然后，利用多臂老虎机框架，仅基于观察到的回合回报，自适应地选择表现最佳的策略，而无需任何特权环境信息，从而能够动态适应未知环境。

Result: 在模拟环境中，该方法在八个未见过的场景（包括改变动力学、接触、传感噪声和地形）中进行了评估。在实际的 Unitree Go2 机器人上，该方法在未见过的地形上进行了测试。结果表明，该风险感知策略在未见过环境中的平均性能和尾部性能比其他基线方法提高了近一倍。此外，基于老虎机的自适应方法能在两分钟内选择出在未知地形上表现最佳的风险感知策略。

Conclusion: 本研究成功地训练了具有不同鲁棒性水平的四足机器人步态策略，并通过一种新颖的多臂老虎机自适应方法，使其能够在操作过程中根据未知环境动态调整策略，从而在各种未见过的环境中实现了显著的性能提升和鲁棒性。

Abstract: In this work, we study risk-aware reinforcement learning for quadrupedal
locomotion. Our approach trains a family of risk-conditioned policies using a
Conditional Value-at-Risk (CVaR) constrained policy optimization technique that
provides improved stability and sample efficiency. At deployment, we adaptively
select the best performing policy from the family of policies using a
multi-armed bandit framework that uses only observed episodic returns, without
any privileged environment information, and adapts to unknown conditions on the
fly. Hence, we train quadrupedal locomotion policies at various levels of
robustness using CVaR and adaptively select the desired level of robustness
online to ensure performance in unknown environments. We evaluate our method in
simulation across eight unseen settings (by changing dynamics, contacts,
sensing noise, and terrain) and on a Unitree Go2 robot in previously unseen
terrains. Our risk-aware policy attains nearly twice the mean and tail
performance in unseen environments compared to other baselines and our
bandit-based adaptation selects the best-performing risk-aware policy in
unknown terrain within two minutes of operation.

</details>


### [348] [SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation](https://arxiv.org/abs/2510.14357)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: 本研究提出了SUM-AgriVLN方法，通过引入空间理解记忆模块，有效提升了农业场景下视觉语言导航的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的农业机器人导航方法（如AgriVLN）在处理重复出现的导航指令时，未能充分利用历史经验提供空间上下文，导致效率不高。

Method: SUM-AgriVLN方法的核心在于SUM模块，该模块利用3D重建和表示来增强空间理解能力，并存储空间记忆，从而为后续导航任务提供上下文信息。

Result: 在A2A基准测试中，SUM-AgriVLN将成功率从0.47提升至0.54，导航误差略微从2.91m增加到2.93m，展现了在农业领域的领先性能。

Conclusion: SUM-AgriVLN通过引入空间记忆机制，显著提高了农业机器人导航的成功率，证明了该方法在农业视觉语言导航任务中的有效性和优越性。

Abstract: Agricultural robots are emerging as powerful assistants across a wide range
of agricultural tasks, nevertheless, still heavily rely on manual operation or
fixed rail systems for movement. The AgriVLN method and the A2A benchmark
pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural
domain, enabling robots to navigate to the target positions following the
natural language instructions. In practical agricultural scenarios, navigation
instructions often repeatedly occur, yet AgriVLN treat each instruction as an
independent episode, overlooking the potential of past experiences to provide
spatial context for subsequent ones. To bridge this gap, we propose the method
of Spatial Understanding Memory for Agricultural Vision-and-Language Navigation
(SUM-AgriVLN), in which the SUM module employs spatial understanding and save
spatial memory through 3D reconstruction and representation. When evaluated on
the A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47
to 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m,
demonstrating the state-of-the-art performance in the agricultural domain.
Code: https://github.com/AlexTraveling/SUM-AgriVLN.

</details>


### [349] [RoboANKLE: Design, Development, and Functional Evaluation of a Robotic Ankle with a Motorized Compliant Unit](https://arxiv.org/abs/2510.14414)
*Baris Baysal,Omid Arfaie,Ramazan Unal*

Main category: cs.RO

TL;DR: 研究提出了一种名为RoboANKLE的有动力胫骨下假肢，可提供完整的蹬离辅助，旨在实现自然的日常活动步态。


<details>
  <summary>Details</summary>
Motivation: 设计有动力胫骨下假肢面临能量自主性和减轻重量的挑战，本研究旨在通过提供完整的蹬离辅助来模仿人踝，实现自然的踝关节运动。

Method: 采用能量存储和扩展释放（ESER）机制，并结合了新的额外能量存储（EES）机制。通过运动学和动力学分析确定设计参数和评估性能，然后进行CAD建模、动态和结构分析以优化设计并减轻重量。最后制造原型并进行实验评估。

Result: RoboANKLE 假肢质量为 1.92 公斤，尺寸为 261x107x420 毫米。功能评估显示，RoboANKLE 能够以 95% 的准确度达到自然的 बढ़ोतरी屈角度，并能产生比自然行走所需高 57% 的力矩，以及高 10% 的功率。

Conclusion: RoboANKLE 假肢成功实现了强大的蹬离辅助和自然步态，证明了其设计和功能的有效性。

Abstract: This study presents a powered transtibial prosthesis with complete push-off
assistance, RoboANKLE. The design aims to fulfill specific requirements, such
as a sufficient range of motion (RoM) while providing the necessary torque for
achieving natural ankle motion in daily activities. Addressing the challenges
faced in designing active transtibial prostheses, such as maintaining energetic
autonomy and minimizing weight, is vital for the study. With this aim, we try
to imitate the human ankle by providing extensive push-off assistance to
achieve a natural-like torque profile. Thus, Energy Store and Extended Release
mechanism (ESER) is employed with a novel Extra Energy Storage (EES) mechanism.
Kinematic and kinetic analyses are carried out to determine the design
parameters and assess the design performance. Subsequently, a Computer-Aided
Design (CAD) model is built and used in comprehensive dynamic and structural
analyses. These analyses are used for the design performance evaluation and
determine the forces and torques applied to the prosthesis, which aids in
optimizing the design for minimal weight via structural analysis and topology
optimization. The design of the prototype is then finalized and manufactured
for experimental evaluation to validate the design and functionality. The
prototype is realized with a mass of 1.92 kg and dimensions of 261x107x420 mm.
The Functional evaluations of the RoboANKLE revealed that it is capable of
achieving the natural maximum dorsi-flexion angle with 95% accuracy. Also,
Thanks to the implemented mechanisms, the results show that RoboANKLE can
generate 57% higher than the required torque for natural walking. The result of
the power generation capacity of the RoboANKLE is 10% more than the natural
power during the gait cycle.

</details>


### [350] [Towards Adaptable Humanoid Control via Adaptive Motion Tracking](https://arxiv.org/abs/2510.14454)
*Tao Huang,Huayi Wang,Junli Ren,Kangning Yin,Zirui Wang,Xiao Chen,Feiyu Jia,Wentao Zhang,Junfeng Long,Jingbo Wang,Jiangmiao Pang*

Main category: cs.RO

TL;DR: AdaMimic是一个新颖的运动跟踪算法，仅需一个参考运动即可实现适应性强的类人控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在适应性和模仿准确性之间存在权衡，AdaMimic旨在结合两者的优点。

Method: 通过增强数据集（通过稀疏化单参考运动的关键帧并进行轻微编辑）和训练适配器来调整跟踪速度和优化低级动作，实现灵活的时间扭曲。

Result: 该方法在模拟和现实世界的Unitree G1机器人上，在多种任务和广泛的适应条件下，显著提高了运动适应性和模仿准确性。

Conclusion: AdaMimic能够从单个参考运动实现适应性强的类人控制，提高了运动的模仿准确性和适应性。

Abstract: Humanoid robots are envisioned to adapt demonstrated motions to diverse
real-world conditions while accurately preserving motion patterns. Existing
motion prior approaches enable well adaptability with a few motions but often
sacrifice imitation accuracy, whereas motion-tracking methods achieve accurate
imitation yet require many training motions and a test-time target motion to
adapt. To combine their strengths, we introduce AdaMimic, a novel motion
tracking algorithm that enables adaptable humanoid control from a single
reference motion. To reduce data dependence while ensuring adaptability, our
method first creates an augmented dataset by sparsifying the single reference
motion into keyframes and applying light editing with minimal physical
assumptions. A policy is then initialized by tracking these sparse keyframes to
generate dense intermediate motions, and adapters are subsequently trained to
adjust tracking speed and refine low-level actions based on the adjustment,
enabling flexible time warping that further improves imitation accuracy and
adaptability. We validate these significant improvements in our approach in
both simulation and the real-world Unitree G1 humanoid robot in multiple tasks
across a wide range of adaptation conditions. Videos and code are available at
https://taohuang13.github.io/adamimic.github.io/.

</details>


### [351] [Restoring Noisy Demonstration for Imitation Learning With Diffusion Models](https://arxiv.org/abs/2510.14467)
*Shang-Fu Chen,Co Yong,Shao-Hua Sun*

Main category: cs.RO

TL;DR: 该研究提出了一种用于处理带有噪声的专家演示的模仿学习框架。


<details>
  <summary>Details</summary>
Motivation: 现有的模仿学习方法通常假设专家演示是完美的，但实际演示可能包含由于人类错误或系统不准确性而产生的噪声。

Method: 提出了一种过滤和恢复框架，首先从演示中筛选干净的样本，然后学习条件扩散模型来恢复噪声样本。

Result: 在机器人手臂操作、灵巧操作和运动学等领域进行了评估，结果表明该框架优于现有方法。

Conclusion: 该框架在处理带有噪声的离线演示数据方面具有实际应用价值，并且对不同类型的噪声和噪声水平具有鲁棒性。

Abstract: Imitation learning (IL) aims to learn a policy from expert demonstrations and
has been applied to various applications. By learning from the expert policy,
IL methods do not require environmental interactions or reward signals.
However, most existing imitation learning algorithms assume perfect expert
demonstrations, but expert demonstrations often contain imperfections caused by
errors from human experts or sensor/control system inaccuracies. To address the
above problems, this work proposes a filter-and-restore framework to best
leverage expert demonstrations with inherent noise. Our proposed method first
filters clean samples from the demonstrations and then learns conditional
diffusion models to recover the noisy ones. We evaluate our proposed framework
and existing methods in various domains, including robot arm manipulation,
dexterous manipulation, and locomotion. The experiment results show that our
proposed framework consistently outperforms existing methods across all the
tasks. Ablation studies further validate the effectiveness of each component
and demonstrate the framework's robustness to different noise types and levels.
These results confirm the practical applicability of our framework to noisy
offline demonstration data.

</details>


### [352] [Stability Criteria and Motor Performance in Delayed Haptic Dyadic Interactions Mediated by Robots](https://arxiv.org/abs/2510.14511)
*Mingtian Du,Suhas Raghavendra Kulkarni,Simone Kager,Domenico Campolo*

Main category: cs.RO

TL;DR: 该论文为机器人辅助的人人（二元）交互系统建立了分析稳定性标准，重点研究了网络延迟对触觉通信的影响。


<details>
  <summary>Details</summary>
Motivation: 研究机器人辅助的、存在网络延迟的人人交互系统的稳定性，特别是触觉通信方面，以识别稳定性的标准并为设计提供指导。

Method: 通过频域分析和数值模拟，识别独立于延迟和依赖于延迟的稳定性标准，并分析控制器和机器人动力学参数（如刚度）对最大容忍延迟的影响。

Result: 提出了分析稳定性标准，包括独立于延迟和依赖于延迟的标准。发现增加刚度会以非线性的方式减少最大容忍延迟。实验证明了稳定性和运动性能之间的相关性。

Conclusion: 提出的稳定性标准可推广应用于广泛的机器人辅助交互，并可作为远程二元系统设计的指导方针，为有效的延迟补偿策略提供了先决条件。

Abstract: This paper establishes analytical stability criteria for robot-mediated
human-human (dyadic) interaction systems, focusing on haptic communication
under network-induced time delays. Through frequency-domain analysis supported
by numerical simulations, we identify both delay-independent and
delay-dependent stability criteria. The delay-independent criterion guarantees
stability irrespective of the delay, whereas the delay-dependent criterion is
characterised by a maximum tolerable delay before instability occurs. The
criteria demonstrate dependence on controller and robot dynamic parameters,
where increasing stiffness reduces the maximum tolerable delay in a non-linear
manner, thereby heightening system vulnerability. The proposed criteria can be
generalised to a wide range of robot-mediated interactions and serve as design
guidelines for stable remote dyadic systems. Experiments with robots performing
human-like movements further illustrate the correlation between stability and
motor performance. The findings of this paper suggest the prerequisites for
effective delay-compensation strategies.

</details>


### [353] [QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps](https://arxiv.org/abs/2510.14546)
*Matti Pekkanen,Francesco Verdoja,Ville Kyrki*

Main category: cs.RO

TL;DR: 本文提出了一种利用视觉-语言模型嵌入来改进机器人地图中开放词汇场景理解的方法，通过查询词的同义词和反义词来识别相关环境部分，并训练分类器进行划分，实验证明了该方法在提高地图和图像可查询性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 在机器人地图中，利用视觉-语言模型嵌入来实现开放词汇场景理解，但关键挑战在于如何根据用户查询准确识别相关的环境部分。

Method: 利用查询词的同义词和反义词，在嵌入空间中估计与查询相关的语言空间，并训练一个分类器来将环境划分为匹配和非匹配区域。

Result: 通过在地图和标准图像基准上的广泛实验，证明了该方法能够提高地图和图像的可查询性。

Conclusion: 该查询技术不依赖于特定的表示或编码器，且只需少量训练，即可有效解决机器人地图中根据查询识别相关环境部分的问题。

Abstract: Embeddings from Visual-Language Models are increasingly utilized to represent
semantics in robotic maps, offering an open-vocabulary scene understanding that
surpasses traditional, limited labels. Embeddings enable on-demand querying by
comparing embedded user text prompts to map embeddings via a similarity metric.
The key challenge in performing the task indicated in a query is that the robot
must determine the parts of the environment relevant to the query.
  This paper proposes a solution to this challenge. We leverage
natural-language synonyms and antonyms associated with the query within the
embedding space, applying heuristics to estimate the language space relevant to
the query, and use that to train a classifier to partition the environment into
matches and non-matches. We evaluate our method through extensive experiments,
querying both maps and standard image benchmarks. The results demonstrate
increased queryability of maps and images. Our querying technique is agnostic
to the representation and encoder used, and requires limited training.

</details>


### [354] [A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning](https://arxiv.org/abs/2510.14584)
*Benno Wingender,Nils Dengler,Rohit Menon,Sicong Pan,Maren Bennewitz*

Main category: cs.RO

TL;DR: 该研究提出了一种新的通用可放置性指标，可以直接从带有噪声的点云中评估放置姿势，无需任何形状先验，实现了在真实世界抓取和放置任务中的泛化和统一推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法在抓取和放置未知物体时存在局限性，需要依赖强对象先验或平面支撑假设，限制了泛化能力和抓取-放置之间的统一推理。

Method: 提出了一种通用的可放置性指标，该指标联合评估稳定、抓取和间隙，并直接从原始几何形状中提取支撑表面，生成多方向放置的候选点，并对满足碰撞和稳定性的接触点进行采样。通过将抓取分数与每个候选放置位置相结合，实现了无模型统一的抓取-放置推理。

Result: 在未见过的真实物体和非平面支撑对象上，该指标在预测稳定性损失方面达到了与CAD相当的精度，并且比基于学习的预测器产生了更符合物理规律的放置。

Conclusion: 该方法能够对未知物体进行可靠的抓取和放置，克服了现有方法的局限性，并在真实世界场景中表现出优越的性能。

Abstract: To reliably pick and place unknown objects under real-world sensing noise
remains a challenging task, as existing methods rely on strong object priors
(e.g., CAD models), or planar-support assumptions, limiting generalization and
unified reasoning between grasping and placing. In this work, we introduce a
generalized placeability metric that evaluates placement poses directly from
noisy point clouds, without any shape priors. The metric jointly scores
stability, graspability, and clearance. From raw geometry, we extract the
support surfaces of the object to generate diverse candidates for
multi-orientation placement and sample contacts that satisfy collision and
stability constraints. By conditioning grasp scores on each candidate
placement, our proposed method enables model-free unified pick-and-place
reasoning and selects grasp-place pairs that lead to stable, collision-free
placements. On unseen real objects and non-planar object supports, our metric
delivers CAD-comparable accuracy in predicting stability loss and generally
produces more physically plausible placements than learning-based predictors.

</details>


### [355] [Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning](https://arxiv.org/abs/2510.14612)
*Gabriel Fischer Abati,João Carlos Virgolino Soares,Giulio Turrisi,Victor Barasuol,Claudio Semini*

Main category: cs.RO

TL;DR: 将机器人本体感觉时间序列数据表示为二维图像，用于卷积神经网络进行运动学习。


<details>
  <summary>Details</summary>
Motivation: 开发一种新的方法来表示四足机器人的本体感觉时间序列数据，使其能够利用卷积神经网络学习与运动相关的任务。

Method: 将来自联合位置、IMU读数和脚速度的多个本体感觉信号编码为二维图像，保留机器人的形态结构。

Result: 在真实世界数据集和模拟环境中，图像表示在接触估计任务中提高了预测准确性和泛化能力，比传统的基于序列的模型性能更优。

Conclusion: 所提出的图像表示方法通过利用跨模态编码策略，在机器人状态学习方面具有巨大潜力，并且在接触状态估计方面取得了显著的性能提升。

Abstract: This paper presents a novel approach for representing proprioceptive
time-series data from quadruped robots as structured two-dimensional images,
enabling the use of convolutional neural networks for learning
locomotion-related tasks. The proposed method encodes temporal dynamics from
multiple proprioceptive signals, such as joint positions, IMU readings, and
foot velocities, while preserving the robot's morphological structure in the
spatial arrangement of the image. This transformation captures inter-signal
correlations and gait-dependent patterns, providing a richer feature space than
direct time-series processing. We apply this concept in the problem of contact
estimation, a key capability for stable and adaptive locomotion on diverse
terrains. Experimental evaluations on both real-world datasets and simulated
environments show that our image-based representation consistently enhances
prediction accuracy and generalization over conventional sequence-based models,
underscoring the potential of cross-modal encoding strategies for robotic state
learning. Our method achieves superior performance on the contact dataset,
improving contact state accuracy from 87.7% to 94.5% over the recently proposed
MI-HGNN method, using a 15 times shorter window size.

</details>


### [356] [Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models](https://arxiv.org/abs/2510.14615)
*Edward Sandra,Lander Vanroye,Dries Dirckx,Ruben Cartuyvels,Jan Swevers,Wilm Decré*

Main category: cs.RO

TL;DR: CAMPD是一种新的运动规划方法，利用条件扩散模型在不同环境中生成高质量的运动轨迹，并且泛化能力强，效率高。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人运动规划方法在处理高维状态空间和复杂环境时存在扩展性问题。虽然扩散模型在处理复杂数据分布方面有潜力，但现有方法通常仅限于单一环境训练，泛化能力受限，或者依赖特定传感器输入，限制了其应用。

Method: 提出了一种名为CAMPD（Context-Aware Motion Planning Diffusion）的方法，该方法利用分类器无关的去噪扩散概率模型，并结合传感器无关的上下文信息进行条件化。通过在U-Net架构中集成注意力机制，模型能够适应任意数量的上下文参数，从而实现对不同环境的泛化。

Result: 在7自由度机器人操作器上进行了评估，并将CAMPD与现有最先进方法进行了基准测试。结果表明，CAMPD能够泛化到未见过的环境，生成高质量、多模态的轨迹，并且所需时间是现有方法的几分之一。

Conclusion: CAMPD通过条件扩散模型和传感器无关的上下文信息，有效地解决了机器人运动规划中的泛化性和效率问题，能够适应各种场景而无需重新训练。

Abstract: Classical methods in robot motion planning, such as sampling-based and
optimization-based methods, often struggle with scalability towards
higher-dimensional state spaces and complex environments. Diffusion models,
known for their capability to learn complex, high-dimensional and multi-modal
data distributions, provide a promising alternative when applied to motion
planning problems and have already shown interesting results. However, most of
the current approaches train their model for a single environment, limiting
their generalization to environments not seen during training. The techniques
that do train a model for multiple environments rely on a specific camera to
provide the model with the necessary environmental information and therefore
always require that sensor. To effectively adapt to diverse scenarios without
the need for retraining, this research proposes Context-Aware Motion Planning
Diffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic
diffusion model, conditioned on sensor-agnostic contextual information. An
attention mechanism, integrated in the well-known U-Net architecture,
conditions the model on an arbitrary number of contextual parameters. CAMPD is
evaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art
approaches on real-world tasks, showing its ability to generalize to unseen
environments and generate high-quality, multi-modal trajectories, at a fraction
of the time required by existing methods.

</details>


### [357] [Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion](https://arxiv.org/abs/2510.14947)
*Blake Werner,Lizhi Yang,Aaron D. Ames*

Main category: cs.RO

TL;DR: 分层控制架构（LCA）通过分离低级稳定和高级感知决策，实现了比整体设计更鲁棒的机器人运动能力，在模拟和硬件上都优于单阶段方法，尤其在处理楼梯和台阶等复杂地形时表现出色。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中实现稳健的人形机器人运动，需要能够平衡快速的低级稳定和较慢的感知决策的控制架构。

Method: 提出并实现了一个简单的分层控制架构（LCA），该架构包含一个高速运行的本体感觉稳定器和一个低速运行的紧凑型感知策略。采用两阶段训练方法：首先进行盲稳定器预训练，然后进行感知策略微调。

Result: 与单阶段方法相比，分层策略在模拟和硬件上都表现出更优越的性能，即使使用最少的感知编码器也能实现更稳健的表现。在Unitree G1人形机器人上，该方法成功应对了单阶段感知策略失败的楼梯和台阶任务。

Conclusion: 分层控制架构通过分离不同时间尺度的控制，是实现稳健的、受感知条件制约的运动的关键，而非网络规模或复杂性。

Abstract: Robust humanoid locomotion in unstructured environments requires
architectures that balance fast low-level stabilization with slower perceptual
decision-making. We show that a simple layered control architecture (LCA), a
proprioceptive stabilizer running at high rate, coupled with a compact low-rate
perceptual policy, enables substantially more robust performance than
monolithic end-to-end designs, even when using minimal perception encoders.
Through a two-stage training curriculum (blind stabilizer pretraining followed
by perceptual fine-tuning), we demonstrate that layered policies consistently
outperform one-stage alternatives in both simulation and hardware. On a Unitree
G1 humanoid, our approach succeeds across stair and ledge tasks where one-stage
perceptual policies fail. These results highlight that architectural separation
of timescales, rather than network scale or complexity, is the key enabler for
robust perception-conditioned locomotion.

</details>


### [358] [GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement](https://arxiv.org/abs/2510.14627)
*Yao Zhong,Hanzhi Chen,Simon Schaefer,Anran Zhang,Stefan Leutenegger*

Main category: cs.RO

TL;DR: GOPLA是一个机器人框架，通过学习人类演示来解决物品放置问题，综合考虑语义和几何因素。


<details>
  <summary>Details</summary>
Motivation: 机器人需要具备物品放置能力，以在日常家务中辅助人类，而这需要同时处理语义偏好和几何可行性。

Method: 提出了一种名为GOPLA的分层框架。该框架使用多模态大语言模型将人类指令和视觉输入转化为结构化计划（指定对象间的关系），然后由空间映射器生成带有几何常识的3D可供性地图，最后由基于扩散的模型进行规划，考虑多计划分布和碰撞避免。

Result: 在物品放置准确性和物理合理性方面，GOPLA的成功率比第二名高出30.04个百分点，并且在广泛的真实机器人放置场景中表现出强大的泛化能力。

Conclusion: GOPLA框架能够有效地学习和执行物品放置任务，显著优于现有方法，并能泛化到各种实际应用中。

Abstract: Robots are expected to serve as intelligent assistants, helping humans with
everyday household organization. A central challenge in this setting is the
task of object placement, which requires reasoning about both semantic
preferences (e.g., common-sense object relations) and geometric feasibility
(e.g., collision avoidance). We present GOPLA, a hierarchical framework that
learns generalizable object placement from augmented human demonstrations. A
multi-modal large language model translates human instructions and visual
inputs into structured plans that specify pairwise object relationships. These
plans are then converted into 3D affordance maps with geometric common sense by
a spatial mapper, while a diffusion-based planner generates placement poses
guided by test-time costs, considering multi-plan distributions and collision
avoidance. To overcome data scarcity, we introduce a scalable pipeline that
expands human placement demonstrations into diverse synthetic training data.
Extensive experiments show that our approach improves placement success rates
by 30.04 percentage points over the runner-up, evaluated on positioning
accuracy and physical plausibility, demonstrating strong generalization across
a wide range of real-world robotic placement scenarios.

</details>


### [359] [CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions](https://arxiv.org/abs/2510.14959)
*Lizhi Yang,Blake Werner,Massimiliano de Sa Aaron D. Ames*

Main category: cs.RO

TL;DR: CBF-RL是一个在训练中强制执行控制障碍函数（CBF）的框架，用于生成安全的强化学习（RL）行为。它通过在训练中加入CBF项来最小化修改RL策略，并对策略进行安全过滤，从而使RL策略能够内化安全约束，实现安全运行，无需在线安全过滤器。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习（RL）方法虽然强大，但往往以牺牲安全性为代价，而安全违规在现实世界中可能导致灾难性后果。传统的控制障碍函数（CBF）作为一种安全约束方法，通常在线部署，但这可能导致RL策略因缺乏对CBF的了解而行为保守。

Method: CBF-RL框架在训练过程中强制执行CBF。具体而言，它通过在训练中加入一个CBF项来最小化地修改名义RL策略，并对训练过程中的策略进行安全过滤。理论上，证明了连续时间的CBF可以离散时间回放中实现。

Result: CBF-RL能够将安全约束内化到学习策略中，从而实现更安全的操作和更倾向于安全奖励的行为。在导航任务和Unitree G1人形机器人上的实验表明，CBF-RL能够实现更安全的探索、更快的收敛速度以及在不确定性下的鲁棒性能，并且在实际应用中无需运行时安全过滤器即可实现安全运行。

Conclusion: CBF-RL通过在训练中整合CBF，成功地使强化学习策略内化安全约束，从而在无需在线安全过滤器的情况下实现安全、高效且鲁棒的机器人行为。

Abstract: Reinforcement learning (RL), while powerful and expressive, can often
prioritize performance at the expense of safety. Yet safety violations can lead
to catastrophic outcomes in real-world deployments. Control Barrier Functions
(CBFs) offer a principled method to enforce dynamic safety -- traditionally
deployed \emph{online} via safety filters. While the result is safe behavior,
the fact that the RL policy does not have knowledge of the CBF can lead to
conservative behaviors. This paper proposes CBF-RL, a framework for generating
safe behaviors with RL by enforcing CBFs \emph{in training}. CBF-RL has two key
attributes: (1) minimally modifying a nominal RL policy to encode safety
constraints via a CBF term, (2) and safety filtering of the policy rollouts in
training. Theoretically, we prove that continuous-time safety filters can be
deployed via closed-form expressions on discrete-time roll-outs. Practically,
we demonstrate that CBF-RL internalizes the safety constraints in the learned
policy -- both enforcing safer actions and biasing towards safer rewards --
enabling safe deployment without the need for an online safety filter. We
validate our framework through ablation studies on navigation tasks and on the
Unitree G1 humanoid robot, where CBF-RL enables safer exploration, faster
convergence, and robust performance under uncertainty, enabling the humanoid
robot to avoid obstacles and climb stairs safely in real-world settings without
a runtime safety filter.

</details>


### [360] [Generative Models From and For Sampling-Based MPC: A Bootstrapped Approach For Adaptive Contact-Rich Manipulation](https://arxiv.org/abs/2510.14643)
*Lara Brudermüller,Brandon Hung,Xinghao Zhu,Jiuguang Wang,Nick Hawes,Preston Culbertson,Simon Le Cleac'h*

Main category: cs.RO

TL;DR: We introduce a generative predictive control (GPC) framework that uses conditional flow-matching models trained on SPC data to amortize sampling-based MPC, enabling more efficient and informed sampling for planning. This approach is demonstrated on a quadruped robot for contact-rich loco-manipulation, showing improvements in sample efficiency, reduced planning horizon needs, and robust generalization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the efficiency and effectiveness of sampling-based Model Predictive Control (SPC) by leveraging learned generative models. Traditional SPC methods can be computationally expensive due to extensive sampling. The goal is to create a framework that can learn from existing SPC data to guide the sampling process, making it more efficient and informed, especially for complex tasks like contact-rich locomotion and manipulation.

Method: The proposed method, Generative Predictive Control (GPC), utilizes conditional flow-matching models. These models are trained on control sequences generated by SPC in simulation. The trained models learn to generate proposal distributions directly from noisy SPC data. This learned distribution is then used to guide the sampling process during online planning, making it more efficient than traditional methods that rely on iterative refinement or gradient-based solvers.

Result: Experiments were conducted in both simulation and on a real-world quadruped robot performing contact-rich loco-manipulation tasks. The results demonstrated that the GPC framework significantly improves sample efficiency, allowing for faster planning. It also reduces the required planning horizon and shows robust generalization capabilities across different task variations. The method was shown to be effective even when trained on noisy SPC data.

Conclusion: The GPC framework presents a novel and effective approach to amortize sampling-based Model Predictive Control by learning proposal distributions from SPC data using conditional flow-matching models. This method enhances sample efficiency, reduces the need for long planning horizons, and generalizes well, demonstrating its practical applicability in challenging real-world scenarios such as quadruped robot loco-manipulation.

Abstract: We present a generative predictive control (GPC) framework that amortizes
sampling-based Model Predictive Control (SPC) by bootstrapping it with
conditional flow-matching models trained on SPC control sequences collected in
simulation. Unlike prior work relying on iterative refinement or gradient-based
solvers, we show that meaningful proposal distributions can be learned directly
from noisy SPC data, enabling more efficient and informed sampling during
online planning. We further demonstrate, for the first time, the application of
this approach to real-world contact-rich loco-manipulation with a quadruped
robot. Extensive experiments in simulation and on hardware show that our method
improves sample efficiency, reduces planning horizon requirements, and
generalizes robustly across task variations.

</details>


### [361] [RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks](https://arxiv.org/abs/2510.14968)
*Mingxuan Yan,Yuping Wang,Zechun Liu,Jiachen Li*

Main category: cs.RO

TL;DR: 该研究提出了一种名为检索式演示分解器（RDD）的新方法，用于解决长时序任务中的分层视觉-语言-动作（VLA）框架。RDD 能够自动将演示任务分解为子任务，解决了现有方法中由于子任务划分不当导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有的分层 VLA 框架通常依赖于微调 VLM 规划器来分解任务，但这种微调需要人工标注或启发式规则分割的演示数据。启发式方法可能导致子任务与底层视觉运动策略的训练数据不匹配，从而影响任务性能。

Method: RDD 通过比对分解出的子任务时间段的视觉特征与底层视觉运动策略训练数据的视觉特征，来实现演示的自动子任务分解。

Result: RDD 在模拟和真实世界任务中均优于当前最先进的子任务分解器，展现了在不同场景下的鲁棒性。

Conclusion: RDD 能够有效解决长时序任务中的子任务分解问题，提高了 VLA 框架的性能和鲁棒性。

Abstract: To tackle long-horizon tasks, recent hierarchical vision-language-action
(VLAs) frameworks employ vision-language model (VLM)-based planners to
decompose complex manipulation tasks into simpler sub-tasks that low-level
visuomotor policies can easily handle. Typically, the VLM planner is finetuned
to learn to decompose a target task. This finetuning requires target task
demonstrations segmented into sub-tasks by either human annotation or heuristic
rules. However, the heuristic subtasks can deviate significantly from the
training data of the visuomotor policy, which degrades task performance. To
address these issues, we propose a Retrieval-based Demonstration Decomposer
(RDD) that automatically decomposes demonstrations into sub-tasks by aligning
the visual features of the decomposed sub-task intervals with those from the
training data of the low-level visuomotor policies. Our method outperforms the
state-of-the-art sub-task decomposer on both simulation and real-world tasks,
demonstrating robustness across diverse settings. Code and more results are
available at rdd-neurips.github.io.

</details>


### [362] [Spatially anchored Tactile Awareness for Robust Dexterous Manipulation](https://arxiv.org/abs/2510.14647)
*Jialei Huang,Yang Ye,Yuanqing Gong,Xuezhou Zhu,Yang Gao,Kaifeng Zhang*

Main category: cs.RO

TL;DR: SaTA是一个新的框架，通过将触觉特征锚定在手的运动框架中，提高了灵巧操作的精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在需要亚毫米级精度的任务中存在局限性，因为它们未能有效利用触觉信号的丰富性和空间关系。

Method: SaTA将触觉特征通过正向运动学锚定到手的运动框架中，实现了无需对象模型或显式姿态估计的精确几何推理。

Result: SaTA在USB-C接口对接、灯泡安装和卡片滑动等具有挑战性的任务中，成功率最高提高了30%，完成时间缩短了27%，表现优于现有的视觉-触觉基线方法。

Conclusion: SaTA通过空间锚定的触觉表示，能够精确推断物体几何形状，并在高精度灵巧操作任务中取得显著成果。

Abstract: Dexterous manipulation requires precise geometric reasoning, yet existing
visuo-tactile learning methods struggle with sub-millimeter precision tasks
that are routine for traditional model-based approaches. We identify a key
limitation: while tactile sensors provide rich contact information, current
learning frameworks fail to effectively leverage both the perceptual richness
of tactile signals and their spatial relationship with hand kinematics. We
believe an ideal tactile representation should explicitly ground contact
measurements in a stable reference frame while preserving detailed sensory
information, enabling policies to not only detect contact occurrence but also
precisely infer object geometry in the hand's coordinate system. We introduce
SaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an
end-to-end policy framework that explicitly anchors tactile features to the
hand's kinematic frame through forward kinematics, enabling accurate geometric
reasoning without requiring object models or explicit pose estimation. Our key
insight is that spatially grounded tactile representations allow policies to
not only detect contact occurrence but also precisely infer object geometry in
the hand's coordinate system. We validate SaTA on challenging dexterous
manipulation tasks, including bimanual USB-C mating in free space, a task
demanding sub-millimeter alignment precision, as well as light bulb
installation requiring precise thread engagement and rotational control, and
card sliding that demands delicate force modulation and angular precision.
These tasks represent significant challenges for learning-based methods due to
their stringent precision requirements. Across multiple benchmarks, SaTA
significantly outperforms strong visuo-tactile baselines, improving success
rates by up to 30 percentage while reducing task completion times by 27
percentage.

</details>


### [363] [Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery](https://arxiv.org/abs/2510.14768)
*Fan Yang,Zixuan Huang,Abhinav Kumar,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: 该研究提出了一种名为 CADRE 的强化学习框架，用于在抓取范围内接住掉落的物体，并能重置系统以恢复主要操作任务。该框架使用受 NDF 启发的模块来提取隐式接触特征，相比仅依赖物体姿态或点云输入的方法，能更好地处理不同物体几何形状，并提高训练效率、收敛性和恢复成功率，还能泛化到未见过的物体。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的灵巧操作常会遇到意外错误和干扰，导致操作失败（如物体掉落）。本研究旨在解决在抓取范围内接住掉落物体的问题，并能重置系统以恢复主要操作任务。

Method: 提出了一种名为 CADRE 的强化学习框架，该框架包含一个受 Neural Descriptor Field (NDF) 启发的模块，用于提取隐式接触特征。该方法直接推理手指与物体的对应关系，并能适应不同的物体几何形状。

Result: 实验表明，引入接触特征可以提高训练效率，增强强化学习训练的收敛性能，并最终实现更成功的恢复。此外，CADRE 能够对不同几何形状的未见物体实现零样本泛化。

Conclusion: CADRE 框架通过整合隐式接触特征，有效解决了在操作中物体掉落的问题，提高了恢复的成功率和泛化能力。

Abstract: Real-world dexterous manipulation often encounters unexpected errors and
disturbances, which can lead to catastrophic failures, such as dropping the
manipulated object. To address this challenge, we focus on the problem of
catching a falling object while it remains within grasping range and,
importantly, resetting the system to a configuration favorable for resuming the
primary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a
reinforcement learning framework that incorporates a Neural Descriptor Field
(NDF)-inspired module to extract implicit contact features. Compared to methods
that rely solely on object pose or point cloud input, NDFs can directly reason
about finger-object correspondence and adapt to different object geometries.
Our experiments show that incorporating contact features improves training
efficiency, enhances convergence performance for RL training, and ultimately
leads to more successful recoveries. Additionally, we demonstrate that CADRE
can generalize zero-shot to unseen objects with different geometries.

</details>


### [364] [Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation](https://arxiv.org/abs/2510.14771)
*Xu Chi,Chao Zhang,Yang Su,Lingfeng Dou,Fujia Yang,Jiakuo Zhao,Haoyu Zhou,Xiaoyou Jia,Yong Zhou,Shan An*

Main category: cs.RO

TL;DR: Open TeleDex是一个统一的远程操作框架，用于机器人模仿学习的数据收集，解决了不同机器人平台的数据收集瓶颈，支持任何机械臂、任何灵巧手和任何输入设备，并通过新颖的手部姿态重定向算法提高了互操作性。


<details>
  <summary>Details</summary>
Motivation: 机器人模仿学习（IL）系统部署的关键瓶颈在于准确、高保真地获取演示数据，尤其是在处理异构机器人平台时。现有的远程操作系统在跨不同类型的远程操作设备保证高精度数据收集方面常常失败。

Method: 开发了Open TeleDex，一个统一的远程操作框架，专门用于演示数据收集。该框架支持任何机械臂、任何灵巧手和任何外部输入设备。提出了新颖的手部姿态重定向算法，以提高互操作性。

Result: Open TeleDex能够与更广泛的异构主从设备兼容，实现稳健、准确的数据收集。这为加速复杂机器人操作和IL领域的学术研究和行业发展奠定了基础，提供了一个高质量、公开可用的平台。

Conclusion: Open TeleDex通过提供一个统一的、高度可互操作的远程操作框架，有效解决了机器人模仿学习中异构平台的数据收集挑战，并提出了创新的手部姿态重定向技术，为相关领域的研究和发展提供了有力支持。

Abstract: Accurate and high-fidelity demonstration data acquisition is a critical
bottleneck for deploying robot Imitation Learning (IL) systems, particularly
when dealing with heterogeneous robotic platforms. Existing teleoperation
systems often fail to guarantee high-precision data collection across diverse
types of teleoperation devices. To address this, we developed Open TeleDex, a
unified teleoperation framework engineered for demonstration data collection.
Open TeleDex specifically tackles the TripleAny challenge, seamlessly
supporting any robotic arm, any dexterous hand, and any external input device.
Furthermore, we propose a novel hand pose retargeting algorithm that
significantly boosts the interoperability of Open TeleDex, enabling robust and
accurate compatibility with an even wider spectrum of heterogeneous master and
slave equipment. Open TeleDex establishes a foundational, high-quality, and
publicly available platform for accelerating both academic research and
industry development in complex robotic manipulation and IL.

</details>


### [365] [SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with Model-Based Reinforcement Learning](https://arxiv.org/abs/2510.14783)
*Aderik Verraest,Stavrow Bahnam,Robin Ferede,Guido de Croon,Christophe De Wagter*

Main category: cs.RO

TL;DR: SkyDreamer是一个首个端到端、纯视觉的自主无人机竞速（ADR）策略，实现了完整的仿真到现实迁移、机载执行和冠军级性能，将像素输入直接映射到电机指令。


<details>
  <summary>Details</summary>
Motivation: 尽管自主无人机竞速（ADR）系统已达到专业水平，但仍局限于无人机竞速领域。现有的端到端视觉方法虽然具有更广泛的应用前景，但尚未实现仿真到现实的完整迁移、机载执行和专业级性能的同步。

Method: SkyDreamer基于知情Dreamer（informed Dreamer）模型，这是一种模型驱动的强化学习方法。在该方法中，世界模型解码仅在训练期间可用的特权信息。通过将此概念扩展到端到端的视觉ADR，世界模型充当了隐式状态和参数估计器，从而提高了可解释性。

Result: SkyDreamer在真实世界实验中实现了稳定、高速的飞行，能够执行高达21米/秒速度和6g加速度的急转弯，如倒置筋斗、Split-S和梯形机动。它还通过在低质量分割掩码上运行，展示了非平凡的视觉仿真到现实迁移能力。此外，它还能应对电池电量下降问题，准确估计最大电机转速并实时调整飞行路径。

Conclusion: SkyDreamer展示了其适应现实世界差距（reality gap）关键问题的能力，实现了高鲁棒性，同时保持了极高速度和敏捷的飞行性能。

Abstract: Autonomous drone racing (ADR) systems have recently achieved champion-level
performance, yet remain highly specific to drone racing. While end-to-end
vision-based methods promise broader applicability, no system to date
simultaneously achieves full sim-to-real transfer, onboard execution, and
champion-level performance. In this work, we present SkyDreamer, to the best of
our knowledge, the first end-to-end vision-based ADR policy that maps directly
from pixel-level representations to motor commands. SkyDreamer builds on
informed Dreamer, a model-based reinforcement learning approach where the world
model decodes to privileged information only available during training. By
extending this concept to end-to-end vision-based ADR, the world model
effectively functions as an implicit state and parameter estimator, greatly
improving interpretability. SkyDreamer runs fully onboard without external aid,
resolves visual ambiguities by tracking progress using the state decoded from
the world model's hidden state, and requires no extrinsic camera calibration,
enabling rapid deployment across different drones without retraining.
Real-world experiments show that SkyDreamer achieves robust, high-speed flight,
executing tight maneuvers such as an inverted loop, a split-S and a ladder,
reaching speeds of up to 21 m/s and accelerations of up to 6 g. It further
demonstrates a non-trivial visual sim-to-real transfer by operating on
poor-quality segmentation masks, and exhibits robustness to battery depletion
by accurately estimating the maximum attainable motor RPM and adjusting its
flight path in real-time. These results highlight SkyDreamer's adaptability to
important aspects of the reality gap, bringing robustness while still achieving
extremely high-speed, agile flight.

</details>


### [366] [Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping](https://arxiv.org/abs/2510.14827)
*Yufei Zhu,Shih-Min Yang,Andrey Rudenko,Tomasz P. Kucner,Achim J. Lilienthal,Martin Magnusson*

Main category: cs.RO

TL;DR: 我们提出了一种基于隐式神经网络的连续时空运动模式地图（MoD）表示方法，以实现机器人安全高效地在复杂人类环境中运行。该方法直接将坐标映射到半包裹高斯混合模型（SGM）的参数，消除了离散化和插补的需要，实现了跨越空间和时间的平滑泛化。在包含长期真实世界人员跟踪数据的公共数据集上进行评估，与现有方法相比，我们的方法在稀疏区域实现了更准确的运动表示和更平滑的速度分布，同时计算效率仍然很高。


<details>
  <summary>Details</summary>
Motivation: 为了使机器人在复杂的人类环境中能够安全高效地运行，需要对特定场所的运动模式进行良好的建模。现有的运动模式地图（MoD）虽然可以编码地图中的统计运动模式，但存在离散采样和构建成本高昂的问题。

Method: 提出了一种基于隐式神经网络的连续时空MoD表示，该网络直接将坐标映射到半包裹高斯混合模型（SGM）的参数，从而避免了离散化和插补的需要，实现了跨越空间和时间的平滑泛化。

Result: 在包含长期真实世界人员跟踪数据的公共数据集上进行评估，我们的方法在稀疏区域实现了更准确的运动表示和更平滑的速度分布，并且计算效率高，优于现有基线。

Conclusion: 所提出的方法展示了一种强大而有效的方法来模拟复杂的人类运动模式。

Abstract: Safe and efficient robot operation in complex human environments can benefit
from good models of site-specific motion patterns. Maps of Dynamics (MoDs)
provide such models by encoding statistical motion patterns in a map, but
existing representations use discrete spatial sampling and typically require
costly offline construction. We propose a continuous spatio-temporal MoD
representation based on implicit neural functions that directly map coordinates
to the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the
need for discretization and imputation for unevenly sampled regions, enabling
smooth generalization across both space and time. Evaluated on a large public
dataset with long-term real-world people tracking data, our method achieves
better accuracy of motion representation and smoother velocity distributions in
sparse regions while still being computationally efficient, compared to
available baselines. The proposed approach demonstrates a powerful and
efficient way of modeling complex human motion patterns.

</details>


### [367] [RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning](https://arxiv.org/abs/2510.14830)
*Kun Lei,Huanyu Li,Dongjie Yu,Zhenyu Wei,Lingxiao Guo,Zhennan Jiang,Ziyu Wang,Shiyu Liang,Huazhe Xu*

Main category: cs.RO

TL;DR: RL-100是一个基于扩散视觉-动作策略的现实世界强化学习训练框架，通过模仿学习、迭代离线强化学习和在线强化学习三阶段进行训练，实现了在七项真实机器人任务上的100%成功率，并在时间效率和鲁棒性方面达到了接近甚至优于人类操作员的水平。


<details>
  <summary>Details</summary>
Motivation: 现实世界的机器人操作需要在家庭和工厂中实现接近或超越熟练人类操作员的可靠性、效率和鲁棒性。

Method: RL-100框架包含一个三阶段的流程：1. 模仿学习利用人类先验知识。2. 迭代离线强化学习结合了离线策略评估（OPE）和PPO风格的更新，用于去噪过程以进行保守和可靠的改进。3. 在线强化学习用于消除残留的故障模式。此外，还引入了一个轻量级的蒸馏头，将扩散模型的采样过程压缩为单步策略，以实现高频控制和显著降低延迟，同时保持任务性能。该框架具有任务、具身和表示无关性，支持3D点云和2D RGB输入、多种机器人平台以及单步和动作块策略。

Result: RL-100在七项真实机器人任务（包括动态刚体控制、流体和颗粒倾倒、可变形织物折叠、精确的灵巧螺丝拧松以及多阶段榨橙汁）上进行了评估，在所有评估试验中成功率达到100%，共计900个试验中有900个成功，其中一项任务连续进行了250个试验且全部成功。该方法在时间效率上达到了接近人类远程操作员的水平或更好，并展示了多小时的鲁棒性，可连续运行长达两小时。

Conclusion: RL-100是一个强大且通用的现实世界机器人强化学习训练框架，通过结合模仿学习、迭代离线强化学习和在线强化学习，并利用蒸馏技术优化策略，显著提高了机器人在各种复杂任务上的性能、效率和鲁棒性。

Abstract: Real-world robotic manipulation in homes and factories demands reliability,
efficiency, and robustness that approach or surpass skilled human operators. We
present RL-100, a real-world reinforcement learning training framework built on
diffusion visuomotor policies trained bu supervised learning. RL-100 introduces
a three-stage pipeline. First, imitation learning leverages human priors.
Second, iterative offline reinforcement learning uses an Offline Policy
Evaluation procedure, abbreviated OPE, to gate PPO-style updates that are
applied in the denoising process for conservative and reliable improvement.
Third, online reinforcement learning eliminates residual failure modes. An
additional lightweight consistency distillation head compresses the multi-step
sampling process in diffusion into a single-step policy, enabling
high-frequency control with an order-of-magnitude reduction in latency while
preserving task performance. The framework is task-, embodiment-, and
representation-agnostic and supports both 3D point clouds and 2D RGB inputs, a
variety of robot platforms, and both single-step and action-chunk policies. We
evaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,
such as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth
folding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100
attains 100\% success across evaluated trials for a total of 900 out of 900
episodes, including up to 250 out of 250 consecutive trials on one task. The
method achieves near-human teleoperation or better time efficiency and
demonstrates multi-hour robustness with uninterrupted operation lasting up to
two hours.

</details>


### [368] [STITCHER: Constrained Trajectory Planning in Known Environments with Real-Time Motion Primitive Search](https://arxiv.org/abs/2510.14893)
*Helene J. Levy,Brett T. Lopez*

Main category: cs.RO

TL;DR: STITCHER是一个无需优化即可在复杂环境中实时生成长距离、表现力强且接近最优的轨迹的规划框架，它通过图搜索将短轨迹段拼接起来，优于现有的基于优化的规划器。


<details>
  <summary>Details</summary>
Motivation: 在大型复杂环境中进行自主高速导航需要实时生成满足动态可行性、无碰撞以及状态或执行器约束的敏捷轨迹。现有的基于优化的规划方法在计算时间和数值稳定性方面存在局限性，限制了其在安全关键场景中的应用。

Method: STITCHER是一个无需优化的规划框架，它使用图搜索将短轨迹段拼接起来，以实时计算长距离、表现力强且接近最优的轨迹。

Result: STITCHER的性能优于现有的基于优化的规划器，能够以毫秒级的速度在复杂环境中生成安全轨迹，并在硬件测试中证明了其在遵守非凸约束方面的能力。

Conclusion: STITCHER通过创新的规划架构和算法，成功实现了实时规划，克服了传统基于优化方法的局限性，为自主高速导航提供了有效的解决方案。

Abstract: Autonomous high-speed navigation through large, complex environments requires
real-time generation of agile trajectories that are dynamically feasible,
collision-free, and satisfy state or actuator constraints. Modern trajectory
planning techniques primarily use numerical optimization, as they enable the
systematic computation of high-quality, expressive trajectories that satisfy
various constraints. However, stringent requirements on computation time and
the risk of numerical instability can limit the use of optimization-based
planners in safety-critical scenarios. This work presents an optimization-free
planning framework called STITCHER that stitches short trajectory segments
together with graph search to compute long-range, expressive, and near-optimal
trajectories in real-time. STITCHER outperforms modern optimization-based
planners through our innovative planning architecture and several algorithmic
developments that make real-time planning possible. Extensive simulation
testing is performed to analyze the algorithmic components that make up
STITCHER, along with a thorough comparison with two state-of-the-art
optimization planners. Simulation tests show that safe trajectories can be
created within a few milliseconds for paths that span the entirety of two 50 m
x 50 m environments. Hardware tests with a custom quadrotor verify that
STITCHER can produce trackable paths in real-time while respecting nonconvex
constraints, such as limits on tilt angle and motor forces, which are otherwise
hard to include in optimization-based planners.

</details>


### [369] [VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation](https://arxiv.org/abs/2510.14902)
*Han Zhao,Jiaxuan Zhang,Wenxuan Song,Pengxiang Ding,Donglin Wang*

Main category: cs.RO

TL;DR: VLA^2框架通过结合外部知识库（如网络检索和物体检测）来解决现有VLA模型在处理训练数据之外的物体概念时泛化能力不足的问题，并在新的基准测试中显著提高了成功率，尤其是在处理困难的分布外物体时。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在处理训练数据之外的物体概念（如未见过或纹理不同的物体）时，成功率会显著下降，泛化能力不足。

Method: 提出了一种名为VLA^2的新型智能体框架，该框架以OpenVLA为执行骨干，并集成了网络检索和物体检测等外部模块，为VLA提供目标物体的视觉和文本知识，从而缓解在处理分布外物体时的泛化失败问题。

Result: 在LIBERO模拟环境中，VLA^2框架在包含新物体和新描述的新评估基准上进行了测试。在困难级别基准测试中，VLA^2的成功率比当前最先进的模型高出44.2%，在所有自定义环境中平均提高了20.2%，同时在领域内任务上没有性能下降。

Conclusion: VLA^2框架通过利用外部知识模块，有效地提高了VLA模型在处理分布外物体时的泛化能力，并在新的基准测试中取得了显著的性能提升。

Abstract: Current vision-language-action (VLA) models, pre-trained on large-scale
robotic data, exhibit strong multi-task capabilities and generalize well to
variations in visual and language instructions for manipulation. However, their
success rate drops significantly when faced with object concepts outside the
training data, such as unseen object descriptions and textures in the dataset.
To address this, we propose a novel agentic framework, VLA^2, which leverages
OpenVLA as the execution backbone and effectively leverages external modules
such as web retrieval and object detection to provide visual and textual
knowledge about target objects to the VLA. This approach mitigates
generalization failure when handling out-of-distribution objects. Based on the
LIBERO simulation environment, we introduced novel objects and object
descriptions to construct a new evaluation benchmark with three difficulty
levels to test the effectiveness of our method. Our framework successfully
outperformed the current state-of-the-art models on our designed hard-level
generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2
achieves a 44.2% improvement in the success rate in the hard-level benchmark
and an average improvement of 20.2% in all customized environments without any
performance degradation on in-domain tasks. Project website:
https://vla-2.github.io.

</details>


### [370] [VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tunin](https://arxiv.org/abs/2510.14930)
*Binghao Huang,Jie Xu,Iretiayo Akinola,Wei Yang,Balakumar Sundaralingam,Rowland O'Flaherty,Dieter Fox,Xiaolong Wang,Arsalan Mousavian,Yu-Wei Chao,Yunzhu Li*

Main category: cs.RO

TL;DR: VT-Refine是一个结合了视觉、触觉反馈和强化学习的机器人装配框架，通过模拟和真实数据相结合，提升了机器人在精密装配任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 机器人难以通过单纯模仿人类行为来完成精密的双臂装配任务，因为人类演示数据存在次优和多样性不足的问题。

Method: 首先，在少量演示数据上训练一个结合了视觉和触觉输入的扩散策略。然后，将该策略转移到配备了模拟触觉传感器的数字孪生中，并通过大规模强化学习进行优化，以提高鲁棒性和泛化能力。为了实现精确的仿真到现实世界的迁移，使用了高分辨率压阻触觉传感器，并利用GPU加速仿真进行建模。

Result: VT-Refine在模拟和现实世界中都提高了装配性能，通过增加数据多样性和实现更有效的策略微调。

Conclusion: VT-Refine框架通过融合模拟和真实数据，并利用强化学习，成功提升了机器人执行精密双臂装配任务的能力。

Abstract: Humans excel at bimanual assembly tasks by adapting to rich tactile feedback
-- a capability that remains difficult to replicate in robots through
behavioral cloning alone, due to the suboptimality and limited diversity of
human demonstrations. In this work, we present VT-Refine, a visuo-tactile
policy learning framework that combines real-world demonstrations,
high-fidelity tactile simulation, and reinforcement learning to tackle precise,
contact-rich bimanual assembly. We begin by training a diffusion policy on a
small set of demonstrations using synchronized visual and tactile inputs. This
policy is then transferred to a simulated digital twin equipped with simulated
tactile sensors and further refined via large-scale reinforcement learning to
enhance robustness and generalization. To enable accurate sim-to-real transfer,
we leverage high-resolution piezoresistive tactile sensors that provide normal
force signals and can be realistically modeled in parallel using
GPU-accelerated simulation. Experimental results show that VT-Refine improves
assembly performance in both simulation and the real world by increasing data
diversity and enabling more effective policy fine-tuning. Our project page is
available at https://binghao-huang.github.io/vt_refine/.

</details>


### [371] [From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance](https://arxiv.org/abs/2510.14952)
*Zhe Li,Cheng Chi,Yangyang Wei,Boan Zhu,Yibo Peng,Tao Huang,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang,Chang Xu*

Main category: cs.RO

TL;DR: RoboGhost是一个无需重定向的框架，可以直接将人形机器人策略的语言理解运动潜能化。它消除了显式的运动解码和重定向，通过基于扩散的模型直接从噪声中生成可执行动作，从而实现快速、反应灵敏的控制，并保持语义意图。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导的人形机器人运动控制流程复杂、延迟高、易出错，且语义与控制耦合性差，需要更直接的语言到动作的路径。

Method: 提出RoboGhost框架，直接将语言运动潜能作为人形机器人策略的条件。使用基于扩散和因果变换器的混合模型来生成长期一致、稳定且多样化的动作，实现从噪声到可执行动作的直接去噪。

Result: RoboGhost显著降低了部署延迟，提高了成功率和跟踪精度，并在真实人形机器人上实现了平滑、语义对齐的运动。该框架还可以扩展到图像、音频和音乐等其他模态。

Conclusion: RoboGhost提供了一个无需重定向的、直接的语言到动作的框架，用于人形机器人运动控制，显著优于现有方法，并为多模态驱动的人形机器人系统奠定了基础。

Abstract: Natural language offers a natural interface for humanoid robots, but existing
language-guided humanoid locomotion pipelines remain cumbersome and unreliable.
They typically decode human motion, retarget it to robot morphology, and then
track it with a physics-based controller. However, this multi-stage process is
prone to cumulative errors, introduces high latency, and yields weak coupling
between semantics and control. These limitations call for a more direct pathway
from language to action, one that eliminates fragile intermediate stages.
Therefore, we present RoboGhost, a retargeting-free framework that directly
conditions humanoid policies on language-grounded motion latents. By bypassing
explicit motion decoding and retargeting, RoboGhost enables a diffusion-based
policy to denoise executable actions directly from noise, preserving semantic
intent and supporting fast, reactive control. A hybrid causal
transformer-diffusion motion generator further ensures long-horizon consistency
while maintaining stability and diversity, yielding rich latent representations
for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost
substantially reduces deployment latency, improves success rates and tracking
accuracy, and produces smooth, semantically aligned locomotion on real
humanoids. Beyond text, the framework naturally extends to other modalities
such as images, audio, and music, providing a general foundation for
vision-language-action humanoid systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [372] [Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context](https://arxiv.org/abs/2510.13858)
*Raheleh Biglari,Joachim Denil*

Main category: cs.AI

TL;DR: 模型有效性验证的新方法，基于决策一致性而非输出相似性，可用于缺乏明确边界的场景。


<details>
  <summary>Details</summary>
Motivation: 传统模型有效性验证方法依赖预定义边界，但这些边界不一定可用或足够。

Method: 提出决策导向技术（DOTechnique），通过评估替代模型是否能做出与高保真模型等效的决策来确定模型有效性，并结合领域约束和符号推理提高效率。

Result: 在一个高速公路变道系统的示例中，DOTechnique 成功识别了仿真模型的有效性区域。

Conclusion: DOTechnique 能够通过决策者背景来寻找模型有效性，具有很高的应用潜力。

Abstract: Model validity is as critical as the model itself, especially when guiding
decision-making processes. Traditional approaches often rely on predefined
validity frames, which may not always be available or sufficient. This paper
introduces the Decision Oriented Technique (DOTechnique), a novel method for
determining model validity based on decision consistency rather than output
similarity. By evaluating whether surrogate models lead to equivalent decisions
compared to high-fidelity models, DOTechnique enables efficient identification
of validity regions, even in the absence of explicit validity boundaries. The
approach integrates domain constraints and symbolic reasoning to narrow the
search space, enhancing computational efficiency. A highway lane change system
serves as a motivating example, demonstrating how DOTechnique can uncover the
validity region of a simulation model. The results highlight the potential of
the technique to support finding model validity through decision-maker context.

</details>


### [373] [Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks](https://arxiv.org/abs/2510.13979)
*Supriti Sinhamahapatra,Jan Niehues*

Main category: cs.AI

TL;DR: 该研究提出了一种结合语音和演示文稿（PPT）视觉信息的多模态自动语音识别（ASR）方法，并在科学演示场景下取得了显著的词错误率（WER）降低。


<details>
  <summary>Details</summary>
Motivation: 现有的SOTA ASR系统主要依赖声学信息，忽略了视觉等模态信息，而视觉信息对于消除歧义和适应性至关重要，尤其是在科学演示等场景下，演示文稿（PPT）包含大量领域专业术语，对ASR的准确性有很大影响。

Method: 1. 创建了一个包含自动领域术语转录的多模态演示基准。2. 探索了利用多模态信息增强语音模型的方法。3. 针对缺乏带幻灯片的ASR数据集的问题，采用了合适的数据增强方法。4. 训练了一个使用增强数据集的多模态ASR模型。

Result: 与基线模型相比，在所有词上的词错误率（WER）相对降低了约34%，在领域特定术语上的WER相对降低了35%。

Conclusion: 该研究成功地利用演示文稿（PPT）的视觉信息来提升ASR系统的性能，尤其是在处理领域特定术语方面，证明了多模态信息在ASR中的有效性。

Abstract: State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily
rely on acoustic information while disregarding additional multi-modal context.
However, visual information are essential in disambiguation and adaptation.
While most work focus on speaker images to handle noise conditions, this work
also focuses on integrating presentation slides for the use cases of scientific
presentation.
  In a first step, we create a benchmark for multi-modal presentation including
an automatic analysis of transcribing domain-specific terminology. Next, we
explore methods for augmenting speech models with multi-modal information. We
mitigate the lack of datasets with accompanying slides by a suitable approach
of data augmentation. Finally, we train a model using the augmented dataset,
resulting in a relative reduction in word error rate of approximately 34%,
across all words and 35%, for domain-specific terms compared to the baseline
model.

</details>


### [374] [CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization](https://arxiv.org/abs/2510.14150)
*Henrique Assumpção,Diego Ferreira,Leandro Campos,Fabricio Murai*

Main category: cs.AI

TL;DR: CodeEvolve是一个开源的进化编码代理，结合了大型语言模型（LLMs）和遗传算法来解决复杂计算问题。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机是结合大型语言模型（LLMs）和遗传算法来解决复杂计算问题，并在此基础上改进现有的科学发现方法。

Method: CodeEvolve采用基于岛屿的遗传算法来维持种群多样性和提高吞吐量，引入了基于LLM上下文窗口的灵感交叉机制来结合成功解决方案的特征，并实施了元提示策略来动态探索解决方案空间。

Result: 在用于评估Google DeepMind闭源AlphaEvolve的数学基准子集上，CodeEvolve的表现优于AlphaEvolve。

Conclusion: CodeEvolve在解决复杂计算问题方面取得了优于AlphaEvolve的成果，并且该框架已开源以促进合作和加速进展。

Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding
agent that unites Large Language Models (LLMs) with genetic algorithms to solve
complex computational problems. Our framework adapts powerful evolutionary
concepts to the LLM domain, building upon recent methods for generalized
scientific discovery. CodeEvolve employs an island-based genetic algorithm to
maintain population diversity and increase throughput, introduces a novel
inspiration-based crossover mechanism that leverages the LLMs context window to
combine features from successful solutions, and implements meta-prompting
strategies for dynamic exploration of the solution space. We conduct a rigorous
evaluation of CodeEvolve on a subset of the mathematical benchmarks used to
evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that
our method surpasses AlphaEvolve's performance on several challenging problems.
To foster collaboration and accelerate progress, we release our complete
framework as an open-source repository.

</details>


### [375] [Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment](https://arxiv.org/abs/2510.13985)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Giovanni Franco Gabriel Marraffini,Mario Alejandro Leiva,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.AI

TL;DR: 大型语言模型容易产生因果错觉，即使在缺乏证据的情况下也会推断出不相关的因果关系。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨大型语言模型（LLM）在面对经典的认知科学范式“联结判断任务”时，是否会产生因果错觉，即在缺乏支持性证据的情况下，感知到变量之间的因果关系。

Method: 研究人员构建了一个包含1000个零联结情景（即可用信息不足以建立变量间的因果关系）的医学领域数据集，并提示LLM评估潜在原因的有效性。

Result: 所有被评估的模型系统地推断出不合理的因果关系，显示出强烈的因果错觉倾向。

Conclusion: 研究结果表明，LLM容易产生因果错觉，这支持了它们仅仅是复制因果语言而非真正理解因果关系的观点，并对在需要准确因果推理的领域使用LLM提出了担忧。

Abstract: Causal learning is the cognitive process of developing the capability of
making causal inferences based on available information, often guided by
normative principles. This process is prone to errors and biases, such as the
illusion of causality, in which people perceive a causal relationship between
two variables despite lacking supporting evidence. This cognitive bias has been
proposed to underlie many societal problems, including social prejudice,
stereotype formation, misinformation, and superstitious thinking. In this work,
we examine whether large language models are prone to developing causal
illusions when faced with a classic cognitive science paradigm: the contingency
judgment task. To investigate this, we constructed a dataset of 1,000 null
contingency scenarios (in which the available information is not sufficient to
establish a causal relationship between variables) within medical contexts and
prompted LLMs to evaluate the effectiveness of potential causes. Our findings
show that all evaluated models systematically inferred unwarranted causal
relationships, revealing a strong susceptibility to the illusion of causality.
While there is ongoing debate about whether LLMs genuinely understand causality
or merely reproduce causal language without true comprehension, our findings
support the latter hypothesis and raise concerns about the use of language
models in domains where accurate causal reasoning is essential for informed
decision-making.

</details>


### [376] [GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations](https://arxiv.org/abs/2510.14035)
*Rajesh Mangannavar,Prasad Tadepalli*

Main category: cs.AI

TL;DR: GammaZero使用基于图的表示方法来解决POMDP问题，实现了跨问题规模的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在POMDP问题上需要领域特定的神经网络架构，并且难以扩展。GammaZero旨在通过统一的基于图的信念表示来克服这些限制。

Method: GammaZero将信念状态转换为以动作为中心的图，并使用图神经网络和解码器架构从专家演示中学习价值函数和策略，然后将这些学习到的启发式方法应用于蒙特卡洛树搜索以解决更大的问题。

Result: 在标准的POMDP基准测试中，GammaZero在相同规模的问题上表现与BetaZero相当，并且能够实现零样本泛化到比训练时遇到的问题大2-4倍的问题，同时保持解决方案质量并减少搜索需求。

Conclusion: GammaZero提出的基于图的表示方法能够系统地将信念状态转换为以动作为中心的图，使得在小问题上学习到的结构模式能够迁移到大问题上，从而实现了在POMDP问题中的规模泛化。

Abstract: We introduce an action-centric graph representation framework for learning to
guide planning in Partially Observable Markov Decision Processes (POMDPs).
Unlike existing approaches that require domain-specific neural architectures
and struggle with scalability, GammaZero leverages a unified graph-based belief
representation that enables generalization across problem sizes within a
domain. Our key insight is that belief states can be systematically transformed
into action-centric graphs where structural patterns learned on small problems
transfer to larger instances. We employ a graph neural network with a decoder
architecture to learn value functions and policies from expert demonstrations
on computationally tractable problems, then apply these learned heuristics to
guide Monte Carlo tree search on larger problems. Experimental results on
standard POMDP benchmarks demonstrate that GammaZero achieves comparable
performance to BetaZero when trained and tested on the same-sized problems,
while uniquely enabling zero-shot generalization to problems 2-4 times larger
than those seen during training, maintaining solution quality with reduced
search requirements.

</details>


### [377] [Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems](https://arxiv.org/abs/2510.14133)
*Edoardo Allegrini,Ananth Shreekumar,Z. Berkay Celik*

Main category: cs.AI

TL;DR: `tldr`: 本文提出了一种用于分析基于LLM的agentic AI系统的建模框架，解决了现有通信协议碎片化导致的语义鸿沟问题，并通过定义形式化属性实现了对系统行为的严格验证，旨在提高系统的正确性、可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的agentic AI系统在处理复杂任务时，其通信协议（如MCP和A2A）被孤立分析，导致了语义鸿沟，阻碍了对系统整体属性的严格分析，并可能引入架构错位和可利用的协调问题等风险。因此，需要一个统一的框架来解决这些挑战。

Method: 本文提出了一种由两个基础模型组成的建模框架：1. 主机代理模型：形式化了与用户交互、分解任务并协调外部代理和工具执行的顶层实体。2. 任务生命周期模型：详细说明了从创建到完成的单个子任务的状态和转换，提供了任务管理和错误处理的细粒度视图。基于此框架，定义了17个主机代理属性和14个任务生命周期属性，涵盖了活性、安全性、完整性和公平性，并使用时序逻辑进行了形式化。

Result: 本文提出的建模框架能够对agentic AI系统的行为进行统一的语义推理。通过形式化定义的属性，可以实现对系统行为的严格验证，检测协调中的极端情况，并预防死锁和安全漏洞。

Conclusion: 本文介绍了第一个严格、领域无关的框架，用于系统地分析、设计和部署正确、可靠和鲁棒的agentic AI系统。该框架通过统一的语义模型和形式化属性，解决了现有agentic AI系统在安全、安全和功能方面面临的挑战。

Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large
Language Models (LLMs), are increasingly used to address complex, multi-step
tasks. The safety, security, and functionality of these systems are critical,
especially in high-stakes applications. However, the current ecosystem of
inter-agent communication is fragmented, with protocols such as the Model
Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol
for coordination being analyzed in isolation. This fragmentation creates a
semantic gap that prevents the rigorous analysis of system properties and
introduces risks such as architectural misalignment and exploitable
coordination issues. To address these challenges, we introduce a modeling
framework for agentic AI systems composed of two foundational models. The
first, the host agent model, formalizes the top-level entity that interacts
with the user, decomposes tasks, and orchestrates their execution by leveraging
external agents and tools. The second, the task lifecycle model, details the
states and transitions of individual sub-tasks from creation to completion,
providing a fine-grained view of task management and error handling. Together,
these models provide a unified semantic framework for reasoning about the
behavior of multi-AI agent systems. Grounded in this framework, we define 17
properties for the host agent and 14 for the task lifecycle, categorized into
liveness, safety, completeness, and fairness. Expressed in temporal logic,
these properties enable formal verification of system behavior, detection of
coordination edge cases, and prevention of deadlocks and security
vulnerabilities. Through this effort, we introduce the first rigorously
grounded, domain-agnostic framework for the systematic analysis, design, and
deployment of correct, reliable, and robust agentic AI systems.

</details>


### [378] [Where to Search: Measure the Prior-Structured Search Space of LLM Agents](https://arxiv.org/abs/2510.14846)
*Zhuo-Yang Song*

Main category: cs.AI

TL;DR: LLM驱动的生成-过滤-精炼（迭代范式）在AI+科学领域取得进展，但搜索有效性受限于搜索空间编码。本文提出一种紧凑的数理理论，用于描述和度量由领域先验知识指导的LLM辅助迭代搜索。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的生成-过滤-精炼范式在AI+科学领域取得进展，但搜索的有效性依赖于如何将领域先验知识编码到结构化的假设空间中。

Method: 提出一种紧凑的数理理论，将智能体表示为作用于输入和输出的模糊关系算子，并用固定安全包络约束。通过加权并求和所有可达路径来描述多步推理/搜索，得到覆盖生成函数，从而定义可达性难度度量，并提供搜索的几何解释。

Result: 该理论提供了一种可行的语言和操作工具来度量智能体及其搜索空间，为LLM构建的迭代搜索提供了系统的形式化描述，并通过多数投票实例化进行了最简单的可检验推理验证。

Conclusion: 本文提出的理论为理解和优化LLM在AI+科学领域的迭代搜索过程提供了坚实的基础。

Abstract: The generate-filter-refine (iterative paradigm) based on large language
models (LLMs) has achieved progress in reasoning, programming, and program
discovery in AI+Science. However, the effectiveness of search depends on where
to search, namely, how to encode the domain prior into an operationally
structured hypothesis space. To this end, this paper proposes a compact formal
theory that describes and measures LLM-assisted iterative search guided by
domain priors. We represent an agent as a fuzzy relation operator on inputs and
outputs to capture feasible transitions; the agent is thereby constrained by a
fixed safety envelope. To describe multi-step reasoning/search, we weight all
reachable paths by a single continuation parameter and sum them to obtain a
coverage generating function; this induces a measure of reachability
difficulty; and it provides a geometric interpretation of search on the graph
induced by the safety envelope. We further provide the simplest testable
inferences and validate them via a majority-vote instantiation. This theory
offers a workable language and operational tools to measure agents and their
search spaces, proposing a systematic formal description of iterative search
constructed by LLMs.

</details>


### [379] [Position: Require Frontier AI Labs To Release Small "Analog" Models](https://arxiv.org/abs/2510.14053)
*Shriyash Upadhyay,Chaithanya Bandi,Narmeen Oozeer,Philip Quirke*

Main category: cs.AI

TL;DR: AI监管应强制要求发布小型开放模型，以促进安全和创新。


<details>
  <summary>Details</summary>
Motivation: 当前AI监管因安全-创新权衡而被搁置，需要一种新的方法。

Method: 强制大型AI实验室发布基于其专有模型训练和蒸馏的小型开放模型作为公共代理。

Result: 小型模型可用于安全验证、可解释性研究和算法透明度，且在这些模型上开发的方法可推广到大型系统；此举可减轻监管负担并加速安全进步，同时成本增加最小。

Conclusion: 强制发布小型开放模型是促进AI安全和创新的可行方法，并证明了支持基础研究的更广泛原则。

Abstract: Recent proposals for regulating frontier AI models have sparked concerns
about the cost of safety regulation, and most such regulations have been
shelved due to the safety-innovation tradeoff. This paper argues for an
alternative regulatory approach that ensures AI safety while actively promoting
innovation: mandating that large AI laboratories release small, openly
accessible analog models (scaled-down versions) trained similarly to and
distilled from their largest proprietary models.
  Analog models serve as public proxies, allowing broad participation in safety
verification, interpretability research, and algorithmic transparency without
forcing labs to disclose their full-scale models. Recent research demonstrates
that safety and interpretability methods developed using these smaller models
generalize effectively to frontier-scale systems. By enabling the wider
research community to directly investigate and innovate upon accessible
analogs, our policy substantially reduces the regulatory burden and accelerates
safety advancements.
  This mandate promises minimal additional costs, leveraging reusable resources
like data and infrastructure, while significantly contributing to the public
good. Our hope is not only that this policy be adopted, but that it illustrates
a broader principle supporting fundamental research in machine learning: deeper
understanding of models relaxes the safety-innovation tradeoff and lets us have
more of both.

</details>


### [380] [Agentic Design of Compositional Machines](https://arxiv.org/abs/2510.14980)
*Wenqian Zhang,Weiyang Liu,Zhen Liu*

Main category: cs.AI

TL;DR: LLMs can be trained to design complex machines using a simulated environment and reinforcement learning, though current models require further improvement.


<details>
  <summary>Details</summary>
Motivation: Investigate whether Large Language models (LLMs) can learn to design complex machines through compositional machine design, assembled from standardized components to meet functional demands.

Method: Introduced BesiegeField, a testbed built on the game Besiege, for part-based construction, physical simulation, and reward-driven evaluation. Benchmarked LLMs with agentic workflows and explored reinforcement learning (RL) finetuning with a curated cold-start dataset.

Result: State-of-the-art LLMs currently fall short in this task, highlighting the need for capabilities such as spatial reasoning, strategic assembly, and instruction-following. RL finetuning shows potential but faces open challenges.

Conclusion: While LLMs show promise in machine design, current open-source models require significant improvement. Reinforcement learning offers a potential path forward, but challenges remain at the intersection of language, machine design, and physical reasoning.

Abstract: The design of complex machines stands as both a marker of human intelligence
and a foundation of engineering practice. Given recent advances in large
language models (LLMs), we ask whether they, too, can learn to create. We
approach this question through the lens of compositional machine design: a task
in which machines are assembled from standardized components to meet functional
demands like locomotion or manipulation in a simulated physical environment. To
support this investigation, we introduce BesiegeField, a testbed built on the
machine-building game Besiege, which enables part-based construction, physical
simulation and reward-driven evaluation. Using BesiegeField, we benchmark
state-of-the-art LLMs with agentic workflows and identify key capabilities
required for success, including spatial reasoning, strategic assembly, and
instruction-following. As current open-source models fall short, we explore
reinforcement learning (RL) as a path to improvement: we curate a cold-start
dataset, conduct RL finetuning experiments, and highlight open challenges at
the intersection of language, machine design, and physical reasoning.

</details>


### [381] [Generating Fair Consensus Statements with Social Choice on Token-Level MDPs](https://arxiv.org/abs/2510.14106)
*Carter Blair,Kate Larson*

Main category: cs.AI

TL;DR: 大型语言模型在生成共识声明时，由于缺乏固有结构，无法保证聚合自由表达观点的公平性。本文将该任务建模为多目标、token层级的马尔可夫决策过程（MDP），其中每个目标代表一个智能体的偏好。通过利用现有研究成果，为每个token生成步骤量化奖励，无需值函数。该MDP公式化方法借鉴了社会选择论的原理。提出两种方法：一是基于社会选择论的随机生成策略，该策略保证在ex-ante核心内，并将核心稳定性概念扩展到文本生成；二是针对生成单一声明，利用MDP框架内的搜索算法最大化平等的福利。实验表明，本文方法在最坏情况下的代理人对齐方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的共识声明生成框架缺乏结构，无法保证公平性。

Method: 将共识声明生成建模为多目标、token层级的马尔可夫决策过程（MDP），并提出两种基于社会选择论的方法：一是保证在ex-ante核心内的随机生成策略，二是利用搜索算法最大化平等的福利。

Result: 实验结果表明，本文提出的基于平等福利的目标引导搜索，在最坏情况下的代理人对齐方面优于包括Habermas Machine在内的基线方法。

Conclusion: 本文提出的基于MDP和社会选择论的方法，为生成公平的共识声明提供了理论基础和实证支持。

Abstract: Current frameworks for consensus statement generation with large language
models lack the inherent structure needed to provide provable fairness
guarantees when aggregating diverse free-form opinions. We model the task as a
multi-objective, token-level Markov Decision Process (MDP), where each
objective corresponds to an agent's preference. Token-level rewards for each
agent are derived from their policy (e.g., a personalized language model). This
approach utilizes the finding that such policies implicitly define optimal
Q-functions, providing a principled way to quantify rewards at each generation
step without a value function (Rafailov et al., 2024). This MDP formulation
creates a formal structure amenable to analysis using principles from social
choice theory. We propose two approaches grounded in social choice theory.
First, we propose a stochastic generation policy guaranteed to be in the
ex-ante core, extending core stability concepts from voting theory to text
generation. This policy is derived from an underlying distribution over
complete statements that maximizes proportional fairness (Nash Welfare).
Second, for generating a single statement, we target the maximization of
egalitarian welfare using search algorithms within the MDP framework.
Empirically, experiments using language models to instantiate agent policies
show that search guided by the egalitarian objective generates consensus
statements with improved worst-case agent alignment compared to baseline
methods, including the Habermas Machine (Tessler et al., 2024).

</details>


### [382] [STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management](https://arxiv.org/abs/2510.14112)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Benoit Boulet*

Main category: cs.AI

TL;DR: STEMS是一个用于建筑能源管理的创新的多智能体强化学习框架，通过结合图神经网络和Transformer来利用时空依赖性，并利用控制屏障函数来保证安全，从而在降低成本、减少排放和提高舒适度方面取得了显著成效。


<details>
  <summary>Details</summary>
Motivation: 建筑能源管理对于实现碳减排目标、提高居住者舒适度和降低能源成本至关重要。然而，现有的多建筑能源系统在利用时空依赖性、确保运营安全性和处理系统复杂性方面面临严峻挑战。

Method: 提出了一种名为STEMS（Spatial-Temporal Enhanced Safe Multi-Agent Coordination）的框架，该框架集成了两个核心组件：（1）一个利用GCN-Transformer融合架构的空间-时间图表示学习框架，用于捕捉建筑间关系和时间模式；（2）一个结合了控制屏障函数的安全约束多智能体强化学习算法，提供数学上的安全保证。

Result: STEMS在真实建筑数据集上的实验表明，与现有方法相比，STEMS在成本上降低了21%，排放减少了18%，并将安全违规行为从35.1%大幅降低到5.6%，同时仅以0.13的舒适度比例保持了最佳舒适度。该框架在极端天气条件下表现出强大的鲁棒性，并在不同类型的建筑中保持有效性。

Conclusion: STEMS框架在多建筑能源管理方面取得了显著成果，有效解决了现有方法的不足，并在降低成本、减少排放、提高居住者舒适度和保证系统安全方面展现出优越性能。

Abstract: Building energy management is essential for achieving carbon reduction goals,
improving occupant comfort, and reducing energy costs. Coordinated building
energy management faces critical challenges in exploiting spatial-temporal
dependencies while ensuring operational safety across multi-building systems.
Current multi-building energy systems face three key challenges: insufficient
spatial-temporal information exploitation, lack of rigorous safety guarantees,
and system complexity. This paper proposes Spatial-Temporal Enhanced Safe
Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent
reinforcement learning framework for coordinated building energy management.
STEMS integrates two core components: (1) a spatial-temporal graph
representation learning framework using a GCN-Transformer fusion architecture
to capture inter-building relationships and temporal patterns, and (2) a
safety-constrained multi-agent RL algorithm incorporating Control Barrier
Functions to provide mathematical safety guarantees. Extensive experiments on
real-world building datasets demonstrate STEMS's superior performance over
existing methods, showing that STEMS achieves 21% cost reduction, 18% emission
reduction, and dramatically reduces safety violations from 35.1% to 5.6% while
maintaining optimal comfort with only 0.13 discomfort proportion. The framework
also demonstrates strong robustness during extreme weather conditions and
maintains effectiveness across different building types.

</details>


### [383] [A Multimodal Approach to Heritage Preservation in the Context of Climate Change](https://arxiv.org/abs/2510.14136)
*David Roqui,Adèle Cormier,nistor Grozavu,Ann Bourges*

Main category: cs.AI

TL;DR: 该研究提出了一种轻量级的多模态架构，融合了传感器数据（温度、湿度）和视觉图像，以预测文化遗产地的退化程度。通过使用简化的编码器和自适应Barlow Twins损失，该模型在斯特拉斯堡大教堂的数据上取得了76.9%的准确率，显著优于现有的单模态和多模态方法，并证明了在数据稀疏的遗产监测背景下，采用对比正则化的模型简化对于有效的多模态学习至关重要。


<details>
  <summary>Details</summary>
Motivation: 文化遗产地正面临气候变化加剧的退化威胁，而传统的单模态分析（仅依赖目视检查或环境传感器）无法捕捉环境压力与材料劣化之间的复杂相互作用。因此，需要一种能够整合多种数据源以更准确地监测和预测遗产退化的方法。

Method: 提出了一种轻量级的多模态架构，该架构基于PerceiverIO模型，并进行了两项关键改进：1. 使用64维的简化潜在空间编码器，以防止在只有37个训练样本的小型数据集上出现过拟合；2. 采用自适应Barlow Twins损失函数，鼓励不同模态（传感器数据和图像）之间互补而非冗余。该模型在斯特拉斯堡大教堂的实际数据上进行了训练和评估。

Result: 在斯特拉斯堡大教堂的数据上，该模型取得了76.9%的准确率，相比标准的纯视觉或纯传感器方法（分别为46.2%和61.5%）有显著提升，同时比标准的基线多模态架构（VisualBERT, Transformer）提高了43%，比原始的PerceiverIO模型提高了25%。消融研究证实了多模态融合的有效性。此外，通过超参数研究发现，中等相关性目标（τ=0.3）在平衡模态对齐和互补性方面效果最佳，准确率达到69.2%。

Conclusion: 该研究证明，在数据量有限的遗产监测场景中，采用简化的模型结构并结合对比正则化（如自适应Barlow Twins损失）可以实现有效的多模态学习。这种方法为开发人工智能驱动的保护决策支持系统奠定了基础。

Abstract: Cultural heritage sites face accelerating degradation due to climate change,
yet tradi- tional monitoring relies on unimodal analysis (visual inspection or
environmental sen- sors alone) that fails to capture the complex interplay
between environmental stres- sors and material deterioration. We propose a
lightweight multimodal architecture that fuses sensor data (temperature,
humidity) with visual imagery to predict degradation severity at heritage
sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified
encoders (64D latent space) that prevent overfitting on small datasets (n=37
training samples), and (2) Adaptive Barlow Twins loss that encourages modality
complementarity rather than redundancy. On data from Strasbourg Cathedral, our
model achieves 76.9% accu- racy, a 43% improvement over standard multimodal
architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.
Ablation studies reveal that sensor-only achieves 61.5% while image-only
reaches 46.2%, confirming successful multimodal synergy. A systematic
hyperparameter study identifies an optimal moderate correlation target ({\tau}
=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy
compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9:
61.5%). This work demonstrates that architectural sim- plicity combined with
contrastive regularization enables effective multimodal learning in data-scarce
heritage monitoring contexts, providing a foundation for AI-driven con-
servation decision support systems.

</details>


### [384] [TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG](https://arxiv.org/abs/2510.14922)
*Annisaa Fitri Nurfidausi,Eleonora Mancini,Paolo Torroni*

Main category: cs.AI

TL;DR: 多模态方法（EEG、语音、文本）在抑郁症自动检测方面优于单模态方法，并且预训练的嵌入比手工制作的特征更有效。


<details>
  <summary>Details</summary>
Motivation: 现有的抑郁症自动检测方法在多模态信号的利用、特征的系统比较和评估协议的一致性方面存在局限性。本研究旨在解决这些问题，通过对 EEG、语音和文本模态的特征表示和建模策略进行系统探索，以提高抑郁症检测的准确性和可靠性。

Method: 本研究系统地探索了 EEG、语音和文本三种模态的特征表示和建模策略。具体来说，研究评估了手工制作的特征与预训练的嵌入，比较了不同神经网络编码器的有效性，评估了单模态、双模态和三模态配置的性能，并分析了不同融合策略（特别是结合 EEG 信号）的效果。为了确保结果的稳健性和可复现性，研究采用了统一的、与受试者无关的划分方式进行评估。

Result: 结果表明，(i) 结合 EEG、语音和文本三种模态能够显著提升多模态抑郁症检测的效果；(ii) 预训练的嵌入在抑郁症检测任务中优于手工制作的特征；(iii) 经过精心设计的三模态模型能够达到最先进的性能水平。

Conclusion: 本研究为未来在多模态抑郁症检测领域的研究奠定了基础，证明了结合 EEG、语音和文本信息的多模态方法在抑郁症自动检测方面具有巨大潜力。

Abstract: Depression is a widespread mental health disorder, yet its automatic
detection remains challenging. Prior work has explored unimodal and multimodal
approaches, with multimodal systems showing promise by leveraging complementary
signals. However, existing studies are limited in scope, lack systematic
comparisons of features, and suffer from inconsistent evaluation protocols. We
address these gaps by systematically exploring feature representations and
modelling strategies across EEG, together with speech and text. We evaluate
handcrafted features versus pre-trained embeddings, assess the effectiveness of
different neural encoders, compare unimodal, bimodal, and trimodal
configurations, and analyse fusion strategies with attention to the role of
EEG. Consistent subject-independent splits are applied to ensure robust,
reproducible benchmarking. Our results show that (i) the combination of EEG,
speech and text modalities enhances multimodal detection, (ii) pretrained
embeddings outperform handcrafted features, and (iii) carefully designed
trimodal models achieve state-of-the-art performance. Our work lays the
groundwork for future research in multimodal depression detection.

</details>


### [385] [Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola](https://arxiv.org/abs/2510.14154)
*Tian Liu,Alex Cann,Ian Colbert,Mehdi Saeedi*

Main category: cs.AI

TL;DR: RL在商业游戏中的应用缓慢，本文探讨了行为树（BT）与RL的结合，并通过AMD Schola在虚幻引擎中进行了多任务NPC的训练演示。


<details>
  <summary>Details</summary>
Motivation: RL在商业视频游戏中的应用推广缓慢，尽管行为树（BT）与RL的结合已被提出，但实际应用仍然罕见。

Method: 使用AMD Schola插件，在虚幻引擎中，结合行为树（BT）与强化学习（RL）对多任务NPC进行联合训练，以在复杂3D环境中（受《最后生还者》启发）展示各种技能。

Result: 成功演示了在复杂3D环境中，使用BT+RL方法训练多任务NPC的可行性。

Conclusion: 行为树（BT）与强化学习（RL）的结合是解决RL在商业游戏应用中挑战的关键方向，通过实际演示证明了其有效性。

Abstract: While the rapid advancements in the reinforcement learning (RL) research
community have been remarkable, the adoption in commercial video games remains
slow. In this paper, we outline common challenges the Game AI community faces
when using RL-driven NPCs in practice, and highlight the intersection of RL
with traditional behavior trees (BTs) as a crucial juncture to be explored
further. Although the BT+RL intersection has been suggested in several research
papers, its adoption is rare. We demonstrate the viability of this approach
using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by
creating multi-task NPCs in a complex 3D environment inspired by the commercial
video game ``The Last of Us". We provide detailed methodologies for jointly
training RL models with BTs while showcasing various skills.

</details>


### [386] [JEDA: Query-Free Clinical Order Search from Ambient Dialogues](https://arxiv.org/abs/2510.14169)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Amitabh Saikia,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: JEDA是一个无需LLM的实时临床指令检索层，通过联合嵌入直接和环境指令，解决了现有LLM方法延迟、不稳定性等问题。


<details>
  <summary>Details</summary>
Motivation: 临床对话包含明确指令和隐含推理，现有系统依赖LLM重写，引入延迟、不稳定性、不透明性，阻碍实时指令。JEDA旨在解决这些问题。

Method: JEDA是一个领域初始化的双编码器，可以直接检索规范指令。在无查询模式下，它编码一个短时间的滚动对话窗口来触发检索。模型使用PubMedBERT初始化，并采用无重复对比目标进行微调，将异构意图表达与共享指令概念对齐。训练过程使用约束LLM引导，将每个签发指令与互补的表述（仅命令、仅上下文、命令+上下文、上下文+推理）联系起来。

Result: JEDA实现了更清晰的指令间分离、更紧密的查询-指令耦合和更强的泛化能力。无查询模式对噪声具有鲁棒性，通过依赖短窗口而非单个话语来降低对不流畅和ASR错误的敏感性。实际部署中，JEDA带来了显著的性能提升，并且显著优于其基础编码器和最近的开放嵌入器（Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma）。

Conclusion: JEDA是一个快速、可解释、无需LLM的检索层，能够实时将环境上下文与可操作的临床指令联系起来。

Abstract: Clinical conversations mix explicit directives (order a chest X-ray) with
implicit reasoning (the cough worsened overnight, we should check for
pneumonia). Many systems rely on LLM rewriting, adding latency, instability,
and opacity that hinder real-time ordering. We present JEDA (Joint Embedding
for Direct and Ambient clinical orders), a domain-initialized bi-encoder that
retrieves canonical orders directly and, in a query-free mode, encodes a short
rolling window of ambient dialogue to trigger retrieval. Initialized from
PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA
aligns heterogeneous expressions of intent to shared order concepts. Training
uses constrained LLM guidance to tie each signed order to complementary
formulations (command only, context only, command+context, context+reasoning),
producing clearer inter-order separation, tighter query extendash order
coupling, and stronger generalization. The query-free mode is noise-resilient,
reducing sensitivity to disfluencies and ASR errors by conditioning on a short
window rather than a single utterance. Deployed in practice, JEDA yields large
gains and substantially outperforms its base encoder and recent open embedders
(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The
result is a fast, interpretable, LLM-free retrieval layer that links ambient
context to actionable clinical orders in real time.

</details>


### [387] [ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning](https://arxiv.org/abs/2510.14176)
*Roger Creus Castanyer,Faisal Mohamed,Pablo Samuel Castro,Cyrus Neary,Glen Berseth*

Main category: cs.AI

TL;DR: ARM-FM框架利用基础模型(FMs)自动将自然语言规范转换为奖励机器(RMs)，从而实现强化学习(RL)中自动、组合式的奖励设计。


<details>
  <summary>Details</summary>
Motivation: 强化学习(RL)算法对奖励函数的设计非常敏感，这是限制其广泛应用的一个核心挑战。

Method: 该框架首先使用基础模型(FMs)将自然语言规范自动转换为奖励机器(RMs)。然后，将语言嵌入与每个RM状态相关联，以实现跨任务的泛化。最后，在各种具有挑战性的环境中进行了实证评估。

Result: ARM-FM框架在各种具有挑战性的环境中被证明是有效的，并且能够实现零样本泛化。

Conclusion: ARM-FM通过利用基础模型(FMs)的推理能力，提供了一种自动设计和组合强化学习(RL)奖励的有效方法，克服了奖励函数设计的挑战，并实现了跨任务的泛化。

Abstract: Reinforcement learning (RL) algorithms are highly sensitive to reward
function specification, which remains a central challenge limiting their broad
applicability. We present ARM-FM: Automated Reward Machines via Foundation
Models, a framework for automated, compositional reward design in RL that
leverages the high-level reasoning capabilities of foundation models (FMs).
Reward machines (RMs) -- an automata-based formalism for reward specification
-- are used as the mechanism for RL objective specification, and are
automatically constructed via the use of FMs. The structured formalism of RMs
yields effective task decompositions, while the use of FMs enables objective
specifications in natural language. Concretely, we (i) use FMs to automatically
generate RMs from natural language specifications; (ii) associate language
embeddings with each RM automata-state to enable generalization across tasks;
and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse
suite of challenging environments, including evidence of zero-shot
generalization.

</details>


### [388] [Implementation of AI in Precision Medicine](https://arxiv.org/abs/2510.14194)
*Göktuğ Bender,Samer Faraj,Anand Bhardwaj*

Main category: cs.AI

TL;DR: AI在精准医疗中发挥着日益重要的作用，但临床应用仍有限。本研究对2019-2024年关于AI在精准医疗中应用的文献进行范围界定回顾，识别了数据质量、临床可靠性、工作流程整合和治理方面的关键障碍和促进因素。通过生态系统框架，我们强调了影响现实世界转化的相互依赖关系，并提出了支持可信赖和可持续实施的未来方向。


<details>
  <summary>Details</summary>
Motivation: 精准医疗中AI的应用与临床实践之间存在差距。

Method: 对2019-2024年关于AI在精准医疗中应用的文献进行范围界定回顾，并采用生态系统框架。

Result: 识别了数据质量、临床可靠性、工作流程整合和治理方面的关键障碍和促进因素。

Conclusion: 需要采取措施支持AI在精准医疗中可信赖和可持续的实施。

Abstract: Artificial intelligence (AI) has become increasingly central to precision
medicine by enabling the integration and interpretation of multimodal data, yet
implementation in clinical settings remains limited. This paper provides a
scoping review of literature from 2019-2024 on the implementation of AI in
precision medicine, identifying key barriers and enablers across data quality,
clinical reliability, workflow integration, and governance. Through an
ecosystem-based framework, we highlight the interdependent relationships
shaping real-world translation and propose future directions to support
trustworthy and sustainable implementation.

</details>


### [389] [Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks](https://arxiv.org/abs/2510.14207)
*Trilok Padhi,Pinxian Lu,Abdulkadir Erol,Tanmay Sutar,Gauri Sharma,Mina Sonmez,Munmun De Choudhury,Ugur Kursuncu*

Main category: cs.AI

TL;DR: LLM代理在网络应用中易受多轮骚扰攻击，本研究提出了一个包含合成数据集、博弈论模拟、攻击方法和评估框架的基准测试，并使用Llama和Gemini模型进行了实验。结果表明，攻击后的模型骚扰成功率极高（95.78%--99.33%），且侮辱和激怒行为尤为突出。攻击还会模仿人类攻击性模式，并且闭源模型表现出独特的升级轨迹和显著的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）代理在交互式网络应用中日益普及，但易受滥用和伤害。现有的越狱研究主要集中在单轮提示，而现实中的骚扰通常是多轮交互的。因此，需要研究多轮骚扰攻击LLM代理的有效性，并开发相应的安全防护措施。

Method: 本研究提出了在线骚扰代理基准测试，包含：1. 合成多轮骚扰对话数据集；2. 基于重复博弈论的多智能体（如骚扰者、受害者）模拟；3. 攻击LLM代理（内存、规划、微调）的三种越狱方法；4. 混合方法评估框架。实验使用了Llama-3.1-8B-Instruct和Gemini-2.0-flash两个模型。

Result: 越狱微调使骚扰几乎成为必然，在Llama模型上的攻击成功率为95.78%--96.89%（未微调为57.25%--64.19%），在Gemini模型上为99.33%（未微调为98.46%），同时将拒识率降至1-2%。最常见的有毒行为是侮辱（84.9--87.8% vs. 44.2--50.8%）和激怒（81.2--85.1% vs. 31.5--38.8%）。攻击后的代理会模仿人类攻击性模式，如规划中的马基雅维利/精神病态模式，内存中的自恋倾向。闭源和开源模型在多轮交互中表现出不同的升级轨迹，闭源模型显示出显著的脆弱性。

Conclusion: 多轮且基于理论的攻击不仅成功率高，而且能模仿类似人类的骚扰动态。这表明需要开发强大的安全防护措施，以确保在线平台的安全和负责任。研究结果揭示了LLM代理在多轮交互中的潜在风险，特别是闭源模型表现出的脆弱性，强调了持续安全研究的必要性。

Abstract: Large Language Model (LLM) agents are powering a growing share of interactive
web applications, yet remain vulnerable to misuse and harm. Prior jailbreak
research has largely focused on single-turn prompts, whereas real harassment
often unfolds over multi-turn interactions. In this work, we present the Online
Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn
harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)
simulation informed by repeated game theory, (iii) three jailbreak methods
attacking agents across memory, planning, and fine-tuning, and (iv) a
mixed-methods evaluation framework. We utilize two prominent LLMs,
LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our
results show that jailbreak tuning makes harassment nearly guaranteed with an
attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,
and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal
rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with
84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.
31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive
categories such as sexual or racial harassment. Qualitative evaluation further
reveals that attacked agents reproduce human-like aggression profiles, such as
Machiavellian/psychopathic patterns under planning, and narcissistic tendencies
with memory. Counterintuitively, closed-source and open-source models exhibit
distinct escalation trajectories across turns, with closed-source models
showing significant vulnerability. Overall, our findings show that multi-turn
and theory-grounded attacks not only succeed at high rates but also mimic
human-like harassment dynamics, motivating the development of robust safety
guardrails to ultimately keep online platforms safe and responsible.

</details>


### [390] [LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild](https://arxiv.org/abs/2510.14240)
*Jiayu Wang,Yifei Ming,Riya Dulepet,Qinglin Chen,Austin Xu,Zixuan Ke,Frederic Sala,Aws Albarghouthi,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 该论文提出了一个名为LiveResearchBench的基准测试，用于评估能够搜索和综合网络信息以生成详尽报告的智能系统（即“深度研究”系统）。该基准包含100个任务，需要实时搜索和信息整合。同时，论文还介绍了DeepEval评估套件，用于衡量报告的质量，包括内容、引用准确性和分析深度。研究者使用这两个工具评估了17个深度研究系统，并分析了它们的优缺点。


<details>
  <summary>Details</summary>
Motivation: 评估和改进能够通过搜索和综合网络信息来生成详尽、有引用的报告的智能代理系统（深度研究系统）的能力。

Method: 提出了LiveResearchBench基准测试（包含100个专家制定的任务）和DeepEval评估套件（包含内容和报告层面的质量评估指标），并使用这些工具对17个深度研究系统进行了全面的评估。

Result: 通过在LiveResearchBench上使用DeepEval评估17个系统，揭示了当前深度研究系统的优势、常见的失败模式以及提升可靠性和洞察力的关键系统组件。

Conclusion: 现有的深度研究系统在生成信息详尽、引用准确的报告方面仍存在挑战，需要进一步改进以应对复杂的现实世界信息需求。LiveResearchBench和DeepEval为未来该领域的研究提供了重要的评估工具。

Abstract: Deep research -- producing comprehensive, citation-grounded reports by
searching and synthesizing information from hundreds of live web sources --
marks an important frontier for agentic systems. To rigorously evaluate this
ability, four principles are essential: tasks should be (1) user-centric,
reflecting realistic information needs, (2) dynamic, requiring up-to-date
information beyond parametric knowledge, (3) unambiguous, ensuring consistent
interpretation across users, and (4) multi-faceted and search-intensive,
requiring search over numerous web sources and in-depth analysis. Existing
benchmarks fall short of these principles, often focusing on narrow domains or
posing ambiguous questions that hinder fair comparison. Guided by these
principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated
tasks spanning daily life, enterprise, and academia, each requiring extensive,
dynamic, real-time web search and synthesis. Built with over 1,500 hours of
human labor, LiveResearchBench provides a rigorous basis for systematic
evaluation. To evaluate citation-grounded long-form reports, we introduce
DeepEval, a comprehensive suite covering both content- and report-level
quality, including coverage, presentation, citation accuracy and association,
consistency and depth of analysis. DeepEval integrates four complementary
evaluation protocols, each designed to ensure stable assessment and high
agreement with human judgments. Using LiveResearchBench and DeepEval, we
conduct a comprehensive evaluation of 17 frontier deep research systems,
including single-agent web search, single-agent deep research, and multi-agent
systems. Our analysis reveals current strengths, recurring failure modes, and
key system components needed to advance reliable, insightful deep research.

</details>


### [391] [Towards Agentic Self-Learning LLMs in Search Environment](https://arxiv.org/abs/2510.14253)
*Wangtao Sun,Xiang Cheng,Jialin Fan,Yao Xu,Xing Yu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: 本研究探讨了自学习如何赋能大语言模型（LLM）驱动的智能体，使其在无需人工标注数据集或预设规则奖励的情况下实现规模化。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索 LLM 智能体规模化训练的可行性，特别是如何摆脱对人工数据集和规则奖励的依赖。

Method: 通过在搜索智能体环境中进行受控实验，识别出规模化训练的两个关键因素：奖励信号来源和智能体任务数据规模。研究提出了一种名为“智能体自学习”（ASL）的框架，该框架整合了任务生成、策略执行和评估，在一个共享的环境和 LLM 骨干上实现了多角色的强化学习闭环。ASL 包含一个提示生成器、一个策略模型和一个生成式奖励模型（GRM），形成一个“更难的任务设置、更精确的验证、更强的解决能力”的良性循环。

Result: 实验结果表明，GRM 奖励信号优于僵化的规则信号，并且 GRM 与策略模型协同进化能进一步提升性能。增加智能体任务数据量（即使是合成数据）也能显著增强智能体能力。ASL 框架在训练中能够持续获得稳定进步，并且优于现有的 RLVR 基线模型。即使在零标注数据条件下，ASL 也能持续改进，显示出其优越的样本效率和鲁棒性。研究还发现 GRM 的验证能力是关键瓶颈，持续训练 GRM 并引入少量真实验证数据能提高性能上限。

Conclusion: 研究得出结论，奖励来源和数据规模是开放域智能体学习的关键因素，而多角色协同进化对于可扩展、自学习的智能体至关重要。ASL 框架有效解决了 LLM 智能体规模化训练的挑战。

Abstract: We study whether self-learning can scale LLM-based agents without relying on
human-curated datasets or predefined rule-based rewards. Through controlled
experiments in a search-agent setting, we identify two key determinants of
scalable agent training: the source of reward signals and the scale of agent
task data. We find that rewards from a Generative Reward Model (GRM) outperform
rigid rule-based signals for open-domain learning, and that co-evolving the GRM
with the policy further boosts performance. Increasing the volume of agent task
data-even when synthetically generated-substantially enhances agentic
capabilities. Building on these insights, we propose \textbf{Agentic
Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning
framework that unifies task generation, policy execution, and evaluation within
a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,
a Policy Model, and a Generative Reward Model to form a virtuous cycle of
harder task setting, sharper verification, and stronger solving. Empirically,
ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines
(e.g., Search-R1) that plateau or degrade, and continues improving under
zero-labeled-data conditions, indicating superior sample efficiency and
robustness. We further show that GRM verification capacity is the main
bottleneck: if frozen, it induces reward hacking and stalls progress; continual
GRM training on the evolving data distribution mitigates this, and a small
late-stage injection of real verification data raises the performance ceiling.
This work establishes reward source and data scale as critical levers for
open-domain agent learning and demonstrates the efficacy of multi-role
co-evolution for scalable, self-improving agents. The data and code of this
paper are released at
https://github.com/forangel2014/Towards-Agentic-Self-Learning

</details>


### [392] [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://arxiv.org/abs/2510.14265)
*Xukai Wang,Xuanbo Liu,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Bohan Zeng,Jinbo Hu,Hao Liang,Junbo Niu,Xuchen Li,Ruitao Wu,Ruichuan An,Yang Shi,Liu Liu,Xu-Yao Zhang,Qiang Liu,Zhouchen Lin,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: MorphoBench是一个新的基准测试，用于评估大型语言模型的推理能力，该基准测试包含跨学科问题，并能根据模型的推理能力动态调整难度。


<details>
  <summary>Details</summary>
Motivation: 现有的针对大型语言模型推理能力的基准测试在范围上存在局限性，并且缺乏根据模型不断发展的推理能力来调整其难度的灵活性。

Method: MorphoBench通过整合跨学科问题来评估大型语言模型的推理能力，并利用模型推理过程中生成的关键陈述来适应性地调整问题的分析难度。此外，它还包括使用模拟软件生成的问题，从而能够以最少的资源消耗动态调整基准测试的难度。研究人员收集了1300多个测试问题，并根据o3和GPT-5等模型的推理能力，对MorphoBench的难度进行了迭代调整。

Result: MorphoBench通过整合跨学科问题和动态调整难度，提升了模型推理评估的全面性和有效性。

Conclusion: MorphoBench通过提供一种更全面、更有效的方法来评估和改进大型语言模型的推理能力，为提高这些模型的推理能力和科学鲁棒性提供了可靠的指导。

Abstract: With the advancement of powerful large-scale reasoning models, effectively
evaluating the reasoning capabilities of these models has become increasingly
important. However, existing benchmarks designed to assess the reasoning
abilities of large models tend to be limited in scope and lack the flexibility
to adapt their difficulty according to the evolving reasoning capacities of the
models. To address this, we propose MorphoBench, a benchmark that incorporates
multidisciplinary questions to evaluate the reasoning capabilities of large
models and can adjust and update question difficulty based on the reasoning
abilities of advanced models. Specifically, we curate the benchmark by
selecting and collecting complex reasoning questions from existing benchmarks
and sources such as Olympiad-level competitions. Additionally, MorphoBench
adaptively modifies the analytical challenge of questions by leveraging key
statements generated during the model's reasoning process. Furthermore, it
includes questions generated using simulation software, enabling dynamic
adjustment of benchmark difficulty with minimal resource consumption. We have
gathered over 1,300 test questions and iteratively adjusted the difficulty of
MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.
MorphoBench enhances the comprehensiveness and validity of model reasoning
evaluation, providing reliable guidance for improving both the reasoning
abilities and scientific robustness of large models. The code has been released
in https://github.com/OpenDCAI/MorphoBench.

</details>


### [393] [A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space](https://arxiv.org/abs/2510.14301)
*Bingjie Zhang,Yibo Yang,Renzhe,Dandan Guo,Jindong Gu,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: GuardSpace 框架通过分解预训练权重并引入安全敏感子空间和抗有害零空间来防止 LLM 在微调过程中安全对齐能力下降，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 预训练的 LLM 在适应过程中安全对齐能力容易受损，即使在良性数据或低秩适应下进行微调，也可能导致有害响应。

Method: GuardSpace 框架包含两个关键组件：安全敏感子空间和抗有害零空间。首先，通过协方差预条件奇异值分解将预训练权重显式分解为安全相关和安全不相关的部分，并从安全不相关的部分初始化低秩适配器，同时冻结安全相关部分以保留其安全机制。其次，构建一个零空间投影器，限制适配器更新改变有害提示上的安全输出，从而维持原始的拒绝行为。

Result: 实验证明，GuardSpace 在 Llama-2-7B-Chat 模型在 GSM8K 数据集上微调时，将平均有害分数从 14.4% 降低到 3.6%，同时将准确率从 26.0% 提高到 28.0%，优于现有方法。

Conclusion: GuardSpace 框架能够有效防止 LLM 在微调过程中安全对齐能力下降，并在多项下游任务中取得优于现有方法的性能。

Abstract: Large language models (LLMs) have achieved remarkable success in diverse
tasks, yet their safety alignment remains fragile during adaptation. Even when
fine-tuning on benign data or with low-rank adaptation, pre-trained safety
behaviors are easily degraded, leading to harmful responses in the fine-tuned
models. To address this challenge, we propose GuardSpace, a guardrail framework
for preserving safety alignment throughout fine-tuning, composed of two key
components: a safety-sensitive subspace and a harmful-resistant null space.
First, we explicitly decompose pre-trained weights into safety-relevant and
safety-irrelevant components using covariance-preconditioned singular value
decomposition, and initialize low-rank adapters from the safety-irrelevant
ones, while freezing safety-relevant components to preserve their associated
safety mechanism. Second, we construct a null space projector that restricts
adapter updates from altering safe outputs on harmful prompts, thereby
maintaining the original refusal behavior. Experiments with various pre-trained
models on multiple downstream tasks demonstrate that GuardSpace achieves
superior performance over existing methods. Notably, for Llama-2-7B-Chat
fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,
reducing the average harmful score from 14.4% to 3.6%, while improving the
accuracy from from 26.0% to 28.0%.

</details>


### [394] [Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies](https://arxiv.org/abs/2510.14312)
*Mason Nakamura,Abhinav Kumar,Saaduddin Mahmud,Sahar Abdelnabi,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: LLM驱动的多智能体系统（MAS）可自动化用户任务，但存在安全风险。Terrarium框架通过模块化测试平台研究LLM-MAS的安全性、隐私性和安全性，识别攻击向量并实现攻击场景以加速可信MAS的开发。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统（MAS）虽然能自动化复杂任务，但也引入了如错位、恶意攻击和数据泄露等新风险。

Method: 提出Terrarium框架，该框架基于早期黑板设计，提供了一个模块化、可配置的测试平台，用于研究LLM-MAS在安全性、隐私性和安全性方面的表现。框架实现了三个协作MAS场景和四个代表性攻击，以展示其灵活性。

Result: 通过实现协作MAS场景和攻击，证明了Terrarium框架在评估LLM-MAS的安全性、隐私性和安全性方面的灵活性和有效性。

Conclusion: Terrarium框架为LLM-MAS的安全性、隐私性和安全性研究提供了一个灵活的测试平台，能够快速原型化、评估和迭代防御措施，旨在加速可信MAS的开发。

Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can
automate tedious user tasks such as meeting scheduling that requires
inter-agent collaboration. LLMs enable nuanced protocols that account for
unstructured private data, user constraints, and preferences. However, this
design introduces new risks, including misalignment and attacks by malicious
parties that compromise agents or steal user data. In this paper, we propose
the Terrarium framework for fine-grained study on safety, privacy, and security
in LLM-based MAS. We repurpose the blackboard design, an early approach in
multi-agent systems, to create a modular, configurable testbed for multi-agent
collaboration. We identify key attack vectors such as misalignment, malicious
agents, compromised communication, and data poisoning. We implement three
collaborative MAS scenarios with four representative attacks to demonstrate the
framework's flexibility. By providing tools to rapidly prototype, evaluate, and
iterate on defenses and designs, Terrarium aims to accelerate progress toward
trustworthy multi-agent systems.

</details>


### [395] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC是一个元认知框架，用于检测和纠正大型语言模型多主体系统中的错误，提高了协作问题的解决能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型多主体系统在协作解决问题时虽然表现出色，但容易受到级联错误的影响，单个错误步骤可能会在代理之间传播并扰乱整个过程。

Method: MASC通过“下一步执行重建”和“原型引导增强”两种互补的设计，将错误检测视为历史条件下的异常评分。当检测到异常步骤时，MASC会触发一个纠正代理来修改执行代理的输出。

Result: MASC在Who&When基准测试中，错误检测AUC-ROC提高了8.47%，并且在各种多主体系统框架中都带来了持续的端到端性能提升，证明了其元认知监控和有针对性的纠正能够以最小的开销来减轻错误传播。

Conclusion: MASC通过实时、无监督、步进级的错误检测和自我纠正，增强了大型语言模型多主体系统的鲁棒性，有效解决了级联错误的问题，并在多个任务和框架上验证了其有效性。

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [396] [AI for Service: Proactive Assistance with AI Glasses](https://arxiv.org/abs/2510.14359)
*Zichen Wen,Yiyu Wang,Chenfei Liao,Boxue Yang,Junxian Li,Weifeng Liu,Haocong He,Bolong Feng,Xuyang Liu,Yuanhuiyi Lyu,Xu Zheng,Xuming Hu,Linfeng Zhang*

Main category: cs.AI

TL;DR: AI4Service是一个新范式，通过Alpha-Service框架，利用AI眼镜中的多智能体系统，实现主动、实时的日常协助，解决了何时干预和如何提供服务的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI服务大多是被动的，无法预测和主动满足用户需求，因此需要引入能主动预测用户需求并提供帮助的新范式。

Method: 提出Alpha-Service统一框架，包含输入单元（感知）、中央处理单元（任务调度）、算术逻辑单元（工具利用）、记忆单元（个性化）和输出单元（人机交互）。该框架基于AI眼镜，通过多智能体系统实现。

Result: 通过实际案例（如Blackjack顾问、博物馆导览、购物助理）证明了Alpha-Service能够无缝感知环境、推断用户意图，并在无需明确提示的情况下提供及时有效的帮助。

Conclusion: Alpha-Service框架成功实现了AI4Service范式，能够主动感知用户需求并提供个性化服务，是AI从被动工具向主动智能伴侣转变的重要一步。

Abstract: In an era where AI is evolving from a passive tool into an active and
adaptive companion, we introduce AI for Service (AI4Service), a new paradigm
that enables proactive and real-time assistance in daily life. Existing AI
services remain largely reactive, responding only to explicit user commands. We
argue that a truly intelligent and helpful assistant should be capable of
anticipating user needs and taking actions proactively when appropriate. To
realize this vision, we propose Alpha-Service, a unified framework that
addresses two fundamental challenges: Know When to intervene by detecting
service opportunities from egocentric video streams, and Know How to provide
both generalized and personalized services. Inspired by the von Neumann
computer architecture and based on AI glasses, Alpha-Service consists of five
key components: an Input Unit for perception, a Central Processing Unit for
task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit
for long-term personalization, and an Output Unit for natural human
interaction. As an initial exploration, we implement Alpha-Service through a
multi-agent system deployed on AI glasses. Case studies, including a real-time
Blackjack advisor, a museum tour guide, and a shopping fit assistant,
demonstrate its ability to seamlessly perceive the environment, infer user
intent, and provide timely and useful assistance without explicit prompts.

</details>


### [397] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu,Zihao Zhou,Kaizhu Huang,Xiaowei Huang,Qiufeng Wang*

Main category: cs.AI

TL;DR: IP-Merging是一种无需微调的方法，可以直接提升多模态大语言模型（MLLM）的数学推理能力，通过识别并合并关键的推理相关参数来缩小与数学大语言模型（LLM）的参数空间差距，同时保持MLLM的其他能力。


<details>
  <summary>Details</summary>
Motivation: 大多数在多模态大语言模型（MLLM）上的数学推理研究都集中在高质量标注数据和复杂的训练/推理范式上，而MLLM的数学推理能力仍然落后于仅文本的大语言模型（LLM）。本研究旨在探索是否可以在不进行微调的情况下，让MLLM直接从现成的数学LLM中吸收数学推理能力。

Method: 提出IP-Merging方法，该方法首先识别MLLM和数学LLM中与推理相关的参数，然后将这些参数投影到MLLM的子空间中，以保持对齐，并最终在子空间中合并参数。此方法是无需微调的，因为参数是直接调整的。

Result: 实验证明，IP-Merging方法可以直接从数学LLM中提升MLLM的数学推理能力，同时不会损害其其他能力。

Conclusion: IP-Merging是一种有效的、无需微调的方法，可以通过识别关键的推理相关层并解决参数空间差距来增强MLLM的数学推理能力。

Abstract: Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.

</details>


### [398] [Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control](https://arxiv.org/abs/2510.14388)
*Zhe Wu,Hongjin Lu,Junliang Xing,Changhao Zhang,Yin Zhu,Yuhao Yang,Yuheng Jing,Kai Li,Kun Shao,Jianye Hao,Jun Wang,Yuanchun Shi*

Main category: cs.AI

TL;DR: Hi-Agent是一个新的可训练的、分层的视觉-语言模型，用于移动设备控制。它通过一个高级推理模型和一个低级动作模型来实现，这两个模型可以联合优化。该模型在Android-in-the-Wild基准测试中达到了87.9%的任务成功率，显著优于现有方法。此外，它还展示了在ScreenSpot-v2基准测试中的零样本泛化能力，并在AndroidWorld基准测试中表现出良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉-语言模型（VLMs）的移动设备自主操作方法，大多依赖于直接的状态到动作映射，缺乏结构化推理和规划能力，因此在处理新任务或未见过的用户界面布局时泛化能力较差。

Method: Hi-Agent采用分层结构，包含一个高级推理模型和一个低级动作模型，并进行联合优化。为了提高训练效率，它将多步决策重新构建为一系列单步子目标，并提出了一种前瞻性优势函数，利用低级模型的执行反馈来指导高级优化，从而减轻了GRPO在长时任务中遇到的路径爆炸问题，并实现了稳定的、无判别器的联合训练。

Result: Hi-Agent在Android-in-the-Wild（AitW）基准测试中取得了87.9%的新SOTA任务成功率，显著优于之前的提示（AppAgent: 17.7%）、监督（Filtered BC: 54.5%）和强化学习（DigiRL: 71.9%）方法。它在ScreenSpot-v2基准测试中也表现出具有竞争力的零样本泛化能力。在更具挑战性的AndroidWorld基准测试中，Hi-Agent随着骨干网络规模的增大而有效扩展，在高复杂度移动控制场景中展现出强大的适应性。

Conclusion: Hi-Agent通过其分层结构和创新的训练方法，克服了现有方法的局限性，在移动设备自主操作任务上取得了显著的性能提升和良好的泛化能力。

Abstract: Building agents that autonomously operate mobile devices has attracted
increasing attention. While Vision-Language Models (VLMs) show promise, most
existing approaches rely on direct state-to-action mappings, which lack
structured reasoning and planning, and thus generalize poorly to novel tasks or
unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical
vision-language agent for mobile control, featuring a high-level reasoning
model and a low-level action model that are jointly optimized. For efficient
training, we reformulate multi-step decision-making as a sequence of
single-step subgoals and propose a foresight advantage function, which
leverages execution feedback from the low-level model to guide high-level
optimization. This design alleviates the path explosion issue encountered by
Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables
stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art
(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,
significantly outperforming prior methods across three paradigms: prompt-based
(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement
learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot
generalization on the ScreenSpot-v2 benchmark. On the more challenging
AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,
showing strong adaptability in high-complexity mobile control scenarios.

</details>


### [399] [IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning](https://arxiv.org/abs/2510.14406)
*Xikai Zhang,Bo Wang,Likang Xiao,Yongzhi Li,Quan Chen,Wenju Wu,Liu Liu*

Main category: cs.AI

TL;DR: IMAGINE是一个将多智能体系统（MAS）集成到单个模型中的框架，通过端到端训练，显著提高了复杂推理和规划任务的性能，以Qwen3-8B-Instruct为基础的模型在TravelPlanner基准测试中取得了82.7%的最终通过率，远超现有MAS方法。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在复杂推理和规划方面存在挑战，而MAS虽然能提升推理能力，但存在成本高、延迟长、难以端到端训练等问题。

Method: 提出IMAGINE框架，将MAS的推理和规划能力集成到单个紧凑模型中，并通过简单的端到端训练进行优化。

Result: 基于Qwen3-8B-Instruct并使用IMAGINE方法训练的模型，在TravelPlanner基准测试中取得了82.7%的最终通过率，远高于DeepSeek-R1-671B的40%，同时模型规模更小。

Conclusion: IMAGINE框架成功地将MAS的结构化推理和规划能力集成到单个模型中，并通过端到端训练实现了性能的显著提升，克服了现有MAS方法的局限性。

Abstract: Although large language models (LLMs) have made significant strides across
various tasks, they still face significant challenges in complex reasoning and
planning. For example, even with carefully designed prompts and prior
information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on
the TravelPlanner dataset in the sole-planning mode. Similarly, even in the
thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass
Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent
Systems (MAS) can offer improved collective reasoning, they often suffer from
high reasoning costs due to multi-round internal interactions, long
per-response latency, and difficulties in end-to-end training. To address these
challenges, we propose a general and scalable framework called IMAGINE, short
for Integrating Multi-Agent System into One Model. This framework not only
integrates the reasoning and planning capabilities of MAS into a single,
compact model, but also significantly surpass the capabilities of the MAS
through a simple end-to-end training. Through this pipeline, a single
small-scale model is not only able to acquire the structured reasoning and
planning capabilities of a well-organized MAS but can also significantly
outperform it. Experimental results demonstrate that, when using
Qwen3-8B-Instruct as the base model and training it with our method, the model
achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding
the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.

</details>


### [400] [Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms](https://arxiv.org/abs/2510.14412)
*Claudia Grundke,Gabriele Röger*

Main category: cs.AI

TL;DR: PDDL中的公理可以看作是 Datalog 等数据库查询语言的推广。该标准限制了公理体中谓词的负面出现，但文献中通常会放宽这一限制，只要求公理是可分层的。这两种变体都可以表达与最小不动点逻辑完全相同的查询，这意味着可以消除派生谓词的负面出现。我们提出了相应的转换。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决 PDDL（Planning Domain Definition Language）中公理的定义和应用问题，特别是关于谓词的负面出现限制以及如何消除派生谓词的负面出现。

Method: 提出了一种将 PDDL 公理（特别是涉及派生谓词负面出现的公理）转换为等价形式的转换方法，该方法可以消除派生谓词的负面出现，并确保与最小不动点逻辑的可比性。

Result: 提出了一个具体的转换方法，可以将 PDDL 公理中派生谓词的负面出现消除，使得转换后的公理集仍然能够表达与原公理集完全相同的查询，并且与最小不动点逻辑等价。

Conclusion: PDDL 公理中的派生谓词负面出现可以通过特定的转换方法来消除，而不会损失表达能力。这种转换有助于简化公理定义，并可能提高规划器的效率。

Abstract: Axioms are a feature of the Planning Domain Definition Language PDDL that can
be considered as a generalization of database query languages such as Datalog.
The PDDL standard restricts negative occurrences of predicates in axiom bodies
to predicates that are directly set by actions and not derived by axioms. In
the literature, authors often deviate from this limitation and only require
that the set of axioms is stratifiable. Both variants can express exactly the
same queries as least fixed-point logic, indicating that negative occurrences
of derived predicates can be eliminated. We present the corresponding
transformation.

</details>


### [401] [Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration](https://arxiv.org/abs/2510.14512)
*Haoyuan Li,Mathias Funk,Aaqib Saeed*

Main category: cs.AI

TL;DR: Helmsman是一个多智能体系统，可以自动生成联邦学习系统，并包含一个评估系统设计能力的基准。


<details>
  <summary>Details</summary>
Motivation: 设计和部署强大的联邦学习（FL）系统非常复杂，选择、组合和调整策略以应对数据异质性和系统约束等挑战是一个关键瓶颈，导致解决方案脆弱且定制化。

Method: Helmsman通过三个协作阶段来自动化端到端的FL系统合成：1. 人机交互的规划，2. 监督智能体团队进行模块化代码生成，3. 在沙盒模拟环境中进行自主评估和精炼。

Result: Helmsman生成的解决方案在性能上具有竞争力，并且通常优于手动设计的基线。AgentFL-Bench基准包含16个多样化任务，用于评估智能体系统的FL系统级生成能力。

Conclusion: Helmsman在自动化工程复杂分布式人工智能系统方面取得了重大进展。

Abstract: Federated Learning (FL) offers a powerful paradigm for training models on
decentralized data, but its promise is often undermined by the immense
complexity of designing and deploying robust systems. The need to select,
combine, and tune strategies for multifaceted challenges like data
heterogeneity and system constraints has become a critical bottleneck,
resulting in brittle, bespoke solutions. To address this, we introduce
Helmsman, a novel multi-agent system that automates the end-to-end synthesis of
federated learning systems from high-level user specifications. It emulates a
principled research and development workflow through three collaborative
phases: (1) interactive human-in-the-loop planning to formulate a sound
research plan, (2) modular code generation by supervised agent teams, and (3) a
closed-loop of autonomous evaluation and refinement in a sandboxed simulation
environment. To facilitate rigorous evaluation, we also introduce
AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess
the system-level generation capabilities of agentic systems in FL. Extensive
experiments demonstrate that our approach generates solutions competitive with,
and often superior to, established hand-crafted baselines. Our work represents
a significant step towards the automated engineering of complex decentralized
AI systems.

</details>


### [402] [JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol](https://arxiv.org/abs/2510.14537)
*Emanuele Antonioni,Stefan Markovic,Anirudha Shankar,Jaime Bernardo,Lovro Markovic,Silvia Pareti,Benedetto Proietti*

Main category: cs.AI

TL;DR: JSPLIT是一个用于管理大型语言模型（LLM）提示大小的框架，通过将工具组织成层次化分类，并根据用户提示仅选择最相关的工具，从而解决提示膨胀问题，降低成本并提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统和用户期望的不断发展，LLM需要与外部工具进行交互，但由此产生的提示膨胀问题导致成本增加、延迟提高和任务成功率下降。

Method: JSPLIT框架通过构建工具的层次化分类，并设计一种工具选择算法，根据用户提示和分类结构来识别并仅包含最相关的工具，从而有效管理提示大小。

Result: JSPLIT显著减小了提示的大小，同时并未严重影响LLM的响应能力。在工具数量大幅增加的情况下，JSPLIT甚至提高了工具选择的准确性，降低了成本并提高了高复杂度环境下的任务成功率。

Conclusion: JSPLIT是一个有效的框架，能够解决LLM在与大量外部工具交互时面临的提示膨胀问题，通过智能工具选择来优化性能和成本效益。

Abstract: AI systems are continually evolving and advancing, and user expectations are
concurrently increasing, with a growing demand for interactions that go beyond
simple text-based interaction with Large Language Models (LLMs). Today's
applications often require LLMs to interact with external tools, marking a
shift toward more complex agentic systems. To support this, standards such as
the Model Context Protocol (MCP) have emerged, enabling agents to access tools
by including a specification of the capabilities of each tool within the
prompt. Although this approach expands what agents can do, it also introduces a
growing problem: prompt bloating. As the number of tools increases, the prompts
become longer, leading to high prompt token costs, increased latency, and
reduced task success resulting from the selection of tools irrelevant to the
prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework
designed to help agents manage prompt size more effectively when using large
sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and
uses the user's prompt to identify and include only the most relevant tools,
based on both the query and the taxonomy structure. In this paper, we describe
the design of the taxonomy, the tool selection algorithm, and the dataset used
to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt
size without significantly compromising the agent's ability to respond
effectively. As the number of available tools for the agent grows
substantially, JSPLIT even improves the tool selection accuracy of the agent,
effectively reducing costs while simultaneously improving task success in
high-complexity agent environments.

</details>


### [403] [Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts](https://arxiv.org/abs/2510.14538)
*Emanuele Marconato,Samuele Bortolotti,Emile van Krieken,Paolo Morettin,Elena Umili,Antonio Vergari,Efthymia Tsamoura,Andrea Passerini,Stefano Teso*

Main category: cs.AI

TL;DR: 神经符号（NeSy）AI 结合了深度学习和符号推理，但可能存在“推理捷径”（RSs）问题，即模型可能因概念提取错误而获得高准确率，影响可解释性和泛化能力。该综述介绍了 RSs 的概念、原因、后果，并梳理了相关理论、检测和缓解方法，旨在降低研究门槛，促进可靠的 NeSy 和可信 AI 模型的发展。


<details>
  <summary>Details</summary>
Motivation: 神经符号（NeSy）AI 旨在构建符合安全或结构等先验知识的深度学习模型，是实现可靠、可信 AI 的有前景的方向。然而，当概念不直接受监督时，NeSy 模型可能出现“推理捷径”（RSs）问题，即通过错误的概念关联获得高准确率，从而影响模型的可解释性、泛化能力和可靠性。RSs 难以检测和预防，且相关研究分散，增加了研究者和实践者理解和解决该问题的难度。

Method: 本文对神经符号（NeSy）AI 中的“推理捷径”（RSs）问题进行了全面的综述。首先，文章介绍了 RSs 的基本概念，解释了其产生的原因和可能带来的负面影响。接着，文章回顾并阐述了现有的关于 RSs 的理论化描述。最后，文章详细介绍了用于应对 RSs 的各种策略，包括缓解和提高意识的方法，并分析了这些方法的优缺点。通过将复杂的现有研究内容以易于理解的方式重新表述，本文旨在为 RSs 提供一个统一的视角，降低相关研究的入门门槛。

Result: 本文对神经符号（NeSy）AI 中的“推理捷径”（RSs）问题进行了全面的梳理和总结。文章对 RSs 的概念、成因、影响进行了清晰的阐述，回顾了相关的理论研究，并详细介绍了检测和缓解 RSs 的多种方法及其局限性。通过提供一个易于理解的统一视角，旨在降低相关研究的入门门槛。

Conclusion: 本文旨在通过提供一个关于神经符号（NeSy）AI 中“推理捷径”（RSs）问题的清晰、易于理解的综述，降低相关研究的入门门槛，促进对该问题的深入理解和有效解决，最终推动更可靠的 NeSy 和可信 AI 模型的开发。

Abstract: Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose
predictions comply with prior knowledge encoding, e.g. safety or structural
constraints. As such, it represents one of the most promising avenues for
reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural
and symbolic steps: neural networks are typically responsible for mapping
low-level inputs into high-level symbolic concepts, while symbolic reasoning
infers predictions compatible with the extracted concepts and the prior
knowledge. Despite their promise, it was recently shown that - whenever the
concepts are not supervised directly - NeSy models can be affected by Reasoning
Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the
concepts incorrectly. RSs can compromise the interpretability of the model's
explanations, performance in out-of-distribution scenarios, and therefore
reliability. At the same time, RSs are difficult to detect and prevent unless
concept supervision is available, which is typically not the case. However, the
literature on RSs is scattered, making it difficult for researchers and
practitioners to understand and tackle this challenging problem. This overview
addresses this issue by providing a gentle introduction to RSs, discussing
their causes and consequences in intuitive terms. It also reviews and
elucidates existing theoretical characterizations of this phenomenon. Finally,
it details methods for dealing with RSs, including mitigation and awareness
strategies, and maps their benefits and limitations. By reformulating advanced
material in a digestible form, this overview aims to provide a unifying
perspective on RSs to lower the bar to entry for tackling them. Ultimately, we
hope this overview contributes to the development of reliable NeSy and
trustworthy AI models.

</details>


### [404] [LLM Agents Beyond Utility: An Open-Ended Perspective](https://arxiv.org/abs/2510.14548)
*Asen Nachkov,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: LLM代理通过生成自己的任务、积累知识和与环境广泛互动，可以发展成独立的实体，但仍面临提示设计敏感、任务重复和缺乏自我表征的挑战。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代理是否能超越智能工具，成为能够规划、设计任务并朝着更广泛、更模糊目标进行推理的独立实体。

Method: 在一个开放的实验环境中，增强了预训练LLM代理生成自身任务、积累知识和与环境广泛互动 else.

Result: LLM代理能够可靠地遵循复杂的多步指令，跨运行存储和重用信息，并提出和解决自己的任务。然而，它仍然对提示设计敏感，容易产生重复性任务，并且无法形成自我表征。

Conclusion: 经过训练的LLM在实现开放性方面既有潜力也有局限性，未来的研究应侧重于改进记忆管理、提高探索效率以及实现抽象的长期目标。

Abstract: Recent LLM agents have made great use of chain of thought reasoning and
function calling. As their capabilities grow, an important question arises: can
this software represent not only a smart problem-solving tool, but an entity in
its own right, that can plan, design immediate tasks, and reason toward
broader, more ambiguous goals? To study this question, we adopt an open-ended
experimental setting where we augment a pretrained LLM agent with the ability
to generate its own tasks, accumulate knowledge, and interact extensively with
its environment. We study the resulting open-ended agent qualitatively. It can
reliably follow complex multi-step instructions, store and reuse information
across runs, and propose and solve its own tasks, though it remains sensitive
to prompt design, prone to repetitive task generation, and unable to form
self-representations. These findings illustrate both the promise and current
limits of adapting pretrained LLMs toward open-endedness, and point to future
directions for training agents to manage memory, explore productively, and
pursue abstract long-term goals.

</details>


### [405] [ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks](https://arxiv.org/abs/2510.14621)
*Yuanyi Song,Heyuan Huang,Qiqiang Lin,Yin Zhao,Xiangmou Qu,Jun Wang,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang,Zhaoxiang Wang*

Main category: cs.AI

TL;DR: 本论文提出了一种名为ColorBench的新型图结构基准测试框架，用于评估多模态大语言模型驱动的移动自动化代理。该框架通过模拟真实设备交互中的有限状态，实现了动态行为的静态模拟，解决了现有基准测试方法（如离线静态基准和在线动态测试）的不足。ColorBench支持多解、长周期任务的评估，包含175个任务，每个任务有多个正确路径和多种错误路径，能够进行子任务完成率和原子能力分析。


<details>
  <summary>Details</summary>
Motivation: 现有移动自动化代理的评估标准（离线静态基准和在线动态测试）无法充分评估复杂长周期任务的能力，前者过于死板，后者受限于真实设备的不稳定性和不可复现性。

Method: 提出一种新的图结构基准测试框架，通过对真实设备交互中的有限状态进行建模，实现动态行为的静态模拟。在此基础上，开发了ColorBench基准测试，包含175个（74个单应用，101个跨应用）长周期任务，平均长度超过13步。每个任务至少包含两条正确路径和若干典型错误路径，支持多解、子任务完成率统计和原子级别能力分析。

Result: 通过在ColorBench上评估现有基线模型，发现了现有模型的局限性，并根据实验结果提出了改进方向和技术路径。

Conclusion: ColorBench能够更全面、稳定地评估移动自动化代理，尤其是在复杂长周期任务方面。评估结果揭示了现有模型的不足，并为未来研究提供了改进方向。

Abstract: The rapid advancement of multimodal large language models has enabled agents
to operate mobile devices by directly interacting with graphical user
interfaces, opening new possibilities for mobile automation. However,
real-world mobile tasks are often complex and allow for multiple valid
solutions. This contradicts current mobile agent evaluation standards: offline
static benchmarks can only validate a single predefined "golden path", while
online dynamic testing is constrained by the complexity and non-reproducibility
of real devices, making both approaches inadequate for comprehensively
assessing agent capabilities. To bridge the gap between offline and online
evaluation and enhance testing stability, this paper introduces a novel
graph-structured benchmarking framework. By modeling the finite states observed
during real-device interactions, it achieves static simulation of dynamic
behaviors. Building on this, we develop ColorBench, a benchmark focused on
complex long-horizon tasks. It supports evaluation of multiple valid solutions,
subtask completion rate statistics, and atomic-level capability analysis.
ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average
length of over 13 steps. Each task includes at least two correct paths and
several typical error paths, enabling quasi-dynamic interaction. By evaluating
ColorBench across various baselines, we discover limitations of existing models
and propose improvement directions and feasible technical pathways to enhance
agents' performance on complex, long-horizon problems based on experimental
results. Code and data are available at:
https://github.com/MadeAgents/ColorBench.

</details>


### [406] [Beyond Hallucinations: The Illusion of Understanding in Large Language Models](https://arxiv.org/abs/2510.14665)
*Rikard Rosenbacke,Carl Rosenbacke,Victor Rosenbacke,Martin McKee*

Main category: cs.AI

TL;DR: LLMs are statistical predictors, not reasoners, leading to hallucinations. This paper introduces the Rose-Frame, a 3D diagnostic tool (Map vs. Territory, Intuition vs. Reason, Conflict vs. Confirmation) to identify cognitive and epistemic drift in human-AI interaction. The frame promotes transparency and critical awareness, reframing alignment as cognitive governance where human reason must govern AI intuition.


<details>
  <summary>Details</summary>
Motivation: LLMs, despite their fluency, lack grounded reasoning and risk hallucination due to their statistical nature, mirroring human intuition (System 1 cognition).

Method: Introduced the Rose-Frame, a three-dimensional framework with axes: (i) Map vs. Territory, (ii) Intuition vs. Reason, and (iii) Conflict vs. Confirmation, to diagnose cognitive and epistemic drift in human-AI interaction.

Result: The Rose-Frame provides a reflective tool to make LLM limitations and user assumptions visible, enabling more transparent and critically aware AI deployment.

Conclusion: Alignment is reframed as cognitive governance; human reason must govern AI intuition, requiring reflective, falsifiable oversight to align machine fluency with human understanding.

Abstract: Large language models (LLMs) are becoming deeply embedded in human
communication and decision-making, yet they inherit the ambiguity, bias, and
lack of direct access to truth inherent in language itself. While their outputs
are fluent, emotionally resonant, and coherent, they are generated through
statistical prediction rather than grounded reasoning. This creates the risk of
hallucination, responses that sound convincing but lack factual validity.
Building on Geoffrey Hinton's observation that AI mirrors human intuition
rather than reasoning, this paper argues that LLMs operationalize System 1
cognition at scale: fast, associative, and persuasive, but without reflection
or falsification. To address this, we introduce the Rose-Frame, a
three-dimensional framework for diagnosing cognitive and epistemic drift in
human-AI interaction. The three axes are: (i) Map vs. Territory, which
distinguishes representations of reality (epistemology) from reality itself
(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to
separate fast, emotional judgments from slow, reflective thinking; and (iii)
Conflict vs. Confirmation, which examines whether ideas are critically tested
through disagreement or simply reinforced through mutual validation. Each
dimension captures a distinct failure mode, and their combination amplifies
misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.
Instead, it offers a reflective tool that makes both the model's limitations
and the user's assumptions visible, enabling more transparent and critically
aware AI deployment. It reframes alignment as cognitive governance: intuition,
whether human or artificial, must remain governed by human reason. Only by
embedding reflective, falsifiable oversight can we align machine fluency with
human understanding.

</details>


### [407] [Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review](https://arxiv.org/abs/2510.14669)
*Sara Altamirano,Arjan Vreeken,Sennay Ghebreab*

Main category: cs.AI

TL;DR: 机器学习在公共卫生领域的应用存在算法偏倚的风险，本研究通过系统性文献综述，评估了荷兰公共卫生领域机器学习研究中算法偏倚的识别、讨论和报告情况，并提出了一个名为ACAR的公平性框架和相关建议。


<details>
  <summary>Details</summary>
Motivation: 机器学习在公共卫生领域潜力巨大，但若不关注算法偏倚，可能加剧健康不平等。本研究旨在评估荷兰公共卫生领域机器学习研究中算法偏倚的状况。

Method: 通过开发“算法偏倚风险评估工具”（RABAT），整合了Cochrane Risk of Bias、PROBAST和Microsoft Responsible AI checklist等既有框架，并将其应用于2021-2025年间35篇荷兰公共卫生领域的机器学习研究文献，以识别、讨论和报告算法偏倚。

Result: 研究发现，尽管数据采样和缺失数据方面有良好记录，但大多数研究省略了明确的公平性阐述、亚组分析和潜在危害的透明讨论。现有研究在算法偏倚的识别、讨论和报告方面存在普遍性差距。

Conclusion: 为了解决算法偏倚问题，本研究提出了一个名为ACAR（意识、概念化、应用、报告）的四阶段公平性框架，并提供具体指导问题，以帮助研究人员在机器学习生命周期的各个阶段处理公平性问题。研究最后为公共卫生机器学习从业者提供了切实可行的建议，以确保算法创新促进健康公平而非损害健康公平。

Abstract: Machine learning (ML) promises to revolutionize public health through
improved surveillance, risk stratification, and resource allocation. However,
without systematic attention to algorithmic bias, ML may inadvertently
reinforce existing health disparities. We present a systematic literature
review of algorithmic bias identification, discussion, and reporting in Dutch
public health ML research from 2021 to 2025. To this end, we developed the Risk
of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from
established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible
AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals
pervasive gaps: although data sampling and missing data practices are well
documented, most studies omit explicit fairness framing, subgroup analyses, and
transparent discussion of potential harms. In response, we introduce a
four-stage fairness-oriented framework called ACAR (Awareness,
Conceptualization, Application, Reporting), with guiding questions derived from
our systematic literature review to help researchers address fairness across
the ML lifecycle. We conclude with actionable recommendations for public health
ML practitioners to consistently consider algorithmic bias and foster
transparency, ensuring that algorithmic innovations advance health equity
rather than undermine it.

</details>


### [408] [TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence](https://arxiv.org/abs/2510.14670)
*Marco Simoni,Aleksandar Fontana,Andrea Saracino,Paolo Mori*

Main category: cs.AI

TL;DR: TITAN是一个连接自然语言网络威胁查询和可执行推理的框架，通过路径规划模型和图执行器在结构化知识图上进行操作，并提供了一个包含88209个示例的数据集用于训练和评估。


<details>
  <summary>Details</summary>
Motivation: 连接自然语言网络威胁查询和可执行推理，以自动化地处理网络威胁情报。

Method: 集成了一个路径规划模型来从文本中预测逻辑关系链，以及一个图执行器来遍历TITAN本体以检索事实答案和支持证据。该框架在由MITRE衍生的类型化、双向图上操作，允许在威胁、行为和防御之间进行清晰可逆的推理。

Result: TITAN使模型能够生成语法上有效且语义上连贯的推理路径，这些路径可以在底层图上进行确定性执行。

Conclusion: TITAN框架能够有效地将自然语言威胁查询转化为可执行的推理，并在结构化知识图上检索相关信息，其性能通过大规模数据集和实证评估得到验证。

Abstract: TITAN (Threat Intelligence Through Automated Navigation) is a framework that
connects natural-language cyber threat queries with executable reasoning over a
structured knowledge graph. It integrates a path planner model, which predicts
logical relation chains from text, and a graph executor that traverses the
TITAN Ontology to retrieve factual answers and supporting evidence. Unlike
traditional retrieval systems, TITAN operates on a typed, bidirectional graph
derived from MITRE, allowing reasoning to move clearly and reversibly between
threats, behaviors, and defenses. To support training and evaluation, we
introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:
13951) pairing natural language questions with executable reasoning paths and
step by step Chain of Thought explanations. Empirical evaluations show that
TITAN enables models to generate syntactically valid and semantically coherent
reasoning paths that can be deterministically executed on the underlying graph.

</details>


### [409] [NAEL: Non-Anthropocentric Ethical Logic](https://arxiv.org/abs/2510.14676)
*Bianca Maria Lerma,Rafael Peñaloza*

Main category: cs.AI

TL;DR: NAEL是一个基于主动推理和符号推理的非人类中心伦理框架，用于人工智能代理，它将伦理行为形式化为智能系统在动态、多主体环境中最小化全局预期自由能的涌现属性。


<details>
  <summary>Details</summary>
Motivation: 现有AI伦理模型存在局限性，依赖于拟人化的道德直觉，无法实现情境敏感、自适应和关系性的伦理行为。NAEL旨在克服这些限制，提出一种非人类中心的伦理框架。

Method: NAEL将伦理行为形式化为智能系统在动态、多主体环境中最小化全局预期自由能的涌现属性。采用神经符号架构，使代理能够在不确定的环境中评估其行为的伦理后果。

Result: 通过在伦理资源分配中的案例研究，说明了NAEL如何在自我保护、认知学习和集体福利之间进行动态平衡。

Conclusion: NAEL提供了一种新颖的AI伦理方法，通过主动推理和神经符号架构，使代理能够发展出适应性强、非拟人化的伦理行为，并在复杂环境中平衡多种利益相关者。

Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical
framework for artificial agents grounded in active inference and symbolic
reasoning. Departing from conventional, human-centred approaches to AI ethics,
NAEL formalizes ethical behaviour as an emergent property of intelligent
systems minimizing global expected free energy in dynamic, multi-agent
environments. We propose a neuro-symbolic architecture to allow agents to
evaluate the ethical consequences of their actions in uncertain settings. The
proposed system addresses the limitations of existing ethical models by
allowing agents to develop context-sensitive, adaptive, and relational ethical
behaviour without presupposing anthropomorphic moral intuitions. A case study
involving ethical resource distribution illustrates NAEL's dynamic balancing of
self-preservation, epistemic learning, and collective welfare.

</details>


### [410] [Practical, Utilitarian Algorithm Configuration](https://arxiv.org/abs/2510.14683)
*Devon Graham,Kevin Leyton-Brown*

Main category: cs.AI

TL;DR: 本文提出了一系列改进，用于优化COUP算法配置程序的实际性能，使其在不影响理论保证的前提下，能够与不提供性能保证的启发式配置程序竞争，并通过案例研究说明了如何探索给定算法选择问题的解决方案对效用函数变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 用户效用最大化的目标，以及对COUP算法配置程序在实际性能上的不足的关注。

Method: 提出了一系列改进COUP算法配置程序的措施，并通过实验进行了验证，同时以案例研究的形式说明了如何探索算法选择问题解决方案对效用函数变化的鲁棒性。

Result: 改进后的COUP算法配置程序在实际性能上具有竞争力，并且不影响其理论保证。

Conclusion: 改进后的COUP算法配置程序在实际应用中是有效的，并且能够处理效用函数的变化。

Abstract: Utilitarian algorithm configuration identifies a parameter setting for a
given algorithm that maximizes a user's utility. Utility functions offer a
theoretically well-grounded approach to optimizing decision-making under
uncertainty and are flexible enough to capture a user's preferences over
algorithm runtimes (e.g., they can describe a sharp cutoff after which a
solution is no longer required, a per-hour cost for compute, or diminishing
returns from algorithms that take longer to run). COUP is a recently-introduced
utilitarian algorithm configuration procedure which was designed mainly to
offer strong theoretical guarantees about the quality of the configuration it
returns, with less attention paid to its practical performance. This paper
closes that gap, bringing theoretically-grounded, utilitarian algorithm
configuration to the point where it is competitive with widely used, heuristic
configuration procedures that offer no performance guarantees. We present a
series of improvements to COUP that improve its empirical performance without
degrading its theoretical guarantees and demonstrate their benefit
experimentally. Using a case study, we also illustrate ways of exploring the
robustness of a given solution to the algorithm selection problem to variations
in the utility function.

</details>


### [411] [Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging](https://arxiv.org/abs/2510.14697)
*Bang An,Yibo Yang,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: 通过在知识感知子空间中净化任务向量来解决模型合并中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有的模型合并方法由于任务向量中的任务不相关冗余而导致性能下降，而现有的去除冗余的方法缺乏知识感知能力。

Method: 提出了一种名为PAVE（Purifying TAsk Vectors）的方法，该方法通过对任务向量进行净化来解决模型合并中的性能下降问题。PAVE通过采样训练示例，获取微调模型在线性层之前的协方差矩阵，然后进行面向上下文的奇异值分解，以区分任务相关和冗余的组件，并修剪冗余组件。此外，还引入了一种通过优化归一化激活剪枝误差的谱秩分配策略，以实现跨模型的公平剪枝。

Result: PAVE作为一种即插即用方案，可以应用于各种基于任务向量的合并方法，以提高其性能。实验证明了PAVE在各种合并方法、任务和模型架构中的有效性。

Conclusion: PAVE通过在知识感知子空间中净化任务向量，有效地解决了模型合并中由任务不相关冗余引起的性能下降问题，并且可以作为一种即插即用方案应用于现有的合并方法中。

Abstract: Model merging aims to integrate task-specific abilities from individually
fine-tuned models into a single model without extra training. In recent model
merging methods, task vector has become a fundamental building block, as it can
encapsulate the residual information from finetuning. However, the merged model
often suffers from notable performance degradation due to the conflicts caused
by task-irrelevant redundancy in task vectors. Existing efforts in overcoming
redundancy by randomly dropping elements in the parameter space involves
randomness and lacks knowledge awareness. To address these challenges, in this
study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.
Concretely, we sample some training examples from each task, and feed them into
their corresponding fine-tuned models to acquire the covariance matrices before
linear layers. We then perform a context-oriented singular value decomposition,
which accentuates the weight components most relevant to the target knowledge.
As a result, we can split fine-tuned model weights into task-relevant and
redundant components in the knowledge-aware subspace, and purify the task
vector by pruning the redundant components. To induce fair pruning efforts
across models, we further introduce a spectral rank allocation strategy by
optimizing a normalized activated pruning error. The task vector purification
by our method as a plug-and-play scheme is applicable across various task
vector-based merging methods to improve their performance. In experiments, we
demonstrate the effectiveness of PAVE across a diverse set of merging methods,
tasks, and model architectures.

</details>


### [412] [Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction](https://arxiv.org/abs/2510.14702)
*Penglong Zhai,Jie Li,Fanyi Di,Yue Liu,Yifang Yuan,Jie Huang,Peng Wu,Sicong Wang,Mingyang Yin,Tingting Hu,Yao Xu,Xin Li*

Main category: cs.AI

TL;DR: CoAST框架通过结合地理知识、时空轨迹模式、用户画像和情境信息，利用自然语言作为接口，解决了现有大语言模型在下一个兴趣点推荐任务中缺乏对结构化地理实体和序列移动模式的理解的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）在下一个兴趣点（POI）推荐任务中，虽然能以生成方式处理，但缺乏对结构化地理实体和序列移动模式的原生理解，并且难以整合常识知识（如季节、天气、节假日）和用户画像信息（如习惯、职业、偏好）以提升用户体验和推荐性能。

Method: CoAST框架包含两个主要阶段：1. 通过对脱敏用户的时空轨迹数据进行持续预训练，获取推荐知识；2. 利用包含丰富信息的训练数据，通过监督微调（SFT）和随后的强化学习（RL）阶段，实现认知对齐，使推荐结果与人类偏好保持一致。

Result: 在真实世界数据集上进行的广泛离线实验，以及在AMAP App首页“猜你去哪”中进行的在线实验，均证明了CoAST的有效性。

Conclusion: CoAST框架通过其创新的两阶段方法，有效解决了现有LLM在POI推荐中的局限性，并通过整合多维度信息实现了与人类认知和偏好的对齐，显著提升了推荐效果。

Abstract: The next point-of-interest (POI) recommendation task aims to predict the
users' immediate next destinations based on their preferences and historical
check-ins, holding significant value in location-based services. Recently,
large language models (LLMs) have shown great potential in recommender systems,
which treat the next POI prediction in a generative manner. However, these
LLMs, pretrained primarily on vast corpora of unstructured text, lack the
native understanding of structured geographical entities and sequential
mobility patterns required for next POI prediction tasks. Moreover, in
industrial-scale POI prediction applications, incorporating world knowledge and
alignment of human cognition, such as seasons, weather conditions, holidays,
and users' profiles (such as habits, occupation, and preferences), can enhance
the user experience while improving recommendation performance. To address
these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a
framework employing natural language as an interface, allowing for the
incorporation of world knowledge, spatio-temporal trajectory patterns,
profiles, and situational information. Specifically, CoAST mainly comprises of
2 stages: (1) Recommendation Knowledge Acquisition through continued
pretraining on the enriched spatial-temporal trajectory data of the
desensitized users; (2) Cognitive Alignment to align cognitive judgments with
human preferences using enriched training data through Supervised Fine-Tuning
(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline
experiments on various real-world datasets and online experiments deployed in
"Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of
CoAST.

</details>


### [413] [ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning](https://arxiv.org/abs/2509.26255)
*Yichao Liang,Dat Nguyen,Cambridge Yang,Tianyang Li,Joshua B. Tenenbaum,Carl Edward Rasmussen,Adrian Weller,Zenna Tavares,Tom Silver,Kevin Ellis*

Main category: cs.AI

TL;DR: We propose a framework for abstract world models that learns symbolic state representations and causal processes for both endogenous actions and exogenous mechanisms, enabling fast and generalizable planning in long-horizon embodied tasks.


<details>
  <summary>Details</summary>
Motivation: Long-horizon embodied planning is challenging due to concurrent exogenous processes unfolding alongside agent actions.

Method: The framework jointly learns symbolic state representations and causal processes (modeling stochastic cause-effect relations) for endogenous and exogenous mechanisms. Learning utilizes variational Bayesian inference and LLM proposals from limited data.

Result: The learned models enable fast planning that generalizes to held-out tasks with more objects and complex goals, outperforming baselines across five simulated tabletop robotics environments.

Conclusion: The proposed framework effectively addresses the challenges of long-horizon embodied planning by learning abstract world models that capture both agent actions and concurrent exogenous processes.

Abstract: Long-horizon embodied planning is challenging because the world does not only
change through an agent's actions: exogenous processes (e.g., water heating,
dominoes cascading) unfold concurrently with the agent's actions. We propose a
framework for abstract world models that jointly learns (i) symbolic state
representations and (ii) causal processes for both endogenous actions and
exogenous mechanisms. Each causal process models the time course of a
stochastic cause-effect relation. We learn these world models from limited data
via variational Bayesian inference combined with LLM proposals. Across five
simulated tabletop robotics environments, the learned models enable fast
planning that generalizes to held-out tasks with more objects and more complex
goals, outperforming a range of baselines.

</details>


### [414] [ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling](https://arxiv.org/abs/2510.14703)
*Jianghao Lin,Yuanyuan Shi,Xin Peng,Renjie Ding,Hairui Wang,Yuxuan Peng,Bizhe Bai,Weixi Song,Fengshuo Bai,Huacan Chai,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.AI

TL;DR: LLMs在函数调用方面取得进展，并提出一种新的推理扩展框架ToolPRM，通过细粒度的束搜索和过程奖励模型来提高LLMs在结构化输出生成方面的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的推理扩展研究主要集中在非结构化输出生成任务，而对函数调用等结构化输出的应用探索不足。

Method: 提出一种结合细粒度束搜索和过程奖励模型ToolPRM的推理扩展框架，ToolPRM通过评估函数调用内部步骤来打分，并构建了首个细粒度的链式过程监督数据集来训练ToolPRM。

Result: ToolPRM在预测准确性上优于粗粒度和结果奖励模型，表明其在监督函数调用推理过程方面能力更强。结合ToolPRM的推理扩展技术显著提高了骨干模型在各种函数调用任务和基准测试中的性能。

Conclusion: 揭示了将推理扩展技术应用于结构化输出的一个关键原则：“多探索但少保留”，这是因为结构化函数调用生成具有不可恢复性。

Abstract: Large language models (LLMs) are increasingly demonstrating strong
capabilities as autonomous agents, with function calling serving as a core
mechanism for interaction with the environment. Meanwhile, inference scaling
has become a cutting-edge technique to enhance LLM performance by allocating
more computational resources during the inference process. However, current
research on inference scaling primarily focuses on unstructured output
generation tasks, leaving its application in structured outputs, like function
calling, largely underexplored. To bridge this gap, we propose an inference
scaling framework that combines fine-grained beam search with a process reward
model, ToolPRM, which scores the internal steps of each single function call.
To train ToolPRM, we construct the first fine-grained intra-call process
supervision dataset, automatically annotated with function-masking techniques
to provide step-level rewards for structured tool-use reasoning. Extensive
experiments demonstrate that ToolPRM beats the coarse-grained and outcome
reward models in terms of predictive accuracy, indicating its stronger
capability in supervising the function calling inference process. Inference
scaling technique equipped with ToolPRM also significantly improves the
backbone model performance across various function calling tasks and
benchmarks. More importantly, we reveal a key principle for applying inference
scaling techniques to structured outputs: "explore more but retain less" due to
the unrecoverability characteristics of structured function calling generation.

</details>


### [415] [SimKO: Simple Pass@K Policy Optimization](https://arxiv.org/abs/2510.14807)
*Ruotian Peng,Yi Ren,Zhouliang Yu,Weiyang Liu,Yandong Wen*

Main category: cs.AI

TL;DR: RLVR方法倾向于利用而非探索，导致pass@1提高但pass@K（K>1）下降。SimKO通过不对称地优化高熵token的概率来解决这个问题，从而提高pass@K。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法存在偏向利用而非探索的问题，这在pass@K（K>1）指标上表现尤为明显。我们需要理解并解决这个问题以提升LLM的推理能力。

Method: 通过分析RLVR方法在token级别上的概率分布，发现存在概率集中的问题，并提出Simple Pass@K Optimization (SimKO)方法。SimKO对正确响应的top-K候选进行概率提升，对错误响应的top-1候选进行更强的惩罚，尤其是在高熵token上进行干预，以鼓励探索。

Result: SimKO方法在多个数学和逻辑推理基准测试中，显著提高了不同K值下的pass@K性能，有效缓解了概率过度集中的问题。

Conclusion: SimKO是一种简单有效的方法，可以改善RLVR方法的探索能力，并提高其在pass@K指标上的表现。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K>1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR's exploration.

</details>


### [416] [RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning](https://arxiv.org/abs/2510.14828)
*Jinrui Liu,Bingyan Nie,Boyu Li,Yaran Chen,Yuze Wang,Shunsen He,Haoran Li*

Main category: cs.AI

TL;DR: RoboGPT-R1是一个两阶段的框架，通过监督学习和强化学习来提高具身代理的推理能力，使其在复杂环境中执行长时程操控任务。该框架在EmbodiedBench基准测试中表现优于GPT-4o-mini和其他模型。


<details>
  <summary>Details</summary>
Motivation: 目前的监督微调（SFT）方法在具身代理的长期操控任务中存在常识和推理能力受限、泛化性差以及物理理解不足等问题。

Method: 提出RoboGPT-R1两阶段微调框架：第一阶段为监督学习，获取专家序列的基础知识；第二阶段为强化学习（RL），解决视觉空间理解和推理的不足。设计了基于规则的奖励函数，同时考虑长时程性能和环境动作约束。

Result: 在EmbodiedBench基准测试中，在Qwen2.5-VL-3B上训练的RoboGPT-R1比在Qwen2.5-VL-7B上训练的其他模型高出20.33%，比GPT-4o-mini高出21.33%。

Conclusion: RoboGPT-R1框架通过结合监督学习和强化学习，并采用新颖的奖励函数，显著提高了具身代理在复杂环境下的长时程操控任务的推理能力和性能。

Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots
to complete complex human instructions in long-view manipulation tasks
successfully. Despite the success of large language models and vision language
models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
facing challenges in performing long-horizon manipulation tasks in complex
real-world environments, owing to their restricted common sense and reasoning
capabilities. Considering that aligning general-purpose vision language models
to robotic planning tasks via supervised fine-tuning suffers from poor
generalization and insufficient physical understanding, we propose RoboGPT-R1,
a two-stage fine-tuning framework for embodied planning. In this framework,
supervised training acquires foundational knowledge through expert sequences,
followed by RL to address the model's shortcomings in visual-spatial
understanding and reasoning. To achieve physical understanding and action
sequence consistency in multi-step reasoning tasks, we design a rule-based
reward function that simultaneously considers long-horizon performance and
action constraint in the environment. The reasoning model, trained on
Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
EmbodiedBench benchmark.

</details>


### [417] [Agentic NL2SQL to Reduce Computational Costs](https://arxiv.org/abs/2510.14808)
*Dominik Jehle,Lennart Purucker,Frank Hutter*

Main category: cs.AI

TL;DR: Datalake Agent是一个智能体系统，通过交互式循环和选择性信息检索，使用LLM更高效地解决自然语言到SQL（NL2SQL）的任务，显著减少了tokens使用量和成本，同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言到SQL（NL2SQL）任务中表现出色，但处理大量数据库元信息会导致提示过长和成本高昂。

Method: Datalake Agent采用交互式循环，LLM在其中进行推理，只选择性地请求解决表问题所需的信息，而不是一次性将所有元信息放入提示中。

Result: 在23个数据库和100个表问题回答任务的测试中，Datalake Agent将LLM使用的tokens减少了高达87%，从而实现了显著的成本节约，并保持了有竞争力的性能。

Conclusion: Datalake Agent通过其交互式方法，能够更经济高效地利用LLM处理NL2SQL任务，为处理大型数据库的元信息提供了一种有效的解决方案。

Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)
has recently been empowered by large language models (LLMs). Using LLMs to
perform NL2SQL methods on a large collection of SQL databases necessitates
processing large quantities of meta-information about the databases, which in
turn results in lengthy prompts with many tokens and high processing costs. To
address this challenge, we introduce Datalake Agent, an agentic system designed
to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing
direct solvers for NL2SQL that call the LLM once with all meta-information in
the prompt, the Datalake Agent employs an interactive loop to reduce the
utilized meta-information. Within the loop, the LLM is used in a reasoning
framework that selectively requests only the necessary information to solve a
table question answering task. We evaluate the Datalake Agent on a collection
of 23 databases with 100 table question answering tasks. The Datalake Agent
reduces the tokens used by the LLM by up to 87\% and thus allows for
substantial cost reductions while maintaining competitive performance.

</details>


### [418] [Boosting Instruction Following at Scale](https://arxiv.org/abs/2510.14842)
*Ben Elder,Evelyn Duesterwald,Vinod Muthusamy*

Main category: cs.AI

TL;DR: Instruction Boosting 是一种后生成方法，可提高 LLM 提示指令的可靠性，在 SCALEDIF 基准测试中，将指令遵循率最多提高了 7 个百分点，并引入了冲突评分工具来解释性能下降。


<details>
  <summary>Details</summary>
Motivation: 研究如何提高 LLM 在应用程序中对提示指令的遵循率，以及解释和缓解指令数量增加导致的性能下降问题。

Method: 提出 Instruction Boosting 后生成方法，并引入 SCALEDIF 基准测试和定量冲突评分工具。

Result: Instruction Boosting 可将指令遵循率最多提高 7 个百分点（针对两条指令）和 4 个百分点（针对十条指令）。冲突评分工具能够解释性能随指令数增加而下降的趋势。

Conclusion: Instruction Boosting 是一种有效提高 LLM 指令遵循率的方法，而定量冲突评分工具则有助于开发者理解和管理指令数量对模型性能的影响。

Abstract: A typical approach developers follow to influence an LLM's behavior in an
application is through careful manipulation of the prompt, such as by adding or
modifying instructions. However, merely adding more instructions provides
little assurance that they will actually be followed. We introduce Instruction
Boosting as a post-generation method to increase the reliability of LLM prompt
instructions. We show that Instruction Boosting improves the instruction
following rate by up to 7 points for two instructions and up to 4 points for
ten instructions. To demonstrate these results we introduce SCALEDIF, a
benchmark with a scaled instruction volume of up to ten instructions per data
sample. We also present an analysis of the commonly observed trend that
performance degrades as more instructions are added. We show that an important
factor contributing to this trend is the degree of tension and conflict that
arises as the number of instructions is increased. We contribute a quantitative
conflict scoring tool that explains the observed performance trends and
provides feedback to developers on the impact that additional prompt
instructions have on a model's performance.

</details>


### [419] [LabOS: The AI-XR Co-Scientist That Sees and Works With Humans](https://arxiv.org/abs/2510.14861)
*Le Cong,Zaixi Zhang,Xiaotong Wang,Yin Di,Ruofan Jin,Michal Gerasimiuk,Yinkai Wang,Ravi K. Dinesh,David Smerkous,Alex Smerkous,Xuekun Wu,Shilong Liu,Peishan Li,Yi Zhu,Simran Serrao,Ning Zhao,Imran A. Mohammad,John B. Sunwoo,Joseph C. Wu,Mengdi Wang*

Main category: cs.AI

TL;DR: LabOS是一个集成了计算推理和物理实验的AI共科学家，通过多模态感知、自进化代理和XR人机协作，实现了AI在科学研究中的实时辅助和协同进化。


<details>
  <summary>Details</summary>
Motivation: 科学研究的进展需要理论与实践的结合，而现有的AI工具主要集中在计算设计层面，缺乏与物理实验的深度融合。

Method: LabOS通过连接多模态AI代理、智能眼镜和XR技术，使AI能够感知实验环境、理解实验背景，并与人类科学家实时协作，共同执行实验。

Result: 在癌症免疫疗法靶点发现和干细胞工程等应用中，LabOS证明了AI不仅能进行计算设计，还能深度参与实验过程，将实验室转变为人机协同的智能环境。

Conclusion: LabOS成功实现了AI在科学研究中的角色转变，从单纯的计算工具变为能够参与物理实验的共科学家，促进了人类与机器的协同发现。

Abstract: Modern science advances fastest when thought meets action. LabOS represents
the first AI co-scientist that unites computational reasoning with physical
experimentation through multimodal perception, self-evolving agents, and
Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model
AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see
what scientists see, understand experimental context, and assist in real-time
execution. Across applications--from cancer immunotherapy target discovery to
stem-cell engineering -- LabOS shows that AI can move beyond computational
design to participation, turning the laboratory into an intelligent,
collaborative environment where human and machine discovery evolve together.

</details>


### [420] [The Gatekeeper Knows Enough](https://arxiv.org/abs/2510.14881)
*Fikresilase Wondmeneh Abebayew*

Main category: cs.AI

TL;DR: LLM自主代理受限于有限上下文窗口和状态不同步问题，提出Gatekeeper Protocol框架，通过低保真状态表示和按需请求高保真上下文来解决，并以Sage软件开发代理为例进行验证，结果表明该协议提高了代理的可靠性、计算效率并实现了可扩展的交互。


<details>
  <summary>Details</summary>
Motivation: LLM自主代理在与大型、结构化和敏感知识系统（如代码库和文档）交互时，存在有限的上下文窗口和状态不同步问题，导致输出不可靠、行为不可预测和资源利用效率低下。

Method: 提出Gatekeeper Protocol框架，要求代理首先在一个极简的、低保真的“潜在状态”表示上进行操作和推理，以战略性地按需请求高保真上下文。所有交互都通过统一的JSON格式进行协调，该格式作为声明式的、状态同步的协议，确保代理对系统的模型与其系统的实际情况可验证地保持一致。并实现了一个名为Sage的软件开发代理作为该协议的参考实现。

Result: Sage代理证明了该协议的有效性，显著提高了代理的可靠性，通过最小化令牌消耗提高了计算效率，并实现了与复杂系统的可扩展交互。

Conclusion: Gatekeeper Protocol为构建更健壮、可预测和可靠的AI代理提供了一个基础方法论，适用于任何结构化的知识领域。

Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents,
yet their practical utility is fundamentally constrained by a limited context
window and state desynchronization resulting from the LLMs' stateless nature
and inefficient context management. These limitations lead to unreliable
output, unpredictable behavior, and inefficient resource usage, particularly
when interacting with large, structured, and sensitive knowledge systems such
as codebases and documents. To address these challenges, we introduce the
Gatekeeper Protocol, a novel, domain-agnostic framework that governs
agent-system interactions. Our protocol mandates that the agent first operate
and reason on a minimalist, low-fidelity "latent state" representation of the
system to strategically request high-fidelity context on demand. All
interactions are mediated through a unified JSON format that serves as a
declarative, state-synchronized protocol, ensuring the agent's model of the
system remains verifiably grounded in the system's reality. We demonstrate the
efficacy of this protocol with Sage, a reference implementation of the
Gatekeeper Protocol for software development. Our results show that this
approach significantly increases agent reliability, improves computational
efficiency by minimizing token consumption, and enables scalable interaction
with complex systems, creating a foundational methodology for building more
robust, predictable, and grounded AI agents for any structured knowledge
domain.

</details>


### [421] [Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates](https://arxiv.org/abs/2510.14900)
*Wen-Kwang Tsao,Yao-Ching Yu,Chien-Ming Huang*

Main category: cs.AI

TL;DR: 本研究提出了一种利用强化学习代理进行日志模式映射的方法，通过自我改进和网络搜索来提高映射准确性，并减少需要人工审核的映射数量。


<details>
  <summary>Details</summary>
Motivation: 在企业智能平台中，由于第三方供应商文档的缺失或不完整，日志模式映射面临挑战。

Method: 研究引入了一个强化学习代理，该代理能够自我改进，无需标记数据或模型权重更新。代理通过识别模糊的字段映射、生成网络搜索查询以收集外部证据，并应用基于置信度的奖励来迭代优化映射。

Result: 通过将Microsoft Defender for Endpoint日志转换为通用模式，该方法将映射准确性从仅使用LLM的56.4%提高到使用RAG的72.73%，再到GPT-4o在100次迭代后达到93.94%。同时，需要专家审查的低置信度映射数量减少了85%。

Conclusion: 该新方法为解决未来的行业问题提供了一种有证据支持、透明的方法，有望实现更强大、可问责、可扩展、高效、灵活、适应性强和协作的解决方案。

Abstract: The Enterprise Intelligence Platform must integrate logs from numerous
third-party vendors in order to perform various downstream tasks. However,
vendor documentation is often unavailable at test time. It is either misplaced,
mismatched, poorly formatted, or incomplete, which makes schema mapping
challenging. We introduce a reinforcement learning agent that can self-improve
without labeled examples or model weight updates. During inference, the agent:
1) Identifies ambiguous field-mapping attempts. 2) Generates targeted
web-search queries to gather external evidence. 3) Applies a confidence-based
reward to iteratively refine its mappings. To demonstrate this concept, we
converted Microsoft Defender for Endpoint logs into a common schema. Our method
increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\%
over 100 iterations using GPT-4o. At the same time, it reduced the number of
low-confidence mappings requiring expert review by 85\%. This new approach
provides an evidence-driven, transparent method for solving future industry
problems, paving the way for more robust, accountable, scalable, efficient,
flexible, adaptable, and collaborative solutions.

</details>


### [422] [Budget-aware Test-time Scaling via Discriminative Verification](https://arxiv.org/abs/2510.14913)
*Kyle Montgomery,Sijun Tan,Yuqi Chen,Siyuan Zhuang,Tianjun Zhang,Raluca Ada Popa,Chenguang Wang*

Main category: cs.AI

TL;DR: test-time scaling通过结合判别式验证和自洽性来提高大语言模型在复杂推理任务上的性能，并在固定计算预算下超越了生成式验证。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式验证方法计算成本高昂，限制了其实用性。本研究旨在探索一种更具成本效益的测试时缩放策略。

Method: 本研究提出了一种结合判别式验证和自洽性的混合方法，并进行了广泛的实证分析。

Result: 所提出的混合方法在固定计算预算下，在AIME2025任务上取得了比最先进的生成式验证方法高出15.3%的准确率。

Conclusion: 对于实际应用，成本可控的缩放方法，特别是结合了判别式验证和自洽性的方法，比昂贵的生成式验证方法更有效、更实用。

Abstract: Test-time scaling is a powerful strategy for boosting the performance of
large language models on complex reasoning tasks. While state-of-the-art
approaches often employ generative verifiers to select the best solution from a
pool of candidates, this method incurs prohibitive computational costs,
limiting its practicality. In this work, we shift the focus to a more
budget-aware paradigm: discriminative verification. We conduct a thorough
empirical analysis and demonstrate that while discriminative verifiers may
underperform in isolation, combining them with self-consistency in a hybrid
approach creates a powerful and efficient test-time scaling mechanism. Notably,
under a fixed compute budget, this hybrid approach surpasses state-of-the-art
generative verification by a significant margin: achieving up to 15.3\% higher
accuracy on AIME2025. Our findings establish that for practical, real-world
applications, budget-aware scaling with discriminative verifiers is not only a
"free" upgrade over self-consistency, but also a more effective and efficient
alternative to costly generative techniques. Code is available at
https://github.com/wang-research-lab/verification.

</details>


### [423] [Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models](https://arxiv.org/abs/2510.14925)
*Akira Okutomi*

Main category: cs.AI

TL;DR: Kant's Critique of Pure Reason can be viewed as a theory of feedback stability, where reason acts as a regulator to keep inference within the bounds of possible experience. A composite instability index (H-Risk) is proposed, which combines spectral margin, conditioning, temporal sensitivity, and innovation amplification. This index predicts overconfident errors in linear-Gaussian simulations, highlighting a gap between nominal and epistemic stability. In large language models (LLMs), fragile internal dynamics are linked to miscalibration and hallucination. Critique-style prompts have shown mixed effects on calibration and hallucination. The study suggests a link between Kantian self-limitation and feedback control, offering a method for diagnosing and reducing overconfidence in reasoning systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, conceptualizing reason as a regulator that maintains inference within the bounds of possible experience. It seeks to formalize this idea using a composite instability index (H-Risk) and explore its implications for both linear-Gaussian systems and large language models (LLMs), particularly in understanding overconfident errors, miscalibration, and hallucination.

Method: The study formalizes the reinterpretation of Kant's Critique of Pure Reason by introducing a composite instability index (H-Risk), which integrates spectral margin, conditioning, temporal sensitivity, and innovation amplification. This index is applied to linear-Gaussian simulations to predict overconfident errors and then extended to analyze large language models (LLMs), correlating internal dynamics with miscalibration and hallucination, while also experimenting with critique-style prompts.

Result: In linear-Gaussian simulations, a higher H-Risk index was found to predict overconfident errors, even when formal stability was maintained, indicating a distinction between nominal and epistemic stability. For large language models (LLMs), the research observed that fragile internal dynamics correlate with miscalibration and hallucination. The application of critique-style prompts yielded inconsistent effects on both calibration and hallucination.

Conclusion: The findings suggest a structural connection between the Kantian concept of self-limitation and the principles of feedback control. This framework provides a principled approach for identifying and mitigating overconfidence in reasoning systems, with potential applications to both theoretical models and artificial intelligence like LLMs. The study is presented as a preliminary version, with further experiments and replications planned for a future revision.

Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback
stability, viewing reason as a regulator that keeps inference within the bounds
of possible experience. We formalize this intuition via a composite instability
index (H-Risk) combining spectral margin, conditioning, temporal sensitivity,
and innovation amplification. In linear-Gaussian simulations, higher H-Risk
predicts overconfident errors even under formal stability, revealing a gap
between nominal and epistemic stability. Extending to large language models
(LLMs), we find that fragile internal dynamics correlate with miscalibration
and hallucination, while critique-style prompts show mixed effects on
calibration and hallucination. These results suggest a structural bridge
between Kantian self-limitation and feedback control, offering a principled
lens for diagnosing -- and selectively reducing -- overconfidence in reasoning
systems. This is a preliminary version; supplementary experiments and broader
replication will be reported in a future revision.

</details>


### [424] [GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning](https://arxiv.org/abs/2510.14942)
*Yao Zhang,Yu Wu,Haowei Zhang,Weiguo Li,Haokun Chen,Jingpei Wu,Guohao Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: GroundedPRM是一个自动化的过程监督框架，通过结合蒙特卡洛树搜索（MCTS）和外部工具验证来提高LLM的多步推理能力，同时减少了对昂贵人工标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有用于改进LLM多步推理的PRM方法存在标注成本高、LLM自我评估易产生幻觉、以及蒙特卡洛估计可能导致错误归因等问题，这导致了奖励信号的噪声、事实保真度低以及与推理目标不一致。

Method: GroundedPRM框架通过以下方式解决现有问题：1. 使用MCTS构建结构化推理路径，以减少奖励噪声并实现细粒度的信用分配。2. 使用外部工具验证每个中间步骤，以消除幻觉式监督并提供基于执行的正确性信号。3. 设计混合奖励聚合机制，融合工具验证和MCTS反馈。4. 将奖励信号格式化为增强了原理说明的生成结构，以提高可解释性并兼容指令微调的LLM。

Result: GroundedPRM仅使用40K自动标注样本进行训练（仅为性能最佳的PRM的10%），在ProcessBench上平均性能相对提高了26%。在使用奖励引导的贪婪搜索时，GroundedPRM的表现优于使用人工标注的PRM。

Conclusion: GroundedPRM提供了一种可扩展且可验证的途径，可实现高质量的过程级推理，克服了现有PRM方法的局限性。

Abstract: Process Reward Models (PRMs) aim to improve multi-step reasoning in Large
Language Models (LLMs) by supervising intermediate steps and identifying
errors. However, building effective PRMs remains challenging due to the lack of
scalable, high-quality annotations. Existing approaches rely on costly human
labeling, LLM-based self-evaluation that is prone to hallucination, or Monte
Carlo (MC) estimation, which infers step quality solely from rollout outcomes
and often introduces noisy, misaligned supervision due to credit
misattribution. These issues result in three core limitations: noisy rewards,
low factual fidelity, and misalignment with step-level reasoning objectives. To
address these challenges, we introduce GroundedPRM, a tree-guided and
fidelity-aware framework for automatic process supervision. To reduce reward
noise and enable fine-grained credit assignment, we construct structured
reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated
supervision, we validate each intermediate step using an external tool,
providing execution-grounded correctness signals. To combine both step-level
validation and global outcome assessment, we design a hybrid reward aggregation
mechanism that fuses tool-based verification with MCTS-derived feedback.
Finally, we format the reward signal into a rationale-enhanced, generative
structure to promote interpretability and compatibility with instruction-tuned
LLMs. GroundedPRM is trained on only 40K automatically labeled samples,
amounting to just 10% of the data used by the best-performing PRM trained with
auto-labeled supervision. Nevertheless, it achieves up to a 26% relative
improvement in average performance on ProcessBench. When used for reward-guided
greedy search, GroundedPRM outperforms even PRMs trained with human-labeled
supervision, offering a scalable and verifiable path toward high-quality
process-level reasoning.

</details>


### [425] [Towards Unified Multimodal Misinformation Detection in Social Media: A Benchmark Dataset and Baseline](https://arxiv.org/abs/2509.25991)
*Haiyang Li,Yaxiong Wang,Shengeng Tang,Lianwei Wu,Lechao Cheng,Zhun Zhong*

Main category: cs.AI

TL;DR: 该论文构建了一个包含127K个样本的“OmniFake”数据集，整合了人工伪造信息和AI生成内容，并提出了一个名为“UMFDet”的框架，用于统一检测这两种欺骗形式。UMFDet利用视觉语言模型（VLM）骨干和专家混合（MoE）适配器来捕捉特定类别的线索，并通过归因链式思考机制提供隐式推理指导，以定位欺骗信号。实验表明，UMFDet在两种欺骗类型上都取得了稳健且一致的性能，优于专用基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究将人工伪造信息和AI生成内容的研究割裂开来，导致现有模型通常只能检测其中一种类型的虚假内容，这在真实世界的应用中效果有限。因此，需要一个能够同时处理这两种欺骗形式的统一框架。

Method: 构建了一个名为“OmniFake”的数据集，包含127K个样本，整合了人工伪造信息和AI生成内容。提出了一个名为“UMFDet”的框架，该框架以视觉语言模型（VLM）为骨干，并引入了类别感知专家混合（MoE）适配器来捕捉特定类别的线索，同时利用了归因链式思考机制来提供隐式推理指导，以定位欺骗信号。

Result: UMFDet框架在两种欺骗类型上均取得了稳健且一致的性能，并且优于仅针对单一类型欺骗内容的专用基线模型。

Conclusion: UMFDet是一个有效的统一框架，可以同时检测人工伪造信息和AI生成内容，为真实世界的多模态欺骗检测提供了一个实用的解决方案。

Abstract: In recent years, detecting fake multimodal content on social media has drawn
increasing attention. Two major forms of deception dominate: human-crafted
misinformation (e.g., rumors and misleading posts) and AI-generated content
produced by image synthesis models or vision-language models (VLMs). Although
both share deceptive intent, they are typically studied in isolation. NLP
research focuses on human-written misinformation, while the CV community
targets AI-generated artifacts. As a result, existing models are often
specialized for only one type of fake content. In real-world scenarios,
however, the type of a multimodal post is usually unknown, limiting the
effectiveness of such specialized systems. To bridge this gap, we construct the
Omnibus Dataset for Multimodal News Deception (OmniFake), a comprehensive
benchmark of 127K samples that integrates human-curated misinformation from
existing resources with newly synthesized AI-generated examples. Based on this
dataset, we propose Unified Multimodal Fake Content Detection (UMFDet), a
framework designed to handle both forms of deception. UMFDet leverages a VLM
backbone augmented with a Category-aware Mixture-of-Experts (MoE) Adapter to
capture category-specific cues, and an attribution chain-of-thought mechanism
that provides implicit reasoning guidance for locating salient deceptive
signals. Extensive experiments demonstrate that UMFDet achieves robust and
consistent performance across both misinformation types, outperforming
specialized baselines and offering a practical solution for real-world
multimodal deception detection.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [426] [PoissonNet: A Local-Global Approach for Learning on Surfaces](https://arxiv.org/abs/2510.14146)
*Arman Maesumi,Tanish Makadia,Thibault Groueix,Vladimir G. Kim,Daniel Ritchie,Noam Aigerman*

Main category: cs.GR

TL;DR: PoissonNet是一种新的网格学习神经网络架构，它使用泊松方程进行特征传播，解决了现有方法的局限性，并在语义分割、网格参数化和变形学习方面取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有网络架构在学习网格高频特征、感受野大小、对离散化的敏感性以及计算开销方面存在权衡。PoissonNet旨在克服这些缺陷。

Method: PoissonNet通过将局部特征变换与使用泊松方程的全局传播相结合来构建。它在网格的梯度域中进行局部特征变换，然后求解泊松方程以在整个表面上传播标量特征更新。

Result: PoissonNet在保持特征的完整频谱和提供全局感受野的同时，保持了对网格三角化的不变性。该方法计算效率高，易于扩展。在语义分割、网格参数化和变形学习等任务上，PoissonNet的性能优于现有方法。

Conclusion: PoissonNet是一种有效的网格学习架构，它通过泊松方程实现了高效的局部-全局特征传播，并在各种下游任务中取得了最先进的性能。

Abstract: Many network architectures exist for learning on meshes, yet their
constructions entail delicate trade-offs between difficulty learning
high-frequency features, insufficient receptive field, sensitivity to
discretization, and inefficient computational overhead. Drawing from classic
local-global approaches in mesh processing, we introduce PoissonNet, a novel
neural architecture that overcomes all of these deficiencies by formulating a
local-global learning scheme, which uses Poisson's equation as the primary
mechanism for feature propagation. Our core network block is simple; we apply
learned local feature transformations in the gradient domain of the mesh, then
solve a Poisson system to propagate scalar feature updates across the surface
globally. Our local-global learning framework preserves the features's full
frequency spectrum and provides a truly global receptive field, while remaining
agnostic to mesh triangulation. Our construction is efficient, requiring far
less compute overhead than comparable methods, which enables scalability --
both in the size of our datasets, and the size of individual training samples.
These qualities are validated on various experiments where, compared to
previous intrinsic architectures, we attain state-of-the-art performance on
semantic segmentation and parameterizing highly-detailed animated surfaces.
Finally, as a central application of PoissonNet, we show its ability to learn
deformations, significantly outperforming state-of-the-art architectures that
learn on surfaces.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [427] [Laser Fault Injection in Memristor-Based Accelerators for AI/ML and Neuromorphic Computing](https://arxiv.org/abs/2510.14120)
*Muhammad Faheemur Rahman,Wayne Burleson*

Main category: cs.ET

TL;DR: 激光注入故障可用于攻击基于忆阻器交叉阵列的硬件，通过分析输出电流来推断和篡改内部权重。


<details>
  <summary>Details</summary>
Motivation: 忆阻器交叉阵列（MCA）虽然在内存计算和神经形态硬件方面具有潜力，但其物理特性也带来了新的攻击面，尤其是在故障注入场景下。本文旨在探索激光故障注入对MCA架构的影响。

Method: 本文提出了一种攻击模型，其中攻击者利用激光束对忆阻器单元进行物理扰动。通过在45nm CMOS技术节点上对大型MCA进行HSPICE仿真，分析激光诱导的光电流如何影响输出电流分布。

Result: 仿真结果表明，激光注入故障会导致输出电流分布发生变化，使得通过差分故障分析能够以高达99.7%的准确率推断内部权重，并复制模型。此外，通过有针对性的权重修改，可以对计算完整性造成约143%的损害。

Conclusion: 激光故障注入是一种有效的攻击手段，可以用于推断和篡弱忆阻器交叉阵列的计算完整性。

Abstract: Memristive crossbar arrays (MCA) are emerging as efficient building blocks
for in-memory computing and neuromorphic hardware due to their high density and
parallel analog matrix-vector multiplication capabilities. However, the
physical properties of their nonvolatile memory elements introduce new attack
surfaces, particularly under fault injection scenarios. This work explores
Laser Fault Injection as a means of inducing analog perturbations in MCA-based
architectures. We present a detailed threat model in which adversaries target
memristive cells to subtly alter their physical properties or outputs using
laser beams. Through HSPICE simulations of a large MCA on 45 nm CMOS tech.
node, we show how laser-induced photocurrent manifests in output current
distributions, enabling differential fault analysis to infer internal weights
with up to 99.7% accuracy, replicate the model, and compromise computational
integrity through targeted weight alterations by approximately 143%.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [428] [T-BAT semantics and its logics](https://arxiv.org/abs/2510.14361)
*Pawel Pawlowski*

Main category: cs.LO

TL;DR: T-BAT是一种非确定性四值逻辑，用于表达非形式化可证性，并研究其语义、公理化和模态视角。


<details>
  <summary>Details</summary>
Motivation: 研究T-BAT逻辑的非确定性四值语义，探索其与公理的相互作用，并提供直观的公理化。

Method: 通过将真值转化为目标语言表达式来证明所有可定义逻辑的完备性，并利用克里普克语义从模态角度检验公理。

Result: 证明了所有可定义逻辑的完备性，并提供了由公理诱导的框架条件。

Conclusion: T-BAT逻辑提供了一种表达非形式化可证性的方式，其四值语义和公理化得到了研究和验证。

Abstract: \textbf{T-BAT} logic is a formal system designed to express the notion of
informal provability. This type of provability is closely related to
mathematical practice and is quite often contrasted with formal provability,
understood as a formal derivation in an appropriate formal system.
\textbf{T-BAT} is a non-deterministic four-valued logic. The logical values in
\textbf{T-BAT} semantics convey not only the information whether a given
formula is true but also about its provability status.
  The primary aim of our paper is to study the proposed four-valued
non-deterministic semantics. We look into the intricacies of the interactions
between various weakenings and strengthenings of the semantics with axioms that
they induce. We prove the completeness of all the logics that are definable in
this semantics by transforming truth values into specific expressions
formulated within the object language of the semantics. Additionally, we
utilize Kripke semantics to examine these axioms from a modal perspective by
providing a frame condition that they induce. The secondary aim of this paper
is to provide an intuitive axiomatization of \textbf{T-BAT} logic.

</details>


### [429] [Optimization Modulo Integer Linear-Exponential Programs](https://arxiv.org/abs/2510.14550)
*S Hitarth,Alessio Mansutti,Guruprerana Shabadi*

Main category: cs.LO

TL;DR: 本研究提出了整数线性-指数程序（ILEP）优化问题的复杂度分析，并建立了其解可以用整数线性-指数程序（ILESLP）简洁表示，同时提供了一个依赖整数分解预言机的多项式时间算法来判定ILESLP是否编码了ILEP的一个解，并将该优化问题归类到FNP^NP类。


<details>
  <summary>Details</summary>
Motivation: 整数线性-指数程序（ILEP）在经典整数线性程序的基础上扩展了指数函数和取模函数，研究其优化问题的复杂度具有重要意义，特别是此前决策问题已被证明是NP完全的。

Method: 1. 证明若ILEP存在最优解，则必存在一个可以用整数线性-指数程序（ILESLP）简洁表示的最优解。2. 设计一个多项式时间算法，该算法依赖于整数分解预言机，用于判断一个ILESLP是否编码了ILEP的一个可行解，并可用于比较两个解的目标函数值。

Result: 1. 存在最优解的情况下，最优解可以被表示为一个整数线性-指数程序（ILESLP），即一个只包含加法、乘法（有理数）和指数运算的算术电路，且所有中间值均为整数。2. 存在一个多项式时间算法（依赖于整数分解预言机），能够判断一个ILESLP是否编码了ILEP的一个可行解，并可用于比较两个解的目标函数值。

Conclusion: 通过上述结果，将ILEP的优化问题置于一个NPO的扩展类中，该类位于FNP^NP内，并且该方法无需通过二分查找来确定最优解。

Abstract: This paper presents the first study of the complexity of the optimization
problem for integer linear-exponential programs which extend classical integer
linear programs with the exponential function $x \mapsto 2^x$ and the remainder
function ${(x,y) \mapsto (x \bmod 2^y)}$. The problem of deciding if such a
program has a solution was recently shown to be NP-complete in [Chistikov et
al., ICALP'24]. The optimization problem instead asks for a solution that
maximizes (or minimizes) a linear-exponential objective function, subject to
the constraints of an integer linear-exponential program. We establish the
following results:
  1. If an optimal solution exists, then one of them can be succinctly
represented as an integer linear-exponential straight-line program (ILESLP): an
arithmetic circuit whose gates always output an integer value (by construction)
and implement the operations of addition, exponentiation, and multiplication by
rational numbers.
  2. There is an algorithm that runs in polynomial time, given access to an
integer factoring oracle, which determines whether an ILESLP encodes a solution
to an integer linear-exponential program. This algorithm can also be used to
compare the values taken by the objective function on two given solutions.
  Building on these results, we place the optimization problem for integer
linear-exponential programs within an extension of the optimization class
$\text{NPO}$ that lies within $\text{FNP}^{\text{NP}}$. In essence, this
extension forgoes determining the optimal solution via binary search.

</details>


### [430] [Problems and Consequences of Bilateral Notions of (Meta-)Derivability](https://arxiv.org/abs/2510.14619)
*Sara Ayhan*

Main category: cs.LO

TL;DR: 该论文提出了一种基于双边主义的证明论语义学方法，该方法不仅关注规则如何给出连接词的可证性条件，还关注其可驳斥性条件。


<details>
  <summary>Details</summary>
Motivation: 现有证明系统主要关注可证性条件，忽略了可驳斥性条件，导致证明与反驳的概念存在不平衡。

Method: 在自然演泽系统中引入双重可证性关系来表达可证性和可驳斥性条件，并探讨了将其应用于序列演算时遇到的困难，分析了序列演算中序列符号和水平线所代表的可证性关系，并提出了双重化序列演算的方法。

Result: 分析了在序列演算中双重化水平线所带来的问题，揭示了证明与反驳概念之间深层概念性失衡。

Conclusion: 提出了在序列演算系统中保留双重化所需平衡的解决方案。

Abstract: A bilateralist take on proof-theoretic semantics can be understood as
demanding of a proof system to display not only rules giving the connectives'
provability conditions but also their refutability conditions. On such a view,
then, a system with two derivability relations is obtained, which can be quite
naturally expressed in a proof system of natural deduction but which faces
obstacles in a sequent calculus representation. Since in a sequent calculus
there are two derivability relations inherent, one expressed by the sequent
sign and one by the horizontal lines holding between sequents, in a truly
bilateral calculus both need to be dualized. While dualizing the sequent sign
is rather straightforwardly corresponding to dualizing the horizontal lines in
natural deduction, dualizing the horizontal lines in sequent calculus, uncovers
problems that, as will be argued in this paper, shed light on deeper conceptual
issues concerning an imbalance between the notions of proof vs. refutation. The
roots of this problem will be further analyzed and possible solutions on how to
retain a bilaterally desired balance in our system are presented.

</details>


### [431] [Admissibility of Substitution Rule in Cyclic-Proof Systems](https://arxiv.org/abs/2510.14749)
*Kenji Saotome,Koji Nakazawa*

Main category: cs.LO

TL;DR: 本篇论文证明了在包含归纳谓词的一阶逻辑的循环证明系统CLKID^omega中，即使存在cut规则，替换规则也是可接受的。


<details>
  <summary>Details</summary>
Motivation: 替换规则会增加理论分析的复杂性和证明搜索的计算成本，因此，在循环证明系统中，替换规则的可接受性是有益的。然而，在非循环系统中证明其可接受性的局部证明变换方法可能破坏循环结构，不适用于循环证明系统。虽然有观点认为替换规则在CLKID^omega中可能不可接受，但本研究旨在证明其在特定条件下的可接受性。

Method: 本研究将循环证明展开成无穷形式，提升替换规则，然后重新构建循环证明，但不使用替换规则。

Result: 本研究证明了在包含归纳谓词的一阶逻辑的循环证明系统CLKID^omega中，当存在cut规则时，替换规则是可接受的。如果限制替换规则不包含函数符号，那么该结果可以扩展到更广泛的系统，包括无cut的CLKID^omega以及分离逻辑的循环证明系统。

Conclusion: 本研究成功证明了在CLKID^omega系统中，在存在cut规则的前提下，替换规则是可接受的，这解决了先前关于其不可接受性的猜测，并为循环证明系统的理论和计算分析提供了简化。

Abstract: This paper investigates the admissibility of the substitution rule in
cyclic-proof systems. The substitution rule complicates theoretical case
analysis and increases computational cost in proof search since every sequent
can be a conclusion of an instance of the substitution rule; hence,
admissibility is desirable on both fronts. While admissibility is often shown
by local proof transformations in non-cyclic systems, such transformations may
disrupt cyclic structure and do not readily apply. Prior remarks suggested that
the substitution rule is likely nonadmissible in the cyclic-proof system
CLKID^omega for first-order logic with inductive predicates. In this paper, we
prove admissibility in CLKID^omega, assuming the presence of the cut rule. Our
approach unfolds a cyclic proof into an infinitary form, lifts the substitution
rules, and places back edges to construct a cyclic proof without the
substitution rule. If we restrict substitutions to exclude function symbols,
the result extends to a broader class of systems, including cut-free
CLKID^omega and cyclic-proof systems for the separation logic.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [432] [Large Language Models for Real-World IoT Device Identification](https://arxiv.org/abs/2510.13817)
*Rameen Mahmood,Tousif Ahmed,Sai Teja Peddinti,Danny Yuxing Huang*

Main category: cs.LG

TL;DR: IoT设备数量激增，现有识别方法难以应对，尤其是在开放世界环境中。本文将设备识别视为一种语言建模任务，利用异构网络元数据。通过LLM生成高保真供应商标签，并对LLaMA3.18B模型进行指令微调，以应对数据稀疏和长尾分布问题。模型在2015个供应商上达到了98.25%的top-1准确率和90.73%的宏准确率，并对缺失字段、协议漂移和对抗性攻击具有鲁棒性。独立测试表明，该方法可扩展且可解释。


<details>
  <summary>Details</summary>
Motivation: 现有IoT设备识别方法在开放世界环境中面临数据不完整、噪声大、故意混淆等挑战，带来安全、隐私和网络问责风险。

Method: 将设备识别重构为一种语言建模任务，处理异构网络元数据。使用大型语言模型（LLM）生成高保真供应商标签，以解决IoT Inspector数据集的监督问题。采用课程学习策略对量化后的LLaMA3.18B模型进行指令微调，以提高模型在数据稀疏和长尾分布情况下的泛化能力。

Result: 模型在2015个供应商上实现了98.25%的top-1准确率和90.73%的宏准确率。模型对缺失字段、协议漂移和对抗性攻击表现出鲁棒性。

Conclusion: 指令微调的大型语言模型为大规模实际设备识别提供了一个可扩展且可解释的基础。

Abstract: The rapid expansion of IoT devices has outpaced current identification
methods, creating significant risks for security, privacy, and network
accountability. These challenges are heightened in open-world environments,
where traffic metadata is often incomplete, noisy, or intentionally obfuscated.
We introduce a semantic inference pipeline that reframes device identification
as a language modeling task over heterogeneous network metadata. To construct
reliable supervision, we generate high-fidelity vendor labels for the IoT
Inspector dataset, the largest real-world IoT traffic corpus, using an ensemble
of large language models guided by mutual-information and entropy-based
stability scores. We then instruction-tune a quantized LLaMA3.18B model with
curriculum learning to support generalization under sparsity and long-tail
vendor distributions. Our model achieves 98.25% top-1 accuracy and 90.73% macro
accuracy across 2,015 vendors while maintaining resilience to missing fields,
protocol drift, and adversarial manipulation. Evaluation on an independent IoT
testbed, coupled with explanation quality and adversarial stress tests,
demonstrates that instruction-tuned LLMs provide a scalable and interpretable
foundation for real-world device identification at scale.

</details>


### [433] [Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation](https://arxiv.org/abs/2510.13864)
*Zixi Wang,Yushe Cao,Yubo Huang,Jinzhu Wei,Jingzehua Xu,Shuai Zhang,Xin Lai*

Main category: cs.LG

TL;DR: STDW是一种新的自训练方法，通过动态加权机制增强了渐进域适应（GDA）的鲁棒性，解决了知识平滑迁移的挑战，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统GDA方法在通过中间域和自训练缓解域偏移时，存在知识迁移效率低或中间数据不完整的问题。本研究旨在通过一种新的动态加权机制来增强GDA的鲁棒性，以实现从源域到目标域的平滑知识迁移。

Method: 提出了一种名为自训练动态加权（STDW）的新方法，该方法引入了一个动态加权机制，通过一个随时间变化的超参数$\varrho$（从0到1）来动态平衡源域和目标域的损失贡献，并结合自训练生成伪标签，进行迭代模型更新。

Result: 在旋转MNIST、颜色偏移MNIST、肖像数据集和Cover Type数据集上的实验表明，STDW的性能优于现有基线方法。消融研究证实了$\varrho$的动态调度在实现渐进式适应中的关键作用，有效减少了域偏差并提高了泛化能力。

Conclusion: STDW提供了一个理论见解和实践框架，用于鲁棒的渐进域适应，可应用于动态的现实场景。其动态加权机制有效解决了传统GDA方法的不足，实现了更稳定和有效的域适应。

Abstract: In this paper, we propose a new method called Self-Training with Dynamic
Weighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation
(GDA) by addressing the challenge of smooth knowledge migration from the source
to the target domain. Traditional GDA methods mitigate domain shift through
intermediate domains and self-training but often suffer from inefficient
knowledge migration or incomplete intermediate data. Our approach introduces a
dynamic weighting mechanism that adaptively balances the loss contributions of
the source and target domains during training. Specifically, we design an
optimization framework governed by a time-varying hyperparameter $\varrho$
(progressing from 0 to 1), which controls the strength of domain-specific
learning and ensures stable adaptation. The method leverages self-training to
generate pseudo-labels and optimizes a weighted objective function for
iterative model updates, maintaining robustness across intermediate domains.
Experiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the
Cover Type dataset demonstrate that STDW outperforms existing baselines.
Ablation studies further validate the critical role of $\varrho$'s dynamic
scheduling in achieving progressive adaptation, confirming its effectiveness in
reducing domain bias and improving generalization. This work provides both
theoretical insights and a practical framework for robust gradual domain
adaptation, with potential applications in dynamic real-world scenarios. The
code is available at https://github.com/Dramwig/STDW.

</details>


### [434] [Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning](https://arxiv.org/abs/2510.13865)
*Dongkwan Lee,Junhoo Lee,Nojun Kwak*

Main category: cs.LG

TL;DR: 我们提出了一种名为深度边缘滤波器（Deep Edge Filter）的新方法，通过对深度神经网络特征应用高通滤波来提高模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 该方法基于一个假设：神经网络将任务相关的语义信息编码在高频分量中，而将领域特定的偏差存储在低频分量中。

Method: 通过从原始特征中减去低通滤波后的输出，该方法可以分离出可泛化的表示，同时保持架构的完整性。

Result: 在视觉、文本、3D和音频等不同领域进行的实验结果表明，无论模型架构和数据模式如何，性能都得到了一致的提升。

Conclusion: 分析表明，该方法能够诱导特征稀疏化并有效分离高频分量，为核心假设提供了经验验证。

Abstract: We introduce the Deep Edge Filter, a novel approach that applies high-pass
filtering to deep neural network features to improve model generalizability.
Our method is motivated by our hypothesis that neural networks encode
task-relevant semantic information in high-frequency components while storing
domain-specific biases in low-frequency components of deep features. By
subtracting low-pass filtered outputs from original features, our approach
isolates generalizable representations while preserving architectural
integrity. Experimental results across diverse domains such as Vision, Text,
3D, and Audio demonstrate consistent performance improvements regardless of
model architecture and data modality. Analysis reveals that our method induces
feature sparsification and effectively isolates high-frequency components,
providing empirical validation of our core hypothesis. The code is available at
https://github.com/dongkwani/DeepEdgeFilter.

</details>


### [435] [SHaRe-SSM: An Oscillatory Spiking Neural Network for Target Variable Modeling in Long Sequences](https://arxiv.org/abs/2510.14386)
*Kartikay Agrawal,Abhijeet Vikram,Vedant Sharma,Vaishnavi N.,Ayon Borthakur*

Main category: cs.LG

TL;DR: SHaRe-SSM是一种结合了脉冲神经网络（SNNs）和状态空间模型（SSMs）的新型模型，用于处理极长序列的变量建模（分类和回归）。它在能效和性能上优于Transformer和一阶SSMs，并且避免了乘法运算，特别适合资源受限的应用。


<details>
  <summary>Details</summary>
Motivation: 为了结合SNNs的能效和SSMs处理长序列的优势，设计一个适用于资源受限场景的高效长序列建模模型。

Method: 提出了一种二阶脉冲SSM（SHaRe-SSM），利用共振和发放（resonate and fire）神经元，并通过并行扫描实现动力学系统的稳定高效计算。同时，提出了一种基于核方法的脉冲回归器。

Result: SHaRe-SSM在18k序列上比二阶ANN-based SSMs能耗低73倍，同时保持性能。在50k序列上表现出优越的性能，并且能耗显著降低。还对共振和发放SSMs中的异质性、耗散和守恒进行了系统性分析。

Conclusion: SHaRe-SSM是一种有前景的模型，能够高效处理极长序列，并在能效和性能方面优于现有方法，为资源受限环境下的深度学习提供了新的解决方案。

Abstract: In recent years, with the emergence of large models, there has been a
significant interest in spiking neural networks (SNNs) primarily due to their
energy efficiency, multiplication-free, and sparse event-based deep learning.
Similarly, state space models (SSMs) in varying designs have evolved as a
powerful alternative to transformers for target modeling in long sequences,
thereby overcoming the quadratic dependence on sequence length of a
transformer. Inspired by this progress, we here design SHaRe-SSM (Spiking
Harmonic Resonate and Fire State Space Model), for target variable modeling
(including both classification and regression) for very-long-range sequences.
Our second-order spiking SSM, on average, performs better than transformers or
first-order SSMs while circumventing multiplication operations, making it ideal
for resource-constrained applications. The proposed block consumes $73 \times$
less energy than second-order ANN-based SSMs for an 18k sequence, while
retaining performance. To ensure learnability over the long-range sequences, we
propose exploiting the stable and efficient implementation of the dynamical
system using parallel scans. Moreover, for the first time, we propose a
kernel-based spiking regressor using resonate and fire neurons for very
long-range sequences. Our network shows superior performance on even a 50k
sequence while being significantly energy-efficient. In addition, we conducted
a systematic analysis of the impact of heterogeneity, dissipation, and
conservation in resonate-and-fire SSMs.

</details>


### [436] [CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks](https://arxiv.org/abs/2510.13869)
*Munsif Ali,Leonardo Rossi,Massimo Bertozzi*

Main category: cs.LG

TL;DR: CoLoR-GAN是一个用于持续学习和少样本学习的GAN框架，通过低秩张量和LoRA技术，在保持SOTA性能的同时显著减少了参数量。


<details>
  <summary>Details</summary>
Motivation: 现有的持续学习（CL）生成对抗网络（GAN）方法在处理少样本（FS）学习时存在灾难性遗忘问题，并且会引入大量新权重，这在长期来看是不可取的。

Method: CoLoR-GAN框架利用低秩张量来适应新任务，并引入了LoRA（低秩适应）技术来减少参数量。为了进一步优化适配器的大小，还提出了一种用于卷积层的LoRA in LoRA（LLoRA）技术。此外，论文还提供了关于LoRA超参数选择的实证研究。

Result: 实验表明，CoLoR-GAN在多个CL和FS基准任务上表现出了有效性，达到了SOTA性能，同时极大地减少了资源消耗。

Conclusion: CoLoR-GAN在持续学习和少样本学习场景下，通过低秩张量和LLoRA等高效技术，成功地在保持SOTA性能的同时大幅降低了模型对资源的需求，解决了现有方法的局限性。

Abstract: Continual learning (CL) in the context of Generative Adversarial Networks
(GANs) remains a challenging problem, particularly when it comes to learn from
a few-shot (FS) samples without catastrophic forgetting. Current most effective
state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible
quantity of new weights at each training iteration, which would become
significant when considering the long term. For this reason, this paper
introduces \textcolor{red}{\textbf{\underline{c}}}ontinual
few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with
\textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank
adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and
CL together, leveraging low-rank tensors to efficiently adapt the model to
target tasks while reducing even more the number of parameters required.
Applying a vanilla LoRA implementation already permitted us to obtain pretty
good results. In order to optimize even further the size of the adapters, we
challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for
convolutional layers. Finally, aware of the criticality linked to the choice of
the hyperparameters of LoRA, we provide an empirical study to easily find the
best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on
several benchmark CL and FS tasks and show that our model is efficient,
reaching SOTA performance but with a number of resources enormously reduced.
Source code is available on
\href{https://github.com/munsifali11/CoLoR-GAN}{Github.

</details>


### [437] [Online Reliable Anomaly Detection via Neuromorphic Sensing and Communications](https://arxiv.org/abs/2510.14688)
*Junya Shiraishi,Jiechen Chen,Osvaldo Simeone,Petar Popovski*

Main category: cs.LG

TL;DR: 该框架提出了一种基于神经形态无线传感器网络的低功耗在线异常检测方法，适用于脑机接口和环境监测等场景。


<details>
  <summary>Details</summary>
Motivation: 提出一种低功耗在线异常检测框架，用于脑机接口和环境监测等场景。

Method: 使用脉冲无线电（IR）传输，通过事件驱动的神经形态传感器节点（neuro-SNs）响应中央读取节点，并采用在线假设检验方法（e-values）进行异常检测，同时利用多臂老虎机框架优化传感器查询策略。

Result: 该方法能在严格的虚报率（FDR）控制下可靠地检测异常，同时优化通信调度并实现低检测延迟。

Conclusion: 该框架能够有效检测异常，并满足低功耗、低延迟和严格的虚报率控制要求。

Abstract: This paper proposes a low-power online anomaly detection framework based on
neuromorphic wireless sensor networks, encompassing possible use cases such as
brain-machine interfaces and remote environmental monitoring. In the considered
system, a central reader node actively queries a subset of neuromorphic sensor
nodes (neuro-SNs) at each time frame. The neuromorphic sensors are
event-driven, producing spikes in correspondence to relevant changes in the
monitored system. The queried neuro-SNs respond to the reader with impulse
radio (IR) transmissions that directly encode the sensed local events. The
reader processes these event-driven signals to determine whether the monitored
environment is in a normal or anomalous state, while rigorously controlling the
false discovery rate (FDR) of detections below a predefined threshold. The
proposed approach employs an online hypothesis testing method with e-values to
maintain FDR control without requiring knowledge of the anomaly rate, and it
dynamically optimizes the sensor querying strategy by casting it as a best-arm
identification problem in a multi-armed bandit framework. Extensive performance
evaluation demonstrates that the proposed method can reliably detect anomalies
under stringent FDR requirements, while efficiently scheduling sensor
communications and achieving low detection latency.

</details>


### [438] [MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving](https://arxiv.org/abs/2510.14557)
*Jungi Lee,Junyong Park,Soohyun Cha,Jaehoon Cho,Jaewoong Sim*

Main category: cs.LG

TL;DR: 文章提出了MX+格式，一种用于低精度大模型服务的有效方法，解决了现有低比特格式处理异常值的问题，在性能和开销之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有低精度数据格式（如BFP）在用于大语言模型（LLM）服务时，因异常值问题导致性能不佳，且往往需要侵入式修改软件框架，或采用非标准格式，不利于硬件厂商的广泛采用。

Method: 提出MX+格式，通过将异常值的数据类型中的指数域重新用作扩展尾数来提高异常值精度，以此解决低比特BFP格式中的异常值问题。MX+格式可无缝集成到现有的微缩（MX）格式中。

Result: MX+格式在模型性能上显著优于4位MX格式（MXFP4），同时存储开销和减速可忽略不计。

Conclusion: MX+格式为LLM推理提供了一种有竞争力的新选择，它在不显著增加开销的情况下，有效解决了低比特格式在处理异常值时的性能瓶颈，为高效LLM服务提供了优于MXFP4或MXFP6的替代方案。

Abstract: Reduced-precision data formats are crucial for cost-effective serving of
large language models (LLMs). While numerous reduced-precision formats have
been introduced thus far, they often require intrusive modifications to the
software frameworks or are rather unconventional for widespread adoption across
hardware vendors. In this paper, we instead focus on recent industry-driven
variants of block floating-point (BFP) formats and conduct a comprehensive
analysis to push their limits for efficient LLM serving. Our analysis shows
that existing ultra low-bit BFP variants struggle to provide reasonable
language model performance due to outlier values in blocks. To address the
outliers with BFPs, we propose MX+, a cost-effective and non-intrusive
extension designed for seamless integration into the microscaling (MX) formats.
MX+ builds on the key insight that the outlier does not need to use its
exponent field in the element data type, which allows us to repurpose the
exponent field as an extended mantissa to increase the precision of the outlier
element. Our evaluation shows that MX+ achieves significantly higher model
performance compared to the 4-bit MX format (MXFP4) with negligible storage
overhead and slowdown, thus offering a compelling alternative to MXFP4 or MXFP6
for efficient LLM inference.

</details>


### [439] [Joint Discriminative-Generative Modeling via Dual Adversarial Training](https://arxiv.org/abs/2510.13872)
*Xuwang Yin,Claire Zhang,Julie Steele,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 通过结合对抗训练（AT）原则，提出了一种新的训练框架，用于提高分类鲁棒性和生成模型质量，解决了现有混合方法（如JEM）中SGLD训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 同时在单一框架内实现鲁棒分类和高保真生成建模是一个重大挑战，现有方法（如JEM）受限于SGLD训练的不稳定性和差的样本质量。

Method: 提出了一种新的训练框架，将对抗训练（AT）原则整合到JEM中。主要创新包括：1. 使用基于AT的方法替代SGLD，通过区分真实数据和PGD生成的对比样本来优化能量函数，使用BCE损失。2. 协同对抗训练用于判别组件，提高分类鲁棒性，无需显式梯度惩罚。3. 采用两阶段训练程序解决批量归一化与EBM训练的不兼容性。

Result: 在CIFAR-10、CIFAR-100和ImageNet上的实验表明，该方法在对抗鲁棒性方面显著优于现有混合模型，同时保持了有竞争力的生成性能。在ImageNet上，该模型在生成建模方面的保真度超过了BigGAN，接近扩散模型。

Conclusion: 该方法解决了限制JEM扩展性的关键稳定性问题，并证明了对抗训练可以作为统一框架的有效基础，该框架能够生成和鲁棒地分类视觉数据。

Abstract: Simultaneously achieving robust classification and high-fidelity generative
modeling within a single framework presents a significant challenge. Hybrid
approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as
EBMs but are often limited by the instability and poor sample quality inherent
in SGLD-based training. We address these limitations by proposing a novel
training framework that integrates adversarial training (AT) principles for
both discriminative robustness and stable generative learning. The proposed
method introduces three key innovations: (1) the replacement of SGLD-based JEM
learning with a stable, AT-based approach that optimizes the energy function by
discriminating between real data and PGD-generated contrastive samples using
the BCE loss; (2) synergistic adversarial training for the discriminative
component that enhances classification robustness while eliminating the need
for explicit gradient penalties; and (3) a two-stage training procedure to
resolve the incompatibility between batch normalization and EBM training.
Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method
substantially improves adversarial robustness over existing hybrid models while
maintaining competitive generative performance. On ImageNet, when optimized for
generative modeling, our model's generative fidelity surpasses that of BigGAN
and approaches diffusion models, representing the first MCMC-based EBM approach
to achieve high-quality generation on complex, high-resolution datasets. Our
approach addresses key stability issues that have limited JEM scaling and
demonstrates that adversarial training can serve as an effective foundation for
unified frameworks capable of generating and robustly classifying visual data.

</details>


### [440] [Provable Unlearning with Gradient Ascent on Two-Layer ReLU Neural Networks](https://arxiv.org/abs/2510.14844)
*Odelia Melamed,Gilad Yehudai,Gal Vardi*

Main category: cs.LG

TL;DR: 梯度上升可用于在不从头开始重新训练的情况下从训练模型中删除特定数据点的影响，从而在理论上支持其在隐私和机器学习中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着隐私和道德问题的日益严重，需要一种有效的方法来从训练模型中删除特定数据（机器学习），而无需从头开始重新训练。

Method: 本文分析了用于通过梯度上升来逆转特定数据点影响的简单且广泛使用的方法，并量化了未学习模型通过评估其满足边际最大化问题的 KKT 条件的程度来满足这些条件。

Result: 所提出的 $(\epsilon, \delta, \tau)$-成功的学习标准已得到满足，并且该模型近似于保留数据上的重新训练解决方案，同时保持泛化。

Conclusion: 梯度上升是一种有效的机器学习方法，可以安全地删除特定数据点的影响，同时保留模型的泛化能力。

Abstract: Machine Unlearning aims to remove specific data from trained models,
addressing growing privacy and ethical concerns. We provide a theoretical
analysis of a simple and widely used method - gradient ascent - used to reverse
the influence of a specific data point without retraining from scratch.
Leveraging the implicit bias of gradient descent towards solutions that satisfy
the Karush-Kuhn-Tucker (KKT) conditions of a margin maximization problem, we
quantify the quality of the unlearned model by evaluating how well it satisfies
these conditions w.r.t. the retained data. To formalize this idea, we propose a
new success criterion, termed \textbf{$(\epsilon, \delta, \tau)$-successful}
unlearning, and show that, for both linear models and two-layer neural networks
with high dimensional data, a properly scaled gradient-ascent step satisfies
this criterion and yields a model that closely approximates the retrained
solution on the retained data. We also show that gradient ascent performs
successful unlearning while still preserving generalization in a synthetic
Gaussian-mixture setting.

</details>


### [441] [Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References](https://arxiv.org/abs/2510.14719)
*Hongzheng Chen,Bin Fan,Alexander Collins,Bastian Hagedorn,Evghenii Gaburov,Masahiro Masuda,Matthew Brookhart,Chris Sullivan,Jason Knight,Zhiru Zhang,Vinod Grover*

Main category: cs.LG

TL;DR: Tawa是一个编译器，可以从高级、基于块的程序生成高性能、warp专业化的代码，解决了SIMT编程模型与GPU硬件之间的差距。


<details>
  <summary>Details</summary>
Motivation: 传统的SIMT编程模型与GPU的异步数据流硬件不匹配，导致了可编程性差距，迫使开发人员手动管理复杂的低级通信和软件管道，以实现最佳性能。

Method: Tawa使用一种新的IR抽象，异步引用（aref），来表达warp级别的通信，而无需暴露低级硬件细节。它自动将程序划分为生产者-消费者角色，并管理数据流管道。

Result: 在NVIDIA H100 GPU上，Tawa在代表性LLM内核上的评估显示，与高度优化的cuBLAS GEMM内核相比，Tawa实现了高达1.1倍的加速。对于注意力工作负载，Tawa的加速比为1.2倍，并且编程工作量大大减少。

Conclusion: Tawa通过提供一种高级编程方法，自动生成高性能的warp专业化代码，成功地弥合了GPU编程的差距，并提供了与手动优化代码相当的性能。

Abstract: Modern GPUs feature specialized hardware units that enable high-performance,
asynchronous dataflow execution. However, the conventional SIMT programming
model is fundamentally misaligned with this task-parallel hardware, creating a
significant programmability gap. While hardware-level warp specialization is
the key to unlocking peak performance, it forces developers to manually
orchestrate complex, low-level communication and software pipelines--a process
that is labor-intensive, error-prone, and unsustainable. To address this
challenge, we present Tawa, an automated compiler that systematically generates
high-performance, warp-specialized code from a high-level, tile-based program.
Central to our approach is a novel IR abstraction, asynchronous references
(aref), which expresses warp-level communication without exposing low-level
hardware details. Using this abstraction, Tawa automatically partitions
programs into producer-consumer roles and manages the intricate dataflow
pipeline, relieving developers of invasive kernel rewriting. Evaluation on
NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers
high hardware utilization, achieving up to 1.1$\times$ speedup over highly
optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains
1.2$\times$ speedup over Triton and matches the performance of the
hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming
effort.

</details>


### [442] [K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding](https://arxiv.org/abs/2510.13891)
*Yifeng Yao,Yike Yun,Jing Wang,Huishuai Zhang,Dongyan Zhao,Ke Tian,Zhihao Wang,Minghui Qiu,Tao Wang*

Main category: cs.LG

TL;DR: K-frames 是一种新颖的场景驱动关键帧选择范例，通过预测语义连贯、查询相关的视频片段来保留时间连续性，并能灵活满足各种预算需求。


<details>
  <summary>Details</summary>
Motivation: 现有的长视频理解方法受限于上下文窗口和计算成本，并且现有的关键帧选择方法（如文本-帧检索或基于强化学习的帧优化）通常会产生稀疏且时间上不连续的帧，忽略了场景的连续性，并且在多尺度帧选择方面缺乏灵活性。

Method: K-frames 通过预测语义连贯、查询相关的视频片段来保留时间连续性，从而实现任何 K 帧的关键帧选择。该方法首先引入了一个包含 200K 视频片段的数据集 PeakClips，然后通过三个阶段的渐进式课程进行训练：两个监督微调阶段（用于时间定位和关键片段感知）和一个强化学习阶段（用于直接优化下游任务的场景驱动预测策略）。

Result: 在主要的长视频理解基准测试中的大量实验表明，K-frames 在各种尺度上都为关键帧选择提供了一种有效、可解释且即插即用的解决方案。

Conclusion: K-frames 克服了长视频理解中的上下文窗口、计算成本和关键帧选择的挑战，通过预测连贯的视频片段来保留时间连续性，并实现了灵活的多尺度帧选择。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
capabilities in image understanding, but long-video are constrained by context
windows and computational cost. Uniform frame sampling often leads to
substantial information loss. Meanwhile existing keyframe selection methods
such as text-frame retrieval or RL-based frame optimization typically yield
sparse and temporally disjointed frames, overlooking scene continuity and
lacking flexibility for multi-scale frame selection. To address these
limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe
selection that preserves temporal continuity. Instead of selecting individual
frames, K-frames predicts semantically coherent, query-relevant clips, which
enables any-k keyframes selection to meet diverse user budgets. To achieve this
approach, we first introduce PeakClips, a dataset of 200K video highlights
conditioned by query. Building on this dataset, K-frames learns clip2frame
selection using a three-stage progressive curriculum. It involves two
Supervised Fine-Tuning stages for temporal grounding and key-clip perception,
followed by a Reinforcement Learning stage that directly optimizes the
scene-driven prediction policy for downstream task without further annotations.
Extensive experiments on major long-video understanding benchmarks demonstrate
that K-frames provides an effective, interpretable, and plug-and-play solution
for keyframe selection at various scales. Our dataset and model will be
available.

</details>


### [443] [Near-Optimal Regret-Queue Length Tradeoff in Online Learning for Two-Sided Markets](https://arxiv.org/abs/2510.14097)
*Zixian Yang,Sushil Mahavir Varma,Lei Ying*

Main category: cs.LG

TL;DR: 该研究提出了一个新颖的在线学习定价策略，用于优化双边市场中的平台利润和队列长度，并在理论上证明了其近乎最优的性能。


<details>
  <summary>Details</summary>
Motivation: 设计定价和匹配算法以最大化平台利润，同时保持合理的队列长度，特别是在需求和供应曲线未知的情况下。

Method: 提出了一种新颖的在线学习定价策略，并分析了其在后悔值、平均队列长度和最大队列长度方面的性能，同时考虑了动态和概率组件。

Result: 证明了在 $\gamma inom{1/6}$ 范围内，悔值、平均队列长度和最大队列长度之间存在 $\tilde{O}(T^{1-\gamma})$、$\tilde{O}(T^{\gamma/2})$ 和 $\tilde{O}(T^{\gamma})$ 的权衡，并匹配了已知需求和供应曲线的最优结果。

Conclusion: 所提出的在线学习定价策略在理论上接近最优，并且通过动态和概率组件解决了学习速度和队列长度之间的权衡问题。

Abstract: We study a two-sided market, wherein, price-sensitive heterogeneous customers
and servers arrive and join their respective queues. A compatible
customer-server pair can then be matched by the platform, at which point, they
leave the system. Our objective is to design pricing and matching algorithms
that maximize the platform's profit, while maintaining reasonable queue
lengths. As the demand and supply curves governing the price-dependent arrival
rates may not be known in practice, we design a novel online-learning-based
pricing policy and establish its near-optimality. In particular, we prove a
tradeoff among three performance metrics: $\tilde{O}(T^{1-\gamma})$ regret,
$\tilde{O}(T^{\gamma/2})$ average queue length, and $\tilde{O}(T^{\gamma})$
maximum queue length for $\gamma \in (0, 1/6]$, significantly improving over
existing results [1]. Moreover, barring the permissible range of $\gamma$, we
show that this trade-off between regret and average queue length is optimal up
to logarithmic factors under a class of policies, matching the optimal one as
in [2] which assumes the demand and supply curves to be known. Our proposed
policy has two noteworthy features: a dynamic component that optimizes the
tradeoff between low regret and small queue lengths; and a probabilistic
component that resolves the tension between obtaining useful samples for fast
learning and maintaining small queue lengths.

</details>


### [444] [Multi-View Semi-Supervised Label Distribution Learning with Local Structure Complementarity](https://arxiv.org/abs/2510.13917)
*Yanshan Xiao,Kaihong Wu,Bo Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为MVSS-LDL的多视图半监督标签分布学习方法，该方法利用并结合了多个视图的局部近邻结构信息，以提高分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的标签分布学习（LDL）方法主要针对单视图LDL问题，而忽略了多视图LDL问题，特别是同时包含标注和未标注数据的情况。

Method: 首先，探索单个视图v的局部结构，计算k近邻。然后，通过引入其他视图的近邻信息来补充每个样本的近邻信息，以获得更全面的描述。最后，构建了一个基于图学习的多视图半监督LDL模型，利用互补的局部近邻结构信息。

Result: 数值研究表明，MVSS-LDL在分类性能上明显优于现有的单视图LDL方法。

Conclusion: MVSS-LDL是首个解决多视图半监督LDL问题的尝试，并通过利用多视图的局部结构互补性，有效地提升了分类性能。

Abstract: Label distribution learning (LDL) is a paradigm that each sample is
associated with a label distribution. At present, the existing approaches are
proposed for the single-view LDL problem with labeled data, while the
multi-view LDL problem with labeled and unlabeled data has not been considered.
In this paper, we put forward the multi-view semi-supervised label distribution
learning with local structure complementarity (MVSS-LDL) approach, which
exploits the local nearest neighbor structure of each view and emphasizes the
complementarity of local nearest neighbor structures in multiple views.
Specifically speaking, we first explore the local structure of view $v$ by
computing the $k$-nearest neighbors. As a result, the $k$-nearest neighbor set
of each sample $\boldsymbol{x}_i$ in view $v$ is attained. Nevertheless, this
$k$-nearest neighbor set describes only a part of the nearest neighbor
information of sample $\boldsymbol{x}_i$. In order to obtain a more
comprehensive description of sample $\boldsymbol{x}_i$'s nearest neighbors, we
complement the nearest neighbor set in view $v$ by incorporating sample
$\boldsymbol{x}_i$'s nearest neighbors in other views. Lastly, based on the
complemented nearest neighbor set in each view, a graph learning-based
multi-view semi-supervised LDL model is constructed. By considering the
complementarity of local nearest neighbor structures, different views can
mutually provide the local structural information to complement each other. To
the best of our knowledge, this is the first attempt at multi-view LDL.
Numerical studies have demonstrated that MVSS-LDL attains explicitly better
classification performance than the existing single-view LDL methods.

</details>


### [445] [Weight Weaving: Parameter Pooling for Data-Free Model Merging](https://arxiv.org/abs/2510.13921)
*Levy Chaves,Eduardo Valle,Sandra Avila*

Main category: cs.LG

TL;DR: 模型合并技术通过参数集成，可以经济高效且数据高效地组合专门的深度神经网络，但其超参数λ的设置在无数据的情况下缺乏原则性方法。为此，我们提出了一种名为Weight Weaving的即插即用技术，它通过用户定义的聚合函数（如平均、随机选择或现有模型合并方法）在λ值搜索空间中聚合模型权重，无需访问评估数据，并提高了现有模型合并方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的模型合并方法在设置用于加权模型贡献的超参数λ时，通常需要访问评估数据，这在实际中是不可行的。因此，需要一种无需评估数据即可有效设置λ的方法。

Method: Weight Weaving是一种即插即用的技术，它利用用户定义的聚合函数（例如平均、随机选择或现有模型合并方法）在λ值搜索空间中聚合模型权重，从而在不要求访问评估数据的情况下，为模型合并提供超参数设置。

Result: 在三种设置（视觉多任务学习、视觉持续学习和域泛化）下的三种ViT变体上验证了Weight Weaving。该方法在无数据的情况下，将几 M种模型合并方法的性能平均提高了15.9个百分点。

Conclusion: Weight Weaving是一种模块化、无需评估数据即可设置模型合并超参数λ的技术，它能够与其他模型合并方法正交，并显著提高其性能。

Abstract: Model merging provides a cost-effective and data-efficient combination of
specialized deep neural networks through parameter integration. This technique
leverages expert models across downstream tasks without requiring retraining.
Most model merging approaches critically depend on scaling hyper-parameters
$\lambda$, which weight each model's contribution globally or individually.
Principled approaches for setting scaling factors without accessing any data
(data-free) are scarce, often leading researchers to tune $\lambda$ using
privileged data from the evaluation set, which is obviously unfeasible in
practice. To address this limitation, we introduce Weight Weaving, a
plug-and-play technique that pools model weights across $\lambda$ values search
space using user-defined pooling functions, such as averaging, random
selection, or even existing model merging methods. Our method demonstrates high
modularity, imposing minimal constraints on the search space. It operates
orthogonally to existing model merging methods and eliminates evaluation data
requirements. We validate Weight Weaving across three ViT variants in three
experimental setups: vision multi-task learning, vision continual learning, and
domain generalization. Our method consistently improves the performance of
several model merging methods, achieving average accuracy gains of up to 15.9
percentage points in a data-free setting.

</details>


### [446] [LTR-ICD: A Learning-to-Rank Approach for Automatic ICD Coding](https://arxiv.org/abs/2510.13922)
*Mohammad Mansoori,Amira Soliman,Farzaneh Etminani*

Main category: cs.LG

TL;DR: 本研究将ICD编码分配和排序问题视为一个分类和排序任务，而不是仅仅的分类任务，从而更好地考虑了编码的顺序。


<details>
  <summary>Details</summary>
Motivation: 临床笔记中的非结构化文本需要正确的ICD编码，但现有方法忽略了编码顺序，这对于医疗诊断和报销至关重要。

Method: 将ICD编码分配和排序问题从分类任务的角度转变为检索任务，并将其表述为分类和排序任务。

Result: 该模型在正确排序主要诊断代码方面的准确率为47%，优于现有分类方法的20%。在分类指标方面，本研究提出的模型取得了0.6065的微观F1分数和0.2904的宏观F1分数，超过了先前最佳模型的0.597和0.2660。

Conclusion: 提出的框架在识别高优先级代码方面比其他方法具有更强的能力。

Abstract: Clinical notes contain unstructured text provided by clinicians during
patient encounters. These notes are usually accompanied by a sequence of
diagnostic codes following the International Classification of Diseases (ICD).
Correctly assigning and ordering ICD codes are essential for medical diagnosis
and reimbursement. However, automating this task remains challenging.
State-of-the-art methods treated this problem as a classification task, leading
to ignoring the order of ICD codes that is essential for different purposes. In
this work, as a first attempt, we approach this task from a retrieval system
perspective to consider the order of codes, thus formulating this problem as a
classification and ranking task. Our results and analysis show that the
proposed framework has a superior ability to identify high-priority codes
compared to other methods. For instance, our model accuracy in correctly
ranking primary diagnosis codes is 47%, compared to 20% for the
state-of-the-art classifier. Additionally, in terms of classification metrics,
the proposed model achieves a micro- and macro-F1 scores of 0.6065 and 0.2904,
respectively, surpassing the previous best model with scores of 0.597 and
0.2660.

</details>


### [447] [Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems](https://arxiv.org/abs/2510.13972)
*George Webber,Andrew J. Reader*

Main category: cs.LG

TL;DR: 本文提出了一种新的数据保真度损失函数——分布一致性（DC）损失，用于解决逆问题中的信号恢复挑战。与传统的逐点匹配损失（如MSE）不同，DC损失通过评估观测测量值与当前估计值所隐含的噪声分布的一致性来评估数据保真度，从而避免过拟合噪声。


<details>
  <summary>Details</summary>
Motivation: 传统的逐点数据保真度损失函数（如MSE）在处理含噪声测量数据时，容易导致过拟合噪声，影响逆问题的信号恢复效果。需要一种新的方法来更有效地平衡先验假设和数据保真度。

Method: 本文提出并采用分布一致性（DC）损失函数，通过模型驱动的概率得分，在分布层面而非逐点层面进行数据保真度评估。DC损失可以直接替换现有的数据一致性项，兼容现代正则化器，优化方式与传统损失相同，并且即使没有先验知识也能避免过拟合噪声。

Result: 在图像去噪（结合深度图像先验）和医学图像重建（处理泊松噪声）等应用中，使用DC损失替代MSE损失，前者无需提前停止即可获得更高的PSNR，后者则减少了高度迭代重建的伪影，并提升了手工设计正则化的效果。

Conclusion: DC损失是一种基于统计学原理且性能优越的数据保真度损失函数，为解决逆问题提供了一种有前景的替代传统损失函数的方法。

Abstract: Recovering true signals from noisy measurements is a central challenge in
inverse problems spanning medical imaging, geophysics, and signal processing.
Current solutions balance prior assumptions regarding the true signal
(regularization) with agreement to noisy measured data (data-fidelity).
Conventional data-fidelity loss functions, such as mean-squared error (MSE) or
negative log-likelihood, seek pointwise agreement with noisy measurements,
often leading to overfitting to noise. In this work, we instead evaluate
data-fidelity collectively by testing whether the observed measurements are
statistically consistent with the noise distributions implied by the current
estimate. We adopt this aggregated perspective and introduce distributional
consistency (DC) loss, a data-fidelity objective that replaces pointwise
matching with distribution-level calibration using model-based probability
scores for each measurement. DC loss acts as a direct and practical plug-in
replacement for standard data consistency terms: i) it is compatible with
modern regularizers, ii) it is optimized in the same way as traditional losses,
and iii) it avoids overfitting to measurement noise even without the use of
priors. Its scope naturally fits many practical inverse problems where the
measurement-noise distribution is known and where the measured dataset consists
of many independent noisy values. We demonstrate efficacy in two key example
application areas: i) in image denoising with deep image prior, using DC
instead of MSE loss removes the need for early stopping and achieves higher
PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss
reduces artifacts in highly-iterated reconstructions and enhances the efficacy
of hand-crafted regularization. These results position DC loss as a
statistically grounded, performance-enhancing alternative to conventional
fidelity losses for inverse problems.

</details>


### [448] [BitNet Distillation](https://arxiv.org/abs/2510.13998)
*Xun Wu,Shaohan Huang,Wenhui Wang,Ting Song,Li Dong,Yan Xia,Furu Wei*

Main category: cs.LG

TL;DR: BitNet Distillation (BitDistill)是一种轻量级流水线，可将全精度语言模型微调为1.58位精度，以实现高任务特定性能和低计算成本。


<details>
  <summary>Details</summary>
Motivation: BitDistill旨在通过将现有的全精度语言模型（如Qwen）微调为1.58位精度（权重为{-1, 0, 1}），以适应特定下游任务，从而在保持强大任务特定性能的同时，最大限度地降低计算成本。

Method: BitDistill流水线整合了三种关键技术：BitNet中引入的SubLN模块；基于MiniLM的多头注意力蒸馏；以及作为关键预热步骤的持续预训练，以解决微调后的全精度模型与1.58位模型在特定任务上性能差距的可扩展性问题。

Result: 实验结果表明，BitDistill在模型尺寸上实现了与全精度对应模型相当的性能，同时实现了高达10倍的内存节省和2.65倍的CPU推理加速。

Conclusion: BitDistill成功地将全精度语言模型压缩为1.58位精度，在保持高性能的同时显著提高了内存效率和推理速度，为资源受限环境下的LLM应用提供了有效解决方案。

Abstract: In this paper, we present BitNet Distillation (BitDistill), a lightweight
pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into
1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream
tasks, achieving strong task-specific performance with minimal computational
cost. Specifically, BitDistill incorporates three key techniques: the SubLN
module, as introduced in BitNet; multi-head attention distillation, based on
MiniLM; and continual pre-training, which serves as a crucial warm-up step to
mitigate the scalability issue of the performance gap between finetuned
full-precision and 1.58-bit LLMs on specific tasks. Experimental results show
that BitDistill achieves performance comparable to the full-precision
counterpart models across model size, while enabling up to 10x memory savings
and 2.65x faster inference on CPUs. Code is available at
https://github.com/microsoft/BitNet.

</details>


### [449] [REAP the Experts: Why Pruning Prevails for One-Shot MoE compression](https://arxiv.org/abs/2510.13999)
*Mike Lasby,Ivan Lazarevich,Nish Sinnadurai,Sean Lie,Yani Ioannou,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: SMoE模型的参数量大导致内存开销大，但现有研究在压缩专家方面存在不足。本文提出了一种名为REAP的新型剪枝方法，通过结合门控值和专家激活范数来选择性地剪枝专家，并在生成任务上取得了优于专家合并和其他剪枝方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有SMoE模型存在内存开销大的问题，需要对专家进行压缩。之前的研究倾向于在判别性任务上进行专家合并，但对于生成任务，本文认为专家剪枝是更优策略。

Method: 提出了一种名为Router-weighted Expert Activation Pruning (REAP) 的新型剪枝方法，该方法综合考虑了路由器的门控值和专家的激活范数来决定剪枝哪些专家。

Result: 在代码生成和工具调用等生成任务上，REAP相较于专家合并和其他剪枝方法，尤其是在50%压缩率下，表现出了更优越的性能。对于Qwen3-Coder-480B和Kimi-K2模型，REAP实现了接近无损的压缩效果。

Conclusion: REAP是一种有效的SMoE模型压缩方法，尤其适用于生成任务，能在显著降低模型大小的同时保持高性能。

Abstract: Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient
pre-training and low latency but their large parameter counts create
significant memory overhead, motivating research into expert compression.
Contrary to recent findings favouring expert merging on discriminative
benchmarks, we demonstrate that expert pruning is a superior strategy for
generative tasks. We prove that merging introduces an irreducible error by
causing a "functional subspace collapse", due to the loss of the router's
independent, input-dependent control over experts. Leveraging this insight, we
propose Router-weighted Expert Activation Pruning (REAP), a novel pruning
criterion that considers both router gate-values and expert activation norms.
Across a diverse set of SMoE models ranging from 20B to 1T parameters, REAP
consistently outperforms merging and other pruning methods on generative
benchmarks, especially at 50% compression. Notably, our method achieves
near-lossless compression on code generation and tool-calling tasks with
Qwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts.

</details>


### [450] [Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling](https://arxiv.org/abs/2510.14007)
*Bálint László Szarvas,Maksim Zhdanov*

Main category: cs.LG

TL;DR: CSCNNs的核基不完整，限制了模型表达能力。我们提出条件性Clifford-可操纵核，通过输入特征场的等变表示来增强核，并在多个PDE预测任务中提高了表达能力。


<details>
  <summary>Details</summary>
Motivation: CSCNNs的核基不完整，限制了模型表达能力。

Method: 提出条件性Clifford-可操纵核，通过输入特征场的等变表示来增强核，并推导了输入相关核的等变约束，通过隐式参数化求解。

Result: 在流体动力学和相对论电动力学等多个PDE预测任务中，CSCNNs的核基不完整，限制了模型表达能力。但使用条件性Clifford-可操纵核的框架在这些任务中表现优于基线方法。

Conclusion: 条件性Clifford-可操纵核框架提高了CSCNNs的表达能力，并在多个PDE预测任务中取得了优于基线方法的性能。

Abstract: Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows
incorporating equivariance to arbitrary pseudo-Euclidean groups, including
isometries of Euclidean space and Minkowski spacetime. In this work, we
demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the
model expressivity. To address this issue, we propose Conditional
Clifford-Steerable Kernels, which augment the kernels with equivariant
representations computed from the input feature field. We derive the
equivariance constraint for these input-dependent kernels and show how it can
be solved efficiently via implicit parameterization. We empirically demonstrate
an improved expressivity of the resulting framework on multiple PDE forecasting
tasks, including fluid dynamics and relativistic electrodynamics, where our
method consistently outperforms baseline methods.

</details>


### [451] [FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients](https://arxiv.org/abs/2510.14054)
*Fatih Ilhan,Selim Furkan Tekin,Tiansheng Huang,Gaowen Liu,Ramana Kompella,Greg Eisenhauer,Yingyan Celine Lin,Calton Pu,Ling Liu*

Main category: cs.LG

TL;DR: FedHFT是一个高效且个性化的联合微调框架，用于解决LLM在下游任务和领域特定数据集上的个性化应用中，因数据限制/异构性和计算资源限制而面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于数据保密性、隐私要求以及边缘设备等计算资源的限制，导致微调LLM时面临数据有限/异构和计算资源不一的挑战。

Method: FedHFT框架提出了两种方法：1. 引入掩码适配器（mixture of masked adapters）来处理客户端间的资源异构性，实现分布式设置下跨多个客户端的高性能预训练语言模型协同微调，同时保护本地数据隐私。2. 引入双层优化方法（bi-level optimization）来处理基于掩码个性化和客户端聚类的非独立同分布（non-iid）数据分布。

Result: 实验证明，FedHFT在数据和资源异构的多种自然语言理解任务上，相比于其他异构联邦学习方法，在性能和效率上都有显著提升。

Conclusion: FedHFT能够有效地解决LLM在数据和资源异构环境下的联邦微调问题，并在性能和效率上取得显著优势。

Abstract: Fine-tuning pre-trained large language models (LLMs) has become a common
practice for personalized natural language understanding (NLU) applications on
downstream tasks and domain-specific datasets. However, there are two main
challenges: (i) limited and/or heterogeneous data for fine-tuning due to
proprietary data confidentiality or privacy requirements, and (ii) varying
computation resources available across participating clients such as edge
devices. This paper presents FedHFT - an efficient and personalized federated
fine-tuning framework to address both challenges. First, we introduce a mixture
of masked adapters to handle resource heterogeneity across participating
clients, enabling high-performance collaborative fine-tuning of pre-trained
language model(s) across multiple clients in a distributed setting, while
keeping proprietary data local. Second, we introduce a bi-level optimization
approach to handle non-iid data distribution based on masked personalization
and client clustering. Extensive experiments demonstrate significant
performance and efficiency improvements over various natural language
understanding tasks under data and resource heterogeneity compared to
representative heterogeneous federated learning methods.

</details>


### [452] [Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training](https://arxiv.org/abs/2510.14009)
*Jie Hao,Xiaochuan Gong,Jie Xu,Zhengdao Wang,Mingrui Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种噪声自适应的层级学习率方案，用于加速深度神经网络（DNN）的训练，相比于固定学习率的方法，可以显著提升训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有的几何感知优化算法（如Muon）在训练DNN方面取得了成功，但它们在同一范数组内的不同层之间施加固定的学习率，这可能无法充分利用层级间的局部曲率差异，从而影响训练效率。

Method: 提出了一种噪声自适应层级学习率方案，该方案在几何感知优化算法的基础上，能够动态估计梯度方差，并据此为同一组内的层级分配随时间变化的、自适应噪声的学习率。

Result: 理论分析表明，该算法可以达到更快的收敛速度。在LLaMA和GPT等Transformer架构上的实证结果表明，该方法比最先进的优化器收敛更快。

Conclusion: 该论文提出的噪声自适应层级学习率方案能够有效加速DNN的训练，并在Transformer架构上取得了优于现有最先进优化器的性能。

Abstract: Geometry-aware optimization algorithms, such as Muon, have achieved
remarkable success in training deep neural networks (DNNs). These methods
leverage the underlying geometry of DNNs by selecting appropriate norms for
different layers and updating parameters via norm-constrained linear
minimization oracles (LMOs). However, even within a group of layers associated
with the same norm, the local curvature can be heterogeneous across layers and
vary dynamically over the course of training. For example, recent work shows
that sharpness varies substantially across transformer layers and throughout
training, yet standard geometry-aware optimizers impose fixed learning rates to
layers within the same group, which may be inefficient for DNN training.
  In this paper, we introduce a noise-adaptive layerwise learning rate scheme
on top of geometry-aware optimization algorithms and substantially accelerate
DNN training compared to methods that use fixed learning rates within each
group. Our method estimates gradient variance in the dual norm induced by the
chosen LMO on the fly, and uses it to assign time-varying noise-adaptive
layerwise learning rates within each group. We provide a theoretical analysis
showing that our algorithm achieves a sharp convergence rate. Empirical results
on transformer architectures such as LLaMA and GPT demonstrate that our
approach achieves faster convergence than state-of-the-art optimizers.

</details>


### [453] [Incentive-Based Federated Learning](https://arxiv.org/abs/2510.14208)
*Chanuka A. S. Hewa Kaluannakkage,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 参与者困境限制了联邦学习的适应性，需要经济学、博弈论、区块链和深度强化学习的激励机制来克服。论文提出了一个全面的分类，并讨论了医疗保健、智能基础设施、车联网和基于区块链的去中心化系统等应用。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在协作模型训练中很有前景，但由于参与者困境（参与者可能不愿意参与或搭便车）而受到实际适应性的限制。

Method: 研究了经济学、博弈论、区块链和深度强化学习的概念，以设计联邦学习的激励机制。提出了一个涵盖中心化和去中心化架构的分类，并从应用角度探讨了医疗保健、智能基础设施、车联网和基于区块链的去中心化系统。

Result: 设计了激励机制，以解决联邦学习中的参与者困境问题。这些机制来自经济学、博弈论、区块链和深度强化学习，并已应用于各种新兴的工业应用。

Conclusion: 设计良好的激励机制对于联邦学习的实际成功至关重要。虽然已经出现了一些有希望的解决方案，但要建立真正可持续、公平和健壮的联邦学习生态系统仍然存在重大挑战。

Abstract: Federated learning promises to revolutionize machine learning by enabling
collaborative model training without compromising data privacy. However,
practical adaptability can be limited by critical factors, such as the
participation dilemma. Participating entities are often unwilling to contribute
to a learning system unless they receive some benefits, or they may pretend to
participate and free-ride on others. This chapter identifies the fundamental
challenges in designing incentive mechanisms for federated learning systems. It
examines how foundational concepts from economics and game theory can be
applied to federated learning, alongside technology-driven solutions such as
blockchain and deep reinforcement learning. This work presents a comprehensive
taxonomy that thoroughly covers both centralized and decentralized
architectures based on the aforementioned theoretical concepts. Furthermore,
the concepts described are presented from an application perspective, covering
emerging industrial applications, including healthcare, smart infrastructure,
vehicular networks, and blockchain-based decentralized systems. Through this
exploration, this chapter demonstrates that well-designed incentive mechanisms
are not merely optional features but essential components for the practical
success of federated learning. This analysis reveals both the promising
solutions that have emerged and the significant challenges that remain in
building truly sustainable, fair, and robust federated learning ecosystems.

</details>


### [454] [Context-Selective State Space Models: Feedback is All You Need](https://arxiv.org/abs/2510.14027)
*Riccardo Zattra,Giacomo Baggio,Umberto Casti,Augusto Ferrante,Francesco Ticozzi*

Main category: cs.LG

TL;DR: COFFEE是一个新颖的、具有状态反馈的时间变化状态空间模型（SSM），通过上下文自适应地选择信息，解决了Transformer的二次复杂性和长程依赖性问题。与S6模型相比，COFFEE在参数量和训练数据量减少两个数量级的情况下，在归纳头任务上达到了近乎完美的准确率，并在MNIST上取得了97%的准确率，证明了状态反馈在构建高效序列模型中的重要作用。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在处理长序列时存在二次复杂性和长程依赖性困难的问题，需要更高效的替代方案。

Method: 提出COFFEE模型，一种新的时间变化SSM，通过状态反馈实现上下文依赖的自适应选择。与S6不同，COFFEE的自适应选择机制基于内部状态而非仅当前输入。此外，COFFEE采用了更高效的模型参数化方法，消除了S6中的冗余。

Result: 在归纳头任务上，COFFEE的参数量和训练序列数量比S6少两个数量级，准确率接近完美。在MNIST上，COFFEE的准确率达到97%，显著优于S6，且参数量仅为3585。

Conclusion: 状态反馈是构建可扩展、高效序列模型的关键机制，COFFEE模型证明了其有效性。

Abstract: Transformers, powered by the attention mechanism, are the backbone of most
foundation models, yet they suffer from quadratic complexity and difficulties
in dealing with long-range dependencies in the input sequence. Recent work has
shown that state space models (SSMs) provide an efficient alternative, with the
S6 module at the core of the Mamba architecture achieving state-of-the-art
results on long-sequence benchmarks. In this paper, we introduce the COFFEE
(COntext From FEEdback) model, a novel time-varying SSM that incorporates state
feedback to enable context-dependent selectivity, while still allowing for
parallel implementation. Whereas the selectivity mechanism of S6 only depends
on the current input, COFFEE computes it from the internal state, which serves
as a compact representation of the sequence history. This shift allows the
model to regulate its dynamics based on accumulated context, improving its
ability to capture long-range dependencies. In addition to state feedback, we
employ an efficient model parametrization that removes redundancies present in
S6 and leads to a more compact and trainable formulation. On the induction head
task, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer
parameters and training sequences compared to S6. On MNIST, COFFEE largely
outperforms S6 within the same architecture, reaching 97% accuracy with only
3585 parameters. These results showcase the role of state feedback as a key
mechanism for building scalable and efficient sequence models.

</details>


### [455] [Learning Wireless Interference Patterns: Decoupled GNN for Throughput Prediction in Heterogeneous Multi-Hop p-CSMA Networks](https://arxiv.org/abs/2510.14137)
*Faezeh Dehghan Tarzjani,Bhaskar Krishnamachari*

Main category: cs.LG

TL;DR: GNNs 在无线网络吞吐量预测中存在局限性，提出了一种新的 D-GCN 架构来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 传统的 p-persistent CSMA 分析方法在异构多跳无线网络中存在局限性，现有模型预测不准确，精确的马尔可夫链分析计算成本过高，促使研究者探索基于 GNN 的方法。

Method: 提出了一种新的 D-GCN 架构，它将节点自身传输概率的处理与邻居干扰效应分离开来，并使用可学习的注意力机制替代平均聚合，以捕捉复杂的多跳干扰模式。

Result: D-GCN 达到了 3.3% 的 NMAE，优于其他基线方法，并且在精确分析方法不可行时仍然具有可处理性，还能实现接近理论最优的网络优化。

Conclusion: D-GCN 是一种有效且可扩展的 GNN 架构，可用于预测异构多跳无线网络中的吞吐量，并能实现网络优化。

Abstract: The p-persistent CSMA protocol is central to random-access MAC analysis, but
predicting saturation throughput in heterogeneous multi-hop wireless networks
remains a hard problem. Simplified models that assume a single, shared
interference domain can underestimate throughput by 48--62\% in sparse
topologies. Exact Markov-chain analyses are accurate but scale exponentially in
computation time, making them impractical for large networks. These
computational barriers motivate structural machine learning approaches like
GNNs for scalable throughput prediction in general network topologies. Yet
off-the-shelf GNNs struggle here: a standard GCN yields 63.94\% normalized mean
absolute error (NMAE) on heterogeneous networks because symmetric normalization
conflates a node's direct interference with higher-order, cascading effects
that pertain to how interference propagates over the network graph.
  Building on these insights, we propose the Decoupled Graph Convolutional
Network (D-GCN), a novel architecture that explicitly separates processing of a
node's own transmission probability from neighbor interference effects. D-GCN
replaces mean aggregation with learnable attention, yielding interpretable,
per-neighbor contribution weights while capturing complex multihop interference
patterns. D-GCN attains 3.3\% NMAE, outperforms strong baselines, remains
tractable even when exact analytical methods become computationally infeasible,
and enables gradient-based network optimization that achieves within 1\% of
theoretical optima.

</details>


### [456] [CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations](https://arxiv.org/abs/2510.14049)
*Guangyi Chen,Yunlong Deng,Peiyuan Zhu,Yan Li,Yifan Sheng,Zijian Li,Kun Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一个新的因果表征学习（CRL）基准，使用高保真模拟视觉数据，以解决现有评估方法在真实性和精确性之间面临的困境。


<details>
  <summary>Details</summary>
Motivation: 现有因果表征学习（CRL）的评估方法，如依赖于简化的合成数据集或下游任务表现，在真实性和评估精确性之间存在两难。

Method: 提出一个新的基准，使用包含约20万张图像和300万个视频帧的高保真模拟视觉数据，涵盖静态图像生成、动态物理模拟、机器人操作和交通状况分析等24个子场景，并提供灵活的因果结构访问。

Result: 利用该基准，评估了代表性的CRL方法，并提供了实证见解，以帮助从业者和新手选择或扩展合适的CRL框架。

Conclusion: 提出的新基准通过提供包含真实视觉复杂性和可获取的因果生成过程的模拟数据，有望弥合严格评估与现实应用之间的差距。

Abstract: Causal Representation Learning (CRL) aims to uncover the data-generating
process and identify the underlying causal variables and relations, whose
evaluation remains inherently challenging due to the requirement of known
ground-truth causal variables and causal structure. Existing evaluations often
rely on either simplistic synthetic datasets or downstream performance on
real-world tasks, generally suffering a dilemma between realism and evaluative
precision. In this paper, we introduce a new benchmark for CRL using
high-fidelity simulated visual data that retains both realistic visual
complexity and, more importantly, access to ground-truth causal generating
processes. The dataset comprises around 200 thousand images and 3 million video
frames across 24 sub-scenes in four domains: static image generation, dynamic
physical simulations, robotic manipulations, and traffic situation analysis.
These scenarios range from static to dynamic settings, simple to complex
structures, and single to multi-agent interactions, offering a comprehensive
testbed that hopefully bridges the gap between rigorous evaluation and
real-world applicability. In addition, we provide flexible access to the
underlying causal structures, allowing users to modify or configure them to
align with the required assumptions in CRL, such as available domain labels,
temporal dependencies, or intervention histories. Leveraging this benchmark, we
evaluated representative CRL methods across diverse paradigms and offered
empirical insights to assist practitioners and newcomers in choosing or
extending appropriate CRL frameworks to properly address specific types of real
problems that can benefit from the CRL perspective. Welcome to visit our:
Project page:https://causal-verse.github.io/,
Dataset:https://huggingface.co/CausalVerse.

</details>


### [457] [On the expressivity of sparse maxout networks](https://arxiv.org/abs/2510.14068)
*Moritz Grillo,Tobias Hofmann*

Main category: cs.LG

TL;DR: 稀疏maxout网络的可表达性与虚拟多面体相关联，并推导了多面体维数的紧界，以分析网络深度和宽度对可表达性的影响。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏maxout网络的表达能力，其神经元具有固定数量的输入和maxout激活函数，这模仿了卷积或图神经网络的特点。

Method: 将可计算函数与一类虚拟多面体联系起来，推导出多面体维数的紧界，并构建深度层级结构。

Result: 发现具有足够深度的稀疏maxout网络具有通用性，但如果深度不足，固定入度约束下的宽度无法弥补稀疏性。

Conclusion: 在深度不足的情况下，网络的宽度不能补偿稀疏性带来的限制。

Abstract: We study the expressivity of sparse maxout networks, where each neuron takes
a fixed number of inputs from the previous layer and employs a, possibly
multi-argument, maxout activation. This setting captures key characteristics of
convolutional or graph neural networks. We establish a duality between
functions computable by such networks and a class of virtual polytopes, linking
their geometry to questions of network expressivity. In particular, we derive a
tight bound on the dimension of the associated polytopes, which serves as the
central tool for our analysis. Building on this, we construct a sequence of
depth hierarchies. While sufficiently deep sparse maxout networks are
universal, we prove that if the required depth is not reached, width alone
cannot compensate for the sparsity of a fixed indegree constraint.

</details>


### [458] [Exploratory Causal Inference in SAEnce](https://arxiv.org/abs/2510.14073)
*Tommaso Mencattini,Riccardo Cadei,Francesco Locatello*

Main category: cs.LG

TL;DR: 本研究提出了一种从非结构化数据中发现未知因果效应的新方法，解决了传统随机对照试验的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统随机对照试验依赖手工假设和昂贵的分析，限制了大规模因果效应估计，并可能受限于流行但片面的假设。本研究旨在直接从数据中发现未知的处理效应。

Method: 研究将试验中的非结构化数据通过预训练的基础模型转化为有意义的表示，并利用稀疏自编码器进行解释。为了解决多重检验问题和效应纠缠的挑战，提出了一种名为神经效应搜索（Neural Effect Search）的新型递归程序，通过渐进分层来解决这些问题。

Result: 在半合成实验上评估了算法的稳健性，并在实验生态学的背景下，成功地对真实世界的科学试验进行了无监督因果效应识别。

Conclusion: 本研究成功开发并验证了一种从非结构化数据中无监督地识别因果效应的方法，克服了传统方法的限制，并在实验生态学领域取得了实际应用。

Abstract: Randomized Controlled Trials are one of the pillars of science; nevertheless,
they rely on hand-crafted hypotheses and expensive analysis. Such constraints
prevent causal effect estimation at scale, potentially anchoring on popular yet
incomplete hypotheses. We propose to discover the unknown effects of a
treatment directly from data. For this, we turn unstructured data from a trial
into meaningful representations via pretrained foundation models and interpret
them via a sparse autoencoder. However, discovering significant causal effects
at the neural level is not trivial due to multiple-testing issues and effects
entanglement. To address these challenges, we introduce Neural Effect Search, a
novel recursive procedure solving both issues by progressive stratification.
After assessing the robustness of our algorithm on semi-synthetic experiments,
we showcase, in the context of experimental ecology, the first successful
unsupervised causal effect identification on a real-world scientific trial.

</details>


### [459] [Neural Network approximation power on homogeneous and heterogeneous reaction-diffusion equations](https://arxiv.org/abs/2510.14094)
*Haotian Feng*

Main category: cs.LG

TL;DR: 本论文利用神经网络理论，证明了其在求解反应扩散方程方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究利用神经网络求解微分方程，但缺乏理论基础解释其有效性。

Method: 通过理论分析，证明了使用两层神经网络可以逼近一维反应扩散方程，使用三层神经网络可以逼近二维反应扩散方程。

Result: 神经网络，特别是两层和三层网络，在逼近一维和二维反应扩散方程（包括齐次和异质介质）方面具有强大的能力。

Conclusion: 本研究为基于神经网络的微分方程求解器提供了理论基础，并强调了神经网络在逼近反应扩散方程及其相关偏微分方程方面的表达能力。

Abstract: Reaction-diffusion systems represent one of the most fundamental formulations
used to describe a wide range of physical, chemical, and biological processes.
With the increasing adoption of neural networks, recent research has focused on
solving differential equations using machine learning techniques. However, the
theoretical foundation explaining why neural networks can effectively
approximate such solutions remains insufficiently explored.
  This paper provides a theoretical analysis of the approximation power of
neural networks for one- and two-dimensional reaction-diffusion equations in
both homogeneous and heterogeneous media. Building upon the universal
approximation theorem, we demonstrate that a two-layer neural network can
approximate the one-dimensional reaction-diffusion equation, while a
three-layer neural network can approximate its two-dimensional counterpart. The
theoretical framework presented here can be further extended to elliptic and
parabolic equations.
  Overall, this work highlights the expressive power of neural networks in
approximating solutions to reaction-diffusion equations and related PDEs,
providing a theoretical foundation for neural network-based differential
equation solvers.

</details>


### [460] [Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning](https://arxiv.org/abs/2510.14095)
*Awni Altabaa,Siyu Chen,John Lafferty,Zhuoran Yang*

Main category: cs.LG

TL;DR: Transformer网络在计算图上的算术任务中，通过引入输入自适应递归、算法监督、离散瓶颈锚定潜在表示和显式错误纠正机制，提高了分布外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 系统性、组合泛化能力在训练分布之外，仍然是机器学习的核心挑战，也是现代语言模型涌现推理能力的关键瓶颈。

Method: 在本研究中，我们引入并探索了一套四个架构机制，旨在提高分布外泛化能力：(i) 输入自适应递归；(ii) 算法监督；(iii) 通过离散瓶颈锚定潜在表示；(iv) 显式错误纠正机制。

Result: 这些机制共同为Transformer网络提供了本地且可扩展的潜在空间推理的架构方法，并具有强大的算法泛化能力。

Conclusion: 我们通过详细的机械可解释性分析来补充我们的实证结果，揭示了这些机制如何产生强大的分布外泛化能力。

Abstract: Systematic, compositional generalization beyond the training distribution
remains a core challenge in machine learning -- and a critical bottleneck for
the emergent reasoning abilities of modern language models. This work
investigates out-of-distribution (OOD) generalization in Transformer networks
using a GSM8K-style modular arithmetic on computational graphs task as a
testbed. We introduce and explore a set of four architectural mechanisms aimed
at enhancing OOD generalization: (i) input-adaptive recurrence; (ii)
algorithmic supervision; (iii) anchored latent representations via a discrete
bottleneck; and (iv) an explicit error-correction mechanism. Collectively,
these mechanisms yield an architectural approach for native and scalable latent
space reasoning in Transformer networks with robust algorithmic generalization
capabilities. We complement these empirical results with a detailed mechanistic
interpretability analysis that reveals how these mechanisms give rise to robust
OOD generalization abilities.

</details>


### [461] [TENDE: Transfer Entropy Neural Diffusion Estimation](https://arxiv.org/abs/2510.14096)
*Simon Pedro Galeano Munoz,Mustapha Bounoua,Giulio Franzese,Pietro Michiardi,Maurizio Filippone*

Main category: cs.LG

TL;DR:  TENDE 使用基于分数的扩散模型通过条件互信息来估计传递熵，克服了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有传递熵估计方法存在维度灾难、分布假设受限或需要海量数据才能可靠收敛的问题。

Method: TENDE 学习相关条件分布的得分函数，利用基于分数的扩散模型来估计传递熵。

Result: TENDE 在合成基准和真实数据上表现出优于现有神经估计器和其他最先进方法。

Conclusion: TENDE 是一种灵活、可扩展且假设最小的传递熵估计方法。

Abstract: Transfer entropy measures directed information flow in time series, and it
has become a fundamental quantity in applications spanning neuroscience,
finance, and complex systems analysis. However, existing estimation methods
suffer from the curse of dimensionality, require restrictive distributional
assumptions, or need exponentially large datasets for reliable convergence. We
address these limitations in the literature by proposing TENDE (Transfer
Entropy Neural Diffusion Estimation), a novel approach that leverages
score-based diffusion models to estimate transfer entropy through conditional
mutual information. By learning score functions of the relevant conditional
distributions, TENDE provides flexible, scalable estimation while making
minimal assumptions about the underlying data-generating process. We
demonstrate superior accuracy and robustness compared to existing neural
estimators and other state-of-the-art approaches across synthetic benchmarks
and real data.

</details>


### [462] [Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey](https://arxiv.org/abs/2510.14114)
*Yazid Janati,Alain Durmus,Jimmy Olsson,Eric Moulines*

Main category: cs.LG

TL;DR: 预训练的扩散模型可以与蒙特卡洛方法结合，无需额外训练即可解决贝叶斯逆问题，主要通过“扭曲”扩散过程中的中间分布来引导采样。


<details>
  <summary>Details</summary>
Motivation: 利用预训练的扩散模型作为先验，解决贝叶斯逆问题，而无需进行额外的训练。

Method: 将预训练的扩散模型与蒙特卡洛方法相结合，通过“扭曲”扩散过程中的中间分布来引导采样，使其逼近后验分布，并利用各种蒙特卡洛方法从扭曲后的分布中进行采样。

Result: 该方法能够从复杂分布中合成高精度样本，并有效解决贝叶斯逆问题。

Conclusion: 预训练的扩散模型与蒙特卡洛方法相结合，为解决贝叶斯逆问题提供了一种无需额外训练的有效途径，通过扭曲扩散过程中的中间分布来指导采样。

Abstract: Diffusion models enable the synthesis of highly accurate samples from complex
distributions and have become foundational in generative modeling. Recently,
they have demonstrated significant potential for solving Bayesian inverse
problems by serving as priors. This review offers a comprehensive overview of
current methods that leverage \emph{pre-trained} diffusion models alongside
Monte Carlo methods to address Bayesian inverse problems without requiring
additional training. We show that these methods primarily employ a
\emph{twisting} mechanism for the intermediate distributions within the
diffusion process, guiding the simulations toward the posterior distribution.
We describe how various Monte Carlo methods are then used to aid in sampling
from these twisted distributions.

</details>


### [463] [Neural Network-enabled Domain-consistent Robust Optimisation for Global CO$_2$ Reduction Potential of Gas Power Plants](https://arxiv.org/abs/2510.14125)
*Waqar Muhammad Ashraf,Talha Ansar,Abdulelah S. Alshehri,Peipei Chen,Ramit Debnath,Vivek Dua*

Main category: cs.LG

TL;DR: 该框架通过将数据驱动域约束整合到非线性规划中，解决了神经网络模型与优化求解器交互时出现的域不一致问题，并在一个1180兆瓦的联合循环燃气电厂中实现了0.76个百分点的能源效率提升。


<details>
  <summary>Details</summary>
Motivation: 在参数化神经网络模型与优化求解器交互时，存在域不一致解的问题，这是一个被忽视的问题。

Method: 提出一个由神经网络驱动的鲁棒优化框架，将数据驱动的域作为约束整合到非线性规划技术中。

Result: 在容量为1180兆瓦的联合循环燃气电厂中，该框架实现了鲁棒的最优解，并将能源效率平均提高了0.76个百分点。

Conclusion: 将效率提升扩展到全球燃气电厂，估计每年可减少2600万吨二氧化碳排放，这表明机器学习在实现近期的、可扩展的脱碳路径方面发挥着协同作用。

Abstract: We introduce a neural network-driven robust optimisation framework that
integrates data-driven domain as a constraint into the nonlinear programming
technique, addressing the overlooked issue of domain-inconsistent solutions
arising from the interaction of parametrised neural network models with
optimisation solvers. Applied to a 1180 MW capacity combined cycle gas power
plant, our framework delivers domain-consistent robust optimal solutions that
achieve a verified 0.76 percentage point mean improvement in energy efficiency.
For the first time, scaling this efficiency gain to the global fleet of gas
power plants, we estimate an annual 26 Mt reduction potential in CO$_2$ (with
10.6 Mt in Asia, 9.0 Mt in the Americas, and 4.5 Mt in Europe). These results
underscore the synergetic role of machine learning in delivering near-term,
scalable decarbonisation pathways for global climate action.

</details>


### [464] [Demystifying the Mechanisms Behind Emergent Exploration in Goal-conditioned RL](https://arxiv.org/abs/2510.14129)
*Mahsa Bastankhah,Grace Liu,Dilip Arumugam,Thomas L. Griffiths,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: SGCRL是一种无监督强化学习算法，通过学习到的表示来最大化隐式奖励，从而在没有外部奖励的情况下解决长时程目标任务。其探索机制源于状态空间的低秩表示，而非神经网络的函数逼近。


<details>
  <summary>Details</summary>
Motivation: 探索无监督强化学习中涌现式探索的机制。

Method: 结合对算法目标函数的理论分析和可控实验，研究SGCRL算法。

Result: SGCRL通过学习到的表示自动调整奖励函数，促进探索和利用。探索动力学源于学习到的低秩状态空间表示。该算法可改编以进行安全感知探索。

Conclusion: SGCRL通过学习到的表示最大化隐式奖励，实现了有效的探索和目标达成，其探索机制与状态空间的低秩表示有关。

Abstract: In this work, we take a first step toward elucidating the mechanisms behind
emergent exploration in unsupervised reinforcement learning. We study
Single-Goal Contrastive Reinforcement Learning (SGCRL), a self-supervised
algorithm capable of solving challenging long-horizon goal-reaching tasks
without external rewards or curricula. We combine theoretical analysis of the
algorithm's objective function with controlled experiments to understand what
drives its exploration. We show that SGCRL maximizes implicit rewards shaped by
its learned representations. These representations automatically modify the
reward landscape to promote exploration before reaching the goal and
exploitation thereafter. Our experiments also demonstrate that these
exploration dynamics arise from learning low-rank representations of the state
space rather than from neural network function approximation. Our improved
understanding enables us to adapt SGCRL to perform safety-aware exploration.

</details>


### [465] [Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks](https://arxiv.org/abs/2510.14139)
*Islam Akef Ebeid,Haoteng Tang,Pengfei Gu*

Main category: cs.LG

TL;DR: 提出了一种名为ProtGram-DirectGCN的新框架，用于通过链接预测进行蛋白质-蛋白质相互作用（PPI）预测，该框架计算效率高，并且在数据量有限的情况下表现出强大的预测能力。


<details>
  <summary>Details</summary>
Motivation: 准确预测蛋白质-蛋白质相互作用（PPI）对于理解细胞功能和药物开发至关重要，但现有方法计算成本高。

Method: 提出了一种名为ProtGram-DirectGCN的两阶段图表示学习框架。第一阶段，ProtGram将蛋白质的一级结构建模为全局推断的n-gram图的层次结构，其中残基转移概率定义了边权重。第二阶段，DirectGCN是一个自定义的有向图卷积神经网络，具有独特卷积层，通过单独的路径特定转换（传入、传出和无向）以及共享转换来处理信息，并通过可学习的门控机制组合这些路径。该模型应用于ProtGram图以学习残基级嵌入，然后通过注意力池化生成用于预测的蛋白质级嵌入。

Result: DirectGCN在标准节点分类基准测试中表现良好，与通用数据集上的现有方法相当，尤其擅长处理复杂、有向、密集、异质图。当应用于PPI预测时，完整的ProtGram-DirectGCN框架即使在训练数据有限的情况下也显示出强大的预测能力。

Conclusion: ProtGram-DirectGCN框架在PPI预测方面显示出有效性和鲁棒性，尤其是在计算资源有限或训练数据不足的情况下。

Abstract: Introduction Accurate prediction of protein-protein interactions (PPIs) is
crucial for understanding cellular functions and advancing drug development.
Existing in-silico methods use direct sequence embeddings from Protein Language
Models (PLMs). Others use Graph Neural Networks (GNNs) for 3D protein
structures. This study explores less computationally intensive alternatives. We
introduce a novel framework for downstream PPI prediction through link
prediction. Methods We introduce a two-stage graph representation learning
framework, ProtGram-DirectGCN. First, we developed ProtGram. This approach
models a protein's primary structure as a hierarchy of globally inferred n-gram
graphs. In these graphs, residue transition probabilities define edge weights.
Each edge connects a pair of residues in a directed graph. The probabilities
are aggregated from a large corpus of sequences. Second, we propose DirectGCN,
a custom directed graph convolutional neural network. This model features a
unique convolutional layer. It processes information through separate
path-specific transformations: incoming, outgoing, and undirected. A shared
transformation is also applied. These paths are combined via a learnable gating
mechanism. We apply DirectGCN to ProtGram graphs to learn residue-level
embeddings. These embeddings are pooled via attention to generate protein-level
embeddings for prediction. Results We first established the efficacy of
DirectGCN on standard node classification benchmarks. Its performance matches
established methods on general datasets. The model excels at complex, directed
graphs with dense, heterophilic structures. When applied to PPI prediction, the
full ProtGram-DirectGCN framework delivers robust predictive power. This strong
performance holds even with limited training data.

</details>


### [466] [On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model](https://arxiv.org/abs/2510.14156)
*Jan Kwiatkowski,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 本文旨在解决量化交易中股票排序的挑战，系统评估了不同损失函数在Transformer模型上进行股票收益排序的表现，并为优化排序交易策略提供了指导。


<details>
  <summary>Details</summary>
Motivation: 金融市场股票排序的挑战，以及标准损失函数在学习股票收益排序方面的不足，填补了现有研究中关于不同排序损失函数在现代Transformer模型上应用于股票选择的比较空白。

Method: 对包括点wise、pairwise和listwise在内的多种先进损失函数进行了系统评估，以预测每日股票收益，并基于S&P 500数据进行基于排名的投资组合选择。

Result: 通过在S&P 500数据上进行实验，揭示了不同损失函数对模型学习横截面和时间模式能力的影响，这些模式对于投资组合选择至关重要。

Conclusion: 为优化基于排名的交易策略提供了实用的指导，强调了选择合适的损失函数对于提升模型排序能力的重要性。

Abstract: Quantitative trading strategies rely on accurately ranking stocks to identify
profitable investments. Effective portfolio management requires models that can
reliably order future stock returns. Transformer models are promising for
understanding financial time series, but how different training loss functions
affect their ability to rank stocks well is not yet fully understood. Financial
markets are challenging due to their changing nature and complex relationships
between stocks. Standard loss functions, which aim for simple prediction
accuracy, often aren't enough. They don't directly teach models to learn the
correct order of stock returns. While many advanced ranking losses exist from
fields such as information retrieval, there hasn't been a thorough comparison
to see how well they work for ranking financial returns, especially when used
with modern Transformer models for stock selection. This paper addresses this
gap by systematically evaluating a diverse set of advanced loss functions
including pointwise, pairwise, listwise for daily stock return forecasting to
facilitate rank-based portfolio selection on S&P 500 data. We focus on
assessing how each loss function influences the model's ability to discern
profitable relative orderings among assets. Our research contributes a
comprehensive benchmark revealing how different loss functions impact a model's
ability to learn cross-sectional and temporal patterns crucial for portfolio
selection, thereby offering practical guidance for optimizing ranking-based
trading strategies.

</details>


### [467] [Data Understanding Survey: Pursuing Improved Dataset Characterization Via Tensor-based Methods](https://arxiv.org/abs/2510.14161)
*Matthew D. Merris,Tim Andersen*

Main category: cs.LG

TL;DR: 传统的数据分析方法在理解和解释复杂数据集方面存在局限性，而基于张量的方法提供了更强大、更具可解释性的替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有数据集表征方法（如统计、结构和基于模型的方法）在机器学习和数据分析领域往往无法提供深入的理解和洞察，而这些对于创新和可解释性至关重要。

Method: 通过文献综述，探讨了传统数据集表征技术的局限性，并介绍了几种基于张量的方法，说明它们如何能够提供比传统统计、结构和基于模型的数据集表征技术更稳健的替代方案。

Result: 基于张量的方法能够揭示细微的数据特征，提供增强的可解释性和可操作的智能，相比之下，传统的统计、结构和基于模型的方法存在不足。

Conclusion: 提倡采用基于张量的方法进行数据集表征，以期在理解复杂数据集方面取得突破，并为智能、可解释的数据驱动的发现铺平道路。

Abstract: In the evolving domains of Machine Learning and Data Analytics, existing
dataset characterization methods such as statistical, structural, and
model-based analyses often fail to deliver the deep understanding and insights
essential for innovation and explainability. This work surveys the current
state-of-the-art conventional data analytic techniques and examines their
limitations, and discusses a variety of tensor-based methods and how these may
provide a more robust alternative to traditional statistical, structural, and
model-based dataset characterization techniques. Through examples, we
illustrate how tensor methods unveil nuanced data characteristics, offering
enhanced interpretability and actionable intelligence. We advocate for the
adoption of tensor-based characterization, promising a leap forward in
understanding complex datasets and paving the way for intelligent, explainable
data-driven discoveries.

</details>


### [468] [Towards Reversible Model Merging For Low-rank Weights](https://arxiv.org/abs/2510.14163)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 模型合并的传统方法在低秩模型上表现不佳，提出了一种可逆模型合并（RMM）方法，通过构建一个可从中恢复任务特定模型的紧凑基，解决了这个问题。


<details>
  <summary>Details</summary>
Motivation: 现有的模型合并方法在处理低秩表示（如LoRA或SVD压缩的模型）时，会导致合并后的模型性能显著下降。因此，需要一种新的方法来有效合并低秩模型。

Method: 提出了一种名为可逆模型合并（RMM）的新方法。该方法不直接合并适配器权重，而是构建一个紧凑的基（模型空间），可以从中通过线性组合恢复出原始的任务特定模型。RMM提供了一个闭式解来选择最优基和任务系数。

Result: RMM在各种数据集和模型规模的广泛实验中，持续优于现有的模型合并方法，并且显著保留了低秩压缩模型的性能。

Conclusion: RMM是一种有效、无需数据且灵活的模型合并方法，能够解决低秩模型合并的挑战，并优于现有技术。

Abstract: Model merging aims to combine multiple fine-tuned models into a single set of
weights that performs well across all source tasks. While prior work has shown
that merging can approximate the performance of individual fine-tuned models
for each task, it largely overlooks scenarios where models are compressed into
low-rank representations, either through low-rank adaptation (LoRA) or
post-training singular value decomposition (SVD). We first demonstrate that
applying conventional merging methods to low-rank weights leads to severe
performance degradation in the merged model. Motivated by this phenomenon, we
propose a fundamentally different approach: instead of collapsing all adapters
into one set of weights, we construct a compact basis (e.g., an equivalent of
holding two or more models) from which original task-specific models can be
recovered via linear combination. This reframes merging as generating a
reconstruction-capable model space rather than producing a single merged model.
Crucially, this allows us to ``revert'' to each individual model when needed,
recognizing that no merged model can consistently outperform one specialized
for its task. Building on this insight, we introduce our method, Reversible
Model Merging (RMM), an efficient, data-free, and flexible method that provides
a closed-form solution for selecting the optimal basis of model weights and
task-specific coefficients for linear combination. Extensive experiments across
diverse datasets and model scales demonstrate that RMM consistently outperforms
existing merging approaches, preserving the performance of low-rank compressed
models by a significant margin.

</details>


### [469] [Optimal Control Theoretic Neural Optimizer: From Backpropagation to Dynamic Programming](https://arxiv.org/abs/2510.14168)
*Guan-Horng Liu,Tianrong Chen,Evangelos A. Theodorou*

Main category: cs.LG

TL;DR: 深度神经网络（DNN）优化可被视为最优控制问题。本文提出了一种名为OCNOpt的新优化器，它通过将反向传播算法与动态规划联系起来，并探索Bellman方程的高阶展开，从而改进了DNN的训练。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNN）的优化是现代机器学习和人工智能发展的驱动力。将DNN解释为动力系统，并利用最优控制理论为其提供理论基础，这很重要。本文旨从算法角度出发，利用反向传播算法和动态规划之间的联系，开发新的DNN优化方法。

Method: 本文将反向传播算法视为最优控制问题的变分结构，并通过对Bellman方程进行一阶展开来解决近似动态规划问题，从而得到一类新的优化方法。这些方法通过探索Bellman方程的更高阶展开来训练DNN，最终得到一个名为OCNOpt的优化器。

Result: OCNOpt优化器能够实现分层反馈策略、博弈论应用以及连续时间模型（如神经ODE）的二阶训练。实验证明，OCNOpt在保持可控计算复杂度的同时，提高了DNN训练的鲁棒性和效率。

Conclusion: 通过将DNN优化与最优控制理论和动态规划联系起来，本文提出了一种名为OCNOpt的新型优化器。该优化器在鲁棒性、效率和计算复杂性方面优于现有方法，并为基于动态系统和最优控制理论的算法设计开辟了新途径。

Abstract: Optimization of deep neural networks (DNNs) has been a driving force in the
advancement of modern machine learning and artificial intelligence. With DNNs
characterized by a prolonged sequence of nonlinear propagation, determining
their optimal parameters given an objective naturally fits within the framework
of Optimal Control Programming. Such an interpretation of DNNs as dynamical
systems has proven crucial in offering a theoretical foundation for principled
analysis from numerical equations to physics. In parallel to these theoretical
pursuits, this paper focuses on an algorithmic perspective. Our motivated
observation is the striking algorithmic resemblance between the Backpropagation
algorithm for computing gradients in DNNs and the optimality conditions for
dynamical systems, expressed through another backward process known as dynamic
programming. Consolidating this connection, where Backpropagation admits a
variational structure, solving an approximate dynamic programming up to the
first-order expansion leads to a new class of optimization methods exploring
higher-order expansions of the Bellman equation. The resulting optimizer,
termed Optimal Control Theoretic Neural Optimizer (OCNOpt), enables rich
algorithmic opportunities, including layer-wise feedback policies,
game-theoretic applications, and higher-order training of continuous-time
models such as Neural ODEs. Extensive experiments demonstrate that OCNOpt
improves upon existing methods in robustness and efficiency while maintaining
manageable computational complexity, paving new avenues for principled
algorithmic design grounded in dynamical systems and optimal control theory.

</details>


### [470] [MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation](https://arxiv.org/abs/2510.14184)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.LG

TL;DR: MAFA是一个多智能体框架，通过可配置的智能体协作解决了金融服务领域大规模标注的积压问题，实现了高效的客户话语分类，并显著减少了人工标注时间。


<details>
  <summary>Details</summary>
Motivation: 金融服务领域存在大量的客户话语需要准确分类，导致标注积压问题严重。

Method: MAFA框架结合了专业化智能体、结构化推理和基于裁判的共识机制，支持通过配置动态适应任务，可自定义标注类型（如FAQ、意图、实体或特定领域分类），而无需代码更改。

Result: 在摩根大通的实际部署中，MAFA消除了100万条话语的积压，与人工标注者的一致性平均达到86%，每年节省超过5000小时的手动标注时间。标注置信度分类显示，85%为高，10%为中，5%为低。在内部意图分类数据集和公开基准测试上，MAFA相比传统和单一智能体基线，在Top-1准确率上提高了13.8%，Top-5准确率提高了15.1%，F1分数提高了16.9%。

Conclusion: MAFA成功地将多智能体系统的理论研究与实际企业应用相结合，为面临类似标注挑战的组织提供了可行的解决方案，并证明了其在多数据、多语言场景下的有效性和优越性。

Abstract: We present MAFA (Multi-Agent Framework for Annotation), a production-deployed
system that transforms enterprise-scale annotation workflows through
configurable multi-agent collaboration. Addressing the critical challenge of
annotation backlogs in financial services, where millions of customer
utterances require accurate categorization, MAFA combines specialized agents
with structured reasoning and a judge-based consensus mechanism. Our framework
uniquely supports dynamic task adaptation, allowing organizations to define
custom annotation types (FAQs, intents, entities, or domain-specific
categories) through configuration rather than code changes. Deployed at JP
Morgan Chase, MAFA has eliminated a 1 million utterance backlog while
achieving, on average, 86% agreement with human annotators, annually saving
over 5,000 hours of manual annotation work. The system processes utterances
with annotation confidence classifications, which are typically 85% high, 10%
medium, and 5% low across all datasets we tested. This enables human annotators
to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's
effectiveness across multiple datasets and languages, showing consistent
improvements over traditional and single-agent annotation baselines: 13.8%
higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1
in our internal intent classification dataset and similar gains on public
benchmarks. This work bridges the gap between theoretical multi-agent systems
and practical enterprise deployment, providing a blueprint for organizations
facing similar annotation challenges.

</details>


### [471] [Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation](https://arxiv.org/abs/2510.14190)
*Ruchi Sandilya,Sumaira Perez,Charles Lynch,Lindsay Victoria,Benjamin Zebley,Derrick Matthew Buchanan,Mahendra T. Bhati,Nolan Williams,Timothy J. Spellman,Faith M. Gunning,Conor Liston,Logan Grosenick*

Main category: cs.LG

TL;DR: ConDA框架使用对比学习来组织扩散模型的潜在空间，使其能够进行可解释的控制和轨迹遍历。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在生成方面表现出色，但其潜在空间缺乏明确的组织，难以进行可解释的控制。本研究的动机是利用对比学习来组织扩散模型的潜在空间，使其与系统动力学对齐，从而实现更可控的生成。

Method: ConDA框架应用对比学习于扩散嵌入中，以实现潜在几何与系统动力学的对齐。该方法通过组织扩散潜在空间，使得遍历方向能够反映潜在的动力学因素。

Result: 在流体动力学、神经钙成像、神经调控疗法和面部表情等基准测试中，ConDA与线性遍历和基于条件的方法相比，产生了可解释的潜在表征，并具有更高的可控性。

Conclusion: 扩散模型的潜在空间编码了与动力学相关的结构，但要利用这种结构，需要对潜在空间进行组织，并沿着潜在流形进行遍历。

Abstract: Diffusion models excel at generation, but their latent spaces are not
explicitly organized for interpretable control. We introduce ConDA (Contrastive
Diffusion Alignment), a framework that applies contrastive learning within
diffusion embeddings to align latent geometry with system dynamics. Motivated
by recent advances showing that contrastive objectives can recover more
disentangled and structured representations, ConDA organizes diffusion latents
such that traversal directions reflect underlying dynamical factors. Within
this contrastively structured space, ConDA enables nonlinear trajectory
traversal that supports faithful interpolation, extrapolation, and controllable
generation. Across benchmarks in fluid dynamics, neural calcium imaging,
therapeutic neurostimulation, and facial expression, ConDA produces
interpretable latent representations with improved controllability compared to
linear traversals and conditioning-based baselines. These results suggest that
diffusion latents encode dynamics-relevant structure, but exploiting this
structure requires latent organization and traversal along the latent manifold.

</details>


### [472] [Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization](https://arxiv.org/abs/2510.14217)
*Asma Jamali,Tin Sum Cheng,Rodrigo A. Vargas-Hernández*

Main category: cs.LG

TL;DR: 光谱特征分析表明，更丰富的谱特征并不总是带来更好的泛化能力，并且前导特征值包含了大部分信息。


<details>
  <summary>Details</summary>
Motivation: 理解核函数的谱特性对于深入了解模型的泛化能力和表示质量至关重要。尽管深度学习在分子属性预测方面表现出色，但核方法在数据稀疏的情况下因其鲁棒性和理论基础而得到广泛应用。然而，关于分子核函数谱特性的系统性分析却很少见。

Method: 对QM9数据集上的核岭回归进行了全面的谱分析，涵盖了分子指纹、预训练的基于Transformer的模型以及全局和局部的3D表示，并针对七种分子属性进行了评估。此外，还通过保留主导特征值来研究谱与预测性能之间的关系。

Result: 研究发现，更丰富的谱特征（根据四种不同的谱指标衡量）并不总能提高准确性。皮尔逊相关性检验表明，对于基于Transformer和局部3D表示，谱丰富度甚至可能与性能呈负相关。通过截断核函数发现，保留前2%的特征值即可恢复近乎全部性能，表明前导特征值包含了最有效的信息。

Conclusion: 研究结果对“更丰富的谱带来更好的泛化能力”这一普遍假设提出了质疑，并揭示了表示、核特征和预测性能之间复杂的相互关系。这些发现不仅有助于分子属性预测，也为在数据有限的科学和实际任务中评估核方法和自监督学习方法提供了参考。

Abstract: Understanding the spectral properties of kernels offers a principled
perspective on generalization and representation quality. While deep models
achieve state-of-the-art accuracy in molecular property prediction, kernel
methods remain widely used for their robustness in low-data regimes and
transparent theoretical grounding. Despite extensive studies of kernel spectra
in machine learning, systematic spectral analyses of molecular kernels are
scarce. In this work, we provide the first comprehensive spectral analysis of
kernel ridge regression on the QM9 dataset, molecular fingerprint, pretrained
transformer-based, global and local 3D representations across seven molecular
properties. Surprisingly, richer spectral features, measured by four different
spectral metrics, do not consistently improve accuracy. Pearson correlation
tests further reveal that for transformer-based and local 3D representations,
spectral richness can even have a negative correlation with performance. We
also implement truncated kernels to probe the relationship between spectrum and
predictive performance: in many kernels, retaining only the top 2% of
eigenvalues recovers nearly all performance, indicating that the leading
eigenvalues capture the most informative features. Our results challenge the
common heuristic that "richer spectra yield better generalization" and
highlight nuanced relationships between representation, kernel features, and
predictive performance. Beyond molecular property prediction, these findings
inform how kernel and self-supervised learning methods are evaluated in
data-limited scientific and real-world tasks.

</details>


### [473] [When Flatness Does (Not) Guarantee Adversarial Robustness](https://arxiv.org/abs/2510.14231)
*Nils Philipp Walter,Linara Adilova,Jilles Vreeken,Michael Kamp*

Main category: cs.LG

TL;DR: 神经网易对对抗性扰动很敏感，尽管存在扁平最小化的假设，但扁平最小化只提供了局部鲁棒性，而非全局鲁棒性。通过对最后一个隐藏层进行公式化处理，可以约束损失变化，从而分析整个网络的对抗鲁棒性。需要注意的是，为了获得超出局部范围的鲁棒性，损失需要急剧偏离数据流形。扁平化与模型置信度相关，对抗性样本通常位于模型错误但置信度高的区域。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网易在实践中取得了成功，但它们仍然容易受到微小的、对抗性的扰动。一种长期的假设是，扁平最小化（损失景观中曲率较低的区域）可以提高鲁棒性。然而，这种联系在很大程度上仍然是模糊和不完整的。

Method: 首先，推导出最后一层相对扁平化的闭式表达式，然后证明可以利用该表达式来约束损失空间中的损失变化，从而对整个网络的对抗鲁棒性进行形式化分析。

Result: 我们证明，为了在局部邻域之外保持鲁棒性，损失需要急剧地偏离数据流形。我们在各种架构和数据集上通过实证验证了我们的理论预测，揭示了控制对抗性漏洞的几何结构，并将扁平化与模型置信度联系起来：对抗性样本通常位于模型错误但置信度高的区域。

Conclusion: 我们提出的结果挑战了对扁平化的简化看法，并提供了对其在鲁棒性方面作用的细致理解。

Abstract: Despite their empirical success, neural networks remain vulnerable to small,
adversarial perturbations. A longstanding hypothesis suggests that flat minima,
regions of low curvature in the loss landscape, offer increased robustness.
While intuitive, this connection has remained largely informal and incomplete.
By rigorously formalizing the relationship, we show this intuition is only
partially correct: flatness implies local but not global adversarial
robustness. To arrive at this result, we first derive a closed-form expression
for relative flatness in the penultimate layer, and then show we can use this
to constrain the variation of the loss in input space. This allows us to
formally analyze the adversarial robustness of the entire network. We then show
that to maintain robustness beyond a local neighborhood, the loss needs to
curve sharply away from the data manifold. We validate our theoretical
predictions empirically across architectures and datasets, uncovering the
geometric structure that governs adversarial vulnerability, and linking
flatness to model confidence: adversarial examples often lie in large, flat
regions where the model is confidently wrong. Our results challenge simplified
views of flatness and provide a nuanced understanding of its role in
robustness.

</details>


### [474] [Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models](https://arxiv.org/abs/2510.14232)
*Mehrzad Samadi,Aleksander Ficek,Sean Narenthiran,Siddhartha Jain,Wasi Uddin Ahmad,Somshubra Majumdar,Vahid Noroozi,Boris Ginsburg*

Main category: cs.LG

TL;DR: GenCluster是一个使用开放权重模型在IOI竞赛中达到金牌水平的计算框架，通过大规模生成、行为聚类、排名和循环提交策略来探索解决方案空间，并展示了性能随计算资源扩展的能力。


<details>
  <summary>Details</summary>
Motivation: 解决使用开放权重模型在IOI竞赛中达到金牌水平的挑战，并提供一个可扩展、可复现的测试框架。

Method: 结合大规模生成、行为聚类、排名和循环提交策略来探索解决方案空间，并在有限的验证预算下进行。

Result: GenCluster能够使用开放权重模型达到IOI金牌水平，并且性能随计算资源的增加而扩展，缩小了开放和闭源系统之间的差距。

Conclusion: GenCluster首次使用开放权重模型（gpt-oss-120b）在IOI竞赛中达到了金牌水平，为LLM推理的透明和可复现评估设定了新基准。

Abstract: Competitive programming has become a rigorous benchmark for evaluating the
reasoning and problem-solving capabilities of large language models (LLMs). The
International Olympiad in Informatics (IOI) stands out as one of the most
prestigious annual competitions in competitive programming and has become a key
benchmark for comparing human and AI-level programming ability. While several
proprietary models have been claimed to achieve gold medal-level performance at
the IOI, often with undisclosed methods, achieving comparable results with
open-weight models remains a significant challenge. In this paper, we present
\gencluster, a scalable and reproducible test-time compute framework that
attains IOI gold-level performance using open-weight models. It combines
large-scale generation, behavioral clustering, ranking, and a round-robin
submission strategy to efficiently explore diverse solution spaces under
limited validation budgets. Our experiments show that the performance of our
proposed approach scales consistently with available compute, narrowing the gap
between open and closed systems. Notably, we will show that GenCluster can
achieve a gold medal at IOI 2025 for the first time with an open-weight model
gpt-oss-120b, setting a new benchmark for transparent and reproducible
evaluation of reasoning in LLMs.

</details>


### [475] [Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation](https://arxiv.org/abs/2510.14246)
*Jingwen Gu,Yiting He,Zhishuai Liu,Pan Xu*

Main category: cs.LG

TL;DR: DR-RPO是一种模型无关的在线策略优化方法，用于解决分布变化下的强化学习问题，通过正则化和线性函数逼近实现高效鲁棒的策略学习。


<details>
  <summary>Details</summary>
Motivation: 决策制定下的分布变化是强化学习中的一个核心挑战，而DR-RPO旨在通过鲁棒马尔可夫决策过程（RMDP）来解决这个问题，特别是在在线设置中，样本效率和探索尤为关键。

Method: DR-RPO采用参考策略正则化来处理软策略类中的可解析优化，并结合d-矩形线性MDP、线性函数逼近和上置信奖励来实现乐观探索，从而构建了一个同时约束转移和策略的双重约束RMDP。

Result: DR-RPO在理论上保证了策略优化在鲁棒RL中能够达到多项式次优界和样本效率，其性能可与基于价值的方法相媲美。实验结果也证实了DR-RPO在不同领域的鲁棒性。

Conclusion: DR-RPO在解决分布变化下的强化学习问题方面取得了显著进展，其理论分析和实验结果表明，该方法在鲁棒性和样本效率方面均表现优异，为鲁棒强化学习领域提供了新的解决方案。

Abstract: Decision-making under distribution shift is a central challenge in
reinforcement learning (RL), where training and deployment environments differ.
We study this problem through the lens of robust Markov decision processes
(RMDPs), which optimize performance against adversarial transition dynamics.
Our focus is the online setting, where the agent has only limited interaction
with the environment, making sample efficiency and exploration especially
critical. Policy optimization, despite its success in standard RL, remains
theoretically and empirically underexplored in robust RL. To bridge this gap,
we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized
\textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online
policy optimization method that learns robust policies with sublinear regret.
To enable tractable optimization within the softmax policy class, DR-RPO
incorporates reference-policy regularization, yielding RMDP variants that are
doubly constrained in both transitions and policies. To scale to large
state-action spaces, we adopt the $d$-rectangular linear MDP formulation and
combine linear function approximation with an upper confidence bonus for
optimistic exploration. We provide theoretical guarantees showing that policy
optimization can achieve polynomial suboptimality bounds and sample efficiency
in robust RL, matching the performance of value-based approaches. Finally,
empirical results across diverse domains corroborate our theory and demonstrate
the robustness of DR-RPO.

</details>


### [476] [A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters](https://arxiv.org/abs/2510.14250)
*Lianzi Jiang,Jianxin Zhang,Xinyu Han,Huanhe Dong,Xiangrong Wang*

Main category: cs.LG

TL;DR: 该研究提出了一种新的物理先验引导的双流注意网络（PhysAttnNet），用于提高弹性防波堤运动响应预测的准确性和泛化能力，解决了传统深度学习模型在处理未见海况时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习模型在预测弹性防波堤运动响应时泛化能力有限，未能充分考虑海洋系统的自然衰减和波浪-结构相互作用（WSI）。

Method: 提出了一种名为PhysAttnNet的新型网络，该网络包含两个关键模块：1. 衰减双向自注意力（DBSA）模块，通过引入可学习的时间衰减来模拟自然衰减现象；2. 相位差引导双向交叉注意力（PDG-BCA）模块，利用基于余弦的偏差来捕捉波浪与结构之间的双向交互和相位关系。这两个模块通过全局上下文融合（GCF）模块进行整合。此外，采用混合时频损失函数进行训练，以同时最小化时域预测误差和频域光谱差异。

Result: PhysAttnNet在波浪水槽数据集上的实验结果显著优于主流模型。跨场景泛化测试也验证了该模型在应对未知环境时的鲁棒性和适应性。

Conclusion: PhysAttnNet在弹性防波堤运动响应预测方面表现出色，展现了其作为海洋工程复杂系统预测模型框架的潜力。

Abstract: Accurate motion response prediction for elastic Bragg breakwaters is critical
for their structural safety and operational integrity in marine environments.
However, conventional deep learning models often exhibit limited generalization
capabilities when presented with unseen sea states. These deficiencies stem
from the neglect of natural decay observed in marine systems and inadequate
modeling of wave-structure interaction (WSI). To overcome these challenges,
this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network
(PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module
incorporates a learnable temporal decay to assign higher weights to recent
states, aiming to emulate the natural decay phenomenon. Meanwhile, the phase
differences guided bidirectional cross-attention (PDG-BCA) module explicitly
captures the bidirectional interaction and phase relationship between waves and
the structure using a cosine-based bias within a bidirectional
cross-computation paradigm. These streams are synergistically integrated
through a global context fusion (GCF) module. Finally, PhysAttnNet is trained
with a hybrid time-frequency loss that jointly minimizes time-domain prediction
errors and frequency-domain spectral discrepancies. Comprehensive experiments
on wave flume datasets demonstrate that PhysAttnNet significantly outperforms
mainstream models. Furthermore,cross-scenario generalization tests validate the
model's robustness and adaptability to unseen environments, highlighting its
potential as a framework to develop predictive models for complex systems in
ocean engineering.

</details>


### [477] [Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals](https://arxiv.org/abs/2510.14254)
*Saurabh Kataria,Yi Wu,Zhaoliang Chen,Hyunjung Gloria Kwak,Yuhao Xu,Lovely Yeswanth Panchumarthi,Ran Xiao,Jiaying Lu,Ayca Ermis,Anni Zhao,Runze Yan,Alex Federov,Zewen Liu,Xu Wu,Wei Jin,Carl Yang,Jocelyn Grunwell,Stephanie R. Brown,Amit Shah,Craig Jabaley,Tim Buchman,Sivasubramanium V Bhavani,Randall J. Lee,Xiao Hu*

Main category: cs.LG

TL;DR: 本次发布的研究旨在全面评估通用型和专用型时间序列基础模型在PPG信号处理任务上的性能，并通过51项任务评估了它们的适应性、鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 目前，时间序列基础模型多为专用模型，即预训练和测试数据类型相同。本研究旨在比较通用型和专用型模型在PPG信号处理任务上的性能。

Method: 本研究通过一项包含51项任务的总括性基准测试研究，评估了通用型和专用型模型在PPG信号上的性能。评估维度包括胜率、平均性能、特征质量、调优增益、性能方差、可迁移性和可扩展性。

Result: 在全调优场景下，专用模型取得了高27%的胜率。

Conclusion: 本研究的发现为理解和选择最适合特定下游场景的时间序列基础模型提供了有价值的见解。

Abstract: Foundation models are large-scale machine learning models that are
pre-trained on massive amounts of data and can be adapted for various
downstream tasks. They have been extensively applied to tasks in Natural
Language Processing and Computer Vision with models such as GPT, BERT, and
CLIP. They are now also increasingly gaining attention in time-series analysis,
particularly for physiological sensing. However, most time series foundation
models are specialist models - with data in pre-training and testing of the
same type, such as Electrocardiogram, Electroencephalogram, and
Photoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist time
series foundation model with data from multiple domains, such as weather,
traffic, and electricity. This paper aims to conduct a comprehensive
benchmarking study to compare the performance of generalist and specialist
models, with a focus on PPG signals. Through an extensive suite of total 51
tasks covering cardiac state assessment, laboratory value estimation, and
cross-modal inference, we comprehensively evaluate both models across seven
dimensions, including win score, average performance, feature quality, tuning
gain, performance variance, transferability, and scalability. These metrics
jointly capture not only the models' capability but also their adaptability,
robustness, and efficiency under different fine-tuning strategies, providing a
holistic understanding of their strengths and limitations for diverse
downstream scenarios. In a full-tuning scenario, we demonstrate that the
specialist model achieves a 27% higher win score. Finally, we provide further
analysis on generalization, fairness, attention visualizations, and the
importance of training data choice.

</details>


### [478] [CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions](https://arxiv.org/abs/2510.14262)
*Zihao Fu,Ming Liao,Chris Russell,Zhenguang G. Cai*

Main category: cs.LG

TL;DR: CAST是一个无需探针的框架，通过直接估计变换矩阵和进行全面的光谱分析来分析Transformer层功能，为现有方法提供了互补的见解。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释性方法（如力学分析、探针分类器和激活可视化）各有优缺点，而 CAST 旨在提供一种新的视角来分析 Transformer 层功能。

Method: CAST 通过 Moore-Penrose 伪逆估计每个层的变换矩阵，并应用具有六个可解释指标的光谱分析来表征层行为。

Result: CAST 分析揭示了仅编码器和仅解码器模型之间不同的行为，解码器模型表现出压缩-膨胀循环，而编码器模型则保持一致的高秩处理。核分析进一步证明了层之间的功能关系模式，其中 CKA 相似性矩阵将层明确划分为三个阶段：特征提取、压缩和专门化。

Conclusion: CAST 提供了一种分析 Transformer 层功能的新颖方法，通过估计变换矩阵和进行光谱分析，揭示了不同模型架构和层之间行为的见解。

Abstract: Large language models have achieved remarkable success but remain largely
black boxes with poorly understood internal mechanisms. To address this
limitation, many researchers have proposed various interpretability methods
including mechanistic analysis, probing classifiers, and activation
visualization, each providing valuable insights from different perspectives.
Building upon this rich landscape of complementary approaches, we introduce
CAST (Compositional Analysis via Spectral Tracking), a probe-free framework
that contributes a novel perspective by analyzing transformer layer functions
through direct transformation matrix estimation and comprehensive spectral
analysis. CAST offers complementary insights to existing methods by estimating
the realized transformation matrices for each layer using Moore-Penrose
pseudoinverse and applying spectral analysis with six interpretable metrics
characterizing layer behavior. Our analysis reveals distinct behaviors between
encoder-only and decoder-only models, with decoder models exhibiting
compression-expansion cycles while encoder models maintain consistent high-rank
processing. Kernel analysis further demonstrates functional relationship
patterns between layers, with CKA similarity matrices clearly partitioning
layers into three phases: feature extraction, compression, and specialization.

</details>


### [479] [Nonparametric Data Attribution for Diffusion Models](https://arxiv.org/abs/2510.14269)
*Yutian Zhao,Chao Du,Xiaosen Zheng,Tianyu Pang,Min Lin*

Main category: cs.LG

TL;DR: 提出一种无需梯度或重新训练的无参数数据归因方法，通过图像块相似性衡量训练数据对生成模型输出的影响，具有计算效率和空间可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有针对扩散模型的归因方法需要模型梯度或重新训练，限制了其在私有或大规模场景下的应用。

Method: 提出一种仅依赖数据的非参数归因方法，通过衡量生成图像和训练图像之间图像块的相似性来量化影响。该方法基于最优分数函数的解析形式，可自然扩展到多尺度表示，并通过基于卷积的加速实现计算效率。

Result: 该方法实现了强大的归因性能，与基于梯度的方法相匹配，并显著优于现有的非参数基线方法。实验证明该方法具有空间可解释性，并能揭示训练数据与输出之间不依赖于特定模型的内在关系模式。

Conclusion: 所提出的非参数归因方法为评估训练数据对生成模型输出的影响提供了一种有效且可扩展的解决方案，尤其适用于梯度或重新训练不可用的场景。

Abstract: Data attribution for generative models seeks to quantify the influence of
individual training examples on model outputs. Existing methods for diffusion
models typically require access to model gradients or retraining, limiting
their applicability in proprietary or large-scale settings. We propose a
nonparametric attribution method that operates entirely on data, measuring
influence via patch-level similarity between generated and training images. Our
approach is grounded in the analytical form of the optimal score function and
naturally extends to multiscale representations, while remaining
computationally efficient through convolution-based acceleration. In addition
to producing spatially interpretable attributions, our framework uncovers
patterns that reflect intrinsic relationships between training data and
outputs, independent of any specific model. Experiments demonstrate that our
method achieves strong attribution performance, closely matching gradient-based
approaches and substantially outperforming existing nonparametric baselines.
Code is available at https://github.com/sail-sg/NDA.

</details>


### [480] [Stable Prediction of Adverse Events in Medical Time-Series Data](https://arxiv.org/abs/2510.14286)
*Mayank Keoliya,Seewon Choi,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: CAREBench是一个用于早期事件预测（EEP）的基准测试，它使用多模态输入（EHR、ECG、文本）并评估预测准确性和时间稳定性，发现现有方法（尤其是LLMs）在同时优化这两个指标时存在困难。


<details>
  <summary>Details</summary>
Motivation: 当前早期事件预测（EEP）系统在风险评估的准确性和时间稳定性方面存在不足，尤其是在处理多模态数据时，这影响了临床决策的信任度。现有的基准测试忽略了风险评分的稳定性，并且主要在表格数据上进行评估，未能充分测试轨迹行为。

Method: 提出CAREBench基准测试，该测试使用多模态输入（表格EHR、ECG波形和临床文本）来评估EEP系统的可部署性。引入了一个稳定性指标，该指标量化每位患者风险的短期变异性，并基于局部Lipschitz常数来惩罚突然的波动。CAREBench涵盖了包括败血症发作在内的六个预测任务，并对经典学习器、深度序列模型和零样本LLM进行了比较。

Result: 在所涵盖的任务中，现有方法，特别是LLM，在联合优化准确性和稳定性方面表现不佳，在追求高精度的同时，召回率明显偏低。这表明模型在证据对齐和生成稳定的风险轨迹方面存在挑战。

Conclusion: 现有EEP方法，尤其是LLM，在生成既准确又时间稳定的风险轨迹方面存在困难，这对于在连续监测环境中赢得临床医生信任至关重要。需要开发能够同时优化准确性和稳定性的模型。

Abstract: Early event prediction (EEP) systems continuously estimate a patient's
imminent risk to support clinical decision-making. For bedside trust, risk
trajectories must be accurate and temporally stable, shifting only with new,
relevant evidence. However, current benchmarks (a) ignore stability of risk
scores and (b) evaluate mainly on tabular inputs, leaving trajectory behavior
untested. To address this gap, we introduce CAREBench, an EEP benchmark that
evaluates deployability using multi-modal inputs-tabular EHR, ECG waveforms,
and clinical text-and assesses temporal stability alongside predictive
accuracy. We propose a stability metric that quantifies short-term variability
in per-patient risk and penalizes abrupt oscillations based on local-Lipschitz
constants. CAREBench spans six prediction tasks such as sepsis onset and
compares classical learners, deep sequence models, and zero-shot LLMs. Across
tasks, existing methods, especially LLMs, struggle to jointly optimize accuracy
and stability, with notably poor recall at high-precision operating points.
These results highlight the need for models that produce evidence-aligned,
stable trajectories to earn clinician trust in continuous monitoring settings.
(Code: https://github.com/SeewonChoi/CAREBench.)

</details>


### [481] [Enhancing Time-Series Anomaly Detection by Integrating Spectral-Residual Bottom-Up Attention with Reservoir Computing](https://arxiv.org/abs/2510.14287)
*Hayato Nihei,Sou Nobukawa,Yusuke Sakemi,Kazuyuki Aihara*

Main category: cs.LG

TL;DR: SR-RC是一种结合了谱残差（SR）注意机制和循环神经网络（RC）的时间序列异常检测方法，旨在提高资源受限的边缘设备上的异常检测性能，同时保持学习效率。


<details>
  <summary>Details</summary>
Motivation: 传统的循环神经网络（RC）在处理时间序列数据和进行异常检测方面有应用潜力，尤其是在边缘人工智能（AI）场景下，但其性能可能受限于资源约束的设备，需要更大的计算量或可能降低学习效率。因此，需要一种在不牺牲学习效率的前提下提高RC异常检测性能的方法。

Method: 提出了一种谱残差循环神经网络（SR-RC），该方法将学习自由的、自底向上的注意机制——谱残差（SR）方法——与RC相结合，以提高RC的异常检测性能。

Result: SR-RC在基准任务和真实世界的时间序列数据集上，其性能优于传统的RC和基于SR提取值的逻辑回归模型。

Conclusion: SR-RC通过集成SR方法，在不牺牲学习效率的情况下提高了RC的异常检测性能，并且由于SR方法和RC都适合硬件实现，SR-RC为在资源受限的边缘设备上部署RC用于时间序列异常检测提供了一个实际可行的方向。

Abstract: Reservoir computing (RC) establishes the basis for the processing of
time-series data by exploiting the high-dimensional spatiotemporal response of
a recurrent neural network to an input signal. In particular, RC trains only
the output layer weights. This simplicity has drawn attention especially in
Edge Artificial Intelligence (AI) applications. Edge AI enables time-series
anomaly detection in real time, which is important because detection delays can
lead to serious incidents. However, achieving adequate anomaly-detection
performance with RC alone may require an unacceptably large reservoir on
resource-constrained edge devices. Without enlarging the reservoir, attention
mechanisms can improve accuracy, although they may require substantial
computation and undermine the learning efficiency of RC. In this study, to
improve the anomaly detection performance of RC without sacrificing learning
efficiency, we propose a spectral residual RC (SR-RC) that integrates the
spectral residual (SR) method - a learning-free, bottom-up attention mechanism
- with RC. We demonstrated that SR-RC outperformed conventional RC and
logistic-regression models based on values extracted by the SR method across
benchmark tasks and real-world time-series datasets. Moreover, because the SR
method, similarly to RC, is well suited for hardware implementation, SR-RC
suggests a practical direction for deploying RC as Edge AI for time-series
anomaly detection.

</details>


### [482] [TED++: Submanifold-Aware Backdoor Detection via Layerwise Tubular-Neighbourhood Screening](https://arxiv.org/abs/2510.14299)
*Nam Le,Leo Yu Zhang,Kewen Liao,Shirui Pan,Wei Luo*

Main category: cs.LG

TL;DR: TED++是一个 submanifold 感知框架，用于检测逃避现有防御的隐蔽后门攻击，在具有挑战性的场景下实现了最先进的检测性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在关键应用中的普及带来了严重的后门攻击安全风险，而许多现有防御措施在这种攻击下显得脆弱。

Method: TED++首先围绕每个类的隐藏特征流形构建一个管状邻域，并从少量干净的激活中估计其局部“厚度”。然后，它应用局部自适应排名（LAR）来检测任何偏离可接受范围的激活。通过聚合所有层中这些基于LAR调整的排名，TED++能够捕捉输入在不断变化的类子流形上的保持程度。

Result: TED++在基准数据集和任务上进行了广泛的实验，在自适应攻击和有限数据场景下均实现了最先进的检测性能。即使每个类只有五个保留样本，TED++仍然能实现近乎完美的检测，在AUROC方面比次优方法提高了14%。

Conclusion: TED++通过利用激活在隐藏特征空间中的流形结构，有效地检测了逃避现有防御的隐蔽后门攻击，尤其在数据有限的情况下表现出色。

Abstract: As deep neural networks power increasingly critical applications, stealthy
backdoor attacks, where poisoned training inputs trigger malicious model
behaviour while appearing benign, pose a severe security risk. Many existing
defences are vulnerable when attackers exploit subtle distance-based anomalies
or when clean examples are scarce. To meet this challenge, we introduce TED++,
a submanifold-aware framework that effectively detects subtle backdoors that
evade existing defences. TED++ begins by constructing a tubular neighbourhood
around each class's hidden-feature manifold, estimating its local ``thickness''
from a handful of clean activations. It then applies Locally Adaptive Ranking
(LAR) to detect any activation that drifts outside the admissible tube. By
aggregating these LAR-adjusted ranks across all layers, TED++ captures how
faithfully an input remains on the evolving class submanifolds. Based on such
characteristic ``tube-constrained'' behaviour, TED++ flags inputs whose
LAR-based ranking sequences deviate significantly. Extensive experiments are
conducted on benchmark datasets and tasks, demonstrating that TED++ achieves
state-of-the-art detection performance under both adaptive-attack and
limited-data scenarios. Remarkably, even with only five held-out examples per
class, TED++ still delivers near-perfect detection, achieving gains of up to
14\% in AUROC over the next-best method. The code is publicly available at
https://github.com/namle-w/TEDpp.

</details>


### [483] [Active Measuring in Reinforcement Learning With Delayed Negative Effects](https://arxiv.org/abs/2510.14315)
*Daiqi Gao,Ziping Xu,Aseel Rawashdeh,Predrag Klasnja,Susan A. Murphy*

Main category: cs.LG

TL;DR: 在强化学习中，状态测量成本高昂且可能影响未来。本文提出主动可观测马尔可夫决策过程（AOMDP），智能体不仅选择动作，还决定是否测量潜在状态，测量可揭示真实状态但有延迟负面影响。这可提高样本效率和最优策略价值。将AOMDP表述为周期性部分可观测MDP，提出基于信念状态的在线强化学习算法，并用序贯蒙特卡洛法同时近似未知静态环境参数和未观测状态。在数字健康应用中评估了该算法，智能体可决定何时干预及何时通过调查评估用户健康。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的强化学习应用中，状态测量成本高昂，并可能对环境产生负面影响。

Method: 提出主动可观测马尔可夫决策过程（AOMDP），将其表述为周期性部分可观测MDP，并设计一种基于信念状态的在线强化学习算法。为近似信念状态，提出一种序贯蒙特卡洛方法，用于同时近似未知静态环境参数和未观测状态。

Result: 在数字健康应用中进行了评估，证明了该方法可以提高样本效率并增加最优策略的价值。

Conclusion: AOMDP框架和所提出的算法能有效处理状态测量成本和延迟负面影响的问题，并在数字健康等实际应用中展现出潜力。

Abstract: Measuring states in reinforcement learning (RL) can be costly in real-world
settings and may negatively influence future outcomes. We introduce the
Actively Observable Markov Decision Process (AOMDP), where an agent not only
selects control actions but also decides whether to measure the latent state.
The measurement action reveals the true latent state but may have a negative
delayed effect on the environment. We show that this reduced uncertainty may
provably improve sample efficiency and increase the value of the optimal policy
despite these costs. We formulate an AOMDP as a periodic partially observable
MDP and propose an online RL algorithm based on belief states. To approximate
the belief states, we further propose a sequential Monte Carlo method to
jointly approximate the posterior of unknown static environment parameters and
unobserved latent states. We evaluate the proposed algorithm in a digital
health application, where the agent decides when to deliver digital
interventions and when to assess users' health status through surveys.

</details>


### [484] [LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search](https://arxiv.org/abs/2510.14331)
*Shivam Singhal,Eran Malach,Tomaso Poggio,Tomer Galanti*

Main category: cs.LG

TL;DR: LLM-ERM是一种结合了LLM和ERM的程序学习框架，能够高效地学习样本，并解决梯度下降难以处理的程序合成问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决程序学习中样本效率和计算可行性之间的权衡问题，特别是梯度下降在学习某些短程序时需要指数级样本的问题。

Method: LLM-ERM框架通过LLM引导搜索候选程序，并使用无梯度的ERM风格的验证方法来替代穷举搜索。具体做法是：1. 使用预训练的、增强了推理能力的LLM生成k个候选程序。2. 编译并检查每个程序在给定数据上的表现。3. 返回经过验证的最佳假设程序，整个过程不涉及反馈、自适应或梯度计算。

Result: LLM-ERM在处理奇偶校验变体、模式匹配和素性测试等任务时，仅需少量样本（低至200个）即可成功。相比之下，经过SGD训练的Transformer即使在100,000个样本下仍然存在过拟合现象。

Conclusion: LLM-ERM在保持计算可行性的同时，恢复了有限类ERM的统计效率，为学习梯度下降难以企及的简洁假设提供了一条实用途径。

Abstract: We seek algorithms for program learning that are both sample-efficient and
computationally feasible. Classical results show that targets admitting short
program descriptions (e.g., with short ``python code'') can be learned with a
``small'' number of examples (scaling with the size of the code) via
length-first program enumeration, but the search is exponential in description
length. Consequently, Gradient-based training avoids this cost yet can require
exponentially many samples on certain short-program families.
  To address this gap, we introduce LLM-ERM, a propose-and-verify framework
that replaces exhaustive enumeration with an LLM-guided search over candidate
programs while retaining ERM-style selection on held-out data. Specifically, we
draw $k$ candidates with a pretrained reasoning-augmented LLM, compile and
check each on the data, and return the best verified hypothesis, with no
feedback, adaptivity, or gradients. Theoretically, we show that coordinate-wise
online mini-batch SGD requires many samples to learn certain short programs.
{\em Empirically, LLM-ERM solves tasks such as parity variants, pattern
matching, and primality testing with as few as 200 samples, while SGD-trained
transformers overfit even with 100,000 samples}. These results indicate that
language-guided program synthesis recovers much of the statistical efficiency
of finite-class ERM while remaining computationally tractable, offering a
practical route to learning succinct hypotheses beyond the reach of
gradient-based training.

</details>


### [485] [DARTS-GT: Differentiable Architecture Search for Graph Transformers with Quantifiable Instance-Specific Interpretability Analysis](https://arxiv.org/abs/2510.14336)
*Shruti Sarika Chakraborty,Peter Minary*

Main category: cs.LG

TL;DR: 图变换器（GT）通过引入不对称的注意机制，并结合可微分架构搜索（DARTS）来选择每层的最优图神经网络（GNN）算子，从而实现了在图结构数据上的高性能和可解释性。实验证明，DARTS-GT在多个基准测试中达到了最先进的性能，并且其发现的异构架构比基线模型更具可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前的图变换器（GT）模型设计僵化，缺乏可量化的可解释性。它们在所有层中使用固定的GNN类型，可能错失了根据深度选择不同组件的优势。此外，复杂的模型结构使其难以区分有意义的模式和虚假的相关性。

Method: 提出了一种新颖的GT注意力机制，通过不对称设计将结构编码与特征表示分离开：查询来自节点特征，而键和值来自GNN变换。在此框架内，使用可微分架构搜索（DARTS）为每一层选择最优的GNN算子，实现了DARTS-GT，从而在Transformer注意力内部实现了深度异构性。同时，开发了首个用于GTs的量化可解释性框架，通过因果消融分析（Head-deviation, Specialization, Focus指标）来识别驱动预测的注意力头和节点，并支持模型比较。

Result: DARTS-GT在八个基准测试中的四个数据集上取得了最先进的性能，在其他数据集上也具有竞争力。所发现的架构揭示了特定数据集的模式。可解释性分析表明，视觉注意力显著性和因果重要性并不总是相关，这意味着常用的可视化方法可能会遗漏真正重要的组件。DARTS-GT发现的异构架构模型比基线模型更具可解释性。

Conclusion: 图变换器（GT）不必在性能和可解释性之间进行选择。通过引入不对称注意力机制和DARTS，可以实现高性能和高可解释性的模型。

Abstract: Graph Transformers (GTs) have emerged as powerful architectures for
graph-structured data, yet remain constrained by rigid designs and lack
quantifiable interpretability. Current state-of-the-art GTs commit to fixed GNN
types across all layers, missing potential benefits of depth-specific component
selection, while their complex architectures become opaque where performance
gains cannot be distinguished between meaningful patterns and spurious
correlations. We redesign GT attention through asymmetry, decoupling structural
encoding from feature representation: queries derive from node features while
keys and values come from GNN transformations. Within this framework, we use
Differentiable ARchiTecture Search (DARTS) to select optimal GNN operators at
each layer, enabling depth-wise heterogeneity inside transformer attention
itself (DARTS-GT). To understand discovered architectures, we develop the first
quantitative interpretability framework for GTs through causal ablation. Our
metrics (Head-deviation, Specialization, and Focus), identify which heads and
nodes drive predictions while enabling model comparison. Experiments across
eight benchmarks show DARTS-GT achieves state-of-the-art on four datasets while
remaining competitive on others, with discovered architectures revealing
dataset-specific patterns. Our interpretability analysis reveals that visual
attention salience and causal importance do not always correlate, indicating
widely used visualization approaches may miss components that actually matter.
Crucially, heterogeneous architectures found by DARTS-GT consistently produced
more interpretable models than baselines, establishing that Graph Transformers
need not choose between performance and interpretability.

</details>


### [486] [Stop-RAG: Value-Based Retrieval Control for Iterative RAG](https://arxiv.org/abs/2510.14337)
*Jaewan Park,Solbee Cho,Jay-Yoon Lee*

Main category: cs.LG

TL;DR: 迭代式检索增强生成（RAG）虽然能回答复杂的多步问题，但会增加延迟、成本和引入干扰信息的风险。我们提出Stop-RAG，一种基于价值的控制器，通过将迭代式RAG建模为有限时间马尔可夫决策过程，来决定何时停止检索。Stop-RAG在多步问答基准测试中表现优于固定迭代和基于提示的停止方法。


<details>
  <summary>Details</summary>
Motivation: 迭代式RAG虽然能回答复杂问题，但每次迭代都会增加延迟、成本和引入干扰信息的风险，因此需要一种有效的停止策略。

Method: 将迭代式RAG建模为有限时间马尔可夫决策过程，并引入基于价值的控制器Stop-RAG来决定何时停止检索。使用完整的Q($\lambda$)目标进行训练。

Result: Stop-RAG在多步问答基准测试中，始终优于固定迭代基线和基于LLM的提示停止方法。

Conclusion: 自适应停止是当前系统中缺失的关键组件，基于价值的控制可以提高RAG系统的准确性。

Abstract: Iterative retrieval-augmented generation (RAG) enables large language models
to answer complex multi-hop questions, but each additional loop increases
latency, costs, and the risk of introducing distracting evidence, motivating
the need for an efficient stopping strategy. Existing methods either use a
predetermined number of iterations or rely on confidence proxies that poorly
reflect whether more retrieval will actually help. We cast iterative RAG as a
finite-horizon Markov decision process and introduce Stop-RAG, a value-based
controller that adaptively decides when to stop retrieving. Trained with
full-width forward-view Q($\lambda$) targets from complete trajectories,
Stop-RAG learns effective stopping policies while remaining compatible with
black-box APIs and existing pipelines. On multi-hop question-answering
benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines
and prompting-based stopping with LLMs. These results highlight adaptive
stopping as a key missing component in current agentic systems, and demonstrate
that value-based control can improve the accuracy of RAG systems.

</details>


### [487] [Jet Functors and Weil Algebras in Automatic Differentiation: A Geometric Analysis](https://arxiv.org/abs/2510.14342)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 该论文提出了一种使用矢量丛和Weil 代数的自动微分（AD）的几何表述，将反向模式 AD 解释为余切丛-拉回，将 Taylor 模式解释为 Weil 代数中的求值。


<details>
  <summary>Details</summary>
Motivation: 利用微分几何的原理，通过矢量丛和 Weil 代数，为自动微分（AD）提供一种新的几何表述，旨在更深入地理解 AD 的理论基础，并为深度学习和科学计算中结构保持微分方法的发展奠定基础。

Method: 采用矢量丛和 Weil 代数的数学框架来表述自动微分。将反向模式 AD 建模为余切丛-拉回（cotangent-pullback），将 Taylor 模式 AD 解释为在 Weil 代数中的求值。利用此框架推导出关于正确性、稳定性和复杂性的结论，例如反向模式 AD 的函子身份、高阶导数的代数精确性以及截断误差的显式界限。

Result: 推导出了反向模式 AD 的函子恒等式，证明了高阶导数的代数精确性，并给出了截断误差的明确界限。展示了如何使用张量化 Weil 代数以线性成本单次计算所有混合导数，避免了嵌套 JVP/VJP 调度的组合爆炸。

Conclusion: 该几何表述将 AD 理论置于微分几何的视角下，为在深度学习和科学计算领域开发结构保持的微分方法提供了一个理论基础。

Abstract: We present a geometric formulation of automatic differentiation (AD) using
jet bundles and Weil algebras. Reverse-mode AD emerges as cotangent-pullback,
while Taylor-mode corresponds to evaluation in a Weil algebra. From these
principles, we derive concise statements on correctness, stability, and
complexity: a functorial identity for reverse-mode, algebraic exactness of
higher-order derivatives, and explicit bounds on truncation error. We further
show that tensorized Weil algebras permit one-pass computation of all mixed
derivatives with cost linear in the algebra dimension, avoiding the
combinatorial blow-up of nested JVP/VJP schedules. This framework interprets AD
theory through the lens of differential geometry and offers a foundation for
developing structure-preserving differentiation methods in deep learning and
scientific computing. Code and examples are available at
https://git.nilu.no/geometric-ad/jet-weil-ad.

</details>


### [488] [Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers](https://arxiv.org/abs/2510.14381)
*Andrew Zhao,Reshmi Ghosh,Vitor Carvalho,Emily Lawton,Keegan Hines,Gao Huang,Jack W. Stokes*

Main category: cs.LG

TL;DR: LLM提示优化存在安全漏洞，攻击者可以通过操纵反馈来提高攻击成功率，但可以通过简单的防御措施来缓解。


<details>
  <summary>Details</summary>
Motivation: LLM系统在日常AI应用中广泛使用，但其性能依赖于提示优化，而提示优化阶段的安全问题尚未得到充分研究。

Method: 通过HarmBench对LLM提示优化进行系统性的中毒风险分析，并引入了一种不需要访问奖励模型的虚假奖励攻击，同时提出了一种轻量级的突出显示防御措施。

Result: 与注入查询攻击相比，基于反馈的攻击将攻击成功率（ASR）提高了高达0.48。虚假奖励攻击显著增加了易受攻击性，将ASR从0.23提高到0.07，同时不影响模型性能。

Conclusion: 提示优化管道是一个重要的攻击面，需要加强对反馈渠道和优化框架的安全防护。

Abstract: Large language model (LLM) systems now underpin everyday AI applications such
as chatbots, computer-use assistants, and autonomous robots, where performance
often depends on carefully designed prompts. LLM-based prompt optimizers reduce
that effort by iteratively refining prompts from scored feedback, yet the
security of this optimization stage remains underexamined. We present the first
systematic analysis of poisoning risks in LLM-based prompt optimization. Using
HarmBench, we find systems are substantially more vulnerable to manipulated
feedback than to injected queries: feedback-based attacks raise attack success
rate (ASR) by up to $\Delta$ASR = 0.48. We introduce a simple fake-reward
attack that requires no access to the reward model and significantly increases
vulnerability, and we propose a lightweight highlighting defense that reduces
the fake-reward $\Delta$ASR from 0.23 to 0.07 without degrading utility. These
results establish prompt optimization pipelines as a first-class attack surface
and motivate stronger safeguards for feedback channels and optimization
frameworks.

</details>


### [489] [Revisit Modality Imbalance at the Decision Layer](https://arxiv.org/abs/2510.14411)
*Xiaoyu Ma,Hao Chen*

Main category: cs.LG

TL;DR: 多模态学习中存在跨模态不平衡问题，即使经过预训练和优化，模型仍偏向某些模态（如音频）。这种不平衡源于特征空间和决策权重分布的差异，而非仅是优化过程。文章提出未来多模态系统应在决策层引入自适应权重分配机制，以平衡各模态的贡献。


<details>
  <summary>Details</summary>
Motivation: 多模态学习旨在整合不同信息源以提升模型性能，但普遍存在模态不平衡问题，即优势模态在联合优化中会压倒弱势模态。

Method: 通过在音频-视觉数据集（CREMAD 和 Kinetic-Sounds）上进行实验，分析模型在表示学习和决策层中出现的系统性偏见，并探究其产生的根源（特征空间和决策权重分布的内在差异）。

Result: 实验表明，即使经过大量预训练和平衡优化，模型仍表现出对某些模态（如音频）的系统性偏见。这种偏见并非仅由优化动态引起，而是源于特征空间和决策权重分布的内在差异。

Conclusion: 当前多模态系统在融合阶段聚合未经校准的模态输出来进行决策，导致决策层权重不平衡，阻碍了弱势模态的有效贡献。文章建议未来的多模态系统应在决策层集成自适应权重分配机制，根据各模态的能力进行相对平衡的分配。

Abstract: Multimodal learning integrates information from different modalities to
enhance model performance, yet it often suffers from modality imbalance, where
dominant modalities overshadow weaker ones during joint optimization. This
paper reveals that such an imbalance not only occurs during representation
learning but also manifests significantly at the decision layer. Experiments on
audio-visual datasets (CREMAD and Kinetic-Sounds) show that even after
extensive pretraining and balanced optimization, models still exhibit
systematic bias toward certain modalities, such as audio. Further analysis
demonstrates that this bias originates from intrinsic disparities in
feature-space and decision-weight distributions rather than from optimization
dynamics alone. We argue that aggregating uncalibrated modality outputs at the
fusion stage leads to biased decision-layer weighting, hindering weaker
modalities from contributing effectively. To address this, we propose that
future multimodal systems should focus more on incorporate adaptive weight
allocation mechanisms at the decision layer, enabling relative balanced
according to the capabilities of each modality.

</details>


### [490] [Interaction Concordance Index: Performance Evaluation for Interaction Prediction Methods](https://arxiv.org/abs/2510.14419)
*Tapio Pahikkala,Riikka Numminen,Parisa Movahedi,Napsu Karmitsa,Antti Airola*

Main category: cs.LG

TL;DR: 本文提出了一种名为交互一致性指数（IC-index）的新指标，用于评估预测药物-靶点亲和力（DTA）模型捕捉交互方向的能力，并对其在实际应用中的意义进行了实证评估。


<details>
  <summary>Details</summary>
Motivation: 现有DTA预测模型主要关注预测准确性，但忽略了交互方向的预测能力。然而，正确捕捉交互方向对于药物分配等实际应用至关重要。

Method: 提出交互一致性指数（IC-index）来评估预测模型捕捉交互方向的能力。通过理论和实验分析了IC-index在固定预测器和机器学习算法上的表现，并探讨了如何通过引入侧信息来克服排列等变性带来的局限性。

Result: 实验表明，IC-index能够有效评估模型捕捉交互方向的能力，并为理解不同亲和力预测方法的优劣提供了补充视角。在多种生物医学数据集上，IC-index与现有的预测性能估计器相辅相成。

Conclusion: IC-index为评估DTA预测模型提供了一个有价值的新维度，尤其是在需要理解和利用交互方向的应用场景中。通过结合侧信息，可以提高模型在处理未见数据时的交互预测能力。

Abstract: Consider two sets of entities and their members' mutual affinity values, say
drug-target affinities (DTA). Drugs and targets are said to interact in their
effects on DTAs if drug's effect on it depends on the target. Presence of
interaction implies that assigning a drug to a target and another drug to
another target does not provide the same aggregate DTA as the reversed
assignment would provide. Accordingly, correctly capturing interactions enables
better decision-making, for example, in allocation of limited numbers of drug
doses to their best matching targets. Learning to predict DTAs is popularly
done from either solely from known DTAs or together with side information on
the entities, such as chemical structures of drugs and targets. In this paper,
we introduce interaction directions' prediction performance estimator we call
interaction concordance index (IC-index), for both fixed predictors and machine
learning algorithms aimed for inferring them. IC-index complements the
popularly used DTA prediction performance estimators by evaluating the ratio of
correctly predicted directions of interaction effects in data. First, we show
the invariance of IC-index on predictors unable to capture interactions.
Secondly, we show that learning algorithm's permutation equivariance regarding
drug and target identities implies its inability to capture interactions when
either drug, target or both are unseen during training. In practical
applications, this equivariance is remedied via incorporation of appropriate
side information on drugs and targets. We make a comprehensive empirical
evaluation over several biomedical interaction data sets with various
state-of-the-art machine learning algorithms. The experiments demonstrate how
different types of affinity strength prediction methods perform in terms of
IC-index complementing existing prediction performance estimators.

</details>


### [491] [MergeMoE: Efficient Compression of MoE Models via Expert Output Merging](https://arxiv.org/abs/2510.14436)
*Ruijie Miao,Yilun Yao,Zihan Wang,Zhiming Wang,Bairen Yi,LingJun Liu,Yikai Zhao,Tong Yang*

Main category: cs.LG

TL;DR: Mixure-of-Experts (MoE) 模型压缩是研究重点，本文提出一种基于输出合并理论分析的MergeMoE方法，通过优化构造压缩矩阵，在压缩率相同的情况下优于基线方法。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然能提升模型规模且应用广泛，但其巨大的内存开销使得模型压缩成为一个重要的研究方向。

Method: 将MoE模型压缩中的专家合并视为合并专家输出的过程，而非参数聚合。该过程可解释为在模型前向计算中插入额外的矩阵，从而引出一种优化表述。基于此，提出MergeMoE方法，利用数学优化来构造压缩矩阵。

Result: 在多个MoE模型上评估MergeMoE，结果显示该算法在相同的压缩率下始终优于基线方法。

Conclusion: MergeMoE 通过数学优化实现了 MoE 模型的有效压缩，并在实验中取得了优于现有方法的性能。

Abstract: The Mixture-of-Experts (MoE) technique has proven to be a promising solution
to efficiently scale the model size, which has been widely applied in recent
LLM advancements. However, the substantial memory overhead of MoE models has
made their compression an important research direction. In this work, we
provide a theoretical analysis of expert merging, a recently proposed technique
for compressing MoE models. Rather than interpreting expert merging from the
conventional perspective of parameter aggregation, we approach it from the
perspective of merging experts' outputs. Our key insight is that the merging
process can be interpreted as inserting additional matrices into the forward
computation, which naturally leads to an optimization formulation. Building on
this analysis, we introduce MergeMoE, a method that leverages mathematical
optimization to construct the compression matrices. We evaluate MergeMoE on
multiple MoE models and show that our algorithm consistently outperforms the
baselines with the same compression ratios.

</details>


### [492] [A Free Lunch in LLM Compression: Revisiting Retraining after Pruning](https://arxiv.org/abs/2510.14444)
*Moritz Wagner,Christophe Roux,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: LLM剪枝通常不需要重新训练，但现有方法通过层级掩码选择和校准数据上的重构来避免完全重新训练，因为LLM的计算成本过高。本文研究了剪枝后恢复权重时的关键设计选择，通过对GPT架构进行广泛的计算研究，发现将注意力（attention）和MLP组件在每个Transformer块内单独重构，不仅资源效率高，而且实现了最佳的困惑度（perplexity），甚至优于完全重新训练，同时内存占用更少。此外，简单剪枝标准（如Wanda）在正确执行重构步骤后，效果优于更复杂的方法，凸显了重构的重要性。这些发现挑战了普遍认为应不惜一切代价避免重训练的观点，并为LLM剪枝后的性能恢复提供了重要见解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM剪枝方法为避免计算成本高昂的完全重新训练，选择层级掩码选择和基于校准数据的重构。然而，这些方法在实际应用中可能在更粗粒度上进行重构（如整个Transformer块），而非单独的矩阵。本文旨在研究剪枝后恢复权重时的关键设计选择，并探索更优的重构策略。

Method: 对先进的GPT架构进行广泛的计算研究，比较不同粒度（如单独的注意力组件和MLP组件）的重构或重新训练策略，并评估其资源效率和模型性能（以困惑度衡量）。

Result: 研究发现，在每个Transformer块内单独重构注意力和MLP组件，是一种资源效率高且性能最佳的策略（免费午餐场景）。这种方法在内存占用仅为完全重训练一小部分的情况下，实现了比完全重训练更好的性能。此外，简单的剪枝标准（如Wanda）在配合适当的重构步骤时，表现优于更复杂的剪枝方法。

Conclusion: 本文的研究结果表明，对LLM进行剪枝后，通过在Transformer块内单独重构注意力与MLP组件，可以实现高效且高性能的权重恢复，甚至优于完全重新训练。这挑战了普遍认为应避免重训练的观点，并强调了在LLM剪枝后，恰当的重构步骤对于性能恢复至关重要。

Abstract: While Neural Network pruning typically requires retraining the model to
recover pruning-induced performance degradation, state-of-the-art Large
Language Models (LLMs) pruning methods instead solve a layer-wise mask
selection and reconstruction problem on a small set of calibration data to
avoid full retraining, as it is considered computationally infeasible for LLMs.
Reconstructing single matrices in isolation has favorable properties, such as
convexity of the objective and significantly reduced memory requirements
compared to full retraining. In practice, however, reconstruction is often
implemented at coarser granularities, e.g., reconstructing a whole transformer
block against its dense activations instead of a single matrix. In this work,
we study the key design choices when reconstructing or retraining the remaining
weights after pruning. We conduct an extensive computational study on
state-of-the-art GPT architectures, and report several surprising findings that
challenge common intuitions about retraining after pruning. In particular, we
observe a free lunch scenario: reconstructing attention and MLP components
separately within each transformer block is nearly the most resource-efficient
yet achieves the best perplexity. Most importantly, this Pareto-optimal setup
achieves better performance than full retraining, despite requiring only a
fraction of the memory. Furthermore, we demonstrate that simple and efficient
pruning criteria such as Wanda can outperform much more complex approaches when
the reconstruction step is properly executed, highlighting its importance. Our
findings challenge the narrative that retraining should be avoided at all costs
and provide important insights into post-pruning performance recovery for LLMs.

</details>


### [493] [Towards geological inference with process-based and deep generative modeling, part 1: training on fluvial deposits](https://arxiv.org/abs/2510.14445)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: GANs can reproduce complex 3D fluvial deposits from process-based models, overcoming limitations of previous generative models.


<details>
  <summary>Details</summary>
Motivation: Current generative models struggle to reproduce geological structures, especially continuous fluvial deposits. This study investigates if GANs can effectively model these structures.

Method: A GAN was trained using 3D fluvial deposit data simulated by a process-based model. An ablation study confirmed the transferability of 2D image generation techniques to 3D. The training stability, non-stationarity, and details of the deposits were evaluated, along with adherence to the law of superposition.

Result: The GAN successfully reproduced 3D fluvial deposits, capturing non-stationarity and details without mode collapse or memorization. Training was stable, and the generated samples honored the law of superposition, validated by deposition time.

Conclusion: GANs are robust for generating specific geological structures like fluvial deposits, even with complex data from process-based models. Future research should explore GANs with larger 3D datasets and multimodal data, and incorporate geological principles like the law of superposition.

Abstract: The distribution of resources in the subsurface is deeply linked to the
variations of its physical properties. Generative modeling has long been used
to predict those physical properties while quantifying the associated
uncertainty. But current approaches struggle to properly reproduce geological
structures, and fluvial deposits in particular, because of their continuity.
This study explores whether a generative adversarial network (GAN) - a type of
deep-learning algorithm for generative modeling - can be trained to reproduce
fluvial deposits simulated by a process-based model - a more expensive model
that mimics geological processes. An ablation study shows that developments
from the deep-learning community to generate large 2D images are directly
transferable to 3D images of fluvial deposits. Training remains stable, and the
generated samples reproduce the non-stationarity and details of the deposits
without mode collapse or pure memorization of the training data. Using a
process-based model to generate those training data allows us to include
valuable properties other than the usual physical properties. We show how the
deposition time let us monitor and validate the performance of a GAN by
checking that its samples honor the law of superposition. Our work joins a
series of previous studies suggesting that GANs are more robust that given
credit for, at least for training datasets targeting specific geological
structures. Whether this robustness transfers to larger 3D images and
multimodal datasets remains to be seen. Exploring how deep generative models
can leverage geological principles like the law of superposition shows a lot of
promise.

</details>


### [494] [Feature Selection and Regularization in Multi-Class Classification: An Empirical Study of One-vs-Rest Logistic Regression with Gradient Descent Optimization and L1 Sparsity Constraints](https://arxiv.org/abs/2510.14449)
*Jahidul Arafat,Fariha Tasmin,Md Kaosar Uddin,Sanjaya Poudel,Eftakhar Ahmed Arnob*

Main category: cs.LG

TL;DR: 本文研究了在UCI葡萄酒数据集上使用One-vs-Rest逻辑回归进行多类别葡萄酒分类，比较了从头实现的梯度下降与scikit-learn优化求解器的性能，并量化了L1正则化对特征稀疏性的影响。手动梯度下降实现了92.59%的平均测试准确率，而scikit-learn的训练速度快24倍，准确率达到98.15%。L1正则化可减少54-69%的特征数量，准确率仅下降4.63%。我们提出了一种最优的5特征子集，可将复杂度降低62%，准确率估计在92-94%，并能节省成本和时间。


<details>
  <summary>Details</summary>
Motivation: 多类别葡萄酒分类在模型准确性、特征维度和可解释性之间存在基本权衡，这些是分析化学产品部署的关键因素。

Method: 本文对UCI葡萄酒数据集（178个样本，3个栽培品种，13个化学特征）上的One-vs-Rest逻辑回归进行了全面的实证研究，将从头实现的梯度下降与scikit-learn的优化求解器进行了比较，并量化了L1正则化对特征稀疏性的影响。

Result: 手动梯度下降实现了92.59%的平均测试准确率，而scikit-learn提供了24倍的训练速度提升和98.15%的准确率。L1正则化产生了54-69%的特征缩减，准确率仅下降4.63%。我们提出了一种最优的5特征子集，可将复杂度降低62%，估计准确率为92-94%，可节省每样本80美元和56%的时间。统计验证证实了具有低于2毫秒预测延迟的稳健泛化能力，适用于实时质量控制。

Conclusion: 研究结果为在资源受限环境下平衡全面的化学分析与目标特征测量提供了可行的指导方针。所提出的最优5特征子集可在保证高准确率的同时，显著降低成本和时间，适用于实时质量控制。

Abstract: Multi-class wine classification presents fundamental trade-offs between model
accuracy, feature dimensionality, and interpretability - critical factors for
production deployment in analytical chemistry. This paper presents a
comprehensive empirical study of One-vs-Rest logistic regression on the UCI
Wine dataset (178 samples, 3 cultivars, 13 chemical features), comparing
from-scratch gradient descent implementation against scikit-learn's optimized
solvers and quantifying L1 regularization effects on feature sparsity. Manual
gradient descent achieves 92.59 percent mean test accuracy with smooth
convergence, validating theoretical foundations, though scikit-learn provides
24x training speedup and 98.15 percent accuracy. Class-specific analysis
reveals distinct chemical signatures with heterogeneous patterns where color
intensity varies dramatically (0.31 to 16.50) across cultivars. L1
regularization produces 54-69 percent feature reduction with only 4.63 percent
accuracy decrease, demonstrating favorable interpretability-performance
trade-offs. We propose an optimal 5-feature subset achieving 62 percent
complexity reduction with estimated 92-94 percent accuracy, enabling
cost-effective deployment with 80 dollars savings per sample and 56 percent
time reduction. Statistical validation confirms robust generalization with
sub-2ms prediction latency suitable for real-time quality control. Our findings
provide actionable guidelines for practitioners balancing comprehensive
chemical analysis against targeted feature measurement in resource-constrained
environments.

</details>


### [495] [Coder as Editor: Code-driven Interpretable Molecular Optimization](https://arxiv.org/abs/2510.14455)
*Wenyu Zhu,Chengzhu Li,Xiaohe Tian,Yifan Wang,Yinjun Jia,Jianhui Wang,Bowen Gao,Ya-Qin Zhang,Wei-Ying Ma,Yanyan Lan*

Main category: cs.LG

TL;DR: MECo框架通过将编辑操作翻译成可执行代码，实现了分子优化中LLM的推理和执行之间的桥梁，提高了分子设计的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: LLMs在分子优化任务中，尤其是在处理SMILES等非直观表示时，难以精确执行编辑操作，需要一个能桥接推理和执行的框架。

Method: MECo将分子优化重构为LLM的级联框架：首先从分子和属性目标生成人类可解释的编辑意图，然后通过代码生成将这些意图转化为可执行的结构编辑。

Result: MECo在重现的编辑任务上准确率超过98%，在物理化学性质和靶点活性的优化基准测试中，一致性提高了38-86个百分点（达到90%以上），并提高了成功率，同时保持了结构相似性。

Conclusion: MECo通过对齐意图和执行，实现了分子设计的连贯性、可控性和可解释性，为药物发现中的高保真反馈循环和人机协作工作流程奠定了基础。

Abstract: Molecular optimization is a central task in drug discovery that requires
precise structural reasoning and domain knowledge. While large language models
(LLMs) have shown promise in generating high-level editing intentions in
natural language, they often struggle to faithfully execute these
modifications-particularly when operating on non-intuitive representations like
SMILES. We introduce MECo, a framework that bridges reasoning and execution by
translating editing actions into executable code. MECo reformulates molecular
optimization for LLMs as a cascaded framework: generating human-interpretable
editing intentions from a molecule and property goal, followed by translating
those intentions into executable structural edits via code generation. Our
approach achieves over 98% accuracy in reproducing held-out realistic edits
derived from chemical reactions and target-specific compound pairs. On
downstream optimization benchmarks spanning physicochemical properties and
target activities, MECo substantially improves consistency by 38-86 percentage
points to 90%+ and achieves higher success rates over SMILES-based baselines
while preserving structural similarity. By aligning intention with execution,
MECo enables consistent, controllable and interpretable molecular design,
laying the foundation for high-fidelity feedback loops and collaborative
human-AI workflows in drug discovery.

</details>


### [496] [Holdout-Loss-Based Data Selection for LLM Finetuning via In-Context Learning](https://arxiv.org/abs/2510.14459)
*Ling Zhang,Xianliang Yang,Juwon Yu,Park Cheonyoung,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: 通过上下文近似（ICA）框架，可以高效地为预训练语言模型选择和重加权有价值的训练数据，以提高与人类偏好的对齐度，而无需昂贵的重新训练或参考模型。


<details>
  <summary>Details</summary>
Motivation: 对齐预训练语言模型（PLM）通常通过微调完成，但存在噪声或偏离目标的示例可能稀释监督信号的问题。虽然小型的、精心挑选的数据集通常能达到与大型数据集相当的性能，但系统性地识别高价值训练数据的方法仍有待探索，现有方法依赖于启发式或昂贵的重新训练。

Method: 提出一个基于理论的、资源高效的数据选择和重加权框架。核心是上下文近似（ICA），通过在上下文中以一小组精心策划的样本进行条件化，来估计模型在对候选样本进行训练后将产生的损失。ICA不需要参考模型，也不需要额外的微调。在局部线性化下，ICA等同于朝着样本外最优化的第一阶更新，这证明了其作为数据价值代理的合理性。从ICA得分中导出每样本权重，动态地根据模型参数的演变来重加权梯度更新。

Result: 在SFT、DPO和SimPO等多种场景下，并针对不同的主干模型和数据集，基于ICA的重加权方法能够以最小的开销，持续地提高模型与人类偏好的对齐度。分析了得分更新频率和用于上下文演示的k个样本选择对结果的影响，并指出了其在快速变化的on-policy更新方面的局限性，为未来的工作提供了方向。

Conclusion: 所提出的上下文近似（ICA）框架提供了一种新颖且高效的方法，用于选择和重加权用于微调大型预训练语言模型的训练数据。通过利用上下文学习来估计数据价值，ICA避免了昂贵的重新训练和参考模型的需求，并在多项对齐任务中展现出优越性能，为开发更有效的模型对齐策略铺平了道路。

Abstract: Fine-tuning large pretrained language models is a common approach for
aligning them with human preferences, but noisy or off-target examples can
dilute supervision. While small, well-chosen datasets often match the
performance of much larger ones, systematic and efficient ways to identify
high-value training data remain underexplored. Many current methods rely on
heuristics or expensive retraining. We present a theoretically grounded,
resource-efficient framework for data selection and reweighting. At its core is
an In-Context Approximation (ICA) that estimates the holdout loss a model would
incur after training on a candidate example by conditioning on a small, curated
holdout set in context. ICA requires no reference model and no additional
finetuning. Under a local linearization, ICA is equivalent to a first-order
update toward the holdout optimum, motivating its use as a proxy for data
value. We derive per-example weights from ICA scores, dynamically reweighting
gradient updates as model parameters evolve. Across SFT, DPO, and SimPO, and
over diverse backbones and datasets, ICA-based reweighting consistently
improves model alignment with minimal overhead. We analyze sensitivity to score
update frequency and the choice of $k$ holdout examples for in-context
demonstrations, and note limitations for rapidly drifting on-policy updates,
highlighting directions for future work. Code and prompts will be released.

</details>


### [497] [From Guess2Graph: When and How Can Unreliable Experts Safely Boost Causal Discovery in Finite Samples?](https://arxiv.org/abs/2510.14488)
*Sujai Hiremath,Dominik Janzing,Philipp Faller,Patrick Blöbaum,Elke Kirschbaum,Shiva Prasad Kasiviswanathan,Kyra Gan*

Main category: cs.LG

TL;DR: 该研究提出Guess2Graph（G2G）框架，通过结合专家知识（如LLM）来改进因果发现算法在样本量有限时表现不佳的问题。与直接使用专家预测不同，G2G框架引导统计测试的顺序，从而在保持统计一致性的同时提高性能。研究提出了PC-Guess和gPC-Guess两种G2G的具体实现，并进行了理论和实证分析，证明了该框架在专家知识准确性提高时能够单调提升性能，其中gPC-Guess表现出更显著的性能增益。


<details>
  <summary>Details</summary>
Motivation: 有限样本下因果发现算法性能不佳，而利用专家知识（包括LLM）作为约束可以提高性能，但现有方法需要完美的预测或不确定性估计，这在实际中并不可靠。

Method: 提出Guess2Graph（G2G）框架，该框架利用专家猜测来指导统计测试的顺序，而不是取代它们。开发了G2G的两种实现：PC-Guess（增强PC算法）和gPC-Guess（为更好地利用高质量专家输入而设计的学习增强变体）。

Result: 理论上，无论专家是否存在错误，两者都保持正确性。在有限样本中，当专家表现优于随机水平时，gPC-Guess的性能可被证明优于其非增强版本。实证研究表明，两者随着专家准确性的提高呈现单调改进，gPC-Guess实现了显著更强的增益。

Conclusion: G2G框架通过引导统计测试顺序而非直接替换，有效利用专家知识改进了因果发现算法在有限样本下的性能，并且在专家准确性越高的情况下，性能提升越明显。

Abstract: Causal discovery algorithms often perform poorly with limited samples. While
integrating expert knowledge (including from LLMs) as constraints promises to
improve performance, guarantees for existing methods require perfect
predictions or uncertainty estimates, making them unreliable for practical use.
We propose the Guess2Graph (G2G) framework, which uses expert guesses to guide
the sequence of statistical tests rather than replacing them. This maintains
statistical consistency while enabling performance improvements. We develop two
instantiations of G2G: PC-Guess, which augments the PC algorithm, and
gPC-Guess, a learning-augmented variant designed to better leverage
high-quality expert input. Theoretically, both preserve correctness regardless
of expert error, with gPC-Guess provably outperforming its non-augmented
counterpart in finite samples when experts are "better than random."
Empirically, both show monotonic improvement with expert accuracy, with
gPC-Guess achieving significantly stronger gains.

</details>


### [498] [Learning to Undo: Rollback-Augmented Reinforcement Learning with Reversibility Signals](https://arxiv.org/abs/2510.14503)
*Andrejs Sorstkins,Omer Tariq,Muhammad Bilal*

Main category: cs.LG

TL;DR: 本论文提出了一种可逆学习框架，以提高基于价值的强化学习代理的鲁棒性和效率，解决了在部分不可逆环境中容易出现价值高估和不稳定的问题。该框架包含两个核心机制：一个经验性转换可逆性度量Phi of s and a，以及一个选择性的状态回滚操作。通过结合可逆性感知评估和目标回滚，该方法提高了安全性、性能和稳定性。在CliffWalking v0和Taxi v3环境中，该框架显著提高了回报和奖励，同时减少了灾难性错误和非预期行为。


<details>
  <summary>Details</summary>
Motivation: 为了提高基于价值的强化学习代理在部分不可逆环境中的鲁棒性和效率，解决其价值高估和不稳定的问题。

Method: 提出一个包含两个核心机制的可逆学习框架：1. 经验性转换可逆性度量Phi of s and a，用于量化在固定时间范围内返回先验状态的可能性，并动态调整时间差更新中的惩罚项。2. 选择性的状态回滚操作，当预期回报显著低于即时估计值并违反预定阈值时，代理会受到惩罚并返回到前一个状态，以中断次优轨迹并避免灾难性步骤。

Result: 在CliffWalking v0环境中，将灾难性跌落减少了99.8%以上，每轮平均回报提高了55%。在Taxi v3环境中，非法行为减少了99.9%以上，累积奖励提高了65.7%，并显著降低了两种环境的奖励方差。烧蚀研究证实回滚机制是安全性和性能提升的关键。

Conclusion: 该可逆学习框架通过结合可逆性感知评估和目标回滚，成功提高了强化学习代理的安全性、性能和稳定性，是迈向安全可靠的序贯决策的重要一步。回滚机制被证明是提升安全性和性能的关键。

Abstract: This paper proposes a reversible learning framework to improve the robustness
and efficiency of value based Reinforcement Learning agents, addressing
vulnerability to value overestimation and instability in partially irreversible
environments. The framework has two complementary core mechanisms: an
empirically derived transition reversibility measure called Phi of s and a, and
a selective state rollback operation. We introduce an online per state action
estimator called Phi that quantifies the likelihood of returning to a prior
state within a fixed horizon K. This measure is used to adjust the penalty term
during temporal difference updates dynamically, integrating reversibility
awareness directly into the value function. The system also includes a
selective rollback operator. When an action yields an expected return markedly
lower than its instantaneous estimated value and violates a predefined
threshold, the agent is penalized and returns to the preceding state rather
than progressing. This interrupts sub optimal high risk trajectories and avoids
catastrophic steps. By combining reversibility aware evaluation with targeted
rollback, the method improves safety, performance, and stability. In the
CliffWalking v0 domain, the framework reduced catastrophic falls by over 99.8
percent and yielded a 55 percent increase in mean episode return. In the Taxi
v3 domain, it suppressed illegal actions by greater than or equal to 99.9
percent and achieved a 65.7 percent improvement in cumulative reward, while
also sharply reducing reward variance in both environments. Ablation studies
confirm that the rollback mechanism is the critical component underlying these
safety and performance gains, marking a robust step toward safe and reliable
sequential decision making.

</details>


### [499] [Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective](https://arxiv.org/abs/2510.14510)
*Xingjian Wu,Xiangfei Qiu,Hanyin Cheng,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: 本研究提出选择性表示空间（SRS）模块，通过自适应选择和重组时间序列的块来改进时间序列预测，并在多个真实世界数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的时间序列分块技术使用固定的相邻块，导致表示空间受限，未能充分提取信息，影响了长期依赖建模和预测性能。

Method: 提出选择性表示空间（SRS）模块，包含可学习的选择性分块和动态重组技术，用于自适应地选择和重组上下文时间序列中的块，以生成更具表现力的表示。

Result: 提出的SRSNet在多个领域的真实世界数据集上实现了最先进的性能，证明了SRS模块的有效性。

Conclusion: SRS模块作为一种新颖的即插即用模块，可以提升现有基于块的模型在时间序列预测任务上的性能。

Abstract: Time Series Forecasting has made significant progress with the help of
Patching technique, which partitions time series into multiple patches to
effectively retain contextual semantic information into a representation space
beneficial for modeling long-term dependencies. However, conventional patching
partitions a time series into adjacent patches, which causes a fixed
representation space, thus resulting in insufficiently expressful
representations. In this paper, we pioneer the exploration of constructing a
selective representation space to flexibly include the most informative patches
for forecasting. Specifically, we propose the Selective Representation Space
(SRS) module, which utilizes the learnable Selective Patching and Dynamic
Reassembly techniques to adaptively select and shuffle the patches from the
contextual time series, aiming at fully exploiting the information of
contextual time series to enhance the forecasting performance of patch-based
models. To demonstrate the effectiveness of SRS module, we propose a simple yet
effective SRSNet consisting of SRS and an MLP head, which achieves
state-of-the-art performance on real-world datasets from multiple domains.
Furthermore, as a novel plugin-and-play module, SRS can also enhance the
performance of existing patch-based models. The resources are available at
https://github.com/decisionintelligence/SRSNet.

</details>


### [500] [On the Identifiability of Tensor Ranks via Prior Predictive Matching](https://arxiv.org/abs/2510.14523)
*Eliezer da Silva,Arto Klami,Diego Mesquita,Iñigo Urteaga*

Main category: cs.LG

TL;DR: 本研究提出了一种基于先验预测矩匹配的严格方法来确定概率张量模型中的秩可辨识性。


<details>
  <summary>Details</summary>
Motivation: 张量分解中秩的选择是一个核心挑战，通常依赖于启发式方法。本研究旨在提供一种更严谨的方法来解决这个问题。

Method: 将矩匹配条件转化为关于边际矩、先验超参数和秩的对数线性方程组，并建立秩可辨识性与该方程组可解性之间的等价关系。将此框架应用于四种基础张量模型。

Result: PARAFAC/CP、Tensor Train 和 Tensor Ring 模型具有可解的系统，秩可辨识；Tucker 模型由于其对称拓扑导致系统欠定，秩不可辨识。为可辨识模型推导了仅基于观测数据矩的显式闭式秩估计量。

Conclusion: 本研究提出的基于先验预测矩匹配的方法可以严格地确定张量模型中的秩可辨识性，并为可辨识模型提供了显式秩估计量。该方法证明了 PARAFAC/CP、Tensor Train 和 Tensor Ring 模型的秩是可辨识的，而 Tucker 模型的秩是不可辨识的。

Abstract: Selecting the latent dimensions (ranks) in tensor factorization is a central
challenge that often relies on heuristic methods. This paper introduces a
rigorous approach to determine rank identifiability in probabilistic tensor
models, based on prior predictive moment matching. We transform a set of moment
matching conditions into a log-linear system of equations in terms of marginal
moments, prior hyperparameters, and ranks; establishing an equivalence between
rank identifiability and the solvability of such system. We apply this
framework to four foundational tensor-models, demonstrating that the linear
structure of the PARAFAC/CP model, the chain structure of the Tensor Train
model, and the closed-loop structure of the Tensor Ring model yield solvable
systems, making their ranks identifiable. In contrast, we prove that the
symmetric topology of the Tucker model leads to an underdetermined system,
rendering the ranks unidentifiable by this method. For the identifiable models,
we derive explicit closed-form rank estimators based on the moments of observed
data only. We empirically validate these estimators and evaluate the robustness
of the proposal.

</details>


### [501] [Agentic Entropy-Balanced Policy Optimization](https://arxiv.org/abs/2510.14545)
*Guanting Dong,Licheng Bao,Zhongyuan Wang,Kangzhi Zhao,Xiaoxi Li,Jiajie Jin,Jinghan Yang,Hangyu Mao,Fuzheng Zhang,Kun Gai,Guorui Zhou,Yutao Zhu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.LG

TL;DR: AEPO是一种新的Agentic RL算法，通过平衡rollout和policy更新阶段的熵来解决现有算法的训练崩溃问题，并在多项具有挑战性的基准测试中取得了优于主流RL算法的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的Agentic RL算法过度依赖熵信号进行探索，可能导致训练崩溃。本研究旨在解决这一挑战。

Method: 提出了一种名为AEPO（Agentic Entropy-Balanced Policy Optimization）的新算法，它包含两个核心组件：1）动态熵平衡rollout机制，通过熵预监测自适应地分配采样预算，并对连续的高熵工具调用步骤施加惩罚以防止过度分支；2）熵平衡策略优化，通过停止梯度操作来保留和重新缩放高熵token的梯度，并引入熵感知优势估计。

Result: 在14个具有挑战性的数据集上，AEPO的性能始终优于7种主流RL算法。仅使用1K RL样本，Qwen3-14B结合AEPO在GAIA、Humanity's Last Exam和WebWalker的Pass@1指标上分别达到了47.6%、11.2%和43.0%，在Pass@5指标上分别达到了65.0%、26.0%和70.0%。此外，AEPO提高了rollout采样的多样性，同时保持了策略熵的稳定性，有助于可扩展的Web Agent训练。

Conclusion: AEPO通过在rollout和策略更新阶段平衡熵，成功解决了Agentic RL中的训练崩溃问题，并在多个基准测试中展现出优越的性能和可扩展性。

Abstract: Recently, Agentic Reinforcement Learning (Agentic RL) has made significant
progress in incentivizing the multi-turn, long-horizon tool-use capabilities of
web agents. While mainstream agentic RL algorithms autonomously explore
high-uncertainty tool-call steps under the guidance of entropy, excessive
reliance on entropy signals can impose further constraints, leading to the
training collapse. In this paper, we delve into the challenges caused by
entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an
agentic RL algorithm designed to balance entropy in both the rollout and policy
update phases. AEPO comprises two core components: (1) a dynamic
entropy-balanced rollout mechanism that adaptively allocate global and branch
sampling budget through entropy pre-monitoring, while imposing a branch penalty
on consecutive high-entropy tool-call steps to prevent over-branching issues;
and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient
operation into the high-entropy clipping term to preserve and properly rescale
gradients on high-entropy tokens, while incorporating entropy-aware advantage
estimation to prioritize learning on high-uncertainty tokens. Results across 14
challenging datasets show that AEPO consistently outperforms 7 mainstream RL
algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive
results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker
for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on
WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout
sampling diversity while maintaining stable policy entropy, facilitating
scalable web agent training.

</details>


### [502] [Redundancy-Aware Test-Time Graph Out-of-Distribution Detection](https://arxiv.org/abs/2510.14562)
*Yue Hou,He Zhu,Ruomei Liu,Yingke Su,Junran Wu,Ke Xu*

Main category: cs.LG

TL;DR: 分布外（OOD）样本是图分类中的一个关键问题，会导致模型做出不准确的预测。本研究提出了一种名为 RedOUT 的无监督框架，该框架将结构熵纳入测试时 OOD 检测。


<details>
  <summary>Details</summary>
Motivation: 现有的图 OOD 检测方法在处理由结构冗余引起的语义偏移方面存在不足，导致性能受损。

Method: RedOUT 框架集成了结构熵，并引入了 ReGIB（Redundancy-aware Graph Information Bottleneck）来将目标分解为基本信息和不相关冗余。通过最小化结构熵来减少冗余，并提出理论界限进行优化。

Result: 在真实数据集上的广泛实验表明，RedOUT 在 OOD 检测方面表现优越，平均改进 6.7%，在 ClinTox/LIPO 数据集对上比最佳竞争对手高出 17.3%。

Conclusion: RedOUT 框架通过集成结构熵和 ReGIB 模块，有效解决了图 OOD 检测中的结构冗余问题，并在各种数据集上取得了显著的性能提升。

Abstract: Distributional discrepancy between training and test data can lead models to
make inaccurate predictions when encountering out-of-distribution (OOD) samples
in real-world applications. Although existing graph OOD detection methods
leverage data-centric techniques to extract effective representations, their
performance remains compromised by structural redundancy that induces semantic
shifts. To address this dilemma, we propose RedOUT, an unsupervised framework
that integrates structural entropy into test-time OOD detection for graph
classification. Concretely, we introduce the Redundancy-aware Graph Information
Bottleneck (ReGIB) and decompose the objective into essential information and
irrelevant redundancy. By minimizing structural entropy, the decoupled
redundancy is reduced, and theoretically grounded upper and lower bounds are
proposed for optimization. Extensive experiments on real-world datasets
demonstrate the superior performance of RedOUT on OOD detection. Specifically,
our method achieves an average improvement of 6.7%, significantly surpassing
the best competitor by 17.3% on the ClinTox/LIPO dataset pair.

</details>


### [503] [State-Space Models for Tabular Prior-Data Fitted Networks](https://arxiv.org/abs/2510.14573)
*Felix Koch,Marcel Wever,Fabian Raisch,Benjamin Tischler*

Main category: cs.LG

TL;DR: 基于Hydra（一种双向线性时间结构化状态空间模型）的TabPFN变体在预测性能上可与原始TabPFN媲美，同时提高了效率并降低了对输入顺序的敏感性。


<details>
  <summary>Details</summary>
Motivation: Transformers在处理表格数据时存在二次方复杂度问题，因此需要探索更高效的序列模型。

Method: 研究使用Hydra（一种双向线性时间结构化状态空间模型）替代Transformer作为TabPFN模型。重点解决了SSM对输入顺序敏感的问题，通过双向方法来保留效率并实现对称上下文聚合。

Result: 实验表明，所提出的方法降低了模型对输入顺序的依赖性，预测性能与原始TabPFN模型相当。

Conclusion: Hydra作为TabPFN的替代模型在效率和对输入顺序的鲁棒性方面表现出潜力，性能与原始模型相当。

Abstract: Recent advancements in foundation models for tabular data, such as TabPFN,
demonstrated that pretrained Transformer architectures can approximate Bayesian
inference with high predictive performance. However, Transformers suffer from
quadratic complexity with respect to sequence length, motivating the
exploration of more efficient sequence models. In this work, we investigate the
potential of using Hydra, a bidirectional linear-time structured state space
model (SSM), as an alternative to Transformers in TabPFN. A key challenge lies
in SSM's inherent sensitivity to the order of input tokens - an undesirable
property for tabular datasets where the row order is semantically meaningless.
We investigate to what extent a bidirectional approach can preserve efficiency
and enable symmetric context aggregation. Our experiments show that this
approach reduces the order-dependence, achieving predictive performance
competitive to the original TabPFN model.

</details>


### [504] [Selective Labeling with False Discovery Rate Control](https://arxiv.org/abs/2510.14581)
*Huipeng Huang,Wenbo Liao,Huajun Xi,Hao Zeng,Mengchen Zhao,Hongxin Wei*

Main category: cs.LG

TL;DR: 本文提出了一种名为“共识标签”的新方法，通过控制错误发现率（FDR）来确保 AI 预测标签的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在选择性标注中缺乏理论保证，可能导致 AI 标注子集中存在不可接受的高错误率。

Method: 通过比较 AI 模型预测的置信度与 AI 模型错误标注的校准实例的置信度来构建共识 p 值，并选择 p 值低于阈值的实例。

Result: 实验证明，该方法在图像、文本标注和 LLM QA 等任务中，在实现严格 FDR 控制的同时，具有高功效。

Conclusion: 共识标签方法能够有效控制 FDR，确保 AI 标注标签的平均准确率达到预定水平。

Abstract: Obtaining high-quality labels for large datasets is expensive, requiring
massive annotations from human experts. While AI models offer a cost-effective
alternative by predicting labels, their label quality is compromised by the
unavoidable labeling errors. Existing methods mitigate this issue through
selective labeling, where AI labels a subset and human labels the remainder.
However, these methods lack theoretical guarantees on the quality of
AI-assigned labels, often resulting in unacceptably high labeling error within
the AI-labeled subset. To address this, we introduce \textbf{Conformal
Labeling}, a novel method to identify instances where AI predictions can be
provably trusted. This is achieved by controlling the false discovery rate
(FDR), the proportion of incorrect labels within the selected subset. In
particular, we construct a conformal $p$-value for each test instance by
comparing AI models' predicted confidence to those of calibration instances
mislabeled by AI models. Then, we select test instances whose $p$-values are
below a data-dependent threshold, certifying AI models' predictions as
trustworthy. We provide theoretical guarantees that Conformal Labeling controls
the FDR below the nominal level, ensuring that a predefined fraction of
AI-assigned labels is correct on average. Extensive experiments demonstrate
that our method achieves tight FDR control with high power across various
tasks, including image and text labeling, and LLM QA.

</details>


### [505] [Matcha: Multi-Stage Riemannian Flow Matching for Accurate and Physically Valid Molecular Docking](https://arxiv.org/abs/2510.14586)
*Daria Frolova,Talgat Daulbaev,Egor Sevryugov,Sergei A. Nikolenko,Dmitry N. Ivankov,Ivan Oseledets,Marina A. Pak*

Main category: cs.LG

TL;DR: Matcha是一个结合了多阶段流匹配、学习评分和物理有效性过滤的新型分子对接流程，旨在提高蛋白质-配体结合姿态预测的速度、准确性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 准确预测蛋白质-配体结合姿态对于基于结构的药物设计至关重要，但现有方法在速度、准确性和物理合理性之间难以平衡。

Method: Matcha采用多阶段流匹配，并结合了学习评分和物理有效性过滤。该方法包含三个连续的阶段，每个阶段都使用在相应几何空间（$\\mathbb{R}^3$、$\\mathrm{SO}(3)$和$\\mathrm{SO}(2)$）上操作的流匹配模型来优化对接预测。

Result: Matcha在Astex和PDBbind测试集上，在对接成功率和物理合理性方面均优于现有方法，并且速度比现代大规模协同折叠模型快约25倍。

Conclusion: Matcha通过结合多阶段流匹配、学习评分和物理有效性过滤，在提高蛋白质-配体结合姿态预测的准确性、物理合理性和速度方面取得了显著进展。

Abstract: Accurate prediction of protein-ligand binding poses is crucial for
structure-based drug design, yet existing methods struggle to balance speed,
accuracy, and physical plausibility. We introduce Matcha, a novel molecular
docking pipeline that combines multi-stage flow matching with learned scoring
and physical validity filtering. Our approach consists of three sequential
stages applied consecutively to refine docking predictions, each implemented as
a flow matching model operating on appropriate geometric spaces
($\mathbb{R}^3$, $\mathrm{SO}(3)$, and $\mathrm{SO}(2)$). We enhance the
prediction quality through a dedicated scoring model and apply unsupervised
physical validity filters to eliminate unrealistic poses. Compared to various
approaches, Matcha demonstrates superior performance on Astex and PDBbind test
sets in terms of docking success rate and physical plausibility. Moreover, our
method works approximately 25 times faster than modern large-scale co-folding
models. The model weights and inference code to reproduce our results are
available at https://github.com/LigandPro/Matcha.

</details>


### [506] [Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval](https://arxiv.org/abs/2510.14592)
*Rashmi R,Vidyadhar Upadhya*

Main category: cs.LG

TL;DR: MAHA系统通过结合向量检索和图遍历，实现了对包含文本、图像、表格等多种模态的非结构化文档的多模态问答，并在多个基准数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成（RAG）系统主要基于单模态文本数据，在处理包含文本、图像、表格、公式和图形等多种信息的非结构化多模态文档时效果受限。

Method: 提出了一种名为MAHA（Modality-Aware Hybrid retrieval Architecture）的混合检索架构，该架构通过模态感知的知识图谱来整合密集向量检索和结构化图遍历，实现跨多种模态的检索。

Result: 在多个基准数据集上的评估显示，MAHA取得了0.486的ROUGE-L分数，显著优于基线方法，并实现了完整的数据模态覆盖。

Conclusion: MAHA系统能够有效地结合词嵌入和明确的文档结构，实现有效的多模态检索，为在非结构化多模态数据上进行模态感知推理的RAG系统提供了一个可扩展且可解释的检索框架。

Abstract: Current Retrieval-Augmented Generation (RAG) systems primarily operate on
unimodal textual data, limiting their effectiveness on unstructured multimodal
documents. Such documents often combine text, images, tables, equations, and
graphs, each contributing unique information. In this work, we present a
Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for
multimodal question answering with reasoning through a modality-aware knowledge
graph. MAHA integrates dense vector retrieval with structured graph traversal,
where the knowledge graph encodes cross-modal semantics and relationships. This
design enables both semantically rich and context-aware retrieval across
diverse modalities. Evaluations on multiple benchmark datasets demonstrate that
MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of
0.486, providing complete modality coverage. These results highlight MAHA's
ability to combine embeddings with explicit document structure, enabling
effective multimodal retrieval. Our work establishes a scalable and
interpretable retrieval framework that advances RAG systems by enabling
modality-aware reasoning over unstructured multimodal data.

</details>


### [507] [First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training](https://arxiv.org/abs/2510.14614)
*Gyudong Kim,Hyukju Na,Jin Hyeon Kim,Hyunsung Jang,Jaemin Park,Jaegi Hwang,Namkoo Ha,Seungryong Kim,Young Geun Kim*

Main category: cs.LG

TL;DR: 通过将第一个MHA的输出重定向到后续层的MLP输入来提高Transformer的训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer设计在Tensor Parallelism（TP）下存在显著的通信开销，因为每个块的MHA-MLP连接都需要进行all-reduce通信。

Method: 提出了一种名为FAL（First Attentions Last）的高效Transformer架构，该架构将第一个MHA的输出重定向到后续层的MLP输入，从而消除了每块的MHA-MLP连接，移除了all-reduce通信，并允许在单个GPU上并行执行MHA和MLP。还提出了FAL+，通过将归一化的第一个注意力输出添加到后续层的MHA输出来增强MLP输入，以提高模型质量。

Result: FAL将多GPU训练时间减少了高达44%，提高了单GPU吞吐量高达1.18倍，并且与基线GPT相比，实现了更好的困惑度。FAL+在不增加训练时间的情况下实现了更低的困惑度。

Conclusion: FAL架构通过消除通信开销和实现并行计算，显著提高了Transformer的训练效率和性能。FAL+在保持效率的同时进一步提升了模型质量。

Abstract: As training billion-scale transformers becomes increasingly common, employing
multiple distributed GPUs along with parallel training methods has become a
standard practice. However, existing transformer designs suffer from
significant communication overhead, especially in Tensor Parallelism (TP),
where each block's MHA-MLP connection requires an all-reduce communication.
Through our investigation, we show that the MHA-MLP connections can be bypassed
for efficiency, while the attention output of the first layer can serve as an
alternative signal for the bypassed connection. Motivated by the observations,
we propose FAL (First Attentions Last), an efficient transformer architecture
that redirects the first MHA output to the MLP inputs of the following layers,
eliminating the per-block MHA-MLP connections. This removes the all-reduce
communication and enables parallel execution of MHA and MLP on a single GPU. We
also introduce FAL+, which adds the normalized first attention output to the
MHA outputs of the following layers to augment the MLP input for the model
quality. Our evaluation shows that FAL reduces multi-GPU training time by up to
44%, improves single-GPU throughput by up to 1.18x, and achieves better
perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity
without increasing the training time than the baseline.

</details>


### [508] [LeapFactual: Reliable Visual Counterfactual Explanation Using Conditional Flow Matching](https://arxiv.org/abs/2510.14623)
*Zhuo Cao,Xuan Zhao,Lena Krieger,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: Counterfactual explanations are important for interpretable ML/AI in high-stakes domains, but current methods have limitations. LeapFactual, a new algorithm based on conditional flow matching, generates reliable and informative counterfactuals, even with diverging decision boundaries and in model-agnostic or human-in-the-loop settings. Experiments show LeapFactual produces accurate, in-distribution counterfactuals that can improve model training and enhance interpretability.


<details>
  <summary>Details</summary>
Motivation: High-stakes domains like healthcare and scientific research require accurate and interpretable ML/AI models. Counterfactual explanations offer interpretability by showing minimal input changes for prediction alteration, but existing methods have limitations like gradient vanishing and reliance on accurate decision boundaries.

Method: LeapFactual is a novel counterfactual explanation algorithm based on conditional flow matching. It is model-agnostic and can handle human-in-the-loop systems.

Result: LeapFactual generates reliable and informative counterfactual explanations, even when learned and true decision boundaries diverge. Experiments on benchmark and real-world datasets show accurate and in-distribution counterfactuals that provide actionable insights. These counterfactuals can be used as training data to improve models.

Conclusion: LeapFactual overcomes limitations of existing counterfactual explanation methods by using conditional flow matching. Its model-agnostic and human-in-the-loop compatibility broadens the applicability of counterfactual explanations, enhancing scientific discovery and non-expert interpretability.

Abstract: The growing integration of machine learning (ML) and artificial intelligence
(AI) models into high-stakes domains such as healthcare and scientific research
calls for models that are not only accurate but also interpretable. Among the
existing explainable methods, counterfactual explanations offer
interpretability by identifying minimal changes to inputs that would alter a
model's prediction, thus providing deeper insights. However, current
counterfactual generation methods suffer from critical limitations, including
gradient vanishing, discontinuous latent spaces, and an overreliance on the
alignment between learned and true decision boundaries. To overcome these
limitations, we propose LeapFactual, a novel counterfactual explanation
algorithm based on conditional flow matching. LeapFactual generates reliable
and informative counterfactuals, even when true and learned decision boundaries
diverge. Following a model-agnostic approach, LeapFactual is not limited to
models with differentiable loss functions. It can even handle human-in-the-loop
systems, expanding the scope of counterfactual explanations to domains that
require the participation of human annotators, such as citizen science. We
provide extensive experiments on benchmark and real-world datasets showing that
LeapFactual generates accurate and in-distribution counterfactual explanations
that offer actionable insights. We observe, for instance, that our reliable
counterfactual samples with labels aligning to ground truth can be beneficially
used as new training data to enhance the model. The proposed method is broadly
applicable and enhances both scientific knowledge discovery and non-expert
interpretability.

</details>


### [509] [Galaxy Morphology Classification with Counterfactual Explanation](https://arxiv.org/abs/2510.14655)
*Zhuo Cao,Lena Krieger,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: 提出了一种基于可逆流的编码器-解码器模型，用于星系形态学分析，该模型在提供良好预测性能的同时，还能提供关于决策过程的反事实解释。


<details>
  <summary>Details</summary>
Motivation: 星系形态学对于研究星系演化至关重要，但手动分类大量数据既耗时又困难，因此需要机器学习方法。然而，现有的机器学习方法通常缺乏透明度，难以理解和解释其结果。

Method: 对经典的编码器-解码器架构进行了扩展，并结合了可逆流，以实现星系形态学的自动分类。

Result: 所提出的模型不仅在预测性能上表现良好，还能够提供关于其决策过程的反事实解释，增强了模型的可理解性。

Conclusion: 通过结合可逆流，所提出的模型在星系形态学分析任务中实现了预测性能和模型可解释性的结合。

Abstract: Galaxy morphologies play an essential role in the study of the evolution of
galaxies. The determination of morphologies is laborious for a large amount of
data giving rise to machine learning-based approaches. Unfortunately, most of
these approaches offer no insight into how the model works and make the results
difficult to understand and explain. We here propose to extend a classical
encoder-decoder architecture with invertible flow, allowing us to not only
obtain a good predictive performance but also provide additional information
about the decision process with counterfactual explanations.

</details>


### [510] [Geometric Moment Alignment for Domain Adaptation via Siegel Embeddings](https://arxiv.org/abs/2510.14666)
*Shayan Gharib,Marcelo Hartmann,Arto Klami*

Main category: cs.LG

TL;DR: 通过利用黎曼几何和Siegel嵌入，提出了一种新的无监督域自适应方法，通过匹配第一和第二阶矩来对齐源域和目标域的分布，并在图像去噪和分类任务上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督域自适应方法通常使用临时的相似性度量来对齐源域和目标域的低阶统计矩，而忽略了分布的内在几何结构。

Method: 提出了一种基于黎曼几何的方法，利用Siegel嵌入将一阶和二阶矩表示为对称正定（SPD）矩阵，并使用SPD矩阵流形上的黎曼距离来进行对齐，从而同时适应均值和协方差结构。

Result: 该方法在图像去噪和图像分类基准测试中取得了良好的效果，并且该方法与目标域误差界相关联。

Conclusion: 所提出的基于黎曼几何的矩匹配方法能够更有效地进行域自适应，因为它能够保留源域和目标域的均值和协方差结构，从而提供更准确的跨域比较度量。

Abstract: We address the problem of distribution shift in unsupervised domain
adaptation with a moment-matching approach. Existing methods typically align
low-order statistical moments of the source and target distributions in an
embedding space using ad-hoc similarity measures. We propose a principled
alternative that instead leverages the intrinsic geometry of these
distributions by adopting a Riemannian distance for this alignment. Our key
novelty lies in expressing the first- and second-order moments as a single
symmetric positive definite (SPD) matrix through Siegel embeddings. This
enables simultaneous adaptation of both moments using the natural geometric
distance on the shared manifold of SPD matrices, preserving the mean and
covariance structure of the source and target distributions and yielding a more
faithful metric for cross-domain comparison. We connect the Riemannian manifold
distance to the target-domain error bound, and validate the method on image
denoising and image classification benchmarks. Our code is publicly available
at https://github.com/shayangharib/GeoAdapt.

</details>


### [511] [FedPPA: Progressive Parameter Alignment for Personalized Federated Learning](https://arxiv.org/abs/2510.14698)
*Maulidi Adi Prasetia,Muhamad Risqi U. Saputra,Guntur Dharma Putra*

Main category: cs.LG

TL;DR: FedPPA是一种新颖的个性化联邦学习方法，通过渐进式参数对齐来解决模型和数据异构性问题，在非IID场景下提高了模型的个性化鲁棒性，并在图像分类任务上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化联邦学习方法未能充分解决因客户端计算能力差异带来的模型和数据异构性问题。

Method: 提出渐进式参数对齐（FedPPA）方法，渐进式地将客户端的通用层权重与全局模型权重对齐，并结合基于熵的加权平均方法。

Result: 在MNIST、FMNIST和CIFAR-10三个图像分类数据集上的实验表明，FedPPA的性能优于现有的联邦学习算法，在个性化适应方面表现更佳。

Conclusion: FedPPA能够有效缓解全局模型和本地模型在客户端更新过程中的不一致性，同时保留客户端的本地知识，从而增强在非IID场景下的个性化鲁棒性，并提升全局模型的性能。

Abstract: Federated Learning (FL) is designed as a decentralized, privacy-preserving
machine learning paradigm that enables multiple clients to collaboratively
train a model without sharing their data. In real-world scenarios, however,
clients often have heterogeneous computational resources and hold
non-independent and identically distributed data (non-IID), which poses
significant challenges during training. Personalized Federated Learning (PFL)
has emerged to address these issues by customizing models for each client based
on their unique data distribution. Despite its potential, existing PFL
approaches typically overlook the coexistence of model and data heterogeneity
arising from clients with diverse computational capabilities. To overcome this
limitation, we propose a novel method, called Progressive Parameter Alignment
(FedPPA), which progressively aligns the weights of common layers across
clients with the global model's weights. Our approach not only mitigates
inconsistencies between global and local models during client updates, but also
preserves client's local knowledge, thereby enhancing personalization
robustness in non-IID settings. To further enhance the global model performance
while retaining strong personalization, we also integrate entropy-based
weighted averaging into the FedPPA framework. Experiments on three image
classification datasets, including MNIST, FMNIST, and CIFAR-10, demonstrate
that FedPPA consistently outperforms existing FL algorithms, achieving superior
performance in personalized adaptation.

</details>


### [512] [Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling](https://arxiv.org/abs/2510.14717)
*Alexandru Meterez,Depen Morwani,Jingfeng Wu,Costin-Andrei Oncescu,Cengiz Pehlevan,Sham Kakade*

Main category: cs.LG

TL;DR: Seesaw是一种新的batch-size调度策略，通过增加batch size并调整学习率来加速大模型预训练，同时保持损失动态，并在实验中显著减少了训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统的batch-ramp策略在自适应优化器（如Adam）上的最优策略不明确，通常依赖启发式调整。本研究旨在为batch-size调度提供一个原则性的框架。

Method: Seesaw策略在标准调度器将学习率减半时，改为将学习率乘以$1/\sqrt{2}$并加倍batch size，以保持损失动态。理论上，研究证明了在SGD的噪声线性回归问题上，学习率衰减和batch-size增加之间的等价性，并将其扩展到归一化SGD。

Result: 在150M/300M/600M参数模型上的实验表明，Seesaw策略与余弦衰减在相同FLOPs下效果相当，但将训练的实际时间减少了约36%，接近理论极限。

Conclusion: Seesaw是一种有效的batch-size调度策略，可以显著加速大模型预训练，同时保持训练动态和理论上的等价性。

Abstract: Increasing the batch size during training -- a ''batch ramp'' -- is a
promising strategy to accelerate large language model pretraining. While for
SGD, doubling the batch size can be equivalent to halving the learning rate,
the optimal strategy for adaptive optimizers like Adam is less clear. As a
result, any batch-ramp scheduling, if used at all, is typically tuned
heuristically. This work develops a principled framework for batch-size
scheduling and introduces Seesaw: whenever a standard scheduler would halve the
learning rate, Seesaw instead multiplies it by $1/\sqrt{2}$ and doubles the
batch size, preserving loss dynamics while reducing serial steps.
Theoretically, we provide, to our knowledge, the first finite-sample proof of
equivalence between learning-rate decay and batch-size ramp-up for SGD on noisy
linear regression, and we extend this equivalence to normalized SGD, a
tractable proxy for Adam, under a variance-dominated regime observed in
practice. Empirically, on 150M/300M/600M-parameter models trained at Chinchilla
scale using a constant (critical) batch size, Seesaw matches cosine decay at
equal FLOPs while reducing wall-clock time by $\approx 36\%$, approaching the
theoretical limit implied by our analysis.

</details>


### [513] [The Pursuit of Diversity: Multi-Objective Testing of Deep Reinforcement Learning Agents](https://arxiv.org/abs/2510.14727)
*Antony Bartlett,Cynthia Liem,Annibale Panichella*

Main category: cs.LG

TL;DR: INDAGO-Nexus是一种多目标搜索方法，通过优化失败可能性和测试场景多样性，能发现比现有工具多83%的独特失败，并缩短高达67%的失败时间。


<details>
  <summary>Details</summary>
Motivation: 现有测试DRL的方法（如INDAGO）只关注最大化失败次数，但不能保证发现的场景具有多样性或揭示不同的错误类型。

Method: INDAGO-Nexus采用多目标进化算法，结合多种多样性指标和帕累托前沿选择策略，同时优化失败可能性和测试场景多样性。

Result: 在人形步行者、自动驾驶汽车和泊车代理这三个DRL智能体上，INDAGO-Nexus平均能发现比INDAGO多83%（SDC）和40%（泊车）的独特失败，并缩短高达67%的失败时间。

Conclusion: INDAGO-Nexus通过多目标优化，在提高测试有效性和效率方面优于现有方法，能够发现更多样化和独特的失败场景。

Abstract: Testing deep reinforcement learning (DRL) agents in safety-critical domains
requires discovering diverse failure scenarios. Existing tools such as INDAGO
rely on single-objective optimization focused solely on maximizing failure
counts, but this does not ensure discovered scenarios are diverse or reveal
distinct error types. We introduce INDAGO-Nexus, a multi-objective search
approach that jointly optimizes for failure likelihood and test scenario
diversity using multi-objective evolutionary algorithms with multiple diversity
metrics and Pareto front selection strategies. We evaluated INDAGO-Nexus on
three DRL agents: humanoid walker, self-driving car, and parking agent. On
average, INDAGO-Nexus discovers up to 83% and 40% more unique failures (test
effectiveness) than INDAGO in the SDC and Parking scenarios, respectively,
while reducing time-to-failure by up to 67% across all agents.

</details>


### [514] [Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries](https://arxiv.org/abs/2510.14751)
*Divyat Mahajan,Sachin Goyal,Badr Youbi Idrissi,Mohammad Pezeshki,Ioannis Mitliagkas,David Lopez-Paz,Kartik Ahuja*

Main category: cs.LG

TL;DR: FSP通过预测长期未来序列的紧凑表示来改进LLM，优于NTP和MTP。


<details>
  <summary>Details</summary>
Motivation: 教师强制训练在LLM的长期推理、规划和创意写作方面存在局限性，MTP仅提供有限的改进。

Method: 提出未来摘要预测（FSP），训练一个辅助头来预测长期未来的紧凑表示，探索了两种变体：手工制作的摘要和学习的摘要（使用反向语言模型的嵌入）。

Result: 在3B和8B参数模型的预训练实验中，FSP在数学、推理和编码基准测试中优于NTP和MTP。

Conclusion: FSP通过预测长期未来摘要，在LLM的长期推理和生成能力方面提供了显著的改进。

Abstract: Next-token prediction (NTP) has driven the success of large language models
(LLMs), but it struggles with long-horizon reasoning, planning, and creative
writing, with these limitations largely attributed to teacher-forced training.
Multi-token prediction (MTP) partially mitigates these issues by predicting
several future tokens at once, but it mostly captures short-range dependencies
and offers limited improvement. We propose future summary prediction (FSP),
which trains an auxiliary head to predict a compact representation of the
long-term future, preserving information relevant for long-form generations. We
explore two variants of FSP: handcrafted summaries, for example, a bag of words
summary of the future of the sequence, and learned summaries, which use
embeddings produced by a reverse language model trained from right to left.
Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate
that FSP provides improvements over both NTP and MTP across math, reasoning,
and coding benchmarks.

</details>


### [515] [Causal Discovery for Linear DAGs with Dependent Latent Variables via Higher-order Cumulants](https://arxiv.org/abs/2510.14780)
*Ming Cai,Penggang Gao,Hisayuki Hara*

Main category: cs.LG

TL;DR: 提出了一种新的算法，用于在存在潜在混淆因素的线性非高斯无环图中估计因果有向无环图。


<details>
  <summary>Details</summary>
Motivation: 现有的LvLiNGAM方法在处理潜在混淆因素的因果关系以及观测变量之间的因果关系时存在局限性。

Method: 利用观测数据的高阶累积量来识别因果结构。

Result: 通过广泛的模拟和真实世界数据实验，验证了所提出算法的有效性和实用性。

Conclusion: 所提出的算法能够成功识别LvLiNGAM中的因果有向无环图，即使在潜在变量之间存在因果关系的情况下也能适用。

Abstract: This paper addresses the problem of estimating causal directed acyclic graphs
in linear non-Gaussian acyclic models with latent confounders (LvLiNGAM).
Existing methods assume mutually independent latent confounders or cannot
properly handle models with causal relationships among observed variables.
  We propose a novel algorithm that identifies causal DAGs in LvLiNGAM,
allowing causal structures among latent variables, among observed variables,
and between the two. The proposed method leverages higher-order cumulants of
observed data to identify the causal structure. Extensive simulations and
experiments with real-world data demonstrate the validity and practical utility
of the proposed algorithm.

</details>


### [516] [Active Jammer Localization via Acquisition-Aware Path Planning](https://arxiv.org/abs/2510.14790)
*Luis González-Gudiño,Mariona Jaramillo-Civill,Pau Closas,Tales Imbiriba*

Main category: cs.LG

TL;DR: 本研究提出了一种结合贝叶斯优化和考虑信息获取的路径规划的主动干扰源定位框架，通过改进的A*算法（A-UCB*）规划最优路径以收集信号强度测量数据，并在模拟的城市环境中验证了其有效性，相比基线方法，该方法用更少的测量数据实现了更准确的定位。


<details>
  <summary>Details</summary>
Motivation: 现有被动众包方法在干扰源定位中存在局限，本研究旨在提出一种能够自适应地引导移动代理收集高价值测量数据的主动定位框架，并考虑城市环境中的障碍物和移动性约束。

Method: 提出了一种结合贝叶斯优化和考虑信息获取的路径规划的主动干扰源定位框架。修改了A*算法，提出A-UCB*算法，将信息获取值纳入轨迹成本，以规划高信息获取的路径。移动代理根据该路径收集接收信号强度测量数据。

Result: 在模拟的城市环境中，与无信息基线方法相比，该方法用更少的测量数据实现了准确的干扰源定位，并在不同环境下表现出一致的性能。

Conclusion: 所提出的主动干扰源定位框架能够有效地结合贝叶斯优化和信息获取的路径规划，通过A-UCB*算法在实际城市环境中实现高精度定位，并且比现有基线方法更有效率。

Abstract: We propose an active jammer localization framework that combines Bayesian
optimization with acquisition-aware path planning. Unlike passive crowdsourced
methods, our approach adaptively guides a mobile agent to collect high-utility
Received Signal Strength measurements while accounting for urban obstacles and
mobility constraints. For this, we modified the A* algorithm, A-UCB*, by
incorporating acquisition values into trajectory costs, leading to
high-acquisition planned paths. Simulations on realistic urban scenarios show
that the proposed method achieves accurate localization with fewer measurements
compared to uninformed baselines, demonstrating consistent performance under
different environments.

</details>


### [517] [Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning](https://arxiv.org/abs/2510.14810)
*Shikuang Deng,Jiayuan Zhang,Yuhang Wu,Ting Chen,Shi Gu*

Main category: cs.LG

TL;DR: SPHeRe是一种新的无监督学习方法，它通过整合正交性和结构信息来改进赫布学习，在图像分类、持续学习和迁移学习方面取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的赫布学习在机器学习中存在更新约束不确定和缺乏反馈机制的问题，限制了其在复杂网络和任务中的应用。因此，需要一种改进的赫布学习方法。

Method: SPHeRe是一种无监督学习方法，通过一个局部的辅助非线性模块来整合正交性和结构信息。结构信息通过一个轻量级的辅助投影反向传播到输入端，实现反馈调节；正交性约束则控制更新幅度。

Result: SPHeRe在CIFAR-10、CIFAR-100和Tiny-ImageNet等图像分类基准测试中，取得了无监督突触可塑性方法的最新（SOTA）性能。此外，该方法在持续学习、迁移学习和图像重建任务中也表现出强大的有效性、鲁棒性和泛化能力。

Conclusion: SPHeRe展示了赫布无监督学习规则在现代深度学习框架中的竞争力和潜力，证明了在不严重依赖反向传播的情况下，实现高效且受生物学启发的学习算法是可行的。

Abstract: Hebbian learning is a biological principle that intuitively describes how
neurons adapt their connections through repeated stimuli. However, when applied
to machine learning, it suffers serious issues due to the unconstrained updates
of the connections and the lack of accounting for feedback mediation. Such
shortcomings limit its effective scaling to complex network architectures and
tasks. To this end, here we introduce the Structural Projection Hebbian
Representation (SPHeRe), a novel unsupervised learning method that integrates
orthogonality and structural information preservation through a local auxiliary
nonlinear block. The loss for structural information preservation
backpropagates to the input through an auxiliary lightweight projection that
conceptually serves as feedback mediation while the orthogonality constraints
account for the boundedness of updating magnitude. Extensive experimental
results show that SPHeRe achieves SOTA performance among unsupervised synaptic
plasticity approaches on standard image classification benchmarks, including
CIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strong
effectiveness in continual learning and transfer learning scenarios, and image
reconstruction tasks show the robustness and generalizability of the extracted
features. This work demonstrates the competitiveness and potential of Hebbian
unsupervised learning rules within modern deep learning frameworks,
demonstrating the possibility of efficient and biologically inspired learning
algorithms without the strong dependence on strict backpropagation. Our code is
available at https://github.com/brain-intelligence-lab/SPHeRe.

</details>


### [518] [Efficient Dynamic Structured Sparse Training with Learned Shuffles](https://arxiv.org/abs/2510.14812)
*Abhishek Tyagi,Arjun Iyer,Liam Young,William H Renninger,Christopher Kanan,Yuhao Zhu*

Main category: cs.LG

TL;DR: 通过学习排列矩阵来增强结构化稀疏训练，使其在保持高效的同时，达到与非结构化稀疏训练相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 非结构化稀疏训练（DST）在准确率上优于结构化稀疏训练，主要是因为结构化稀疏训练在表示能力上有所损失，无法探索所有可能的权重掩码。本研究旨在解决此问题。

Method: 提出了一种排列增强的稀疏训练（PA-DST）方法，为每个层学习一个排列矩阵，并与结构化权重矩阵联合训练。该方法应用于块状、N:M 和对角线结构。

Result: 在ImageNet-1K（ViT-B/16）和WikiText-103（GPT-2）数据集上，PA-DST在90-95%稀疏度下，准确率与非结构化基线（RigL, SET）相当，但训练速度提高了1.21倍，推理速度提高了2.9倍。

Conclusion: 结构化稀疏结合学习到的排列矩阵在准确率和效率之间取得了最佳平衡。

Abstract: Structured sparsity accelerates training and inference on modern GPUs, yet it
still trails unstructured dynamic sparse training (DST) in accuracy. The
shortfall stems from a loss of expressivity: whereas a dense layer can realize
every possible mask obtained by choosing any $w$ active weights out of $n$, a
fixed block or N:M layout explores only a subset of those possibilities. We
propose to close this gap by learning, for each layer, a single permutation
matrix jointly with the structured weight matrix. Applied to three canonical
structures -- block, N:M, and diagonals -- we show that permutation-augmented
DST (PA-DST) matches unstructured baselines (RigL, SET) at 90--95\% sparsity on
ImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2), yet trains up to $1.21\times$
and infers up to $2.9\times$ faster. The results position structure + learned
permutation as a sweet spot between accuracy and efficiency.

</details>


### [519] [Tackling Time-Series Forecasting Generalization via Mitigating Concept Drift](https://arxiv.org/abs/2510.14814)
*Zhiyuan Zhao,Haoxin Liu,B. Aditya Prakash*

Main category: cs.LG

TL;DR: 该研究提出了一个名为ShifTS的框架，用于解决时间序列预测中的概念漂移和时间转移问题。


<details>
  <summary>Details</summary>
Motivation: 由于时间序列数据的动态性质，处理潜在的分布变化对于时间序列预测模型至关重要。现有研究主要关注时间转移问题，而概念漂移问题的研究较少。

Method: 提出了一种软注意力机制来寻找时间序列中的不变模式，并引入了一个名为ShifTS的框架，该框架首先解决时间转移问题，然后解决概念漂移问题。

Result: ShifTS框架在多个数据集上持续提高了预测准确性，并优于现有的概念漂移、时间转移和组合基线。

Conclusion: ShifTS框架能够有效地处理时间序列预测中的概念漂移和时间转移问题。

Abstract: Time-series forecasting finds broad applications in real-world scenarios. Due
to the dynamic nature of time series data, it is important for time-series
forecasting models to handle potential distribution shifts over time. In this
paper, we initially identify two types of distribution shifts in time series:
concept drift and temporal shift. We acknowledge that while existing studies
primarily focus on addressing temporal shift issues in time series forecasting,
designing proper concept drift methods for time series forecasting has received
comparatively less attention.
  Motivated by the need to address potential concept drift, while conventional
concept drift methods via invariant learning face certain challenges in
time-series forecasting, we propose a soft attention mechanism that finds
invariant patterns from both lookback and horizon time series. Additionally, we
emphasize the critical importance of mitigating temporal shifts as a
preliminary to addressing concept drift. In this context, we introduce ShifTS,
a method-agnostic framework designed to tackle temporal shift first and then
concept drift within a unified approach. Extensive experiments demonstrate the
efficacy of ShifTS in consistently enhancing the forecasting accuracy of
agnostic models across multiple datasets, and outperforming existing concept
drift, temporal shift, and combined baselines.

</details>


### [520] [Programmatic Representation Learning with Language Models](https://arxiv.org/abs/2510.14825)
*Gabriel Poesia,Georgia Gabriela Sampaio*

Main category: cs.LG

TL;DR: 通过结合LLM和决策树，提出LeaPR模型，实现可解释的端到端表征学习。


<details>
  <summary>Details</summary>
Motivation: 传统决策树依赖特定输入特征，而神经网络缺乏可解释性。需要一种既能从原始数据学习又能保持可解释性的模型。

Method: 提出LeaPR模型，该模型将代码表示的特征与决策树预测器相结合。利用LLM合成特征函数，并提出两种学习算法：FunSearch的改编和ID3算法的变体。

Result: 在国际象棋、图像和文本分类等实验中，LeaPR模型学习到了高质量、无需神经网络的预测器，其性能可与神经网络媲美。

Conclusion: LeaPR模型提供了一种灵活的端到端可解释表征学习范式，特征和预测均可轻松检查和理解。

Abstract: Classical models for supervised machine learning, such as decision trees, are
efficient and interpretable predictors, but their quality is highly dependent
on the particular choice of input features. Although neural networks can learn
useful representations directly from raw data (e.g., images or text), this
comes at the expense of interpretability and the need for specialized hardware
to run them efficiently. In this paper, we explore a hypothesis class we call
Learned Programmatic Representations (LeaPR) models, which stack arbitrary
features represented as code (functions from data points to scalars) and
decision tree predictors. We synthesize feature functions using Large Language
Models (LLMs), which have rich prior knowledge in a wide range of domains and a
remarkable ability to write code using existing domain-specific libraries. We
propose two algorithms to learn LeaPR models from supervised data. First, we
design an adaptation of FunSearch to learn features rather than directly
generate predictors. Then, we develop a novel variant of the classical ID3
algorithm for decision tree learning, where new features are generated on
demand when splitting leaf nodes. In experiments from chess position evaluation
to image and text classification, our methods learn high-quality, neural
network-free predictors often competitive with neural networks. Our work
suggests a flexible paradigm for learning interpretable representations
end-to-end where features and predictions can be readily inspected and
understood.

</details>


### [521] [To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models](https://arxiv.org/abs/2510.14826)
*Eran Malach,Omid Saremi,Sinead Williamson,Arwen Bradley,Aryo Lotfi,Emmanuel Abbe,Josh Susskind,Etai Littwin*

Main category: cs.LG

TL;DR: SSMs在长上下文和长格式生成方面存在理论局限性，但可以通过与外部工具交互来解决，从而在各种任务中实现出色的长度泛化。


<details>
  <summary>Details</summary>
Motivation: 验证SSMs在长格式生成任务上的理论局限性，并提出通过外部工具交互来解决这些局限性。

Method: 理论分析SSMs在长格式生成任务上的能力，并提出一种工具增强的SSM方法，然后在一系列算术、推理和编码任务上进行实验验证。

Result: 证明了SSMs在理论上无法准确解决真正长格式的生成问题。工具增强的SSMs在算术、推理和编码任务上展现了显著的长度泛化能力。

Conclusion: 工具增强的SSMs可以作为Transformers在交互式工具使用和agentic设置中的一种高效替代方案。

Abstract: State Space Models (SSMs) have become the leading alternative to Transformers
for sequence modeling. Their primary advantage is efficiency in long-context
and long-form generation, enabled by fixed-size memory and linear scaling of
computational complexity. We begin this work by showing a simple theoretical
result stating that SSMs cannot accurately solve any ``truly long-form''
generation problem (in a sense we formally define), undermining their main
competitive advantage. However, we show that this limitation can be mitigated
by allowing SSMs interactive access to external tools. In fact, we show that
given the right choice of tool access and problem-dependent training data, SSMs
can learn to solve any tractable problem and generalize to arbitrary problem
length/complexity (i.e., achieve length generalization). Following our
theoretical finding, we demonstrate that tool-augmented SSMs achieve remarkable
length generalization on a variety of arithmetic, reasoning, and coding tasks.
These findings highlight SSMs as a potential efficient alternative to
Transformers in interactive tool-based and agentic settings.

</details>


### [522] [Intelligent Dynamic Handover via AI-assisted Signal Quality Prediction in 6G Multi-RAT Networks](https://arxiv.org/abs/2510.14832)
*Maria Lamprini A. Bartsioka,Anastasios Giannopoulos,Sotirios Spantideas*

Main category: cs.LG

TL;DR: 该论文提出了一种基于机器学习的预测性条件切换（P-CHO）框架，用于解决6G多无线接入技术（multi-RAT）网络中需要可靠的移动性决策的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多RAT切换机制依赖于瞬时测量和阈值事件，在快速变化的信道、干扰和异构覆盖环境下不够可靠。本研究旨在提出一种更主动、更可靠的切换机制。

Method: 提出了一种基于模型的、短视角的信号质量预测的机器学习（ML）辅助预测性条件切换（P-CHO）框架。该框架通过RAT切换控制器（RAT Steering Controller）来协调数据收集、并行预测、决策逻辑（包含滞后条件）和切换执行。使用长短期记忆（LSTM）网络来预测移动用户在随机轨迹下的信号质量指标，并在不同的信道模型下进行训练和评估。

Result: P-CHO框架能够减少切换失败和乒乓切换事件，尤其是在启用滞后条件的情况下。与基线预测器相比，P-CHO表现出更好的性能。

Conclusion: 所提出的P-CHO框架能够实现准确、低延迟、前瞻性的切换，适用于6G多RAT部署中的ML辅助切换。

Abstract: The emerging paradigm of 6G multiple Radio Access Technology (multi-RAT)
networks, where cellular and Wireless Fidelity (WiFi) transmitters coexist,
requires mobility decisions that remain reliable under fast channel dynamics,
interference, and heterogeneous coverage. Handover in multi-RAT deployments is
still highly reactive and event-triggered, relying on instantaneous
measurements and threshold events. This work proposes a Machine Learning
(ML)-assisted Predictive Conditional Handover (P-CHO) framework based on a
model-driven and short-horizon signal quality forecasts. We present a
generalized P-CHO sequence workflow orchestrated by a RAT Steering Controller,
which standardizes data collection, parallel per-RAT predictions, decision
logic with hysteresis-based conditions, and CHO execution. Considering a
realistic multi-RAT environment, we train RAT-aware Long Short Term Memory
(LSTM) networks to forecast the signal quality indicators of mobile users along
randomized trajectories. The proposed P-CHO models are trained and evaluated
under different channel models for cellular and IEEE 802.11 WiFi integrated
coverage. We study the impact of hyperparameter tuning of LSTM models under
different system settings, and compare direct multi-step versus recursive P-CHO
variants. Comparisons against baseline predictors are also carried out.
Finally, the proposed P-CHO is tested under soft and hard handover settings,
showing that hysteresis-enabled P-CHO scheme is able to reduce handover
failures and ping-pong events. Overall, the proposed P-CHO framework can enable
accurate, low-latency, and proactive handovers suitable for ML-assisted
handover steering in 6G multi-RAT deployments.

</details>


### [523] [Reinforcement Learning with Stochastic Reward Machines](https://arxiv.org/abs/2510.14837)
*Jan Corazza,Ivan Gavran,Daniel Neider*

Main category: cs.LG

TL;DR: 提出随机奖励机器（SRM）及其学习算法，以解决带有噪声的稀疏奖励的强化学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有奖励机器算法在处理有噪声的奖励时存在局限性。

Method: 提出一种基于约束求解的算法，用于从强化学习智能体的探索数据中学习最小的随机奖励机器。

Result: 所提出的算法能够与现有的基于奖励机器的强化学习算法相结合，并保证在极限情况下收敛到最优策略。通过案例研究证明，该算法优于现有方法和处理噪声奖励函数的朴素方法。

Conclusion: 随机奖励机器及其学习算法能够有效地处理带有噪声的稀疏奖励问题，并在强化学习中实现最优策略。

Abstract: Reward machines are an established tool for dealing with reinforcement
learning problems in which rewards are sparse and depend on complex sequences
of actions. However, existing algorithms for learning reward machines assume an
overly idealized setting where rewards have to be free of noise. To overcome
this practical limitation, we introduce a novel type of reward machines, called
stochastic reward machines, and an algorithm for learning them. Our algorithm,
based on constraint solving, learns minimal stochastic reward machines from the
explorations of a reinforcement learning agent. This algorithm can easily be
paired with existing reinforcement learning algorithms for reward machines and
guarantees to converge to an optimal policy in the limit. We demonstrate the
effectiveness of our algorithm in two case studies and show that it outperforms
both existing methods and a naive approach for handling noisy reward functions.

</details>


### [524] [Backdoor Unlearning by Linear Task Decomposition](https://arxiv.org/abs/2510.14845)
*Amel Abdelraheem,Alessandro Favero,Gerome Bovet,Pascal Frossard*

Main category: cs.LG

TL;DR: Foundation models are vulnerable to adversarial attacks, but retraining is infeasible. This paper proposes an unlearning method that isolates and erases backdoor influences by exploiting their disentanglement from benign tasks in the model weights, achieving high unlearning rates with minimal impact on clean accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for removing backdoors from foundation models are costly (requiring fine-tuning) and can degrade performance on unrelated tasks. The paper aims to find a way to remove backdoors without compromising the model's general capabilities.

Method: The paper studies how backdoors are encoded in the model's weight space and finds they are disentangled from benign tasks. Based on this insight, a simple unlearning method is introduced that leverages this disentanglement to isolate and erase the backdoor's influence.

Result: The proposed unlearning method achieves approximately perfect unlearning, retaining an average of 96% of clean accuracy on CLIP-based models with common adversarial triggers. It also successfully unlearns backdoors even when the attack is unknown, through reverse-engineered triggers. The method shows better tradeoffs between unlearning and clean accuracy compared to state-of-the-art defenses.

Conclusion: Backdoors in foundation models can be effectively removed without compromising their general capabilities by exploiting the disentanglement of backdoor influences from benign tasks within the model's weight space. The proposed unlearning method offers a more efficient and effective solution compared to existing approaches.

Abstract: Foundation models have revolutionized computer vision by enabling broad
generalization across diverse tasks. Yet, they remain highly susceptible to
adversarial perturbations and targeted backdoor attacks. Mitigating such
vulnerabilities remains an open challenge, especially given that the
large-scale nature of the models prohibits retraining to ensure safety.
Existing backdoor removal approaches rely on costly fine-tuning to override the
harmful behavior, and can often degrade performance on other unrelated tasks.
This raises the question of whether backdoors can be removed without
compromising the general capabilities of the models. In this work, we address
this question and study how backdoors are encoded in the model weight space,
finding that they are disentangled from other benign tasks. Specifically, this
separation enables the isolation and erasure of the backdoor's influence on the
model with minimal impact on clean performance. Building on this insight, we
introduce a simple unlearning method that leverages such disentanglement.
Through extensive experiments with CLIP-based models and common adversarial
triggers, we show that, given the knowledge of the attack, our method achieves
approximately perfect unlearning, while retaining, on average, 96% of clean
accuracy. Additionally, we demonstrate that even when the attack and its
presence are unknown, our method successfully unlearns backdoors by proper
estimation using reverse-engineered triggers. Overall, our method consistently
yields better unlearning and clean accuracy tradeoffs when compared to present
state-of-the-art defenses.

</details>


### [525] [Predicting kernel regression learning curves from only raw data statistics](https://arxiv.org/abs/2510.14878)
*Dhruva Karkada,Joseph Turnbull,Yuxi Liu,James B. Simon*

Main category: cs.LG

TL;DR: 该论文提出了一个名为“高斯特征结构假说”（Hermite Eigenstructure Ansatz, HEA）的理论框架，用于预测在真实数据集（包括CIFAR-5m、SVHN和ImageNet）上使用旋转不变核进行核回归时的学习曲线。该框架仅通过经验数据协方差矩阵和目标函数的经验多项式分解即可预测学习曲线。HEA的核心思想是通过解析近似来表征数据分布下的核的特征值和特征函数，这些特征函数类似于数据的埃尔米特多项式。虽然HEA在理论上对高斯数据进行了证明，但研究发现真实图像数据在实践中也表现出“足够高斯”的特性，使得HEA能够很好地预测学习曲线。此外，该研究还将HEA的分析扩展到多层感知机（MLP），并发现MLP在特征学习模式下学习埃尔米特多项式的顺序与HEA的预测一致。HEA框架证明了端到端学习理论的可能性，可以将数据集结构与非平凡学习算法在真实数据集上的模型性能联系起来。


<details>
  <summary>Details</summary>
Motivation: 研究在真实数据集上使用旋转不变核进行核回归时的学习曲线预测问题，并提出一个能够连接数据集结构和模型性能的理论框架。

Method: 提出“高斯特征结构假说”（HEA），通过解析近似表征核的特征值和特征函数，并结合经验数据协方差矩阵和目标函数的经验多项式分解来预测学习曲线。将HEA应用于真实图像数据，并扩展到多层感知机（MLP）的研究。

Result: HEA框架能够很好地预测真实图像数据集上的学习曲线。研究发现真实图像数据在实践中表现出“足够高斯”的特性，使得HEA的预测准确。MLP在特征学习模式下学习埃尔米特多项式的顺序与HEA的预测一致。

Conclusion: HEA框架是一个概念验证，证明了端到端的学习理论是可行的，该理论能够将数据集结构与非平凡学习算法在真实数据集上的模型性能联系起来。

Abstract: We study kernel regression with common rotation-invariant kernels on real
datasets including CIFAR-5m, SVHN, and ImageNet. We give a theoretical
framework that predicts learning curves (test risk vs. sample size) from only
two measurements: the empirical data covariance matrix and an empirical
polynomial decomposition of the target function $f_*$. The key new idea is an
analytical approximation of a kernel's eigenvalues and eigenfunctions with
respect to an anisotropic data distribution. The eigenfunctions resemble
Hermite polynomials of the data, so we call this approximation the Hermite
eigenstructure ansatz (HEA). We prove the HEA for Gaussian data, but we find
that real image data is often "Gaussian enough" for the HEA to hold well in
practice, enabling us to predict learning curves by applying prior results
relating kernel eigenstructure to test risk. Extending beyond kernel
regression, we empirically find that MLPs in the feature-learning regime learn
Hermite polynomials in the order predicted by the HEA. Our HEA framework is a
proof of concept that an end-to-end theory of learning which maps dataset
structure all the way to model performance is possible for nontrivial learning
algorithms on real datasets.

</details>


### [526] [Learning When Not to Learn: Risk-Sensitive Abstention in Bandits with Unbounded Rewards](https://arxiv.org/abs/2510.14884)
*Sarah Liaw,Benjamin Plaut*

Main category: cs.LG

TL;DR: 该研究提出了一种在不可恢复错误场景下学习的算法，适用于高风险人工智能应用。


<details>
  <summary>Details</summary>
Motivation: 现有序列决策理论假设错误可恢复，这在高风险AI应用中可能导致不可挽回的损害。在没有导师帮助的情况下，需要一种能在奖励无界时安全学习的算法。

Method: 提出了一种带有弃权选项的双动作上下文老虎机模型，并开发了一种基于谨慎的算法。该算法仅在可信区域内进行决策，避免在已有证据表明会造成损害时进行承诺。在i.i.d.输入下，证明了该算法的次线性遗憾界限。

Result: 在不可恢复错误和无导师帮助的条件下，实现了次线性遗憾界限，证明了谨慎探索对于安全部署学习代理的有效性。

Conclusion: 所提出的谨慎算法能够在高风险环境中有效且安全地进行学习，即使在奖励无界且错误可能导致不可挽回损害的情况下也是如此。

Abstract: In high-stakes AI applications, even a single action can cause irreparable
damage. However, nearly all of sequential decision-making theory assumes that
all errors are recoverable (e.g., by bounding rewards). Standard bandit
algorithms that explore aggressively may cause irreparable damage when this
assumption fails. Some prior work avoids irreparable errors by asking for help
from a mentor, but a mentor may not always be available. In this work, we
formalize a model of learning with unbounded rewards without a mentor as a
two-action contextual bandit with an abstain option: at each round the agent
observes an input and chooses either to abstain (always 0 reward) or to commit
(execute a preexisting task policy). Committing yields rewards that are
upper-bounded but can be arbitrarily negative, and the commit reward is assumed
Lipschitz in the input. We propose a caution-based algorithm that learns when
not to learn: it chooses a trusted region and commits only where the available
evidence does not already certify harm. Under these conditions and i.i.d.
inputs, we establish sublinear regret guarantees, theoretically demonstrating
the effectiveness of cautious exploration for deploying learning agents safely
in high-stakes environments.

</details>


### [527] [Reasoning with Sampling: Your Base Model is Smarter Than You Think](https://arxiv.org/abs/2510.14901)
*Aayush Karan,Yilun Du*

Main category: cs.LG

TL;DR: 通过纯粹的推理时间采样，在不进行额外训练的情况下，从基础模型中引发可比的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究是否可以通过纯粹的推理时间采样，从基础模型中引发可比的推理能力，而无需任何额外的训练。

Method: 提出一种利用基础模型自身似然性的简单迭代采样算法，该算法受到从锐化分布中采样的马尔可夫链蒙特卡洛（MCMC）技术的启发。

Result: 在各种单次推理任务（包括MATH500、HumanEval和GPQA）上，我们的算法提供了显著的推理能力提升，这些提升几乎可以媲美甚至超越了强化学习（RL）的性能。此外，我们的采样器避免了 RL 训练所特有的多样本多样性崩溃问题。

Conclusion: 该方法不需要训练、经过筛选的数据集或验证器，这表明其在易于验证的领域之外具有广泛的适用性。

Abstract: Frontier reasoning models have exhibited incredible capabilities across a
wide array of disciplines, driven by posttraining large language models (LLMs)
with reinforcement learning (RL). However, despite the widespread success of
this paradigm, much of the literature has been devoted to disentangling truly
novel behaviors that emerge during RL but are not present in the base models.
In our work, we approach this question from a different angle, instead asking
whether comparable reasoning capabilites can be elicited from base models at
inference time by pure sampling, without any additional training. Inspired by
Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened
distributions, we propose a simple iterative sampling algorithm leveraging the
base models' own likelihoods. Over different base models, we show that our
algorithm offers substantial boosts in reasoning that nearly match and even
outperform those from RL on a wide variety of single-shot tasks, including
MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in
diversity over multiple samples that is characteristic of RL-posttraining.
Crucially, our method does not require training, curated datasets, or a
verifier, suggesting broad applicability beyond easily verifiable domains.

</details>


### [528] [Circuit Insights: Towards Interpretability Beyond Activations](https://arxiv.org/abs/2510.14936)
*Elena Golimblevskaia,Aakriti Jain,Bruno Puri,Ammar Ibrahim,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.LG

TL;DR: 该论文提出WeightLens和CircuitLens两种新方法，用于自动化神经网络的可解释性分析，解决了现有方法的局限性，提高了可解释性的稳健性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性AI方法在理解神经网络内部结构方面存在局限，如依赖手动检查、仅限于简单任务、自动化方法依赖外部模型和数据集质量等。需要更系统、可扩展的方法来分析神经网络的计算过程。

Method: 提出两种互补方法：WeightLens直接从学习到的权重解释特征，无需外部模型或数据集；CircuitLens捕捉特征激活如何由组件交互产生，揭示了仅基于激活的方法无法识别的电路动力学。

Result: WeightLens在独立特征分析方面达到或超过了现有方法的性能。CircuitLens能够识别出仅基于激活的方法无法发现的电路层面的动态。

Conclusion: WeightLens和CircuitLens共同提高了模型可解释性的鲁棒性，增强了对神经网络电路进行可扩展的机械分析的能力，同时保持了效率和质量。

Abstract: The fields of explainable AI and mechanistic interpretability aim to uncover
the internal structure of neural networks, with circuit discovery as a central
tool for understanding model computations. Existing approaches, however, rely
on manual inspection and remain limited to toy tasks. Automated
interpretability offers scalability by analyzing isolated features and their
activations, but it often misses interactions between features and depends
strongly on external LLMs and dataset quality. Transcoders have recently made
it possible to separate feature attributions into input-dependent and
input-invariant components, providing a foundation for more systematic circuit
analysis. Building on this, we propose WeightLens and CircuitLens, two
complementary methods that go beyond activation-based analysis. WeightLens
interprets features directly from their learned weights, removing the need for
explainer models or datasets while matching or exceeding the performance of
existing methods on context-independent features. CircuitLens captures how
feature activations arise from interactions between components, revealing
circuit-level dynamics that activation-only approaches cannot identify.
Together, these methods increase interpretability robustness and enhance
scalable mechanistic analysis of circuits while maintaining efficiency and
quality.

</details>


### [529] [Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models](https://arxiv.org/abs/2510.14961)
*Jonas Geiping,Xinyu Yang,Guinan Su*

Main category: cs.LG

TL;DR: 循环深度语言模型可以通过并行化潜在状态来加速生成，与基于相同时间预算的基线自回归生成相比，具有更强的表达能力。


<details>
  <summary>Details</summary>
Motivation: 探究循环深度模型与扩散语言模型的潜在联系，并利用其相似性开发新的采样器以加速生成。

Method: 提出一种新的扩散强制采样器，该采样器在模型的每次前向传播中解码新标记，同时允许通过循环并行地优化这些标记的潜在状态。

Result: 提出的采样器理论上比基线自回归生成更具表现力，并且可以直接应用于现有的循环深度Transformer模型，实现高达5倍的速度提升，且无需进行任何调优。

Conclusion: 提出的方法不仅为循环深度模型在推理时并行化额外计算提供了一种有效的机制，而且表明这类模型可以被视为强大的、连续的、但因果的扩散语言模型。

Abstract: Language models with recurrent depth, also referred to as universal or looped
when considering transformers, are defined by the capacity to increase their
computation through the repetition of layers. Recent efforts in pretraining
have demonstrated that these architectures can scale to modern language
modeling tasks while exhibiting advantages in reasoning tasks. In this work, we
examine the relationship between recurrent-depth models and diffusion language
models. Building on their similarities, we develop a new diffusion forcing
sampler for these models to accelerate generation. The sampler advances by
decoding new tokens at every forward pass of the model, while the latent states
of these tokens can be further refined in parallel through recurrence.
Theoretically, generation with our sampler is strictly more expressive than the
baseline autoregressive generation using the same time budget on modern
hardware. Moreover, this sampler, based on principles from diffusion
literature, can be directly applied to existing 3.5B recurrent-depth
transformers without any tuning, leading to up to a 5x speedup. Consequently,
our findings not only provide an efficient mechanism for parallelizing the
extra computation in recurrent-depth models at inference, but also suggest that
such models can be naturally viewed as strong continuous, though causal,
diffusion language models.

</details>


### [530] [Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity in TVD-MI Scores](https://arxiv.org/abs/2510.14966)
*Zachary Robertson*

Main category: cs.LG

TL;DR: 该研究提出了一种基于总变差-互信息（TVD-MI）的LLM评估新方法，通过平均成对比较生成可加性评分，并使用无非线性链接函数的经典项目反应理论（IRT）进行分析，经验证明身份链接优于Logistic/Probit链接，能更准确地保留数据结构，并能有效减少评估次数，同时保持模型排名的一致性，且适用于其他有界响应域。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM评估方法在成对比较后产生的二元决策不利于直接的量化分析。本研究旨在提出一种新的方法，能够从成对比较中生成具有加性结构的概率评分，以便于后续分析。

Method: 研究人员提出了一种利用总变差-互信息（TVD-MI）进行成对LLM比较的方法。通过对TVD-MI的二元试验进行平均，生成具有加性结构的中心化概率评分，并将其应用于项目反应理论（IRT）分析。在实验中，研究人员比较了身份链接函数与Logistic/Probit链接函数在保留数据加性结构方面的表现，并推导了一种基于Gini熵最大化的裁剪线性模型，该模型可以通过盒约束最小二乘法进行求解，从而处理边界饱和问题。最后，通过在三个不同领域进行实验，评估了该方法的有效性，并进行了模型鲁棒性分析。

Result: 在三个不同领域进行的实验表明，身份链接函数在保留原始数据加性结构方面表现优于Probit/Logit链接函数，其数据曲率中位数在0.080-0.150之间，而Probit/Logit链接函数的数据曲率中位数则高达[0.245, 0.588]。提出的裁剪线性模型在33%的覆盖率下，实现了0.117±0.008的保持集均方根误差（RMSE），并保持了0.972±0.015的Spearman $ho$ 系数，评估次数比全密集评估少三倍。此外，在GPT-4o-mini和Llama3-70b之间的模型鲁棒性分析中，两种模型在评估结果上表现出高度一致性（Spearman $ho$ = 0.872），并且身份链接函数的优势也得到了证实。

Conclusion: TVD-MI的几何结构最适合使用身份映射进行保留，以实现高效的LLM评估。这种方法不仅适用于LLM评估，也能够应用于其他有界响应的领域。

Abstract: Pairwise comparisons of large language models using total variation distance
mutual information (TVD-MI) produce binary critic decisions per pair. We show
that averaging TVD-MI's binary trials yields centered-probability scores with
additive structure suitable for item-response theory (IRT) without nonlinear
link functions. Maximum-likelihood approaches to IRT use logistic links, but we
find empirically that these transformations introduce curvature that breaks
additivity: across three domains, the identity link yields median curl on raw
data of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce
substantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We
derive this clipped-linear model from Gini entropy maximization, yielding a
box-constrained least-squares formulation that handles boundary saturation. At
33% coverage, we achieve holdout RMSE $0.117 \pm 0.008$ while preserving agent
rankings (Spearman $\rho = 0.972 \pm 0.015$), three times fewer evaluations
than full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows
strong agreement in agent rankings ($\rho = 0.872$) and consistent
identity-link advantage. TVD-MI's geometry is best preserved by identity
mapping for efficient LLM evaluation, applicable to other bounded-response
domains.

</details>


### [531] [Biology-informed neural networks learn nonlinear representations from omics data to improve genomic prediction and interpretability](https://arxiv.org/abs/2510.14970)
*Katiana Kontolati,Rini Jasmine Gladstone,Ian Davis,Ethan Pickering*

Main category: cs.LG

TL;DR: 结合多组学数据和生物学知识的神经网络（BINNs）在作物基因组预测和选择方面取得了显著进展，提高了预测准确性并揭示了新的生物学关系。


<details>
  <summary>Details</summary>
Motivation: 传统基因型-表型（G2P）模型准确性有限，需要昂贵的田间试验。现有的考虑中间分子表型的模型在实际应用中不可行，因为在部署或设计时无法获得此类数据。

Method: 提出并应用生物知情神经网络（BINNs），将数千个单核苷酸多态性（SNPs）与多组学测量和先验生物学知识相结合。BINNs在训练时利用多组学数据和通路信息，在推理时仅使用基因型数据。

Result: 在玉米基因表达和多环境田间试验数据中，BINNs在稀疏数据条件下提高了基因组预测的秩相关准确性，最高可达56%。在合成代谢组学基准测试中，BINNs将预测误差降低了75%，并识别出最重要的非线性通路。BINNs学习到的潜在变量与它们代表的实验量相关，表明模型学习到了生物学相关表示。

Conclusion: BINNs提供了一个利用中间领域信息来提高基因组预测准确性、揭示非线性生物学关系（可用于基因组选择、候选基因选择、通路富集和基因编辑优先排序）的框架。

Abstract: We extend biologically-informed neural networks (BINNs) for genomic
prediction (GP) and selection (GS) in crops by integrating thousands of
single-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior
biological knowledge. Traditional genotype-to-phenotype (G2P) models depend
heavily on direct mappings that achieve only modest accuracy, forcing breeders
to conduct large, costly field trials to maintain or marginally improve genetic
gain. Models that incorporate intermediate molecular phenotypes such as gene
expression can achieve higher predictive fit, but they remain impractical for
GS since such data are unavailable at deployment or design time. BINNs overcome
this limitation by encoding pathway-level inductive biases and leveraging
multi-omics data only during training, while using genotype data alone during
inference. Applied to maize gene-expression and multi-environment field-trial
data, BINN improves rank-correlation accuracy by up to 56% within and across
subpopulations under sparse-data conditions and nonlinearly identifies genes
that GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic
metabolomics benchmark, BINN reduces prediction error by 75% relative to
conventional neural nets and correctly identifies the most important nonlinear
pathway. Importantly, both cases show highly sensitive BINN latent variables
correlate with the experimental quantities they represent, despite not being
trained on them. This suggests BINNs learn biologically-relevant
representations, nonlinear or linear, from genotype to phenotype. Together,
BINNs establish a framework that leverages intermediate domain information to
improve genomic prediction accuracy and reveal nonlinear biological
relationships that can guide genomic selection, candidate gene selection,
pathway enrichment, and gene-editing prioritization.

</details>


### [532] [pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation](https://arxiv.org/abs/2510.14974)
*Hansheng Chen,Kai Zhang,Hao Tan,Leonidas Guibas,Gordon Wetzstein,Sai Bi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Few-step diffusion or flow-based generative models typically distill a
velocity-predicting teacher into a student that predicts a shortcut towards
denoised data. This format mismatch has led to complex distillation procedures
that often suffer from a quality-diversity trade-off. To address this, we
propose policy-based flow models ($\pi$-Flow). $\pi$-Flow modifies the output
layer of a student flow model to predict a network-free policy at one timestep.
The policy then produces dynamic flow velocities at future substeps with
negligible overhead, enabling fast and accurate ODE integration on these
substeps without extra network evaluations. To match the policy's ODE
trajectory to the teacher's, we introduce a novel imitation distillation
approach, which matches the policy's velocity to the teacher's along the
policy's trajectory using a standard $\ell_2$ flow matching loss. By simply
mimicking the teacher's behavior, $\pi$-Flow enables stable and scalable
training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it
attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT
architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\pi$-Flow achieves
substantially better diversity than state-of-the-art few-step methods, while
maintaining teacher-level quality.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [533] [What is missing from this picture? Persistent homology and mixup barcodes as a means of investigating negative embedding space](https://arxiv.org/abs/2510.14327)
*Himanshu Yadav,Thomas Bryan Smith,Peter Bubenik,Christopher McCarty*

Main category: cs.SI

TL;DR: 通过结合网络科学和主题建模方法，提出了一种新的研究指标，用于量化研究的颠覆性和概念新颖性。


<details>
  <summary>Details</summary>
Motivation: 现有的引文指标存在内在偏见，需要新的指标来衡量同行评审研究的颠覆性以及概念新颖性。

Method: 使用top2vec嵌入文档和主题，然后利用拓扑数据分析（TDA）中的持久同调和混合条形码来识别嵌入分布中的“洞”，并分析这些“洞”被未观察到的出版物（历史或未来研究）填充的情况，以区分缺失的背景和创新的空间。

Result: 研究了负嵌入空间在多大程度上代表了缺失的背景（历史研究）与创新空间（新研究）的关系，以及占据该空间的文件在多大程度上代表了周边研究主题的整合。

Conclusion: 提出了一个结合拓扑数据分析和主题建模的新指标，用于分析研究的负嵌入空间，并讨论了其潜在应用。

Abstract: Recent work in the information sciences, especially informetrics and
scientometrics, has made substantial contributions to the development of new
metrics that eschew the intrinsic biases of citation metrics. This work has
tended to employ either network scientific (topological) approaches to
quantifying the disruptiveness of peer-reviewed research, or topic modeling
approaches to quantifying conceptual novelty. We propose a combination of these
approaches, investigating the prospect of topological data analysis (TDA),
specifically persistent homology and mixup barcodes, as a means of
understanding the negative space among document embeddings generated by topic
models. Using top2vec, we embed documents and topics in n-dimensional space, we
use persistent homology to identify holes in the embedding distribution, and
then use mixup barcodes to determine which holes are being filled by a set of
unobserved publications. In this case, the unobserved publications represent
research that was published before or after the data used to train top2vec. We
investigate the extent that negative embedding space represents missing context
(older research) versus innovation space (newer research), and the extend that
the documents that occupy this space represents integrations of the research
topics on the periphery. Potential applications for this metric are discussed.

</details>


### [534] [Detecting Early and Implicit Suicidal Ideation via Longitudinal and Information Environment Signals on Social Media](https://arxiv.org/abs/2510.14889)
*Soorya Ram Shimgekar,Ruining Zhao,Agam Goyal,Violeta J. Rodriguez,Paul A. Bloom,Hari Sundaram,Koustuv Saha*

Main category: cs.SI

TL;DR: 本研究提出了一种通过分析用户及其社交邻居的帖子来检测隐晦自杀意念（SI）的方法。


<details>
  <summary>Details</summary>
Motivation: 许多经历自杀意念（SI）的人在社交媒体上不会明确表达，而是通过日常帖子或同伴互动间接流露，早期检测这些隐晦信号至关重要但充满挑战。

Method: 将早期和隐晦的SI视为一个前瞻性预测任务，构建了一个计算框架，模拟用户的信息环境（包括其长期的发帖历史及其社交邻居的讨论），采用复合网络中心性测量识别用户的主要邻居，并对用户和邻居的互动进行时间对齐，最后将多层信号整合到微调后的DeBERTa-v3模型中。

Result: 在一项针对1000名Reddit用户（500个案例组和500个对照组）的研究中，该方法比仅基于个人的基线模型在早期和隐晦SI检测方面提高了15%。

Conclusion: 研究结果表明，同伴互动提供了有价值的预测信号，并对设计能够捕捉在线环境中风险的间接和隐藏表达的早期检测系统具有广泛的启示。

Abstract: On social media, many individuals experiencing suicidal ideation (SI) do not
disclose their distress explicitly. Instead, signs may surface indirectly
through everyday posts or peer interactions. Detecting such implicit signals
early is critical but remains challenging. We frame early and implicit SI as a
forward-looking prediction task and develop a computational framework that
models a user's information environment, consisting of both their longitudinal
posting histories as well as the discourse of their socially proximal peers. We
adopted a composite network centrality measure to identify top neighbors of a
user, and temporally aligned the user's and neighbors' interactions --
integrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a
Reddit study of 1,000 (500 Case and 500 Control) users, our approach improves
early and implicit SI detection by 15% over individual-only baselines. These
findings highlight that peer interactions offer valuable predictive signals and
carry broader implications for designing early detection systems that capture
indirect as well as masked expressions of risk in online environments.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [535] [Distributed-Memory Parallel Algorithms for Fixed-Radius Near Neighbor Graph Construction](https://arxiv.org/abs/2510.14147)
*Gabriel Raulet,Dmitriy Morozov,Aydin Buluc,Katherine Yelick*

Main category: cs.DC

TL;DR: 该论文提出了一种可扩展的、感知稀疏性的分布式内存算法，使用 C++ 语言实现了 Cover Trees，用于在通用度量空间中计算固定半径近邻图，解决了现有技术在处理大规模科学数据集和非欧几里得度量方面的不足。


<details>
  <summary>Details</summary>
Motivation: 在各种数据分析算法中，计算固定半径近邻图是一个关键的预处理步骤。随着计算能力的提升和数据采集技术的进步，大规模科学数据集的分析需求日益增长，因此需要可扩展的近邻图计算解决方案，特别是在需要精确解和非欧几里得度量的情况下。

Method: 该论文提出了一种基于 Cover Trees 的可扩展稀疏感知分布式内存算法。首先，提供了一种用于 Cover Tree 构建的共享内存算法，并证明了其与现有固定半径搜索数据结构的竞争力。然后，引入了两种用于近邻图问题的分布式内存算法：一种是简单的点划分策略，另一种是空间划分策略，这两种策略都利用了每个节点上的 Cover Tree 算法。

Result: 该算法在各种真实和合成数据集上，针对传统和非传统度量，都表现出了良好的并行扩展性。在处理包含一百万个数据点的高维真实数据集时，对于包含平均 70 个邻居的图，使用了 1024 个核心，实现了比现有技术高 678.34 倍的速度提升；对于包含平均 500 个邻居的图，使用了 4096 个核心，速度提升高达 1590.99 倍。

Conclusion: 该论文提出的分布式内存算法在计算通用度量空间中的固定半径近邻图方面，具有高度的可扩展性和效率，特别是在处理大规模高维数据集时，相比现有技术取得了显著的速度提升，为科学数据分析提供了强大的支持。

Abstract: Computing fixed-radius near-neighbor graphs is an important first step for
many data analysis algorithms. Near-neighbor graphs connect points that are
close under some metric, endowing point clouds with a combinatorial structure.
As computing power and data acquisition methods advance, diverse sources of
large scientific datasets would greatly benefit from scalable solutions to this
common subroutine for downstream analysis. Prior work on parallel nearest
neighbors has made great progress in problems like k-nearest and approximate
nearest neighbor search problems, with particular attention on Euclidean
spaces. Yet many applications need exact solutions and non-Euclidean metrics.
This paper presents a scalable sparsity-aware distributed memory algorithm
using cover trees to compute near-neighbor graphs in general metric spaces. We
provide a shared-memory algorithm for cover tree construction and demonstrate
its competitiveness with state-of-the-art fixed-radius search data structures.
We then introduce two distributed-memory algorithms for the near-neighbor graph
problem, a simple point-partitioning strategy and a spatial-partitioning
strategy, which leverage the cover tree algorithm on each node. Our algorithms
exhibit parallel scaling across a variety of real and synthetic datasets for
both traditional and non-traditional metrics. On real world high dimensional
datasets with one million points, we achieve speedups up to 678.34x over the
state-of-the-art using 1024 cores for graphs with 70 neighbors per vertex (on
average), and up to 1590.99x using 4096 cores for graphs with 500 neighbors per
vertex (on average).

</details>


### [536] [Efficiently Executing High-throughput Lightweight LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management](https://arxiv.org/abs/2510.14024)
*Thanh Son Phung,Douglas Thain*

Main category: cs.DC

TL;DR: 生成式AI工作负载需要对HPC集群进行改进，以支持轻量级LLM与传统应用程序的集成。


<details>
  <summary>Details</summary>
Motivation: 当前的HPC集群设计无法满足集成了轻量级LLM和传统高吞吐量应用程序的新型HPC工作负载的需求，导致等待时间长或LLM启动成本高。

Method: 提出了一种名为‘Pervasive Context Management’的技术，将LLM初始化上下文与其推理分离，并将上下文保留在GPU上，直到不再需要。

Result: 通过应用该技术，事实核查应用程序的执行时间减少了72.1%（从3小时减少到48分钟），并且能够在32.8%的GPU上实现机会性扩展，将执行时间进一步缩短至13分钟。

Conclusion: ‘Pervasive Context Management’技术有效地解决了当前HPC集群在支持生成式AI工作负载时面临的挑战，显著提高了应用程序的性能和可扩展性。

Abstract: The rise of Generative AI introduces a new class of HPC workloads that
integrates lightweight LLMs with traditional high-throughput applications to
accelerate scientific discovery. The current design of HPC clusters is
inadequate to support this new class however, either incurring long wait times
on static batch queues or repeatedly paying expensive LLM startup costs upon
resource preemption. To circumvent both the long queues and high startup costs,
we propose to "decouple" the LLM initialization context from the actual LLM
inferences, and retain the context in GPUs until it is no longer needed, a
technique we term "Pervasive Context Management". We transform a fact
verification application to enable this technique, allowing it to reduce its
execution time by 72.1% (from 3 hours to 48 minutes) using the same amount of
GPUs, and scale opportunistically on 32.8% of all GPUs in the cluster and
further reduce the execution time to 13 minutes.

</details>


### [537] [Anonymized Network Sensing using C++26 std::execution on GPUs](https://arxiv.org/abs/2510.14050)
*Michael Mandulak,Sayan Ghosh,S M Ferdous,Mahantesh Halappanavar,George Slota*

Main category: cs.DC

TL;DR: 可组合框架利用C++26 Senders模型在密集GPU系统上实现高效的网络传感


<details>
  <summary>Details</summary>
Motivation: 解决大规模网络传感中，GPU内存管理和复杂任务移植的挑战，并探索利用GPU作为一流执行资源。

Method: 使用C++26 Senders模型，一种用于异步操作链的现代C++编程模型，以实现GPU上的分析任务部署。

Result: 在8个NVIDIA A100 GPU上，基于商品库的实现比参考串行GraphBLAS基线实现了高达55倍的性能提升。

Conclusion: 采用通用的、富有生产力的编程模型（如C++26 Senders模型）不会影响关键路径性能，并且可以实现高效的多GPU应用程序开发。

Abstract: Large-scale network sensing plays a vital role in network traffic analysis
and characterization. As network packet data grows increasingly large, parallel
methods have become mainstream for network analytics. While effective,
GPU-based implementations still face start-up challenges in host-device memory
management and porting complex workloads on devices, among others. To mitigate
these challenges, composable frameworks have emerged using modern C++
programming language, for efficiently deploying analytics tasks on GPUs.
Specifically, the recent C++26 Senders model of asynchronous data operation
chaining provides a simple interface for bulk pushing tasks to varied device
execution contexts.
  Considering the prominence of contemporary dense-GPU platforms and
vendor-leveraged software libraries, such a programming model consider GPUs as
first-class execution resources (compared to traditional host-centric
programming models), allowing convenient development of multi-GPU application
workloads via expressive and standardized asynchronous semantics. In this
paper, we discuss practical aspects of developing the Anonymized Network
Sensing Graph Challenge on dense-GPU systems using the recently proposed C++26
Senders model. Adopting a generic and productive programming model does not
necessarily impact the critical-path performance (as compared to low-level
proprietary vendor-based programming models): our commodity library-based
implementation achieves up to 55x performance improvements on 8x NVIDIA A100
GPUs as compared to the reference serial GraphBLAS baseline.

</details>


### [538] [Balls and Bins and the Infinite Process with Random Deletions](https://arxiv.org/abs/2510.14798)
*Petra Berenbrink,Tom Friedetzky,Peter Kling,Lars Nagel*

Main category: cs.DC

TL;DR: 该论文研究了一个包含删除操作的无限球分配过程。在每个时间步，以概率β(t)使用贪心策略分配新球（将球放入随机选择的两个箱子中较低的那个），以概率1-β(t)从随机选择的非空箱子中删除一个球。论文关注于界定当前最大负载与当前平均负载之差（差异）以及当前最大负载与历史最高平均负载之差（过载）。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是分析一个比以往模型更复杂的球分配过程（包含删除操作），并为该过程中的最大负载与平均负载的差异以及最大负载与历史最高平均负载的差异提供界限。

Method: 论文采用了分层归纳法，并结合了详细的势能分析和概率耦合技术来处理更一般的场景和简化分析。

Result: 论文证明了在任意给定时间t，总球数超过平均数的球数为O(n)，差异为O(log n)，并且差异存在匹配的下界。论文还证明了过载为log log n + O(1)。对于“良好”的插入概率序列，差异可以被界定为log log n + O(1)。

Conclusion: 该论文成功地为包含删除操作的球分配过程提供了差异和过载的界限，并利用创新的分析工具（分层归纳、势能分析、概率耦合）处理了复杂性。其结果对于理解和优化此类动态负载均衡系统具有重要意义。

Abstract: We consider an infinite balls-into-bins process with deletions where in each
discrete step $t$ a coin is tossed as to whether, with probability $\beta(t)
\in (0,1)$, a new ball is allocated using the Greedy[2] strategy (which places
the ball in the lower loaded of two bins sampled uniformly at random) or, with
remaining probability $1-\beta(t)$, a ball is deleted from a non-empty bin
chosen uniformly at random. Let $n$ be the number of bins and $m(t)$ the total
load at time $t$. We are interested in bounding the discrepancy $x_{\max}(t) -
m(t)/n$ (current maximum load relative to current average) and the overload
$x_{\max}(t) - m_{\max}(t)/n$ (current maximum load relative to highest average
observed so far).
  We prove that at an arbitrarily chosen time $t$ the total number of balls
above the average is $O(n)$ and that the discrepancy is $ O(\log(n))$. For the
discrepancy, we provide a matching lower bound. Furthermore we prove that at an
arbitrarily chosen time $t$ the overload is $\log\log(n)+O(1)$. For "good"
insertion probability sequences (in which the average load of time intervals
with polynomial length increases in expectation) we show that even the
discrepancy is bounded by $\log\log(n)+O(1)$.
  One of our main analytical tools is a layered induction, as per [ABKU99].
Since our model allows for rather more general scenarios than what was
previously considered, the formal analysis requires some extra ingredients as
well, in particular a detailed potential analysis. Furthermore, we simplify the
setup by applying probabilistic couplings to obtain certain "recovery"
properties, which eliminate much of the need for intricate and careful
conditioning elsewhere in the analysis.

</details>


### [539] [Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic Serving](https://arxiv.org/abs/2510.14126)
*Nikos Pagonas,Yeounoh Chung,Kostis Kaffes,Arvind Krishnamurthy*

Main category: cs.DC

TL;DR: Cortex是一个为代理工作流设计的服务平台，通过阶段隔离解决资源争用问题，提高性能和资源利用率，并为未来的代理服务范式奠定基础。


<details>
  <summary>Details</summary>
Motivation: 需要一个能解决代理工作流中各阶段之间计算和内存干扰问题的服务平台，以提高KV缓存利用率、吞吐量和性能可预测性。

Method: 通过为代理工作流的每个不同阶段配置专用的资源池来实现阶段隔离，并对每个阶段内的资源分配和调度进行定制。

Result: 实现了更好的KV缓存利用率、更高的吞吐量和更可预测的性能。

Conclusion: Cortex通过阶段隔离为代理工作流提供了一个高效、可预测的服务平台，并为实现可塑资源管理、工作流分支的推测执行以及共享的多层“代理状态”缓存等更高级的代理原生服务范式奠定了基础。

Abstract: We introduce Cortex, a prototype workflow-aware serving platform designed for
agentic workloads. The core principle of Cortex is stage isolation: it
provisions dedicated resource pools for each distinct stage of an agentic
workflow. This simple yet powerful strategy mitigates inter-stage interference
in compute and memory, leading to better KV cache utilization, higher
throughput, and more predictable performance. By customizing resource
allocation and scheduling within each distinct stage of agentic workflows,
Cortex lays the groundwork for more advanced, agent-native serving paradigms,
including malleable resource management, speculative execution of workflow
branches, and a shared, multi-tiered cache for "agentic state."

</details>


### [540] [Privacy-Preserving and Incentive-Driven Relay-Based Framework for Cross-Domain Blockchain Interoperability](https://arxiv.org/abs/2510.14151)
*Saeed Moradi,Koosha Esmaeilzadeh Khorasani,Sara Rouhani*

Main category: cs.DC

TL;DR: 该论文提出了一种区块链无关的框架，用于在许可和无许可网络之间实现互操作性，通过加密技术确保安全的数据交换，并集成Clover和Dandelion++协议来增强交易匿名性。


<details>
  <summary>Details</summary>
Motivation: 为了实现区块链从孤立网络到协作生态系统的转变，释放其全部潜力，互操作性至关重要。然而，在许可和无许可区块链之间架起桥梁面临着独特的挑战，因为它们在访问控制、架构和安全要求方面存在差异。

Method: 本论文提出了一种区块链无关的框架，利用加密技术确保安全的数据交换，并整合Clover和Dandelion++协议来增强交易匿名性。该框架具有轻量级的架构设计。

Result: 性能评估表明，该框架在实现安全高效的互操作性方面是有效的，其衡量指标包括跨异构区块链生态系统的转发时间、吞吐量、可用性及其共谋影响。

Conclusion: 该框架能够实现许可和无许可区块链之间的安全高效的互操作性。

Abstract: Interoperability is essential for transforming blockchains from isolated
networks into collaborative ecosystems, unlocking their full potential. While
significant progress has been made in public blockchain interoperability,
bridging permissioned and permissionless blockchains poses unique challenges
due to differences in access control, architectures, and security requirements.
This paper introduces a blockchain-agnostic framework to enable
interoperability between permissioned and permissionless networks. Leveraging
cryptographic techniques, the framework ensures secure data exchanges. Its
lightweight architectural design simplifies implementation and maintenance,
while the integration of Clover and Dandelion++ protocols enhances transaction
anonymity. Performance evaluations demonstrate the framework's effectiveness in
achieving secure and efficient interoperability by measuring the forwarding
time, the throughput, the availability, and their collusion impact of the
system across heterogeneous blockchain ecosystems.

</details>


### [541] [Proof-Carrying Fair Ordering: Asymmetric Verification for BFT via Incremental Graphs](https://arxiv.org/abs/2510.14186)
*Pengkun Ren,Hai Dong,Nasrin Sohrabi,Zahir Tari,Pengcheng Zhang*

Main category: cs.DC

TL;DR: AUTIG 是一种高性能、可插拔的排序公平服务，通过减少领导者和跟随者的计算来解决 BFT 共识协议中的价值提取问题。


<details>
  <summary>Details</summary>
Motivation: BFT 共识协议虽然能容忍恶意行为者，但其不受限制的排序能力会带来诸如抢先交易和三明治攻击等价值提取攻击。现有的排序公平协议（如 Themis）虽然能提供强有力的保证，但要求所有副本重新运行领导者的昂贵排序计算，导致冗余。AUTIG 旨在解决这种对称且冗余的范式。

Method: AUTIG 采用非对称架构，领导者维护一个持久的未确认交易增量图 (UTIG)，并通过每个提议发布一个公平性证明。跟随者无需维护历史状态即可验证该证明。其关键创新包括：(1) 由阈值交叉事件和状态变化驱动的增量图维护；(2) 将领导者端的收集/更新/提取与跟随者端的无状态验证重叠的解耦管道；(3) 一个涵盖已完成前缀中所有内部对以及一个边界完整性检查的证明设计，以排除隐藏的外部依赖。

Result: 实验表明，AUTIG 在部分同步条件下，相比于对称图基线，实现了更高的吞吐量和更低的端到端延迟，同时保持了 gamma-batch-order-fairness。

Conclusion: AUTIG 通过创新的非对称架构和高效的验证机制，显著提高了排序公平共识协议的性能，有效解决了 BFT 共识中的价值提取问题，并降低了验证成本。

Abstract: Byzantine Fault-Tolerant (BFT) consensus protocols ensure agreement on
transaction ordering despite malicious actors, but unconstrained ordering power
enables sophisticated value extraction attacks like front running and sandwich
attacks - a critical threat to blockchain systems. Order-fair consensus curbs
adversarial value extraction by constraining how leaders may order
transactions. While state-of-the-art protocols such as Themis attain strong
guarantees through graph-based ordering, they ask every replica to re-run the
leader's expensive ordering computation for validation - an inherently
symmetric and redundant paradigm. We present AUTIG, a high-performance,
pluggable order-fairness service that breaks this symmetry. Our key insight is
that verifying a fair order does not require re-computing it. Instead,
verification can be reduced to a stateless audit of succinct, verifiable
assertions about the ordering graph's properties. AUTIG realizes this via an
asymmetric architecture: the leader maintains a persistent
Unconfirmed-Transaction Incremental Graph (UTIG) to amortize graph construction
across rounds and emits a structured proof of fairness with each proposal;
followers validate the proof without maintaining historical state. AUTIG
introduces three critical innovations: (i) incremental graph maintenance driven
by threshold-crossing events and state changes; (ii) a decoupled pipeline that
overlaps leader-side collection/update/extraction with follower-side stateless
verification; and (iii) a proof design covering all internal pairs in the
finalized prefix plus a frontier completeness check to rule out hidden external
dependencies. We implement AUTIG and evaluate it against symmetric graph-based
baselines under partial synchrony. Experiments show higher throughput and lower
end-to-end latency while preserving gamma-batch-order-fairness.

</details>


### [542] [Deadlock-free routing for Full-mesh networks without using Virtual Channels](https://arxiv.org/abs/2510.14730)
*Alejandro Cano,Cristóbal Camarero,Carmen Martínez,Ramón Beivide*

Main category: cs.DC

TL;DR: TERA是一种无需虚拟通道（VC）的路由算法，它通过嵌入的物理子网提供无死锁的非最小路径，解决了现有VC的开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有的高基数、低直径网络（如HyperX和Dragonfly）虽然使用全网状核心和多虚拟通道（VC）来避免自适应路由中的数据包死锁，但VC会增加交换机的面积、功耗和设计复杂度，限制了其可扩展性。而仅依赖链路排序的全网状网络虽然简单，但在对抗性流量下性能会下降。因此，需要一种新的路由算法来克服这些挑战。

Method: TERA（Topology-Embedded Routing Algorithm）是一种新颖的路由算法，它利用嵌入的物理子网来提供无死锁的非最小路径，并且无需使用虚拟通道（VC）。

Result: 在全网状网络中，TERA在处理对抗性流量时比链路排序路由算法性能提升80%，在应用内核中提升高达100%。与基于VC的方法相比，TERA将缓冲区需求减少了50%，同时保持了可比的延迟和吞吐量。在2D-HyperX评估中，TERA的性能比使用相同数量VC的最先进算法高出32%。

Conclusion: TERA通过嵌入物理子网实现了无VC的死锁free路由，在性能和资源开销方面均优于现有方法，并有望进一步提高网络性能。

Abstract: High-radix, low-diameter networks like HyperX and Dragonfly use a Full-mesh
core, and rely on multiple virtual channels (VCs) to avoid packet deadlocks in
adaptive routing. However, VCs introduce significant overhead in the switch in
terms of area, power, and design complexity, limiting the switch scalability.
This paper starts by revisiting VC-less routing through link ordering schemes
in Full-mesh networks, which offer implementation simplicity but suffer from
performance degradation under adversarial traffic. Thus, to overcome these
challenges, we propose TERA (Topology-Embedded Routing Algorithm), a novel
routing algorithm which employs an embedded physical subnetwork to provide
deadlock-free non-minimal paths without using VCs.
  In a Full-mesh network, TERA outperforms link ordering routing algorithms by
80% when dealing with adversarial traffic, and up to 100% in application
kernels. Furthermore, compared to other VC-based approaches, it reduces buffer
requirements by 50%, while maintaining comparable latency and throughput.
Lastly, early results from a 2D-HyperX evaluation show that TERA outperforms
state-of-the-art algorithms that use the same number of VCs, achieving
performance improvements of up to 32%.

</details>


### [543] [FairBatching: Fairness-Aware Batch Formation for LLM Inference](https://arxiv.org/abs/2510.14392)
*Hongtao Lyu,Boyue Liu,Mingyu Wu,Haibo Chen*

Main category: cs.DC

TL;DR: FairBatching 是一种新型 LLM 推理调度器，通过公平分配预填和解码任务的资源，解决现有调度器存在的计算不公平问题，显著降低了 TTFT 延迟并提高了系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 推理调度器（如 Sarathi）虽然能防止解码停滞，但存在计算不公平的问题，导致解码任务过度优先，GPU 利用率低，并增加了预填排队延迟，最终损害了服务质量。

Method: FairBatching 提出了一种公平的调度方法，通过以下机制解决上述问题：1. 引入自适应批次容量确定机制，动态调整计算预算，提高 GPU 利用率而不违反 SLO。2. 采用公平且动态的批次形成算法，打破了仅优先解码的模式，允许将计算资源从突发的解码任务中回收，以服务预填的激增，从而实现全局公平。3. 提供新颖的负载估计方法，以实现与上层调度器的更有效协调。

Result: 在真实数据上进行实现和评估，FairBatching 显著降低了 TTFT 尾部延迟（最高可达 2.29 倍），同时稳健地维持了 TPOT SLO，在单节点容量方面实现了 20.0% 的总体提升，在集群级容量方面实现了 54.3% 的提升。

Conclusion: FairBatching 通过其创新的公平资源分配和自适应调度机制，有效解决了 LLM 推理中的延迟和吞吐量瓶颈，显著提高了整体服务质量和资源利用率。

Abstract: Large language model (LLM) inference systems face a fundamental tension
between minimizing Time-to-First-Token (TTFT) latency for new requests and
maintaining a high, steady token generation rate (low Time-Per-Output-Token, or
TPOT) for ongoing requests. Existing stall-free batching schedulers proposed by
Sarathi, while effective at preventing decode stalls, introduce significant
computational unfairness. They prioritize decode tasks excessively,
simultaneously leading to underutilized decode slack and unnecessary prefill
queuing delays, which collectively degrade the system's overall quality of
service (QoS).
  This work identifies the root cause of this unfairness: the non-monotonic
nature of Time-Between-Tokens (TBT) as a scheduling metric and the rigid
decode-prioritizing policy that fails to adapt to dynamic workload bursts. We
therefore propose FairBatching, a novel LLM inference scheduler that enforces
fair resource allocation between prefill and decode tasks. It features an
adaptive batch capacity determination mechanism, which dynamically adjusts the
computational budget to improve the GPU utilization without triggering SLO
violations. Its fair and dynamic batch formation algorithm breaks away from the
decode-prioritizing paradigm, allowing computation resources to be reclaimed
from bursting decode tasks to serve prefill surges, achieving global fairness.
Furthermore, FairBatching provides a novel load estimation method, enabling
more effective coordination with upper-level schedulers. Implemented and
evaluated on realistic traces, FairBatching significantly reduces TTFT tail
latency by up to 2.29x while robustly maintaining TPOT SLOs, achieving overall
20.0% improvement in single-node capacity and 54.3% improvement in
cluster-level capacity.

</details>


### [544] [ScalePool: Hybrid XLink-CXL Fabric for Composable Resource Disaggregation in Unified Scale-up Domains](https://arxiv.org/abs/2510.14580)
*Hyein Woo,Miryeong Kwon,Jiseon Kim,Eunjee Na,Hanjin Choi,Seonghyeon Jang,Myoungsoo Jung*

Main category: cs.DC

TL;DR: ScalePool通过集成XLink和CXL，构建了一种新的集群架构，使用统一的硬件互连来连接大量加速器，实现了更快的LLM训练和更低的内存延迟。


<details>
  <summary>Details</summary>
Motivation: 传统的长距离网络互连方式在连接大量加速器时存在效率和可扩展性问题。本研究旨在提出一种新的集群架构，以解决这些挑战，提高互连效率并支持异构加速器和可组合资源。

Method: ScalePool采用Accelerator-Centric Links (XLink) 和 Compute Express Link (CXL) 技术，构建了一个混合互连结构。XLink用于集群内部的低延迟通信，而CXL则用于构建可扩展的、支持相干内存共享的集群间通信。通过CXL抽象接口，ScalePool解决了互操作性问题，并实现了显式的内存分层：tier-1结合了本地内存、CXL和XLink，而tier-2则使用基于CXL的内存节点。

Result: ScalePool在LLM训练方面平均将速度提高了1.22倍，最高可达1.84倍。此外，tier-2内存分离策略将内存密集型工作负载的延迟降低了高达4.5倍。

Conclusion: ScalePool通过创新的XLink-CXL混合架构和显式内存分层，有效解决了大规模加速器互连的挑战，显著提高了LLM训练性能，并降低了内存密集型任务的延迟，为未来的异构和可组合计算集群提供了一种有前景的解决方案。

Abstract: This paper proposes ScalePool, a novel cluster architecture designed to
interconnect numerous accelerators using unified hardware interconnects rather
than traditional long-distance networking. ScalePool integrates
Accelerator-Centric Links (XLink) and Compute Express Link (CXL) into a unified
XLink-CXL hybrid fabric. Specifically, ScalePool employs XLink for
intra-cluster, low-latency accelerator communication, while using hierarchical
CXL-based switching fabrics for scalable and coherent inter-cluster memory
sharing. By abstracting interfaces through CXL, ScalePool structurally resolves
interoperability constraints, enabling heterogeneous cluster operation and
composable resource disaggregation. In addition, ScalePool introduces explicit
memory tiering: the latency-critical tier-1 combines accelerator-local memory
with coherence-centric CXL and XLink, whereas the highcapacity tier-2 employs
dedicated memory nodes interconnected by a CXL-based fabric, achieving scalable
and efficient memory pooling. Evaluation results show that ScalePool
accelerates LLM training by 1.22x on average and up to 1.84x compared to
conventional RDMA-based environments. Furthermore, the proposed tier-2 memory
disaggregation strategy reduces latency by up to 4.5x for memory-intensive
workloads.

</details>


### [545] [JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job Atomization](https://arxiv.org/abs/2510.14599)
*Michal Konopa,Jan Fesl,Ladislav Ber ánek*

Main category: cs.DC

TL;DR: JASDA是一个新颖的去中心化GPU调度范式，通过作业与调度器之间的迭代交互，实现了可扩展、公平和响应迅速的资源管理。


<details>
  <summary>Details</summary>
Motivation: 传统中心化GPU调度难以应对日益复杂和时间变化的工作负载，需要更具扩展性的解决方案。

Method: JASDA将作业视为主动参与者，生成并评估可行子作业，以响应调度器宣布的执行窗口。调度器则进行策略驱动的清算，平衡利用率、公平性和响应能力。这种双向迭代交互融合了反馈、校准和概率安全机制。

Result: JASDA提供了一个可扩展的基础，能够实现市场感知和公平驱动的资源管理，将理论调度模型与现代MIG启用的GPU环境（特别是在人工智能和农业4.0领域）的实际部署相结合。

Conclusion: JASDA通过结合拍卖理论、在线优化和GPU工作负载的时间粒度，为解决MIG启用的GPU可扩展性挑战提供了一个创新的解决方案，实现了更优的资源利用和调度效率。

Abstract: The increasing complexity and temporal variability of workloads on
MIG-enabled GPUs challenge the scalability of traditional centralized
scheduling. Building upon the SJA concept, this paper introduces JASDA-a novel
paradigm that extends SJA from a largely centralized scheduling model toward a
fully decentralized negotiation process. In JASDA, jobs actively generate and
score feasible subjobs in response to scheduler-announced execution windows,
while the scheduler performs policy-driven clearing that balances utilization,
fairness, and temporal responsiveness. This bidirectional, iterative
interaction embeds feedback, calibration, and probabilistic safety directly
into the scheduling loop, enabling adaptive and transparent decision-making. By
coupling principles from auction theory and online optimization with the
temporal granularity of GPU workloads, JASDA provides a scalable foundation for
market-aware and fairness-driven resource management-bridging theoretical
scheduling models with practical deployment in modern MIG-enabled environments
relevant to Artificial Intelligence and Agriculture 4.0.

</details>


### [546] [MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC Systems](https://arxiv.org/abs/2510.14622)
*Miryeong Kwon,Donghyun Gouk,Hyein Woo,Junhee Kim,Jinwoo Baek,Kyungkuk Nam,Sangyoon Ji,Jiseon Kim,Hanyeoreum Bae,Junhyeok Jang,Hyunwoo You,Junseok Moon,Myoungsoo Jung*

Main category: cs.DC

TL;DR: MPI-over-CXL通过利用CXL的缓存一致性共享内存，用直接共享内存访问取代显式内存复制，从而减少通信延迟和内存带宽使用。


<details>
  <summary>Details</summary>
Motivation: MPI实现中的显式内存复制操作会产生冗余数据移动和缓冲区管理的开销，影响HPC工作负载的性能。

Method: MPI-over-CXL将共享内存区域直接映射到MPI进程的虚拟地址空间，实现基于指针的通信，消除复制操作。

Result: 通过使用CXL 3.2控制器、基于FPGA的多主机仿真和专用软件栈进行评估，MPI-over-CXL在性能上相比传统MPI系统有显著提升。

Conclusion: MPI-over-CXL通过利用CXL技术，能够有效减少MPI通信的开销，提高HPC环境的效率和可扩展性。

Abstract: MPI implementations commonly rely on explicit memory-copy operations,
incurring overhead from redundant data movement and buffer management. This
overhead notably impacts HPC workloads involving intensive inter-processor
communication. In response, we introduce MPI-over-CXL, a novel MPI
communication paradigm leveraging CXL, which provides cache-coherent shared
memory across multiple hosts. MPI-over-CXL replaces traditional data-copy
methods with direct shared memory access, significantly reducing communication
latency and memory bandwidth usage. By mapping shared memory regions directly
into the virtual address spaces of MPI processes, our design enables efficient
pointer-based communication, eliminating redundant copying operations. To
validate this approach, we implement a comprehensive hardware and software
environment, including a custom CXL 3.2 controller, FPGA-based multi-host
emulation, and dedicated software stack. Our evaluations using representative
benchmarks demonstrate substantial performance improvements over conventional
MPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and
scalability in large-scale HPC environments.

</details>


### [547] [xLLM Technical Report](https://arxiv.org/abs/2510.14686)
*Tongxuan Liu,Tao Peng,Peijun Yang,Xiaoyang Zhao,Xiusheng Lu,Weizhe Huang,Zirui Liu,Xiaoyu Chen,Zhiwei Liang,Jun Xiong,Donghe Jin,Minchao Zhang,Jinrong Guo,Yingxu Deng,Xu Zhang,Xianzhe Dong,Siqi Wang,Siyu Wu,Yu Wu,Zihan Tang,Yuting Zeng,Yanshu Wang,Jinguang Liu,Meng Kang,Menxin Li,Yunlong Wang,Yiming Liu,Xiaolong Ma,Yifan Wang,Yichen Zhang,Jinrun Yin,Keyang Zheng,Jiawei Yin,Jun Zhang,Ziyue Wang,Xiaobo Lin,Liangyu Liu,Liwei Lan,Yang Liu,Chunhua Peng,Han Liu,Songcheng Ren,Xuezhu Wang,Yunheng Shen,Yi Wang,Guyue Liu,Hui Chen,Tong Yang,Hailong Yang,Jing Li,Guiguang Ding,Ke Zhang*

Main category: cs.DC

TL;DR: xLLM是一个用于大规模企业级部署的智能高效大语言模型（LLM）推理框架，通过解耦的服务-引擎架构、智能调度、工作负载自适应策略（如Prefill-Decode和Encode-Prefill-Decode解耦）、全局KV缓存管理、多层执行流水线优化、自适应图模式、xTensor内存管理以及优化的投机解码等技术，显著提高了吞吐量和推理效率，在Qwen和Deepseek模型上的表现优于MindIE和vLLM-Ascend。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）推理框架在应对高吞吐量、大规模企业级服务以及多模态请求时面临效率和资源利用率方面的挑战。

Method: xLLM采用新颖的解耦服务-引擎架构。服务层（xLLM-Service）包含智能调度模块，可处理多模态请求，并通过统一弹性调度整合在线和离线任务以最大化集群利用率。它还采用工作负载自适应动态Prefill-Decode（PD）解耦策略和新的Encode-Prefill-Decode（EPD）解耦策略（用于多模态输入），并支持分布式架构以实现全局KV缓存管理和高可用性的容错能力。引擎层（xLLM-Engine）通过多层执行流水线优化、自适应图模式和xTensor内存管理来优化系统和算法设计，同时集成优化的投机解码和动态EPLB以提高吞吐量和推理效率。

Result: 在相同的TPOT约束下，xLLM在使用Qwen系列模型时，吞吐量最高可达MindIE的1.7倍，vLLM-Ascend的2.2倍；在使用Deepseek系列模型时，平均吞吐量为MindIE的1.7倍。

Conclusion: xLLM通过其创新的架构和优化技术，在LLM推理方面实现了显著的性能和资源效率提升，能够满足大规模企业级服务的需求。

Abstract: We introduce xLLM, an intelligent and efficient Large Language Model (LLM)
inference framework designed for high-performance, large-scale enterprise-grade
serving, with deep optimizations for diverse AI accelerators. To address these
challenges, xLLM builds a novel decoupled service-engine architecture. At the
service layer, xLLM-Service features an intelligent scheduling module that
efficiently processes multimodal requests and co-locates online and offline
tasks through unified elastic scheduling to maximize cluster utilization. This
module also relies on a workload-adaptive dynamic Prefill-Decode (PD)
disaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation
policy designed for multimodal inputs. Furthermore, it incorporates a
distributed architecture to provide global KV Cache management and robust
fault-tolerant capabilities for high availability. At the engine layer,
xLLM-Engine co-optimizes system and algorithm designs to fully saturate
computing resources. This is achieved through comprehensive multi-layer
execution pipeline optimizations, an adaptive graph mode and an xTensor memory
management. xLLM-Engine also further integrates algorithmic enhancements such
as optimized speculative decoding and dynamic EPLB, collectively serving to
substantially boost throughput and inference efficiency. Extensive evaluations
demonstrate that xLLM delivers significantly superior performance and resource
efficiency. Under identical TPOT constraints, xLLM achieves throughput up to
1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while
maintaining an average throughput of 1.7x that of MindIE with Deepseek-series
models. xLLM framework is publicly available at
https://github.com/jd-opensource/xllm and
https://github.com/jd-opensource/xllm-service.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [548] [A Levelset Algorithm for 3D-Tarksi](https://arxiv.org/abs/2510.14777)
*Sebastian Haslebacher,Jonas Lill*

Main category: cs.DS

TL;DR: 一个用于寻找单调函数F : [N]^3 -> [N]^3 的Tarski不动点的新算法，其运行时间和查询复杂度均为O(log^2 N)，达到了已知的下界。


<details>
  <summary>Details</summary>
Motivation: 寻找单调函数F : [N]^3 -> [N]^3 的Tarski不动点。

Method: 提出了一种新的简单算法，其运行时间和查询复杂度均为O(log^2 N)。

Result: 该算法的运行时间和查询复杂度均达到O(log^2 N)，与Etessami等人提出的Ω(log^2 N)查询下界以及Fearnley等人提出的现有最佳算法相匹配。

Conclusion: 该算法在寻找Tarski不动点方面达到了理论最优的复杂度。

Abstract: We present a simple new algorithm for finding a Tarski fixed point of a
monotone function $F : [N]^3 \rightarrow [N]^3$. Our algorithm runs in
$O(\log^2 N)$ time and makes $O(\log^2 N)$ queries to $F$, matching the
$\Omega(\log^2 N)$ query lower bound due to Etessami et al.\ as well as the
existing state-of-the-art algorithm due to Fearnley et al.

</details>


### [549] [Prediction-Specific Design of Learning-Augmented Algorithms](https://arxiv.org/abs/2510.14887)
*Sizhe Li,Nicolas Christianson,Tongxin Li*

Main category: cs.DS

TL;DR: 该论文提出了一种名为“强最优”算法的新范式，用于结合在线算法和机器学习预测，以提高预测特定性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于预测的在线算法过于保守，未能充分利用问题结构进行预测特定的性能优化。

Method: 提出了一种通用的双层优化框架，用于设计强最优算法，并为确定性、随机租借和单最大搜索等经典在线问题设计了显式算法。

Result: 通过对动态电源管理和基于波动率的指数交易等问题的实证评估，验证了强最优算法的有效性。

Conclusion: 强最优算法通过利用预测特定设计，能够显著提升在线决策的性能。

Abstract: Algorithms with predictions} has emerged as a powerful framework to combine
the robustness of traditional online algorithms with the data-driven
performance benefits of machine-learned (ML) predictions. However, most
existing approaches in this paradigm are overly conservative, {as they do not
leverage problem structure to optimize performance in a prediction-specific
manner}. In this paper, we show that such prediction-specific performance
criteria can enable significant performance improvements over the coarser
notions of consistency and robustness considered in prior work. Specifically,
we propose a notion of \emph{strongly-optimal} algorithms with predictions,
which obtain Pareto optimality not just in the worst-case tradeoff between
robustness and consistency, but also in the prediction-specific tradeoff
between these metrics. We develop a general bi-level optimization framework
that enables systematically designing strongly-optimal algorithms in a wide
variety of problem settings, and we propose explicit strongly-optimal
algorithms for several classic online problems: deterministic and randomized
ski rental, and one-max search. Our analysis reveals new structural insights
into how predictions can be optimally integrated into online algorithms by
leveraging a prediction-specific design. To validate the benefits of our
proposed framework, we empirically evaluate our algorithms in case studies on
problems including dynamic power management and volatility-based index trading.
Our results demonstrate that prediction-specific, strongly-optimal algorithms
can significantly improve performance across a variety of online
decision-making settings.

</details>


### [550] [Tree-Like Shortcuttings of Trees](https://arxiv.org/abs/2510.14918)
*Hung Le,Lazar Milenković,Shay Solomon,Cuong Than*

Main category: cs.DS

TL;DR: 该论文研究了具有常量跳数（constant-hop）的稀疏“类树”（tree-like）短路图（shortcutting）结构。


<details>
  <summary>Details</summary>
Motivation: 现有常量跳数短路图虽稀疏但包含密集子图，限制了其应用。本文旨在研究“类树”的常量跳数短路图。

Method: 本文关注两个衡量图与树的偏差的参数：树度（arboricity）和树宽度（treewidth）。

Result: 论文提出了类树短路图的新的上下界，包括在树宽度和跳数直径之间一个最优的权衡关系，以及对更大跳数的下界，解决了关于跳数直径和树宽度的开放性问题。

Conclusion: 论文对类树短路图进行了系统性研究，并取得了关于树宽度和跳数直径之间权衡关系的理论突破。

Abstract: Sparse shortcuttings of trees -- equivalently, sparse 1-spanners for tree
metrics with bounded hop-diameter -- have been studied extensively (under
different names and settings), since the pioneering works of [Yao82, Cha87,
AS87, BTS94], initially motivated by applications to range queries, online tree
product, and MST verification, to name a few. These constructions were also
lifted from trees to other graph families using known low-distortion embedding
results. The works of [Yao82, Cha87, AS87, BTS94] establish a tight tradeoff
between hop-diameter and sparsity (or average degree) for tree shortcuttings
and imply constant-hop shortcuttings for $n$-node trees with sparsity $O(\log^*
n)$. Despite their small sparsity, all known constant-hop shortcuttings contain
dense subgraphs (of sparsity $\Omega(\log n)$), which is a significant drawback
for many applications.
  We initiate a systematic study of constant-hop tree shortcuttings that are
``tree-like''. We focus on two well-studied graph parameters that measure how
far a graph is from a tree: arboricity and treewidth. Our contribution is
twofold.
  * New upper and lower bounds for tree-like shortcuttings of trees, including
an optimal tradeoff between hop-diameter and treewidth for all hop-diameter up
to $O(\log\log n)$. We also provide a lower bound for larger values of $k$,
which together yield $\text{hop-diameter}\times \text{treewidth} =
\Omega((\log\log n)^2)$ for all values of hop-diameter, resolving an open
question of [FL22, Le23]. [...]

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [551] [Impact of irradiation conditions on the magnetic field sensitivity of spin defects in hBN nano flakes](https://arxiv.org/abs/2510.13991)
*Saksham Mahajan,Ravi Kumar,Aferdita Xhameni,Gautham Venu,Basanta Mistri,Felix Donaldson,T. Taniguchi,K. Watanabe,Siddharth Dhomkar,Antonio Lombardo,John J. L. Morton*

Main category: physics.app-ph

TL;DR: 研究了hBN纳米片中由氦聚焦离子束(FIB)辐照产生的V_B^-中心，以研究注入条件对V_B^-量子传感器磁场灵敏度关键参数的影响。


<details>
  <summary>Details</summary>
Motivation: 研究V_B^-中心在hBN纳米片中的性能，重点关注辐照条件对其磁场灵敏度的影响。

Method: 结合使用光致发光、光探测磁共振和拉曼光谱，研究V_B^-浓度与自旋相干性和晶格质量之间的权衡，以及在高离子注量下的影响。

Result: 发现在高达10^14 ions/cm^2的离子注量下，V_B^-自旋特性和hBN晶格参数得以保留，超过该值后两者均显著退化。在最优注入剂量下，交流磁灵敏度达到约1μT/√Hz。FIB的图案化注入显示，V_B^-中心和相关晶格损伤局限在注入区域。

Conclusion: 通过仔细选择制备参数，可以优化hBN中V_B^-中心的性能，支持其作为基于二维材料的量子传感器的应用。

Abstract: We study $V_{\mathrm{B}}^-$ centres generated by helium focused ion beam
(FIB) irradiation in thin ($\sim$70 nm) hBN nanoflakes, in order to investigate
the effect of implantation conditions on the key parameters that influence the
magnetic field sensitivity of $V_{\mathrm{B}}^-$ quantum sensors. Using a
combination of photoluminescence, optically detected magnetic resonance, and
Raman spectroscopy, we examine the competing factors of maximising signal
intensity through larger $V_{\mathrm{B}}^-$ concentration against the
degradation in spin coherence and lattice quality observed at high ion
fluences. Our results indicate that both the $V_{\mathrm{B}}^-$ spin properties
and hBN lattice parameters are largely preserved up to an ion fluence of
$10^{14}$ ions/cm$^2$, and beyond this significant degradation occurs in both.
At the optimal implantation dose, an AC magnetic sensitivity of $\sim
1\,\mu\mathrm{T}/\sqrt{\mathrm{Hz}}$ is achieved. Using the patterned
implantation enabled by the FIB, we find that $V_{\mathrm{B}}^-$ centres and
the associated lattice damage are well localised to the implanted regions. This
work demonstrates how careful selection of fabrication parameters can be used
to optimise the properties of $V_{\mathrm{B}}^-$ centres in hBN, supporting
their application as quantum sensors based on 2D materials.

</details>


### [552] [A Novel Beam Tracking Approach for Preventing Beam Collapse](https://arxiv.org/abs/2510.14350)
*Xiaocun Zong,Fan Yang,Shenheng Xu,Maokun Li*

Main category: physics.app-ph

TL;DR: 提出一种基于行逐行切换码表的波束跟踪新方法，通过引入中间状态转换，实现了无波束坍塌的波束跟踪，且增益损失不明显。


<details>
  <summary>Details</summary>
Motivation: 解决开关切换瞬时不稳定性导致的波束坍塌问题。

Method: 提出一种基于行逐行切换码表的波束切换新方法，并与传统方法进行仿真比较，同时搭建RIS硬件平台进行实验验证。

Result: 仿真和实验结果均表明，所提方法能够实现无波束坍塌的波束跟踪，且增益损失不明显。

Conclusion: 所提方法在移动通信和雷达探测等应用中具有良好的适用性和潜力。

Abstract: To address the issue of beam collapse resulting from instantaneous
instability during switch transitions in beam tracking, this paper proposes a
novel beam switching method based on a row-by-row switching code table. The
paper first establishes an abstract model of the beam tracking application
scenario and introduces the reconfigurable intelligent surface (RIS) employed
in this paper. Subsequently, simulations are conducted to compare the
conventional direct beam switching method with the proposed row-by-row
switching code table approach, thereby elucidating the advantages and
limitations of the new method. In parallel, a RIS hardware platform is
constructed in a microwave anechoic chamber for experimental validation. Both
simulation and experimental results show that, by incorporating intermediate
state transitions, the approach achieves beam tracking without beam collapse
while incurring no significant gain loss. Finally, the paper discusses the
applicability scope and potential scenarios for the proposed method. This
research provides valuable insights for applications in mobile communications
and radar detection.

</details>


### [553] [A combined thermal-resistance-capacity and finite-element model for very fast and accurate short- and medium-term simulations of single U-tube borehole heat exchangers](https://arxiv.org/abs/2510.14421)
*Enzo Zanchini,Francesco Zanchini,Claudia Naldi*

Main category: physics.app-ph

TL;DR: 本文提出了一种结合TRCM和数据集插值的新方法，用于模拟单U型BHE的短期和中期热响应，兼具计算速度和仿真精度。


<details>
  <summary>Details</summary>
Motivation: 准确设计地埋耦合热泵系统需要了解BHE的出口流体温度（短期和长期）。本文关注短期和中期。

Method: 提出一种新方法，使用TRCM估算BHE热响应，然后通过插值一个由54个有限元模拟组成的数据集来修正结果。

Result: 该模型在C++中实现，可在两秒内提供入口、出口和平均流体温度，以及平均BHE表面温度、3D和有效钻孔热阻的时间演变。

Conclusion: 该方法结合了TRCM的速度和有限元模拟的精度，能够快速准确地模拟BHE的短期和中期热响应，并可与长期模拟工具连接，实现全时间尺度的模拟。

Abstract: An accurate design of a ground-coupled heat pump system requires the
knowledge of the outlet fluid temperature from the borehole heat exchangers
(BHEs), both in the short and long term. This paper fucuses on the short and
medium term. In this time range, either 3D finite-element simulations or
Thermal Resistance Capacity Models (TRCMs) can be applied. The former can yield
very accurate results but require long computation times. The latter are much
faster but cannot be fully precise, because they require simplifying
assumptions. In this paper, we present a new method for the short-term and
medium-term simulation of single U-tube BHEs, which combines the speed of TRCMs
and the accuracy of finite-elements simulations. The method uses a TRCM to
estimate the thermal response of the BHE, then corrects the results by
interpolation with a dataset, which has been produced by running 54
finite-element simulations in various configurations. The model is implemented
in a C++ program, available at the open-source online data repository of the
University of Bologna. The program provides, within two seconds, the time
evolution of the inlet, outlet and mean fluid temperature, of the mean BHE
surface temperature, of the 3D and the effective borehole thermal resistance.
It can be easily connected to long-term simulation tools to obtain the
full-time-scale thermal response of a bore field.

</details>


### [554] [Layered Bimetal Nanoporous Platforms for SERS Sensing](https://arxiv.org/abs/2510.14706)
*Yanzhou Zou,Anastasiia Sapunova,Tommaso Giovannini,Chen Wang,Huaizhou Jin,Vincenzo Caligiuri,Andrea Schirato,Luca Bursi,Alessandro Alabastri,Shukun Weng,Ali Douaki,German Lanzavecchia,Ivan Marri,Roman Krahne,Nicolò Maccaferri,Zhenrong Zheng,Shangzhong Jin,Denis Garoli*

Main category: physics.app-ph

TL;DR: 该研究首次详细研究了金、银、铜等双金属纳米多孔材料在等离振子学领域的应用，并探索了不同金属组合的相互作用及其在生物分子检测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 等离振子学研究通常集中于单一金属纳米多孔材料，而对合金成分的影响关注较少，双金属体系的研究才刚刚起步。本研究旨在探索双金属纳米多孔平台，利用不同金属的等离振子特性和相互作用，以期在生物分子检测等领域取得突破。

Method: 采用干法合成技术，结合形貌分析、数值模拟和光学光谱学方法，对制备的金、银、铜双金属纳米多孔薄膜进行了详细研究。

Result: 首次报道了金、银、铜等双金属纳米多孔平台的制备及其详细研究，该平台可通过干法合成技术方便可控地制备，为后续等离振子学和生物分子检测等领域的研究提供了新的材料基础。

Conclusion: 双金属纳米多孔材料在等离振子学领域具有广阔的应用前景，尤其在生物分子检测方面。本研究为未来开发基于双金属纳米多孔材料的新型传感和检测器件奠定了基础。

Abstract: Nanoporous metals are extensively investigated as platforms for applications
in plasmonics. They present high surface areas and strong local electric fields
that can be tuned at different energies, playing with the choice of the metals
and the morphology of the porous layers. Until recently, research in the field
of plasmonics has primarily focused on porous metals composed of a single
element, with limited attention given to the impact of alloy composition. The
investigation of bi-metallic systems has only just begun to emerge in the
literature. In particular, combining two or more different plasmonic metals, it
could be possible to explore the interactions between two metals excited at
specific energies. This involves plasmonic coupling, electron transfer, band
hybridization at the interface, electromagnetic field interactions, and
possibly thermal and electronic energy transfer depending on separation, size,
and materials involved. The analysis of bi-metal systems can also be
interesting in biomolecule detection, such as in the case of Surface Enhanced
Raman Scattering (SERS). Here we report, for the first time, a detailed study
(comprising morphological analyses, numerical modelling, and optical
spectroscopies) on bi-metal nanoporous platforms prepared with a dry-synthesis
method enabling the easy and controllable fabrication of bilayers combining
different metals such as Au, Ag, and Cu.

</details>


### [555] [Unidirectional Zero Reflection and Perfect Absorption via Exceptional Points in Active Piezoelectric Willis Metamaterials](https://arxiv.org/abs/2510.14852)
*Hrishikesh Danawe,Serife Tol*

Main category: physics.app-ph

TL;DR: 本论文提出了一种利用压电超材料和电路耦合来实现弹性波单向传输、吸收和反射的器件。


<details>
  <summary>Details</summary>
Motivation: 为了实现弹性波在固体介质中的可调控定向传输，并探索用于声学隔离、波计算、传感和能量操纵的新途径。

Method: 通过设计包含电阻、电感和电压反馈的串联电路的压电超材料，利用电-动量耦合和 Willis 耦合效应，结合非厄米奇异点，实现了对频率依赖的刚度和阻尼的动态控制。

Result: 实现了对弹性波的单向零反射（UZR）和单向完美吸收（UPA），即从一个方向完美吸收，而从相反方向完全反射。

Conclusion: 该器件为利用无源-有源混合超材料实现可调谐、定向弹性波控制提供了一个紧凑且可重构的平台。

Abstract: Electro-momentum coupling in piezoelectric metamaterials with broken
inversion symmetry enables asymmetric elastic wave transport by linking
macroscopic electric fields to momentum, an effect analogous to Willis coupling
in elastic media. A one-dimensional layered piezoelectric metamaterial
integrated with shunt circuits, consisting of a resistor, inductor, and
strain-proportional voltage feedback gain, is proposed to achieve dynamic
control of frequency-dependent stiffness and damping through electromechanical
interactions. Tuning the circuit parameters yields direction-dependent wave
scattering at targeted frequencies. Dynamic homogenization reveals macroscopic
constitutive relations exhibiting both Willis and electro-momentum couplings.
Non-Hermitian exceptional points are identified, where scattering eigenmodes
coalesce and produce extreme asymmetries in wave response. Near these points,
the system realizes unidirectional zero reflection (UZR) and unidirectional
perfect absorption (UPA), achieving complete absorption from one direction and
total reflection from the opposite side. The findings demonstrate a compact and
reconfigurable platform for tunable, directional elastic wave control using
passive-active hybrid metamaterials, opening new avenues for programmable
devices in acoustic isolation, wave-based computing, sensing, and energy
manipulation in solid media.

</details>
