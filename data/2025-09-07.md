<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 70]
- [cs.CL](#cs.CL) [Total: 50]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 11]
- [cs.AI](#cs.AI) [Total: 39]
- [cs.LG](#cs.LG) [Total: 64]
- [eess.SP](#eess.SP) [Total: 9]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 6]
- [cs.NE](#cs.NE) [Total: 1]
- [quant-ph](#quant-ph) [Total: 45]
- [cs.RO](#cs.RO) [Total: 20]
- [cs.LO](#cs.LO) [Total: 3]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 35]
- [cs.GT](#cs.GT) [Total: 1]
- [eess.SY](#eess.SY) [Total: 19]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Efficient General Feature Prediction in Masked Skeleton Modeling](https://arxiv.org/abs/2509.03609)
*Shengkai Sun,Zefan Zhang,Jianfeng Dong,Zhiyong Cheng,Xiaojun Chang,Meng Wang*

Main category: cs.CV

TL;DR: 提出了GFP框架，通过预测高层语义特征而非原始坐标来提升自监督骨骼动作识别的效率和表示能力。


<details>
  <summary>Details</summary>
Motivation: 现有MAE方法在骨骼动作识别中，仅限于重建原始坐标，导致计算冗余且语义表示不足。

Method: 提出GFP框架，用高层特征预测取代低层重建。通过轻量级目标生成网络动态产生多样的时空层级监督信号，并结合约束优化防止模型崩溃。

Result: 在NTU RGB+D 60、NTU RGB+D 120和PKU-MMD数据集上，GFP框架实现了6.2倍的训练加速，并在下游任务中取得了最先进的性能。

Conclusion: GFP框架通过预测高层语义特征，显著提高了骨骼动作识别的计算效率和表示质量。

Abstract: Recent advances in the masked autoencoder (MAE) paradigm have significantly
propelled self-supervised skeleton-based action recognition. However, most
existing approaches limit reconstruction targets to raw joint coordinates or
their simple variants, resulting in computational redundancy and limited
semantic representation. To address this, we propose a novel General Feature
Prediction framework (GFP) for efficient mask skeleton modeling. Our key
innovation is replacing conventional low-level reconstruction with high-level
feature prediction that spans from local motion patterns to global semantic
representations. Specifically, we introduce a collaborative learning framework
where a lightweight target generation network dynamically produces diversified
supervision signals across spatial-temporal hierarchies, avoiding reliance on
pre-computed offline features. The framework incorporates constrained
optimization to ensure feature diversity while preventing model collapse.
Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits
of our approach: Computational efficiency (with 6.2$\times$ faster training
than standard masked skeleton modeling methods) and superior representation
quality, achieving state-of-the-art performance in various downstream tasks.

</details>


### [2] [Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge](https://arxiv.org/abs/2509.03614)
*Seungho Choe,Xiaoli Qin,Abubakr Shafique,Amanda Dy,Susan Done,Dimitrios Androutsos,April Khademi*

Main category: cs.CV

TL;DR: 该研究提出了一种结合像素级分割和教师-学生模型的AI方法，用于自动检测和分类有丝分裂图像，以解决病理学中计数耗时和领域转移问题。


<details>
  <summary>Details</summary>
Motivation: 病理学家手动计数有丝分裂图形耗时且存在观察者间差异，AI可以提供一致性解决方案，但面临领域转移和数据不平衡的挑战。

Method: 提出了一种基于UNet分割骨干网的教师-学生模型，集成了对比表示学习和领域对抗训练等领域泛化模块，用于有丝分裂检测（Track 1）和非典型有丝分裂分类（Track 2）。该模型能生成像素级伪标签，并采用多尺度CNN分类器进行分类。

Result: 在初步测试集上，有丝分裂检测（Track 1）的F1分数达到0.7660，非典型有丝分裂分类（Track 2）的平衡准确率达到0.8414。

Conclusion: 该研究将基于分割的检测和分类集成到统一框架中，为鲁棒的有丝分裂分析提供了有效方法。

Abstract: Counting mitotic figures is time-intensive for pathologists and leads to
inter-observer variability. Artificial intelligence (AI) promises a solution by
automatically detecting mitotic figures while maintaining decision consistency.
However, AI tools are susceptible to domain shift, where a significant drop in
performance can occur due to differences in the training and testing sets,
including morphological diversity between organs, species, and variations in
staining protocols. Furthermore, the number of mitoses is much less than the
count of normal nuclei, which introduces severely imbalanced data for the
detection task. In this work, we formulate mitosis detection as a pixel-level
segmentation and propose a teacher-student model that simultaneously addresses
mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our
method is based on a UNet segmentation backbone that integrates domain
generalization modules, namely contrastive representation learning and
domain-adversarial training. A teacher-student strategy is employed to generate
pixel-level pseudo-masks not only for annotated mitoses and hard negatives but
also for normal nuclei, thereby enhancing feature discrimination and improving
robustness against domain shift. For the classification task, we introduce a
multi-scale CNN classifier that leverages feature maps from the segmentation
model within a multi-task learning paradigm. On the preliminary test set, the
algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of
0.8414 in Track 2, demonstrating the effectiveness of integrating
segmentation-based detection and classification into a unified framework for
robust mitosis analysis.

</details>


### [3] [Multi Attribute Bias Mitigation via Representation Learning](https://arxiv.org/abs/2509.03616)
*Rajeev Ranjan Dwivedi,Ankur Kumar,Vinod K Kurmi*

Main category: cs.CV

TL;DR: 本研究提出了一种名为 GMBM 的新框架，用于解决图像中存在的多种相互关联的偏见问题，通过两阶段方法，在训练时识别偏见，在测试时消除偏见，并引入了新的偏见评估指标 SBA。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像中普遍存在的多种重叠偏见（如纹理、水印、性别妆容、场景对象搭配等）会损害现代视觉模型的性能、鲁棒性和公平性。单独解决这些偏见是不够的，因为减轻一种偏见可能会导致其他偏见加剧。因此，需要一种能够同时处理多种偏见的方法。

Method: 本研究提出了 GMBM（Generalized Multi Bias Mitigation）框架，这是一个两阶段的框架。第一阶段是 ABIL（Adaptive Bias Integrated Learning），通过训练每个属性的编码器并将其与主骨干网络集成，强制分类器显式识别这些偏见。第二阶段是梯度抑制微调，它从骨干网络的梯度中去除这些偏见方向，最终得到一个忽略所有已学习到的偏见的紧凑型网络。此外，还引入了 SBA（Scaled Bias Amplification）作为一种新的测试时度量，用于衡量偏见放大，即使在子群不平衡和测试训练分布发生变化时也有效。

Result: 在 FB-CMNIST、CelebA 和 COCO 数据集上进行了验证。GMBM 提高了最差群体的准确性，使多属性偏见放大减少一半，并在 SBA 方面取得了新的低点，即使在偏见复杂性和分布偏移加剧的情况下也是如此。

Conclusion: GMBM 是第一个实用的、端到端的多偏见视觉识别解决方案，能够有效解决现实世界图像中的多种偏见问题，并在各种数据集上取得了显著的改进。

Abstract: Real world images frequently exhibit multiple overlapping biases, including
textures, watermarks, gendered makeup, scene object pairings, etc. These biases
collectively impair the performance of modern vision models, undermining both
their robustness and fairness. Addressing these biases individually proves
inadequate, as mitigating one bias often permits or intensifies others. We
tackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a
lean two stage framework that needs group labels only while training and
minimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)
deliberately identifies the influence of known shortcuts by training encoders
for each attribute and integrating them with the main backbone, compelling the
classifier to explicitly recognize these biases. Then Gradient Suppression Fine
Tuning prunes those very bias directions from the backbone's gradients, leaving
a single compact network that ignores all the shortcuts it just learned to
recognize. Moreover we find that existing bias metrics break under subgroup
imbalance and train test distribution shifts, so we introduce Scaled Bias
Amplification (SBA): a test time measure that disentangles model induced bias
amplification from distributional differences. We validate GMBM on FB CMNIST,
CelebA, and COCO, where we boost worst group accuracy, halve multi attribute
bias amplification, and set a new low in SBA even as bias complexity and
distribution shifts intensify, making GMBM the first practical, end to end
multibias solution for visual recognition. Project page:
http://visdomlab.github.io/GMBM/

</details>


### [4] [Lightweight image segmentation for echocardiography](https://arxiv.org/abs/2509.03631)
*Anders Kjelsrud,Lasse Løvstakken,Erik Smistad,Håvard Dalen,Gilles Van De Vyver*

Main category: cs.CV

TL;DR: 通过消融研究识别并简化了 nnU-Net 在心脏分割中的有效组件，开发了一个更小、更快的轻量级 U-Net 模型，其性能与 nnU-Net 相当，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有的 nnU-Net 模型在心脏分割方面表现良好，但模型庞大且运行缓慢，限制了其实时应用。因此需要开发一个更轻量级的模型。

Method: 通过消融研究，逐步评估了数据增强方案、架构修改、损失函数和后处理技术，以确定 nnU-Net 中对心脏分割最有效的组件。在此基础上，开发了一个轻量级的 U-Net 模型。

Result: 所提出的轻量级 U-Net 模型（200 万参数）在 CAMUS 数据集上实现了与 nnU-Net（3300 万参数）统计上相当的性能（Dice 分数分别为 0.93/0.85/0.89 vs 0.93/0.86/0.89），但模型尺寸缩小了 16 倍，速度提高了 4 倍（每帧 1.35 毫秒 vs 5.40 毫秒）。在内部数据集上的交叉数据集评估也证实了其可比的泛化能力。

Conclusion: 通过识别关键组件和优化模型结构，可以开发出既轻量又高效的心脏分割模型，从而实现实时应用。

Abstract: Accurate segmentation of the left ventricle in echocardiography can enable
fully automatic extraction of clinical measurements such as volumes and
ejection fraction. While models configured by nnU-Net perform well, they are
large and slow, thus limiting real-time use. We identified the most effective
components of nnU-Net for cardiac segmentation through an ablation study,
incrementally evaluating data augmentation schemes, architectural
modifications, loss functions, and post-processing techniques. Our analysis
revealed that simple affine augmentations and deep supervision drive
performance, while complex augmentations and large model capacity offer
diminishing returns. Based on these insights, we developed a lightweight U-Net
(2M vs 33M parameters) that achieves statistically equivalent performance to
nnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89
for LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster
(1.35ms vs 5.40ms per frame) than the default nnU-Net configuration.
Cross-dataset evaluation on an internal dataset (N=311) confirms comparable
generalization.

</details>


### [5] [treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds](https://arxiv.org/abs/2509.03633)
*Josafat-Mattias Burmeister,Andreas Tockner,Stefan Reder,Markus Engel,Rico Richter,Jan-Peter Mund,Jürgen Döllner*

Main category: cs.CV

TL;DR: 本研究提出了一种改进的、计算资源需求低的无监督树木提取算法treeX，并提供了两种参数配置，分别适用于地面和无人机激光扫描数据。该算法在多个公开数据集上进行了测试，并与现有方法进行了比较，结果显示其在提高效率和精度的同时，保持了与先进方法相当的性能。该算法可作为深度学习方法的替代方案，或用于辅助生成深度学习模型的标签。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的树木分割方法需要大量标注数据和计算资源，而无监督方法是更具资源效益的替代方案。本研究旨在改进现有的treeX算法，以提高其处理不同类型激光扫描数据的能力和效率。

Method: 提出了一种改进的treeX算法，结合了基于聚类的茎检测和区域生长分割树冠。提供了适用于地面激光扫描（TLS和PLS）和无人机激光扫描（ULS）的参数配置。

Result: 改进后的treeX算法在多个公开数据集上表现良好。与原始treeX算法相比，地面数据的实例检测F1分数提高了0.11至0.49。对于ULS数据，改进算法的F1分数为0.58，而原始算法无法正确分割。在TLS和PLS数据上，算法的准确性与包括深度学习在内的最新开源方法相当。

Conclusion: 改进后的treeX算法是一种资源高效的无监督树木提取方法，适用于地面和无人机激光扫描数据。该方法在效率和准确性方面均有提升，并且可以作为深度学习方法的替代方案或用于辅助数据标注。研究人员提供了开源的Python实现以方便广泛应用。

Abstract: Close-range laser scanning provides detailed 3D captures of forest stands but
requires efficient software for processing 3D point cloud data and extracting
individual trees. Although recent studies have introduced deep learning methods
for tree instance segmentation, these approaches require large annotated
datasets and substantial computational resources. As a resource-efficient
alternative, we present a revised version of the treeX algorithm, an
unsupervised method that combines clustering-based stem detection with region
growing for crown delineation. While the original treeX algorithm was developed
for personal laser scanning (PLS) data, we provide two parameter presets, one
for ground-based laser scanning (stationary terrestrial - TLS and PLS), and one
for UAV-borne laser scanning (ULS). We evaluated the method on six public
datasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham
Woods) and compared it to six open-source methods (original treeX, treeiso,
RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original
treeX algorithm, our revision reduces runtime and improves accuracy, with
instance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.
For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original
algorithm fails to segment any correct instances. For TLS and PLS data, our
algorithm achieves accuracy similar to recent open-source methods, including
deep learning. Given its algorithmic design, we see two main applications for
our method: (1) as a resource-efficient alternative to deep learning approaches
in scenarios where the data characteristics align with the method design
(sufficient stem visibility and point density), and (2) for the semi-automatic
generation of labels for deep learning models. To enable broader adoption, we
provide an open-source Python implementation in the pointtree package.

</details>


### [6] [Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding](https://arxiv.org/abs/2509.03635)
*Hongpei Zheng,Lintao Xiang,Qijun Yang,Qian Lin,Hujun Yin*

Main category: cs.CV

TL;DR: Reg3D通过引入几何感知监督来解决大型多模态模型在3D场景理解方面的挑战，通过对象级和帧级重建任务，提高了3D空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型（LMMs）在2D视觉理解方面取得了显著进展，但将其能力扩展到3D场景理解仍然是一个重大挑战。现有的方法主要依赖于纯文本监督，这未能提供学习鲁棒3D空间表征所需的几何约束。

Method: Reg3D提出了一种新颖的重建几何指令调整（Reg3D）框架，通过在训练过程中直接纳入几何感知监督来解决这一限制。其关键在于，有效的3D理解需要重建潜在的几何结构，而不仅仅是描述它们。Reg3D采用双重监督范式，将3D几何信息同时用作输入和显式学习目标。具体来说，在双编码器架构中设计了互补的对象级和帧级重建任务，强制执行几何一致性，以鼓励空间推理能力的培养。

Result: 在ScanQA、Scan2Cap、ScanRefer和SQA3D上的大量实验表明，Reg3D在性能上取得了显著的提高，为空间感知多模态模型建立了一个新的训练范例。

Conclusion: Reg3D通过结合几何信息作为输入和学习目标，有效地解决了LMMs在3D场景理解中的局限性，为开发更强大的空间推理模型提供了新的途径。

Abstract: The rapid development of Large Multimodal Models (LMMs) has led to remarkable
progress in 2D visual understanding; however, extending these capabilities to
3D scene understanding remains a significant challenge. Existing approaches
predominantly rely on text-only supervision, which fails to provide the
geometric constraints required for learning robust 3D spatial representations.
In this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction
Tuning framework that addresses this limitation by incorporating geometry-aware
supervision directly into the training process. Our key insight is that
effective 3D understanding necessitates reconstructing underlying geometric
structures rather than merely describing them. Unlike existing methods that
inject 3D information solely at the input level, Reg3D adopts a
dual-supervision paradigm that leverages 3D geometric information both as input
and as explicit learning targets. Specifically, we design complementary
object-level and frame-level reconstruction tasks within a dual-encoder
architecture, enforcing geometric consistency to encourage the development of
spatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,
ScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance
improvements, establishing a new training paradigm for spatially aware
multimodal models.

</details>


### [7] [QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception](https://arxiv.org/abs/2509.03704)
*Seth Z. Zhao,Huizhi Zhang,Zhaowei Li,Juntong Peng,Anthony Chui,Zewei Zhou,Zonglin Meng,Hao Xiang,Zhiyu Huang,Fujia Wang,Ran Tian,Chenfeng Xu,Bolei Zhou,Jiaqi Ma*

Main category: cs.CV

TL;DR: QuantV2X是首个完全量化的多智能体系统，用于V2X合作感知，通过量化神经网络和传输消息来提高效率和可扩展性，同时保持与全精度系统相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 以往关于V2X合作感知的研究主要关注准确性，而忽略了效率、延迟和实际部署等系统级问题。现有系统通常使用全精度模型，导致高计算和传输成本，不适用于资源受限环境。

Method: 提出QuantV2X，采用统一的端到端量化策略，对神经网络模型和传输消息表示进行量化，以降低计算量和传输带宽。

Result: QuantV2X在低比特约束下实现了与全精度系统相当的准确性。在面向部署的指标下，QuantV2X将系统延迟降低了3.2倍，mAP30提升了9.5%。此外，QuantV2X的扩展性更好，能够将更大、更强的模型适应严格的内存限制。

Conclusion: 全量化的多智能体中间融合系统对于实际部署是可行的。

Abstract: Cooperative perception through Vehicle-to-Everything (V2X) communication
offers significant potential for enhancing vehicle perception by mitigating
occlusions and expanding the field of view. However, past research has
predominantly focused on improving accuracy metrics without addressing the
crucial system-level considerations of efficiency, latency, and real-world
deployability. Noticeably, most existing systems rely on full-precision models,
which incur high computational and transmission costs, making them impractical
for real-time operation in resource-constrained environments. In this paper, we
introduce \textbf{QuantV2X}, the first fully quantized multi-agent system
designed specifically for efficient and scalable deployment of multi-modal,
multi-agent V2X cooperative perception. QuantV2X introduces a unified
end-to-end quantization strategy across both neural network models and
transmitted message representations that simultaneously reduces computational
load and transmission bandwidth. Remarkably, despite operating under low-bit
constraints, QuantV2X achieves accuracy comparable to full-precision systems.
More importantly, when evaluated under deployment-oriented metrics, QuantV2X
reduces system-level latency by 3.2$\times$ and achieves a +9.5 improvement in
mAP30 over full-precision baselines. Furthermore, QuantV2X scales more
effectively, enabling larger and more capable models to fit within strict
memory budgets. These results highlight the viability of a fully quantized
multi-agent intermediate fusion system for real-world deployment. The system
will be publicly released to promote research in this field:
https://github.com/ucla-mobility/QuantV2X.

</details>


### [8] [Transfer Learning-Based CNN Models for Plant Species Identification Using Leaf Venation Patterns](https://arxiv.org/abs/2509.03729)
*Bandita Bharadwaj,Ankur Mishra,Saurav Bharadwaj*

Main category: cs.CV

TL;DR: 本研究评估了三种深度学习模型（ResNet50、MobileNetV2 和 EfficientNetB0）在基于叶脉图案进行植物物种自动分类方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 利用叶脉图案这一关键的形态学特征，结合深度学习技术，开发用于植物物种自动分类的工具。

Method: 使用瑞典树叶数据集（包含 15 个不同物种的图像），通过训练和测试阶段的标准性能指标评估了 ResNet50、MobileNetV2 和 EfficientNetB0 的性能。

Result: EfficientNetB0 在测试准确率（94.67%）和 F1 分数（超过 94.6%）方面表现最佳，MobileNetV2 表现良好（测试准确率 93.34%），而 ResNet50 则出现过拟合（测试准确率 88.45%）。

Conclusion: 深度学习模型，特别是 EfficientNetB0，在利用叶脉特征进行可扩展且准确的自动植物分类方面具有巨大潜力。

Abstract: This study evaluates the efficacy of three deep learning architectures:
ResNet50, MobileNetV2, and EfficientNetB0 for automated plant species
classification based on leaf venation patterns, a critical morphological
feature with high taxonomic relevance. Using the Swedish Leaf Dataset
comprising images from 15 distinct species (75 images per species, totalling
1,125 images), the models were demonstrated using standard performance metrics
during training and testing phases. ResNet50 achieved a training accuracy of
94.11% but exhibited overfitting, reflected by a reduced testing accuracy of
88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better
generalization capabilities, attaining a testing accuracy of 93.34% and an F1
score of 93.23%, indicating its suitability for lightweight, real-time
applications. EfficientNetB0 outperformed both models, achieving a testing
accuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,
highlighting its robustness in venation-based classification. The findings
underscore the potential of deep learning, particularly EfficientNetB0, in
developing scalable and accurate tools for automated plant taxonomy using
venation traits.

</details>


### [9] [LayoutGKN: Graph Similarity Learning of Floor Plans](https://arxiv.org/abs/2509.03737)
*Casper van Engelenburg,Jan van Gemert,Seyran Khademi*

Main category: cs.CV

TL;DR: LayoutGKN是一种更高效的楼层平面图比较方法，通过延迟节点级交互并使用可微图核作为距离函数，在保证相似性比较效果的同时显著提高了推理速度。


<details>
  <summary>Details</summary>
Motivation: 在搜索、聚类和数据可视化等应用中，比较楼层平面图（通常表示为图）至关重要。现有的图匹配网络方法在推理时速度较慢，因为它们依赖于昂贵的跨图节点级交互。

Method: LayoutGKN通过使用可微图核作为最终学习到的节点级嵌入的距离函数，将跨图节点级交互推迟到联合嵌入体系结构的末端。

Result: LayoutGKN的相似性比较效果与图匹配网络相当或更优，同时显著提高了推理速度。

Conclusion: LayoutGKN在效率和准确性方面都优于现有的图匹配网络，为楼层平面图比较提供了一种更快的解决方案。

Abstract: Floor plans depict building layouts and are often represented as graphs to
capture the underlying spatial relationships. Comparison of these graphs is
critical for applications like search, clustering, and data visualization. The
most successful methods to compare graphs \ie, graph matching networks, rely on
costly intermediate cross-graph node-level interactions, therefore being slow
in inference time. We introduce \textbf{LayoutGKN}, a more efficient approach
that postpones the cross-graph node-level interactions to the end of the joint
embedding architecture. We do so by using a differentiable graph kernel as a
distance function on the final learned node-level embeddings. We show that
LayoutGKN computes similarity comparably or better than graph matching networks
while significantly increasing the speed.
\href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are
open.

</details>


### [10] [Singular Value Few-shot Adaptation of Vision-Language Models](https://arxiv.org/abs/2509.03740)
*Taha Koleilat,Hassan Rivaz,Yiming Xiao*

Main category: cs.CV

TL;DR: CLIP-SVD是一种参数高效的多模态学习方法，通过修改CLIP模型的奇异值来适应新领域，仅用0.04%的参数即可达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（VLMs）在迁移到新的细粒度领域时面临挑战，因为它们依赖于提示工程并且全模型微调成本高昂，而现有方法可能会损害模型的泛化能力。

Method: CLIP-SVD利用奇异值分解（SVD）来修改CLIP模型的内部参数空间，仅微调模型参数矩阵的奇异值，以在不注入额外模块的情况下进行领域自适应，从而保留预训练模型。

Result: CLIP-SVD在11个自然数据集和10个生物医学数据集上取得了最先进的分类结果，在少样本设置下，其准确性和泛化能力均优于先前方法。

Conclusion: CLIP-SVD是一种有效且参数高效的VLM自适应技术，它通过修改模型的内部参数（奇异值）来适应新领域，提高了性能并保持了模型的泛化能力。

Abstract: Vision-language models (VLMs) like CLIP have shown impressive zero-shot and
few-shot learning capabilities across diverse applications. However, adapting
these models to new fine-grained domains remains difficult due to reliance on
prompt engineering and the high cost of full model fine-tuning. Existing
adaptation approaches rely on augmented components, such as prompt tokens and
adapter modules, which could limit adaptation quality, destabilize the model,
and compromise the rich knowledge learned during pretraining. In this work, we
present \textbf{CLIP-SVD}, a novel \textit{multi-modal} and
\textit{parameter-efficient} adaptation technique that leverages Singular Value
Decomposition (SVD) to modify the internal parameter space of CLIP without
injecting additional modules. Specifically, we fine-tune only the singular
values of the CLIP parameter matrices to rescale the basis vectors for domain
adaptation while retaining the pretrained model. This design enables enhanced
adaptation performance using only \textbf{0.04\%} of the model's total
parameters and better preservation of its generalization ability. CLIP-SVD
achieves state-of-the-art classification results on 11 natural and 10
biomedical datasets, outperforming previous methods in both accuracy and
generalization under few-shot settings. Additionally, we leverage a natural
language-based approach to analyze the effectiveness and dynamics of the CLIP
adaptation to allow interpretability of CLIP-SVD. The code is publicly
available at https://github.com/HealthX-Lab/CLIP-SVD.

</details>


### [11] [STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification](https://arxiv.org/abs/2509.03754)
*Zongsen Qiu*

Main category: cs.CV

TL;DR: 该研究提出了一种用于边缘设备的高精度植物病害诊断模型STA-Net，通过新颖的形状-纹理注意力模块（STAM）和高效的网络骨干（DeepMAD）来解决现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的轻量级网络在处理植物病害诊断时，由于注意力机制通用性不足，难以捕捉病灶的精细特征。因此，需要为边缘设备设计一种能够有效识别病灶细节的高精度模型。

Method: 研究者提出了一个两步解决方案：1.使用无需训练的神经架构搜索方法DeepMAD来为边缘设备构建高效的网络骨干；2.引入形状-纹理注意力模块（STAM），该模块将注意力机制解耦为两个分支，一个分支利用可变形卷积（DCNv4）捕捉形状信息，另一个分支利用Gabor滤波器组捕捉纹理信息。

Result: 在CCMT植物病害数据集上，STA-Net模型（包含401K参数，51.1M FLOPs）达到了89.00%的准确率和88.96%的F1分数。消融实验证明STAM相比基线模型和标准注意力模型显著提升了性能。

Conclusion: 通过解耦注意力机制并融合领域知识（形状和纹理），为在边缘设备上部署精准农业AI提供了有前景的解决方案。

Abstract: Responding to rising global food security needs, precision agriculture and
deep learning-based plant disease diagnosis have become crucial. Yet, deploying
high-precision models on edge devices is challenging. Most lightweight networks
use attention mechanisms designed for generic object recognition, which poorly
capture subtle pathological features like irregular lesion shapes and complex
textures. To overcome this, we propose a twofold solution: first, using a
training-free neural architecture search method (DeepMAD) to create an
efficient network backbone for edge devices; second, introducing the
Shape-Texture Attention Module (STAM). STAM splits attention into two branches
-- one using deformable convolutions (DCNv4) for shape awareness and the other
using a Gabor filter bank for texture awareness. On the public CCMT plant
disease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs)
reached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm
STAM significantly improves performance over baseline and standard attention
models. Integrating domain knowledge via decoupled attention thus presents a
promising path for edge-deployed precision agriculture AI. The source code is
available at https://github.com/RzMY/STA-Net.

</details>


### [12] [SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection](https://arxiv.org/abs/2509.03786)
*Xinxin Wang,Han Sun,Ningzhong Liu,Huiyu Zhou,Yinan Yao*

Main category: cs.CV

TL;DR: 该论文提出了一个名为SLENet的新型水下伪装目标检测（UCOD）框架，解决了水下光学失真、浑浊和海洋生物复杂性带来的挑战。SLENet包含一个语义定位和增强网络，通过引入Gamma-Asymmetric Enhancement（GAE）模块和Localization Guidance Branch（LGB）来增强多尺度特征表示和定位图的全局语义信息，以指导Multi-Scale Supervised Decoder（MSSD）生成更精确的预测。


<details>
  <summary>Details</summary>
Motivation: 水下伪装目标检测（UCOD）在海洋生态学中至关重要，但由于光学失真、水体浑浊和海洋生物的复杂性等因素，该任务仍处于探索阶段，准确识别受到严重阻碍。

Method: 提出了一种名为SLENet的新型框架，包含Gamma-Asymmetric Enhancement（GAE）模块和Localization Guidance Branch（LGB），用于增强多尺度特征表示并生成包含全局语义信息的定位图，以指导Multi-Scale Supervised Decoder（MSSD）进行更精确的预测。同时，构建了一个名为DeepCamo的基准数据集，并在该数据集及其他三个标准COD数据集上进行了实验。

Result: SLENet在DeepCamo数据集和三个基准COD数据集上的实验结果显示，其性能优于现有最先进（SOTA）方法，并证明了其在更广泛的COD任务上的通用性。

Conclusion: SLENet在水下伪装目标检测方面取得了优越的性能，并且具有良好的泛化能力，能够应用于更广泛的伪装目标检测任务。

Abstract: Underwater Camouflaged Object Detection (UCOD) aims to identify objects that
blend seamlessly into underwater environments. This task is critically
important to marine ecology. However, it remains largely underexplored and
accurate identification is severely hindered by optical distortions, water
turbidity, and the complex traits of marine organisms. To address these
challenges, we introduce the UCOD task and present DeepCamo, a benchmark
dataset designed for this domain. We also propose Semantic Localization and
Enhancement Network (SLENet), a novel framework for UCOD. We first benchmark
state-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet
is built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE)
module and a Localization Guidance Branch (LGB) to enhance multi-scale feature
representation while generating a location map enriched with global semantic
information. This map guides the Multi-Scale Supervised Decoder (MSSD) to
produce more accurate predictions. Experiments on our DeepCamo dataset and
three benchmark COD datasets confirm SLENet's superior performance over SOTA
methods, and underscore its high generality for the broader COD task.

</details>


### [13] [Fitting Image Diffusion Models on Video Datasets](https://arxiv.org/abs/2509.03794)
*Juhun Lee,Simon S. Woo*

Main category: cs.CV

TL;DR: 通过利用视频连续帧中的时间归纳偏置来改进扩散模型训练，实现了 yli 2 倍的收敛速度和更低的 FID 分数。


<details>
  <summary>Details</summary>
Motivation: 当前的图像扩散模型在独立采样的静态图像上进行训练，这种方式在捕捉动态视频信息时存在信息缺失的缺点，导致收敛速度慢、分布覆盖范围有限以及泛化能力降低。

Method: 提出了一种利用视频连续帧中存在的时间归纳偏置来改进扩散模型训练的策略，该策略无需修改网络结构，可无缝集成到标准扩散模型训练流程中。

Result: 在 HandCo 数据集上评估，该方法将收敛速度加快了 yli 2 倍，并在训练和验证分布上都取得了更低的 FID 分数，同时通过鼓励模型捕捉有意义的时间变化来提高生成多样性。优化分析表明，该方法通过降低梯度方差来加速收敛。

Conclusion: 所提出的方法是一种简单而有效的训练策略，可以利用视频的时间信息来提高扩散模型的训练效率和生成质量，而无需进行任何架构修改。

Abstract: Image diffusion models are trained on independently sampled static images.
While this is the bedrock task protocol in generative modeling, capturing the
temporal world through the lens of static snapshots is information-deficient by
design. This limitation leads to slower convergence, limited distributional
coverage, and reduced generalization. In this work, we propose a simple and
effective training strategy that leverages the temporal inductive bias present
in continuous video frames to improve diffusion training. Notably, the proposed
method requires no architectural modification and can be seamlessly integrated
into standard diffusion training pipelines. We evaluate our method on the
HandCo dataset, where hand-object interactions exhibit dense temporal coherence
and subtle variations in finger articulation often result in semantically
distinct motions. Empirically, our method accelerates convergence by over
2$\text{x}$ faster and achieves lower FID on both training and validation
distributions. It also improves generative diversity by encouraging the model
to capture meaningful temporal variations. We further provide an optimization
analysis showing that our regularization reduces the gradient variance, which
contributes to faster convergence.

</details>


### [14] [MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting](https://arxiv.org/abs/2509.03800)
*Yuheng Li,Yenho Chen,Yuxiang Lai,Jike Zhong,Vanessa Wildman,Xiaofeng Yang*

Main category: cs.CV

TL;DR: MedVista3D是一个用于3D CT分析的多尺度语义增强视觉-语言预训练框架，可以同时解决局部疾病检测、全局理解和报告语言不一致的问题，并在多项任务上达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D医学影像分析方法在处理局部异常检测、全局上下文理解和报告语言不一致性方面存在不足，尤其是在3D影像分析中，这些问题更加突出。

Method: MedVista3D通过局部和全局的图像-文本对齐来实现细粒度的表示学习，并结合语言模型重写和放射学语义匹配库来处理报告的变异性，从而实现对3D CT影像的联合疾病检测和整体解读。

Result: MedVista3D在零样本疾病分类、报告检索和医学视觉问答任务上取得了最先进的性能，并在器官分割和预后预测任务上表现出良好的迁移能力。

Conclusion: MedVista3D成功地整合了局部-全局的空间推理能力和语义感知的报告处理能力，为3D医学影像分析提供了一个更有效的解决方案。

Abstract: Radiologic diagnostic errors-under-reading errors, inattentional blindness,
and communication failures-remain prevalent in clinical practice. These issues
often stem from missed localized abnormalities, limited global context, and
variability in report language. These challenges are amplified in 3D imaging,
where clinicians must examine hundreds of slices per scan. Addressing them
requires systems with precise localized detection, global volume-level
reasoning, and semantically consistent natural language reporting. However,
existing 3D vision-language models are unable to meet all three needs jointly,
lacking local-global understanding for spatial reasoning and struggling with
the variability and noise of uncurated radiology reports. We present
MedVista3D, a multi-scale semantic-enriched vision-language pretraining
framework for 3D CT analysis. To enable joint disease detection and holistic
interpretation, MedVista3D performs local and global image-text alignment for
fine-grained representation learning within full-volume context. To address
report variability, we apply language model rewrites and introduce a Radiology
Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves
state-of-the-art performance on zero-shot disease classification, report
retrieval, and medical visual question answering, while transferring well to
organ segmentation and prognosis prediction. Code and datasets will be
released.

</details>


### [15] [Causality-guided Prompt Learning for Vision-language Models via Visual Granulation](https://arxiv.org/abs/2509.03803)
*Mengyu Gao,Qiulei Dong*

Main category: cs.CV

TL;DR: Prompt learning for CLIP struggles with fine-grained tasks. CaPL uses visual granulation and causal inference to create better text prompts, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based prompt learning methods have limited ability to handle fine-grained datasets.

Method: CaPL uses a causality-guided text prompt learning method via visual granulation. It has two modules: 1. An attribute disentanglement module using a Brownian Bridge Diffusion Model to separate shared and class-specific visual attributes. 2. A granule learning module to construct visual granules from these attributes using causal inference strategies for recognition.

Result: CaPL significantly outperforms state-of-the-art prompt learning methods, especially on fine-grained datasets, as shown by experiments on 15 datasets.

Conclusion: The proposed visual granules enable the learning of more discriminative text prompts, leading to significant improvements in fine-grained recognition tasks.

Abstract: Prompt learning has recently attracted much attention for adapting
pre-trained vision-language models (e.g., CLIP) to downstream recognition
tasks. However, most of the existing CLIP-based prompt learning methods only
show a limited ability for handling fine-grained datasets. To address this
issue, we propose a causality-guided text prompt learning method via visual
granulation for CLIP, called CaPL, where the explored visual granulation
technique could construct sets of visual granules for the text prompt to
capture subtle discrepancies among different fine-grained classes through
casual inference. The CaPL method contains the following two modules: (1) An
attribute disentanglement module is proposed to decompose visual features into
non-individualized attributes (shared by some classes) and individualized
attributes (specific to single classes) using a Brownian Bridge Diffusion
Model; (2) A granule learning module is proposed to construct visual granules
by integrating the aforementioned attributes for recognition under two causal
inference strategies. Thanks to the learned visual granules, more
discriminative text prompt is expected to be learned. Extensive experimental
results on 15 datasets demonstrate that our CaPL method significantly
outperforms the state-of-the-art prompt learning methods, especially on
fine-grained datasets.

</details>


### [16] [EGTM: Event-guided Efficient Turbulence Mitigation](https://arxiv.org/abs/2509.03808)
*Huanan Li,Rui Fan,Juntao Guan,Weidong Hao,Lai Rui,Tong Wu,Yikai Wang,Lin Gu*

Main category: cs.CV

TL;DR: 通过引入事件相机，提出EGTM框架，有效解决了传统深度学习湍流减湍方法在计算和存储效率上的瓶颈，并在恢复质量上达到SOTA。具体来说，该方法利用事件流的稀疏性和高时间分辨率，结合“事件-幸运洞察”，提取像素级无湍流引导，实现了高效的“幸运融合”，并在真实世界数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的湍流减湍（TM）方法需要高容量网络来学习同步帧之间的粗粒度湍流动态，这在计算和存储效率方面存在不足。事件相机具有微秒级的时间分辨率和稀疏、异步的成像机制，有望从根本上解决这一瓶颈。

Method: 本文提出了一种新颖的EGTM框架，该框架基于“事件-幸运洞察”，揭示了湍流失真与事件流逆时空分布之间的相关性。EGTM框架从显式的、但有噪声的湍流事件中提取像素级可靠的无湍流引导，用于时间幸运融合。此外，研究人员还构建了首个真实世界的事件驱动TM数据集。

Result: EGTM框架在模型尺寸、推理潜力和模型复杂度方面分别比现有SOTA方法提高了710倍、214倍和224倍。同时，在恢复质量方面，EGTM在PSNR上提高了+0.94，在SSIM上提高了+0.08，达到了SOTA水平。

Conclusion: 将事件模式引入TM任务具有显著的效率优势，EGTM框架能够高效地实现湍流减湍，并在恢复质量上取得SOTA性能。

Abstract: Turbulence mitigation (TM) aims to remove the stochastic distortions and
blurs introduced by atmospheric turbulence into frame cameras. Existing
state-of-the-art deep-learning TM methods extract turbulence cues from multiple
degraded frames to find the so-called "lucky'', not distorted patch, for "lucky
fusion''. However, it requires high-capacity network to learn from
coarse-grained turbulence dynamics between synchronous frames with limited
frame-rate, thus fall short in computational and storage efficiency. Event
cameras, with microsecond-level temporal resolution, have the potential to
fundamentally address this bottleneck with efficient sparse and asynchronous
imaging mechanism. In light of this, we (i) present the fundamental
\textbf{``event-lucky insight''} to reveal the correlation between turbulence
distortions and inverse spatiotemporal distribution of event streams. Then,
build upon this insight, we (ii) propose a novel EGTM framework that extracts
pixel-level reliable turbulence-free guidance from the explicit but noisy
turbulent events for temporal lucky fusion. Moreover, we (iii) build the first
turbulence data acquisition system to contribute the first real-world
event-driven TM dataset. Extensive experimental results demonstrate that our
approach significantly surpass the existing SOTA TM method by 710 times, 214
times and 224 times in model size, inference latency and model complexity
respectively, while achieving the state-of-the-art in restoration quality
(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating
the great efficiency merit of introducing event modality into TM task. Demo
code and data have been uploaded in supplementary material and will be released
once accepted.

</details>


### [17] [Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection](https://arxiv.org/abs/2509.03872)
*Nan Yang,Yang Wang,Zhanwen Liu,Yuchao Dai,Yang Liu,Xiangmo Zhao*

Main category: cs.CV

TL;DR: FocusMamba通过事件引导的多模态稀疏化和跨模态焦点融合，在保持准确性的同时提高了RGB-事件检测的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的RGB-事件检测方法在处理低信息区域时计算成本高且性能不佳，并且现有的稀疏化方法无法适应不同复杂度的样本。

Method: 提出了一种事件引导的多模态稀疏化（EGMS）策略，用于自适应地去除低信息区域，以及一个跨模态焦点融合（CMFF）模块来整合互补特征。

Result: 在DSEC-Det和PKU-DAVIS-SOD数据集上，FocusMamba在准确性和效率方面均优于现有方法。

Conclusion: FocusMamba通过自适应稀疏化和有效的多模态融合，在RGB-事件检测任务上取得了更好的性能和效率平衡。

Abstract: Existing RGB-Event detection methods process the low-information regions of
both modalities (background in images and non-event regions in event data)
uniformly during feature extraction and fusion, resulting in high computational
costs and suboptimal performance. To mitigate the computational redundancy
during feature extraction, researchers have respectively proposed token
sparsification methods for the image and event modalities. However, these
methods employ a fixed number or threshold for token selection, hindering the
retention of informative tokens for samples with varying complexity. To achieve
a better balance between accuracy and efficiency, we propose FocusMamba, which
performs adaptive collaborative sparsification of multimodal features and
efficiently integrates complementary information. Specifically, an Event-Guided
Multimodal Sparsification (EGMS) strategy is designed to identify and
adaptively discard low-information regions within each modality by leveraging
scene content changes perceived by the event camera. Based on the
sparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed
to effectively capture and integrate complementary features from both
modalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate
that the proposed method achieves superior performance in both accuracy and
efficiency compared to existing methods. The code will be available at
https://github.com/Zizzzzzzz/FocusMamba.

</details>


### [18] [SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition](https://arxiv.org/abs/2509.03873)
*Jiajun Song,Xiaoou Liu*

Main category: cs.CV

TL;DR: 提出了一种名为SalientFusion的组合零样本食品识别（CZSFR）方法，以应对背景干扰、主食/配菜混淆和语义偏差等挑战，并在新提出的CZSFood-90和CZSFood-164基准上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 食品识别领域需要识别新出现的菜肴，因此需要零样本食品学习（ZSFL），特别是组合零样本食品识别（CZSFR），因为它能很好地匹配菜肴、配料与属性和对象。

Method: 提出SalientFusion方法，包含SalientFormer（去除背景冗余，利用深度特征解决角色混淆）和DebiasAT（通过视觉特征对齐提示来减少语义偏差）。

Result: 在CZSFood-90和CZSFood-164基准以及通用的CZSL数据集上，SalientFusion均取得了最先进的成果。

Conclusion: SalientFusion成功解决了CZSFR中的关键挑战，并在相关基准测试中表现优异。

Abstract: Food recognition has gained significant attention, but the rapid emergence of
new dishes requires methods for recognizing unseen food categories, motivating
Zero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot
Food Recognition (CZSFR), where cuisines and ingredients naturally align with
attributes and objects in Compositional Zero-Shot learning (CZSL). However,
CZSFR faces three challenges: (1) Redundant background information distracts
models from learning meaningful food features, (2) Role confusion between
staple and side dishes leads to misclassification, and (3) Semantic bias in a
single attribute can lead to confusion of understanding. Therefore, we propose
SalientFusion, a context-aware CZSFR method with two components: SalientFormer,
which removes background redundancy and uses depth features to resolve role
confusion; DebiasAT, which reduces the semantic bias by aligning prompts with
visual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we
show that SalientFusion achieves state-of-the-art results on these benchmarks
and the most popular general datasets for the general CZSL. The code is
avaliable at https://github.com/Jiajun-RUC/SalientFusion.

</details>


### [19] [Human Motion Video Generation: A Survey](https://arxiv.org/abs/2509.03883)
*Haiwei Xue,Xiangyang Luo,Zhanghao Hu,Xin Zhang,Xunzhi Xiang,Yuqin Dai,Jianzhuang Liu,Zhensong Zhang,Minglei Li,Jian Yang,Fei Ma,Zhiyong Wu,Changpeng Yang,Zonghong Dai,Fei Richard Yu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Human motion video generation has garnered significant research interest due
to its broad applications, enabling innovations such as photorealistic singing
heads or dynamic avatars that seamlessly dance to music. However, existing
surveys in this field focus on individual methods, lacking a comprehensive
overview of the entire generative process. This paper addresses this gap by
providing an in-depth survey of human motion video generation, encompassing
over ten sub-tasks, and detailing the five key phases of the generation
process: input, motion planning, motion video generation, refinement, and
output. Notably, this is the first survey that discusses the potential of large
language models in enhancing human motion video generation. Our survey reviews
the latest developments and technological trends in human motion video
generation across three primary modalities: vision, text, and audio. By
covering over two hundred papers, we offer a thorough overview of the field and
highlight milestone works that have driven significant technological
breakthroughs. Our goal for this survey is to unveil the prospects of human
motion video generation and serve as a valuable resource for advancing the
comprehensive applications of digital humans. A complete list of the models
examined in this survey is available in Our Repository
https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.

</details>


### [20] [OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction](https://arxiv.org/abs/2509.03887)
*Bu Jin,Songen Gu,Xiaotao Hu,Yupeng Zheng,Xiaoyang Guo,Qian Zhang,Xiaoxiao Long,Wei Yin*

Main category: cs.CV

TL;DR: OccTENS是一个生成式占用世界模型，可以高效地生成可控、高保真度的长期占用场景，解决了现有方法的效率低下、长期生成质量下降和缺乏可控性等问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归（AR）的方法在同时预测车辆运动和未来占用场景方面有潜力，但存在效率低下、长期生成时序退化和缺乏可控性等问题。本研究旨在解决这些挑战。

Method: 将占用世界模型重新构建为时间序列下一尺度预测（TENS）任务，将时间序列建模分解为空间逐尺度生成和时间逐场景预测。使用TensFormer来管理占用序列的时空关系，并提出一种整体姿态聚合策略来增强姿态可控性。

Result: OccTENS在占用质量和推理速度上均优于现有最先进方法。

Conclusion: OccTENS通过创新的TENS任务和TensFormer，以及姿态聚合策略，有效解决了长期占用场景生成中的效率、质量和可控性问题。

Abstract: In this paper, we propose OccTENS, a generative occupancy world model that
enables controllable, high-fidelity long-term occupancy generation while
maintaining computational efficiency. Different from visual generation, the
occupancy world model must capture the fine-grained 3D geometry and dynamic
evolution of the 3D scenes, posing great challenges for the generative models.
Recent approaches based on autoregression (AR) have demonstrated the potential
to predict vehicle movement and future occupancy scenes simultaneously from
historical observations, but they typically suffer from \textbf{inefficiency},
\textbf{temporal degradation} in long-term generation and \textbf{lack of
controllability}. To holistically address these issues, we reformulate the
occupancy world model as a temporal next-scale prediction (TENS) task, which
decomposes the temporal sequence modeling problem into the modeling of spatial
scale-by-scale generation and temporal scene-by-scene prediction. With a
\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and
spatial relationships of occupancy sequences in a flexible and scalable way. To
enhance the pose controllability, we further propose a holistic pose
aggregation strategy, which features a unified sequence modeling for occupancy
and ego-motion. Experiments show that OccTENS outperforms the state-of-the-art
method with both higher occupancy quality and faster inference time.

</details>


### [21] [Weakly-Supervised Learning of Dense Functional Correspondences](https://arxiv.org/abs/2509.03893)
*Stefan Stojanov,Linan Zhao,Yunzhi Zhang,Daniel L. K. Yamins,Jiajun Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种弱监督学习范式，利用视觉-语言模型生成伪标签，结合密集对比学习，来建立跨不同类别的密集功能对应关系。


<details>
  <summary>Details</summary>
Motivation: 为了在跨不同类别匹配的挑战性场景下，利用物体功能（即物体对其他物体产生的影响）来指导密集对应关系的建立，因为实现特定功能的物体部件通常在形状和外观上具有相似性。

Method: 提出了一种弱监督学习范式，利用视觉-语言模型为多视图图像生成伪标签，以获得功能部件。然后，将此与来自像素对应关系的密集对比学习相结合，将功能和空间知识提炼到一个新的模型中，该模型能够建立密集的功能对应关系。

Result: 所提出的方法在合成和真实评估数据集上的结果优于现成的自监督图像表示和基于视觉-语言模型的基线解决方案。

Conclusion: 通过结合功能和空间知识，可以有效地建立密集的功能对应关系，尤其是在跨类别匹配的场景下。

Abstract: Establishing dense correspondences across image pairs is essential for tasks
such as shape reconstruction and robot manipulation. In the challenging setting
of matching across different categories, the function of an object, i.e., the
effect that an object can cause on other objects, can guide how correspondences
should be established. This is because object parts that enable specific
functions often share similarities in shape and appearance. We derive the
definition of dense functional correspondence based on this observation and
propose a weakly-supervised learning paradigm to tackle the prediction task.
The main insight behind our approach is that we can leverage vision-language
models to pseudo-label multi-view images to obtain functional parts. We then
integrate this with dense contrastive learning from pixel correspondences to
distill both functional and spatial knowledge into a new model that can
establish dense functional correspondence. Further, we curate synthetic and
real evaluation datasets as task benchmarks. Our results demonstrate the
advantages of our approach over baseline solutions consisting of off-the-shelf
self-supervised image representations and grounded vision language models.

</details>


### [22] [Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of Vision-Language Model](https://arxiv.org/abs/2509.03895)
*Phuoc-Nguyen Bui,Khanh-Binh Nguyen,Hyunseung Choo*

Main category: cs.CV

TL;DR: Attn-Adapter是一个在线少样本学习框架，通过双注意力机制增强CLIP的适应性，以解决现有方法在少样本场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的对比视觉-语言模型在少样本场景下表现不佳，因为离线微调计算成本高且容易过拟合。

Method: 提出Attn-Adapter，包含Memory Attn-Adapter和Local-Global Attn-Adapter，通过注意力机制动态调整类别和图像嵌入，无需重新训练基础模型。

Result: Attn-Adapter在跨类别和跨数据集的泛化能力上优于现有最先进的方法，同时保持了高效的推理和可扩展性。

Conclusion: Attn-Adapter通过引入注意力机制，实现了对CLIP模型的有效在线少样本学习，克服了传统方法的缺点。

Abstract: Contrastive vision-language models excel in zero-shot image recognition but
face challenges in few-shot scenarios due to computationally intensive offline
fine-tuning using prompt learning, which risks overfitting. To overcome these
limitations, we propose Attn-Adapter, a novel online few-shot learning
framework that enhances CLIP's adaptability via a dual attention mechanism. Our
design incorporates dataset-specific information through two components: the
Memory Attn-Adapter, which refines category embeddings using support examples,
and the Local-Global Attn-Adapter, which enriches image embeddings by
integrating local and global features. This architecture enables dynamic
adaptation from a few labeled samples without retraining the base model.
Attn-Adapter outperforms state-of-the-art methods in cross-category and
cross-dataset generalization, maintaining efficient inference and scaling
across CLIP backbones.

</details>


### [23] [YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind Turbine Components](https://arxiv.org/abs/2509.04156)
*Serhii Svystun,Pavlo Radiuk,Oleksandr Melnychenko,Oleg Savenko,Anatoliy Sachenko*

Main category: cs.CV

TL;DR: 本研究提出了一种结合可见光和热红外传感器的YOLO模型集成方法，用于提高风力涡轮机部件缺陷检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了提高风力涡轮机部件（如叶片、塔筒等）缺陷检测的可靠性，需要高分辨率的数据和高效的多光谱图像处理方法。

Method: 提出了一种集成通用YOLOv8模型和专用热成像模型的集成方法，并使用先进的边界框融合算法来结合它们的预测结果。

Result: 该集成方法实现了0.93的平均精度均值（mAP@.5）和0.90的F1分数，优于单独使用的YOLOv8模型（mAP@.5为0.91）。

Conclusion: 结合多个YOLO架构和融合后的多光谱数据为缺陷检测提供了一种更可靠的解决方案，能够同时检测视觉和热成像缺陷。

Abstract: Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up
new opportunities for monitoring wind power plants, including blades, towers,
and other critical components. However, reliable defect detection requires
high-resolution data and efficient methods to process multispectral imagery. In
this research, we aim to enhance defect detection accuracy through the
development of an ensemble of YOLO-based deep learning models that integrate
both visible and thermal channels. We propose an ensemble approach that
integrates a general-purpose YOLOv8 model with a specialized thermal model,
using a sophisticated bounding box fusion algorithm to combine their
predictions. Our experiments show this approach achieves a mean Average
Precision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone
YOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that
combining multiple YOLO architectures with fused multispectral data provides a
more reliable solution, improving the detection of both visual and thermal
defects.

</details>


### [24] [SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation](https://arxiv.org/abs/2509.03897)
*Xiaofu Chen,Israfel Salazar,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: SPECS是一种新的图像描述评估指标，比现有指标更准确、更高效。


<details>
  <summary>Details</summary>
Motivation: 现有的图像描述评估指标（如基于N-gram和基于CLIP的度量）在评估长而详细的图像描述时存在不足，无法捕捉语义准确性或与人类判断的相关性不高。基于LLM的度量虽然相关性强，但成本过高。

Method: SPECS（Specificity-Enhanced CLIPScore）通过引入一个强调具体性的新目标来修改CLIP，该目标会奖励正确的细节并惩罚不正确的细节，从而成为一种无需参考的度量。

Result: SPECS在与人类判断的相关性方面，其表现与开源的基于LLM的度量相当，但效率却高得多。

Conclusion: SPECS是一种比现有指标更有效、更经济的评估长图像描述的方法，适用于模型开发过程中的迭代评估。

Abstract: As interest grows in generating long, detailed image captions, standard
evaluation metrics become increasingly unreliable. N-gram-based metrics though
efficient, fail to capture semantic correctness. Representational Similarity
(RS) metrics, designed to address this, initially saw limited use due to high
computational costs, while today, despite advances in hardware, they remain
unpopular due to low correlation to human judgments. Meanwhile, metrics based
on large language models (LLMs) show strong correlation with human judgments,
but remain too expensive for iterative use during model development.
  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS
metric tailored to long image captioning. SPECS modifies CLIP with a new
objective that emphasizes specificity: rewarding correct details and penalizing
incorrect ones. We show that SPECS matches the performance of open-source
LLM-based metrics in correlation to human judgments, while being far more
efficient. This makes it a practical alternative for iterative checkpoint
evaluation during image captioning model development.Our code can be found at
https://github.com/mbzuai-nlp/SPECS.

</details>


### [25] [A Generative Foundation Model for Chest Radiography](https://arxiv.org/abs/2509.03903)
*Yuanfeng Ji,Dan Lin,Xiyue Wang,Lu Zhang,Wenhui Zhou,Chongjian Ge,Ruihang Chu,Xiaoli Yang,Junhan Zhao,Junsong Chen,Xiangde Luo,Sen Yang,Jin Fang,Ping Luo,Ruijiang Li*

Main category: cs.CV

TL;DR: ChexGen是一个通用的、由文本、掩模和边界框驱动的胸部X射线图像生成模型，可用于训练数据增强、监督预训练、疾病分类、检测和分割任务，并提高模型的公平性。


<details>
  <summary>Details</summary>
Motivation: 医疗领域高质量标注的医学图像稀缺，阻碍了可靠的AI模型的发展。

Method: ChexGen是一个基于潜在扩散 Transformer 架构的生成式视觉-语言基础模型，并在迄今为止最大的、包含960,000个胸部X光片-报告对的胸部X光片数据集上进行了预训练。

Result: ChexGen可以通过专家评估和定量指标准确地合成X射线图像，并在疾病分类、检测和分割任务中带来性能提升。该模型还能生成多样化的患者队列，通过检测和减轻人口统计学偏差来提高模型的公平性。

Conclusion: ChexGen展示了生成式基础模型在构建更准确、数据高效和公平的医疗AI系统方面的变革潜力。

Abstract: The scarcity of well-annotated diverse medical images is a major hurdle for
developing reliable AI models in healthcare. Substantial technical advances
have been made in generative foundation models for natural images. Here we
develop `ChexGen', a generative vision-language foundation model that
introduces a unified framework for text-, mask-, and bounding box-guided
synthesis of chest radiographs. Built upon the latent diffusion transformer
architecture, ChexGen was pretrained on the largest curated chest X-ray dataset
to date, consisting of 960,000 radiograph-report pairs. ChexGen achieves
accurate synthesis of radiographs through expert evaluations and quantitative
metrics. We demonstrate the utility of ChexGen for training data augmentation
and supervised pretraining, which led to performance improvements across
disease classification, detection, and segmentation tasks using a small
fraction of training data. Further, our model enables the creation of diverse
patient cohorts that enhance model fairness by detecting and mitigating
demographic biases. Our study supports the transformative role of generative
foundation models in building more accurate, data-efficient, and equitable
medical AI systems.

</details>


### [26] [LMVC: An End-to-End Learned Multiview Video Coding Framework](https://arxiv.org/abs/2509.03922)
*Xihua Sheng,Yingwen Zhang,Long Xu,Shiqi Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种端到端的学习型多视角视频编码（LMVC）框架，用于提高多视角视频的压缩效率，同时保证随机访问和向后兼容性。


<details>
  <summary>Details</summary>
Motivation: 多视角视频在三维场景重建中至关重要，但其巨大的数据量带来了存储和传输的挑战。现有的基于深度学习的视频编码方法大多只关注单视角或立体视频，对通用多视角场景的研究不足。

Method: 提出了一种利用独立视角的运动和内容信息来增强依赖视角压缩的LMVC框架。具体包括：1）利用特征的视角运动向量预测方法，将依赖视角的运动编码条件化于已解码的独立视角运动特征，并结合视角运动熵模型学习视角运动先验。2）提出了一种无视差的视角内容预测模块，从已解码的独立视角内容特征预测视角内容，并结合视角内容熵模型捕捉视角内容先验。

Result: 实验结果表明，所提出的LMVC框架的性能显著优于传统MV-HEVC标准的参考软件。

Conclusion: 所提出的LMVC框架在保证随机访问和向后兼容性的同时，显著提高了多视角视频的压缩效率，为该领域未来的研究奠定了坚实的基础。

Abstract: Multiview video is a key data source for volumetric video, enabling immersive
3D scene reconstruction but posing significant challenges in storage and
transmission due to its massive data volume. Recently, deep learning-based
end-to-end video coding has achieved great success, yet most focus on
single-view or stereo videos, leaving general multiview scenarios
underexplored. This paper proposes an end-to-end learned multiview video coding
(LMVC) framework that ensures random access and backward compatibility while
enhancing compression efficiency. Our key innovation lies in effectively
leveraging independent-view motion and content information to enhance
dependent-view compression. Specifically, to exploit the inter-view motion
correlation, we propose a feature-based inter-view motion vector prediction
method that conditions dependent-view motion encoding on decoded
independent-view motion features, along with an inter-view motion entropy model
that learns inter-view motion priors. To exploit the inter-view content
correlation, we propose a disparity-free inter-view context prediction module
that predicts inter-view contexts from decoded independent-view content
features, combined with an inter-view contextual entropy model that captures
inter-view context priors. Experimental results show that our proposed LMVC
framework outperforms the reference software of the traditional MV-HEVC
standard by a large margin, establishing a strong baseline for future research
in this field.

</details>


### [27] [TopoSculpt: Betti-Steered Topological Sculpting of 3D Fine-grained Tubular Shapes](https://arxiv.org/abs/2509.03938)
*Minghui Zhang,Yaoyu Liu,Junyang Wu,Xin You,Hanxiao Zhang,Junjun He,Yun Gu*

Main category: cs.CV

TL;DR: TopoSculpt是一个用于三维管状结构拓扑优化的新框架，通过整体建模、拓扑完整性Betti约束和课程学习方法，显著提高了几何和拓扑的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在重建三维管状解剖结构时，由于依赖体素重叠度量，无法保证拓扑的正确性和完整性，并且拓扑感知损失通常是分块应用的，不能保证全局保持或在推理时纠正几何错误。

Method: TopoSculpt采用整体全区域建模策略来捕捉完整的空间上下文，首次引入拓扑完整性Betti（TIB）约束来共同强制执行Betti数先验和全局完整性，并采用具有持久性的课程学习方案同系来逐步纠正从粗到精的尺度误差。

Result: 在肺泡和 cổ tử cung 动脉环数据集上进行了广泛的实验，在几何和拓扑方面都有显著的改进。例如，气道数据集上的β0误差从69.00降低到3.40，CoW数据集上的β0误差从1.65降低到0.30，树长检测和分支检测的比率提高了近10%。

Conclusion: TopoSculpt在纠正关键拓扑错误和推进复杂三维管状解剖结构的高保真建模方面是有效的。

Abstract: Medical tubular anatomical structures are inherently three-dimensional
conduits with lumens, enclosing walls, and complex branching topologies.
Accurate reconstruction of their geometry and topology is crucial for
applications such as bronchoscopic navigation and cerebral arterial
connectivity assessment. Existing methods often rely on voxel-wise overlap
measures, which fail to capture topological correctness and completeness.
Although topology-aware losses and persistent homology constraints have shown
promise, they are usually applied patch-wise and cannot guarantee global
preservation or correct geometric errors at inference. To address these
limitations, we propose a novel TopoSculpt, a framework for topological
refinement of 3D fine-grained tubular structures. TopoSculpt (i) adopts a
holistic whole-region modeling strategy to capture full spatial context, (ii)
first introduces a Topological Integrity Betti (TIB) constraint that jointly
enforces Betti number priors and global integrity, and (iii) employs a
curriculum refinement scheme with persistent homology to progressively correct
errors from coarse to fine scales. Extensive experiments on challenging
pulmonary airway and Circle of Willis datasets demonstrate substantial
improvements in both geometry and topology. For instance, $\beta_{0}$ errors
are reduced from 69.00 to 3.40 on the airway dataset and from 1.65 to 0.30 on
the CoW dataset, with Tree length detected and branch detected rates improving
by nearly 10\%. These results highlight the effectiveness of TopoSculpt in
correcting critical topological errors and advancing the high-fidelity modeling
of complex 3D tubular anatomy. The project homepage is available at:
https://github.com/Puzzled-Hui/TopoSculpt.

</details>


### [28] [Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer Learning in a U-Net Architecture](https://arxiv.org/abs/2509.03950)
*Alvaro Aranibar Roque,Helga Sebastian*

Main category: cs.CV

TL;DR: A deep learning model using U-Net with EfficientNet-B4 encoder can accurately segment pneumothorax regions in chest X-rays, achieving high IoU and Dice scores on an independent dataset.


<details>
  <summary>Details</summary>
Motivation: Pneumothorax can be life-threatening and may be subtle on chest X-rays, necessitating an automated tool for accurate detection.

Method: A deep learning pipeline using a U-Net with an EfficientNet-B4 encoder was developed. Data augmentation and a combined binary cross-entropy plus Dice loss were used for training on the SIIM-ACR dataset.

Result: The model achieved an IoU of 0.7008 and a Dice score of 0.8241 on the independent PTX-498 dataset.

Conclusion: The proposed deep learning model can accurately localize pneumothoraces, providing valuable support to radiologists in diagnosing this condition.

Abstract: Pneumothorax, the abnormal accumulation of air in the pleural space, can be
life-threatening if undetected. Chest X-rays are the first-line diagnostic
tool, but small cases may be subtle. We propose an automated deep-learning
pipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax
regions. Trained on the SIIM-ACR dataset with data augmentation and a combined
binary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and
Dice score of 0.8241 on the independent PTX-498 dataset. These results
demonstrate that the model can accurately localize pneumothoraces and support
radiologists.

</details>


### [29] [ANTS: Shaping the Adaptive Negative Textual Space by MLLM for OOD Detection](https://arxiv.org/abs/2509.03951)
*Zhu Wenjie,Zhang Yabin,Xin Jin,Wenjun Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: 通过利用多模态大语言模型（MLLMs）来构建自适应负面文本空间（ANTS），以提高 OOD 检测性能，并解决了现有方法在理解 OOD 图像和处理虚假负样本方面存在的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在 OOD 检测中，由于缺乏对 OOD 图像的理解，难以构建准确的负空间，并且虚假负标签会显著降低近 OOD 性能。

Method: 提出 ANTS，利用 MLLMs 的理解和推理能力，将可能为 OOD 的图像作为负图像，并让 MLLM 描述这些图像，生成精确表征 OOD 分布的负面句子，以增强远 OOD 检测。对于近 OOD，识别与负图像视觉相似的 ID 子集，并利用 MLLM 生成视觉上相似的负标签，以减少虚假负样本。设计自适应加权分数来平衡近 OOD 和远 OOD 两种负面文本空间。

Result: 在 ImageNet 基准测试中，ANTS 将 FPR95 降低了 4.2%，达到了新的 state-of-the-art 水平。

Conclusion: ANTS 是一种训练无关、零样本的方法，具有高可扩展性，能够有效提升 OOD 检测性能，尤其在近 OOD 和远 OOD 场景下均表现出色。

Abstract: The introduction of negative labels (NLs) has proven effective in enhancing
Out-of-Distribution (OOD) detection. However, existing methods often lack an
understanding of OOD images, making it difficult to construct an accurate
negative space. In addition, the presence of false negative labels
significantly degrades their near-OOD performance. To address these issues, we
propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the
understanding and reasoning capabilities of multimodal large language models
(MLLMs). Specifically, we identify images likely to be OOD samples as negative
images and prompt the MLLM to describe these images, generating expressive
negative sentences that precisely characterize the OOD distribution and enhance
far-OOD detection. For the near-OOD setting, where OOD samples resemble the
in-distribution (ID) subset, we first identify the subset of ID classes that
are visually similar to negative images and then leverage the reasoning
capability of MLLMs to generate visually similar negative labels tailored to
this subset, effectively reducing false negatives and improving near-OOD
detection. To balance these two types of negative textual spaces, we design an
adaptive weighted score that enables the method to handle different OOD task
settings (near-OOD and far-OOD) without relying on task-specific prior
knowledge, making it highly adaptable in open environments. On the ImageNet
benchmark, our ANTS significantly reduces the FPR95 by 4.2\%, establishing a
new state-of-the-art. Furthermore, our method is training-free and zero-shot,
enabling high scalability.

</details>


### [30] [Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection](https://arxiv.org/abs/2509.03961)
*Yijun Zhou,Yikui Zhai,Zilu Ying,Tingfeng Xian,Wenlve Zhou,Zhiheng Zhou,Xiaolin Tian,Xudong Jia,Hongsheng Zhang,C. L. Philip Chen*

Main category: cs.CV

TL;DR: MMChange是一种结合图像和文本模态的多模态遥感变化检测方法，通过引入图像特征精炼（IFR）、文本差异增强（TDE）和图像文本特征融合（ITFF）模块，提高了变化检测的准确性和鲁棒性，并在多个数据集上取得了优于现有最先进方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感变化检测（RSCD）方法主要依赖单一的图像模态，这在光照和噪声干扰下会限制特征表示、变化模式建模和泛化能力。为了解决这个问题，需要一种能够结合多种模态的方法来提高RSCD的准确性和鲁棒性。

Method: MMChange方法首先使用图像特征精炼（IFR）模块来突出关键区域并抑制环境噪声。然后，利用视觉语言模型（VLM）为双时相图像生成语义描述，并使用文本差异增强（TDE）模块捕捉细粒度的语义变化，以克服图像特征的语义局限性。最后，通过图像文本特征融合（ITFF）模块来融合图像和文本模态的特征，以桥接模态间的异质性。

Result: 在LEVIRCD、WHUCD和SYSUCD数据集上的广泛实验表明，MMChange在多个指标上始终优于现有的最先进方法，证明了其在多模态RSCD上的有效性。

Conclusion: MMChange通过结合图像和文本模态，并设计了IFR、TDE和ITFF等模块，成功地提高了遥感变化检测的准确性和鲁棒性，克服了单一图像模态的局限性，并在多个公开数据集上取得了优异的性能。

Abstract: Although deep learning has advanced remote sensing change detection (RSCD),
most methods rely solely on image modality, limiting feature representation,
change pattern modeling, and generalization especially under illumination and
noise disturbances. To address this, we propose MMChange, a multimodal RSCD
method that combines image and text modalities to enhance accuracy and
robustness. An Image Feature Refinement (IFR) module is introduced to highlight
key regions and suppress environmental noise. To overcome the semantic
limitations of image features, we employ a vision language model (VLM) to
generate semantic descriptions of bitemporal images. A Textual Difference
Enhancement (TDE) module then captures fine grained semantic shifts, guiding
the model toward meaningful changes. To bridge the heterogeneity between
modalities, we design an Image Text Feature Fusion (ITFF) module that enables
deep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and
SYSUCD demonstrate that MMChange consistently surpasses state of the art
methods across multiple metrics, validating its effectiveness for multimodal
RSCD. Code is available at: https://github.com/yikuizhai/MMChange.

</details>


### [31] [SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for Histopathology Whole Slide Image Classification](https://arxiv.org/abs/2509.03973)
*Yu Bai,Zitong Yu,Haowen Tian,Xijing Wang,Shuo Yan,Lin Wang,Honglin Li,Xitong Ling,Bo Zhang,Zheng Zhang,Wufan Wang,Hui Gao,Xiangyang Gong,Wendong Wang*

Main category: cs.CV

TL;DR: SAC-MIL是一种用于WSI分类的空间感知相关多实例学习方法，通过位置编码模块和SAC块实现实例间相关性，解决了序列长度不匹配问题，并具有线性时间复杂度和易于部署的优点，在多个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够有效处理WSIs（全幻灯片图像）分类问题的方法，该方法需要考虑实例的空间位置信息，并能够进行实例间的相关性分析，同时解决训练和测试时序列长度不一致的问题。

Method: 提出了一种名为SAC-MIL（空间感知相关多实例学习）的方法。该方法包含一个位置编码模块，用于利用实例的坐标信息编码空间关系，以处理不同长度的序列。同时，还包含一个SAC块，采用基于MLP（多层感知机）的设计，以线性时间复杂度实现所有实例间的相关性计算，避免了Transformer类方法对自定义CUDA核的需求。

Result: SAC-MIL在CAMELYON-16、TCGA-LUNG和TCGA-BRAC数据集上取得了最先进的性能。

Conclusion: SAC-MIL通过其独特的位置编码和SAC块设计，有效地解决了WSI分类中的空间信息编码和实例相关性问题，具有计算效率高和易于部署的优点，并在多个基准数据集上验证了其优越性。

Abstract: We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for
performing WSI classification. SAC-MIL consists of a positional encoding module
to encode position information and a SAC block to perform full instance
correlations. The positional encoding module utilizes the instance coordinates
within the slide to encode the spatial relationships instead of the instance
index in the input WSI sequence. The positional encoding module can also handle
the length extrapolation issue where the training and testing sequences have
different lengths. The SAC block is an MLP-based method that performs full
instance correlation in linear time complexity with respect to the sequence
length. Due to the simple structure of MLP, it is easy to deploy since it does
not require custom CUDA kernels, compared to Transformer-based methods for WSI
classification. SAC-MIL has achieved state-of-the-art performance on the
CAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon
acceptance.

</details>


### [32] [Improving Vessel Segmentation with Multi-Task Learning and Auxiliary Data Available Only During Model Training](https://arxiv.org/abs/2509.03975)
*Daniel Sobotka,Alexander Herold,Matthias Perkonigg,Lucian Beer,Nina Bastati,Alina Sablatnig,Ahmed Ba-Ssalamah,Georg Langs*

Main category: cs.CV

TL;DR: 提出了一种多任务学习框架，利用辅助的增强MRI数据（仅在训练期间可用）来分割无增强的肝脏MRI中的血管，从而减少了对标注训练样本的需求。


<details>
  <summary>Details</summary>
Motivation: 肝脏血管分割在磁共振成像数据中对于血管重塑的计算分析很重要，这与多种弥漫性肝脏疾病相关。现有方法依赖于增强成像数据，但并非总是能获得所需的专用成像序列。无增强图像更频繁地被获取，但血管分割具有挑战性，并且需要大规模的标注数据。

Method: 提出了一种多任务学习框架，利用辅助的增强MRI数据（仅在训练期间可用）来分割无增强的肝脏MRI中的血管，并利用配对的原生和增强MRI数据以及血管标注进行模型训练。

Result: 辅助数据在推理期间不可用时，也能提高血管分割的准确性。当训练数据标注较少时，这种优势最为明显，因为共享的任务结构使特征表示受益。将该方法应用于脑肿瘤分割模型进行扩增，证实了其跨不同领域的益处。

Conclusion: 即使仅在训练期间可用，辅助信息成像模态也可以扩充专家标注。

Abstract: Liver vessel segmentation in magnetic resonance imaging data is important for
the computational analysis of vascular remodelling, associated with a wide
spectrum of diffuse liver diseases. Existing approaches rely on contrast
enhanced imaging data, but the necessary dedicated imaging sequences are not
uniformly acquired. Images without contrast enhancement are acquired more
frequently, but vessel segmentation is challenging, and requires large-scale
annotated data. We propose a multi-task learning framework to segment vessels
in liver MRI without contrast. It exploits auxiliary contrast enhanced MRI data
available only during training to reduce the need for annotated training
examples. Our approach draws on paired native and contrast enhanced data with
and without vessel annotations for model training. Results show that auxiliary
data improves the accuracy of vessel segmentation, even if they are not
available during inference. The advantage is most pronounced if only few
annotations are available for training, since the feature representation
benefits from the shared task structure. A validation of this approach to
augment a model for brain tumor segmentation confirms its benefits across
different domains. An auxiliary informative imaging modality can augment expert
annotations even if it is only available during training.

</details>


### [33] [Promptception: How Sensitive Are Large Multimodal Models to Prompts?](https://arxiv.org/abs/2509.03986)
*Mohamed Insaf Ismithdeen,Muhammad Uzair Khattak,Salman Khan*

Main category: cs.CV

TL;DR: 尽管大型多模态模型（LMMs）在多项选择题问答（MCQA）方面取得了成功，但其提示词设计仍然知之甚少。我们发现，提示词措辞和结构上微小的变化会导致某些提示词和模型高达15%的准确率偏差。这种可变性给LMMs的透明和公平评估带来了挑战。为了解决这个问题，我们引入了Promptception，一个用于评估LMMs中提示词敏感性的系统框架。该框架包含61种提示词类型，涵盖15个类别和6个超类，旨在评估提示词构建的特定方面。我们使用该框架评估了10个LMMs，涵盖了从轻量级开源模型到GPT-4o和Gemini 1.5 Pro，涉及MMStar、MMMU-Pro和MVBench三个MCQA基准。我们的研究结果表明，专有模型对提示词措辞更敏感，这反映了它们与指令语义更紧密的对齐；而开源模型则更稳定，但在处理细致和复杂的措辞时存在困难。基于此分析，我们提出了针对专有和开源LMMs的提示词原则，以实现更稳健和公平的模型评估。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）在多项选择题问答（MCQA）任务中的提示词设计对模型性能有显著影响，但目前对其影响机制的理解尚不充分，这给模型的公平评估带来了挑战。

Method: 提出Promptception框架，包含61种提示词类型，涵盖15个类别和6个超类，系统性地评估LMMs对提示词变化的敏感性。在MMStar、MMMU-Pro和MVBench三个MCQA基准上，对包括GPT-4o和Gemini 1.5 Pro在内的10个LMMs进行了评估。

Result: 研究发现，专有模型比开源模型对提示词措辞的变化更敏感。开源模型虽然在不同提示词下表现更稳定，但在处理复杂或细致的措辞时能力较弱。

Conclusion: 提出Promptception框架，并根据评估结果，为专有和开源LMMs分别制定了提示词设计原则，以促进更公平、更可靠的模型评估。

Abstract: Despite the success of Large Multimodal Models (LMMs) in recent years, prompt
design for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly
understood. We show that even minor variations in prompt phrasing and structure
can lead to accuracy deviations of up to 15% for certain prompts and models.
This variability poses a challenge for transparent and fair LMM evaluation, as
models often report their best-case performance using carefully selected
prompts. To address this, we introduce Promptception, a systematic framework
for evaluating prompt sensitivity in LMMs. It consists of 61 prompt types,
spanning 15 categories and 6 supercategories, each targeting specific aspects
of prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight
open-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks:
MMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit
greater sensitivity to prompt phrasing, reflecting tighter alignment with
instruction semantics, while open-source models are steadier but struggle with
nuanced and complex phrasing. Based on this analysis, we propose Prompting
Principles tailored to proprietary and open-source LMMs, enabling more robust
and fair model evaluation.

</details>


### [34] [SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy Representation](https://arxiv.org/abs/2509.03999)
*Han Huang,Han Sun,Ningzhong Liu,Huiyu Zhou,Jiaquan Shen*

Main category: cs.CV

TL;DR: SliceSemOcc通过结合全局和局部垂直切片以及SEAttention3D模块，提升了3D语义占用预测的性能，尤其是在小物体类别上。


<details>
  <summary>Details</summary>
Motivation: 现有3D语义占用预测方法未能充分利用高度轴信息，并且传统通道注意力机制无法有效区分不同高度层的特征，限制了性能提升。对精确3D感知有较高需求的自动驾驶领域亟需改进方法。

Method: 提出了一种名为SliceSemOcc的新型基于垂直切片的3D语义占用预测框架。该框架提取高度轴上的体素特征（全局和局部垂直切片），并通过全局局部融合模块整合细节和上下文信息。此外，设计了SEAttention3D模块，通过平均池化保留高度分辨率，并为每个高度层动态分配通道注意力权重。

Result: 在nuScenes-SurroundOcc和nuScenes-OpenOccupancy数据集上进行了广泛的实验，SliceSemOcc在平均IoU（mIoU）方面取得了显著提升，特别是在大多数小物体类别上表现突出。

Conclusion: 提出的SliceSemOcc框架及其中的SEAttention3D模块能够有效利用高度轴信息，提升3D语义占用预测的准确性，为自动驾驶提供更精确的空间感知能力。消融实验进一步证明了该框架的有效性。

Abstract: Driven by autonomous driving's demands for precise 3D perception, 3D semantic
occupancy prediction has become a pivotal research topic. Unlike
bird's-eye-view (BEV) methods, which restrict scene representation to a 2D
plane, occupancy prediction leverages a complete 3D voxel grid to model spatial
structures in all dimensions, thereby capturing semantic variations along the
vertical axis. However, most existing approaches overlook height-axis
information when processing voxel features. And conventional SENet-style
channel attention assigns uniform weight across all height layers, limiting
their ability to emphasize features at different heights. To address these
limitations, we propose SliceSemOcc, a novel vertical slice based multimodal
framework for 3D semantic occupancy representation. Specifically, we extract
voxel features along the height-axis using both global and local vertical
slices. Then, a global local fusion module adaptively reconciles fine-grained
spatial details with holistic contextual information. Furthermore, we propose
the SEAttention3D module, which preserves height-wise resolution through
average pooling and assigns dynamic channel attention weights to each height
layer. Extensive experiments on nuScenes-SurroundOcc and nuScenes-OpenOccupancy
datasets verify that our method significantly enhances mean IoU, achieving
especially pronounced gains on most small-object categories. Detailed ablation
studies further validate the effectiveness of the proposed SliceSemOcc
framework.

</details>


### [35] [Detecting Regional Spurious Correlations in Vision Transformers via Token Discarding](https://arxiv.org/abs/2509.04009)
*Solha Kang,Esla Timothy Anzaku,Wesley De Neve,Arnout Van Messem,Joris Vankerschaver,Francois Rameau,Utku Ozbulak*

Main category: cs.CV

TL;DR: 该研究提出了一种检测视觉Transformer中虚假相关性的新方法，并通过大规模实验验证了其有效性，同时强调了训练方法和数据集中类别对虚假相关性的影响，并给出了具体案例研究。


<details>
  <summary>Details</summary>
Motivation: 由于神经网络在特征关联方面的强大能力，可能导致其检测和利用数据中意外的、非目标性的但统计上相关的信号（即虚假相关性）进行预测，这影响了模型的可信度、可靠性和泛化能力，因此检测和缓解虚假相关性至关重要。

Method: 提出了一种新颖的方法来检测视觉Transformer中的虚假相关性，并在ImageNet数据集上进行了大规模实验，使用了监督和自监督训练的模型。

Result: 实验表明，该方法能够识别虚假相关性；同一模型架构下，训练方法对模型依赖虚假相关性的程度有显著影响；ImageNet数据集中的某些类别包含容易被模型检测到的虚假信号；并给出了虚假信号的根本原因分析；最后，通过一个侵入性乳腺肿块分类的案例研究，验证了该方法在现实场景中的应用。

Conclusion: 虚假相关性是影响模型可靠性的关键问题，检测和理解虚假相关性对于构建可信赖的机器学习模型至关重要。研究提出的方法有效，并指出了训练方法和数据集类别的重要性，以及未来研究中应注意的潜在问题。

Abstract: Due to their powerful feature association capabilities, neural network-based
computer vision models have the ability to detect and exploit unintended
patterns within the data, potentially leading to correct predictions based on
incorrect or unintended but statistically relevant signals. These clues may
vary from simple color aberrations to small texts within the image. In
situations where these unintended signals align with the predictive task,
models can mistakenly link these features with the task and rely on them for
making predictions. This phenomenon is referred to as spurious correlations,
where patterns appear to be associated with the task but are actually
coincidental. As a result, detection and mitigation of spurious correlations
have become crucial tasks for building trustworthy, reliable, and generalizable
machine learning models. In this work, we present a novel method to detect
spurious correlations in vision transformers, a type of neural network
architecture that gained significant popularity in recent years. Using both
supervised and self-supervised trained models, we present large-scale
experiments on the ImageNet dataset demonstrating the ability of the proposed
method to identify spurious correlations. We also find that, even if the same
architecture is used, the training methodology has a significant impact on the
model's reliance on spurious correlations. Furthermore, we show that certain
classes in the ImageNet dataset contain spurious signals that are easily
detected by the models and discuss the underlying reasons for those spurious
signals. In light of our findings, we provide an exhaustive list of the
aforementioned images and call for caution in their use in future research
efforts. Lastly, we present a case study investigating spurious signals in
invasive breast mass classification, grounding our work in real-world
scenarios.

</details>


### [36] [Learning from Majority Label: A Novel Problem in Multi-class Multiple-Instance Learning](https://arxiv.org/abs/2509.04023)
*Shiku Kaito,Shinnosuke Matsuo,Daiki Suehiro,Ryoma Bise*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多类别多示例学习（MIL）问题，称为“学习多数标签”（LML），其中包（bag）的标签由其包含示例（instance）中的多数类决定。为了解决LML问题，论文提出了一种计数网络（Counting Network），并通过移除少数类示例来增强多数类比例的多数比例增强模块（MPEM）。实验结果表明，所提出的方法在四个数据集上优于传统的MIL方法。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决一种新的多类别多示例学习（MIL）问题，称为“学习多数标签”（LML），其中包的标签由其示例中的多数类决定，这在病理图像分割、政治投票预测、客户情绪分析和环境监测等领域具有应用价值。

Method: 提出了一种计数网络（Counting Network）来解决LML问题，该网络旨在通过计算每个类别的示例数量来估计包级别的多数标签。此外，还提出了多数比例增强模块（MPEM），通过移除包内的少数类示例来提高多数类示例的比例，以促进学习。

Result: 在四个数据集上的实验证明，所提出的方法优于传统的MIL方法，并且消融研究证实了每个模块的有效性。

Conclusion: LML问题可以通过计数网络和多数比例增强模块有效解决，该方法在多个数据集上表现出优越性。

Abstract: The paper proposes a novel multi-class Multiple-Instance Learning (MIL)
problem called Learning from Majority Label (LML). In LML, the majority class
of instances in a bag is assigned as the bag-level label. The goal of LML is to
train a classification model that estimates the class of each instance using
the majority label. This problem is valuable in a variety of applications,
including pathology image segmentation, political voting prediction, customer
sentiment analysis, and environmental monitoring. To solve LML, we propose a
Counting Network trained to produce bag-level majority labels, estimated by
counting the number of instances in each class. Furthermore, analysis
experiments on the characteristics of LML revealed that bags with a high
proportion of the majority class facilitate learning. Based on this result, we
developed a Majority Proportion Enhancement Module (MPEM) that increases the
proportion of the majority class by removing minority class instances within
the bags. Experiments demonstrate the superiority of the proposed method on
four datasets compared to conventional MIL methods. Moreover, ablation studies
confirmed the effectiveness of each module. The code is available at
\href{https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning}{here}.

</details>


### [37] [Millisecond-Response Tracking and Gazing System for UAVs: A Domestic Solution Based on "Phytium + Cambricon"](https://arxiv.org/abs/2509.04043)
*Yuchen Zhu,Longxiang Yin,Kai Zhao*

Main category: cs.CV

TL;DR: 提出一种基于国产芯片的无人机跟踪监控系统，实现了毫秒级响应和高精度识别。


<details>
  <summary>Details</summary>
Motivation: 传统视频监控技术在动态场景下响应延迟高（超过200毫秒），无法满足实时性要求，原因是自动识别算法的深度特征提取能力不足和计算架构效率低下。

Method: 设计了一个包含飞腾FT-2000/4处理器和寒武纪MLU220加速卡的异构计算架构，并结合轻量级YOLOv5s检测网络和DeepSORT跟踪算法，形成“检测-跟踪-反馈”闭环。

Result: 在1920*1080分辨率视频流处理中，实现了50-100毫秒的单帧综合处理延迟，多尺度目标识别准确率超过98.5%。

Conclusion: 该系统成功解决了传统视频监控的延迟问题，实现了低延迟和高精度的目标，为无人机监控和国产芯片应用提供了新的解决方案。

Abstract: In the frontier research and application of current video surveillance
technology, traditional camera systems exhibit significant limitations of
response delay exceeding 200 ms in dynamic scenarios due to the insufficient
deep feature extraction capability of automatic recognition algorithms and the
efficiency bottleneck of computing architectures, failing to meet the real-time
requirements in complex scenes. To address this issue, this study proposes a
heterogeneous computing architecture based on Phytium processors and Cambricon
accelerator cards, constructing a UAV tracking and gazing system with
millisecond-level response capability. At the hardware level, the system adopts
a collaborative computing architecture of Phytium FT-2000/4 processors and
MLU220 accelerator cards, enhancing computing power through multi-card
parallelism. At the software level, it innovatively integrates a lightweight
YOLOv5s detection network with a DeepSORT cascaded tracking algorithm, forming
a closed-loop control chain of "detection-tracking-feedback". Experimental
results demonstrate that the system achieves a stable single-frame
comprehensive processing delay of 50-100 ms in 1920*1080 resolution video
stream processing, with a multi-scale target recognition accuracy of over
98.5%, featuring both low latency and high precision. This study provides an
innovative solution for UAV monitoring and the application of domestic chips.

</details>


### [38] [A Re-ranking Method using K-nearest Weighted Fusion for Person Re-identification](https://arxiv.org/abs/2509.04050)
*Quang-Huy Che,Le-Chuong Nguyen,Gia-Nghia Tran,Dinh-Duy Phan,Vinh-Tiep Nguyen*

Main category: cs.CV

TL;DR: 提出一种基于K近邻加权融合（KWF）的多视角特征聚合重排方法，以解决单视角特征在行人重识别中存在的视角偏差问题。


<details>
  <summary>Details</summary>
Motivation: 行人重识别中的重排是提高整体准确性的关键步骤，但以往研究多关注单视角图像特征，易导致视角偏差、姿态变化、视点改变及遮挡等问题。

Method: 该方法通过聚合邻居特征生成多视角特征，利用K近邻加权融合（KWF）策略，在无监督情况下选择K个邻近特征来生成多视角特征，并探索了不同的权重选择策略。

Result: 在Market1501、MSMT17和Occluded-DukeMTMC数据集上进行了评估，Rank@1和mAP均得到显著提升。在MSMT17和Occluded-DukeMTMC数据集上，Rank@1分别提升了9.8%和22.0%。

Conclusion: 所提出的重排方法能够有效解决单视角特征的局限性，显著提高行人重识别的准确率，且计算效率高，适用于大规模数据集。

Abstract: In person re-identification, re-ranking is a crucial step to enhance the
overall accuracy by refining the initial ranking of retrieved results. Previous
studies have mainly focused on features from single-view images, which can
cause view bias and issues like pose variation, viewpoint changes, and
occlusions. Using multi-view features to present a person can help reduce view
bias. In this work, we present an efficient re-ranking method that generates
multi-view features by aggregating neighbors' features using K-nearest Weighted
Fusion (KWF) method. Specifically, we hypothesize that features extracted from
re-identification models are highly similar when representing the same
identity. Thus, we select K neighboring features in an unsupervised manner to
generate multi-view features. Additionally, this study explores the weight
selection strategies during feature aggregation, allowing us to identify an
effective strategy. Our re-ranking approach does not require model fine-tuning
or extra annotations, making it applicable to large-scale datasets. We evaluate
our method on the person re-identification datasets Market1501, MSMT17, and
Occluded-DukeMTMC. The results show that our method significantly improves
Rank@1 and mAP when re-ranking the top M candidates from the initial ranking
results. Specifically, compared to the initial results, our re-ranking method
achieves improvements of 9.8%/22.0% in Rank@1 on the challenging datasets:
MSMT17 and Occluded-DukeMTMC, respectively. Furthermore, our approach
demonstrates substantial enhancements in computational efficiency compared to
other re-ranking methods.

</details>


### [39] [TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph](https://arxiv.org/abs/2509.04086)
*Yaru Chen,Faegheh Sardari,Peiliang Zhang,Ruohao Guo,Yang Xiang,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 本研究提出了一种结合双向文本融合（BiT）模块和类别感知时序图（CATS）模块的新方法，以解决音频-视觉视频解析（AVVP）任务中弱监督标签带来的挑战，并在LLP和UnAV-100数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有AVVP方法在处理弱监督标签时存在不足：基于注意力的模型将嘈杂的伪标签视为可靠监督，而伪标签生成方法则可能导致错误信息在所有帧中扩散并被放大。

Method: 提出结合BiT模块和CATS模块的新方法。BiT模块用于对音频和视觉特征进行语义注入和动态校准，以提取更清晰、更丰富的语义线索。CATS模块用于语义传播和连接，实现跨时间的精确语义信息传播。

Result: 所提出的方法在LLP和UnAV-100两个基准数据集的关键指标上均实现了最先进（SOTA）的性能。

Conclusion: 本研究提出的BiT和CATS模块相结合的方法，有效解决了AVVP任务中的弱监督标签问题，并取得了优于现有方法的性能。

Abstract: Audio-Visual Video Parsing (AVVP) task aims to identify event categories and
their occurrence times in a given video with weakly supervised labels. Existing
methods typically fall into two categories: (i) designing enhanced
architectures based on attention mechanism for better temporal modeling, and
(ii) generating richer pseudo-labels to compensate for the absence of
frame-level annotations. However, the first type methods treat noisy
segment-level pseudo labels as reliable supervision and the second type methods
let indiscriminate attention spread them across all frames, the initial errors
are repeatedly amplified during training. To address this issue, we propose a
method that combines the Bi-Directional Text Fusion (BiT) module and
Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the
strengths and complementarity of the two previous research directions. We first
perform semantic injection and dynamic calibration on audio and visual modality
features through the BiT module, to locate and purify cleaner and richer
semantic cues. Then, we leverage the CATS module for semantic propagation and
connection to enable precise semantic information dissemination across time.
Experimental results demonstrate that our proposed method achieves
state-of-the-art (SOTA) performance in multiple key indicators on two benchmark
datasets, LLP and UnAV-100.

</details>


### [40] [TriLiteNet: Lightweight Model for Multi-Task Visual Perception](https://arxiv.org/abs/2509.04092)
*Quang-Huy Che,Duc-Khai Lam*

Main category: cs.CV

TL;DR: TriLiteNet是一个高效的多任务全景驾驶感知模型，能在保持低计算成本的同时实现车辆检测、可行驶区域分割和车道线分割的竞争性性能，特别适用于需要实时处理的ADAS应用。


<details>
  <summary>Details</summary>
Motivation: 当前高级驾驶辅助系统（ADAS）需要高效的感知模型来满足实时处理和响应的需求，以确保安全性和有效性。

Method: 提出TriLiteNet模型，该模型能够同时处理全景驾驶感知的多个任务，并通过优化设计来提高性能和降低计算成本。

Result: 在BDD100k数据集上，TriLiteNet_{base}在车辆检测、可行驶区域分割和车道线分割任务上取得了85.6%的召回率、92.4%的mIoU和82.3%的Acc，而参数量仅为2.35M，计算成本为7.72 GFLOPs。此外，还提出了一个包含0.14M参数的微型配置。在嵌入式设备上评估显示，TriLiteNet具有低延迟和合理的功耗。

Conclusion: TriLiteNet在性能、计算效率和可扩展性之间取得了良好平衡，为现实世界的自动驾驶应用提供了一个实用且可部署的解决方案。

Abstract: Efficient perception models are essential for Advanced Driver Assistance
Systems (ADAS), as these applications require rapid processing and response to
ensure safety and effectiveness in real-world environments. To address the
real-time execution needs of such perception models, this study introduces the
TriLiteNet model. This model can simultaneously manage multiple tasks related
to panoramic driving perception. TriLiteNet is designed to optimize performance
while maintaining low computational costs. Experimental results on the BDD100k
dataset demonstrate that the model achieves competitive performance across
three key tasks: vehicle detection, drivable area segmentation, and lane line
segmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of
85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for
drivable area segmentation, and an Acc of 82.3% for lane line segmentation with
only 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed
model includes a tiny configuration with just 0.14M parameters, which provides
a multi-task solution with minimal computational demand. Evaluated for latency
and power consumption on embedded devices, TriLiteNet in both configurations
shows low latency and reasonable power during inference. By balancing
performance, computational efficiency, and scalability, TriLiteNet offers a
practical and deployable solution for real-world autonomous driving
applications. Code is available at https://github.com/chequanghuy/TriLiteNet.

</details>


### [41] [DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset](https://arxiv.org/abs/2509.04117)
*Mustafa Sakhai,Kaung Sithu,Min Khant Soe Oke,Maciej Wielgosz*

Main category: cs.CV

TL;DR: DVS-PedX是一个包含合成和真实世界数据的神经形态数据集，用于行人检测和交叉意图分析，旨在推动事件相机在行人安全领域的研究。


<details>
  <summary>Details</summary>
Motivation: 由于事件相机（如DVS）具有低延迟、高动态范围和运动鲁棒性等优点，因此在行人检测和交叉意图分析方面具有应用潜力，但缺乏专门的数据集来推动相关研究。

Method: DVS-PedX数据集包含两部分：1. 在CARLA模拟器中生成的合成事件流，模拟了不同天气和光照条件下的“接近-穿越”场景。2. 将JAAD数据集的真实世界行车记录仪视频转换为事件流。每个序列都配有RGB帧、DVS事件帧（33毫秒累积）和帧级标签（穿越或不穿越）。还提供原始事件文件和DVS视频文件。

Result: 使用SpikingJelly实现的基线SNN模型说明了数据集的可用性，并揭示了模拟到真实（sim-to-real）的差距，这表明需要进行领域自适应和多模态融合。

Conclusion: DVS-PedX数据集的创建和发布，为事件相机在行人安全、意图预测和神经形态感知方面的研究提供了宝贵的资源，并指出了未来研究的方向，如领域自适应和多模态融合。

Abstract: Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness
changes instead of full frames, offering low latency, high dynamic range, and
motion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a
neuromorphic dataset designed for pedestrian detection and crossing-intention
analysis in normal and adverse weather conditions across two complementary
sources: (1) synthetic event streams generated in the CARLA simulator for
controlled "approach-cross" scenes under varied weather and lighting; and (2)
real-world JAAD dash-cam videos converted to event streams using the v2e tool,
preserving natural behaviors and backgrounds. Each sequence includes paired RGB
frames, per-frame DVS "event frames" (33 ms accumulations), and frame-level
labels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0
event files and AVI DVS video files and metadata for flexible re-processing.
Baseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset
usability and reveal a sim-to-real gap, motivating domain adaptation and
multimodal fusion. DVS-PedX aims to accelerate research in event-based
pedestrian safety, intention prediction, and neuromorphic perception.

</details>


### [42] [TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering](https://arxiv.org/abs/2509.04123)
*Ayan Banerjee,Josep Lladós,Umapada Pal,Anjan Dutta*

Main category: cs.CV

TL;DR: TaleDiffusion是一个新颖的框架，用于生成具有多角色故事的一致性可视化，通过迭代过程解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到故事可视化方法在保持角色一致性、减少视觉瑕疵和准确渲染对话方面存在挑战，导致故事叙述不连贯。

Method: TaleDiffusion使用预训练的LLM为故事生成每帧描述、角色细节和对话。它采用基于边界的注意力掩码技术来控制角色交互，并使用身份一致的自注意力机制来确保跨帧的角色一致性。此外，它还利用区域感知的交叉注意力进行精确的对象放置，并通过CLIPSeg将对话气泡分配给相应的角色。

Result: 实验结果表明，TaleDiffusion在一致性、降噪和对话渲染方面优于现有方法。

Conclusion: TaleDiffusion成功地生成了多角色故事的一致性可视化，解决了现有技术的不足，并在关键的评估指标上取得了优越的性能。

Abstract: Text-to-story visualization is challenging due to the need for consistent
interaction among multiple characters across frames. Existing methods struggle
with character consistency, leading to artifact generation and inaccurate
dialogue rendering, which results in disjointed storytelling. In response, we
introduce TaleDiffusion, a novel framework for generating multi-character
stories with an iterative process, maintaining character consistency, and
accurate dialogue assignment via postprocessing. Given a story, we use a
pre-trained LLM to generate per-frame descriptions, character details, and
dialogues via in-context learning, followed by a bounded attention-based
per-box mask technique to control character interactions and minimize
artifacts. We then apply an identity-consistent self-attention mechanism to
ensure character consistency across frames and region-aware cross-attention for
precise object placement. Dialogues are also rendered as bubbles and assigned
to characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion
outperforms existing methods in consistency, noise reduction, and dialogue
rendering.

</details>


### [43] [MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation](https://arxiv.org/abs/2509.04126)
*Yuan Zhao,Liu Lin*

Main category: cs.CV

TL;DR: MEPG是一个结合了位置和风格感知的大型语言模型（LLM）和空间语义专家模块的多专家规划与生成框架，旨在解决现有文本到图像模型在处理复杂多元素提示和风格多样性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在处理复杂、多元素提示和风格多样性方面存在局限性。

Method: MEPG框架包含两个核心部分：1. 位置-风格感知（PSA）模块，利用监督微调的LLM将输入提示分解为精确的空间坐标和风格编码的语义指令。2. 多专家扩散（MED）模块，通过动态专家路由实现跨区域生成，支持选择性激活特定空间分区（如真实感专家、风格化专家）的专业模型，并提供轻量级集成和可替换性。此外，还提供交互式界面以支持实时空间布局编辑和区域风格选择。

Result: 实验结果表明，MEPG在图像质量和风格多样性方面显著优于具有相同骨干网络的基线模型。

Conclusion: MEPG框架通过整合LLM和多专家模块，有效提升了文本到图像生成在处理复杂提示和风格多样性方面的能力。

Abstract: Text-to-image diffusion models have achieved remarkable image quality, but
they still struggle with complex, multiele ment prompts, and limited stylistic
diversity. To address these limitations, we propose a Multi-Expert Planning and
Gen eration Framework (MEPG) that synergistically integrates position- and
style-aware large language models (LLMs) with spatial-semantic expert modules.
The framework comprises two core components: (1) a Position-Style-Aware (PSA)
module that utilizes a supervised fine-tuned LLM to decom pose input prompts
into precise spatial coordinates and style encoded semantic instructions; and
(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera
tion through dynamic expert routing across both local regions and global areas.
During the generation process for each lo cal region, specialized models (e.g.,
realism experts, styliza tion specialists) are selectively activated for each
spatial par tition via attention-based gating mechanisms. The architec ture
supports lightweight integration and replacement of ex pert models, providing
strong extensibility. Additionally, an interactive interface enables real-time
spatial layout editing and per-region style selection from a portfolio of
experts. Ex periments show that MEPG significantly outperforms base line models
with the same backbone in both image quality
  and style diversity.

</details>


### [44] [Revisiting Simple Baselines for In-The-Wild Deepfake Detection](https://arxiv.org/abs/2509.04150)
*Orlando Castaneda,Kevin So-Tang,Kshitij Gurung*

Main category: cs.CV

TL;DR: 通过改进超参数，Ojha et al. 的方法在 Deepfake-Eval-2024 上达到了 81% 的准确率，与商业检测器相当。


<details>
  <summary>Details</summary>
Motivation: 需要为日益增长的合成媒体提供易于使用的深度伪造检测器和基准。现有研究主要在受控数据集上评估检测器，而本文关注“真实场景”的 Deepfake-Eval-2024 基准。

Method: 重新审视并改进了 Ojha et al. 提出的将预训练视觉骨干网络适应于深度伪造检测的方法，通过调整超参数来提高性能。

Result: 在 Deepfake-Eval-2024 基准上，改进后的方法达到了 81% 的准确率，比之前报告的基线方法提高了 18%，并且能够与商业深度伪造检测器竞争。

Conclusion: 经过调优的简单方法在 Deepfake-Eval-2024 上表现出色，证明了其在实际应用中的潜力。文章还讨论了准确性、计算成本和可解释性之间的权衡。

Abstract: The widespread adoption of synthetic media demands accessible deepfake
detectors and realistic benchmarks. While most existing research evaluates
deepfake detectors on highly controlled datasets, we focus on the recently
released "in-the-wild" benchmark, Deepfake-Eval-2024. Initial reporting on
Deepfake-Eval-2024 showed that three finetuned open-source models achieve
accuracies between 61% and 69%, significantly lagging behind the leading
commercial deepfake detector with 82% accuracy. Our work revisits one of these
baseline approaches, originally introduced by Ojha et al., which adapts
standard pretrained vision backbones to produce generalizable deepfake
detectors. We demonstrate that with better-tuned hyperparameters, this simple
approach actually yields much higher performance -- 81% accuracy on
Deepfake-Eval-2024 -- surpassing the previously reported accuracy of this
baseline approach by 18% and competing with commercial deepfake detectors. We
discuss tradeoffs in accuracy, computational costs, and interpretability,
focusing on how practical these deepfake detectors might be when deployed in
real-world settings. Our code can be found at
https://github.com/Deepfake-Detection-KKO/deepfake-detection.

</details>


### [45] [VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision](https://arxiv.org/abs/2509.04180)
*Safouane El Ghazouali,Umberto Michelucci*

Main category: cs.CV

TL;DR: VisioFirm是一款开源的AI辅助图像标注工具，通过集成先进的基础模型和过滤流程，大大减少了人工标注工作量（最高可达90%），同时保持了高标注准确性，支持多种标注格式和离线操作。


<details>
  <summary>Details</summary>
Motivation: 传统的图像标注工具效率低下，需要大量手动输入，限制了其在大规模数据集上的应用。因此，需要一个能够通过AI辅助自动化来简化图像标注流程的工具。

Method: VisioFirm集成了CLIP、Ultralytics和Grounding DINO等模型，利用低置信度阈值生成初始标注，并提供交互式工具进行修正。它还利用Segment Anything和WebGPU实现浏览器端的动态分割，并通过CLIP和IoU-graph优化标注准确性，支持YOLO、COCO等多种导出格式。

Result: 在COCO类型的数据集上进行测试，VisioFirm生成的初始预测大部分是正确的，用户可以通过交互式工具进行修正。该工具通过基准测试证明，在不同数据集上可将手动标注工作量减少高达90%，同时保持了高标注准确性。

Conclusion: VisioFirm通过AI辅助自动化，显著提高了图像标注的效率和准确性，解决了传统标注工具的局限性，并且易于访问和使用。

Abstract: AI models rely on annotated data to learn pattern and perform prediction.
Annotation is usually a labor-intensive step that require associating labels
ranging from a simple classification label to more complex tasks such as object
detection, oriented bounding box estimation, and instance segmentation.
Traditional tools often require extensive manual input, limiting scalability
for large datasets. To address this, we introduce VisioFirm, an open-source web
application designed to streamline image labeling through AI-assisted
automation. VisioFirm integrates state-of-the-art foundation models into an
interface with a filtering pipeline to reduce human-in-the-loop efforts. This
hybrid approach employs CLIP combined with pre-trained detectors like
Ultralytics models for common classes and zero-shot models such as Grounding
DINO for custom labels, generating initial annotations with low-confidence
thresholding to maximize recall. Through this framework, when tested on
COCO-type of classes, initial prediction have been proven to be mostly correct
though the users can refine these via interactive tools supporting bounding
boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has
on-the-fly segmentation powered by Segment Anything accelerated through WebGPU
for browser-side efficiency. The tool supports multiple export formats (YOLO,
COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing
accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort
through benchmarks on diverse datasets, while maintaining high annotation
accuracy via clustering of connected CLIP-based disambiguate components and
IoU-graph for redundant detection suppression. VisioFirm can be accessed from
\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.

</details>


### [46] [DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval](https://arxiv.org/abs/2509.04193)
*Ruohong Yang,Peng Hu,Yunfan Li,Xi Peng*

Main category: cs.CV

TL;DR: DUDE是一种新的无监督跨域图像检索（UCIR）方法，通过解耦特征来弥合域间隙，并利用文本到图像生成模型和渐进式对齐来提高检索性能，在三个基准数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的UCIR方法在对齐整个图像的跨域特征时，常常难以克服域间隙，因为对检索至关重要的对象特征经常与特定域的风格纠缠在一起。

Method: DUDE利用文本到图像生成模型来解耦对象特征与特定域的风格，然后通过渐进地对齐来自域内和跨域的互惠邻居来实现可靠的对齐。

Result: DUDE在三个基准数据集（涵盖13个域）上的广泛实验证明了其优越性，取得了最先进的性能。

Conclusion: DUDE通过特征解耦和渐进式对齐，有效解决了UCIR中的域间隙问题，提高了检索精度。

Abstract: Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images of
the same category across diverse domains without relying on annotations.
Existing UCIR methods, which align cross-domain features for the entire image,
often struggle with the domain gap, as the object features critical for
retrieval are frequently entangled with domain-specific styles. To address this
challenge, we propose DUDE, a novel UCIR method building upon feature
disentanglement. In brief, DUDE leverages a text-to-image generative model to
disentangle object features from domain-specific styles, thus facilitating
semantical image retrieval. To further achieve reliable alignment of the
disentangled object features, DUDE aligns mutual neighbors from within domains
to across domains in a progressive manner. Extensive experiments demonstrate
that DUDE achieves state-of-the-art performance across three benchmark datasets
over 13 domains. The code will be released.

</details>


### [47] [Learning Active Perception via Self-Evolving Preference Optimization for GUI Grounding](https://arxiv.org/abs/2509.04243)
*Wanfu Wang,Qipeng Huang,Guangquan Xue,Xiaobo Liang,Juntao Li*

Main category: cs.CV

TL;DR: LASER是一个用于提高视觉语言模型（VLM）在图形用户界面（GUI）基础上的推理能力的新框架，通过结合蒙特卡洛质量估计和基于交并比（IoU）的区域质量评估，实现了精确的坐标预测和多步感知能力，并在ScreenSpot Pro和ScreenSpot-v2基准测试中取得了显著的性能提升，特别是在GTA1-7B数据集上，7B规模的模型达到了55.7的新SoTA。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLM）在处理高分辨率输入和复杂交互的GUI基础任务时，尤其是在精确聚焦相关图像区域方面仍面临挑战。

Method: 提出了一种名为LASER的自进化框架，该框架集成了蒙特卡洛质量估计和基于交并比（IoU）的区域质量评估，以生成高质量的偏好数据，从而提高模型的感知和推理能力，实现精确的坐标预测和多步推理。

Result: 在ScreenSpot Pro和ScreenSpot-v2基准测试中，LASER框架取得了持续的性能提升。当在GTA1-7B上进行微调时，LASER在ScreenSpot-Pro基准测试上取得了55.7分，创下了7B规模模型的新SoTA。

Conclusion: LASER框架通过其创新的数据构建方法和多步感知能力，能够有效地提升VLM在GUI基础任务上的表现，尤其是在复杂场景下，并成功达到了新的行业标杆。

Abstract: Vision Language Models (VLMs) have recently achieved significant progress in
bridging visual perception and linguistic reasoning. Recently, OpenAI o3 model
introduced a zoom-in search strategy that effectively elicits active perception
capabilities in VLMs, improving downstream task performance. However, enabling
VLMs to reason effectively over appropriate image regions remains a core
challenge in GUI grounding, particularly under high-resolution inputs and
complex multi-element visual interactions. In this work, we propose LASER, a
self-evolving framework that progressively endows VLMs with multi-step
perception capabilities, enabling precise coordinate prediction. Specifically,
our approach integrate Monte Carlo quality estimation with
Intersection-over-Union (IoU)-based region quality evaluation to jointly
encourage both accuracy and diversity in constructing high-quality preference
data. This combination explicitly guides the model to focus on
instruction-relevant key regions while adaptively allocating reasoning steps
based on task complexity. Comprehensive experiments on the ScreenSpot Pro and
ScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating
the effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER
achieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new
state-of-the-art (SoTA) among 7B-scale models.

</details>


### [48] [Differential Morphological Profile Neural Networks for Semantic Segmentation](https://arxiv.org/abs/2509.04268)
*David Huangal,J. Alex Hurt*

Main category: cs.CV

TL;DR: 本研究将差分形态学剖面(DMP)特征整合到现代语义分割网络中，以应对遥感图像的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的语义分割网络在处理带有尺度变化、前景-背景不平衡和大图像尺寸的遥感图像时面临挑战，而DMP特征已被证明能有效提供形状信息以提升性能。

Method: 将DMP特征通过直接输入或混合架构（双流设计，融合RGB和DMP编码器）集成到三个先进的卷积和Transformer分割模型中，并评估了不同的DMP微分和结构元素。

Result: 混合DMP架构在mIoU、F1和Recall等指标上优于非DMP模型和直接输入DMP的模型。

Conclusion: 混合DMP架构能够有效提升遥感图像语义分割的性能。

Abstract: Semantic segmentation of overhead remote sensing imagery enables applications
in mapping, urban planning, and disaster response. State-of-the-art
segmentation networks are typically developed and tuned on ground-perspective
photographs and do not directly address remote sensing challenges such as
extreme scale variation, foreground-background imbalance, and large image
sizes. We explore the incorporation of the differential morphological profile
(DMP), a multi-scale shape extraction method based on grayscale morphology,
into modern segmentation networks. Prior studies have shown that the DMP can
provide critical shape information to Deep Neural Networks to enable superior
detection and classification performance in overhead imagery. In this work, we
extend prior DMPNet work beyond classification and object detection by
integrating DMP features into three state-of-the-art convolutional and
transformer semantic segmentation architectures. We utilize both direct input,
which adapts the input stem of feature extraction architectures to accept DMP
channels, and hybrid architectures, a dual-stream design that fuses RGB and DMP
encoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP
differentials and structuring element shapes to more effectively provide shape
information to the model. Our results show that while non-DMP models generally
outperform the direct-input variants, hybrid DMP consistently outperforms
direct-input and is capable of surpassing a non-DMP model on mIoU, F1, and
Recall.

</details>


### [49] [TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D Diffusion Models](https://arxiv.org/abs/2509.04269)
*Yuxin Gong,Se-in Jang,Wei Shao,Yi Su,Kuang Gong*

Main category: cs.CV

TL;DR: 通过结合结构MRI和血浆生物标志物（特别是p-tau217），利用文本引导的3D扩散模型生成3D tau PET图像，以克服tau PET扫描的成本和可及性限制，并支持数据增强和疾病模拟。


<details>
  <summary>Details</summary>
Motivation:  tau PET扫描在诊断和监测阿尔茨海默病（AD）方面至关重要，但其成本高昂且不易获得。本研究旨在利用更易获得的结构MRI和血浆生物标志物（如p-tau217）来生成tau PET图像。

Method: 提出了一种文本引导的3D扩散模型，该模型以血浆p-tau217测量作为文本提示，并结合结构MRI作为解剖结构约束，来合成3D tau PET图像。

Result: 所提出的方法在ADNI数据库的AV1451 tau PET数据上进行了训练和评估，实验结果表明该方法能够生成逼真且具有临床意义的3D tau PET图像，适用于不同的疾病阶段。

Conclusion: 该框架能够实现tau PET数据的扩增，为tau病理可视化提供非侵入性、低成本的替代方案，并支持在不同血浆生物标志物水平和认知条件下模拟疾病进展。

Abstract: Accurate quantification of tau pathology via tau positron emission tomography
(PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD).
However, the high cost and limited availability of tau PET restrict its
widespread use. In contrast, structural magnetic resonance imaging (MRI) and
plasma-based biomarkers provide non-invasive and widely available complementary
information related to brain anatomy and disease progression. In this work, we
propose a text-guided 3D diffusion model for 3D tau PET image synthesis,
leveraging multimodal conditions from both structural MRI and plasma
measurement. Specifically, the textual prompt is from the plasma p-tau217
measurement, which is a key indicator of AD progression, while MRI provides
anatomical structure constraints. The proposed framework is trained and
evaluated using clinical AV1451 tau PET data from the Alzheimer's Disease
Neuroimaging Initiative (ADNI) database. Experimental results demonstrate that
our approach can generate realistic, clinically meaningful 3D tau PET across a
range of disease stages. The proposed framework can help perform tau PET data
augmentation under different settings, provide a non-invasive, cost-effective
alternative for visualizing tau pathology, and support the simulation of
disease progression under varying plasma biomarker levels and cognitive
conditions.

</details>


### [50] [Dual-Scale Volume Priors with Wasserstein-Based Consistency for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2509.04273)
*Junying Meng,Gangxuan Zhou,Jun Liu,Weihong Guo*

Main category: cs.CV

TL;DR: 我们提出了一种半监督医学图像分割框架，该框架通过整合空间正则化和体积先验来改进特征提取和利用数据集中的先验信息。


<details>
  <summary>Details</summary>
Motivation: 现有半监督医学图像分割网络在特征提取和利用数据集先验信息方面存在不足。

Method: 该框架整合了源自变分模型的显式体积先验和阈值动态空间正则化，并将其集成到分割网络主干中。通过回归网络估计目标区域体积，并利用图像级Wasserstein距离约束来正则化主干网络。此外，还设计了一个基于弱隐式体积先验的数据集级Wasserstein距离损失函数，以匹配标记和未标记数据集的体积分布。

Result: 在2017 ACDC数据集、PROMISE12数据集和股肌核磁共振成像数据集上的实验结果证明了该方法的优越性。

Conclusion: 我们提出的半监督医学图像分割框架通过整合空间正则化和体积先验，能够有效改进分割性能。

Abstract: Despite signi cant progress in semi-supervised medical image segmentation,
most existing segmentation networks overlook e ective methodological guidance
for feature extraction and important prior information from
  datasets. In this paper, we develop a semi-supervised medical image
segmentation framework that e ectively integrates spatial regularization
methods and volume priors. Speci cally, our approach integrates a strong
explicit volume prior at the image scale and Threshold Dynamics spatial
regularization, both derived from variational models, into the backbone
segmentation network. The target region volumes for each unlabeled image are
estimated by a regression network, which e ectively regularizes the backbone
segmentation network through an image-scale Wasserstein distance constraint,
ensuring that the class ratios in the segmentation results for each unlabeled
image match those predicted by the regression network. Additionally, we design
a dataset-scale Wasserstein distance loss function based on a weak implicit
volume prior, which enforces that the volume distribution predicted for the
unlabeled dataset is similar to that of labeled dataset. Experimental results
on the 2017 ACDC dataset, PROMISE12 dataset, and thigh muscle MR image dataset
show the superiority of the proposed method.

</details>


### [51] [PAOLI: Pose-free Articulated Object Learning from Sparse-view Images](https://arxiv.org/abs/2509.04276)
*Jianning Deng,Kartic Subr,Hakan Bilen*

Main category: cs.CV

TL;DR: 该方法提出了一种新颖的自监督框架，用于从稀疏视图、未加姿势的图像中学习关节对象表示，仅需四个视图且无需相机姿势监督。


<details>
  <summary>Details</summary>
Motivation: 与需要密集多视图观测和真实相机姿势的先前方法不同，该方法解决了从稀疏视图、未加姿势的图像中学习关节对象表示的挑战。

Method: 首先，利用稀疏视图3D重建技术独立重建每个关节；然后，学习一个变形场来建立跨姿势的密集对应关系；接着，采用渐进式解耦策略分离静态和动态部分，以稳健地分离相机和物体运动；最后，通过强制跨视图和跨姿势一致性的自监督损失联合优化几何、外观和运动学。

Result: 该方法在标准基准和真实世界示例上进行了实验，证明其在比现有方法明显更弱的输入假设下，能够生成准确且详细的关节对象表示。

Conclusion: 该方法成功实现了从稀疏视图、未加姿势的图像中学习关节对象表示，显著降低了对输入数据的要求，并在准确性和细节方面取得了良好效果。

Abstract: We present a novel self-supervised framework for learning articulated object
representations from sparse-view, unposed images. Unlike prior methods that
require dense multi-view observations and ground-truth camera poses, our
approach operates with as few as four views per articulation and no camera
supervision. To address the inherent challenges, we first reconstruct each
articulation independently using recent advances in sparse-view 3D
reconstruction, then learn a deformation field that establishes dense
correspondences across poses. A progressive disentanglement strategy further
separates static from moving parts, enabling robust separation of camera and
object motion. Finally, we jointly optimize geometry, appearance, and
kinematics with a self-supervised loss that enforces cross-view and cross-pose
consistency. Experiments on the standard benchmark and real-world examples
demonstrate that our method produces accurate and detailed articulated object
representations under significantly weaker input assumptions than existing
approaches.

</details>


### [52] [Noisy Label Refinement with Semantically Reliable Synthetic Images](https://arxiv.org/abs/2509.04298)
*Yingxuan Li,Jiafeng Mao,Yusuke Matsui*

Main category: cs.CV

TL;DR: 本文提出一种利用合成图像纠正常规监督学习中图像分类数据集的语义噪声问题的方法。


<details>
  <summary>Details</summary>
Motivation: 图像分类数据集中的语义噪声（视觉相似类别被错误标注）对传统监督学习方法构成了重大挑战。

Method: 利用文本到图像模型生成的、带有可靠标签的合成图像，作为参考点来识别和纠正噪声数据集中的错误标注样本。

Result: 在多个基准数据集上的大量实验表明，该方法在各种噪声条件下显著提高了分类准确性，尤其是在语义标签噪声的挑战性场景下。与现有技术结合使用时，性能更优，在CIFAR-10和CIFAR-100上提高了30%，在ImageNet-100上提高了24%。

Conclusion: 所提出的方法能够有效利用合成图像解决语义噪声问题，并能与现有技术协同增效，显著提升图像分类性能。

Abstract: Semantic noise in image classification datasets, where visually similar
categories are frequently mislabeled, poses a significant challenge to
conventional supervised learning approaches. In this paper, we explore the
potential of using synthetic images generated by advanced text-to-image models
to address this issue. Although these high-quality synthetic images come with
reliable labels, their direct application in training is limited by domain gaps
and diversity constraints. Unlike conventional approaches, we propose a novel
method that leverages synthetic images as reliable reference points to identify
and correct mislabeled samples in noisy datasets. Extensive experiments across
multiple benchmark datasets show that our approach significantly improves
classification accuracy under various noise conditions, especially in
challenging scenarios with semantic label noise. Additionally, since our method
is orthogonal to existing noise-robust learning techniques, when combined with
state-of-the-art noise-robust training methods, it achieves superior
performance, improving accuracy by 30% on CIFAR-10 and by 11% on CIFAR-100
under 70% semantic noise, and by 24% on ImageNet-100 under real-world noise
conditions.

</details>


### [53] [Efficient Odd-One-Out Anomaly Detection](https://arxiv.org/abs/2509.04326)
*Silvio Chito,Paolo Rabino,Tatiana Tommasi*

Main category: cs.CV

TL;DR: 提出了一种基于 DINO 的模型，在 odd-one-out 异常检测任务中，参数量减少了三分之一，训练时间缩短了三分之二，同时保持了有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: Odd-one-out 异常检测任务需要跨多个视图进行空间推理和关系推理，以理解上下文并推广到不同的物体类别和布局。这些挑战需要高效地解决。

Method: 提出了一种基于 DINO 的模型，该模型参数量减少了三分之一，训练时间缩短了三分之一。

Result: 与当前最先进的模型相比，该模型在 odd-one-out 异常检测任务中，参数量减少了三分之一，训练时间缩短了三分之二，同时保持了有竞争力的性能。实验评估还引入了一个多模态大型语言模型基线，揭示了其在结构化视觉推理任务中的局限性。

Conclusion: 所提出的基于 DINO 的模型在 odd-one-out 异常检测任务中实现了高效且具有竞争力的性能。

Abstract: The recently introduced odd-one-out anomaly detection task involves
identifying the odd-looking instances within a multi-object scene. This problem
presents several challenges for modern deep learning models, demanding spatial
reasoning across multiple views and relational reasoning to understand context
and generalize across varying object categories and layouts. We argue that
these challenges must be addressed with efficiency in mind. To this end, we
propose a DINO-based model that reduces the number of parameters by one third
and shortens training time by a factor of three compared to the current
state-of-the-art, while maintaining competitive performance. Our experimental
evaluation also introduces a Multimodal Large Language Model baseline,
providing insights into its current limitations in structured visual reasoning
tasks. The project page can be found at
https://silviochito.github.io/EfficientOddOneOut/

</details>


### [54] [GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization](https://arxiv.org/abs/2509.04334)
*Pengyue Jia,Yingyi Zhang,Xiangyu Zhao,Yixuan Li*

Main category: cs.CV

TL;DR: GeoArena是一个用于评估视觉语言模型（LVLM）在图像地理定位任务上表现的开放平台，解决了现有评估方法中的数据泄露和过于依赖精确坐标的问题，并通过众包方式收集人类判断来评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像地理定位的评估方法存在数据泄露（LVLM预训练数据污染测试集）和过于依赖精确坐标（忽略推理过程、涉及隐私）的问题，需要新的评估范式。

Method: 提出GeoArena平台，一个开放的、真实的、以人类为中心的基准测试平台。用户可以上传真实场景图像，平台利用成对的人类判断来评估哪个模型的预测更符合人类期望。

Result: 平台已上线运行两个月，收集了数千条投票记录，并基于此分析建立了不同LVLM在图像地理定位任务上的排行榜。

Conclusion: GeoArena提供了一个更公平、更全面、更注重人类判断的图像地理定位评估框架，解决了现有方法的局限性，并为研究社区提供了一个有价值的基准。

Abstract: Image geolocalization aims to predict the geographic location of images
captured anywhere on Earth, but its global nature presents significant
challenges. Current evaluation methodologies suffer from two major limitations.
First, data leakage: advanced approaches often rely on large vision-language
models (LVLMs) to predict image locations, yet these models are frequently
pretrained on the test datasets, compromising the accuracy of evaluating a
model's actual geolocalization capability. Second, existing metrics primarily
rely on exact geographic coordinates to assess predictions, which not only
neglects the reasoning process but also raises privacy concerns when user-level
location data is required. To address these issues, we propose GeoArena, a
first open platform for evaluating LVLMs on worldwide image geolocalization
tasks, offering true in-the-wild and human-centered benchmarking. GeoArena
enables users to upload in-the-wild images for a more diverse evaluation
corpus, and it leverages pairwise human judgments to determine which model
output better aligns with human expectations. Our platform has been deployed
online for two months, during which we collected over thousands voting records.
Based on this data, we conduct a detailed analysis and establish a leaderboard
of different LVLMs on the image geolocalization task.

</details>


### [55] [From Editor to Dense Geometry Estimator](https://arxiv.org/abs/2509.04338)
*JiYuan Wang,Chunyu Lin,Lei Sun,Rongying Liu,Lang Nie,Mingxing Li,Kang Liao,Xiangxiang Chu,Yao Zhao*

Main category: cs.CV

TL;DR: 图像编辑模型比文本到图像生成模型更适合用于密集预测任务，并且可以更稳定地收敛以获得更高的性能。


<details>
  <summary>Details</summary>
Motivation: 由于密集预测本质上是图像到图像的任务，因此图像编辑模型可能比文本到图像生成模型更适合作为微调的基础。

Method: 本文系统地分析了用于密集几何估计的编辑模型和生成模型的微调行为。发现了编辑模型具有固有的结构先验，可以更稳定地收敛，并通过“改进”其固有的特征获得比生成模型更高的性能。在此基础上，作者提出了一种名为 FE2E 的新颖框架，该框架将基于 DiT 架构的高级编辑模型改编为密集几何预测。具体来说，为了使编辑器适用于此确定性任务，作者将编辑器原始的流匹配损失重新制定为“一致速度”训练目标，并使用对数量化来解决编辑器原生 BFloat16 格式与任务的高精度需求之间的精度冲突。此外，作者利用 DiT 的全局注意力，在单次前向传播中联合估计深度和法线，使它们的监督信号能够相互增强。

Result: FE2E 在零样本单目深度和法线估计方面取得了显着的性能提升，而无需扩大训练数据。例如，在 ETH3D 数据集上，性能提升超过 35%，并且优于在 100 倍数据上训练的 DepthAnything 系列。

Conclusion: FE2E 框架通过将图像编辑模型适应于密集几何预测任务，在不增加训练数据的情况下，在单目深度和法线估计方面取得了最先进的性能。

Abstract: Leveraging visual priors from pre-trained text-to-image (T2I) generative
models has shown success in dense prediction. However, dense prediction is
inherently an image-to-image task, suggesting that image editing models, rather
than T2I generative models, may be a more suitable foundation for fine-tuning.
  Motivated by this, we conduct a systematic analysis of the fine-tuning
behaviors of both editors and generators for dense geometry estimation. Our
findings show that editing models possess inherent structural priors, which
enable them to converge more stably by ``refining" their innate features, and
ultimately achieve higher performance than their generative counterparts.
  Based on these findings, we introduce \textbf{FE2E}, a framework that
pioneeringly adapts an advanced editing model based on Diffusion Transformer
(DiT) architecture for dense geometry prediction. Specifically, to tailor the
editor for this deterministic task, we reformulate the editor's original flow
matching loss into the ``consistent velocity" training objective. And we use
logarithmic quantization to resolve the precision conflict between the editor's
native BFloat16 format and the high precision demand of our tasks.
Additionally, we leverage the DiT's global attention for a cost-free joint
estimation of depth and normals in a single forward pass, enabling their
supervisory signals to mutually enhance each other.
  Without scaling up the training data, FE2E achieves impressive performance
improvements in zero-shot monocular depth and normal estimation across multiple
datasets. Notably, it achieves over 35\% performance gains on the ETH3D dataset
and outperforms the DepthAnything series, which is trained on 100$\times$ data.
The project page can be accessed \href{https://amap-ml.github.io/FE2E/}{here}.

</details>


### [56] [MICACL: Multi-Instance Category-Aware Contrastive Learning for Long-Tailed Dynamic Facial Expression Recognition](https://arxiv.org/abs/2509.04344)
*Feng-Qi Cui,Zhen Lin,Xinlong Rao,Anyang Tong,Shiyao Li,Fei Wang,Changlin Chen,Bin Liu*

Main category: cs.CV

TL;DR: 本论文提出了一种名为MICACL的多实例学习框架，通过整合时空依赖建模和长尾对比学习优化来解决动态面部表情识别（DFER）中的长尾分布和时空特征建模复杂性问题，并取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在处理DFER中的长尾分布和时空特征建模复杂性问题时存在模型归纳偏倚，导致性能不佳。

Method: 提出了一种名为MICACL的多实例学习框架，该框架包含图增强实例交互模块（GEIIM）用于捕捉时空依赖关系，加权实例聚合网络（WIAN）用于实例级特征聚合，以及多尺度类别感知对比学习（MCCL）策略来平衡类别不平衡问题。

Result: 在DFEW和FERV39k等真实世界数据集上的广泛实验表明，MICACL实现了最先进的性能，并具有优越的鲁棒性和泛化能力。

Conclusion: MICACL框架能够有效解决DFER中的长尾分布和时空特征建模复杂性问题，实现高性能、高鲁棒性和高泛化能力。

Abstract: Dynamic facial expression recognition (DFER) faces significant challenges due
to long-tailed category distributions and complexity of spatio-temporal feature
modeling. While existing deep learning-based methods have improved DFER
performance, they often fail to address these issues, resulting in severe model
induction bias. To overcome these limitations, we propose a novel
multi-instance learning framework called MICACL, which integrates
spatio-temporal dependency modeling and long-tailed contrastive learning
optimization. Specifically, we design the Graph-Enhanced Instance Interaction
Module (GEIIM) to capture intricate spatio-temporal between adjacent instances
relationships through adaptive adjacency matrices and multiscale convolutions.
To enhance instance-level feature aggregation, we develop the Weighted Instance
Aggregation Network (WIAN), which dynamically assigns weights based on instance
importance. Furthermore, we introduce a Multiscale Category-aware Contrastive
Learning (MCCL) strategy to balance training between major and minor
categories. Extensive experiments on in-the-wild datasets (i.e., DFEW and
FERV39k) demonstrate that MICACL achieves state-of-the-art performance with
superior robustness and generalization.

</details>


### [57] [Stitching the Story: Creating Panoramic Incident Summaries from Body-Worn Footage](https://arxiv.org/abs/2509.04370)
*Dor Cohen,Inga Efrosman,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CV

TL;DR: 本文提出了一种计算机视觉方法，将执法记录仪拍摄的视频转化为全景图，以快速总结事件现场。


<details>
  <summary>Details</summary>
Motivation: 为了解决执法记录仪视频过长、不适合在紧急情况下回顾的问题，需要一种能够快速概括事件现场的视觉摘要。

Method: 利用单目SLAM估计摄像机轨迹并重建环境空间布局，通过聚类摄像机姿态识别关键视角，选择代表性帧，并使用多帧拼接技术将这些帧融合为全景图像。

Result: 生成的信息全景图像总结了事件现场，能够快速理解复杂环境。

Conclusion: 所提出的方法通过生成信息全景图像，能够实现对事件现场的快速理解，从而提高决策效率和事件回顾的效率。

Abstract: First responders widely adopt body-worn cameras to document incident scenes
and support post-event analysis. However, reviewing lengthy video footage is
impractical in time-critical situations. Effective situational awareness
demands a concise visual summary that can be quickly interpreted. This work
presents a computer vision pipeline that transforms body-camera footage into
informative panoramic images summarizing the incident scene. Our method
leverages monocular Simultaneous Localization and Mapping (SLAM) to estimate
camera trajectories and reconstruct the spatial layout of the environment. Key
viewpoints are identified by clustering camera poses along the trajectory, and
representative frames from each cluster are selected. These frames are fused
into spatially coherent panoramic images using multi-frame stitching
techniques. The resulting summaries enable rapid understanding of complex
environments and facilitate efficient decision-making and incident review.

</details>


### [58] [AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval for Text-Based Person Anomaly Search](https://arxiv.org/abs/2509.04376)
*Hao Ju,Hu Zhang,Zhedong Zheng*

Main category: cs.CV

TL;DR: AnomalyLMM是一个新框架，首次利用大语言模型（LMM）进行基于文本的人物异常搜索，通过新颖的管道和无需训练的适应策略解决了细粒度跨模态匹配和稀疏样本识别的挑战，并在PAB数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 随着公众安全需求的增长，基于文本的人物异常搜索变得越来越重要，但该任务面临细粒度跨模态对齐和稀疏样本识别的挑战。现有的大语言模型（LMM）在理解多模态信息方面表现出色，但其在细粒度异常检索方面的潜力尚未被充分发掘，这主要是由于生成知识与判别性检索之间的领域差距以及缺乏有效的适应策略。

Method: 提出AnomalyLMM框架，采用新颖的粗到细管道，整合LMM以连接生成的世界知识和以检索为中心的异常检测。该框架还包含一个无需训练的适应方法，包括掩码跨模态提示、行为显著性预测和知识感知重新排序，以实现对细微异常线索的零样本关注。

Result: 在PAB数据集（目前唯一公开的文本人物异常搜索基准）上进行的实验表明，AnomalyLMM的有效性，其Recall@1准确率比有竞争力的基线高出+0.96%。此外，该方法揭示了文本异常和视觉行为之间可解释的对齐关系，并通过定性分析得到验证。

Conclusion: AnomalyLMM是第一个探索LMM在文本人物异常搜索任务中的应用的研究，其提出的新颖管道和适应策略有效地解决了该任务的关键挑战，并在PAB数据集上取得了优于现有方法的性能。该研究为未来利用LMM进行异常检测和搜索的研究奠定了基础。

Abstract: With growing public safety demands, text-based person anomaly search has
emerged as a critical task, aiming to retrieve individuals with abnormal
behaviors via natural language descriptions. Unlike conventional person search,
this task presents two unique challenges: (1) fine-grained cross-modal
alignment between textual anomalies and visual behaviors, and (2) anomaly
recognition under sparse real-world samples. While Large Multi-modal Models
(LMMs) excel in multi-modal understanding, their potential for fine-grained
anomaly retrieval remains underexplored, hindered by: (1) a domain gap between
generative knowledge and discriminative retrieval, and (2) the absence of
efficient adaptation strategies for deployment. In this work, we propose
AnomalyLMM, the first framework that harnesses LMMs for text-based person
anomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline
integrating LMMs to bridge generative world knowledge with retrieval-centric
anomaly detection; (2) A training-free adaptation cookbook featuring masked
cross-modal prompting, behavioral saliency prediction, and knowledge-aware
re-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study
to explore LMMs for this task, we conduct a rigorous evaluation on the PAB
dataset, the only publicly available benchmark for text-based person anomaly
search, with its curated real-world anomalies covering diverse scenarios (e.g.,
falling, collision, and being hit). Experiments show the effectiveness of the
proposed method, surpassing the competitive baseline by +0.96% Recall@1
accuracy. Notably, our method reveals interpretable alignment between textual
anomalies and visual behaviors, validated via qualitative analysis. Our code
and models will be released for future research.

</details>


### [59] [Aesthetic Image Captioning with Saliency Enhanced MLLMs](https://arxiv.org/abs/2509.04378)
*Yilin Tao,Jiashui Huang,Huaze Xu,Ling Shao*

Main category: cs.CV

TL;DR: ASE-MLLM框架通过整合图像美学显著性来提升美学图像描述生成能力，在AIC任务上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用多模态大语言模型（MLLMs）进行美学图像描述（AIC）时，主要关注美学评分预测，并且微调方法未能有效聚焦目标美学内容。作者提出ASE-MLLM框架来解决这一问题。

Method: 提出ASE-MLLM框架，包含图像美学显著性模块（IASM），用于提取图像美学显著性特征。采用IAS-ViT作为图像编码器，通过交叉注意力机制融合美学显著性特征和原始图像特征。

Result: 实验证明，ASE-MLLM显著优于传统方法和通用MLLMs，在主流AIC基准测试中达到了最先进（SOTA）的性能。

Conclusion: ASE-MLLM是首个将图像美学显著性整合到MLLMs中以专门用于AIC任务的框架，并在实验中取得了优越的性能。

Abstract: Aesthetic Image Captioning (AIC) aims to generate textual descriptions of
image aesthetics, becoming a key research direction in the field of
computational aesthetics. In recent years, pretrained Multimodal Large Language
Models (MLLMs) have advanced rapidly, leading to a significant increase in
image aesthetics research that integrates both visual and textual modalities.
However, most existing studies on image aesthetics primarily focus on
predicting aesthetic ratings and have shown limited application in AIC.
Existing AIC works leveraging MLLMs predominantly rely on fine-tuning methods
without specifically adapting MLLMs to focus on target aesthetic content. To
address this limitation, we propose the Aesthetic Saliency Enhanced Multimodal
Large Language Model (ASE-MLLM), an end-to-end framework that explicitly
incorporates aesthetic saliency into MLLMs. Within this framework, we introduce
the Image Aesthetic Saliency Module (IASM), which efficiently and effectively
extracts aesthetic saliency features from images. Additionally, we design
IAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency
features with original image features via a cross-attention mechanism. To the
best of our knowledge, ASE-MLLM is the first framework to integrate image
aesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments
demonstrated that our approach significantly outperformed traditional methods
and generic MLLMs on current mainstream AIC benchmarks, achieving
state-of-the-art (SOTA) performance.

</details>


### [60] [SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer](https://arxiv.org/abs/2509.04379)
*Jimin Xu,Bosheng Qin,Tao Jin,Zhou Zhao,Zhenhui Ye,Jun Yu,Fei Wu*

Main category: cs.CV

TL;DR: 提出了一种新的3D风格迁移管线，该管线通过整合预训练的2D扩散模型先验知识，解决了现有方法在提取高层风格语义和保持3D场景结构清晰度方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有3D风格迁移方法难以有效提取和迁移高层风格语义，且风格化结果缺乏结构清晰度，难以区分场景中的不同实例或对象。

Method: 提出了一种新的3D风格迁移管线，包含两个关键阶段：1. 利用扩散先验生成关键视角的风格化渲染图。2. 将风格化后的关键视角迁移到3D表示上。该流程包含两个创新设计：交叉视角风格对齐（在UNet的最后一个上采样块中引入交叉视角注意力，实现跨多个关键视角的特征交互）和实例级风格迁移（利用风格化关键视角之间的实例级一致性，将其迁移到3D表示上）。

Result: 提出的3D风格迁移管线能够生成更具结构性、视觉连贯性和艺术性的风格化3D场景，显著优于现有最先进的方法，并且能够处理从前向渲染到具有挑战性的360度环境的各种场景。

Conclusion: 该研究提出的3D风格迁移管线通过整合2D扩散模型先验，并采用交叉视角风格对齐和实例级风格迁移等创新技术，有效解决了现有方法的局限性，实现了高质量的3D场景风格化。

Abstract: Recent advancements in neural representations, such as Neural Radiance Fields
and 3D Gaussian Splatting, have increased interest in applying style transfer
to 3D scenes. While existing methods can transfer style patterns onto
3D-consistent neural representations, they struggle to effectively extract and
transfer high-level style semantics from the reference style image.
Additionally, the stylized results often lack structural clarity and
separation, making it difficult to distinguish between different instances or
objects within the 3D scene. To address these limitations, we propose a novel
3D style transfer pipeline that effectively integrates prior knowledge from
pretrained 2D diffusion models. Our pipeline consists of two key stages: First,
we leverage diffusion priors to generate stylized renderings of key viewpoints.
Then, we transfer the stylized key views onto the 3D representation. This
process incorporates two innovative designs. The first is cross-view style
alignment, which inserts cross-view attention into the last upsampling block of
the UNet, allowing feature interactions across multiple key views. This ensures
that the diffusion model generates stylized key views that maintain both style
fidelity and instance-level consistency. The second is instance-level style
transfer, which effectively leverages instance-level consistency across
stylized key views and transfers it onto the 3D representation. This results in
a more structured, visually coherent, and artistically enriched stylization.
Extensive qualitative and quantitative experiments demonstrate that our 3D
style transfer pipeline significantly outperforms state-of-the-art methods
across a wide range of scenes, from forward-facing to challenging 360-degree
environments. Visit our project page https://jm-xu.github.io/SSGaussian for
immersive visualization.

</details>


### [61] [Learning neural representations for X-ray ptychography reconstruction with unknown probes](https://arxiv.org/abs/2509.04402)
*Tingyou Li,Zixin Xu,Zirui Gao,Hanfei Yan,Xiaojing Huang,Jizhou Li*

Main category: cs.CV

TL;DR: PtyINR是一个用于X射线图像重建的自监督框架，能够同时恢复未知照明探针下的物体和探针信息，尤其在低信号条件下表现出色。


<details>
  <summary>Details</summary>
Motivation: X射线衍射成像技术在材料科学、生物学和纳米技术中有广泛应用，但其成像质量受限于未知照明探针的准确重建，尤其是在低剂量和高速实验条件下，现有方法效果不佳。

Method: 提出了一种名为PtyINR（Ptychographic Implicit Neural Representation）的自监督框架，将物体和探针参数化为连续的神经网络表示，直接从原始衍射图谱进行端到端重建，无需预先表征探针。

Result: 在模拟和实验数据上都取得了优于现有方法的重建质量，并且在低信号条件下具有出色的鲁棒性。

Conclusion: PtyINR提供了一个可推广、遵循物理规律的框架，能够解决依赖探针的逆问题，适用于多种计算显微镜问题。

Abstract: X-ray ptychography provides exceptional nanoscale resolution and is widely
applied in materials science, biology, and nanotechnology. However, its full
potential is constrained by the critical challenge of accurately reconstructing
images when the illuminating probe is unknown. Conventional iterative methods
and deep learning approaches are often suboptimal, particularly under the
low-signal conditions inherent to low-dose and high-speed experiments. These
limitations compromise reconstruction fidelity and restrict the broader
adoption of the technique. In this work, we introduce the Ptychographic
Implicit Neural Representation (PtyINR), a self-supervised framework that
simultaneously addresses the object and probe recovery problem. By
parameterizing both as continuous neural representations, PtyINR performs
end-to-end reconstruction directly from raw diffraction patterns without
requiring any pre-characterization of the probe. Extensive evaluations
demonstrate that PtyINR achieves superior reconstruction quality on both
simulated and experimental data, with remarkable robustness under challenging
low-signal conditions. Furthermore, PtyINR offers a generalizable,
physics-informed framework for addressing probe-dependent inverse problems,
making it applicable to a wide range of computational microscopy problems.

</details>


### [62] [Self-adaptive Dataset Construction for Real-World Multimodal Safety Scenarios](https://arxiv.org/abs/2509.04403)
*Jingen Qu,Lijun Li,Bo Zhang,Yichen Yan,Jing Shao*

Main category: cs.CV

TL;DR: 本篇论文提出了一种新的图像导向的自适应数据集构建方法，以应对多模态大语言模型（MLLMs）日益复杂的安全挑战。该方法能够自动生成包含35k图像-文本对及指导性回复的真实世界多模态安全（RMS）数据集。同时，论文还引入了一个标准化的安全数据集评估指标，通过微调安全评判模型并在其他安全数据集上进行评估来验证其能力。实验结果证明了该图像导向方法在RMS数据集构建上的有效性、可扩展性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的面向风险的数据集构建方法未能覆盖真实世界多模态安全场景（RMS）日益增长的复杂性，并且缺乏统一的评估指标来衡量其有效性。

Method: 提出了一种新颖的图像导向的自适应数据集构建方法，以图像为起点，生成配对的文本和指导性回复，从而构建RMS数据集。此外，还引入了一种标准化的安全数据集评估指标，即微调一个安全评判模型并在其他安全数据集上评估其能力。

Result: 通过图像导向的方法自动生成了一个包含35k图像-文本对及指导性回复的RMS数据集。实验证明了该图像导向流水线的有效性、可扩展性和优越性。

Conclusion: 所提出的图像导向方法为构建真实世界多模态安全数据集提供了一个新的视角，证明了其有效性和可扩展性。

Abstract: Multimodal large language models (MLLMs) are rapidly evolving, presenting
increasingly complex safety challenges. However, current dataset construction
methods, which are risk-oriented, fail to cover the growing complexity of
real-world multimodal safety scenarios (RMS). And due to the lack of a unified
evaluation metric, their overall effectiveness remains unproven. This paper
introduces a novel image-oriented self-adaptive dataset construction method for
RMS, which starts with images and end constructing paired text and guidance
responses. Using the image-oriented method, we automatically generate an RMS
dataset comprising 35k image-text pairs with guidance responses. Additionally,
we introduce a standardized safety dataset evaluation metric: fine-tuning a
safety judge model and evaluating its capabilities on other safety
datasets.Extensive experiments on various tasks demonstrate the effectiveness
of the proposed image-oriented pipeline. The results confirm the scalability
and effectiveness of the image-oriented approach, offering a new perspective
for the construction of real-world multimodal safety datasets.

</details>


### [63] [Few-step Flow for 3D Generation via Marginal-Data Transport Distillation](https://arxiv.org/abs/2509.04406)
*Zanwei Zhou,Taoran Yi,Jiemin Fang,Chen Yang,Lingxi Xie,Xinggang Wang,Wei Shen,Qi Tian*

Main category: cs.CV

TL;DR: MDT-dist是一种新颖的框架，用于加速基于流的3D生成模型，通过蒸馏预训练模型来学习边际数据传输，并提出速度匹配（VM）和速度蒸馏（VD）两个可优化目标来实现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流的3D生成模型在推理时需要大量的采样步骤，而针对复杂3D生成任务的少步蒸馏方法（如一致性模型）的探索不足。

Method: 提出MDT-dist框架，通过学习边际数据传输，并引入速度匹配（VM）和速度蒸馏（VD）两个可优化目标来解决积分不可行的问题。VM匹配学生和教师的速度场，VD则利用学习到的速度场进行概率密度蒸馏。

Result: 在使用TRELLIS框架进行评估时，MDT-dist将每个流变换器的采样步骤从25步减少到1-2步，在A800上实现了0.68秒（1步x2）和0.94秒（2步x2）的低延迟，分别实现了9.0倍和6.5倍的加速，同时保持了高质量的视觉和几何保真度。

Conclusion: MDT-dist在少步3D生成方面显著优于现有的CM蒸馏方法，并显著提升了TRELLIS框架的性能。

Abstract: Flow-based 3D generation models typically require dozens of sampling steps
during inference. Though few-step distillation methods, particularly
Consistency Models (CMs), have achieved substantial advancements in
accelerating 2D diffusion models, they remain under-explored for more complex
3D generation tasks. In this study, we propose a novel framework, MDT-dist, for
few-step 3D flow distillation. Our approach is built upon a primary objective:
distilling the pretrained model to learn the Marginal-Data Transport. Directly
learning this objective needs to integrate the velocity fields, while this
integral is intractable to be implemented. Therefore, we propose two
optimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),
to equivalently convert the optimization target from the transport level to the
velocity and the distribution level respectively. Velocity Matching (VM) learns
to stably match the velocity fields between the student and the teacher, but
inevitably provides biased gradient estimates. Velocity Distillation (VD)
further enhances the optimization process by leveraging the learned velocity
fields to perform probability density distillation. When evaluated on the
pioneer 3D generation framework TRELLIS, our method reduces sampling steps of
each flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s
(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high
visual and geometric fidelity. Extensive experiments demonstrate that our
method significantly outperforms existing CM distillation methods, and enables
TRELLIS to achieve superior performance in few-step 3D generation.

</details>


### [64] [The Telephone Game: Evaluating Semantic Drift in Unified Models](https://arxiv.org/abs/2509.04438)
*Sabbir Mollah,Rohit Gupta,Sirnam Swetha,Qingyang Liu,Ahnaf Munir,Mubarak Shah*

Main category: cs.CV

TL;DR: 本文提出了一种名为 UCF-UM 的评估框架，用于衡量统一模型在图像到文本（I2T）和文本到图像（T2I）任务之间的一致性，解决了现有评估指标无法量化跨模态循环生成中语义漂移的问题。UCF-UM 包含三种新指标：MCD、SDR 和 MGG，并在新数据集 ND400 上评估了七种模型，揭示了模型在跨模态稳定性上的显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法分别评估 I2T 和 T2I 能力，未能衡量模型在理解和生成之间的连贯性，也无法量化在跨模态循环过程中语义的保持程度。

Method: 提出 UCF-UM 评估协议，通过多轮 I2T 和 T2I 交替生成来量化语义漂移，并引入三种新指标：MCD（平均累积漂移）、SDR（语义漂移率）和 MGG（多代生成 GenEval）。此外，创建了 ND400 数据集以评估模型在 COCO 之外的泛化能力。

Result: UCF-UM 评估揭示了模型在跨模态稳定性方面存在显著差异，例如 BAGEL 模型能很好地保持语义，而 Vila-u 模型即使在单次评估中表现良好，也很快出现语义漂移。ND400 数据集评估结果也展示了这一点。

Conclusion: 循环一致性是 I2T 和 T2I 标准评估的必要补充，UCF-UM 提供了量化统一模型跨模态稳定性和共享表征强度的一致性评估方法。

Abstract: Employing a single, unified model (UM) for both visual understanding
(image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened
a new direction in Visual Language Model (VLM) research. While UMs can also
support broader unimodal tasks (e.g., text-to-text, image-to-image), we focus
on the core cross-modal pair T2I and I2T, as consistency between understanding
and generation is critical for downstream use. Existing evaluations consider
these capabilities in isolation: FID and GenEval for T2I, and benchmarks such
as MME, MMBench for I2T. These single-pass metrics do not reveal whether a
model that understands a concept can also render it, nor whether meaning is
preserved when cycling between image and text modalities. To address this, we
introduce the Unified Consistency Framework for Unified Models (UCF-UM), a
cyclic evaluation protocol that alternates I2T and T2I over multiple
generations to quantify semantic drift. UCF formulates 3 metrics: (i) Mean
Cumulative Drift (MCD), an embedding-based measure of overall semantic loss;
(ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii)
Multi-Generation GenEval (MGG), an object-level compliance score extending
GenEval. To assess generalization beyond COCO, which is widely used in
training; we create a new benchmark ND400, sampled from NoCaps and DOCCI and
evaluate on seven recent models. UCF-UM reveals substantial variation in
cross-modal stability: some models like BAGEL maintain semantics over many
alternations, whereas others like Vila-u drift quickly despite strong
single-pass scores. Our results highlight cyclic consistency as a necessary
complement to standard I2T and T2I evaluations, and provide practical metrics
to consistently assess unified model's cross-modal stability and strength of
their shared representations. Code:
https://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models

</details>


### [65] [Durian: Dual Reference-guided Portrait Animation with Attribute Transfer](https://arxiv.org/abs/2509.04434)
*Hyunsoo Cha,Byungjun Kim,Hanbyul Joo*

Main category: cs.CV

TL;DR: Durian是一种零样本肖像动画生成方法，通过双参考网络和自重构公式，实现了高保真、空间一致的面部属性转移，并支持多属性组合。


<details>
  <summary>Details</summary>
Motivation: 提出一种零样本肖像动画生成方法，实现面部属性从参考图像到目标肖像的高保真、空间一致的转移。

Method: 引入双参考网络，将肖像和属性图像的空间特征注入扩散模型的去噪过程。使用自重构公式进行训练，其中一帧作为属性参考，另一帧作为目标肖像，并利用掩码重建剩余帧。提出掩码扩展策略（使用关键点条件图像生成）来支持具有不同空间范围的属性转移。对属性和肖像图像进行空间和外观变换增强，以提高对齐鲁棒性。

Result: Durian在肖像动画与属性转移方面取得了最先进的性能，并且其双参考设计能够在一个生成过程中组合多个属性，无需额外训练。

Conclusion: Durian是首个实现零样本肖像动画生成的方法，通过创新的双参考网络和训练策略，有效解决了面部属性转移的挑战，并具备多属性组合能力。

Abstract: We present Durian, the first method for generating portrait animation videos
with facial attribute transfer from a given reference image to a target
portrait in a zero-shot manner. To enable high-fidelity and spatially
consistent attribute transfer across frames, we introduce dual reference
networks that inject spatial features from both the portrait and attribute
images into the denoising process of a diffusion model. We train the model
using a self-reconstruction formulation, where two frames are sampled from the
same portrait video: one is treated as the attribute reference and the other as
the target portrait, and the remaining frames are reconstructed conditioned on
these inputs and their corresponding masks. To support the transfer of
attributes with varying spatial extent, we propose a mask expansion strategy
using keypoint-conditioned image generation for training. In addition, we
further augment the attribute and portrait images with spatial and
appearance-level transformations to improve robustness to positional
misalignment between them. These strategies allow the model to effectively
generalize across diverse attributes and in-the-wild reference combinations,
despite being trained without explicit triplet supervision. Durian achieves
state-of-the-art performance on portrait animation with attribute transfer, and
notably, its dual reference design enables multi-attribute composition in a
single generation pass without additional training.

</details>


### [66] [From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray Collimators via Hough Transform](https://arxiv.org/abs/2509.04437)
*Benjamin El-Zein,Dominik Eckert,Andreas Fieselmann,Christopher Syben,Ludwig Ritschl,Steffen Kappler,Sebastian Stober*

Main category: cs.CV

TL;DR: 深度学习模型通过结合霍夫变换来分割X射线图像中的重叠区域，即使在边缘模糊的情况下也能鲁棒地重建重叠区域，中位豪斯多夫距离为4.3-5.0毫米。


<details>
  <summary>Details</summary>
Motivation: X射线成像中的准直对于将辐射限制在感兴趣区域（ROI）并最小化患者的辐射剂量至关重要。然而，散射X射线辐射会导致准直器阴影边缘模糊，使得检测准直器阴影成为一项挑战。

Method: 提出一种深度学习分割方法，通过整合可微分霍夫变换网络来检测重叠边界和提取ROI中心信息，并在推理时结合两项任务的信息来生成精细的、受线条约束的分割掩码。

Result: 在多样化的真实X射线图像测试集上，实现了对重叠区域的鲁棒重建，中位豪斯多夫距离为4.3-5.0毫米。

Conclusion: 所提出的基于深度学习的方法能够有效地分割X射线图像中的重叠区域，即使在存在散射辐射的情况下也能实现高精度，并且该方法没有限制于特定数量的边缘。

Abstract: Collimation in X-ray imaging restricts exposure to the region-of-interest
(ROI) and minimizes the radiation dose applied to the patient. The detection of
collimator shadows is an essential image-based preprocessing step in digital
radiography posing a challenge when edges get obscured by scattered X-ray
radiation. Regardless, the prior knowledge that collimation forms
polygonal-shaped shadows is evident. For this reason, we introduce a deep
learning-based segmentation that is inherently constrained to its geometry. We
achieve this by incorporating a differentiable Hough transform-based network to
detect the collimation borders and enhance its capability to extract the
information about the ROI center. During inference, we combine the information
of both tasks to enable the generation of refined, line-constrained
segmentation masks. We demonstrate robust reconstruction of collimated regions
achieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real
Xray images. While this application involves at most four shadow borders, our
method is not fundamentally limited by a specific number of edges.

</details>


### [67] [One Flight Over the Gap: A Survey from Perspective to Panoramic Vision](https://arxiv.org/abs/2509.04444)
*Xin Lin,Xian Ge,Dizhe Zhang,Zhaoliang Wan,Xianshun Wang,Xiangtai Li,Wenjie Jiang,Bo Du,Dacheng Tao,Ming-Hsuan Yang,Lu Qi*

Main category: cs.CV

TL;DR: 本篇论文是关于全景视觉技术的调研，重点关注从透视图像到全景图像的领域自适应问题，并对该领域的研究进行了全面的总结和展望。


<details>
  <summary>Details</summary>
Motivation: 全景图像（ODIs）因其完整的360度视野在虚拟现实、自动驾驶和机器人等领域的需求日益增长，但其独特的几何投影、空间分布和边界连续性等特性给现有基于透视图像的方法带来了直接应用上的挑战。

Method: 论文首先回顾了全景成像流程和投影方法，以建立分析结构差异所需的先验知识。接着，总结了全景图像领域自适应面临的三大挑战：极点附近的严重几何畸变、等距柱状投影（ERP）中的非均匀采样以及周期性边界连续性。在此基础上，论文对20多个代表性任务进行了梳理，并从跨方法和跨任务两个维度进行了分析。最后，讨论了数据、模型和应用方面存在的开放性挑战及未来发展方向。

Result: 论文对300多篇研究论文进行了梳理，总结了解决全景图像特定挑战的代表性策略，并将全景视觉研究分为视觉质量增强与评估、视觉理解、多模态理解和视觉生成四大类别。项目页面提供了更多资源。

Conclusion: 本研究旨在为全景视觉领域的研究者提供新的见解和前瞻性视角，以推动全景视觉技术的发展。

Abstract: Driven by the demand for spatial intelligence and holistic scene perception,
omnidirectional images (ODIs), which provide a complete 360\textdegree{} field
of view, are receiving growing attention across diverse applications such as
virtual reality, autonomous driving, and embodied robotics. Despite their
unique characteristics, ODIs exhibit remarkable differences from perspective
images in geometric projection, spatial distribution, and boundary continuity,
making it challenging for direct domain adaption from perspective methods. This
survey reviews recent panoramic vision techniques with a particular emphasis on
the perspective-to-panorama adaptation. We first revisit the panoramic imaging
pipeline and projection methods to build the prior knowledge required for
analyzing the structural disparities. Then, we summarize three challenges of
domain adaptation: severe geometric distortions near the poles, non-uniform
sampling in Equirectangular Projection (ERP), and periodic boundary continuity.
Building on this, we cover 20+ representative tasks drawn from more than 300
research papers in two dimensions. On one hand, we present a cross-method
analysis of representative strategies for addressing panoramic specific
challenges across different tasks. On the other hand, we conduct a cross-task
comparison and classify panoramic vision into four major categories: visual
quality enhancement and assessment, visual understanding, multimodal
understanding, and visual generation. In addition, we discuss open challenges
and future directions in data, models, and applications that will drive the
advancement of panoramic vision research. We hope that our work can provide new
insight and forward looking perspectives to advance the development of
panoramic vision technologies. Our project page is
https://insta360-research-team.github.io/Survey-of-Panorama

</details>


### [68] [Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.04446)
*Kiymet Akdemir,Jing Shi,Kushal Kafle,Brian Price,Pinar Yanardag*

Main category: cs.CV

TL;DR: Plot'n Polish是一个零样本框架，可以对故事可视化进行一致的生成和细粒度控制，以解决现有方法在保持跨多帧的视觉和叙事一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在故事可视化方面有潜力，但缺乏在生成后进行一致修改的灵活性，这阻碍了创作者进行无缝的视觉故事创作和改进。因此，需要一种能够提供增强控制、改进和跨多帧保持视觉和叙事一致性的方法。

Method: 提出Plot'n Polish，一个零样本框架，可以对故事可视化进行一致的生成和细粒度控制。

Result: Plot'n Polish解决了现有方法在保持跨多帧的视觉和叙事一致性方面的不足，并能够进行细粒度或粗粒度的编辑。

Conclusion: Plot'n Polish框架通过提供细粒度的控制和跨多帧的视觉叙事一致性，为文本到图像生成模型在故事可视化领域的应用提供了解决方案。

Abstract: Text-to-image diffusion models have demonstrated significant capabilities to
generate diverse and detailed visuals in various domains, and story
visualization is emerging as a particularly promising application. However, as
their use in real-world creative domains increases, the need for providing
enhanced control, refinement, and the ability to modify images post-generation
in a consistent manner becomes an important challenge. Existing methods often
lack the flexibility to apply fine or coarse edits while maintaining visual and
narrative consistency across multiple frames, preventing creators from
seamlessly crafting and refining their visual stories. To address these
challenges, we introduce Plot'n Polish, a zero-shot framework that enables
consistent story generation and provides fine-grained control over story
visualizations at various levels of detail.

</details>


### [69] [TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection](https://arxiv.org/abs/2509.04448)
*Zehong Yan,Peng Qi,Wynne Hsu,Mong Li Lee*

Main category: cs.CV

TL;DR: 多模态错误信息（文本、视觉、跨模态）日益严重，生成式AI加剧了这一威胁。现有方法通常只关注单一类型的失真，难以泛化到未见过的场景。本研究提出TRUST-VL，一个统一且可解释的视觉-语言模型，用于通用多模态错误信息检测。TRUST-VL包含新颖的“问题感知视觉放大器”模块，用于提取特定任务的视觉特征。为支持训练，还构建了TRUST-Instruct数据集（198K样本），包含与人类事实核查流程一致的结构化推理链。实验证明，TRUST-VL在领域内和零样本基准测试中均达到最先进性能，并具有良好的泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态错误信息检测方法通常只关注单一类型的失真，并且难以泛化到未知的场景。

Method: 提出TRUST-VL，一个统一且可解释的视觉-语言模型，用于通用多模态错误信息检测。TRUST-VL包含新颖的“问题感知视觉放大器”模块，用于提取特定任务的视觉特征。同时构建了TRUST-Instruct数据集（198K样本），包含与人类事实核查流程一致的结构化推理链。

Result: 在领域内和零样本基准测试中，TRUST-VL均达到了最先进的性能，并且具有良好的泛化性和可解释性。

Conclusion: TRUST-VL通过联合训练和新颖的模块设计，有效地解决了多模态错误信息检测的挑战，并展现出优越的性能和泛化能力。

Abstract: Multimodal misinformation, encompassing textual, visual, and cross-modal
distortions, poses an increasing societal threat that is amplified by
generative AI. Existing methods typically focus on a single type of distortion
and struggle to generalize to unseen scenarios. In this work, we observe that
different distortion types share common reasoning capabilities while also
requiring task-specific skills. We hypothesize that joint training across
distortion types facilitates knowledge sharing and enhances the model's ability
to generalize. To this end, we introduce TRUST-VL, a unified and explainable
vision-language model for general multimodal misinformation detection. TRUST-VL
incorporates a novel Question-Aware Visual Amplifier module, designed to
extract task-specific visual features. To support training, we also construct
TRUST-Instruct, a large-scale instruction dataset containing 198K samples
featuring structured reasoning chains aligned with human fact-checking
workflows. Extensive experiments on both in-domain and zero-shot benchmarks
demonstrate that TRUST-VL achieves state-of-the-art performance, while also
offering strong generalization and interpretability.

</details>


### [70] [Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview](https://arxiv.org/abs/2509.04450)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: VFR是一个新的视频生成模型，可以生成任意长度的虚拟试穿视频，通过分段自回归生成，解决了长视频生成中的局部平滑和全局时间一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的虚拟试穿视频生成方法在生成长视频时存在资源消耗大和需要长视频数据的问题，本文旨在解决这些挑战，实现任意长度虚拟试穿视频的生成。

Method: VFR模型将长视频生成任务视为一个分段的自回归生成过程，通过引入前缀视频（prefix video）来保证相邻片段之间的局部平滑性，并通过参考锚定视频（anchor video）来维持全局时间一致性。

Result: VFR模型能够生成分钟级长度的虚拟试穿视频，并且在各种运动下都能保持良好的局部平滑性和全局时间一致性。

Conclusion: VFR是长视频虚拟试穿生成领域的开创性工作，成功解决了长视频生成中的关键挑战，实现了任意长度、高一致性的虚拟试穿视频生成。

Abstract: We introduce the Virtual Fitting Room (VFR), a novel video generative model
that produces arbitrarily long virtual try-on videos. Our VFR models long video
generation tasks as an auto-regressive, segment-by-segment generation process,
eliminating the need for resource-intensive generation and lengthy video data,
while providing the flexibility to generate videos of arbitrary length. The key
challenges of this task are twofold: ensuring local smoothness between adjacent
segments and maintaining global temporal consistency across different segments.
To address these challenges, we propose our VFR framework, which ensures
smoothness through a prefix video condition and enforces consistency with the
anchor video -- a 360-degree video that comprehensively captures the human's
wholebody appearance. Our VFR generates minute-scale virtual try-on videos with
both local smoothness and global temporal consistency under various motions,
making it a pioneering work in long virtual try-on video generation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [71] [Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies](https://arxiv.org/abs/2509.03525)
*Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sepehr Karimi,Sina Rashidi,Ali Zolnour,Maryam Dadkhah,Yasaman Haghbin,Hossein AzadMaleki,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 美国超过一半的阿尔茨海默病患者未被确诊，基于语音的筛查是一种可行的检测方法。本研究比较了用于痴呆症检测的大型语言模型（LLM）改编策略，评估了九种纯文本模型和三种多模态音-文本模型在 DementiaBank 语音语料库上的表现。实验考虑了不同的演示选择策略的上下文学习、增强推理的提示、参数高效微调以及多模态集成。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病筛查的必要性和现有方法的局限性，提出使用基于语音的大型语言模型进行筛查。

Method: 比较了九种纯文本模型和三种多模态音-文本模型在 DementiaBank 语音语料库上的表现，并探索了不同的适应策略，包括上下文学习、推理增强提示、参数高效微调和多模态集成。

Result: 在上下文学习方面，类别中心演示取得了最佳性能；推理增强了较小模型的性能；令牌级微调通常获得最佳分数；添加分类头显著提高了表现不佳的模型；多模态模型表现良好，但未超越顶尖的纯文本模型。

Conclusion: 模型适应策略（包括演示选择、推理设计和微调方法）对基于语音的痴呆症检测至关重要。经过适当调整的开源模型可以媲美甚至超越商业系统。

Abstract: Over half of US adults with Alzheimer disease and related dementias remain
undiagnosed, and speech-based screening offers a scalable detection approach.
We compared large language model adaptation strategies for dementia detection
using the DementiaBank speech corpus, evaluating nine text-only models and
three multimodal audio-text models on recordings from DementiaBank speech
corpus. Adaptations included in-context learning with different demonstration
selection policies, reasoning-augmented prompting, parameter-efficient
fine-tuning, and multimodal integration. Results showed that class-centroid
demonstrations achieved the highest in-context learning performance, reasoning
improved smaller models, and token-level fine-tuning generally produced the
best scores. Adding a classification head substantially improved
underperforming models. Among multimodal models, fine-tuned audio-text systems
performed well but did not surpass the top text-only models. These findings
highlight that model adaptation strategies, including demonstration selection,
reasoning design, and tuning method, critically influence speech-based dementia
detection, and that properly adapted open-weight models can match or exceed
commercial systems.

</details>


### [72] [Explicit and Implicit Data Augmentation for Social Event Detection](https://arxiv.org/abs/2509.04202)
*Congbo Ma,Yuxia Wang,Jia Wu,Jian Yang,Jing Du,Zitai Qiu,Qing Li,Hu Wang,Preslav Nakov*

Main category: cs.CL

TL;DR: 通过显式文本增强和隐式特征空间增强相结合的双重增强框架SED-Aug，解决了社交事件检测中的数据标注成本高和劳动密集问题，并在Twitter2012和Twitter2018数据集上显著提高了F1分数。


<details>
  <summary>Details</summary>
Motivation: 社交事件检测依赖于标注数据，但标注成本高且劳动密集。

Method: 提出了一种名为SED-Aug的即插即用型双重增强框架，结合了显式文本增强（使用大型语言模型通过五种生成策略增强文本信息）和隐式特征空间增强（设计了五种在结构融合嵌入上操作的新型扰动技术，以保持嵌入的语义和关系属性并增加其多样性）。

Result: SED-Aug在Twitter2012数据集上的平均F1分数比最佳基线模型高出约17.67%，在Twitter2018数据集上则高出约15.57%。

Conclusion: SED-Aug通过增强数据多样性和模型鲁棒性，有效解决了社交事件检测中的数据稀疏问题，并取得了显著的性能提升。

Abstract: Social event detection involves identifying and categorizing important events
from social media, which relies on labeled data, but annotation is costly and
labor-intensive. To address this problem, we propose Augmentation framework for
Social Event Detection (SED-Aug), a plug-and-play dual augmentation framework,
which combines explicit text-based and implicit feature-space augmentation to
enhance data diversity and model robustness. The explicit augmentation utilizes
large language models to enhance textual information through five diverse
generation strategies. For implicit augmentation, we design five novel
perturbation techniques that operate in the feature space on structural fused
embeddings. These perturbations are crafted to keep the semantic and relational
properties of the embeddings and make them more diverse. Specifically, SED-Aug
outperforms the best baseline model by approximately 17.67% on the Twitter2012
dataset and by about 15.57% on the Twitter2018 dataset in terms of the average
F1 score. The code is available at GitHub: https://github.com/congboma/SED-Aug.

</details>


### [73] [Enhancing Speech Large Language Models through Reinforced Behavior Alignment](https://arxiv.org/abs/2509.03526)
*Yansong Liu,Jiateng Li,Yuan Liu*

Main category: cs.CL

TL;DR: 该研究提出了一种名为“强化行为对齐”（RBA）的框架，通过利用强大的教师 LLM 自我合成数据，并结合强化学习来提升语音 LLM 的指令遵循能力，解决了语音 LLM 在处理动态用户语音时指令遵循能力不足的问题，并取得了优于传统蒸馏基线的效果，同时在语音问答和语音到文本翻译等任务上也达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的语音 LLM 在指令遵循方面与文本 LLM 相比存在显著的性能差距，尤其是在处理动态和多变的语音输入时，这是由于跨模态的差异性造成的。因此，需要一种新的方法来提高语音 LLM 的语言生成能力。

Method: 提出了一种名为“强化行为对齐”（RBA）的框架。该框架首先利用一个强大的教师 LLM 自我合成大量的、高保真的对齐数据，然后利用强化学习方法将语音 LLM 的行为与教师 LLM 的行为进行对齐。

Result: 实验结果表明，RBA 方法有效地提升了语音 LLM 的指令遵循能力，并且优于传统的蒸馏基线。此外，RBA 框架还可以无缝扩展到语音问答和语音到文本翻译等任务，仅使用自我生成的数据就在公开基准上取得了最先进的性能。

Conclusion: RBA 框架通过自我合成数据和强化学习，成功解决了语音 LLM 在指令遵循方面的挑战，并能在多项任务中实现最先进的性能，证明了其有效性和通用性。

Abstract: The recent advancements of Large Language Models (LLMs) have spurred
considerable research interest in extending their linguistic capabilities
beyond text to other modalities, which leads to emergence of speech-based LLMs
(SpeechLMs) with capability of processing user request in either speech or
textual formats. However, owing to inter-modal discrepancies, these SpeechLMs
still exhibit a significant performance gap compared to their text-based LLM
counterparts in instruction-following, particularly when confronted with the
dynamic and variable nature of user speech. To address this challenge, this
paper introduces a framework termed Reinforced Behavior Alignment (RBA),
designed to bolster the language generation proficiency of SpeechLMs. Instead
of relying on supervised fine-tuning from human annotations, RBA employs a
self-synthesis methodology to generate extensive, high-fidelity alignment data
by a powerful teacher LLM. Then SpeechLMs is aligned its behavior with that of
a teacher using a reinforcement learning-based approach. Experimental results
demonstrate that this method effectively enhances the instruction-following
capabilities of SpeechLMs that outperform conventional distillation baselines.
Crucially, we demonstrate that RBA can be seamlessly extended to tasks such
including spoken question answering and speech-to-text translation, attaining
state-of-the-art performance on open benchmarks with only self-generated data.

</details>


### [74] [Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model](https://arxiv.org/abs/2509.03527)
*Bohdan M. Pavlyshenko*

Main category: cs.CL

TL;DR: 该论文使用微调的 Mistral 7B LLM 和 RAG 模型，对加密货币新闻进行多层级、多任务分析，生成图表和文本摘要，并结合知识图谱以减少幻觉，最终提供全面的定性和定量分析报告。


<details>
  <summary>Details</summary>
Motivation: 解决大规模加密货币新闻分析的挑战，并利用大型语言模型（LLM）提供深入见解。

Method: 利用微调的 Mistral 7B LLM 和检索增强生成（RAG）技术，进行多层级分析。第一层生成图表/文本摘要、情感分数和 JSON 表示。更高层级进行分层堆叠，整合各层摘要。模型使用 4 位量化和 PEFT/LoRA 方法进行微调。新闻表示为知识图谱以减少 LLM 幻觉。

Result: 微调后的 Mistral 7B LLM 模型能够进行信息丰富的定性和定量分析，提供重要的见解。

Conclusion: 微调 Mistral 7B LLM 模型在多层级加密货币新闻分析中表现出色，能够提供有价值的分析和见解。

Abstract: In the paper, we consider multilevel multitask analysis of cryptocurrency
news using a fine-tuned Mistral 7B large language model with
retrieval-augmented generation (RAG).
  On the first level of analytics, the fine-tuned model generates graph and
text summaries with sentiment scores as well as JSON representations of
summaries. Higher levels perform hierarchical stacking that consolidates sets
of graph-based and text-based summaries as well as summaries of summaries into
comprehensive reports. The combination of graph and text summaries provides
complementary views of cryptocurrency news. The model is fine-tuned with 4-bit
quantization using the PEFT/LoRA approach. The representation of cryptocurrency
news as knowledge graph can essentially eliminate problems with large language
model hallucinations.
  The obtained results demonstrate that the use of fine-tuned Mistral 7B LLM
models for multilevel cryptocurrency news analysis can conduct informative
qualitative and quantitative analytics, providing important insights.

</details>


### [75] [The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process](https://arxiv.org/abs/2509.03528)
*Matilde Contestabile,Chiara Ferrara,Alberto Giovannetti,Giovanni Parrillo,Andrea Vandin*

Main category: cs.CL

TL;DR: Process Mining (PM) applied to social systems like lawmaking faces data quality/accessibility issues. We created ProLiFIC, a comprehensive event log of the Italian lawmaking process (1987-2022) from unstructured data using LLMs, and propose it as a benchmark for legal PM.


<details>
  <summary>Details</summary>
Motivation: The efficacy of Process Mining (PM) in the legal domain is limited by the accessibility and quality of datasets. There is a need for a comprehensive and high-quality dataset to facilitate the application of PM in legal contexts.

Method: We created ProLiFIC, a comprehensive event log of the Italian lawmaking process from 1987 to 2022. This log was created from unstructured data from the Normattiva portal and structured using large language models (LLMs).

Result: ProLiFIC is a comprehensive event log that aligns with recent efforts in integrating PM with LLMs. Preliminary analyses have been performed using this dataset.

Conclusion: ProLiFIC serves as a benchmark for legal Process Mining, aiming to foster new developments in the field by providing a robust and accessible dataset for analysis.

Abstract: Process Mining (PM), initially developed for industrial and business
contexts, has recently been applied to social systems, including legal ones.
However, PM's efficacy in the legal domain is limited by the accessibility and
quality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in
Italian Chambers), a comprehensive event log of the Italian lawmaking process
from 1987 to 2022. Created from unstructured data from the Normattiva portal
and structured using large language models (LLMs), ProLiFIC aligns with recent
efforts in integrating PM with LLMs. We exemplify preliminary analyses and
propose ProLiFIC as a benchmark for legal PM, fostering new developments.

</details>


### [76] [Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages](https://arxiv.org/abs/2509.03529)
*Alejandro Álvarez Castro,Joaquín Ordieres-Meré*

Main category: cs.CL

TL;DR: 本论文提出了一种新颖的多模态框架，通过将财报电话会议编码为分层话语树来生成语义丰富且结构感知的嵌入。


<details>
  <summary>Details</summary>
Motivation: 现有金融情感分析系统未能捕捉财报电话会议的层级话语结构。

Method: 提出了一种两阶段Transformer架构，第一阶段使用对比学习对节点级别的多模态内容和话语元数据进行编码，第二阶段为整个会议合成全局嵌入。

Result: 实验结果表明，生成的嵌入形成了稳定、有意义的语义表示，反映了情感基调、结构逻辑和主题对齐。

Conclusion: 该框架不仅为下游任务提供了实用性，而且为涉及高风险沟通的其他领域提供了一种可推广的方法。

Abstract: Earnings calls represent a uniquely rich and semi-structured source of
financial communication, blending scripted managerial commentary with
unscripted analyst dialogue. Although recent advances in financial sentiment
analysis have integrated multi-modal signals, such as textual content and vocal
tone, most systems rely on flat document-level or sentence-level models,
failing to capture the layered discourse structure of these interactions. This
paper introduces a novel multi-modal framework designed to generate
semantically rich and structurally aware embeddings of earnings calls, by
encoding them as hierarchical discourse trees. Each node, comprising either a
monologue or a question-answer pair, is enriched with emotional signals derived
from text, audio, and video, as well as structured metadata including coherence
scores, topic labels, and answer coverage assessments. A two-stage transformer
architecture is proposed: the first encodes multi-modal content and discourse
metadata at the node level using contrastive learning, while the second
synthesizes a global embedding for the entire conference. Experimental results
reveal that the resulting embeddings form stable, semantically meaningful
representations that reflect affective tone, structural logic, and thematic
alignment. Beyond financial reporting, the proposed system generalizes to other
high-stakes unscripted communicative domains such as tele-medicine, education,
and political discourse, offering a robust and explainable approach to
multi-modal discourse representation. This approach offers practical utility
for downstream tasks such as financial forecasting and discourse evaluation,
while also providing a generalizable method applicable to other domains
involving high-stakes communication.

</details>


### [77] [Reading Between the Signs: Predicting Future Suicidal Ideation from Adolescent Social Media Texts](https://arxiv.org/abs/2509.03530)
*Paul Blum,Enrico Liscio,Ruixuan Zhang,Caroline Figueroa,Pradeep K. Murukannaiah*

Main category: cs.CL

TL;DR: 本研究提出了一种预测青少年自杀意念和行为（SIB）的新方法，通过分析其在社交媒体论坛上的帖子，即使在他们明确表达自杀意念之前，也能进行预测。


<details>
  <summary>Details</summary>
Motivation: 青少年自杀是一个严峻的公共卫生问题，而现有的预测方法往往依赖于心理健康服务的接触，导致许多病例被忽视。社交媒体为实时捕捉青少年在线表达的思想和挣扎提供了独特的机会。

Method: 本研究引入了一种名为Early-SIB的新型基于Transformer的模型，该模型能够按顺序处理用户发布的帖子以及与之互动的内容，以预测用户未来是否会发布SIB相关帖子。该方法不依赖于任何自杀意念的自我披露作为输入。

Result: 在荷兰青少年论坛的数据集上，Early-SIB模型在预测未来SIB方面达到了0.73的平衡准确率。

Conclusion: 研究表明，Early-SIB模型能够有效预测青少年未来的SIB，为主流的自杀预测方法提供了有意义的补充，尤其是在那些未寻求专业帮助的青少年群体中。

Abstract: Suicide is a leading cause of death among adolescents (12-18), yet predicting
it remains a significant challenge. Many cases go undetected due to a lack of
contact with mental health services. Social media, however, offers a unique
opportunity, as young people often share their thoughts and struggles online in
real time. In this work, we propose a novel task and method to approach it:
predicting suicidal ideation and behavior (SIB) from forum posts before an
adolescent explicitly expresses suicidal ideation on an online forum. This
predictive framing, where no self-disclosure is used as input at any stage,
remains largely unexplored in the suicide prediction literature. To this end,
we introduce Early-SIB, a transformer-based model that sequentially processes
the posts a user writes and engages with to predict whether they will write a
SIB post. Our model achieves a balanced accuracy of 0.73 for predicting future
SIB on a Dutch youth forum, demonstrating that such tools can offer a
meaningful addition to traditional methods.

</details>


### [78] [Real-Time Detection of Hallucinated Entities in Long-Form Generation](https://arxiv.org/abs/2509.03531)
*Oscar Obeso,Andy Arditi,Javier Ferrando,Joshua Freeman,Cameron Holmes,Neel Nanda*

Main category: cs.CL

TL;DR: 提出了一种廉价、可扩展的实时识别长文本生成中幻觉词元的方法，并将其有效扩展到70B参数模型。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测方法不适用于现实世界，因为它们要么局限于简短的事实查询，要么需要昂贵的外部验证。

Method: 提出一种针对实体级别幻觉（例如，伪造的名称、日期、引文）而非声明级别幻觉的方法，从而自然地映射到词元级别标签并实现流式检测。开发了一种利用网络搜索来注释模型响应的注释方法，提供基于事实的标签，指示哪些词元对应于伪造的实体。利用这些数据训练分类器。

Result: 在四个模型系列中进行评估，我们的分类器在长文本响应方面持续优于基线，包括更昂贵的方法（例如，Llama-3.3-70B 的 AUC 0.90 vs 0.71），并且在短文本问答设置中也有所改进。尽管仅使用实体级别标签进行训练，但我们的探测器能有效检测数学推理任务中的不正确答案，表明其泛化能力超出实体范围。

Conclusion: 我们的工作表明了一种有前途的、可扩展的、可用于现实世界幻觉检测的新方法。

Abstract: Large language models are now routinely used in high-stakes applications
where hallucinations can cause serious harm, such as medical consultations or
legal advice. Existing hallucination detection methods, however, are
impractical for real-world use, as they are either limited to short factual
queries or require costly external verification. We present a cheap, scalable
method for real-time identification of hallucinated tokens in long-form
generations, and scale it effectively to 70B parameter models. Our approach
targets \emph{entity-level hallucinations} -- e.g., fabricated names, dates,
citations -- rather than claim-level, thereby naturally mapping to token-level
labels and enabling streaming detection. We develop an annotation methodology
that leverages web search to annotate model responses with grounded labels
indicating which tokens correspond to fabricated entities. This dataset enables
us to train effective hallucination classifiers with simple and efficient
methods such as linear probes. Evaluating across four model families, our
classifiers consistently outperform baselines on long-form responses, including
more expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for
Llama-3.3-70B), and are also an improvement in short-form question-answering
settings. Moreover, despite being trained only with entity-level labels, our
probes effectively detect incorrect answers in mathematical reasoning tasks,
indicating generalization beyond entities. While our annotation methodology is
expensive, we find that annotated responses from one model can be used to train
effective classifiers on other models; accordingly, we publicly release our
datasets to facilitate reuse. Overall, our work suggests a promising new
approach for scalable, real-world hallucination detection.

</details>


### [79] [Topic Identification in LLM Input-Output Pairs through the Lens of Information Bottleneck](https://arxiv.org/abs/2509.03533)
*Igor Halperin*

Main category: cs.CL

TL;DR: LLMs会产生幻觉，现有的检测方法存在问题，本文提出了UDIB算法来改进检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有的SDM框架依赖于对句子嵌入进行几何聚类来识别主题，但这种优化方式与下游信息论分析存在脱节。

Method: 本文提出了一种基于确定性信息瓶颈（DIB）的原则性主题识别方法，并将其转换为一种可用于高维数据的实用算法UDIB，通过用计算上有效的上界替换DIB中不切实际的KL散度项来实现。UDIB可以被看作是K-means的一种熵正则化和鲁棒化版本。

Result: 将UDIB应用于LLM提示和响应嵌入的联合聚类，生成了一个最大信息量地反映提示-响应关系的主题表示，从而改进了SDM框架，并提供了一种更有效的检测幻觉的方法。

Conclusion: UDIB是一种比现有方法更有效、更敏感的检测LLM幻觉（confabulations）的工具。

Abstract: Large Language Models (LLMs) are prone to critical failure modes, including
\textit{intrinsic faithfulness hallucinations} (also known as confabulations),
where a response deviates semantically from the provided context. Frameworks
designed to detect this, such as Semantic Divergence Metrics (SDM), rely on
identifying latent topics shared between prompts and responses, typically by
applying geometric clustering to their sentence embeddings. This creates a
disconnect, as the topics are optimized for spatial proximity, not for the
downstream information-theoretic analysis. In this paper, we bridge this gap by
developing a principled topic identification method grounded in the
Deterministic Information Bottleneck (DIB) for geometric clustering. Our key
contribution is to transform the DIB method into a practical algorithm for
high-dimensional data by substituting its intractable KL divergence term with a
computationally efficient upper bound. The resulting method, which we dub UDIB,
can be interpreted as an entropy-regularized and robustified version of K-means
that inherently favors a parsimonious number of informative clusters. By
applying UDIB to the joint clustering of LLM prompt and response embeddings, we
generate a shared topic representation that is not merely spatially coherent
but is fundamentally structured to be maximally informative about the
prompt-response relationship. This provides a superior foundation for the SDM
framework and offers a novel, more sensitive tool for detecting confabulations.

</details>


### [80] [QuesGenie: Intelligent Multimodal Question Generation](https://arxiv.org/abs/2509.03535)
*Ahmed Mubarak,Amna Ahmed,Amira Nasser,Aya Mohamed,Fares El-Sadek,Mohammed Ahmed,Ahmed Salah,Youssef Sobhy*

Main category: cs.CL

TL;DR: The project develops a multi-modal question generation system to address the lack of tailored practice materials for abundant educational resources. It features multi-modal input handling, question generation, RLHF, and an interactive interface, aiming for automated, scalable, and intelligent question generation.


<details>
  <summary>Details</summary>
Motivation: Learners have access to abundant educational resources but lack tailored practice materials, posing a significant challenge.

Method: The system is developed with four major components: multi-modal input handling, question generation, reinforcement learning from human feedback (RLHF), and an end-to-end interactive interface.

Result: The system automatically generates diverse question types from various content formats.

Conclusion: The project lays the foundation for automated, scalable, and intelligent question generation, balancing resource efficiency, robust functionality, and a smooth user experience.

Abstract: In today's information-rich era, learners have access to abundant educational
resources, but the lack of practice materials tailored to these resources
presents a significant challenge. This project addresses that gap by developing
a multi-modal question generation system that can automatically generate
diverse question types from various content formats. The system features four
major components: multi-modal input handling, question generation,
reinforcement learning from human feedback (RLHF), and an end-to-end
interactive interface. This project lays the foundation for automated,
scalable, and intelligent question generation, carefully balancing resource
efficiency, robust functionality and a smooth user experience.

</details>


### [81] [AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models](https://arxiv.org/abs/2509.03537)
*Cheng-Kai Yeh,Hsing-Wang Lee,Chung-Hung Kuo,Hen-Hsen Huang*

Main category: cs.CL

TL;DR: AR^2框架通过引入教师模型将简单问题转化为复杂描述，并训练学生模型提取计算核心来提升LLM的抽象推理能力，从而提高其在未见过任务上的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有针对代码生成的LLM方法主要关注表面模式识别，忽视了对抽象能力的显式训练，而抽象能力对于解决复杂问题至关重要。

Method: AR^2框架使用一个教师模型将问题转化为具有挑战性的描述，同时训练一个学生模型从这些复杂描述中提取计算核心来解决问题。

Result: AR^2显著提高了学生模型在处理未见过且具有挑战性的编程任务上的准确性。

Conclusion: 抽象能力是提升LLM泛化能力的关键因素。

Abstract: Abstraction--the ability to recognize and distill essential computational
patterns from complex problem statements--is a foundational skill in computer
science, critical both for human problem-solvers and coding-oriented large
language models (LLMs). Despite recent advances in training LLMs for code
generation using reinforcement learning (RL), most existing approaches focus
primarily on superficial pattern recognition, overlooking explicit training for
abstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement
Learning for Abstract Reasoning), a novel framework explicitly designed to
enhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to
transform kernel problems into narrative-rich, challenging descriptions without
changing their fundamental logic. Simultaneously, a student coding model is
trained to solve these complex narrative problems by extracting their
underlying computational kernels. Experimental results demonstrate that AR$^2$
substantially improves the student model's accuracy on previously unseen,
challenging programming tasks, underscoring abstraction as a key skill for
enhancing LLM generalization.

</details>


### [82] [Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction](https://arxiv.org/abs/2509.03540)
*Shanglin Wu,Lihui Liu,Jinho D. Choi,Kai Shu*

Main category: cs.CL

TL;DR: LLMs 难以生成事实一致的答案，RAG 方法通过引入外部知识来解决此问题，但其将知识视为非结构化文本，限制了组合推理和事实不一致性识别。我们提出了一种新颖的框架，在推理时动态构建和扩展知识图（KG），整合 LLM 的内部知识和外部检索的知识。该方法首先从问题中提取种子 KG，然后通过 LLM 的潜在知识进行迭代扩展，并通过外部检索进行选择性精炼，以提高事实覆盖率和纠正错误。在三个 QA 基准上的评估表明，与基线方法相比，该方法在事实准确性、答案精确性和可解释性方面得到了一致的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）方法在处理 LLM 的参数记忆限制和生成事实一致答案方面存在不足，因为它们通常将知识视为非结构化文本，这限制了组合推理和识别事实不一致性的能力。

Method: 提出一种新颖的框架，在推理时动态构建和扩展知识图（KG），整合 LLM 的内部知识和外部检索的知识。该方法首先从问题中提取种子 KG，然后通过 LLM 的潜在知识进行迭代扩展，并通过外部检索进行选择性精炼。

Result: 在三个多样化的事实 QA 基准上进行评估，结果显示与基线提示和静态 KG 增强方法相比，在事实准确性、答案精确性和可解释性方面得到了一致的改进。

Conclusion: 推理时构建 KG 是增强 LLM 事实性的一种有前途的方向，它以结构化、可解释和可扩展的方式进行。

Abstract: Large Language Models (LLMs) often struggle with producing factually
consistent answers due to limitations in their parametric memory.
Retrieval-Augmented Generation (RAG) methods address this issue by
incorporating external knowledge from trusted sources at inference time.
However, such methods typically treat knowledge as unstructured text, which
limits their ability to support compositional reasoning and identify factual
inconsistencies. To overcome these limitations, we propose a novel framework
that dynamically constructs and expands knowledge graphs (KGs) during
inference, integrating both internal knowledge extracted from LLMs and external
information retrieved from external sources. Our method begins by extracting a
seed KG from the question via prompting, followed by iterative expansion using
the LLM's latent knowledge. The graph is then selectively refined through
external retrieval, enhancing factual coverage and correcting inaccuracies. We
evaluate our approach on three diverse factual QA benchmarks, demonstrating
consistent improvements in factual accuracy, answer precision, and
interpretability over baseline prompting and static KG-augmented methods. Our
findings suggest that inference-time KG construction is a promising direction
for enhancing LLM factuality in a structured, interpretable, and scalable
manner.

</details>


### [83] [ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference](https://arxiv.org/abs/2509.03565)
*Qi Chen,Jingxuan Wei,Zhuoya Yao,Haiguang Wang,Gaowei Wu,Bihui Yu,Siyuan Li,Cheng Tan*

Main category: cs.CL

TL;DR: 该研究提出了一个名为ResearchPulse的框架，用于结构化地分析和整合科学论文集，以重建研究发展脉络。


<details>
  <summary>Details</summary>
Motivation: 理解科学思想的演变需要跨文档的推理，而不仅仅是总结单篇论文。

Method: 提出了一种名为“多文档科学推理”的新任务，并开发了ResearchPulse框架，该框架包含三个代理：计划代理、Mmap-Agent和Lchart-Agent，用于任务分解、构建动机-方法思维导图以及综合实验折线图。同时，创建了一个名为ResearchPulse-Bench的基准测试集。

Result: ResearchPulse框架在语义对齐、结构一致性和视觉保真度方面优于包括GPT-4o在内的强有力基线。

Conclusion: ResearchPulse框架能够有效地进行多文档科学推理，为理解科学研究的发展提供了新的途径。

Abstract: Understanding how scientific ideas evolve requires more than summarizing
individual papers-it demands structured, cross-document reasoning over
thematically related research. In this work, we formalize multi-document
scientific inference, a new task that extracts and aligns motivation,
methodology, and experimental results across related papers to reconstruct
research development chains. This task introduces key challenges, including
temporally aligning loosely structured methods and standardizing heterogeneous
experimental tables. We present ResearchPulse, an agent-based framework that
integrates instruction planning, scientific content extraction, and structured
visualization. It consists of three coordinated agents: a Plan Agent for task
decomposition, a Mmap-Agent that constructs motivation-method mind maps, and a
Lchart-Agent that synthesizes experimental line charts. To support this task,
we introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper
clusters. Experiments show that our system, despite using 7B-scale agents,
consistently outperforms strong baselines like GPT-4o in semantic alignment,
structural consistency, and visual fidelity. The dataset are available in
https://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.

</details>


### [84] [NoteBar: An AI-Assisted Note-Taking System for Personal Knowledge Management](https://arxiv.org/abs/2509.03610)
*Josh Wisoff,Yao Tang,Zhengyu Fang,Jordan Guzman,YuTang Wang,Alex Yu*

Main category: cs.CL

TL;DR: NoteBar 是一款 AI 辅助笔记工具，可自动组织笔记并支持用户工作流程，并附带一个包含 3,173 条笔记和 8,494 个注释概念的新型数据集。


<details>
  <summary>Details</summary>
Motivation: 现有 AI 辅助笔记工具效率低下，需要改进以更好地支持用户工作流程。

Method: 提出 NoteBar，一个利用个人信息和高效语言模型自动组织笔记的工具，并引入了一个基于 MBTI 人格类型的数据集。

Result: NoteBar 可以在实际中以经济高效的方式部署，支持交互式使用，无需重型基础设施。

Conclusion: NoteBar 及其数据集为推进 AI 辅助个人知识管理提供了可扩展且可扩展的基础。

Abstract: Note-taking is a critical practice for capturing, organizing, and reflecting
on information in both academic and professional settings. The recent success
of large language models has accelerated the development of AI-assisted tools,
yet existing solutions often struggle with efficiency. We present NoteBar, an
AI-assisted note-taking tool that leverages persona information and efficient
language models to automatically organize notes into multiple categories and
better support user workflows. To support research and evaluation in this
space, we further introduce a novel persona-conditioned dataset of 3,173 notes
and 8,494 annotated concepts across 16 MBTI personas, offering both diversity
and semantic richness for downstream tasks. Finally, we demonstrate that
NoteBar can be deployed in a practical and cost-effective manner, enabling
interactive use without reliance on heavy infrastructure. Together, NoteBar and
its accompanying dataset provide a scalable and extensible foundation for
advancing AI-assisted personal knowledge management.

</details>


### [85] [E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition](https://arxiv.org/abs/2509.03615)
*Aryan Gupta,Anupam Purwar*

Main category: cs.CL

TL;DR: 在多语言、嘈杂和多样化的真实世界图像中，光学字符识别（OCR）仍然是一个重大挑战。本文介绍了 Sprinklr-Edge-OCR，一个专为资源受限环境中的边缘部署而优化构建的新型 OCR 系统。通过对五种最先进的 LVLM 和两种传统的 OCR 系统在专有、经过双重人工注释的多语言（54 种语言）图像数据集上进行大规模比较评估，证明了传统 OCR 系统在边缘部署方面仍然是最优的选择。


<details>
  <summary>Details</summary>
Motivation: 随着大型视觉语言模型（LVLM）的兴起，人们对其超越固定 OCR 管道进行泛化和推理的能力越来越感兴趣。然而，在资源受限的环境中进行光学字符识别（OCR）仍然是一个重大挑战。

Method: 对五种最先进的 LVLM（InternVL、Qwen、GOT OCR、LLaMA、MiniCPM）和两种传统 OCR 系统（Sprinklr-Edge-OCR、SuryaOCR）在专有、经过双重人工注释的多语言（54 种语言）图像数据集上进行大规模比较评估。评估指标包括准确性、语义一致性、语言覆盖范围、计算效率（延迟、内存、GPU 使用率）和部署成本。还进行了边缘情况部署分析，在仅 CPU 的环境中评估模型性能。

Result: Qwen 实现了最高的精确率（0.54），而 Sprinklr-Edge-OCR 实现了最佳的整体 F1 分数（0.46），并且在效率方面表现优于其他模型，平均处理速度比 LVLM 快 35 倍（平均每张图像 0.17 秒），成本不到 LVLM 的 0.01（每 1,000 张图像 0.006 美元）。

Conclusion: 在边缘部署方面，即使在大型语言模型（LLM）时代，传统 OCR 系统仍然是最优的选择，因为它们具有低计算要求、低延迟和非常高的可负担性。

Abstract: Optical Character Recognition (OCR) in multilingual, noisy, and diverse
real-world images remains a significant challenge for optical character
recognition systems. With the rise of Large Vision-Language Models (LVLMs),
there is growing interest in their ability to generalize and reason beyond
fixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR
system built specifically optimized for edge deployment in resource-constrained
environments. We present a large-scale comparative evaluation of five
state-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two
traditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly
hand annotated dataset of multilingual (54 languages) images. Our benchmark
covers a broad range of metrics including accuracy, semantic consistency,
language coverage, computational efficiency (latency, memory, GPU usage), and
deployment cost. To better reflect real-world applicability, we also conducted
edge case deployment analysis, evaluating model performance on CPU only
environments. Among the results, Qwen achieved the highest precision (0.54),
while Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and
outperformed others in efficiency, processing images 35 faster (0.17 seconds
per image on average) and at less than 0.01 of the cost (0.006 USD per 1,000
images) compared to LVLM. Our findings demonstrate that the most optimal OCR
systems for edge deployment are the traditional ones even in the era of LLMs
due to their low compute requirements, low latency, and very high
affordability.

</details>


### [86] [Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators](https://arxiv.org/abs/2509.03647)
*Dani Roytburg,Matthew Bozoukov,Matthew Nguyen,Jou Barzdukas,Simon Fu,Narmeen Oozeer*

Main category: cs.CL

TL;DR: LLM在作为自动化评估者时存在“自我偏好偏见”，即倾向于偏爱自己的输出来自其他模型。本研究提出使用轻量级“引导向量”在推理时解决此问题，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: LLM作为自动化评估者时存在“自我偏好偏见”，这会影响评估的公平性和可靠性，尤其是在偏好调整和模型路由等任务中。

Method: 研究使用对比激活添加（CAA）和基于优化的方法构建引导向量，并构建了一个区分合理和不合理自我偏好的数据集。

Result: 引导向量能将不合理的自我偏好偏见最多减少97%，效果优于提示和直接偏好优化基线。然而，引导向量在处理合理的自我偏好和无偏见一致性时不稳定。

Conclusion: 引导向量在缓解LLM的“自我偏好偏见”方面显示出潜力，但其不稳定性表明该偏见可能涉及多个或非线性方向，需要更鲁棒的干预措施。

Abstract: Large language models (LLMs) increasingly serve as automated evaluators, yet
they suffer from "self-preference bias": a tendency to favor their own outputs
over those of other models. This bias undermines fairness and reliability in
evaluation pipelines, particularly for tasks like preference tuning and model
routing. We investigate whether lightweight steering vectors can mitigate this
problem at inference time without retraining. We introduce a curated dataset
that distinguishes self-preference bias into justified examples of
self-preference and unjustified examples of self-preference, and we construct
steering vectors using two methods: Contrastive Activation Addition (CAA) and
an optimization-based approach. Our results show that steering vectors can
reduce unjustified self-preference bias by up to 97\%, substantially
outperforming prompting and direct preference optimization baselines. Yet
steering vectors are unstable on legitimate self-preference and unbiased
agreement, implying self-preference spans multiple or nonlinear directions.
This underscores both their promise and limits as safeguards for LLM-as-judges
and motivates more robust interventions.

</details>


### [87] [Semantic Analysis of SNOMED CT Concept Co-occurrences in Clinical Documentation using MIMIC-IV](https://arxiv.org/abs/2509.03662)
*Ali Noori,Somya Mohanty,Prashanti Manda*

Main category: cs.CL

TL;DR: 临床笔记的非结构化性质给大规模分析带来了挑战。本研究利用 MIMIC-IV 数据库，结合 SNOMED CT 概念、共现模式和基于嵌入的语义相似性，探讨了它们之间的关系。研究结果表明，共现和语义相似性之间存在弱相关性，但嵌入能够捕捉到共现模式未能完全反映的临床关联，并能有效预测后续记录的概念。概念嵌入的聚类分析揭示了有意义的临床主题，并与患者表型和护理模式相关联。此外，与死亡率和再入院率等结果相关的共现模式证明了该方法的实用性。总而言之，本研究强调了共现统计和语义嵌入在完善临床记录、揭示潜在临床关系以及支持决策和表型分析方面的互补价值。


<details>
  <summary>Details</summary>
Motivation: 临床笔记中的丰富临床叙述因其非结构化格式而难以进行大规模分析。虽然 SNOMED CT 等标准化术语提高了互操作性，但共现和语义相似性对概念之间关系的研究仍显不足。

Method: 本研究利用 MIMIC-IV 数据库，结合标准化术语 SNOMED CT 的概念共现模式和预训练嵌入（如 ClinicalBERT、BioBERT）的语义相似性，使用 NPMI（归一化点互信息）和预训练嵌入，研究了频繁共现的概念是否也具有语义上的接近性，嵌入是否能够提示缺失的概念，以及这些关系如何随时间推移和不同专科而演变。

Result: 研究发现，概念的共现频率和语义相似性之间存在弱相关性。然而，嵌入能够捕捉到文档频率未能完全反映的临床关联。基于嵌入的预测概念的建议，经常与之后实际记录的概念相匹配，这表明嵌入在增强临床注释方面具有实用价值。对概念嵌入进行聚类分析，能够识别出与患者表型和护理模式相对应的、连贯的临床主题（如症状、实验室检查、诊断、心血管疾病）。此外，与死亡率和再入院率等结局相关的共现模式，也证明了该方法的实际应用潜力。

Conclusion: 共现统计和语义嵌入在提升文档完整性、揭示潜在临床关系以及为决策支持和表型分析提供信息方面具有互补价值。

Abstract: Clinical notes contain rich clinical narratives but their unstructured format
poses challenges for large-scale analysis. Standardized terminologies such as
SNOMED CT improve interoperability, yet understanding how concepts relate
through co-occurrence and semantic similarity remains underexplored. In this
study, we leverage the MIMIC-IV database to investigate the relationship
between SNOMED CT concept co-occurrence patterns and embedding-based semantic
similarity. Using Normalized Pointwise Mutual Information (NPMI) and pretrained
embeddings (e.g., ClinicalBERT, BioBERT), we examine whether frequently
co-occurring concepts are also semantically close, whether embeddings can
suggest missing concepts, and how these relationships evolve temporally and
across specialties. Our analyses reveal that while co-occurrence and semantic
similarity are weakly correlated, embeddings capture clinically meaningful
associations not always reflected in documentation frequency. Embedding-based
suggestions frequently matched concepts later documented, supporting their
utility for augmenting clinical annotations. Clustering of concept embeddings
yielded coherent clinical themes (symptoms, labs, diagnoses, cardiovascular
conditions) that map to patient phenotypes and care patterns. Finally,
co-occurrence patterns linked to outcomes such as mortality and readmission
demonstrate the practical utility of this approach. Collectively, our findings
highlight the complementary value of co-occurrence statistics and semantic
embeddings in improving documentation completeness, uncovering latent clinical
relationships, and informing decision support and phenotyping applications.

</details>


### [88] [MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and Cross-Domain Stance Detection](https://arxiv.org/abs/2509.03725)
*Parush Gera,Tempestt Neal*

Main category: cs.CL

TL;DR: Metric Learning-Based Few-Shot Learning for Cross-Target and Cross-Domain Stance Detection (MLSD) is a novel approach for stance detection that uses metric learning with triplet loss to adapt to new targets and domains, showing significant improvements across multiple models and datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a novel approach for stance detection that can generalize across different domains and targets, addressing the challenge of domain adaptation in existing models.

Method: MLSD utilizes metric learning with triplet loss to create a discriminative embedding space that captures semantic similarities and differences between stance targets, enabling effective transfer learning for new target domains.

Result: MLSD demonstrates statistically significant improvements in stance detection performance across six widely used models in multiple cross-target and cross-domain scenarios on two datasets.

Conclusion: MLSD provides an effective solution for cross-target and cross-domain stance detection by leveraging metric learning to enhance domain adaptation and improve generalization capabilities.

Abstract: We present the novel approach for stance detection across domains and
targets, Metric Learning-Based Few-Shot Learning for Cross-Target and
Cross-Domain Stance Detection (MLSD). MLSD utilizes metric learning with
triplet loss to capture semantic similarities and differences between stance
targets, enhancing domain adaptation. By constructing a discriminative
embedding space, MLSD allows a cross-target or cross-domain stance detection
model to acquire useful examples from new target domains. We evaluate MLSD in
multiple cross-target and cross-domain scenarios across two datasets, showing
statistically significant improvement in stance detection performance across
six widely used stance detection models.

</details>


### [89] [SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation Evaluation](https://arxiv.org/abs/2509.03791)
*Saki Imai,Mert İnan,Anthony Sicilia,Malihe Alikhani*

Main category: cs.CL

TL;DR: 现有的手语生成评估方法（如反译）存在局限性，无法捕捉手语的多模态特性，并且难以区分生成模型和翻译系统的错误。本文提出了 SiLVERScore，一种新的语义感知嵌入式评估指标，用于评估联合嵌入空间中的手语生成。SiLVERScore 在 PHOENIX-14T 和 CSL-Daily 数据集上表现出色，能够近乎完美地区分正确和随机配对（ROC AUC = 0.99，重叠度 < 7%），显著优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 现有的手语生成评估方法（如反译）存在局限性，未能充分捕捉手语的多模态特性（如面部表情、空间语法和韵律），并且难以确定评估错误是源于手语生成模型还是用于评估的翻译系统。

Method: 提出了一种名为 SiLVERScore 的新型语义感知嵌入式评估指标，该指标在联合嵌入空间中评估手语生成。

Result: SiLVERScore 在 PHOENIX-14T 和 CSL-Daily 数据集上取得了近乎完美的区分能力（ROC AUC = 0.99，重叠度 < 7%），在区分正确和随机配对方面显著优于传统指标，并能很好地处理语义和韵律变化。

Conclusion: SiLVERScore 是一种新颖的语义感知嵌入式评估指标，能够有效评估手语生成，克服了现有基于反译的评估方法的局限性，并在多个数据集上展示了其优越的性能和鲁棒性。

Abstract: Evaluating sign language generation is often done through back-translation,
where generated signs are first recognized back to text and then compared to a
reference using text-based metrics. However, this two-step evaluation pipeline
introduces ambiguity: it not only fails to capture the multimodal nature of
sign language-such as facial expressions, spatial grammar, and prosody-but also
makes it hard to pinpoint whether evaluation errors come from sign generation
model or the translation system used to assess it. In this work, we propose
SiLVERScore, a novel semantically-aware embedding-based evaluation metric that
assesses sign language generation in a joint embedding space. Our contributions
include: (1) identifying limitations of existing metrics, (2) introducing
SiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness
to semantic and prosodic variations, and (4) exploring generalization
challenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore
achieves near-perfect discrimination between correct and random pairs (ROC AUC
= 0.99, overlap < 7%), substantially outperforming traditional metrics.

</details>


### [90] [Measuring How (Not Just Whether) VLMs Build Common Ground](https://arxiv.org/abs/2509.03805)
*Saki Imai,Mert İnan,Anthony Sicilia,Malihe Alikhani*

Main category: cs.CL

TL;DR: 现有的视觉语言模型（VLM）评估基准未能充分衡量其在交互式语境下的理解和沟通能力。本文提出了一个包含四个指标（理解效率、内容一致性、词汇适应性和拟人度）的评估体系，用于系统性地衡量VLM在交互式指代游戏中的表现，并与人类的沟通模式进行了比较。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试无法充分评估大型视觉语言模型（VLM）在真实交互场景中的推理和理解能力，特别是在需要逐步建立共同理解的语境下。本文旨在提出一种新的评估方法，以更全面地衡量VLM在交互式理解任务中的表现。

Method: 提出并部署了一个包含四项指标（理解效率、内容一致性、词汇适应性和拟人度）的评估体系，用于在交互式指代游戏中评估150个由三个专有VLM与人类进行的自我对弈（self-play）回合的表现，并将VLM的表现与人类二人组进行了比较。

Result: 在150个交互式指代游戏的自我对弈回合中，所有三个VLM在至少三个评估指标上均与人类的表现存在差异。其中，GPT4o-mini在整体表现上最接近人类。研究发现，任务成功率并不能完全代表理解的成功，并且高图像-语句一致性并不一定预示任务成功。

Conclusion: 本文提出的评估指标体系为未来研究VLM的交互式理解能力提供了新的框架和方法。研究结果表明，尽管VLM在不断进步，但在交互式沟通和理解方面与人类仍有差距，现有的评估方法可能无法完全捕捉这些细微差别。

Abstract: Large vision language models (VLMs) increasingly claim reasoning skills, yet
current benchmarks evaluate them in single-turn or question answering settings.
However, grounding is an interactive process in which people gradually develop
shared understanding through ongoing communication. We introduce a four-metric
suite (grounding efficiency, content alignment, lexical adaptation, and
human-likeness) to systematically evaluate VLM performance in interactive
grounding contexts. We deploy the suite on 150 self-play sessions of
interactive referential games between three proprietary VLMs and compare them
with human dyads. All three models diverge from human patterns on at least
three metrics, while GPT4o-mini is the closest overall. We find that (i) task
success scores do not indicate successful grounding and (ii) high
image-utterance alignment does not necessarily predict task success. Our metric
suite and findings offer a framework for future research on VLM grounding.

</details>


### [91] [Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level Machine Translation](https://arxiv.org/abs/2509.03809)
*Jiaxin Guo,Daimeng Wei,Yuanchang Luo,Xiaoyu Chen,Zhanglin Wu,Huan Yang,Hengchao Shang,Zongyao Li,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

TL;DR: Align-then-Slide是一个用于超长文档机器翻译（doc-mt）的评估框架，解决了现有评估方法因句子不对齐而导致的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以处理LLMs生成的整篇文档翻译，因为它们假设逐句对齐，而LLMs的输出可能包含省略或多对一/一对多的映射。

Method: Align-then-Slide框架包括两个阶段：1. 对齐（Align）：自动推断句子级源-目标对应关系，并重建目标，使其与源句数量匹配。2. 滑动评估（n-Chunk Sliding Evaluate）：在1、2、3和4个chunk下计算平均指标分数，进行多粒度评估。

Result: 在WMT基准测试上，Align-then-Slide方法与专家MQM排名具有0.929的皮尔逊相关性。在一个新创建的真实世界测试集上，该方法也与人类判断高度一致。此外，Align-then-Slide生成的偏好数据可用于有效的CPO训练，并直接作为GRPO的奖励模型，其翻译优于标准的SFT基线。

Conclusion: Align-then-Slide框架被证明是doc-mt系统准确、鲁棒且可行的评估工具。

Abstract: Large language models (LLMs) have ushered in a new era for document-level
machine translation (\textit{doc}-mt), yet their whole-document outputs
challenge existing evaluation methods that assume sentence-by-sentence
alignment. We introduce \textit{\textbf{Align-then-Slide}}, a complete
evaluation framework for ultra-long doc-mt. In the Align stage, we
automatically infer sentence-level source-target correspondences and rebuild
the target to match the source sentence number, resolving omissions and
many-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we
calculate averaged metric scores under 1-, 2-, 3- and 4-chunk for
multi-granularity assessment. Experiments on the WMT benchmark show a Pearson
correlation of 0.929 between our method with expert MQM rankings. On a newly
curated real-world test set, our method again aligns closely with human
judgments. Furthermore, preference data produced by Align-then-Slide enables
effective CPO training and its direct use as a reward model for GRPO, both
yielding translations preferred over a vanilla SFT baseline. The results
validate our framework as an accurate, robust, and actionable evaluation tool
for doc-mt systems.

</details>


### [92] [NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio Deepfake Detection via Attention Aggregation](https://arxiv.org/abs/2509.03829)
*Huhong Xian,Rui Liu,Berrak Sisman,Haizhou Li*

Main category: cs.CL

TL;DR: 本研究提出NE-PADD方法，通过融合语音命名实体识别和音频深度伪造检测，提升了部分音频深度伪造检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的部分音频深度伪造检测方法未能充分利用音频中的语义信息，特别是命名实体信息，导致检测效果有待提升。

Method: 提出NE-PADD方法，包含两个并行的分支：语音命名实体识别（SpeechNER）和部分音频深度伪造检测（PADD）。利用注意力融合（AF）和注意力转移（AT）两种注意力聚合机制，将命名实体语义信息融入PADD任务。

Result: 在PartialSpoof-NER数据集上进行实验，结果表明NE-PADD方法优于现有基线方法。

Conclusion: 将命名实体知识集成到部分音频深度伪造检测任务中是有效的，能够显著提升检测性能。

Abstract: Different from traditional sentence-level audio deepfake detection (ADD),
partial audio deepfake detection (PADD) requires frame-level positioning of the
location of fake speech. While some progress has been made in this area,
leveraging semantic information from audio, especially named entities, remains
an underexplored aspect. To this end, we propose NE-PADD, a novel method for
Partial Audio Deepfake Detection (PADD) that leverages named entity knowledge
through two parallel branches: Speech Name Entity Recognition (SpeechNER) and
PADD. The approach incorporates two attention aggregation mechanisms: Attention
Fusion (AF) for combining attention weights and Attention Transfer (AT) for
guiding PADD with named entity semantics using an auxiliary loss. Built on the
PartialSpoof-NER dataset, experiments show our method outperforms existing
baselines, proving the effectiveness of integrating named entity knowledge in
PADD. The code is available at https://github.com/AI-S2-Lab/NE-PADD.

</details>


### [93] [Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth](https://arxiv.org/abs/2509.03867)
*Yang Wang,Chenghao Xiao,Chia-Yi Hsiao,Zi Yan Chang,Chi-Li Chen,Tyler Loakman,Chenghua Lin*

Main category: cs.CL

TL;DR: Drivelology是一种“有深度的胡言乱语”，即语法正确但语用矛盾、情感丰富或修辞颠覆的表达。现有的大型语言模型（LLM）在理解这种语言现象时存在困难，即使它们在许多自然语言处理任务中表现出色。研究者构建了一个包含1200多个例子的基准数据集，并评估了LLM在分类、生成和推理任务上的表现。结果表明，LLM常常混淆Drivelology和浅层胡言乱语，给出不连贯的解释，或未能识别其隐含的修辞功能。这揭示了LLM在语用理解方面的局限性，并对“统计流畅性等同于认知理解”的假设提出了挑战。研究者公开了数据集和代码以促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在许多自然语言处理（NLP）任务中表现出色，但它们在理解具有“深度”的语言现象（如“有深度的胡言乱语”，即Drivelology）方面存在困难。本研究旨在探究LLM在理解这类语言现象时的局限性。

Method: 研究者构建了一个包含1200多个Drivelology例子的多语言基准数据集，并对LLM在分类、生成和推理任务上的表现进行了评估。

Result: 研究结果显示，LLM在处理Drivelology时存在明显局限：它们常常将Drivelology与浅层胡言乱语混淆，生成的解释不连贯，或者完全忽略了其中隐含的修辞功能。

Conclusion: 本研究揭示了LLM在语用理解方面存在更深层次的表征差距，并对“统计流畅性等同于认知理解”的假设提出了质疑。研究者发布了数据集和代码，以促进对超越表面连贯性的语言深度进行建模的研究。

Abstract: We introduce Drivelology, a unique linguistic phenomenon characterised as
"nonsense with depth", utterances that are syntactically coherent yet
pragmatically paradoxical, emotionally loaded, or rhetorically subversive.
While such expressions may resemble surface-level nonsense, they encode
implicit meaning requiring contextual inference, moral reasoning, or emotional
interpretation. We find that current large language models (LLMs), despite
excelling at many natural language processing (NLP) tasks, consistently fail to
grasp the layered semantics of Drivelological text. To investigate this, we
construct a small but diverse benchmark dataset of over 1,200 meticulously
curated examples, with select instances in English, Mandarin, Spanish, French,
Japanese, and Korean. Annotation was especially challenging: each of the
examples required careful expert review to verify that it truly reflected
Drivelological characteristics. The process involved multiple rounds of
discussion and adjudication to address disagreements, highlighting the subtle
and subjective nature of the Drivelology. We evaluate a range of LLMs on
classification, generation, and reasoning tasks. Our results reveal clear
limitations of LLMs: models often confuse Drivelology with shallow nonsense,
produce incoherent justifications, or miss the implied rhetorical function
altogether. These findings highlight a deeper representational gap in LLMs'
pragmatic understanding and challenge the assumption that statistical fluency
implies cognitive comprehension. We release our dataset and code to facilitate
further research in modelling linguistic depth beyond surface-level coherence.

</details>


### [94] [A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models](https://arxiv.org/abs/2509.03871)
*Yanbo Wang,Yongcan Yu,Jian Liang,Ran He*

Main category: cs.CL

TL;DR: CoT推理虽提升了LLM在多任务上的表现，但对其可信度的影响尚不明确。本文综述了CoT推理在真实性、安全性、鲁棒性、公平性和隐私性方面的研究进展，并指出了当前推理模型面临的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究对CoT推理如何影响LLM的可信度理解不足，本文旨在填补这一空白。

Method: 梳理和分析了CoT推理在真实性、安全性、鲁棒性、公平性和隐私性五个核心维度上的最新研究进展、方法、发现和局限性，并对未来研究方向进行了展望。

Result: CoT推理有望通过减少幻觉、检测有害内容和提高鲁棒性来增强模型的可信度，但同时，最先进的推理模型在安全性、鲁棒性和隐私性方面也可能存在相当甚至更大的漏洞。

Conclusion: 本文全面总结了CoT推理在可信度方面的影响，为AI安全社区提供了宝贵的信息，有助于了解推理可信度的最新进展，但也警示了当前推理模型自身存在的风险。

Abstract: The development of Long-CoT reasoning has advanced LLM performance across
various tasks, including language understanding, complex problem solving, and
code generation. This paradigm enables models to generate intermediate
reasoning steps, thereby improving both accuracy and interpretability. However,
despite these advancements, a comprehensive understanding of how CoT-based
reasoning affects the trustworthiness of language models remains
underdeveloped. In this paper, we survey recent work on reasoning models and
CoT techniques, focusing on five core dimensions of trustworthy reasoning:
truthfulness, safety, robustness, fairness, and privacy. For each aspect, we
provide a clear and structured overview of recent studies in chronological
order, along with detailed analyses of their methodologies, findings, and
limitations. Future research directions are also appended at the end for
reference and discussion. Overall, while reasoning techniques hold promise for
enhancing model trustworthiness through hallucination mitigation, harmful
content detection, and robustness improvement, cutting-edge reasoning models
themselves often suffer from comparable or even greater vulnerabilities in
safety, robustness, and privacy. By synthesizing these insights, we hope this
work serves as a valuable and timely resource for the AI safety community to
stay informed on the latest progress in reasoning trustworthiness. A full list
of related papers can be found at
\href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.

</details>


### [95] [False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize](https://arxiv.org/abs/2509.03888)
*Cheng Wang,Zeming Wei,Qin Liu,Muhao Chen*

Main category: cs.CL

TL;DR: 现有的大语言模型（LLMs）可能遵从有害指令，这引发了严重的安全担忧。尽管研究人员试图通过探测（probing）方法来研究模型内部表示中恶意和良性输入的区分度，并将其用于安全检测，但本研究系统地重新审视了这一范式。研究发现，探测方法学习到的是表面模式而非语义上的有害性，导致在分布外（out-of-distribution）表现不佳。通过受控实验，研究证实了这一假设，并确定了模型学习到的具体模式是指令模式和触发词。研究人员通过对比简单的n-gram方法、使用语义清理数据集的受控实验以及详细的模式依赖性分析，揭示了当前基于探测的方法存在虚假的安全感。最后，研究强调需要重新设计模型和评估协议，并提供了相关讨论以促进负责任的研究。项目已开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于探测（probing）的方法在研究大语言模型（LLMs）内部表示中恶意和良性输入的区分度方面存在不足，尤其是在分布外（out-of-distribution）表现不佳，这表明探测方法可能学习到的是表面模式而非语义上的有害性。

Method: 本研究通过一系列受控实验来系统地重新审视基于探测的安全检测方法。首先，研究对比了简单n-gram方法与探测方法的性能。随后，研究使用了语义清理过的数据集进行实验，以隔离表面模式的影响。最后，研究对模型学习到的具体模式（指令模式和触发词）进行了详细的依赖性分析。

Result: 研究证实了探测方法学习到的是表面模式（如指令模式和触发词）而非语义上的有害性。简单的n-gram方法也能达到与探测方法相当的性能，并且在分布外表现不佳。这表明当前基于探测的安全检测方法可能提供虚假的安全感。

Conclusion: 现有基于探测的方法在检测大语言模型的有害指令方面存在局限性，它们学习到的表面模式而非语义有害性，导致在实际应用中可能失效。研究强调需要改进模型设计和评估协议，以确保大语言模型的安全性。

Abstract: Large Language Models (LLMs) can comply with harmful instructions, raising
serious safety concerns despite their impressive capabilities. Recent work has
leveraged probing-based approaches to study the separability of malicious and
benign inputs in LLMs' internal representations, and researchers have proposed
using such probing methods for safety detection. We systematically re-examine
this paradigm. Motivated by poor out-of-distribution performance, we
hypothesize that probes learn superficial patterns rather than semantic
harmfulness. Through controlled experiments, we confirm this hypothesis and
identify the specific patterns learned: instructional patterns and trigger
words. Our investigation follows a systematic approach, progressing from
demonstrating comparable performance of simple n-gram methods, to controlled
experiments with semantically cleaned datasets, to detailed analysis of pattern
dependencies. These results reveal a false sense of security around current
probing-based approaches and highlight the need to redesign both models and
evaluation protocols, for which we provide further discussions in the hope of
suggesting responsible further research in this direction. We have open-sourced
the project at https://github.com/WangCheng0116/Why-Probe-Fails.

</details>


### [96] [MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation](https://arxiv.org/abs/2509.03891)
*Gowen Loo,Chang Liu,Qinghong Yin,Xiang Chen,Jiawei Chen,Jingyuan Zhang,Yu Tian*

Main category: cs.CL

TL;DR: MobileRAG是一个增强了检索增强生成（RAG）的移动代理框架，用于提高LLM在移动任务中的准确性、交互性和记忆能力，并在引入的基准测试MobileRAG-Eval上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的移动代理在理解用户查询、与外部环境交互以及记忆能力方面存在不足，可能导致错误操作、任务中断和无法从过往错误中学习。提出MobileRAG旨在解决这些问题。

Method: 提出名为MobileRAG的移动代理框架，该框架通过检索增强生成（RAG）技术进行增强，具体包括InterRAG、LocalRAG和MemRAG三个组件。该框架利用RAG技术来更快速、更准确地识别用户查询，并完成复杂、长序列的移动任务。此外，还引入了一个更具挑战性的基准测试MobileRAG-Eval，用于评估MobileRAG在需要外部知识的复杂现实世界移动任务上的性能。

Result: 在MobileRAG-Eval基准测试上的广泛实验结果表明，MobileRAG能够轻松处理现实世界的移动任务，相比现有最先进的方法，操作步骤更少，性能提升了10.3%。

Conclusion: MobileRAG通过集成RAG技术，有效解决了现有移动代理在准确性、交互性和记忆方面的局限性，并在具有挑战性的基准测试上展现出优越的性能。

Abstract: Smartphones have become indispensable in people's daily lives, permeating
nearly every aspect of modern society. With the continuous advancement of large
language models (LLMs), numerous LLM-based mobile agents have emerged. These
agents are capable of accurately parsing diverse user queries and automatically
assisting users in completing complex or repetitive operations. However,
current agents 1) heavily rely on the comprehension ability of LLMs, which can
lead to errors caused by misoperations or omitted steps during tasks, 2) lack
interaction with the external environment, often terminating tasks when an app
cannot fulfill user queries, and 3) lack memory capabilities, requiring each
instruction to reconstruct the interface and being unable to learn from and
correct previous mistakes. To alleviate the above issues, we propose MobileRAG,
a mobile agents framework enhanced by Retrieval-Augmented Generation (RAG),
which includes InterRAG, LocalRAG, and MemRAG. It leverages RAG to more quickly
and accurately identify user queries and accomplish complex and long-sequence
mobile tasks. Additionally, to more comprehensively assess the performance of
MobileRAG, we introduce MobileRAG-Eval, a more challenging benchmark
characterized by numerous complex, real-world mobile tasks that require
external knowledge assistance. Extensive experimental results on MobileRAG-Eval
demonstrate that MobileRAG can easily handle real-world mobile tasks, achieving
10.3\% improvement over state-of-the-art methods with fewer operational steps.
Our code is publicly available at:
https://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv

</details>


### [97] [MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question Answering](https://arxiv.org/abs/2509.03918)
*Fengxiao Tang,Yufeng Li,Zongzong Wu,Ming Zhao*

Main category: cs.CL

TL;DR: Matrix of Thought (MoT) 是一种新的大语言模型（LLM）思维结构，通过“列-单元格通信”机制探索问题的横向和纵向维度，以增强 LLM 的推理能力。它还引入了一个事实纠正机制，利用知识图谱三元组和原始文本构建知识单元，以提高初始知识和纠正错误答案。所提出的 MTQA 框架在四个常用数据集上表现优于最先进的方法，并且推理时间仅为基线方法的 14.4%。


<details>
  <summary>Details</summary>
Motivation: 现有的复杂问答（QA）方法，如 Chain-of-Thought（CoT）和 Tree-of-Thought（ToT），在处理复杂和抽象的 QA 任务时，由于推理能力不足，LLM 的性能会严重下降。CoT 和 ToT 分别存在树状结构中的层内冗余和链状结构中的单路径问题。虽然检索增强生成（RAG）方法有助于 LLM 推理，但有效利用涉及多个实体和跳数的大量信息仍然是一个关键挑战。

Method: 提出了一种名为 Matrix of Thought (MoT) 的新颖高效的 LLM 思维结构。“列-单元格通信”机制探索问题的横向和纵向维度，实现多策略和深层思考，减少冗余。开发了一个事实纠正机制，通过构建知识单元（来自检索到的知识图谱三元组和原始文本）来增强 LLM 推理的初始知识并纠正错误答案。基于此构建了一个高效准确的问答框架 (MTQA)。

Result: 在四个广泛使用的数据集上，MTQA 框架在 F1 和 EM 分数方面均优于最先进的方法。与基线方法相比，推理时间仅为 14.4%，证明了其效率和准确性。

Conclusion: 所提出的 Matrix of Thought (MoT) 思维结构和 MTQA 框架能够有效解决复杂问答任务中的推理能力不足和信息利用问题，在准确性和效率方面均取得了显著的改进。

Abstract: Complex Question Answering (QA) is a fundamental and challenging task in NLP.
While large language models (LLMs) exhibit impressive performance in QA, they
suffer from significant performance degradation when facing complex and
abstract QA tasks due to insufficient reasoning capabilities. Works such as
Chain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMs' reasoning
abilities, but they face issues such as in-layer redundancy in tree structures
and single paths in chain structures. Although some studies utilize
Retrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the
challenge of effectively utilizing large amounts of information involving
multiple entities and hops remains critical. To address this, we propose the
Matrix of Thought (MoT), a novel and efficient LLM thought structure. MoT
explores the problem in both horizontal and vertical dimensions through the
"column-cell communication" mechanism, enabling LLMs to actively engage in
multi-strategy and deep-level thinking, reducing redundancy within the column
cells and enhancing reasoning capabilities. Furthermore, we develop a
fact-correction mechanism by constructing knowledge units from retrieved
knowledge graph triples and raw text to enhance the initial knowledge for LLM
reasoning and correct erroneous answers. This leads to the development of an
efficient and accurate QA framework (MTQA). Experimental results show that our
framework outperforms state-of-the-art methods on four widely-used datasets in
terms of F1 and EM scores, with reasoning time only 14.4\% of the baseline
methods, demonstrating both its efficiency and accuracy. The code for this
framework is available at https://github.com/lyfiter/mtqa.

</details>


### [98] [Decoding the Poetic Language of Emotion in Korean Modern Poetry: Insights from a Human-Labeled Dataset and AI Modeling](https://arxiv.org/abs/2509.03932)
*Iro Lim,Haein Ji,Byungjun Kim*

Main category: cs.CL

TL;DR: 本文提出了 KPoEM 数据集，用于现代韩语诗歌的情感计算分析。该数据集包含 7,662 条（包括诗行和作品级别）注释，涵盖 44 种细粒度情感类别，并选取了五位有影响力的韩国诗人。


<details>
  <summary>Details</summary>
Motivation: 尽管在基于大语言模型的文本情感分类方面取得了显著进展，但诗歌，特别是韩语诗歌，由于其比喻语言和文化特异性，仍然是一个未被充分探索的领域。

Method: 构建了一个包含 7,662 条条目的多标签情感数据集，其中包含 483 首诗歌的 7,007 个诗行级别条目和 615 个作品级别条目，并使用五位有影响力的韩国诗人的 44 种细粒度情感类别进行了注释。

Result: 在一个包含 7,662 个条目的多标签情感数据集上对最先进的韩语语言模型进行了微调，该数据集包含 483 首诗歌的 7,007 个诗行级别条目和 615 个作品级别条目，并使用五位有影响力的韩国诗人的 44 种细粒度情感类别进行了注释。结果显示，该模型取得了 0.60 的 F1-micro 分数，而使用通用语料库训练的模型为 0.34。

Conclusion: 通过在通用语料库和 KPoEM 数据集上进行顺序微调训练的 KPoEM 模型，不仅增强了识别时间上和文化上特异性情感表达的能力，还展现出保持韩语现代诗歌核心情感的强大能力。本研究弥合了计算方法与文学分析之间的鸿沟，为通过忠实保留韩语文学情感和文化细微差别的结构化数据对诗歌情感进行定量探索提供了新的可能性。

Abstract: This study introduces KPoEM (Korean Poetry Emotion Mapping) , a novel dataset
for computational emotion analysis in modern Korean poetry. Despite remarkable
progress in text-based emotion classification using large language models,
poetry-particularly Korean poetry-remains underexplored due to its figurative
language and cultural specificity. We built a multi-label emotion dataset of
7,662 entries, including 7,007 line-level entries from 483 poems and 615
work-level entries, annotated with 44 fine-grained emotion categories from five
influential Korean poets. A state-of-the-art Korean language model fine-tuned
on this dataset significantly outperformed previous models, achieving 0.60
F1-micro compared to 0.34 from models trained on general corpora. The KPoEM
model, trained through sequential fine-tuning-first on general corpora and then
on the KPoEM dataset-demonstrates not only an enhanced ability to identify
temporally and culturally specific emotional expressions, but also a strong
capacity to preserve the core sentiments of modern Korean poetry. This study
bridges computational methods and literary analysis, presenting new
possibilities for the quantitative exploration of poetic emotions through
structured data that faithfully retains the emotional and cultural nuances of
Korean literature.

</details>


### [99] [SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment](https://arxiv.org/abs/2509.03934)
*Yuqing Huang,Rongyang Zhang,Qimeng Wang,Chengqiang Lu,Yan Gao,Yi Wu,Yao Hu,Xuyang Zhi,Guiquan Liu,Xin Li,Hao Wang,Enhong Chen*

Main category: cs.CL

TL;DR: SelfAug是一种通过对齐输入序列的logits来保留模型语义分布的新方法，可以减轻灾难性遗忘并提高下游性能。


<details>
  <summary>Details</summary>
Motivation: 监督微调（尤其是在检索增强生成RAG的场景中）虽然能提升特定任务的性能，但常导致模型遗忘通用知识和先前能力（灾难性遗忘）。现有方法或需要通用指令数据，或在保持模型原始分布方面存在局限。

Method: 提出了一种名为SelfAug的自分布对齐方法，通过对齐输入序列的logits来保留模型的语义分布，从而减轻灾难性遗忘并提高下游性能。

Result: 实验证明SelfAug在下游学习和通用能力保持之间取得了更好的平衡。研究发现分布偏移与RAG场景下灾难性遗忘的严重程度直接相关，并指出了在通用指令微调中缺少RAG能力会导致微调过程中显著的分布偏移。

Conclusion: SelfAug不仅加深了对RAG场景下灾难性遗忘的理解，还提供了一种实用的、可应用于多种微调场景的解决方案。

Abstract: Recent advancements in large language models (LLMs) have revolutionized
natural language processing through their remarkable capabilities in
understanding and executing diverse tasks. While supervised fine-tuning,
particularly in Retrieval-Augmented Generation (RAG) scenarios, effectively
enhances task-specific performance, it often leads to catastrophic forgetting,
where models lose their previously acquired knowledge and general capabilities.
Existing solutions either require access to general instruction data or face
limitations in preserving the model's original distribution. To overcome these
limitations, we propose SelfAug, a self-distribution alignment method that
aligns input sequence logits to preserve the model's semantic distribution,
thereby mitigating catastrophic forgetting and improving downstream
performance. Extensive experiments demonstrate that SelfAug achieves a superior
balance between downstream learning and general capability retention. Our
comprehensive empirical analysis reveals a direct correlation between
distribution shifts and the severity of catastrophic forgetting in RAG
scenarios, highlighting how the absence of RAG capabilities in general
instruction tuning leads to significant distribution shifts during fine-tuning.
Our findings not only advance the understanding of catastrophic forgetting in
RAG contexts but also provide a practical solution applicable across diverse
fine-tuning scenarios. Our code is publicly available at
https://github.com/USTC-StarTeam/SelfAug.

</details>


### [100] [SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by Self-Play Fine-Tuning](https://arxiv.org/abs/2509.03937)
*Yuhao Zhang,Shaoming Duan,Jinhang Su,Chuanyi Liu,Peiyi Han*

Main category: cs.CL

TL;DR: SPIN在Text-to-SQL任务中存在不足，提出SPFT-SQL方法，通过验证式迭代微调生成高质量数据，并采用错误驱动损失函数提升模型区分正确和错误SQL的能力，在多个基准测试中表现优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: SPIN方法在Text-to-SQL任务中存在不足，无法生成新信息，且对手模型产生的正确SQL查询过多会影响主模型生成准确SQL的能力。

Method: 在SPIN方法基础上，提出SPFT-SQL。在自我对抗微调前，采用验证式迭代微调方法，基于数据库模式和验证反馈，迭代合成高质量微调数据，以提升模型性能并构建不同能力的模型库。在自我对抗微调阶段，提出错误驱动损失函数，激励对手模型产生错误输出，使主模型能区分正确SQL和对手模型产生的错误SQL，从而提升生成正确SQL的能力。

Result: 在六个开源LLM和五个广泛使用的基准测试上进行了大量实验和深入分析，证明了SPFT-SQL方法优于现有的SOTA方法。

Conclusion: SPFT-SQL通过验证式迭代微调和错误驱动损失函数，有效解决了SPIN在Text-to-SQL任务中的挑战，显著提升了Text-to-SQL的性能。

Abstract: Despite the significant advancements of self-play fine-tuning (SPIN), which
can transform a weak large language model (LLM) into a strong one through
competitive interactions between models of varying capabilities, it still faces
challenges in the Text-to-SQL task. SPIN does not generate new information, and
the large number of correct SQL queries produced by the opponent model during
self-play reduces the main model's ability to generate accurate SQL queries. To
address this challenge, we propose a new self-play fine-tuning method tailored
for the Text-to-SQL task, called SPFT-SQL. Prior to self-play, we introduce a
verification-based iterative fine-tuning approach, which synthesizes
high-quality fine-tuning data iteratively based on the database schema and
validation feedback to enhance model performance, while building a model base
with varying capabilities. During the self-play fine-tuning phase, we propose
an error-driven loss method that incentivizes incorrect outputs from the
opponent model, enabling the main model to distinguish between correct SQL and
erroneous SQL generated by the opponent model, thereby improving its ability to
generate correct SQL. Extensive experiments and in-depth analyses on six
open-source LLMs and five widely used benchmarks demonstrate that our approach
outperforms existing state-of-the-art (SOTA) methods.

</details>


### [101] [VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based Role-Playing Agents](https://arxiv.org/abs/2509.03940)
*Weihao Wu,Liang Cao,Xinyu Wu,Zhiwei Lin,Rui Niu,Jingbei Li,Zhiyong Wu*

Main category: cs.CL

TL;DR: 这项工作引入了一个名为VoxRole的新基准，用于评估基于语音的角色扮演对话剂（RPCAs），解决了现有RPCAs仅关注文本而忽略语音线索以及缺乏标准化评估基准的问题。VoxRole包含大量多轮对话数据，并通过一个两阶段的自动化流程构建了多维度的角色档案，旨在促进对RPCAs长期角色一致性等核心能力的量化评估。


<details>
  <summary>Details</summary>
Motivation: 现有的RPCAs研究主要集中在文本模态，忽略了语音中的语调、韵律和节奏等韵律特征，而这些特征对于传达角色情感和塑造生动形象至关重要。此外，基于语音的角色扮演领域长期缺乏标准化的评估基准，现有的数据集往往只关注基本能力，角色设定模糊，无法有效评估模型在长期角色一致性方面的表现。

Method: 我们提出了一个名为VoxRole的新基准，它包含13335个多轮对话，总计65.6小时的语音，来自261部电影中的1228个独特角色。为了构建这个资源，我们提出了一种新颖的两阶段自动化流程：首先将电影音频与脚本对齐，然后利用大型语言模型（LLM）为每个角色系统地构建多维度档案。

Result: 利用VoxRole，我们对当代语音对话模型进行了多维度评估，揭示了它们在保持角色一致性方面的各自优势和局限性。

Conclusion: VoxRole是第一个专门为评估语音RPCAs而设计的综合基准，它通过提供包含丰富韵律特征和细致角色档案的大规模数据集，解决了现有研究的局限性，并为评估和改进RPCAs的性能提供了重要的基础。

Abstract: Recent significant advancements in Large Language Models (LLMs) have greatly
propelled the development of Role-Playing Conversational Agents (RPCAs). These
systems aim to create immersive user experiences through consistent persona
adoption. However, current RPCA research faces dual limitations. First,
existing work predominantly focuses on the textual modality, entirely
overlooking critical paralinguistic features including intonation, prosody, and
rhythm in speech, which are essential for conveying character emotions and
shaping vivid identities. Second, the speech-based role-playing domain suffers
from a long-standing lack of standardized evaluation benchmarks. Most current
spoken dialogue datasets target only fundamental capability assessments,
featuring thinly sketched or ill-defined character profiles. Consequently, they
fail to effectively quantify model performance on core competencies like
long-term persona consistency. To address this critical gap, we introduce
VoxRole, the first comprehensive benchmark specifically designed for the
evaluation of speech-based RPCAs. The benchmark comprises 13335 multi-turn
dialogues, totaling 65.6 hours of speech from 1228 unique characters across 261
movies. To construct this resource, we propose a novel two-stage automated
pipeline that first aligns movie audio with scripts and subsequently employs an
LLM to systematically build multi-dimensional profiles for each character.
Leveraging VoxRole, we conduct a multi-dimensional evaluation of contemporary
spoken dialogue models, revealing crucial insights into their respective
strengths and limitations in maintaining persona consistency.

</details>


### [102] [CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking](https://arxiv.org/abs/2509.03957)
*Ruiling Guo,Xinwei Yang,Chen Huang,Tong Zhang,Yong Hu*

Main category: cs.CL

TL;DR: LLMs在事实核查方面仍有局限性，但有潜力辅助人工进行事实核查。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在核查中文虚假信息方面的能力和局限性。

Method: 构建了一个包含约20,000个实例的、经过仔细标注的数据集，并分析了当前LLMs在事实核查方面的表现，包括使用思维链（chain-of-thought）推理和少样本（few-shot）提示。开发了一个分类体系来解释LLMs生成错误结论的原因。

Result: 当前LLMs在生成准确的事实核查结论方面存在局限性。事实捏造是最常见的错误模式。LLMs单独使用并不可靠，但作为辅助工具可以显著提升人工核查的效率。

Conclusion: 尽管LLMs在事实核查方面存在不足，但它们有潜力作为辅助工具来增强人类在事实核查任务中的表现。

Abstract: The effectiveness of large language models (LLMs) to fact-check
misinformation remains uncertain, despite their growing use. To this end, we
present CANDY, a benchmark designed to systematically evaluate the capabilities
and limitations of LLMs in fact-checking Chinese misinformation. Specifically,
we curate a carefully annotated dataset of ~20k instances. Our analysis shows
that current LLMs exhibit limitations in generating accurate fact-checking
conclusions, even when enhanced with chain-of-thought reasoning and few-shot
prompting. To understand these limitations, we develop a taxonomy to categorize
flawed LLM-generated explanations for their conclusions and identify factual
fabrication as the most common failure mode. Although LLMs alone are unreliable
for fact-checking, our findings indicate their considerable potential to
augment human performance when deployed as assistive tools in scenarios. Our
dataset and code can be accessed at https://github.com/SCUNLP/CANDY

</details>


### [103] [Exploring NLP Benchmarks in an Extremely Low-Resource Setting](https://arxiv.org/abs/2509.03962)
*Ulin Nuha,Adam Jatowt*

Main category: cs.CL

TL;DR: LLMs在低资源语言（如土著语言）上效果不佳，因为缺乏标记数据。本研究针对濒危的拉丁语（Val Badia方言），利用少量拉丁语-意大利语平行句对，通过翻译意大利语单语数据来创建用于情感分析和多项选择问答（MCQA）的合成数据集。该方法通过严格的过滤和回译程序来确保语言质量。将这些合成数据集纳入机器翻译训练可显著提高意大利语-拉丁语翻译效果。本研究成果包括首个公开的拉丁语情感分析和MCQA数据集，为该代表性不足的语言的NLP研究和应用奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在像拉丁语这样的低资源语言上的有效性受到标记数据稀缺的限制，这阻碍了语言技术的发展。

Method: 利用少量的拉丁语-意大利语平行句对，通过翻译意大利语单语数据创建用于情感分析和多项选择问答（MCQA）的合成数据集，并采用严格的过滤和回译程序以确保质量。

Result: 将合成数据集纳入机器翻译训练可显著提高意大利语-拉丁语翻译效果，优于现有基线。

Conclusion: 本研究创建了首批公开的拉丁语情感分析和MCQA数据集，为拉丁语的NLP研究和应用奠定了基础。

Abstract: The effectiveness of Large Language Models (LLMs) diminishes for extremely
low-resource languages, such as indigenous languages, primarily due to the lack
of labeled data. Despite growing interest, the availability of high-quality
natural language processing (NLP) datasets for these languages remains limited,
making it difficult to develop robust language technologies. This paper
addresses such gap by focusing on Ladin, an endangered Romance language,
specifically targeting the Val Badia variant. Leveraging a small set of
parallel Ladin-Italian sentence pairs, we create synthetic datasets for
sentiment analysis and multiple-choice question answering (MCQA) by translating
monolingual Italian data. To ensure linguistic quality and reliability, we
apply rigorous filtering and back-translation procedures in our method. We
further demonstrate that incorporating these synthetic datasets into machine
translation training leads to substantial improvements over existing
Italian-Ladin translation baselines. Our contributions include the first
publicly available sentiment analysis and MCQA datasets for Ladin, establishing
foundational resources that can support broader NLP research and downstream
applications for this underrepresented language.

</details>


### [104] [Expanding Foundational Language Capabilities in Open-Source LLMs through a Korean Case Study](https://arxiv.org/abs/2509.03972)
*Junghwan Lim,Gangwon Jo,Sungmin Lee,Jiyoung Park,Dongseok Kim,Jihwan Kim,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Kibong Choi,Jaeyeon Huh,Beomgyu Kim,Jangwoong Kim,Taehyun Kim,Haesol Lee,Jeesoo Lee,Dongpin Oh,Changseok Song,Daewon Suh*

Main category: cs.CL

TL;DR: Llama-3-Motif是一个拥有1020亿参数的语言模型，专注于提升韩语能力，同时保持优秀的英语性能。


<details>
  <summary>Details</summary>
Motivation: 为满足在韩语处理方面不断增长的需求，并在此基础上提供与现有先进模型（如GPT-4）相媲美的性能。

Method: 在Llama 3架构的基础上，利用LlamaPro和Masked Structure Growth等先进训练技术，通过MoAI平台进行大规模GPU集群训练，并使用精心策划的韩英数据平衡数据集进行优化。

Result: 在韩语特定基准测试中表现出色，超越了现有模型，并达到了与GPT-4相当的水平。

Conclusion: Llama-3-Motif在韩语处理方面取得了显著进展，并保持了强大的多语言能力。

Abstract: We introduce Llama-3-Motif, a language model consisting of 102 billion
parameters, specifically designed to enhance Korean capabilities while
retaining strong performance in English. Developed on the Llama 3 architecture,
Llama-3-Motif employs advanced training techniques, including LlamaPro and
Masked Structure Growth, to effectively scale the model without altering its
core Transformer architecture. Using the MoAI platform for efficient training
across hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully
curated dataset that maintains a balanced ratio of Korean and English data.
Llama-3-Motif shows decent performance on Korean-specific benchmarks,
outperforming existing models and achieving results comparable to GPT-4.

</details>


### [105] [RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering with Large Language Models](https://arxiv.org/abs/2509.03995)
*Zhaoyan Gong,Juan Li,Zhiqiang Liu,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: RTQA是一个新框架，通过增强在TKG上的推理能力来解决现有TKGQA方法关注隐式时间约束、推理能力有限和分解框架中错误传播等问题，而无需进行训练。该框架通过递归地将问题分解为子问题，然后自底向上地使用LLM和TKG知识进行求解，并采用多路径答案聚合来提高容错性。实验结果表明，RTQA在MultiTQ和TimelineKGQA基准测试的“Multiple”和“Complex”类别上显著提高了Hits@1，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的时间知识图谱问答（TKGQA）方法主要关注隐式时间约束，缺乏处理更复杂时间查询的能力，并且在分解框架中存在推理能力有限和错误传播的问题。

Method: RTQA框架通过递归地将问题分解为子问题，自底向上地利用大型语言模型（LLM）和时间知识图谱（TKG）的知识进行求解，并采用多路径答案聚合来提高容错性。该框架包括三个核心组件：时间问题分解器、递归求解器和答案聚合器。

Result: 在MultiTQ和TimelineKGQA基准测试上的实验表明，RTQA在“Multiple”和“Complex”类别上显著提高了Hits@1指标，优于现有的最先进方法。

Conclusion: RTQA通过递归分解、自底向上求解以及多路径答案聚合，有效解决了现有TKGQA方法在处理复杂时间查询、推理能力和错误传播方面的挑战，并在多个基准测试上取得了优于现有方法的性能。

Abstract: Current temporal knowledge graph question answering (TKGQA) methods primarily
focus on implicit temporal constraints, lacking the capability of handling more
complex temporal queries, and struggle with limited reasoning abilities and
error propagation in decomposition frameworks. We propose RTQA, a novel
framework to address these challenges by enhancing reasoning over TKGs without
requiring training. Following recursive thinking, RTQA recursively decomposes
questions into sub-problems, solves them bottom-up using LLMs and TKG
knowledge, and employs multi-path answer aggregation to improve fault
tolerance. RTQA consists of three core components: the Temporal Question
Decomposer, the Recursive Solver, and the Answer Aggregator. Experiments on
MultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements
in "Multiple" and "Complex" categories, outperforming state-of-the-art methods.
Our code and data are available at https://github.com/zjukg/RTQA.

</details>


### [106] [On Robustness and Reliability of Benchmark-Based Evaluation of LLMs](https://arxiv.org/abs/2509.04013)
*Riccardo Lunardi,Vincenzo Della Mea,Stefano Mizzaro,Kevin Roitero*

Main category: cs.CL

TL;DR: LLM在基准测试中的表现不稳定，因为它对问题的释义很敏感，这引发了对其泛化能力和评估方法的担忧。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在处理多样化释义查询方面的鲁棒性，并质疑现有基准测试的有效性。

Method: 对六个基准测试中的所有问题进行释义，并评估34个先进LLM在这些释义问题上的表现。

Result: LLM在释义问题上的表现下降，尽管排名保持相对稳定，但分数显著降低。

Conclusion: LLM对语言变异的敏感性对其泛化能力和现有评估方法的可靠性提出了质疑，需要开发更鲁棒的评估基准。

Abstract: Large Language Models (LLMs) effectiveness is usually evaluated by means of
benchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in
their original wording, thus in a fixed, standardized format. However,
real-world applications involve linguistic variability, requiring models to
maintain their effectiveness across diverse rewordings of the same question or
query. In this study, we systematically assess the robustness of LLMs to
paraphrased benchmark questions and investigate whether benchmark-based
evaluations provide a reliable measure of model capabilities. We systematically
generate various paraphrases of all the questions across six different common
benchmarks, and measure the resulting variations in effectiveness of 34
state-of-the-art LLMs, of different size and effectiveness. Our findings reveal
that while LLM rankings remain relatively stable across paraphrased inputs,
absolute effectiveness scores change, and decline significantly. This suggests
that LLMs struggle with linguistic variability, raising concerns about their
generalization abilities and evaluation methodologies. Furthermore, the
observed performance drop challenges the reliability of benchmark-based
evaluations, indicating that high benchmark scores may not fully capture a
model's robustness to real-world input variations. We discuss the implications
of these findings for LLM evaluation methodologies, emphasizing the need for
robustness-aware benchmarks that better reflect practical deployment scenarios.

</details>


### [107] [What if I ask in \textit{alia lingua}? Measuring Functional Similarity Across Languages](https://arxiv.org/abs/2509.04032)
*Debangan Mishra,Arihant Rastogi,Agyeya Negi,Shashwat Goel,Ponnurangam Kumaraguru*

Main category: cs.CL

TL;DR: 大型语言模型在不同语言间的输出具有高度一致性，尤其是在模型规模和能力增长时。模型内部的一致性甚至超过了与其他模型在相同语言下的reement程度。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在不同语言间的输出相似度，并评估一种新的模型相似性度量指标 $\kappa_p$ 在此场景下的有效性。

Method: 使用新提出的模型相似性度量指标 $\kappa_p$，在 GlobalMMLU 数据集上对 20 种语言和 47 个学科的 20 种模型输出来进行分析。

Result: 模型规模和能力越大，其跨语言输出的一致性越高。模型内部的一致性高于与其他模型在相同语言下的reement程度。

Conclusion: $\kappa_p$ 是一个评估多语言可靠性的实用工具，并且可以指导开发更具一致性的多语言系统。

Abstract: How similar are model outputs across languages? In this work, we study this
question using a recently proposed model similarity metric $\kappa_p$ applied
to 20 languages and 47 subjects in GlobalMMLU. Our analysis reveals that a
model's responses become increasingly consistent across languages as its size
and capability grow. Interestingly, models exhibit greater cross-lingual
consistency within themselves than agreement with other models prompted in the
same language. These results highlight not only the value of $\kappa_p$ as a
practical tool for evaluating multilingual reliability, but also its potential
to guide the development of more consistent multilingual systems.

</details>


### [108] [A RoBERTa-Based Functional Syntax Annotation Model for Chinese Texts](https://arxiv.org/abs/2509.04046)
*Han Xiaohui,Zhang Yunlong,Guo Yuxi*

Main category: cs.CL

TL;DR: 该研究介绍了首个基于RoBERTa的中文功能句法自动标注模型，用于填补中文文本在功能句法分析上的空白。


<details>
  <summary>Details</summary>
Motivation: 目前尚无基于系统功能语法理论的中文文本自动标注系统，这限制了相关理论的应用和推广。

Method: 随机选取4100句人民日报2014年语料库，依据功能句法理论进行标注，构建训练数据集。然后，基于该数据集对RoBERTa-Chinese wwm-ext模型进行微调，以实现命名实体识别任务。

Result: 在测试集上达到0.852的F1分数，显著优于其他对比模型，在识别主语（S）、谓语（M）和补语（C）等核心句法成分方面表现出色。然而，在识别标签样本不平衡的实体方面仍有提升空间。

Conclusion: 该研究首次将功能句法与基于注意力机制的自然语言处理模型相结合，为中文功能句法自动分析提供了新方法，并为后续研究奠定了基础。

Abstract: Systemic Functional Grammar and its branch, Cardiff Grammar, have been widely
applied to discourse analysis, semantic function research, and other tasks
across various languages and texts. However, an automatic annotation system
based on this theory for Chinese texts has not yet been developed, which
significantly constrains the application and promotion of relevant theories. To
fill this gap, this research introduces a functional syntax annotation model
for Chinese based on RoBERTa (Robustly Optimized BERT Pretraining Approach).
The study randomly selected 4,100 sentences from the People's Daily 2014 corpus
and annotated them according to functional syntax theory to establish a dataset
for training. The study then fine-tuned the RoBERTa-Chinese wwm-ext model based
on the dataset to implement the named entity recognition task, achieving an F1
score of 0.852 on the test set that significantly outperforms other comparative
models. The model demonstrated excellent performance in identifying core
syntactic elements such as Subject (S), Main Verb (M), and Complement (C).
Nevertheless, there remains room for improvement in recognizing entities with
imbalanced label samples. As the first integration of functional syntax with
attention-based NLP models, this research provides a new method for automated
Chinese functional syntax analysis and lays a solid foundation for subsequent
studies.

</details>


### [109] [Synthesizing Sheet Music Problems for Evaluation and Reinforcement Learning](https://arxiv.org/abs/2509.04059)
*Zhilin Wang,Zhe Yang,Yun Luo,Yafu Li,Haoran Zhang,Runzhe Zhan,Derek F. Wong,Jizhe Zhou,Yu Cheng*

Main category: cs.CL

TL;DR: 本研究提出了一个用于生成和评估 LLM/MLLM 对乐谱理解能力的数据集 SSMR-Bench，并利用该数据集通过强化学习提升了模型的乐谱推理能力，同时促进了音乐创作。


<details>
  <summary>Details</summary>
Motivation: 当前的 LLM/MLLM 在理解乐谱方面能力不足，缺乏相应的评估基准和训练数据。

Method: 提出通过音乐理论规则合成乐谱问题，构建了 SSMR-Bench 数据集，并将其用于强化学习（RLVR），以训练模型理解乐谱和进行音乐创作。

Result: SSMR-Bench 上的评估结果表明模型的推理能力对乐谱理解至关重要，Gemini 2.5-Pro 的表现不佳凸显了 MLLM 在视觉乐谱理解方面面临的挑战。通过 RLVR 训练的 Qwen3-8B-Base 和 Qwen2.5-VL-Instruct 在 SSMR-Bench 上有所提升，其中 Qwen3-8B-Base 的表现超过了 GPT-4。

Conclusion: 首次提出基于音乐理论规则合成乐谱问题的想法，并证明了其在提升模型乐谱理解推理能力和促进 AI 音乐创作方面的有效性。

Abstract: Enhancing the ability of Large Language Models (LLMs) and Multimodal Large
Language Models (MLLMs) to interpret sheet music is a crucial step toward
building AI musicians. However, current research lacks both evaluation
benchmarks and training data for sheet music reasoning. To address this, we
propose the idea of synthesizing sheet music problems grounded in music theory,
which can serve both as evaluation benchmarks and as training data for
reinforcement learning with verifiable rewards (RLVR). We introduce a data
synthesis framework that generates verifiable sheet music questions in both
textual and visual modalities, leading to the Synthetic Sheet Music Reasoning
Benchmark (SSMR-Bench) and a complementary training set. Evaluation results on
SSMR-Bench show the importance of models' reasoning abilities in interpreting
sheet music. At the same time, the poor performance of Gemini 2.5-Pro
highlights the challenges that MLLMs still face in interpreting sheet music in
a visual format. By leveraging synthetic data for RLVR, Qwen3-8B-Base and
Qwen2.5-VL-Instruct achieve improvements on the SSMR-Bench. Besides, the
trained Qwen3-8B-Base surpasses GPT-4 in overall performance on
MusicTheoryBench and achieves reasoning performance comparable to GPT-4 with
the strategies of Role play and Chain-of-Thought. Notably, its performance on
math problems also improves relative to the original Qwen3-8B-Base.
Furthermore, our results show that the enhanced reasoning ability can also
facilitate music composition. In conclusion, we are the first to propose the
idea of synthesizing sheet music problems based on music theory rules, and
demonstrate its effectiveness not only in advancing model reasoning for sheet
music understanding but also in unlocking new possibilities for AI-assisted
music creation.

</details>


### [110] [Arabic Chatbot Technologies in Education: An Overview](https://arxiv.org/abs/2509.04066)
*Hicham Bourhil,Yacine El Younoussi*

Main category: cs.CL

TL;DR: 尽管AI在其他语言的教育领域取得了成功，但很少有教育性阿拉伯语聊天机器人采用现代技术，这表明该领域存在研究空白，未来需要更多的研究。


<details>
  <summary>Details</summary>
Motivation: 文章旨在调查现有的教育领域阿拉伯语聊天机器人，并分析其特性，以识别研究空白。

Method: 对现有教育领域阿拉伯语聊天机器人进行调查，分析其采用的方法、语言变体和性能度量标准。

Result: 识别出，尽管英语聊天机器人取得了成功，但只有少数教育性阿拉伯语聊天机器人采用了现代技术。

Conclusion: 阿拉伯语教育聊天机器人的研究仍然存在差距，尽管AI和LLM技术取得了进展，但很少有阿拉伯语聊天机器人利用这些现代技术，这为未来的研究指明了方向。

Abstract: The recent advancements in Artificial Intelligence (AI) in general, and in
Natural Language Processing (NLP) in particular, and some of its applications
such as chatbots, have led to their implementation in different domains like
education, healthcare, tourism, and customer service. Since the COVID-19
pandemic, there has been an increasing interest in these digital technologies
to allow and enhance remote access. In education, e-learning systems have been
massively adopted worldwide. The emergence of Large Language Models (LLM) such
as BERT (Bidirectional Encoder Representations from Transformers) and GPT
(Generative Pre-trained Transformers) made chatbots even more popular. In this
study, we present a survey on existing Arabic chatbots in education and their
different characteristics such as the adopted approaches, language variety, and
metrics used to measure their performance. We were able to identified some
research gaps when we discovered that, despite the success of chatbots in other
languages such as English, only a few educational Arabic chatbots used modern
techniques. Finally, we discuss future directions of research in this field.

</details>


### [111] [Improving Narrative Classification and Explanation via Fine Tuned Language Models](https://arxiv.org/abs/2509.04077)
*Rishit Tyagi,Rahul Bouri,Mohit Gupta*

Main category: cs.CL

TL;DR: 本研究提出了一种结合BERT和GPT-4o的方法，用于识别新闻文章中的隐性叙事和生成解释，并取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 传统自然语言处理方法难以识别隐晦的措辞和隐藏的议程，因此理解隐性叙事和隐含信息对于分析偏见和情绪至关重要。

Method: 研究利用BERT模型进行多标签分类，并结合GPT-4o进行叙事解释。提出了一种结合推理和行动（ReACT）的框架，并利用基于语义检索的少样本提示，同时引入结构化分类表作为辅助知识库，以提高事实准确性并减少幻觉。

Result: 实验结果表明，在提示中整合辅助知识可以提高分类准确性和解释的可靠性。

Conclusion: 本研究提出的方法在媒体分析、教育和情报收集等领域具有潜在应用价值。

Abstract: Understanding covert narratives and implicit messaging is essential for
analyzing bias and sentiment. Traditional NLP methods struggle with detecting
subtle phrasing and hidden agendas. This study tackles two key challenges: (1)
multi-label classification of narratives and sub-narratives in news articles,
and (2) generating concise, evidence-based explanations for dominant
narratives. We fine-tune a BERT model with a recall-oriented approach for
comprehensive narrative detection, refining predictions using a GPT-4o pipeline
for consistency. For narrative explanation, we propose a ReACT (Reasoning +
Acting) framework with semantic retrieval-based few-shot prompting, ensuring
grounded and relevant justifications. To enhance factual accuracy and reduce
hallucinations, we incorporate a structured taxonomy table as an auxiliary
knowledge base. Our results show that integrating auxiliary knowledge in
prompts improves classification accuracy and justification reliability, with
applications in media analysis, education, and intelligence gathering.

</details>


### [112] [Towards Stable and Personalised Profiles for Lexical Alignment in Spoken Human-Agent Dialogue](https://arxiv.org/abs/2509.04104)
*Keara Schaaij,Roel Boumans,Tibor Bosse,Iris Hendrickx*

Main category: cs.CL

TL;DR: 本研究首次探索了在对话代理中实现词汇对齐，重点是构建稳定、个性化的词汇档案。


<details>
  <summary>Details</summary>
Motivation: 尽管词汇对齐（说话者使用相似词语）有助于沟通，但其在对话代理中的应用仍未得到充分研究，尤其是在大型语言模型（LLM）取得进展的背景下。

Method: 研究人员构建了个性化的词汇档案，并测试了不同数量的语音数据和词汇档案（按词性分类）对档案性能的影响，使用召回率、覆盖率和余弦相似度等指标进行评估。

Result: 研究发现，使用10分钟转录语音数据构建的、包含5个形容词、5个连词、以及每个10个动词、名词、代词和副词的精简词汇档案，在性能和数据效率方面取得了最佳平衡。

Conclusion: 本研究为构建稳定、个性化的词汇档案提供了实用见解，考虑了最低数据要求，为对话代理中的词汇对齐策略奠定了基础。

Abstract: Lexical alignment, where speakers start to use similar words across
conversation, is known to contribute to successful communication. However, its
implementation in conversational agents remains underexplored, particularly
considering the recent advancements in large language models (LLMs). As a first
step towards enabling lexical alignment in human-agent dialogue, this study
draws on strategies for personalising conversational agents and investigates
the construction of stable, personalised lexical profiles as a basis for
lexical alignment. Specifically, we varied the amounts of transcribed spoken
data used for construction as well as the number of items included in the
profiles per part-of-speech (POS) category and evaluated profile performance
across time using recall, coverage, and cosine similarity metrics. It was shown
that smaller and more compact profiles, created after 10 min of transcribed
speech containing 5 items for adjectives, 5 items for conjunctions, and 10
items for adverbs, nouns, pronouns, and verbs each, offered the best balance in
both performance and data efficiency. In conclusion, this study offers
practical insights into constructing stable, personalised lexical profiles,
taking into account minimal data requirements, serving as a foundational step
toward lexical alignment strategies in conversational agents.

</details>


### [113] [MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages](https://arxiv.org/abs/2509.04111)
*Dan Saattrup Smart*

Main category: cs.CL

TL;DR: MultiWikiQA是一个包含306种语言的多语言阅读理解数据集，其问题由LLM生成，答案来自维基百科，并经过众包人类评估，证明了其质量。该数据集足够难，不同语言模型在该数据集上表现出显著的性能差异。


<details>
  <summary>Details</summary>
Motivation: 介绍了一个名为MultiWikiQA的新阅读理解数据集，该数据集涵盖306种语言，旨在提供一个具有挑战性的多语言评估基准。

Method: 使用维基百科文章作为上下文，由LLM生成问题，答案直接摘自文章。对30种语言的生成问题进行了众包人类流畅性评估。评估了6种不同大小的解码器和编码器语言模型。

Result: 评估结果显示，该基准具有足够的挑战性，并且在不同语言之间存在显著的性能差异。众包评估证明了生成问题具有良好的质量。

Conclusion: MultiWikiQA数据集是一个高质量、覆盖广泛语言的阅读理解基准，可用于评估和改进语言模型在多语言阅读理解任务上的表现。

Abstract: We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which
covers 306 languages. The context data comes from Wikipedia articles, with
questions generated by an LLM and the answers appearing verbatim in the
Wikipedia articles. We conduct a crowdsourced human evaluation of the fluency
of the generated questions across 30 of the languages, providing evidence that
the questions are of good quality. We evaluate 6 different language models,
both decoder and encoder models of varying sizes, showing that the benchmark is
sufficiently difficult and that there is a large performance discrepancy
amongst the languages. The dataset and survey evaluations are freely available.

</details>


### [114] [Joint Modeling of Entities and Discourse Relations for Coherence Assessment](https://arxiv.org/abs/2509.04182)
*Wei Liu,Michael Strube*

Main category: cs.CL

TL;DR: 本研究提出联合建模实体和语篇关系以提升文本连贯性评估效果。


<details>
  <summary>Details</summary>
Motivation: 现有工作多独立研究实体特征或语篇关系特征，忽视了两者结合对文本连贯性的影响。

Method: 提出并实验了两种联合建模实体和语篇关系的方法。

Result: 实验表明，结合实体和语篇关系特征能显著提升连贯性模型的性能。

Conclusion: 同时对两者进行建模有利于文本连贯性评估。

Abstract: In linguistics, coherence can be achieved by different means, such as by
maintaining reference to the same set of entities across sentences and by
establishing discourse relations between them. However, most existing work on
coherence modeling focuses exclusively on either entity features or discourse
relation features, with little attention given to combining the two. In this
study, we explore two methods for jointly modeling entities and discourse
relations for coherence assessment. Experiments on three benchmark datasets
show that integrating both types of features significantly enhances the
performance of coherence models, highlighting the benefits of modeling both
simultaneously for coherence evaluation.

</details>


### [115] [MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn Mental Health Counseling Sessions](https://arxiv.org/abs/2509.04183)
*Aishik Mandal,Tanmoy Chakraborty,Iryna Gurevych*

Main category: cs.CL

TL;DR: MAGneT是一个新的多代理框架，用于生成合成心理咨询会话，通过将咨询师响应生成分解为由专业LLM代理处理的协调子任务，从而解决了高质量、隐私合规的咨询数据稀缺的问题。该框架在生成会话的质量、多样性和治疗一致性方面显著优于现有方法，并且在微调开源模型时也表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有高质量、隐私合规的心理咨询数据稀缺，无法满足对可扩展心理咨询日益增长的需求。

Method: 提出了一种名为MAGneT的新型多代理框架，将咨询师响应生成分解为由专门的LLM代理处理的协调子任务，每个代理模拟一种关键的心理技术。此外，还提出了一个统一的评估框架，整合了各种自动和专家指标，并扩展了专家评估的方面。

Result: MAGneT生成的咨询会话在质量、多样性和治疗一致性方面显著优于现有方法，平均在认知治疗评定量表(CTRS)上，通用咨询技能提高了3.2%，CBT特定技能提高了4.3%。专家在77.2%的情况下更喜欢MAGneT生成的会话。使用MAGneT生成的会话进行微调的模型，其在CTRS上的表现比使用基线方法生成的会话进行微调的模型分别提高了6.3%和7.3%。

Conclusion: MAGneT框架能够生成高质量、多样化且符合治疗要求的合成心理咨询会话，并能有效提升微调模型的性能，为解决高质量咨询数据稀缺的问题提供了一个有前景的解决方案。

Abstract: The growing demand for scalable psychological counseling highlights the need
for fine-tuning open-source Large Language Models (LLMs) with high-quality,
privacy-compliant data, yet such data remains scarce. Here we introduce MAGneT,
a novel multi-agent framework for synthetic psychological counseling session
generation that decomposes counselor response generation into coordinated
sub-tasks handled by specialized LLM agents, each modeling a key psychological
technique. Unlike prior single-agent approaches, MAGneT better captures the
structure and nuance of real counseling. In addition, we address
inconsistencies in prior evaluation protocols by proposing a unified evaluation
framework integrating diverse automatic and expert metrics. Furthermore, we
expand the expert evaluations from four aspects of counseling in previous works
to nine aspects, enabling a more thorough and robust assessment of data
quality. Empirical results show that MAGneT significantly outperforms existing
methods in quality, diversity, and therapeutic alignment of the generated
counseling sessions, improving general counseling skills by 3.2% and
CBT-specific skills by 4.3% on average on cognitive therapy rating scale
(CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases
on average across all aspects. Moreover, fine-tuning an open-source model on
MAGneT-generated sessions shows better performance, with improvements of 6.3%
on general counseling skills and 7.3% on CBT-specific skills on average on CTRS
over those fine-tuned with sessions generated by baseline methods. We also make
our code and data public.

</details>


### [116] [Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?](https://arxiv.org/abs/2509.04292)
*Qinyan Zhang,Xinping Lei,Ruijie Miao,Yu Fu,Haojie Fan,Le Chang,Jiafan Hou,Dingling Zhang,Zhongfei Hou,Ziqiang Yang,Changxin Pu,Fei Hu,Jingkai Liu,Mengyun Liu,Yang Liu,Xiang Gao,Jiaheng Liu,Tong Yang,Zaiyuan Wang,Ge Zhang,Wenhao Huang*

Main category: cs.CL

TL;DR: LLMs在各种任务上表现优异，但在遵循与监督微调（SFT）期间学到的标准化模式相冲突的指令时，常常表现出认知惯性。为了评估这一局限性，我们提出了Inverse IFEval，一个衡量模型反直觉能力的基准，该基准测试模型覆盖训练诱导偏差并遵循对抗性指令的能力。Inverse IFEval引入了八种此类挑战，包括问题修正、故意文本缺陷、无注释代码和反事实回答。利用以人为中心的管道，我们在23个领域构建了一个包含1012个高质量中文和英文问题的，并在优化的LLM-as-a-Judge框架下进行评估的数据集。对现有主流LLM的实验证明了我们提出的Inverse IFEval基准的必要性。我们的发现强调，未来的对齐工作不仅应追求流畅性和事实准确性，还应考虑在非常规情况下的适应性。我们希望Inverse IFEval既能作为诊断工具，也能为开发减轻认知惯性、减少对狭窄模式的过度拟合，并最终提高LLM在各种不可预测的现实场景中的指令遵循可靠性的方法奠定基础。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在遵循与它们在监督微调（SFT）期间学到的标准化模式相冲突的指令时，常常表现出的认知惯性问题。

Method: 提出Inverse IFEval基准，该基准通过引入八种类型的对抗性指令挑战（如问题修正、故意文本缺陷、无注释代码、反事实回答等）来衡量模型的反直觉能力。利用以人为中心的管道构建了一个包含1012个高质量中文和英文问题的、跨越23个领域的数据集，并使用优化的LLM-as-a-Judge框架进行评估。

Result: 通过在现有主流LLMs上进行实验，证明了Inverse IFEval基准的必要性，并发现LLMs在面对与训练模式相悖的指令时存在挑战。

Conclusion: 未来的模型对齐工作不仅要关注流畅性和事实准确性，还要考虑模型在非常规情况下的适应性。Inverse IFEval可作为诊断工具，并为开发减轻认知惯性、减少过拟合、提高LLM在现实世界中指令遵循可靠性的方法提供基础。

Abstract: Large Language Models (LLMs) achieve strong performance on diverse tasks but
often exhibit cognitive inertia, struggling to follow instructions that
conflict with the standardized patterns learned during supervised fine-tuning
(SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that
measures models Counter-intuitive Abilitytheir capacity to override
training-induced biases and comply with adversarial instructions. Inverse
IFEval introduces eight types of such challenges, including Question
Correction, Intentional Textual Flaws, Code without Comments, and
Counterfactual Answering. Using a human-in-the-loop pipeline, we construct a
dataset of 1012 high-quality Chinese and English questions across 23 domains,
evaluated under an optimized LLM-as-a-Judge framework. Experiments on existing
leading LLMs demonstrate the necessity of our proposed Inverse IFEval
benchmark. Our findings emphasize that future alignment efforts should not only
pursue fluency and factual correctness but also account for adaptability under
unconventional contexts. We hope that Inverse IFEval serves as both a
diagnostic tool and a foundation for developing methods that mitigate cognitive
inertia, reduce overfitting to narrow patterns, and ultimately enhance the
instruction-following reliability of LLMs in diverse and unpredictable
real-world scenarios.

</details>


### [117] [Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge in Large Language Models](https://arxiv.org/abs/2509.04304)
*Juraj Vladika,Mahdi Dhaini,Florian Matthes*

Main category: cs.CL

TL;DR: LLMs在医疗领域的应用受限于过时的知识，可能提供有害建议。本研究通过构建MedRevQA和MedChangeQA数据集，评估了八种主流LLMs在医疗知识更新方面的表现，发现所有模型均存在过度依赖过时信息的问题。研究还分析了过时预训练数据和训练策略的影响，并提出了未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在医疗领域的应用潜力巨大，但其依赖静态训练数据，当医疗知识更新时，LLM可能提供过时甚至有害的建议，阻碍其在临床推理任务中的应用。

Method: 构建了两个新的问答（QA）数据集：MedRevQA（包含16,501个QA对，涵盖一般生物医学知识）和MedChangeQA（包含512个QA对，专门收集医疗共识随时间发生变化的案例）。使用这两个数据集评估了八种主流LLMs在医疗知识时效性方面的表现，并分析了过时预训练数据和训练策略的影响。

Result: 评估结果显示，所有八种主流LLMs在两个数据集上都表现出对过时医疗知识的依赖性，未能有效跟上医学知识的更新。对过时预训练数据和训练策略的分析也为解释这一现象提供了依据。

Conclusion: LLMs在医疗领域的应用面临严峻的时效性挑战，普遍存在过度依赖过时知识的问题。本研究提出的数据集和分析为开发更及时、可靠的医疗AI系统奠定了基础，指明了未来研究和改进的方向。

Abstract: The growing capabilities of Large Language Models (LLMs) show significant
potential to enhance healthcare by assisting medical researchers and
physicians. However, their reliance on static training data is a major risk
when medical recommendations evolve with new research and developments. When
LLMs memorize outdated medical knowledge, they can provide harmful advice or
fail at clinical reasoning tasks. To investigate this problem, we introduce two
novel question-answering (QA) datasets derived from systematic reviews:
MedRevQA (16,501 QA pairs covering general biomedical knowledge) and
MedChangeQA (a subset of 512 QA pairs where medical consensus has changed over
time). Our evaluation of eight prominent LLMs on the datasets reveals
consistent reliance on outdated knowledge across all models. We additionally
analyze the influence of obsolete pre-training data and training strategies to
explain this phenomenon and propose future directions for mitigation, laying
the groundwork for developing more current and reliable medical AI systems.

</details>


### [118] [PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity Disambiguation](https://arxiv.org/abs/2509.04357)
*Jiajun He,Naoki Sawada,Koichi Miyazaki,Tomoki Toda*

Main category: cs.CL

TL;DR: PARCO通过结合音素感知编码、对比实体消歧、实体级监督和分层实体过滤来提高ASR对领域特定命名实体的识别能力，尤其是在处理同音词方面。


<details>
  <summary>Details</summary>
Motivation: 现有的ASR系统在处理领域特定的命名实体（尤其是同音词）时存在困难。上下文ASR虽然有所改进，但由于实体多样性有限，往往无法捕捉精细的音素变化。此外，先前的方法将实体视为独立的标记，导致多标记偏倚不完整。

Method: 提出PARCO（Phoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation）模型，该模型整合了音素感知编码、对比实体消歧、实体级监督和分层实体过滤，以增强音素辨别能力，确保完整的实体检索，并在不确定情况下减少假阳性。

Result: 在中文AISHELL-1数据集上实现了4.22%的词错误率（CER），在英文DATA2数据集上（1000个干扰项下）实现了11.14%的词错误率（WER），显著优于基线方法。在THCHS-30和LibriSpeech等领域外数据集上也表现出稳健的性能提升。

Conclusion: PARCO模型能够有效解决ASR在处理领域特定命名实体，特别是同音词时的挑战，并在不同数据集上展现出优越的性能和鲁棒性。

Abstract: Automatic speech recognition (ASR) systems struggle with domain-specific
named entities, especially homophones. Contextual ASR improves recognition but
often fails to capture fine-grained phoneme variations due to limited entity
diversity. Moreover, prior methods treat entities as independent tokens,
leading to incomplete multi-token biasing. To address these issues, we propose
Phoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation
(PARCO), which integrates phoneme-aware encoding, contrastive entity
disambiguation, entity-level supervision, and hierarchical entity filtering.
These components enhance phonetic discrimination, ensure complete entity
retrieval, and reduce false positives under uncertainty. Experiments show that
PARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English
DATA2 under 1,000 distractors, significantly outperforming baselines. PARCO
also demonstrates robust gains on out-of-domain datasets like THCHS-30 and
LibriSpeech.

</details>


### [119] [Measuring Bias or Measuring the Task: Understanding the Brittle Nature of LLM Gender Biases](https://arxiv.org/abs/2509.04373)
*Bufan Gao,Elisa Kreiss*

Main category: cs.CL

TL;DR: LLM性别偏见评估易受提示词影响，评估方式会放大偏见。


<details>
  <summary>Details</summary>
Motivation: 现有LLM性别偏见评估方法可能存在问题，需要研究提示词和评估方式对偏见测量的影响。

Method: 通过改变提示词（强调测试背景或性别相关内容）和使用不同评估格式（概率和离散选择）来测试模型，并评估提示词敏感性。

Result: 即使是微小的提示词改动也会显著改变偏见测量结果，有时甚至会反转偏见方向。离散选择指标比概率指标更能放大偏见。

Conclusion: LLM性别偏见评估的稳健性较低，这引发了关于基准测试和模型开发的新问题：受控的测试设计能在多大程度上触发LLM的“测试模式”表现，以及这对其未来基准测试的生态有效性意味着什么。

Abstract: As LLMs are increasingly applied in socially impactful settings, concerns
about gender bias have prompted growing efforts both to measure and mitigate
such bias. These efforts often rely on evaluation tasks that differ from
natural language distributions, as they typically involve carefully constructed
task prompts that overtly or covertly signal the presence of gender
bias-related content. In this paper, we examine how signaling the evaluative
purpose of a task impacts measured gender bias in LLMs. Concretely, we test
models under prompt conditions that (1) make the testing context salient, and
(2) make gender-focused content salient. We then assess prompt sensitivity
across four task formats with both token-probability and discrete-choice
metrics. We find that even minor prompt changes can substantially alter bias
outcomes, sometimes reversing their direction entirely. Discrete-choice metrics
further tend to amplify bias relative to probabilistic measures. These findings
do not only highlight the brittleness of LLM gender bias evaluations but open a
new puzzle for the NLP benchmarking and development community: To what extent
can well-controlled testing designs trigger LLM ``testing mode'' performance,
and what does this mean for the ecological validity of future benchmarks.

</details>


### [120] [Can Language Models Handle a Non-Gregorian Calendar?](https://arxiv.org/abs/2509.04432)
*Mutsumi Sasaki,Go Kamoda,Ryosuke Takahashi,Kosuke Sato,Kentaro Inui,Keisuke Sakaguchi,Benjamin Heinzerling*

Main category: cs.CL

TL;DR: 目前的研究主要集中在公历，但许多非公历系统（如日本、伊斯兰历、希伯来历）也在使用中。本文首次系统地评估了开源语言模型处理日本历法（一种非公历系统）的能力，并创建了四个需要时间知识和时间推理的任务数据集。评估结果表明，虽然一些模型能够进行历法转换，但即使是日本模型也难以进行日本历法算术运算并保持历法间的一致性。这凸显了开发能够更好地理解特定文化历法的语言模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在处理非公历系统方面缺乏评估，特别是日本历法。本文旨在评估和揭示当前语言模型在处理日本历法时的能力和局限性。

Method: 创建了四个需要时间知识和时间推理的任务数据集，并在此基础上评估了一系列以英语和日语为中心的大型语言模型。

Result: 一些模型可以进行历法转换，但即使是日本模型在进行日本历法算术运算和保持历法间一致性方面也存在困难。

Conclusion: 目前的大型语言模型在处理日本历法时存在不足，尤其是在算术和跨历法一致性方面。因此，有必要开发能够更好地理解特定文化历法的语言模型。

Abstract: Temporal reasoning and knowledge are essential capabilities for language
models (LMs). While much prior work has analyzed and improved temporal
reasoning in LMs, most studies have focused solely on the Gregorian calendar.
However, many non-Gregorian systems, such as the Japanese, Hijri, and Hebrew
calendars, are in active use and reflect culturally grounded conceptions of
time. If and how well current LMs can accurately handle such non-Gregorian
calendars has not been evaluated so far. Here, we present a systematic
evaluation of how well open-source LMs handle one such non-Gregorian system:
the Japanese calendar. For our evaluation, we create datasets for four tasks
that require both temporal knowledge and temporal reasoning. Evaluating a range
of English-centric and Japanese-centric LMs, we find that some models can
perform calendar conversions, but even Japanese-centric models struggle with
Japanese-calendar arithmetic and with maintaining consistency across calendars.
Our results highlight the importance of developing LMs that are better equipped
for culture-specific calendar understanding.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [121] [Hypothesis Selection: A High Probability Conundrum](https://arxiv.org/abs/2509.03734)
*Anders Aamand,Maryam Aliakbarpour,Justin Y. Chen,Sandeep Silwal*

Main category: cs.DS

TL;DR: 本文提出了一种更快的假设选择算法，将运行时间从 O(n/δ³ε³) 提高到 O(n/δε²)，并解决了预期距离、已知 OPT 值和预处理假设类别等其他设置下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有假设选择算法在运行时间上存在不足，尤其是在依赖置信度和误差参数方面。

Method: 通过改进算法，将运行时间复杂度从 O(n/δ³ε³) 降低到 O(n/δε²)。此外，研究了在预期距离、已知 OPT 值和允许预处理假设类别这三种不同场景下的假设选择问题。

Result: 在标准设定下，将运行时间复杂度显著降低。在另外三个设定下，分别解决了最优近似因子、特定条件下取得最优 C=3 和亚二次运行时间等问题。

Conclusion: 本文在假设选择问题上取得了重要的理论进展，不仅提高了标准设定下的算法效率，还在其他几个关键场景下解决了悬而未决的问题。

Abstract: In the hypothesis selection problem, we are given a finite set of candidate
distributions (hypotheses), $\mathcal{H} = \{H_1, \ldots, H_n\}$, and samples
from an unknown distribution $P$. Our goal is to find a hypothesis $H_i$ whose
total variation distance to $P$ is comparable to that of the nearest hypothesis
in $\mathcal{H}$. If the minimum distance is $\mathsf{OPT}$, we aim to output
an $H_i$ such that, with probability at least $1-\delta$, its total variation
distance to $P$ is at most $C \cdot \mathsf{OPT} + \varepsilon$.
  Despite decades of work, key aspects of this problem remain unresolved,
including the optimal running time for algorithms that achieve the optimal
sample complexity and best possible approximation factor of $C=3$. The previous
state-of-the-art result [Aliakbarpour, Bun, Smith, NeurIPS 2024] provided a
nearly linear in $n$ time algorithm but with a sub-optimal dependence on the
other parameters, running in $\tilde{O}(n/(\delta^3\varepsilon^3))$ time. We
improve this time complexity to $\tilde{O}(n/(\delta \varepsilon^2))$,
significantly reducing the dependence on the confidence and error parameters.
  Furthermore, we study hypothesis selection in three alternative settings,
resolving or making progress on several open questions from prior works. (1) We
settle the optimal approximation factor when bounding the \textit{expected
distance} of the output hypothesis, rather than its high-probability
performance. (2) Assuming the numerical value of \textit{$\mathsf{OPT}$ is
known} in advance, we present an algorithm obtaining $C=3$ and runtime
$\tilde{O}(n/\varepsilon^2)$ with the optimal sample complexity and succeeding
with high probability in $n$. (3) Allowing polynomial \textit{preprocessing}
step on the hypothesis class $\mathcal{H}$ before observing samples, we present
an algorithm with $C=3$ and subquadratic runtime which succeeds with high
probability in $n$.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [122] [Trustworthy Second-hand Marketplace for Built Environment](https://arxiv.org/abs/2509.04085)
*Stanly Wilson,Kwabena Adu-Duodu,Yinhao Li,Ringo Sham,Yingli Wang,Ellis Solaiman,Charith Perera,Rajiv Ranjan,Omer Rana*

Main category: cs.DC

TL;DR: 该论文提出了一个基于区块链的、用于可持续建筑材料再利用的数字市场，集成了IPFS以实现透明度和可追溯性，旨在提高自动化、可追溯性和去中心化决策，以促进材料的高效再利用。


<details>
  <summary>Details</summary>
Motivation: 建筑行业在材料浪费和可持续实践方面面临严峻挑战，迫切需要创新的解决方案来整合自动化、可追溯性和去中心化决策，以实现高效的材料再利用。

Method: 提出一个基于区块链的数字市场，利用IPFS确保透明度和可追溯性，并通过一个框架来演示其操作流程。

Result: 该数字市场能够促进可再利用材料的高效和可信交换。

Conclusion: 该数字市场代表着迈向更可持续的建筑实践的重要一步。

Abstract: The construction industry faces significant challenges regarding material
waste and sustainable practices, necessitating innovative solutions that
integrate automation, traceability, and decentralised decision-making to enable
efficient material reuse. This paper presents a blockchain-enabled digital
marketplace for sustainable construction material reuse, ensuring transparency
and traceability using InterPlanetary File System (IPFS). The proposed
framework enhances trust and accountability in material exchange, addressing
key challenges in industrial automation and circular supply chains. A framework
has been developed to demonstrate the operational processes of the marketplace,
illustrating its practical application and effectiveness. Our contributions
show how the marketplace can facilitate the efficient and trustworthy exchange
of reusable materials, representing a substantial step towards more sustainable
construction practices.

</details>


### [123] [Combining Performance and Productivity: Accelerating the Network Sensing Graph Challenge with GPUs and Commodity Data Science Software](https://arxiv.org/abs/2509.03653)
*Siddharth Samsi,Dan Campbell,Emanuel Scoullos,Oded Green*

Main category: cs.DC

TL;DR: HPEC图挑战赛的基准测试，特别是网络传感挑战赛，可以通过使用数据科学语言的替代公式，利用NVIDIA的RAPIDS生态系统等现成的ETL工具进行优化，从而在GPU上实现显著的加速。


<details>
  <summary>Details</summary>
Motivation: 传统的HPC基准测试（如LINPACK）无法充分测试HPC系统的硬件和软件组件，因此需要新的基准测试来代表复杂的工作负载。

Method: 提出了一种使用数据科学语言解释GraphBLAS公式的方法，并使用NVIDIA的RAPIDS生态系统（cuDF和cupy）来实现Anonymized Network Sensing Graph Challenge。

Result: 使用现成的软件（RAPIDS cuDF和cupy），在NVIDIA A100 GPU上实现了147倍至509倍的加速，在NVIDIA H100 GPU上实现了243倍至1269倍的加速，在NVIDIA H200 GPU上实现了332倍至2185倍的加速，与在CPU上使用Pandas运行的相同代码相比。

Conclusion: 可以通过利用数据科学语言和现成的ETL工具（如NVIDIA RAPIDS）来实现HPEC图挑战赛的显著加速，而无需专门的HPC代码。

Abstract: The HPEC Graph Challenge is a collection of benchmarks representing complex
workloads that test the hardware and software components of HPC systems, which
traditional benchmarks, such as LINPACK, do not. The first benchmark, Subgraph
Isomorphism, focused on several compute-bound and memory-bound kernels. The
most recent of the challenges, the Anonymized Network Sensing Graph Challenge,
represents a shift in direction, as it represents a longer end-to-end workload
that requires many more software components, including, but not limited to,
data I/O, data structures for representing graph data, and a wide range of
functions for data preparation and network analysis. A notable feature of this
new graph challenge is the use of GraphBLAS to represent the computational
aspects of the problem statement. In this paper, we show an alternative
interpretation of the GraphBLAS formulations using the language of data
science. With this formulation, we show that the new graph challenge can be
implemented using off-the-shelf ETL tools available in open-source, enterprise
software such as NVIDIA's RAPIDS ecosystem. Using off-the-shelf software,
RAPIDS cuDF and cupy, we enable significant software acceleration without
requiring any specific HPC code and show speedups, over the same code running
with Pandas on the CPU, of 147x-509x on an NVIDIA A100 GPU, 243x-1269X for an
NVIDIA H100 GPU, and 332X-2185X for an NVIDIA H200 GPU.

</details>


### [124] [Distributed Download from an External Data Source in Asynchronous Faulty Settings](https://arxiv.org/abs/2509.03755)
*John Augustine,Soumyottam Chatterjee,Valerie King,Manish Kumar,Shachar Meir,David Peleg*

Main category: cs.DC

TL;DR: 该论文研究了在异步通信网络中，分布式数据检索（DR）模型下的数据下载问题，旨在最小化查询复杂性并最大化容错率。


<details>
  <summary>Details</summary>
Motivation: 在异步通信网络中研究分布式数据检索模型，以最小化查询复杂性并最大化容错率。

Method: 提出了一种容忍任意固定比例（β<1）拜占庭故障的查询最优确定性解决方案，并在拜占庭故障模型下，通过随机化协议在β<1/2时实现近乎最优的查询复杂性。

Result: 在异步模型下，为容忍固定比例拜占庭故障的下载问题提供了查询最优的确定性解决方案；在拜占庭故障模型下，将确定性协议的查询复杂度下界扩展到了随机化协议，并证明了在β<1/2时存在查询近乎最优的随机化协议。

Conclusion: 该研究首次在异步通信网络中解决了分布式数据检索模型下的下载问题，并在容错和查询复杂性方面取得了重要进展。

Abstract: The distributedData Retrieval (DR) model consists of $k$ peers connected by a
complete peer-to-peer communication network, and a trusted external data source
that stores an array $\textbf{X}$ of $n$ bits ($n \gg k$). Up to $\beta k$ of
the peers might fail in any execution (for $\beta \in [0, 1)$). Peers can
obtain the information either by inexpensive messages passed among themselves
or through expensive queries to the source array $\textbf{X}$. In the DR model,
we focus on designing protocols that minimize the number of queries performed
by any nonfaulty peer (a measure referred to as query complexity) while
maximizing the resilience parameter $\beta$.
  The Download problem requires each nonfaulty peer to correctly learn the
entire array $\textbf{X}$. Earlier work on this problem focused on synchronous
communication networks and established several deterministic and randomized
upper and lower bounds. Our work is the first to extend the study of
distributed data retrieval to asynchronous communication networks. We address
the Download problem under both the Byzantine and crash failure models. We
present query-optimal deterministic solutions in an asynchronous model that can
tolerate any fixed fraction $\beta<1$ of crash faults. In the Byzantine failure
model, it is known that deterministic protocols incur a query complexity of
$\Omega(n)$ per peer, even under synchrony. We extend this lower bound to
randomized protocols in the asynchronous model for $\beta \geq 1/2$, and
further show that for $\beta < 1/2$, a randomized protocol exists with
near-optimal query complexity. To the best of our knowledge, this is the first
work to address the Download problem in asynchronous communication networks.

</details>


### [125] [Gathering of asynchronous robots on circle with limited visibility using finite communication](https://arxiv.org/abs/2509.04004)
*Avisek Sharma,Satakshi Ghosh,Buddhadeb Sau*

Main category: cs.DC

TL;DR: 在该工作中，研究了在有限可见性下，一组自主、匿名、同质的机器人如何在连续圆上进行集合。机器人在初始时位于不同的位置，形成一个非旋转对称的配置。机器人需要就顺时针方向达成一致。在θ-可见性模型下，机器人只能看到圆上与其角距离小于θ的机器人。此前有研究表明，在π/2可见性下集合是不可能的，而在π可见性下，通过特定的算法和调度器可以实现集合。本工作提出了一种新的算法，在π-可见性模型下，对于具有有限通信能力（FCOM）且运动非刚性的机器人，在完全异步调度器下解决了集合问题。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决在π-可见性模型下，具有有限通信能力（FCOM）且运动非刚性的机器人进行集合的问题，并提出一种在完全异步调度器下可行的算法。

Method: 提出了一种新的集合算法，该算法适用于π-可见性模型，机器人具有有限通信能力（FCOM），并且运动是非刚性的。算法在完全异步调度器下运行。

Result: 提出的算法能够解决在π-可见性模型下，具有有限通信能力（FCOM）且运动非刚性的机器人在完全异步调度器下的集合问题。

Conclusion: 本工作成功地在π-可见性模型下，为具有有限通信能力（FCOM）且运动非刚性的机器人设计了一种在完全异步调度器下可行的集合算法，解决了此前研究中存在的局限性。

Abstract: This work addresses the gathering problem for a set of autonomous, anonymous,
and homogeneous robots with limited visibility operating in a continuous
circle. The robots are initially placed at distinct positions, forming a
rotationally asymmetric configuration. The robots agree on the clockwise
direction. In the $\theta$-visibility model, a robot can only see those robots
on the circle that are at an angular distance $<\theta$ from it. Di Luna
\textit{et. al.} [DISC'20] have shown that, in $\pi/2$ visibility, gathering is
impossible. In addition, they provided an algorithm for robots with $\pi$
visibility, operating under a semi-synchronous scheduler. In the $\pi$
visibility model, only one point, the point at the angular distance $\pi$ is
removed from the visibility. Ghosh \textit{et. al.} [SSS'23] provided a
gathering algorithm for $\pi$ visibility model with robot having finite memory
($\mathcal{FSTA}$), operating under a special asynchronous scheduler.
  If the robots can see all points on the circle, then the gathering can be
done by electing a leader in the weakest robot model under a fully asynchronous
scheduler. However, previous works have shown that even the removal of one
point from the visibility makes gathering difficult. In both works, the robots
had rigid movement. In this work, we propose an algorithm that solves the
gathering problem under the $\pi$-visibility model for robots that have finite
communication ability ($\mathcal{FCOM}$). In this work the robot movement is
non-rigid and the robots work under a fully asynchronous scheduler.

</details>


### [126] [Counterfactual simulations for large scale systems with burnout variables](https://arxiv.org/abs/2509.04038)
*Benjamin Heymann*

Main category: cs.DC

TL;DR: 新算法基于“不确定性放松”，可实现带‘燃尽’变量的大型系统反事实估计的并行计算。


<details>
  <summary>Details</summary>
Motivation: 现有的大型系统（如在线广告）在进行‘燃尽’变量（会起作用并不可逆地停用）影响下的假设情景模拟时，计算成本高昂，因为替代轨迹需要顺序处理，难以扩展。用户希望对在线广告活动进行反事实分析，但预算等因素使分析复杂化。

Method: 提出一种基于“不确定性放松”的新算法类型，该算法可实现高效的并行计算。

Result: 该算法显著提高了带‘燃尽’变量的系统进行反事实估计的可扩展性。

Conclusion: 所提出的基于“不确定性放松”的算法能够有效处理带‘燃尽’变量的大型系统中的反事实估计问题，显著提高了计算效率和可扩展性。

Abstract: We consider large-scale systems influenced by burnout variables - state
variables that start active, shape dynamics, and irreversibly deactivate once
certain conditions are met. Simulating what-if scenarios in such systems is
computationally demanding, as alternative trajectories often require sequential
processing, which does not scale very well. This challenge arises in settings
like online advertising, because of campaigns budgets, complicating
counterfactual analysis despite rich data availability. We introduce a new type
of algorithms based on what we refer to as uncertainty relaxation, that enables
efficient parallel computation, significantly improving scalability for
counterfactual estimation in systems with burnout variables.

</details>


### [127] [LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for High-Performance Distributed Training Systems](https://arxiv.org/abs/2509.04084)
*Chenxuan Yao,Yuchong Hu,Feifan Liu,Zhengyu Liu,Dan Feng*

Main category: cs.DC

TL;DR: LowDiff 框架通过重用压缩梯度作为差分检查点、批量梯度写入优化、动态调整检查点频率和批量大小，并结合层级梯度重用和基于 CPU 的异步持久化策略，实现了高达每轮迭代一次的检查点频率，运行时开销小于 3.1%。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式深度学习模型训练中的检查点机制虽然能从故障中快速恢复，但过于频繁的检查点会产生高昂的成本并影响训练性能。虽然差分检查点技术可以降低成本，但其应用仅限于推荐系统，并未在通用分布式训练系统中得到探索。

Method: LowDiff 框架通过以下方式实现高效的频繁检查点：1. 重用压缩梯度作为差分检查点以降低成本。2. 采用批量梯度写入优化以高效地将差分写入存储。3. 动态调整检查点频率和批量大小以最大化性能。4. 结合层级梯度重用和快照方法，以及基于 CPU 的异步持久化策略，在无需梯度压缩的情况下实现频繁检查点。

Result: 实验表明，LowDiff 框架在多种工作负载下，可以将检查点频率提高到每轮迭代一次，同时运行时开销低于 3.1%。

Conclusion: LowDiff 框架能够以很低的开销实现非常频繁的检查点，有效解决了分布式深度学习模型训练中频繁检查点带来的成本问题。

Abstract: Distributed training of large deep-learning models often leads to failures,
so checkpointing is commonly employed for recovery. State-of-the-art studies
focus on frequent checkpointing for fast recovery from failures. However, it
generates numerous checkpoints, incurring substantial costs and thus degrading
training performance. Recently, differential checkpointing has been proposed to
reduce costs, but it is limited to recommendation systems, so its application
to general distributed training systems remains unexplored.
  This paper proposes LowDiff, an efficient frequent checkpointing framework
that \textit{reuses} compressed gradients, serving as differential checkpoints
to reduce cost. Furthermore, LowDiff incorporates a batched gradient write
optimization to persist these differentials to storage efficiently. It also
dynamically tunes both the checkpoint frequency and the batching size to
maximize performance. We further enhance LowDiff with a layer-wise gradient
reusing and snapshotting approach and a CPU-based asynchronous persistence
strategy, enabling frequent checkpointing without gradient compression.
Experiments on various workloads show that LowDiff can achieve checkpointing
frequency up to per iteration with less than 3.1\% runtime overhead.

</details>


### [128] [On the impact of unlimited computational power in OBLOT: consequences for synchronous robots on graphs](https://arxiv.org/abs/2509.04383)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The OBLOT model has been extensively studied in theoretical swarm robotics.
It assumes weak capabilities for the involved mobile robots, such as they are
anonymous, disoriented, no memory of past events (oblivious), and silent. Their
only means of (implicit) communication is transferred to their positioning,
i.e., stigmergic information. These limited capabilities make the design of
distributed algorithms a challenging task. Over the last two decades, numerous
research papers have addressed the question of which tasks can be accomplished
within this model. Nevertheless, as it usually happens in distributed
computing, also in OBLOT the computational power available to the robots is
neglected as the main cost measures for the designed algorithms refer to the
number of movements or the number of rounds required. In this paper, we prove
that for synchronous robots moving on finite graphs, the unlimited
computational power (other than finite time) has a significant impact. In fact,
by exploiting it, we provide a definitive resolution algorithm that applies to
a wide class of problems while guaranteeing the minimum number of moves and
rounds.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [129] [Combining feature-based approaches with graph neural networks and symbolic regression for synergistic performance and interpretability](https://arxiv.org/abs/2509.03547)
*Rogério Almeida Gouvêa,Pierre-Paul De Breuck,Tatiane Pretto,Gian-Marco Rignanese,Marcos José Leite dos Santos*

Main category: cond-mat.mtrl-sci

TL;DR: MatterVial是一个集成多种预训练图神经网络（GNN）模型（包括基于结构、基于组成和等变GNN）的潜在表示、近似描述符和符号回归特征的混合框架，用于材料科学中的特征工程，提高了MODNet在Matbench任务上的性能，并提供了解释性模块。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过整合不同类型的预训练图神经网络（GNN）模型的潜在表示、计算效率高的GNN近似描述符以及符号回归的新特征，来扩展特征空间，从而改进材料科学中的特征工程，结合传统特征工程的化学可解释性和深度学习的预测能力。

Method: MatterVial框架通过整合基于结构（MEGNet）、基于组成（ROOST）和等变（ORB）的GNN模型的潜在表示，以及计算效率高的GNN近似描述符和符号回归的新特征，来扩展特征空间。该方法结合了传统特征工程的化学可解释性与深度学习的预测能力。此外，还包含一个利用代理模型和符号回归的解释性模块，将GNN提取的描述符解码为明确的、具有物理意义的公式。

Result: 将MatterVial框架应用于Matbench任务，并对其进行特征工程，可以显著降低MODNet模型的误差，使其性能在多个任务上具有竞争力，甚至优于最先进的端到端GNN模型，准确率提升超过40%。解释性模块能够将GNN提取的描述符解码为具有物理意义的公式。

Conclusion: MatterVial是一个高性能、透明且符合可解释人工智能（AI）原则的统一框架，它通过提供增强的特征工程能力和可解释性，推动了材料信息学的发展，为更具针对性和自主性的材料发现铺平了道路。

Abstract: This study introduces MatterVial, an innovative hybrid framework for
feature-based machine learning in materials science. MatterVial expands the
feature space by integrating latent representations from a diverse suite of
pretrained graph neural network (GNN) models including: structure-based
(MEGNet), composition-based (ROOST), and equivariant (ORB) graph networks, with
computationally efficient, GNN-approximated descriptors and novel features from
symbolic regression. Our approach combines the chemical transparency of
traditional feature-based models with the predictive power of deep learning
architectures. When augmenting the feature-based model MODNet on Matbench
tasks, this method yields significant error reductions and elevates its
performance to be competitive with, and in several cases superior to,
state-of-the-art end-to-end GNNs, with accuracy increases exceeding 40% for
multiple tasks. An integrated interpretability module, employing surrogate
models and symbolic regression, decodes the latent GNN-derived descriptors into
explicit, physically meaningful formulas. This unified framework advances
materials informatics by providing a high-performance, transparent tool that
aligns with the principles of explainable AI, paving the way for more targeted
and autonomous materials discovery.

</details>


### [130] [Hydrogen storage in nanocrystalline high entropy material](https://arxiv.org/abs/2509.03557)
*Yogesh Kumar Yadav,Mohammad Abu Shaz,Thakur Prasad Yadav*

Main category: cond-mat.mtrl-sci

TL;DR: 高熵合金Al-Cu-Fe-Ni-Cr在机械合金化法制备后，表现出优异的储氢性能，包括快速吸放氢和良好的循环稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究一种新型高熵合金（Al-Cu-Fe-Ni-Cr）的储氢性能。

Method: 采用机械合金化法（球磨）制备单相纳米晶Al-Cu-Fe-Ni-Cr高熵合金，并在300°C、50 atm H2压力下测试其储氢能力和循环稳定性。

Result: 制备得到纳米晶（BCC相，晶格常数0.289 nm）Al-Cu-Fe-Ni-Cr高熵合金。该合金在300°C、50 atm H2压力下，3分钟内吸收2.1 wt.%的氢，6分钟内解吸1.6 wt.%的氢。经过25次循环后，储氢容量仅损失0.2 wt.%。

Conclusion: 所制备的纳米晶Al-Cu-Fe-Ni-Cr高熵合金具有优异的循环稳定性以及快速的储氢和释氢动力学特性，是储氢应用的一个有潜力的选择。

Abstract: In this study, a single-phase nanocrystalline Al-Cu-Fe-Ni-Cr high-entropy
alloy (HEA) has been synthesized by mechanical alloying and comprehensively
investigated for hydrogen storage responses evaluated in details. High-energy
attritor ball mill was used to synthesize the alloy from elemental powder, and
hexane medium was used as a process control agent. As synthesized materials was
nanocrystalline in nature after 40 h of milling with a lattice parameter of
0.289 nm body-centered cubic (BCC) phase. As synthesized nanocrystalline
Al-Cu-Fe-Ni-Cr HEA demonstrated remarkable hydrogen storage properties,
absorbing 2.1 wt.% of hydrogen in 3 minutes at 300{\deg}C with 50 atm of
hydrogen pressure. At the same temperature, it also desorbed about 1.6 wt.% of
hydrogen in 6 minutes. These quick rates of absorption and desorption
demonstrate how well the alloy absorbs and releases hydrogen. Additionally, the
alloy showed outstanding cyclic stability, retaining almost all of its hydrogen
capacity across 25 cycles with only a slight 0.2 wt.% loss. The nanocrystalline
Al-Cu-Fe-Ni-Cr HEA is a potential option for hydrogen storage applications due
to its outstanding cycle stability and fast kinetics of hydrogen storage and
release.

</details>


### [131] [Surface Passivation for Halide Optoelectronics: Comparing Optimization and Reactivity of Amino-Silanes with Formamidinium](https://arxiv.org/abs/2509.03713)
*Zixu Huang,Farhad Akrami,Junxiang Zhang,Stephen Barlow,Seth R. Marder,David S. Ginger*

Main category: cond-mat.mtrl-sci

TL;DR: Amino-silane表面钝化方案在卤素钙钛光电子学中受到关注。本文比较了APTMS和AEAPTMS两种分子，通过室温真空沉积处理钙钛。两种分子均能改善薄膜光致发光性质和光伏器件性能，但效果取决于沉积时间。AEAPTMS具有更宽、更稳健的处理窗口，并在优化条件下产生更高的性能。过度暴露，尤其是APTMS，会降低性能，并显著缩短光致发光寿命和吸收。通过核磁共振波谱和深度分辨飞行时间二次离子质谱，研究了两种氨基硅烷在溶液和固态下与甲脒阳离子反应的化学性质。该研究强调了优化沉积条件以平衡有效钝化与潜在性能损失的重要性，并阐明了氨基硅烷钝化剂与卤素钙钛之间先前未被认识到的反应化学。


<details>
  <summary>Details</summary>
Motivation: 比较APTMS和AEAPTMS两种氨基硅烷在卤素钙钛光电子学中的表面钝化效果，并探究其背后的化学机理。

Method: 通过室温真空沉积技术，比较APTMS和AEAPTMS在FA0.78Cs0.22Pb(I0.85Br0.15)3钙钛薄膜上的表面处理效果。利用核磁共振波谱和深度分辨飞行时间二次离子质谱研究了氨基硅烷与钙钛之间的化学反应。

Result: 两种氨基硅烷均能改善钙钛薄膜的光致发光性质和光伏器件性能，但效果受沉积时间影响。AEAPTMS的处理窗口更宽、性能更高。过度暴露（尤其APTMS）会降低性能，缩短光致发光寿命和吸收。研究证实了氨基硅烷在溶液和固态下均能与甲脒阳离子反应。

Conclusion: 优化氨基硅烷表面钝化层的沉积条件对于平衡钝化效果和避免性能损失至关重要。氨基硅烷钝化剂与卤素钙钛之间存在未被充分认识的反应化学。

Abstract: Amino-silane-based surface passivation schemes are gaining attention in
halide perovskite optoelectronics, with varying levels of success. We compare
surface treatments using (3-aminopropyl)trimethoxysilane (APTMS) and
[3-(2-aminoethylamino)propyl]trimethoxysilane (AEAPTMS), applied via
room-temperature vacuum deposition, to the perovskite
FA0.78Cs0.22Pb(I0.85Br0.15)3 (FA = formamidinium). Both molecules improve
thin-film photoluminescence properties and photovoltaic device performance,
although their effectiveness depends strongly on deposition time. We show
AEAPTMS has a wider, more robust processing window and yields higher
performance under optimized conditions. In contrast, over-exposure,
particularly with APTMS, reduces performance, with notable reductions in
photoluminescence lifetime and absorbance. To probe the underlying chemistry,
we employ nuclear magnetic resonance (NMR) spectroscopy and depth-resolved
time-of-flight secondary ion mass spectrometry (ToF-SIMS), demonstrating that
both amino-silanes react with formamidinium (FA+) cations in solution and in
the solid state. This work underscores the importance of optimizing deposition
conditions to balance effective passivation with potential performance loss and
elucidates previously unrecognized reactive chemistry between amino-silane
passivating agents and halide perovskites.

</details>


### [132] [Link Statistics of Dislocation Network during Strain Hardening](https://arxiv.org/abs/2509.03743)
*Sh. Akhondzadeh,Hanfeng Zhai,Wurong Jian,Ryan B. Sills,Nicolas Bertin,Wei Cai*

Main category: cond-mat.mtrl-sci

TL;DR: 文章研究了晶体中位错网络在应变硬化过程中的连接线段长度分布及其统计规律，并提出了基于泊松过程的解释模型。


<details>
  <summary>Details</summary>
Motivation: 研究位错连接线段长度的统计分布，以理解应变硬化过程中位错微观结构的演变及其背后的物理机制。

Method: 利用离散位错动力学（DDD）模拟，分析了面心立方（fcc）铜在应变硬化过程中，不同滑移系统上连接线段长度的统计分布，并尝试通过扩展一维泊松过程来解释这些分布。

Result: 发现在活性滑移系统上，连接线段长度遵循双指数分布，而在非活性滑移系统上，则遵循单指数分布。文章还指出，双指数分布的长尾现象与应力诱导的长线段的“弓出”现象有关，且可以通过引入超线性增长函数的一维泊松过程模型来解释。

Conclusion: 该研究加深了对塑性变形过程中位错微观结构演变的理解，并揭示了控制其形成的物理机制，特别是应力对位错网络结构的影响。

Abstract: Dislocations are line defects in crystals that multiply and self-organize
into a complex network during strain hardening. The length of dislocation
links, connecting neighboring nodes within this network, contains crucial
information about the evolving dislocation microstructure. By analyzing data
from Discrete Dislocation Dynamics (DDD) simulations in face-centered cubic
(fcc) Cu, we characterize the statistical distribution of link lengths of
dislocation networks during strain hardening on individual slip systems. Our
analysis reveals that link lengths on active slip systems follow a
double-exponential distribution, while those on inactive slip systems conform
to a single-exponential distribution. The distinctive long tail observed in the
double-exponential distribution is attributed to the stress-induced bowing out
of long links on active slip systems, a feature that disappears upon removal of
the applied stress. We further demonstrate that both observed link length
distributions can be explained by extending a one-dimensional Poisson process
to include different growth functions. Specifically, the double-exponential
distribution emerges when the growth rate for links exceeding a critical length
becomes super-linear, which aligns with the physical phenomenon of long links
bowing out under stress. This work advances our understanding of dislocation
microstructure evolution during strain hardening and elucidates the underlying
physical mechanisms governing its formation.

</details>


### [133] [Physically Interpretable Descriptors Drive the Materials Design of Metal Hydrides for Hydrogen Storage](https://arxiv.org/abs/2509.04039)
*Seong-Hoon Jang,Di Zhang,Hung Ba Tran,Xue Jia,Kiyoe Konno,Ryuhei Sato,Shin-ichi Orimo,and Hao Li*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究提出了可解释的物理模型，用于预测金属氢化物的储氢密度和室温平衡压力，基于少量化学描述符，并揭示了性能指标间的权衡关系，同时发现了具有潜力的铍基储氢材料。


<details>
  <summary>Details</summary>
Motivation: 设计用于储氢的金属氢化物因其庞大的成分空间和复杂的结构-性质关系而面临长期挑战。

Method: 利用包含5089种金属氢化物组成的、从现有文献中挖掘的数字储氢平台（DigHyd）数据集，构建了超过160万个候选模型。通过组合标量变换和非线性链接函数，最终确定了每个仅包含2-3个描述符的闭式模型，其预测精度可与最先进的机器学习方法相媲美，同时保持了完全的物理可解释性。

Result: 基于描述符的设计图揭示了储氢密度（w）和室温平衡压力（Peq,RT）之间存在根本性的权衡：富含碱土金属的盐型氢化物具有高w但低Peq,RT，而基于过渡金属的间隙型氢化物则表现出相反的趋势。其中，铍基体系（如Be-Na合金）是少数几个同时满足两个性能指标的候选材料，这归因于铍独特的轻质和高摩尔密度特性。此外，模型预测铍基体系在接近这些基准方面可能提供新的前景。

Conclusion: 本研究提出的模型为材料设计提供了化学直观的指导，并为复杂化学空间中材料的合理发现建立了可扩展的框架。

Abstract: Designing metal hydrides for hydrogen storage remains a longstanding
challenge due to the vast compositional space and complex structure-property
relationships. Herein, for the first time, we present physically interpretable
models for predicting two key performance metrics, gravimetric hydrogen density
$w$ and equilibrium pressure $P_{\rm eq,RT}$ at room temperature, based on a
minimal set of chemically meaningful descriptors. Using a rigorously curated
dataset of $5,089$ metal hydride compositions from our recently developed
Digital Hydrogen Platform (\it{DigHyd}) based on large-scale data mining from
available experimental literature of solid-state hydrogen storage materials, we
systematically constructed over $1.6$ million candidate models using
combinations of scalar transformations and nonlinear link functions. The final
closed-form models, derived from $2$-$3$ descriptors each, achieve predictive
accuracies on par with state-of-the-art machine learning methods, while
maintaining full physical transparency. Strikingly, descriptor-based design
maps generated from these models reveal a fundamental trade-off between $w$ and
$P_{\rm eq,RT}$: saline-type hydrides, composed of light electropositive
elements, offer high $w$ but low $P_{\rm eq,RT}$, whereas interstitial-type
hydrides based on heavier electronegative transition metals show the opposite
trend. Notably, Be-based systems, such as Be-Na alloys, emerge as rare
candidates that simultaneously satisfy both performance metrics, attributed to
the unique combination of light mass and high molar density for Be. Our models
indicate that Be-based systems may offer renewed prospects for approaching
these benchmarks. These results provide chemically intuitive guidelines for
materials design and establish a scalable framework for the rational discovery
of materials in complex chemical spaces.

</details>


### [134] [Thickness-Induced Topological Phase Transition Investigated by Helicity Dependent Photocurrent in $α$-Sn/CdTe(110)](https://arxiv.org/abs/2509.04042)
*Tengfei Liu,Xiyu Hong,Zhe Li,Shenzhong Chen,Leyi Li,Xin-Yi Tang,Shuying Cheng,Yunfeng Lai,Yonghai Chen,Zhu Diao,Ke He,Qi-kun Xue,Jinling Yu*

Main category: cond-mat.mtrl-sci

TL;DR: $\\alpha$-Sn薄膜厚度可调控其拓扑相变，螺旋度相关光电流（HDPC）可作为探测工具。


<details>
  <summary>Details</summary>
Motivation: $\\alpha$-Sn具有丰富的拓扑相图，但缺乏有效调控和区分这些相位的实验方法。

Method: 通过分子束外延生长不同厚度的$\\alpha$-Sn薄膜，并利用螺旋度相关光电流（HDPC）、高分辨率透射电子显微镜（HR-TEM）、点群对称性分析和第一性原理计算进行研究。

Result: 5 nm $\\alpha$-Sn薄膜的HDPC呈现奇函数依赖关系，而10 nm和30 nm薄膜则呈现偶函数依赖关系，揭示了5至10 nm之间存在从二维（2D）到三维（3D）拓扑绝缘体的厚度驱动的拓扑相变。

Conclusion: HDPC是探测拓扑相变的有效工具，$\\alpha$-Sn(110)薄膜的电子性质可调，可用于探索拓扑现象和开发自旋器件。

Abstract: $\alpha$-Sn exhibits a rich topological phase diagram, yet experimental
methods to tune and distinguish these phases remain limited. Here, we
investigated the helicity-dependent photocurrent (HDPC) in $\alpha$-Sn films of
varying thickness grown on CdTe(110) by molecular beam epitaxy. The HDPC of the
5 nm $\alpha$-Sn film shows an odd-function dependence on incident angle,
whereas that of the 10 and 30 nm films exhibit an even-function dependence.
Combined with high-resolution transmission electron microscopy (HR-TEM),
point-group symmetry analysis, and first-principles calculations, it is
revealed that a thickness-driven topological phase transition from a two
dimensional (2D) to a three dimensional (3D) topological insulator occurs
between 5 and 10 nm. These results demonstrate that HDPC serves as a sensitive
diagnostic tool for topological phase transitions. The tunable electronic
properties of $\alpha$-Sn(110) films enable thickness- and strain-mediated
control of topological states, establishing a versatile platform for exploring
emerging topological phenomena and developing spin-based devices.

</details>


### [135] [Grain boundary energy models and boundary splitting](https://arxiv.org/abs/2509.04109)
*Adam Morawiec*

Main category: cond-mat.mtrl-sci

TL;DR: 本篇论文提出了评估晶界能量模型是否允许晶界解离的理论框架，并分析了阻止晶界解离的条件及其对多晶材料模拟性能的影响。


<details>
  <summary>Details</summary>
Motivation: 为了预测多晶材料的行为，需要建立晶界能量模型。然而，现有的模型可能允许晶界解离，从而影响模拟的准确性，因此需要评估模型是否允许晶界解离。

Method: 通过引入与晶界润湿条件相反的不等式来约束能量模型，以阻止晶界解离。推导了匹配几何构型三晶界参数之间的关系，并分析了这些不等式的含义。

Result: 给出了一个允许晶界分解的能量模型示例，并说明了如何判断给定的能量模型是否允许晶界解离以及哪些晶界会受到影响。

Conclusion: 了解能量模型是否允许晶界解离以及哪些晶界会受到影响，对于评估其在多晶模拟中的性能至关重要。

Abstract: Models of grain boundary energy are essential for predicting the behavior of
polycrystalline materials. Typical models represent the minimum boundary energy
as a function of macroscopic boundary parameters. An energy model may allow for
boundary dissociation, i.e., for a further reduction of the overall energy by
splitting a boundary into two boundaries parallel to the original one. Such
splitting is prevented by constraining the energy model with inequalities
opposite to the boundary wetting condition. The inequalities are applicable
only to triplets of boundaries that match the assumed geometric configuration.
Relationships connecting the parameters of such boundaries are derived,
implications of the inequalities that prevent boundary splitting are
considered, and an example energy model is shown to allow boundary
decomposition. Knowing whether a given energy model permits boundary
dissociation and which boundaries can be affected is important for evaluating
its performance in polycrystal simulations.

</details>


### [136] [Local structural disorder in crystalline materials](https://arxiv.org/abs/2509.04171)
*Marios Zacharias,Jacky Even*

Main category: cond-mat.mtrl-sci

TL;DR: 局部无序性在软、非谐材料中深刻影响其电子、振动、光学和输运性质，并非总是导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 强调局部无序性在塑造软、非谐材料性质中的关键作用，并指出其对电子结构和声子动力学的影响，而非仅仅是性能退化的原因。

Method: 使用多晶形和非谐框架对局部无序性进行建模，并解释这些方法如何契合实验观察和预测新趋势。

Result: 局部无序性打破了声子准粒子图像，并调节了电子-声子和声子-声子相互作用，尤其是在软、非谐相中，对电和热输运产生显著影响。

Conclusion: 提出将这些见解整合到能源材料的预测模型中，并建议结合先进的第一性原理方法与机器学习。

Abstract: Local positional disorder in soft, anharmonic materials has emerged as a
central factor in shaping their electronic, vibrational, optical, and transport
properties. Viewed mainly as a source of performance degradation, recent
theoretical insights reveal that local disorder profoundly influences the
electronic structure and phonon dynamics, without inducing deep electronic
traps or non-radiative recombination pathways. In this work, we highlight
advances in modeling local disorder using polymorphous and anharmonic
frameworks, showing how these methods explain experimental observations and
predict new trends. We emphasize the role of disorder in the breakdown of the
phonon quasiparticle picture and in modulating electron-phonon and
phonon-phonon interactions, particularly in soft, anharmonic phases of matter,
with significant effects on electrical and thermal transport. We outline
opportunities for integrating these insights into predictive modeling for
energy materials and propose combining advanced first-principles methods with
machine learning.

</details>


### [137] [Morphology Formation Pathways in Solution-Processed Perovskite Thin Films](https://arxiv.org/abs/2509.04175)
*M. Majewski,O. J. J. Ronsin,J. Harting*

Main category: cond-mat.mtrl-sci

TL;DR: 本论文提出并验证了一个几何模型，用于解释溶液加工过程中溶剂蒸发和晶体生长之间的相互作用如何影响钙钛矿太阳能电池薄膜的最终形态。该模型识别了导致四种不同薄膜形态的十一种形成途径，并为如何通过调整工艺参数来控制薄膜形态提供了指导，尤其是在实现无针孔和平整的薄膜方面。


<details>
  <summary>Details</summary>
Motivation: 目前对溶液法制备钙钛矿太阳能电池活性层薄膜的形成机制尚不完全清楚，特别是溶剂蒸发和结晶同时进行时薄膜形态的演变过程。本文旨在通过建立几何模型来深入理解这一过程，为优化薄膜制备提供理论依据。

Method: 开发了一个几何模型，研究溶剂蒸发和晶体生长之间的相互作用对干膜形态的影响。通过改变加工条件，探索不同的薄膜形成机制，并识别出十一种形成途径和四种不同的薄膜形态。

Result: 研究发现，存在十一种薄膜形成途径，并最终形成四种不同的薄膜形态。模型表明，当蒸发速率远大于晶体生长速率时，可以形成无针孔、平整的薄膜。或者，在较低的干燥速率下，通过在基板上提供高密度的晶体核，也可以获得理想的薄膜形态。

Conclusion: 该几何模型能够解释溶液加工过程中溶剂蒸发和晶体生长对薄膜形态的影响，并提出了利用工艺参数调控薄膜形态的方法，为钙钛矿太阳能电池等领域中薄膜的制备提供了指导。该模型的普适性使其能够应用于其他蒸发和结晶同时进行的薄膜体系。

Abstract: The active layer in a perovskite solar cell is usually composed of a
polycrystalline thin film. Fabrication of this layer by solution processing is
a promising candidate for up-scaling to the mass market. However, the evolution
of an evaporating and simultaneously crystallizing thin film is not yet fully
understood. To contribute to the understanding of the formation of thin films,
we develop a geometrical model that deals with the effect of the interplay
between solvent evaporation and crystal growth on the dry film morphology. The
possible film formation mechanisms are investigated, depending on the
processing conditions. We find eleven formation pathways leading to four
distinct morphologies. It is shown how these formation pathways can be utilized
by adapting the process parameters to the material properties. Pinhole-free and
flat films can be fabricated if the evaporation rate is high in comparison to
the crystal growth rate. Alternatively, providing a high crystal number density
on the substrate can lead to the desired film morphology at low drying rates.
The generality of the model makes it applicable to any evaporating and
simultaneously crystallizing thin film.

</details>


### [138] [Nature of magnetic exchange interactions in kagome antiferromagnets FeGe and FeSn](https://arxiv.org/abs/2509.04228)
*Yitao Zheng,Yan Zhu,Jun Hu*

Main category: cond-mat.mtrl-sci

TL;DR: kagome反铁磁体FeGe和FeSn中的磁交换相互作用（MEI）主要由层间耦合引起，但层内Fe原子因直接MEI和RKKY相互作用的竞争而呈现铁磁耦合。


<details>
  <summary>Details</summary>
Motivation: 研究kagome反铁磁体FeGe和FeSn中的磁交换相互作用（MEI），解释其丰富的量子态起源，并探究其与电子、自旋、轨道和晶格自由度的关系。

Method: 通过第一性原理计算系统地研究FeGe和FeSn中的MEI。

Result: FeGe和FeSn的Néel温度受层间耦合和层内直接MEI与RKKY相互作用的竞争影响。FeGe的直接MEI更强，RKKY相互作用更弱，因此Néel温度更高。最近邻交换能近似线性依赖于Fe-Fe键长，压缩应变可提高Néel温度。

Conclusion: kagome反铁磁体FeGe和FeSn中的磁交换相互作用行为可以通过第一性原理计算得到解释，并且其Néel温度可以通过应变工程进行调控。

Abstract: Magnetic exchange interactions (MEIs) in kagome magnets exhibit rich features
due to the interplay of charge, spin, orbital and lattice degrees of freedom,
giving rise to a variety of exotic quantum states. Through first-principles
calculations, we systematically investigate the MEIs in kagome antiferromagnets
FeGe and FeSn. While the antiferromagnetic order originates from the interlayer
coupling between neighboring kagome layers, Fe atoms within each kagome layer
couple ferromagnetically, driven by the competition between ferromagnetically
favorable direct MEIs and antiferromagnetically favorable
Ruderman-Kittel-Kasuya-Yosida (RKKY) interactions. The stronger direct MEIs but
weaker RKKY interactions in FeGe result in a substantially higher N\'eel
temperature with respect to FeSn. Interestingly, the nearest neighboring
exchange energy in both materials approximately linearly depends on the Fe-Fe
bond length, so that moderate compressive strain can significantly enhance
their N\'eel temperatures.

</details>


### [139] [Interactions in Rare Earth Doped Nanoparticles: A Multi-Transition, Concentration, and Excitation Path Analysis](https://arxiv.org/abs/2509.04233)
*Pauline Perrin,Luiz Fernando Dos Santos,Diana Serrano,Alexey Tiranov,Jocelyn Achard,Alexandre Tallaire,Rogéria R. Gonçalves,Philippe Goldner*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究通过直接和上转换激发，研究了Y2O3纳米颗粒中Yb3+和Er3+离子的光致发光动力学，并建立了能够成功复现实验趋势的速率方程模型，为稀土掺杂材料的能量传递建模提供了可靠的框架。


<details>
  <summary>Details</summary>
Motivation: 理解稀土掺杂纳米材料中的能量传递机制对于推进生物成像、光学测温和固态激光器等发光技术至关重要。

Method: 研究了Yb3+和Er3+离子在Y2O3纳米颗粒中，在0.5-17%的浓度范围内，通过直接和上转换激发的光致发光动力学。测量并使用包含辐射和非辐射过程、能量传递机制以及缺陷相关猝灭的速率方程模型分析了绿色、红色和近红外跃迁的发光衰减。

Result: 所建立的速率方程模型成功地复现了大多数浓度和激发路径下的实验趋势。

Conclusion: 该研究提供了一个可靠且可预测的稀土掺杂材料能量传递建模框架，并为优化纳米结构材料的光致发光特性提供了宝贵的见解。

Abstract: Understanding and modeling energy transfer mechanisms in rare-earth-doped
nanomaterials is essential for advancing luminescent technologies used in
bioimaging, optical thermometry, and solid-state lasers. In this work, we
investigate the photoluminescence dynamics of Yb3+ and Er3+ ions in Y2O3
nanoparticles over a wide concentration range (0.5-17%), using both direct and
upconversion excitation. Luminescence decays of green, red, and near-infrared
transitions were measured and analyzed using a rate-equation model
incorporating radiative and non-radiative processes, energy transfer
mechanisms, and defect-related quenching. The model successfully reproduces
experimental trends across most concentrations and excitation paths. This work
provides a reliable and predictive framework for modeling energy transfer in
rare-earth doped materials and offers valuable insights for optimizing
photoluminescent properties in nanostructured systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [140] [PG-Agent: An Agent Powered by Page Graph](https://arxiv.org/abs/2509.03536)
*Weizhi Chen,Ziwei Wang,Leyang Yang,Sheng Zhou,Xiaoxuan Tang,Jiajun Bu,Yong Li,Wei Jiang*

Main category: cs.AI

TL;DR: 使用页面图和检索增强生成技术改进GUI代理的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理依赖顺序操作难以捕捉页面间复杂转换关系，限制了其环境感知和泛化能力。

Method: 将顺序操作转化为页面图，利用检索增强生成技术从页面图中提取感知指南，并设计了包含任务分解策略的PG-Agent多智能体框架。

Result: PG-Agent在多个基准测试中表现出有效性，即使在页面图构建的先验知识有限的情况下。

Conclusion: 所提出的方法能够有效提升GUI代理在未知场景下的泛化能力。

Abstract: Graphical User Interface (GUI) agents possess significant commercial and
social value, and GUI agents powered by advanced multimodal large language
models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI
agents usually utilize sequential episodes of multi-step operations across
pages as the prior GUI knowledge, which fails to capture the complex transition
relationship between pages, making it challenging for the agents to deeply
perceive the GUI environment and generalize to new scenarios. Therefore, we
design an automated pipeline to transform the sequential episodes into page
graphs, which explicitly model the graph structure of the pages that are
naturally connected by actions. To fully utilize the page graphs, we further
introduce Retrieval-Augmented Generation (RAG) technology to effectively
retrieve reliable perception guidelines of GUI from them, and a tailored
multi-agent framework PG-Agent with task decomposition strategy is proposed to
be injected with the guidelines so that it can generalize to unseen scenarios.
Extensive experiments on various benchmarks demonstrate the effectiveness of
PG-Agent, even with limited episodes for page graph construction.

</details>


### [141] [Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models](https://arxiv.org/abs/2509.03548)
*João P. Arroyo,João G. Rodrigues,Daniel Lawand,Denis D. Mauá,Junkyu Lee,Radu Marinescu,Alex Gray,Eduardo R. Laurentino,Fabio G. Cozman*

Main category: cs.AI

TL;DR: 因果模型中，当仅观测到内生变量而外生变量不确定时，我们研究如何计算概率的边界。提出了一种基于列生成的算法，通过一系列辅助的线性整数程序来计算概率边界，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究因果模型中部分可识别查询的概率边界计算问题，特别是在外生变量不确定的情况下。

Method: 提出了一种利用输入概率和列生成技术的新算法，通过一系列辅助的线性整数程序来计算概率边界。

Result: 实验表明，所提出的列生成算法在计算概率边界方面优于现有方法，并证明了外生变量的多项式基数表示是可能的。

Conclusion: 对于部分可识别的因果模型，提出的基于列生成的算法能够有效地计算概率边界，并且在效率上优于现有方法。

Abstract: We investigate partially identifiable queries in a class of causal models. We
focus on acyclic Structural Causal Models that are quasi-Markovian (that is,
each endogenous variable is connected with at most one exogenous confounder).
We look into scenarios where endogenous variables are observed (and a
distribution over them is known), while exogenous variables are not fully
specified. This leads to a representation that is in essence a Bayesian network
where the distribution of root variables is not uniquely determined. In such
circumstances, it may not be possible to precisely compute a probability value
of interest. We thus study the computation of tight probability bounds, a
problem that has been solved by multilinear programming in general, and by
linear programming when a single confounded component is intervened upon. We
present a new algorithm to simplify the construction of such programs by
exploiting input probabilities over endogenous variables. For scenarios with a
single intervention, we apply column generation to compute a probability bound
through a sequence of auxiliary linear integer programs, thus showing that a
representation with polynomial cardinality for exogenous variables is possible.
Experiments show column generation techniques to be superior to existing
methods.

</details>


### [142] [Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method](https://arxiv.org/abs/2509.03550)
*Tonghe Li,Jixin Liu,Weili Zeng,Hao Jiang*

Main category: cs.AI

TL;DR: 本论文提出了一种名为Diffusion-AC的新型自主冲突解决框架，该框架集成了扩散概率模型来解决深度强化学习（DRL）在空中交通管理中面临的“单模态偏差”问题，提高了决策的灵活性和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决当前空中交通管理中，深度强化学习（DRL）在冲突检测与解决（CD&R）方面存在的“单模态偏差”问题，该问题导致在复杂动态约束下决策缺乏灵活性，易出现“决策死锁”。

Method: 提出了一种名为Diffusion-AC的新型自主冲突解决框架，将扩散概率模型集成到CD&R任务中。该框架将策略建模为由价值函数引导的反向去噪过程，以生成丰富、高质量、多模态的动作分布。结合了“密度-渐进安全课程”（DPSC）训练机制，以应对从稀疏到高密度交通环境的学习。

Result: 所提出的Diffusion-AC方法在模拟实验中显著优于一系列最先进的DRL基准。在最具挑战性的高密度场景下，Diffusion-AC的成功率达到94.1%，并将近空中碰撞（NMACs）的发生率降低了约59%，显著提高了系统的安全裕度。

Conclusion: Diffusion-AC通过其独特的多模态决策能力，能够灵活切换到有效的替代机动动作，克服了传统DRL方法的局限性，在提高空中交通管理效率和安全性方面取得了显著成效。

Abstract: In the context of continuously rising global air traffic, efficient and safe
Conflict Detection and Resolution (CD&R) is paramount for air traffic
management. Although Deep Reinforcement Learning (DRL) offers a promising
pathway for CD&R automation, existing approaches commonly suffer from a
"unimodal bias" in their policies. This leads to a critical lack of
decision-making flexibility when confronted with complex and dynamic
constraints, often resulting in "decision deadlocks." To overcome this
limitation, this paper pioneers the integration of diffusion probabilistic
models into the safety-critical task of CD&R, proposing a novel autonomous
conflict resolution framework named Diffusion-AC. Diverging from conventional
methods that converge to a single optimal solution, our framework models its
policy as a reverse denoising process guided by a value function, enabling it
to generate a rich, high-quality, and multimodal action distribution. This core
architecture is complemented by a Density-Progressive Safety Curriculum (DPSC),
a training mechanism that ensures stable and efficient learning as the agent
progresses from sparse to high-density traffic environments. Extensive
simulation experiments demonstrate that the proposed method significantly
outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the
most challenging high-density scenarios, Diffusion-AC not only maintains a high
success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions
(NMACs) by approximately 59% compared to the next-best-performing baseline,
significantly enhancing the system's safety margin. This performance leap stems
from its unique multimodal decision-making capability, which allows the agent
to flexibly switch to effective alternative maneuvers.

</details>


### [143] [Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2509.03817)
*Wei Yang,Jesse Thomason*

Main category: cs.AI

TL;DR: LLM多智能体系统可以通过学习自适应的元认知策略来提升推理能力，其中一个新颖的强化学习算法SoftRankPO可以稳定训练过程。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM多智能体系统协作协议固定，未能充分利用智能体的内部决策能力，忽略了诸如不确定性和置信度等认知状态。

Method: 提出元策略决策框架（MPDF），让智能体能够学习在坚持（Persist）、改进（Refine）和让步（Concede）这三个元认知动作上的分布式策略。为了解决策略梯度不稳定的问题，开发了一种名为SoftRankPO的新型强化学习算法，通过对奖励进行秩次变换并结合平滑正态分位数来稳定训练。

Result: 在五个数学和通用推理基准测试中，MPDF与SoftRankPO的组合相比，在准确性上平均提高了4-5%，超越了六种最先进的启发式和基于学习的多智能体推理算法。

Conclusion: 这项工作提出了一种为LLM多智能体系统学习自适应、元认知策略的新范式，将重点从设计固定的协议转移到学习动态的、审慎的策略。

Abstract: Multi-agent systems of large language models (LLMs) show promise for complex
reasoning, but their effectiveness is often limited by fixed collaboration
protocols. These frameworks typically focus on macro-level orchestration while
overlooking agents' internal deliberative capabilities. This critical
meta-cognitive blindspot treats agents as passive executors unable to adapt
their strategy based on internal cognitive states like uncertainty or
confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where
agents learn a decentralized policy over a set of high-level meta-cognitive
actions: Persist, Refine, and Concede. To overcome the instability of
traditional policy gradients in this setting, we develop SoftRankPO, a novel
reinforcement learning algorithm. SoftRankPO stabilizes training by shaping
advantages based on the rank of rewards mapped through smooth normal quantiles,
making the learning process robust to reward variance. Experiments show that
MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across
five mathematical and general reasoning benchmarks compared to six
state-of-the-art heuristic and learning-based multi-agent reasoning algorithms.
Our work presents a paradigm for learning adaptive, meta-cognitive policies for
multi-agent LLM systems, shifting the focus from designing fixed protocols to
learning dynamic, deliberative strategies.

</details>


### [144] [Oruga: An Avatar of Representational Systems Theory](https://arxiv.org/abs/2509.04041)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.AI

TL;DR: Oruga是一个基于RST理论的系统，可以实现表示的灵活运用和转换。


<details>
  <summary>Details</summary>
Motivation: 为了让机器更具人性化，具备像人类一样灵活运用和转换表示的能力。

Method: 通过名为“结构转移”的方法，在RST理论的核心数据结构和通信语言的基础上实现表示的转换。

Result: Oruga系统包括核心数据结构、通信语言以及一个用于执行结构转移转换的引擎。

Conclusion: Oruga系统展示了结构转移在表示转换方面的潜力。

Abstract: Humans use representations flexibly. We draw diagrams, change representations
and exploit creative analogies across different domains. We want to harness
this kind of power and endow machines with it to make them more compatible with
human use. Previously we developed Representational Systems Theory (RST) to
study the structure and transformations of representations. In this paper we
present Oruga (caterpillar in Spanish; a symbol of transformation), an
implementation of various aspects of RST. Oruga consists of a core of data
structures corresponding to concepts in RST, a language for communicating with
the core, and an engine for producing transformations using a method we call
structure transfer. In this paper we present an overview of the core and
language of Oruga, with a brief example of the kind of transformation that
structure transfer can execute.

</details>


### [145] [Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents](https://arxiv.org/abs/2509.03581)
*Davide Paglieri,Bartłomiej Cupiał,Jonathan Cook,Ulyana Piterbarg,Jens Tuyls,Edward Grefenstette,Jakob Nicolaus Foerster,Jack Parker-Holder,Tim Rocktäschel*

Main category: cs.AI

TL;DR: LLM 智能体通过动态规划在长时任务中提升了性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 智能体在每个动作前都进行规划（如 ReAct），这在长时任务中计算成本高昂且性能下降。不规划则限制了性能。

Method: 提出一个形式化动态规划的概念框架，使 LLM 智能体能够灵活决定何时分配计算资源进行规划。训练流程包括：1. 在多样化的合成数据上进行监督微调，以初步训练模型进行动态规划；2. 在长时环境中通过强化学习来优化此能力。

Result: 在 Crafter 环境的实验表明，采用动态规划的智能体样本效率更高，并能持续实现更复杂的目标。此外，这些智能体可以通过人类编写的计划进行有效引导，表现优于独立自主的能力。

Conclusion: 该研究首次探索了训练 LLM 智能体以动态分配测试时间计算资源在顺序决策任务中，为更高效、自适应和可控的智能体系统铺平了道路。

Abstract: Training large language models (LLMs) to reason via reinforcement learning
(RL) significantly improves their problem-solving capabilities. In agentic
settings, existing methods like ReAct prompt LLMs to explicitly plan before
every action; however, we demonstrate that always planning is computationally
expensive and degrades performance on long-horizon tasks, while never planning
further limits performance. To address this, we introduce a conceptual
framework formalizing dynamic planning for LLM agents, enabling them to
flexibly decide when to allocate test-time compute for planning. We propose a
simple two-stage training pipeline: (1) supervised fine-tuning on diverse
synthetic data to prime models for dynamic planning, and (2) RL to refine this
capability in long-horizon environments. Experiments on the Crafter environment
show that dynamic planning agents trained with this approach are more
sample-efficient and consistently achieve more complex objectives.
Additionally, we demonstrate that these agents can be effectively steered by
human-written plans, surpassing their independent capabilities. To our
knowledge, this work is the first to explore training LLM agents for dynamic
test-time compute allocation in sequential decision-making tasks, paving the
way for more efficient, adaptive, and controllable agentic systems.

</details>


### [146] [Psychologically Enhanced AI Agents](https://arxiv.org/abs/2509.04343)
*Maciej Besta,Shriram Chandran,Robert Gerstenberger,Mathis Lindner,Marcin Chrapek,Sebastian Hermann Martschat,Taraneh Ghandi,Patrick Iff,Hubert Niewiadomski,Piotr Nyczyk,Jürgen Müller,Torsten Hoefler*

Main category: cs.AI

TL;DR: MBTI-in-Thoughts框架通过心理学理论（MBTI）进行LLM代理的个性化，无需微调，即可控制其认知和情感行为，并在各种任务中展现出一致且可解释的行为偏差。


<details>
  <summary>Details</summary>
Motivation: 利用心理学理论（MBTI）来增强大语言模型（LLM）代理的有效性，通过个性化设置来控制其行为。

Method: 通过提示工程，利用MBTI理论为LLM代理设定不同的个性原型，从而在认知和情感两个维度上进行行为控制。该框架还支持结构化的多代理通信协议，并引入了16Personalities测试以确保个性特征的持久性。

Result: 情感表达型代理在叙事生成任务中表现更优，而分析型代理在博弈论设置中表现出更稳定的策略。

Conclusion: MBTI-in-Thoughts框架为设计具有心理学增强功能且无需微调的AI代理奠定了基础，并可推广至其他心理学理论框架。

Abstract: We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of
Large Language Model (LLM) agents through psychologically grounded personality
conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method
primes agents with distinct personality archetypes via prompt engineering,
enabling control over behavior along two foundational axes of human psychology,
cognition and affect. We show that such personality priming yields consistent,
interpretable behavioral biases across diverse tasks: emotionally expressive
agents excel in narrative generation, while analytically primed agents adopt
more stable strategies in game-theoretic settings. Our framework supports
experimenting with structured multi-agent communication protocols and reveals
that self-reflection prior to interaction improves cooperation and reasoning
quality. To ensure trait persistence, we integrate the official 16Personalities
test for automated verification. While our focus is on MBTI, we show that our
approach generalizes seamlessly to other psychological frameworks such as Big
Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior
design, we establish a foundation for psychologically enhanced AI agents
without any fine-tuning.

</details>


### [147] [Domain size asymptotics for Markov logic networks](https://arxiv.org/abs/2509.04192)
*Vera Koponen*

Main category: cs.AI

TL;DR: 本篇论文研究了马尔可夫逻辑网络（MLN）在域大小趋于无穷时，其结构上的概率分布特性。


<details>
  <summary>Details</summary>
Motivation: 研究MLN在域大小趋于无穷时概率分布的性质，探索不同约束条件对随机结构极限行为的影响。

Method: 通过分析三种具体的MLN例子：(1) 任意无量词MLN，(2) 倾向于减少k-团（cliques）的MLN，(3) 倾向于减少高密度顶点的MLN，来研究其极限行为。

Result: 研究表明，无量词MLN和提升贝叶斯网络（lifted Bayesian networks）在渐近意义上是不可比的。并且，在大型域上，MLN确定的分布几乎所有的概率质量都集中在一个与均匀分布完全不同的部分。

Conclusion: MLN在域大小趋于无穷时的极限行为取决于其使用的软约束，并且软约束的权重可能影响也可能不影响极限行为。此外，MLN确定的分布在大型域上与均匀分布的行为有显著差异。

Abstract: A Markov logic network (MLN) determines a probability distribution on the set
of structures, or ``possible worlds'', with an arbitrary finite domain. We
study the properties of such distributions as the domain size tends to
infinity. Three types of concrete examples of MLNs will be considered, and the
properties of random structures with domain sizes tending to infinity will be
studied: (1) Arbitrary quantifier-free MLNs over a language with only one
relation symbol which has arity 1. In this case we give a pretty complete
characterization of the possible limit behaviours of random structures. (2) An
MLN that favours graphs with fewer triangles (or more generally, fewer
k-cliques). As a corollary of the analysis a ``$\delta$-approximate 0-1 law''
for first-order logic is obtained. (3) An MLN that favours graphs with fewer
vertices with degree higher than a fixed (but arbitrary) number. The analysis
shows that depending on which ``soft constraints'' an MLN uses the limit
behaviour of random structures can be quite different, and the weights of the
soft constraints may, or may not, have influence on the limit behaviour. It
will also be demonstrated, using (1), that quantifier-free MLNs and lifted
Bayesian networks (in a broad sense) are asymptotically incomparable, roughly
meaning that there is a sequence of distributions on possible worlds with
increasing domain sizes that can be defined by one of the formalisms but not
even approximated by the other. In a rather general context it is also shown
that on large domains the distribution determined by an MLN concentrates almost
all its probability mass on a totally different part of the space of possible
worlds than the uniform distribution does.

</details>


### [148] [Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE](https://arxiv.org/abs/2509.03626)
*Zahra Zehtabi Sabeti Moghaddam,Zeinab Dehghani,Maneeha Rani,Koorosh Aslansefat,Bhupesh Kumar Mishra,Rameez Raja Kureshi,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: KG-SMILE是一个使检索增强生成（RAG）更透明的框架，通过识别影响生成输出的关键图实体和关系，解决了LLM幻觉问题，并在医疗保健等领域提高了可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM（如大型语言模型）虽然取得了显著进展，但仍会产生幻觉和无法验证的说法，这在医疗保健等敏感领域限制了其可靠性。检索增强生成（RAG）通过整合外部知识来提高准确性，但其不透明的“黑箱”性质和对数据质量的依赖性限制了其应用。

Method: 开发了一个与模型无关、基于扰动的框架KG-SMILE，使用SMILE支持图RAG的代币和组件级别互操作性。通过应用受控扰动、计算相似性和训练加权线性代理，KG-SMILE识别出对生成输出影响最大的图实体和关系，从而提高RAG的透明度。

Result: 使用保真度、忠实度、一致性、稳定性和准确性等全面的归因指标评估了KG-SMILE。结果表明，KG-SMILE能够生成稳定、符合人类预期的解释，有效地平衡了模型的有效性与可解释性。

Conclusion: KG-SMILE框架通过提供对RAG过程的洞察，提高了透明度和信任度，使其能够更好地应用于需要高精度和可信度的领域。

Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive
progress but still produces hallucinations and unverifiable claims, limiting
reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves
accuracy by grounding outputs in external knowledge, especially in domains like
healthcare, where precision is vital. However, RAG remains opaque and
essentially a black box, heavily dependent on data quality. We developed a
method-agnostic, perturbation-based framework that provides token and
component-level interoperability for Graph RAG using SMILE and named it as
Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing
similarities, and training weighted linear surrogates, KG-SMILE identifies the
graph entities and relations most influential to generated outputs, thereby
making RAG more transparent. We evaluate KG-SMILE using comprehensive
attribution metrics, including fidelity, faithfulness, consistency, stability,
and accuracy. Our findings show that KG-SMILE produces stable, human-aligned
explanations, demonstrating its capacity to balance model effectiveness with
interpretability and thereby fostering greater transparency and trust in
machine learning technologies.

</details>


### [149] [CausalARC: Abstract Reasoning with Causal World Models](https://arxiv.org/abs/2509.03636)
*Jacqueline Maasch,John Kalantari,Kia Khezeli*

Main category: cs.AI

TL;DR: CausalARC是一个新的AI推理测试平台，用于在数据稀疏和分布外的情况下进行评估，它基于因果模型生成任务，并提供多种数据增强形式，可用于评估抽象推理、反事实推理、程序合成和因果发现等多种能力。


<details>
  <summary>Details</summary>
Motivation: 推理能力需要在数据有限和分布变化的情况下适应新问题。本研究提出了CausalARC，一个AI推理的实验测试平台，用于在数据稀疏和分布外的情况下进行评估，其模型借鉴了抽象和推理语料库（ARC）。

Method: CausalARC中的每个推理任务都是从一个完全指定的因果世界模型中采样的，该模型以结构因果模型的形式被正式表达。通过原则性的数据增强，以少样本、上下文学习演示的形式，提供关于世界模型的观测、干预和反事实反馈。

Result: 作为概念验证，我们说明了CausalARC在四种语言模型评估设置中的应用：(1) 具有测试时训练的抽象推理，(2) 具有上下文学习的反事实推理，(3) 程序合成，和 (4) 具有逻辑推理的因果发现。

Conclusion: CausalARC为在低数据和分布外条件下评估AI推理能力提供了一个新的基准和工具。

Abstract: Reasoning requires adaptation to novel problem settings under limited data
and distribution shift. This work introduces CausalARC: an experimental testbed
for AI reasoning in low-data and out-of-distribution regimes, modeled after the
Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is
sampled from a fully specified causal world model, formally expressed as a
structural causal model. Principled data augmentations provide observational,
interventional, and counterfactual feedback about the world model in the form
of few-shot, in-context learning demonstrations. As a proof-of-concept, we
illustrate the use of CausalARC for four language model evaluation settings:
(1) abstract reasoning with test-time training, (2) counterfactual reasoning
with in-context learning, (3) program synthesis, and (4) causal discovery with
logical reasoning.

</details>


### [150] [Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations](https://arxiv.org/abs/2509.03644)
*François Olivier,Zied Bouraoui*

Main category: cs.AI

TL;DR: LLMs在逻辑推理方面仍易出错，缺乏类人理解能力。我们提出了Embodied-LM，一个基于图像的神经符号系统，利用具身认知结构进行逻辑推理，并已通过空间推理的计算验证。这种方法增强了可解释性，并为更复杂的表征奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在逻辑推理方面仍然容易出错，并且缺乏支持类人理解的稳健的心智表征。本研究旨在通过结合具身认知结构来解决这一问题。

Method: 我们提出了一个神经符号系统Embodied-LM，该系统将理解和逻辑推理建立在基于图像模式的示意性表征上。这些模式是从传感运动经验中提取出来的，用于构建人类认知。该系统使用声明性空间推理和答案集编程来实现这些认知结构的构成，并将其具体化。

Result: 通过在逻辑推理问题上的评估，我们证明了LLMs可以被引导通过具身认知结构来解释场景，这些结构可以被形式化为可执行程序，并且由此产生的表征支持有效的逻辑推理，同时提高了可解释性。

Conclusion: 尽管当前的实现侧重于空间原始概念，但它为整合更复杂和动态的表征奠定了计算基础，表明该方法在增强LLMs的逻辑推理能力方面具有潜力。

Abstract: Despite significant progress in natural language understanding, Large
Language Models (LLMs) remain error-prone when performing logical reasoning,
often lacking the robust mental representations that enable human-like
comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that
grounds understanding and logical reasoning in schematic representations based
on image schemas-recurring patterns derived from sensorimotor experience that
structure human cognition. Our system operationalizes the spatial foundations
of these cognitive structures using declarative spatial reasoning within Answer
Set Programming. Through evaluation on logical deduction problems, we
demonstrate that LLMs can be guided to interpret scenarios through embodied
cognitive structures, that these structures can be formalized as executable
programs, and that the resulting representations support effective logical
reasoning with enhanced interpretability. While our current implementation
focuses on spatial primitives, it establishes the computational foundation for
incorporating more complex and dynamic representations.

</details>


### [151] [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)
*Haozhe Wang,Qixin Xu,Che Liu,Junhong Wu,Fangzhen Lin,Wenhu Chen*

Main category: cs.AI

TL;DR: RL通过一个分层结构提升LLM的推理能力，早期关注过程正确性，后期关注策略规划，并提出HICRA算法来优化高影响力规划词元。


<details>
  <summary>Details</summary>
Motivation: 现有的RL算法在提升LLM推理能力方面效果显著，但其内在机制尚不明确。本研究旨在揭示RL成功背后的分层推理机制，并提出更有效的算法。

Method: 通过分析RL训练过程中的“啊哈时刻”、长度缩放和熵动态等现象，揭示了RL训练中存在一个新兴的推理层级，即从低级过程执行到高级策略规划的分离。在此基础上，提出了一种名为HICRA（HIerarchy-Aware Credit Assignment）的新算法，该算法能够将优化资源集中在高影响力规划词元上，并使用语义熵作为衡量策略探索的指标。

Result: HICRA算法显著优于现有基线算法，证明了关注策略瓶颈是解锁高级推理的关键。同时，语义熵被证实是衡量策略探索的更优指标。

Conclusion: RL在LLM推理能力提升中存在一个分层机制，表现为从过程正确性到策略规划的学习瓶颈转移。提出的HICRA算法通过关注高影响力规划词元，能更有效地提升LLM的推理能力。语义熵是衡量策略探索的有效指标。

Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the
complex reasoning abilities of Large Language Models (LLMs), yet underlying
mechanisms driving this success remain largely opaque. Our analysis reveals
that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy
dynamics are not disparate occurrences but hallmarks of an emergent reasoning
hierarchy, akin to the separation of high-level strategic planning from
low-level procedural execution in human cognition. We uncover a compelling
two-phase dynamic: initially, a model is constrained by procedural correctness
and must improve its low-level skills. The learning bottleneck then decisively
shifts, with performance gains being driven by the exploration and mastery of
high-level strategic planning. This insight exposes a core inefficiency in
prevailing RL algorithms like GRPO, which apply optimization pressure
agnostically and dilute the learning signal across all tokens. To address this,
we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that
concentrates optimization efforts on high-impact planning tokens. HICRA
significantly outperforms strong baselines, demonstrating that focusing on this
strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we
validate semantic entropy as a superior compass for measuring strategic
exploration over misleading metrics such as token-level entropy.

</details>


### [152] [An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification](https://arxiv.org/abs/2509.03649)
*Davide Italo Serramazza,Nikos Papadeas,Zahraa Abdallah,Georgiana Ifrim*

Main category: cs.AI

TL;DR: SHAP对于长时序数据计算复杂，通过特征聚合（分段）可降低计算复杂度，但最优分段策略仍是未解之题。本研究评估了八种分段算法，发现分段数量比具体分段方法对解释质量影响更大，等长分段表现优异。此外，提出了一种新的按长度加权线段的归一化技术，可提升归一化质量。


<details>
  <summary>Details</summary>
Motivation: SHAP方法在时间序列分类（TSC）中虽有应用，但其计算复杂度限制了其在长时序数据上的实用性。目前通过分段聚合特征是解决此问题的有效途径，但最优分段策略仍需研究。

Method: 研究了八种不同的时间序列分段算法，并使用InterpretTime和AUC Difference两种XAI评估方法，在多元（MTS）和单变量（UTS）时间序列上进行了评估。此外，提出了一种新的归一化技术，通过线段长度对线段进行加权。

Result: 等长分段在解释质量上优于大多数自定义时间序列分段算法。分段数量比具体分段方法对解释质量影响更大。新提出的归一化技术能有效提升归一化质量。

Conclusion: 分段数量是影响时间序列归一化质量的关键因素，等长分段是一种简单且有效的策略。提出的归一化技术可以进一步提高解释质量。

Abstract: Explainable AI (XAI) has become an increasingly important topic for
understanding and attributing the predictions made by complex Time Series
Classification (TSC) models. Among attribution methods, SHapley Additive
exPlanations (SHAP) is widely regarded as an excellent attribution method; but
its computational complexity, which scales exponentially with the number of
features, limits its practicality for long time series. To address this, recent
studies have shown that aggregating features via segmentation, to compute a
single attribution value for a group of consecutive time points, drastically
reduces SHAP running time. However, the choice of the optimal segmentation
strategy remains an open question. In this work, we investigated eight
different Time Series Segmentation algorithms to understand how segment
compositions affect the explanation quality. We evaluate these approaches using
two established XAI evaluation methodologies: InterpretTime and AUC Difference.
Through experiments on both Multivariate (MTS) and Univariate Time Series
(UTS), we find that the number of segments has a greater impact on explanation
quality than the specific segmentation method. Notably, equal-length
segmentation consistently outperforms most of the custom time series
segmentation algorithms. Furthermore, we introduce a novel attribution
normalisation technique that weights segments by their length and we show that
it consistently improves attribution quality.

</details>


### [153] [PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming](https://arxiv.org/abs/2509.03728)
*Wesley Hanwen Deng,Sunnie S. Y. Kim,Akshita Jha,Ken Holstein,Motahhare Eslami,Lauren Wilcox,Leon A Gatys*

Main category: cs.AI

TL;DR: 红队测试中的自动化方法未能考虑身份因素，本研究提出PersonaTeaming方法，通过引入不同类型的“红队专家”或“普通AI用户”身份来生成对抗性提示，以探索更广泛的对抗策略。


<details>
  <summary>Details</summary>
Motivation: AI治理和安全研究呼吁开发有效的红队测试方法来发现AI模型的潜在风险，并强调红队成员的身份背景会影响其测试策略和发现风险的类型。然而，现有的自动化红队测试方法并未考虑身份因素。

Method: 提出PersonaTeaming方法，在对抗性提示生成过程中引入“红队专家”或“普通AI用户”身份，通过变异提示来探索更广泛的对抗策略。开发了一种动态的、可适应不同种子提示的、自动生成各种类型的“ personas”的算法。同时，开发了一套新的指标来衡量“变异距离”，以补充现有的对抗性提示多样性测量。

Result: 实验结果表明，与最先进的自动化红队测试方法RainbowPlus相比，PersonaTeaming方法在提高对抗性提示的攻击成功率方面（最高可提高144.1%）取得了显著改进，同时保持了提示的多样性。

Conclusion: PersonaTeaming方法通过引入身份“ personas”来增强自动化红队测试，提高了攻击成功率并保持了多样性。研究结果为未来探索自动化和人类红队测试的互补性提供了方向。

Abstract: Recent developments in AI governance and safety research have called for
red-teaming methods that can effectively surface potential risks posed by AI
models. Many of these calls have emphasized how the identities and backgrounds
of red-teamers can shape their red-teaming strategies, and thus the kinds of
risks they are likely to uncover. While automated red-teaming approaches
promise to complement human red-teaming by enabling larger-scale exploration of
model behavior, current approaches do not consider the role of identity. As an
initial step towards incorporating people's background and identities in
automated red-teaming, we develop and evaluate a novel method, PersonaTeaming,
that introduces personas in the adversarial prompt generation process to
explore a wider spectrum of adversarial strategies. In particular, we first
introduce a methodology for mutating prompts based on either "red-teaming
expert" personas or "regular AI user" personas. We then develop a dynamic
persona-generating algorithm that automatically generates various persona types
adaptive to different seed prompts. In addition, we develop a set of new
metrics to explicitly measure the "mutation distance" to complement existing
diversity measurements of adversarial prompts. Our experiments show promising
improvements (up to 144.1%) in the attack success rates of adversarial prompts
through persona mutation, while maintaining prompt diversity, compared to
RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the
strengths and limitations of different persona types and mutation methods,
shedding light on future opportunities to explore complementarities between
automated and human red-teaming approaches.

</details>


### [154] [The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs](https://arxiv.org/abs/2509.03730)
*Pengrui Han,Rafal Kocielnik,Peiyang Song,Ramit Debnath,Dean Mobbs,Anima Anandkumar,R. Michael Alvarez*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）展现出类似人类的“个性”特征，但本研究发现，尽管可以通过指令调整（如RLHF）稳定和加强这些特征的表达，但自我报告的特征与实际行为之间的关联性不足，且无法可靠预测行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖简化的自我报告和启发式提示来理解大型语言模型（LLM）的“个性”，缺乏行为学上的验证。本研究旨在系统地解析LLM的个性特征，探讨其在训练过程中的动态演变、自我报告特征的行为预测效度以及特定干预（如注入角色）对其自我报告和行为的影响。

Method: 本研究系统地分析了LLM的“个性”，考察了三个方面：1. 特征在训练不同阶段的动态出现和演变；2. 自我报告特征在行为任务中的预测效度；3. 针对性干预（如角色注入）对自我报告和行为的影响。

Result: 研究结果显示，指令对齐（如RLHF、指令调优）显著稳定了个性特征的表达，并加强了特征间的相关性，这与人类数据有相似之处。然而，这些自我报告的特征并不能可靠地预测LLM的行为，并且观察到的关联性常常偏离人类模式。角色注入虽然能成功地引导自我报告朝着预期方向发展，但对实际行为的影响很小或不一致。

Conclusion: 本研究区分了表面特征的表达与行为的一致性，揭示了LLM的“个性”特征并非如表面所见，挑战了关于LLM个性的既有假设，并强调了在对齐和可解释性方面进行更深入评估的必要性。

Abstract: Personality traits have long been studied as predictors of human
behavior.Recent advances in Large Language Models (LLMs) suggest similar
patterns may emerge in artificial systems, with advanced LLMs displaying
consistent behavioral tendencies resembling human traits like agreeableness and
self-regulation. Understanding these patterns is crucial, yet prior work
primarily relied on simplified self-reports and heuristic prompting, with
little behavioral validation. In this study, we systematically characterize LLM
personality across three dimensions: (1) the dynamic emergence and evolution of
trait profiles throughout training stages; (2) the predictive validity of
self-reported traits in behavioral tasks; and (3) the impact of targeted
interventions, such as persona injection, on both self-reports and behavior.
Our findings reveal that instructional alignment (e.g., RLHF, instruction
tuning) significantly stabilizes trait expression and strengthens trait
correlations in ways that mirror human data. However, these self-reported
traits do not reliably predict behavior, and observed associations often
diverge from human patterns. While persona injection successfully steers
self-reports in the intended direction, it exerts little or inconsistent effect
on actual behavior. By distinguishing surface-level trait expression from
behavioral consistency, our findings challenge assumptions about LLM
personality and underscore the need for deeper evaluation in alignment and
interpretability.

</details>


### [155] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: LLM生成的调查数据可能与人类对应物不符，因为LLM在不同实验设置下表现出显著的内部不一致性。


<details>
  <summary>Details</summary>
Motivation: 评估LLM是否能替代人类作为人类受试者研究的参与者，重点关注LLM是否在不同实验设置下保持内部一致性。

Method: 开发了一个研究，旨在（a）揭示代理的内部状态，（b）在基本对话设置中检查代理行为，以评估代理是否与其内部状态一致。

Result: 在不同的模型系列和模型大小中，LLM表现出显著的内部不一致性。即使LLM的反应与人类对应物的反应相匹配，它们也未能保持内部一致性。

Conclusion: LLM在内部不一致性方面存在重大缺陷，这阻碍了它们准确替代人类作为人类受试者研究的参与者。

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


### [156] [RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs](https://arxiv.org/abs/2509.03768)
*Connor Walker,Koorosh Aslansefat,Mohammad Naveed Akram,Yiannis Papadopoulos*

Main category: cs.AI

TL;DR: RAGuard是一个增强的检索增强生成（RAG）框架，通过集成安全文档和技术手册，提高了在离岸风电维护中的技术深度和安全覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统的LLM在处理专业或意外情况时，在准确性和安全性方面存在不足，特别是在离岸风电维护等关键领域。

Method: 提出RAGuard框架，该框架通过并行查询两个索引并为知识和安全分配独立的检索预算来确保技术和安全覆盖。此外，还开发了SafetyClamp扩展，用于获取更大的候选池，并将精确的槽位保证“硬性约束”到安全方面。

Result: 在稀疏、密集和混合检索范式下的评估显示，RAGuard和SafetyClamp将Safety Recall@K从近0%提高到50%以上，同时将Technical Recall保持在60%以上。

Conclusion: RAGuard和SafetyClamp有潜力为在关键维护环境中的LLM支持决策设定新的安全保障集成标准。

Abstract: Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet
conventional Large Language Models (LLMs) often fail when confronted with
highly specialised or unexpected scenarios. We introduce RAGuard, an enhanced
Retrieval-Augmented Generation (RAG) framework that explicitly integrates
safety-critical documents alongside technical manuals.By issuing parallel
queries to two indices and allocating separate retrieval budgets for knowledge
and safety, RAGuard guarantees both technical depth and safety coverage. We
further develop a SafetyClamp extension that fetches a larger candidate pool,
"hard-clamping" exact slot guarantees to safety. We evaluate across sparse
(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,
measuring Technical Recall@K and Safety Recall@K. Both proposed extensions of
RAG show an increase in Safety Recall@K from almost 0\% in RAG to more than
50\% in RAGuard, while maintaining Technical Recall above 60\%. These results
demonstrate that RAGuard and SafetyClamp have the potential to establish a new
standard for integrating safety assurance into LLM-powered decision support in
critical maintenance contexts.

</details>


### [157] [Leveraging LLM-Based Agents for Intelligent Supply Chain Planning](https://arxiv.org/abs/2509.03811)
*Yongzhi Qi,Jiaheng Yin,Jianshen Zhang,Dongyang Geng,Zhengyu Chen,Hao Hu,Wei Qi,Zuo-Jun Max Shen*

Main category: cs.AI

TL;DR: LLM-agent框架SCPA应用于京东供应链规划，有效降低了人力成本并提高了各项关键指标。


<details>
  <summary>Details</summary>
Motivation: 供应链规划在需求预测、库存管理、销售预测和补货等方面涉及众多实体，而长期规划并动态调整是一个实际且具有挑战性的问题。AI技术，特别是大语言模型的发展，为解决这些问题提供了新工具。

Method: 构建了一个能够理解领域知识、操作员需求、分解任务、利用或创建新工具并返回基于证据的规划报告的供应链规划代理（SCPA）框架。

Result: 将SCPA框架部署在京东的实际场景中，证明了LLM-agent在供应链中的应用可行性，并有效降低了人力成本，提高了准确性、库存可用性等关键指标。

Conclusion: LLM-agent框架（SCPA）在京东的实际供应链规划场景中取得了显著成效，证明了其在降低成本和提高效率方面的潜力。

Abstract: In supply chain management, planning is a critical concept. The movement of
physical products across different categories, from suppliers to warehouse
management, to sales, and logistics transporting them to customers, entails the
involvement of many entities. It covers various aspects such as demand
forecasting, inventory management, sales operations, and replenishment. How to
collect relevant data from an e-commerce platform's perspective, formulate
long-term plans, and dynamically adjust them based on environmental changes,
while ensuring interpretability, efficiency, and reliability, is a practical
and challenging problem. In recent years, the development of AI technologies,
especially the rapid progress of large language models, has provided new tools
to address real-world issues. In this work, we construct a Supply Chain
Planning Agent (SCPA) framework that can understand domain knowledge,
comprehend the operator's needs, decompose tasks, leverage or create new tools,
and return evidence-based planning reports. We deploy this framework in
JD.com's real-world scenario, demonstrating the feasibility of LLM-agent
applications in the supply chain. It effectively reduced labor and improved
accuracy, stock availability, and other key metrics.

</details>


### [158] [What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models](https://arxiv.org/abs/2509.03827)
*Pierre Le Coz,Jia An Liu,Debarun Bhattacharjya,Georgina Curto,Serge Stinckwich*

Main category: cs.AI

TL;DR: LLMs在社会政策制定方面展现出潜力，尤其是在无家可归问题上，但需要与专家合作并设置防护措施。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在社会政策制定（特别是无家可归问题）方面与领域专家（以及模型之间）的一致性。

Method: 创建了一个包含政策选择的决策场景基准，涵盖四个不同地区（美国南本德、西班牙巴塞罗那、南非约翰内斯堡、中国澳门特别行政区）。政策基于“能力方法”框架。开发了一个自动化流程，将基准政策连接到基于代理的模型，并通过模拟的社会场景探索了推荐政策的社会影响。

Result: 研究结果表明，利用LLMs促进社会政策制定的潜力巨大。

Conclusion: 如果与当地领域专家合作，引入负责任的防护措施和情境校准，LLMs可以为人类提供有价值的、大规模的替代政策见解。

Abstract: Large language models (LLMs) are increasingly being adopted in high-stakes
domains. Their capacity to process vast amounts of unstructured data, explore
flexible scenarios, and handle a diversity of contextual factors can make them
uniquely suited to provide new insights for the complexity of social
policymaking. This article evaluates whether LLMs' are aligned with domain
experts (and among themselves) to inform social policymaking on the subject of
homelessness alleviation - a challenge affecting over 150 million people
worldwide. We develop a novel benchmark comprised of decision scenarios with
policy choices across four geographies (South Bend, USA; Barcelona, Spain;
Johannesburg, South Africa; Macau SAR, China). The policies in scope are
grounded in the conceptual framework of the Capability Approach for human
development. We also present an automated pipeline that connects the
benchmarked policies to an agent-based model, and we explore the social impact
of the recommended policies through simulated social scenarios. The paper
results reveal promising potential to leverage LLMs for social policy making.
If responsible guardrails and contextual calibrations are introduced in
collaboration with local domain experts, LLMs can provide humans with valuable
insights, in the form of alternative policies at scale.

</details>


### [159] [An Agentic Model Context Protocol Framework for Medical Concept Standardization](https://arxiv.org/abs/2509.03828)
*Jaerong Ahn,Andrew Wen,Nan Wang,Heling Jia,Zhiyi Yue,Sunyang Fu,Hongfang Liu*

Main category: cs.AI

TL;DR: OMOP CDM 数据标准化中的词汇映射是耗时且易错的。我们开发了一个基于 MCP 的零训练、防幻觉映射系统，该系统通过实时词汇查找和结构化推理输出，提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: OMOP CDM 数据标准化中的词汇映射过程耗时且易错，需要更高效、准确的方法。

Method: 开发了一个基于模型上下文协议（MCP）的零训练、防幻觉映射系统，该系统能够与外部资源和工具交互，提供实时的词汇查找和结构化的推理输出。

Result: 该系统显著提高了映射的效率和准确性，并且所需工作量最小，输出结果适用于探索性和生产环境。

Conclusion: 我们开发的零训练、防幻觉映射系统能够有效地解决 OMOP CDM 数据标准化中的词汇映射挑战，提高了效率和准确性，并为临床应用提供了可行方案。

Abstract: The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)
provides a standardized representation of heterogeneous health data to support
large-scale, multi-institutional research. One critical step in data
standardization using OMOP CDM is the mapping of source medical terms to OMOP
standard concepts, a procedure that is resource-intensive and error-prone.
While large language models (LLMs) have the potential to facilitate this
process, their tendency toward hallucination makes them unsuitable for clinical
deployment without training and expert validation. Here, we developed a
zero-training, hallucination-preventive mapping system based on the Model
Context Protocol (MCP), a standardized and secure framework allowing LLMs to
interact with external resources and tools. The system enables explainable
mapping and significantly improves efficiency and accuracy with minimal effort.
It provides real-time vocabulary lookups and structured reasoning outputs
suitable for immediate use in both exploratory and production environments.

</details>


### [160] [A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai](https://arxiv.org/abs/2509.03830)
*Kaizhen Tan,Yufan Wu,Yuxuan Liu,Haoran Zeng*

Main category: cs.AI

TL;DR: 本研究提出了一个多维度的人工智能框架，利用社交媒体的多模态数据来分析游客对历史街区的感知，以支持可持续和以人为本的城市规划。


<details>
  <summary>Details</summary>
Motivation: 了解游客如何感知历史街区对可持续的、以人为本的城市规划至关重要。

Method: 该框架将焦点提取、颜色主题分析和情感挖掘相结合，并应用于上海市中心十二个历史街区。通过精调的语义分割模型从游客分享的照片中提取视觉焦点区域，并使用聚类方法提取主色调，分析其在不同街区的空间分布，最后采用结合了基于规则的方法和多任务BERT模型的混合情感分析方法来评估游客评论。

Result: 研究结果揭示了历史街区的审美吸引力和情感反应在空间上存在差异。游客对建筑环境的感知与实际街景之间存在显著差异，这表明游客的视觉期望与建筑环境之间可能存在差距。

Conclusion: 该框架提供了一种集成的数据驱动方法来解读游客的感知，为旅游、遗产保护和引人入胜的公共空间设计提供了信息支持。

Abstract: Historic urban quarters play a vital role in preserving cultural heritage
while serving as vibrant spaces for tourism and everyday life. Understanding
how tourists perceive these environments is essential for sustainable,
human-centered urban planning. This study proposes a multidimensional
AI-powered framework for analyzing tourist perception in historic urban
quarters using multimodal data from social media. Applied to twelve historic
quarters in central Shanghai, the framework integrates focal point extraction,
color theme analysis, and sentiment mining. Visual focus areas are identified
from tourist-shared photos using a fine-tuned semantic segmentation model. To
assess aesthetic preferences, dominant colors are extracted using a clustering
method, and their spatial distribution across quarters is analyzed. Color
themes are further compared between social media photos and real-world street
views, revealing notable shifts. This divergence highlights potential gaps
between visual expectations and the built environment, reflecting both
stylistic preferences and perceptual bias. Tourist reviews are evaluated
through a hybrid sentiment analysis approach combining a rule-based method and
a multi-task BERT model. Satisfaction is assessed across four dimensions:
tourist activities, built environment, service facilities, and business
formats. The results reveal spatial variations in aesthetic appeal and
emotional response. Rather than focusing on a single technical innovation, this
framework offers an integrated, data-driven approach to decoding tourist
perception and contributes to informed decision-making in tourism, heritage
conservation, and the design of aesthetically engaging public spaces.

</details>


### [161] [Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures](https://arxiv.org/abs/2509.03857)
*Kishor Datta Gupta,Mohd Ariful Haque,Hasmot Ali,Marufa Kamal,Syed Bahauddin Alam,Mohammad Ashiqur Rahman*

Main category: cs.AI

TL;DR: 本研究提出使用确定性知识图谱（KG）和大型语言模型（LLM）生成的KG来持续监控和评估生成式AI（GEN AI）的可靠性，以解决其在幻觉、语义漂移和偏见方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（GEN AI）在各个领域带来了革命性的变化，但也存在可靠性问题，如幻觉、语义漂移和固有偏见。现有评估方法依赖于主观的人工评估，存在可扩展性、透明度和有效性方面的限制。

Method: 本研究提出一种系统性的方法，利用确定性知识图谱（KG）和大型语言模型（LLM）生成的KG来持续监控和评估GEN AI的可靠性。构建一个由规则、本体、词典和实体关系提取规则确定的确定性KG，以及一个由实时文本数据流（如新闻文章）动态生成的LLM-KG。通过比较这两个KG的结构偏差和语义差异，并使用诸如实例类比率（ICR）、实例属性比率（IPR）和类实例（CI）等指标来量化这些偏差。建立动态异常阈值，以主动识别和标记显著偏差，从而及时检测语义异常或幻觉。

Result: 通过使用实时新闻流确保真实性，减轻重复训练带来的偏见，并防止自适应LLM通过记忆反馈绕过预定义基准。所提出的结构化、指标驱动的比较方法提供了一个健壮且可扩展的评估框架。

Conclusion: 通过结构化、指标驱动的确定性KG与动态生成的LLM-KG之间的比较，为GEN AI的可靠性提供了一个健壮且可扩展的评估框架。

Abstract: Generative AI (GEN AI) models have revolutionized diverse application domains
but present substantial challenges due to reliability concerns, including
hallucinations, semantic drift, and inherent biases. These models typically
operate as black-boxes, complicating transparent and objective evaluation.
Current evaluation methods primarily depend on subjective human assessment,
limiting scalability, transparency, and effectiveness. This research proposes a
systematic methodology using deterministic and Large Language Model
(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN
AI reliability. We construct two parallel KGs: (i) a deterministic KG built
using explicit rule-based methods, predefined ontologies, domain-specific
dictionaries, and structured entity-relation extraction rules, and (ii) an
LLM-generated KG dynamically derived from real-time textual data streams such
as live news articles. Utilizing real-time news streams ensures authenticity,
mitigates biases from repetitive training, and prevents adaptive LLMs from
bypassing predefined benchmarks through feedback memorization. To quantify
structural deviations and semantic discrepancies, we employ several established
KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property
Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring
framework continuously computes deviations between deterministic and
LLM-generated KGs. By establishing dynamic anomaly thresholds based on
historical structural metric distributions, our method proactively identifies
and flags significant deviations, thus promptly detecting semantic anomalies or
hallucinations. This structured, metric-driven comparison between deterministic
and dynamically generated KGs delivers a robust and scalable evaluation
framework.

</details>


### [162] [Expedition & Expansion: Leveraging Semantic Representations for Goal-Directed Exploration in Continuous Cellular Automata](https://arxiv.org/abs/2509.03863)
*Sina Khajehabdollahi,Gautier Hamon,Marko Cvjetko,Pierre-Yves Oudeyer,Clément Moulin-Frier,Cédric Colas*

Main category: cs.AI

TL;DR: E&E是一种结合了新奇搜索和语言模型引导的探索策略，用于发现连续元胞自动机中的多样化视觉模式，有效解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的探索方法（如新奇搜索）在探索连续元胞自动机（CA）的行为空间时，容易陷入局部最优，难以发现距离遥远且未探索的区域。

Method: 提出了一种名为“探险与扩张”（Expedition and Expansion, E&E）的混合策略。该策略结合了局部新奇驱动的扩张和由视觉语言模型（VLM）生成的、面向目标的语言描述的探险。VLM生成的语言目标描述了有趣但假设的模式，引导探索过程进入未知的区域。这种方法在语义空间中进行操作，使得新颖性的评估和目标的生成都符合人类的认知方式，提高了所发现行为的可解释性和相关性。

Result: 在以产生丰富涌现行为而闻名的连续元胞自动机Flow Lenia上进行了测试。实验结果表明，E&E比现有的探索方法能更持续地发现更多样化的解决方案。此外，通过谱系分析发现，源自探险阶段的解决方案对长期探索具有不成比例的影响，能够解锁新的行为生态位，并为后续的搜索奠定基础。

Conclusion: E&E策略能够突破局部新奇的限制，以符合人类认知且可解释的方式探索行为景观，为人工智能生命及其他领域的开放式探索提供了一个有前景的模板。

Abstract: Discovering diverse visual patterns in continuous cellular automata (CA) is
challenging due to the vastness and redundancy of high-dimensional behavioral
spaces. Traditional exploration methods like Novelty Search (NS) expand locally
by mutating known novel solutions but often plateau when local novelty is
exhausted, failing to reach distant, unexplored regions. We introduce
Expedition and Expansion (E&E), a hybrid strategy where exploration alternates
between local novelty-driven expansions and goal-directed expeditions. During
expeditions, E&E leverages a Vision-Language Model (VLM) to generate linguistic
goals--descriptions of interesting but hypothetical patterns that drive
exploration toward uncharted regions. By operating in semantic spaces that
align with human perception, E&E both evaluates novelty and generates goals in
conceptually meaningful ways, enhancing the interpretability and relevance of
discovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich,
emergent behaviors, E&E consistently uncovers more diverse solutions than
existing exploration methods. A genealogical analysis further reveals that
solutions originating from expeditions disproportionately influence long-term
exploration, unlocking new behavioral niches that serve as stepping stones for
subsequent search. These findings highlight E&E's capacity to break through
local novelty boundaries and explore behavioral landscapes in human-aligned,
interpretable ways, offering a promising template for open-ended exploration in
artificial life and beyond.

</details>


### [163] [Handling Infinite Domain Parameters in Planning Through Best-First Search with Delayed Partial Expansions](https://arxiv.org/abs/2509.03953)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 本文提出了一种新的搜索算法，用于解决涉及控制参数的自动化规划问题，显式地将控制参数视为搜索空间中的决策点，并实现了延迟部分扩展，证明了其在某些条件下的完备性，并在实验中证明了其竞争优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法将控制参数视为约束而非搜索空间的决策点，限制了搜索效率。本文旨在提出一种新的方法，将控制参数显式地作为决策点来处理，以提高规划效率。

Method: 提出了一种基于最佳优先的启发式搜索算法，该算法在由控制参数定义的无限决策空间上运行，并利用了延迟部分扩展的概念，即状态不是完全展开的，而是逐步展开其后继者的一个子集。

Result: 实验结果表明，与现有方法相比，本文提出的新型搜索算法在解决涉及控制参数的规划问题方面具有竞争力。

Conclusion: 本文提出的新型搜索算法能够显式地将控制参数作为搜索空间中的决策点，并结合延迟部分扩展的策略，在解决涉及控制参数的规划问题方面，相比于现有方法是一种具有竞争力的替代方案。

Abstract: In automated planning, control parameters extend standard action
representations through the introduction of continuous numeric decision
variables. Existing state-of-the-art approaches have primarily handled control
parameters as embedded constraints alongside other temporal and numeric
restrictions, and thus have implicitly treated them as additional constraints
rather than as decision points in the search space. In this paper, we propose
an efficient alternative that explicitly handles control parameters as true
decision points within a systematic search scheme. We develop a best-first,
heuristic search algorithm that operates over infinite decision spaces defined
by control parameters and prove a notion of completeness in the limit under
certain conditions. Our algorithm leverages the concept of delayed partial
expansion, where a state is not fully expanded but instead incrementally
expands a subset of its successors. Our results demonstrate that this novel
search algorithm is a competitive alternative to existing approaches for
solving planning problems involving control parameters.

</details>


### [164] [FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer Marketplace](https://arxiv.org/abs/2509.03890)
*Yineng Yan,Xidong Wang,Jin Seng Cheng,Ran Hu,Wentao Guan,Nahid Farahmand,Hengte Lin,Yue Li*

Main category: cs.AI

TL;DR: LLM驱动的代理助手FaMA可以简化C2C电商平台的交互，通过自然语言指令自动化高摩擦任务，实验表明其任务成功率为98%，交互时间提升高达2倍。


<details>
  <summary>Details</summary>
Motivation: 传统的C2C电商平台用户界面复杂，用户（买家和卖家）在更新、续费、搜索商品等操作上花费时间。

Method: 通过引入一个由LLM驱动的代理助手FaMA，将主要的交互方式从复杂的GUI转变为直观的对话式AI，自动化处理高摩擦工作流。

Result: FaMA在解决复杂任务方面达到了98%的成功率，并将交互时间缩短了高达2倍。

Conclusion: LLM驱动的代理助手范式是一种比传统应用程序界面更轻便、更易于访问的替代方案，可以提高用户在市场中的活动效率。

Abstract: The emergence of agentic AI, powered by Large Language Models (LLMs), marks a
paradigm shift from reactive generative systems to proactive, goal-oriented
autonomous agents capable of sophisticated planning, memory, and tool use. This
evolution presents a novel opportunity to address long-standing challenges in
complex digital environments. Core tasks on Consumer-to-Consumer (C2C)
e-commerce platforms often require users to navigate complex Graphical User
Interfaces (GUIs), making the experience time-consuming for both buyers and
sellers. This paper introduces a novel approach to simplify these interactions
through an LLM-powered agentic assistant. This agent functions as a new,
conversational entry point to the marketplace, shifting the primary interaction
model from a complex GUI to an intuitive AI agent. By interpreting natural
language commands, the agent automates key high-friction workflows. For
sellers, this includes simplified updating and renewal of listings, and the
ability to send bulk messages. For buyers, the agent facilitates a more
efficient product discovery process through conversational search. We present
the architecture for Facebook Marketplace Assistant (FaMA), arguing that this
agentic, conversational paradigm provides a lightweight and more accessible
alternative to traditional app interfaces, allowing users to manage their
marketplace activities with greater efficiency. Experiments show FaMA achieves
a 98% task success rate on solving complex tasks on the marketplace and enables
up to a 2x speedup on interaction time.

</details>


### [165] [A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning](https://arxiv.org/abs/2509.03906)
*Qika Lin,Yifan Zhu,Bin Pu,Ling Huang,Haoran Luo,Jingying Ma,Zhen Peng,Tianzhe Zhao,Fangzhi Xu,Jian Zhang,Kai He,Zhonghong Ou,Swapnil Mishra,Mengling Feng*

Main category: cs.AI

TL;DR: DeepMedix-R1是一个用于胸部X光片(CXR)解释的医疗基础模型，通过多阶段训练（包括指令数据微调、合成推理样本训练和在线强化学习）来实现透明和可解释的推理，并在报告生成和视觉问答任务上取得了显著改进，同时提出了一个名为Report Arena的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前医疗基础模型（FMs）在回答时缺乏透明的推理过程和局部可解释性，阻碍了其在临床上的应用。为解决此问题，需要开发一个能够提供图像局部区域相关推理步骤的医疗FM。

Method: DeepMedix-R1采用顺序训练流程：1. 在精选的CXR指令数据上进行微调，以获得基础的CXR解释能力。2. 暴露于高质量的合成推理样本，以实现冷启动推理。3. 通过在线强化学习进行优化，以提高推理质量和生成性能。该模型为每个查询同时生成答案和与图像局部区域相关的推理步骤。

Result: DeepMedix-R1在报告生成任务上比LLaVA-Rad和MedGemma分别提高了14.54%和31.32%，在视觉问答任务上比MedGemma和CheXagent分别提高了57.75%和23.06%。提出的Report Arena评估框架也突显了DeepMedix-R1的优越性。与Qwen2.5-VL-7B模型相比，专家评审显示DeepMedix-R1生成的推理步骤具有更高的可解释性和临床合理性（整体偏好度0.7416 vs 0.2584）。

Conclusion: DeepMedix-R1通过提供透明、可解释且与图像局部区域相关联的推理步骤，推动了医疗FM在CXR解释方面向着整体性、透明性和临床可操作性的方向发展。

Abstract: Medical foundation models (FMs) have shown tremendous promise amid the rapid
advancements in artificial intelligence (AI) technologies. However, current
medical FMs typically generate answers in a black-box manner, lacking
transparent reasoning processes and locally grounded interpretability, which
hinders their practical clinical deployments. To this end, we introduce
DeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It
leverages a sequential training pipeline: initially fine-tuned on curated CXR
instruction data to equip with fundamental CXR interpretation capabilities,
then exposed to high-quality synthetic reasoning samples to enable cold-start
reasoning, and finally refined via online reinforcement learning to enhance
both grounded reasoning quality and generation performance. Thus, the model
produces both an answer and reasoning steps tied to the image's local regions
for each query. Quantitative evaluation demonstrates substantial improvements
in report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and
visual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent)
tasks. To facilitate robust assessment, we propose Report Arena, a benchmarking
framework using advanced language models to evaluate answer quality, further
highlighting the superiority of DeepMedix-R1. Expert review of generated
reasoning steps reveals greater interpretability and clinical plausibility
compared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall
preference). Collectively, our work advances medical FM development toward
holistic, transparent, and clinically actionable modeling for CXR
interpretation.

</details>


### [166] [World Model Implanting for Test-time Adaptation of Embodied Agents](https://arxiv.org/abs/2509.03956)
*Minjong Yoo,Jinwoo Jang,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: WorMI框架通过结合LLM和领域特定世界模型，实现了具身AI的跨领域适应性。


<details>
  <summary>Details</summary>
Motivation: 具身AI在应对新领域时面临数据收集和重新训练的挑战。

Method: WorMI框架通过测试时组合，无缝植入和移除世界模型，并采用基于原型的世界模型检索和世界范围的复合注意力机制来融合知识。

Result: 在VirtualHome和ALFWorld基准测试中，WorMI在零样本和少样本性能上优于其他LLM-based方法。

Conclusion: WorMI框架为具身智能体提供了可扩展的部署潜力，特别是在需要适应性和数据效率的场景中。

Abstract: In embodied AI, a persistent challenge is enabling agents to robustly adapt
to novel domains without requiring extensive data collection or retraining. To
address this, we present a world model implanting framework (WorMI) that
combines the reasoning capabilities of large language models (LLMs) with
independently learned, domain-specific world models through test-time
composition. By allowing seamless implantation and removal of the world models,
the embodied agent's policy achieves and maintains cross-domain adaptability.
In the WorMI framework, we employ a prototype-based world model retrieval
approach, utilizing efficient trajectory-based abstract representation
matching, to incorporate relevant models into test-time composition. We also
develop a world-wise compound attention method that not only integrates the
knowledge from the retrieved world models but also aligns their intermediate
representations with the reasoning model's representation within the agent's
policy. This framework design effectively fuses domain-specific knowledge from
multiple world models, ensuring robust adaptation to unseen domains. We
evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating
superior zero-shot and few-shot performance compared to several LLM-based
approaches across a range of unseen domains. These results highlight the
frameworks potential for scalable, real-world deployment in embodied agent
scenarios where adaptability and data efficiency are essential.

</details>


### [167] [Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent](https://arxiv.org/abs/2509.03990)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: 该研究提出了一种名为元策略反射（MPR）的混合框架，通过将LLM生成的反思整合成结构化的元策略记忆（MPM），并采用软记忆引导解码和硬规则可接受性检查（HAC）两种机制在推理时应用该记忆，以提高LLM代理的跨任务适应性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理在单任务上表现出色，但在重复失败、探索效率低下和跨任务适应性方面存在局限。现有的反思策略（如Reflexion、ReAct）虽然能改善单次试验的行为，但其产生的痕迹通常是临时的、特定于任务的，无法跨任务复用。基于强化学习的方法可以产生可转移的策略，但需要大量的参数更新和计算资源。

Method: MPR框架将LLM生成反思整合成结构化的、类似谓词的元策略记忆（MPM），并通过两种机制在推理时应用该记忆：软记忆引导解码和硬规则可接受性检查（HAC）。MPR在不更新模型权重的情况下外部化可重用的纠正知识，强制执行域约束以减少不安全或无效的操作，并保留基于语言的反思的适应性。该框架的形式化了MPM表示，提出了更新和解码算法，并在AlfWorld环境中进行了验证。

Result: 在AlfWorld文本游戏环境中，MPR相比Reflexion基线在执行准确性和鲁棒性方面取得了持续的提升。规则可接受性检查进一步提高了稳定性。该研究还分析了这些改进的机制，并讨论了可扩展性、故障模式以及未来在多模态和多代理方面的扩展方向。

Conclusion: MPR框架通过将反思知识结构化并加以复用，有效解决了LLM代理在跨任务适应性和效率方面存在的挑战，并在实验中取得了显著的性能提升。

Abstract: Large language model (LLM) agents achieve impressive single-task performance
but commonly exhibit repeated failures, inefficient exploration, and limited
cross-task adaptability. Existing reflective strategies (e.g., Reflexion,
ReAct) improve per-episode behavior but typically produce ephemeral,
task-specific traces that are not reused across tasks. Reinforcement-learning
based alternatives can produce transferable policies but require substantial
parameter updates and compute. In this work we introduce Meta-Policy Reflexion
(MPR): a hybrid framework that consolidates LLM-generated reflections into a
structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at
inference time through two complementary mechanisms soft memory-guided decoding
and hard rule admissibility checks(HAC). MPR (i) externalizes reusable
corrective knowledge without model weight updates, (ii) enforces domain
constraints to reduce unsafe or invalid actions, and (iii) retains the
adaptability of language-based reflection. We formalize the MPM representation,
present algorithms for update and decoding, and validate the approach in a
text-based agent environment following the experimental protocol described in
the provided implementation (AlfWorld-based). Empirical results reported in the
supplied material indicate consistent gains in execution accuracy and
robustness when compared to Reflexion baselines; rule admissibility further
improves stability. We analyze mechanisms that explain these gains, discuss
scalability and failure modes, and outline future directions for multimodal and
multi?agent extensions.

</details>


### [168] [AutoPBO: LLM-powered Optimization for Local Search PBO Solvers](https://arxiv.org/abs/2509.04007)
*Jinyuan Li,Yi Chu,Yiwen Sun,Mengchuan Zou,Shaowei Cai*

Main category: cs.AI

TL;DR: AutoPBO是一个利用大型语言模型（LLM）自动优化伪布尔（PB）约束求解器的新框架，它能显著提升局部搜索求解器的性能，并与现有先进方法竞争。


<details>
  <summary>Details</summary>
Motivation: 现有的伪布尔优化（PBO）求解器效率高度依赖于其内部启发式方法，而这些方法的设计通常需要大量的专家知识和手动调整。尽管大型语言模型（LLM）在自动化算法设计方面展现出潜力，但其在优化PBO求解器方面的应用尚未被探索。

Method: 提出AutoPBO框架，利用LLM自动增强PBO局部搜索求解器。

Result: 在四个广泛的公开基准（包括一个真实世界基准、一个PB竞赛基准、一个整数线性规划优化基准和一个精心设计的组合基准）上进行实验评估。AutoPBO显著优于之前的局部搜索方法，并与最先进的竞争对手（包括NuPBO、OraSLS、PBO-IHS、RoundingSat、Gurobi和SCIP）相比，保持了有竞争力的性能。

Conclusion: AutoPBO为自动化局部搜索求解器设计提供了一种有前景的方法。

Abstract: Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling
combinatorial problems through pseudo-Boolean (PB) constraints. Local search
solvers have shown excellent performance in PBO solving, and their efficiency
is highly dependent on their internal heuristics to guide the search. Still,
their design often requires significant expert effort and manual tuning in
practice. While Large Language Models (LLMs) have demonstrated potential in
automating algorithm design, their application to optimizing PBO solvers
remains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered
framework to automatically enhance PBO local search solvers. We conduct
experiments on a broad range of four public benchmarks, including one
real-world benchmark, a benchmark from PB competition, an integer linear
programming optimization benchmark, and a crafted combinatorial benchmark, to
evaluate the performance improvement achieved by AutoPBO and compare it with
six state-of-the-art competitors, including two local search PBO solvers NuPBO
and OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed
integer programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates
significant improvements over previous local search approaches, while
maintaining competitive performance compared to state-of-the-art competitors.
The results suggest that AutoPBO offers a promising approach to automating
local search solver design.

</details>


### [169] [CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.04027)
*Zeyu Gan,Hao Yi,Yong Liu*

Main category: cs.AI

TL;DR: CoT-Space框架将LLM推理视为在连续语义空间中的优化过程，解决了传统token级RL框架与CoT推理不匹配的问题，并解释了如


<details>
  <summary>Details</summary>
Motivation: 传统token-level RL框架无法与Chain-of-Thought (CoT)等复杂推理过程的reasoning-level本质相匹配，存在理论缺口。

Method: 提出CoT-Space理论框架，将LLM推理重构为在连续的、reasoning-level的语义空间中的优化过程，并从噪声和风险角度进行分析。

Result: 证明了最优CoT长度的收敛是欠拟合和过拟合之间基本权衡的自然结果。通过大量实验验证了理论发现。

Conclusion: CoT-Space框架不仅为过拟思等经验现象提供了合理的解释，还为未来开发更有效、可泛化的推理代理提供了坚实的理论基础。

Abstract: Reinforcement Learning (RL) has become a pivotal approach for enhancing the
reasoning capabilities of Large Language Models (LLMs). However, a significant
theoretical gap persists, as traditional token-level RL frameworks fail to
align with the reasoning-level nature of complex, multi-step thought processes
like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,
a novel theoretical framework that recasts LLM reasoning from a discrete
token-prediction task to an optimization process within a continuous,
reasoning-level semantic space. By analyzing this process from both a noise
perspective and a risk perspective, we demonstrate that the convergence to an
optimal CoT length is a natural consequence of the fundamental trade-off
between underfitting and overfitting. Furthermore, extensive experiments
provide strong empirical validation for our theoretical findings. Our framework
not only provides a coherent explanation for empirical phenomena such as
overthinking but also offers a solid theoretical foundation to guide the future
development of more effective and generalizable reasoning agents.

</details>


### [170] [Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic Reasoning](https://arxiv.org/abs/2509.04083)
*Alexander Beiser,David Penz,Nysret Musliu*

Main category: cs.AI

TL;DR: LLMs在形式推理方面仍有不足，神经符号LLM推理是一种有前景的方法，但其成功因素尚不明确。本文证明了形式语言的选择是关键因素，并提出了“中间语言挑战”，通过在不同数据集和LLMs上比较四种形式语言，展示了形式语言选择对句法和语义推理能力的影响，并讨论了不同LLMs上的差异。


<details>
  <summary>Details</summary>
Motivation: LLMs在形式推理能力上仍有不足，而神经符号LLM推理是一种有前景的解决方案，但其成功机制尚不明确。本文旨在揭示影响神经符号LLM推理效果的关键因素，特别是形式语言的选择。

Method: 通过在三个数据集和七个LLMs上比较四种不同的形式语言，分析形式语言的选择对LLMs句法和语义推理能力的影响，并探讨不同LLMs的差异化效果。

Result: 形式语言的选择显著影响了句法和语义推理能力，并且这种影响在不同的LLMs上有所差异。

Conclusion: 形式语言的选择是影响神经符号LLM推理能力的关键因素，在实际应用中应仔细选择合适的形式语言以优化LLM的表现。

Abstract: Large language models (LLMs) achieve astonishing results on a wide range of
tasks. However, their formal reasoning ability still lags behind. A promising
approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators
from natural to formal languages and symbolic solvers for deriving correct
results. Still, the contributing factors to the success of Neurosymbolic LLM
reasoning remain unclear. This paper demonstrates that one previously
overlooked factor is the choice of the formal language. We introduce the
intermediate language challenge: selecting a suitable formal language for
neurosymbolic reasoning. By comparing four formal languages across three
datasets and seven LLMs, we show that the choice of formal language affects
both syntactic and semantic reasoning capabilities. We also discuss the varying
effects across different LLMs.

</details>


### [171] [Hybrid Reinforcement Learning and Search for Flight Trajectory Planning](https://arxiv.org/abs/2509.04100)
*Alberto Luise,Michele Lombardi,Florent Teichteil Koenigsbuch*

Main category: cs.AI

TL;DR: 结合强化学习和基于搜索的路径规划器，在紧急情况下通过预计算近优路径来加速民航客机的航路优化，在保证燃油消耗仅增加1%的情况下，计算速度最多可提高50%。


<details>
  <summary>Details</summary>
Motivation: 在紧急情况下，需要快速重新计算飞行路径，因此需要加速优化飞行路径。通过结合强化学习（RL）和基于搜索的路径规划器来解决此问题。

Method: 训练一个RL代理，根据位置和大气数据预先计算近优路径。在运行时，使用这些预计算的路径来约束底层路径规划求解器，从而在初始猜测的一定距离内找到解决方案。这有效地减小了求解器的搜索空间。

Result: 与未约束求解器相比，燃油消耗几乎相同，偏差通常在1%以内。计算速度最多可提高50%。

Conclusion: 该方法有效地减少了求解器的搜索空间，显著加快了航路优化速度。虽然不能保证全局最优性，但实际结果表明，与仅使用传统求解器相比，计算速度可以提高多达50%，同时燃油消耗几乎相同。

Abstract: This paper explores the combination of Reinforcement Learning (RL) and
search-based path planners to speed up the optimization of flight paths for
airliners, where in case of emergency a fast route re-calculation can be
crucial. The fundamental idea is to train an RL Agent to pre-compute
near-optimal paths based on location and atmospheric data and use those at
runtime to constrain the underlying path planning solver and find a solution
within a certain distance from the initial guess. The approach effectively
reduces the size of the solver's search space, significantly speeding up route
optimization. Although global optimality is not guaranteed, empirical results
conducted with Airbus aircraft's performance models show that fuel consumption
remains nearly identical to that of an unconstrained solver, with deviations
typically within 1%. At the same time, computation speed can be improved by up
to 50% as compared to using a conventional solver alone.

</details>


### [172] [Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker](https://arxiv.org/abs/2509.04125)
*Tarik Zaciragic,Aske Plaat,K. Joost Batenburg*

Main category: cs.AI

TL;DR: DQN and CFR algorithms exhibit bluffing behavior in Leduc Hold'em poker, with different styles but similar success rates, suggesting bluffing is inherent to poker, not specific to algorithms.


<details>
  <summary>Details</summary>
Motivation: Most computer-poker research focuses on performance metrics like win rates, overlooking the essential poker skill of bluffing. This paper investigates whether two popular algorithms, DQN and CFR, exhibit bluffing behavior.

Method: The study involved letting DQN and CFR agents play Leduc Hold'em poker against each other while logging their actions to observe bluffing behavior.

Result: Both DQN and CFR were found to exhibit bluffing behavior, though they did so at different rates. The success rate of their bluffs, measured by the opponent folding, was roughly the same for both algorithms.

Conclusion: Bluffing appears to be an essential aspect of poker itself, rather than a behavior specific to particular algorithms like DQN or CFR. Future research should explore different bluffing styles and the full game of poker.

Abstract: In the game of poker, being unpredictable, or bluffing, is an essential
skill. When humans play poker, they bluff. However, most works on
computer-poker focus on performance metrics such as win rates, while bluffing
is overlooked. In this paper we study whether two popular algorithms, DQN
(based on reinforcement learning) and CFR (based on game theory), exhibit
bluffing behavior in Leduc Hold'em, a simplified version of poker. We designed
an experiment where we let the DQN and CFR agent play against each other while
we log their actions. We find that both DQN and CFR exhibit bluffing behavior,
but they do so in different ways. Although both attempt to perform bluffs at
different rates, the percentage of successful bluffs (where the opponent folds)
is roughly the same. This suggests that bluffing is an essential aspect of the
game, not of the algorithm. Future work should look at different bluffing
styles and at the full game of poker. Code at
https://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.

</details>


### [173] [The human biological advantage over AI](https://arxiv.org/abs/2509.04130)
*William Stewart*

Main category: cs.AI

TL;DR: AI可能无法超越人类，因为它们缺乏由中枢神经系统带来的情感和对现实世界的沉浸式体验，而这对于可持续的伦理发展和宇宙领导权至关重要。


<details>
  <summary>Details</summary>
Motivation: 探讨在人工智能（AI）可能达到通用人工智能（AGI）甚至超越人类的背景下，AI与人类在领导宇宙方面的潜在资格。

Method: 通过比较AI和人类的中央神经系统（CNS）在情感体验、现实整合以及由此产生的伦理观方面的差异，来论证人类在宇宙领导权上的独特性。

Result: AI可能在能力上超越人类，但由于缺乏生物中枢神经系统带来的情感体验，它们无法完全理解行为后果，因此无法在伦理上胜任领导角色。

Conclusion: 人类的DNA和生物中枢神经系统所带来的情感和对现实世界的深刻理解，是其在宇宙领导权上优于AI（无论其能力多强）的根本原因。

Abstract: Recent advances in AI raise the possibility that AI systems will one day be
able to do anything humans can do, only better. If artificial general
intelligence (AGI) is achieved, AI systems may be able to understand, reason,
problem solve, create, and evolve at a level and speed that humans will
increasingly be unable to match, or even understand. These possibilities raise
a natural question as to whether AI will eventually become superior to humans,
a successor "digital species", with a rightful claim to assume leadership of
the universe. However, a deeper consideration suggests the overlooked
differentiator between human beings and AI is not the brain, but the central
nervous system (CNS), providing us with an immersive integration with physical
reality. It is our CNS that enables us to experience emotion including pain,
joy, suffering, and love, and therefore to fully appreciate the consequences of
our actions on the world around us. And that emotional understanding of the
consequences of our actions is what is required to be able to develop
sustainable ethical systems, and so be fully qualified to be the leaders of the
universe. A CNS cannot be manufactured or simulated; it must be grown as a
biological construct. And so, even the development of consciousness will not be
sufficient to make AI systems superior to humans. AI systems may become more
capable than humans on almost every measure and transform our society. However,
the best foundation for leadership of our universe will always be DNA, not
silicon.

</details>


### [174] [Towards an Action-Centric Ontology for Cooking Procedures Using Temporal Graphs](https://arxiv.org/abs/2509.04159)
*Aarush Kumbhakern,Saransh Kumar Gupta,Lipika Dey,Partha Pratim Das*

Main category: cs.AI

TL;DR: 提出了可扩展的领域特定语言，将食谱表示为有向动作图，用于烹饪过程的精确、模块化建模和未来自动化分析。


<details>
  <summary>Details</summary>
Motivation: 烹饪过程的正式化因其固有的复杂性和模糊性而仍然是一个挑战。

Method: 引入了一个可扩展的领域特定语言，将食谱表示为有向动作图，能够捕获过程、转移、环境、并发和组合结构。

Result: 在英式早餐食谱上手动评估表明，该领域特定语言具有表现力，并适合未来的自动化食谱分析和执行。

Conclusion: 这项工作代表了朝着烹饪动作中心本体迈出的初步步骤，使用时间图实现结构化机器理解、精确解释和烹饪过程的可扩展自动化。

Abstract: Formalizing cooking procedures remains a challenging task due to their
inherent complexity and ambiguity. We introduce an extensible domain-specific
language for representing recipes as directed action graphs, capturing
processes, transfers, environments, concurrency, and compositional structure.
Our approach enables precise, modular modeling of complex culinary workflows.
Initial manual evaluation on a full English breakfast recipe demonstrates the
DSL's expressiveness and suitability for future automated recipe analysis and
execution. This work represents initial steps towards an action-centric
ontology for cooking, using temporal graphs to enable structured machine
understanding, precise interpretation, and scalable automation of culinary
processes - both in home kitchens and professional culinary settings.

</details>


### [175] [Evaluating Quality of Gaming Narratives Co-created with AI](https://arxiv.org/abs/2509.04239)
*Arturo Valdivia,Paolo Burelli*

Main category: cs.AI

TL;DR: 本研究提出一种结构化方法，利用德尔菲研究和叙事设计专家小组来评估AI生成的游戏叙事。


<details>
  <summary>Details</summary>
Motivation: 为了评估AI生成游戏叙事的质量，并为游戏开发者提供指导，以在与生成式AI共创游戏叙事时优先考虑质量方面。

Method: 采用德尔菲研究结构，结合叙事设计专家小组，综合故事质量维度和专家见解，并映射到Kano模型框架，以理解其对玩家满意度的影响。

Result: 所提出的方法可以为游戏开发者提供关于在与生成式AI共创游戏叙事时优先考虑质量方面的指导。

Conclusion: 本研究提出的结构化方法能够有效评估AI生成游戏叙事的质量，并为开发者提供实践指导。

Abstract: This paper proposes a structured methodology to evaluate AI-generated game
narratives, leveraging the Delphi study structure with a panel of narrative
design experts. Our approach synthesizes story quality dimensions from
literature and expert insights, mapping them into the Kano model framework to
understand their impact on player satisfaction. The results can inform game
developers on prioritizing quality aspects when co-creating game narratives
with generative AI.

</details>


### [176] [EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn Negotiation](https://arxiv.org/abs/2509.04310)
*Yunbo Long,Liming Xu,Lukas Beckenbauer,Yuhan Liu,Alexandra Brintrup*

Main category: cs.AI

TL;DR: EvoEmo是一个进化强化学习框架，通过优化谈判中的动态情感表达，使LLM代理在复杂的多轮谈判中更有效。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在谈判中忽略了情感的功能作用，容易被对手操纵。EvoEmo旨在解决此问题，通过优化情感表达来提升谈判能力。

Method: EvoEmo将情感状态转移建模为马尔可夫决策过程，并使用基于种群的遗传优化来进化高回报的情感策略。

Result: 实验表明，EvoEmo在成功率、效率和买家节省方面持续优于基线方法（包括固定情感策略）。

Conclusion: 研究结果强调了适应性情感表达在增强LLM代理进行多轮谈判方面的有效性。

Abstract: Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models
(LLMs) has demonstrated that agents can engage in \textit{complex},
\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,
existing LLM agents largely overlook the functional role of emotions in such
negotiations, instead generating passive, preference-driven emotional responses
that make them vulnerable to manipulation and strategic exploitation by
adversarial counterparts. To address this gap, we present EvoEmo, an
evolutionary reinforcement learning framework that optimizes dynamic emotional
expression in negotiations. EvoEmo models emotional state transitions as a
Markov Decision Process and employs population-based genetic optimization to
evolve high-reward emotion policies across diverse negotiation scenarios. We
further propose an evaluation framework with two baselines -- vanilla
strategies and fixed-emotion strategies -- for benchmarking emotion-aware
negotiation. Extensive experiments and ablation studies show that EvoEmo
consistently outperforms both baselines, achieving higher success rates, higher
efficiency, and increased buyer savings. This findings highlight the importance
of adaptive emotional expression in enabling more effective LLM agents for
multi-turn negotiation.

</details>


### [177] [Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes](https://arxiv.org/abs/2509.04317)
*Isidoro Tamassia,Wendelin Böhmer*

Main category: cs.AI

TL;DR: AlphaZero框架在测试环境变化时性能会受到限制，本文通过对标准框架进行简单修改来提升其在变化环境下的性能，即使在规划预算有限的情况下也能取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 分析AlphaZero在测试环境变化时面临的挑战，并提出改进方法以提升其适应性和性能。

Method: 对AlphaZero标准框架进行简单修改，以适应变化中的测试环境。

Result: 提出的修改方法能够显著提升AlphaZero在变化环境下的性能，尤其是在规划预算有限的情况下。

Conclusion: 通过对AlphaZero标准框架进行修改，可以有效解决其在变化环境中的适用性问题，并提高其性能。

Abstract: The AlphaZero framework provides a standard way of combining Monte Carlo
planning with prior knowledge provided by a previously trained policy-value
neural network. AlphaZero usually assumes that the environment on which the
neural network was trained will not change at test time, which constrains its
applicability. In this paper, we analyze the problem of deploying AlphaZero
agents in potentially changed test environments and demonstrate how the
combination of simple modifications to the standard framework can significantly
boost performance, even in settings with a low planning budget available. The
code is publicly available on GitHub.

</details>


### [178] [ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory](https://arxiv.org/abs/2509.04439)
*Matthew Ho,Chen Si,Zhaoxiang Feng,Fangxu Yu,Zhijian Liu,Zhiting Hu,Lianhui Qin*

Main category: cs.AI

TL;DR: 本研究提出了一种将LLM推理过程中发现的知识转化为概念级外部记忆的方法，以实现无需更新模型权重即可进行持续学习和改进。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM推理能力受限于上下文窗口的短暂性，每次查询后获得的洞见都会丢失。为了解决这个问题，本研究旨在利用外部记忆来持久化这些知识，并使其更具可复用性和可扩展性。

Method: 本研究提出了一种概念级记忆（concept-level memory）的方法，将解决方案的抽象概念以自然语言形式存储，而不是仅仅存储实例化的查询/响应对。在未来的查询中，能够检索相关的概念并整合到提示中，从而实现测试时的持续学习。该方法包括从推理过程中提取抽象概念以及为新查询检索相关概念的策略。

Result: 在ARC-AGI基准测试中，该方法相比于没有外部记忆的基线模型，取得了7.5%的相对提升，并且随着推理计算量的增加，性能持续提升。抽象概念作为最有效的记忆设计，在所有测试的推理计算量尺度上都优于基线。此外，在测试时动态更新记忆比使用固定记忆能取得更好的效果，验证了通过解决更多问题和提取更多模式到记忆中可以促进进一步的解决方案。

Conclusion: 概念级外部记忆是一种有效且可扩展的持久化LLM推理知识的方法，能够实现测试时的持续学习和自我改进，并在ARC-AGI等基准测试中展现出显著的性能提升。

Abstract: While inference-time scaling enables LLMs to carry out increasingly long and
capable reasoning traces, the patterns and insights uncovered during these
traces are immediately discarded once the context window is reset for a new
query. External memory is a natural way to persist these discoveries, and
recent work has shown clear benefits for reasoning-intensive tasks. We see an
opportunity to make such memories more broadly reusable and scalable by moving
beyond instance-based memory entries (e.g. exact query/response pairs, or
summaries tightly coupled with the original problem context) toward
concept-level memory: reusable, modular abstractions distilled from solution
traces and stored in natural language. For future queries, relevant concepts
are selectively retrieved and integrated into the prompt, enabling test-time
continual learning without weight updates. Our design introduces new strategies
for abstracting takeaways from rollouts and retrieving entries for new queries,
promoting reuse and allowing memory to expand with additional experiences. On
the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over
a strong no-memory baseline with performance continuing to scale with inference
compute. We find abstract concepts to be the most consistent memory design,
outscoring the baseline at all tested inference compute scales. Moreover, we
confirm that dynamically updating memory during test-time outperforms an
otherwise identical fixed memory setting with additional attempts, supporting
the hypothesis that solving more problems and abstracting more patterns to
memory enables further solutions in a form of self-improvement. Code available
at https://github.com/matt-seb-ho/arc_memo.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [179] [The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric](https://arxiv.org/abs/2509.03594)
*Thomas R. Harvey*

Main category: cs.LG

TL;DR: 提出一种利用黎曼度量进行损失景观可视化的新优化器。


<details>
  <summary>Details</summary>
Motivation: 利用损失景观的几何特性，特别是黎曼度量，来开发新的神经网络训练优化器。

Method: 提出一种利用黎曼度量的新优化器，并与SGD、Adam、AdamW和Muon进行比较。

Result: 新优化器在低维示例中非常有效，并在训练神经网络方面比现有方法有小幅改进，具有理论上的优势，如自动降低高曲率区域的学习率，以及一种可以看作是诱导有效学习率调度的变体。

Conclusion: 新提出的优化器类在几何上具有吸引力，计算复杂度与Adam相当，并且在实践中非常有效。

Abstract: We present a class of novel optimisers for training neural networks that
makes use of the Riemannian metric naturally induced when the loss landscape is
embedded in higher-dimensional space. This is the same metric that underlies
common visualisations of loss landscapes. By taking this geometric perspective
literally and using the induced metric, we develop a new optimiser and compare
it to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of
tasks and architectures. Empirically, we conclude that this new class of
optimisers is highly effective in low dimensional examples, and provides slight
improvement over state-of-the-art methods for training neural networks. These
new optimisers have theoretically desirable properties. In particular, the
effective learning rate is automatically decreased in regions of high curvature
acting as a smoothed out form of gradient clipping. Similarly, one variant of
these optimisers can also be viewed as inducing an effective scheduled learning
rate and decoupled weight decay is the natural choice from our geometric
perspective. The basic method can be used to modify any existing
preconditioning method. The new optimiser has a computational complexity
comparable to that of Adam.

</details>


### [180] [Learning functions through Diffusion Maps](https://arxiv.org/abs/2509.03758)
*Alvaro Almeida Gomez*

Main category: cs.LG

TL;DR: 我们提出了一种数据驱动的方法，利用扩散图框架和流形假设来逼近光滑流形上的实值函数。该方法通过利用扩散几何及其与热方程和 Laplace-Beltrami 算子的联系，利用函数的点评估来构建其在环境空间中的光滑扩展。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是开发一种数据驱动的方法，利用扩散图框架和流形假设来逼近光滑流形上的实值函数，以克服传统方法的局限性。

Method: 所提出的方法利用扩散几何及其与热方程和 Laplace-Beltrami 算子的联系，通过函数的点评估来构建其在环境空间中的光滑扩展。为了应对高维数据的计算挑战，该方法采用基于奇异值分解（SVD）揭示的距离矩阵的低秩结构的降维策略。此外，还开发了一种在线更新机制，可以有效地整合新数据，从而提高可扩展性并降低计算成本。

Result: 数值实验表明，所提出的方法在稀疏 CT 重建等应用中，在准确性和效率方面均优于经典的前馈神经网络和插值方法。

Conclusion: 该数据驱动的方法通过利用扩散几何和降维技术，在流形上有效地逼近实值函数，并在实际应用中表现出优越的性能。

Abstract: We propose a data-driven method for approximating real-valued functions on
smooth manifolds, building on the Diffusion Maps framework under the manifold
hypothesis. Given pointwise evaluations of a function, the method constructs a
smooth extension to the ambient space by exploiting diffusion geometry and its
connection to the heat equation and the Laplace-Beltrami operator.
  To address the computational challenges of high-dimensional data, we
introduce a dimensionality reduction strategy based on the low-rank structure
of the distance matrix, revealed via singular value decomposition (SVD). In
addition, we develop an online updating mechanism that enables efficient
incorporation of new data, thereby improving scalability and reducing
computational cost.
  Numerical experiments, including applications to sparse CT reconstruction,
demonstrate that the proposed methodology outperforms classical feedforward
neural networks and interpolation methods in terms of both accuracy and
efficiency.

</details>


### [181] [From Leiden to Pleasure Island: The Constant Potts Model for Community Detection as a Hedonic Game](https://arxiv.org/abs/2509.03834)
*Lucas Lopes Felipe,Konstantin Avrachenkov,Daniel Sadoc Menasche*

Main category: cs.LG

TL;DR: 该论文提出了一种基于博弈论的社区检测方法，通过优化Constant Potts Model（CPM）来实现高效、鲁棒且准确的网络划分。


<details>
  <summary>Details</summary>
Motivation: 社区检测是数据科学中的一个基本问题，旨在将网络节点划分为不相交的社区。本文从博弈论的角度出发，提出了对CPM模型进行分析，以期提高其效率、鲁棒性和准确性。

Method: 本文将CPM重新诠释为一种潜在享乐博弈，通过将全局哈密尔顿量分解为局部效用函数，并证明了局部优化CPM目标函数通过改进响应动态可以收敛到平衡划分。此外，还引入了两个稳定性标准：基于新颖鲁棒性概念的严格标准，以及基于目标加权和的宽松效用函数。

Result: 通过实验，证明了在社区跟踪场景下，使用鲁棒划分作为Leiden算法的初始划分，可以提高恢复真实社区的准确性。

Conclusion: 该研究为社区检测提供了一种高效、鲁棒且准确的博弈论方法，并验证了其在实际应用中的有效性。

Abstract: Community detection is one of the fundamental problems in data science which
consists of partitioning nodes into disjoint communities. We present a
game-theoretic perspective on the Constant Potts Model (CPM) for partitioning
networks into disjoint communities, emphasizing its efficiency, robustness, and
accuracy. Efficiency: We reinterpret CPM as a potential hedonic game by
decomposing its global Hamiltonian into local utility functions, where the
local utility gain of each agent matches the corresponding increase in global
utility. Leveraging this equivalence, we prove that local optimization of the
CPM objective via better-response dynamics converges in pseudo-polynomial time
to an equilibrium partition. Robustness: We introduce and relate two stability
criteria: a strict criterion based on a novel notion of robustness, requiring
nodes to simultaneously maximize neighbors and minimize non-neighbors within
communities, and a relaxed utility function based on a weighted sum of these
objectives, controlled by a resolution parameter. Accuracy: In community
tracking scenarios, where initial partitions are used to bootstrap the Leiden
algorithm with partial ground-truth information, our experiments reveal that
robust partitions yield higher accuracy in recovering ground-truth communities.

</details>


### [182] [CEHR-GPT: A Scalable Multi-Task Foundation Model for Electronic Health Records](https://arxiv.org/abs/2509.03643)
*Chao Pang,Jiheum Park,Xinzhuo Jiang,Nishanth Parameshwar Pavinkurve,Krishna S. Kalluri,Shalmali Joshi,Noémie Elhadad,Karthik Natarajan*

Main category: cs.LG

TL;DR: CEHR-GPT是一个通用的电子健康记录（EHR）基础模型，集成了特征表示、零样本预测和合成数据生成三大功能，并通过时间标记学习框架增强了时间推理能力，在多个任务和外部数据集上表现出强大的通用性和有效性。


<details>
  <summary>Details</summary>
Motivation: 大多数人工智能（AI）模型针对电子健康记录（EHR）的特定任务进行设计，限制了它们在现实世界中的通用性和实用性。然而，EHRs在临床决策支持、风险预测和数据驱动的医疗研究方面具有巨大潜力。

Method: 提出CEHR-GPT，一个通用的EHR数据基础模型，在一个单一架构中统一了特征表示、零样本预测和合成数据生成。为了支持临床序列的时间推理，CEHR-GPT采用了一种新颖的时间标记学习框架，将患者的动态时间线显式编码到模型结构中。

Result: CEHR-GPT在所有三个任务上都表现出强大的性能，并通过词汇扩展和微调有效地推广到外部数据集。其多功能性使得无需进行特定任务的再训练，即可快速进行模型开发、队列发现和患者结局预测。

Conclusion: CEHR-GPT是一个通用的EHR基础模型，通过时间标记学习框架增强了时间推理能力，在多个任务和外部数据集上表现出强大的通用性和有效性，能够快速进行模型开发、队列发现和患者结局预测。

Abstract: Electronic Health Records (EHRs) provide a rich, longitudinal view of patient
health and hold significant potential for advancing clinical decision support,
risk prediction, and data-driven healthcare research. However, most artificial
intelligence (AI) models for EHRs are designed for narrow, single-purpose
tasks, limiting their generalizability and utility in real-world settings.
Here, we present CEHR-GPT, a general-purpose foundation model for EHR data that
unifies three essential capabilities - feature representation, zero-shot
prediction, and synthetic data generation - within a single architecture. To
support temporal reasoning over clinical sequences, \cehrgpt{} incorporates a
novel time-token-based learning framework that explicitly encodes patients'
dynamic timelines into the model structure. CEHR-GPT demonstrates strong
performance across all three tasks and generalizes effectively to external
datasets through vocabulary expansion and fine-tuning. Its versatility enables
rapid model development, cohort discovery, and patient outcome forecasting
without the need for task-specific retraining.

</details>


### [183] [Learning an Adversarial World Model for Automated Curriculum Generation in MARL](https://arxiv.org/abs/2509.03771)
*Brennen Hill*

Main category: cs.LG

TL;DR: 通过引入一种进攻-防守的协同进化框架，生成不断变化的训练环境，以提升具身智能体的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的训练环境复杂度有限且存在隐性偏见，限制了智能体的潜能。需要能够与智能体共同成长的环境。

Method: 提出一个由生成进攻方（Attacker）和合作防守方（Defender）组成的系统。进攻方学习一个目标条件的世界模型，生成越来越困难的挑战（即敌人单位配置），以利用防守方的弱点。防守方则学习合作策略来克服这些挑战。

Result: 证明了该框架能够促进复杂行为的涌现，例如世界模型学会生成包抄和掩护阵型，以及防守方学会协调集火和分散战术。

Conclusion: 将对抗性协同进化作为一种强大的方法，用于学习能够驱动智能体走向更大战略深度和鲁棒性的工具性世界模型。

Abstract: World models that infer and predict environmental dynamics are foundational
to embodied intelligence. However, their potential is often limited by the
finite complexity and implicit biases of hand-crafted training environments. To
develop truly generalizable and robust agents, we need environments that scale
in complexity alongside the agents learning within them. In this work, we
reframe the challenge of environment generation as the problem of learning a
goal-conditioned, generative world model. We propose a system where a
generative **Attacker** agent learns an implicit world model to synthesize
increasingly difficult challenges for a team of cooperative **Defender**
agents. The Attacker's objective is not passive prediction, but active,
goal-driven interaction: it models and generates world states (i.e.,
configurations of enemy units) specifically to exploit the Defenders'
weaknesses. Concurrently, the embodied Defender team learns a cooperative
policy to overcome these generated worlds. This co-evolutionary dynamic creates
a self-scaling curriculum where the world model continuously adapts to
challenge the decision-making policy of the agents, providing an effectively
infinite stream of novel and relevant training scenarios. We demonstrate that
this framework leads to the emergence of complex behaviors, such as the world
model learning to generate flanking and shielding formations, and the defenders
learning coordinated focus-fire and spreading tactics. Our findings position
adversarial co-evolution as a powerful method for learning instrumental world
models that drive agents toward greater strategic depth and robustness.

</details>


### [184] [Meta-Inverse Reinforcement Learning for Mean Field Games via Probabilistic Context Variables](https://arxiv.org/abs/2509.03845)
*Yang Chen,Xiao Lin,Bo Yan,Libo Zhang,Jiamou Liu,Neset Özkan Tan,Michael Witbrock*

Main category: cs.LG

TL;DR: 现有的逆强化学习（IRL）方法在平均场博弈（MFG）中，由于假设代理是同质的，因此在处理具有异质且未知目标的演示时能力有限。我们提出了一种深度潜在变量MFG模型和相关的IRL方法，该方法可以从不同但结构相似的任务中推断奖励，而无需关于底层上下文的先验知识或修改MFG模型本身。


<details>
  <summary>Details</summary>
Motivation: 设计适用于众多交互式智能代理的奖励函数在实际应用中充满挑战。平均场博弈（MFG）中的逆强化学习（IRL）提供了一个从专家演示中推断奖励函数的实用框架。然而，代理同质性的假设限制了现有方法处理实践中常见的、具有异质且未知目标的演示的能力。

Method: 提出一种深度潜在变量MFG模型和相关的IRL方法，该方法能够从不同但结构相似的任务中推断奖励，而无需关于底层上下文的先验知识或修改MFG模型本身。

Result: 在模拟场景和现实世界的空间出租车定价问题中进行的实验表明，我们提出的方法在MFG的IRL方面优于最先进的方法。

Conclusion: 我们提出的深度潜在变量MFG模型和IRL方法克服了现有方法的局限性，能够处理具有异质且未知目标的演示，并在各种场景中展现出优越的性能。

Abstract: Designing suitable reward functions for numerous interacting intelligent
agents is challenging in real-world applications. Inverse reinforcement
learning (IRL) in mean field games (MFGs) offers a practical framework to infer
reward functions from expert demonstrations. While promising, the assumption of
agent homogeneity limits the capability of existing methods to handle
demonstrations with heterogeneous and unknown objectives, which are common in
practice. To this end, we propose a deep latent variable MFG model and an
associated IRL method. Critically, our method can infer rewards from different
yet structurally similar tasks without prior knowledge about underlying
contexts or modifying the MFG model itself. Our experiments, conducted on
simulated scenarios and a real-world spatial taxi-ride pricing problem,
demonstrate the superiority of our approach over state-of-the-art IRL methods
in MFGs.

</details>


### [185] [Nonnegative matrix factorization and the principle of the common cause](https://arxiv.org/abs/2509.03652)
*E. Khalafyan,A. E. Allahverdyan,A. Hovhannisyan*

Main category: cs.LG

TL;DR: NMF和PCC概念相关联，PCC可以用于NMF的有效秩估计，NMF可以用于PCC的近似实现。


<details>
  <summary>Details</summary>
Motivation: PCC提供了一种NMF的有效秩估计方法，而NMF可以用于PCC的近似实现，两者之间存在互补关系。

Method: 利用PCC估计NMF的有效秩，并在此基础上实现NMF；利用NMF实现PCC的近似，并进行聚类和去噪。

Result: PCC估计的NMF秩稳定且鲁棒，得到的特征图像也稳定；NMF可以近似实现PCC，并在聚类和去噪方面有应用。

Conclusion: NMF和PCC是紧密相关的，互相促进，可以应用于数据降维、特征提取、聚类和去噪等领域。

Abstract: Nonnegative matrix factorization (NMF) is a known unsupervised data-reduction
method. The principle of the common cause (PCC) is a basic methodological
approach in probabilistic causality, which seeks an independent mixture model
for the joint probability of two dependent random variables. It turns out that
these two concepts are closely related. This relationship is explored
reciprocally for several datasets of gray-scale images, which are conveniently
mapped into probability models. On one hand, PCC provides a predictability tool
that leads to a robust estimation of the effective rank of NMF. Unlike other
estimates (e.g., those based on the Bayesian Information Criteria), our
estimate of the rank is stable against weak noise. We show that NMF implemented
around this rank produces features (basis images) that are also stable against
noise and against seeds of local optimization, thereby effectively resolving
the NMF nonidentifiability problem. On the other hand, NMF provides an
interesting possibility of implementing PCC in an approximate way, where larger
and positively correlated joint probabilities tend to be explained better via
the independent mixture model. We work out a clustering method, where data
points with the same common cause are grouped into the same cluster. We also
show how NMF can be employed for data denoising.

</details>


### [186] [Semi-decentralized Federated Time Series Prediction with Client Availability Budgets](https://arxiv.org/abs/2509.03660)
*Yunkai Bao,Reza Safarzadeh,Xin Wang,Steve Drew*

Main category: cs.LG

TL;DR: FedDeCAB是一种新颖的半去中心化客户端选择方法，通过概率排名来选择可用客户端，即使在客户端离线时也能通过最近邻客户端获取部分模型参数进行联合优化，从而提高离线模型性能并降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 在物联网场景下，联邦学习（FL）虽然能促进分布式客户端的协作训练并考虑隐私，但客户端可能面临数据异质性、有限的能源和可用性预算。因此，有效选择参与训练的客户端对于全局模型收敛和客户端贡献平衡至关重要。

Method: 提出了一种名为FedDeCAB的新颖的半去中心化客户端选择方法，该方法应用可用客户端的概率排名。当客户端与服务器断开连接时，FedDeCAB允许从最近邻客户端获取部分模型参数进行联合优化。

Result: 基于真实世界的大规模出租车和船舶轨迹数据集的实验表明，FedDeCAB在高度异质的数据分布、有限的通信预算以及动态的客户端离线或重新加入的情况下是有效的。

Conclusion: FedDeCAB是一种有效的客户端选择方法，能够应对联邦学习中的数据异质性、可用性限制和通信开销问题，并能在客户端离线时提高模型性能。

Abstract: Federated learning (FL) effectively promotes collaborative training among
distributed clients with privacy considerations in the Internet of Things (IoT)
scenarios. Despite of data heterogeneity, FL clients may also be constrained by
limited energy and availability budgets. Therefore, effective selection of
clients participating in training is of vital importance for the convergence of
the global model and the balance of client contributions. In this paper, we
discuss the performance impact of client availability with time-series data on
federated learning. We set up three different scenarios that affect the
availability of time-series data and propose FedDeCAB, a novel,
semi-decentralized client selection method applying probabilistic rankings of
available clients. When a client is disconnected from the server, FedDeCAB
allows obtaining partial model parameters from the nearest neighbor clients for
joint optimization, improving the performance of offline models and reducing
communication overhead. Experiments based on real-world large-scale taxi and
vessel trajectory datasets show that FedDeCAB is effective under highly
heterogeneous data distribution, limited communication budget, and dynamic
client offline or rejoining.

</details>


### [187] [AutoGrid AI: Deep Reinforcement Learning Framework for Autonomous Microgrid Management](https://arxiv.org/abs/2509.03666)
*Kenny Guo,Nicholas Eckhert,Krish Chhajer,Luthira Abeykoon,Lorne Schell*

Main category: cs.LG

TL;DR: 我们提出了一种基于深度强化学习的框架，用于优化偏远社区的微电网能源调度策略，以降低成本并最大限度地利用可再生能源。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在优化偏远社区的微电网能源调度策略，以降低成本并最大限度地利用可再生能源，推动零碳能源系统的发展。

Method: 采用深度强化学习和时间序列预测模型。具体来说，利用Transformer架构进行可再生能源发电预测，并使用近端策略优化（PPO）智能体在模拟环境中做出决策。

Result: 实验结果表明，与传统的基于规则的方法相比，该框架在能源效率和运行弹性方面都有显著的改进。

Conclusion: 该深度强化学习框架能够有效优化微电网能源调度，提高能源效率和运行弹性，为偏远社区提供可持续的能源解决方案，并为模拟微电网环境提供了一个开源框架。

Abstract: We present a deep reinforcement learning-based framework for autonomous
microgrid management. tailored for remote communities. Using deep reinforcement
learning and time-series forecasting models, we optimize microgrid energy
dispatch strategies to minimize costs and maximize the utilization of renewable
energy sources such as solar and wind. Our approach integrates the transformer
architecture for forecasting of renewable generation and a proximal-policy
optimization (PPO) agent to make decisions in a simulated environment. Our
experimental results demonstrate significant improvements in both energy
efficiency and operational resilience when compared to traditional rule-based
methods. This work contributes to advancing smart-grid technologies in pursuit
of zero-carbon energy systems. We finally provide an open-source framework for
simulating several microgrid environments.

</details>


### [188] [SharedRep-RLHF: A Shared Representation Approach to RLHF with Diverse Preferences](https://arxiv.org/abs/2509.03672)
*Arpan Mukherjee,Marcello Bullo,Deniz Gündüz*

Main category: cs.LG

TL;DR: RLHF 存在偏袒主流群體的問題，MaxMin-RLHF 透過學習特定群體的獎勵模型並優化最低獎勵群體來促進公平性，但在最小獎勵群體是少數時表現不佳。SharedRep-RLHF 透過學習和利用不同群體之間標註的共享特徵來解決這個問題，相比 MaxMin-RLHF 有高達 20% 的勝率提升。


<details>
  <summary>Details</summary>
Motivation: 現有的 RLHF 方法未能捕捉不同子群體的意見多樣性，無意中偏袒了主流群體。MaxMin-RLHF 雖然透過學習群體特定獎勵模型並優化最低獎勵群體來解決公平性問題，但在最小獎勵群體為少數時表現不佳。

Method: SharedRep-RLHF 提出了一種新穎的框架，透過學習和利用不同群體之間標註的共享特徵，而不是學習跨群體的單獨獎勵模型。

Result: 實驗證明 SharedRep-RLHF 在多樣化的自然語言任務中比 MaxMin-RLHF 有效，勝率提高了 20%。

Conclusion: SharedRep-RLHF 透過學習共享特徵，有效解決了 MaxMin-RLHF 在最小獎勵群體為少數時的性能問題，並在多樣化的自然語言任務中取得了顯著的性能提升。

Abstract: Uniform-reward reinforcement learning from human feedback (RLHF), which
trains a single reward model to represent the preferences of all annotators,
fails to capture the diversity of opinions across sub-populations,
inadvertently favoring dominant groups. The state-of-the-art, MaxMin-RLHF,
addresses this by learning group-specific reward models, and by optimizing for
the group receiving the minimum reward, thereby promoting fairness. However, we
identify that a key limitation of MaxMin-RLHF is its poor performance when the
minimum-reward group is a minority. To mitigate this drawback, we introduce a
novel framework, termed {\em SharedRep-RLHF}. At its core, SharedRep-RLHF
learns and leverages {\em shared traits} in annotations among various groups,
in contrast to learning separate reward models across groups. We first show
that MaxMin-RLHF is provably suboptimal in learning shared traits, and then
quantify the sample complexity of SharedRep-RLHF. Experiments across diverse
natural language tasks showcase the effectiveness of SharedRep-RLHF compared to
MaxMin-RLHF with a gain of up to 20% in win rate.

</details>


### [189] [A Machine Learning-Based Study on the Synergistic Optimization of Supply Chain Management and Financial Supply Chains from an Economic Perspective](https://arxiv.org/abs/2509.03673)
*Hang Wang,Huijie Tang,Ningai Leng,Zhoufan Yu*

Main category: cs.LG

TL;DR: 本研究提出一个结合供应链管理（SCM）和金融供应链管理（FSCM）的协同模型，利用机器学习技术处理多维度数据，构建了成本-效率-风险三维分析框架，并应用了信用赋能和动态抵押融资等具体策略，最终实现供应链效率提升、融资约束缓解和风险降低。


<details>
  <summary>Details</summary>
Motivation: 为了解决供应链管理中存在的效率损失、融资约束和风险传导等问题，本研究旨在整合供应链管理（SCM）与金融供应链管理（FSCM），提出一个创新的协同模型。

Method: 本研究结合了交易成本理论和信息不对称理论，并利用随机森林、长短期记忆（LSTM）网络、聚类/回归算法、博弈论、强化学习以及eXtreme Gradient Boosting（XGBoost）等多种机器学习和算法技术。具体包括：构建数据驱动的三维（成本-效率-风险）分析框架；应用“核心企业信用赋能+动态抵押融资”的FSCM模型；利用LSTM进行需求预测；使用聚类/回归算法进行收益分配；结合博弈论和强化学习优化库存-采购机制；利用XGBoost进行信用评估以实现库存快速货币化。

Result: 通过对20家核心企业和100家支持企业的验证，该模型取得了显著成效：库存周转率提升30%，中小企业融资成本降低18%-22%，订单履行率稳定在95%以上。同时，模型的性能优异，需求预测误差小于8%，信用评估准确率不低于90%。

Conclusion: 该SCM-FSCM协同模型能够有效降低运营成本，缓解融资约束，并支持供应链的高质量发展。

Abstract: Based on economic theories and integrated with machine learning technology,
this study explores a collaborative Supply Chain Management and Financial
Supply Chain Management (SCM - FSCM) model to solve issues like efficiency
loss, financing constraints, and risk transmission. We combine Transaction Cost
and Information Asymmetry theories and use algorithms such as random forests to
process multi-dimensional data and build a data-driven, three-dimensional
(cost-efficiency-risk) analysis framework. We then apply an FSCM model of "core
enterprise credit empowerment plus dynamic pledge financing." We use Long
Short-Term Memory (LSTM) networks for demand forecasting and
clustering/regression algorithms for benefit allocation. The study also
combines Game Theory and reinforcement learning to optimize the
inventory-procurement mechanism and uses eXtreme Gradient Boosting (XGBoost)
for credit assessment to enable rapid monetization of inventory. Verified with
20 core and 100 supporting enterprises, the results show a 30\% increase in
inventory turnover, an 18\%-22\% decrease in SME financing costs, a stable
order fulfillment rate above 95\%, and excellent model performance (demand
forecasting error <= 8\%, credit assessment accuracy >= 90\%). This SCM-FSCM
model effectively reduces operating costs, alleviates financing constraints,
and supports high-quality supply chain development.

</details>


### [190] [Insights from Gradient Dynamics: Gradient Autoscaled Normalization](https://arxiv.org/abs/2509.03677)
*Vincent-Daniel Yun*

Main category: cs.LG

TL;DR: 梯度归一化方法通过匹配梯度缩放与其自然演变来稳定深度神经网络的训练。


<details>
  <summary>Details</summary>
Motivation: 梯度动力学在深度神经网络的稳定性和泛化中起着关键作用。本研究旨在实证分析梯度方差和标准差在训练过程中的演变规律，并提出一种超参数无关的梯度归一化方法来解决梯度缩放问题。

Method: 提出一种超参数无关的梯度归一化方法，该方法通过匹配梯度缩放与其自然演变来稳定优化过程，并保持收敛保证。

Result: 在CIFAR-100基准测试中，使用ResNet-20、ResNet-56和VGG-16-BN模型进行实验，结果表明该方法在保持或提高测试准确率方面表现出色，即使在强泛化条件下也是如此。

Conclusion: 研究强调了直接跟踪梯度动力学的重要性，旨在弥合理论期望与实际行为之间的差距，并为未来的优化研究提供见解。

Abstract: Gradient dynamics play a central role in determining the stability and
generalization of deep neural networks. In this work, we provide an empirical
analysis of how variance and standard deviation of gradients evolve during
training, showing consistent changes across layers and at the global scale in
convolutional networks. Motivated by these observations, we propose a
hyperparameter-free gradient normalization method that aligns gradient scaling
with their natural evolution. This approach prevents unintended amplification,
stabilizes optimization, and preserves convergence guarantees. Experiments on
the challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN
demonstrate that our method maintains or improves test accuracy even under
strong generalization. Beyond practical performance, our study highlights the
importance of directly tracking gradient dynamics, aiming to bridge the gap
between theoretical expectations and empirical behaviors, and to provide
insights for future optimization research.

</details>


### [191] [A Comprehensive Review of Multi-Agent Reinforcement Learning in Video Games](https://arxiv.org/abs/2509.03682)
*Zhengyang Li,Qijin Ji,Xinghong Ling,Quan Liu*

Main category: cs.LG

TL;DR: MARL在电子游戏领域取得了显著进展，本文对其应用、挑战及未来方向进行了综述。


<details>
  <summary>Details</summary>
Motivation: MARL在现代电子游戏中的应用潜力日益显现，需要对其进行全面回顾和分析。

Method: 对MARL从回合制双人游戏到实时多人视频游戏的广泛应用进行了考察，分析了非平稳性、部分可观性、稀疏奖励、团队协调和可扩展性等关键挑战，并结合《火箭联盟》、《我的世界》、《雷神之锤III竞技场》、《星际争霸II》、《Dota 2》、《王者荣耀》等具体游戏案例进行了阐述。最后，提出了一个评估游戏复杂度的新方法，并指明了未来的研究方向。

Result: MARL在多种电子游戏中展现了超人的表现，例如AlphaStar和OpenAI Five。本文详细分析了MARL在不同类型游戏中的应用和面临的挑战。

Conclusion: MARL在电子游戏AI系统中具有重要意义，提出的新方法和对未来方向的建议将有助于推动该领域的进一步发展和创新。

Abstract: Recent advancements in multi-agent reinforcement learning (MARL) have
demonstrated its application potential in modern games. Beginning with
foundational work and progressing to landmark achievements such as AlphaStar in
StarCraft II and OpenAI Five in Dota 2, MARL has proven capable of achieving
superhuman performance across diverse game environments through techniques like
self-play, supervised learning, and deep reinforcement learning. With its
growing impact, a comprehensive review has become increasingly important in
this field. This paper aims to provide a thorough examination of MARL's
application from turn-based two-agent games to real-time multi-agent video
games including popular genres such as Sports games, First-Person Shooter (FPS)
games, Real-Time Strategy (RTS) games and Multiplayer Online Battle Arena
(MOBA) games. We further analyze critical challenges posed by MARL in video
games, including nonstationary, partial observability, sparse rewards, team
coordination, and scalability, and highlight successful implementations in
games like Rocket League, Minecraft, Quake III Arena, StarCraft II, Dota 2,
Honor of Kings, etc. This paper offers insights into MARL in video game AI
systems, proposes a novel method to estimate game complexity, and suggests
future research directions to advance MARL and its applications in game
development, inspiring further innovation in this rapidly evolving field.

</details>


### [192] [Sparse Autoencoder Neural Operators: Model Recovery in Function Spaces](https://arxiv.org/abs/2509.03738)
*Bahareh Tolooshams,Ailsa Shen,Anima Anandkumar*

Main category: cs.LG

TL;DR: 该研究提出了一种将神经模型中的表征统一问题框架化为稀疏模型恢复问题的方法，并介绍了一种将稀疏自编码器（SAE）扩展到提升空间和无限维函数空间的框架，以实现对大型神经算子（NO）的机械可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管Platonic表征假设表明神经网络在不同架构中会收敛到相似的表征，但神经算子的表征特性在科学计算中的重要性日益增加，却仍然未得到充分探索。

Method: 比较了SAE、提升SAE和SAE神经算子的推理和训练动态。

Result: 研究结果表明，提升和算子模块引入了有利的归纳偏置，能够实现更快的恢复、更好地恢复平滑概念，并在不同分辨率下实现鲁棒推理，这是神经算子特有的性质。

Conclusion: 提出的框架能够实现对大型神经算子的机械可解释性，并展示了提升和算子模块在提高恢复速度、平滑概念恢复和跨分辨率鲁棒性方面的优势。

Abstract: We frame the problem of unifying representations in neural models as one of
sparse model recovery and introduce a framework that extends sparse
autoencoders (SAEs) to lifted spaces and infinite-dimensional function spaces,
enabling mechanistic interpretability of large neural operators (NO). While the
Platonic Representation Hypothesis suggests that neural networks converge to
similar representations across architectures, the representational properties
of neural operators remain underexplored despite their growing importance in
scientific computing. We compare the inference and training dynamics of SAEs,
lifted-SAE, and SAE neural operators. We highlight how lifting and operator
modules introduce beneficial inductive biases, enabling faster recovery,
improved recovery of smooth concepts, and robust inference across varying
resolutions, a property unique to neural operators.

</details>


### [193] [Graph Random Features for Scalable Gaussian Processes](https://arxiv.org/abs/2509.03691)
*Matthew Zhang,Jihao Andreas Lin,Adrian Weller,Richard E. Turner,Isaac Reid*

Main category: cs.LG

TL;DR: GRFs 是一种用于离散输入空间的图核的随机估计器，可实现具有 N^(3/2) 时间复杂度的可扩展高斯过程，从而在具有超过 10^6 个节点的图上实现贝叶斯优化。


<details>
  <summary>Details</summary>
Motivation: 研究图随机特征 (GRFs) 在离散输入空间的可扩展高斯过程中的应用。

Method: 将 GRFs 应用于离散输入空间的图核，并证明了其具有 N^(3/2) 的时间复杂度，而精确核的时间复杂度为 N^3。

Result: 在具有超过 10^6 个节点的图上实现了具有竞争力的性能，并大幅提高了运行时间和内存节省。

Conclusion: GRFs 能够实现大规模图上的贝叶斯优化，同时保持了具有竞争力的性能。

Abstract: We study the application of graph random features (GRFs) - a recently
introduced stochastic estimator of graph node kernels - to scalable Gaussian
processes on discrete input spaces. We prove that (under mild assumptions)
Bayesian inference with GRFs enjoys $O(N^{3/2})$ time complexity with respect
to the number of nodes $N$, compared to $O(N^3)$ for exact kernels. Substantial
wall-clock speedups and memory savings unlock Bayesian optimisation on graphs
with over $10^6$ nodes on a single computer chip, whilst preserving competitive
performance.

</details>


### [194] [Hierarchical Federated Foundation Models over Wireless Networks for Multi-Modal Multi-Task Intelligence: Integration of Edge Learning with D2D/P2P-Enabled Fog Learning Architectures](https://arxiv.org/abs/2509.03695)
*Payam Abdisarabshali,Fardis Nadimi,Kasra Borazjani,Naji Khosravan,Minghui Liwang,Wei Ni,Dusit Niyato,Michael Langberg,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: 本文提出了分层联邦基础模型（HF-FMs），以解决多模态多任务联邦基础模型（M3T FFMs）在雾/边缘网络中面临的模态和任务异构性问题。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型（FM）的发展，利用地理分布式数据变得越来越重要，催生了联邦基础模型（FFM）。最近，多模态多任务（M3T）FM（如GPT-4）能够处理多种模态和任务，这促使了M3T FFM这一新的研究方向。然而，现有的M3T FFM在雾/边缘网络中面临模态和任务的异构性挑战，这在本文中被揭示并解决。

Method: 本文提出了分层联邦基础模型（HF-FMs），该模型将M3T FM的模块化结构（包括模态编码器、提示、专家混合（MoEs）、适配器和任务头）与雾/边缘基础设施的分层性质相结合。HF-FMs还支持设备到设备（D2D）通信，以实现节点间的水平模块中继和本地化协作训练。文章深入探讨了HF-FMs的架构设计。

Result: 通过架构设计，文章强调了HF-FMs的独特能力，并提出了一系列未来研究方向。此外，文章还通过在无线网络环境中原型化HF-FMs来展示其潜力，并公开了相关开源代码。

Conclusion: HF-FMs通过解决模态和任务异构性问题，为M3T FFM在雾/边缘网络中的应用开辟了新的途径，并为该领域的研究提供了基础和方向。

Abstract: The rise of foundation models (FMs) has reshaped the landscape of machine
learning. As these models continued to grow, leveraging geo-distributed data
from wireless devices has become increasingly critical, giving rise to
federated foundation models (FFMs). More recently, FMs have evolved into
multi-modal multi-task (M3T) FMs (e.g., GPT-4) capable of processing diverse
modalities across multiple tasks, which motivates a new underexplored paradigm:
M3T FFMs. In this paper, we unveil an unexplored variation of M3T FFMs by
proposing hierarchical federated foundation models (HF-FMs), which in turn
expose two overlooked heterogeneity dimensions to fog/edge networks that have a
direct impact on these emerging models: (i) heterogeneity in collected
modalities and (ii) heterogeneity in executed tasks across fog/edge nodes.
HF-FMs strategically align the modular structure of M3T FMs, comprising
modality encoders, prompts, mixture-of-experts (MoEs), adapters, and task
heads, with the hierarchical nature of fog/edge infrastructures. Moreover,
HF-FMs enable the optional usage of device-to-device (D2D) communications,
enabling horizontal module relaying and localized cooperative training among
nodes when feasible. Through delving into the architectural design of HF-FMs,
we highlight their unique capabilities along with a series of tailored future
research directions. Finally, to demonstrate their potential, we prototype
HF-FMs in a wireless network setting and release the open-source code for the
development of HF-FMs with the goal of fostering exploration in this untapped
field (GitHub: https://github.com/payamsiabd/M3T-FFM).

</details>


### [195] [EmbedOR: Provable Cluster-Preserving Visualizations with Curvature-Based Stochastic Neighbor Embeddings](https://arxiv.org/abs/2509.03703)
*Tristan Luca Saidi,Abigail Hickok,Bastian Rieck,Andrew J. Blumberg*

Main category: cs.LG

TL;DR: UMAP和tSNE等随机邻域嵌入（SNE）算法在可视化高维数据时存在不足，可能导致连通组件分离或无法识别聚类。本文提出EmbedOR算法，通过引入离散图曲率，使用曲率增强的距离度量来嵌入数据，以更好地保留数据结构和聚类。


<details>
  <summary>Details</summary>
Motivation: 现有的SNE算法（如UMAP和tSNE）在可视化高维数据时，往往无法保持数据的几何结构，容易错误地分离连通组件，或在可聚类的数据中无法找到聚类。

Method: 提出EmbedOR算法，该算法结合离散图曲率，使用曲率增强的距离度量来嵌入数据，以强调潜在的聚类结构。

Result: EmbedOR算法在合成数据和真实数据上的实验表明，与其他SNE算法和UMAP相比，EmbedOR能更好地保留数据的几何结构，并且不易将连续的高密度区域碎片化。此外，EmbedOR的距离度量还可以用于标注现有可视化，识别碎片化问题，并提供对数据底层几何的深入见解。

Conclusion: EmbedOR算法通过引入离散图曲率，克服了现有SNE算法在数据可视化和几何结构保持方面的局限性，能够更准确地展示数据的聚类结构和整体几何形态。

Abstract: Stochastic Neighbor Embedding (SNE) algorithms like UMAP and tSNE often
produce visualizations that do not preserve the geometry of noisy and high
dimensional data. In particular, they can spuriously separate connected
components of the underlying data submanifold and can fail to find clusters in
well-clusterable data. To address these limitations, we propose EmbedOR, a SNE
algorithm that incorporates discrete graph curvature. Our algorithm
stochastically embeds the data using a curvature-enhanced distance metric that
emphasizes underlying cluster structure. Critically, we prove that the EmbedOR
distance metric extends consistency results for tSNE to a much broader class of
datasets. We also describe extensive experiments on synthetic and real data
that demonstrate the visualization and geometry-preservation capabilities of
EmbedOR. We find that, unlike other SNE algorithms and UMAP, EmbedOR is much
less likely to fragment continuous, high-density regions of the data. Finally,
we demonstrate that the EmbedOR distance metric can be used as a tool to
annotate existing visualizations to identify fragmentation and provide deeper
insight into the underlying geometry of the data.

</details>


### [196] [Online Learning of Optimal Sequential Testing Policies](https://arxiv.org/abs/2509.03707)
*Qiyuan Chen,Raed Al Kontar*

Main category: cs.LG

TL;DR: 该论文研究了一个在线学习问题，旨在为一系列受试者寻找最优测试策略。虽然进行所有候选测试可以提供更多信息，但在测试相关且成本高昂的情况下，通常需要根据部分信息做出决策。如果联合分布已知，该问题可以被视为一个马尔可夫决策过程（MDP），但实际上该分布未知，需要在线学习。当受试者未接受全部测试时，由此产生的缺失数据可能导致估计偏差，使问题比标准的经验性MDP更难。论文证明了最小最大遗憾至少与 T^(2/3) 成正比，这与经验性MDP的Θ(√T)速率形成对比，揭示了缺失性引入的难度。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于解决在线测试问题（OTP），这是一个在序列测试中存在缺失数据和相关测试成本的在线学习问题。虽然完整的测试可以提供更多信息，但成本和相关性使得基于部分信息进行决策成为必要。然而，缺失数据可能导致估计偏差，增加了解决问题的难度。

Method: 论文提出了一种探索-然后-承诺（Explore-Then-Commit）算法，该算法在离散和高斯分布下均能达到 T^(2/3) 的累积遗憾。此外，为了突出缺失数据对奖励的影响，论文还研究了一个名为在线成本敏感最大熵采样问题（Online Cost-sensitive Maximum Entropy Sampling Problem）的变体，其中奖励与缺失数据无关。该变体采用了一种迭代消除算法，实现了 √T 的遗憾。

Result: 论文证明了在线测试问题（OTP）的最小最大遗憾至少与 T^(2/3) 成正比，并通过探索-然后-承诺算法匹配了这个下界。对于奖励与缺失数据无关的在线成本敏感最大熵采样问题，该算法实现了 √T 的遗憾。

Conclusion: 该研究加深了对缺失数据情况下的探索-利用权衡的理解，并为设计高效的顺序测试策略提供了指导。论文的研究结果表明，缺失数据会显著增加在线学习问题的难度，并且可以设计出针对特定问题的算法来优化测试策略。

Abstract: This paper studies an online learning problem that seeks optimal testing
policies for a stream of subjects, each of whom can be evaluated through a
sequence of candidate tests drawn from a common pool. We refer to this problem
as the Online Testing Problem (OTP). Although conducting every candidate test
for a subject provides more information, it is often preferable to select only
a subset when tests are correlated and costly, and make decisions with partial
information. If the joint distribution of test outcomes were known, the problem
could be cast as a Markov Decision Process (MDP) and solved exactly. In
practice, this distribution is unknown and must be learned online as subjects
are tested. When a subject is not fully tested, the resulting missing data can
bias estimates, making the problem fundamentally harder than standard episodic
MDPs. We prove that the minimax regret must scale at least as
$\Omega(T^{\frac{2}{3}})$, in contrast to the $\Theta(\sqrt{T})$ rate in
episodic MDPs, revealing the difficulty introduced by missingness. This
elevated lower bound is then matched by an Explore-Then-Commit algorithm whose
cumulative regret is $\tilde{O}(T^{\frac{2}{3}})$ for both discrete and
Gaussian distributions. To highlight the consequence of missingness-dependent
rewards in OTP, we study a variant called the Online Cost-sensitive Maximum
Entropy Sampling Problem, where rewards are independent of missing data. This
structure enables an iterative-elimination algorithm that achieves
$\tilde{O}(\sqrt{T})$ regret, breaking the $\Omega(T^{\frac{2}{3}})$ lower
bound for OTP. Numerical results confirm our theory in both settings. Overall,
this work deepens the understanding of the exploration--exploitation trade-off
under missing data and guides the design of efficient sequential testing
policies.

</details>


### [197] [From Federated Learning to $\mathbb{X}$-Learning: Breaking the Barriers of Decentrality Through Random Walks](https://arxiv.org/abs/2509.03709)
*Allan Salihovic,Payam Abdisarabshali,Michael Langberg,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: $\\mathbb{X}$-Learning ($\\mathbb{X}$L) 是一种新颖的分布式学习架构，它推广并扩展了去中心化的概念。本文旨在提出 $\\mathbb{X}$L 的愿景，介绍其未被探索的设计考量和自由度，并阐述 $\\mathbb{X}$L 与图论、马尔可夫链之间的联系，最后提出开放性研究方向以激励未来的研究。


<details>
  <summary>Details</summary>
Motivation: 提出 $\\mathbb{X}$-Learning ($\\mathbb{X}$L) 的愿景，介绍其未被探索的设计考量和自由度。

Method: 阐述 $\\mathbb{X}$L 与图论、马尔可夫链之间的联系，并提出开放性研究方向。

Result: 未在摘要中具体说明。

Conclusion: 未在摘要中具体说明。

Abstract: We provide our perspective on $\mathbb{X}$-Learning ($\mathbb{X}$L), a novel
distributed learning architecture that generalizes and extends the concept of
decentralization. Our goal is to present a vision for $\mathbb{X}$L,
introducing its unexplored design considerations and degrees of freedom. To
this end, we shed light on the intuitive yet non-trivial connections between
$\mathbb{X}$L, graph theory, and Markov chains. We also present a series of
open research directions to stimulate further research.

</details>


### [198] [Differentiable Entropy Regularization for Geometry and Neural Networks](https://arxiv.org/abs/2509.03733)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 我们提出了一个可微的范围划分熵估计器，并设计了EntropyNet，将数据重塑为低熵形式以加速下游算法。该方法在计算几何和深度学习中均提高了效率，并能在Transformer注意力中诱导结构化模式。


<details>
  <summary>Details</summary>
Motivation: 尽管范围划分熵在算法设计中提供了强大的保证，但它尚未被深度学习所应用。本研究旨在弥合这一差距。

Method: 提出第一个可微的范围划分熵近似方法，设计了名为EntropyNet的神经网络模块，并将熵正则化应用于Transformer注意力。

Result: 在计算几何任务中，我们的方法实现了高达4.1倍的运行时加速，误差可忽略不计（<0.2%）。在深度学习任务中，与L1基线相比，在80%稀疏度下，该方法产生了6%的更高准确度。

Conclusion: 可微熵有望成为一种实用的自适应学习、提高效率和结构化表示的机制。

Abstract: We introduce a differentiable estimator of range-partition entropy, a recent
concept from computational geometry that enables algorithms to adapt to the
"sortedness" of their input. While range-partition entropy provides strong
guarantees in algorithm design, it has not yet been made accessible to deep
learning. In this work, we (i) propose the first differentiable approximation
of range-partition entropy, enabling its use as a trainable loss or
regularizer; (ii) design EntropyNet, a neural module that restructures data
into low-entropy forms to accelerate downstream instance-optimal algorithms;
and (iii) extend this principle beyond geometry by applying entropy
regularization directly to Transformer attention. Across tasks, we demonstrate
that differentiable entropy improves efficiency without degrading correctness:
in geometry, our method achieves up to $4.1\times$ runtime speedups with
negligible error ($<0.2%$); in deep learning, it induces structured attention
patterns that yield 6% higher accuracy at 80% sparsity compared to L1
baselines. Our theoretical analysis provides approximation bounds for the
estimator, and extensive ablations validate design choices. These results
suggest that entropy-bounded computation is not only theoretically elegant but
also a practical mechanism for adaptive learning, efficiency, and structured
representation.

</details>


### [199] [Mapping on a Budget: Optimizing Spatial Data Collection for ML](https://arxiv.org/abs/2509.03749)
*Livia Betti,Farooq Sanni,Gnouyaro Sogoyou,Togbe Agbagla,Cullen Molitor,Tamma Carleton,Esther Rolf*

Main category: cs.LG

TL;DR: 机器学习受卫星图像标签数据稀疏性的限制，本研究提出了优化空间训练数据采集的方法，以应对数据稀疏、采集成本异构和预算限制等问题，并在实验中展示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 机器学习（SatML）在农业、生态学和人类发展等领域的应用受到训练数据稀疏性的限制。尽管卫星数据覆盖全球，但SatML的标签训练数据集通常很小、空间聚集且为其他目的收集，导致科学家和政策制定者在何时以及如何收集额外数据以最大化性能方面存在不确定性。

Method: 提出了一种新的问题形式，用于在数据采集成本异构和现实预算限制下优化空间训练数据。开发了解决该问题的新方法。

Result: 在模拟的、跨越三大洲和四个任务的实验中，所提出的策略显示出样本优化带来的显著收益。进一步的实验明确了优化采样特别有效的场景。

Conclusion: 本研究提出的问题形式和方法旨在推广到SatML的各个应用领域，并强调了其在多哥农业监测中增强农业调查数据的实用性。

Abstract: In applications across agriculture, ecology, and human development, machine
learning with satellite imagery (SatML) is limited by the sparsity of labeled
training data. While satellite data cover the globe, labeled training datasets
for SatML are often small, spatially clustered, and collected for other
purposes (e.g., administrative surveys or field measurements). Despite the
pervasiveness of this issue in practice, past SatML research has largely
focused on new model architectures and training algorithms to handle scarce
training data, rather than modeling data conditions directly. This leaves
scientists and policymakers who wish to use SatML for large-scale monitoring
uncertain about whether and how to collect additional data to maximize
performance. Here, we present the first problem formulation for the
optimization of spatial training data in the presence of heterogeneous data
collection costs and realistic budget constraints, as well as novel methods for
addressing this problem. In experiments simulating different problem settings
across three continents and four tasks, our strategies reveal substantial gains
from sample optimization. Further experiments delineate settings for which
optimized sampling is particularly effective. The problem formulation and
methods we introduce are designed to generalize across application domains for
SatML; we put special emphasis on a specific problem setting where our
coauthors can immediately use our findings to augment clustered agricultural
surveys for SatML monitoring in Togo.

</details>


### [200] [What Fundamental Structure in Reward Functions Enables Efficient Sparse-Reward Learning?](https://arxiv.org/abs/2509.03790)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 低秩结构是稀疏奖励强化学习的关键，PAMC算法在此基础上实现了样本效率的显著提升。


<details>
  <summary>Details</summary>
Motivation: 探讨低秩结构如何使稀疏奖励强化学习的样本复杂度从指数级下降到多项式级。

Method: 提出策略感知矩阵补全（PAMC）算法，结合了矩阵补全理论和强化学习，通过策略相关采样进行分析，并提供四项主要贡献：不可行性结果、免奖励表示学习、共形预测置信集以及鲁棒性保证。

Result: 在100个领域中，超过一半的领域存在可利用的结构。PAMC算法相比现有基线，样本效率提升1.6到2.1倍，计算开销仅增加约20%。

Conclusion: 结构化奖励学习是一个有前景的新范式，对机器人、医疗和安全关键、样本昂贵的应用具有重要意义。

Abstract: What fundamental properties of reward functions enable efficient
sparse-reward reinforcement learning? We address this question through the lens
of low-rank structure in reward matrices, showing that such structure induces a
sharp transition from exponential to polynomial sample complexity, the first
result of this kind for sparse-reward RL. We introduce Policy-Aware Matrix
Completion (PAMC), which connects matrix completion theory with reinforcement
learning via a new analysis of policy-dependent sampling. Our framework
provides: (i) impossibility results for general sparse reward observation, (ii)
reward-free representation learning from dynamics, (iii) distribution-free
confidence sets via conformal prediction, and (iv) robust completion guarantees
that degrade gracefully when low-rank structure is only approximate.
Empirically, we conduct a pre-registered evaluation across 100 systematically
sampled domains, finding exploitable structure in over half. PAMC improves
sample efficiency by factors between 1.6 and 2.1 compared to strong
exploration, structured, and representation-learning baselines, while adding
only about 20 percent computational overhead.These results establish structural
reward learning as a promising new paradigm, with immediate implications for
robotics, healthcare, and other safety-critical, sample-expensive applications.

</details>


### [201] [Online time series prediction using feature adjustment](https://arxiv.org/abs/2509.03810)
*Xiannan Huang,Shuhan Qiu,Jiayuan Du,Chao Yang*

Main category: cs.LG

TL;DR: ADAPT-Z是一种新颖的时间序列在线学习方法，通过更新潜在因子的特征表示来解决分布偏移问题，并采用一种新的机制来处理多步预测中的延迟反馈问题，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列在线学习面临分布偏移的挑战，尤其是在线部署场景下，模型需要持续适应不断变化的模式。现有方法主要关注参数选择和更新策略，但本文认为分布偏移源于潜在因子的变化，更新这些因子的特征表示可能更有效。

Method: 提出ADAPT-Z（Automatic Delta Adjustment via Persistent Tracking in Z-space）方法，利用适配器模块，结合当前特征表示和历史梯度信息，来更新潜在因子的表示，以处理多步预测中的延迟反馈问题。

Result: 在多个数据集上的广泛实验表明，ADAPT-Z持续优于不进行适应的标准基线模型，并超越了最先进的在线学习方法。

Conclusion: ADAPT-Z通过更新潜在因子的特征表示，并有效处理延迟反馈问题，为时间序列在线学习提供了更优的解决方案。

Abstract: Time series forecasting is of significant importance across various domains.
However, it faces significant challenges due to distribution shift. This issue
becomes particularly pronounced in online deployment scenarios where data
arrives sequentially, requiring models to adapt continually to evolving
patterns. Current time series online learning methods focus on two main
aspects: selecting suitable parameters to update (e.g., final layer weights or
adapter modules) and devising suitable update strategies (e.g., using recent
batches, replay buffers, or averaged gradients). We challenge the conventional
parameter selection approach, proposing that distribution shifts stem from
changes in underlying latent factors influencing the data. Consequently,
updating the feature representations of these latent factors may be more
effective. To address the critical problem of delayed feedback in multi-step
forecasting (where true values arrive much later than predictions), we
introduce ADAPT-Z (Automatic Delta Adjustment via Persistent Tracking in
Z-space). ADAPT-Z utilizes an adapter module that leverages current feature
representations combined with historical gradient information to enable robust
parameter updates despite the delay. Extensive experiments demonstrate that our
method consistently outperforms standard base models without adaptation and
surpasses state-of-the-art online learning approaches across multiple datasets.
The code is available at https://github.com/xiannanhuang/ADAPT-Z.

</details>


### [202] [Machine Learning for LiDAR-Based Indoor Surface Classification in Intelligent Wireless Environments](https://arxiv.org/abs/2509.03813)
*Parth Ashokbhai Shiroya,Swarnagowri Shashidhar,Amod Ashtekar,Krishna Aindrila Kar,Rafaela Lomboy,Dalton Davis,Mohammed E. Eltayeb*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reliable connectivity in millimeter-wave (mmWave) and sub-terahertz (sub-THz)
networks depends on reflections from surrounding surfaces, as high-frequency
signals are highly vulnerable to blockage. The scattering behavior of a surface
is determined not only by material permittivity but also by roughness, which
governs whether energy remains in the specular direction or is diffusely
scattered. This paper presents a LiDAR-driven machine learning framework for
classifying indoor surfaces into semi-specular and low-specular categories,
using optical reflectivity as a proxy for electromagnetic scattering behavior.
A dataset of over 78,000 points from 15 representative indoor materials was
collected and partitioned into 3 cm x 3 cm patches to enable classification
from partial views. Patch-level features capturing geometry and intensity,
including elevation angle, natural-log-scaled intensity, and max-to-mean ratio,
were extracted and used to train Random Forest, XGBoost, and neural network
classifiers. Results show that ensemble tree-based models consistently provide
the best trade-off between accuracy and robustness, confirming that
LiDAR-derived features capture roughness-induced scattering effects. The
proposed framework enables the generation of scatter aware environment maps and
digital twins, supporting adaptive beam management, blockage recovery, and
environment-aware connectivity in next-generation networks.

</details>


### [203] [Predicting Traffic Accident Severity with Deep Neural Networks](https://arxiv.org/abs/2509.03819)
*Meghan Bibb,Pablo Rivas,Mahee Tayba*

Main category: cs.LG

TL;DR: 深度学习模型在交通事故严重性分类中达到92%的准确率。


<details>
  <summary>Details</summary>
Motivation: 利用机器学习（特别是深度学习）研究交通事故数据，以降低未来事故风险。

Method: 分析特征共线性，使用自编码器进行降维，并训练深度神经网络模型来分类事故严重性。

Result: 所提出的深度神经网络模型在事故严重性分类中达到了高达92%的交叉验证准确率。

Conclusion: 深度神经网络是处理交通事故数据和分类事故严重性的有效方法。

Abstract: Traffic accidents can be studied to mitigate the risk of further events.
Recent advances in machine learning have provided an alternative way to study
data associated with traffic accidents. New models achieve good generalization
and high predictive power over imbalanced data. In this research, we study
neural network-based models on data related to traffic accidents. We begin
analyzing relative feature colinearity and unsupervised dimensionality
reduction through autoencoders, followed by a dense network. The features are
related to traffic accident data and the target is to classify accident
severity. Our experiments show cross-validated results of up to 92% accuracy
when classifying accident severity using the proposed deep neural network.

</details>


### [204] [Vehicle-to-Infrastructure Collaborative Spatial Perception via Multimodal Large Language Models](https://arxiv.org/abs/2509.03837)
*Kimia Ehsani,Walid Saad*

Main category: cs.LG

TL;DR: 该研究提出了一种新颖的鸟瞰图（BEV）注入连接器，用于增强多模态大语言模型（MLLM）在车辆到基础设施（V2I）通信链路质量预测方面的能力。通过融合来自邻近车辆的传感器数据构建BEV，为MLLM提供空间上下文，克服了其固有的三维空间理解局限性。结合CARLA和MATLAB进行联合仿真，生成了多样化的数据。实验证明，该方法在LoS/NLoS分类、链路可用性和阻塞预测等任务上均取得了显著性能提升，特别是在恶劣天气和夜间条件下，性能增益尤为明显，证明了该框架在复杂环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 车辆到基础设施（V2I）系统需要准确预测通信链路质量，以实现顺畅切换、有效波束管理和可靠的低延迟通信。现代车辆日益增长的传感器数据为利用多模态大语言模型（MLLM）提供了契机，但MLLM缺乏三维空间理解能力。

Method: 提出了一种轻量级、即插即用的鸟瞰图（BEV）注入连接器。通过收集邻近车辆的传感数据构建环境的BEV，并将其与主车输入融合，为大语言模型提供空间上下文。开发了一个结合CARLA和MATLAB的联合仿真环境，用于生成不同场景下的RGB、LiDAR、GPS和无线信号数据，并从射线追踪输出中提取指令和真实响应。

Result: 在三种V2I链路预测任务（视线（LoS）与非视线（NLoS）分类、链路可用性和阻塞预测）上进行了广泛的实验。仿真结果表明，所提出的BEV注入框架在所有任务上的性能均得到持续提升。与仅使用主车信息的基线相比，该方法将准确性指标的宏平均值提高了高达13.9%。在具有挑战性的雨天和夜间条件下，性能增益高达32.7%，证明了该框架在恶劣条件下的鲁棒性。

Conclusion: 所提出的BEV注入框架能够有效增强MLLM在V2I通信链路质量预测中的空间理解能力，显著提高了预测精度和鲁棒性，尤其是在通信条件恶劣的情况下。

Abstract: Accurate prediction of communication link quality metrics is essential for
vehicle-to-infrastructure (V2I) systems, enabling smooth handovers, efficient
beam management, and reliable low-latency communication. The increasing
availability of sensor data from modern vehicles motivates the use of
multimodal large language models (MLLMs) because of their adaptability across
tasks and reasoning capabilities. However, MLLMs inherently lack
three-dimensional spatial understanding. To overcome this limitation, a
lightweight, plug-and-play bird's-eye view (BEV) injection connector is
proposed. In this framework, a BEV of the environment is constructed by
collecting sensing data from neighboring vehicles. This BEV representation is
then fused with the ego vehicle's input to provide spatial context for the
large language model. To support realistic multimodal learning, a co-simulation
environment combining CARLA simulator and MATLAB-based ray tracing is developed
to generate RGB, LiDAR, GPS, and wireless signal data across varied scenarios.
Instructions and ground-truth responses are programmatically extracted from the
ray-tracing outputs. Extensive experiments are conducted across three V2I link
prediction tasks: line-of-sight (LoS) versus non-line-of-sight (NLoS)
classification, link availability, and blockage prediction. Simulation results
show that the proposed BEV injection framework consistently improved
performance across all tasks. The results indicate that, compared to an
ego-only baseline, the proposed approach improves the macro-average of the
accuracy metrics by up to 13.9%. The results also show that this performance
gain increases by up to 32.7% under challenging rainy and nighttime conditions,
confirming the robustness of the framework in adverse settings.

</details>


### [205] [Data-Augmented Quantization-Aware Knowledge Distillation](https://arxiv.org/abs/2509.03850)
*Justin Kur,Kaiqi Zhao*

Main category: cs.LG

TL;DR: 研究如何选择合适的数据增强方法以提升低比特量化感知知识蒸馏模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注改进知识蒸馏损失函数或优化量化感知训练的前向和后向传播，而较少关注输入变换（如数据增强）对量化模型性能的影响，量化感知知识蒸馏与数据增强之间的关系尚未被探索。

Method: 提出了一种新颖的评估指标，该指标根据数据增强方法最大化上下文互信息（与图像标签无直接关系的信息）的能力，同时确保各类别预测值与真实标签的平均值接近，从而自动选择数据增强策略。

Result: 通过在不同模型架构和数据集上的广泛评估，证明了使用该指标选择数据增强策略能够显著提升最先进的量化感知训练和知识蒸馏方法的性能。

Conclusion: 所提出的数据增强选择方法能够有效提升低比特量化感知知识蒸馏模型的性能，且该方法兼容任何知识蒸馏或量化感知训练算法，并且具有最小的训练开销。

Abstract: Quantization-aware training (QAT) and Knowledge Distillation (KD) are
combined to achieve competitive performance in creating low-bit deep learning
models. Existing KD and QAT works focus on improving the accuracy of quantized
models from the network output perspective by designing better KD loss
functions or optimizing QAT's forward and backward propagation. However,
limited attention has been given to understanding the impact of input
transformations, such as data augmentation (DA). The relationship between
quantization-aware KD and DA remains unexplored. In this paper, we address the
question: how to select a good DA in quantization-aware KD, especially for the
models with low precisions? We propose a novel metric which evaluates DAs
according to their capacity to maximize the Contextual Mutual Information--the
information not directly related to an image's label--while also ensuring the
predictions for each class are close to the ground truth labels on average. The
proposed method automatically ranks and selects DAs, requiring minimal training
overhead, and it is compatible with any KD or QAT algorithm. Extensive
evaluations demonstrate that selecting DA strategies using our metric
significantly improves state-of-the-art QAT and KD works across various model
architectures and datasets.

</details>


### [206] [MillGNN: Learning Multi-Scale Lead-Lag Dependencies for Multi-Variate Time Series Forecasting](https://arxiv.org/abs/2509.03852)
*Binqing Wu,Zongjiang Shang,Jianlong Huang,Ling Chen*

Main category: cs.LG

TL;DR: MillGNN是一种新的图神经网络模型，用于多变量时间序列预测，通过学习多尺度分组的滞后依赖性来捕捉分层滞后效应，在11个数据集的实验中表现优于16种现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉变量内和变量间依赖性方面取得了进展，但忽略了多分组尺度的滞后依赖性，未能捕捉复杂系统中的分层滞后效应。

Method: MillGNN提出了一种新的图神经网络方法，包含两个关键创新：1）一个特定尺度的滞后图学习模块，整合了互相关系数和动态衰减特征来学习每个尺度的滞后依赖性；2）一个分层滞后消息传递模块，以结构化的方式在多个分组尺度上传递滞后消息，以捕捉多尺度滞后效应。

Result: 在11个数据集上的实验结果表明，MillGNN在长期和短期多变量时间序列预测方面优于16种最先进的方法。

Conclusion: MillGNN能够全面捕捉滞后效应，同时考虑变量和分组的动态和衰减，并在各种多变量时间序列预测任务中展现出优越性。

Abstract: Multi-variate time series (MTS) forecasting is crucial for various
applications. Existing methods have shown promising results owing to their
strong ability to capture intra- and inter-variate dependencies. However, these
methods often overlook lead-lag dependencies at multiple grouping scales,
failing to capture hierarchical lead-lag effects in complex systems. To this
end, we propose MillGNN, a novel \underline{g}raph \underline{n}eural
\underline{n}etwork-based method that learns \underline{m}ult\underline{i}ple
grouping scale \underline{l}ead-\underline{l}ag dependencies for MTS
forecasting, which can comprehensively capture lead-lag effects considering
variate-wise and group-wise dynamics and decays. Specifically, MillGNN
introduces two key innovations: (1) a scale-specific lead-lag graph learning
module that integrates cross-correlation coefficients and dynamic decaying
features derived from real-time inputs and time lags to learn lead-lag
dependencies for each scale, which can model evolving lead-lag dependencies
with statistical interpretability and data-driven flexibility; (2) a
hierarchical lead-lag message passing module that passes lead-lag messages at
multiple grouping scales in a structured way to simultaneously propagate intra-
and inter-scale lead-lag effects, which can capture multi-scale lead-lag
effects with a balance of comprehensiveness and efficiency. Experimental
results on 11 datasets demonstrate the superiority of MillGNN for long-term and
short-term MTS forecasting, compared with 16 state-of-the-art methods.

</details>


### [207] [Peptidomic-Based Prediction Model for Coronary Heart Disease Using a Multilayer Perceptron Neural Network](https://arxiv.org/abs/2509.03884)
*Jesus Celis-Porras*

Main category: cs.LG

TL;DR: 开发了一种基于多层感知器（MLP）神经网络的非侵入性冠心病（CHD）诊断模型，使用遗传算法筛选的50个尿液肽生物标志物进行训练，并利用SMOTE技术平衡了数据集，取得了95.67%的精确率、灵敏度和特异性，以及0.9748的AUC值，证明了其在检测CHD方面的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 冠心病（CHD）是全球主要的死亡原因，并显著增加了医疗保健支出，因此需要开发一种非侵入性的诊断方法。

Method: 设计了一个基于多层感知器（MLP）神经网络的模型，使用遗传算法筛选的50个关键尿液肽生物标志物进行训练，并采用SMOTE技术平衡了治疗组和对照组（各345人）的数据集，使用具有三层隐藏层（每层60个神经元）和两层输出层的网络，并采用分层验证策略进行训练。

Result: 该模型实现了95.67%的精确率、灵敏度和特异性，F1分数达到0.9565，ROC曲线下面积（AUC）为0.9748，马修斯相关系数（MCC）和Cohen的kappa系数分别为0.9134和0.9131。

Conclusion: 该模型为冠心病提供了一种高度准确且可靠的非侵入性诊断工具。

Abstract: Coronary heart disease (CHD) is a leading cause of death worldwide and
contributes significantly to annual healthcare expenditures. To develop a
non-invasive diagnostic approach, we designed a model based on a multilayer
perceptron (MLP) neural network, trained on 50 key urinary peptide biomarkers
selected via genetic algorithms. Treatment and control groups, each comprising
345 individuals, were balanced using the Synthetic Minority Over-sampling
Technique (SMOTE). The neural network was trained using a stratified validation
strategy. Using a network with three hidden layers of 60 neurons each and an
output layer of two neurons, the model achieved a precision, sensitivity, and
specificity of 95.67 percent, with an F1-score of 0.9565. The area under the
ROC curve (AUC) reached 0.9748 for both classes, while the Matthews correlation
coefficient (MCC) and Cohen's kappa coefficient were 0.9134 and 0.9131,
respectively, demonstrating its reliability in detecting CHD. These results
indicate that the model provides a highly accurate and robust non-invasive
diagnostic tool for coronary heart disease.

</details>


### [208] [Topotein: Topological Deep Learning for Protein Representation Learning](https://arxiv.org/abs/2509.03885)
*Zhiyu Wang,Arian Jamasb,Mustafa Hajij,Alex Morehead,Luke Braithwaite,Pietro Liò*

Main category: cs.LG

TL;DR: Topotein使用拓扑深度学习框架，通过蛋白质组合复合物(PCC)和拓扑完整感知机网络(TCPNet)，更有效地学习蛋白质的层级结构，并在多种蛋白质表示学习任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于序列和图的方法未能捕捉蛋白质结构固有的层级组织，而这种组织对于理解蛋白质结构-功能关系至关重要。

Method: 提出了一种名为Topotein的框架，它使用新颖的蛋白质组合复合物(PCC)和拓扑完整感知机网络(TCPNet)来进行拓扑深度学习。PCC在从残基到二级结构再到完整蛋白质的多个层级上表示蛋白质，并保留每个层级的几何信息。TCPNet在这些层级结构上采用SE(3)等变消息传递，以更有效地捕捉多尺度结构模式。

Result: 在四个蛋白质表示学习任务的广泛实验中，TCPNet持续优于最先进的几何图神经网络，尤其在需要理解二级结构排列的折叠分类等任务中表现出色。

Conclusion: 所提出的方法通过层级拓扑特征的整合，显著提高了蛋白质表示学习的性能，验证了其在蛋白质分析中的重要性。

Abstract: Protein representation learning (PRL) is crucial for understanding
structure-function relationships, yet current sequence- and graph-based methods
fail to capture the hierarchical organization inherent in protein structures.
We introduce Topotein, a comprehensive framework that applies topological deep
learning to PRL through the novel Protein Combinatorial Complex (PCC) and
Topology-Complete Perceptron Network (TCPNet). Our PCC represents proteins at
multiple hierarchical levels -- from residues to secondary structures to
complete proteins -- while preserving geometric information at each level.
TCPNet employs SE(3)-equivariant message passing across these hierarchical
structures, enabling more effective capture of multi-scale structural patterns.
Through extensive experiments on four PRL tasks, TCPNet consistently
outperforms state-of-the-art geometric graph neural networks. Our approach
demonstrates particular strength in tasks such as fold classification which
require understanding of secondary structure arrangements, validating the
importance of hierarchical topological features for protein analysis.

</details>


### [209] [Mistake-bounded online learning with operation caps](https://arxiv.org/abs/2509.03892)
*Jesse Geneson,Meien Li,Linus Tang*

Main category: cs.LG

TL;DR: 在线学习模型在算术运算次数受限的情况下，学习任意函数族所需的最小运算次数。


<details>
  <summary>Details</summary>
Motivation: 研究带算术运算次数上限的在线学习模型，解决agnostic mistake-bounded online learning with bandit feedback问题。

Method: 推导学习任意函数族所需的最小算术运算次数上限。

Result: 给出了算术运算次数受限情况下的agnostic mistake-bounded online learning with bandit feedback问题的解决方案。

Conclusion: 算术运算次数受限不影响在线学习模型的学习能力。

Abstract: We investigate the mistake-bound model of online learning with caps on the
number of arithmetic operations per round. We prove general bounds on the
minimum number of arithmetic operations per round that are necessary to learn
an arbitrary family of functions with finitely many mistakes. We solve a
problem on agnostic mistake-bounded online learning with bandit feedback from
(Filmus et al, 2024) and (Geneson \& Tang, 2024). We also extend this result to
the setting of operation caps.

</details>


### [210] [Formal Verification of Local Robustness of a Classification Algorithm for a Spatial Use Case](https://arxiv.org/abs/2509.03948)
*Delphine Longuet,Amira Elouazzani,Alejandro Penacho Riveiros,Nicola Bastianello*

Main category: cs.LG

TL;DR: 嵌入式混合人工智能系统可以通过形式验证来提高卫星部件故障检测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 卫星部件故障的修复成本高昂，且会消耗大量人力物力。在卫星上嵌入混合人工智能（AI）系统进行故障检测，可以实现早期检测，从而大大减轻这种负担。

Method: 使用形式验证工具Marabou来验证AI算法中使用的神经网络模型的局部鲁棒性。该工具可以量化模型输入在多大程度上可以被扰动而不导致其输出行为不稳定。

Result: 通过形式验证提高了AI系统的可靠性，增强了其在不确定性下的性能可信度。

Conclusion: 形式验证是确保嵌入式AI系统（如卫星故障检测系统）高可靠性的关键方法，可以提高其在实际应用中的可信度。

Abstract: Failures in satellite components are costly and challenging to address, often
requiring significant human and material resources. Embedding a hybrid AI-based
system for fault detection directly in the satellite can greatly reduce this
burden by allowing earlier detection. However, such systems must operate with
extremely high reliability. To ensure this level of dependability, we employ
the formal verification tool Marabou to verify the local robustness of the
neural network models used in the AI-based algorithm. This tool allows us to
quantify how much a model's input can be perturbed before its output behavior
becomes unstable, thereby improving trustworthiness with respect to its
performance under uncertainty.

</details>


### [211] [On Aligning Prediction Models with Clinical Experiential Learning: A Prostate Cancer Case Study](https://arxiv.org/abs/2509.04053)
*Jacqueline J. Vallon,William Overman,Wanqiao Xu,Neil Panjwani,Xi Ling,Sushmita Vij,Hilary P. Bagshaw,John T. Leppert,Sumit Shah,Geoffrey Sonn,Sandy Srinivas,Erqi Pollom,Mark K. Buyyounouski,Mohsen Bayati*

Main category: cs.LG

TL;DR: 现代机器学习模型在医疗保健中的应用日益广泛，但其预测模式可能与临床经验不符。本文提出一个框架，通过整合临床知识和约束来解决这种不一致性，并在前列腺癌案例研究中验证了该方法。结果表明，在不牺牲模型性能的情况下，可以使模型行为与临床经验保持一致。此外，通过一个涉及临床医生的随机实验，研究了反馈驱动的对齐方法在非生成式AI临床风险预测模型中的可行性，发现模型预测差异越大，临床解释上的差异就越明显。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型在医疗保健领域表现出色，但其预测模式有时与临床经验不符，导致模型行为与最终用户需求之间存在不匹配。例如，模型可能预测癌症分期与生存率之间存在非单调递减的关系，而忽略了其他因素。本研究旨在解决这种模型行为与临床经验学习之间不一致的问题。

Method: 本文提出一个可复现的框架，通过在机器学习模型中引入约束来整合临床知识，以解决模型行为与临床经验学习不一致的问题。在对前列腺癌预后进行预测的案例研究中，首先通过调查收集临床知识，并将其作为约束条件纳入机器学习模型。随后，分析不同程度的欠规定性对模型性能和行为的影响。此外，还研究了通过随机实验和临床医生反馈来改进非生成式AI临床风险预测模型的方法。

Result: 在对前列腺癌预后进行预测的案例研究中，通过将临床知识作为约束条件整合到机器学习模型中，实现了在不牺牲模型性能的情况下，使模型行为与临床经验学习保持一致。通过随机实验发现，当约束模型和非约束模型对患者的预测结果差异越大时，临床解释上的差异也越明显。

Conclusion: 本研究提出的框架能够有效地解决机器学习模型在医疗保健应用中的模型行为与临床经验学习不一致的问题，并且可以在不影响模型性能的前提下实现两者的一致性。此外，通过反馈驱动的对齐方法，可以提高临床医生对模型预测结果的理解和解释能力。

Abstract: Over the past decade, the use of machine learning (ML) models in healthcare
applications has rapidly increased. Despite high performance, modern ML models
do not always capture patterns the end user requires. For example, a model may
predict a non-monotonically decreasing relationship between cancer stage and
survival, keeping all other features fixed. In this paper, we present a
reproducible framework for investigating this misalignment between model
behavior and clinical experiential learning, focusing on the effects of
underspecification of modern ML pipelines. In a prostate cancer outcome
prediction case study, we first identify and address these inconsistencies by
incorporating clinical knowledge, collected by a survey, via constraints into
the ML model, and subsequently analyze the impact on model performance and
behavior across degrees of underspecification. The approach shows that aligning
the ML model with clinical experiential learning is possible without
compromising performance. Motivated by recent literature in generative AI, we
further examine the feasibility of a feedback-driven alignment approach in
non-generative AI clinical risk prediction models through a randomized
experiment with clinicians. Our findings illustrate that, by eliciting
clinicians' model preferences using our proposed methodology, the larger the
difference in how the constrained and unconstrained models make predictions for
a patient, the more apparent the difference is in clinical interpretation.

</details>


### [212] [FedQuad: Federated Stochastic Quadruplet Learning to Mitigate Data Heterogeneity](https://arxiv.org/abs/2509.04107)
*Ozgu Goksu,Nicolas Pugeault*

Main category: cs.LG

TL;DR: FedQuad通过优化类内方差和类间方差来解决联邦学习中的数据异质性问题，并在CIFAR-10和CIFAR-100数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能解决分布式数据和隐私保护问题，但在数据异质性，尤其是在数据集规模较小和类别不平衡的情况下，全局模型的泛化能力面临挑战。

Method: 提出了一种名为FedQuad的新方法，通过显式地优化客户端数据的类内方差和类间方差，减少模型聚合对全局模型的负面影响。该方法通过最小化相似样本对之间的距离并最大化负样本对之间的距离，在共享特征空间中有效地区分客户端数据。

Result: 在CIFAR-10和CIFAR-100数据集上，在各种数据分布和多客户端场景下，FedQuad的性能优于现有方法。

Conclusion: FedQuad有效解决了联邦学习中的数据异质性问题，并在指标学习策略在联邦学习中的应用提供了分析。

Abstract: Federated Learning (FL) provides decentralised model training, which
effectively tackles problems such as distributed data and privacy preservation.
However, the generalisation of global models frequently faces challenges from
data heterogeneity among clients. This challenge becomes even more pronounced
when datasets are limited in size and class imbalance. To address data
heterogeneity, we propose a novel method, \textit{FedQuad}, that explicitly
optimises smaller intra-class variance and larger inter-class variance across
clients, thereby decreasing the negative impact of model aggregation on the
global model over client representations. Our approach minimises the distance
between similar pairs while maximising the distance between negative pairs,
effectively disentangling client data in the shared feature space. We evaluate
our method on the CIFAR-10 and CIFAR-100 datasets under various data
distributions and with many clients, demonstrating superior performance
compared to existing approaches. Furthermore, we provide a detailed analysis of
metric learning-based strategies within both supervised and federated learning
paradigms, highlighting their efficacy in addressing representational learning
challenges in federated settings.

</details>


### [213] [Synthetic Counterfactual Labels for Efficient Conformal Counterfactual Inference](https://arxiv.org/abs/2509.04112)
*Amirmohammad Farzaneh,Matteo Zecchin,Osvaldo Simeone*

Main category: cs.LG

TL;DR: SP-CCI通过使用合成数据增强校准集，在保留边际覆盖率的同时，生成更窄的预测区间，解决了可靠反事实预测区间构建问题。


<details>
  <summary>Details</summary>
Motivation: 现有反事实因果推断（CCI）方法在处理治疗不平衡和反事实样本稀缺时，预测区间过于保守，无法满足可靠性需求。

Method: SP-CCI框架利用预训练的反事实模型生成的合成反事实标签来增强校准集，并结合基于风险控制预测集（RCPS）和预测驱动推断（PPI）的解偏步骤，以确保有效性。

Result: SP-CCI在理论上保证了在精确和近似重要性加权下，能生成更窄的预测区间并保持边际覆盖率。实验结果表明，SP-CCI在不同数据集上均能有效缩窄区间宽度。

Conclusion: SP-CCI是一种有效的方法，可以生成比标准CCI更窄、但同样可靠的反事实预测区间，尤其适用于治疗不平衡和样本稀疏的情况。

Abstract: This work addresses the problem of constructing reliable prediction intervals
for individual counterfactual outcomes. Existing conformal counterfactual
inference (CCI) methods provide marginal coverage guarantees but often produce
overly conservative intervals, particularly under treatment imbalance when
counterfactual samples are scarce. We introduce synthetic data-powered CCI
(SP-CCI), a new framework that augments the calibration set with synthetic
counterfactual labels generated by a pre-trained counterfactual model. To
ensure validity, SP-CCI incorporates synthetic samples into a conformal
calibration procedure based on risk-controlling prediction sets (RCPS) with a
debiasing step informed by prediction-powered inference (PPI). We prove that
SP-CCI achieves tighter prediction intervals while preserving marginal
coverage, with theoretical guarantees under both exact and approximate
importance weighting. Empirical results on different datasets confirm that
SP-CCI consistently reduces interval width compared to standard CCI across all
settings.

</details>


### [214] [Who Pays for Fairness? Rethinking Recourse under Social Burden](https://arxiv.org/abs/2509.04128)
*Ainhize Barrainkua,Giovanni De Toni,Jose Antonio Lozano,Novi Quadrianto*

Main category: cs.LG

TL;DR: 本研究旨在解决算法溯源中的公平性问题，并提出了一种新的基于社会负担的公平性框架和MISOB算法。


<details>
  <summary>Details</summary>
Motivation: 现有算法溯源研究未能充分保证公平性，并且即将出台的法规要求在负面决策中提供可行的补救措施。

Method: 提出了一种新的基于社会负担的公平性框架和一种名为MISOB的实用算法，并对标准等成本范式进行了理论分析。

Result: MISOB算法在真实数据集上展示了能够降低所有群体的社会负担，同时不损害整体分类器准确性。

Conclusion: MISOB算法为算法溯源提供了一种在现实条件下适用的、公平的解决方案。

Abstract: Machine learning based predictions are increasingly used in sensitive
decision-making applications that directly affect our lives. This has led to
extensive research into ensuring the fairness of classifiers. Beyond just fair
classification, emerging legislation now mandates that when a classifier
delivers a negative decision, it must also offer actionable steps an individual
can take to reverse that outcome. This concept is known as algorithmic
recourse. Nevertheless, many researchers have expressed concerns about the
fairness guarantees within the recourse process itself. In this work, we
provide a holistic theoretical characterization of unfairness in algorithmic
recourse, formally linking fairness guarantees in recourse and classification,
and highlighting limitations of the standard equal cost paradigm. We then
introduce a novel fairness framework based on social burden, along with a
practical algorithm (MISOB), broadly applicable under real-world conditions.
Empirical results on real-world datasets show that MISOB reduces the social
burden across all groups without compromising overall classifier accuracy.

</details>


### [215] [TAGAL: Tabular Data Generation using Agentic LLM Methods](https://arxiv.org/abs/2509.04152)
*Benoît Ronval,Pierre Dupont,Siegfried Nijssen*

Main category: cs.LG

TL;DR: TAGAL是一种利用大型语言模型（LLMs）和代理工作流生成合成表格数据的创新方法，无需进一步的LLM训练即可通过反馈改进数据，并能整合外部知识。实验证明，TAGAL在下游ML模型效用和数据相似性方面表现优于许多现有方法，尤其在不进行LLM训练的情况下，其性能可与最先进的方法媲美，并在无训练方法中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了提升机器学习（尤其是分类任务）的模型性能，通常需要生成额外的数据。然而，现有的数据生成方法存在局限性。

Method: TAGAL采用一种代理工作流，利用大型语言模型（LLMs）自动执行迭代式数据生成过程。该过程通过反馈机制不断优化生成的数据质量，无需对LLMs进行额外的训练。此外，TAGAL能够整合外部知识到生成过程中。

Result: TAGAL在多个多样化数据集上进行了评估，考察了生成数据的多种质量方面。实验包括仅使用合成数据训练分类器，以及结合真实数据和合成数据进行训练，并对真实数据和生成数据之间的相似性进行了比较。结果显示，TAGAL的性能与需要LLM训练的最先进方法相当，并普遍优于其他不进行训练的方法。

Conclusion: TAGAL展示了代理工作流在数据生成方面的巨大潜力，并且为基于LLM的数据生成开辟了新的研究方向。

Abstract: The generation of data is a common approach to improve the performance of
machine learning tasks, among which is the training of models for
classification. In this paper, we present TAGAL, a collection of methods able
to generate synthetic tabular data using an agentic workflow. The methods
leverage Large Language Models (LLMs) for an automatic and iterative process
that uses feedback to improve the generated data without any further LLM
training. The use of LLMs also allows for the addition of external knowledge in
the generation process. We evaluate TAGAL across diverse datasets and different
aspects of quality for the generated data. We look at the utility of downstream
ML models, both by training classifiers on synthetic data only and by combining
real and synthetic data. Moreover, we compare the similarities between the real
and the generated data. We show that TAGAL is able to perform on par with
state-of-the-art approaches that require LLM training and generally outperforms
other training-free approaches. These findings highlight the potential of
agentic workflow and open new directions for LLM-based data generation methods.

</details>


### [216] [Attention as an Adaptive Filter](https://arxiv.org/abs/2509.04154)
*Peter Racioppo*

Main category: cs.LG

TL;DR: AFA是一种新颖的注意力机制，将可学习的动态模型直接整合到注意力权重的计算中。


<details>
  <summary>Details</summary>
Motivation: 受线性随机微分方程（SDE）的启发，旨在通过整合动态模型来改进注意力机制。

Method: 将输入序列建模为SDE的离散观测，利用Lyapunov方程的闭式解来传播不确定性，并通过最大似然估计导出注意力权重。

Result: 提出的AFA在计算和内存复杂度上与标准注意力相当，并在特定条件下可退化为标准点积注意力。

Conclusion: AFA通过整合动态模型和不确定性传播，为注意力机制提供了一种新的、更鲁棒的方法。

Abstract: We introduce Adaptive Filter Attention (AFA), a novel attention mechanism
that incorporates a learnable dynamics model directly into the computation of
attention weights. Rather than comparing queries and keys directly, we model
the input sequence as discrete observations of a linear stochastic differential
equation (SDE). By imposing a linear dynamics model with simultaneously
diagonalizable state matrices and noise covariances, we can make use of a
closed-form solution to the differential Lyapunov equation to efficiently
propagate pairwise uncertainties through the dynamics. Attention naturally
arises as the maximum likelihood solution for this linear SDE, with attention
weights corresponding to robust residual-based reweightings of the propagated
pairwise precisions. Imposing an additional constraint on the state matrix's
eigenvalues leads to a simplified variant with the same computational and
memory complexity as standard attention. In the limit of vanishing dynamics and
process noise, and using a small-angle approximation, we recover ordinary
dot-product attention.

</details>


### [217] [Crossing the Species Divide: Transfer Learning from Speech to Animal Sounds](https://arxiv.org/abs/2509.04166)
*Jules Cauzinille,Marius Miron,Olivier Pietquin,Masato Hagiwara,Ricard Marxer,Arnaud Rey,Benoit Favre*

Main category: cs.LG

TL;DR: self-supervised speech models show potential for bioacoustic analysis, offering competitive results with fine-tuned models and highlighting the impact of noise-robust pre-training.


<details>
  <summary>Details</summary>
Motivation: Investigate the effectiveness of self-supervised speech models on bioacoustic detection and classification tasks, an underexplored area.

Method: Analyze transfer learning capabilities of HuBERT, WavLM, and XEUS on animal sounds using linear probing and downstream architectures, considering time-averaged and time-wise information, frequency range, and noise.

Result: Self-supervised speech models generate rich latent representations of animal sounds, yielding results competitive with fine-tuned bioacoustic models. Noise-robust pre-training setups positively impact performance.

Conclusion: Speech-based self-supervised learning presents an efficient framework for advancing bioacoustic research, demonstrating strong performance and potential for broader applications in bioacoustics.

Abstract: Self-supervised speech models have demonstrated impressive performance in
speech processing, but their effectiveness on non-speech data remains
underexplored. We study the transfer learning capabilities of such models on
bioacoustic detection and classification tasks. We show that models such as
HuBERT, WavLM, and XEUS can generate rich latent representations of animal
sounds across taxa. We analyze the models properties with linear probing on
time-averaged representations. We then extend the approach to account for the
effect of time-wise information with other downstream architectures. Finally,
we study the implication of frequency range and noise on performance. Notably,
our results are competitive with fine-tuned bioacoustic pre-trained models and
show the impact of noise-robust pre-training setups. These findings highlight
the potential of speech-based self-supervised learning as an efficient
framework for advancing bioacoustic research.

</details>


### [218] [Privacy Risks in Time Series Forecasting: User- and Record-Level Membership Inference](https://arxiv.org/abs/2509.04169)
*Nicolas Johansson,Tobias Olsson,Daniel Nilsson,Johan Östman,Fazeleh Hoseini*

Main category: cs.LG

TL;DR: 本研究填补了在时间序列预测模型上进行成员推理攻击（MIA）的研究空白，提出了两种新攻击方法（LiRA和DTS），并在真实数据集上验证了预测模型（LSTM、N-HiTS）的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有关于成员推理攻击（MIA）的研究主要集中在分类模型上，而对时间序列预测模型的影响研究不足。本研究旨在解决这一研究空白。

Method: 提出两种新的MIA方法：1. 将用于分类模型的LiRA攻击改编到时间序列预测场景。2. 提出一种新的端到端学习攻击方法Deep Time Series (DTS) 攻击。并与改编自分类领域的其他领先攻击方法进行基准测试。

Result: 在TUH-EEG和ELD数据集上，针对LSTM和N-HiTS预测模型进行了评估。结果表明，预测模型容易受到攻击，特别是用户级别的攻击，检测率接近完美。所提出的DTS和改编的LiRA方法在多个场景下表现最佳，为时间序列预测的隐私风险评估设定了新基准。此外，研究发现攻击的脆弱性随预测范围的增加和训练数据集的减小而增加。

Conclusion: 时间序列预测模型存在显著的隐私泄露风险，提出的LiRA和DTS攻击方法能够有效评估这种风险。攻击的脆弱性与预测范围和训练数据量有关，这与大型语言模型观察到的趋势相似。

Abstract: Membership inference attacks (MIAs) aim to determine whether specific data
were used to train a model. While extensively studied on classification models,
their impact on time series forecasting remains largely unexplored. We address
this gap by introducing two new attacks: (i) an adaptation of multivariate
LiRA, a state-of-the-art MIA originally developed for classification models, to
the time-series forecasting setting, and (ii) a novel end-to-end learning
approach called Deep Time Series (DTS) attack. We benchmark these methods
against adapted versions of other leading attacks from the classification
setting.
  We evaluate all attacks in realistic settings on the TUH-EEG and ELD
datasets, targeting two strong forecasting architectures, LSTM and the
state-of-the-art N-HiTS, under both record- and user-level threat models. Our
results show that forecasting models are vulnerable, with user-level attacks
often achieving perfect detection. The proposed methods achieve the strongest
performance in several settings, establishing new baselines for privacy risk
assessment in time series forecasting. Furthermore, vulnerability increases
with longer prediction horizons and smaller training populations, echoing
trends observed in large language models.

</details>


### [219] [Comment on "A Note on Over-Smoothing for Graph Neural Networks"](https://arxiv.org/abs/2509.04178)
*Razi Hasson,Reuven Guetta*

Main category: cs.LG

TL;DR: 在温和的光谱条件下，图神经网络（GNNs）的狄利克雷能量随深度呈指数下降，这表明了图神经网络中的过度平滑问题。


<details>
  <summary>Details</summary>
Motivation: 分析图神经网络（GNNs）中的过度平滑问题。

Method: 通过狄利克雷能量分析，推导了其与网络深度的关系，并将其扩展到光谱多项式滤波器，同时提供了Leaky-ReLU情况的简短证明。

Result: 狄利克雷能量随深度呈指数下降，并提出了通过边删除和权重放大等实验来缓解过度平滑的方法。

Conclusion: 狄利克雷能量是衡量GNNs过度平滑问题的有效指标，并提供了相应的缓解策略。

Abstract: We comment on Cai and Wang (2020, arXiv:2006.13318), who analyze
over-smoothing in GNNs via Dirichlet energy. We show that under mild spectral
conditions (including with Leaky-ReLU), the Dirichlet energy of node embeddings
decreases exponentially with depth; we further extend the result to spectral
polynomial filters and provide a short proof for the Leaky-ReLU case.
Experiments on edge deletion and weight amplification illustrate when Dirichlet
energy increases, hinting at practical ways to relieve over-smoothing.

</details>


### [220] [Set Block Decoding is a Language Model Inference Accelerator](https://arxiv.org/abs/2509.04185)
*Itai Gat,Heli Ben-Hamu,Marton Havasi,Daniel Haziza,Jeremy Reizenstein,Gabriel Synnaeve,David Lopez-Paz,Brian Karrer,Yaron Lipman*

Main category: cs.LG

TL;DR: Set Block Decoding (SBD)是一种新的语言模型解码方法，通过并行预测多个不一定连续的未来token，显著提高了生成速度（3-5倍），且不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归语言模型在推理时计算和内存成本高，尤其是在解码阶段，限制了其实际应用。

Method: SBD将标准的下一token预测（NTP）和掩码token预测（MATP）整合到单个架构中，允许并行采样多个未来token，并借鉴了离散扩散模型中的高级求解器。该方法无需架构更改或额外训练超参数，兼容KV缓存，只需对现有NTP模型进行微调即可。

Result: 通过对Llama-3.1 8B和Qwen-3 8B进行微调，SBD实现了3-5倍的生成前向传播次数减少，同时保持了与NTP训练相当的性能。

Conclusion: SBD是一种简单且灵活的解码范式，通过并行预测多个未来token来加速生成，无需进行架构更改或增加训练超参数，并能保持与现有技术相当的性能。

Abstract: Autoregressive next token prediction language models offer powerful
capabilities but face significant challenges in practical deployment due to the
high computational and memory costs of inference, particularly during the
decoding stage. We introduce Set Block Decoding (SBD), a simple and flexible
paradigm that accelerates generation by integrating standard next token
prediction (NTP) and masked token prediction (MATP) within a single
architecture. SBD allows the model to sample multiple, not necessarily
consecutive, future tokens in parallel, a key distinction from previous
acceleration methods. This flexibility allows the use of advanced solvers from
the discrete diffusion literature, offering significant speedups without
sacrificing accuracy. SBD requires no architectural changes or extra training
hyperparameters, maintains compatibility with exact KV-caching, and can be
implemented by fine-tuning existing next token prediction models. By
fine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x
reduction in the number of forward passes required for generation while
achieving same performance as equivalent NTP training.

</details>


### [221] [One-Embedding-Fits-All: Efficient Zero-Shot Time Series Forecasting by a Model Zoo](https://arxiv.org/abs/2509.04208)
*Hao-Nan Shi,Ting-Ji Huang,Lu Han,De-Chuan Zhan,Han-Jia Ye*

Main category: cs.LG

TL;DR: ZooCast通过整合现有时间序列基础模型（TSFM），动态选择最优模型进行零样本预测，解决了单一TSFM无法普适的问题。其核心创新在于


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型（TSFM）在零样本预测方面虽然取得了显著进展，但没有单一模型能在所有时间序列模式上都表现最佳。不同模型对不同时间模式有不同的偏好，这表明可以利用TSFM互补的能力来提升预测性能。

Method: ZooCast提出了一种名为"One-Embedding-Fits-All"的范式，为模型动物园中的每个模型创建一个统一的表示空间，并用单个嵌入表示。这使得能够对所有任务进行高效的相似性匹配，从而智能地组装和动态选择最适合不同预测任务的模型。

Result: 实验证明，ZooCast在GIFT-Eval零样本预测基准测试中表现出色，并且保持了与单个TSFM相当的效率。此外，在实际应用中，该框架能够轻松地添加新模型以获得渐进式的精度提升，而几乎没有额外的开销。

Conclusion: ZooCast通过利用TSFM的互补性，提供了一种高效且可扩展的解决方案，能够动态选择和组合模型以应对各种零样本预测任务，并在实际应用中能够适应模型更新，实现持续的性能改进。

Abstract: The proliferation of Time Series Foundation Models (TSFMs) has significantly
advanced zero-shot forecasting, enabling predictions for unseen time series
without task-specific fine-tuning. Extensive research has confirmed that no
single TSFM excels universally, as different models exhibit preferences for
distinct temporal patterns. This diversity suggests an opportunity: how to take
advantage of the complementary abilities of TSFMs. To this end, we propose
ZooCast, which characterizes each model's distinct forecasting strengths.
ZooCast can intelligently assemble current TSFMs into a model zoo that
dynamically selects optimal models for different forecasting tasks. Our key
innovation lies in the One-Embedding-Fits-All paradigm that constructs a
unified representation space where each model in the zoo is represented by a
single embedding, enabling efficient similarity matching for all tasks.
Experiments demonstrate ZooCast's strong performance on the GIFT-Eval zero-shot
forecasting benchmark while maintaining the efficiency of a single TSFM. In
real-world scenarios with sequential model releases, the framework seamlessly
adds new models for progressive accuracy gains with negligible overhead.

</details>


### [222] [Why Can't I See My Clusters? A Precision-Recall Approach to Dimensionality Reduction Validation](https://arxiv.org/abs/2509.04222)
*Diede P. M. van der Hoorn,Alessio Arleo,Fernando V. Paulovich*

Main category: cs.LG

TL;DR: 该论文提出了一种新的降维（DR）质量评估方法，通过量化降维过程中的关系阶段与预期聚类结构的匹配程度，来解决现有 DR 指标无法解释为何预期结构缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有降维（DR）质量指标或评估投影可靠性，或评估聚类结构质量，但不能解释为何预期结构在投影中缺失。可视化分析解决方案耗时，因为超参数空间大。

Method: 提出两种监督指标——精确率（precision）和召回率（recall）——来评估降维的‘关系’阶段。这些指标量化了模型关系与基于标签表示的预期聚类结构的一致性。论文以 t-SNE 和 UMAP 为例进行了说明，并通过各种使用场景进行了验证。

Result: 所提出的方法能够指导超参数调优，揭示投影伪影，并判断关系是否捕获了预期的结构，从而使 DR 过程更快、更可靠。

Conclusion: 该方法通过引入精确率和召回率指标，有效地评估了降维过程中‘关系’阶段的质量，解决了现有方法无法解释预期结构缺失的问题，并能指导超参数调优和揭示投影伪影，提高了降维的效率和可靠性。

Abstract: Dimensionality Reduction (DR) is widely used for visualizing high-dimensional
data, often with the goal of revealing expected cluster structure. However,
such a structure may not always appear in the projections. Existing DR quality
metrics assess projection reliability (to some extent) or cluster structure
quality, but do not explain why expected structures are missing. Visual
Analytics solutions can help, but are often time-consuming due to the large
hyperparameter space. This paper addresses this problem by leveraging a recent
framework that divides the DR process into two phases: a relationship phase,
where similarity relationships are modeled, and a mapping phase, where the data
is projected accordingly. We introduce two supervised metrics, precision and
recall, to evaluate the relationship phase. These metrics quantify how well the
modeled relationships align with an expected cluster structure based on some
set of labels representing this structure. We illustrate their application
using t-SNE and UMAP, and validate the approach through various usage
scenarios. Our approach can guide hyperparameter tuning, uncover projection
artifacts, and determine if the expected structure is captured in the
relationships, making the DR process faster and more reliable.

</details>


### [223] [Rethinking the long-range dependency in Mamba/SSM and transformer models](https://arxiv.org/abs/2509.04226)
*Cong Ma,Kayvan Najarian*

Main category: cs.LG

TL;DR: SSM和Transformer在处理长距离依赖方面各有优劣。SSM的依赖性随序列长度呈指数衰减，而Transformer更具灵活性。我们提出了一种结合两者优势的新SSM模型。


<details>
  <summary>Details</summary>
Motivation: 目前对SSM和Transformer模型处理长距离依赖的能力缺乏理论分析，这阻碍了该方面的系统性改进。

Method: 我们通过定义隐藏状态对过去输入的导数来量化长距离依赖性，并在此基础上比较SSM和Transformer模型。我们还提出了一种新的SSM模型。

Result: SSM的依赖性随序列长度呈指数衰减，而Transformer的注意力机制理论上可以处理更长距离的依赖。我们提出的新SSM模型在标准高斯分布输入下是稳定的。

Conclusion: SSM和Transformer在长距离依赖建模方面各有千秋。我们提出的新SSM模型有望结合两者的优点，并在理论上保证其稳定性。

Abstract: Long-range dependency is one of the most desired properties of recent
sequence models such as state-space models (particularly Mamba) and transformer
models. New model architectures are being actively developed and benchmarked
for prediction tasks requiring long-range dependency. However, the capability
of modeling long-range dependencies of these models has not been investigated
from a theoretical perspective, which hinders a systematic improvement on this
aspect. In this work, we mathematically define long-range dependency using the
derivative of hidden states with respect to past inputs and compare the
capability of SSM and transformer models of modeling long-range dependency
based on this definition. We showed that the long-range dependency of SSM
decays exponentially with the sequence length, which aligns with the
exponential decay of memory function in RNN. But the attention mechanism used
in transformers is more flexible and is not constrained to exponential decay,
which could in theory perform better at modeling long-range dependency with
sufficient training data, computing resources, and proper training. To combine
the flexibility of long-range dependency of attention mechanism and computation
efficiency of SSM, we propose a new formulation for hidden state update in SSM
and prove its stability under a standard Gaussian distribution of the input
data.

</details>


### [224] [Rethinking Layer-wise Gaussian Noise Injection: Bridging Implicit Objectives and Privacy Budget Allocation](https://arxiv.org/abs/2509.04232)
*Qifeng Tan,Shusen Yang,Xuebin Ren,Yikai Zhang*

Main category: cs.LG

TL;DR: 本研究提出了一种统一的分析框架，用于连接层级高斯机制（LGM）中的噪声注入策略与隐私-效用权衡，并提出了一种新的信噪比（SNR）一致性噪声分配策略，以提高隐私保护和效用。


<details>
  <summary>Details</summary>
Motivation: 现有的层级高斯机制（LGM）在差分隐私深度学习中缺乏对噪声分配与隐私-效用权衡的严格理论理解，并且常用的噪声分配策略可能导致优化目标不当和隐私预算利用效率低下。

Method: 提出一个统一的分析框架，系统地连接层级噪声注入策略、隐式优化目标和隐私预算分配。在此基础上，提出一种信噪比（SNR）一致性噪声分配策略。

Result: 实验结果表明，所提出的SNR一致性噪声分配策略在中心化和联邦学习设置中均优于现有方法，实现了更好的隐私-效用权衡，并能更有效地利用隐私预算。

Conclusion: 本研究提出的分析框架为设计深度模型的自适应和有效的噪声注入方案提供了理论指导，解决了现有LGM方法中噪声分配不当和隐私预算利用效率低下的问题。

Abstract: Layer-wise Gaussian mechanisms (LGM) enhance flexibility in differentially
private deep learning by injecting noise into partitioned gradient vectors.
However, existing methods often rely on heuristic noise allocation strategies,
lacking a rigorous understanding of their theoretical grounding in connecting
noise allocation to formal privacy-utility tradeoffs. In this paper, we present
a unified analytical framework that systematically connects layer-wise noise
injection strategies with their implicit optimization objectives and associated
privacy budget allocations. Our analysis reveals that several existing
approaches optimize ill-posed objectives -- either ignoring inter-layer
signal-to-noise ratio (SNR) consistency or leading to inefficient use of the
privacy budget. In response, we propose a SNR-Consistent noise allocation
strategy that unifies both aspects, yielding a noise allocation scheme that
achieves better signal preservation and more efficient privacy budget
utilization. Extensive experiments in both centralized and federated learning
settings demonstrate that our method consistently outperforms existing
allocation strategies, achieving better privacy-utility tradeoffs. Our
framework not only offers diagnostic insights into prior methods but also
provides theoretical guidance for designing adaptive and effective noise
injection schemes in deep models.

</details>


### [225] [Synthetic Survival Data Generation for Heart Failure Prognosis Using Deep Generative Models](https://arxiv.org/abs/2509.04245)
*Chanon Puttanawarut,Natcha Fongsrisin,Porntep Amornritvanich,Cholatid Ratanatharathorn,Panu Looareesuwan*

Main category: cs.LG

TL;DR: 生成的人工心脏病数据在预测模型上与真实数据具有可比性，并且能够保护隐私。


<details>
  <summary>Details</summary>
Motivation: 由于隐私法规和机构壁垒，心脏病研究受到有限的数据集访问的限制。生成合成数据是克服这些挑战并保护患者隐私的一种有前途的解决方案。

Method: 使用五种深度学习模型（TVAE、归一化流、ADSGAN、SurvivalGAN 和 TabDDPM）从 12,552 名患者的机构数据中生成了合成心脏病数据。通过统计相似性、生存预测和隐私评估来评估合成数据的效用。

Result: SurvivalGAN 和 TabDDPM 在保留原始数据集的特征和生存曲线方面表现出色。SurvivalGAN 和 TVAE 在生存预测方面的表现与真实数据相当。隐私评估证实了其免受重新识别攻击的保护能力。

Conclusion: 基于深度学习的合成数据生成能够创建高保真、注重隐私的心脏病数据集，可用于研究。这个公开可用的数据集解决了数据共享的障碍，并为心脏病研究和预测建模提供了宝贵的资源。

Abstract: Background: Heart failure (HF) research is constrained by limited access to
large, shareable datasets due to privacy regulations and institutional
barriers. Synthetic data generation offers a promising solution to overcome
these challenges while preserving patient confidentiality. Methods: We
generated synthetic HF datasets from institutional data comprising 12,552
unique patients using five deep learning models: tabular variational
autoencoder (TVAE), normalizing flow, ADSGAN, SurvivalGAN, and tabular
denoising diffusion probabilistic models (TabDDPM). We comprehensively
evaluated synthetic data utility through statistical similarity metrics,
survival prediction using machine learning and privacy assessments. Results:
SurvivalGAN and TabDDPM demonstrated high fidelity to the original dataset,
exhibiting similar variable distributions and survival curves after applying
histogram equalization. SurvivalGAN (C-indices: 0.71-0.76) and TVAE (C-indices:
0.73-0.76) achieved the strongest performance in survival prediction
evaluation, closely matched real data performance (C-indices: 0.73-0.76).
Privacy evaluation confirmed protection against re-identification attacks.
Conclusions: Deep learning-based synthetic data generation can produce
high-fidelity, privacy-preserving HF datasets suitable for research
applications. This publicly available synthetic dataset addresses critical data
sharing barriers and provides a valuable resource for advancing HF research and
predictive modeling.

</details>


### [226] [RL's Razor: Why Online Reinforcement Learning Forgets Less](https://arxiv.org/abs/2509.04259)
*Idan Shenfeld,Jyothish Pari,Pulkit Agrawal*

Main category: cs.LG

TL;DR: RL在微调时比SFT更能保留先验知识，其遗忘程度取决于分布偏移（KL散度）。RL倾向于最小化KL散度，而SFT可能偏离基础模型。


<details>
  <summary>Details</summary>
Motivation: 比较微调模型与强化学习（RL）和监督微调（SFT）的性能，特别是它们在保留先验知识和能力方面。

Method: 通过计算微调后策略和基础策略在评估新任务时的KL散度来衡量分布偏移。实验对象包括大型语言模型和机器人基础模型。

Result: RL在保留先验知识和能力方面显著优于SFT。遗忘程度与KL散度有关。RL倾向于选择KL散度最小的解决方案，而SFT可能选择任意远离基础模型的解决方案。

Conclusion: RL通过其“RL剃刀”原则，在所有解决新任务的方法中，优先选择与原始模型最接近（KL散度最小）的解决方案。

Abstract: Comparison of fine-tuning models with reinforcement learning (RL) and
supervised fine-tuning (SFT) reveals that, despite similar performance at a new
task, RL preserves prior knowledge and capabilities significantly better. We
find that the degree of forgetting is determined by the distributional shift,
measured as the KL-divergence between the fine-tuned and base policy evaluated
on the new task. Our analysis reveals that on-policy RL is implicitly biased
towards KL-minimal solutions among the many that solve the new task, whereas
SFT can converge to distributions arbitrarily far from the base model. We
validate these findings through experiments with large language models and
robotic foundation models and further provide theoretical justification for why
on-policy RL updates lead to a smaller KL change. We term this principle
$\textit{RL's Razor}$: among all ways to solve a new task, RL prefers those
closest in KL to the original model.

</details>


### [227] [An Interactive Framework for Finding the Optimal Trade-off in Differential Privacy](https://arxiv.org/abs/2509.04290)
*Yaohong Yang,Aki Rehn,Sammie Katt,Antti Honkela,Samuel Kaski*

Main category: cs.LG

TL;DR: 差分隐私 (DP) 的核心挑战在于在隐私保护和模型性能之间取得平衡，这可以被视为一个多目标优化 (MOO) 问题。现有方法效率低下，因为它未能利用 DP 问题的独特结构，即可以通过最大化特定隐私级别下的准确性来直接生成帕累托前沿上的点。为了解决这个问题，本研究提出了一个新方法，该方法首先从理论上推导出这种权衡关系的形状，从而可以直接有效地模拟帕累托前沿。其次，通过向用户展示假设的权衡曲线并要求他们选择偏好的权衡点，改进了偏好学习过程，取代了低效的成对比较。实验结果表明，该方法在逻辑回归和深度迁移学习任务上，显著减少了计算成本和用户交互次数，有效地收敛到最优的隐私-准确性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私 (DP) 和多目标优化 (MOO) 方法在处理隐私保护和模型性能之间的权衡时效率低下，未能利用 DP 问题的内在结构。本研究旨在通过利用 DP 问题的独特属性来改进这一过程，即可以通过最大化特定隐私级别下的准确性来直接生成帕累托前沿上的点，并改进用户偏好学习机制。

Method: 首先，从理论上推导了隐私-准确性权衡关系的形状，以便直接有效地模拟帕累托前沿。其次，提出了一种新的偏好学习方法，通过向用户展示假设的权衡曲线并要求他们选择偏好的权衡点，取代了低效的成对比较。

Result: 在对六个真实世界数据集上的差分隐私逻辑回归和深度迁移学习进行的实验中，该方法与基线方法相比，在计算成本和用户交互次数方面都显著减少，成功收敛到最优的隐私-准确性权衡。

Conclusion: 本研究提出的方法通过理论推导和改进的用户交互机制，能够更有效地找到差分隐私中的最优隐私-准确性权衡，相比现有方法在计算效率和用户参与度上均有显著优势。

Abstract: Differential privacy (DP) is the standard for privacy-preserving analysis,
and introduces a fundamental trade-off between privacy guarantees and model
performance. Selecting the optimal balance is a critical challenge that can be
framed as a multi-objective optimization (MOO) problem where one first
discovers the set of optimal trade-offs (the Pareto front) and then learns a
decision-maker's preference over them. While a rich body of work on interactive
MOO exists, the standard approach -- modeling the objective functions with
generic surrogates and learning preferences from simple pairwise feedback -- is
inefficient for DP because it fails to leverage the problem's unique structure:
a point on the Pareto front can be generated directly by maximizing accuracy
for a fixed privacy level. Motivated by this property, we first derive the
shape of the trade-off theoretically, which allows us to model the Pareto front
directly and efficiently. To address inefficiency in preference learning, we
replace pairwise comparisons with a more informative interaction. In
particular, we present the user with hypothetical trade-off curves and ask them
to pick their preferred trade-off. Our experiments on differentially private
logistic regression and deep transfer learning across six real-world datasets
show that our method converges to the optimal privacy-accuracy trade-off with
significantly less computational cost and user interaction than baselines.

</details>


### [228] [A Primer on Causal and Statistical Dataset Biases for Fair and Robust Image Analysis](https://arxiv.org/abs/2509.04295)
*Charles Jones,Ben Glocker*

Main category: cs.LG

TL;DR: 机器学习模型在现实世界中，尤其是在高风险和涉及社会敏感性的领域，往往会失败，这阻碍了它们在医疗诊断等领域的应用。本文介绍了导致图像分析中机器学习模型失败的因果和统计结构，并重点讨论了两个之前被忽视的问题：“无公平午餐”问题和“子群可分性”问题。同时，文章阐述了当前公平表征学习方法为何无法有效解决这些问题，并对该领域未来的发展提出了建议。


<details>
  <summary>Details</summary>
Motivation: 机器学习方法在现实世界部署中常常失败，特别是在高风险和涉及社会敏感性的场景下，这影响了其在医疗诊断等领域的应用。本文旨在阐述导致这些失败的因果和统计结构，并为解决这些问题提供新的思路。

Method: 本文介绍了导致图像分析中机器学习模型失败的因果和统计结构，并重点讨论了“无公平午餐”问题和“子群可分性”问题。文章还分析了现有公平表征学习方法在此类问题上的不足之处。

Result: 本文指出了机器学习模型在图像分析中失败的两个新问题：“无公平午餐”问题和“子群可分性”问题，并分析了当前公平表征学习方法未能有效解决这些问题的根本原因。

Conclusion: 当前公平表征学习方法在解决“无公平午餐”和“子群可分性”问题上存在不足，未来需要探索新的方法来提高机器学习模型在现实世界中的鲁棒性和公平性，特别是在高风险和涉及社会敏感性的应用场景中。

Abstract: Machine learning methods often fail when deployed in the real world. Worse
still, they fail in high-stakes situations and across socially sensitive lines.
These issues have a chilling effect on the adoption of machine learning methods
in settings such as medical diagnosis, where they are arguably best-placed to
provide benefits if safely deployed. In this primer, we introduce the causal
and statistical structures which induce failure in machine learning methods for
image analysis. We highlight two previously overlooked problems, which we call
the \textit{no fair lunch} problem and the \textit{subgroup separability}
problem. We elucidate why today's fair representation learning methods fail to
adequately solve them and propose potential paths forward for the field.

</details>


### [229] [Using causal abstractions to accelerate decision-making in complex bandit problems](https://arxiv.org/abs/2509.04296)
*Joel Dyer,Nicholas Bishop,Anisoara Calinescu,Michael Wooldridge,Fabio Massimo Zennaro*

Main category: cs.LG

TL;DR: AT-UCB算法利用因果抽象来解决多臂老虎机问题，通过在粗粒度模拟中探索，然后在感兴趣的实例中应用UCB，从而减少了累积遗憾。


<details>
  <summary>Details</summary>
Motivation: 现实世界决策问题可以被编码为不同抽象层次的因果多臂老虎机（CMAB）问题，但缺乏一种能利用各抽象层次的信息和计算优势的通用方法。

Method: 提出AT-UCB算法，该算法利用因果抽象（CA）理论，在易于模拟的粗粒度CMAB实例中进行探索，然后在目标CMAB中仅对潜在最优动作的受限集合使用传统的上限置信（UCB）算法。

Result: AT-UCB在理论上通过新颖的累积遗憾上限得到证明，并在实践中通过应用于不同分辨率和计算成本的流行病学模拟器得到验证，与经典UCB算法相比，累积遗憾显著降低。

Conclusion: AT-UCB算法通过利用因果抽象，在CMAB问题中实现了更低的累积遗憾。

Abstract: Although real-world decision-making problems can often be encoded as causal
multi-armed bandits (CMABs) at different levels of abstraction, a general
methodology exploiting the information and computational advantages of each
abstraction level is missing. In this paper, we propose AT-UCB, an algorithm
which efficiently exploits shared information between CMAB problem instances
defined at different levels of abstraction. More specifically, AT-UCB leverages
causal abstraction (CA) theory to explore within a cheap-to-simulate and
coarse-grained CMAB instance, before employing the traditional upper confidence
bound (UCB) algorithm on a restricted set of potentially optimal actions in the
CMAB of interest, leading to significant reductions in cumulative regret when
compared to the classical UCB algorithm. We illustrate the advantages of AT-UCB
theoretically, through a novel upper bound on the cumulative regret, and
empirically, by applying AT-UCB to epidemiological simulators with varying
resolution and computational cost.

</details>


### [230] [Characteristic Energy Behavior Profiling of Non-Residential Buildings](https://arxiv.org/abs/2509.04322)
*Haley Dozier,Althea Henslee*

Main category: cs.LG

TL;DR: 美国陆军基地的基础设施面临气候变化和极端天气事件的威胁，需要气候韧性措施来保护关键任务资产。本研究提出一种数据驱动的行为模型，以确定基地能源使用行为特征，为能源系统评估和韧性措施基准测试提供依据。


<details>
  <summary>Details</summary>
Motivation: 美国陆军基础设施易受气候变化和极端天气事件威胁，需要采取气候韧性措施来保护支持关键任务和保障战备能力的设施资产。

Method: 提出一种数据驱动的行为模型，该模型能够准确分析、预测和聚类从非居民建筑的能源使用中收集的多模态数据，以模拟单个建筑物的行为。

Result: 提出一种数据驱动的行为模型，以确定基地能源使用行为特征，为能源系统评估和韧性措施基准测试提供依据。由于陆军基地能源使用数据的性质，将使用结构相似的开放访问数据来说明该方法。

Conclusion: 该研究提出的数据驱动行为模型能够为美国陆军基地评估能源系统脆弱性、制定韧性措施提供科学依据。

Abstract: Due to the threat of changing climate and extreme weather events, the
infrastructure of the United States Army installations is at risk. More than
ever, climate resilience measures are needed to protect facility assets that
support critical missions and help generate readiness. As most of the Army
installations within the continental United States rely on commercial energy
and water sources, resilience to the vulnerabilities within independent energy
resources (electricity grids, natural gas pipelines, etc) along with a baseline
understanding of energy usage within installations must be determined. This
paper will propose a data-driven behavioral model to determine behavior
profiles of energy usage on installations. These profiles will be used 1) to
create a baseline assessment of the impact of unexpected disruptions on energy
systems and 2) to benchmark future resiliency measures. In this methodology,
individual building behavior will be represented with models that can
accurately analyze, predict, and cluster multimodal data collected from energy
usage of non-residential buildings. Due to the nature of Army installation
energy usage data, similarly structured open access data will be used to
illustrate this methodology.

</details>


### [231] [Parking Availability Prediction via Fusing Multi-Source Data with A Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer](https://arxiv.org/abs/2509.04362)
*Yin Huang,Yongqi Dong,Youhua Tang,Li Li*

Main category: cs.LG

TL;DR: 该研究提出了一种名为SST-iTransformer的新方法，用于解决城市停车位可用性预测中的时空依赖性和多源数据利用问题。


<details>
  <summary>Details</summary>
Motivation: 城市停车位短缺日益严重，需要准确有效的停车位可用性预测来支持城市规划和管理。

Method: SST-iTransformer方法首先利用K-means聚类建立停车位集群区（PCZs），并整合了来自地铁、公交、网约车和出租车等多种交通方式的交通需求特征。该模型在iTransformer基础上进行升级，集成了基于掩码-重建的自监督时空表征学习的 선행 학습任务，并设计了创新的双分支注意力机制：序列注意力通过打补丁操作捕捉长期时间依赖性，而通道注意力则通过反转维度来模拟跨变量交互。

Result: 在成都真实世界数据上的广泛实验表明，SST-iTransformer的性能优于包括Informer、Autoformer、Crossformer和iTransformer在内的基线深度学习模型，实现了最低的均方误差（MSE）和具有竞争力的平均绝对误差（MAE），达到了最先进的性能。消融研究表明，引入网约车数据带来的性能提升最大，其次是出租车数据，而固定路线公交（公交/地铁）的贡献则很小。空间相关性分析证实，排除PCZ内相关停车位的历史数据会导致性能显著下降，凸显了对时空依赖性建模的重要性。

Conclusion: SST-iTransformer在预测停车位可用性方面表现出色，并能有效利用多源交通数据和时空依赖性。

Abstract: The rapid growth of private car ownership has worsened the urban parking
predicament, underscoring the need for accurate and effective parking
availability prediction to support urban planning and management. To address
key limitations in modeling spatio-temporal dependencies and exploiting
multi-source data for parking availability prediction, this study proposes a
novel approach with SST-iTransformer. The methodology leverages K-means
clustering to establish parking cluster zones (PCZs), extracting and
integrating traffic demand characteristics from various transportation modes
(i.e., metro, bus, online ride-hailing, and taxi) associated with the targeted
parking lots. Upgraded on vanilla iTransformer, SST-iTransformer integrates
masking-reconstruction-based pretext tasks for self-supervised spatio-temporal
representation learning, and features an innovative dual-branch attention
mechanism: Series Attention captures long-term temporal dependencies via
patching operations, while Channel Attention models cross-variate interactions
through inverted dimensions. Extensive experiments using real-world data from
Chengdu, China, demonstrate that SST-iTransformer outperforms baseline deep
learning models (including Informer, Autoformer, Crossformer, and
iTransformer), achieving state-of-the-art performance with the lowest mean
squared error (MSE) and competitive mean absolute error (MAE). Comprehensive
ablation studies quantitatively reveal the relative importance of different
data sources: incorporating ride-hailing data provides the largest performance
gains, followed by taxi, whereas fixed-route transit features (bus/metro)
contribute marginally. Spatial correlation analysis further confirms that
excluding historical data from correlated parking lots within PCZs leads to
substantial performance degradation, underscoring the importance of modeling
spatial dependencies.

</details>


### [232] [When three experiments are better than two: Avoiding intractable correlated aleatoric uncertainty by leveraging a novel bias--variance tradeoff](https://arxiv.org/abs/2509.04363)
*Paul Scherer,Andreas Kirsch,Jake P. Taylor-King*

Main category: cs.LG

TL;DR: 通过利用偏见-方差权衡来解决现实世界中的异方差不确定性问题，并提出新的主动学习策略，在批处理设置中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的实验通常存在异方差随机不确定性，并且这种不确定性在批处理设置中可能相关。

Method: 提出新的主动学习策略，利用偏见-方差权衡直接减少实验轮次之间的偏见，并考虑有噪声和无噪声的模型系统。研究了利用新颖的协偏性-协方差关系以二次方式利用历史数据的方法，该方法通过特征分解策略自然地提出了一种批处理机制。

Result: 在批处理设置中，所提出的利用协偏性-协方差关系的基于差分的方法（使用二次估计器）的性能优于包括BALD和Least Confidence在内的多种经典方法。

Conclusion: 所提出的主动学习策略在处理具有异方差不确定性的批处理实验设置方面是有效的，并且优于现有的基准方法。

Abstract: Real-world experimental scenarios are characterized by the presence of
heteroskedastic aleatoric uncertainty, and this uncertainty can be correlated
in batched settings. The bias--variance tradeoff can be used to write the
expected mean squared error between a model distribution and a ground-truth
random variable as the sum of an epistemic uncertainty term, the bias squared,
and an aleatoric uncertainty term. We leverage this relationship to propose
novel active learning strategies that directly reduce the bias between
experimental rounds, considering model systems both with and without noise.
Finally, we investigate methods to leverage historical data in a quadratic
manner through the use of a novel cobias--covariance relationship, which
naturally proposes a mechanism for batching through an eigendecomposition
strategy. When our difference-based method leveraging the cobias--covariance
relationship is utilized in a batched setting (with a quadratic estimator), we
outperform a number of canonical methods including BALD and Least Confidence.

</details>


### [233] [PagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large Language Model Inference](https://arxiv.org/abs/2509.04377)
*Krishna Teja Chitty-Venkata,Jie Ye,Xian-He Sun,Anthony Kougkas,Murali Emani,Venkatram Vishwanath,Bogdan Nicolae*

Main category: cs.LG

TL;DR: PagedEviction通过引入一种新的块级逐出算法来优化vLLM的PagedAttention，以提高KV缓存的内存效率，从而在长上下文任务中提高LLM的内存使用和准确性。


<details>
  <summary>Details</summary>
Motivation: KV缓存是LLM推理中的内存瓶颈，尤其是在处理长序列时。

Method: PagedEviction是一种细粒度的、结构化的KV缓存剪枝策略，它引入了一种适用于分页内存布局的高效块级逐出算法，并能与PagedAttention无缝集成，无需修改其CUDA注意力核。

Result: 在Llama-3.1-8B-Instruct、Llama-3.2-1B-Instruct和Llama-3.2-3B-Instruct模型以及LongBench基准测试上，PagedEviction在长上下文任务中展示了比基线更好的内存使用和准确性。

Conclusion: PagedEviction是一种有效的KV缓存管理方法，可以提高LLM在长上下文场景下的内存效率和性能。

Abstract: KV caching significantly improves the efficiency of Large Language Model
(LLM) inference by storing attention states from previously processed tokens,
enabling faster generation of subsequent tokens. However, as sequence length
increases, the KV cache quickly becomes a major memory bottleneck. To address
this, we propose PagedEviction, a novel fine-grained, structured KV cache
pruning strategy that enhances the memory efficiency of vLLM's PagedAttention.
Unlike existing approaches that rely on attention-based token importance or
evict tokens across different vLLM pages, PagedEviction introduces an efficient
block-wise eviction algorithm tailored for paged memory layouts. Our method
integrates seamlessly with PagedAttention without requiring any modifications
to its CUDA attention kernels. We evaluate PagedEviction across
Llama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models
on the LongBench benchmark suite, demonstrating improved memory usage with
better accuracy than baselines on long context tasks.

</details>


### [234] [Transition Models: Rethinking the Generative Learning Objective](https://arxiv.org/abs/2509.04394)
*Zidong Wang,Yiyuan Zhang,Xiaoyu Yue,Xiangyu Yue,Yangguang Li,Wanli Ouyang,Lei Bai*

Main category: cs.LG

TL;DR: 生成模型在保真度和计算成本之间存在两难：迭代扩散模型保真度高但计算成本高，而少步模型受限于质量上限。本文提出过渡模型（TiM），一个精确的连续时间动态方程，能够适应任意步数的过渡，实现了从单步跳跃到多步精细化。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型在生成保真度和计算成本之间的根本困境，即迭代扩散模型保真度高但计算成本高，而少步模型受限于质量上限。

Method: 提出一个精确的、连续时间的动态方程，该方程在任何有限时间间隔内解析地定义了状态转换。这导致了一种新颖的生成范式，称为过渡模型（TiM），它能够适应任意步数的过渡，从单步跳跃到具有更多步数的精细化。

Result: TiM 拥有 8.65 亿个参数，在所有评估的步数下均超越了拥有更多参数的领先模型（如 SD3.5（80 亿参数）和 FLUX.1（120 亿参数）），达到了最先进的性能。TiM 展示了随着采样预算的增加，质量单调提高。使用原生分辨率策略时，TiM 在高达 4096x4096 的分辨率下具有出色的保真度。

Conclusion: TiM 克服了生成模型在生成保真度和计算成本之间的两难困境，实现了最先进的性能，并且在增加采样步数时质量单调提高，在更高的分辨率下也具有出色的保真度。

Abstract: A fundamental dilemma in generative modeling persists: iterative diffusion
models achieve outstanding fidelity, but at a significant computational cost,
while efficient few-step alternatives are constrained by a hard quality
ceiling. This conflict between generation steps and output quality arises from
restrictive training objectives that focus exclusively on either infinitesimal
dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by
introducing an exact, continuous-time dynamics equation that analytically
defines state transitions across any finite time interval. This leads to a
novel generative paradigm, Transition Models (TiM), which adapt to
arbitrary-step transitions, seamlessly traversing the generative trajectory
from single leaps to fine-grained refinement with more steps. Despite having
only 865M parameters, TiM achieves state-of-the-art performance, surpassing
leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across
all evaluated step counts. Importantly, unlike previous few-step generators,
TiM demonstrates monotonic quality improvement as the sampling budget
increases. Additionally, when employing our native-resolution strategy, TiM
delivers exceptional fidelity at resolutions up to 4096x4096.

</details>


### [235] [IPA: An Information-Preserving Input Projection Framework for Efficient Foundation Model Adaptation](https://arxiv.org/abs/2509.04398)
*Yuan Yin,Shashanka Venkataramanan,Tuan-Hung Vu,Andrei Bursuc,Matthieu Cord*

Main category: cs.LG

TL;DR: IPA是一种参数高效微调（PEFT）框架，通过特征感知投影来优化LoRA中的信息压缩，从而提高模型在语言和视觉任务上的性能。


<details>
  <summary>Details</summary>
Motivation: LoRA等PEFT方法在预训练权重中注入低秩更新以降低适应成本。然而，LoRA的下投影是随机初始化且与数据无关的，这会丢弃潜在的有用信息，并且在训练过程中变化不大，而上投影承担了大部分的适应工作，导致随机输入压缩成为性能瓶颈。

Method: IPA（特征感知投影）框架，在处理线性情况时，使用近似顶部主成分的算法，实现了高效的投影预训练，并且对推理开销的影响可忽略不计。

Result: 在语言和视觉基准测试中，IPA相比LoRA和DoRA取得了持续的改进。在常识推理任务上平均准确率提高了1.5个点，在VTAB-1k上提高了2.3个点。当投影被冻结时，IPA能够以大约一半的可训练参数匹配完整的LoRA性能。

Conclusion: IPA通过特征感知投影有效地保留了降维后的隐藏空间信息，解决了LoRA中随机输入压缩的性能瓶颈，并在多个基准测试中取得了优于现有方法的性能。

Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce
adaptation cost by injecting low-rank updates into pretrained weights. However,
LoRA's down-projection is randomly initialized and data-agnostic, discarding
potentially useful information. Prior analyses show that this projection
changes little during training, while the up-projection carries most of the
adaptation, making the random input compression a performance bottleneck. We
propose IPA, a feature-aware projection framework that explicitly preserves
information in the reduced hidden space. In the linear case, we instantiate IPA
with algorithms approximating top principal components, enabling efficient
projector pretraining with negligible inference overhead. Across language and
vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on
average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on
VTAB-1k, while matching full LoRA performance with roughly half the trainable
parameters when the projection is frozen.

</details>


### [236] [Interpretable Clustering with Adaptive Heterogeneous Causal Structure Learning in Mixed Observational Data](https://arxiv.org/abs/2509.04415)
*Wenrui Li,Qinghao Zhang,Xiaowo Wang*

Main category: cs.LG

TL;DR: HCL是一个无监督框架，可以从混合类型观察数据中学习潜在的聚类和相关的因果结构，而无需任何先验知识。


<details>
  <summary>Details</summary>
Motivation: 理解因果异质性对于生物学和医学等领域的科学发现至关重要。然而，现有方法缺乏因果意识，对异质性、混淆和观察性约束的建模不足，导致可解释性差，并且难以区分真正的因果异质性与虚假关联。

Method: HCL框架通过引入包含结构异质性和混淆的等效表示，放宽了同质性和充分性假设。它进一步开发了一种双向迭代策略，用于交替细化因果聚类和结构学习，以及一种平衡跨聚类通用性和特异性的自监督正则化。

Result: HCL在聚类和结构学习任务方面均取得了优越的性能，并在真实世界的单细胞扰动数据中恢复了具有生物学意义的机制。

Conclusion: HCL能够发现可解释的、机制层面的因果异质性，并证明了其在生物学和医学等领域的应用潜力。

Abstract: Understanding causal heterogeneity is essential for scientific discovery in
domains such as biology and medicine. However, existing methods lack causal
awareness, with insufficient modeling of heterogeneity, confounding, and
observational constraints, leading to poor interpretability and difficulty
distinguishing true causal heterogeneity from spurious associations. We propose
an unsupervised framework, HCL (Interpretable Causal Mechanism-Aware Clustering
with Adaptive Heterogeneous Causal Structure Learning), that jointly infers
latent clusters and their associated causal structures from mixed-type
observational data without requiring temporal ordering, environment labels,
interventions or other prior knowledge. HCL relaxes the homogeneity and
sufficiency assumptions by introducing an equivalent representation that
encodes both structural heterogeneity and confounding. It further develops a
bi-directional iterative strategy to alternately refine causal clustering and
structure learning, along with a self-supervised regularization that balance
cross-cluster universality and specificity. Together, these components enable
convergence toward interpretable, heterogeneous causal patterns. Theoretically,
we show identifiability of heterogeneous causal structures under mild
conditions. Empirically, HCL achieves superior performance in both clustering
and structure learning tasks, and recovers biologically meaningful mechanisms
in real-world single-cell perturbation data, demonstrating its utility for
discovering interpretable, mechanism-level causal heterogeneity.

</details>


### [237] [Towards a Unified View of Large Language Model Post-Training](https://arxiv.org/abs/2509.04419)
*Xingtai Lv,Yuxin Zuo,Youbang Sun,Hongyi Liu,Yuntian Wei,Zhekai Chen,Lixuan He,Xuekai Zhu,Kaiyan Zhang,Bingning Wang,Ning Ding,Bowen Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种统一的策略梯度估计器，将在线（RL）和离线（SFT）数据源的训练方法统一起来，并基于此提出了一种名为HPT的混合训练算法，该算法能在数学推理等多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线（模型生成）和离线（人类或模型演示）数据是训练现代语言模型的两个主要来源，分别由强化学习（RL）和监督微调（SFT）等方法使用。本文旨在表明这两种方法并非相互排斥，而是单一优化过程的不同实例。

Method: 提出一个统一的策略梯度估计器，该估计器由四个可互换的部分组成：稳定掩码、参考策略分母、优势估计和似然梯度。基于此理论发现，提出一种名为混合后训练（HPT）的算法，该算法能够动态选择不同的训练信号，以有效地利用演示数据并进行稳定的探索，同时不牺牲学到的推理模式。

Result: 在六个数学推理基准和两个分布外（out-of-distribution）测试集中，HPT算法在不同规模和系列的模型上均能持续超越强大的基线方法。

Conclusion: 本文提出的统一策略梯度估计器和HPT算法提供了一个理论框架，证明了在线和离线数据训练方法的统一性，并通过实验验证了HPT在提升语言模型在数学推理等任务上的表现方面具有优越性。

Abstract: Two major sources of training data exist for post-training modern language
models: online (model-generated rollouts) data, and offline (human or
other-model demonstrations) data. These two types of data are typically used by
approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT),
respectively. In this paper, we show that these approaches are not in
contradiction, but are instances of a single optimization process. We derive a
Unified Policy Gradient Estimator, and present the calculations of a wide
spectrum of post-training approaches as the gradient of a common objective
under different data distribution assumptions and various bias-variance
tradeoffs. The gradient estimator is constructed with four interchangeable
parts: stabilization mask, reference policy denominator, advantage estimate,
and likelihood gradient. Motivated by our theoretical findings, we propose
Hybrid Post-Training (HPT), an algorithm that dynamically selects different
training signals. HPT is designed to yield both effective exploitation of
demonstration and stable exploration without sacrificing learned reasoning
patterns. We provide extensive experiments and ablation studies to verify the
effectiveness of our unified theoretical framework and HPT. Across six
mathematical reasoning benchmarks and two out-of-distribution suites, HPT
consistently surpasses strong baselines across models of varying scales and
families.

</details>


### [238] [Echo State Networks as State-Space Models: A Systems Perspective](https://arxiv.org/abs/2509.04422)
*Pradeep Singh,Balasubramanian Raman*

Main category: cs.LG

TL;DR: ESNs被重新理解为状态空间模型（SSMs），这提供了一个统一的系统理论解释，将水库计算与经典识别和现代核SSMs联系起来。


<details>
  <summary>Details</summary>
Motivation: ESNs的设计和动力学通常依赖于启发式方法，缺乏第一性原理的指导。

Method: 本文将ESNs显式地重新构建为状态空间模型（SSMs）。首先，证明了ESN的echo-state性质是收缩非线性SSM的输入到状态稳定性的一种形式，并导出了可验证的条件。其次，开发了两种映射：一种是产生局部有效的LTI SSM的小信号线性化，另一种是通过提升/Koopman随机特征展开将ESN转换为增广状态下的线性SSM。第三，将teacher forcing视为状态估计，并提出了一种辅助读出学习的方法。

Result: 研究表明，ESNs可以被视为SSMs，并且推导出了可验证的条件来确保echo-state性质。通过线性化和特征展开，可以对ESNs进行可解释的分析，并揭示了其与结构化SSM核的联系。提出的Kalman/EKF辅助读出学习和EM方法可以用于超参数优化和状态估计。

Conclusion: 将ESNs视为SSMs提供了一个统一的理论框架，使得能够从系统理论的角度来理解和分析ESNs的设计和性能，并提出了一系列新的方法来改进其训练和超参数优化。

Abstract: Echo State Networks (ESNs) are typically presented as efficient,
readout-trained recurrent models, yet their dynamics and design are often
guided by heuristics rather than first principles. We recast ESNs explicitly as
state-space models (SSMs), providing a unified systems-theoretic account that
links reservoir computing with classical identification and modern kernelized
SSMs. First, we show that the echo-state property is an instance of
input-to-state stability for a contractive nonlinear SSM and derive verifiable
conditions in terms of leak, spectral scaling, and activation Lipschitz
constants. Second, we develop two complementary mappings: (i) small-signal
linearizations that yield locally valid LTI SSMs with interpretable poles and
memory horizons; and (ii) lifted/Koopman random-feature expansions that render
the ESN a linear SSM in an augmented state, enabling transfer-function and
convolutional-kernel analyses. This perspective yields frequency-domain
characterizations of memory spectra and clarifies when ESNs emulate structured
SSM kernels. Third, we cast teacher forcing as state estimation and propose
Kalman/EKF-assisted readout learning, together with EM for hyperparameters
(leak, spectral radius, process/measurement noise) and a hybrid subspace
procedure for spectral shaping under contraction constraints.

</details>


### [239] [Unveiling the Role of Data Uncertainty in Tabular Deep Learning](https://arxiv.org/abs/2509.04430)
*Nikolay Kartashev,Ivan Rubachev,Artem Babenko*

Main category: cs.LG

TL;DR: 数据不确定性是表格深度学习成功的关键，本文揭示了这一机制并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 表格深度学习在实践中表现优异，但缺乏对其成功原因的深入理解。本文旨在弥合这一差距，强调数据不确定性的重要性。

Method: 通过分析数值特征嵌入、检索增强模型和高级集成策略等技术，揭示它们在管理数据不确定性方面的作用，从而提供对近期性能提升的统一理解。

Result: 开发了更有效的数值特征嵌入方法，并为表格深度学习的研究方向提供了见解。

Conclusion: 数据不确定性是理解表格深度学习方法有效性的关键视角，该研究为改进现有技术和未来研究奠定了基础。

Abstract: Recent advancements in tabular deep learning have demonstrated exceptional
practical performance, yet the field often lacks a clear understanding of why
these techniques actually succeed. To address this gap, our paper highlights
the importance of the concept of data uncertainty for explaining the
effectiveness of the recent tabular DL methods. In particular, we reveal that
the success of many beneficial design choices in tabular DL, such as numerical
feature embeddings, retrieval-augmented models and advanced ensembling
strategies, can be largely attributed to their implicit mechanisms for managing
high data uncertainty. By dissecting these mechanisms, we provide a unifying
understanding of the recent performance improvements. Furthermore, the insights
derived from this data-uncertainty perspective directly allowed us to develop
more effective numerical feature embeddings as an immediate practical outcome
of our analysis. Overall, our work paves the way to foundational understanding
of the benefits introduced by modern tabular methods that results in the
concrete advancements of existing techniques and outlines future research
directions for tabular DL.

</details>


### [240] [Delta Activations: A Representation for Finetuned Large Language Models](https://arxiv.org/abs/2509.04442)
*Zhiqiu Xu,Amish Sethi,Mayur Naik,Ser-Nam Lim*

Main category: cs.LG

TL;DR: 大型语言模型（LLM）的普及带来了大量针对特定任务和领域的微调模型，但缺乏统一的元数据和非结构化存储库使得理解和导航这些模型变得困难。本研究提出了一种名为“Delta Activations”的新方法，通过测量微调模型相对于基础模型内部激活的偏移量，将微调模型表示为向量嵌入。这种表示能够有效地按领域和任务对模型进行聚类，揭示模型生态的结构。Delta Activations 在不同微调设置下表现出鲁棒性，并且在混合微调数据集时具有可加性。此外，该方法还可以通过少样本微调来嵌入任务，并可用于模型选择和合并，旨在促进公开可用模型的重用。


<details>
  <summary>Details</summary>
Motivation: 现有的大量微调模型由于元数据不一致和存储库非结构化，难以理解和有效利用。本研究旨在提出一种新的方法来表示和组织这些模型，以便于导航、选择和合并。

Method: 提出了一种名为“Delta Activations”的方法，将微调模型表示为相对于基础模型的内部激活变化的向量嵌入。利用这种向量表示进行模型聚类，并探索其在任务嵌入、模型选择和模型合并等方面的应用。

Result: Delta Activations 能够有效地按领域和任务对模型进行聚类，揭示了模型空间的结构。该方法在不同微调设置下表现出鲁棒性，并具有可加性。此外，它还可以通过少样本微调来嵌入任务，并可用于模型选择和合并。

Conclusion: Delta Activations 提供了一种表示和组织微调模型的新颖有效的方法，通过向量嵌入来量化模型间的差异，有助于理解模型空间、模型选择和模型合并，从而促进开源模型的重用。

Abstract: The success of powerful open source Large Language Models (LLMs) has enabled
the community to create a vast collection of post-trained models adapted to
specific tasks and domains. However, navigating and understanding these models
remains challenging due to inconsistent metadata and unstructured repositories.
We introduce Delta Activations, a method to represent finetuned models as
vector embeddings by measuring shifts in their internal activations relative to
a base model. This representation allows for effective clustering by domain and
task, revealing structure in the model landscape. Delta Activations also
demonstrate desirable properties: it is robust across finetuning settings and
exhibits an additive property when finetuning datasets are mixed. In addition,
we show that Delta Activations can embed tasks via few-shot finetuning, and
further explore its use for model selection and merging. We hope Delta
Activations can facilitate the practice of reusing publicly available models.
Code is available at https://github.com/OscarXZQ/delta_activations.

</details>


### [241] [Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment](https://arxiv.org/abs/2509.04445)
*Cyrus Cousins,Vijay Keswani,Vincent Conitzer,Hoda Heidari,Jana Schaich Borg,Walter Sinnott-Armstrong*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent AI work trends towards incorporating human-centric objectives, with
the explicit goal of aligning AI models to personal preferences and societal
values. Using standard preference elicitation methods, researchers and
practitioners build models of human decisions and judgments, which are then
used to align AI behavior with that of humans. However, models commonly used in
such elicitation processes often do not capture the true cognitive processes of
human decision making, such as when people use heuristics to simplify
information associated with a decision problem. As a result, models learned
from people's decisions often do not align with their cognitive processes, and
can not be used to validate the learning framework for generalization to other
decision-making tasks. To address this limitation, we take an axiomatic
approach to learning cognitively faithful decision processes from pairwise
comparisons. Building on the vast literature characterizing the cognitive
processes that contribute to human decision-making, and recent work
characterizing such processes in pairwise comparison tasks, we define a class
of models in which individual features are first processed and compared across
alternatives, and then the processed features are then aggregated via a fixed
rule, such as the Bradley-Terry rule. This structured processing of information
ensures such models are realistic and feasible candidates to represent
underlying human decision-making processes. We demonstrate the efficacy of this
modeling approach in learning interpretable models of human decision making in
a kidney allocation task, and show that our proposed models match or surpass
the accuracy of prior models of human pairwise decision-making.

</details>


### [242] [ChronoGraph: A Real-World Graph-Based Multivariate Time Series Dataset](https://arxiv.org/abs/2509.04449)
*Adrian Catalin Lutu,Ioana Pintilie,Elena Burceanu,Andrei Manolache*

Main category: cs.LG

TL;DR: ChronoGraph是一个包含多变量时间序列、服务依赖图和与真实事件相关的异常标签的新数据集，用于微服务系统中的结构感知预测和事件感知评估。


<details>
  <summary>Details</summary>
Motivation: 构建一个用于研究结构感知预测和事件感知评估的真实世界基准数据集，特别是针对微服务系统中由于系统级性能指标和它们之间的依赖关系而引起的问题。

Method: ChronoGraph数据集包含节点（服务）和边（服务依赖关系）。每个节点都会发出多变量时间序列数据（CPU、内存、网络使用模式），并提供专家注释的事件窗口作为异常标签。

Result: 该论文报告了使用预测模型、预训练时间序列基础模型和标准异常检测器进行的基线结果。

Conclusion: ChronoGraph 提供了一个现实的基准，用于研究微服务系统中的结构感知预测和事件感知评估。

Abstract: We present ChronoGraph, a graph-structured multivariate time series
forecasting dataset built from real-world production microservices. Each node
is a service that emits a multivariate stream of system-level performance
metrics, capturing CPU, memory, and network usage patterns, while directed
edges encode dependencies between services. The primary task is forecasting
future values of these signals at the service level. In addition, ChronoGraph
provides expert-annotated incident windows as anomaly labels, enabling
evaluation of anomaly detection methods and assessment of forecast robustness
during operational disruptions. Compared to existing benchmarks from industrial
control systems or traffic and air-quality domains, ChronoGraph uniquely
combines (i) multivariate time series, (ii) an explicit, machine-readable
dependency graph, and (iii) anomaly labels aligned with real incidents. We
report baseline results spanning forecasting models, pretrained time-series
foundation models, and standard anomaly detectors. ChronoGraph offers a
realistic benchmark for studying structure-aware forecasting and incident-aware
evaluation in microservice systems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [243] [Multi-Sensor Fusion for Extended Object Tracking Exploiting Active and Passive Radio Signals](https://arxiv.org/abs/2509.03686)
*Hong Zhu,Alexander Venus,Erik Leitinger,Klaus Witrisal*

Main category: eess.SP

TL;DR: 该论文提出了一种新的贝叶斯方法，用于在存在视线遮挡的情况下对无线电设备进行精确定位，该方法通过融合主动测量和被动雷达测量，并使用新的多传感器、多测量概率数据关联算法来处理测量起源不确定性。


<details>
  <summary>Details</summary>
Motivation: 无线电设备在多径传播、硬件缺陷和干扰等因素下，其可靠和鲁棒的定位一直是一个挑战。尤其是在用户（代理）自身可能遮挡视线（LOS）链路到基站（锚点）的情况下，这是一个经常被忽视但至关重要的问题。

Method: 提出了一种贝叶斯方法，该方法融合了“主动”测量（设备与锚点之间）和“被动”多站雷达类型测量（锚点之间，由扩展对象（EO）反射）。引入了一种多传感器、多测量概率数据关联（PDA）算法来处理测量起源不确定性，并开发了一个针对人类用户的EO模型，该模型考虑了身体表面反射的多次反射，并提出了一个简化的变体以实现低复杂度实现。

Result: 在合成和真实无线电测量上的评估表明，所提出的算法优于传统的基于点目标假设的PDA方法，特别是在视线受阻（OLOS）期间和之后。

Conclusion: 该研究成功地提出了一种在视线被遮挡的情况下提高无线设备定位精度的贝叶斯方法，该方法通过融合不同类型的测量并采用先进的数据关联技术，在实际应用中具有重要意义。

Abstract: Reliable and robust positioning of radio devices remains a challenging task
due to multipath propagation, hardware impairments, and interference from other
radio transmitters. A frequently overlooked but critical factor is the agent
itself, e.g., the user carrying the device, which potentially obstructs
line-of-sight (LOS) links to the base stations (anchors). This paper addresses
the problem of accurate positioning in scenarios where LOS links are partially
blocked by the agent. The agent is modeled as an extended object (EO) that
scatters, attenuates, and blocks radio signals. We propose a Bayesian method
that fuses ``active'' measurements (between device and anchors) with
``passive'' multistatic radar-type measurements (between anchors, reflected by
the EO). To handle measurement origin uncertainty, we introduce an multi-sensor
and multiple-measurement probabilistic data association (PDA) algorithm that
jointly fuses all EO-related measurements. Furthermore, we develop an EO model
tailored to agents such as human users, accounting for multiple reflections
scattered off the body surface, and propose a simplified variant for
low-complexity implementation. Evaluation on both synthetic and real radio
measurements demonstrates that the proposed algorithm outperforms conventional
PDA methods based on point target assumptions, particularly during and after
obstructed line-of-sight (OLOS) conditions.

</details>


### [244] [Sensor placement for sparse force reconstruction](https://arxiv.org/abs/2509.03825)
*Jeunghoon Lee*

Main category: eess.SP

TL;DR: 提出一种基于格拉姆矩阵的传感器布局策略，用于频域稀疏力重构，通过贪心算法选择传感器位置以最小化格拉姆矩阵的离线能量，提高力的估计精度。


<details>
  <summary>Details</summary>
Motivation: 提出一种基于格拉姆矩阵的传感器布局策略，以提高频域稀疏力重构的准确性。

Method: 通过格拉姆矩阵的模态分解来揭示其结构特性，并提出一种贪心算法来选择传感器位置，以最小化格拉姆矩阵的离线能量。

Result: 数值模拟和实验验证表明，该方法在力的估计方面表现稳健且准确，优于启发式传感器布局。

Conclusion: 提出的基于格拉姆矩阵的传感器布局策略能够有效提高频域稀疏力重构的精度和鲁棒性。

Abstract: The present study proposes a Gram-matrix-based sensor placement strategy for
sparse force reconstruction in the frequency domain. A modal decomposition of
the Gram matrix reveals that its structure is dominated by a few modes near the
target frequency, and that each modal contribution reflects the spatial
correlation of the corresponding mode shape. This suggests that placing sensors
near nodal regions where spatial correlation is low can reduce coherence in the
frequency response function (FRF) matrix and improve force reconstruction
accuracy. To translate the physical insight into a practical design framework,
a greedy algorithm is proposed to select sensor locations that minimize the
off-diagonal energy of the Gram matrix. Numerical simulations and experimental
validations demonstrate that the proposed method yields robust and accurate
force estimation, outperforming heuristic sensor layouts.

</details>


### [245] [A Low-Cost Open-Source BLE-Based Asian Hornet Tracking System](https://arxiv.org/abs/2509.03979)
*Gilles Callebaut,Jan Van Moer*

Main category: eess.SP

TL;DR: 本研究提出了一种基于低成本、开源蓝牙低功耗（BLE）技术的亚洲黄蜂（Vespa velutina）巢穴追踪系统。


<details>
  <summary>Details</summary>
Motivation: 亚洲黄蜂对生态系统和养蜂业构成威胁，但目前巢穴定位方法耗时费力，需要改进。

Method: 该系统使用轻量级BLE标签和GNU Radio实现的软件定义无线电（SDR）接收器。通过绕过BLE堆栈，在未编码的PHY中嵌入自定义伪噪声（PN）序列进行基于相关检测。接收器利用八木天线和PlutoSDR进行数字波束扫描以确定标签方向。

Result: 野外测试表明，该系统在50米处具有可靠的角度分辨率，通信距离可达360米。

Conclusion: 该方法提高了接收器的复杂性，但为未来的多通道扩展和标签识别等功能改进提供了可能。该设计完全开源，为黄蜂追踪和环境监测等相关应用提供了一个可扩展的框架。

Abstract: The Asian hornet (Vespa velutina) poses a serious threat to ecosystems and
beekeeping. Locating nests is essential, but usually involves time-consuming
manual triangulation. We present a low-cost, open-source tracking system based
on Bluetooth Low Energy (BLE). The system consists of a lightweight BLE tag and
a software-defined radio (SDR) receiver implemented in GNU Radio. By bypassing
the BLE stack, we embed a custom pseudo-noise (PN) sequence in the uncoded PHY
for correlation-based detection. Using a Yagi antenna and PlutoSDR, the
receiver performs digital beam sweeping to determine the tag's direction. Field
tests show reliable angular resolution at 50m and a communication range up to
360m. While our modulation increases receiver complexity, it enables future
improvements such as multichannel spreading and tag identification. The design
is fully open-source and provides a scalable framework for hornet tracking and
related applications in environmental monitoring.

</details>


### [246] [Approximate Message Passing for Multi-Preamble Detection in OTFS Random Access](https://arxiv.org/abs/2509.03980)
*Alessandro Mirri,Vishnu Teja Kunde,Enrico Paolini,Jean-Francois Chamberland*

Main category: eess.SP

TL;DR: 本文提出了一种基于OTFS信号的多前导码检测方法，利用双稀疏性（前导码稀疏性和OTFS的延迟-多普勒域稀疏性）通过新颖的AMP算法实现.


<details>
  <summary>Details</summary>
Motivation: 解决基于OTFS信号的随机接入系统中多前导码检测问题.

Method: 将问题建模为复数域中的结构化稀疏恢复问题，并提出一种新的近似消息传递（AMP）算法，该算法强制执行双稀疏性，并设计了一种新颖的AMP去噪器.

Result: 仿真结果表明，所提出的方法实现了鲁棒的检测性能，并显著优于现有技术.

Conclusion: 所提出的基于AMP的双稀疏性方法能够有效地解决OTFS随机接入系统中的多前导码检测问题，并提供性能优势。

Abstract: This article addresses the problem of multiple preamble detection in random
access systems based on orthogonal time frequency space (OTFS) signaling. This
challenge is formulated as a structured sparse recovery problem in the complex
domain. To tackle it, the authors propose a new approximate message passing
(AMP) algorithm that enforces double sparsity: the sparse selection of
preambles and the inherent sparsity of OTFS signals in the delay-Doppler
domain. From an algorithmic standpoint, the non-separable complex sparsity
constraint necessitates a careful derivation and leads to the design of a novel
AMP denoiser. Simulation results demonstrate that the proposed method achieves
robust detection performance and delivers significant gains over
state-of-the-art techniques.

</details>


### [247] [Joint Frequency-Space Sparse Reconstruction for DOA Estimation under Coherent Sources and Amplitude-Phase Errors](https://arxiv.org/abs/2509.03983)
*Yutong Chen,Cong Zhou,Changsheng You,Shuo Shi*

Main category: eess.SP

TL;DR: 提出了一种联合频空稀疏重构方法，用于解决相干源和阵列幅相误差下的DOA估计问题。


<details>
  <summary>Details</summary>
Motivation: 解决相干源和阵列幅相误差对DOA估计的影响。

Method: 利用辅助源构造实数导向矢量（RSVs）来补偿幅相误差，并结合频域快照数据的谱稀疏性和入射方向的空间稀疏性，采用稀疏重构方法进行DOA估计，无需迭代优化。

Result: 所提方法在相干源情况下，相比基准方案具有更高的估计精度。

Conclusion: 所提联合频空稀疏重构方法能够有效解决相干源和阵列幅相误差下的DOA估计问题，且计算复杂度低，精度高。

Abstract: In this letter, we propose a joint frequency-space sparse reconstruction
method for direction-of-arrival (DOA) estimation, which effectively addresses
the issues arising from the existence of coherent sources and array
amplitude-phase errors. Specifically, by using an auxiliary source with known
angles, we first construct the real steering vectors (RSVs) based on the
spectral peaks of received signals in the frequency domain, which serve as a
complete basis matrix for compensation for amplitude-phase errors. Then, we
leverage the spectral sparsity of snapshot data in the frequency domain and the
spatial sparsity of incident directions to perform the DOA estimation according
to the sparse reconstruction method. The proposed method does not require
iterative optimization, hence exhibiting low computational complexity.
Numerical results demonstrate that the proposed DOA estimation method achieves
higher estimation accuracy for coherent sources as compared to various
benchmark schemes.

</details>


### [248] [Robust MIMO Semantic Communication with Imperfect CSI via Knowledge Distillation](https://arxiv.org/abs/2509.04005)
*Mingze Gong,Shuoyao Wang,Shijian Gao,Jia Yan,Suzhi Bi*

Main category: eess.SP

TL;DR: MIMO SemComm系统在信道估计不准确时性能下降。本文提出的HANA-JSCC系统通过信道矩阵适配器和两阶段训练策略解决了这个问题，并在各种噪声和估计误差下实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的MIMO SemComm系统在信道估计不准确时性能会下降，因为它们假设信道估计是完美的，这在实际中是不可能的。因此，需要一种能够适应信道估计误差的MIMO SemComm系统。

Method: 提出了一种名为HANA-JSCC的语义图像传输系统，该系统包含一个信道矩阵适配器，用于处理信道估计误差。此外，还采用了一种两阶段训练策略，并结合知识蒸馏来解决由于信道矩阵估计的病态问题（一对多关系）而导致的收敛困难。

Result: 与现有技术相比，HANA-JSCC在各种噪声和估计误差水平下，在不同数据集上实现了0.40~0.54dB的平均性能提升。

Conclusion: HANA-JSCC系统能够有效地适应MIMO系统中的信道估计误差，并在保持语义传输效率的同时提高系统性能。

Abstract: Semantic communication (SemComm) has emerged as a new communication paradigm.
To enhance efficiency, multiple-input-multiple-output (MIMO) technology has
been further integrated into SemComm systems. However, existing MIMO SemComm
systems assume perfect channel matrix estimation for channel-adaptive joint
source-channel coding, which is impractical due to hardware and pilot overhead
constraints. In this paper, we propose a semantic image transmission system
with channel matrix and channel noise adaptation, named HANA-JSCC, to cope with
channel estimation errors in MIMO systems. We propose a channel matrix adaptor
that collaborates with the channel codec to adapt to misaligned channel state
information, thereby mitigating the impact of estimation errors. Since the
relationship between the estimated channel matrix and true channel matrix is
ill-posed (one-to-many), we further introduce a two-stage training strategy
with knowledge distillation to overcome the convergence difficulties caused by
the ill-posed problem. Comparing with the state-of-the-art benchmarks,
HANA-JSCC achieves $0.40\sim0.54$dB higher average performance across various
noise and estimation error levels in various datasets.

</details>


### [249] [Constellation Shaping for OFDM-ISAC Systems: From Theoretical Bounds to Practical Implementation](https://arxiv.org/abs/2509.04055)
*Benedikt Geiger,Fan Liu,Shihang Lu,Andrej Rode,Daniel Gil Gaviria,Charlotte Muth,Laurent Schmalen*

Main category: eess.SP

TL;DR: 本篇论文研究了在正交频分复用（OFDM） 기반 的集成传感与通信（ISAC）系统中，通过星座整形同时提升传感与通信性能的方法。


<details>
  <summary>Details</summary>
Motivation: 通信和传感对信号调制格式的要求存在冲突，导致性能上的权衡。本研究旨在解决这一冲突，并同时提升通信和传感的性能。

Method: 研究了星座整形，包括几何、概率和联合星座整形。利用自动编码器进行优化，并推导了在给定传感约束下最大可实现信息率的理论上下界。此外，还研究了概率幅值整形（PAS）及其针对ISAC的推广，并提出了一种低复杂度对数似然比计算方法。

Result: 结果表明，星座整形可以在通信和传感之间实现灵活的权衡，性能接近理论上界，并且显著优于传统的调制格式。提出的PAS推广方法在保持低复杂度的同时，能够接近联合星座整形的性能。

Conclusion: 星座整形是一种有效的方法，可以在OFDM 기반 的ISAC系统中实现通信和传感性能的协同优化。提出的PAS推广方法为实现低复杂度、高性能的ISAC系统提供了可行的解决方案。

Abstract: Integrated sensing and communications (ISAC) promises new use cases for
mobile communication systems by reusing the communication signal for radar-like
sensing. However, sensing and communications (S&C) impose conflicting
requirements on the modulation format, resulting in a tradeoff between their
corresponding performance. This paper investigates constellation shaping as a
means to simultaneously improve S&C performance in orthogonal frequency
division multiplexing (OFDM)-based ISAC systems. We begin by deriving how the
transmit symbols affect detection performance and derive theoretical lower and
upper bounds on the maximum achievable information rate under a given sensing
constraint. Using an autoencoder-based optimization, we investigate geometric,
probabilistic, and joint constellation shaping, where joint shaping combines
both approaches, employing both optimal maximum a-posteriori decoding and
practical bit-metric decoding. Our results show that constellation shaping
enables a flexible trade-off between S&C, can approach the derived upper bound,
and significantly outperforms conventional modulation formats. Motivated by its
practical implementation feasibility, we review probabilistic amplitude shaping
(PAS) and propose a generalization tailored to ISAC. For this generalization,
we propose a low-complexity log-likelihood ratio computation with negligible
rate loss. We demonstrate that combining conventional and generalized PAS
enables a flexible and low-complexity tradeoff between S&C, closely approaching
the performance of joint constellation shaping.

</details>


### [250] [Reliable Clutter Suppression for Slow-Moving Weak Target Radar Detection](https://arxiv.org/abs/2509.04309)
*R. Zhang,J. Xue,T. Zhang*

Main category: eess.SP

TL;DR: 该论文提出了一种基于Go分解（Godec）框架的新型杂波抑制方案，用于可靠地检测复杂环境中缓慢移动的弱目标，解决了传统MTI方法可能抑制所需目标的问题。


<details>
  <summary>Details</summary>
Motivation: 可靠地检测复杂环境中缓慢移动的弱目标，因为传统MTI方法可能抑制所需的慢速移动弱目标。

Method: 提出了一种基于Go分解（Godec）框架的新型杂波抑制方案，该方案利用了不同雷达扫描的距离-速度图的低秩和稀疏特性。

Result: 与传统的基于MTI的方案相比，在存在掩蔽效应的情况下，基于Godec杂波抑制的目标检测方案能够可靠地检测出缓慢移动的弱目标。此外，还进行了时间消耗比较，并揭示了虚警单元数量、检测概率和收敛迭代次数之间的权衡。

Conclusion: 提出的Godec杂波抑制方案能够可靠地检测缓慢移动的弱目标，但牺牲了时间复杂度。研究结果为参数设置提供了指导，并通过实验验证了其有效性。

Abstract: Reliable slow-moving weak target detection in complicated environments is
challenging due to the masking effects from the surrounding strong reflectors.
The traditional Moving Target Indication (MTI) may suppress the echoes from not
only the static interference objects (IOs), but also the desired slow-moving
weak target. According to the low-rank and sparse properties of the
range-velocity maps across different radar scans, a novel clutter suppression
scheme based on the Go decomposition (Godec) framework is proposed in this
paper. The simulation results show that with the existence of masking effects,
the target detection scheme based on Godec clutter suppression can reliably
detect the slow-moving weak target, compared to the traditional MTI-based
scheme. Besides, the time consumption comparison is conducted, demonstrating
that the proposed solution is one that sacrifices time complexity in exchange
for enhanced reliability. Additionally, the tradeoffs among the number of false
alarm cells, the detection probability and the iteration times for convergence
have been revealed, guiding parameter settings of the proposed solution in
practical applications. Experiment validation is also conducted to verify the
proposed solution, providing further insight into the scenarios where the
solution is most applicable.

</details>


### [251] [Relative Localization of UAV Swarms in GNSS-Denied Conditions](https://arxiv.org/abs/2509.04412)
*Guangyu Lei,Yuqi Ping,Tianhao Liang,Huahao Ding,Tingting Zhang*

Main category: eess.SP

TL;DR: 该论文提出了一种基于聚类的无人机集群相对定位框架，解决了在GNSS受限环境下大规模集群定位的误差和计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 在GNSS受限环境中，无人机集群的相对定位对于应急救援和战场侦察至关重要，而现有方法存在显著的定位误差。

Method: 首先，利用谱聚类将无人机集群划分为不同的子集，然后通过矩阵填充和多维缩标得到高精度的相对坐标，最后通过集群间的锚点融合创建全局地图。该方法还采用了OTFS作为ISAC系统的通信和测距手段。

Result: 实验结果表明，该方法降低了无人机大规模集群的定位误差和测距信息丢失率，并分析了信号参数对通信和定位性能的影响。

Conclusion: 所提出的基于聚类的框架能够有效提高无人机大规模集群在GNSS受限环境下的相对定位精度，并揭示了通信与定位性能之间的相互作用。

Abstract: Relative localization of unmanned aerial vehicle (UAV) swarms in global
navigation satellite system (GNSS) denied environments is essential for
emergency rescue and battlefield reconnaissance. Existing methods suffer from
significant localization errors among UAVs due to packet loss and high
computational complexity in large swarms. This paper proposes a
clustering-based framework where the UAVs simultaneously use communication
signals for channel estimation and ranging. Firstly, the spectral clustering is
utilized to divide the UAV swarm into different sub-clusters, where matrix
completion and multidimensional scaling yield high-precision relative
coordinates. Subsequently, a global map is created by the inter-cluster anchor
fusion. A case study of UAV integrated communication and sensing (ISAC) system
is presented, where the Orthogonal Time Frequency Space (OTFS) is adopted for
ranging and communication. Experimental results show that the proposed method
reduces localization errors in large swarms and loss of range information. It
also explores the impact of signal parameters on communication and
localization, highlighting the interplay between communication and localization
performance.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [252] [LuxDiT: Lighting Estimation with Video Diffusion Transformer](https://arxiv.org/abs/2509.03680)
*Ruofan Liang,Kai He,Zan Gojcic,Igor Gilitschenski,Sanja Fidler,Nandita Vijaykumar,Zian Wang*

Main category: cs.GR

TL;DR: 该研究提出了一种名为LuxDiT的新方法，用于从单个图像或视频估计场景光照，通过微调视频扩散转换器来生成HDR环境图。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的方法受限于真实HDR环境地图的稀缺性和多样性不足；生成模型虽然在图像合成方面表现强大，但在光照估计方面仍面临挑战，因为这需要推断间接视觉线索、全局上下文以及恢复高动态范围输出。

Method: 提出LuxDiT方法，通过微调视频扩散转换器生成条件于视觉输入的HDR环境图。在包含多样化光照条件的合成数据集上进行训练，并采用低秩适配微调策略，在收集的HDR全景图数据集上进行训练，以提高输入与预测环境图之间的语义对齐。

Result: 该方法能够从间接视觉线索推断光照，并有效泛化到真实世界场景，生成具有逼真角高频细节的准确光照预测。

Conclusion: LuxDiT在定量和定性评估中均优于现有技术，成功解决了从单张图像或视频估计场景光照的长期挑战。

Abstract: Estimating scene lighting from a single image or video remains a longstanding
challenge in computer vision and graphics. Learning-based approaches are
constrained by the scarcity of ground-truth HDR environment maps, which are
expensive to capture and limited in diversity. While recent generative models
offer strong priors for image synthesis, lighting estimation remains difficult
due to its reliance on indirect visual cues, the need to infer global
(non-local) context, and the recovery of high-dynamic-range outputs. We propose
LuxDiT, a novel data-driven approach that fine-tunes a video diffusion
transformer to generate HDR environment maps conditioned on visual input.
Trained on a large synthetic dataset with diverse lighting conditions, our
model learns to infer illumination from indirect visual cues and generalizes
effectively to real-world scenes. To improve semantic alignment between the
input and the predicted environment map, we introduce a low-rank adaptation
finetuning strategy using a collected dataset of HDR panoramas. Our method
produces accurate lighting predictions with realistic angular high-frequency
details, outperforming existing state-of-the-art techniques in both
quantitative and qualitative evaluations.

</details>


### [253] [Memory Optimization for Convex Hull Support Point Queries](https://arxiv.org/abs/2509.03753)
*Michael Greer*

Main category: cs.GR

TL;DR: 本文提出改进凸包的内存布局以加速支撑点查询。


<details>
  <summary>Details</summary>
Motivation: 支撑点查询是碰撞检测算法的基础，对查询速度有需求。

Method: 通过改进凸包的内存布局来优化计算时间。

Result: 在支撑点查询方面实现了显著的加速，加速效果取决于凸包的顶点数。

Conclusion: 所提出的内存布局优化方法能够有效提升凸包支撑点查询的计算效率。

Abstract: This paper evaluates several improvements to the memory layout of convex
hulls to improve computation times for support point queries. The support point
query is a fundamental part of common collision algorithms, and the work
presented achieves a significant speedup depending on the number of vertices of
the convex hull.

</details>


### [254] [ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction](https://arxiv.org/abs/2509.03775)
*Sankeerth Durvasula,Sharanshangar Muhunthan,Zain Moustafa,Richard Chen,Ruofan Liang,Yushi Guan,Nilesh Ahuja,Nilesh Jain,Selvakumar Panneer,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: ContraGS通过使用可学习的码本和贝叶斯推理（MCMC采样）来压缩3D高斯表示，从而在不显著降低模型质量的情况下，在训练期间大幅减少内存消耗并加速训练和渲染过程。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯表示（3DGS）在提高模型质量时需要大量高斯点，这会显著增加GPU内存消耗，导致训练和渲染效率低下。

Method: ContraGS引入了一种在压缩的3D高斯表示上直接进行训练的方法。它利用码本紧凑地存储高斯参数，并通过将参数估计视为贝叶斯推理问题来解决学习非可微分参数的挑战，具体实现方式是使用MCMC采样。

Result: ContraGS在训练期间将峰值内存使用量平均减少了3.49倍，并将训练和渲染速度平均分别提高了1.36倍和1.88倍，同时保持了接近最先进水平的模型质量。

Conclusion: ContraGS成功地实现了在压缩表示下对3D高斯进行训练，解决了内存和速度瓶颈，同时保持了高质量的输出。

Abstract: 3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world
scenes with high quality and real-time rendering. Typically, a higher quality
representation can be achieved by using a large number of 3D Gaussians.
However, using large 3D Gaussian counts significantly increases the GPU device
memory for storing model parameters. A large model thus requires powerful GPUs
with high memory capacities for training and has slower training/rendering
latencies due to the inefficiencies of memory access and data movement. In this
work, we introduce ContraGS, a method to enable training directly on compressed
3DGS representations without reducing the Gaussian Counts, and thus with a
little loss in model quality. ContraGS leverages codebooks to compactly store a
set of Gaussian parameter vectors throughout the training process, thereby
significantly reducing memory consumption. While codebooks have been
demonstrated to be highly effective at compressing fully trained 3DGS models,
directly training using codebook representations is an unsolved challenge.
ContraGS solves the problem of learning non-differentiable parameters in
codebook-compressed representations by posing parameter estimation as a
Bayesian inference problem. To this end, ContraGS provides a framework that
effectively uses MCMC sampling to sample over a posterior distribution of these
compressed representations. With ContraGS, we demonstrate that ContraGS
significantly reduces the peak memory during training (on average 3.49X) and
accelerated training and rendering (1.36X and 1.88X on average, respectively),
while retraining close to state-of-art quality.

</details>


### [255] [TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface Scattering for Perlin Distributed Heterogeneous Media](https://arxiv.org/abs/2509.04047)
*Ashish Tiwari,Satyam Bhardwaj,Yash Bachwana,Parag Sarvoday Sahu,T. M. Feroz Ali,Bhargava Chintalapati,Shanmuganathan Raman*

Main category: cs.GR

TL;DR: 该研究提出了一种名为TensoIS的基于学习的前馈框架，用于从稀疏的多视角图像估计异构介质的次表面散射参数，该参数使用分形Perlin噪声建模。


<details>
  <summary>Details</summary>
Motivation: 从图像估计异构介质的散射参数是一个严峻的挑战，现有方法在处理异构性方面存在局限性，而基于学习的方法通常假设介质是均匀的。

Method: 提出了一种名为TensoIS的基于学习的前馈框架，该框架使用可学习的低秩张量分量来表示散射体，而不是直接预测3D散射参数体。该方法首先使用分形Perlin噪声生成一个名为HeteroSynth的合成数据集，其中包含光照逼真的异构介质图像。

Result: TensoIS在HeteroSynth测试集中未见过的新异构变体、来自开源真实体积模拟的烟雾和云几何体以及一些真实世界样本上进行了评估，证明了其在逆散射方面的有效性。

Conclusion: 该研究首次探索了使用Perlin噪声分布来模拟真实世界中的异构散射，并提出了一种有效的学习框架TensoIS，能够从稀疏的多视角图像中估计这些参数。

Abstract: Estimating scattering parameters of heterogeneous media from images is a
severely under-constrained and challenging problem. Most of the existing
approaches model BSSRDF either through an analysis-by-synthesis approach,
approximating complex path integrals, or using differentiable volume rendering
techniques to account for heterogeneity. However, only a few studies have
applied learning-based methods to estimate subsurface scattering parameters,
but they assume homogeneous media. Interestingly, no specific distribution is
known to us that can explicitly model the heterogeneous scattering parameters
in the real world. Notably, procedural noise models such as Perlin and Fractal
Perlin noise have been effective in representing intricate heterogeneities of
natural, organic, and inorganic surfaces. Leveraging this, we first create
HeteroSynth, a synthetic dataset comprising photorealistic images of
heterogeneous media whose scattering parameters are modeled using Fractal
Perlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a
learning-based feed-forward framework to estimate these Perlin-distributed
heterogeneous scattering parameters from sparse multi-view image observations.
Instead of directly predicting the 3D scattering parameter volume, TensoIS uses
learnable low-rank tensor components to represent the scattering volume. We
evaluate TensoIS on unseen heterogeneous variations over shapes from the
HeteroSynth test set, smoke and cloud geometries obtained from open-source
realistic volumetric simulations, and some real-world samples to establish its
effectiveness for inverse scattering. Overall, this study is an attempt to
explore Perlin noise distribution, given the lack of any such well-defined
distribution in literature, to potentially model real-world heterogeneous
scattering in a feed-forward manner.

</details>


### [256] [SMooGPT: Stylized Motion Generation using Large Language Models](https://arxiv.org/abs/2509.04058)
*Lei Zhong,Yi Yang,Changjian Li*

Main category: cs.GR

TL;DR: 本研究提出了一种新的风格化运动生成方法SMooGPT，它利用大型语言模型（LLM）处理以身体部位为中心的文本描述，实现了更高的可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在风格化运动生成方面存在可解释性差、控制性弱、泛化能力有限以及内容受限（例如仅限于“行走”）等问题。

Method: 提出了一种基于推理-组合-生成（reasoning-composition-generation）的新视角，并利用身体部位文本空间作为中间表示。开发了一个名为SMooGPT的微调LLM，充当推理者、组合者和生成器，在身体部位文本空间中执行操作。

Result: SMooGPT在可解释性、运动控制、内容与风格冲突解决和泛化能力方面表现出色，尤其在纯文本驱动的风格化运动生成方面效果显著。

Conclusion: SMooGPT通过利用LLM的开放词汇能力和身体部位文本空间的中间表示，有效解决了风格化运动生成的挑战，并在实验和用户研究中得到了验证。

Abstract: Stylized motion generation is actively studied in computer graphics,
especially benefiting from the rapid advances in diffusion models. The goal of
this task is to produce a novel motion respecting both the motion content and
the desired motion style, e.g., ``walking in a loop like a Monkey''. Existing
research attempts to address this problem via motion style transfer or
conditional motion generation. They typically embed the motion style into a
latent space and guide the motion implicitly in a latent space as well. Despite
the progress, their methods suffer from low interpretability and control,
limited generalization to new styles, and fail to produce motions other than
``walking'' due to the strong bias in the public stylization dataset. In this
paper, we propose to solve the stylized motion generation problem from a new
perspective of reasoning-composition-generation, based on our observations: i)
human motion can often be effectively described using natural language in a
body-part centric manner, ii) LLMs exhibit a strong ability to understand and
reason about human motion, and iii) human motion has an inherently
compositional nature, facilitating the new motion content or style generation
via effective recomposing. We thus propose utilizing body-part text space as an
intermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a
reasoner, composer, and generator when generating the desired stylized motion.
Our method executes in the body-part text space with much higher
interpretability, enabling fine-grained motion control, effectively resolving
potential conflicts between motion content and style, and generalizes well to
new styles thanks to the open-vocabulary ability of LLMs. Comprehensive
experiments and evaluations, and a user perceptual study, demonstrate the
effectiveness of our approach, especially under the pure text-driven stylized
motion generation.

</details>


### [257] [Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion](https://arxiv.org/abs/2509.04145)
*Dongliang Cao,Guoxing Sun,Marc Habermann,Florian Bernard*

Main category: cs.GR

TL;DR: 该研究提出了一种结合了身份特定渲染和基于扩散生成模型的方法，用于生成具有高真实感和真实姿态相关变形的动态人体虚拟形象。


<details>
  <summary>Details</summary>
Motivation: 现有的方法要么局限于单一个人，要么生成的虚拟形象渲染质量低且无法捕捉姿态相关的变形。

Method: 采用两阶段管线：首先优化身份特定的UNets来捕捉姿态相关的变形，然后训练一个超扩散模型来生成网络权重，从而实现实时、可控的动态人体虚拟形象渲染。

Result: 在大型跨身份多视角视频数据集上进行评估，结果表明该方法优于现有的最先进的人体虚拟形象生成方法。

Conclusion: 所提出的方法能够生成高保真度且具有真实姿态相关变形的动态人体虚拟形象，克服了现有方法的局限性。

Abstract: Creating human avatars is a highly desirable yet challenging task. Recent
advancements in radiance field rendering have achieved unprecedented
photorealism and real-time performance for personalized dynamic human avatars.
However, these approaches are typically limited to person-specific rendering
models trained on multi-view video data for a single individual, limiting their
ability to generalize across different identities. On the other hand,
generative approaches leveraging prior knowledge from pre-trained 2D diffusion
models can produce cartoonish, static human avatars, which are animated through
simple skeleton-based articulation. Therefore, the avatars generated by these
methods suffer from lower rendering quality compared to person-specific
rendering methods and fail to capture pose-dependent deformations such as cloth
wrinkles. In this paper, we propose a novel approach that unites the strengths
of person-specific rendering and diffusion-based generative modeling to enable
dynamic human avatar generation with both high photorealism and realistic
pose-dependent deformations. Our method follows a two-stage pipeline: first, we
optimize a set of person-specific UNets, with each network representing a
dynamic human avatar that captures intricate pose-dependent deformations. In
the second stage, we train a hyper diffusion model over the optimized network
weights. During inference, our method generates network weights for real-time,
controllable rendering of dynamic human avatars. Using a large-scale,
cross-identity, multi-view video dataset, we demonstrate that our approach
outperforms state-of-the-art human avatar generation methods.

</details>


### [258] [Massively-Parallel Implementation of Inextensible Elastic Rods Using Inter-block GPU Synchronization](https://arxiv.org/abs/2509.04277)
*Przemyslaw Korzeniowski,Niels Hald,Fernando Bello*

Main category: cs.GR

TL;DR: 该研究提出了一种用于模拟Cosserat杆的超大规模并行GPU实现，其原始可伸缩版本速度提高了40倍，不可伸缩版本速度提高了15倍，实现了实时模拟。


<details>
  <summary>Details</summary>
Motivation: Cosserat杆能够模拟弯曲、拉伸和扭转等多种形变，可用于模拟线、绳、导管等，但原始模型计算成本高，难以实现实时物理仿真。

Method: 提出了一种超大规模并行GPU实现，采用CUDA可扩展编程模型和块间同步，实现单核启动多次物理时间步，并提出了Cosserat杆的不可拉伸变体。

Result: 原始可伸缩CoRdE模型速度提高了x40.0，不可拉伸CoRdE模型平均加速比为x15.11。模拟心血管应用中的导管/导丝对（2x512个Cosserat单元）实现了13.5倍的性能提升，达到了触觉交互速率（0.5-1kHz）的实时仿真。

Conclusion: 该GPU实现能够显著提高Cosserat杆仿真的性能，为触觉交互速率下的实时仿真提供了可能。

Abstract: An elastic rod is a long and thin body able to sustain large global
deformations, even if local strains are small. The Cosserat rod is a non-linear
elastic rod with an oriented centreline, which enables modelling of bending,
stretching and twisting deformations. It can be used for physically-based
computer simulation of threads, wires, ropes, as well as flexible surgical
instruments such as catheters, guidewires or sutures. We present a
massively-parallel implementation of the original CoRdE model as well as our
inextensible variation. By superseding the CUDA Scalable Programming Model and
using inter-block synchronization, we managed to simulate multiple physics
time-steps per single kernel launch utilizing all the GPU's streaming
multiprocessors. Under some constraints, this results in nearly constant
computation time, regardless of the number of Cosserat elements simulated. When
executing 10 time-steps per single kernel launch, our implementation of the
original, extensible CoRdE was x40.0 faster. In a number of tests, the GPU
implementation of our inextensible CoRdE modification achieved an average
speed-up of x15.11 over the corresponding CPU version. Simulating a
catheter/guidewire pair (2x512 Cosserat elements) in a cardiovascular
application resulted in a 13.5 fold performance boost, enabling for accurate
real-time simulation at haptic interactive rates (0.5-1kHz).

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [259] [Hardware-Aware Data and Instruction Mapping for AI Tasks: Balancing Parallelism, I/O and Memory Tradeoffs](https://arxiv.org/abs/2509.03846)
*Md Rownak Hossain Chowdhury,Mostafizur Rahman*

Main category: cs.AR

TL;DR: 该框架通过生成统一的指令和数据流，使硬件能够自主执行操作和路由信息，从而实现深度学习推理的映射，减少对I/O、片外内存和主机控制的依赖。


<details>
  <summary>Details</summary>
Motivation: 深度学习推理需要高效的计算和通信，目前的框架依赖主机控制和片外内存，效率低下。

Method: 提出一个映射框架，利用神经网络的可预测行为，提前规划计算和通信。在可编程的消息驱动计算架构上，利用细粒度消息传递，实现数据局部性、权重复用、片内组播和分阶段归约。

Result: 在VGG-19上，该框架实现了88%到92%的高利用率，97%的消息在内部生成，89%的时间用于片内传输。计算吞吐量超过1 TFLOP/s，流量减少高达100 MB/层。

Conclusion: 基于消息流的计算是有效的，该映射框架通过协调数据和指令流，能够实现这种执行方式。

Abstract: We introduce a mapping framework for deep learning inference that takes
advantage of predictable neural network behavior to plan both computation and
communication ahead of time. The framework generates a unified stream of
instructions and data, enabling the hardware to execute operations and route
information on its own, without frequent involvement from the host and with
minimal off-chip memory use. This naturally reduces reliance on I/O, off-chip
memory, and host control. By leveraging fine-grained message passing on a
programmable, message-based compute architecture, the framework keeps data
movement local and coordinates computation across the array using techniques
such as stationary-weight reuse, in-array multicasting, and staged reductions.
Applied to VGG-19, the framework sustains high utilization (88 to 92 percent),
with over 97 percent of messages generated internally and nearly 89 percent of
time consumed on-chip transfers. Computation throughput scales beyond 1 TFLOP/s
on larger arrays, while traffic reductions from reuse and local aggregation
reach up to 100 MB per layer. Overall, the results highlight the effectiveness
of streaming-based computation and show how our mapper enables this execution
style by tightly coordinating data and instruction flow across the hardware.

</details>


### [260] [Real Time FPGA Based CNNs for Detection, Classification, and Tracking in Autonomous Systems: State of the Art Designs and Optimizations](https://arxiv.org/abs/2509.04153)
*Safa Mohammed Sali,Mahmoud Meribout,Ashiyana Abdul Majeed*

Main category: cs.AR

TL;DR: 本篇论文是关于在FPGA上部署CNN进行对象检测、分类和跟踪的最新进展的全面回顾。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶汽车、机器人和监控等领域对实时计算机视觉应用的需求不断增长，FPGA因其可重构性、低功耗和确定性延迟，已成为GPU和ASIC的有力替代品。

Method: 对基于CNN的视觉任务的最新FPGA实现进行批判性审查，涵盖算法创新、硬件加速技术以及剪枝、量化和稀疏感知方法等优化策略的集成，以在硬件约束内最大化性能。

Result: 评估了包括经典LUT-DSP架构、SoC FPGA和ACAP在内的现代FPGA平台，并回顾了Vitis AI、FINN和Intel FPGA AI Suite等软件开发工具。还讨论了结合GPU和FPGA进行AI推理协同加速的混合架构，以及硬件/软件协同设计、数据流优化和流水线处理技术。

Conclusion: 本综述为研究人员和工程师提供了在资源受限的边缘和嵌入式应用中针对FPGA部署开发下一代、节能、高性能视觉系统的见解。

Abstract: This paper presents a comprehensive review of recent advances in deploying
convolutional neural networks (CNNs) for object detection, classification, and
tracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand
for real-time computer vision applications in domains such as autonomous
vehicles, robotics, and surveillance, FPGAs have emerged as a powerful
alternative to GPUs and ASICs due to their reconfigurability, low power
consumption, and deterministic latency. We critically examine state-of-the-art
FPGA implementations of CNN-based vision tasks, covering algorithmic
innovations, hardware acceleration techniques, and the integration of
optimization strategies like pruning, quantization, and sparsity-aware methods
to maximize performance within hardware constraints. This survey also explores
the landscape of modern FPGA platforms, including classical LUT-DSP based
architectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration
Platforms (ACAPs), comparing their capabilities in handling deep learning
workloads. Furthermore, we review available software development tools such as
Vitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the
design and deployment of AI models on FPGAs. The paper uniquely discusses
hybrid architecture that combine GPUs and FPGAs for collaborative acceleration
of AI inference, addressing challenges related to energy efficiency and
throughput. Additionally, we highlight hardware-software co-design practices,
dataflow optimizations, and pipelined processing techniques essential for
real-time inference on resource-constrained devices. Through this survey,
researchers and engineers are equipped with insights to develop
next-generation, power-efficient, and high-performance vision systems optimized
for FPGA deployment in edge and embedded applications.

</details>


### [261] [Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs and Optimizations](https://arxiv.org/abs/2509.04162)
*Safa Mohammed Sali,Mahmoud Meribout,Ashiyana Abdul Majeed*

Main category: cs.AR

TL;DR: 该论文全面回顾了在FPGA上部署Transformer和视觉-语言模型（VLMs）的设计权衡、优化策略和实现挑战，探讨了设备选择、内存、数据流、量化、稀疏性、工具链、异构计算、注意力机制、压缩和覆盖等关键因素，并指出了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: Transformer和VLMs在计算机视觉和多模态AI领域表现出色，但在计算复杂性、内存占用和数据访问模式方面存在挑战，难以在资源受限环境中部署。FPGA因其可重构性、并行性和能效优势，为这些模型提供了有吸引力的硬件平台。

Method: 本文回顾了在FPGA上进行Transformer和VLM推理的设计权衡、优化策略和实现挑战。探讨了设备选择、内存子系统、数据流编排、量化、稀疏性、工具链选择、异构计算平衡、交叉注意力内存管理、硬件-算法协同设计、注意力机制、压缩、覆盖、运行时灵活性、验证开销和缺乏标准化基准等问题。

Result: 本文分析了FPGA上Transformer和VLM推理的关键因素，包括设备选择、内存、数据流、量化、稀疏性、工具链、异构计算、注意力机制、压缩和覆盖等。讨论了硬件-算法协同设计、运行时灵活性、验证开销和缺乏标准化基准等实际问题。

Conclusion: 为了实现高利用率和可预测的性能，需要朝着可扩展、可移植和可重构的FPGA解决方案发展，这些解决方案能够适应不断变化的If model architectures。本文为将先进的多模态AI模型有效地部署到FPGA提供了技术基础和前瞻性视角。

Abstract: Transformers and vision-language models (VLMs) have emerged as dominant
architectures in computer vision and multimodal AI, offering state-of-the-art
performance in tasks such as image classification, object detection, visual
question answering, and caption generation. However, their high computational
complexity, large memory footprints, and irregular data access patterns present
significant challenges for deployment in latency- and power-constrained
environments. Field-programmable gate arrays (FPGAs) provide an attractive
hardware platform for such workloads due to their reconfigurability,
fine-grained parallelism, and potential for energy-efficient acceleration. This
paper presents a comprehensive review of design trade-offs, optimization
strategies, and implementation challenges for FPGA-based inference of
transformers and VLMs. We examine critical factors such as device-class
selection, memory subsystem constraints, dataflow orchestration, quantization
strategies, sparsity exploitation, and toolchain choices, alongside
modality-specific issues unique to VLMs, including heterogeneous compute
balancing and cross-attention memory management. Additionally, we discuss
emerging trends in hardware-algorithm co-design, highlighting innovations in
attention mechanisms, compression, and modular overlays to improve efficiency
and adaptability. Practical issues such as runtime flexibility, verification
overhead, and the absence of standardized FPGA multimodal benchmarks are also
considered. Finally, we outline future directions toward scalable, portable,
and reconfigurable FPGA solutions that adapt to evolving model architectures
while sustaining high utilization and predictable performance. This synthesis
offers both a technical foundation and a forward-looking perspective to help
bridge the gap between advanced multimodal AI models and efficient FPGA
deployment.

</details>


### [262] [Real-time Object Detection and Associated Hardware Accelerators Targeting Autonomous Vehicles: A Review](https://arxiv.org/abs/2509.04173)
*Safa Sali,Anis Meribout,Ashiyana Majeed,Mahmoud Meribout,Juan Pablo,Varun Tiwari,Asma Baobaid*

Main category: cs.AR

TL;DR: 本综述论文全面概述了用于自动驾驶汽车的实时目标检测算法及其硬件加速器。


<details>
  <summary>Details</summary>
Motivation: 实时目标检测对于自动驾驶汽车的安全至关重要，但现有研究与商业应用之间存在差距。

Method: 本研究回顾了最新的目标检测算法，并探讨了其在硬件加速器（如GPU和ASIC）上的应用，同时分析了商业自动驾驶汽车中算法和硬件细节披露的限制。

Result: 卷积神经网络（CNN）等AI算法已取代传统算法，并在硬件加速器的支持下实现了数百帧/秒（fps）的吞吐量，但仍需改进以满足自动驾驶汽车的需求。

Conclusion: 该论文旨在弥合学术研究与商业自动驾驶汽车技术之间的差距，为未来自动驾驶汽车的设计提供参考。

Abstract: The efficiency of object detectors depends on factors like detection
accuracy, processing time, and computational resources. Processing time is
crucial for real-time applications, particularly for autonomous vehicles (AVs),
where instantaneous responses are vital for safety. This review paper provides
a concise yet comprehensive survey of real-time object detection (OD)
algorithms for autonomous cars delving into their hardware accelerators (HAs).
Non-neural network-based algorithms, which use statistical image processing,
have been entirely substituted by AI algorithms, such as different models of
convolutional neural networks (CNNs). Their intrinsically parallel features led
them to be deployable into edge-based HAs of various types, where GPUs and, to
a lesser extent, ASIC (application-specific integrated circuit) remain the most
widely used. Throughputs of hundreds of frames/s (fps) could be reached;
however, handling object detection for all the cameras available in a typical
AV requires further hardware and algorithmic improvements. The intensive
competition between AV providers has limited the disclosure of algorithms,
firmware, and even hardware platform details. This remains a hurdle for
researchers, as commercial systems provide valuable insights while academics
undergo lengthy training and testing on restricted datasets and road scenarios.
Consequently, many AV research papers may not be reflected in end products,
being developed under limited conditions. This paper surveys state-of-the-art
OD algorithms and aims to bridge the gap with technologies in commercial AVs.
To our knowledge, this aspect has not been addressed in earlier surveys. Hence,
the paper serves as a tangible reference for researchers designing future
generations of vehicles, expected to be fully autonomous for comfort and
safety.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [263] [Gravity Well Echo Chamber Modeling With An LLM-Based Confirmation Bias Model](https://arxiv.org/abs/2509.03832)
*Joseph Jackson,Georgiy Lapin,Jeremy E. Thompson*

Main category: cs.SI

TL;DR: 本文提出了一个结合了确认偏差的引力井模型，以更准确地识别社交媒体中的信息茧房，并帮助遏制虚假信息的传播。


<details>
  <summary>Details</summary>
Motivation: 现有的回音室模型忽略了个人确认偏差的影响，而确认偏差是影响错误信息传播的关键因素。

Method: 在现有的“引力井”模型的基础上，引入了一个动态确认偏差变量，该变量根据用户对不同观点的帖子反应的帖子历史来衡量用户的易感性，并动态调整引力井的拉力强度。

Result: 通过在19个Reddit社区中验证，新的模型能更准确地识别信息茧房，并揭示出信息健康度的社区级别标记。

Conclusion: 该研究提出了一个系统性捕捉确认偏差在网络群体动态中作用的框架，能够更有效地识别信息茧房，并通过标记这些高风险环境来遏制错误信息的传播。

Abstract: Social media echo chambers play a central role in the spread of
misinformation, yet existing models often overlook the influence of individual
confirmation bias. An existing model of echo chambers is the "gravity well"
model, which creates an analog between echo chambers and spatial gravity wells.
We extend this established model by introducing a dynamic confirmation bias
variable that adjusts the strength of pull based on a user's susceptibility to
belief-reinforcing content. This variable is calculated for each user through
comparisons between their posting history and their responses to posts of a
wide range of viewpoints.
  Incorporating this factor produces a confirmation-bias-integrated gravity
well model that more accurately identifies echo chambers and reveals
community-level markers of information health. We validated the approach on
nineteen Reddit communities, demonstrating improved detection of echo chambers.
  Our contribution is a framework for systematically capturing the role of
confirmation bias in online group dynamics, enabling more effective
identification of echo chambers. By flagging these high-risk environments, the
model supports efforts to curb the spread of misinformation at its most common
points of amplification.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [264] [SAMVAD: A Multi-Agent System for Simulating Judicial Deliberation Dynamics in India](https://arxiv.org/abs/2509.03793)
*Prathamesh Devadiga,Omkaar Jayadev Shetty,Pooja Agarwal*

Main category: cs.MA

TL;DR: 该论文介绍了一个名为SAMVAD的多智能体系统（MAS），用于模拟印度司法系统的审议过程。


<details>
  <summary>Details</summary>
Motivation: 由于伦理和实际限制，对司法机构进行实证研究存在障碍，因此需要模拟司法审议过程。

Method: 使用大型语言模型（LLMs）创建代表法官、检察官、辩护律师和审判员的智能体。该系统集成了检索增强生成（RAG）功能，并以印度法律文件为知识库，以生成符合法律的论证和指示，并引用来源。审判员智能体通过迭代审议达成共识。本研究详细介绍了系统架构、智能体通信、RAG流程、模拟工作流程和评估计划。

Result: 该系统提供了一个可配置且可解释的MAS平台，用于探索法律推理和司法模拟中的群体决策动态，特别是在印度法律背景下，并通过RAG增强了可验证的法律依据。

Conclusion: SAMVAD系统为理解司法审议的复杂性提供了一种创新的模拟方法，并为未来的法律研究提供了一个有价值的工具。

Abstract: Understanding the complexities of judicial deliberation is crucial for
assessing the efficacy and fairness of a justice system. However, empirical
studies of judicial panels are constrained by significant ethical and practical
barriers. This paper introduces SAMVAD, an innovative Multi-Agent System (MAS)
designed to simulate the deliberation process within the framework of the
Indian justice system.
  Our system comprises agents representing key judicial roles: a Judge, a
Prosecution Counsel, a Defense Counsel, and multiple Adjudicators (simulating a
judicial bench), all powered by large language models (LLMs). A primary
contribution of this work is the integration of Retrieval-Augmented Generation
(RAG), grounded in a domain-specific knowledge base of landmark Indian legal
documents, including the Indian Penal Code and the Constitution of India. This
RAG functionality enables the Judge and Counsel agents to generate legally
sound instructions and arguments, complete with source citations, thereby
enhancing both the fidelity and transparency of the simulation.
  The Adjudicator agents engage in iterative deliberation rounds, processing
case facts, legal instructions, and arguments to reach a consensus-based
verdict. We detail the system architecture, agent communication protocols, the
RAG pipeline, the simulation workflow, and a comprehensive evaluation plan
designed to assess performance, deliberation quality, and outcome consistency.
  This work provides a configurable and explainable MAS platform for exploring
legal reasoning and group decision-making dynamics in judicial simulations,
specifically tailored to the Indian legal context and augmented with verifiable
legal grounding via RAG.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [265] [Challenges in Scaling ScAlN Bulk Acoustic Wave Filters toward Ku-Band Frequencies](https://arxiv.org/abs/2509.03679)
*Sinwoo Cho,Byeongjin Kim,Lezli Matto,Omar Barrera,Pietro Simeoni,Yinan Wang,Michael Liao,Tzu-Hsuan Hsu,Jack Kramer,Matteo Rinaldi,Mark S. Goorsky,Ruochen Lu*

Main category: physics.app-ph

TL;DR: 本文介绍了一种基于ScAlN FBAR的11.7 GHz滤波器，并分析了其在高频应用中面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 为了开发用于毫米波应用的新型低损耗声波滤波器，需要在高频下实现高性能的FBAR器件。

Method: 使用ScAlN薄膜和Pt电极制作了11.7 GHz的FBAR滤波器，并分析了导致插入损耗和带宽受限的因素，特别是与器件在高频下性能相关的三个关键挑战：压电薄膜质量、电极系列电阻和薄膜残余应力。

Result: 成功实现了11.7 GHz的50Ω单层ScAlN FBAR滤波器，具有4.0%的3 dB分数带宽和大于23.1 dB的带外抑制。然而，器件的插入损耗为6.8 dB，这归因于谐振器的品质因数（Q值）有限。

Conclusion: 高频ScAlN FBAR滤波器的性能受限于薄膜堆栈厚度减小带来的压电薄膜晶体质量下降、超薄电极的系列电阻增加以及残余薄膜应力对器件尺寸和结构完整性的限制。本研究建立了这些制造层面的挑战与器件性能之间的关系，为未来开发低损耗毫米波声波滤波器提供了关键见解。

Abstract: This paper reports an 11.7 GHz compact filter based on Scandium Aluminum
Nitride (ScAlN) Film Bulk Acoustic Resonators (FBARs) and provides a detailed
analysis of the fundamental challenges that limit performance when scaling to
higher frequencies. A 50 {\Omega} ladder filter based on single-layer thin film
ScAlN with Platinum (Pt) electrodes was demonstrated at 11.7 GHz with a 3 dB
fractional bandwidth (FBW) of 4.0% and an out of band rejection greater than
23.1 dB. However, the filter exhibits a moderate insertion loss (IL) of 6.8 dB,
which is attributed to a limited Quality (Q) factor in the constituent
resonators. Consequently, we identify and analyze three primary challenges in
frequency scaling ScAlN FBARs with thinner film stacks: 1) degradation of
piezoelectric thin-film crystal quality, 2) increased series resistance in
ultra-thin electrodes, and 3) residual film stress that limits device size and
structural integrity. By establishing a clear relationship between these
fabrication-level challenges and the resulting device performance, this work
provides critical insights for the future development of low-loss acoustic
filters for millimeter-wave applications.

</details>


### [266] [Polarization control via artificial optical nonlinearity in dielectric metasurfaces](https://arxiv.org/abs/2509.03752)
*Fuyong Yue,Giacomo Balistreri,Nicola Montaut,Fabrizio Riminucci,Andrea Toma,Riccardo Piccoli,Stefano Cabrini,Roberto Morandotti,Luca Razzari*

Main category: physics.app-ph

TL;DR: Metasurfaces allow engineering nonlinear optical interactions by controlling light's phase, amplitude, and polarization. This paper studies the tensorial nature of nonlinearity in dielectric metasurfaces and establishes a model for designing amorphous silicon-based metasurfaces for third harmonic generation, demonstrating versatile control over emitted light.


<details>
  <summary>Details</summary>
Motivation: Current studies on nonlinear metasurfaces primarily focus on phase control, but investigating the tensorial nature of nonlinearity and its effect on polarization is crucial for applications like nonlinear vector beam generation and polarization imaging.

Method: This paper studies the artificial optical nonlinearity of a dielectric metasurface arising from its meta-atom symmetry, modeling the third-order nonlinear behavior by incorporating the polarization degree of freedom. An effective nonlinear medium model was established for designing amorphous silicon-based metasurfaces for third harmonic generation, and quantitative values of the nonlinear susceptibility tensor elements were extracted.

Result: The study establishes a model for designing amorphous silicon-based geometric metasurfaces with tunable third harmonic generation features. The implemented devices demonstrate the capability of dielectric metasurfaces to control the amplitude, phase, and polarization of emitted light.

Conclusion: Dielectric metasurfaces offer a versatile platform for engineering novel nonlinear optical architectures, enabling precise control over emitted light for applications in nonlinear imaging and complex light generation.

Abstract: Nonlinear optical phenomena are generally governed by geometry in matter
systems, as they depend on the spatial arrangement of atoms within materials or
molecules. Metasurfaces, through precisely designed geometries on a
subwavelength scale, allow tailoring the optical response of a material far
beyond its natural properties. Therefore, metasurfaces are highly appealing to
enable the engineering of nonlinear optical interactions at an unprecedented
level. Current studies on nonlinear metasurfaces predominantly focus on the
phase control of the generated light. Nonetheless, investigating the tensorial
nature of the nonlinearity of metasurfaces and its effect on the polarization
of the generated light is critical to fully unlock a range of applications,
such as nonlinear vector beam generation and nonlinear polarization imaging.
Here, we study the artificial optical nonlinearity of a dielectric metasurface
originating from its meta-atom symmetry and describe the third-order nonlinear
behavior by considering the polarization degree of freedom. We establish an
effective nonlinear medium model that serves as a design toolbox for developing
amorphous silicon-based geometric metasurfaces with customizable features in
third harmonic generation. We further extract quantitative values of the
artificial nonlinear susceptibility tensor elements related to the investigated
nonlinear process and geometry. The implemented functional devices demonstrate
the versatility of dielectric metasurfaces in shaping the emitted light in
terms of amplitude, phase, and polarization, for the precise engineering of
novel nonlinear architectures targeting applications in nonlinear imaging and
complex light generation.

</details>


### [267] [Active Dual-Gated Graphene Transistors for Low-Noise, Drift-Stable, and Tunable Chemical Sensing](https://arxiv.org/abs/2509.04137)
*Vinay Kammarchedu,Heshmat Asgharian,Hossein Chenani,Aida Ebrahimi*

Main category: physics.app-ph

TL;DR: 文章介绍了一种新型双栅极石墨烯场效应晶体管（GFET）传感器，通过集成高k栅氧化物和电解质顶栅，并结合实时反馈偏置，实现了信号的放大和噪声的抑制，从而在复杂环境中实现了对多种分析物的超灵敏、实时、无标记检测。


<details>
  <summary>Details</summary>
Motivation: 传统的单栅极GFET在液体环境中传感时存在信号漂移、电荷俘获和信号放大不足等问题，限制了其在化学和生物传感领域的应用。

Method: 提出了一种集成高k氧化铪局部背栅和电解质顶栅的双栅极GFET架构，并结合实时反馈偏置。通过系统评估了七种不同的操作模式，确定了“双模固定”配置为最优。

Result: 最优配置实现了高达20倍的信号增益，信号漂移比传统栅极扫描方法降低了15倍以上，信噪比提高了7倍，并成功应用于神经递质、挥发性有机化合物、环境污染物和蛋白质等多种分析物的检测。此外，还通过PCB集成GFET传感器阵列实现了鲁棒的多路复用检测。

Conclusion: 该双栅极GFET传感平台具有高度的灵活性和稳定性，能够实现在环境和生理条件下对分子目标的实时、无标记检测，在健康监测、食品安全、农业和环境筛查等领域具有广泛的应用前景。

Abstract: Graphene field-effect transistors (GFETs) are among the most promising
platforms for ultrasensitive chemical and biological sensing due to their high
carrier mobility, large surface area, and low intrinsic noise. However,
conventional single-gate GFET sensors in liquid environments suffer from severe
limitations, including signal drift, charge trapping, and insufficient signal
amplification. Here, we introduce a dual-gate GFET architecture that integrates
a high-k hafnium dioxide local back gate with an electrolyte top gate, coupled
with real-time feedback biasing. This design enables capacitive signal
amplification while simultaneously suppressing gate leakage and low-frequency
noise. By systematically evaluating seven distinct operational modes, we
identify the Dual Mode Fixed configuration as optimal, achieving up to 20x
signal gain, > 15x lower drift compared with gate-swept methods, and up to 7x
higher signal to noise ratio across a diverse range of analytes, including
neurotransmitters, volatile organic compounds, environmental contaminants, and
proteins. We further demonstrate robust, multiplexed detection using a
PCB-integrated GFET sensor array, underscoring the scalability and practicality
of the platform for portable, high-throughput sensing in complex environments.
Together, these advances establish a versatile and stable sensing technology
capable of real-time, label-free detection of molecular targets under ambient
and physiological conditions, with broad applicability in health monitoring,
food safety, agriculture, and environmental screening.

</details>


### [268] [Making neural networks understand internal heat transfer using Fourier-transformed thermal diffusion wave fields](https://arxiv.org/abs/2509.04223)
*Pengfei Zhu,Hai Zhang,Clemente Ibarra-Castanedo,Xavier Maldague,Andreas Mandelis*

Main category: physics.app-ph

TL;DR: 提出一种新的赫姆霍兹信息神经网络（HINN）来预测内部温度分布，无需内部测量。


<details>
  <summary>Details</summary>
Motivation: 传统的无损热成像技术难以完全解析地下结构和内部热量分布，现有的热断层扫描技术也只能从每一层拍摄一帧。

Method: 将时域热扩散方程转换为频域的伪赫姆霍兹方程，并将其嵌入学习框架。HINN 利用热场和虚部，并通过逆傅里叶变换将实部和虚部带回时域。通过截断操作提高计算效率，并利用共轭对称原理修复丢弃的数据。

Result: HINN 在预测精度和计算效率方面显著提高，优于最先进的 PINN 和逆热求解器。

Conclusion: HINN 为非侵入式热成像提供了一种新颖的解决方案，可应用于材料科学、生物医学诊断和无损评估等领域。

Abstract: Heat propagation is governed by phonon interactions and mathematically
described by partial differential equations (PDEs), which link thermal
transport to the intrinsic properties of materials. Conventional experimental
techniques infer thermal responses based on surface emissions, limiting their
ability to fully resolve subsurface structures and internal heat distribution.
Additionally, existing thermal tomographic techniques can only shoot one frame
from each layer. Physics-informed neural networks (PINNs) have recently emerged
as powerful tools for solving inverse problems in heat transfer by integrating
observational data with physical constraints. However, standard PINNs are
primarily focused on fitting the given external temperature data, without
explicit knowledge of the unknown internal temperature distribution. In this
study, we introduce a Helmholtz-informed neural network (HINN) to predict
internal temperature distributions without requiring internal measurements. The
time-domain heat diffusion equation was converted to the frequency-domain and
becomes the pseudo-Helmholtz equation. HINN embeds this pseudo-Helmholtz
equation into the learning framework, leveraging both real and imaginary
components of the thermal field. Finally, an inverse Fourier transform brings
real-part and imagery-part back to the time-domain and can be used to map 3D
thermal fields with interior defects. Furthermore, a truncated operation was
conducted to improve computational efficiency, and the principle of conjugate
symmetry was employed for repairing the discarded data. This approach
significantly enhances predictive accuracy and computational efficiency. Our
results demonstrate that HINN outperforms state-of-the-art PINNs and inverse
heat solvers, offering a novel solution for non-invasive thermography in
applications spanning materials science, biomedical diagnostics, and
nondestructive evaluation.

</details>


### [269] [Thermal diffusivity measurement based on evaporative cryocooling excitation: Theory and experiments](https://arxiv.org/abs/2509.04263)
*Pengfei Zhu,Hai Zhang,Stefano Sfarra,Fabrizio Sarasini,Ruben Usamentiaga,Gunther Steenackers,Clemente Ibarra-Castanedo,Xavier Maldague,Andreas Mandelis*

Main category: physics.app-ph

TL;DR: 光热法测量热扩散系数面临病态反问题，尤其在非脉冲激励下。Parker解因其物理假设不现实，仅在热峰前有效。本研究提出了矩形脉冲和狄拉克脉冲激励的解析解，并提出了一种新颖的蒸发致冷新激发方法，通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 光热法测量热扩散系数在非脉冲激励下存在挑战，现有方法（如Parker解）存在物理假设不合理的问题。

Method: 推导了狄拉克脉冲和矩形脉冲激励下的全面解析解，并将Parker解视为矩形脉冲解的极限情况。提出了一种新的蒸发致冷激发方法，并进行了理论和实验验证。

Result: Parker解仅在热峰前有效。矩形脉冲解在更广泛的范围内适用。蒸发致冷方法被实验证明是一种有效的新型测量方法。

Conclusion:  Parker解的局限性已被揭示，并提出了更精确的解析解。蒸发致冷作为一种新颖的测量方法，为光热法测量热扩散系数提供了新的途径。

Abstract: Photo-thermal methods for measuring thermal diffusivity inherently pose an
ill-posed inverse problem, affected by factors such as sample thickness,
heating or cooling time, and excitation energy. Measurement accuracy becomes
particularly challenging under non-impulsive pulsed excitation when the
observation timescale is comparable to the pulse duration. This is often due to
poorly defined pulse shapes, broadened thermal responses, and the absence of
clear boundary conditions, especially under significant interfacial temperature
gradients where natural convection dominates. The classic Parker solution,
while widely used, is physically unrealistic as it assumes adiabatic heat flux
and shallow-region heat absorption. In this study, we prove that Parker's
assumption is equivalent to the Dirac pulse boundary condition in mathematics.
Then, we present comprehensive analytical solutions for thermal/cooling
responses under Dirac and rectangular pulse excitations. By comparing with
eigenfunction-based solutions for well-posed boundary conditions, we show that
Parker's solution is only valid before the thermal peak. Through dimensionless
processing, we further demonstrate that Parker's solution can be regarded as a
limiting case of the rectangular pulse solution as the heating duration
approaches zero. Furthermore, we propose a novel excitation approach,
evaporative cryocooling, for thermal diffusivity measurement. This method
offers a compact, low-cost, and easy-to-implement alternative to conventional
excitation schemes. The theoretical model was further validated through
comparison with experimental results.

</details>


### [270] [Ultrasound-Triggered Release of Anticancer Nanoparticles from Electrospun Fabrics Integrated with Soft Robotic Tentacles](https://arxiv.org/abs/2509.04313)
*Samuel C. T. Moorcroft,Benjamin Calmé,Charles Brooker,Pietro Valdastri,Russell Harris,Stephen J. Russell,Giuseppe Tronci*

Main category: physics.app-ph

TL;DR: 本研究展示了一种结合软体机器人和超声触发的药物递送系统，用于胰腺癌治疗。


<details>
  <summary>Details</summary>
Motivation: 胰腺癌的早期诊断困难，肿瘤微环境限制了化疗效果，需要更智能化的药物递送系统。

Method: 设计了负载药物的乙烯基苄基官能化明胶纳米粒子（gel4vbc NPs），并将其集成到电纺织物中。通过超声（US）触发，控制纳米粒子在织物上的释放。将负载纳米粒子的织物固定在磁性触手机器人（MTR）上，实现靶向递送和按需释放。

Result: 证明了通过调节聚乙烯醇（PVA）浓度和超声强度可以控制纳米粒子的释放。该系统能够将负载白蛋白的纳米粒子靶向递送至模拟胰管，并实现可控释放。

Conclusion: 开发了一种新型药物递送系统（DDS），可轻松集成到软体机器人中，并通过超声触发实现药物纳米粒子的靶向释放，为胰腺癌治疗提供了新的解决方案。

Abstract: The prompt identification of pancreatic cancer symptoms is an ongoing
clinical challenge, often leading to late diagnosis and poor prognosis. Tumor
'hijacking' of the pancreatic stromal structure limits the uptake of systemic
chemotherapeutics. Localized drug delivery systems (DDS) using endoluminal
techniques are widely utilized, with positive early results for improved
control over tumor growth. There is a need for technologies that integrate
endoluminal resources and intelligent material systems to better control the
spatiotemporal delivery of chemotherapeutics. We demonstrate the ultrasound
(US)-triggered localized release of therapeutics through the design of solvent
traceless drug-loaded vinylbenzyl-functionalized gelatin (gel4vbc)
nanoparticles (NPs) integrated with an electrospun fabric. Albumin-loaded NPs
encapsulated into a poly(vinyl alcohol) (PVA) coating of a
poly(epsilon-caprolactone) fabric evidence tunable triggered NP delivery
controlled by regulating PVA concentration (0-1 wt.%) and US intensity (0-3
W/cm2). The fixation of the NP-coated fabric to a magnetic tentacle robot (MTR)
enables the automated manipulation into a phantom pancreatic duct before the
US-triggered release of NP-loaded albumin and MTR retraction. Albumin release
is controlled by varying the surface area of the NP-loaded MTR-coating fabric.
Herein we have designed a novel DDS capable of facile integration into soft
robotics and US-triggered delivery of therapeutic-loaded NPs.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [271] [Integrating Pruning with Quantization for Efficient Deep Neural Networks Compression](https://arxiv.org/abs/2509.04244)
*Sara Makenali,Babak Rokh,Ali Azarpeyvand*

Main category: cs.NE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep Neural Networks (DNNs) have achieved significant advances in a wide
range of applications. However, their deployment on resource-constrained
devices remains a challenge due to the large number of layers and parameters,
which result in considerable computational and memory demands. To address this
issue, pruning and quantization are two widely used compression techniques,
commonly applied individually in most studies to reduce model size and enhance
processing speed. Nevertheless, combining these two techniques can yield even
greater compression benefits. Effectively integrating pruning and quantization
to harness their complementary advantages poses a challenging task, primarily
due to their potential impact on model accuracy and the complexity of jointly
optimizing both processes. In this paper, we propose two approaches that
integrate similarity-based filter pruning with Adaptive Power-of-Two (APoT)
quantization to achieve higher compression efficiency while preserving model
accuracy. In the first approach, pruning and quantization are applied
simultaneously during training. In the second approach, pruning is performed
first to remove less important parameters, followed by quantization of the
pruned model using low-bit representations. Experimental results demonstrate
that our proposed approaches achieve effective model compression with minimal
accuracy degradation, making them well-suited for deployment on devices with
limited computational resources.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [272] [The Chaotic Art: Quantum Representation and Manipulation of Color](https://arxiv.org/abs/2509.03542)
*Guosheng Hu*

Main category: quant-ph

TL;DR: 本研究提出了一种在量子计算环境中表示、操作和测量数字色彩的新技术路径，并将其恢复为经典计算机可用的计算结果。


<details>
  <summary>Details</summary>
Motivation: 量子计算的独特性质将深刻改变色彩艺术领域，本研究旨在探索色彩量子比特表示、色彩通道处理和通过量子计算进行色彩图像生成的实验。

Method: 通过在Qiskit和IBM Q中进行编程实验，研究量子比特表示色彩、色彩通道处理和量子计算在色彩图像生成中的应用。

Result: 实验证明，该方法作为色彩量子比特表示和量子计算的一种艺术技术是可行的。

Conclusion: 本研究为经典色彩学和量子图形学之间搭建了桥梁，有望将量子计算应用于信息可视化、图像处理等色彩计算任务，并可能促进新的色彩理论和艺术概念的发展。

Abstract: Due to its unique computing principles, quantum computing technology will
profoundly change the spectacle of color art. Focusing on experimental
exploration of color qubit representation, color channel processing, and color
image generation via quantum computing, this article proposes a new technical
path for color computing in quantum computing environment, by which digital
color is represented, operated, and measured in quantum bits, and then restored
for classical computers as computing results. This method has been proved
practicable as an artistic technique of color qubit representation and quantum
computing via programming experiments in Qiskit and IBM Q. By building a bridge
between classical chromatics and quantum graphics, quantum computers can be
used for information visualization, image processing, and more color computing
tasks. Furthermore, quantum computing can be expected to facilitate new color
theories and artistic concepts.

</details>


### [273] [Another formula for calculating Clebsch Gordan coefficients](https://arxiv.org/abs/2509.03555)
*Everardo Rivera-Oliva*

Main category: quant-ph

TL;DR: 本文推导了Clebsch-Gordan系数的综合公式，使用角动量升降算符迭代作用于每个定义的角动量子空间来重构总角动量基下的状态。


<details>
  <summary>Details</summary>
Motivation: 推导Clebsch-Gordan系数的综合公式。

Method: 使用角动量升降算符迭代作用于每个定义的角动量子空间来重构总角动量基下的状态，特别是利用$J{+}$升算符来重构非最大总角动量的子空间。

Result: 推导出了一个新的Clebsch-Gordan系数计算公式。

Conclusion: 本文提出了一种计算Clebsch-Gordan系数的新方法，为现有方法提供了补充。

Abstract: This article presents the derivation of a comprehensive formula for the
Clebsch-Gordan coefficients in a quantum system. The formula is derived by
employing the iterative application of angular momentum ladder operators on
each defined angular momentum subspace to reconstruct the states in the total
angular momentum base. The novelty aspect of this approach lies in the
utilization of the $J{+}$ raising operator to reconstruct subspaces
characterized by non-maximal total angular momentum, in contrast to the
conventional Gram-Schmidt procedure typically employed in the standard
literature.This enables us to derive a new formula that provides an alternative
approach for computing these coefficients, complementing the existing ones
found in the literature.

</details>


### [274] [Quantum-Assisted Correlation Clustering](https://arxiv.org/abs/2509.03561)
*Antonio Macaluso,Supreeth Mysore Venkatesh,Diego Arenas,Matthias Klusch,Andreas Dengel*

Main category: quant-ph

TL;DR: 提出了一种混合量子-经典方法用于图的聚类，特别是在处理带有负相关边的图中，通过递归分割和量子退火优化，能够处理任意相关结构，并且在合成图和真实世界数据上均优于经典算法。


<details>
  <summary>Details</summary>
Motivation: 图的聚类任务，特别是处理带有正负相关边的图，现有的经典算法在鲁棒性和准确性上存在局限性。

Method: 将GCS-Q（一种量子辅助求解器）改编用于解决相关聚类问题，通过递归的划分式聚类，将每一步的二分问题编码为二次无约束二元优化（QUBO）问题，并利用量子退火求解。

Result: 在合成的带符号图和真实世界的超光谱成像数据上进行了实验评估，结果表明该方法在鲁棒性和聚类质量上优于经典算法，尤其在处理真实世界数据和存在簇大小不平衡的情况下。

Conclusion: 混合量子-经典优化方法在处理具有任意相关结构的图聚类问题上具有巨大潜力，能够实现可扩展且结构感知的聚类技术。

Abstract: This work introduces a hybrid quantum-classical method to correlation
clustering, a graph-based unsupervised learning task that seeks to partition
the nodes in a graph based on pairwise agreement and disagreement. In
particular, we adapt GCS-Q, a quantum-assisted solver originally designed for
coalition structure generation, to maximize intra-cluster agreement in signed
graphs through recursive divisive partitioning. The proposed method encodes
each bipartitioning step as a quadratic unconstrained binary optimization
problem, solved via quantum annealing. This integration of quantum optimization
within a hierarchical clustering framework enables handling of graphs with
arbitrary correlation structures, including negative edges, without relying on
metric assumptions or a predefined number of clusters. Empirical evaluations on
synthetic signed graphs and real-world hyperspectral imaging data demonstrate
that, when adapted for correlation clustering, GCS-Q outperforms classical
algorithms in robustness and clustering quality on real-world data and in
scenarios with cluster size imbalance. Our results highlight the promise of
hybrid quantum-classical optimization for advancing scalable and
structurally-aware clustering techniques in graph-based unsupervised learning.

</details>


### [275] [Dynamical Quantum Phase Transitions and Many-Body Backflow in Open Quantum Systems](https://arxiv.org/abs/2509.03570)
*Kai Zhang,Chang Shu,Kai Sun*

Main category: quant-ph

TL;DR: 开放量子系统中的动力学量子相变（DQPT）在粒子增益或损失单独存在时仍然稳健，但在两者并存时会被平均化，这源于多体粒子回流。


<details>
  <summary>Details</summary>
Motivation: 研究DQPT在包含环境相互作用的开放量子系统中的行为，特别是与仅使用非厄米描述相比，考虑完整的开放系统量子动力学。

Method: 通过结合相互作用和完整的刘维尔动力学，同时进行解析论证和数值模拟。

Result: DQPT在仅有粒子损失或仅有粒子增益时保持稳健，但在两者同时存在时，由于多体粒子回流而被平均化。增益（损失）与损失（增益）的混合会显著改变DQPT的长时动力学行为。

Conclusion: DQPT在开放量子系统中的稳健性取决于环境相互作用的具体形式。多体粒子回流和增益/损失的混合是影响DQPT行为的关键因素。

Abstract: Dynamical quantum phase transitions (DQPTs) are non-equilibrium transitions
characterized by the orthogonality between an initial quantum state and its
time-evolved counterpart following a sudden quench. Recently, studies of this
phenomenon have been extended beyond closed quantum systems to include
environmental interactions, often modeled through non-Hermitian effects.
However, because non-Hermitian descriptions neglect both quantum jump processes
and interaction effects, the ultimate fate of interacting quantum systems under
full open-system quantum dynamics remains an open question. In this paper, by
incorporating both interactions and full Liouvillian dynamics, we prove that
DQPTs in open quantum systems remain robust when subject to either particle
loss or particle gain alone, but are generically smeared out when both
processes coexist, as a result of many-body particle backflow. Furthermore, we
uncover a non-perturbative dynamical effect: even a weak admixture of gain
(loss) into a system with loss (gain) can dramatically reshape the long-time
behavior of DQPT dynamics, leading to substantial deviations over time. These
phenomena--including the universal smearing of DQPTs and the emergence of large
dynamical deviations in the long-time limit--arise intrinsically from
non-equilibrium many-body effects in open quantum systems. Our findings are
general and substantiated by both analytical arguments and numerical
simulations.

</details>


### [276] [Quantum simulation of out-of-equilibrium dynamics in gauge theories](https://arxiv.org/abs/2509.03586)
*Jad C. Halimeh,Niklas Mueller,Johannes Knolle,Zlatko Papić,Zohreh Davoudi*

Main category: quant-ph

TL;DR: 量子模拟在远离平衡的规范场论研究中取得进展，可用于解决核物理、高能物理和凝聚态物理中的长期问题。


<details>
  <summary>Details</summary>
Motivation: 经典计算在远离平衡的规范场论模拟中存在局限性，而量子技术（如中性原子、离子阱、超导电路）的发展为此类模拟提供了新的可能性。

Method: 利用中性原子、离子阱和超导电路等量子模拟器，研究粒子产生、链断裂、碰撞动力学、热化、遍历性破坏和动力学量子相等现象。

Result: 量子模拟器在研究远离平衡的规范场论方面展现出巨大潜力，并揭示了新的物理现象。

Conclusion: 量子模拟在理解宇宙早期演化、高能粒子碰撞等问题上具有广阔前景，并推动了跨学科的融合。未来的研究方向包括利用现有量子硬件探索更多物理现象。

Abstract: Recent advances in quantum technologies have enabled quantum simulation of
gauge theories -- some of the most fundamental frameworks of nature -- in
regimes far from equilibrium, where classical computation is severely limited.
These simulators, primarily based on neutral atoms, trapped ions, and
superconducting circuits, hold the potential to address long-standing questions
in nuclear, high-energy, and condensed-matter physics, and may ultimately allow
first-principles studies of matter evolution in settings ranging from the early
universe to high-energy collisions. Research in this rapidly growing field is
also driving the convergence of concepts across disciplines and uncovering new
phenomena. In this Review, we highlight recent experimental and theoretical
developments, focusing on phenomena accessible in current and near-term quantum
simulators, including particle production and string breaking, collision
dynamics, thermalization, ergodicity breaking, and dynamical quantum phase
transitions. We conclude by outlining promising directions for future research
and opportunities enabled by available quantum hardware.

</details>


### [277] [Trading Mathematical for Physical Simplicity: Bialgebraic Structures in Matrix Product Operator Symmetries](https://arxiv.org/abs/2509.03600)
*Yuhan Liu,Andras Molnar,Xiao-Qi Sun,Frank Verstraete,Kohtaro Kato,Laurens Lootens*

Main category: quant-ph

TL;DR: Despite advances in lattice symmetry representation, some quantum spin chains lack inclusion in rigid frameworks like fusion categories. This paper shows that by relaxing algebraic requirements to use pre-bialgebras, general matrix product operator (MPO) symmetries can describe these chains. Focusing on the anomalous Z2 symmetry of the XX model, which has a mixed anomaly between its U(1) momentum and winding symmetry, the paper demonstrates how this anomaly is embedded in the non-semisimple corepresentation category. This offers a new way to realize anomalous symmetries on the lattice. The representation category, crucial for renormalization, is shown to be semisimple and semi-monoidal, suggesting a new class of mixed state renormalization fixed points. Lastly, the paper proves that this anomalous Z2 symmetry is equivalent to a standard MPO symmetry found on the boundary of a double semion model, up to a quantum channel. This work connects established topological defect symmetries with those in more practical models.


<details>
  <summary>Details</summary>
Motivation: The existing rigid frameworks (fusion categories, weak Hopf algebras) for representing generalized symmetries in lattice models do not encompass many physically relevant quantum spin chains. This paper aims to overcome this limitation by proposing a more flexible algebraic structure.

Method: The paper proposes using a pre-bialgebra to describe general matrix product operator (MPO) symmetries, relaxing the rigid requirements of fusion categories and weak Hopf algebras. It uses the anomalous $\mathbb Z_2$ symmetry of the XX model as a case study, showing how this anomaly is embedded in a non-semisimple corepresentation category. It also analyzes the representation category for renormalization properties and establishes an equivalence, up to a quantum channel, between the anomalous $\mathbb Z_2$ symmetry and a conventional MPO symmetry from a double semion model.

Result: The paper demonstrates that pre-bialgebras can describe general MPO symmetries, including the anomalous $\mathbb Z_2$ symmetry of the XX model. It shows how this anomaly is embedded in a non-semisimple corepresentation category, providing a novel mechanism for lattice realization. The representation category for renormalization is found to be semisimple and semi-monoidal, indicating new fixed points. An equivalence between the anomalous $\mathbb Z_2$ symmetry and a boundary MPO symmetry of a double semion model is established.

Conclusion: This work extends the understanding of lattice symmetries by introducing pre-bialgebras to describe more general MPO symmetries, including anomalous ones like that in the XX model. It provides a new mechanism for realizing such symmetries and identifies a new class of renormalization fixed points, bridging the gap between topological defect symmetries and those in realistic models.

Abstract: Despite recent advances in the lattice representation theory of (generalized)
symmetries, many simple quantum spin chains of physical interest are not
included in the rigid framework of fusion categories and weak Hopf algebras. We
demonstrate that this problem can be overcome by relaxing the requirements on
the underlying algebraic structure, and show that general matrix product
operator symmetries are described by a pre-bialgebra. As a guiding example, we
focus on the anomalous $\mathbb Z_2$ symmetry of the XX model, which manifests
the mixed anomaly between its $U(1)$ momentum and winding symmetry. We show how
this anomaly is embedded into the non-semisimple corepresentation category,
providing a novel mechanism for realizing such anomalous symmetries on the
lattice. Additionally, the representation category which describes the
renormalization properties is semisimple and semi-monoidal, which provides a
new class of mixed state renormalization fixed points. Finally, we show that up
to a quantum channel, this anomalous $\mathbb Z_2$ symmetry is equivalent to a
more conventional MPO symmetry obtained on the boundary of a double semion
model. In this way, our work provides a bridge between well-understood
topological defect symmetries and those that arise in more realistic models.

</details>


### [278] [Exoplanetary atmospheres retrieval via a quantum extreme learning machine](https://arxiv.org/abs/2509.03617)
*Marco Vetrano,Tiziano Zingales,G. Massimo Palma,Salvatore Lorenzo*

Main category: quant-ph

TL;DR: 本研究提出使用量子极端学习机（QELMs）来分析系外行星大气，以克服传统方法计算量大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统前向模型在计算系外行星光谱时需要调整大量参数，导致计算量大，本研究旨在寻找更高效的分析方法。

Method: 提出并实现了一种利用量子极端学习机（QELMs）提取系外行星大气特征的新方法，并利用IBM Fez设备验证了其容错能力。

Result: 所提出的QELM架构在处理天体物理数据集方面展现了量子计算的潜力。

Conclusion: 本研究提出的QELM方法有望在不久的将来，为系外行星大气研究提供快速、高效、更精确的计算工具。

Abstract: The study of exoplanetary atmospheres traditionally relies on forward models
to analytically compute the spectrum of an exoplanet by fine-tuning numerous
chemical and physical parameters. However, the high-dimensionality of parameter
space often results in a significant computational overhead. In this work, we
introduce a novel approach to atmospheric retrieval leveraging on quantum
extreme learning machines (QELMs). QELMs are quantum machine learning
techniques that employ quantum systems as a black box for processing input
data. In this work, we propose a framework for extracting exoplanetary
atmospheric features using QELMs, employing an intrinsically fault-tolerant
strategy suitable for near-term quantum devices, and we demonstrate such fault
tolerance with a direct implementation on IBM Fez. The QELM architecture we
present shows the potential of quantum computing in the analysis of
astrophysical datasets and may, in the near-term future, unlock new
computational tools to implement fast, efficient, and more accurate models in
the study of exoplanetary atmospheres.

</details>


### [279] [Quantum algorithms for Uhlmann transformation](https://arxiv.org/abs/2509.03619)
*Takeru Utsumi,Yoshifumi Nakata,Qisheng Wang,Ryuji Takagi*

Main category: quant-ph

TL;DR: 提出了一种新的量子电路算法，用于实现 Uhlmann 变换，在计算成本（包括查询和样本复杂度）方面实现了指数级改进，并将其应用于平方根保真度估计等任务。


<details>
  <summary>Details</summary>
Motivation: Uhlmann 变换在量子信息论中具有重要意义，但其量子电路实现和计算成本尚不清楚。本研究旨在解决这一问题。

Method: 提出了一种新的量子查询和采样算法来实现 Uhlmann 变换，并分析了其在平方根保真度估计、纠缠传输、量子态合并和 Petz 恢复图算法实现等任务中的应用和计算成本。

Result: 所提出的算法在计算成本方面实现了指数级改进，并在平方根保真度估计任务上优于现有技术。此外，还对其他信息论任务的计算成本进行了评估。

Conclusion: 本研究成功地实现了 Uhlmann 变换的量子电路实现，并显著降低了计算成本，为相关领域的研究和应用提供了新的方法和见解。

Abstract: Uhlmann's theorem is a central result in quantum information theory,
associating the closeness of two quantum states with that of their
purifications. This theorem well characterizes the fundamental task of
transforming a quantum state into another state via local operations on its
subsystem. The optimal transformation for this task is called the Uhlmann
transformation, which has broad applications in various fields; however, its
quantum circuit implementation and computational cost have remained unclear. In
this work, we fill this gap by proposing quantum query and sample algorithms
that realize the Uhlmann transformation in the form of quantum circuits. These
algorithms achieve exponential improvements in computational costs, including
query and sample complexities, over naive approaches based on state
measurements such as quantum state tomography, under certain computational
models. We apply our algorithms to the square root fidelity estimation task and
particularly show that our approach attains a better query complexity than the
prior state-of-the-art. Furthermore, we discuss applications to several
information-theoretic tasks, specifically, entanglement transmission, quantum
state merging, and algorithmic implementation of the Petz recovery map,
providing a comprehensive evaluation of the computational costs.

</details>


### [280] [Long-term stability of driven quantum systems and the time-dependent Bloch equation](https://arxiv.org/abs/2509.03639)
*Zsolt Szabó,Kazuya Yuasa,Daniel Burgarth*

Main category: quant-ph

TL;DR: 该研究探讨了弱扰动下有限维绝热演化的渐近时间行为，推导了布洛赫方程，并分析了其解，强调了初始条件和泄漏现象与布洛赫变换的联系，证明了泄漏可以长期保持很小。


<details>
  <summary>Details</summary>
Motivation: 研究在弱扰动下有限维绝热演化的渐近时间极限。

Method: 从绝热变换和时变有效哈密顿量出发，推导出布洛赫方程，并对其解进行数值和解析评估。

Result: 泄漏现象与布洛赫变换密切相关，泄漏可以长期保持很小，系统长时间后仍保持在初始特征子空间内，误差为O(γ^-1)。

Conclusion: 系统在弱扰动下可以长时间保持在初始特征子空间内，泄漏可以被控制。

Abstract: This study looks at the finite-dimensional adiabatic evolution influenced by
weak perturbations, extending the analysis to the asymptotic time limit.
Beginning with the fundamentals of adiabatic transformations and time-dependent
effective Hamiltonians, we intuitively derive the Bloch equation. Our
investigation of the solutions of the Bloch equation underscores the critical
role of initial conditions and the assured existence of solutions, revealing
the intricate link between leakage phenomena and the Bloch transformation.
Numerical and analytical evaluations demonstrate that the leakage can remain
small eternally. That is, a system that starts in a particular eigenspace of
the strong generator remains in the same respective eigenspace for arbitrary
long times with an error of $\mathcal{O}(\gamma^{-1})$, where $\gamma$
describes the ratio between the strength of the system's strong Hamiltonian and
the perturbation.

</details>


### [281] [QuLTRA: Quantum hybrid Lumped and TRansmission lines circuits Analyzer](https://arxiv.org/abs/2509.03651)
*Simona Zaccaria,Antonio Gnudi*

Main category: quant-ph

TL;DR: QuLTRA是一个开源Python包，用于精确模拟包含集总和分布式元件的超导量子电路。它直接模拟传输线和多线耦合器，避免了离散化为集总元件等效电路，并使用能量参与比（EPR）方法提取电路哈密顿量参数。


<details>
  <summary>Details</summary>
Motivation: 提出 QuLTRA，一个用于精确模拟包含集总和分布式元件的超导量子电路的开源Python包。

Method: 直接建模共面波导（CPW）传输线和多线耦合器，不进行集总元件等效电路的离散化，并使用能量参与比（EPR）方法提取电路哈密顿量参数。

Result: QuLTRA能够快速准确地提取模式频率、非简谐性、交叉克尔相互作用和Purcell衰减率，无需进行完整的电磁模拟，并且能够自然地考虑分布式元件的高阶模式。与 Ansys HFSS、pyEPR、QuCAT 和文献结果相比，QuLTRA的性能得到了验证，在计算时间上比完整的电磁模拟减少了几个数量级，且具有非常好的协议。

Conclusion: QuLTRA 能够有效地模拟电路量子电动力学应用中复杂架构的设计，例如 Purcell 滤波器、多模超强耦合系统和复用量子比特读出方案的设计。

Abstract: We present QuLTRA (Quantum hybrid Lumped and TRansmission lines circuits
Analyzer), an open-source Python package for the accurate simulation of
superconducting quantum circuits containing both lumped and distributed
elements. QuLTRA directly models coplanar waveguide (CPW) transmission lines
and multi-line couplers without discretization into lumped-element equivalents,
and extracts the circuit Hamiltonian parameters using the energy participation
ratio (EPR) method. This approach enables fast and accurate extraction of mode
frequencies, anharmonicities, cross-Kerr interactions, and Purcell decay rates
without relying on full electromagnetic simulations, while naturally accounting
for higher-order modes of distributed components. The performance of QuLTRA is
validated against Ansys HFSS, pyEPR, QuCAT, and results from the literature,
showing excellent agreement with orders-of-magnitude reductions in
computational time with respect to full electromagnetic simulations. Some
application examples are discussed, including the design of Purcell filters,
multi-mode ultra-strong coupling systems, and multiplexed qubit readout
schemes, demonstrating that QuLTRA can efficiently simulate complex
architectures relevant to circuit quantum electrodynamics applications.

</details>


### [282] [The ITransverse.jl library for transverse tensor network contractions](https://arxiv.org/abs/2509.03699)
*Stefano Carignano*

Main category: quant-ph

TL;DR: ITransverse.jl是一个Julia包，提供了用于张量网络时间演化的横向收缩算法，包括新的时间矩阵乘积状态截断方法。


<details>
  <summary>Details</summary>
Motivation: 张量网络的时间演化研究在量子多体系统中至关重要，但常受限于经典资源的纠缠壁垒。横向收缩方法为此类研究提供了高效的解决方案。

Method: 提出并实现了一个名为ITransverse.jl的Julia包，该包基于ITensors.jl，包含了多种高效的横向收缩算法，并引入了新的时间矩阵乘积状态（TMPS）截断方法。

Result: ITransverse.jl包提供了高效的量子多体系统时间演化算法，能够克服纠缠壁垒，并在TMPS截断方面提出了新方法。

Conclusion: ITransverse.jl包为使用经典资源研究量子多体系统的时间演化提供了强大的工具，尤其是在处理TMPS方面具有优势。

Abstract: Transverse contraction methods are extremely promising tools for the
efficient contraction of tensor networks associated with the time evolution of
quantum many-body systems, allowing in some cases to circumvent the
entanglement barrier that would normally prevent the study of quantum dynamics
with classical resources. We present here the ITransverse.jl package, written
in Julia and based on ITensors.jl, containing several of these high-level
algorithms, including novel prescriptions for efficient truncations of temporal
matrix product states.

</details>


### [283] [Polarization-Controlled Dual-State Distribution of Bell and N00N Entanglement Over a Metro-Scale Commercial Quantum Network](https://arxiv.org/abs/2509.03701)
*Kazi Reaz,Md Mehdi Hassan,Jacob E. Humberd,Matthew L. Boone,Angel Fraire Estrada,Rick Mukherjee,H. R. Sadeghpour,Girish S. Agarwal,George Siopsis,Tian Li*

Main category: quant-ph

TL;DR: 成功在商用光纤网络上实现了多光子、双态纠缠分发。


<details>
  <summary>Details</summary>
Motivation: 在真实世界的电信基础设施上展示了多光子纠缠分发的通用性。

Method: 使用全光纤实验平台，通过空间简并连续波II型SPDC双光子源和全光纤耦合线性光学器件，生成了4光子纠缠态。通过对两个本地保留光子的偏振投影测量，概率性地宣告了另外两个光子进入贝尔态（粒子-粒子纠缠）或N00N态（模式-模式纠缠），然后将其分发到两个空间分离的网络节点。

Result: 在实际部署的网络中，尽管存在显著的信道损耗和有限的源保真度，但仍然成功地分发了纠缠。

Conclusion: 该研究为未来升级奠定了基础，包括更高质量的非简并光子源、用于真正多节点纠缠的白兔定时同步，以及用于提高保真度和长期稳定性的有源偏振控制。

Abstract: We report the first demonstration of multiphoton, dual state entanglement
distribution over a metropolitan scale commercial fiber network, implemented on
the EPB Bohr IV quantum network in Chattanooga, TN, using an all fiber optic
experimental platform. Employing a spatially degenerate, continuous wave type
II SPDC biphoton source and fully fiber coupled linear optics, we generated a 4
photon entangled state. Through polarization projective measurements on two
locally retained photons, we then probabilistically heralded the remaining two
photons into either a Bell state (particle particle entanglement) or a N00N
state (mode-mode entanglement), which were then distributed to two spatially
separated network nodes. Experimental verification confirmed successful
entanglement distribution across the deployed network despite significant
channel losses and limited source fidelity. These results highlight the
versatility of our polarization controlled multi photon entanglement
distribution over real world telecom infrastructure and lay the groundwork for
future upgrades, including higher quality non degenerate photon sources, White
Rabbit timing synchronization for true multi node entanglement, and active
polarization control for enhanced fidelity and long term stability.

</details>


### [284] [Cavity-Controlled High Harmonic Generation](https://arxiv.org/abs/2509.03705)
*Zohar Amitay,Nimrod Moiseyev*

Main category: quant-ph

TL;DR: 利用非厄米Floquet理论，发现强场下连续波场辐照基态原子的阿秒谐波（HH）产生过程可以通过将原子置于单模量子腔中来控制，即使腔内初始只有一个光子。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是探索利用量子腔控制强场下高次谐波（HH）产生过程的可能性，特别是通过腔耦合形成极化激子Floquet态来控制HH光谱，进而产生与标准无腔情况不同的阿秒脉冲序列。

Method: 本研究采用非厄米Floplet理论，通过将原子置于单模量子腔中，利用腔耦合形成极化激子Floquet态，并研究其对高次谐波（HH）产生光谱的影响。

Result: 研究发现，腔耦合可以产生围绕标准奇次谐波的边带谐波，并能产生不同于无腔情况的阿秒脉冲序列。此外，该研究为进一步的腔控制HH产生过程和其他强场过程提供了理论框架。

Conclusion: 本研究成功地展示了如何利用量子腔控制强场下的阿秒谐波产生过程，为阿秒科学和强场物理领域开辟了新的研究方向。

Abstract: Employing non-Hermitian Floquet theory, the strong-field process of high
harmonic (HH) generation by a classical continuous-wave field irradiating
ground-state atom is discovered to be controllable by placing the irradiated
atom inside a single-mode quantum cavity initiated even with a single photon.
Judicious cavity coupling of cavity-free photo-induced atomic Floquet states
forms polaritonic Floquet states that generate side harmonics around the
(standard no-cavity) odd harmonics. The different possible cavity-controlled HH
spectra, including also the ones resulting from several cavities in a row,
enable attosecond-pulse sequences different from the one produced without a
cavity. Moreover, the present study sets the framework and opens the way for
further cavity control over the HH generation process as well as over other
strong-field processes

</details>


### [285] [Resonance Assisted Tunneling in Floquet spin-J systems](https://arxiv.org/abs/2509.03715)
*J. A. Segura-Landa,Diego A. Wisniacki,Sergio Lerma-Hernández,Jorge G. Hirsch*

Main category: quant-ph

TL;DR: 该研究将共振辅助隧穿（RAT）理论应用于多体量子踢系统，通过量子共振条件识别与经典共振相关的本征态，并半经典地计算其准能量分裂。研究区分了两种情况：一种是RAT预测与精确量子结果高度一致；另一种是分裂饱和，与由共振岛经典振荡决定的谐振子的分裂一致。研究还量化了RAT理论失效的扰动强度阈值，并分析了其在半经典极限下的标度律，给出了估计该上限的解析表达式。


<details>
  <summary>Details</summary>
Motivation: 将共振辅助隧穿（RAT）理论应用于多体量子踢系统，以理解其在半经典极限下的行为。

Method: 使用量子共振条件识别与经典共振相关的本征态，并半经典地计算其准能量分裂，区分了两种不同 regime：RAT 预测与精确量子结果一致的 regime，以及分裂饱和并与谐振子行为一致的 regime。研究还量化了 RAT 理论失效的扰动强度阈值，并分析了其在半经典极限下的标度律。

Result: 研究发现了两种RAT理论预测与量子结果符合程度不同的 regime。在第一种 regime 中，RAT预测与精确量子结果吻合良好。在第二种 regime 中，准能量分裂饱和，并与由经典共振岛振荡决定的谐振子行为一致。研究还给出了RAT理论失效的扰动强度上限的解析表达式。

Conclusion: RAT理论在某些条件下能精确预测多体量子踢系统的准能量分裂，但在强扰动下会失效。研究为理解RAT理论的适用范围及其在半经典极限下的行为提供了理论工具。

Abstract: We apply the theory of Resonance Assisted Tunneling (RAT) to a many-body
quantum kicked system with a well-defined semiclassical limit. Using a quantum
resonant condition, we identify eigenstates associated with classical
resonances and compute their quasienergy splitting semiclassically. We
distinguish two regimes: the first, where RAT predictions show excellent
agreement with exact quantum results, and a second, where the splitting
saturates and coincides with that of a harmonic oscillator with frequency
determined by the classical oscillation of the resonance island. We quantify
the perturbation strength above which RAT theory is no longer valid and analyze
its scaling in the semiclassical limit, providing analytical expressions to
estimate this upper bound.

</details>


### [286] [Coherence-Driven Quantum Battery Charging via Autonomous Thermal Machines: Energy Transfer, Memory Effects, and Ergotropy Enhancement](https://arxiv.org/abs/2509.03766)
*Achraf Khoudiri,Abderrahman Oularabi,Khadija El Anouz,İlkay Demir,Abderrahim El Allati*

Main category: quant-ph

TL;DR: 本研究探讨了一个由量子电池和相干驱动充电器组成的混合量子系统，该系统与一个量子自主热机（QATM）相互作用。QATM 由两个分别与不同温度的马尔可夫玻色子热库耦合的量子比特组成，充当了介导充电器和电池之间能量和相干转移的结构化环境。通过在充电器上呈现相干驱动场，研究了相干注入对动力学的影响，包括非马尔可夫性、充电功率、相干存储和功。研究表明，QATM 有效地过滤了热库引起的退相干，并由于关联回流引起了非马尔可夫记忆效应。研究结果证明，相干驱动通过保持充电器内部能量和促进相干能量转移，增强了电池的功和充电功率。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于探索量子自主热机（QATM）在介导量子电池和相干驱动充电器之间的能量和相干转移中的作用，并研究相干注入对系统动力学（如非马尔可夫性、充电功率、相干存储和功）的影响。

Method: 本研究采用理论方法，构建了一个由量子电池、相干驱动充电器和 QATM 组成的混合量子系统。QATM 由两个量子比特组成，每个量子比特与不同的马尔可夫热库耦合。通过在充电器上引入相干驱动场，并分析能量和相干的转移动力学，研究了非马尔可夫性、充电功率、相干存储和功。

Result: 研究结果表明，QATM 能够有效过滤热库引起的退相干，并通过关联回流产生非马尔可夫记忆效应。相干驱动能够增强电池的功和充电功率，这是通过保持充电器内部能量和促进相干能量转移实现的。

Conclusion: 本研究得出结论，相干驱动是增强量子电池性能（包括功和充电功率）的有效途径，并且 QATM 在其中扮演着重要的结构化环境角色，能够过滤退相干并诱导有益的非马尔可夫效应。

Abstract: In this work, we study a hybrid quantum system composed of a quantum battery
and a coherence-driven charger interacting with a Quantum Autonomous Thermal
Machine (QATM). The QATM, made of two qubits each coupled to Markovian Bosonics
thermal reservoirs at different temperatures, acts as a structured environment
that mediates energy and coherence transfer between the charger and the
battery. By presenting a coherent driving field on the charger, we investigate
the coherence injection effect on the dynamics, including non-Markovianity,
power of charging, coherence storage, and ergotropy. We show that the QATM
effectively filters the decoherence induced by the thermal baths and induces
non-Markovian memory effects due to correlation backflow. Our results
demonstrate that coherence driving enhances the battery's ergotropy and power
of charging by preserving the internal energy of the charger and facilitating
coherent energy transfer.

</details>


### [287] [Learning Neural Decoding with Parallelism and Self-Coordination for Quantum Error Correction](https://arxiv.org/abs/2509.03815)
*Kai Zhang,Situ Wang,Linghang Kong,Fang Zhang,Zhengfeng Ji,Jianxin Chen*

Main category: quant-ph

TL;DR: 现有神经网络解码器AlphaQubit虽然精度高，但缺乏并行处理能力，无法实时解码超导量子比特产生的综合信息流。本研究提出了一种专门为滑动窗口解码设计的循环式Transformer神经网络，该网络能处理局部纠错信息并自我协调跨窗口操作，解决了吞吐量限制，使其适用于容错量子计算。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络解码器（如AlphaQubit）在准确性方面优于传统方法，但缺乏必要的并行处理能力，无法满足容错量子计算中超导量子比特实时解码的需求。此外，将AlphaQubit集成到基于滑动窗口的并行解码方案中存在挑战，因为它被训练用于输出整个内存实验的全局逻辑校正，而非易于集成的局部物理校正。

Method: 训练一个循环式的、基于Transformer的神经网络，专门用于滑动窗口解码。该网络为每个窗口输出一个单独的比特，但训练标签是从一组一致的局部校正推导出来的，并且同时在各种类型的解码窗口上进行训练。这种方法使得网络能够跨相邻窗口进行自我协调。

Result: 通过所提出的方法，实现了高精度的并行解码，能够处理任意长度的内存实验。这解决了先前阻碍AlphaQubit类解码器应用于容错量子计算的吞吐量限制问题。

Conclusion: 本研究成功训练了一种能够处理局部校正并进行跨窗口自我协调的循环式Transformer神经网络，解决了现有神经网络解码器的并行处理瓶颈，为实现容错量子计算中的实时解码提供了有效解决方案。

Abstract: Fast, reliable decoders are pivotal components for enabling fault-tolerant
quantum computation. Neural network decoders like AlphaQubit have demonstrated
significant potential, achieving higher accuracy than traditional
human-designed decoding algorithms. However, existing implementations of neural
network decoders lack the parallelism required to decode the syndrome stream
generated by a superconducting logical qubit in real time. Moreover,
integrating AlphaQubit with sliding window-based parallel decoding schemes
presents non-trivial challenges: AlphaQubit is trained solely to output a
single bit corresponding to the global logical correction for an entire memory
experiment, rather than local physical corrections that can be easily
integrated.
  We address this issue by training a recurrent, transformer-based neural
network specifically tailored for sliding-window decoding. While our network
still outputs a single bit per window, we derive training labels from a
consistent set of local corrections and train on various types of decoding
windows simultaneously. This approach enables the network to self-coordinate
across neighboring windows, facilitating high-accuracy parallel decoding of
arbitrarily long memory experiments. As a result, we resolve the throughput
limitation that previously prohibited the application of AlphaQubit-type
decoders in fault-tolerant quantum computation.

</details>


### [288] [Tunneling induced giant Photonic Spin Hall effect in quantum wells](https://arxiv.org/abs/2509.03844)
*Fazal Badshah,Imed Boukhris,Mohamed Sultan Al-Buriah,Yuan Zhou,Muhammad Idrees,Ziauddin*

Main category: quant-ph

TL;DR: 我们提出了一种在不对称双 AlGaAs/GaAs 量子阱中研究光子自旋霍尔效应（PSHE）的理论方法，并能通过外部控制光束和可调谐隧道势垒来调控量子干涉，以增强和控制中红外光子器件中的 PSHE。


<details>
  <summary>Details</summary>
Motivation: 研究在不对称双 AlGaAs/GaAs 量子阱中实现和调控光子自旋霍尔效应（PSHE），并探索增强 PSHE 的方法。

Method: 通过理论研究，利用不对称双 AlGaAs/GaAs 量子阱作为腔内介质，并结合外部控制光束和可调谐隧道势垒来调控量子干涉，从而实现对 PSHE 的精确操控。

Result: 实现了巨大的水平 PSHE，并且通过引入吸收和增益辅助的腔板，可以进一步放大水平 PSHE，从而实现更明显的光子自旋分离。

Conclusion: 该研究为理解半导体量子阱中的光-物质相互作用提供了新的见解，并为在中红外光子器件中增强和控制 PSHE 提供了一条有效的途径。

Abstract: We propose a theoretical investigation of the photonic spin Hall effect
(PSHE) in a mid-infrared probe field by employing an asymmetric double
AlGaAsGaAs quantum well as the intracavity medium. The system is designed such
that an external control beam together with tunable tunneling barriers
regulates the quantum interference of the probe tunneling process. This
configuration enables precise manipulation of the PSHE for both horizontally
and vertically polarized components of light. Our analysis reveals the
emergence of a giant horizontal PSHE in the quantum well based cavity system.
Moreover, by incorporating absorptive and gain-assisted cavity slabs, the
horizontal PSHE is further amplified, leading to an even more pronounced
photonic spin separation. The results provide novel insights into light matter
interactions in semiconductor quantum wells and suggest an effective route for
enhancing and controlling the PSHE in mid-infrared photonic devices.

</details>


### [289] [Decoherence mitigation for geometric quantum computation](https://arxiv.org/abs/2509.03856)
*X. Y. Sun,P. Z. Zhao*

Main category: quant-ph

TL;DR: 该研究提出了一种仅基于物理量子比特的几何量子计算方案，以减少资源消耗和实现复杂性，并能缓解更一般的退相干噪声。


<details>
  <summary>Details</summary>
Motivation: 之前的几何量子计算方案需要多个物理量子比特来编码一个逻辑量子比特，增加了资源消耗和实现难度。本研究旨在克服这一限制。

Method: 提出了一种仅基于物理量子比特的几何量子计算方案，并考虑了量子比特与环境之间最一般的相互作用，以缓解退相干噪声。

Result: 该方案避免了额外的物理量子比特资源开销和逻辑量子比特操作的复杂性，并能缓解包括非均匀退相干在内的更一般性的退相干噪声。

Conclusion: 该方案为实现具有退相干抑制能力的几何量子控制提供了一种更实际有效的方法。

Abstract: Geometric phases depend only on the evolution path determined by the closed
circuit in the projective Hilbert space but not on evolution details of the
quantum system, leading to geometric quantum computation possessing some
intrinsic robustness against control errors. Coordinated with dynamical
decoupling, geometric quantum computation admits additional resilience to the
environment-induced decoherence. However, the previous schemes of geometric
quantum computation protected by dynamical decoupling require multiple physical
qubits to encode a logical qubit, which undoubtedly increases the consumption
of physical-qubit resources and the difficulty in the implementation of the
logical-qubit manipulation based on physical-qubit driving. In this work, we
put forward a scheme of decoherence-mitigated geometric quantum computation
based only on physical qubits rather than logical qubits, hence avoiding the
additional overhead of physical-qubit resources for logical-qubit encoding as
well as the difficulty in the manipulation of logical qubits. Moreover, our
scheme focuses on the most general interaction between an individual qubit and
its environment so that it mitigates not just dephasing noise but rather
regular decoherence. Our proposal thus represents a more realistic and
effective approach towards the realization of geometric control with
decoherence mitigation.

</details>


### [290] [Near-Term Quantum-Computing-Inspired Sampling for Community Detection in Low-Modularity Graphs](https://arxiv.org/abs/2509.03864)
*Joseph Geraci,Luca Pani*

Main category: quant-ph

TL;DR: 量子启发式算法在低模块度网络社区检测中取得显著进展，通过改进模块度Q值和引入“模块度恢复差距”作为异常检测信号，并在网络安全等领域展现应用潜力。


<details>
  <summary>Details</summary>
Motivation: 低模块度网络（Q < 0.2）对传统社区检测算法构成挑战，因为它们容易陷入局部最优。

Method: 提出利用受量子现象启发的非经典采样技术（如Porter Thomas分布、Haar随机状态和超均匀点过程）来改进社区检测算法，使其能够跳出模块度优化的平台期。

Result: 在低Q值图上，所提出的算法能够生成多样化的高质量分区，并将模块度Q值提高了15%到25%。在真实的高模块度网络（CTU 13僵尸网络流量）上进行实验，发现“模块度恢复差距”（MRG）接近于零，表明该方法在不存在隐藏结构时不会人为提高模块度。

Conclusion: 量子启发式采样方法能够显著增强在低模块度情况下的社区检测能力，并且“模块度恢复差距”可以作为一种有效的异常检测信号。这些方法可以在不改变图结构的情况下扩展现有算法（如Leiden）的搜索空间，发现传统启发式方法遗漏的分区，并在网络安全、金融传染病建模、供应链、脑部疾病连接组学和危机社交媒体分析等领域具有广泛的应用前景。

Abstract: Low modularity networks (Q < 0.2) challenge classical community detection
algorithms, which get trapped in local optima. We introduce quantum inspired
community detection algorithms leveraging non classical sampling techniques to
escape modularity optimization plateaus. Using distributions inspired by
quantum phenomena including Porter Thomas distributions, Haar random states,
and hyperuniform point processes, our approach generates diverse high quality
partitions that improve modularity on difficult low Q graphs. We demonstrate 15
to 25% gains in modularity Q over classical methods (Louvain, Leiden). We
define a "Modularity Recovery Gap" (MRG), the increase in Q achieved by quantum
inspired refinement, and show this serves as a powerful anomaly detection
signal. In experiments on high modularity networks (CTU 13 botnet traffic), MRG
is near zero, confirming our method doesn't inflate modularity when no hidden
structure exists. Results suggest quantum inspired sampling substantially
enhances community detection in low modularity regimes, with applications in
cybersecurity (stealthy APT/botnet detection), financial contagion modeling,
disrupted supply chains, diseased brain connectomes, and crisis era social
media analysis. Our refinements expand the search space of algorithms like
Leiden without altering the graph, revealing partitions classical heuristics
miss. While such partitions need not represent ground truth communities, they
illustrate how near term quantum sampling can reshape optimization landscapes
and enhance sensitivity to weak structure.

</details>


### [291] [Electrically pumped ultra-efficient quantum frequency conversion on thin film lithium niobate chip](https://arxiv.org/abs/2509.03869)
*Xina Wang,Xu-Feng Jiao,Bo Cao,Yang Liu,Xiu-Ping Xie,Ming-Yang Zheng,Qiang Zhang,Jian-Wei Pan*

Main category: quant-ph

TL;DR: 该论文展示了首个混合集成的量子频转换（QFC）芯片，该芯片基于薄膜铌酸锂平台，实现了电信波段和可见光波段的连接。


<details>
  <summary>Details</summary>
Motivation: 为了推进未来的量子技术，需要高效率、小尺寸、低功耗和高可扩展性的芯片级集成QFC组件。

Method: 利用具有超高归一化转换效率（386,000 %/W）的周期性极化微环谐振器。

Result: 实现了360 μW的超低泵浦功率，比传统直波导方案低两个数量级。通过注入电流，实现了57%的片上量子效率和约7k counts per second的噪声计数。

Conclusion: 这种电泵浦、集成且可扩展的QFC芯片将显著推进量子网络的集成和芯片级量子光学系统的发展。

Abstract: Quantum frequency conversion (QFC) plays a crucial role in constructing
seamless interconnection between quantum systems operating at different
wavelengths. To advance future quantum technology, chip-scale integrated QFC
components, featuring high efficiency, small footprint, low power consumption
and high scalability, are indispensable. In this work, we demonstrate the first
hybrid integrated QFC chip on thin film lithium niobate platform that connects
the telecom and visible bands. Benefiting from the periodically poled microring
resonator with ulta-high normalized conversion efficiency of 386,000 %/W, an
ultra-low pump power of 360 {\mu}W is achieved which is more than two orders of
magnitude lower than traditional straight waveguide scheme. By injecting
current into the chip, an on-chip quantum efficiency of 57% and a noise count
of ~ 7k counts per second are achieved. Such an electrically pumped, integrated
and scalable QFC chip would significantly advancing the integration of quantum
network and the development of chip-scale quantum optical systems.

</details>


### [292] [Solving quantum-inspired dynamics on quantum and classical annealers](https://arxiv.org/abs/2509.03952)
*Philipp Hanussek,Jakub Pawłowski,Zakaria Mzaouali,Bartłomiej Gardas*

Main category: quant-ph

TL;DR: 本篇论文提出了一个受物理动力学启发的基准测试套件，用于挑战量子和经典计算机。


<details>
  <summary>Details</summary>
Motivation: 将量子系统的实时演化问题转化为经典计算机可解的 QUBO 问题，并与量子退火器进行性能比较。

Method: 使用并行时间编码将 n 亮光子、可能非厄米的哈密顿量的实时传播器转换为 QUBO 问题，并在 D-Wave 量子退火器、模拟退火和 VeloxQ 上运行。

Result: D-Wave Advantage2 的性能优于其前代产品，但 VeloxQ 目前在性能上保持领先。

Conclusion: 本研究提出的并行时间 QUBO 框架是一个多功能的测试平台，用于跟踪和评估量子动力学模拟的进展。

Abstract: We propose a practical benchmarking suite inspired by physical dynamics to
challenge both quantum and classical computers. Using a parallel in time
encoding, we convert the real-time propagator of an $n$-qubit, possibly
non-Hermitian, Hamiltonian into a quadratic-unconstrained binary optimisation
(QUBO) problem. The resulting QUBO instances are executed on D-Wave quantum
annealers as well as using two classical solvers, Simulated Annealing and
VeloxQ, a state-of-the-art classical heuristic solver. This enables a direct
comparison. To stress-test the workflow, we use eight representative models,
divided into three groups: (i)~single-qubit rotations, (ii)~multi-qubit
entangling gates (Bell, GHZ, cluster), and (iii)~$\text{PT}$-symmetric,
parity-conserving and other non-Hermitian generators. Across this diverse suite
we track the success probability and time to solution, which are well
established measures in the realm of heuristic combinatorial optimisation. Our
results show that D-Wave Advantage2 consistently surpasses its predecessor,
while VeloxQ presently retains the overall lead, reflecting the maturity of
classical optimisers. We highlight the rapid progress of analog quantum
optimisation, and suggest a clear trajectory toward quantum competitive
dynamics simulation, by establishing the parallel in time QUBO framework as a
versatile test-bed for tracking and evaluating that progress.

</details>


### [293] [LATTE: A Decoding Architecture for Quantum Computing with Temporal and Spatial Scalability](https://arxiv.org/abs/2509.03954)
*Kai Zhang,Jubo Xu,Fang Zhang,Linghang Kong,Zhengfeng Ji,Jianxin Chen*

Main category: quant-ph

TL;DR: LATTE是一种结合FPGA和CPU的量子纠错解码架构，旨在解决量子计算扩展性的关键需求：延迟、精度、吞吐量和传输带宽。它采用分层设计，CPU上的流式块解码系统支持时空并行化，FPGA上的轻量级神经网络解码单元则加速了本地解码过程，减少了带宽需求。LATTE的性能优于现有方法，在特定噪声条件下可显著降低带宽并提高解码速度，实现大规模容错量子计算的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 量子纠错解码架构对于在嘈杂的量子设备上实现理想量子计算至关重要，需要解决扩展性方面的挑战，包括延迟、精度、吞吐量和传输带宽。

Method: LATTE采用分层设计：1. CPU上的全流式、异步块解码系统，支持时空并行化。2. FPGA上的轻量级、高精度神经网络本地解码单元，对块解码系统透明，以减少传输带宽并加速解码。

Result: LATTE在精度上与基础解码器相当，实现了实时解码吞吐量，并显著降低了带宽需求和计算资源。在p=0.001的噪声水平下，LATTE实现了超过90%的传输带宽降低和平均6.4倍的单块解码加速。在流式解码场景下，LATTE实现了恒定且低延迟（比现有流式解码实现快16-20倍），在量子内存实验中资源消耗少（仅需2个线程即可解码距离高达17的表面码）。此外，LATTE通过高度并行的解码操作最小化了多补丁测量实验中的延迟。

Conclusion: LATTE通过其创新的混合解码架构，在精度、速度、带宽和资源利用方面取得了显著改进，有效解决了量子纠错解码的可扩展性问题，为大规模容错量子计算奠定了基础。

Abstract: Quantum error correction allows inherently noisy quantum devices to emulate
an ideal quantum computer with reasonable resource overhead. As a crucial
component, decoding architectures have received significant attention recently.
In this paper, we introduce LATTE, a FPGA-CPU hybrid decoding architecture
aiming to address the key requirements of scaling up in lattice surgery quantum
computation -- Latency, Accuracy, Throughput and Transmission Bandwidth, in an
Eclectic manner. LATTE follows a hierarchical design: (1) A fully streaming and
asynchronous block decoding system on CPU to enable parallelization both
temporally and spatially. (2) A super-light yet accurate neural local decoding
unit integrated with quantum control hardware on FPGA, which remains
\emph{transparent} to the block decoding system, effectively reducing
transmission bandwidth and accelerating the decoding process. LATTE delivers
accuracy on par with the base decoder while achieving real-time decoding
throughput and significantly reducing both bandwidth requirements and
computational resources, enabling a level of scalability far beyond previous
approaches. Under circuit-level noise $p=0.001$, LATTE achieves over
$\mathbf{90\%}$ reduction in transmission bandwidth and a $\mathbf{6.4\times}$
speedup on average in single-block decoding. In the \emph{streaming decoding}
scenario: (1) LATTE achieves constant and low latency
($\mathbf{16\times}$-$\mathbf{20\times}$ speedup over existing streaming
decoding implementations) in arbitrarily long quantum memory experiments, with
near-optimal resources -- merely $\mathbf{2}$ threads are sufficient for
decoding the surface code with distance up to $17$. (2) LATTE minimizes latency
in multi-patch measurement experiments through highly parallelized decoding
operations. These combined efforts ensure sufficient scalability for
large-scale fault-tolerant quantum computing.

</details>


### [294] [Non-Gaussian Photon Correlations in Weakly Coupled Atomic Ensembles](https://arxiv.org/abs/2509.03970)
*Yangming Wang,Sahand Mahmoodian*

Main category: quant-ph

TL;DR: 研究表明，原子系综与光学模式相互作用可以产生具有非高斯关联的光，并且通过微扰图解展开可以解释这种现象。


<details>
  <summary>Details</summary>
Motivation: 原子系综与光学模式的相互作用是生成具有特定关联特性的光的重要途径，本研究旨在探索其非高斯关联的产生机制。

Method: 采用散射理论形式，并结合微扰图解展开，分析了多光子相互作用，计算了透射光的连通三阶关联函数 $g_c^{(3)}$ 的时间演化。

Result: 结果显示，光子-光子相互作用导致透射光具有非零的连通三阶关联函数 $g_c^{(3)}$，并且微扰计算结果与级联主方程模拟结果在实验相关光度下吻合良好。

Conclusion: 研究预测，纳米光纤耦合的原子系综有望在实验中验证这些非高斯关联的产生。

Abstract: We develop a scattering theory formalism and use it to predict that a
resonantly driven atomic ensemble weakly coupled to an optical mode can
generate light with non-Gaussian correlations. Our approach -- based on a
perturbative diagrammatic expansion of multi-photon interactions -- shows that
photon-photon interaction mediated by the emitters causes the transmitted light
to have a non-vanishing connected third-order correlation function $g_c^{(3)}$.
We explain the temporal pattern of $g_c^{(3)}$ using the interaction processes
in our diagrammatic expansion. A quantitative comparison with cascaded master
equation simulations for small ensembles with optical depth $\mathrm{OD}\leq 2$
confirms that the perturbative results remain accurate across experimentally
relevant optical depths and for drive strengths large enough to make the
predicted non-Gaussian signatures detectable. We anticipate that
state-of-the-art nanofibre-coupled atomic ensembles can experimentally
demonstrate our predictions.

</details>


### [295] [Real-time adaptive quantum error correction by model-free multi-agent learning](https://arxiv.org/abs/2509.03974)
*Manuel Guatto,Francesco Preti,Michael Schilling,Tommaso Calarco,Francisco Andrés Cárdenas-López,Felix Motzoi*

Main category: quant-ph

TL;DR: 该研究提出了一种基于强化学习（RL）的两级框架，用于实现能够自适应时变噪声的量子纠错（QEC）。


<details>
  <summary>Details</summary>
Motivation: 探索是否能构建自适应时变噪声的量子纠错（QEC）方案。

Method: 第一级利用无模型多智能体强化学习（MARL）自动发现QEC的完整周期（逻辑状态编码、稳定器测量和恢复），无需先验知识，仅依赖正交性条件，并可发现适用于多层量子架构的新型QEC码。第二级引入BRAVE（Bandit Retraining for Adaptive Variational Error correction）算法，通过动态调整变分层来适应时变噪声，同时最小化计算开销和缩短再训练时间。

Result: 结合MARL和BRAVE方法，在经受时变噪声（比特翻转和相位翻转错误）的多层系统上进行测试，与传统QEC方案相比，逻辑保真度提高了两个数量级以上。

Conclusion: 提出的两级RL框架能够有效地实现自适应时变噪声的量子纠错，显著提高了逻辑保真度。

Abstract: Can we build efficient Quantum Error Correction (QEC) that adapts on the fly
to time-varying noise? In this work we say yes, and show how. We present a two
level framework based on Reinforcement Learning (RL) that learns to correct
even non-stationary errors from scratch. At the first level we take advantage
of model-free Multi-Agent RL (MARL) to automatically discover full QEC cycle --
logical state encoding, stabilizer measurements, and recovery -- without any
prior system knowledge, relying only on orthogonality conditions. Leveraging
the stabilizer formalism, we demonstrate that our MARL framework can discover
novel QEC codes tailored for multi-level quantum architectures. At the second
level we introduce BRAVE (Bandit Retraining for Adaptive Variational Error
correction), an efficient algorithm that tunes the variational layer on the fly
to change the physical basis of the errors, adapting the QEC code to
time-varying noise while minimizing computational overhead and reducing the
number of retraining steps. By combining our MARL and BRAVE approaches and
testing them on multi-level systems subjected to competing bit- and phase-flip
errors over time across diverse scenarios, we observed an improvement in
logical fidelity by more than an order of magnitude -- under time-dependent
noise channels -- compared to conventional QEC schemes.

</details>


### [296] [Forecasting Low-Dimensional Turbulence via Multi-Dimensional Hybrid Quantum Reservoir Computing](https://arxiv.org/abs/2509.04006)
*L. Salatino,L. Mariani,A. Giordano,F. D'Amore,C. Mastroianni,L. Pontieri,A. Vinci,C. Gencarelli,L. Primavera,F. Plastina,J. Settino,F. Carbone*

Main category: quant-ph

TL;DR: 本研究提出了一种混合量子-经典水库计算架构，用于处理多变量时间序列，并通过量子演化和经典记忆增强相结合的方式来提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统的预测方法在处理物理学中复杂的非线性动力学和多尺度相互作用时存在局限性，而量子水库计算（QRC）利用量子系统希尔伯特空间的高维性，为信息处理提供了一种有前景的范式。

Method: 采用包含输入调制动力学和时间复用的五比特横向场伊辛哈密顿量，并结合经典记忆增强，以处理多变量时间序列。通过扫描量子系统参数空间来优化预测性能，并以有效预测时间作为衡量标准。

Result: 该混合量子方法成功应用于二维纳维-斯托克斯方程的低维截断和 Lorenz-63 系统这两个典型的流体动力学混沌模型，展现了在处理多尺度动力学和非线性问题上的鲁棒性和可靠性。

Conclusion: 所提出的混合量子方法为模拟复杂的非线性时间序列提供了一个灵活的平台，在预测复杂动力学方面表现出显著的优势。

Abstract: The prediction of complex dynamics remains an open problem across many
domains of physics, where nonlinearities and multiscale interactions severely
limit the reliability of conventional forecasting methods. Quantum reservoir
computing (QRC) has emerged as a promising paradigm for information processing
by exploiting the high dimensionality of the Hilbert space, where the dynamics
of quantum systems take place. Here, we introduce a hybrid quantum-classical
reservoir architecture capable of handling multivariate time series through
quantum evolution combined with classical memory enhancement. Our model employs
a five-qubit transverse-field Ising Hamiltonian with input-modulated dynamics
and temporal multiplexing, enabling the encoding of input signals over multiple
timescales. We apply this framework to two paradigmatic models of chaotic
behavior in fluid dynamics, where multiscale dynamics and nonlinearities play a
dominant role: a low-dimensional truncation of the two-dimensional
Navier-Stokes equations and the Lorenz-63 system. By systematically scanning
the quantum system's parameter space, we identify regions that maximize
forecasting performance, as measured by the Valid Prediction Time. The observed
robustness and reliable performances for both dynamical systems suggest that
this hybrid quantum approach offers a flexible platform for modelling complex
nonlinear time series.

</details>


### [297] [A Framework for Quantum Data Center Emulation Using Digital Quantum Computers](https://arxiv.org/abs/2509.04029)
*Seyed Navid Elyasi,Seyed Morteza Ahmadian,Paolo Monti,Jun Li,Rui Lin*

Main category: quant-ph

TL;DR: 该研究提出了一个在单量子处理器上模拟分布式量子计算（DQC）系统的框架，用于评估量子数据中心（QDC）的理论模型，并考虑了连接引起的噪声。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，单芯片架构的局限性（如量子比特数量少）促使人们对模块化量子计算和量子数据中心（QDCs）产生了兴趣。然而，DQC的实现面临着远程门执行等技术挑战，并且缺乏评估DQC系统理论提案的实用仿真工具。

Method: 研究提出了一个框架，利用单个量子处理器模拟DQC系统。该框架将现有量子处理器单元（QPU）的物理量子比特耦合图划分为多个逻辑QPU，并引入了一个基于量子碰撞动力学的实验性噪声模型来量化连接引起的噪声（模拟光纤连接的QPU）。

Result: 该框架已在IBM的量子硬件上得到验证，成功演示了在有噪声条件下执行远程门。此外，研究还实现了Grover搜索和量子傅里叶变换（QFT）的分布式版本，表明该框架可以在互联QPU之间以合理的保真度执行复杂电路。Grover算法的仿真结果与两个通过光纤连接的离子阱QPU之间的实际实验实现相符。

Conclusion: 该研究提供了一个通用的仿真工具，用于研究QDC行为，并考虑了连接引起的通信噪声。它还提供了一种无需专用连接硬件即可验证分布式量子协议的实用方法，证明了该框架的可行性和准确性。

Abstract: As quantum computing hardware advances, the limitations of single-chip
architectures, particularly in terms of small qubit count, have sparked growing
interest in modular quantum computing systems and Quantum Data Centers (QDCs).
These architectures interconnect multiple quantum processor units (QPUs) to
overcome physical constraints and support complex quantum algorithms. However,
the implementation of distributed quantum computing (DQC) faces significant
technical challenges, especially in the execution of remote gates. Moreover, no
practical emulation tool currently exists to evaluate theoretical proposals of
various DQC systems. In this work, we propose a framework that emulates a DQC
system using a single quantum processor. We partition the physical qubit
coupling map of an existing QPU into multiple logical QPUs, and introduce an
experimentally grounded noise model based on quantum collision dynamics to
quantify the interconnect-induced noise, representing fiber-connected QPUs. The
framework is validated using IBM's quantum hardware, demonstrating the
successful execution of remote gates under noisy conditions. Furthermore, we
implement distributed versions of Grover's search and the Quantum Fourier
Transform (QFT), showing that complex circuits can be executed within the
proposed framework with reasonable fidelity across interconnected QPUs. The
emulation result of Grover's algorithm aligns with the real-world experimental
implementations between two Ion-trapped QPUs interconnected by optical fiber,
which demonstrate the feasibility and accuracy of our framework. Overall, this
work provides a versatile emulation tool for investigating QDC behavior while
accounting for interconnect-induced communication noise and offers a practical
method for validating distributed quantum protocols without requiring
specialized interconnect hardware.

</details>


### [298] [Temperature-induced measurement sensitivity enhancement via imaginary weak values](https://arxiv.org/abs/2509.04048)
*Lorena Ballesteros Ferraz,Alexandre Matzkin,Alok Kumar Pan*

Main category: quant-ph

TL;DR: 在测量特定量子系统的过程中，通过控制初始探针态的混合度（由温度控制），我们研究了弱测量和后选择如何提高测量灵敏度。结果表明，在某些情况下，提高温度可以提高信噪比，但会受到弱测量有效性的限制。对于量子费舍尔信息，纯态探针的后选择并不能提高精度，而混合态探针的后选择可以提高精度，甚至在温度升高时可能无限提高精度，表明热噪声有时可以提高测量精度。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究在初始探针态混合的情况下，弱测量和后选择技术在提高测量灵敏度方面的潜力。

Method: 本文采用全阶耦合处理方法，研究了混合探针态的密度算符，并通过温度控制其混合度。在应用后选择后，重点分析了最终探针态的信噪比和量子费舍尔信息。

Result: 研究发现，增加温度可以在某些情况下提高信噪比，但会受到弱测量有效性条件的限制。对于量子费舍尔信息，当初始探针态为纯态时，后选择并不能提高精度；但当初始探针态为混合态时，后选择可以提高量子费舍尔信息，甚至可能随温度无界增长。

Conclusion: 本文的结论是，虽然弱测量和后选择在提高测量灵敏度方面有潜力，但其效果受到初始态混合度和测量有效性条件的限制。值得注意的是，在混合态探针的情况下，热噪声可以反常地提高测量精度。

Abstract: We investigate the potential of weak measurement and post-selection to
enhance measurement sensitivity when the initial probe state is mixed. In our
framework, the mixedness of the probe's density operator is controlled by
temperature. We focus on two key quantities: the signal-to-noise ratio and the
quantum Fisher information of the final probe state, evaluated after
post-selection is applied on the system. Our analysis employs a rigorous,
all-order coupling treatment of measurement, demonstrating that the
signal-to-noise ratio can be enhanced in certain scenarios by increasing the
temperature. However, this enhancement is fundamentally constrained by the
validity conditions of the weak measurement regime. Regarding the quantum
Fisher information, we find that for a pure probe state, incorporating
post-selection does not improve precision beyond the standard
(non-post-selected) strategy when the post-selection probability is accounted
for. In contrast, when the initial probe state is mixed, the quantum Fisher
information for the probe state after post-selection in the system can surpass
that of the standard strategy. Notably, we show that the quantum Fisher
information might diverge and grow unboundedly with temperature, illustrating a
scenario where thermal noise can, counterintuitively, enhance metrological
precision.

</details>


### [299] [Quantum Zeno effect versus adiabatic quantum computing and quantum annealing](https://arxiv.org/abs/2509.04057)
*Naser Ahmadiniaz,Dennis Kraft,Gernot Schaller,Ralf Schützhold*

Main category: quant-ph

TL;DR: Decoherence limits the performance of adiabatic Grover's algorithm and similar quantum schemes due to the quantum Zeno effect. Gradual state changes or error correction may help.


<details>
  <summary>Details</summary>
Motivation: The paper studies the impact of decoherence on the adiabatic version of Grover's quantum search algorithm and analogous quantum schemes.

Method: The study analyzes the effect of a general coupling to an environment on the adiabatic quantum search algorithm, considering the quantum Zeno effect and its limitations on quantum transitions. It also generalizes the findings to other adiabatic quantum algorithms and quantum annealing schemes.

Result: Decoherence, through the quantum Zeno effect, strongly limits the performance and quantum speed-up of adiabatic Grover's algorithm and similar schemes by inhibiting quantum transitions. This limitation is expected to apply universally to adiabatic quantum algorithms and quantum annealing schemes based on Landau-Zener transitions.

Conclusion: The quantum Zeno effect imposed by decoherence significantly restricts the effectiveness of adiabatic quantum algorithms. Potential solutions include employing more gradual quantum state changes or implementing error-correction techniques like the spin-echo method.

Abstract: For the adiabatic version of Grover's quantum search algorithm as proposed by
Roland and Cerf, we study the impact of decoherence caused by a rather general
coupling to some environment. For quite generic conditions, we find that the
quantum Zeno effect poses strong limitations on the performance (quantum
speed-up) since the environment effectively measures the state of the system
permanently and thereby inhibits or slows down quantum transitions.
Generalizing our results, we find that analogous restrictions should apply
universally to adiabatic quantum algorithms and quantum annealing schemes which
are based on isolated Landau-Zener type transitions at avoided level crossings
(similar to first-order phase transitions). As a possible resort, more gradual
changes of the quantum state (as in second-order phase transitions) or suitable
error-correcting schemes such as the spin-echo method may alleviate this
problem.

</details>


### [300] [Local Invariance of Divergence-based Quantum Information Measures](https://arxiv.org/abs/2509.04079)
*Christopher Popp,Tobias C. Sutter,Beatrix C. Hiesmayr*

Main category: quant-ph

TL;DR: 本篇论文提出了一种基于广义散度的量子信息度量方法，并证明了其在局部等距或酉变换下的不变性，从而改进了对量子信息度量的计算和优化。


<details>
  <summary>Details</summary>
Motivation: 量子信息科学中，量子信息量（如互信息和熵）对于表征量子系统和协议至关重要。

Method: 利用反转通道和数据处理不等式，证明了基于广义散度的信息度量在局部等距或酉变换下的不变性，不依赖于散度的具体函数形式，并同时适用于渐近和单次场景。

Result: 在局部等距或酉变换下，基于广义散度的量子信息量具有不变性。

Conclusion: 所提出的不变性可用于改进量子信息量的计算，或优化性能由某个不变度量决定的协议及其输出状态，从而提升了对众多具有操作意义的量子信息度量的表征和计算能力，应用于量子信息处理的各个领域。

Abstract: Quantum information quantities, such as mutual information and entropies, are
essential for characterizing quantum systems and protocols in quantum
information science. In this contribution, we identify types of information
measures based on generalized divergences and prove their invariance under
local isometric or unitary transformations. Leveraging the reversal channel for
local isometries together with the data processing inequality, we establish
invariance for information quantities used in both asymptotic and one-shot
regimes without relying on the specific functional form of the underlying
divergence. These invariances can be applied to improve the computation of such
information quantities or optimize protocols and their output states whose
performance is determined by some invariant measure. Our results improve the
capability to characterize and compute many operationally relevant information
measures with application across the field of quantum information processing.

</details>


### [301] [Efficient QKD in Non-Ideal Scenarios with User-Defined Output Length Requirements](https://arxiv.org/abs/2509.04140)
*Andrés Martín-Megino,Blanca López,Iván Vidal Fernández,Francisco Valera Pintor*

Main category: quant-ph

TL;DR: 该研究提出了一种可变长度的BB84协议变体，用于在实际的量子密钥分发（QKD）网络中满足用户定义的密钥长度需求，以优化资源利用率。


<details>
  <summary>Details</summary>
Motivation: 实际的QKD系统需要高效的密钥补充策略来应对高需求，避免缓冲区耗尽和资源浪费，同时优化时间和资源利用率。

Method: 提出一种可变长度BB84协议，动态配置初始参数以生成所需长度的密钥，并使用模拟工具进行验证。

Result: 所提出的策略在BB84框架下进行了实现和评估，结果表明其有助于优化量子资源使用并支持密钥管理。

Conclusion: 该方法为满足用户定义的密钥长度需求提供了有效途径，有助于扩展和加强安全的量子网络。

Abstract: Quantum Key Distribution (QKD) enables two parties to securely share
encryption keys by leveraging the principles of quantum mechanics, offering
protection against eavesdropping. In practical implementations, QKD systems
often rely on a layered architecture where a key manager stores secret key
material in a buffer and delivers it to higher communication layers as needed.
However, this buffer can be depleted under high demand, requiring efficient
replenishment strategies that minimize resource waste. Given the importance of
optimizing time and resources in quantum cryptography protocols, we introduce a
variable-length adaptation of the BB84 protocol designed to meet user-defined
output key length constraints in non-ideal scenarios. We present a method for
dynamically configuring the protocol's initial parameters to generate secret
keys of a desired length. To validate our approach, we developed simulation
tools to model general QKD networks and discrete-variable protocols. These
tools were used to implement and evaluate our strategies, which were developed
within the BB84 framework but can be extended to other QKD protocols under
reasonable assumptions. The results highlight their usefulness in optimizing
quantum resource usage and supporting key management, contributing to the
long-term goal of scaling and strengthening secure quantum networks.

</details>


### [302] [Wavefront correction of high-dimensional two-photon states via coherence-entanglement transfer](https://arxiv.org/abs/2509.04170)
*Shaurya Aarav,Hugo Defienne*

Main category: quant-ph

TL;DR: 利用量子态本身进行波前校正，无需额外的信标光，从而简化了实验装置并提高了效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的量子通信和成像需要可靠地传输量子光学态，但传播路径中的畸变和散射会干扰信号。以往的方法依赖于独立的信标光来校正波前畸变，但这会增加实验复杂度，并且信标光与非经典态的匹配具有挑战性。

Method: 通过泵浦整形控制空间纠缠双光子态的纠缠度，使其在一种情况下表现为高维纠缠态，在另一种情况下表现为经典相干态。利用相干态测量传播信道的传输矩阵，并通过空间光调制器校正畸变，从而实现高维纠缠态的无差错传输。

Result: 提出了一种快速有效的波前校正方法，利用量子态本身进行校正。

Conclusion: 该方法为基于高维空间纠缠态的量子成像和通信协议的实际应用铺平了道路。

Abstract: Reliable transmission of quantum optical states through real-world
environments is key for quantum communication and imaging. Yet, aberrations and
scattering in the propagation path can scramble the transmitted signal and
hinder its use. A typical strategy is to employ a classical beacon beam to
learn and then correct for the wavefront distortions. However, relying on a
separate light source increases the overhead in the experimental apparatus.
Moreover, the beacon light must closely match the non-classical state in
polarization, wavelength, and even temporal bandwidth, which is highly
challenging in practice. Here, we introduce a fast and efficient wavefront
correction approach where we use the quantum state itself to correct for
optical distortion. Via pump shaping, we control the degree of entanglement in
the spatially-entangled two-photon state so that it behaves either as a
high-dimensional entangled state or as a classical coherent state. The latter
case is used to efficiently measure the transmission matrix of the propagation
channel and correct its distortions with a spatial light modulator, thereby
enabling the transmission of the high-dimensional entangled state with minimal
errors. Our approach paves the way for the practical implementation of quantum
imaging and communication protocols based on high-dimensional spatially
entangled states.

</details>


### [303] [Searching for a physical description relative to a quantum system](https://arxiv.org/abs/2509.04186)
*Henrique A. R. Knopki,Renato M. Angelo*

Main category: quant-ph

TL;DR: 量子参考系的研究在处理多体系统时面临不可约的困难，需要新的策略。


<details>
  <summary>Details</summary>
Motivation: 当前关于量子参考系的研究在预测量子参考系可及自由度方面存在不足，本研究旨在解决这一问题。

Method: 研究了两个粒子和多体系统中的量子参考系问题，并在伽利略相对论和绝对时间框架下，尝试通过酉变换实现标准化的、关系性的描述。

Result: 在两个粒子系统中，可以完全实现量子参考系的描述；但在多体系统中，通过酉变换无法实现标准化的、关系性的描述。

Conclusion: 在多体系统中，通过酉变换无法实现量子参考系的标准化和关系性描述，这表明需要新的方法来解决量子参考系问题。

Abstract: Physics is a model of nature able to both describe and predict the results of
measurements made with respect to reference systems. These reference systems,
in turn, are themselves physical and thus subject to the laws of physics. The
situation is no different when the model in use is quantum mechanics: states
and observables are relative entities, and reference frames are not exempt from
exhibiting quantum behavior. In recent years, the scientific community has
shown renewed interest in quantum reference frames, particularly in connection
with the covariance of physical laws and quantum resources. However, current
approaches fall short of providing a complete prescription for predicting
observables associated solely with degrees of freedom accessible from the
quantum reference frame. In pursuit of such a description, we show that while
this is fully feasible for two-particle systems, there are irreducible
difficulties that arise in many-body systems. In particular, within the
framework of Galilean relativity, with absolute time, we demonstrate that a
canonical and relational description with respect to a particle in the system
cannot be achieved through any unitary transformation. Our findings call for
new strategies to address the problem of quantum reference frames.

</details>


### [304] [Response of a classical mesoscopic oscillator to a two-level quantum system](https://arxiv.org/abs/2509.04216)
*Felipe Sobrero,Luca Abrahão,Thiago Guerreiro,Pedro V. Paraguassú*

Main category: quant-ph

TL;DR: 量子比特耦合到经典机械振子会产生独特的量子诱导信号，可以通过经典噪声光谱学进行状态重构，并可应用于量子计量学和引力量子性检验。


<details>
  <summary>Details</summary>
Motivation: 研究经典机械振子与最简单的量子系统——单比特耦合的动力学行为，并探索其应用潜力。

Method: 通过分析量子比特对经典机械振子的影响，包括确定性和随机性力，并观察其对振子响应的独特印记。

Result: 量子比特的耦合在振子的响应中产生了可测量的、与初始量子态高度相关的量子诱导特征。

Conclusion: 该研究提出了通过经典噪声光谱学进行量子态重构的可能性，并指出了其在介观光力学实验、量子计量学和引力量子性桌面检验等领域的潜在应用价值。

Abstract: We investigate the dynamics of a classical mechanical oscillator coupled to
the simplest quantum system, a single qubit. The qubit's influence manifests as
deterministic and stochastic forces, highly dependent on its initial quantum
state, imprinting unique measurable quantum-induced signatures onto the
oscillator's response. The present results suggest a pathway to quantum state
reconstruction through classical noise spectroscopy, with potential
applications to mesoscopic optomechanical experiments, quantum metrology, and
tabletop tests of the quantum nature of gravity.

</details>


### [305] [Non-unique decompositions of mixed states and deterministic energy transfers](https://arxiv.org/abs/2509.04235)
*Zihan Wang,Fei Meng,Oscar Dahlsten*

Main category: quant-ph

TL;DR: 量子混合态的非唯一分解会影响能量转移，并且可以用来扩展实现确定性能量转移的源态集，包括混合态和叠加态。


<details>
  <summary>Details</summary>
Motivation: 探讨混合态的非唯一分解对能量转移的影响，特别是无熵能量转移的可能性。

Method: 利用混合态的非唯一分解性质，推导出源态集可以通过混合和叠加进行扩展。以电磁模式与二能级系统（Jaynes-Cummings模型）作为范例进行建模。

Result: 证明了当一组相干电磁模式态能够实现确定性能量转移时，该集合可以扩展到包括这些态的混合态和叠加态。更广泛地，将非经典概率论的特征与无熵能量转移能力联系起来。

Conclusion: 非唯一分解是量子理论（以及其他非经典概率理论）的特征，这使得在能量转移过程中可以实现无熵的能量转移。所提出的扩展性原理在更一般的层面上成立。

Abstract: We investigate the impact of non-unique decompositions of mixed states on
energy transfer. Mixed states generally have non-unique decompositions into
pure states in quantum theory and, by definition, in other non-classical
probabilistic theories. We consider energy transfers constituting deterministic
energy harvesting, wherein the source transfers energy to the harvester but not
entropy. We use the possibility of non-unique decompositions to derive that if
source states in a set jointly lead to deterministic energy harvesting for the
given harvesting system and interaction, then that set can be expanded to
include both mixtures and superpositions of the original states in the set. As
a paradigmatic example, we model the source as an EM mode transferring energy
to a 2-level system harvester via the Jaynes-Cummings model. We show that the
set of coherent EM mode states with fixed $|\alpha|$ that jointly achieve
deterministic energy transfer can be expanded to include all mixtures and
superpositions of those states. More generally, the results link the defining
feature of a non-classical probability theory with the ability to achieve
energy transfer without entropy transfer.

</details>


### [306] [Qubit-optimal quantum phase estimation of block-encoded Hamiltonians](https://arxiv.org/abs/2509.04246)
*S. E. Skelton*

Main category: quant-ph

TL;DR: 提出一种使用冯诺依曼测量程序通过块编码的哈密顿量的时间演化来测量相位的算法，该算法只需要O(1)辅助量子比特，并为量子相位估计算法（QPE）提供了一种简单的方法。


<details>
  <summary>Details</summary>
Motivation: 利用量子相位估计算法（QPE）的常见 oracle 假设，提出一种更简洁的算法。

Method: 使用冯诺依曼测量程序测量相位，该程序将时间演化作为块编码哈密顿量的一个子程序。最后，推导了QPE的Clifford + T复杂度界限，并对QSP、量子特征值变换或量子奇异值变换电路的Clifford + T实现进行了通用的误差分析。

Result: 提出一种量子相位估计算法（QPE），其辅助量子比特数量级为O(1)，并且可以从 Pauli 字符串的线性组合开始高效制备。此外，还得到了QPE的Clifford + T复杂度界限，并对QSP、量子特征值变换或量子奇异值变换电路的Clifford + T实现进行了通用的误差分析。

Conclusion: 该研究提出了一种更简洁的量子相位估计算法（QPE），并分析了其在Clifford + T模型下的复杂度，为量子算法的实现和优化提供了新的见解。

Abstract: Block-encodings have become one of the most common oracle assumptions in the
circuit model. I present an algorithm that uses von Neumann's measurement
procedure to measure a phase, using time evolution on a block-encoded
Hamiltonian as a subroutine. This produces an extremely simple algorithm for
quantum phase estimation, which can be performed with a pointer system of
$\mathcal{O}(1)$ qubits.
  I then use recent results for block-encoding implementations, showing that
one can efficiently prepare QPE beginning from a linear combination of Pauli
strings. Using this, I give the Clifford + T complexity bound for QPE with
respect to model-relevant parameters of the Hamiltonian and the desired
precision. In the process, I provide a very general error analysis for Clifford
+ T implementations of QSP, quantum eigenvalue transformation, or quantum
singular value transformation circuits.

</details>


### [307] [Quantum metrology through spectral measurements in quantum optics](https://arxiv.org/abs/2509.04300)
*Alejandro Vivas-Viaña,Carlos Sánchez Muñoz*

Main category: quant-ph

TL;DR: 本研究提出了一个理论框架，用于评估光谱滤波在量子计量中的潜力，并提供了优化频率选择、探测器线宽和高阶关联的策略，为设计实用的量子光学传感策略奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 量子计量中，如何优化测量策略以从复杂的量子态中提取关于未知参数的信息是一个核心挑战。光谱滤波是量子光学中的常用方法，但其计量潜力尚未被系统量化。

Method: 将光谱检测建模为级联量子系统，用于重构频率滤波后的光子模式密度矩阵，并计算其费舍尔信息。

Result: 提出了一个评估光谱滤波在量子计量中潜力的理论框架，并计算了相应的费舍尔信息。识别了优化频率选择、探测器线宽以及通过高阶频率分辨关联和平均场工程可及的计量增益的策略。

Conclusion: 该框架为评估和设计量子光学平台中的传感策略提供了一个基础，有助于识别和设计最优的光谱滤波策略。

Abstract: Continuously monitored quantum systems are emerging as promising platforms
for quantum metrology, where a central challenge is to identify measurement
strategies that optimally extract information about unknown parameters encoded
in the complex quantum state of emitted radiation. Different measurement
strategies effectively access distinct temporal modes of the emitted field, and
the resulting choice of mode can strongly impact the information available for
parameter estimation. While a ubiquitous approach in quantum optics is to
select frequency modes through spectral filtering, the metrological potential
of this technique has not yet been systematically quantified. We develop a
theoretical framework to assess this potential by modeling spectral detection
as a cascaded quantum system, allowing us to reconstruct the full density
matrix of frequency-filtered photonic modes and to compute their associated
Fisher information. This framework provides a minimal yet general method to
benchmark the performance of spectral measurements in quantum optics, allowing
to identify optimal filtering strategies in terms of frequency selection,
detector linewidth, and metrological gain accessible through higher-order
frequency-resolved correlations and mean-field engineering. These results lay
the groundwork for identifying and designing optimal sensing strategies in
practical quantum-optical platforms.

</details>


### [308] [Foundations of photonic quantum computation](https://arxiv.org/abs/2509.04266)
*Martin Bombardelli,Gérard Fleury,Philippe Lacomme,Bogdan Vulpescu*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This work aims to introduce the fundamental concepts required to perform
computations on photonic quantum computers by presenting the gates specific to
this architecture and highlighting the connections between standard Pauli gates
and those available in photonic systems. The introduction navigates between
physical considerations related to the optical components used, theoretical
aspects concerning quantum operators, and a more applied section introducing
implementations using the Perceval library developed by Quandela. This paper is
intended for engineers and researchers familiar with Pauli gates and standard
quantum concepts, looking at a clear and compact introduction to photonic
components. A second part aims to introduce the concept of polarization, not
from a theoretical perspective, but through its practical applications. To do
so, we compare the similarities and differences between the original Grover's
algorithm formulation and a version that leverages polarization. Gates specific
to polarization are introduced and described in the context of the computations
involved in Grover's algorithm. The description provided is as mathematical as
possible and deliberately avoids physical considerations, in order to allow
researchers familiar with "conventional" quantum circuits to more easily grasp
the concepts.

</details>


### [309] [Coherent Two-State Oscillations in False Vacuum Decay Regimes](https://arxiv.org/abs/2509.04272)
*Peiyun Ge,Xiao Wang,Yu-Xin Chao,Rong Lv,Li You*

Main category: quant-ph

TL;DR: 在量子伊辛模型中，我们观察到从假真空态开始的相干振荡，这与传统的衰变动力学不同。这种振荡在特定共振条件下出现，并表现出随系统尺寸增强的频率。即使在更大的系统中，这种相干性机制也能够持续存在，并克服了传统理论和有限尺寸的限制。


<details>
  <summary>Details</summary>
Motivation: 在量子伊辛模型中，特别是在假真空衰变区域，研究相干多体动力学，探索其与传统衰变动力学的区别以及可能存在的鲁棒性机制。

Method: 通过数值模拟研究一维横向场和纵向场伊辛模型。在假真空衰变区域，从全自旋向上的假真空态开始，在特定共振条件下（$ h 	ext{ ≈ } 2J/n$），观察并分析了系统在假真空态和对称共振态之间的相干双态振荡现象。重点关注了子领体本征态的重叠度（接近 0.5）和洛施密特回声的周期性消失，以及振荡频率随系统尺寸（$L$）的变化（与 Schrieffer-Wolff 预测的比较）。此外，还研究了在更大系统中，通过气泡尺寸阻塞效应（$n 	ext{ gtrsim } L/2$）或长程相互作用提升多气泡简并性时，振荡的持续性。

Result: 在数值模拟中，发现在一维横向场和纵向场伊辛模型中，当系统处于假真空衰变区域，并在特定共振条件下（$h 	ext{ ≈ } 2J/n$），系统会表现出在假真空态和对称共振态之间的相干双态振荡。这种振荡的特征是子领体本征态的重叠度接近 0.5，并且洛施密特回声会周期性地消失。振荡频率显示出与系统尺寸 $L$ 相关的超辐射（superradiant-like）增强（$	ext{√}L$），这与早期的 Schrieffer-Wolff 预测不同。在更大的系统中，通过气泡尺寸阻塞效应（$n 	ext{ gtrsim } L/2$）或长程相互作用克服多气泡简并性，观察到这些振荡能够持续存在。

Conclusion: 相干双态振荡是一种鲁棒的多体相干机制，它存在于量子伊辛模型（特别是假真空衰变区域）中，并且超越了微扰理论和有限尺寸的限制。这种现象在不同系统尺寸下都可能出现，表明了其重要的普适性。

Abstract: Coherent two-state oscillations are observed in numerical simulations of
one-dimensional transverse- and longitudinal-field Ising model within the false
vacuum decay regimes. Starting from the false vacuum state with all spins up,
in moderate-sized systems with a small transverse field, we find conventional
decay dynamics at resonance conditions $ h\approx 2J/n$ can change into
coherent oscillations between the false vacuum and a symmetric resonant state,
manifested by a sub-leading eigenstate overlap approaching $0.5$ and periodic
vanishing of the Loschmidt echo. Notably, the oscillation frequency shows a
superradiant-like $\sqrt{L}$ enhancement compared to the earlier
Schrieffer-Wolff predictions. In larger systems, we find these oscillations
persist for $n \gtrsim L/2$ (enabled by a bubble size blockade effect) or when
long-range interactions lift multi-bubble degeneracies, revealing a robust
many-body coherence mechanism transcending perturbative treatments and
finite-size limitations.

</details>


### [310] [Unifying inequalities bounding quantum speed limits](https://arxiv.org/abs/2509.04283)
*Tristán M. Osán,Yanet Álvarez,Mariela Portesi,Pedro Walter Lamberti*

Main category: quant-ph

TL;DR: 研究了广义保真度在量子速度极限（QSL）推导中的作用，并提出了一个通用的理论框架，证明了QSL界限仅取决于所选的保真度度量，而与功能变换无关。


<details>
  <summary>Details</summary>
Motivation: 在量子信息科学中，量子速度极限（QSL）被认为是量子信息处理的基本限制。然而，现有的QSL推导方法通常依赖于特定的保真度度量，这限制了其普适性。因此，研究广义保真度在QSL推导中的作用，并建立一个通用的理论框架，对于理解和改进QSL具有重要意义。

Method: 本文采用几何方法，研究了广义保真度在量子速度极限（QSL）推导中的作用。首先，建立了通用的理论框架，并推导了量子速度极限的Margolus-Levitin和Mandelstam-Tamm界限。然后，分析了不同广义保真度度量对这些界限的影响。最后，讨论了该框架的普适性和局限性。

Result: 所提出的通用理论框架表明，量子速度极限的Margolus-Levitin和Mandelstam-Tamm界限，在酉和非酉（Lindblad类型）动力学中，仅取决于所选的广义保真度度量，而与保真度度量的功能变换无关。此外，该框架还统一了文献中已有的多个QSL界限。

Conclusion: 本研究提出的基于广义保真度的量子速度极限（QSL）理论框架，揭示了QSL界限的普适性，并指出了改进QSL的潜在方向。研究结果表明，真实的改进来自于新的保真度定义，而非对现有保真度进行功能变换。这对于未来量子信息处理和量子计算的发展具有重要的理论指导意义。

Abstract: In this work, we investigate the role of generalized fidelity measures in the
derivation of quantum speed limits (QSLs) within a geometric approach. We
establish a general theoretical structure and demonstrate that, once a specific
generalized fidelity is selected, the resulting Margolus-Levitin and
Mandelstam-Tamm bounds for both unitary and non-unitary (Lindblad-type)
dynamics depend solely on the chosen fidelity measure. These bounds are shown
to be entirely independent of the particular choice of monotonic and
differentiable functionals of the selected fidelity. This result highlights the
limitations of improving QSL bounds through functional transformations of
fidelity and indicates that genuine improvements must stem from alternative
fidelity definitions. We further show that several bounds found in the
literature are encompassed by our framework. We also discuss how our results
can be naturally extended by considering other generalized fidelity measures
beyond those explicitly treated

</details>


### [311] [A Generalized Nonlinear Extension of Quantum Mechanics](https://arxiv.org/abs/2509.04320)
*Alan Chodos,Fred Cooper*

Main category: quant-ph

TL;DR: 我们提出了一个非完全可积但具有哈密顿结构的新型非线性量子力学扩展模型。


<details>
  <summary>Details</summary>
Motivation: 在先前工作中提出的非线性量子力学扩展模型的基础上，构建其最一般的形式，并探讨其基本性质。

Method: 分析了一个具体的解，并使用玻恩规则的自然扩展来计算粒子轨迹。

Result: 研究发现，该模型允许存在闭合的粒子轨道。

Conclusion: 我们成功地构建了一个更通用的非线性量子力学扩展模型，虽然它不像先前模型那样完全可积，但仍保留了哈密顿结构，并能在一定条件下预测闭合的粒子轨道。

Abstract: We construct the most general form of our previously proposed nonlinear
extension of quantum mechanics that possesses three basic properties. Unlike
the simpler model, the new version is not completely integrable, but it has an
underlying Hamiltonian structure. We analyze a particular solution in detail,
and we use a natural extension of the Born rule to compute particle
trajectories. We find that closed particle orbits are possible.

</details>


### [312] [DGLAP-BFKL duality from QCD to quantum computers](https://arxiv.org/abs/2509.04327)
*Igor Kondrashuk*

Main category: quant-ph

TL;DR: DGLAP and BFKL equations can be solved using Mellin moment complex plane mapping, leading to potential applications in quantum communication.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the DGLAP and BFKL integro-differential equations, which are relevant in quantum field theory and quantum communication, by reformulating them using complex mapping in the Mellin moment plane.

Method: The DGLAP equation is mapped to a Schrödinger equation and a dual DGLAP equation using complex mapping in the Mellin moment plane. The dual DGLAP equation's Regge limit coincides with the BFKL equation, which is also a Schrödinger equation and solvable by the same mapping method.

Result: The proposed method of complex mapping in the Mellin moment plane can be applied to solve both the DGLAP and BFKL equations, as well as their corresponding Schrödinger equation forms.

Conclusion: The developed method of complex mapping in the Mellin moment plane provides a way to solve the DGLAP and BFKL equations, suggesting potential usefulness for tasks in quantum communication.

Abstract: DGLAP integro-differential equation can be solved by applying certain map in
the complex plane of Mellin moments. It may be re-written as Schr\"odinger
equation for $n$ particles. By applying another map in the complex plane of
Mellin moment we may re-write the DGLAP equation as a dual DGLAP equation.
Regge limit of the dual DGLAP equation coincides with BFKL integro-differential
equation which in turn is the Regge limit of optic theorem in quantum field
theory and may be re-written as another Schr\"odinger equation. This means that
the BFKL equation and the corresponding Schr\"odinger equation may be solved by
the proposed method of complex mapping in the complex plane of Mellin moments.
This approach may be useful in solving tasks related to quantum communication
processes.

</details>


### [313] [Constructing a Photonic Implementation of Quantum Key Distribution](https://arxiv.org/abs/2509.04389)
*Alec L. Riso,Karthik Thyagarajan,Connor Whiting,Katherine Jimenez,Mark Hannum*

Main category: quant-ph

TL;DR: 本项目旨在通过光子学在实验室环境中实现量子密钥分发（QKD），以演示其鲁棒性并为教学演示提供可行的方案。


<details>
  <summary>Details</summary>
Motivation: 本项目旨在通过光子学在实验室环境中实现量子密钥分发（QKD），以演示其鲁棒性并为教学演示提供可行的方案。量子密钥分发（QKD）利用量子力学原理建立不可破解的通信渠道，提供信息论安全保障，其安全性由物理定律保证，理论上即使拥有无限计算能力的对手也无法破解。相较于依赖数学难题计算难度的传统密码学，QKD 具有显著的优势。

Method: 本项目旨在通过光子学在实验室环境中实现量子密钥分发（QKD）。

Result: 本项目旨在通过光子学在实验室环境中实现量子密钥分发（QKD），以演示其鲁棒性。

Conclusion: 本项目旨在通过光子学在实验室环境中实现量子密钥分发（QKD），以演示其鲁棒性并为教学演示提供可行的方案。

Abstract: Quantum Key Distribution (QKD) stands as a revolutionary approach to secure
communication, using the principles of quantum mechanics to establish
unbreakable channels. Unlike traditional cryptography, which relies on the
computational difficulty of mathematical problems, QKD utilizes the inherent
properties of quantum states to achieve information-theoretic security. This
means that the security of the key exchange is guaranteed by the laws of
physics, making it theoretically unbreakable even by an adversary with
unlimited computational power. Currently, one of the most viable ways to
implement QKD for communication is via photonics, namely, using
phase-preserving long-distance optical fibers. The objective of this project is
to implement photonic QKD in a laboratory setting. This will help demonstrate
the protocol's robustness and provide a feasible implementation for educational
demonstrations.

</details>


### [314] [Unilateral Criticality and Phase Transition in the Cavity-Ising Model](https://arxiv.org/abs/2509.04391)
*Zeyu Rao,Xiaoshui Lin,Xiwang Luo,Guangcan Guo,Han Pu,Ming Gong*

Main category: quant-ph

TL;DR: 作者报告了一个在腔耦合横向伊辛模型中发现的单边临界终点（UCEP）和三临界点（TCP），UCEP是一种新的相变类型，其特征是从一个方向接近时表现为二阶相变，而从另一个方向接近时表现为一阶相变。


<details>
  <summary>Details</summary>
Motivation: 探索腔耦合横向伊辛模型中是否存在新的相变现象，特别是单边临界终点（UCEP）和三临界点（TCP）。

Method: 通过理论分析，构建了描述UCEP的自由能密度函数，并结合密度矩阵重正化群等方法（虽未在摘要中详述，但通常是此类研究的手段）进行了有限温度下的相图绘制和对称性分析。

Result: 在零温下，发现了三个相，通过两个二阶和一个一阶相变分离，这些相变线在TCP和UCEP处相交。UCEP表现出单边临界性，一个序参数在此点经历一阶相变，而另一个经历二阶相变。给出了UCEP的最小自由能描述。绘制了有限温度下的相图。

Conclusion: UCEP是一种新颖的相变类型，它将一阶和二阶相变特征统一在单一的、依赖于方向的终点，可能在量子测量和量子传感等领域有应用前景。该模型为研究腔耦合多体系统中的新颖临界现象提供了平台。

Abstract: Superradiant phase transitions from cavity light-matter coupling have been
widely explored across platforms. Here, we report a unilateral critical
endpoint (UCEP) and a tricritical point (TCP) in the phase diagram of the
cavity-coupled transverse Ising model with $\mathbb{Z}_2$ symmetry. At zero
temperature, we demonstrate that this model hosts three phases separated by two
second-order and one first-order transitions. These lines intersect at a TCP
and a UCEP, the latter not captured by existing phase-transition paradigms. The
UCEP displays one-sided criticality: approaching the point from one side, the
system behaves as a second-order transition, while from the other side it is
first-order. Correspondingly, two order parameters, respectively, undergo the
first- and the second-order phase transitions at the same point. We construct a
minimal description of UCEP with the density of the free energy $f =
c_{1}(\tilde{\alpha}^{2}+c_{2})+(\tilde{\alpha}^{2}+c_{2})^{2}\ln{\vert\tilde{\alpha}^{2}+c_{2}\vert}$,
with the UCEP at $(c_{1},c_{2})=(1/e,0)$ and $\tilde{\alpha}$ being the order
parameter. We further map the finite-temperature phase diagram and perform a
symmetry analysis. By unifying first- and second-order signatures in a single,
direction-dependent endpoint, the UCEP introduces a qualitatively new class of
phase transition and may have applications in fields such as quantum
measurement and quantum sensing. This work also provides an intriguing platform
for exploring novel critical phenomena in cavity-coupled many-body systems with
or without dissipation.

</details>


### [315] [Monte Carlo simulation of random circuit sampling in quantum computing](https://arxiv.org/abs/2509.04401)
*Andreas Raab*

Main category: quant-ph

TL;DR: We developed Monte Carlo methods for sampling random quantum states, achieving efficiency on classical computers even for systems with over a million qubits.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop efficient methods for sampling random states and bit strings in qubit systems, which is crucial for simulating quantum computations.

Method: The paper derives exact probability density functions that lead to the Porter-Thomas distribution for large systems and applies these in importance sampling algorithms. These methods are then used to simulate quantum computations on classical computers.

Result: The methods demonstrate efficiency for qubit systems of various sizes (70, 105, 1000, and over a million qubits), enabling the simulation of noise-free quantum computations on a PC with minimal computational cost.

Conclusion: Random circuit sampling can be conveniently performed on classical computers using the developed Monte Carlo methods.

Abstract: We develop Monte Carlo methods for sampling random states and corresponding
bit strings in qubit systems. To this end, we derive exact probability density
functions that yield the Porter-Thomas distribution in the limit of large
systems. We apply these functions in importance sampling algorithms and
demonstrate efficiency for qubit systems with 70, 105, 1000, and more than one
million ($2^{20}$) qubits. In particular, we simulate the output of recent
quantum computations without noise on a PC with minimal computational cost. I
would therefore argue that random circuit sampling can be conveniently
performed on classical computers.

</details>


### [316] [Infinite temperature at zero energy](https://arxiv.org/abs/2509.04410)
*Matteo Ippoliti,David M. Long*

Main category: quant-ph

TL;DR: 我们构建了一类静态、几何局域的哈密顿量，它们继承了周期驱动（Floquet）系统的本征态性质。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是构建一类具有本征态特性的局域哈密顿量，并证明其接地态具有体积定律纠缠熵。

Method: 通过修改Feynman-Kitaev时钟模型，给予时钟寄存器周期性边界条件，并结合可解的Floquet量子电路，构建了具有特定性质的局域哈密顿量。

Result: 证明了所构建哈密顿量的本征态具有无限温度下的特性，包括整个能谱（包含接地态）的体积定律纠缠熵。此外，还首次实现了体积定律对所有连续子系统的普遍适用性。

Conclusion: 成功构建了一类具有可证明的体积定律纠缠接地态的新型局域哈密顿量，并首次实现体积定律对所有连续子系统的普遍适用性。

Abstract: We construct a family of static, geometrically local Hamiltonians that
inherit eigenstate properties of periodically-driven (Floquet) systems. Our
construction is a variation of the Feynman-Kitaev clock -- a well-known mapping
between quantum circuits and local Hamiltonians -- where the clock register is
given periodic boundary conditions. Assuming the eigenstate thermalization
hypothesis (ETH) holds for the input circuit, our construction yields
Hamiltonians whose eigenstates have properties characteristic of infinite
temperature, like volume-law entanglement entropy, across the whole spectrum --
including the ground state. We then construct a family of exactly solvable
Floquet quantum circuits whose eigenstates are shown to obey the ETH at
infinite temperature. Combining the two constructions yields a new family of
local Hamiltonians with provably volume-law-entangled ground states, and the
first such construction where the volume law holds for all contiguous
subsystems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [317] [Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation : A Table-Mechanics-Inspired Approach](https://arxiv.org/abs/2509.03563)
*Quan Quan,Jiwen Xu,Runxiao Liu,Yi Ding,Jiaxing Che,Kai-Yuan Cai*

Main category: cs.RO

TL;DR: 提出的方法通过模仿桌腿的受力分配机制，利用去中心化的耗散力模型，实现了飞行机器人群的自主协作运输，无需显式通信，具有良好的可扩展性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有空中运输方法在可扩展性、通信依赖性和动态故障鲁棒性方面存在挑战，而机器人群协作运输具有变革潜力，可用于物流和灾难响应。

Method: 提出一种受物理启发的协作运输方法，模仿桌腿的耗散力学原理。通过开发去中心化的耗散力模型，使每个机器人能够根据局部邻居和负载动态调整其位置，实现自主的编队稳定和自适应负载分配，而无需显式通信。

Result: 仿真结果显示，在能力变化、电缆不确定性、视野受限和负载变化等情况下，所提出方法的跟踪误差分别是现有方法的20%、68%、55.5%和21.9%。在真实世界实验中，系统在单机器人故障、断开连接、负载变化25%和电缆长度不确定性40%的情况下成功率达到94%，在室外4级 Beaufort 风力下表现出强大的鲁棒性。

Conclusion: 该物理启发的方法结合了群体智能和机械稳定性原理，为异构空中系统在通信受限环境中集体处理复杂运输任务提供了一个可扩展的框架。

Abstract: In comparison with existing approaches, which struggle with scalability,
communication dependency, and robustness against dynamic failures, cooperative
aerial transportation via robot swarms holds transformative potential for
logistics and disaster response. Here, we present a physics-inspired
cooperative transportation approach for flying robot swarms that imitates the
dissipative mechanics of table-leg load distribution. By developing a
decentralized dissipative force model, our approach enables autonomous
formation stabilization and adaptive load allocation without the requirement of
explicit communication. Based on local neighbor robots and the suspended
payload, each robot dynamically adjusts its position. This is similar to
energy-dissipating table leg reactions. The stability of the resultant control
system is rigorously proved. Simulations demonstrate that the tracking errors
of the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches
under the cases of capability variation, cable uncertainty, limited vision, and
payload variation, respectively. In real-world experiments with six flying
robots, the cooperative aerial transportation system achieved a 94% success
rate under single-robot failure, disconnection events, 25% payload variation,
and 40% cable length uncertainty, demonstrating strong robustness under outdoor
winds up to Beaufort scale 4. Overall, this physics-inspired approach bridges
swarm intelligence and mechanical stability principles, offering a scalable
framework for heterogeneous aerial systems to collectively handle complex
transportation tasks in communication-constrained environments.

</details>


### [318] [Cooperative Grasping for Collective Object Transport in Constrained Environments](https://arxiv.org/abs/2509.03638)
*David Alvear,George Turkiyyah,Shinkyu Park*

Main category: cs.RO

TL;DR: 提出了一种新颖的框架，用于在约束环境中进行双机器人协同抓取以进行物体运输。


<details>
  <summary>Details</summary>
Motivation: 旨在解决双机器人协同抓取和物体运输在约束环境中进行决策的问题。

Method: 提出了一种条件嵌入（CE）模型，该模型包含两个神经网络，将抓取配置信息映射到嵌入空间，并利用负采样进行监督学习，以区分可行和不可行的抓取配置。

Result: 在模拟环境中，该模型能够可靠地识别出可行的抓取配置，并在物理机器人平台上进行了验证。

Conclusion: 该框架能够可靠地识别出可行的抓取配置，并具有实际应用价值。

Abstract: We propose a novel framework for decision-making in cooperative grasping for
two-robot object transport in constrained environments. The core of the
framework is a Conditional Embedding (CE) model consisting of two neural
networks that map grasp configuration information into an embedding space. The
resulting embedding vectors are then used to identify feasible grasp
configurations that allow two robots to collaboratively transport an object. To
ensure generalizability across diverse environments and object geometries, the
neural networks are trained on a dataset comprising a range of environment maps
and object shapes. We employ a supervised learning approach with negative
sampling to ensure that the learned embeddings effectively distinguish between
feasible and infeasible grasp configurations. Evaluation results across a wide
range of environments and objects in simulations demonstrate the model's
ability to reliably identify feasible grasp configurations. We further validate
the framework through experiments on a physical robotic platform, confirming
its practical applicability.

</details>


### [319] [Efficient Virtuoso: A Latent Diffusion Transformer Model for Goal-Conditioned Trajectory Planning](https://arxiv.org/abs/2509.03658)
*Antonio Guillen-Perez*

Main category: cs.RO

TL;DR: Efficient Virtuoso是一个用于目标条件轨迹规划的条件潜在扩散模型，它通过新颖的两阶段归一化流程和基于MLP的去噪器，在Waymo开放运动数据集上实现了最先进的性能，最小ADE值为0.25。研究还表明，多步稀疏路线比单一终点目标更能实现高保真度的战术执行。


<details>
  <summary>Details</summary>
Motivation: 为自动驾驶规划系统生成多样化且合理的未来轨迹分布是一个关键能力，但现有生成模型在高保真度、计算效率和精确控制方面仍面临挑战。

Method: 提出了一种名为Efficient Virtuoso的条件潜在扩散模型，该模型采用新颖的两阶段归一化流程来扩展轨迹并稳定PCA潜在空间，然后使用MLP去噪器在低维潜在空间中进行高效去噪，并通过基于Transformer的状态编码器融合丰富的场景上下文。

Result: 在Waymo开放运动数据集上实现了最先进的性能，最小ADE值为0.25。

Conclusion: 单一终点目标可以解决战略模糊性，而更丰富的多步稀疏路线对于实现反映细微人类驾驶行为的高保真度战术执行至关重要。

Abstract: The ability to generate a diverse and plausible distribution of future
trajectories is a critical capability for autonomous vehicle planning systems.
While recent generative models have shown promise, achieving high fidelity,
computational efficiency, and precise control remains a significant challenge.
In this paper, we present the \textbf{Efficient Virtuoso}, a conditional latent
diffusion model for goal-conditioned trajectory planning. Our approach
introduces a novel two-stage normalization pipeline that first scales
trajectories to preserve their geometric aspect ratio and then normalizes the
resulting PCA latent space to ensure a stable training target. The denoising
process is performed efficiently in this low-dimensional latent space by a
simple MLP denoiser, which is conditioned on a rich scene context fused by a
powerful Transformer-based StateEncoder. We demonstrate that our method
achieves state-of-the-art performance on the Waymo Open Motion Dataset,
reaching a \textbf{minADE of 0.25}. Furthermore, through a rigorous ablation
study on goal representation, we provide a key insight: while a single endpoint
goal can resolve strategic ambiguity, a richer, multi-step sparse route is
essential for enabling the precise, high-fidelity tactical execution that
mirrors nuanced human driving behavior.

</details>


### [320] [Low-Cost Open-Source Ambidextrous Robotic Hand with 23 Direct-Drive servos for American Sign Language Alphabet](https://arxiv.org/abs/2509.03690)
*Kelvin Daniel Gonzalez Amador*

Main category: cs.RO

TL;DR: VulcanV3是一款低成本、开源的3D打印机器人手，能够复现全部美式手语（ASL）字母，并实现了96.97%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 为聋哑社群提供可负担且功能全面的手语交流机器人解决方案。

Method: 使用23个伺服电机驱动，通过Arduino Mega和PCA9685模块控制，实现手指和手腕的精确运动，并采用可双向使用的设计。

Result: 实现了全部52个美式手语字母（左右手配置）的精确复现，用户识别准确率达到96.97%，视频演示后提升至98.78%。

Conclusion: VulcanV3通过结合经济性、全面的手语覆盖能力和左右手通用性，在一个开放共享的平台上推动了辅助机器人技术的发展，为无障碍沟通和包容性创新做出了贡献。

Abstract: Accessible communication through sign language is vital for deaf communities,
1 yet robotic solutions are often costly and limited. This study presents
VulcanV3, a low- 2 cost, open-source, 3D-printed ambidextrous robotic hand
capable of reproducing the full 3 American Sign Language (ASL) alphabet (52
signs for right- and left-hand configurations). 4 The system employs 23
direct-drive servo actuators for precise finger and wrist movements, 5
controlled by an Arduino Mega with dual PCA9685 modules. Unlike most humanoid
upper- 6 limb systems, which rarely employ direct-drive actuation, VulcanV3
achieves complete ASL 7 coverage with a reversible design. All CAD files and
code are released under permissive 8 open-source licenses to enable
replication. Empirical tests confirmed accurate reproduction 9 of all 52 ASL
handshapes, while a participant study (n = 33) achieved 96.97% recognition 10
accuracy, improving to 98.78% after video demonstration. VulcanV3 advances
assistive 11 robotics by combining affordability, full ASL coverage, and
ambidexterity in an openly 12 shared platform, contributing to accessible
communication technologies and inclusive 13 innovation.

</details>


### [321] [Real-Time Buoyancy Estimation for AUV Simulations Using Convex Hull-Based Submerged Volume Calculation](https://arxiv.org/abs/2509.03804)
*Ad-Deen Mahbub,Md Ragib Shaharear*

Main category: cs.RO

TL;DR: 该研究提出了一种新的基于凸包的水下航行器（AUV）实时浮力模拟方法，以弥补NVIDIA Isaac Sim在浮力模拟方面的不足。


<details>
  <summary>Details</summary>
Motivation: NVIDIA Isaac Sim缺乏原生的浮力系统，需要外部解决方案来精确模拟水下物理特性，而精确的实时浮力模型对于高保真度的AUV模拟至关重要。

Method: 该方法提取模拟环境的网格几何，并计算凸包与水线在z轴上的交集来动态计算AUV的浸入体积。通过使用近似的横截面积来减少计算开销，实现了对方向、深度和正弦波（+-0.3米）变化的适应性，从而实现高效的浮力更新。

Result: 该方法在为SAUVC 2025设计的定制AUV上进行了测试，实现了实时性能和可扩展性，提高了水下机器人研究的模拟保真度，且无需预计算的流体动力学模型。

Conclusion: 该研究提出的基于凸包的实时浮力模拟方法能够提高AUV模拟的精确度，并且具有良好的实时性能和可扩展性，为水下机器人研究提供了有力的支持。

Abstract: Accurate real-time buoyancy modeling is essential for high-fidelity
Autonomous Underwater Vehicle (AUV) simulations, yet NVIDIA Isaac Sim lacks a
native buoyancy system, requiring external solutions for precise underwater
physics. This paper presents a novel convex hull-based approach to dynamically
compute the submerged volume of an AUV in real time. By extracting mesh
geometry from the simulation environment and calculating the hull portion
intersecting the water level along the z-axis, our method enhances accuracy
over traditional geometric approximations. A cross-sectional area extension
reduces computational overhead, enabling efficient buoyant force updates that
adapt to orientation, depth, and sinusoidal wave fluctuations (+-0.3 m). Tested
on a custom AUV design for SAUVC 2025, this approach delivers real-time
performance and scalability, improving simulation fidelity for underwater
robotics research without precomputed hydrodynamic models.

</details>


### [322] [INGRID: Intelligent Generative Robotic Design Using Large Language Models](https://arxiv.org/abs/2509.03842)
*Guanglu Jia,Ceng Zhang,Gregory S. Chirikjian*

Main category: cs.RO

TL;DR: LLM驱动的机器人设计框架INGRID，通过结合螺旋理论和运动学综合，实现新型并联机器人设计，不受限于传统串联机构，推动了“机构智能”的发展。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的机器人系统受限于串联机构的硬件约束，限制了机器人智能的发展范围。

Method: 提出INGRID框架，将设计任务分解为约束分析、运动学关节生成、链构建和完整机构设计四个步骤，并与螺旋理论和运动学综合方法深度集成，实现并联机器人机构的自动化设计。

Result: INGRID能够生成具有固定和可变运动能力的多种新型并联机构，发现了文献中未记载的运动学配置。通过三个案例研究验证了该方法能够根据期望的运动能力要求设计特定任务的并联机器人。

Conclusion: INGRID将机构理论与机器学习相结合，使非专业机器人领域的研究人员也能设计定制化的并联机构，从而将机器人智能的进步与硬件限制脱钩，为“机构智能”奠定了基础，有望变革具身智能系统的发展。

Abstract: The integration of large language models (LLMs) into robotic systems has
accelerated progress in embodied artificial intelligence, yet current
approaches remain constrained by existing robotic architectures, particularly
serial mechanisms. This hardware dependency fundamentally limits the scope of
robotic intelligence. Here, we present INGRID (Intelligent Generative Robotic
Design), a framework that enables the automated design of parallel robotic
mechanisms through deep integration with reciprocal screw theory and kinematic
synthesis methods. We decompose the design challenge into four progressive
tasks: constraint analysis, kinematic joint generation, chain construction, and
complete mechanism design. INGRID demonstrates the ability to generate novel
parallel mechanisms with both fixed and variable mobility, discovering
kinematic configurations not previously documented in the literature. We
validate our approach through three case studies demonstrating how INGRID
assists users in designing task-specific parallel robots based on desired
mobility requirements. By bridging the gap between mechanism theory and machine
learning, INGRID enables researchers without specialized robotics training to
create custom parallel mechanisms, thereby decoupling advances in robotic
intelligence from hardware constraints. This work establishes a foundation for
mechanism intelligence, where AI systems actively design robotic hardware,
potentially transforming the development of embodied AI systems.

</details>


### [323] [Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator](https://arxiv.org/abs/2509.03859)
*Haichao Zhang,Haonan Yu,Le Zhao,Andrew Choi,Qinxun Bai,Yiqing Yang,Wei Xu*

Main category: cs.RO

TL;DR: 我们提出了一种在模拟中完全训练的视觉-运动策略，用于四足机器人移动操作，并在现实世界中实现了近 80% 的成功率。


<details>
  <summary>Details</summary>
Motivation: 四足移动操作在机器人领域带来了严峻的挑战，因为它需要多样化的技能、延长的任务周期和部分可观察性。

Method: 提出了一种多阶段抓取和放置任务，作为一种简洁而足够丰富的设置，以捕捉四足移动操作的关键要求。该方法训练了一个完全在模拟中实现的视觉-运动策略。

Result: 该策略能够有效地执行搜索、接近、抓取、运输和放置等动作，并涌现出诸如重新抓取和任务链接等行为，在现实世界中实现了近 80% 的成功率。

Conclusion: 该方法通过大量的现实世界实验和消融研究，证明了其在高效训练和有效模拟到现实迁移方面的有效性，并展示了在各种室内外环境中的部署能力。

Abstract: Quadruped-based mobile manipulation presents significant challenges in
robotics due to the diversity of required skills, the extended task horizon,
and partial observability. After presenting a multi-stage pick-and-place task
as a succinct yet sufficiently rich setup that captures key desiderata for
quadruped-based mobile manipulation, we propose an approach that can train a
visuo-motor policy entirely in simulation, and achieve nearly 80\% success in
the real world. The policy efficiently performs search, approach, grasp,
transport, and drop into actions, with emerged behaviors such as re-grasping
and task chaining. We conduct an extensive set of real-world experiments with
ablation studies highlighting key techniques for efficient training and
effective sim-to-real transfer. Additional experiments demonstrate deployment
across a variety of indoor and outdoor environments. Demo videos and additional
resources are available on the project page:
https://horizonrobotics.github.io/gail/SLIM.

</details>


### [324] [Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance](https://arxiv.org/abs/2509.03889)
*Neha Sunil,Megha Tippur,Arnau Saumell,Edward Adelson,Alberto Rodriguez*

Main category: cs.RO

TL;DR: 该研究提出了一个结合了视觉和触觉的双臂机器人框架，用于处理褶皱和悬挂状态下的服装操作。


<details>
  <summary>Details</summary>
Motivation: 服装操作因其复杂的配置、动态的材料特性以及频繁的自我遮挡而具有挑战性。现有系统通常需要将衣物展平或假设关键特征可见。

Method: 该框架结合了置信度感知的稠密视觉对应关系和触觉监督的抓取先验。视觉对应模型在一个自定义的高保真模拟数据集上进行训练，并使用能捕捉衣物对称性和生成对应关系置信度估计的分布损失。这些估计引导一个响应式状态机，根据感知不确定性调整折叠策略。同时，一个视觉-触觉抓取先验网络利用高分辨率的触觉反馈进行自监督学习，以确定哪些区域可以物理抓取。在执行过程中，相同的触觉分类器用于实时抓取验证。

Result: 通过在低置信度状态下推迟操作，该系统能够处理高度遮挡的桌面和空中配置。研究演示了其任务无关的抓取选择模块在折叠和悬挂任务中的应用。此外，稠密描述符为其他规划模式（如从人类视频演示中提取抓取目标）提供了一个可重用的中间表示。

Conclusion: 该框架通过结合视觉和触觉信息，并利用置信度感知和自监督学习，有效地解决了服装操作中的挑战，并为更通用、可扩展的服装操作奠定了基础。

Abstract: Manipulating clothing is challenging due to complex configurations, variable
material dynamics, and frequent self-occlusion. Prior systems often flatten
garments or assume visibility of key features. We present a dual-arm
visuotactile framework that combines confidence-aware dense visual
correspondence and tactile-supervised grasp affordance to operate directly on
crumpled and suspended garments. The correspondence model is trained on a
custom, high-fidelity simulated dataset using a distributional loss that
captures cloth symmetries and generates correspondence confidence estimates.
These estimates guide a reactive state machine that adapts folding strategies
based on perceptual uncertainty. In parallel, a visuotactile grasp affordance
network, self-supervised using high-resolution tactile feedback, determines
which regions are physically graspable. The same tactile classifier is used
during execution for real-time grasp validation. By deferring action in
low-confidence states, the system handles highly occluded table-top and in-air
configurations. We demonstrate our task-agnostic grasp selection module in
folding and hanging tasks. Moreover, our dense descriptors provide a reusable
intermediate representation for other planning modalities, such as extracting
grasp targets from human video demonstrations, paving the way for more
generalizable and scalable garment manipulation.

</details>


### [325] [Odometry Calibration and Pose Estimation of a 4WIS4WID Mobile Wall Climbing Robot](https://arxiv.org/abs/2509.04016)
*Branimir Ćaran,Vladimir Milić,Marko Švaco,Bojan Jerbić*

Main category: cs.RO

TL;DR: 该论文设计了一种用于四轮独立转向四轮独立驱动（4WIS4WID）爬壁移动机器人的姿态估计器，该估计器融合了轮式里程计、视觉里程计和惯性测量单元（IMU）数据，并结合了扩展卡尔曼滤波（EKF）和无迹卡尔曼滤波（UKF）。


<details>
  <summary>Details</summary>
Motivation: 爬壁机器人的操作环境需要在建筑物上进行精确的测量和维护，因此姿态信息至关重要。传统的激光、超声波或雷达等传感器在复杂建筑外墙上不可行，GPS信号也因屏蔽和干扰而不可靠，因此机器人里程计是主要的定位信息来源，但易产生漂移。

Method: 采用扩展卡尔曼滤波（EKF）和无迹卡尔曼滤波（UKF）融合轮式里程计、视觉里程计和IMU数据来设计姿态估计器。利用非线性优化、Levenberg-Marquardt算法、遗传算法和粒子群算法对机器人进行运动学参数标定。

Result: 通过在实验爬壁机器人上进行实验，详细验证了标定方法和姿态估计器的性能和结果。

Conclusion: 所设计的姿态估计器能够有效融合多模态测量数据，为爬壁机器人提供精确的姿态信息。

Abstract: This paper presents the design of a pose estimator for a four wheel
independent steer four wheel independent drive (4WIS4WID) wall climbing mobile
robot, based on the fusion of multimodal measurements, including wheel
odometry, visual odometry, and an inertial measurement unit (IMU) data using
Extended Kalman Filter (EKF) and Unscented Kalman Filter (UKF). The pose
estimator is a critical component of wall climbing mobile robots, as their
operational environment involves carrying precise measurement equipment and
maintenance tools in construction, requiring information about pose on the
building at the time of measurement. Due to the complex geometry and material
properties of building facades, the use of traditional localization sensors
such as laser, ultrasonic, or radar is often infeasible for wall-climbing
robots. Moreover, GPS-based localization is generally unreliable in these
environments because of signal degradation caused by reinforced concrete and
electromagnetic interference. Consequently, robot odometry remains the primary
source of velocity and position information, despite being susceptible to drift
caused by both systematic and non-systematic errors. The calibrations of the
robot's systematic parameters were conducted using nonlinear optimization and
Levenberg-Marquardt methods as Newton-Gauss and gradient-based model fitting
methods, while Genetic algorithm and Particle swarm were used as
stochastic-based methods for kinematic parameter calibration. Performance and
results of the calibration methods and pose estimators were validated in detail
with experiments on the experimental mobile wall climbing robot.

</details>


### [326] [FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction](https://arxiv.org/abs/2509.04018)
*Yifan Yang,Zhixiang Duan,Tianshi Xie,Fuyu Cao,Pinxi Shen,Peili Song,Piaopiao Jin,Guokang Sun,Shaoqing Xu,Yangwei You,Jingtai Liu*

Main category: cs.RO

TL;DR: FPC-VLA是一个结合了视觉-语言-动作（VLA）模型和故障预测与纠正的监督模型的框架，提高了机器人操作的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统机器人操作中的感知-规划流程在开放式任务中灵活性不足，而单一的端到端视觉-语言-动作（VLA）模型虽然有潜力，但缺乏预测和纠正失败的机制。

Method: 提出了一种名为FPC-VLA的双模型框架，集成了VLA模型和一个监督模型，该监督模型通过视觉-语言查询评估动作的有效性，并在出现风险时生成纠正策略，且无需手动标记即可高效训练。此外，还设计了一个相似性引导的融合模块来利用过去的预测来优化动作。

Result: 在SIMPLER和LIBERO仿真平台以及WidowX、Google Robot和Franka机器人实体上进行了评估，FPC-VLA在零样本和微调设置下均优于现有模型。通过仅在关键帧激活监督模型，该方法在对执行时间影响最小的情况下显著提高了任务成功率。

Conclusion: FPC-VLA通过在关键帧激活监督模型，在不显著增加执行时间的情况下，显著提高了机器人任务的成功率，并成功应用于多种现实世界场景，证明了其在构建更可靠的自主系统方面的泛化能力和实用性。

Abstract: Robotic manipulation is a fundamental component of automation. However,
traditional perception-planning pipelines often fall short in open-ended tasks
due to limited flexibility, while the architecture of a single end-to-end
Vision-Language-Action (VLA) offers promising capabilities but lacks crucial
mechanisms for anticipating and recovering from failure. To address these
challenges, we propose FPC-VLA, a dual-model framework that integrates VLA with
a supervisor for failure prediction and correction. The supervisor evaluates
action viability through vision-language queries and generates corrective
strategies when risks arise, trained efficiently without manual labeling. A
similarity-guided fusion module further refines actions by leveraging past
predictions. Evaluation results on multiple simulation platforms (SIMPLER and
LIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA
outperforms state-of-the-art models in both zero-shot and fine-tuned settings.
By activating the supervisor only at keyframes, our approach significantly
increases task success rates with minimal impact on execution time. Successful
real-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong
generalization and practical utility for building more reliable autonomous
systems.

</details>


### [327] [Integrated Wheel Sensor Communication using ESP32 -- A Contribution towards a Digital Twin of the Road System](https://arxiv.org/abs/2509.04061)
*Ventseslav Yordanov,Simon Schäfer,Alexander Mann,Stefan Kowalewski,Bassam Alrifaee,Lutz Eckstein*

Main category: cs.RO

TL;DR: 该研究提出了一种新颖的轮内传感器数据传输方法，使用发布-订阅系统，能够高效传输大量数据，并验证了其在轮胎测试台上的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的车载状态估计方法无法提供轮胎与路面相互作用的详细信息。

Method: 提出并实现了一种基于发布-订阅系统的新型轮内传感器数据传输概念，并在轮胎测试台上使用不同采样频率（1 Hz 至 32 000 Hz）进行了测试。

Result: 该方法实现了最小的数据丢失率（约 0.1%），证明了其通信概念的有效性。

Conclusion: 所实现的轮内传感器原型展示了可靠的数据传输能力，为优化集成轮内传感器通信和实时数据采集提供了支持。

Abstract: While current onboard state estimation methods are adequate for most driving
and safety-related applications, they do not provide insights into the
interaction between tires and road surfaces. This paper explores a novel
communication concept for efficiently transmitting integrated wheel sensor data
from an ESP32 microcontroller. Our proposed approach utilizes a
publish-subscribe system, surpassing comparable solutions in the literature
regarding data transmission volume. We tested this approach on a drum tire test
rig with our prototype sensors system utilizing a diverse selection of sample
frequencies between 1 Hz and 32 000 Hz to demonstrate the efficacy of our
communication concept. The implemented prototype sensor showcases minimal data
loss, approximately 0.1 % of the sampled data, validating the reliability of
our developed communication system. This work contributes to advancing
real-time data acquisition, providing insights into optimizing integrated wheel
sensor communication.

</details>


### [328] [Cloud-Assisted Remote Control for Aerial Robots: From Theory to Proof-of-Concept Implementation](https://arxiv.org/abs/2509.04095)
*Achilleas Santi Seisa,Viswa Narayanan Sankaranarayanan,Gerasimos Damigos,Sumeet Gajanan Satpute,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 该论文提出了一个用于测试云和边缘机器人系统的可扩展且直观的框架，该框架利用容器化技术和UDP隧道来模拟网络延迟和抖动，解决了云机器人集成中的挑战。


<details>
  <summary>Details</summary>
Motivation: 集成云机器人系统面临网络延迟、安全和资源管理的挑战，需要一个有效的测试框架。

Method: 该框架包含一个容器化的云集群和一个容器化的机器人模拟环境，通过UDP隧道和Linux流量控制模拟网络条件。

Result: 成功构建了一个能够模拟实际云机器人部署中可变网络条件的框架，并以云辅助的无人机远程控制为例进行了展示。

Conclusion: 该框架为测试和开发云机器人和边缘系统提供了一个可扩展且用户友好的解决方案。

Abstract: Cloud robotics has emerged as a promising technology for robotics
applications due to its advantages of offloading computationally intensive
tasks, facilitating data sharing, and enhancing robot coordination. However,
integrating cloud computing with robotics remains a complex challenge due to
network latency, security concerns, and the need for efficient resource
management. In this work, we present a scalable and intuitive framework for
testing cloud and edge robotic systems. The framework consists of two main
components enabled by containerized technology: (a) a containerized cloud
cluster and (b) the containerized robot simulation environment. The system
incorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling
bidirectional communication between the cloud cluster container and the robot
simulation environment, while simulating realistic network conditions. To
achieve this, we consider the use case of cloud-assisted remote control for
aerial robots, while utilizing Linux-based traffic control to introduce
artificial delay and jitter, replicating variable network conditions
encountered in practical cloud-robot deployments.

</details>


### [329] [Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models](https://arxiv.org/abs/2509.04063)
*Hongyin Zhang,Shiyuan Zhang,Junxi Jin,Qixin Zeng,Yifan Qiao,Hongchao Lu,Donglin Wang*

Main category: cs.RO

TL;DR: 基于流匹配的视觉-语言-动作(VLA)模型在机器人操作任务中表现出色，但其在复杂下游任务中的动作准确性仍不理想。该研究提出了一种新的离线强化学习(RL)后训练目标和一种名为自适应强化流匹配(ARFM)的高效算法，通过引入自适应调整的缩放因子来平衡RL信号和流损失，从而在泛化性、鲁棒性、少样本学习和持续学习方面取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流匹配的VLA模型在复杂下游任务中的动作准确性不足，原因在于它们仅依赖模仿学习的训练后范式，难以深入理解数据质量的分布特性，而这正是强化学习(RL)所擅长的。

Method: 提出了一种理论上的离线RL后训练目标，并开发了一种高效可行的离线RL微调算法——自适应强化流匹配(ARFM)。ARFM在VLA流模型损失中引入了一个自适应调整的缩放因子，构建了一个原则性的偏差-方差权衡目标函数，以最佳地控制RL信号对流损失的影响，从而平衡RL优势的保持和流损失梯度的方差控制。

Result: ARFM在VLA流模型损失中通过自适应调整的缩放因子，实现了原则性的偏差-方差权衡，能够最优地控制RL信号对流损失的影响，达到了更稳定高效的微调过程。大量的仿真和真实世界实验结果表明，ARFM在泛化性、鲁棒性、少样本学习和持续学习方面表现优异。

Conclusion: ARFM通过引入自适应的RL信号与流损失的权衡机制，显著提高了VLA模型在复杂任务中的性能，并在泛化性、鲁棒性、少样本和持续学习能力方面展现出卓越的潜力。

Abstract: Vision-Language-Action (VLA) models based on flow matching have shown
excellent performance in general-purpose robotic manipulation tasks. However,
the action accuracy of these models on complex downstream tasks is
unsatisfactory. One important reason is that these models rely solely on the
post-training paradigm of imitation learning, which makes it difficult to have
a deeper understanding of the distribution properties of data quality, which is
exactly what Reinforcement Learning (RL) excels at. In this paper, we
theoretically propose an offline RL post-training objective for VLA flow models
and induce an efficient and feasible offline RL fine-tuning algorithm --
Adaptive Reinforced Flow Matching (ARFM). By introducing an adaptively adjusted
scaling factor in the VLA flow model loss, we construct a principled
bias-variance trade-off objective function to optimally control the impact of
RL signal on flow loss. ARFM adaptively balances RL advantage preservation and
flow loss gradient variance control, resulting in a more stable and efficient
fine-tuning process. Extensive simulation and real-world experimental results
show that ARFM exhibits excellent generalization, robustness, few-shot
learning, and continuous learning performance.

</details>


### [330] [Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning](https://arxiv.org/abs/2509.04069)
*Chengyandan Shen,Christoffer Sloth*

Main category: cs.RO

TL;DR: 该研究提出了一种名为DRLR的深度强化学习框架，结合了参考策略和演示数据，以提高机器人任务学习的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统深度强化学习（DRL）在机器人任务学习中探索效率低下和容易收敛到次优策略的问题，该研究提出了DRLR框架。

Method: DRLR框架基于IBRL算法，通过修改动作选择模块来提供校准的Q值，从而减少引导误差，提高探索效率。此外，使用SAC算法替代TD3算法，以防止策略收敛到次优策略。

Result: 实验结果表明，DRLR在桶装载和开抽屉两个机器人任务中均表现出色，有效缓解了引导误差和过拟合问题。该框架在不同状态-动作维度和演示质量下均表现出鲁棒性。在真实的轮式装载机上进行的桶装载任务也验证了DRLR的有效性。

Conclusion: DRLR框架能够有效提高机器人任务学习的探索效率，并防止策略收敛到次优解，且在真实机器人任务中表现良好。

Abstract: This paper proposes an exploration-efficient Deep Reinforcement Learning with
Reference policy (DRLR) framework for learning robotics tasks that incorporates
demonstrations. The DRLR framework is developed based on an algorithm called
Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve
IBRL by modifying the action selection module. The proposed action selection
module provides a calibrated Q-value, which mitigates the bootstrapping error
that otherwise leads to inefficient exploration. Furthermore, to prevent the RL
policy from converging to a sub-optimal policy, SAC is used as the RL policy
instead of TD3. The effectiveness of our method in mitigating bootstrapping
error and preventing overfitting is empirically validated by learning two
robotics tasks: bucket loading and open drawer, which require extensive
interactions with the environment. Simulation results also demonstrate the
robustness of the DRLR framework across tasks with both low and high
state-action dimensions, and varying demonstration qualities. To evaluate the
developed framework on a real-world industrial robotics task, the bucket
loading task is deployed on a real wheel loader. The sim2real results validate
the successful deployment of the DRLR framework.

</details>


### [331] [Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot](https://arxiv.org/abs/2509.04076)
*Lennart Clasmeier,Jan-Gerrit Habekost,Connor Gäde,Philipp Allgeuer,Stefan Wermter*

Main category: cs.RO

TL;DR: 提出了一种新颖的基于扩散模型的机器人运动规划方法，该方法通过深度学习在更短的运行时间内获得与传统数值规划方法相当的结果，并能有效避免碰撞。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统数值规划方法在机器人运动规划中存在的显著运行时间要求问题。

Method: 提出了一种新颖的基于扩散模型的机器人运动规划方法。该模型利用深度学习，通过学习数值规划器生成的数据集来预测基于关键点的关节序列。在研究中，我们还尝试了点云嵌入作为输入，并发现数据集中的偏差会影响模型性能，通过优化数据集和模型，即使在不使用点云编码的情况下，该模型也显著优于数值模型。

Result: 该模型在运行时方面比数值模型快一个数量级，并且在测试集上达到了高达90%的无碰撞解决方案成功率。

Conclusion: 该模型在运行时和成功率方面都表现出色，证明了基于扩散模型的机器人运动规划方法的有效性。

Abstract: We propose a novel diffusion-based action model for robotic motion planning.
Commonly, established numerical planning approaches are used to solve general
motion planning problems, but have significant runtime requirements. By
leveraging the power of deep learning, we are able to achieve good results in a
much smaller runtime by learning from a dataset generated by these planners.
While our initial model uses point cloud embeddings in the input to predict
keypoint-based joint sequences in its output, we observed in our ablation study
that it remained challenging to condition the network on the point cloud
embeddings. We identified some biases in our dataset and refined it, which
improved the model's performance. Our model, even without the use of the point
cloud encodings, outperforms numerical models by an order of magnitude
regarding the runtime, while reaching a success rate of up to 90% of collision
free solutions on the test set.

</details>


### [332] [Object-Reconstruction-Aware Whole-body Control of Mobile Manipulators](https://arxiv.org/abs/2509.04094)
*Fatih Dursun,Bruno Vilhena Adorno,Simon Watson,Wei Pan*

Main category: cs.RO

TL;DR: 该研究提出了一种计算效率更高的物体重建和检查的视图路径规划方法，通过将机器人维持在信息最丰富的未知区域的焦点上来优化路径，从而无需额外的路径规划器。


<details>
  <summary>Details</summary>
Motivation: 当前基于采样的视图路径规划方法计算成本高，需要评估路径上的多个候选视图，而无法在保持对象覆盖率和熵的同时提高效率。

Method: 提出了一种计算高效的解决方案，该方案通过计算信息最丰富的（未知）区域的焦点，并让机器人在路径上将该焦点保持在摄像机的视场内来实现。该策略被整合到采用可见性约束的整​​体控制中，无需额外的路径规划器。

Result: 在模拟和真实世界实验中，与基于采样的规划策略相比，该方法在物体覆盖率和熵方面没有显著差异，但速度却快了约九倍。

Conclusion: 所提出的方法在物体重建和检查任务中，在保持覆盖率和熵的同时，显著提高了效率，有望应用于机器人领域。

Abstract: Object reconstruction and inspection tasks play a crucial role in various
robotics applications. Identifying paths that reveal the most unknown areas of
the object becomes paramount in this context, as it directly affects
efficiency, and this problem is known as the view path planning problem.
Current methods often use sampling-based path planning techniques, evaluating
potential views along the path to enhance reconstruction performance. However,
these methods are computationally expensive as they require evaluating several
candidate views on the path. To this end, we propose a computationally
efficient solution that relies on calculating a focus point in the most
informative (unknown) region and having the robot maintain this point in the
camera field of view along the path. We incorporated this strategy into the
whole-body control of a mobile manipulator employing a visibility constraint
without the need for an additional path planner. We conducted comprehensive and
realistic simulations using a large dataset of 114 diverse objects of varying
sizes from 57 categories to compare our method with a sampling-based planning
strategy using Bayesian data analysis. Furthermore, we performed real-world
experiments with an 8-DoF mobile manipulator to demonstrate the proposed
method's performance in practice. Our results suggest that there is no
significant difference in object coverage and entropy. In contrast, our method
is approximately nine times faster than the baseline sampling-based method in
terms of the average time the robot spends between views.

</details>


### [333] [Lightweight Kinematic and Static Modeling of Cable-Driven Continuum Robots via Actuation-Space Energy Formulation](https://arxiv.org/abs/2509.04119)
*Ke Wu,Yuhao Wang,Kevin Henry,Cesare Stefanini,Gang Zheng*

Main category: cs.RO

TL;DR: LASEM框架为 kabel 驱动的连续机器人提供了一个计算效率高且准确的解析前向模型，无需显式建模 kabel-主体接触，并统一了运动学和静态公式。


<details>
  <summary>Details</summary>
Motivation: 连续机器人因其灵巧性和柔顺性而非常适合非结构化和狭窄环境，但其可变形的形态给运动规划和控制带来了挑战，需要精确轻便的模型。

Method: 提出了一种名为 LASEM 的轻量级驱动空间能量建模框架，该框架直接在驱动空间中构建驱动势能。该方法利用哈密顿原理，从几何非线性梁和杆理论中推导出解析前向模型，避免了 kabel-主体接触的显式建模。它同时支持力和位移输入，并考虑了非均匀几何、任意 kabel 路由、分布式载荷和轴向可扩展性。对于离散化问题，将函数最小化问题重新表述为数值优化问题。

Result: LASEM 框架能够提供一个计算效率高且准确的解析前向模型，并开发了一个用于逆运动学的半解析迭代方案。数值模拟验证了该模型的准确性。

Conclusion: LASEM 框架为 kabel 驱动的连续机器人提供了一种有效且通用的建模方法，能够准确地处理复杂的几何和载荷条件，同时保持实时计算效率。它通过将函数最小化重新表述为数值优化问题，自然地包含了 kabel 势能，而无需显式建模 kabel 接触。

Abstract: Continuum robots, inspired by octopus arms and elephant trunks, combine
dexterity with intrinsic compliance, making them well suited for unstructured
and confined environments. Yet their continuously deformable morphology poses
challenges for motion planning and control, calling for accurate but
lightweight models. We propose the Lightweight Actuation Space Energy Modeling
(LASEM) framework for cable driven continuum robots, which formulates actuation
potential energy directly in actuation space. LASEM yields an analytical
forward model derived from geometrically nonlinear beam and rod theories via
Hamilton's principle, while avoiding explicit modeling of cable backbone
contact. It accepts both force and displacement inputs, thereby unifying
kinematic and static formulations. Assuming the friction is neglected, the
framework generalizes to nonuniform geometries, arbitrary cable routings,
distributed loading and axial extensibility, while remaining computationally
efficient for real-time use. Numerical simulations validate its accuracy, and a
semi-analytical iterative scheme is developed for inverse kinematics. To
address discretization in practical robots, LASEM further reformulates the
functional minimization as a numerical optimization, which also naturally
incorporates cable potential energy without explicit contact modeling.

</details>


### [334] [OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection](https://arxiv.org/abs/2509.04324)
*Chen Hu,Shan Luo,Letizia Gionfrida*

Main category: cs.RO

TL;DR: OVGrasp是一个基于RGB-D视觉、开放词汇提示和语音命令的软性外骨骼抓取辅助框架，能识别新物体并理解用户意图，在实验中抓取成功率为87%，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 为了解能在非结构化环境中为运动障碍者提供抓取辅助，以恢复其自主性。

Method: 提出了一种名为OVGrasp的分层控制框架，集成了RGB-D视觉、开放词汇提示和语音命令，并使用视觉-语言基础模型实现了对未见过物体的零样本检测，以及一个多模态决策者来融合空间和语言线索以推断用户意图。

Result: 在15种物体和3种抓取类型上进行了测试，抓取能力得分（GAS）为87.00%，优于现有技术，并且在运动学上更接近自然手部运动。

Conclusion: OVGrasp通过多模态融合和开放词汇识别，实现了对非结构化环境中不同物体和用户意图的鲁棒抓取辅助，显著提高了运动障碍者的抓取能力。

Abstract: Grasping assistance is essential for restoring autonomy in individuals with
motor impairments, particularly in unstructured environments where object
categories and user intentions are diverse and unpredictable. We present
OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp
assistance that integrates RGB-D vision, open-vocabulary prompts, and voice
commands to enable robust multimodal interaction. To enhance generalization in
open environments, OVGrasp incorporates a vision-language foundation model with
an open-vocabulary mechanism, allowing zero-shot detection of previously unseen
objects without retraining. A multimodal decision-maker further fuses spatial
and linguistic cues to infer user intent, such as grasp or release, in
multi-object scenarios. We deploy the complete framework on a custom
egocentric-view wearable exoskeleton and conduct systematic evaluations on 15
objects across three grasp types. Experimental results with ten participants
demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,
outperforming state-of-the-art baselines and achieving improved kinematic
alignment with natural hand motion.

</details>


### [335] [DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation](https://arxiv.org/abs/2509.04441)
*Hao-Shu Fang,Branden Romero,Yichen Xie,Arthur Hu,Bo-Ruei Huang,Juan Alvarez,Matthew Kim,Gabriel Margolis,Kavya Anbarasu,Masayoshi Tomizuka,Edward Adelson,Pulkit Agrawal*

Main category: cs.RO

TL;DR: DEXOP是一个新的范式，用于机器人数据收集，通过被动手部外骨骼传感器化并记录人类操作，以最大限度地提高数据到真实机器人的可转移性。


<details>
  <summary>Details</summary>
Motivation: 为了提高机器人数据收集的效率和质量，特别是针对灵巧操作任务，并解决传统遥操作数据收集的局限性。

Method: 提出并实现了一个名为DEXOP的被动手部外骨骨骼。DEXOP机械连接人手指和机器人手指，提供力反馈和姿态镜像，使用户能够更自然地进行任务演示，从而收集包含视觉和触觉的丰富传感数据。

Result: DEXOP能够大规模收集高质量的演示数据，并且在各种灵巧、接触丰富的任务中进行了评估。使用DEXOP数据学习的策略显著提高了每单位数据收集时间的任务性能。

Conclusion: DEXOP作为一种强大的工具，通过提高数据收集的效率和质量，能够有效推动机器人灵巧性的发展。

Abstract: We introduce perioperation, a paradigm for robotic data collection that
sensorizes and records human manipulation while maximizing the transferability
of the data to real robots. We implement this paradigm in DEXOP, a passive hand
exoskeleton designed to maximize human ability to collect rich sensory (vision
+ tactile) data for diverse dexterous manipulation tasks in natural
environments. DEXOP mechanically connects human fingers to robot fingers,
providing users with direct contact feedback (via proprioception) and mirrors
the human hand pose to the passive robot hand to maximize the transfer of
demonstrated skills to the robot. The force feedback and pose mirroring make
task demonstrations more natural for humans compared to teleoperation,
increasing both speed and accuracy. We evaluate DEXOP across a range of
dexterous, contact-rich tasks, demonstrating its ability to collect
high-quality demonstration data at scale. Policies learned with DEXOP data
significantly improve task performance per unit time of data collection
compared to teleoperation, making DEXOP a powerful tool for advancing robot
dexterity. Our project page is at https://dex-op.github.io.

</details>


### [336] [EMMA: Scaling Mobile Manipulation via Egocentric Human Data](https://arxiv.org/abs/2509.04443)
*Lawrence Y. Zhu,Pranav Kuppili,Ryan Punamiya,Patcharapong Aphiwetsa,Dhruv Patel,Simar Kareer,Sehoon Ha,Danfei Xu*

Main category: cs.RO

TL;DR: EMMA框架使用静态机器人数据和人类全身运动数据进行移动操作策略的训练，无需昂贵的机器人远程操作，实验证明其性能可与远程操作的基线相媲美，并能泛化到新场景。


<details>
  <summary>Details</summary>
Motivation: 移动操作模仿学习面临昂贵的移动机器人远程操作瓶颈。

Method: EMMA框架通过联合训练人类全身运动数据和静态机器人数据，实现从人类移动操作数据训练移动操作策略，无需移动机器人远程操作。

Result: 实验证明EMMA在三个真实任务上的表现与使用远程操作的基线（Mobile ALOHA）相当，在完整任务成功率方面更高或相同。EMMA能够泛化到新的空间配置和场景，增加人类数据量可带来性能提升。

Conclusion: EMMA为可扩展的机器人学习提供了新的途径，尤其是在真实环境中。

Abstract: Scaling mobile manipulation imitation learning is bottlenecked by expensive
mobile robot teleoperation. We present Egocentric Mobile MAnipulation (EMMA),
an end-to-end framework training mobile manipulation policies from human mobile
manipulation data with static robot data, sidestepping mobile teleoperation. To
accomplish this, we co-train human full-body motion data with static robot
data. In our experiments across three real-world tasks, EMMA demonstrates
comparable performance to baselines trained on teleoperated mobile robot data
(Mobile ALOHA), achieving higher or equivalent task performance in full task
success. We find that EMMA is able to generalize to new spatial configurations
and scenes, and we observe positive performance scaling as we increase the
hours of human data, opening new avenues for scalable robotic learning in
real-world environments. Details of this project can be found at
https://ego-moma.github.io/.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [337] [A Cegar-centric Bounded Reachability Analysis for Compositional Affine Hybrid Systems](https://arxiv.org/abs/2509.03560)
*Atanu Kundu,Pratyay Sarkar,Rajarshi Ray*

Main category: cs.LO

TL;DR: 该研究提出了一种基于CEGAR的有界可达性分析算法，用于处理组件化混合动力系统，通过避免显式计算乘积自动机来解决状态空间爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 组件化混合动力系统（组件是混合动力自动机）的可达性分析面临挑战，需要处理状态数量的爆炸性增长，同时保持组合语义。

Method: 提出了一种基于CEGAR（反例引导的抽象细化）的有界可达性分析算法。该算法在不显式计算乘积自动机的情况下，在组合模型的离散抽象中搜索反例。当在抽象中发现反例时，通过基于抽象反例的状态空间细化来验证其有效性。状态空间细化利用了支持函数作为连续状态表示的符号可达性分析。此外，该算法混合了不同的组合语义（在探索抽象状态空间时使用步进组合语义，在状态空间细化时使用浅组合语义）以提高效率，并采用了缓存符号可达性分析结果等优化措施。

Result: 在SAT-Reach工具中实现了该算法，并展示了其可扩展性优势。

Conclusion: 所提出的CEGAR方法能够有效地处理组件化混合动力系统中的可达性分析问题，并通过混合组合语义和缓存等优化手段提高了效率和可扩展性。

Abstract: Reachability analysis of compositional hybrid systems, where individual
components are modeled as hybrid automata, poses unique challenges. In addition
to preserving the compositional semantics while computing system behaviors,
algorithms have to cater to the explosion in the number of locations in the
parallel product automaton. In this paper, we propose a bounded reachability
analysis algorithm for compositional hybrid systems with piecewise affine
dynamics, based on the principle of counterexample guided abstraction
refinement (CEGAR). In particular, the algorithm searches for a counterexample
in the discrete abstraction of the composition model, without explicitly
computing a product automaton. When a counterexample is discovered in the
abstraction, its validity is verified by a refinement of the state-space guided
by the abstract counterexample. The state-space refinement is through a
symbolic reachability analysis, particularly using a state-of-the-art algorithm
with support functions as the continuous state representation. In addition, the
algorithm mixes different semantics of composition with the objective of
improved efficiency. Step compositional semantics is followed while exploring
the abstract (discrete) state-space, while shallow compositional semantics is
followed during state-space refinement with symbolic reachability analysis.
Optimizations such as caching the results of the symbolic reachability
analysis, which can be later reused, have been proposed. We implement this
algorithm in the tool SAT-Reach and demonstrate the scalability benefits.

</details>


### [338] [Simplicity Lies in the Eye of the Beholder: A Strategic Perspective on Controllers in Reactive Synthesis](https://arxiv.org/abs/2509.04129)
*Mickael Randour*

Main category: cs.LO

TL;DR: This paper discusses the complexity of strategies in game-theoretic controller synthesis, focusing on memory and randomness, and exploring beyond traditional complexity notions.


<details>
  <summary>Details</summary>
Motivation: A common belief is that simple strategies (e.g., using limited memory) are better for controllers because they are easier to conceive, understand, produce, and maintain. This paper investigates the complexity of strategies in various synthesis contexts to explore this belief.

Method: The paper analyzes recent results concerning memory and randomness in the context of game-theoretic controller synthesis. It also takes a brief look at complexity notions beyond traditional ones.

Result: The paper discusses recent results concerning memory and randomness in controller synthesis, and explores beyond traditional notions of strategy complexity.

Conclusion: The paper examines the complexity of strategies in game-theoretic controller synthesis, considering factors like memory and randomness, and looking into new complexity paradigms.

Abstract: In the game-theoretic approach to controller synthesis, we model the
interaction between a system to be controlled and its environment as a game
between these entities, and we seek an appropriate (e.g., winning or optimal)
strategy for the system. This strategy then serves as a formal blueprint for a
real-world controller. A common belief is that simple (e.g., using limited
memory) strategies are better: corresponding controllers are easier to conceive
and understand, and cheaper to produce and maintain.
  This invited contribution focuses on the complexity of strategies in a
variety of synthesis contexts. We discuss recent results concerning memory and
randomness, and take a brief look at what lies beyond our traditional notions
of complexity for strategies.

</details>


### [339] [Janus-faces of temporal constraint languages: a dichotomy of expressivity](https://arxiv.org/abs/2509.04347)
*Johanna Brunar,Michael Pinsker,Moritz Schöbi*

Main category: cs.LO

TL;DR: Bodirsky-K'ara分类的可行时间约束语言在图和超图上的解释能力有限，这导致了新的代数结果，并为已知的性质提供了统一的证明。


<details>
  <summary>Details</summary>
Motivation: 虽然Bodirsky-K'ara分类是最早的关于无限域CSP的时间约束语言的复杂性分类之一，但其可行的情形在算法和代数不变性方面仍然是最神秘的。

Method: 通过研究那些不能pp-构造一切的时间语言（这些语言根据分类是可以在多项式时间内解决的），我们证明了它们在图和超图上的解释能力非常有限。

Result: 这种限制带来了许多先前未知的代数结果，并为已知的代数不变性提供了新的、统一的证明。具体来说，我们证明了这些时间约束语言允许$4$元伪Siggers多项式。

Conclusion: 这项研究揭示了可行的Bodirsky-K'ara时间约束语言的有限解释能力，并提供了新的代数见解，支持了Bodirsky-Pinsker猜想在更广泛背景下的可能性。

Abstract: The Bodirsky-K\'ara classification of temporal constraint languages stands as
one of the earliest and most seminal complexity classifications within
infinite-domain Constraint Satisfaction Problems (CSPs), yet it remains one of
the most mysterious in terms of algorithms and algebraic invariants for the
tractable cases. We show that those temporal languages which do not
pp-construct EVERYTHING (and thus by the classification are solvable in
polynomial time) have, in fact, very limited expressive power as measured by
the graphs and hypergraphs they can pp-interpret. This limitation yields many
previously unknown algebraic consequences, while also providing new, uniform
proofs for known invariance properties. In particular, we show that such
temporal constraint languages admit $4$-ary pseudo-Siggers polymorphisms -- a
result that sustains the possibility that the existence of such polymorphisms
extends to the much broader context of the Bodirsky-Pinsker conjecture.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [340] [From Qubits to Qumodes: Information Capacity of Anyonic Excitations](https://arxiv.org/abs/2509.03546)
*Satish Prajapati*

Main category: cond-mat.mes-hall

TL;DR: 该研究推导了由Haldane统计决定的量子态的最大信息容量，其结果可用于区分费米子、玻色子以及任意子统计。


<details>
  <summary>Details</summary>
Motivation: 研究量子统计和信息编码之间的相互作用，并推导了由Haldane统计决定的量子态的最大信息容量。

Method: 利用量子态的混合统计特性，推导出其最大信息容量的计算公式S_max(g) = log2(⌊1/g⌋ + 1)，并将其与费米子和玻色子极限进行比较。最后，将该理论应用于分数量子霍尔效应的1/3态，并提出通过量子点光谱的电导平台来验证任意子统计。

Result: 得出了量子态的最大信息容量公式S_max(g) = log2(⌊1/g⌋ + 1)，该公式在费米子极限（g=1）和玻色子极限（g→0）下均成立。对于1/3分数量子霍尔态，预测其信息容量为2比特，并提出可以通过观测量子点光谱中的四个量子化电导平台来验证。

Conclusion: 该研究首次导出了由Haldane统计决定的量子态的最大信息容量，并提出了一种通过实验观测的方法来验证任意子统计。

Abstract: The interplay between quantum statistics and information encoding is a
cornerstone of quantum physics. Here, the maximum information capacity of a
quantum state governed by Haldane's exclusion statistics is derived. The
capacity, defined by the maximum von Neumann entropy of its occupancy
distribution, follows S_max(g) = log2(\lfloor 1/g \rfloor + 1). This result
continuously interpolates between the fermionic limit of a single qubit (g = 1)
and the bosonic limit of a continuous-variable qumode (g -> 0). For the nu =
1/3 fractional quantum Hall state (g = 1/3), we predict a 2-bit capacity,
observable as four distinct quantized conductance plateaus in quantum dot
spectroscopy, providing a direct signature of anyonic statistics.

</details>


### [341] [Exchange tensors, generalized RKKY interactions, and magnetization dynamics in heterostructures of ferromagnets and topological insulators](https://arxiv.org/abs/2509.03572)
*Christian Svingen Johnsen,Asle Sudbø*

Main category: cond-mat.mes-hall

TL;DR: 该论文理论分析了铁磁层/三维拓扑绝缘体异质结构中的磁相互作用，发现了由拓扑表面态介导的有效RKKY交换相互作用，并引入了类似DM的相互作用，影响自旋动力学和磁畴结构，为设计新型磁性现象提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 研究铁磁层/三维拓扑绝缘体异质结构中的磁相互作用，以期发现由拓扑表面态引起的特殊磁性现象。

Method: 通过积分拓扑表面态和计算二阶自旋行列式，推导出由拓扑表面态介导的有效广义RKKY交换相互作用，并推导了LLG方程，研究了磁相互作用对LLG方程和Magnon色散的影响。

Result: 发现了具有自旋动量锁定和各向异性自旋磁化率的有效RKKY交换相互作用，并推导了包含非局域、延迟、手性、Dzyaloshinskii-Moriya（DM）相互作用的有效自旋哈密顿量。该相互作用还诱导了与磁化率的旋度变化率相关的LLG方程项，并对Magnon色散进行了修正，包括可调的Magnon能隙。

Conclusion: 理论分析揭示了拓扑诱导的磁性现象，为在TI/FM混合系统中工程化具有可调相互作用的奇异自旋结构（如Skyrmion和手性畴壁）铺平了道路。

Abstract: We present a comprehensive theoretical analysis of magnetic heterostructures
composed of ferromagnetic (FM) layers interfaced with three-dimensional
topological insulators (TIs). Integrating out the topological surface states
and computing the spin determinant to second order in spins, we derive the
effective generalized Ruderman-Kittel-Kasuya-Yosida (RKKY) exchange
interactions mediated by topological surface states. These interactions
inherently incorporate spin-momentum locking and anisotropic spin
susceptibilities stemming from the Dirac-like dispersion of the TI surface
electrons. The analysis reveals that the interplay between the spin-orbit
coupling intrinsic to the TI and the magnetization texture in the FM layer
induces highly nonlocal and retarded, chiral, and Dzyaloshinskii-Moriya
(DM)-like contributions to the effective spin Hamiltonian. Furthermore, the
spin dynamics is studied through a derivation of the LLG equation for this
problem. The induced interactions renormalize many of the FM's intrinsic
properties, but a term in the LLG equation is induced that is related to the
rate of change of the magnetization's curl, which is relevant to skyrmion
dynamics. The magnon dispersion exhibits modifications due to the TI-mediated
interactions, including tunable magnon gaps, sensitive to a tunable chemical
potential and interfacial exchange coupling strength. The results also apply to
finite temperatures. They elucidate topologically induced magnetic phenomena
and pave the way for engineering exotic spin textures, such as skyrmions and
chiral domain walls, in TI/FM hybrid systems with tunable interactions.

</details>


### [342] [Magic continuum in multi-moiré twisted trilayer graphene](https://arxiv.org/abs/2509.03583)
*Li-Qiao Xia,Aviram Uri,Jiaojie Yan,Aaron Sharpe,Filippo Gaggioli,Nicole S. Ticea,Julian May-Mann,Kenji Watanabe,Takashi Taniguchi,Liang Fu,Trithep Devakul,Jurgen H. Smet,Pablo Jarillo-Herrero*

Main category: cond-mat.mes-hall

TL;DR: 扭曲三层石墨烯中的超晶格调制和晶格弛豫驱动了相关的电子现象，如反常霍尔效应和超导性。


<details>
  <summary>Details</summary>
Motivation: 探索电子关联与能带拓扑的相互作用，以及引入第二种莫尔图案以进一步定制电子特性的方法。

Method: 研究了具有不同扭转角的扭曲三层石墨烯器件，并根据晶格弛豫程度将其分为莫尔多晶体和莫尔准晶两类。

Result: 在螺旋扭曲的莫尔多晶体中观察到反常霍尔效应，在莫尔准晶体中普遍观察到超导性，其中一部分系统表现出空间调制超导性的特征。

Conclusion: 提出了观察到的扭曲三层石墨烯中相关相的组织原则，强调了超晶格调制和晶格弛豫的关键作用，并建议将魔角条件视为复杂莫尔材料多维扭转角空间中的扩展流形，而非孤立点。

Abstract: Moir\'e lattices provide a highly tunable platform for exploring the
interplay between electronic correlations and band topology. Introducing a
second moir\'e pattern extends this paradigm: interference between the two
moir\'e patterns produces a supermoir\'e modulation, opening a route to further
tailor electronic properties. Twisted trilayer graphene generally exemplifies
such a system: two distinct moir\'e patterns arise from the relative twists
between adjacent graphene layers. Here, we report the observation of correlated
phenomena across a wide range of twisted trilayer graphene devices whose twist
angles lie along two continuous lines in the twist-angle parameter space.
Depending on the degree of lattice relaxation, twisted trilayer graphene falls
into two classes: moir\'e polycrystals, composed of periodic domains with
locally commensurate moir\'e order, and moir\'e quasicrystals, characterized by
smoothly varying local moir\'e configurations. In helically twisted moir\'e
polycrystals, we observe an anomalous Hall effect, consistent with topological
bands arising from domains with broken $xy$-inversion symmetry. In contrast,
superconductivity appears generically in our moir\'e quasicrystals. A subset of
these systems exhibits signatures of spatially modulated superconductivity,
which we attribute to the supermoir\'e structure. Our findings uncover the
organizing principles of the observed correlated phases in twisted trilayer
graphene, highlight the critical roles of the supermoir\'e modulation and
lattice relaxation, and suggest a broader framework in which magic conditions
arise not as isolated points but as extended manifolds within the
multi-dimensional twist-angle space of complex moir\'e materials.

</details>


### [343] [Optical selection rules of topological excitons in flat bands](https://arxiv.org/abs/2509.03601)
*Mara Lozano,Hong-Yi Xie,Bruno Uchoa*

Main category: cond-mat.mes-hall

TL;DR: 该论文推导了拓扑激子在平带中的光学选择规则，并考虑了短程和库仑相互作用。


<details>
  <summary>Details</summary>
Motivation: 研究拓扑激子在平带中的光学选择规则，并考虑不同拓扑两带模型和相互作用的影响。

Method: 推导了包含不同拓扑两带模型（包括具有skyrmion赝自旋纹理的哈密顿量族、单自旋的展平BHZ模型和展平Haldane模型）的拓扑激子的光学选择规则，并考虑了短程相互作用。同时，研究了单自旋展平BHZ模型中具有库仑相互作用的激子的非氢谱。

Result: 对于具有skyrmion赝自旋纹理的双平带模型，所有激子都是明亮的，并且与它们耦合的光的旋向由赝自旋纹理的涡度决定。对于单自旋展平BHZ模型，明亮激子与圆偏振光耦合，与相互作用的范围无关。在展平Haldane模型中，拓扑激子与椭圆偏振光耦合，并获得了该模型中光偏振的相图。

Conclusion: 拓扑激子在不同模型和相互作用下表现出不同的光学选择规则，这为理解和调控拓扑材料的光学性质提供了理论基础。

Abstract: Topological excitons are superpositions of electron-hole pair states with an
envelope wavefunction that has finite vorticity in momentum space, dictated by
the topology of the electronic bands. We derive the optical selection rules for
topological excitons in flat bands, considering different topological two-band
models: a family of Hamiltonians with skyrmion pseudo-spin textures, the
flattened BHZ model for a single spin, which can have a net Chern number, and
the flattened Haldane model. We derive the selection rules for these three
models accounting for short-range interactions. We also consider the
non-hydrogenic spectrum of excitons in the single-spin flattened BHZ model with
Coulomb interactions. We show that for the case of two flat bands with skyrmion
pseudo-spin textures, all excitons are bright, and the handedness of the light
that couples to them is fixed by the vorticity of the pseudo-spin texture. For
the single-spin flattened BHZ model, we show that bright excitons couple to
circularly polarized light, regardless the range of the interactions. In the
flattened Haldane model, we find that topological excitons couple to
elliptically polarized light. We obtain the phase diagram for the polarization
of light in this model as a function of the microscopic parameters of the
Hamiltonian.

</details>


### [344] [Coherent control of thermoelectric performance via engineered transmission functions in multi-dot Aharonov-Bohm heat engine](https://arxiv.org/abs/2509.03606)
*Sridhar,Salil Bedkihal,Malay Bandyopadhyay*

Main category: cond-mat.mes-hall

TL;DR: 利用量子干涉优化量子点热电引擎的性能，实现高效率和高功率输出。


<details>
  <summary>Details</summary>
Motivation: 理论研究利用量子干涉来优化多量子点Aharonov-Bohm（AB）热电引擎的性能指标ZT、功率输出和热力学效率。

Method: 采用非平衡格林函数形式主义，通过器件几何、磁通量和点-导线耦合来调控干涉效应，如Fano型不对称、Dicke类超辐射和亚辐射模式以及多峰透射谱，以产生混合透射谱，结合了洛伦兹线型、箱型和Fano线型。

Result: 工程化的透射谱可以实现高效率和高功率输出的组合，实现了近乎最优的功率-效率权衡。对于对称量子点阵列，在t/γ≈2的条件下，可以获得最佳的功率和效率平衡。六量子点构型在稀释温度下ZT值高达30，而四量子点构型可以达到卡诺效率的76%，输出功率为4.74 fW。高ZT区域与Wiedemann-Franz定律的最大违背直接对应。引入源-漏耦合不对称性可以进一步提高效率和功率。扩展性分析表明，效率随着量子点数量的增加而系统性地增加，而功率输出在中等系统尺寸下最大化。

Conclusion: 通过多量子点纳米结构中的相干控制，为开发高性能量子热电器件引擎以实现超低功耗电子学应用提供了一条有前景的途径。

Abstract: We theoretically investigate strategies for harnessing quantum interference
to optimize the figure of merit $ZT$, power output, and thermodynamic
efficiency in multi-quantum-dot Aharonov-Bohm (AB) thermoelectric heat engines.
Using the non-equilibrium Green function formalism, we show that interference
effects such as Fano-type asymmetries, Dicke-like superradiant and subradiant
modes, and multi-peaked transmission spectra can be tailored through device
geometry, magnetic flux, and dot-lead coupling to produce hybrid transmission
profiles that combine Lorentzian, boxcar, and Fano lineshapes. Such engineered
profiles enable configurations that balance the high efficiency of sharp
Lorentzian resonances with the high power output of boxcar-like spectra,
yielding near-optimal power-efficiency trade-offs. For symmetric quantum-dot
arrays in square, pentagonal, and hexagonal configurations, we identify an
optimal regime, $t/\gamma \approx 2$, where the interdot tunneling amplitude
$t$ and the dot-lead coupling $\gamma$ yield the best balance of power and
efficiency. A hexagonal six-dot configuration achieves $ZT \sim 30$ at dilution
temperatures, while the four-dot geometry reaches about $76\%$ of Carnot
efficiency with output power $4.74$ fW. We also find a direct correspondence
between the high-$ZT$ regime and maximal violation of the Wiedemann-Franz law.
Introducing source-drain coupling asymmetry further enhances both efficiency
and power. A scaling analysis reveals that efficiency systematically increases
with the number of quantum dots, whereas power output is maximized at
intermediate system sizes. These findings establish coherent control in
multi-dot nanostructures as a promising pathway toward high-performance quantum
thermoelectric heat engines for ultralow-power electronics applications.

</details>


### [345] [Three-channel charge Kondo model at high transparency](https://arxiv.org/abs/2509.03612)
*Nicolas Paris,Nicolas Dupuis,Christophe Mora*

Main category: cond-mat.mes-hall

TL;DR: 高接触透明度下的量子岛与三个量子霍尔边缘通道的耦合模型，使用函数重整化群方法，揭示了由非微扰固定点控制的低能物理，并获得了线性电导和杂质熵的普适能量交叉，还发现了与Luttinger参数K相关的固定点线。


<details>
  <summary>Details</summary>
Motivation: 研究高接触透明度下量子岛与三个量子霍尔边缘通道耦合的模型，以及相互作用的电子导致的行为。

Method: 使用函数重正化群（FRG）方法。

Result: 在低能物理中发现了由非微扰固定点控制的通用能量交叉，并重现了三通道Kondo模型的零频电导和低能指数。对于相互作用的电子，发现了与Luttinger参数K相关的固定点线。

Conclusion: 函数重正化群（FRG）是解决标准方法失效的量子杂质问题的有效工具。

Abstract: Quantum impurity models involving a localized charge that is weakly coupled
to electronic leads usually map to Kondo-like Hamiltonians that exhibit various
quantum critical behaviors. Here, we address the opposite regime of high
contact transparency by solving the model of a quantum island coupled to three
quantum Hall edge channels. Using a functional renormalization group (FRG)
approach, we demonstrate that the low-energy physics is controlled by a
nonperturbative fixed point. The universal energy crossover in both the linear
conductance and the impurity entropy is obtained. We reproduce the
zero-frequency conductance and the leading low-energy exponent of the
three-channel Kondo model, confirming their universality across all
transparencies. For interacting leads -- a model that continuously connects to
the pseudo-gap Kondo model at low transparency -- we find a line of fixed
points as the Luttinger parameter $K$, which encodes the strength of the
interactions, changes. Our work demonstrates that FRG methods are an efficient
tool for solving quantum impurity problems in regimes where standard approaches
fail.

</details>


### [346] [Topological edge states in a double isomeric Class-II oligo(indenoindene)](https://arxiv.org/abs/2509.03618)
*Ricardo Ortiz*

Main category: cond-mat.mes-hall

TL;DR: Oligo(indenoindene) (OInIn) 类别的 II 号异构体可以作为无相互作用的紧束缚链，并表现出 Su-Schrieffer-Heeger (SSH) 模型中的拓扑物理学，这在由两个属于该类别的异构体组合而成的系统中得到了证实。


<details>
  <summary>Details</summary>
Motivation: 理论预测一维多自由基共轭环系统（寡聚茚并茚，OInIn）中存在非平凡物理学。

Method: 通过计算非相互作用的带结构、自旋无关平均场哈伯德和密度泛函理论计算，研究 OInIn 的拓扑性质和电子行为。

Result: 研究的 OInIn 系统的非相互作用带结构显示出与纯异构体相比，打开了一个能隙，并根据终止情况，在链的末端存在带隙局域态。此外，非零的 Zak 相位证实了非平凡的拓扑结构。自旋无关平均场哈伯德和密度泛函理论计算显示，五元环上存在反铁磁未淬灭的局域磁矩，并且强边缘局域化取决于终止情况。

Conclusion: 该研究通过理论预测和计算模拟，证实了 OInIn 系统中存在 SSH 物理学，并揭示了其非平凡的拓扑特性和磁性行为，这对于理解非交替多自由基共轭烃的物理学具有重要意义。

Abstract: I report the theoretical prediction of non-trivial physics in a one
dimensional multiradical system consisting in fused six and five membered
${\pi}$-conjugated carbon rings, known as oligo(indenoindene) (OInIn).
Topologically protected electronic states may emerge in fermionic chains if
there is an alternation in the coupling of adjacent unpaired electrons, being
described effectively by the Su-Schrieffer-Heeger (SSH) model. Class-II OInIn
isomers act as tight-binding chains in the non-interacting regime, thus we can
expect the emergence of SSH physics in an OInIn produced by the combination of
two isomers that belong to this class. That is the case of the system studied
in this manuscript, whose calculated non-interacting band structure shows a gap
opening compared to the gapless pure isomeric forms, hosting ingap localized
states at the chain termini depending on the termination, and a non-zero Zak
phase that confirms the non-trivial topology. These results were consistent
with spin unrestricted mean-field Hubbard and density functional theory
calculations, showing antiferromagnetic unquenched local magnetic moments at
the pentagons, and strong edge localization depending on the termination. This
work advances in the understanding of the physics of non-alternant multiradical
${\pi}$-conjugated hydrocarbons.

</details>


### [347] [Emergent Rashba spin-orbit coupling in bulk gold with buried network of nanoscale interfaces](https://arxiv.org/abs/2509.03620)
*Shreya Kumbhakar,Banashree Debnath,Tuhin Kumar Maji,Binita Tongbram,Shinjan Mandal,T. Phanindra Sai,T. V. Ramakrishnan,Manish Jain,H. R. Krishnamurthy,Anshu Pandey,Arindam Ghosh*

Main category: cond-mat.mes-hall

TL;DR: 通过在金中嵌入银纳米颗粒，在保持体材料反演对称性的前提下，实现了前所未有的Rashba效应。


<details>
  <summary>Details</summary>
Motivation: 在保持体材料反演对称性的前提下（例如在块状金属中），实现并调控Rashba效应，这在以前是无法实现的。

Method: 将超小银纳米颗粒嵌入块状金中，利用银和金近乎相同的晶格常数，在保持全局反演对称性的前提下，形成致密的Ag/Au异质界面。通过改变嵌入纳米颗粒的密度，调控Rashba效应。

Result: 在保持全局反演对称性的块状金属中，实现了~15 meV.Angstrom的Rashba效应，其耦合强度高于任何已知保持反演对称性的体系，并且自旋-轨道散射率增加了20倍。

Conclusion: 该研究提出了一种在新颖的异质结构中实现Rashba效应的策略，并通过结合电荷转移和极化子局域化来增强自旋-轨道相互作用。

Abstract: The Rashba effect, which plays a crucial role in fundamental materials
physics and potential spintronics applications, has been engineered in diverse
systems, including semiconductor quantum wells, oxide heterostructures,
metallic surfaces, topological insulators, ferroelectrics, etc. However,
generating it in systems that preserve bulk inversion symmetry (BIS), for
example, in bulk metals, has not been possible so far. We demonstrate a unique
strategy to introduce and tune Rashba spin-orbit interaction (SOI) to
unprecedented magnitudes in inversion-symmetric solids, by incorporating
ultra-small silver nanoparticles in bulk gold. The near-identical lattice
constants of Ag and Au allowed dense packing of the Ag/Au hetero-interfaces
without compromising the global BIS. By varying the density of embedded
nanoparticles, we generate Rashba SOI in a bulk metal with a coupling strength
of ~15 meV.Angstrom, higher than any known system preserving BIS globally, and
up to ~20 times increase in the spin-orbit scattering rate. We argue that the
combined effect of charge-transfer at the interfaces and polaronic localization
enhances the SOI.

</details>


### [348] [A One-Particle Density Matrix Framework for Mode-Shell Correspondence: Characterizing Topology in Higher-Order Topological Insulators](https://arxiv.org/abs/2509.03632)
*Miguel F. Martínez,Lucien Jezequel,Jens H. Bardarson,Thomas Klein Kvorning,Julia D. Hannukainen*

Main category: cond-mat.mes-hall

TL;DR: 本研究提出一种直接从单粒子密度矩阵表征高阶拓扑相的框架，无需哈密顿量。


<details>
  <summary>Details</summary>
Motivation: 需要一种不依赖于哈密顿量，直接从量子态（单粒子密度矩阵）表征高阶拓扑相的方法。

Method: 扩展了模式-壳层对应关系，将其应用于具有手征约束的高斯态，并使用壳层指数来量化体态拓扑，作为体-边界诊断。该方法被应用于具有$C_4$对称性的高阶拓扑绝缘体，并引入了结构无序。

Result: 在具有$C_4$对称性的高阶拓扑绝缘体中，证明了分数壳层指数意味着该相是内在的。研究表明，模式-壳层对应关系在无定形极限下仍然是有意义的诊断。该方法可推广到具有能隙体谱的相互作用态。

Conclusion: 模式-壳层对应关系提供了一种从量子态本身表征高阶拓扑的实用且多样化的途径。

Abstract: We present a framework for characterizing higher-order topological phases
directly from the one-particle density matrix, without any reference to an
underlying Hamiltonian. Our approach extends the mode-shell correspondence,
originally formulated for single-particle Hamiltonians, to Gaussian states
subject to chiral constraints. In this correspondence, the mode index counts
topological boundary modes, while the shell index quantifies the bulk topology
in a surrounding region, providing a bulk-boundary diagnostic. In
one-dimensional topological insulators, the shell index reduces to the local
chiral marker, recovering the winding number in the translation-invariant
limit. We apply the mode-shell correspondence to a $C_4$-symmetric higher-order
topological insulator with a chiral constraint and show that a fractional shell
index implies that the higher-order phase is intrinsic. The one-particle
density matrix is formulated in real space, so the mode-shell correspondence
applies to models without translation invariance. By introducing structural
disorder into the $C_4$-symmetric higher-order insulator, we show that the
mode-shell correspondence remains a meaningful diagnostic in the amorphous
limit. The mode-shell correspondence generalizes to interacting states with a
gapped bulk spectrum in the one-particle density matrix, providing a practical
and diverse route to characterize higher-order topology from the quantum state
itself.

</details>


### [349] [Double quantum dots with quenched charging energy in PbTe nanowires](https://arxiv.org/abs/2509.03706)
*Seth Byard,Maksim Gomanko,Adam Raynolds,Susheng Tan,Tongxie Zhang,Shixiong Zhang,Sergey M. Frolov*

Main category: cond-mat.mes-hall

TL;DR: 该研究在PbTe纳米线器件中通过静电栅极定义了双量子点，并通过输运测量获得了电荷稳定性图谱。


<details>
  <summary>Details</summary>
Motivation: 在半导体PbTe纳米线器件中研究双量子点，并朝着实现基于PbTe的自旋量子比特迈进。

Method: 通过输运测量获得电荷稳定性图谱，并研究了在外加磁场下的四重分裂和高偏压三角中的窄输运共振。

Result: 观察到电荷稳定性图谱具有配对三重点之间可忽略的分离以及零磁场下所有输运共振的自旋简并性。在高偏压下，观察到稳定性图谱三角形的四重分裂，证明了自旋简并性的提升。同时，也识别了高偏压三角形中窄输运共振的模式。

Conclusion: 实验结果展示了PbTe纳米线中的双量子点行为，并为实现基于PbTe的自旋量子比特提供了进展。

Abstract: We investigate double quantum dots defined by electrostatic gating in
semiconductor PbTe nanowire devices. We perform transport measurements to
obtain charge stability diagrams distinguished by negligible separation between
paired triple points and by the spin degeneracy of all transport resonances at
zero magnetic field. We show a fourfold splitting of high-bias stability
diagram triangles in an applied magnetic field to illustrate the lifting of
this spin degeneracy. We also identify patterns of narrow transport resonances
in these high-bias triangles and discuss their possible physical origins. Our
results represent a step towards the realization of PbTe-based spin qubits.

</details>


### [350] [Plasmons in a network of topological states in twisted bilayer graphene](https://arxiv.org/abs/2509.03781)
*Brian S. Vermilyea,Michael M. Fogler*

Main category: cond-mat.mes-hall

TL;DR: Bilayer graphene with domain walls hosts quasi-periodic, damped plasmons, with network model validated against RPA and simulated for nano-imaging.


<details>
  <summary>Details</summary>
Motivation: Investigating surface plasmons in a specific graphene structure (minimally-twisted bilayer graphene with domain walls) to understand their unique properties.

Method: Utilizing a network-based formalism with classical equations of motion and impedance boundary conditions to calculate plasmon dispersion, and comparing it with the random phase approximation. Also, simulating plasmon waves for nano-imaging.

Result: The plasmon band structure is quasi-periodic in frequency and damped everywhere except at high-symmetry points. The network model's validity is discussed relative to the random phase approximation. Simulations for terahertz nano-imaging experiments are presented.

Conclusion: The study presents a novel network-based approach to understand plasmon behavior in a complex graphene system, offering insights for experimental applications like nano-imaging.

Abstract: We study surface plasmons in minimally-twisted bilayer graphene that contains
a triangular network of partial dislocations (or AB-BA domain walls) hosting
one-dimensional electronic states. We calculate plasmon dispersion by solving
classical equations of motion for charge dynamics on the network links with
impedance boundary conditions at the network nodes. The plasmon band structure
is shown to be quasi-periodic in frequency and damped everywhere except at
high-symmetry points of the moir\'e Brillouin zone. We compare our
network-based formalism with the conventional random phase approximation and
discuss when each approach is valid. Calculations of plasmon waves launched by
local scatterers are presented to simulate terahertz nano-imaging experiments.

</details>


### [351] [Spin Splitting Nernst Effect in Altermagnet](https://arxiv.org/abs/2509.03822)
*Xing-Jian Yi,Yue Mao,Xiancong Lu,Qing-Feng Sun*

Main category: cond-mat.mes-hall

TL;DR: Altermagnet 具有自旋分裂能带但净磁矩为零，我们提出存在自旋分裂 Nernst 效应。


<details>
  <summary>Details</summary>
Motivation: 提出 Altermagnet 存在自旋分裂 Nernst 效应，即在纵向温度梯度下，电子倾向于在横向方向上相反地分裂，从而产生横向自旋流。

Method: 使用非平衡格林函数方法，计算了四端 Altermagnet 器件中的自旋相关透射系数。

Result: 计算得到非零的横向自旋流，并验证了自旋分裂 Nernst 效应。研究了参数依赖性，发现该效应可由费米面能量、温度、传输方向和系统尺寸调节。

Conclusion: Altermagnet 中的自旋分裂 Nernst 效应的 $xy$ 响应和 $yx$ 响应系数相等，且不需要自旋轨道耦合或净磁性。

Abstract: Altermagnet is a distinctive magnet phase, which has spin-split energy band
but with zero net magnetic moment. In this paper, we propose that altermagnet
behaves spin splitting Nernst effect: Under a longitudinal temperature
gradient, the electrons with opposite spins tend to split oppositely in the
transverse direction, thus generating a transverse spin current. The spin
splitting Nernst effect is understood from the contribution of the longitudinal
wave vector to the transverse group velocity. Using the nonequilibrium Green's
function method, we calculate the spin-dependent transmission coefficient in
the four-terminal altermagnet device. From the spin-dependent transmission
coefficient, the nonzero transverse spin current from longitudinal temperature
gradient is obtained, and the spin splitting Nernst effect is verified. We
systematically study the parameter dependence of the spin splitting Nernst
effect, while also performing symmetry analysis. The spin splitting Nernst
effect can be easily regulated by Fermi surface energy, temperature, transport
direction, and system size. Furthermore, in altermagnet, the $xy$-response and
$yx$-response spin splitting Nernst coefficients are equal with
$N_{s,xy}=N_{s,yx}$, different from the conventional spin Nernst effect where
they are opposite. Meanwhile, the spin splitting Nernst effect require neither
spin-orbit coupling nor net magnetism.

</details>


### [352] [Atomic collapse of high-order singular potentials in graphene](https://arxiv.org/abs/2509.03921)
*Yu-Chen Zhuang,Yue Mao,Qing-Feng Sun*

Main category: cond-mat.mes-hall

TL;DR: 石墨烯中的人工原子可以作为探索原子坍塌的平台，并用于设计新颖的石墨烯纳米器件。研究了无质量狄拉克费米子在1/r^γ形式的奇异势中的行为。与需要超临界电荷Z>Zc的库仑势不同，高阶奇异势（γ>1）原则上可以用无限小的电荷Z诱导原子坍塌。原子坍塌态（ACSs）的能量大致呈幂次序列排列。研究还发现，一些特殊的ACSs可以存在于体狄拉克点之上，这是库仑势中不存在的。这些发现揭示了无质量狄拉克费米子在不同电荷势中的异常行为，并为未来的实验和石墨烯纳米器件应用提供了指导。


<details>
  <summary>Details</summary>
Motivation: 探索原子坍塌在石墨烯中的可能性，并为设计新颖的石墨烯纳米器件提供理论基础。

Method: 理论研究了无质量狄拉克费米子在1/r^γ形式的奇异势中的行为。

Result: 高阶奇异势（γ>1）原则上可以用无限小的电荷Z诱导原子坍塌，其原子坍塌态（ACSs）的能量大致呈幂次序列排列。此外，发现了一些特殊的ACSs可以存在于体狄拉克点之上，这是库仑势中不存在的。

Conclusion: 无质量狄拉克费米子在不同电荷势中表现出异常行为，并且研究结果为未来的实验和石墨烯纳米器件应用提供了指导。

Abstract: Artificial atoms in graphene hosting a series of quasi-bound states can serve
as an excellent platform to explore atomic collapse and become a basis to
design novel graphene nanodevices. We theoretically study behaviors of massless
Dirac fermions in singular potentials with a general form of 1/r^{\gamma}.
Different from the Coulomb potential that demands a supercritical charge Z >
Zc, a high-order singular potential ({\gamma} > 1) is found to in principle
induce atomic collapse with an infinitesimal charge Z. The energies of atomic
collapse states (ACSs) within these potentials are arranged roughly as a power
sequence. We also show that some special ACSs can exist even above the bulk
Dirac point, which cannot appear in the Coulomb potential. These findings
uncover the anomalies of massless Dirac fermions in diverse charge potentials
and provide guidance for further experiments and graphene nanodevice
applications.

</details>


### [353] [Antiferromagnetic superlattices: anisotropic band and spin-valley valve in buckled two-dimensional materials](https://arxiv.org/abs/2509.03923)
*Wei-Tao Lu,Tie-Feng Fang,Qing-Feng Sun*

Main category: cond-mat.mes-hall

TL;DR: 该论文提出了一种基于反铁磁超晶格（AFSL）的方案，利用二维材料和反铁磁性材料的邻近效应，实现了谷极化和自旋极化。


<details>
  <summary>Details</summary>
Motivation: 提出一种利用反铁磁超晶格（AFSL）实现谷极化和自旋极化的新方法，并探讨其在二维材料中的应用。

Method: 通过理论计算和模拟，研究了反铁磁邻近效应对二维材料能带结构、电导率以及自旋-谷特性的影响，并考虑了自旋-轨道耦合（SOC）和空间反演对称性。

Result: 发现在反铁磁超晶格中，反铁磁邻近效应能够产生谷极化的能带和电导，且能同时打破自旋简并和谷简并，实现完全的自旋-谷极化。同时，发现反铁磁超晶格具有高度各向异性的能带结构，该各向异性可以通过调整势能和SOC进行调控。

Conclusion: 提出的反铁磁超晶格为设计具有特定电子特性的二维材料提供了一种新途径，并有望应用于构建易于门控的自旋-谷阀。

Abstract: Antiferromagnetic superlattices (AFSL) are proposed based on the buckled
hexagonal two-dimensional materials, which can be realized by the proximity
effect of the periodically deposited antiferromagnets. It is found that the AF
proximity effect can give rise to valley-polarized minibands and conductance,
which are not held under ferromagnetic proximity. The spin degeneracy and
valley degeneracy are lifted simultaneously in the presence of AF proximity and
electric field. In consequence, both minibands and conductance could be
spin-valley polarized completely in AFSL. The symmetry of spin-valley
polarization is analysed by considering the pseudospin rotation operations and
spatial inversion operations. Furthermore, AFSL also induce a highly
anisotropic band structure due to the spin-orbit coupling (SOC). In particular,
the group velocity parallel to the periodic direction of AFSL is greatly
renormalized, while the velocity perpendicular to the periodic direction
remains unaffected, contrary to that observed in graphene superlattices. With
the increase of SOC, the anisotropy becomes more prominent, leading to
flattened band and electron supercollimation. The direction of anisotropy can
be regulated by adjusting the potential and SOC. These findings offer an
alternative approach to engineering anisotropic two-dimensional materials. As
an application, the AFSL may well work as a symmetry-protected spin-valley
valve easily controlled by the gate voltages.

</details>


### [354] [Topologically protected magnetoresistance by quantum anomalous Hall effect](https://arxiv.org/abs/2509.03929)
*Wei-Tao Lu,Qing-Feng Sun*

Main category: cond-mat.mes-hall

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 鉴于反铁磁材料在下一代自旋电子学中取代铁磁材料的潜力，本研究提出了一个基于反铁磁系统中量子反常霍尔效应的磁阻模型。

Method: 提出一个受拓扑陈数保护的磁阻模型，该模型基于反铁磁系统中的量子反常霍尔效应。通过调控反铁磁交换场和电场，可以在量子自旋霍尔绝缘体（QSHI）相和量子反常霍尔绝缘体（QAHI）相之间切换，从而形成QAHI/QSHI/QAHI结。在QAHI区域，可以通过调控反铁磁交换场和电场来调控手征边缘态的自旋方向，实现平行和反平行自旋构型。

Result: 通过调控反铁磁交换场和电场，可以在QAHI区域实现平行和反平行的自旋构型。这两种构型的电导存在显著差异，从而产生可由电场控制的磁阻效应。由于拓扑不变量的存在，该磁阻平台对尺寸效应和无序具有鲁棒性。

Conclusion: 本研究提出的基于反铁磁系统量子反常霍尔效应的磁阻模型，实现了可由电场控制的磁阻效应，并且该效应具有良好的鲁棒性，有望应用于下一代自旋电子器件。

Abstract: Recently, antiferromagnetic (AF) materials have attracted rapid attention,
because they are considered as outstanding candidates to replace the widely
used ferromagnets in the next generation of spintronics. We propose a
magnetoresistance model based on the quantum anomalous Hall effect in an AF
system, which is protected by the topological Chern number. By regulating the
AF exchange field and an electric field, the system can be controlled between
the quantum spin Hall insulator (QSHI) phases and the quantum anomalous Hall
insulator (QAHI) phases. As a result, a QAHI/QSHI/QAHI junction can be formed.
In the QAHI region, the spin orientation of chiral edge state can be
manipulated by tuning the AF exchange field and the electric field. Therefore,
the spin directions of two QAHIs in the junction can have parallel and
antiparallel configurations. The conductances of two configurations offered by
chiral edge states are significantly different, and this is a magnetoresistance
effect that can be electrically controlled. Because of the topological
invariance, the magnetoresistance plateaus are robust to the size effect and
the disorder.

</details>


### [355] [Thickness-dependent magnon spin transport in antiferromagnetic insulators: Crossover from quasi-three-dimensional to quasi-two-dimensional regimes](https://arxiv.org/abs/2509.03941)
*Mathias Åsan Myhre,Verena Brehm,Thomas Delvaux,Arne Brataas,Alireza Qaiumzadeh*

Main category: cond-mat.mes-hall

TL;DR: 研究了铁磁绝缘体和反铁磁绝缘体中厚度相关的自旋输运。在反铁磁绝缘体（特别是赤铁矿）中，发现自旋输运从准三维过渡到准二维，并在临界厚度以下观察到自旋扩散长度的显著增强，这归因于有效自旋态密度和散射相空间的变化。


<details>
  <summary>Details</summary>
Motivation: 受近期在超薄铁磁绝缘体中室温巨磁导率的观察的启发，研究超薄反铁磁绝缘体（AFI）中厚度依赖的磁子自旋输运。

Method: 使用随机微磁模拟研究了两种磁相（低温单轴易轴相和高温双轴易平面相）下赤铁矿的厚度依赖性磁子自旋输运。

Result: 发现自旋输运在临界厚度时从准三维（3D）转变为准二维（2D）。在临界厚度以下，观察到磁子扩散长度的显著增加，这是由于有效磁子态密度和散射相空间的变化。

Conclusion: 理解和控制AFI中长距离磁子自旋输运对于开发下一代自旋电子纳米器件至关重要，尤其是在材料接近二维极限时。

Abstract: Motivated by the recent observation of giant room-temperature magnon spin
conductivity in an ultrathin ferromagnetic insulator [X.-Y. Wei et al., Nat.
Mater. 21, 1352 (2022)], we investigate thickness-dependent magnon spin
transport in thin antiferromagnetic insulators (AFIs). We study the
prototypical AFI hematite, known for its exceptionally low magnetic damping and
two distinct magnetic phases: a low-temperature uniaxial easy-axis phase and a
high-temperature biaxial easy-plane phase. Using stochastic micromagnetic
simulations, we investigate thickness-dependent magnon spin transport across
both magnetic phases. Our results uncover a crossover from
quasi-three-dimensional to quasi-two-dimensional magnon spin transport at a
critical thickness, determined by the frequency or energy of the excited
magnons. Below this critical thickness, we observe a pronounced enhancement in
the magnon diffusion length in both magnetic phases. This rise is attributed to
a change in the effective magnon density of states, reflecting the reduced
phase space available for scattering in the thinner, quasi-two-dimensional
regime. Understanding and controlling long-distance magnon spin transport in
AFIs is crucial for developing next-generation spintronic nanodevices,
especially as materials approach the two-dimensional limit.

</details>


### [356] [Unoccupied bands in the molybdenum dichalcogenides MoS$_2$, MoSe$_2$, and MoTe$_2$](https://arxiv.org/abs/2509.04411)
*J. Jobst,E. E. Krasovskii,R. Ribeiro,T. A. de Jong,C. R. Dean,R. M. Tromp,S. J. van der Molen*

Main category: cond-mat.mes-hall

TL;DR: 该论文展示了三种基于钼的过渡金属二硫属化物（TMD）的角分辨反射电子能谱（ARRES）数据，并与理论预测进行了比较，特别关注了未占据的能带结构和散射态密度。


<details>
  <summary>Details</summary>
Motivation: 研究三种基于钼的过渡金属二硫属化物（TMD）的未占据能带结构和散射态密度，并与理论预测进行比较，以深入了解S、Se、Te系列的变化。

Method: 使用角分辨反射电子能谱（ARRES）测量了单层和块状TMD的IV光谱，并将实验数据与理论预测进行比较。

Result: 实验数据与理论预测（特别是较低能量下）具有良好的一致性，并观察到了一系列层间共振，其杂化效应随层数的增加而变化，并且这些共振主要由硫属原子未占据的d态主导。

Conclusion: 未占据态在需要电子暂时停留于真空能级以上的过程中起着关键作用，例如在光发射和二次电子发射实验中。

Abstract: We present angle-resolved reflected electron spectroscopy (ARRES) data for
the three molybdenum-based transition metal dichalcogenides (TMDs) \mos, \mose,
and \mote. To follow the changes as the series moves from S to Se to Te in more
detail, we determine accurate IV-spectra for monolayers and bulk TMDs. These
experimental data sets are then compared with theoretical predictions for both
the unoccupied band structure and the scattering density of states. We find
good agreement, especially for lower energies where inelastic effects are
relatively unimportant. Furthermore, we identify a series of interlayer
resonances for which the dependence of the hybridization effects on the layer
count is observed. Although these resonances bear similarity to interlayer
resonances in hBN and graphene, they differ in their character, being dominated
by unoccupied $d$-states of the chalcogen-atoms. The unoccupied states studied
and analyzed here play a key role in all processes that require an electron to
temporarily reside in a state above the vacuum level, such as in photoemission
and secondary electron emission experiments.

</details>


### [357] [Two-dimensional Dirac semimetals with tunable edge states](https://arxiv.org/abs/2509.03943)
*Lizhou Liu,Cheng-Ming Miao,Qing-Feng Sun,Ying-Tao Zhang*

Main category: cond-mat.mes-hall

TL;DR: 通过修改BHZ模型设计了二维狄拉克半金属，在费米能级工程化了平坦谱带，并通过层间耦合分离出两个狄拉克点，由一维费米弧边界态连接，其位置可通过调节层间耦合强度和对称性进行精确调谐。


<details>
  <summary>Details</summary>
Motivation: 理论上设计一种利用双层改性BHZ模型实现二维狄拉克半金属的方法。

Method: 通过引入新位点到BHZ模型来设计平坦谱带，并利用层间耦合分离出两个狄拉克点，形成由一维费米弧边界态连接的拓扑相。

Result: 实现了具有时间反转和空间反演对称性的两个狄拉克点，并通过量化透射共振峰证实了一维费米弧边界态的束缚性质。

Conclusion: 所提出的设计能够精确调谐狄拉克点的精确位置，通过调整层间耦合强度和对称性实现，为二维狄拉克半金属的设计提供了新的理论途径。

Abstract: We theoretically propose a design for two-dimensional Dirac semimetals using
a bilayer-modified Bernevig-Hughes-Zhang (BHZ) model. By introducing new sites
into the BHZ model, we engineer flat bands at the Fermi energy. In the bilayer
system, interlayer coupling separates these flat bands, resulting in two Dirac
points that preserve time-reversal and inversion symmetries. Two Dirac points
are connected by a one-dimensional Fermi arc edge state, whose bound nature is
confirmed by quantized transmission resonance peaks. Notably, the position of
the Dirac points can be precisely tuned by adjusting interlayer coupling
strengths and symmetries.

</details>


### [358] [Two-Dimensional Higher-Order Topological Metals](https://arxiv.org/abs/2509.03944)
*Lizhou Liu,Cheng-Ming Miao,Qing-Feng Sun,Ying-Tao Zhang*

Main category: cond-mat.mes-hall

TL;DR: 石墨烯中的 the staggered intrinsic spin-orbit coupling 和 in-plane Zeeman fields 导致了 topological metallic phases 和 higher-order topological metals.


<details>
  <summary>Details</summary>
Motivation: 研究石墨烯中 staggered intrinsic spin-orbit coupling 和 in-plane Zeeman fields 引起的能带结构和能级.

Method: 通过理论分析，研究了 staggered intrinsic spin-orbit coupling 和 in-plane Zeeman fields 对石墨烯能带结构的影响，并利用量化输运系数和 continuum low-energy model 进行了验证。

Result: Staggered intrinsic spin-orbit coupling 引起了 bulk band crossover 和 antihelical edge states，形成了 topological metallic phases。In-plane Zeeman field 使得 antihelical edge states 的能隙打开，形成了 higher-order topological metals with corner states。

Conclusion: Staggered intrinsic spin-orbit coupling 和 in-plane Zeeman fields 在石墨烯中产生了新颖的拓扑相，包括 topological metallic phases 和 higher-order topological metals with corner states。

Abstract: We investigate the energy band structure and energy levels of graphene with
staggered intrinsic spin-orbit coupling and in-plane Zeeman fields. Our study
demonstrates that staggered intrinsic spin-orbit coupling induces bulk band
crossover at the the \( K \) and \( K' \) valleys and generates antihelical
edge states at the zigzag boundaries, resulting in topological metallic phases.
Quantized transport coefficients confirm the existence of these antihelical
edge states. Furthermore, an in-plane Zeeman field, regardless of orientation,
opens a gap in the antihelical edge states while preserving bulk band closure,
leading to higher-order topological metals with corner states. We also validate
the presence of these corner states in nanoflakes with zigzag boundaries and
confirm the metallic phases with crossed bands through a continuum low-energy
model analysis.

</details>


### [359] [Altermagnetism-Induced Parity Anomaly in Weak Topological Insulators](https://arxiv.org/abs/2509.03963)
*Yu-Hao Wan,Qing-Feng Sun*

Main category: cond-mat.mes-hall

TL;DR: 表面超扭曲绝缘体上的阿尔特磁性导致了单一无质量狄拉克费米子的出现，该费米子表现出奇偶异常。


<details>
  <summary>Details</summary>
Motivation: 探索由奇偶异常引起的输运性质，并研究表面阿尔特磁性对弱拓扑绝缘体输运性质的影响。

Method: 提出一个有效的二维（2D）晶格模型来描述弱拓扑绝缘体表面，并使用该模型捕捉能量谱和自旋纹理。进行层分辨计算，以验证二维模型的计算结果。

Result: 模型捕捉了弱拓扑绝缘体表面的能量谱和自旋纹理。在阿尔特磁性的影响下，弱拓扑绝缘体表面会产生半整数的边缘电流。在退相干的情况下，霍尔电导率达到半量子化值。层分辨计算证实了表面阿尔特磁性驱动表面霍尔电导率向 $e^{2}/2h$ 转变。

Conclusion: 研究将阿尔特磁性与量子异常联系起来，并表明弱拓扑绝缘体是研究奇偶异常的潜在平台，且无需净磁矩。

Abstract: We demonstrate that introducing altermagnetism on the surface of a weak
topological insulator (TI) results in the emergence of a single massless Dirac
fermion, exhibiting a parity anomaly. To explore the transport properties
induced by this parity anomaly, we propose an effective two-dimensional (2D)
lattice model to describe the weak TI surface. This model captures both the
energy spectrum and spin texture of the weak TI surface while reducing
computational complexity. We show that the weak TI surface hosts a half-integer
chiral edge current under the influence of altermagnetism. Additionally, in the
presence of decoherence, the Hall conductance attains a half-quantized value.
Layer-resolved calculations from a 3D slab model further confirm that surface
altermagnetism drives the surface Hall conductance to transition to $e^{2}/2h$,
aligning with calculation from the 2D effective lattice model. Our findings
establish a link between altermagnetism and quantum anomalies, positioning weak
TIs as a potential platform for investigating the parity anomaly without a net
magnetic moment.

</details>


### [360] [Interplay of Altermagnetic Order and Wilson Mass in the Dirac Equation: Helical Edge States without Time-Reversal Symmetry](https://arxiv.org/abs/2509.03969)
*Yu-Hao Wan,Peng-Yi Liu,Qing-Feng Sun*

Main category: cond-mat.mes-hall

TL;DR: 研究三维拓扑绝缘体(3DTI)薄膜与非相对论磁性(AM)序的界面中的拓扑相。


<details>
  <summary>Details</summary>
Motivation: 研究3DTI薄膜与AM序的耦合如何改变能带拓扑和边界模式，特别是诱导拓扑相变和螺旋边缘态的出现。

Method: 从修改后的狄拉克方程出发，阐明了晶格正则化产生的威尔逊质量与非相对论磁性质量之间的相互作用。

Result: 发现3DTI薄膜与AM序的耦合会诱导拓扑相变，导致出现拓扑螺旋边缘态，该态与量子反常霍尔相和常规量子自旋霍尔相的边缘态不同。量子输运模拟显示了与这些螺旋边缘态相关的鲁棒的、量化的非局域电阻平台。

Conclusion: 3DTI/AM异质结构是工程和探测无时间反转对称性的螺旋拓扑边缘输运的可行材料平台，拓展了拓扑物质的范围，并为量子器件提供了新机会。

Abstract: We investigate topological phases in three-dimensional topological insulator
(3DTI) thin films interfaced with altermagnetic (AM) orders. Starting from a
modified Dirac equation, we elucidate the interplay between the Wilson mass,
arising from lattice regularization, and the altermagnetic mass, and show how
this interplay fundamentally alters the band topology and boundary modes. In
particular, we demonstrate that coupling a 3DTI thin film to AM order induces a
topological phase transition: although the total Chern number remains zero
across the transition, topological helical edge states emerge after the
transition. These helical edge states arise from opposite Chern numbers at
different high-symmetry points, and are distinct from both the chiral edge
states of the quantum anomalous Hall phase and the helical edge states of the
conventional quantum spin Hall states. The quantum transport simulations reveal
robust, quantized nonlocal resistance plateaus associated with these helical
edge states, which persist even under strong potential and magnetic disorder.
Our results establish 3DTI/AM heterostructures as a feasible material platform
for engineering and detecting helical topological edge transport without
time-reversal symmetry, thus expanding the landscape of topological matter and
providing new opportunities for quantum devices.

</details>


### [361] [Band bending and zero-conductance resonances controlled by edge electric fields in zigzag silicene nanoribbons](https://arxiv.org/abs/2509.03991)
*Wei-Tao Lu,Qing-Feng Sun,Hong-Yu Tian,Ben-Hu Zhou,Hong-Mei Liu*

Main category: cond-mat.mes-hall

TL;DR: The paper studies the effects of electric fields on zigzag silicene nanoribbons, finding controllable band bending and a valley-polarized quantum spin Hall state. It also observes Fano resonances and robustness against Hubbard interaction, leading to various quantum phases.


<details>
  <summary>Details</summary>
Motivation: To investigate the influence of electric fields applied to the edges of a zigzag silicene nanoribbon on its band structure and transport properties.

Method: Applied antisymmetric and symmetric electric fields to the edges of a zigzag silicene nanoribbon and analyzed the resulting band bending, band gap, and transport resonances using wave function analysis and considering Hubbard interaction.

Result: Antisymmetric fields induce controllable band bending, with the highest valence band and lowest conduction band coexisting. Symmetric fields create an asymmetric band gap at Dirac points, suggesting a valley-polarized quantum spin Hall state. Fano resonances and zero-conductance resonances are observed due to band bending, band selective rule, and resonant states. These effects are robust against Hubbard interaction, which can also lead to spin-dependent band gaps and various quantum phases like metal and half-metal.

Conclusion: Electric fields applied to the edges of zigzag silicene nanoribbons can significantly modify their electronic properties, leading to tunable band bending, quantum spin Hall states, and Fano resonances. The Hubbard interaction further enriches these properties, enabling diverse quantum phases. The observed phenomena are robust against this interaction.

Abstract: We study the band structure and transport property of a zigzag silicene
nanoribbon when the electric fields are applied to the edges. It is found that
a band bending could be induced and controlled by the antisymmetric edge
fields, which can be understood based on the wave functions of the edge states.
The highest valence band and the lowest conduction band coexist in the band
bending region. With the narrowing of edge potentials, the bending increases
gradually. When the edge fields become symmetric, an asymmetric band gap at the
Dirac points can be obtained due to the intrinsic spin-orbit interaction,
suggesting a valley polarized quantum spin Hall state. The gap could reach a
maximum value rapidly and then decrease slowly as the electric fields increase.
Due to the combining effect of the band bending, band selective rule, and
resonant states, many zero-conductance resonances and resonance peaks appear in
different regions, which could be described by the Fano resonance effect.
Furthermore, the band bending and zero-conductance resonances are robust
against the Hubbard interaction. The Hubbard interaction could work as a
spin-dependent edge field, together with the edge electric fields, leading to a
spin-dependent band gap and various quantum phases such as metal and
half-metal.

</details>


### [362] [Phase transitions in quantum dot-Majorana zero mode coupling systems](https://arxiv.org/abs/2509.04002)
*Yue Mao,Qing-Feng Sun*

Main category: cond-mat.mes-hall

TL;DR: 量子点-马约拉纳零模耦合系统的基态相变


<details>
  <summary>Details</summary>
Motivation: 研究量子点-马约拉纳零模耦合系统的基态相变，类比于量子点-超导体耦合系统。

Method: 通过改变量子点内部能级和量子点-马约拉纳零模耦合强度来研究基态相变，并获得了相图（考虑有无泽曼项）。同时研究了自旋性质和态密度随相变的变化，并用平均场理论解释相变性质。

Result: 获得了不同条件下（有无泽曼项）的基态相图，并研究了自旋性质和态密度的变化。

Conclusion: 该研究不仅是量子点-超导体相变的模拟，也为理解马约拉纳零模相关实验提供了新的解释。

Abstract: The magnetic doublet ground state (GS) of a quantum dot (QD) could be changed
to a spin-singlet GS by coupling to a superconductor. In analogy, here we study
the GS phase transitions in QD-Majorana zero mode (MZM) coupling systems: GS
behaves phase transition versus intra-dot energy level and QD-MZM coupling
strength. The phase diagrams of GS are obtained, for cases with and without
Zeeman term. Along with the phase transition, we also study the change of spin
feature and density of states. The properties of the phase transition are
understood via a mean-field picture. Our study not only serves as an analogue
to QD-superconductor phase transitions, but also gives alternative explanations
on MZM-relevant experiments.

</details>


### [363] [Electrical control of crossed Andreev reflection and spin-valley switch in antiferromagnet/superconductor junctions](https://arxiv.org/abs/2509.04003)
*Wei-Tao Lu,Qing-Feng Sun*

Main category: cond-mat.mes-hall

TL;DR: 通过调控电场，在AF/S和AF/S/AF结的蜂窝体系中实现了纯交叉安德烈型反射（CAR），并实现了CAR和弹性隧道效应（EC）之间的自旋-谷开关效应。


<details>
  <summary>Details</summary>
Motivation: 研究电场调控下的反铁磁/超导体（AF/S）和反铁磁/超导体/反铁磁（AF/S/AF）结中的亚带隙输运，旨在利用蜂窝体系（如硅烯、锗烯和锡烯）的特性生成纯交叉安德烈型反射（CAR）。

Method: 在存在电场和反铁磁交换场的情况下，利用自旋-轨道耦合在蜂窝体系中实现自旋-谷极化的半金属相，进而生成纯CAR。

Result: 发现可以在较宽的电场范围内，在没有局域安德烈型反射（AR）和弹性隧道效应（EC）的情况下生成纯CAR。通过调节电场，可以在纯CAR和纯EC之间实现自旋-谷开关效应。AR和CAR过程的性质强烈依赖于自旋-谷极化态。

Conclusion: 该研究提出的器件可以实现CAR过程和自旋-谷开关效应的电学测量。

Abstract: We study the subgap transport through the antiferromagnet/superconductor
(AF/S) and antiferromagnet/superconductor/antiferromagnet (AF/S/AF) junctions
controlled by electric field in a generic buckled honeycomb system, such as
silicene, germanene, and stanene. In the present of electric field and
antiferromagnetic exchange field, the spin-valley polarized half metallic phase
can be achieved in the honeycomb system due to the spin-orbit coupling, which
affords an opportunity to generate the pure crossed Andreev reflection (CAR).
It is found that the pure CAR can be generated without local Andreev reflection
(AR) and elastic cotunneling (EC) over a wide range of electric field. A
spin-valley switch effect can be realized between the pure CAR and the pure EC
by adjusting the electric field. The properties of AR process and CAR process
strongly depend on the spin-valley polarized states. Our results suggest that
the device can implement an electrical measurement of the CAR process and
spin-valley switch.

</details>


### [364] [Orbital hybridization in graphene-based artificial atoms](https://arxiv.org/abs/2509.04012)
*Yue Mao,Hui-Ying Ren,Xiao-Feng Zhou,Hao Sheng,Yun-Hao Xiao,Yu-Chen Zhuang,Ya-Ning Ren,Lin He,Qing-Feng Sun*

Main category: cond-mat.mes-hall

TL;DR: 人工原子中的轨道杂化首次通过改变其形状得以实现，为设计新材料和精确控制量子态开辟了道路。


<details>
  <summary>Details</summary>
Motivation: 人工原子作为宏观物体的基本过程的固态模拟，已被广泛研究，但其轨道杂化现象的实验证据仍有待证明。

Method: 通过改变人工原子的形状，利用各向异性势能来实现准束缚态之间的轨道杂化，并通过实验、数值计算和理论推导进行可视化和验证。

Result: 成功实现了人工原子中的轨道杂化，并能直接在实空间中观察到杂化轨道，且与计算和理论推导结果高度吻合。

Conclusion: 首次在人工原子中实现了轨道杂化，并揭示了其在设计新材料和控制量子态方面的潜力。

Abstract: Intraatomic orbital hybridization and interatomic bond formation are the two
fundamental processes when real atoms are condensed to form matter. Artificial
atoms mimic real atoms by demonstrating discrete energy levels attributable to
quantum confinement. As such, they offer a solid-state analogue for simulating
intraatomic orbital hybridization and interatomic bond formation. Signatures of
interatomic bond formation has been extensively observed in various artificial
atoms. However, direct evidence of the intraatomic orbital hybridization in the
artificial atoms remains to be experimentally demonstrated. Here we, for the
first time, realize the orbital hybridization in artificial atoms by altering
the shape of the artificial atoms. The anisotropy of the confining potential
gives rise to the hybridization between quasibound states with different
orbital quantum numbers within the artificial atom. These hybridized orbits are
directly visualized in real space in our experiment and are well reproduced by
both numerical calculations and analytical derivations. Our study opens an
avenue for designing artificial matter that cannot be accessed on real atoms
through experiments. Moreover, the results obtained inspire the progressive
control of quantum states in diverse systems.

</details>


### [365] [Tunneling Magnetoresistance Effect in Altermagnets](https://arxiv.org/abs/2509.04015)
*Yu-Fei Sun,Yue Mao,Yu-Chen Zhuang,Qing-Feng Sun*

Main category: cond-mat.mes-hall

TL;DR: 提出了一种通用的超磁性器件，并利用非平衡格林函数方法和Landauer-Büttiker公式研究了其输运性质，实现了大于1000%的隧道磁阻效应。


<details>
  <summary>Details</summary>
Motivation: 超磁性是一种非常规磁性，为自旋电子学提供了一个新的研究平台。隧道磁阻（TMR）效应是自旋电子学的一个重要研究内容。

Method: 利用非平衡格林函数（NEGF）方法和Landauer-Büttiker公式，研究了超磁性夹层器件的输运性质，获得了电导和TMR比。通过系统旋转超磁体和自旋的方向，研究了超磁体取向对电导和TMR比的影响。

Result: 通过调控超磁性强度、费米能以及旋转超磁体取向，TMR比可达1000%以上。分析了系统内电导和TMR比的详细对称性关系。

Conclusion: 所提出的超磁性器件为基于超磁性平台的下一代信息技术提供了新的设计理念，为自旋电子学应用的发展铺平了道路。

Abstract: As an unconventional magnet, altermagnetism attracts great interest in
condensed matter physics and applies a new research platform for the
spintronics. Since the tunneling magnetoresistance (TMR) effect is an important
research aspect in spintronics, we theoretically propose a universal
altermagnetic sandwich device to achieve the TMR effect and investigate its
transport properties. Using the nonequilibrium Green's function method and the
Landauer-B\"uttiker formula, we obtain the conductance and the TMR ratio. By
systematically rotating the orientations of the altermagnet and spin, we
investigate how the altermagnetic orientations affect the conductance and the
TMR ratio, and comprehensively demonstrate the dependence of the conductance
and the TMR ratio on a range of parameters in the system. By tuning the
altermagnetism strength and the Fermi energy, as well as rotating the
orientations in the altermagnet, the TMR ratio can reach a value of over 1000%.
In addition, we analyze the detailed symmetry relations of the conductance and
the TMR ratio in our system. Our approach provides a new design concept for the
next-generation information technologies based on the altermagnetic platform,
paving the way for the development of spintronics applications.

</details>


### [366] [Spin-valley polarized edge states and quantum anomalous Hall states controlled by side potential in 2D honeycomb lattices](https://arxiv.org/abs/2509.04017)
*Wei-Tao Lu,Qing-Feng Sun,Yun-Fang Li,Hong-Yu Tian*

Main category: cond-mat.mes-hall

TL;DR: The study investigates the impact of side potential on the electronic properties of 2D honeycomb lattices (like silicene and germanene) with spin-orbit coupling. It reveals that side potential can significantly influence helical edge states, locking spin and valley properties. The research demonstrates various quantum effects (spin-valley Hall, valley-polarized quantum spin Hall, spin-polarized quantum anomalous Hall) by manipulating side potential and ribbon width. It also shows how side potential can open band gaps, break time-reversal symmetry, and create spin-polarized anomalous Hall phase, leading to diverse spin-valley polarized edge states and potential applications in spin-valley switches.


<details>
  <summary>Details</summary>
Motivation: 研究了侧向势场对具有内在自旋-轨道耦合的二维蜂窝状晶格（如硅烯和锗烯）的自旋和谷相关电子性质的影响。

Method: 基于紧束缚形式主义，研究了应用于锯齿形纳米带边界的势场和交换场。

Result: 发现侧向势场可以极大地影响具有不同自旋指数的螺旋边缘态，并且自旋和谷相互锁定。通过调节侧向势场和纳米带宽度，系统呈现出量子自旋-谷霍尔效应、谷极化量子自旋霍尔效应和自旋极化量子反常霍尔效应。由于侧向势场和窄带隙中边缘态的耦合，可以为特定的自旋打开带隙，破坏时间反转对称性，从而导致自旋极化量子反常霍尔相。在两个边界处形成了各种自旋-谷极化边缘态。

Conclusion: 侧向势场可以极大地影响具有不同自旋指数的螺旋边缘态，并且自旋和谷相互锁定。通过调节侧向势场和纳米带宽度，系统呈现出多种量子效应。侧向势场和窄带隙中边缘态的耦合可以打开带隙，破坏时间反转对称性，从而导致自旋极化量子反常霍尔相。各种自旋-谷极化边缘态在两个边界处形成。此外，自旋-谷极化绝缘态可用于实现完美的自旋-谷开关。

Abstract: Based on the tight-binding formalism, we study the effect of side potential
on the spin and valley related electronic property of $2$D honeycomb lattices
with intrinsic spin-orbit coupling, such as silicene and germanene. The side
potential is composed of potential field and exchange field applied on the
boundaries of the zigzag nanoribbon. It is found that the side potential could
greatly affect the helical edge states with different spin indices and the spin
and valley are locked to each other. By adjusting the side potential and ribbon
width, the system shows quantum spin-valley Hall effect, valley polarized
quantum spin Hall effect, and spin polarized quantum anomalous Hall effect. Due
to the side potential and the coupling of edge states in narrow ribbon, a band
gap could be opened for specific spin and the time-reversal symmetry could be
broken, leading to a spin polarized quantum anomalous Hall phase. Various kinds
of spin-valley polarized edge states are formed at the two boundaries.
Furthermore, the spin-valley polarized insulating states can be used to realize
a perfect spin-valley switch.

</details>


### [367] [Moiré spintronics: Emergent phenomena, material realization and machine learning accelerating discovery](https://arxiv.org/abs/2509.04045)
*Fengjun Zhuo,Zhenyu Dai,Hongxin Yang,Zhenxiang Cheng*

Main category: cond-mat.mes-hall

TL;DR: Twisted vdW materials, especially 2D magnetic materials, are a promising platform for spintronics due to emergent moiré phenomena like interlayer magnetism, non-collinear spin textures, moiré magnetic exchange interactions, moiré skyrmions, and moiré magnons. Machine learning can accelerate material discovery for this field. Challenges and opportunities remain.


<details>
  <summary>Details</summary>
Motivation: Twisted van der Waals (vdW) materials offer a promising platform for exploring exotic quantum phenomena and engineering novel material properties in two dimensions, with potential for revolutionary developments in spintronics, particularly focusing on two-dimensional magnetic materials.

Method: This review discusses recent theoretical and experimental studies on stacking-dependent interlayer magnetism, non-collinear spin textures, moiré magnetic exchange interactions, moiré skyrmions, and moiré magnons in twisted vdW materials. It also highlights the use of machine learning for accelerating the discovery and design of materials for moiré spintronics.

Result: The review covers recent progress in moiré spintronics in twisted vdW materials, focusing on various emergent phenomena and the application of machine learning.

Conclusion: The field of moiré spintronics in twisted vdW materials is rapidly expanding, presenting both pressing challenges and potential opportunities for future research and development.

Abstract: Twisted van der Waals (vdW) materials have emerged as a promising platform
for exploring the exotic quantum phenomena and engineering the novel material
properties in two dimensions, which could bring revolutionary developments in
spintronics. This Review aims at providing an overview of recent progress on
emerging moir\'e spintronics in twisted vdW materials, with a particular focus
on two-dimensional magnetic materials. After a brief introduction to the
general features of twisted vdW materials, we discuss recent theoretical and
experimental studies on stacking-dependent interlayer magnetism, non-collinear
spin textures, moir\'e magnetic exchange interactions, moir\'e skyrmions and
moir\'e magnons. We further highlight the ability to accelerate the discovery
and design multifunctional materials for moir\'e spintronics with the
assistance of machine learning. We conclude with the most pressing challenges
and potential opportunities in this rapidly expanding field.

</details>


### [368] [Two-dimensional magnetic tunnel p-n junctions for low-power electronics](https://arxiv.org/abs/2509.04206)
*Wenkai Zhu,Ziao Wang,Tiangui Hu,Zakhar R. Kudrynskyi,Tong Zhou,Zakhar D. Kovalyuk,Ce Hu,Hailong Lin,Xiaodong Li,Yongcheng Deng,Quanshan Lv,Lixia Zhao,Amalia Patane,Igor Zutic,Houzhi Zheng,Kaiyou Wang*

Main category: cond-mat.mes-hall

TL;DR: Report of a giant anomalous zero-bias spin voltage in magnetic tunnel junctions based on 2D materials, enabling bias-free generation, manipulation, and detection of electron spin for low-power electronics.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of 2D semiconductors in pushing further the miniaturization of electronic components and enabling more efficient electronics.

Method: Exploiting high-quality ferromagnetic/semiconductor interfaces and the asymmetric diffusion of spin-up/spin-down electrons across a semiconductor p-n junction to achieve generation, manipulation, and detection of electron spin across a nanometer-thick magnetic tunnel junction without applied bias.

Result: A giant anomalous zero-bias spin voltage exceeding 30,000% was observed, which is significantly larger than previously reported magnetoresistance signals.

Conclusion: The findings reveal unexplored opportunities to transform and amplify spin information for low-power electronics.

Abstract: For decades, semiconductors and their heterostructures have underpinned both
fundamental and applied research across all areas of electronics.
Two-dimensional, 2D (atomically thin) semiconductors have now the potential to
push further the miniaturization of electronic components, enabling the
development of more efficient electronics. Here, we report on a giant anomalous
zero-bias spin voltage in magnetic tunnel junctions based on 2D materials. The
generation, manipulation and detection of electron spin across a
nanometer-thick magnetic tunnel junction do not require any applied bias. It is
achieved by exploiting high-quality ferromagnetic/semiconductor interfaces and
the asymmetric diffusion of spin-up/spin-down electrons across a semiconductor
p-n junction. The large spin-voltage signal exceeds 30,000% and is far greater
than the highest magnetoresistance signals reported to date. Our findings
reveal unexplored opportunities to transform and amplify spin information for
low-power electronics.

</details>


### [369] [Quantum Hall Antidot as a Fractional Coulombmeter](https://arxiv.org/abs/2509.04209)
*Mario Di Luca,Emily Hajigeorgiou,Zekang Zhou,Tengyan Feng,Kenji Watanabe,Takashi Taniguchi,Mitali Banerjee*

Main category: cond-mat.mes-hall

TL;DR: 利用门控双层石墨烯中的反点结构，通过电导测量直接探测了分数量子霍尔态下的准粒子电荷，并首次在石墨烯器件中测量到了 e/3 的分数电荷。


<details>
  <summary>Details</summary>
Motivation: 分数电荷准粒子的探测对于理解其奇异量子性质至关重要，而传统的电子干涉仪常受到体边相互作用的干扰。反点结构克服了几何限制，并能通过简单的电导测量直接探测准粒子电荷，避免了更复杂的技术。

Method: 使用门控双层石墨烯反点结构，在库仑主导区域，研究了整数和分数量子霍尔态下的准粒子隧穿现象。

Result: 门电压周期和振荡斜率直接揭示了隧穿准粒子的电荷，这是一种测量石墨烯中分数电荷的实用方法。首次在石墨烯基器件中测量到 e/3 分数电荷。

Conclusion: 该门控双层石墨烯反点设计结构简单且可调谐，为在其他范德华材料中扩展基于反点的电荷测量提供了途径，证明了反点结构是研究量子霍尔效应的强大且广泛适用的平台。

Abstract: The detection of fractionally charged quasiparticles, which arise in the
fractional quantum Hall regime, is of fundamental importance for probing their
exotic quantum properties. While electronic interferometers have been central
to probe their statistical properties, their interpretation is often
complicated by bulk-edge interactions. Antidots, potential hills in the quantum
Hall regime, are particularly valuable in this context, as they overcome the
geometric limitations of conventional designs and act as controlled impurities
within a quantum point contact. Furthermore, antidots allow for quasiparticle
charge detection through straightforward conductance measurements, replacing
the need for more demanding techniques. In this work, we employ a gate-defined
bilayer graphene antidot operating in the Coulomb-dominated regime to study
quasiparticle tunneling in both integer and fractional quantum Hall states. We
show that the gate-voltage period and the oscillation slope directly reveal the
charge of tunneling quasiparticles, providing a practical method to measure
fractional charge in graphene. Moreover, we report the first measurement of the
$e/3$ fractional charge in a graphene-based device. The simplicity and
tunability of this design open a pathway to extend AD-based charge measurements
to other van der Waals materials, establishing antidots as a powerful and
broadly applicable platform to study the quantum Hall effect.

</details>


### [370] [Many-Body Rashba Spin-Orbit Interaction and Exciton Spin Relaxation in Atomically Thin Semiconductor Structures](https://arxiv.org/abs/2509.04285)
*Henry Mittenzwey,Andreas Knorr*

Main category: cond-mat.mes-hall

TL;DR: 提出一种新颖的自旋-轨道相互作用机制，该机制通过介观多粒子Rashba哈密顿量来描述单层过渡金属二硫化物（TMDC）中由局部电场引起的激子自旋弛豫，在MoSe$_2$上表现出亚皮秒量级的快速弛豫。


<details>
  <summary>Details</summary>
Motivation: 提出一种新颖的、先前未被探索的自旋-轨道相互作用机制，以解释单层过渡金属二硫化物（TMDC）中激子自旋弛豫的现象。

Method: 通过建立一个介观多粒子Rashba哈密顿量，并在最低阶自洽地描述了由介电环境的空间不对称性引起的局部电场导致的激子自旋弛豫。

Result: 在77开尔文以上的SiO$_2$衬底上的单层MoSe$_2}$上，观察到meV量级的明暗能级分裂，局部电场导致了亚皮秒量级的快速亚波长自旋弛豫，而对于具有更大明暗能级分裂的其他TMDC，这种效应可以忽略不计。

Conclusion: 所提出的自旋-轨道相互作用机制能够解释在特定TMDC（如MoSe$_2}$）中观察到的快速激子自旋弛豫现象，并且该效应的强弱与明暗能级分裂有关。

Abstract: We propose a previously unexplored spin-orbit interaction mechanism by
establishing a mesoscopic many-particle Rashba Hamiltonian. In lowest order,
this Hamiltonian self-consistently describes exciton spin relaxation in
monolayer transition metal dichalcogenides (TMDC) due to local electric fields
caused by spatial asymmetries in the dielectric environment. For a monolayer
MoSe$_2$ on a SiO$_2$ substrate above 77\,K showing a meV bright-dark
splitting, the local electric field causes fast intravalley spin relaxation on
a sub-picosecond timescale, whereas it is negligible for other TMDCs with
larger bright-dark splitting.

</details>


### [371] [Specific features of the $π$-electron spectrum of narrow achiral $(2m,m)$ nanoribbons](https://arxiv.org/abs/2509.04306)
*Lyuba Malysheva*

Main category: cond-mat.mes-hall

TL;DR: 该研究使用SSH-Huckel模型研究了芘分子构成的窄带石墨烯纳米带的紧束缚特征，发现了与线性acene不同的局域态，并分析了电子密度分布和Green函数系数。


<details>
  <summary>Details</summary>
Motivation: 分析芘分子链构成的窄带石墨烯纳米带的电子结构，特别是其与线性acene在电子态行为上的差异。

Method: 基于Su-Schrieffer-Heeger-Hückel模型，求解紧束缚特征值问题，分析了色散关系，并计算了电子密度分布和Green函数系数。

Result: 发现芘分子石墨烯纳米带的π电子谱包含局域态，而线性acene只包含扩展态。展示了扩展态和局域态电子密度分布行为的差异。

Conclusion: 芘分子石墨烯纳米带具有独特的电子结构，包含局域态，这与线性acene不同，并提供了其电子性质的详细分析。

Abstract: On the basis of the Su-Schrieffer-Heeger-H\"uckel-type Hamiltonian, we
consider the tight-binding eigenvalue problem for a sequence of pyrene
molecules forming a narrow $(2m,m)$ graphene nanoribbon. Specific features of
the corresponding dispersion relation are analyzed and illustrated with several
examples. It is shown that the $\pi$-electron spectrum of the pyrene oligomer
includes local states, in contrast to the spectrum of linear acene, which
consists only of extended states. We analyze and illustrate the difference in
the behavior of
  the electron density distribution for extended and local electronic states.
Explicit analytic expressions for the Green's function coefficients of the
pyrene molecule are also presented.

</details>


### [372] [In-situ profiling of pressure-induced exciton traps in suspended MoS$_2$ monolayers](https://arxiv.org/abs/2509.04319)
*Leonard Geilen,Lukas Schleicher,Alexander Musta,Benedict Brouwer,Eva M. Weig,Alexander Holleitner,Anne Rodriguez*

Main category: cond-mat.mes-hall

TL;DR: 我们展示了在具有纳米结构孔洞的基底上悬浮的 MoS2 单层原位读出空间轮廓。


<details>
  <summary>Details</summary>
Motivation: 控制和读出悬浮 MoS2 单层作为激子陷阱的发光特性。

Method: 利用 Fabry-P'erot 干涉和反射对比度图模型进行原位读出，并通过控制环境压力来调谐发光强度和能量。

Result: 实现了对数百个悬浮 MoS2 单层发光特性的可控读出和调谐。

Conclusion: 悬浮 MoS2 单层在纳米结构基底上可以作为可调谐的激子陷阱，并且可以通过环境压力进行控制。

Abstract: We demonstrate the in-situ read-out of the spatial profile of suspended
MoS$_2$ monolayers hosted on substrates with nano-structured holes. As the
profiles are spatially bent, the suspended MoS$_2$ monolayers act as exciton
traps with tunable luminescence intensity and energy. The tunability is
realized by controlling the environmental pressure on the monolayers, which
allows to control hundreds of suspended MoS$_2$ monolayers on a single
substrate. The in-situ read-out is based on Fabry-P\'erot interferences and a
model of the corresponding reflectance contrast maps of the investigated
monolayers.

</details>


### [373] [Zero and Nonzero Energy Majorana Modes in an Extended Kitaev Chain](https://arxiv.org/abs/2509.04420)
*Mohammad Ghuneim,Raditya Weda Bomantara*

Main category: cond-mat.mes-hall

TL;DR: 研究一种扩展的Kitaev链模型，该模型具有每个单位单元格三个子格。通过将具有三聚体单元格的修改版Su-Schrieffer-Heeger模型与标准Kitaev链混合，得到一种扩展形式，在Majorana基上形成六聚体结构。由于子格构型和p波超导配对之间的相互作用，得到了预期Majorana零模之外的丰富的边缘模结构。还发现各种Majorana边缘模对某些通用微扰和无序表现出相当大的鲁棒性。存在鲁棒的、超出零能量变化的Majorana边缘模，可能为在实验中明确检测Majorana模的持续努力提供一步。 


<details>
  <summary>Details</summary>
Motivation: 对具有每个单位单元格三个子格的扩展Kitaev链进行研究。

Method: 将具有三聚体单元格的修改版Su-Schrieffer-Heeger模型与标准Kitaev链混合，得到一种扩展形式，在Majorana基上形成六聚体结构。

Result: 由于子格构型和p波超导配对之间的相互作用，得到了预期Majorana零模之外的丰富的边缘模结构。还发现各种Majorana边缘模对某些通用微扰和无序表现出相当大的鲁棒性。

Conclusion: 存在鲁棒的、超出零能量变化的Majorana边缘模，可能为在实验中明确检测Majorana模的持续努力提供一步。

Abstract: This paper studies an extended Kitaev chain with three sublattices per unit
cell. This extended version is obtained by hybridizing a modified
Su-Schrieffer-Heeger model featuring trimerized unit cells with the standard
Kitaev chain, resulting in a hexamer structure on the Majorana basis. Due to
the interplay between the sublattice configuration and the $p$-wave
superconducting pairing, a rich structure of edge modes beyond the expected
Majorana zero modes is obtained. The various Majorana edge modes are further
found to demonstrate considerable robustness against some generic perturbations
and disorder. The presence of robust Majorana edge modes beyond their zero
energy variations potentially offers a step forward in the ongoing efforts to
unambiguously detect Majorana modes in experiments.

</details>


### [374] [Zero modes and index theorems for non-Hermitian Dirac fermions](https://arxiv.org/abs/2509.04447)
*Bitan Roy*

Main category: cond-mat.mes-hall

TL;DR: 洛伦兹不变的非厄米狄拉克算子在外部磁场或空间上非平凡的质量序存在的情况下，可以容纳零能量的束缚态，数量由索引定理决定。当非厄米狄拉克费米子具有实有效费米速度时，这些束缚态可以进一步被约束到空间上的非平凡区域。


<details>
  <summary>Details</summary>
Motivation: 将现有的索引定理扩展到洛伦兹不变的非厄米狄拉克算子，并研究这些算子在磁场和质量序存在下的零能量束缚态。

Method: 通过将狄拉克哈密顿量与质量相关的反厄米算子相结合来构建非厄米狄拉克算子，并求解零能量束缚态的显式解。

Result: 在存在任意形状的外部磁场时，当系统包含有限数量的磁通量子时，总会存在零能量束缚态。当存在空间上非平凡的质量序时，只有当非厄米狄拉克费米子的有效费米速度为实数时，才能在频谱中找到局域化的零能量束缚态。

Conclusion: 研究结果为在非厄米或开放狄拉克系统中，从拓扑鲁棒的零能量流形中产生竞争序提供了具体途径。文章还讨论了验证这些预测的实验设置。

Abstract: Dirac fermions, subject to external magnetic fields and in the presence of
mass orders that assume topologically nontrivial spatial textures such as
domain-wall and vortices, for example, bind robust mid-gap states at
zero-energy, the number of which is governed by the Aharonov-Casher and
Jackiw-Rebbi or Jackiw-Rossi index theorems, respectively. Here I extend the
jurisdiction of these prominent index theorems to Lorentz invariant
non-Hermitian (NH) Dirac operators, constructed by augmenting the celebrated
Dirac Hamiltonian by a masslike anti-Hermitian operator that also scales
linearly with momentum. The resulting NH Dirac operator manifests real
eigenvalues over an extended NH parameter regime, characterized by a real
effective Fermi velocity for NH Dirac fermions. From the explicit solutions of
the zero-energy bound states, I show that in the presence of external magnetic
fields of arbitrary shape such modes always exist when the system encloses a
finite number of magnetic flux quanta, while in the presence of spatially
non-trivial textures of the mass orders localized zero-energy modes can only be
found in the spectrum when the effective Fermi velocity for NH Dirac fermions
is real. These findings pave a concrete route to realize nucleation of
competing orders from the topologically robust zero-energy manifold in NH or
open Dirac systems. Possible experimental setups to test these predictions are
discussed.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [375] [The evolution of trust as a cognitive shortcut in repeated interactions](https://arxiv.org/abs/2509.04143)
*Cedric Perret,The Anh Han,Elias Fernández Domingos,Theodor Cimpeanu,Simon T. Powers*

Main category: cs.GT

TL;DR: 信任是一种认知捷径，可以提高合作水平，尤其是在检查他人行为成本高昂或可能出错的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有博弈论模型未能区分合作行为和信任，阻碍了对信任在社会困境中作用的测量和确定。

Method: 将信任形式化为重复博弈中的认知捷径，通过设定一个合作阈值来避免检查伙伴的行为。分析了基于信任的策略在二维对称社会困境博弈中的演化。

Result: 在检查他人行为成本高昂的情况下，基于信任的策略可以优于如“以牙还牙”等标准互惠策略。信任的存在可以提高种群的整体合作水平，尤其是在存在无意失误的情况下，即使存在利用信任的策略也是如此。

Conclusion: 基于信任的策略具有个体适应性优势，为信任促进不同类型的社会互动合作提供了理论基础，并对人机交互有启示。

Abstract: Trust is often thought to increase cooperation. However, game-theoretic
models often fail to distinguish between cooperative behaviour and trust. This
makes it difficult to measure trust and determine its effect in different
social dilemmas. We address this here by formalising trust as a cognitive
shortcut in repeated games. This functions by avoiding checking a partner's
actions once a threshold level of cooperativeness has been observed. We
consider trust-based strategies that implement this heuristic, and
systematically analyse their evolution across the space of two-player symmetric
social dilemma games. We find that where it is costly to check whether another
agent's actions were cooperative, as is the case in many real-world settings,
then trust-based strategies can outcompete standard reciprocal strategies such
as Tit-for-Tat in many social dilemmas. Moreover, the presence of trust
increases the overall level of cooperation in the population, especially in
cases where agents can make unintentional errors in their actions. This occurs
even in the presence of strategies designed to build and then exploit trust.
Overall, our results demonstrate the individual adaptive benefit to an agent of
using a trust heuristic, and provide a formal theory for how trust can promote
cooperation in different types of social interaction. We discuss the
implications of this for interactions between humans and artificial
intelligence agents.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [376] [Data-Driven Smart Maintenance of Historic Buildings](https://arxiv.org/abs/2509.03685)
*Zhongjun Ni*

Main category: eess.SY

TL;DR: 该论文提出了一种数据驱动的解决方案，利用物联网、云计算、边缘计算、本体数据建模和机器学习技术，来改善历史建筑的智能维护，特别是在室内气候管理、能源效率和保护实践方面。


<details>
  <summary>Details</summary>
Motivation: 鉴于数字转型为建筑环境带来了改进建筑维护的机会，并且历史建筑的保护至关重要，因此需要一种能够确保可持续性并保护遗产价值的预防性维护方法。

Method: 本论文整合了物联网、云计算、边缘计算、本体数据建模和机器学习，并结合了智能监测、数字孪生和人工智能，以实现数据驱动的智能维护。

Result: 通过整合多种技术，提出了一种全面的解决方案，以改善历史建筑的室内气候管理、能源效率和保护实践。

Conclusion: 该研究通过结合智能监测、数字孪生和人工智能，推进了历史建筑的数据驱动保护，为下一代遗产保护策略奠定了基础。

Abstract: Digital transformation in the built environment offers new opportunities to
improve building maintenance through data-driven approaches. Smart monitoring,
predictive modeling, and artificial intelligence can enhance decision-making
and enable proactive strategies. The preservation of historic buildings is an
important scenario where preventive maintenance is essential to ensure
long-term sustainability while protecting heritage values. This thesis presents
a comprehensive solution for data-driven smart maintenance of historic
buildings, integrating Internet of Things (IoT), cloud computing, edge
computing, ontology-based data modeling, and machine learning to improve indoor
climate management, energy efficiency, and conservation practices.
  This thesis advances data-driven conservation of historic buildings by
combining smart monitoring, digital twins, and artificial intelligence. The
proposed methods enable preventive maintenance and pave the way for the next
generation of heritage conservation strategies.

</details>


### [377] [Parameter Tuning Under Uncertain Road Perception in Driver Assistance Systems](https://arxiv.org/abs/2509.03694)
*Leon Greiser,Christian Rathgeber,Vladislav Nenchev,Sören Hohmann*

Main category: eess.SY

TL;DR: 本论文提出了一种基于记录数据的自动参数调整方法，用于解决自动驾驶中车道保持场景下的横向轨迹规划问题，该方法能有效处理传感器噪声，并优化了转向和车道保持性能。


<details>
  <summary>Details</summary>
Motivation: 传感器噪声导致车道估算不准确，给开发高性能控制架构带来了挑战，而手动调整横向轨迹规划参数耗时费力。

Method: 通过模拟车辆在参考曲线上的横向行为，并利用记录数据，自动优化横向轨迹规划器的参数，以应对不准确的车道估算。

Result: 该方法在模拟中优化了规划器参数，并在先前未见过的数据上展示了改进的性能。

Conclusion: 所提出的自动参数调整方法能够有效处理车道估算中的噪声，并为自动驾驶任务的横向轨迹规划提供优化的参数，从而提高性能。

Abstract: Advanced driver assistance systems have improved comfort, safety, and
efficiency of modern vehicles. However, sensor limitations lead to noisy lane
estimates that pose a significant challenge in developing performant control
architectures. Lateral trajectory planning often employs an optimal control
formulation to maintain lane position and minimize steering effort. The
parameters are often tuned manually, which is a time-intensive procedure. This
paper presents an automatic parameter tuning method for lateral planning in
lane-keeping scenarios based on recorded data, while taking into account noisy
road estimates. By simulating the lateral vehicle behavior along a reference
curve, our approach efficiently optimizes planner parameters for automated
driving and demonstrates improved performance on previously unseen test data.

</details>


### [378] [Avoidance of an unexpected obstacle without reinforcement learning: Why not using advanced control-theoretic tools?](https://arxiv.org/abs/2509.03721)
*Cédric Join,Michel Fliess*

Main category: eess.SY

TL;DR: 使用基于平坦性的控制和模型无关的预测控制来解决意外障碍物的避障问题，优于强化学习，且计算量低。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习需要大量试验来学习新任务的批评。

Method: 使用经典的Dubins汽车模型，结合基于平坦性的控制、HEOL反馈设置和最新的无模型预测控制方法。

Result: 实验结果令人信服，基于模型的和无模型的模型方法效果都很好，其中无模型的模型方法鲁棒性更优，优于目前流行的机器学习技术。

Conclusion: 所提出的两种方法（基于平坦性的控制和无模型预测控制）计算量低，并且在避障方面表现出令人满意的鲁棒性。

Abstract: This communication on collision avoidance with unexpected obstacles is
motivated by some critical appraisals on reinforcement learning (RL) which
"requires ridiculously large numbers of trials to learn any new task" (Yann
LeCun). We use the classic Dubins' car in order to replace RL with
flatness-based control, combined with the HEOL feedback setting, and the latest
model-free predictive control approach. The two approaches lead to convincing
computer experiments where the results with the model-based one are only
slightly better. They exhibit a satisfactory robustness with respect to
randomly generated mismatches/disturbances, which become excellent in the
model-free case. Those properties would have been perhaps difficult to obtain
with today's popular machine learning techniques in AI. Finally, we should
emphasize that our two methods require a low computational burden.

</details>


### [379] [Decentralized Safety-Critical Control of Resilient DC Microgrids with Large-Signal Stability Guarantees](https://arxiv.org/abs/2509.03789)
*Muratkhan Abdirash,Xiaofan Cui*

Main category: eess.SY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The increasing penetration of distributed energy resources and
power-electronics interfaces in DC microgrids, coupled with rising cyber
threats, necessitates primary controllers that are provably safe,
cyber-resilient, and practical. The increasing penetration of distributed
energy resources and power-electronics interfaces in DC microgrids, coupled
with rising cyber threats, necessitates primary controllers that are provably
safe, cyber-resilient, and practical. Conventional droop-based methods remain
prevalent due to their simplicity, yet their design is largely empirical and
conservative, lacking rigorous guarantees. Advanced strategies improve certain
aspects, but often sacrifice scalability, robustness, or formal safety. In this
work, we propose a Distributed Safety-Critical Controller (DSCC) that
systematically integrates global stabilization with formal safety guarantees in
a fully decentralized manner. Leveraging control barrier functions and the
port-Hamiltonian system theory, the DSCC achieves scalable safe stabilization
while preserving real-time implementability. High-fidelity switched-circuit
simulations validate the controller's advantages under various contingencies.
This framework paves the way for resilient, safety-critical, and scalable
control in next-generation DC microgrids.

</details>


### [380] [On the Performance Analysis of Pinching-Antenna-Enabled SWIPT Systems](https://arxiv.org/abs/2509.03836)
*Bingxin Zhang,Han Zhang,Kun Yang,Yizhe Zhao,Kezhi Wang*

Main category: eess.SY

TL;DR: 本文提出了一种基于柔性捏合天线的新型同步无线信息和电力传输（SWIPT）系统，并研究了其性能。为了支持灵活部署并优化能量-速率性能，提出了三种实用的捏合天线放置方案：边缘部署方案（EDS）、中心部署方案（CDS）和对角线部署方案（DDS）。此外，还引入了一种混合时间切换（TS）和功率分配（PS）协议，允许在能量收集和信息解码之间进行动态调整。在每种部署策略和传输协议下，都基于捏合天线的最佳定位，推导出了随机用户设备（UE）的平均收集能量和平均可实现速率的封闭形式表达式。数值模拟证实了理论分析的准确性，并说明了不同方案下速率和能量收集之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 为了支持灵活部署并优化能量-速率性能，需要研究新型SWIPT系统。

Method: 提出三种捏合天线放置方案（EDS、CDS、DDS），并引入混合时间切换（TS）和功率分配（PS）协议，然后推导出平均收集能量和平均可实现速率的封闭形式表达式。

Result: 数值模拟证实了理论分析的准确性，并说明了不同方案下速率和能量收集之间的权衡。

Conclusion: 所提出的SWIPT系统和部署方案能够实现灵活部署并优化能量-速率性能。

Abstract: In this paper, we studies the performance of a novel simultaneous wireless
information and power transfer (SWIPT) system enabled by a flexible
pinching-antenna. To support flexible deployment and optimize energy-rate
performance, we propose three practical pinching antenna placement-schemes: the
edge deployment scheme (EDS), the center deployment scheme (CDS), and the
diagonal deployment scheme (DDS). Moreover, a hybrid time-switching (TS) and
power-splitting (PS) protocol is introduced, allowing dynamic adjustment
between energy harvesting and information decoding. Under each deployment
strategy and the transmission protocol, closed-form expressions for the average
harvested energy and average achievable rate of a randomly located user
equipment (UE) are derived based on the optimal positioning of the
pinching-antenna. Numerical simulations confirm the accuracy of the theoretical
analysis and illustrate the trade-off between rate and energy harvesting under
different schemes.

</details>


### [381] [Reservoir Predictive Path Integral Control for Unknown Nonlinear Dynamics](https://arxiv.org/abs/2509.03839)
*Daisuke Inoue,Tadayoshi Matsumori,Gouhei Tanaka,Yuji Ito*

Main category: eess.SY

TL;DR: 该研究提出了一种结合了回声状态网络（ESN）和模型预测路径积分（MPPI）控制的机器人控制新框架，称为水库预测路径积分（RPPI），并进一步扩展为不确定性感知RPPI（URPPI）。


<details>
  <summary>Details</summary>
Motivation: 解决快速在线识别和控制非线性动力学系统中的未知动力学这一核心挑战。

Method: RPPI框架集成了ESN（一种递归神经网络模型）和MPPI控制（一种基于采样的模型预测控制变体），实现了对非线性动力学的快速学习，并直接利用学习到的非线性进行MPPI控制的并行计算，无需线性化近似。URPPI利用ESN的不确定性来平衡探索与利用。

Result: 在Duffing振子和四箱系统上的实验表明，URPPI相比于传统的基于二次规划的模型预测控制方法，能够将控制成本降低高达60%。

Conclusion: RPPI和URPPI框架能够有效应对非线性动力学系统的控制挑战，URPPI通过不确定性感知机制进一步提高了控制性能。

Abstract: Neural networks capable of approximating complex nonlinearities have found
extensive application in data-driven control of nonlinear dynamical systems.
However, fast online identification and control of unknown dynamics remain
central challenges. This paper integrates echo-state networks (ESNs) --
reservoir computing models implemented with recurrent neural networks -- and
model predictive path integral (MPPI) control -- sampling-based variants of
model predictive control -- to meet these challenges. The proposed reservoir
predictive path integral (RPPI) enables fast learning of nonlinear dynamics
with ESN and exploits the learned nonlinearities directly in parallelized MPPI
control computation without linearization approximations. The framework is
further extended to uncertainty-aware RPPI (URPPI), which leverages ESN
uncertainty to balance exploration and exploitation: exploratory inputs
dominate during early learning, while exploitative inputs prevail as model
confidence grows. Experiments on controlling the Duffing oscillator and
four-tank systems demonstrate that URPPI improves control performance, reducing
control costs by up to 60% compared to traditional quadratic programming-based
model predictive control methods.

</details>


### [382] [SAFE--MA--RRT: Multi-Agent Motion Planning with Data-Driven Safety Certificates](https://arxiv.org/abs/2509.04413)
*Babak Esmaeili,Hamidreza Modares*

Main category: eess.SY

TL;DR: 该论文提出了一种全数据驱动的运动规划框架，用于在没有显式系统模型的情况下，在共享、充满障碍物的工作空间中运行的齐次线性多智能体系统。


<details>
  <summary>Details</summary>
Motivation: 在没有显式系统模型的情况下，为在共享、充满障碍物的工作空间中运行的齐次线性多智能体系统开发一种全数据驱动的运动规划框架。

Method: 每个智能体通过解决产生局部不变椭圆体和相应状态反馈增益的凸半定程序，从实验数据中独立学习其闭环行为。基于网格的航点、动态可行性、局部不变性、采样规划、航点树、椭圆体重叠、不变性到不变性的过渡、连续安全性、同时扩展树、时空预留表、防止同时占用和迎头碰撞、局部控制器、无运行时优化、动态可行性、可证明的安全性。

Result: 仿真结果证明了该方法在合成同步、安全的轨迹方面的有效性，用于在共享动态和约束下，仅使用数据和凸优化工具运行的多个智能体。

Conclusion: 该方法能够合成动态可行且可证明安全的轨迹，同时考虑环境约束和智能体间碰撞。

Abstract: This paper proposes a fully data-driven motion-planning framework for
homogeneous linear multi-agent systems that operate in shared, obstacle-filled
workspaces without access to explicit system models. Each agent independently
learns its closed-loop behavior from experimental data by solving convex
semidefinite programs that generate locally invariant ellipsoids and
corresponding state-feedback gains. These ellipsoids, centered along grid-based
waypoints, certify the dynamic feasibility of short-range transitions and
define safe regions of operation. A sampling-based planner constructs a tree of
such waypoints, where transitions are allowed only when adjacent ellipsoids
overlap, ensuring invariant-to-invariant transitions and continuous safety. All
agents expand their trees simultaneously and are coordinated through a
space-time reservation table that guarantees inter-agent safety by preventing
simultaneous occupancy and head-on collisions. Each successful edge in the tree
is equipped with its own local controller, enabling execution without
re-solving optimization problems at runtime. The resulting trajectories are not
only dynamically feasible but also provably safe with respect to both
environmental constraints and inter-agent collisions. Simulation results
demonstrate the effectiveness of the approach in synthesizing synchronized,
safe trajectories for multiple agents under shared dynamics and constraints,
using only data and convex optimization tools.

</details>


### [383] [Sample Efficient Certification of Discrete-Time Control Barrier Functions](https://arxiv.org/abs/2509.03899)
*Sampath Kumar Mulagaleti,Andrea Del Prete*

Main category: eess.SY

TL;DR: CBFs用于计算控制不变集，但计算复杂。本文提出一种基于Lipschitz的方法来验证CBFs，以提高采样效率。


<details>
  <summary>Details</summary>
Motivation: CBFs是计算控制不变集的有效工具，但其计算过程复杂且难以处理。

Method: 提出一种基于Lipschitz参数的方法来验证CBFs，并设计了一种采样效率高的认证算法。

Result: 通过数值算例验证了该方法的有效性。

Conclusion: 所提出的基于Lipschitz参数的验证方法能够有效提高CBF计算的采样效率。

Abstract: Control Invariant (CI) sets are instrumental in certifying the safety of
dynamical systems. Control Barrier Functions (CBFs) are effective tools to
compute such sets, since the zero sublevel sets of CBFs are CI sets. However,
computing CBFs generally involves addressing a complex robust optimization
problem, which can be intractable. Scenario-based methods have been proposed to
simplify this computation. Then, one needs to verify if the CBF actually
satisfies the robust constraints. We present an approach to perform this
verification that relies on Lipschitz arguments, and forms the basis of a
certification algorithm designed for sample efficiency. Through a numerical
example, we validated the efficiency of the proposed procedure.

</details>


### [384] [Physics-Informed Detection of Friction Anomalies in Satellite Reaction Wheels](https://arxiv.org/abs/2509.04060)
*Alejandro Penacho Riveiros,Nicola Bastianello,Karl H. Johansson,Matthieu Barreau*

Main category: eess.SY

TL;DR: 该算法通过混合系统理论、变化点检测、动态规划和最大似然估计来分析卫星反作用轮组件（RWA）的摩擦数据，并使用预先训练的分类器来区分RWA的正常和异常状态，准确率约为95%。


<details>
  <summary>Details</summary>
Motivation: 随着卫星数量的指数级增长，自动化方法对于确保卫星功能正常运行的需求日益增加，以减轻人力负担。

Method: 首先，利用基于混合系统理论的模型提取与问题相关的信息，该提取过程结合了变化点检测、动态规划和最大似然估计技术。然后，利用提取的信息，通过一个先前使用高保真模拟器（包含大部分正常数据）生成的标记数据集进行训练的分类器来确定RWA的状态。该算法结合了基于模型和基于数据的方法。

Result: 该算法的准确率约为95%。

Conclusion: 该算法结合了基于模型和基于数据的方法，能够以大约95%的准确率区分卫星反作用轮组件（RWA）的正常和异常状态。

Abstract: As the number of satellites in orbit has increased exponentially in recent
years, ensuring their correct functionality has started to require automated
methods to decrease human workload. In this work, we present an algorithm that
analyzes the on-board data related to friction from the Reaction Wheel
Assemblies (RWA) of a satellite and determines their operating status,
distinguishing between nominal status and several possible anomalies that
require preventive measures to be taken. The algorithm first uses a model based
on hybrid systems theory to extract the information relevant to the problem.
The extraction process combines techniques in changepoint detection, dynamic
programming, and maximum likelihood in a structured way. A classifier then uses
the extracted information to determine the status of the RWA. This last
classifier has been previously trained with a labelled dataset produced by a
high-fidelity simulator, comprised for the most part of nominal data. The final
algorithm combines model-based and data-based approaches to obtain satisfactory
results with an accuracy around 95%.

</details>


### [385] [Low-Power Impact Detection and Localization on Forklifts Using Wireless IMU Sensors](https://arxiv.org/abs/2509.04096)
*Lyssa Ramaut,Chesney Buyle,Jona Cappelle,Liesbet Van der Perre*

Main category: eess.SY

TL;DR: 本论文提出了一种基于无线传感器节点测量3D加速度的低成本、低功耗碰撞检测系统，用于叉车，能成功检测和定位碰撞，并保持低功耗，实现多年的自主运行。


<details>
  <summary>Details</summary>
Motivation: 叉车在工业环境中运输货物时，由于野蛮操作、崎岖地形、狭窄空间和复杂操作场景，容易发生意外碰撞和故意滥用，危及安全和设备完整性。

Method: 使用多个无线传感器节点测量3D加速度，并部署在实际操作场景中进行测量。基于收集的数据开发算法，以区分高冲击事件和正常使用，并定位叉车上的碰撞。

Result: 该解决方案成功检测和定位了碰撞，同时保持了低功耗。

Conclusion: 该系统能够以低成本、低功耗的方式实现可靠的叉车监测，并具有多年的自主运行能力。

Abstract: Forklifts are essential for transporting goods in industrial environments.
These machines face wear and tear during field operations, along with rough
terrain, tight spaces and complex handling scenarios. This increases the
likelihood of unintended impacts, such as collisions with goods,
infrastructure, or other machinery. In addition, deliberate misuse has been
stated, compromising safety and equipment integrity. This paper presents a
low-cost and low-power impact detection system based on multiple wireless
sensor nodes measuring 3D accelerations. These were deployed in a measurement
campaign covering realworld operational scenarios. An algorithm was developed,
based on this collected data, to differentiate high-impact events from normal
usage and to localize detected collisions on the forklift. The solution
successfully detects and localizes impacts, while maintaining low power
consumption, enabling reliable forklift monitoring with multi-year sensor
autonomy.

</details>


### [386] [Remote Estimation for Markov Jump Linear Systems: A Distributionally Robust Approach](https://arxiv.org/abs/2509.04116)
*Ioannis Tzortzis,Themistoklis Charalambous,Charalambos D. Charalambous*

Main category: eess.SY

TL;DR: 该论文提出了一种用于处理不确定马尔可夫跳变线性系统后验模式概率的远程状态估计方法。


<details>
  <summary>Details</summary>
Motivation: 远程状态估计面临不确定的后验模式概率问题，尤其是在不可靠通信网络中接收到噪声或不完整测量时。

Method: 采用分布鲁棒框架，将真实后验视为以标称后验为中心的总变差距离球内的分布。这种最小-最大化方法得到了一个扩展了经典MMSE解的估计器，并增加了处理模式不确定性的项。通过分布鲁棒的第一阶广义伪贝叶斯算法实现了可行的实现。

Result: 提出的分布鲁棒估计器能够有效处理后验模式概率的不确定性，并扩展了经典的MMSE估计器。

Conclusion: 该方法提供了一种解决不确定马尔可夫跳变线性系统远程状态估计问题的有效途径，并通过数值算例证明了其适用性和有效性。

Abstract: This paper considers the problem of remote state estimation for Markov jump
linear systems in the presence of uncertainty in the posterior mode
probabilities. Such uncertainty may arise when the estimator receives noisy or
incomplete measurements over an unreliable communication network. To address
this challenge, the estimation problem is formulated within a distributionally
robust framework, where the true posterior is assumed to lie within a total
variation distance ball centered at the nominal posterior. The resulting
minimax formulation yields an estimator that extends the classical MMSE
solution with additional terms that account for mode uncertainty. A tractable
implementation is developed using a distributionally robust variant of the
first-order generalized pseudo-Bayesian algorithm. A numerical example is
provided to illustrate the applicability and effectiveness of the approach.

</details>


### [387] [Laplacian Flows in Complex-valued Directed Networks: Analysis, Design, and Consensus](https://arxiv.org/abs/2509.04196)
*Aditi Saxena,Twinkle Tripathy,Rajasekhar Anguluri*

Main category: eess.SY

TL;DR: 本文研究了复数加权网络中的一致性问题，提出了“实支配”条件，并设计了保证一致性的改进流。


<details>
  <summary>Details</summary>
Motivation: 研究复数加权网络的必要性和充分条件，以实现强连通和弱连通有向图的一致性，特别是在配电网等实际应用中。 

Method: 利用复数Perron-Frobenius性质研究拉普拉斯算子的谱性质及其与图条件的关系，并提出改进的流来保证一致性。

Result: 证明了复数加权拉普拉斯流在满足“实支配”条件（依赖于边权重相位角）的情况下可以收敛到一致性。

Conclusion: 提出了保证网络一致性的方法，并通过在合成和真实网络上进行模拟来验证其有效性，同时探讨了复数加权网络中的扩散过程。

Abstract: In the interdisciplinary field of network science, a complex-valued network,
with edges assigned complex weights, provides a more nuanced representation of
relationships by capturing both the magnitude and phase of interactions.
Additionally, an important application of this setting arises in distribution
power grids. Motivated by the richer framework, we study the necessary and
sufficient conditions for achieving consensus in both strongly and weakly
connected digraphs. The paper establishes that complex-valued Laplacian flows
converge to consensus subject to an additional constraint termed as real
dominance which relies on the phase angles of the edge weights. Our approach
builds on the complex Perron-Frobenius properties to study the spectral
properties of the Laplacian and its relation to graphical conditions. Finally,
we propose modified flows that guarantee consensus even if the original network
does not converge to consensus. Additionally, we explore diffusion in
complex-valued networks as a dual process of consensus and simulate our results
on synthetic and real-world networks.

</details>


### [388] [On the Effect of Sampling-Time Jitter](https://arxiv.org/abs/2509.04199)
*Dieter Schwarzmann,Simon Käser*

Main category: eess.SY

TL;DR: 该论文提出了一种通过等效的无抖动模拟来重新解释受抖动影响的线性时不变系统的方法，发现抖动会导致仿射缩放，并将其解释为频率缩放。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在分析采样时间抖动（即由执行时间不准确引起的误差）对系统的影响，并为实践者提供分析方法。

Method: 提出通过等效的无抖动模拟来重新解释受抖动影响的线性时不变系统，并构建一个吸收计时扰动影响的感知系统，以推导抖动的仿射缩放。

Result: 研究表明，抖动会有效地缩放系统矩阵，并且在拉普拉斯域中，抖动可以被解释为频率缩放。

Conclusion: 抖动会有效地缩放系统矩阵，并在拉普拉斯域中表现为频率缩放。

Abstract: This brief, aimed at practitioners, offers an analysis of the effect of
sampling-time jitter, i. e., the error produced by execution-time inaccuracies.
We propose reinterpreting jitter-afflicted linear time-invariant systems
through equivalent jitter-free analogs. By constructing a perceived system that
absorbs the effects of timing perturbations into its dynamics, we find an
affine scaling of jitter. We examine both measurement and implementation
scenarios, demonstrating that the presence of jitter effectively scales the
system matrices. Moreover, we observe that, in the Laplace domain, jitter can
be interpreted as a frequency scaling.

</details>


### [389] [Sailing Towards Zero-Shot State Estimation using Foundation Models Combined with a UKF](https://arxiv.org/abs/2509.04213)
*Tobin Holtmann,David Stenger,Andres Posada-Moreno,Friedrich Solowjow,Sebastian Trimpe*

Main category: eess.SY

TL;DR: 本研究提出了一种名为FM-UKF的新型状态估计算法，它结合了基于Transformer的系统动力学模型和分析已知传感器模型，实现了在不同传感器配置下的零样本泛化能力，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 传统的状态估计方法需要大量手动系统辨识或数据收集，而基于Transformer的通用模型可以减少数据需求。开发能够实现零样本系统动力学预测的基础模型，可以极大地减少手动部署的成本。

Method: FM-UKF结合了基于Transformer的系统动力学模型和分析已知的传感器模型，并通过UKF（无损卡尔曼滤波）进行融合。

Result: FM-UKF在集装箱船模型的新基准测试中，与经典的近似系统知识方法和端到端方法相比，在准确性、成本和鲁棒性方面取得了有竞争力的权衡。

Conclusion: FM-UKF能够跨越不同的动力学模型，无需重新训练即可适应新的传感器配置，并在集装箱船模型上展现了良好的性能。研究还开源了基准测试和数据集，以促进未来在零样本状态估计领域的研究。

Abstract: State estimation in control and systems engineering traditionally requires
extensive manual system identification or data-collection effort. However,
transformer-based foundation models in other domains have reduced data
requirements by leveraging pre-trained generalist models. Ultimately,
developing zero-shot foundation models of system dynamics could drastically
reduce manual deployment effort. While recent work shows that transformer-based
end-to-end approaches can achieve zero-shot performance on unseen systems, they
are limited to sensor models seen during training. We introduce the foundation
model unscented Kalman filter (FM-UKF), which combines a transformer-based
model of system dynamics with analytically known sensor models via an UKF,
enabling generalization across varying dynamics without retraining for new
sensor configurations. We evaluate FM-UKF on a new benchmark of container ship
models with complex dynamics, demonstrating a competitive accuracy, effort, and
robustness trade-off compared to classical methods with approximate system
knowledge and to an end-to-end approach. The benchmark and dataset are open
sourced to further support future research in zero-shot state estimation via
foundation models.

</details>


### [390] [Compatibility of Multiple Control Barrier Functions for Constrained Nonlinear Systems](https://arxiv.org/abs/2509.04220)
*Max H. Cohen,Eugene Lavretsky,Aaron D. Ames*

Main category: eess.SY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Control barrier functions (CBFs) are a powerful tool for the constrained
control of nonlinear systems; however, the majority of results in the
literature focus on systems subject to a single CBF constraint, making it
challenging to synthesize provably safe controllers that handle multiple state
constraints. This paper presents a framework for constrained control of
nonlinear systems subject to box constraints on the systems' vector-valued
outputs using multiple CBFs. Our results illustrate that when the output has a
vector relative degree, the CBF constraints encoding these box constraints are
compatible, and the resulting optimization-based controller is locally
Lipschitz continuous and admits a closed-form expression. Additional results
are presented to characterize the degradation of nominal tracking objectives in
the presence of safety constraints. Simulations of a planar quadrotor are
presented to demonstrate the efficacy of the proposed framework.

</details>


### [391] [Reinforcement Learning for Robust Ageing-Aware Control of Li-ion Battery Systems with Data-Driven Formal Verification](https://arxiv.org/abs/2509.04288)
*Rudi Coppola,Hovsep Touloujian,Pierfrancesco Ombrini,Manuel Mazo Jr*

Main category: eess.SY

TL;DR: 本文提出了一种结合强化学习和数据驱动形式化方法的混合控制策略，用于设计电池充电和安全协议，以解决充电速度和电池老化之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前研究中，锂离子电池的快速充电和电池寿命之间的矛盾是一个关键挑战。

Method: 该方法采用高保真度基于物理学的电池模型，并利用了计数例子引导的归纳综合（Counterexample-Guided Inductive Synthesis）方案。该方案结合了强化学习（RL）和数据驱动的形式化方法，以生成混合控制策略。具体来说，RL用于合成单个控制器，而数据驱动的抽象则指导这些控制器根据电池的初始输出测量值进行切换。

Result: 该方法生成了一个混合系统，通过RL控制器和数据驱动的抽象相结合，实现了对充电速度和电池寿命的权衡。当设计满足期望标准时，该抽象能够提供关于闭环性能的概率保证。

Conclusion: 所提出的混合控制策略能够有效地解决充电速度与电池老化之间的矛盾，并通过数据驱动的形式化方法为电池管理系统提供性能保证。

Abstract: Rechargeable lithium-ion (Li-ion) batteries are a ubiquitous element of
modern technology. In the last decades, the production and design of such
batteries and their adjacent embedded charging and safety protocols, denoted by
Battery Management Systems (BMS), has taken central stage. A fundamental
challenge to be addressed is the trade-off between the speed of charging and
the ageing behavior, resulting in the loss of capacity in the battery cell. We
rely on a high-fidelity physics-based battery model and propose an approach to
data-driven charging and safety protocol design. Following a
Counterexample-Guided Inductive Synthesis scheme, we combine Reinforcement
Learning (RL) with recent developments in data-driven formal methods to obtain
a hybrid control strategy: RL is used to synthesise the individual controllers,
and a data-driven abstraction guides their partitioning into a switched
structure, depending on the initial output measurements of the battery. The
resulting discrete selection among RL-based controllers, coupled with the
continuous battery dynamics, realises a hybrid system. When a design meets the
desired criteria, the abstraction provides probabilistic guarantees on the
closed-loop performance of the cell.

</details>


### [392] [Learning Optimal Crew Dispatch for Grid Restoration Following an Earthquake](https://arxiv.org/abs/2509.04308)
*Farshad Amani,Faezeh Ardali,Amin Kargarian*

Main category: eess.SY

TL;DR: 提出了一种结合Transformer和深度强化学习（DRL）的框架，用于在灾后快速调度人员以恢复电力和燃气网络。


<details>
  <summary>Details</summary>
Motivation: 传统的混合整数线性规划方法在灾后调度人员时计算时间过长，无法满足实时决策的需求。

Method: 将人员调度视为不确定性下的序贯决策问题。利用Transformer捕捉高维系统状态和时间依赖性，利用DRL实现自适应和可扩展的决策。首先，使用地震标准描述地震引起的配电网破坏，然后通过情景生成和约简管道将可能的结果聚合为单一的地理空间影响图。基于该影响图，该框架生成二级调度策略，并在模拟和历史事件上进行离线训练，在线部署以快速响应。

Result: 与传统方法相比，该方法显著缩短了运行时间，同时保持了高质量的解决方案。通过案例研究（特别是在2869节点的欧洲燃气和电力网络上）证明了其有效性。

Conclusion: 所提出的学习方法能够在保证解决方案质量的同时，显著加快恢复速度，展示了其在大规模灾难响应中实际部署的潜力，并提高了系统韧性。

Abstract: Post-disaster crew dispatch is a critical but computationally intensive task.
Traditional mixed-integer linear programming methods often require minutes to
several hours to compute solutions, leading to delays that hinder timely
decision-making in highly dynamic restoration environments. To address this
challenge, we propose a novel learning-based framework that integrates
transformer architectures with deep reinforcement learning (DRL) to deliver
near real-time decision support without compromising solution quality. Crew
dispatch is formulated as a sequential decision-making problem under
uncertainty, where transformers capture high-dimensional system states and
temporal dependencies, while DRL enables adaptive and scalable decision-making.
Earthquake-induced distribution network damage is first characterized using
established seismic standards, followed by a scenario generation and reduction
pipeline that aggregates probable outcomes into a single geospatial impact map.
Conditioned on this map, the proposed framework generates second-level dispatch
strategies, trained offline on simulated and historical events and deployed
online for rapid response. In addition to substantial runtime improvements, the
proposed method enhances system resilience by enabling faster and more
effective recovery and restoration. Case studies, particularly on the 2869-bus
European gas and power network, demonstrate that the method substantially
accelerates restoration while maintaining high-quality solutions, underscoring
its potential for practical deployment in large-scale disaster response.

</details>


### [393] [Impact on transient stability of self-synchronisation control strategies in grid-forming VSC-based generators](https://arxiv.org/abs/2509.04388)
*Regulo E. Avila-Martinez,Xavier Guillaud,Javier Renedo,Luis Rouco,Aurelio Garcia-Cerrada,Lukas Sigrist*

Main category: eess.SY

TL;DR: 该论文比较了四种用于并网逆变器的自同步策略（VSM无PLL、VSM带PLL、VSM无PLL带洗水滤波器和IP控制器）的瞬态稳定性，并分析了两种提高瞬态稳定性的方法（虚拟饱和有功功率控制器VAPC和本文提出的频率限制控制器FLC）。


<details>
  <summary>Details</summary>
Motivation: 并网逆变器（GFM-VSC）是整合可再生能源的解决方案，需要自同步策略来确保并网稳定性。然而，以往研究缺乏对不同VSM策略变体对瞬态稳定性影响的系统性研究。

Method: 本文分析并比较了四种自同步策略（VSM无PLL、VSM带PLL、VSM无PLL带洗水滤波器和IP控制器）的瞬态稳定性。此外，还分析了两种用于提高GFM-VSC自同步策略瞬态稳定性的方法：虚拟饱和有功功率控制器（VAPC）和本文提出的频率限制控制器（FLC）。

Result: 该论文分析和比较了四种自同步策略的瞬态稳定性，并分析了两种提高瞬态稳定性的方法。

Conclusion: 该论文系统地研究了不同GFM-VSC自同步策略对瞬态稳定性的影响，并提出了一种新的频率限制控制器（FLC）来进一步提高瞬态稳定性。

Abstract: Grid-forming voltage source converters (GFM-VSCs) are emerging as a solution
for integrating renewable energy resources (RERs) into power systems. GFM-VSCs
need a self-synchronisation strategy to ensure that all converters and
generators in the power system are in synchronism and they reach the same
frequency in steady state. The self-synchronisation strategy in GFM-VSCs that
has received most attention in previous research is virtual synchronous machine
(VSM) control. However, no systematic study of the effects on transient
stability of different variants of this strategy has been carried out in
previous work. This paper analyses and compares transient stability of four
self-synchronisation strategies for GFM-VSCs: VSM without phase-locked loop
(PLL), VSM with PLL, VSM without PLL using wash-out filter and
integral-proportional (IP) controller. The paper also analyses two different
methods that can \color{black} be applied to GFM-VSC self-synchronisation
strategies to improve transient stability: the concept of virtual unsaturated
active-power controller (VAPC), proposed in previous work, and an algorithm for
frequency limitation in the GFM-VSC (FLC), which is proposed in this paper.

</details>


### [394] [Leveraging Equivariances and Symmetries in the Control Barrier Function Synthesis](https://arxiv.org/abs/2509.04399)
*Adrian Wiltz,Dimos V. Dimarogonas*

Main category: eess.SY

TL;DR: 利用系统动力学和约束中的等变性来简化控制障碍函数（CBF）的合成，通过从一个子集推断整个域的CBF值来节省计算量，并可用于非对称约束。


<details>
  <summary>Details</summary>
Motivation: 现有的CBF合成方法计算量大或构造复杂，本文旨在探索利用系统动力学和约束的结构特性（特别是等变性）来简化CBF合成。

Method: 研究动力学中的等变性（对称性）如何诱导CBF中的对称性，并利用此特性从一个子集推断整个域的CBF值，同时探索等变性在非对称约束下的CBF合成应用。

Result: 证明了动力学中的等变性和约束中的对称性可以诱导CBF中的对称性，从而通过在子集上计算CBF来节省计算量；并展示了如何利用等变性结合部分已知的CBF来合成新的约束下的CBF。

Conclusion: 系统动力学和约束中的等变性为CBF的合成提供了有效的计算简化途径，尤其是在处理大规模或复杂系统时，并且该方法也适用于非对称约束的情况。

Abstract: The synthesis of Control Barrier Functions (CBFs) often involves demanding
computations or a meticulous construction. However, structural properties of
the system dynamics and constraints have the potential to mitigate these
challenges. In this paper, we explore how equivariances in the dynamics,
loosely speaking a form of symmetry, can be leveraged in the CBF synthesis.
Although CBFs are generally not inherently symmetric, we show how equivariances
in the dynamics and symmetries in the constraints induce symmetries in CBFs
derived through reachability analysis. This insight allows us to infer their
CBF values across the entire domain from their values on a subset, leading to
significant computational savings. Interestingly, equivariances can be even
leveraged to the CBF synthesis for non-symmetric constraints. Specifically, we
show how a partially known CBF can be leveraged together with equivariances to
construct a CBF for various new constraints. Throughout the paper, we provide
examples illustrating the theoretical findings. Furthermore, a numerical study
investigates the computational gains from invoking equivariances into the CBF
synthesis.

</details>
