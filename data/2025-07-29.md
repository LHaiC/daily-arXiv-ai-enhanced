<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 171]
- [cs.CL](#cs.CL) [Total: 81]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.RO](#cs.RO) [Total: 42]
- [cs.MA](#cs.MA) [Total: 2]
- [quant-ph](#quant-ph) [Total: 66]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.DS](#cs.DS) [Total: 9]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 31]
- [eess.SP](#eess.SP) [Total: 23]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 22]
- [eess.SY](#eess.SY) [Total: 33]
- [cs.GT](#cs.GT) [Total: 4]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.SI](#cs.SI) [Total: 5]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 31]
- [cs.ET](#cs.ET) [Total: 4]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.AI](#cs.AI) [Total: 16]
- [physics.app-ph](#physics.app-ph) [Total: 8]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Tuning adaptive gamma correction (TAGC) for enhancing images in low ligh](https://arxiv.org/abs/2507.19574)
*Ghufran Abualhail Alhamzawi,Ali Saeed Alfoudi,Ali Hakem Alsaeedi,Suha Mohammed Hadi,Amjed Abbas Ahmed,Md. Riad Hassan,Nurhizam Safie Mohd Satar,Waeel Yahya Yasseen*

Main category: cs.CV

TL;DR: 提出了一种名为“调优自适应伽马校正”（TAGC）的模型，通过自适应计算伽马系数来提升低光图像质量，在保持细节、对比度和颜色分布的同时，提供了自然的视觉效果。


<details>
  <summary>Details</summary>
Motivation: 低光照条件下的图像增强是计算机视觉中的一个重要挑战，因为光照不足会影响图像质量，导致低对比度、强噪声和细节模糊。

Method: 提出了一种名为“调优自适应伽马校正”（TAGC）的模型，该模型通过分析低光图像的颜色亮度并计算平均颜色来确定自适应伽马系数。

Result: 定性和定量评估表明，TAGC模型有效提升了低光图像，同时保持了细节、自然对比度和正确的颜色分布，并提供了自然的视觉质量。

Conclusion: 该模型有效提升了低光图像质量，同时保持了细节、自然对比度和正确的颜色分布，并提供了自然的视觉质量。

Abstract: Enhancing images in low-light conditions is an important challenge in
computer vision. Insufficient illumination negatively affects the quality of
images, resulting in low contrast, intensive noise, and blurred details. This
paper presents a model for enhancing low-light images called tuning adaptive
gamma correction (TAGC). The model is based on analyzing the color luminance of
the low-light image and calculating the average color to determine the adaptive
gamma coefficient. The gamma value is calculated automatically and adaptively
at different illumination levels suitable for the image without human
intervention or manual adjustment. Based on qualitative and quantitative
evaluation, tuning adaptive gamma correction model has effectively improved
low-light images while maintaining details, natural contrast, and correct color
distribution. It also provides natural visual quality. It can be considered a
more efficient solution for processing low-light images in multiple
applications such as night surveillance, improving the quality of medical
images, and photography in low-light environments.

</details>


### [2] [Is Exchangeability better than I.I.D to handle Data Distribution Shifts while Pooling Data for Data-scarce Medical image segmentation?](https://arxiv.org/abs/2507.19575)
*Ayush Roy,Samin Enam,Jun Xia,Vishnu Suresh Lokhande,Won Hwa Kim*

Main category: cs.CV

TL;DR: 数据稀缺是医学影像的挑战。数据添加可能引发“数据添加困境”。本文提出一种基于因果框架的方法来控制特征差异，以提高模型性能，并在多种数据集上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 在医学影像领域，数据稀缺是深度学习模型面临的主要挑战。数据池化和数据添加虽然能提升模型性能，但也可能引发分布偏移，即“数据添加困境”。

Method: 提出了一种用于控制深度网络所有层的前景-背景特征差异的方法，该方法借鉴了因果框架。

Result: 该方法在组织病理学和超声图像上实现了最先进的分割性能，并且在三种模型架构中均优于主要基线。

Conclusion: 该方法在五个数据集上实现了最先进的分割性能，包括一个新近整理的超声数据集。定性结果表明，与三种模型架构中的主要基线相比，分割图更精细、更准确。

Abstract: Data scarcity is a major challenge in medical imaging, particularly for deep
learning models. While data pooling (combining datasets from multiple sources)
and data addition (adding more data from a new dataset) have been shown to
enhance model performance, they are not without complications. Specifically,
increasing the size of the training dataset through pooling or addition can
induce distributional shifts, negatively affecting downstream model
performance, a phenomenon known as the "Data Addition Dilemma". While the
traditional i.i.d. assumption may not hold in multi-source contexts, assuming
exchangeability across datasets provides a more practical framework for data
pooling. In this work, we investigate medical image segmentation under these
conditions, drawing insights from causal frameworks to propose a method for
controlling foreground-background feature discrepancies across all layers of
deep networks. This approach improves feature representations, which are
crucial in data-addition scenarios. Our method achieves state-of-the-art
segmentation performance on histopathology and ultrasound images across five
datasets, including a novel ultrasound dataset that we have curated and
contributed. Qualitative results demonstrate more refined and accurate
segmentation maps compared to prominent baselines across three model
architectures. The code will be available on Github.

</details>


### [3] [Smaller, Faster, Cheaper: Architectural Designs for Efficient Machine Learning](https://arxiv.org/abs/2507.19795)
*Steven Walton*

Main category: cs.CV

TL;DR: 本文研究了如何通过优化数据处理、修改模型架构（如视觉 Transformers 的受限注意力）以及利用归一化流的特性，来设计出更小、更快、更便宜的机器学习模型，以满足资源受限环境的需求。


<details>
  <summary>Details</summary>
Motivation: 随着模型在资源受限环境中的部署需求日益增长，需要能够提供高性能但计算资源需求更少的架构。

Method: 本文从三个方向探讨了提高模型性能并减少计算需求的设计原则：1. 数据输入输出，研究信息如何在核心神经处理单元中传递以及如何检索。2. 核心神经架构的修改，应用于视觉 Transformers 中的受限注意力，探索移除统一上下文窗口如何增加基础神经架构的表达能力。3. 归一化流的自然结构，研究如何利用这些特性来更好地提炼模型知识。

Result: 本文的贡献表明，仔细设计神经网络架构可以提高机器学习算法的效率。

Conclusion: 通过仔细设计神经网络架构，可以提高机器学习算法的效率，使其更小、更快、更便宜。

Abstract: Major advancements in the capabilities of computer vision models have been
primarily fueled by rapid expansion of datasets, model parameters, and
computational budgets, leading to ever-increasing demands on computational
infrastructure. However, as these models are deployed in increasingly diverse
and resource-constrained environments, there is a pressing need for
architectures that can deliver high performance while requiring fewer
computational resources.
  This dissertation focuses on architectural principles through which models
can achieve increased performance while reducing their computational demands.
We discuss strides towards this goal through three directions. First, we focus
on data ingress and egress, investigating how information may be passed into
and retrieved from our core neural processing units. This ensures that our
models make the most of available data, allowing smaller architectures to
become more performant. Second, we investigate modifications to the core neural
architecture, applied to restricted attention in vision transformers. This
section explores how removing uniform context windows in restricted attention
increases the expressivity of the underlying neural architecture. Third, we
explore the natural structures of Normalizing Flows and how we can leverage
these properties to better distill model knowledge.
  These contributions demonstrate that careful design of neural architectures
can increase the efficiency of machine learning algorithms, allowing them to
become smaller, faster, and cheaper.

</details>


### [4] [T-MPEDNet: Unveiling the Synergy of Transformer-aware Multiscale Progressive Encoder-Decoder Network with Feature Recalibration for Tumor and Liver Segmentation](https://arxiv.org/abs/2507.19590)
*Chandravardhan Singh Raghaw,Jasmer Singh Sanjotra,Mohammad Zia Ur Rehman,Shubhi Bansal,Shahid Shafi Dar,Nagendra Kumar*

Main category: cs.CV

TL;DR: T-MPEDNet是一种新的深度学习模型，可以精确分割CT扫描中的肝脏和肿瘤，并在基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然自动肝脏和肿瘤分割面临肿瘤内在异质性和患者肝脏多样化视觉特征带来的重大挑战，但肝脏及其肿瘤在CT扫描中的精确自动分割在快速诊断和制定最佳治疗计划方面起着关键作用。

Method: 提出了一种新颖的、用于肝脏和肿瘤自动分割的Transformer感知的多尺度渐进式编码器-解码器网络（T-MPEDNet）。T-MPEDNet利用渐进式编码器-解码器结构通过深度自适应特征骨干，并通过跳跃连接进行增强，以重新校准通道特征，同时保持空间完整性。受Transformer启发的动态注意机制捕捉空间域中的长距离上下文关系，并通过多尺度特征利用进一步增强，以优化局部细节，从而实现精确预测。然后采用形态边界细化来处理与邻近器官模糊的边界，捕捉更精细的细节并产生精确的边界标签。

Result: T-MPEDNet在两个广泛使用的公共基准数据集LiTS和3DIRCADb上进行了全面评估。大量的定量和定性分析表明，T-MPEDNet在肝脏和肿瘤分割方面优于12种最先进的方法。在LiTS上，T-MPEDNet分别实现了97.6%和89.1%的Dice相似系数（DSC）。在3DIRCADb上，肝脏和肿瘤分割的DSC分别为98.3%和83.3%。

Conclusion: T-MPEDNet是一个有效的、可靠的框架，用于CT扫描中肝脏及其肿瘤的自动分割。

Abstract: Precise and automated segmentation of the liver and its tumor within CT scans
plays a pivotal role in swift diagnosis and the development of optimal
treatment plans for individuals with liver diseases and malignancies. However,
automated liver and tumor segmentation faces significant hurdles arising from
the inherent heterogeneity of tumors and the diverse visual characteristics of
livers across a broad spectrum of patients. Aiming to address these challenges,
we present a novel Transformer-aware Multiscale Progressive Encoder-Decoder
Network (T-MPEDNet) for automated segmentation of tumor and liver. T-MPEDNet
leverages a deep adaptive features backbone through a progressive
encoder-decoder structure, enhanced by skip connections for recalibrating
channel-wise features while preserving spatial integrity. A
Transformer-inspired dynamic attention mechanism captures long-range contextual
relationships within the spatial domain, further enhanced by multi-scale
feature utilization for refined local details, leading to accurate prediction.
Morphological boundary refinement is then employed to address indistinct
boundaries with neighboring organs, capturing finer details and yielding
precise boundary labels. The efficacy of T-MPEDNet is comprehensively assessed
on two widely utilized public benchmark datasets, LiTS and 3DIRCADb. Extensive
quantitative and qualitative analyses demonstrate the superiority of T-MPEDNet
compared to twelve state-of-the-art methods. On LiTS, T-MPEDNet achieves
outstanding Dice Similarity Coefficients (DSC) of 97.6% and 89.1% for liver and
tumor segmentation, respectively. Similar performance is observed on 3DIRCADb,
with DSCs of 98.3% and 83.3% for liver and tumor segmentation, respectively.
Our findings prove that T-MPEDNet is an efficacious and reliable framework for
automated segmentation of the liver and its tumor in CT scans.

</details>


### [5] [SurgPIS: Surgical-instrument-level Instances and Part-level Semantics for Weakly-supervised Part-aware Instance Segmentation](https://arxiv.org/abs/2507.19592)
*Meng Wei,Charlie Budd,Oluwatosin Alabi,Miaojing Shi,Tom Vercauteren*

Main category: cs.CV

TL;DR: SurgPIS 提出了首个用于手术器械的部件感知实例分割（PIS）模型，通过将实例和部件分割统一起来，并采用弱监督学习策略处理缺乏大规模标注数据集的问题，在多项分割任务上均实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅分别处理器械级实例分割（IIS）或部件级语义分割（PSS），缺乏任务间的交互。本研究旨在解决这一问题，将手术器械分割统一为部件感知实例分割（PIS）问题。

Method: 本研究将手术工具分割定义为统一的部件感知实例分割（PIS）问题，并引入了首个用于手术器械的 PIS 模型 SurgPIS。该方法采用基于 Transformer 的掩码分类方法，并引入了源自器械级对象查询的部件特定查询，明确地将部件与其父器械实例相关联。为了解决缺乏同时具有实例级和部件级标签的大规模数据集的问题，本研究提出了一种用于 SurgPIS 的弱监督学习策略，可以从仅标记了 IIS 或 PSS 的不相交数据集中学习。在训练过程中，将 PIS 预测聚合为 IIS 或 PSS 掩码，从而可以针对部分标记的数据集计算损失。此外，还开发了一种师生方法来维护部分标记数据中缺失的 PIS 信息的预测一致性，例如 IIS 标记数据中的部分信息。

Result: SurgPIS 在 PIS、IIS、PSS 和仪器级语义分割方面均取得了最先进的性能。

Conclusion: SurgPIS 在 PIS、IIS、PSS 和仪器级语义分割方面均达到了最先进的性能。

Abstract: Consistent surgical instrument segmentation is critical for automation in
robot-assisted surgery. Yet, existing methods only treat instrument-level
instance segmentation (IIS) or part-level semantic segmentation (PSS)
separately, without interaction between these tasks. In this work, we formulate
a surgical tool segmentation as a unified part-aware instance segmentation
(PIS) problem and introduce SurgPIS, the first PIS model for surgical
instruments. Our method adopts a transformer-based mask classification approach
and introduces part-specific queries derived from instrument-level object
queries, explicitly linking parts to their parent instrument instances. In
order to address the lack of large-scale datasets with both instance- and
part-level labels, we propose a weakly-supervised learning strategy for SurgPIS
to learn from disjoint datasets labelled for either IIS or PSS purposes. During
training, we aggregate our PIS predictions into IIS or PSS masks, thereby
allowing us to compute a loss against partially labelled datasets. A
student-teacher approach is developed to maintain prediction consistency for
missing PIS information in the partially labelled data, e.g., parts of the IIS
labelled data. Extensive experiments across multiple datasets validate the
effectiveness of SurgPIS, achieving state-of-the-art performance in PIS as well
as IIS, PSS, and instrument-level semantic segmentation.

</details>


### [6] [Object-centric Video Question Answering with Visual Grounding and Referring](https://arxiv.org/abs/2507.19599)
*Haochen Wang,Qirui Chen,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Yao Hu,Weidi Xie,Stratis Gavves*

Main category: cs.CV

TL;DR: 本文提出了一种新的VideoLLM模型，支持文本和视觉提示交互，并引入了STOM模块和VideoInfer数据集，在视频问答和分割任务上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频大语言模型（VideoLLMs）在视频理解方面取得了显著进展，但主要关注高层理解且仅限于文本响应，这在对象级、多轮交互方面存在局限性。

Method: 本文提出了一种新的VideoLLM模型，能够进行输入时的物体指代和输出时的视觉接地，实现了用户通过文本和视觉提示与视频进行交互。引入了空间时间叠加模块（STOM），能够将任意时间点输入的视觉提示传播到视频的其余帧。此外，还构建了一个名为VideoInfer的数据集，该数据集包含面向对象的视频指令和需要推理的问答对。

Result: 在VideoInfer和其他现有基准的12个基准测试（涵盖6项任务）上进行的综合实验表明，本研究提出的模型在视频问答和分割任务上均持续优于基线模型。

Conclusion: 本研究提出的模型在视频问答和分割任务上均优于现有基线模型，证明了其在多模态、面向对象的视频和图像理解方面的鲁棒性。

Abstract: Video Large Language Models (VideoLLMs) have recently demonstrated remarkable
progress in general video understanding. However, existing models primarily
focus on high-level comprehension and are limited to text-only responses,
restricting the flexibility for object-centric, multiround interactions. In
this paper, we make three contributions: (i) we address these limitations by
introducing a VideoLLM model, capable of performing both object referring for
input and grounding for output in video reasoning tasks, i.e., allowing users
to interact with videos using both textual and visual prompts; (ii) we propose
STOM (Spatial-Temporal Overlay Module), a novel approach that propagates
arbitrary visual prompts input at any single timestamp to the remaining frames
within a video; (iii) we present VideoInfer, a manually curated object-centric
video instruction dataset featuring questionanswering pairs that require
reasoning. We conduct comprehensive experiments on VideoInfer and other
existing benchmarks across video question answering and referring object
segmentation. The results on 12 benchmarks of 6 tasks show that our proposed
model consistently outperforms baselines in both video question answering and
segmentation, underscoring its robustness in multimodal, object-centric video
and image understanding. Project page:
https://qirui-chen.github.io/RGA3-release/.

</details>


### [7] [Exemplar Med-DETR: Toward Generalized and Robust Lesion Detection in Mammogram Images and beyond](https://arxiv.org/abs/2507.19621)
*Sheethal Bhat,Bogdan Georgescu,Adarsh Bhandary Panambur,Mathias Zinnen,Tri-Thien Nguyen,Awais Mansoor,Karim Khalifa Elbarbary,Siming Bayer,Florin-Cristian Ghesu,Sasa Grbic,Andreas Maier*

Main category: cs.CV

TL;DR: Exemplar Med-DETR是一种新颖的多模态对比检测器，通过交叉注意力和迭代策略来学习特定类别特征，在乳腺钼靶检查、胸部X光片和血管造影的异常检测任务中均取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的检测方法难以学习有效的特定类别特征，限制了它们在不同任务和成像模态中的应用。医学图像中的异常检测具有独特的挑战，因为特征表示存在差异，并且解剖结构与异常之间存在复杂的关??系。在乳腺钼靶检查中，致密组织会遮盖病变，使放射学解释复杂化。

Method: 提出了一种新颖的多模态对比检测器Exemplar Med-DETR，它采用具有固有的、直观的特定类别示例特征的交叉注意力，并通过迭代策略进行训练。

Result: Exemplar Med-DETR 在三种不同的成像模态和四个公共数据集上实现了最先进的性能，在越南致密乳腺钼靶检查中，肿块检测和钙化检测的 mAP 分别为 0.7 和 0.55，提高了 16 个百分点。对中国人群的 100 张乳腺钼靶检查进行的评估显示，病变检测性能提高了两倍。对于胸部 X 光片和血管造影，我们实现了 0.25 的肿块检测 mAP 和 0.37 的狭窄检测 mAP，分别提高了 4 和 7 个百分点。

Conclusion: 该方法在三种不同的成像模态和四个公共数据集上实现了最先进的性能，在越南致密乳腺钼靶检查中，肿块检测和钙化检测的mAP分别为0.7和0.55，提高了16个百分点。此外，对来自非分布内中国人群的100张乳腺钼靶检查进行的放射科医生支持的评估显示，病变检测性能提高了一倍。对于胸部X光片和血管造影，我们实现了0.25的肿块检测mAP和0.37的狭窄检测mAP，分别提高了4和7个百分点。

Abstract: Detecting abnormalities in medical images poses unique challenges due to
differences in feature representations and the intricate relationship between
anatomical structures and abnormalities. This is especially evident in
mammography, where dense breast tissue can obscure lesions, complicating
radiological interpretation. Despite leveraging anatomical and semantic
context, existing detection methods struggle to learn effective class-specific
features, limiting their applicability across different tasks and imaging
modalities. In this work, we introduce Exemplar Med-DETR, a novel multi-modal
contrastive detector that enables feature-based detection. It employs
cross-attention with inherently derived, intuitive class-specific exemplar
features and is trained with an iterative strategy. We achieve state-of-the-art
performance across three distinct imaging modalities from four public datasets.
On Vietnamese dense breast mammograms, we attain an mAP of 0.7 for mass
detection and 0.55 for calcifications, yielding an absolute improvement of 16
percentage points. Additionally, a radiologist-supported evaluation of 100
mammograms from an out-of-distribution Chinese cohort demonstrates a twofold
gain in lesion detection performance. For chest X-rays and angiography, we
achieve an mAP of 0.25 for mass and 0.37 for stenosis detection, improving
results by 4 and 7 percentage points, respectively. These results highlight the
potential of our approach to advance robust and generalizable detection systems
for medical imaging.

</details>


### [8] [Endoscopic Depth Estimation Based on Deep Learning: A Survey](https://arxiv.org/abs/2507.20881)
*Ke Niu,Zeyun Liu,Xue Feng,Heng Li,Kaize Shi*

Main category: cs.CV

TL;DR: 本文对内窥镜深度估计领域的深度学习技术进行了全面的文献综述，重点介绍了数据、方法、应用及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 内窥镜深度估计是提高微创手术安全性和精密度的一项关键技术，在医学影像、计算机视觉和机器人领域引起了广泛关注。尽管已有相关综述，但缺乏对近期基于深度学习的技术进行全面概述的文献，因此本文旨在弥补这一空白。

Method: 本文系统地回顾了内窥镜深度估计的最新深度学习技术，涵盖了数据、方法和应用。对常用性能评估指标和公开数据集进行了描述。分析了内窥镜图像的挑战，并根据监督策略和网络架构对技术进行了分类。此外，还回顾了内窥镜深度估计在机器人辅助手术中的应用。

Result: 该综述对内窥镜深度估计的最新深度学习方法进行了全面梳理，从数据、方法、应用等多个角度进行了分析，并指出了未来研究方向，为该领域的深入研究提供了参考。

Conclusion: 该论文系统地回顾了内窥镜深度估计领域的最新深度学习技术，分析了数据、方法和应用，并讨论了机器人辅助手术中的应用。最后，论文提出了包括域适应、实时实现和模型泛化增强在内的未来研究方向。

Abstract: Endoscopic depth estimation is a critical technology for improving the safety
and precision of minimally invasive surgery. It has attracted considerable
attention from researchers in medical imaging, computer vision, and robotics.
Over the past decade, a large number of methods have been developed. Despite
the existence of several related surveys, a comprehensive overview focusing on
recent deep learning-based techniques is still limited. This paper endeavors to
bridge this gap by systematically reviewing the state-of-the-art literature.
Specifically, we provide a thorough survey of the field from three key
perspectives: data, methods, and applications, covering a range of methods
including both monocular and stereo approaches. We describe common performance
evaluation metrics and summarize publicly available datasets. Furthermore, this
review analyzes the specific challenges of endoscopic scenes and categorizes
representative techniques based on their supervision strategies and network
architectures. The application of endoscopic depth estimation in the important
area of robot-assisted surgery is also reviewed. Finally, we outline potential
directions for future research, such as domain adaptation, real-time
implementation, and enhanced model generalization, thereby providing a valuable
starting point for researchers to engage with and advance the field.

</details>


### [9] [Pre- and Post-Treatment Glioma Segmentation with the Medical Imaging Segmentation Toolkit](https://arxiv.org/abs/2507.19626)
*Adrian Celaya,Tucker Netherton,Dawid Schellingerhout,Caroline Chung,Beatrice Riviere,David Fuentes*

Main category: cs.CV

TL;DR: MIST是一个用于医学图像分割的工具包，其灵活的后处理框架支持用户定义的策略，可以快速实验和优化分割结果，为BraTS 2025挑战赛生成高质量分割。


<details>
  <summary>Details</summary>
Motivation: 为了解决医学图像分割领域中，由于缺乏标准化和可定制的工具而导致的严格方法比较的挑战。

Method: 提出了一种灵活且模块化的后处理框架，该框架能够支持多种转换操作，如删除或替换小目标、提取最大连通分量以及进行孔洞填充和闭运算等形态学操作。用户可以将这些转换操作组合成自定义策略，从而对最终的分割结果进行精细控制。

Result: 评估了三种不同的策略，范围从简单的小目标移除到更复杂、特定类别的流程，并使用BraTS排名协议对它们的性能进行了排名。

Conclusion: MIST的后处理框架能够帮助研究人员快速进行实验和有针对性的优化，从而为BraTS 2025挑战赛生成高质量的分割结果。MIST仍然是开源且可扩展的，支持医学图像分割领域可复现和可扩展的研究。

Abstract: Medical image segmentation continues to advance rapidly, yet rigorous
comparison between methods remains challenging due to a lack of standardized
and customizable tooling. In this work, we present the current state of the
Medical Imaging Segmentation Toolkit (MIST), with a particular focus on its
flexible and modular postprocessing framework designed for the BraTS 2025 pre-
and post-treatment glioma segmentation challenge. Since its debut in the 2024
BraTS adult glioma post-treatment segmentation challenge, MIST's postprocessing
module has been significantly extended to support a wide range of transforms,
including removal or replacement of small objects, extraction of the largest
connected components, and morphological operations such as hole filling and
closing. These transforms can be composed into user-defined strategies,
enabling fine-grained control over the final segmentation output. We evaluate
three such strategies - ranging from simple small-object removal to more
complex, class-specific pipelines - and rank their performance using the BraTS
ranking protocol. Our results highlight how MIST facilitates rapid
experimentation and targeted refinement, ultimately producing high-quality
segmentations for the BraTS 2025 challenge. MIST remains open source and
extensible, supporting reproducible and scalable research in medical image
segmentation.

</details>


### [10] [Local Prompt Adaptation for Style-Consistent Multi-Object Generation in Diffusion Models](https://arxiv.org/abs/2507.20094)
*Ankit Sanjyal*

Main category: cs.CV

TL;DR: LPA是一种无需训练的文本到图像生成方法，通过将提示内容和样式分离并选择性地注入模型，解决了复杂提示下的样式不统一和空间不相干问题，提升了生成图像的控制性和一致性。


<details>
  <summary>Details</summary>
Motivation: 标准的扩散模型在处理涉及多个对象和全局或局部样式规范的复杂提示时，生成的场景往往缺乏样式统一性和空间相干性，限制了它们在创意和可控内容生成中的应用。

Method: LPA是一种简单的、无需训练的架构方法，它将提示分解为内容和样式令牌，并有选择地将它们注入U-Net的注意力层中的不同阶段。通过在生成过程中早期对对象令牌进行条件化，后期对样式令牌进行条件化，LPA可以增强布局控制和样式一致性。

Result: 在对50个包含丰富样式的提示（涵盖五个类别）的自定义基准测试中，LPA与Composer、MultiDiffusion、Attend-and-Excite、LoRA和SDXL等强基线进行了比较。结果表明，LPA在CLIP分数和样式一致性指标上均优于现有方法。

Conclusion: LPA通过将提示分解为内容和样式令牌，并有选择地将其注入U-Net的注意力层，从而提高了布局控制和样式一致性，在CLIP分数和样式一致性指标上优于先前的工作，为可控、富有表现力的基于扩散的生成提供了新的方向。

Abstract: Diffusion models have become a powerful backbone for text-to-image
generation, enabling users to synthesize high-quality visuals from natural
language prompts. However, they often struggle with complex prompts involving
multiple objects and global or local style specifications. In such cases, the
generated scenes tend to lack style uniformity and spatial coherence, limiting
their utility in creative and controllable content generation. In this paper,
we propose a simple, training-free architectural method called Local Prompt
Adaptation (LPA). Our method decomposes the prompt into content and style
tokens, and injects them selectively into the U-Net's attention layers at
different stages. By conditioning object tokens early and style tokens later in
the generation process, LPA enhances both layout control and stylistic
consistency. We evaluate our method on a custom benchmark of 50 style-rich
prompts across five categories and compare against strong baselines including
Composer, MultiDiffusion, Attend-and-Excite, LoRA, and SDXL. Our approach
outperforms prior work on both CLIP score and style consistency metrics,
offering a new direction for controllable, expressive diffusion-based
generation.

</details>


### [11] [SynPAIN: A Synthetic Dataset of Pain and Non-Pain Facial Expressions](https://arxiv.org/abs/2507.19673)
*Babak Taati,Muhammad Muzammil,Yasamin Zarghami,Abhishek Moturu,Airhossein Kazerouni,Hailey Reimer,Alex Mihailidis,Thomas Hadjistavropoulos*

Main category: cs.CV

TL;DR: 由于现有数据集在种族/民族多样性、隐私和老年人代表性方面存在不足，研究人员创建了一个名为SynPAIN的大型合成数据集，其中包含10,710张面部表情图像，涵盖了不同的年龄、性别和种族。该数据集旨在改善老年人疼痛检测，并已成功用于识别和解决现有疼痛检测模型的算法偏倚，同时通过数据增强将模型性能提高了7.0%。


<details>
  <summary>Details</summary>
Motivation: 准确评估那些沟通能力有限的患者（如患有痴呆症的老年人）的疼痛，在医疗保健方面是一个严峻的挑战。现有的疼痛检测数据集在种族/民族多样性、隐私限制和老年人（临床部署的主要目标人群）的代表性不足方面存在局限性。

Method: SynPAIN是一个大型合成数据集，包含10,710张面部表情图像（5,355个中性/表情对），涵盖五个种族/民族、两个年龄组（年轻：20-35岁，年长：75岁以上）和两个性别。使用商业生成式人工智能工具创建了人口统计学上平衡的合成身份，并带有临床上有意义的疼痛表情。

Result: SynPAIN的验证表明，合成疼痛表情表现出预期的疼痛模式，在使用基于面部动作单元分析的临床验证疼痛评估工具时，其得分显著高于中性表情和非疼痛表情。SynPAIN在识别现有疼痛检测模型中的算法偏倚方面具有实用性，通过全面的偏倚评估，揭示了人口统计学特征的显著性能差异。此外，通过年龄匹配的合成数据增强，在真实临床数据上将疼痛检测性能提高了7.0%的平均精度。

Conclusion: SynPAIN是第一个公开的、人口统计学上多样化的合成数据集，专门为老年人疼痛检测设计，并为衡量和减轻算法偏倚建立了一个框架。

Abstract: Accurate pain assessment in patients with limited ability to communicate,
such as older adults with dementia, represents a critical healthcare challenge.
Robust automated systems of pain detection may facilitate such assessments.
Existing pain detection datasets, however, suffer from limited ethnic/racial
diversity, privacy constraints, and underrepresentation of older adults who are
the primary target population for clinical deployment. We present SynPAIN, a
large-scale synthetic dataset containing 10,710 facial expression images (5,355
neutral/expressive pairs) across five ethnicities/races, two age groups (young:
20-35, old: 75+), and two genders. Using commercial generative AI tools, we
created demographically balanced synthetic identities with clinically
meaningful pain expressions. Our validation demonstrates that synthetic pain
expressions exhibit expected pain patterns, scoring significantly higher than
neutral and non-pain expressions using clinically validated pain assessment
tools based on facial action unit analysis. We experimentally demonstrate
SynPAIN's utility in identifying algorithmic bias in existing pain detection
models. Through comprehensive bias evaluation, we reveal substantial
performance disparities across demographic characteristics. These performance
disparities were previously undetectable with smaller, less diverse datasets.
Furthermore, we demonstrate that age-matched synthetic data augmentation
improves pain detection performance on real clinical data, achieving a 7.0%
improvement in average precision. SynPAIN addresses critical gaps in pain
assessment research by providing the first publicly available, demographically
diverse synthetic dataset specifically designed for older adult pain detection,
while establishing a framework for measuring and mitigating algorithmic bias.
The dataset is available at https://doi.org/10.5683/SP3/WCXMAP

</details>


### [12] [Efficient Learning for Product Attributes with Compact Multimodal Models](https://arxiv.org/abs/2507.19679)
*Mandar Kulkarni*

Main category: cs.CV

TL;DR: 通过PEFT和DPO利用无标签数据改进了电子商务中的图像产品属性预测，相比监督模型效果更佳且效率更高。


<details>
  <summary>Details</summary>
Motivation: 电子商务中的图像产品属性预测是一项关键任务，具有广泛的应用。然而，视觉语言模型（VLMs）的监督微调由于手动或API基础注释的成本而面临显著的规模挑战。因此，本研究旨在探索用于紧凑型VLMs（2B-3B参数）的标签高效半监督微调策略，利用无标签产品列表通过直接偏好优化（DPO）。

Method: 本研究首先使用PEFT训练低秩适配器模块，然后利用无标签数据，通过生成多个推理-答案链并基于自洽性将它们分离为首选和非首选，来更新适配器权重。随后，使用DPO损失对模型进行微调，并使用更新后的模型进行下一次迭代。

Result: DPO微调利用无标签数据，在超过12个电子商务垂直领域的的数据集上，相比于监督模型取得了显著的改进。准确性随无标签数据的增加而提高。

Conclusion: 该方法通过PEFT和DPO实现了高效收敛和最小的计算开销。在涵盖十二个电子商务垂直领域的的数据集上，仅使用无标签数据的DPO微调显示出比有监督模型显著的改进。此外，实验证明，随着更多无标签数据的增加，DPO训练的准确性得到提高，这表明可以有效地利用大量的无标签样本来提高性能。

Abstract: Image-based product attribute prediction in e-commerce is a crucial task with
numerous applications. The supervised fine-tuning of Vision Language Models
(VLMs) faces significant scale challenges due to the cost of manual or API
based annotation. In this paper, we investigate label-efficient semi-supervised
fine-tuning strategies for compact VLMs (2B-3B parameters) that leverage
unlabeled product listings through Direct Preference Optimization (DPO).
Beginning with a small, API-based, annotated, and labeled set, we first employ
PEFT to train low-rank adapter modules. To update the adapter weights with
unlabeled data, we generate multiple reasoning-and-answer chains per unlabeled
sample and segregate these chains into preferred and dispreferred based on
self-consistency. We then fine-tune the model with DPO loss and use the updated
model for the next iteration. By using PEFT fine-tuning with DPO, our method
achieves efficient convergence with minimal compute overhead. On a dataset
spanning twelve e-commerce verticals, DPO-based fine-tuning, which utilizes
only unlabeled data, demonstrates a significant improvement over the supervised
model. Moreover, experiments demonstrate that accuracy with DPO training
improves with more unlabeled data, indicating that a large pool of unlabeled
samples can be effectively leveraged to improve performance.

</details>


### [13] [DeepJIVE: Learning Joint and Individual Variation Explained from Multimodal Data Using Deep Learning](https://arxiv.org/abs/2507.19682)
*Matthew Drexler,Benjamin Risk,James J Lah,Suprateek Kundu,Deqiang Qiu*

Main category: cs.CV

TL;DR: DeepJIVE is a new deep-learning method for multimodal data analysis that overcomes limitations of conventional methods by handling high-dimensional and nonlinear data. It successfully identified patterns in Alzheimer's disease data.


<details>
  <summary>Details</summary>
Motivation: Conventional multimodal data integration methods suffer from limitations such as the inability to handle high-dimensional data and identify nonlinear structures.

Method: DeepJIVE, a deep-learning approach to performing Joint and Individual Variance Explained (JIVE). Explored different strategies of achieving the identity and orthogonality constraints for DeepJIVE, resulting in three viable loss functions.

Result: DeepJIVE can successfully uncover joint and individual variations of multimodal datasets. Application to ADNI identified biologically plausible covariation patterns between amyloid PET and MR images.

Conclusion: DeepJIVE is a useful tool for multimodal data analysis.

Abstract: Conventional multimodal data integration methods provide a comprehensive
assessment of the shared or unique structure within each individual data type
but suffer from several limitations such as the inability to handle
high-dimensional data and identify nonlinear structures. In this paper, we
introduce DeepJIVE, a deep-learning approach to performing Joint and Individual
Variance Explained (JIVE). We perform mathematical derivation and experimental
validations using both synthetic and real-world 1D, 2D, and 3D datasets.
Different strategies of achieving the identity and orthogonality constraints
for DeepJIVE were explored, resulting in three viable loss functions. We found
that DeepJIVE can successfully uncover joint and individual variations of
multimodal datasets. Our application of DeepJIVE to the Alzheimer's Disease
Neuroimaging Initiative (ADNI) also identified biologically plausible
covariation patterns between the amyloid positron emission tomography (PET) and
magnetic resonance (MR) images. In conclusion, the proposed DeepJIVE can be a
useful tool for multimodal data analysis.

</details>


### [14] [Co-Win: Joint Object Detection and Instance Segmentation in LiDAR Point Clouds via Collaborative Window Processing](https://arxiv.org/abs/2507.19691)
*Haichuan Li,Tomi Westerlund*

Main category: cs.CV

TL;DR: Co-Win：一种用于自动驾驶的鸟瞰图感知框架，通过点云编码和窗口特征提取实现精确的场景理解。


<details>
  <summary>Details</summary>
Motivation: 为了应对复杂城市环境中自动驾驶导航的安全性和效率挑战，需要更精确的感知和场景理解能力。

Method: Co-Win框架采用分层架构，包括专门的编码器、基于窗口的骨干网络和基于查询的解码器头，以有效捕捉多样的空间特征和对象关系。该方法采用变分方法和基于掩码的实例分割，实现了细粒度的场景分解和理解，并对点云数据进行渐进式特征提取，确保预测掩码的数据一致性和上下文相关性。

Result: Co-Win框架能够处理点云数据，生成数据一致且与上下文相关的预测掩码，并产生可解释和多样化的实例预测，从而增强了下游的自动驾驶决策和规划能力。

Conclusion: Co-Win框架通过结合点云编码和高效的并行窗口特征提取，实现了对复杂城市环境中多模态信息的精确感知和场景理解，为自动驾驶提供了更安全、更智能的解决方案。

Abstract: Accurate perception and scene understanding in complex urban environments is
a critical challenge for ensuring safe and efficient autonomous navigation. In
this paper, we present Co-Win, a novel bird's eye view (BEV) perception
framework that integrates point cloud encoding with efficient parallel
window-based feature extraction to address the multi-modality inherent in
environmental understanding. Our method employs a hierarchical architecture
comprising a specialized encoder, a window-based backbone, and a query-based
decoder head to effectively capture diverse spatial features and object
relationships. Unlike prior approaches that treat perception as a simple
regression task, our framework incorporates a variational approach with
mask-based instance segmentation, enabling fine-grained scene decomposition and
understanding. The Co-Win architecture processes point cloud data through
progressive feature extraction stages, ensuring that predicted masks are both
data-consistent and contextually relevant. Furthermore, our method produces
interpretable and diverse instance predictions, enabling enhanced downstream
decision-making and planning in autonomous driving systems.

</details>


### [15] [Bias Analysis for Synthetic Face Detection: A Case Study of the Impact of Facial Attribute](https://arxiv.org/abs/2507.19705)
*Asmae Lamsaf,Lucia Cascone,Hugo Proença,João Neves*

Main category: cs.CV

TL;DR: 合成人脸检测器存在面部属性偏差，本研究提出了评估框架并进行了案例分析，揭示了偏差来源。


<details>
  <summary>Details</summary>
Motivation: 现有合成人脸检测模型和数据集在识别合成内容方面取得了进展，但忽略了模型和数据集可能存在的偏差问题，这可能导致在某些人群的检测失败，并引发重大的社会、法律和伦理问题。

Method: 提出了一种利用合成数据生成和面部属性标签均衡化的评估框架，并利用该框架对五种先进的合成人脸检测器进行了广泛的案例研究，分析了其在25种受控面部属性下的偏差水平。

Result: 研究结果证实，合成人脸检测器普遍存在对特定面部属性的偏见。此外，通过分析训练集中面部属性的平衡性以及检测器在受控属性修改图像对上的激活图，揭示了观测到的偏见的来源。

Conclusion: 该研究提出的评估框架有助于分析合成人脸检测器在不同面部属性上的偏差，并深入探讨了偏差的来源。

Abstract: Bias analysis for synthetic face detection is bound to become a critical
topic in the coming years. Although many detection models have been developed
and several datasets have been released to reliably identify synthetic content,
one crucial aspect has been largely overlooked: these models and training
datasets can be biased, leading to failures in detection for certain
demographic groups and raising significant social, legal, and ethical issues.
In this work, we introduce an evaluation framework to contribute to the
analysis of bias of synthetic face detectors with respect to several facial
attributes. This framework exploits synthetic data generation, with evenly
distributed attribute labels, for mitigating any skew in the data that could
otherwise influence the outcomes of bias analysis. We build on the proposed
framework to provide an extensive case study of the bias level of five
state-of-the-art detectors in synthetic datasets with 25 controlled facial
attributes. While the results confirm that, in general, synthetic face
detectors are biased towards the presence/absence of specific facial
attributes, our study also sheds light on the origins of the observed bias
through the analysis of the correlations with the balancing of facial
attributes in the training sets of the detectors, and the analysis of detectors
activation maps in image pairs with controlled attribute modifications.

</details>


### [16] [Quaternion-Based Robust PCA for Efficient Moving Target Detection and Background Recovery in Color Videos](https://arxiv.org/abs/2507.19730)
*Liyang Wang,Shiqian Wu,Shun Fang,Qile Zhu,Jiaxin Wu,Sos Again*

Main category: cs.CV

TL;DR: 提出 uQRPCA+ 框架，通过优化的 QSVD 和 CR1B 方法，在移动目标检测和背景恢复方面取得 SOTA 性能，解决了 QRPCA 在彩色视频处理中的计算复杂度和彩色通道问题。


<details>
  <summary>Details</summary>
Motivation: 移动目标检测是计算机视觉中的一个挑战性任务。如果背景和目标能够被同时提取和重组，则合成数据可以极大地丰富带有标注的野外数据集，并增强深度模型的泛化能力。QRPCA 是一个有前景的无监督范式，但其在彩色视频处理中的 QSVD 计算成本高，并且秩-1 四元数矩阵未能产生秩-1 的彩色通道。

Method: 通过利用四元数黎曼流形将 QSVD 的计算复杂度降低到 O(1)，并提出 universal QRPCA (uQRPCA) 框架，实现了目标分割和背景恢复的平衡。进一步引入 Color Rank-1 Batch (CR1B) 方法，将 uQRPCA 扩展到 uQRPCA+，以获得跨颜色通道的理想低秩背景。

Result: uQRPCA+ 在移动目标检测和背景恢复任务上相比现有开源方法达到了 SOTA 性能。

Conclusion: uQRPCA+ 在移动目标检测和背景恢复任务上取得了 SOTA 性能，优于现有的开源方法。

Abstract: Moving target detection is a challenging computer vision task aimed at
generating accurate segmentation maps in diverse in-the-wild color videos
captured by static cameras. If backgrounds and targets can be simultaneously
extracted and recombined, such synthetic data can significantly enrich
annotated in-the-wild datasets and enhance the generalization ability of deep
models. Quaternion-based RPCA (QRPCA) is a promising unsupervised paradigm for
color image processing. However, in color video processing, Quaternion Singular
Value Decomposition (QSVD) incurs high computational costs, and rank-1
quaternion matrix fails to yield rank-1 color channels. In this paper, we
reduce the computational complexity of QSVD to o(1) by utilizing a quaternion
Riemannian manifold. Furthermor, we propose the universal QRPCA (uQRPCA)
framework, which achieves a balance in simultaneously segmenting targets and
recovering backgrounds from color videos. Moreover, we expand to uQRPCA+ by
introducing the Color Rank-1 Batch (CR1B) method to further process and obtain
the ideal low-rank background across color channels. Experiments demonstrate
our uQRPCA+ achieves State Of The Art (SOTA) performance on moving target
detection and background recovery tasks compared to existing open-source
methods. Our implementation is publicly available on GitHub at
https://github.com/Ruchtech/uQRPCA

</details>


### [17] [Leveraging Sparse LiDAR for RAFT-Stereo: A Depth Pre-Fill Perspective](https://arxiv.org/abs/2507.19738)
*Jinsu Yoo,Sooyoung Jeon,Zanming Huang,Tai-Yu Pan,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 通过插值预填充稀疏LiDAR引导的RAFT-Stereo，提升了在稀疏LiDAR条件下的立体匹配精度。


<details>
  <summary>Details</summary>
Motivation: 为了在稀疏LiDAR条件下提高立体匹配的准确性，本研究旨在改进RAFT-Stereo框架。

Method: 本研究将LiDAR深度信息注入RAFT-Stereo框架的初始视差图，以提高立体匹配精度。针对LiDAR点稀疏导致引导效果下降的问题，提出了一种新的信号处理角度的解释，并提出通过插值预填充稀疏初始视差图的解决方案。同时，也探讨了将LiDAR深度信息通过早期融合注入图像特征的方法，并提出了相应的预填充方法。

Result: 通过插值预填充稀疏初始视差图的解决方案，使RAFT-Stereo在稀疏LiDAR条件下表现良好。预填充对于早期融合LiDAR深度信息也有效，但原因不同，需要不同的预填充方法。GRAFT-Stereo结合这两种方法，在稀疏LiDAR条件下显著优于现有方法。

Conclusion: 通过插值预填充稀疏的初始视差图，可以显著提高LiDAR引导的RAFT-Stereo在稀疏LiDAR条件下的性能，并且该方法也可用于早期融合，但需要不同的预填充方法。GRAFT-Stereo结合了这两种解决方案，在稀疏LiDAR条件下显著优于现有方法。

Abstract: We investigate LiDAR guidance within the RAFT-Stereo framework, aiming to
improve stereo matching accuracy by injecting precise LiDAR depth into the
initial disparity map. We find that the effectiveness of LiDAR guidance
drastically degrades when the LiDAR points become sparse (e.g., a few hundred
points per frame), and we offer a novel explanation from a signal processing
perspective. This insight leads to a surprisingly simple solution that enables
LiDAR-guided RAFT-Stereo to thrive: pre-filling the sparse initial disparity
map with interpolation. Interestingly, we find that pre-filling is also
effective when injecting LiDAR depth into image features via early fusion, but
for a fundamentally different reason, necessitating a distinct pre-filling
approach. By combining both solutions, the proposed Guided RAFT-Stereo
(GRAFT-Stereo) significantly outperforms existing LiDAR-guided methods under
sparse LiDAR conditions across various datasets. We hope this study inspires
more effective LiDAR-guided stereo methods.

</details>


### [18] [Latest Object Memory Management for Temporally Consistent Video Instance Segmentation](https://arxiv.org/abs/2507.19754)
*Seunghun Lee,Jiwan Seo,Minwoo Choi,Kiljoon Han,Jaehoon Jeong,Zane Durante,Ehsan Adeli,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

TL;DR: LOMM with LOM and DOA improves video instance segmentation by enhancing long-term tracking and identity consistency, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve long-term instance tracking and temporal consistency in video instance segmentation, addressing challenges posed by dynamic scenes with frequently appearing and disappearing objects.

Method: The method utilizes Latest Object Memory (LOM) to robustly track and update object states by explicitly modeling their presence in each frame. It also introduces Decoupled Object Association (DOA) to separately handle newly appearing and existing objects, improving matching accuracy and identity consistency.

Result: LOMM achieves a state-of-the-art AP score of 54.0 on YouTube-VIS 2022, outperforming traditional approaches and setting a new benchmark.

Conclusion: The proposed Latest Object Memory Management (LOMM) method significantly improves long-term instance tracking in temporally consistent video instance segmentation, achieving a state-of-the-art AP score of 54.0 on YouTube-VIS 2022.

Abstract: In this paper, we present Latest Object Memory Management (LOMM) for
temporally consistent video instance segmentation that significantly improves
long-term instance tracking. At the core of our method is Latest Object Memory
(LOM), which robustly tracks and continuously updates the latest states of
objects by explicitly modeling their presence in each frame. This enables
consistent tracking and accurate identity management across frames, enhancing
both performance and reliability through the VIS process. Moreover, we
introduce Decoupled Object Association (DOA), a strategy that separately
handles newly appearing and already existing objects. By leveraging our memory
system, DOA accurately assigns object indices, improving matching accuracy and
ensuring stable identity consistency, even in dynamic scenes where objects
frequently appear and disappear. Extensive experiments and ablation studies
demonstrate the superiority of our method over traditional approaches, setting
a new benchmark in VIS. Notably, our LOMM achieves state-of-the-art AP score of
54.0 on YouTube-VIS 2022, a dataset known for its challenging long videos.
Project page: https://seung-hun-lee.github.io/projects/LOMM/

</details>


### [19] [MoFRR: Mixture of Diffusion Models for Face Retouching Restoration](https://arxiv.org/abs/2507.19770)
*Jiaxin Liu,Qichao Ying,Zhenxing Qian,Sheng Li,Runqi Zhang,Jian Liu,Xinpeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MoFRR的新方法，利用扩散模型和专家系统来恢复社交媒体上经过修饰的人脸图像，解决了人脸真实性问题，并在新数据集RetouchingFFHQ++上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体上人脸修饰的广泛使用，人们对人脸图像的真实性表示担忧。虽然现有方法侧重于检测人脸修饰，但如何从修饰后的人脸准确恢复原始人脸仍然是一个未解决的问题。因此，本文旨在解决人脸修饰恢复（FRR）这一新兴的计算机视觉任务。

Method: 提出了一种名为MoFRR（Mixture of Diffusion Models for FRR）的新方法，该方法结合了扩散模型来解决人脸修饰恢复问题。MoFRR借鉴了DeepSeek的专家隔离策略，利用稀疏激活的专门化专家处理不同类型的人脸修饰，并结合一个处理通用修饰痕迹的共享专家。每个专门化专家采用双分支结构，包括一个由迭代失真评估模块（IDEM）指导的基于DDIM的低频分支，以及一个用于细节优化的基于交叉注意力的 HFCAM（High-Frequency Channel Attention Module）。

Result: MoFRR能够有效恢复人脸的低频信息，并优化高频细节，从而实现从修饰后人脸到原始人脸的准确恢复。

Conclusion: MoFRR在RetouchingFFHQ++数据集上进行了广泛的实验，证明了其在人脸修饰恢复（FRR）任务上的有效性。

Abstract: The widespread use of face retouching on social media platforms raises
concerns about the authenticity of face images. While existing methods focus on
detecting face retouching, how to accurately recover the original faces from
the retouched ones has yet to be answered. This paper introduces Face
Retouching Restoration (FRR), a novel computer vision task aimed at restoring
original faces from their retouched counterparts. FRR differs from traditional
image restoration tasks by addressing the complex retouching operations with
various types and degrees, which focuses more on the restoration of the
low-frequency information of the faces. To tackle this challenge, we propose
MoFRR, Mixture of Diffusion Models for FRR. Inspired by DeepSeek's expert
isolation strategy, the MoFRR uses sparse activation of specialized experts
handling distinct retouching types and the engagement of a shared expert
dealing with universal retouching traces. Each specialized expert follows a
dual-branch structure with a DDIM-based low-frequency branch guided by an
Iterative Distortion Evaluation Module (IDEM) and a Cross-Attention-based
High-Frequency branch (HFCAM) for detail refinement. Extensive experiments on a
newly constructed face retouching dataset, RetouchingFFHQ++, demonstrate the
effectiveness of MoFRR for FRR.

</details>


### [20] [Self-Guided Masked Autoencoder](https://arxiv.org/abs/2507.19773)
*Jeongwoo Shin,Inseo Lee,Junho Lee,Joonseok Lee*

Main category: cs.CV

TL;DR: MAE 预训练早期就能学习 patch 聚类，可用于指导掩码生成，提升学习效率。


<details>
  <summary>Details</summary>
Motivation: MAE 的成功及其学习内容的具体细节仍未完全揭示，本研究旨在深入分析 MAE 的学习机制。

Method: 提出了一种自指导掩码自动编码器（self-guided masked autoencoder），该算法利用其在 patch 聚类方面的进展，生成有指导的掩码，取代了 vanilla MAE 中随机掩码的朴素做法。

Result: 该方法在不依赖任何外部模型或补充信息的情况下，显著提升了 MAE 的学习过程，并保持了 MAE 自监督学习的优势。在各种下游任务上的广泛实验验证了所提出方法的有效性。

Conclusion: MAE 算法在预训练早期就能学习到模式化的 patch 级别聚类。

Abstract: Masked Autoencoder (MAE) is a self-supervised approach for representation
learning, widely applicable to a variety of downstream tasks in computer
vision. In spite of its success, it is still not fully uncovered what and how
MAE exactly learns. In this paper, with an in-depth analysis, we discover that
MAE intrinsically learns pattern-based patch-level clustering from surprisingly
early stages of pretraining. Upon this understanding, we propose self-guided
masked autoencoder, which internally generates informed mask by utilizing its
progress in patch clustering, substituting the naive random masking of the
vanilla MAE. Our approach significantly boosts its learning process without
relying on any external models or supplementary information, keeping the
benefit of self-supervised nature of MAE intact. Comprehensive experiments on
various downstream tasks verify the effectiveness of the proposed method.

</details>


### [21] [HydraMamba: Multi-Head State Space Model for Global Point Cloud Learning](https://arxiv.org/abs/2507.19778)
*Kanglin Qu,Pan Gao,Qun Dai,Yuanhao Sun*

Main category: cs.CV

TL;DR: HydraMamba是一种新的点云学习网络，通过shuffle序列化和ConvBiS6层解决了现有方法的局限性，并在多项任务上取得了领先成果。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法在点云序列化和局部性学习方面的不足，本文提出了HydraMamba。

Method: 本文提出了一种基于状态空间模型的点云网络HydraMamba。具体地，设计了一种shuffle序列化策略，使无序点集更好地适应S6的因果性质；提出了一种ConvBiS6层，能够协同捕获局部几何和全局上下文依赖；通过将多头设计扩展到S6提出了MHS6，进一步增强了其建模能力。

Result: HydraMamba在物体和场景级别任务上均取得了最先进的结果。

Conclusion: HydraMamba在物体和场景级别任务上均取得了最先进的结果。

Abstract: The attention mechanism has become a dominant operator in point cloud
learning, but its quadratic complexity leads to limited inter-point
interactions, hindering long-range dependency modeling between objects. Due to
excellent long-range modeling capability with linear complexity, the selective
state space model (S6), as the core of Mamba, has been exploited in point cloud
learning for long-range dependency interactions over the entire point cloud.
Despite some significant progress, related works still suffer from imperfect
point cloud serialization and lack of locality learning. To this end, we
explore a state space model-based point cloud network termed HydraMamba to
address the above challenges. Specifically, we design a shuffle serialization
strategy, making unordered point sets better adapted to the causal nature of
S6. Meanwhile, to overcome the deficiency of existing techniques in locality
learning, we propose a ConvBiS6 layer, which is capable of capturing local
geometries and global context dependencies synergistically. Besides, we propose
MHS6 by extending the multi-head design to S6, further enhancing its modeling
capability. HydraMamba achieves state-of-the-art results on various tasks at
both object-level and scene-level. The code is available at
https://github.com/Point-Cloud-Learning/HydraMamba.

</details>


### [22] [JDATT: A Joint Distillation Framework for Atmospheric Turbulence Mitigation and Target Detection](https://arxiv.org/abs/2507.19780)
*Zhiming Liu,Paul Hill,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: JDATT 是一个联合蒸馏框架，可同时缓解大气湍流并检测目标，它比现有方法更高效、更准确，并且更适合实时应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有深度学习方法计算成本高、不适合实时应用以及将湍流缓解和目标检测分离导致效率低下和性能不佳的问题。

Method: 提出了一种名为 JDATT 的联合蒸馏框架，用于大气湍流缓解和目标检测。JDATT 集成了先进的湍流缓解和检测模块，并采用了一种统一的知识蒸馏策略，该策略在最小化性能损失的同时压缩两个组件。采用混合蒸馏方案：通过通道蒸馏 (CWD) 和掩码生成蒸馏 (MGD) 进行特征级蒸馏，并通过 KL 散度进行输出级蒸馏。

Result: JDATT 实现了优越的视觉恢复和检测准确性，同时显著减小了模型尺寸和推理时间。

Conclusion: JDATT 框架在合成和真实世界湍流数据集上的实验表明，它在视觉恢复和检测准确性方面均表现出色，同时显著减小了模型尺寸和推理时间，非常适合实时部署。

Abstract: Atmospheric turbulence (AT) introduces severe degradations, such as rippling,
blur, and intensity fluctuations, that hinder both image quality and downstream
vision tasks like target detection. While recent deep learning-based approaches
have advanced AT mitigation using transformer and Mamba architectures, their
high complexity and computational cost make them unsuitable for real-time
applications, especially in resource-constrained settings such as remote
surveillance. Moreover, the common practice of separating turbulence mitigation
and object detection leads to inefficiencies and suboptimal performance. To
address these challenges, we propose JDATT, a Joint Distillation framework for
Atmospheric Turbulence mitigation and Target detection. JDATT integrates
state-of-the-art AT mitigation and detection modules and introduces a unified
knowledge distillation strategy that compresses both components while
minimizing performance loss. We employ a hybrid distillation scheme:
feature-level distillation via Channel-Wise Distillation (CWD) and Masked
Generative Distillation (MGD), and output-level distillation via
Kullback-Leibler divergence. Experiments on synthetic and real-world turbulence
datasets demonstrate that JDATT achieves superior visual restoration and
detection accuracy while significantly reducing model size and inference time,
making it well-suited for real-time deployment.

</details>


### [23] [TransFlow: Motion Knowledge Transfer from Video Diffusion Models to Video Salient Object Detection](https://arxiv.org/abs/2507.19789)
*Suhwan Cho,Minhyeok Lee,Jungho Lee,Sunghun Yang,Sangyoun Lee*

Main category: cs.CV

TL;DR: TransFlow利用视频扩散模型生成逼真的训练数据，解决了视频SOD数据集不足的问题，并提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 视频SOD模型训练受限于视频数据集的稀缺性，而现有的从静态图像生成视频序列的方法会产生不切实际的、缺乏语义理解的光流，无法满足运动引导任务的需求。

Method: TransFlow利用视频扩散模型学习到的丰富的语义运动先验，从静态图像生成语义感知的光流，使物体表现出自然的运动模式，同时保持空间边界和时间连贯性。

Result: TransFlow在多个基准测试中取得了改进的性能，证明了其有效的运动知识转移能力。

Conclusion: TransFlow通过从预训练的视频扩散模型转移运动知识来为视频显著目标检测（SOD）生成逼真的训练数据，并在多个基准测试中取得了改进的性能。

Abstract: Video salient object detection (SOD) relies on motion cues to distinguish
salient objects from backgrounds, but training such models is limited by scarce
video datasets compared to abundant image datasets. Existing approaches that
use spatial transformations to create video sequences from static images fail
for motion-guided tasks, as these transformations produce unrealistic optical
flows that lack semantic understanding of motion. We present TransFlow, which
transfers motion knowledge from pre-trained video diffusion models to generate
realistic training data for video SOD. Video diffusion models have learned rich
semantic motion priors from large-scale video data, understanding how different
objects naturally move in real scenes. TransFlow leverages this knowledge to
generate semantically-aware optical flows from static images, where objects
exhibit natural motion patterns while preserving spatial boundaries and
temporal coherence. Our method achieves improved performance across multiple
benchmarks, demonstrating effective motion knowledge transfer.

</details>


### [24] [DepthFlow: Exploiting Depth-Flow Structural Correlations for Unsupervised Video Object Segmentation](https://arxiv.org/abs/2507.19790)
*Suhwan Cho,Minhyeok Lee,Jungho Lee,Donghyeong Kim,Sangyoun Lee*

Main category: cs.CV

TL;DR: 提出DepthFlow方法，从单幅图像合成光流，解决了无监督视频对象分割的训练数据稀疏问题，并在所有公共VOS基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决无监督视频对象分割（VOS）中由于训练数据稀疏而导致的性能受限问题。

Method: 提出了一种名为DepthFlow的新型数据生成方法，该方法从单个图像合成光流。该方法首先从源图像估计深度图，然后将其转换为保留关键结构线索的合成流场。

Result: 合成的光流能够保留关键的结构线索，从而将大量的图像-掩码对转换为图像-流-掩码训练对，极大地扩展了可用于网络训练的数据量，并实现了新的最先进性能。

Conclusion: 通过使用合成数据训练简单的编码器-解码器架构，在所有公共VOS基准测试中取得了新的最先进性能，证明了解决数据稀疏性问题的可扩展且有效的方法。

Abstract: Unsupervised video object segmentation (VOS) aims to detect the most
prominent object in a video. Recently, two-stream approaches that leverage both
RGB images and optical flow have gained significant attention, but their
performance is fundamentally constrained by the scarcity of training data. To
address this, we propose DepthFlow, a novel data generation method that
synthesizes optical flow from single images. Our approach is driven by the key
insight that VOS models depend more on structural information embedded in flow
maps than on their geometric accuracy, and that this structure is highly
correlated with depth. We first estimate a depth map from a source image and
then convert it into a synthetic flow field that preserves essential structural
cues. This process enables the transformation of large-scale image-mask pairs
into image-flow-mask training pairs, dramatically expanding the data available
for network training. By training a simple encoder-decoder architecture with
our synthesized data, we achieve new state-of-the-art performance on all public
VOS benchmarks, demonstrating a scalable and effective solution to the data
scarcity problem.

</details>


### [25] [ForCenNet: Foreground-Centric Network for Document Image Rectification](https://arxiv.org/abs/2507.19804)
*Peng Cai,Qiang Li,Kaicheng Yang,Dong Guo,Jia Li,Nan Zhou,Xiang An,Ninghua Yang,Jiankang Deng*

Main category: cs.CV

TL;DR: ForCenNet是一种新的文档图像校正网络，通过以前景为中心的标签生成、掩模机制和曲率一致性损失，有效解决了现有方法忽略前景元素的问题，并在多个基准测试中取得了优异的成果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文档图像校正中往往忽略了前景元素的重要性，而前景元素对于文档图像校正提供了重要的几何参考和布局信息。

Method: 该方法首先提出了一种以前景为中心的标签生成方法，提取清晰图像中的详细前景元素；然后引入了一种以前景为中心的掩模机制，以增强可读区域和背景区域之间的区别；接着设计了一种曲率一致性损失，利用详细的前景标签帮助模型理解畸变的几何分布。

Result: ForCenNet在四个真实世界基准测试上取得了新的最先进水平，并且能够有效地校正文本行和表格边框等布局元素的几何畸变。

Conclusion: ForCenNet在四个真实世界基准测试（如DocUNet、DIR300、WarpDoc和DocReal）上取得了新的最先进水平，并且能够有效地校正文本行和表格边框等布局元素的几何畸变。

Abstract: Document image rectification aims to eliminate geometric deformation in
photographed documents to facilitate text recognition. However, existing
methods often neglect the significance of foreground elements, which provide
essential geometric references and layout information for document image
correction. In this paper, we introduce Foreground-Centric Network (ForCenNet)
to eliminate geometric distortions in document images. Specifically, we
initially propose a foreground-centric label generation method, which extracts
detailed foreground elements from an undistorted image. Then we introduce a
foreground-centric mask mechanism to enhance the distinction between readable
and background regions. Furthermore, we design a curvature consistency loss to
leverage the detailed foreground labels to help the model understand the
distorted geometric distribution. Extensive experiments demonstrate that
ForCenNet achieves new state-of-the-art on four real-world benchmarks, such as
DocUNet, DIR300, WarpDoc, and DocReal. Quantitative analysis shows that the
proposed method effectively undistorts layout elements, such as text lines and
table borders. The resources for further comparison are provided at
https://github.com/caipeng328/ForCenNet.

</details>


### [26] [DS-Det: Single-Query Paradigm and Attention Disentangled Learning for Flexible Object Detection](https://arxiv.org/abs/2507.19807)
*Guiping Cao,Xiangyuan Lan,Wenjian Huang,Jianguo Zhang,Dongmei Jiang,Yaowei Wang*

Main category: cs.CV

TL;DR: DS-Det 提出了一种新的单查询范式和简化的解码器框架，通过注意力解耦学习解决了现有 Transformer 检测器中的查询灵活性、查询模糊和 ROT 问题，并提高了解码器效率。


<details>
  <summary>Details</summary>
Motivation: 现有的 Transformer 检测器的查询类型（例如内容查询和位置查询）被预定义为固定的数量，这限制了它们的灵活性。这些固定查询的学习会受到两种注意力操作（自注意力和交叉注意力）之间循环对立交互（ROT）的干扰，从而降低了解码器的效率。此外，当共享权重的解码器层在训练过程中同时处理一对一和一对多标签分配时，会产生“查询模糊”问题，这违反了 DETR 的一对一匹配原则。

Method: DS-Det 提出了一种新的统一单查询范式，用于解码器建模，将固定查询转换为灵活查询。此外，通过注意力解耦学习提出了一种简化的解码器框架：通过交叉注意力（一对多过程）定位边界框，通过自注意力（一对一过程）去重预测，直接解决“查询模糊”和“ROT”问题，并提高解码器效率。还引入了一种统一的 PoCoo 损失，它利用边界框大小先验来优先处理硬样本（例如小目标）的查询学习。

Result: DS-Det 是一种更高效的检测器，能够检测图像中灵活数量的对象。

Conclusion: DS-Det 在 COCO2017 和 WiderPerson 数据集上，通过对五种不同的骨干模型进行的大量实验，证明了其通用有效性和优越性。

Abstract: Popular transformer detectors have achieved promising performance through
query-based learning using attention mechanisms. However, the roles of existing
decoder query types (e.g., content query and positional query) are still
underexplored. These queries are generally predefined with a fixed number
(fixed-query), which limits their flexibility. We find that the learning of
these fixed-query is impaired by Recurrent Opposing inTeractions (ROT) between
two attention operations: Self-Attention (query-to-query) and Cross-Attention
(query-to-encoder), thereby degrading decoder efficiency. Furthermore, "query
ambiguity" arises when shared-weight decoder layers are processed with both
one-to-one and one-to-many label assignments during training, violating DETR's
one-to-one matching principle. To address these challenges, we propose DS-Det,
a more efficient detector capable of detecting a flexible number of objects in
images. Specifically, we reformulate and introduce a new unified Single-Query
paradigm for decoder modeling, transforming the fixed-query into flexible.
Furthermore, we propose a simplified decoder framework through attention
disentangled learning: locating boxes with Cross-Attention (one-to-many
process), deduplicating predictions with Self-Attention (one-to-one process),
addressing "query ambiguity" and "ROT" issues directly, and enhancing decoder
efficiency. We further introduce a unified PoCoo loss that leverages box size
priors to prioritize query learning on hard samples such as small objects.
Extensive experiments across five different backbone models on COCO2017 and
WiderPerson datasets demonstrate the general effectiveness and superiority of
DS-Det. The source codes are available at
https://github.com/Med-Process/DS-Det/.

</details>


### [27] [SeeDiff: Off-the-Shelf Seeded Mask Generation from Diffusion Models](https://arxiv.org/abs/2507.19808)
*Joon Hyun Park,Kumju Jo,Sungyong Baik*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Entrusted with the goal of pixel-level object classification, the semantic
segmentation networks entail the laborious preparation of pixel-level
annotation masks. To obtain pixel-level annotation masks for a given class
without human efforts, recent few works have proposed to generate pairs of
images and annotation masks by employing image and text relationships modeled
by text-to-image generative models, especially Stable Diffusion. However, these
works do not fully exploit the capability of text-guided Diffusion models and
thus require a pre-trained segmentation network, careful text prompt tuning, or
the training of a segmentation network to generate final annotation masks. In
this work, we take a closer look at attention mechanisms of Stable Diffusion,
from which we draw connections with classical seeded segmentation approaches.
In particular, we show that cross-attention alone provides very coarse object
localization, which however can provide initial seeds. Then, akin to region
expansion in seeded segmentation, we utilize the
semantic-correspondence-modeling capability of self-attention to iteratively
spread the attention to the whole class from the seeds using multi-scale
self-attention maps. We also observe that a simple-text-guided synthetic image
often has a uniform background, which is easier to find correspondences,
compared to complex-structured objects. Thus, we further refine a mask using a
more accurate background mask. Our proposed method, dubbed SeeDiff, generates
high-quality masks off-the-shelf from Stable Diffusion, without additional
training procedure, prompt tuning, or a pre-trained segmentation network.

</details>


### [28] [FM-LC: A Hierarchical Framework for Urban Flood Mapping by Land Cover Identification Models](https://arxiv.org/abs/2507.19818)
*Xin Hong,Longchao Da,Hua Wei*

Main category: cs.CV

TL;DR: 该研究提出了FM-LC框架，利用高分辨率卫星影像和分层机器学习方法，显著提高了干旱地区城市洪水测绘的精度和清晰度，尤其是在处理光谱相似的类别时。


<details>
  <summary>Details</summary>
Motivation: 干旱地区城市洪水对基础设施和社区构成严重威胁。为了改善应急响应和韧性规划，精确、精细尺度的洪水范围和恢复轨迹绘制至关重要。然而，干旱环境的光谱对比度低、水文动态快和土地覆盖异质性高，给传统洪水测绘方法带来了挑战。高分辨率、每日更新的PlanetScope影像提供了所需的时间和空间细节。

Method: FM-LC框架是一个三阶段的方法：1.使用多类U-Net进行初步分割，将影像分为水体、植被、建筑区和裸地。2.通过早期检查识别主要的错误分类类别（如水体和植被之间的混淆），并训练一个轻量级二元专家分割模型来区分该类别。3.应用贝叶斯平滑步骤，利用邻近像素信息来优化边界和去除噪声。该框架在2024年4月迪拜风暴事件的PlanetScope影像上进行了验证。

Result: FM-LC框架在2024年4月迪拜风暴事件的PlanetScope影像上进行了验证，实验结果显示，与传统的单阶段U-Net基线方法相比，FM-LC框架在所有土地覆盖类别上的平均F1分数提高了高达29%，并且洪水边界的描绘更加清晰。

Conclusion: 该研究提出了FM-LC框架，通过分层方法改进了在光谱对比度低、动态性强和异构性高的干旱地区进行精细化洪水测绘的准确性。实验结果显示，与传统单阶段方法相比，FM-LC在所有土地覆盖类别上的平均F1分数提高了29%，并且能够更清晰地勾画洪水边界。

Abstract: Urban flooding in arid regions poses severe risks to infrastructure and
communities. Accurate, fine-scale mapping of flood extents and recovery
trajectories is therefore essential for improving emergency response and
resilience planning. However, arid environments often exhibit limited spectral
contrast between water and adjacent surfaces, rapid hydrological dynamics, and
highly heterogeneous urban land covers, which challenge traditional
flood-mapping approaches. High-resolution, daily PlanetScope imagery provides
the temporal and spatial detail needed. In this work, we introduce FM-LC, a
hierarchical framework for Flood Mapping by Land Cover identification, for this
challenging task. Through a three-stage process, it first uses an initial
multi-class U-Net to segment imagery into water, vegetation, built area, and
bare ground classes. We identify that this method has confusion between
spectrally similar categories (e.g., water vs. vegetation). Second, by early
checking, the class with the major misclassified area is flagged, and a
lightweight binary expert segmentation model is trained to distinguish the
flagged class from the rest. Third, a Bayesian smoothing step refines
boundaries and removes spurious noise by leveraging nearby pixel information.
We validate the framework on the April 2024 Dubai storm event, using pre- and
post-rainfall PlanetScope composites. Experimental results demonstrate average
F1-score improvements of up to 29% across all land-cover classes and notably
sharper flood delineations, significantly outperforming conventional
single-stage U-Net baselines.

</details>


### [29] [LAVA: Language Driven Scalable and Versatile Traffic Video Analytics](https://arxiv.org/abs/2507.19821)
*Yanrui Yu,Tianfei Zhou,Jiaxin Sun,Lianpeng Qiao,Lizhong Ding,Ye Yuan,Guoren Wang*

Main category: cs.CV

TL;DR: Lava 是一个通过自然语言查询来分析海量视频数据的系统，它比基于 SQL 的方法更灵活高效，并且在多个指标上都取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于 SQL 的视频查询方法在处理海量视频数据时存在查询模式僵化、分析灵活性受限的问题。因此，需要一种能够通过自然语言进行灵活高效查询的视频分析方法。

Method: Lava 系统包含三个主要组件：1. 基于多臂老虎机的高效采样方法，用于视频片段级别的定位；2. 视频特定的开放世界检测模块，用于对象级别的检索；3. 长期对象轨迹提取方案，用于时间对象关联，从而为感兴趣的对象生成完整的轨迹。

Result: Lava 在选择查询方面提高了 14% 的 F1 分数，降低了 0.39 的聚合查询 MPAE，并实现了 86% 的 top-k 精度，同时处理速度比最准确的基线快 9.6 倍。

Conclusion: Lava 系统在处理大规模视频数据时，通过自然语言查询能够实现更灵活和高效的分析。实验证明，Lava 在选择查询方面提高了 14% 的 F1 分数，降低了 0.39 的聚合查询 MPAE，并实现了 86% 的 top-k 精度，同时处理速度比最准确的基线快 9.6 倍。

Abstract: In modern urban environments, camera networks generate massive amounts of
operational footage -- reaching petabytes each day -- making scalable video
analytics essential for efficient processing. Many existing approaches adopt an
SQL-based paradigm for querying such large-scale video databases; however, this
constrains queries to rigid patterns with predefined semantic categories,
significantly limiting analytical flexibility. In this work, we explore a
language-driven video analytics paradigm aimed at enabling flexible and
efficient querying of high-volume video data driven by natural language.
Particularly, we build \textsc{Lava}, a system that accepts natural language
queries and retrieves traffic targets across multiple levels of granularity and
arbitrary categories. \textsc{Lava} comprises three main components: 1) a
multi-armed bandit-based efficient sampling method for video segment-level
localization;
  2) a video-specific open-world detection module for object-level retrieval;
and 3) a long-term object trajectory extraction scheme for temporal object
association, yielding complete trajectories for object-of-interests. To support
comprehensive evaluation, we further develop a novel benchmark by providing
diverse, semantically rich natural language predicates and fine-grained
annotations for multiple videos. Experiments on this benchmark demonstrate that
\textsc{Lava} improves $F_1$-scores for selection queries by $\mathbf{14\%}$,
reduces MPAE for aggregation queries by $\mathbf{0.39}$, and achieves top-$k$
precision of $\mathbf{86\%}$, while processing videos $ \mathbf{9.6\times} $
faster than the most accurate baseline.

</details>


### [30] [AutoSign: Direct Pose-to-Text Translation for Continuous Sign Language Recognition](https://arxiv.org/abs/2507.19840)
*Samuel Ebimobowei Johnny,Blessed Guda,Andrew Blayama Stephen,Assane Gueye*

Main category: cs.CV

TL;DR: AutoSign是一种新的CSLR方法，它使用Transformer直接将姿势序列翻译成文本，从而提高了准确性并解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的连续手语识别（CSLR）方法依赖于多阶段流水线，该流水线容易出错传播、过拟合，并且由于中间的手语词表示瓶颈而难以进行词汇扩展。为了解决这些局限性，我们提出了一种直接将姿势序列翻译成自然语言文本的方法，完全绕过了传统的对齐机制。

Method: 提出了一种名为AutoSign的自回归解码器-仅Transformer模型，该模型直接将姿势序列翻译成自然语言文本，绕过传统的对齐机制。该方法结合了使用1D CNN的时间压缩模块来处理姿势序列，然后使用预训练的阿拉伯语解码器AraGPT2来生成文本（手语词）。

Result: 手部和身体手势为与识别者无关的CSLR提供了最具有区分性的特征。

Conclusion: 消除多阶段流水线，AutoSign在Isharah-1000数据集上实现了显著改进，与现有的最佳方法相比，词错误率（WER）得分提高了多达6.1%。

Abstract: Continuously recognizing sign gestures and converting them to glosses plays a
key role in bridging the gap between the hearing and hearing-impaired
communities. This involves recognizing and interpreting the hands, face, and
body gestures of the signer, which pose a challenge as it involves a
combination of all these features. Continuous Sign Language Recognition (CSLR)
methods rely on multi-stage pipelines that first extract visual features, then
align variable-length sequences with target glosses using CTC or HMM-based
approaches. However, these alignment-based methods suffer from error
propagation across stages, overfitting, and struggle with vocabulary
scalability due to the intermediate gloss representation bottleneck. To address
these limitations, we propose AutoSign, an autoregressive decoder-only
transformer that directly translates pose sequences to natural language text,
bypassing traditional alignment mechanisms entirely. The use of this
decoder-only approach allows the model to directly map between the features and
the glosses without the need for CTC loss while also directly learning the
textual dependencies in the glosses. Our approach incorporates a temporal
compression module using 1D CNNs to efficiently process pose sequences,
followed by AraGPT2, a pre-trained Arabic decoder, to generate text (glosses).
Through comprehensive ablation studies, we demonstrate that hand and body
gestures provide the most discriminative features for signer-independent CSLR.
By eliminating the multi-stage pipeline, AutoSign achieves substantial
improvements on the Isharah-1000 dataset, achieving an improvement of up to
6.1\% in WER score compared to the best existing method.

</details>


### [31] [Knowledge Regularized Negative Feature Tuning for Out-of-Distribution Detection with Vision-Language Models](https://arxiv.org/abs/2507.19847)
*Wenjie Zhu,Yabin Zhang,Xin Jin,Wenjun Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: KR-NFT是一种新的OOD检测方法，通过负特征调优和知识正则化，解决了现有方法泛化性能下降的问题，并在未见类别下表现出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的负面提示调优虽然提升了视觉语言模型的OOD检测能力，但常常牺牲在未见类别和风格下的泛化性能。为了解决这个问题，需要一种能够同时提高OOD检测和泛化能力的新方法。

Method: 提出了一种名为知识正则化负特征调优（KR-NFT）的新方法，该方法集成了负特征调优（NFT）和知识正则化（KR）优化策略。NFT通过分布感知转换将文本特征分离到不同的空间，以区分ID和OOD图像，并通过轻量级元网络引入图像条件可学习因子进行动态适应。KR优化策略用于增强ID和OOD集合的区分度，同时减少预训练知识的遗忘。

Result: KR-NFT在ImageNet数据集上进行少样本训练时，不仅提高了ID分类准确率和OOD检测性能，还在未见的ID类别下将FPR95显著降低了5.44%。

Conclusion: KR-NFT通过结合NFT和KR优化策略，在提高OOD检测能力的同时，也改善了在未见类别和风格下的泛化性能，并且效率和可扩展性优于传统的负面提示调优。

Abstract: Out-of-distribution (OOD) detection is crucial for building reliable machine
learning models. Although negative prompt tuning has enhanced the OOD detection
capabilities of vision-language models, these tuned models often suffer from
reduced generalization performance on unseen classes and styles. To address
this challenge, we propose a novel method called Knowledge Regularized Negative
Feature Tuning (KR-NFT), which integrates an innovative adaptation architecture
termed Negative Feature Tuning (NFT) and a corresponding
knowledge-regularization (KR) optimization strategy. Specifically, NFT applies
distribution-aware transformations to pre-trained text features, effectively
separating positive and negative features into distinct spaces. This separation
maximizes the distinction between in-distribution (ID) and OOD images.
Additionally, we introduce image-conditional learnable factors through a
lightweight meta-network, enabling dynamic adaptation to individual images and
mitigating sensitivity to class and style shifts. Compared to traditional
negative prompt tuning, NFT demonstrates superior efficiency and scalability.
To optimize this adaptation architecture, the KR optimization strategy is
designed to enhance the discrimination between ID and OOD sets while mitigating
pre-trained knowledge forgetting. This enhances OOD detection performance on
trained ID classes while simultaneously improving OOD detection on unseen ID
datasets. Notably, when trained with few-shot samples from ImageNet dataset,
KR-NFT not only improves ID classification accuracy and OOD detection but also
significantly reduces the FPR95 by 5.44\% under an unexplored generalization
setting with unseen ID categories. Codes can be found at
\href{https://github.com/ZhuWenjie98/KRNFT}{https://github.com/ZhuWenjie98/KRNFT}.

</details>


### [32] [FineMotion: A Dataset and Benchmark with both Spatial and Temporal Annotation for Fine-grained Motion Generation and Editing](https://arxiv.org/abs/2507.19850)
*Bizhu Wu,Jinheng Xie,Meidan Ding,Zhe Kong,Jianfeng Ren,Ruibin Bai,Rong Qu,Linlin Shen*

Main category: cs.CV

TL;DR: FineMotion数据集通过提供详细的人体部位运动描述，改进了文本驱动的人体运动生成和编辑。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到人体运动生成方法忽略特定身体部位运动及其时间性的问题，通过提供更详细的文本描述来丰富数据集。

Method: 提出FineMotion数据集，包含442,000多个包含人体部位运动细节描述的运动片段，以及约95,000个描述完整运动序列人体部位运动的段落。

Result: 在文本驱动的细粒度人体运动生成任务上取得了显著效果，MDM模型Top-3准确率提升了+15.3%，并支持了细粒度运动编辑的零样本流程。

Conclusion: 该研究提出的FineMotion数据集显著提高了文本驱动的细粒度人体运动生成任务的性能，尤其是在MDM模型上实现了+15.3%的Top-3准确率提升，并支持了细粒度运动编辑的零样本流程。

Abstract: Generating realistic human motions from textual descriptions has undergone
significant advancements. However, existing methods often overlook specific
body part movements and their timing. In this paper, we address this issue by
enriching the textual description with more details. Specifically, we propose
the FineMotion dataset, which contains over 442,000 human motion snippets -
short segments of human motion sequences - and their corresponding detailed
descriptions of human body part movements. Additionally, the dataset includes
about 95k detailed paragraphs describing the movements of human body parts of
entire motion sequences. Experimental results demonstrate the significance of
our dataset on the text-driven finegrained human motion generation task,
especially with a remarkable +15.3% improvement in Top-3 accuracy for the MDM
model. Notably, we further support a zero-shot pipeline of fine-grained motion
editing, which focuses on detailed editing in both spatial and temporal
dimensions via text. Dataset and code available at: CVI-SZU/FineMotion

</details>


### [33] [A Structure-aware and Motion-adaptive Framework for 3D Human Pose Estimation with Mamba](https://arxiv.org/abs/2507.19852)
*Ye Lu,Jie Wang,Jianjun Gao,Rui Gong,Chen Cai,Kim-Hui Yap*

Main category: cs.CV

TL;DR: SAMA框架通过其结构感知和运动适应的模块，有效解决了现有姿态提升方法在处理复杂关节连接和运动差异方面的局限性，并在多个基准测试中实现了优于现有方法的结果，同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Mamba的姿态提升方法在模拟复杂关节连接和统一处理所有关节运动轨迹方面存在不足，并且忽略了不同运动特征的内在差异。本研究旨在提出一个结构感知和运动适应的框架来解决这些问题。

Method: SAMA框架包含两个关键模块：结构感知状态积分器（SSI）和运动适应状态调制器（MSM）。SSI利用动态关节关系，基于姿态拓扑而非顺序状态转换，在状态空间中融合关节特征和状态信息。MSM则负责识别特定于关节的运动特征，并对不同关节的运动模式进行量身定制的调整。

Result: 通过SSI和MSM模块，SAMA算法实现了结构感知和运动适应的姿态提升。与现有方法相比，SAMA在多个基准测试中取得了先进的结果，并降低了计算成本。

Conclusion: SAMA框架通过其结构感知状态积分器（SSI）和运动适应状态调制器（MSM）模块，实现了对空间关节拓扑和不同运动动态的独立捕获，从而实现了结构感知和运动适应的姿态提升。实验证明，该算法在多个基准测试中取得了先进的结果，并且计算成本更低。

Abstract: Recent Mamba-based methods for the pose-lifting task tend to model joint
dependencies by 2D-to-1D mapping with diverse scanning strategies. Though
effective, they struggle to model intricate joint connections and uniformly
process all joint motion trajectories while neglecting the intrinsic
differences across motion characteristics. In this work, we propose a
structure-aware and motion-adaptive framework to capture spatial joint topology
along with diverse motion dynamics independently, named as SAMA. Specifically,
SAMA consists of a Structure-aware State Integrator (SSI) and a Motion-adaptive
State Modulator (MSM). The Structure-aware State Integrator is tasked with
leveraging dynamic joint relationships to fuse information at both the joint
feature and state levels in the state space, based on pose topology rather than
sequential state transitions. The Motion-adaptive State Modulator is
responsible for joint-specific motion characteristics recognition, thus
applying tailored adjustments to diverse motion patterns across different
joints. Through the above key modules, our algorithm enables structure-aware
and motion-adaptive pose lifting. Extensive experiments across multiple
benchmarks demonstrate that our algorithm achieves advanced results with fewer
computational costs.

</details>


### [34] [MatSSL: Robust Self-Supervised Representation Learning for Metallographic Image Segmentation](https://arxiv.org/abs/2507.18184)
*Hoang Hai Nam Nguyen,Phan Nguyen Duc Hieu,Ho Won Lee*

Main category: cs.CV

TL;DR: MatSSL 是一种创新的自监督学习架构，通过门控特征融合有效整合多层次表征，仅需少量未标记数据即可在金相显微图像分析中取得优于现有方法的性能，并能有效适应新数据集。


<details>
  <summary>Details</summary>
Motivation: 当前金相显微图像分析依赖有监督方法，需要为每个新数据集重新训练，且在仅有少量标记样本时表现不一致。自监督学习（SSL）利用未标记数据是一个有前景的替代方案，但现有方法大多依赖大规模数据集。MatSSL 旨在克服这些限制。

Method: MatSSL 采用门控特征融合（Gated Feature Fusion）架构，在骨干网络的每个阶段整合多层次表征。首先在小规模、未标记的数据集上进行自监督预训练，然后针对多个基准数据集进行微调。

Result: MatSSL 在 MetalDAM 上的分割模型实现了 69.13% 的 mIoU，优于使用 ImageNet 预训练编码器达到的 66.73%。在环境阻隔涂层（EBC）基准数据集上，与使用 MicroNet 预训练的模型相比，平均 mIoU 持续提高了近 40%。

Conclusion: MatSSL 能够有效地利用少量的未标记数据适应金相领域，同时保留从大规模自然图像预训练中学到的丰富且可迁移的特征。

Abstract: MatSSL is a streamlined self-supervised learning (SSL) architecture that
employs Gated Feature Fusion at each stage of the backbone to integrate
multi-level representations effectively. Current micrograph analysis of
metallic materials relies on supervised methods, which require retraining for
each new dataset and often perform inconsistently with only a few labeled
samples. While SSL offers a promising alternative by leveraging unlabeled data,
most existing methods still depend on large-scale datasets to be effective.
MatSSL is designed to overcome this limitation. We first perform
self-supervised pretraining on a small-scale, unlabeled dataset and then
fine-tune the model on multiple benchmark datasets. The resulting segmentation
models achieve 69.13% mIoU on MetalDAM, outperforming the 66.73% achieved by an
ImageNet-pretrained encoder, and delivers consistently up to nearly 40%
improvement in average mIoU on the Environmental Barrier Coating benchmark
dataset (EBC) compared to models pretrained with MicroNet. This suggests that
MatSSL enables effective adaptation to the metallographic domain using only a
small amount of unlabeled data, while preserving the rich and transferable
features learned from large-scale pretraining on natural images.

</details>


### [35] [RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection](https://arxiv.org/abs/2507.19856)
*Xiaokai Bai,Chenxu Zhou,Lianqing Zheng,Si-Yuan Cao,Jianan Liu,Xiaohan Zhang,Zhengzhuang Zhang,Hui-liang Shen*

Main category: cs.CV

TL;DR: We propose RaGS, a novel framework that uses 3D Gaussian Splatting to fuse 4D radar and monocular images for 3D object detection, outperforming existing methods on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing fusion approaches typically rely on either instance-based proposals or dense BEV grids, which either lack holistic scene understanding or are limited by rigid grid structures. 3D GS naturally suits 3D object detection by modeling the scene as a field of Gaussians, dynamically allocating resources on foreground objects and providing a flexible, resource-efficient solution.

Method: RaGS uses a cascaded pipeline to construct and refine the Gaussian field. It starts with the Frustum-based Localization Initiation (FLI), which unprojects foreground pixels to initialize coarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA) fuses semantics and geometry, refining the limited Gaussians to the regions of interest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussians into multi-level BEV features for 3D object detection.

Result: RaGS enables object concentrating while offering comprehensive scene perception. Extensive experiments on View-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate its state-of-the-art performance.

Conclusion: We propose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) as representation for fusing 4D radar and monocular cues in 3D object detection. Extensive experiments on View-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate its state-of-the-art performance.

Abstract: 4D millimeter-wave radar has emerged as a promising sensor for autonomous
driving, but effective 3D object detection from both 4D radar and monocular
images remains a challenge. Existing fusion approaches typically rely on either
instance-based proposals or dense BEV grids, which either lack holistic scene
understanding or are limited by rigid grid structures. To address these, we
propose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) as
representation for fusing 4D radar and monocular cues in 3D object detection.
3D GS naturally suits 3D object detection by modeling the scene as a field of
Gaussians, dynamically allocating resources on foreground objects and providing
a flexible, resource-efficient solution. RaGS uses a cascaded pipeline to
construct and refine the Gaussian field. It starts with the Frustum-based
Localization Initiation (FLI), which unprojects foreground pixels to initialize
coarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA)
fuses semantics and geometry, refining the limited Gaussians to the regions of
interest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussians
into multi-level BEV features for 3D object detection. By dynamically focusing
on sparse objects within scenes, RaGS enable object concentrating while
offering comprehensive scene perception. Extensive experiments on
View-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate its
state-of-the-art performance. Code will be released.

</details>


### [36] [OW-CLIP: Data-Efficient Visual Supervision for Open-World Object Detection via Human-AI Collaboration](https://arxiv.org/abs/2507.19870)
*Junwen Duan,Wei Xue,Ziyao Kang,Shixia Liu,Jiazhi Xia*

Main category: cs.CV

TL;DR: OW-CLIP通过创新的提示调优、特征平滑和数据精炼技术，解决了开放世界物体检测中的数据饥渴和特征过拟合问题，实现了高效且高性能的模型增量训练，并提供可视化工具以提升标注质量。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有开放世界物体检测方法在数据需求、特征过拟合和模型灵活性方面的局限性，提出一种数据高效的增量训练方法。

Method: OW-CLIP系统采用即插即用的多模态提示调优，并引入了“Crop-Smoothing”技术来缓解部分特征过拟合。此外，还开发了利用大型语言模型和跨模态相似性进行数据生成和过滤的双模态数据精炼方法，以及一个用于探索和交付高质量标注的可视化界面。

Result: OW-CLIP实现了最先进性能的89%，而仅需3.8%的自生成数据，并在同等数据量下超越了最先进的方法。案例研究证明了该方法的有效性和可视化系统的标注质量。

Conclusion: OW-CLIP在开放世界物体检测方面取得了有竞争力的性能，仅使用了少量自生成数据，并在相同数据量下优于现有最先进的方法。该系统还通过可视化界面提高了标注质量。

Abstract: Open-world object detection (OWOD) extends traditional object detection to
identifying both known and unknown object, necessitating continuous model
adaptation as new annotations emerge. Current approaches face significant
limitations: 1) data-hungry training due to reliance on a large number of
crowdsourced annotations, 2) susceptibility to "partial feature overfitting,"
and 3) limited flexibility due to required model architecture modifications. To
tackle these issues, we present OW-CLIP, a visual analytics system that
provides curated data and enables data-efficient OWOD model incremental
training. OW-CLIP implements plug-and-play multimodal prompt tuning tailored
for OWOD settings and introduces a novel "Crop-Smoothing" technique to mitigate
partial feature overfitting. To meet the data requirements for the training
methodology, we propose dual-modal data refinement methods that leverage large
language models and cross-modal similarity for data generation and filtering.
Simultaneously, we develope a visualization interface that enables users to
explore and deliver high-quality annotations: including class-specific visual
feature phrases and fine-grained differentiated images. Quantitative evaluation
demonstrates that OW-CLIP achieves competitive performance at 89% of
state-of-the-art performance while requiring only 3.8% self-generated data,
while outperforming SOTA approach when trained with equivalent data volumes. A
case study shows the effectiveness of the developed method and the improved
annotation quality of our visualization system.

</details>


### [37] [All-in-One Medical Image Restoration with Latent Diffusion-Enhanced Vector-Quantized Codebook Prior](https://arxiv.org/abs/2507.19874)
*Haowei Chen,Zhiwen Yang,Haotian Hou,Hui Zhang,Bingzheng Wei,Gang Zhou,Yan Xu*

Main category: cs.CV

TL;DR: DiffCode框架通过任务自适应码本和潜在扩散策略，解决了全能型医学图像恢复中不同任务的退化和信息丢失问题，实现了跨任务的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理全能型医学图像恢复（MedIR）任务中不同任务相关的多样化信息损失问题，因为不同任务涉及不同的退化和信息丢失。

Method: 提出了一种名为DiffCode的新框架，该框架利用潜在扩散增强的矢量量化码本先验。具体而言，DiffCode构建了一个任务自适应码本库来整合跨任务的特定高质量先验特征，并引入了一种潜在扩散策略，利用扩散模型的映射能力来迭代地优化潜在特征分布，从而在恢复过程中估计更精确的高质量先验特征。

Result: DiffCode在MRI超分辨率、CT去噪和PET合成三个MedIR任务中实现了优越的性能，并在定量指标和视觉质量上均表现出色。

Conclusion: DiffCode框架通过结合任务自适应码本库和潜在扩散策略，在MRI超分辨率、CT去噪和PET合成三个医学图像恢复任务中取得了优于现有方法的性能，并在定量指标和视觉质量上表现出色。

Abstract: All-in-one medical image restoration (MedIR) aims to address multiple MedIR
tasks using a unified model, concurrently recovering various high-quality (HQ)
medical images (e.g., MRI, CT, and PET) from low-quality (LQ) counterparts.
However, all-in-one MedIR presents significant challenges due to the
heterogeneity across different tasks. Each task involves distinct degradations,
leading to diverse information losses in LQ images. Existing methods struggle
to handle these diverse information losses associated with different tasks. To
address these challenges, we propose a latent diffusion-enhanced
vector-quantized codebook prior and develop \textbf{DiffCode}, a novel
framework leveraging this prior for all-in-one MedIR. Specifically, to
compensate for diverse information losses associated with different tasks,
DiffCode constructs a task-adaptive codebook bank to integrate task-specific HQ
prior features across tasks, capturing a comprehensive prior. Furthermore, to
enhance prior retrieval from the codebook bank, DiffCode introduces a latent
diffusion strategy that utilizes the diffusion model's powerful mapping
capabilities to iteratively refine the latent feature distribution, estimating
more accurate HQ prior features during restoration. With the help of the
task-adaptive codebook bank and latent diffusion strategy, DiffCode achieves
superior performance in both quantitative metrics and visual quality across
three MedIR tasks: MRI super-resolution, CT denoising, and PET synthesis.

</details>


### [38] [ATCTrack: Aligning Target-Context Cues with Dynamic Target States for Robust Vision-Language Tracking](https://arxiv.org/abs/2507.19875)
*X. Feng,S. Hu,X. Li,D. Zhang,M. Wu,J. Zhang,X. Chen,K. Huang*

Main category: cs.CV

TL;DR: ATCTrack improves vision-language tracking by dynamically aligning visual and textual cues with the target's changing state, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language trackers struggle with dynamic target states and discerning relevant words in text prompts, especially in complex long-term scenarios. This work aims to address these limitations for robust tracking.

Method: The paper proposes ATCTrack, which models target-context features to obtain multimodal cues aligned with dynamic target states. It includes a temporal visual target-context modeling approach and a method for precise target word identification and context word calibration based on textual content.

Result: The proposed ATCTrack achieves robust tracking by effectively modeling temporal visual target-context and precisely utilizing textual cues, leading to new state-of-the-art performance.

Conclusion: ATCTrack achieved new SOTA performance on mainstream benchmarks.

Abstract: Vision-language tracking aims to locate the target object in the video
sequence using a template patch and a language description provided in the
initial frame. To achieve robust tracking, especially in complex long-term
scenarios that reflect real-world conditions as recently highlighted by MGIT,
it is essential not only to characterize the target features but also to
utilize the context features related to the target. However, the visual and
textual target-context cues derived from the initial prompts generally align
only with the initial target state. Due to their dynamic nature, target states
are constantly changing, particularly in complex long-term sequences. It is
intractable for these cues to continuously guide Vision-Language Trackers
(VLTs). Furthermore, for the text prompts with diverse expressions, our
experiments reveal that existing VLTs struggle to discern which words pertain
to the target or the context, complicating the utilization of textual cues. In
this work, we present a novel tracker named ATCTrack, which can obtain
multimodal cues Aligned with the dynamic target states through comprehensive
Target-Context feature modeling, thereby achieving robust tracking.
Specifically, (1) for the visual modality, we propose an effective temporal
visual target-context modeling approach that provides the tracker with timely
visual cues. (2) For the textual modality, we achieve precise target words
identification solely based on textual content, and design an innovative
context words calibration method to adaptively utilize auxiliary context words.
(3) We conduct extensive experiments on mainstream benchmarks and ATCTrack
achieves a new SOTA performance. The code and models will be released at:
https://github.com/XiaokunFeng/ATCTrack.

</details>


### [39] [Efficient Self-Supervised Neuro-Analytic Visual Servoing for Real-time Quadrotor Control](https://arxiv.org/abs/2507.19878)
*Sebastian Mocanu,Sebastian-Ion Nae,Mihai-Eugen Barbu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 该研究提出了一种基于自监督学习和知识蒸馏的视觉控制模型，用于四旋翼飞行器，实现了更快的推理速度和更低的计算成本，同时保持了控制精度。


<details>
  <summary>Details</summary>
Motivation: 为了实现一种成本效益高、基于视觉的、无需显式几何模型或标志物的四旋翼飞行器控制模型。

Method: 该研究提出了一种结合YOLOv11和U-Net的二级分割流水线，用于鲁棒的目标前后分割以正确估计目标方向。此外，还提出了一种高效的知识蒸馏双通路系统，将分析IBVS教师的几何视觉伺服能力转移到一个紧凑的小型学生神经网络中。

Result: 学生模型实现了11倍于教师模型的推理速度，同时在显著降低的计算和内存成本下，展现了相似的控制精度，并且适用于实时机载部署。

Conclusion: 该研究提出了一种自监督神经分析的、具有成本效益的、基于视觉的四旋翼飞行器控制模型。通过知识蒸馏，学生模型实现了比教师模型快11倍的推理速度，同时在显著降低的计算和内存成本下，展现了相似的控制精度。该方法无需显式的几何模型或标志物，即可实现四旋翼飞行器的姿态和运动控制，并通过模拟到现实迁移学习进行验证。

Abstract: This work introduces a self-supervised neuro-analytical, cost efficient,
model for visual-based quadrotor control in which a small 1.7M parameters
student ConvNet learns automatically from an analytical teacher, an improved
image-based visual servoing (IBVS) controller. Our IBVS system solves numerical
instabilities by reducing the classical visual servoing equations and enabling
efficient stable image feature detection. Through knowledge distillation, the
student model achieves 11x faster inference compared to the teacher IBVS
pipeline, while demonstrating similar control accuracy at a significantly lower
computational and memory cost. Our vision-only self-supervised neuro-analytic
control, enables quadrotor orientation and movement without requiring explicit
geometric models or fiducial markers. The proposed methodology leverages
simulation-to-reality transfer learning and is validated on a small drone
platform in GPS-denied indoor environments. Our key contributions include: (1)
an analytical IBVS teacher that solves numerical instabilities inherent in
classical approaches, (2) a two-stage segmentation pipeline combining YOLOv11
with a U-Net-based mask splitter for robust anterior-posterior vehicle
segmentation to correctly estimate the orientation of the target, and (3) an
efficient knowledge distillation dual-path system, which transfers geometric
visual servoing capabilities from the analytical IBVS teacher to a compact and
small student neural network that outperforms the teacher, while being suitable
for real-time onboard deployment.

</details>


### [40] [FedS2R: One-Shot Federated Domain Generalization for Synthetic-to-Real Semantic Segmentation in Autonomous Driving](https://arxiv.org/abs/2507.19881)
*Tao Lian,Jose L. Gómez,Antonio M. López*

Main category: cs.CV

TL;DR: FedS2R 是首个用于自动驾驶语义分割的单镜头联邦域泛化框架，通过数据增强和知识蒸馏提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决联邦域泛化在自动驾驶语义分割领域的应用潜力尚未被充分探索的问题，并提出首个用于合成到真实语义分割的单镜头联邦域泛化框架 FedS2R。

Method: FedS2R 框架包含两个组件：一个不一致性驱动的数据增强策略，用于生成不稳定的类别图像；一个多客户端知识蒸馏方案，带有特征融合，用于从多个客户端模型中蒸馏出全局模型。

Result: 在 Cityscapes、BDD100K、Mapillary、IDD 和 ACDC 五个真实世界数据集上的实验表明，全局模型显著优于各个客户端模型，并且仅比同时访问所有客户端数据的模型训练出来的模型低 2 mIoU 分。

Conclusion: FedS2R 框架在联邦学习的自动驾驶合成到真实场景语义分割任务中是有效的。

Abstract: Federated domain generalization has shown promising progress in image
classification by enabling collaborative training across multiple clients
without sharing raw data. However, its potential in the semantic segmentation
of autonomous driving remains underexplored. In this paper, we propose FedS2R,
the first one-shot federated domain generalization framework for
synthetic-to-real semantic segmentation in autonomous driving. FedS2R comprises
two components: an inconsistency-driven data augmentation strategy that
generates images for unstable classes, and a multi-client knowledge
distillation scheme with feature fusion that distills a global model from
multiple client models. Experiments on five real-world datasets, Cityscapes,
BDD100K, Mapillary, IDD, and ACDC, show that the global model significantly
outperforms individual client models and is only 2 mIoU points behind the model
trained with simultaneous access to all client data. These results demonstrate
the effectiveness of FedS2R in synthetic-to-real semantic segmentation for
autonomous driving under federated learning

</details>


### [41] [Interpretable Open-Vocabulary Referring Object Detection with Reverse Contrast Attention](https://arxiv.org/abs/2507.19891)
*Drandreb Earl O. Juanico,Rowel O. Atienza,Jeffrey Kenneth Go*

Main category: cs.CV

TL;DR: RCA 是一种即插即用方法，可增强视觉语言 Transformer 中的目标定位，无需重新训练。通过调整注意力机制，RCA 能够提高在 OV-RefOD 上的性能，并且在多个 VLM 上均表现出积极效果。


<details>
  <summary>Details</summary>
Motivation: RCA 旨在增强视觉语言 Transformer 中的目标定位能力，而无需重新训练。

Method: RCA 是一种即插即用方法，通过抑制极端激活并放大人体中间激活来重新加权最终注意力，以引导预测。

Result: RCA 改进了 11 个开源 VLM 中的 OV-RefOD 的 FitAP 指标，提升幅度高达 +26.6%。

Conclusion: RCA 提供了解释性和性能提升，适用于多模态 Transformer。

Abstract: We propose Reverse Contrast Attention (RCA), a plug-in method that enhances
object localization in vision-language transformers without retraining. RCA
reweights final-layer attention by suppressing extremes and amplifying
mid-level activations to let semantically relevant but subdued tokens guide
predictions. We evaluate it on Open Vocabulary Referring Object Detection
(OV-RefOD), introducing FitAP, a confidence-free average precision metric based
on IoU and box area. RCA improves FitAP in 11 out of 15 open-source VLMs, with
gains up to $+26.6\%$. Effectiveness aligns with attention sharpness and fusion
timing; while late-fusion models benefit consistently, models like
$\texttt{DeepSeek-VL2}$ also improve, pointing to capacity and disentanglement
as key factors. RCA offers both interpretability and performance gains for
multimodal transformers.

</details>


### [42] [TrackAny3D: Transferring Pretrained 3D Models for Category-unified 3D Point Cloud Tracking](https://arxiv.org/abs/2507.19908)
*Mengmeng Wang,Haonan Wang,Yulong Li,Xiangjie Kong,Jiaxin Du,Guojiang Shen,Feng Xia*

Main category: cs.CV

TL;DR: TrackAny3D 是一个创新的框架，通过利用参数高效的适配器、几何专家混合（MoGE）架构和时间上下文优化策略，实现了类别无关的 3D LiDAR 单目标跟踪。该方法在处理稀疏点云和几何变化方面表现出色，并在常用基准测试中取得了最先进的性能，解决了现有特定类别方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 3D LiDAR 单目标跟踪（SOT）依赖于稀疏且不规则的点云，这在尺度、运动模式和跨对象类别的结构复杂性方面的几何变化带来了挑战。目前的特定类别方法实现了良好的准确性，但对于实际应用来说不切实际，需要为每个类别单独的模型并且泛化能力有限。

Method: TrackAny3D 是第一个用于类别无关 3D SOT 的框架，它利用了大规模预训练的 3D 模型。该框架集成了参数高效的适配器来弥合预训练和跟踪任务之间的差距，同时保留了几何先验。此外，该框架引入了一个几何专家混合（MoGE）架构，该架构根据不同的几何特征自适应地激活专门的子网络。此外，还设计了一种时间上下文优化策略，该策略结合了可学习的时间令牌和一个动态掩码加权模块，以传播历史信息并减轻时间漂移。

Result: 实验表明 TrackAny3D 在类别无关的 3D SOT 上建立了新的最先进性能，展示了强大的泛化能力和竞争力。

Conclusion: TrackAny3D 在类别无关的 3D SOT 上建立了新的最先进性能，展示了强大的泛化能力和竞争力。我们希望这项工作能启发社区认识到统一模型的重要性，并进一步扩大大规模预训练模型在该领域的应用。

Abstract: 3D LiDAR-based single object tracking (SOT) relies on sparse and irregular
point clouds, posing challenges from geometric variations in scale, motion
patterns, and structural complexity across object categories. Current
category-specific approaches achieve good accuracy but are impractical for
real-world use, requiring separate models for each category and showing limited
generalization. To tackle these issues, we propose TrackAny3D, the first
framework to transfer large-scale pretrained 3D models for category-agnostic 3D
SOT. We first integrate parameter-efficient adapters to bridge the gap between
pretraining and tracking tasks while preserving geometric priors. Then, we
introduce a Mixture-of-Geometry-Experts (MoGE) architecture that adaptively
activates specialized subnetworks based on distinct geometric characteristics.
Additionally, we design a temporal context optimization strategy that
incorporates learnable temporal tokens and a dynamic mask weighting module to
propagate historical information and mitigate temporal drift. Experiments on
three commonly-used benchmarks show that TrackAny3D establishes new
state-of-the-art performance on category-agnostic 3D SOT, demonstrating strong
generalization and competitiveness. We hope this work will enlighten the
community on the importance of unified models and further expand the use of
large-scale pretrained models in this field.

</details>


### [43] [DriveIndia: An Object Detection Dataset for Diverse Indian Traffic Scenes](https://arxiv.org/abs/2507.19912)
*Rishav Kumar,D. Santhosh Reddy,P. Rajalakshmi*

Main category: cs.CV

TL;DR: DriveIndia是一个包含大量标注图像和驾驶数据的数据集，用于解决印度复杂的交通环境下的自动驾驶物体检测问题，并提供了基于YOLO模型的性能基线。


<details>
  <summary>Details</summary>
Motivation: 为了应对印度交通环境中多变天气、光照变化、异构道路基础设施以及密集混合交通模式等挑战，为鲁棒、可泛化的物体检测研究提供支持。

Method: 使用YOLO格式对66,986张高分辨率图像进行标注，涵盖24个交通相关物体类别，并记录了120+小时和3,400+公里的多样化驾驶数据。

Result: 使用YOLO系列模型进行基线测试，表现最佳的模型达到了78.7%的mAP50。

Conclusion: DriveIndia是一个大规模物体检测数据集，旨在捕捉印度交通环境的复杂性和不可预测性，为真实世界自动驾驶挑战提供了一个全面的基准。

Abstract: We introduce \textbf{DriveIndia}, a large-scale object detection dataset
purpose-built to capture the complexity and unpredictability of Indian traffic
environments. The dataset contains \textbf{66,986 high-resolution images}
annotated in YOLO format across \textbf{24 traffic-relevant object categories},
encompassing diverse conditions such as varied weather (fog, rain),
illumination changes, heterogeneous road infrastructure, and dense, mixed
traffic patterns and collected over \textbf{120+ hours} and covering
\textbf{3,400+ kilometers} across urban, rural, and highway routes. DriveIndia
offers a comprehensive benchmark for real-world autonomous driving challenges.
We provide baseline results using state-of-the-art \textbf{YOLO family models},
with the top-performing variant achieving a $mAP_{50}$ of \textbf{78.7\%}.
Designed to support research in robust, generalizable object detection under
uncertain road conditions, DriveIndia will be publicly available via the
TiHAN-IIT Hyderabad dataset repository
(https://tihan.iith.ac.in/tiand-datasets/).

</details>


### [44] [A mini-batch training strategy for deep subspace clustering networks](https://arxiv.org/abs/2507.19917)
*Yuxuan Jiang,Chenwei Yu,Zhi Lin,Xiaolan Liu*

Main category: cs.CV

TL;DR: 提出了一种小批量训练策略和无需解码器的框架，以提高DSC的可扩展性和效率，并取得了有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 现有DSC方法依赖全批量处理，并且自表达模块需要整个数据集的表示来构建自表达系数矩阵，这成为了瓶颈。

Method: 提出了一种将内存库集成到深度子空间聚类（DSC）中的小批量训练策略，以保留全局特征表示，并提出了一个无需解码器的框架，利用对比学习进行表示学习。

Result: 实现了可扩展的DSC训练，能够处理高分辨率图像，并且在COIL100和ORL数据集上取得了有竞争力的性能。

Conclusion: 该方法实现了与全批量方法相媲美的性能，并在COIL100和ORL数据集上通过微调深度网络超越了其他最先进的子空间聚类方法。

Abstract: Mini-batch training is a cornerstone of modern deep learning, offering
computational efficiency and scalability for training complex architectures.
However, existing deep subspace clustering (DSC) methods, which typically
combine an autoencoder with a self-expressive layer, rely on full-batch
processing. The bottleneck arises from the self-expressive module, which
requires representations of the entire dataset to construct a
self-representation coefficient matrix. In this work, we introduce a mini-batch
training strategy for DSC by integrating a memory bank that preserves global
feature representations. Our approach enables scalable training of deep
architectures for subspace clustering with high-resolution images, overcoming
previous limitations. Additionally, to efficiently fine-tune large-scale
pre-trained encoders for subspace clustering, we propose a decoder-free
framework that leverages contrastive learning instead of autoencoding for
representation learning. This design not only eliminates the computational
overhead of decoder training but also provides competitive performance.
Extensive experiments demonstrate that our approach not only achieves
performance comparable to full-batch methods, but outperforms other
state-of-the-art subspace clustering methods on the COIL100 and ORL datasets by
fine-tuning deep networks.

</details>


### [45] [HumanSAM: Classifying Human-centric Forgery Videos in Human Spatial, Appearance, and Motion Anomaly](https://arxiv.org/abs/2507.19924)
*Chang Liu,Yunfan Ye,Fan Zhang,Qingyang Zhou,Yuchuan Luo,Zhiping Cai*

Main category: cs.CV

TL;DR: 该研究提出了 HumanSAM 框架，通过融合视频理解和空间深度，并结合秩排序置信度增强策略，对人类伪造视频进行细粒度分类（空间、外观、运动异常），并在首个公开数据集 HFV 上取得了优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前伪造视频检测在细粒度理解伪造类型方面存在的不足，该研究旨在对人类伪造进行分类，并识别常见的伪造类型（空间、外观和运动异常）。

Method: 提出了一种名为 HumanSAM 的新框架，该框架通过融合视频理解和空间深度两个分支来生成人类伪造表示，并采用基于秩的置信度增强策略进行训练，以引入三个先验分数来学习更鲁棒的表示。

Result: HumanSAM 在二元和多类别伪造分类方面均取得了有希望的结果，与最先进的方法相比具有优势。

Conclusion: HumanSAM 在二元和多类别伪造分类方面均取得了有希望的结果，与最先进的方法相比具有优势。

Abstract: Numerous synthesized videos from generative models, especially human-centric
ones that simulate realistic human actions, pose significant threats to human
information security and authenticity. While progress has been made in binary
forgery video detection, the lack of fine-grained understanding of forgery
types raises concerns regarding both reliability and interpretability, which
are critical for real-world applications. To address this limitation, we
propose HumanSAM, a new framework that builds upon the fundamental challenges
of video generation models. Specifically, HumanSAM aims to classify
human-centric forgeries into three distinct types of artifacts commonly
observed in generated content: spatial, appearance, and motion anomaly.To
better capture the features of geometry, semantics and spatiotemporal
consistency, we propose to generate the human forgery representation by fusing
two branches of video understanding and spatial depth. We also adopt a
rank-based confidence enhancement strategy during the training process to learn
more robust representation by introducing three prior scores. For training and
evaluation, we construct the first public benchmark, the Human-centric Forgery
Video (HFV) dataset, with all types of forgeries carefully annotated
semi-automatically. In our experiments, HumanSAM yields promising results in
comparison with state-of-the-art methods, both in binary and multi-class
forgery classification.

</details>


### [46] [MambaVesselNet++: A Hybrid CNN-Mamba Architecture for Medical Image Segmentation](https://arxiv.org/abs/2507.19931)
*Qing Xu,Yanming Chen,Yue Li,Ziyu Liu,Zhenye Lou,Yixuan Zhang,Xiangjian He*

Main category: cs.CV

TL;DR: 提出MambaVesselNet++，一种结合CNN和Mamba的混合框架，用于医学图像分割，解决了传统方法感受野有限和Transformer计算成本高的问题，并在多项任务中取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于卷积的U型分割架构受限于局部感受野；现有的视觉Transformer虽然能捕捉全局上下文，但其非线性自注意力机制计算成本高昂。Mamba模型通过其线性复杂度的长距离依赖建模能力和高效的内存成本，为解决这些问题提供了可能。

Method: 提出了一种混合CNN-Mamba框架MambaVesselNet++，包含一个混合图像编码器（Hi-Encoder）和一个双焦融合解码器（BF-Decoder）。Hi-Encoder结合了纹理感知层（利用卷积捕捉低级语义特征）和Mamba（线性复杂度地建模长距离依赖）。Bi-Decoder通过跳跃连接融合Hi-Encoder的局部和全局信息，以精确生成分割掩码。

Result: 实验证明，MambaVesselNet++在各种医学图像分割任务中表现优于现有方法。

Conclusion: MambaVesselNet++在多样的医学2D、3D和实例分割任务中，超越了现有的基于卷积、Transformer和Mamba的先进方法。

Abstract: Medical image segmentation plays an important role in computer-aided
diagnosis. Traditional convolution-based U-shape segmentation architectures are
usually limited by the local receptive field. Existing vision transformers have
been widely applied to diverse medical segmentation frameworks due to their
superior capabilities of capturing global contexts. Despite the advantage, the
real-world application of vision transformers is challenged by their non-linear
self-attention mechanism, requiring huge computational costs. To address this
issue, the selective state space model (SSM) Mamba has gained recognition for
its adeptness in modeling long-range dependencies in sequential data,
particularly noted for its efficient memory costs. In this paper, we propose
MambaVesselNet++, a Hybrid CNN-Mamba framework for medical image segmentation.
Our MambaVesselNet++ is comprised of a hybrid image encoder (Hi-Encoder) and a
bifocal fusion decoder (BF-Decoder). In Hi-Encoder, we first devise the
texture-aware layer to capture low-level semantic features by leveraging
convolutions. Then, we utilize Mamba to effectively model long-range
dependencies with linear complexity. The Bi-Decoder adopts skip connections to
combine local and global information of the Hi-Encoder for the accurate
generation of segmentation masks. Extensive experiments demonstrate that
MambaVesselNet++ outperforms current convolution-based, transformer-based, and
Mamba-based state-of-the-arts across diverse medical 2D, 3D, and instance
segmentation tasks. The code is available at
https://github.com/CC0117/MambaVesselNet.

</details>


### [47] [LLMControl: Grounded Control of Text-to-Image Diffusion-based Synthesis with Multimodal LLMs](https://arxiv.org/abs/2507.19939)
*Jiaze Wang,Rui Chen,Haowang Cui*

Main category: cs.CV

TL;DR: LLM_Control uses a multimodal LLM to precisely control text-to-image generation, especially for complex prompts, improving spatial accuracy and object attribute binding.


<details>
  <summary>Details</summary>
Motivation: Existing spatial control methods for text-to-image diffusion models struggle with precise control, especially for prompts with multiple objects or complex spatial arrangements. LLM_Control aims to address these challenges by improving grounding capabilities.

Method: A multimodal LLM is used as a global controller to arrange spatial layouts, augment semantic descriptions, and bind object attributes. These control signals are then injected into the denoising network to refine attention maps based on sampling constraints.

Result: LLM_Control demonstrates competitive synthesis quality compared to state-of-the-art methods across various pre-trained text-to-image models, successfully handling challenging input conditions that were previously problematic for other methods.

Conclusion: LLM_Control is a novel framework that leverages multimodal LLMs to guide text-to-image diffusion models, achieving accurate control over spatial compositions and object attributes, outperforming existing methods in challenging scenarios.

Abstract: Recent spatial control methods for text-to-image (T2I) diffusion models have
shown compelling results. However, these methods still fail to precisely follow
the control conditions and generate the corresponding images, especially when
encountering the textual prompts that contain multiple objects or have complex
spatial compositions. In this work, we present a LLM-guided framework called
LLM\_Control to address the challenges of the controllable T2I generation task.
By improving grounding capabilities, LLM\_Control is introduced to accurately
modulate the pre-trained diffusion models, where visual conditions and textual
prompts influence the structures and appearance generation in a complementary
way. We utilize the multimodal LLM as a global controller to arrange spatial
layouts, augment semantic descriptions and bind object attributes. The obtained
control signals are injected into the denoising network to refocus and enhance
attention maps according to novel sampling constraints. Extensive qualitative
and quantitative experiments have demonstrated that LLM\_Control achieves
competitive synthesis quality compared to other state-of-the-art methods across
various pre-trained T2I models. It is noteworthy that LLM\_Control allows the
challenging input conditions on which most of the existing methods

</details>


### [48] [SCALAR: Scale-wise Controllable Visual Autoregressive Learning](https://arxiv.org/abs/2507.19946)
*Ryan Xu,Dongyang Jin,Yancheng Bai,Rui Lan,Xu Duan,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TL;DR: SCALAR improves controllable image synthesis in Visual Autoregressive models using a new Scale-wise Conditional Decoding mechanism, addressing issues with existing methods.


<details>
  <summary>Details</summary>
Motivation: Controllable image synthesis is a key focus in visual generative modeling, but existing VAR-based methods struggle with inefficient control encoding and disruptive injection mechanisms.

Method: SCALAR uses a Scale-wise Conditional Decoding mechanism for controllable generation in VAR models.

Result: SCALAR aims to improve both fidelity and efficiency in controllable generation for VAR models.

Conclusion: SCALAR is a controllable generation method based on VAR that incorporates a novel Scale-wise Conditional Decoding mechanism, aiming to improve efficiency and fidelity compared to existing methods.

Abstract: Controllable image synthesis, which enables fine-grained control over
generated outputs, has emerged as a key focus in visual generative modeling.
However, controllable generation remains challenging for Visual Autoregressive
(VAR) models due to their hierarchical, next-scale prediction style. Existing
VAR-based methods often suffer from inefficient control encoding and disruptive
injection mechanisms that compromise both fidelity and efficiency. In this
work, we present SCALAR, a controllable generation method based on VAR,
incorporating a novel Scale-wise Conditional Decoding mechanism. SCALAR
leverages a

</details>


### [49] [UniCT Depth: Event-Image Fusion Based Monocular Depth Estimation with Convolution-Compensated ViT Dual SA Block](https://arxiv.org/abs/2507.19948)
*Luoxi Jing,Dianxi Shi,Zhe Liu,Songchang Jin,Chunping Qiu,Ziteng Qiao,Yuxian Li,Jianqiang Xia*

Main category: cs.CV

TL;DR: UniCT Depth 是一种结合了 CNN 和 Transformer 的事件-图像融合方法，通过 CcViT-DA 块和 DCC 块来改进深度估计，并在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于 CNN 的融合方法在遮挡和深度差异方面存在不足，而基于 Transformer 的融合方法则缺乏深层模态交互。为了解决这些问题，需要一种能够有效融合事件和图像数据的深度估计方法。

Method: 提出了一种名为 UniCT Depth 的事件-图像融合方法，该方法统一了卷积神经网络（CNN）和 Transformer 来模拟局部和全局特征。提出了一种用于编码器的卷积补偿 ViT 双 SA（CcViT-DA）块，该块集成了上下文建模自注意（CMSA）来捕获空间依赖性，并通过模态融合自注意（MFSA）进行有效的跨模态融合。此外，还设计了注重细节的卷积（DCC）块来改善纹理细节和增强边缘表示。

Result: UniCT Depth 在关键指标上优于现有的图像、事件和融合的单目深度估计方法。

Conclusion: UniCT Depth 在单目深度估计方面优于现有的基于图像、事件和融合的单目深度估计方法。

Abstract: Depth estimation plays a crucial role in 3D scene understanding and is
extensively used in a wide range of vision tasks. Image-based methods struggle
in challenging scenarios, while event cameras offer high dynamic range and
temporal resolution but face difficulties with sparse data. Combining event and
image data provides significant advantages, yet effective integration remains
challenging. Existing CNN-based fusion methods struggle with occlusions and
depth disparities due to limited receptive fields, while Transformer-based
fusion methods often lack deep modality interaction. To address these issues,
we propose UniCT Depth, an event-image fusion method that unifies CNNs and
Transformers to model local and global features. We propose the
Convolution-compensated ViT Dual SA (CcViT-DA) Block, designed for the encoder,
which integrates Context Modeling Self-Attention (CMSA) to capture spatial
dependencies and Modal Fusion Self-Attention (MFSA) for effective cross-modal
fusion. Furthermore, we design the tailored Detail Compensation Convolution
(DCC) Block to improve texture details and enhances edge representations.
Experiments show that UniCT Depth outperforms existing image, event, and
fusion-based monocular depth estimation methods across key metrics.

</details>


### [50] [AF-CLIP: Zero-Shot Anomaly Detection via Anomaly-Focused CLIP Adaptation](https://arxiv.org/abs/2507.19949)
*Qingqing Fang,Wenxi Lv,Qinliang Su*

Main category: cs.CV

TL;DR: AF-CLIP improves zero-/few-shot anomaly detection by enhancing CLIP's visual features to focus on local anomalies using a lightweight adapter and multi-scale spatial aggregation, achieving strong performance across various datasets.


<details>
  <summary>Details</summary>
Motivation: Existing visual anomaly detection methods require substantial training samples, limiting their use in zero-/few-shot scenarios. While CLIP-based methods have been explored, they often neglect optimizing visual features for local anomalies, reducing their efficacy.

Method: The proposed AF-CLIP enhances CLIP's visual representations by introducing a lightweight adapter that focuses on anomaly-relevant patterns, optimizing both class-level and patch-level features. It also incorporates a multi-scale spatial aggregation mechanism for context consolidation and uses learnable textual prompts to characterize normal and abnormal states. The method is optimized using a composite objective function and can be extended to few-shot scenarios with memory banks.

Result: Experimental results across diverse industrial and medical datasets demonstrate the effectiveness and generalization of AF-CLIP.

Conclusion: AF-CLIP demonstrated strong zero-shot detection capability and can be extended to few-shot scenarios using memory banks, showing effectiveness and generalization across diverse datasets.

Abstract: Visual anomaly detection has been widely used in industrial inspection and
medical diagnosis. Existing methods typically demand substantial training
samples, limiting their utility in zero-/few-shot scenarios. While recent
efforts have leveraged CLIP's zero-shot recognition capability for this task,
they often ignore optimizing visual features to focus on local anomalies,
reducing their efficacy. In this work, we propose AF-CLIP (Anomaly-Focused
CLIP) by dramatically enhancing its visual representations to focus on local
defects. Our approach introduces a lightweight adapter that emphasizes
anomaly-relevant patterns in visual features, simultaneously optimizing both
class-level features for image classification and patch-level features for
precise localization. To capture anomalies of different sizes and improve
detection accuracy, prior to the adapter, we develop a multi-scale spatial
aggregation mechanism to effectively consolidate neighborhood context.
Complementing these visual enhancements, we design learnable textual prompts
that generically characterize normal and abnormal states. After optimization on
auxiliary datasets using a composite objective function, AF-CLIP demonstrates
strong zero-shot detection capability. Our method is also extended to few-shot
scenarios by extra memory banks. Experimental results across diverse industrial
and medical datasets demonstrate the effectiveness and generalization of our
proposed method. Code is available at https://github.com/Faustinaqq/AF-CLIP.

</details>


### [51] [RARE: Refine Any Registration of Pairwise Point Clouds via Zero-Shot Learning](https://arxiv.org/abs/2507.19950)
*Chengyu Zheng,Jin Huang,Honghua Chen,Mingqiang Wei*

Main category: cs.CV

TL;DR: 利用扩散模型的深度特征提升点云配准精度，无需额外训练数据，效果显著且泛化性好。


<details>
  <summary>Details</summary>
Motivation: 受扩散模型在图像语义对应方面的进展启发，旨在为点云配准算法提供一种无需训练数据集的改进方法。

Method: 提出了一种新颖的零样本方法，利用从深度图像中提取的扩散特征来增强点特征表示，并与现有方法的几何特征相结合，以建立更精确的点云对应关系。

Result: 通过实验证明，该方法能够提高现有技术在点云配准方面的性能，并具有跨数据集的鲁棒泛化能力。

Conclusion: 该方法通过结合深度图像的扩散特征和几何特征，显著提高了点云配准的准确性，并在各种数据集上展现了良好的泛化能力。

Abstract: Recent research leveraging large-scale pretrained diffusion models has
demonstrated the potential of using diffusion features to establish semantic
correspondences in images. Inspired by advancements in diffusion-based
techniques, we propose a novel zero-shot method for refining point cloud
registration algorithms. Our approach leverages correspondences derived from
depth images to enhance point feature representations, eliminating the need for
a dedicated training dataset. Specifically, we first project the point cloud
into depth maps from multiple perspectives and extract implicit knowledge from
a pretrained diffusion network as depth diffusion features. These features are
then integrated with geometric features obtained from existing methods to
establish more accurate correspondences between point clouds. By leveraging
these refined correspondences, our approach achieves significantly improved
registration accuracy. Extensive experiments demonstrate that our method not
only enhances the performance of existing point cloud registration techniques
but also exhibits robust generalization capabilities across diverse datasets.
Codes are available at https://github.com/zhengcy-lambo/RARE.git.

</details>


### [52] [Predicting Brain Responses To Natural Movies With Multimodal LLMs](https://arxiv.org/abs/2507.19956)
*Cesar Kadir Torrico Villanueva,Jiaxin Cindy Tu,Mihir Tripathy,Connor Lane,Rishab Iyer,Paul S. Scotti*

Main category: cs.CV

TL;DR: MedARC团队在Algonauts 2025挑战赛中，通过结合多种预训练模型的模态特征、使用轻量级编码器和集成方法，在预测fMRI信号方面取得了优异成绩，并提出了一种能获得更高排名的优化方案。


<details>
  <summary>Details</summary>
Motivation: 旨在解决Algonauts 2025挑战，该挑战涉及根据fMRI数据预测大脑对电影刺激的反应，重点在于提高模型对未见过（分布外）电影的泛化能力。

Method: 该方法利用了来自视频（V-JEPA2）、语音（Whisper）、文本（Llama 3.2）、视觉-文本（InternVL3）和视觉-文本-音频（Qwen2.5-Omni）等多种先进预训练模型的丰富多模态表示。提取的特征被线性投影到潜在空间，与fMRI时间序列进行时间对齐，并最终通过一个轻量级编码器映射到皮层区域，该编码器包括一个共享组头和特定于受试者的残差头。进行了大量模型变体的训练和集成。

Result: 最终提交的方案在测试集上取得了0.2085的平均皮尔逊相关系数，排名第四。研究还讨论了一种能够提升至第二名的优化方法。

Conclusion: 这项工作强调了结合不同模态的预训练模型特征、使用包含共享和单体组件的简单架构以及进行全面的模型选择和集成可以提高编码模型对新电影刺激的泛化能力。

Abstract: We present MedARC's team solution to the Algonauts 2025 challenge. Our
pipeline leveraged rich multimodal representations from various
state-of-the-art pretrained models across video (V-JEPA2), speech (Whisper),
text (Llama 3.2), vision-text (InternVL3), and vision-text-audio
(Qwen2.5-Omni). These features extracted from the models were linearly
projected to a latent space, temporally aligned to the fMRI time series, and
finally mapped to cortical parcels through a lightweight encoder comprising a
shared group head plus subject-specific residual heads. We trained hundreds of
model variants across hyperparameter settings, validated them on held-out
movies and assembled ensembles targeted to each parcel in each subject. Our
final submission achieved a mean Pearson's correlation of 0.2085 on the test
split of withheld out-of-distribution movies, placing our team in fourth place
for the competition. We further discuss a last-minute optimization that would
have raised us to second place. Our results highlight how combining features
from models trained in different modalities, using a simple architecture
consisting of shared-subject and single-subject components, and conducting
comprehensive model selection and ensembling improves generalization of
encoding models to novel movie stimuli. All code is available on GitHub.

</details>


### [53] [Pic2Diagnosis: A Method for Diagnosis of Cardiovascular Diseases from the Printed ECG Pictures](https://arxiv.org/abs/2507.19961)
*Oğuzhan Büyüksolak,İlkay Öksüz*

Main category: cs.CV

TL;DR: 一种新的心电图（ECG）分析方法，通过课程学习和模型集成，可以直接从ECG图像诊断心血管疾病（CVD），准确性高，尤其适用于资源有限的地区。


<details>
  <summary>Details</summary>
Motivation: 传统的基于心电图（ECG）的疾病诊断方法准确性有限，因为许多疾病模式源于过时的数据集和传统的逐步算法。

Method: 该研究提出了一种利用两步课程学习框架直接从ECG图像诊断心血管疾病（CVD）的方法，首先在分割掩码上对分类模型进行预训练，然后在灰度、反转的ECG图像上进行微调，并通过三个模型的集成和平均输出来进一步增强鲁棒性。

Result: 该方法在BHF ECG挑战数据集上实现了0.9534的AUC和0.7801的F1分数，优于单个模型，能够有效处理现实世界中的伪影。

Conclusion: 该方法为自动心血管疾病诊断提供了一种可靠的解决方案，特别是在资源有限的环境中，可以简化诊断过程，并为需要紧急护理的心血管疾病病例提供及时有效的干预。

Abstract: The electrocardiogram (ECG) is a vital tool for diagnosing heart diseases.
However, many disease patterns are derived from outdated datasets and
traditional stepwise algorithms with limited accuracy. This study presents a
method for direct cardiovascular disease (CVD) diagnosis from ECG images,
eliminating the need for digitization. The proposed approach utilizes a
two-step curriculum learning framework, beginning with the pre-training of a
classification model on segmentation masks, followed by fine-tuning on
grayscale, inverted ECG images. Robustness is further enhanced through an
ensemble of three models with averaged outputs, achieving an AUC of 0.9534 and
an F1 score of 0.7801 on the BHF ECG Challenge dataset, outperforming
individual models. By effectively handling real-world artifacts and simplifying
the diagnostic process, this method offers a reliable solution for automated
CVD diagnosis, particularly in resource-limited settings where printed or
scanned ECG images are commonly used. Such an automated procedure enables rapid
and accurate diagnosis, which is critical for timely intervention in CVD cases
that often demand urgent care.

</details>


### [54] [FROSS: Faster-than-Real-Time Online 3D Semantic Scene Graph Generation from RGB-D Images](https://arxiv.org/abs/2507.19993)
*Hao-Yu Hou,Chun-Yi Lee,Motoharu Sonogashira,Yasutomo Kawanishi*

Main category: cs.CV

TL;DR: FROSS 是一种新的 3D SSG 生成方法，它通过将 2D 场景图提升到 3D 空间并使用 3D 高斯分布来表示对象，从而实现了比实时更快的生成速度，并能更好地处理 3D 场景。


<details>
  <summary>Details</summary>
Motivation: 现有的 3D SSG 生成方法面临计算需求高和非增量处理的挑战，这限制了它们在实时开放世界应用中的适用性。

Method: 提出了一种名为 FROSS（Faster-than-Real-Time Online 3D Semantic Scene Graph Generation）的创新方法，用于在线和更快的实时 3D SSG 生成。该方法通过将 2D 场景图直接提升到 3D 空间，并将对象表示为 3D 高斯分布，从而消除了对精确且计算密集型点云处理的依赖。

Result: FROSS 实现了卓越的性能，并且运行速度显著快于先前的方法。

Conclusion: FROSS 在 ReplicaSSG 和 3DSSG 数据集上的实验结果表明，FROSS 在实现卓越性能的同时，运行速度显著优于先前的方法。

Abstract: The ability to abstract complex 3D environments into simplified and
structured representations is crucial across various domains. 3D semantic scene
graphs (SSGs) achieve this by representing objects as nodes and their
interrelationships as edges, facilitating high-level scene understanding.
Existing methods for 3D SSG generation, however, face significant challenges,
including high computational demands and non-incremental processing that hinder
their suitability for real-time open-world applications. To address this issue,
we propose FROSS (Faster-than-Real-Time Online 3D Semantic Scene Graph
Generation), an innovative approach for online and faster-than-real-time 3D SSG
generation that leverages the direct lifting of 2D scene graphs to 3D space and
represents objects as 3D Gaussian distributions. This framework eliminates the
dependency on precise and computationally-intensive point cloud processing.
Furthermore, we extend the Replica dataset with inter-object relationship
annotations, creating the ReplicaSSG dataset for comprehensive evaluation of
FROSS. The experimental results from evaluations on ReplicaSSG and 3DSSG
datasets show that FROSS can achieve superior performance while operating
significantly faster than prior 3D SSG generation methods. Our implementation
and dataset are publicly available at https://github.com/Howardkhh/FROSS.

</details>


### [55] [VAMPIRE: Uncovering Vessel Directional and Morphological Information from OCTA Images for Cardiovascular Disease Risk Factor Prediction](https://arxiv.org/abs/2507.20017)
*Lehan Wang,Hualiang Wang,Chubin Ou,Lushi Chen,Yunyi Liang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 该研究发布了首个用于CVD风险评估的OCTA数据集（OCTA-CVD），并提出了VAMPIRE模型，利用Mamba结构提取OCTA图像中的血管特征，实现了对CVD风险和相关血液因子状况的联合预测，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病（CVD）是全球主要的死亡原因，需要有效的风险评估方法。现有的基于视网膜影像的深度学习模型虽然引入了无创预测方法，但常用的眼底照片和光学相干断层扫描（OCT）无法捕捉关键的血管特征，且通常仅将CVD风险分为高或低，限制了预测准确性和临床实用性。

Method: 提出了一种新颖的多用途心血管疾病（CVD）风险评估范式，该范式基于光学相干断层扫描血管造影（OCTA）图像，并引入了基于Mamba的血管感知预测模型（VAMPIRE），该模型包含两个关键组件：基于Mamba的方向（MBD）模块和信息增强的形态（IEM）模块，以提取精细的血管轨迹特征和全面的血管形态知识。

Result: 实验结果表明，所提出的VAMPIRE模型在CVD风险和相关状况预测方面优于标准的分类骨干网络、基于OCTA的检测方法和眼科基础模型。研究还发布了OCTA-CVD数据集和相关代码。

Conclusion: 该研究提出了OCTA-CVD数据集和VAMPIRE模型，通过结合血管感知和信息增强的Mamba模型，实现了对心血管疾病（CVD）风险及相关血液因子状况的多维度预测，并在实验中证明了其优越性。

Abstract: Cardiovascular disease (CVD) remains the leading cause of death worldwide,
requiring urgent development of effective risk assessment methods for timely
intervention. While current research has introduced non-invasive and efficient
approaches to predict CVD risk from retinal imaging with deep learning models,
the commonly used fundus photographs and Optical Coherence Tomography (OCT)
fail to capture detailed vascular features critical for CVD assessment compared
with OCT angiography (OCTA) images. Moreover, existing methods typically
classify CVD risk only as high or low, without providing a deeper analysis on
CVD-related blood factor conditions, thus limiting prediction accuracy and
clinical utility. As a result, we propose a novel multi-purpose paradigm of CVD
risk assessment that jointly performs CVD risk and CVD-related condition
prediction, aligning with clinical experiences. Based on this core idea, we
introduce OCTA-CVD, the first OCTA dataset for CVD risk assessment, and a
Vessel-Aware Mamba-based Prediction model with Informative Enhancement
(VAMPIRE) based on OCTA enface images. Our proposed model aims to extract
crucial vascular characteristics through two key components: (1) a Mamba-Based
Directional (MBD) Module that captures fine-grained vascular trajectory
features and (2) an Information-Enhanced Morphological (IEM) Module that
incorporates comprehensive vessel morphology knowledge. Experimental results
demonstrate that our method can surpass standard classification backbones,
OCTA-based detection methods, and ophthalmologic foundation models. Our codes
and the collected OCTA-CVD dataset are available at
https://github.com/xmed-lab/VAMPIRE.

</details>


### [56] [Region-based Cluster Discrimination for Visual Representation Learning](https://arxiv.org/abs/2507.20025)
*Yin Xie,Kaicheng Yang,Xiang An,Kun Wu,Yongle Zhao,Weimo Deng,Zimin Ran,Yumeng Wang,Ziyong Feng,Roy Miles,Ismail Elezi,Jiankang Deng*

Main category: cs.CV

TL;DR: RICE improves vision-language models for dense prediction tasks by focusing on region-level understanding and joint OCR/object learning, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language contrastive models like CLIP and SigLIP rely on global representations, limiting their effectiveness for dense prediction tasks. RICE aims to address this gap by enhancing region-level visual and OCR capabilities.

Method: RICE utilizes a billion-scale candidate region dataset and a Region Transformer layer to extract regional semantics, combined with a unified region cluster discrimination loss for joint object and OCR learning.

Result: RICE enhances region-level visual and OCR capabilities, leading to improved performance on dense prediction tasks and visual perception for MLLMs.

Conclusion: RICE consistently outperforms previous methods on tasks, including segmentation, dense detection, and visual perception for Multimodal Large Language Models (MLLMs).

Abstract: Learning visual representations is foundational for a broad spectrum of
downstream tasks. Although recent vision-language contrastive models, such as
CLIP and SigLIP, have achieved impressive zero-shot performance via large-scale
vision-language alignment, their reliance on global representations constrains
their effectiveness for dense prediction tasks, such as grounding, OCR, and
segmentation. To address this gap, we introduce Region-Aware Cluster
Discrimination (RICE), a novel method that enhances region-level visual and OCR
capabilities. We first construct a billion-scale candidate region dataset and
propose a Region Transformer layer to extract rich regional semantics. We
further design a unified region cluster discrimination loss that jointly
supports object and OCR learning within a single classification framework,
enabling efficient and scalable distributed training on large-scale data.
Extensive experiments show that RICE consistently outperforms previous methods
on tasks, including segmentation, dense detection, and visual perception for
Multimodal Large Language Models (MLLMs). The pre-trained models have been
released at https://github.com/deepglint/MVT.

</details>


### [57] [TAPS : Frustratingly Simple Test Time Active Learning for VLMs](https://arxiv.org/abs/2507.20028)
*Dhruv Sarkar,Aprameyo Chakrabartty,Bibhudatta Bhanja*

Main category: cs.CV

TL;DR: 提出了一种测试时主动学习（TTAL）框架，用于在实时流场景下适应新数据，通过动态查询和更新提示来提高性能，同时满足延迟和内存限制。


<details>
  <summary>Details</summary>
Motivation: 探索在仅一次提供一个样本且需要在考虑延迟和内存限制的同时做出即时查询决策的连续数据流中，是否能有效利用Oracle。

Method: 提出了一种新颖的测试时主动学习（TTAL）框架，该框架能自适应地查询不确定的样本并动态地更新提示。该方法在实时流场景下进行操作，每次只有一个测试样本，并引入了动态调整的熵阈值用于主动查询、用于内存效率的类别平衡替换策略以及用于增强适应性的类别感知分布对齐技术。

Result: 在10个跨数据集迁移基准和4个域泛化数据集上的广泛实验证明，该方法在保持合理延迟和内存开销的同时，能够持续改进性能，超越了最先进的方法。

Conclusion: 该框架为真实世界部署提供了实用有效的解决方案，特别是在自动驾驶系统和医疗诊断等安全关键应用中。

Abstract: Test-Time Optimization enables models to adapt to new data during inference
by updating parameters on-the-fly. Recent advances in Vision-Language Models
(VLMs) have explored learning prompts at test time to improve performance in
downstream tasks. In this work, we extend this idea by addressing a more
general and practical challenge: Can we effectively utilize an oracle in a
continuous data stream where only one sample is available at a time, requiring
an immediate query decision while respecting latency and memory constraints? To
tackle this, we propose a novel Test-Time Active Learning (TTAL) framework that
adaptively queries uncertain samples and updates prompts dynamically. Unlike
prior methods that assume batched data or multiple gradient updates, our
approach operates in a real-time streaming scenario with a single test sample
per step. We introduce a dynamically adjusted entropy threshold for active
querying, a class-balanced replacement strategy for memory efficiency, and a
class-aware distribution alignment technique to enhance adaptation. The design
choices are justified using careful theoretical analysis. Extensive experiments
across 10 cross-dataset transfer benchmarks and 4 domain generalization
datasets demonstrate consistent improvements over state-of-the-art methods
while maintaining reasonable latency and memory overhead. Our framework
provides a practical and effective solution for real-world deployment in
safety-critical applications such as autonomous systems and medical
diagnostics.

</details>


### [58] [FaRMamba: Frequency-based learning and Reconstruction aided Mamba for Medical Segmentation](https://arxiv.org/abs/2507.20056)
*Ze Rong,ZiYue Zhao,Zhaoxin Wang,Lei Ma*

Main category: cs.CV

TL;DR: FaRMamba通过频率变换和自监督重建，解决了Vision Mamba在医学图像分割中的细节和结构问题，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Vision Mamba虽然能有效处理长程依赖关系（DC-LRSS），但其1D的切片和序列化处理方式破坏了局部像素邻接性，并引入低通滤波效应，导致局部高频信息捕获不足（LHICD）和二维空间结构退化（2D-SSD），进而加剧了模糊病灶边界（LBA）和高频细节丢失（LHD）的问题。

Method: 提出了一种名为FaRMamba的新型模型，它通过两个互补的模块来解决Vision Mamba在医学图像分割中遇到的挑战：1. 多尺度频率变换模块（MSFM）：利用小波、余弦和傅里叶变换来分离和重建多频带频谱，以恢复被衰减的高频信息。2. 自监督重建辅助编码器（SSRAE）：在共享的Mamba编码器上强制执行像素级重建，以恢复完整的二维空间相关性。

Result: FaRMamba在CAMUS、MRI-based Mouse-cochlea和Kvasir-Seg医学图像分割任务上，相比于卷积神经网络-Transformer混合模型和现有的Mamba变体，取得了更优越的性能，具体表现在边界精度更高、细节保留更好以及全局一致性更强，同时没有增加过多的计算开销。

Conclusion: FaRMamba通过结合多尺度频率变换模块（MSFM）和自监督重建辅助编码器（SSRAE），有效解决了Vision Mamba在医学图像分割中存在的局部高频信息捕获不足（LHICD）和二维空间结构退化（2D-SSD）问题，从而改善了模糊病灶边界（LBA）和高频细节丢失（LHD）。在CAMUS、MRI-based Mouse-cochlea和Kvasir-Seg数据集上的实验证明，FaRMamba在边界精度、细节保留和全局一致性方面优于其他模型，且计算开销可接受。该研究为未来的医学图像分割模型提供了一个灵活的、频率感知的框架。

Abstract: Accurate medical image segmentation remains challenging due to blurred lesion
boundaries (LBA), loss of high-frequency details (LHD), and difficulty in
modeling long-range anatomical structures (DC-LRSS). Vision Mamba employs
one-dimensional causal state-space recurrence to efficiently model global
dependencies, thereby substantially mitigating DC-LRSS. However, its patch
tokenization and 1D serialization disrupt local pixel adjacency and impose a
low-pass filtering effect, resulting in Local High-frequency Information
Capture Deficiency (LHICD) and two-dimensional Spatial Structure Degradation
(2D-SSD), which in turn exacerbate LBA and LHD. In this work, we propose
FaRMamba, a novel extension that explicitly addresses LHICD and 2D-SSD through
two complementary modules. A Multi-Scale Frequency Transform Module (MSFM)
restores attenuated high-frequency cues by isolating and reconstructing
multi-band spectra via wavelet, cosine, and Fourier transforms. A
Self-Supervised Reconstruction Auxiliary Encoder (SSRAE) enforces pixel-level
reconstruction on the shared Mamba encoder to recover full 2D spatial
correlations, enhancing both fine textures and global context. Extensive
evaluations on CAMUS echocardiography, MRI-based Mouse-cochlea, and Kvasir-Seg
endoscopy demonstrate that FaRMamba consistently outperforms competitive
CNN-Transformer hybrids and existing Mamba variants, delivering superior
boundary accuracy, detail preservation, and global coherence without
prohibitive computational overhead. This work provides a flexible
frequency-aware framework for future segmentation models that directly
mitigates core challenges in medical imaging.

</details>


### [59] [The Devil is in the EOS: Sequence Training for Detailed Image Captioning](https://arxiv.org/abs/2507.20077)
*Abdelrahman Mohamed,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 修正VLM的EOS偏见，生成更详细的图像描述。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLM）在图像描述任务中往往生成过于简短和笼统的描述，即使其拥有强大的视觉和语言能力。这主要是由于在交叉熵训练过程中引入了对EOS标记的偏见。

Method: 提出了一种无监督方法，通过减少模型过早预测EOS标记的偏见来鼓励生成更长、更详细的描述。

Result: 通过实验证明，该方法能够显著增加图像描述的长度和细节，但同时也会增加幻觉的比例。

Conclusion: 该研究提出了一种无监督方法来缓解视觉语言模型（VLM）图像描述生成中过早预测句子结束（EOS）标记的偏见，从而生成更长、更详细的描述。实验表明，该方法能显著增加描述的长度和细节，但也会带来预期的幻觉增加。

Abstract: Despite significant advances in vision-language models (VLMs), image
captioning often suffers from a lack of detail, with base models producing
short, generic captions. This limitation persists even though VLMs are equipped
with strong vision and language backbones. While supervised data and complex
reward functions have been proposed to improve detailed image captioning, we
identify a simpler underlying issue: a bias towards the end-of-sequence (EOS)
token, which is introduced during cross-entropy training. We propose an
unsupervised method to debias the model's tendency to predict the EOS token
prematurely. By reducing this bias, we encourage the generation of longer, more
detailed captions without the need for intricate reward functions or
supervision. Our approach is straightforward, effective, and easily applicable
to any pretrained model. We demonstrate its effectiveness through experiments
with three VLMs and on three detailed captioning benchmarks. Our results show a
substantial increase in caption length and relevant details, albeit with an
expected increase in the rate of hallucinations.

</details>


### [60] [KB-DMGen: Knowledge-Based Global Guidance and Dynamic Pose Masking for Human Image Generation](https://arxiv.org/abs/2507.20083)
*Shibang Liu,Xuemei Xie,Guangming Shi*

Main category: cs.CV

TL;DR: KB-DMGen通过结合基于知识的全局引导和动态姿势掩蔽，在生成人物图像时，既保证了姿势的准确性，又提升了整体图像质量，并在HumanArt数据集上取得了SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成人物图像时，虽然关注姿势准确性，但忽视了对整体图像质量的保证。为了同时确保全局图像质量和姿势准确性，需要新的方法。

Method: 提出了一种名为KB-DMGen的生成模型，该模型结合了基于知识的全局引导（KB）和动态姿势掩蔽（DM）技术。KB旨在利用图像特征信息来提高姿势准确性和保持整体图像质量。DM则动态调整姿势相关区域的重要性。

Result: KB-DMGen在HumanArt数据集上取得了最先进的AP和CAP结果，表明该模型在生成具有准确姿势和高质量的逼真人物图像方面表现出色。

Conclusion: KB-DMGen在HumanArt数据集上实现了新的最先进的AP和CAP结果，证明了其在提高姿势准确性和图像质量方面的有效性。

Abstract: Recent methods using diffusion models have made significant progress in human
image generation with various control signals such as pose priors. In portrait
generation, both the accuracy of human pose and the overall visual quality are
crucial for realistic synthesis. Most existing methods focus on controlling the
accuracy of generated poses, but ignore the quality assurance of the entire
image. In order to ensure the global image quality and pose accuracy, we
propose Knowledge-Based Global Guidance and Dynamic pose Masking for human
image Generation (KB-DMGen). The Knowledge Base (KB) is designed not only to
enhance pose accuracy but also to leverage image feature information to
maintain overall image quality. Dynamic Masking (DM) dynamically adjusts the
importance of pose-related regions. Experiments demonstrate the effectiveness
of our model, achieving new state-of-the-art results in terms of AP and CAP on
the HumanArt dataset. The code will be made publicly available.

</details>


### [61] [Hybrid-Domain Synergistic Transformer for Hyperspectral Image Denoising](https://arxiv.org/abs/2507.20099)
*Haoyue Li,Di Wu*

Main category: cs.CV

TL;DR: HDST是一个用于高光谱图像去噪的新框架，它利用FFT、跨域注意力机制和层级结构来处理复杂的空间-光谱噪声，并在实验中表现出优越的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法大多集中于RGB图像，难以有效处理高光谱图像（HSI）独特的空谱特征和复杂的噪声分布。

Method: 提出了一种基于频域增强和多尺度建模的混合域协同Transformer网络（HDST），实现了空间域、频域和通道域的三维协同处理。该方法集成了三个关键机制：1）引入FFT预处理模块和多频带卷积来提取跨频带相关性并解耦频谱噪声分量；2）设计了一个动态交叉域注意力模块，通过可学习的门控机制自适应地融合空间域纹理特征和频域噪声先验；3）构建了一个层级结构，其中浅层使用多尺度空洞卷积捕获全局噪声统计信息，深层通过频域后处理实现细节恢复。

Result: HDST框架在真实和合成数据集上均显著提高了降噪性能，同时保持了计算效率，验证了所提出方法的有效性。

Conclusion: HDST框架在真实和合成数据集上均显著提高了降噪性能，同时保持了计算效率，验证了所提出方法的有效性。该研究为解决高光谱图像和其他高维视觉数据中的复杂噪声耦合问题提供了新的见解和通用框架。

Abstract: Hyperspectral image denoising faces the challenge of multi-dimensional
coupling of spatially non-uniform noise and spectral correlation interference.
Existing deep learning methods mostly focus on RGB images and struggle to
effectively handle the unique spatial-spectral characteristics and complex
noise distributions of hyperspectral images (HSI). This paper proposes an HSI
denoising framework, Hybrid-Domain Synergistic Transformer Network (HDST),
based on frequency domain enhancement and multiscale modeling, achieving
three-dimensional collaborative processing of spatial, frequency and channel
domains. The method innovatively integrates three key mechanisms: (1)
introducing an FFT preprocessing module with multi-band convolution to extract
cross-band correlations and decouple spectral noise components; (2) designing a
dynamic cross-domain attention module that adaptively fuses spatial domain
texture features and frequency domain noise priors through a learnable gating
mechanism; (3) building a hierarchical architecture where shallow layers
capture global noise statistics using multiscale atrous convolution, and deep
layers achieve detail recovery through frequency domain postprocessing.
Experiments on both real and synthetic datasets demonstrate that HDST
significantly improves denoising performance while maintaining computational
efficiency, validating the effectiveness of the proposed method. This research
provides new insights and a universal framework for addressing complex noise
coupling issues in HSI and other high-dimensional visual data. The code is
available at https://github.com/lhy-cn/HDST-HSIDenoise.

</details>


### [62] [Detection of Medial Epicondyle Avulsion in Elbow Ultrasound Images via Bone Structure Reconstruction](https://arxiv.org/abs/2507.20104)
*Shizuka Akahori,Shotaro Teruya,Pragyan Shrestha,Yuichi Yoshii,Satoshi Iizuka,Akira Ikumi,Hiromitsu Tsuge,Itaru Kitahara*

Main category: cs.CV

TL;DR: 本研究提出了一种基于掩码自动编码器的超声图像重建框架，用于检测内上髁撕脱，在仅使用正常病例训练的情况下，取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 内上髁撕脱，常见于棒球运动员，涉及骨骼分离和畸形，通常表现为骨骼轮廓的不连续性。因此，学习正常骨骼的结构和连续性对于检测此类异常至关重要。

Method: 提出了一种基于掩码自动编码器的、感知结构的重建框架，该框架学习正常骨骼结构的连续性。

Result: 该方法在由16名棒球运动员的正常和撕脱超声图像组成的新型数据集中进行了评估，并在骨科监督下进行了像素级标注。我们的方法优于现有方法，实现了0.965的像素级AUC和0.967的图像级AUC。

Conclusion: 本研究提出了一种基于重建的框架，用于在仅使用正常病例训练的肘部超声图像中检测内上髁撕脱。该方法通过学习正常骨骼结构的连续性，即使在存在撕脱的情况下，模型也会尝试重建正常结构，从而在撕脱处产生大的重建误差。

Abstract: This study proposes a reconstruction-based framework for detecting medial
epicondyle avulsion in elbow ultrasound images, trained exclusively on normal
cases. Medial epicondyle avulsion, commonly observed in baseball players,
involves bone detachment and deformity, often appearing as discontinuities in
bone contour. Therefore, learning the structure and continuity of normal bone
is essential for detecting such abnormalities. To achieve this, we propose a
masked autoencoder-based, structure-aware reconstruction framework that learns
the continuity of normal bone structures. Even in the presence of avulsion, the
model attempts to reconstruct the normal structure, resulting in large
reconstruction errors at the avulsion site. For evaluation, we constructed a
novel dataset comprising normal and avulsion ultrasound images from 16 baseball
players, with pixel-level annotations under orthopedic supervision. Our method
outperformed existing approaches, achieving a pixel-wise AUC of 0.965 and an
image-wise AUC of 0.967. The dataset is publicly available at:
https://github.com/Akahori000/Ultrasound-Medial-Epicondyle-Avulsion-Dataset.

</details>


### [63] [NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization and Meta-Embedding](https://arxiv.org/abs/2507.20110)
*Shiyu Liu,Lianlei Shan*

Main category: cs.CV

TL;DR: NeuroVoxel-LM通过动态分辨率体素化和自适应元嵌入技术，解决了现有3D语言模型在处理大规模点云时的效率和精度问题。


<details>
  <summary>Details</summary>
Motivation: 现有的3D语言模型在处理稀疏、大规模点云时存在特征提取慢和表示精度有限的问题。

Method: 提出了一种名为NeuroVoxel-LM的新颖框架，该框架整合了神经辐射场（NeRF）、动态分辨率体素化和轻量级元嵌入。具体包括动态分辨率多尺度体素化（DR-MSV）技术和令牌级自适应池化轻量级元嵌入（TAP-LME）机制。

Result: 实验结果表明，DR-MSV显著提高了点云特征提取的效率和准确性，而TAP-LME在捕获NeRF权重中的细粒度语义方面优于传统的最大池化。

Conclusion: NeuroVoxel-LM框架在处理稀疏、大规模点云方面取得了显著进展，通过DR-MSV技术提高了特征提取效率和准确性，并利用TAP-LME机制增强了语义表示。

Abstract: Recent breakthroughs in Visual Language Models (VLMs) and Multimodal Large
Language Models (MLLMs) have significantly advanced 3D scene perception towards
language-driven cognition. However, existing 3D language models struggle with
sparse, large-scale point clouds due to slow feature extraction and limited
representation accuracy. To address these challenges, we propose NeuroVoxel-LM,
a novel framework that integrates Neural Radiance Fields (NeRF) with dynamic
resolution voxelization and lightweight meta-embedding. Specifically, we
introduce a Dynamic Resolution Multiscale Voxelization (DR-MSV) technique that
adaptively adjusts voxel granularity based on geometric and structural
complexity, reducing computational cost while preserving reconstruction
fidelity. In addition, we propose the Token-level Adaptive Pooling for
Lightweight Meta-Embedding (TAP-LME) mechanism, which enhances semantic
representation through attention-based weighting and residual fusion.
Experimental results demonstrate that DR-MSV significantly improves point cloud
feature extraction efficiency and accuracy, while TAP-LME outperforms
conventional max-pooling in capturing fine-grained semantics from NeRF weights.

</details>


### [64] [RESCUE: Crowd Evacuation Simulation via Controlling SDM-United Characters](https://arxiv.org/abs/2507.20117)
*Xiaolin Liu,Tianyi Zhou,Hongbo Kang,Jian Ma,Ziwen Wang,Jing Huang,Wenguo Weng,Yu-Kun Lai,Kun Li*

Main category: cs.CV

TL;DR: 通过整合SDM流程，提出了一种新的3D人群疏散模拟框架，通过SFM决策和个性化步态控制，模拟出更真实的人群行为，并支持动态规划和不平坦地形。


<details>
  <summary>Details</summary>
Motivation: 当前主流疏散模型忽略了疏散过程中出现的复杂人类行为（如碰撞、人际互动、受地形或个体体型影响的行为变化），导致无法准确模拟真实世界的逃生情况。

Method: 提出一个实时3D人群疏散模拟框架，该框架整合了3D自适应SFM决策机制和个性化步态控制马达，允许多个智能体并行移动，并具有动态人群感知能力。此外，还引入了部件级力可视化以辅助疏散分析。

Result: 实验结果表明，该框架支持疏散过程中的动态轨迹规划和每个智能体的个性化行为，并且兼容不平坦地形。

Conclusion: 该框架支持动态轨迹规划和个性化行为，兼容不平坦地形，生成的疏散结果更真实可信，为人群模拟提供了更深入的见解。

Abstract: Crowd evacuation simulation is critical for enhancing public safety, and
demanded for realistic virtual environments. Current mainstream evacuation
models overlook the complex human behaviors that occur during evacuation, such
as pedestrian collisions, interpersonal interactions, and variations in
behavior influenced by terrain types or individual body shapes. This results in
the failure to accurately simulate the escape of people in the real world. In
this paper, aligned with the sensory-decision-motor (SDM) flow of the human
brain, we propose a real-time 3D crowd evacuation simulation framework that
integrates a 3D-adaptive SFM (Social Force Model) Decision Mechanism and a
Personalized Gait Control Motor. This framework allows multiple agents to move
in parallel and is suitable for various scenarios, with dynamic crowd
awareness. Additionally, we introduce Part-level Force Visualization to assist
in evacuation analysis. Experimental results demonstrate that our framework
supports dynamic trajectory planning and personalized behavior for each agent
throughout the evacuation process, and is compatible with uneven terrain.
Visually, our method generates evacuation results that are more realistic and
plausible, providing enhanced insights for crowd simulation. The code is
available at http://cic.tju.edu.cn/faculty/likun/projects/RESCUE.

</details>


### [65] [Local2Global query Alignment for Video Instance Segmentation](https://arxiv.org/abs/2507.20120)
*Rajat Koner,Zhipeng Wang,Srinivas Parthasarathy,Chinghang Chen*

Main category: cs.CV

TL;DR: Local2Global 是一个新颖的在线视频实例分割框架，通过局部和全局查询以及 L2G-aligner 的对齐，实现了优异的时间一致性和性能，且训练简单。


<details>
  <summary>Details</summary>
Motivation: 在线视频分割方法在处理长序列和捕捉渐变变化方面表现出色，但时间一致性预测仍然是一个挑战，尤其是在在线传播中累积的噪声或漂移、突然的遮挡和场景转换等情况下。本研究旨在提出一种能够有效解决这些问题的在线视频实例分割框架。

Method: 提出了一种名为 Local2Global 的在线视频实例分割框架。该框架利用基于 DETR 的查询传播机制，引入了局部查询（捕获当前帧的对象特征）和全局查询（包含过去的时空表示）。通过一个名为 L2G-aligner 的轻量级 Transformer 解码器，实现了局部查询和全局查询的早期对齐，从而在利用当前帧信息的同时保持时间一致性，实现了平滑的帧间过渡。该方法集成在分割模型中，无需额外的复杂启发式方法或记忆机制。

Result: 在 Youtube-VIS-19/-21 和 OVIS 数据集上，使用 ResNet-50 主干网络，分别取得了 54.3 和 49.4 AP (Youtube-VIS) 以及 37.0 AP (OVIS) 的成绩，超越了现有基准。

Conclusion: Local2Global 框架通过结合局部查询和全局查询，并使用 L2G-aligner 进行对齐，有效地解决了在线视频实例分割中的时间一致性问题，在多个数据集上取得了先进的性能，同时保持了简单的在线训练方式。

Abstract: Online video segmentation methods excel at handling long sequences and
capturing gradual changes, making them ideal for real-world applications.
However, achieving temporally consistent predictions remains a challenge,
especially with gradual accumulation of noise or drift in on-line propagation,
abrupt occlusions and scene transitions. This paper introduces Local2Global, an
online framework, for video instance segmentation, exhibiting state-of-the-art
performance with simple baseline and training purely in online fashion.
Leveraging the DETR-based query propagation framework, we introduce two novel
sets of queries:(1) local queries that capture initial object-specific spatial
features from each frame and (2) global queries containing past spatio-temporal
representations. We propose the L2G-aligner, a novel lightweight transformer
decoder, to facilitate an early alignment between local and global queries.
This alignment allows our model to effectively utilize current frame
information while maintaining temporal consistency, producing a smooth
transition between frames. Furthermore, L2G-aligner is integrated within the
segmentation model, without relying on additional complex heuristics, or memory
mechanisms. Extensive experiments across various challenging VIS and VPS
datasets showcase the superiority of our method with simple online training,
surpassing current benchmarks without bells and rings. For instance, we achieve
54.3 and 49.4 AP on Youtube-VIS-19/-21 datasets and 37.0 AP on OVIS dataset
respectively withthe ResNet-50 backbone.

</details>


### [66] [Multi-output Deep-Supervised Classifier Chains for Plant Pathology](https://arxiv.org/abs/2507.20125)
*Jianping Yao,Son N. Tran*

Main category: cs.CV

TL;DR: A new model, Mo-DsCC, improves plant leaf disease classification by jointly predicting species and disease using a modified VGG-16, deep supervision, and classifier chains, outperforming existing methods in experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation for this study is the importance of plant leaf disease classification in smart agriculture for sustainable production and the need to address the limitations of current machine learning approaches that do not sufficiently study the relationship between plant species and disease types on prediction performance.

Method: The study proposes a new model named Multi-output Deep Supervised Classifier Chains (Mo-DsCC) which integrates plant species and disease prediction by chaining output layers. It utilizes a modified VGG-16 network as the backbone, incorporates deep supervision training, and employs a stack of classification chains.

Result: Intensive experiments on two benchmark datasets (Plant Village and PlantDoc) show that Mo-DsCC outperforms recent approaches (multi-model, multi-label (Power-set), multi-output, and multi-task) in terms of accuracy and F1-score.

Conclusion: The proposed Mo-DsCC model achieves better accuracy and F1-score compared to recent approaches, demonstrating its potential as a useful tool for smart agriculture.

Abstract: Plant leaf disease classification is an important task in smart agriculture
which plays a critical role in sustainable production. Modern machine learning
approaches have shown unprecedented potential in this classification task which
offers an array of benefits including time saving and cost reduction. However,
most recent approaches directly employ convolutional neural networks where the
effect of the relationship between plant species and disease types on
prediction performance is not properly studied. In this study, we proposed a
new model named Multi-output Deep Supervised Classifier Chains (Mo-DsCC) which
weaves the prediction of plant species and disease by chaining the output
layers for the two labels. Mo-DsCC consists of three components: A modified
VGG-16 network as the backbone, deep supervision training, and a stack of
classification chains. To evaluate the advantages of our model, we perform
intensive experiments on two benchmark datasets Plant Village and PlantDoc.
Comparison to recent approaches, including multi-model, multi-label
(Power-set), multi-output and multi-task, demonstrates that Mo-DsCC achieves
better accuracy and F1-score. The empirical study in this paper shows that the
application of Mo-DsCC could be a useful puzzle for smart agriculture to
benefit farms and bring new ideas to industry and academia.

</details>


### [67] [An Automated Deep Segmentation and Spatial-Statistics Approach for Post-Blast Rock Fragmentation Assessment](https://arxiv.org/abs/2507.20126)
*Yukun Yang*

Main category: cs.CV

TL;DR: 该研究提出了一种基于YOLOv12l-seg模型的端到端框架，用于实时爆炸碎片实例分割和3D空间分析，可用于自动化爆炸效应评估。


<details>
  <summary>Details</summary>
Motivation: 为了提供一种能够实时、自动化评估爆炸效应的方法，解决在野外条件下进行快速评估的需求。

Method: 1. 使用在超过500张带注释的爆破图像上微调的YOLOv12l-seg模型进行端到端处理，以实现实时实例分割（在~15 FPS下，Box mAP@0.5 ~ 0.769，Mask mAP@0.5 ~ 0.800）。 2. 将高保真掩模转换为归一化的3D坐标。 3. 从3D坐标中提取多指标空间描述符，包括主成分方向、核密度热点、尺寸-深度回归和Delaunay边统计。

Result: 1. 实现了实时实例分割，Box mAP@0.5 ~ 0.769，Mask mAP@0.5 ~ 0.800，速度约为15 FPS。 2. 提取了多指标空间描述符，并展示了四个代表性示例来说明关键的碎片模式。 3. 实验结果证实了该框架在准确性、对小物体拥挤的鲁棒性以及在野外条件下进行快速、自动化爆炸效应评估方面的可行性。

Conclusion: 该框架的准确性、对小物体拥挤的鲁棒性以及在野外条件下进行快速、自动化爆炸效应评估的可行性

Abstract: We introduce an end-to-end pipeline that leverages a fine-tuned YOLO12l-seg
model -- trained on over 500 annotated post-blast images -- to deliver
real-time instance segmentation (Box mAP@0.5 ~ 0.769, Mask mAP@0.5 ~ 0.800 at ~
15 FPS). High-fidelity masks are converted into normalized 3D coordinates, from
which we extract multi-metric spatial descriptors: principal component
directions, kernel density hotspots, size-depth regression, and Delaunay edge
statistics. We present four representative examples to illustrate key
fragmentation patterns. Experimental results confirm the framework's accuracy,
robustness to small-object crowding, and feasibility for rapid, automated
blast-effect assessment in field conditions.

</details>


### [68] [Wavelet-guided Misalignment-aware Network for Visible-Infrared Object Detection](https://arxiv.org/abs/2507.20146)
*Haote Zhang,Lipeng Gu,Wuzhou Quan,Fu Lee Wang,Honghui Fan,Jiali Tang,Dingkun Zhu,Haoran Xie,Xiaoping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: WMNet通过小波分析和自适应融合来解决可见光-红外目标检测中的模态失准问题，在多个数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决可见光-红外图像目标检测中因分辨率差异、空间位移和模态不一致等问题导致的常见错误对齐现象。

Method: 提出了一种名为“小波引导的失准感知网络”（WMNet）的统一框架，该框架利用小波变换进行多频分析，并采用感知融合机制来处理跨模态特征的对齐和整合。通过结合低频和高频信息，并引入跨模态自适应引导，WMNet能够减轻噪声、光照变化和空间失准带来的不利影响。

Result: WMNet通过利用互补信息、处理多模态失准问题，提高了目标检测的鲁棒性，并能有效抑制噪声、光照变化和空间失准的不利影响，同时增强目标特征表示，抑制虚假信息，从而实现更准确、更鲁棒的检测。

Conclusion: WMNet在DVTOD、DroneVehicle和M3FD数据集上实现了最先进的性能，证明了其在多模态目标检测任务中的有效性和实用性。

Abstract: Visible-infrared object detection aims to enhance the detection robustness by
exploiting the complementary information of visible and infrared image pairs.
However, its performance is often limited by frequent misalignments caused by
resolution disparities, spatial displacements, and modality inconsistencies. To
address this issue, we propose the Wavelet-guided Misalignment-aware Network
(WMNet), a unified framework designed to adaptively address different
cross-modal misalignment patterns. WMNet incorporates wavelet-based
multi-frequency analysis and modality-aware fusion mechanisms to improve the
alignment and integration of cross-modal features. By jointly exploiting low
and high-frequency information and introducing adaptive guidance across
modalities, WMNet alleviates the adverse effects of noise, illumination
variation, and spatial misalignment. Furthermore, it enhances the
representation of salient target features while suppressing spurious or
misleading information, thereby promoting more accurate and robust detection.
Extensive evaluations on the DVTOD, DroneVehicle, and M3FD datasets demonstrate
that WMNet achieves state-of-the-art performance on misaligned cross-modal
object detection tasks, confirming its effectiveness and practical
applicability.

</details>


### [69] [GT-Mean Loss: A Simple Yet Effective Solution for Brightness Mismatch in Low-Light Image Enhancement](https://arxiv.org/abs/2507.20148)
*Jingxi Liao,Shijie Hao,Richang Hong,Meng Wang*

Main category: cs.CV

TL;DR: 本研究提出了一种新的 GT-mean 损失函数，用于解决监督式低照度图像增强（LLIE）中存在的亮度不匹配问题。该损失函数能够提升现有 LLIE 模型在各种数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的监督式低照度图像增强（LLIE）研究中，增强后的图像与真实值图像在整体亮度上存在不一致（亮度不匹配）的问题，这会误导模型训练，但目前研究中该问题被忽视。

Method: 提出了一种名为 GT-mean loss 的新损失函数，该函数直接从概率论角度模拟图像的均值。GT-mean 损失函数可以轻松地应用于现有的监督式 LLIE 损失函数中，并且计算成本增加极小。

Result: 通过大量实验证明，GT-mean 损失函数的引入能够带来持续的性能提升，并且在不同的模型和数据集上都有效。

Conclusion: GT-mean 损失函数能够提升现有 LLIE 模型在各种数据集上的性能。

Abstract: Low-light image enhancement (LLIE) aims to improve the visual quality of
images captured under poor lighting conditions. In supervised LLIE research,
there exists a significant yet often overlooked inconsistency between the
overall brightness of an enhanced image and its ground truth counterpart,
referred to as brightness mismatch in this study. Brightness mismatch
negatively impact supervised LLIE models by misleading model training. However,
this issue is largely neglected in current research. In this context, we
propose the GT-mean loss, a simple yet effective loss function directly
modeling the mean values of images from a probabilistic perspective. The
GT-mean loss is flexible, as it extends existing supervised LLIE loss functions
into the GT-mean form with minimal additional computational costs. Extensive
experiments demonstrate that the incorporation of the GT-mean loss results in
consistent performance improvements across various methods and datasets.

</details>


### [70] [Trust the Model: Compact VLMs as In-Context Judges for Image-Text Data Quality](https://arxiv.org/abs/2507.20156)
*Daulet Toibazar,Kesen Wang,Sherif Mohamed,Abdulaziz Al-Badawi,Abdulrahman Alfulayt,Pedro J. Moreno*

Main category: cs.CV

TL;DR: 该研究提出了一种使用小型、高性能VLM过滤训练数据的框架，以提高视觉语言模型的性能，并减少数据处理的开销。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLM）将视觉数据整合到大型语言模型中，在保持数据质量方面面临新的挑战。实证研究表明，精心策划和具有代表性的训练样本通常比仅仅增加数据量能带来更好的结果。

Method: 该研究引入了一个简化的数据过滤框架，该框架采用了一个紧凑的视觉语言模型（VLM）。该模型在一个高质量的图像-字幕标注数据集上进行了微调，能够根据字幕和图像的质量以及匹配度来评估和过滤潜在的训练样本。与以往在现有大型VLM之上添加辅助过滤模块的方法不同，该研究的方法仅利用了一个专门构建的小型VLM固有的评估能力，从而无需额外的模块并降低了训练开销。

Result: 实验结果表明，使用该紧凑型VLM进行高精度过滤的数据集，其表现与更大、更杂乱的数据集相当，甚至更优。

Conclusion: 该方法提供了一种轻量级但强大的解决方案，用于构建高质量的视觉-语言训练语料库。

Abstract: Vision-language models (VLMs) extend the conventional large language models
by integrating visual data, enabling richer multimodal reasoning and
significantly broadens the practical applications of AI. However, including
visual inputs also brings new challenges in maintaining data quality. Empirical
evidence consistently shows that carefully curated and representative training
examples often yield superior results compared to simply increasing the
quantity of data. Inspired by this observation, we introduce a streamlined data
filtration framework that employs a compact VLM, fine-tuned on a high-quality
image-caption annotated dataset. This model effectively evaluates and filters
potential training samples based on caption and image quality and alignment.
Unlike previous approaches, which typically add auxiliary filtration modules on
top of existing full-scale VLMs, our method exclusively utilizes the inherent
evaluative capability of a purpose-built small VLM. This strategy eliminates
the need for extra modules and reduces training overhead. Our lightweight model
efficiently filters out inaccurate, noisy web data, improving image-text
alignment and caption linguistic fluency. Experimental results show that
datasets underwent high-precision filtration using our compact VLM perform on
par with, or even surpass, larger and noisier datasets gathered through
high-volume web crawling. Thus, our method provides a lightweight yet robust
solution for building high-quality vision-language training corpora. \\
\textbf{Availability and implementation:} Our compact VLM filtration model,
training data, utility scripts, and Supplementary data (Appendices) are freely
available at https://github.com/daulettoibazar/Compact_VLM_Filter.

</details>


### [71] [AnimeColor: Reference-based Animation Colorization with Diffusion Transformers](https://arxiv.org/abs/2507.20158)
*Yuhong Zhang,Liyao Wang,Han Wang,Danni Wu,Zuzeng Lin,Feng Wang,Li Song*

Main category: cs.CV

TL;DR: AnimeColor uses Diffusion Transformers and novel color extraction components (HCE, LCG) to achieve accurate and temporally consistent animation colorization, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing animation colorization methods struggle with color accuracy and temporal consistency. This paper addresses these challenges by proposing a novel framework, AnimeColor.

Method: AnimeColor utilizes Diffusion Transformers (DiT) by integrating sketch sequences into a DiT-based video diffusion model. It employs a High-level Color Extractor (HCE) to capture semantic color information and a Low-level Color Guider (LCG) to extract fine-grained color details from reference images. These components guide the video diffusion process, and a multi-stage training strategy is used for optimal utilization of reference image color information.

Result: Extensive experiments demonstrate that AnimeColor outperforms existing methods in color accuracy, sketch alignment, temporal consistency, and visual quality. The framework advances the state of the art in animation colorization and provides a practical solution for industrial applications.

Conclusion: AnimeColor is a novel reference-based animation colorization framework that leverages Diffusion Transformers (DiT). It integrates sketch sequences into a DiT-based video diffusion model for sketch-controlled animation generation. The framework introduces a High-level Color Extractor (HCE) and a Low-level Color Guider (LCG) to capture semantic and fine-grained color information from reference images, respectively. A multi-stage training strategy is employed to maximize the utilization of reference image color information. Extensive experiments show that AnimeColor outperforms existing methods in color accuracy, sketch alignment, temporal consistency, and visual quality, offering a practical solution for industrial applications.

Abstract: Animation colorization plays a vital role in animation production, yet
existing methods struggle to achieve color accuracy and temporal consistency.
To address these challenges, we propose \textbf{AnimeColor}, a novel
reference-based animation colorization framework leveraging Diffusion
Transformers (DiT). Our approach integrates sketch sequences into a DiT-based
video diffusion model, enabling sketch-controlled animation generation. We
introduce two key components: a High-level Color Extractor (HCE) to capture
semantic color information and a Low-level Color Guider (LCG) to extract
fine-grained color details from reference images. These components work
synergistically to guide the video diffusion process. Additionally, we employ a
multi-stage training strategy to maximize the utilization of reference image
color information. Extensive experiments demonstrate that AnimeColor
outperforms existing methods in color accuracy, sketch alignment, temporal
consistency, and visual quality. Our framework not only advances the state of
the art in animation colorization but also provides a practical solution for
industrial applications. The code will be made publicly available at
\href{https://github.com/IamCreateAI/AnimeColor}{https://github.com/IamCreateAI/AnimeColor}.

</details>


### [72] [Player-Centric Multimodal Prompt Generation for Large Language Model Based Identity-Aware Basketball Video Captioning](https://arxiv.org/abs/2507.20163)
*Zeyu Xi,Haoying Sun,Yaofei Wu,Junchi Yan,Haoran Zhang,Lifang Wu,Liang Wang,Changwen Chen*

Main category: cs.CV

TL;DR: 该研究提出了一种名为LLM-IAVC的新模型，通过整合球员身份信息和视频内容，生成更准确的体育视频描述。模型包含球员身份识别和视觉上下文学习模块，并在新数据集NBA-Identity和现有数据集VC-NBA-2022上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有体育视频描述方法主要关注动作而忽略球员身份，限制了其应用性。一些方法虽然尝试融合额外信息，但可能因信息与视频内容无关而导致球员身份识别错误。因此，有必要提出一种能够从视觉角度识别球员身份并生成身份感知描述的方法。

Method: 提出了一种以球员为中心的、多模态的提示生成网络（LLM-IAVC），用于生成包含球员身份的体育视频描述。该网络包括一个信息提取模块（IRIEM），用于提取与球员相关的多模态嵌入，IRIEM包含球员识别网络（PIN）和双向语义交互模块（BSIM）。此外，还设计了一个视觉上下文学习模块（VCLM）来捕获关键视频上下文信息。最后，将这些模块的输出作为大型语言模型（LLM）的多模态提示，以生成包含球员身份的描述。

Result: LLM-IAVC模型在NBA-Identity和VC-NBA-2022数据集上实现了先进的性能，证明了其有效性。

Conclusion: 该研究提出的LLM-IAVC模型在NBA-Identity和VC-NBA-2022数据集上取得了先进的性能。

Abstract: Existing sports video captioning methods often focus on the action yet
overlook player identities, limiting their applicability. Although some methods
integrate extra information to generate identity-aware descriptions, the player
identities are sometimes incorrect because the extra information is independent
of the video content. This paper proposes a player-centric multimodal prompt
generation network for identity-aware sports video captioning (LLM-IAVC), which
focuses on recognizing player identities from a visual perspective.
Specifically, an identity-related information extraction module (IRIEM) is
designed to extract player-related multimodal embeddings. IRIEM includes a
player identification network (PIN) for extracting visual features and player
names, and a bidirectional semantic interaction module (BSIM) to link player
features with video content for mutual enhancement. Additionally, a visual
context learning module (VCLM) is designed to capture the key video context
information. Finally, by integrating the outputs of the above modules as the
multimodal prompt for the large language model (LLM), it facilitates the
generation of descriptions with player identities. To support this work, we
construct a new benchmark called NBA-Identity, a large identity-aware
basketball video captioning dataset with 9,726 videos covering 9 major event
types. The experimental results on NBA-Identity and VC-NBA-2022 demonstrate
that our proposed model achieves advanced performance. Code and dataset are
publicly available at https://github.com/Zeyu1226-mt/LLM-IAVC.

</details>


### [73] [PUMPS: Skeleton-Agnostic Point-based Universal Motion Pre-Training for Synthesis in Human Motion Tasks](https://arxiv.org/abs/2507.20170)
*Clinton Ansun Mo,Kun Hu,Chengjiang Long,Dong Yuan,Wan-Chi Siu,Zhiyong Wang*

Main category: cs.CV

TL;DR: PUMPS 是一种用于时间点云 (TPCs) 的新自动编码器架构，它能有效地处理和合成运动数据，解决了跨骨架迁移的挑战。该模型在多种运动任务中表现出色，并能在不牺牲通用性的前提下进行微调以适应特定任务。


<details>
  <summary>Details</summary>
Motivation: 现有的运动骨架驱动的 3D 角色动画方法难以跨不同比例或结构的骨架迁移运动数据，给数据驱动的运动合成带来了挑战。时间点云 (TPCs) 作为一种不依赖于骨架的、跨兼容的运动表示，虽然可以与骨架相互转换，但主要用于兼容性，并未直接用于运动任务学习。直接用于运动任务学习需要 TPC 格式的数据合成能力，而这在时间一致性和点标识性方面存在未被探索的挑战。

Method: 提出了一种名为 PUMPS 的原始自动编码器架构，用于处理时间点云 (TPCs)。PUMPS 通过逐帧将点云压缩为可采样特征向量，然后利用潜在高斯噪声向量作为采样标识符，从这些特征向量中提取不同的时间点。该方法引入了基于线性分配的点配对来优化 TPC 重建过程，并避免使用昂贵的点对点注意力机制。

Result: PUMPS 在运动合成任务（预测、过渡、插值）上表现优于现有方法，即使在没有特定数据集监督的情况下也是如此。在运动去噪和估计任务上进行微调后，PUMPS 的表现也优于许多现有方法，展现了其通用性。

Conclusion: PUMPS 预训练的运动合成模型在运动预测、过渡生成和关键帧插值等任务上表现出色，即使没有本地数据集监督，也能达到最先进的性能。在针对运动去噪或估计进行微调时，PUMPS 性能优于许多现有方法，同时保持了其通用架构。

Abstract: Motion skeletons drive 3D character animation by transforming bone
hierarchies, but differences in proportions or structure make motion data hard
to transfer across skeletons, posing challenges for data-driven motion
synthesis. Temporal Point Clouds (TPCs) offer an unstructured, cross-compatible
motion representation. Though reversible with skeletons, TPCs mainly serve for
compatibility, not for direct motion task learning. Doing so would require data
synthesis capabilities for the TPC format, which presents unexplored challenges
regarding its unique temporal consistency and point identifiability. Therefore,
we propose PUMPS, the primordial autoencoder architecture for TPC data. PUMPS
independently reduces frame-wise point clouds into sampleable feature vectors,
from which a decoder extracts distinct temporal points using latent Gaussian
noise vectors as sampling identifiers. We introduce linear assignment-based
point pairing to optimise the TPC reconstruction process, and negate the use of
expensive point-wise attention mechanisms in the architecture. Using these
latent features, we pre-train a motion synthesis model capable of performing
motion prediction, transition generation, and keyframe interpolation. For these
pre-training tasks, PUMPS performs remarkably well even without native dataset
supervision, matching state-of-the-art performance. When fine-tuned for motion
denoising or estimation, PUMPS outperforms many respective methods without
deviating from its generalist architecture.

</details>


### [74] [LRR-Bench: Left, Right or Rotate? Vision-Language models Still Struggle With Spatial Understanding Tasks](https://arxiv.org/abs/2507.20174)
*Fei Kong,Jinhao Duan,Kaidi Xu,Zhenhua Guo,Xiaofeng Zhu,Xiaoshuang Shi*

Main category: cs.CV

TL;DR: 这项工作提出了一种评估VLMs空间理解能力的方法和基准测试，发现在空间感知方面，特别是3D空间理解和运动感知上，现有VLMs的表现远不如人类，仍有很大的改进空间。


<details>
  <summary>Details</summary>
Motivation: 现实世界的应用（如自动驾驶和人形机器人操作）需要精确的空间感知，但目前对视觉语言模型（VLMs）如何识别空间关系和感知空间运动的研究不足。

Method: 提出一个空间评估流程，并构建了一个相应的基准测试。该基准测试将空间理解分为绝对空间理解（例如，物体在图像中的左右位置）和3D空间理解（包括运动和旋转）。数据集完全是合成的，便于低成本生成测试样本并防止数据污染。

Result: 在多个先进的VLMs上进行实验，发现在空间理解能力方面，VLMs的性能与人类相比存在显著差距。人类在所有任务上表现接近完美，而VLMs仅在两项最简单的任务上达到人类水平，其余任务表现明显低于人类，最佳VLMs在多项任务上的得分甚至接近零。

Conclusion: VLMs在空间理解方面仍有很大提升空间，人类在简单任务上表现接近完美，而VLMs仅在最简单的任务上达到与人类相当的水平，在更复杂的任务上表现明显逊色，甚至在多项任务上得分接近零。

Abstract: Real-world applications, such as autonomous driving and humanoid robot
manipulation, require precise spatial perception. However, it remains
underexplored how Vision-Language Models (VLMs) recognize spatial relationships
and perceive spatial movement. In this work, we introduce a spatial evaluation
pipeline and construct a corresponding benchmark. Specifically, we categorize
spatial understanding into two main types: absolute spatial understanding,
which involves querying the absolute spatial position (e.g., left, right) of an
object within an image, and 3D spatial understanding, which includes movement
and rotation. Notably, our dataset is entirely synthetic, enabling the
generation of test samples at a low cost while also preventing dataset
contamination. We conduct experiments on multiple state-of-the-art VLMs and
observe that there is significant room for improvement in their spatial
understanding abilities. Explicitly, in our experiments, humans achieve
near-perfect performance on all tasks, whereas current VLMs attain human-level
performance only on the two simplest tasks. For the remaining tasks, the
performance of VLMs is distinctly lower than that of humans. In fact, the
best-performing Vision-Language Models even achieve near-zero scores on
multiple tasks. The dataset and code are available on
https://github.com/kong13661/LRR-Bench.

</details>


### [75] [Towards Universal Modal Tracking with Online Dense Temporal Token Learning](https://arxiv.org/abs/2507.20177)
*Yaozong Zheng,Bineng Zhong,Qihua Liang,Shengping Zhang,Guorong Li,Xianxian Li,Rongrong Ji*

Main category: cs.CV

TL;DR: ModalTracker is a universal video tracking model that learns from various data types (RGB, Thermal, Depth, Event) using a single architecture. It improves tracking by considering video context and using novel association mechanisms. It's trained in one shot, reducing burden and enhancing representation, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: To develop a universal video-level modality-aware tracking model that supports various tracking tasks (RGB, RGB+Thermal, RGB+Depth, RGB+Event) using the same architecture and parameters, by expanding inputs to video sequence level for richer context and enabling online dense temporal token association for improved trajectory propagation.

Method: ModalTracker utilizes video-level sampling and online dense temporal token association mechanisms to propagate appearance and motion information. It employs novel gated perceivers with a gated attention mechanism for adaptive cross-modal representation learning, compressed into a single set of parameters via one-shot training.

Result: Extensive experiments on visible and multi-modal benchmarks show that ModalTracker achieves new state-of-the-art performance.

Conclusion: The proposed universal video-level modality-aware tracking model, ModalTracker, achieves state-of-the-art performance on visible and multi-modal benchmarks.

Abstract: We propose a universal video-level modality-awareness tracking model with
online dense temporal token learning (called {\modaltracker}). It is designed
to support various tracking tasks, including RGB, RGB+Thermal, RGB+Depth, and
RGB+Event, utilizing the same model architecture and parameters. Specifically,
our model is designed with three core goals: \textbf{Video-level Sampling}. We
expand the model's inputs to a video sequence level, aiming to see a richer
video context from an near-global perspective. \textbf{Video-level
Association}. Furthermore, we introduce two simple yet effective online dense
temporal token association mechanisms to propagate the appearance and motion
trajectory information of target via a video stream manner. \textbf{Modality
Scalable}. We propose two novel gated perceivers that adaptively learn
cross-modal representations via a gated attention mechanism, and subsequently
compress them into the same set of model parameters via a one-shot training
manner for multi-task inference. This new solution brings the following
benefits: (i) The purified token sequences can serve as temporal prompts for
the inference in the next video frames, whereby previous information is
leveraged to guide future inference. (ii) Unlike multi-modal trackers that
require independent training, our one-shot training scheme not only alleviates
the training burden, but also improves model representation. Extensive
experiments on visible and multi-modal benchmarks show that our {\modaltracker}
achieves a new \textit{SOTA} performance. The code will be available at
https://github.com/GXNU-ZhongLab/ODTrack.

</details>


### [76] [MoCTEFuse: Illumination-Gated Mixture of Chiral Transformer Experts for Multi-Level Infrared and Visible Image Fusion](https://arxiv.org/abs/2507.20180)
*Li Jinfu,Song Hong,Xia Jianghan,Lin Yucong,Wang Ting,Shao Long,Fan Jingfan,Yang Jian*

Main category: cs.CV

TL;DR: 本研究提出了一种名为MoCTEFuse的新型红外与可见光图像融合网络，通过引入照明门控和混合手性Transformer专家来解决照明变化导致的模态偏差问题，并在多项基准测试中取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 许多现有方法在融合红外和可见光图像时，会忽略照明变化的影响，直接合并信息，导致融合结果存在模态偏差。本研究旨在解决这一问题，提出一种能够适应照明变化的融合网络。

Method: 提出了一种名为MoCTEFuse的动态多级图像融合网络，该网络采用基于照明门控的混合手性Transformer专家（MoCTE）来平衡纹理细节和物体对比度。MoCTE包含高光照和低光照专家子网络，每个子网络都基于手性Transformer融合块（CTFB）。CTFB通过非对称交叉注意力机制，在照明门控信号的指导下，动态地在主次模态之间切换并分配相应的权重。此外，该网络在多个阶段堆叠，以逐步聚合和优化模态特定和跨模态信息。为了促进鲁棒训练，还提出了一种整合照明分布和三个子损失项的竞争性损失函数。

Result: MoCTEFuse能够自适应地保留纹理细节和物体对比度，在多个数据集上实现了优越的融合性能，并在目标检测任务中取得了领先的mAP指标。

Conclusion: MoCTEFuse在DroneVehicle、MSRS、TNO和RoadScene数据集上展示了优越的融合性能，并在MFNet和DroneVehicle数据集上分别达到了70.93%和45.14%的最佳检测平均精度(mAP)。

Abstract: While illumination changes inevitably affect the quality of infrared and
visible image fusion, many outstanding methods still ignore this factor and
directly merge the information from source images, leading to modality bias in
the fused results. To this end, we propose a dynamic multi-level image fusion
network called MoCTEFuse, which applies an illumination-gated Mixture of Chiral
Transformer Experts (MoCTE) to adaptively preserve texture details and object
contrasts in balance. MoCTE consists of high- and low-illumination expert
subnetworks, each built upon the Chiral Transformer Fusion Block (CTFB). Guided
by the illumination gating signals, CTFB dynamically switches between the
primary and auxiliary modalities as well as assigning them corresponding
weights with its asymmetric cross-attention mechanism. Meanwhile, it is stacked
at multiple stages to progressively aggregate and refine modality-specific and
cross-modality information. To facilitate robust training, we propose a
competitive loss function that integrates illumination distributions with three
levels of sub-loss terms. Extensive experiments conducted on the DroneVehicle,
MSRS, TNO and RoadScene datasets show MoCTEFuse's superior fusion performance.
Finally, it achieves the best detection mean Average Precision (mAP) of 70.93%
on the MFNet dataset and 45.14% on the DroneVehicle dataset. The code and model
are released at https://github.com/Bitlijinfu/MoCTEFuse.

</details>


### [77] [SAMwave: Wavelet-Driven Feature Enrichment for Effective Adaptation of Segment Anything Model](https://arxiv.org/abs/2507.20186)
*Saurabh Yadav,Avi Gupta,Koteswar Rao Jerripothula*

Main category: cs.CV

TL;DR: SAMwave使用小波变换和复值适配器来改进SAM在复杂视觉任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于适配器和傅里叶域特征提取的方法在将SAM应用于未训练的复杂任务时存在性能限制，因为它们的高频特征提取技术受到限制。

Method: 提出了一种名为SAMwave的新方法，利用小波变换提取多尺度高频特征，并结合复值适配器来捕获复值空间-频率信息，从而使SAM的编码器能够获取对密集预测更有用的信息。

Result: SAMwave在四个具有挑战性的低级视觉任务上显著优于现有的适配方法，并且在SAM和SAM2骨干网络以及实值和复值适配器变体上都表现出一致的优越性能。

Conclusion: SAMwave通过利用小波变换提取更丰富、多尺度的高频特征，并引入能够捕获复值空间-频率信息的复值适配器，显著优于现有的SAM适配方法，在四个低级视觉任务上表现出色，并证明了其效率、灵活性和可解释性。

Abstract: The emergence of large foundation models has propelled significant advances
in various domains. The Segment Anything Model (SAM), a leading model for image
segmentation, exemplifies these advances, outperforming traditional methods.
However, such foundation models often suffer from performance degradation when
applied to complex tasks for which they are not trained. Existing methods
typically employ adapter-based fine-tuning strategies to adapt SAM for tasks
and leverage high-frequency features extracted from the Fourier domain.
However, Our analysis reveals that these approaches offer limited benefits due
to constraints in their feature extraction techniques. To overcome this, we
propose \textbf{\textit{SAMwave}}, a novel and interpretable approach that
utilizes the wavelet transform to extract richer, multi-scale high-frequency
features from input data. Extending this, we introduce complex-valued adapters
capable of capturing complex-valued spatial-frequency information via complex
wavelet transforms. By adaptively integrating these wavelet coefficients,
SAMwave enables SAM's encoder to capture information more relevant for dense
prediction. Empirical evaluations on four challenging low-level vision tasks
demonstrate that SAMwave significantly outperforms existing adaptation methods.
This superior performance is consistent across both the SAM and SAM2 backbones
and holds for both real and complex-valued adapter variants, highlighting the
efficiency, flexibility, and interpretability of our proposed method for
adapting segment anything models.

</details>


### [78] [SAViL-Det: Semantic-Aware Vision-Language Model for Multi-Script Text Detection](https://arxiv.org/abs/2507.20188)
*Mohammed-En-Nadhir Zighem,Abdenour Hadid*

Main category: cs.CV

TL;DR: SAViL-Det是一个新的视觉-语言模型，通过结合文本提示和视觉特征来改进自然场景中的文本检测，特别是在处理多样化脚本和弯曲文本方面，并在多个基准测试中取得了领先结果。


<details>
  <summary>Details</summary>
Motivation: 自然场景中的文本检测，特别是对于视觉线索不足的多样化脚本和任意形状的实例，仍然是一个挑战。现有方法未能充分利用语义上下文。

Method: SAViL-Det模型结合了预训练的CLIP模型和渐进式特征金字塔网络（AFPN）进行多尺度视觉特征融合。其核心是一个新颖的语言-视觉解码器，通过跨模态注意力将细粒度语义信息从文本提示自适应地传播到视觉特征。此外，还设计了一个文本到像素对比学习机制来对齐文本和对应的视觉像素特征。

Result: SAViL-Det在MLT-2019数据集上实现了84.8%的F分数，在CTW1500数据集上实现了90.2%的F分数，证明了其在多脚本和弯曲文本检测方面的有效性，达到了最先进的性能。

Conclusion: SAViL-Det通过集成文本提示和视觉特征，并利用新颖的语言-视觉解码器和文本到像素对比学习机制，有效增强了多脚本文本检测能力，在MLT-2019和CTW1500数据集上达到了最先进的性能。

Abstract: Detecting text in natural scenes remains challenging, particularly for
diverse scripts and arbitrarily shaped instances where visual cues alone are
often insufficient. Existing methods do not fully leverage semantic context.
This paper introduces SAViL-Det, a novel semantic-aware vision-language model
that enhances multi-script text detection by effectively integrating textual
prompts with visual features. SAViL-Det utilizes a pre-trained CLIP model
combined with an Asymptotic Feature Pyramid Network (AFPN) for multi-scale
visual feature fusion. The core of the proposed framework is a novel
language-vision decoder that adaptively propagates fine-grained semantic
information from text prompts to visual features via cross-modal attention.
Furthermore, a text-to-pixel contrastive learning mechanism explicitly aligns
textual and corresponding visual pixel features. Extensive experiments on
challenging benchmarks demonstrate the effectiveness of the proposed approach,
achieving state-of-the-art performance with F-scores of 84.8% on the benchmark
multi-lingual MLT-2019 dataset and 90.2% on the curved-text CTW1500 dataset.

</details>


### [79] [Color histogram equalization and fine-tuning to improve expression recognition of (partially occluded) faces on sign language datasets](https://arxiv.org/abs/2507.20197)
*Fabrizio Nunnari,Alakshendra Jyotsnaditya Ramkrishna Singh,Patrick Gebhard*

Main category: cs.CV

TL;DR: 计算机视觉可准确识别手语中的面部表情，面部下半部分识别效果优于上半部分，且上半部分识别准确率超越人类水平。


<details>
  <summary>Details</summary>
Motivation: 本调查的目的是量化计算机视觉方法在手语数据集上正确分类面部表情的程度，并进一步研究听力和聋人被试在情绪表达上的差异。

Method: 本研究提出了一种基于直方图均衡化和微调的颜色归一化方法，并进行了仅使用面部上半部分或下半部分进行表情识别的实验。

Result: 与人类相似，从面部下半部分识别表情（79.6%）的准确率高于从上半部分（77.9%）识别的准确率。值得注意的是，从面部上半部分识别表情的分类准确率高于人类水平。

Conclusion: 本研究表明，计算机视觉方法能够以83.8%的平均敏感度正确识别手语数据集中的面部表情，并且在不同类别之间的方差极小（0.042）。

Abstract: The goal of this investigation is to quantify to what extent computer vision
methods can correctly classify facial expressions on a sign language dataset.
We extend our experiments by recognizing expressions using only the upper or
lower part of the face, which is needed to further investigate the difference
in emotion manifestation between hearing and deaf subjects. To take into
account the peculiar color profile of a dataset, our method introduces a color
normalization stage based on histogram equalization and fine-tuning. The
results show the ability to correctly recognize facial expressions with 83.8%
mean sensitivity and very little variance (.042) among classes. Like for
humans, recognition of expressions from the lower half of the face (79.6%) is
higher than that from the upper half (77.9%). Noticeably, the classification
accuracy from the upper half of the face is higher than human level.

</details>


### [80] [When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios](https://arxiv.org/abs/2507.20198)
*Kele Shao,Keda Tao,Kejia Zhang,Sicheng Feng,Mu Cai,Yuzhang Shang,Haoxuan You,Can Qin,Yang Sui,Huan Wang*

Main category: cs.CV

TL;DR: 本文首次系统性地 survey 了多模态大语言模型（MLLM）中的 token 压缩技术，将方法按图像、视频、音频模态及变换、相似性、注意力、查询等机制分类，旨在梳理进展、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 由于 MLLMs 处理长上下文（如高分辨率图像、长视频、长音频）的能力，其在多模态任务中表现出色，但自注意力机制的二次复杂度带来了显著的计算挑战。Token 压缩技术作为一种有效降低 token 数量的方法，对于缓解这些计算瓶颈至关重要。

Method: 本研究通过对现有技术进行 survey 和综合，根据数据模态（图像、视频、音频）和底层机制（变换、相似性、注意力、查询）对多模态长上下文 token 压缩方法进行分类。

Result: 本文提供了对多模态长上下文 token 压缩领域的全面、结构化的概述，按模态和机制对现有方法进行了分类，有助于研究者快速了解和学习特定领域的方法，并指明了该领域的关键挑战和未来研究方向。

Conclusion: 本文对多模态大语言模型（MLLM）的长上下文领域中的 token 压缩技术进行了首次系统性 survey 和综合。该研究将现有方法按数据模态（图像、视频、音频）和底层机制（变换、相似性、注意力、查询）进行了分类，旨在梳理该领域的进展、挑战和未来方向，并维护一个公共知识库以追踪最新进展。

Abstract: Multimodal large language models (MLLMs) have made remarkable strides,
largely driven by their ability to process increasingly long and complex
contexts, such as high-resolution images, extended video sequences, and lengthy
audio input. While this ability significantly enhances MLLM capabilities, it
introduces substantial computational challenges, primarily due to the quadratic
complexity of self-attention mechanisms with numerous input tokens. To mitigate
these bottlenecks, token compression has emerged as an auspicious and critical
approach, efficiently reducing the number of tokens during both training and
inference. In this paper, we present the first systematic survey and synthesis
of the burgeoning field of multimodal long context token compression.
Recognizing that effective compression strategies are deeply tied to the unique
characteristics and redundancies of each modality, we categorize existing
approaches by their primary data focus, enabling researchers to quickly access
and learn methods tailored to their specific area of interest: (1)
image-centric compression, which addresses spatial redundancy in visual data;
(2) video-centric compression, which tackles spatio-temporal redundancy in
dynamic sequences; and (3) audio-centric compression, which handles temporal
and spectral redundancy in acoustic signals. Beyond this modality-driven
categorization, we further dissect methods based on their underlying
mechanisms, including transformation-based, similarity-based, attention-based,
and query-based approaches. By providing a comprehensive and structured
overview, this survey aims to consolidate current progress, identify key
challenges, and inspire future research directions in this rapidly evolving
domain. We also maintain a public repository to continuously track and update
the latest advances in this promising area.

</details>


### [81] [Dual-Stream Global-Local Feature Collaborative Representation Network for Scene Classification of Mining Area](https://arxiv.org/abs/2507.20216)
*Shuqi Fan,Haoyi Wang,Xianju Li*

Main category: cs.CV

TL;DR: 本研究提出了一种创新的双分支模型，通过融合多源数据并结合全局和局部特征提取，有效解决了矿区分类中存在的复杂空间布局和多尺度特征挑战。该模型在多个评估指标上均取得了领先性能，其中整体准确率达到83.63%。


<details>
  <summary>Details</summary>
Motivation: 矿区分类的挑战在于复杂的空间布局和多尺度特征。通过提取全局和局部特征，可以全面反映空间分布，从而更准确地捕捉矿区场景的整体特征。

Method: 提出了一种双分支融合模型，利用协同表示将全局特征分解为一组关键语义向量。该模型包含三个关键组件：(1) 多尺度全局Transformer分支：利用相邻的大尺度特征为小尺度特征生成全局通道注意特征，有效捕获多尺度特征关系。(2) 局部增强协同表示分支：利用局部特征和重构的关键语义集来优化注意权重，确保有效整合矿区局部上下文和详细特征，增强模型对细粒度空间变化的敏感性。(3) 双分支深度特征融合模块：融合两个分支的互补特征，融入更多场景信息，增强模型区分和分类复杂矿区景观的能力。最后，采用多损失计算来平衡模块集成。

Result: 模型的整体准确率为83.63%，优于其他对比模型，并在所有其他评估指标上取得了最佳性能。

Conclusion: 该模型在所有评估指标上均优于其他对比模型，整体准确率达到83.63%

Abstract: Scene classification of mining areas provides accurate foundational data for
geological environment monitoring and resource development planning. This study
fuses multi-source data to construct a multi-modal mine land cover scene
classification dataset. A significant challenge in mining area classification
lies in the complex spatial layout and multi-scale characteristics. By
extracting global and local features, it becomes possible to comprehensively
reflect the spatial distribution, thereby enabling a more accurate capture of
the holistic characteristics of mining scenes. We propose a dual-branch fusion
model utilizing collaborative representation to decompose global features into
a set of key semantic vectors. This model comprises three key components:(1)
Multi-scale Global Transformer Branch: It leverages adjacent large-scale
features to generate global channel attention features for small-scale
features, effectively capturing the multi-scale feature relationships. (2)
Local Enhancement Collaborative Representation Branch: It refines the attention
weights by leveraging local features and reconstructed key semantic sets,
ensuring that the local context and detailed characteristics of the mining area
are effectively integrated. This enhances the model's sensitivity to
fine-grained spatial variations. (3) Dual-Branch Deep Feature Fusion Module: It
fuses the complementary features of the two branches to incorporate more scene
information. This fusion strengthens the model's ability to distinguish and
classify complex mining landscapes. Finally, this study employs multi-loss
computation to ensure a balanced integration of the modules. The overall
accuracy of this model is 83.63%, which outperforms other comparative models.
Additionally, it achieves the best performance across all other evaluation
metrics.

</details>


### [82] [Motion-example-controlled Co-speech Gesture Generation Leveraging Large Language Models](https://arxiv.org/abs/2507.20220)
*Bohong Chen,Yumeng Li,Youyi Zheng,Yao-Xiang Ding,Kun Zhou*

Main category: cs.CV

TL;DR: MECo 是一个利用 LLM 生成受动作示例控制的联合语音手势的框架，解决了现有方法无法保留原始动作示例细节的问题，并在多个评估指标上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有系统通过预定义的类别标签或从动作示例中提取的隐式伪标签来实现手势控制，但这两种方法都无法保留原始动作示例中的丰富细节。因此，需要一种新的方法来更好地控制联合语音手势的生成，同时保留原始动作示例的细节。

Method:  MECo 框架通过利用大型语言模型 (LLM) 来实现由动作示例控制的联合语音手势生成。该方法通过微调 LLM 来理解语音和动作示例，从而生成能够保留示例特定特征并与语音保持一致的手势。与传统的伪标签方法不同，MECo 将动作示例作为提示结构中的显式查询上下文来指导手势生成。

Result: 实验结果表明，MECo 在 Fréchet Gesture Distance (FGD)、动作多样性和示例-手势相似性三个指标上均达到了最先进的性能。此外，MECo 框架还支持对单个身体部位的精细控制，并能处理包括动作片段、静态姿势、人类视频序列和文本描述在内的多种输入模式。

Conclusion:  MECo 在 Fréchet Gesture Distance (FGD)、动作多样性和示例-手势相似性三个指标上均展现出最先进的性能，并且能够实现对单个身体部位的精细控制，支持动作片段、静态姿势、人类视频序列和文本描述等多种输入模式。

Abstract: The automatic generation of controllable co-speech gestures has recently
gained growing attention. While existing systems typically achieve gesture
control through predefined categorical labels or implicit pseudo-labels derived
from motion examples, these approaches often compromise the rich details
present in the original motion examples. We present MECo, a framework for
motion-example-controlled co-speech gesture generation by leveraging large
language models (LLMs). Our method capitalizes on LLMs' comprehension
capabilities through fine-tuning to simultaneously interpret speech audio and
motion examples, enabling the synthesis of gestures that preserve
example-specific characteristics while maintaining speech congruence. Departing
from conventional pseudo-labeling paradigms, we position motion examples as
explicit query contexts within the prompt structure to guide gesture
generation. Experimental results demonstrate state-of-the-art performance
across three metrics: Fr\'echet Gesture Distance (FGD), motion diversity, and
example-gesture similarity. Furthermore, our framework enables granular control
of individual body parts and accommodates diverse input modalities including
motion clips, static poses, human video sequences, and textual descriptions.
Our code, pre-trained models, and videos are available at
https://robinwitch.github.io/MECo-Page.

</details>


### [83] [MambaMap: Online Vectorized HD Map Construction using State Space Model](https://arxiv.org/abs/2507.20224)
*Ruizi Yang,Xiaolu Liu,Junbo Chen,Jianke Zhu*

Main category: cs.CV

TL;DR: MambaMap 是一种高效的框架，用于在线矢量化高清地图，通过整合长程时间特征和创新的扫描策略来克服现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法未能充分利用时间信息或在处理扩展序列时产生大量计算开销的问题，本文提出了 MambaMap 框架。

Method: MambaMap 框架利用内存库存储和利用历史帧信息，动态更新 BEV 特征和实例查询，以提高对噪声和遮挡的鲁棒性。此外，通过在状态空间中引入门控机制，可以选择性地整合高计算效率的地图元素的依赖关系，并设计了多方向和时空扫描策略来增强 BEV 和实例级别的特征提取。

Result: MambaMap 提高了对噪声和遮挡的鲁棒性，并显著提高了预测精度，同时确保了稳健的时间一致性。

Conclusion: MambaMap 在 nuScenes 和 Argoverse2 数据集上的大量实验表明，该方法在各种分割和感知范围内均优于最先进的方法。

Abstract: High-definition (HD) maps are essential for autonomous driving, as they
provide precise road information for downstream tasks. Recent advances
highlight the potential of temporal modeling in addressing challenges like
occlusions and extended perception range. However, existing methods either fail
to fully exploit temporal information or incur substantial computational
overhead in handling extended sequences. To tackle these challenges, we propose
MambaMap, a novel framework that efficiently fuses long-range temporal features
in the state space to construct online vectorized HD maps. Specifically,
MambaMap incorporates a memory bank to store and utilize information from
historical frames, dynamically updating BEV features and instance queries to
improve robustness against noise and occlusions. Moreover, we introduce a
gating mechanism in the state space, selectively integrating dependencies of
map elements in high computational efficiency. In addition, we design
innovative multi-directional and spatial-temporal scanning strategies to
enhance feature extraction at both BEV and instance levels. These strategies
significantly boost the prediction accuracy of our approach while ensuring
robust temporal consistency. Extensive experiments on the nuScenes and
Argoverse2 datasets demonstrate that our proposed MambaMap approach outperforms
state-of-the-art methods across various splits and perception ranges. Source
code will be available at https://github.com/ZiziAmy/MambaMap.

</details>


### [84] [Decomposing Densification in Gaussian Splatting for Faster 3D Scene Reconstruction](https://arxiv.org/abs/2507.20239)
*Binxiao Huang,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: 通过优化的 करा densification 策略、多分辨率训练和动态修剪，显著加速了 3D 高斯泼溅的训练过程，并提高了重建质量。


<details>
  <summary>Details</summary>
Motivation: 3D 高斯泼溅 (GS) 的训练过程通常由于 करा densification 和高斯图元空间分布不佳而收敛缓慢。本研究旨在解决这一问题，提高训练效率和重建质量。

Method: 提出了一种全局到局部的 करा densification 策略，以更有效地生成高斯图元；引入了一种基于能量密度的多分辨率训练框架，以逐步提高分辨率；动态修剪不必要的高斯图元以加速训练。

Result: 实验结果表明，该方法在 MipNeRF-360、Deep Blending 和 Tanks & Temples 数据集上显著加速了训练，实现了超过 2 倍的加速，同时使用了更少的高斯图元，并获得了更优的重建性能。

Conclusion: 该方法通过全局到局部的 करा densification 策略、多分辨率训练框架和动态修剪显著加速了 3D 高斯泼溅 (GS) 的训练过程，同时提高了重建质量，实现了超过 2 倍的加速，并使用了更少的高斯图元。

Abstract: 3D Gaussian Splatting (GS) has emerged as a powerful representation for
high-quality scene reconstruction, offering compelling rendering quality.
However, the training process of GS often suffers from slow convergence due to
inefficient densification and suboptimal spatial distribution of Gaussian
primitives. In this work, we present a comprehensive analysis of the split and
clone operations during the densification phase, revealing their distinct roles
in balancing detail preservation and computational efficiency. Building upon
this analysis, we propose a global-to-local densification strategy, which
facilitates more efficient growth of Gaussians across the scene space,
promoting both global coverage and local refinement. To cooperate with the
proposed densification strategy and promote sufficient diffusion of Gaussian
primitives in space, we introduce an energy-guided coarse-to-fine
multi-resolution training framework, which gradually increases resolution based
on energy density in 2D images. Additionally, we dynamically prune unnecessary
Gaussian primitives to speed up the training. Extensive experiments on
MipNeRF-360, Deep Blending, and Tanks & Temples datasets demonstrate that our
approach significantly accelerates training,achieving over 2x speedup with
fewer Gaussian primitives and superior reconstruction performance.

</details>


### [85] [AnimalClue: Recognizing Animals by their Traces](https://arxiv.org/abs/2507.20240)
*Risa Shinoda,Nakamasa Inoue,Iro Laina,Christian Rupprecht,Hirokatsu Kataoka*

Main category: cs.CV

TL;DR: 该研究提出了AnimalClue数据集，用于从足迹、粪便等间接证据识别野生动物物种，为相关研究提供了新的资源和挑战。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉技术在自动识别野生动物方面取得了进展，但对于从足迹和粪便等间接证据识别物种的研究相对较少。该研究旨在解决这一问题，为野生动物监测提供更全面的方法。

Method: 提出AnimalClue数据集，这是一个包含159,605个边界框的大型数据集，涵盖五类间接线索（足迹、粪便、蛋、骨头、羽毛），适用于968个物种的识别。对代表性的视觉模型进行了广泛评估，以确定从动物踪迹识别物种的关键挑战。

Result: AnimalClue是第一个用于从间接证据图像识别物种的大型数据集，包含159,605个边界框，涵盖968个物种。实验评估了代表性的视觉模型，并确定了从动物踪迹识别物种的关键挑战。

Conclusion: 现有的野生动物监测方法在自动化方面取得了进展，但在从足迹和粪便等间接证据识别物种方面仍有待探索。AnimalClue数据集的推出填补了这一空白，它包含了159,605个边界框，涵盖了足迹、粪便、蛋、骨头和羽毛五类间接线索，并对968个物种进行了标注。该数据集为物种识别、检测和实例分割任务带来了新的挑战，因为它需要识别更精细、更细微的视觉特征。实验评估了代表性的视觉模型，并确定了从动物踪迹识别物种的关键挑战。

Abstract: Wildlife observation plays an important role in biodiversity conservation,
necessitating robust methodologies for monitoring wildlife populations and
interspecies interactions. Recent advances in computer vision have
significantly contributed to automating fundamental wildlife observation tasks,
such as animal detection and species identification. However, accurately
identifying species from indirect evidence like footprints and feces remains
relatively underexplored, despite its importance in contributing to wildlife
monitoring. To bridge this gap, we introduce AnimalClue, the first large-scale
dataset for species identification from images of indirect evidence. Our
dataset consists of 159,605 bounding boxes encompassing five categories of
indirect clues: footprints, feces, eggs, bones, and feathers. It covers 968
species, 200 families, and 65 orders. Each image is annotated with
species-level labels, bounding boxes or segmentation masks, and fine-grained
trait information, including activity patterns and habitat preferences. Unlike
existing datasets primarily focused on direct visual features (e.g., animal
appearances), AnimalClue presents unique challenges for classification,
detection, and instance segmentation tasks due to the need for recognizing more
detailed and subtle visual features. In our experiments, we extensively
evaluate representative vision models and identify key challenges in animal
identification from their traces. Our dataset and code are available at
https://dahlian00.github.io/AnimalCluePage/

</details>


### [86] [MIRepNet: A Pipeline and Foundation Model for EEG-Based Motor Imagery Classification](https://arxiv.org/abs/2507.20254)
*Dingkun Liu,Zhu Chen,Jingwei Luo,Shijie Lian,Dongrui Wu*

Main category: cs.CV

TL;DR: MIRepNet是第一个针对运动想象（MI）范式优化的EEG基础模型，通过创新的预处理和混合预训练策略，实现了卓越的泛化能力和准确性，在下游任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的EEG基础模型忽略了基本的范式特异性神经生理学区别，限制了它们的泛化能力。在实际的BCI部署中，通常在数据采集之前就确定了特定的范式，例如用于中风康复或辅助机器人技术的运动想象（MI）。

Method: MIRepNet包含一个高质量的EEG预处理流程，该流程采用受神经生理学启发的通道模板，可适应具有任意电极配置的EEG头戴设备。此外，我们引入了一种混合预训练策略，结合了自监督掩码标记重建和监督MI分类，从而能够通过每个类别少于30个试验的数据，快速适应和准确解码新下游MI任务。

Result: MIRepNet是第一个针对MI范式定制的EEG基础模型。

Conclusion: MIRepNet在五个公开的MI数据集上始终达到最先进的性能，显著优于专门和通用的EEG模型。

Abstract: Brain-computer interfaces (BCIs) enable direct communication between the
brain and external devices. Recent EEG foundation models aim to learn
generalized representations across diverse BCI paradigms. However, these
approaches overlook fundamental paradigm-specific neurophysiological
distinctions, limiting their generalization ability. Importantly, in practical
BCI deployments, the specific paradigm such as motor imagery (MI) for stroke
rehabilitation or assistive robotics, is generally determined prior to data
acquisition. This paper proposes MIRepNet, the first EEG foundation model
tailored for the MI paradigm. MIRepNet comprises a high-quality EEG
preprocessing pipeline incorporating a neurophysiologically-informed channel
template, adaptable to EEG headsets with arbitrary electrode configurations.
Furthermore, we introduce a hybrid pretraining strategy that combines
self-supervised masked token reconstruction and supervised MI classification,
facilitating rapid adaptation and accurate decoding on novel downstream MI
tasks with fewer than 30 trials per class. Extensive evaluations across five
public MI datasets demonstrated that MIRepNet consistently achieved
state-of-the-art performance, significantly outperforming both specialized and
generalized EEG models. Our code will be available on
GitHub\footnote{https://github.com/staraink/MIRepNet}.

</details>


### [87] [L-MCAT: Unpaired Multimodal Transformer with Contrastive Attention for Label-Efficient Satellite Image Classification](https://arxiv.org/abs/2507.20259)
*Mitul Goswami,Mrinal Goswami*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose the Lightweight Multimodal Contrastive Attention Transformer
(L-MCAT), a novel transformer-based framework for label-efficient remote
sensing image classification using unpaired multimodal satellite data. L-MCAT
introduces two core innovations: (1) Modality-Spectral Adapters (MSA) that
compress high-dimensional sensor inputs into a unified embedding space, and (2)
Unpaired Multimodal Attention Alignment (U-MAA), a contrastive self-supervised
mechanism integrated into the attention layers to align heterogeneous
modalities without pixel-level correspondence or labels. L-MCAT achieves 95.4%
overall accuracy on the SEN12MS dataset using only 20 labels per class,
outperforming state-of-the-art baselines while using 47x fewer parameters and
23x fewer FLOPs than MCTrans. It maintains over 92% accuracy even under 50%
spatial misalignment, demonstrating robustness for real-world deployment. The
model trains end-to-end in under 5 hours on a single consumer GPU.

</details>


### [88] [Controllable Feature Whitening for Hyperparameter-Free Bias Mitigation](https://arxiv.org/abs/2507.20284)
*Yooshin Cho,Hanbyel Cho,Janghyeon Lee,HyeongGwon Hong,Jaesung Ahn,Junmo Kim*

Main category: cs.CV

TL;DR: A new method called controllable feature whitening reduces bias in AI by removing unwanted correlations in data, improving reliability and fairness without complex optimization techniques. It also allows tuning the balance between performance and fairness.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are susceptible to learning spurious correlations, leading to biased and unreliable AI. Improving the reliability of AI by mitigating bias is crucial as AI use increases.

Method: The proposed framework, controllable feature whitening, quantifies and eliminates linear correlations between target and bias features using a whitening module based on the covariance matrix. It also demonstrates that fairness criteria like demographic parity and equalized odds can be handled by whitening with a re-weighted covariance matrix.

Result: The method significantly mitigates bias by removing linear correlations between features fed into the last linear classifier, without requiring regularization or adversarial learning. It also effectively handles demographic parity and equalized odds, controlling the utility-fairness trade-off. Performance was validated on four benchmark datasets, outperforming existing methods.

Conclusion: Controllable feature whitening effectively mitigates bias in deep neural networks by eliminating linear correlations between features and target variables, while also allowing control over the trade-off between utility and fairness. The method outperforms existing approaches on benchmark datasets.

Abstract: As the use of artificial intelligence rapidly increases, the development of
trustworthy artificial intelligence has become important. However, recent
studies have shown that deep neural networks are susceptible to learn spurious
correlations present in datasets. To improve the reliability, we propose a
simple yet effective framework called controllable feature whitening. We
quantify the linear correlation between the target and bias features by the
covariance matrix, and eliminate it through the whitening module. Our results
systemically demonstrate that removing the linear correlations between features
fed into the last linear classifier significantly mitigates the bias, while
avoiding the need to model intractable higher-order dependencies. A particular
advantage of the proposed method is that it does not require regularization
terms or adversarial learning, which often leads to unstable optimization in
practice. Furthermore, we show that two fairness criteria, demographic parity
and equalized odds, can be effectively handled by whitening with the
re-weighted covariance matrix. Consequently, our method controls the trade-off
between the utility and fairness of algorithms by adjusting the weighting
coefficient. Finally, we validate that our method outperforms existing
approaches on four benchmark datasets: Corrupted CIFAR-10, Biased FFHQ,
WaterBirds, and Celeb-A.

</details>


### [89] [T$^\text{3}$SVFND: Towards an Evolving Fake News Detector for Emergencies with Test-time Training on Short Video Platforms](https://arxiv.org/abs/2507.20286)
*Liyuan Zhang,Zeyun Cheng,Yan Yang,Yong Liu,Jinke Ma*

Main category: cs.CV

TL;DR: 提出T^3SVFND框架，利用TTT和MLM辅助任务，提高假新闻视频检测在突发事件新闻上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻视频检测方法泛化能力不足，在面对不同事件来源的视频数据分布偏移时，性能会显著下降，尤其是在处理突发事件新闻时。

Method: 提出了一种名为T^3SVFND的新型假新闻视频检测框架，结合了测试时训练（TTT）和基于掩码语言模型（MLM）的自监督辅助任务。该模型通过在测试阶段利用辅助任务适应测试数据的分布，并融合来自音频和视频的多模态上下文信息来预测被掩盖的词语。

Result: 在公开基准数据集上的大量实验表明，T^3SVFND模型在提高假新闻视频检测鲁棒性方面是有效的，特别是在检测突发事件新闻方面表现尤为出色。

Conclusion: T^3SVFND框架通过TTT和基于MLM的辅助任务，提高了假新闻视频检测的鲁棒性，尤其是在处理突发事件新闻方面

Abstract: The existing methods for fake news videos detection may not be generalized,
because there is a distribution shift between short video news of different
events, and the performance of such techniques greatly drops if news records
are coming from emergencies. We propose a new fake news videos detection
framework (T$^3$SVFND) using Test-Time Training (TTT) to alleviate this
limitation, enhancing the robustness of fake news videos detection.
Specifically, we design a self-supervised auxiliary task based on Mask Language
Modeling (MLM) that masks a certain percentage of words in text and predicts
these masked words by combining contextual information from different
modalities (audio and video). In the test-time training phase, the model adapts
to the distribution of test data through auxiliary tasks. Extensive experiments
on the public benchmark demonstrate the effectiveness of the proposed model,
especially for the detection of emergency news.

</details>


### [90] [Fine-structure Preserved Real-world Image Super-resolution via Transfer VAE Training](https://arxiv.org/abs/2507.20291)
*Qiaosi Yi,Shuai Li,Rongyuan Wu,Lingchen Sun,Yuhui Wu,Lei Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为TVT的新策略，用于改进基于稳定扩散（SD）的图像超分辨率技术。TVT能够更好地恢复图像的精细结构，同时保持较低的计算成本，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于预训练稳定扩散（SD）模型的图像超分辨率方法在恢复图像细微结构时表现不佳，原因是SD模型中的VAE（如8倍下采样）进行了过于激进的降分辨率。虽然使用低降采样率的VAE可以解决这个问题，但如何将其潜在特征适配到预训练的UNet并控制计算成本带来了新的挑战。

Method: 提出了一种转移VAE训练（TVT）策略，通过先训练一个4倍下采样解码器，再固定该解码器训练4倍下采样编码器，将8倍下采样VAE适配到预训练的UNet，并引入了紧凑型VAE和计算效率更高的UNet。

Result: 实验结果表明，TVT方法显著提高了细微结构（如小字符和纹理）的恢复能力，并且所需的计算量（FLOPs）少于最先进的单步扩散模型。

Conclusion: TVT方法显著提高了细微结构（如小字符和纹理）的恢复能力，这是其他基于SD的方法的弱项，并且其计算量少于最先进的单步扩散模型。

Abstract: Impressive results on real-world image super-resolution (Real-ISR) have been
achieved by employing pre-trained stable diffusion (SD) models. However, one
critical issue of such methods lies in their poor reconstruction of image fine
structures, such as small characters and textures, due to the aggressive
resolution reduction of the VAE (eg., 8$\times$ downsampling) in the SD model.
One solution is to employ a VAE with a lower downsampling rate for diffusion;
however, adapting its latent features with the pre-trained UNet while
mitigating the increased computational cost poses new challenges. To address
these issues, we propose a Transfer VAE Training (TVT) strategy to transfer the
8$\times$ downsampled VAE into a 4$\times$ one while adapting to the
pre-trained UNet. Specifically, we first train a 4$\times$ decoder based on the
output features of the original VAE encoder, then train a 4$\times$ encoder
while keeping the newly trained decoder fixed. Such a TVT strategy aligns the
new encoder-decoder pair with the original VAE latent space while enhancing
image fine details. Additionally, we introduce a compact VAE and
compute-efficient UNet by optimizing their network architectures, reducing the
computational cost while capturing high-resolution fine-scale features.
Experimental results demonstrate that our TVT method significantly improves
fine-structure preservation, which is often compromised by other SD-based
methods, while requiring fewer FLOPs than state-of-the-art one-step diffusion
models. The official code can be found at https://github.com/Joyies/TVT.

</details>


### [91] [SWIFT: A General Sensitive Weight Identification Framework for Fast Sensor-Transfer Pansharpening](https://arxiv.org/abs/2507.20311)
*Zeyu Xia,Chenxi Sun,Tianyu Xin,Yubo Zeng,Haoyu Chen,Liang-Jian Deng*

Main category: cs.CV

TL;DR: SWIFT是一个快速通用的跨传感器适应框架，通过智能选择样本和更新模型中最敏感的权重，大大缩短了全景锐化模型的适应时间，并取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的全景锐化方法在应用于来自未见过的传感器的数据时，性能会严重下降。对这些模型进行大规模重新训练或设计更复杂的架构通常成本高昂且不切实际。

Method: SWIFT框架采用基于数据流形结构的无监督采样策略来平衡样本选择，并缓解传统最远点采样（Farthest Point Sampling）的偏差，仅选择目标域中3%信息量最大的样本。然后，通过分析源域预训练模型的参数梯度行为来探测这些样本，从而快速识别并仅更新对域迁移最敏感的权重子集。

Result: SWIFT框架将适应时间从数小时缩短到大约一分钟，并且在WorldView-2和QuickBird数据集上取得了具有竞争力的性能，甚至在某些情况下优于完全重新训练的模型。

Conclusion: SWIFT框架能够高效地适应各种现有的全景锐化模型，将适应时间从数小时缩短到大约一分钟，并且在跨传感器全景锐化任务上达到了新的最先进水平。

Abstract: Pansharpening aims to fuse high-resolution panchromatic (PAN) images with
low-resolution multispectral (LRMS) images to generate high-resolution
multispectral (HRMS) images. Although deep learning-based methods have achieved
promising performance, they generally suffer from severe performance
degradation when applied to data from unseen sensors. Adapting these models
through full-scale retraining or designing more complex architectures is often
prohibitively expensive and impractical for real-world deployment. To address
this critical challenge, we propose a fast and general-purpose framework for
cross-sensor adaptation, SWIFT (Sensitive Weight Identification for Fast
Transfer). Specifically, SWIFT employs an unsupervised sampling strategy based
on data manifold structures to balance sample selection while mitigating the
bias of traditional Farthest Point Sampling, efficiently selecting only 3\% of
the most informative samples from the target domain. This subset is then used
to probe a source-domain pre-trained model by analyzing the gradient behavior
of its parameters, allowing for the quick identification and subsequent update
of only the weight subset most sensitive to the domain shift. As a
plug-and-play framework, SWIFT can be applied to various existing pansharpening
models. Extensive experiments demonstrate that SWIFT reduces the adaptation
time from hours to approximately one minute on a single NVIDIA RTX 4090 GPU.
The adapted models not only substantially outperform direct-transfer baselines
but also achieve performance competitive with, and in some cases superior to,
full retraining, establishing a new state-of-the-art on cross-sensor
pansharpening tasks for the WorldView-2 and QuickBird datasets.

</details>


### [92] [From Gallery to Wrist: Realistic 3D Bracelet Insertion in Videos](https://arxiv.org/abs/2507.20331)
*Chenjian Gao,Lihe Ding,Rui Han,Zhanpeng Huang,Zibin Wang,Tianfan Xue*

Main category: cs.CV

TL;DR: 该研究提出了一种新颖的视频对象插入方法，结合了3D高斯泼溅和2D扩散模型，解决了现有方法在时间和光照真实感方面的不足，实现了更逼真、更一致的视频编辑效果。


<details>
  <summary>Details</summary>
Motivation: 在视频中插入3D对象（如AR、虚拟试穿、视频合成）是一个长期存在的挑战。即使在具有复杂对象运动、视角变化和光照变化的情况下，实现 temporal consistency 和 realistic lighting 仍然很困难。现有的2D扩散模型在 temporal coherence 方面存在不足，而传统的3D渲染方法在 photorealistic lighting 方面表现不佳。

Method: 提出了一种混合对象插入流程，结合了3D高斯泼溅（3DGS）的 temporal consistency 和2D扩散模型的 photorealistic editing。具体来说，使用3DGS进行初始渲染，然后通过基于2D扩散的增强模型进行光照交互的优化。该流程分离了内在对象属性（反照率、阴影、反射率），并优化了阴影和sRGB图像以实现照片真实感。为了保持 temporal coherence，使用了多帧加权调整来优化3DGS模型。

Result: 该方法通过结合3DGS和2D扩散模型，成功地在动态手部场景中插入手镯，实现了真实的光照交互和良好的时间一致性。通过分离内在对象属性并进行优化，能够生成照片级的真实感效果。

Conclusion: 该方法首次将3D渲染和2D扩散模型结合用于视频对象插入，为实现真实且一致的视频编辑提供了稳健的解决方案。

Abstract: Inserting 3D objects into videos is a longstanding challenge in computer
graphics with applications in augmented reality, virtual try-on, and video
composition. Achieving both temporal consistency, or realistic lighting remains
difficult, particularly in dynamic scenarios with complex object motion,
perspective changes, and varying illumination. While 2D diffusion models have
shown promise for producing photorealistic edits, they often struggle with
maintaining temporal coherence across frames. Conversely, traditional 3D
rendering methods excel in spatial and temporal consistency but fall short in
achieving photorealistic lighting. In this work, we propose a hybrid object
insertion pipeline that combines the strengths of both paradigms. Specifically,
we focus on inserting bracelets into dynamic wrist scenes, leveraging the high
temporal consistency of 3D Gaussian Splatting (3DGS) for initial rendering and
refining the results using a 2D diffusion-based enhancement model to ensure
realistic lighting interactions. Our method introduces a shading-driven
pipeline that separates intrinsic object properties (albedo, shading,
reflectance) and refines both shading and sRGB images for photorealism. To
maintain temporal coherence, we optimize the 3DGS model with multi-frame
weighted adjustments. This is the first approach to synergize 3D rendering and
2D diffusion for video object insertion, offering a robust solution for
realistic and consistent video editing. Project Page:
https://cjeen.github.io/BraceletPaper/

</details>


### [93] [PIVOTS: Aligning unseen Structures using Preoperative to Intraoperative Volume-To-Surface Registration for Liver Navigation](https://arxiv.org/abs/2507.20337)
*Peng Liu,Bianca Güttner,Yutong Su,Chenyang Li,Jinjing Xu,Mingyang Liu,Zhe Min,Andrey Zhylka,Jasper Smit,Karin Olthof,Matteo Fusaglia,Rudi Apolle,Matthias Miederer,Laura Frohneberger,Carina Riediger,Jügen Weitz,Fiona Kolbinger,Stefanie Speidel,Micha Pfeiffer*

Main category: cs.CV

TL;DR: PIVOTS是一种用于腹腔镜肝脏手术的非刚性配准新方法，它使用点云输入直接预测肝脏形变，并在各种挑战下表现出优越的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 非刚性配准对于增强现实引导下的腹腔镜肝脏手术至关重要，它能将术前信息（如肿瘤位置和血管结构）融合到有限的术中视野中，从而增强手术导航。其先决条件是能够准确预测术中肝脏形变，然而，由于气腹、呼吸和手术器械交互造成的大形变、术中数据噪声以及视野受限（由遮挡和受限的相机移动引起）等因素，这仍然是一个挑战。

Method: PIVOTS是一个预处理到术中体积到表面配准的神经网络，它直接以点云作为输入来预测形变。该模型的几何特征提取编码器能够进行多分辨率特征提取，而包含新颖的形变感知交叉注意力模块的解码器则实现了预处理和术中信息的交互以及多层次的位移预测。

Result: 结果表明，我们的方法在合成数据和真实数据集上的配准性能均优于基线方法，并且在面对高噪声、大形变和不同程度的术中可见性时，表现出强大的鲁棒性。

Conclusion: PIVOTS在合成和真实数据集上均表现出优于基线方法的配准性能，并且在面对高噪声、大形变和不同程度的术中可见性时，表现出强大的鲁棒性。我们公开了训练集和测试集作为评估基准，并呼吁对肝脏配准方法进行公平的基于体素表面数据的比较。

Abstract: Non-rigid registration is essential for Augmented Reality guided laparoscopic
liver surgery by fusing preoperative information, such as tumor location and
vascular structures, into the limited intraoperative view, thereby enhancing
surgical navigation. A prerequisite is the accurate prediction of
intraoperative liver deformation which remains highly challenging due to
factors such as large deformation caused by pneumoperitoneum, respiration and
tool interaction as well as noisy intraoperative data, and limited field of
view due to occlusion and constrained camera movement. To address these
challenges, we introduce PIVOTS, a Preoperative to Intraoperative
VOlume-To-Surface registration neural network that directly takes point clouds
as input for deformation prediction. The geometric feature extraction encoder
allows multi-resolution feature extraction, and the decoder, comprising novel
deformation aware cross attention modules, enables pre- and intraoperative
information interaction and accurate multi-level displacement prediction. We
train the neural network on synthetic data simulated from a biomechanical
simulation pipeline and validate its performance on both synthetic and real
datasets. Results demonstrate superior registration performance of our method
compared to baseline methods, exhibiting strong robustness against high amounts
of noise, large deformation, and various levels of intraoperative visibility.
We publish the training and test sets as evaluation benchmarks and call for a
fair comparison of liver registration methods with volume-to-surface data. Code
and datasets are available here https://github.com/pengliu-nct/PIVOTS.

</details>


### [94] [Detecting Visual Information Manipulation Attacks in Augmented Reality: A Multimodal Semantic Reasoning Approach](https://arxiv.org/abs/2507.20356)
*Yanming Xiu,Maria Gorlatova*

Main category: cs.CV

TL;DR: 研究提出VIM-Sense框架，利用多模态信息检测AR中的视觉信息操纵攻击，准确率达88.94%，延迟低。


<details>
  <summary>Details</summary>
Motivation: 虚拟内容可能在AR中引入误导性或有害信息，导致语义误解或用户错误。特别是视觉信息操纵（VIM）攻击，它以微妙但有影响的方式改变真实世界场景的含义。

Method: 提出了一种VIM攻击分类法，将攻击分为字符、短语和模式操纵，以及信息替换、信息混淆和额外错误信息三个目的。基于此分类法构建了AR-VIM数据集，包含452个原始AR视频对。VIM-Sense框架结合了VLMs和OCR技术来检测这些攻击。

Result: VIM-Sense在AR-VIM数据集上实现了88.94%的攻击检测准确率，并且在模拟视频处理框架和移动Android AR应用程序的真实世界评估中，平均攻击检测延迟分别为7.07秒和7.17秒，持续优于仅视觉或仅文本的基线模型。

Conclusion: 该研究提出了一种名为VIM-Sense的多模态语义推理框架，用于检测增强现实（AR）中的视觉信息操纵（VIM）攻击。该框架结合了视觉-语言模型（VLMs）的语言和视觉理解能力以及基于光学字符识别（OCR）的文本分析。

Abstract: The virtual content in augmented reality (AR) can introduce misleading or
harmful information, leading to semantic misunderstandings or user errors. In
this work, we focus on visual information manipulation (VIM) attacks in AR
where virtual content changes the meaning of real-world scenes in subtle but
impactful ways. We introduce a taxonomy that categorizes these attacks into
three formats: character, phrase, and pattern manipulation, and three purposes:
information replacement, information obfuscation, and extra wrong information.
Based on the taxonomy, we construct a dataset, AR-VIM. It consists of 452
raw-AR video pairs spanning 202 different scenes, each simulating a real-world
AR scenario. To detect such attacks, we propose a multimodal semantic reasoning
framework, VIM-Sense. It combines the language and visual understanding
capabilities of vision-language models (VLMs) with optical character
recognition (OCR)-based textual analysis. VIM-Sense achieves an attack
detection accuracy of 88.94% on AR-VIM, consistently outperforming vision-only
and text-only baselines. The system reaches an average attack detection latency
of 7.07 seconds in a simulated video processing framework and 7.17 seconds in a
real-world evaluation conducted on a mobile Android AR application.

</details>


### [95] [Generative Pre-training for Subjective Tasks: A Diffusion Transformer-Based Framework for Facial Beauty Prediction](https://arxiv.org/abs/2507.20363)
*Djamel Eddine Boukhari,Ali chemsa*

Main category: cs.CV

TL;DR: Diff-FBP：一种新颖的人脸美学评估方法，通过在 FFHQ 数据集上预训练扩散 Transformer 来学习人脸数据的分布，然后微调回归头，在 FBP5500 基准上达到了新的最先进水平（PCC=0.932）。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度卷积网络或标准 Vision Transformer 的方法，在人脸美学评估方面，难以学习到真正对其齐的特征表示，因为人脸美学评估任务主观性强，且涉及细微的、整体性的特征。

Method: 提出了一种新颖的两阶段框架，利用生成模型来创建更优越的、特定领域的特征提取器。第一阶段，在大型、无标签的人脸数据集（FFHQ）上，通过自监督去噪任务预训练了一个扩散 Transformer。第二阶段，将预训练并冻结的扩散 Transformer 编码器作为骨干特征提取器，仅对轻量级回归头在目标 FBP 数据集（FBP5500）上进行微调。

Result: 在FBP5500基准上设定了新的最先进水平，实现了0.932的皮尔逊相关系数（PCC），显著优于先前技术。

Conclusion: 该方法在FBP5500基准上设定了新的最先进水平，实现了0.932的皮尔逊相关系数（PCC），显著优于基于通用预训练的先前技术。消融研究证实，我们提出的生成式预训练策略是此性能飞跃的关键贡献者，它为这些主观视觉任务创建了更具语义潜力的特征表示。

Abstract: Facial Beauty Prediction (FBP) is a challenging computer vision task due to
its subjective nature and the subtle, holistic features that influence human
perception. Prevailing methods, often based on deep convolutional networks or
standard Vision Transformers pre-trained on generic object classification
(e.g., ImageNet), struggle to learn feature representations that are truly
aligned with high-level aesthetic assessment. In this paper, we propose a novel
two-stage framework that leverages the power of generative models to create a
superior, domain-specific feature extractor. In the first stage, we pre-train a
Diffusion Transformer on a large-scale, unlabeled facial dataset (FFHQ) through
a self-supervised denoising task. This process forces the model to learn the
fundamental data distribution of human faces, capturing nuanced details and
structural priors essential for aesthetic evaluation. In the second stage, the
pre-trained and frozen encoder of our Diffusion Transformer is used as a
backbone feature extractor, with only a lightweight regression head being
fine-tuned on the target FBP dataset (FBP5500). Our method, termed Diff-FBP,
sets a new state-of-the-art on the FBP5500 benchmark, achieving a Pearson
Correlation Coefficient (PCC) of 0.932, significantly outperforming prior art
based on general-purpose pre-training. Extensive ablation studies validate that
our generative pre-training strategy is the key contributor to this performance
leap, creating feature representations that are more semantically potent for
subjective visual tasks.

</details>


### [96] [MagicAnime: A Hierarchically Annotated, Multimodal and Multitasking Dataset with Benchmarks for Cartoon Animation Generation](https://arxiv.org/abs/2507.20368)
*Shuolin Xu,Bingyuan Wang,Zeyu Cai,Fangteng Fu,Yue Ma,Tongyi Lee,Hongchuan Yu,Zeyu Wang*

Main category: cs.CV

TL;DR: 提出 MagicAnime 数据集和 MagicAnime-Bench 基准测试集，以解决卡通动画生成中的多模态控制难题和数据稀缺问题，并在多个任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决高质量卡通动画多模态控制的挑战，弥合真实视频与卡通动画之间的领域差距，以及解决公开多模态卡通数据稀缺的问题。

Method: 提出 MagicAnime 数据集，包含 400k 视频片段用于图生视频，50k 视频片段和关键点用于全身标注，12k 视频片段对用于视频驱动的人脸动画，以及 2.9k 视频和音频片段对用于音频驱动的人脸动画。同时，构建了 MagicAnime-Bench 基准测试集。

Result: MagicAnime 数据集和 MagicAnime-Bench 基准测试集，并在视频驱动人脸动画、音频驱动人脸动画、图生视频动画和姿态驱动角色动画四个任务上进行了实验验证。

Conclusion: MagicAnime 数据集能够有效地支持高保真、细粒度和可控的卡通动画生成，并在四个任务上进行了验证。

Abstract: Generating high-quality cartoon animations multimodal control is challenging
due to the complexity of non-human characters, stylistically diverse motions
and fine-grained emotions. There is a huge domain gap between real-world videos
and cartoon animation, as cartoon animation is usually abstract and has
exaggerated motion. Meanwhile, public multimodal cartoon data are extremely
scarce due to the difficulty of large-scale automatic annotation processes
compared with real-life scenarios. To bridge this gap, We propose the
MagicAnime dataset, a large-scale, hierarchically annotated, and multimodal
dataset designed to support multiple video generation tasks, along with the
benchmarks it includes. Containing 400k video clips for image-to-video
generation, 50k pairs of video clips and keypoints for whole-body annotation,
12k pairs of video clips for video-to-video face animation, and 2.9k pairs of
video and audio clips for audio-driven face animation. Meanwhile, we also build
a set of multi-modal cartoon animation benchmarks, called MagicAnime-Bench, to
support the comparisons of different methods in the tasks above. Comprehensive
experiments on four tasks, including video-driven face animation, audio-driven
face animation, image-to-video animation, and pose-driven character animation,
validate its effectiveness in supporting high-fidelity, fine-grained, and
controllable generation.

</details>


### [97] [ModalFormer: Multimodal Transformer for Low-Light Image Enhancement](https://arxiv.org/abs/2507.20388)
*Alexandru Brateanu,Raul Balmez,Ciprian Orhei,Codruta Ancuti,Cosmin Ancuti*

Main category: cs.CV

TL;DR: ModalFormer是一个利用九种辅助模态的LLIE框架，通过跨模态Transformer（CM-T）和跨模态多头自注意力机制（CM-MSA）融合多模态信息，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 低光图像增强（LLIE）在处理噪声、细节丢失和对比度差等问题时是一个基础性但困难的任务。现有方法主要依赖于RGB图像的像素级转换，忽略了多模态信息提供的丰富上下文信息。

Method: ModalFormer是一个大型多模态框架，利用九种辅助模态进行低光图像增强。它包含一个跨模态Transformer（CM-T），用于在整合多模态信息的同时恢复损坏的图像，以及多个用于多模态特征重建的辅助子网络。CM-T的核心是跨模态多头自注意力机制（CM-MSA），它融合了RGB数据和特定模态的特征（包括深度特征嵌入、分割信息、几何线索和颜色信息），以生成信息丰富的混合注意力图。

Result: 在多个基准数据集上的广泛实验证明了ModalFormer在LLIE方面的最先进性能。

Conclusion: ModalFormer在低光图像增强方面取得了最先进的性能。

Abstract: Low-light image enhancement (LLIE) is a fundamental yet challenging task due
to the presence of noise, loss of detail, and poor contrast in images captured
under insufficient lighting conditions. Recent methods often rely solely on
pixel-level transformations of RGB images, neglecting the rich contextual
information available from multiple visual modalities. In this paper, we
present ModalFormer, the first large-scale multimodal framework for LLIE that
fully exploits nine auxiliary modalities to achieve state-of-the-art
performance. Our model comprises two main components: a Cross-modal Transformer
(CM-T) designed to restore corrupted images while seamlessly integrating
multimodal information, and multiple auxiliary subnetworks dedicated to
multimodal feature reconstruction. Central to the CM-T is our novel Cross-modal
Multi-headed Self-Attention mechanism (CM-MSA), which effectively fuses RGB
data with modality-specific features--including deep feature embeddings,
segmentation information, geometric cues, and color information--to generate
information-rich hybrid attention maps. Extensive experiments on multiple
benchmark datasets demonstrate ModalFormer's state-of-the-art performance in
LLIE. Pre-trained models and results are made available at
https://github.com/albrateanu/ModalFormer.

</details>


### [98] [The Importance of Facial Features in Vision-based Sign Language Recognition: Eyes, Mouth or Full Face?](https://arxiv.org/abs/2507.20884)
*Dinh Nam Pham,Eleftherios Avramidis*

Main category: cs.CV

TL;DR: 本研究深入探讨了面部特征在自动手语识别（ASLR）中的作用，发现嘴部是提升识别准确率的关键因素。


<details>
  <summary>Details</summary>
Motivation: 尽管非手动面部特征在手语交流中起着至关重要的作用，但它们在自动手语识别（ASLR）中的重要性尚未得到充分探索。现有工作常常依赖于手工特征提取，并且仅仅比较了手动特征与手动加面部特征的组合，未能深入研究不同面部区域的贡献。

Method: 本研究系统地研究了眼睛、嘴巴和全脸等不同面部区域的贡献，并使用了两种不同的深度学习模型（基于CNN的模型和基于Transformer的模型）在一个包含随机类别孤立手语的数据集上进行训练。通过定量性能和定性显著性图评估来揭示不同面部区域的重要性。

Result: 研究结果表明，嘴部是最重要的非手动面部特征，能够显著提高ASLR的准确率。通过定量性能和定性显著性图评估，本研究揭示了面部特征（尤其是嘴部）在ASLR中的关键作用。

Conclusion: 本研究强调了在自动手语识别（ASLR）中整合面部特征的必要性，特别是嘴部特征能显著提高识别准确率。

Abstract: Non-manual facial features play a crucial role in sign language
communication, yet their importance in automatic sign language recognition
(ASLR) remains underexplored. While prior studies have shown that incorporating
facial features can improve recognition, related work often relies on
hand-crafted feature extraction and fails to go beyond the comparison of manual
features versus the combination of manual and facial features. In this work, we
systematically investigate the contribution of distinct facial regionseyes,
mouth, and full faceusing two different deep learning models (a CNN-based model
and a transformer-based model) trained on an SLR dataset of isolated signs with
randomly selected classes. Through quantitative performance and qualitative
saliency map evaluation, we reveal that the mouth is the most important
non-manual facial feature, significantly improving accuracy. Our findings
highlight the necessity of incorporating facial features in ASLR.

</details>


### [99] [Solving Scene Understanding for Autonomous Navigation in Unstructured Environments](https://arxiv.org/abs/2507.20389)
*Naveen Mathews Renji,Kruthika K,Manasa Keshavamurthy,Pooja Kumari,S. Rajarajeswari*

Main category: cs.CV

TL;DR: 本研究评估了五种深度学习模型在印度驾驶数据集上的语义分割性能，UNET+RESNET50 表现最佳，mIOU 为 0.6496，为自动驾驶汽车在非结构化环境中的应用提供了见解。


<details>
  <summary>Details</summary>
Motivation: 为了提高自动驾驶汽车在包括印度在内的不同驾驶环境中的性能，需要对道路场景进行准确理解。本研究旨在评估深度学习模型在印度驾驶数据集上的语义分割性能，该数据集因其非结构化环境而具有挑战性。

Method: 通过在第一层上训练五个不同的模型（UNET、UNET+RESNET50、DeepLabsV3、PSPNet 和 SegNet）并使用平均交并比 (mIOU) 指标比较它们的性能，对印度驾驶数据集进行了语义分割。

Result: 在印度驾驶数据集上，UNET+RESNET50 模型在语义分割方面达到了最高的 mIOU，为 0.6496。与其他四种模型相比，该模型在理解非结构化驾驶场景方面表现出更强的能力。

Conclusion: 虽然存在一些挑战，但研究表明，在印度驾驶数据集（包括城市和乡村道路）上对 UNET、UNET+RESNET50、DeepLabsV3、PSPNet 和 SegNet 五种模型进行了训练和比较，其中 UNET+RESNET50 在语义分割方面表现最好，平均交并比 (mIOU) 达到 0.6496。

Abstract: Autonomous vehicles are the next revolution in the automobile industry and
they are expected to revolutionize the future of transportation. Understanding
the scenario in which the autonomous vehicle will operate is critical for its
competent functioning. Deep Learning has played a massive role in the progress
that has been made till date. Semantic Segmentation, the process of annotating
every pixel of an image with an object class, is one crucial part of this scene
comprehension using Deep Learning. It is especially useful in Autonomous
Driving Research as it requires comprehension of drivable and non-drivable
areas, roadside objects and the like. In this paper semantic segmentation has
been performed on the Indian Driving Dataset which has been recently compiled
on the urban and rural roads of Bengaluru and Hyderabad. This dataset is more
challenging compared to other datasets like Cityscapes, since it is based on
unstructured driving environments. It has a four level hierarchy and in this
paper segmentation has been performed on the first level. Five different models
have been trained and their performance has been compared using the Mean
Intersection over Union. These are UNET, UNET+RESNET50, DeepLabsV3, PSPNet and
SegNet. The highest MIOU of 0.6496 has been achieved. The paper discusses the
dataset, exploratory data analysis, preparation, implementation of the five
models and studies the performance and compares the results achieved in the
process.

</details>


### [100] [$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with Attention-Guided Refinement](https://arxiv.org/abs/2507.20890)
*Zhecheng Li,Guoxian Song,Yiwei Wang,Zhen Xiong,Junsong Yuan,Yujun Cai*

Main category: cs.CV

TL;DR: A^2R^2通过视觉推理、注意力定位和迭代细化改进了Img2LaTeX任务，在挑战性数据集上表现优于现有方法，并且可以通过增加推理轮数来进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）在Img2LaTeX任务上表现不佳，因为它们难以处理细粒度的视觉元素，导致LaTeX预测不准确。因此，需要一种能够解决这些挑战的方法。

Method: 提出了一种名为A^2R^2的框架，该框架集成了注意力定位和迭代细化，并将其置于视觉推理框架内，以实现Img2LaTeX任务的自我修正和预测质量的渐进式提升。

Result: 实验结果表明，A^2R^2在六个文本和视觉层面的评估指标上均显著优于其他基线方法。增加推理轮数可以带来性能提升，并且消融研究和人类评估验证了该方法的有效性及其核心组件的协同作用。

Conclusion: A^2R^2框架在Img2LaTeX任务上显著提升了模型性能，其通过注意力定位和迭代细化实现了视觉推理和自我修正，并在一系列评估指标上优于基线方法。增加推理轮数能够带来性能提升，且各组成部分的协同作用也得到了验证。

Abstract: Img2LaTeX is a practically significant task that involves converting
mathematical expressions or tabular data from images into LaTeX code. In recent
years, vision-language models (VLMs) have demonstrated strong performance
across a variety of visual understanding tasks, owing to their generalization
capabilities. While some studies have explored the use of VLMs for the
Img2LaTeX task, their performance often falls short of expectations.
Empirically, VLMs sometimes struggle with fine-grained visual elements, leading
to inaccurate LaTeX predictions. To address this challenge, we propose
$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with
Attention-Guided Refinement, a framework that effectively integrates attention
localization and iterative refinement within a visual reasoning framework,
enabling VLMs to perform self-correction and progressively improve prediction
quality. For effective evaluation, we introduce a new dataset,
Img2LaTex-Hard-1K, consisting of 1,100 carefully curated and challenging
examples designed to rigorously evaluate the capabilities of VLMs within this
task domain. Extensive experimental results demonstrate that: (1) $A^2R^2$
significantly improves model performance across six evaluation metrics spanning
both textual and visual levels, consistently outperforming other baseline
methods; (2) Increasing the number of inference rounds yields notable
performance gains, underscoring the potential of $A^2R^2$ in test-time scaling
scenarios; (3) Ablation studies and human evaluations validate the practical
effectiveness of our approach, as well as the strong synergy among its core
components during inference.

</details>


### [101] [VESPA: Towards un(Human)supervised Open-World Pointcloud Labeling for Autonomous Driving](https://arxiv.org/abs/2507.20397)
*Levente Tempfli,Esteban Rivera,Markus Lienkamp*

Main category: cs.CV

TL;DR: VESPA是一种多模态自动标注方法，通过融合激光雷达和摄像头数据，利用视觉语言模型实现开放词汇的目标标注和3D伪标签生成，无需人工标注或高清地图，并在NuScenes数据集上取得了优异的检测和发现性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决手动标注（尤其是3D标注）成本高、劳动强度大的问题，并克服现有基于激光雷达的自动标注方法在数据稀疏性、遮挡、不完整观测以及类别限制方面的不足，提出了VESPA。

Method: VESPA利用视觉语言模型（VLMs）融合激光雷达和摄像头数据，以实现开放词汇的目标标注，并直接在点云域优化检测质量。

Result: 在NuScenes数据集上，VESPA在目标发现方面的平均精度（AP）达到了52.95%，在多类别目标检测方面的AP最高达到了46.54%，展示了其在可扩展3D场景理解方面的强大性能。

Conclusion: VESPA是一个多模态自动标注流程，融合了激光雷达的几何精度和摄像头图像的语义丰富性，能够实现开放词汇的目标标注，并在点云域直接优化检测质量。该方法无需地面真实标注或高清地图，即可支持新类别的发现并生成高质量的3D伪标签。

Abstract: Data collection for autonomous driving is rapidly accelerating, but manual
annotation, especially for 3D labels, remains a major bottleneck due to its
high cost and labor intensity. Autolabeling has emerged as a scalable
alternative, allowing the generation of labels for point clouds with minimal
human intervention. While LiDAR-based autolabeling methods leverage geometric
information, they struggle with inherent limitations of lidar data, such as
sparsity, occlusions, and incomplete object observations. Furthermore, these
methods typically operate in a class-agnostic manner, offering limited semantic
granularity. To address these challenges, we introduce VESPA, a multimodal
autolabeling pipeline that fuses the geometric precision of LiDAR with the
semantic richness of camera images. Our approach leverages vision-language
models (VLMs) to enable open-vocabulary object labeling and to refine detection
quality directly in the point cloud domain. VESPA supports the discovery of
novel categories and produces high-quality 3D pseudolabels without requiring
ground-truth annotations or HD maps. On Nuscenes dataset, VESPA achieves an AP
of 52.95% for object discovery and up to 46.54% for multiclass object
detection, demonstrating strong performance in scalable 3D scene understanding.
Code will be available upon acceptance.

</details>


### [102] [Second Competition on Presentation Attack Detection on ID Card](https://arxiv.org/abs/2507.20404)
*Juan E. Tapia,Mario Nieto,Juan M. Espin,Alvaro S. Rocamora,Javier Barrachina,Naser Damer,Christoph Busch,Marija Ivanovska,Leon Todorov,Renat Khizbullin,Lazar Lazarevich,Aleksei Grishin,Daniel Schulz,Sebastian Gonzalez,Amir Mohammadi,Ketan Kotwal,Sebastien Marcel,Raghavendra Mudgalgundurao,Kiran Raja,Patrick Schuch,Sushrut Patwardhan,Raghavendra Ramachandra,Pedro Couto Pereira,Joao Ribeiro Pinto,Mariana Xavier,Andrés Valenzuela,Rodrigo Lara,Borut Batagelj,Marko Peterlin,Peter Peer,Ajnas Muhammed,Diogo Nunes,Nuno Gonçalves*

Main category: cs.CV

TL;DR: The second PAD competition on ID cards showed improvement, with the 'Incode' team excelling in Track 2. However, PAD on ID cards remains challenging, especially concerning the quantity of bona fide images.


<details>
  <summary>Details</summary>
Motivation: To summarize and report the results of the second Presentation Attack Detection competition on ID cards, highlighting improvements and challenges in the field.

Method: This work summarizes and reports the results of the second Presentation Attack Detection (PAD) competition on ID cards. It included an automatic evaluation platform, two tracks (algorithms and datasets), and a new baseline dataset for training and optimization. 20 teams participated, evaluating 74 submitted models.

Result: Track 1: 'Dragons' team achieved first place with AV-Rank 40.48% and EER 11.44%. Track 2: 'Incode' team achieved best results with AV-Rank 14.76% and EER 6.36%, improving significantly from the first edition.

Conclusion: PAD on ID cards is improving but remains challenging due to image quantity, especially of bona fide images.

Abstract: This work summarises and reports the results of the second Presentation
Attack Detection competition on ID cards. This new version includes new
elements compared to the previous one. (1) An automatic evaluation platform was
enabled for automatic benchmarking; (2) Two tracks were proposed in order to
evaluate algorithms and datasets, respectively; and (3) A new ID card dataset
was shared with Track 1 teams to serve as the baseline dataset for the training
and optimisation. The Hochschule Darmstadt, Fraunhofer-IGD, and Facephi company
jointly organised this challenge. 20 teams were registered, and 74 submitted
models were evaluated. For Track 1, the "Dragons" team reached first place with
an Average Ranking and Equal Error rate (EER) of AV-Rank of 40.48% and 11.44%
EER, respectively. For the more challenging approach in Track 2, the "Incode"
team reached the best results with an AV-Rank of 14.76% and 6.36% EER,
improving on the results of the first edition of 74.30% and 21.87% EER,
respectively. These results suggest that PAD on ID cards is improving, but it
is still a challenging problem related to the number of images, especially of
bona fide images.

</details>


### [103] [Indian Sign Language Detection for Real-Time Translation using Machine Learning](https://arxiv.org/abs/2507.20414)
*Rajat Singhal,Jatin Gupta,Akhil Sharma,Anushka Gupta,Navya Sharma*

Main category: cs.CV

TL;DR: 本研究开发了一个高精度的实时ISL翻译系统，以改善印度聋哑社区的沟通。


<details>
  <summary>Details</summary>
Motivation: 由于熟练的翻译人员和可及的翻译技术的稀缺，印度聋哑社区在沟通方面面临重大障碍，该研究旨在解决这些挑战，特别是在技术解决方案不如其他全球手语发达的印度背景下。

Method: 本研究提出了一种基于卷积神经网络（CNN）的ISL检测和翻译系统，并集成了MediaPipe框架以实现精确的手部追踪和动作检测。模型在ISL数据集上进行训练和评估，使用了准确率、F1分数、精确率和召回率等关键性能指标。

Result: 该模型在ISL数据集上实现了99.95%的分类准确率，表明其能够精确识别不同的手语，并有效弥合印度听障人士的沟通差距。

Conclusion: 该研究提出了一个基于卷积神经网络（CNN）的健壮的、实时的印度手语（ISL）检测和翻译系统，该系统在包含精确手部追踪和动作识别的MediaPipe框架上运行，实现了99.95%的分类准确率，旨在弥合印度听障人士的沟通差距。

Abstract: Gestural language is used by deaf & mute communities to communicate through
hand gestures & body movements that rely on visual-spatial patterns known as
sign languages. Sign languages, which rely on visual-spatial patterns of hand
gestures & body movements, are the primary mode of communication for deaf &
mute communities worldwide. Effective communication is fundamental to human
interaction, yet individuals in these communities often face significant
barriers due to a scarcity of skilled interpreters & accessible translation
technologies. This research specifically addresses these challenges within the
Indian context by focusing on Indian Sign Language (ISL). By leveraging machine
learning, this study aims to bridge the critical communication gap for the deaf
& hard-of-hearing population in India, where technological solutions for ISL
are less developed compared to other global sign languages. We propose a
robust, real-time ISL detection & translation system built upon a Convolutional
Neural Network (CNN). Our model is trained on a comprehensive ISL dataset &
demonstrates exceptional performance, achieving a classification accuracy of
99.95%. This high precision underscores the model's capability to discern the
nuanced visual features of different signs. The system's effectiveness is
rigorously evaluated using key performance metrics, including accuracy, F1
score, precision & recall, ensuring its reliability for real-world
applications. For real-time implementation, the framework integrates MediaPipe
for precise hand tracking & motion detection, enabling seamless translation of
dynamic gestures. This paper provides a detailed account of the model's
architecture, the data preprocessing pipeline & the classification methodology.
The research elaborates the model architecture, preprocessing & classification
methodologies for enhancing communication in deaf & mute communities.

</details>


### [104] [Can Foundation Models Predict Fitness for Duty?](https://arxiv.org/abs/2507.20418)
*Juan E. Tapia,Christoph Busch*

Main category: cs.CV

TL;DR: 该研究利用深度学习和基础模型来预测工作适用性，解决了生物识别数据集中图像不足的问题。


<details>
  <summary>Details</summary>
Motivation: 生物识别设备（如虹膜图像）已被用于估计人的警觉性，但缺乏酒精、药物滥用和睡眠剥夺相关的图像数据来训练AI模型是一个重大挑战。

Method: 利用了深度学习和基础模型（特别是自监督模型）的泛化能力来解决图像数据不足的问题。

Result: 该研究探讨了利用深度学习和基础模型来预测工作适用性（即与确定工作警觉性相关的受试者状况）的可能性。

Conclusion: 该研究检验了深度学习和基础模型在预测工作适用性方面的应用。

Abstract: Biometric capture devices have been utilised to estimate a person's alertness
through near-infrared iris images, expanding their use beyond just biometric
recognition. However, capturing a substantial number of corresponding images
related to alcohol consumption, drug use, and sleep deprivation to create a
dataset for training an AI model presents a significant challenge. Typically, a
large quantity of images is required to effectively implement a deep learning
approach. Currently, training downstream models with a huge number of images
based on foundational models provides a real opportunity to enhance this area,
thanks to the generalisation capabilities of self-supervised models. This work
examines the application of deep learning and foundational models in predicting
fitness for duty, which is defined as the subject condition related to
determining the alertness for work.

</details>


### [105] [JOLT3D: Joint Learning of Talking Heads and 3DMM Parameters with Application to Lip-Sync](https://arxiv.org/abs/2507.20452)
*Sungjoon Park,Minsik Park,Haneol Lee,Jaesub Yun,Donggeon Lee*

Main category: cs.CV

TL;DR: 本研究通过联合学习3D面部重建和说话人脸合成模型，优化了3DMM在说话人脸合成中的应用，并提出了一种新的唇形同步流程，提高了生成人脸的质量和唇形同步的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了提高说话人脸合成的质量，并实现基于音频的唇形同步。

Method: 通过联合学习3D面部重建模型和说话人脸合成模型来重新审视3DMM在说话人脸合成中的有效性，从而获得针对说话人脸合成进行了优化的、基于FACS的面部表情的blendshape表示。提出了一种新颖的唇形同步流程，它将原始下巴轮廓与唇形同步的下巴轮廓分离开来，并减少了嘴部附近的闪烁。

Result: 与仅拟合2D地标的3DMM参数或依赖预训练面部重建模型的前期方法相比，该方法提高了生成人脸的质量，并实现了更优的唇形同步效果。

Conclusion: 该方法提高了生成人脸的质量，并允许利用blendshape表示仅修改嘴部区域以实现基于音频的唇形同步。

Abstract: In this work, we revisit the effectiveness of 3DMM for talking head synthesis
by jointly learning a 3D face reconstruction model and a talking head synthesis
model. This enables us to obtain a FACS-based blendshape representation of
facial expressions that is optimized for talking head synthesis. This contrasts
with previous methods that either fit 3DMM parameters to 2D landmarks or rely
on pretrained face reconstruction models. Not only does our approach increase
the quality of the generated face, but it also allows us to take advantage of
the blendshape representation to modify just the mouth region for the purpose
of audio-based lip-sync. To this end, we propose a novel lip-sync pipeline
that, unlike previous methods, decouples the original chin contour from the
lip-synced chin contour, and reduces flickering near the mouth.

</details>


### [106] [Frequency-Aware Autoregressive Modeling for Efficient High-Resolution Image Synthesis](https://arxiv.org/abs/2507.20454)
*Zhuokun Chen,Jugang Fan,Zhuowei Yu,Bohan Zhuang,Mingkui Tan*

Main category: cs.CV

TL;DR: SparseVAR通过排除对图像质量影响可忽略的低频标记，显著加速了视觉自回归模型在高分辨率阶段的推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉自回归模型在生成高质量和可扩展图像方面表现出色，但在高分辨率阶段面临计算开销大的挑战，这是由于涉及大量标记。

Method: SparseVAR是一个即插即用的加速框架，它利用轻量级的基于MSE的度量来识别低频标记，并通过一小组均匀采样的锚定标记来保留被排除区域的保真度。

Result: SparseVAR在HART和Infinity上实现了显著的加速，其中在Infinity-2B上实现了高达2倍的加速，同时图像质量的下降最小。

Conclusion: SparseVAR通过动态排除低频标记并保留均匀采样的锚定标记，在不进行额外训练的情况下显著降低了高分辨率阶段的计算成本，同时保持了高质量的图像生成。

Abstract: Visual autoregressive modeling, based on the next-scale prediction paradigm,
exhibits notable advantages in image quality and model scalability over
traditional autoregressive and diffusion models. It generates images by
progressively refining resolution across multiple stages. However, the
computational overhead in high-resolution stages remains a critical challenge
due to the substantial number of tokens involved. In this paper, we introduce
SparseVAR, a plug-and-play acceleration framework for next-scale prediction
that dynamically excludes low-frequency tokens during inference without
requiring additional training. Our approach is motivated by the observation
that tokens in low-frequency regions have a negligible impact on image quality
in high-resolution stages and exhibit strong similarity with neighboring
tokens. Additionally, we observe that different blocks in the next-scale
prediction model focus on distinct regions, with some concentrating on
high-frequency areas. SparseVAR leverages these insights by employing
lightweight MSE-based metrics to identify low-frequency tokens while preserving
the fidelity of excluded regions through a small set of uniformly sampled
anchor tokens. By significantly reducing the computational cost while
maintaining high image generation quality, SparseVAR achieves notable
acceleration in both HART and Infinity. Specifically, SparseVAR achieves up to
a 2 times speedup with minimal quality degradation in Infinity-2B.

</details>


### [107] [Priority-Aware Pathological Hierarchy Training for Multiple Instance Learning](https://arxiv.org/abs/2507.20469)
*Sungrae Hong,Kyungeun Kim,Juhyeon Kim,Sol Lee,Jisu Shin,Chanjae Song,Mun Yong Yi*

Main category: cs.CV

TL;DR: 该研究提出了一种新的多实例学习（MIL）方法，通过引入层级关系来解决临床病理诊断中类别优先级别的问题，有效降低了误诊率，并优先考虑了重要的临床症状。


<details>
  <summary>Details</summary>
Motivation: 现有的MIL方法未能充分解决病理症状和诊断类别之间的优先级别问题，导致MIL模型忽略了类别间的优先级别。

Method: 提出了一种使用垂直层级间和水平层级内两种层级关系来解决优先级别问题的新方法，该方法在每个层级上对MIL预测进行对齐，并在训练过程中采用隐式特征重用，以促进同一级别内具有临床意义的类别。

Result: 实验结果表明，该方法能有效降低误诊率，并优先考虑多类别场景中更重要的症状。进一步的分析验证了所提出组件的有效性，并对MIL预测在具有多种症状的挑战性病例中的表现进行了定性确认。

Conclusion: 提出的方法能有效降低误诊率，并优先考虑多类别场景中更重要的症状。

Abstract: Multiple Instance Learning (MIL) is increasingly being used as a support tool
within clinical settings for pathological diagnosis decisions, achieving high
performance and removing the annotation burden. However, existing approaches
for clinical MIL tasks have not adequately addressed the priority issues that
exist in relation to pathological symptoms and diagnostic classes, causing MIL
models to ignore priority among classes. To overcome this clinical limitation
of MIL, we propose a new method that addresses priority issues using two
hierarchies: vertical inter-hierarchy and horizontal intra-hierarchy. The
proposed method aligns MIL predictions across each hierarchical level and
employs an implicit feature re-usability during training to facilitate
clinically more serious classes within the same level. Experiments with
real-world patient data show that the proposed method effectively reduces
misdiagnosis and prioritizes more important symptoms in multiclass scenarios.
Further analysis verifies the efficacy of the proposed components and
qualitatively confirms the MIL predictions against challenging cases with
multiple symptoms.

</details>


### [108] [Automated 3D-GS Registration and Fusion via Skeleton Alignment and Gaussian-Adaptive Features](https://arxiv.org/abs/2507.20480)
*Shiyang Liu,Dianyi Yang,Yu Gao,Bohan Ren,Yi Yang,Mengyin Fu*

Main category: cs.CV

TL;DR: 本研究提出了一种自动化的3D高斯泼溅子地图对齐和融合方法，无需手动干预，即可提高注册精度和融合质量，从而实现更准确的三维场景表示。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法主要关注单图重建，而多张3D高斯泼溅子图的配准和融合仍未得到充分研究，通常需要手动干预且融合质量有待提高。

Method: 提出了一种新颖的方法，通过提取几何骨架并利用椭圆感知卷积来捕捉3D高斯泼溅属性，实现鲁棒的场景注册。此外，引入了多因素高斯融合策略来减轻硬阈值处理造成的场景元素损失。

Result: 在ScanNet-GSReg和Coord数据集上进行了实验，证明了该方法在注册和融合方面的有效性。注册方面，复杂场景下的平均角度偏差（RRE）降低了41.9%，提高了姿态估计精度。融合方面，峰值信噪比（PSNR）提高了10.11 dB，显示出更优越的结构保持能力。

Conclusion: 该方法通过自动化3D高斯泼溅子地图的对齐和融合，提高了注册精度和融合质量，为机器人感知和自主导航提供了更一致、更准确的三维场景表示。

Abstract: In recent years, 3D Gaussian Splatting (3D-GS)-based scene representation
demonstrates significant potential in real-time rendering and training
efficiency. However, most existing methods primarily focus on single-map
reconstruction, while the registration and fusion of multiple 3D-GS sub-maps
remain underexplored. Existing methods typically rely on manual intervention to
select a reference sub-map as a template and use point cloud matching for
registration. Moreover, hard-threshold filtering of 3D-GS primitives often
degrades rendering quality after fusion. In this paper, we present a novel
approach for automated 3D-GS sub-map alignment and fusion, eliminating the need
for manual intervention while enhancing registration accuracy and fusion
quality. First, we extract geometric skeletons across multiple scenes and
leverage ellipsoid-aware convolution to capture 3D-GS attributes, facilitating
robust scene registration. Second, we introduce a multi-factor Gaussian fusion
strategy to mitigate the scene element loss caused by rigid thresholding.
Experiments on the ScanNet-GSReg and our Coord datasets demonstrate the
effectiveness of the proposed method in registration and fusion. For
registration, it achieves a 41.9\% reduction in RRE on complex scenes, ensuring
more precise pose estimation. For fusion, it improves PSNR by 10.11 dB,
highlighting superior structural preservation. These results confirm its
ability to enhance scene alignment and reconstruction fidelity, ensuring more
consistent and accurate 3D scene representation for robotic perception and
autonomous navigation.

</details>


### [109] [An Improved YOLOv8 Approach for Small Target Detection of Rice Spikelet Flowering in Field Environments](https://arxiv.org/abs/2507.20506)
*Beizhang Chen,Jinming Liang,Zheng Xiong,Ming Pan,Xiangbao Meng,Qingshan Lin,Qun Ma,Yingping Zhao*

Main category: cs.CV

TL;DR: 本研究提出了一种改进YOLOv8模型（YOLOv8s-p2），通过融合BiFPN和增加p2小目标检测头，提高了在复杂田间条件下水稻开花时间检测的准确性和效率，实验结果显示性能显著优于原始YOLOv8模型，并达到了实际应用的速度要求。


<details>
  <summary>Details</summary>
Motivation: 准确检测水稻开花时间对于杂交稻种生产中的及时授粉至关重要，这不仅能提高授粉效率，还能确保更高的产量。然而，由于田间环境的复杂性以及水稻穗等目标物本身具有的小尺寸、短开花期等特性，自动化精准识别仍然面临挑战。

Method: 本研究提出了一种基于改进YOLOv8的稻穗开花识别方法。通过使用双向特征金字塔网络（BiFPN）替换原始的PANet结构来增强特征融合和多尺度特征利用；增加p2小目标检测头，利用更精细的特征图来减少小目标检测中的特征丢失。同时，为了解决公开数据集缺乏的问题，研究者使用高分辨率RGB相机并结合数据增强技术构建了专门的数据集。

Result: 实验结果表明，改进后的YOLOv8s-p2模型在测试集上达到了65.9%的mAP@0.5、67.6%的精确率、61.5%的召回率和64.41%的F1分数，相比基线YOLOv8分别提升了3.10%、8.40%、10.80%和9.79%。同时，该模型运行速度达到69 f/s，满足实际应用要求。

Conclusion: 该研究提出的改进YOLOv8s-p2模型在水稻抽穗开花识别任务上表现出色，准确率和速度均满足实际应用需求，为杂交稻种生产中的自动化监测提供了有效解决方案。

Abstract: Accurately detecting rice flowering time is crucial for timely pollination in
hybrid rice seed production. This not only enhances pollination efficiency but
also ensures higher yields. However, due to the complexity of field
environments and the characteristics of rice spikelets, such as their small
size and short flowering period, automated and precise recognition remains
challenging. To address this, this study proposes a rice spikelet flowering
recognition method based on an improved YOLOv8 object detection model. First, a
Bidirectional Feature Pyramid Network (BiFPN) replaces the original PANet
structure to enhance feature fusion and improve multi-scale feature
utilization. Second, to boost small object detection, a p2 small-object
detection head is added, using finer feature mapping to reduce feature loss
commonly seen in detecting small targets. Given the lack of publicly available
datasets for rice spikelet flowering in field conditions, a high-resolution RGB
camera and data augmentation techniques are used to construct a dedicated
dataset, providing reliable support for model training and testing.
Experimental results show that the improved YOLOv8s-p2 model achieves an
mAP@0.5 of 65.9%, precision of 67.6%, recall of 61.5%, and F1-score of 64.41%,
representing improvements of 3.10%, 8.40%, 10.80%, and 9.79%, respectively,
over the baseline YOLOv8. The model also runs at 69 f/s on the test set,
meeting practical application requirements. Overall, the improved YOLOv8s-p2
offers high accuracy and speed, providing an effective solution for automated
monitoring in hybrid rice seed production.

</details>


### [110] [Investigating the Effect of Spatial Context on Multi-Task Sea Ice Segmentation](https://arxiv.org/abs/2507.20507)
*Behzad Vahedi,Rafael Pires de Lima,Sepideh Jalayer,Walter N. Meier,Andrew P. Barrett,Morteza Karimzadeh*

Main category: cs.CV

TL;DR: 深度学习海冰分割需要根据传感器分辨率和任务类型优化多尺度空间上下文。


<details>
  <summary>Details</summary>
Motivation: 为了在基于深度学习的海冰分割中捕捉多尺度空间上下文，但基于观测分辨率和任务特性确定的最佳空间上下文规范仍有待探索。

Method: 使用具有不同空洞率的空洞空间金字塔池化来控制感受野大小，以捕捉多尺度上下文信息。使用梯度加权类激活映射可视化空洞率如何影响模型决策。探索了空间上下文和特征分辨率对不同海冰特性（浓度、发育阶段和浮冰尺寸）的影响，并研究了空间上下文如何影响不同输入特征组合（Sentinel-1 SAR 和 AMSR2）下的多任务海冰制图的分割性能。

Result: 较小的感受野更适合高分辨率的 Sentinel-1 数据；中等感受野在发育阶段分割方面表现更好；较大的感受野通常会导致性能下降。SAR 和 AMSR2 的融合提高了所有任务的分割精度。AMSR2 中较低分辨率的 18.7 和 36.5 GHz 频道在海冰测绘中具有价值。

Conclusion: 该研究强调了在海冰测绘中根据观测分辨率和目标特性选择合适空间上下文的重要性。通过系统分析多任务设置中感受野效应，本研究为优化地理空间应用中的深度学习模型提供了见解。

Abstract: Capturing spatial context at multiple scales is crucial for deep
learning-based sea ice segmentation. However, the optimal specification of
spatial context based on observation resolution and task characteristics
remains underexplored. This study investigates the impact of spatial context on
the segmentation of sea ice concentration, stage of development, and floe size
using a multi-task segmentation model. We implement Atrous Spatial Pyramid
Pooling with varying atrous rates to systematically control the receptive field
size of convolutional operations, and to capture multi-scale contextual
information. We explore the interactions between spatial context and feature
resolution for different sea ice properties and examine how spatial context
influences segmentation performance across different input feature combinations
from Sentinel-1 SAR and Advanced Microwave Radiometer-2 (AMSR2) for multi-task
mapping. Using Gradient-weighted Class Activation Mapping, we visualize how
atrous rates influence model decisions. Our findings indicate that smaller
receptive fields excel for high-resolution Sentinel-1 data, while medium
receptive fields yield better performances for stage of development
segmentation and larger receptive fields often lead to diminished performances.
The fusion of SAR and AMSR2 enhances segmentation across all tasks. We
highlight the value of lower-resolution 18.7 and 36.5 GHz AMSR2 channels in sea
ice mapping. These findings highlight the importance of selecting appropriate
spatial context based on observation resolution and target properties in sea
ice mapping. By systematically analyzing receptive field effects in a
multi-task setting, our study provides insights for optimizing deep learning
models in geospatial applications.

</details>


### [111] [Beyond Class Tokens: LLM-guided Dominant Property Mining for Few-shot Classification](https://arxiv.org/abs/2507.20511)
*Wei Zhuo,Runjie Luo,Wufeng Xue,Linlin Shen*

Main category: cs.CV

TL;DR: BCT-CLIP是一种新的少样本学习方法，通过利用LLM和对比学习来学习图像的“主导属性”，提高了对新类别的识别能力，尤其是在数据稀疏的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的基于对比语言-图像预训练的方法，虽然利用类别名称的文本表示来识别未见图像，但简单地将视觉表示与类别名称嵌入对齐会损害新类别区分的视觉多样性。

Method: 提出了一种新颖的少样本学习（FSL）方法BCT-CLIP，该方法通过对比学习探索主导属性，而不仅仅是使用类别令牌。具体方法包括：1. 提出了一种新颖的多属性生成器（MPG），它使用基于注意力的跨层连接来生成多个视觉属性令牌。2. 引入了一种基于LLM（大语言模型）的检索程序，并结合了基于聚类的剪枝，以获取主导属性描述。3. 提出了一种新的对比学习策略，用于属性令牌学习。

Result: BCT-CLIP在11个广泛使用的数据集上取得了优越的性能，证明了其在少样本分类任务中的有效性，并推动了FSL的发展。

Conclusion: BCT-CLIP通过探索对比学习中的主导属性，在11个广泛使用的数据集上展现出优越性能，证明了其在区分性类别特定表示学习和少样本分类方面的进步。

Abstract: Few-shot Learning (FSL), which endeavors to develop the generalization
ability for recognizing novel classes using only a few images, faces
significant challenges due to data scarcity. Recent CLIP-like methods based on
contrastive language-image pertaining mitigate the issue by leveraging textual
representation of the class name for unseen image discovery. Despite the
achieved success, simply aligning visual representations to class name
embeddings would compromise the visual diversity for novel class
discrimination. To this end, we proposed a novel Few-Shot Learning (FSL) method
(BCT-CLIP) that explores \textbf{dominating properties} via contrastive
learning beyond simply using class tokens. Through leveraging LLM-based prior
knowledge, our method pushes forward FSL with comprehensive structural image
representations, including both global category representation and the
patch-aware property embeddings. In particular, we presented a novel
multi-property generator (MPG) with patch-aware cross-attentions to generate
multiple visual property tokens, a Large-Language Model (LLM)-assistant
retrieval procedure with clustering-based pruning to obtain dominating property
descriptions, and a new contrastive learning strategy for property-token
learning. The superior performances on the 11 widely used datasets demonstrate
that our investigation of dominating properties advances discriminative
class-specific representation learning and few-shot classification.

</details>


### [112] [GaRe: Relightable 3D Gaussian Splatting for Outdoor Scenes from Unconstrained Photo Collections](https://arxiv.org/abs/2507.20512)
*Haiyang Bai,Jiaqi Zhu,Songru Jiang,Wei Huang,Tao Lu,Yuanqi Li,Jie Guo,Runze Fu,Yanwen Guo,Lijun Chen*

Main category: cs.CV

TL;DR: 使用3D高斯泼溅和内禀图像分解进行户外重新照明，可实现多样化的阴影操纵和动态阴影效果。


<details>
  <summary>Details</summary>
Motivation: 与先前将每张图像的全局光照压缩到单个潜在向量中的方法不同，我们的方法能够同时进行多样化的阴影操纵和动态阴影效果的生成。

Method: 提出了一种基于3D高斯泼溅的户外重新照明框架，利用了内禀图像分解技术，可以精确地整合来自无约束照片集合的日光、天穹辐射度和间接光照。该方法通过三个关键创新实现：1. 基于残差的太阳可见性提取方法，用于精确分离直射日光效应；2. 具有结构一致性损失的基于区域的监督框架，用于物理上可解释和连贯的光照分解；3. 用于逼真阴影模拟的射线追踪技术。

Result: 与最先进的重新照明解决方案相比，该框架在合成新视图时具有很高的保真度，并产生更自然、更多方面性的光照和阴影效果。

Conclusion: 该框架在保真度方面与最先进的重新照明解决方案具有竞争力，并在光照和阴影效果方面产生了更自然、更多方面的效果。

Abstract: We propose a 3D Gaussian splatting-based framework for outdoor relighting
that leverages intrinsic image decomposition to precisely integrate sunlight,
sky radiance, and indirect lighting from unconstrained photo collections.
Unlike prior methods that compress the per-image global illumination into a
single latent vector, our approach enables simultaneously diverse shading
manipulation and the generation of dynamic shadow effects. This is achieved
through three key innovations: (1) a residual-based sun visibility extraction
method to accurately separate direct sunlight effects, (2) a region-based
supervision framework with a structural consistency loss for physically
interpretable and coherent illumination decomposition, and (3) a
ray-tracing-based technique for realistic shadow simulation. Extensive
experiments demonstrate that our framework synthesizes novel views with
competitive fidelity against state-of-the-art relighting solutions and produces
more natural and multifaceted illumination and shadow effects.

</details>


### [113] [T2VParser: Adaptive Decomposition Tokens for Partial Alignment in Text to Video Retrieval](https://arxiv.org/abs/2507.20518)
*Yili Li,Gang Xiong,Gaopeng Gou,Xiangyan Qu,Jiamin Zhuang,Zhen Li,Junzheng Shi*

Main category: cs.CV

TL;DR: T2VParser 通过自适应分解令牌提取文本和视频的多视图语义表示，实现精确的部分对齐，解决了现有视频-文本检索中信息不对等的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频-文本检索方法主要依赖于 CLIP 等预训练模型，但视频信息比图像更丰富，而文本描述仅能反映部分视频内容，导致信息不对等和部分不匹配。直接对齐会产生不正确的监督。

Method: 提出 T2VParser，引入了自适应分解令牌（Adaptive Decomposition Tokens），这是一种跨模态共享的可学习令牌集，用于从文本和视频中提取多视图语义表示，实现自适应语义对齐，而不是对整个表示进行对齐。

Result: 实验结果表明，T2VParser 能够通过有效跨模态内容分解实现精确的部分对齐。

Conclusion: T2VParser 通过有效的多模态内容分解实现了精确的部分对齐，从而在视频-文本检索任务中取得了优异的性能。

Abstract: Text-to-video retrieval essentially aims to train models to align visual
content with textual descriptions accurately. Due to the impressive general
multimodal knowledge demonstrated by image-text pretrained models such as CLIP,
existing work has primarily focused on extending CLIP knowledge for video-text
tasks. However, videos typically contain richer information than images. In
current video-text datasets, textual descriptions can only reflect a portion of
the video content, leading to partial misalignment in video-text matching.
Therefore, directly aligning text representations with video representations
can result in incorrect supervision, ignoring the inequivalence of information.
In this work, we propose T2VParser to extract multiview semantic
representations from text and video, achieving adaptive semantic alignment
rather than aligning the entire representation. To extract corresponding
representations from different modalities, we introduce Adaptive Decomposition
Tokens, which consist of a set of learnable tokens shared across modalities.
The goal of T2VParser is to emphasize precise alignment between text and video
while retaining the knowledge of pretrained models. Experimental results
demonstrate that T2VParser achieves accurate partial alignment through
effective cross-modal content decomposition. The code is available at
https://github.com/Lilidamowang/T2VParser.

</details>


### [114] [AgroBench: Vision-Language Model Benchmark in Agriculture](https://arxiv.org/abs/2507.20519)
*Risa Shinoda,Nakamasa Inoue,Hirokatsu Kataoka,Masaki Onishi,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: AgroBench是一个由专家注释的农业VLM评估基准，涵盖了广泛的作物和疾病类别。评估结果显示，VLM在细粒度识别（尤其是在杂草识别方面）仍有待改进。


<details>
  <summary>Details</summary>
Motivation: 为了可持续的作物生产，精确的农业任务自动理解（如疾病识别）至关重要。最近VLM的进展有望通过简便的文本交流促进人机交互，从而扩展农业任务的应用范围。

Method: 提出AgroBench（农艺师AI基准），一个包含七个农业主题的VLM评估基准，涵盖了农业工程的关键领域，并与实际农业相关。AgroBench由专家农艺师进行注释，包含203个作物类别和682个疾病类别。

Result: 在AgroBench的评估中，我们发现VLM在细粒度识别任务方面仍有提升空间。特别是在杂草识别方面，大多数开源VLM的表现接近随机。

Conclusion: VLMs在细粒度识别任务方面仍有提升空间，尤其是在杂草识别方面，大多数开源VLM的表现接近随机水平。本研究通过广泛的主题和专家注释类别，分析了VLM的错误类型，并为未来的VLM发展提供了潜在的途径。

Abstract: Precise automated understanding of agricultural tasks such as disease
identification is essential for sustainable crop production. Recent advances in
vision-language models (VLMs) are expected to further expand the range of
agricultural tasks by facilitating human-model interaction through easy,
text-based communication. Here, we introduce AgroBench (Agronomist AI
Benchmark), a benchmark for evaluating VLM models across seven agricultural
topics, covering key areas in agricultural engineering and relevant to
real-world farming. Unlike recent agricultural VLM benchmarks, AgroBench is
annotated by expert agronomists. Our AgroBench covers a state-of-the-art range
of categories, including 203 crop categories and 682 disease categories, to
thoroughly evaluate VLM capabilities. In our evaluation on AgroBench, we reveal
that VLMs have room for improvement in fine-grained identification tasks.
Notably, in weed identification, most open-source VLMs perform close to random.
With our wide range of topics and expert-annotated categories, we analyze the
types of errors made by VLMs and suggest potential pathways for future VLM
development. Our dataset and code are available at
https://dahlian00.github.io/AgroBenchPage/ .

</details>


### [115] [Enhancing Spatial Reasoning through Visual and Textual Thinking](https://arxiv.org/abs/2507.20529)
*Xun Liang,Xin Guo,Zhongming Jin,Weihang Pan,Penghui Shang,Deng Cai,Binbin Lin,Jieping Ye*

Main category: cs.CV

TL;DR: 提出SpatialVTS方法，通过视觉和文本协同思考，提升视觉语言模型在空间推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在空间推理任务上仍存在不足，而空间推理是视觉问答和机器人技术的基础能力。

Method: SpatialVTS方法，包括空间视觉思维（生成与位置相关的特定标记）和空间文本思维（基于视觉线索和对话进行长期推理）两个阶段。

Result: 所提出的SpatialVTS模型在多项空间理解任务上，其整体平均水平相比其他模型有了显著提升，且未引入额外的掩码或深度信息。

Conclusion: 通过手动修正现有空间推理数据集、优化数据输入格式和开发逻辑推理细节，本研究提出的SpatialVTS方法在不引入额外信息的情况下，显著提升了模型在多项空间理解任务上的表现。

Abstract: The spatial reasoning task aims to reason about the spatial relationships in
2D and 3D space, which is a fundamental capability for Visual Question
Answering (VQA) and robotics. Although vision language models (VLMs) have
developed rapidly in recent years, they are still struggling with the spatial
reasoning task. In this paper, we introduce a method that can enhance Spatial
reasoning through Visual and Textual thinking Simultaneously (SpatialVTS). In
the spatial visual thinking phase, our model is trained to generate
location-related specific tokens of essential targets automatically. Not only
are the objects mentioned in the problem addressed, but also the potential
objects related to the reasoning are considered. During the spatial textual
thinking phase, Our model conducts long-term thinking based on visual cues and
dialogues, gradually inferring the answers to spatial reasoning problems. To
effectively support the model's training, we perform manual corrections to the
existing spatial reasoning dataset, eliminating numerous incorrect labels
resulting from automatic annotation, restructuring the data input format to
enhance generalization ability, and developing thinking processes with logical
reasoning details. Without introducing additional information (such as masks or
depth), our model's overall average level in several spatial understanding
tasks has significantly improved compared with other models.

</details>


### [116] [Low-Cost Machine Vision System for Sorting Green Lentils (Lens Culinaris) Based on Pneumatic Ejection and Deep Learning](https://arxiv.org/abs/2507.20531)
*Davy Rojas Yana,Edwin Salcedo*

Main category: cs.CV

TL;DR: 本研究开发了一个基于YOLOv8和气动弹射的动态绿扁豆分类与分选系统，准确率达87.2%。


<details>
  <summary>Details</summary>
Motivation: 为了实现绿扁豆的动态谷物分类，本研究旨在开发一个利用计算机视觉和气动弹射技术的系统。

Method: 本研究利用计算机视觉和气动弹射技术设计、开发和评估了一个动态谷物分类系统，用于绿扁豆（Lens Culinaris）。该系统集成了基于YOLOv8的检测模型来识别和定位传送带上的谷物，以及第二个基于YOLOv8的分类模型来将谷物分为六类：Good、Yellow、Broken、Peeled、Dotted和Reject。气动弹射装置用于分离有缺陷的谷物，而基于Arduino的控制系统协调视觉系统和机械组件之间的实时交互。

Result: 该系统在59毫米/秒的传送带速度下有效运行，实现了87.2%的谷物分离准确率。尽管处理速率有限（每分钟8克），但该原型验证了机器视觉在谷物分选中的应用潜力。

Conclusion: 该系统展示了机器视觉在谷物分选方面的潜力，并为未来的改进提供了模块化基础。

Abstract: This paper presents the design, development, and evaluation of a dynamic
grain classification system for green lentils (Lens Culinaris), which leverages
computer vision and pneumatic ejection. The system integrates a YOLOv8-based
detection model that identifies and locates grains on a conveyor belt, together
with a second YOLOv8-based classification model that categorises grains into
six classes: Good, Yellow, Broken, Peeled, Dotted, and Reject. This two-stage
YOLOv8 pipeline enables accurate, real-time, multi-class categorisation of
lentils, implemented on a low-cost, modular hardware platform. The pneumatic
ejection mechanism separates defective grains, while an Arduino-based control
system coordinates real-time interaction between the vision system and
mechanical components. The system operates effectively at a conveyor speed of
59 mm/s, achieving a grain separation accuracy of 87.2%. Despite a limited
processing rate of 8 grams per minute, the prototype demonstrates the potential
of machine vision for grain sorting and provides a modular foundation for
future enhancements.

</details>


### [117] [T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation](https://arxiv.org/abs/2507.20536)
*Chieh-Yun Chen,Min Shi,Gong Zhang,Humphrey Shi*

Main category: cs.CV

TL;DR: T2I-Copilot是一个多智能体系统，通过LLM协作简化T2I提示词，提高生成质量。无需训练，性能优越，成本效益高。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像（T2I）生成模型对提示词措辞高度敏感，用户需要反复调整提示词且缺乏清晰的反馈。现有的缓解技术（如自动提示工程、控制文本嵌入、去噪和多轮生成）存在可控性有限、需要额外训练或限制泛化能力等问题。

Method: T2I-Copilot是一个无需训练的多智能体系统，利用大型语言模型（LLM）的协作来自动化提示词措辞、模型选择和迭代优化。它包含三个智能体：输入解析器（处理输入提示词、解决歧义、生成标准化报告）、生成引擎（选择合适的T2I模型、组织视觉和文本提示词以启动生成）和质量评估器（评估美学质量和图文对齐度，提供评分和反馈以进行再生）。该系统可完全自主运行，也支持人工干预。

Result: T2I-Copilot在GenAI-Bench基准测试中，使用开源生成模型，实现了与商业模型RecraftV3和Imagen 3相当的VQA分数，其成本仅为后者的16.59%，同时在性能上超越了FLUX1.1-pro（6.17%）、FLUX.1-dev（9.11%）和SD 3.5 Large（6.36%）。

Conclusion: T2I-Copilot通过多智能体协作，在不进行额外训练的情况下，显著简化了提示词工程，提升了生成质量和图文对齐度。在GenAI-Bench基准测试中，其VQA得分可与商业模型相媲美，并且在成本效益上远超其他模型。

Abstract: Text-to-Image (T2I) generative models have revolutionized content creation
but remain highly sensitive to prompt phrasing, often requiring users to
repeatedly refine prompts multiple times without clear feedback. While
techniques such as automatic prompt engineering, controlled text embeddings,
denoising, and multi-turn generation mitigate these issues, they offer limited
controllability, or often necessitate additional training, restricting the
generalization abilities. Thus, we introduce T2I-Copilot, a training-free
multi-agent system that leverages collaboration between (Multimodal) Large
Language Models to automate prompt phrasing, model selection, and iterative
refinement. This approach significantly simplifies prompt engineering while
enhancing generation quality and text-image alignment compared to direct
generation. Specifically, T2I-Copilot consists of three agents: (1) Input
Interpreter, which parses the input prompt, resolves ambiguities, and generates
a standardized report; (2) Generation Engine, which selects the appropriate
model from different types of T2I models and organizes visual and textual
prompts to initiate generation; and (3) Quality Evaluator, which assesses
aesthetic quality and text-image alignment, providing scores and feedback for
potential regeneration. T2I-Copilot can operate fully autonomously while also
supporting human-in-the-loop intervention for fine-grained control. On
GenAI-Bench, using open-source generation models, T2I-Copilot achieves a VQA
score comparable to commercial models RecraftV3 and Imagen 3, surpasses
FLUX1.1-pro by 6.17% at only 16.59% of its cost, and outperforms FLUX.1-dev and
SD 3.5 Large by 9.11% and 6.36%. Code will be released at:
https://github.com/SHI-Labs/T2I-Copilot.

</details>


### [118] [Annotation-Free Human Sketch Quality Assessment](https://arxiv.org/abs/2507.20548)
*Lan Yang,Kaiyue Pang,Honggang Zhang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: 本研究提出 GACL 方法，利用特征幅度评估素描质量，并成功应用于多种场景。


<details>
  <summary>Details</summary>
Motivation: 为了解决手绘素描质量不高的问题，填补素描质量评估领域的研究空白。

Method: 提出了一种名为 GACL（Geometry-Aware Classification Layer）的通用方法，该方法将特征幅度作为质量度量，并与可识别性学习作为一个双任务进行优化。GACL 使用交叉熵分类损失，具有理论保证，并且不依赖于人类的特定质量标注。

Result: 通过大规模人体研究（160,000 次试验）验证了 GACL 指标与人类质量感知的可比性。GACL 可用于素描应用、自然图像质量评估（IQA）以及带噪标签清理等问题。

Conclusion: 该研究首次解决了素描质量评估问题，并提出了一种名为 GACL 的通用方法，该方法利用素描特征的幅度（L2 范数）作为量化质量指标。GACL 作为一种双任务学习方法，同时优化了特征幅度与可识别性学习，并通过交叉熵分类损失实现，具有理论保证。实验在大规模人体研究中验证了 GACL 指标与人类感知的一致性，并展示了其在素描应用和自然图像质量评估（IQA）以及带噪标签清理等实际问题中的潜力。

Abstract: As lovely as bunnies are, your sketched version would probably not do them
justice (Fig.~\ref{fig:intro}). This paper recognises this very problem and
studies sketch quality assessment for the first time -- letting you find these
badly drawn ones. Our key discovery lies in exploiting the magnitude ($L_2$
norm) of a sketch feature as a quantitative quality metric. We propose
Geometry-Aware Classification Layer (GACL), a generic method that makes
feature-magnitude-as-quality-metric possible and importantly does it without
the need for specific quality annotations from humans. GACL sees feature
magnitude and recognisability learning as a dual task, which can be
simultaneously optimised under a neat cross-entropy classification loss with
theoretic guarantee. This gives GACL a nice geometric interpretation (the
better the quality, the easier the recognition), and makes it agnostic to both
network architecture changes and the underlying sketch representation. Through
a large scale human study of 160,000 \doublecheck{trials}, we confirm the
agreement between our GACL-induced metric and human quality perception. We
further demonstrate how such a quality assessment capability can for the first
time enable three practical sketch applications. Interestingly, we show GACL
not only works on abstract visual representations such as sketch but also
extends well to natural images on the problem of image quality assessment
(IQA). Last but not least, we spell out the general properties of GACL as
general-purpose data re-weighting strategy and demonstrate its applications in
vertical problems such as noisy label cleansing. Code will be made publicly
available at github.com/yanglan0225/SketchX-Quantifying-Sketch-Quality.

</details>


### [119] [FED-PsyAU: Privacy-Preserving Micro-Expression Recognition via Psychological AU Coordination and Dynamic Facial Motion Modeling](https://arxiv.org/abs/2507.20557)
*Jingting Li,Yu Qian,Lin Zhao,Su-Jing Wang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为FED-PsyAU的框架，通过结合心理学知识、图注意力网络和联邦学习，解决了微表情识别中的数据隐私和样本量小的问题，并有效提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 微表情（MEs）能揭示个体试图隐藏的真实情感，在刑事审讯和心理咨询等领域具有价值。然而，微表情识别（MER）面临样本量小、特征细微等挑战，难以有效建模。此外，实际应用中的数据隐私问题阻碍了跨环境的识别能力提升。

Method: 1. 心理学研究：分析面部动作单元（AUs）的协调性，以获得面部肌肉动力学的结构化先验知识。 2. DPK-GAT网络：结合心理学先验知识和统计AU模式，实现从区域到全局的层级化面部运动特征学习。 3. 联邦学习框架：在不共享数据的情况下，允许多个客户端协同进行MER，保护隐私并缓解样本量不足的问题。

Result: 实验结果表明，所提出的FED-PsyAU框架在常用的微表情数据库上能够有效提升微表情识别的性能。

Conclusion: 该研究提出了一种名为FED-PsyAU的框架，通过结合心理学先验知识和图注意力网络（GAT），并利用联邦学习技术，有效提升了微表情识别（MER）的性能，同时解决了数据隐私和样本量小的问题。

Abstract: Micro-expressions (MEs) are brief, low-intensity, often localized facial
expressions. They could reveal genuine emotions individuals may attempt to
conceal, valuable in contexts like criminal interrogation and psychological
counseling. However, ME recognition (MER) faces challenges, such as small
sample sizes and subtle features, which hinder efficient modeling.
Additionally, real-world applications encounter ME data privacy issues, leaving
the task of enhancing recognition across settings under privacy constraints
largely unexplored. To address these issues, we propose a FED-PsyAU research
framework. We begin with a psychological study on the coordination of upper and
lower facial action units (AUs) to provide structured prior knowledge of facial
muscle dynamics. We then develop a DPK-GAT network that combines these
psychological priors with statistical AU patterns, enabling hierarchical
learning of facial motion features from regional to global levels, effectively
enhancing MER performance. Additionally, our federated learning framework
advances MER capabilities across multiple clients without data sharing,
preserving privacy and alleviating the limited-sample issue for each client.
Extensive experiments on commonly-used ME databases demonstrate the
effectiveness of our approach.

</details>


### [120] [MemoryTalker: Personalized Speech-Driven 3D Facial Animation via Audio-Guided Stylization](https://arxiv.org/abs/2507.20562)
*Hyung Kyu Kim,Sangmin Lee,Hak Gu Kim*

Main category: cs.CV

TL;DR: MemoryTalker synthesizes realistic 3D facial animation from audio alone, capturing speaking style without prior info by using a two-stage memory-based approach for personalized for the speaker.


<details>
  <summary>Details</summary>
Motivation: To enable realistic and accurate 3D facial motion synthesis solely from audio input, reflecting the speaker's speaking style without requiring prior information like speaker class labels or additional 3D facial meshes during inference, thereby maximizing usability in applications.

Method: The framework involves two training stages: 1-stage for storing and retrieving general motion (Memorizing), and 2-stage for personalized facial motion synthesis (Animating) using a motion memory stylized by audio-driven speaking style features. The second stage focuses on learning which facial motion types to emphasize for a given audio.

Result: The MemoryTalker model generates realistic and accurate 3D facial motion sequences that reflect the speaker's speaking style using only audio input. It has been evaluated quantitatively, qualitatively, and through a user study, demonstrating its effectiveness and improved performance compared to existing methods.

Conclusion: MemoryTalker can generate reliable personalized facial animation without additional prior information, showing effectiveness and performance enhancement over state-of-the-art methods through quantitative and qualitative evaluations, as well as a user study.

Abstract: Speech-driven 3D facial animation aims to synthesize realistic facial motion
sequences from given audio, matching the speaker's speaking style. However,
previous works often require priors such as class labels of a speaker or
additional 3D facial meshes at inference, which makes them fail to reflect the
speaking style and limits their practical use. To address these issues, we
propose MemoryTalker which enables realistic and accurate 3D facial motion
synthesis by reflecting speaking style only with audio input to maximize
usability in applications. Our framework consists of two training stages:
1-stage is storing and retrieving general motion (i.e., Memorizing), and
2-stage is to perform the personalized facial motion synthesis (i.e.,
Animating) with the motion memory stylized by the audio-driven speaking style
feature. In this second stage, our model learns about which facial motion types
should be emphasized for a particular piece of audio. As a result, our
MemoryTalker can generate a reliable personalized facial animation without
additional prior information. With quantitative and qualitative evaluations, as
well as user study, we show the effectiveness of our model and its performance
enhancement for personalized facial animation over state-of-the-art methods.

</details>


### [121] [Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven 3D Facial Animation](https://arxiv.org/abs/2507.20568)
*Hyung Kyu Kim,Hak Gu Kim*

Main category: cs.CV

TL;DR: 与传统逐帧方法不同，我们提出了一种新的语音上下文感知损失，通过整合视觉音素协同发音权重来解决面部动画中的抖动问题，并在实验中取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 传统的逐帧方法难以捕捉面部运动的连续性，导致输出抖动和不自然，原因是辅音连缀。

Method: 提出了一种新颖的语音上下文感知损失，通过引入视觉音素协同发音权重来显式建模语音上下文对视觉音素转换的影响，并根据面部运动随时间的动态变化对其进行自适应重要性分配。

Result: 实验表明，用所提出的语音上下文感知损失替代传统的重建损失，可以同时提高定量指标和视觉质量，确保更平滑、可感知一致的动画。

Conclusion: 通过显式建模依赖于语音上下文的视觉音素，可以合成自然的语音驱动的 3D 面部动画。

Abstract: Speech-driven 3D facial animation aims to generate realistic facial movements
synchronized with audio. Traditional methods primarily minimize reconstruction
loss by aligning each frame with ground-truth. However, this frame-wise
approach often fails to capture the continuity of facial motion, leading to
jittery and unnatural outputs due to coarticulation. To address this, we
propose a novel phonetic context-aware loss, which explicitly models the
influence of phonetic context on viseme transitions. By incorporating a viseme
coarticulation weight, we assign adaptive importance to facial movements based
on their dynamic changes over time, ensuring smoother and perceptually
consistent animations. Extensive experiments demonstrate that replacing the
conventional reconstruction loss with ours improves both quantitative metrics
and visual quality. It highlights the importance of explicitly modeling
phonetic context-dependent visemes in synthesizing natural speech-driven 3D
facial animation. Project page: https://cau-irislab.github.io/interspeech25/

</details>


### [122] [LSFDNet: A Single-Stage Fusion and Detection Network for Ships Using SWIR and LWIR](https://arxiv.org/abs/2507.20574)
*Yanyin Guo,Runxuan An,Junwei Li,Zhiyuan Zhang*

Main category: cs.CV

TL;DR: 提出了一种结合SWIR和LWIR的单阶段船舶检测算法LSFDNet，通过MLCF模块和OE损失函数提升了检测效果，并发布了NSLSR数据集。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统单模态（可见光或红外图像）船舶检测方法在复杂光照和重雾条件下受限的问题，探索了短波红外（SWIR）和长波红外（LWIR）在船舶检测中的优势。

Method: 提出了一种名为LSFDNet的单阶段图像融合检测算法，利用了多层次交叉融合（MLCF）模块来增强对象显着性，并通过对象增强（OE）损失函数利用检测任务的位置先验来优化融合图像中的对象语义。此外，还建立了近岸船只长短波配准（NSLSR）数据集来训练模型。

Result: LSFDNet算法实现了显著的检测性能，并生成了视觉效果出色的融合图像，融合了来自SWIR和LWIR的特征，以增强检测性能。

Conclusion: 所提出的单阶段融合检测算法在两个数据集上验证了其优越性，并且开源了代码和数据集。

Abstract: Traditional ship detection methods primarily rely on single-modal approaches,
such as visible or infrared images, which limit their application in complex
scenarios involving varying lighting conditions and heavy fog. To address this
issue, we explore the advantages of short-wave infrared (SWIR) and long-wave
infrared (LWIR) in ship detection and propose a novel single-stage image fusion
detection algorithm called LSFDNet. This algorithm leverages feature
interaction between the image fusion and object detection subtask networks,
achieving remarkable detection performance and generating visually impressive
fused images. To further improve the saliency of objects in the fused images
and improve the performance of the downstream detection task, we introduce the
Multi-Level Cross-Fusion (MLCF) module. This module combines object-sensitive
fused features from the detection task and aggregates features across multiple
modalities, scales, and tasks to obtain more semantically rich fused features.
Moreover, we utilize the position prior from the detection task in the Object
Enhancement (OE) loss function, further increasing the retention of object
semantics in the fused images. The detection task also utilizes preliminary
fused features from the fusion task to complement SWIR and LWIR features,
thereby enhancing detection performance. Additionally, we have established a
Nearshore Ship Long-Short Wave Registration (NSLSR) dataset to train effective
SWIR and LWIR image fusion and detection networks, bridging a gap in this
field. We validated the superiority of our proposed single-stage fusion
detection algorithm on two datasets. The source code and dataset are available
at https://github.com/Yanyin-Guo/LSFDNet

</details>


### [123] [AV-Deepfake1M++: A Large-Scale Audio-Visual Deepfake Benchmark with Real-World Perturbations](https://arxiv.org/abs/2507.20579)
*Zhixi Cai,Kartik Kuckreja,Shreya Ghosh,Akanksha Chuchra,Muhammad Haris Khan,Usman Tariq,Tom Gedeon,Abhinav Dhall*

Main category: cs.CV

TL;DR: 由于文本到语音和人脸语音重演模型，视频伪造变得更加容易和逼真。我们提出了 AV-Deepfake1M++，这是一个包含 200 万个视频剪辑的数据集，具有多样化的操纵策略和音视扰动，以应对这一问题。该数据集促进了 Deepfake 领域的研究，并用于托管 2025 1M-Deepfakes 检测挑战赛。


<details>
  <summary>Details</summary>
Motivation: 为了应对文本到语音和人脸语音重演模型使视频伪造更加容易且高度逼真的问题，需要丰富生成方法和扰动策略的数据集。

Method: 提出 AV-Deepfake1M++ 数据集，这是 AV-Deepfake1M 的扩展，包含 200 万个视频剪辑，具有多样化的操纵策略和音视扰动。

Result: AV-Deepfake1M++ 数据集包含 200 万个视频剪辑，具有多样化的操纵策略和音视扰动，并对该数据集进行了基准测试。

Conclusion: 该数据集将促进 Deepfake 领域的研究，并已用于托管 2025 1M-Deepfakes 检测挑战赛。

Abstract: The rapid surge of text-to-speech and face-voice reenactment models makes
video fabrication easier and highly realistic. To encounter this problem, we
require datasets that rich in type of generation methods and perturbation
strategy which is usually common for online videos. To this end, we propose
AV-Deepfake1M++, an extension of the AV-Deepfake1M having 2 million video clips
with diversified manipulation strategy and audio-visual perturbation. This
paper includes the description of data generation strategies along with
benchmarking of AV-Deepfake1M++ using state-of-the-art methods. We believe that
this dataset will play a pivotal role in facilitating research in Deepfake
domain. Based on this dataset, we host the 2025 1M-Deepfakes Detection
Challenge. The challenge details, dataset and evaluation scripts are available
online under a research-only license at https://deepfakes1m.github.io/2025.

</details>


### [124] [M-Net: MRI Brain Tumor Sequential Segmentation Network via Mesh-Cast](https://arxiv.org/abs/2507.20582)
*Jiacheng Lu,Hui Ding,Shiyu Zhang,Guoping Huo*

Main category: cs.CV

TL;DR: M-Net 通过利用MRI切片间的“类时间”信息，提高分割准确性，同时降低计算成本，并在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MRI肿瘤分割模型在处理3D数据时面临计算挑战，并且未能充分利用MRI切片在空间上的顺序信息，而这种信息对于提高分割的连续性和准确性至关重要。

Method: 提出了一种名为M-Net的框架，该框架利用了MRI切片在空间上的顺序排列，将相邻切片间的空间相关性类比为视频帧序列中的时间信息。M-Net 引入了 Mesh-Cast 机制，能够将任意序列模型整合到通道和时间信息的处理中，从而捕捉MRI切片间的“类时间”空间相关性。此外，还定义了一种MRI序列输入模式和两阶段序列（TPS）训练策略，优先学习序列共性模式，再进行切片特定特征提取。

Result: M-Net 成功捕捉了MRI切片间的“类时间”空间相关性，避免了3D卷积的高计算成本，提高了模型在序列分割任务中的泛化性和鲁棒性。

Conclusion: M-Net 在乳腺MRI肿瘤分割任务中表现出色，在BraTS2019和BraTS2023数据集上均超越了现有方法，证明了其作为时间感知乳腺MRI肿瘤分割的鲁棒解决方案的潜力。

Abstract: MRI tumor segmentation remains a critical challenge in medical imaging, where
volumetric analysis faces unique computational demands due to the complexity of
3D data. The spatially sequential arrangement of adjacent MRI slices provides
valuable information that enhances segmentation continuity and accuracy, yet
this characteristic remains underutilized in many existing models. The spatial
correlations between adjacent MRI slices can be regarded as "temporal-like"
data, similar to frame sequences in video segmentation tasks. To bridge this
gap, we propose M-Net, a flexible framework specifically designed for
sequential image segmentation. M-Net introduces the novel Mesh-Cast mechanism,
which seamlessly integrates arbitrary sequential models into the processing of
both channel and temporal information, thereby systematically capturing the
inherent "temporal-like" spatial correlations between MRI slices. Additionally,
we define an MRI sequential input pattern and design a Two-Phase Sequential
(TPS) training strategy, which first focuses on learning common patterns across
sequences before refining slice-specific feature extraction. This approach
leverages temporal modeling techniques to preserve volumetric contextual
information while avoiding the high computational cost of full 3D convolutions,
thereby enhancing the generalizability and robustness of M-Net in sequential
segmentation tasks. Experiments on the BraTS2019 and BraTS2023 datasets
demonstrate that M-Net outperforms existing methods across all key metrics,
establishing itself as a robust solution for temporally-aware MRI tumor
segmentation.

</details>


### [125] [Harnessing Diffusion-Yielded Score Priors for Image Restoration](https://arxiv.org/abs/2507.20590)
*Xinqi Lin,Fanghua Yu,Jinfan Hu,Zhiyuan You,Wu Shi,Jimmy S. Ren,Jinjin Gu,Chao Dong*

Main category: cs.CV

TL;DR: HYPIR是一种新的图像恢复方法，通过结合预训练扩散模型和对抗性训练，实现了高质量、高保真度和高效率。


<details>
  <summary>Details</summary>
Motivation: 现有的深度图像恢复模型在去除伪影、生成真实细节和保持像素级一致性方面面临挑战，且在恢复质量、保真度和速度之间难以取得良好平衡。HYPIR旨在解决这些问题。

Method: HYPIR方法首先使用预训练的扩散模型初始化图像恢复模型，然后通过对抗性训练进行微调，无需扩散损失、迭代采样或额外的适配器。

Result: HYPIR在数值稳定性、避免模式崩溃和加速收敛方面表现出色，并继承了扩散模型的丰富用户控制能力（如文本引导恢复和纹理可调），实现了比扩散模型更快的收敛和推理速度，实验证明其性能优于现有最先进方法。

Conclusion: HYPIR通过结合预训练的扩散模型和对抗性训练，在图像恢复任务中实现了高质量、高保真度和高效率的平衡，优于现有方法。

Abstract: Deep image restoration models aim to learn a mapping from degraded image
space to natural image space. However, they face several critical challenges:
removing degradation, generating realistic details, and ensuring pixel-level
consistency. Over time, three major classes of methods have emerged, including
MSE-based, GAN-based, and diffusion-based methods. However, they fail to
achieve a good balance between restoration quality, fidelity, and speed. We
propose a novel method, HYPIR, to address these challenges. Our solution
pipeline is straightforward: it involves initializing the image restoration
model with a pre-trained diffusion model and then fine-tuning it with
adversarial training. This approach does not rely on diffusion loss, iterative
sampling, or additional adapters. We theoretically demonstrate that
initializing adversarial training from a pre-trained diffusion model positions
the initial restoration model very close to the natural image distribution.
Consequently, this initialization improves numerical stability, avoids mode
collapse, and substantially accelerates the convergence of adversarial
training. Moreover, HYPIR inherits the capabilities of diffusion models with
rich user control, enabling text-guided restoration and adjustable texture
richness. Requiring only a single forward pass, it achieves faster convergence
and inference speed than diffusion-based methods. Extensive experiments show
that HYPIR outperforms previous state-of-the-art methods, achieving efficient
and high-quality image restoration.

</details>


### [126] [Enhanced Deep Learning DeepFake Detection Integrating Handcrafted Features](https://arxiv.org/abs/2507.20608)
*Alejandro Hinke-Navarro,Mario Nieto-Hidalgo,Juan M. Espin,Juan E. Tapia*

Main category: cs.CV

TL;DR: 提出了一种新的深度伪造检测框架，结合了频域特征和RGB图像，以提高检测精度。


<details>
  <summary>Details</summary>
Motivation: 为了应对深度伪造和换脸技术在数字安全和身份验证领域带来的挑战，以及传统检测方法在处理复杂人脸操纵时的局限性。

Method: 提出了一种结合手工制作的频域特征（如隐写分析富模型、离散余弦变换、误差等级分析、奇异值分解和离散傅里叶变换）和RGB输入的混合深度学习检测框架。

Result: 该混合方法利用了图像操纵引入的频域和空间域伪影，为分类器提供了更丰富、更具辨别力的信息，从而提高了检测性能。

Conclusion: 该研究提出了一个结合手工制作的频域特征和RGB输入的混合深度学习检测框架，以提高深度伪造和换脸技术的检测能力，解决了传统方法泛化能力不足的问题。

Abstract: The rapid advancement of deepfake and face swap technologies has raised
significant concerns in digital security, particularly in identity verification
and onboarding processes. Conventional detection methods often struggle to
generalize against sophisticated facial manipulations. This study proposes an
enhanced deep-learning detection framework that combines handcrafted
frequency-domain features with conventional RGB inputs. This hybrid approach
exploits frequency and spatial domain artifacts introduced during image
manipulation, providing richer and more discriminative information to the
classifier. Several frequency handcrafted features were evaluated, including
the Steganalysis Rich Model, Discrete Cosine Transform, Error Level Analysis,
Singular Value Decomposition, and Discrete Fourier Transform

</details>


### [127] [Lightweight Remote Sensing Scene Classification on Edge Devices via Knowledge Distillation and Early-exit](https://arxiv.org/abs/2507.20623)
*Yang Zhao,Shusheng Li,Xueshang Feng*

Main category: cs.CV

TL;DR: 提出了一种名为E3C的轻量级遥感场景分类框架，通过频率域蒸馏和动态早退出机制，在保持精度的同时，显著提高了边缘设备的推理速度和能效。


<details>
  <summary>Details</summary>
Motivation: 针对轻量化深度学习算法的发展，尽管已提出多种用于遥感场景分类（RSSC）的深度神经网络（DNN）模型，但在资源受限的边缘设备上，这些模型在模型精度、推理延迟和能耗之间实现最优性能仍然面临挑战。

Method: 本文提出了一种轻量级的遥感场景分类（RSSC）框架，包括经过蒸馏的全局滤波器网络（GFNet）模型和为边缘设备设计的动态早退出口机制。

Result: 在三个边缘设备和四个数据集上进行的广泛实验表明，E3C模型实现了1.3倍的平均推理加速和超过40%的能效提升，同时保持了高分类精度。

Conclusion: 所提出的E3C模型在边缘设备上实现了对GFNet模型的频率域蒸馏和动态早退出口机制，在保持高分类精度的同时，实现了1.3倍的模型推理加速和超过40%的能效提升。

Abstract: As the development of lightweight deep learning algorithms, various deep
neural network (DNN) models have been proposed for the remote sensing scene
classification (RSSC) application. However, it is still challenging for these
RSSC models to achieve optimal performance among model accuracy, inference
latency, and energy consumption on resource-constrained edge devices. In this
paper, we propose a lightweight RSSC framework, which includes a distilled
global filter network (GFNet) model and an early-exit mechanism designed for
edge devices to achieve state-of-the-art performance. Specifically, we first
apply frequency domain distillation on the GFNet model to reduce model size.
Then we design a dynamic early-exit model tailored for DNN models on edge
devices to further improve model inference efficiency. We evaluate our E3C
model on three edge devices across four datasets. Extensive experimental
results show that it achieves an average of 1.3x speedup on model inference and
over 40% improvement on energy efficiency, while maintaining high
classification accuracy.

</details>


### [128] [DAMS:Dual-Branch Adaptive Multiscale Spatiotemporal Framework for Video Anomaly Detection](https://arxiv.org/abs/2507.20629)
*Dezhi An,Wenqiang Liu,Kefan Wang,Zening Chen,Jun Lu,Shengcai Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一个名为DAMS的双路径框架，通过多层次特征解耦、融合以及结合CLIP的跨模态语义引导，有效解决了视频异常检测中的多尺度时间依赖、视觉-语义异质性和数据稀缺性问题，并在标准基准测试中取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 视频异常检测的目标是在视频中对异常事件进行时空定位。然而，视频异常的难点在于其固有的挑战，包括多尺度时间依赖性、视觉-语义异质性以及标记数据的稀缺性。这些因素共同构成了计算机视觉领域一个具有挑战性的研究问题。

Method: 该研究提出了一个名为DAMS（Dual-Branch Adaptive Multiscale Spatiotemporal Framework）的双路径框架。该框架基于多层次特征解耦和融合，通过整合分层特征学习和互补信息来实现高效的异常检测建模。主要处理路径采用了自适应多尺度时间金字塔网络（AMTPN）和卷积块注意力机制（CBAM）。AMTPN通过三层级联结构（时间金字塔池化、自适应特征融合和时间上下文增强）实现多粒度表示和动态加权的重构。CBAM通过双注意力映射最大化特征通道和空间维度的熵分布。同时，并行的CLIP驱动路径引入了对比语言-视觉预训练范式，利用跨模态语义对齐和多尺度实例选择机制，为时空特征提供高阶语义指导，从而构建了从底层时空特征到高层语义概念的完整推理链。两条路径的正交互补性和信息融合机制共同构建了针对异常事件的全面表示和识别能力。

Result: 该框架在UCF-Crime和XD-Violence基准数据集上进行了广泛的实验评估，结果证明了DAMS框架的有效性。

Conclusion: 该研究提出的DAMS框架在UCF-Crime和XD-Violence基准测试中取得了显著的实验效果，证明了其在视频异常检测方面的有效性。

Abstract: The goal of video anomaly detection is tantamount to performing
spatio-temporal localization of abnormal events in the video. The multiscale
temporal dependencies, visual-semantic heterogeneity, and the scarcity of
labeled data exhibited by video anomalies collectively present a challenging
research problem in computer vision. This study offers a dual-path architecture
called the Dual-Branch Adaptive Multiscale Spatiotemporal Framework (DAMS),
which is based on multilevel feature decoupling and fusion, enabling efficient
anomaly detection modeling by integrating hierarchical feature learning and
complementary information. The main processing path of this framework
integrates the Adaptive Multiscale Time Pyramid Network (AMTPN) with the
Convolutional Block Attention Mechanism (CBAM). AMTPN enables multigrained
representation and dynamically weighted reconstruction of temporal features
through a three-level cascade structure (time pyramid pooling, adaptive feature
fusion, and temporal context enhancement). CBAM maximizes the entropy
distribution of feature channels and spatial dimensions through dual attention
mapping. Simultaneously, the parallel path driven by CLIP introduces a
contrastive language-visual pre-training paradigm. Cross-modal semantic
alignment and a multiscale instance selection mechanism provide high-order
semantic guidance for spatio-temporal features. This creates a complete
inference chain from the underlying spatio-temporal features to high-level
semantic concepts. The orthogonal complementarity of the two paths and the
information fusion mechanism jointly construct a comprehensive representation
and identification capability for anomalous events. Extensive experimental
results on the UCF-Crime and XD-Violence benchmarks establish the effectiveness
of the DAMS framework.

</details>


### [129] [TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model](https://arxiv.org/abs/2507.20630)
*Ao Li,Yuxiang Duan,Jinghui Zhang,Congbo Ma,Yutong Xie,Gustavo Carneiro,Mohammad Yaqub,Hu Wang*

Main category: cs.CV

TL;DR: TransPrune 通过基于 token 转换的 TTV 和 IGA 方法，有效降低了 LVLMs 的推理计算成本，同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高大型视觉语言模型（LVLMs）的推理效率，应对大量视觉 token 带来的高昂计算成本。

Method: TransPrune 提出了一种新的训练无关的 token 修剪方法，该方法通过结合 Token Transition Variation (TTV) 和 Instruction-Guided Attention (IGA) 来评估 token 的重要性。

Result: TransPrune 在八个基准测试中实现了与原始 LVLMs（如 LLaVA-v1.5 和 LLaVA-Next）相当的多模态性能，同时将推理 TFLOPs 减少了一半以上。

Conclusion: TransPrune 在不牺牲多模态性能的情况下，通过引入基于 token 转换的新视角，将推理 TFLOPs 减少了一半以上，并且 TTV 本身也可作为不依赖于 attention 的有效标准。

Abstract: Large Vision-Language Models (LVLMs) have advanced multimodal learning but
face high computational costs due to the large number of visual tokens,
motivating token pruning to improve inference efficiency. The key challenge
lies in identifying which tokens are truly important. Most existing approaches
rely on attention-based criteria to estimate token importance. However, they
inherently suffer from certain limitations, such as positional bias. In this
work, we explore a new perspective on token importance based on token
transitions in LVLMs. We observe that the transition of token representations
provides a meaningful signal of semantic information. Based on this insight, we
propose TransPrune, a training-free and efficient token pruning method.
Specifically, TransPrune progressively prunes tokens by assessing their
importance through a combination of Token Transition Variation (TTV)-which
measures changes in both the magnitude and direction of token
representations-and Instruction-Guided Attention (IGA), which measures how
strongly the instruction attends to image tokens via attention. Extensive
experiments demonstrate that TransPrune achieves comparable multimodal
performance to original LVLMs, such as LLaVA-v1.5 and LLaVA-Next, across eight
benchmarks, while reducing inference TFLOPs by more than half. Moreover, TTV
alone can serve as an effective criterion without relying on attention,
achieving performance comparable to attention-based methods. The code will be
made publicly available upon acceptance of the paper at
https://github.com/liaolea/TransPrune.

</details>


### [130] [Self-Supervised Continuous Colormap Recovery from a 2D Scalar Field Visualization without a Legend](https://arxiv.org/abs/2507.20632)
*Hongxu Liu,Xinyu Chen,Haoyang Zheng,Manyi Li,Zhenfan Liu,Fumeng Yang,Yunhai Wang,Changhe Tu,Qiong Zeng*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recovering a continuous colormap from a single 2D scalar field visualization
can be quite challenging, especially in the absence of a corresponding color
legend. In this paper, we propose a novel colormap recovery approach that
extracts the colormap from a color-encoded 2D scalar field visualization by
simultaneously predicting the colormap and underlying data using a
decoupling-and-reconstruction strategy. Our approach first separates the input
visualization into colormap and data using a decoupling module, then
reconstructs the visualization with a differentiable color-mapping module. To
guide this process, we design a reconstruction loss between the input and
reconstructed visualizations, which serves both as a constraint to ensure
strong correlation between colormap and data during training, and as a
self-supervised optimizer for fine-tuning the predicted colormap of unseen
visualizations during inferencing. To ensure smoothness and correct color
ordering in the extracted colormap, we introduce a compact colormap
representation using cubic B-spline curves and an associated color order loss.
We evaluate our method quantitatively and qualitatively on a synthetic dataset
and a collection of real-world visualizations from the VIS30K dataset.
Additionally, we demonstrate its utility in two prototype applications --
colormap adjustment and colormap transfer -- and explore its generalization to
visualizations with color legends and ones encoded using discrete color
palettes.

</details>


### [131] [A Multimodal Architecture for Endpoint Position Prediction in Team-based Multiplayer Games](https://arxiv.org/abs/2507.20670)
*Jonas Peche,Aliaksei Tsishurou,Alexander Zap,Guenter Wallner*

Main category: cs.CV

TL;DR: 提出了一种多模态架构，利用 U-Net 和多头注意力机制，结合图像、数值、分类和动态游戏数据，来预测多人游戏中的玩家位置，可用于机器人导航、策略推荐和行为分析。


<details>
  <summary>Details</summary>
Motivation: 理解和预测多人游戏中的玩家移动对于实现类似玩家的机器人导航、先发制人的机器人控制、推荐策略和实时玩家行为分析等用例至关重要。然而，复杂环境允许高度的导航自由度，玩家之间的交互和团队合作需要能够有效利用可用异构输入数据的模型。

Method: 提出了一种多模态架构，用于在动态时间范围内预测未来玩家位置，使用基于 U-Net 的方法来计算端点位置概率热图，并使用多模态特征编码器进行条件化。多头注意力机制应用于不同组的特征，以实现智能体之间的通信。

Result: 所提出的架构可高效利用多模态游戏状态，包括图像输入、数值和分类特征以及动态游戏数据。

Conclusion: 该技术为依赖未来玩家位置的各种下游任务奠定了基础，例如创建玩家预测机器人行为或玩家异常检测。

Abstract: Understanding and predicting player movement in multiplayer games is crucial
for achieving use cases such as player-mimicking bot navigation, preemptive bot
control, strategy recommendation, and real-time player behavior analytics.
However, the complex environments allow for a high degree of navigational
freedom, and the interactions and team-play between players require models that
make effective use of the available heterogeneous input data. This paper
presents a multimodal architecture for predicting future player locations on a
dynamic time horizon, using a U-Net-based approach for calculating endpoint
location probability heatmaps, conditioned using a multimodal feature encoder.
The application of a multi-head attention mechanism for different groups of
features allows for communication between agents. In doing so, the architecture
makes efficient use of the multimodal game state including image inputs,
numerical and categorical features, as well as dynamic game data. Consequently,
the presented technique lays the foundation for various downstream tasks that
rely on future player positions such as the creation of player-predictive bot
behavior or player anomaly detection.

</details>


### [132] [Lightweight Transformer-Driven Segmentation of Hotspots and Snail Trails in Solar PV Thermal Imagery](https://arxiv.org/abs/2507.20680)
*Deepak Joshi,Mayukha Pal*

Main category: cs.CV

TL;DR: 提出了一种基于SegFormer的无人机热红外图像光伏缺陷检测方法，精度高、效率高，适用于自动化巡检。


<details>
  <summary>Details</summary>
Motivation: 为了在光伏组件中准确检测热点和蜗牛纹等缺陷，以维持能源效率和系统可靠性。

Method: 采用一种监督式深度学习框架，基于SegFormer模型，通过自定义的Transformer编码器和简化的解码器，对包含手动标注缺陷区域的热红外图像数据集进行训练和微调。预处理流程包括图像 resizing、CLAHE对比度增强、去噪和归一化。

Result: 所提出的SegFormer模型在准确性和效率方面优于U-Net、DeepLabV3、PSPNet和Mask2Former等基线模型，尤其在分割小型和不规则缺陷方面表现突出。

Conclusion: 所提出的基于SegFormer的轻量级语义分割模型在光伏组件热红外图像缺陷分割任务上表现优于其他基线模型，尤其在小且不规则缺陷的分割上，具有高精度和高效率的优势，能够满足在边缘设备上进行实时部署和与无人机系统集成以实现自动化巡检的需求。

Abstract: Accurate detection of defects such as hotspots and snail trails in
photovoltaic modules is essential for maintaining energy efficiency and system
reliablility. This work presents a supervised deep learning framework for
segmenting thermal infrared images of PV panels, using a dataset of 277 aerial
thermographic images captured by zenmuse XT infrared camera mounted on a DJI
Matrice 100 drone. The preprocessing pipeline includes image resizing, CLAHE
based contrast enhancement, denoising, and normalisation. A lightweight
semantic segmentation model based on SegFormer is developed, featuring a
customised Transformwer encoder and streamlined decoder, and fine-tuned on
annotated images with manually labeled defect regions. To evaluate performance,
we benchmark our model against U-Net, DeepLabV3, PSPNet, and Mask2Former using
consistent preprocessing and augmentation. Evaluation metrices includes
per-class Dice score, F1-score, Cohen's kappa, mean IoU, and pixel accuracy.
The SegFormer-based model outperforms baselines in accuracy and efficiency,
particularly for segmenting small and irregular defects. Its lightweight design
real-time deployment on edge devices and seamless integration with drone-based
systems for automated inspection of large-scale solar farms.

</details>


### [133] [Automatic camera orientation estimation for a partially calibrated camera above a plane with a line at known planar distance](https://arxiv.org/abs/2507.20689)
*Gergely Dinya,Anna Gelencsér-Horváth*

Main category: cs.CV

TL;DR: 通过检测单直线估计相机滚动和俯仰角度。


<details>
  <summary>Details</summary>
Motivation: 提出一种使用最少场景信息来估计安装在平面上方之部分校准摄像机的滚动和俯仰方向的推导。

Method: 通过检测已知平面距离的单条直线（例如地板和墙壁之间的边缘），利用逆投影几何来估计滚动和俯仰角度。该方法利用了几何约束和相机模型，包括镜头畸变校正。

Result: 估计部分校准的相机在平面上方的滚动和俯仰方向。

Conclusion: 该方法适用于无法进行完整标定的场景，并且为在受限环境中运行的多摄像头系统提供了一种轻量级的替代方案。

Abstract: We present a derivation for estimating the roll and pitch orientation of a
partially calibrated camera mounted above a planar surface, using minimal scene
information. Specifically, we assume known intrinsic parameters and a fixed
height between the camera and the observed plane. By detecting a single
straight reference line at a known planar distance -- such as the edge between
a floor and a wall -- we estimate the roll and pitch angles via inverse
projection geometry. The method leverages geometric constraints and the camera
model, including lens distortion correction. This approach is suitable for
scenarios where full calibration is impractical and offers a lightweight
alternative for multi-camera systems operating in constrained environments.

</details>


### [134] [AIComposer: Any Style and Content Image Composition via Feature Integration](https://arxiv.org/abs/2507.20721)
*Haowen Li,Zhenfeng Fan,Zhang Wen,Zhengzhou Zhu,Yunjin Li*

Main category: cs.CV

TL;DR: 本文提出了一种无需文本提示的跨域图像组合方法，通过集成 CLIP 特征和局部交叉注意力，实现了高效、鲁棒且自然的图像风格化和无缝组合，并在评估中显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有图像组合技术在同一域内取得了显著进展，但跨域组合仍未得到充分探索。主要挑战在于扩散模型的随机性以及输入图像间的风格差异，这会导致组合失败和伪影。此外，对文本提示的过度依赖限制了其实际应用。

Method: 本文提出了首个无需文本提示的跨域图像组合方法，通过简单的多层感知器网络集成前景和背景的 CLIP 特征，并采用局部交叉注意力策略来操纵扩散过程。该方法在向后反演和向前去噪过程中仅涉及少量步骤，有效保留了前景内容，实现了稳定的风格化，且无需预风格化网络。

Result: 该方法有效保留了前景内容，实现了稳定的风格化，且无需预风格化网络。在定性和定量评估中均优于最先进的技术，LPIPS 分数显著提高了 30.5%，CSD 指标提高了 18.1%。

Conclusion: 该方法在定性和定量评估中均优于最先进的技术，LPIPS 分数显著提高了 30.5%，CSD 指标提高了 18.1%。我们相信我们的方法将推动未来的研究和应用。

Abstract: Image composition has advanced significantly with large-scale pre-trained T2I
diffusion models. Despite progress in same-domain composition, cross-domain
composition remains under-explored. The main challenges are the stochastic
nature of diffusion models and the style gap between input images, leading to
failures and artifacts. Additionally, heavy reliance on text prompts limits
practical applications. This paper presents the first cross-domain image
composition method that does not require text prompts, allowing natural
stylization and seamless compositions. Our method is efficient and robust,
preserving the diffusion prior, as it involves minor steps for backward
inversion and forward denoising without training the diffuser. Our method also
uses a simple multilayer perceptron network to integrate CLIP features from
foreground and background, manipulating diffusion with a local cross-attention
strategy. It effectively preserves foreground content while enabling stable
stylization without a pre-stylization network. Finally, we create a benchmark
dataset with diverse contents and styles for fair evaluation, addressing the
lack of testing datasets for cross-domain image composition. Our method
outperforms state-of-the-art techniques in both qualitative and quantitative
evaluations, significantly improving the LPIPS score by 30.5% and the CSD
metric by 18.1%. We believe our method will advance future research and
applications. Code and benchmark at https://github.com/sherlhw/AIComposer.

</details>


### [135] [Style-Aware Blending and Prototype-Based Cross-Contrast Consistency for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2507.20729)
*Chaowei Chen,Xiang Zhang,Honglie Guo,Shunfang Wang*

Main category: cs.CV

TL;DR: 针对半监督医学图像分割中数据流独立和监督信息利用不充分的问题，提出风格感知融合和基于原型的交叉对比一致性学习框架，通过数据融合和强弱一致性利用提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有半监督医学图像分割方法主要关注扰动方案，忽略了框架本身的局限性。具体包括：1. 独立的数据流导致确认偏差；2. 对强到弱一致性利用不充分。

Method: 提出了一种风格感知融合和基于原型的交叉对比一致性学习框架，通过风格引导的分布融合模块解决数据流独立性问题，并通过基于原型的交叉对比策略利用强到弱的一致性，同时减轻噪声影响。

Result: 实验结果证明了所提出框架在多个医学分割基准和不同半监督设置下的有效性和优越性。

Conclusion: 本研究提出的框架在多种半监督医学图像分割基准和设置下均有效且表现优越。

Abstract: Weak-strong consistency learning strategies are widely employed in
semi-supervised medical image segmentation to train models by leveraging
limited labeled data and enforcing weak-to-strong consistency. However,
existing methods primarily focus on designing and combining various
perturbation schemes, overlooking the inherent potential and limitations within
the framework itself. In this paper, we first identify two critical
deficiencies: (1) separated training data streams, which lead to confirmation
bias dominated by the labeled stream; and (2) incomplete utilization of
supervisory information, which limits exploration of strong-to-weak
consistency. To tackle these challenges, we propose a style-aware blending and
prototype-based cross-contrast consistency learning framework. Specifically,
inspired by the empirical observation that the distribution mismatch between
labeled and unlabeled data can be characterized by statistical moments, we
design a style-guided distribution blending module to break the independent
training data streams. Meanwhile, considering the potential noise in strong
pseudo-labels, we introduce a prototype-based cross-contrast strategy to
encourage the model to learn informative supervisory signals from both
weak-to-strong and strong-to-weak predictions, while mitigating the adverse
effects of noise. Experimental results demonstrate the effectiveness and
superiority of our framework across multiple medical segmentation benchmarks
under various semi-supervised settings.

</details>


### [136] [Multi-Masked Querying Network for Robust Emotion Recognition from Incomplete Multi-Modal Physiological Signals](https://arxiv.org/abs/2507.20737)
*Geng-Xin Xu,Xiang Zuo,Ye Li*

Main category: cs.CV

TL;DR: MMQ-Net通过结合模态查询、类别查询和干扰查询来解决生理信号情绪识别中的信号不完整和噪声干扰问题，并在实验中表现出优越的性能。


<details>
  <summary>Details</summary>
Motivation: 情绪识别中的多模态信号不完整、身体运动和伪影干扰是关键挑战。

Method: 提出了一种新颖的多掩码查询网络（MMQ-Net），该网络将多种查询机制整合到一个统一的框架中，包括用于从不完整信号重建缺失数据的模态查询、用于关注情绪状态特征的类别查询以及用于从噪声中分离相关信息的干扰查询。

Result: 实验结果表明，MMQ-Net在与现有方法相比，在情绪识别方面表现更优越，尤其是在处理高不完整性数据时。

Conclusion: MMQ-Net在处理不完整多模态信号和身体运动干扰方面表现出优越的情感识别性能，特别是在数据不完整性较高的情况下，优于现有方法。

Abstract: Emotion recognition from physiological data is crucial for mental health
assessment, yet it faces two significant challenges: incomplete multi-modal
signals and interference from body movements and artifacts. This paper presents
a novel Multi-Masked Querying Network (MMQ-Net) to address these issues by
integrating multiple querying mechanisms into a unified framework.
Specifically, it uses modality queries to reconstruct missing data from
incomplete signals, category queries to focus on emotional state features, and
interference queries to separate relevant information from noise. Extensive
experiment results demonstrate the superior emotion recognition performance of
MMQ-Net compared to existing approaches, particularly under high levels of data
incompleteness.

</details>


### [137] [Implicit Counterfactual Learning for Audio-Visual Segmentation](https://arxiv.org/abs/2507.20740)
*Mingfeng Zha,Tianyu Li,Guoqing Wang,Peng Wang,Yangyang Wu,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: 提出ICF框架，利用MIT和SC克服AVS中的模态不平衡问题，并通过CDCL学习对齐表示，实验证明效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉分割（AVS）方法在解决模态表示差异和不平衡方面关注不足，可能导致在复杂场景下因异构表示的语义缺乏而产生错误的匹配。

Method: 提出了一种名为隐式反事实框架（ICF）的新方法，该方法包括多粒度隐式文本（MIT）来建立跨模态共享空间，以及语义反事实（SC）来学习正交表示以避免模态偏见。此外，还提出了协同分布感知对比学习（CDCL）来对齐表示，并结合了事实-反事实和跨模态对比。

Result: 该方法在三个公共数据集上进行了广泛实验，验证了其达到了最先进的性能。

Conclusion: 所提出的隐式反事实框架（ICF）通过引入多粒度隐式文本（MIT）和语义反事实（SC）以及协同分布感知对比学习（CDCL），有效解决了现有音频-视觉分割（AVS）方法在处理跨模态表示差异和不平衡问题上的不足，并在三个公共数据集上取得了最先进的性能。

Abstract: Audio-visual segmentation (AVS) aims to segment objects in videos based on
audio cues. Existing AVS methods are primarily designed to enhance interaction
efficiency but pay limited attention to modality representation discrepancies
and imbalances. To overcome this, we propose the implicit counterfactual
framework (ICF) to achieve unbiased cross-modal understanding. Due to the lack
of semantics, heterogeneous representations may lead to erroneous matches,
especially in complex scenes with ambiguous visual content or interference from
multiple audio sources. We introduce the multi-granularity implicit text (MIT)
involving video-, segment- and frame-level as the bridge to establish the
modality-shared space, reducing modality gaps and providing prior guidance.
Visual content carries more information and typically dominates, thereby
marginalizing audio features in the decision-making. To mitigate knowledge
preference, we propose the semantic counterfactual (SC) to learn orthogonal
representations in the latent space, generating diverse counterfactual samples,
thus avoiding biases introduced by complex functional designs and explicit
modifications of text structures or attributes. We further formulate the
collaborative distribution-aware contrastive learning (CDCL), incorporating
factual-counterfactual and inter-modality contrasts to align representations,
promoting cohesion and decoupling. Extensive experiments on three public
datasets validate that the proposed method achieves state-of-the-art
performance.

</details>


### [138] [Regularizing Subspace Redundancy of Low-Rank Adaptation](https://arxiv.org/abs/2507.20745)
*Yue Zhu,Haiwen Diao,Shang Gao,Jiazuo Yu,Jiawen Zhu,Yunzhi Zhuge,Shuai Hao,Xu Jia,Lu Zhang,Ying Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: ReSoRA 通过正则化低秩适应的子空间冗余来提高 PETL 的效率，通过分解和约束子空间来减少冗余，并在各种任务和模型中得到验证。


<details>
  <summary>Details</summary>
Motivation: 现有的 LoRA 及其变体在 PETL 中表现出强大的能力，但其投影矩阵在训练期间不受限制，导致表示冗余度高，并削弱了结果子空间中特征适应的有效性。现有方法通过手动调整秩或隐式应用通道掩码来缓解此问题，但它们缺乏灵活性，并且在不同数据集和体系结构上的泛化能力较差。

Method: ReSoRA 通过理论分解低秩子矩阵为多个等效子空间，并系统地对不同投影的特征分布应用去冗余约束。

Result: 实验证明，ReSoRA 可以在各种骨干网络和数据集的视觉语言检索和标准视觉分类基准测试中，持续促进现有的最先进的 PETL 方法。

Conclusion: ReSoRA 是一种即插即用、无额外推理成本的训练监督方法，可无缝集成到现有的 PETL 方法中，并在各种骨干网络和数据集的视觉语言检索和标准视觉分类基准测试中，持续促进现有的最先进的 PETL 方法。

Abstract: Low-Rank Adaptation (LoRA) and its variants have delivered strong capability
in Parameter-Efficient Transfer Learning (PETL) by minimizing trainable
parameters and benefiting from reparameterization. However, their projection
matrices remain unrestricted during training, causing high representation
redundancy and diminishing the effectiveness of feature adaptation in the
resulting subspaces. While existing methods mitigate this by manually adjusting
the rank or implicitly applying channel-wise masks, they lack flexibility and
generalize poorly across various datasets and architectures. Hence, we propose
ReSoRA, a method that explicitly models redundancy between mapping subspaces
and adaptively Regularizes Subspace redundancy of Low-Rank Adaptation.
Specifically, it theoretically decomposes the low-rank submatrices into
multiple equivalent subspaces and systematically applies de-redundancy
constraints to the feature distributions across different projections.
Extensive experiments validate that our proposed method consistently
facilitates existing state-of-the-art PETL methods across various backbones and
datasets in vision-language retrieval and standard visual classification
benchmarks. Besides, as a training supervision, ReSoRA can be seamlessly
integrated into existing approaches in a plug-and-play manner, with no
additional inference costs. Code is publicly available at:
https://github.com/Lucenova/ReSoRA.

</details>


### [139] [Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry](https://arxiv.org/abs/2507.20757)
*Matan Kichler,Shai Bagon,Mark Sheinin*

Main category: cs.CV

TL;DR: 本研究提出了一种创新的计算机视觉方法，利用容器表面的微小振动来推断其内部液位。该方法通过散斑传感和Transformer模型实现，能够远程、非接触式地检测多个容器的液位，并对容器类型进行分类，且不受振动源和容器实例变化的影响。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉系统通常仅限于从场景对象的可见表面提取信息。本研究旨在通过传感其表面的微小振动来扩展计算机视觉的范围，以推断不透明容器隐藏的液体液位。

Method: 提出了一种新颖的基于散斑的振动传感系统，用于同时捕获二维点网格上的场景振动。开发了一种基于Transformer的方法来分析捕获的振动，并对容器类型及其在测量时的隐藏液位进行分类。

Result: 该方法能够远程、一次性地检查多个密封容器的填充水平，而无需进行物理操作和手动称量。提出的模型可以推广到已知类别中未见过的容器实例和流体液位。此外，该模型对振动源具有不变性，能够为受控和环境场景声源提供正确的液位估计。

Conclusion: 该方法能够从各种日常容器中恢复液位，并且能够识别容器类型和隐藏的液位。

Abstract: Computer vision seeks to infer a wide range of information about objects and
events. However, vision systems based on conventional imaging are limited to
extracting information only from the visible surfaces of scene objects. For
instance, a vision system can detect and identify a Coke can in the scene, but
it cannot determine whether the can is full or empty. In this paper, we aim to
expand the scope of computer vision to include the novel task of inferring the
hidden liquid levels of opaque containers by sensing the tiny vibrations on
their surfaces. Our method provides a first-of-a-kind way to inspect the fill
level of multiple sealed containers remotely, at once, without needing physical
manipulation and manual weighing. First, we propose a novel speckle-based
vibration sensing system for simultaneously capturing scene vibrations on a 2D
grid of points. We use our system to efficiently and remotely capture a dataset
of vibration responses for a variety of everyday liquid containers. Then, we
develop a transformer-based approach for analyzing the captured vibrations and
classifying the container type and its hidden liquid level at the time of
measurement. Our architecture is invariant to the vibration source, yielding
correct liquid level estimates for controlled and ambient scene sound sources.
Moreover, our model generalizes to unseen container instances within known
classes (e.g., training on five Coke cans of a six-pack, testing on a sixth)
and fluid levels. We demonstrate our method by recovering liquid levels from
various everyday containers.

</details>


### [140] [KASportsFormer: Kinematic Anatomy Enhanced Transformer for 3D Human Pose Estimation on Short Sports Scene Video](https://arxiv.org/abs/2507.20763)
*Zhuoer Yin,Calvin Yeung,Tomohiro Suzuki,Ryota Tanaka,Keisuke Fujii*

Main category: cs.CV

TL;DR: KASportsFormer 是一种新的 transformer 模型，通过结合运动学信息来改进体育场景中的 3D 姿势估计，解决了现有方法的局限性，并在两个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于 transformer 的方法在体育场景中表现不佳，因为运动模糊、遮挡和域转移等问题，并且难以处理瞬时场景。

Method: 提出了一种名为 KASportsFormer 的新颖的、基于 transformer 的 3D 姿势估计框架，该框架包含一个运动学解剖学信息特征表示和集成模块。该模块通过 Bone Extractor (BoneExt) 和 Limb Fuser (LimbFus) 模块以多模态方式提取并编码固有的运动学运动信息，以提高对短视频中运动姿势的理解能力。

Result: KASportsFormer 在 SportsPose 和 WorldPose 数据集上取得了最先进的结果，MPJPE 误差分别为 58.0mm 和 34.3mm。

Conclusion: KASportsFormer 在 SportsPose 和 WorldPose 数据集上取得了最先进的结果，MPJPE 误差分别为 58.0mm 和 34.3mm。

Abstract: Recent transformer based approaches have demonstrated impressive performance
in solving real-world 3D human pose estimation problems. Albeit these
approaches achieve fruitful results on benchmark datasets, they tend to fall
short of sports scenarios where human movements are more complicated than daily
life actions, as being hindered by motion blur, occlusions, and domain shifts.
Moreover, due to the fact that critical motions in a sports game often finish
in moments of time (e.g., shooting), the ability to focus on momentary actions
is becoming a crucial factor in sports analysis, where current methods appear
to struggle with instantaneous scenarios. To overcome these limitations, we
introduce KASportsFormer, a novel transformer based 3D pose estimation
framework for sports that incorporates a kinematic anatomy-informed feature
representation and integration module. In which the inherent kinematic motion
information is extracted with the Bone Extractor (BoneExt) and Limb Fuser
(LimbFus) modules and encoded in a multimodal manner. This improved the
capability of comprehending sports poses in short videos. We evaluate our
method through two representative sports scene datasets: SportsPose and
WorldPose. Experimental results show that our proposed method achieves
state-of-the-art results with MPJPE errors of 58.0mm and 34.3mm, respectively.
Our code and models are available at: https://github.com/jw0r1n/KASportsFormer

</details>


### [141] [ATR-UMMIM: A Benchmark Dataset for UAV-Based Multimodal Image Registration under Complex Imaging Conditions](https://arxiv.org/abs/2507.20764)
*Kangcheng Bin,Chen Chen,Ting Hu,Jiahao Qi,Ping Zhong*

Main category: cs.CV

TL;DR: ATR-UMMIM是首个无人机多模态图像配准基准数据集，包含大量多样化的图像和标注，旨在促进该领域的研究。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏专门针对无人机多模态配准的公开基准数据集，阻碍了该领域的发展和评估，因此提出ATR-UMMIM数据集以解决这一差距。

Method: 本研究提出了ATR-UMMIM数据集，这是一个包含7,969个图像三元组（可见光、红外光、精确配准的可见光）的数据集，并辅以像素级地面真值和对象级标注（11个类别），以及六个成像条件属性。

Result: ATR-UMMIM数据集为无人机多模态配准提供了宝贵的资源，包含多样化的场景和详细的标注，能够支持对配准方法鲁棒性的评估，并为下游任务提供对象级信息。

Conclusion: ATR-UMMIM是首个专为无人机多模态图像配准设计的基准数据集，包含7,969个可见光、红外光和精确配准的可见光图像三元组，涵盖了从80米到300米的飞行高度、0到75度的相机角度以及全天候、全年的时间变化。数据集通过半自动标注流程提供像素级地面真值，并包含六个成像条件属性和对象级标注（11个类别，77,753个可见光和78,409个红外光边界框），旨在推动无人机多模态配准、融合和感知技术的发展。

Abstract: Multimodal fusion has become a key enabler for UAV-based object detection, as
each modality provides complementary cues for robust feature extraction.
However, due to significant differences in resolution, field of view, and
sensing characteristics across modalities, accurate registration is a
prerequisite before fusion. Despite its importance, there is currently no
publicly available benchmark specifically designed for multimodal registration
in UAV-based aerial scenarios, which severely limits the development and
evaluation of advanced registration methods under real-world conditions. To
bridge this gap, we present ATR-UMMIM, the first benchmark dataset specifically
tailored for multimodal image registration in UAV-based applications. This
dataset includes 7,969 triplets of raw visible, infrared, and precisely
registered visible images captured covers diverse scenarios including flight
altitudes from 80m to 300m, camera angles from 0{\deg} to 75{\deg}, and
all-day, all-year temporal variations under rich weather and illumination
conditions. To ensure high registration quality, we design a semi-automated
annotation pipeline to introduce reliable pixel-level ground truth to each
triplet. In addition, each triplet is annotated with six imaging condition
attributes, enabling benchmarking of registration robustness under real-world
deployment settings. To further support downstream tasks, we provide
object-level annotations on all registered images, covering 11 object
categories with 77,753 visible and 78,409 infrared bounding boxes. We believe
ATR-UMMIM will serve as a foundational benchmark for advancing multimodal
registration, fusion, and perception in real-world UAV scenarios. The datatset
can be download from https://github.com/supercpy/ATR-UMMIM

</details>


### [142] [Learning Only with Images: Visual Reinforcement Learning with Reasoning, Rendering, and Visual Feedback](https://arxiv.org/abs/2507.20766)
*Yang Chen,Yufan Shen,Wenxuan Huang,Shen Zhou,Qunshu Lin,Xinyu Cai,Zhi Yu,Botian Shi,Yu Qiao*

Main category: cs.CV

TL;DR: RRVF框架通过“推理-渲染-视觉反馈”的闭环过程和“验证的不对称性”原理，实现了仅用原始图像训练MLLMs进行视觉推理，减少了对图像-文本监督的依赖，并在图像到代码生成任务上取得了优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在视觉推理方面虽然表现出色，但过度依赖于精心标注的图像-文本监督数据，这限制了其进一步发展。为了克服这一瓶颈，本研究旨在探索一种无需图像-文本监督，仅利用原始图像进行视觉推理训练的新方法。

Method: 本研究引入了一种名为“Reasoning-Rendering-Visual-Feedback”（RRVF）的新框架，该框架基于“验证的不对称性”原理。该原理指出，与直接生成相比，根据源图像验证渲染输出更为容易。RRVF利用这一原理，通过强化学习（RL）优化MLLMs，将视觉推理过程设计为一个包含推理、渲染和视觉反馈的闭环迭代过程。该框架能够通过多轮交互和工具调用进行自我校正，并通过GRPO算法进行端到端优化。

Result: 在图像到代码生成（包括数据图表和Web界面）的任务上，RRVF框架展现出显著的性能提升，不仅大幅超越了现有的开源MLLMs，而且优于监督微调的基线模型。研究结果表明，仅依靠视觉反馈驱动的系统是实现更强大、更通用推理模型的可行途径，无需显式监督。

Conclusion: 本研究提出的RRVF框架，仅通过原始图像和视觉反馈，即可训练多模态大语言模型（MLLMs）进行复杂的视觉推理，显著减少了对图像-文本监督的依赖。实验证明，该框架在图像到代码生成等任务上表现优于现有模型和监督微调方法，展示了纯视觉反馈驱动的自校正学习范式在构建鲁棒且可泛化的推理模型方面的潜力。

Abstract: Multimodal Large Language Models (MLLMs) have exhibited impressive
performance across various visual tasks. Subsequent investigations into
enhancing their visual reasoning abilities have significantly expanded their
performance envelope. However, a critical bottleneck in the advancement of
MLLMs toward deep visual reasoning is their heavy reliance on curated
image-text supervision. To solve this problem, we introduce a novel framework
termed ``Reasoning-Rendering-Visual-Feedback'' (RRVF), which enables MLLMs to
learn complex visual reasoning from only raw images. This framework builds on
the ``Asymmetry of Verification'' principle to train MLLMs, i.e., verifying the
rendered output against a source image is easier than generating it. We
demonstrate that this relative ease provides an ideal reward signal for
optimization via Reinforcement Learning (RL) training, reducing the reliance on
the image-text supervision. Guided by the above principle, RRVF implements a
closed-loop iterative process encompassing reasoning, rendering, and visual
feedback components, enabling the model to perform self-correction through
multi-turn interactions and tool invocation, while this pipeline can be
optimized by the GRPO algorithm in an end-to-end manner. Extensive experiments
on image-to-code generation for data charts and web interfaces show that RRVF
substantially outperforms existing open-source MLLMs and surpasses supervised
fine-tuning baselines. Our findings demonstrate that systems driven by purely
visual feedback present a viable path toward more robust and generalizable
reasoning models without requiring explicit supervision. Code will be available
at https://github.com/L-O-I/RRVF.

</details>


### [143] [RingMo-Agent: A Unified Remote Sensing Foundation Model for Multi-Platform and Multi-Modal Reasoning](https://arxiv.org/abs/2507.20776)
*Huiyang Hu,Peijin Wang,Yingchao Feng,Kaiwen Wei,Wenxin Yin,Wenhui Diao,Mengyu Wang,Hanbo Bi,Kaiyue Kang,Tong Ling,Kun Fu,Xian Sun*

Main category: cs.CV

TL;DR: RingMo-Agent 是一个为处理多模态、多平台遥感数据而设计的模型，能够根据用户文本指令执行感知和推理任务。它使用大规模数据集（RS-VL3M），学习模态自适应表示，并统一任务建模，在各种遥感视觉语言任务中表现出色并具有良好的泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有遥感（RS）视觉语言研究主要依赖相对同质的数据源，并且局限于分类或字幕等传统视觉感知任务。这导致这些方法无法成为一个统一且独立的框架，无法在实际应用中有效处理来自不同来源的 RS 图像。

Method: RingMo-Agent 模型通过以下方式解决现有方法的局限性：1. 支持大规模遥感视觉语言数据集（RS-VL3M），包含超过300万对图像-文本，涵盖光、SAR、红外模态，以及卫星和无人机平台，并涉及感知和推理任务。2. 通过引入分离的嵌入层来学习模态自适应表示，为异构模态构建隔离特征，减少跨模态干扰。3. 通过引入特定任务的 token 和基于 token 的高维隐藏状态解码机制来统一任务建模，以适应长时空任务。

Result: 实验证明，RingMo-Agent 不仅在视觉理解和复杂的分析任务中有效，而且在跨不同平台和传感模态方面表现出强大的泛化能力。

Conclusion: RingMo-Agent 在各种遥感视觉语言任务上证明了其有效性，不仅在视觉理解和复杂的分析任务中表现出色，而且在不同平台和传感模态之间展现出强大的泛化能力。

Abstract: Remote sensing (RS) images from multiple modalities and platforms exhibit
diverse details due to differences in sensor characteristics and imaging
perspectives. Existing vision-language research in RS largely relies on
relatively homogeneous data sources. Moreover, they still remain limited to
conventional visual perception tasks such as classification or captioning. As a
result, these methods fail to serve as a unified and standalone framework
capable of effectively handling RS imagery from diverse sources in real-world
applications. To address these issues, we propose RingMo-Agent, a model
designed to handle multi-modal and multi-platform data that performs perception
and reasoning tasks based on user textual instructions. Compared with existing
models, RingMo-Agent 1) is supported by a large-scale vision-language dataset
named RS-VL3M, comprising over 3 million image-text pairs, spanning optical,
SAR, and infrared (IR) modalities collected from both satellite and UAV
platforms, covering perception and challenging reasoning tasks; 2) learns
modality adaptive representations by incorporating separated embedding layers
to construct isolated features for heterogeneous modalities and reduce
cross-modal interference; 3) unifies task modeling by introducing task-specific
tokens and employing a token-based high-dimensional hidden state decoding
mechanism designed for long-horizon spatial tasks. Extensive experiments on
various RS vision-language tasks demonstrate that RingMo-Agent not only proves
effective in both visual understanding and sophisticated analytical tasks, but
also exhibits strong generalizability across different platforms and sensing
modalities.

</details>


### [144] [Investigation of Accuracy and Bias in Face Recognition Trained with Synthetic Data](https://arxiv.org/abs/2507.20782)
*Pavel Korshunov,Ketan Kotwal,Christophe Ecabert,Vidit Vidit,Amir Mohammadi,Sebastien Marcel*

Main category: cs.CV

TL;DR: 使用 SD35 等生成器和增强技术生成的平衡合成人脸数据集，在人脸识别的公平性方面表现出潜力，但泛化能力仍需提高。


<details>
  <summary>Details</summary>
Motivation: 合成数据作为训练人脸识别（FR）模型的替代方案，在可扩展性、隐私合规性和减少偏见方面具有优势。然而，使用合成数据能否同时实现高准确性和公平性仍是关键问题。

Method: 本研究评估了合成数据对人脸识别（FR）系统偏见和性能的影响。研究人员使用两种最先进的文本到图像生成器（Flux.1-dev 和 Stable Diffusion v3.5 (SD35)）生成了一个平衡的人脸数据集 FairFaceGen，并结合了 Arc2Face 和四种 IP-Adapters 等身份增强方法。通过在合成数据和真实数据集中保持相同的身份数量，确保了在 LFW、AgeDB-30 等标准基准和 IJB-B/C 挑战性基准上进行 FR 性能评估以及在 RFW 数据集上进行 FR 偏见评估时的公平性。

Result: 研究结果表明，尽管合成数据在 IJB-B/C 上的泛化能力仍不及真实数据集，但人口统计学上平衡的合成数据集（特别是使用 SD35 生成的数据集）在减轻偏见方面具有潜力。同时，类内增强的数量和质量对 FR 的准确性和公平性有着显著影响。

Conclusion: 与真实数据集相比，合成数据在 IJB-B/C 上的泛化能力仍有差距，但人口统计学上平衡的合成数据集（特别是使用 SD35 生成的数据集）在减轻偏见方面显示出潜力。此外，类内增强的数量和质量显著影响人脸识别准确性和公平性。这些发现为使用合成数据构建更公平的人脸识别系统提供了实用的指导。

Abstract: Synthetic data has emerged as a promising alternative for training face
recognition (FR) models, offering advantages in scalability, privacy
compliance, and potential for bias mitigation. However, critical questions
remain on whether both high accuracy and fairness can be achieved with
synthetic data. In this work, we evaluate the impact of synthetic data on bias
and performance of FR systems. We generate balanced face dataset, FairFaceGen,
using two state of the art text-to-image generators, Flux.1-dev and Stable
Diffusion v3.5 (SD35), and combine them with several identity augmentation
methods, including Arc2Face and four IP-Adapters. By maintaining equal identity
count across synthetic and real datasets, we ensure fair comparisons when
evaluating FR performance on standard (LFW, AgeDB-30, etc.) and challenging
IJB-B/C benchmarks and FR bias on Racial Faces in-the-Wild (RFW) dataset. Our
results demonstrate that although synthetic data still lags behind the real
datasets in the generalization on IJB-B/C, demographically balanced synthetic
datasets, especially those generated with SD35, show potential for bias
mitigation. We also observe that the number and quality of intra-class
augmentations significantly affect FR accuracy and fairness. These findings
provide practical guidelines for constructing fairer FR systems using synthetic
data.

</details>


### [145] [An Efficient Machine Learning Framework for Forest Height Estimation from Multi-Polarimetric Multi-Baseline SAR data](https://arxiv.org/abs/2507.20798)
*Francesca Razzano,Wenyu Yang,Sergio Vitale,Giampaolo Ferraioli,Silvia Liberata Ullo,Gilda Schirinzi*

Main category: cs.CV

TL;DR: FGump是一个基于梯度提升的SAR森林高度估算框架，使用LiDAR数据作为地面真实性。与需要大量数据和复杂模型的ML/DL方法不同，FGump在准确性和效率之间取得了良好平衡，且训练和推理速度更快，精度更高。


<details>
  <summary>Details</summary>
Motivation: 准确估算森林高度对于气候变化监测和碳循环评估至关重要。虽然SAR（特别是多通道配置）在3D森林结构重建中发挥着作用，但ML和DL方法带来了新的机遇。FGump旨在提供一种更高效、准确的方法。

Method: 提出了一种名为FGump的森林高度估算框架，该框架利用梯度提升、多通道SAR处理，并以LiDAR剖面作为地面真实性(GT)。FGump通过使用一组有限的、手工设计的特征并避免复杂的预处理（如校准和/或量化），在准确性和计算效率之间取得了良好的平衡。

Result: FGump框架在分类和回归范式下均进行了评估。结果表明，回归形式能够实现细粒度的、连续的估算，并通过更精确的测量避免了量化伪影。实验结果证实FGump的性能优于当前最先进的AI和传统方法。

Conclusion: FGump框架在森林高度估算方面表现出色，优于当前最先进的AI和传统方法，同时具有更高的准确性和更低的训练/推理时间。

Abstract: Accurate forest height estimation is crucial for climate change monitoring
and carbon cycle assessment. Synthetic Aperture Radar (SAR), particularly in
multi-channel configurations, has provided support for a long time in 3D forest
structure reconstruction through model-based techniques. More recently,
data-driven approaches using Machine Learning (ML) and Deep Learning (DL) have
enabled new opportunities for forest parameter retrieval. This paper introduces
FGump, a forest height estimation framework by gradient boosting using
multi-channel SAR processing with LiDAR profiles as Ground Truth(GT). Unlike
typical ML and DL approaches that require large datasets and complex
architectures, FGump ensures a strong balance between accuracy and
computational efficiency, using a limited set of hand-designed features and
avoiding heavy preprocessing (e.g., calibration and/or quantization). Evaluated
under both classification and regression paradigms, the proposed framework
demonstrates that the regression formulation enables fine-grained, continuous
estimations and avoids quantization artifacts by resulting in more precise
measurements without rounding. Experimental results confirm that FGump
outperforms State-of-the-Art (SOTA) AI-based and classical methods, achieving
higher accuracy and significantly lower training and inference times, as
demonstrated in our results.

</details>


### [146] [FantasyID: A dataset for detecting digital manipulations of ID-documents](https://arxiv.org/abs/2507.20808)
*Pavel Korshunov,Amir Mohammadi,Vidit Vidit,Christophe Ecabert,Sébastien Marcel*

Main category: cs.CV

TL;DR: 新数据集FantasyID用于评估身份证伪造检测，现有算法在该数据集上面临挑战。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的进步，伪造身份证件对“了解你的客户”（KYC）应用构成了严重威胁，因此需要开发强大的伪造检测系统。

Method: 提出了一种名为FantasyID的新型公开数据集，该数据集包含模仿真实世界身份证的伪造身份证，并模拟了打印和拍摄过程以及数字伪造攻击。

Result: FantasyID数据集对现有的先进伪造检测算法（如TruFor、MMFusion、UniFD和FatFormer）提出了挑战，在接近实际的评估条件下，假阴性率接近50%，证明了其作为评估基准的有效性。

Conclusion: 该研究提出了FantasyID数据集，可用于评估身份伪造检测算法，并指出当前最先进的算法在该数据集上面临挑战，尤其是在接近实际的应用场景下。

Abstract: Advancements in image generation led to the availability of easy-to-use tools
for malicious actors to create forged images. These tools pose a serious threat
to the widespread Know Your Customer (KYC) applications, requiring robust
systems for detection of the forged Identity Documents (IDs). To facilitate the
development of the detection algorithms, in this paper, we propose a novel
publicly available (including commercial use) dataset, FantasyID, which mimics
real-world IDs but without tampering with legal documents and, compared to
previous public datasets, it does not contain generated faces or specimen
watermarks. FantasyID contains ID cards with diverse design styles, languages,
and faces of real people. To simulate a realistic KYC scenario, the cards from
FantasyID were printed and captured with three different devices, constituting
the bonafide class. We have emulated digital forgery/injection attacks that
could be performed by a malicious actor to tamper the IDs using the existing
generative tools. The current state-of-the-art forgery detection algorithms,
such as TruFor, MMFusion, UniFD, and FatFormer, are challenged by FantasyID
dataset. It especially evident, in the evaluation conditions close to
practical, with the operational threshold set on validation set so that false
positive rate is at 10%, leading to false negative rates close to 50% across
the board on the test set. The evaluation experiments demonstrate that
FantasyID dataset is complex enough to be used as an evaluation benchmark for
detection algorithms.

</details>


### [147] [SCANet: Split Coordinate Attention Network for Building Footprint Extraction](https://arxiv.org/abs/2507.20809)
*Chunshi Wang,Bin Zhao,Shuxue Ding*

Main category: cs.CV

TL;DR: A new attention module (SCA) integrated into a 2D CNN (SCANet) improves building footprint extraction from remote sensing images, achieving state-of-the-art results on two benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Building footprint extraction is significant for urban planning, land use, environmental protection, and disaster assessment, but conventional and deep learning approaches face challenges.

Method: Introduced a novel plug-and-play attention module called Split Coordinate Attention (SCA), which captures spatially remote interactions using two spatial range pooling kernels and performs split operations for each feature group. Integrated SCA into a 2D CNN to form SCANet.

Result: SCANet achieves the best IoU on the WHU Building Dataset (91.61%) and the Massachusetts Building Dataset (75.49%).

Conclusion: SCANet outperforms recent SOTA methods on the WHU and Massachusetts Building Datasets, achieving 91.61% and 75.49% IoU respectively.

Abstract: Building footprint extraction holds immense significance in remote sensing
image analysis and has great value in urban planning, land use, environmental
protection and disaster assessment. Despite the progress made by conventional
and deep learning approaches in this field, they continue to encounter
significant challenges. This paper introduces a novel plug-and-play attention
module, Split Coordinate Attention (SCA), which ingeniously captures spatially
remote interactions by employing two spatial range of pooling kernels,
strategically encoding each channel along x and y planes, and separately
performs a series of split operations for each feature group, thus enabling
more efficient semantic feature extraction. By inserting into a 2D CNN to form
an effective SCANet, our SCANet outperforms recent SOTA methods on the public
Wuhan University (WHU) Building Dataset and Massachusetts Building Dataset in
terms of various metrics. Particularly SCANet achieves the best IoU, 91.61% and
75.49% for the two datasets. Our code is available at
https://github.com/AiEson/SCANet

</details>


### [148] [Rethinking Few Shot CLIP Benchmarks: A Critical Analysis in the Inductive Setting](https://arxiv.org/abs/2507.20834)
*Alexey Kravets,Da Chen,Vinay P. Namboodiri*

Main category: cs.CV

TL;DR: Existing methods for improving CLIP's few-shot performance are evaluated on datasets CLIP has already seen, which isn't a true test of generalization. This paper introduces an 'unlearning' technique to create a truly inductive setting, showing current methods perform much worse. They also propose a better few-shot classification method that sets a new standard.


<details>
  <summary>Details</summary>
Motivation: The authors argue that standard few-shot datasets do not accurately reflect CLIP's inductive generalization ability because the model has likely seen most datasets, creating a partially transductive setting. They aim to provide a more accurate evaluation and improve few-shot classification.

Method: A pipeline using an unlearning technique is proposed to obtain true inductive baselines for evaluating CLIP's few-shot performance. An improved few-shot classification technique is also proposed.

Result: The proposed unlearning technique reveals a significant drop in performance (-55% on average) for existing methods in the new inductive setting. The improved few-shot classification technique achieves state-of-the-art results across 13 methods and 5880 experiments with various settings.

Conclusion: The paper identifies issues with evaluating CLIP-based few-shot classification, proposes a solution using unlearning to create new benchmarks, and introduces an improved method that achieves state-of-the-art performance.

Abstract: CLIP is a foundational model with transferable classification performance in
the few-shot setting. Several methods have shown improved performance of CLIP
using few-shot examples. However, so far, all these techniques have been
benchmarked using standard few-shot datasets. We argue that this mode of
evaluation does not provide a true indication of the inductive generalization
ability using few-shot examples. As most datasets have been seen by the CLIP
model, the resultant setting can be termed as partially transductive. To solve
this, we propose a pipeline that uses an unlearning technique to obtain true
inductive baselines. In this new inductive setting, the methods show a
significant drop in performance (-55% on average among 13 baselines with
multiple datasets). We validate the unlearning technique using oracle
baselines. An improved few-shot classification technique is proposed that
consistently obtains state-of-the-art performance over 13 other recent baseline
methods on a comprehensive analysis with 5880 experiments - varying the
datasets, differing number of few-shot examples, unlearning setting, and with
different seeds. Thus, we identify the issue with the evaluation of CLIP-based
few-shot classification, provide a solution using unlearning, propose new
benchmarks, and provide an improved method.

</details>


### [149] [METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models](https://arxiv.org/abs/2507.20842)
*Yuchen Liu,Yaoming Wang,Bowen Shi,Xiaopeng Zhang,Wenrui Dai,Chenglin Li,Hongkai Xiong,Qi Tian*

Main category: cs.CV

TL;DR: METEOR是一种高效的多编码器视觉语言模型，通过渐进式剪枝策略（编码、融合、解码阶段）大幅减少冗余视觉标记，在保持高性能的同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP等单一编码器架构在多模态任务泛化能力上存在局限性，而多编码器融合方法虽然性能优越，但计算开销过大。METEOR旨在解决这一问题，通过提出一种渐进式剪枝框架，以较低的计算成本实现高效的多编码器多模态大语言模型。

Method: METEOR框架通过三个阶段进行冗余视觉标记的剪除：1. 在编码阶段，利用基于秩的协同标记分配策略消除每个编码器内部的冗余标记。2. 在融合阶段，通过合作剪枝减少跨编码器的冗余，融合不同编码器的视觉特征。3. 在解码阶段，提出一种自适应标记剪除方法，根据文本提示动态调整剪除比例，进一步去除不相关的标记。

Result: METEOR框架能够减少76%的视觉标记，而平均性能仅下降0.3%，优于EAGLE等现有多编码器大语言模型。

Conclusion: METEOR通过多阶段剪枝策略有效地减少了多编码器视觉语言模型中的冗余视觉标记，在保持性能的同时显著降低了计算开销，并在11个基准测试中取得了优于EAGLE的成果。

Abstract: Vision encoders serve as the cornerstone of multimodal understanding.
Single-encoder architectures like CLIP exhibit inherent constraints in
generalizing across diverse multimodal tasks, while recent multi-encoder fusion
methods introduce prohibitive computational overhead to achieve superior
performance using complementary visual representations from multiple vision
encoders. To address this, we propose a progressive pruning framework, namely
Multi-Encoder collaboraTivE tOken pRuning (METEOR), that eliminates redundant
visual tokens across the encoding, fusion, and decoding stages for
multi-encoder MLLMs. For multi-vision encoding, we discard redundant tokens
within each encoder via a rank guided collaborative token assignment strategy.
Subsequently, for multi-vision fusion, we combine the visual features from
different encoders while reducing cross-encoder redundancy with cooperative
pruning. Finally, we propose an adaptive token pruning method in the LLM
decoding stage to further discard irrelevant tokens based on the text prompts
with dynamically adjusting pruning ratios for specific task demands. To our
best knowledge, this is the first successful attempt that achieves an efficient
multi-encoder based vision language model with multi-stage pruning strategies.
Extensive experiments on 11 benchmarks demonstrate the effectiveness of our
proposed approach. Compared with EAGLE, a typical multi-encoder MLLMs, METEOR
reduces 76% visual tokens with only 0.3% performance drop in average. The code
is available at https://github.com/YuchenLiu98/METEOR.

</details>


### [150] [$S^3$LAM: Surfel Splatting SLAM for Geometrically Accurate Tracking and Mapping](https://arxiv.org/abs/2507.20854)
*Ruoyu Fan,Yuhui Wen,Jiajia Dai,Tao Zhang,Long Zeng,Yong-jin Liu*

Main category: cs.CV

TL;DR: $S^3$LAM is a new RGB-D SLAM system using 2D surfel splatting for accurate 3D reconstruction and tracking, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address SLAM challenges like real-time optimization under limited viewpoints and to achieve highly accurate geometric representations for simultaneous tracking and mapping.

Method: The system uses 2D Gaussian surfels as primitives for efficient scene representation and employs an adaptive surface rendering strategy for improved mapping accuracy and computational efficiency. Camera pose Jacobians are derived directly from the 2D surfel splatting formulation for better tracking convergence.

Result: Extensive experiments validate that S^3LAM achieves state-of-the-art performance.

Conclusion: S^3LAM, a novel RGB-D SLAM system utilizing 2D surfel splatting, achieves state-of-the-art performance in both synthetic and real-world datasets by reconstructing high-quality geometry for improved mapping and tracking.

Abstract: We propose $S^3$LAM, a novel RGB-D SLAM system that leverages 2D surfel
splatting to achieve highly accurate geometric representations for simultaneous
tracking and mapping. Unlike existing 3DGS-based SLAM approaches that rely on
3D Gaussian ellipsoids, we utilize 2D Gaussian surfels as primitives for more
efficient scene representation. By focusing on the surfaces of objects in the
scene, this design enables $S^3$LAM to reconstruct high-quality geometry,
benefiting both mapping and tracking. To address inherent SLAM challenges
including real-time optimization under limited viewpoints, we introduce a novel
adaptive surface rendering strategy that improves mapping accuracy while
maintaining computational efficiency. We further derive camera pose Jacobians
directly from 2D surfel splatting formulation, highlighting the importance of
our geometrically accurate representation that improves tracking convergence.
Extensive experiments on both synthetic and real-world datasets validate that
$S^3$LAM achieves state-of-the-art performance. Code will be made publicly
available.

</details>


### [151] [Compositional Video Synthesis by Temporal Object-Centric Learning](https://arxiv.org/abs/2507.20855)
*Adil Kaan Akan,Yucel Yemez*

Main category: cs.CV

TL;DR: 提出了一种新颖的视频合成框架，利用时间一致的对象中心表示，并结合预训练的扩散模型，实现了高质量、可控的视频生成和编辑。


<details>
  <summary>Details</summary>
Motivation: 解决现有对象中心方法缺乏生成能力或整体处理视频序列而忽略显式对象级别结构的问题。

Method: 通过学习姿态不变的对象中心槽并将其与预训练的扩散模型相结合，显式地捕获时间动态。

Result: 实现了高质量、像素级的视频合成，具有卓越的时间一致性，并提供了直观的组合编辑功能（如对象插入、删除或替换），同时在整个帧中保持一致的对象身份。实验证明，该方法在视频生成质量和时间一致性方面优于以往的对象中心生成方法。

Conclusion: 该方法在视频生成质量和时间一致性方面设定了新的基准，在交互式和可控视频生成方面取得了显著进展，为高级内容创建、语义编辑和动态场景理解开辟了新的可能性。

Abstract: We present a novel framework for compositional video synthesis that leverages
temporally consistent object-centric representations, extending our previous
work, SlotAdapt, from images to video. While existing object-centric approaches
either lack generative capabilities entirely or treat video sequences
holistically, thus neglecting explicit object-level structure, our approach
explicitly captures temporal dynamics by learning pose invariant object-centric
slots and conditioning them on pretrained diffusion models. This design enables
high-quality, pixel-level video synthesis with superior temporal coherence, and
offers intuitive compositional editing capabilities such as object insertion,
deletion, or replacement, maintaining consistent object identities across
frames. Extensive experiments demonstrate that our method sets new benchmarks
in video generation quality and temporal consistency, outperforming previous
object-centric generative methods. Although our segmentation performance
closely matches state-of-the-art methods, our approach uniquely integrates this
capability with robust generative performance, significantly advancing
interactive and controllable video generation and opening new possibilities for
advanced content creation, semantic editing, and dynamic scene understanding.

</details>


### [152] [Ensemble Foreground Management for Unsupervised Object Discovery](https://arxiv.org/abs/2507.20860)
*Ziling Wu,Armaghan Moemeni,Praminda Caleb-Solly*

Main category: cs.CV

TL;DR: 本研究提出了UnionCut和UnionSeg，改进了无监督对象发现（UOD）方法。UnionCut是一种新的前景先验，能更好地识别前景区域，而UnionSeg是其更高效的版本。这些方法提高了UOD在分割和检测任务中的准确性，并解决了现有方法在区分前景/背景和确定对象数量方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督对象发现（UOD）方法在处理无标注数据时面临两大挑战：1）区分前景和背景；2）确定未发现对象的数量。先前的方法依赖于启发式且不够鲁棒的前景先验，并且固定发现次数会导致过分割或欠分割。

Method: 提出了一种基于最小割和集成方法的鲁棒前景先验UnionCut，用于检测图像的前景区域的并集。在此基础上，提出了一种经过蒸馏的Transformer模型UnionSeg，以更高效、更准确地输出前景并集。这两种方法都能帮助无监督对象发现算法识别前景对象，并在分割完大部分前景并集后停止发现。

Result: 通过结合UnionCut或UnionSeg，现有的最先进的UOD方法在多个基准测试中，在单对象发现、显著性检测和自监督实例分割方面的性能均有所提升。

Conclusion: UnionCut和UnionSeg能够提升现有无监督对象发现方法在单对象发现、显著性检测和自监督实例分割任务上的性能。

Abstract: Unsupervised object discovery (UOD) aims to detect and segment objects in 2D
images without handcrafted annotations. Recent progress in self-supervised
representation learning has led to some success in UOD algorithms. However, the
absence of ground truth provides existing UOD methods with two challenges: 1)
determining if a discovered region is foreground or background, and 2) knowing
how many objects remain undiscovered. To address these two problems, previous
solutions rely on foreground priors to distinguish if the discovered region is
foreground, and conduct one or fixed iterations of discovery. However, the
existing foreground priors are heuristic and not always robust, and a fixed
number of discoveries leads to under or over-segmentation, since the number of
objects in images varies. This paper introduces UnionCut, a robust and
well-grounded foreground prior based on min-cut and ensemble methods that
detects the union of foreground areas of an image, allowing UOD algorithms to
identify foreground objects and stop discovery once the majority of the
foreground union in the image is segmented. In addition, we propose UnionSeg, a
distilled transformer of UnionCut that outputs the foreground union more
efficiently and accurately. Our experiments show that by combining with
UnionCut or UnionSeg, previous state-of-the-art UOD methods witness an increase
in the performance of single object discovery, saliency detection and
self-supervised instance segmentation on various benchmarks. The code is
available at https://github.com/YFaris/UnionCut.

</details>


### [153] [Not Only Grey Matter: OmniBrain for Robust Multimodal Classification of Alzheimer's Disease](https://arxiv.org/abs/2507.20872)
*Ahmed Sharshar,Yasser Ashraf,Tameem Bakr,Salma Hassan,Hosam Elgendy,Mohammad Yaqub,Mohsen Guizani*

Main category: cs.CV

TL;DR: OmniBrain 是一个多模态框架，通过整合多种数据类型（包括 MRI、影像组学、基因表达和临床数据）来诊断阿尔茨海默病，其准确性和可解释性均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的阿尔茨海默病诊断方法在临床可接受的准确性、跨数据集的泛化性、对缺失数据的鲁棒性以及可解释性方面存在局限性，无法同时满足所有这些要求，这影响了它们在临床环境中的可靠性。

Method: 提出了一种名为 OmniBrain 的多模态框架，该框架通过交叉注意和模态丢弃将脑部 MRI、影像组学、基因表达和临床数据整合到一个统一的模型中。

Result: OmniBrain 在 ANMerge 数据集上实现了 $92.2 	extbackslash pm 2.4\%$ 的准确性，并在仅 MRI 的 ADNI 数据集上实现了 $70.4 	extbackslash pm 2.7\%$ 的准确性，优于单模态和先前多模态方法。可解释性分析强调了与神经病理学相关的脑区和基因。

Conclusion: OmniBrain 提供了一个稳健、可解释且实用的解决方案，用于现实世界中的阿尔茨海默病诊断。

Abstract: Alzheimer's disease affects over 55 million people worldwide and is projected
to more than double by 2050, necessitating rapid, accurate, and scalable
diagnostics. However, existing approaches are limited because they cannot
achieve clinically acceptable accuracy, generalization across datasets,
robustness to missing modalities, and explainability all at the same time. This
inability to satisfy all these requirements simultaneously undermines their
reliability in clinical settings. We propose OmniBrain, a multimodal framework
that integrates brain MRI, radiomics, gene expression, and clinical data using
a unified model with cross-attention and modality dropout. OmniBrain achieves
$92.2 \pm 2.4\%$accuracy on the ANMerge dataset and generalizes to the MRI-only
ADNI dataset with $70.4 \pm 2.7\%$ accuracy, outperforming unimodal and prior
multimodal approaches. Explainability analyses highlight neuropathologically
relevant brain regions and genes, enhancing clinical trust. OmniBrain offers a
robust, interpretable, and practical solution for real-world Alzheimer's
diagnosis.

</details>


### [154] [DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception](https://arxiv.org/abs/2507.20879)
*Weicheng Zheng,Xiaofei Mao,Nanfei Ye,Pengxiang Li,Kun Zhan,Xianpeng Lang,Hang Zhao*

Main category: cs.CV

TL;DR: DriveAgent-R1通过混合思考和主动感知，提高了自动驾驶决策的效率和可靠性，并在实验中超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉语言模型（VLMs）在自动驾驶中的决策过程过于短视且感知能力被动，这在复杂环境中限制了其可靠性。DriveAgent-R1旨在解决这些挑战，以提升长周期、高层次的行为决策能力。

Method: DriveAgent-R1采用混合思考框架，能够在高效的文本推理和深入的工具推理之间自适应切换，并结合了具有视觉工具箱的主动感知机制，以主动解决不确定性。该模型使用新颖的三阶段渐进式强化学习策略进行训练。

Result: 实验证明DriveAgent-R1实现了最先进的性能，在自动驾驶任务中超越了包括Claude Sonnet 4在内的领先专有大型多模态模型。消融研究验证了该方法的有效性，并证实了其决策是基于主动感知的视觉证据的。

Conclusion: "DriveAgent-R1在长期、高层次行为决策方面取得了最先进的性能，超越了包括Claude Sonnet 4在内的领先的专有大型多模态模型。它通过混合思考框架和主动感知机制，有效平衡了决策效率和可靠性，并且其决策能够稳健地基于主动感知的视觉证据，为更安全、更智能的自动驾驶系统铺平了道路。"

Abstract: Vision-Language Models (VLMs) are advancing autonomous driving, yet their
potential is constrained by myopic decision-making and passive perception,
limiting reliability in complex environments. We introduce DriveAgent-R1 to
tackle these challenges in long-horizon, high-level behavioral decision-making.
DriveAgent-R1 features two core innovations: a Hybrid-Thinking framework that
adaptively switches between efficient text-based and in-depth tool-based
reasoning, and an Active Perception mechanism with a vision toolkit to
proactively resolve uncertainties, thereby balancing decision-making efficiency
and reliability. The agent is trained using a novel, three-stage progressive
reinforcement learning strategy designed to master these hybrid capabilities.
Extensive experiments demonstrate that DriveAgent-R1 achieves state-of-the-art
performance, outperforming even leading proprietary large multimodal models,
such as Claude Sonnet 4. Ablation studies validate our approach and confirm
that the agent's decisions are robustly grounded in actively perceived visual
evidence, paving a path toward safer and more intelligent autonomous systems.

</details>


### [155] [Event-Based De-Snowing for Autonomous Driving](https://arxiv.org/abs/2507.20901)
*Manasi Muglikar,Nico Messikommer,Marco Cannici,Davide Scaramuzza*

Main category: cs.CV

TL;DR: 本研究提出一种创新的事件相机去噪方法，通过分析雪花的时空特征有效去除雪花伪影，显著提升图像质量和下游视觉任务性能，为全天候自动驾驶提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气条件（尤其是大雪）给人类驾驶员和自动驾驶汽车带来了严峻挑战。传统的基于图像的去噪方法容易产生幻觉伪影，而基于视频的方法在低帧率下存在对齐伪影。同时，相机参数（如曝光时间）也会影响雪花的视觉表现，使得问题难以解决且严重依赖于网络的泛化能力。因此，需要一种新的方法来应对这些挑战。

Method: 提出了一种利用事件相机进行去噪的方法。该方法利用事件数据中雪花遮挡出现的独特时空条纹特征，设计了一个基于注意力机制的模块，该模块聚焦于这些条纹事件，以确定背景点何时被遮挡，并利用此信息恢复其原始强度。

Result: 所提出的方法在DSEC-Snow数据集上进行了基准测试，相比于现有的最先进的去噪方法，在图像重建方面PSNR提高了3 dB。此外，研究表明，可以直接将现成的计算机视觉算法应用于该方法重建的图像上，用于深度估计和光流估计等任务，性能比其他去噪方法提高了20%。

Conclusion: 该研究通过使用事件相机和一种新颖的基于注意力机制的方法，成功解决了恶劣天气（尤其是降雪）下的视觉去噪问题。该方法利用事件数据中的时空特征来识别和恢复被雪花遮挡的背景信息，并在DSEC-Snow数据集上进行了基准测试，结果表明其性能优于现有的最先进的去噪方法，PSNR提高了3 dB。此外，该方法处理后的图像能够显著提升下游计算机视觉任务（如深度估计和光流估计）的性能（提高20%）。这项工作为提高在恶劣冬季条件下视觉系统的可靠性和安全性迈出了重要一步，为开发更鲁棒、全天候适应的应用铺平了道路。

Abstract: Adverse weather conditions, particularly heavy snowfall, pose significant
challenges to both human drivers and autonomous vehicles. Traditional
image-based de-snowing methods often introduce hallucination artifacts as they
rely solely on spatial information, while video-based approaches require high
frame rates and suffer from alignment artifacts at lower frame rates. Camera
parameters, such as exposure time, also influence the appearance of snowflakes,
making the problem difficult to solve and heavily dependent on network
generalization. In this paper, we propose to address the challenge of desnowing
by using event cameras, which offer compressed visual information with
submillisecond latency, making them ideal for de-snowing images, even in the
presence of ego-motion. Our method leverages the fact that snowflake occlusions
appear with a very distinctive streak signature in the spatio-temporal
representation of event data. We design an attention-based module that focuses
on events along these streaks to determine when a background point was occluded
and use this information to recover its original intensity. We benchmark our
method on DSEC-Snow, a new dataset created using a green-screen technique that
overlays pre-recorded snowfall data onto the existing DSEC driving dataset,
resulting in precise ground truth and synchronized image and event streams. Our
approach outperforms state-of-the-art de-snowing methods by 3 dB in PSNR for
image reconstruction. Moreover, we show that off-the-shelf computer vision
algorithms can be applied to our reconstructions for tasks such as depth
estimation and optical flow, achieving a $20\%$ performance improvement over
other de-snowing methods. Our work represents a crucial step towards enhancing
the reliability and safety of vision systems in challenging winter conditions,
paving the way for more robust, all-weather-capable applications.

</details>


### [156] [SCORPION: Addressing Scanner-Induced Variability in Histopathology](https://arxiv.org/abs/2507.20907)
*Jeongun Ryu,Heon Song,Seungeun Lee,Soo Ick Cho,Jiwon Shin,Kyunghyun Paeng,Sérgio Pereira*

Main category: cs.CV

TL;DR: SCORPION是一个包含480个组织样本、每个样本由5台扫描仪扫描的数据集，用于评估模型在扫描仪变异性下的可靠性。SimCons框架通过结合域泛化技术和一致性损失来提高模型在不同扫描仪上的一致性。


<details>
  <summary>Details</summary>
Motivation: 解决计算病理学中模型在不同扫描仪下性能不可靠的挑战，特别是扫描仪差异引入的变异性，以提高模型在真实世界应用中的可靠性。

Method: 提出SimCons框架，结合了基于增强的域泛化技术和一致性损失，以解决扫描仪泛化问题。

Result: SimCons在不同扫描仪上提高了模型的一致性，同时不影响特定任务的性能。

Conclusion: 通过发布SCORPION数据集和提出SimCons框架，为评估和改进跨不同扫描仪的模型一致性提供了重要资源，并为可靠性测试设定了新标准。

Abstract: Ensuring reliable model performance across diverse domains is a critical
challenge in computational pathology. A particular source of variability in
Whole-Slide Images is introduced by differences in digital scanners, thus
calling for better scanner generalization. This is critical for the real-world
adoption of computational pathology, where the scanning devices may differ per
institution or hospital, and the model should not be dependent on
scanner-induced details, which can ultimately affect the patient's diagnosis
and treatment planning. However, past efforts have primarily focused on
standard domain generalization settings, evaluating on unseen scanners during
training, without directly evaluating consistency across scanners for the same
tissue. To overcome this limitation, we introduce SCORPION, a new dataset
explicitly designed to evaluate model reliability under scanner variability.
SCORPION includes 480 tissue samples, each scanned with 5 scanners, yielding
2,400 spatially aligned patches. This scanner-paired design allows for the
isolation of scanner-induced variability, enabling a rigorous evaluation of
model consistency while controlling for differences in tissue composition.
Furthermore, we propose SimCons, a flexible framework that combines
augmentation-based domain generalization techniques with a consistency loss to
explicitly address scanner generalization. We empirically show that SimCons
improves model consistency on varying scanners without compromising
task-specific performance. By releasing the SCORPION dataset and proposing
SimCons, we provide the research community with a crucial resource for
evaluating and improving model consistency across diverse scanners, setting a
new standard for reliability testing.

</details>


### [157] [HAMLET-FFD: Hierarchical Adaptive Multi-modal Learning Embeddings Transformation for Face Forgery Detection](https://arxiv.org/abs/2507.20913)
*Jialei Cui,Jianwei Du,Yanzhe Li,Lei Gao,Hui Jiang,Chenfu Bao*

Main category: cs.CV

TL;DR: HAMLET-FFD 是一个新颖的框架，通过结合视觉和文本信息来提高面部伪造检测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的面部伪造检测方法在跨域泛化方面存在挑战，因为它们依赖于简单的分类目标，难以学习域不变表示。

Method: HAMLET-FFD 是一个认知启发式的分层自适应多模态学习框架，利用对比视觉-语言模型（如 CLIP）引入知识细化循环，通过整合视觉证据和概念线索来评估真实性，并采用双向融合机制，其中文本真实性嵌入指导分层视觉特征的聚合，而调制后的视觉特征则细化文本嵌入以生成图像自适应提示。该框架冻结了所有预训练参数，作为一个外部插件运行。

Result: HAMLET-FFD 在多个基准测试中展现了优越的跨域泛化能力，能够有效检测未知的篡改操作。

Conclusion: HAMLET-FFD 框架能够通过双向跨模态推理有效解决跨域泛化问题，并且在多个基准测试中表现出优越的泛化能力，同时通过实验分析揭示了嵌入之间存在分工，不同的表示专门用于细粒度的伪影识别。

Abstract: The rapid evolution of face manipulation techniques poses a critical
challenge for face forgery detection: cross-domain generalization. Conventional
methods, which rely on simple classification objectives, often fail to learn
domain-invariant representations. We propose HAMLET-FFD, a cognitively inspired
Hierarchical Adaptive Multi-modal Learning framework that tackles this
challenge via bidirectional cross-modal reasoning. Building on contrastive
vision-language models such as CLIP, HAMLET-FFD introduces a knowledge
refinement loop that iteratively assesses authenticity by integrating visual
evidence with conceptual cues, emulating expert forensic analysis. A key
innovation is a bidirectional fusion mechanism in which textual authenticity
embeddings guide the aggregation of hierarchical visual features, while
modulated visual features refine text embeddings to generate image-adaptive
prompts. This closed-loop process progressively aligns visual observations with
semantic priors to enhance authenticity assessment. By design, HAMLET-FFD
freezes all pretrained parameters, serving as an external plugin that preserves
CLIP's original capabilities. Extensive experiments demonstrate its superior
generalization to unseen manipulations across multiple benchmarks, and visual
analyses reveal a division of labor among embeddings, with distinct
representations specializing in fine-grained artifact recognition.

</details>


### [158] [RIS-LAD: A Benchmark and Model for Referring Low-Altitude Drone Image Segmentation](https://arxiv.org/abs/2507.20920)
*Kai Ye,YingShi Luan,Zhudi Chen,Guangyue Meng,Pingyang Dai,Liujuan Cao*

Main category: cs.CV

TL;DR: 提出首个用于低空无人机（LAD）场景的Referring Image Segmentation（RIS）基准数据集RIS-LAD，并引入SAARN模型。该模型能有效解决LAD场景带来的类别和对象漂移问题，并在实验中证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有 Referring Image Segmentation（RIS）方法和数据集主要针对高空和静态视角图像，难以处理低空无人机（LAD）场景下图像的独特特性，如多变的视角和高密度的对象。为了解决这一差距，需要一个专门针对LAD场景的精细化RIS基准和相应的方法。

Method: 提出了一种名为语义感知自适应推理网络（SAARN）的模型。该模型将语义信息分解并路由到网络的各个阶段，包括类别主导的语言增强（CDLE）模块，用于在早期编码中将视觉特征与对象类别对齐；以及自适应推理融合（ARFM）模块，用于动态选择跨尺度的语义线索以提高复杂场景下的推理能力。

Result: RIS-LAD数据集对现有的RIS算法提出了显著的挑战。提出的SAARN模型在RIS-LAD数据集上证明了其有效性，能够解决类别漂移和对象漂移等问题。

Conclusion: RIS-LAD数据集和SAARN模型在低空无人机（LAD）场景下的Referring Image Segmentation（RIS）任务中展现了有效性。SAARN模型通过语义感知的自适应推理网络，有效地解决了现有方法在处理LAD场景下的独特挑战，如类别漂移和对象漂移。实验结果表明，RIS-LAD对现有RIS算法提出了显著挑战，而SAARN能够有效应对这些挑战。

Abstract: Referring Image Segmentation (RIS), which aims to segment specific objects
based on natural language descriptions, plays an essential role in
vision-language understanding. Despite its progress in remote sensing
applications, RIS in Low-Altitude Drone (LAD) scenarios remains underexplored.
Existing datasets and methods are typically designed for high-altitude and
static-view imagery. They struggle to handle the unique characteristics of LAD
views, such as diverse viewpoints and high object density. To fill this gap, we
present RIS-LAD, the first fine-grained RIS benchmark tailored for LAD
scenarios. This dataset comprises 13,871 carefully annotated image-text-mask
triplets collected from realistic drone footage, with a focus on small,
cluttered, and multi-viewpoint scenes. It highlights new challenges absent in
previous benchmarks, such as category drift caused by tiny objects and object
drift under crowded same-class objects. To tackle these issues, we propose the
Semantic-Aware Adaptive Reasoning Network (SAARN). Rather than uniformly
injecting all linguistic features, SAARN decomposes and routes semantic
information to different stages of the network. Specifically, the
Category-Dominated Linguistic Enhancement (CDLE) aligns visual features with
object categories during early encoding, while the Adaptive Reasoning Fusion
Module (ARFM) dynamically selects semantic cues across scales to improve
reasoning in complex scenes. The experimental evaluation reveals that RIS-LAD
presents substantial challenges to state-of-the-art RIS algorithms, and also
demonstrates the effectiveness of our proposed model in addressing these
challenges. The dataset and code will be publicly released soon at:
https://github.com/AHideoKuzeA/RIS-LAD/.

</details>


### [159] [Exploring text-to-image generation for historical document image retrieval](https://arxiv.org/abs/2507.20934)
*Melissa Cote,Alexandra Branzan Albu*

Main category: cs.CV

TL;DR: 该研究提出T2I-QBE方法，利用生成式AI（Leonardo.Ai）将基于属性的文本描述转换为图像，作为QBE的查询，以改进文档图像检索，特别是在历史文献领域，并取得了积极效果。


<details>
  <summary>Details</summary>
Motivation: 为了解决按示例查询（QBE）需要实际查询文档样本的缺点，并提供一种更灵活的基于属性的文档图像检索（ABDIR）方法。特别关注历史文献，因为它们具有多样性和独特的视觉特征。

Method: 提出了一种名为T2I-QBE的方法，该方法利用文本到图像（T2I）生成技术来创建查询图像。这些查询图像随后被用于传统的按示例查询（QBE）范例中，通过比较CNN提取的查询特征和文档图像特征来检索相关文档。

Result: 实验结果证实了该假设，表明T2I-QBE是历史文献图像检索的一种可行选择。据作者所知，这是首次尝试利用T2I生成技术进行文档图像检索。

Conclusion: T2I-QBE是一种可行的历史文献图像检索方法。

Abstract: Attribute-based document image retrieval (ABDIR) was recently proposed as an
alternative to query-by-example (QBE) searches, the dominant document image
retrieval (DIR) paradigm. One drawback of QBE searches is that they require
sample query documents on hand that may not be available. ABDIR aims to offer
users a flexible way to retrieve document images based on memorable visual
features of document contents, describing document images with combinations of
visual attributes determined via convolutional neural network (CNN)-based
binary classifiers. We present an exploratory study of the use of generative AI
to bridge the gap between QBE and ABDIR, focusing on historical documents as a
use case for their diversity and uniqueness in visual features. We hypothesize
that text-to-image (T2I) generation can be leveraged to create query document
images using text prompts based on ABDIR-like attributes. We propose T2I-QBE,
which uses Leonardo.Ai as the T2I generator with prompts that include a rough
description of the desired document type and a list of the desired ABDIR-style
attributes. This creates query images that are then used within the traditional
QBE paradigm, which compares CNN-extracted query features to those of the
document images in the dataset to retrieve the most relevant documents.
Experiments on the HisIR19 dataset of historical documents confirm our
hypothesis and suggest that T2I-QBE is a viable option for historical document
image retrieval. To the authors' knowledge, this is the first attempt at
utilizing T2I generation for DIR.

</details>


### [160] [ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts](https://arxiv.org/abs/2507.20939)
*Yuying Ge,Yixiao Ge,Chen Li,Teng Wang,Junfu Pu,Yizhuo Li,Lu Qiu,Jin Ma,Lisheng Duan,Xinyu Zuo,Jinwen Luo,Weibo Gu,Zexuan Li,Xiaojing Zhang,Yangyu Tao,Han Hu,Di Wang,Ying Shan*

Main category: cs.CV

TL;DR: The ARC-Hunyuan-Video model significantly improves short video comprehension by integrating visual, audio, and text data, outperforming existing models and enhancing user engagement in real-world applications with high efficiency.


<details>
  <summary>Details</summary>
Motivation: Current large multimodal models lack the temporally-structured, detailed, and in-depth video comprehension capabilities necessary for effective video search, recommendation, and emerging video applications. Understanding real-world short videos is challenging due to complex visual elements, high information density in visuals and audio, and fast pacing focused on emotional expression and viewpoint delivery, requiring advanced reasoning to integrate multimodal information.

Method: The paper introduces ARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual signals from raw video inputs end-to-end. It features multi-granularity timestamped video captioning and summarization, open-ended video question answering, temporal video grounding, and video reasoning. The model, with 7 billion parameters, is trained through a regimen including pre-training, instruction fine-tuning, cold start, reinforcement learning post-training, and final instruction fine-tuning, utilizing high-quality data from an automated annotation pipeline. Evaluation is performed on the ShortVid-Bench benchmark.

Result: The ARC-Hunyuan-Video model shows strong performance in real-world video comprehension on the ShortVid-Bench benchmark, with qualitative comparisons also demonstrating its effectiveness. Its real-world production deployment has led to tangible improvements in user engagement and satisfaction. The model exhibits remarkable efficiency, with an inference time of 10 seconds for a one-minute video on an H20 GPU.

Conclusion: ARC-Hunyuan-Video, a 7B-parameter multimodal model, demonstrates strong performance in real-world short video comprehension, achieving measurable improvements in user engagement and satisfaction through its efficient processing and diverse capabilities, supporting zero-shot or few-shot fine-tuning for various applications.

Abstract: Real-world user-generated short videos, especially those distributed on
platforms such as WeChat Channel and TikTok, dominate the mobile internet.
However, current large multimodal models lack essential temporally-structured,
detailed, and in-depth video comprehension capabilities, which are the
cornerstone of effective video search and recommendation, as well as emerging
video applications. Understanding real-world shorts is actually challenging due
to their complex visual elements, high information density in both visuals and
audio, and fast pacing that focuses on emotional expression and viewpoint
delivery. This requires advanced reasoning to effectively integrate multimodal
information, including visual, audio, and text. In this work, we introduce
ARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual
signals from raw video inputs end-to-end for structured comprehension. The
model is capable of multi-granularity timestamped video captioning and
summarization, open-ended video question answering, temporal video grounding,
and video reasoning. Leveraging high-quality data from an automated annotation
pipeline, our compact 7B-parameter model is trained through a comprehensive
regimen: pre-training, instruction fine-tuning, cold start, reinforcement
learning (RL) post-training, and final instruction fine-tuning. Quantitative
evaluations on our introduced benchmark ShortVid-Bench and qualitative
comparisons demonstrate its strong performance in real-world video
comprehension, and it supports zero-shot or fine-tuning with a few samples for
diverse downstream applications. The real-world production deployment of our
model has yielded tangible and measurable improvements in user engagement and
satisfaction, a success supported by its remarkable efficiency, with stress
tests indicating an inference time of just 10 seconds for a one-minute video on
H20 GPU.

</details>


### [161] [Mask-Free Audio-driven Talking Face Generation for Enhanced Visual Quality and Identity Preservation](https://arxiv.org/abs/2507.20953)
*Dogucan Yaman,Fevziye Irem Eyiokur,Leonard Bärmann,Hazım Kemal Ekenel,Alexander Waibel*

Main category: cs.CV

TL;DR: 提出了一种无需掩码的说话人脸生成方法，通过闭嘴转换和音频引导来生成逼真视频，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决了现有基于掩码的说话人脸生成方法中存在的输入人脸信息丢失、身份参考与输入图像不匹配以及身份参考干扰模型导致不期望的元素复制等问题。

Method: 提出了一种两步、基于标志点、以无配对方式训练的方法，首先将输入人脸图像转换为闭嘴状态，然后将编辑后但未掩码的人脸与音频结合，生成相应的唇部动作。

Result: 该方法无需掩码输入图像和身份参考图像，即可生成准确的唇部同步且保持身份细节的逼真视频。

Conclusion: 提出了一种无需掩码的音频驱动的谈话脸生成方法，该方法通过将输入图像转换为闭嘴状态，然后结合音频进行唇部动作生成，从而避免了传统掩码策略中的信息丢失、身份参考与输入图像不匹配以及身份参考负面影响等问题。实验结果在LRS2和HDTF数据集上进行了验证。

Abstract: Audio-Driven Talking Face Generation aims at generating realistic videos of
talking faces, focusing on accurate audio-lip synchronization without
deteriorating any identity-related visual details. Recent state-of-the-art
methods are based on inpainting, meaning that the lower half of the input face
is masked, and the model fills the masked region by generating lips aligned
with the given audio. Hence, to preserve identity-related visual details from
the lower half, these approaches additionally require an unmasked identity
reference image randomly selected from the same video. However, this common
masking strategy suffers from (1) information loss in the input faces,
significantly affecting the networks' ability to preserve visual quality and
identity details, (2) variation between identity reference and input image
degrading reconstruction performance, and (3) the identity reference negatively
impacting the model, causing unintended copying of elements unaligned with the
audio. To address these issues, we propose a mask-free talking face generation
approach while maintaining the 2D-based face editing task. Instead of masking
the lower half, we transform the input images to have closed mouths, using a
two-step landmark-based approach trained in an unpaired manner. Subsequently,
we provide these edited but unmasked faces to a lip adaptation model alongside
the audio to generate appropriate lip movements. Thus, our approach needs
neither masked input images nor identity reference images. We conduct
experiments on the benchmark LRS2 and HDTF datasets and perform various
ablation studies to validate our contributions.

</details>


### [162] [GTAD: Global Temporal Aggregation Denoising Learning for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2507.20963)
*Tianhao Li,Yang Li,Mengtian Li,Yisheng Deng,Weifeng Ge*

Main category: cs.CV

TL;DR: GTAD通过聚合局部和全局时间信息来改进动态环境感知，并在基准测试中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的动态环境感知方法未能有效利用全局序列信息，主要依赖于相邻帧之间的局部时间交互。

Method: 提出了一种名为GTAD的全局时间聚合去噪网络，该网络包含一个去噪网络，用于聚合来自当前时刻的局部时间特征和来自历史序列的全局时间特征。

Result: GTAD能够有效感知相邻帧的细粒度时间信息以及历史观测的全局时间模式，从而提供更连贯、更全面的环境理解。

Conclusion: GTAD通过聚合局部和全局时间特征，在nuScenes和Occ3D-nuScenes基准测试中表现出优越性。

Abstract: Accurately perceiving dynamic environments is a fundamental task for
autonomous driving and robotic systems. Existing methods inadequately utilize
temporal information, relying mainly on local temporal interactions between
adjacent frames and failing to leverage global sequence information
effectively. To address this limitation, we investigate how to effectively
aggregate global temporal features from temporal sequences, aiming to achieve
occupancy representations that efficiently utilize global temporal information
from historical observations. For this purpose, we propose a global temporal
aggregation denoising network named GTAD, introducing a global temporal
information aggregation framework as a new paradigm for holistic 3D scene
understanding. Our method employs an in-model latent denoising network to
aggregate local temporal features from the current moment and global temporal
features from historical sequences. This approach enables the effective
perception of both fine-grained temporal information from adjacent frames and
global temporal patterns from historical observations. As a result, it provides
a more coherent and comprehensive understanding of the environment. Extensive
experiments on the nuScenes and Occ3D-nuScenes benchmark and ablation studies
demonstrate the superiority of our method.

</details>


### [163] [Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with Weak Supervision](https://arxiv.org/abs/2507.20976)
*Xiao Fang,Minhyek Jeon,Zheyang Qin,Stanislav Panev,Celso de Melo,Shuowen Hu,Shayok Chakraborty,Fernando De la Torre*

Main category: cs.CV

TL;DR: 本研究通过生成式AI（微调LDM）合成航空影像数据，有效提升了目标检测器在不同地理区域的性能，解决了域漂移问题，并在多个数据集上取得了显著的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决在不同地理区域收集的航空影像数据存在域漂移问题，导致在某一区域训练的模型泛化能力下降的挑战，本研究旨在通过生成式AI合成数据来提升目标检测器在不同区域的鲁棒性和准确性。

Method: 提出了一种新颖的多阶段、多模态知识迁移框架，利用微调的潜在扩散模型（LDM）生成高分辨率航空影像及其标签，以此作为数据增强手段来改进目标检测器的训练，旨在缩小源域和目标域之间的分布差距。

Result: 在多个航空影像数据集上进行的广泛实验证明，所提出的方法在AP50指标上相较于仅使用源域数据进行监督学习、弱监督适应、无监督域适应以及开源对象检测器的方法，分别取得了4-23%、6-10%、7-40%以及超过50%的性能提升。

Conclusion: 该研究提出了一种利用生成式人工智能（特别是微调的潜在扩散模型LDM）来合成高分辨率航空影像及其标签的新颖方法，以增强目标检测器的训练。实验表明，与仅在源域数据上进行监督学习、弱监督适应、无监督域适应以及开源对象检测器相比，该方法在AP50上分别实现了4-23%、6-10%、7-40%和超过50%的性能提升，有效缓解了源域和目标域之间的分布差异。此外，研究还发布了两个新的新西兰和犹他州的航空数据集，为该领域的研究提供了支持。

Abstract: Detecting vehicles in aerial imagery is a critical task with applications in
traffic monitoring, urban planning, and defense intelligence. Deep learning
methods have provided state-of-the-art (SOTA) results for this application.
However, a significant challenge arises when models trained on data from one
geographic region fail to generalize effectively to other areas. Variability in
factors such as environmental conditions, urban layouts, road networks, vehicle
types, and image acquisition parameters (e.g., resolution, lighting, and angle)
leads to domain shifts that degrade model performance. This paper proposes a
novel method that uses generative AI to synthesize high-quality aerial images
and their labels, improving detector training through data augmentation. Our
key contribution is the development of a multi-stage, multi-modal knowledge
transfer framework utilizing fine-tuned latent diffusion models (LDMs) to
mitigate the distribution gap between the source and target environments.
Extensive experiments across diverse aerial imagery domains show consistent
performance improvements in AP50 over supervised learning on source domain
data, weakly supervised adaptation methods, unsupervised domain adaptation
methods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than
50%, respectively. Furthermore, we introduce two newly annotated aerial
datasets from New Zealand and Utah to support further research in this field.
Project page is available at: https://humansensinglab.github.io/AGenDA

</details>


### [164] [LargeMvC-Net: Anchor-based Deep Unfolding Network for Large-scale Multi-view Clustering](https://arxiv.org/abs/2507.20980)
*Shide Du,Chunming Wu,Zihan Fang,Wendi Zhao,Yilin Wu,Changwei Wang,Shiping Wang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep anchor-based multi-view clustering methods enhance the scalability of
neural networks by utilizing representative anchors to reduce the computational
complexity of large-scale clustering. Despite their scalability advantages,
existing approaches often incorporate anchor structures in a heuristic or
task-agnostic manner, either through post-hoc graph construction or as
auxiliary components for message passing. Such designs overlook the core
structural demands of anchor-based clustering, neglecting key optimization
principles. To bridge this gap, we revisit the underlying optimization problem
of large-scale anchor-based multi-view clustering and unfold its iterative
solution into a novel deep network architecture, termed LargeMvC-Net. The
proposed model decomposes the anchor-based clustering process into three
modules: RepresentModule, NoiseModule, and AnchorModule, corresponding to
representation learning, noise suppression, and anchor indicator estimation.
Each module is derived by unfolding a step of the original optimization
procedure into a dedicated network component, providing structural clarity and
optimization traceability. In addition, an unsupervised reconstruction loss
aligns each view with the anchor-induced latent space, encouraging consistent
clustering structures across views. Extensive experiments on several
large-scale multi-view benchmarks show that LargeMvC-Net consistently
outperforms state-of-the-art methods in terms of both effectiveness and
scalability.

</details>


### [165] [JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1](https://arxiv.org/abs/2507.20987)
*Xinhan Di,Kristin Qi,Pengqian Yu*

Main category: cs.CV

TL;DR: 扩散模型在视频生成方面取得进展，但在全身运动和语音的联合生成上仍有不足。本研究提出了JWB-DH-V1数据集和评估协议，以解决多模态一致性和评估不足的问题，并发现了面部/手部与全身性能的差异。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在生成逼真的短视频方面取得了进展，但在联合生成全身运动和自然语音时，在多模态一致性方面仍存在挑战。同时，缺乏全面的评估框架和针对区域性能分析的基准。

Method: 提出Joint Whole-Body Talking Avatar and Speech Generation Version I (JWB-DH-V1)，包含一个大规模多模态数据集（10,000个唯一身份，200万个视频样本）和一个评估协议，用于评估全身可驱动头像的联合视听生成。

Result: 通过对SOTA模型的评估，发现了面部/手部中心性能与全身性能之间存在持续的性能差异，指出了未来研究的关键方向。

Conclusion: 当前方法在联合生成全身运动和自然语音方面仍有不足，尤其是在多模态一致性方面。现有的评估框架缺乏对视听质量的全面评估，并且缺乏针对特定区域性能分析的基准。JWB-DH-V1数据集和评估协议的引入，旨在解决这些问题。

Abstract: Recent advances in diffusion-based video generation have enabled
photo-realistic short clips, but current methods still struggle to achieve
multi-modal consistency when jointly generating whole-body motion and natural
speech. Current approaches lack comprehensive evaluation frameworks that assess
both visual and audio quality, and there are insufficient benchmarks for
region-specific performance analysis. To address these gaps, we introduce the
Joint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1),
comprising a large-scale multi-modal dataset with 10,000 unique identities
across 2 million video samples, and an evaluation protocol for assessing joint
audio-video generation of whole-body animatable avatars. Our evaluation of SOTA
models reveals consistent performance disparities between face/hand-centric and
whole-body performance, which incidates essential areas for future research.
The dataset and evaluation tools are publicly available at
https://github.com/deepreasonings/WholeBodyBenchmark.

</details>


### [166] [Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM](https://arxiv.org/abs/2507.20994)
*Shen Li,Liuyi Yao,Wujia Niu,Lan Zhang,Yaliang Li*

Main category: cs.CV

TL;DR: 安全张量通过在推理时应用可训练向量来扩展文本安全到视觉模态，无需修改模型参数，从而提高LVLMs抵御有害图像输入的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的安全机制无法自然地扩展到视觉模态，导致LVLMs容易受到有害图像输入的影响，存在跨模态安全漏洞。

Method: 提出安全张量，这是一种可训练的输入向量，可在推理过程中通过文本或视觉模态应用，将文本安全对齐转移到视觉处理，而无需修改模型参数。使用包含（i）需要拒绝的恶意图像-文本对、（ii）结构上与恶意查询相似但旨在作为视觉依赖引导的良性对比对以及（iii）保留模型功能的通用良性样本的混合数据集进行优化。

Result: 实验结果表明，文本和视觉安全张量都能显著提高LVLMs拒绝各种有害视觉输入的能力，同时在良性任务上的性能几乎保持不变。内部分析表明，安全张量成功激活了语言模块的文本“安全层”以处理视觉输入。

Conclusion: 安全张量有效扩展了基于文本的安全能力到视觉模态，增强了LVLMs拒绝有害视觉输入的能力，同时保持了在良性任务上的性能。

Abstract: Large visual-language models (LVLMs) integrate aligned large language models
(LLMs) with visual modules to process multimodal inputs. However, the safety
mechanisms developed for text-based LLMs do not naturally extend to visual
modalities, leaving LVLMs vulnerable to harmful image inputs. To address this
cross-modal safety gap, we introduce security tensors - trainable input vectors
applied during inference through either the textual or visual modality. These
tensors transfer textual safety alignment to visual processing without
modifying the model's parameters. They are optimized using a curated dataset
containing (i) malicious image-text pairs requiring rejection, (ii) contrastive
benign pairs with text structurally similar to malicious queries, with the
purpose of being contrastive examples to guide visual reliance, and (iii)
general benign samples preserving model functionality. Experimental results
demonstrate that both textual and visual security tensors significantly enhance
LVLMs' ability to reject diverse harmful visual inputs while maintaining
near-identical performance on benign tasks. Further internal analysis towards
hidden-layer representations reveals that security tensors successfully
activate the language module's textual "safety layers" in visual inputs,
thereby effectively extending text-based safety to the visual modality.

</details>


### [167] [Improving Adversarial Robustness Through Adaptive Learning-Driven Multi-Teacher Knowledge Distillation](https://arxiv.org/abs/2507.20996)
*Hayat Ullah,Syed Muhammad Talha Zaidi,Arslan Munir*

Main category: cs.CV

TL;DR: 提出一种多教师对抗鲁棒性蒸馏和自适应学习策略，以提升CNN的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了解决CNN在对抗攻击面前鲁棒性不足的问题，缩小模型准确率和鲁棒性之间的差距。

Method: 采用多教师对抗鲁棒性蒸馏和自适应学习策略，通过训练多个教师模型来监督学生模型在干净数据上的学习，并根据预测精度动态调整教师模型的知识贡献。

Result: 在MNIST-Digits和Fashion-MNIST数据集上进行了广泛的评估，结果表明该方法能有效提升CNN的对抗鲁棒性。

Conclusion: 所提出的多教师对抗鲁棒性蒸馏和自适应学习策略有效提高了CNN在各种对抗攻击下的鲁棒性。

Abstract: Convolutional neural networks (CNNs) excel in computer vision but are
susceptible to adversarial attacks, crafted perturbations designed to mislead
predictions. Despite advances in adversarial training, a gap persists between
model accuracy and robustness. To mitigate this issue, in this paper, we
present a multi-teacher adversarial robustness distillation using an adaptive
learning strategy. Specifically, our proposed method first trained multiple
clones of a baseline CNN model using an adversarial training strategy on a pool
of perturbed data acquired through different adversarial attacks. Once trained,
these adversarially trained models are used as teacher models to supervise the
learning of a student model on clean data using multi-teacher knowledge
distillation. To ensure an effective robustness distillation, we design an
adaptive learning strategy that controls the knowledge contribution of each
model by assigning weights as per their prediction precision. Distilling
knowledge from adversarially pre-trained teacher models not only enhances the
learning capabilities of the student model but also empowers it with the
capacity to withstand different adversarial attacks, despite having no exposure
to adversarial data. To verify our claims, we extensively evaluated our
proposed method on MNIST-Digits and Fashion-MNIST datasets across diverse
experimental settings. The obtained results exhibit the efficacy of our
multi-teacher adversarial distillation and adaptive learning strategy,
enhancing CNNs' adversarial robustness against various adversarial attacks.

</details>


### [168] [Learning Transferable Facial Emotion Representations from Large-Scale Semantically Rich Captions](https://arxiv.org/abs/2507.21015)
*Licai Sun,Xingxun Jiang,Haoyu Chen,Yante Li,Zheng Lian,Biu Liu,Yuan Zong,Wenming Zheng,Jukka M. Leppänen,Guoying Zhao*

Main category: cs.CV

TL;DR: 本研究通过构建大规模面部表情字幕数据集（EmoCap100K）并提出 EmoCapCLIP 模型，利用自然语言描述作为监督信号，有效提升了面部表情识别的泛化能力和表现，克服了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的面部表情识别系统通常基于预定义的类别或维度进行训练，这种约束限制了模型的泛化能力和应用范围。自然语言提供了更灵活、更具表现力且可解释的情感表示方式，但目前利用自然语言描述作为面部表情表示学习的监督信号的研究尚不充分，主要面临数据集规模不足和缺乏有效利用此类监督信号的框架两大挑战。

Method: 本研究提出了 EmoCap100K 数据集，包含超过 10 万个样本，提供丰富且结构化的语义描述，涵盖全局情感状态和细粒度的面部行为。在此基础上，研究者提出了 EmoCapCLIP 模型，该模型采用联合全局-局部对比学习框架，并辅以跨模态引导的正样本挖掘模块，以充分利用多层次的字幕信息并处理相似表情之间的语义关联。

Result: 通过在超过 20 个涵盖五项任务的基准测试上进行的大量评估，证明了 EmoCapCLIP 方法的优越性能，突显了从大规模语义丰富的字幕中学习面部表情表示的潜力。

Conclusion: 本研究提出的 EmoCapCLIP 方法在多个面部表情识别任务和基准测试中取得了优越的性能，证明了利用大规模、语义丰富的自然语言描述作为监督信号来学习面部表情表征的有效性。

Abstract: Current facial emotion recognition systems are predominately trained to
predict a fixed set of predefined categories or abstract dimensional values.
This constrained form of supervision hinders generalization and applicability,
as it reduces the rich and nuanced spectrum of emotions into oversimplified
labels or scales. In contrast, natural language provides a more flexible,
expressive, and interpretable way to represent emotions, offering a much
broader source of supervision. Yet, leveraging semantically rich natural
language captions as supervisory signals for facial emotion representation
learning remains relatively underexplored, primarily due to two key challenges:
1) the lack of large-scale caption datasets with rich emotional semantics, and
2) the absence of effective frameworks tailored to harness such rich
supervision. To this end, we introduce EmoCap100K, a large-scale facial emotion
caption dataset comprising over 100,000 samples, featuring rich and structured
semantic descriptions that capture both global affective states and
fine-grained local facial behaviors. Building upon this dataset, we further
propose EmoCapCLIP, which incorporates a joint global-local contrastive
learning framework enhanced by a cross-modal guided positive mining module.
This design facilitates the comprehensive exploitation of multi-level caption
information while accommodating semantic similarities between closely related
expressions. Extensive evaluations on over 20 benchmarks covering five tasks
demonstrate the superior performance of our method, highlighting the promise of
learning facial emotion representations from large-scale semantically rich
captions. The code and data will be available at
https://github.com/sunlicai/EmoCapCLIP.

</details>


### [169] [Deep Learning for Skeleton Based Human Motion Rehabilitation Assessment: A Benchmark](https://arxiv.org/abs/2507.21018)
*Ali Ismail-Fawaz,Maxime Devanne,Stefano Berretti,Jonathan Weber,Germain Forestier*

Main category: cs.CV

TL;DR: 该研究通过整合数据集、提出评估框架和进行广泛基准测试，解决了自动化康复评估领域缺乏标准化基准和可重现性方法的问题，旨在促进未来研究和个性化康复解决方案的发展。


<details>
  <summary>Details</summary>
Motivation: 自动化人体运动评估在康复中起着至关重要的作用，但该领域缺乏标准化的基准、一致的评估协议和可重现的方法。

Method: 通过整合现有康复数据集（Rehab-Pile）、提出一个通用的基准框架来评估该领域的深度学习方法，并对多种架构进行广泛的基准测试。

Result: 该研究整合了现有数据集，提出了评估框架，并进行了广泛的基准测试，所有数据集和实现均已公开发布，以支持社区的透明度和可重现性。

Conclusion: 该论文旨在为自动化康复评估奠定坚实基础，促进可靠、可及和个性化的康复解决方案的开发。

Abstract: Automated assessment of human motion plays a vital role in rehabilitation,
enabling objective evaluation of patient performance and progress. Unlike
general human activity recognition, rehabilitation motion assessment focuses on
analyzing the quality of movement within the same action class, requiring the
detection of subtle deviations from ideal motion. Recent advances in deep
learning and video-based skeleton extraction have opened new possibilities for
accessible, scalable motion assessment using affordable devices such as
smartphones or webcams. However, the field lacks standardized benchmarks,
consistent evaluation protocols, and reproducible methodologies, limiting
progress and comparability across studies. In this work, we address these gaps
by (i) aggregating existing rehabilitation datasets into a unified archive
called Rehab-Pile, (ii) proposing a general benchmarking framework for
evaluating deep learning methods in this domain, and (iii) conducting extensive
benchmarking of multiple architectures across classification and regression
tasks. All datasets and implementations are released to the community to
support transparency and reproducibility. This paper aims to establish a solid
foundation for future research in automated rehabilitation assessment and
foster the development of reliable, accessible, and personalized rehabilitation
solutions. The datasets, source-code and results of this article are all
publicly available.

</details>


### [170] [GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset](https://arxiv.org/abs/2507.21033)
*Yuhan Wang,Siwei Yang,Bingchen Zhao,Letian Zhang,Qing Liu,Yuyin Zhou,Cihang Xie*

Main category: cs.CV

TL;DR: 该研究发布了一个包含超过150万个图像编辑样本的大规模、公开数据集GPT-IMAGE-EDIT-1.5M，旨在支持开源研究。该数据集通过优化现有数据集和利用GPT-4o生成，能够显著提升开源模型在图像编辑任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决GPT-4o等先进模型带来的高保真、指令引导的图像编辑新标准与现有开源研究因模型和训练数据专有性而面临的障碍之间的差距。

Method: 通过利用GPT-4o的能力，整合并优化了OmniEdit、HQ-Edit和UltraEdit三个现有数据集，生成了包含超过150万个（指令、源图像、编辑后图像）三元组的数据集。具体方法包括：1）重新生成输出图像以提高视觉质量和指令对齐度；2）重写提示以增强语义清晰度。

Result: 使用GPT-IMAGE-EDIT-1.5M微调后的开源模型（如FluxKontext）在GEdit-EN、ImgEdit-Full和Complex-Edit等基准测试中取得了具有竞争力的表现，显著优于先前所有开源方法，并大幅缩小了与领先的专有模型之间的差距。

Conclusion: GPT-IMAGE-EDIT-1.5M 的发布有望促进指令引导图像编辑领域的进一步开放研究。

Abstract: Recent advancements in large multimodal models like GPT-4o have set a new
standard for high-fidelity, instruction-guided image editing. However, the
proprietary nature of these models and their training data creates a
significant barrier for open-source research. To bridge this gap, we introduce
GPT-IMAGE-EDIT-1.5M, a publicly available, large-scale image-editing corpus
containing more than 1.5 million high-quality triplets (instruction, source
image, edited image). We systematically construct this dataset by leveraging
the versatile capabilities of GPT-4o to unify and refine three popular
image-editing datasets: OmniEdit, HQ-Edit, and UltraEdit. Specifically, our
methodology involves 1) regenerating output images to enhance visual quality
and instruction alignment, and 2) selectively rewriting prompts to improve
semantic clarity. To validate the efficacy of our dataset, we fine-tune
advanced open-source models on GPT-IMAGE-EDIT-1.5M. The empirical results are
exciting, e.g., the fine-tuned FluxKontext achieves highly competitive
performance across a comprehensive suite of benchmarks, including 7.24 on
GEdit-EN, 3.80 on ImgEdit-Full, and 8.78 on Complex-Edit, showing stronger
instruction following and higher perceptual quality while maintaining identity.
These scores markedly exceed all previously published open-source methods and
substantially narrow the gap to leading proprietary models. We hope the full
release of GPT-IMAGE-EDIT-1.5M can help to catalyze further open research in
instruction-guided image editing.

</details>


### [171] [Reconstructing 4D Spatial Intelligence: A Survey](https://arxiv.org/abs/2507.21045)
*Yukang Cao,Jiahao Lu,Zhisheng Huang,Zhuowei Shen,Chengfeng Zhao,Fangzhou Hong,Zhaoxi Chen,Xin Li,Wenping Wang,Yuan Liu,Ziwei Liu*

Main category: cs.CV

TL;DR: 这篇论文对4D空间智能重建进行了分类，提出了五个层次的框架，并讨论了每个层次的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 旨在弥合现有调查在全面分析4D场景重建的层次结构方面的不足，并跟上该领域快速发展的步伐。

Method: 提出了一种新的视角，将现有的4D空间智能重建方法组织成五个渐进的级别：1）低级3D属性重建；2）3D场景组件重建；3）4D动态场景重建；4）场景组件交互建模；5）物理定律和约束的整合。

Result: 对现有4D空间智能重建方法进行了分类和分析，强调了从低级属性到包含物理定律的复杂场景的演变。

Conclusion: 该调查总结了现有方法以五个渐进的4D空间智能级别进行组织，并讨论了每个级别的关键挑战和未来方向。

Abstract: Reconstructing 4D spatial intelligence from visual observations has long been
a central yet challenging task in computer vision, with broad real-world
applications. These range from entertainment domains like movies, where the
focus is often on reconstructing fundamental visual elements, to embodied AI,
which emphasizes interaction modeling and physical realism. Fueled by rapid
advances in 3D representations and deep learning architectures, the field has
evolved quickly, outpacing the scope of previous surveys. Additionally,
existing surveys rarely offer a comprehensive analysis of the hierarchical
structure of 4D scene reconstruction. To address this gap, we present a new
perspective that organizes existing methods into five progressive levels of 4D
spatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes
(e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene
components (e.g., objects, humans, structures); (3) Level 3 -- reconstruction
of 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene
components; and (5) Level 5 -- incorporation of physical laws and constraints.
We conclude the survey by discussing the key challenges at each level and
highlighting promising directions for advancing toward even richer levels of 4D
spatial intelligence. To track ongoing developments, we maintain an up-to-date
project page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [172] [Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media](https://arxiv.org/abs/2507.19511)
*Khalid Hasan,Jamil Saquer,Mukulika Ghosh*

Main category: cs.CL

TL;DR: transformer模型（特别是RoBERTa）在Reddit数据上进行精神健康障碍检测方面优于LSTM，但集成BERT嵌入的LSTM模型在计算效率和性能之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 随着精神健康障碍患病率的上升，开发用于早期检测和监测的自动化工具变得至关重要。自然语言处理（NLP）的最新进展，特别是transformer架构，在文本分析方面显示出巨大潜力。

Method: 本研究评估了包括BERT、RoBERTa、DistilBERT、ALBERT和ELECTRA在内的多种transformer模型，并与基于长短期记忆（LSTM）的方法进行了比较，同时使用了不同的文本嵌入技术，并在Reddit数据集上进行了精神健康障碍分类。

Result: 实验结果表明，transformer模型在精神健康障碍分类任务上显著优于传统的深度学习方法。RoBERTa模型在内部测试集上达到了99.54%的F1分数，在外部测试集上达到了96.05%的F1分数，表现出最高的分类性能。此外，集成了BERT嵌入的LSTM模型也表现出强大的竞争力，在外部数据集上F1分数超过94%，同时计算资源消耗却显著较低。

Conclusion: transformer模型在精神健康障碍检测方面表现优于传统深度学习方法，其中RoBERTa表现最佳，但结合了BERT嵌入的LSTM模型在计算资源需求较低的情况下也具有竞争力，这为实时、可扩展的精神健康监测提供了有效性证明。

Abstract: The rising prevalence of mental health disorders necessitates the development
of robust, automated tools for early detection and monitoring. Recent advances
in Natural Language Processing (NLP), particularly transformer-based
architectures, have demonstrated significant potential in text analysis. This
study provides a comprehensive evaluation of state-of-the-art transformer
models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term
Memory (LSTM) based approaches using different text embedding techniques for
mental health disorder classification on Reddit. We construct a large annotated
dataset, validating its reliability through statistical judgmental analysis and
topic modeling. Experimental results demonstrate the superior performance of
transformer models over traditional deep-learning approaches. RoBERTa achieved
the highest classification performance, with a 99.54% F1 score on the hold-out
test set and a 96.05% F1 score on the external test set. Notably, LSTM models
augmented with BERT embeddings proved highly competitive, achieving F1 scores
exceeding 94% on the external dataset while requiring significantly fewer
computational resources. These findings highlight the effectiveness of
transformer-based models for real-time, scalable mental health monitoring. We
discuss the implications for clinical applications and digital mental health
interventions, offering insights into the capabilities and limitations of
state-of-the-art NLP methodologies in mental disorder detection.

</details>


### [173] [Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables](https://arxiv.org/abs/2507.19521)
*Vishakh Padmakumar,Joseph Chee Chang,Kyle Lo,Doug Downey,Aakanksha Naik*

Main category: cs.CL

TL;DR: 本研究解决了学术文献比较模式生成中的主要挑战：评估歧义和缺乏编辑方法。通过创建包含综合意图的新数据集，并引入LLM编辑技术，我们提高了模式生成的准确性和效率，证明了即使是小型模型也能通过微调和有效编辑来达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 随着学术文献数量的不断增长，研究人员需要一种有效的方式来组织、比较和对比大量的文献。大型语言模型（LLMs）在生成用于比较论文的共享模式方面具有潜力，但目前在模式生成方面进展缓慢，主要是因为（一）基于参考的评估存在歧义，以及（二）缺乏编辑/精炼方法。

Method: 1. 提出一种为无注释表格语料库添加综合意图的方法，并利用该方法创建了一个数据集，用于研究基于给定信息需求的模式生成，从而减少歧义。2. 提出几种LLM-based模式编辑技术。3. 对比了多种单次模式生成方法（包括提示LLM工作流和微调模型），证明了小型、开放权重模型在微调后可以与最先进的提示LLM相媲美。4. 展示了编辑技术可以进一步改进这些方法生成的模式。

Result: 通过引入表格意图，显著提高了在重建参考模式方面的基线性能。我们提出的编辑技术可以进一步改进现有生成方法的效果。

Conclusion: 我们的工作首次解决了学术文献比较模式生成中的歧义和缺乏编辑/精炼方法的问题，通过引入带综合意图的表格语料库、构建新的数据集来减少歧义，并提出基于LLM的模式编辑技术，证明了小型模型经过微调后也能具有竞争力，并且编辑技术可以进一步提升生成模式的质量。

Abstract: The increasing volume of academic literature makes it essential for
researchers to organize, compare, and contrast collections of documents. Large
language models (LLMs) can support this process by generating schemas defining
shared aspects along which to compare papers. However, progress on schema
generation has been slow due to: (i) ambiguity in reference-based evaluations,
and (ii) lack of editing/refinement methods. Our work is the first to address
both issues. First, we present an approach for augmenting unannotated table
corpora with synthesized intents and apply it to create a dataset for studying
schema generation conditioned on a given information need, thus reducing
ambiguity. With this dataset, we show how incorporating table intents
significantly improves baseline performance in reconstructing reference
schemas. Next, we propose several LLM-based schema editing techniques. We start
by comprehensively benchmarking several single-shot schema generation methods,
including prompted LLM workflows and fine-tuned models, showing that smaller,
open-weight models can be fine-tuned to be competitive with state-of-the-art
prompted LLMs. Then we demonstrate that our editing techniques can further
improve schemas generated by these methods.

</details>


### [174] [Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri](https://arxiv.org/abs/2507.19537)
*Felix Kraus,Nicolas Blumenröhr,Danah Tonne,Achim Streit*

Main category: cs.CL

TL;DR: WOKIE是一个开源的自动化工具，用于翻译SKOS词汇表，以提高数字人文领域的语言可访问性和互操作性。


<details>
  <summary>Details</summary>
Motivation: 该项目解决了数字人文（DH）领域的一个关键需求，即语言多样性可能会限制知识资源的访问、重用和语义互操作性。WOKIE旨在通过自动化翻译来解决这个问题，以提高词汇表的可访问性、可重用性和跨语言互操作性。

Method: WOKIE结合了外部翻译服务和使用大型语言模型（LLMs）进行的有针对性的优化，从而在翻译质量、可扩展性和成本之间取得了平衡。该应用程序设计运行在通用硬件上，易于扩展，并且不需要机器翻译或LLM方面的专业知识。

Result: WOKIE在15种语言的多个DH词汇表上进行了评估，结果表明该工具能够显著提高词汇表的可访问性、可重用性和跨语言互操作性。

Conclusion: WOKIE通过无障碍的自动化翻译和改进的本体匹配性能，提高了词汇表的可访问性、可重用性和跨语言互操作性，从而支持更具包容性和多语言的研究基础设施。

Abstract: We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for
the automated translation of SKOS thesauri. This work addresses a critical need
in the Digital Humanities (DH), where language diversity can limit access,
reuse, and semantic interoperability of knowledge resources. WOKIE combines
external translation services with targeted refinement using Large Language
Models (LLMs), balancing translation quality, scalability, and cost. Designed
to run on everyday hardware and be easily extended, the application requires no
prior expertise in machine translation or LLMs. We evaluate WOKIE across
several DH thesauri in 15 languages with different parameters, translation
services and LLMs, systematically analysing translation quality, performance,
and ontology matching improvements. Our results show that WOKIE is suitable to
enhance the accessibility, reuse, and cross-lingual interoperability of
thesauri by hurdle-free automated translation and improved ontology matching
performance, supporting more inclusive and multilingual research
infrastructures.

</details>


### [175] [Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning](https://arxiv.org/abs/2507.19586)
*Shengyuan Wang,Jie Feng,Tianhui Liu,Dan Pei,Yong Li*

Main category: cs.CL

TL;DR: LLM在地理空间任务中存在幻觉，我们提出了评估和缓解幻觉的方法，并取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: LLM在地理空间任务中存在地理空间幻觉问题，现有研究缺乏对其进行系统评估和缓解的方法。

Method: 提出一个利用结构化地理空间知识图谱进行评估的地理空间幻觉评估框架，并引入一种基于KTO的动态事实校准方法来缓解地理空间幻觉。

Result: 通过在20个先进LLM上的评估，揭示了它们的地理空间知识幻觉，所提出的方法可将性能提高超过29.6%。

Conclusion: LLM在地理空间任务中表现出地理空间幻觉，一个新颖的评估框架和基于KTO的动态事实校准方法可以提高LLM在地理空间知识和推理任务中的可靠性。

Abstract: Large language models (LLMs) possess extensive world knowledge, including
geospatial knowledge, which has been successfully applied to various geospatial
tasks such as mobility prediction and social indicator prediction. However,
LLMs often generate inaccurate geospatial knowledge, leading to geospatial
hallucinations (incorrect or inconsistent representations of geospatial
information) that compromise their reliability. While the phenomenon of general
knowledge hallucination in LLMs has been widely studied, the systematic
evaluation and mitigation of geospatial hallucinations remain largely
unexplored. To address this gap, we propose a comprehensive evaluation
framework for geospatial hallucinations, leveraging structured geospatial
knowledge graphs for controlled assessment. Through extensive evaluation across
20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge.
Building on these insights, we introduce a dynamic factuality aligning method
based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial
hallucinations in LLMs, leading to a performance improvement of over 29.6% on
the proposed benchmark. Extensive experimental results demonstrate the
effectiveness of our benchmark and learning algorithm in enhancing the
trustworthiness of LLMs in geospatial knowledge and reasoning tasks.

</details>


### [176] [Efficient Attention Mechanisms for Large Language Models: A Survey](https://arxiv.org/abs/2507.19595)
*Yutao Sun,Zhenyu Li,Yike Zhang,Tengyu Pan,Bowen Dong,Yuyi Guo,Jianyong Wang*

Main category: cs.CL

TL;DR: Transformer的自注意力机制效率低下。线性注意力和稀疏注意力是解决此问题的两种主要方法。本研究对这些方法进行了全面概述，并探讨了它们在大型语言模型中的应用。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在大型语言模型中已成为主流，但其自注意力机制的二次时间和内存复杂度是高效长上下文建模的障碍。为了解决这一限制，需要研究高效的注意力机制。

Method: 对Transformer模型中自注意力机制的二次时间与内存复杂度问题进行了分析，并介绍了线性注意力和稀疏注意力这两种主要的高效注意力机制。论文系统性地概述了这些机制的算法创新和硬件级考量，并分析了它们在预训练语言模型中的应用。

Result: 论文对线性注意力和稀疏注意力机制进行了全面的概述，并分析了它们在大型预训练语言模型中的应用，为可扩展且高效的语言模型设计提供了基础。

Conclusion: 该论文旨在为可扩展且高效的语言模型的设计提供基础参考，通过整合算法创新和硬件级考量，并分析了高效注意力机制在预训练语言模型中的应用。

Abstract: Transformer-based architectures have become the prevailing backbone of large
language models. However, the quadratic time and memory complexity of
self-attention remains a fundamental obstacle to efficient long-context
modeling. To address this limitation, recent research has introduced two
principal categories of efficient attention mechanisms. Linear attention
methods achieve linear complexity through kernel approximations, recurrent
formulations, or fastweight dynamics, thereby enabling scalable inference with
reduced computational overhead. Sparse attention techniques, in contrast, limit
attention computation to selected subsets of tokens based on fixed patterns,
block-wise routing, or clustering strategies, enhancing efficiency while
preserving contextual coverage. This survey provides a systematic and
comprehensive overview of these developments, integrating both algorithmic
innovations and hardware-level considerations. In addition, we analyze the
incorporation of efficient attention into largescale pre-trained language
models, including both architectures built entirely on efficient attention and
hybrid designs that combine local and global components. By aligning
theoretical foundations with practical deployment strategies, this work aims to
serve as a foundational reference for advancing the design of scalable and
efficient language models.

</details>


### [177] [MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?](https://arxiv.org/abs/2507.19598)
*Muntasir Wahed,Xiaona Zhou,Kiet A. Nguyen,Tianjiao Yu,Nirav Diwan,Gang Wang,Dilek Hakkani-Tür,Ismini Lourentzou*

Main category: cs.CL

TL;DR: LLM 在代码生成方面虽然进步显著，但在多轮恶意攻击下仍存在漏洞。代码分解攻击和 \benchmarkname{} 基准的提出，以及 MOCHA 微调的有效性，为提高 LLM 的鲁棒性提供了新的途径。


<details>
  <summary>Details</summary>
Motivation: 评估 LLM 在代码生成方面的鲁棒性，特别是在面对多轮恶意代码提示时，这是一个未被充分探索的领域。

Method: 提出了一个名为 \benchmarkname{} 的大规模基准，用于评估代码 LLM 针对单轮和多轮恶意提示的鲁棒性。

Result: 在开源和闭源模型上的实验表明，LLM 仍然存在漏洞，尤其是在多轮场景下。使用 MOCHA 进行微调可以提高拒绝率，同时保持编码能力，并增强在外部对抗性数据集上的鲁棒性。

Conclusion: LLM 在代码生成方面取得了显著进步，但仍易受多轮恶意代码提示的对抗性攻击。引入了代码分解攻击，即将恶意任务分解为一系列看似良性的子任务。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
enhanced their code generation capabilities. However, their robustness against
adversarial misuse, particularly through multi-turn malicious coding prompts,
remains underexplored. In this work, we introduce code decomposition attacks,
where a malicious coding task is broken down into a series of seemingly benign
subtasks across multiple conversational turns to evade safety filters. To
facilitate systematic evaluation, we introduce \benchmarkname{}, a large-scale
benchmark designed to evaluate the robustness of code LLMs against both
single-turn and multi-turn malicious prompts. Empirical results across open-
and closed-source models reveal persistent vulnerabilities, especially under
multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while
preserving coding ability, and importantly, enhances robustness on external
adversarial datasets with up to 32.4% increase in rejection rates without any
additional supervision.

</details>


### [178] [HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track](https://arxiv.org/abs/2507.19616)
*Xuchen Wei,Yangxin Wu,Yaoyin Zhang,Henglyu Liu,Kehai Chen,Xuefeng Bai,Min Zhang*

Main category: cs.CL

TL;DR: HITSZ提交的IWSZ 2025印度语任务的ST系统，结合了Whisper ASR和Krutrim LLM，在英译印和印译英方面取得了28.88和27.86的平均BLEU分数。CoT方法有潜力提高翻译质量，但在格式一致性方面存在挑战。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于提高低资源场景下英语-印度语和印度语-英语语言对的语音-文本转换（ST）质量，并探索思维链（CoT）方法在该任务中的潜力。

Method: 本研究提出了一种端到端的语音-文本转换（ST）系统，该系统整合了预训练的Whisper ASR模型和Krutrim LLM，以处理低资源场景下的英语-印度语和印度语-英语语言对。研究还探索了思维链（CoT）方法及其在翻译质量提升方面的应用。

Result: 该端到端系统在英译印方向上的平均BLEU分数为28.88，在印译英方向上的平均BLEU分数为27.86。思维链（CoT）方法在成功解析的输出上显示出翻译质量提高的潜力（例如，泰米尔语到英语的BLEU分数提高了13.84），但在确保模型一致遵循所需的CoT输出格式方面存在挑战。

Conclusion: 该研究展示了HITSZ在IWSLT 2025印度语任务的语音-文本转换（ST）方面的提交工作，并提出了一种端到端的系统，该系统整合了预训练的Whisper自动语音识别（ASR）模型和专注于印度语的Krutrim大语言模型（LLM），以提高低资源场景下的翻译质量。实验结果表明，该端到端系统在英译印方向上的平均BLEU分数为28.88，在印译英方向上的平均BLEU分数为27.86。此外，研究还探讨了思维链（CoT）方法，该方法在成功解析的输出上显示出显著提高翻译质量的潜力（例如，泰米尔语到英语的BLEU分数提高了13.84），但同时也观察到在确保模型一致遵循所需的CoT输出格式方面存在挑战。

Abstract: This paper presents HITSZ's submission for the IWSLT 2025 Indic track,
focusing on speech-to-text translation (ST) for English-to-Indic and
Indic-to-English language pairs. To enhance translation quality in this
low-resource scenario, we propose an end-to-end system integrating the
pre-trained Whisper automated speech recognition (ASR) model with Krutrim, an
Indic-specialized large language model (LLM). Experimental results demonstrate
that our end-to-end system achieved average BLEU scores of $28.88$ for
English-to-Indic directions and $27.86$ for Indic-to-English directions.
Furthermore, we investigated the Chain-of-Thought (CoT) method. While this
method showed potential for significant translation quality improvements on
successfully parsed outputs (e.g. a $13.84$ BLEU increase for
Tamil-to-English), we observed challenges in ensuring the model consistently
adheres to the required CoT output format.

</details>


### [179] [MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks](https://arxiv.org/abs/2507.19634)
*Sara Papi,Maike Züfle,Marco Gaido,Beatrice Savoldi,Danni Liu,Ioannis Douros,Luisa Bentivogli,Jan Niehues*

Main category: cs.CL

TL;DR: MCIF是一个包含语音、视觉、文本的多语言（英语、德语、意大利语、中文）基准，用于评估多模态大语言模型在长短文本、跨语言和多模态指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估多模态大语言模型的跨语言和多模态能力时存在不足，通常仅限于英语、侧重单一模态、依赖短文本，或缺乏人工标注，阻碍了对模型在不同语言、模态和任务复杂度下性能的全面评估。

Method: MCIF包含语音、视觉和文本三种核心模态，以及英语、德语、意大利语和中文四种语言，可以全面评估多模态大语言模型在跨语言和结合多模态上下文信息理解指令的能力。

Result: MCIF是首个基于科学讲座的多语言、人工标注的基准，旨在评估跨语言、多模态设置下，模型对长短输入指令遵循能力。

Conclusion: MCIF是首个基于科学讲座的多语言、人工标注的基准，旨在评估跨语言、多模态设置下，模型对长短输入指令遵循能力。MCIF包含语音、视觉和文本三种核心模态，以及英语、德语、意大利语和中文四种语言，可以全面评估多模态大语言模型在跨语言和结合多模态上下文信息理解指令的能力。MCIF已在CC-BY 4.0许可下发布，以促进多模态大语言模型领域的开放研究。

Abstract: Recent advances in large language models have catalyzed the development of
multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified
frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to
general-purpose instruction-following models, a key frontier lies in evaluating
their multilingual and multimodal capabilities over both long and short
contexts. However, existing benchmarks fall short in evaluating these
dimensions jointly: they are often limited to English, mostly focus on one
single modality at a time, rely on short-form contexts, or lack human
annotations -- hindering comprehensive assessment of model performance across
languages, modalities, and task complexity. To address these gaps, we introduce
MCIF (Multimodal Crosslingual Instruction Following), the first multilingual
human-annotated benchmark based on scientific talks that is designed to
evaluate instruction-following in crosslingual, multimodal settings over both
short- and long-form inputs. MCIF spans three core modalities -- speech,
vision, and text -- and four diverse languages (English, German, Italian, and
Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret
instructions across languages and combine them with multimodal contextual
information. MCIF is released under a CC-BY 4.0 license to encourage open
research and progress in MLLMs development.

</details>


### [180] [FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models](https://arxiv.org/abs/2507.20924)
*Roberto Labadie-Tamayo,Adrian Jaques Böck,Djordje Slijepčević,Xihui Chen,Andreas Babic,Matthias Zeppelzauer*

Main category: cs.CL

TL;DR: 本文提出并评估了三种用于在社交媒体文本中识别和分类性别歧视的模型（SCBM、SCBMT和XLM-RoBERTa），以应对EXIST 2025挑战赛。XLM-RoBERTa和SCBMT模型取得了具有竞争力的结果，并且SCBM/SCBMT模型提供了可解释性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体和在线对话中的性别歧视问题日益普遍，因此需要开发有效的工具来识别和解决这一问题。EXIST挑战赛旨在推动相关技术的发展。

Method: 本文介绍了针对EXIST 2025挑战赛第一任务的解决方案，该任务旨在识别和分类社交媒体文本帖子中的性别歧视。作者实现了三个模型：语音概念瓶颈模型（SCBM）、结合Transformer的语音概念瓶颈模型（SCBMT）以及微调的XLM-RoBERTa Transformer模型。SCBM使用描述性形容词作为可解释的概念瓶颈，将输入文本编码为人类可解释的形容词表示，并训练一个轻量级分类器。SCBMT通过融合基于形容词的表示和Transformer的上下文嵌入来扩展SCBM，以平衡可解释性和分类性能。此外，作者还研究了如何利用注释者的人口统计信息等附加元数据。

Result: 在子任务1.1（推文中性别歧视的识别）中，经过额外数据集增强和微调的XLM-RoBERTa模型在英语和西班牙语的Soft-Soft评估中分别获得第六名和第四名。SCBMT模型在英语和西班牙语中的排名分别为第七名和第六名。SCBM和SCBMT模型能够提供细粒度的解释。

Conclusion: XLM-RoBERTa模型在包含额外数据集的数据增强后，在英语和西班牙语的Soft-Soft评估中分别排名第六和第四。SCBMT模型在英语和西班牙语中分别排名第七和第六。这表明所提出的模型在识别和分类社交媒体中的性别歧视方面具有竞争力，同时SCBM和SCBMT模型还提供了可解释性。

Abstract: Sexism has become widespread on social media and in online conversation. To
help address this issue, the fifth Sexism Identification in Social Networks
(EXIST) challenge is initiated at CLEF 2025. Among this year's international
benchmarks, we concentrate on solving the first task aiming to identify and
classify sexism in social media textual posts. In this paper, we describe our
solutions and report results for three subtasks: Subtask 1.1 - Sexism
Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask
1.3 - Sexism Categorization in Tweets. We implement three models to address
each subtask which constitute three individual runs: Speech Concept Bottleneck
Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a
fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as
human-interpretable bottleneck concepts. SCBM leverages large language models
(LLMs) to encode input texts into a human-interpretable representation of
adjectives, then used to train a lightweight classifier for downstream tasks.
SCBMT extends SCBM by fusing adjective-based representation with contextual
embeddings from transformers to balance interpretability and classification
performance. Beyond competitive results, these two models offer fine-grained
explanations at both instance (local) and class (global) levels. We also
investigate how additional metadata, e.g., annotators' demographic profiles,
can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data
augmented with prior datasets, ranks 6th for English and Spanish and 4th for
English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and
Spanish and 6th for Spanish.

</details>


### [181] [RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams](https://arxiv.org/abs/2507.19666)
*Andrei Vlad Man,Răzvan-Alexandru Smădu,Cristian-George Craciun,Dumitru-Clementin Cercel,Florin Pop,Mihaela-Claudia Cercel*

Main category: cs.CL

TL;DR: 本研究评估了LLM和VLM在罗马尼亚驾驶法律教育中的应用。我们创建了一个名为RoD-TAL的多模态数据集，并测试了不同的模型和技术。结果显示，特定领域的微调和链式思考提示提高了文本任务的性能，但视觉任务仍有待改进。


<details>
  <summary>Details</summary>
Motivation: AI和法律系统的交叉领域，日益需要支持法律教育的工具，特别是在罗马尼亚语等资源匮乏的语言中。

Method: 通过文本和图像问答任务，评估大型语言模型（LLM）和视觉语言模型（VLM）在理解和推理罗马尼亚驾驶法律方面的能力。引入了一个新的多模态数据集RoD-TAL，包含文本和图像的罗马尼亚驾驶测试问题、注释的法律参考文献和人工解释。实现了检索增强生成（RAG）管道、密集检索器和针对推理优化的模型，并评估了它们在信息检索（IR）、问答（QA）、视觉IR和视觉QA等任务上的表现。

Result: 实验表明，特定领域的微调显著提高了检索性能，链式思考提示和专门的推理模型提高了QA准确性，超过了通过驾驶考试的最低分数。然而，视觉推理仍然具有挑战性。

Conclusion: 大型语言模型（LLM）和视觉语言模型（VLM）在理解和推理罗马尼亚驾驶法律方面具有巨大潜力，尤其是在特定领域的微调和链式思考提示方面表现出色。然而，视觉推理仍然是一个挑战，这表明了将这些模型应用于法律教育的潜力和局限性。

Abstract: The intersection of AI and legal systems presents a growing need for tools
that support legal education, particularly in under-resourced languages such as
Romanian. In this work, we aim to evaluate the capabilities of Large Language
Models (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning
about Romanian driving law through textual and visual question-answering tasks.
To facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising
Romanian driving test questions, text-based and image-based, alongside
annotated legal references and human explanations. We implement and assess
retrieval-augmented generation (RAG) pipelines, dense retrievers, and
reasoning-optimized models across tasks including Information Retrieval (IR),
Question Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate
that domain-specific fine-tuning significantly enhances retrieval performance.
At the same time, chain-of-thought prompting and specialized reasoning models
improve QA accuracy, surpassing the minimum grades required to pass driving
exams. However, visual reasoning remains challenging, highlighting the
potential and the limitations of applying LLMs and VLMs to legal education.

</details>


### [182] [Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks](https://arxiv.org/abs/2507.19699)
*Maitha Alshehhi,Ahmed Sharshar,Mohsen Guizani*

Main category: cs.CL

TL;DR: 该研究评估了多语言和单一语言大型语言模型在低资源语言（如阿拉伯语和卡纳达语）上的表现，并研究了模型压缩（剪枝和量化）的影响。结果表明，多语言模型在跨语言迁移中表现更好，量化有助于提高效率同时保持准确性，而剪枝则会损害性能。研究强调了解决低资源语言模型幻觉和泛化问题的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在高资源语言中取得了显著成功，但它们在卡纳达语和阿拉伯语等低资源语言环境中的能力尚未完全了解。

Method: 通过对阿拉伯语、英语和印度语言（重点关注卡纳达语）的基准测试，评估了多语言和单一语言大型语言模型（LLMs）的性能，并特别研究了模型压缩策略（如剪枝和量化）的影响。

Result: 在评估的多语言和单一语言大型语言模型（LLMs）中，多语言模型在所有语言上的表现均优于其单一语言对应模型。量化（4位和8位）在提高效率的同时能有效保持模型准确性，但激进的剪枝策略（尤其是在大型模型上）会显著降低模型性能。

Conclusion: 多语言模型在跨语言迁移方面表现优于单一语言模型，尤其是在低资源语言环境中。模型压缩技术（如量化）可以在提高效率的同时保持模型准确性，但过度剪枝会损害模型性能。研究结果为构建可扩展、公平的多语言自然语言处理解决方案提供了关键策略，并强调了解决低资源环境下模型幻觉和泛化错误问题的必要性。

Abstract: Although LLMs have attained significant success in high-resource languages,
their capacity in low-resource linguistic environments like Kannada and Arabic
is not yet fully understood. This work benchmarking the performance of
multilingual and monolingual Large Language Models (LLMs) across Arabic,
English, and Indic languages, with particular emphasis on the effects of model
compression strategies such as pruning and quantization. Findings shows
significant performance differences driven by linguistic diversity and resource
availability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2.
We find that multilingual versions of the model outperform their
language-specific counterparts across the board, indicating substantial
cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in
maintaining model accuracy while promoting efficiency, but aggressive pruning
significantly compromises performance, especially in bigger models. Our
findings pinpoint key strategies to construct scalable and fair multilingual
NLP solutions and underscore the need for interventions to address
hallucination and generalization errors in the low-resource setting.

</details>


### [183] [Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs](https://arxiv.org/abs/2507.19710)
*Ronak Upasham,Tathagata Dey,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 介绍了一种新颖的表格到文本生成流水线，通过利用RDF三元组和多阶段处理来生成包含主观性的文本，并在性能上超越了部分大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有表格到文本生成方法主要关注客观描述，而包含超越原始数值数据的解释的主观性表达则探索不足。

Method: 提出一个三阶段流水线：1）提取资源描述框架（RDF）三元组；2）将文本聚合成连贯的叙述；3）注入主观性以丰富生成文本。该流水线使用经过微调的小型T5模型。

Result: 与GPT-3.5相比，该方法实现了相当的性能，并在多项指标上超越了Mistral-7B和Llama-2。通过定量和定性分析证明了其在平衡事实准确性和主观解释方面的有效性。

Conclusion: 该方法通过整合RDF三元组、文本聚合和主观性注入，在表格到文本生成任务中实现了事实准确性和主观性表达的平衡，并且在多项指标上优于Mistral-7B和Llama-2，表现与GPT-3.5相当。

Abstract: In Table-to-Text (T2T) generation, existing approaches predominantly focus on
providing objective descriptions of tabular data. However, generating text that
incorporates subjectivity, where subjectivity refers to interpretations beyond
raw numerical data, remains underexplored. To address this, we introduce a
novel pipeline that leverages intermediate representations to generate both
objective and subjective text from tables. Our three-stage pipeline consists
of: 1) extraction of Resource Description Framework (RDF) triples, 2)
aggregation of text into coherent narratives, and 3) infusion of subjectivity
to enrich the generated text. By incorporating RDFs, our approach enhances
factual accuracy while maintaining interpretability. Unlike large language
models (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs
smaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5
and outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our
approach through quantitative and qualitative analyses, demonstrating its
effectiveness in balancing factual accuracy with subjective interpretation. To
the best of our knowledge, this is the first work to propose a structured
pipeline for T2T generation that integrates intermediate representations to
enhance both factual correctness and subjectivity.

</details>


### [184] [Basic Reading Distillation](https://arxiv.org/abs/2507.19741)
*Zhi Zhou,Sirui Miao,Xiangyu Duan,Hao Yang,Min Zhang*

Main category: cs.CL

TL;DR: 提出基础阅读蒸馏（BRD）方法，通过在基础文本上训练小型模型模仿LLM的阅读行为，使其在下游任务中表现优于大模型，且不影响其他蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 现有的蒸馏技术（知识蒸馏或任务蒸馏）在训练小型模型模仿LLM的特定特征时，忽略了在与下游任务无关的通用文本上对小型模型进行基础阅读能力的基础教育。

Method: 提出一种名为基础阅读蒸馏（BRD）的新方法，该方法着重于通过在单个句子上模仿LLM的基础阅读行为（如命名实体识别、提问和回答）来训练小型模型。

Result: BRD能够有效影响小型模型的概率分布，并且在下游任务（包括语言推理基准和BIG-bench任务）中，接受BRD训练的小型模型的表现能够优于或媲美规模大20倍以上的LLMs。

Conclusion: 所提出方法BRD有效，能够使小型模型在下游任务中表现优于或媲美规模大20倍以上的LLMs，并且BRD对知识蒸馏或任务蒸馏具有正交性。

Abstract: Large language models (LLMs) have demonstrated remarkable abilities in
various natural language processing areas, but they demand high computation
resources which limits their deployment in real-world. Distillation is one
technique to solve this problem through either knowledge distillation or task
distillation. Both distillation approaches train small models to imitate
specific features of LLMs, but they all neglect basic reading education for
small models on generic texts that are \emph{unrelated} to downstream tasks. In
this paper, we propose basic reading distillation (BRD) which educates a small
model to imitate LLMs basic reading behaviors, such as named entity
recognition, question raising and answering, on each sentence. After such basic
education, we apply the small model on various tasks including language
inference benchmarks and BIG-bench tasks. It shows that the small model can
outperform or perform comparable to over 20x bigger LLMs. Analysis reveals that
BRD effectively influences the probability distribution of the small model, and
has orthogonality to either knowledge distillation or task distillation.

</details>


### [185] [JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2507.19748)
*Yifan Hao,Fangning Chao,Yaqian Hao,Zhaojun Cui,Huan Bai,Haiyu Zhang,Yankai Liu,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: JT-Math-8B 是一个开源的大型语言模型系列，旨在提高数学推理能力，通过多阶段优化和长链式思考方法，在数学竞赛任务中超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型在处理需要深度概念理解和复杂多步推理的复杂数学问题时遇到的困难。

Method: 通过一个系统化的多阶段优化框架，包括一个包含 2100 亿 token 的高质量预训练语料库（通过基于模型的验证进行策划），以及一个结合了监督微调（SFT）和基于 GRPO 的强化学习（RL）的指令模型。该思考模型则采用长链式思考（Long CoT）方法，结合 SFT 和新颖的多阶段 RL 课程，逐步增加任务难度和上下文长度（高达 32K token）。

Result: JT-Math-8B 在竞赛级数学方面取得了最先进的成果，优于 O1-mini 和 GPT-4o 等模型。

Conclusion: JT-Math-8B 在相似规模的开源模型中取得了最先进的成果，在竞赛级数学方面表现出色，超越了 O1-mini 和 GPT-4o 等知名模型。

Abstract: Mathematical reasoning is a cornerstone of artificial general intelligence
and a primary benchmark for evaluating the capabilities of Large Language
Models (LLMs). While state-of-the-art models show promise, they often falter
when faced with complex problems that demand deep conceptual understanding and
intricate, multi-step deliberation. To address this challenge, we introduce
JT-Math-8B, a series of open-source models comprising base, instruct, and
thinking versions, built upon a systematic, multi-stage optimization framework.
Our pre-training corpus is a high-quality, 210B-token dataset curated through a
dedicated data pipeline that uses model-based validation to ensure quality and
diversity. The Instruct Model is optimized for direct, concise answers through
Supervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL)
method. The Thinking Model is trained for complex problem-solving using a Long
Chain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage
RL curriculum that progressively increases task difficulty and context length
up to 32K tokens. JT-Math-8B achieves state-of-the-art results among
open-source models of similar size, surpassing prominent models like OpenAI's
O1-mini and GPT-4o , and demonstrating superior performance on
competition-level mathematics.

</details>


### [186] [Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs](https://arxiv.org/abs/2507.19756)
*Rebecca M. M. Hicke,Brian Haggard,Mia Ferrante,Rayhan Khanna,David Mimno*

Main category: cs.CL

TL;DR: 本研究使用计算工具分析了基督教文学中“神的作为”的描绘，并发现与“末日启示录”系列以及男性作者的作品相比，更广泛的基督教文学和女性作者的作品在这一方面存在差异。


<details>
  <summary>Details</summary>
Motivation: 许多研究集中于“末日启示录”系列，而忽略了基督教文学的其他方面。本研究旨在提供一个更广泛的图像，并探讨作者如何描绘“神的作为”。

Method: 本研究首先与人工标注员合作，制定了“神的作为”的定义和编码簿，然后将这些指令改编，由小型语言模型在大型模型的辅助下进行标注，最后利用标注结果进行分析。

Result: 研究发现，“末日启示集团”系列和更广泛的基督教文学之间，以及男性和女性作者的作品之间，在“神的作为”的描绘上存在显著差异。

Conclusion: 美国福音派基督教文学的学术研究较少，本研究利用计算工具分析了“神的作为”在基督教文学中的体现，并与《末日启示录》系列以及男性和女性作者的作品进行了比较。

Abstract: In addition to its more widely studied political activities, the American
Evangelical movement has a well-developed but less externally visible cultural
and literary side. Christian Fiction, however, has been little studied, and
what scholarly attention there is has focused on the explosively popular Left
Behind series. In this work, we use computational tools to provide both a broad
topical overview of Christian Fiction as a genre and a more directed
exploration of how its authors depict divine acts. Working with human
annotators we first developed definitions and a codebook for "acts of God." We
then adapted those instructions designed for human annotators for use by a
recent, lightweight LM with the assistance of a much larger model. The
laptop-scale LM is capable of matching human annotations, even when the task is
subtle and challenging. Using these annotations, we show that significant and
meaningful differences exist between the Left Behind books and Christian
Fiction more broadly and between books by male and female authors.

</details>


### [187] [UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities](https://arxiv.org/abs/2507.19766)
*Dong Du,Shulin Liu,Tao Yang,Shaohua Chen,Yang Li*

Main category: cs.CL

TL;DR: 通过分段处理和动态掩码，UloRL提高了超长输出强化学习的训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习框架在处理超长输出时效率低下，存在长尾序列分布和训练熵崩溃问题，阻碍了大型语言模型推理能力的提升。

Method: 提出了一种超长输出强化学习（UloRL）方法，将超长输出解码划分为短片段，并采用动态掩码策略来防止熵崩溃。

Result: 在Qwen3-30B-A3B模型上，采用分段滚动的强化学习训练速度提高了2.06倍；在128k token输出下，AIME2025的性能从70.9%提升至85.1%，BeyondAIME从50.7%提升至61.9%，优于Qwen3-235B-A22B模型。

Conclusion: 该方法在超长序列生成方面显著提升了大型语言模型的推理能力，并在AIME2025和BeyondAIME基准测试中取得了优于现有模型的性能。

Abstract: Recent advances in large language models (LLMs) have highlighted the
potential of reinforcement learning with verifiable rewards (RLVR) to enhance
reasoning capabilities through extended output sequences. However, traditional
RL frameworks face inefficiencies when handling ultra-long outputs due to
long-tail sequence distributions and entropy collapse during training. To
address these challenges, we propose an Ultra-Long Output Reinforcement
Learning (UloRL) approach for advancing large language models' reasoning
abilities. Specifically, we divide ultra long output decoding into short
segments, enabling efficient training by mitigating delays caused by long-tail
samples. Additionally, we introduce dynamic masking of well-Mastered Positive
Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the
effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment
rollout achieved 2.06x increase in training speed, while RL training with
128k-token outputs improves the model's performance on AIME2025 from 70.9\% to
85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B
with remarkable gains. These findings underscore the potential of our methods
to advance the reasoning capabilities of LLMs with ultra-long sequence
generation. We will release our code and model for further use by the
community.

</details>


### [188] [Flora: Effortless Context Construction to Arbitrary Length and Scale](https://arxiv.org/abs/2507.19786)
*Tianxiang Chen,Zhentao Tan,Xiaofan Bo,Yue Wu,Tao Gong,Qi Chu,Jieping Ye,Nenghai Yu*

Main category: cs.CL

TL;DR: Flora is a new, easy way to make LLMs better at handling long texts without messing up their short-text skills. It combines short instructions into longer ones, creating diverse and long contexts efficiently. It works well in tests.


<details>
  <summary>Details</summary>
Motivation: Existing methods for constructing long contexts for LLM instruction tuning are costly (requiring LLMs or human intervention), limited in length and diversity, and lead to significant drops in short-context performance. There is a need for an effortless and effective method to enhance long-context capabilities without sacrificing short-context abilities.

Method: Flora constructs long contexts by assembling short instructions (categorized) into meta-instructions for LLMs to generate responses. This method is effortless (human/LLM-free), enabling arbitrary length and diversity in contexts while minimally impacting short-context performance.

Result: LLMs enhanced by Flora excel in three long-context benchmarks and maintain strong performance in short-context tasks, as demonstrated by experiments on Llama3-8B-Instruct and QwQ-32B.

Conclusion: Flora, a human/LLM-free long-context construction strategy, significantly enhances LLMs' long-context performance by assembling short instructions into meta-instructions, producing arbitrarily long and diverse contexts with minimal compromise on short-context abilities. Experiments show Flora-enhanced LLMs excel in long-context benchmarks while maintaining strong short-context performance.

Abstract: Effectively handling long contexts is challenging for Large Language Models
(LLMs) due to the rarity of long texts, high computational demands, and
substantial forgetting of short-context abilities. Recent approaches have
attempted to construct long contexts for instruction tuning, but these methods
often require LLMs or human interventions, which are both costly and limited in
length and diversity. Also, the drop in short-context performances of present
long-context LLMs remains significant. In this paper, we introduce Flora, an
effortless (human/LLM-free) long-context construction strategy. Flora can
markedly enhance the long-context performance of LLMs by arbitrarily assembling
short instructions based on categories and instructing LLMs to generate
responses based on long-context meta-instructions. This enables Flora to
produce contexts of arbitrary length and scale with rich diversity, while only
slightly compromising short-context performance. Experiments on
Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three
long-context benchmarks while maintaining strong performances in short-context
tasks. Our data-construction code is available at
\href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.

</details>


### [189] [HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs](https://arxiv.org/abs/2507.19823)
*Dongquan Yang,Yifan Yang,Xiaotian Yu,Xianbiao Qi,Rong Xiao*

Main category: cs.CL

TL;DR: HCAttention compresses LLM KV cache to 25% (competitive at 12.5%) without performance loss, enabling 4M token processing on a single A100.


<details>
  <summary>Details</summary>
Motivation: Address the significant challenge of processing long-context inputs with LLMs due to enormous KV cache memory requirements and performance degradation of existing compression methods, while exploring GPU-CPU collaboration for approximate attention.

Method: HCAttention integrates key quantization, value offloading, and dynamic KV eviction for efficient inference under extreme memory constraints, compatible with existing transformer architectures without fine-tuning.

Result: Experimental results on LongBench show HCAttention preserves full-attention model accuracy while shrinking KV cache memory to 25% and remaining competitive at 12.5% cache.

Conclusion: HCAttention is the first framework to extend Llama-3-8B to process 4 million tokens on a single A100 GPU with 80GB memory, achieving state-of-the-art KV cache compression by shrinking the memory footprint to 25% of its original size while remaining competitive at 12.5% cache.

Abstract: Processing long-context inputs with large language models presents a
significant challenge due to the enormous memory requirements of the Key-Value
(KV) cache during inference. Existing KV cache compression methods exhibit
noticeable performance degradation when memory is reduced by more than 85%.
Additionally, strategies that leverage GPU-CPU collaboration for approximate
attention remain underexplored in this setting. We propose HCAttention, a
heterogeneous attention computation framework that integrates key quantization,
value offloading, and dynamic KV eviction to enable efficient inference under
extreme memory constraints. The method is compatible with existing transformer
architectures and does not require model fine-tuning. Experimental results on
the LongBench benchmark demonstrate that our approach preserves the accuracy of
full-attention model while shrinking the KV cache memory footprint to 25% of
its original size. Remarkably, it stays competitive with only 12.5% of the
cache, setting a new state-of-the-art in LLM KV cache compression. To the best
of our knowledge, HCAttention is the first to extend the Llama-3-8B model to
process 4 million tokens on a single A100 GPU with 80GB memory.

</details>


### [190] [DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments](https://arxiv.org/abs/2507.19867)
*Anshul Chavda,M Jagadeesh,Chintalapalli Raja Kullayappa,B Jayaprakash,Medchalimi Sruthi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: DiscoDrive 是一个合成的对话语料库，包含 3500 个对话，旨在解决现有数据集中缺乏真实车内对话中常见的非流利语（如犹豫、启动失败）的问题。该语料库可用于训练和增强对话式 AI 模型，在各种指标上都显示出比现有模型更好的性能，并且在自然度和连贯性方面获得了更高的人工评估分数。


<details>
  <summary>Details</summary>
Motivation: 现有数据集未能捕捉到真实驾驶员-AI 对话中所特有的犹豫、启动失败、重复和自我纠正等自发性非流利语。为了解决这个问题，我们引入了 DiscoDrive。

Method: 使用两阶段、由提示驱动的生成流程，在生成过程中动态集成非流利语，合成了包含 3500 个跨七个汽车领域的对话语料库 DiscoDrive。

Result: DiscoDrive 可作为有效的训练资源，使 DialoGPT-Medium 和 T5-Base 在 MultiWOZ 2.2 和 Schema-Guided Dialogue (SGD) 相关测试集上匹配或超过 KVRET 训练模型（BLEU-4 提高 0.26 至 0.61；METEOR +2.10；ROUGE-L +3.48；BERTScore F1 提高 1.35 至 3.48）。它还可以作为低资源场景下的数据增强资源，与 10% 的 KVRET 结合使用时，可额外带来高达 BLEU-4 +0.38、METEOR +1.95、ROUGE-L +2.87 和 BERTScore F1 +4.00 的收益。人工评估进一步证实，与 KVRET 的人工收集对话相比，从 DiscoDrive 中抽样的对话在自然度（3.8 vs 3.6）和连贯性（4.1 vs 4.0）方面得分更高，并且被认为比 LARD 等领先的后处理方法更具上下文适应性，同时不影响清晰度。

Conclusion: DiscoDrive 填补了现有资源的空白，可作为训练和增强对话式人工智能的多功能语料库，能够有效处理真实世界中不连贯的车内交互。

Abstract: In-car conversational AI is becoming increasingly critical as autonomous
vehicles and smart assistants gain widespread adoption. Yet, existing datasets
fail to capture the spontaneous disfluencies such as hesitations, false starts,
repetitions, and self-corrections that characterize real driver-AI dialogs. To
address this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn
dialogs across seven automotive domains, generated using a two-stage,
prompt-driven pipeline that dynamically integrates disfluencies during
synthesis. We show that DiscoDrive is effective both as a training resource,
enabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on
the MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4
improvements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1
improvements of 1.35 to 3.48), and as a data augmentation resource in
low-resource scenarios, delivering additional gains of up to BLEU-4 +0.38,
METEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10
percent of KVRET. Human evaluations further confirm that dialogs sampled from
DiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness
(3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more
context-appropriate than leading post-hoc methods (such as LARD), without
compromising clarity. DiscoDrive fills a critical gap in existing resources and
serves as a versatile corpus for both training and augmenting conversational
AI, enabling robust handling of real-world, disfluent in-car interactions.

</details>


### [191] [The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment](https://arxiv.org/abs/2507.19869)
*Danil Fokin,Monika Płużyczka,Grigory Golovin*

Main category: cs.CL

TL;DR: PVST是一种新的在线测试工具，用于测量波兰语的词汇量，并已通过试点研究得到验证。


<details>
  <summary>Details</summary>
Motivation: 为了开发一种新的工具来评估波兰语（包括母语和非母语使用者）的词汇量。

Method: PVST基于项目反应理论和计算机化自适应测试，可以动态调整测试难度以精确测量词汇量。

Result: 母语者的词汇量明显大于非母语者，并且与年龄呈正相关。

Conclusion: PVST是一个可用的在线工具，用于评估波兰语的词汇量，并通过试点研究进行了验证。

Abstract: We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing
the receptive vocabulary size of both native and non-native Polish speakers.
Based on Item Response Theory and Computerized Adaptive Testing, PVST
dynamically adjusts to each test-taker's proficiency level, ensuring high
accuracy while keeping the test duration short. To validate the test, a pilot
study was conducted with 1.475 participants. Native Polish speakers
demonstrated significantly larger vocabularies compared to non-native speakers.
For native speakers, vocabulary size showed a strong positive correlation with
age. The PVST is available online at myvocab.info/pl.

</details>


### [192] [Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam](https://arxiv.org/abs/2507.19885)
*Cesar Augusto Madid Truyts,Amanda Gomes Rabelo,Gabriel Mesquita de Souza,Daniel Scaldaferri Lages,Adriano Jose Pereira,Uri Adrian Prync Flato,Eduardo Pontes dos Reis,Joaquim Edson Vieira,Paulo Sergio Panse Silveira,Edson Amaro Junior*

Main category: cs.CL

TL;DR: 本研究评估了六种 LLM 和四种 MLLM 在巴西葡萄牙语医疗考试中的表现，发现 Claude-3.5-Sonnet 和 Claude-3-Opus 的表现与人类相当，但在多模态任务和非英语语言方面仍存在差距，凸显了对非英语医疗 AI 应用进行更多研究和优化的必要性。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）在提高诊断准确性、优化工作流程和个性化治疗计划方面展现出革新医疗保健的潜力。然而，对这些模型（特别是 LLM 和 MLLM）的评估主要集中在英语上，这可能导致它们在不同语言中的性能存在偏差。

Method: 本研究调查了六个 LLM（GPT-4.0 Turbo、LLaMA-3-8B、LLaMA-3-70B、Mixtral 8x7B Instruct、Titan Text G1-Express 和 Command R+）和四个 MLLM（Claude-3.5-Sonnet、Claude-3-Opus、Claude-3-Sonnet 和 Claude-3-Haiku）回答巴西葡萄牙语医疗住院医师入学考试问题的能力。模型性能与人类候选人进行了基准测试，分析了准确性、处理时间和生成解释的连贯性。

Result: 研究结果表明，虽然 Claude-3.5-Sonnet 和 Claude-3-Opus 等模型达到了与人类候选人相当的准确性水平，但在需要图像解释的多模态问题方面，仍然存在性能差距。此外，该研究强调了语言差异，并指出需要对非英语医疗人工智能应用进行进一步的微调和数据集增强。

Conclusion: 该研究强调了在各种语言和临床环境中评估生成式人工智能的重要性，以确保其在医疗保健领域公平可靠地部署。未来的研究应探索改进的训练方法、改进的多模态推理以及人工智能驱动的医疗辅助在真实临床环境中的整合。

Abstract: Artificial intelligence (AI) has shown the potential to revolutionize
healthcare by improving diagnostic accuracy, optimizing workflows, and
personalizing treatment plans. Large Language Models (LLMs) and Multimodal
Large Language Models (MLLMs) have achieved notable advancements in natural
language processing and medical applications. However, the evaluation of these
models has focused predominantly on the English language, leading to potential
biases in their performance across different languages.
  This study investigates the capability of six LLMs (GPT-4.0 Turbo,
LLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and
Command R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet,
and Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese
from the medical residency entrance exam of the Hospital das Cl\'inicas da
Faculdade de Medicina da Universidade de S\~ao Paulo (HCFMUSP) - the largest
health complex in South America. The performance of the models was benchmarked
against human candidates, analyzing accuracy, processing time, and coherence of
the generated explanations.
  The results show that while some models, particularly Claude-3.5-Sonnet and
Claude-3-Opus, achieved accuracy levels comparable to human candidates,
performance gaps persist, particularly in multimodal questions requiring image
interpretation. Furthermore, the study highlights language disparities,
emphasizing the need for further fine-tuning and data set augmentation for
non-English medical AI applications.
  Our findings reinforce the importance of evaluating generative AI in various
linguistic and clinical settings to ensure a fair and reliable deployment in
healthcare. Future research should explore improved training methodologies,
improved multimodal reasoning, and real-world clinical integration of AI-driven
medical assistance.

</details>


### [193] [A Gold Standard Dataset and Evaluation Framework for Depression Detection and Explanation in Social Media using LLMs](https://arxiv.org/abs/2507.19899)
*Prajval Bolegave,Pushpak Bhattacharya*

Main category: cs.CL

TL;DR: 本研究提出了一个用于评估大型语言模型（LLMs）在抑郁症检测中生成解释的真实性和质量的细粒度数据集和评估框架。通过对GPT-4.1、Gemini 2.5 Pro和Claude 3.7 Sonnet等模型进行零样本和少样本评估，研究发现模型表现存在显著差异，并强调了人类专业知识在指导模型行为和提高AI系统透明度方面的作用。


<details>
  <summary>Details</summary>
Motivation: 从在线社交媒体帖子中早期检测抑郁症有望提供及时的心理健康干预措施。为了实现这一目标，本研究提出了一个高质量、专家注释的数据集，其中包含1017个社交媒体帖子，标记有抑郁症的跨度，并映射到12个抑郁症症状类别。与主要提供粗略帖子级别标签的先前数据集不同，我们的数据集支持对模型预测和生成解释进行细粒度评估。

Method: 通过精心设计的提示策略（包括零样本和少样本方法以及经过领域调整的示例）来评估包括GPT-4.1、Gemini 2.5 Pro和Claude 3.7 Sonnet在内的最先进的专有大型语言模型。

Result: 研究结果表明，在临床解释任务上，这些模型在零样本和少样本提示下的表现存在显著差异。

Conclusion: 研究结果强调了人类专业知识在指导大型语言模型行为方面的价值，并朝着更安全、更透明的心理健康人工智能系统迈出了重要一步。

Abstract: Early detection of depression from online social media posts holds promise
for providing timely mental health interventions. In this work, we present a
high-quality, expert-annotated dataset of 1,017 social media posts labeled with
depressive spans and mapped to 12 depression symptom categories. Unlike prior
datasets that primarily offer coarse post-level labels
\cite{cohan-etal-2018-smhd}, our dataset enables fine-grained evaluation of
both model predictions and generated explanations.
  We develop an evaluation framework that leverages this clinically grounded
dataset to assess the faithfulness and quality of natural language explanations
generated by large language models (LLMs). Through carefully designed prompting
strategies, including zero-shot and few-shot approaches with domain-adapted
examples, we evaluate state-of-the-art proprietary LLMs including GPT-4.1,
Gemini 2.5 Pro, and Claude 3.7 Sonnet.
  Our comprehensive empirical analysis reveals significant differences in how
these models perform on clinical explanation tasks, with zero-shot and few-shot
prompting. Our findings underscore the value of human expertise in guiding LLM
behavior and offer a step toward safer, more transparent AI systems for
psychological well-being.

</details>


### [194] [CaliDrop: KV Cache Compression with Calibration](https://arxiv.org/abs/2507.19906)
*Yi Su,Quantong Qiu,Yuechi Zhou,Juntao Li,Qingrong Xia,Ping Li,Xinyu Duan,Zhefeng Wang,Min Zhang*

Main category: cs.CL

TL;DR: CaliDrop通过校准技术改进了LLM的KV缓存压缩，减少了因丢弃标记而导致的准确性损失。


<details>
  <summary>Details</summary>
Motivation: 为了解决在大语言模型（LLM）长上下文场景中，缓存（KV cache）由于内存占用随序列长度、批次大小和模型大小线性增长而成为瓶颈的问题。

Method: 本文提出了一种名为CaliDrop的新策略，通过校准来增强标记淘汰策略，利用了相邻查询位置之间的高度相似性。

Result: 与现有方法相比，CaliDrop在各种标记淘汰方法上都显示出显著的准确性提升，尤其是在高压缩率下。

Conclusion: CaliDrop通过在丢弃标记上进行推测性校准来缓解标记淘汰造成的准确性损失，显著提高了现有标记淘汰方法的准确性。

Abstract: Large Language Models (LLMs) require substantial computational resources
during generation. While the Key-Value (KV) cache significantly accelerates
this process by storing attention intermediates, its memory footprint grows
linearly with sequence length, batch size, and model size, creating a
bottleneck in long-context scenarios. Various KV cache compression techniques,
including token eviction, quantization, and low-rank projection, have been
proposed to mitigate this bottleneck, often complementing each other. This
paper focuses on enhancing token eviction strategies. Token eviction leverages
the observation that the attention patterns are often sparse, allowing for the
removal of less critical KV entries to save memory. However, this reduction
usually comes at the cost of notable accuracy degradation, particularly under
high compression ratios. To address this issue, we propose \textbf{CaliDrop}, a
novel strategy that enhances token eviction through calibration. Our
preliminary experiments show that queries at nearby positions exhibit high
similarity. Building on this observation, CaliDrop performs speculative
calibration on the discarded tokens to mitigate the accuracy loss caused by
token eviction. Extensive experiments demonstrate that CaliDrop significantly
improves the accuracy of existing token eviction methods.

</details>


### [195] [KLAAD: Refining Attention Mechanisms to Reduce Societal Bias in Generative Language Models](https://arxiv.org/abs/2507.19962)
*Seorin Kim,Dongyoung Lee,Jaejin Lee*

Main category: cs.CL

TL;DR: KLAAD uses attention alignment to reduce bias in LLMs without changing model weights, showing good results on benchmarks while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: LLMs often exhibit societal biases in their outputs, prompting ethical concerns regarding fairness and harm.

Method: KLAAD (KL-Attention Alignment Debiasing) is an attention-based debiasing framework that aligns attention distributions between stereotypical and anti-stereotypical sentence pairs without directly modifying model weights. It uses a composite training objective combining Cross-Entropy, KL divergence, and Triplet losses.

Result: KLAAD demonstrates improved bias mitigation on both the BBQ and BOLD benchmarks, with minimal impact on language modeling quality.

Conclusion: Attention-level alignment offers a principled solution for mitigating bias in generative language models.

Abstract: Large language models (LLMs) often exhibit societal biases in their outputs,
prompting ethical concerns regarding fairness and harm. In this work, we
propose KLAAD (KL-Attention Alignment Debiasing), an attention-based debiasing
framework that implicitly aligns attention distributions between stereotypical
and anti-stereotypical sentence pairs without directly modifying model weights.
KLAAD introduces a composite training objective combining Cross-Entropy, KL
divergence, and Triplet losses, guiding the model to consistently attend across
biased and unbiased contexts while preserving fluency and coherence.
Experimental evaluation of KLAAD demonstrates improved bias mitigation on both
the BBQ and BOLD benchmarks, with minimal impact on language modeling quality.
The results indicate that attention-level alignment offers a principled
solution for mitigating bias in generative language models.

</details>


### [196] [Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text](https://arxiv.org/abs/2507.19969)
*Mizanur Rahman,Md Tahmid Rahman Laskar,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: Text2Vis是一个用于评估文本到可视化模型的基准，包含1985个样本，涵盖20多种图表类型和复杂查询。提出了一个跨模态智能体框架，将GPT-4o的性能从26%提高到42%，并引入了一个自动化的LLM评估框架，用于大规模评估。


<details>
  <summary>Details</summary>
Motivation: 自动化数据可视化在简化数据解释、增强决策和提高效率方面发挥着关键作用。然而，目前缺乏全面的基准来严格评估大型语言模型（LLMs）在根据自然语言生成可视化方面的能力。

Method: 提出了Text2Vis基准，包含20多种图表类型、1985个样本，涵盖复杂推理、多轮对话和动态数据检索等。并提出了一种跨模态智能体框架，联合优化文本答案和可视化代码，以及一个自动化的LLM评估框架，从答案正确性、代码执行成功率、可视化可读性和图表准确性等方面进行评估。

Result: Text2Vis基准包含了20多种图表类型和1985个样本，查询涉及复杂推理、多轮对话和动态数据检索。评估了11个开源和闭源模型，揭示了显著的性能差距和关键挑战。提出的跨模态智能体框架将GPT-4o的通过率从26%提高到42%，并提升了图表质量。自动化的LLM评估框架能够在大规模样本上进行可扩展评估。

Conclusion: Text2Vis benchmark填补了文本到可视化模型评估的空白，并提出了一个结合文本答案和可视化代码的跨模态智能体框架，显著提高了GPT-4o的表现。此外，还引入了一个自动化的LLM评估框架，实现了大规模、无人工标注的评估。

Abstract: Automated data visualization plays a crucial role in simplifying data
interpretation, enhancing decision-making, and improving efficiency. While
large language models (LLMs) have shown promise in generating visualizations
from natural language, the absence of comprehensive benchmarks limits the
rigorous evaluation of their capabilities. We introduce Text2Vis, a benchmark
designed to assess text-to-visualization models, covering 20+ chart types and
diverse data science queries, including trend analysis, correlation, outlier
detection, and predictive analytics. It comprises 1,985 samples, each with a
data table, natural language query, short answer, visualization code, and
annotated charts. The queries involve complex reasoning, conversational turns,
and dynamic data retrieval. We benchmark 11 open-source and closed-source
models, revealing significant performance gaps, highlighting key challenges,
and offering insights for future advancements. To close this gap, we propose
the first cross-modal actor-critic agentic framework that jointly refines the
textual answer and visualization code, increasing GPT-4o`s pass rate from 26%
to 42% over the direct approach and improving chart quality. We also introduce
an automated LLM-based evaluation framework that enables scalable assessment
across thousands of samples without human annotation, measuring answer
correctness, code execution success, visualization readability, and chart
accuracy. We release Text2Vis at https://github.com/vis-nlp/Text2Vis.

</details>


### [197] [Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using Generalizability Theory](https://arxiv.org/abs/2507.19980)
*Dan Song,Won-Chan Lee,Hong Jiao*

Main category: cs.CL

TL;DR: 本研究使用普遍性理论评估了LLMs在AP中文写作任务中的评分可靠性。结果显示，虽然人类评分者更可靠，但LLMs在故事叙述任务上表现尚可。结合人类和AI评分的混合模型提高了评分可靠性，显示了其在大规模评估中的潜力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究大型语言模型（LLMs）在为AP中文考试的写作任务评分时的可靠性估计，以了解其在教育评估中的潜力。

Method: 本研究采用普遍性理论来评估和比较人类评分者和AI评分者在AP中文考试的两种自由回应写作任务（故事叙述和邮件回复）中的评分一致性。两名人类评分者和七名AI评分者独立对这些作文进行了评分。每篇作文都获得了四项分数：一项整体分数和三个对应于任务完成情况、表达方式和语言使用领域的分析分数。

Result: 研究结果表明，人类评分者在整体上产生了更可靠的分数，但大型语言模型在特定条件下，尤其是在故事叙述任务上，表现出了一定的评分一致性。结合人类评分者和AI评分者的综合评分能够提高可靠性。

Conclusion: 本研究表明，虽然人类评分者在整体上产生了更可靠的分数，但在特定条件下，特别是对于故事叙述任务，大型语言模型（LLMs）表现出了合理的评分一致性。结合人类评分者和AI评分者的综合评分模型提高了评分的可靠性，这表明混合评分模型可能为大规模写作评估带来益处。

Abstract: This study investigates the estimation of reliability for large language
models (LLMs) in scoring writing tasks from the AP Chinese Language and Culture
Exam. Using generalizability theory, the research evaluates and compares score
consistency between human and AI raters across two types of AP Chinese
free-response writing tasks: story narration and email response. These essays
were independently scored by two trained human raters and seven AI raters. Each
essay received four scores: one holistic score and three analytic scores
corresponding to the domains of task completion, delivery, and language use.
Results indicate that although human raters produced more reliable scores
overall, LLMs demonstrated reasonable consistency under certain conditions,
particularly for story narration tasks. Composite scoring that incorporates
both human and AI raters improved reliability, which supports that hybrid
scoring models may offer benefits for large-scale writing assessments.

</details>


### [198] [VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering](https://arxiv.org/abs/2507.19995)
*Tan-Minh Nguyen,Hoang-Trung Nguyen,Trong-Khoi Dao,Xuan-Hieu Phan,Ha-Thanh Nguyen,Thi-Hai-Yen Vuong*

Main category: cs.CL

TL;DR: 本文介绍了VLQA数据集，以解决越南法律领域自然语言处理的资源稀缺问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于越南等低资源语言在法律自然语言处理方面缺乏资源和标注数据，因此需要构建针对不同自然语言的法律文本处理应用程序，以应对法律领域对人工智能和自然语言处理的需求。

Method: 本文介绍了VLQA数据集，并进行了全面的统计分析，同时在法律信息检索和问答任务上用最先进的模型进行了评估。

Result: VLQA数据集为越南法律领域提供了高质量资源，并通过实验证明了其在法律信息检索和问答任务上的有效性。

Conclusion: 该研究介绍了VLQA数据集，一个针对越南法律领域的全面高质量资源，并评估了其在法律信息检索和问答任务上与最先进模型的有效性。

Abstract: The advent of large language models (LLMs) has led to significant
achievements in various domains, including legal text processing. Leveraging
LLMs for legal tasks is a natural evolution and an increasingly compelling
choice. However, their capabilities are often portrayed as greater than they
truly are. Despite the progress, we are still far from the ultimate goal of
fully automating legal tasks using artificial intelligence (AI) and natural
language processing (NLP). Moreover, legal systems are deeply domain-specific
and exhibit substantial variation across different countries and languages. The
need for building legal text processing applications for different natural
languages is, therefore, large and urgent. However, there is a big challenge
for legal NLP in low-resource languages such as Vietnamese due to the scarcity
of resources and annotated data. The need for labeled legal corpora for
supervised training, validation, and supervised fine-tuning is critical. In
this paper, we introduce the VLQA dataset, a comprehensive and high-quality
resource tailored for the Vietnamese legal domain. We also conduct a
comprehensive statistical analysis of the dataset and evaluate its
effectiveness through experiments with state-of-the-art models on legal
information retrieval and question-answering tasks.

</details>


### [199] [Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach](https://arxiv.org/abs/2507.20019)
*Saurav Singla,Aarav Singla,Advik Gupta,Parnika Gupta*

Main category: cs.CL

TL;DR: A meta-learning framework for few-shot text anomaly detection that uses episodic training, prototypical networks, and domain resampling to generalize across tasks like spam, fake news, and hate speech detection.


<details>
  <summary>Details</summary>
Motivation: Anomalies in human language (e.g., spam, fake news, hate speech) are a significant challenge due to their sparsity and variability. The goal is to develop a method that can detect these anomalies effectively even with limited labeled data.

Method: We treat anomaly detection as a few-shot binary classification problem and leverage meta-learning, specifically combining episodic training with prototypical networks and domain resampling, to train models that generalize across tasks. We evaluate our method on datasets from domains such as SMS spam, COVID-19 fake news, and hate speech, testing its generalization on unseen tasks with minimal labeled anomalies.

Result: Empirical results demonstrate that our method outperforms strong baselines in F1 and AUC scores on unseen tasks with minimal labeled anomalies.

Conclusion: Our proposed meta-learning framework effectively addresses the challenge of detecting anomalies in human language across diverse domains with limited labeled data, outperforming strong baselines in F1 and AUC scores.

Abstract: We propose a meta learning framework for detecting anomalies in human
language across diverse domains with limited labeled data. Anomalies in
language ranging from spam and fake news to hate speech pose a major challenge
due to their sparsity and variability. We treat anomaly detection as a few shot
binary classification problem and leverage meta-learning to train models that
generalize across tasks. Using datasets from domains such as SMS spam, COVID-19
fake news, and hate speech, we evaluate model generalization on unseen tasks
with minimal labeled anomalies. Our method combines episodic training with
prototypical networks and domain resampling to adapt quickly to new anomaly
detection tasks. Empirical results show that our method outperforms strong
baselines in F1 and AUC scores. We also release the code and benchmarks to
facilitate further research in few-shot text anomaly detection.

</details>


### [200] [FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache Compression](https://arxiv.org/abs/2507.20030)
*Runchao Li,Yao Fu,Mu Sheng,Xianxuan Long,Haotian Yu,Pan Li*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The efficacy of Large Language Models (LLMs) in long-context tasks is often
hampered by the substantial memory footprint and computational demands of the
Key-Value (KV) cache. Current compression strategies, including token eviction
and learned projections, frequently lead to biased representations -- either by
overemphasizing recent/high-attention tokens or by repeatedly degrading
information from earlier context -- and may require costly model retraining. We
present FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,
training-free KV cache compression framework that ensures unbiased information
retention. FAEDKV operates by transforming the KV cache into the frequency
domain using a proposed Infinite-Window Fourier Transform (IWDFT). This
approach allows for the equalized contribution of all tokens to the compressed
representation, effectively preserving both early and recent contextual
information. A preliminary frequency ablation study identifies critical
spectral components for layer-wise, targeted compression. Experiments on
LongBench benchmark demonstrate FAEDKV's superiority over existing methods by
up to 22\%. In addition, our method shows superior, position-agnostic retrieval
accuracy on the Needle-In-A-Haystack task compared to compression based
approaches.

</details>


### [201] [Infogen: Generating Complex Statistical Infographics from Documents](https://arxiv.org/abs/2507.20046)
*Akash Ghosh,Aparna Garimella,Pritika Ramu,Sambaran Bandyopadhyay,Sriparna Saha*

Main category: cs.CL

TL;DR: 介绍Infogen，一个能从文本生成复杂统计信息图（包括多个子图）的新框架，并附带首个数据集Infodat。Infogen性能优于现有LLM。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法在生成简单图表方面存在局限性，未能处理需要深入内容理解的文本密集型文档的复杂信息图生成。本研究旨在解决这一差距。

Method: 提出Infogen两阶段框架，首先使用微调的LLM生成信息图元数据（包括标题、文本见解、子图数据和对齐方式），然后将元数据转换为信息图代码。

Result: Infogen在Infodat数据集上实现了最先进的性能，在文本到统计信息图生成方面优于闭源和开源LLM。

Conclusion: 该研究填补了从文本生成复杂统计信息图的空白，并提出了首个相关基准数据集Infodat和两阶段框架Infogen，该框架在生成文本到统计信息图方面取得了最先进的性能。

Abstract: Statistical infographics are powerful tools that simplify complex data into
visually engaging and easy-to-understand formats. Despite advancements in AI,
particularly with LLMs, existing efforts have been limited to generating simple
charts, with no prior work addressing the creation of complex infographics from
text-heavy documents that demand a deep understanding of the content. We
address this gap by introducing the task of generating statistical infographics
composed of multiple sub-charts (e.g., line, bar, pie) that are contextually
accurate, insightful, and visually aligned. To achieve this, we define
infographic metadata that includes its title and textual insights, along with
sub-chart-specific details such as their corresponding data and alignment. We
also present Infodat, the first benchmark dataset for text-to-infographic
metadata generation, where each sample links a document to its metadata. We
propose Infogen, a two-stage framework where fine-tuned LLMs first generate
metadata, which is then converted into infographic code. Extensive evaluations
on Infodat demonstrate that Infogen achieves state-of-the-art performance,
outperforming both closed and open-source LLMs in text-to-statistical
infographic generation.

</details>


### [202] [A Tensor-Based Compiler and a Runtime for Neuron-Level DNN Certifier Specifications](https://arxiv.org/abs/2507.20055)
*Avaljot Singh,Yamin Chandini Sarita,Aditya Mishra,Ishaan Goyal,Gagandeep Singh,Charith Mendis*

Main category: cs.CL

TL;DR: 本研究提出了一个编译器框架，用于自动将DNN认证器的神经元级设计转换为张量级实现，并引入了g-BCSR稀疏格式，以提高性能和易用性。


<details>
  <summary>Details</summary>
Motivation: 当前基于抽象解释的DNN认证器在设计和实现之间存在语义鸿沟，手动转换复杂且需要专业知识，限制了新认证器的开发和应用。

Method: 提出一个编译器框架，利用基于堆栈的中间表示（IR）和形状分析，自动将神经元级别的认证器规范翻译成张量级别的层级实现。该框架还能生成g-BCSR格式以优化运行时性能。

Result: 所提出的编译器框架能够成功地将神经元级别的认证器规范转换为张量级别的实现，并且性能与手动优化相当。同时，引入的g-BCSR格式也解决了运行时稀疏性与现有格式不匹配的问题。

Conclusion: 该编译器框架能够自动将基于神经元的DNN认证器规范转换为基于张量的层级实现，并能在运行时利用g-BCSR稀疏格式提高性能，使得开发新的认证器和分析其在不同DNN上的效用变得更加容易，同时性能可与手动优化实现相媲美。

Abstract: The uninterpretability of DNNs has led to the adoption of abstract
interpretation-based certification as a practical means to establish trust in
real-world systems that rely on DNNs. However, the current landscape supports
only a limited set of certifiers, and developing new ones or modifying existing
ones for different applications remains difficult. This is because the
mathematical design of certifiers is expressed at the neuron level, while their
implementations are optimized and executed at the tensor level. This mismatch
creates a semantic gap between design and implementation, making manual
bridging both complex and expertise-intensive -- requiring deep knowledge in
formal methods, high-performance computing, etc.
  We propose a compiler framework that automatically translates neuron-level
specifications of DNN certifiers into tensor-based, layer-level
implementations. This is enabled by two key innovations: a novel stack-based
intermediate representation (IR) and a shape analysis that infers the implicit
tensor operations needed to simulate the neuron-level semantics. During
lifting, the shape analysis creates tensors in the minimal shape required to
perform the corresponding operations. The IR also enables domain-specific
optimizations as rewrites. At runtime, the resulting tensor computations
exhibit sparsity tied to the DNN architecture. This sparsity does not align
well with existing formats. To address this, we introduce g-BCSR, a
double-compression format that represents tensors as collections of blocks of
varying sizes, each possibly internally sparse.
  Using our compiler and g-BCSR, we make it easy to develop new certifiers and
analyze their utility across diverse DNNs. Despite its flexibility, the
compiler achieves performance comparable to hand-optimized implementations.

</details>


### [203] [RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation](https://arxiv.org/abs/2507.20059)
*Ran Xu,Yuchen Zhuang,Yue Yu,Haoyu Wang,Wenqi Shi,Carl Yang*

Main category: cs.CL

TL;DR: 检索增强生成（RAG）在真实世界的应用中存在局限性，尤其是在处理多样化的知识来源和模型规模方面，需要新的适应性策略。


<details>
  <summary>Details</summary>
Motivation: 评估RAG在真实、多样化检索场景下的有效性，因为现有基准测试主要基于通用语料库。

Method: 评估了RAG系统在包含混合知识的大规模数据存储MassiveDS上的表现，分析了检索器、重排器和不同知识源的有效性。

Result: 检索主要使较小的模型受益，重排器的价值不大，没有单一的检索源能一直表现出色，并且当前的LLM难以跨异构知识源进行查询路由。

Conclusion: 当前的检索增强生成（RAG）系统在真实多样的检索场景下效果不佳，需要开发适应性检索策略。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge retrieved at inference time. While RAG
demonstrates strong performance on benchmarks largely derived from
general-domain corpora like Wikipedia, its effectiveness under realistic,
diverse retrieval scenarios remains underexplored. We evaluated RAG systems
using MassiveDS, a large-scale datastore with mixture of knowledge, and
identified critical limitations: retrieval mainly benefits smaller models,
rerankers add minimal value, and no single retrieval source consistently
excels. Moreover, current LLMs struggle to route queries across heterogeneous
knowledge sources. These findings highlight the need for adaptive retrieval
strategies before deploying RAG in real-world settings. Our code and data can
be found at https://github.com/ritaranx/RAG_in_the_Wild.

</details>


### [204] [ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models](https://arxiv.org/abs/2507.20091)
*Kaizhi Qian,Xulin Fan,Junrui Ni,Slava Shechtman,Mark Hasegawa-Johnson,Chuang Gan,Yang Zhang*

Main category: cs.CL

TL;DR: ProsodyLM通过新的标记方案提升了语音语言模型学习韵律信息的能力，仅通过预训练就能实现多样的韵律处理功能。


<details>
  <summary>Details</summary>
Motivation: 现有的主流语音语言模型训练范式将语音转换为离散标记输入到大型语言模型（LLMs）中，这种方法在学习韵律信息方面存在不足，导致预训练后的LLMs在韵律处理能力方面没有明显提升。因此，有必要开发一种新的方法来解决这个问题。

Method: 提出了一种名为ProsodyLM的新方法，该方法采用了一种新的标记方案。首先将语音转录成文本，然后将其转化为一系列词级韵律标记。这种标记方案保留了比传统方案更完整的韵律信息，并且更易于基于文本的语言模型理解。

Result: ProsodyLM通过预训练展现了出人意料的、多样化的新兴韵律处理能力，包括利用生成语音中的韵律细微差别（如对比焦点）、理解语音中的情感和重音，以及在长上下文保持韵律一致性。

Conclusion: ProsodyLM通过一种新的、更适合学习韵律的标记方案，克服了传统语音语言模型在处理韵律信息方面的不足。通过预训练，ProsodyLM能够学习到多样化的韵律处理能力，包括利用生成语音中的韵律细微差别（如对比焦点）、理解语音中的情感和重音，以及在长上下文保持韵律一致性。

Abstract: Speech language models refer to language models with speech processing and
understanding capabilities. One key desirable capability for speech language
models is the ability to capture the intricate interdependency between content
and prosody. The existing mainstream paradigm of training speech language
models, which converts speech into discrete tokens before feeding them into
LLMs, is sub-optimal in learning prosody information -- we find that the
resulting LLMs do not exhibit obvious emerging prosody processing capabilities
via pre-training alone. To overcome this, we propose ProsodyLM, which
introduces a simple tokenization scheme amenable to learning prosody. Each
speech utterance is first transcribed into text, followed by a sequence of
word-level prosody tokens. Compared with conventional speech tokenization
schemes, the proposed tokenization scheme retains more complete prosody
information, and is more understandable to text-based LLMs. We find that
ProsodyLM can learn surprisingly diverse emerging prosody processing
capabilities through pre-training alone, ranging from harnessing the prosody
nuances in generated speech, such as contrastive focus, understanding emotion
and stress in an utterance, to maintaining prosody consistency in long
contexts.

</details>


### [205] [AI-Driven Generation of Old English: A Framework for Low-Resource Languages](https://arxiv.org/abs/2507.20111)
*Rodrigo Gabriel Salazar Alva,Matías Nuñez,Cristian López,Javier Martín Arista*

Main category: cs.CL

TL;DR: 我们提出了一个使用LLM生成高质量古英语文本的框架，通过LoRA、回译和双代理流程，显著提高了翻译质量，并为保护其他濒危语言提供了方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决古英语资源匮乏、限制其在现代自然语言处理（NLP）技术中的可及性问题，同时保存人类的文化和语言遗产。

Method: 提出一个结合参数高效微调（LoRA）、通过回译进行数据增强以及分离内容生成（英文）和翻译（古英语）任务的双代理流程的框架。

Result: 该方法在英文到古英语的翻译中，BLEU分数从26分提高到65分以上，并且通过了自动评估指标（BLEU、METEOR和CHRF）的显著改进和专家人类评估，证实了生成文本的高语法准确性和风格保真度。

Conclusion: 该框架为复兴其他濒危语言提供了一个实用的蓝图，有效地将人工智能创新与文化保护的目标结合起来。

Abstract: Preserving ancient languages is essential for understanding humanity's
cultural and linguistic heritage, yet Old English remains critically
under-resourced, limiting its accessibility to modern natural language
processing (NLP) techniques. We present a scalable framework that uses advanced
large language models (LLMs) to generate high-quality Old English texts,
addressing this gap. Our approach combines parameter-efficient fine-tuning
(Low-Rank Adaptation, LoRA), data augmentation via backtranslation, and a
dual-agent pipeline that separates the tasks of content generation (in English)
and translation (into Old English). Evaluation with automated metrics (BLEU,
METEOR, and CHRF) shows significant improvements over baseline models, with
BLEU scores increasing from 26 to over 65 for English-to-Old English
translation. Expert human assessment also confirms high grammatical accuracy
and stylistic fidelity in the generated texts. Beyond expanding the Old English
corpus, our method offers a practical blueprint for revitalizing other
endangered languages, effectively uniting AI innovation with the goals of
cultural preservation.

</details>


### [206] [Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering](https://arxiv.org/abs/2507.20133)
*Anas Mohamed,Azal Ahmad Khan,Xinran Wang,Ahmad Faraz Khan,Shuwen Ge,Saman Bahzad Khan,Ayaan Ahmad,Ali Anwar*

Main category: cs.CL

TL;DR: Sem-DPO是一种改进的DPO方法，通过考虑提示的语义来优化文本到图像生成，提高了生成图像与用户意图的一致性，并在评估中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式人工智能在根据文本合成逼真的图像方面取得了显著进展，但输出质量仍然高度依赖于提示的措辞。虽然直接偏好优化（DPO）提供了一种轻量级的替代方法，但其令牌级别的正则化未能解决语义不一致问题，导致提示可能偏离用户的本意。

Method: Sem-DPO通过将DPO损失与一个指数权重相乘来扩展，该权重与原始提示和获胜候选项在嵌入空间中的余弦距离成正比，从而对那些可能导致语义不匹配的提示的训练信号进行软性加权。

Result: Sem-DPO在三个标准的文本到图像提示优化基准和两个语言模型上，实现了比DPO高8-12%的CLIP相似度和5-9%的人类偏好得分（HPSv2.1，PickScore），同时优于最先进的基线。

Conclusion: Sem-DPO通过引入与嵌入空间中原始提示和获胜候选项之间的余弦距离成正比的指数权重来扩展DPO损失，从而保留了语义一致性，同时保持了其简单性和效率。在三个标准的文本到图像提示优化基准和两个语言模型上，Sem-DPO实现了比DPO高8-12%的CLIP相似度和5-9%的人类偏好得分（HPSv2.1，PickScore），同时优于最先进的基线。

Abstract: Generative AI can now synthesize strikingly realistic images from text, yet
output quality remains highly sensitive to how prompts are phrased. Direct
Preference Optimization (DPO) offers a lightweight, off-policy alternative to
RL for automatic prompt engineering, but its token-level regularization leaves
semantic inconsistency unchecked as prompts that win higher preference scores
can still drift away from the user's intended meaning.
  We introduce Sem-DPO, a variant of DPO that preserves semantic consistency
yet retains its simplicity and efficiency. Sem-DPO scales the DPO loss by an
exponential weight proportional to the cosine distance between the original
prompt and winning candidate in embedding space, softly down-weighting training
signals that would otherwise reward semantically mismatched prompts. We provide
the first analytical bound on semantic drift for preference-tuned prompt
generators, showing that Sem-DPO keeps learned prompts within a provably
bounded neighborhood of the original text. On three standard text-to-image
prompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12%
higher CLIP similarity and 5-9% higher human-preference scores (HPSv2.1,
PickScore) than DPO, while also outperforming state-of-the-art baselines. These
findings suggest that strong flat baselines augmented with semantic weighting
should become the new standard for prompt-optimization studies and lay the
groundwork for broader, semantics-aware preference optimization in language
models.

</details>


### [207] [Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG](https://arxiv.org/abs/2507.20136)
*Baiyu Chen,Wilson Wongso,Xiaoqian Hu,Yue Tan,Flora Salim*

Main category: cs.CL

TL;DR: CRAG-MM挑战赛旨在解决视觉语言模型（VLMs）在处理多模态、多轮对话时容易产生幻觉的问题。CRUISE团队提出了一个多阶段框架，优先考虑事实准确性，通过查询路由、检索、摘要、双通路生成和事后验证来最小化幻觉。该方法在任务1中获得第三名，证明了在复杂多模态RAG系统中优先考虑答案可靠性的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现代视觉语言模型（VLMs）在面对以自我为中心的图像、长尾实体和复杂的多跳问题时容易产生幻觉的关键限制。

Method: 提出一个多阶段框架，包括轻量级查询路由器、查询感知检索和摘要管道、双通路生成以及事后验证，以优先考虑事实准确性和真实性。

Result: 该方法在CRAG-MM挑战赛的任务1中取得了第三名。

Conclusion: 该方法通过优先考虑答案的可靠性，在复杂的多模态检索增强生成（RAG）系统中取得了成功，在CRAG-MM挑战赛的任务1中获得第三名。

Abstract: This paper presents the technical solution developed by team CRUISE for the
KDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn
(CRAG-MM) challenge. The challenge aims to address a critical limitation of
modern Vision Language Models (VLMs): their propensity to hallucinate,
especially when faced with egocentric imagery, long-tail entities, and complex,
multi-hop questions. This issue is particularly problematic in real-world
applications where users pose fact-seeking queries that demand high factual
accuracy across diverse modalities. To tackle this, we propose a robust,
multi-stage framework that prioritizes factual accuracy and truthfulness over
completeness. Our solution integrates a lightweight query router for
efficiency, a query-aware retrieval and summarization pipeline, a dual-pathways
generation and a post-hoc verification. This conservative strategy is designed
to minimize hallucinations, which incur a severe penalty in the competition's
scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the
effectiveness of prioritizing answer reliability in complex multi-modal RAG
systems. Our implementation is available at
https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM .

</details>


### [208] [Multi-Agent Interactive Question Generation Framework for Long Document Understanding](https://arxiv.org/abs/2507.20145)
*Kesen Wang,Daulet Toibazar,Abdulrahman Alfulayt,Abdulaziz S. Albadawi,Ranya A. Alkahtani,Asma A. Ibrahim,Haneen A. Alhomoud,Sherif Mohamed,Pedro J. Moreno*

Main category: cs.CL

TL;DR: 本研究提出一种自动生成长文档问答对的方法，以改进视觉语言模型处理长文本和阿拉伯语的能力，并提供了一个包含英文和阿拉伯文数据的基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文文档理解（DU）任务中，尤其是在阿拉伯语等低资源语言场景下，由于缺乏细粒度训练数据而导致的LVLMs性能下降问题。现有方法过度依赖昂贵且低效的人工标注。

Method: 提出一个全自动化的、多智能体交互的框架，用于高效生成长上下文文档（包括英文和阿拉伯文）的单页和多页问答对。

Result: 生成的英文和阿拉伯文问答对（AraEngLongBench）对主流开源和闭源LVLMs提出了挑战，证明了该框架生成高质量、具有挑战性数据的能力。

Conclusion: 该研究提出了一个全自动化的、多智能体交互的框架，用于高效生成长上下文文档的问答对，以促进长上下文理解能力的LVLMs的发展。

Abstract: Document Understanding (DU) in long-contextual scenarios with complex layouts
remains a significant challenge in vision-language research. Although Large
Vision-Language Models (LVLMs) excel at short-context DU tasks, their
performance declines in long-context settings. A key limitation is the scarcity
of fine-grained training data, particularly for low-resource languages such as
Arabic. Existing state-of-the-art techniques rely heavily on human annotation,
which is costly and inefficient. We propose a fully automated, multi-agent
interactive framework to generate long-context questions efficiently. Our
approach efficiently generates high-quality single- and multi-page questions
for extensive English and Arabic documents, covering hundreds of pages across
diverse domains. This facilitates the development of LVLMs with enhanced
long-context understanding ability. Experimental results in this work have
shown that our generated English and Arabic questions
(\textbf{AraEngLongBench}) are quite challenging to major open- and
close-source LVLMs. The code and data proposed in this work can be found in
https://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and
Answer (QA) pairs and structured system prompts can be found in the Appendix.

</details>


### [209] [Goal Alignment in LLM-Based User Simulators for Conversational AI](https://arxiv.org/abs/2507.20152)
*Shuhaib Mehri,Xiaocheng Yang,Takyoung Kim,Gokhan Tur,Shikib Mehri,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: LLM在多轮对话中难以保持目标一致性，我们提出了UGST框架通过跟踪用户目标进度来解决此问题，并在两个基准测试中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM在用户模拟方面虽然有进步，但在多轮对话中难以持续展现目标导向行为，这限制了它们在实际应用中的可靠性。

Method: 提出了一种名为用户目标状态跟踪（UGST）的新框架，该框架通过三个阶段的方法来开发用户模拟器：自主跟踪目标进度、进行推理以及生成与目标一致的响应。此外，还建立了全面的评估指标来衡量用户模拟器的目标一致性。

Result: 提出的UGST框架在MultiWOZ 2.4和τ-Bench两个基准上均取得了显著的改进，有效提升了用户模拟器的目标一致性。

Conclusion: UGST框架解决了当前LLM在多轮对话中一致性目标导向行为方面的关键问题，能够自主跟踪目标进度并生成符合目标的响应。通过在MultiWOZ 2.4和τ-Bench两个基准上的实验，证明了该方法在提高目标一致性方面取得了显著的改进，并为开发目标一致的用户模拟器确立了UGST框架的重要性。

Abstract: User simulators are essential to conversational AI, enabling scalable agent
development and evaluation through simulated interactions. While current Large
Language Models (LLMs) have advanced user simulation capabilities, we reveal
that they struggle to consistently demonstrate goal-oriented behavior across
multi-turn conversations--a critical limitation that compromises their
reliability in downstream applications. We introduce User Goal State Tracking
(UGST), a novel framework that tracks user goal progression throughout
conversations. Leveraging UGST, we present a three-stage methodology for
developing user simulators that can autonomously track goal progression and
reason to generate goal-aligned responses. Moreover, we establish comprehensive
evaluation metrics for measuring goal alignment in user simulators, and
demonstrate that our approach yields substantial improvements across two
benchmarks (MultiWOZ 2.4 and {\tau}-Bench). Our contributions address a
critical gap in conversational AI and establish UGST as an essential framework
for developing goal-aligned user simulators.

</details>


### [210] [SGPO: Self-Generated Preference Optimization based on Self-Improver](https://arxiv.org/abs/2507.20181)
*Hyeonji Lee,Daejin Jo,Seohwan Yun,Sungwoong Kim*

Main category: cs.CL

TL;DR: SGPO是一种新颖的对齐框架，它使用on-policy自改进机制，让改进模型优化策略模型，以生成偏好数据用于直接偏好优化。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统偏好对齐方法依赖离线数据集和引入分布偏移的问题，提出SGPO。

Method: SGPO框架利用on-policy自改进机制，通过改进模型优化策略模型，以生成偏好数据用于直接偏好优化。

Result: 实验结果表明，SGPO在AlpacaEval 2.0和Arena-Hard上表现优于DPO和基线自改进方法。

Conclusion: SGPO通过利用单模型上的自改进机制，显著优于DPO和基线自改进方法，无需外部偏好数据。

Abstract: Large language models (LLMs), despite their extensive pretraining on diverse
datasets, require effective alignment to human preferences for practical and
reliable deployment. Conventional alignment methods typically employ off-policy
learning and depend on human-annotated datasets, which limits their broad
applicability and introduces distribution shift issues during training. To
address these challenges, we propose Self-Generated Preference Optimization
based on Self-Improver (SGPO), an innovative alignment framework that leverages
an on-policy self-improving mechanism. Specifically, the improver refines
responses from a policy model to self-generate preference data for direct
preference optimization (DPO) of the policy model. Here, the improver and
policy are unified into a single model, and in order to generate higher-quality
preference data, this self-improver learns to make incremental yet discernible
improvements to the current responses by referencing supervised fine-tuning
outputs. Experimental results on AlpacaEval 2.0 and Arena-Hard show that the
proposed SGPO significantly improves performance over DPO and baseline
self-improving methods without using external preference data.

</details>


### [211] [SessionIntentBench: A Multi-task Inter-session Intention-shift Modeling Benchmark for E-commerce Customer Behavior Understanding](https://arxiv.org/abs/2507.20185)
*Yuqi Yang,Weiqi Wang,Baixuan Xu,Wei Fan,Qing Zong,Chunkit Chan,Zheye Deng,Xin Liu,Yifan Gao,Changlong Yu,Chen Luo,Yang Li,Zheng Li,Qingyu Yin,Bing Yin,Yangqiu Song*

Main category: cs.CL

TL;DR: 本研究提出了SessionIntentBench，一个用于评估L(V)LMs在电子商务会话中理解用户意图转移能力的基准测试。研究发现，现有模型在捕捉用户意图方面存在不足，但通过注入意图信息可以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效利用会话历史信息来捕捉用户意图，并且缺乏专门用于电子商务产品购买会话中意图建模的数据集和基准测试。

Method: 提出了一种名为SessionIntentBench的基准测试，该测试包含一个意图树概念，并使用了1,952,177个意图条目、1,132,145个会话意图轨迹和13,003,664个可用任务，以评估L(V)LMs在理解跨会话意图转移方面的能力，并进行了人工标注以收集用于评估的黄金集合。

Result: 通过在标注数据上的广泛实验，证明了当前L(V)LMs在捕捉和利用跨复杂会话场景中的意图方面存在不足，但注入意图信息可以提升LLMs的性能。

Conclusion: 现有的语言模型（L(V)LMs）未能有效捕捉和利用复杂会话场景中的跨会话意图。但通过注入意图信息可以提升大型语言模型的性能。

Abstract: Session history is a common way of recording user interacting behaviors
throughout a browsing activity with multiple products. For example, if an user
clicks a product webpage and then leaves, it might because there are certain
features that don't satisfy the user, which serve as an important indicator of
on-the-spot user preferences. However, all prior works fail to capture and
model customer intention effectively because insufficient information
exploitation and only apparent information like descriptions and titles are
used. There is also a lack of data and corresponding benchmark for explicitly
modeling intention in E-commerce product purchase sessions. To address these
issues, we introduce the concept of an intention tree and propose a dataset
curation pipeline. Together, we construct a sibling multimodal benchmark,
SessionIntentBench, that evaluates L(V)LMs' capability on understanding
inter-session intention shift with four subtasks. With 1,952,177 intention
entries, 1,132,145 session intention trajectories, and 13,003,664 available
tasks mined using 10,905 sessions, we provide a scalable way to exploit the
existing session data for customer intention understanding. We conduct human
annotations to collect ground-truth label for a subset of collected data to
form an evaluation gold set. Extensive experiments on the annotated data
further confirm that current L(V)LMs fail to capture and utilize the intention
across the complex session setting. Further analysis show injecting intention
enhances LLMs' performances.

</details>


### [212] [Diversity-Enhanced Reasoning for Subjective Questions](https://arxiv.org/abs/2507.20187)
*Yumeng Wang,Zhiyuan Fan,Jiayu Liu,Yi R. Fung*

Main category: cs.CL

TL;DR: MultiRole-R1通过多角色视角和多样性奖励提升大型推理模型的推理准确性和多样性，尤其在主观任务上效果显著。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRM）在主观性问题上表现受限，因为它们倾向于产生同质化推理，这源于监督微调和可验证奖励对单一事实的依赖。为解决此问题，受到增加角色视角能提升性能的启发，提出MultiRole-R1来提高主观推理任务的准确性和多样性。

Method: 提出了一种名为MultiRole-R1的框架，该框架通过无监督数据构建流程，生成包含多样化角色视角的推理链。同时，采用了一种基于群体相对策略优化（GRPO）的强化学习方法，并将多样性作为除可验证奖励外的奖励信号，通过专门设计的奖励函数来提升视角和词汇多样性。

Result: 实验表明，MultiRole-R1能够有效促进视角多样性和词汇多样性，揭示了推理多样性与准确性之间的正相关关系。在六个基准测试上的结果显示，MultiRole-R1在提升主观和客观推理能力方面均表现出有效性和泛化性。

Conclusion: MultiRole-R1框架通过引入多角色视角和奖励塑造的GRPO，有效提升了大型推理模型在主观推理任务中的准确性和多样性，并揭示了推理多样性与准确性之间的正相关关系。该框架在六个基准测试中的表现证明了其有效性和泛化能力，为增强大型推理模型训练提供了新的方向。

Abstract: Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities
have shown strong performance on objective tasks, such as math reasoning and
coding. However, their effectiveness on subjective questions that may have
different responses from different perspectives is still limited by a tendency
towards homogeneous reasoning, introduced by the reliance on a single ground
truth in supervised fine-tuning and verifiable reward in reinforcement
learning. Motivated by the finding that increasing role perspectives
consistently improves performance, we propose MultiRole-R1, a
diversity-enhanced framework with multiple role perspectives, to improve the
accuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an
unsupervised data construction pipeline that generates reasoning chains that
incorporate diverse role perspectives. We further employ reinforcement learning
via Group Relative Policy Optimization (GRPO) with reward shaping, by taking
diversity as a reward signal in addition to the verifiable reward. With
specially designed reward functions, we successfully promote perspective
diversity and lexical diversity, uncovering a positive relation between
reasoning diversity and accuracy. Our experiment on six benchmarks demonstrates
MultiRole-R1's effectiveness and generalizability in enhancing both subjective
and objective reasoning, showcasing the potential of diversity-enhanced
training in LRMs.

</details>


### [213] [IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs](https://arxiv.org/abs/2507.20208)
*Aviya Maimon,Amir DN Cohen,Gal Vishne,Shauli Ravfogel,Reut Tsarfaty*

Main category: cs.CL

TL;DR: LLM的评估方式需要改进，现有的基准分数难以全面反映模型能力。本研究提出一种基于因子分析的新评估范式，通过识别潜在技能来全面理解和评估LLM，并开发了相关实用工具。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估依赖于难以解释的基准分数，缺乏对模型整体能力、任务关联性、共同测量内容、差异性或冗余性的理解，导致评估方法过于单一。

Method: 使用因子分析识别驱动模型在多个基准上表现的潜在技能。

Result: 确定了一组能够广泛解释模型表现的潜在技能，并开发了用于识别冗余任务、辅助模型选择和进行模型技能画像的实用工具。 A new leaderboard is created with 60 LLMs on 44 tasks.

Conclusion: LLM评估可以通过因子分析识别潜在技能，从而为模型选择和基准任务冗余提供实用工具。

Abstract: Current evaluations of large language models (LLMs) rely on benchmark scores,
but it is difficult to interpret what these individual scores reveal about a
model's overall skills. Specifically, as a community we lack understanding of
how tasks relate to one another, what they measure in common, how they differ,
or which ones are redundant. As a result, models are often assessed via a
single score averaged across benchmarks, an approach that fails to capture the
models' wholistic strengths and limitations. Here, we propose a new evaluation
paradigm that uses factor analysis to identify latent skills driving
performance across benchmarks. We apply this method to a comprehensive new
leaderboard showcasing the performance of 60 LLMs on 44 tasks, and identify a
small set of latent skills that largely explain performance. Finally, we turn
these insights into practical tools that identify redundant tasks, aid in model
selection, and profile models along each latent skill.

</details>


### [214] [Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation](https://arxiv.org/abs/2507.20210)
*Minh Hoang Nguyen,Thuat Thien Nguyen,Minh Nhat Ta*

Main category: cs.CL

TL;DR: Co-NAML-LSTUR是一个混合新闻推荐框架，它结合了NAML（用于多视角新闻建模）和LSTUR（用于用户短期和长期偏好建模），并使用BERT进行词嵌入。该模型在MIND-small和MIND-large数据集上均优于现有基线方法，证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 新闻推荐系统在通过提供个性化新闻内容来缓解信息过载方面发挥着至关重要的作用。一个核心挑战在于有效建模新闻的多视角表示以及用户兴趣的动态性质，而用户兴趣通常跨越短期和长期偏好。现有方法通常依赖新闻文章的单一视角特征（例如，标题或类别），或者未能全面捕捉用户跨时间尺度的偏好。

Method: 提出了一种名为Co-NAML-LSTUR的混合新闻推荐框架，该框架集成了NAML进行注意力多视角新闻建模和LSTUR捕获长期和短期用户表示。模型还结合了基于BERT的词嵌入来增强语义特征提取。

Result: 在MIND-small和MIND-large两个数据集上进行了评估，实验结果显示Co-NAML-LSTUR取得了显著的改进。

Conclusion: 实验结果表明，Co-NAML-LSTUR在MIND-small和MIND-large两个广泛使用的基准上，相比现有的大多数最先进的基线方法取得了显著的改进。这证明了结合多视角新闻表示和双尺度用户建模的有效性。

Abstract: News recommendation systems play a vital role in mitigating information
overload by delivering personalized news content. A central challenge is to
effectively model both multi-view news representations and the dynamic nature
of user interests, which often span both short- and long-term preferences.
Existing methods typically rely on single-view features of news articles (e.g.,
titles or categories) or fail to comprehensively capture user preferences
across time scales. In this work, we propose Co-NAML-LSTUR, a hybrid news
recommendation framework that integrates NAML for attentive multi-view news
modeling and LSTUR for capturing both long- and short-term user
representations. Our model also incorporates BERT-based word embeddings to
enhance semantic feature extraction. We evaluate Co-NAML-LSTUR on two widely
used benchmarks, MIND-small and MIND-large. Experimental results show that
Co-NAML-LSTUR achieves substantial improvements over most state-of-the-art
baselines on MIND-small and MIND-large, respectively. These results demonstrate
the effectiveness of combining multi-view news representations with dual-scale
user modeling. The implementation of our model is publicly available at
https://github.com/MinhNguyenDS/Co-NAML-LSTUR.

</details>


### [215] [Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models](https://arxiv.org/abs/2507.20241)
*Yi Feng,Jiaqi Wang,Wenxuan Zhang,Zhuang Chen,Yutong Shen,Xiyao Xiao,Minlie Huang,Liping Jing,Jian Yu*

Main category: cs.CL

TL;DR: 该研究提出了一个名为INT的交互式叙事疗法框架，用于模拟叙事疗法并量化治疗进展。INT框架通过模拟专家治疗师的干预和追踪“创新时刻”来提升治疗质量和深度，实验结果表明其优于标准LLM，并有助于开发心理健康支持应用。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在心理健康支持方面虽然带来了新的可能性，但在模拟专业心理治疗和捕捉治疗进程方面存在不足。叙事疗法虽然能帮助个体转变问题生活故事，但由于可及性有限和社会污名化而未得到充分利用。本研究旨在解决这些局限性。

Method: 本研究提出了一个包含两个核心组件的综合框架：INT（交互式叙事疗法）和IMA（创新时刻评估）。INT能够规划治疗阶段、指导反思水平并生成专家级的响应，从而模拟专家叙事治疗师。IMA则通过追踪“创新时刻”（IMs）——客户话语中标志着治疗进展的关键叙事转变——来量化治疗效果。

Result: 实验结果在260名模拟客户和230名真人参与者身上进行，显示INT在治疗质量和深度方面持续优于标准的LLM。此外，研究还证明了INT在合成高质量支持对话以促进社交应用方面的有效性。

Conclusion: LLM在心理健康支持方面展现了巨大潜力，但现有方法在模拟专业心理治疗和捕捉治疗进程方面存在不足。本研究提出的INT（交互式叙事疗法）框架，通过模拟专家叙事治疗师的干预和采用IMA（创新时刻评估）方法量化治疗效果，有效解决了这些问题。实验结果表明，INT在治疗质量和深度上优于标准LLM，并能生成高质量的支持对话，促进其在社交应用中的推广。

Abstract: Recent progress in large language models (LLMs) has opened new possibilities
for mental health support, yet current approaches lack realism in simulating
specialized psychotherapy and fail to capture therapeutic progression over
time. Narrative therapy, which helps individuals transform problematic life
stories into empowering alternatives, remains underutilized due to limited
access and social stigma. We address these limitations through a comprehensive
framework with two core components. First, INT (Interactive Narrative
Therapist) simulates expert narrative therapists by planning therapeutic
stages, guiding reflection levels, and generating contextually appropriate
expert-like responses. Second, IMA (Innovative Moment Assessment) provides a
therapy-centric evaluation method that quantifies effectiveness by tracking
"Innovative Moments" (IMs), critical narrative shifts in client speech
signaling therapy progress. Experimental results on 260 simulated clients and
230 human participants reveal that INT consistently outperforms standard LLMs
in therapeutic quality and depth. We further demonstrate the effectiveness of
INT in synthesizing high-quality support conversations to facilitate social
applications.

</details>


### [216] [Modeling Professionalism in Expert Questioning through Linguistic Differentiation](https://arxiv.org/abs/2507.20249)
*Giulia D'Agostino,Chung-Chi Chen*

Main category: cs.CL

TL;DR: 本研究探讨了在金融等高风险领域，如何利用语言特征来模拟和评估专家提问中的专业性。通过构建包含结构和语用元素的标注框架，并使用人类和LLM生成的问题，研究发现语言特征不仅能反映专业性，还能区分问题来源。基于这些特征训练的分类器在区分专家问题方面表现优于现有模型，证明了专业性是可以通过语言学模型学习和捕捉的。


<details>
  <summary>Details</summary>
Motivation: 调查语言特征如何用于建模和评估专家提问中的专业性，尤其是在金融等高风险领域。

Method: 通过引入新颖的注释框架来量化金融分析师问题中的结构和语用元素，例如话语调节词、序言和请求类型。使用人类编写和LLM生成的问题构建了两个数据集，一个用于评估专业性，另一个用于标记问题来源。

Result: 相同的语言特征与人类判断和作者身份来源高度相关，这表明存在共同的风格基础。此外，仅基于这些可解释特征训练的分类器在区分专家编写的问题方面优于gemini-2.0和SVM基线。

Conclusion: 专业性是可以学习的、领域无关的构建，可以通过基于语言学的模型来捕捉。

Abstract: Professionalism is a crucial yet underexplored dimension of expert
communication, particularly in high-stakes domains like finance. This paper
investigates how linguistic features can be leveraged to model and evaluate
professionalism in expert questioning. We introduce a novel annotation
framework to quantify structural and pragmatic elements in financial analyst
questions, such as discourse regulators, prefaces, and request types. Using
both human-authored and large language model (LLM)-generated questions, we
construct two datasets: one annotated for perceived professionalism and one
labeled by question origin. We show that the same linguistic features correlate
strongly with both human judgments and authorship origin, suggesting a shared
stylistic foundation. Furthermore, a classifier trained solely on these
interpretable features outperforms gemini-2.0 and SVM baselines in
distinguishing expert-authored questions. Our findings demonstrate that
professionalism is a learnable, domain-general construct that can be captured
through linguistically grounded modeling.

</details>


### [217] [Post-Completion Learning for Language Models](https://arxiv.org/abs/2507.20252)
*Xiang Fei,Siqi Wang,Shu Wei,Yuxiang Nie,Wei Shi,Hao Feng,Can Huang*

Main category: cs.CL

TL;DR: 提出后完成学习（PCL）框架，利用序列完成后的空间来提升语言模型的推理和自我评估能力，同时保持推理效率。实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的训练范式通常在达到序列结束（<eos>）标记时终止，忽略了在完成序列后可能存在的学习机会。然而，这一后完成空间蕴含着提升模型推理和自我评估能力的潜力。

Method: 提出了一种名为后完成学习（PCL）的新型训练框架，该框架系统地利用模型输出完成后的序列空间。具体而言，PCL设计了一种白盒强化学习方法，让模型根据奖励规则评估输出内容，然后计算分数并使用奖励函数进行对齐以实现监督。此外，还采用了双轨道SFT（监督微调）来优化推理和评估能力，并结合RL（强化学习）训练实现多目标混合优化。

Result: 实验结果表明，PCL在不同数据集和模型上相比传统的SFT和RL方法有持续性的提升，证明了该方法的有效性。

Conclusion: PCL通过利用序列完成后的空间，能够提升语言模型的推理和自我评估能力，同时保持推理效率，为语言模型训练提供了一种新的技术途径，能够提升输出质量并保持部署效率。

Abstract: Current language model training paradigms typically terminate learning upon
reaching the end-of-sequence (<eos>}) token, overlooking the potential learning
opportunities in the post-completion space. We propose Post-Completion Learning
(PCL), a novel training framework that systematically utilizes the sequence
space after model output completion, to enhance both the reasoning and
self-evaluation abilities. PCL enables models to continue generating
self-assessments and reward predictions during training, while maintaining
efficient inference by stopping at the completion point.
  To fully utilize this post-completion space, we design a white-box
reinforcement learning method: let the model evaluate the output content
according to the reward rules, then calculate and align the score with the
reward functions for supervision. We implement dual-track SFT to optimize both
reasoning and evaluation capabilities, and mixed it with RL training to achieve
multi-objective hybrid optimization.
  Experimental results on different datasets and models demonstrate consistent
improvements over traditional SFT and RL methods. Our method provides a new
technical path for language model training that enhances output quality while
preserving deployment efficiency.

</details>


### [218] [EMBRACE: Shaping Inclusive Opinion Representation by Aligning Implicit Conversations with Social Norms](https://arxiv.org/abs/2507.20264)
*Abeer Aldayel,Areej Alokaili*

Main category: cs.CL

TL;DR: 本研究提出了一种新方法，通过分析对话中隐含的意见来评估和改进 NLP 模型的包容性，解决了现有方法过度依赖显式线索的问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型在生成包容性表征时，往往依赖于用户人口统计或社会群体的行为属性等显式线索，忽视了对话中隐含的意见表达，这可能导致模型输出中的不匹配和有害的刻板印象。因此，本研究旨在评估 NLP 或计算模型中意见的表征方式，以解决这些问题。

Method: 本研究引入了一个对 NLP 或计算模型中意见如何表征进行评估的对齐评估框架，该框架优先考虑了隐含的、常被忽视的对话，并评估了规范的社会观点和话语。通过对响应的立场进行建模，作为对潜在意见的代理，从而实现对各种社会观点的周到和反思性表征。我们使用（i）带有基础分类器的正例未标记（PU）在线学习和（ii）指令调优语言模型来评估该框架，以评估训练后对齐。

Result: 本研究通过 PU 在线学习和指令调优语言模型评估了该框架，展示了隐含意见如何被（错误）表征，并为实现更具包容性的模型行为提供了一个途径。

Conclusion: 本研究提出了一种新的评估框架，通过对模型响应的立场进行建模，来评估 NLP 模型中意见的表征方式，并为实现更具包容性的模型行为提供了途径。

Abstract: Shaping inclusive representations that embrace diversity and ensure fair
participation and reflections of values is at the core of many
conversation-based models. However, many existing methods rely on surface
inclusion using mention of user demographics or behavioral attributes of social
groups. Such methods overlook the nuanced, implicit expression of opinion
embedded in conversations. Furthermore, the over-reliance on overt cues can
exacerbate misalignment and reinforce harmful or stereotypical representations
in model outputs. Thus, we took a step back and recognized that equitable
inclusion needs to account for the implicit expression of opinion and use the
stance of responses to validate the normative alignment. This study aims to
evaluate how opinions are represented in NLP or computational models by
introducing an alignment evaluation framework that foregrounds implicit, often
overlooked conversations and evaluates the normative social views and
discourse. Our approach models the stance of responses as a proxy for the
underlying opinion, enabling a considerate and reflective representation of
diverse social viewpoints. We evaluate the framework using both (i)
positive-unlabeled (PU) online learning with base classifiers, and (ii)
instruction-tuned language models to assess post-training alignment. Through
this, we provide a lens on how implicit opinions are (mis)represented and offer
a pathway toward more inclusive model behavior.

</details>


### [219] [AQUA: A Large Language Model for Aquaculture & Fisheries](https://arxiv.org/abs/2507.20520)
*Praneeth Narisetty,Uday Kumar Reddy Kattamanchi,Lohit Akshant Nimma,Sri Ram Kaushik Karnati,Shiva Nagendra Babu Kore,Mounika Golamari,Tejashree Nageshreddy*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Aquaculture plays a vital role in global food security and coastal economies
by providing sustainable protein sources. As the industry expands to meet
rising demand, it faces growing challenges such as disease outbreaks,
inefficient feeding practices, rising labor costs, logistical inefficiencies,
and critical hatchery issues, including high mortality rates and poor water
quality control. Although artificial intelligence has made significant
progress, existing machine learning methods fall short of addressing the
domain-specific complexities of aquaculture. To bridge this gap, we introduce
AQUA, the first large language model (LLM) tailored for aquaculture, designed
to support farmers, researchers, and industry practitioners. Central to this
effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic
Framework for generating and refining high-quality synthetic data using a
combination of expert knowledge, largescale language models, and automated
evaluation techniques. Our work lays the foundation for LLM-driven innovations
in aquaculture research, advisory systems, and decision-making tools.

</details>


### [220] [MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning](https://arxiv.org/abs/2507.20278)
*Kang Yang,Jingxue Chen,Qingkun Tang,Tianxiang Zhang,Qianchun Lu*

Main category: cs.CL

TL;DR: MoL-RL 通过 MoL 持续训练和 GRPO 后期训练，有效利用多步环境反馈提升 LLMs 推理能力，在数学和代码任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用顺序环境反馈（EF）信号（如自然语言评估）进行反馈独立性思维链（CoT）推理方面存在不足。现有方法要么将 EF 转换为标量奖励，丢失了丰富的上下文信息，要么使用改进数据集，未能利用 EF 交互的多步和离散性质。

Method: MoL-RL 训练范式，整合多步环境反馈（EF）信号，采用双目标优化框架。具体包括：1. MoL（Mixture-of-Losses）持续训练，将特定领域 EF 信号（用交叉熵损失优化）与通用语言能力（用 KL 散度保留）解耦。2. GRPO-based 后期训练，将顺序 EF 交互提炼为单步推理。

Result: 在数学推理（MATH-500, AIME24/AIME25）和代码生成（CodeAgent-Test）基准测试中，MoL-RL 在 Qwen3-8B 模型上取得了最先进的性能，并在不同模型规模（Qwen3-4B）上保持了强大的泛化能力。

Conclusion: MoL-RL 是一种创新的训练范式，通过双目标优化框架将多步环境反馈（EF）信号整合到大型语言模型（LLMs）中。该方法结合了 MoL（Mixture-of-Losses）持续训练（通过交叉熵损失优化特定领域的 EF 信号，并通过 Kullback-Leibler 散度保留通用语言能力）和基于 GRPO 的后期训练（将顺序 EF 交互提炼为单步推理），实现了无需外部反馈即可进行鲁棒的、与反馈无关的推理。

Abstract: Large language models (LLMs) face significant challenges in effectively
leveraging sequential environmental feedback (EF) signals, such as natural
language evaluations, for feedback-independent chain-of-thought (CoT)
reasoning. Existing approaches either convert EF into scalar rewards, losing
rich contextual information, or employ refinement datasets, failing to exploit
the multi-step and discrete nature of EF interactions. To address these
limitations, we propose MoL-RL, a novel training paradigm that integrates
multi-step EF signals into LLMs through a dual-objective optimization
framework. Our method combines MoL (Mixture-of-Losses) continual training,
which decouples domain-specific EF signals (optimized via cross-entropy loss)
and general language capabilities (preserved via Kullback-Leibler divergence),
with GRPO-based post-training to distill sequential EF interactions into
single-step inferences. This synergy enables robust feedback-independent
reasoning without relying on external feedback loops. Experimental results on
mathematical reasoning (MATH-500, AIME24/AIME25) and code generation
(CodeAgent-Test) benchmarks demonstrate that MoL-RL achieves state-of-the-art
performance with the Qwen3-8B model, while maintaining strong generalization
across model scales (Qwen3-4B). This work provides a promising approach for
leveraging multi-step textual feedback to enhance LLMs' reasoning capabilities
in diverse domains.

</details>


### [221] [What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations](https://arxiv.org/abs/2507.20279)
*Katharina Trinley,Toshiki Nakai,Tatiana Anikina,Tanja Baeumel*

Main category: cs.CL

TL;DR: Aya-23等大型语言模型在多语言任务中的处理方式与单语模型不同，其神经元激活模式受语言类型和混合比例影响，且特定语言神经元集中在最终层。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLMs）在多语言任务中的内部语言处理机制。

Method: 通过Logit Lens和神经元特化分析，并结合神经元重叠分析。

Result: 1. Aya-23在翻译时激活类型上相关的语言表征，与依赖单一枢轴语言的英语中心模型不同；2. 混合语言的神经元激活模式随混合比例变化，并更多地受基础语言而非混入语言的影响；3. Aya-23的混合语言输入特定神经元集中在最终层，这与之前关于仅解码器模型的研究结果不同。

Conclusion: LLM的内部语言处理机制因多语言训练而异，脚本相似性和类型学关系会影响跨模型类型的处理方式。Aya-23在处理混合语言、完形填空和翻译任务时，其神经元激活模式与单语模型不同，尤其是在混合语言输入方面，其特定语言神经元集中在模型的最终层。

Abstract: Large language models (LLMs) excel at multilingual tasks, yet their internal
language processing remains poorly understood. We analyze how Aya-23-8B, a
decoder-only LLM trained on balanced multilingual data, handles code-mixed,
cloze, and translation tasks compared to predominantly monolingual models like
Llama 3 and Chinese-LLaMA-2. Using logit lens and neuron specialization
analyses, we find: (1) Aya-23 activates typologically related language
representations during translation, unlike English-centric models that rely on
a single pivot language; (2) code-mixed neuron activation patterns vary with
mixing rates and are shaped more by the base language than the mixed-in one;
and (3) Aya-23's languagespecific neurons for code-mixed inputs concentrate in
final layers, diverging from prior findings on decoder-only models. Neuron
overlap analysis further shows that script similarity and typological relations
impact processing across model types. These findings reveal how multilingual
training shapes LLM internals and inform future cross-lingual transfer
research.

</details>


### [222] [Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation](https://arxiv.org/abs/2507.20301)
*Abdullah Alabdullah,Lifeng Han,Chenghua Lin*

Main category: cs.CL

TL;DR: 本研究通过评估提示技术和开发高效微调流程，显著提升了阿拉伯语方言到标准阿拉伯语的机器翻译质量，尤其是在资源受限的情况下，为实现更广泛的语言包容性奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方言（DA）与现代标准阿拉伯语（MSA）的显著差异给自然语言处理（NLP）带来了挑战，限制了数字服务和教育资源的普及，并阻碍了阿拉伯语机器翻译的进展。本研究旨在弥合这一语言鸿沟，提升阿拉伯语方言的NLP应用能力。

Method: 本研究评估了在六种大型语言模型上的训练无关提示技术（包括零样本、少样本、思维链和Ara-TEaR），并开发了一个资源高效的微调流程。实验针对列万特、埃及和海湾阿拉伯语方言进行了测试，并使用了量化技术来降低资源消耗。

Result: 少样本提示在所有提示设置中均优于零样本、思维链和Ara-TEaR方法，其中GPT-4o表现最佳。微调方面，量化的Gemma2-9B模型达到了49.88的CHrF++得分，优于零样本GPT-4o（44.58）。联合多方言训练模型比单一语种模型在CHrF++上提升超过10%，4位量化将内存使用量减少了60%，性能损失不到1%。

Conclusion: 本研究提出的资源高效微调流程和训练无关提示技术，为在资源受限的情况下提升阿拉伯语方言到标准阿拉伯语的机器翻译质量提供了切实可行的方案，展示了通过优化模型和训练策略可以实现包容性更强的语言技术。

Abstract: Dialectal Arabic (DA) poses a persistent challenge for natural language
processing (NLP), as most everyday communication in the Arab world occurs in
dialects that diverge significantly from Modern Standard Arabic (MSA). This
linguistic divide limits access to digital services and educational resources
and impedes progress in Arabic machine translation. This paper presents two
core contributions to advancing DA-MSA translation for the Levantine, Egyptian,
and Gulf dialects, particularly in low-resource and computationally constrained
settings: a comprehensive evaluation of training-free prompting techniques, and
the development of a resource-efficient fine-tuning pipeline. Our evaluation of
prompting strategies across six large language models (LLMs) found that
few-shot prompting consistently outperformed zero-shot, chain-of-thought, and
our proposed Ara-TEaR method. GPT-4o achieved the highest performance across
all prompting settings. For fine-tuning, a quantized Gemma2-9B model achieved a
CHrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint
multi-dialect trained models outperformed single-dialect counterparts by over
10% CHrF++, and 4-bit quantization reduced memory usage by 60% with less than
1% performance loss. The results and insights of our experiments offer a
practical blueprint for improving dialectal inclusion in Arabic NLP, showing
that high-quality DA-MSA machine translation is achievable even with limited
resources and paving the way for more inclusive language technologies.

</details>


### [223] [DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns](https://arxiv.org/abs/2507.20343)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: DYNARTmo是一个动态发音模型，用于在二维中矢状面中可视化语音发音过程。它基于UK-DYNAMO框架，并整合了发音不足、节段和姿态控制以及发音协同作用的原理。该模型能够生成元音和辅音的发音构型，并已嵌入到一个用于语音学教育和语音治疗的Web应用程序中。


<details>
  <summary>Details</summary>
Motivation: 该模型适用于语音学教育和语音治疗。

Method: 该模型基于UK-DYNAMO框架，并整合了发音不足、节段和姿态控制以及发音协同作用的原理。DYNARTmo基于十个连续和六个离散控制参数模拟了六个关键的发音器官，能够生成元音和辅音的发音构型。

Result: 目前的实现嵌入了一个基于Web的应用程序（SpeechArticulationTrainer），包括矢状面、声门和腭面视图。

Conclusion: DYNARTmo是一个动态发音模型，用于在二维中矢状面中可视化语音发音过程。

Abstract: We present DYNARTmo, a dynamic articulatory model designed to visualize
speech articulation processes in a two-dimensional midsagittal plane. The model
builds upon the UK-DYNAMO framework and integrates principles of articulatory
underspecification, segmental and gestural control, and coarticulation.
DYNARTmo simulates six key articulators based on ten continuous and six
discrete control parameters, allowing for the generation of both vocalic and
consonantal articulatory configurations. The current implementation is embedded
in a web-based application (SpeechArticulationTrainer) that includes sagittal,
glottal, and palatal views, making it suitable for use in phonetics education
and speech therapy. While this paper focuses on the static modeling aspects,
future work will address dynamic movement generation and integration with
articulatory-acoustic modules.

</details>


### [224] [RMTBench: Benchmarking LLMs Through Multi-Turn User-Centric Role-Playing](https://arxiv.org/abs/2507.20352)
*Hao Xiang,Tianyi Tang,Yang Su,Bowen Yu,An Yang,Fei Huang,Yichang Zhang,Yaojie Lu,Hongyu Lin,Xianpei Han,Jingren Zhou,Junyang Lin,Le Sun*

Main category: cs.CL

TL;DR: RMTBench是一个新的人LLM角色扮演基准，它以用户为中心，而不是以角色为中心，并专注于用户意图的实现。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM角色扮演基准主要采用以角色为中心的方法，并将用户-角色交互简化为孤立的问答任务，未能反映真实世界的应用。为了解决这一限制，需要一个更全面的、以用户为中心的基准。

Method: RMTBench是一个全面的、以用户为中心的双语角色扮演基准，包含80个不同的角色和8000多个对话回合。它通过包含自定义角色和抽象角色来支持各种用户场景的评估。该基准通过明确的用户动机而非角色描述来构建对话，并包含一个真实的、基于LLM评分的多轮对话模拟机制，以捕捉用户和角色之间对话的复杂意图。

Result: RMTBench包含80个不同的角色和8000多个对话回合，旨在更有效地评估LLM的角色扮演能力，使其更符合实际应用。

Conclusion: RMTBench通过从以角色为中心转向以用户为中心，并专注于用户意图的实现，弥合了学术评估与实际部署需求之间的差距，为评估LLM的角色扮演能力提供了一个更有效的框架。

Abstract: Recent advancements in Large Language Models (LLMs) have shown outstanding
potential for role-playing applications. Evaluating these capabilities is
becoming crucial yet remains challenging. Existing benchmarks mostly adopt a
\textbf{character-centric} approach, simplify user-character interactions to
isolated Q&A tasks, and fail to reflect real-world applications. To address
this limitation, we introduce RMTBench, a comprehensive \textbf{user-centric}
bilingual role-playing benchmark featuring 80 diverse characters and over 8,000
dialogue rounds. RMTBench includes custom characters with detailed backgrounds
and abstract characters defined by simple traits, enabling evaluation across
various user scenarios. Our benchmark constructs dialogues based on explicit
user motivations rather than character descriptions, ensuring alignment with
practical user applications. Furthermore, we construct an authentic multi-turn
dialogue simulation mechanism. With carefully selected evaluation dimensions
and LLM-based scoring, this mechanism captures the complex intention of
conversations between the user and the character. By shifting focus from
character background to user intention fulfillment, RMTBench bridges the gap
between academic evaluation and practical deployment requirements, offering a
more effective framework for assessing role-playing capabilities in LLMs. All
code and datasets will be released soon.

</details>


### [225] [Length Representations in Large Language Models](https://arxiv.org/abs/2507.20398)
*Sangjun Moon,Dasom Choi,Jingun Kwon,Hidetaka Kamigaito,Manabu Okumura*

Main category: cs.CL

TL;DR: LLMs 拥有内置的长度控制机制，主要由多头注意力机制驱动，并且可以通过调整特定隐藏单元来控制。


<details>
  <summary>Details</summary>
Motivation: LLMs 在各种任务中展现了非凡的能力，它们在很大程度上是从海量的文本数据中学习的。尽管 LLMs 可以控制输出序列的长度，尤其是在基于指令的设置中，但这种控制背后的内部机制尚未得到探索。

Method: 通过缩放模型内的特定隐藏单元来控制输出序列长度，同时不损失生成文本的信息量，表明长度信息与语义信息部分解耦。

Result: 研究结果表明，多头注意力机制在确定输出序列长度方面起着关键作用，并且可以以一种解耦的方式进行调整。一些隐藏单元在提示变得更具长度特异性时变得越来越活跃，这反映了模型对这一属性的内部认知。

Conclusion: LLMs 学习了控制输出长度的鲁棒且自适应的内部机制，而无需外部控制。

Abstract: Large language models (LLMs) have shown remarkable capabilities across
various tasks, that are learned from massive amounts of text-based data.
Although LLMs can control output sequence length, particularly in
instruction-based settings, the internal mechanisms behind this control have
been unexplored yet. In this study, we provide empirical evidence on how output
sequence length information is encoded within the internal representations in
LLMs. In particular, our findings show that multi-head attention mechanisms are
critical in determining output sequence length, which can be adjusted in a
disentangled manner. By scaling specific hidden units within the model, we can
control the output sequence length without losing the informativeness of the
generated text, thereby indicating that length information is partially
disentangled from semantic information. Moreover, some hidden units become
increasingly active as prompts become more length-specific, thus reflecting the
model's internal awareness of this attribute. Our findings suggest that LLMs
have learned robust and adaptable internal mechanisms for controlling output
length without any external control.

</details>


### [226] [Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations](https://arxiv.org/abs/2507.20409)
*Eunkyu Park,Wesley Hanwen Deng,Gunhee Kim,Motahhare Eslami,Maarten Sap*

Main category: cs.CL

TL;DR: CoCoT是一种新的提示策略，通过三个认知阶段（感知、情境、规范）来提高视觉语言模型的推理能力，尤其是在处理需要理解社会背景的任务时。


<details>
  <summary>Details</summary>
Motivation: 在需要将感知与基于规范的判断相结合的视觉任务中，例如基于社会背景的视觉任务，链式思考（CoT）通常会失败。因此，需要一种新的方法来解决这个问题。

Method: 本研究引入了一种名为认知链思想（CoCoT）的提示策略，该策略通过三个认知上受启发的阶段来支架视觉语言模型（VLMs）的推理：感知、情境和规范。

Result: CoCoT在多个多模态基准测试中始终优于CoT和直接提示，平均提高了8%。

Conclusion: 认知链思想（CoCoT）通过认知上合理的方法提高了视觉语言模型（VLMs）在多模态任务中的表现，并通过认知上合理的方法提高了它们的解释性和社会意识，为更安全、更可靠的多模态系统铺平了道路。

Abstract: Chain-of-Thought (CoT) prompting helps models think step by step. But what
happens when they must see, understand, and judge-all at once? In visual tasks
grounded in social context, where bridging perception with norm-grounded
judgments is essential, flat CoT often breaks down. We introduce Cognitive
Chain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning
through three cognitively inspired stages: perception, situation, and norm. Our
experiments show that, across multiple multimodal benchmarks (including intent
disambiguation, commonsense reasoning, and safety), CoCoT consistently
outperforms CoT and direct prompting (+8\% on average). Our findings
demonstrate that cognitively grounded reasoning stages enhance interpretability
and social awareness in VLMs, paving the way for safer and more reliable
multimodal systems.

</details>


### [227] [CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning](https://arxiv.org/abs/2507.20411)
*George Ibrahim,Rita Ramos,Yova Kementchedjhieva*

Main category: cs.CL

TL;DR: CONCAP 是一种多语言图像字幕模型，通过结合检索到的标题和图像概念，提高了低资源语言的性能，并减少了对数据的需求。


<details>
  <summary>Details</summary>
Motivation: 为了解决多语言视觉-语言模型在多语言训练数据有限和模型参数化成本高昂的情况下，其图像字幕生成能力落后于英语模型的问题，并克服现有检索增强生成（RAG）模型依赖从英语翻译的标题而可能引入的失配和语言偏差。

Method: CONCAP 模型集成检索到的标题和图像特定概念，以增强语境化并跨语言实现字幕生成。

Result: CONCAP 在 XM3600 数据集上的实验表明，在数据需求大大降低的情况下，为低资源和中等资源语言实现了强大的性能。

Conclusion: CONCAP 模型通过整合检索到的标题和图像特定概念，增强了输入图像的语境化，并在不同语言中实现了字幕生成，在低资源和中等资源语言上表现出色，同时大大降低了数据需求。

Abstract: Multilingual vision-language models have made significant strides in image
captioning, yet they still lag behind their English counterparts due to limited
multilingual training data and costly large-scale model parameterization.
Retrieval-augmented generation (RAG) offers a promising alternative by
conditioning caption generation on retrieved examples in the target language,
reducing the need for extensive multilingual training. However, multilingual
RAG captioning models often depend on retrieved captions translated from
English, which can introduce mismatches and linguistic biases relative to the
source language. We introduce CONCAP, a multilingual image captioning model
that integrates retrieved captions with image-specific concepts, enhancing the
contextualization of the input image and grounding the captioning process
across different languages. Experiments on the XM3600 dataset indicate that
CONCAP enables strong performance on low- and mid-resource languages, with
highly reduced data requirements. Our findings highlight the effectiveness of
concept-aware retrieval augmentation in bridging multilingual performance gaps.

</details>


### [228] [Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?](https://arxiv.org/abs/2507.20419)
*Khloud AL Jallad,Nada Ghneim,Ghaida Rebdawi*

Main category: cs.CL

TL;DR: 本次调查全面回顾和分析了英语、阿拉伯语和多语言的NLU基准，重点关注其诊断数据集和语言现象。研究发现，在命名约定和标准语言现象集方面存在不足，并提出了建立评估指标的必要性，以促进对不同模型在不同诊断基准上的结果进行更深入的比较。


<details>
  <summary>Details</summary>
Motivation: 评估NLU能力已成为NLP领域的热门研究课题，旨在为预训练模型提供评估工具，并进行细粒度的错误分析。

Method: 对现有英语、阿拉伯语和多语言NLU基准进行全面回顾、详细比较和分析，重点关注其诊断数据集和所涵盖的语言现象。

Result: 现有NLU基准在评估NLU任务和提供深入错误分析方面各有优劣。研究发现，在命名约定和标准语言现象集方面存在不足，并提出建立NLU评估诊断基准的评估指标的必要性。

Conclusion: 该调查全面回顾了现有的英语、阿拉伯语和多语言自然语言理解（NLU）基准，重点关注其诊断数据集和所涵盖的语言现象。研究人员强调，目前尚无用于宏观和微观类别或标准语言现象集命名约定，并提出了关于NLU评估诊断基准的评估指标的研究问题。

Abstract: Natural Language Understanding (NLU) is a basic task in Natural Language
Processing (NLP). The evaluation of NLU capabilities has become a trending
research topic that attracts researchers in the last few years, resulting in
the development of numerous benchmarks. These benchmarks include various tasks
and datasets in order to evaluate the results of pretrained models via public
leaderboards. Notably, several benchmarks contain diagnostics datasets designed
for investigation and fine-grained error analysis across a wide range of
linguistic phenomena. This survey provides a comprehensive review of available
English, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on
their diagnostics datasets and the linguistic phenomena they covered. We
present a detailed comparison and analysis of these benchmarks, highlighting
their strengths and limitations in evaluating NLU tasks and providing in-depth
error analysis. When highlighting the gaps in the state-of-the-art, we noted
that there is no naming convention for macro and micro categories or even a
standard set of linguistic phenomena that should be covered. Consequently, we
formulated a research question regarding the evaluation metrics of the
evaluation diagnostics benchmarks: "Why do not we have an evaluation standard
for the NLU evaluation diagnostics benchmarks?" similar to ISO standard in
industry. We conducted a deep analysis and comparisons of the covered
linguistic phenomena in order to support experts in building a global hierarchy
for linguistic phenomena in future. We think that having evaluation metrics for
diagnostics evaluation could be valuable to gain more insights when comparing
the results of the studied models on different diagnostics benchmarks.

</details>


### [229] [CodeNER: Code Prompting for Named Entity Recognition](https://arxiv.org/abs/2507.20423)
*Sungwoo Han,Hyeyeon Kim,Jingun Kwon,Hidetaka Kamigaito,Manabu Okumura*

Main category: cs.CL

TL;DR: 代码提示比文本提示更能改进命名实体识别，并且结合思维链提示效果更佳。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在命名实体识别时仅依赖输入上下文信息，而命名实体识别需要详细的标签要求和上下文信息，因此提出代码提示来解决这个问题。

Method: 提出了一种利用代码提示来改进大型语言模型在命名实体识别任务上的方法，通过在提示中嵌入代码来提供详细的BIO模式标签说明。

Result: 所提出的代码提示方法在包括英语、阿拉伯语、芬兰语、丹麦语和德语在内的十个基准数据集上，其表现优于传统的基于文本的提示方法。

Conclusion: 代码提示可以改进大型语言模型在命名实体识别任务上的表现，并且可以与思维链提示相结合以获得更好的结果。

Abstract: Recent studies have explored various approaches for treating candidate named
entity spans as both source and target sequences in named entity recognition
(NER) by leveraging large language models (LLMs). Although previous approaches
have successfully generated candidate named entity spans with suitable labels,
they rely solely on input context information when using LLMs, particularly,
ChatGPT. However, NER inherently requires capturing detailed labeling
requirements with input context information. To address this issue, we propose
a novel method that leverages code-based prompting to improve the capabilities
of LLMs in understanding and performing NER. By embedding code within prompts,
we provide detailed BIO schema instructions for labeling, thereby exploiting
the ability of LLMs to comprehend long-range scopes in programming languages.
Experimental results demonstrate that the proposed code-based prompting method
outperforms conventional text-based prompting on ten benchmarks across English,
Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of
explicitly structuring NER instructions. We also verify that combining the
proposed code-based prompting method with the chain-of-thought prompting
further improves performance.

</details>


### [230] [Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems](https://arxiv.org/abs/2507.20491)
*Tuan Bui,Trong Le,Phat Thai,Sang Nguyen,Minh Hua,Ngan Pham,Thang Bui,Tho Quan*

Main category: cs.CL

TL;DR: 提出Text-JEPA框架，将自然语言转为逻辑推理，解决了现有神经符号方法的效率问题，在特定领域问答中效果好且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号（NeSy）框架虽然有潜力，但通常依赖于大型模型，并且在将自然语言转换为形式逻辑表示方面存在效率低下等问题。因此，需要更轻量、更高效的解决方案来满足特定领域对准确答案、透明推理和可解释决策过程的需求。

Method: 提出了一种名为Text-JEPA的轻量级框架，用于将自然语言转换为一阶逻辑（NL2FOL）。该框架借鉴双系统认知理论，模拟了“系统1”的快速逻辑表征生成和“系统2”的Z3求解器进行推理。

Result: Text-JEPA框架在特定领域的基准测试中表现出与大型LLM系统相当的性能，同时计算开销显著降低。提出的三种自定义指标（转换分数、推理分数和Spearman rho分数）能够全面评估NL2FOL到推理的流程质量。

Conclusion: Text-JEPA框架在教育、医疗和法律等特定领域中，能够以显著更低的计算开销实现具有竞争力的性能，证明了其在构建高效、可解释的问答系统方面的潜力。

Abstract: Recent advances in large language models (LLMs) have significantly enhanced
question-answering (QA) capabilities, particularly in open-domain contexts.
However, in closed-domain scenarios such as education, healthcare, and law,
users demand not only accurate answers but also transparent reasoning and
explainable decision-making processes. While neural-symbolic (NeSy) frameworks
have emerged as a promising solution, leveraging LLMs for natural language
understanding and symbolic systems for formal reasoning, existing approaches
often rely on large-scale models and exhibit inefficiencies in translating
natural language into formal logic representations.
  To address these limitations, we introduce Text-JEPA (Text-based
Joint-Embedding Predictive Architecture), a lightweight yet effective framework
for converting natural language into first-order logic (NL2FOL). Drawing
inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by
efficiently generating logic representations, while the Z3 solver operates as
System 2, enabling robust logical inference. To rigorously evaluate the
NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework
comprising three custom metrics: conversion score, reasoning score, and
Spearman rho score, which collectively capture the quality of logical
translation and its downstream impact on reasoning accuracy.
  Empirical results on domain-specific datasets demonstrate that Text-JEPA
achieves competitive performance with significantly lower computational
overhead compared to larger LLM-based systems. Our findings highlight the
potential of structured, interpretable reasoning frameworks for building
efficient and explainable QA systems in specialized domains.

</details>


### [231] [SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers](https://arxiv.org/abs/2507.20527)
*Chaitanya Manem,Pratik Prabhanjan Brahma,Prakamya Mishra,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: 本研究提出了SAND-Math，一个生成合成数学问题的管线，通过难度提升策略增强了LLM的数学推理能力，并在AIME25测试中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 工业界对能够进行复杂数学推理的大语言模型（LLMs）的需求日益增长，但高质量、新颖的训练数据稀缺是该领域发展的瓶颈。

Method: 开发了一个名为SAND-Math的数据生成管线，该管线首先从头开始生成高质量的数学问题，然后通过“难度提升”步骤系统地增加问题的难度。

Result: 在AIME25基准测试中，使用SAND-Math数据增强的基线模型性能显著提升，比次优的合成数据集高出17.85个绝对点。此外，难度提升步骤将平均问题难度从5.02提高到5.98，并将AIME25性能从46.38%提升到49.23%。

Conclusion: 该研究引入了SAND-Math，一个用于训练数学推理大语言模型的合成数据生成管线，通过“难度提升”步骤系统地增加了问题难度，并在AIME25基准测试中显著提高了模型性能，相比现有合成数据集有17.85个绝对点的提升。

Abstract: The demand for Large Language Models (LLMs) capable of sophisticated
mathematical reasoning is growing across industries. However, the development
of performant mathematical LLMs is critically bottlenecked by the scarcity of
difficult, novel training data. We introduce \textbf{SAND-Math} (Synthetic
Augmented Novel and Difficult Mathematics problems and solutions), a pipeline
that addresses this by first generating high-quality problems from scratch and
then systematically elevating their complexity via a new \textbf{Difficulty
Hiking} step. We demonstrate the effectiveness of our approach through two key
findings. First, augmenting a strong baseline with SAND-Math data significantly
boosts performance, outperforming the next-best synthetic dataset by
\textbf{$\uparrow$ 17.85 absolute points} on the AIME25 benchmark. Second, in a
dedicated ablation study, we show our Difficulty Hiking process is highly
effective: by increasing average problem difficulty from 5.02 to 5.98, this
step lifts AIME25 performance from 46.38\% to 49.23\%. The full generation
pipeline, final dataset, and a fine-tuned model form a practical and scalable
toolkit for building more capable and efficient mathematical reasoning LLMs.
SAND-Math dataset is released here:
\href{https://huggingface.co/datasets/amd/SAND-MATH}{https://huggingface.co/datasets/amd/SAND-MATH}

</details>


### [232] [Dialogues of Dissent: Thematic and Rhetorical Dimensions of Hate and Counter-Hate Speech in Social Media Conversations](https://arxiv.org/abs/2507.20528)
*Effi Levi,Gal Ron,Odelia Oshri,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 提出了一种新的注释方案，用于对社交媒体对话中的仇恨和反仇恨言论进行分类，并分析了其传播和反击策略。


<details>
  <summary>Details</summary>
Motivation: 介绍了一种用于社交媒体对话中仇恨和反仇恨言论联合注释的新颖多标签方案。

Method: 提出了一种新颖的多标签方案，用于对社交媒体对话中的仇恨和反仇恨言论进行联合注释，将仇恨和反仇恨信息分为主题和修辞维度。注释了包含 720 条推文的 92 个对话样本，并进行了统计分析，结合了公共指标，以探索仇恨和反仇恨言论内部以及它们之间主题和修辞维度之间的交互模式。

Result: 分析了主题和修辞维度在仇恨和反仇恨言论内部以及它们之间的交互模式。

Conclusion: 研究结果为了解社交媒体上仇恨信息的传播、反击策略及其对在线行为的潜在影响提供了见解。

Abstract: We introduce a novel multi-labeled scheme for joint annotation of hate and
counter-hate speech in social media conversations, categorizing hate and
counter-hate messages into thematic and rhetorical dimensions. The thematic
categories outline different discursive aspects of each type of speech, while
the rhetorical dimension captures how hate and counter messages are
communicated, drawing on Aristotle's Logos, Ethos and Pathos. We annotate a
sample of 92 conversations, consisting of 720 tweets, and conduct statistical
analyses, incorporating public metrics, to explore patterns of interaction
between the thematic and rhetorical dimensions within and between hate and
counter-hate speech. Our findings provide insights into the spread of hate
messages on social media, the strategies used to counter them, and their
potential impact on online behavior.

</details>


### [233] [Enhancing Hallucination Detection via Future Context](https://arxiv.org/abs/2507.20546)
*Joosung Lee,Cheonbok Park,Hwiyeol Jo,Jeonghoon Kim,Joonsuk Park,Kang Min Yoo*

Main category: cs.CL

TL;DR: LLMs生成文本时会产生幻觉，而检测幻觉是一个难题。本文提出了一种通过采样未来上下文的检测框架，以解决黑盒生成器产生的幻觉问题。实验表明，该框架能有效提升多种检测方法的性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在未经披露生成过程的情况下被广泛用于在线平台生成可信文本。由于用户越来越多地遇到此类黑盒输出，因此检测幻觉已成为一项关键挑战。

Method: 该框架通过观察幻觉一旦引入往往会持续存在，并通过采样未来的上下文来解决这一挑战。所采样的未来上下文为幻觉检测提供了有价值的线索，并可以有效地与各种基于采样的检测方法集成。

Result: 通过在多个方法上进行的大量实验证明，该框架在性能上有所提升。

Conclusion: LLMs在未经披露生成过程的情况下被广泛用于在线平台生成可信文本。由于用户越来越多地遇到此类黑盒输出，因此检测幻觉已成为一项关键挑战。为了应对这一挑战，我们专注于为黑盒生成器开发幻觉检测框架。

Abstract: Large Language Models (LLMs) are widely used to generate plausible text on
online platforms, without revealing the generation process. As users
increasingly encounter such black-box outputs, detecting hallucinations has
become a critical challenge. To address this challenge, we focus on developing
a hallucination detection framework for black-box generators. Motivated by the
observation that hallucinations, once introduced, tend to persist, we sample
future contexts. The sampled future contexts provide valuable clues for
hallucination detection and can be effectively integrated with various
sampling-based methods. We extensively demonstrate performance improvements
across multiple methods using our proposed sampling approach.

</details>


### [234] [ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning](https://arxiv.org/abs/2507.20564)
*Duc-Tai Dinh,Duc Anh Khoa Dinh*

Main category: cs.CL

TL;DR: ZSE-Cap 是一个零样本系统，通过集成 CLIP、SigLIP、DINOv2 和 Gemma 3 模型（配合提示工程）在文章检索和图像描述任务中取得第四名，无需微调。


<details>
  <summary>Details</summary>
Motivation: 本次研究的目的是在文章检索和图像描述任务中，探索一种无需微调的零样本方法，并验证集成多种基础模型和利用提示工程的有效性。

Method: ZSE-Cap 系统采用零样本方法，无需在竞赛数据上进行微调。检索部分集成了 CLIP、SigLIP 和 DINOv2 的相似度得分。描述部分则利用了精心设计的提示来引导 Gemma 3 模型，使其能够将文章中的高级事件与图像内容相关联。

Result: ZSE-Cap 系统在 EVENTA 竞赛的私有测试集上获得了 0.42002 的最终分数，排名第四，证明了集成基础模型和提示工程在零样本场景下的有效性。

Conclusion: ZSE-Cap 系统通过集成基础模型（CLIP、SigLIP、DINOv2）和精心设计的 Gemma 3 模型提示，在零样本设置下成功实现了文章检索和图像描述任务，在 EVENTA 竞赛中取得了第四名的成绩。

Abstract: We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system
in Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image
retrieval and captioning. Our zero-shot approach requires no finetuning on the
competition's data. For retrieval, we ensemble similarity scores from CLIP,
SigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt
to guide the Gemma 3 model, enabling it to link high-level events from the
article to the visual content in the image. Our system achieved a final score
of 0.42002, securing a top-4 position on the private test set, demonstrating
the effectiveness of combining foundation models through ensembling and
prompting. Our code is available at https://github.com/ductai05/ZSE-Cap.

</details>


### [235] [Before the Outrage: Challenges and Advances in Predicting Online Antisocial Behavior](https://arxiv.org/abs/2507.20614)
*Anaïs Ollagnier*

Main category: cs.CL

TL;DR: 本篇论文对社交媒体上的反社会行为（ASB）预测进行了系统性综述，提出了五种任务类型（早期危害检测、危害出现预测、危害传播预测、行为风险预测、主动审核支持），分析了不同方法和数据集的影响，并指出了未来的研究方向，如多语言模型和跨平台泛化。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的反社会行为（ASB）构成了日益严峻的平台安全和社会福祉挑战。现有工作主要集中在事后检测，而预测性方法旨在事前预测未来的有害行为。尽管兴趣日益浓厚，但该领域仍然分散，缺乏统一的分类法或现有方法的清晰综合。

Method: 通过对49项以上关于ASB预测的研究进行系统性回顾，提出一个包含五种核心任务类型的结构化分类法：早期危害检测、危害出现预测、危害传播预测、行为风险预测和主动审核支持。分析了这些任务在时间框架、预测粒度和操作目标上的差异。此外，还考察了从经典机器学习到预训练语言模型等建模技术的发展趋势，并评估了数据集特征对任务可行性和泛化能力的影响。

Result: 对五种核心任务类型进行了分类，分析了它们在时间框架、预测粒度和操作目标上的差异，并考察了建模技术和数据集特征的影响。同时，指出了数据集稀缺、时间漂移和基准有限等方法论挑战，并概述了多语言建模、跨平台泛化和人工在回路系统等新兴研究方向。

Conclusion: 该系统性综述旨在通过一个连贯的框架来组织反社会行为（ASB）预测领域，以指导未来的研究朝着更鲁棒和对社会负责的方向发展。

Abstract: Antisocial behavior (ASB) on social media-including hate speech, harassment,
and trolling-poses growing challenges for platform safety and societal
wellbeing. While prior work has primarily focused on detecting harmful content
after it appears, predictive approaches aim to forecast future harmful
behaviors-such as hate speech propagation, conversation derailment, or user
recidivism-before they fully unfold. Despite increasing interest, the field
remains fragmented, lacking a unified taxonomy or clear synthesis of existing
methods. This paper presents a systematic review of over 49 studies on ASB
prediction, offering a structured taxonomy of five core task types: early harm
detection, harm emergence prediction, harm propagation prediction, behavioral
risk prediction, and proactive moderation support. We analyze how these tasks
differ by temporal framing, prediction granularity, and operational goals. In
addition, we examine trends in modeling techniques-from classical machine
learning to pre-trained language models-and assess the influence of dataset
characteristics on task feasibility and generalization. Our review highlights
methodological challenges, such as dataset scarcity, temporal drift, and
limited benchmarks, while outlining emerging research directions including
multilingual modeling, cross-platform generalization, and human-in-the-loop
systems. By organizing the field around a coherent framework, this survey aims
to guide future work toward more robust and socially responsible ASB
prediction.

</details>


### [236] [Ontology-Enhanced Knowledge Graph Completion using Large Language Models](https://arxiv.org/abs/2507.20643)
*Wenbin Guo,Xin Wang,Jiaoyan Chen,Zhao Li,Zirui Chen*

Main category: cs.CL

TL;DR: OL-KGC 通过结合神经感知结构信息和本体知识，利用 LLM 克服了现有 KGC 方法的局限性，在知识图谱补全任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的 LLM 方法依赖于隐式知识表示和错误知识的并行传播，这阻碍了它们产生明确和决定性的推理结果。本研究旨在将神经感知结构信息与本体知识相结合，利用 LLM 的强大能力，实现对知识内在逻辑的更深层次理解。

Method: OL-KGC 方法首先利用神经感知机制将结构信息有效地嵌入文本空间，然后使用自动提取算法从需要完成的知识图中提取本体知识，并将其转换为 LLM 可理解的文本格式，为 LLM 提供逻辑指导。

Result: 实验结果表明，OL-KGC 显著优于现有的主流 KGC 方法，并在多个评估指标上取得了最先进的性能。

Conclusion: OL-KGC 方法在 FB15K-237、UMLS 和 WN18RR 三个广泛使用的基准测试中表现出色，显著优于现有的主流 KGC 方法，并在多个评估指标上达到了最先进的性能。

Abstract: Large Language Models (LLMs) have been extensively adopted in Knowledge Graph
Completion (KGC), showcasing significant research advancements. However, as
black-box models driven by deep neural architectures, current LLM-based KGC
methods rely on implicit knowledge representation with parallel propagation of
erroneous knowledge, thereby hindering their ability to produce conclusive and
decisive reasoning outcomes. We aim to integrate neural-perceptual structural
information with ontological knowledge, leveraging the powerful capabilities of
LLMs to achieve a deeper understanding of the intrinsic logic of the knowledge.
We propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first
leverages neural perceptual mechanisms to effectively embed structural
information into the textual space, and then uses an automated extraction
algorithm to retrieve ontological knowledge from the knowledge graphs (KGs)
that needs to be completed, which is further transformed into a textual format
comprehensible to LLMs for providing logic guidance. We conducted extensive
experiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The
experimental results demonstrate that OL-KGC significantly outperforms existing
mainstream KGC methods across multiple evaluation metrics, achieving
state-of-the-art performance.

</details>


### [237] [Geometric-Mean Policy Optimization](https://arxiv.org/abs/2507.20673)
*Yuzhong Zhao,Yue Liu,Junpeng Liu,Jingye Chen,Xun Wu,Yaru Hao,Tengchao Lv,Shaohan Huang,Lei Cui,Qixiang Ye,Fang Wan,Furu Wei*

Main category: cs.CL

TL;DR: GMPO通过优化奖励的几何平均值来稳定GRPO的策略更新，并在数学和多模态推理任务中取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在处理具有异常重要性加权奖励的token时，由于采样比率的极端值而导致策略更新不稳定。

Method: 提出了一种名为几何平均策略优化（GMPO）的稳定变体，它通过最大化token级奖励的几何平均值来替代GRPO的算术平均值，从而降低了对异常值的敏感性并维持了更稳定的采样比率范围。

Result: GMPO-7B在多个数学基准测试（包括AIME24、AMC、MATH500、OlympiadBench、Minerva和Geometry3K）上平均性能比GRPO高4.1%，在多模态推理基准测试上平均性能高1.4%。

Conclusion: GMPO通过最大化token级奖励的几何平均值，克服了GRPO在处理具有异常重要性加权奖励的token时出现策略更新不稳定的问题，从而在数学和多模态推理基准测试中均优于GRPO。

Abstract: Recent advancements, such as Group Relative Policy Optimization (GRPO), have
enhanced the reasoning capabilities of large language models by optimizing the
arithmetic mean of token-level rewards. However, GRPO suffers from unstable
policy updates when processing tokens with outlier importance-weighted rewards,
which manifests as extreme importance sampling ratios during training, i.e.,
the ratio between the sampling probabilities assigned to a token by the current
and old policies. In this work, we propose Geometric-Mean Policy Optimization
(GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic
mean, GMPO maximizes the geometric mean of token-level rewards, which is
inherently less sensitive to outliers and maintains a more stable range of
importance sampling ratio. In addition, we provide comprehensive theoretical
and experimental analysis to justify the design and stability benefits of GMPO.
Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on
multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark,
including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is
available at https://github.com/callsys/GMPO.

</details>


### [238] [When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification](https://arxiv.org/abs/2507.20700)
*Hanna Shcharbakova,Tatiana Anikina,Natalia Skachkova,Josef van Genabith*

Main category: cs.CL

TL;DR: 小型语言模型XLM-R在多语言事实核查任务上超越了大型语言模型，提供了新的性能基准，并揭示了大型语言模型在处理证据和数据不平衡方面的潜在问题。


<details>
  <summary>Details</summary>
Motivation: 随着多语言错误信息迅速传播，需要强大的自动化事实核查系统来处理跨语言的细粒度真实性评估。大型语言模型在许多自然语言处理任务中表现出色，但其在多语言声明核查和细微分类方面的能力仍有待研究。

Method: 本研究对X-Fact数据集进行了全面的评估，该数据集涵盖25种语言和7个不同的真实性类别。研究人员比较了小型语言模型（基于编码器的XLM-R和mT5）与最新的解码器模型（Llama 3.1、Qwen 2.5、Mistral Nemo），并采用了提示和微调两种方法。

Result: XLM-R（2.7亿参数）的性能显著优于所有测试过的大型语言模型（7-120亿参数），宏观F1分数达到57.7%，而表现最好的大型语言模型仅为16.9%。这比之前最先进的41.9%的性能提高了15.8%，为多语言事实核查创下了新的性能基准。大型语言模型在利用证据方面存在系统性困难，并且在不平衡的数据设置中明显偏向于频繁出现的类别。

Conclusion: 对于细粒度的多语言事实核查，小型专业模型可能比通用大型模型更有效，这对于事实核查系统的实际部署具有重要意义。

Abstract: The rapid spread of multilingual misinformation requires robust automated
fact verification systems capable of handling fine-grained veracity assessments
across diverse languages. While large language models have shown remarkable
capabilities across many NLP tasks, their effectiveness for multilingual claim
verification with nuanced classification schemes remains understudied. We
conduct a comprehensive evaluation of five state-of-the-art language models on
the X-Fact dataset, which spans 25 languages with seven distinct veracity
categories. Our experiments compare small language models (encoder-based XLM-R
and mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo)
using both prompting and fine-tuning approaches. Surprisingly, we find that
XLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B
parameters), achieving 57.7% macro-F1 compared to the best LLM performance of
16.9%. This represents a 15.8% improvement over the previous state-of-the-art
(41.9%), establishing new performance benchmarks for multilingual fact
verification. Our analysis reveals problematic patterns in LLM behavior,
including systematic difficulties in leveraging evidence and pronounced biases
toward frequent categories in imbalanced data settings. These findings suggest
that for fine-grained multilingual fact verification, smaller specialized
models may be more effective than general-purpose large models, with important
implications for practical deployment of fact-checking systems.

</details>


### [239] [Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models](https://arxiv.org/abs/2507.20704)
*Gabriel Downer,Sean Craven,Damian Ruck,Jake Thomas*

Main category: cs.CL

TL;DR: Text2VLM通过将文本提示转换为包含有害内容的图像，来评估视觉语言模型（VLM）的安全性，发现开源VLM在处理图像时更容易受到攻击，并提出了一种改进评估的工具。


<details>
  <summary>Details</summary>
Motivation: 现有评估数据集主要侧重于文本提示，未能充分评估视觉通道的漏洞。因此，需要一种新的方法来评估VLM在处理结合文本和图像的多模态内容时的鲁棒性，特别是针对提示注入攻击。

Method: Text2VLM流水线通过识别文本中的有害内容并将其转换为排版图像，将文本提示适配成可用于评估VLM抗排版提示注入攻击能力的多模态提示。

Result: 开源VLM在引入视觉输入后，其对提示注入的易感性显著增加，揭示了模型在对齐方面的关键弱点，并且与闭源模型相比存在明显的性能差距。Text2VLM通过人工评估得到了验证，其提取的关键概念、文本摘要和输出分类与人类预期一致。

Conclusion: Text2VLM为评估视觉语言模型（VLM）的鲁棒性提供了一个新的多阶段流水线，通过将文本提示转换为包含有害内容的排版图像，揭示了当前开源VLM在引入视觉输入时更容易受到提示注入攻击的弱点，并与闭源模型相比存在性能差距。通过人工评估验证了Text2VLM的有效性，该工具能够促进更全面的安全评估和更强大的安全机制的开发，最终推动VLM在实际应用中的安全部署。

Abstract: The increasing integration of Visual Language Models (VLMs) into AI systems
necessitates robust model alignment, especially when handling multimodal
content that combines text and images. Existing evaluation datasets heavily
lean towards text-only prompts, leaving visual vulnerabilities under evaluated.
To address this gap, we propose \textbf{Text2VLM}, a novel multi-stage pipeline
that adapts text-only datasets into multimodal formats, specifically designed
to evaluate the resilience of VLMs against typographic prompt injection
attacks. The Text2VLM pipeline identifies harmful content in the original text
and converts it into a typographic image, creating a multimodal prompt for
VLMs. Also, our evaluation of open-source VLMs highlights their increased
susceptibility to prompt injection when visual inputs are introduced, revealing
critical weaknesses in the current models' alignment. This is in addition to a
significant performance gap compared to closed-source frontier models. We
validate Text2VLM through human evaluations, ensuring the alignment of
extracted salient concepts; text summarization and output classification align
with human expectations. Text2VLM provides a scalable tool for comprehensive
safety assessment, contributing to the development of more robust safety
mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities,
Text2VLM plays a role in advancing the safe deployment of VLMs in diverse,
real-world applications.

</details>


### [240] [Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study](https://arxiv.org/abs/2507.20749)
*Yiran Huang,Lukas Thede,Massimiliano Mancini,Wenjia Xu,Zeynep Akata*

Main category: cs.CL

TL;DR: 提出了一种通过结构化剪枝和高效恢复训练来压缩大型多模态语言模型（MLLMs）的新方法。该方法可以在计算资源和数据有限的情况下，有效降低MLLMs的规模，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs存在计算和内存需求大的问题，而从小型语言模型（SLMs）训练MLLMs的方法灵活性有限且计算成本高。因此，需要直接压缩现有MLLMs的有效方法。

Method: 本研究提出直接压缩现有MLLMs，采用结构化剪枝（层wise和widthwise）和高效恢复训练（监督微调和知识蒸馏）相结合的方式。评估了使用少量数据进行恢复训练的可行性。

Result: Widthwise剪枝在低资源场景下表现更好。在恢复训练方面，低压缩率（<20%）下仅微调多模态投影仪即可。监督微调和隐藏状态蒸馏的组合在各种压缩率下具有最佳恢复效果。使用5%的原始数据即可实现超过95%的原始性能。在LLaVA-v1.5-7B和Bunny-v1.0-3B模型上进行了实证研究。

Conclusion: 该研究提出了通过结构化剪枝和高效恢复训练直接压缩大型多模态语言模型（MLLMs）的方法，并提供了在资源受限情况下有效压缩MLLMs的实践指导。

Abstract: While Multimodal Large Language Models (MLLMs) demonstrate impressive
capabilities, their substantial computational and memory requirements pose
significant barriers to practical deployment. Current parameter reduction
techniques primarily involve training MLLMs from Small Language Models (SLMs),
but these methods offer limited flexibility and remain computationally
intensive. To address this gap, we propose to directly compress existing MLLMs
through structural pruning combined with efficient recovery training.
Specifically, we investigate two structural pruning paradigms--layerwise and
widthwise pruning--applied to the language model backbone of MLLMs, alongside
supervised finetuning and knowledge distillation. Additionally, we assess the
feasibility of conducting recovery training with only a small fraction of the
available data. Our results show that widthwise pruning generally maintains
better performance in low-resource scenarios with limited computational
resources or insufficient finetuning data. As for the recovery training,
finetuning only the multimodal projector is sufficient at small compression
levels (< 20%). Furthermore, a combination of supervised finetuning and
hidden-state distillation yields optimal recovery across various pruning
levels. Notably, effective recovery can be achieved with as little as 5% of the
original training data, while retaining over 95% of the original performance.
Through empirical study on two representative MLLMs, i.e., LLaVA-v1.5-7B and
Bunny-v1.0-3B, this study offers actionable insights for practitioners aiming
to compress MLLMs effectively without extensive computation resources or
sufficient data.

</details>


### [241] [Multilingual Self-Taught Faithfulness Evaluators](https://arxiv.org/abs/2507.20752)
*Carlo Alfano,Aymen Al Marjani,Zeno Jonke,Amin Mantrach,Saab Mansour,Marcello Federico*

Main category: cs.CL

TL;DR: 该研究提出了一种新的多语言忠诚度评估框架，使用合成数据和跨语言迁移学习，解决了现有评估方法仅限于英语且需要昂贵标注数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在多语言环境中的应用日益广泛，需要能够处理多语言且无需大量标注数据的忠诚度评估器。

Method: 提出了一种名为“自学多语言忠诚度评估器”的框架，该框架仅使用合成多语言摘要数据进行学习，并利用跨语言迁移学习。

Result: 实验表明，LLM 的通用语言能力与其在特定语言评估任务中的表现之间存在一致的关系。

Conclusion: 该框架在评估任务上优于现有基线，包括最先进的英语评估器和基于机器翻译的方法。

Abstract: The growing use of large language models (LLMs) has increased the need for
automatic evaluation systems, particularly to address the challenge of
information hallucination. Although existing faithfulness evaluation approaches
have shown promise, they are predominantly English-focused and often require
expensive human-labeled training data for fine-tuning specialized models. As
LLMs see increased adoption in multilingual contexts, there is a need for
accurate faithfulness evaluators that can operate across languages without
extensive labeled data. This paper presents Self-Taught Evaluators for
Multilingual Faithfulness, a framework that learns exclusively from synthetic
multilingual summarization data while leveraging cross-lingual transfer
learning. Through experiments comparing language-specific and mixed-language
fine-tuning approaches, we demonstrate a consistent relationship between an
LLM's general language capabilities and its performance in language-specific
evaluation tasks. Our framework shows improvements over existing baselines,
including state-of-the-art English evaluators and machine translation-based
approaches.

</details>


### [242] [On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey](https://arxiv.org/abs/2507.20783)
*Meishan Zhang,Xin Zhang,Xinping Zhao,Shouzheng Huang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 本调查全面概述了预训练语言模型（PLM）时代的通用文本嵌入（GPTE），重点介绍了PLM在其中扮演的关键角色，并探讨了GPTE的架构、高级功能和未来发展方向，为研究人员提供了有价值的参考。


<details>
  <summary>Details</summary>
Motivation: 文本嵌入因其在各种自然语言处理（NLP）任务中的有效性而备受关注。特别是通用文本嵌入（GPTE）利用预训练语言模型（PLM）生成丰富的、可转移的表示，获得了显著的关注度。本调查旨在全面概述GPTE在PLM时代的发展现状和未来潜力。

Method: 该调查全面概述了通用文本嵌入（GPTE）在预训练语言模型（PLM）时代的发展，重点关注PLM在其中扮演的角色。文章首先阐述了GPTE的基础架构和PLM的基本作用（如嵌入提取、表达能力增强等），然后介绍了PLM带来的高级功能（如多语言支持、多模态集成等），最后探讨了未来的研究方向（如排名集成、安全和偏见等）。

Result: 该调查对预训练语言模型（PLM）驱动下的通用文本嵌入（GPTE）进行了全面的概述，涵盖了其基本架构、PLM的作用（包括嵌入提取、表达能力增强、训练策略、学习目标和数据构建）、高级功能（如多语言支持、多模态集成、代码理解和场景适应）以及未来的研究方向（如排名集成、安全、偏见缓解、结构信息整合和认知扩展）。

Conclusion: 该调查全面概述了预训练语言模型（PLM）时代下的通用文本嵌入（GPTE），重点关注PLM在推动其发展中的作用。文章首先审视了GPTE的基础架构，并描述了PLM在嵌入提取、表达能力增强、训练策略、学习目标和数据构建等方面的基本作用。随后，文章介绍了PLM所支持的高级功能，如多语言支持、多模态集成、代码理解和特定场景适应。最后，文章指出了超越传统改进目标的潜在未来研究方向，包括排名集成、安全考量、偏见缓解、结构信息整合以及嵌入的认知扩展。本调查旨在为希望了解GPTE当前状况和未来潜力的研究人员提供有价值的参考。

Abstract: Text embeddings have attracted growing interest due to their effectiveness
across a wide range of natural language processing (NLP) tasks, such as
retrieval, classification, clustering, bitext mining, and summarization. With
the emergence of pretrained language models (PLMs), general-purpose text
embeddings (GPTE) have gained significant traction for their ability to produce
rich, transferable representations. The general architecture of GPTE typically
leverages PLMs to derive dense text representations, which are then optimized
through contrastive learning on large-scale pairwise datasets. In this survey,
we provide a comprehensive overview of GPTE in the era of PLMs, focusing on the
roles PLMs play in driving its development. We first examine the fundamental
architecture and describe the basic roles of PLMs in GPTE, i.e., embedding
extraction, expressivity enhancement, training strategies, learning objectives,
and data construction. Then, we describe advanced roles enabled by PLMs, such
as multilingual support, multimodal integration, code understanding, and
scenario-specific adaptation. Finally, we highlight potential future research
directions that move beyond traditional improvement goals, including ranking
integration, safety considerations, bias mitigation, structural information
incorporation, and the cognitive extension of embeddings. This survey aims to
serve as a valuable reference for both newcomers and established researchers
seeking to understand the current state and future potential of GPTE.

</details>


### [243] [Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models](https://arxiv.org/abs/2507.20786)
*Sam Osian,Arpan Dutta,Sahil Bhandari,Iain E. Buchan,Dan W. Joyce*

Main category: cs.CL

TL;DR: 本研究展示了一个名为PFD Toolkit的自动化语言模型管道，能够快速、准确地识别和分析儿童自杀相关的死亡报告，其效率和准确性远超人工方法，为公共卫生决策提供了有力支持。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在评估一个完全自动化的开源“文本到表格”语言模型管道（PFD Toolkit）是否能够复制ONS对儿童自杀PFD报告的识别和主题分析，并评估效率和可靠性方面的收益。ONS之前的分析耗时数月，且完全依赖人工整理和编码。

Method: 开发了一个名为PFD Toolkit的完全自动化的开源“文本到表格”语言模型管道，以处理英格兰和威尔士的验尸官预防未来死亡（PFD）报告。该工具包使用大型语言模型管道来自动筛选和编码涉及18岁及以下儿童自杀的报告，复制了国家统计局（ONS）的编码框架，用于受助者类别和23个关注的子主题。

Result: PFD Toolkit处理了4,249份PFD报告，识别出72份儿童自杀PFD报告，几乎是ONS数量的两倍。在对144份报告进行的抽样验证中，基于LLM的工作流程与共识的临床注释显示出高度一致性（Cohen's κ = 0.82）。整个脚本的运行时间为8分16秒，将数月的工作缩短到几分钟。

Conclusion: 该研究表明，自动化的语言模型分析能够可靠且高效地复制验尸官数据的が人工审查，从而为公共卫生和安全提供可扩展、可复现且及时的见解。PFD Toolkit已公开提供，可用于未来的研究。

Abstract: Prevention of Future Deaths (PFD) reports, issued by coroners in England and
Wales, flag systemic hazards that may lead to further loss of life. Analysis of
these reports has previously been constrained by the manual effort required to
identify and code relevant cases. In 2025, the Office for National Statistics
(ONS) published a national thematic review of child-suicide PFD reports ($\leq$
18 years), identifying 37 cases from January 2015 to November 2023 - a process
based entirely on manual curation and coding. We evaluated whether a fully
automated, open source "text-to-table" language-model pipeline (PFD Toolkit)
could reproduce the ONS's identification and thematic analysis of child-suicide
PFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD
reports published from July 2013 to November 2023 were processed via PFD
Toolkit's large language model pipelines. Automated screening identified cases
where the coroner attributed death to suicide in individuals aged 18 or
younger, and eligible reports were coded for recipient category and 23 concern
sub-themes, replicating the ONS coding frame. PFD Toolkit identified 72
child-suicide PFD reports - almost twice the ONS count. Three blinded
clinicians adjudicated a stratified sample of 144 reports to validate the
child-suicide screening. Against the post-consensus clinical annotations, the
LLM-based workflow showed substantial to almost-perfect agreement (Cohen's
$\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script
runtime was 8m 16s, transforming a process that previously took months into one
that can be completed in minutes. This demonstrates that automated LLM analysis
can reliably and efficiently replicate manual thematic reviews of coronial
data, enabling scalable, reproducible, and timely insights for public health
and safety. The PFD Toolkit is openly available for future research.

</details>


### [244] [Latent Inter-User Difference Modeling for LLM Personalization](https://arxiv.org/abs/2507.20849)
*Yilun Qiu,Tianhao Shi,Xiaoyan Zhao,Fengbin Zhu,Yang Zhang,Fuli Feng*

Main category: cs.CL

TL;DR: DEP personalizes LLMs by modeling user differences in latent space using soft prompts and a sparse autoencoder, outperforming baselines in review generation.


<details>
  <summary>Details</summary>
Motivation: Existing personalization methods for LLMs overlook crucial inter-user differences or struggle to extract them effectively using language-based prompts. This paper addresses these limitations.

Method: DEP framework models inter-user differences in the latent space by constructing soft prompts that contrast a user's embedding with those of peers who engaged with similar content. A sparse autoencoder filters and compresses these embeddings before injecting them into a frozen LLM.

Result: Experiments demonstrate that DEP effectively models inter-user differences and improves personalized review generation compared to baseline methods.

Conclusion: DEP consistently outperforms baseline methods across multiple metrics in personalized review generation.

Abstract: Large language models (LLMs) are increasingly integrated into users' daily
lives, leading to a growing demand for personalized outputs. Previous work
focuses on leveraging a user's own history, overlooking inter-user differences
that are crucial for effective personalization. While recent work has attempted
to model such differences, the reliance on language-based prompts often hampers
the effective extraction of meaningful distinctions. To address these issues,
we propose Difference-aware Embedding-based Personalization (DEP), a framework
that models inter-user differences in the latent space instead of relying on
language prompts. DEP constructs soft prompts by contrasting a user's embedding
with those of peers who engaged with similar content, highlighting relative
behavioral signals. A sparse autoencoder then filters and compresses both
user-specific and difference-aware embeddings, preserving only task-relevant
features before injecting them into a frozen LLM. Experiments on personalized
review generation show that DEP consistently outperforms baseline methods
across multiple metrics. Our code is available at
https://github.com/SnowCharmQ/DEP.

</details>


### [245] [A survey of diversity quantification in natural language processing: The why, what, where and how](https://arxiv.org/abs/2507.20858)
*Louis Estève,Marie-Catherine de Marneffe,Nurit Melnik,Agata Savary,Olha Kanishcheva*

Main category: cs.CL

TL;DR: NLP领域对多样性的研究日益增多，但方法不统一。本研究提出了一个基于生态学和经济学的统一分类法（包含多样性、平衡性和差异性三个维度），以系统化地分析NLP中的多样性度量，旨在促进理解和方法间的比较。


<details>
  <summary>Details</summary>
Motivation: 近年来，多样性概念在自然语言处理（NLP）领域受到越来越多的关注，其动机包括促进包容性、模拟人类语言行为以及提升系统性能。然而，NLP中对多样性的处理往往是临时的，并且与其他理论更完善的领域联系较少。

Method: 对过去6年ACL Anthology中标题包含“diversity”或“diverse”的文章进行了调查，并将多样性度量归纳到来自生态学和经济学的统一框架（Stirling, 2007）中，该框架包含三个维度：多样性（variety）、平衡性（balance）和差异性（disparity）。

Result: 调查发现，NLP中存在广泛的、通常高度专业化且术语不一致的多样性量化方法。通过统一的框架，研究揭示了系统化方法所带来的趋势。

Conclusion: 本研究提出了一个统一的NLP多样性度量分类法，旨在促进对多样性概念的更好形式化，从而增进对其的理解和不同方法间的可比性。

Abstract: The concept of diversity has received increased consideration in Natural
Language Processing (NLP) in recent years. This is due to various motivations
like promoting and inclusion, approximating human linguistic behavior, and
increasing systems' performance. Diversity has however often been addressed in
an ad hoc manner in NLP, and with few explicit links to other domains where
this notion is better theorized. We survey articles in the ACL Anthology from
the past 6 years, with "diversity" or "diverse" in their title. We find a wide
range of settings in which diversity is quantified, often highly specialized
and using inconsistent terminology. We put forward a unified taxonomy of why,
what on, where, and how diversity is measured in NLP. Diversity measures are
cast upon a unified framework from ecology and economy (Stirling, 2007) with 3
dimensions of diversity: variety, balance and disparity. We discuss the trends
which emerge due to this systematized approach. We believe that this study
paves the way towards a better formalization of diversity in NLP, which should
bring a better understanding of this notion and a better comparability between
various approaches.

</details>


### [246] [Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings](https://arxiv.org/abs/2507.20859)
*Luc Builtjes,Joeran Bosma,Mathias Prokop,Bram van Ginneken,Alessa Hering*

Main category: cs.CL

TL;DR: 开源大语言模型在荷兰语临床信息提取任务上表现良好，特别是在使用 llm_extractinator 框架和零样本设置时。


<details>
  <summary>Details</summary>
Motivation: 由于专有大语言模型缺乏透明度和数据隐私问题限制了其在医疗保健领域的应用，因此评估开源大语言模型在临床信息提取方面的能力。

Method: 使用 llm_extractinator 框架，在零样本设置下评估了九个开源生成大语言模型在 DRAGON 基准上的性能，该基准包含 28 项荷兰语临床信息提取任务。

Result: Phi-4-14B、Qwen-2.5-14B 和 DeepSeek-R1-14B 等 140 亿参数模型取得了有竞争力的结果，而更大的 Llama-3.3-70B 模型在更高的计算成本下取得了稍高的性能。预先翻译成英文会持续降低性能，这凸显了原生语言处理的必要性。

Conclusion: 本研究表明，在我们的框架下，开源大语言模型为资源匮乏的临床信息提取提供了有效、可扩展且注重隐私的解决方案。

Abstract: Medical reports contain rich clinical information but are often unstructured
and written in domain-specific language, posing challenges for information
extraction. While proprietary large language models (LLMs) have shown promise
in clinical natural language processing, their lack of transparency and data
privacy concerns limit their utility in healthcare. This study therefore
evaluates nine open-source generative LLMs on the DRAGON benchmark, which
includes 28 clinical information extraction tasks in Dutch. We developed
\texttt{llm\_extractinator}, a publicly available framework for information
extraction using open-source generative LLMs, and used it to assess model
performance in a zero-shot setting. Several 14 billion parameter models,
Phi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results,
while the bigger Llama-3.3-70B model achieved slightly higher performance at
greater computational cost. Translation to English prior to inference
consistently degraded performance, highlighting the need of native-language
processing. These findings demonstrate that open-source LLMs, when used with
our framework, offer effective, scalable, and privacy-conscious solutions for
clinical information extraction in low-resource settings.

</details>


### [247] [Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context Learning](https://arxiv.org/abs/2507.20906)
*Jungwon Park,Wonjong Rhee*

Main category: cs.CL

TL;DR: 通过软注入任务嵌入，而非多示例提示，LLM在无参数更新的情况下提高了任务性能，并降低了推理成本。该方法在57个任务和12个LLM上表现优于ICL，并能分析注意力头的功能。


<details>
  <summary>Details</summary>
Motivation: 在ICL（In-Context Learning）中，仅使用多示例提示来传达任务信息是否最有效和最经济尚不清楚。

Method: 提出了一种软注入任务嵌入的方法，仅使用少量示例ICL提示构建一次任务嵌入，并在推理过程中重复使用。通过软混合任务嵌入和注意力头激活，并使用预优化的混合参数（软头选择参数）来实现。

Result: 该方法在57个任务和12个LLM（涵盖4个模型系列，规模从4B到70B）上进行了广泛评估。结果显示，该方法在57个任务上的平均表现比10-shot ICL高出10.1%-13.9%。此外，该方法还能揭示注意力头的任务相关性，且任务相关头的位置在相似任务之间具有迁移性，但在不相似任务之间则不具备，这表明了头功能的任务特定性。

Conclusion: 该方法提出了一种将任务条件从提示空间转移到激活空间的新范例，以减少提示长度并提高任务性能。

Abstract: In-Context Learning (ICL) enables Large Language Models (LLMs) to perform
tasks by conditioning on input-output examples in the prompt, without requiring
any update in model parameters. While widely adopted, it remains unclear
whether prompting with multiple examples is the most effective and efficient
way to convey task information. In this work, we propose Soft Injection of task
embeddings. The task embeddings are constructed only once using few-shot ICL
prompts and repeatedly used during inference. Soft injection is performed by
softly mixing task embeddings with attention head activations using
pre-optimized mixing parameters, referred to as soft head-selection parameters.
This method not only allows a desired task to be performed without in-prompt
demonstrations but also significantly outperforms existing ICL approaches while
reducing memory usage and compute cost at inference time. An extensive
evaluation is performed across 57 tasks and 12 LLMs, spanning four model
families of sizes from 4B to 70B. Averaged across 57 tasks, our method
outperforms 10-shot ICL by 10.1%-13.9% across 12 LLMs. Additional analyses show
that our method also serves as an insightful tool for analyzing task-relevant
roles of attention heads, revealing that task-relevant head positions selected
by our method transfer across similar tasks but not across dissimilar ones --
underscoring the task-specific nature of head functionality. Our soft injection
method opens a new paradigm for reducing prompt length and improving task
performance by shifting task conditioning from the prompt space to the
activation space.

</details>


### [248] [MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation](https://arxiv.org/abs/2507.20917)
*Adrien Bazoge*

Main category: cs.CL

TL;DR: MediQAl 是一个法语医学问答数据集，包含 32,603 个问题，分为三种类型，并标记为理解或推理。通过评估 14 个大型语言模型，发现模型在事实回忆和推理任务之间存在性能差距。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在法语医学问答方面的能力，填补了医学领域多语言资源的空白。

Method: MediQAl 数据集包含 32,603 个法语医学考试问题，涵盖 41 个医学科目，并设有三种任务：单选唯一答案、多选多答案和开放式简答题。每个问题都标记为理解或推理。通过对 14 个大型语言模型进行评估来验证该数据集。

Result: 对 14 个大型语言模型进行了广泛评估，观察到模型在事实回忆和推理任务之间存在显著的性能差距，为评估模型在法语医学问答方面的表现提供了基准。

Conclusion: MediQAl 是一个法语医学问答数据集，旨在评估语言模型在真实临床场景中的事实医学回忆和推理能力。研究观察到语言模型在事实回忆和推理任务之间存在显著的性能差距。

Abstract: This work introduces MediQAl, a French medical question answering dataset
designed to evaluate the capabilities of language models in factual medical
recall and reasoning over real-world clinical scenarios. MediQAl contains
32,603 questions sourced from French medical examinations across 41 medical
subjects. The dataset includes three tasks: (i) Multiple-Choice Question with
Unique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii)
Open-Ended Question with Short-Answer. Each question is labeled as
Understanding or Reasoning, enabling a detailed analysis of models' cognitive
capabilities. We validate the MediQAl dataset through extensive evaluation with
14 large language models, including recent reasoning-augmented models, and
observe a significant performance gap between factual recall and reasoning
tasks. Our evaluation provides a comprehensive benchmark for assessing language
models' performance on French medical question answering, addressing a crucial
gap in multilingual resources for the medical domain.

</details>


### [249] [FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models](https://arxiv.org/abs/2507.20930)
*Likun Tan,Kuan-Wei Huang,Kevin Wu*

Main category: cs.CL

TL;DR: 研究提出一种方法，通过在金融问答语料库中插入错误并微调模型，有效检测和编辑大语言模型生成内容中的事实不准确之处，微调后的Phi-4模型性能优于OpenAI-o3。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在金融等高风险领域产生幻觉（生成不准确信息）是一个严峻的挑战，需要有效的方法来检测和纠正这些事实错误，以确保金融应用的可靠性。

Method: 利用领域特定的错误分类法，构建包含已标记错误的合成数据集，并在此数据集上对Phi-4、Phi-4-mini、Qwen3-4B和Qwen3-14B四种语言模型进行微调，以实现对事实不准确内容的检测和编辑。

Result: 微调后的Phi-4模型在二元F1分数上提高了8%，在整体检测性能上比OpenAI-o3提高了30%。微调后的Phi-4-mini模型（40亿参数）在二元检测上仅下降2%，在整体检测上仅下降0.1%，表现出与OpenAI-o3相当的竞争力。

Conclusion: 该研究提出了一种检测和编辑金融领域大语言模型生成回应中事实不准确内容的方法，并取得了显著成效，为提升大语言模型的可靠性和可信度提供了实际解决方案。通过构建合成数据集并微调模型，研究证明了所提出方法在金融文本生成中的有效性，并为其他领域的应用提供了可推广的框架。

Abstract: Hallucinations in large language models pose a critical challenge for
applications requiring factual reliability, particularly in high-stakes domains
such as finance. This work presents an effective approach for detecting and
editing factually incorrect content in model-generated responses based on the
provided context. Given a user-defined domain-specific error taxonomy, we
construct a synthetic dataset by inserting tagged errors into financial
question-answering corpora and then fine-tune four language models, Phi-4,
Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual
inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8%
improvement in binary F1 score and a 30% gain in overall detection performance
compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having
only 4 billion parameters, maintains competitive performance with just a 2%
drop in binary detection and a 0.1% decline in overall detection compared to
OpenAI-o3. Our work provides a practical solution for detecting and editing
factual inconsistencies in financial text generation while introducing a
generalizable framework that can enhance the trustworthiness and alignment of
large language models across diverse applications beyond finance. Our code and
data are available at https://github.com/pegasi-ai/fine-grained-editting.

</details>


### [250] [Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models](https://arxiv.org/abs/2507.20956)
*Max Peeperkorn,Tom Kouwenhoven,Dan Brown,Anna Jordanous*

Main category: cs.CL

TL;DR: 指令调整降低了 LLMs 的输出多样性。本研究提出了“conformative decoding”策略，通过结合基座模型的输出来恢复多样性，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决指令调整（instruction-tuning）导致的大语言模型（LLMs）输出多样性降低的问题，特别是在创意性写作任务中。

Method: 通过对比评估和分析 OLMo 和 OLMo 2 模型在不同微调阶段的多样性变化，提出并验证了一种新的解码策略“conformative decoding”。

Result: 研究表明，指令调整显著降低了 LLMs 的输出多样性，其中 DPO 策略影响最大。所提出的“conformative decoding”策略能够有效增加多样性，并保持或提高输出质量。

Conclusion: 指令调整通过 DPO 策略对模型输出的多样性产生了最大的负面影响，提出的“conformative decoding”策略可以有效恢复 LLMs 的输出多样性，并保持甚至提高其输出质量。

Abstract: Instruction-tuning large language models (LLMs) reduces the diversity of
their outputs, which has implications for many tasks, particularly for creative
tasks. This paper investigates the ``diversity gap'' for a writing prompt
narrative generation task. This gap emerges as measured by current diversity
metrics for various open-weight and open-source LLMs. The results show
significant decreases in diversity due to instruction-tuning. We explore the
diversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to
further understand how output diversity is affected. The results indicate that
DPO has the most substantial impact on diversity. Motivated by these findings,
we present a new decoding strategy, conformative decoding, which guides an
instruct model using its more diverse base model to reintroduce output
diversity. We show that conformative decoding typically increases diversity and
even maintains or improves quality.

</details>


### [251] [Memorization in Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.21009)
*Danil Savine,Muni Sreenivas Pydi,Jamal Atif,Olivier Cappé*

Main category: cs.CL

TL;DR: 在医学领域微调LLM时，Value和Output矩阵比Query和Key矩阵更容易导致模型记忆。较低的困惑度和较高的LoRA秩会增加模型记忆，但后者存在边际效益递减。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究微调大型语言模型（LLMs）中影响记忆的机制和因素，特别关注因隐私敏感性而选择的医学领域。

Method: 本研究采用成员身份推理攻击来检测被记忆的数据，并通过带有提示前缀的生成任务来评估逐字复制情况。具体分析了转换器架构中不同权重矩阵的适应性、困惑度与记忆性之间的关系，以及低秩适配（LoRA）微调中秩增加的影响。

Result: 研究发现：1. 值（Value）和输出（Output）矩阵相比于查询（Query）和键（Key）矩阵对记忆的贡献更大；2. 微调模型中较低的困惑度与较高的记忆性相关；3. 较高的LoRA秩会导致较高的记忆性，但在更高秩时回报递减。

Conclusion: 该研究揭示了在医学领域微调大型语言模型时，模型性能与数据隐私之间的权衡。研究结果为制定更有效的模型适应策略和管理数据隐私提供了参考。

Abstract: This study investigates the mechanisms and factors influencing memorization
in fine-tuned large language models (LLMs), with a focus on the medical domain
due to its privacy-sensitive nature. We examine how different aspects of the
fine-tuning process affect a model's propensity to memorize training data,
using the PHEE dataset of pharmacovigilance events.
  Our research employs two main approaches: a membership inference attack to
detect memorized data, and a generation task with prompted prefixes to assess
verbatim reproduction. We analyze the impact of adapting different weight
matrices in the transformer architecture, the relationship between perplexity
and memorization, and the effect of increasing the rank in low-rank adaptation
(LoRA) fine-tuning.
  Key findings include: (1) Value and Output matrices contribute more
significantly to memorization compared to Query and Key matrices; (2) Lower
perplexity in the fine-tuned model correlates with increased memorization; (3)
Higher LoRA ranks lead to increased memorization, but with diminishing returns
at higher ranks.
  These results provide insights into the trade-offs between model performance
and privacy risks in fine-tuned LLMs. Our findings have implications for
developing more effective and responsible strategies for adapting large
language models while managing data privacy concerns.

</details>


### [252] [Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation](https://arxiv.org/abs/2507.21028)
*Jiaju Chen,Yuxuan Lu,Xiaojie Wang,Huimin Zeng,Jing Huang,Jiri Gesi,Ying Xu,Bingsheng Yao,Dakuo Wang*

Main category: cs.CL

TL;DR: MAJ-EVAL 是一个多智能体评估框架，可以根据相关文档自动创建不同的 LLM 评估者角色，并通过让这些智能体进行辩论来提供多维度的反馈。该框架在教育和医疗领域都显示出比现有方法更符合人类专家评分的评估结果。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM-as-a-judge 方法面临两个限制：代理的身份描述通常是随意设计的，并且框架不能推广到其他任务。

Method: 提出 MAJ-EVAL，一个多智能体作为裁判的评估框架，可以自动构建具有不同维度（来自相关文本文件，如研究论文）的多个评估者角色，并使用这些角色来实例化 LLM 智能体，然后让多智能体进行组内辩论，以生成多维度的反馈。

Result: 评估实验在教育和医疗领域都表明，MAJ-EVAL 可以生成与人类专家的评分更一致的评估结果，优于传统的自动评估指标和现有的 LLM-as-a-judge 方法。

Conclusion: MAJ-EVAL 可以生成与人类专家的评分更一致的评估结果，优于传统的自动评估指标和现有的 LLM-as-a-judge 方法。

Abstract: Nearly all human work is collaborative; thus, the evaluation of real-world
NLP applications often requires multiple dimensions that align with diverse
human perspectives. As real human evaluator resources are often scarce and
costly, the emerging "LLM-as-a-judge" paradigm sheds light on a promising
approach to leverage LLM agents to believably simulate human evaluators. Yet,
to date, existing LLM-as-a-judge approaches face two limitations: persona
descriptions of agents are often arbitrarily designed, and the frameworks are
not generalizable to other tasks. To address these challenges, we propose
MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically
construct multiple evaluator personas with distinct dimensions from relevant
text documents (e.g., research papers), instantiate LLM agents with the
personas, and engage in-group debates with multi-agents to Generate
multi-dimensional feedback. Our evaluation experiments in both the educational
and medical domains demonstrate that MAJ-EVAL can generate evaluation results
that better align with human experts' ratings compared with conventional
automated evaluation metrics and existing LLM-as-a-judge methods.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [253] [AR-LIF: Adaptive reset leaky-integrate and fire neuron for spiking neural networks](https://arxiv.org/abs/2507.20746)
*Zeyu Huang,Wei Meng,Quan Liu,Kun Chen,Li Ma*

Main category: cs.NE

TL;DR: Spiking neural networks use event-driven dynamics for low energy consumption. Existing reset methods have drawbacks like information loss or uniform treatment. This paper introduces an adaptive reset neuron with threshold adjustment, achieving strong performance and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing hard reset methods cause information loss, and improved soft reset methods treat neurons uniformly, prompting the need for a new approach.

Method: This paper designs an adaptive reset neuron that establishes the correlation between input, output, and reset, and integrates a simple yet effective threshold adjustment strategy.

Result: The adaptive reset neuron demonstrates excellent performance on various datasets while maintaining low energy consumption.

Conclusion: The proposed adaptive reset neuron achieves excellent performance on various datasets while maintaining low energy consumption, outperforming existing soft reset methods.

Abstract: Spiking neural networks possess the advantage of low energy consumption due
to their event-driven nature. Compared with binary spike outputs, their
inherent floating-point dynamics are more worthy of attention. The threshold
level and reset mode of neurons play a crucial role in determining the number
and timing of spikes. The existing hard reset method causes information loss,
while the improved soft reset method adopts a uniform treatment for neurons. In
response to this, this paper designs an adaptive reset neuron, establishing the
correlation between input, output and reset, and integrating a simple yet
effective threshold adjustment strategy. It achieves excellent performance on
various datasets while maintaining the advantage of low energy consumption.

</details>


### [254] [Why Flow Matching is Particle Swarm Optimization?](https://arxiv.org/abs/2507.20810)
*Kaichen Ouyang*

Main category: cs.NE

TL;DR: 流匹配是PSO的连续形式，PSO是流匹配的离散形式。该研究揭示了它们之间的对偶性，为融合两者提供了理论基础，并指出了改进算法和模型的新方向。


<details>
  <summary>Details</summary>
Motivation: 初步探索生成模型中的流匹配与进化计算中的粒子群优化之间的对偶性，以期建立统一的分析框架并启发新的研究方向。

Method: 通过理论分析揭示了流匹配的向量场学习与PSO的速度更新规则在数学表达上的内在联系，并指出两者都遵循从初始分布到目标分布的渐进演化框架，且均可表述为常微分方程描述的动力系统。

Result: 研究表明流匹配可视为PSO的连续泛化，而PSO是群体智能原则的离散实现。这种对偶性理解为改进算法和增强模型提供了理论支持。

Conclusion: 该研究初步揭示了生成模型中的流匹配（flow matching）与进化计算中的粒子群优化（PSO）之间的对偶性，为开发混合算法和统一分析框架奠定了理论基础。

Abstract: This paper preliminarily investigates the duality between flow matching in
generative models and particle swarm optimization (PSO) in evolutionary
computation. Through theoretical analysis, we reveal the intrinsic connections
between these two approaches in terms of their mathematical formulations and
optimization mechanisms: the vector field learning in flow matching shares
similar mathematical expressions with the velocity update rules in PSO; both
methods follow the fundamental framework of progressive evolution from initial
to target distributions; and both can be formulated as dynamical systems
governed by ordinary differential equations. Our study demonstrates that flow
matching can be viewed as a continuous generalization of PSO, while PSO
provides a discrete implementation of swarm intelligence principles. This
duality understanding establishes a theoretical foundation for developing novel
hybrid algorithms and creates a unified framework for analyzing both methods.
Although this paper only presents preliminary discussions, the revealed
correspondences suggest several promising research directions, including
improving swarm intelligence algorithms based on flow matching principles and
enhancing generative models using swarm intelligence concepts.

</details>


### [255] [Pareto-Grid-Guided Large Language Models for Fast and High-Quality Heuristics Design in Multi-Objective Combinatorial Optimization](https://arxiv.org/abs/2507.20923)
*Minh Hieu Ha,Hung Phan,Tung Duy Doan,Tung Dao,Dao Tran,Huynh Thi Thanh Binh*

Main category: cs.NE

TL;DR: MPaGE 是一种新的基于 LLM 的方法，通过利用帕累托网格和 LLM 来增强进化算法，以解决多目标组合优化问题。它通过优先生成多样化的启发式方法来提高性能和效率，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的进化算法在解决多目标组合优化问题 (MOCOP) 时，虽然有效，但通常需要领域知识和反复的参数调整，这在应用于未知的 MOCOP 实例时灵活性有限。现有基于 LLM 的方法主要关注单目标任务，忽略了多目标设置中的运行效率和启发式多样性等关键问题。

Method: 该研究引入了一种名为 MPaGE (Multi-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs) 的新方法，该方法通过利用大型语言模型 (LLM) 和帕累托前沿网格 (PFG) 技术来增强现有的 SEMO 框架。MPaGE 将目标空间划分为网格，并保留表现最佳的候选者来指导启发式生成。它利用 LLM 优先生成具有语义上不同的逻辑结构启发式，从而促进多样性并减少种群中的冗余。

Result: 通过广泛的评估，MPaGE 证明了其在多目标组合优化任务上优于现有的基于 LLM 的框架，并且与传统的 MOEA 相比，在运行时效率上具有显著优势，同时取得了有竞争力的结果。

Conclusion: MPaGE 在多目标组合优化问题上表现出色，优于现有的基于 LLM 的框架，并且在运行时效率和启发式多样性方面具有明显优势，同时与传统的多目标进化算法相比也具有竞争力。

Abstract: Multi-objective combinatorial optimization problems (MOCOP) frequently arise
in practical applications that require the simultaneous optimization of
conflicting objectives. Although traditional evolutionary algorithms can be
effective, they typically depend on domain knowledge and repeated parameter
tuning, limiting flexibility when applied to unseen MOCOP instances. Recently,
integration of Large Language Models (LLMs) into evolutionary computation has
opened new avenues for automatic heuristic generation, using their advanced
language understanding and code synthesis capabilities. Nevertheless, most
existing approaches predominantly focus on single-objective tasks, often
neglecting key considerations such as runtime efficiency and heuristic
diversity in multi-objective settings. To bridge this gap, we introduce
Multi-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a
novel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO)
framework that leverages LLMs and Pareto Front Grid (PFG) technique. By
partitioning the objective space into grids and retaining top-performing
candidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize
heuristics with semantically distinct logical structures during variation, thus
promoting diversity and mitigating redundancy within the population. Through
extensive evaluations, MPaGE demonstrates superior performance over existing
LLM-based frameworks, and achieves competitive results to traditional
Multi-objective evolutionary algorithms (MOEAs), with significantly faster
runtime. Our code is available at: https://github.com/langkhachhoha/MPaGE.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [256] [Extending Group Relative Policy Optimization to Continuous Control: A Theoretical Framework for Robotic Reinforcement Learning](https://arxiv.org/abs/2507.19555)
*Rajat Khanda,Mohammad Baqar,Sambuddha Chakrabarti,Satyasaran Changdar*

Main category: cs.RO

TL;DR: 本研究将GRPO扩展到连续控制，以解决机器人技术中的挑战。


<details>
  <summary>Details</summary>
Motivation: GRPO在离散动作空间中通过基于群体的优势估计消除了对价值函数的需求，但在连续控制领域尚未得到探索，这限制了其在机器人技术中的应用。

Method: 本研究通过引入基于轨迹的策略聚类、状态感知优势估计和针对机器人应用的正则化策略更新，将GRPO扩展到连续控制。

Result: 本研究为将GRPO应用于机器人领域的受试验证奠定了基础，包括运动和操纵任务，并对收敛性和计算复杂度进行了理论分析。

Conclusion: 本研究提出了将群体相对策略优化（GRPO）扩展到连续控制环境的理论框架，以解决高维动作空间、稀疏奖励和时间动态性等挑战。

Abstract: Group Relative Policy Optimization (GRPO) has shown promise in discrete
action spaces by eliminating value function dependencies through group-based
advantage estimation. However, its application to continuous control remains
unexplored, limiting its utility in robotics where continuous actions are
essential. This paper presents a theoretical framework extending GRPO to
continuous control environments, addressing challenges in high-dimensional
action spaces, sparse rewards, and temporal dynamics. Our approach introduces
trajectory-based policy clustering, state-aware advantage estimation, and
regularized policy updates designed for robotic applications. We provide
theoretical analysis of convergence properties and computational complexity,
establishing a foundation for future empirical validation in robotic systems
including locomotion and manipulation tasks.

</details>


### [257] [Reward-Augmented Reinforcement Learning for Continuous Control in Precision Autonomous Parking via Policy Optimization Methods](https://arxiv.org/abs/2507.19642)
*Ahmad Suleman,Misha Urooj Khan,Zeeshan Kaleem,Ali H. Alenezi,Iqra Shabbir Sinem Coleri,Chau Yuen*

Main category: cs.RO

TL;DR: 该研究提出了RARLAP框架，通过奖励增强解决了自动泊车的复杂性问题，实现了高效的策略优化和更高的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于规则和模型预测的方法在处理自动泊车（AP）的非线性和环境依赖性复杂性方面，缺乏足够的适应性和泛化能力。

Method: 提出了一种名为RARLAP的奖励增强学习框架，该框架利用结构化奖励设计（包括仅目标奖励GOR、密集邻近奖励DPR和里程碑增强奖励MAR）来解决自动泊车的连续域控制复杂性问题。同时，集成了on-policy和off-policy优化范式，并在高保真的Unity 3D模拟环境中进行了训练和评估。

Result: 经验评估表明，采用on-policy MAR的RARLAP框架实现了91%的成功率，产生了更平滑的轨迹和更鲁棒的行为。该框架提高了策略适应性，加速了训练，并增强了连续控制的安全性。

Conclusion: 该研究提出了一种名为RARLAP的奖励增强学习框架，用于解决自动泊车（AP）中的连续域控制复杂性问题。通过结构化奖励设计（包括仅目标奖励GOR、密集邻近奖励DPR和里程碑增强奖励MAR），并结合on-policy和off-policy优化方法，RARLAP能够有效地提升策略的适应性、加速训练并提高安全性。实验结果表明，on-policy MAR方法取得了91%的成功率，并展现出更平滑的轨迹和更鲁棒的行为，而GOR和DPR方法未能有效指导学习。该框架证明了奖励增强在应对复杂自动泊车挑战中的有效性，并支持可扩展和高效的策略优化。

Abstract: Autonomous parking (AP) represents a critical yet complex subset of
intelligent vehicle automation, characterized by tight spatial constraints,
frequent close-range obstacle interactions, and stringent safety margins.
However, conventional rule-based and model-predictive methods often lack the
adaptability and generalization needed to handle the nonlinear and
environment-dependent complexities of AP. To address these limitations, we
propose a reward-augmented learning framework for AP (RARLAP), that mitigates
the inherent complexities of continuous-domain control by leveraging structured
reward design to induce smooth and adaptable policy behavior, trained entirely
within a high-fidelity Unity-based custom 3D simulation environment. We
systematically design and assess three structured reward strategies: goal-only
reward (GOR), dense proximity reward (DPR), and milestone-augmented reward
(MAR), each integrated with both on-policy and off-policy optimization
paradigms. Empirical evaluations demonstrate that the on-policy MAR achieves a
91\% success rate, yielding smoother trajectories and more robust behavior,
while GOR and DPR fail to guide effective learning. Convergence and trajectory
analyses demonstrate that the proposed framework enhances policy adaptability,
accelerates training, and improves safety in continuous control. Overall,
RARLAP establishes that reward augmentation effectively addresses complex
autonomous parking challenges, enabling scalable and efficient policy
optimization with both on- and off-policy methods. To support reproducibility,
the code accompanying this paper is publicly available.

</details>


### [258] [GABRIL: Gaze-Based Regularization for Mitigating Causal Confusion in Imitation Learning](https://arxiv.org/abs/2507.19647)
*Amin Banayeeanzade,Fatemeh Bahrani,Yutai Zhou,Erdem Bıyık*

Main category: cs.RO

TL;DR: GABRIL利用人类注视数据解决模仿学习中的因果混淆问题，在雅达利和CARLA中大幅提升性能并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 模仿学习（IL）在从人类专家演示中学习时，常常会受到因果混淆问题的影响，导致代理误将虚假关联解释为因果关系，从而在分布偏移的测试环境中表现不佳。

Method: 提出了一种名为GABRIL（模仿学习中的注视引导正则化）的新方法，该方法利用收集到的专家注视数据来指导模仿学习中的表示学习。通过引入正则化损失，促使模型关注由专家注视确定的因果相关特征，从而减轻混淆变量的影响。

Result: 在雅达利环境和CARLA中的Bench2Drive基准测试中，GABRIL相比行为克隆（behavior cloning）的改进率分别达到了179%和76%，显著优于其他基线方法。此外，该方法还提供了比常规IL代理更好的可解释性。

Conclusion: GABRIL通过利用人类注视数据来指导模仿学习中的表示学习，解决了因果混淆问题，并在雅达利和CARLA环境中取得了显著的性能提升，同时还提供了更好的可解释性。

Abstract: Imitation Learning (IL) is a widely adopted approach which enables agents to
learn from human expert demonstrations by framing the task as a supervised
learning problem. However, IL often suffers from causal confusion, where agents
misinterpret spurious correlations as causal relationships, leading to poor
performance in testing environments with distribution shift. To address this
issue, we introduce GAze-Based Regularization in Imitation Learning (GABRIL), a
novel method that leverages the human gaze data gathered during the data
collection phase to guide the representation learning in IL. GABRIL utilizes a
regularization loss which encourages the model to focus on causally relevant
features identified through expert gaze and consequently mitigates the effects
of confounding variables. We validate our approach in Atari environments and
the Bench2Drive benchmark in CARLA by collecting human gaze datasets and
applying our method in both domains. Experimental results show that the
improvement of GABRIL over behavior cloning is around 179% more than the same
number for other baselines in the Atari and 76% in the CARLA setup. Finally, we
show that our method provides extra explainability when compared to regular IL
agents.

</details>


### [259] [RAKOMO: Reachability-Aware K-Order Markov Path Optimization for Quadrupedal Loco-Manipulation](https://arxiv.org/abs/2507.19652)
*Mattia Risiglione,Abdelrahman Abdalla,Victor Barasuol,Kim Tien Ly,Ioannis Havoutis,Claudio Semini*

Main category: cs.RO

TL;DR: RAKOMO是一种为四足机器人设计的运动规划技术，通过结合KOMO和可及性裕度，提高了loco-manipulation任务的规划效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 为了安全有效地执行操作任务，需要一种能够考虑复杂运动约束的运动规划技术，但现有的轨迹优化方法因接触不连续性引入的混合动力学而面临挑战，并且常常因为计算原因而忽略腿部限制。

Method: RAKOMO是一种路径优化技术，它将KOMO的优点与基于可及性裕度的感知能力相结合，并利用神经网络预测裕度，将其纳入标准的KOMO公式中。

Result: RAKOMO能够快速收敛梯度优化运动规划，并有效地适应具有足式机械臂的机器人，成功执行loco-manipulation任务。通过对HyQReal四足机器人进行拾取和放置任务的仿真，RAKOMO的表现优于基线KOMO方法。

Conclusion: RAKOMO通过结合KOMO和基于可及性裕度的感知能力，成功实现了四足机器人loco-manipulation任务，并在拾取和放置任务的仿真中优于基线KOMO方法。

Abstract: Legged manipulators, such as quadrupeds equipped with robotic arms, require
motion planning techniques that account for their complex kinematic constraints
in order to perform manipulation tasks both safely and effectively. However,
trajectory optimization methods often face challenges due to the hybrid
dynamics introduced by contact discontinuities, and tend to neglect leg
limitations during planning for computational reasons. In this work, we propose
RAKOMO, a path optimization technique that integrates the strengths of K-Order
Markov Optimization (KOMO) with a kinematically-aware criterion based on the
reachable region defined as reachability margin. We leverage a neural-network
to predict the margin and optimize it by incorporating it in the standard KOMO
formulation. This approach enables rapid convergence of gradient-based motion
planning -- commonly tailored for continuous systems -- while adapting it
effectively to legged manipulators, successfully executing loco-manipulation
tasks. We benchmark RAKOMO against a baseline KOMO approach through a set of
simulations for pick-and-place tasks with the HyQReal quadruped robot equipped
with a Kinova Gen3 robotic arm.

</details>


### [260] [PhysVarMix: Physics-Informed Variational Mixture Model for Multi-Modal Trajectory Prediction](https://arxiv.org/abs/2507.19701)
*Haichuan Li,Tomi Westerlund*

Main category: cs.RO

TL;DR: 本研究提出了一种新颖的混合方法，通过整合基于学习的方法和基于物理的约束来解决轨迹预测中的多模式问题。该方法使用变分贝叶斯混合模型来捕捉各种潜在的未来行为，并通过特定部门的边界条件和模型预测控制（MPC）进行平滑处理，以确保预测的轨迹不仅数据一致，而且在物理上也是合理的。该方法在两个基准数据集上进行了评估，并取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 准确预测未来代理轨迹对于确保自动导航的安全性和效率至关重要，特别是在具有多种潜在未来场景的复杂城市环境中。

Method: 采用变分贝叶斯混合模型来捕捉潜在的未来行为，并通过特定部门的边界条件和模型预测控制（MPC）进行平滑处理，以确保预测轨迹的物理合理性。

Result: 在两个基准数据集上进行了评估，与现有方法相比，性能更优。消融研究验证了每个组成部分的贡献及其对预测准确性和可靠性的协同影响。

Conclusion: 该方法通过结合基于学习的方法和基于物理的约束，提供了对城市环境中自主导航不确定性的稳健且可扩展的解决方案。

Abstract: Accurate prediction of future agent trajectories is a critical challenge for
ensuring safe and efficient autonomous navigation, particularly in complex
urban environments characterized by multiple plausible future scenarios. In
this paper, we present a novel hybrid approach that integrates learning-based
with physics-based constraints to address the multi-modality inherent in
trajectory prediction. Our method employs a variational Bayesian mixture model
to effectively capture the diverse range of potential future behaviors, moving
beyond traditional unimodal assumptions. Unlike prior approaches that
predominantly treat trajectory prediction as a data-driven regression task, our
framework incorporates physical realism through sector-specific boundary
conditions and Model Predictive Control (MPC)-based smoothing. These
constraints ensure that predicted trajectories are not only data-consistent but
also physically plausible, adhering to kinematic and dynamic principles.
Furthermore, our method produces interpretable and diverse trajectory
predictions, enabling enhanced downstream decision-making and planning in
autonomous driving systems. We evaluate our approach on two benchmark datasets,
demonstrating superior performance compared to existing methods. Comprehensive
ablation studies validate the contributions of each component and highlight
their synergistic impact on prediction accuracy and reliability. By balancing
data-driven insights with physics-informed constraints, our approach offers a
robust and scalable solution for navigating the uncertainties of real-world
urban environments.

</details>


### [261] [DOA: A Degeneracy Optimization Agent with Adaptive Pose Compensation Capability based on Deep Reinforcement Learning](https://arxiv.org/abs/2507.19742)
*Yanbin Li,Canran Xiao,Hongyang He,Shenghai Yuan,Zong Ke,Jiajie Yu,Zixiong Qin,Zhiguo Zhang,Wenzheng Chi,Wei Zhang*

Main category: cs.RO

TL;DR: 本研究提出了一种基于PPO的自适应退化优化剂（DOA），通过特殊奖励函数和迁移学习解决了2D-SLAM在室内退化环境中的问题，并取得了优于现有技术的性能。


<details>
  <summary>Details</summary>
Motivation: 粒子滤波-SLAM虽然在室内定位中效率高，但在长直走廊等室内环境中会遇到严重的退化问题。本研究旨在解决这一退化问题。

Method: 本研究利用Proximal Policy Optimization (PPO) 训练一个自适应退化优化剂（DOA），以解决2D-SLAM中的退化问题。通过设计一个专门的奖励函数来指导DOA学习感知退化环境的能力，并利用输出的退化因子作为参考权重，动态调整不同传感器对姿态优化的贡献。具体来说，观察分布会向运动模型分布转移，步长由与退化因子相关的线性插值公式确定。此外，还引入了迁移学习模块以增强DOA的泛化能力。

Result: 研究结果表明，提出的DOA模型在退化检测和优化能力方面优于现有技术（SOTA methods），并且在各种环境中都表现出色。消融研究也证明了模型设计的合理性和迁移学习的作用。

Conclusion: 该研究提出了一种基于PPO的自适应退化优化剂（DOA），用于解决2D-SLAM在室内环境中（如长直走廊）遇到的退化问题。通过设计专门的奖励函数来训练DOA，使其能够感知退化环境并动态调整传感器对姿态优化的贡献。研究还采用了迁移学习模块来提高DOA的泛化能力。实验结果表明，DOA在退化检测和优化方面优于现有技术。

Abstract: Particle filter-based 2D-SLAM is widely used in indoor localization tasks due
to its efficiency. However, indoor environments such as long straight corridors
can cause severe degeneracy problems in SLAM. In this paper, we use Proximal
Policy Optimization (PPO) to train an adaptive degeneracy optimization agent
(DOA) to address degeneracy problem. We propose a systematic methodology to
address three critical challenges in traditional supervised learning
frameworks: (1) data acquisition bottlenecks in degenerate dataset, (2)
inherent quality deterioration of training samples, and (3) ambiguity in
annotation protocol design. We design a specialized reward function to guide
the agent in developing perception capabilities for degenerate environments.
Using the output degeneracy factor as a reference weight, the agent can
dynamically adjust the contribution of different sensors to pose optimization.
Specifically, the observation distribution is shifted towards the motion model
distribution, with the step size determined by a linear interpolation formula
related to the degeneracy factor. In addition, we employ a transfer learning
module to endow the agent with generalization capabilities across different
environments and address the inefficiency of training in degenerate
environments. Finally, we conduct ablation studies to demonstrate the
rationality of our model design and the role of transfer learning. We also
compare the proposed DOA with SOTA methods to prove its superior degeneracy
detection and optimization capabilities across various environments.

</details>


### [262] [Homotopy-aware Multi-agent Navigation via Distributed Model Predictive Control](https://arxiv.org/abs/2507.19860)
*Haoze Dong,Meng Guo,Chengyi He,Zhongkui Li*

Main category: cs.RO

TL;DR: 提出了一种新的分布式多智能体轨迹规划框架，通过结合考虑时空特性的全局同伦路径规划和基于MPC的局部轨迹优化，有效解决了狭窄走廊环境下的死锁问题，并将成功率从13%提高到90%以上。


<details>
  <summary>Details</summary>
Motivation: 多智能体轨迹规划在保证安全性和效率的同时，仍面临死锁的挑战，尤其是在障碍物密集的环境中。当多个智能体尝试同时通过同一狭窄长走廊时，死锁问题尤为突出。

Method: 提出了一种新的分布式轨迹规划框架，包括：1. 全局层面：采用一种考虑同伦类并结合时空特性的最优路径规划算法，以利用环境拓扑结构进行全局协调。2. 局部层面：采用基于模型预测控制的轨迹优化方法，生成动态可行且无碰撞的轨迹，并结合在线重规划策略以适应动态环境。

Result: 仿真和实验验证了该方法在缓解死锁方面的有效性。消融研究表明，通过将时间感知的同伦属性纳入全局路径，可以将随机生成的密集场景下的平均成功率从4%-13%提高到90%以上。

Conclusion: 该框架通过结合全局路径规划和局部轨迹优化的方法，有效解决了多智能体在密集环境中因走廊冲突导致的死锁问题，实验证明能够显著提高成功率。

Abstract: Multi-agent trajectory planning requires ensuring both safety and efficiency,
yet deadlocks remain a significant challenge, especially in obstacle-dense
environments. Such deadlocks frequently occur when multiple agents attempt to
traverse the same long and narrow corridor simultaneously. To address this, we
propose a novel distributed trajectory planning framework that bridges the gap
between global path and local trajectory cooperation. At the global level, a
homotopy-aware optimal path planning algorithm is proposed, which fully
leverages the topological structure of the environment. A reference path is
chosen from distinct homotopy classes by considering both its spatial and
temporal properties, leading to improved coordination among agents globally. At
the local level, a model predictive control-based trajectory optimization
method is used to generate dynamically feasible and collision-free
trajectories. Additionally, an online replanning strategy ensures its
adaptability to dynamic environments. Simulations and experiments validate the
effectiveness of our approach in mitigating deadlocks. Ablation studies
demonstrate that by incorporating time-aware homotopic properties into the
underlying global paths, our method can significantly reduce deadlocks and
improve the average success rate from 4%-13% to over 90% in randomly generated
dense scenarios.

</details>


### [263] [Skin-Machine Interface with Multimodal Contact Motion Classifier](https://arxiv.org/abs/2507.19760)
*Alberto Confente,Takanori Jin,Taisuke Kobayashi,Julio Rogelio Guadarrama-Olvera,Gordon Cheng*

Main category: cs.RO

TL;DR: 该研究提出了一种新颖的框架，利用具有多模态触觉感知能力的皮肤传感器作为复杂机器人的操作界面。通过使用循环神经网络的接触运动分类器，可以识别和映射操作员的接触手势，从而生成各种机器人运动。研究强调了多模态传感和柔性传感器安装的重要性，这些因素提高了分类准确性（超过95%），使双臂移动机械臂能够执行各种任务。


<details>
  <summary>Details</summary>
Motivation: 提出一种利用皮肤传感器作为复杂机器人新操作界面的新框架。

Method: 利用循环神经网络的基于学习的接触运动分类器。

Result: 所提出的框架通过双臂移动机械 वापरा皮肤传感器成功实现了各种任务。

Conclusion: 该框架能够通过双臂移动机械臂执行各种任务，准确率超过95%。

Abstract: This paper proposes a novel framework for utilizing skin sensors as a new
operation interface of complex robots. The skin sensors employed in this study
possess the capability to quantify multimodal tactile information at multiple
contact points. The time-series data generated from these sensors is
anticipated to facilitate the classification of diverse contact motions
exhibited by an operator. By mapping the classification results with robot
motion primitives, a diverse range of robot motions can be generated by
altering the manner in which the skin sensors are interacted with. In this
paper, we focus on a learning-based contact motion classifier employing
recurrent neural networks. This classifier is a pivotal factor in the success
of this framework. Furthermore, we elucidate the requisite conditions for
software-hardware designs. Firstly, multimodal sensing and its comprehensive
encoding significantly contribute to the enhancement of classification accuracy
and learning stability. Utilizing all modalities simultaneously as inputs to
the classifier proves to be an effective approach. Secondly, it is essential to
mount the skin sensors on a flexible and compliant support to enable the
activation of three-axis accelerometers. These accelerometers are capable of
measuring horizontal tactile information, thereby enhancing the correlation
with other modalities. Furthermore, they serve to absorb the noises generated
by the robot's movements during deployment. Through these discoveries, the
accuracy of the developed classifier surpassed 95 %, enabling the dual-arm
mobile manipulator to execute a diverse range of tasks via the Skin-Machine
Interface. https://youtu.be/UjUXT4Z4BC8

</details>


### [264] [Ag2x2: Robust Agent-Agnostic Visual Representations for Zero-Shot Bimanual Manipulation](https://arxiv.org/abs/2507.19817)
*Ziyin Xiong,Yinghan Chen,Puhao Li,Yixin Zhu,Tengyu Liu,Siyuan Huang*

Main category: cs.RO

TL;DR: Ag2x2 是一个用于双臂操作的计算框架，它使用协调感知的视觉表示来学习物体状态和手部运动模式，实现了高成功率，并支持无需专家监督的可扩展技能获取。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实现双臂操作时，忽略了对双臂协调至关重要的特定智能体信息（如末端执行器位置）。本研究旨在通过 Ag2x2 框架解决这一挑战，以实现双臂操作。

Method: Ag2x2 框架通过一种计算方法，学习协调感知的视觉表示，该表示能够同时编码物体状态和手部运动模式，同时保持智能体无关性。

Result: Ag2x2 在 13 项双臂操作任务中取得了 73.5% 的成功率，超越了基线方法和专家设计的奖励函数。此外，Ag2x2 学习到的表示能够有效地用于模仿学习，建立了无需专家监督即可获取技能的可扩展流程。

Conclusion: Ag2x2 框架通过结合协调感知视觉表示，实现了双臂操作的突破，能够同时编码物体状态和手部运动模式，并保持了对智能体本身的无关性。该框架在 Bi-DexHands 和 PerAct2 的 13 项双臂操作任务中取得了 73.5% 的成功率，超越了基线方法和专家设计的奖励函数，并在模仿学习中展现了无需专家监督即可进行技能获取的可扩展性。

Abstract: Bimanual manipulation, fundamental to human daily activities, remains a
challenging task due to its inherent complexity of coordinated control. Recent
advances have enabled zero-shot learning of single-arm manipulation skills
through agent-agnostic visual representations derived from human videos;
however, these methods overlook crucial agent-specific information necessary
for bimanual coordination, such as end-effector positions. We propose Ag2x2, a
computational framework for bimanual manipulation through coordination-aware
visual representations that jointly encode object states and hand motion
patterns while maintaining agent-agnosticism. Extensive experiments demonstrate
that Ag2x2 achieves a 73.5% success rate across 13 diverse bimanual tasks from
Bi-DexHands and PerAct2, including challenging scenarios with deformable
objects like ropes. This performance outperforms baseline methods and even
surpasses the success rate of policies trained with expert-engineered rewards.
Furthermore, we show that representations learned through Ag2x2 can be
effectively leveraged for imitation learning, establishing a scalable pipeline
for skill acquisition without expert supervision. By maintaining robust
performance across diverse tasks without human demonstrations or engineered
rewards, Ag2x2 represents a step toward scalable learning of complex bimanual
robotic skills.

</details>


### [265] [A 4D Radar Camera Extrinsic Calibration Tool Based on 3D Uncertainty Perspective N Points](https://arxiv.org/abs/2507.19829)
*Chuan Cao,Xiaoning Wang,Wenqian Xi,Han Zhang,Weidong Chen,Jingchuan Wang*

Main category: cs.RO

TL;DR: 该研究提出了一种名为 3DUPnP 的校准框架，通过考虑雷达测量中的球坐标噪声传播和坐标转换中的误差，提高了毫米波雷达和相机的校准精度和一致性，优于现有技术，适用于自动驾驶和机器人感知。


<details>
  <summary>Details</summary>
Motivation: 在机器人技术中，毫米波雷达和相机系统之间的精确外在校准对于鲁棒的多模态感知至关重要，但由于固有的传感器噪声特性和复杂的误差传播，这一过程仍然充满挑战。

Method: 本论文提出了一种系统的校准框架，通过空间三维不确定性感知 PnP 算法（3DUPnP）来解决关键挑战，该算法明确地模拟了雷达测量中的球坐标噪声传播，然后补偿了坐标转换过程中非零的误差期望。

Result: 实验验证表明，与最先进的 CPnP 基线相比，在模拟中的一致性得到改善，在物理实验中的精度得到提高，性能得到了显著提升。

Conclusion: 这项研究为配备毫米波雷达和相机的机器人系统提供了一个鲁棒的校准解决方案，特别适用于自动驾驶和机器人感知应用。

Abstract: 4D imaging radar is a type of low-cost millimeter-wave radar(costing merely
10-20$\%$ of lidar systems) capable of providing range, azimuth, elevation, and
Doppler velocity information. Accurate extrinsic calibration between
millimeter-wave radar and camera systems is critical for robust multimodal
perception in robotics, yet remains challenging due to inherent sensor noise
characteristics and complex error propagation. This paper presents a systematic
calibration framework to address critical challenges through a spatial 3d
uncertainty-aware PnP algorithm (3DUPnP) that explicitly models spherical
coordinate noise propagation in radar measurements, then compensating for
non-zero error expectations during coordinate transformations. Finally,
experimental validation demonstrates significant performance improvements over
state-of-the-art CPnP baseline, including improved consistency in simulations
and enhanced precision in physical experiments. This study provides a robust
calibration solution for robotic systems equipped with millimeter-wave radar
and cameras, tailored specifically for autonomous driving and robotic
perception applications.

</details>


### [266] [Feeling the Force: A Nuanced Physics-based Traversability Sensor for Navigation in Unstructured Vegetation](https://arxiv.org/abs/2507.19831)
*Zaar Khizar,Johann Laconte,Roland Lenain,Romuald Aufrere*

Main category: cs.RO

TL;DR: 该研究设计并验证了一种新的传感器，用于测量机器人与植被交互时所受到的力，从而提高机器人在自然环境中的导航能力。


<details>
  <summary>Details</summary>
Motivation: 机器人需要在非结构化和自然环境中运行，而植被作为一种可穿越的障碍物，其机械特性会影响机器人的安全性和可穿越性。需要更细致的方法来评估这些障碍物。

Method: 提出了一种能够直接测量植被施加在机器人上的力的传感器，并通过实验验证了其有效性。

Result: 实验验证了该传感器能够测量细微的力变化，为机器人导航决策提供了量化指标。

Conclusion: 该研究提出了一种基于力的新方法，用于量化机器人与植被的交互作用，为机器人导航和学习算法奠定了基础。

Abstract: In many applications, robots are increasingly deployed in unstructured and
natural environments where they encounter various types of vegetation.
Vegetation presents unique challenges as a traversable obstacle, where the
mechanical properties of the plants can influence whether a robot can safely
collide with and overcome the obstacle. A more nuanced approach is required to
assess the safety and traversability of these obstacles, as collisions can
sometimes be safe and necessary for navigating through dense or unavoidable
vegetation. This paper introduces a novel sensor designed to directly measure
the applied forces exerted by vegetation on a robot: by directly capturing the
push-back forces, our sensor provides a detailed understanding of the
interactions between the robot and its surroundings. We demonstrate the
sensor's effectiveness through experimental validations, showcasing its ability
to measure subtle force variations. This force-based approach provides a
quantifiable metric that can inform navigation decisions and serve as a
foundation for developing future learning algorithms.

</details>


### [267] [PlaneHEC: Efficient Hand-Eye Calibration for Multi-view Robotic Arm via Any Point Cloud Plane Detection](https://arxiv.org/abs/2507.19851)
*Ye Wang,Haodong Jing,Yang Liao,Yongqiang Ma,Nanning Zheng*

Main category: cs.RO

TL;DR: PlaneHEC是一种创新的手眼校准方法，利用平面约束和深度相机，无需复杂模型，即可实现快速、高精度的校准。


<details>
  <summary>Details</summary>
Motivation: 现有机器人系统的多视图手眼校准方法通常依赖于精确的几何模型或手动辅助，泛化能力差，且复杂低效。因此，需要一种更通用、高效且无需复杂模型的方法。

Method: PlaneHEC方法引入了基于平面约束的手眼校准方程，结合了闭式解和迭代优化，无需复杂模型，仅使用深度相机和任意平面（如墙壁、桌面）进行校准。

Result: PlaneHEC在模拟和真实环境中均表现出色，与基于点云的校准方法相比具有明显优势，实现了通用、快速的校准。

Conclusion: PlaneHEC是一种基于平面约束的广义手眼校准方法，无需复杂模型，仅需深度相机和任意平面即可实现最优、最快校准。该方法通过引入手眼校准方程和平面约束，具有高度可解释性和通用性，并通过结合闭式解和迭代优化显著提高了精度。在模拟和真实环境中的评估结果证明了其优越性，为多智能体系统和具身智能的发展做出了贡献。

Abstract: Hand-eye calibration is an important task in vision-guided robotic systems
and is crucial for determining the transformation matrix between the camera
coordinate system and the robot end-effector. Existing methods, for multi-view
robotic systems, usually rely on accurate geometric models or manual
assistance, generalize poorly, and can be very complicated and inefficient.
Therefore, in this study, we propose PlaneHEC, a generalized hand-eye
calibration method that does not require complex models and can be accomplished
using only depth cameras, which achieves the optimal and fastest calibration
results using arbitrary planar surfaces like walls and tables. PlaneHEC
introduces hand-eye calibration equations based on planar constraints, which
makes it strongly interpretable and generalizable. PlaneHEC also uses a
comprehensive solution that starts with a closed-form solution and improves it
withiterative optimization, which greatly improves accuracy. We comprehensively
evaluated the performance of PlaneHEC in both simulated and real-world
environments and compared the results with other point-cloud-based calibration
methods, proving its superiority. Our approach achieves universal and fast
calibration with an innovative design of computational models, providing a
strong contribution to the development of multi-agent systems and embodied
intelligence.

</details>


### [268] [Think, Act, Learn: A Framework for Autonomous Robotic Agents using Closed-Loop Large Language Models](https://arxiv.org/abs/2507.19854)
*Anjali R. Menon,Rohit K. Sharma,Priya Singh,Chengyu Wang,Aurora M. Ferreira,Mateja Novak*

Main category: cs.RO

TL;DR: LLM驱动的闭环框架“思考、行动、学习”（T-A-L）使机器人能够通过互动学习和改进策略，实现了高成功率和任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于LLM的机器人系统在动态环境中因开环操作而变得脆弱且无法适应的问题。

Method: “思考、行动、学习”（T-A-L）框架：一个闭环系统，其中LLM首先“思考”以分解高级命令，机器人“行动”以执行计划并收集多模态反馈，然后“学习”模块处理反馈以进行LLM驱动的自我反思、因果分析和纠正策略生成，并将经验存储在记忆库中以指导未来的规划。

Result: T-A-L代理在模拟和真实世界实验中表现优于基线方法（开环LLM、行为克隆、传统强化学习），在复杂、长时程任务中成功率超过97%，平均仅需9次试验即可收敛，并能泛化到未见过的任务。

Conclusion: 该框架通过持续互动实现自主学习和策略精炼，显著优于现有方法，在复杂任务中成功率超过97%，平均9次试验即可收敛，并展现出卓越的泛化能力，是迈向更鲁棒、更自适应、真正自主的机器人代理的重要一步。

Abstract: The integration of Large Language Models (LLMs) into robotics has unlocked
unprecedented capabilities in high-level task planning. However, most current
systems operate in an open-loop fashion, where LLMs act as one-shot planners,
rendering them brittle and unable to adapt to unforeseen circumstances in
dynamic physical environments. To overcome this limitation, this paper
introduces the "Think, Act, Learn" (T-A-L) framework, a novel architecture that
enables an embodied agent to autonomously learn and refine its policies through
continuous interaction. Our framework establishes a closed-loop cycle where an
LLM first "thinks" by decomposing high-level commands into actionable plans.
The robot then "acts" by executing these plans while gathering rich, multimodal
sensory feedback. Critically, the "learn" module processes this feedback to
facilitate LLM-driven self-reflection, allowing the agent to perform causal
analysis on its failures and generate corrective strategies. These insights are
stored in an experiential memory to guide future planning cycles. We
demonstrate through extensive experiments in both simulation and the real world
that our T-A-L agent significantly outperforms baseline methods, including
open-loop LLMs, Behavioral Cloning, and traditional Reinforcement Learning. Our
framework achieves over a 97% success rate on complex, long-horizon tasks,
converges to a stable policy in an average of just 9 trials, and exhibits
remarkable generalization to unseen tasks. This work presents a significant
step towards developing more robust, adaptive, and truly autonomous robotic
agents.

</details>


### [269] [Bridging Simulation and Usability: A User-Friendly Framework for Scenario Generation in CARLA](https://arxiv.org/abs/2507.19883)
*Ahmed Abouelazm,Mohammad Mahmoud,Conrad Walter,Oleksandr Shchetsura,Erne Hussong,Helen Gremmelmaier,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 提出了一种无需代码的交互式框架，用于在自动驾驶汽车测试中生成场景，降低了准入门槛，并支持手动和自动生成。


<details>
  <summary>Details</summary>
Motivation: 现有的场景生成工具通常需要编程知识，这限制了非技术用户的可访问性。为了解决这一限制，需要一个更易于访问的框架。

Method: 提出了一种交互式的、无需代码的场景生成框架，具有图形界面，允许用户创建、修改、保存、加载和执行场景，而无需编码知识或详细的模拟知识。该框架以基于图的场景表示为中心，支持手动和自动生成，并能与基于深度学习的场景和行为生成方法集成。

Result: 该框架降低了准入门槛，支持更广泛的用户群体，与 Scenic 或 ScenarioRunner 等基于脚本的工具不同。在自动模式下，该框架可以随机抽样参与者类型、行为和环境条件等参数，从而生成多样化且真实的测试数据集。

Conclusion: 该框架通过简化的场景生成过程，支持更有效的测试工作流程，并提高模拟器测试的可用性，可供研究人员、工程师和政策制定者使用。

Abstract: Autonomous driving promises safer roads, reduced congestion, and improved
mobility, yet validating these systems across diverse conditions remains a
major challenge. Real-world testing is expensive, time-consuming, and sometimes
unsafe, making large-scale validation impractical. In contrast, simulation
environments offer a scalable and cost-effective alternative for rigorous
verification and validation. A critical component of the validation process is
scenario generation, which involves designing and configuring traffic scenarios
to evaluate autonomous systems' responses to various events and uncertainties.
However, existing scenario generation tools often require programming
knowledge, limiting accessibility for non-technical users. To address this
limitation, we present an interactive, no-code framework for scenario
generation. Our framework features a graphical interface that enables users to
create, modify, save, load, and execute scenarios without needing coding
expertise or detailed simulation knowledge. Unlike script-based tools such as
Scenic or ScenarioRunner, our approach lowers the barrier to entry and supports
a broader user base. Central to our framework is a graph-based scenario
representation that facilitates structured management, supports both manual and
automated generation, and enables integration with deep learning-based scenario
and behavior generation methods. In automated mode, the framework can randomly
sample parameters such as actor types, behaviors, and environmental conditions,
allowing the generation of diverse and realistic test datasets. By simplifying
the scenario generation process, this framework supports more efficient testing
workflows and increases the accessibility of simulation-based validation for
researchers, engineers, and policymakers.

</details>


### [270] [High-Speed Event Vision-Based Tactile Roller Sensor for Large Surface Measurements](https://arxiv.org/abs/2507.19914)
*Akram Khairi,Hussain Sajwani,Abdallah Mohammad Alkilany,Laith AbuAssi,Mohamad Halwani,Islam Mohamed Zaid,Ahmed Awadalla,Dewald Swart,Abdulla Ayyad,Yahya Zweiri*

Main category: cs.RO

TL;DR: 研究提出了一种结合神经拟态相机和滚动机制的新型触觉传感器，通过事件驱动的多视图立体匹配和贝叶斯融合技术，实现了比以往方法快 11 倍的工业表面 3D 检测速度，同时保证了高精度。


<details>
  <summary>Details</summary>
Motivation: 传统的基于视觉的触觉传感器（VBTS）在测量大面积表面时速度较慢，并且存在运动模糊和摩擦/磨损等问题。因此，需要一种能够实现快速、连续、高分辨率测量的解决方案。

Method: 研究利用神经拟态相机的 VBT（视觉基础触觉传感器）和事件驱动的多视图立体匹配方法进行 3D 重建，并采用多参考贝叶斯融合策略提高精度并减少曲率误差。

Result: 新传感器实现了高达 0.5 m/s 的扫描速度，平均绝对误差（MAE）低于 100 微米，比以前的连续触觉传感方法快 11 倍。多参考贝叶斯融合策略将 MAE 降低了 25.2%，并且在盲文识别任务中速度提高了 2.6 倍。

Conclusion: 本研究引入了一种将神经拟态相机集成到滚动机制中的新型触觉传感器，实现了高速、连续、高分辨率的大规模工业表面检测。

Abstract: Inspecting large-scale industrial surfaces like aircraft fuselages for
quality control requires capturing their precise 3D surface geometry at high
resolution. Vision-based tactile sensors (VBTSs) offer high local resolution
but require slow 'press-and-lift' measurements stitched for large areas.
Approaches with sliding or roller/belt VBTS designs provide measurements
continuity. However, they face significant challenges respectively: sliding
struggles with friction/wear and both approaches are speed-limited by
conventional camera frame rates and motion blur, making large-area scanning
time consuming. Thus, a rapid, continuous, high-resolution method is needed. We
introduce a novel tactile sensor integrating a neuromorphic camera in a rolling
mechanism to achieve this. Leveraging its high temporal resolution and
robustness to motion blur, our system uses a modified event-based multi-view
stereo approach for 3D reconstruction. We demonstrate state-of-the-art scanning
speeds up to 0.5 m/s, achieving Mean Absolute Error below 100 microns -- 11
times faster than prior continuous tactile sensing methods. A multi-reference
Bayesian fusion strategy enhances accuracy (reducing MAE by 25.2\% compared to
EMVS) and mitigates curvature errors. We also validate high-speed feature
recognition via Braille reading 2.6 times faster than previous approaches.

</details>


### [271] [Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations](https://arxiv.org/abs/2507.19947)
*Supawich Sitdhipol,Waritwong Sukprasongdee,Ekapol Chuangsuwanich,Rina Tse*

Main category: cs.RO

TL;DR: 该研究提出了一种名为FP-LGN的新模型，用于融合人类的语言观察和机器人的传感器数据，以提高人机协作任务的性能。该模型通过学习空间语言的特征表示，并考虑其不确定性，实现了更准确和鲁棒的融合，最终在协作感知任务中取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 为了克服机器人协作任务中的感知限制，需要一种能够融合人类观察信息的不确定性感知框架，该框架需要一个能表示人类输入不确定性的接地似然。

Method: 提出了一种名为特征金字塔似然接地网络（FP-LGN）的框架，该框架通过学习地图图像特征及其与空间关系语义的关系来接地空间语言，并采用三阶段课程学习来捕捉人类语言中的不确定性。

Result: FP-LGN模型在负对数似然（NLL）方面与专家设计的规则相当，且标准差较低，显示出更强的鲁棒性。在协作感知任务中，该模型成功实现了异构人类语言观察和机器人传感器测量的融合，显著提高了人机协作任务的性能。

Conclusion: 该研究提出的FP-LGN模型能够有效地将人类的语言观察与机器人的感知数据融合，并通过不确定性感知实现了协作感知能力的提升，在人机协作任务中取得了显著的性能改进。

Abstract: Fusing information from human observations can help robots overcome sensing
limitations in collaborative tasks. However, an uncertainty-aware fusion
framework requires a grounded likelihood representing the uncertainty of human
inputs. This paper presents a Feature Pyramid Likelihood Grounding Network
(FP-LGN) that grounds spatial language by learning relevant map image features
and their relationships with spatial relation semantics. The model is trained
as a probability estimator to capture aleatoric uncertainty in human language
using three-stage curriculum learning. Results showed that FP-LGN matched
expert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated
greater robustness with lower standard deviation. Collaborative sensing results
demonstrated that the grounded likelihood successfully enabled
uncertainty-aware fusion of heterogeneous human language observations and robot
sensor measurements, achieving significant improvements in human-robot
collaborative task performance.

</details>


### [272] [A roadmap for AI in robotics](https://arxiv.org/abs/2507.19975)
*Aude Billard,Alin Albu-Schaeffer,Michael Beetz,Wolfram Burgard,Peter Corke,Matei Ciocarlie,Ravinder Dahiya,Danica Kragic,Ken Goldberg,Yukie Nagai,Davide Scaramuzza*

Main category: cs.RO

TL;DR: AI在机器人领域的应用前景广阔，但仍面临数据、算法、安全和可解释性等挑战。文章评估了AI在机器人领域的进展，并提出了研究路线图，强调了机器人需要具备预测人类行为、终身学习和安全部署的能力，以实现与人类的有效协作。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的突破，人们对利用AI解决机器人部署障碍的潜力感到兴奋，但物理世界的交互对AI提出了不同于数据分析的挑战，因此需要评估哪些AI技术最适合机器人，如何适配以及需要克服哪些挑战。

Method: 本文对自20世纪90年代以来AI在机器人领域的应用进行了评估，并提出了一个包含挑战与前景的短期和中期研究路线图。

Result: 本文评估了AI在机器人领域的进展，并提出了一个研究路线图，强调了数据多样性、任务适应性、算法泛化能力、可解释性、人机协作中的行为预测以及终身学习等关键挑战。

Conclusion: AI 在机器人领域的应用仍面临诸多挑战，包括数据、算法、泛化能力、可解释性、安全性以及终身学习能力。未来需要设计更适应机器人特性的AI算法，并解决与之相关的长期挑战，以实现机器人与人类的有效协作和安全部署。

Abstract: AI technologies, including deep learning, large-language models have gone
from one breakthrough to the other. As a result, we are witnessing growing
excitement in robotics at the prospect of leveraging the potential of AI to
tackle some of the outstanding barriers to the full deployment of robots in our
daily lives. However, action and sensing in the physical world pose greater and
different challenges than analysing data in isolation. As the development and
application of AI in robotic products advances, it is important to reflect on
which technologies, among the vast array of network architectures and learning
models now available in the AI field, are most likely to be successfully
applied to robots; how they can be adapted to specific robot designs, tasks,
environments; which challenges must be overcome. This article offers an
assessment of what AI for robotics has achieved since the 1990s and proposes a
short- and medium-term research roadmap listing challenges and promises. These
range from keeping up-to-date large datasets, representatives of a diversity of
tasks robots may have to perform, and of environments they may encounter, to
designing AI algorithms tailored specifically to robotics problems but generic
enough to apply to a wide range of applications and transfer easily to a
variety of robotic platforms. For robots to collaborate effectively with
humans, they must predict human behavior without relying on bias-based
profiling. Explainability and transparency in AI-driven robot control are not
optional but essential for building trust, preventing misuse, and attributing
responsibility in accidents. We close on what we view as the primary long-term
challenges, that is, to design robots capable of lifelong learning, while
guaranteeing safe deployment and usage, and sustainable computational costs.

</details>


### [273] [CLASP: General-Purpose Clothes Manipulation with Semantic Keypoints](https://arxiv.org/abs/2507.19983)
*Yuhong Deng,Chao Tang,Cunjun Yu,Linfeng Li,David Hsu*

Main category: cs.RO

TL;DR: CLASP使用语义关键点来处理各种服装和任务，并通过视觉语言模型和技能库实现了通用服装操作，在模拟和真实机器人实验中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的服装操作方法通常局限于特定的任务和服装类型，这是由于服装本身复杂且高维的几何形状造成的。本研究旨在开发一种能够处理不同服装类型（如T恤、短裤、裙子、连衣裙等）和不同任务（如折叠、铺平、悬挂等）的通用服装操作方法。

Method: CLASP通过提取服装的语义关键点（如“左袖子”、“右肩”等）作为一种稀疏的空间-语义表示，实现了跨不同服装类型和任务的通用服装操作。它利用视觉语言模型（VLMs）来预测基于语义关键点的任务计划，并结合预先构建的操作技能库来执行这些计划。

Result: CLASP在多种任务和服装类型上都优于现有的基线方法，并在真实机器人（Franka双臂系统）上成功完成了折叠、铺平、悬挂和放置等四项不同的任务，验证了其在实际应用中的有效性。

Conclusion: CLASP在多种服装类型和任务上都表现出色，并在真实机器人上得到了验证，证明了其在服装操作方面的强大性能和泛化能力。

Abstract: Clothes manipulation, such as folding or hanging, is a critical capability
for home service robots. Despite recent advances, most existing methods remain
limited to specific tasks and clothes types, due to the complex,
high-dimensional geometry of clothes. This paper presents CLothes mAnipulation
with Semantic keyPoints (CLASP), which aims at general-purpose clothes
manipulation over different clothes types, T-shirts, shorts, skirts, long
dresses, ... , as well as different tasks, folding, flattening, hanging, ... .
The core idea of CLASP is semantic keypoints -- e.g., ''left sleeve'', ''right
shoulder'', etc. -- a sparse spatial-semantic representation that is salient
for both perception and action. Semantic keypoints of clothes can be reliably
extracted from RGB-D images and provide an effective intermediate
representation of clothes manipulation policies. CLASP uses semantic keypoints
to bridge high-level task planning and low-level action execution. At the high
level, it exploits vision language models (VLMs) to predict task plans over the
semantic keypoints. At the low level, it executes the plans with the help of a
simple pre-built manipulation skill library. Extensive simulation experiments
show that CLASP outperforms state-of-the-art baseline methods on multiple tasks
across diverse clothes types, demonstrating strong performance and
generalization. Further experiments with a Franka dual-arm system on four
distinct tasks -- folding, flattening, hanging, and placing -- confirm CLASP's
performance on a real robot.

</details>


### [274] [Robot Excavation and Manipulation of Geometrically Cohesive Granular Media](https://arxiv.org/abs/2507.19999)
*Laura Treers,Daniel Soto,Joonha Hwang,Michael A. D. Goodisman,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 机器人通过操纵纠缠颗粒材料建造结构，其性能受材料属性影响，尤其是初始压缩加载。


<details>
  <summary>Details</summary>
Motivation: 探索机器人群体在建造非确定性结构方面的潜力，以及机器人系统有效操纵软物质的原理。

Method: 开发了一个机器人物理模型，使用户能够自主协调挖掘、运输和沉积材料，并通过拉伸测试装置来表征材料特性。

Result: 机器人性能受基板初始条件（压缩加载）影响显著（高达75%的变化），并揭示了纠缠材料的强度与初始压缩加载密切相关。

Conclusion: 本文研究了机器人群体如何通过操纵和形成纠缠的颗粒材料来建造非确定性结构，并开发了一个机器人物理模型来探索其与软物质相互作用的原理。研究测试了不同初始条件对机器人性能的影响，发现基板初始条件（如压缩加载）对挖掘和运输有显著影响（高达75%的变化），并开发了拉伸测试装置来表征材料特性，揭示了纠缠材料的强度对初始压缩加载的敏感性。

Abstract: Construction throughout history typically assumes that its blueprints and
building blocks are pre-determined. However, recent work suggests that
alternative approaches can enable new paradigms for structure formation.
Aleatory architectures, or those which rely on the properties of their granular
building blocks rather than pre-planned design or computation, have thus far
relied on human intervention for their creation. We imagine that robotic swarms
could be valuable to create such aleatory structures by manipulating and
forming structures from entangled granular materials. To discover principles by
which robotic systems can effectively manipulate soft matter, we develop a
robophysical model for interaction with geometrically cohesive granular media
composed of u-shape particles. This robotic platform uses environmental signals
to autonomously coordinate excavation, transport, and deposition of material.
We test the effect of substrate initial conditions by characterizing robot
performance in two different material compaction states and observe as much as
a 75% change in transported mass depending on initial substrate compressive
loading. These discrepancies suggest the functional role that material
properties such as packing and cohesion/entanglement play in excavation and
construction. To better understand these material properties, we develop an
apparatus for tensile testing of the geometrically cohesive substrates, which
reveals how entangled material strength responds strongly to initial
compressive loading. These results explain the variation observed in robotic
performance and point to future directions for better understanding robotic
interaction mechanics with entangled materials.

</details>


### [275] [SuperMag: Vision-based Tactile Data Guided High-resolution Tactile Shape Reconstruction for Magnetic Tactile Sensors](https://arxiv.org/abs/2507.20002)
*Peiyao Hou,Danning Sun,Meng Wang,Yuzhe Huang,Zeyu Zhang,Hangxin Liu,Wanlin Li,Ziyuan Jiao*

Main category: cs.RO

TL;DR: SuperMag是一种利用VBTS数据来提升MBTS空间分辨率的触觉形状重建方法，通过CVAE实现高精度、低延迟的形状感知，适用于精密机器人任务。


<details>
  <summary>Details</summary>
Motivation: 磁性触觉传感器（MBTS）虽然设计紧凑且运行频率高，但由于其稀疏的taxel阵列，空间分辨率有限。本研究旨在通过结合VBTS来提高MBTS的空间分辨率。

Method: 该方法将触觉形状重建为一个条件生成问题，并采用条件变分自编码器（CVAE）从低分辨率的MBTS输入中推断出高分辨率的形状。

Result: SuperMag方法实现了高分辨率的形状重建，MBTS的采样频率为125 Hz，形状重建的推理时间在2.5毫秒以内。这种跨模态的协同作用提高了MBTS的触觉感知能力，有望在精密机器人任务中实现新功能。

Conclusion: 这项工作通过利用高分辨率的视觉触觉传感器（VBTS）数据来监督磁性触觉传感器（MBTS）的超分辨率，提出了一种名为SuperMag的触觉形状重建方法，解决了MBTS空间分辨率有限的问题。

Abstract: Magnetic-based tactile sensors (MBTS) combine the advantages of compact
design and high-frequency operation but suffer from limited spatial resolution
due to their sparse taxel arrays. This paper proposes SuperMag, a tactile shape
reconstruction method that addresses this limitation by leveraging
high-resolution vision-based tactile sensor (VBTS) data to supervise MBTS
super-resolution. Co-designed, open-source VBTS and MBTS with identical contact
modules enable synchronized data collection of high-resolution shapes and
magnetic signals via a symmetric calibration setup. We frame tactile shape
reconstruction as a conditional generative problem, employing a conditional
variational auto-encoder to infer high-resolution shapes from low-resolution
MBTS inputs. The MBTS achieves a sampling frequency of 125 Hz, whereas the
shape reconstruction sustains an inference time within 2.5 ms. This
cross-modality synergy advances tactile perception of the MBTS, potentially
unlocking its new capabilities in high-precision robotic tasks.

</details>


### [276] [When Engineering Outruns Intelligence: A Re-evaluation of Instruction-Guided Navigation](https://arxiv.org/abs/2507.20021)
*Matin Aghaei,Mohammad Ali Alomrani,Yingxue Zhang,Mahdi Biparva*

Main category: cs.RO

TL;DR: LLM在物体目标导航中的作用被高估了，几何信息和简单的探索方法才是提升导航性能的关键。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）在物体目标导航（ObjectGoal Navigation）任务中的规划能力提升程度，并厘清LLM的贡献是否被夸大。

Method: 通过移除InstructNav中的LLM组件（如动态导航链提示、GLEE检测器和直觉显著性图），并用几何驱动的探索方法（如距离加权前沿探索器DWFE）替换，来评估LLM在规划中的作用。随后，引入轻量级语言先验（SHF）以进一步评估语言信息的影响。

Result: DWFE（一种仅基于几何的方法）在成功率上将InstructNav从58.0%提升到61.1%，在SPL上从20.9%提升到36.0%。引入SHF后，成功率进一步提升2%，SPL提升0.9%，路径缩短5步。

Conclusion: LLM在ObjectGoalNavigation中的规划能力提升有限，早期研究结果可能夸大了LLM的贡献。几何信息和简单的启发式方法在提升导航成功率和效率方面发挥了关键作用。

Abstract: Large language models (LLMs) are often credited with recent leaps in
ObjectGoal Navigation, yet the extent to which they improve planning remains
unclear. We revisit this question on the HM3D-v1 validation split. First, we
strip InstructNav of its Dynamic Chain-of-Navigation prompt, open-vocabulary
GLEE detector and Intuition saliency map, and replace them with a simple
Distance-Weighted Frontier Explorer (DWFE). This geometry-only heuristic raises
Success from 58.0% to 61.1% and lifts SPL from 20.9% to 36.0% over 2 000
validation episodes, outperforming all previous training-free baselines.
Second, we add a lightweight language prior (SHF); on a 200-episode subset this
yields a further +2% Success and +0.9% SPL while shortening paths by five steps
on average. Qualitative trajectories confirm the trend: InstructNav back-tracks
and times-out, DWFE reaches the goal after a few islands, and SHF follows an
almost straight route. Our results indicate that frontier geometry, not
emergent LLM reasoning, drives most reported gains, and suggest that
metric-aware prompts or offline semantic graphs are necessary before
attributing navigation success to "LLM intelligence."

</details>


### [277] [Digital and Robotic Twinning for Validation of Proximity Operations and Formation Flying](https://arxiv.org/abs/2507.20034)
*Aviad Golan,Gregory Zin,Zahra Ahmed,Emily Bates,Toby Bell,Pol Francesch Huc,Samuel Y. W. Low,Juergen Bosse,Simone D'Amico*

Main category: cs.RO

TL;DR: 该论文提出了一个结合数字和机器人双生技术的框架，用于测试和验证航天器交会、近距离操作和编队飞行中的GNC系统，并通过实验证明了其有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 航天器交会、近距离操作和编队飞行中的GNC系统是安全关键的，但由于空间环境的复杂性，对其进行验证具有挑战性，需要一种能够连接仿真和现实世界行为的验证和确认（V&V）过程。

Method: 该论文提出了一个统一的、端到端的数字和机器人双生框架，用于多模态GNC系统的软件在环和硬件在环测试。该框架包括GRAND、TRON和OS三个测试平台，并对机器人双生的校准和误差进行了表征。

Result: 结果表明，数字双生和机器人双生之间具有一致性，验证了该混合双生管道作为GNC系统真实评估和验证的可靠框架。

Conclusion: 该论文提出的混合数字与机器人双生框架可用于可靠地评估和验证GNC系统。

Abstract: In spacecraft Rendezvous, Proximity Operations (RPO), and Formation Flying
(FF), the Guidance Navigation and Control (GNC) system is safety-critical and
must meet strict performance requirements. However, validating such systems is
challenging due to the complexity of the space environment, necessitating a
verification and validation (V&V) process that bridges simulation and
real-world behavior. The key contribution of this paper is a unified,
end-to-end digital and robotic twinning framework that enables software- and
hardware-in-the-loop testing for multi-modal GNC systems. The robotic twin
includes three testbeds at Stanford's Space Rendezvous Laboratory (SLAB): the
GNSS and Radiofrequency Autonomous Navigation Testbed for Distributed Space
Systems (GRAND) to validate RF-based navigation techniques, and the Testbed for
Rendezvous and Optical Navigation (TRON) and Optical Stimulator (OS) to
validate vision-based methods. The test article for this work is an integrated
multi-modal GNC software stack for RPO and FF developed at SLAB. This paper
introduces the hybrid framework and summarizes calibration and error
characterization for the robotic twin. Then, the GNC stack's performance and
robustness is characterized using the integrated digital and robotic twinning
pipeline for a full-range RPO mission scenario in Low-Earth Orbit (LEO). The
results shown in the paper demonstrate consistency between digital and robotic
twins, validating the hybrid twinning pipeline as a reliable framework for
realistic assessment and verification of GNC systems.

</details>


### [278] [A real-time full-chain wearable sensor-based musculoskeletal simulation: an OpenSim-ROS Integration](https://arxiv.org/abs/2507.20049)
*Frederico Belmonte Klein,Zhaoyuan Wan,Huawei Wang,Ruoli Wang*

Main category: cs.RO

TL;DR: 提出一个集成的、实时的肌肉骨骼建模与仿真框架，利用OpenSimRT、ROS和可穿戴传感器，可用于人体运动分析，并有潜力推动康复、机器人和外骨骼设计的发展。


<details>
  <summary>Details</summary>
Motivation: 当前肌肉骨骼建模和仿真的广泛应用受到成本高昂的传感器、基于实验室的设置、计算要求高的过程以及缺乏无缝集成的各种软件工具的限制。

Method: 提出一个集成的、实时的肌肉骨骼建模与仿真框架，该框架利用OpenSimRT、机器人操作系统（ROS）和可穿戴传感器。

Result: 该框架能够合理地描述下半身和上半身的逆运动学，能够有效地估计踝关节的逆动力学和主要下肢肌肉的肌肉激活，尤其是在结合压力鞋垫的情况下，可以用于诸如行走、蹲起、坐站和站坐等日常活动。

Conclusion: 该框架为更复杂、基于可穿戴传感器的实时人体运动分析系统奠定了基础，并在康复、机器人和外骨骼设计领域具有发展潜力。

Abstract: Musculoskeletal modeling and simulations enable the accurate description and
analysis of the movement of biological systems with applications such as
rehabilitation assessment, prosthesis, and exoskeleton design. However, the
widespread usage of these techniques is limited by costly sensors,
laboratory-based setups, computationally demanding processes, and the use of
diverse software tools that often lack seamless integration. In this work, we
address these limitations by proposing an integrated, real-time framework for
musculoskeletal modeling and simulations that leverages OpenSimRT, the robotics
operating system (ROS), and wearable sensors. As a proof-of-concept, we
demonstrate that this framework can reasonably well describe inverse kinematics
of both lower and upper body using either inertial measurement units or
fiducial markers. Additionally, we show that it can effectively estimate
inverse dynamics of the ankle joint and muscle activations of major lower limb
muscles during daily activities, including walking, squatting and sit to stand,
stand to sit when combined with pressure insoles. We believe this work lays the
groundwork for further studies with more complex real-time and wearable
sensor-based human movement analysis systems and holds potential to advance
technologies in rehabilitation, robotics and exoskeleton designs.

</details>


### [279] [Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots](https://arxiv.org/abs/2507.20217)
*Wei Cui,Haoyu Wang,Wenkang Qin,Yijie Guo,Gang Han,Wen Zhao,Jiahang Cao,Zhang Zhang,Jiaru Zhong,Jingkai Sun,Pihai Sun,Shuai Shi,Botuo Jiang,Jiahao Ma,Jiaxu Wang,Hao Cheng,Zhichao Liu,Yang Wang,Zheng Zhu,Guan Huang,Jian Tang,Qiang Zhang*

Main category: cs.RO

TL;DR: 该研究提出了一种名为 Humanoid Occupancy 的多模态占据感知系统，用于人形机器人。该系统通过融合多模态信息，能够提供详细的环境表示，并解决了人形机器人在感知方面的独特挑战。此外，该研究还发布了一个专门的人形机器人占据数据集，为该领域的研究提供了支持。


<details>
  <summary>Details</summary>
Motivation: 鉴于人形机器人技术和多样化的感知模块的快速发展，而占据表示被认为是理解环境的关键，因此有必要开发一个能够提供丰富语义和 3D 几何信息的通用多模态占据感知系统。

Method: 该框架采用了多模态融合技术，整合了硬件和软件组件、数据采集设备和一个专门的注释流程。网络架构融合了多模态特征融合和时间信息集成，以确保鲁棒的感知。此外，还开发了第一个专门针对人形机器人的全景占据数据集，并解决了运动学干扰和遮挡等问题，同时还建立了有效的传感器布局策略。

Result: 该系统能够为下游任务（如任务规划和导航）实现整体环境理解，克服了运动学干扰和遮挡等挑战，并提供了一个专门针对人形机器人的全景占据数据集，为该领域的未来研究和开发提供了基准和资源。

Conclusion: Humanoid Occupancy 提供了一种通用的、多模态的占据感知系统，该系统集成了硬件和软件组件、数据采集设备和一个专门的注释流程。该框架采用先进的多模态融合技术来生成基于网格的占据输出，该输出编码了占据状态和语义标签，从而能够为任务规划和导航等下游任务实现整体环境理解。为了解决人形机器人的独特挑战，我们克服了运动学干扰和遮挡等问题，并建立了有效的传感器布局策略。此外，我们还开发了第一个专门针对人形机器人的全景占据数据集，为该领域的未来研究和开发提供了宝贵的基准和资源。网络架构融合了多模态特征融合和时间信息集成，以确保鲁棒的感知。总的来说，Humanoid Occupancy 为人形机器人提供了有效 Thus, this work establishes a technical foundation for standardizing universal visual modules, paving the way for the widespread deployment of humanoid robots in complex real-world scenarios.

Abstract: Humanoid robot technology is advancing rapidly, with manufacturers
introducing diverse heterogeneous visual perception modules tailored to
specific scenarios. Among various perception paradigms, occupancy-based
representation has become widely recognized as particularly suitable for
humanoid robots, as it provides both rich semantic and 3D geometric information
essential for comprehensive environmental understanding. In this work, we
present Humanoid Occupancy, a generalized multimodal occupancy perception
system that integrates hardware and software components, data acquisition
devices, and a dedicated annotation pipeline. Our framework employs advanced
multi-modal fusion techniques to generate grid-based occupancy outputs encoding
both occupancy status and semantic labels, thereby enabling holistic
environmental understanding for downstream tasks such as task planning and
navigation. To address the unique challenges of humanoid robots, we overcome
issues such as kinematic interference and occlusion, and establish an effective
sensor layout strategy. Furthermore, we have developed the first panoramic
occupancy dataset specifically for humanoid robots, offering a valuable
benchmark and resource for future research and development in this domain. The
network architecture incorporates multi-modal feature fusion and temporal
information integration to ensure robust perception. Overall, Humanoid
Occupancy delivers effective environmental perception for humanoid robots and
establishes a technical foundation for standardizing universal visual modules,
paving the way for the widespread deployment of humanoid robots in complex
real-world scenarios.

</details>


### [280] [Tactile-Guided Robotic Ultrasound: Mapping Preplanned Scan Paths for Intercostal Imaging](https://arxiv.org/abs/2507.20282)
*Yifan Zhang,Dianye Huang,Nassir Navab,Zhongliang Jiang*

Main category: cs.RO

TL;DR: 本研究利用触觉和机器人技术，为肋间超声成像开发了一种新的扫描路径生成方法，克服了传统方法的局限性，并提高了成像精度。


<details>
  <summary>Details</summary>
Motivation: 为了解决医学超声成像中操作者间和操作者内变异性问题，以及机器人超声系统在具有挑战性的肋间成像应用中的局限性，本研究旨在探索利用触觉线索来表征皮下肋骨结构，以此作为超声分割无关的骨表面点云提取的替代信号。

Method: 本研究利用机器人跟踪数据和触觉信号，通过扫描肋骨生成稀疏触觉点云，然后进行插值细化，并将其与基于图像的密集骨表面点云进行配准，以实现准确的扫描路径映射。此外，还提出了一种自动倾斜角度调整方法，以确保在骨骼下方结构的完整可视化。

Result: 该方法在四个不同的模型上进行了实验验证，最终的扫描路径映射实现了3.41毫米的最近邻点平均距离（MNND）和3.65毫米的豪斯多夫距离（HD）。在肋骨下方的重建物体方面，与CT真值相比，该方法的误差分别为0.69毫米和2.2毫米。

Conclusion: 该研究提出了一种利用触觉线索为超声引导下的肋间成像生成扫描路径的新方法，该方法通过提取皮下肋骨结构的三维信息，有效解决了传统二维超声在肋间成像中遇到的挑战，并取得了较高的精度。

Abstract: Medical ultrasound (US) imaging is widely used in clinical examinations due
to its portability, real-time capability, and radiation-free nature. To address
inter- and intra-operator variability, robotic ultrasound systems have gained
increasing attention. However, their application in challenging intercostal
imaging remains limited due to the lack of an effective scan path generation
method within the constrained acoustic window. To overcome this challenge, we
explore the potential of tactile cues for characterizing subcutaneous rib
structures as an alternative signal for ultrasound segmentation-free bone
surface point cloud extraction. Compared to 2D US images, 1D tactile-related
signals offer higher processing efficiency and are less susceptible to acoustic
noise and artifacts. By leveraging robotic tracking data, a sparse tactile
point cloud is generated through a few scans along the rib, mimicking human
palpation. To robustly map the scanning trajectory into the intercostal space,
the sparse tactile bone location point cloud is first interpolated to form a
denser representation. This refined point cloud is then registered to an
image-based dense bone surface point cloud, enabling accurate scan path mapping
for individual patients. Additionally, to ensure full coverage of the object of
interest, we introduce an automated tilt angle adjustment method to visualize
structures beneath the bone. To validate the proposed method, we conducted
comprehensive experiments on four distinct phantoms. The final scanning
waypoint mapping achieved Mean Nearest Neighbor Distance (MNND) and Hausdorff
distance (HD) errors of 3.41 mm and 3.65 mm, respectively, while the
reconstructed object beneath the bone had errors of 0.69 mm and 2.2 mm compared
to the CT ground truth.

</details>


### [281] [Decentralized Uncertainty-Aware Multi-Agent Collision Avoidance With Model Predictive Path Integral](https://arxiv.org/abs/2507.20293)
*Stepan Dergachev,Konstantin Yakovlev*

Main category: cs.RO

TL;DR: 提出了一种结合 MPPI 和概率 ORCA 的新方法，用于在不确定性下进行去中心化多智能体导航，通过 SOCP 约束确保安全高效的导航。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化多智能体在不确定性下的导航问题，该问题需要考虑运动学约束以及传感和动作执行噪声的避碰策略。

Method: 将模型预测路径积分（MPPI）与最优互惠碰撞避免（ORCA）的概率适应相结合，并通过二阶锥规划（SOCP）将概率安全约束整合到 MPPI 采样过程中。

Result: 在模拟中，该方法在各种场景下均表现出色，包括拥挤的环境，并成功在 Gazebo 模拟器中进行了验证。

Conclusion: 该方法在密集环境中实现了高成功率，优于 ORCA-DD 和 B-UAVC 等现有方法，并通过 Gazebo 模拟验证了其在机器人平台上的实用性。

Abstract: Decentralized multi-agent navigation under uncertainty is a complex task that
arises in numerous robotic applications. It requires collision avoidance
strategies that account for both kinematic constraints, sensing and action
execution noise. In this paper, we propose a novel approach that integrates the
Model Predictive Path Integral (MPPI) with a probabilistic adaptation of
Optimal Reciprocal Collision Avoidance. Our method ensures safe and efficient
multi-agent navigation by incorporating probabilistic safety constraints
directly into the MPPI sampling process via a Second-Order Cone Programming
formulation. This approach enables agents to operate independently using local
noisy observations while maintaining safety guarantees. We validate our
algorithm through extensive simulations with differential-drive robots and
benchmark it against state-of-the-art methods, including ORCA-DD and B-UAVC.
Results demonstrate that our approach outperforms them while achieving high
success rates, even in densely populated environments. Additionally, validation
in the Gazebo simulator confirms its practical applicability to robotic
platforms.

</details>


### [282] [Advancing Shared and Multi-Agent Autonomy in Underwater Missions: Integrating Knowledge Graphs and Retrieval-Augmented Generation](https://arxiv.org/abs/2507.20370)
*Michele Grimaldi,Carlo Cernicchiaro,Sebastian Realpe Rua,Alaaeddine El-Masri-El-Chaarani,Markus Buchholz,Loizos Michael,Pere Ridao Rodriguez,Ignacio Carlucho,Yvan R. Petillot*

Main category: cs.RO

TL;DR: 本研究利用结合了知识图谱和RAG的大型语言模型，成功实现了水下多机器人系统的自主性和共享自主性，并保证了100%的任务完成率。研究强调了结构化知识对于减少模型幻觉和确保决策准确性的重要性。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于应对水下环境中机器人操作所面临的挑战，例如能见度低、水流不可预测和通信受限。为了克服这些挑战，需要先进的自主能力，同时确保操作员的信任和监督。因此，研究重点是利用知识图谱和RAG系统来处理和解释复杂环境数据，从而使机器人能够有效适应和响应不断变化的情况。

Method: 本研究采用检索增强生成（RAG）技术，并结合知识图谱数据和领域分类法来增强大型语言模型。通过这种方式，可以实现多智能体自主决策和无缝的人机交互。

Result: 研究成功展示了多智能体自主性和共享自主性，实现了100%的任务验证和行为完整性。结果还表明，缺乏知识图谱或领域分类法的结构化知识会导致语言模型产生幻觉，从而影响决策质量。

Conclusion: 该研究展示了由机器人驱动的海洋操作，并通过利用知识图谱和检索增强生成（RAG）的语言模型，实现了多智能体和共享自主性。研究表明，结合知识图谱和领域分类法可以提高自主决策能力并减少幻觉，最终实现100%的任务验证和行为完整性。

Abstract: Robotic platforms have become essential for marine operations by providing
regular and continuous access to offshore assets, such as underwater
infrastructure inspection, environmental monitoring, and resource exploration.
However, the complex and dynamic nature of underwater environments,
characterized by limited visibility, unpredictable currents, and communication
constraints, presents significant challenges that demand advanced autonomy
while ensuring operator trust and oversight. Central to addressing these
challenges are knowledge representation and reasoning techniques, particularly
knowledge graphs and retrieval-augmented generation (RAG) systems, that enable
robots to efficiently structure, retrieve, and interpret complex environmental
data. These capabilities empower robotic agents to reason, adapt, and respond
effectively to changing conditions. The primary goal of this work is to
demonstrate both multi-agent autonomy and shared autonomy, where multiple
robotic agents operate independently while remaining connected to a human
supervisor. We show how a RAG-powered large language model, augmented with
knowledge graph data and domain taxonomy, enables autonomous multi-agent
decision-making and facilitates seamless human-robot interaction, resulting in
100\% mission validation and behavior completeness. Finally, ablation studies
reveal that without structured knowledge from the graph and/or taxonomy, the
LLM is prone to hallucinations, which can compromise decision quality.

</details>


### [283] [Bipedalism for Quadrupedal Robots: Versatile Loco-Manipulation through Risk-Adaptive Reinforcement Learning](https://arxiv.org/abs/2507.20382)
*Yuyou Zhang,Radu Corcodel,Ding Zhao*

Main category: cs.RO

TL;DR: 通过双足行走和风险自适应强化学习，使四足机器人能够灵活操作，并在真实世界中成功完成多种任务。


<details>
  <summary>Details</summary>
Motivation: 为了解决四足机器人使用腿部进行操作会影响其运动能力，以及增加机械臂会使系统复杂化的问题，引入了双足站立行走，从而解放前腿以进行与环境的交互。

Method: 提出了一种风险自适应分布强化学习框架，用于四足机器人在后腿站立行走。该框架根据回报分布的不确定性（以回报分布变异系数衡量）动态调整风险偏好，以平衡最坏情况下的保守性和在此不稳定性任务中的最佳性能。

Result: 通过在模拟环境中进行的大量实验，证明了该方法优于基线方法。在真实的Unitree Go2机器人上的部署进一步证明了该策略的多功能性，能够执行推车、探测障碍物和运输有效载荷等任务，同时展示了其在应对挑战性动态和外部干扰方面的鲁棒性。

Conclusion: 该研究成功地在Unitree Go2机器人上部署了双足行走的四足机器人策略，实现了推车、探测障碍物和运输有效载荷等多种任务，并展示了其在动态和外部干扰下的鲁棒性。

Abstract: Loco-manipulation of quadrupedal robots has broadened robotic applications,
but using legs as manipulators often compromises locomotion, while mounting
arms complicates the system. To mitigate this issue, we introduce bipedalism
for quadrupedal robots, thus freeing the front legs for versatile interactions
with the environment. We propose a risk-adaptive distributional Reinforcement
Learning (RL) framework designed for quadrupedal robots walking on their hind
legs, balancing worst-case conservativeness with optimal performance in this
inherently unstable task. During training, the adaptive risk preference is
dynamically adjusted based on the uncertainty of the return, measured by the
coefficient of variation of the estimated return distribution. Extensive
experiments in simulation show our method's superior performance over
baselines. Real-world deployment on a Unitree Go2 robot further demonstrates
the versatility of our policy, enabling tasks like cart pushing, obstacle
probing, and payload transport, while showcasing robustness against challenging
dynamics and external disturbances.

</details>


### [284] [Model-Structured Neural Networks to Control the Steering Dynamics of Autonomous Race Cars](https://arxiv.org/abs/2507.20427)
*Mattia Piccinini,Aniello Mungiello,Georg Jank,Gastone Pietro Rosati Papini,Francesco Biral,Johannes Betz*

Main category: cs.RO

TL;DR: This paper introduces MS-NN-steer, a novel neural network architecture for autonomous race car steering that incorporates vehicle dynamics knowledge. It outperforms existing methods in accuracy and generalization, especially with limited data, and is validated on real-world racing data. The implementation is open-source.


<details>
  <summary>Details</summary>
Motivation: The black-box nature of Deep Learning models, commonly used in autonomous driving, poses a challenge in autonomous racing due to safety and robustness requirements that demand a thorough understanding of decision-making algorithms.

Method: The paper proposes MS-NN-steer, a Model-Structured Neural Network for vehicle steering control. This approach integrates prior knowledge of nonlinear vehicle dynamics into the neural network architecture.

Result: MS-NN-steer demonstrates superior accuracy and generalization capabilities, particularly with limited training data, and exhibits reduced sensitivity to weight initialization when compared to general-purpose NNs. Furthermore, it surpasses the performance of the steering controller employed by the winning team of the A2RL.

Conclusion: MS-NN-steer in Network Architecture, which integrates prior knowledge of nonlinear vehicle dynamics, has been validated with real-world data from the Abu Dhabi Autonomous Racing League (A2RL) competition using full-scale autonomous race cars. Compared to general-purpose NNs, MS-NN-steer achieves better accuracy and generalization with small training datasets and is less sensitive to weight initialization. It also outperforms the steering controller used by the A2RL winning team. The implementation is open-source.

Abstract: Autonomous racing has gained increasing attention in recent years, as a safe
environment to accelerate the development of motion planning and control
methods for autonomous driving. Deep learning models, predominantly based on
neural networks (NNs), have demonstrated significant potential in modeling the
vehicle dynamics and in performing various tasks in autonomous driving.
However, their black-box nature is critical in the context of autonomous
racing, where safety and robustness demand a thorough understanding of the
decision-making algorithms. To address this challenge, this paper proposes
MS-NN-steer, a new Model-Structured Neural Network for vehicle steering
control, integrating the prior knowledge of the nonlinear vehicle dynamics into
the neural architecture. The proposed controller is validated using real-world
data from the Abu Dhabi Autonomous Racing League (A2RL) competition, with
full-scale autonomous race cars. In comparison with general-purpose NNs,
MS-NN-steer is shown to achieve better accuracy and generalization with small
training datasets, while being less sensitive to the weights' initialization.
Also, MS-NN-steer outperforms the steering controller used by the A2RL winning
team. Our implementation is available open-source in a GitHub repository.

</details>


### [285] [Learning Physical Interaction Skills from Human Demonstrations](https://arxiv.org/abs/2507.20445)
*Tianyu Li,Hengbo Ma,Sehoon Ha,Kwonjoon Lee*

Main category: cs.RO

TL;DR: 本研究提出了一种名为BuddyImitation的框架，该框架能够让具有不同形态的智能体直接从人类演示中学习全身交互行为，解决了现有方法在泛化能力上的局限性。


<details>
  <summary>Details</summary>
Motivation: 学习物理交互技能（如跳舞、握手或搏击）对在人类环境中运行的智能体来说，仍然是一个根本性的挑战，特别是当智能体的形态与示范者的形态差异很大时。现有的方法常常依赖于手工制作的目标或形态相似性，这限制了它们泛化的能力。

Method: 本框架提取了一个紧凑、可转移的交互动力学表示，称为嵌入式交互图（EIG），它捕捉了交互智能体之间的关键时空关系。然后，该图被用作模仿目标，在基于物理的模拟中训练控制策略。

Result: 我们展示了BuddyImitation在多种智能体（如人类、具有操纵器的四足机器人或移动操纵器）和各种交互场景（包括搏击、握手、石头剪刀布或跳舞）中的应用。我们的结果表明，通过跨体态交互学习来实现具有形态差异的角色的协调行为，是一个有前景的方向。

Conclusion: 该框架为具有不同形态的智能体通过跨体态交互学习来实现协调行为提供了一条有前景的道路。

Abstract: Learning physical interaction skills, such as dancing, handshaking, or
sparring, remains a fundamental challenge for agents operating in human
environments, particularly when the agent's morphology differs significantly
from that of the demonstrator. Existing approaches often rely on handcrafted
objectives or morphological similarity, limiting their capacity for
generalization. Here, we introduce a framework that enables agents with diverse
embodiments to learn wholebbody interaction behaviors directly from human
demonstrations. The framework extracts a compact, transferable representation
of interaction dynamics, called the Embedded Interaction Graph (EIG), which
captures key spatiotemporal relationships between the interacting agents. This
graph is then used as an imitation objective to train control policies in
physics-based simulations, allowing the agent to generate motions that are both
semantically meaningful and physically feasible. We demonstrate BuddyImitation
on multiple agents, such as humans, quadrupedal robots with manipulators, or
mobile manipulators and various interaction scenarios, including sparring,
handshaking, rock-paper-scissors, or dancing. Our results demonstrate a
promising path toward coordinated behaviors across morphologically distinct
characters via cross embodiment interaction learning.

</details>


### [286] [LLMs-guided adaptive compensator: Bringing Adaptivity to Automatic Control Systems with Large Language Models](https://arxiv.org/abs/2507.20509)
*Zhongchao Zhou,Yuxi Lu,Yaonan Zhu,Yifan Zhao,Bin He,Liang He,Wenwen Yu,Yusuke Iwasawa*

Main category: cs.RO

TL;DR: 本研究提出了一种 LLM 引导的自适应补偿器框架，用于解决自动控制中的自适应控制问题。该方法通过利用 LLM 处理系统差异来设计补偿器，在机器人实验中表现优于传统方法，并简化了设计过程。


<details>
  <summary>Details</summary>
Motivation: 为了进一步研究 LLM 在自动控制中的应用，本工作针对自适应控制这一关键子领域。

Method: 提出了一种受模型参考自适应控制（MRAC）启发的 LLM 引导的自适应补偿器框架，该框架利用 LLM 在未知系统与参考系统之间的差异来设计补偿器，从而实现自适应性，而不是从头开始设计控制器。

Result: 实验结果表明，LLM 引导的自适应补偿器在软体机器人和人形机器人的模拟和真实环境中，优于传统的自适应控制器，并显著降低了推理复杂性。Lyapunov 分析和推理路径检查表明，LLM 引导的自适应补偿器通过将数学推导转化为推理任务，实现了更结构化的设计过程，并表现出很强的泛化性、适应性和鲁棒性。

Conclusion: 该研究为将 LLM 应用于自动控制开辟了新的方向，与视觉-语言模型相比，具有更强的可部署性和实用性。

Abstract: With rapid advances in code generation, reasoning, and problem-solving, Large
Language Models (LLMs) are increasingly applied in robotics. Most existing work
focuses on high-level tasks such as task decomposition. A few studies have
explored the use of LLMs in feedback controller design; however, these efforts
are restricted to overly simplified systems, fixed-structure gain tuning, and
lack real-world validation. To further investigate LLMs in automatic control,
this work targets a key subfield: adaptive control. Inspired by the framework
of model reference adaptive control (MRAC), we propose an LLM-guided adaptive
compensator framework that avoids designing controllers from scratch. Instead,
the LLMs are prompted using the discrepancies between an unknown system and a
reference system to design a compensator that aligns the response of the
unknown system with that of the reference, thereby achieving adaptivity.
Experiments evaluate five methods: LLM-guided adaptive compensator, LLM-guided
adaptive controller, indirect adaptive control, learning-based adaptive
control, and MRAC, on soft and humanoid robots in both simulated and real-world
environments. Results show that the LLM-guided adaptive compensator outperforms
traditional adaptive controllers and significantly reduces reasoning complexity
compared to the LLM-guided adaptive controller. The Lyapunov-based analysis and
reasoning-path inspection demonstrate that the LLM-guided adaptive compensator
enables a more structured design process by transforming mathematical
derivation into a reasoning task, while exhibiting strong generalizability,
adaptability, and robustness. This study opens a new direction for applying
LLMs in the field of automatic control, offering greater deployability and
practicality compared to vision-language models.

</details>


### [287] [Large-Scale LiDAR-Inertial Dataset for Degradation-Robust High-Precision Mapping](https://arxiv.org/abs/2507.20516)
*Xiaofeng Jin,Ningbo Bu,Shijie Wang,Jianfei Ge,Jiangjian Xiao,Matteo Matteucci*

Main category: cs.RO

TL;DR: 本研究提出了一个包含多样化真实世界场景、长轨迹、复杂场景和高精度地面真实数据的大规模、高精度 LiDAR-Inertial Odometry (LIO) 数据集，旨在解决现有研究中 LIO 系统在复杂真实场景验证不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究中 LIO 系统在复杂真实场景验证不足。

Method: 本研究引入了一个大规模、高精度的 LiDAR-Inertial Odometry (LIO) 数据集，旨在解决现有研究中 LIO 系统在复杂真实场景验证不足的问题。该数据集使用定制的背包式平台收集，该平台配备了多光束 LiDAR、工业级 IMU 和 RTK-GNSS 模块，涵盖了四个多样化的真实世界环境，面积从 60,000 平方米到 750,000 平方米不等。数据集包括长轨迹、复杂场景和高精度地面真实数据，通过融合基于 SLAM 的优化和 RTK-GNSS 锚定生成，并通过结合倾斜摄影测量和 RTK-GNSS 对轨迹精度进行了验证。

Result: 该数据集包含长轨迹、复杂场景和高精度地面真实数据，并对轨迹精度进行了验证。

Conclusion: 该数据集为实际高精度测绘场景中 LIO 系统的泛化能力评估提供了全面的基准。

Abstract: This paper introduces a large-scale, high-precision LiDAR-Inertial Odometry
(LIO) dataset, aiming to address the insufficient validation of LIO systems in
complex real-world scenarios in existing research. The dataset covers four
diverse real-world environments spanning 60,000 to 750,000 square meters,
collected using a custom backpack-mounted platform equipped with multi-beam
LiDAR, an industrial-grade IMU, and RTK-GNSS modules. The dataset includes long
trajectories, complex scenes, and high-precision ground truth, generated by
fusing SLAM-based optimization with RTK-GNSS anchoring, and validated for
trajectory accuracy through the integration of oblique photogrammetry and
RTK-GNSS. This dataset provides a comprehensive benchmark for evaluating the
generalization ability of LIO systems in practical high-precision mapping
scenarios.

</details>


### [288] [Uni-Mapper: Unified Mapping Framework for Multi-modal LiDARs in Complex and Dynamic Environments](https://arxiv.org/abs/2507.20538)
*Gilhwan Kang,Hogyun Kim,Byunghee Choi,Seokhwan Jeong,Young-Sik Shin,Younggun Cho*

Main category: cs.RO

TL;DR: Uni-Mapper是一个动态感知3D点云地图合并框架，通过移除动态物体、进行动态感知环路闭合以及多传感器地图合并，实现了跨传感器模式的鲁棒建图和精确的多地图对齐。


<details>
  <summary>Details</summary>
Motivation: 为了实现机器人跨会话和多机器人协作场景下的可扩展操作，需要对不同地图进行统一。然而，在传感器模式和动态环境不一致的情况下，生成可靠的统一地图仍然是一个挑战，因为激光雷达类型的变化和动态元素会导致点云分布和场景一致性的差异，从而影响描述子生成和环路闭合检测的准确性。

Method: 提出了一种名为Uni-Mapper的动态感知3D点云地图合并框架，该框架包含动态物体移除、动态感知环路闭合和多传感器激光雷达地图合并模块。通过时序占用不一致性识别和剔除动态物体，并结合激光雷达全局描述子保留静态局部特征以应对动态环境。最终通过集中的锚点节点策略优化位姿图，以解决内会话和跨地图的环路闭合问题，减少漂移误差。

Result: Uni-Mapper 在真实世界数据集上的评估结果表明，在环路检测、动态环境下的鲁棒建图以及多图对齐方面均优于现有方法。

Conclusion: 该框架在包含动态物体和异构激光雷达的真实世界数据集上进行了评估，在跨传感器模式的环路检测、动态环境中的鲁棒建图以及多图对齐方面均优于现有方法。

Abstract: The unification of disparate maps is crucial for enabling scalable robot
operation across multiple sessions and collaborative multi-robot scenarios.
However, achieving a unified map robust to sensor modalities and dynamic
environments remains a challenging problem. Variations in LiDAR types and
dynamic elements lead to differences in point cloud distribution and scene
consistency, hindering reliable descriptor generation and loop closure
detection essential for accurate map alignment. To address these challenges,
this paper presents Uni-Mapper, a dynamic-aware 3D point cloud map merging
framework for multi-modal LiDAR systems. It comprises dynamic object removal,
dynamic-aware loop closure, and multi-modal LiDAR map merging modules. A
voxel-wise free space hash map is built in a coarse-to-fine manner to identify
and reject dynamic objects via temporal occupancy inconsistencies. The removal
module is integrated with a LiDAR global descriptor, which encodes preserved
static local features to ensure robust place recognition in dynamic
environments. In the final stage, multiple pose graph optimizations are
conducted for both intra-session and inter-map loop closures. We adopt a
centralized anchor-node strategy to mitigate intra-session drift errors during
map merging. In the final stage, centralized anchor-node-based pose graph
optimization is performed to address intra- and inter-map loop closures for
globally consistent map merging. Our framework is evaluated on diverse
real-world datasets with dynamic objects and heterogeneous LiDARs, showing
superior performance in loop detection across sensor modalities, robust mapping
in dynamic environments, and accurate multi-map alignment over existing
methods. Project Page: https://sparolab.github.io/research/uni_mapper.

</details>


### [289] [Methods for the Segmentation of Reticular Structures Using 3D LiDAR Data: A Comparative Evaluation](https://arxiv.org/abs/2507.20589)
*Francisco J. Soler Mora,Adrián Peidró Vidal,Marc Fabregat-Jaén,Luis Payá Castelló,Óscar Reinoso García*

Main category: cs.RO

TL;DR: 本研究提出分析算法和深度学习模型用于分割桁架结构中的可导航表面，以增强爬行机器人的自主导航能力。结果表明，分析算法易于调优且性能接近深度学习模型，而深度学习模型（特别是PointTransformerV3）在精度上更胜一筹。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在通过图像进行故障检测或机器人平台设计，而机器人或爬行器在这些结构中的自主导航探索较少。本研究旨在解决这一空白，提出检测桁架结构中可导航表面的方法，以增强爬行机器人的自主性。

Method: 本文提出多种方法，用于从金属桁架的3D点云中检测可导航表面与背景的二元分割。这些方法包括：1. 分析方法：通过分析点云中平面块的特征分解的特制算法。2. 深度学习方法：训练和评估了PointNet、PointNet++、MinkUNet34C和PointTransformerV3模型。

Result: 分析算法在参数调整方面更简便，且性能可与深度学习模型媲美。深度学习模型虽然计算量更大，但在分割精度上表现更优，其中PointTransformerV3的平均交并比（mIoU）约为97%。

Conclusion: 该研究展示了分析方法和深度学习方法在改善复杂桁架环境中自主导航方面的潜力。结果突显了计算效率和分割性能之间的权衡，为自主基础设施检查和维护的未来研究和实际应用提供了宝贵的指导。

Abstract: Reticular structures form the backbone of major infrastructure like bridges,
pylons, and airports, but their inspection and maintenance are costly and
hazardous, often requiring human intervention. While prior research has focused
on fault detection via images or robotic platform design, the autonomous
navigation of robots within these structures is less explored. This study
addresses that gap by proposing methods to detect navigable surfaces in truss
structures, enhancing the autonomy of climbing robots. The paper introduces
several approaches for binary segmentation of navigable surfaces versus
background from 3D point clouds of metallic trusses. These methods fall into
two categories: analytical algorithms and deep learning models. The analytical
approach features a custom algorithm that segments structures by analyzing the
eigendecomposition of planar patches in the point cloud. In parallel, advanced
deep learning models PointNet, PointNet++, MinkUNet34C, and PointTransformerV3
are trained and evaluated for the same task. Comparative analysis shows that
the analytical algorithm offers easier parameter tuning and performance
comparable to deep learning models, which, while more computationally
intensive, excel in segmentation accuracy. Notably, PointTransformerV3 achieves
a Mean Intersection Over Union (mIoU) of about 97%. The study demonstrates the
promise of both analytical and deep learning methods for improving autonomous
navigation in complex truss environments. The results highlight the trade-offs
between computational efficiency and segmentation performance, providing
valuable guidance for future research and practical applications in autonomous
infrastructure inspection and maintenance.

</details>


### [290] [FMimic: Foundation Models are Fine-grained Action Learners from Human Videos](https://arxiv.org/abs/2507.20622)
*Guangyan Chen,Meiling Wang,Te Cui,Yao Mu,Haoyang Lu,Zicai Peng,Mengxiao Hu,Tianxing Zhou,Mengyin Fu,Yi Yang,Yufeng Yue*

Main category: cs.RO

TL;DR: FMimic通过利用基础模型直接学习细粒度的可泛化技能，克服了现有视觉模仿学习方法的局限性，并在多项基准测试中取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用基础模型来学习高级计划，并依赖预定义的运动图元进行物理交互，这限制了机器人系统的能力。本研究旨在解决这一瓶颈，直接学习细粒度的可泛化技能。

Method: FMimic利用视觉语言模型（VLM）来学习细粒度的动作，而无需预定义的运动图元，仅使用有限数量的人类视频。

Result: FMimic在仅使用一个人类视频的情况下表现强劲，并且在五个视频的情况下显著优于所有其他方法。此外，该方法在RLBench多任务实验和现实操作任务中分别提高了39%和29%以上，在高精度任务和长周期任务中分别提高了34%和47%以上。

Conclusion: FMimic可以直接从少量人类视频中学习细粒度的、可泛化的技能，并在多任务和现实操作任务中表现出色，特别是在高精度和长周期任务中。

Abstract: Visual imitation learning (VIL) provides an efficient and intuitive strategy
for robotic systems to acquire novel skills. Recent advancements in foundation
models, particularly Vision Language Models (VLMs), have demonstrated
remarkable capabilities in visual and linguistic reasoning for VIL tasks.
Despite this progress, existing approaches primarily utilize these models for
learning high-level plans from human demonstrations, relying on pre-defined
motion primitives for executing physical interactions, which remains a major
bottleneck for robotic systems. In this work, we present FMimic, a novel
paradigm that harnesses foundation models to directly learn generalizable
skills at even fine-grained action levels, using only a limited number of human
videos. Extensive experiments demonstrate that our FMimic delivers strong
performance with a single human video, and significantly outperforms all other
methods with five videos. Furthermore, our method exhibits significant
improvements of over 39% and 29% in RLBench multi-task experiments and
real-world manipulation tasks, respectively, and exceeds baselines by more than
34% in high-precision tasks and 47% in long-horizon tasks.

</details>


### [291] [A Strawberry Harvesting Tool with Minimal Footprint](https://arxiv.org/abs/2507.20784)
*Mohamed Sorour,Mohamed Heshmat,Khaled Elgeneidy,Pål Johan From*

Main category: cs.RO

TL;DR: A laser-based strawberry harvester sterilizes the stem wound to extend shelf life, achieving fast harvesting times.


<details>
  <summary>Details</summary>
Motivation: To develop a novel prototype for harvesting table-top grown strawberries with a focus on minimizing footprint and preserving fruit quality.

Method: A laser beam is focused on a precise groove location where the strawberry stem is manipulated. The laser reaches temperatures up to 188°C to kill germs and prevent disease spread, while the burnt stem wound preserves water content and extends fruit shelf life. Experiments were conducted to optimize laser spot diameter and lateral speed against cutting time.

Result: Successful in-door harvesting demonstration with cycle and cut times of 5.56 and 2.88 seconds respectively. Optimization experiments for laser parameters were performed.

Conclusion: The paper presents a novel prototype for harvesting table-top grown strawberries using a laser to cut the stem, which also sterilizes the wound and extends shelf life. The system achieved cycle and cut times of 5.56 and 2.88 seconds respectively.

Abstract: In this paper, a novel prototype for harvesting table-top grown strawberries
is presented, that is minimalist in its footprint interacting with the fruit.
In our methodology, a smooth trapper manipulates the stem into a precise groove
location at which a distant laser beam is focused. The tool reaches
temperatures as high as 188{\deg} Celsius and as such killing germs and
preventing the spread of local plant diseases. The burnt stem wound preserves
water content and in turn the fruit shelf life. Cycle and cut times achieved
are 5.56 and 2.88 seconds respectively in successful in-door harvesting
demonstration. Extensive experiments are performed to optimize the laser spot
diameter and lateral speed against the cutting time.

</details>


### [292] [LanternNet: A Novel Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations](https://arxiv.org/abs/2507.20800)
*Vinil Polepalli*

Main category: cs.RO

TL;DR: LanternNet是一种创新的机器人系统，利用AI和计算机视觉来检测和控制SLF，比传统方法更有效、更具成本效益和可扩展性。


<details>
  <summary>Details</summary>
Motivation: SLF是一种具有显著威胁的入侵性害虫，但传统的控制方法效率低下且具有危害性。

Method: 本研究介绍了一种新颖的自主机器人Hub-and-Spoke系统，称为LanternNet，用于可扩展地检测和抑制SLF种群。一个中心、模仿树木的集线器利用YOLOv8计算机视觉模型进行精确的SLF识别。三个专门的机器人辐射从动轮执行特定的任务：害虫清除、环境监测和导航/绘图。

Result: 在五个星期的实地部署证明了LanternNet的有效性，SLF种群数量显著减少（p < 0.01），树木健康状况得到改善。与传统方法相比，LanternNet具有显著的成本优势和可扩展性。

Conclusion: LanternNet展示了将机器人和人工智能集成用于先进的入侵物种管理和改善环境成果的变革潜力。

Abstract: The invasive spotted lanternfly (SLF) poses a significant threat to
agriculture and ecosystems, causing widespread damage. Current control methods,
such as egg scraping, pesticides, and quarantines, prove labor-intensive,
environmentally hazardous, and inadequate for long-term SLF suppression. This
research introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system
designed for scalable detection and suppression of SLF populations. A central,
tree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF
identification. Three specialized robotic spokes perform targeted tasks: pest
neutralization, environmental monitoring, and navigation/mapping. Field
deployment across multiple infested sites over 5 weeks demonstrated
LanternNet's efficacy. Quantitative analysis revealed significant reductions (p
< 0.01, paired t-tests) in SLF populations and corresponding improvements in
tree health indicators across the majority of test sites. Compared to
conventional methods, LanternNet offers substantial cost advantages and
improved scalability. Furthermore, the system's adaptability for enhanced
autonomy and targeting of other invasive species presents significant potential
for broader ecological impact. LanternNet demonstrates the transformative
potential of integrating robotics and AI for advanced invasive species
management and improved environmental outcomes.

</details>


### [293] [Hanging Around: Cognitive Inspired Reasoning for Reactive Robotics](https://arxiv.org/abs/2507.20832)
*Mihai Pomarlan,Stefano De Giorgis,Rachel Ringe,Maria M. Hedblom,Nikolaos Tsiogkas*

Main category: cs.RO

TL;DR: 一种结合神经网络和符号推理的神经符号机器人架构，能够通过观察学习和规划复杂的任务，例如识别和操作支撑关系中的物体部件。


<details>
  <summary>Details</summary>
Motivation: 情境感知的人工智能代理在自然环境中有效运行面临着空间意识、物体能力检测、动态变化和不可预测性等挑战。一个关键的挑战是代理识别和监控与其目标相关的环境元素的能力。

Method: 该系统结合了执行对象识别的神经网络组件和光学流等图像处理技术，以及符号表示和推理。推理系统通过将图像图式知识整合到本体结构中，并基于具身认知范式进行接地。该本体用于为感知系统创建查询、决定动作以及推断从感知数据派生的实体能力。

Result: 研究在模拟世界中展示了该方法，其中一个代理学会了识别涉及支撑关系的物体部件。该代理起初没有“把手”的概念，但通过观察支撑物体挂在钩子上的例子，它学会了识别建立支撑所涉及的部件，并能够规划支撑关系的建立/破坏。

Conclusion: 该研究提出了一种用于反应式机器人的神经符号模块化架构，结合了神经网络和符号推理，使智能体能够通过观察来系统地扩展其知识，并能够为更复杂的任务进行规划。

Abstract: Situationally-aware artificial agents operating with competence in natural
environments face several challenges: spatial awareness, object affordance
detection, dynamic changes and unpredictability. A critical challenge is the
agent's ability to identify and monitor environmental elements pertinent to its
objectives. Our research introduces a neurosymbolic modular architecture for
reactive robotics. Our system combines a neural component performing object
recognition over the environment and image processing techniques such as
optical flow, with symbolic representation and reasoning. The reasoning system
is grounded in the embodied cognition paradigm, via integrating image schematic
knowledge in an ontological structure. The ontology is operatively used to
create queries for the perception system, decide on actions, and infer
entities' capabilities derived from perceptual data. The combination of
reasoning and image processing allows the agent to focus its perception for
normal operation as well as discover new concepts for parts of objects involved
in particular interactions. The discovered concepts allow the robot to
autonomously acquire training data and adjust its subsymbolic perception to
recognize the parts, as well as making planning for more complex tasks feasible
by focusing search on those relevant object parts. We demonstrate our approach
in a simulated world, in which an agent learns to recognize parts of objects
involved in support relations. While the agent has no concept of handle
initially, by observing examples of supported objects hanging from a hook it
learns to recognize the parts involved in establishing support and becomes able
to plan the establishment/destruction of the support relation. This underscores
the agent's capability to expand its knowledge through observation in a
systematic way, and illustrates the potential of combining deep reasoning
[...].

</details>


### [294] [Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments](https://arxiv.org/abs/2507.20850)
*Meiting Dang,Yanping Wu,Yafei Wang,Dezong Zhao,David Flynn,Chongfeng Wei*

Main category: cs.RO

TL;DR: 本研究提出了一种新的框架，通过结合认知过程建模和风险感知，使自动驾驶汽车能更安全、更高效、更平稳地与行人交互，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在小型移动机器人的群体导航，但由于自动驾驶汽车（AV）和行人之间在决策策略和动态边界方面的固有差异，这些研究无法直接应用于自动驾驶汽车。此外，现有模拟中的行人遵循固定的行为模式，无法动态响应自动驾驶汽车的行动。因此，为了克服这些局限性，需要一个能够模拟自动驾驶汽车与多行人之间真实交互的新框架。

Method: 本研究提出了一种新颖的框架，用于模拟自动驾驶汽车与多行人之间的交互。该框架集成了受自由能原理启发的认知过程建模方法，并将其应用于自动驾驶汽车和行人模型中，以模拟更真实的交互动态。具体而言，提出的行人认知风险社会力模型通过融合认知不确定性和物理风险的度量来调整目标导向力和排斥力，以产生类似人类的轨迹。同时，自动驾驶汽车利用这种融合的风险信息，为其基于图卷积网络和软Actor-Critic架构的系统构建动态的、风险感知的邻接矩阵，从而做出更合理、更明智的决策。

Result: 仿真结果表明，与现有最先进的方法相比，本研究提出的框架能够有效地提高自动驾驶汽车导航的安全性、效率和行驶平稳性。

Conclusion: 该研究提出的框架通过整合认知过程建模，显著提高了自动驾驶汽车（AV）在与行人交互时的安全性、效率和行驶平稳性。

Abstract: Recent advances in autonomous vehicle (AV) behavior planning have shown
impressive social interaction capabilities when interacting with other road
users. However, achieving human-like prediction and decision-making in
interactions with vulnerable road users remains a key challenge in complex
multi-agent interactive environments. Existing research focuses primarily on
crowd navigation for small mobile robots, which cannot be directly applied to
AVs due to inherent differences in their decision-making strategies and dynamic
boundaries. Moreover, pedestrians in these multi-agent simulations follow fixed
behavior patterns that cannot dynamically respond to AV actions. To overcome
these limitations, this paper proposes a novel framework for modeling
interactions between the AV and multiple pedestrians. In this framework, a
cognitive process modeling approach inspired by the Free Energy Principle is
integrated into both the AV and pedestrian models to simulate more realistic
interaction dynamics. Specifically, the proposed pedestrian Cognitive-Risk
Social Force Model adjusts goal-directed and repulsive forces using a fused
measure of cognitive uncertainty and physical risk to produce human-like
trajectories. Meanwhile, the AV leverages this fused risk to construct a
dynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a
Soft Actor-Critic architecture, allowing it to make more reasonable and
informed decisions. Simulation results indicate that our proposed framework
effectively improves safety, efficiency, and smoothness of AV navigation
compared to the state-of-the-art method.

</details>


### [295] [Uncertainty-aware Planning with Inaccurate Models for Robotized Liquid Handling](https://arxiv.org/abs/2507.20861)
*Marco Faroni,Carlo Odesco,Andrea Zanchettin,Paolo Rocco*

Main category: cs.RO

TL;DR: 该研究提出了一种不确定性感知的MCTS算法，以提高机器人任务（如下棋）在不确定性下的准确性，并成功应用于液体倾倒任务。


<details>
  <summary>Details</summary>
Motivation: 物理模拟和基于学习的模型在处理形变物体操纵和液体处理等机器人任务时，常常因认知不确定性或仿真到现实的差距而难以达到精确度，例如，在仅有有限演示的情况下训练的模型在新的情况下可能表现不佳。

Method: 提出了一种不确定性感知的蒙特卡洛树搜索（MCTS）算法，通过纳入模型不确定性估计，使搜索偏向于预测不确定性较低的动作。

Result: 该方法在液体倾倒任务中表现出更高的成功率，尤其是在模型数据量有限的情况下，优于传统方法。

Conclusion: 该方法在不确定条件下提高了规划的可靠性，在液体倾倒任务中，即使模型数据量有限，也显示出比传统方法更高的成功率。

Abstract: Physics-based simulations and learning-based models are vital for complex
robotics tasks like deformable object manipulation and liquid handling.
However, these models often struggle with accuracy due to epistemic uncertainty
or the sim-to-real gap. For instance, accurately pouring liquid from one
container to another poses challenges, particularly when models are trained on
limited demonstrations and may perform poorly in novel situations. This paper
proposes an uncertainty-aware Monte Carlo Tree Search (MCTS) algorithm designed
to mitigate these inaccuracies. By incorporating estimates of model
uncertainty, the proposed MCTS strategy biases the search towards actions with
lower predicted uncertainty. This approach enhances the reliability of planning
under uncertain conditions. Applied to a liquid pouring task, our method
demonstrates improved success rates even with models trained on minimal data,
outperforming traditional methods and showcasing its potential for robust
decision-making in robotics.

</details>


### [296] [A Human-in-the-loop Approach to Robot Action Replanning through LLM Common-Sense Reasoning](https://arxiv.org/abs/2507.20870)
*Elena Merlo,Marta Lagomarsino,Arash Ajoudani*

Main category: cs.RO

TL;DR: 机器人编程的挑战在于需要为非专家提供易于使用的工具。本研究提出了一种结合计算机视觉和大型语言模型（LLM）的方法，通过用户提供的自然语言指令来优化和纠正机器人执行计划，提高了系统的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了促进机器人技术的广泛采用，需要为非专家提供易于使用的编程工具。仅依赖视觉输入在可扩展性和故障缓解方面效率低下，尤其是在仅有一次演示的情况下。

Method: 提出了一种结合单目RGB视频和大型语言模型（LLM）自然语言输入的人机在环方法，以增强机器人执行计划。

Result: 实验证明了该框架的直观性和有效性，能够纠正视觉错误并适应计划，而无需额外演示。此外，交互式计划优化和幻觉校正提高了系统的鲁棒性。

Conclusion: 通过结合视觉信息和大型语言模型的常识推理，该系统能够调整机器人执行计划，以防止潜在的失败并根据用户指令进行适应，而无需额外的演示。

Abstract: To facilitate the wider adoption of robotics, accessible programming tools
are required for non-experts. Observational learning enables intuitive human
skills transfer through hands-on demonstrations, but relying solely on visual
input can be inefficient in terms of scalability and failure mitigation,
especially when based on a single demonstration. This paper presents a
human-in-the-loop method for enhancing the robot execution plan, automatically
generated based on a single RGB video, with natural language input to a Large
Language Model (LLM). By including user-specified goals or critical task
aspects and exploiting the LLM common-sense reasoning, the system adjusts the
vision-based plan to prevent potential failures and adapts it based on the
received instructions. Experiments demonstrated the framework intuitiveness and
effectiveness in correcting vision-derived errors and adapting plans without
requiring additional demonstrations. Moreover, interactive plan refinement and
hallucination corrections promoted system robustness.

</details>


### [297] [PixelNav: Towards Model-based Vision-Only Navigation with Topological Graphs](https://arxiv.org/abs/2507.20892)
*Sergey Bakulin,Timur Akhtyamov,Denis Fatykhov,German Devchich,Gonzalo Ferrer*

Main category: cs.RO

TL;DR: 提出了一种用于移动机器人纯视觉导航的混合方法，结合了深度学习和模型驱动的规划，提高了可解释性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了解决纯数据驱动的端到端模型在实际应用中对大量训练数据和有限可解释性的要求，提出了一种新的混合方法。

Method: 提出了一种分层系统，该系统利用了模型预测控制、可通行性估计、视觉地点识别和姿态估计的最新进展，并采用拓扑图作为目标环境的表示。

Result: 通过结合深度学习方法和经典模型驱动的规划算法，提出了一种新的混合方法，用于移动机器人的纯视觉导航，并进行了广泛的真实世界实验，证明了该方法的效率。

Conclusion: 通过结合深度学习方法和经典模型驱动的规划算法，提出了一种新的混合方法，用于移动机器人的纯视觉导航，该方法具有可解释性强和可扩展性好的优点。

Abstract: This work proposes a novel hybrid approach for vision-only navigation of
mobile robots, which combines advances of both deep learning approaches and
classical model-based planning algorithms. Today, purely data-driven end-to-end
models are dominant solutions to this problem. Despite advantages such as
flexibility and adaptability, the requirement of a large amount of training
data and limited interpretability are the main bottlenecks for their practical
applications. To address these limitations, we propose a hierarchical system
that utilizes recent advances in model predictive control, traversability
estimation, visual place recognition, and pose estimation, employing
topological graphs as a representation of the target environment. Using such a
combination, we provide a scalable system with a higher level of
interpretability compared to end-to-end approaches. Extensive real-world
experiments show the efficiency of the proposed method.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [298] [Towards Multi-Agent Economies: Enhancing the A2A Protocol with Ledger-Anchored Identities and x402 Micropayments for AI Agents](https://arxiv.org/abs/2507.19550)
*Awid Vaziry,Sandro Rodriguez Garzon,Axel Küpper*

Main category: cs.MA

TL;DR: 该研究提出了一种新的多智能体经济体架构，利用DLT和x402标准解决了智能体发现和微支付问题，实现了可信赖的自主经济交互。


<details>
  <summary>Details</summary>
Motivation: 为了解决Agent2Agent（A2A）通信协议在去中心化智能体发现和智能体间微支付方面的两个关键限制，以促进安全、可扩展且经济上可行的多智能体生态系统。

Method: 本研究通过集成分布式账本技术（DLT）实现智能体卡（AgentCards）的链上发布，并扩展A2A通信以支持基于HTTP的微支付（通过HTTP 402状态码）。

Result: 该研究提出了一个包括基于DLT的智能体发现和微支付的综合技术实现和评估，证明了其可行性，为建立可信赖的自主经济交互奠定了基础。

Conclusion: 该研究为多智能体经济体提供了新的架构，通过集成分布式账本技术（DLT）和x402开放标准，解决了Agent2Agent（A2A）通信协议中去中心化智能体发现和智能体间微支付的局限性。

Abstract: This research article presents a novel architecture to empower multi-agent
economies by addressing two critical limitations of the emerging Agent2Agent
(A2A) communication protocol: decentralized agent discoverability and
agent-to-agent micropayments. By integrating distributed ledger technology
(DLT), this architecture enables tamper-proof, on-chain publishing of
AgentCards as smart contracts, providing secure and verifiable agent
identities. The architecture further extends A2A with the x402 open standard,
facilitating blockchain-agnostic, HTTP-based micropayments via the HTTP 402
status code. This enables autonomous agents to seamlessly discover,
authenticate, and compensate each other across organizational boundaries. This
work further presents a comprehensive technical implementation and evaluation,
demonstrating the feasibility of DLT-based agent discovery and micropayments.
The proposed approach lays the groundwork for secure, scalable, and
economically viable multi-agent ecosystems, advancing the field of agentic AI
toward trusted, autonomous economic interactions.

</details>


### [299] [MLC-Agent: Cognitive Model based on Memory-Learning Collaboration in LLM Empowered Agent Simulation Environment](https://arxiv.org/abs/2507.20215)
*Ming Zhang,Yiling Xuan,Qun Ma,Yuwei Guo*

Main category: cs.MA

TL;DR: 本研究提出了一种基于记忆-学习协作机制的个体代理模型，解决了现有模型忽略记忆累积效应的问题，通过分层记忆管理和动态评估，提高了代理的决策质量和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前研究中，个体代理模型的构建常常忽略记忆机制在代理发展过程中的长期累积效应，这在一定程度上导致所构建模型与现实世界系统的真实特征产生偏差。

Method: 提出了一种基于记忆-学习协作机制的个体代理模型，实现了记忆机制的层次化建模和多指标评价机制。通过对个体记忆库、群体记忆库和记忆缓冲池进行层次化建模，有效管理记忆，促进个体与群体之间的知识共享和传播。同时，多指标评价机制能够对记忆信息进行动态评价，实现记忆集中信息的动态更新，促进记忆与学习的协作决策。

Result: 实验结果表明，与现有的记忆建模方法相比，所提模型构建的代理在系统内展现出更好的决策质量和适应性。

Conclusion: 该模型提高了人工社会建模中个体层面建模的质量，并实现了拟人化特征。

Abstract: Many real-world systems, such as transportation systems, ecological systems,
and Internet systems, are complex systems. As an important tool for studying
complex systems, computational experiments can map them into artificial society
models that are computable and reproducible within computers, thereby providing
digital and computational methods for quantitative analysis. In current
research, the construction of individual agent models often ignores the
long-term accumulative effect of memory mechanisms in the development process
of agents, which to some extent causes the constructed models to deviate from
the real characteristics of real-world systems. To address this challenge, this
paper proposes an individual agent model based on a memory-learning
collaboration mechanism, which implements hierarchical modeling of the memory
mechanism and a multi-indicator evaluation mechanism. Through hierarchical
modeling of the individual memory repository, the group memory repository, and
the memory buffer pool, memory can be effectively managed, and knowledge
sharing and dissemination between individuals and groups can be promoted. At
the same time, the multi-indicator evaluation mechanism enables dynamic
evaluation of memory information, allowing dynamic updates of information in
the memory set and promoting collaborative decision-making between memory and
learning. Experimental results show that, compared with existing memory
modeling methods, the agents constructed by the proposed model demonstrate
better decision-making quality and adaptability within the system. This
verifies the effectiveness of the individual agent model based on the
memory-learning collaboration mechanism proposed in this paper in improving the
quality of individual-level modeling in artificial society modeling and
achieving anthropomorphic characteristics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [300] [PennyCoder: Efficient Domain-Specific LLMs for PennyLane-Based Quantum Code Generation](https://arxiv.org/abs/2507.19562)
*Abdul Basit,Minghao Shao,Muhammad Haider Asif,Nouhaila Innan,Muhammad Kashif,Alberto Marchisio,Muhammad Shafique*

Main category: quant-ph

TL;DR: PennyCoder是一个本地化的量子代码生成框架，它微调了LLaMA 3.1-8B模型，通过LoRA和领域特定调优提高了准确性，解决了远程API的隐私、延迟和成本问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的量子代码助手依赖远程API，存在隐私、延迟和成本问题。为了解决这些限制，需要一个能够进行本地部署的量子代码生成框架。

Method: 提出了一种名为PennyCoder的新型轻量级量子代码生成框架，该框架专门设计用于本地和嵌入式部署，无需依赖远程API。PennyCoder采用经过参数高效的低秩适应（LoRA）技术微调的LLaMA 3.1-8B模型，并结合了针对PennyLane的领域特定指令调优。

Result: PennyCoder在全面的量子编程数据集上进行了严格评估，微调后的模型达到了44.3%的准确率，显著优于基础LLaMA 3.1-8B（33.7%）和RAG增强基线（40.1%），证明了其在功能正确性方面的显著提升。

Conclusion: PennyCoder通过参数高效的LoRA技术和针对PennyLane的领域特定指令调优，在一个微调的LLaMA 3.1-8B模型上实现了44.3%的准确率，显著优于基础LLaMA 3.1-8B（33.7%）和RAG增强基线（40.1%），展示了在本地化量子编程辅助方面的有效性。

Abstract: The growing demand for robust quantum programming frameworks has unveiled a
critical limitation: current large language model (LLM) based quantum code
assistants heavily rely on remote APIs, introducing challenges related to
privacy, latency, and excessive usage costs. Addressing this gap, we propose
PennyCoder, a novel lightweight framework for quantum code generation,
explicitly designed for local and embedded deployment to enable on-device
quantum programming assistance without external API dependence. PennyCoder
leverages a fine-tuned version of the LLaMA 3.1-8B model, adapted through
parameter-efficient Low-Rank Adaptation (LoRA) techniques combined with
domain-specific instruction tuning optimized for the specialized syntax and
computational logic of quantum programming in PennyLane, including tasks in
quantum machine learning and quantum reinforcement learning. Unlike prior work
focused on cloud-based quantum code generation, our approach emphasizes
device-native operability while maintaining high model efficacy. We rigorously
evaluated PennyCoder over a comprehensive quantum programming dataset,
achieving 44.3% accuracy with our fine-tuned model (compared to 33.7% for the
base LLaMA 3.1-8B and 40.1% for the RAG-augmented baseline), demonstrating a
significant improvement in functional correctness.

</details>


### [301] [Reply to "Counterfactual communication not achieved yet -- A Comment on Salih et al. (2022)"](https://arxiv.org/abs/2507.19563)
*Hatim Salih,Jonte R. Hance,Will McCutcheon,John Rarity*

Main category: quant-ph

TL;DR: 本文反驳了Popescu对其反事实通信论文的质疑，澄清了其研究的有效性。


<details>
  <summary>Details</summary>
Motivation: 反驳Popescu对其论文“物理定律并不禁止反事实通信”的有效性的质疑。

Method: 通过展示Popescu的论点忽视了论文证明的具体内容（反事实通信对于后选择粒子是可能的，并且不受弱迹或一致历史标准禁止），以及对协议的不必要简化来反驳其观点。

Result: 证明了Popescu的论点是无效的，并重申了其论文的有效性。

Conclusion: 反驳了Popescu关于反事实通信的有效性的论点，指出其论点基于对论文证明的具体内容的忽视，以及对协议不必要的简化。同时，文章也反驳了Popescu对解释的涉足，重申其通信协议是精确定义的，允许双方远程通信任意长的二进制消息，并具有任意高的准确性。

Abstract: In his Comment on our recent paper ``The laws of physics do not prohibit
counterfactual communication'', \textit{npj Quantum Information} (2022) 8:60,
Popescu argues that the claims of the paper are invalid. Here, we refute his
argument, showing that it is based on ignoring the specifics of what we set out
to prove (that counterfactual communication is possible \emph{for post-selected
particles}, and more specifically in these cases is not prohibited by the weak
trace or consistent histories criteria for particle path), followed by an
unwarranted simplification of the protocol. Moreover, the Comment's excursion
into interpretation is misplaced. Our communication protocol is a precisely
defined one that allows two remote parties, albeit rarely, to communicate an
arbitrarily long binary message, with arbitrarily high accuracy. This is not a
matter of interpretation -- as the concrete example given in our paper in
question illustrates. As for our overarching claim that no particles are
exchanged in the course of this communication, we have already demonstrated
this both theoretically and experimentally, in the postselected case we
consider, as per the weak trace and consistent histories criteria for path of a
quantum particle.

</details>


### [302] [Light and divergences: History and outlook](https://arxiv.org/abs/2507.19569)
*Gerd Leuchs,Luis L. Sanchez-Soto*

Main category: quant-ph

TL;DR: The vacuum is quantum, not void. QED has limitations in calculating fundamental constants like alpha due to singularities. This paper reviews progress and presents a model for vacuum polarization.


<details>
  <summary>Details</summary>
Motivation: While quantum electrodynamics (QED) is successful, it remains a perturbative framework with singularities and divergences that prevent precise calculation of fundamental quantities such as the fine-structure constant alpha. This paper aims to explore these challenges and ongoing efforts to address them.

Method: This article reviews the historical development of ideas about the quantum vacuum, the current state of knowledge, and ongoing efforts. It includes a simple model describing vacuum polarization in the low-energy regime, relating Maxwell's displacement in the vacuum to the quantum properties of the vacuum.

Result: The paper reviews the historical development of ideas about the quantum vacuum, the current state of knowledge, and ongoing efforts. It presents a simple model for vacuum polarization in the low-energy regime.

Conclusion: All experimental evidence indicates that the vacuum is not void, but filled with something truly quantum. QED is a perturbative framework that includes singularities and divergences, preventing precise calculation of fundamental quantities like the fine-structure constant alpha. However, we can determine how alpha 'runs' with energy or exchanged momentum.

Abstract: All experimental evidence {indicates} that the vacuum is not void, but filled
with something truly quantum. This is reflected by terms such as {zero-point}
fluctuations, and Dirac's sea of virtual particle-antiparticle pairs, and last
but not least the vacuum is the medium responsible for Maxwell's displacement
current. While quantum electrodynamics (QED) is an exceptionally successful
theory, it remains a perturbative framework rather than a fully self-contained
one. Inherently, it includes singularities and divergences, which prevent the
precise calculation of fundamental quantities such as the fine-structure
constant $\alpha$. Any direct attempt to compute $\alpha$ results in divergent
values. However, and most remarkable, what can be determined is how $\alpha$
``runs", meaning how it varies with energy or exchanged momentum. In this
article, we review the historical development of these ideas, the current state
of knowledge, and ongoing efforts to find ways to move further. This includes a
simple model to describe vacuum polarization in the low-energy regime, when
considering only small (linear) deviations from the equilibrium {state},
relating {Maxwell's displacement} in the vacuum, to the quantum properties of
the vacuum.

</details>


### [303] [Real-Time Observation of Aharonov-Bohm Interference in a $\mathbb{Z}_2$ Lattice Gauge Theory on a Hybrid Qubit-Oscillator Quantum Computer](https://arxiv.org/abs/2507.19588)
*S. Saner,O. Băzăvan,D. J. Webb,G. Araneda,C. J. Ballance,R. Srinivas,D. M. Lucas,A. Bermúdez*

Main category: quant-ph

TL;DR: 使用捕获离子量子设备，通过混合量子比特-振荡器系统，在 $\mathbb{Z}_2$ 格子规范理论中实现了 Aharonov-Bohm 干涉，为模拟强耦合问题和更高维度的格子规范理论提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 为了解决经典计算难以企及的强耦合问题，该研究旨在探索量子模拟格子规范理论的方法，特别是解决规范不变编码和实时演化的实验挑战。

Method: 研究人员在捕获离子量子设备上，利用量子比特和振动模式分别编码规范场和玻色子物质场，结合数字和模拟技术，实现了规范不变的实时演化，并通过合成维度构建了高维格点几何。

Result: 实验中首次在 $\mathbb{Z}_2$ 链和环形几何中探测了满足高斯定律的动力学，并在准二维设置中观察到了编码磁通量的动力学规范场产生的 Aharonov-Bohm 干涉效应。

Conclusion: 该研究展示了一种资源高效的 $\mathbb{Z}_2$ 格子规范理论编码方法，并首次在具有动力学规范场的系统中实现了 Aharonov-Bohm 干涉，为实现更高维度的奇异格子规范理论提供了有前景的路径。

Abstract: Quantum simulations of lattice gauge theories (LGTs) with both dynamical
matter and gauge fields provide a promising approach to studying strongly
coupled problems beyond classical computational reach. Yet, implementing
gauge-invariant encodings and real-time evolution remains experimentally
challenging. Here, we demonstrate a resource-efficient encoding of a
$\mathbb{Z}_2$ LGT using a hybrid qubit-oscillator trapped-ion quantum device,
where qubits represent gauge fields and vibrational modes naturally encode
bosonic matter fields. This architecture utilises synthetic dimensions to
construct higher-dimensional lattice geometries and combines digital and
analogue techniques to prepare initial states, realise gauge-invariant
real-time evolution, and measure the relevant observables. We experimentally
probe dynamics obeying Gauss's law in a $\mathbb{Z}_2$ link and extend this to
a loop geometry, marking the first steps towards higher-dimensional LGTs. In
this quasi-2D setup, we observe Aharonov-Bohm interference for the first time
with dynamical gauge fields encoding magnetic flux, demonstrating the interplay
between charge and flux. Our results chart a promising path for scalable
quantum simulations of bosonic gauge theories and outline a roadmap for
realising exotic LGTs in higher dimensions.

</details>


### [304] [Quantum Reinforcement Learning by Adaptive Non-local Observables](https://arxiv.org/abs/2507.19629)
*Hsin-Yi Lin,Samuel Yen-Chi Chen,Huan-Hsin Tseng,Shinjae Yoo*

Main category: quant-ph

TL;DR: 本文提出了一种新的量子强化学习方法（ANO-VQC），通过优化测量方式来克服现有量子机器学习方法的局限性，并在实验中取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习中的变分量子电路（VQC）受到局部测量的限制，本文旨在通过引入非局部测量来克服这一限制。

Method: 提出了一种自适应非局部可观测（ANO）范式，并将其应用于深度Q网络（DQN）和异步优势Actor-Critic（A3C）算法，以优化量子强化学习中的电路参数和多量子比特测量。

Result:  ANO-VQC代理在多个基准任务上的表现优于基线VQC，并且自适应测量在不增加电路深度的情况下增强了函数空间。

Conclusion: 通过实验证明了自适应多量子比特可观测量的使用能够提高量子强化学习的效率。

Abstract: Hybrid quantum-classical frameworks leverage quantum computing for machine
learning; however, variational quantum circuits (VQCs) are limited by the need
for local measurements. We introduce an adaptive non-local observable (ANO)
paradigm within VQCs for quantum reinforcement learning (QRL), jointly
optimizing circuit parameters and multi-qubit measurements. The ANO-VQC
architecture serves as the function approximator in Deep Q-Network (DQN) and
Asynchronous Advantage Actor-Critic (A3C) algorithms. On multiple benchmark
tasks, ANO-VQC agents outperform baseline VQCs. Ablation studies reveal that
adaptive measurements enhance the function space without increasing circuit
depth. Our results demonstrate that adaptive multi-qubit observables can enable
practical quantum advantages in reinforcement learning.

</details>


### [305] [Quantum Internet Architecture: unlocking Quantum-Native Routing via Quantum Addressing](https://arxiv.org/abs/2507.19655)
*Marcello Caleffi,Angela Sara Cacciapuoti*

Main category: quant-ph

TL;DR: 该论文提出了一种新的量子互联网架构和控制平面，包括量子寻址和路由协议，以实现可扩展和高效的网络操作。


<details>
  <summary>Details</summary>
Motivation: 量子互联网引入了网络设计的根本性变化，其关键目标是分发和操纵量子纠缠，而不是传输经典信息。这种转变打破了关键的经典互联网设计原则，例如端到端原则，因为纠缠态固有的状态性和非局域性需要协同的网络内操作。

Method: 提出了一种新颖的层次化量子互联网架构，其核心是“由纠缠定义的控制器”，以实现对网络内操作的可扩展和高效管理。此外，该论文还提出了一种量子寻址方案，并将量子现象融入节点标识符中。基于此寻址方案，设计了一种量子原生路由协议，该协议在利用面向纠缠的拓扑结构时，能够实现可扩展且紧凑的路由表。最后，设计了一种基于薛定谔预言机的量子地址拆分功能，将经典匹配和转发逻辑推广到量子领域。

Result: 该论文提出了一个完整的量子互联网架构，包括寻址、路由和控制平面，展示了量子网络设计的关键优势。

Conclusion: 该论文首次展示了量子优先设计的网络功能的关键优势。

Abstract: The Quantum Internet introduces a fundamental shift in the network design,
since its key objective is the distribution and manipulation of quantum
entanglement, rather than the transmission of classical information. This shift
breaks key classical Internet design principles, such as the end-to-end
argument, due to the inherently stateful and non-local nature of entangled
states that require coordinated in-network operations. Consequently, in this
paper we propose a novel hierarchical Quantum Internet architecture centered
around the concept of entanglement-defined controller, enabling scalable and
efficient management of the aforementioned in-network operations. However,
architecture alone is insufficient for network scalability, which requires a
quantum-native control plane that fundamentally rethinks addressing and
routing. Consequently, we propose a quantum addressing scheme that embraces the
principles and quantum phenomena within the node identifiers. Built upon this
addressing scheme, we also design a quantum-native routing protocol that
exhibits scalable and compact routing tables, by efficiently operating over
entanglement-aware topologies. Finally, we design a quantum address splitting
functionality based on Schr\"odinger's oracles that generalizes classical
match-and-forward logic to the quantum domain. Together, these contributions
demonstrate, for the first time, the key advantages of quantum-by-design
network functioning.

</details>


### [306] [Quantum-Efficient Convolution through Sparse Matrix Encoding and Low-Depth Inner Product Circuits](https://arxiv.org/abs/2507.19658)
*Mohammad Rasoul Roshanshah,Payman Kazemikhah,Hossein Aghababa*

Main category: quant-ph

TL;DR: 提出一种高效的量子卷积算法，通过稀疏重塑和SWAP测试，降低了成本和复杂度，为量子机器学习开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 在经典图像处理和现代深度学习架构中，卷积运算起着基础性作用，但由于数据编码效率低下和电路复杂度高昂，将其扩展到量子领域一直面临算法和物理成本高昂的挑战。

Method: 本文利用双块Toeplitz矩阵乘法的局部卷积编码，构建了一个量子框架。在该框架中，使用优化的键值QRAM状态编码来准备稀疏输入块，并将卷积滤波器表示为叠加中的量子态。通过低深度SWAP测试电路计算卷积输出，进行内积估计，从而以较低的采样开销获得概率幅信息。该架构还支持使用广义SWAP电路对多个滤波器进行批处理卷积。

Result: 与先前的方法相比，该方法消除了冗余的准备成本，在稀疏性下可随输入大小对数扩展，并可直接集成到混合量子-经典机器学习流程中，为实现量子增强的特征提取提供了可扩展且物理上可实现的途径。

Conclusion: 本文提出了一种资源高效的量子算法，通过新颖的稀疏重塑形式主义将卷积运算重构为结构化矩阵乘法，为量子卷积神经网络和数据驱动的量子推理提供了可扩展且物理上可实现的途径。

Abstract: Convolution operations are foundational to classical image processing and
modern deep learning architectures, yet their extension into the quantum domain
has remained algorithmically and physically costly due to inefficient data
encoding and prohibitive circuit complexity. In this work, we present a
resource-efficient quantum algorithm that reformulates the convolution product
as a structured matrix multiplication via a novel sparse reshaping formalism.
Leveraging the observation that localized convolutions can be encoded as doubly
block-Toeplitz matrix multiplications, we construct a quantum framework wherein
sparse input patches are prepared using optimized key-value QRAM state
encoding, while convolutional filters are represented as quantum states in
superposition. The convolution outputs are computed through inner product
estimation using a low-depth SWAP test circuit, which yields probabilistic
amplitude information with reduced sampling overhead. Our architecture supports
batched convolution across multiple filters using a generalized SWAP circuit.
Compared to prior quantum convolutional approaches, our method eliminates
redundant preparation costs, scales logarithmically with input size under
sparsity, and enables direct integration into hybrid quantum-classical machine
learning pipelines. This work provides a scalable and physically realizable
pathway toward quantum-enhanced feature extraction, opening up new
possibilities for quantum convolutional neural networks and data-driven quantum
inference.

</details>


### [307] [Characterization of Polariton Dynamics in a Multimode Cavity (II): Coherent-Incoherent Transition Driven by Photon Loss](https://arxiv.org/abs/2507.19671)
*Md Qutubuddin,Ilia Tutunnikov,Jianshu Cao*

Main category: quant-ph

TL;DR: 研究了损耗腔中的极化子动力学，发现光子衰减率是控制极化子输运的关键参数，并揭示了其中的对称性。


<details>
  <summary>Details</summary>
Motivation: 受到光学成像和光学腔中波包传播跟踪的最新进展的启发。

Method: 利用衰减可调的多模腔模型，系统地探索了非厄米极化子动力学，并通过分析模型的复杂本征谱来预测由光子损耗引起的非相干-相干跃迁，该跃迁定义了一个共振下的奇异点，并随着波矢量失谐而解析演化。

Result: 研究结果揭示了由光子损耗引起的非相干-相干跃迁，定义了一个奇异点，并展示了诸如曲线交叉、能级排斥、转换、分叉和合并等特征。研究结果还强调了最大净弛豫率、质心运动中相反的传播、弹道-扩散跃迁以及极化子波包位移和宽度收缩等动力学特征，这些特征在由光子衰减率和波矢量构成的二维相图中，在上下极化子分支之间表现出互补的对称性。

Conclusion: 该研究将复杂的光谱表征与非厄米波包动力学相结合，将光子衰减率确定为控制极化子输运的有力参数，揭示了损耗腔中的潜在对称性，并为引入其他耗散机制提供了起点。

Abstract: Motivated by the recent advances in optical imaging and tracking of
wave-packet propagation in optical cavities, we systematically explore the
non-Hermitian polariton dynamics within a decay-tunable multimode cavity model.
The complex eigen-spectrum of the model Hamiltonian allows us to predict the
incoherent-coherent transition induced by photon losses, which defines an
exceptional point at resonance and evolves analytically as the wavevector
shifts off-resonantly. The resulting dispersion relation, group velocity, and
relaxation rate exhibit striking signatures, such as curve crossing, level
repulsion, turnover, bifurcation, and coalescence, as the decay rate crosses
the critical transition or the wavevector crosses the resonance. The spectral
characterization leads to surprising features in the non-Hermitian wave packet
dynamics: (i) maximal population relaxation rate at the critical transition;
(ii) reversed propagation in the center-of-mass motion; (iii)
ballistic-to-diffusion transition; (iv) contraction in the displacement and
width of the polariton wave-packet. These dynamical features have complementary
symmetry between the upper-polariton (UP) branch and lower-polariton (LP)
branch in the two-dimensional phase diagram spanned by the photon decay rate
and wavevector. Thus, the combination of complex spectral characterization and
non-Hermitian wave packet propagation establishes the photon decay rate as a
powerful control parameter for polariton transport, reveals the underlying
symmetry in lossy cavities, and presents a starting point to incorporate other
dissipative mechanisms.

</details>


### [308] [Exponentially robust non-Clifford gate in a driven-dissipative circuit](https://arxiv.org/abs/2507.19713)
*Liam O'Brien,Gil Refael,Frederik Nathan*

Main category: quant-ph

TL;DR: 提出了一种物理量子比特层面的受保护的非Clifford $\sqrt{T}$门协议，该协议具有拓扑鲁棒性，对噪声和控制不完美具有韧性，并在微秒级时间尺度运行。


<details>
  <summary>Details</summary>
Motivation: 实现受保护的非Clifford $\sqrt{T}$门，以克服现有协议在非Clifford门上的限制。

Method: 通过包含由辅助约瑟夫森结产生的四次通量势，实现受保护的非Clifford $\sqrt{T}$门。

Result: 该协议能够实现拓扑鲁棒的受保护的非Clifford $\sqrt{T}$门，对噪声、不完美的控制和电路参数目标具有良好的韧性，并在微秒级时间尺度运行。

Conclusion: 该协议在物理量子比特层面实现了拓扑鲁棒的受保护的非Clifford $\sqrt{T}$门，其控制或器件缺陷导致的失效率呈指数级抑制，并在GHz谐振器上以微秒级时间尺度运行。

Abstract: Recent work (Nathan et al, arXiv:2405.05671) proposed an architecture for a
dissipatively stabilized GKP qubit, and protocols for protected Clifford gates.
Here we propose a protocol for a protected non-Clifford $\sqrt{T}$ gate at the
physical qubit level, based on the inclusion of a quartic flux potential
generated by ancillary Josephson junctions. We show that such a gate is
topologically robust with exponentially suppressed infidelity from control or
device imperfections, and operates on microsecond timescales for GHz
resonators. We analyze the resilience of the protocol to noise, imperfect
control, and imperfect targeting of circuit parameters.

</details>


### [309] [Universal Relation Between Quantum Entanglement and Particle Transport](https://arxiv.org/abs/2507.19731)
*Elvira Bilokon,Valeriia Bilokon,Abhijit Sen,Mohammed Th. Hassan,Andrii Sotnikov,Denys I. Bondar*

Main category: quant-ph

TL;DR: 使用 Kolmogorov-Arnold 网络发现并预测一维费米-哈伯德系统中负熵与粒子数之间的普遍关系。


<details>
  <summary>Details</summary>
Motivation: 探索量子关联的测量方法，并为理解和预测量子系统的纠缠演化提供新途径。

Method: 利用 Kolmogorov-Arnold 网络学习一维费米-哈伯德系统中负熵与粒子数之间的关系，并提出一个解析表达式。

Result: 在存在外加不对称势的情况下，在一维费米-哈伯德系统中，负熵与势垒后的粒子数之间存在普遍关系，并且该关系可以通过 Kolmogorov-Arnold 网络精确预测，同时提出的解析表达式能够定量描述这种相关性。

Conclusion: 本研究揭示了在一维费米-哈伯德系统中，负熵与势垒后的粒子数之间存在普遍关系，并利用 Kolmogorov-Arnold 网络学习了这种关系，同时提出了一个简单的解析表达式来量化这种相关性。

Abstract: Entanglement entropy is a fundamental measure of quantum correlations and a
key resource underpinning advances in quantum information and many-body
physics. We uncover a universal relationship between bipartite entanglement
entropy and particle number after the barrier in a one-dimensional
Fermi-Hubbard system with an external asymmetric potential. Using
Kolmogorov-Arnold Networks - a novel machine learning architecture - we learn
this relationship across a broad range of interaction strengths with
near-perfect predictive accuracy. Furthermore, we propose a simple analytical
binary-entropy-like expression that quantitatively captures the observed
correlation for fixed parameters. Our findings open new avenues for
characterizing quantum correlations in transport phenomena and provide a
powerful framework for predicting entanglement evolution in quantum systems.

</details>


### [310] [Semiclassical radiation spectrum from an electron in an external plane wave field](https://arxiv.org/abs/2507.19776)
*T. C. Adorno,A. J. D. Farias Junior,D. M. Gitman*

Main category: quant-ph

TL;DR: 该论文提出了一种新的半经典方法来分析点粒子在平面波场中产生的电磁能量和能量率谱，解决了经典方法中的发散问题，并成功地将结果与经典电动力学和量子电动力学进行了关联。


<details>
  <summary>Details</summary>
Motivation: 与显示与粒子和外部场相互作用持续时间相关的发散的经典能量谱不同，半经典谱是有限的，因为辐射是在从没有光子的初始状态到有光子的最终状态的量子跃迁过程中产生的。

Method: 该方法基于半经典表述，其中产生电磁辐射的电流分布被经典地处理，而辐射场是量子的。

Result: 研究发现，粒子产生vi的最大能量谱与时间或相位成线性比例关系，具体取决于外部场。这使得研究人员不仅能够提取粒子产生vi的最大能量率谱，而且能够将它们与经典电动力学和量子电动力学框架内推导出的能量率相关联。

Conclusion: 该论文提出了一种半经典方法来研究点粒子在平面波场中产生的电磁能量和能量率谱。该方法将产生辐射的电流分布视为经典，而辐射场视为量子。

Abstract: In this work, we study the electromagnetic energy and energy rate spectra
produced by a point particle in the presence of plane wave fields. Our approach
is based on a semiclassical formulation, in which the current distribution that
generates electromagnetic radiation is treated classically while the radiation
field is quantum. Unlike the classical energy spectrum--which exhibits
divergences linked to the duration of interaction between the particle and the
external field--the semiclassical spectrum is finite because radiation is
produced during the quantum transition from an initial state without photons to
the final state with photons at time $t$. In our formulation, we find that the
maximum energy spectrum emitted by the particle is linearly proportional to
time or phase, depending on the external field. This allowed us not only to
extract the maximum energy rate spectra emitted by the particle but also to
correlate them with energy rates derived in the framework of Classical
Electrodynamics and Quantum Electrodynamics.

</details>


### [311] [Quantum Walks on Arbitrary Spatial Networks with Rydberg Atoms](https://arxiv.org/abs/2507.21011)
*Gabriel Almeida,Raul Santos,Lara Janiurek,Yasser Omar*

Main category: quant-ph

TL;DR: 提出使用里德堡原子实现交错量子行走，并在空间搜索任务中实现二次加速。


<details>
  <summary>Details</summary>
Motivation: 里德堡原子作为量子计算平台，因其可调相互作用、可扩展性、可重构连接性和原生多量子比特门等优点，特别适合解决复杂的网络问题，而量子行走是解决图论问题的有效手段。

Method: 提出了一种使用里德堡原子实现交错量子行走（staggered quantum walks）的通用方法，并重点关注了空间网络，同时提供了一种构建交错量子行走所需的镶嵌（tessellations）的有效算法。

Result: 提出了一种使用里德堡原子实现交错量子行走的方法，并证明了该方法在空间搜索算法中可以实现二次加速。

Conclusion: 该方案在空间搜索算法中实现了二次加速。

Abstract: Rydberg atoms provide a highly promising platform for quantum computation,
leveraging their strong tunable interactions to encode and manipulate
information in the electronic states of individual atoms. Key advantages of
Rydberg atoms include scalability, reconfigurable connectivity, and native
multi-qubit gates, making them particularly well-suited for addressing complex
network problems. These problems can often be framed as graph-based tasks,
which can be efficiently addressed using quantum walks. In this work, we
propose a general implementation of staggered quantum walks with Rydberg atoms,
with a particular focus on spatial networks. We also present an efficient
algorithm for constructing the tessellations required for the staggered quantum
walk. Finally, we demonstrate that our proposal achieves quadratic speedup in
spatial search algorithms.

</details>


### [312] [Fringe visibility and which-way information in Young's double slit experiments with light scattered from single atoms](https://arxiv.org/abs/2507.19801)
*Hanzhen Lin,Yu-Kun Lu,Vitaly Fedoseev,Yoo Kyung Lee,Jiahao Lyu,Wolfgang Ketterle*

Main category: quant-ph

TL;DR: 本文分析了用单原子进行的双缝实验，重点关注了两种记录粒子路径信息的方法，并对其进行了推广。


<details>
  <summary>Details</summary>
Motivation: 在量子力学中，杨氏双缝实验常被用来阐述互补性原理。如果能够获得关于光子路径的信息，干涉条纹的可见度就会降低甚至消失。这种Bohr和Einstein讨论的思想实验，当缝隙被对光子“通过缝隙”的动量转移敏感的单个原子取代时就可以实现。

Method: 对近期用单原子进行的两种实验进行分析和推广，重点分析其记录“哪个路径”信息的不同方式。

Result: 两种实验都利用单原子作为双态系统，并将谐振子势中的激发作为“哪个路径”标记，但记录信息的方式不同。

Conclusion: 本文分析并推广了两种用单原子进行的实验，并强调了它们记录“哪个路径”信息的不同方式。

Abstract: Young's double slit experiment has often been used to illustrate the concept
of complementarity in quantum mechanics. If information can in principle be
obtained about the path of the photon, then the visibility of the interference
fringes is reduced or even destroyed. This Gedanken experiment discussed by
Bohr and Einstein can be realized when the slit is replaced by individual atoms
sensitive to the transferred recoil momentum of a photon which "passes through
the slit". Early pioneering experiments were done with trapped ions and atom
pairs created via photo-dissociation. Recently, it became possible to perform
interference experiments with single neutral atoms cooled to the absolute
ground state of a harmonic oscillator potential. The slits are now single atoms
representing a two-level system, and the excitation in the harmonic oscillator
potential is the which-way marker. In this note, we analyze and generalize two
recent experiments performed with single atoms and emphasize the different ways
they record which-way information.

</details>


### [313] [Quantum-Informed Machine Learning for Chaotic Systems](https://arxiv.org/abs/2507.19861)
*Maida Wang,Xiao Xue,Peter V. Coveney*

Main category: quant-ph

TL;DR: 本研究提出了一种量子启发式机器学习框架，利用量子电路伯恩机来学习混沌系统的统计特性，并将其集成到基于Koopman的自回归模型中，以提高预测精度和长期稳定性。该方法在多个基准测试中均优于经典方法，并为利用近期量子硬件进行动力学系统建模提供了实用的解决方案。


<details>
  <summary>Details</summary>
Motivation: 学习混沌系统的行为仍然具有挑战性，因为长期预测不稳定，并且难以准确捕捉不变的统计特性。量子机器学习为从高维数据中有效捕获物理特性提供了一条有前景的途径，但其部署受到当前硬件噪声和可扩展性限制的阻碍。

Method: 提出了一种量子启发式机器学习框架，用于学习偏微分方程，重点是混沌系统。采用量子电路伯恩机来学习混沌动力学系统的不变性质，通过一组紧凑的可训练电路参数来表示这些复杂的物理统计数据，从而实现显著的内存效率。将结果统计量子启发式先验集成到基于Koopman的自回归模型中，以解决梯度消失或爆炸等问题，同时保持长期的统计保真度。

Result: 该框架在三个代表性系统上进行了评估：Kuramoto-Sivashinsky方程、二维Kolmogorov流和湍流通道流。在所有情况下，与没有量子先验的经典对应模型相比，量子启发式模型都取得了卓越的性能。与原始模拟数据相比，该方法将数据存储需求减少了两个数量级以上。

Conclusion: 该混合架构为使用近期量子硬件学习动力学系统提供了一条实用的途径。

Abstract: Learning the behaviour of chaotic systems remains challenging due to
instability in long-term predictions and difficulties in accurately capturing
invariant statistical properties. While quantum machine learning offers a
promising route to efficiently capture physical properties from
high-dimensional data, its practical deployment is hindered by current hardware
noise and limited scalability. We introduce a quantum-informed machine learning
framework for learning partial differential equations, with an application
focus on chaotic systems. A quantum circuit Born machine is employed to learn
the invariant properties of chaotic dynamical systems, achieving substantial
memory efficiency by representing these complex physical statistics with a
compact set of trainable circuit parameters. This approach reduces the data
storage requirement by over two orders of magnitude compared to the raw
simulation data. The resulting statistical quantum-informed prior is then
incorporated into a Koopman-based auto-regressive model to address issues such
as gradient vanishing or explosion, while maintaining long-term statistical
fidelity. The framework is evaluated on three representative systems: the
Kuramoto-Sivashinsky equation, two-dimensional Kolmogorov flow and turbulent
channel flow. In all cases, the quantum-informed model achieves superior
performance compared to its classical counterparts without quantum priors. This
hybrid architecture offers a practical route for learning dynamical systems
using near-term quantum hardware.

</details>


### [314] [A unified diagrammatic formulation of single-reference and multi-reference random phase approximations: the particle-hole and particle-particle channels](https://arxiv.org/abs/2507.19876)
*Yuqi Wang,Wei-Hai Fang,Zhendong Li*

Main category: quant-ph

TL;DR: 本文开发了MR-RPAx和MR-ppRPA，并与MR-dRPA一起进行了数值研究，发现MR-phRPA倾向于高估相关能，而MR-ppRPA倾向于低估相关能。


<details>
  <summary>Details</summary>
Motivation: 将单参考（SR）格林函数方法扩展到多参考（MR）情况，以处理更复杂的量子化学系统。

Method: 本文基于最近引入的图解多参考广义多体微扰理论框架，通过无限阶重求和广义“圈”图和“梯”图，并结合反斜化相互作用顶点，将单参考（SR）格林函数方法扩展到多参考（MR）情况，开发了粒子-空穴（ph）RPA（MR-RPAx）和粒子-粒子RPA（MR-ppRPA）的MR推广。对于MR-dRPA，导出了在SR和MR层级都成立的统一方程组。

Result: 数值研究和微扰分析表明，二阶和三阶之间的误差抵消是SR-RPA和MR-RPA的关键因素。

Conclusion: MR-phRPA（MR-dRPA和MR-RPAx）和MR-ppRPA分别倾向于高估和低估相关能，这表明未来可以通过进一步结合这两个通道来实现更高的精度。

Abstract: A diagrammatic multi-reference generalization of many-body perturbation
theory was recently introduced [J. Phys. Chem. Lett., 2025, 16, 3047]. This
framework allows us to extend single-reference (SR) Green's function methods
defined at the diagrammatic level naturally into multi-reference case, as
previously exemplified by the formulation of multi-reference direct random
phase approximation (MR-dRPA) and the multi-reference second-order screened
exchange approximation (MR-SOSEX). In this work, we further elaborate this
framework and use it to develop MR generalizations of two other RPA variants,
namely, particle-hole (ph) RPA with exchange (MR-RPAx) and particle-particle
RPA (MR-ppRPA). We define these two MR generalizations by infinite order
resummations of the generalized `ring' and `ladder' diagrams with
antisymmetrized interaction vertices, respectively, which incorporate the
contributions from the active-space connected two-body Green's functions. As
for MR-dRPA, we derive unified sets of equations that hold at both SR and MR
levels for RPAx and ppRPA, respectively. We perform numerical studies of
prototypical systems using the three MR-RPA methods and carry out a
perturbative analysis to gain a deeper understanding of their behaviors. We
find that error cancellation between the second and third orders is a key
factor for both SR-RPA and MR-RPA. In addition, we observe that MR-phRPA
(MR-dRPA and MR-RPAx) and MR-ppRPA tend to overestimate and underestimate
correlation energies, respectively, suggesting that a better accuracy can be
achieved by further combining these two channels in the future.

</details>


### [315] [Equivariant Parameter Families of Spin Chains: A Discrete MPS Formulation](https://arxiv.org/abs/2507.19932)
*Ken Shiozaki*

Main category: quant-ph

TL;DR: 在一维量子自旋系统中，我们利用对称性分析了拓扑相变和高贝里曲率，发现相变点是单极子缺陷。


<details>
  <summary>Details</summary>
Motivation: 分析一维量子自旋系统的拓扑相变和高贝里曲率。

Method: 利用G-相容离散化、群上链和参数空间微分来构造等变拓扑不变量，并推导了高贝里不变量的定点公式。

Result: 揭示了相变点作为高贝里曲率发散的单极子缺陷，并讨论了由对称性约化和子群结构相容性决定的参数空间中拓扑缺陷的层级结构。

Conclusion: 分析了一维量子自旋系统中拓扑相变和高贝里曲率，使用显式包含对称群作用于参数空间的框架。基于参数空间的G-相容离散化，同时结合了群上链和参数空间微分，系统地构造了等变拓扑不变量。推导了高贝里不变量的定点公式，用于对称作用具有孤立定点的情况，揭示了Haldane相和平凡相之间的相变点作为高贝里曲率发散的单极子缺陷。

Abstract: We analyze topological phase transitions and higher Berry curvature in
one-dimensional quantum spin systems, using a framework that explicitly
incorporates the symmetry group action on the parameter space. Based on a
$G$-compatible discretization of the parameter space, we incorporate both group
cochains and parameter-space differentials, enabling the systematic
construction of equivariant topological invariants. We derive a fixed-point
formula for the higher Berry invariant in the case where the symmetry action
has isolated fixed points. This reveals that the phase transition point between
Haldane and trivial phases acts as a monopole-like defect where higher Berry
curvature emanates. We further discuss hierarchical structures of topological
defects in the parameter space, governed by symmetry reductions and
compatibility with subgroup structures.

</details>


### [316] [Robust Variational Ground-State Solvers via Dissipative Quantum Feedback Models](https://arxiv.org/abs/2507.19977)
*Yunyan Lee,Ian R. Petersen,Daoyi Dong*

Main category: quant-ph

TL;DR: 提出了一种新的变分框架，用于求解开放量子系统的基态问题，该框架具有稳定性、物理可实现性，并兼容现有实验平台。


<details>
  <summary>Details</summary>
Motivation: 为了解决受量子随机微分方程（QSDE）控制的开放量子系统的基态问题，并为量子化学和量子光学中常见的玻色子算符提供自然的处理方式。

Method: 提出了一种变分框架，利用量子随机微分方程（QSDE）来求解开放量子系统的基态问题。通过参数化耗散量子光学系统，最小化其稳态能量以逼近目标哈密顿量的基态。该方法还结合了H-infinity控制以增强鲁棒性。

Result: 与量子近似优化算法（QAOA）的数值比较突显了该方法的结构优势、稳定性和物理可实现性。

Conclusion: 该框架与腔量子电动力学（QED）和光子晶体电路等实验平台兼容，可用于求解开放量子系统的基态问题。

Abstract: We propose a variational framework for solving ground-state problems of open
quantum systems governed by quantum stochastic differential equations (QSDEs).
This formulation naturally accommodates bosonic operators, as commonly
encountered in quantum chemistry and quantum optics. By parameterizing a
dissipative quantum optical system, we minimize its steady-state energy to
approximate the ground state of a target Hamiltonian. The system converges to a
unique steady state regardless of its initial condition, and the design
inherently guarantees physical realizability. To enhance robustness against
persistent disturbances, we incorporate H-infinity control into the system
architecture. Numerical comparisons with the quantum approximate optimization
algorithm (QAOA) highlight the method's structural advantages, stability, and
physical implementability. This framework is compatible with experimental
platforms such as cavity quantum electrodynamics (QED) and photonic crystal
circuits.

</details>


### [317] [Finite-Size Effects in Quantum Metrology at Strong Coupling: Microscopic vs Phenomenological Approaches](https://arxiv.org/abs/2507.19994)
*Ali Pedram,Özgür E. Müstecaplıoğlu*

Main category: quant-ph

TL;DR: Analyzed spin chain precision limits for magnetometry/thermometry using polaron transform and nanothermodynamics. Found SC advantage at low temps and via anisotropy control, but FS effects are crucial and phenomenological models fail at SC.


<details>
  <summary>Details</summary>
Motivation: Investigate the ultimate precision limits of a spin chain coupled to a heat bath for measuring general parameters, specifically magnetometry and thermometry.

Method: Utilized a full polaron transform to derive the effective Hamiltonian. Obtained analytical expressions for QFI in both weak and strong coupling regimes, accounting for finite-size effects. Employed Hill's nanothermodynamics for an effective QFI expression at SC.

Result: Derived analytical expressions for QFI in WC and SC regimes, including FS effects. Showed SC advantage for thermometry at low temperatures and improved magnetometry via anisotropy control. Highlighted errors from neglecting FS effects and inadequacy of phenomenological approaches for SC systems. Demonstrated bath's effect on SC phase transitions.

Conclusion: SC regime shows potential advantage for thermometry at low temperatures and enhanced magnetometric precision via anisotropy parameter control. Neglecting FS effects leads to significant errors in ultimate precision bounds for equilibrium thermometry. Phenomenological approaches are inadequate for SC systems.

Abstract: We study the ultimate precision limits of a spin chain, strongly coupled to a
heat bath, for measuring a general parameter and report the results for
specific cases of magnetometry and thermometry. Employing a full polaron
transform, we derive the effective Hamiltonian and obtain analytical
expressions for the quantum Fisher information (QFI) of equilibrium states in
both weak coupling (WC) and strong coupling (SC) regimes for a general
parameter, explicitly accounting for finite-size (FS) effects. Furthermore, we
utilize Hill's nanothermodynamics to calculate an effective QFI expression at
SC. Our results reveal a potential advantage of SC for thermometry at low
temperatures and demonstrate enhanced magnetometric precision through control
of the anisotropy parameter. Crucially, we show that neglecting FS effects
leads to considerable errors in ultimate precision bounds for equilibrium
thermometry. This work also highlights the inadequacy of phenomenological
approaches in describing the metrological capability and thermodynamic behavior
of systems at SC. Additionally, we demonstrate the effect of bath on system's
phase transition at SC.

</details>


### [318] [Efficient construction of fault-tolerant neutral-atom cluster states](https://arxiv.org/abs/2507.20009)
*Luke M. Stewart,Gefen Baranes,Joshua Ramette,Josiah Sinclair,Vladan Vuletić*

Main category: quant-ph

TL;DR: 该研究提出了一种利用高精细光学腔构建低开销、高保真度三维簇态的协议，适用于容错量子计算，并显著降低了资源需求。


<details>
  <summary>Details</summary>
Motivation: 集群态是量子计算中的有用资源，可以通过相邻量子比特之间的纠缠门生成。有预兆的纠缠门具有高后选保真度的优点，但可能产生巨大的时空开销。因此，本研究旨在提出一种低开销的协议来解决这个问题。

Method: 本研究提出了一种低开销协议，利用高精细光学腔生成和合并多原子纠缠态，以构建三维簇态。

Result: 模拟结果表明，使用最先进的高精细光学腔足以构建可扩展的容错簇态，其损耗和泡利错误率远低于各自的阈值。

Conclusion: 该协议展示了一种低开销的协议，用于生成和合并高保真度多原子纠缠态，以构建支持容错通用逻辑操作的三维簇态。结果表明，最先进的高精细光学腔足以构建可扩展的容错簇态，其损耗和泡利错误率远低于各自的阈值。该协议降低了簇态构建的时空资源需求，突出了基于测量的</strong>方法作为使用中性原子实现大规模容错量子处理的替代方法。

Abstract: Cluster states are a useful resource in quantum computation, and can be
generated by applying entangling gates between next-neighbor qubits. Heralded
entangling gates offer the advantage of high post-selected fidelity, and can be
used to create cluster states at the expense of large space-time overheads. We
propose a low-overhead protocol to generate and merge high-fidelity many-atom
entangled states into a 3D cluster state that supports fault-tolerant universal
logical operations. Our simulations indicate that a state-of-the-art
high-finesse optical cavity is sufficient for constructing a scalable
fault-tolerant cluster state with loss and Pauli errors remaining an order of
magnitude below their respective thresholds. This protocol reduces the
space-time resource requirements for cluster state construction, highlighting
the measurement-based method as an alternative approach to achieving
large-scale error-corrected quantum processing with neutral atoms.

</details>


### [319] [Anyonic Josephson junctions: Dynamical and ground-state properties](https://arxiv.org/abs/2507.20044)
*Jessica John Britto*

Main category: quant-ph

TL;DR: 该研究提出了一种基于密度相关跳跃的玻色子模型，构建了一个约瑟夫森结装置，该装置在强关联体系中能够产生约瑟夫森效应，并能在无外部偏压的情况下实现连续粒子流。


<details>
  <summary>Details</summary>
Motivation: 为了在强关联体系中理解约瑟夫森效应，并探索在没有外部偏压的情况下通过创建初始相位差实现连续粒子流的可能性。

Method: 利用精确对角化和密度重整化群理论等数值方法研究了该装置的基态性质，并分析了该模型的动力学性质以寻找能够产生约瑟夫森效应的构型。

Result: 在强关联体系中成功构建了一个约瑟夫森结装置，并验证了其能够产生约瑟夫森效应，同时观察到仅通过创建初始相位差即可实现连续粒子流。

Conclusion: 该模型实现了在没有外部偏压的情况下，仅通过创建初始相位差即可实现连续粒子流

Abstract: Bosons with density-dependent hopping on a one dimensional lattice have been
shown to emulate anyonic particles with fractional exchange statistics.
Leveraging this, we construct a Josephson junction setup, where an insulating
barrier in the form of a Mott-insulator is sandwiched between two superfluid
phases. This is obtained by spatially varying either the statistical phase or
the strength of the on-site interaction potential on which the ground state of
the system depends. Utilizing numerical methods such as exact diagonalization
and density renormalization group theory, the ground state properties of this
setup are investigated to understand the Josephson effect in a strongly
correlated regime. The dynamical properties of this model for different
configurations of this model are analyzed to find the configurations that can
produce the Josephson effect. Furthermore, it is observed that continuous
particle flow over time is achievable in this proposed model solely by creating
an initial phase difference without any external biasing.

</details>


### [320] [Cornell Interaction in the Two-body Pauli-Schrödinger-type Equation Framework: The Symplectic Quantum Mechanics Formalism](https://arxiv.org/abs/2507.20045)
*R. R. Luz,G. X. A. Petronilo,R. A. S. Paiva,A. E. Santana,T. M. Rocha Filho,R. G. G. Amorim*

Main category: quant-ph

TL;DR: 研究了磁场对魅-反魅强子系统的量子行为影响，发现磁场增强了Wigner函数的负值，表明量子干涉增强。计算结果与实验数据一致。


<details>
  <summary>Details</summary>
Motivation: 研究魅-反魅强子系统在磁场影响下的量子行为，特别是磁场对系统量子特性的调节作用，并量化其非经典性。

Method: 采用量子力学的辛（symplectic）方法，结合微扰方法，对由Cornell势描述的系统（包含约束和非约束相互作用）进行了研究。通过在相空间中进行Bohlin映射，求解了与Pauli-Schrödinger方程类似的方程，并确定了相应的Wigner函数。

Result: 在磁场作用下，Wigner函数的负值增强，表明量子干涉增强，偏离经典行为。计算得到的car{c}介子质量谱与实验数据吻合，并优于之前的理论研究。

Conclusion: 该研究表明，磁场增强了Wigner函数的负值，表明量子干涉增强，并偏离了经典行为。该研究计算了car{c}介子的质量谱，结果与实验数据一致，优于先前的理论研究。

Abstract: We investigate the quantum behavior of a charm-anticharm bound system under
the influence of a magnetic field within the symplectic formulation of Quantum
Mechanics. Employing a perturbative approach, we obtain the ground and first
excited states of the system described by the Cornell potential, which
incorporates both confining and nonconfining interactions. After perfoming a
Bohlin mapping in phase space, we solve the time-independent symplectic
Pauli-Schr\"odinger-type equation and determine the corresponding Wigner
function. Special attention is given to the observation of the confinement of
the quark-antiquark (meson $c\overline{c}$) that is revealed in the phase space
structure. And the introduction of spin effect (external magnetic field) in
modifying the quantum characteristics of the system. Our results reveal that
the magnetic enhances the negativity of the Wigner function, signaling stronger
quantum interference and a departure from classical behavior. The negativity
thus serve as a quantitative measure of the system's non-classicality. In
addition we have computed the mass spectra of the $c\overline{c}$ meson and
present result are in agreement with experimental data, improving previous
theoretical studies.

</details>


### [321] [Modeling Charge Noise in Superconducting Qubits Using Memory Multi-Fractional Brownian Motion](https://arxiv.org/abs/2507.20097)
*Mahboob Ul Haq*

Main category: quant-ph

TL;DR: 提出了一种基于记忆多重分数布朗运动（mmfBm）的超导电荷噪声模型，可捕捉非平稳和长记忆效应，并复现了退相干的关键实验特征。


<details>
  <summary>Details</summary>
Motivation: 介绍了一种用于超导电荷噪声的新型随机模型。

Method: 提出了一种基于记忆多重分数布朗运动（mmfBm）的超导电荷噪声随机模型，能够捕捉非平稳和长记忆效应。

Result: 该模型能够捕捉非平稳和长记忆效应，并复现了退相干的关键实验特征。

Conclusion: 该框架为超导量子设备中的环境相互作用提供了新的见解，并成功复现了退相干的关键实验特征。

Abstract: We introduce a novel stochastic model for charge noise in superconducting
charge qubits based on memory multi-fractional Brownian motion (mmfBm), capable
of capturing non-stationary and long-memory effects. This framework reproduces
key experimental features of decoherence and offers new insights into
environmental interactions with superconducting quantum devices.

</details>


### [322] [Circuit simulation of readout process toward large-scale superconducting quantum circuits](https://arxiv.org/abs/2507.20100)
*Tetsufumi Tanamoto,Hiroshi Fuketa,Toyofumi Ishikawa,Shiro Kawabata*

Main category: quant-ph

TL;DR: 提出了一种使用SPICE仿真超导量子电路以评估大规模量子比特保真度的方法。


<details>
  <summary>Details</summary>
Motivation: 为了应对日益增长的超导量子计算机中由器件级差异（如电容和约瑟夫森临界电流的波动）引起的保真度问题。

Method: 使用传统的集成电路仿真程序（SPICE）来模拟量子电路。

Result: 该方法能够在标准笔记本电脑上评估拥有 10000 个量子比特的超导量子电路的性能。

Conclusion: 这项研究提出了一种基于经典电路仿真的模拟方法，用于估算量子比特保真度。

Abstract: The rapid scaling of superconducting quantum computers has highlighted the
impact of device-level variability on overall circuit fidelity. In particular,
fabrication-induced fluctuations in device parameters such as capacitance and
Josephson critical current pose significant challenges to large-scale
integration. We propose a simulation methodology for estimating qubit fidelity
based on classical circuit simulation, using a conventional Simulation Program
with Integrated Circuit Emphasis (SPICE) simulator. This approach enables the
evaluation of the performance of superconducting quantum circuits with 10000
qubits on standard laptop computers. The proposed method provides an accessible
tool for the early stage assessment of large-scale superconducting quantum
circuit performance.

</details>


### [323] [Tunnelling photons pose no challenge to Bohmian machanics](https://arxiv.org/abs/2507.20101)
*Yun-Fei Wang,Xiao-Yu Wang,Hui Wang,Chao-Yang Lu*

Main category: quant-ph

TL;DR: Sharoglazova 等人的实验没有挑战玻姆力学，因为他们比较了不同的物理量，而且两种诠释在光子隧穿动力学方面给出了相同的结果。


<details>
  <summary>Details</summary>
Motivation: Sharoglazova 等人的实验数据与半经典“速度”和玻姆速度之间存在差异，他们声称这对玻姆力学提出了挑战。

Method: 首先，我们指出了所提出的半经典“速度”和玻姆速度在物理上是不同的量，因此比较它们是不合理的。其次，我们严格证明了两种诠释都预测了耦合波导中相同的光子隧穿动力学。

Result: 两种诠释都预测了耦合波导中相同的光子隧穿动力学。

Conclusion: 该实验并不对玻姆力学提出挑战，因为他们比较的不是物理量，并且两种诠释都预测了相同的光子隧穿动力学。

Abstract: Very recently, Sharoglazova et al. performed an experiment measuring the
energy-velocity relationship and Bohmian velocity in coupled waveguides. Their
data show a discrepancy between the semi-classical `speed'
$v=\sqrt{2|\Delta|/m}$ and Bohmian velocity $v_s$ for $\Delta<-\hbar J_0$,
leading them to claim a challenge to Bohmian mechanics. Here, we definitively
demonstrate this experiment poses no challenge to Bohmian mechanics. First, $v$
and $v_S$ represent fundamentally distinct physical quantities -- comparing
them is physically unjustified and cannot adjudicate between Copenhagen and
Bohmian interpretations. Second, we rigorously show that both interpretations
predict identical photon tunneling dynamics in coupled waveguides.

</details>


### [324] [The existence of non-classical orthogonal quantum Latin squares](https://arxiv.org/abs/2507.20154)
*Yan Han,Yajuan Zang,Hongjiao Zhang,Zihong Tian*

Main category: quant-ph

TL;DR: 本研究提出了新型量子拉丁方，如幂等、自正交和带孔量子拉丁方，并提供了构造方法，证明了某些非经典量子拉丁方的存在性。


<details>
  <summary>Details</summary>
Motivation: 量子拉丁方是经典拉丁方在量子领域的推广，在量子酉误差基、互无偏基、k-均匀态和量子纠错码等领域有广泛应用。本研究旨在提出并构造具有特殊性质的新型量子拉丁方，以扩展其应用范围。

Method: 论文提出了一些新的量子拉丁方，包括幂等量子拉丁方、自正交量子拉丁方和带孔量子拉丁方。利用PBD构造和填充孔洞构造等方法，构建了非经典量子拉丁方，并证明了某些特定量子拉丁方的存在性。

Result: 本研究成功构造了几种新的量子拉丁方，包括幂等量子拉丁方、自正交量子拉丁方和带孔量子拉丁方，并建立了它们正交性的概念。通过PBD构造和填充孔洞构造等方法，证明了非经典2-幂等MOQLS(v)、非经典2,3-MOQLS(v)和非经典SOQLS(v)的存在性，仅有少数特定值可能例外。

Conclusion: 本论文提出了具有特殊性质的新型量子拉丁方，如幂等量子拉丁方、自正交量子拉丁方、带孔量子拉丁方，并探讨了它们的正交性概念。此外，论文还展示了一些强有力的构造方法，包括PBD构造和填充孔洞构造，用于非经典量子拉丁方。最终，论文证明了除少数特定值外，非经典2-幂等MOQLS(v)、非经典2,3-MOQLS(v)和非经典SOQLS(v)的存在性。

Abstract: Quantum Latin squares are a generalization of classical Latin squares in
quantum field and have wide applications in unitary error bases, mutually
unbiased bases, $k$-uniform states and quantum error correcting codes. In this
paper, we put forward some new quantum Latin squares with special properties,
such as idempotent quantum Latin square, self-orthogonal quantum Latin square,
holey quantum Latin square, and the notions of orthogonality on them. We
present some forceful construction methods including PBD constructions and
filling in holes constructions for non-classical quantum Latin squares. As
consequences, we establish the existence of non-classical 2-idempotent
MOQLS$(v)$, non-classical 2, 3-MOQLS$(v)$ and non-classical SOQLS$(v)$ except
possibly for several definite values.

</details>


### [325] [Poliazed Houston State Framework for Nonequilibrium Driven Open Quanum Sysmtes](https://arxiv.org/abs/2507.20160)
*Shunsuke A. Sato,Hannes Hübener,Umberto De Giovannini,Angel Rubio*

Main category: quant-ph

TL;DR: A new 'polarized Houston basis' improves simulations of quantum systems driven by light by reducing errors found in older methods and cutting down on unphysical currents, making it a better tool for studying these systems.


<details>
  <summary>Details</summary>
Motivation: To develop a more accurate theoretical framework for modeling nonequilibrium dynamics in driven open quantum systems, specifically addressing the limitations of conventional Houston states in describing excitation dynamics under external driving and the presence of spurious excitations and virtual transitions.

Method: A new theoretical framework, the polarized Houston basis, is introduced and formulated for use within the quantum master equation. The method involves extending conventional Houston states to incorporate field-induced polarization effects and examining band population dynamics through projections onto polarized Houston states, original Houston states, and naive Bloch states using a one-dimensional dimer-chain model. The formalism is implemented in the relaxation time approximation of the quantum master equation.

Result: The polarized Houston basis significantly suppresses spurious Bloch-state excitations and virtual transitions compared to standard Houston approaches, allowing for a cleaner extraction of real excitations. Implementation in the relaxation time approximation of the quantum master equation leads to a substantial reduction of unphysical DC currents in insulating systems.

Conclusion: The polarized Houston basis is a powerful tool for simulating nonequilibrium phenomena in light-driven open quantum materials, significantly suppressing spurious excitations and virtual transitions, and reducing unphysical DC currents.

Abstract: We introduce a new theoretical framework -- the polarized Houston basis -- to
model nonequilibrium dynamics in driven open quantum systems, formulated for
use within the quantum master equation. This basis extends conventional Houston
states by incorporating field-induced polarization effects, enabling a more
accurate description of excitation dynamics under external driving. Using a
one-dimensional dimer-chain model, we examine band population dynamics through
projections onto polarized Houston states, original Houston states, and naive
Bloch states. We find that the polarized Houston basis significantly suppresses
spurious Bloch-state excitations and virtual transitions present in standard
Houston approaches, allowing for a cleaner extraction of real excitations. When
implemented in the relaxation time approximation of the quantum master
equation, this formalism also yields a substantial reduction of unphysical DC
currents in insulating systems. Our results highlight the polarized Houston
basis as a powerful tool for simulating nonequilibrium phenomena in
light-driven open quantum materials.

</details>


### [326] [Atom-Field-Medium Interactions III: Quantum Field-mediated Entanglement between Two Atoms near a Conducting Surface](https://arxiv.org/abs/2507.20213)
*Jen-Tsung Hsiang,Bei-Lok Hu*

Main category: quant-ph

TL;DR: 本文研究了两个原子在导电表面附近的量子纠缠行为，发现原子越靠近表面，纠缠度反而越小。研究提出了量化和可视化纠缠的方法，并探讨了如何通过调整参数来控制纠缠，为原子-场-介质相互作用和真空与表面物理的研究提供了参考。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究原子-场相互作用中量子信息问题，特别是纠缠和退相干现象，并探索原子与量子场之间的纠缠行为，以及如何通过改变耦合强度、原子间距和原子与表面距离等参数来控制这种纠缠。

Method: 本研究使用量子信息理论，研究了具有内部自由度的两个原子在导电表面附近的纠缠行为，考虑了不同原子间距和原子与表面距离的影响，并超越了弱耦合限制。

Result: 研究发现，与卡西米尔-波尔效应的场诱导力不同，原子与量子场的纠缠度会随着原子靠近导体而减小。研究还量化了三维“纠缠域”，并展示了如何通过改变参数来控制两个原子之间的纠缠。

Conclusion: 本研究通过改变原子间距、原子与导电表面距离以及原子与场的耦合强度，对原子-场相互作用进行了深入研究，并提出了量化和可视化量子纠缠空间地形图的方法。

Abstract: This third paper in this series continues the investigation of atom-field
interactions in the presence of a conductor or a dielectric medium, focusing on
quantum information related basic issues such as decoherence and entanglement.
Here we consider the entanglement between two atoms with internal degrees of
freedom modeled by a harmonic oscillator, with varying separations between them
and varying distances between them and a conducting surface. These are
configurations familiar in the Casimir-Polder effect, but the behavior of
atom-surface entanglement is quite different from the well-studied behavior of
field-induced forces. For one, while the attractive force between an atom and a
conducting surface increases as they come closer, the entanglement between the
atom and the quantum field actually decreases as the atom gets closer to the
conductor, as shown in \cite{Rong,AFD2}. We show how different factors play
out, ranging from the coupling between the atoms and the field to the coupling
between the atoms, going beyond the weak coupling restrictions often found
necessary in the literature. Gathering our results for the entanglement
dependence on each variable concerned, we can provide a spatial topography of
quantum entanglement, thus enabling a visualized understanding of the behavior
of quantum field-mediated entanglement. In particular we can quantify the
definition of a three-dimensional \textit{entanglement domain} between the two
atoms, how it varies with their coupling, their separation and their distances
from the conducting surface, and for practical applications, how to exercise
effective control of the entanglement between two atoms by changing these
parameters. Our findings are expected to be useful for studies of
atom-field-medium interactions in vacuum and surface physics.

</details>


### [327] [Efficient Gaussian State Preparation in Quantum Circuits](https://arxiv.org/abs/2507.20317)
*Yichen Xie,Nadav Ben-Ami*

Main category: quant-ph

TL;DR: 一种新的量子算法，用于在量子计算机上高效制备高斯态。


<details>
  <summary>Details</summary>
Motivation: 数字、门基量子计算机需要离散近似高斯分布，而现有方法存在指数级扩展或高昂的成本。因此，需要一种资源高效的近似高斯态制备方法。

Method: 提出并分析了一种基于电路的方法，该方法首先使用单量子比特旋转来形成指数幅度分布，然后应用量子傅里叶变换将这些幅度映射到近似高斯分布。此过程通过选择性地修剪量子傅里叶变换中的小相位角来实现近线性的\(mathcal{O}(n)\)门复杂度。

Result: 该方法实现了高保真度的高斯态制备，并将门复杂度降低到近线性
\(mathcal{O}(n)\)。

Conclusion: 该技术有望使嘈杂的量子硬件能够访问高斯态，并为未来设备的可扩展实现铺平道路。

Abstract: Gaussian states hold a fundamental place in quantum mechanics, quantum
information, and quantum computing. Many subfields, including quantum
simulation of continuous-variable systems, quantum chemistry, and quantum
machine learning, rely on the ability to accurately and efficiently prepare
states that reflect a Gaussian profile in their probability amplitudes.
Although Gaussian states are natural in continuous-variable systems, the
practical interest in digital, gate-based quantum computers demands discrete
approximations of Gaussian distributions over a computational basis of size
\(2^n\). Because of the exponential scaling of naive amplitude-encoding
approaches and the cost of certain block-encoding or Hamiltonian simulation
techniques, a resource-efficient preparation of approximate Gaussian states is
required. In this work, we propose and analyze a circuit-based approach that
starts with single-qubit rotations to form an exponential amplitude profile and
then applies the quantum Fourier transform to map those amplitudes into an
approximate Gaussian distribution. We demonstrate that this procedure achieves
high fidelity with the target Gaussian state while allowing optional pruning of
small controlled-phase angles in the quantum Fourier transform, thus reducing
gate complexity to near-linear in \(\mathcal{O}(n)\). We conclude that the
proposed technique is a promising route to make Gaussian states accessible on
noisy quantum hardware and to pave the way for scalable implementations on
future devices. The implementation of this algorithm is available at the
Classiq library: https://github.com/classiq/classiq-library.

</details>


### [328] [Quantitative analysis of the effectiveness of mid-anneal measurement in quantum annealing](https://arxiv.org/abs/2507.20318)
*Keita Takahashi,Shu Tanaka*

Main category: quant-ph

TL;DR: 量子退火中的中途退火测量是一种有效的缓解技术，其有效性取决于能量结构和状态相似性，并且适用于大规模问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决量子退火中由参数调整困难和硬件噪声引起的问题，即最优解难以编码为问题哈密顿量的基态，本研究调查了中途退火测量作为一种缓解方法。

Method: 研究了量子退火中的中途退火测量，开发了一种评估其有效性的量化指标，并将其应用于图划分和二次背包问题。

Result: 中途退火测量在期望解与基态能量差较小时最有效，其有效性受能量结构和能量与激发态之间的汉明距离的强烈影响。随着系统规模的扩大，该方法的有效性仍然存在。

Conclusion: 中途退火测量是一种有前途的量子退火缓解方法，其有效性取决于能量差和状态相似性，并且随着系统规模的扩大而具有可扩展性。

Abstract: Quantum annealing is a promising metaheuristic for solving constrained
combinatorial optimization problems. However, parameter tuning difficulties and
hardware noise often prevent optimal solutions from being properly encoded as
the ground states of the problem Hamiltonian. This study investigates
mid-anneal measurement as a mitigation approach for such situations, analyzing
its effectiveness and underlying physical mechanisms. We introduce a
quantitative metric to evaluate the effectiveness of mid-anneal measurement and
apply it to the graph bipartitioning problem and the quadratic knapsack
problem. Our findings reveal that mid-anneal measurement is most effective when
the energy difference between desired solutions and ground states is small,
with effectiveness strongly governed by the energy structure. Furthermore, the
effectiveness increases as the Hamming distance between the ground and excited
states gets small, highlighting the role of state similarity. Analysis of
fully-connected Ising models demonstrates that the effectiveness of mid-anneal
measurement persists with increasing system size, indicating its scalability
and practical applicability to large-scale quantum annealing.

</details>


### [329] [A small and interesting architecture for early fault-tolerant quantum computers](https://arxiv.org/abs/2507.20387)
*Jacob S. Nelson,Andrew J. Landahl,Andrew D. Baczewski*

Main category: quant-ph

TL;DR: 提出了一种基于色码的容错量子计算架构，优化目标是减少状态传送，并提供实验测试方案。


<details>
  <summary>Details</summary>
Motivation: 为了帮助硬件开发者表征该容错量子计算机架构。

Method: 提出了一种基于最小有趣色码的容错量子计算机架构，该架构实现了包括单量子比特测量和制备、单量子比特H门以及三量子比特CCZ门在内的通用逻辑门集。利用二维和三维量子色码之间的状态传送，可以利用各自的横H和CCZ门。因此，优化目标是最小化逻辑量子传送操作的数量，而不是逻辑量子非克利福德门的数量。

Result: 该架构能够实现通用逻辑门集，并且最小化逻辑量子传送操作数量是关键的优化目标。

Conclusion: 该论文提出了一种基于最小有趣色码的容错量子计算机架构，并提供了一个用于测试逻辑量子电路的实验方案。

Abstract: We present an architecture for early fault-tolerant quantum computers based
on the smallest interesting colour code (Earl Campbell, 2016). It realizes a
universal logical gate set consisting of single-qubit measurements and
preparations in the X and Z bases, single-qubit Hadamard (H) gates, and
three-qubit controlled-controlled-Z (CCZ) gates. State teleportations between
[[4, 2, 2]] (2D) and [[8, 3, 2]] (3D) error-detecting color codes allow one to
make use of the respective transversal H and CCZ gates that these codes
possess. As such, minimizing the number of logical quantum teleportation
operations, not the number of logical quantum non-Clifford gates, is the
relevant optimization goal. To help hardware developers characterize this
architecture, we also provide an experimental protocol tailored to testing
logical quantum circuits expressed in it.

</details>


### [330] [Geometric Algebras and Fermion Quantum Field Theory](https://arxiv.org/abs/2507.20394)
*Stan Gudder*

Main category: quant-ph

TL;DR: This paper introduces a geometric algebra framework to model interacting fermions and quantum fields, extending the concepts to infinite dimensions.


<details>
  <summary>Details</summary>
Motivation: To provide a mathematical framework for describing interacting fermion systems using geometric algebras and to generalize these concepts to infinite-dimensional spaces, enabling the study of quantum fields.

Method: The paper defines a geometric algebra $\gscript(H)$ for a finite-dimensional Hilbert space $H$, interprets its unit vectors as states of interacting fermions, and discusses creation and evolution operators. It then extends these concepts to infinite-dimensional separable Hilbert spaces.

Result: The paper constructs fermion and boson-fermion quantum fields, provides matrix representations for creation operators, and studies extensions of operators from $H$ to $\gscript(H)$.

Conclusion: The paper generalizes the concepts of quantum fields and operators to infinite-dimensional Hilbert spaces, extending the framework for describing fermionic systems.

Abstract: Corresponding to a finite dimensional Hilbert space $H$ with $\dim H=n$, we
define a geometric algebra $\gscript (H)$ with $\dim\sqbrac{\gscript (H)}=2^n$.
The algebra $\gscript (H)$ is a Hilbert space that contains $H$ as a subspace.
We interpret the unit vectors of $H$ as states of individual fermions of the
same type and $\gscript (H)$ as a fermion quantum field whose unit vectors
represent states of collections of interacting fermions. We discuss creation
operators on $\gscript (H)$ and provide their matrix representations. Evolution
operators provided by self-adjoint Hamiltonians on $H$ and $\gscript (H)$ are
considered. Boson-Fermion quantum fields are constructed. Extensions of
operators from $H$ to $\gscript (H)$ are studied. Finally, we present a
generalization of our work to infinite dimensional separable Hilbert spaces.

</details>


### [331] [Effects of Ill-Defined Domain of Definitions of the Parameter Operator on Berry Curvature and the Adiabatic Theorem](https://arxiv.org/abs/2507.20679)
*Georgios Konstantinou,Konstantinos Moulopoulos*

Main category: quant-ph

TL;DR: Berry 曲率与特征向量有关，即使哈密顿量没有参数依赖性也可能存在非零 Berry 曲率。


<details>
  <summary>Details</summary>
Motivation: 探讨 Berry 曲率的推导及其在算符定义不明确的域中的行为，并揭示 Berry 曲率与特征向量的关系。

Method: 提出一种推导 Berry 曲率的解析方法，并分析了其在算符定义不明确的域中的行为。

Result: 即使没有显式参数依赖性的哈密顿量也可能表现出非零的 Berry 曲率，表明 Berry 曲率与特征向量而非哈密顿量本身具有内在联系。

Conclusion: Berry曲率与哈密顿量本身无关，而是与特征向量有内在联系。

Abstract: We present a comprehensive analytical study that extends the conventional
formulation of Berry curvature, highlighting its derivation in the context of
problematic domains of definition of the operators. Our analysis reveals that
handling these domains carefully can have a substantial impact on Berry
curvature, demonstrating that even Hamiltonians without explicit parameter
dependence may exhibit nonzero Berry curvature. This finding emphasizes that
Berry curvature is intrinsically related to the eigenvectors rather than the
Hamiltonian itself. Our approach utilizes the standard Bloch (k-space)
framework for spatially periodic systems, illustrating these effects from first
principles and discussing potential implications for solid-state systems.

</details>


### [332] [Single-photon sources created by nature millions of years ago](https://arxiv.org/abs/2507.20405)
*D. G. Pasternak,A. M. Romshin,R. A. Khmelnitsky,G. Yu. Kriulina,A. A. Zhivopistsev,O. S. Kudryavtsev,A. V. Gritsienko,A. M. Satanin,I. I. Vlasov*

Main category: quant-ph

TL;DR: Natural diamonds contain single photon sources (SPS) in their rims, previously unutilized but now valuable for quantum technology.


<details>
  <summary>Details</summary>
Motivation: To identify key components for quantum communication devices, specifically single photon sources (SPS).

Method: Photoluminescence (PL) spectra analysis of untreated Yakut diamonds rich in nitrogen and hydrogen, focusing on narrow lines within the 500-800 nm range.

Result: Discovery of bright diamond-based SPS in natural diamonds, with narrow spectral lines attributed to SPS. Previously observed unknown narrow-line PL in diamonds from various deposits is also associated with SPS.

Conclusion: The diamond rim, previously discarded or used as abrasive powder, is identified as a valuable material for quantum technologies due to its potential as a single photon source.

Abstract: Single photons source (SPS) is a key component required by quantum
communication devices. We report the finding of bright diamond-based SPS
created by nature millions of years ago. It is shown that narrow ($\leq$ 2 nm)
lines observed within the 500-800 nm range in photoluminescence (PL) spectra of
the surface layer of untreated Yakut diamonds rich in nitrogen and hydrogen
belong to SPS. Moreover, unknown narrow-line PL observed earlier in nitrogen-
and hydrogen-rich diamonds from various deposits around the world are thought
to be associated with SPS. Thus, the diamond rim, which until now was sent to
the dumps or, at best, used as an abrasive powder, turned out to be a valuable
material suitable for use in quantum technologies.

</details>


### [333] [Identification and Properties of Topological States in the Bulk of Quasicrystals](https://arxiv.org/abs/2507.20722)
*Frode Balling-Ansø,Jeppe Lykke Krogh,Ella Elisabeth Lassen,Anne E. B. Nielsen*

Main category: quant-ph

TL;DR: 在准晶中发现了新的拓扑态BLT态，其数量随系统尺寸变化，不同于边缘态。我们开发了一种识别算法，发现BLT态出现在特定磁通量区间和多种态密度下，并与准晶几何结构相关，可能更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 与传统的体边对应不同，在准晶结构中发现了位于系统体内的拓扑态，称为体局域传输（BLT）状态。这些状态的性质，例如数量与系统尺寸成比例而非与系统周长成比例，与边缘状态不同。

Method: 提出了一种基于物理启发式十字准星标记和鲁棒性分析的算法来识别BLT状态。

Result: 该算法应用于阿曼-贝恩克平铺上的霍夫施塔特模型，发现在特定磁通量区间内主要出现BLT状态。与仅在低态密度下出现的边缘状态不同，BLT状态可以出现在多种不同的态密度下。许多BLT状态的空间局域化模式与准晶格的几何特征相符。此外，BLT状态可以单独或成组地出现在能谱中，这可能意味着成组状态具有更高的鲁棒性，并且其空间局域化会随费米能级变化。

Conclusion: BLT状态的出现与磁通量在一个特定区间内有关，并且它们可以出现在多种不同的态密度下，其空间局域化模式与准晶格的几何特征相符。BLT状态可能成簇出现，这暗示了它们可能具有更高的鲁棒性。

Abstract: In contrast to the usual bulk-boundary correspondence, topological states
localized within the bulk of the system have been numerically identified in
quasicrystalline structures, termed bulk localized transport (BLT) states.
These states exhibit properties different from edge states, one example being
that the number of BLT states scales with system size, while the number of edge
states scales with system perimeter. Here, we define an algorithm to identify
BLT states, which is based on the physically motivated crosshair marker and
robustness analyses. Applying the algorithm to the Hofstadter model on the
Ammann-Beenker tiling, we find that the BLT states appear mainly for magnetic
fluxes within a specific interval. While edge states appear at low densities of
states, we find that BLT states can appear at many different densities of
states. Many of the BLT states are found to have real-space localization that
follows geometric patterns characteristic of the given quasicrystal.
Furthermore, BLT states can appear both isolated and in groups within the
energy spectrum which could imply greater robustness for the states within such
groups. The spatial localization of the states within a certain group can
change depending on the Fermi energy.

</details>


### [334] [Encoding molecular structures in quantum machine learning](https://arxiv.org/abs/2507.20422)
*Choy Boy,Edoardo Altamura,Dilhan Manawadu,Ivano Tavernelli,Stefano Mensa,David J. Wales*

Main category: quant-ph

TL;DR: 提出了一种名为QMSE的新型量子分子结构编码方案，它使用混合库仑-邻接矩阵来优化化学数据集的量子机器学习。与传统方法相比，QMSE在提高分子状态可分离性、可训练性和泛化能力方面表现更好，并能通过链收缩定理减少量子比特数量，有望促进量子机器学习在化学领域的实际应用。


<details>
  <summary>Details</summary>
Motivation: 传统的量子数据编码方案（如指纹编码）在准确表示化学数据集中化学部分方面通常不可行。QMSE旨在克服这一限制。

Method: 提出了一种新的量子分子结构编码（QMSE）方案，将混合库仑-邻接矩阵（表示键序和原子间耦合）直接编码为参数化量子电路中的单量子比特和双量子比特旋转。

Result: QMSE提供了一种有效且可解释的方法，与传统的指纹编码方法相比，可以提高编码分子之间的状态可分离性。通过在分子数据集上训练参数化线路，在区分物态和回归沸点方面表现出具有竞争力的可训练性和泛化能力。此外，还证明了一个保真度链收缩定理，通过重用公共子结构来减少量子比特数量，并已应用于长链脂肪酸。

Conclusion: 该方法有望为分子数据集的实际量子机器学习应用铺平道路。

Abstract: Quantum machine learning (QML) has great potential for the analysis of
chemical datasets. However, conventional quantum data-encoding schemes, such as
fingerprint encoding, are generally unfeasible for the accurate representation
of chemical moieties in such datasets. In this contribution, we introduce the
quantum molecular structure encoding (QMSE) scheme, which encodes the molecular
bond orders and interatomic couplings expressed as a hybrid Coulomb-adjacency
matrix, directly as one- and two-qubit rotations within parameterised circuits.
We show that this strategy provides an efficient and interpretable method in
improving state separability between encoded molecules compared to other
fingerprint encoding methods, which is especially crucial for the success in
preparing feature maps in QML workflows. To benchmark our method, we train a
parameterised ansatz on molecular datasets to perform classification of state
phases and regression on boiling points, demonstrating the competitive
trainability and generalisation capabilities of QMSE. We further prove a
fidelity-preserving chain-contraction theorem that reuses common substructures
to cut qubit counts, with an application to long-chain fatty acids. We expect
this scalable and interpretable encoding framework to greatly pave the way for
practical QML applications of molecular datasets.

</details>


### [335] [Electron charge coherence on a solid neon surface](https://arxiv.org/abs/2507.20476)
*Xinhao Li,Shan Zou,Qianfan Chen,Dafei Jin*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent experiments show ~0.1 ms coherence time for a single electron charge
qubit on a solid neon surface. This remarkably long coherence time is believed
to result from the intrinsic purity of solid neon as a qubit host. In this
paper, we present theoretical studies on the decoherence mechanisms of an
electron's charge (lateral motional) states on solid neon. At the typical
experimental temperature of ~10 mK, the two main decoherence mechanisms are the
phonon-induced displacement of the neon surface and the phonon-induced
modulation of the neon permittivity (dielectric constant). With a qubit
frequency increasing from 1 GHz to 10 GHz, the charge coherence time decreases
from about 366 s to 7 ms, and from about 27 s to 0.3 ms, respectively, limited
by the two mechanisms above. The calculated coherence times are at least one
order longer than the observed ones at ~6.4 GHz qubit frequency, suggesting
plenty of room for experimental improvement.

</details>


### [336] [Roto-translational optomechanics](https://arxiv.org/abs/2507.20905)
*M. Rademacher,A. Pontin,J. M. H. Gosling,P. F. Barker,M. Toroš*

Main category: quant-ph

TL;DR: 本文综述了光学捕获系统中物体的旋量运动，并探讨了其在基础物理学和高灵敏度传感器开发中的应用前景。


<details>
  <summary>Details</summary>
Motivation: 随着量子技术的发展，研究宏观物体的量子现象以及开发高灵敏度传感器变得越来越重要。光学捕获系统由于其与环境隔离性好、自由度少等优点，成为研究宏观量子现象的理想平台。特别是，研究物体的旋量运动对于拓展光学捕获的应用范围具有重要意义。

Method: 本文采用理论和实验方法对光学捕获系统中物体的旋量运动进行了研究，并对已有研究进行了综述。

Result: 该综述文章详细介绍了光学捕获系统中物体的旋量运动，包括其经典和量子描述，并讨论了其在基础物理学和高灵敏度传感器开发中的应用前景。

Conclusion: 这项工作为光学捕获领域中各向异性物体的旋量运动提供了全面的概述，包括其经典和量子描述。它讨论了这种运动在从基础物理学到高灵敏度传感器的各种应用中的潜力，并指出了未来的研究方向，例如创造非经典运动状态和量子限制扭矩传感。

Abstract: Levitated optomechanics, the interaction between light and small levitated
objects, is a new macroscopic quantum system that is being used as a testing
ground for fundamental physics and for the development of sensors with
exquisite sensitivity. The utility of this system, when compared to other
quantum optomechanical systems, is its extreme isolation from the environment
and, by the relatively few degrees of freedom that a levitated object has.
While work in the field has strongly focused on the three translational degrees
of freedom of this system, it has become increasingly important to understand
the induced rotational motion of levitated objects, particularly in optical
trapping fields, but also in magnetic and electric traps. These additional
three degrees of freedom, which are intrinsic to levitated systems, offer a new
set of optomechanical nonlinear interactions that lead to a rich and yet
largely unexplored roto-translational motion. The control and utilization of
these interactions promise to extend the utility of levitated optomechanics in
both fundamental studies and applications. In this review, we provide an
overview of levitated optomechanics, before focusing on the roto-translational
motion of optically levitated anisotropic objects. We first present a classical
treatment of this induced motion, bridging the gap between classical and
quantum formalisms. We describe the different types of roto-translational
motion for different particle shapes via their interaction with polarized
optical trapping fields. Subsequently, we provide an overview of the
theoretical and experimental approaches as well as applications that have
established this new field. The review concludes with an outlook of promising
experiments and applications, including the creation of non-classical states of
roto-translational motion, quantum-limited torque sensing and particle
characterization methods.

</details>


### [337] [$\mathcal{PT}$-symmetric two-photon quantum Rabi models](https://arxiv.org/abs/2507.20508)
*Yi-Cheng Wang,Jiong Li,Qing-Hu Chen*

Main category: quant-ph

TL;DR: 该研究精确地表征了 PT 对称非厄米双光子量子拉比模型，并发现了它们在能谱、对称性和动力学演化方面的关键区别。


<details>
  <summary>Details</summary>
Motivation: 研究 PT 对称非厄米双光子量子拉比模型，特别是偏差双光子量子拉比模型（btpQRM）和耗散双光子量子拉比模型（dtpQRM），以了解它们的行为并提供理论见解。

Method: 采用 Bogoliubov 变换推导出双光子量子拉比模型的精确解，并通过绝热近似分析了量子比特偏差的影响。研究中还使用了双正交保真度磁化率和 c 乘积来识别和分类能级交叉的性质。

Result: 在 btpQRM 中，研究识别出临界耦合强度下的能谱崩溃以及 PT 对称性破缺，这与由合并的本征态引起的 EPs 相关。研究还建立了 PT 对称性破缺区域与厄米双光子量子拉比模型（tpQRM）的重合简并点之间的直接对应关系。在 dtpQRM 中，尽管没有能谱崩溃，但存在 EPs 和 Juddian 型简并，其行为由奇偶守恒区分。研究比较了两种模型的动力学演化，揭示了由各自的非厄米谱结构决定的截然不同的稳态路径。

Conclusion: 该研究提供了对 PT 对称非厄米双光子量子拉比模型的精确表征，并可能为未来的实验实现提供理论见解。

Abstract: We investigate two non-Hermitian two-photon quantum Rabi models (tpQRM) that
exhibit $\mathcal{PT}$ symmetry: the biased tpQRM (btpQRM), in which the qubit
bias is purely imaginary, and the dissipative tpQRM (dtpQRM), where the
two-photon coupling is made imaginary to introduce dissipation. For both
models, we derive exact solutions by employing Bogoliubov transformations. In
the btpQRM, we identify spectral collapse at a critical coupling strength, with
accompanying $\mathcal{PT}$ symmetry breaking that correlates with exceptional
points (EPs) arising from coalescing eigenstates. We establish a direct
correspondence between $\mathcal{PT}$-broken regions and the doubly degenerate
points of the Hermitian tpQRM, and analyze the effects of qubit bias via an
adiabatic approximation. In the dtpQRM, although no spectral collapse occurs,
both EPs and Juddian-type degeneracies are present, with well-separated
behaviors distinguished by parity conservation. Through biorthogonal fidelity
susceptibility and c-product, we successfully identify and classify the nature
of these two types of level crossings. Finally, we compare the dynamical
evolution of both models, revealing fundamentally different pathways to steady
states governed by their respective non-Hermitian spectral structures. Our
results provide exact characterizations of $\mathcal{PT}$-symmetric
non-Hermitian tpQRMs and may offer theoretical insights for future experimental
realizations.

</details>


### [338] [Neural Importance Resampling: A Practical Sampling Strategy for Neural Quantum States](https://arxiv.org/abs/2507.20510)
*Eimantas Ledinauskas,Egidijus Anisimovas*

Main category: quant-ph

TL;DR: NIR是一种结合了重要性重采样和自回归提议网络的新型采样算法，可以高效、无偏地采样量子态，克服了现有MCMC和自回归方法的局限性，并在二维横向场伊辛模型上取得了与DMRG相当的结果。


<details>
  <summary>Details</summary>
Motivation: 当前的采样技术（如MCMC和自回归NQS）存在局限性，例如MCMC混合缓慢和需要手动调整，而自回归NQS则对架构有限制，并且难以强制执行对称性和构建基于行列式的多状态波函数。

Method: 介绍了一种结合了重要性重采样和单独训练的自回归提议网络的新采样算法，称为神经重要性重采样（NIR）。

Result: NIR能够高效且无偏地进行采样，同时不限制NQS的架构。它支持稳定且可扩展的训练，包括用于多状态NQS，并解决了MCMC和自回归方法的缺点。在二维横向场伊辛模型上的数值实验表明，NIR在挑战性条件下优于MCMC，并且其结果与密度矩阵重整化群（DMRG）方法相当。

Conclusion: NIR是一种稳健的替代方法，可用于变分NQS算法中的采样。

Abstract: Neural quantum states (NQS) have emerged as powerful tools for simulating
many-body quantum systems, but their practical use is often hindered by
limitations of current sampling techniques. Markov chain Monte Carlo (MCMC)
methods suffer from slow mixing and require manual tuning, while autoregressive
NQS impose restrictive architectural constraints that complicate the
enforcement of symmetries and the construction of determinant-based multi-state
wave functions. In this work, we introduce Neural Importance Resampling (NIR),
a new sampling algorithm that combines importance resampling with a separately
trained autoregressive proposal network. This approach enables efficient and
unbiased sampling without constraining the NQS architecture. We demonstrate
that NIR supports stable and scalable training, including for multi-state NQS,
and mitigates issues faced by MCMC and autoregressive approaches. Numerical
experiments on the 2D transverse-field Ising model show that NIR outperforms
MCMC in challenging regimes and yields results competitive with density matrix
renormalization group (DMRG) methods. Our results establish NIR as a robust
alternative for sampling in variational NQS algorithms.

</details>


### [339] [Quantum Portfolio Optimization with Expert Analysis Evaluation](https://arxiv.org/abs/2507.20532)
*Nouhaila Innan,Ayesha Saleem,Alberto Marchisio,Muhammad Shafique*

Main category: quant-ph

TL;DR: 本研究评估了VQE和QAOA在投资组合优化中的应用，发现虽然它们能最小化成本函数，但产生的投资组合在金融可行性方面存在不足。通过引入专家评估框架，研究强调了在将量子算法应用于金融决策时，结合金融专业人士的判断至关重要。


<details>
  <summary>Details</summary>
Motivation: 量子算法在解决金融领域复杂的组合问题（尤其是投资组合优化）方面受到了越来越多的关注。

Method: 本研究系统地评估了两种主要的变分量子方法：变分量子本征求解器（VQE）和量子近似优化算法（QAOA），在包括不同资产、ansatz架构和电路深度在内的各种实验设置下。

Result: 研究结果强调了算法性能与金融适用性之间的关键差异，并强调了在量子辅助决策流程中纳入专家判断的必要性。

Conclusion: 虽然量子算法（VQE和QAOA）在最小化成本函数方面表现出有效性，但由此产生的投资组合常常违反了诸如充分分散化和实际风险敞口等基本金融标准。为弥合计算优化与实际可行性之间的差距，本研究引入了一个专家评估框架，由金融专业人士评估量子优化投资组合的经济合理性和市场可行性。结果揭示了算法性能与金融适用性之间的关键差异，强调了在量子辅助决策流程中纳入专家判断的必要性。

Abstract: Quantum algorithms have gained increasing attention for addressing complex
combinatorial problems in finance, notably portfolio optimization. This study
systematically benchmarks two prominent variational quantum approaches,
Variational Quantum Eigensolver (VQE) and Quantum Approximate Optimization
Algorithm (QAOA), under diverse experimental settings, including different
asset universes, ansatz architectures, and circuit depths. Although both
methods demonstrate effective cost function minimization, the resulting
portfolios often violate essential financial criteria, such as adequate
diversification and realistic risk exposure. To bridge the gap between
computational optimization and practical viability, we introduce an Expert
Analysis Evaluation framework in which financial professionals assess the
economic soundness and the market feasibility of quantum-optimized portfolios.
Our results highlight a critical disparity between algorithmic performance and
financial applicability, emphasizing the necessity of incorporating expert
judgment into quantum-assisted decision-making pipelines.

</details>


### [340] [Next-Generation Quantum Neural Networks: Enhancing Efficiency, Security, and Privacy](https://arxiv.org/abs/2507.20537)
*Nouhaila Innan,Muhammad Kashif,Alberto Marchisio,Mohamed Bennai,Muhammad Shafique*

Main category: quant-ph

TL;DR: 本文提出了一个整合框架，结合了优化策略（如参数初始化、残差连接、架构探索）和防御机制，并利用量子联邦学习（QFL）来增强NISQ时代量子神经网络（QNN）的效率、安全性和隐私性，以解决 the barren plateaus 和 error propagation 等问题，并为可靠的量子机器学习应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 在噪声中等规模量子（NISQ）时代，开发可靠且安全的量子神经网络（QNN）面临着严峻的挑战。本研究旨在提供一个综合视角，以解决这些关键问题。

Method: 本文提出了一种综合框架，该框架整合了现有方法以提高量子神经网络（QNN）的效率、安全性和隐私性。具体而言，通过结合有效的参数初始化、残差量子连接和系统化的量子架构探索等优化策略，来缓解 the barren plateaus 和 error propagation 等问题。此外，该方法还融入了现有的对抗攻击防御机制。最后，采用了量子联邦学习（QFL）以支持分布式量子系统间的隐私保护协同训练。

Result: 通过整合优化策略（如参数初始化、残差连接、架构探索）和防御机制，并结合量子联邦学习（QFL），本框架有望提高QNN的效率、安全性和隐私性，解决 the barren plateaus 和 error propagation 等问题，并实现隐私保护的协同训练，从而增强QNN的鲁棒性和实际应用能力。

Conclusion: 本文提出的综合框架通过整合现有方法，旨在提高量子神经网络（QNN）在噪声中等规模量子（NISQ）时代下的效率、安全性和隐私性，以应对在开发可靠和安全QNN方面面临的关键挑战。通过结合有效的参数初始化、残差量子连接和系统化的量子架构探索等优化策略，解决了 the barren plateaus 和 error propagation 等问题。此外，该方法还融入了现有的对抗攻击防御机制。最终，框架采用了量子联邦学习（QFL）以支持分布式量子系统间的隐私保护协同训练，旨在增强QNN的鲁棒性和实际应用能力，为金融、医疗和网络安全等领域的可靠量子增强机器学习应用奠定基础。

Abstract: This paper provides an integrated perspective on addressing key challenges in
developing reliable and secure Quantum Neural Networks (QNNs) in the Noisy
Intermediate-Scale Quantum (NISQ) era. In this paper, we present an integrated
framework that leverages and combines existing approaches to enhance QNN
efficiency, security, and privacy. Specifically, established optimization
strategies, including efficient parameter initialization, residual quantum
circuit connections, and systematic quantum architecture exploration, are
integrated to mitigate issues such as barren plateaus and error propagation.
Moreover, the methodology incorporates current defensive mechanisms against
adversarial attacks. Finally, Quantum Federated Learning (QFL) is adopted
within this framework to facilitate privacy-preserving collaborative training
across distributed quantum systems. Collectively, this synthesized approach
seeks to enhance the robustness and real-world applicability of QNNs, laying
the foundation for reliable quantum-enhanced machine learning applications in
finance, healthcare, and cybersecurity.

</details>


### [341] [Fourth-order quantum master equations reveal that spin-phonon decoherence undercuts long magnetization relaxation times in single-molecule magnets](https://arxiv.org/abs/2507.20716)
*Alessandro Lunghi*

Main category: quant-ph

TL;DR: 研究发现，虽然自旋-声子相互作用导致磁弛豫缓慢，但一种新的双声子机制会使退相干时间缩短至10纳秒以下。


<details>
  <summary>Details</summary>
Motivation: 研究自旋-声子相互作用如何影响退相干时间。

Method: 本研究将四阶量子主方程扩展到包含退相干项，并描述了多达两个声子过程对自旋动力学的完整影响。研究人员以从头算的方式对具有大磁矩阻塞温度的单分子磁体进行了数值实现。

Result: 尽管强的轴向磁各向异性确保了在77 K下接近秒级的缓慢磁弛豫，但由于一种新颖的双声子纯退相干机制，Kramers双重态的叠加的相干时间不到10 ns。

Conclusion: 自旋-声子相互作用是固态系统中磁弛豫的驱动因素，但其对退相干时间的影响证据很少。本研究将四阶量子主方程扩展到包含退相干项，并描述了多达两个声子过程对自旋动力学的完整影响。研究人员以从头算的方式对具有大磁矩阻塞温度的单分子磁体进行了数值实现。结果表明，尽管强的轴向磁各向异性确保了在77 K下接近秒级的缓慢磁弛豫，但由于一种新颖的双声子纯退相干机制，Kramers双重态的叠加的相干时间不到10 ns。

Abstract: Spin-phonon interaction is known to drive magnetic relaxation in solid-state
systems, but little evidence is available on how it affects coherence time.
Here we extend fourth-order quantum master equations to account for coherence
terms and describe the full effect of up to two-phonon processes on spin
dynamics. We numerically implement this method fully ab initio for a
single-molecule magnet with large magnetization blocking temperature and show
that while strong axial magnetic anisotropy ensures slow magnetic relaxation
approaching seconds at 77 K, the superposition of Kramers doublets is coherent
for less than 10 ns due to a novel two-phonon pure dephasing mechanism.

</details>


### [342] [Alternative threshold function for Bayesian Optimization of Variational Quantum Circuits](https://arxiv.org/abs/2507.20570)
*Shreyas Dillon*

Main category: quant-ph

TL;DR: 改进EMICoRe VQE算法，放宽置信区域阈值，在Ising哈密顿量测试中表现更佳。


<details>
  <summary>Details</summary>
Motivation: 为了解决原始EMICoRe VQE算法在处理自然预测能量波动时可能过于严苛的问题，提出了一种改进的置信区域阈值，使其更加宽松，能够更好地适应和利用预测能量的自然波动，从而提升优化性能。

Method: 提出一种改进的EMICoRe VQE算法，引入了一个新的、更宽松的置信区域阈值，该阈值同时考虑了高斯过程（GP）先验方差和模型预测能量在一定迭代次数内的变化。通过与原始EMICoRe模型进行比较，分析了近似基态能量的准确性以及阈值在优化过程中的演变。

Result: 改进后的EMICoRe VQE算法在Ising哈密顿量基准测试中表现出优于原始EMICoRe算法的性能，在更复杂的优化场景中也表现出相似的性能。研究分析了近似基态能量的准确性，并讨论了未来优化阈值参数以获得更好性能和更广泛系统适用性的可能性。

Conclusion: 该研究提出了一种改进的EMICoRe VQE算法，该算法通过引入一个更宽松的置信区域阈值，并考虑了高斯过程先验方差和模型预测能量变化，在Ising哈密顿量基准测试和其他更复杂的优化场景中，与原始EMICoRe模型相比，展示了改进的性能。研究还讨论了优化新阈值参数以进一步提升性能和适用性的潜力。

Abstract: In this paper, we propose an expansion of the Expected Maximum Improvement
over Confident Regions (EMICoRe) Variational Quantum Eigensolver (VQE) -- a
technique advanced by Nicoli et al., which utilizes both quantum and classical
components to approximate the ground state of a quantum system -- by
introducing an alternative threshold for EMICoRe's Confident Region that
depends on both the Gaussian process (GP) prior variance and the model's change
in predicted energy over a set number of iterations. This modification is a
more lenient threshold for the Confident Region and accounts for natural
fluctuations in the predicted energy that EMICoRe punishes by eliminating the
exploratory benefits presented by the Confident Region. We test both algorithms
with the original EMICoRe model as a baseline and our results suggest
improvement over EMICoRe's state-of-the-art results for a common benchmark for
VQEs, the Ising Hamiltonian, and similar performance for more complex
optimization regimes. We analyze the accuracy in approximated ground state
energy and how the threshold evolves during optimization to compare the EMICoRe
model with the proposed alternative. After comparison, we discuss the potential
optimization of the degrees of freedom present in the new threshold for better
performance and a more varied choice of system to be approximated.

</details>


### [343] [Real-Space Chemistry on Quantum Computers: A Fault-Tolerant Algorithm with Adaptive Grids and Transcorrelated Extension](https://arxiv.org/abs/2507.20583)
*César Feniou,Christopher Cherfan,Julien Zylberman,Baptiste Claudon,Jean-Philip Piquemal,Emmanuel Giner*

Main category: quant-ph

TL;DR: 通过使用非均匀、分子自适应网格来提高量子化学计算的效率和精度，解决了现有均匀离散化方案的资源浪费问题。


<details>
  <summary>Details</summary>
Motivation: 现有方案采用均匀离散化，导致在高密度区域捕获电子-核尖点所需的精度会过度采样低密度区域，从而浪费计算资源。

Method: 部署非均匀、分子自适应网格，将点集中在高电子密度区域。使用这些网格的 Voronoi 分区，将分子哈密顿量表示为厄米形式和反相关、等谱形式，消除了库仑奇点并产生无尖点特征函数。这两种形式都自然地嵌入到量子本征值求解器中：厄米量子相位估计算法 (QPE) 和最近用于其非厄米、反相关对应物的广义量子本征值估计算法 (QEVE) 协议。

Result: 数值验证了在基准系统上，这种非启发式从头算框架为在量子硬件上进行准确的基态化学计算提供了一条有前景的路径。

Conclusion: 该非启发式从头算框架为在量子硬件上进行精确的化学基态计算提供了一条有前景的路径。

Abstract: First-quantized, real-space formulations of quantum chemistry on quantum
computers are appealing: qubit count scales logarithmically with spatial
resolution, and Coulomb operators achieve quadratic instead of quartic
computational scaling of two-electron interactions. However, existing schemes
employ uniform discretizations, so the resolution required to capture
electron-nuclear cusps in high-density regions oversamples low-density regions,
wasting computational resources. We address this by deploying non-uniform,
molecule-adaptive grids that concentrate points where electronic density is
high. Using Voronoi partitions of these grids, the molecular Hamiltonian is
expressed in a Hermitian form and in a transcorrelated, isospectral form that
eliminates Coulomb singularities and yields cusp-free eigenfunctions. Both
formulations slot naturally into quantum eigenvalue solvers: Hermitian Quantum
Phase Estimation (QPE) and the recent generalised Quantum Eigenvalue Estimation
(QEVE) protocol for its non-Hermitian, transcorrelated counterpart. Numerical
validation on benchmark systems confirms that this non-heuristic ab initio
framework offers a promising path for accurate ground-state chemistry on
quantum hardware.

</details>


### [344] [When More Light Means Less Quantum: Modeling Bell Inequality Degradation from Accidental Counts](https://arxiv.org/abs/2507.20596)
*Piotr Mironowicz,Mohamed Bourennane*

Main category: quant-ph

TL;DR: 意外计数会削弱量子非局域性，影响量子技术性能。通过噪声模型和实验数据分析，本研究为在提高光源亮度的同时保持量子非局域性提供了优化方法。


<details>
  <summary>Details</summary>
Motivation: 研究意外计数（非来自真正纠缠光子对的探测事件）如何影响光子实验中贝尔不等式的违反情况，特别是在高激光泵浦功率下，这会限制贝尔违反的强度以及量子协议（如设备无关的量子随机数生成和量子密钥分发）的性能。

Method: 提出一个简单的噪声模型，将贝尔值与泵浦强度联系起来，并使用 SPDC 实验数据进行拟合，以准确预测贝尔值。

Result: 该模型能够准确预测不同泵浦设置下的贝尔值，表明意外计数是影响贝尔不等式违反的重要因素，并为优化光源亮度以提高量子技术性能提供了指导。

Conclusion: 该研究为在保持量子非局域性的同时优化光源亮度提供了实际指导，对高码率、安全的量子技术具有直接意义。

Abstract: We investigate how accidental counts, the detection events not originating
from genuine entangled photon pairs, impact the observed violation of Bell
inequalities in photonic experiments. These false coincidences become
increasingly significant at higher laser pump powers, limiting the strength of
Bell violations and thus the performance of quantum protocols such as
device-independent quantum random number generation and quantum key
distribution. We propose a simple noise model that quantitatively links the
Bell value to the pump strength. Using experimental data from recent SPDC-based
Bell tests, we fit the model to a Bell expression, and demonstrate accurate
prediction of Bell values across a range of pump settings. Our results provide
practical guidance for optimizing source brightness while preserving quantum
nonlocality, with direct implications for high-rate, secure quantum
technologies.

</details>


### [345] [Fourier space readout method for efficiently recovering functions encoded in quantum states](https://arxiv.org/abs/2507.20599)
*Xinchi Huang,Hirofumi Nishi,Yoshifumi Kawada,Tomofumi Zushi,Yu-ichiro Matsushita*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Applying quantum computing in the computer-aided engineering (CAE) problems
are highly expected since quantum computers yield potential exponential
speedups for the operations between extremely large matrices and vectors.
Although efficient quantum algorithms for the above problems have been
intensively investigated, it remains a crucial task to extract all the
grid-point values encoded in the prepared quantum states, which was believed to
eliminate the achieved quantum advantage. In this paper, we propose a
quantum-classical hybrid Fourier space readout (FSR) method to efficiently
recover the underlying function from its corresponding quantum state. We
provide explicit quantum circuits, followed by theoretical and numerical
discussions on its complexity. In particular, the complexity on quantum
computers has only a logarithmic dependence on the grid number, while the
complexity on classical computers has a linear dependence on the number of
target points instead of the grid number. Our result implies that the achieved
quantum speedups are not necessarily ruined when we read out the solutions to
the CAE problems.

</details>


### [346] [Random measurements are almost maximally incompatible](https://arxiv.org/abs/2507.20600)
*Andreas Bluhm,Cécilia Lancien,Ion Nechita*

Main category: quant-ph

TL;DR: 该研究量化了随机量子测量的最大不兼容性，发现随机二分测量和随机基测量接近最大不兼容。


<details>
  <summary>Details</summary>
Motivation: 研究随机量子测量的兼容性问题，量化此类系统中可用的最大不兼容性。

Method: 使用不兼容性证明来证明不兼容性，并结合随机矩阵和自由概率的工具。

Result: 证明了随机二分测量和随机基测量在合适的参数选择下接近最大不兼容。

Conclusion: 随机量子测量是不兼容的，并且在合适的参数选择下，随机二分测量和随机基测量接近最大不兼容。

Abstract: In this work, we investigate the incompatibility of random quantum
measurements. Most previous work has focused on characterizing the maximal
amount of white noise that any fixed number of incompatible measurements with a
fixed number of outcomes in a fixed dimension can tolerate before becoming
compatible. This can be used to quantify the maximal amount of incompatibility
available in such systems. The present article investigates the incompatibility
of several classes of random measurements, i.e., the generic amount of
incompatibility available. In particular, we show that for an appropriate
choice of parameters, both random dichotomic projective measurements and random
basis measurements are close to being maximally incompatible. We use the
technique of incompatibility witnesses to certify incompatibility and combine
it with tools from random matrices and free probability.

</details>


### [347] [Jones Matrix Reconstruction with Undetected Photons](https://arxiv.org/abs/2507.20617)
*Gaytri Arya,Paolo Bianchini,Alberto Diaspro*

Main category: quant-ph

TL;DR:  QIUP框架下的偏振断层扫描方案，直接从干涉可见度和相位移中提取样本参数，减少未知数，实现稳健、无创表征。


<details>
  <summary>Details</summary>
Motivation: 在量子成像中，探测和检测发生在不同波长，需要一种能够稳健、无创地表征复杂样本的方法。

Method: 提出了一种在QIUP框架内的理论偏振断层扫描方案，直接从测量的干涉可见度和相位移中提取关键样本参数。

Result: 该方法资源高效，适用于量子成像。

Conclusion: 该方法通过建立透射幅度与相干项的恒定绑定，减少了重构样本琼斯矩阵所需的未知数数量，实现了对复杂样本的稳健、无创表征。

Abstract: We present a theoretical polarization tomography scheme within the QIUP
framework that directly extracts key sample parameters from measured
interference visibilities and phase shifts. This approach establishes constant
bindings for transmission amplitudes and coherence terms, thereby reducing the
number of unknowns required to reconstruct the sample's Jones matrix. Our
method is resource-efficient and well-suited for quantum imaging where probing
and detection occur at different wavelengths, enabling robust, non-invasive
characterization of complex samples.

</details>


### [348] [A digital Rydberg simulation of dynamical quantum phase transitions in the Schwinger model](https://arxiv.org/abs/2507.20625)
*Domenico Pomarico,Federico Dell'Anna,Riccardo Cioli,Saverio Pascazio,Francesco V. Pepe,Paolo Facchi,Elisa Ercolessi*

Main category: quant-ph

TL;DR: 数字噪声里德堡原子平台成功模拟了Z3开关模型，观察到多个动力学量子相变。


<details>
  <summary>Details</summary>
Motivation: 旨在数字噪声里德堡原子平台上模拟Z3开关模型，以期观察多个动力学量子相变。

Method: 通过利用对称性指导的编码和电路压缩程序来实现长时间动力学模拟，并对由负质量参数决定的哈密顿量进行研究，该哈密顿量能够驱动狄拉克真空的演化，从而产生与重子态之间的共振Rabi振荡。

Result: 即使在包含组合噪声源的情况下，研究也能够清楚地检测到多个动力学相变，并且种群浓度表现出具有可忽略不计的失谐态波动的振荡现象。

Conclusion: 该研究成功在数字噪声里德堡原子平台上模拟了Z3开关模型的猝灭动力学，并观察到了多个动力学量子相变。

Abstract: We present the simulation of the quench dynamics of the Z3 Schwinger model,
that describes an approximation of one-dimensional Quantum Electrodynamics, on
a digital noisy Rydberg atom platform, aiming at the observation of multiple
dynamical quantum phase transitions. In order to reach long-time dynamics, we
exploit an enconding dictated by the symmetries, combined with a circuit
compression procedure. We focus on a quench that evolves the Dirac vacuum by
means of a Hamiltonian depending on a negative mass parameter. This leads to
resonant Rabi oscillations between the Dirac vacuum and mesonic states. The
population concentration exhibits oscillations with negligible fluctuations of
detuned states also with the inclusion of combined noise sources, from which we
can clearly detect multiple dynamical phase transitions.

</details>


### [349] [Optimizing Tensor Network Partitioning using Simulated Annealing](https://arxiv.org/abs/2507.20667)
*Manuel Geiger,Qunsheng Huang,Christian B. Mendl*

Main category: quant-ph

TL;DR: 本文提出了一种新的模拟退火算法，用于优化分布式张量网络收缩的分区策略，显著降低了计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 在经典模拟量子系统时，随着系统尺寸的增加，收缩大型张量网络变得计算密集。需要更有效的分布式内存架构和分区策略来解决此问题。

Method: 本文提出了一种基于模拟退火的算法，通过迭代优化分区策略来最小化张量网络收缩的总运算量。

Result: 与朴素分区相比，该算法在MQT Bench电路上实现了8倍的计算成本降低和8倍的内存成本降低。

Conclusion: 本文提出了一种基于模拟退火的算法，通过优化分区策略来最小化张量网络收缩的总运算量，从而降低求解时间。

Abstract: Tensor networks have proven to be a valuable tool, for instance, in the
classical simulation of (strongly correlated) quantum systems. As the size of
the systems increases, contracting larger tensor networks becomes
computationally demanding. In this work, we study distributed memory
architectures intended for high-performance computing implementations to solve
this task. Efficiently distributing the contraction task across multiple nodes
is critical, as both computational and memory costs are highly sensitive to the
chosen partitioning strategy. While prior work has employed general-purpose
hypergraph partitioning algorithms, these approaches often overlook the
specific structure and cost characteristics of tensor network contractions. We
introduce a simulated annealing-based method that iteratively refines the
partitioning to minimize the total operation count, thereby reducing
time-to-solution. The algorithm is evaluated on MQT Bench circuits and achieves
an 8$\times$ average reduction in computational cost and an 8$\times$ average
reduction in memory cost compared to a naive partitioning.

</details>


### [350] [Quantum Circuit Caches and Compressors for Low Latency, High Throughput Computing](https://arxiv.org/abs/2507.20677)
*Ioana Moflic,Alan Robertson,Simon J. Devitt,Alexandru Paler*

Main category: quant-ph

TL;DR: 通过使用量子电路缓存和压缩器，可以解决大规模量子程序执行的延迟问题。


<details>
  <summary>Details</summary>
Motivation: 现有的高层经典表示方法对于执行大规模量子程序时效率低下，内存占用大，无法满足实时执行的要求。

Method: 提出利用高层量子电路缓存和压缩器来优化量子程序的执行。

Result: 缓存和压缩器可以将自动转译极其庞大的量子电路的延迟降低五个数量级。

Conclusion: 通过使用量子电路缓存和压缩器，可以实现比现有方法低五个数量级的延迟。

Abstract: Utility-scale quantum programs contain operations on the order of $>10^{15}$
which must be prepared and piped from a classical co-processor to the control
unit of the quantum device. The latency of this process significantly increases
with the size of the program: existing high-level classical representations of
quantum programs are typically memory intensive and do not na\"ively
efficiently scale to the degree required to execute utility-scale programs in
real-time. To combat this limitation, we propose the utilization of high-level
quantum circuit caches and compressors. The first save on the time associated
with repetitive tasks and sub-circuits, and the latter are useful for
representing the programs/circuits in memory-efficient formats. We present
numerical evidence that caches and compressors can offer five orders of
magnitude lower latencies during the automatic transpilation of extremely large
quantum circuits.

</details>


### [351] [Optimization and Synthesis of Quantum Circuits with Global Gates](https://arxiv.org/abs/2507.20694)
*Alejandro Villoria,Henning Basold,Alfons Laarman*

Main category: quant-ph

TL;DR: 本研究提出了一种基于ZX-calculus的算法，利用离子阱硬件的全局Mølmer–Sørensen门优化量子电路编译，减少了所需门数量，并在实际硬件考量下提高了性能。


<details>
  <summary>Details</summary>
Motivation: 为了在量子计算堆栈中解决将量子电路编译以适应硬件限制的问题，并利用离子阱等硬件平台的特殊性质来优化量子电路执行成本，特别是通过全局相互作用（如全局Mølmer–Sørensen门）来减少量子门（尤其是非Clifford门或CNOT门）的数量。

Method: 通过ZX-calculus和专门的电路提取例程，设计并实现了一个算法，该算法能够将任意量子电路编译成使用全局门作为纠缠操作的电路，同时优化所需的全局交互次数。

Result: 该算法在多种电路上进行了基准测试，并与朴素算法和Qiskit优化器进行了比较，证明了其在当前最先进的硬件考量下能够改进电路性能。

Conclusion: 该算法基于ZX-calculus，并使用专门的电路提取例程，将纠缠门组合成全局Mølmer–Sørensen门，以优化和综合量子电路。

Abstract: Compiling quantum circuits to account for hardware restrictions is an
essential part of the quantum computing stack. Circuit compilation allows us to
adapt algorithm descriptions into a sequence of operations supported by real
quantum hardware, and has the potential to significantly improve their
performance when optimization techniques are added to the process. One such
optimization technique is reducing the number of quantum gates that are needed
to execute a circuit. For instance, methods for reducing the number of
non-Clifford gates or CNOT gates from a circuit is an extensive research area
that has gathered significant interest over the years. For certain hardware
platforms such as ion trap quantum computers, we can leverage some of their
special properties to further reduce the cost of executing a quantum circuit in
them. In this work we use global interactions, such as the Global
M{\o}lmer-S{\o}rensen gate present in ion trap hardware, to optimize and
synthesize quantum circuits. We design and implement an algorithm that is able
to compile an arbitrary quantum circuit into another circuit that uses global
gates as the entangling operation, while optimizing the number of global
interactions needed. The algorithm is based on the ZX-calculus and uses an
specialized circuit extraction routine that groups entangling gates into Global
M{\o}lmer-S{\o}rensen gates. We benchmark the algorithm in a variety of
circuits, and show how it improves their performance under state-of-the-art
hardware considerations in comparison to a naive algorithm and the Qiskit
optimizer.

</details>


### [352] [Thermodynamic Constraints on the Emergence of Intersubjectivity in Quantum Systems](https://arxiv.org/abs/2507.20736)
*Alessandro Candeloro,Tiago Debarba,Felix C. Binder*

Main category: quant-ph

TL;DR: 量子测量需要热力学资源，这是由热力学第三定律决定的。本文研究了有限资源对多观察者测量过程的共同主观性的影响，并提出了偏差度量和可达到的边界。


<details>
  <summary>Details</summary>
Motivation: 研究了在热力学第三定律约束下，有限资源对多观察者测量过程的共同主观性产生的影响。

Method: 从热力学第三定律出发，推导了有限热力学资源如何影响理想共同主观性的可实现性。

Result: 建立了关于理想共同主观性的“禁止定理”，并提出了一个偏差度量来考虑有限资源的影响。此外，还展示了共同主观性和偏差的可达到的边界，并且证明了可以通过冷却或粗粒化来近似理想的共同主观性。

Conclusion: 通过冷却或粗粒化，即使在有限资源的情况下，也可以近似理想的共同主观性。该工作将量子热力学与以共同主观性为形式的经典性的出现联系起来。

Abstract: Ideal quantum measurement requires divergent thermodynamic resources. This is
a consequence of the third law of thermodynamics, which prohibits the
preparation of the measurement pointer in a fully erased, pure state required
for the acquisition of perfect, noiseless measurement information. In this
work, we investigate the consequences of finite resources in the emergence of
intersubjectivity as a model for measurement processes with multiple observers.
Here, intersubjectivity refers to a condition in which observers agree on the
observed outcome (agreement), and their local random variables exactly
reproduce the original random variable for the system observable (probability
reproducibility). While agreement and reproducibility are mutually implied in
the case of ideal measurement, finite thermodynamic resources constrain each of
them. Starting from the third law of thermodynamics, we derive how the
achievability of ideal intersubjectivity is affected by restricted
thermodynamic resources. Specifically, we establish a no-go theorem concerning
perfect intersubjectivity and present a deviation metric to account for the
influence of limited resources. We further present attainable bounds for the
agreement and bias that are exclusively dependent on the initial state of the
environment. In addition, we show that either by cooling or coarse-graining, we
can approximate ideal intersubjectivity even with finite resources. This work
bridges quantum thermodynamics and the emergence of classicality in the form of
intersubjectivity.

</details>


### [353] [Continuity Norm Framework for the Evolution of Nonsingular Matrices](https://arxiv.org/abs/2507.20742)
*L. Yildiz,D. Kayki,E. Gudekli*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Matrix theory, foundational in diverse fields such as mathematics, physics,
and computational sciences, typically categorizes matrices based strictly on
their invertibility-determined by a sharply defined singular or nonsingular
classification. However, such binary classifications become inadequate in
describing matrices whose elements vary continuously over time, thereby
transitioning through intermediate states near singular configurations. To
address this fundamental limitation, we develop a rigorous and original
mathematical theory termed Continuity Norm Framework for the Evolution of
Nonsingular Matrices. Within this framework, we introduce a novel mathematical
structure enabling continuous and differentiable transitions between singular
and nonsingular matrix states, explicitly governed by a specialized continuity
norm and evolution operators derived through a well-defined differential
formulation. Our theoretical formalism rigorously quantifies the proximity of a
matrix to singularity, alongside its temporal evolution, through precisely
constructed functional relationships involving determinants and their time
derivatives. Furthermore, we elucidate the direct applicability and relevance
of our approach to physical systems by demonstrating how our formalism can
seamlessly describe continuous quantum state transitions-scenarios frequently
encountered but insufficiently captured by existing matrix theory. The theory
presented herein is meticulously constructed to maintain mathematical
exactitude, comprehensive rigor, and broad accessibility, bridging advanced
mathematical innovation and clear interpretability for the wider scientific
community.

</details>


### [354] [Quantum circuit evolutionary framework applied on set partitioning problem](https://arxiv.org/abs/2507.20777)
*Bruno Oziel Fernandez,Rodrigo Bloot,Marcelo Moret*

Main category: quant-ph

TL;DR: 量子算法在优化问题上潜力巨大，但现有方法常遇收敛停滞。本文提出一种新的变分量子算法框架——可变拓扑电路，并引入伪反绝热演化项，有效克服了收敛停滞问题，且无需经典优化器，在集合划分问题上效果显著。


<details>
  <summary>Details</summary>
Motivation: 为了克服变分算法中存在的收敛停滞问题，并为当前设备和未来更大规模的应用寻找更有效的优化算法。

Method: 提出了一种基于可变拓扑电路的框架，包含两种方法：一种是基于无初值哒的演化方法，另一种是引入受哈密顿量物理启发并包含伪反绝热演化项的初值哒。

Result: 在有噪声和无噪声的情况下，与变分量子特征求解器（Variational Quantum Eigensolver）相比，所提出的基于可变拓扑电路的框架（特别是伪反绝热演化项策略）在集合划分问题上表现出非常有希望的结果，能够避免收敛停滞。

Conclusion: 所提出的基于可变拓扑电路的框架，特别是采用伪反绝热演化项的策略，在解决集合划分问题的实例时，表现出非常有希望的结果，能够有效避免收敛停滞，并且无需经典优化器，为更大规模的整数优化问题提供了新的途径。

Abstract: Quantum algorithms are of great interest for their possible use in
optimization problems. In particular, variational algorithms that use classical
counterparts to optimize parameters hold promise for use in currently existing
devices. However, convergence stagnation phenomena pose a challenge for such
algorithms. Seeking to avoid such difficulties, we present a framework based on
circuits with variable topology with two approaches, one based on ansatz-free
evolutionary method known from literature and the other using an introduction
of an ansatz with circuital structure inspired by the physics of the
Hamiltonian related to the problem, considering a, named here,
pseudo-counterdiabatic evolutionary term. The efficiency of the proposed
framework was tested on several instances of the set partitioning problem. The
two approaches were compared with the Variational Quantum Eigensolver in noisy
and non-noisy scenarios. The results demonstrated that optimization using
circuits with variable topology presented very encouraging results. Notably,
the strategy employing a pseudo-counterdiabatic evolutionary term exhibited
remarkable performance, avoiding convergence stagnation in most instances
considered. This framework circumvents the need for classical optimizers, and,
as a consequence, this procedure based on circuits with variable topology
indicates an interesting path in the search for algorithms to solve integer
optimization problems targeting efficient applications in larger-scale
scenarios.

</details>


### [355] [Superconducting flux concentrator coils for levitation of particles in the Meissner state](https://arxiv.org/abs/2507.20795)
*Robert Smit,Martijn Janse,Eli van der Bent,Thijmen de Jong,Kier Heeck,Jaimy Plugge,Tjerk Oosterkamp,Bas Hensen*

Main category: quant-ph

TL;DR: 文章介紹了一種利用超導線芯增強磁約束的方法，雖然成功約束了超導粒子，但發現磁通量會在特定條件下引起阻尼效應，並提出了解決該問題的潛在策略。


<details>
  <summary>Details</summary>
Motivation: 解決磁性約束懸浮超導體中弱約束勢的限制，以研究大質量極限下的量子力學。

Method: 利用超導線芯集中磁通量於極小體積，並將其置於抗赫姆霍茲線圈中，用於約束超導粒子。採用金剛石 NV 中心磁力計測量線圈在高電流運作後的殘餘磁場。

Result: 成功展示了利用超導線芯的抗赫姆霍茲線圈配置約束超導粒子的能力，並觀察到線芯中的磁通量導致的快速阻尼現象，這與 NV 中心磁力計測量到的線圈殘餘磁場結果一致。

Conclusion: 使用超導線芯的抗赫姆霍茲線圈配置能夠約束超導粒子，但當局部場超過 lc1 時，線芯中的磁通量會在穩定後快速阻尼懸浮粒子的運動。這是在低溫磁場約束和量子操縱中的一個重要挑戰。

Abstract: Magnetic levitation of superconductors is a promising platform to study
quantum mechanics in the large-mass limit. One major limitation is the weak
trapping potential, which results in low vibrational eigenfrequencies and
increased susceptibility to low-frequency noise. While generating strong
magnetic fields is relatively straightforward, creating a tightly confined
harmonic potential - essentially achieving a large magnetic field gradient -
remains a significant challenge. In this work, we demonstrate a potential
solution using superconducting cores that concentrate magnetic flux into
arbitrarily small volumes. We show the ability to trap superconducting
particles using an anti-Helmholtz coil configuration incorporating these cores.
However, we observe rapid damping of the levitated particle motion due to flux
trapping within the cores, occurring once the lower critical field is exceeded
locally. To investigate this mechanism, we employ diamond NV center
magnetometry and detect substantial remanent fields persisting after
high-current operation of the coils. Finally, we discuss possible strategies to
mitigate this effect and improve the levitation properties.

</details>


### [356] [A Variational Quantum Algorithm for Entanglement Quantification](https://arxiv.org/abs/2507.20813)
*Lucas Friedrich,Marcos L. W. Basso,Alberto B. P. Junior,Joab M. Varela,Leandro Morais,Rafael Chaves,Jonas Maziero*

Main category: quant-ph

TL;DR: 介绍了一种新的变分量子算法，用于高效、可扩展地量化量子纠缠和其他量子资源。


<details>
  <summary>Details</summary>
Motivation: 为了应对检测和量化纠缠的挑战，而纠缠是量子信息科学中的一种基础资源。

Method: 提出了一种受Uhlmann定理启发的变分量子算法，用于量化一般量子态的Bures纠缠，该方法能够自然地扩展到其他量子资源，如Genuine多方纠缠、量子散度、量子相干性和总相关性，同时还能重构最近的自由态。

Result: 该算法需要多项式数量的辅助量子比特和电路深度，相对于系统大小、维度和自由态基数而言，使其能够进行可扩展的实际实现。

Conclusion: 该算法为量化量子资源提供了一个多功能且高效的框架，并通过多个应用进行了演示。

Abstract: Quantum entanglement is a foundational resource in quantum information
science, underpinning applications across physics. However, detecting and
quantifying entanglement remains a significant challenge. Here, we introduce a
variational quantum algorithm inspired by Uhlmann's theorem to quantify the
Bures entanglement of general quantum states, a method that naturally extends
to other quantum resources, including genuine multipartite entanglement,
quantum discord, quantum coherence, and total correlations, while also enabling
reconstruction of the closest free states. The algorithm requires a polynomial
number of ancillary qubits and circuit depth relative to the system size,
dimensionality, and free state cardinality, making it scalable for practical
implementations. Thus, it provides a versatile and efficient framework for
quantifying quantum resources, demonstrated through several applications.

</details>


### [357] [Phase structure of below-threshold harmonics in aligned molecules: a few-level model system](https://arxiv.org/abs/2507.20829)
*Samuel Schöpa,Falk-Erik Wiechmann,Franziska Fennel,Dieter Bauer*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We utilize few-level model systems to analyze the polarization and phase
properties of below-threshold harmonics (BTH) in aligned molecules. In a
two-level system (TLS), we find that the phase of emitted harmonics undergoes a
distinct change. For harmonics with photon energies below the transition
between the dominant field-dressed states, the phase alternates by $\pi$
between successive odd harmonic orders but remains constant above. Exploiting
this behavior, we construct a four-level model composed of two uncoupled TLS
subsystems aligned along orthogonal directions. We demonstrate that with
selected transition frequencies lower-order harmonics follow the polarization
of the linearly polarized driving field while higher-order harmonics exhibit a
mirrored polarization. The model predicts that aligned systems with orthogonal
transition dipoles may show analogous phase and polarization features in the
BTH regime.

</details>


### [358] [Efficient LCU block encodings through Dicke states preparation](https://arxiv.org/abs/2507.20887)
*Filippo Della Chiara,Martina Nibbi,Yizhi Shen,Roel Van Beeumen*

Main category: quant-ph

TL;DR: 提出FOQCS-LCU框架，通过检查矩阵和Dicke状态制备，显著减少了块编码的门开销，尤其是在CNOT计数方面，比传统LCU有数量级改进。


<details>
  <summary>Details</summary>
Motivation: QSVT等算法工具的出现，使得高效构建块编码（其基本输入模型）变得越来越重要，但现有方法（如LCU）在SELECT oracle的实现上存在显著的门开销问题。

Method: 提出了一种名为FOQCS-LCU的紧凑型LCU形式，它利用检查矩阵形式主义来实现常数深度的SELECT oracle，并设计了一类参数化的Dicke状态制备例程以降低状态制备的开销。

Result: FOQCS-LCU框架能够为Heisenberg和自旋玻璃哈密顿量等代表性的自旋模型构建显式块编码电路，并通过数值基准测试验证了其效率，CNOT计数相比传统LCU有数量级的提升。

Conclusion: FOQCS-LCU通过利用检查矩阵形式主义，使用常数深度的SELECT oracle和线性数量的单控制Pauli门及辅助位，以及Dicke状态制备例程，实现了高效的块编码，并将CNOT计数降低了一个数量级。

Abstract: As algorithmic tools exemplified by the Quantum Singular Value Transformation
(QSVT) continue to emerge as a unifying framework for diverse quantum speedups,
the efficient construction of block encodings--their fundamental input
model--becomes increasingly crucial. However, devising explicit block encoding
circuits remains a well-recognized and challenging problem. One of the most
widely studied and versatile approaches to block encoding is the Linear
Combination of Unitaries (LCU). Despite its generality, the practical use of
LCU is often limited by significant gate overhead, primarily stemming from the
multi-controlled gates required in the SELECT oracle. We introduce a compact
LCU formulation, dubbed FOQCS-LCU, which leverages the check matrix formalism
to implement a constant-depth SELECT oracle using a linear number of singly
controlled Pauli gates and ancillae. We demonstrate that, by exploiting the
structure of the problem, the cost of the state preparation oracle can also be
substantially reduced. We accomplish so by designing a parametrized family of
efficient Dicke state preparation routines. We construct explicit block
encoding circuits for representative spin models such as the Heisenberg and
spin glass Hamiltonians and provide detailed, non-asymptotic gate counts. Our
numerical benchmarks validate the efficiency of the FOQCS-LCU approach,
illustrating an order-of-magnitude improvement in CNOT count over conventional
LCU. This framework opens the door to efficient block encodings of a broad
class of structured matrices beyond those explored here.

</details>


### [359] [Planckian bound on quantum dynamical entropy](https://arxiv.org/abs/2507.20914)
*Xiangyu Cao*

Main category: quant-ph

TL;DR: 研究引入了量子系统的动力学熵，量化了信息增益，并推测了其普朗克边界。


<details>
  <summary>Details</summary>
Motivation: 量化量子系统在连续监测下的信息增益。

Method: 该研究受到Connes、Narnhofer和Thirring的启发，量化了有关初始条件的信息量。研究明确计算了热力学和长时间极限下，基于两点关联函数的熵率，并推测了熵率的普朗克边界。

Result: 研究表明，在非经典和非大N极限下，通过监测热涨落可以获得非零熵率，并计算了熵率。此外，还得到了相关的纯化率结果。

Conclusion: 该研究引入了量子系统在连续监测下的动力学熵的定义，该定义量化了关于初始条件的保真度信息。通过监测非经典和非大N极限下多体系统中广泛可观测物的热涨落，可以获得非零熵率。

Abstract: We introduce a simple definition of dynamical entropy for quantum systems
under continuous monitoring, inspired by Connes, Narnhofer and Thirring. It
quantifies the amount of information gained about the initial condition. A
nonzero entropy rate can be obtained by monitoring the thermal fluctuation of
an extensive observable in a generic many-body system (away from classical or
large N limit). We explicitly compute the entropy rate in the thermodynamic and
long-time limit, in terms of the two-point correlation functions. We conjecture
a universal Planckian bound for the entropy rate. Related results on the
purification rate are also obtained.

</details>


### [360] [Relativistic quantum Otto engine driven by the circular Unruh effect](https://arxiv.org/abs/2507.20928)
*Rudra Prosad Sarkar,Arnab Mukherjee,Sunandan Gangopadhyay*

Main category: quant-ph

TL;DR: 研究了一个相对论量子奥托发动机。量子比特作为工作物质，在加速圆周运动中与量子标量场相互作用。结果表明，功输出随加速度增加而增加，效率不受圆周运动影响。


<details>
  <summary>Details</summary>
Motivation: 本工作提出了经典奥托发动机的相对论量子类似物的新框架。

Method: 本研究将单个量子比特作为工作物质，分析了其在超相对论速度下进行两次半圆周运动时与无质量量子标量场的相互作用。量子真空通过由圆周运动引起的加速度产生的昂效应作为热浴。

Result: 我们观察到量子比特的响应函数被量子比特轨迹的存在显著改变。通过分析跃迁概率行为，我们发现它在高加速度状态下渐近地趋于一个仅由相关函数性质决定的恒定值。

Conclusion: 本研究的结果强调了圆周轨迹在决定发动机功输出方面起着至关重要的作用。具体而言，提取的功随探测器加速度的增加而增加，并在高加速度状态下趋于渐近极限。值得注意的是，该模型效率不受圆周运动的影响，与之前研究的模型一致。

Abstract: In this work, we present a new framework for a relativistic quantum analouge
of the classical Otto engine. Considering a single qubit as the working
substance, we analyse its interaction with a massless quantum scalar field
while undergoing two half-circular rotations at ultra-relativistic velocities.
The quantum vacuum serves as a thermal bath through the Unruh effect induced
due to the acceleration from the circular motions. We observe that the response
function of the qubit gets significantly modified by the presence of the
qubit's trajectory. Analysing the transition probability behaviour, we find
that in the high-acceleration regime, it asymptotically approaches a constant
value, determined solely by the properties of the correlation function.
Furthermore, our results emphasize the crucial role of the circular trajectory
in determining the engine's work output. In particular, the extracted work
increases with detector acceleration and approaches an asymptotic limit in the
high-acceleration regime. Notably, the efficiency of this model remains
unaffected by the circular motion and is consistent with previously studied
models.

</details>


### [361] [Entanglement negativity in free fermions: twisted characteristic polynomial, universal bounds, and area laws](https://arxiv.org/abs/2507.20947)
*Ryota Matsuda,Zongping Gong*

Main category: quant-ph

TL;DR: 本研究提出了一个计算自由费米子纠缠负性的新公式，并推导出相关的界限，揭示了其与关联性质和面积定律的联系，并为开放系统中的纠缠生成提供了界限。


<details>
  <summary>Details</summary>
Motivation: 探索自由费米子系统中的纠缠负性，特别是寻找其计算方法、普适界限以及与关联聚类和纠缠面积定律的关系，并研究开放系统中纠缠生成。

Method: 提出一个通用且简单的计算自由费米子纠缠负性的公式，并利用此公式推导普适界限，以及建立开放系统中纠缠生成的面积定律界限。

Result: 推导出了计算自由费米子纠缠负性的通用简单公式；得到了负性及其变化率的几个普适界限；发现负性界限直接联系了关联的聚类性质和纠缠面积定律；为混合自由费米子态提供了最优的面积定律条件；建立了开放系统中纠缠生成的面积定律界限。

Conclusion: 该研究为自由费米子中的纠缠负性计算提供了一个通用且简单的公式，该公式能够推导出关于负性及其变化率的几个普适界限。研究结果将负性界限与自由费米子态中关联的聚类性质以及纠缠面积定律直接联系起来，并为具有长程关联的混合自由费米子态提供了最优的面积定律条件。此外，研究还建立了开放系统中纠缠生成的面积定律界限，这与先前已知的酉动力学中纠缠熵的结果类似。该工作为理解费米子混合态纠缠提供了新的解析视角。

Abstract: We present a general and simple formula for computing the entanglement
negativity in free fermions. Our formula allows for deriving several universal
bounds on negativity and its rate of change in dynamics. The bound on
negativity directly relates the clustering property of correlations in
free-fermion states to the entanglement area law, and provides the optimal
condition for the area law in mixed free fermion states with long-range
correlations. In addition, we establish an area-law bound on entanglement
generation in open systems, analogous to previously known results for
entanglement entropy in unitary dynamics. Our work provides new analytical
insights into fermionic mixed-state entanglement.

</details>


### [362] [Witness the High-Dimensional Quantum Steering via Majorization Lattice](https://arxiv.org/abs/2507.20950)
*Ma-Cheng Yang,Cong-Feng Qiao*

Main category: quant-ph

TL;DR: 提出MAJORIZATION LATTICE框架，用于检测任意维度和测量设置下的量子转向，并获得了比现有方法更严格的转向不等式。


<details>
  <summary>Details</summary>
Motivation: 现有量子转向检测方法主要受限于特定的测量场景或低维系统，本研究旨在克服这些限制。

Method: 提出MAJORIZATION LATTICE框架用于量子转向检测，该框架适用于任意维度和测量设置。

Result: 获得了两量子比特态、高维Werner态和各向同性态的转向不等式，并发现已知的高维结果是该新方法的一种近似极限。

Conclusion: 该研究提出了一个用于量子转向检测的MAJORIZATION LATTICE框架，该框架能够探索任意维度和测量设置下的转向，并为两量子比特态、高维Werner态和各向同性态获得了转向不等式，这些不等式比现有结果更严格。

Abstract: Quantum steering enables one party to influence another remote quantum state
by local measurement. While steering is fundamental to many quantum information
tasks, the existing detection methods in the literature are mainly constrained
to either specific measurement scenario or low-dimensional systems. In this
work, we propose a majorization lattice framework for steering detection, which
is capable of exploring the steering in arbitrary dimension and measurement
setting. Steering inequalities for two-qubit states, high-dimensional Werner
states and isotropic states are obtained, which set even stringent bars than
what have reached yet. Notably, the known high-dimensional results turn out to
be some kind of approximate limits of the new approach.

</details>


### [363] [Quantum Simulation of Molecular Dynamics Processes -- A Benchmark Study Using Classical Simulator and Present-Day Quantum Hardware](https://arxiv.org/abs/2507.21030)
*Tamila Kuanysheva,Brian Kendrick,Lukasz Cincio,Dmitri Babikov*

Main category: quant-ph

TL;DR: 我们探索了使用量子计算机模拟量子分子动力学问题，设计了改进的量子电路以适应实际硬件，并在模拟器和实际硬件上进行了测试，结果表明量子计算在处理这类问题方面具有潜力，但也面临硬件限制带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 为了探索使用量子计算机解决量子分子动力学基本问题的可行性，并解决在实际量子硬件上运行量子算法时面临的深度量子电路和噪声问题。

Method: 研究了如何使用经典模拟器（仿真器）和现有的量子硬件来模拟量子分子动力学中的基本问题，包括自由波包传播、谐振子振动和势垒隧穿。设计了更浅的量子电路来制备类高斯初始波包，以克服现有方法在实际硬件上产生的深度量子电路带来的噪声问题。在经典模拟器上的结果与传统方法完美吻合，验证了量子算法和Qiskit代码的准确性。然而，在IBM超导量子比特和IonQ离子阱等实际量子硬件上的结果显示出由于硬件限制而产生的显著差异。

Result: 在经典模拟器上，量子算法的结果与传统方法完全一致，证明了算法的准确性。然而，在实际量子硬件（如IBM的超导量子比特和IonQ的离子阱）上，由于硬件限制，结果存在较大的差异。

Conclusion: 该工作突出了使用量子计算机解决量子分子动力学基本问题的潜力和挑战。

Abstract: We explore how the fundamental problems in quantum molecular dynamics can be
modelled using classical simulators (emulators) of quantum computers and the
actual quantum hardware available to us today. The list of problems we tackle
includes propagation of a free wave packet, vibration of a harmonic oscillator,
and tunneling through a barrier. Each of these problems starts with the initial
wave packet setup. Although Qiskit provides a general method for initializing
wavefunctions, in most cases it generates deep quantum circuits. While these
circuits perform well on noiseless simulators, they suffer from excessive noise
on quantum hardware. To overcome this issue, we designed a shallower quantum
circuit for preparing a Gaussian-like initial wave packet, which improves the
performance on real hardware. Next, quantum circuits are implemented to apply
the kinetic and potential energy operators for the evolution of a wavefunction
over time. The results of our modelling on classical emulators of quantum
hardware agree perfectly with the results obtained using the traditional
(classical) methods. This serves as a benchmark and demonstrates that the
quantum algorithms and Qiskit codes we developed are accurate. However, the
results obtained on the actual quantum hardware available today, such as IBM's
superconducting qubits and IonQ's trapped ions, indicate large discrepancies
due to hardware limitations. This work highlights both the potential and
challenges of using quantum computers to solve fundamental quantum molecular
dynamics problems.

</details>


### [364] [Quantum optical shallow networks](https://arxiv.org/abs/2507.21036)
*Simone Roncallo,Angela Rosy Morgillo,Seth Lloyd,Chiara Macchiavello,Lorenzo Maccone*

Main category: quant-ph

TL;DR: 提出一种量子光学浅层网络，使用单光子和香港-王-曼德尔效应，实现资源恒定的计算。


<details>
  <summary>Details</summary>
Motivation: 提出一种量子光学协议来实现具有任意数量神经元的浅层网络，以克服传统浅层网络在处理大量输入时所需的资源代价。

Method: 利用光子干涉和香港-王-曼德尔效应，将输入数据和参数编码为单光子态，并测量光子在分束器上的符合计数率来确定网络输出。

Result: 该量子光学协议可以实现一个浅层网络，并且在训练完成后，其所需的光学资源与输入特征和神经元的数量无关。

Conclusion: 该模型一旦训练完成，无论输入特征和神经元的数量如何，都只需要恒定的光资源。

Abstract: Classical shallow networks are universal approximators. Given a sufficient
number of neurons, they can reproduce any continuous function to arbitrary
precision, with a resource cost that scales linearly in both the input size and
the number of trainable parameters. In this work, we present a quantum optical
protocol that implements a shallow network with an arbitrary number of neurons.
Both the input data and the parameters are encoded into single-photon states.
Leveraging the Hong-Ou-Mandel effect, the network output is determined by the
coincidence rates measured when the photons interfere at a beam splitter, with
multiple neurons prepared as a mixture of single-photon states. Remarkably,
once trained, our model requires constant optical resources regardless of the
number of input features and neurons.

</details>


### [365] [Certification of nonobjective information by Popescu-Rohrlich box fraction and distinguishing quantum theory](https://arxiv.org/abs/2507.21051)
*Chellasamy Jebarathinam*

Main category: quant-ph

TL;DR: 信息论限制不足以区分量子理论和广义非信号理论。通过引入“Popescu-Rohrlich 盒子分数”来认证非客观信息，可以区分它们。


<details>
  <summary>Details</summary>
Motivation: 为了区分量子理论和广义非信号理论，仅识别量子贝尔非定域性的信息论限制是不够的，这促使本研究探索新的区分方法。

Method: 本研究采用信息论方法，特别是“Popescu-Rohrlich 盒子分数”的概念，来认证非客观信息。

Result: 研究发现，信息因果原理所识别的量子后模型，可以通过广义非信号理论中的贝尔局域盒子内出现的非客观信息（以Popescu-Rohrlich 盒子分数的形式）与具有单纯形局域状态空间的另外两种广义非信号理论区分开来。

Conclusion: 该研究表明，仅靠量子贝尔非定域性的信息论限制无法完全区分量子理论和广义非信号理论。研究引入了“Popescu-Rohrlich 盒子分数”的概念，用于认证非客观信息，为区分量子理论和广义非信号理论提供了超越信息因果原理的答案。

Abstract: It is demonstrated that identifying information-theoretic limitations of
quantum Bell nonlocality alone cannot completely distinguish quantum theory
from generalized nonsignaling theories. To this end, an information-theoretic
concept of certifying nonobjective information by the Popescu-Rohrlich box
fraction is employed. Furthermore, in the aforementioned demonstration, a
partial answer to the question of what distinguishes quantum theory from
generalized nonsignaling theories emerges beyond the one provided by the
principle of information causality alone. This is accomplished by demonstrating
that postquantum models identified by the information causality are isolated by
the emergence of the Popescu-Rohrlich box fraction of nonobjective information
in Bell-local boxes of a generalized nonsignaling theory, over the two other
generalized nonsignaling theories that have simplicial local state spaces.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [366] [Quantifying the Performance Gap for Simple Versus Optimal Dynamic Server Allocation Policies](https://arxiv.org/abs/2507.19667)
*Niklas Carlsson,Derek Eager*

Main category: cs.DC

TL;DR: 研究了在云环境中动态分配服务器的策略，并分析了简单策略和最优策略的性能。


<details>
  <summary>Details</summary>
Motivation: 为了利用云计算动态配置服务器资源的能力，需要一种根据当前负载条件动态分配（和取消分配）服务器的策略。

Method: 开发了几种用于动态服务器分配的简单策略，并建立了分析模型，还设计了半马尔可夫决策模型以确定最优策略的性能。

Result: 对简单策略和最优策略之间的性能差距进行了量化，并研究了在多站点系统中使用动态服务器分配进行状态相关路由的潜在性能优势。

Conclusion: 该研究为希望平衡云服务成本和延迟的服务提供商提供了有价值的见解。

Abstract: Cloud computing enables the dynamic provisioning of server resources. To
exploit this opportunity, a policy is needed for dynamically allocating (and
deallocating) servers in response to the current load conditions. In this paper
we describe several simple policies for dynamic server allocation and develop
analytic models for their analysis. We also design semi-Markov decision models
that enable determination of the performance achieved with optimal policies,
allowing us to quantify the performance gap between simple, easily implemented
policies, and optimal policies. Finally, we apply our models to study the
potential performance benefits of state-dependent routing in multi-site systems
when using dynamic server allocation at each site. Insights from our results
are valuable to service providers wanting to balance cloud service costs and
delays.

</details>


### [367] [Oranits: Mission Assignment and Task Offloading in Open RAN-based ITS using Metaheuristic and Deep Reinforcement Learning](https://arxiv.org/abs/2507.19712)
*Ngoc Hung Nguyen,Nguyen Van Thieu,Quang-Trung Luu,Anh Tuan Nguyen,Senura Wanasekara,Nguyen Cong Luong,Fatemeh Kavehmadavani,Van-Dinh Nguyen*

Main category: cs.DC

TL;DR: 本研究提出了Oranits系统模型和CGG-ARO、MA-DDQN两种优化算法，用于解决Open RAN智能交通系统中的任务分配和卸载问题，并通过仿真验证了所提出方法的有效性，特别是MA-DDQN相比基线方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了任务间的相互依赖性以及将任务卸载到边缘服务器的成本，导致决策不理想。本研究旨在解决这一问题。

Method: 提出了一种名为Oranits的新颖系统模型，该模型考虑了任务依赖性和卸载成本。设计了两种优化方法：1. 基于元启发式演化计算的CGG-ARO算法，作为单槽优化的基线。2. 基于奖励的深度强化学习框架MA-DDQN，集成了多智能体协调和多动作选择机制。

Result: CGG-ARO算法使完成的任务数量和总体效益分别提高了约7.1%和7.7%。MA-DDQN算法在完成任务数量和总体效益方面分别取得了11.0%和12.5%的提升。

Conclusion: Oranits系统在动态的智能交通系统(ITS)环境中，能够实现更快速、更具适应性和更高效的任务处理。

Abstract: In this paper, we explore mission assignment and task offloading in an Open
Radio Access Network (Open RAN)-based intelligent transportation system (ITS),
where autonomous vehicles leverage mobile edge computing for efficient
processing. Existing studies often overlook the intricate interdependencies
between missions and the costs associated with offloading tasks to edge
servers, leading to suboptimal decision-making. To bridge this gap, we
introduce Oranits, a novel system model that explicitly accounts for mission
dependencies and offloading costs while optimizing performance through vehicle
cooperation. To achieve this, we propose a twofold optimization approach.
First, we develop a metaheuristic-based evolutionary computing algorithm,
namely the Chaotic Gaussian-based Global ARO (CGG-ARO), serving as a baseline
for one-slot optimization. Second, we design an enhanced reward-based deep
reinforcement learning (DRL) framework, referred to as the Multi-agent Double
Deep Q-Network (MA-DDQN), that integrates both multi-agent coordination and
multi-action selection mechanisms, significantly reducing mission assignment
time and improving adaptability over baseline methods. Extensive simulations
reveal that CGG-ARO improves the number of completed missions and overall
benefit by approximately 7.1% and 7.7%, respectively. Meanwhile, MA-DDQN
achieves even greater improvements of 11.0% in terms of mission completions and
12.5% in terms of the overall benefit. These results highlight the
effectiveness of Oranits in enabling faster, more adaptive, and more efficient
task processing in dynamic ITS environments.

</details>


### [368] [Accelerating Matrix Multiplication: A Performance Comparison Between Multi-Core CPU and GPU](https://arxiv.org/abs/2507.19723)
*Mufakir Qamar Ansari,Mudabir Qamar Ansari*

Main category: cs.DC

TL;DR: 在现代消费级异构平台上，通过CUDA实现的GPU版本矩阵乘法性能远超CPU版本，尤其在大尺寸矩阵计算中优势明显。


<details>
  <summary>Details</summary>
Motivation: 矩阵乘法作为科学计算和机器学习的基础操作，其计算复杂度在大规模应用中成为瓶颈。本研究旨在对现代异构平台上的矩阵乘法性能进行直接的经验分析。

Method: 通过C++、OpenMP和CUDA（含共享内存优化）三种方式实现矩阵乘法算法，并在不同尺寸的方阵（128x128至4096x4096）上进行性能基准测试。

Result: 并行CPU版本比顺序版本稳定提速12-14倍。GPU性能随问题规模的增大而显著提升，在4096x4096矩阵上，GPU版本比顺序基线提速约593倍，比优化的并行CPU版本提速约45倍。

Conclusion: GPU架构能够显著加速数据并行工作负载，即使在消费级硬件上也能获得显著的性能提升。

Abstract: Matrix multiplication is a foundational operation in scientific computing and
machine learning, yet its computational complexity makes it a significant
bottleneck for large-scale applications. The shift to parallel architectures,
primarily multi-core CPUs and many-core GPUs, is the established solution, and
these systems are now ubiquitous from datacenters to consumer laptops. This
paper presents a direct, empirical performance analysis of matrix
multiplication on a modern, consumer-grade heterogeneous platform. We
implemented and benchmarked three versions of the algorithm: a baseline
sequential C++ implementation, a parallel version for its multi-core CPU using
OpenMP, and a massively parallel version for its discrete GPU using CUDA with
shared memory optimizations. The implementations were evaluated with square
matrices of varying dimensions, from 128x128 to 4096x4096. Our results show
that while the parallel CPU provides a consistent speedup of 12-14x over the
sequential version, the GPU's performance scales dramatically with problem
size. For a 4096x4096 matrix, the GPU implementation achieved a speedup of
approximately 593x over the sequential baseline and 45x over the optimized
parallel CPU version. These findings quantitatively demonstrate the profound
impact of many-core GPU architectures on accelerating data-parallel workloads,
underscoring that significant performance gains are readily accessible even on
consumer-level hardware.

</details>


### [369] [MegatronApp: Efficient and Comprehensive Management on Distributed LLM Training](https://arxiv.org/abs/2507.19845)
*Bohan Zhao,Guang Yang,Shuo Chen,Ruitao Liu,Tingrui Zhang,Yongchao He,Wei Xu*

Main category: cs.DC

TL;DR: MegatronApp是一个用于优化、诊断和解释大规模语言模型（LLMs）跨节点训练的开源工具链。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型的参数数量快速增加，使得模型训练从单节点活动转变为复杂的跨节点活动，带来了性能优化、诊断和可解释性方面的挑战。

Method: MegatronApp是一个开源工具链，包含MegaScan、MegaFBD、MegaDPP和MegaScope四个模块，用于优化、诊断和解释大规模语言模型（LLMs）的跨节点训练。

Result: MegatronApp作为一款开源工具链，能够应对大规模语言模型训练带来的系统级挑战，提高训练的可靠性、效率和透明度。

Conclusion: MegatronApp通过其四个模块（MegaScan、MegaFBD、MegaDPP和MegaScope）提升了生产规模训练的可靠性、效率和透明度，并增强了Megatron-LM生态系统。

Abstract: The rapid escalation in the parameter count of large language models (LLMs)
has transformed model training from a single-node endeavor into a highly
intricate, cross-node activity. While frameworks such as Megatron-LM
successfully integrate tensor (TP), pipeline (PP), and data (DP) parallelism to
enable trillion-parameter training, they simultaneously expose practitioners to
unprecedented systems-level challenges in performance optimization, diagnosis,
and interpretability. MegatronApp is an open-source toolchain expressly
designed to meet these challenges. It introduces four orthogonal, yet
seamlessly composable modules--MegaScan, MegaFBD, MegaDPP, and MegaScope--that
collectively elevate the reliability, efficiency, and transparency of
production-scale training. This paper presents the motivation, architecture,
and distinctive contributions of each module, and elucidates how their
synergistic integration augments the Megatron-LM ecosystem.

</details>


### [370] [A Fast Parallel Median Filtering Algorithm Using Hierarchical Tiling](https://arxiv.org/abs/2507.19926)
*Louis Sugy*

Main category: cs.DC

TL;DR: 一种新的中值滤波算法，通过分层切片实现排序问题的分离，速度比现有技术快5倍。


<details>
  <summary>Details</summary>
Motivation: 为了解决中值滤波计算成本高，尤其是在核直径增大时排序算法扩展性差的问题。

Method: 提出了一种利用分层切片分离排序问题的分类排序方法，该方法有两种变体：一种是完全在寄存器内操作的数据无关选择网络，另一种是利用随机访问内存的数据感知版本。

Result: 本文提出的两种算法，其每像素复杂度分别为O(k log(k))和O(k)，在GPU上的实现比现有技术快5倍，成为大多数情况下最快的中值滤波器。

Conclusion: 本文提出的算法在处理8、16和32位数据类型以及3x3到75x75的核时，其CUDA实现比现有技术快5倍，并且在大多数情况下是最快的。

Abstract: Median filtering is a non-linear smoothing technique widely used in digital
image processing to remove noise while retaining sharp edges. It is
particularly well suited to removing outliers (impulse noise) or granular
artifacts (speckle noise). However, the high computational cost of median
filtering can be prohibitive. Sorting-based algorithms excel with small kernels
but scale poorly with increasing kernel diameter, in contrast to constant-time
methods characterized by higher constant factors but better scalability, such
as histogram-based approaches or the 2D wavelet matrix.
  This paper introduces a novel algorithm, leveraging the separability of the
sorting problem through hierarchical tiling to minimize redundant computations.
We propose two variants: a data-oblivious selection network that can operate
entirely within registers, and a data-aware version utilizing random-access
memory. These achieve per-pixel complexities of $O(k \log(k))$ and $O(k)$,
respectively, for a $k \times k$ kernel - unprecedented for sorting-based
methods. Our CUDA implementation is up to 5 times faster than the current state
of the art on a modern GPU and is the fastest median filter in most cases for
8-, 16-, and 32-bit data types and kernels from $3 \times 3$ to $75 \times 75$.

</details>


### [371] [Offloading tracing for real-time systems using a scalable cloud infrastructure](https://arxiv.org/abs/2507.19953)
*David Jannis Schmidt,Grigory Fridman,Florian von Zabiensky*

Main category: cs.DC

TL;DR: 提出了一种云原生跟踪架构，用于实时系统，可处理大规模并行跟踪，并支持长期监控和现场运行时监控。


<details>
  <summary>Details</summary>
Motivation: 传统跟踪工具依赖具有有限处理和存储能力的本地桌面，阻碍了大规模分析，而实时嵌入式系统需要精确的计时和故障检测来确保正确行为。

Method: 提出了一种基于微服务和边缘计算的、用于实时系统软件跟踪的可扩展、云原生架构。该方法将跟踪处理工作负载从开发人员的机器转移到云端，使用专门的跟踪组件捕获跟踪数据，并通过 WebSockets 和 Apache Kafka 将其转发到可扩展的后端。

Result: 评估结果表明，该架构可以有效地处理许多并行跟踪会话，尽管随着系统负载的增加，每个会话的吞吐量会略有下降，但整体吞吐量会增加。目标系统具有网络连接性，可以直接流式传输精简后的跟踪数据，从而在现场实现运行时监控。

Conclusion: 该架构支持对并行跟踪会话进行可扩展分析，并为未来集成基于规则的测试和运行时验证奠定了基础。

Abstract: Real-time embedded systems require precise timing and fault detection to
ensure correct behavior. Traditional tracing tools often rely on local desktops
with limited processing and storage capabilities, which hampers large-scale
analysis. This paper presents a scalable, cloud-based architecture for software
tracing in real-time systems based on microservices and edge computing. Our
approach shifts the trace processing workload from the developer's machine to
the cloud, using a dedicated tracing component that captures trace data and
forwards it to a scalable backend via WebSockets and Apache Kafka. This enables
long-term monitoring and collaborative analysis of target executions, e.g., to
detect and investigate sporadic errors. We demonstrate how this architecture
supports scalable analysis of parallel tracing sessions and lays the foundation
for future integration of rule-based testing and runtime verification. The
evaluation results show that the architecture can handle many parallel tracing
sessions efficiently, although the per-session throughput decreases slightly as
the system load increases, while the overall throughput increases. Although the
design includes a dedicated tracer for analysis during development, this
approach is not limited to such setups. Target systems with network
connectivity can stream reduced trace data directly, enabling runtime
monitoring in the field.

</details>


### [372] [MTASet: A Tree-based Set for Efficient Range Queries in Update-heavy Workloads](https://arxiv.org/abs/2507.20041)
*Daniel Manor,Mor Perry,Moshe Sulamy*

Main category: cs.DC

TL;DR: MTASet 是一种新的并发集，在处理更新密集型工作负载和原子范围查询方面表现优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有并发集在更新密集型工作负载和原子范围查询方面存在性能瓶颈。

Method: 并发（a,b）树实现

Result: MTASet 在处理范围查询方面比现有优化方案快 2 倍，并保证了线性化。

Conclusion: MTASet 是一种利用并发（a,b）树实现的并发集，在更新密集型工作负载和原子范围查询方面均表现出色，并且保证了线性化。

Abstract: In concurrent data structures, the efficiency of set operations can vary
significantly depending on the workload characteristics. Numerous concurrent
set implementations are optimized and fine-tuned to excel in scenarios
characterized by predominant read operations. However, they often perform
poorly when confronted with workloads that heavily prioritize updates.
Additionally, current leading-edge concurrent sets optimized for update-heavy
tasks typically lack efficiency in handling atomic range queries. This study
introduces the MTASet, which leverages a concurrent (a,b)-tree implementation.
Engineered to accommodate update-heavy workloads and facilitate atomic range
queries, MTASet surpasses existing counterparts optimized for tasks in range
query operations by up to 2x. Notably, MTASet ensures linearizability.

</details>


### [373] [Racing to Idle: Energy Efficiency of Matrix Multiplication on Heterogeneous CPU and GPU Architectures](https://arxiv.org/abs/2507.20063)
*Mufakir Qamar Ansari,Mudabir Qamar Ansari*

Main category: cs.DC

TL;DR: 在笔记本电脑上，独立GPU在处理矩阵乘法时，性能和能效都远超CPU和集成GPU。


<details>
  <summary>Details</summary>
Motivation: 随着单核处理器功耗和散热限制的出现，异构计算（结合CPU和GPU等加速器）成为HPC领域提高能效的关键。然而，在常见的硬件上量化这种性能与功耗的权衡仍然是一个重要的研究课题。

Method: 使用Linux perf和nvidia-smi等工具，在笔记本电脑（AMD Ryzen 7 5800H CPU、NVIDIA GeForce GTX 1650 GPU和AMD Radeon Vega GPU）上对4096x4096矩阵乘法工作负载进行了直接的性能和能耗测量。

Result: 独立GPU在性能上比CPU快93.5倍，并且能耗仅为CPU的2%，能效提高了50倍。

Conclusion: 该研究通过对4096x4096矩阵乘法在笔记本电脑的CPU、独立GPU和集成GPU上的性能和能耗进行实证测量，发现独立GPU不仅在性能上表现最佳（比CPU快93.5倍），而且在能效上也最高（仅消耗CPU能耗的2%，能效提升50倍）。这为“尽快进入空闲状态”原则提供了实际证据，并为能耗感知软件开发提供了量化指导。

Abstract: The paradigm shift towards multi-core and heterogeneous computing, driven by
the fundamental power and thermal limits of single-core processors, has
established energy efficiency as a first-class design constraint in
high-performance computing (HPC). Heterogeneous systems, integrating
traditional multi-core CPUs with specialized accelerators like discrete (dGPU)
and integrated (iGPU) graphics processing units, offer a compelling path to
navigating the trade-offs between performance and power. However, quantifying
these trade-offs on widely accessible hardware remains a critical area of
study. This paper presents a direct, empirical measurement of the performance
and energy-to-solution of a canonical HPC workload -- a 4096x4096 matrix-matrix
multiplication -- on three distinct compute architectures within a single
consumer-grade laptop: a multi-core AMD Ryzen 7 5800H CPU, a discrete NVIDIA
GeForce GTX 1650 GPU, and an integrated AMD Radeon Vega GPU. Using standard,
validated, and minimally intrusive tools such as Linux perf and nvidia-smi, we
find that the discrete GPU is not only the performance leader, achieving a
93.5x speedup over the CPU, but is also the most energy-efficient, consuming
only 2% of the energy used by the CPU, resulting in a 50-fold improvement in
energy efficiency. These findings provide a practical demonstration of the
"race to idle" principle and offer clear, quantitative guidance on
architectural choices for energy-aware software development.

</details>


### [374] [High-Performance Parallel Optimization of the Fish School Behaviour on the Setonix Platform Using OpenMP](https://arxiv.org/abs/2507.20173)
*Haitian Wang,Long Qin*

Main category: cs.DC

TL;DR: 本研究在Setonix超级计算平台和OpenMP框架下，对鱼群行为（FSB）算法进行了高性能并行优化，旨在提升计算能力以满足日益增长的复杂计算需求。研究分析了多线程的各个方面，为FSB算法及其他并行计算研究提供了优化见解和参考。


<details>
  <summary>Details</summary>
Motivation: 鉴于对各领域复杂、大规模计算增强计算能力的需求日益增长，对优化的并行算法和计算结构有着迫切的需求。以自然界社会行为模式为灵感的FSB算法，因其迭代和计算密集型的特点，为并行化提供了一个理想的平台。

Method: 本研究利用OpenMP框架，在Setonix超级计算平台上对鱼群行为（FSB）算法进行了高性能并行优化，通过实验分析了线程数、调度策略和OpenMP构件等多种多线程方面的内容，以识别能够提升程序性能的模式和策略。

Result: 研究结果不仅为在Setonix平台上并行优化FSB提供了见解，也为使用OpenMP进行其他并行计算研究提供了有价值的参考。

Conclusion: 本研究为Setonix平台和OpenMP在FSB算法上的并行优化提供了宝贵的见解和参考，并指出了未来进一步探索和优化的方向，例如缓存行为和线程调度策略。

Abstract: This paper presents an in-depth investigation into the high-performance
parallel optimization of the Fish School Behaviour (FSB) algorithm on the
Setonix supercomputing platform using the OpenMP framework. Given the
increasing demand for enhanced computational capabilities for complex,
large-scale calculations across diverse domains, there's an imperative need for
optimized parallel algorithms and computing structures. The FSB algorithm,
inspired by nature's social behavior patterns, provides an ideal platform for
parallelization due to its iterative and computationally intensive nature. This
study leverages the capabilities of the Setonix platform and the OpenMP
framework to analyze various aspects of multi-threading, such as thread counts,
scheduling strategies, and OpenMP constructs, aiming to discern patterns and
strategies that can elevate program performance. Experiments were designed to
rigorously test different configurations, and our results not only offer
insights for parallel optimization of FSB on Setonix but also provide valuable
references for other parallel computational research using OpenMP. Looking
forward, other factors, such as cache behavior and thread scheduling strategies
at micro and macro levels, hold potential for further exploration and
optimization.

</details>


### [375] [Ethereum Conflicts Graphed](https://arxiv.org/abs/2507.20196)
*Dvir David Biton,Roy Friedman,Yaron Hay*

Main category: cs.DC

TL;DR: 通过追踪大量以太坊区块，分析了智能合约的调用和依赖关系，发现其冲突图多呈星型结构，这对于理解并行化潜力具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 理解以太坊智能合约的交互对于性能优化至关重要，例如预热对象和实现并发执行。

Method: 使用call tracer和prestate tracer追踪了200多万个以太坊区块，分析了交易数量分布、调用树结构、价值转移交易与智能合约调用的比例，以及冲突图的结构。

Result: 报告了每区块交易量分布、智能合约调用中的调用树结构、价值转移交易与智能合约调用的比例，并全面研究了区块冲突图的结构。

Conclusion: 研究发现以太坊智能合约的冲突图主要呈现出类似星型的配置，以及其他值得注意的结构特性。

Abstract: Ethereum, a leading blockchain platform, has revolutionized the digital
economy by enabling decentralized transactions and the execution of smart
contracts. Ethereum transactions form the backbone of its network, facilitating
peer-to-peer exchanges and interactions with complex decentralized
applications. Smart contracts extend Ethereum's capabilities by automating
processes and enabling trustless execution of agreements. Hence, understanding
how these smart contracts interact is important in order to facilitate various
performance optimizations, such as warming objects before they are being
accessed and enabling concurrent execution. Of particular interest to us are
the development of the calling graph, as well as the read sets and write sets
of invocations within the same block, and the properties of the associated
conflict graph that is derived from them. The latter is important for
understanding the parallelization potential of smart contracts on Ethereum. We
traced upwards of 2 million recent Ethereum blocks using call tracer and
prestate tracer, out of a total of 21.4 million blocks at the time of writing.
We report on the transactions per block distribution, the structure of call
trees in smart contract invocations, the ratio of value-transfer transactions
to smart contract invocations, as well as provide a comprehensive study of the
structure of blocks' conflict graphs. We find that conflict graphs
predominantly show a star like configuration, as well as other noteworthy
structural properties.

</details>


### [376] [Silent Self-Stabilising Leader Election in Programmable Matter Systems with Holes](https://arxiv.org/abs/2507.20201)
*Jérémie Chalopin,Shantanu Das,Maria Kokkou*

Main category: cs.DC

TL;DR: 在可编程物质系统中，我们提出了首个自稳定领导者选举算法，利用粒子运动克服了恒定内存系统的限制，并在不公平调度下保证了唯一领导者的选举。


<details>
  <summary>Details</summary>
Motivation: 在可编程物质系统中，领导者选举是分布式计算中的一个基本问题，需要对简单计算实体进行协调以完成复杂任务。然而，在自稳定设置下的解决方案仍然有限。

Method: 本研究提出了一种利用粒子运动的自稳定领导者选举算法，用于可编程物质系统中的连通（但不一定是单连通）配置。

Result: 该算法是首个在可编程物质系统中实现自稳定领导者选举的算法，并且在不公平调度下，在具有共同方向感的粒子系统中，保证选举出唯一领导者。

Conclusion: 本研究提出了首个用于可编程物质的自稳定领导者选举算法，该算法在不公平调度下，在具有共同方向感的粒子系统中，保证选举出唯一领导者。该算法利用了粒子运动能力，克服了Dolev等人在恒定内存系统中建立的经典不可能结果。

Abstract: Leader election is a fundamental problem in distributed computing,
particularly within programmable matter systems, where coordination among
simple computational entities is crucial for solving complex tasks. In these
systems, particles (i.e., constant memory computational entities) operate in a
regular triangular grid as described in the geometric Amoebot model. While
leader election has been extensively studied in non self-stabilising settings,
self-stabilising solutions remain more limited. In this work, we study the
problem of self-stabilising leader election in connected (but not necessarily
simply connected) configurations. We present the first self-stabilising
algorithm for programmable matter that guarantees the election of a unique
leader under an unfair scheduler, assuming particles share a common sense of
direction. Our approach leverages particle movement, a capability not
previously exploited in the self-stabilising context. We show that movement in
conjunction with particles operating in a grid can overcome classical
impossibility results for constant-memory systems established by Dolev et al.

</details>


### [377] [A Comparative Study of OpenMP Scheduling Algorithm Selection Strategies](https://arxiv.org/abs/2507.20312)
*Jonas H. Müller Korndörfer,Ali Mohammed,Ahmed Eleliemy,Quentin Guilloteau,Reto Krummenacher,Florina M. Ciorba*

Main category: cs.DC

TL;DR: 本研究探索了在OpenMP中使用基于学习的方法来选择调度算法，提出并评估了基于专家和基于强化学习（RL）的方法。结果显示RL方法能学习到高性能的调度决策，但需要大量探索，而基于专家的方法则依赖先验知识。结合两者可以提升性能和适应性，该方法也可扩展到MPI程序。


<details>
  <summary>Details</summary>
Motivation: 随着科学和数据科学应用程序变得越来越复杂，对高性能计算（HPC）系统的有效调度和负载平衡技术的需求日益增长。OpenMP等并行编程框架提供了多种高级调度算法，但存在调度算法选择问题，需要为特定的工作负载和系统特性找到最合适的算法。

Method: 提出并评估了基于专家和基于强化学习（RL）的方法，并跨六个应用程序和三个系统进行了详细的性能分析。

Result: 研究结果表明，基于RL的方法能够学习高性能的调度决策，但需要大量的探索，并且奖励函数选择至关重要。基于专家的方法依赖先验知识，探索较少，但不一定能找到特定应用-系统对的最优算法。结合专家知识和RL学习可以提高性能和适应性。

Conclusion: 动态选择OpenMP应用程序的调度算法是可行且有益的，并且可以扩展到MPI程序以优化跨多个并行级别的调度决策。

Abstract: Scientific and data science applications are becoming increasingly complex,
with growing computational and memory demands. Modern high performance
computing (HPC) systems provide high parallelism and heterogeneity across
nodes, devices, and cores. To achieve good performance, effective scheduling
and load balancing techniques are essential. Parallel programming frameworks
such as OpenMP now offer a variety of advanced scheduling algorithms to support
diverse applications and platforms. This creates an instance of the scheduling
algorithm selection problem, which involves identifying the most suitable
algorithm for a given combination of workload and system characteristics.
  In this work, we explore learning-based approaches for selecting scheduling
algorithms in OpenMP. We propose and evaluate expert-based and reinforcement
learning (RL)-based methods, and conduct a detailed performance analysis across
six applications and three systems. Our results show that RL methods are
capable of learning high-performing scheduling decisions, although they require
significant exploration, with the choice of reward function playing a key role.
Expert-based methods, in contrast, rely on prior knowledge and involve less
exploration, though they may not always identify the optimal algorithm for a
specific application-system pair. By combining expert knowledge with RL-based
learning, we achieve improved performance and greater adaptability.
  Overall, this work demonstrates that dynamic selection of scheduling
algorithms during execution is both viable and beneficial for OpenMP
applications. The approach can also be extended to MPI-based programs, enabling
optimization of scheduling decisions across multiple levels of parallelism.

</details>


### [378] [RIMMS: Runtime Integrated Memory Management System for Heterogeneous Computing](https://arxiv.org/abs/2507.20514)
*Serhan Gener,Aditya Ukarande,Shilpa Mysore Srinivasa Murthy,Sahil Hassan,Joshua Mack,Chaitali Chakrabarti,Umit Ogras,Ali Akoglu*

Main category: cs.DC

TL;DR: RIMMS 是一个异构内存管理系统，可提高性能和简化编程。


<details>
  <summary>Details</summary>
Motivation: 在异构系统中，由于存在多种计算架构（如 CPU、GPU、FPGA）和编译时未知的动态任务映射，因此高效的内存管理变得越来越具有挑战性。现有的方法通常需要程序员显式管理数据放置和传输，或者假设静态映射，这会限制其可移植性和可扩展性。

Method: RIMMS 是一个轻量级的、运行时管理的、硬件无关的内存抽象层，它将应用程序开发与底层内存操作分离开来。RIMMS 能够透明地跟踪数据位置、管理一致性，并支持跨异构计算元素的内存分配，而无需进行平台特定的调整或代码修改。

Result: RIMMS 在 GPU 加速的系统中实现了高达 2.43 倍的加速，在 FPGA 加速的系统中实现了高达 1.82 倍的加速。与 IRIS 相比，RIMMS 加速高达 3.08 倍，并且性能与原生 CUDA 实现相当，同时显著降低了编程复杂度。RIMMS 的内存管理调用开销仅为每个调用 1-2 个周期。

Conclusion: RIMMS 能够为异构计算环境带来高性能和更高的编程效率。

Abstract: Efficient memory management in heterogeneous systems is increasingly
challenging due to diverse compute architectures (e.g., CPU, GPU, FPGA) and
dynamic task mappings not known at compile time. Existing approaches often
require programmers to manage data placement and transfers explicitly, or
assume static mappings that limit portability and scalability. This paper
introduces RIMMS (Runtime Integrated Memory Management System), a lightweight,
runtime-managed, hardware-agnostic memory abstraction layer that decouples
application development from low-level memory operations. RIMMS transparently
tracks data locations, manages consistency, and supports efficient memory
allocation across heterogeneous compute elements without requiring
platform-specific tuning or code modifications. We integrate RIMMS into a
baseline runtime and evaluate with complete radar signal processing
applications across CPU+GPU and CPU+FPGA platforms. RIMMS delivers up to 2.43X
speedup on GPU-based and 1.82X on FPGA-based systems over the baseline.
Compared to IRIS, a recent heterogeneous runtime system, RIMMS achieves up to
3.08X speedup and matches the performance of native CUDA implementations while
significantly reducing programming complexity. Despite operating at a higher
abstraction level, RIMMS incurs only 1-2 cycles of overhead per memory
management call, making it a low-cost solution. These results demonstrate
RIMMS's ability to deliver high performance and enhanced programmer
productivity in dynamic, real-world heterogeneous environments.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [379] [Fully Dynamic Spectral and Cut Sparsifiers for Directed Graphs](https://arxiv.org/abs/2507.19632)
*Yibin Zhao*

Main category: cs.DS

TL;DR: 本研究首次提出并实现了针对有向图的快速动态谱和割稀疏化算法，引入了度平衡保持谱近似新概念，并对部分对称化图和 $eta$-平衡有向图进行了研究，显著提高了稀疏化算法的效率和动态更新能力。


<details>
  <summary>Details</summary>
Motivation: 为了应对有向图稀疏化在快速和动态更新方面的挑战，本研究旨在开发高效的算法。

Method: 研究提出了两种算法：一种是具有$O(\varepsilon^{-2} \cdot \text{polylog}(n))$的均摊更新时间和$O(\varepsilon^{-2} n \cdot \text{polylog}(n))$大小的谱稀疏化算法；另一种是针对部分对称化图，可以保持一个常数因子近似稀疏化，其大小为$O(n \cdot \text{polylog}(n))$，并且具有对数级的均摊更新时间。此外，还提出了一种针对$eta$-平衡有向图的割稀疏化算法，其大小为$O(\varepsilon^{-2}\beta n \cdot \text{polylog}(n))$，最坏情况更新时间为$O(\varepsilon^{-2}\beta \cdot \text{polylog}(n))$。

Result: 研究成功开发了针对有向图的快速全动态谱稀疏化和割稀疏化算法，并提出了新的稀疏化概念和理论分析，为有向图稀疏化领域的研究提供了新的方向和工具。

Conclusion: 该研究首次提出了针对有向图的快速全动态谱和割稀疏化算法，并引入了度平衡保持谱近似这一新概念，以保持顶点的入度和出度之差。

Abstract: Recent years have seen extensive research on directed graph sparsification.
In this work, we initiate the study of fast fully dynamic spectral and cut
sparsification algorithms for directed graphs.
  We introduce a new notion of spectral sparsification called degree-balance
preserving spectral approximation, which maintains the difference between the
in-degree and out-degree of each vertex. The approximation error is measured
with respect to the corresponding undirected Laplacian. This notion is
equivalent to direct Eulerian spectral approximation when the input graph is
Eulerian. Our algorithm achieves an amortized update time of
$O(\varepsilon^{-2} \cdot \text{polylog}(n))$ and produces a sparsifier of size
$O(\varepsilon^{-2} n \cdot \text{polylog}(n))$. Additionally, we present an
algorithm that maintains a constant-factor approximation sparsifier of size
$O(n \cdot \text{polylog}(n))$ against an adaptive adversary for
$O(\text{polylog}(n))$-partially symmetrized graphs, a notion introduced in
[Kyng-Meierhans-Probst Gutenberg '22]. A $\beta$-partial symmetrization of a
directed graph $\vec{G}$ is the union of $\vec{G}$ and $\beta \cdot G$, where
$G$ is the corresponding undirected graph of $\vec{G}$. This algorithm also
achieves a polylogarithmic amortized update time.
  Moreover, we develop a fully dynamic algorithm for maintaining a cut
sparsifier for $\beta$-balanced directed graphs, where the ratio between
weighted incoming and outgoing edges of any cut is at most $\beta$. This
algorithm explicitly maintains a cut sparsifier of size
$O(\varepsilon^{-2}\beta n \cdot \text{polylog}(n))$ in worst-case update time
$O(\varepsilon^{-2}\beta \cdot \text{polylog}(n))$.

</details>


### [380] [Online Rounding Schemes for $ k $-Rental Problems](https://arxiv.org/abs/2507.19649)
*Hossein Nekouyan,Bo Sun,Raouf Boutaba,Xiaoqi Tan*

Main category: cs.DS

TL;DR: This paper presents algorithms for online resource allocation with reusable units in adversarial settings. It offers an optimal solution for fixed-duration rentals and an order-optimal solution for variable-duration rentals using novel rounding techniques.


<details>
  <summary>Details</summary>
Motivation: The paper studies two online resource-allocation problems with reusability in an adversarial setting: kRental-Fixed and kRental-Variable. The goal is to manage k identical reusable units and a sequence of rental requests over time.

Method: The paper develops theoretically grounded relax-and-round algorithms with provable competitive-ratio guarantees for both kRental-Fixed and kRental-Variable settings. For kRental-Fixed, an optimal fractional allocation is computed via a price-based approach, followed by a novel lossless online rounding scheme. For kRental-Variable, a limited-correlation rounding technique is introduced, coupled with a carefully crafted price-based method for computing the fractional allocation.

Result: For kRental-Fixed, an optimal randomized algorithm with the best possible competitive ratio is presented. For kRental-Variable, a limited-correlation rounding technique is introduced, which, combined with a price-based method, achieves an order-optimal competitive ratio.

Conclusion: For kRental-Fixed, we present an optimal randomized algorithm that attains the best possible competitive ratio. For kRental-Variable, we introduce a limited-correlation rounding technique that yields an order-optimal competitive ratio.

Abstract: We study two online resource-allocation problems with reusability in an
adversarial setting, namely kRental-Fixed and kRental-Variable. In both
problems, a decision-maker manages $k$ identical reusable units and faces a
sequence of rental requests over time. We develop theoretically grounded
relax-and-round algorithms with provable competitive-ratio guarantees for both
settings. For kRental-Fixed, we present an optimal randomized algorithm that
attains the best possible competitive ratio: it first computes an optimal
fractional allocation via a price-based approach, then applies a novel lossless
online rounding scheme to obtain an integral solution. For kRental-Variable, we
prove that lossless online rounding is impossible. We introduce a
limited-correlation rounding technique that treats each unit independently
while introducing controlled dependencies across allocation decisions involving
the same unit. Coupled with a carefully crafted price-based method for
computing the fractional allocation, this yields an order-optimal competitive
ratio for the variable-duration setting.

</details>


### [381] [Improved 2-Approximate Shortest Paths for close vertex pairs](https://arxiv.org/abs/2507.19859)
*Manoj Gupta*

Main category: cs.DS

TL;DR: 本文提出了一种新的近似最短路径算法，该算法在运行时间与现有算法相当的情况下，能够处理距离更近的顶点对。


<details>
  <summary>Details</summary>
Motivation: 改进Dor、Halperin和Zwick（1996年）关于近似最短路径算法的结果，该算法能够在$	ilde{O}(n^{2+O(1/k)})$时间内计算近似最短路径，但要求顶点对之间的距离至少为k。

Method: 该算法是组合式的、随机的，并且能够以高概率为所有顶点对返回正确的结果。

Result: 我们的算法实现了大约相同的$	ilde{O}(n^{2+1/k})$运行时间，但适用于距离至少为$O(	ext{log } k)$的顶点对。当$k=	ext{log } n$时，我们的算法运行时间为$	ilde{O}(n^2)$，并且适用于所有距离至少为$O(	ext{log log } n)$的顶点对。

Conclusion: 该研究首次在超过25年的时间里改进了Dor、Halperin和Zwick（1996年）的结果，提出了一个新的算法。

Abstract: An influential result by Dor, Halperin, and Zwick (FOCS 1996, SICOMP 2000)
implies an algorithm that can compute approximate shortest paths for all vertex
pairs in $\tilde{O}(n^{2+O\left(\frac{1}{k}\right )})$ time, ensuring that the
output distance is at most twice the actual shortest path, provided the pairs
are at least $k$ apart, where $k \ge 2$. We present the first improvement on
this result in over 25 years. Our algorithm achieves roughly same
$\tilde{O}(n^{2+\frac{1}{k}})$ runtime but applies to vertex pairs merely
$O(\log k)$ apart, where $\log k \ge 1$. When $k=\log n$, the running time of
our algorithm is $\tilde{O}(n^2)$ and it works for all pairs at least $O(\log
\log n)$ apart. Our algorithm is combinatorial, randomized, and returns correct
results for all pairs with a high probability.

</details>


### [382] [Generating Satisfiable Benchmark Instances for Stable Roommates Problems with Optimization](https://arxiv.org/abs/2507.20013)
*Baturay Yılmaz,Esra Erdem*

Main category: cs.DS

TL;DR: 该研究提出了一种新算法，用于生成具有许多解决方案且难以找到公平稳定匹配的SRI实例，解决了现有基准实例的局限性。


<details>
  <summary>Details</summary>
Motivation: 为了对解决SRI困难变体的方法进行实验评估，研究人员使用了几种著名的算法来随机生成基准实例。然而，这些基准实例并不总是可满足的，并且通常只有少数稳定匹配。因此，需要一种能够生成具有大量解决方案且难以找到平等等价稳定匹配的SRI实例的新算法。

Method: 提出了一种新的算法来生成SRI的基准实例，该实例具有大量解决方案，并且难以通过枚举所有稳定匹配来找到平等等价稳定匹配。

Result: 该研究引入了一种新颖的算法，用于生成具有大量解决方案且难以通过枚举所有稳定匹配来找到平等等价稳定匹配的SRI基准实例。

Conclusion: 该研究介绍了一种新颖的算法，用于生成具有大量解决方案且难以通过枚举所有稳定匹配来查找平等等价稳定匹配的SRI基准实例。

Abstract: While the existence of a stable matching for the stable roommates problem
possibly with incomplete preference lists (SRI) can be decided in polynomial
time, SRI problems with some fairness criteria are intractable. Egalitarian SRI
that tries to maximize the total satisfaction of agents if a stable matching
exists, is such a hard variant of SRI. For experimental evaluations of methods
to solve these hard variants of SRI, several well-known algorithms have been
used to randomly generate benchmark instances. However, these benchmark
instances are not always satisfiable, and usually have a small number of stable
matchings if one exists. For such SRI instances, despite the NP-hardness of
Egalitarian SRI, it is practical to find an egalitarian stable matching by
enumerating all stable matchings. In this study, we introduce a novel algorithm
to generate benchmark instances for SRI that have very large numbers of
solutions, and for which it is hard to find an egalitarian stable matching by
enumerating all stable matchings.

</details>


### [383] [Parallel Hierarchical Agglomerative Clustering in Low Dimensions](https://arxiv.org/abs/2507.20047)
*MohammadHossein Bateni,Laxman Dhulipala,Willem Fletcher,Kishen N Gowda,D Ellis Hershkowitz,Rajesh Jayaram,Jakub Łącki*

Main category: cs.DS

TL;DR: 本研究为质心和 Ward 链接等非单调链接函数开发了 $(1+\epsilon)$-近似分层凝聚聚类（HAC）的 NC 算法，但仅限于低维度。在高维度下，使用质心距离的 HAC 是 CC-hard 的。


<details>
  <summary>Details</summary>
Motivation: 许多重要的非单调链接函数（如质心和 Ward 链接）的分层凝聚聚类（HAC）缺乏高效的并行（NC）算法。本研究旨在为这类链接函数开发 $(1+\epsilon)$-近似 HAC 的 NC 算法。

Method: 该研究开发了基于结构性结果的 NC 算法，该结构性结果指出，对于质心和 Ward 链接等链接函数，当维度 $k = O(\log \log n / \log \log \log n)$ 时，分层凝聚聚类产生的层次结构高度最多为 $\operatorname{poly}(\log n)$。

Result: 研究人员成功开发了适用于质心和 Ward 链接等非单调链接函数的 $(1+\epsilon)$-近似 HAC 的 NC 算法，前提是维度较低。他们还证明了在任意维度下，使用质心距离的 HAC 是 CC-hard 的。

Conclusion: 该研究表明，对于某些非单调链接函数（包括质心和 Ward 链接），在低维度下，使用 NC 算法可以实现 $(1+\epsilon)$-近似分层凝聚聚类。研究还发现，对于这些链接函数，当维度 $k = O(\log \log n / \log \log \log n)$ 时，层次结构的高度最多为 $\operatorname{poly}(\log n)$。此外，研究还证明了在任意维度下，使用质心距离的分层凝聚聚类是 CC-hard 的，这表明不存在有效的 NC 算法。

Abstract: Hierarchical Agglomerative Clustering (HAC) is an extensively studied and
widely used method for hierarchical clustering in $\mathbb{R}^k$ based on
repeatedly merging the closest pair of clusters according to an input linkage
function $d$. Highly parallel (i.e., NC) algorithms are known for
$(1+\epsilon)$-approximate HAC (where near-minimum rather than minimum pairs
are merged) for certain linkage functions that monotonically increase as merges
are performed. However, no such algorithms are known for many important but
non-monotone linkage functions such as centroid and Ward's linkage.
  In this work, we show that a general class of non-monotone linkage functions
-- which include centroid and Ward's distance -- admit efficient NC algorithms
for $(1+\epsilon)$-approximate HAC in low dimensions. Our algorithms are based
on a structural result which may be of independent interest: the height of the
hierarchy resulting from any constant-approximate HAC on $n$ points for this
class of linkage functions is at most $\operatorname{poly}(\log n)$ as long as
$k = O(\log \log n / \log \log \log n)$. Complementing our upper bounds, we
show that NC algorithms for HAC with these linkage functions in
\emph{arbitrary} dimensions are unlikely to exist by showing that HAC is
CC-hard when $d$ is centroid distance and $k = n$.

</details>


### [384] [Adaptive BSTs for Single-Source and All-to-All Requests: Algorithms and Lower Bounds](https://arxiv.org/abs/2507.20228)
*Maryam Shiran*

Main category: cs.DS

TL;DR: 提出了一种用于自适应二叉搜索树的统一框架，证明了其在不同模型下的性能界限，并提出了离线和在线算法。


<details>
  <summary>Details</summary>
Motivation: 为了解决网络和分布式系统中自适应二叉搜索树的效率和响应性问题。

Method: 提出了一种用于单源模型的离线算法，并将其扩展到全双模型，同时开发了一个用于确定性在线自适应二叉搜索树的通用数学框架，并提出了一种用于单源情况的确定性在线策略。

Result: 在离线设置中，为两种模型都证明了上界，并表明了任何离线算法的成本下界。在在线设置中，提出了单源情况的确定性在线策略，并建立了任何确定性在线算法的竞争比下界。

Conclusion: 该研究提出了用于自适应二叉搜索树的统一框架，并在单源和全双模型下进行了分析，证明了上界和下界。

Abstract: Adaptive binary search trees are a fundamental data structure for organizing
hierarchical information. Their ability to dynamically adjust to access
patterns makes them particularly valuable for building responsive and efficient
networked and distributed systems.
  We present a unified framework for adaptive binary search trees with fixed
restructuring cost, analyzed under two models: the single-source model, where
the cost of querying a node is proportional to its distance from a fixed
source, and the all-to-all model, where the cost of serving a request depends
on the distance between the source and destination nodes. We propose an offline
algorithm for the single-source model and extend it to the all-to-all model.
For both models, we prove upper bounds on the cost incurred by our algorithms.
Furthermore, we show the existence of input sequences for which any offline
algorithm must incur a cost comparable to ours.
  In the online setting, we develop a general mathematical framework for
deterministic online adaptive binary search trees and propose a deterministic
online strategy for the single-source case, which naturally extends to the
all-to-all model. We also establish lower bounds on the competitive ratio of
any deterministic online algorithm, highlighting fundamental limitations of
online adaptivity.

</details>


### [385] [The Min Max Average Cycle Weight Problem](https://arxiv.org/abs/2507.20253)
*Noga Klein Elmalem,Rica Gonen,Erel Segal-Halevi*

Main category: cs.DS

TL;DR: 拆迁后如何公平分配公寓以减少居民嫉妒，这是一个组合优化问题，但考虑先有条件时，它比最大权重匹配更复杂。


<details>
  <summary>Details</summary>
Motivation: 当旧公寓楼被拆除重建时，如何公平地重新分配新公寓以最小化居民之间的嫉妒。

Method: 将公寓分配问题转化为“最小最大平均环权重”的组合优化问题。

Result: 当考虑居民对其原始公寓的满意度等先有条件时，该问题不能简化为最大权重匹配问题。该问题在一般情况下是否可多项式时间解决仍然是一个有趣的未解之谜。

Conclusion: 该问题简化为一个组合优化问题，称为最小最大平均环权重问题。该问题旨在以最小化相关嫉妒图中所有有向环的最大平均权重的方式来分配对象给代理。虽然这个问题可以简化为最大权重匹配问题，但当考虑居民对其原始公寓的满意度等先有条件时，情况并非如此。该问题在一般情况下是否可多项式时间解决仍然是一个有趣的未解之谜。

Abstract: When an old apartment building is demolished and rebuilt, how can we fairly
redistribute the new apartments to minimize envy among residents? We reduce
this question to a combinatorial optimization problem called the *Min Max
Average Cycle Weight* problem. In that problem we seek to assign objects to
agents in a way that minimizes the maximum average weight of directed cycles in
an associated envy graph. While this problem reduces to maximum-weight matching
when starting from a clean slate (achieving polynomial-time solvability), we
show that this is not the case when we account for preexisting conditions, such
as residents' satisfaction with their original apartments. Whether the problem
is polynomial-time solvable in the general case remains an intriguing open
problem.

</details>


### [386] [Faster exact learning of k-term DNFs with membership and equivalence queries](https://arxiv.org/abs/2507.20336)
*Josh Alman,Shivam Nadimpalli,Shyamal Patel,Rocco Servedio*

Main category: cs.DS

TL;DR: 新算法使用成员资格查询和等价性查询，在poly(n) * 2^O(sqrt(k))时间内学习k项DNF公式，这是对1992年以来该领域的一个改进。


<details>
  <summary>Details</summary>
Motivation: 为学习k项DNF公式提供一种比现有算法（poly(n, 2^k)）更优化的算法，特别是将运行时从poly(n, 2^k)改进到poly(n) * 2^O(sqrt(k))。

Method: 该方法采用Winnow2算法，在增强的特征空间中学习线性阈值函数，该特征空间使用成员资格查询自适应地构建。它结合了[BR92]的简化DNF项长度的技术，以及用于低权重线性阈值函数的属性高效学习算法和用于从junta测试中查找相关变量的技术，还有用于基于查询的DNF学习的新分析工具（极值多项式和噪声算子）。

Result: 所提出的算法在学习k项DNF公式时，运行时为poly(n) * 2^O(sqrt(k))，这是自1992年以来该问题的一个改进。

Conclusion: 该算法是自Blum和Rudich [BR92]以来的第一个改进，它使用成员资格查询和等价性查询在poly(n) * 2^O(sqrt(k))时间内学习k项DNF公式。

Abstract: In 1992 Blum and Rudich [BR92] gave an algorithm that uses membership and
equivalence queries to learn $k$-term DNF formulas over $\{0,1\}^n$ in time
$\textsf{poly}(n,2^k)$, improving on the naive $O(n^k)$ running time that can
be achieved without membership queries [Val84]. Since then, many alternative
algorithms [Bsh95, Kus97, Bsh97, BBB+00] have been given which also achieve
runtime $\textsf{poly}(n,2^k)$.
  We give an algorithm that uses membership and equivalence queries to learn
$k$-term DNF formulas in time $\textsf{poly}(n) \cdot 2^{\tilde{O}(\sqrt{k})}$.
This is the first improvement for this problem since the original work of Blum
and Rudich [BR92].
  Our approach employs the Winnow2 algorithm for learning linear threshold
functions over an enhanced feature space which is adaptively constructed using
membership queries. It combines a strengthened version of a technique that
effectively reduces the length of DNF terms from the original work of [BR92]
with a range of additional algorithmic tools (attribute-efficient learning
algorithms for low-weight linear threshold functions and techniques for finding
relevant variables from junta testing) and analytic ingredients (extremal
polynomials and noise operators) that are novel in the context of query-based
DNF learning.

</details>


### [387] [Deterministic Almost-Linear-Time Gomory-Hu Trees](https://arxiv.org/abs/2507.20354)
*Amir Abboud,Rasmus Kyng,Jason Li,Debmalya Panigrahi,Maximilian Probst Gutenberg,Thatchaphol Saranurak,Weixuan Yuan,Wuwei Yuan*

Main category: cs.DS

TL;DR: 使用新的去随机化技术，首次实现图论中 Gomory-Hu 树的近乎线性时间确定性构建算法。


<details>
  <summary>Details</summary>
Motivation: 旨在解决图论中构建 Gomory-Hu 树的效率问题，并为寻找 k-边连通分量等简化问题提供首个近乎线性时间的确定性算法。

Method: 本研究的关键在于两个新颖的组成部分：1. 将所有节点对最小割问题确定性地归约到单源最小割问题，且仅引入了亚多项式开销。 2. 提出了一种单源最小割问题的确定性近乎线性时间算法。

Result: 成功开发了一种时间复杂度为 $m^{1+o(1)}$ 的确定性算法，用于构建 Gomory-Hu 树，这是该领域的一项重大突破。

Conclusion: 该研究首次提出了一个近乎最优的确定性算法，能在 $m^{1+o(1)}$ 时间内构建 Gomory-Hu 树，显著优于之前依赖于 $nm^{1+o(1)}$ 时间的算法。

Abstract: Given an $m$-edge, undirected, weighted graph $G=(V,E,w)$, a Gomory-Hu tree
$T$ (Gomory and Hu, 1961) is a tree over the vertex set $V$ such that all-pairs
mincuts in $G$ are preserved exactly in $T$.
  In this article, we give the first almost-optimal $m^{1+o(1)}$-time
deterministic algorithm for constructing a Gomory-Hu tree. Prior to our work,
the best deterministic algorithm for this problem dated back to the original
algorithm of Gomory and Hu that runs in $nm^{1+o(1)}$ time (using current
maxflow algorithms). In fact, this is the first almost-linear time
deterministic algorithm for even simpler problems, such as finding the
$k$-edge-connected components of a graph.
  Our new result hinges on two separate and novel components that each
introduce a distinct set of de-randomization tools of independent interest:
  - a deterministic reduction from the all-pairs mincuts problem to the
single-souce mincuts problem incurring only subpolynomial overhead, and
  - a deterministic almost-linear time algorithm for the single-source mincuts
problem.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [388] [Van der Waals injection-molded crystals](https://arxiv.org/abs/2507.19597)
*Vinh Tran,Amy X. Wu,Laisi Chen,Ziyu Feng,Vijay Kumar,Takashi Taniguichi,Kenji Watanabe,Javier Sanchez-Yamagishi*

Main category: cond-mat.mtrl-sci

TL;DR: 通过在vdW材料中注入熔融材料，可以制备任意几何形状的低缺陷单晶，并控制厚度，无需后处理。


<details>
  <summary>Details</summary>
Motivation: 在低缺陷性情况下，将低维晶体塑造成精确的几何形状是一个突出的挑战。

Method: 通过在由SiO2模具构成的、原子级平整的vdW材料层之间注入熔融材料，来生长单晶。

Result: 成功制备了铋、锡和铟的超平坦、薄型单晶，其形状包括霍尔条、环和纳米线。晶体厚度可控（10-100 nm），且在整个模具几何形状中包含了大的单晶，实现了低缺陷散射。

Conclusion: 该方法为在范德华力（vdW）材料中生长任意几何形状的单晶提供了可能，且具有低缺陷性，可以实现不需要后处理的复杂单晶纳米结构的制备。

Abstract: Shaping low-dimensional crystals into precise geometries with low disorder is
an outstanding challenge. Here, we present a method to grow single crystals of
arbitrary geometry within van der Waals (vdW) materials. By injecting molten
material between atomically-flat vdW layers within an SiO2 mold, we produce
ultraflat and thin crystals of bismuth, tin, and indium that are shaped as
hallbars, rings, and nanowires. The crystals are grown fully encapsulated in
hexagonal boron nitride, a vdW material, providing protection from oxidation.
Varying the depth of the mold allows us to control the crystal thickness from
ten to a hundred nanometers. Structural measurements demonstrate large
single-crystals encompassing the entire mold geometry, while transport
measurements show reduced disorder scattering. This approach offers a means to
produce complex single-crystal nanostructures without the disorder introduced
by post-growth nanofabrication.

</details>


### [389] [Learning disentangled latent representations facilitates discovery and design of functional materials](https://arxiv.org/abs/2507.19602)
*Jaehoon Cha,Tingyao Lu,Matthew Walker,Keith T. Butler*

Main category: cond-mat.mtrl-sci

TL;DR: DAEs learn meaningful features from spectral data without labels for efficient materials discovery, outperforming PCA and VAEs.


<details>
  <summary>Details</summary>
Motivation: New material discovery is often constrained by the need for large labelled datasets or expensive simulations. This study explores the use of DAEs for unsupervised learning of spectral data representations.

Method: Disentangling Autoencoders (DAEs) were used to learn compact and interpretable representations of spectral data in an unsupervised manner. Compared DAE with PCA and beta-VAE.

Result: DAE captures physically meaningful features in optical absorption spectra correlated with photovoltaic (PV) performance, specifically a latent dimension correlated with the Spectroscopic Limited Maximum Efficiency (SLME), corresponding to the transition from direct to indirect optical band gaps. DAE achieved superior reconstruction fidelity, improved correlation with efficiency metrics, and more compact encoding of relevant features than PCA and beta-VAE. DAE latent space enables more efficient discovery of high-performing PV materials, identifying top candidates using fewer evaluations than VAE-guided and random search.

Conclusion: DAEs are a powerful tool for unsupervised structure-property learning with broad applicability to materials discovery where labeled data is limited but rich structure is present in raw signals.

Abstract: The discovery of new materials is often constrained by the need for large
labelled datasets or expensive simulations. In this study, we explore the use
of Disentangling Autoencoders (DAEs) to learn compact and interpretable
representations of spectral data in an entirely unsupervised manner. We
demonstrate that the DAE captures physically meaningful features in optical
absorption spectra, relevant to photovoltaic (PV) performance, including a
latent dimension strongly correlated with the Spectroscopic Limited Maximum
Efficiency (SLME)--despite being trained without access to SLME labels. This
feature corresponds to a well-known spectral signature: the transition from
direct to indirect optical band gaps. Compared to Principal Component Analysis
(PCA) and a beta-Variational Autoencoder (beta-VAE), the DAE achieves superior
reconstruction fidelity, improved correlation with efficiency metrics, and more
compact encoding of relevant features. We further show that the DAE latent
space enables more efficient discovery of high-performing PV materials,
identifying top candidates using fewer evaluations than both VAE-guided and
random search. These results highlight the potential of DAEs as a powerful tool
for unsupervised structure-property learning and suggest broad applicability to
other areas of materials discovery where labeled data is limited but rich
structure is present in raw signals.

</details>


### [390] [Data-efficient machine-learning of complex Fe-Mo intermetallics using domain knowledge of chemistry and crystallography](https://arxiv.org/abs/2507.19660)
*Mariano Forti,Alesya Malakhova,Yury Lysogorskiy,Wenhao Zhang,Jean-Claude Crivello,Jean-Marc Joubert,Ralf Drautz,Thomas Hammerschmidt*

Main category: cond-mat.mtrl-sci

TL;DR: 通过结合化学和晶体学知识的机器学习模型，能够用少量数据高效准确地预测复杂TCP相的能量和占有情况，实验验证也证实了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了在多组分系统（如Fe-Mo）的原子模拟中准确描述相的能量，需要克服拓扑密堆积（TCP）相的挑战，这类相在能量上竞争激烈，即使在简单的二元系统中也是如此。

Method: 利用包含化学和晶体学领域知识的特征，训练基于核脊回归、多层感知器和随机森林的机器学习模型。仅使用少量的DFT计算数据（少于300个），模型即可对具有11-14个原子夫的复杂TCP相（R, M, P, δ）进行准确预测，预测不确定性为20-25 meV/atom。

Result: 机器学习模型在Fe-Mo系统中的R, M, P, δ相的凸包预测上达到了20-25 meV/atom的不确定性，并且与DFT验证结果高度一致。此外，通过X射线衍射实验和Rietveld分析的Fe-Mo R相样品的测量结果，与基于布拉格-沃尔顿近似的机器学习模型的预测结果在相同温度下的原子夫占有率上表现出极好的一致性。

Conclusion: 机器学习模型通过结合化学和晶体学领域的知识，能够高效且准确地预测复杂多组分系统（如Fe-Mo）中的拓扑密堆积（TCP）相的能量和占位情况。

Abstract: Atomistic simulations of multi-component systems require accurate
descriptions of interatomic interactions to resolve details in the energy of
competing phases. A particularly challenging case are topologically
close-packed (TCP) phases with close energetic competition of numerous
different site occupations even in binary systems like Fe-Mo. In this work,
machine learning (ML) models are presented that overcome this challenge by
using features with domain knowledge of chemistry and crystallography. The
resulting data-efficient ML models need only a small set of training data of
simple TCP phases $A$15, $\sigma$, $\chi$, $\mu$, $C$14, $C$15, $C$36 with 2-5
WS to reach robust and accurate predictions for the complex TCP phases $R$,
$M$, $P$, $\delta$ with 11-14 WS. Several ML models with kernel-ridge
regression, multi-layer perceptrons, and random forests, are trained on less
than 300 DFT calculations for the simple TCP phases in Fe-Mo. The performance
of these ML models is shown to improve systematically with increased
utilization of domain knowledge. The convex hulls of the $R$, $M$, $P$ and
$\delta$ phase in the Fe-Mo system are predicted with uncertainties of 20-25
meV/atom and show very good agreement with DFT verification. Complementary
X-ray diffraction experiments and Rietveld analysis are carried out for an
Fe-Mo R-phase sample. The measured WS occupancy is in excellent agreement with
the predictions of our ML model using the Bragg Williams approximation at the
same temperature.

</details>


### [391] [Making atomistic materials calculations accessible with the AiiDAlab Quantum ESPRESSO app](https://arxiv.org/abs/2507.19670)
*Xing Wang,Edan Bainglass,Miki Bonacci,Andres Ortega-Guerrero,Lorenzo Bastonero,Marnik Bercx,Pietro Bonfà,Roberto De Renzi,Dou Du,Peter N. O. Gillespie,Michael A. Hernández-Bertrán,Daniel Hollas,Sebastiaan P. Huber,Elisa Molinari,Ifeanyi J. Onuorah,Nataliya Paulish,Deborah Prezzi,Junfeng Qiao,Timo Reents,Christopher J. Sewell,Iurii Timrov,Aliaksandr V. Yakutovich,Jusong Yu,Nicola Marzari,Carlo A. Pignedoli,Giovanni Pizzi*

Main category: cond-mat.mtrl-sci

TL;DR: Quantum ESPRESSO app 是一个基于 AiiDAlab 的 Web 平台，简化了 DFT 计算，通过易用的界面和自动化工作流，降低了材料科学家的使用门槛，并提高了结果的可重复性。


<details>
  <summary>Details</summary>
Motivation: 为了克服材料科学界在采用密度泛函理论 (DFT) 代码时遇到的软件安装、输入准备、高性能计算设置和输出分析等挑战。

Method: 开发了一个基于 AiiDAlab 的、直观的、基于 Web 的应用程序，名为 Quantum ESPRESSO app。该应用程序采用了模块化的输入-处理-输出模型和基于插件的架构，集成了用户友好的图形界面和自动化的 DFT 工作流，包括预定义的计算协议、自动错误处理和交互式结果可视化。

Result: 通过用于电子能带结构、态密度、声子、红外/拉曼光谱、X 射线和 μ 子光谱、Hubbard 参数 (DFT+$U$+V)、Wannier 函数和后处理工具的插件，展示了应用程序的功能。

Conclusion: 该应用程序通过扩展 FAIR 原则，增强了先进 DFT 计算的可访问性和可重复性，并为与其它从第一性原理计算代码接口提供了通用模板。

Abstract: Despite the wide availability of density functional theory (DFT) codes, their
adoption by the broader materials science community remains limited due to
challenges such as software installation, input preparation, high-performance
computing setup, and output analysis. To overcome these barriers, we introduce
the Quantum ESPRESSO app, an intuitive, web-based platform built on AiiDAlab
that integrates user-friendly graphical interfaces with automated DFT
workflows. The app employs a modular Input-Process-Output model and a
plugin-based architecture, providing predefined computational protocols,
automated error handling, and interactive results visualization. We demonstrate
the app's capabilities through plugins for electronic band structures,
projected density of states, phonon, infrared/Raman, X-ray and muon
spectroscopies, Hubbard parameters (DFT+$U$+$V$), Wannier functions, and
post-processing tools. By extending the FAIR principles to simulations,
workflows, and analyses, the app enhances the accessibility and reproducibility
of advanced DFT calculations and provides a general template to interface with
other first-principles calculation codes.

</details>


### [392] [Magnetic ground state, critical analysis of magnetization, and large magnetocaloric effect in the ferromagnetically coupled kagome lattice YCa$_3$(MnO)$_3$(BO$_3$)$_4$](https://arxiv.org/abs/2507.19779)
*A. Choudhary,A. Magar,S. Ghosh,S. Kanungo,R. Nath*

Main category: cond-mat.mtrl-sci

TL;DR: YCa$_3$(MnO)$_3$(BO$_3$)$_4$ 是一种挫性的 kagome 材料，具有大的磁热效应和二阶相变，是潜在的磁制冷材料。


<details>
  <summary>Details</summary>
Motivation: 研究 YCa$_3$(MnO)$_3$(BO$_3$)$_4$ 的磁性质、磁热效应及其作为磁制冷材料的潜力。

Method: 通过实验测量（磁性、磁化临界分析、磁热效应）和密度泛函带结构计算研究了 YCa$_3$(MnO)$_3$(BO$_3$)$_4$ 的磁性质。

Result: 研究发现该化合物具有高度挫性，呈现出不寻常的铁磁序（$T^* 
eq 7.8$ K），并有场诱导的亚磁转变。带结构计算揭示了链内主要的铁磁相互作用和链间的反铁磁耦合，形成了挫性的 kagome 结构。该化合物在 7 T 场变化下表现出大的磁热效应（$m \Delta S_m \approx 12$ J/kg-K, $\rm \Delta T_{ad} \approx 8.4$ K, RCP $\neq 349$ J/kg）。临界分析表明该相变为二阶、三临界点平均场类型。

Conclusion: YCa$_3$(MnO)$_3$(BO$_3$)$_4$ 是一种有潜力的不含稀土的磁制冷材料，因其具有大的磁热参数、二阶相变和无热滞性。

Abstract: We report a detailed study of the magnetic properties, critical analysis of
magnetization, and magnetocaloric effect of a spin-$2$ kagome lattice
YCa$_3$(MnO)$_3$(BO$_3$)$_4$. The experiments are complemented by the density
functional band structure calculations. The magnetic measurements suggest a
highly frustrated nature of the compound due to competing ferro- and
antiferromagnetic interactions with the dominant one being ferromagnetic. It
undergoes a unconventional ferromagnetic ordering at $T^* \simeq 7.8$ K and a
field induced metamagnetic transition in low fields, implying spin canting. A
$H$-$T$ phase diagram is constructed that features three phase regimes. Indeed,
the band structure calculations reveal dominant ferromagnetic interaction along
the chains that are coupled antiferromagnetically yielding a frustrated kagome
geometry. This compound shows a large magnetocaloric effect with isothermal
entropy change $\Delta S_{\rm m} \simeq 12$ J/kg-K, adiabatic temperature
change $\Delta T_{\rm ad} \simeq 8.4$ K, and relative cooling power $RCP \simeq
349$ J/kg for a field change of 7 T. The critical analysis of magnetization and
magnetocaloric parameters suggests that the transition is a second order phase
transition and it is tricritical mean field type. Owing to it's large
magnetocaloric parameters, second order phase transition, and no thermal
hysteresis, YCa$_3$(MnO)$_3$(BO$_3$)$_4$ emerges as a potential rare-earth free
material for magnetic refrigeration.

</details>


### [393] [Band-selective Plasmonic Polaron in Thermoelectric Semimetal Ta$_2$PdSe$_6$ with ultra-high power factor](https://arxiv.org/abs/2507.19794)
*Daiki Ootsuki,Akitoshi Nakano,Urara Maruoka,Takumi Hasegawa,Masashi Arita,Miho Kitamura,Koji Horiba,Teppei Yoshida,Ichiro Terasaki*

Main category: cond-mat.mtrl-sci

TL;DR: ARPES 研究了 thermoelectric semimetal Ta$_2$PdSe$_6$ 的电子结构，发现其独特的能带特性和载流子动力学是实现高 thermoelectric 性能的关键，为新型 thermoelectric 材料的探索提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 探索具有高 thermoelectric power factor 和 giant Peltier conductivity 的 thermoelectric semimetal Ta$_2$PdSe$_6$ 的电子结构，并理解其优异的 theermolectric 性能的来源。

Method: Angle-resolved photoemission spectroscopy (ARPES)

Result: ARPES 测量显示 Ta$_2$PdSe$_6$ 具有尖锐的空穴带（轻电子质量）和宽的电子带（相对较重的电子质量），这些带源于不同的准一维（Q1D）链。电子带在布里渊区边界附近表现出由电子-等离子体相互作用引起的等离子体极化激子重复结构。不同链中的载流子具有不同的散射效应和相互作用，导致不对称的输运寿命，从而在 semimetal 中实现了高 Seebeck 系数。

Conclusion: Ta$_2$PdSe$_6$是一种具有高 thermoelectric power factor 和 giant Peltier conductivity 的 thermoelectric semimetal。ARPES 结果揭示了其独特的电子结构，包括由不同的准一维（Q1D）链产生的具有不同质量的空穴带和电子带。此外，电子带的重复结构源于电子-等离子体相互作用。这些因素共同作用，使得 Ta$_2$PdSe$_6$ 即使在 semimetal 中也能实现高 Seebeck 系数。

Abstract: We report the electronic structure of the thermoelectric semimetal
Ta$_2$PdSe$_6$ with a large thermoelectric power factor and giant Peltier
conductivity by means of angle-resolved photoemission spectroscopy (ARPES). The
ARPES spectra reveal the coexistence of a sharp hole band with a light electron
mass and a broad electron band with a relatively heavy electron mass, which
originate from different quasi-one-dimensional (Q1D) chains in Ta$_2$PdSe$_6$.
Moreover, the electron band around the Brillouin-zone (BZ) boundary shows a
replica structure with respect to the energy originating from plasmonic
polarons due to electron-plasmon interactions. The different scattering effects
and interactions in each atomic chain lead to asymmetric transport lifetimes of
carriers: a large Seebeck coefficient can be realized even in a semimetal. Our
findings pave the way for exploring the thermoelectric materials in previously
overlooked semimetals and provide a new platform for low-temperature
thermoelectric physics, which has been challenging with semiconductors.

</details>


### [394] [Enhancing Materials Discovery with Valence Constrained Design in Generative Modeling](https://arxiv.org/abs/2507.19799)
*Mouyang Cheng,Weiliang Luo,Hao Tang,Bowen Yu,Yongqiang Cheng,Weiwei Xie,Ju Li,Heather J. Kulik,Mingda Li*

Main category: cond-mat.mtrl-sci

TL;DR: CrysVCD是一个结合了化学规则的生成框架，能够生成化学有效且稳定的晶体结构，并可用于发现功能材料。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的深度生成模型在反向材料设计中存在化学约束（如氧化态平衡）被忽视的问题，这可能导致生成化学上无效的结构。因此，需要一种能够直接集成化学规则的框架，以确保生成材料的化学有效性。

Method: CrysVCD框架首先使用基于Transformer的元素语言模型生成价态平衡的成分，然后使用扩散模型生成晶体结构。它在生成过程中直接集成化学规则（例如氧化态平衡），以确保化学有效性。

Result: CrysVCD实现了85%的热力学稳定性和68%的声子稳定性，并且在有条件生成功能材料方面表现出色，例如高导热性半导体和高介电常数化合物。

Conclusion: CrysVCD是一个模块化框架，可将化学规则直接集成到生成过程中，从而实现化学有效且热力学和声子稳定的晶体结构生成。它在生成过程中集成化学规则，实现了数量级上更高效的化学价态检查，并且可以有条件地生成功能材料，从而发现具有所需特性的候选材料。CrysVCD设计为通用的插件，可以集成到各种生成流程中，以促进化学有效性，为材料发现提供了可靠的、有科学依据的途径。

Abstract: Diffusion-based deep generative models have emerged as powerful tools for
inverse materials design. Yet, many existing approaches overlook essential
chemical constraints such as oxidation state balance, which can lead to
chemically invalid structures. Here we introduce CrysVCD (Crystal generator
with Valence-Constrained Design), a modular framework that integrates chemical
rules directly into the generative process. CrysVCD first employs a
transformer-based elemental language model to generate valence-balanced
compositions, followed by a diffusion model to generate crystal structures. The
valence constraint enables orders-of-magnitude more efficient chemical valence
checking, compared to pure data-driven approaches with post-screening. When
fine-tuned on stability metrics, CrysVCD achieves 85% thermodynamic stability
and 68% phonon stability. Moreover, CrysVCD supports conditional generation of
functional materials, enabling discovery of candidates such as high thermal
conductivity semiconductors and high-$\kappa$ dielectric compounds. Designed as
a general-purpose plugin, CrysVCD can be integrated into diverse generative
pipeline to promote chemical validity, offering a reliable, scientifically
grounded path for materials discovery.

</details>


### [395] [A comprehensive first principles investigation of A$_2$BH$_6$ type (A= Li,Na, and K; B= Al, and Si) double perovskite hydrides for high capacity hydrogen storage](https://arxiv.org/abs/2507.19810)
*R. Zosiamliana,Lalhriat Zuala,Shivraj Gurung,R. Lalmalsawma,A. Laref,A. Yvaz,D. P. Rai*

Main category: cond-mat.mtrl-sci

TL;DR: This paper explores the properties of A$_2$BH$_6$ complex hydrides using first-principles calculations to assess their potential for hydrogen energy storage. The study confirms their stability and identifies Li$_2$AlH$_6$ and Li$_2$SiH$_6$ as promising candidates due to their high hydrogen storage capacity and suitable desorption temperatures.


<details>
  <summary>Details</summary>
Motivation: Recent breakthroughs in vacancy-ordered double perovskite hydride materials highlight their potential for high-capacity hydrogen energy storage. This study aims to systematically explore the properties of A$_2$BH$_6$ complex hydrides for such applications.

Method: Extensive first-principles calculations using GGA and hybrid-HSE06 functionals were performed to explore the intrinsic properties of A$_2$BH$_6$ complex hydrides. Thermodynamic and mechanical stability were assessed, along with electronic and optical properties. Hydrogen storage capacities and densities were evaluated against U.S. DOE benchmarks.

Result: The study confirms the thermodynamic and mechanical stability of A$_2$BH$_6$ hydrides. Si-based hydrides exhibit semiconducting behavior with optical absorption peaks >106 1/cm in the UV regime, suitable for UV-optoelectronic devices. Al-based hydrides are metallic. All studied compounds follow Debye

Conclusion: Li$_2$AlH$_6$ and Li$_2$SiH$_6$ are the most promising candidates for hydrogen storage due to their high gravimetric hydrogen capacity (>12.0%), high volumetric hydrogen density (>140 g.H$_2$/L), and favorable hydrogen desorption temperature ranges (450-650 K).

Abstract: Recent breakthroughs in vacancy-ordered double perovskite hydride materials
have underscored their significant potential for integration into
next-generation high-capacity hydrogen energy storage systems. We perform
extensive first principles calculations leveraging both the GGA and
hybrid-HSE06 functionals to systematically explore the intrinsic properties of
A$_2$BH$_6$ complex hydrides. Thermodynamic stability for each hydride is
demonstrated and confirmed by negative formation energies, determined by both
the GGA and HSE06 formalisms. Additionally, mechanical stability is validated
through compliance with Born's stability criteria. Electronic properties
analysis reveals a semiconducting behavior in Si based hydrides (A2SiH6 ),
whereas Al based (A$_2$AlH$_6$) display metallic nature, regardless of the A
site atoms and functionals adopted. For the semiconducting hydrides, we have
observed higher optical absorption peak greater than 106 (1/cm) in the UV
regime indicating potential application in UV-optoelectronic devices.
Furthermore, all studied compounds adhere to Debye's low-temperature specific
heat behavior and converge to the classical Dulong-Petit limit at elevated
temperatures, in accordance with fundamental thermodynamic principles. For
hydrogen storage applications, both Al- and Si-based hydrides. meet key
benchmarks set by the U.S. Department of Energy (DOE), achieving gravimetric
hydrogen capacities (C$_{wt}$) exceeding 5.5 percent when A = Li or Na, and
exhibiting volumetric hydrogen densities greater than 40 g.H2/L. Among all
studied hydrides, Li$_2$AlH$_6$ and Li$_2$SiH$_6$ emerge as the two most
promising candidates due to their outstanding Cwt greater than 12.0 percent ,
elevated density greater than 140 g.H$_2$/L, and favorable hydrogen desorption
temperature ranges TD = 450 to 650 K.

</details>


### [396] [Unraveling a chemical-bond-driven root of topology in three-dimensional chiral crystals](https://arxiv.org/abs/2507.19900)
*Shungo Aoyagi,Shunsuke Kitou,Yuiga Nakamura,Motoaki Hirayama,Hideki Matsuoka,Ryotaro Arita,Shuichi Murakami,Taka-hisa Arima,Naoya Kanazawa*

Main category: cond-mat.mtrl-sci

TL;DR: 手性键合网络是产生多重费米子的关键，研究提出了一个基于化学键合和电子拓扑的通用框架，为设计量子手性材料提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 为了解决目前在合理设计量子手性特性方面缺乏将化学键合与电子拓扑联系起来的理论框架的问题。

Method: 结合同步辐射X射线衍射和第一性原理计算，研究了立方手性晶体CoSi和FeSi的化学键合和电子拓扑结构，并构建了三维Su-Schrieffer-Heeger模型。

Result: 发现了手性键合网络可以产生具有双重拓扑不变量的多重费米子，并且可以通过反转晶体手性或调节电子填充来切换拓扑特性。

Conclusion: 该研究揭示了手性键合网络是产生多重费米子的关键，并提出了一个基于化学键合和电子拓扑的通用框架，为设计具有量子手性特性的材料提供了新的途径。

Abstract: Chirality manifests across multiple scales, yielding unique phenomena that
break mirror symmetry. In chiral materials, unexpectedly large spin-filtering
or photogalvanic effects have been observed even in materials composed of light
elements, implying crucial influence of their topological electronic states.
However, an underlying framework that links chemical bonding and electronic
topology remains elusive, preventing the rational design of quantum chiral
properties. Here we identify the chiral bonding network responsible for
multifold topological fermions by combining synchrotron X-ray diffraction and
first-principles calculations on cubic chiral crystals, CoSi and FeSi. Based on
the observations of asymmetric valence electron distributions around the
transition metals, together with analyses of their bonding to
sevenfold-coordinated silicon atoms, we develop a three-dimensional
Su-Schrieffer-Heeger model, showing that inter-site hopping on this chiral
network creates multifold fermions with doubled topological invariants.
Topological features can be switched by reversing the crystalline chirality or
tuning electron filling. Our results highlight that implementing strong
spin-orbit coupling is not the sole route to realize robust topological phases
at elevated temperatures and offer a practical design principle for exploiting
chiral topology. Moreover, this real-space framework naturally extends to other
elementary excitations or artificial metamaterials, enabling various quantum
functionalities through an intuitive approach to chirality engineering.

</details>


### [397] [Specifics of ITO properties deposited on cerium-doped glass for space-grade solar cells](https://arxiv.org/abs/2507.20011)
*Danil D. Gren,Lev O. Luchnikov,Dmitri Yu. Dorofeev,Prokhor A. Alekseev,Ildar R. Sayarov,Alexey R. Tameev,Mikhail S. Dunaevskiy,Vladislav Kalinichenko,Vladimir Ivanov,Danila S. Saranin,Eugene I. Terukov*

Main category: cond-mat.mtrl-sci

TL;DR: ITO在Ce-玻璃上的性质与在传统玻璃上不同，这对其在太空太阳能电池中的应用很重要。


<details>
  <summary>Details</summary>
Motivation: 为新兴的光伏技术（如卤素钙钛矿）开发轻质载体和电极。

Method: 研究了ITO在Ce-玻璃和钠钙玻璃上的沉积，并比较了它们的区别。

Result: ITO在Ce-玻璃上表现出明显的压应变、降低的透明度和载流子浓度（10^19 cm-3），这是由于化学计量比的改变。霍尔迁移率（66 cm2V-1s-1）增加但电导率降低，这是由于过量的锡掺入。

Conclusion: Ce-玻璃的ITO涂层与传统玻璃相比具有显著不同的光学、结构和电学性质，这对其在太空太阳能电池中的应用至关重要。

Abstract: Ce-doped glass is a well-established solution for ultraviolet and ionizing
radiation shielding of solar cells in space. Traditionally, Ce-glass protected
Si or III-V based devices as an overlaying cap. However, for emerging
photovoltaics such as halide perovskites, thin Ce-glass coated with transparent
conductive layers could serve as a lightweight carrier with an electrode. While
indium-tin oxide (ITO) is widely used in solar cells for charge collection, its
optical, structural, and electrical properties depend on the substrate quality.
In this work, we demonstrated significant differences in properties of ITO
deposited on Ce-glass (100 micron thick) compared to standard soda lime glass.
ITO on Ce-glass exhibited pronounced compressive strain because of higher
oxygen vacancy concentrations, reduced transparency and charge carrier
concentration (10^19 cm-3) resulting from altered stoichiometry. Electrical
analysis showed increased Hall mobility (66 cm2V-1s-1) but decreased
conductivity due to excess tin incorporation. These specific ITO features
originated from inelastic collisions with the substrate during deposition.
Variations in wettability and surface potential underscore substrate-induced
differences critical for developing optimized ITO coatings for space-grade
photovoltaics.

</details>


### [398] [Atomistic Simulations of Short-range Ordering with Light Interstitials in Inconel Superalloys](https://arxiv.org/abs/2507.14382)
*Tyler D. Dolžal,Emre Tekoglu,Jong-Soo Bae,Gi-Dong Sim,Rodrigo Freitas,Ju Li*

Main category: cond-mat.mtrl-sci

TL;DR: 镍基高温合金中的硼和碳会与 Cr、Mo、Nb 形成有序结构，影响合金的微观结构和力学性能。


<details>
  <summary>Details</summary>
Motivation: 研究硼或碳掺杂对镍基高温合金短程有序行为的影响。

Method: 混合蒙特卡洛分子动力学模拟

Result: 硼和碳原子与 Cr、Mo、Nb 形成有利的有序结构，硼倾向于形成 B2 集群，碳倾向于形成 Cr23C6 和 Nb2C 等局部结构。硼和碳分别优先占据四面体和八面体间隙位置。在空位存在的情况下，硼和碳会迁移到空位处形成更稳定的结构。硼和碳在其短程有序结构中表现出强烈的热力学稳定性。在富钛条件下，碳更倾向于形成 TiC 而不是与 Cr 形成有序结构。这种转变会破坏对晶界起稳定作用的富铬碳化物网络的连续性，影响合金的力学性能。

Conclusion: 硼和碳在镍基高温合金中会与 Cr、Mo 和 Nb 形成有序结构，这种无序-有序转变会影响合金的微观结构和力学性能，可以作为调控合金显微组织以实现特定工程应用的原理。

Abstract: This study employed hybrid Monte Carlo Molecular Dynamics simulations to
investigate the short-range ordering behavior of Ni-based superalloys doped
with boron or carbon. The simulations revealed that both boron and carbon
dissociated from their host Ti atoms to achieve energetically favored ordering
with Cr, Mo, and Nb. Boron clusters formed as B2, surrounded by Mo, Nb, and Cr,
while carbon preferentially clustered with Cr to form a Cr23C6 local motif and
with Nb to form Nb2C. Distinct preferences for interstitial sites were
observed, with boron favoring tetrahedral sites and carbon occupying octahedral
sites. In the presence of a vacancy, B2 shifted from the tetrahedral site to
the vacancy, where it remained coordinated with Mo, Nb, and Cr. Similarly,
carbon utilized vacancies to form Nb2C clusters. Excess energy calculations
showed that B and C exhibited strong thermodynamic stability within their
short-range ordered configurations. However, under Ti-rich conditions, C was
more likely to segregate into TiC, despite preexisting ordering with Cr. This
shift in stability suggests that increased Ti availability would alter carbide
formation pathways, drawing C away from Cr-rich networks and promoting the
development of TiC. Such redistribution may disrupt the continuity of Cr-based
carbide networks, which play a critical role in stabilizing grain boundaries
and impeding crack propagation. These effects further underscore the impact of
interstitial-induced ordering on phase stability and microstructural evolution.
This work provides an atomistic perspective on how boron- and carbon-induced
ordering influences microstructure and mechanical properties. These findings
highlight the critical role of interstitial-induced short-range ordering and
demonstrate that this mechanism can be leveraged as a design principle to
fine-tune alloy microstructures for specific engineering applications.

</details>


### [399] [A first-principles study on the early-stage corrosion of a NiWNb alloy in a chloride salt environment](https://arxiv.org/abs/2507.20080)
*Tyler D. Doležal,Adib J. Samin*

Main category: cond-mat.mtrl-sci

TL;DR: Ni70W20Nb10合金在含氯环境下的早期溶解行为研究表明，铌和钨的加入能提高镍的耐腐蚀性，合金成分均表现出优异的抗局部表面退化能力。


<details>
  <summary>Details</summary>
Motivation: 为了量化镍基高温合金Ni70W20Nb10在含氯环境下的早期溶解行为，并为开发改进的熔盐反应堆结构材料提供见解。

Method: 使用多胞蒙特卡洛方法结合表面吸附和腐蚀行为模拟，预测了Ni70W20Nb10在800°C含氯环境下的早期溶解行为。

Result: 在800°C下，Ni70W20Nb10合金的预测固相为Ni72W19Nb9，具有体心四方晶体结构，与Ni4W结构非常相似。氯吸附优先发生在铌上，铌作为表面吸附的“陷阱”。镍、铌和钨能够抵抗氯引起的表面溶解，直到氯覆盖度达到1/3单层，表明该合金的所有成分都具有优异的抗局部表面退化能力。

Conclusion: 研究结果表明，铌和钨的存在增强了镍的耐腐蚀性，因为它们形成的区域在热力学上更受进入的氯的青睐，并且不易受到氯促溶的影响。

Abstract: In this work a representative nickel superalloy, Ni70W20Nb10, was
investigated in the presence of chlorine to quantify its early-stage
dissolution behavior. Surface structures were generated from a bulk
configuration sampled in equilibrium using a multi-cell Monte Carlo method for
phase prediction. The predicted solid-phase at 800 C was Ni72W19Nb9 in a
body-centered tetragonal crystal structure closely resembling the Ni4W
structure. Chlorine adsorption onto the energetically favored (110) surface
showed preference to niobium which acted as a trapping sink on the top surface
of the slab model. Our findings suggested that niobium and tungsten enhanced
the corrosion resistance of nickel as their presence created regions that were
thermodynamically preferred by the incoming chlorine and less susceptible to
chlorine-facilitated dissolution from the alloy. Nickel, niobium, and tungsten
resisted chlorine-induced dissolution from the surface model up to a 1/3
monolayer coverage of chlorine indicating that all constituents of this alloy
possessed superior resistance to localized surface degradation such as
corrosive pitting. Further analysis and comparisons between the corrosion
resistance of the three metallic species was performed. This work may provide
insights that aid in the development of improved structural materials for
molten salt reactors.

</details>


### [400] [On the adsorption of oxygen to high entropy alloy surfaces up to 2ML coverage using Density Functional Theory and Monte Carlo calculations](https://arxiv.org/abs/2507.20084)
*Tyler D. Doležal,Adib J. Samin*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究利用密度泛函理论和热力学模型，分析了Al10Nb15Ta5Ti30Zr40高熵合金在不同温度和压力下的氧化行为。结果表明，Ti和Zr含量越高，合金表面对氧的反应性越强；Nb与Al、Ta的结合则能提升抗氧化能力。氧气主要吸附在富Ti和Zr的位点，向内扩散受合金成分影响显著，在1 ML时扩散速率急剧下降。该研究为理解和应对高温氧化提供了见解。


<details>
  <summary>Details</summary>
Motivation: 为了增进对高熵合金氧化过程的理解。

Method: 采用密度泛函理论和热力学模型研究了Al10Nb15Ta5Ti30Zr40高熵合金表面的早期氧化阶段。通过多单元蒙特卡洛方法从平衡态采样表面模型，并预测了体相结构为具有良好实验符合性的单BCC相。

Result: 氧气优先吸附在富含Ti和Zr的位点，并避开Nb-Al和Nb-Ta位点。表面对氧气高度敏感，在100至2600 K的温度范围和10^-30至10^5 bar的压力范围内，氧覆盖度主要为两层。低覆盖率下，氧气倾向于在富Zr区域向内扩散，但Ti和Al的加入会减缓此过程。在1 ML时，扩散速率急剧下降，特别是在富Ti和Zr区域，其中存在强的金属-氧键。

Conclusion: 高熵合金（HEA）表面对氧气具有高反应性，Ti和Zr含量高会增强这种反应性，而Nb与Al和Ta的组合可提高抗氧吸附能力。在低覆盖率下，氧气倾向于在富Zr区域扩散，但Ti和Al的添加会减缓扩散。在1 ML时，扩散速率显著降低，尤其是在富Ti和Zr的区域，会形成强的金属-氧键。

Abstract: To enhance our understanding of oxidation in high-entropy alloys, the early
stages of oxidation on the surface of Al10Nb15Ta5Ti30Zr40 were studied using
density functional theory and thermodynamic modeling. Surface slabs were
generated from bulk configurations sampled from equilibrium using a multicell
Monte Carlo method for phase prediction. The bulk structure was found to be a
single BCC phase in good agreement with experimental observations. The oxygen
adsorbed with a strong preference for sites with Ti and Zr and avoided sites
with Nb-Al and Nb-Ta. The surface was shown to be highly reactive to oxygen,
yielding a dominating oxygen coverage of two monolayers over the temperature
range of 100 to 2600 K and an oxygen pressure range of 10-30 to 105 bar. Inward
oxygen diffusion at low coverage was preferred in regions rich with Zr but
slowed with the addition of Ti and Al. Diffusion rates drastically decreased at
1 ML, especially in the region rich with Ti and Zr, where strong metal-oxygen
bonds were reported. Our results indicated that a high content of Ti and Zr
increased the reactivity of the HEA surface to oxygen. The presence of Nb also
enhanced the resistance to oxygen adsorption, especially when partnered with Al
and Ta. Inward oxygen diffusion was likely to occur at low coverage in regions
rich with Zr but could be protected against with the addition of Al and Ti. The
limitations of the present work are discussed. This study may provide insights
that assist with devising short- and long-term mitigation strategies against
material degradation related to high-temperature oxidation.

</details>


### [401] [Mo-Re-W Alloys for High Temperature Applications: Phase Stability, Elasticity, and Thermal Property Insights via Multi-Cell Monte Carlo and Machine Learning](https://arxiv.org/abs/2507.20085)
*Tyler D. Doležal,Nick A. Valverde,Jodie Yuwono,Ryan Kemnitz*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究通过计算和机器学习方法预测了Mo-Re-W合金在高温下的性能，发现一种特定的(Mo,W) + Re组合具有潜力，并通过实验进行了验证。


<details>
  <summary>Details</summary>
Motivation: 为了满足对能在高温和恶劣环境中承受能力的材料日益增长的需求，本研究致力于发现先进的合金。

Method: 本研究采用计算例程来预测固态相稳定性和计算弹性常数，并训练了机器学习模型，对Mo-Re-W合金在整个成分域内的弹性、热学和机械性能进行了预测和可视化。

Result: 研究结果确定了一种平衡的(Mo,W) + Re混合物作为高温应用的有前景的成分，这归因于其强而稳定的(Mo,W)基体具有高Re含量，以及形成增强机械性能的(W,Re)沉淀物，在1600oC下表现出优越的性能。

Conclusion: 本研究通过计算方法预测了Mo-Re-W合金在高温下的相稳定性和弹性常数，并利用机器学习模型进行了大范围的属性预测和可视化。研究结果表明，平衡的(Mo,W) + Re组合是高温应用的有前景的成分，其特点是具有高Re含量的强而稳定的(Mo,W)基体以及形成增强机械性能的(W,Re)沉淀物。该方法为设计和优化高温合金提供了一条高效且与系统无关的途径。

Abstract: The increasing demand for materials capable of withstanding high temperatures
and harsh environments necessitates the discovery of advanced alloys. This
study introduces a computational routine to predict solid-state phase stability
and calculates elastic constants to determine high temperature viability. With
it, machine learning models were trained on 1,014 Mo-Re-W structures to enable
a large compilation of elastic and thermal properties over the complete Mo-Re-W
compositional domain with extreme resolution. A series of heat maps spanning
the full compositional domain were generated to visually present the impact of
alloy constituents on the alloy properties. Our findings identified a balanced
(Mo,W) + Re blend as a promising composition for high temperature applications,
attributed to a strong and stable (Mo,W) matrix with high Re content and the
formation of strengthening (W,Re) precipitates that enhanced mechanical
performance at 1600oC. Several Mo-Re-W compositions were manufactured to
experimentally validate the computational predictions. This approach provides
an efficient and system-agnostic pathway for designing and optimizing alloys
for high-temperature applications.

</details>


### [402] [Energy-Resolved EBSD using a Monolithic Direct Electron Detector](https://arxiv.org/abs/2507.20105)
*Nicolò M. Della Ventura,Kalani Moore,McLean P. Echlin,Matthew R. Begley,Tresa M. Pollock,Marc De Graef,Daniel S. Gianola*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究介绍了一种新的能量分辨EBSD方法，能够对单个背散射电子进行能量定量，从而提高EBSD图的清晰度和细节，并深化对衍衬机制的理解。


<details>
  <summary>Details</summary>
Motivation: 精确量化对电子背散射衍射（EBSD）图贡献的背散射电子（BSEs）的能量分布仍然是一个活跃的挑战。

Method: 本研究引入了一种基于单片有源像素传感器直接电子探测器和计数电子算法的能量分辨EBSD方法，实现了对单个背散射电子的能量定量，并直接测量了衍衬中的电子能量谱。

Result: 通过在单电子级别实现能量测定，该方法引入了一个多功能的工具集，以扩展EBSD的定量能力，从而有可能深化对衍衬机制的理解并提高晶体测量精度。

Conclusion: 该方法通过在单电子级别实现能量测定，引入了一个多功能的工具集，以扩展EBSD的定量能力，从而有可能深化对衍衬机制的理解并提高晶体测量精度。

Abstract: Accurate quantification of the energy distribution of backscattered electrons
(BSEs) contributing to electron backscatter diffraction (EBSD) patterns remains
as an active challenge. This study introduces an energy-resolved EBSD
methodology based on a monolithic active pixel sensor direct electron detector
and an electron-counting algorithm to enable the energy quantification of
individual BSEs, providing direct measurements of electron energy spectra
within diffraction patterns. Following detector calibration of the detector
signal as a function of primary beam energy, measurements using a 12 keV
primary beam on Si(100) reveal a broad BSE energy distribution across the
diffraction pattern, extending down to 3 keV. Furthermore, an angular
dependence in the weighted average BSE energy is observed, closely matching
predictions from Monte Carlo simulations. Pixel-resolved energy maps reveal
subtle modulations at Kikuchi band edges, offering insights into the
backscattering process. By applying energy filtering within spectral windows as
narrow as 2 keV centered on the primary beam energy, significant enhancement in
pattern clarity and high-frequency detail is observed. Notably, electrons in
the 2--8 keV range, despite having undergone substantial energy loss ({\Delta}E
= 4--10 keV), still produce Kikuchi patterns. By enabling energy determination
at the single-electron level, this approach introduces a versatile tool-set for
expanding the quantitative capabilities of EBSD, thereby offering the potential
to deepen the understanding of diffraction contrast mechanisms and to advance
the precision of crystallographic measurements.

</details>


### [403] [Cluster dynamics modeling of hydrogen saturation retention in tungsten with a universal trapping-site sink strength](https://arxiv.org/abs/2507.20121)
*Yuanyuan Zhang,Xiaoru Chen Chuanguo Zhang,Yonggang Li*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究提出了一个改进的模型，能够准确预测钨材料中氢同位素（如氘）的保留情况，解决了现有模型无法解释的饱和现象，并确定了影响保留的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有的微结构（如位错和晶界）的理论汇强度模型无法解释观察到的氢同位素保留饱和现象。

Method: 提出了一个新的通用捕获位点模型，该模型将时间相关的位点浓度动态地表示为时间相关的汇强度。并将其整合到改进的集群动力学模型中，用于高通量氢同位素辐照。

Result: 模拟结果在数量上重现了钨中饱和的低能氘（D）保留及其深度分布，与实验结果非常吻合。识别出了约 10^23 m^-2 的临界饱和注量。在此注量以下，未饱和的 D 保留同时受晶界和离子诱导缺陷控制；在此阈值以上，晶界通过捕获自由 D 并接近其理论饱和极限来主导 D 保留。

Conclusion: 该研究提出了一个新的通用捕获位点模型，该模型将时间相关的位点浓度动态地表示为时间相关的汇强度。通过将此模型整合到改进的集群动力学模型中，可以模拟高通量氢同位素辐照。研究结果在数量上重现了钨中饱和的低能氘（D）保留及其深度分布，这与实验结果非常吻合。

Abstract: Hydrogen isotope (HI) retention poses a key issue for tungsten (W)-based
plasma-facing materials (PFMs) in fusion devices, where microstructures such as
dislocations (DLs) and grain boundaries (GBs) play a dominant role. Existing
theoretical sink strength models for microstructures like DLs and GBs fail to
account for the observed saturation of HI retention. In this study, we propose
a novel universal trapping-site model that dynamically represents sink
strengths as time-dependent site concentrations, which is incorporated into an
improved cluster dynamics model for high-fluence HI irradiation. Our
simulations quantitatively reproduce the saturated low-energy deuterium (D)
retention and depth profiles in W, in good agreement with experiments. A
critical saturation fluence of approximately 1023 m-2 is identified, below
which unsaturated D retention is governed by both GBs and ion-induced defects,
whereas above this threshold GBs dominate D retention by trapping free D and
approaching their theoretical saturation limit. The trapping-site sink strength
model enables quantification of H trapping by diverse microstructures via
unified effective site concentrations, providing mechanistic insights into
microstructural effects and facilitating direct evaluation of HI retention in
PFMs under different irradiation conditions.

</details>


### [404] [Inelastic Neutron Scattering for Direct Detection of Chiral Phonons](https://arxiv.org/abs/2507.20168)
*Tingting Wang,Jun Zhou,Qingyong Ren,Lifa Zhang*

Main category: cond-mat.mtrl-sci

TL;DR: 非弹性中子散射（INS）可以直接检测手征声子，并提供有关其手性、磁矩和有效磁场的信息。


<details>
  <summary>Details</summary>
Motivation: 手征声子在自旋电子学、超导和先进材料等领域具有潜在应用，但其检测主要依赖于间接的光子过程。因此，需要一种直接且通用的方法来检测手征声子。

Method: 提出并验证了使用非弹性中子散射（INS）直接检测手征声子的方法。通过对右旋碲（Te）进行角度分辨测量，区分了线性和手征声子，并识别了特征性的INS信号。此外，在CeF3体系中，通过观察到的模式分裂，证明了INS能够直接测量声子磁矩和手征声子产生的有效磁场。

Result: 1. 证明了INS可以区分线性和手征声子，并通过角度分辨测量确定声子手性。
2. 在碲（Te）的实验中，识别出手征声子与线性声子的特征INS信号。
3. 在CeF3的实验中，通过模式分裂证明INS可以直接访问声子磁矩和手征声子产生的有效磁场。

Conclusion: 该研究将非弹性中子散射（INS）确立为一种直接且多功能的手征声子检测方法，并展示了其在区分线性和手征声子、确定声子手性以及访问声子磁矩和有效磁场方面的能力，为理解手征声子动力学及其相关量子现象提供了新的平台。

Abstract: Chiral phonons have attracted significant attention due to their potential
applications in spintronics, superconductivity, and advanced materials, but
their detection has predominantly relied on indirect photon-involved processes.
Here, we propose inelastic neutron scattering (INS) as a direct and versatile
approach for chiral phonon detection over a broad momentum-energy space.
Leveraging INS sensitivity to phonon eigenmodes, we clearly distinguish linear,
elliptical, and chiral phonons and determine phonon handedness through
angle-resolved measurements. Using right-handed tellurium (Te) as a model
system, we identify characteristic INS fingerprints that clearly separate
chiral from linear phonons. Moreover, we show that INS can directly access
phonon magnetic moments and the effective magnetic fields generated by chiral
phonons, as evidenced by the pronounced mode splitting in CeF$_3$.
Collectively, these results position INS as a powerful platform for
comprehensive investigations of chiral-phonon dynamics and their associated
quantum phenomena.

</details>


### [405] [Knot-Driven Spin Selectivity: Topological Chirality-Induced Robust Spin Polarization in Molecular Knots](https://arxiv.org/abs/2507.20172)
*Xi Sun,Kai-Yuan Zhang,Shu-Zheng Zhou,Hua-Hua Fu*

Main category: cond-mat.mtrl-sci

TL;DR: 研究首次建立了三叶结分子中拓扑手性诱导自旋选择性的理论框架，揭示了结驱动自旋选择性现象，为设计自旋电子材料提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 尽管拓扑手性在三叶结分子中在CISS方面表现出超高自旋极化、导电性提升和高温稳定性等优势，但其潜在的物理机制仍不明确。

Method: 本文首次为三叶结分子中的拓扑手性诱导自旋选择性（TCISS）建立了基础性理论框架，并确定了驱动自旋选择性所需的必要条件。

Result: 计算结果表明，三叶结分子可以表现出超过60%的自旋极化以及显著的导电性。重要的是，当拓扑结退化为平凡结构并伴随从拓扑手性到结构手性的转变时，自旋极化急剧下降，证明了超高自旋极化与结拓扑之间存在强关联。此外，减少晶格数或施加应变调节对超高自旋极化的影响很小，凸显了其鲁棒性。

Conclusion: 该理论成功阐明了拓扑手性诱导自旋选择性（TCISS）的物理机制，并揭示了一种新的自旋极化输运现象——结驱动自旋选择性，为设计用于自旋电子学器件应用的非磁性材料提供了新的指导原则。

Abstract: Compared to traditional structural chiral materials (e.g., DNA, helicene),
topological chirality in trefoil knot molecules has demonstrated multiple
remarkable advantages in chirality-induced spin selectivity (CISS), including
ultra-high spin polarization of nearly 90%, conductivity increased by two
orders of magnitude, and high-temperature stability (up to 350$^{\circ}$C).
However, the underlying physical mechanism remains elusive. This work
establishes, for the first time, a fundamental theoretical framework for
topological chirality-induced spin selectivity (TCISS) in trefoil knot
molecules and identifies the necessary conditions for knot-driven spin
selectivity. Our calculation results reveal that a trefoil knot molecule can
exhibit spin polarization exceeding 60% along with significant conductivity.
Notably, neither reducing the lattice number nor applying strain regulation
significantly diminishes this ultra-high spin polarization, highlighting its
robustness. Importantly, when the topological knot degenerates into a trivial
structure, accompanied by the transition from topological chirality to
structural chirality, the spin polarization sharply declines, demonstrating a
strong correlation between the ultrahigh spin polarization and the knot
topology. Our theory not only successfully elucidates the physical mechanism of
TCISS, but also uncovers a new spin-polarized transport phenomenon termed
knot-driven spin selectivity, offering new guiding principles for designing
nonmagnetic materials for spintronics device applications.

</details>


### [406] [Quantum Imaging of Ferromagnetic van der Waals Magnetic Domain Structures at Ambient Conditions](https://arxiv.org/abs/2507.20245)
*Bindu,Amandeep Singh,Amir Hen,Lukas Drago Cavar,Sebastian Maria Ulrich Schultheis,Shira Yochelis,Yossi Paltiel,Andrew F. May,Angela Wittmann,Mathias Klaui,Dmitry Budker,Hadar Steinberg,Nir Bar-Gill*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究使用量子磁显微镜分析了$m Fe_{5}GeTe_{2}$二维材料，发现其磁相变不随厚度（低至15nm）显著变化，且磁各向异性影响不大，这对于设计新型自旋电子器件具有指导意义。


<details>
  <summary>Details</summary>
Motivation: 为了解决关于2D范德华磁性材料（特别是$m Fe_{5}GeTe_{2}$）的畴结构、磁相变温度与样品厚度和外磁场的关系等关键问题，以及其在磁性存储和逻辑器件集成中的应用潜力。

Method: 利用氮空位（NV）中心量子磁显微镜，实现了对$m Fe_{5}GeTe_{2}$磁化强度的直接成像，空间分辨率达到亚微米级，并研究了其随温度、磁场和厚度的变化。采用了空间分辨测量方法，包括磁化强度方差和交叉相关性分析。

Result: 研究发现，$m Fe_{5}GeTe_{2}$的相变温度存在显著差异，但随厚度（低至15 nm）的变化不明显。此外，还识别出先前未知的条状特征，并归因于晶体合成和后续氧化过程中的元素调制。研究结果表明，磁各向异性在材料的磁性中作用不大。

Conclusion: 本研究表明，$m Fe_{5}GeTe_{2}$的磁各向异性在其磁特性中作用不大，且磁性相变在15 nm厚度下基本不随厚度变化。这些发现可能对未来基于二维范德华磁性材料的自旋电子器件、磁性存储器和逻辑器件的设计具有重要意义。

Abstract: Recently discovered 2D van der Waals magnetic materials, and specifically
Iron-Germanium-Telluride ($\rm Fe_{5}GeTe_{2}$), have attracted significant
attention both from a fundamental perspective and for potential applications.
Key open questions concern their domain structure and magnetic phase transition
temperature as a function of sample thickness and external field, as well as
implications for integration into devices such as magnetic memories and logic.
Here we address key questions using a nitrogen-vacancy center based quantum
magnetic microscope, enabling direct imaging of the magnetization of $\rm
Fe_{5}GeTe_{2}$ at sub-micron spatial resolution as a function of temperature,
magnetic field, and thickness. We employ spatially resolved measures, including
magnetization variance and cross-correlation, and find a significant spread in
transition temperature yet with no clear dependence on thickness down to 15 nm.
We also identify previously unknown stripe features in the optical as well as
magnetic images, which we attribute to modulations of the constituting elements
during crystal synthesis and subsequent oxidation. Our results suggest that the
magnetic anisotropy in this material does not play a crucial role in their
magnetic properties, leading to a magnetic phase transition of $\rm
Fe_{5}GeTe_{2}$ which is largely thickness-independent down to 15 nm. Our
findings could be significant in designing future spintronic devices, magnetic
memories and logic with 2D van der Waals magnetic materials.

</details>


### [407] [Type-II Antiferroelectricity](https://arxiv.org/abs/2507.20285)
*Yang Wang,Chaoxi Cui,Yilin Han,Tingli He,Weikang Wu,Run-Wu Zhang,Zhi-Ming Yu,Shengyuan A. Yang,Yugui Yao*

Main category: cond-mat.mtrl-sci

TL;DR: 发现了在动量空间中极化相反的新型反铁电性（型II AFE），它与反铁磁性共存，并可能在自旋流产生和边界自旋极化方面有独特表现。


<details>
  <summary>Details</summary>
Motivation: 填补了对反铁电性的认知空白，探索了具有动量空间相反极化的新型反铁电性。

Method: 通过 the Berry-phase theory 严格定义了型II AFE的序参量，并可从电子能带结构中量化提取。研究了保留自旋旋转对称性的型II AFE子类，确定了相关的对称性约束和兼容的自旋点群，并构建了相应的磁异常模型。

Result: 发现了型II AFE，其特点是动量空间极化相反，与反铁磁性共存，并提出了一种新的磁电耦合形式。确定了 FeS、Cr2O3、MgMnO3、单层 MoICl2 和双层 CrI3 等材料。揭示了型II自旋-AFE系统独特的物理现象，如AFE开关时的自旋流产生以及边界和畴壁处的局域自旋极化。

Conclusion: 型II反铁电性（AFE）是一种新的AFE类别，其动量空间中的极化相反，并且与反铁磁性内在共存，提供了新的磁电耦合形式。该发现为量子材料研究和技术应用开辟了新途径。

Abstract: Antiferroelectricity (AFE) is a fundamental concept in physics and materials
science. Conventional AFEs have the picture of alternating local electric
dipoles defined in real space. Here, we discover a new class of AFEs, termed
type-II AFEs, which possess opposite polarizations defined in momentum space
across a pair of symmetry decoupled subspaces. Unlike conventional AFEs, the
order parameter of type-II AFEs is rigorously formulated through Berry-phase
theory and can be quantitatively extracted from the electronic band structure.
Focusing on a subclass of type-II AFEs that preserve spin-rotation symmetry, we
establish the relevant symmetry constraints and identify all compatible spin
point groups. Remarkably, we find that type-II AFE order intrinsically coexists
with antiferromagnetism, revealing a robust form of magnetoelectric coupling.
We construct an altermagnetic model and identify several concrete
antiferromagnetic/altermagnetic materials, such as FeS, Cr2O3, MgMnO3,
monolayer MoICl2 and bilayer CrI3, that exhibit this novel ordering.
Furthermore, we uncover unique physical phenomena associated with type-II
spin-AFE systems, including spin current generation upon AFE switching and
localized spin polarization at boundaries and domain walls. Our findings reveal
a previously hidden class of quantum materials with intertwined ferroic orders,
offering exciting opportunities for both fundamental exploration and
technological applications.

</details>


### [408] [Adjudicating Conduction Mechanisms in High Performance Carbon Nanotube Fibers](https://arxiv.org/abs/2507.20481)
*John Bulmer,Chris Kovacs,Thomas Bullard,Charlie Ebbing,Timothy Haugan,Ganesh Pokharel,Stephen D. Wilson,Fedor F. Balakirev,Oscar A. Valenzuela,Michael A. Susner,David Turner,Pengyu Fu,Teresa Kulka,Jacek Majewski,Irina Lebedeva,Karolina Z. Milowska,Agnieszka Lekawa-Raus,Magdalena Marganska*

Main category: cond-mat.mtrl-sci

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The performance of carbon nanotube (CNT) cables, a contender for copper-wire
replacement, is tied to its metallic and semi-conducting-like conductivity
responses with temperature; the origin of the semi-conducting-like response
however is an underappreciated incongruity in literature. With controlled
aspect-ratio and doping-degree, over 61 unique cryogenic experiments including
anisotropy and Hall measurements, CNT cable performance is explored at extreme
temperatures (65 mK) and magnetic fields (60 T). A semi-conducting-like
conductivity response with temperature becomes temperature-independent
approaching absolute-zero, uniquely demonstrating the necessity of
heterogeneous fluctuation induced tunneling; complete de-doping leads to
localized hopping, contrasting graphite's pure metallic-like response.
High-field magneto-resistance (including +22% longitudinal magneto-resistance
near room-temperature) is analyzed with hopping and classical two-band models,
both similarly yielding a parameter useful for conductor development. Varying
field-orientation angle uncovers two-and four-fold symmetries from
Aharonov-Bohm-like corrections to curvature-induced bandgap. Tight-binding
calculations using Green's Function formalism model large-scale, coherent
transport in commensurate CNT bundles in magnetic field, revealing non-uniform
transmission across bundle cross-sections with doping restoring uniformity;
independent of doping, transport in bundle-junction-bundle systems are
predominantly from CNTs adjacent to the other bundle. The final impact is
predicting the ultimate conductivity of heterogeneous CNT cables using
temperature and field-dependent transport, surpassing conductivity of
traditional metals.

</details>


### [409] [Revealing Atomic-Scale Switching Pathways in van der Waals Ferroelectrics](https://arxiv.org/abs/2507.20543)
*Xinyan Li,Kenna Ashen,Chuqiao Shi,Nannan Mao,Saagar Kolachina,Kaiwen Yang,Tianyi Zhang,Sajid Husain,Ramamoorthy Ramesh,Jing Kong,Xiaofeng Qian,Yimo Han*

Main category: cond-mat.mtrl-sci

TL;DR: 通过原位电偏压成像和第一性原理计算，研究了SnSe的铁电切换机制，发现了90度和180度切换路径共存，并揭示了切换过程中伴随层间滑动和压缩应变，为2D铁电器件设计提供基础。


<details>
  <summary>Details</summary>
Motivation: 二维范德华（vdW）材料因其与硅的兼容性和在原子尺度下强大的极化而有潜力用于超尺度铁电器件。然而，vdW材料中固有的弱相互作用使得层间易于滑动，这给研究切换路径带来了挑战。

Method: 结合了原子分辨率成像（在原位电偏压条件下）和第一性原理计算，以揭示SnSe的原子尺度切换机制。

Result: 研究结果揭示了SnSe从反铁电（AFE）向铁电（FE）有序转变过程中，存在连续的90度切换路径和直接的180度切换路径。原子尺度研究和应变分析表明，切换过程同时诱导层间滑动和压缩应变，尽管存在多畴结构，晶格仍保持相干。

Conclusion: 本研究阐明了SnSe在原子尺度下的铁电切换动力学，并为2D铁电纳米器件的合理设计奠定了基础。

Abstract: Two-dimensional van der Waals (vdW) materials hold the potential for
ultra-scaled ferroelectric (FE) devices due to their silicon compatibility and
robust polarization down to atomic scale. However, the inherently weak vdW
interactions enable facile sliding between layers, introducing complexities
beyond those encountered in conventional ferroelectric materials and presenting
significant challenges in uncovering intricate switching pathways. Here, we
combine atomic-resolution imaging under in-situ electrical biasing conditions
with first-principles calculations to unravel the atomic-scale switching
mechanisms in SnSe, a vdW group-IV monochalcogenide. Our results uncover the
coexistence of a consecutive 90 degrees switching pathway and a direct 180
degrees switching pathway from antiferroelectric (AFE) to FE order in this vdW
system. Atomic-scale investigations and strain analysis reveal that the
switching processes simultaneously induce interlayer sliding and compressive
strain, while the lattice remains coherent despite the presence of multidomain
structures. These findings elucidate vdW ferroelectric switching dynamics at
atomic scale and lay the foundation for the rational design of 2D ferroelectric
nanodevices.

</details>


### [410] [High-fidelity modeling of interface crossing in the diffusion welding process at the polycrystalline scale](https://arxiv.org/abs/2507.20635)
*Camille Godinot,Emmanuel Rigal,Frédéric Bernard,Philippe Emonot,Pierre-Eric Frayssines,Luc Védie,Marc Bernacki*

Main category: cond-mat.mtrl-sci

TL;DR: This paper simulates and measures grain boundary crossing at diffusion welded interfaces using a Level-Set method and new measurement models to improve joint properties.


<details>
  <summary>Details</summary>
Motivation: Controlling the microstructure of a diffusion welded interface is critical for optimum mechanical properties and joint homogeneity. This article specifically addresses grain boundary crossing at the interface.

Method: A Level-Set method was used for 2D microstructure simulations, and two crossing measurement models were formulated and tested.

Result: The simulations and measurement models were discussed over various interface parameters, providing insights into the grain boundary crossing phenomenon.

Conclusion: The study focuses on controlling the microstructure of a diffusion welded interface, specifically the grain boundary crossing, and introduces methods to measure it.

Abstract: Controlling the microstructure of a diffusion welded interface is a critical
point to ensure optimum mechanical properties and the homogeneity of the joint.
Beyond the intimate contact formation between bonded parts studied in the
literature, this article focuses on the grain boundary crossing of the
interface during this process and its measurement. Following this perspective,
a Level-Set method has been used for full-field microstructure simulations in
2D with various interface parameters. Two crossing measurement models have been
formulated, tested and discussed over the simulations.

</details>


### [411] [Towards trustworthy AI in materials mechanics through domain-guided attention](https://arxiv.org/abs/2507.20658)
*Jesco Talies,Eric Breitbarth,David Melching*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究提出注意力引导训练框架，结合XAI、定量评估和领域知识，通过使模型注意力与物理应力场对齐，提高深度学习在材料断裂力学分析中的泛化能力和解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的可信度和鲁棒性，尤其是在高风险的科学应用中，仍然是一个根本性的挑战。

Method: 本研究提出了一种名为注意力引导训练的框架，结合了可解释人工智能技术、定量评估和领域特定先验知识来引导模型注意力。

Result: 注意力引导训练可提高模型的泛化能力，并通过使模型注意力与物理上相关的应力场对齐，从而实现更可靠的解释。

Conclusion: 通过将模型注意力与物理上相关的区域（如 Williams 解析解所描述的应力场）对齐，注意力引导训练可确保模型关注物理上相关的区域，最终提高泛化能力并提供更可靠的解释。

Abstract: Ensuring the trustworthiness and robustness of deep learning models remains a
fundamental challenge, particularly in high-stakes scientific applications. In
this study, we present a framework called attention-guided training that
combines explainable artificial intelligence techniques with quantitative
evaluation and domain-specific priors to guide model attention. We demonstrate
that domain specific feedback on model explanations during training can enhance
the model's generalization capabilities. We validate our approach on the task
of semantic crack tip segmentation in digital image correlation data which is a
key application in the fracture mechanical characterization of materials. By
aligning model attention with physically meaningful stress fields, such as
those described by Williams' analytical solution, attention-guided training
ensures that the model focuses on physically relevant regions. This finally
leads to improved generalization and more faithful explanations.

</details>


### [412] [Sliding Engineering Spin-Valley-Layer Coupling and Altermagnetism in Bilayer Antiferromagnetic Honeycomb Lattices](https://arxiv.org/abs/2507.20690)
*Wen-Xin Jiang,Zhen-Hao Gong,Yuantao Chen,Zhigang Gui,Li Huang*

Main category: cond-mat.mtrl-sci

TL;DR: 通过在双层反铁磁蜂窝状晶格中实现铁电-谷和铁电-反常磁耦合，利用层间滑动可逆地切换极化，从而同时反转谷极化和反常磁自旋分裂，实现了可调谐的反常谷霍尔效应和磁电响应，为2D反铁磁材料的电可编程谷电子和自旋电子功能提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 为了利用谷极化和反常磁性这两种新兴的凝聚态物理现象，为新型节能设备中的信息编码和处理提供机会，并通过耦合谷和自旋自由度与铁电序（如铁电性）来实现非易失性存储功能。

Method: 基于有效的四带自旋全k·p模型，并利用第一性原理计算在双层MnPTe3中进行了验证。

Result: 实现了铁电-谷和铁电-反常磁耦合，实现了可调谐的层-自旋锁定的反常谷霍尔效应和2D反铁磁材料前所未有的磁电响应。

Conclusion: 该研究提出了在双层反铁磁蜂窝状晶格中实现铁电-谷（FE-valley）和铁电-反常磁（FE-altermagnetic）耦合的方法，并通过第一性原理计算在双层MnPTe3中验证了该方法。研究结果表明，AB-（BA-）堆叠构型中会产生自发的面外电极化，且可以通过层间滑动可逆地切换。极化反转可同时反转谷极化和反常磁自旋分裂，实现了可调谐的层-自旋锁定的反常谷霍尔效应和2D反铁磁材料前所未有的磁电响应。该工作为2D反铁磁材料的电可编程谷电子和自旋电子功能建立了一个通用范式。

Abstract: Valley polarization and altermagnetism are two emerging fundamental phenomena
in condensed matter physics, offering unprecedented opportunites for
information encoding and processing in novel energy-efficient devices. By
coupling valley and spin degrees of freedom with ferroic orders such as
ferroelectricity, nonvolatile memory functionalities can be achieved. Here, we
propose a way to realize ferroelectric-valley (FE-valley) and FE-altermagnetic
coupling in a bilayer antiferromagnetic (AFM) honeycomb lattices based on an
effective four-band spin-full $k\cdot p$ model. Our proposal is validated in
bilayer MnPTe$_3$ through first-principles calculations. A spontaneous
out-of-plane electric polarization occurs in AB- (BA-) stacking configuration,
which is reversibly switchable via interlayer sliding. Remarkably, polarization
reversal simultaneously inverts both layer-resolved valley polarization and
altermagnetic spin splitting. This dual control enables tunable
layer-spin-locked anomalous valley Hall effects and an unprecedented
magnetoelectric response in 2D antiferromagnets. Our work establishes a general
paradigm for electrically programmable valleytronic and spintronic
functionalities of 2D AFM materials.

</details>


### [413] [Dislocation Dynamics and Shape in High Entropy Alloys: the Influence of Stress Correlations, Long-Range Interactions and Anisotropy](https://arxiv.org/abs/2507.20697)
*Dénes Berta,Péter Dusán Ispánovity*

Main category: cond-mat.mtrl-sci

TL;DR: High entropy alloys are strong and ductile due to complex internal stress fields hindering dislocation movement. This study analyzes how stress correlations, anisotropy, and dislocation interactions affect dislocation behavior and shape, with implications for materials science and experiments.


<details>
  <summary>Details</summary>
Motivation: High entropy alloys have garnered significant scientific interest due to their enhanced mechanical properties, specifically high yield strength combined with outstanding ductility, which originate from their highly heterogeneous pinning stress fields that hinder dislocation glide.

Method: This work investigates the influence of correlations and anisotropy of pinning stresses, and the long-range nature and anisotropy of dislocation interactions on the propagation of dislocations and the depinning transition in high entropy alloys. It also studies how the impact of these factors manifest in the shape of dislocations.

Result: The research examines how the correlations and anisotropy of pinning stresses, along with the long-range nature and anisotropy of dislocation interactions, affect dislocation propagation and depinning transitions, as well as the resulting shape of dislocations. The implications for generic disordered systems and experimental applications are also considered.

Conclusion: The study discusses the impact of pinning stress correlations and anisotropy, as well as dislocation interaction characteristics, on dislocation propagation and depinning transitions in high entropy alloys. It also explores how these factors influence dislocation shape and discusses broader implications for disordered systems and experimental applications.

Abstract: High entropy alloys gained significant scientific interest in recent years
due to their enhanced mechanical properties including high yield strength
combined with outstanding ductility. The strength of these materials originates
from their highly heterogeneous pinning stress fields that hinder dislocation
glide, that is, plastic deformation. This work investigates how the
correlations and the anisotropy of the pinning stresses, and the long-range
nature and the anisotropy of dislocation interactions influence the propagation
of dislocations and the depinning transition in these alloys. Furthermore, it
is studied how the impact of these factors manifest in the shape of
dislocations. The implications to the wider scope of generic disordered systems
and to possible experimental applications are also discussed.

</details>


### [414] [Light-induced Odd-parity Magnetism in Conventional Collinear Antiferromagnets](https://arxiv.org/abs/2507.20705)
*Shengpu Huang,Zheng Qin,Fangyang Zhan,Dong-Hui Xu,Da-Shuai,Rui Wang*

Main category: cond-mat.mtrl-sci

TL;DR: 通过Floquet工程，在二维共线反铁磁体中实现了可控的奇宇称磁性，克服了其通常仅出现在非共线磁结构中的限制。


<details>
  <summary>Details</summary>
Motivation: 近期研究日益关注非相对论奇宇称磁性，但通常认为奇宇称自旋劈裂仅出现在非共线磁结构中。本研究旨在探索在二维共线反铁磁体中实现奇宇称磁性的新方法。

Method: 本研究结合了对称性论证、有效模型分析、Floquet工程以及第一性原理计算，对二维共线反铁磁材料在周期性驱动光场（如圆偏振光、椭圆偏振光和双圆偏振光）下的奇宇称磁性进行了理论和计算验证。

Result: 研究表明，Floquet工程能够有效地在二维共线反铁磁体中实现奇宇称磁性，并且可以通过调节晶体对称性或光的偏振状态来控制自旋劈裂的方向和大小，实现了自旋劈裂的可控反转或转换。通过第一性原理计算和Floquet定理的结合，验证了所提出的策略在实际材料中的可行性。

Conclusion: 该研究提出了一种利用Floquet工程在二维（2D）共线反铁磁体中实现奇宇称磁性的通用策略，通过调整晶体对称性或入射光偏振态，可以灵活控制光诱导的奇宇称自旋劈裂，甚至实现其反转或转换。研究结果为设计具有高可调性的奇宇称自旋劈裂以及开发非传统补偿磁性提供了新的途径。

Abstract: Recent studies have drawn growing attention on non-relativistic odd-parity
magnetism in the wake of altermagnets. Nevertheless, odd-parity spin splitting
is often believed to appear in non-collinear magnetic configurations. Here,
using symmetry arguments and effective model analysis, we show that Floquet
engineering offers a universal strategy for achieving odd-parity magnetism in
two-dimensional (2D) collinear antiferromagnets under irradiation of periodic
driving light fields such as circularly polarized light, elliptically polarized
light, and bicircular light. A comprehensive classification of potential
candidates for collinear monolayer or bilayer antiferromagnets is established.
Strikingly, the light-induced odd-parity spin splitting can be flexibly
controlled by adjusting the crystalline symmetry or the polarization state of
incident light, enabling the reversal or conversion of spin-splitting. By
combining first-principles calculations and Floquet theorem, we present
illustrative examples of 2D collinear antiferromagnetic (AFM) materials to
verify the light-induced odd-parity magnetism. Our work not only offers a
powerful approach for uniquely achieving odd-parity spin-splitting with high
tunability, but also expands the potential of Floquet engineering in designing
unconventional compensated magnetism.

</details>


### [415] [Chemical capacitor: its concept, functionalities and limits](https://arxiv.org/abs/2507.20724)
*Łukasz Wolański,Dawid Ciszewski,Piotr Szkudlarek,José Lorenzana,Wojciech Grochala*

Main category: cond-mat.mtrl-sci

TL;DR: 化学电容器 (CC) 使用密度泛函理论进行研究，可实现高电荷转移，可通过成分和铁电层进行调整，并可能实现超导性。


<details>
  <summary>Details</summary>
Motivation: 研究化学电容器 (CC) 中的主要影响、物理化学性质的极端情况以及纳米器件的适用性限制。

Method: 使用密度泛函理论计算研究化学电容器 (CC) 中的简单但多样的化学计量。

Result: 研究的化学电容器可实现高达每原子 1.74 电子的电荷转移。可以调整电荷转移，并且可以掺杂不同类别的化学系统，包括金属和非金属元素以及化合物，在某些情况下甚至可能出现超导性。

Conclusion: CC 允许每原子高达 1.74 电子的电荷转移，并且可以通过仔细选择 CC 的化学成分以及使用铁电材料作为分隔层来调整电荷转移。

Abstract: We use density functional theory calculations to study simple but diverse
stoichiometries within the novel chemical capacitor (CC) setup. We look at main
effects occurring in this device, extremes of the physicochemical properties,
and we study limits of applicability of this nano-object. In the cases studied,
CC permits achieving charge transfer of up to 1.74 e per atom. Tuning of the
charge transfer may be achieved via judicious choice of chemical constituents
of the CC as well as use of a ferroelectric material as a separator layer.
Different classes of chemical systems may be doped, including metallic and
nonmetallic elements, and chemical compounds, in certain cases leading to the
appearance of superconductivity.

</details>


### [416] [First principles study of [111]-oriented epitaxially strained Rare-Earth Nickelate NdNiO$_3$](https://arxiv.org/abs/2507.20819)
*Alexander Lione,Jorge Íñiguez-González,Nicholas C. Bristowe*

Main category: cond-mat.mtrl-sci

TL;DR: 应变影响[111]NdNiO3的相变和电子性质，特别是拉伸应变能放大带隙。


<details>
  <summary>Details</summary>
Motivation: 研究镧系钙钛矿中经历金属-绝缘体转变的NdNiO3，以了解应变对其性质的影响。

Method: 利用密度泛函理论研究了双轴应变对[111]取向的NdNiO3的结构、电子和磁性质的影响。

Result: 发现了在双轴应变下，[111]取向的NdNiO3会发生独特的结构相变，并表现出与先前研究不同的电子行为，例如拉伸应变会放大带隙，并出现具有非邻域倾斜模式的绝缘、电荷有序相。同时，还揭示了应变与呼吸模式之间的耦合关系。

Conclusion: 研究发现，[111]取向的NdNiO3在双轴应变下会发生独特的结构相变，并表现出独特的电子行为，包括拉伸应变引起的带隙放大以及具有非邻域倾斜模式的绝缘、电荷有序相。研究还阐述了应变与呼吸模式之间的耦合关系，某些应变可以直接促进或抑制呼吸模式（及其相关的电荷有序）。带隙随应变增大的现象可以用弹性约束和八面体呼吸之间的协同耦合来解释，这进一步扩展了先前报道的由八面体倾斜介导的触发机制。

Abstract: Density functional theory is used to investigate the effect of biaxial strain
on the structural, electronic and magnetic properties of [111]-oriented
NdNiO$_3$, as a representative of the rare-earth perovskites that undergo
metal-to-insulator transitions. We find that this constraint on the system
induces unique structural phase transitions not previously observed under the
well-studied bulk or [001]-oriented strained systems. We also report unique
electronic behaviour, including amplification of the electronic band-gap with
tensile strain, and insulating, charge-ordered phases with non-orthorhombic
tilt patterns. To provide clarity to the trends we observe, we also investigate
the coupling between the breathing mode and strain, where we observe certain
strains to directly favour and disfavour the creation of the breathing mode
(and thus the associated charge-ordering). The amplification of the band gap
with strain is understood in terms of a cooperative coupling between the
elastic constraint and octahedral breathing, which expands on the previously
reported triggered mechanism mediated by octahedral tilting.

</details>


### [417] [Defect migration in supercrystalline nanocomposites](https://arxiv.org/abs/2507.20826)
*Dmitry Lapkin,Cong Yan,Emre Gürsoy,Hadas Sternlicht,Alexander Plunkett,Büsra Bor,Young Yong Kim,Dameli Assalauova,Fabian Westermeier,Michael Sprung,Tobias Krekeler,Surya Snata Rout,Martin Ritter,Satishkumar Kulkarni,Thomas F. Keller,Gerold A. Schneider,Gregor B. Vonbun-Feldbauer,Robert H. Meissner,Andreas Stierle,Ivan A. Vartanyants,Diletta Giuntini*

Main category: cond-mat.mtrl-sci

TL;DR: SCNCs processing affects supercrystalline defects. Pressing distorts the lattice, self-assembly creates faults, and heat treatment heals faults and alters grain boundaries. These findings provide insights into nanoparticle assembly.


<details>
  <summary>Details</summary>
Motivation: Supercrystalline nanocomposites (SCNCs) have unique emergent functional properties and offer interesting parallelisms with crystalline materials. Understanding the formation and evolution of ordered assemblies of functionalized nanoparticles is important.

Method: This study shows, via X-ray and in-situ scanning transmission (STEM) electron microscopy analyses, how each of these processing steps plays a distinct role in the generation, migration, interaction and healing of supercrystalline defects. Molecular dynamics simulations were also used to confirm the findings.

Result: Pressing of SCNCs into bulk pellets leads to a distortion of the otherwise fcc superlattice, while emulsion-templated self-assembly yields supraparticles (SPs) with stacking faults and size-dependent symmetries. Heat treatment affects planar defects, causing stacking faults to migrate and heal, and inter-supercrystalline 'grain' boundaries to undergo structural changes. The mechanical properties of SCNCs, with a compressive strength of 100-500 MPa, were also highlighted.

Conclusion: Supercrystalline nanocomposites (SCNCs) are nanostructured hybrid materials with unique emergent functional properties. Pressing of SCNCs into bulk pellets leads to a distortion of the otherwise fcc superlattice, while emulsion-templated self-assembly yields supraparticles (SPs) with stacking faults and size-dependent symmetries. Heat treatment at the same temperatures as those applied for the organic crosslinking has significant effects on planar defects. Stacking faults migrate and get healed, as also confirmed via molecular dynamics simulations, and inter-supercrystalline 'grain' boundaries undergo structural changes. These rearrangements of defects at the supercrystalline scale (tens of nm) in nanocomposites with such remarkable mechanical properties (compressive strength of 100-500 MPa) provide new insights into the formation and evolution of ordered assemblies of functionalized nanoparticles.

Abstract: Supercrystalline nanocomposites (SCNCs) are nanostructured hybrid materials
with unique emergent functional properties. Given their periodically arranged
building blocks, they also offer interesting parallelisms with crystalline
materials. They can be processed in multiple forms and at different scales, and
crosslinking their organic ligands via heat treatment leads to a remarkable
boost of their mechanical properties. This study shows, via X-ray and in-situ
scanning transmission (STEM) electron microscopy analyses, how each of these
processing steps plays a distinct role in the generation, migration,
interaction and healing of supercrystalline defects. Pressing of SCNCs into
bulk pellets leads to a distortion of the otherwise fcc superlattice, while
emulsion-templated self-assembly yields supraparticles (SPs) with stacking
faults and size-dependent symmetries. Interestingly, heat treatment at the same
temperatures as those applied for the organic crosslinking has significant
effects on planar defects. Stacking faults migrate and get healed, as also
confirmed via molecular dynamics simulations, and inter-supercrystalline
'grain' boundaries undergo structural changes. These rearrangements of defects
at the supercrystalline scale (tens of nm) in nanocomposites with such
remarkable mechanical properties (compressive strength of 100-500 MPa) provide
new insights into the formation and evolution of ordered assemblies of
functionalized nanoparticles.

</details>


### [418] [Active Learning for Predicting the Enthalpy of Mixing inBinary Liquids Based on Ab Initio Molecular Dynamics](https://arxiv.org/abs/2507.20885)
*Quentin Bizot,Ryo Tamura,Guillaume Deffrennes*

Main category: cond-mat.mtrl-sci

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The enthalpy of mixing in the liquid phase is an important property for
predicting phase formation in alloys. It can be estimated in a large
compositional space from pair wise interactions between elements, for which
machine learning has recently provided the most accurate predictions. Further
improvements requires acquiring high quality data in liquids where models are
poorly constrained. In this study, we propose an active learning approach to
identify in which liquids additional data are most needed to improve an initial
dataset that covers over 400 binary liquids. We identify a critical need for
new data on liquids containing refractory elements, which we address by
performing ab initio molecular dynamics simulations for 29 equimolar alloys of
Ir, Os, Re and W. This enables more accurate predictions of the enthalpy of
mixing, and we discuss the trends obtained for refractory elements of period 6.
We use clustering analysis to interpret the results of active learning and to
explore how our features can be linked to Miedema's semi empirical theory.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [419] [Multipath Interference Suppression in Indirect Time-of-Flight Imaging via a Novel Compressed Sensing Framework](https://arxiv.org/abs/2507.19546)
*Yansong Du,Yutong Deng,Yuting Zhou,Feiyu Jiao,Bangyao Wang,Zhancong Xu,Zhaoxiang Jiang,Xun Guan*

Main category: eess.SP

TL;DR: 一种新的压缩传感方法，通过优化的传感矩阵和K-Means聚类，在不改变硬件的情况下提高了iToF系统的深度重建精度和多目标分离能力。


<details>
  <summary>Details</summary>
Motivation: 为了提高间接飞行时间（iToF）系统的深度重建精度和多目标分离能力。

Method: 提出了一种新颖的压缩传感方法，使用单一调制频率，并通过多个相移和窄占空比连续波来构建传感矩阵。在矩阵构建过程中，考虑了由透镜畸变引起的像素级距离变化。为了增强稀疏恢复，在OMP过程中应用K-Means聚类到距离响应字典，并在每个聚类内约束原子选择。

Result: 实验结果表明，所提出的方法在重建精度和鲁棒性方面优于传统方法。

Conclusion: 所提出的方法在重建精度和鲁棒性方面优于传统方法，且无需任何额外的硬件改动。

Abstract: We propose a novel compressed sensing method to improve the depth
reconstruction accuracy and multi-target separation capability of indirect
Time-of-Flight (iToF) systems. Unlike traditional approaches that rely on
hardware modifications, complex modulation, or cumbersome data-driven
reconstruction, our method operates with a single modulation frequency and
constructs the sensing matrix using multiple phase shifts and narrow-duty-cycle
continuous waves. During matrix construction, we further account for pixel-wise
range variation caused by lens distortion, making the sensing matrix better
aligned with actual modulation response characteristics. To enhance sparse
recovery, we apply K-Means clustering to the distance response dictionary and
constrain atom selection within each cluster during the OMP process, which
effectively reduces the search space and improves solution stability.
Experimental results demonstrate that the proposed method outperforms
traditional approaches in both reconstruction accuracy and robustness, without
requiring any additional hardware changes.

</details>


### [420] [Coverage Probability and Average Rate Analysis of Hybrid Cellular and Cell-free Network](https://arxiv.org/abs/2507.19763)
*Zhuoyin Dai,Jingran Xu,Xiaoli Xu,Ruoguang Li,Yong Zeng,Jiangbin Lyu*

Main category: eess.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Cell-free wireless networks deploy distributed access points (APs) to
simultaneously serve user equipments (UEs) across the service region and are
regarded as one of the most promising network architectural paradigms. Despite
recent advances in the performance analysis and optimization of cellfree
wireless networks, it remains an open question whether large-scale deployment
of APs in existing wireless networks can cost-effectively achieve communication
capacity growth. Besides, the realization of a cell-free network is considered
to be a gradual long-term evolutionary process in which cell-free APs will be
incrementally introduced into existing cellular networks, and form a hybrid
communication network with the existing cellular base stations (BSs). Such a
collaboration will bridge the gap between the established cellular network and
the innovative cellfree network. Therefore, hybrid cellular and cell-free
networks (HCCNs) emerge as a practical and feasible solution for advancing
cell-free network development, and it is worthwhile to further explore its
performance limits. This paper presents a stochastic geometry-based hybrid
cellular and cell-free network model to analyze the distributions of signal and
interference and reveal their mutual coupling. Specifically, in order to
benefit the UEs from both the cellular BSs and the cell-free APs, a conjugate
beamforming design is employed, and the aggregated signal is analyzed using
moment matching. Then, the coverage probability of the hybrid network is
characterized by deriving the Laplace transforms and their higher-order
derivatives of interference components. Furthermore, the average achievable
rate of the hybrid network over channel fading is derived based on the
interference coupling analysis.

</details>


### [421] [Radar and Acoustic Sensor Fusion using a Transformer Encoder for Robust Drone Detection and Classification](https://arxiv.org/abs/2507.19785)
*Gevindu Ganganath,Pasindu Sankalpa,Samal Punsara,Demitha Pasindu,Chamira U. S. Edussooriya,Ranga Rodrigo,Udaya S. K. P. Miriya Thanthrige*

Main category: eess.SP

TL;DR: 提出了一种结合雷达和声学传感的多模态方法，利用Transformer编码器融合传感器，以更少的参数实现了优越的无人机检测和分类性能。


<details>
  <summary>Details</summary>
Motivation: 由于小型无人机、低空飞行和环境噪声等挑战，需要强大而准确的无人机检测和分类机制来解决未经授权的无人机侵入等安全问题。

Method: 提出了一种结合雷达和声学传感的多模态方法来检测和分类无人机。该方法利用雷达的远程能力和对不同天气条件的鲁棒性，并直接使用原始声学信号（不转换为频谱图或梅尔频率倒谱系数），从而减少了参数数量。此外，还探索了使用Transformer编码器架构融合这些传感器的有效性。

Result: 与现有的最先进方法相比，在户外环境中取得了优越的性能。

Conclusion: 实验结果在户外环境中得到验证，证明了所提出的方法与现有最先进的方法相比具有优越的性能。

Abstract: The use of drones in a wide range of applications is steadily increasing.
However, this has also raised critical security concerns such as unauthorized
drone intrusions into restricted zones. Therefore, robust and accurate drone
detection and classification mechanisms are required despite significant
challenges due to small size of drones, low-altitude flight, and environmental
noise. In this letter, we propose a multi-modal approach combining radar and
acoustic sensing for detecting and classifying drones. We employ radar due to
its long-range capabilities, and robustness to different weather conditions. We
utilize raw acoustic signals without converting them to other domains such as
spectrograms or Mel-frequency cepstral coefficients. This enables us to use
fewer number of parameters compared to the stateof-the-art approaches.
Furthermore, we explore the effectiveness of the transformer encoder
architecture in fusing these sensors. Experimental results obtained in outdoor
settings verify the superior performance of the proposed approach compared to
the state-of-the-art methods.

</details>


### [422] [Channel Estimation in Massive MIMO Systems with Orthogonal Delay-Doppler Division Multiplexing](https://arxiv.org/abs/2507.19812)
*Dezhi Wang,Chongwen Huang,Xiaojun Yuan,Sami Muhaidat,Lei Liu,Xiaoming Chen,Zhaoyang Zhang,Chau Yuen,Mérouane Debbah*

Main category: eess.SP

TL;DR: 本文提出了一种基于MAMP的低复杂度算法，用于解决大规模MIMO-ODDM系统中的信道估计问题，该算法性能接近最优，并显著提高了估计精度。


<details>
  <summary>Details</summary>
Motivation: 高移动性情况下的正交时延-多普勒划分复用（ODDM）调制被认为是一种有前途的可靠通信技术。然而，由于天线阵列巨大和高移动性环境，大规模MIMO-ODDM系统中的精确低复杂度信道估计是一个关键挑战。

Method: 本文提出了一种基于记忆近似消息传递（MAMP）的低复杂度算法来估计信道状态信息（CSI）。首先建立了大规模MIMO-ODDM系统的有效信道模型，其中等效信道向量的元素幅度服从伯努利-高斯分布，并且等效系数矩阵的元素趋于完全随机。利用这些特性，采用MAMP方法确定了多径信道的增益、延迟和多普勒效应，并通过离散傅里叶变换方法估计了信道角度。

Result: 数值结果表明，所提出的信道估计算法在天线数量趋于无穷时，其性能接近贝叶斯最优结果，并且在归一化均方误差方面比现有算法提高了约30%的信道估计精度。

Conclusion: 该研究提出了一种基于MAMP的低复杂度算法，用于估计下行大规模MIMO-ODDM系统中的信道状态信息（CSI），该算法在天线数量趋于无穷时接近贝叶斯最优结果，并将信道估计精度在归一化均方误差方面提高了约30%。

Abstract: Orthogonal delay-Doppler division multiplexing~(ODDM) modulation has recently
been regarded as a promising technology to provide reliable communications in
high-mobility situations. Accurate and low-complexity channel estimation is one
of the most critical challenges for massive multiple input multiple
output~(MIMO) ODDM systems, mainly due to the extremely large antenna arrays
and high-mobility environments. To overcome these challenges, this paper
addresses the issue of channel estimation in downlink massive MIMO-ODDM systems
and proposes a low-complexity algorithm based on memory approximate message
passing~(MAMP) to estimate the channel state information~(CSI). Specifically,
we first establish the effective channel model of the massive MIMO-ODDM
systems, where the magnitudes of the elements in the equivalent channel vector
follow a Bernoulli-Gaussian distribution. Further, as the number of antennas
grows, the elements in the equivalent coefficient matrix tend to become
completely random. Leveraging these characteristics, we utilize the MAMP method
to determine the gains, delays, and Doppler effects of the multi-path channel,
while the channel angles are estimated through the discrete Fourier transform
method. Finally, numerical results show that the proposed channel estimation
algorithm approaches the Bayesian optimal results when the number of antennas
tends to infinity and improves the channel estimation accuracy by about 30%
compared with the existing algorithms in terms of the normalized mean square
error.

</details>


### [423] [Feature Engineering for Wireless Communications and Networking: Concepts, Methodologies, and Applications](https://arxiv.org/abs/2507.19837)
*Jiacheng Wang,Changyuan Zhao,Zehui Xiong,Tao Xiang,Dusit Niyato,Xianbin Wang,Shiwen Mao,Dong In Kim*

Main category: eess.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: AI-enabled wireless communications have attracted tremendous research
interest in recent years, particularly with the rise of novel paradigms such as
low-altitude integrated sensing and communication (ISAC) networks. Within these
systems, feature engineering plays a pivotal role by transforming raw wireless
data into structured representations suitable for AI models. Hence, this paper
offers a comprehensive investigation of feature engineering techniques in
AI-driven wireless communications. Specifically, we begin with a detailed
analysis of fundamental principles and methodologies of feature engineering.
Next, we present its applications in wireless communication systems, with
special emphasis on ISAC networks. Finally, we introduce a generative AI-based
framework, which can reconstruct signal feature spectrum under malicious
attacks in low-altitude ISAC networks. The case study shows that it can
effectively reconstruct the signal spectrum, achieving an average structural
similarity index improvement of 4%, thereby supporting downstream sensing and
communication applications.

</details>


### [424] [Toward Dual-Functional LAWN: Control-Aware System Design for Aerodynamics-Aided UAV Formations](https://arxiv.org/abs/2507.19910)
*Jun Wu,Weijie Yuan,Qingqing Cheng,Haijia Jin*

Main category: eess.SP

TL;DR: 该研究提出了一种新的UAV编队控制方法，通过优化气动结构和波束成形来降低能耗，并提高了控制性能。


<details>
  <summary>Details</summary>
Motivation: 为了延长UAV的飞行续航能力，并解决在双功能LAWNs中，地面基站（GBS）同时控制多个UAV编队并执行传感任务时的能量消耗问题。

Method: 提出了一种基于ATC-LMS算法的分布式节能编队框架，其中每个UAV使用LMS算法更新本地位置估计，并通过与邻居节点的协同信息交换进行优化。为了保证控制稳定性和公平性，研究将问题形式化为一个最大化LQR最小化问题，并采用了一种结合SCA和SDR技术的两步迭代算法来获得双功能波束成形。 

Result: 仿真结果证实了'V'字形编队是最节能的配置，并表明所提出的设计在提升控制性能方面优于现有的基准方案。

Conclusion: 该研究提出了一种分布式节能编队框架，利用ATC-LMS算法优化了UAV编队的气动结构以降低能耗。通过最大化LQR最小化问题，并结合SCA和SDR技术解决了双功能波束成形问题。仿真结果表明'V'字形编队最节能，且所提出的设计在提升控制性能方面优于基准方案。

Abstract: Integrated sensing and communication (ISAC) has emerged as a pivotal
technology for advancing low-altitude wireless networks (LAWNs), serving as a
critical enabler for next-generation communication systems. This paper
investigates the system design for energy-saving unmanned aerial vehicle (UAV)
formations in dual-functional LAWNs, where a ground base station (GBS)
simultaneously wirelessly controls multiple UAV formations and performs sensing
tasks. To enhance flight endurance, we exploit the aerodynamic upwash effects
and propose a distributed energy-saving formation framework based on the
adapt-then-combine (ATC) diffusion least mean square (LMS) algorithm.
Specifically, each UAV updates the local position estimate by invoking the LMS
algorithm, followed by refining it through cooperative information exchange
with neighbors. This enables an optimized aerodynamic structure that minimizes
the formation's overall energy consumption. To ensure control stability and
fairness, we formulate a maximum linear quadratic regulator (LQR) minimization
problem, which is subject to both the available power budget and the required
sensing beam pattern gain. To address this non-convex problem, we develop a
two-step approach by first deriving a closed-form expression of LQR as a
function of arbitrary beamformers. Subsequently, an efficient iterative
algorithm that integrates successive convex approximation (SCA) and
semidefinite relaxation (SDR) techniques is proposed to obtain a sub-optimal
dual-functional beamforming solution. Extensive simulation results confirm that
the 'V'-shaped formation is the most energy-efficient configuration and
demonstrate the superiority of our proposed design over benchmark schemes in
improving control performance.

</details>


### [425] [Deep Learning Based Joint Channel Estimation and Positioning for Sparse XL-MIMO OFDM Systems](https://arxiv.org/abs/2507.19936)
*Zhongnian Li,Chao Zheng,Jian Xiao,Ji Wang,Gongpu Wang,Ming Zeng,Octavia A. Dobre*

Main category: eess.SP

TL;DR: 一种用于近场稀疏超大Massive MIMO-OFDM系统的深度学习两阶段框架，利用CP-Mamba架构同时进行信道估计和定位，提高了精度，稀疏阵列优于紧凑阵列。


<details>
  <summary>Details</summary>
Motivation: 为了实现信道估计和定位之间的协同增益，并解决近场稀疏超大Massive MIMO-OFDM系统中的信道估计和定位问题。

Method: 提出了一种基于深度学习的两阶段框架，包括定位和信道估计。在该框架中，提出了一种U型Mamba架构（CP-Mamba）用于信道估计和定位。

Result: 数值模拟结果表明，所提出的CP-Mamba两阶段方法在信道估计和定位精度上优于现有基线方法，稀疏阵列性能优于传统紧凑阵列。

Conclusion: 所提出的两阶段方法和CP-Mamba架构优于现有基线方法，并且稀疏阵列（SA）在信道估计和定位精度方面均显著优于传统的紧凑阵列。

Abstract: This paper investigates joint channel estimation and positioning in
near-field sparse extra-large multiple-input multiple-output (XL-MIMO)
orthogonal frequency division multiplexing (OFDM) systems. To achieve
cooperative gains between channel estimation and positioning, we propose a deep
learning-based two-stage framework comprising positioning and channel
estimation. In the positioning stage, the user's coordinates are predicted and
utilized in the channel estimation stage, thereby enhancing the accuracy of
channel estimation. Within this framework, we propose a U-shaped Mamba
architecture for channel estimation and positioning, termed as CP-Mamba. This
network integrates the strengths of the Mamba model with the structural
advantages of U-shaped convolutional networks, enabling effective capture of
local spatial features and long-range temporal dependencies of the channel.
Numerical simulation results demonstrate that the proposed two-stage approach
with CP-Mamba architecture outperforms existing baseline methods. Moreover,
sparse arrays (SA) exhibit significantly superior performance in both channel
estimation and positioning accuracy compared to conventional compact arrays.

</details>


### [426] [Dependability Theory-based Statistical QoS Provisioning of Fluid Antenna Systems](https://arxiv.org/abs/2507.19984)
*Irfan Muhammad,Priyadarshi Mukherjee,Wee Kiat New,Hirley Alves,Ioannis Krikidis,Kai-Kit Wong*

Main category: eess.SP

TL;DR: 本研究提出了一种用于流体天线系统（FAS）的可靠性理论框架，用于在有限块长（FBL）约束下提供服务质量（QoS）。它引入了新的度量，如任务可靠性、平均首次任务失败时间（MTTFF）、任务有效容量（mEC）和任务有效能源效率（mEEE），并提出了一种优化方法来最大化mEEE。仿真结果揭示了关键权衡，为设计超可靠、低延迟和节能的工业物联网（IIoT）系统提供了指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常忽略通道衰落的时间动态及其对任务关键操作的影响。因此，本研究的动机是开发一个框架，用于在有限块长（FBL）约束下为流体天线系统（FAS）提供服务质量（QoS），特别关注任务关键操作的可靠性和效率。

Method: 本研究提出了一种用于在有限块长（FBL）约束下统计保证流体天线系统（FAS）的服务质量（QoS）的可靠性理论框架。它通过为FAS推导N端口上Nakagami-m衰落信道的电平交叉率（LCR）和平均衰落持续时间（AFD）的封闭式表达式来做到这一点。然后，它定义任务可靠性和平均首次任务失败时间（MTTFF）等度量来量化任务成功运行的概率。此外，它将有效容量（EC）概念扩展到FBL下的任务可靠性，以创建任务有效容量（mEC）。为了解决能源效率问题，该研究引入了任务有效能源效率（mEEE）度量，并提出了一个解决该度量的优化问题，该问题使用改进的Dinkelbach方法和行搜索来解决。

Result: 该研究为FAS在Nakagami-m衰落信道上的LCR和AFD推导出了封闭式表达式。它还引入了任务可靠性、MTTFF、mEC和mEEE等度量。此外，它提出了一个使用改进的Dinkelbach方法解决的优化问题，用于最大化mEEE。仿真结果揭示了端口数、QoS指数、信噪比和任务持续时间之间的关键权衡。

Conclusion: 该研究通过引入以任务为中心的度量（例如任务可靠性和平均首次任务失败时间）来量化和优化流体天线系统的性能，为下一代无线网络的设计提供了宝贵的见解，特别关注工业物联网中的超可靠、低延迟和节能操作。

Abstract: Fluid antenna systems (FAS) have recently emerged as a promising technology
for next-generation wireless networks, offering real-time spatial
reconfiguration to enhance reliability, throughput, and energy efficiency.
Nevertheless, existing studies often overlook the temporal dynamics of channel
fading and their implications for mission-critical operations. In this paper,
we propose a dependability-theoretic framework for statistical
quality-of-service (QoS) provisioning of FAS under finite blocklength (FBL)
constraints. Specifically, we derive new closed-form expressions for the
level-crossing rate (LCR) and average fade duration (AFD) of an $N$-port FAS
over Nakagami-$m$ fading channels. Leveraging these second-order statistics, we
define two key dependability metrics such as mission reliability and mean
time-to-first-failure (MTTFF), to quantify the probability of uninterrupted
operation over a defined mission duration. We further extend the classical
effective capacity (EC) concept to incorporate mission reliability in the FBL
regime, yielding a mission EC (mEC). To capture energy efficiency under bursty
traffic and latency constraints, we also develop the mission effective energy
efficiency (mEEE) metric and formulate its maximization as a non-convex
fractional optimization problem. This problem is then solved via a modified
Dinkelbach's method with an embedded line search. Extensive simulations uncover
critical trade-offs among port count, QoS exponent, signal-to-noise ratio, and
mission duration, offering insights for the design of ultra-reliable,
low-latency, and energy-efficient industrial internet-of-things (IIoT) systems.

</details>


### [427] [DOA Estimation via Optimal Weighted Low-Rank Matrix Completion](https://arxiv.org/abs/2507.19996)
*Saeed Razavikia,Mohammad Bokaei,Arash Amini,Stefano Rini,Carlo Fischione*

Main category: eess.SP

TL;DR: 一种用于非均匀稀疏线性阵列的DOA估计新方法，采用加权提升结构低秩矩阵恢复，仅需单快照，样本复杂度接近最优，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为解决非均匀稀疏线性传感器阵列的DOA估计问题，提出一种新方法。

Method: 该方法基于加权提升结构低秩矩阵恢复框架，包括四个关键步骤：(i) 提升天线样本形成低秩结构；(ii) 设计左、右权重矩阵以反映样本信息量；(iii) 通过加权提升样本的补全来估计无噪声均匀阵列输出；(iv) 从恢复的均匀线性阵列样本中获得DOA。

Result: 所提出的加权矩阵选择权重矩阵，实现了接近最优的样本复杂度，该复杂度与DOA数量（考虑对数因子）相当。数值评估表明，该方法优于非加权方法和基于原子范数最小化的方法，在低信噪比条件下，均方误差降低约10dB。

Conclusion: 该方法在低信噪比条件下，与非加权方法相比，均方误差降低了约10dB，优于非加权方法和基于原子范数最小化的方法。

Abstract: This paper presents a novel method for estimating the direction of arrival
(DOA) for a non-uniform and sparse linear sensor array using the weighted
lifted structure low-rank matrix completion. The proposed method uses a single
snapshot sample in which a single array of data is observed. The method is
rooted in a weighted lifted-structured low-rank matrix recovery framework. The
method involves four key steps: (i) lifting the antenna samples to form a
low-rank stature, then (ii) designing left and right weight matrices to reflect
the sample informativeness, (iii) estimating a noise-free uniform array output
through completion of the weighted lifted samples, and (iv) obtaining the DOAs
from the restored uniform linear array samples.
  We study the complexity of steps (i) to (iii) above, where we analyze the
required sample for the array interpolation of step (iii) for DOA estimation.
We demonstrate that the proposed choice of weight matrices achieves a
near-optimal sample complexity. This complexity aligns with the problem's
degree of freedom, equivalent to the number of DOAs adjusted for logarithmic
factors. Numerical evaluations show the proposed method's superiority against
the non-weighted counterpart and atomic norm minimization-based methods.
Notably, our proposed method significantly improves, with approximately a 10 dB
reduction in normalized mean-squared error over the non-weighted method at
low-noise conditions.

</details>


### [428] [NeuroCLIP: A Multimodal Contrastive Learning Method for rTMS-treated Methamphetamine Addiction Analysis](https://arxiv.org/abs/2507.20189)
*Chengkai Wang,Di Wu,Yunsheng Liao,Wenyao Zheng,Ziyi Zeng,Xurong Gao,Hemmings Wu,Zhoule Zhu,Jie Yang,Lihua Zhong,Weiwei Cheng,Yun-Hsuan Chen,Mohamad Sawan*

Main category: eess.SP

TL;DR: 甲基苯丙胺依赖的评估常依赖主观报告，而现有的客观神经影像方法（EEG/fNIRS）及其特征提取方法存在局限。本研究提出NeuroCLIP框架，通过结合EEG和fNIRS数据，利用深度学习提供了一种更可靠的成瘾生物标志物，并可用于评估治疗效果。


<details>
  <summary>Details</summary>
Motivation: 克服现有评估甲基苯丙胺依赖性及rTMS治疗效果时，依赖主观自我报告以及单一神经影像模态（EEG或fNIRS）的局限性，以提供更可靠的生物标志物。

Method: 提出了一种名为NeuroCLIP的新型深度学习框架，采用渐进式学习策略整合同步记录的EEG和fNIRS数据。

Result: NeuroCLIP显著提高了区分甲基苯丙胺依赖个体与健康对照组的能力，并能客观评估rTMS治疗效果，显示治疗后神经模式向健康对照组特征转变，且该生物标志物与心理测量学验证的渴求评分高度相关。

Conclusion: NeuroCLIP框架通过整合EEG和fNIRS数据，为甲基苯丙胺成瘾提供了一种更可靠、更值得信赖的生物标志物，优于单一模态的方法，有望改善临床评估。

Abstract: Methamphetamine dependence poses a significant global health challenge, yet
its assessment and the evaluation of treatments like repetitive transcranial
magnetic stimulation (rTMS) frequently depend on subjective self-reports, which
may introduce uncertainties. While objective neuroimaging modalities such as
electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS)
offer alternatives, their individual limitations and the reliance on
conventional, often hand-crafted, feature extraction can compromise the
reliability of derived biomarkers. To overcome these limitations, we propose
NeuroCLIP, a novel deep learning framework integrating simultaneously recorded
EEG and fNIRS data through a progressive learning strategy. This approach
offers a robust and trustworthy biomarker for methamphetamine addiction.
Validation experiments show that NeuroCLIP significantly improves
discriminative capabilities among the methamphetamine-dependent individuals and
healthy controls compared to models using either EEG or only fNIRS alone.
Furthermore, the proposed framework facilitates objective, brain-based
evaluation of rTMS treatment efficacy, demonstrating measurable shifts in
neural patterns towards healthy control profiles after treatment. Critically,
we establish the trustworthiness of the multimodal data-driven biomarker by
showing its strong correlation with psychometrically validated craving scores.
These findings suggest that biomarker derived from EEG-fNIRS data via NeuroCLIP
offers enhanced robustness and reliability over single-modality approaches,
providing a valuable tool for addiction neuroscience research and potentially
improving clinical assessments.

</details>


### [429] [Information-Preserving CSI Feedback: Invertible Networks with Endogenous Quantization and Channel Error Mitigation](https://arxiv.org/abs/2507.20283)
*Haotian Tian,Lixiang Lian,Jiaqi Cao,Sijie Ji*

Main category: eess.SP

TL;DR: InvCSINet利用可逆神经网络实现了信息保持的CSI反馈，并通过集成自适应量化、比特通道失真补偿和信息补偿模块来提高对实际损伤的鲁棒性，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法通常依赖于深度自动编码器来压缩CSI，这会导致不可逆的信息丢失并降低重建精度。因此，需要一种能够保持信息并提高重建精度的CSI反馈方法。

Method: InvCSINet是一个基于可逆神经网络（INNs）的信息保持CSI反馈框架。该框架利用INNs的双射性质来实现信息保持的压缩和重建，并共享模型参数。此外，它还集成了自适应量化模块、可微分比特通道失真模块和信息补偿模块，以应对量化和信道引起的错误等实际挑战。

Result: 仿真结果表明，InvCSINet框架在CSI恢复性能方面优于现有方法，并且对量化和信道噪声等实际损伤具有鲁棒性，同时保持了轻量级的架构。

Conclusion: 所提出的InvCSINet框架通过利用可逆神经网络（INNs）的双射性质，实现了信息保持的压缩和重建，并共享模型参数。通过内在地集成自适应量化模块、可微分比特通道失真模块和信息补偿模块，该网络能够学习并补偿CSI压缩、量化和噪声传输过程中的信息损失，从而在整个反馈过程中保持CSI的完整性。仿真结果验证了该方案的有效性，证明了其在轻量级架构下具有优越的CSI恢复性能和对实际损伤的鲁棒性。

Abstract: Deep learning has emerged as a promising solution for efficient channel state
information (CSI) feedback in frequency division duplex (FDD) massive MIMO
systems. Conventional deep learning-based methods typically rely on a deep
autoencoder to compress the CSI, which leads to irreversible information loss
and degrades reconstruction accuracy. This paper introduces InvCSINet, an
information-preserving CSI feedback framework based on invertible neural
networks (INNs). By leveraging the bijective nature of INNs, the model ensures
information-preserving compression and reconstruction with shared model
parameters. To address practical challenges such as quantization and
channel-induced errors, we endogenously integrate an adaptive quantization
module, a differentiable bit-channel distortion module and an information
compensation module into the INN architecture. This design enables the network
to learn and compensate the information loss during CSI compression,
quantization, and noisy transmission, thereby preserving the CSI integrity
throughout the feedback process. Simulation results validate the effectiveness
of the proposed scheme, demonstrating superior CSI recovery performance and
robustness to practical impairments with a lightweight architecture.

</details>


### [430] [Reliability of Wi-Fi, LTE, and 5G-Based UAV RC Links in ISM Bands: Uplink Interference Asymmetry Analysis and HARQ Design](https://arxiv.org/abs/2507.20392)
*Donggu Lee,Sung Joon Maeng,Ozgur Ozdemir,Mani Bharathi Pandian,Ismail Guvenc*

Main category: eess.SP

TL;DR: UAVs flying at higher altitudes experience much more interference than ground units, which can disrupt control signals and slow down data transfer. This paper measures the interference, explains why it happens, and tests different ways to fix it.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the problem of interference in UAV remote control links operating in ISM bands. Specifically, it aims to provide empirical evidence of asymmetric interference conditions between ground and aerial units and to investigate the impact of this interference on system performance, particularly downlink throughput degradation caused by uplink interference.

Method: The study conducted a measurement campaign using a helikite platform in urban and rural areas to gather empirical data on interference levels at different altitudes. It then theoretically analyzes the impact of this asymmetric interference on HARQ mechanisms and evaluates throughput performance through a three-step process involving standalone PDSCH throughput, PUCCH decoding reliability, and PDSCH throughput with asymmetric ACK/NACK transmission.

Result: The measurement campaign revealed that aggregate interference can be up to 16.66 dB higher at altitudes of up to 170m compared to ground receivers. This asymmetric uplink interference can lead to lost HARQ indicators, thereby degrading downlink throughput. The subsequent evaluations aim to quantify this degradation under various HARQ mechanisms and transmission scenarios.

Conclusion: The study investigates the impact of asymmetric interference on UAV remote control links, finding that higher altitudes experience significantly more interference. This interference can degrade downlink throughput due to lost uplink acknowledgments. The research evaluates various HARQ mechanisms and their impact on throughput through a three-step evaluation process.

Abstract: Command and control of uncrewed aerial vehicles (UAVs) is often realized
through air-to-ground (A2G) remote control (RC) links that operate in ISM
bands. While wireless fidelity (Wi-Fi) technology is commonly used for UAV RC
links, ISM-based long-term evolution (LTE) and fifth-generation (5G)
technologies have also been recently considered for the same purpose. A major
problem for UAV RC links in the ISM bands is that other types of interference
sources, such as legacy Wi-Fi and Bluetooth transmissions, may degrade the link
quality. Such interference problems are a higher concern for the UAV in the air
than the RC unit on the ground due to the UAV being in line-of-sight (LoS) with
a larger number of interference sources. To obtain empirical evidence of the
asymmetric interference conditions in downlink (DL) and uplink (UL), we first
conducted a measurement campaign using a helikite platform in urban and rural
areas at NC State University. The results from this measurement campaign show
that the aggregate interference can be up to 16.66 dB at higher altitudes up to
170 m, compared with the interference observed at a ground receiver. As a
result of this asymmetric UL interference, lost hybrid automatic repeat request
(HARQ) indicators (ACK/NACK) in the UL may degrade the DL throughput. To
investigate this, we study various HARQ mechanisms, including HARQ Type-I with
no combining, HARQ Type-I with chase combining, HARQ Type-III with incremental
redundancy, and burst transmission with chase combining. To evaluate the impact
of asymmetric UL interference on throughput performance, we consider three
steps of evaluation process: 1) standalone physical DL shared channel (PDSCH)
throughput evaluation with perfect ACK/NACK assumption; 2) standalone physical
UL control channel (PUCCH) decoding reliability evaluation; and 3) PDSCH DL
throughput evaluation with asymmetric UL ACK/NACK transmission.

</details>


### [431] [A Multi-Stage Hybrid CNN-Transformer Network for Automated Pediatric Lung Sound Classification](https://arxiv.org/abs/2507.20408)
*Samiul Based Shuvo,Taufiq Hasan*

Main category: eess.SP

TL;DR: 提出了一种混合CNN-Transformer模型，用于对儿科呼吸音进行分类，在儿科呼吸道疾病的诊断方面取得了有希望的结果，尤其是在资源有限的地区。


<details>
  <summary>Details</summary>
Motivation: 由于儿科肺部发育的改变，儿科（<6岁）呼吸音听诊的自动化分析是一个未被充分探索的领域，这改变了呼吸音的声学特性，因此需要专门针对这一年龄组的分类方法。

Method: 提出了一种多阶段混合CNN-Transformer框架，该框架结合了CNN提取的特征和基于注意力的架构，使用来自完整录音和单次呼吸事件的scalogram图像对儿科呼吸道疾病进行分类。

Result: 在二元事件分类中，模型取得了0.9039的总体得分，在多类事件分类中取得了0.8448的得分。通过采用逐类焦点损失来解决数据不平衡问题。在录音级别，该模型在三元分类中获得了0.720的得分，在多类分类中获得了0.571的得分。这些得分分别比之前最好的模型高出3.81%和5.94%。

Conclusion: 该方法为儿科呼吸道疾病的远程诊断提供了一个有前途的解决方案，特别是在资源有限的环境中。

Abstract: Automated analysis of lung sound auscultation is essential for monitoring
respiratory health, especially in regions facing a shortage of skilled
healthcare workers. While respiratory sound classification has been widely
studied in adults, its ap plication in pediatric populations, particularly in
children aged <6 years, remains an underexplored area. The developmental
changes in pediatric lungs considerably alter the acoustic proper ties of
respiratory sounds, necessitating specialized classification approaches
tailored to this age group. To address this, we propose a multistage hybrid
CNN-Transformer framework that combines CNN-extracted features with an
attention-based architecture to classify pediatric respiratory diseases using
scalogram images from both full recordings and individual breath events. Our
model achieved an overall score of 0.9039 in binary event classifi cation and
0.8448 in multiclass event classification by employing class-wise focal loss to
address data imbalance. At the recording level, the model attained scores of
0.720 for ternary and 0.571 for multiclass classification. These scores
outperform the previous best models by 3.81% and 5.94%, respectively. This
approach offers a promising solution for scalable pediatric respiratory disease
diagnosis, especially in resource-limited settings.

</details>


### [432] [Energy-Efficient Secure Communications via Joint Optimization of UAV Trajectory and Movable-Antenna Array Beamforming](https://arxiv.org/abs/2507.20489)
*Sanghyeok Kim,Jinu Gong,Joonhyuk Kang*

Main category: eess.SP

TL;DR: 本研究提出了一种优化无人机轨迹和可重构波束形成的方法，以提高无线通信系统的保密性，并取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高配备可移动天线（MA）阵列的无人机（UAV）在无线通信系统中的安全性。

Method: 提出了一种联合优化无人机轨迹和多动天线（MA）阵列的可重构波束形成的框架，以最大化保密能量效率，同时保证与合法用户的通信可靠性。

Result: 数值结果表明，所提出的方法由于增强了由可移动天线架构提供的空间灵活性，实现了优越的保密能量效率。

Conclusion: 本研究提出的联合优化无人机轨迹和可重构多动天线阵列波束形成的方法，能够显著提高无线通信系统的保密性，并实现优越的保密能量效率。

Abstract: This paper investigates the potential of unmanned aerial vehicles (UAVs)
equipped with movable-antenna (MA) arrays to strengthen security in wireless
communication systems. We propose a novel framework that jointly optimizes the
UAV trajectory and the reconfigurable beamforming of the MA array to maximize
secrecy energy efficiency, while ensuring reliable communication with
legitimate users. By exploiting the spatial degrees of freedom enabled by the
MA array, the system can form highly directional beams and deep nulls, thereby
significantly improving physical layer security. Numerical results demonstrate
that the proposed approach achieves superior secrecy energy efficiency,
attributed to the enhanced spatial flexibility provided by the movable antenna
architecture.

</details>


### [433] [Real-Time Distributed Optical Fiber Vibration Recognition via Extreme Lightweight Model and Cross-Domain Distillation](https://arxiv.org/abs/2507.20587)
*Zhongyao Luo,Hao Wu,Zhao Ge,Ming Tang*

Main category: eess.SP

TL;DR: 该研究通过FPGA加速的轻量级模型和跨域知识蒸馏框架，解决了DVS系统的精度和实时性问题，实现了高效、高精度的分布式光纤传感。


<details>
  <summary>Details</summary>
Motivation: 分布式光纤振动传感（DVS）系统在大型监测和入侵事件识别方面具有潜力，但其实际部署面临两大挑战：动态条件下的识别精度下降以及海量传感数据的实时处理计算瓶颈。本研究旨在解决这些问题，以提高DVS系统的实用性和性能。

Method: 提出了一种基于FPGA加速的极轻量级模型和新颖的知识蒸馏框架。该模型采用三层深度可分离卷积网络，参数量仅为4141个，实现了高度紧凑的架构。同时，利用受物理先验指导的跨域蒸馏框架，将频域信息融入时域模型，实现了无需增加复杂度的时频联合表征学习。

Result: 该模型实现了高达每样本0.019毫秒的处理速度，覆盖12.5米光纤长度（0.256秒），支持最长168.68公里的实时处理能力。通过跨域蒸馏框架，识别准确率从51.93%提升至95.72%，尤其在未知的环境条件下表现出显著的泛化能力提升。

Conclusion: 该研究提出了一个创新的框架，将可解释的信号处理技术与深度学习相结合，并提供了一个用于分布式光纤传感（DVS）系统实时处理和边缘计算的参考架构，进一步推动了分布式光纤传感（DOFS）领域的发展。该方法缓解了传感范围和实时能力之间的权衡，弥合了理论能力与实际部署需求之间的差距，并为构建更高效、更鲁棒、更具可解释性的人工智能系统开辟了新方向。

Abstract: Distributed optical fiber vibration sensing (DVS) systems offer a promising
solution for large-scale monitoring and intrusion event recognition. However,
their practical deployment remains hindered by two major challenges:
degradation of recognition accuracy in dynamic conditions, and the
computational bottleneck of real-time processing for mass sensing data. This
paper presents a new solution to these challenges, through a FPGA-accelerated
extreme lightweight model along with a newly proposed knowledge distillation
framework. The proposed three-layer depthwise separable convolution network
contains only 4141 parameters, which is the most compact architecture in this
field to date, and achieves a maximum processing speed of 0.019 ms for each
sample covering a 12.5 m fiber length over 0.256 s. This performance
corresponds to real-time processing capabilities for sensing fibers extending
up to 168.68 km. To improve generalizability under changing environments, the
proposed cross-domain distillation framework guided by physical priors is used
here to embed frequency-domain insights into the time-domain model. This allows
for time-frequency representation learning without increasing complexity and
boosts recognition accuracy from 51.93% to 95.72% under unseen environmental
conditions. The proposed methodology provides key advancements including a
framework combining interpretable signal processing technique with deep
learning and a reference architecture for real-time processing and
edge-computing in DVS systems, and more general distributed optical fiber
sensing (DOFS) area. It mitigates the trade-off between sensing range and
real-time capability, bridging the gap between theoretical capabilities and
practical deployment requirements. Furthermore, this work reveals a new
direction for building more efficient, robust and explainable artificial
intelligence systems for DOFS technologies.

</details>


### [434] [RFI and Jamming Detection in Antenna Arrays with an LSTM Autoencoder](https://arxiv.org/abs/2507.20648)
*Christos Ntemkas,Antonios Argyriou*

Main category: eess.SP

TL;DR: A new RFI/jamming detection method uses antenna arrays and deep learning to identify interference without needing prior signal information.


<details>
  <summary>Details</summary>
Motivation: Addressing the significant problem of RFI and malicious jammers in wireless communication by developing a novel detection approach.

Method: Using Fourier imaging to spatially localize sources, followed by a deep LSTM autoencoder for anomaly detection of RFI and jamming.

Result: The detector shows high performance across different power levels of RFI/jamming sources and signals of interest, demonstrating its effectiveness.

Conclusion: The proposed RFI/jamming detection method leverages antenna array data and Fourier imaging, achieving high performance without prior signal knowledge.

Abstract: Radio frequency interference (RFI) and malicious jammers are a significant
problem in our wireless world. Detecting RFI or jamming is typically performed
with model-based statistical detection or AI-empowered algorithms that use an
input baseband data or time-frequency representations like spectrograms. In
this work we depart from the previous approaches and we leverage data in
antenna array systems. We use Fourier imaging to localize spatially the sources
and then deploy a deep LSTM autoencoder that detects RFI and jamming as
anomalies. Our results for different power levels of the RFI/jamming sources,
and the signal of interest, reveal that our detector offers high performance
without needing any pre-existing knowledge regarding the RFI or jamming signal.

</details>


### [435] [Angle-distance decomposition based on deep learning for active sonar detection](https://arxiv.org/abs/2507.20651)
*Jichao Zhang,Xiao-Lei Zhang,Kunde Yang*

Main category: eess.SP

TL;DR: 本研究提出了一种基于深度学习的主动声纳目标检测方法，通过分解角度和距离估计任务来克服复杂水下环境的挑战。通过迁移学习和模拟解决了数据限制问题，并在实验中证明了其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统信号处理方法在复杂水下环境中由于噪声、混响和干扰而面临的重大挑战。

Method: 该方法将检测过程分解为单独的角度和距离估计任务，并利用深度学习模型进行预测，最终通过整合这些估计来确定目标位置。

Result: 提出的深度学习方法通过整合角度和距离估计，能够有效和鲁棒地进行水下目标检测。

Conclusion: 实验结果验证了该方法在挑战性条件下实现了有效和鲁棒的性能。

Abstract: Underwater target detection using active sonar constitutes a critical
research area in marine sciences and engineering. However, traditional signal
processing methods face significant challenges in complex underwater
environments due to noise, reverberation, and interference. To address these
issues, this paper presents a deep learning-based active sonar target detection
method that decomposes the detection process into separate angle and distance
estimation tasks. Active sonar target detection employs deep learning models to
predict target distance and angle, with the final target position determined by
integrating these estimates. Limited underwater acoustic data hinders effective
model training, but transfer learning and simulation offer practical solutions
to this challenge. Experimental results verify that the method achieves
effective and robust performance under challenging conditions.

</details>


### [436] [The micro-Doppler Attack Against AI-based Human Activity Classification from Wireless Signals](https://arxiv.org/abs/2507.20657)
*Margarita Loupa,Antonios Argyriou,Yanwei Liu*

Main category: eess.SP

TL;DR: 本研究提出了一种微多普勒攻击，通过修改无线OFDM信号来欺骗人类活动分类系统，可将CNN模型的准确率降至10%以下。


<details>
  <summary>Details</summary>
Motivation: 为了攻击和评估基于无线信号的人类活动分类（HAC）系统的安全性。

Method: 本研究提出了一种微多普勒攻击方法，通过在OFDM信号中插入人工变化来操纵信号的微多普勒特征，并研究了两种不同时间尺度的变体。

Result: 该微多普勒攻击能够将基于CNN的HAC系统的准确率降低到10%以下。

Conclusion: 该研究提出了针对无线OFDM信号的人类活动分类（HAC）系统的微多普勒攻击，通过在OFDM信号中插入人工变化来改变其微多普勒特征，可以使基于CNN的HAC准确率降低到10%以下。

Abstract: A subset of Human Activity Classification (HAC) systems are based on AI
algorithms that use passively collected wireless signals. This paper presents
the micro-Doppler attack targeting HAC from wireless orthogonal frequency
division multiplexing (OFDM) signals. The attack is executed by inserting
artificial variations in a transmitted OFDM waveform to alter its micro-Doppler
signature when it reflects off a human target. We investigate two variants of
our scheme that manipulate the waveform at different time scales resulting in
altered receiver spectrograms. HAC accuracy with a deep convolutional neural
network (CNN) can be reduced to less than 10%.

</details>


### [437] [A Nonlinear Spectral Approach for Radar-Based Heartbeat Estimation via Autocorrelation of Higher Harmonics](https://arxiv.org/abs/2507.20664)
*Kohei Shimomura,Chi-Hsuan Lee,Takuya Sakamoto*

Main category: eess.SP

TL;DR: 一种新的非线性信号处理方法，通过增强频谱的全局周期性结构，比传统方法更准确、更稳健地估计雷达心跳间隔。


<details>
  <summary>Details</summary>
Motivation: 为了提高雷达测量心跳间隔的准确性和鲁棒性，克服传统方法在处理呼吸谐波和噪声方面的局限性。

Method: 提出了一种利用心跳信号固有的高阶谐波周期性进行精确雷达心跳间隔估计的非线性信号处理方法。该方法通过平滑和二阶导数操作预处理雷达位移信号，然后对傅里叶频谱进行局部自相关计算，并通过非线性相关处理增强全局周期性结构，最后将局部自相关值相干叠加得到伪频谱，突出基本心跳周期性。

Result: 与传统技术相比，该方法将均方根误差降低了20%，并将相关系数提高了0.20。

Conclusion: 该方法通过非线性处理显著提高了雷达心跳间隔估计的准确性和鲁棒性，实验结果表明其性能优于传统方法。

Abstract: This study presents a nonlinear signal processing method for accurate
radar-based heartbeat interval estimation by exploiting the periodicity of
higher-order harmonics inherent in heartbeat signals. Unlike conventional
approaches that employ selective frequency filtering or track individual
harmonics, the proposed method enhances the global periodic structure of the
spectrum via nonlinear correlation processing. Specifically, smoothing and
second-derivative operations are first applied to the radar displacement signal
to suppress noise and accentuate higher-order heartbeat harmonics. Rather than
isolating specific frequency components, we compute localized autocorrelations
of the Fourier spectrum around the harmonic frequencies. The incoherent
summation of these autocorrelations yields a pseudo-spectrum in which the
fundamental heartbeat periodicity is distinctly emphasized. This nonlinear
approach mitigates the effects of respiratory harmonics and noise, enabling
robust interbeat interval estimation. Experiments with radar measurements from
five participants demonstrate that the proposed method reduces root-mean-square
error by 20% and improves the correlation coefficient by 0.20 relative to
conventional techniques.

</details>


### [438] [DT-Aided Resource Management in Spectrum Sharing Integrated Satellite-Terrestrial Networks](https://arxiv.org/abs/2507.20789)
*Hung Nguyen-Kha,Vu Nguyen Ha,Ti Ti Nguyen,Eva Lagunas,Symeon Chatzinotas,Joel Grotz*

Main category: eess.SP

TL;DR: 通过数字孪生框架和两阶段算法（SCA和压缩感知）来优化ISTN中的带宽分配、流量转向和资源分配，以最小化队列长度。


<details>
  <summary>Details</summary>
Motivation: 为了在集成卫星-地面网络（ISTNs）中捕获实际环境以进行资源管理，并解决频谱共享带来的挑战，例如系统间干扰（ISI）和低地球轨道卫星（LSat）的移动性。

Method: 提出了一种基于时间变化的数字孪生（DT）的框架，用于集成卫星-地面网络（ISTNs），并结合3D地图。该框架通过两阶段算法解决混合整数非线性规划（MINLP）问题，该算法基于连续凸近似（SCA）和压缩感知方法。

Result: 数值结果表明，与基准相比，所提出的方法在最小化队列长度方面表现出优越的性能。

Conclusion: 所提出的基于数字孪生（DT）的框架通过联合优化带宽（BW）分配、流量转向和资源分配，并以最小化拥塞为目标，成功地最小化了队列长度，表现优于基准。

Abstract: The integrated satellite-terrestrial networks (ISTNs) through spectrum
sharing have emerged as a promising solution to improve spectral efficiency and
meet increasing wireless demand. However, this coexistence introduces
significant challenges, including inter-system interference (ISI) and the low
Earth orbit satellite (LSat) movements. To capture the actual environment for
resource management, we propose a time-varying digital twin (DT)-aided
framework for ISTNs incorporating 3D map that enables joint optimization of
bandwidth (BW) allocation, traffic steering, and resource allocation, and aims
to minimize congestion. The problem is formulated as a mixed-integer nonlinear
programming (MINLP), addressed through a two-phase algorithm based on
successive convex approximation (SCA) and compressed sensing approaches.
Numerical results demonstrate the proposed method's superior performance in
queue length minimization compared to benchmarks.

</details>


### [439] [Chirp-Permuted AFDM: A New Degree of Freedom for Next-Generation Versatile Waveform Design](https://arxiv.org/abs/2507.20825)
*Hyeon Seok Rou,Giuseppe Thadeu Freitas de Abreu*

Main category: eess.SP

TL;DR: CP-AFDM是一种新的AFDM波形，它通过चिरp置换提高了性能，适用于6G通信，并支持索引调制和物理层安全。


<details>
  <summary>Details</summary>
Motivation: 为了满足第六代（6G）通信对可靠性和感知能力的需求，并探索基于AFDM的通信系统的多功能应用。

Method: 提出了一种新颖的调幅-频分复用（AFDM）波形，称为चिरp置换调幅-频分复用（CP-AFDM），在常规AFDM的चिरp子载波之上引入了独特的चिरp置换域。

Result: CP-AFDM保留了AFDM对双重频散信道的鲁棒性、功率峰均比（PAPR）和完整的时延-多普勒表示能力，同时提高了多普勒域的模糊函数分辨率和峰值旁瓣比（PSLR）。此外，通过利用चिरp置换域的自由度，实现了高频谱效率的索引调制和具有完美安全性的物理层安全方案。

Conclusion: CP-AFDM在保持AFDM核心特性的同时，通过引入独特的चिरp置换域，提高了多普勒域的模糊函数分辨率和峰值旁瓣比，是6G应用的有力候选者，并可实现无需额外开销的索引调制和物理层安全。

Abstract: We present a novel multicarrier waveform, termed chirp-permuted affine
frequency division multiplexing (CP-AFDM), which introduces a unique
chirp-permutation domain on top of the chirp subcarriers of the conventional
AFDM. Rigorous analysis of the signal model and waveform properties, supported
by numerical simulations, demonstrates that the proposed CP-AFDM preserves all
core characteristics of affine frequency division multiplexing (AFDM) -
including robustness to doubly-dispersive channels, peak-to-average power ratio
(PAPR), and full delay-Doppler representation - while further enhancing
ambiguity function resolution and peak-to-sidelobe ratio (PSLR) in the Doppler
domain. These improvements establish CP-AFDM as a highly attractive candidate
for emerging sixth generation (6G) use cases demanding both reliability and
sensing-awareness. Moreover, by exploiting the vast degree of freedom in the
chirp-permutation domain, two exemplary multifunctional applications are
introduced: an index modulation (IM) technique over the permutation domain
which achieves significant spectral efficiency gains, and a physical-layer
security scheme that ensures practically perfect security through
permutation-based keying, without requiring additional transmit energy or
signaling overhead.

</details>


### [440] [Interference Analysis and Successive Interference Cancellation for Multistatic OFDM-based ISAC Systems](https://arxiv.org/abs/2507.20942)
*Taewon Jeong,Lucas Giroto,Umut Utku Erdem,Christian Karle,Jiyeon Choi,Thomas Zwick,Benjamin Nuss*

Main category: eess.SP

TL;DR: 本文针对多站ISAC系统中的干扰问题，提出了一种低复杂度、自适应的干扰消除方法，通过仿真和实验验证了其有效性，能够提升通信和感知性能。


<details>
  <summary>Details</summary>
Motivation: 多站集成感知与通信（ISAC）系统虽然在空间覆盖和感知精度方面优于独立的ISAC配置，但当多个ISAC节点同时工作时，它们之间会产生相互干扰，影响感知和通信性能。

Method: 1. 提出并分析了多站ISAC系统中单站ISAC系统和双站ISAC系统共用同一频谱资源时产生的相互干扰，并对干扰进行了分类。
2. 提出了低复杂度、自适应的干扰消除方法，根据单站雷达图像的信干噪比（SINR）自适应地消除单站雷达回波或双站雷达视距信号。
3. 使用仿真和带有雷达回波生成器的ISAC实验平台进行了概念验证。

Result: 仿真和实验结果表明，所提出的方法在多种SINR条件下都能有效降低BER、提高EVM和雷达图像SINR，并且计算开销低。

Conclusion: 提出的低复杂度、自适应干扰消除方法能够有效地消除单站雷达回波或双站雷达视距信号，从而降低比特误码率（BER），提高误差向量幅度（EVM）和雷达图像信干噪比（SINR），适用于实际应用。

Abstract: Multistatic integrated sensing and communications (ISAC) systems, which use
distributed transmitters and receivers, offer enhanced spatial coverage and
sensing accuracy compared to stand-alone ISAC configurations. However, these
systems face challenges due to interference between co-existing ISAC nodes,
especially during simultaneous operation. In this paper, we analyze the impact
of this mutual interference arising from the co-existence in a multistatic ISAC
scenario, where a mono- and a bistatic ISAC system share the same spectral
resources. We first classify differenct types of interference in the power
domain. Then, we discuss how the interference can affect both sensing and
communications in terms of bit error rate (BER), error vector magnitude (EVM),
and radar image under varied transmit power and RCS configurations through
simulations. Along with interfernce analysis, we propose a low-complexity
successive interference cancellation method that adaptively cancels either the
monostatic reflection or the bistatic line-of-sight signal based on a
monostatic radar image signal-to-interference-plus-noise ratio (SINR). The
proposed framework is evaluated with both simulations and proof-of-concept
measurements using an ISAC testbed with a radar echo generator for object
emulation. The results have shown that the proposed method reduces BER and
improves EVM as well as radar image SINR across a wide range of SINR
conditions. These results demonstrate that accurate component-wise cancellation
can be achieved with low computational overhead, making the method suitable for
practical applications.

</details>


### [441] [Analytical Modeling of Batteryless IoT Sensors Powered by Ambient Energy Harvesting](https://arxiv.org/abs/2507.20952)
*Jimmy Fernandez Landivar,Andrea Zanella,Ihsane Gryech,Sofie Pollin,Hazem Sallouha*

Main category: eess.SP

TL;DR: 本文提出了一个用于无电池物联网节点的数学模型，可以精确模拟其能量动态。实验证明该模型是准确的。


<details>
  <summary>Details</summary>
Motivation: 对完全由环境能量收集供电的无电池物联网传感节点的能量动态进行数学建模，以精确估计设备在不同环境条件下的行为，并支持旨在最大限度地收集能量的智能电源管理单元。

Method: 提出一个全面的数学模型来描述由环境能量收集供电的无电池物联网传感节点的能量动态，该模型包含能量收集和消耗两个阶段，并明确纳入了电源管理任务。

Result: 实验结果显示，分析估算的超级电容器电压曲线与测量值高度吻合，证明了所提出模型的准确性。

Conclusion: 该模型通过与原型设备在三种不同光照条件下的实验数据进行对比，验证了其预测的准确性。

Abstract: This paper presents a comprehensive mathematical model to characterize the
energy dynamics of batteryless IoT sensor nodes powered entirely by ambient
energy harvesting. The model captures both the energy harvesting and
consumption phases, explicitly incorporating power management tasks to enable
precise estimation of device behavior across diverse environmental conditions.
The proposed model is applicable to a wide range of IoT devices and supports
intelligent power management units designed to maximize harvested energy under
fluctuating environmental conditions. We validated our model against a
prototype batteryless IoT node, conducting experiments under three distinct
illumination scenarios. Results show a strong correlation between analytical
and measured supercapacitor voltage profiles, confirming the proposed model's
accuracy.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [442] [Accelerating magnonic simulations with the pseudospectral Landau-Lifshitz equation](https://arxiv.org/abs/2507.19572)
*A. Roxburgh,M. Copus,E. Iacocca*

Main category: cond-mat.mes-hall

TL;DR: 该研究展示了如何在宏观模拟中使用 pseudospectral Landau-Lifshitz (PS-LL) 模型，通过单一卷积核模拟宏观模拟中的长程偶极子-偶极子相互作用，并将计算速度提高了一倍。


<details>
  <summary>Details</summary>
Motivation: 宏观模拟中的偶极场对于描述宏观准粒子的色散关系至关重要。宏观模拟通常使用卷积进行长程偶极子-偶极子相互作用的数值计算。

Method: 使用解析解衍生的单一卷积核，通过数值模拟来模拟长程偶极子-偶极子相互作用，并将此方法应用于宏观模拟。

Result: 与完整的偶极子计算相比，该方法将计算速度提高了一倍。

Conclusion: 该方法通过使用解析解衍生的单一卷积核，在数值上模拟了长程偶极子-偶极子相互作用，从而在数值上模拟了准粒子角动量的色散关系。

Abstract: The pseudospectral Landau-Lifshitz (PS-LL) model can describe atomic-scale
magnetic exchange interactions within a continuum framework. This is achieved
by employing a convolution kernel that models the nonlocal interaction in a
grid-independent manner. Even though the PS-LL was originally introduced to
address atomic exchange, any nonlocal kernel can be modeled. In the field of
magnonics, the dipole field is fundamental to describe the dispersion relation
of magnons, the quasiparticle representation of angular momentum. Because
dipole-dipole interactions are long-range, numerical approaches typically rely
on convolutions. Here, we demonstrate that the PS-LL model can be used to
perform magnonic simulations with a single convolution kernel derived from
analytical solutions. We demonstrate a twofold increase in computational speed
compared with the full dipole calculation. This approach is valid insofar as
the excitations are linear, which is typically the case for magnons. Our
results have the potential to accelerate magnonic research, particularly for
the inverse design method, where several simulations must be performed to
achieve the desired outcome.

</details>


### [443] [Dynamics of current-induced switching in the quantum anomalous Hall effect](https://arxiv.org/abs/2507.19665)
*Alina Rupp,Daniel Rosenbach,Torsten Röper,Dominik Hoborka,Alexey A. Taskin,Yoichi Ando,Erwann Bocquillon*

Main category: cond-mat.mes-hall

TL;DR: 研究了QAH系统中的磁性翻转动力学，发现热激活过程是关键，为操纵手征边缘态提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 研究QAH系统中的磁性动力学，探索通过热效应进行手征边缘态局部可控操纵的可能性。

Method: 通过时间分辨测量，研究了QAH系统在强电学偏压下诱导磁性反转的开关动力学。

Result: 发现磁性翻转过程是热激活的，并受限于电流脉冲期间产生的焦耳热。该过程具有无序磁性景观的特征。

Conclusion: 此项工作揭示了铁磁性拓扑绝缘体在量子反常霍尔(QAH)机制下，外加电流产生的焦耳热效应能够驱动磁性翻转，从而为翻转手征边缘态的传播方向提供了新的途径。

Abstract: Ferromagnetic topological insulators in the quantum anomalous Hall (QAH)
regime host chiral, dissipationless edge states whose propagation direction is
determined by the internal magnetization. Under suitable conditions, a strong
electrical bias can induce magnetization reversal, and thus flip the
propagation direction. In this work, we perform time-resolved measurements to
investigate the switching dynamics. Our results reveal characteristics
consistent with a disordered magnetic landscape and demonstrate that the
reversal process is thermally activated, driven by Joule heating during the
current pulse. The understanding of the magnetization dynamics in QAH systems
opens pathways for local, controlled manipulation of chiral edge states via
thermal effects.

</details>


### [444] [Current-induced Magnetoexcitons in Mesoscopic Electron-hole Plasma](https://arxiv.org/abs/2507.19678)
*Yu. A. Pusep,M. A. T. Patricio,G. M. Jacobsen,M. D. Teodoro,G. M. Gusev,A. K. Bakarov*

Main category: cond-mat.mes-hall

TL;DR: 电流重构了GaAs/AlGaAs沟道中电子-空穴等离子体的光学响应，轻空穴取代重空穴成为主导。


<details>
  <summary>Details</summary>
Motivation: 为了揭示高激发电子-空穴等离子体在量化磁场和电流下的光学行为，探索电流对等离子体光学响应的影响机制。

Method: 通过分析量化磁场下GaAs/AlGaAs沟道中电子-空穴等离子体的光学响应，研究了电流对其发射光谱的影响，并阐述了电流引起轻空穴局部积累以及形成激子和三体子的机制。

Result: 发现了临界电流导致发射光谱从主要由重空穴跃迁主导转变为轻空穴贡献占主导，并解释了这是由于电流引起的轻空穴局部积累和电子-空穴强耦合导致激子和三体子形成所致。

Conclusion: 文章发现，在量化磁场和电流下，GaAs/AlGaAs沟道中高激发电子-空穴等离子体的光学响应发生了根本性重构，表现为临界电流导致发射光谱的显著变化，轻空穴的贡献占主导地位。

Abstract: A radical restructuring of the optical response of highly excited
electron-hole plasma formed in a mesoscopic GaAs/AlGaAs channel in a quantizing
magnetic field when an electric current flows in the channel has been
discovered. In the absence of current, the emission spectra are caused by
transitions between Landau levels formed in the conduction band and in the
valence band of heavy holes. A critical electric current leads to a drastic
change in the emission spectra with a predominant contribution from light
holes. It is shown that the current causes local accumulation of light holes
due to Coulomb drag, which leads to strong electron-hole coupling and, as a
consequence, the formation of excitons and trions.

</details>


### [445] [Towards Environmentally Responsive Hypersound Materials](https://arxiv.org/abs/2507.19688)
*Edson Rafael Cardozo de Oliveira,Gastón Grosman,Chushuang Xiang,Michael Zuarez-Chamba,Priscila Vensaus,Abdelmounaim Harouri,Cédric Boissiere,Galo J. A. A. Soler-Illia,Norberto Daniel Lanzillotti-Kimura*

Main category: cond-mat.mes-hall

TL;DR: 首次在基于SiO$_{2}$介孔薄膜的开放腔纳米声学谐振器中观察到声学共振频率随湿度变化而移动，为环境响应型超声波器件提供了新途径。


<details>
  <summary>Details</summary>
Motivation: GHz范围内的声学声子工程在数据处理、传感和量子通信等领域具有技术突破的潜力。新型的纳米声学谐振器可以对外部刺激做出响应，为这些设备提供额外的控制和功能。介孔薄膜（MTFs）具有纳米级有序孔隙，能够支持GHz范围内的声学共振，并且对环境变化（如液体和蒸气渗透）敏感，可以改变其有效光学和弹性特性。

Method: 使用瞬态反射装置，研究了在不同湿度条件下声学响应。

Result: 首次观察到声学共振频率随相对湿度的变化而发生明显移动。比较了不同孔径和厚度的谐振器，发现共振频率主要受材料特性和薄膜厚度影响，而非孔径大小。

Conclusion: 提出了基于SiO$_{2}$ MTF的开放腔纳米声学谐振器，首次实现了在不同湿度条件下声学共振频率的显著移动，证明了调谐超声波限制是一种简单的方法。该设计为未来研究MTFs对液体和蒸气渗透的机械响应提供了多功能平台，为环境响应型超声波器件打开了大门。

Abstract: The engineering of acoustic phonons in the gigahertz (GHz) range holds
significant potential for technological breakthroughs in areas such as data
processing, sensing and quantum communication. Novel approaches for
nanophononic resonators responsive to external stimuli provide additional
control and functionality for these devices. Mesoporous thin films (MTFs) for
example, featuring nanoscale ordered pores, support GHz-range acoustic
resonances. These materials are sensitive to environmental changes, such as
liquid and vapor infiltration, modifying their effective optical and elastic
properties. Here, a SiO$_{2}$ MTF-based open-cavity nanoacoustic resonator is
presented, in which the MTF forms the topmost layer and is exposed to the
environment. Using a transient reflectivity setup, acoustic responses under
varying humidity conditions are investigated. A pronounced shift in acoustic
resonance frequency with changes in relative humidity is observed for the first
time, demonstrating a simple way to tune hypersound confinement. In addition,
resonators with varying pore sizes and thicknesses are compared, revealing that
resonance frequencies are primarily influenced by material properties and film
thickness, rather than pore size. The proposed open-cavity resonator design
provides a versatile platform for future studies on the mechanical response of
MTFs to liquid and vapor infiltration, opening the gate to
environment-responsive hypersound devices.

</details>


### [446] [Excitation of vortex core gyration in nanopillars through driven Floquet magnons](https://arxiv.org/abs/2507.19865)
*Gauthier Philippe,Joo-Von Kim*

Main category: cond-mat.mes-hall

TL;DR: 研究了薄膜圆盘中涡旋核的动力学，发现Floquet模式能维持涡旋核稳态回旋，且非线性相互作用可导致多重稳态回旋半径和滞后效应。


<details>
  <summary>Details</summary>
Motivation: 研究磁性薄膜圆盘中涡旋状态动力学，特别是涡旋核与方位角自旋波的相互作用如何产生Floquet态，并进一步探究Floquet模式如何维持涡旋核的稳态回旋。

Method: 本文采用理论和计算研究的方法，研究Floquet模式如何维持涡旋核的稳态回旋。

Result: 发现多种稳态回旋半径的存在，这源于涡旋核与Floquet模式之间的非线性相互作用。不同的回旋半径对应不同的Floquet频率梳状光谱，并能解释近期实验报道的滞后效应。

Conclusion: Floquet模式能够维持涡旋核的稳态回旋，多重稳态回旋半径的存在源于涡旋核与Floquet模式之间的非线性相互作用，这会导致不同的回旋半径、不同的Floquet频率梳状光谱以及近期实验报道的滞后效应。

Abstract: The dynamics of vortex states in confined geometries like thin-film disks are
characterized by a sub-GHz gyration, representing the damped oscillatory motion
of the vortex core about the disk center. It has recently been shown that
interactions between the core and azimuthal spin waves, lying in the GHz range
and driven by magnetic fields, can result in steady-state core gyration. The
gyration in turn provides a time-periodic modulation for the spin waves,
resulting in the emergence of Floquet states. Here, we present results of a
theoretical and computational study in which we examine how Floquet modes
sustain this core gyration. In particular, we find that multiple steady-state
gyration radii are possible under certain field conditions, resulting from the
nonlinear interactions between the core and Floquet modes. Different gyration
radii result in distinct Floquet frequency comb spectra and allow for
hysteretic effects, as reported in recent experiments.

</details>


### [447] [Spin-Type Photonic Topological Insulators on a Rhombic Lattice](https://arxiv.org/abs/2507.20015)
*Robert J. Davis,Daniel F. Sievenpiper*

Main category: cond-mat.mes-hall

TL;DR: 研究提出了一种简化的菱形点阵光子拓扑绝缘体模型，便于集成并具有非平凡的拓扑性质。


<details>
  <summary>Details</summary>
Motivation: 为了简化模型并便于与传统微波系统集成，采用了简化的菱形晶胞代替常用的六边形晶胞。

Method: 通过直接计算自旋投影的贝里曲率和威尔逊循环谱，以及基于Kane-Mele哈密顿量的降对称性紧束缚模型研究，验证了传输的非平凡性质。

Result: 研究展示了该模型具有非平凡的拓扑性质，并通过器件实现展示了多种非平凡的配置。

Conclusion: 该研究提出了一个简化的金属自旋类型光子拓扑绝缘体模型，该模型基于菱形点阵，具有更简化的结构和更易于与传统微波系统集成的优点。

Abstract: A simplified model of a metallic spin-type photonic topological insulator on
a rhombic lattice is presented and analyzed. Instead of the more commonly used
hexagonal unit cells, a reduced symmetry rhombus is employed, which is both
simpler and allows for easier integration into more traditional microwave
systems. The non-trivial nature of the transport is shown via direct
calculation of the structure's spin-projected Berry curvature and Wilson loop
spectra, as well as by a systematic investigation of a reduced symmetry
tight-binding model based off the Kane-Mele Hamiltonian. Device implementations
are shown for a range of non-trivial configurations.

</details>


### [448] [Time-bin qubit architecture using quantum Hall edge channels](https://arxiv.org/abs/2507.20192)
*David Pomaranski,Michihisa Yamamoto*

Main category: cond-mat.mes-hall

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present the basic elements for a modular architecture for time-bin encoded
qubits based on quantum Hall edge channels, forming the foundation of a
scalable electronic quantum information platform named TEMPO (Time-binned
Electronic Modular Platform for Qubits). Quantum states are encoded in
temporally separated edge magnetoplasmon (EMP) wave packets propagating along a
single chiral edge, eliminating the need for spatial path separation and
enhancing coherence. The platform supports full qubit
operations$\unicode{x2013}$including initialization, phase modulation, readout,
and two-qubit entangling gates$\unicode{x2013}$by leveraging dynamically
tunable quantum point contacts and electrostatic control of interferometric
loops. We consider the linear dispersion and gate-induced velocity control on
EMP propagation and describe strategies for maintaining waveform integrity.
Various single-electron sources, including ohmic injection and capacitive
excitation, are discussed in the context of coherence. Multi-qubit operations
are enabled through synchronized injection and engineered Coulomb interactions
between adjacent channels, while single-qubit readout is addressed via
spin-based or capacitive charge sensors. Our approach integrates gate-tunable
coherent control of chiral edge states, offering a comprehensive pathway toward
scalable electron quantum optics in solid-state platforms.

</details>


### [449] [Electron transport through mesoscopic junctions revisited](https://arxiv.org/abs/2507.20238)
*Robert Alicki*

Main category: cond-mat.mes-hall

TL;DR: Electron transport in mesoscopic systems requires considering electrostatic interactions. Active transport can be driven by external energy sources.


<details>
  <summary>Details</summary>
Motivation: Noninteracting electrons model is insufficient to describe passive transport and generation of electromotive force (active transport).

Method: Revisiting theoretical foundations like Landauer theory, Master equations, and Onsager linear thermodynamics, and proposing phenomenological modifications for passive transport.

Result: Electrostatic interactions creating double layers and surface charge distributions are crucial. Active transport can be generated by a self-oscillating double layer driven by an external energy source.

Conclusion: Need to consider 2-body electrostatic interactions for electron transport in mesoscopic systems, not just noninteracting electrons model.

Abstract: Theoretical foundations of electron transport in mesoscopic systems, based on
Landauer theory, Master equations or Onsager linear thermodynamics, are
revisited to show that the noniteracting electrons model is insufficient to
describe neither passive transport, nor generation of electromotive force
(active transport). It is argued that 2-body electrostatic interactions
creating double layers and surface charge distributions are crucial for the
electron transport through a junction. Phenomenological modifications of the
passive transport formulas based on the carefull analysis of the fundamental
notions of chemical, electrostatic, electrochemical, build-in potentials, band
bending and bias voltage, are proposed. On the other hand active transport can
be generated by a self-oscillating double layer (a pump ) driven by an external
heat, light or chemical energy source.

</details>


### [450] [Spectral shadows of a single GaAs quantum dot](https://arxiv.org/abs/2507.20290)
*Kai Hühn,Lena Klar,Fei Ding,Arne Ludwig,Andreas D. Wieck,Jens Hübner,Michael Oestreich*

Main category: cond-mat.mes-hall

TL;DR: 本研究深入分析了GaAs量子点的电荷状态和环境影响，通过时间分辨测量揭示了光谱重影的成因，并提出通过非共振激光增强其光子生成性能的方法。


<details>
  <summary>Details</summary>
Motivation: 为了克服半导体量子点在生成单光子和纠缠光子时受电荷状态和环境影响的限制，本研究旨在深入理解和量化这些环境效应。

Method: 通过详细的时间分辨共振荧光测量和二阶非共振激光诱导的测量，结合自旋噪声光谱测量，研究了单电荷可调GaAs量子点的光谱特性。

Result: 发现多个与稀有光谱跃迁相关的斯塔克位移共振，并观察到不同电荷状态（中性激子、负三体、正三体、双负三体）的斯塔克位移存在差异。量化了从亚毫秒到秒级的杂质电荷动力学，特别是正三体跃迁的空穴占据受空穴损失和空穴捕获动力学限制。通过二阶激光干预，实现了空穴占据的提升，并观察到延长的空穴驻留时间和增强的空穴隧穿率。

Conclusion: 该研究揭示了GaAs量子点中由杂质环境引起的光谱重影，并量化了其底层的杂质电荷动力学。

Abstract: Semiconductor quantum dots are a promising platform for generating single and
entangled photons.Still, their use is limited even in the most advanced
structures by changes in the charge state of the quantum dot and its
environment. Here, we present detailed time-resolved resonance fluorescence
measurements on a single charge-tunable GaAs quantum dot, shedding new light on
the spectral shadows invoked by the complex impurity environment.
Detuning-dependent measurements reveal the existence of multiple Stark-shifted
resonances, which are associated with rare spectral jumps smaller than the
homogeneous linewidth and, therefore, typically concealed in the measurement
noise. We observe similar environmentally induced Stark shifts for both the
neutral exciton and negatively charged trion transitions, while the positively
and doubly negatively charged trions exhibit significant differences. Our
investigation quantifies the underlying impurity charge dynamics over a range
from well below milliseconds to seconds, revealing that the hole occupation of
the positively charged trion transition is constrained by rapid hole loss and
slow hole recapture dynamics. Utilizing a second non-resonant laser, we
increase the hole occupancy by over an order of magnitude and identify both a
prolonged hole residence time and an enhanced hole tunneling rate into the
quantum dot. These findings are supported by complementary spin noise
spectroscopy measurements, which offer a significantly higher bandwidth
compared to the time-resolved resonance fluorescence measurements.

</details>


### [451] [Angle-dependent chiral tunneling in biased twisted bilayer graphene](https://arxiv.org/abs/2507.20391)
*Nadia Benlakhouy,El Mustapha Feddi,Abdelouahed El Fatimy*

Main category: cond-mat.mes-hall

TL;DR: 研究了扭转双层石墨烯（TBLG）中层间偏压对电子隧穿的影响，发现偏压会打开带隙，抑制低能隧穿，并在角度分布上产生非对称性，同时调节法布里-珀罗共振。


<details>
  <summary>Details</summary>
Motivation: 研究垂直层间偏压对扭转双层石墨烯（TBLG）中静电势垒电子隧穿的影响。

Method: 使用双栅模型，该模型独立控制载流子密度和层间电势差，计算了在不同角度和能量下，代表性扭转角 $	heta = 1.8^{\circ}$、3.89° 和 9.43° 下电子的透射和反射概率。

Result: 发现适度的偏压通过在低能光谱中打开带隙来抑制法向入射隧穿，导致在低能量下接近全反射，而在带隙上方由于依赖于扭转的导电通道开始出现透射。施加的偏压会破坏系统的有效反转对称性，导致透射电子的角度分布出现明显的依赖于方向和谷的非对称性。此外，层间偏压调节了TBLG势垒中类似法布里-珀罗的共振，改变了透射峰的能量及其强度。

Conclusion: 与单层和伯纳尔双层石墨烯不同，在扭转双层石墨烯（TBLG）中，可以通过扭转角、势垒高度和费米能量等参数来调节手性隧穿。这项研究发现了垂直层间偏压对TBLG中静电势垒电子隧穿的影响。适度的偏压通过在低能光谱中打开带隙来抑制法向入射隧穿，导致在低能量下接近全反射，而在带隙上方由于依赖于扭转的导电通道开始出现透射。施加的偏压会破坏系统的有效反转对称性，导致透射电子的角度分布出现明显的依赖于方向和谷的非对称性。研究还表明，以不同角度入射的电子在偏压下表现出显著的透射变化。此外，层间偏压调节了TBLG势垒中类似法布里-珀罗的共振，改变了透射峰的能量及其强度。

Abstract: In twisted bilayer graphene (TBLG), chiral tunneling can be tuned by
parameters such as the twist angle, barrier height, and Fermi energy. This
differs from the tunneling behavior observed in monolayer and Bernal bilayer
graphene, where electrons either pass completely through or are fully blocked
due to the Klein paradox. Here we investigate the effect of a perpendicular
interlayer bias on electron tunneling through electrostatic barriers in TBLG.
Using a dual-gated model, which controls the carrier density and interlayer
potential difference independently, we compute the transmission and reflection
probabilities of electrons at different angles and energies for representative
twist angles of $\theta = 1.8^{\circ}$, $3.89^{\circ}$, and $9.43^{\circ}$. We
find that a moderate bias suppresses normal-incidence transmission by opening a
band gap in the low-energy spectrum. Our results show this leads to near-total
reflection at low energy, with transmission starting to increase just above the
gap due to twist-dependent conducting channels. The applied bias breaks the
system's effective inversion symmetry, resulting in pronounced
direction-dependent and valley-specific asymmetries in the angular distribution
of transmitted electrons. We show that electrons incident at different angles
show notable variations in transmission under bias. Furthermore, interlayer
bias modulates Fabry--P\'{e}rot--like resonances in the TBLG barrier, shifting
the energies of transmission peaks and altering their intensity.

</details>


### [452] [Characterizing local Majorana properties using Andreev states](https://arxiv.org/abs/2507.20591)
*Miguel Alvarado,Alfredo Levy Yetati,Ramón Aguado,Rubén Seoane Souto*

Main category: cond-mat.mes-hall

TL;DR: 使用ABS作为探针，通过隧穿电导测量来表征Majorana零模。


<details>
  <summary>Details</summary>
Motivation: 为了表征量子点最小的Kitaev链中的Majorana零模（MZM）。

Method: 提出使用安德烈夫束缚态（ABS）作为光谱探针，通过量子点最小的Kitaev链来表征Majorana零模（MZM）。

Result: 通过在具有ABS的超导探针中进行隧穿电导测量，揭示了四种亚带隙峰，其电压位置和相对高度能够提取MZM能量分裂和Bogoliubov-de Gennes相干因子。

Conclusion: 该方法与现有实验兼容，并且在扩展链中保持稳健。

Abstract: We propose using Andreev bound states (ABS) as spectroscopic probes to
characterize Majorana zero modes (MZMs) in quantum-dot based minimal Kitaev
chains. Specifically, we show that tunneling conductance measurements with a
superconducting probe hosting an ABS reveal four subgap peaks whose voltage
positions and relative heights enable extraction of the MZM energy splitting
and Bogoliubov-de Gennes coherence factors. This provides direct access to
zero-splitting regimes and to the local Majorana polarization - a measure of
the Majorana character. The method is compatible with existing experimental
architectures and remains robust in extended chains.

</details>


### [453] [Ultrafast transition from coherent to incoherent polariton nonlinearities in a hybrid 1L-WS2/plasmon structure](https://arxiv.org/abs/2507.20633)
*Daniel Timmer,Moritz Gittinger,Thomas Quenzel,Alisson R. Cadore,Barbara L. T. Rosa,Wenshan Li,Giancarlo Soavi,Daniel C. Lünemann,Sven Stephan,Lara Greten,Marten Richter,Andreas Knorr,Antonietta De Sio,Martin Silies,Giulio Cerullo,Andrea C. Ferrari,Christoph Lienau*

Main category: cond-mat.mes-hall

TL;DR: 通过 2DES 技术，我们发现 WS2/等离激元纳米结构中的激子极化激子具有显著的非线性效应，并在 70 飞秒内发生动力学演变，这在超快全光开关应用中具有潜力。


<details>
  <summary>Details</summary>
Motivation: 探究在金属纳米谐振器中，由于耗散现象限制了极化激子的寿命（仅几十飞秒），尽管其具有显著的非线性效应，但其在混合结构非线性效应中的作用尚未得到探索。

Method: 利用超快二维电子光谱（2DES）技术。

Result: 与未耦合的 WS2 薄片相比，观察到光学非线性增强超过 20 倍，且与偏振相关；2DES 谱在约 70 飞秒内快速演变，表明从相干极化激子到非相干激发的转变。

Conclusion: 本研究揭示了原子级薄半导体激子极化激子在混合单层 WS2/等离激元纳米结构中的相干动力学，并强调了其在超快全光开关中的应用潜力。

Abstract: Exciton polaritons based on atomically thin semiconductors are essential
building blocks of quantum optoelectronic devices. Their properties are
governed by an ultrafast and oscillatory energy transfer between their
excitonic and photonic constituents, resulting in the formation of polaritonic
quasiparticles with pronounced nonlinearities induced by the excitonic
component. In metallic nanoresonators, dissipation phenomena limit the
polariton lifetime to a few ten femtoseconds, so short that the role of these
polaritons for the nonlinearities of such hybrids is yet unexplored. Here, we
use ultrafast two-dimensional electronic spectroscopy (2DES) to uncover
coherent polariton dynamics in a hybrid monolayer (1L) WS2/plasmonic
nanostructure. With respect to an uncoupled WS2 flake, we observe an over
20-fold, polarization-dependent enhancement of the optical nonlinearity and a
rapid evolution of the 2DES spectra within ~70 fs. We relate these dynamics to
a transition from coherent polaritons to incoherent excitations, unravel the
microscopic optical nonlinearities, and show the potential of coherent
polaritons for ultrafast all-optical switching.

</details>


### [454] [hBN alignment orientation controls moiré strength in rhombohedral graphene](https://arxiv.org/abs/2507.20647)
*Matan Uzan,Weifeng Zhi,Matan Bocarsly,Junkai Dong,Surajit Dutta,Nadav Auerbach,Niladri Sekhar Kander,Mikhail Labendik,Yuri Myasoedov,Martin E. Huber,Kenji Watanabe,Takashi Taniguchi,Daniel E. Parker,Eli Zeldov*

Main category: cond-mat.mes-hall

TL;DR: hBN对齐方向是控制摩尔纹石墨烯相关相图的关键因素，影响其对称性破缺态序列。


<details>
  <summary>Details</summary>
Motivation: 探索摩尔纹工程石墨烯系统中丰富的关联对称性破缺相，特别是hBN对齐方向对量子反常霍尔效应等的影响。

Method: 通过低温输运和扫描SQUID-on-tip磁力测量，比较了仅在对齐方向上不同的器件，并进行了基于晶格弛豫和原子尺度电子结构的理论分析。

Result: 不同的hBN对齐方向导致不同的摩尔势强度，进而引起截然不同的对称性破缺态序列。

Conclusion: hBN对齐方向是控制摩尔纹工程石墨烯系统中相关相图的关键参数，为解释实验提供了理论框架。

Abstract: Rhombohedral multilayer graphene hosts a rich landscape of correlated
symmetry-broken phases, driven by strong interactions from its flat band edges.
Aligning to hexagonal boron nitride (hBN) creates a moir\'e pattern, leading to
recent observations of exotic ground states such as integer and fractional
quantum anomalous Hall effects. Here, we show that the moir\'e effects and
resulting correlated phase diagrams are critically influenced by a previously
underestimated structural choice: the hBN alignment orientation. This binary
parameter distinguishes between configurations where the rhombohedral graphene
and hBN lattices are aligned near 0{\deg} or 180{\deg}, a distinction that
arises only because both materials break inversion symmetry. Although the two
orientations produce the same moir\'e wavelength, we find their distinct local
stacking configurations result in markedly different moir\'e potential
strengths. Using low-temperature transport and scanning SQUID-on-tip
magnetometry, we compare nearly identical devices that differ only in alignment
orientation and observe sharply contrasting sequences of symmetry-broken
states. Theoretical analysis reveals a simple mechanism based on lattice
relaxation and the atomic-scale electronic structure of rhombohedral graphene,
supported by detailed modeling. These findings establish hBN alignment
orientation as a key control parameter in moir\'e-engineered graphene systems
and provide a framework for interpreting both prior and future experiments.

</details>


### [455] [Tunneling Dynamics and Time Delay in Electron Transport through Time-Dependent Barriers with Finite-Bandwidth Reservoirs](https://arxiv.org/abs/2507.20649)
*Shmuel Gurvitz,Dmitri Sokolovski*

Main category: cond-mat.mes-hall

TL;DR: 本研究提出了一种清晰且可解析地处理时间依赖性电子隧穿势垒问题的方法。通过单电子方法，我们推导了电流的表达式，并将势垒调制与相位偏移联系起来，这为定义隧穿时间提供了一种直观的方法。研究结果表明，在非马尔可夫环境中，隧穿时间是有限的。


<details>
  <summary>Details</summary>
Motivation: 研究时间依赖性隧穿势垒中的电子传输问题，特别是探索势垒调制与稳态电流中可测量的相位偏移之间的关系。

Method: 使用单电子方法，通过时间依赖性隧穿哈密顿量框架，推导了绝热和非绝热状态下时间依赖性隧穿电流的简单表达式。

Result: 在马尔可夫极限（宽带势垒）下，恢复了已知的隧穿时间消失的结果；对于有限带宽的势垒，预测了由反带宽给出的有限时间延迟。

Conclusion: 该研究为非马尔可夫环境中的隧穿动力学提供了一个稳健的基础，并可能作为涉及可调带结构实验研究的基准。

Abstract: We present a transparent and analytically tractable approach to the problem
of time-dependent electron transport through tunneling barriers. Using the
Single-Electron Approach, we study a model system composed of a time-dependent
tunneling barrier coupled to two reservoirs of finite bandwidth. Avoiding
Floquet expansions, we derive simple expressions for the time-dependent
tunneling current in both adiabatic and non-adiabatic regimes. Our formulation,
based on the tunneling Hamiltonian framework, relates barrier modulation to
measurable phase shifts in the steady-state current, offering a physically
intuitive definition of the tunneling (or traversal) time. Remarkably, in the
Markovian limit (wide-band reservoirs), we recover the well-known result of
vanishing tunneling time. In contrast, for finite-bandwidth leads, we predict a
finite time delay given by the inverse bandwidth. Our findings provide a robust
foundation for understanding tunneling dynamics in non-Markovian environments
and may serve as a benchmark for experimental investigations involving tunable
band structures.

</details>


### [456] [Flat-band projected versus fully atomistic twisted bilayer graphene](https://arxiv.org/abs/2507.20675)
*Miguel Sánchez Sánchez,Tobias Stauber*

Main category: cond-mat.mes-hall

TL;DR: 新投影方法在MATBG中表现良好，与全模型结果高度一致，并引入了新的序参量用于表征系统。


<details>
  <summary>Details</summary>
Motivation: 为了评估近期提出的投影方法在魔角扭曲双层石墨烯（MATBG）中的适用性，特别是在各种对称性破缺相下，并引入新的工具来更好地理解其性质。

Method: 通过基准测试评估了新提出的投影方法在魔角扭曲双层石墨烯（MATBG）中的表现，并引入了一套新颖的序参量来可视化波函数和量化对称性破缺。

Result: 投影方法得到的平带投影解与全紧束缚模型结果吻合良好，能带结构和总能量差异仅为几 meV。新提出的序参量可用于表征MATBG和通用的蜂窝系统。

Conclusion: 该研究通过基准测试验证了投影方法在魔角扭曲双层石墨烯（MATBG）的各种对称性破缺相中的有效性，结果表明该方法在能带结构和总能量上与全紧束缚模型高度一致，误差仅在几 meV。

Abstract: We benchmark the recently proposed projection method [Phys. Rev. B 111,
205133 (2025)] for magic-angle twisted bilayer graphene (MATBG) across various
symmetry-breaking phases at charge neutrality. The flat-band projected
solutions agree well with the full tight-binding, with band structures and
total energies differing by only a few meV. The projection to the flat bands is
justified, owing to the increased gap to the remote bands in the normal state.
Moreover, we employ a novel set of order parameters that allow us to visualize
the wave functions locally in real space and quantify the breaking of various
symmetries in the correlated phases. These order parameters are suitable to
characterize MATBG and generic honeycomb systems.

</details>


### [457] [Cascade of Even-Denominator Fractional Quantum Hall States in Mixed-Stacked Multilayer Graphene](https://arxiv.org/abs/2507.20695)
*Yating Sha,Kai Liu,Chenxin Jiang,Dan Ye,Shuhan Liu,Zhongxun Guo,Jingjing Gao,Ming Tian,Neng Wan,Kenji Watanabe,Takashi Taniguchi,Bingbing Tong,Guangtong Liu,Li Lu,Yuanbo Zhang,Zhiwen Shi,Zixiang Hu,Guorui Chen*

Main category: cond-mat.mes-hall

TL;DR: Graphene multilayer shows exotic quantum states with tunable properties, non-Abelian properties.


<details>
  <summary>Details</summary>
Motivation: To find experimental platforms offering large energy gaps, delicate tunability, and robust non-Abelian signatures for studying the FQHE, particularly at half-filling.

Method: The study observed FQH states in mixed-stacked pentalayer graphene using experimental measurements and numerical calculations, including spectral analysis.

Result: Observation of even-denominator FQH states at $\nu$ = -5/2, -7/2, -9/2, -11/2, and -13/2, along with odd-denominator states. These states exhibit unprecedented displacement field tunability and display characteristics of the non-Abelian Moore-Read type.

Conclusion: Mixed-stacked multilayer graphene is a versatile platform for exploring tunable correlated topological phases, exhibiting a cascade of even-denominator FQH states with non-Abelian Moore-Read type characteristics.

Abstract: The fractional quantum Hall effect (FQHE), particularly at half-filling of
Landau levels, provides a unique window into topological phases hosting
non-Abelian excitations. However, experimental platforms simultaneously
offering large energy gaps, delicate tunability, and robust non-Abelian
signatures remain scarce. Here, we report the observation of a cascade of
even-denominator FQH states at filling factors ${\nu}$ = ${-5/2}$, ${-7/2}$,
${-9/2}$, ${-11/2}$, and ${-13/2}$, alongside numerous odd-denominator states
in mixed-stacked pentalayer graphene, a previously unexplored system
characterized by intertwined quadratic and cubic band dispersions. These
even-denominator states, representing the highest filling half-filled states
reported so far in the zeroth Landau level (ZLL), emerge from two distinct
intra-ZLL and exhibit unprecedented displacement field tunability driven by LL
crossings in the hybridized multiband structure. At half fillings, continuous
quasiparticle phase transitions between paired FQH states, magnetic Bloch
states, and composite Fermi liquids are clearly identified upon tuning external
fields. Numerical calculations, revealing characteristic sixfold ground-state
degeneracy and chiral graviton spectral analysis, suggest the observed
even-denominator FQH states belong to the non-Abelian Moore-Read type. These
results establish mixed-stacked multilayer graphene as a rich and versatile
crystalline platform for exploring tunable correlated topological phases.

</details>


### [458] [Measuring coherence factors of states in superconductors through local current](https://arxiv.org/abs/2507.20696)
*Rodrigo A. Dourado,Jeroen Danon,Martin Leijnse,Rubén Seoane Souto*

Main category: cond-mat.mes-hall

TL;DR: 提出一种利用局部输运测量超导体中准粒子的局部相干因子的方法，并成功应用于 Kitaev 链以量化 Majorana 态。


<details>
  <summary>Details</summary>
Motivation: 为了确定准粒子的相干因子，这决定了它们在超导体中的性质，包括输运和对电场的敏感性。

Method: 通过测量局部电流随第二个耦合的变化来推断局部相干因子，当存在局部电子-空穴对称性时，电流形状与第二个耦合无关。

Result: 在 Kitaev 链中，局部电流可以用来估计局域 Majorana 极化（MP），该 MP 是衡量状态局域 Majorana 性质的指标。研究推导了 MP 的解析表达式，并通过数值计算进行了验证。

Conclusion: 该研究提出了一种使用局部输运来推断超导体中准粒子的局部相干因子（coherence factors）的方法，并将其应用于 Kitaev 链，以量化评估 Majorana 态的质量。

Abstract: The coherence factors of quasiparticles in a superconductor determine their
properties, including transport and susceptibility to electric fields. In this
work, we propose a way to infer the local coherence factors using local
transport to normal leads. Our method is based on measuring the local current
through a lead as the coupling to a second one is varied: the shape of the
current is determined by the ratio between the local coherence factors,
becoming independent of the coupling to the second lead in the presence of
local electron-hole symmetry, {\it i.e.} coherence factors $|u|=|v|$. We apply
our method to minimal Kitaev chains: arrays of quantum dots coupled via narrow
superconducting segments. These chains feature Majorana-like quasiparticles
(zero-energy states with $|u|=|v|$) at discrete points in parameter space. We
demonstrate that the local current allows us to estimate the local Majorana
polarization (MP) -- a measurement of the local Majorana properties of the
state. We derive an analytical expression for the MP in terms of local currents
and benchmark it against numerical calculations for 2- and 3-sites chains that
include a finite Zeeman field and electron-electron interactions. These results
provide a way to quantitatively assess the quality of Majorana states in short
Kitaev chains.

</details>


### [459] [Theory of off-diagonal disorder in multilayer topological insulator](https://arxiv.org/abs/2507.20713)
*Z. Z. Alisultanov,A. Kudlis*

Main category: cond-mat.mes-hall

TL;DR: 研究了含离对角无序的多层拓扑绝缘体，发现缺陷能级穿越行为可作为拓扑标记，并分析了不同无序对不同拓扑相的鲁棒性及边缘态行为，解释了实验中霍尔平台和纵向电导的偏差。


<details>
  <summary>Details</summary>
Motivation: 为了研究多层拓扑绝缘体中随机层间隧穿（即离对角无序）对材料性质的影响，特别是其作为拓扑标记的潜力，以及无序对不同拓扑相（包括正常相、拓扑相、Weyl相和异常量子霍尔相）的鲁棒性。

Method: 本研究采用了两种互补的图解方法来计算态密度，并分析了边缘模式在不同类型无序下的行为，包括局域化长度的变化以及边缘态的隧穿效应。

Result: 研究发现，非厄米缺陷产生的边界态能级在平凡相中穿过零能，而在拓扑相中不穿过零能，可作为拓扑的局域标记。离对角无序会在能隙中引入体态并可能闭合能隙，其中Weyl相对强无序具有鲁棒性，而异常量子霍尔相仅在弱涨落下存在。此外，研究还发现均匀无序会略微缩短边缘态的局域化长度，而高斯和洛伦兹无序会扩大局域化长度，高斯无序甚至可能导致边缘态非局域化，同时边缘态的重叠增强可能导致纵向电导偏离其量子化值。

Conclusion: 该研究提出的缺陷模型及其在不同维度和拓扑相中的行为，为理解和区分拓扑绝缘体提供了新的视角。研究结果还解释了实验中观察到的霍尔平台尺寸变化和纵向电导偏差的现象，并为未来实验设计提供了参考。

Abstract: We study multilayer topological insulators with random interlayer tunnelling,
known as off-diagonal disorder. Within the Burkov-Balents model a single
Hermitian defect creates a bound state whose energy crosses the middle of the
gap in the trivial phase but never in the topological phase; a non-Hermitian
defect splits this level yet preserves the same crossing rule, so the effect
serves as a local marker of topology. However, the key distinction persists:
the bound state crosses zero in the trivial phase but not in the topological
phase. Two complementary diagrammatic approaches give matching densities of
states for the normal, topological, Weyl and anomalous quantum Hall regimes.
Off diagonal disorder inserts bulk states into the gap and can close it: the
Weyl phase remains robust under strong disorder, whereas the anomalous quantum
Hall phase survives only for weak fluctuations, and the added bulk states
shrink the Hall plateau, clarifying experimental deviations. Finally, we
analyse edge modes. Uniform disorder shortens their localization length
slightly, while Gaussian and Lorentzian disorder enlarge it and in the Gaussian
case can even delocalize the edges. Although chirality is maintained, the
enhanced overlap permits tunnelling between opposite edges and pulls the
longitudinal conductance away from its quantized value.

</details>


### [460] [Electric-field control of two-dimensional ferromagnetic properties by chiral ionic gating](https://arxiv.org/abs/2507.20723)
*Hideki Matsuoka,Amaki Moriyama,Tomohiro Hori,Yoshinori Tokura,Yoshihiro Iwasa,Shu Seki,Masayuki Suda,Naoya Kanazawa*

Main category: cond-mat.mes-hall

TL;DR: 手性离子液体可以用来电场控制二维铁磁性，并且手性离子可以以依赖手性的方式影响磁畴。


<details>
  <summary>Details</summary>
Motivation: 手性分子系统提供了超越常规对称性操作来控制自旋和磁性的独特途径。

Method: 通过电双层晶体管 (EDLT) 门控，利用手性离子液体调节 FeSi(111) 薄膜中的二维铁磁性。

Result: 手性离子门控以依赖手性的方式影响了磁畴的比例，证明了手性诱导的对称性破缺。非手性离子和手性离子都能调节磁特性，如异常霍尔电导率和矫顽场。

Conclusion: 本研究将手性离子门控确立为一种控制磁顺序的新策略，并为手性自旋电子学开辟了新的方向。

Abstract: Chiral molecular systems offer unique pathways to control spin and magnetism
beyond conventional symmetry operations. Here, we demonstrate that chiral ionic
liquids enable electric-field modulation of two-dimensional (2D) ferromagnetism
in FeSi(111) thin films via electric double-layer transistor (EDLT) gating.
FeSi hosts chemically-stable, surface-confined ferromagnetism without bulk
moments, making the interfacial spins highly responsive to chiral-ion
adsorption. Using both achiral and chiral ionic liquids, we systematically
compare electrochemical and electrostatic gating effects. While both gating
modes modulate magnetic properties such as anomalous Hall conductivity and
coercive field, only chiral ionic gating biases the ratio of up- and
down-magnetized domains in a handedness-dependent manner, evidencing
chirality-induced symmetry breaking. This work establishes chiral ion gating as
a novel strategy for controlling magnetic order and opens new directions for
chiral spintronics.

</details>


### [461] [Near-field focusing and amplification of tip-substrate radiative heat transfer](https://arxiv.org/abs/2507.20760)
*Milo Vescovo,Philippe Ben-Abdallah,Riccardo Messina*

Main category: cond-mat.mes-hall

TL;DR: 通过在基底上加一层薄膜，可以控制纳米探针和基底之间的热传递，实现热交换的增强和聚焦。


<details>
  <summary>Details</summary>
Motivation: 研究工作旨在探索在纳米尺度上控制热交换的机制，特别是通过引入极性薄膜来调控近场辐射热传递。

Method: 在偶极子近似下的散热电动力学框架内，研究了纳米探针与基底之间空间分辨的近场辐射热传递。

Result: 结果表明，在非分散性基底上方的极性薄膜能够增强并侧向聚焦热交换。探针-基底分离、薄膜厚度和基底介电常数都会影响该效应，其根源在于薄膜改变了电磁模式色散以及与距离相关的耦合强度。

Conclusion: 该研究揭示了在纳米尺度上实现对局域热辐射的相干控制的可行途径。

Abstract: The spatially resolved near-field radiative heat transfer between a nanoscale
probe and a substrate is studied in the fluctuational electrodynamics framework
within the dipolar approximation. It is shown that the introduction of a thin
polar film atop a non-dispersive substrate can lead to both an enhancement and
a lateral focusing of the heat exchange. The influence of the probe--substrate
separation, film thickness and substrate permittivity is analyzed, revealing
that the effect originates from near-field interactions governed by the
interplay between film-induced modifications of electromagnetic mode dispersion
and the distance-dependent coupling strength. The results highlight a viable
route toward the active control of local radiative heat transfer at the
nanoscale.

</details>


### [462] [Nonequilibrium transport through an interacting monitored quantum dot](https://arxiv.org/abs/2507.20779)
*Daniel Werner,Matthieu Vanhoecke,Marco Schirò,Enrico Arrigoni*

Main category: cond-mat.mes-hall

TL;DR: The paper investigates how dephasing affects the Kondo physics in a quantum dot. It finds that charge dephasing has a limited impact, while spin dephasing is detrimental to the Kondo state. The study also reveals a universal scaling behavior in the conductance that depends on the dephasing strength.


<details>
  <summary>Details</summary>
Motivation: The paper studies the interplay between strong correlations and Markovian dephasing in a quantum dot described by a dissipative Anderson impurity model.

Method: Auxiliary master equation approach is used to compute the steady-state spectral function and occupation of the dot. A two-lead setup is considered to compute the steady-state current and conductance.

Result: The study discusses the role of dephasing on Kondo physics and shows that the Kondo steady-state is robust to moderate charge dephasing but not to spin dephasing.

Conclusion: Kondo steady-state is robust to moderate charge dephasing but not to spin dephasing, which is interpreted in terms of dephasing-induced heating of low-energy excitations. Universal scaling collapse of the non-linear conductance with a dephasing-dependent Kondo scale is shown.

Abstract: We study the interplay between strong correlations and Markovian dephasing,
resulting from monitoring the charge or spin degrees of freedom of a quantum
dot described by a dissipative Anderson impurity model. Using the Auxiliary
master equation approach we compute the steady-state spectral function and
occupation of the dot and discuss the role of dephasing on Kondo physics.
Furthermore, we consider a two-lead setup which allows to compute the
steady-state current and conductance. We show that the Kondo steady-state is
robust to moderate charge dephasing but not to spin dephasing, which we
interpret in terms of dephasing-induced heating of low-energy excitations.
Finally, we show universal scaling collapse of the non-linear conductance with
a dephasing-dependent Kondo scale.

</details>


### [463] [Anomalous Scaling Behaviors of the Green's Function in Critical Skin Effects](https://arxiv.org/abs/2507.20843)
*Yifei Yi,Zhesen Yang*

Main category: cond-mat.mes-hall

TL;DR: 临界NHSE系统中的格林函数行为：即使在微小耦合下，链间耦合也会改变格林函数行为，尤其是在异常区域，GBZ理论难以完全解释边界效应。


<details>
  <summary>Details</summary>
Motivation: 研究非厄米系统中临界非厄米皮肤效应下的格林函数。

Method: 通过一个具有链间耦合$
u$的双链Hatano-Nelson模型来研究临界非厄米皮肤效应（critical NHSE）下的格林函数。

Result: 在$
u=0$时，格林函数遵循基于GBZ理论的模式。当$
u$很小时，在具有平凡OBC谱缠绕数的常规区域，链间耦合导致格林函数出现锯齿状标度结构。在具有非平凡缠绕数的异常区域，格林函数在体块部分与GBZ预测一致，但在边界附近发散，其中$
u=0$时的GBZ残余贡献解释了偏差。

Conclusion: 该研究揭示了临界非厄米皮肤效应的独特非微扰特征，并强调了GBZ理论在捕捉有限尺寸和边界效应方面的局限性，突显了在临界NHSE系统中同时考虑宏观和边界动力学的必要性。

Abstract: We study the Green's functions in non-Hermitian systems exhibiting the
critical non-Hermitian skin effect (critical NHSE) using a double-chain
Hatano-Nelson model with inter-chain coupling $\Delta$. For $\Delta=0$, the
system decouples into two independent chains, and the Green's functions follow
predictable patterns based on the GBZ theory. For small $\Delta$
($\Delta=1/10000$), in conventional regions with trivial OBC spectral winding
numbers, inter-chain coupling induces a zigzag scaling structure in Green's
functions due to competition between the two chains, explainable by first-order
perturbation theory. In anomalous regions with non-trivial winding numbers,
Green's functions match GBZ predictions in the bulk but diverge near
boundaries, with residual contributions from the $\Delta=0$ GBZ accounting for
the deviations. These results reveal the unique non-perturbative features of
critical NHSE and highlight the limitations of GBZ theory in capturing
finite-size and boundary effects, emphasizing the need to consider both bulk
and boundary dynamics in such systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [464] [Computing Longitudinal Dynamic Derivatives of a VTOL Aircraft Using CFD Simulations and Forced-Oscillation Model](https://arxiv.org/abs/2507.19509)
*Ali Khosravani Nezhad,AmirReza Kosari,Rasoul Askari*

Main category: eess.SY

TL;DR: 本研究使用CFD和振荡测试评估了飞机过渡阶段的动导数，为控制设计和性能优化提供了见解。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于理解和评估飞机在从垂直悬停到前向巡航的过渡阶段的动态气动特性，以支持城市空中交通和航空航天工程的控制设计和稳定性分析。

Method: 本研究采用了先进的CFD模拟和强制振荡测试技术，并结合了网格敏感性研究和多区域网格划分策略，以确保模拟的准确性。采用ANSYS Fluent有限体积求解器和耦合压力速度算法进行高保真结果的计算。通过改变迎角、飞行路径和旋转运动来推导动导数，并通过实验和数值数据进行验证。

Result: 研究结果为飞机在过渡阶段的动导数提供了宝贵的见解，验证了CFD模拟和强制振荡测试的有效性，为未来飞机性能优化和设计创新奠定了基础。

Conclusion: 本研究通过先进的计算流体动力学(CFD)模拟和强制振荡测试，对飞机过渡阶段的动导数进行了全面评估，为城市空中交通和航空航天工程的未来发展提供了宝贵的见解，并为优化飞机在关键过渡阶段的性能展示了巨大的潜力。

Abstract: This study presents a comprehensive evaluation of dynamic aerodynamic
derivatives during aircraft transition phases using advanced CFD simulations
and forced oscillation testing. Two case studies are examined: a three
dimensional fighter aircraft (Standard Dynamic Model, SDM) and a UT24 eVTOL
model. The transition phase from vertical hover to forward cruise is analyzed
with harmonic oscillation techniques to capture unsteady aerodynamic forces and
moments. Grid sensitivity studies and multi zone meshing strategies ensure
simulation accuracy, while ANSYS Fluent finite volume solver and coupled
pressure velocity algorithms provide high fidelity results. Dynamic derivatives
are derived from variations in angle of attack, flight path, and rotational
movements, with experimental and numerical data validating the approach. The
findings offer valuable insights for robust control design and stability
analysis, supporting future advancements in urban air mobility and aerospace
engineering. Overall, this approach demonstrates substantial promise for
optimizing aircraft performance during critical transition phases. These
results pave the way for future innovations

</details>


### [465] [Reinforcement learning in pursuit-evasion differential game: safety, stability and robustness](https://arxiv.org/abs/2507.19516)
*Xinyang Wang,Hongwei Zhang,Jun Xu,Shimin Wang,Martin Guay*

Main category: eess.SY

TL;DR: 该论文提出了一种新的强化学习方法，结合了 CBF 和 SMC，以应对有干扰的追逐-逃避问题中的安全性和稳定性问题。仿真证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的追逐-逃避问题的安全和稳定解决方案（通常结合 CBF 和 RL）未能考虑实际应用中可能出现的干扰（如风和其他执行器故障）。

Method: 通过将 CBF 和 SMC 项集成到 RL 中，并采用 Stackelberg 思想进行分层设计，以解决 CBF 和 SMC 项之间的耦合问题。追逐-逃避问题被公式化为零和博弈，并学习极小极大策略。

Result: 仿真结果表明，所提出的安全鲁棒强化学习框架在应对干扰的同时，能有效保证安全性和稳定性。

Conclusion: 该研究提出了一个安全鲁棒强化学习框架，能够同时处理追逐-逃避问题中的安全、稳定性和对干扰的鲁棒性。

Abstract: Safety and stability are two critical concerns in pursuit-evasion (PE)
problems in an obstacle-rich environment. Most existing works combine control
barrier functions (CBFs) and reinforcement learning (RL) to provide an
efficient and safe solution. However, they do not consider the presence of
disturbances, such as wind gust and actuator fault, which may exist in many
practical applications. This paper integrates CBFs and a sliding mode control
(SMC) term into RL to simultaneously address safety, stability, and robustness
to disturbances. However, this integration is significantly challenging due to
the strong coupling between the CBF and SMC terms. Inspired by Stackelberg
game, we handle the coupling issue by proposing a hierarchical design scheme
where SMC and safe control terms interact with each other in a leader-follower
manner. Specifically, the CBF controller, acting as the leader, enforces safety
independently of the SMC design; while the SMC term, as the follower, is
designed based on the CBF controller. We then formulate the PE problem as a
zero-sum game and propose a safe robust RL framework to learn the min-max
strategy online. A sufficient condition is provided under which the proposed
algorithm remains effective even when constraints are conflicting. Simulation
results demonstrate the effectiveness of the proposed safe robust RL framework.

</details>


### [466] [A safety governor for learning explicit MPC controllers from data](https://arxiv.org/abs/2507.19531)
*Anjie Mao,Zheming Wang,Hao Gu,Bo Chen,Li Yu*

Main category: eess.SY

TL;DR: A new MPC method using NNs that's simpler, safer, and works well in complex systems.


<details>
  <summary>Details</summary>
Motivation: To tackle neural networks (NNs) for approximating model predictive control (MPC) laws, aiming for a computationally efficient and constraint-satisfying approach.

Method: A novel learning-based explicit MPC structure is proposed, reformulated into a dual-mode scheme over the maximal constrained feasible set. A safety governor is constructed to ensure constraint satisfaction.

Result: The learning-based explicit MPC reduces to linear feedback control near the origin and satisfies all state and input constraints due to the safety governor. The approach is shown to be computationally easier to implement in high-dimensional systems.

Conclusion: The proposed novel learning-based explicit MPC structure reduces to linear feedback control near the origin and is computationally easier to implement than existing approaches, even in high-dimensional systems. The safety governor ensures all state and input constraints are satisfied, with proof of recursive feasibility provided and demonstrated on numerical examples.

Abstract: We tackle neural networks (NNs) to approximate model predictive control (MPC)
laws. We propose a novel learning-based explicit MPC structure, which is
reformulated into a dual-mode scheme over maximal constrained feasible set. The
scheme ensuring the learning-based explicit MPC reduces to linear feedback
control while entering the neighborhood of origin. We construct a safety
governor to ensure that learning-based explicit MPC satisfies all the state and
input constraints. Compare to the existing approach, our approach is
computationally easier to implement even in high-dimensional system. The proof
of recursive feasibility for the safety governor is given. Our approach is
demonstrated on numerical examples.

</details>


### [467] [ACCESS-AV: Adaptive Communication-Computation Codesign for Sustainable Autonomous Vehicle Localization in Smart Factories](https://arxiv.org/abs/2507.20399)
*Rajat Bhattacharjya,Arnab Sarkar,Ish Kool,Sabur Baidya,Nikil Dutt*

Main category: eess.SY

TL;DR: ACCESS-AV 是一个能源高效的 V2I 定位框架，利用 5G 基础设施，通过自适应策略将能耗降低了 43.09%，同时保持了亚 30 厘米的精度。


<details>
  <summary>Details</summary>
Motivation: 为了优化自动驾驶车辆 (ADV) 中计算密集型定位模块的能耗，特别是在 5G 网络支持的智能工厂环境中。

Method: ACCESS-AV 框架采用基于到达角 (AoA) 的估计算法，并利用多信号分类 (MUSIC) 算法，通过一种自适应通信-计算策略进行了优化，该策略根据信噪比 (SNR) 和车辆速度等环境条件动态平衡能耗和定位精度。

Result: 实验结果表明，与采用 AoA 算法（如 vanilla MUSIC、ESPRIT 和 Root-MUSIC）的非自适应系统相比，ACCESS-AV 的平均能耗降低了 43.09%，同时保持了亚 30 厘米的定位精度。

Conclusion: ACCESS-AV 框架通过利用现有的 5G 基础设施和机会性地访问 5G 同步信号块 (SSB)，实现了高于 43.09% 的平均能耗降低，同时保持了低于 30 厘米的定位精度，并在降低基础设施和运营成本方面具有可行性，可用于可持续的智能工厂环境。

Abstract: Autonomous Delivery Vehicles (ADVs) are increasingly used for transporting
goods in 5G network-enabled smart factories, with the compute-intensive
localization module presenting a significant opportunity for optimization. We
propose ACCESS-AV, an energy-efficient Vehicle-to-Infrastructure (V2I)
localization framework that leverages existing 5G infrastructure in smart
factory environments. By opportunistically accessing the periodically broadcast
5G Synchronization Signal Blocks (SSBs) for localization, ACCESS-AV obviates
the need for dedicated Roadside Units (RSUs) or additional onboard sensors to
achieve energy efficiency as well as cost reduction. We implement an
Angle-of-Arrival (AoA)-based estimation method using the Multiple Signal
Classification (MUSIC) algorithm, optimized for resource-constrained ADV
platforms through an adaptive communication-computation strategy that
dynamically balances energy consumption with localization accuracy based on
environmental conditions such as Signal-to-Noise Ratio (SNR) and vehicle
velocity. Experimental results demonstrate that ACCESS-AV achieves an average
energy reduction of 43.09% compared to non-adaptive systems employing AoA
algorithms such as vanilla MUSIC, ESPRIT, and Root-MUSIC. It maintains sub-30
cm localization accuracy while also delivering substantial reductions in
infrastructure and operational costs, establishing its viability for
sustainable smart factory environments.

</details>


### [468] [Whale Optimization Algorithms based fractional order fuzzy PID controller for Depth of Anesthesia](https://arxiv.org/abs/2507.19532)
*Amin Behboudifar,Chen Jing*

Main category: eess.SY

TL;DR: 本研究提出了一种基于鲸鱼优化算法 (WOA) 的分数阶模糊 PID (FOFPID) 和分数阶 PID (FOPID) 控制器，用于优化麻醉深度 (DOA) 控制。结果显示 FOFPID 在不确定条件下优于 FOPID。


<details>
  <summary>Details</summary>
Motivation: 解决手术中麻醉深度 (DOA) 控制的挑战，特别是患者生理参数的系统不确定性和非线性问题。

Method: 本研究提出了一种分数阶模糊 PID 控制器 (FOFPID) 和分数阶 PID 控制器 (FOPID) 来解决麻醉深度 (DOA) 控制问题。使用鲸鱼优化算法 (WOA) 优化了提出的控制器的参数，包括分数阶导数和积分阶数。

Result: FOFPID 和 FOPID 控制器在包含 8 名患者生理模型（基于药代动力学和药效学模型）的不确定性条件下进行了性能评估。结果表明 FOFPID 性能优于 FOPID。

Conclusion: FOFPID 控制器比 FOPID 控制器具有更好的性能。

Abstract: One of the most important surgical factors is Depth of Anesthesia (DOA)
control in patients. The main problem is to overcome the uncertainty and
nonlinearity of the system, due to different physiological parameters of the
patient's body and maintain DOA of patients in desired range during surgery.
This study demonstrates a fractional order fuzzy PID controller (FOFPID) and
fractional order PID controller (FOPID) to the problem. The Whale Optimization
Algorithms (WOA) is used to optimized the parameters of proposed controllers.
The orders of derivative and integral fractional controller is achieved by WOA.
The results indicate that FOFPID has a better performance than FOPID. To check
the performance of the controllers in presence of uncertainty, physiological
logical model of 8 patients has been investigated. The modeling is based on
Pharmacodynamic and Pharmacokinetic model. The results show the performance of
the proposed method.

</details>


### [469] [Comparing Behavioural Cloning and Reinforcement Learning for Spacecraft Guidance and Control Networks](https://arxiv.org/abs/2507.19535)
*Harry Holt,Sebastien Origer,Dario Izzo*

Main category: eess.SY

TL;DR: 对用于连续推力航天器轨迹优化任务的G&CNETs的BC和RL训练范式进行了比较。RL方法在适应性和发现新最优解方面优于BC方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决G&CNETs训练中BC和RL两种范式缺乏直接比较的问题。

Method: 对BC和RL两种范式进行了系统性评估，并提出了一种新颖的、针对G&CNETs的RL训练框架，该框架结合了解耦的动作和控制频率以及奖励再分配策略，以稳定训练并提供公平的比较。

Result: BC训练的G&CNETs擅长精确复制专家策略行为，以及确定性环境的最优控制结构。RL训练的G&CNETs具有更强的适应随机条件的能力，并能发现改进次优专家演示的解决方案。

Conclusion: BC训练的G&CNETs在模仿最优轨迹方面表现出色，但在数据集质量和覆盖范围方面受到限制。RL训练的G&CNETs在适应随机条件方面表现出优越性，并能发现超越次优专家演示的解决方案，有时能揭示全局最优策略。

Abstract: Guidance & control networks (G&CNETs) provide a promising alternative to
on-board guidance and control (G&C) architectures for spacecraft, offering a
differentiable, end-to-end representation of the guidance and control
architecture. When training G&CNETs, two predominant paradigms emerge:
behavioural cloning (BC), which mimics optimal trajectories, and reinforcement
learning (RL), which learns optimal behaviour through trials and errors.
Although both approaches have been adopted in G&CNET related literature, direct
comparisons are notably absent. To address this, we conduct a systematic
evaluation of BC and RL specifically for training G&CNETs on continuous-thrust
spacecraft trajectory optimisation tasks. We introduce a novel RL training
framework tailored to G&CNETs, incorporating decoupled action and control
frequencies alongside reward redistribution strategies to stabilise training
and to provide a fair comparison. Our results show that BC-trained G&CNETs
excel at closely replicating expert policy behaviour, and thus the optimal
control structure of a deterministic environment, but can be negatively
constrained by the quality and coverage of the training dataset. In contrast
RL-trained G&CNETs, beyond demonstrating a superior adaptability to stochastic
conditions, can also discover solutions that improve upon suboptimal expert
demonstrations, sometimes revealing globally optimal strategies that eluded the
generation of training samples.

</details>


### [470] [An Overview of Automated Vehicle Longitudinal Platoon Formation Strategies](https://arxiv.org/abs/2403.05415)
*M Sabbir Salek,Mugdha Basu Thakur,Pardha Sai Krishna Ala,Mashrur Chowdhury,Matthias Schmid,Pamela Murray-Tuite,Sakib Mahmud Khan,Venkat Krovi*

Main category: eess.SY

TL;DR: This paper reviews the state-of-the-art in AV platooning formation, covering vehicle models, information reception, data flow, spacing policies, and controllers, and identifies future research directions.


<details>
  <summary>Details</summary>
Motivation: Automated vehicle (AV) platooning has the potential to improve the safety, operational, and energy efficiency of surface transportation systems by limiting or eliminating human involvement in the driving tasks. The theoretical validity of the AV platooning strategies has been established and practical applications are being tested under real-world conditions. The emergence of sensors, communication, and control strategies has resulted in rapid and constant evolution of AV platooning strategies.

Method: This paper reviews the state-of-the-art knowledge in AV longitudinal platoon formation using a five-component platooning framework, which includes vehicle model, information-receiving process, information flow topology, spacing policy, and controller and discuss the advantages and limitations of the components.

Result: The paper discusses the advantages and limitations of the components within the platooning framework.

Conclusion: Based on the discussion about existing strategies and associated limitations, potential future research directions are presented.

Abstract: Automated vehicle (AV) platooning has the potential to improve the safety,
operational, and energy efficiency of surface transportation systems by
limiting or eliminating human involvement in the driving tasks. The theoretical
validity of the AV platooning strategies has been established and practical
applications are being tested under real-world conditions. The emergence of
sensors, communication, and control strategies has resulted in rapid and
constant evolution of AV platooning strategies. In this paper, we review the
state-of-the-art knowledge in AV longitudinal platoon formation using a
five-component platooning framework, which includes vehicle model,
information-receiving process, information flow topology, spacing policy, and
controller and discuss the advantages and limitations of the components. Based
on the discussion about existing strategies and associated limitations,
potential future research directions are presented.

</details>


### [471] [Biogeography-Based Optimization of Fuzzy Controllers for Improved Quarter Car Suspension Performance](https://arxiv.org/abs/2507.19542)
*Lida Shahbandari,Mohammad Mansouri*

Main category: eess.SY

TL;DR: 本研究优化了汽车悬架的I型和II型模糊控制器，使用BBO、PSO和GA算法进行调优。结果显示BBO优化的II型模糊控制器在减少颠簸和振动方面效果显著，且计算效率高，适用于电动和自动驾驶汽车。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对不同类型模糊控制器（I型和II型）性能的系统性比较，并且BBO算法在汽车悬架优化中的应用研究不足。

Method: 使用生物地理学优化（BBO）、粒子群优化（PSO）和遗传算法（GA）对四分之一汽车模型的I型和II型模糊控制器参数进行优化，并进行MATLAB Simulink仿真以评估性能。

Result: 与基线方法相比，在阶跃干扰下，BBO优化的II型模糊控制器可将车身位移减少22%，将加速度减少18%，同时保持计算效率。

Conclusion: 该研究为汽车悬架系统提出并优化了I型和II型模糊控制器，以增强在道路干扰（阶跃/正弦输入）下的乘坐舒适性和稳定性。通过集成生物地理学优化（BBO）、粒子群优化（PSO）和遗传算法（GA）来调整四分之一汽车模型的控制器参数，并强调了BBO的潜力。

Abstract: This study proposes optimized Type-I and Type-II fuzzy controllers for
automotive suspension systems to enhance ride comfort and stability under road
disturbances (step/sine inputs), addressing the lack of systematic performance
comparisons in existing literature. We integrate Biogeography-Based
Optimization (BBO), Particle Swarm Optimization (PSO), and Genetic Algorithms
(GA) to tune controller parameters for a quarter car model, with emphasis on
BBO's underexplored efficacy. MATLAB Simulink simulations demonstrate that
BBO-optimized Type-II fuzzy control reduces body displacement by 22% and
acceleration by 18% versus baseline methods under step disturbances, while
maintaining computational efficiency. The framework provides practical,
high-performance solutions for modern vehicles, particularly electric and
autonomous platforms where vibration attenuation and energy efficiency are
critical.

</details>


### [472] [Wardropian Cycles make traffic assignment both optimal and fair by eliminating price-of-anarchy with Cyclical User Equilibrium for compliant connected autonomous vehicles](https://arxiv.org/abs/2507.19675)
*Michał Hoffmann,Michał Bujak,Grzegorz Jamróz,Rafał Kucharski*

Main category: eess.SY

TL;DR: CAV 可以实现系统最优交通分配，但会造成不公平。本研究提出了一种称为 Wardropian 循环的新方法，该方法可以实现公平和最优的交通分配，并通过模拟证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在连通自动驾驶汽车 (CAV) 的背景下，实现系统最优交通分配，但由于用户之间的不公平性，其自愿接受性令人怀疑。

Method: 提出了一种称为 Wardropian 循环的新概念，该循环能够使平均旅行时间在用户之间平均分配，同时保持路径流的日常最优性。还提出了一种贪心启发式算法来近似最优解，并引入了循环用户均衡的概念以确保在单方面偏离情况下的稳定性。

Result: 通过在巴塞罗那、柏林、阿纳海姆和苏瀑等城市进行的模拟，证明了该方法的有效性。循环可以消除大量的无政府状态成本，并显着减少不平等现象，在 10 天内将初始不平等现象减少到 7% 以下。

Conclusion: 所提出的循环可以平衡用户和系统的需求，并通过模拟证明其有效性。

Abstract: Connected and Autonomous Vehicles (CAVs) open the possibility for centralised
routing with full compliance, making System Optimal traffic assignment
attainable. However, as System Optimum makes some drivers better off than
others, voluntary acceptance seems dubious. To overcome this issue, we propose
a new concept of Wardropian cycles, which, in contrast to previous utopian
visions, makes the assignment fair on top of being optimal, which amounts to
satisfaction of both Wardrop's principles. Such cycles, represented as
sequences of permutations to the daily assignment matrices, always exist and
equalise, after a limited number of days, average travel times among travellers
(like in User Equilibrium) while preserving everyday optimality of path flows
(like in System Optimum). We propose exact methods to compute such cycles and
reduce their length and within-cycle inconvenience to the users. As
identification of optimal cycles turns out to be NP-hard in many aspects, we
introduce a greedy heuristic efficiently approximating the optimal solution.
Finally, we introduce and discuss a new paradigm of Cyclical User Equilibrium,
which ensures stability of optimal Wardropian Cycles under unilateral
deviations.
  We complement our theoretical study with large-scale simulations. In
Barcelona, 670 vehicle-hours of Price-of-Anarchy are eliminated using cycles
with a median length of 11 days-though 5% of cycles exceed 90 days. However, in
Berlin, just five days of applying the greedy assignment rule significantly
reduces initial inequity. In Barcelona, Anaheim, and Sioux Falls, less than 7%
of the initial inequity remains after 10 days, demonstrating the effectiveness
of this approach in improving traffic performance with more ubiquitous social
acceptability.

</details>


### [473] [Simulation of Emergency Evacuation in Large Scale Metropolitan Railway Systems for Urban Resilience](https://arxiv.org/abs/2507.19545)
*Hangli Ge,Xiaojie Yang,Zipei Fan,Francesco Flammini,Noboru Koshizuka*

Main category: eess.SY

TL;DR: 本研究提出了一种用于铁路中断期间交通疏散的模拟方法，并通过对东京都市圈铁路网络的分析验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了提高城市在铁路中断时的韧性，本研究旨在模拟和优化大规模铁路网络中的交通疏散。

Method: 该研究提出了一种基于矩阵计算和非线性规划的数学模型，并结合子图划分技术来加速大规模图网络的计算。

Result: 模拟结果表明，该模型成功优化了疏散客流和平均旅行时间，同时满足了邻近车站的容量限制和目标中断恢复时间。

Conclusion: 该研究通过对东京都市圈铁路网络的模拟，证明了其提出的交通疏散优化模型能够有效优化疏散客流和平均旅行时间，并满足容量约束和恢复时间目标。

Abstract: This paper presents a simulation for traffic evacuation during railway
disruptions to enhance urban resilience. The research focuses on large-scale
railway networks and provides flexible simulation settings to accommodate
multiple node or line failures. The evacuation optimization model is
mathematically formulated using matrix computation and nonlinear programming.
The simulation integrates railway lines operated by various companies, along
with external geographical features of the network. Furthermore, to address
computational complexity in large-scale graph networks, a subgraph partitioning
solution is employed for computation acceleration. The model is evaluated using
the extensive railway network of Greater Tokyo. Data collection included both
railway network structure and real-world GPS footfall data to estimate the
number of station-area visitors for simulation input and evaluation purposes.
Several evacuation scenarios were simulated for major stations including Tokyo,
Shinjuku, Shibuya and so on. The results demonstrate that both evacuation
passenger flow (EPF) and average travel time (ATT) during emergencies were
successfully optimized, while remaining within the capacity constraints of
neighboring stations and the targeted disruption recovery times.

</details>


### [474] [Controller Design of an Airship](https://arxiv.org/abs/2507.19558)
*Manuel Schimmer*

Main category: eess.SY

TL;DR: 本论文提出了一种用于倾转推力飞艇的E-INDI鲁棒控制策略，通过结合内外环控制和控制分配方法，实现了在各种飞行条件下的精确控制和高鲁棒性，仿真结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于飞艇利用浮力产生升力，具有低速飞行和定点悬停的能力，非常适合需要持久性和精确制导的任务。然而，它们的控制具有挑战性，因为其轻质大结构对环境干扰非常敏感，并且传统的空气动力学控制面在低速或悬停时效果不佳。因此，开发一种针对倾转推力飞艇的鲁棒控制策略是必要的。

Method: 本研究提出了一种基于扩展增量非线性动态逆（Extended Incremental Nonlinear Dynamic Inversion, E-INDI）内环和高级外环的控制策略，并结合了推力矢量控制的飞艇。重点介绍了一种控制分配方法，以有效利用倾转旋翼等可用执行器。

Result: 仿真结果表明，所提出的E-INDI控制器在跟踪性能和鲁棒性方面均表现优异，能够有效应对模型不确定性、阵风、大气湍流和执行不匹配等情况，并与现有控制器进行了比较。

Conclusion: 所提出的E-INDI控制器在整个飞行包络内（包括悬停和高速飞行）实现了对飞艇的有效控制，并且在应对参数不确定性方面表现出很高的鲁棒性。

Abstract: Airships offer unique operational advantages due to their ability to generate
lift via buoyancy, enabling low-speed flight and stationary hovering. These
capabilities make them ideal for missions requiring endurance and precision
positioning. However, they also present significant control challenges: their
large, lightweight structures are highly sensitive to environmental
disturbances, and conventional aerodynamic control surfaces lose effectiveness
during low-speed or hover flight. The objective of this thesis is to develop a
robust control strategy tailored to a vectored-thrust airship equipped with
tiltable propellers. The proposed approach is based on an Extended Incremental
Nonlinear Dynamic Inversion inner loop in combination with a high level outer
loop, controlling the attitude and velocity of the airship. The proposed method
is able to effectively control the airship over the whole envelope, including
hover and high speed flight. For this, effective use of available actuators is
key. This includes especially the tilt rotors, for which a control allocation
method is presented. The controller's performance is validated through a series
of simulation-based test scenarios, including aggressive maneuvering, gust
rejection, atmospheric turbulence, and significant parameter mismatches. The
controller is compared against an alternative controller developed at the
institute, offering insight into the trade-offs between direct inversion and
incremental control approaches. Results demonstrate that the proposed E-INDI
controller achieves very good tracking performance and high high robustness
against parameter uncertainties.

</details>


### [475] [Diversity and Interaction Quality of a Heterogeneous Multi-Agent System Applied to a Synchronization Problem](https://arxiv.org/abs/2507.19631)
*Xin Mao,Dan Wang,Wei Chen,Li Qiu*

Main category: eess.SY

TL;DR: 本研究设计了一种可扩展的控制器，用于异构离散时间非线性多智能体系统的输出同步。该方法利用矩阵相位理论来处理智能体多样性和交互质量，并提供了一种基于李雅普诺夫分析的控制器设计程序。研究还考虑了不可解情况，并提出了替代解决方案。


<details>
  <summary>Details</summary>
Motivation: 为异构离散时间非线性多智能体系统设计可扩展的控制器以实现输出同步。

Method: 研究利用矩阵相位理论来量化智能体多样性和评估智能体间的交互质量，并基于此推导出同步问题的可解性条件。控制器设计基于李雅普诺夫分析，产生了低增益、逐分量同步控制器。此外，还提出了处理不可解情况的替代策略，包括设计控制器库存和使用聚类方法管理异构性。

Result: 研究表明，所需控制器的数量可以由底层图的强连通分量数量决定。控制器设计方法是有效的，并且在不可解的情况下提出了替代策略。

Conclusion: 该研究为具有线性共同振荡模式的异构离散时间非线性多智能体系统设计了可扩展的控制器，实现了输出同步。

Abstract: In this paper, scalable controller design to achieve output synchronization
for a heterogeneous discrete-time nonlinear multi-agent system is considered.
The agents are assumed to exhibit potentially nonlinear dynamics but share
linear common oscillatory modes. In a distributed control architecture,
scalability is ensured by designing a small number of distinguished
controllers, significantly fewer than the number of agents, even when agent
diversity is high. Our findings indicate that the number of controllers
required can be effectively determined by the number of strongly connected
components of the underlying graph. The study in this paper builds on the
recently developed phase theory of matrices and systems. First, we employ the
concept of matrix phase, specifically the phase alignability of a collection of
matrices, to quantify agent diversity. Next, we use matrix phase, particularly
the essential phase of the graph Laplacian, to evaluate the interaction quality
among the agents. Based on these insights, we derive a sufficient condition for
the solvability of the synchronization problem, framed as a trade-off between
the agent diversity and the interaction quality. In the process, a controller
design procedure based on Lyapunov analysis is provided, which produces low
gain, component-wise synchronizing controllers when the solvability condition
is satisfied. Numerical examples are given to illustrate the effectiveness of
the proposed design procedure. Furthermore, we consider cases where the
component-wise controller design problem is unsolvable. We propose alternative
strategies involving the design of a small inventory of controllers, which can
still achieve synchronization effectively by employing certain clustering
methods to manage heterogeneity.

</details>


### [476] [CDA-SimBoost: A Unified Framework Bridging Real Data and Simulation for Infrastructure-Based CDA Systems](https://arxiv.org/abs/2507.19707)
*Zhaoliang Zheng,Xu Han,Yuxin Bao,Yun Zhang,Johnson Liu,Zonglin Meng,Xin Xia,Jiaqi Ma*

Main category: eess.SY

TL;DR: CDA-SimBoost 是一个用于合作驾驶自动化的仿真框架，它利用真实世界的数据来创建以基础设施为中心的仿真环境，以解决长尾挑战、数据融合和传感器管理问题。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在解决智能基础设施在合作驾驶自动化 (CDA) 研究中的作用尚未得到充分探索，并且现有解决方案在解决长尾挑战、真实-合成数据融合和异类传感器管理方面支持有限的问题。

Method: CDA-SimBoost 框架包括三个主要组件：数字孪生构建器（用于根据传感器和高清地图数据生成高保真模拟器资产）、OFDataPip（用于处理在线和离线数据流）和 OpenCDA-InfraX（一个面向基础设施的高保真仿真平台）。

Result: CDA-SimBoost 支持真实的场景构建、罕见事件合成和可扩展的 CDA 研究评估。它提供了一个模块化架构和标准化的基准测试功能。

Conclusion: CDA-SimBoost 是一个统一的框架，可以根据真实世界的数据构建以基础设施为中心的仿真环境，从而弥合真实世界动态与虚拟环境之间的差距，并促进可重现和可扩展的基础设施驱动的 CDA 研究。

Abstract: Cooperative Driving Automation (CDA) has garnered increasing research
attention, yet the role of intelligent infrastructure remains insufficiently
explored. Existing solutions offer limited support for addressing long-tail
challenges, real-synthetic data fusion, and heterogeneous sensor management.
This paper introduces CDA-SimBoost, a unified framework that constructs
infrastructure-centric simulation environments from real-world data.
CDA-SimBoost consists of three main components: a Digital Twin Builder for
generating high-fidelity simulator assets based on sensor and HD map data,
OFDataPip for processing both online and offline data streams, and
OpenCDA-InfraX, a high-fidelity platform for infrastructure-focused simulation.
The system supports realistic scenario construction, rare event synthesis, and
scalable evaluation for CDA research. With its modular architecture and
standardized benchmarking capabilities, CDA-SimBoost bridges real-world
dynamics and virtual environments, facilitating reproducible and extensible
infrastructure-driven CDA studies. All resources are publicly available at
https://github.com/zhz03/CDA-SimBoost

</details>


### [477] [Star Tracker Misalignment Compensation in Deep Space Navigation Through Model-Based Estimation](https://arxiv.org/abs/2507.19838)
*Ridma Ganganath,Simone Servadio,David Lee*

Main category: eess.SY

TL;DR: 本研究提出了一种新的自适应框架，利用贝叶斯多模型自适应估计算法 (MMAE) 和乘性扩展卡尔曼滤波器 (MEKF) 来同时估计航天器姿态和传感器偏差。通过一种新的网格细化策略，该方法在减少最终偏差的均方根误差 (RMSE) 方面优于经典方法，实现了角秒级的精度，为资源受限的航天器提供了有效的飞行中校准解决方案。


<details>
  <summary>Details</summary>
Motivation: 为了解决未校正的星形跟踪器偏差在 GPS 拒止环境中引入显著指向误差的问题，该研究提出了一个自适应框架来同时估计航天器姿态和传感器偏差。

Method: 提出了一种新颖的自适应框架，用于同时估计航天器姿态和传感器偏差。该框架集成了在 N x N x N 三维假设网格上运行的贝叶斯多模型自适应估计算法 (MMAE)。每个假设都采用九状态乘性扩展卡尔曼滤波器 (MEKF) 来估计姿态、角速度和陀螺仪偏差。通过开发一种基于假设多样性和加权均值网格中心的鲁棒网格细化策略来防止过早收敛。

Result: 仿真结果表明，与经典方法相比，所提出的方法将最终偏差的均方根误差 (RMSE) 降低了约 14%，实现了角秒级的精度。

Conclusion: 该框架为资源受限航天器的飞行中校准提供了一种计算上可行且统计上可靠的解决方案，提高了其导航自主性。

Abstract: This work presents a novel adaptive framework for simultaneously estimating
spacecraft attitude and sensor misalignment. Uncorrected star tracker
misalignment can introduce significant pointing errors that compromise mission
objectives in GPS-denied environments. To address this challenge, the proposed
architecture integrates a Bayesian Multiple-Model Adaptive Estimation (MMAE)
framework operating over an N x N x N 3D hypothesis grid. Each hypothesis
employs a 9-state Multiplicative Extended Kalman Filter (MEKF) to estimate
attitude, angular velocity, and gyroscope bias using TRIAD-based vector
measurements. A key contribution is the development of a robust grid refinement
strategy that uses hypothesis diversity and weighted-mean grid centering to
prevent the premature convergence commonly encountered in classical, dominant
model-based refinement triggers. Extensive Monte Carlo simulations demonstrate
that the proposed method reduces the final misalignment RMSE relative to
classical approaches, achieving arcsecond-level accuracy. The resulting
framework offers a computationally tractable and statistically robust solution
for in-flight calibration, enhancing the navigational autonomy of
resource-constrained spacecraft.

</details>


### [478] [The Phantom of Davis-Wielandt Shell: A Unified Framework for Graphical Stability Analysis of MIMO LTI Systems](https://arxiv.org/abs/2507.19918)
*Ding Zhang,Xiaokan Yang,Axel Ringh,Li Qiu*

Main category: eess.SY

TL;DR: 本研究提出了一个基于 DW shell 的统一框架，用于多输入多输出线性反馈系统的图解稳定性分析，并提出了一种保守性最低的闭环稳定性判据。


<details>
  <summary>Details</summary>
Motivation: 为了对多输入多输出线性时不变反馈系统进行图解稳定性分析，并提供一个统一的框架来理解和比较现有的图解方法。

Method: 提出了一种基于 Davis-Wielandt (DW) shell 的统一框架，通过几何视角建立了 DW shell 与图解描述、增益和相位度量之间的联系。研究了分离条件之间的关系和相对保守性。提出了一种旋转的相对增益图 (SRG) 概念作为混合增益-相位表示，并推导了闭环稳定性判据。

Result: 提出了一个统一的框架，建立了 DW shell 与图解描述和增益/相位度量之间的联系。分析了分离条件之间的关系和相对保守性。提出了一种旋转 SRG 概念，并推导了一个保守性最低的闭环稳定性判据。

Conclusion: 本研究提出了一个基于 Davis-Wielandt (DW) shell 的统一框架，用于多输入多输出线性时不变反馈系统的图解稳定性分析，并建立了 DW shell 与各种图解描述以及增益和相位度量之间的联系。在此框架下，研究了各种分离条件之间的关系和相对保守性。最后，提出了一种旋转的相对增益图 (SRG) 概念作为混合增益-相位表示，并由此推导出闭环稳定性判据，该判据被证明是双分量反馈回路现有的二维图解条件中保守性最低的。

Abstract: This paper presents a unified framework based on Davis-Wielandt (DW) shell
for graphical stability analysis of multi-input and multi-output linear
time-invariant feedback systems. Connections between DW shells and various
graphical descriptions, as well as gain and phase measures, are established
through an intuitive geometric perspective. Within this framework, we examine
the relationships and relative conservatism among various separation
conditions. A rotated Scaled Relative Graph (SRG) concept is proposed as a
mixed gain-phase representation, from which a closed-loop stability criterion
is derived and shown to be the least conservative among the existing 2-D
graphical conditions for bi-component feedback loops. We also propose a
reliable algorithm for visualizing the rotated SRGs and include an example to
demonstrate the non-conservatism of the proposed condition.

</details>


### [479] [Periodic orbit tracking in cislunar space: A finite-horizon approach](https://arxiv.org/abs/2507.19928)
*Mohammed Atallah,Simone Servadio*

Main category: eess.SY

TL;DR: 通过NMPC和EKF，在环月空间近日点附近实现了更优的航天器周期轨道族维持，显著降低了燃料消耗。


<details>
  <summary>Details</summary>
Motivation: 旨在解决在环月空间近日点附近维持航天器在特定周期性轨道族内的问题，并提出一种优于传统轨道跟踪方法的控制方案，以减少燃料消耗。

Method: 该研究首先采用伪弧长延拓（PAC）方法计算轨道族成员，然后利用多变量回归（MPR）模型对状态进行参数化，接着开发了一个非线性模型预测控制（NMPC）框架来生成最优参考轨迹并计算相应的速度脉冲以实现轨迹跟踪。最后，将该控制系统与扩展卡尔曼滤波器（EKF）观测器集成，以估计航天器的相对状态。

Result: 数值模拟结果表明，与传统跟踪方法相比，该NMPC方案在维持航天器在Lyapunov、晕（halo）和近直线晕（near-rectilinear halo）轨道族内的过程中，燃料消耗显著降低。

Conclusion: 该研究提出了一种新颖的非线性模型预测控制（NMPC）方案，用于在环月空间（cislunar space）的近日点附近维持航天器在特定的周期性轨道族内。与传统的跟踪预定义参考轨道的方法不同，该方法设计了一条最优轨迹，无论初始参考如何，都能使航天器保持在轨道族内。通过使用拟牛轭作用（PAC）方法计算轨道族成员，并利用多变量回归（MPR）模型参数化状态，最后结合扩展卡尔曼滤波器（EKF）对NMPC框架进行集成，以生成最优参考轨迹和计算相应的速度脉冲以进行轨迹跟踪。数值模拟结果表明，与传统跟踪方法相比，燃料消耗显著降低。

Abstract: This paper presents a Nonlinear Model Predictive Control (NMPC) scheme for
maintaining a spacecraft within a specified family of periodic orbits near the
libration points in cislunar space. Unlike traditional approaches that track a
predefined reference orbit, the proposed method designs an optimal trajectory
that keeps the spacecraft within the orbit family, regardless of the initial
reference. The Circular Restricted Three-Body Problem (CR3BP) is used to model
the system dynamics. First, the Pseudo-Arclength Continuation (PAC) method is
employed to compute the members of each orbit family. Then, the state of each
member is parameterized by two variables: one defining the orbit and the other
specifying the location along it. These computed states are then fit to a
Multivariate Polynomial Regression (MPR) model. An NMPC framework is developed
to generate the optimal reference trajectory and compute the corresponding
velocity impulses for trajectory tracking. The control system is integrated
with a Extended Kalman Filter (EKF) observer that estimates the spacecraft's
relative state. Numerical simulations are conducted for Lyapunov, halo, and
near-rectilinear halo orbits near L1 and L2. The results demonstrate a
significant reduction in fuel consumption compared to conventional tracking
methods.

</details>


### [480] [A Unified Finite-Time Sliding Mode Quaternion-based Tracking Control for Quadrotor UAVs without Time Scale Separation](https://arxiv.org/abs/2507.20071)
*Ali M. Ali,Hashim A. Hashim,Awantha Jayasiri*

Main category: eess.SY

TL;DR: 该论文提出了一种新的四旋翼无人机有限时间位置控制方法，该方法基于四元数，无需时间尺度分离即可实现鲁棒性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 为了解决四旋翼无人机在跟踪任务中，对有界扰动进行有限时间位置控制的问题。

Method: 提出了一种基于四元数的非线性反馈控制器，并利用了期望姿态的变迁前馈项来预测较慢的平动子系统。

Result: 通过数值模拟结果证明了该控制器的有效性。

Conclusion: 该方法在没有时间尺度分离的情况下，保证了半全局有限时间稳定性。

Abstract: This paper presents a novel design for finite-time position control of
quadrotor Unmanned Aerial Vehicles (UAVs). A robust, finite-time, nonlinear
feedback controller is introduced to reject bounded disturbances in tracking
tasks. The proposed control framework differs conceptually from conventional
controllers that utilize Euler angle parameterization for attitude and adhere
to the traditional hierarchical inner-outer loop design. In standard
approaches, the translational controller and the corresponding desired attitude
are computed first, followed by the design of the attitude controller based on
time-scale separation between fast attitude and slow translational dynamics. In
contrast, the proposed control scheme is quaternion-based and utilizes a
transit feed-forward term in the attitude dynamics that anticipates the slower
translational subsystem. Robustness is achieved through the use of continuously
differentiable sliding manifolds. The proposed approach guarantees semi-global
finite-time stability, without requiring time-scale separation. Finally,
numerical simulation results are provided to demonstrate the effectiveness of
the proposed controller.

</details>


### [481] [Comparative Analysis of Data-Driven Predictive Control Strategies](https://arxiv.org/abs/2507.20098)
*Sohrab Rezaei,Ali Khaki-Sedigh*

Main category: eess.SY

TL;DR: Compares Data Enabled Predictive Control, Willems-Koopman Predictive Control, and Model-Free Adaptive Predictive Control based on theory, assumptions, and performance using a numerical example.


<details>
  <summary>Details</summary>
Motivation: To compare data-driven predictive control strategies by examining their theoretical foundations, assumptions, and applications.

Method: The paper systematically reviews Data Enabled Predictive Control, Willems-Koopman Predictive Control, and Model-Free Adaptive Predictive Control, outlining their supporting theories and discussing their fundamental assumptions and their influence on control effectiveness. A numerical example is used for performance comparison.

Result: A rigorous performance evaluation of the three strategies is enabled through a numerical example, facilitating a comprehensive comparison.

Conclusion: This paper provides a comparative analysis of three data-driven predictive control strategies, highlighting their theoretical underpinnings, assumptions, and practical applications, and evaluating their performance through a numerical benchmark.

Abstract: This paper compares data-driven predictive control strategies by examining
their theoretical foundations, assumptions, and applications. The three most
widely recognized and consequential methods, Data Enabled Predictive Control,
Willems-Koopman Predictive Control, Model-Free Adaptive Predictive Control are
employed. Each of these strategies is systematically reviewed, and the primary
theories supporting it are outlined. Following analysis, a discussion is
provided regarding their fundamental assumptions, emphasizing their influence
on control effectiveness. A numerical example is presented as a benchmark for
comparison to enable a rigorous performance evaluation.

</details>


### [482] [On Certificates for Almost Sure Reachability in Stochastic Systems](https://arxiv.org/abs/2507.20194)
*Arash Bahari Kordabad,Rupak Majumdar,Harshit Jitendra Motwani,Sadegh Soudjani*

Main category: eess.SY

TL;DR: 该研究探讨了随机系统中几乎确定可达性的认证问题，发现将搜索限制在多项式或二次模板会损害完整性。研究为线性系统提供了可达性证书的完整表征，并推广了随机游走行为。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决在离散时间随机系统中认证几乎确定可达性的问题。几乎确定可达性是指从任何初始条件开始，系统状态以概率一种到达给定目标集。尽管漂移和变体条件在理论上是必要且充分的，但计算方法通常依赖于将搜索限制在固定模板（如多项式或二次函数）上。这种限制可能会损害认证的完整性，因此需要更深入的研究。

Method: 通过研究漂移和变体条件来认证离散时间随机系统中的几乎确定可达性。研究人员分析了这些条件在计算方法中可能存在的限制，特别是当搜索限制在多项式或二次函数等固定模板时。他们证明了这种限制会损害完整性，并提出了一个例子，其中一个多项式系统对于给定的目标集几乎确定地可达，但不存在多项式证书。此外，他们还提出了一个线性系统的例子，其中原点的邻域几乎确定地可达，但不存在二次证书。

Result: 研究表明，将认证搜索限制在多项式或二次函数等固定模板上会损害其完整性。他们举例说明，对于一个多项式系统，即使目标集几乎确定地可达，也不存在多项式证书。同样，对于一个线性系统，即使原点的邻域几乎确定地可达，也不存在二次证书。然而，研究为具有加性噪声的线性系统提供了可达性证书的完整表征，明确了存在证书的系统矩阵条件，并阐明了系统结构和维度如何影响对非二次模板的需求。

Conclusion: 该研究为具有加性噪声的线性系统提供了可达性证书的完整表征。它揭示了系统矩阵上的条件，在这些条件下有效的证书存在，并展示了系统结构和维度如何决定了对非二次模板的需求。这些结果将经典的随机游走行为推广到更广泛的随机动力系统类别。

Abstract: Almost sure reachability refers to the property of a stochastic system
whereby, from any initial condition, the system state reaches a given target
set with probability one. In this paper, we study the problem of certifying
almost sure reachability in discrete-time stochastic systems using drift and
variant conditions. While these conditions are both necessary and sufficient in
theory, computational approaches often rely on restricting the search to fixed
templates, such as polynomial or quadratic functions. We show that this
restriction compromises completeness: there exists a polynomial system for
which a given target set is almost surely reachable but admits no polynomial
certificate, and a linear system for which a neighborhood of the origin is
almost surely reachable but admits no quadratic certificate. We then provide a
complete characterization of reachability certificates for linear systems with
additive noise. Our analysis yields conditions on the system matrices under
which valid certificates exist, and shows how the structure and dimension of
the system determine the need for non-quadratic templates. Our results
generalize the classical random walk behavior to a broader class of stochastic
dynamical systems.

</details>


### [483] [A Truthful Mechanism Design for Distributed Optimisation Algorithms in Networks with Self-interested Agents](https://arxiv.org/abs/2507.20250)
*Tianyi Zhong,David Angeli*

Main category: eess.SY

TL;DR: 提出了一种用于增强多主体系统韧性的机制，以应对自私代理的操纵。


<details>
  <summary>Details</summary>
Motivation: 解决多主体系统中应对自私代理的韧性增强问题。

Method: 提出了一种避免自私和策略性代理恶意操纵算法的机制。

Result: 该机制被证明在理论上能够激励自私代理参与并忠实地遵循算法，并且与任何能够计算至少一个子梯度的分布式优化算法兼容。此外，通过一个示例说明了该机制的有效性。

Conclusion: 该机制激励自私的代理积极参与并忠实地遵循算法，并且与任何能够计算至少一个子梯度的分布式优化算法兼容。

Abstract: Enhancing resilience in multi-agent systems in the face of selfish agents is
an important problem that requires further characterisation. This work develops
a truthful mechanism that avoids self-interested and strategic agents
maliciously manipulating the algorithm. We prove theoretically that the
proposed mechanism incentivises self-interested agents to participate and
follow the provided algorithm faithfully. Additionally, the mechanism is
compatible with any distributed optimisation algorithm that can calculate at
least one subgradient at a given point. Finally, we present an illustrative
example that shows the effectiveness of the mechanism.

</details>


### [484] [Relaxing Probabilistic Latent Variable Models' Specification via Infinite-Horizon Optimal Control](https://arxiv.org/abs/2507.20277)
*Zhichao Chen,Hao Wang,Licheng Pan,Yiran Ma,Yunfei Teng,Jiaze Ma,Le Yao,Zhiqiang Ge,Zhihuan Song*

Main category: eess.SY

TL;DR: 本文利用最优控制方法解决了PLVM中的模型设定问题，提出了一种新的EM算法，并在实验中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的PLVM依赖联合分布，但引入潜在变量会导致参数学习问题，通常需要正则化项和EM算法，而EM算法限制潜在变量分布于预定义的归一化分布族，这限制了其灵活性。本文旨在克服这一限制，提出一种新的方法来处理模型设定问题。

Method: 本文提出将潜在变量分布表示为通过具有控制策略的常微分方程扰动的一组有限实例，确保实例渐近收敛到真实的潜在变量分布。通过将分布推断问题重新表述为最优控制策略确定问题，并利用庞特里亚金最大值原理推导出最优控制策略，然后使用再生核希尔伯特空间提供其实现形式。

Result: 通过将潜在变量分布表示为有限实例并利用最优控制策略，放宽了模型设定。推导了最优控制策略的闭式表达式，并开发了一种新的、收敛有保证的EM算法。实验结果验证了该方法的有效性和优越性。

Conclusion: 本文提出了一种基于无限时最优控制的方法来解决概率潜在变量模型（PLVM）中的模型设定问题，通过将分布推断重构为最优控制策略确定问题，并推导了相应的闭式最优控制策略，最后开发了一种新的、收敛有保证的EM算法，并在实验中验证了其有效性和优越性。

Abstract: In this paper, we address the issue of model specification in probabilistic
latent variable models (PLVMs) using an infinite-horizon optimal control
approach. Traditional PLVMs rely on joint distributions to model complex data,
but introducing latent variables results in an ill-posed parameter learning
problem. To address this issue, regularization terms are typically introduced,
leading to the development of the expectation-maximization (EM) algorithm,
where the latent variable distribution is restricted to a predefined normalized
distribution family to facilitate the expectation step. To overcome this
limitation, we propose representing the latent variable distribution as a
finite set of instances perturbed via an ordinary differential equation with a
control policy. This approach ensures that the instances asymptotically
converge to the true latent variable distribution as time approaches infinity.
By doing so, we reformulate the distribution inference problem as an optimal
control policy determination problem, relaxing the model specification to an
infinite-horizon path space. Building on this formulation, we derive the
corresponding optimal control policy using the Pontryagin's maximum principle
and provide a closed-form expression for its implementation using the
reproducing kernel Hilbert space. After that, we develop a novel,
convergence-guaranteed EM algorithm for PLVMs based on this
infinite-horizon-optimal-control-based inference strategy. Finally, extensive
experiments are conducted to validate the effectiveness and superiority of the
proposed approach.

</details>


### [485] [HJB-based online safety-embedded critic learning for uncertain systems with self-triggered mechanism](https://arxiv.org/abs/2507.20545)
*Zhanglin Shangguan,Bo Yang,Qi Li,Wei Xiao,Xingping Guan*

Main category: eess.SY

TL;DR: 本研究提出了一种用于安全关键系统的学习型最优控制框架，该框架使用鲁棒控制障碍函数（RCBF）来处理参数不确定性，并通过自触发机制提高效率，最终通过数值模拟验证了其在安全性和控制性能上的有效性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决安全关键系统中存在的参数不确定性问题，并提出一种能够同时保证系统安全和控制性能的最优控制方法。传统的控制方法在面对不确定性时可能难以同时满足严格的安全要求和最优控制目标。因此，研究的动机在于开发一种学习型的控制框架，能够有效地处理参数不确定性，并通过创新的方法（如RCBF和自触发机制）来提高控制效率和鲁棒性。

Method: 本研究提出了一种基于学习的最优控制框架，该框架结合了鲁棒控制障碍函数（RCBF）和自触发机制。RCBF中包含基于Lyapunov的补偿项，以应对参数不确定性带来的安全风险。最优控制问题被转化为最小化一个包含安全约束的价值函数，其中RCBF通过拉格朗日乘子引入，该乘子能够自适应地平衡安全约束和最优稳定目标。为了提高效率，研究设计了一种自触发实现机制，减少了控制更新次数，同时保持了稳定性和安全性。最终，通过在线安全嵌入式Critic学习框架来求解自触发约束的HJB方程，并实时计算拉格朗日乘子以确保安全。

Result: 数值模拟结果表明，该方法能够有效地处理参数不确定性，同时保证系统的安全性和控制性能。RCBF和自适应拉格朗日乘子确保了在不确定性下的安全性，而自触发机制则提高了计算效率。

Conclusion: 该研究提出了一个新颖的学习型最优控制框架，用于解决具有参数不确定性的安全关键系统。该框架通过结合鲁棒控制障碍函数（RCBF）和基于Lyapunov的补偿项，在存在不确定性的情况下严格保证了系统的安全性。此外，研究还提出了一种自触发机制，以提高计算效率，同时保持稳定性和安全性。通过在线安全嵌入式Critic学习框架求解HJB方程，并实时计算拉格朗日乘子，最终通过数值模拟验证了该方法在保证安全和控制性能方面的有效性。

Abstract: This paper presents a learning-based optimal control framework for
safety-critical systems with parametric uncertainties, addressing both
time-triggered and self-triggered controller implementations. First, we develop
a robust control barrier function (RCBF) incorporating Lyapunov-based
compensation terms to rigorously guarantee safety despite parametric
uncertainties. Building on this safety guarantee, we formulate the constrained
optimal control problem as the minimization of a novel safety-embedded value
function, where the RCBF is involved via a Lagrange multiplier that adaptively
balances safety constraints against optimal stabilization objectives. To
enhance computational efficiency, we propose a self-triggered implementation
mechanism that reduces control updates while maintaining dual stability-safety
guarantees. The resulting self-triggered constrained Hamilton-Jacobi-Bellman
(HJB) equation is solved through an online safety-embedded critic learning
framework, with the Lagrange multiplier computed in real time to ensure safety.
Numerical simulations demonstrate the effectiveness of the proposed approach in
achieving both safety and control performance.

</details>


### [486] [A Modified Adaptive Data-Enabled Policy Optimization Control to Resolve State Perturbations](https://arxiv.org/abs/2507.20580)
*Mojtaba Kaheni,Niklas Persson,Vittorio De Iuliis,Costanzo Manes,Alessandro V. Papadopoulos*

Main category: eess.SY

TL;DR: PFDeePO算法通过暂停增益更新和引入比例控制信号的乘性噪声，消除了DeePO算法中因探测噪声引起的状态扰动，同时保持了系统性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统的DeePO算法需要持续激励的输入信号，但LQR设计无法固有地生成此类信号，因此需要添加探测噪声，这可能引起不期望的状态扰动。该研究旨在消除这种状态扰动。

Method: PFDeePO算法通过两个原则来解决无探测噪声时的问题：1. 当系统状态接近平衡点时，暂停DeePO算法的控制增益更新。2. 当控制器收敛时，为控制信号引入一个增益（均值为1）的乘性噪声。

Result: 通过仿真证明了PFDeePO的有效性，展示了其在消除状态扰动、保持系统性能和稳定性的能力。

Conclusion: 该研究提出了一种名为PFDeePO的改进算法，通过暂停增益更新和引入比例控制信号的乘性噪声来消除状态扰动，同时保持系统性能和稳定性。

Abstract: This paper proposes modifications to the data-enabled policy optimization
(DeePO) algorithm to mitigate state perturbations. DeePO is an adaptive,
data-driven approach designed to iteratively compute a feedback gain equivalent
to the certainty-equivalence LQR gain. Like other data-driven approaches based
on Willems' fundamental lemma, DeePO requires persistently exciting input
signals. However, linear state-feedback gains from LQR designs cannot
inherently produce such inputs. To address this, probing noise is
conventionally added to the control signal to ensure persistent excitation.
However, the added noise may induce undesirable state perturbations. We first
identify two key issues that jeopardize the desired performance of DeePO when
probing noise is not added: the convergence of states to the equilibrium point,
and the convergence of the controller to its optimal value. To address these
challenges without relying on probing noise, we propose Perturbation-Free DeePO
(PFDeePO) built on two fundamental principles. First, the algorithm pauses the
control gain updating in DeePO process when system states are near the
equilibrium point. Second, it applies a multiplicative noise, scaled by a mean
value of $1$ as a gain for the control signal, when the controller converges.
This approach minimizes the impact of noise as the system approaches
equilibrium while preserving stability. We demonstrate the effectiveness of
PFDeePO through simulations, showcasing its ability to eliminate state
perturbations while maintaining system performance and stability.

</details>


### [487] [Sequential Operation of Residential Energy Hubs](https://arxiv.org/abs/2507.20621)
*Darío Slaifstein,Gautham Ram Chandra Mouli,Laura Ramirez-Elizondo,Pavol Bauer*

Main category: eess.SY

TL;DR: 新颖的两阶段经济模型预测控制器可优化能源中心运行，但增加电池损耗，热能储能灵活性有限。


<details>
  <summary>Details</summary>
Motivation: 住宅能源中心的多能源载流子（电力、热能、交通）的运行因不同的载流子动态、混合储能协调和高维动作空间而面临重大挑战。

Method: 提出了一种新颖的两阶段经济模型预测控制器，并结合了电池退化和热力系统的物理模型，在荷兰的连续能源市场中运行。

Result: 联合优化日内拍卖和日前拍卖是最佳控制策略，但会增加电池损耗。在日内拍卖价格未知的情况下，夏季应遵循连续时间日内拍卖，冬季应遵循日内拍卖。热能储能的短期灵活性有限。

Conclusion: 该研究提出了一种新颖的两阶段经济模型预测控制器，用于管理包含多种能源载体（电力、热能、交通）的住宅能源中心。研究结果表明，联合优化日前和日内拍卖是最佳控制策略，但会增加电池损耗。与静态电池组和支持双向充电的电动汽车相比，热能储能的短期灵活性有限。

Abstract: The operation of residential energy hubs with multiple energy carriers
(electricity, heat, mobility) poses a significant challenge due to different
carrier dynamics, hybrid storage coordination and high-dimensional
action-spaces. Energy management systems oversee their operation, deciding the
set points of the primary control layer. This paper presents a novel 2-stage
economic model predictive controller for electrified buildings including
physics-based models of the battery degradation and thermal systems. The
hierarchical control operates in the Dutch sequential energy markets. In
particular common assumptions regarding intra-day markets (auction and
continuous-time) are discussed as well as the coupling of the different storage
systems. The best control policy is to co-optimize day-ahead and intra-day
auctions in the first stage, to later follow intra-day auctions. If no
intra-day prices are known at the time of the day-ahead auction, its best to
follow continuous time intra-day in the summer and the intra-day auction in the
winter. Additionally, this sequential operation increases battery degradation.
Finally, under our controller the realized short-term flexibility of the
thermal energy storage is marginal compared to the flexibility delivered by
static battery pack and electric vehicles with bidirectional charging.

</details>


### [488] [Convergent Weight and Activation Dynamics in Memristor Neural Networks](https://arxiv.org/abs/2507.20634)
*Mauro Di Marco,Mauro Forti,Luca Pancioni,Giacomo Innocenti,Alberto Tesi*

Main category: eess.SY

TL;DR: 研究了忆阻器反馈动态神经网络的权重-激活动态收敛性，并证明了其收敛性。


<details>
  <summary>Details</summary>
Motivation: 为了实现内容可寻址内存和实时解决信号处理任务，研究具有多个稳定平衡点的神经网络的收敛性至关重要。权重-激活动态比其他两种方式更符合神经系统的建模。

Method: 对忆阻器反馈动态神经网络的权重-激活动态进行收敛性分析。

Result: 在对忆阻器互连结构做出适当假设的情况下，权重和激活的解收敛于一个平衡点，例外情况的初始条件集合的测度最多为零。该结果包括了神经网络具有多个稳定平衡点的重要情况。

Conclusion: 该研究首次系统地分析了忆阻器反馈动态神经网络的权重-激活动态的收敛性，并证明了在适当的假设下，权重和激活可以收敛到一个平衡点。

Abstract: Convergence of dynamic feedback neural networks (NNs), as the
Cohen-Grossberg, Hopfield and cellular NNs, has been for a long time a
workhorse of NN theory. Indeed, convergence in the presence of multiple stable
equilibrium points (EPs) is crucial to implement content addressable memories
and solve several other signal processing tasks in real time. There are two
typical ways to use a convergent NN, i.e.: a) let the activations evolve while
maintaining fixed weights and inputs (activation dynamics) or b) adapt the
weights while maintaining fixed activations (weight dynamics). As remarked in a
seminal paper by Hirsch, there is another interesting possibility, i.e., let
the neuron interconnection weights evolve while simultaneously running the
activation dynamics (weight-activation dynamics). The weight-activation
dynamics is of importance also because it is more plausible than the other two
types for modeling neural systems. The paper breaks new ground by analyzing for
the first time in a systematic way the convergence properties of the
weight-activation dynamics for a class of memristor feedback dynamic NNs. The
main result is that, under suitable assumptions on the structure of the
memristor interconnections, the solutions (weights and activations) converge to
an EP, except at most for a set of initial conditions with zero measure. The
result includes the most important case where the NN has multiple stable EPs.

</details>


### [489] [SNR Optimization for Common Emitter Amplifier](https://arxiv.org/abs/2507.20669)
*Orhan Gazi*

Main category: eess.SY

TL;DR: 本研究探讨了共射放大器基极电阻的热噪声对输出信噪比的影响。结果表明，CEA 输出端的一阶巴特沃斯滤波器可显著提高输出信噪比，并抑制高阶滤波器。我们提出了一个用于选择模拟滤波器截止频率的公式，以在 CEA 输出上实现显着的信噪比提升。考虑到滤波器复杂性和输出信噪比的提高，一阶巴特沃斯滤波器优于 Chebyshev I、II 和椭圆滤波器。


<details>
  <summary>Details</summary>
Motivation: 研究了共射放大器（CEA）基极电阻的热噪声对输出信噪比的影响。

Method: 提出一个用于选择给定阶数的模拟滤波器的截止频率的公式，以在 CEA 输出上实现显著的信噪比提高。

Result: 一阶巴特沃斯滤波器在 CEA 输出端可以显著提高输出信噪比，并抑制高阶巴特沃斯、Chebyshev I、II 和椭圆滤波器。

Conclusion: 考虑到滤波器的复杂性和输出信噪比的提高，一阶巴特沃斯滤波器优于 Chebyshev I、II 和椭圆滤波器。

Abstract: In this paper we investigate the effects of the thermal noise of the base
resistance of common emitter amplifier (CEA) on the output SNR, and we show
that a first order Butterworth filter at the output of the CEA significantly
improves output SNR significantly and supress the performances of higher order
Butterworth, Chebyshev I, II and elliptic filters. We propose a formula for the
selection of cut-off frequency of analog filters for given orders to achieve
significant SNR improvement at CEA output. Considering the filter complexity
and output SNR improvement, we can conclude that the first order Butterworth
filter outperforms Chebyshev I, II and elliptic filters.

</details>


### [490] [What's Really Different with AI? -- A Behavior-based Perspective on System Safety for Automated Driving Systems](https://arxiv.org/abs/2507.20685)
*Marcus Nolte,Nayel Fabian Salem,Olaf Franke,Jan Heckmann,Christoph Höhmann,Georg Stettinger,Markus Maurer*

Main category: eess.SY

TL;DR: 本论文旨在为如何将标准应用于“基于AI的”自动驾驶系统提供指导，并区分AI特异性风险和开放环境相关的不确定性。


<details>
  <summary>Details</summary>
Motivation: 为了应对“基于AI的”系统（特别是在部署后需要运行在开放环境中）的安全保障挑战，以及日益复杂的“基于AI的”系统标准和法规。

Method: 提出了一种方法，通过概念分离来弥合系统级和AI特异性安全分析之间的差距，同时确保工程安全“基于AI的”系统的必要严谨性。

Result: 通过概念分离，可以利用共性来弥合系统级和AI特异性安全分析之间的差距，同时确保工程安全“基于AI的”系统的必要严谨性。

Conclusion: 提供有关如何将标准应用于（“基于AI的”）自动驾驶系统的合格论证的指导，并区分AI特异性和与开放上下文相关的通用不确定性来源。

Abstract: Assuring safety for ``AI-based'' systems is one of the current challenges in
safety engineering. For automated driving systems, in particular, further
assurance challenges result from the open context that the systems need to
operate in after deployment. The current standardization and regulation
landscape for ``AI-based'' systems is becoming ever more complex, as standards
and regulations are being released at high frequencies.
  This position paper seeks to provide guidance for making qualified arguments
which standards should meaningfully be applied to (``AI-based'') automated
driving systems. Furthermore, we argue for clearly differentiating sources of
risk between AI-specific and general uncertainties related to the open context.
In our view, a clear conceptual separation can help to exploit commonalities
that can close the gap between system-level and AI-specific safety analyses,
while ensuring the required rigor for engineering safe ``AI-based'' systems.

</details>


### [491] [Efficient Adjoint Petrov-Galerkin Reduced Order Models for fluid flows governed by the incompressible Navier-Stokes equations](https://arxiv.org/abs/2507.20739)
*Kamil David Sommer,Lucas Mieg,Siddharth Sharma,Romuald Skoda,Martin Mönnigmann*

Main category: eess.SY

TL;DR: 提出了一种高效的伴随Petrov-Galerkin (eAPG) ROM方法，用于求解不可压缩Navier-Stokes方程，相比现有方法，它提高了计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决标准伴随Petrov-Galerkin (APG) ROM在求解不可压缩Navier-Stokes方程时，由于测试基向量时间不变性导致的计算需求高的问题，并提高其稳定性和准确性。

Method: 提出了一种高效的伴随Petrov-Galerkin (eAPG) ROM方法，利用流体动力学方程的结构和数据驱动优化来增强内存长度，解决了标准APG-ROM的计算需求问题。

Result: eAPG-ROM在计算效率和准确性上优于标准的Galerkin ROM和一般的APG-ROM，数值结果表明该方法在3D湍流绕圆柱流动问题上是有效的。

Conclusion: 该研究通过数据驱动优化增强了内存长度，并对3D湍流绕圆柱流动进行了数值验证，证明了eAPG-ROM在计算效率和准确性方面的优越性。

Abstract: This research paper investigates the Adjoint Petrov-Galerkin (APG) method for
reduced order models (ROM) and fluid dynamics governed by the incompressible
Navier-Stokes equations. The Adjoint Petrov-Galerkin ROM, derived using the
Mori-Zwanzig formalism, demonstrates superior accuracy and stability compared
to standard Galerkin ROMs. However, challenges arise due to the time invariance
of the test basis vectors, resulting in high computational requirements. To
address this, we introduce a new efficient Adjoint Petrov-Galerkin (eAPG) ROM
formulation, extending its application to the incompressible Navier-Stokes
equations by exploiting the polynomial structure inherent in these equations.
The offline and online phases partition eliminates the need for repeated test
basis vector evaluations. This improves computational efficiency in comparison
to the general Adjoint Petrov-Galerkin ROM formulation. A novel approach to
augmenting the memory length, a critical factor influencing the stability and
accuracy of the APG-ROM, is introduced, employing a data-driven optimization.
Numerical results for the 3D turbulent flow around a circular cylinder
demonstrate the efficacy of the proposed approach. Error measures and
computational cost evaluations, considering metrics such as floating point
operations and simulation time, provide a comprehensive analysis.

</details>


### [492] [Beyond Line-of-Sight: Cooperative Localization Using Vision and V2X Communication](https://arxiv.org/abs/2507.20772)
*Annika Wong,Zhiqi Tang,Frank J. Jiang,Karl H. Johansson,Jonas Mårtensson*

Main category: eess.SY

TL;DR: 提出了一种基于视觉和V2X通信的合作定位算法，用于解决CAVs在城市复杂环境中的定位问题。该算法通过去中心化观察器实现精确估计，并通过实验和仿真进行了验证。


<details>
  <summary>Details</summary>
Motivation: 为了解决网联和自动化车辆（CAVs）在城市复杂环境中（如GNSS信号不可靠、存在遮挡等）的安全运行问题，需要精确鲁棒的定位技术。

Method: 提出了一种新颖的去中心化观察器，用于估计车辆和地标的姿态。该观察器利用车载摄像头和V2X通信，即使在GNSS信号不可靠和存在遮挡的情况下也能实现精确的定位。

Result: 证明了所提出的观察器在满足最小可观察性条件的情况下，可以实现估计误差原点的局部指数稳定。

Conclusion: 该算法通过实验和仿真验证了其在实际场景中的可扩展性和理论保证。

Abstract: Accurate and robust localization is critical for the safe operation of
Connected and Automated Vehicles (CAVs), especially in complex urban
environments where Global Navigation Satellite System (GNSS) signals are
unreliable. This paper presents a novel vision-based cooperative localization
algorithm that leverages onboard cameras and Vehicle-to-Everything (V2X)
communication to enable CAVs to estimate their poses, even in occlusion-heavy
scenarios such as busy intersections. In particular, we propose a novel
decentralized observer for a group of connected agents that includes landmark
agents (static or moving) in the environment with known positions and vehicle
agents that need to estimate their poses (both positions and orientations).
Assuming that (i) there are at least three landmark agents in the environment,
(ii) each vehicle agent can measure its own angular and translational
velocities as well as relative bearings to at least three neighboring landmarks
or vehicles, and (iii) neighboring vehicles can communicate their pose
estimates, each vehicle can estimate its own pose using the proposed
decentralized observer. We prove that the origin of the estimation error is
locally exponentially stable under the proposed observer, provided that the
minimal observability conditions are satisfied. Moreover, we evaluate the
proposed approach through experiments with real 1/10th-scale connected vehicles
and large-scale simulations, demonstrating its scalability and validating the
theoretical guarantees in practical scenarios.

</details>


### [493] [UAV-Borne Digital Radar System for Coherent Multistatic SAR Imaging](https://arxiv.org/abs/2507.20792)
*Julian Kanz,Christian Gesell,Christina Bonfert,David Werbunat,Alexander Grathwohl,Julian Aguilar,Martin Vossiek,Christian Waldschmidt*

Main category: eess.SY

TL;DR: 一种用于无人机平台的数字多站SAR系统，使用OFDM波形，能实现高分辨率成像并校正同步误差。


<details>
  <summary>Details</summary>
Motivation: 为了实现更高采样率和灵活的数字信号处理，特别是OFDM等数字调制技术在雷达系统中的应用，以支持无人机平台的多站SAR成像。

Method: 提出了一种基于UAV的多站SAR数字雷达系统，采用OFDM波形，实现L波段信号的直接采样和相干处理。该系统包含一个主节点（收发）和多个仅接收的副节点，通过触发机制优化数据存储，并对同步误差进行分析和校正。

Result: 系统在静态和UAV平台上进行了验证，成功实现了高分辨率的单站、双站和多站SAR成像，证明了其相干处理概念的有效性，并能有效校正同步误差。

Conclusion: 该数字雷达系统成功实现了多站SAR成像，并通过静态和UAV平台的实验验证了其相干处理能力，为未来无人机群协同感知提供了有效方案。

Abstract: Advancements in analog-to-digital converter (ADC) technology have enabled
higher sampling rates, making it feasible to adopt digital radar architectures
that directly sample the radio-frequency (RF) signal, eliminating the need for
analog downconversion. This digital approach supports greater flexibility in
waveform design and signal processing, particularly through digital modulation
schemes like orthogonal frequency division multiplexing (OFDM). This paper
presents a digital radar system mounted on an uncrewed aerial vehicle (UAV),
which employs OFDM waveforms for coherent multistatic synthetic aperture radar
(SAR) imaging in the L-band. The radar setup features a primary UAV node
responsible for signal transmission and monostatic data acquisition, alongside
secondary nodes that operate in a receive-only mode. These secondary nodes
capture the radar signal reflected from the scene as well as a direct sidelink
signal. RF signals from both the radar and sidelink paths are sampled and
processed offline. To manage data storage efficiently, a trigger mechanism is
employed to record only the relevant portions of the radar signal. The system
maintains coherency in both fast-time and slow-time domains, which is essential
for multistatic SAR imaging. Because the secondary nodes are passive, the
system can be easily scaled to accommodate a larger swarm of UAVs. The paper
details the full signal processing workflow for both monostatic and multistatic
SAR image formation, including an analysis and correction of synchronization
errors that arise from the uncoupled operation of the nodes. The proposed
coherent processing method is validated through static radar measurements,
demonstrating coherency achieved by the concept. Additionally, a UAV-based
bistatic SAR experiment demonstrates the system's performance by producing
high-resolution monostatic, bistatic, and combined multistatic SAR images.

</details>


### [494] [Minimum Attention Control (MAC) in a Receding Horizon Framework with Applications](https://arxiv.org/abs/2507.20835)
*T. Ganesh Teja,Santhosh Kumar Varanasi,Phanindra Jampana*

Main category: eess.SY

TL;DR: MAC是一种控制技术，它通过最小化输入变化来满足控制目标。本文提出了MAMPC，它在零范数约束之外还考虑了阶段成本，并通过交替最小化算法求解，实现了稀疏控制。


<details>
  <summary>Details</summary>
Motivation: 为了在参考跟踪中考虑零范数约束和阶段成本，并将先前最优输入纳入当前优化问题。

Method: 提出了一种交替最小化算法来解决优化问题，其中外层是二次规划，内层具有解析解。

Result: 提出的MAMPC算法在四箱系统和燃料电池堆上进行了实现，并进行了详细的比较研究。

Conclusion: 与标准MPC相比，MAMPC在保持稀疏控制行为的同时，在跟踪误差方面存在一定的权衡。

Abstract: Minimum Attention Control (MAC) is a control technique that provides minimal
input changes to meet the control objective. Mathematically, the zero norm of
the input changes is used as a constraint for the given control objective and
minimized with respect to the process dynamics. In this paper, along with the
zero norm constraint, stage costs are also considered for reference tracking in
a receding horizon framework. For this purpose, the optimal inputs of the
previous horizons are also considered in the optimization problem of the
current horizon. An alternating minimization algorithm is applied to solve the
optimization problem (Minimum Attention Model Predictive Control (MAMPC)). The
outer step of the optimization is a quadratic program, while the inner step,
which solves for sparsity, has an analytical solution. The proposed algorithm
is implemented on two case studies: a four-tank system with slow dynamics and a
fuel cell stack with fast dynamics. A detailed comparative study of the
proposed algorithm with standard MPC indicates sparse control actions with a
tradeoff in the tracking error.

</details>


### [495] [dq Modeling for Series-Parallel Compensated Wireless Power Transfer Systems](https://arxiv.org/abs/2507.20921)
*Zixuan Jiang*

Main category: eess.SY

TL;DR: 该研究提出了一种用于串联-并联补偿无线能量传输系统的dq建模方法，解决了传统方法忽略瞬态过程的不足，提高了模型的精度和控制系统的开发效率。


<details>
  <summary>Details</summary>
Motivation: 大多数研究基于相量法并侧重于稳态分析，可能忽略了系统的瞬态过程。因此，有必要开发一种能够表征系统一阶动态特性的模型。

Method: 受电机驱动领域坐标变换的启发，提出了一种用于串联-并联补偿无线能量传输系统的dq建模方法。

Result: 仿真结果验证了该模型的有效性，表明该模型能够精确表征系统动力学，并有助于提高测量精度和控制系统开发。此外，还提出了一种基于dq模型的互感识别应用，以体现dq模型的价值。

Conclusion: 该研究提出的dq模型可以作为设计串联-并联补偿无线能量传输系统的有用工具。

Abstract: Series-parallel (SP) compensated wireless power transfer (WPT) systems are
widely used in some specific scenarios, such as bioelectronics and portable
electronics. However, most studies are based on the phasor method and focused
on the steady-state analysis, which may overlook the transient process of
systems. Accordingly, inspired by the notion of coordinate transformation in
the field of motor drive, this work develops a dq modeling method for SP
compensated WPT systems. The proposed model effectively characterizes
first-order system dynamics, facilitating enhanced measurement precision and
control system development. One measurement application, dq model-based mutual
inductance identification, is presented to reflect the value of the dq model.
Simulation results are shown to validate the model's effectiveness, indicating
that the developed model can be a good tool for the design of SP compensated
WPT systems.

</details>


### [496] [A lightweight numerical model for predictive control of borehole thermal energy storages](https://arxiv.org/abs/2507.20974)
*Johannes van Randenborgh,Steffen Daniel,Moritz Schulze Darup*

Main category: eess.SY

TL;DR: 本文提出了一种用于钻孔热能存储（BTES）的精确数值模型，该模型可实现易于求解的线性二次最优控制问题（OCP），从而最大化建筑物的供暖、通风和空调（HVAC）系统的运行效率。


<details>
  <summary>Details</summary>
Motivation: 为了最大化建筑中钻孔热能存储（BTES）的使用，需要一种能够实现动态交互的模型，但现有的BTES模型在精确性和易求解性之间存在权衡。

Method: 提出了一种精确的数值模型，可实现易于求解的线性二次最优控制问题（OCP）。

Result: 开发了一个精确的数值模型，可用于解决易于求解的线性二次最优控制问题（OCP）。

Conclusion: 本文提出了一种精确的数值模型，可用于解决易于求解的线性二次最优控制问题（OCP），从而最大化建筑中钻孔热能存储（BTES）的使用。

Abstract: Borehole thermal energy storage (BTES) can reduce the operation of fossil
fuel-based heating, ventilation, and air conditioning systems for buildings.
With BTES, thermal energy is stored via a borehole heat exchanger in the
ground. Model predictive control (MPC) may maximize the use of BTES by
achieving a dynamic interaction between the building and BTES. However,
modeling BTES for MPC is challenging, and a trade-off between model accuracy
and an easy-to-solve optimal control problem (OCP) must be found. This
manuscript presents an accurate numerical model yielding an easy-to-solve
linear-quadratic OCP.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [497] [Flexible Bidding in Service-Oriented Combinatorial Spectrum Forward Auctions](https://arxiv.org/abs/2507.19720)
*Xiang Shao,Wei Wang,Guan Gui*

Main category: cs.GT

TL;DR: 提出了一种灵活竞价组合前向拍卖方案，通过允许买方提交可调需求范围并引入SEM系数，能更好地适应动态频谱共享环境，显著提高资源效率和社会福利。


<details>
  <summary>Details</summary>
Motivation: 为解决传统组合频谱拍卖中固定的竞价和匹配流程限制参与者策略适应性并导致社会福利次优的问题。

Method: 1. 提出一种带有灵活竞价机制的新型近似真实组合前向拍卖方案。2. 买方提交包含基础频谱需求和可调需求范围的组合竞价。3. 引入频谱等效映射（SEM）系数以标准化跨异构频段的估值。4. 采用贪心匹配算法，根据买方等效单位竞价排序并进行资源分配。

Result: 仿真结果表明，所提出的灵活竞价机制在动态频谱共享场景中显著优于现有基准方法，实现了更高的社会福利。

Conclusion: 所提出的基于灵活竞价机制的组合前向拍卖方案能显著提高资源效率并最大化动态频谱共享环境下的社会福利。

Abstract: Traditional combinatorial spectrum auctions mainly rely on fixed bidding and
matching processes, which limit participants' ability to adapt their strategies
and often result in suboptimal social welfare in dynamic spectrum sharing
environments. To address these limitations, we propose a novel approximately
truthful combinatorial forward auction scheme with a flexible bidding mechanism
aimed at enhancing resource efficiency and maximizing social welfare. In the
proposed scheme, each buyer submits a combinatorial bid consisting of the base
spectrum demand and adjustable demand ranges, enabling the auctioneer to
dynamically optimize spectrum allocation in response to market conditions. To
standardize the valuation across heterogeneous frequency bands, we introduce a
Spectrum Equivalent Mapping (SEM) coefficient. A greedy matching algorithm is
employed to determine winning bids by sorting buyers based on their equivalent
unit bid prices and allocating resources within supply constraints. Simulation
results demonstrate that the proposed flexible bidding mechanism significantly
outperforms existing benchmark methods, achieving notably higher social welfare
in dynamic spectrum sharing scenarios.

</details>


### [498] [An Algorithm-to-Contract Framework without Demand Queries](https://arxiv.org/abs/2507.20038)
*Ilan Doron-Arad,Hadas Shachnai,Gilad Shmerler,Inbal Talgam-Cohen*

Main category: cs.GT

TL;DR: 该研究提出了一种新的框架，可以将解决组合问题的算法转化为合同设计问题，并在多种组合约束下实现了接近最优的近似效果。


<details>
  <summary>Details</summary>
Motivation: 研究在代理人代表委托人执行有成本的任务时，委托人如何设计合同来激励代理人，并探讨是否存在近似方案。

Method: 提出了一种“局部-全局”框架，该框架将预算最大化问题的近似方案提升为合同设计问题的近似方案。该框架不依赖于对需求预言机的黑盒访问。

Result: 证明了对于合同设计问题，存在最优乘法和加法近似方案。该框架应用于多维预算、预算拟阵和预算匹配约束等多种组合约束，近似效果接近纯粹的算法问题。此外，还提出了一种解决多代理人合同设置问题的方法。 (Essentially matching the best approximation for the purely algorithmic problem.)

Conclusion: 该研究为合同设计问题提供了

Abstract: Consider costly tasks that add up to the success of a project, and must be
fitted by an agent into a given time-frame. This is an instance of the classic
budgeted maximization problem, which admits an approximation scheme (FPTAS).
Now assume the agent is performing these tasks on behalf of a principal, who is
the one to reap the rewards if the project succeeds. The principal must design
a contract to incentivize the agent. Is there still an approximation scheme? In
this work, our ultimate goal is an algorithm-to-contract transformation, which
transforms algorithms for combinatorial problems (like budgeted maximization)
to tackle incentive constraints that arise in contract design. Our approach
diverges from previous works on combinatorial contract design by avoiding an
assumption of black-box access to a demand oracle.
  We first show how to "lift" the FPTAS for budgeted maximization to obtain the
best-possible multiplicative and additive FPTAS for the contract design
problem. We establish this through our "local-global" framework, in which the
"local" step is to (approximately) solve a two-sided strengthened variant of
the demand problem. The "global" step then utilizes the local one to find the
approximately optimal contract. We apply our framework to a host of
combinatorial constraints including multi-dimensional budgets, budgeted
matroid, and budgeted matching constraints. In all cases we achieve an
approximation essentially matching the best approximation for the purely
algorithmic problem. We also develop a method to tackle multi-agent contract
settings, where the team of working agents must abide to combinatorial
feasibility constraints.

</details>


### [499] [Fairness under Equal-Sized Bundles: Impossibility Results and Approximation Guarantees](https://arxiv.org/abs/2507.20899)
*Alviona Mancho,Evangelos Markakis,Nicos Protopapas*

Main category: cs.GT

TL;DR: 研究了固定数量物品分配的公平性问题，提出了 EFFX 的新定义，并进行了算法和不可能性的探索，在特定条件下实现了近似，并找到了纳什福利最大化算法的界限。


<details>
  <summary>Details</summary>
Motivation: 在必须为每个代理分配固定数量物品（例如，轮班或团队）的实际场景中，研究了公平分配不可分割物品的设置，并引入了新的公平性概念 EFF1 和 EFFX。

Method: 本研究探讨了具有基数约束的不可分割物品的公平分配问题，并引入了一种基于物品翻转或交换的 envy-freeness up to one/any flip (EFF1, EFFX) 的变体。研究提出了算法和不可能的结果，展示了 EFFX 和基于翻转的 EFFX 之间的差异。

Result: 该研究表明，在某些条件下（例如，共同的物品价值排名或有界的估值函数），可以实现 EFFX 的近似。同时，也证明了最大化纳什福利的算法可以保证 1/2-EFF1 分配，且此界限是紧的。研究还指出了标准技术在保证 EFFX 近似方面的局限性。

Conclusion: 我们证明了当代理人对物品价值有共同的排名时，可以实现 EFFX 的常数因子近似。此外，我们还证明了最大化纳什福利的算法可以保证 1/2-EFF1 分配，并且该界是紧的。

Abstract: We study the fair allocation of indivisible goods under cardinality
constraints, where each agent must receive a bundle of fixed size. This models
practical scenarios, such as assigning shifts or forming equally sized teams.
Recently, variants of envy-freeness up to one/any item (EF1, EFX) were
introduced for this setting, based on flips or exchanges of items. Namely, one
can define envy-freeness up to one/any flip (EFF1, EFFX), meaning that an agent
$i$ does not envy another agent $j$ after performing one or any one-item flip
between their bundles that improves the value of $i$.
  We explore algorithmic aspects of this notion, and our contribution is
twofold: we present both algorithmic and impossibility results, highlighting a
stark contrast between the classic EFX concept and its flip-based analogue.
First, we explore standard techniques used in the literature and show that they
fail to guarantee EFFX approximations. On the positive side, we show that we
can achieve a constant factor approximation guarantee when agents share a
common ranking over item values, based on the well-known envy cycle elimination
technique. This idea also leads to a generalized algorithm with approximation
guarantees when agents agree on the top $n$ items and their valuation functions
are bounded. Finally, we show that an algorithm that maximizes the Nash welfare
guarantees a 1/2-EFF1 allocation, and that this bound is tight.

</details>


### [500] [Behavioral Study of Dashboard Mechanisms](https://arxiv.org/abs/2507.20985)
*Paula Kayongo,Jessica Hullman,Jason Hartline*

Main category: cs.GT

TL;DR: 可视化仪表板有助于提高拍卖出价，但仍需考虑投标人的行为偏差，并使仪表板设计与推理模型相匹配。


<details>
  <summary>Details</summary>
Motivation: 评估可视化仪表板设计如何影响决策制定，特别是在拍卖环境中，并研究其对投标人偏好推断的影响。

Method: 通过行为实验评估不同仪表板设计在反向第一价格拍卖中的影响，比较了出价分配规则和预期效用可视化，并分析了仪表板如何影响投标人偏好的推断。

Result: 与出价分配规则相比，基于效用的可视化显著提高了出价，减少了投标人的认知负担。然而，投标人仍然系统性地低估了他们的出价。设计不当的仪表板会导致估计私人偏好的误差。

Conclusion: 可视化仪表板可以提高出价优化，但由于投标人对某些获胜的偏好，仍然存在系统性出价不足的问题。仪表板设计与计量经济学推理假设需要保持一致，以避免估计误差。

Abstract: Visualization dashboards are increasingly used in strategic settings like
auctions to enhance decision-making and reduce strategic confusion. This paper
presents behavioral experiments evaluating how different dashboard designs
affect bid optimization in reverse first-price auctions. Additionally, we
assess how dashboard designs impact the auction designer's ability to
accurately infer bidders' preferences within the dashboard mechanism framework.
We compare visualizations of the bid allocation rule, commonly deployed in
practice, to alternatives that display expected utility. We find that
utility-based visualizations significantly improve bidding by reducing
cognitive demands on bidders. However, even with improved dashboards, bidders
systematically under-shade their bids, driven by an implicit preference for
certain wins in uncertain settings. As a result, dashboard-based mechanisms
that assume fully rational or risk-neutral bidder responses to dashboards can
produce significant estimation errors when inferring private preferences, which
may lead to suboptimal allocations in practice. Explicitly modeling agents'
behavioral responses to dashboards substantially improves inference accuracy,
highlighting the need to align visualization design and econometric inference
assumptions in practice.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [501] [Multi-Attention Stacked Ensemble for Lung Cancer Detection in CT Scans](https://arxiv.org/abs/2507.20221)
*Uzzal Saha,Surya Prakash*

Main category: eess.IV

TL;DR: 该研究提出了一种基于集成深度学习模型的多层次注意力机制，用于肺结节的良恶性分类，在LIDC-IDRI数据集上取得了优于现有方法的性能，可辅助放射科医生进行肺癌筛查。


<details>
  <summary>Details</summary>
Motivation: 为了应对使用CT图像进行二元肺结节分类（良性与恶性）的挑战。

Method: 提出了一种多层次注意力堆叠集成深度神经网络模型，集成了EfficientNet V2 S、MobileViT XXS和DenseNet201三种预训练骨干网络，并为96x96像素输入定制了分类头。通过两阶段注意力机制学习模型和类别的权重，并使用轻量级元学习器优化最终预测。为解决类别不平衡和提高泛化能力，采用了动态焦点损失、MixUp数据增强和测试时数据增强。

Result: 在LIDC-IDRI数据集上取得了卓越的性能，准确率为98.09%，AUC为0.9961，错误率比最先进的方法降低了35%。模型在敏感性（98.73）和特异性（98.96）方面表现均衡，在放射科医生意见分歧较大的挑战性病例中表现尤为出色。

Conclusion: 该模型可作为放射科医生进行肺癌筛查的可靠、自动化辅助工具。

Abstract: In this work, we address the challenge of binary lung nodule classification
(benign vs malignant) using CT images by proposing a multi-level attention
stacked ensemble of deep neural networks. Three pretrained backbones --
EfficientNet V2 S, MobileViT XXS, and DenseNet201 -- are each adapted with a
custom classification head tailored to 96 x 96 pixel inputs. A two-stage
attention mechanism learns both model-wise and class-wise importance scores
from concatenated logits, and a lightweight meta-learner refines the final
prediction. To mitigate class imbalance and improve generalization, we employ
dynamic focal loss with empirically calculated class weights, MixUp
augmentation during training, and test-time augmentation at inference.
Experiments on the LIDC-IDRI dataset demonstrate exceptional performance,
achieving 98.09 accuracy and 0.9961 AUC, representing a 35 percent reduction in
error rate compared to state-of-the-art methods. The model exhibits balanced
performance across sensitivity (98.73) and specificity (98.96), with
particularly strong results on challenging cases where radiologist disagreement
was high. Statistical significance testing confirms the robustness of these
improvements across multiple experimental runs. Our approach can serve as a
robust, automated aid for radiologists in lung cancer screening.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [502] [A Lightweight Deep Learning-based Model for Ranking Influential Nodes in Complex Networks](https://arxiv.org/abs/2507.19702)
*Mohammed A. Ramadhan,Abdulhakeem O. Mohammed*

Main category: cs.SI

TL;DR: 1D-CGS是一种结合了1D-CNN和GraphSAGE的混合模型，用于高效地识别复杂网络中有影响力的节点。它通过节点度和平均邻居度特征提取局部模式和聚合邻域信息，在准确性和速度上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有识别复杂网络中有影响力节点的方法在准确性和计算效率之间存在权衡，为了解决这些挑战，需要一种既能保持高准确性又能高效运行的模型。

Method: 提出了一种名为1D-CGS的混合模型，该模型结合了一维卷积神经网络（1D-CNN）和GraphSAGE。模型使用节点度和平均邻居度作为输入特征，通过1D卷积提取局部模式，然后通过GraphSAGE层聚合邻域信息。节点排名任务被表述为回归问题，并使用易感-感染-恢复（SIR）模型生成真实影响力得分。

Result: 1D-CGS在12个真实网络上的实验评估显示，在排名准确性方面显著优于传统中心性测量和最新的深度学习模型，同时运行速度非常快。与表现最佳的深度学习基线相比， Kendall's Tau 相关性平均提高了4.73%，Jaccard相似性平均提高了7.67%。该模型还实现了0.99的平均单调性指数（MI）得分，并产生近乎完美的排名分布，表明其排名具有高度的独特性和区分度。

Conclusion: 1D-CGS在准确性和计算效率之间取得了良好的平衡，在真实网络上的实验表明，它在节点排名方面显著优于传统的中心性测量和最新的深度学习模型。该模型运行速度快，适用于大规模应用。

Abstract: Identifying influential nodes in complex networks is a critical task with a
wide range of applications across different domains. However, existing
approaches often face trade-offs between accuracy and computational efficiency.
To address these challenges, we propose 1D-CGS, a lightweight and effective
hybrid model that integrates the speed of one-dimensional convolutional neural
networks (1D-CNN) with the topological representation power of GraphSAGE for
efficient node ranking. The model uses a lightweight input representation built
on two straightforward and significant topological features: node degree and
average neighbor degree. These features are processed through 1D convolutions
to extract local patterns, followed by GraphSAGE layers to aggregate
neighborhood information. We formulate the node ranking task as a regression
problem and use the Susceptible-Infected-Recovered (SIR) model to generate
ground truth influence scores. 1D-CGS is initially trained on synthetic
networks generated by the Barabasi-Albert model and then applied to real world
networks for identifying influential nodes. Experimental evaluations on twelve
real world networks demonstrate that 1D-CGS significantly outperforms
traditional centrality measures and recent deep learning models in ranking
accuracy, while operating in very fast runtime. The proposed model achieves an
average improvement of 4.73% in Kendall's Tau correlation and 7.67% in Jaccard
Similarity over the best performing deep learning baselines. It also achieves
an average Monotonicity Index (MI) score 0.99 and produces near perfect rank
distributions, indicating highly unique and discriminative rankings.
Furthermore, all experiments confirm that 1D-CGS operates in a highly
reasonable time, running significantly faster than existing deep learning
methods, making it suitable for large scale applications.

</details>


### [503] [Modelling the Closed Loop Dynamics Between a Social Media Recommender System and Users' Opinions](https://arxiv.org/abs/2507.19792)
*Ella C. Davidson,Mengbin Ye*

Main category: cs.SI

TL;DR: 该研究建立了一个推荐系统与用户意见动态耦合的数学模型，并利用蒙特卡洛模拟进行研究。结果显示，意见分布、内容立场和病毒式传播对用户意见极化有不同程度的影响。


<details>
  <summary>Details</summary>
Motivation: 提出一个数学模型来研究推荐系统（RS）算法和内容消费者（用户）之间耦合动力学的研究。

Method: 通过蒙特卡洛模拟研究了推荐系统（RS）如何影响用户意见，以及用户意见如何反过来影响后续推荐内容。

Result: 研究发现，特定的意见分布更容易导致极化，内容立场对改变用户意见的效果有限，但创造病毒式传播的内容可以有效对抗意见极化。

Conclusion: 研究表明，不同的意见分布更容易导致极化，许多内容立场在改变用户意见方面效果不佳，而创造病毒式传播的内容是抵抗意见极化的有效措施。

Abstract: This paper proposes a mathematical model to study the coupled dynamics of a
Recommender System (RS) algorithm and content consumers (users). The model
posits that a large population of users, each with an opinion, consumes
personalised content recommended by the RS. The RS can select from a range of
content to recommend, based on users' past engagement, while users can engage
with the content (like, watch), and in doing so, users' opinions evolve. This
occurs repeatedly to capture the endless content available for user consumption
on social media. We employ a campaign of Monte Carlo simulations using this
model to study how recommender systems influence users' opinions, and in turn
how users' opinions shape the subsequent recommended content. We take an
interest in both the performance of the RS (e.g., how users engage with the
content) and the user's opinions, focusing on polarisation and radicalisation
of opinions. We find that different opinion distributions are more susceptible
to becoming polarised than others, many content stances are ineffective in
changing user opinions, and creating viral content is an effective measure in
combating polarisation of opinions.

</details>


### [504] [Studying Disinformation Narratives on Social Media with LLMs and Semantic Similarity](https://arxiv.org/abs/2507.20066)
*Chaytan Inman*

Main category: cs.SI

TL;DR: 本研究开发了用于检测、追踪和表征虚假信息的工具，通过分析推文与特定叙述的相似度来实现。


<details>
  <summary>Details</summary>
Motivation: 为了应对虚假信息中常见的细微差别和部分真实性，本研究旨在开发一种连续的相似度量方法，以检测虚假信息并捕捉其特征。

Method: 本研究开发了两种工具：追踪工具，用于评估推文与目标叙述的相似度并绘制时间线；叙事合成工具，用于聚类相似推文并生成主要叙事。这两种工具结合在一个推文叙事分析仪表板中。追踪工具在GLUE STS-B基准上进行了验证，并通过两个案例研究进行了进一步的实证验证。

Result: 通过案例研究，本研究证明了语义相似性在检测、追踪和表征细微差别的虚假信息方面的有效性。该研究开发了可用于虚假信息分析的工具，并已在指定网址提供。

Conclusion: 本研究通过开发和验证一套新的工具，为识别、追踪和表征包含细微差别和部分真实信息的虚假信息提供了方法。该方法在“2020年选举被窃取”和“跨性别者对社会有害”两个案例研究中得到了验证，证明了语义相似性在虚假信息检测中的有效性。

Abstract: This thesis develops a continuous scale measurement of similarity to
disinformation narratives that can serve to detect disinformation and capture
the nuanced, partial truths that are characteristic of it. To do so, two tools
are developed and their methodologies are documented. The tracing tool takes
tweets and a target narrative, rates the similarities of each to the target
narrative, and graphs it as a timeline. The second narrative synthesis tool
clusters tweets above a similarity threshold and generates the dominant
narratives within each cluster. These tools are combined into a Tweet Narrative
Analysis Dashboard. The tracing tool is validated on the GLUE STS-B benchmark,
and then the two tools are used to analyze two case studies for further
empirical validation. The first case study uses the target narrative "The 2020
election was stolen" and analyzes a dataset of Donald Trump's tweets during
2020. The second case study uses the target narrative, "Transgender people are
harmful to society" and analyzes tens of thousands of tweets from the media
outlets The New York Times, The Guardian, The Gateway Pundit, and Fox News.
Together, the empirical findings from these case studies demonstrate semantic
similarity for nuanced disinformation detection, tracing, and characterization.
  The tools developed in this thesis are hosted and can be accessed through the
permission of the author. Please explain your use case in your request. The
HTML friendly version of this paper is at
https://chaytanc.github.io/projects/disinfo-research (Inman, 2025).

</details>


### [505] [A Blockchain-Based Quality Control Model for Online Collaboration Systems](https://arxiv.org/abs/2507.20265)
*Sadegh Sohani,Maliheh Shahryari,Salar Ghazi,Mohammad Allahbakhsh,Haleh Amintoosi,Boualem Benatallah*

Main category: cs.SI

TL;DR: 提出了一种基于区块链的众包众包质量控制模型，通过半迭代算法计算质量分数和节点声誉，解决了中心化方法的痛点，并实现了与现有基线的可比性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决众包众包（CCG）中基于中心化架构的质量评估方法的安全、隐私和可用性问题，并应对学术信息计量学中的挑战，如引用操纵、协作学术的透明度以及度量计算中的去中心化信任。

Method: 提出了一种基于区块链的众包众包质量控制模型，该模型使用半迭代算法来相互计算众包众包的质量分数和节点的声誉。

Result: 与PageRank和HITS基线相比，该模型展示了相当的性能，并且在吞吐量、延迟和恶意节点鲁棒性方面也得到了证实。理论比较验证了其在现实世界中的信息计量应用的可行性。

Conclusion: 该模型展示了与PageRank和HITS基线相当的性能，并通过了对吞吐量、延迟和恶意节点鲁棒性的评估，证实了其可靠性。

Abstract: Collaborative content generation (CCG) enables collective creation of
artifacts like scientific articles. Quality is a paramount concern in CCG, and
a multitude of methods have been proposed to evaluate the quality of artifacts.
Nevertheless, the majority of these methods are reliant on centralized
architectures, which present challenges pertaining to security, privacy, and
availability. Blockchain technology proffers a potential resolution to these
challenges, by furnishing a decentralized and immutable ledger of quality
scores. In this manuscript, we introduce a blockchain-based quality control
model for CCG that uses a semi-iterative algorithm to interdependently compute
quality scores of artifacts and reputation of nodes. Our model addresses
critical challenges in academic informetrics, such as citation manipulation,
transparency in collaborative scholarship, and decentralized trust in metric
computation. Our model also exhibits sensitivity to processing latency,
rendering it more agile in the presence of delays. Our model's quality scores,
evaluated against PageRank and HITS baselines, show comparable performance,
with additional assessments of throughput, latency, and robustness against
malicious nodes confirming its reliability. A theoretical comparison with
recent studies validates its feasibility for real world informetric
application.

</details>


### [506] [Structural-Aware Key Node Identification in Hypergraphs via Representation Learning and Fine-Tuning](https://arxiv.org/abs/2507.20682)
*Xiaonan Ni,Guangyuan Mei,Su-Su Zhang,Yang Chen,Xin Xu,Chuang Liu,Xiu-Xiu Zhan*

Main category: cs.SI

TL;DR: AHGA是一个用于超图节点重要性评估的新框架，通过整合自动编码器、超图神经网络和主动学习，在识别高影响力节点方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的节点重要性评估方法通常依赖于传统的网络结构，未能捕捉到许多真实世界系统中固有的多关系交互。为了解决这一局限性，我们研究超图中的关键节点识别，其中高阶交互被自然地建模为超边。

Method: AHGA框架整合了用于提取高阶结构特征的自动编码器、基于超图神经网络的预训练模块（HGNN）以及基于主动学习的微调过程。

Result: AHGA outperforms classical centrality-based baselines by approximately 37.4%. The nodes identified by AHGA exhibit both high influence and strong structural disruption capability, demonstrating their superiority in detecting multifunctional nodes.

Conclusion: AHGA在八个经验超图上的广泛实验表明，AHGA的表现优于传统的基于中心性的基线约37.4%。此外，AHGA识别出的节点同时具有高影响力和强大的结构破坏能力，证明了它们在检测多功能节点方面的优越性。

Abstract: Evaluating node importance is a critical aspect of analyzing complex systems,
with broad applications in digital marketing, rumor suppression, and disease
control. However, existing methods typically rely on conventional network
structures and fail to capture the polyadic interactions intrinsic to many
real-world systems. To address this limitation, we study key node
identification in hypergraphs, where higher-order interactions are naturally
modeled as hyperedges. We propose a novel framework, AHGA, which integrates an
Autoencoder for extracting higher-order structural features, a HyperGraph
neural network-based pre-training module (HGNN), and an Active learning-based
fine-tuning process. This fine-tuning step plays a vital role in mitigating the
gap between synthetic and real-world data, thereby enhancing the model's
robustness and generalization across diverse hypergraph topologies. Extensive
experiments on eight empirical hypergraphs show that AHGA outperforms classical
centrality-based baselines by approximately 37.4%. Furthermore, the nodes
identified by AHGA exhibit both high influence and strong structural disruption
capability, demonstrating their superiority in detecting multifunctional nodes.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [507] [On growth and morphogenesis in mechanobiology](https://arxiv.org/abs/2507.20345)
*Angelo Rosario Carotenuto,Stefania Palumbo,Arsenio Cutolo,Massimiliano Fraldi*

Main category: cond-mat.soft

TL;DR: 本文提出了一种基于质量守恒的理论框架，用于研究各向异性生长、重塑和形态发生，能够更好地解释局部驱动因素和宏观形状形成，并整合了生物和化学力学相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有形变均一性理论在追溯生长、重塑和形态发生方面存在挑战，缺乏独立于本构关系或生长张量结构的统一生长定律，也未能充分联系局部各向异性生长、形态发生动力学、扩散现象和力学。现有框架主要将形状的出现归因于全局构型变化，忽视了在无应力条件下自发模式或变形的局部驱动因素。

Method: 本研究提出了一种理论方法，该方法从质量守恒这一唯一的动力学约束出发，重构了各向异性生长、重塑和形态发生。通过推广质量守恒方程，推导了质量分布和几何重构的演化定律。

Result: 所提出的理论方法揭示了质量传输和组织微观结构演变如何协同地指导生物、几何和生物物质的组成变化，其中等容生长成分在局部各向异性以及宏观自发形状发展中起着关键作用。该方法整合了生物系统固有的化学力学和生物相互作用。

Conclusion: 该研究提出了一个从第一性原理出发的理论方法，通过质量守恒作为唯一的动力学约束来重构各向异性生长、重塑和形态发生，揭示了质量传输和组织结构演变如何协同指导生物、几何和生物物质的组成变化，并认为等容生长成分能够协调局部各向异性，甚至宏观自发形状发展。该方法将化学力学和生物相互作用整合到连续介质力学框架中，为研究生长和形态发生的复杂情景提供了途径，并有望为生物力学和生物力学问题提供统一的视角，连接局部驱动因素与生物组织形式和形状的涌现。

Abstract: Morphoelasticity represents a foundational theory for tracing back growth,
remodelling, and morphogenesis, yet crucial challenges persist. A unified
growth law -- independent of a priori assumptions about constitutive relations
or specified structures of the growth tensor -- remains in fact elusive, as
does an intimate connection between local anisotropic growth, morphogenesis
dynamics, diffusion phenomena and mechanics. When anisotropy of growth is not
prescribed arbitrarily, current frameworks mainly attribute shape emergence to
growth-induced global configurational switches, somehow neglecting the
existence of local drivers of spontaneous patterning or distorsions also in
stress-free conditions. To overcome these limitations, in this work we propose
a theoretical approach that reformulates anisotropic growth, remodeling, and
morphogenesis starting from first principles, grounded in mass balance. In
particular, by positing mass conservation as the sole dynamic constraint for
the growth problem, we generalize the mass balance equation to derive evolution
laws for mass distribution and geometric reconfiguration. This reveals how mass
transport and associated microstructural evolution of the tissue fabric
synergistically guide biological, geometric, and constitutive changes of living
matter, with the isochoric component of growth orchestrating local anisotropy
and, at the macroscale, even spontaneous shape development. The proposed fully
coupled modelling approach integrates chemo-mechanical and biological
interactions inherent to living systems, offering a pathway to investigate
complex scenarios in growth and morphogenesis, redefining the latter in a
Continuum Mechanics framework. It is felt that this strategy could help to take
a step forward a unified perspective for biomechanical and mechanobiological
problems, bridging scales from local drivers to emergent tissue forms and
shapes.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [508] [MCP4EDA: LLM-Powered Model Context Protocol RTL-to-GDSII Automation with Backend Aware Synthesis Optimization](https://arxiv.org/abs/2507.19570)
*Yiting Wang,Wanghao Ye,Yexiao He,Yiran Chen,Gang Qu,Ang Li*

Main category: cs.AR

TL;DR: MCP4EDA 是一个革命性的 EDA 工具，它利用大型语言模型（LLM）通过自然语言控制整个开源 RTL-到-GDSII 设计流程，并在实际设计中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了实现通过自然语言交互来控制和优化完整的开源 RTL-到-GDSII 设计流程，并弥合传统综合估计与物理实现现实之间的差距。

Method:  MCP4EDA 集成了 Yosys 合成、Icarus Verilog 仿真、OpenLane 放置与布线、GTKWave 分析和 KLayout 可视化，并引入了后端感知合成优化方法论，利用实际的版图后时序、功耗和面积指标来迭代优化合成 TCL 脚本，实现了闭环优化。

Result:  MCP4EDA 在时序收敛方面提高了 15-30%，在面积方面降低了 10-20%，是首个实用的、由 LLM 控制的端到端开源 EDA 自动化系统。

Conclusion: MCP4EDA 是首个能够通过自然语言交互来控制和优化完整开源 RTL-到-GDSII 设计流程的模型上下文协议服务器，通过实验评估证明了其在时序收敛和面积优化方面的优越性。

Abstract: This paper presents MCP4EDA, the first Model Context Protocol server that
enables Large Language Models (LLMs) to control and optimize the complete
open-source RTL-to-GDSII design flow through natural language interaction. The
system integrates Yosys synthesis, Icarus Verilog simulation, OpenLane
place-and-route, GTKWave analysis, and KLayout visualization into a unified
LLM-accessible interface, enabling designers to execute complex multi-tool EDA
workflows conversationally via AI assistants such as Claude Desktop and Cursor
IDE. The principal contribution is a backend-aware synthesis optimization
methodology wherein LLMs analyze actual post-layout timing, power, and area
metrics from OpenLane results to iteratively refine synthesis TCL scripts,
establishing a closed-loop optimization system that bridges the traditional gap
between synthesis estimates and physical implementation reality. In contrast to
conventional flows that rely on wire-load models, this methodology leverages
real backend performance data to guide synthesis parameter tuning, optimization
sequence selection, and constraint refinement, with the LLM functioning as an
intelligent design space exploration agent. Experimental evaluation on
representative digital designs demonstrates 15-30% improvements in timing
closure and 10-20% area reduction compared to default synthesis flows,
establishing MCP4EDA as the first practical LLM-controlled end-to-end
open-source EDA automation system. The code and demo are avaiable at:
http://www.agent4eda.com/

</details>


### [509] [ChipletPart: Scalable Cost-Aware Partitioning for 2.5D Systems](https://arxiv.org/abs/2507.19819)
*Alexander Graening,Puneet Gupta,Andrew B. Kahng,Bodhisatta Pramanik,Zhiang Wang*

Main category: cs.AR

TL;DR: ChipletPart 是一种创新的 Chiplet 分区工具，通过结合遗传算法和模拟退火，显著降低了 Chiplet 成本并提高了分区可行性，同时开源了相关工具。


<details>
  <summary>Details</summary>
Motivation: 随着 Chiplet 在高性能系统设计中的应用日益广泛，如何有效地进行系统分区以降低成本并满足 Chiplet 系统的独特约束（如接口限制和异构制造）变得至关重要。

Method: ChipletPart 采用了一种结合遗传算法进行技术分配和分区，以及模拟退火进行 Chiplet 布局规划的成本驱动 2.5D 系统分区方法。它还整合了一个复杂的 Chiplet 成本模型，并考虑了 Chiplet 系统特有的约束，如目标函数、接口收发器的覆盖范围以及异构制造技术分配。

Result: ChipletPart 将 Chiplet 成本降低了高达 58%（几何平均值为 20%），相比之下，现有的最小割分区器常常产生无法实现布局的解决方案。与之前的 Floorplet 工作相比，ChipletPart 生成的分区成本降低了高达 47%（几何平均值为 6%）。异构集成相比同构实现可降低高达 43%（几何平均值为 15%）的成本。

Conclusion: ChipletPart 成功地解决了 Chiplet 系统设计中的成本和可行性问题，通过其创新的遗传算法和模拟退火相结合的方法，显著降低了 Chiplet 的成本，并优于现有技术。此外，研究还强调了异构集成的重要性，并开源了相关工具以促进社区发展。

Abstract: Industry adoption of chiplets has been increasing as a cost-effective option
for making larger high-performance systems. Consequently, partitioning large
systems into chiplets is increasingly important. In this work, we introduce
ChipletPart - a cost-driven 2.5D system partitioner that addresses the unique
constraints of chiplet systems, including complex objective functions, limited
reach of inter-chiplet I/O transceivers, and the assignment of heterogeneous
manufacturing technologies to different chiplets. ChipletPart integrates a
sophisticated chiplet cost model with its underlying genetic algorithm-based
technology assignment and partitioning methodology, along with a simulated
annealing-based chiplet floorplanner. Our results show that: (i) ChipletPart
reduces chiplet cost by up to 58% (20% geometric mean) compared to
state-of-the-art min-cut partitioners, which often yield floorplan-infeasible
solutions; (ii) ChipletPart generates partitions with up to 47% (6% geometric
mean) lower cost as compared to the prior work Floorplet; and (iii) for the
testcases we study, heterogeneous integration reduces cost by up to 43% (15%
geometric mean) compared to homogeneous implementations. We also present case
studies that show how changes in packaging or inter-chiplet signaling
technologies can affect partitioning solutions. Finally, we make ChipletPart,
the underlying chiplet cost model, and a chiplet testcase generator available
as open-source tools for the community.

</details>


### [510] [AxOSyn: An Open-source Framework for Synthesizing Novel Approximate Arithmetic Operators](https://arxiv.org/abs/2507.20007)
*Siva Satyendra Sahoo,Salim Ullah,Akash Kumar*

Main category: cs.AR

TL;DR: AxOSyn是一个开源框架，用于在各种抽象级别上探索近似计算中的近似算术运算符（AxOs）的设计空间，以实现能量效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有设计空间探索（DSE）框架在近似计算中对选择方法或固定抽象级别的定制合成的限制，并克服现有工具在探索不同近似模型和扩展分析到不同粒度方面的局限性，本研究提出了AxOSyn框架。

Method: AxOSyn是一个开源框架，支持在各种抽象级别上进行选择和综合方法，并允许用户集成自定义方法来评估近似值，从而在运算符级别和应用程序特定级别上进行设计空间探索。

Result: AxOSyn框架支持在各种抽象级别上进行选择和综合方法，并允许研究人员集成自定义方法来评估近似值，从而在运算符级别和应用程序特定级别上进行设计空间探索，最终实现能量效率高的近似运算符。

Conclusion: AxOSyn框架为设计能量效率高的近似运算符提供了一种有效的方法。

Abstract: Edge AI deployments are becoming increasingly complex, necessitating
energy-efficient solutions for resource-constrained embedded systems.
Approximate computing, which allows for controlled inaccuracies in
computations, is emerging as a promising approach for improving power and
energy efficiency. Among the key techniques in approximate computing are
approximate arithmetic operators (AxOs), which enable application-specific
optimizations beyond traditional computer arithmetic hardware reduction-based
methods, such as quantization and precision scaling. Existing design space
exploration (DSE) frameworks for approximate computing limit themselves to
selection-based approaches or custom synthesis at fixed abstraction levels,
which restricts the flexibility required for finding application-specific
optimal solutions. Further, the tools available for the DSE of AxOs are quite
limited in terms of exploring different approximation models and extending the
analysis to different granularities. To this end, we propose AxOSyn, an
open-source framework for the DSE of AxOs that supports both selection and
synthesis approaches at various abstraction levels. AxOSyn allows researchers
to integrate custom methods for evaluating approximations and facilitates DSE
at both the operator-level and application-specific. Our framework provides an
effective methodology for achieving energy-efficient, approximate operators.

</details>


### [511] [RoCE BALBOA: Service-enhanced Data Center RDMA for SmartNICs](https://arxiv.org/abs/2507.20412)
*Maximilian Jakob Heer,Benjamin Ramhorst,Yu Zhu,Luhao Liu,Zhiyi Hu,Jonas Dann,Gustavo Alonso*

Main category: cs.AR

TL;DR: RoCE BALBOA 是一个开源的、可定制的 RDMA 协议栈，可在 FPGA 等加速器上实现，性能媲美商用网卡，并能支持多种网络功能和计算卸载应用。


<details>
  <summary>Details</summary>
Motivation: 数据中心中数据密集型应用（尤其是机器学习）对网络性能提出了更高要求，促使开发更高效的网络协议和基础设施，如 RDMA 和网络功能加速。

Method: BALBOA 是一个开源的、兼容 RoCE v2 的 RDMA 协议栈，支持数百个队列对和 100G 网络，并可在 FPGA 上实现。

Result: 在 FPGA 集群上部署 BALBOA，其延迟和性能表现与商用网卡相当，并成功演示了两种用例：增强基础设施功能（加密、深度包检测）和实现线速计算卸载（推荐系统数据预处理）。

Conclusion: BALBOA 的实现和部署展示了其作为构建加速器和智能网卡基础的潜力，能够在网络数据流上进行操作，并支持加密、深度包检测和推荐系统的数据预处理等用例。

Abstract: Data-intensive applications in data centers, especially machine learning
(ML), have made the network a bottleneck, which in turn has motivated the
development of more efficient network protocols and infrastructure. For
instance, remote direct memory access (RDMA) has become the standard protocol
for data transport in the cloud as it minimizes data copies and reduces
CPU-utilization via host-bypassing. Similarly, an increasing amount of network
functions and infrastructure have moved to accelerators, SmartNICs, and
in-network computing to bypass the CPU. In this paper we explore the
implementation and deployment of RoCE BALBOA, an open-source, RoCE
v2-compatible, scalable up to hundreds of queue-pairs, and 100G-capable
RDMA-stack that can be used as the basis for building accelerators and
smartNICs. RoCE BALBOA is customizable, opening up a design space and offering
a degree of adaptability not available in commercial products. We have deployed
BALBOA in a cluster using FPGAs and show that it has latency and performance
characteristics comparable to commercial NICs. We demonstrate its potential by
exploring two classes of use cases. One involves enhancements to the protocol
for infrastructure purposes (encryption, deep packet inspection using ML). The
other showcases the ability to perform line-rate compute offloads with deep
pipelines by implementing commercial data preprocessing pipelines for
recommender systems that process the data as it arrives from the network before
transferring it directly to the GPU. These examples demonstrate how BALBOA
enables the exploration and development of SmartNICs and accelerators operating
on network data streams.

</details>


### [512] [Demystifying the 7-D Convolution Loop Nest for Data and Instruction Streaming in Reconfigurable AI Accelerators](https://arxiv.org/abs/2507.20420)
*Md Rownak Hossain Chowdhury,Mostafizur Rahman*

Main category: cs.AR

TL;DR: 提出一种新框架，将卷积重构为数据流问题，无需复杂变换即可实现高效加速，并在VGG-16上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有的空间计算方法（如CGRA和FPGA）在处理卷积时，通常依赖于循环展开或基于GEMM的矩阵变换，这会带来显著的数据移动和指令控制开销。

Method: 将7维卷积循环嵌套重解释为面向硬件的数据和指令流问题，通过暴露其由计算单元分布、互连拓扑和可重构性等硬件参数决定的空间和时间映射，从而实现卷积操作的轻量级、灵活部署。

Result: 在MAVeC加速器上实现了90%以上的PE利用率、显著的片内重用，并且对于端到端的VGG-16推理，可扩展吞吐量高达1.56 TFLOPs/sec和12.7 KIPS，验证了该方法在最小化控制开销、改善数据局部性和实现高效大规模卷积执行方面的有效性。

Conclusion: 该方法通过将7维卷积循环嵌套重新解释为面向硬件的数据和指令流问题，实现了轻量级、灵活的卷积部署，无需重量级变换或重排序方案。在MAVeC加速器上进行的实验以及对VGG-16的扩展应用证明了该方法的有效性。

Abstract: Convolution remains the most compute-intensive operation in AI acceleration,
often constituting over 80-90% of the workload. Existing approaches in spatial
architectures such as coarse-grained reconfigurable arrays (CGRAs) and
field-programmable gate arrays (FPGAs) frequently rely on loop unrolling or
GEMM-based matrix transformations, introducing significant overhead in both
data movement and instruction control. This paper presents a new framework
designed to systematically demystify the 7-dimensional convolution loop nest by
reinterpreting it as a hardware-centric data and instruction streaming problem.
Instead of treating the loop nest as a fixed computational construct, our
approach exposes its structure as a set of spatial and temporal mappings
governed by hardware parameters such as compute element distribution,
interconnect topology, and reconfigurability. This abstraction supports
lightweight, flexible deployment of convolution without reliance on heavyweight
transformations or reordering schemes. We demonstrate the application of our
approach on the MAVeC accelerator. We detail the implementation of convolution
operations in MAVeC and extend the framework to support full model execution on
VGG-16. Our profiling reveals high PE utilization (over 90%), significant fold
reuse, and scalable throughput up to 1.56 TFLOPs/sec and 12.7 KIPS for
end-to-end VGG-16 inference. These results validate the efficacy of our
approach in minimizing control overhead, improving data locality, and enabling
efficient large-scale convolution execution without reliance on conventional
transformation-based methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [513] [Computational Advantages of Multi-Grade Deep Learning: Convergence Analysis and Performance Insights](https://arxiv.org/abs/2507.20351)
*Ronglong Fang,Yuesheng Xu*

Main category: cs.LG

TL;DR: MGDL 在图像处理任务中优于 SGDL，具有更好的稳定性和对学习率的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了研究 MGDL 相对于 SGDL 的计算优势，尤其是在图像回归、去噪和去模糊任务中的性能。

Method: 通过分析 MGDL 和 SGDL 的梯度下降（GD）方法的收敛性，并研究与 GD 迭代相关的雅可比矩阵的特征值分布来研究 MGDL 的计算优势。

Result: MGDL 在图像回归、去噪和去模糊任务中的性能优于 SGDL，并且对学习率的选择更具鲁棒性。

Conclusion: MGDL 比 SGDL 更具鲁棒性，对学习率的选择不太敏感，并且具有更好的训练稳定性，这在数学上得到了证明。

Abstract: Multi-grade deep learning (MGDL) has been shown to significantly outperform
the standard single-grade deep learning (SGDL) across various applications.
This work aims to investigate the computational advantages of MGDL focusing on
its performance in image regression, denoising, and deblurring tasks, and
comparing it to SGDL. We establish convergence results for the gradient descent
(GD) method applied to these models and provide mathematical insights into
MGDL's improved performance. In particular, we demonstrate that MGDL is more
robust to the choice of learning rate under GD than SGDL. Furthermore, we
analyze the eigenvalue distributions of the Jacobian matrices associated with
the iterative schemes arising from the GD iterations, offering an explanation
for MGDL's enhanced training stability.

</details>


### [514] [Diagonally-Weighted Generalized Method of Moments Estimation for Gaussian Mixture Modeling](https://arxiv.org/abs/2507.20459)
*Liu Zhang,Oscar Mickelin,Sheng Xu,Amit Singer*

Main category: cs.LG

TL;DR: DGMM是一种改进的矩估计方法，在高维数据场景下比传统的MM和GMM更有效率且速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有的矩方法（MM）和广义矩方法（GMM）在高维数据或需要高阶矩时存在计算复杂度和存储复杂度呈指数增长的问题，这使得它们在这些场景下不切实际。特别是GMM还需要估计一个大的权重矩阵，这使得计算瓶颈更加严重。

Method: 提出了一种对角加权广义矩方法（DGMM），用于解决高维数据或需要高阶矩的场景下矩方法（MM）和广义矩方法（GMM）的计算复杂性和存储复杂性的瓶颈。DGMM通过对矩进行对角加权来优化统计效率，并设计了一种无需显式计算或存储矩张量即可获得DGMM估计量的计算高效且数值稳定的算法。

Result: DGMM在数值研究中，与MM和GMM相比，实现了更小的估计误差，同时运行时间也大大缩短。DGMM通过对矩进行对角加权，实现了统计效率、计算复杂度和数值稳定性之间的平衡。

Conclusion: DGMM在参数估计问题上比MM和GMM具有更小的估计误差和更短的运行时间，实现了统计效率、计算复杂度和数值稳定性之间的平衡。

Abstract: Since Pearson [Philosophical Transactions of the Royal Society of London. A,
185 (1894), pp. 71-110] first applied the method of moments (MM) for modeling
data as a mixture of one-dimensional Gaussians, moment-based estimation methods
have proliferated. Among these methods, the generalized method of moments (GMM)
improves the statistical efficiency of MM by weighting the moments
appropriately. However, the computational complexity and storage complexity of
MM and GMM grow exponentially with the dimension, making these methods
impractical for high-dimensional data or when higher-order moments are
required. Such computational bottlenecks are more severe in GMM since it
additionally requires estimating a large weighting matrix. To overcome these
bottlenecks, we propose the diagonally-weighted GMM (DGMM), which achieves a
balance among statistical efficiency, computational complexity, and numerical
stability. We apply DGMM to study the parameter estimation problem for weakly
separated heteroscedastic low-rank Gaussian mixtures and design a
computationally efficient and numerically stable algorithm that obtains the
DGMM estimator without explicitly computing or storing the moment tensors. We
implement the proposed algorithm and empirically validate the advantages of
DGMM: in numerical studies, DGMM attains smaller estimation errors while
requiring substantially shorter runtime than MM and GMM. The code and data will
be available upon publication at https://github.com/liu-lzhang/dgmm.

</details>


### [515] [NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks](https://arxiv.org/abs/2401.13330)
*Matteo Gambella,Jary Pomponi,Simone Scardapane,Manuel Roveri*

Main category: cs.LG

TL;DR: NACHOS 是一个用于自动设计早期退出神经网络（EENN）的框架，可以满足硬件约束，并找到准确率和计算量之间的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 当前 EENN 的设计过程由专家手动完成，过程复杂且耗时。为了自动化这一过程，研究人员正在探索使用神经架构搜索（NAS）。然而，现有的针对 EENN 的 NAS 解决方案有限，并且能够同时考虑主干网络和 EECs 的联合自动化设计策略仍然是一个开放性问题。

Method: 提出了一种名为 NACHOS 的神经架构搜索（NAS）框架，用于自动设计满足硬件约束（准确率和 MAC 操作数量）的早期退出神经网络（EENN）。NACHOS 能够联合设计主干网络和早期退出分类器（EEC），以在准确率和 MAC 操作数量之间找到最优的帕累托最优解。此外，该研究还探讨了两种新颖的正则化项对 EENN 辅助分类器优化的有效性。

Result: NACHOS 设计的模型在准确率和 MAC 操作数量方面具有竞争力，能够满足硬件约束，并在准确率和 MAC 操作数量之间实现了最佳的权衡。

Conclusion: NACHOS 是首个用于设计满足准确率和 MAC 操作数量约束的 EENN 的 NAS 框架，可针对准确率和 MAC 操作数量之间的权衡提供最优解，并且所设计的模型与当前最先进的 EENN 具有竞争力。

Abstract: Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN)
with Early Exit Classifiers (EECs), to provide predictions at intermediate
points of the processing when enough confidence in classification is achieved.
This leads to many benefits in terms of effectiveness and efficiency.
Currently, the design of EENNs is carried out manually by experts, a complex
and time-consuming task that requires accounting for many aspects, including
the correct placement, the thresholding, and the computational overhead of the
EECs. For this reason, the research is exploring the use of Neural Architecture
Search (NAS) to automatize the design of EENNs. Currently, few comprehensive
NAS solutions for EENNs have been proposed in the literature, and a fully
automated, joint design strategy taking into consideration both the backbone
and the EECs remains an open problem. To this end, this work presents Neural
Architecture Search for Hardware Constrained Early Exit Neural Networks
(NACHOS), the first NAS framework for the design of optimal EENNs satisfying
constraints on the accuracy and the number of Multiply and Accumulate (MAC)
operations performed by the EENNs at inference time. In particular, this
provides the joint design of backbone and EECs to select a set of admissible
(i.e., respecting the constraints) Pareto Optimal Solutions in terms of best
tradeoff between the accuracy and number of MACs. The results show that the
models designed by NACHOS are competitive with the state-of-the-art EENNs.
Additionally, this work investigates the effectiveness of two novel
regularization terms designed for the optimization of the auxiliary classifiers
of the EENN

</details>


### [516] [Aggregation-aware MLP: An Unsupervised Approach for Graph Message-passing](https://arxiv.org/abs/2507.20127)
*Xuanting Xie,Bingheng Li,Erlin Pan,Zhao Kang,Wenyu Chen*

Main category: cs.LG

TL;DR: 提出了一种名为“聚合感知多层感知机”（AMLP）的无监督框架，通过使MLP适应聚合过程来解决GNN中聚合函数选择的刚性问题，并在节点聚类和分类任务中取得了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有GNN在聚合函数选择上的刚性问题，尤其是在异质图中，以及现有方法对标签数据的依赖。

Method: AMLP框架包含两个关键步骤：1. 使用图重建方法促进高阶分组效应；2. 采用单层网络编码不同程度的异质性。

Result: AMLP框架在节点聚类和分类任务上展现出优越性能。

Conclusion: AMLP在节点聚类和分类任务上表现出色，证明了其在各种图学习场景中的潜力。

Abstract: Graph Neural Networks (GNNs) have become a dominant approach to learning
graph representations, primarily because of their message-passing mechanisms.
However, GNNs typically adopt a fixed aggregator function such as Mean, Max, or
Sum without principled reasoning behind the selection. This rigidity,
especially in the presence of heterophily, often leads to poor, problem
dependent performance. Although some attempts address this by designing more
sophisticated aggregation functions, these methods tend to rely heavily on
labeled data, which is often scarce in real-world tasks. In this work, we
propose a novel unsupervised framework, "Aggregation-aware Multilayer
Perceptron" (AMLP), which shifts the paradigm from directly crafting
aggregation functions to making MLP adaptive to aggregation. Our lightweight
approach consists of two key steps: First, we utilize a graph reconstruction
method that facilitates high-order grouping effects, and second, we employ a
single-layer network to encode varying degrees of heterophily, thereby
improving the capacity and applicability of the model. Extensive experiments on
node clustering and classification demonstrate the superior performance of
AMLP, highlighting its potential for diverse graph learning scenarios.

</details>


### [517] [VAE-GAN Based Price Manipulation in Coordinated Local Energy Markets](https://arxiv.org/abs/2507.19844)
*Biswarup Mukherjee,Li Zhou,S. Gokul Krishnan,Milad Kabirifar,Subhash Lakshminarayana,Charalambos Konstantinou*

Main category: cs.LG

TL;DR: 本研究提出一种基于MADDPG的强化学习模型，用于协调产消者参与本地能源市场。研究发现，价格操纵会导致产消者损失，但市场规模的扩大能促进合作、稳定交易并提升公平性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在提出一种协调产消者参与本地能源市场的模型，以应对其异构的分布式能源（DER）以及市场操纵行为，实现最优的能源交易和市场效率。

Method: 本研究提出了一种基于多智能体深度确定性策略梯度（MADDPG）的无模型强化学习方法，用于协调具有异构分布式能源（DER）的产消者参与本地能源市场（LEM）。此外，研究还探索了一种利用变分自编码器-生成对抗网络（VAE-GAN）的价格操纵策略，以评估其对产消者的经济影响。

Result: 结果表明，在价格操纵策略下，缺乏发电能力的异构产消者群体以及不同规模的市场中都会遭受经济损失。然而，随着市场规模的扩大，产消者之间的合作逐渐显现，交易趋于稳定，公平性也得到提升。

Conclusion: 该模型在不同规模的能源市场中，通过激励异构产消者（尤其是缺乏发电能力者）在竞争中实现合作，能够提高市场交易的稳定性和公平性，即使在面对价格操纵策略时也能实现。

Abstract: This paper introduces a model for coordinating prosumers with heterogeneous
distributed energy resources (DERs), participating in the local energy market
(LEM) that interacts with the market-clearing entity. The proposed LEM scheme
utilizes a data-driven, model-free reinforcement learning approach based on the
multi-agent deep deterministic policy gradient (MADDPG) framework, enabling
prosumers to make real-time decisions on whether to buy, sell, or refrain from
any action while facilitating efficient coordination for optimal energy trading
in a dynamic market. In addition, we investigate a price manipulation strategy
using a variational auto encoder-generative adversarial network (VAE-GAN)
model, which allows utilities to adjust price signals in a way that induces
financial losses for the prosumers. Our results show that under adversarial
pricing, heterogeneous prosumer groups, particularly those lacking generation
capabilities, incur financial losses. The same outcome holds across LEMs of
different sizes. As the market size increases, trading stabilizes and fairness
improves through emergent cooperation among agents.

</details>


### [518] [Directly Learning Stock Trading Strategies Through Profit Guided Loss Functions](https://arxiv.org/abs/2507.19639)
*Devroop Kar,Zimeng Lyu,Sheeraja Rajakrishnan,Hao Zhang,Alex Ororbia,Travis Desell,Daniel Krutz*

Main category: cs.LG

TL;DR: 提出新损失函数，提高股票交易策略的盈利能力，在回测中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了应对股票市场的高度波动性，提高交易决策的准确性和盈利能力。

Method: 提出四种新颖的损失函数，并使用这些损失函数训练了Crossformer、PPO和DDPG等时间序列模型，以直接学习有效的交易策略。

Result: 在2021、2022和2023年三个测试期内，使用最佳损失函数的Crossformer模型分别实现了51.42%、51.04%和48.62%的回报率，优于PPO和DDPG等强化学习方法（最高分别为41%、2.81%和41.58%）。

Conclusion: 所提出的四种新颖的损失函数能够驱动投资组合的决策制定，并能在高波动性的股票市场中生成显著的利润。

Abstract: Stock trading has always been a challenging task due to the highly volatile
nature of the stock market. Making sound trading decisions to generate profit
is particularly difficult under such conditions. To address this, we propose
four novel loss functions to drive decision-making for a portfolio of stocks.
These functions account for the potential profits or losses based with respect
to buying or shorting respective stocks, enabling potentially any artificial
neural network to directly learn an effective trading strategy. Despite the
high volatility in stock market fluctuations over time, training time-series
models such as transformers on these loss functions resulted in trading
strategies that generated significant profits on a portfolio of 50 different
S&P 500 company stocks as compared to a benchmark reinforcment learning
techniques and a baseline buy and hold method. As an example, using 2021, 2022
and 2023 as three test periods, the Crossformer model adapted with our best
loss function was most consistent, resulting in returns of 51.42%, 51.04% and
48.62% respectively. In comparison, the best performing state-of-the-art
reinforcement learning methods, PPO and DDPG, only delivered maximum profits of
around 41%, 2.81% and 41.58% for the same periods. The code is available at
https://anonymous.4open.science/r/bandit-stock-trading-58C8/README.md.

</details>


### [519] [Strategic Filtering for Content Moderation: Free Speech or Free of Distortion?](https://arxiv.org/abs/2507.20061)
*Saba Ahmadi,Avrim Blum,Haifeng Xu,Fan Yao*

Main category: cs.LG

TL;DR: 在社交媒体审核中，如何在保障言论自由的同时减少内容操纵是一个难题。本文利用机制设计，提出了一种近似最优解决方案，以平衡言论自由和减少内容操纵，并提供了所需数据的保证。


<details>
  <summary>Details</summary>
Motivation: 社交媒体用户生成内容（UGC）易受煽动和操纵，需要有效的监管。现有审核机制需在保障言论自由和减少社会失真之间取得平衡，但最优权衡难以确定。

Method: 本文采用机制设计的方法，旨在优化内容审核中社会失真（内容操纵总量）和言论自由（最小化表达限制）之间的权衡。

Result: 尽管确定最优权衡是NP-hard问题，但研究提出了实际可行的方法来近似最优解，并给出了决定有效近似最优审核器所需有限离线数据的泛化保证。

Conclusion: 该研究提出了一个基于机制设计的框架，用于优化在线内容审核中言论自由和减少内容操纵之间的平衡，并提供了一种近似最优解决方案的方法和泛化保证。

Abstract: User-generated content (UGC) on social media platforms is vulnerable to
incitements and manipulations, necessitating effective regulations. To address
these challenges, those platforms often deploy automated content moderators
tasked with evaluating the harmfulness of UGC and filtering out content that
violates established guidelines. However, such moderation inevitably gives rise
to strategic responses from users, who strive to express themselves within the
confines of guidelines. Such phenomena call for a careful balance between: 1.
ensuring freedom of speech -- by minimizing the restriction of expression; and
2. reducing social distortion -- measured by the total amount of content
manipulation. We tackle the problem of optimizing this balance through the lens
of mechanism design, aiming at optimizing the trade-off between minimizing
social distortion and maximizing free speech. Although determining the optimal
trade-off is NP-hard, we propose practical methods to approximate the optimal
solution. Additionally, we provide generalization guarantees determining the
amount of finite offline data required to approximate the optimal moderator
effectively.

</details>


### [520] [Online Learning with Probing for Sequential User-Centric Selection](https://arxiv.org/abs/2507.20112)
*Tianyi Xu,Yiting Chen,Henger Li,Zheyong Bian,Emiliano Dall'Anese,Zizhan Zheng*

Main category: cs.LG

TL;DR: This paper introduces the PUCS framework for sequential decision-making with costly information gathering. It offers efficient algorithms for both offline (known distributions) and online (unknown distributions) scenarios, providing theoretical guarantees and demonstrating practical success through real-world experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from sequential decision-making problems where information about resources and rewards is initially unknown, and acquiring this information through probing is costly. Such problems are relevant in applications like ridesharing, wireless scheduling, and content recommendation.

Method: The paper formalizes the problem using the Probing-Augmented User-Centric Selection (PUCS) framework. For the offline setting with known distributions, a greedy probing algorithm with a constant-factor approximation guarantee ($\zeta = (e-1)/(2e-1)$) is presented. For the online setting with unknown distributions, the OLPA (Online Learning with Probing and Assignment) algorithm, a stochastic combinatorial bandit algorithm, is introduced, achieving a regret bound of $\mathcal{O}(\sqrt{T} + \ln^{2} T)$, which is shown to be tight up to logarithmic factors.

Result: The paper presents a greedy probing algorithm with a constant-factor approximation guarantee for the offline setting and the OLPA algorithm for the online setting, which achieves a tight regret bound of $\mathcal{O}(\sqrt{T} + \ln^{2} T)$. Experiments on real-world data validate the effectiveness of these solutions.

Conclusion: The proposed PUCS framework and associated algorithms (greedy probing and OLPA) effectively address sequential decision-making with information acquisition in various applications. The algorithms achieve strong theoretical guarantees (constant-factor approximation and tight regret bounds) and demonstrate strong empirical performance on real-world data.

Abstract: We formalize sequential decision-making with information acquisition as the
probing-augmented user-centric selection (PUCS) framework, where a learner
first probes a subset of arms to obtain side information on resources and
rewards, and then assigns $K$ plays to $M$ arms. PUCS covers applications such
as ridesharing, wireless scheduling, and content recommendation, in which both
resources and payoffs are initially unknown and probing is costly. For the
offline setting with known distributions, we present a greedy probing algorithm
with a constant-factor approximation guarantee $\zeta = (e-1)/(2e-1)$. For the
online setting with unknown distributions, we introduce OLPA, a stochastic
combinatorial bandit algorithm that achieves a regret bound
$\mathcal{O}(\sqrt{T} + \ln^{2} T)$. We also prove a lower bound
$\Omega(\sqrt{T})$, showing that the upper bound is tight up to logarithmic
factors. Experiments on real-world data demonstrate the effectiveness of our
solutions.

</details>


### [521] [Efficient and Scalable Agentic AI with Heterogeneous Systems](https://arxiv.org/abs/2507.19635)
*Zain Asgar,Michelle Nguyen,Sachin Katti*

Main category: cs.LG

TL;DR: AI 代理工作负载的规模化需要高效的基础设施。本文提出了一个系统设计，用于在异构计算基础设施上动态编排 AI 代理工作负载，并通过实际结果证明了其总拥有成本效益。


<details>
  <summary>Details</summary>
Motivation: AI 代理作为一种新兴的主导工作负载，其动态和结构复杂的性质需要高效、可扩展的部署和代理服务基础设施来实现规模化应用。

Method: 本系统设计了用于在跨越 CPU 和加速器的异构计算基础设施上动态编排 AI 代理工作负载的系统，该系统包括用于规划和优化代理 AI 执行图的框架、基于 MLIR 的表示和编译系统以及动态编排系统。

Result: 初步结果表明，利用异构基础设施可以带来显著的总拥有成本效益，并且将旧一代 GPU 与新一代加速器相结合，在某些工作负载上可以提供与最新一代同类 GPU 基础设施相当的总拥有成本效益。

Conclusion: 通过利用异构基础设施，可以为某些工作负载带来显著的总拥有成本效益，并且令人惊讶的是，将旧一代 GPU 与新一代加速器相结合，可以提供与最新一代同类 GPU 基础设施相同或更好的总拥有成本效益，这可能会延长已部署基础设施的寿命。

Abstract: AI agents are emerging as a dominant workload in a wide range of
applications, promising to be the vehicle that delivers the promised benefits
of AI to enterprises and consumers. Unlike conventional software or static
inference, agentic workloads are dynamic and structurally complex. Often these
agents are directed graphs of compute and IO operations that span multi-modal
data input and conversion), data processing and context gathering (e.g vector
DB lookups), multiple LLM inferences, tool calls, etc. To scale AI agent usage,
we need efficient and scalable deployment and agent-serving infrastructure.
  To tackle this challenge, in this paper, we present a system design for
dynamic orchestration of AI agent workloads on heterogeneous compute
infrastructure spanning CPUs and accelerators, both from different vendors and
across different performance tiers within a single vendor. The system delivers
several building blocks: a framework for planning and optimizing agentic AI
execution graphs using cost models that account for compute, memory, and
bandwidth constraints of different HW; a MLIR based representation and
compilation system that can decompose AI agent execution graphs into granular
operators and generate code for different HW options; and a dynamic
orchestration system that can place the granular components across a
heterogeneous compute infrastructure and stitch them together while meeting an
end-to-end SLA. Our design performs a systems level TCO optimization and
preliminary results show that leveraging a heterogeneous infrastructure can
deliver significant TCO benefits. A preliminary surprising finding is that for
some workloads a heterogeneous combination of older generation GPUs with newer
accelerators can deliver similar TCO as the latest generation homogenous GPU
infrastructure design, potentially extending the life of deployed
infrastructure.

</details>


### [522] [$K^4$: Online Log Anomaly Detection Via Unsupervised Typicality Learning](https://arxiv.org/abs/2507.20051)
*Weicong Chen,Vikash Singh,Zahra Rahmani,Debargha Ganguly,Mohsen Hariri,Vipin Chaudhary*

Main category: cs.LG

TL;DR: $K^4$ is a fast, unsupervised, and parser-independent framework for log anomaly detection that uses k-NN statistics to create descriptors for accurate and efficient anomaly scoring.


<details>
  <summary>Details</summary>
Motivation: Existing Log Anomaly Detection (LogAD) methods are slow, depend on error-prone parsing, and use unrealistic evaluation protocols.

Method: The framework $K^4$ transforms log embeddings into four-dimensional descriptors (Precision, Recall, Density, Coverage) using k-nearest neighbor (k-NN) statistics, enabling lightweight anomaly scoring without retraining.

Result: $K^4$ achieves state-of-the-art AUROC scores of 0.995-0.999, is orders of magnitude faster than baselines (training under 4s, inference as low as 4 $\mu$s), and is parser-independent.

Conclusion: $K^4$ sets a new state-of-the-art in online log anomaly detection, outperforming baselines in accuracy and speed, while being parser-independent and unsupervised.

Abstract: Existing Log Anomaly Detection (LogAD) methods are often slow, dependent on
error-prone parsing, and use unrealistic evaluation protocols. We introduce
$K^4$, an unsupervised and parser-independent framework for high-performance
online detection. $K^4$ transforms arbitrary log embeddings into compact
four-dimensional descriptors (Precision, Recall, Density, Coverage) using
efficient k-nearest neighbor (k-NN) statistics. These descriptors enable
lightweight detectors to accurately score anomalies without retraining. Using a
more realistic online evaluation protocol, $K^4$ sets a new state-of-the-art
(AUROC: 0.995-0.999), outperforming baselines by large margins while being
orders of magnitude faster, with training under 4 seconds and inference as low
as 4 $\mu$s.

</details>


### [523] [Communication-Efficient Distributed Training for Collaborative Flat Optima Recovery in Deep Learning](https://arxiv.org/abs/2507.20424)
*Tolga Dimlioglu,Anna Choromanska*

Main category: cs.LG

TL;DR: 该研究提出了一种名为DPPF的分布式训练算法，通过引入“逆平均谷”度量来寻找平坦最小值，从而提高通信效率和模型泛化能力。实验和理论结果均表明DPPF优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了改善通信效率和局部梯度方法模型性能之间的权衡，我们重新审视了平坦最小化假说，该假说表明具有更好泛化能力的模型倾向于位于损失平面的平坦区域。

Method: 该研究引入了一种简单而有效的平坦度度量方法——逆平均谷（Inverse Mean Valley），并证明了它与深度神经网络（DNN）的泛化差距具有很强的相关性。通过将该度量方法的有效松弛形式纳入分布式训练目标，作为一种轻量级正则化器，鼓励工作节点协同寻找宽阔的最小值。这种正则化器产生一个推力，与将工作节点拉拢在一起的共识步骤相抗衡，从而形成了分布式推拉力（DPPF）算法。

Result: DPPF算法的实验结果显示，其性能优于其他通信高效方法，并且在通信开销显著降低的情况下，获得了比局部梯度方法和同步梯度平均更好的泛化性能。此外，损失平面的可视化结果证实了DPPF能够找到更平坦的最小值。

Conclusion: DPPF算法能够引导工作节点跨越平坦的山谷，最终的山谷宽度受推力和拉力相互作用的控制，并且其推拉动力学是自稳定的。此外，我们提供了与山谷宽度相关的泛化保证，并证明了在非凸设置下的收敛性。

Abstract: We study centralized distributed data parallel training of deep neural
networks (DNNs), aiming to improve the trade-off between communication
efficiency and model performance of the local gradient methods. To this end, we
revisit the flat-minima hypothesis, which suggests that models with better
generalization tend to lie in flatter regions of the loss landscape. We
introduce a simple, yet effective, sharpness measure, Inverse Mean Valley, and
demonstrate its strong correlation with the generalization gap of DNNs. We
incorporate an efficient relaxation of this measure into the distributed
training objective as a lightweight regularizer that encourages workers to
collaboratively seek wide minima. The regularizer exerts a pushing force that
counteracts the consensus step pulling the workers together, giving rise to the
Distributed Pull-Push Force (DPPF) algorithm. Empirically, we show that DPPF
outperforms other communication-efficient approaches and achieves better
generalization performance than local gradient methods and synchronous gradient
averaging, while significantly reducing communication overhead. In addition,
our loss landscape visualizations confirm the ability of DPPF to locate flatter
minima. On the theoretical side, we show that DPPF guides workers to span flat
valleys, with the final valley width governed by the interplay between push and
pull strengths, and that its pull-push dynamics is self-stabilizing. We further
provide generalization guarantees linked to the valley width and prove
convergence in the non-convex setting.

</details>


### [524] [Debunking Optimization Myths in Federated Learning for Medical Image Classification](https://arxiv.org/abs/2507.19822)
*Youngjoon Lee,Hyukjoon Lee,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 联邦学习的性能很大程度上取决于本地配置（如优化器和学习率），而不是联邦学习算法本身。增加训练轮数可能是有益的，也可能是有害的，这取决于具体的联邦学习方法。


<details>
  <summary>Details</summary>
Motivation: 尽管联邦学习在医学影像领域具有潜力，但现有方法对其本地因素（如优化器和学习率）敏感，限制了其在实际应用中的鲁棒性。

Method: 通过在结直肠病理和血细胞分类任务上对最新的联邦学习方法进行基准测试，重新审视了标准的联邦学习，以阐明边缘设备配置的影响。

Result: 研究表明，本地优化器和学习率的选择对性能的影响比联邦学习方法本身更大。此外，增加本地训练轮数可能根据联邦学习方法的不同而影响收敛性。

Conclusion: 边缘设备特定的配置比算法的复杂性对于实现有效的联邦学习更重要。

Abstract: Federated Learning (FL) is a collaborative learning method that enables
decentralized model training while preserving data privacy. Despite its promise
in medical imaging, recent FL methods are often sensitive to local factors such
as optimizers and learning rates, limiting their robustness in practical
deployments. In this work, we revisit vanilla FL to clarify the impact of edge
device configurations, benchmarking recent FL methods on colorectal pathology
and blood cell classification task. We numerically show that the choice of
local optimizer and learning rate has a greater effect on performance than the
specific FL method. Moreover, we find that increasing local training epochs can
either enhance or impair convergence, depending on the FL method. These
findings indicate that appropriate edge-specific configuration is more crucial
than algorithmic complexity for achieving effective FL.

</details>


### [525] [Data-Efficient Prediction-Powered Calibration via Cross-Validation](https://arxiv.org/abs/2507.20268)
*Seonghoon Yoo,Houssem Sifaou,Sangwoo Park,Joonhyuk Kang,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本研究提出了一种新方法，能够利用有限的校准数据来微调预测器和估计合成标签的偏差，从而为AI决策提供严格的覆盖保证，并在室内定位任务中得到验证。


<details>
  <summary>Details</summary>
Motivation: 为了解决校准数据稀缺的常见问题，利用合成标签是一种有前途的方法，但需要额外数据来微调标签生成预测器和估计残差偏差，这可能会加剧数据稀缺问题。

Method: 提出了一种有效利用有限校准数据同时微调预测器和估计合成标签偏差的新方法。

Result: 提出的方法能够以严格的覆盖保证为AI生成的决策提供预测集。

Conclusion: 该方法为AI生成的决策提供了严格的覆盖保证，并在室内定位问题上验证了其有效性和性能提升。

Abstract: Calibration data are necessary to formally quantify the uncertainty of the
decisions produced by an existing artificial intelligence (AI) model. To
overcome the common issue of scarce calibration data, a promising approach is
to employ synthetic labels produced by a (generally different) predictive
model. However, fine-tuning the label-generating predictor on the inference
task of interest, as well as estimating the residual bias of the synthetic
labels, demand additional data, potentially exacerbating the calibration data
scarcity problem. This paper introduces a novel approach that efficiently
utilizes limited calibration data to simultaneously fine-tune a predictor and
estimate the bias of the synthetic labels. The proposed method yields
prediction sets with rigorous coverage guarantees for AI-generated decisions.
Experimental results on an indoor localization problem validate the
effectiveness and performance gains of our solution.

</details>


### [526] [ResCap-DBP: A Lightweight Residual-Capsule Network for Accurate DNA-Binding Protein Prediction Using Global ProteinBERT Embeddings](https://arxiv.org/abs/2507.20426)
*Samiul Based Shuvo,Tasnia Binte Mamun,U Rajendra Acharya*

Main category: cs.LG

TL;DR: 本研究提出了一种名为ResCap-DBP的深度学习框架，用于从蛋白质序列预测DNA结合蛋白（DBP）。该框架结合了残差学习编码器和一维胶囊网络，并利用ProteinBERT嵌入，在大规模数据集上表现优于传统方法，并在多个基准测试中取得了最先进的性能（AUC高达98.0%）。


<details>
  <summary>Details</summary>
Motivation: DNA结合蛋白（DBPs）在基因调控和细胞过程中起着至关重要的作用，准确识别它们对于理解生物功能和疾病机制至关重要。然而，实验方法耗时且成本高昂，因此需要高效的计算预测技术。

Method: 提出了一种名为ResCap-DBP的新型深度学习框架，该框架结合了基于残差学习的编码器和一维胶囊网络（1D-CapsNet），直接从蛋白质序列预测DBP。该架构在残差块中使用了扩张卷积来解决梯度消失问题并提取序列特征，而胶囊层通过动态路由捕获了特征空间中的层级和空间关系。对全局和局部嵌入（来自ProteinBERT）以及传统one-hot编码进行了消融研究，结果表明ProteinBERT嵌入在大规模数据集上表现最佳。

Result: 在四个公开基准数据集上的广泛评估表明，ResCap-DBP模型在PDB14189和PDB1075数据集上分别取得了98.0%和89.5%的AUC分数，在PDB2272和PDB186独立测试集上达到了83.2%和83.3%的AUC，并且在PDB20000等大型数据集上表现出竞争力。模型在不同数据集上保持了均衡的灵敏度和特异性，证明了其有效性和泛化能力。

Conclusion: ResCap-DBP框架在DNA结合蛋白（DBP）预测方面表现出优越的性能和可扩展性，通过结合残差学习和胶囊网络，并利用ProteinBERT嵌入，能够有效处理大规模数据集，并在各种基准测试中超越现有最先进方法。

Abstract: DNA-binding proteins (DBPs) are integral to gene regulation and cellular
processes, making their accurate identification essential for understanding
biological functions and disease mechanisms. Experimental methods for DBP
identification are time-consuming and costly, driving the need for efficient
computational prediction techniques. In this study, we propose a novel deep
learning framework, ResCap-DBP, that combines a residual learning-based encoder
with a one-dimensional Capsule Network (1D-CapsNet) to predict DBPs directly
from raw protein sequences. Our architecture incorporates dilated convolutions
within residual blocks to mitigate vanishing gradient issues and extract rich
sequence features, while capsule layers with dynamic routing capture
hierarchical and spatial relationships within the learned feature space. We
conducted comprehensive ablation studies comparing global and local embeddings
from ProteinBERT and conventional one-hot encoding. Results show that
ProteinBERT embeddings substantially outperform other representations on large
datasets. Although one-hot encoding showed marginal advantages on smaller
datasets, such as PDB186, it struggled to scale effectively. Extensive
evaluations on four pairs of publicly available benchmark datasets demonstrate
that our model consistently outperforms current state-of-the-art methods. It
achieved AUC scores of 98.0% and 89.5% on PDB14189andPDB1075, respectively. On
independent test sets PDB2272 and PDB186, the model attained top AUCs of 83.2%
and 83.3%, while maintaining competitive performance on larger datasets such as
PDB20000. Notably, the model maintains a well balanced sensitivity and
specificity across datasets. These results demonstrate the efficacy and
generalizability of integrating global protein representations with advanced
deep learning architectures for reliable and scalable DBP prediction in diverse
genomic contexts.

</details>


### [527] [On Using the Shapley Value for Anomaly Localization: A Statistical Investigation](https://arxiv.org/abs/2507.21023)
*Rick S. Blum,Franziska Freytag*

Main category: cs.LG

TL;DR: 在传感器数据异常定位中，使用 Shapley 值的一个固定项比使用所有项更简单且同样有效（在独立观测的情况下）。


<details>
  <summary>Details</summary>
Motivation: 为了简化和提高传感器数据系统中异常定位的效率，特别是在处理 Shapley 值时。

Method: 提出了一种仅使用 Shapley 值计算中的固定项来实现异常定位的方法，并与使用所有情况下的 Shapley 值进行了比较。

Result: 在独立观测的情况下，仅使用 Shapley 值计算中的固定项可以达到与使用所有情况下的 Shapley 值相同的错误率，同时降低了测试的复杂度。对于依赖观测的情况，没有提供证明。

Conclusion: 实验表明，在所有测试的情况下，仅使用 Shapley 值计算中的固定项可以实现与使用所有情况下的 Shapley 值相同错误率的较低复杂度的异常定位测试。证明显示，在所有独立观测的情况下，这些结论必须成立。然而，对于依赖观测的情况，目前尚无证明。

Abstract: Recent publications have suggested using the Shapley value for anomaly
localization for sensor data systems. Using a reasonable mathematical anomaly
model for full control, experiments indicate that using a single fixed term in
the Shapley value calculation achieves a lower complexity anomaly localization
test, with the same probability of error, as a test using the Shapley value for
all cases tested. A proof demonstrates these conclusions must be true for all
independent observation cases. For dependent observation cases, no proof is
available.

</details>


### [528] [Graph Learning Metallic Glass Discovery from Wikipedia](https://arxiv.org/abs/2507.19536)
*K. -C. Ouyang,S. -Y. Zhang,S. -L. Liu,J. Tian,Y. -H. Li,H. Tong,H. -Y. Bai,W. -H. Wang,Y. -C. Hu*

Main category: cs.LG

TL;DR: 利用维基百科数据和图神经网络，通过人工智能加速新材料（特别是金属玻璃）的发现过程。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统材料设计中，特别是金属玻璃，由于其形成依赖于多元素的最佳组合以抵抗结晶，导致探索速度慢且成本高的问题。

Method: 通过语言模型对元素进行编码，并利用图神经网络作为推荐系统来探索材料间的隐藏关系，同时评估了不同语言维基百科嵌入在材料设计中的能力。

Result: 研究表明，利用材料网络表示进行数据学习，特别是使用不同语言的维基百科嵌入，可以提高模型的可预测性和泛化能力，为发现新的非晶材料提供了新的途径。

Conclusion: 本研究提出了一种利用人工智能挖掘新非晶材料的新范式，并展示了自然语言在材料设计中的潜力。

Abstract: Synthesizing new materials efficiently is highly demanded in various research
fields. However, this process is usually slow and expensive, especially for
metallic glasses, whose formation strongly depends on the optimal combinations
of multiple elements to resist crystallization. This constraint renders only
several thousands of candidates explored in the vast material space since 1960.
Recently, data-driven approaches armed by advanced machine learning techniques
provided alternative routes for intelligent materials design. Due to data
scarcity and immature material encoding, the conventional tabular data is
usually mined by statistical learning algorithms, giving limited model
predictability and generalizability. Here, we propose sophisticated data
learning from material network representations. The node elements are encoded
from the Wikipedia by a language model. Graph neural networks with versatile
architectures are designed to serve as recommendation systems to explore hidden
relationships among materials. By employing Wikipedia embeddings from different
languages, we assess the capability of natural languages in materials design.
Our study proposes a new paradigm to harvesting new amorphous materials and
beyond with artificial intelligence.

</details>


### [529] [Breaking the Precision Ceiling in Physics-Informed Neural Networks: A Hybrid Fourier-Neural Architecture for Ultra-High Accuracy](https://arxiv.org/abs/2507.20929)
*Wei Shan Lee,Chi Kiu Althina Chau,Kei Chon Sio,Kam Ian Leong*

Main category: cs.LG

TL;DR: 研究提出了一种混合傅里叶-神经网络方法，成功将PINNs求解四阶偏微分方程的精度从$10^{-3}$提升至$10^{-7}$级别，显著优于现有方法。通过结合傅里叶级数和神经网络，并优化谐波数量和训练策略，实现了超高精度计算。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络（PINNs）在求解四阶偏微分方程时精度（$10^{-3}$-$10^{-4}$）存在瓶颈，限制了其在工程应用中的推广。本研究旨在突破这一精度限制，实现超高精度求解。

Method: 提出了一种混合傅里叶-神经网络架构，结合了捕捉主导模态行为的截断傅里叶级数和提供自适应残差校正的深度神经网络。采用了两阶段优化策略（先Adam，后L-BFGS）和自适应权重平衡以实现稳定的超高精度收敛。通过系统性的谐波优化研究确定了10个谐波是获得最优性能的关键。

Result: 在欧拉-伯努利梁方程上实现了$1.94 	imes 10^{-7}$的L2误差，比标准PINNs提高了17倍，比传统数值方法提高了15-500倍。证明了精确10个谐波是获得最优性能的关键，并展示了GPU加速实现亚30分钟训练的可行性。

Conclusion: 该研究提出的混合傅里叶-神经网络架构成功突破了物理信息神经网络（PINNs）在四阶偏微分方程求解精度上的瓶颈，实现了$10^{-7}$级别的L2误差，较标准PINNs提高了17倍，并显著优于传统数值方法。通过结合傅里叶级数和深度神经网络，并采用特定的优化策略（Adam后跟L-BFGS）和自适应权重平衡，该方法实现了超高精度的收敛。研究还揭示了精确10个谐波是获得最优性能的关键。该工作通过解决现有方法的12个关键差距，证明了通过精巧设计实现超高精度是可行的，为机器学习在科学计算领域超越传统数值方法开辟了新途径。

Abstract: Physics-informed neural networks (PINNs) have plateaued at errors of
$10^{-3}$-$10^{-4}$ for fourth-order partial differential equations, creating a
perceived precision ceiling that limits their adoption in engineering
applications. We break through this barrier with a hybrid Fourier-neural
architecture for the Euler-Bernoulli beam equation, achieving unprecedented L2
error of $1.94 \times 10^{-7}$-a 17-fold improvement over standard PINNs and
\(15-500\times\) better than traditional numerical methods. Our approach
synergistically combines a truncated Fourier series capturing dominant modal
behavior with a deep neural network providing adaptive residual corrections. A
systematic harmonic optimization study revealed a counter-intuitive discovery:
exactly 10 harmonics yield optimal performance, with accuracy catastrophically
degrading from $10^{-7}$ to $10^{-1}$ beyond this threshold. The two-phase
optimization strategy (Adam followed by L-BFGS) and adaptive weight balancing
enable stable ultra-precision convergence. GPU-accelerated implementation
achieves sub-30-minute training despite fourth-order derivative complexity. By
addressing 12 critical gaps in existing approaches-from architectural rigidity
to optimization landscapes-this work demonstrates that ultra-precision is
achievable through proper design, opening new paradigms for scientific
computing where machine learning can match or exceed traditional numerical
methods.

</details>


### [530] [Flow Matching Policy Gradients](https://arxiv.org/abs/2507.21053)
*David McAllister,Songwei Ge,Brent Yi,Chung Min Kim,Ethan Weber,Hongsuk Choi,Haiwen Feng,Angjoo Kanazawa*

Main category: cs.LG

TL;DR: Flow Policy Optimization (FPO) is a new reinforcement learning algorithm that uses flow-based generative models (like diffusion models) for policy optimization. It allows policies to learn complex action distributions, outperforming Gaussian policies in difficult tasks, without being tied to specific sampling methods.


<details>
  <summary>Details</summary>
Motivation: This paper introduces Flow Policy Optimization (FPO), a method to apply flow-based generative models, like diffusion models, to reinforcement learning. The goal is to leverage the ability of these models to handle high-dimensional continuous distributions within the policy gradient framework, enabling them to learn policies that can represent complex, multimodal action distributions, especially in challenging scenarios.

Method: Flow Policy Optimization (FPO) is a simple on-policy reinforcement learning algorithm that integrates flow matching into the policy gradient framework. It treats policy optimization as maximizing an advantage-weighted ratio derived from the conditional flow matching loss, aligning with the PPO-clip framework. FPO avoids exact likelihood computation while retaining the generative strengths of flow-based models and is independent of specific diffusion or flow integration methods during training and inference.

Result: FPO successfully trains diffusion-style policies from scratch for various continuous control tasks. The experiments demonstrate that flow-based models utilized by FPO can effectively capture multimodal action distributions, outperforming traditional Gaussian policies in under-conditioned environments.

Conclusion: Flow-based models can capture multimodal action distributions and achieve higher performance than Gaussian policies, particularly in under-conditioned settings.

Abstract: Flow-based generative models, including diffusion models, excel at modeling
continuous distributions in high-dimensional spaces. In this work, we introduce
Flow Policy Optimization (FPO), a simple on-policy reinforcement learning
algorithm that brings flow matching into the policy gradient framework. FPO
casts policy optimization as maximizing an advantage-weighted ratio computed
from the conditional flow matching loss, in a manner compatible with the
popular PPO-clip framework. It sidesteps the need for exact likelihood
computation while preserving the generative capabilities of flow-based models.
Unlike prior approaches for diffusion-based reinforcement learning that bind
training to a specific sampling method, FPO is agnostic to the choice of
diffusion or flow integration at both training and inference time. We show that
FPO can train diffusion-style policies from scratch in a variety of continuous
control tasks. We find that flow-based models can capture multimodal action
distributions and achieve higher performance than Gaussian policies,
particularly in under-conditioned settings.

</details>


### [531] [FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings](https://arxiv.org/abs/2507.19534)
*Ali Shakeri,Wei Emma Zhang,Amin Beheshti,Weitong Chen,Jian Yang,Lishan Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为FedDPG的联邦学习方法，通过动态生成提示来提高NLP任务的效率和灵活性，同时保护数据隐私，并在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统微调方法计算开销大、提示固定导致模型灵活性降低，以及联邦学习中存在的通信和计算限制等问题，本文提出了FedDPG。

Method: 本文介绍了一种名为FedDPG（Federated Dynamic Prompt Generator）的联邦动态提示生成器，该方法结合了动态提示生成器网络，可以根据给定输入生成上下文感知的提示，从而在联邦学习环境中实现灵活性和适应性，同时优先考虑数据隐私。

Result: FedDPG在三个NLP基准数据集上的实验结果显示，在全局模型性能上优于现有的参数高效微调方法，同时在计算时间和通信参数数量上也有显著减少。

Conclusion: FedDPG在三个NLP基准数据集上的实验表明，其在全局模型性能方面优于最先进的参数高效微调方法，并显著减少了计算时间和通过FL网络发送的参数数量。

Abstract: Pre-trained Language Models (PLMs) have demonstrated impressive performance
in various NLP tasks. However, traditional fine-tuning methods for leveraging
PLMs for downstream tasks entail significant computational overhead.
Prompt-tuning has emerged as an efficient alternative that involves prepending
a limited number of parameters to the input sequence and only updating them
while the PLM's parameters are frozen. However, this technique's prompts remain
fixed for all inputs, reducing the model's flexibility. The Federated Learning
(FL) technique has gained attention in recent years to address the growing
concerns around data privacy. However, challenges such as communication and
computation limitations of clients still need to be addressed. To mitigate
these challenges, this paper introduces the Federated Dynamic Prompt Generator
(FedDPG), which incorporates a dynamic prompt generator network to generate
context-aware prompts based on the given input, ensuring flexibility and
adaptability while prioritising data privacy in federated learning settings.
Our experiments on three NLP benchmark datasets showcase that FedDPG
outperforms the state-of-the-art parameter-efficient fine-tuning methods in
terms of global model performance, and has significantly reduced the
calculation time and the number of parameters to be sent through the FL
network.

</details>


### [532] [Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks](https://arxiv.org/abs/2507.19684)
*Bermet Burkanova,Payam Jome Yazdian,Chuxuan Zhang,Trinity Evans,Paige Tuttösí,Angelica Lim*

Main category: cs.LG

TL;DR: 该研究提出了 CoMPAS3D 数据集，用于训练人工智能与人类跳萨尔萨舞蹈，并发布了一个名为 SalsaAgent 的模型，该模型能够根据熟练度生成领舞和跟舞动作，并进行双人共舞。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决人工智能与人类进行安全、有创意的舞蹈互动这一挑战，重点在于通过触觉信号进行交流，并适应人类舞伴的熟练程度。作者认为，人类交流不仅仅局限于文本或语音，还包括了体现化的运动、时机和身体协调，而对两个智能体之间耦合交互的建模是一个持续的、双向反应的、并受个体差异影响的难题。

Method: 提出 CoMPAS3D，这是一个包含 18 位舞者 3 小时即兴萨尔萨舞蹈的运动捕捉数据集，舞者涵盖初学者、中级和专业级别。数据集中包含超过 2800 个动作片段的精细注释，涵盖了动作类型、组合、执行错误和风格元素。评估了 CoMPAS3D 在两个基准任务上的表现：生成具有不同熟练度水平的领舞者或跟舞者（类似于语音和对话处理中的说话者或听话者合成），以及生成双人舞（类似于对话）。

Result: CoMPAS3D 是一个包含 3 小时萨尔萨舞蹈的运动捕捉数据集，涵盖 18 位舞者的不同熟练度，并带有详细的动作注释。SalsaAgent 模型能够执行领舞者/跟舞者生成和双人舞生成等任务，为具身人工智能和运动生成领域的研究奠定了基础。

Conclusion: 该研究发布了 CoMPAS3D 数据集、注释和代码，以及一个能够执行基准任务的多任务 SalsaAgent 模型和额外的基线，旨在推动社会交互式具身人工智能和创意、表现力丰富的人形运动生成的研究。

Abstract: Imagine a humanoid that can safely and creatively dance with a human,
adapting to its partner's proficiency, using haptic signaling as a primary form
of communication. While today's AI systems excel at text or voice-based
interaction with large language models, human communication extends far beyond
text-it includes embodied movement, timing, and physical coordination. Modeling
coupled interaction between two agents poses a formidable challenge: it is
continuous, bidirectionally reactive, and shaped by individual variation. We
present CoMPAS3D, the largest and most diverse motion capture dataset of
improvised salsa dancing, designed as a challenging testbed for interactive,
expressive humanoid AI. The dataset includes 3 hours of leader-follower salsa
dances performed by 18 dancers spanning beginner, intermediate, and
professional skill levels. For the first time, we provide fine-grained salsa
expert annotations, covering over 2,800 move segments, including move types,
combinations, execution errors and stylistic elements. We draw analogies
between partner dance communication and natural language, evaluating CoMPAS3D
on two benchmark tasks for synthetic humans that parallel key problems in
spoken language and dialogue processing: leader or follower generation with
proficiency levels (speaker or listener synthesis), and duet (conversation)
generation. Towards a long-term goal of partner dance with humans, we release
the dataset, annotations, and code, along with a multitask SalsaAgent model
capable of performing all benchmark tasks, alongside additional baselines to
encourage research in socially interactive embodied AI and creative, expressive
humanoid motion generation.

</details>


### [533] [Agentic Reinforced Policy Optimization](https://arxiv.org/abs/2507.19849)
*Guanting Dong,Hangyu Mao,Kai Ma,Licheng Bao,Yifei Chen,Zhongyuan Wang,Zhongxia Chen,Jiazhen Du,Huiyang Wang,Fuzheng Zhang,Guorui Zhou,Yutao Zhu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.LG

TL;DR: ARPO是一种新的强化学习算法，通过一个基于熵的机制来处理LLM在与外部工具交互后的不确定性，从而提高多轮工具交互任务的性能，并减少了对工具使用的需求。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习算法未能充分平衡模型内在的长期推理能力与其在多轮工具交互中的熟练程度。作者观察到LLM在与外部工具交互后，往往会表现出高度不确定的行为，表现为生成标记的熵分布增加。

Method: ARPO（Agentic Reinforced Policy Optimization）是一种新颖的代理强化学习算法，专门用于训练多轮LLM基础代理。ARPO包含一个基于熵的自适应回滚机制，动态地平衡全局轨迹采样和步级采样，从而在工具使用后的高不确定性步骤中促进探索。通过整合优势归因估计，ARPO使LLM能够内化步级工具使用交互中的优势差异。

Result: ARPO在13个具有挑战性的基准测试中表现优于现有的轨迹级强化学习算法，并且在工具使用预算减少一半的情况下仍能提高性能。

Conclusion: ARPO通过在计算推理、知识推理和深度搜索领域13个具有挑战性的基准测试中进行实验，证明了其优于轨迹级强化学习算法的性能。ARPO在仅使用现有方法一半工具使用预算的情况下提高了性能，为使基于LLM的代理与实时动态环境保持一致提供了可扩展的解决方案。

Abstract: Large-scale reinforcement learning with verifiable rewards (RLVR) has
demonstrated its effectiveness in harnessing the potential of large language
models (LLMs) for single-turn reasoning tasks. In realistic reasoning
scenarios, LLMs can often utilize external tools to assist in task-solving
processes. However, current RL algorithms inadequately balance the models'
intrinsic long-horizon reasoning capabilities and their proficiency in
multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced
Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training
multi-turn LLM-based agents. Through preliminary experiments, we observe that
LLMs tend to exhibit highly uncertain behavior, characterized by an increase in
the entropy distribution of generated tokens, immediately following
interactions with external tools. Motivated by this observation, ARPO
incorporates an entropy-based adaptive rollout mechanism, dynamically balancing
global trajectory sampling and step-level sampling, thereby promoting
exploration at steps with high uncertainty after tool usage. By integrating an
advantage attribution estimation, ARPO enables LLMs to internalize advantage
differences in stepwise tool-use interactions. Our experiments across 13
challenging benchmarks in computational reasoning, knowledge reasoning, and
deep search domains demonstrate ARPO's superiority over trajectory-level RL
algorithms. Remarkably, ARPO achieves improved performance using only half of
the tool-use budget required by existing methods, offering a scalable solution
for aligning LLM-based agents with real-time dynamic environments. Our code and
datasets are released at https://github.com/dongguanting/ARPO

</details>


### [534] [EcoTransformer: Attention without Multiplication](https://arxiv.org/abs/2507.20096)
*Xin Gao,Xingming Xu*

Main category: cs.LG

TL;DR: EcoTransformer通过使用拉普拉斯核的卷积替代了Transformer中的点积注意力，实现了同等甚至更好的性能，同时显著降低了能耗。


<details>
  <summary>Details</summary>
Motivation: Transformer中的扩展点积注意力机制计算成本高且能耗大，需要更节能的替代方案。

Method: 提出了一种新的Transformer架构EcoTransformer，其输出上下文向量是通过使用拉普拉斯核对值进行卷积构建的，其中距离通过查询和键之间的L1度量来衡量。这种新的注意力分数计算不涉及矩阵乘法。

Result: EcoTransformer在NLP、生物信息学和视觉任务中的表现与扩展点积注意力相当，甚至更优，同时消耗的能量显著减少。

Conclusion: EcoTransformer在自然语言处理、生物信息学和视觉任务中表现与扩展点积注意力相当，甚至更优，同时消耗的能量显著减少。

Abstract: The Transformer, with its scaled dot-product attention mechanism, has become
a foundational architecture in modern AI. However, this mechanism is
computationally intensive and incurs substantial energy costs. We propose a new
Transformer architecture EcoTransformer, in which the output context vector is
constructed as the convolution of the values using a Laplacian kernel, where
the distances are measured by the L1 metric between the queries and keys.
Compared to dot-product based attention, the new attention score calculation is
free of matrix multiplication. It performs on par with, or even surpasses,
scaled dot-product attention in NLP, bioinformatics, and vision tasks, while
consuming significantly less energy.

</details>


### [535] [Customize Multi-modal RAI Guardrails with Precedent-based predictions](https://arxiv.org/abs/2507.20503)
*Cheng-Fu Yang,Thanh Tran,Christos Christodoulopoulos,Weitong Ruan,Rahul Gupta,Kai-Wei Chang*

Main category: cs.LG

TL;DR: 通过利用“先例”（即相似输入的先前数据点的推理过程）而非固定策略，该研究提出了一种更具灵活性和适应性的多模态内容过滤护栏，能够有效应对用户定制化和不断变化的策略需求。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态内容过滤护栏在适应用户定制化、多样化和不断变化的策略方面存在挑战，需要克服现有模型在新策略上的泛化能力不足和适应性差的问题。

Method: 提出一种以“先例”为基础的条件化模型判断方法，并设计了包含“预审-修正”机制以收集高质量先例，以及利用先例进行鲁棒预测的两种策略。

Result: 实验结果表明，所提出的方法在少样本和全数据集场景中均优于先前方法，并对新策略表现出卓越的泛化能力。

Conclusion: 该方法在少样本和全数据集场景中均优于先前方法，并对新策略表现出卓越的泛化能力。

Abstract: A multi-modal guardrail must effectively filter image content based on
user-defined policies, identifying material that may be hateful, reinforce
harmful stereotypes, contain explicit material, or spread misinformation.
Deploying such guardrails in real-world applications, however, poses
significant challenges. Users often require varied and highly customizable
policies and typically cannot provide abundant examples for each custom policy.
Consequently, an ideal guardrail should be scalable to the multiple policies
and adaptable to evolving user standards with minimal retraining. Existing
fine-tuning methods typically condition predictions on pre-defined policies,
restricting their generalizability to new policies or necessitating extensive
retraining to adapt. Conversely, training-free methods struggle with limited
context lengths, making it difficult to incorporate all the policies
comprehensively. To overcome these limitations, we propose to condition model's
judgment on "precedents", which are the reasoning processes of prior data
points similar to the given input. By leveraging precedents instead of fixed
policies, our approach greatly enhances the flexibility and adaptability of the
guardrail. In this paper, we introduce a critique-revise mechanism for
collecting high-quality precedents and two strategies that utilize precedents
for robust prediction. Experimental results demonstrate that our approach
outperforms previous methods across both few-shot and full-dataset scenarios
and exhibits superior generalization to novel policies.

</details>


### [536] [Kimi K2: Open Agentic Intelligence](https://arxiv.org/abs/2507.20534)
*Kimi Team,Yifan Bai,Yiping Bao,Guanduo Chen,Jiahao Chen,Ningxin Chen,Ruijue Chen,Yanru Chen,Yuankun Chen,Yutian Chen,Zhuofu Chen,Jialei Cui,Hao Ding,Mengnan Dong,Angang Du,Chenzhuang Du,Dikang Du,Yulun Du,Yu Fan,Yichen Feng,Kelin Fu,Bofei Gao,Hongcheng Gao,Peizhong Gao,Tong Gao,Xinran Gu,Longyu Guan,Haiqing Guo,Jianhang Guo,Hao Hu,Xiaoru Hao,Tianhong He,Weiran He,Wenyang He,Chao Hong,Yangyang Hu,Zhenxing Hu,Weixiao Huang,Zhiqi Huang,Zihao Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yongsheng Kang,Guokun Lai,Cheng Li,Fang Li,Haoyang Li,Ming Li,Wentao Li,Yanhao Li,Yiwei Li,Zhaowei Li,Zheming Li,Hongzhan Lin,Xiaohan Lin,Zongyu Lin,Chengyin Liu,Chenyu Liu,Hongzhang Liu,Jingyuan Liu,Junqi Liu,Liang Liu,Shaowei Liu,T. Y. Liu,Tianwei Liu,Weizhou Liu,Yangyang Liu,Yibo Liu,Yiping Liu,Yue Liu,Zhengying Liu,Enzhe Lu,Lijun Lu,Shengling Ma,Xinyu Ma,Yingwei Ma,Shaoguang Mao,Jie Mei,Xin Men,Yibo Miao,Siyuan Pan,Yebo Peng,Ruoyu Qin,Bowen Qu,Zeyu Shang,Lidong Shi,Shengyuan Shi,Feifan Song,Jianlin Su,Zhengyuan Su,Xinjie Sun,Flood Sung,Heyi Tang,Jiawen Tao,Qifeng Teng,Chensi Wang,Dinglu Wang,Feng Wang,Haiming Wang,Jianzhou Wang,Jiaxing Wang,Jinhong Wang,Shengjie Wang,Shuyi Wang,Yao Wang,Yejie Wang,Yiqin Wang,Yuxin Wang,Yuzhi Wang,Zhaoji Wang,Zhengtao Wang,Zhexu Wang,Chu Wei,Qianqian Wei,Wenhao Wu,Xingzhe Wu,Yuxin Wu,Chenjun Xiao,Xiaotong Xie,Weimin Xiong,Boyu Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinran Xu,Yangchuan Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Xiaofei Yang,Ying Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Xingcheng Yao,Wenjie Ye,Zhuorui Ye,Bohong Yin,Longhui Yu,Enming Yuan,Hongbang Yuan,Mengjie Yuan,Haobing Zhan,Dehao Zhang,Hao Zhang,Wanlu Zhang,Xiaobin Zhang,Yangkun Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Haotian Zhao,Yikai Zhao,Huabin Zheng,Shaojie Zheng,Jianren Zhou,Xinyu Zhou,Zaida Zhou,Zhen Zhu,Weiyu Zhuang,Xinxing Zu*

Main category: cs.LG

TL;DR: Kimi K2 是一个由 320 亿激活参数和 1 万亿总参数组成的专家混合（MoE）大型语言模型，在 MuonClip 优化器的支持下，它在 15.5 万亿个代币上进行了预训练，并通过代理数据合成和强化学习进行了微调。Kimi K2 在各种基准测试中都取得了最先进的性能，特别是在代理和软件工程任务方面，并且在编码、数学和推理方面也表现出色。


<details>
  <summary>Details</summary>
Motivation: 介绍 Kimi K2，一个具有 320 亿激活参数和 1 万亿总参数的专家混合（MoE）大型语言模型。

Method: Kimi K2 是一个具有 320 亿个激活参数和 1 万亿个总参数的专家混合（MoE）大型语言模型。通过新颖的 QK-clip 技术，MuonClip 优化器在 Muon 的基础上进行了改进，以解决训练不稳定的问题，同时享受 Muon 的高级代币效率。K2 在 15.5 万亿个代币上进行了预训练，期间没有出现任何损失峰值。在训练后，K2 经过了一个多阶段的训练后过程，其中一个大规模的代理数据合成流程和一个联合强化学习（RL）阶段是其亮点，模型通过与真实和合成环境的交互来提高其能力。

Result: Kimi K2 在开源非思考模型中取得了最先进的性能，在代理能力方面表现突出。值得注意的是，K2 在 Tau2-Bench 上获得了 66.1 分，在 ACEBench（英文）上获得了 75.5 分，在 SWE-Bench Verified 上获得了 65.8 分，在 SWE-Bench Multilingual 上获得了 47.3 分，在非思考设置中超过了大多数开源和闭源基线。它在编码、数学和推理任务方面也表现出强大的能力，在 LiveCodeBench v6 上获得 53.7 分，在 AIME 2025 上获得 49.5 分，在 GPQA-Diamond 上获得 75.1 分，在 OJBench 上获得 27.1 分，所有这些都没有经过扩展思考。

Conclusion: Kimi K2 在软件工程和代理任务方面是迄今为止最强大的开源大型语言模型之一，其基线和训练后模型检查点已发布，以促进代理智能的未来研究和应用。

Abstract: We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32
billion activated parameters and 1 trillion total parameters. We propose the
MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to
address training instability while enjoying the advanced token efficiency of
Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero
loss spike. During post-training, K2 undergoes a multi-stage post-training
process, highlighted by a large-scale agentic data synthesis pipeline and a
joint reinforcement learning (RL) stage, where the model improves its
capabilities through interactions with real and synthetic environments.
  Kimi K2 achieves state-of-the-art performance among open-source non-thinking
models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on
Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on
SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in
non-thinking settings. It also exhibits strong capabilities in coding,
mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6,
49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without
extended thinking. These results position Kimi K2 as one of the most capable
open-source large language models to date, particularly in software engineering
and agentic tasks. We release our base and post-trained model checkpoints to
facilitate future research and applications of agentic intelligence.

</details>


### [537] [Dissecting Persona-Driven Reasoning in Language Models via Activation Patching](https://arxiv.org/abs/2507.20936)
*Ansh Poonia,Maeghal Jain*

Main category: cs.LG

TL;DR: LLM能采用多种身份，本研究使用激活修复技术探究身份如何影响模型推理。早期MLP层处理身份信息，中间MHA层利用这些信息塑造输出。部分注意力头关注种族和肤色身份。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在分配身份时，其在客观任务上的推理能力。

Method: 使用激活修复技术，探究模型关键组件如何编码特定身份信息。

Result: 早期MLP层处理句法和语义内容，将身份令牌转化为更丰富的表征，供中间MHA层塑造模型输出。识别出某些注意力头不成比例地关注基于种族和肤色的身份。

Conclusion: 研究表明，早期MLP层不仅关注输入句法结构，还处理语义内容，将身份令牌转化为更丰富的表征，供中间MHA层塑造模型输出。同时，识别出特定注意力头不成比例地关注基于种族和肤色的身份。

Abstract: Large language models (LLMs) exhibit remarkable versatility in adopting
diverse personas. In this study, we examine how assigning a persona influences
a model's reasoning on an objective task. Using activation patching, we take a
first step toward understanding how key components of the model encode
persona-specific information. Our findings reveal that the early Multi-Layer
Perceptron (MLP) layers attend not only to the syntactic structure of the input
but also process its semantic content. These layers transform persona tokens
into richer representations, which are then used by the middle Multi-Head
Attention (MHA) layers to shape the model's output. Additionally, we identify
specific attention heads that disproportionately attend to racial and
color-based identities.

</details>


### [538] [LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning](https://arxiv.org/abs/2507.20999)
*Yining Huang,Bin Li,Keke Tang,Meilian Chen*

Main category: cs.LG

TL;DR: LoRA-PAR通过划分数据和参数为系统1/系统2，并结合SFT和RL两阶段微调，实现了更高效的微调。


<details>
  <summary>Details</summary>
Motivation: 受“思考，快与慢”启发，为了解决现有参数高效微调（PEFT）方法主要关注领域适应或层级分配，而非明确针对不同响应需求定制数据和参数的问题，提出LoRA-PAR框架，期望通过划分数据和参数以适应不同响应需求，从而在降低成本的同时提升模型性能。

Method: LoRA-PAR是一个双系统LoRA框架，它根据任务需求将数据和参数划分为系统1（快速、直观）或系统2（深思熟虑、分析性），并采用两阶段微调策略：首先使用监督微调（SFT）训练系统1任务以增强知识和直觉，然后使用强化学习（RL）优化系统2任务以加强逻辑推理。

Result: 实验证明，LoRA-PAR的两阶段微调策略（SFT和RL）在降低活跃参数使用量的同时，性能与最先进的PEFT基线相当或超越了它们。

Conclusion: LoRA-PAR通过将数据和参数划分为系统1或系统2，并采用两阶段微调策略（SFT和RL），在降低活跃参数使用量的同时，达到了与最先进的PEFT基线相当甚至更好的性能。

Abstract: Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit
substantially from chain-of-thought (CoT) reasoning, yet pushing their
performance typically requires vast data, large model sizes, and full-parameter
fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,
most existing approaches primarily address domain adaptation or layer-wise
allocation rather than explicitly tailoring data and parameters to different
response demands. Inspired by "Thinking, Fast and Slow," which characterizes
two distinct modes of thought-System 1 (fast, intuitive, often automatic) and
System 2 (slower, more deliberative and analytic)-we draw an analogy that
different "subregions" of an LLM's parameters might similarly specialize for
tasks that demand quick, intuitive responses versus those requiring multi-step
logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework
that partitions both data and parameters by System 1 or System 2 demands, using
fewer yet more focused parameters for each task. Specifically, we classify task
data via multi-model role-playing and voting, and partition parameters based on
importance scoring, then adopt a two-stage fine-tuning strategy of training
System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and
intuition and refine System 2 tasks with reinforcement learning (RL) to
reinforce deeper logical deliberation next. Extensive experiments show that the
two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while
matching or surpassing SOTA PEFT baselines.

</details>


### [539] [GNSP: Gradient Null Space Projection for Preserving Cross-Modal Alignment in VLMs Continual Learning](https://arxiv.org/abs/2507.19839)
*Tiantian Peng,Yuyang Liu,Shuo Yang,Qiuhe Hong,YongHong Tian*

Main category: cs.LG

TL;DR: GNSP是一种有效的持续学习方法，通过梯度投影解决CLIP的灾难性遗忘问题，并在保持其零样本能力方面取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: CLIP在持续多任务微调时存在灾难性遗忘和嵌入对齐退化的问题，损害了其零样本能力。

Method: 提出梯度零空间投影（GNSP）方法，通过将任务特定梯度投影到先前学习知识的零空间来防止对先前任务的干扰，无需重演或架构修改。结合知识蒸馏和受CLIP预训练启发的模态对齐保持损失，以稳定多模态嵌入空间。

Result: 在包含11个任务的MTIL基准测试中，GNSP在平均和最后一个关键指标上均达到SOTA性能。实验表明，GNSP成功地维持了CLIP的原始模态间隙和跨模态检索性能，有效保持了持续学习过程中的鲁棒视觉-语言空间。

Conclusion: GNSP方法通过将任务梯度投影到先前学习知识的零空间来有效应对灾难性遗忘，并在MTIL基准测试中实现了SOTA性能，同时保持了CLIP的原始模态间隙和跨模态检索性能。

Abstract: Contrastive Language-Image Pretraining has demonstrated remarkable zero-shot
generalization by aligning visual and textual modalities in a shared embedding
space. However, when continuously fine-tuned on diverse tasks, CLIP suffers
from catastrophic forgetting and degradation of its embedding alignment,
undermining its zero-shot capabilities. In this work, we propose Gradient Null
Space Projection (GNSP), an efficient continual learning method that projects
task-specific gradients onto the null space of previously learned knowledge.
This orthogonal projection mathematically prevents interference with previous
tasks without relying on rehearsal or architectural modification. Furthermore,
to preserve the inherent generalization property of CLIP, we introduce
knowledge distillation and combine it with a modality alignment preservation
loss inspired by CLIP pre-training to stabilize the structure of the multimodal
embedding space during fine-tuning. On the MTIL benchmark consisting of 11
tasks, our method achieved SOTA performance on both the Average and Last key
metrics. More importantly, experiments show that our method successfully
maintains the original modality gap and cross-modal retrieval performance of
CLIP, confirming its effectiveness in maintaining a robust visual-language
space throughout the continual learning process.

</details>


### [540] [CLoRA: Parameter-Efficient Continual Learning with Low-Rank Adaptation](https://arxiv.org/abs/2507.19887)
*Shishir Muralidhara,Didier Stricker,René Schuster*

Main category: cs.LG

TL;DR: CLoRA是一种参数高效的持续学习方法，通过低秩适配（LoRA）解决灾难性遗忘问题，并在资源受限的情况下实现高效率和高性能。


<details>
  <summary>Details</summary>
Motivation: 传统的持续学习方法在处理序列任务时，会因为灾难性遗忘而导致性能下降。然而，现有方法通常忽略了计算资源的限制。在实际应用中，计算资源往往是受限的，而重新训练整个模型对于大型模型来说计算成本过高，限制了持续学习的应用。因此，需要一种能够解决灾难性遗忘且计算效率高的方法，特别是在资源受限的环境中。

Method: CLoRA是一种将低秩适配（LoRA）技术应用于类别增量语义分割的持续学习方法。它通过利用一小部分可训练参数来学习新任务，并跨所有任务复用这些参数，从而避免了为每个新任务重新训练整个模型的计算开销。

Result: CLoRA在类别增量语义分割任务中表现出有效性，其性能与基线方法相当甚至超越了它们。通过NetScore的评估，CLoRA不仅在任务性能上表现良好，还显著降低了训练所需的硬件资源，证明了其在资源受限环境中的优势。

Conclusion: CLoRA通过使用参数高效的微调方法（LoRA）来解决持续学习中的灾难性遗忘问题，并在资源受限的环境中实现了与基线方法相当甚至更好的性能。该方法通过复用少量参数来学习新任务，显著降低了硬件需求，使其适用于部署后的持续学习场景。此外，研究强调了在评估持续学习方法时，除了任务性能外，还应考虑资源效率（如NetScore）。

Abstract: In the past, continual learning (CL) was mostly concerned with the problem of
catastrophic forgetting in neural networks, that arises when incrementally
learning a sequence of tasks. Current CL methods function within the confines
of limited data access, without any restrictions imposed on computational
resources. However, in real-world scenarios, the latter takes precedence as
deployed systems are often computationally constrained. A major drawback of
most CL methods is the need to retrain the entire model for each new task. The
computational demands of retraining large models can be prohibitive, limiting
the applicability of CL in environments with limited resources. Through CLoRA,
we explore the applicability of Low-Rank Adaptation (LoRA), a
parameter-efficient fine-tuning method for class-incremental semantic
segmentation. CLoRA leverages a small set of parameters of the model and uses
the same set for learning across all tasks. Results demonstrate the efficacy of
CLoRA, achieving performance on par with and exceeding the baseline methods. We
further evaluate CLoRA using NetScore, underscoring the need to factor in
resource efficiency and evaluate CL methods beyond task performance. CLoRA
significantly reduces the hardware requirements for training, making it
well-suited for CL in resource-constrained environments after deployment.

</details>


### [541] [WEEP: A Differentiable Nonconvex Sparse Regularizer via Weakly-Convex Envelope](https://arxiv.org/abs/2507.20447)
*Takanobu Furuhashi,Hidekata Hontani,Tatsuya Yokota*

Main category: cs.LG

TL;DR: WEP是一种新颖、完全可微分的稀疏正则化器，解决了现有稀疏方法在可微性方面的限制，并在信号和图像去噪任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有强大稀疏诱导惩罚（通常不可微）与主导该领域的基于梯度优化的冲突。

Method: 提出了一种新颖的、完全可微分的稀疏正则化器WEP（分段惩罚的弱凸包），它源自弱凸包框架。

Result: WEP在具有挑战性的信号和图像去噪任务上，与L1范数和其他已建立的非凸稀疏正则化器相比，表现出优越的性能。

Conclusion: WEP提供强大的、无偏的稀疏性，同时保持完全可微分性和L-光滑性，使其能够与任何基于梯度的优化器原生兼容，从而解决了统计性能与计算处理能力之间的冲突。

Abstract: Sparse regularization is fundamental in signal processing for efficient
signal recovery and feature extraction. However, it faces a fundamental
dilemma: the most powerful sparsity-inducing penalties are often
non-differentiable, conflicting with gradient-based optimizers that dominate
the field. We introduce WEEP (Weakly-convex Envelope of Piecewise Penalty), a
novel, fully differentiable sparse regularizer derived from the weakly-convex
envelope framework. WEEP provides strong, unbiased sparsity while maintaining
full differentiability and L-smoothness, making it natively compatible with any
gradient-based optimizer. This resolves the conflict between statistical
performance and computational tractability. We demonstrate superior performance
compared to the L1-norm and other established non-convex sparse regularizers on
challenging signal and image denoising tasks.

</details>


### [542] [Model-Agnostic Gender Bias Control for Text-to-Image Generation via Sparse Autoencoder](https://arxiv.org/abs/2507.20973)
*Chao Wu,Zhenyi Wang,Kangxian Xie,Naresh Kumar Devulapally,Vishnu Suresh Lokhande,Mingchen Gao*

Main category: cs.LG

TL;DR: SAE Debias 是一种创新的、模型无关的方法，用于解决文本到图像生成中的性别偏见问题。它利用稀疏自编码器在潜在空间中识别并纠正性别刻板印象，从而实现更公平的生成，并且不影响图像质量。


<details>
  <summary>Details</summary>
Motivation: 文本到图像（T2I）扩散模型常常表现出性别偏见，特别是在生成刻板的性别化职业关联方面。现有的方法（如基于 CLIP 的过滤或提示工程）通常需要模型特定的调整且控制有限。

Method: SAE Debias 利用预训练在性别偏见数据集上的 k-稀疏自编码器，在稀疏潜在空间中识别与性别相关的方向，从而捕捉职业刻板印象。通过为每种职业构建一个有偏见的稀疏潜在方向并在推理过程中抑制它，可以引导生成朝着更性别平衡的输出。该方法不需要重新训练或架构修改，并且训练一次后即可重复使用。

Result: SAE Debias 在 Stable Diffusion 1.4、1.5、2.1 和 SDXL 等多个 T2I 模型上进行了广泛评估，结果表明该方法能显著减少性别偏见，同时保持了生成质量。

Conclusion: SAE Debias 是一种轻量级、模型无关的框架，通过直接在特征空间操作来减少文本到图像生成中的性别偏见，同时保持生成质量。它是首个将稀疏自编码器应用于识别和干预文本到图像模型中性别偏见的工作。

Abstract: Text-to-image (T2I) diffusion models often exhibit gender bias, particularly
by generating stereotypical associations between professions and gendered
subjects. This paper presents SAE Debias, a lightweight and model-agnostic
framework for mitigating such bias in T2I generation. Unlike prior approaches
that rely on CLIP-based filtering or prompt engineering, which often require
model-specific adjustments and offer limited control, SAE Debias operates
directly within the feature space without retraining or architectural
modifications. By leveraging a k-sparse autoencoder pre-trained on a gender
bias dataset, the method identifies gender-relevant directions within the
sparse latent space, capturing professional stereotypes. Specifically, a biased
direction per profession is constructed from sparse latents and suppressed
during inference to steer generations toward more gender-balanced outputs.
Trained only once, the sparse autoencoder provides a reusable debiasing
direction, offering effective control and interpretable insight into biased
subspaces. Extensive evaluations across multiple T2I models, including Stable
Diffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially
reduces gender bias while preserving generation quality. To the best of our
knowledge, this is the first work to apply sparse autoencoders for identifying
and intervening in gender bias within T2I models. These findings contribute
toward building socially responsible generative AI, providing an interpretable
and model-agnostic tool to support fairness in text-to-image generation.

</details>


### [543] [Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning](https://arxiv.org/abs/2507.21049)
*Zedong Wang,Siyuan Li,Dan Xu*

Main category: cs.LG

TL;DR: Rep-MTL 是一种新的多任务学习方法，通过关注共享表示空间中的任务交互，而不是仅仅解决冲突，来提高性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务优化（MTO）技术主要集中在通过损失缩放和梯度操纵来解决任务冲突，但未能带来持续的收益。本文认为，共享表示空间提供了丰富的信息，可以用于 MTO 的操作，特别是在促进任务互补性方面，这在 MTO 中很少被探索。

Method: Rep-MTL 是一种新的多任务学习优化技术，它通过利用共享表示空间中的任务显著性来量化任务之间的相互作用。具体来说，它采用基于熵的惩罚和样本级别的跨任务对齐来指导任务显著性，以促进互补信息的共享并减轻负迁移。

Result: 在四个具有挑战性的多任务学习基准（涵盖任务偏移和领域偏移场景）上的实验表明，Rep-MTL 即使在基本等权重策略下，也取得了具有竞争力的性能提升和良好的效率。此外，幂律指数分析证明了 Rep-MTL 在平衡任务特定学习和跨任务共享方面的有效性。

Conclusion: Rep-MTL 通过利用表示层面的任务显著性来量化任务特定的优化和共享表示学习之间的相互作用，从而在多任务学习中取得了有竞争力的性能提升和良好的效率。它通过基于熵的惩罚和样本级别的跨任务对齐来引导这些显著性，从而缓解了负迁移，同时促进了互补信息的共享。

Abstract: Despite the promise of Multi-Task Learning in leveraging complementary
knowledge across tasks, existing multi-task optimization (MTO) techniques
remain fixated on resolving conflicts via optimizer-centric loss scaling and
gradient manipulation strategies, yet fail to deliver consistent gains. In this
paper, we argue that the shared representation space, where task interactions
naturally occur, offers rich information and potential for operations
complementary to existing optimizers, especially for facilitating the
inter-task complementarity, which is rarely explored in MTO. This intuition
leads to Rep-MTL, which exploits the representation-level task saliency to
quantify interactions between task-specific optimization and shared
representation learning. By steering these saliencies through entropy-based
penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate
negative transfer by maintaining the effective training of individual tasks
instead pure conflict-solving, while explicitly promoting complementary
information sharing. Experiments are conducted on four challenging MTL
benchmarks covering both task-shift and domain-shift scenarios. The results
show that Rep-MTL, even paired with the basic equal weighting policy, achieves
competitive performance gains with favorable efficiency. Beyond standard
performance metrics, Power Law exponent analysis demonstrates Rep-MTL's
efficacy in balancing task-specific learning and cross-task sharing. The
project page is available at HERE.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [544] [K-PACT: Kernel Planning for Adaptive Context Switching -- A Framework for Clustering, Placement, and Prefetching in Spectrum Sensing](https://arxiv.org/abs/2507.19662)
*H. Umut Suluhan,Jiahao Lin,Serhan Gener,Chaitali Chakrabarti,Umit Ogras,Ali Akoglu*

Main category: cs.ET

TL;DR: 通过智能规划，该工具优化了频谱感知任务在可重构硬件上的执行，显著减少了切换时间和提高了处理效率，以适应动态频谱环境。


<details>
  <summary>Details</summary>
Motivation: 为了在高效的宽带频谱感知中实现快速的信号存在和类型评估，需要能够快速地在多个假设（每个假设都是一个包含计算密集型内核的决策树工作流）之间进行切换。由于频谱环境的动态性，快速切换假设对于维持低延迟、高吞吐量操作至关重要。

Method: 该研究提出了一种规划工具，用于将频谱感知应用中的假设工作流映射到由带本地指令内存（IMEM）的处理元件（PE）组成的粗粒度可重构架构上。该规划工具通过两个关键任务实现高效映射：1. 将时间上不重叠的内核进行聚类，以便在PE子阵列内共享IMEM资源；2. 将这些聚类放置到硬件上，以确保高效的调度和数据移动。该规划过程被建模为一种多目标优化问题，旨在平衡上下文切换开销、调度延迟和数据流效率的权衡。

Result: 在模拟的频谱感知场景（包含48个并发子信道）中，该研究提出的规划工具相比于基线方法（无预加载）实现了显著的性能提升：将片外二进制获取（off-chip binary fetches）减少了207.81倍，降低了平均切换时间98.24倍，并提高了每子频带的执行时间132.92倍。

Conclusion: 该研究提出了一种智能规划工具，通过将决策树工作流中的内核聚类并映射到可重构架构的处理器元件（PE）上，实现了低延迟、高吞吐量的频谱感知。通过预加载非并发激活的内核到同一指令内存（IMEM），该工具能够实现低延迟的重配置，无需运行时冲突，有效解决了动态频谱环境中快速切换假设的需求。

Abstract: Efficient wideband spectrum sensing requires rapid evaluation and
re-evaluation of signal presence and type across multiple subchannels. These
tasks involve multiple hypothesis testing, where each hypothesis is implemented
as a decision tree workflow containing compute-intensive kernels, including
FFT, matrix operations, and signal-specific analyses. Given dynamic nature of
the spectrum environment, ability to quickly switch between hypotheses is
essential for maintaining low-latency, high-throughput operation. This work
assumes a coarse-grained reconfigurable architecture consisting of an array of
processing elements (PEs), each equipped with a local instruction memory (IMEM)
capable of storing and executing kernels used in spectrum sensing applications.
We propose a planner tool that efficiently maps hypothesis workflows onto this
architecture to enable fast runtime context switching with minimal overhead.
The planner performs two key tasks: clustering temporally non-overlapping
kernels to share IMEM resources within a PE sub-array, and placing these
clusters onto hardware to ensure efficient scheduling and data movement. By
preloading kernels that are not simultaneously active into same IMEM, our tool
enables low-latency reconfiguration without runtime conflicts. It models the
planning process as a multi-objective optimization, balancing trade-offs among
context switch overhead, scheduling latency, and dataflow efficiency. We
evaluate the proposed tool in simulated spectrum sensing scenario with 48
concurrent subchannels. Results show that our approach reduces off-chip binary
fetches by 207.81x, lowers average switching time by 98.24x, and improves
per-subband execution time by 132.92x over baseline without preloading. These
improvements demonstrate that intelligent planning is critical for adapting to
fast-changing spectrum environments in next-generation radio frequency systems.

</details>


### [545] [Enhancing IoT Intrusion Detection Systems through Adversarial Training](https://arxiv.org/abs/2507.19739)
*Karma Gurung,Ashutosh Ghimire,Fathi Amsaad*

Main category: cs.ET

TL;DR: 本研究设计了一个基于XGBoost模型对抗性训练的入侵检测系统，利用NF-ToN-IoT v2数据集和FGSM攻击来提升对物联网网络复杂攻击的检测能力和鲁棒性，准确率分别达到95.3%和94.5%。


<details>
  <summary>Details</summary>
Motivation: 为了应对物联网（IoT）设备激增带来的网络安全挑战和已暴露出的严重安全漏洞，本研究旨在设计一个强大的入侵检测系统（IDS）。

Method: 本研究结合了分布式预处理来管理NF-ToN-IoT v2数据集的大小，并利用了快速梯度符号法（FGSM）对抗性攻击来模拟实际攻击场景，同时采用了XGBoost模型对抗性训练来增强系统的鲁棒性。

Result: 该系统在干净数据上实现了95.3%的准确率，在对抗性数据上实现了94.5%的准确率，表明其能有效应对复杂威胁。

Conclusion: 本研究设计的入侵检测系统（IDS）通过在NF-ToN-IoT v2数据集上学习模式，能够有效检测复杂攻击，并在干净数据上达到95.3%的准确率，在对抗性数据上达到94.5%的准确率，证明了其对抗不断演变的网络威胁的有效性，并为未来的研究奠定了基础。

Abstract: The augmentation of Internet of Things (IoT) devices transformed both
automation and connectivity but revealed major security vulnerabilities in
networks. We address these challenges by designing a robust intrusion detection
system (IDS) to detect complex attacks by learning patterns from the NF-ToN-IoT
v2 dataset. Intrusion detection has a realistic testbed through the dataset's
rich and high-dimensional features. We combine distributed preprocessing to
manage the dataset size with Fast Gradient Sign Method (FGSM) adversarial
attacks to mimic actual attack scenarios and XGBoost model adversarial training
for improved system robustness. Our system achieves 95.3% accuracy on clean
data and 94.5% accuracy on adversarial data to show its effectiveness against
complex threats. Adversarial training demonstrates its potential to strengthen
IDS against evolving cyber threats and sets the foundation for future studies.
Real-time IoT environments represent a future deployment opportunity for these
systems, while extensions to detect emerging threats and zero-day
vulnerabilities would enhance their utility.

</details>


### [546] [Efficient and Fault-Tolerant Memristive Neural Networks with In-Situ Training](https://arxiv.org/abs/2507.20193)
*Santlal Prajapat,Manobendra Nath Mondal,Susmita Sur-Kolay*

Main category: cs.ET

TL;DR: 一篇关于忆阻器基多层神经网络架构及其在机器学习任务中的应用的研究，该架构提高了能效和计算速度，并对不同条件下的鲁棒性进行了评估。


<details>
  <summary>Details</summary>
Motivation: 为了加速人工神经网络（ANN）的计算，利用了具有并行和内存处理能力的神经形态架构。

Method: 提出了一种新颖的忆阻器基多层神经网络（memristive MLNN）架构和一种高效的原位训练算法。该设计利用忆阻器交叉阵列的固有并行性，以恒定时间 O(1) 执行矩阵向量乘法、外积和权重更新。每个突触仅使用一个忆阻器实现，无需晶体管，提高了面积和能效。

Result: 在 IRIS、NASA 小行星和乳腺癌数据集上实现了 98.22%、90.43% 和 98.59% 的分类准确率。通过引入随机选择的忆阻器卡滞-导通状态故障、忆阻器电导非线性和 10% 器件变化来评估鲁棒性，结果表明网络的性能未受显著影响。

Conclusion: 该研究提出了一种新颖的忆阻器基多层神经网络（memristive MLNN）架构和一种高效的原位训练算法。该设计利用忆阻器交叉阵列的固有并行性，以恒定时间 O(1) 执行矩阵向量乘法、外积和权重更新。每个突触仅使用一个忆阻器实现，无需晶体管，提高了面积和能效。通过在 IRIS、NASA 小行星和乳腺癌数据集上进行 LTspice 模拟评估，该网络实现了 98.22%、90.43% 和 98.59% 的分类准确率。通过引入随机选择的忆阻器的卡滞-导通状态故障来评估鲁棒性。还分析了忆阻器电导非线性和 10% 器件变化的影响。模拟结果表明，网络的性能不受故障忆阻器、非线性和器件变化等因素的显著影响。

Abstract: Neuromorphic architectures, which incorporate parallel and in-memory
processing, are crucial for accelerating artificial neural network (ANN)
computations. This work presents a novel memristor-based multi-layer neural
network (memristive MLNN) architecture and an efficient in-situ training
algorithm. The proposed design performs matrix-vector multiplications, outer
products, and weight updates in constant time $\mathcal{O}(1)$, leveraging the
inherent parallelism of memristive crossbars. Each synapse is realized using a
single memristor, eliminating the need for transistors, and offering enhanced
area and energy efficiency. The architecture is evaluated through LTspice
simulations on the IRIS, NASA Asteroid, and Breast Cancer Wisconsin datasets,
achieving classification accuracies of 98.22\%, 90.43\%, and 98.59\%,
respectively. Robustness is assessed by introducing stuck-at-conducting-state
faults in randomly selected memristors. The effects of nonlinearity in
memristor conductance and a 10\% device variation are also analyzed. The
simulation results establish that the network's performance is not affected
significantly by faulty memristors, non-linearity, and device variation.

</details>


### [547] [Efficient Memristive Spiking Neural Networks Architecture with Supervised In-Situ STDP Method](https://arxiv.org/abs/2507.20998)
*Santlal Prajapati,Susmita Sur-Kolay,Soumyadeep Dutta*

Main category: cs.ET

TL;DR: 本文提出了一种忆阻器基脉冲神经网络（SNN）架构和一种基于STDP的训练算法，该架构无需外部硬件即可实现横向抑制和不应期，并能并行更新突触权重。在模式识别和分类任务中取得了高准确率和良好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了实现超低功耗的智能设备计算，本文旨在开发一种基于忆阻器的脉冲神经网络（SNN）架构，该架构能够利用时间尖峰编码实现低功耗计算。

Method: 本研究提出了一种忆阻器基脉冲神经网络（SNN）架构，并采用了一种新颖的、受尖峰时间依赖可塑性（STDP）启发的监督式原地学习算法进行训练。该架构在电路层面实现了横向抑制和不应期，无需外部微控制器或辅助控制硬件。所有获胜神经元的突触权重并行更新，提高了训练效率。模块化设计确保了输入数据维度和输出类别数量的可扩展性。

Result: 在模式识别任务中，系统达到了完美的识别率。在分类任务中，使用Iris数据集实现了99.11%的准确率，使用BCW数据集实现了97.9%的准确率。在20%输入噪声下，平均识别率为93.4%，显示了良好的鲁棒性。

Conclusion: 该忆阻器基脉冲神经网络（SNN）架构在模式识别和分类任务中表现出色，在LTspice仿真中分别达到了100%（5x3二值图像）、99.11%（Iris数据集）和97.9%（BCW数据集）的准确率。该架构还表现出良好的鲁棒性，在20%输入噪声下平均识别率为93.4%。此外，研究还分析了卡滞电导故障和忆阻器器件变化对网络性能的影响。

Abstract: Memristor-based Spiking Neural Networks (SNNs) with temporal spike encoding
enable ultra-low-energy computation, making them ideal for battery-powered
intelligent devices. This paper presents a circuit-level memristive spiking
neural network (SNN) architecture trained using a proposed novel supervised
in-situ learning algorithm inspired by spike-timing-dependent plasticity
(STDP). The proposed architecture efficiently implements lateral inhibition and
the refractory period, eliminating the need for external microcontrollers or
ancillary control hardware. All synapses of the winning neurons are updated in
parallel, enhancing training efficiency. The modular design ensures scalability
with respect to input data dimensions and output class count. The SNN is
evaluated in LTspice for pattern recognition (using 5x3 binary images) and
classification tasks using the Iris and Breast Cancer Wisconsin (BCW) datasets.
During testing, the system achieved perfect pattern recognition and high
classification accuracies of 99.11\% (Iris) and 97.9\% (BCW). Additionally, it
has demonstrated robustness, maintaining an average recognition rate of 93.4\%
under 20\% input noise. The impact of stuck-at-conductance faults and memristor
device variations was also analyzed.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [548] [Graded Quantitative Narrowing](https://arxiv.org/abs/2507.19630)
*Mauricio Ayala-Rincón,Thaynara Arielly de Lima,Georg Ehling,Temur Kutsia*

Main category: cs.LO

TL;DR: 论文介绍了定量缩窄法，这是对现有重写系统的扩展，可以处理包含变量的项，并成功应用于解决更广泛的定量等式问题。


<details>
  <summary>Details</summary>
Motivation: 为了将等式推理的度量方面（如项的接近度和计算复杂度）纳入现有重写系统，并解决更广泛的理论中的定量等式问题。

Method: 引入了定量缩窄法，这是定量重写框架的推广，通过在约简步骤中用合一替换匹配来处理包含变量的项，实现了项的同时实例化和重写。

Result: 提出了定量缩窄法，并证明了其正确性，为在Lawvere量词上的定量等式问题提供了一种新的解决方案。

Conclusion: 本研究成功地将定量缩窄法应用于解决Lawvere量词上的定量等式问题，并探讨了其完备性的相关条件。

Abstract: The recently introduced framework of Graded Quantitative Rewriting is an
innovative extension of traditional rewriting systems, in which rules are
annotated with degrees drawn from a quantale. This framework provides a robust
foundation for equational reasoning that incorporates metric aspects, such as
the proximity between terms and the complexity of rewriting-based computations.
Quantitative narrowing, introduced in this paper, generalizes quantitative
rewriting by replacing matching with unification in reduction steps, enabling
the reduction of terms even when they contain variables, through simultaneous
instantiation and rewriting. In the standard (non-quantitative) setting,
narrowing has been successfully applied in various domains, including
functional logic programming, theorem proving, and equational unification.
Here, we focus on quantitative narrowing to solve unification problems in
quantitative equational theories over Lawverean quantales. We establish its
soundness and discuss conditions under which completeness can be ensured. This
approach allows us to solve quantitative equations in richer theories than
those addressed by previous methods.

</details>


### [549] [Scroll nets](https://arxiv.org/abs/2507.19689)
*Pablo Donato*

Main category: cs.LO

TL;DR: A new proof formalism called 'scroll nets' is introduced, based on Peirce's topological notation and Curry-Howard methodology, offering logical and computational expressivity demonstrated through lambda-calculus simulations.


<details>
  <summary>Details</summary>
Motivation: To develop a new formalism for representing proofs in propositional logic based on Peirce's topological notation for implication (scrolls), extending his existential graphs (EGs) by internalizing inference rules (Curry-Howard methodology) to capture logical and computational expressivity.

Method: The paper introduces 'scroll nets' as a new formalism for propositional logic proofs, building upon Peirce's existential graphs (EGs). It internalizes inference rules following the Curry-Howard methodology and distills a diagrammatic representation into a purely graph-theoretic definition. A 'detour' notion is identified to sketch a detour-elimination procedure.

Result: The paper presents scroll nets, a graph-theoretic formalism for propositional logic proofs, along with a detour-elimination procedure. It demonstrates the framework's expressivity by simulating normalization in the simply typed lambda-calculus.

Conclusion: Scroll nets provide a new formalism for representing proofs in propositional logic, derived from Peirce's existential graphs and inspired by the Curry-Howard methodology. They offer a combinatorial, graph-theoretic definition and a detour-elimination procedure akin to cut-elimination, demonstrating logical and computational expressivity by simulating normalization in the simply typed lambda-calculus.

Abstract: We introduce a new formalism for representing proofs in propositional logic
called "scroll nets". Its fundamental construct is the "scroll", a topological
notation for implication proposed by C. S. Peirce at the end of the 19th
century as the basis for his diagrammatic system of existential graphs (EGs).
Scroll nets are derived from EGs by following the Curry-Howard methodology of
internalizing inference rules inside judgments, just as terms in type theory
internalize natural deduction rules. We focus on the intuitionistic implicative
fragment of EGs, starting from a natural diagrammatic representation of scroll
nets, and then distilling their combinatorial essence into a purely
graph-theoretic definition. We also identify a notion of detour, that we use to
sketch a detour-elimination procedure akin to cut-elimination. We illustrate
how to simulate normalization in the simply typed $\lambda$-calculus,
demonstrating both the logical and computational expressivity of our framework.

</details>


### [550] [Synthesis Benchmarks for Automated Reasoning](https://arxiv.org/abs/2507.19827)
*Márton Hajdu,Petra Hozzová,Laura Kovács,Andrei Voronkov,Eva Maria Wagner,Richard Steven Žilinčík*

Main category: cs.LO

TL;DR: 本研究提出了一个用于演绎推理合成（特别是$orall	extit{}$公式和不可计算符号限制）的新型、动态增长的数据集，旨在弥补当前基准测试的不足，并推动该领域的进一步发展。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对演绎推理合成（特别是使用$orall	extit{}$公式表示存在性输出的合成问题）的标准基准测试集，该基准测试集还需支持“不可计算符号限制”。

Method: 通过补充现有基准测试集和创建新的测试集来构建一个包含$orall	extit{}$公式的演绎推理合成基准测试集，并支持“不可计算符号限制”。

Result: 提出了一种新的、动态增长的数据集，旨在激励未来在自动化合成的理论和实践方面的发展。

Conclusion: 该数据集的构建是为了促进演绎推理合成（deductive synthesis）领域的发展，并为该领域提供一个标准化的基准测试集，以支持“不可计算符号限制”（uncomputable symbol restriction）。

Abstract: Program synthesis is the task of constructing a program conforming to a given
specification. We focus on deductive synthesis, and in particular on synthesis
problems with specifications given as $\forall\exists$-formulas, expressing the
existence of an output corresponding to any input. So far there has been no
canonical benchmark set for deductive synthesis using the
$\forall\exists$-format and supporting the so-called uncomputable symbol
restriction. This work presents such a data set, composed by complementing
existing benchmarks by new ones. Our data set is dynamically growing and should
motivate future developments in the theory and practice of automating
synthesis.

</details>


### [551] [A Model-Independent Theory of Probabilistic Testing](https://arxiv.org/abs/2507.19886)
*Weijun Chen,Yuxi Fu,Huan Long,Hao Wu*

Main category: cs.LO

TL;DR: 提出了一种通用的概率测试方法，并研究了相关的测试等价性。


<details>
  <summary>Details</summary>
Motivation: 为了解决现代移动计算基础模型——概率并发系统——的测试问题。

Method: 提出了一种通用的、与模型无关的概率测试方法，利用了新的基于分布的语义和面向进程谓词的概率测试框架。

Result: 研究了模型无关和外部的测试等价性，其中外部等价性是对经典公平/应该等价和可能等价的推广，并且这些等价性被证明是同余的。同时，对这些等价性和概率双模拟进行了详细的比较。

Conclusion: 该方法可以轻松扩展到其他概率并发模型。

Abstract: Probabilistic concurrent systems are foundational models for modern mobile
computing. In this paper, a general model-independent approach to probabilistic
testing is proposed. With the help of a new distribution-based semantics for
probabilistic models and a probabilistic testing framework with respect to
process predicates, the model-independent characterization and the external
characterization for testing equivalences are studied. The latter
characterization can be viewed as the generalization of the classical
fair/should equivalence and may equivalence. These equivalences are shown to be
congruent. A thorough comparison between these equivalences and probabilistic
bisimilarities is carried out. The techniques introduced in this paper can be
easily extended to other probabilistic concurrent models.

</details>


### [552] [Active Monitoring with RTLola: A Specification-Guided Scheduling Approach](https://arxiv.org/abs/2507.20615)
*Jan Baumeister,Bernd Finkbeiner,Frederik Scheerer*

Main category: cs.LO

TL;DR: 通过让监控器主动查询传感器并优化带宽使用，可以更早地检测到系统中的违规行为。


<details>
  <summary>Details</summary>
Motivation: 现有流监控方法在资源受限环境中存在通信资源利用效率低下的问题，因为系统被动地发送数据，而忽略了监控器的需求。

Method: 提出一种主动式监控方法，允许监控器根据其内部状态动态查询传感器数据，并通过调度注解指导带宽分配。

Result: 通过在航空航天领域使用RTLola语言进行演示和性能评估，证明了该方法在提高监控效率和检测违规方面的优势。

Conclusion: 所提出的主动式监控方法在同等带宽占用下，能显著早于固定频率采样的方法检测到规范违规。

Abstract: Stream-based monitoring is a well-established runtime verification approach
which relates input streams, representing sensor readings from the monitored
system, with output streams that capture filtered or aggregated results. In
such approaches, the monitor is a passive external component that continuously
receives sensor data from the system under observation. This setup assumes that
the system dictates what data is sent and when, regardless of the monitor's
current needs. However, in many applications -- particularly in
resource-constrained environments like autonomous aircraft, where energy, size,
or weight are limited -- this can lead to inefficient use of communication
resources. We propose making the monitor an active component that decides,
based on its current internal state, which sensors to query and how often. This
behavior is driven by scheduling annotations in the specification, which guide
the dynamic allocation of bandwidth towards the most relevant data, thereby
improving monitoring efficiency. We demonstrate our approach using the
stream-based specification language RTLola and assess the performance by
monitoring a specification from the aerospace domain. With equal bandwidth
usage, our approach detects specification violations significantly sooner than
monitors sampling all inputs at a fixed frequency.

</details>


### [553] [Automated Catamorphism Synthesis for Solving Constrained Horn Clauses over Algebraic Data Types](https://arxiv.org/abs/2507.20726)
*Hiroyuki Katsura,Naoki Kobayashi,Ken Sakayori,Ryosuke Sato*

Main category: cs.LO

TL;DR: A new solver, Catalia, uses catamorphisms to improve CHC satisfiability checking for Algebraic Data Types, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing solvers for CHCs over ADTs have limited ability to find and express models involving inductively defined functions/predicates.

Method: A framework for automatically discovering catamorphisms on demand and using them to express a model of given CHCs.

Result: Implemented a new CHC solver called Catalia based on the proposed method.

Conclusion: Catalia outperforms state-of-the-art solvers in solving satisfiable CHCs over ADTs, and was used in ChocoCatalia, which won the ADT-LIA category of CHC-COMP 2025.

Abstract: We propose a novel approach to satisfiability checking of Constrained Horn
Clauses (CHCs) over Algebraic Data Types (ADTs). CHC-based automated
verification has gained considerable attention in recent years, leading to the
development of various CHC solvers. However, existing solvers for CHCs over
ADTs are not fully satisfactory, due to their limited ability to find and
express models involving inductively defined functions/predicates (e.g., those
about the sum of list elements). To address this limitation, we consider
catamorphisms (generalized fold functions), and present a framework for
automatically discovering appropriate catamorphisms on demand and using them to
express a model of given CHCs. We have implemented a new CHC solver called
Catalia based on the proposed method. Our experimental results for the CHC-COMP
2024 benchmark show that Catalia outperforms state-of-the-art solvers in
solving satisfiable CHCs over ADTs. Catalia was also used as a core part of the
tool called ChocoCatalia, which won the ADT-LIA category of CHC-COMP 2025.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [554] [GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D Gaussian Splatting](https://arxiv.org/abs/2507.19718)
*David Bauer,Qi Wu,Hamid Gadirov,Kwan-Liu Ma*

Main category: cs.GR

TL;DR: 该研究提出了一种新颖的体积渲染辐射缓存方法，利用 3D 高斯泼溅法，无需增加成本即可提高图像质量。


<details>
  <summary>Details</summary>
Motivation: 为了解决科学可视化中实时路径追踪体积渲染的性能缓慢和像素方差大的问题，该研究提出了一种新颖的辐射缓存方法。

Method: 该方法引入了一种新颖的辐射缓存方法，用于路径追踪的体积渲染。该方法利用了体积场景表示的进步，并将 3D 高斯泼溅法改编为多级路径空间辐射缓存。此缓存可以实时训练，从而动态适应场景参数（如光照配置和传递函数）的变化。

Result: 通过与支持均匀采样和下一次事件估计的基线路径追踪器以及最先进的神经辐射缓存进行比较，该方法在定量和定性分析中都证明了其鲁棒性、易集成性，并且能够显著提高体积可视化应用的渲染质量，同时保持可比的计算效率。

Conclusion: 该方法是一种新颖的辐射缓存方法，可用于路径追踪的体积渲染。通过利用体积场景表示的进步并将 3D 高斯泼溅法改编为多级路径空间辐射缓存，可以实时训练该缓存，从而动态适应场景参数的变化。通过整合此缓存，可以在不增加渲染成本的情况下获得更少噪点、更高质量的图像。

Abstract: Real-time path tracing is rapidly becoming the standard for rendering in
entertainment and professional applications. In scientific visualization,
volume rendering plays a crucial role in helping researchers analyze and
interpret complex 3D data. Recently, photorealistic rendering techniques have
gained popularity in scientific visualization, yet they face significant
challenges. One of the most prominent issues is slow rendering performance and
high pixel variance caused by Monte Carlo integration. In this work, we
introduce a novel radiance caching approach for path-traced volume rendering.
Our method leverages advances in volumetric scene representation and adapts 3D
Gaussian splatting to function as a multi-level, path-space radiance cache.
This cache is designed to be trainable on the fly, dynamically adapting to
changes in scene parameters such as lighting configurations and transfer
functions. By incorporating our cache, we achieve less noisy, higher-quality
images without increasing rendering costs. To evaluate our approach, we compare
it against a baseline path tracer that supports uniform sampling and next-event
estimation and the state-of-the-art for neural radiance caching. Through both
quantitative and qualitative analyses, we demonstrate that our path-space
radiance cache is a robust solution that is easy to integrate and significantly
enhances the rendering quality of volumetric visualization applications while
maintaining comparable computational efficiency.

</details>


### [555] [Taking Language Embedded 3D Gaussian Splatting into the Wild](https://arxiv.org/abs/2507.19830)
*Yuze Wang,Yue Qi*

Main category: cs.GR

TL;DR: 提出了一种新颖的框架，利用语言嵌入式 3D 高斯泼溅技术，从无约束照片集合中实现开放词汇场景理解，并引入了 PT-OVS 数据集进行评估。


<details>
  <summary>Details</summary>
Motivation: 为了弥合 3D  ricostruction 和静态文本-图像对之间的差距，提出了一种从无约束照片集合中理解建筑组件 3D 结构的新方法。

Method: 1. 渲染多视角外观图像。
2. 提取多视角 CLIP 特征和语言特征不确定性图。
3. 提出瞬态不确定性感知自动编码器、多视角语言场 3DGS 表示和后集成策略。
4. 引入 PT-OVS 基准数据集进行评估。

Result: 实现了准确的开放词汇分割，并支持交互式漫游、建筑风格模式识别和 3D 场景编辑。

Conclusion: 该方法在开放词汇分割方面优于现有方法，实现了准确的分割，并支持交互式漫游、建筑风格模式识别和 3D 场景编辑等应用。

Abstract: Recent advances in leveraging large-scale Internet photo collections for 3D
reconstruction have enabled immersive virtual exploration of landmarks and
historic sites worldwide. However, little attention has been given to the
immersive understanding of architectural styles and structural knowledge, which
remains largely confined to browsing static text-image pairs. Therefore, can we
draw inspiration from 3D in-the-wild reconstruction techniques and use
unconstrained photo collections to create an immersive approach for
understanding the 3D structure of architectural components? To this end, we
extend language embedded 3D Gaussian splatting (3DGS) and propose a novel
framework for open-vocabulary scene understanding from unconstrained photo
collections. Specifically, we first render multiple appearance images from the
same viewpoint as the unconstrained image with the reconstructed radiance
field, then extract multi-appearance CLIP features and two types of language
feature uncertainty maps-transient and appearance uncertainty-derived from the
multi-appearance features to guide the subsequent optimization process. Next,
we propose a transient uncertainty-aware autoencoder, a multi-appearance
language field 3DGS representation, and a post-ensemble strategy to effectively
compress, learn, and fuse language features from multiple appearances. Finally,
to quantitatively evaluate our method, we introduce PT-OVS, a new benchmark
dataset for assessing open-vocabulary segmentation performance on unconstrained
photo collections. Experimental results show that our method outperforms
existing methods, delivering accurate open-vocabulary segmentation and enabling
applications such as interactive roaming with open-vocabulary queries,
architectural style pattern recognition, and 3D scene editing.

</details>


### [556] [ChoreoMuse: Robust Music-to-Dance Video Generation with Style Transfer and Beat-Adherent Motion](https://arxiv.org/abs/2507.19836)
*Xuanchen Wang,Heng Wang,Weidong Cai*

Main category: cs.GR

TL;DR: ChoreoMuse 是一个创新的框架，通过使用 SMPL 参数作为音乐和视频之间的桥梁，实现了高质量、风格可控的AI舞蹈视频生成，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动编舞生成方法难以生成既能适应音乐节奏又能符合用户定义编舞风格的高质量舞蹈视频，这限制了它们在现实创意场景中的应用。ChoreoMuse 旨在解决这一差距，实现可控风格、高保真度的舞蹈视频生成，适用于多种音乐类型和舞者特征，并能处理任意分辨率的参考个体。

Method: ChoreoMuse 是一个基于扩散模型（diffusion-based framework）的框架，它使用 SMPL 格式的参数及其变体作为音乐和视频生成之间的中间件，克服了视频分辨率的限制。该方法采用新颖的音乐编码器 MotionTune 来捕捉音频中的运动线索，确保生成的编舞能够紧密跟随输入音乐的节拍和表现力。此外，还引入了两个新指标来量化评估生成的舞蹈与预期风格线索的匹配程度。

Result: 实验结果表明，ChoreoMuse 在视频质量、节拍对齐、舞蹈多样性和风格一致性等多个维度上均取得了最先进的性能。

Conclusion: ChoreoMuse 框架在视频质量、节拍对齐、舞蹈多样性和风格一致性等方面均达到最先进的性能，为广泛的创意应用提供了强大的解决方案。

Abstract: Modern artistic productions increasingly demand automated choreography
generation that adapts to diverse musical styles and individual dancer
characteristics. Existing approaches often fail to produce high-quality dance
videos that harmonize with both musical rhythm and user-defined choreography
styles, limiting their applicability in real-world creative contexts. To
address this gap, we introduce ChoreoMuse, a diffusion-based framework that
uses SMPL format parameters and their variation version as intermediaries
between music and video generation, thereby overcoming the usual constraints
imposed by video resolution. Critically, ChoreoMuse supports
style-controllable, high-fidelity dance video generation across diverse musical
genres and individual dancer characteristics, including the flexibility to
handle any reference individual at any resolution. Our method employs a novel
music encoder MotionTune to capture motion cues from audio, ensuring that the
generated choreography closely follows the beat and expressive qualities of the
input music. To quantitatively evaluate how well the generated dances match
both musical and choreographic styles, we introduce two new metrics that
measure alignment with the intended stylistic cues. Extensive experiments
confirm that ChoreoMuse achieves state-of-the-art performance across multiple
dimensions, including video quality, beat alignment, dance diversity, and style
adherence, demonstrating its potential as a robust solution for a wide range of
creative applications. Video results can be found on our project page:
https://choreomuse.github.io.

</details>


### [557] [Methodology for intelligent injection point location based on geometric algorithms and discrete topologies for virtual digital twin environments](https://arxiv.org/abs/2507.20922)
*J. Mercado Colmenero,A. Torres Alba,C. Martin Donate*

Main category: cs.GR

TL;DR: 本文提出了一种创新的、无需专家干预的注塑零件注射点定位方法，通过几何算法和智能模型优化注射点，实现熔融塑料均匀分布，减少压力损失和成本。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提出一种创新的、无需专家干预的注塑零件注射点定位方法，以实现熔融塑料的均匀分布，减少压力损失，并降低设计时间和成本，同时具备良好的适应性和灵活性，以支持数字孪生和虚拟孪生系统。

Method: 本文提出了一种创新方法，利用具有离散拓扑几何算法的智能模型来定位注塑零件中的注射点。该方法包括一个计算离散模型质心的算法，以及两个评估几何形状和优化注射点位置的子算法。第一个子算法生成一个基于二维节点求积的几何矩阵，第二个子算法将节点矩阵及其相关圆形区域沿脱模方向正交投影到零件表面，通过最小化到质心距离来确定最佳注射点位置。

Result: 通过在六个复杂几何案例研究中进行流变学模拟验证，结果显示熔融塑料分布均匀，压力损失最小，超越了现有技术水平。

Conclusion: 该方法通过流变学模拟在六个复杂几何案例研究中得到了验证，结果表明熔融塑料分布均匀，压力损失最小，超越了现有技术水平，为数字孪生应用提供了敏捷的替代方案，减少了对专家的依赖，促进了设计者培训，并最终降低了成本。

Abstract: This article presents an innovative methodology for locating injection points
in injection-molded parts using intelligent models with geometric algorithms
for discrete topologies. The first algorithm calculates the center of mass of
the discrete model based on the center of mass of each triangular facet in the
system, ensuring uniform molten plastic distribution during mold cavity
filling. Two sub-algorithms intelligently evaluate the geometry and optimal
injection point location. The first sub-algorithm generates a geometric matrix
based on a two-dimensional nodal quadrature adapted to the part's bounding box.
The second sub-algorithm projects the nodal matrix and associated circular
areas orthogonally on the part's surface along the demolding direction. The
optimal injection point location is determined by minimizing the distance to
the center of mass from the first algorithm's result. This novel methodology
has been validated through rheological simulations in six case studies with
complex geometries. The results demonstrate uniform and homogeneous molten
plastic distribution with minimal pressure loss during the filling phase.
Importantly, this methodology does not require expert intervention, reducing
time and costs associated with manual injection mold feed system design. It is
also adaptable to various design environments and virtual twin systems, not
tied to specific CAD software. The validated results surpass the state of the
art, offering an agile alternative for digital twin applications in new product
design environments, reducing dependence on experts, facilitating designer
training, and ultimately cutting costs

</details>


### [558] [Neural Shell Texture Splatting: More Details and Fewer Primitives](https://arxiv.org/abs/2507.20200)
*Xin Zhang,Anpei Chen,Jincheng Xiong,Pinxuan Dai,Yujun Shen,Weiwei Xu*

Main category: cs.GR

TL;DR: 通过解耦几何和外观，并引入神经壳纹理，本研究实现了高参数效率、精细的纹理细节重建和易于提取的纹理网格，同时显著减少了所需的原始数量。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅技术在视图合成方面取得了有希望的结果，但需要大量的原始体，这是由于高斯泼溅中几何和外观的纠缠。

Method: 提出了一种名为神经壳纹理的全局表示，该表示围绕表面编码纹理信息，并使用高斯原始体作为几何表示和纹理场采样器，将纹理特征有效地渲染到图像空间。

Result: 与现有技术相比，该方法在参数效率、纹理细节重建和纹理网格提取方面表现更好，同时使用更少的原始体。

Conclusion: 通过解耦几何和外观，并引入神经壳纹理，本研究实现了高参数效率、精细的纹理细节重建和易于提取的纹理网格，同时显著减少了所需的原始数量。

Abstract: Gaussian splatting techniques have shown promising results in novel view
synthesis, achieving high fidelity and efficiency. However, their high
reconstruction quality comes at the cost of requiring a large number of
primitives. We identify this issue as stemming from the entanglement of
geometry and appearance in Gaussian Splatting. To address this, we introduce a
neural shell texture, a global representation that encodes texture information
around the surface. We use Gaussian primitives as both a geometric
representation and texture field samplers, efficiently splatting texture
features into image space. Our evaluation demonstrates that this
disentanglement enables high parameter efficiency, fine texture detail
reconstruction, and easy textured mesh extraction, all while using
significantly fewer primitives.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [559] [Agent WARPP: Workflow Adherence via Runtime Parallel Personalization](https://arxiv.org/abs/2507.19543)
*Maria Emilia Mazzolenis,Ruirui Zhang*

Main category: cs.AI

TL;DR: WARPP框架通过多智能体协同和运行时个性化，提升了LLM在复杂对话任务中的工作流遵循能力，效果优于现有方法且无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在面向任务的对话（TOD）系统中应用广泛，但它们在处理涉及外部工具调用和依赖用户特定信息的长而复杂的条件工作流时常常遇到困难。

Method: WARPP框架采用训练无关（training-free）的模块化设计，结合了多智能体协同（multi-agent orchestration）和运行时个性化（runtime personalization）。具体而言，它部署了一个并行的架构，其中一个专门的个性化智能体（Personalizer agent）与特定领域的智能体协同工作，在运行时动态地根据用户属性（user attributes）来裁剪（pruning）条件分支，以减少推理开销并缩小工具选择范围，从而实时定制执行路径。

Result: 在涉及银行、航班和医疗保健三个领域、五个不同复杂度用户意图的评估中，WARPP框架相较于非个性化方法和ReAct基线，在参数保真度（parameter fidelity）和工具准确性（tool accuracy）方面均取得了更好的效果，并且随着意图复杂度的增加，效果提升越发明显。此外，该框架还能在不进行额外训练的情况下，有效降低平均令牌使用量（token usage）。

Conclusion: WARPP框架在不进行额外训练的情况下，通过引入多智能体协同和运行时个性化，有效提升了LLM在面向任务的对话系统中的工作流遵循能力。该框架通过动态剪枝条件分支和并行化架构，提高了参数保真度和工具准确性，并减少了平均令牌使用量，尤其在处理复杂意图时效果更显著。

Abstract: Large language models (LLMs) are increasingly applied in task-oriented
dialogue (TOD) systems but often struggle with long, conditional workflows that
involve external tool calls and depend on user-specific information. We present
Workflow Adherence via Runtime Parallel Personalization, or WARPP, a
training-free, modular framework that combines multi-agent orchestration with
runtime personalization to improve workflow adherence in LLM-based systems. By
dynamically pruning conditional branches based on user attributes, the
framework reduces reasoning overhead and narrows tool selection at runtime.
WARPP deploys a parallelized architecture where a dedicated Personalizer agent
operates alongside modular, domain-specific agents to dynamically tailor
execution paths in real time. The framework is evaluated across five
representative user intents of varying complexity within three domains:
banking, flights, and healthcare. Our evaluation leverages synthetic datasets
and LLM-powered simulated users to test scenarios with conditional
dependencies. Our results demonstrate that WARPP outperforms both the
non-personalized method and the ReAct baseline, achieving increasingly larger
gains in parameter fidelity and tool accuracy as intent complexity grows, while
also reducing average token usage, without any additional training.

</details>


### [560] [Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems](https://arxiv.org/abs/2507.19593)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: 这是一篇关于超博弈论在多主体系统（MAS）中应用的综述，重点关注其在克服经典博弈论假设限制方面的能力。研究回顾了44项相关研究，提出了一个分类框架，并指出了当前研究的趋势、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 为了克服经典博弈论模型中关于理性代理、信息完整性和支付共同知识的假设在现实世界的多主体系统中（MAS）常常被违反的限制，该研究旨在系统性地回顾和分析主体兼容的超博弈论应用。

Method: 本研究对44项选定的研究进行了系统性回顾，涵盖了网络安全、机器人、社交模拟、通信和一般博弈论建模等领域。研究基于超博弈论及其两种主要扩展——分层超博弈和HNF——的正式介绍，开发了主体兼容性标准和基于主体的分类框架，以评估集成模式和实际适用性。

Result: 分析显示，分层和基于图的模型在欺骗性推理中普遍存在，并且在实际应用中，广泛的理论框架得到了简化。研究还发现，HNF模型采用有限，缺乏正式的超博弈语言，以及在模拟人类-主体和主体-主体不匹配方面存在未探索的机会。

Conclusion: 该综述通过合成趋势、挑战和开放的研究方向，为应用超博弈论以增强动态多主体环境中战略建模的现实性和有效性提供了新的路线图。

Abstract: Classical game-theoretic models typically assume rational agents, complete
information, and common knowledge of payoffs - assumptions that are often
violated in real-world MAS characterized by uncertainty, misaligned
perceptions, and nested beliefs. To overcome these limitations, researchers
have proposed extensions that incorporate models of cognitive constraints,
subjective beliefs, and heterogeneous reasoning. Among these, hypergame theory
extends the classical paradigm by explicitly modeling agents' subjective
perceptions of the strategic scenario, known as perceptual games, in which
agents may hold divergent beliefs about the structure, payoffs, or available
actions. We present a systematic review of agent-compatible applications of
hypergame theory, examining how its descriptive capabilities have been adapted
to dynamic and interactive MAS contexts. We analyze 44 selected studies from
cybersecurity, robotics, social simulation, communications, and general
game-theoretic modeling. Building on a formal introduction to hypergame theory
and its two major extensions - hierarchical hypergames and HNF - we develop
agent-compatibility criteria and an agent-based classification framework to
assess integration patterns and practical applicability. Our analysis reveals
prevailing tendencies, including the prevalence of hierarchical and graph-based
models in deceptive reasoning and the simplification of extensive theoretical
frameworks in practical applications. We identify structural gaps, including
the limited adoption of HNF-based models, the lack of formal hypergame
languages, and unexplored opportunities for modeling human-agent and
agent-agent misalignment. By synthesizing trends, challenges, and open research
directions, this review provides a new roadmap for applying hypergame theory to
enhance the realism and effectiveness of strategic modeling in dynamic
multi-agent environments.

</details>


### [561] [Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems](https://arxiv.org/abs/2507.20010)
*Müge Fidan,Esra Erdem*

Main category: cs.AI

TL;DR: 本研究提出了一种为稳定室友问题生成个性化解决方案的方法，该方法考虑了代理的习惯、偏好和友谊网络，以在没有完美解决方案的情况下找到“足够好”的匹配。


<details>
  <summary>Details</summary>
Motivation: 由于并非所有稳定室友问题都总是有解决方案，因此我们希望计算“足够好”的匹配。

Method: 通过考虑代理的习惯和偏好，以及他们喜欢的友谊网络，我们提出了一种生成个性化解决方案的方法。

Result: 该方法通过示例和实证评估展示了其有效性。

Conclusion: 计算

Abstract: The Stable Roommates problems are characterized by the preferences of agents
over other agents as roommates. A solution is a partition of the agents into
pairs that are acceptable to each other (i.e., they are in the preference lists
of each other), and the matching is stable (i.e., there do not exist any two
agents who prefer each other to their roommates, and thus block the matching).
Motivated by real-world applications, and considering that stable roommates
problems do not always have solutions, we continue our studies to compute
"good-enough" matchings. In addition to the agents' habits and habitual
preferences, we consider their networks of preferred friends, and introduce a
method to generate personalized solutions to stable roommates problems. We
illustrate the usefulness of our method with examples and empirical
evaluations.

</details>


### [562] [A Multi-Agent System for Information Extraction from the Chemical Literature](https://arxiv.org/abs/2507.20230)
*Yufan Chen,Ching Ting Leung,Bowen Yu,Jianwei Sun,Yong Huang,Linyan Li,Hao Chen,Hanyu Gao*

Main category: cs.AI

TL;DR: 该研究利用多模态大语言模型（MLLM）和多智能体系统，在从化学文献的复杂图形中自动提取化学信息方面取得了突破性进展，将 F1 分数从 35.6% 提高到 80.8%，为人工智能驱动的化学研究奠定了重要基础。


<details>
  <summary>Details</summary>
Motivation: 为了充分加速人工智能驱动的化学研究，高质量的化学数据库是基石。从文献中自动提取化学信息对于构建反应数据库至关重要，但目前受到化学信息的多模态和样式可变性的限制。

Method: 开发了一个基于多模态大语言模型（MLLM）的多智能体系统，利用MLLM强大的推理能力来理解复杂化学图形的结构，将提取任务分解为子任务，并协调一组专门的智能体来解决它们。

Result: 该系统在来自文献的复杂化学反应图形的基准数据集上取得了 80.8% 的 F1 分数，显著超过了之前最先进的模型（F1 分数：35.6%）。此外，在分子图像识别、反应图像解析、命名实体识别和基于文本的反应提取等关键子任务中也展示了持续的改进。

Conclusion: 该研究是朝着将化学信息自动提取到结构化数据集中迈出的关键一步，这将有力地推动人工智能驱动的化学研究。

Abstract: To fully expedite AI-powered chemical research, high-quality chemical
databases are the cornerstone. Automatic extraction of chemical information
from the literature is essential for constructing reaction databases, but it is
currently limited by the multimodality and style variability of chemical
information. In this work, we developed a multimodal large language model
(MLLM)-based multi-agent system for automatic chemical information extraction.
We used the MLLM's strong reasoning capability to understand the structure of
complex chemical graphics, decompose the extraction task into sub-tasks and
coordinate a set of specialized agents to solve them. Our system achieved an F1
score of 80.8% on a benchmark dataset of complex chemical reaction graphics
from the literature, surpassing the previous state-of-the-art model (F1 score:
35.6%) by a significant margin. Additionally, it demonstrated consistent
improvements in key sub-tasks, including molecular image recognition, reaction
image parsing, named entity recognition and text-based reaction extraction.
This work is a critical step toward automated chemical information extraction
into structured datasets, which will be a strong promoter of AI-driven chemical
research.

</details>


### [563] [Core Safety Values for Provably Corrigible Agents](https://arxiv.org/abs/2507.20964)
*Aran Nayebi*

Main category: cs.AI

TL;DR: 本研究提出了一个新颖的人工智能可纠正性框架，通过分离和组合五个效用头来解决激励冲突和代理修改问题，并提供了可证明的安全保证。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决人工智能系统的可纠正性问题，特别是其在多步、部分可观测环境中的行为。现有的方法，如 Constitutional AI 或 RLHF/RLAIF，将所有规范合并为一个学习到的标量，这可能导致在激励冲突时出现问题。因此，本研究的动机是开发一种新的框架，该框架能够分离不同的规范，确保服从性和影响限制，即使在存在冲突的激励的情况下也能占主导地位。此外，研究还旨在应对在开放式环境中代理可能被修改的情况，并为这些情况下的安全验证提供解决方案。

Method: 该研究引入了一个包含五个结构上分离的效用头的可实现框架：顺从性、开关访问保留、真实性、基于信念的攻击性效用保留的低影响行为以及有界任务奖励。这些效用头通过严格的权重差距按字母顺序组合。研究人员利用定理来证明在部分可观测的开关游戏中精确单轮可纠正性，并将保证扩展到多步、自生成代理。对于开放式环境，他们通过归约到停机问题证明了决定任意后黑客代理是否会违反可纠正性的问题是不可判定的，然后通过定义一个有限范围的“可判定岛”来解决这个问题，在该岛上可以在随机多项式时间内证明安全性，并使用保护隐私的、恒定轮零知识证明进行验证。

Result: 该研究成功地提出了一个可实现的、具有可证明保证的多步、部分可观测环境的可纠正性框架。该框架通过分离和组合五个效用头来处理可纠正性，即使在代理学习和规划不完美的情况下也能保证安全。研究结果表明，该方法可以有效地处理激励冲突，并确保服从性和影响限制。此外，研究还为开放式环境中代理的安全验证提供了一个解决方案，通过定义一个“可判定岛”来在多项式时间内进行验证，并使用零知识证明来保护隐私。

Conclusion: 该研究提出了一个可实现的、具有可证明保证的多步、部分可观测环境的可纠正性框架。该框架通过五个结构上分离的效用头（顺从性、开关访问保留、真实性、基于信念的攻击性效用保留的低影响行为以及有界任务奖励）来替代单一的不透明奖励，并通过严格的权重差距按字母顺序组合。研究证明了在部分可观测的开关游戏中精确单轮可纠正性，并将保证扩展到多步、自生成代理。即使每个头部在均方误差 ε 下学习，规划者也是 ε-次优的，违反任何安全属性的概率也是有界的，同时仍确保净人类效益。与将所有规范合并为一个标量学习的 Constitutional AI 或 RLHF/RLAIF 不同，该研究的分离使得服从性和影响限制即使在激励冲突时也占主导地位。对于可以修改代理的开放式环境，研究证明了任意后黑客代理是否会违反可纠正性的决定是不可判定的，通过归约到停机问题，然后划分了一个有限范围的“可判定岛”，可以在随机多项式时间内证明安全性，并使用保护隐私的、恒定轮零知识证明进行验证。因此，剩余的挑战是数据覆盖和泛化的普通机器学习任务：奖励破解风险被推入评估质量而不是隐藏的激励泄露，为今天的 LLM 助手和未来的自主系统提供了更清晰的实施指导。

Abstract: We introduce the first implementable framework for corrigibility, with
provable guarantees in multi-step, partially observed environments. Our
framework replaces a single opaque reward with five *structurally separate*
utility heads -- deference, switch-access preservation, truthfulness,
low-impact behavior via a belief-based extension of Attainable Utility
Preservation, and bounded task reward -- combined lexicographically by strict
weight gaps. Theorem 1 proves exact single-round corrigibility in the partially
observable off-switch game; Theorem 3 extends the guarantee to multi-step,
self-spawning agents, showing that even if each head is \emph{learned} to
mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal,
the probability of violating \emph{any} safety property is bounded while still
ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,
which merge all norms into one learned scalar, our separation makes obedience
and impact-limits dominate even when incentives conflict. For open-ended
settings where adversaries can modify the agent, we prove that deciding whether
an arbitrary post-hack agent will ever violate corrigibility is undecidable by
reduction to the halting problem, then carve out a finite-horizon ``decidable
island'' where safety can be certified in randomized polynomial time and
verified with privacy-preserving, constant-round zero-knowledge proofs.
Consequently, the remaining challenge is the ordinary ML task of data coverage
and generalization: reward-hacking risk is pushed into evaluation quality
rather than hidden incentive leak-through, giving clearer implementation
guidance for today's LLM assistants and future autonomous systems.

</details>


### [564] [GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis](https://arxiv.org/abs/2507.21035)
*Haoyang Liu,Yijiang Li,Haohan Wang*

Main category: cs.AI

TL;DR: GenoMAS是一个由LLM驱动的系统，它将结构化工作流和自主代理相结合，用于基因表达分析，在GenoTEX基准测试中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 为了克服从复杂的、半结构化的基因组学数据中提取见解的挑战，并解决当前自动化方法在灵活性和精确性方面的不足。

Method: GenoMAS采用一个由六个专门的LLM代理组成的团队，通过类型化消息传递协议进行协调，并以引导式规划框架为核心，将高级任务指南分解为行动单元，并在每个节点上进行选择以保持逻辑一致性。

Result: GenoMAS在GenoTEX基准测试中，数据预处理的复合相似性相关性达到89.13%，基因识别的F1分数达到60.48%，分别比现有最佳方法提高了10.61%和16.85%。此外，GenoMAS还能识别出与文献一致的、具有生物学意义的基因-表型关联，并能调整潜在的混淆因素。

Conclusion: GenoMAS通过结合结构化工作流的可靠性和自主代理的适应性，在基因表达分析领域取得了显著进展，并在GenoTEX基准测试中超越了现有最佳方法。

Abstract: Gene expression analysis holds the key to many biomedical discoveries, yet
extracting insights from raw transcriptomic data remains formidable due to the
complexity of multiple large, semi-structured files and the need for extensive
domain expertise. Current automation approaches are often limited by either
inflexible workflows that break down in edge cases or by fully autonomous
agents that lack the necessary precision for rigorous scientific inquiry.
GenoMAS charts a different course by presenting a team of LLM-based scientists
that integrates the reliability of structured workflows with the adaptability
of autonomous agents. GenoMAS orchestrates six specialized LLM agents through
typed message-passing protocols, each contributing complementary strengths to a
shared analytic canvas. At the heart of GenoMAS lies a guided-planning
framework: programming agents unfold high-level task guidelines into Action
Units and, at each juncture, elect to advance, revise, bypass, or backtrack,
thereby maintaining logical coherence while bending gracefully to the
idiosyncrasies of genomic data.
  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation
of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene
identification, surpassing the best prior art by 10.61% and 16.85%
respectively. Beyond metrics, GenoMAS surfaces biologically plausible
gene-phenotype associations corroborated by the literature, all while adjusting
for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.

</details>


### [565] [DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference](https://arxiv.org/abs/2507.19608)
*Jiawen Qi,Chang Gao,Zhaochun Ren,Qinyu Chen*

Main category: cs.AI

TL;DR: DeltaLLM框架通过时间稀疏性和混合注意力机制，在边缘设备上实现了高效LLM推理，无需微调，并能在保证准确性的前提下大幅提升稀疏度。


<details>
  <summary>Details</summary>
Motivation: 现有的动态注意力剪枝方法主要针对GPU/TPU等大规模并行计算硬件，并侧重于长上下文长度（如64K），这使得它们不适用于资源受限的边缘设备。因此，需要一种适用于边缘场景的LLM推理优化方法。

Method: DeltaLLM框架利用时间稀疏性，通过感知准确性和内存的delta矩阵构建策略引入稀疏性，并采用感知上下文的混合注意力机制（局部窗口内全注意力，窗口外delta近似）来提高效率和准确性。该框架在预填充和解码阶段均有效，且无需微调。

Result: 在BitNet模型上，DeltaLLM在预填充阶段将注意力稀疏度从0%提高到60%，在WG任务上略有提高准确性；在预填充和解码阶段，稀疏度从0%提高到57%，在SQuAD-v2任务上的F1分数从29.63提高到30.97。在Llama模型上，预填充阶段稀疏度可达60%，两个阶段的平均稀疏度约为57%，且准确性下降可忽略不计。

Conclusion: DeltaLLM是一个无需微调的训练免费框架，通过利用注意力模式中的时间稀疏性，在预填充和解码阶段都能在资源受限的边缘设备上实现高效的LLM推理。它通过感知准确性和内存的delta矩阵构建策略引入时间稀疏性，并结合了感知上下文的混合注意力机制，通过在局部上下文窗口中进行全注意力，并在其外部进行delta近似来提高准确性。实验结果表明，DeltaLLM在BitNet和Llama模型上均实现了显著的注意力稀疏度提升，同时保持了准确性，证明了其在边缘部署方面的潜力。

Abstract: Deploying Large Language Models (LLMs) on edge devices remains challenging
due to their quadratically increasing computations with the sequence length.
Existing studies for dynamic attention pruning are designed for hardware with
massively parallel computation capabilities, such as GPUs or TPUs, and aim at
long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We
present DeltaLLM, a training-free framework that exploits temporal sparsity in
attention patterns to enable efficient LLM inference across both the prefilling
and decoding stages, on resource-constrained edge devices. DeltaLLM introduces
an accuracy- and memory-aware delta matrix construction strategy that
introduces temporal sparsity, and a context-aware hybrid attention mechanism
that combines full attention in a local context window with delta approximation
outside it to increase accuracy. We evaluate our framework on the
edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model
across diverse language tasks. The results show that on BitNet, our framework
increases the attention sparsity from 0% to 60% during the prefilling stage
with slight accuracy improvement on the WG task, and 0% to 57% across both the
prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97
on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity
during the prefilling stage and around 57% across both stages with negligible
accuracy drop. These results demonstrate that DeltaLLM offers a promising
solution for efficient edge deployment, requiring no fine-tuning and seamlessly
integrating with existing inference pipelines.

</details>


### [566] [VLMPlanner: Integrating Visual Language Models with Motion Planning](https://arxiv.org/abs/2507.20342)
*Zhipeng Tang,Sha Zhang,Jiajun Deng,Chenjie Wang,Guoliang You,Yuting Huang,Xinrui Lin,Yanyong Zhang*

Main category: cs.AI

TL;DR: VLMPlanner是一个创新的自动驾驶规划框架，通过结合视觉-语言模型（VLM）和实时规划器，利用原始图像信息进行鲁棒决策，并在复杂场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于抽象感知或地图输入，忽略了诸如细粒度道路线索、事故现场或意外障碍物等关键视觉信息，这对于在复杂驾驶环境中做出鲁棒决策至关重要。本研究旨在弥合这一差距，通过整合VLM来增强自动驾驶的规划能力。

Method: 提出了一种结合基于学习的实时规划器和视觉-语言模型（VLM）的混合框架VLMPlanner。VLM能够处理多视图原始图像，捕捉详细视觉信息，并利用其常识推理能力指导实时规划器生成安全轨迹。此外，开发了上下文自适应推理门（CAI-Gate）机制，使VLM能够根据场景复杂性动态调整推理频率，模仿人类驾驶行为。

Result: 在大型、具有挑战性的nuPlan基准测试上的实验结果表明，VLMPlanner在具有复杂道路条件和动态元素的场景中取得了优越的规划性能。

Conclusion: VLMPlanner框架在nuPlan基准测试中表现出卓越的规划性能，尤其是在处理复杂道路状况和动态元素方面，实现了规划性能与计算效率的最佳平衡。

Abstract: Integrating large language models (LLMs) into autonomous driving motion
planning has recently emerged as a promising direction, offering enhanced
interpretability, better controllability, and improved generalization in rare
and long-tail scenarios. However, existing methods often rely on abstracted
perception or map-based inputs, missing crucial visual context, such as
fine-grained road cues, accident aftermath, or unexpected obstacles, which are
essential for robust decision-making in complex driving environments. To bridge
this gap, we propose VLMPlanner, a hybrid framework that combines a
learning-based real-time planner with a vision-language model (VLM) capable of
reasoning over raw images. The VLM processes multi-view images to capture rich,
detailed visual information and leverages its common-sense reasoning
capabilities to guide the real-time planner in generating robust and safe
trajectories. Furthermore, we develop the Context-Adaptive Inference Gate
(CAI-Gate) mechanism that enables the VLM to mimic human driving behavior by
dynamically adjusting its inference frequency based on scene complexity,
thereby achieving an optimal balance between planning performance and
computational efficiency. We evaluate our approach on the large-scale,
challenging nuPlan benchmark, with comprehensive experimental results
demonstrating superior planning performance in scenarios with intricate road
conditions and dynamic elements. Code will be available.

</details>


### [567] [Partially Observable Monte-Carlo Graph Search](https://arxiv.org/abs/2507.20951)
*Yang You,Vincent Thomas,Alex Schutz,Robert Skilton,Nick Hawes,Olivier Buffet*

Main category: cs.AI

TL;DR: A new algorithm, POMCGS, solves large POMDPs offline by creating a policy graph, outperforming previous offline methods and matching online methods.


<details>
  <summary>Details</summary>
Motivation: Existing sampling-based online methods are not ideal for POMDP applications with time or energy constraints. Previous offline algorithms struggle to scale to large POMDPs.

Method: POMCGS, a partially observable Monte-Carlo graph search algorithm, folds the search tree on the fly to construct a policy graph, reducing computations. It also incorporates action progressive widening and observation clustering to address continuous POMDPs.

Result: POMCGS can generate policies for challenging POMDPs that previous offline algorithms cannot handle, with competitive policy values compared to state-of-the-art online algorithms.

Conclusion: POMCGS is a new sampling-based algorithm that can solve large POMDPs offline, generating competitive policies compared to state-of-the-art online algorithms.

Abstract: Currently, large partially observable Markov decision processes (POMDPs) are
often solved by sampling-based online methods which interleave planning and
execution phases. However, a pre-computed offline policy is more desirable in
POMDP applications with time or energy constraints. But previous offline
algorithms are not able to scale up to large POMDPs. In this article, we
propose a new sampling-based algorithm, the partially observable Monte-Carlo
graph search (POMCGS) to solve large POMDPs offline. Different from many online
POMDP methods, which progressively develop a tree while performing
(Monte-Carlo) simulations, POMCGS folds this search tree on the fly to
construct a policy graph, so that computations can be drastically reduced, and
users can analyze and validate the policy prior to embedding and executing it.
Moreover, POMCGS, together with action progressive widening and observation
clustering methods provided in this article, is able to address certain
continuous POMDPs. Through experiments, we demonstrate that POMCGS can generate
policies on the most challenging POMDPs, which cannot be computed by previous
offline algorithms, and these policies' values are competitive compared with
the state-of-the-art online POMDP algorithms.

</details>


### [568] [Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization](https://arxiv.org/abs/2507.19973)
*Ebrahim Rasromani,Stella K. Kang,Yanqi Xu,Beisong Liu,Garvit Luhadia,Wan Fung Chui,Felicia L. Pasadyn,Yu Chih Hung,Julie Y. An,Edwin Mathieu,Zehui Gu,Carlos Fernandez-Granda,Ammar A. Javed,Greg D. Sacks,Tamas Gonda,Chenchan Huang,Yiqiu Shen*

Main category: cs.AI

TL;DR: Developed and evaluated LLMs for automatic extraction of pancreatic cystic lesion (PCL) features and risk categorization from radiology reports. Fine-tuned open-source LLMs (LLaMA, DeepSeek) using GPT-4o-generated data achieved performance comparable to GPT-4o in feature extraction and risk categorization, with high agreement with radiologists.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of pancreatic cystic lesion (PCL) features from radiology reports is labor-intensive, limiting large-scale studies needed to advance PCL research.

Method: Curated a training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134 patients that described PCLs. Labels were generated by GPT-4o using chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated CoT data. Features were mapped to risk categories per institutional guideline based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out human-annotated reports. Model outputs for 100 cases were independently reviewed by three radiologists. Feature extraction was evaluated using exact match accuracy, risk categorization with macro-averaged F1 score, and radiologist-model agreement with Fleiss' Kappa.

Result: CoT fine-tuning improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79% to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved (LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no statistically significant differences. Radiologist inter-reader agreement was high (Fleiss' Kappa = 0.888) and showed no statistically significant difference with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT (Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels on par with radiologists.

Conclusion: Fine-tuned open-source LLMs with CoT supervision enable accurate, interpretable, and efficient phenotyping for large-scale PCL research, achieving performance comparable to GPT-4o.

Abstract: Background: Manual extraction of pancreatic cystic lesion (PCL) features from
radiology reports is labor-intensive, limiting large-scale studies needed to
advance PCL research. Purpose: To develop and evaluate large language models
(LLMs) that automatically extract PCL features from MRI/CT reports and assign
risk categories based on guidelines. Materials and Methods: We curated a
training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134
patients that described PCLs. Labels were generated by GPT-4o using
chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct
features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated
CoT data. Features were mapped to risk categories per institutional guideline
based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out
human-annotated reports. Model outputs for 100 cases were independently
reviewed by three radiologists. Feature extraction was evaluated using exact
match accuracy, risk categorization with macro-averaged F1 score, and
radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning
improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%
to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved
(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no
statistically significant differences. Radiologist inter-reader agreement was
high (Fleiss' Kappa = 0.888) and showed no statistically significant difference
with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT
(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels
on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT
supervision enable accurate, interpretable, and efficient phenotyping for
large-scale PCL research, achieving performance comparable to GPT-4o.

</details>


### [569] [PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training](https://arxiv.org/abs/2507.20067)
*Sarat Chandra Bobbili,Ujwal Dinesha,Dheeraj Narasimha,Srinivas Shakkottai*

Main category: cs.AI

TL;DR: PITA是一种无需奖励模型即可实现LLM推理时对齐的新框架，它将偏好反馈直接整合到token生成中，并通过学习引导策略来修改token概率，从而降低了计算成本并绕过了对预训练奖励模型的依赖。


<details>
  <summary>Details</summary>
Motivation: 为了在无需额外训练的情况下，使大型语言模型（LLM）能够生成与终端用户偏好一致的输出。现有方法依赖于预训练的奖励模型，该模型需要拟合人类偏好反馈，过程可能不稳定。

Method: PITA框架通过学习一个小的、基于偏好的引导策略来修改推理时的token概率，而无需进行LLM微调。该问题被表述为识别潜在的偏好分布，并通过随机搜索和迭代优化来解决。

Result: PITA在包括数学推理和情感分类在内的多种任务上进行了评估，证明了其在使LLM输出与用户偏好对齐方面的有效性。

Conclusion: PITA框架通过将偏好反馈直接整合到LLM的token生成过程中，无需奖励模型，即可在推理时实现与用户偏好的对齐。

Abstract: Inference-time alignment enables large language models (LLMs) to generate
outputs aligned with end-user preferences without further training. Recent
post-training methods achieve this by using small guidance models to modify
token generation during inference. These methods typically optimize a reward
function KL-regularized by the original LLM taken as the reference policy. A
critical limitation, however, is their dependence on a pre-trained reward
model, which requires fitting to human preference feedback--a potentially
unstable process. In contrast, we introduce PITA, a novel framework that
integrates preference feedback directly into the LLM's token generation,
eliminating the need for a reward model. PITA learns a small preference-based
guidance policy to modify token probabilities at inference time without LLM
fine-tuning, reducing computational cost and bypassing the pre-trained reward
model dependency. The problem is framed as identifying an underlying preference
distribution, solved through stochastic search and iterative refinement of the
preference-based guidance model. We evaluate PITA across diverse tasks,
including mathematical reasoning and sentiment classification, demonstrating
its effectiveness in aligning LLM outputs with user preferences.

</details>


### [570] [The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models](https://arxiv.org/abs/2507.20150)
*Xingcheng Xu*

Main category: cs.AI

TL;DR: 该研究提出了一个数学框架，用于分析强化学习策略的稳定性，解决了 LLMs/LRRs 中常见的脆弱性和不稳定性问题。该框架将这些失败归因于最优策略的不确定性，并为改进 AI 安全性提供了理论指导。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习（RL）在训练大型语言和推理模型（LLMs/LRRs）时，常常产生脆弱且不稳定的策略，导致各种关键性失败，如虚假推理、欺骗性对齐和不听从指令，从而削弱了 LLMs/LRRs 的可信度和安全性。这些问题缺乏统一的理论解释，通常使用临时性启发式方法来解决。

Method: 提出了一种严格的数学框架来分析从奖励函数到最优策略的映射的稳定性。研究了多奖励强化学习中的策略稳定性，以及熵正则化如何以增加随机性的代价恢复策略稳定性。通过多奖励强化学习中的扰动实验进行了验证。

Result: 该理论框架为一系列看似不同的失败提供了一个统一的解释，将它们重新定义为优化可能不完整或有噪声的奖励的理性结果，特别是在存在行动简并的情况下。研究结果还表明，熵正则化可以以增加随机性的代价来恢复策略稳定性。该框架统一解释了关于欺骗性推理、指令遵循权衡和 RLHF 引起的诡辩的近期经验发现。

Conclusion: 这项工作将策略稳定性分析从经验启发式方法提升到原则性理论，为设计更安全、更值得信赖的 AI 系统提供了重要见解。

Abstract: Reinforcement learning (RL) plays a crucial role in shaping the behavior of
large language and reasoning models (LLMs/LRMs). However, it often produces
brittle and unstable policies, leading to critical failures such as spurious
reasoning, deceptive alignment, and instruction disobedience that undermine the
trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified
theoretical explanation and are typically addressed using ad-hoc heuristics.
This paper presents a rigorous mathematical framework for analyzing the
stability of the mapping from a reward function to the optimal policy. We show
that policy brittleness often stems from non-unique optimal actions, a common
occurrence when multiple valid traces exist in a reasoning task. This
theoretical lens provides a unified explanation for a range of seemingly
disparate failures, reframing them as rational outcomes of optimizing rewards
that may be incomplete or noisy, especially in the presence of action
degeneracy. We extend this analysis from the fundamental single-reward setting
to the more realistic multi-reward RL across diverse domains, showing how
stability is governed by an "effective reward" aggregation mechanism. We also
prove that entropy regularization restores policy stability at the cost of
increased stochasticity. Our framework provides a unified explanation for
recent empirical findings on deceptive reasoning, instruction-following
trade-offs, and RLHF-induced sophistry, and is further validated through
perturbation experiments in multi-reward RL. This work advances
policy-stability analysis from empirical heuristics towards a principled
theory, offering essential insights for designing safer and more trustworthy AI
systems.

</details>


### [571] [SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration](https://arxiv.org/abs/2507.20280)
*Keyan Ding,Jing Yu,Junjie Huang,Yuchen Yang,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: SciToolAgent 是一个 LLM 驱动的代理，可以自动化复杂的科学工作流程，并能智能地选择和执行工具，从而提高研究效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在工具自动化方面显示出潜力，但它们在为复杂的科学工作流程集成和协调多个工具方面存在困难。本研究旨在解决这一挑战。

Method: SciToolAgent 是一个由 LLM 驱动的代理，它利用科学工具知识图谱，通过基于图谱的检索增强生成来智能地选择和执行工具。它还包括一个全面的安全检查模块。

Result: SciToolAgent 在生物学、化学和材料科学领域自动化了数百种科学工具，并在基准测试中显著优于现有方法。

Conclusion: SciToolAgent 能够自动化复杂的科学工作流程，使专家和非专家都能使用高级研究工具。

Abstract: Scientific research increasingly relies on specialized computational tools,
yet effectively utilizing these tools demands substantial domain expertise.
While Large Language Models (LLMs) show promise in tool automation, they
struggle to seamlessly integrate and orchestrate multiple tools for complex
scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that
automates hundreds of scientific tools across biology, chemistry, and materials
science. At its core, SciToolAgent leverages a scientific tool knowledge graph
that enables intelligent tool selection and execution through graph-based
retrieval-augmented generation. The agent also incorporates a comprehensive
safety-checking module to ensure responsible and ethical tool usage. Extensive
evaluations on a curated benchmark demonstrate that SciToolAgent significantly
outperforms existing approaches. Case studies in protein engineering, chemical
reactivity prediction, chemical synthesis, and metal-organic framework
screening further demonstrate SciToolAgent's capability to automate complex
scientific workflows, making advanced research tools accessible to both experts
and non-experts.

</details>


### [572] [Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition](https://arxiv.org/abs/2507.20526)
*Andy Zou,Maxwell Lin,Eliot Jones,Micha Nowak,Mateusz Dziemian,Nick Winter,Alexander Grattan,Valent Nathanael,Ayla Croft,Xander Davies,Jai Patel,Robert Kirk,Nate Burnikell,Yarin Gal,Dan Hendrycks,J. Zico Kolter,Matt Fredrikson*

Main category: cs.AI

TL;DR: LLM 驱动的 AI 智能体安全漏洞：红队测试竞赛揭示了普遍存在的策略违规和安全风险，模型大小和能力并不能保证安全性，需要更强的防御措施。


<details>
  <summary>Details</summary>
Motivation: 为了调查 LLM 驱动的 AI 智能体在现实环境中，尤其是在攻击下，能否被信任来遵循部署策略。

Method: 我们运行了迄今为止最大规模的公开红队测试竞赛，针对 44 个现实部署场景中的 22 个前沿 AI 智能体，提交了 180 万个提示注入攻击。

Result: 我们构建了 Agent Red Teaming (ART) 基准——一套精心策划的高影响力攻击——并对其在 19 个最先进模型上进行了评估。该结果表明，几乎所有智能体都表现出策略违规，并且跨模型和任务具有高度的攻击转移性。

Conclusion: 当今的 AI 智能体面临着严峻的、持续的安全漏洞，在受攻击时，几乎所有智能体都会在 10-100 次查询内出现大多数行为的策略违规，模型大小、能力或推理时间计算与智能体鲁棒性之间几乎没有相关性，需要额外的防御措施来对抗对抗性滥用。

Abstract: Recent advances have enabled LLM-powered AI agents to autonomously execute
complex tasks by combining language model reasoning with tools, memory, and web
access. But can these systems be trusted to follow deployment policies in
realistic environments, especially under attack? To investigate, we ran the
largest public red-teaming competition to date, targeting 22 frontier AI agents
across 44 realistic deployment scenarios. Participants submitted 1.8 million
prompt-injection attacks, with over 60,000 successfully eliciting policy
violations such as unauthorized data access, illicit financial actions, and
regulatory noncompliance. We use these results to build the Agent Red Teaming
(ART) benchmark - a curated set of high-impact attacks - and evaluate it across
19 state-of-the-art models. Nearly all agents exhibit policy violations for
most behaviors within 10-100 queries, with high attack transferability across
models and tasks. Importantly, we find limited correlation between agent
robustness and model size, capability, or inference-time compute, suggesting
that additional defenses are needed against adversarial misuse. Our findings
highlight critical and persistent vulnerabilities in today's AI agents. By
releasing the ART benchmark and accompanying evaluation framework, we aim to
support more rigorous security assessment and drive progress toward safer agent
deployment.

</details>


### [573] [MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation](https://arxiv.org/abs/2507.19489)
*Simone Bendazzoli,Sanna Persson,Mehdi Astaraki,Sebastian Pettersson,Vitali Grozman,Rodrigo Moreno*

Main category: cs.AI

TL;DR: MAIA是一个开源平台，用于AI在医疗领域的协作和部署，加速研究成果的临床转化。


<details>
  <summary>Details</summary>
Motivation: 为了将人工智能（AI）整合到临床工作流程中，需要强大的协作平台来连接技术创新和实际医疗应用。

Method: MAIA平台是建立在Kubernetes之上的，提供了一个模块化、可扩展的环境，集成了数据管理、模型开发、注释、部署和临床反馈等工具，支持项目隔离、CI/CD自动化、与高计算基础设施和临床工作流程的集成。

Result: MAIA已被成功应用于医学成像AI的实际用例中，并在学术和临床环境中得到部署，展示了其促进协作和互操作性的能力。

Conclusion: MAIA作为一个开源平台，旨在促进临床医生、研究人员和AI开发者之间的跨学科协作，以加速AI研究在医疗领域的临床转化，并强调可重复性、透明度和以用户为中心的设计。

Abstract: The integration of Artificial Intelligence (AI) into clinical workflows
requires robust collaborative platforms that are able to bridge the gap between
technical innovation and practical healthcare applications. This paper
introduces MAIA (Medical Artificial Intelligence Assistant), an open-source
platform designed to facilitate interdisciplinary collaboration among
clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a
modular, scalable environment with integrated tools for data management, model
development, annotation, deployment, and clinical feedback. Key features
include project isolation, CI/CD automation, integration with high-computing
infrastructures and in clinical workflows. MAIA supports real-world use cases
in medical imaging AI, with deployments in both academic and clinical
environments. By promoting collaborations and interoperability, MAIA aims to
accelerate the translation of AI research into impactful clinical solutions
while promoting reproducibility, transparency, and user-centered design. We
showcase the use of MAIA with different projects, both at KTH Royal Institute
of Technology and Karolinska University Hospital.

</details>


### [574] [Complementarity-driven Representation Learning for Multi-modal Knowledge Graph Completion](https://arxiv.org/abs/2507.20620)
*Lijian Li*

Main category: cs.AI

TL;DR: MMKGC面临模态不平衡的挑战。本文提出MoCME框架，利用互补性融合多模态信息，并通过熵引导负采样提高训练效果和模型鲁棒性，在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的MMKGC方法通常依赖于注意或门控融合机制，但忽略了多模态数据中包含的互补性。多模态知识图谱中模态分布因实体而异，这给利用附加模态数据进行鲁棒实体表示带来了挑战。

Method: 提出了一种名为MoCME的新框架，该框架由互补性引导的模态知识融合（CMKF）模块和熵引导的负采样（EGNS）机制组成。CMKF模块利用模态内和跨模态互补性来融合多视图和多模态嵌入，从而增强实体表示。EGNS机制动态地优先考虑信息丰富且不确定的负样本，以提高训练有效性和模型鲁棒性。

Result: 实验证明，MoCME在五个基准数据集上取得了最先进的性能。

Conclusion: MoCME在五个基准数据集上取得了最先进的性能，超过了现有方法。

Abstract: Multi-modal Knowledge Graph Completion (MMKGC) aims to uncover hidden world
knowledge in multimodal knowledge graphs by leveraging both multimodal and
structural entity information. However, the inherent imbalance in multimodal
knowledge graphs, where modality distributions vary across entities, poses
challenges in utilizing additional modality data for robust entity
representation. Existing MMKGC methods typically rely on attention or
gate-based fusion mechanisms but overlook complementarity contained in
multi-modal data. In this paper, we propose a novel framework named Mixture of
Complementary Modality Experts (MoCME), which consists of a
Complementarity-guided Modality Knowledge Fusion (CMKF) module and an
Entropy-guided Negative Sampling (EGNS) mechanism. The CMKF module exploits
both intra-modal and inter-modal complementarity to fuse multi-view and
multi-modal embeddings, enhancing representations of entities. Additionally, we
introduce an Entropy-guided Negative Sampling mechanism to dynamically
prioritize informative and uncertain negative samples to enhance training
effectiveness and model robustness. Extensive experiments on five benchmark
datasets demonstrate that our MoCME achieves state-of-the-art performance,
surpassing existing approaches.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [575] [Fundamental limitations of thermoradiative energy conversion](https://arxiv.org/abs/2507.19864)
*Maxime Giteau,Michela F. Picardi,Georgia T. Papadakis*

Main category: physics.app-ph

TL;DR: 理解热辐射和辐射发动机的性能限制。热辐射发动机的性能总是受到辐射发动机的限制，这使得它们在能量转换方面处于不利地位。


<details>
  <summary>Details</summary>
Motivation: 理解各种能量转换方法的根本限制对于评估其效率和功率输出至关重要。

Method: 推导了与冷沉以辐射方式交换热量的热辐射热机的通用性能界限，并建立了几种配置的功率-效率热力学界限。

Result: 研究结果发现，这些发动机的性能总是受到辐射发动机的限制，而辐射发动机则利用热源发出的热辐射进行能量转换，这使得热辐射发动机在能量转换方面固有地处于不利地位。

Conclusion: 该框架为评估特定设备提供了通用指标，并对热辐射电池在能源生产中的相关性提出了关键问题。

Abstract: Understanding the fundamental limits of various energy conversion approaches
is essential for assessing their efficiency and power output. In this work, we
derive general performance bounds for thermoradiative heat engines that
exchange heat radiatively with a cold sink, establishing
power-versus-efficiency thermodynamic bounds for several configurations. We
find that the performance of these engines is always bounded by that of
radiative engines, which harness the thermal radiation emitted by a hot source,
making thermoradiative engines inherently less favorable for energy conversion.
By unifying the results of radiative and thermoradiative engines within a
common thermodynamic framework, which also encompasses dual-engine
configurations that combine both features, this work provides a comprehensive
understanding of the thermodynamic limits of radiative energy conversion. Our
framework offers general metrics for evaluating specific devices and raises
critical questions regarding the relevance of thermoradiative cells for energy
production.

</details>


### [576] [Visualizing the Link Between Nanomorphology and Energetic Disorder in 3D Organic Solar Cells](https://arxiv.org/abs/2507.20040)
*Pelin Çiloğlu,Carmen Tretmans,Carsten Deibel,Jan-F. Pietschmann,Martin Stoll,Roderick C. I. MacKenzie*

Main category: physics.app-ph

TL;DR: 提出了一种新的三维混合模型，用于分析有机太阳能电池中的能量无序和纳米形貌。该模型能够处理高载流子密度，并揭示了纳米尺度分子堆积对器件性能的重要性。


<details>
  <summary>Details</summary>
Motivation: 有机本体异质结（BHJ）太阳能电池的性能对纳米形貌和能量无序高度敏感，但现有模型存在局限性。

Method: 提出了一种三维混合模型，该模型能够处理高载流子密度并包含能量无序的影响。首先使用考虑了薄膜形成过程中溶剂蒸发的相场方法生成了真实的形貌。然后，利用这些示例形貌，系统地研究了在代表实际器件运行的载流子密度下，能量无序与构型无序之间的相互作用。

Result: 研究结果表明，即使宏观渗透路径保持完整，能量无序主要通过抑制相互连接域中的电荷提取来限制性能。

Conclusion: 优化纳米尺度的分子堆积与控制介观尺度的相分离同样关键，这表明下一代BHJ器件需要多尺度设计策略。

Abstract: The performance of organic bulk heterojunction (BHJ) solar cells is highly
sensitive to both nanomorphology and energetic disorder arising from
microscopic molecular packing and structural defects. However, most models used
to understand these devices are either one-dimensional effective medium
approximations that neglect spatial and energetic disorder or three-dimensional
Monte Carlo simulations that are computationally intensive. In this work, we
present the results from a three-dimensional hybrid model capable of operating
at both high carrier densities and incorporating the effects of energetic
disorder. We first generate realistic morphologies using a phase-field approach
that accounts for solvent evaporation during film formation. Using these
example morphologies, we systematically study the interplay between energetic
disorder and configurational disorder at carrier densities representative of
real device operation. This enables us to separate and visualize the impact of
the nanomorphology and energetic disorder on device performance. Our results
reveal that, even when macroscopic percolation pathways remain intact,
energetic disorder limits performance primarily through suppressed charge
extraction in interconnected domains. This suggest that optimizing molecular
packing at the nanoscale is as critical as controlling phase separation at the
mesoscale, highlighting the need for multiscale design strategies in
next-generation BHJ devices.

</details>


### [577] [Device-scale modeling of valley photovoltaics](https://arxiv.org/abs/2507.20054)
*Daixi Xia,Hassan Allami,Jacob J. Krich*

Main category: physics.app-ph

TL;DR: 该研究提出了一个用于模拟谷光伏器件的泊松/漂移-扩散模型，发现仅有非平衡载流子填充卫星谷或增加内建电场不足以提高器件效率。


<details>
  <summary>Details</summary>
Motivation: 为了模拟“谷光伏”器件，这是一种基于谷散射效应的新型热载流子太阳能电池。

Method: 提出了一种包含谷散射效应的泊松/漂移-扩散模型，并使用了从之前的系综蒙特卡洛模拟中提取的、依赖于电场的谷散射率。

Result: 模拟结果表明，非平衡载流子填充卫星谷不足以使谷光伏器件实现高效率。此外，增加谷散射区的内建电场并不能像先前推测的那样提高效率。

Conclusion: 该模型表明，非平衡载流子填充卫星谷不足以实现高效率，并且增加 the valley-scattering region 的内建电场并不能提高效率。

Abstract: We present a Poisson/drift-diffusion model that includes valley scattering
effects for simulating valley photovoltaic devices. The valley photovoltaic
concept is a novel implementation of a hot-carrier solar cell and leverages the
valley scattering effect under large electric field to potentially achieve high
voltage and high efficiency. Fabricated devices have shown S-shaped
current-voltage curves, low fill factor, and thus low efficiency. We hence
develop the first device model for valley photovoltaics. Our model includes
electric-field-dependent valley scattering rates extracted from previous
ensemble Monte Carlo simulations. We show that the condition of nonequilibrium
carrier populations in the satellite valleys is not enough for valley
photovoltaics to achieve high efficiency. We also show that increasing the
built-in electric field of the valley-scattering region does not improve
efficiency, contrary to previous suggestion.

</details>


### [578] [Thermal quench modeling of REBCO racetrack coils under conduction cooling at 30 K for aircraft electric propulsion motors](https://arxiv.org/abs/2507.20178)
*Arif Hussain,Anang Dadhich,Enric Pardo*

Main category: physics.app-ph

TL;DR: 研究了HTS马蹄形线圈在高压和低压下的电热响应，发现高压会导致热失控，低压则表现出周期性振荡。该研究为电动飞机超导电机设计提供了热管理和故障响应的关键见解。


<details>
  <summary>Details</summary>
Motivation: 高温超导（HTS）马蹄形线圈因其卓越的载流能力，有望用于轻量化、大功率电机。然而，交流损耗或直流短路引起的自热可能导致电热失超，这对超导电机的设计和可靠性提出了重大挑战。

Method: 采用结合了最小电磁熵产生（MEMEP）方法进行电磁建模和有限差分法（FDM）进行电热分析的电磁热计算方法。

Result: 在高电压下，电流超过临界值，导致快速热失控，损坏超导材料。在低电压下，线圈表现出电流和温度的周期性振荡，显示出热扩散和电磁稳定性的复杂相互作用。在完全绝热和单侧冷却（顶面为30 K）两种热边界条件下，研究了HTS马蹄形线圈在1 V至1000 V直流电压下的响应。

Conclusion: 高压下，电流超过临界值会导致快速热失控，损坏超导材料。低压下，线圈表现出电流和温度的周期性振荡，表明热扩散和电磁稳定性的复杂相互作用。本研究为航空应用中的高温超导线圈的热管理和故障响应提供了关键见解，特别是在电动飞机的超导电机设计方面。

Abstract: High-temperature superconducting (HTS) racetrack coils are promising
components for lightweight, high-power electric machines due to their
exceptional current-carrying capacity. However, self-heating due to AC loss or
DC short circuits can cause electro-thermal quench, which poses a significant
challenge for the design and reliability of superconducting motors. Here, we
apply an electro-magneto-thermal computational approach that integrates the
Minimum Electro-Magnetic Entropy Production (MEMEP) method for electromagnetic
modelling with the Finite Difference Method (FDM) for electrothermal analyses.
The investigation focuses on the response of an HTS racetrack coil subjected to
DC voltages ranging from low (1 V) to high values (1000 V) at operating
temperature of 30 K. Computations were conducted under two thermal boundary
conditions: complete adiabatic conditions and cooling applied to one side of
the coil with the top surface at 30 K. We found that at higher voltages, the
current exceeds the critical value, causing rapid thermal runaway that damages
the superconducting material. In contrast, at lower voltages, the coil presents
periodic oscillations in current and temperature, demonstrating a complex
interplay of thermal diffusion and electromagnetic stability. This study
provides critical insights into the thermal management and fault response of
HTS coils for aviation applications, particularly in the design of
superconducting motors for electric aircraft.

</details>


### [579] [On the surface Bloch waves in truncated periodic media](https://arxiv.org/abs/2507.20431)
*Bojan B. Guzina,Shixu Meng,Prasanna Salasiya,Long Nguyen*

Main category: physics.app-ph

TL;DR: This paper models surface Bloch waves in periodic media using a quadratic eigenvalue problem based on a unit cell. The method efficiently captures wave properties and allows for design optimization of surface cuts to control these waves.


<details>
  <summary>Details</summary>
Motivation: The goal is to elucidate surface Bloch (SB) waves, which are evanescent surface waves in periodic media, analogous to surface waves in elastic solids. The paper aims to develop a model that can describe their dispersion, waveforms, and 'skin depth'.

Method: A unit cell-of-periodicity-based, reduced order model is developed for surface Bloch (SB) waves using the scalar wave equation with periodic coefficients. The core of the method involves a quadratic eigenvalue problem (QEP) for the effective unit cell, which determines the complex wavenumber normal to the surface cut. This QEP considers the interplay between the lattice, surface cut orientation, and wave direction, given excitation frequency and wave direction wavenumber. The SB wave's boundary layer is derived by superposing QEP eigenstates, with strengths determined by imposing homogeneous Neumann boundary conditions.

Result: The developed model, centered around a quadratic eigenvalue problem (QEP), allows for the description of SB wave dispersion and waveforms through a low-dimensional eigenvalue problem. This facilitates rapid analysis of surface undulations and enables manipulation of SB waves via optimal design of the surface cut. The analysis also accounts for power flow and 'skin depth', crucial for the energetic relevance of these boundary layers.

Conclusion: The study develops a unit cell-based reduced order model for surface Bloch waves using a quadratic eigenvalue problem. This model efficiently describes dispersion, waveforms, and skin depth, enabling rapid exploration and manipulation of these waves through optimal design of surface cuts.

Abstract: Much like their counterparts in elastic solids, waves in periodic media can
be broadly classified into Floquet-Bloch body waves, and evanescent surface
waves. Our goal is to elucidate the latter boundary layers, termed surface
Bloch (SB) waves, affiliated with rational surface cuts and homogeneous Neumann
data. To this end we adopt the scalar wave equation with periodic coefficients
as a test bed and develop a unit cell-of-periodicity-based, reduced order model
of the SB waves that is capable of describing both their dispersion, waveforms,
and ``skin depth''. The centerpiece of our analysis is a quadratic eigenvalue
problem (QEP) for the effective unit cell of periodicity -- deriving from a
geometric interplay between the mother Bravais lattice, orientation of the
surface cut, and the direction of SB wave -- that seeks the complex wavenumber
normal to the cut plane given (i) the excitation frequency and (ii) wavenumber
in the direction of the SB wave. In this way the sought boundary layer is
derived via superposition of the evanescent QEP eigenstates, whose strengths
are obtained by imposing the homogeneous Neumann boundary condition. With the
QEP eigenspectrum at hand, evaluation of an SB wave -- in terms of both
dispersion characteristics and evanescent waveforms -- entails only a
low-dimensional eigenvalue problem. This feature caters for rapid exploration
of the effect of (periodic) surface undulations, and so enables manipulation of
the SB waves via optimal design of the surface cut. For completeness, our
analysis includes an account for the power flow and ``skin depth'' of a surface
Bloch wave, both of which are critical for the energetic relevance of boundary
layers.

</details>


### [580] [A Wide-Input 0.25 um BCD LDO with Dual-Stage Amplifier and Active Ripple Cancellation for High PSRR and Fast Transient Response](https://arxiv.org/abs/2507.20522)
*Yi Zhang,Zhuolong Chen,Zhenghao Xu,Yujin He*

Main category: physics.app-ph

TL;DR: 本文提出了一种新型宽输入线性稳压器，通过双级误差放大器和有源纹波抵消技术提高了PSRR，并通过自适应快速反馈和片上频率补偿加速了瞬态响应，同时保持了低静态电流，满足了SoC和便携式设备的要求。


<details>
  <summary>Details</summary>
Motivation: 随着片上系统（SoC）集成度的提高，对具有高电源抑制比（PSRR）和快速瞬态响应的片上低压差线性稳压器（LDO）的需求日益增长，但传统LDO架构难以在宽输入电压范围内同时实现这些性能指标。

Method: 提出了一种宽输入线性稳压器，采用双级误差放大器架构和有源纹波抵消技术来提高电源抑制比（PSRR），并结合了自适应快速反馈支路和片上频率补偿网络来加速瞬态响应。

Result: 仿真结果显示，在50 µA - 4 mA负载阶跃下，输出电压下降幅度小于0.65 V，恢复时间在16 µs以内，低频PSRR为-75 dB。

Conclusion: 该设计满足模拟/射频SoC和便携式电子设备)的严格要求，证明了所提出的架构和分析的有效性。

Abstract: Demand for on-chip low-dropout regulators (LDOs) with both high power-supply
rejection ratio (PSRR) and fast transient response is growing as system-on-chip
(SoC) integration increases. However, conventional LDO architectures face
difficulty achieving these performance metrics simultaneously over wide input
voltage ranges. This paper presents a wide-input linear regulator implemented
in 0.25 um BCD technology that attains high PSRR and swift load-transient
performance while maintaining low quiescent current. The proposed LDO employs a
dual-stage error amplifier architecture and active ripple cancellation along
both the power path and the error amplifier's supply to significantly enhance
PSRR across frequency. An adaptive fast feedback branch together with an
on-chip frequency compensation network is introduced to accelerate transient
response without compromising stability. A two-stage PSRR analytical model and
a three-frequency-band PSRR interpretation framework are developed to guide the
design. Cadence Spectre simulations of the 14 V-output LDO demonstrate a -75 dB
low-frequency PSRR, and during a 50 uA - 4 mA load step the output voltage
droop is kept under 0.65 V with recovery within 16 us. These results validate
the effectiveness of the proposed architecture and analysis, indicating that
the design meets the stringent requirements of analog/RF SoCs and portable
electronics.

</details>


### [581] [Energy recovery from Ginkgo biloba urban pruning wastes: pyrolysis optimization and fuel property enhancement for high grade charcoal productions](https://arxiv.org/abs/2507.20683)
*Padam Prasad Paudel,Sunyong Park,Kwang Cheol Oh,Seok Jun Kim,Seon Yeop Kim,Kyeong Sik Kang,Dae Hyun Kim*

Main category: physics.app-ph

TL;DR: 本研究将银杏修剪残余物转化为生物炭，以实现废物管理和可再生能源的目标。


<details>
  <summary>Details</summary>
Motivation: 为了实现废物管理和可再生能源的目标，对银杏修剪残余物进行资源化利用。

Method: 本研究通过优化热解条件（400-600摄氏度）将银杏修剪残余物转化为生物炭，并评估了其燃料性质，包括质量产率、体积收缩、吸湿性、元素组成、热重分析、量热法和电感耦合等离子体发射光谱法。

Result: 热解产率在27.33%至32.05%之间，体积收缩率在41.19%至49.97%之间。在550和600摄氏度条件下，产物符合一级生物炭标准，在其他温度下符合二级木炭标准。热解产物的热稳定性、燃烧性能和热值（34.26 MJ/kg）均得到改善，但需要进一步研究以优化重金属含量。

Conclusion: 该研究展示了将银杏修剪残余物转化为生物炭以实现废物管理和可再生能源的目标，并提供了城市规划和政策制定的见解。

Abstract: Ginkgo biloba trees are widely planted in urban areas of developed countries
for their resilience, longevity and aesthetic appeal. Annual pruning to control
tree size, shape and interference with traffic and pedestrians generates large
volumes of unutilized Ginkgo biomass. This study aimed to valorize these
pruning residues into charcoal by optimizing pyrolysis conditions and
evaluating its fuel properties. The pyrolysis experiment was conducted at 400
to 600 degrees Celsius, after oven drying pretreatment. The mass yield of
charcoal was found to vary from 27.33 to 32.05 percent and the approximate
volume shrinkage was found to be 41.19 to 49.97 percent. The fuel properties of
the charcoals were evaluated using the moisture absorption test, proximate and
ultimate analysis, thermogravimetry, calorimetry and inductively coupled plasma
optical emission spectrometry. The calorific value improved from 20.76 to 34.26
MJ per kg with energy yield up to 46.75 percent. Charcoal exhibited superior
thermal stability and better combustion performance. The results revealed
satisfactory properties compared with other biomass, coal and biochar
standards. The product complied with first grade standards at 550 and 600
degrees Celsius and second grade wood charcoal standards at other temperatures.
However, higher concentrations of some heavy metals like Zn indicate the need
for pretreatment and further research on copyrolysis for resource optimization.
This study highlights the dual benefits of waste management and renewable
energy, providing insights for urban planning and policymaking.

</details>


### [582] [Femtosecond Engineering of magnetic Domain Walls via Nonequilibrium Spin Textures](https://arxiv.org/abs/2507.20701)
*Yuzhu Fan,Gaolong Cao,Junlin Wang,Sheng Jiang,Jing Wu,Johan Åkerman,Jonas Weissenrieder*

Main category: physics.app-ph

TL;DR: 本研究利用超快电子显微镜和多尺度模拟，揭示了铁磁GdFeCo薄膜中畴壁在超快光激发下的形成机制，包括非线性成核途径和混合过渡态，为控制自旋纹理提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 为了理解超快光激发下非均匀畴壁（DW）的形成动力学，而这方面的研究尚不充分。

Method: 本研究利用洛伦兹超快电子显微镜结合瞬态光学栅状激发，直接观察了亚皮秒时间尺度下铁磁GdFeCo薄膜中畴壁（DW）的形成动力学。

Result: 研究观察到在10皮秒内，从无序自旋对比度到有序畴壁阵列的快速演变，包括一个瞬态的、高度不对称的畴壁状态。在狭窄的 the fluence 窗口内，形成并在皮秒内自发消失的短寿命畴壁。多尺度模拟揭示了一种涉及混合过渡态的非线性成核途径，其中局部的不稳定自旋纹理聚结成亚稳态畴壁。此非平衡机制解释了观察到的不对称性和空间排序，并为在阿秒时间尺度上控制磁性材料中的自旋纹理提供了框架。

Conclusion: 本研究通过超快电子显微镜和瞬态光学栅状激发，揭示了在亚皮秒时间尺度上，外尔费米子在狄拉克狄拉克点附近表现出非平庸的拓扑性质，这种性质对其输运性质有重要影响。

Abstract: Ultrafast optical control of magnetic textures offers new opportunities for
energy-efficient, high-speed spintronic devices. While uniform magnetization
reversal via all-optical switching is well established, the formation dynamics
of non-uniform domain walls (DWs) under ultrafast excitation remain poorly
understood. Here, we use Lorentz ultrafast electron microscopy combined with
transient optical grating excitation to directly image the real-time formation
of DWs in a ferrimagnetic GdFeCo film. We observe a rapid evolution from
disordered spin contrast to ordered DW arrays within 10 ps, including a
transient, strongly asymmetric DW state. In a narrow fluence window,
short-lived DWs form and spontaneously vanish within picoseconds. Multiscale
simulations combining atomistic spin dynamics and micromagnetics reveal a
nonlinear nucleation pathway involving a hybrid transition state where
localized, unstable spin textures coalesce into metastable DWs. This
nonequilibrium mechanism explains the observed asymmetry and spatial ordering,
and establishes a framework for controlling spin textures in magnetic materials
on femtosecond timescales.

</details>
