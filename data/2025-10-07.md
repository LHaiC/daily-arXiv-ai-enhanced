<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 170]
- [cs.CL](#cs.CL) [Total: 111]
- [cs.SI](#cs.SI) [Total: 4]
- [cs.GT](#cs.GT) [Total: 7]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]
- [eess.SP](#eess.SP) [Total: 29]
- [cs.RO](#cs.RO) [Total: 55]
- [eess.SY](#eess.SY) [Total: 32]
- [cs.MA](#cs.MA) [Total: 7]
- [quant-ph](#quant-ph) [Total: 84]
- [cs.GR](#cs.GR) [Total: 14]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.LO](#cs.LO) [Total: 9]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 25]
- [cs.SY](#cs.SY) [Total: 1]
- [cs.LG](#cs.LG) [Total: 285]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 18]
- [cs.AI](#cs.AI) [Total: 41]
- [physics.app-ph](#physics.app-ph) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics](https://arxiv.org/abs/2510.03287)
*Moinak Bhattacharya,Gagandeep Singh,Prateek Prasanna*

Main category: cs.CV

TL;DR: SoC-DT框架结合了反应扩散模型、离散的标准护理干预措施以及基因组和人口统计学个性化，可以预测治疗后影像学上的肿瘤结构，并在合成数据和真实世界胶质瘤数据上均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 肿瘤在标准护理疗法下的轨迹预测在肿瘤学中仍然是一个未被满足的重大需求，而传统的反应扩散模型在捕捉异质性治疗范式下的肿瘤动力学方面存在局限性，因此需要能够模拟标准护理干预措施并考虑患者间基因组、人口统计学和治疗方案变异性的计算框架。

Method: 提出了一种名为SoC-DT的可微分框架，该框架整合了反应扩散肿瘤生长模型、离散的标准护理干预（手术、化学疗法、放射疗法）以及基因组和人口统计学个性化，并提出了一种名为IMEX-SoC的隐式-显式指数时间差分求解器，以确保在SoC治疗情况下的稳定性和可扩展性。

Result: 在合成数据和真实世界胶质瘤数据上，SoC-DT在预测肿瘤动力学方面持续优于经典的偏微分方程基线模型和纯数据驱动的神经网络模型。

Conclusion: SoC-DT通过结合机制可解释性与现代可微分求解器，为肿瘤学中患者特异性的数字孪生建立了原则性基础，能够进行生物学上一致的肿瘤动力学估算。

Abstract: Accurate prediction of tumor trajectories under standard-of-care (SoC)
therapies remains a major unmet need in oncology. This capability is essential
for optimizing treatment planning and anticipating disease progression.
Conventional reaction-diffusion models are limited in scope, as they fail to
capture tumor dynamics under heterogeneous therapeutic paradigms. There is
hence a critical need for computational frameworks that can realistically
simulate SoC interventions while accounting for inter-patient variability in
genomics, demographics, and treatment regimens. We introduce Standard-of-Care
Digital Twin (SoC-DT), a differentiable framework that unifies
reaction-diffusion tumor growth models, discrete SoC interventions (surgery,
chemotherapy, radiotherapy) along with genomic and demographic personalization
to predict post-treatment tumor structure on imaging. An implicit-explicit
exponential time-differencing solver, IMEX-SoC, is also proposed, which ensures
stability, positivity, and scalability in SoC treatment situations. Evaluated
on both synthetic data and real world glioma data, SoC-DT consistently
outperforms classical PDE baselines and purely data-driven neural models in
predicting tumor dynamics. By bridging mechanistic interpretability with modern
differentiable solvers, SoC-DT establishes a principled foundation for
patient-specific digital twins in oncology, enabling biologically consistent
tumor dynamics estimation. Code will be made available upon acceptance.

</details>


### [2] [Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data](https://arxiv.org/abs/2510.03292)
*Doğanay Demir,İlknur Durgar Elkahlout*

Main category: cs.CV

TL;DR: 该论文提出了一种结合分布式多GPU推理系统和交互式可视化平台的混合框架，用于分析视频中的名人动态。该系统能高效处理大量视频数据，生成时间戳的出现记录，并通过多种可视化图表（如频率图、网络图、热力图等）提供多维度的数据洞察，揭示名人活跃度、屏幕时间分布、时间动态、共同出现关系及随剧集和季度的变化。交互式设计使用户能够动态探索数据，发现关键时刻和演变关系。该研究通过结合分布式识别和可视化分析，为娱乐分析、内容创作和受众研究开辟了新的可能性。


<details>
  <summary>Details</summary>
Motivation: 随着视频内容的激增，理解其结构和动态变得至关重要，尤其是在娱乐领域。

Method: 提出一个混合框架，结合了分布式多GPU推理系统和交互式可视化平台。推理系统利用优化的ONNX模型、异构批推理和高吞吐量并行处理，高效生成带时间戳的名人出现记录。这些记录随后被转化为多种可视化图表（包括出现频率、时长分析、饼图、共现矩阵、网络图、堆叠面积图、季节比较和热力图）。

Result: 该系统能够高效处理大量视频数据，生成详细的名人出现记录，并通过一系列可视化图表提供多维度的数据洞察，揭示名人活跃度、屏幕时间、时间动态、共现关系以及随时间和剧集的变化趋势。用户可以通过交互式界面动态探索数据。

Conclusion: 该混合框架通过结合分布式识别和结构化的、视觉驱动的分析，为娱乐分析、内容创作策略和观众参与度研究提供了新的方法和可能性，能够深入理解视频内容中的名人动态。

Abstract: In an era dominated by video content, understanding its structure and
dynamics has become increasingly important. This paper presents a hybrid
framework that combines a distributed multi-GPU inference system with an
interactive visualization platform for analyzing celebrity dynamics in video
episodes. The inference framework efficiently processes large volumes of video
data by leveraging optimized ONNX models, heterogeneous batch inference, and
high-throughput parallelism, ensuring scalable generation of timestamped
appearance records. These records are then transformed into a comprehensive
suite of visualizations, including appearance frequency charts, duration
analyses, pie charts, co-appearance matrices, network graphs, stacked area
charts, seasonal comparisons, and heatmaps. Together, these visualizations
provide multi-dimensional insights into video content, revealing patterns in
celebrity prominence, screen-time distribution, temporal dynamics,
co-appearance relationships, and intensity across episodes and seasons. The
interactive nature of the system allows users to dynamically explore data,
identify key moments, and uncover evolving relationships between individuals.
By bridging distributed recognition with structured, visually-driven analytics,
this work enables new possibilities for entertainment analytics, content
creation strategies, and audience engagement studies.

</details>


### [3] [Domain-Robust Marine Plastic Detection Using Vision Models](https://arxiv.org/abs/2510.03294)
*Saanvi Kataria*

Main category: cs.CV

TL;DR: 海洋塑料污染严峻，水下垃圾检测自动化需求迫切。然而，现有视觉模型在不同数据集上表现会因域漂移而下降。本研究评估了模型在跨域鲁棒性方面的表现，在标记的水下数据集上训练了卷积神经网络（CNNs）和视觉Transformer（ViTs），并在包含不同来源塑料图像和训练域负样本的跨域测试集上进行了评估。此外，还评估了两种零样本模型（CLIP ViT-L14 和 Gemini 2.0 Flash）。结果显示，轻量级 MobileNetV2 在跨域表现上最佳（F1 0.97），优于大型模型。所有微调模型在精确率上表现优异（约 99%），但在召回率上存在差异。零样本 CLIP 模型召回率较高（约 80%），但精确率较低（约 56%）；Gemini 模型则精确率高（约 99%），召回率适中（约 81%）。错误分析显示，模型常将珊瑚纹理、悬浮颗粒和镜面反射误认为塑料。结论是，经过监督训练的紧凑型 CNN 模型能有效泛化于跨域水下检测，而大型预训练的视觉-语言模型也具有互补优势。


<details>
  <summary>Details</summary>
Motivation: 海洋塑料污染是一个严峻的环境威胁，因此需要可靠的自动化水下垃圾检测方法。然而，在单一数据集上训练的视觉系统在面对新图像时，由于域漂移（domain shift）会导致性能下降。本研究旨在解决这一问题，评估模型在跨域鲁棒性方面的表现。

Method: 本研究在标记的水下数据集上训练了卷积神经网络（CNNs），包括 MobileNetV2、ResNet-18 和 EfficientNet-B0，以及视觉Transformer（ViTs），包括 DeiT-Tiny 和 ViT-B16。随后，在构建的、平衡的跨域测试集上评估了这些模型的性能。该测试集包含了来自不同来源的塑料图像以及来自训练域的非塑料图像。此外，还评估了两种零样本（zero-shot）模型：CLIP ViT-L14 和 Google 的 Gemini 2.0 Flash。这些模型利用预训练能力，无需进行微调即可对图像进行分类。

Result: 轻量级 MobileNetV2 模型在跨域表现上最为出色，F1 分数达到 0.97，超过了其他更大型的模型。所有经过微调的模型在精确率（Precision）方面都达到了约 99% 的高水平，但在召回率（Recall）方面存在差异，这表明它们对塑料实例的敏感度不同。零样本 CLIP 模型相对更敏感，召回率约为 80%，但精确率仅为 56%，容易产生误报。而 Gemini 模型则表现出相反的特点，精确率约为 99%，召回率约为 81%。错误分析发现，模型经常将珊瑚纹理、悬浮颗粒和镜面反射等与塑料混淆。

Conclusion: 经过监督训练的紧凑型卷积神经网络（CNNs）在跨域水下检测任务中能够实现有效的泛化。同时，大型预训练的视觉-语言模型（如 Gemini）也显示出其独特的优势和互补性，能够为水下垃圾检测提供更全面的解决方案。

Abstract: Marine plastic pollution is a pressing environmental threat, making reliable
automation for underwater debris detection essential. However, vision systems
trained on one dataset often degrade on new imagery due to domain shift. This
study benchmarks models for cross-domain robustness, training convolutional
neural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision
transformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then
evaluates them on a balanced cross-domain test set built from plastic-positive
images drawn from a different source and negatives from the training domain.
Two zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,
that leverage pretraining to classify images without fine-tuning. Results show
the lightweight MobileNetV2 delivers the strongest cross-domain performance (F1
0.97), surpassing larger models. All fine-tuned models achieved high Precision
(around 99%), but differ in Recall, indicating varying sensitivity to plastic
instances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet
prone to false positives (Precision around 56%), whereas Gemini exhibits the
inverse profile (Precision around 99%, Recall around 81%). Error analysis
highlights recurring confusions with coral textures, suspended particulates,
and specular glare. Overall, compact CNNs with supervised training can
generalize effectively for cross-domain underwater detection, while large
pretrained vision-language models provide complementary strengths.

</details>


### [4] [Learned Display Radiance Fields with Lensless Cameras](https://arxiv.org/abs/2510.03356)
*Ziyang Chen,Yuta Itoh,Kaan Akşit*

Main category: cs.CV

TL;DR: 该研究提出了一种无需专门设备即可进行显示器校准的方法。


<details>
  <summary>Details</summary>
Motivation: 显示器校准对于内容创作者保持最佳视觉体验至关重要，但现有方法通常需要专门设备和暗室，这使得大多数用户难以进行。该研究旨在解决这一难题，提供一种更便捷的校准方式。

Method: 该研究提出了一种结合无透镜相机和隐式神经表示（Implicit Neural Representation）算法的方法，用于从不同视角捕获显示器特性。该方法能够从 46.6° X 37.6° 的视角范围内重建显示器发出的光场。

Result: 该方法实现了从多个视角高效重建显示器光场的能力。

Conclusion: 该研究为实现轻松的显示器校准和特性描述奠定了基础，是迈向更便捷校准流程的第一步。

Abstract: Calibrating displays is a basic and regular task that content creators must
perform to maintain optimal visual experience, yet it remains a troublesome
issue. Measuring display characteristics from different viewpoints often
requires specialized equipment and a dark room, making it inaccessible to most
users. To avoid specialized hardware requirements in display calibrations, our
work co-designs a lensless camera and an Implicit Neural Representation based
algorithm for capturing display characteristics from various viewpoints. More
specifically, our pipeline enables efficient reconstruction of light fields
emitted from a display from a viewing cone of 46.6{\deg} X 37.6{\deg}. Our
emerging pipeline paves the initial steps towards effortless display
calibration and characterization.

</details>


### [5] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: VLCAP是一个集成CLIP视觉标签检索和多模态文本生成的阿拉伯语图像字幕框架，通过提取视觉概念来生成更准确、更符合文化背景的字幕。


<details>
  <summary>Details</summary>
Motivation: 提出了一种新的阿拉伯语图像字幕方法VLCAP，旨在解决现有方法过度依赖端到端生成而忽略可解释性的问题，并生成更符合文化和语境的字幕。

Method: VLCAP首先使用mCLIP、AraCLIP和Jina V4三种多语言编码器提取图像的视觉标签，然后将这些标签与训练字幕结合构建混合词汇表。接着，将检索到的前k个标签转换为阿拉伯语提示，并与原始图像一起输入到Qwen-VL或Gemini Pro Vision模型中进行字幕生成。

Result: 在六种不同的编码器-解码器配置下进行评估，其中mCLIP + Gemini Pro Vision在BLEU-1（5.34%）和余弦相似度（60.01%）方面表现最佳，而AraCLIP + Qwen-VL在LLM评判得分（36.33%）方面最高。

Conclusion: VLCAP框架通过结合可解释的视觉标签检索和多模态文本生成，能够生成在文化上连贯且在语境上准确的阿拉伯语图像字幕。

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [6] [Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes](https://arxiv.org/abs/2510.03297)
*Akshar Gothi*

Main category: cs.CV

TL;DR: EfficientNet-B0 and ViT-Base在SpaceNet数据集上进行了受控比较，发现在自然不平衡和平衡重采样的数据集上，EfficientNet-B0在效率和性能上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 评估卷积神经网络（EfficientNet-B0）和视觉Transformer（ViT-Base）在SpaceNet数据集不同标签分布下的性能。

Method: 在自然不平衡和平衡重采样的数据集上，使用相同的预处理、轻量级增强和40个训练周期，比较EfficientNet-B0和ViT-Base的准确率、宏F1分数、平衡准确率、每个类别的召回率以及模型大小和延迟。

Result: 在不平衡数据集上，EfficientNet-B0达到93%的测试准确率，宏F1分数高，延迟低；ViT-Base也达到93%的准确率，但参数量和运行时间更大。在平衡数据集上，两者性能都很强，EfficientNet-B0达到99%，ViT-Base也具有竞争力，表明平衡数据集缩小了模型间的性能差距，但CNN在效率方面仍有优势。

Conclusion: EfficientNet-B0在SpaceNet数据集上，尤其是在不平衡和效率方面，优于ViT-Base。数据平衡可以缩小模型间的性能差距，但CNN的效率优势依然存在。

Abstract: We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.

</details>


### [7] [A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety](https://arxiv.org/abs/2510.03314)
*Shucheng Zhang,Yan Shi,Bingzhang Wang,Yuang Zhang,Muhammad Monjurul Karim,Kehua Chen,Chenxi Liu,Mehrdad Nasri,Yinhai Wang*

Main category: cs.CV

TL;DR: 本篇论文是对摄像头感知人工智能系统在保护弱势道路使用者（VRU）安全方面应用的最新进展进行的回顾，重点关注过去五年和新兴研究趋势。论文探讨了检测、跟踪、轨迹预测和意图识别这四个核心任务，并指出了数据、模型和部署方面的四个主要挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的基于基础设施的措施在动态城市环境中对保护弱势道路使用者（VRU）的效果有限。人工智能（AI），特别是视觉感知和推理的进步，为主动和情境感知的VRU保护提供了新的机会。然而，现有的关于AI在VRU应用方面的调查主要集中在检测，对其他对全面理解和保护VRU至关重要的视觉任务的覆盖有限。

Method: 对过去五年和新兴研究趋势中，在摄像头感知人工智能系统在VRU安全应用方面的进展进行了系统性审查。重点关注了检测与分类、跟踪与再识别、轨迹预测以及意图识别与预测这四个核心任务。

Result: 论文审查了检测与分类、跟踪与再识别、轨迹预测以及意图识别与预测这四个核心任务在AI驱动的VRU保护中的应用，并强调了数据、模型和部署方面的四大挑战，旨在为下一代VRU安全传感系统的发展提供参考。

Conclusion: 本篇论文对摄像头感知AI在VRU安全领域的应用进行了全面的回顾，涵盖了关键任务和未来挑战，为开发下一代VRU安全传感系统奠定了基础。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, remains a critical global challenge, as conventional
infrastructure-based measures often prove inadequate in dynamic urban
environments. Recent advances in artificial intelligence (AI), particularly in
visual perception and reasoning, open new opportunities for proactive and
context-aware VRU protection. However, existing surveys on AI applications for
VRUs predominantly focus on detection, offering limited coverage of other
vision-based tasks that are essential for comprehensive VRU understanding and
protection. This paper presents a state-of-the-art review of recent progress in
camera-based AI sensing systems for VRU safety, with an emphasis on
developments from the past five years and emerging research trends. We
systematically examine four core tasks, namely detection and classification,
tracking and reidentification, trajectory prediction, and intent recognition
and prediction, which together form the backbone of AI-empowered proactive
solutions for VRU protection in intelligent transportation systems. To guide
future research, we highlight four major open challenges from the perspectives
of data, model, and deployment. By linking advances in visual AI with practical
considerations for real-world implementation, this survey aims to provide a
foundational reference for the development of next-generation sensing systems
to enhance VRU safety.

</details>


### [8] [The View From Space: Navigating Instrumentation Differences with EOFMs](https://arxiv.org/abs/2510.03316)
*Ryan P. Demilt,Nicholas LaHaye,Karis Tenneson*

Main category: cs.CV

TL;DR: 地球观测基础模型（EOFMs）在处理海量遥感数据和执行地球监测任务方面日益普及。然而，大多数EOFMs仅在单一数据模态上进行训练，并且在跨模态匹配时进行评估。本研究表明，EOFMs的表示空间对传感器架构高度敏感，理解这种差异对于改进EOFMs的设计至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有EOFMs多为单一模态训练，且在跨模态应用时评估方式有待商榷，不清楚不同传感器架构对EOFMs内部表征的影响。

Method: 通过分析EOFMs的表示空间对传感器架构的敏感性，来研究不同传感器架构对EOFMs内部表征的影响。

Result: EOFMs的表示空间对传感器架构高度敏感。

Conclusion: 理解EOFMs表示空间对传感器架构的敏感性，有助于揭示当前EOFMs设计的缺陷，并为模型开发者、用户和整个社区提供改进方向。

Abstract: Earth Observation Foundation Models (EOFMs) have exploded in prevalence as
tools for processing the massive volumes of remotely sensed and other earth
observation data, and for delivering impact on the many essential earth
monitoring tasks. An emerging trend posits using the outputs of pre-trained
models as 'embeddings' which summarize high dimensional data to be used for
generic tasks such as similarity search and content-specific queries. However,
most EOFM models are trained only on single modalities of data and then applied
or benchmarked by matching bands across different modalities. It is not clear
from existing work what impact diverse sensor architectures have on the
internal representations of the present suite of EOFMs. We show in this work
that the representation space of EOFMs is highly sensitive to sensor
architecture and that understanding this difference gives a vital perspective
on the pitfalls of current EOFM design and signals for how to move forward as
model developers, users, and a community guided by robust remote-sensing
science.

</details>


### [9] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: 该研究提出了一种新的基于图像修复的扰动解释技术，用于提高计算机视觉模型在生态监测中的可信度。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉模型在生态监测中的预测不透明，限制了其在野外的应用。需要一种能够揭示模型决策依据并保持图像真实感和场景一致性的解释方法。

Method: 提出了一种引导式图像修复的扰动解释技术。该技术使用图像修复来生成局部编辑，以保持场景的上下文。具体方法包括（i）移除/替换目标（例如，将海豹替换为冰/水或船只），以及（ii）将原始图像中的动物替换到新的背景中。研究使用YOLOv9模型对冰川湾的无人机图像进行港海豹检测，并利用Segment-Anything-Model进行掩码细化。

Result: 该方法生成的解释能够精确定位诊断结构，避免了传统扰动方法中常见的删除伪影。通过重新评估扰动图像（翻转率、置信度下降）和专家评审（生态学合理性、可解释性），证明了该方法生成的解释具有领域相关性，支持专家验证。

Conclusion: 这种基于图像修复的扰动解释技术能够生成逼真的、局部化的编辑，能够揭示模型预测所依赖的细粒度形态线索，提高了AI在生态学领域部署的可信度。

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque
predictions limit trust and field adoption. We present an inpainting-guided,
perturbation-based explanation technique that produces photorealistic,
mask-localized edits that preserve scene context. Unlike masking or blurring,
these edits stay in-distribution and reveal which fine-grained morphological
cues drive predictions in tasks such as species recognition and trait
attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for
harbor seal detection in Glacier Bay drone imagery, using
Segment-Anything-Model-refined masks to support two interventions: (i) object
removal/replacement (e.g., replacing seals with plausible ice/water or boats)
and (ii) background replacement with original animals composited onto new
scenes. Explanations are assessed by re-scoring perturbed images (flip rate,
confidence drop) and by expert review for ecological plausibility and
interpretability. The resulting explanations localize diagnostic structures,
avoid deletion artifacts common to traditional perturbations, and yield
domain-relevant insights that support expert validation and more trustworthy
deployment of AI in ecology.

</details>


### [10] [Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications](https://arxiv.org/abs/2510.03318)
*Ahmed Kabil,Ghada Khoriba,Mina Yousef,Essam A. Rashed*

Main category: cs.CV

TL;DR: 本文全面系统地 survey 了医学图像分割（MIS）方法，涵盖传统图像处理技术到最先进的深度学习方法，并探讨了新兴趋势和挑战。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割（MIS）在精确诊断、治疗规划和病情监测中至关重要，但传统方法与现代深度学习方法之间存在差距，需要系统性 survey。

Method: 本文 survey 了包括阈值法、边缘检测、区域分割、聚类算法、基于模型的技术、卷积神经网络（CNN）、全卷积网络（FCN）、U-Net 及其变体、注意力机制、半监督学习、生成对抗网络（GANs）和 Transformer 模型在内的 MIS 方法，并探讨了混合架构、跨模态学习、联邦/分布式学习和主动学习等新兴趋势，最后以腰椎分割为例进行案例研究。

Result: survey 涵盖了广泛的 MIS 方法，并深入探讨了新兴技术和特定解剖区域（如腰椎）的分割，展示了该领域的进展。

Conclusion: 尽管 MIS 取得了显著进展，但在数据集偏差、域适应、模型可解释性和临床工作流程集成方面仍存在挑战。

Abstract: Medical Image Segmentation (MIS) stands as a cornerstone in medical image
analysis, playing a pivotal role in precise diagnostics, treatment planning,
and monitoring of various medical conditions. This paper presents a
comprehensive and systematic survey of MIS methodologies, bridging the gap
between traditional image processing techniques and modern deep learning
approaches. The survey encompasses thresholding, edge detection, region-based
segmentation, clustering algorithms, and model-based techniques while also
delving into state-of-the-art deep learning architectures such as Convolutional
Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely
adopted U-Net and its variants. Moreover, integrating attention mechanisms,
semi-supervised learning, generative adversarial networks (GANs), and
Transformer-based models is thoroughly explored. In addition to covering
established methods, this survey highlights emerging trends, including hybrid
architectures, cross-modality learning, federated and distributed learning
frameworks, and active learning strategies, which aim to address challenges
such as limited labeled datasets, computational complexity, and model
generalizability across diverse imaging modalities. Furthermore, a specialized
case study on lumbar spine segmentation is presented, offering insights into
the challenges and advancements in this relatively underexplored anatomical
region. Despite significant progress in the field, critical challenges persist,
including dataset bias, domain adaptation, interpretability of deep learning
models, and integration into real-world clinical workflows.

</details>


### [11] [DECOR: Deep Embedding Clustering with Orientation Robustness](https://arxiv.org/abs/2510.03328)
*Fiona Victoria Stanley Jothiraj,Arunaggiri Pandian Karunanidhi,Seth A. Eichmeyer*

Main category: cs.CV

TL;DR: DECOR是一个深度聚类框架，能够鲁棒地处理具有方向变化的复杂晶圆缺陷模式，并在混合WM38数据集上实现无需手动调整的聚类。


<details>
  <summary>Details</summary>
Motivation: 在半导体制造中，优化产品良率的关键在于尽早检测晶圆缺陷。然而，原始晶圆质量测试数据通常复杂、无标签、不平衡，且单个晶圆可能存在多种缺陷，因此设计在不完美数据条件下仍可靠的聚类方法至关重要。

Method: DECOR（Deep Clustering with Orientation Robustness）框架，通过考虑晶圆图中的方向变化，将复杂缺陷模式分组到一致的簇中。

Result: 在开源的MixedWM38数据集上进行评估，DECOR能够发现簇，且无需手动调整。实验表明，该方法优于现有的聚类基线方法。

Conclusion: DECOR为自动化视觉检测系统提供了一种可靠且可扩展的解决方案，能够鲁棒地处理具有方向变化的复杂晶圆缺陷模式。

Abstract: In semiconductor manufacturing, early detection of wafer defects is critical
for product yield optimization. However, raw wafer data from wafer quality
tests are often complex, unlabeled, imbalanced and can contain multiple defects
on a single wafer, making it crucial to design clustering methods that remain
reliable under such imperfect data conditions. We introduce DECOR, a deep
clustering with orientation robustness framework that groups complex defect
patterns from wafer maps into consistent clusters. We evaluate our method on
the open source MixedWM38 dataset, demonstrating its ability to discover
clusters without manual tuning. DECOR explicitly accounts for orientation
variations in wafer maps, ensuring that spatially similar defects are
consistently clustered regardless of its rotation or alignment. Experiments
indicate that our method outperforms existing clustering baseline methods, thus
providing a reliable and scalable solution in automated visual inspection
systems.

</details>


### [12] [Error correction in multiclass image classification of facial emotion on unbalanced samples](https://arxiv.org/abs/2510.03337)
*Andrey A. Lebedev,Victor B. Kazantsev,Sergey V. Stasenko*

Main category: cs.CV

TL;DR: 该论文提出了一种基于LSTM和注意力机制的神经网络模型，用于解决人脸图像不平衡样本的多分类错误校正问题。


<details>
  <summary>Details</summary>
Motivation: 在人脸图像的多分类任务中，特别是在样本不平衡的情况下，存在类别不平衡和错误校正的挑战。

Method: 使用基于LSTM和注意力机制的神经网络模型，该模型能够关注人脸中对情绪识别有信息量的关键区域。在实验中，模型在六个类别的子集上进行训练，然后对排除在外的第七个类别进行错误校正。

Result: 实验表明，所有类别的错误校正都是可能的，但成功程度不同。在测试样本中，对某些类别的校正提高了关键质量指标，尤其是在小类别上，这表明该方法在寻找罕见事件（如反欺诈系统）的应用问题上具有潜力。

Conclusion: 所提出的方法可以有效地应用于面部表情分析系统以及需要处理类别分布不均的稳定分类任务。

Abstract: This paper considers the problem of error correction in multi-class
classification of face images on unbalanced samples. The study is based on the
analysis of a data frame containing images labeled by seven different emotional
states of people of different ages. Particular attention is paid to the problem
of class imbalance, in which some emotions significantly prevail over others.
To solve the classification problem, a neural network model based on LSTM with
an attention mechanism focusing on key areas of the face that are informative
for emotion recognition is used. As part of the experiments, the model is
trained on all possible configurations of subsets of six classes with
subsequent error correction for the seventh class, excluded at the training
stage. The results show that correction is possible for all classes, although
the degree of success varies: some classes are better restored, others are
worse. In addition, on the test sample, when correcting some classes, an
increase in key quality metrics for small classes was recorded, which indicates
the promise of the proposed approach in solving applied problems related to the
search for rare events, for example, in anti-fraud systems. Thus, the proposed
method can be effectively applied in facial expression analysis systems and in
tasks requiring stable classification under skewed class distribution.

</details>


### [13] [OpusAnimation: Code-Based Dynamic Chart Generation](https://arxiv.org/abs/2510.03341)
*Bozheng Li,Miao Yang,Zhenhan Chen,Jiawang Cao,Mushui Liu,Yi Lu,Yongliang Wu,Bin Zhang,Yangguang Ji,Licheng Tang,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: 本论文提出 DCG-Bench 基准测试和 Qwen2.5-VL-DCG-3B 模型，以评估和提升多模态大语言模型（MLLM）在动态图表生成（DCG）方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLMs 在处理动态图表生成方面能力不足，研究空白有待填补。

Method: 创建了 DCG-8K 数据集，包含指令-代码-视频三元组和问答对；提出了一个两阶段训练方案，并引入了联合代码视觉奖励（Joint-Code-Visual Reward）以优化策略。

Result: 在 DCG-Bench 基准测试中，提出的 Qwen2.5-VL-DCG-3B 模型在三个任务上的平均性能比现有最佳开源 MLLM 提升了 8.31%，并且在仅有 3B 参数的情况下，表现与闭源模型相当。

Conclusion: 本研究填补了 MLLM 在动态图表生成领域的空白，提出的模型和训练方法证明了其有效性，为未来该领域的研究奠定了基础。

Abstract: Dynamic Chart Generation (DCG) involves producing code-rendered animated
visualizations as charts. While recent advances in multi-modal large language
models (MLLMs) have significantly improved their capability on static chart
generation and comprehension, MLLMs' potential for handling dynamic chart
generation and understanding remains underexplored. To bridge this research
gap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first
benchmark evaluating MLLM's capability on dynamic chart generation tasks from
three dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and
Video-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with
annotations covering instruction-code-video triplets and QA pairs for both code
and video evaluation. Based on DCG-8K, we explored a two-stage training recipe,
proposing Joint-Code-Visual Reward for group relative policy optimization to
construct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking
result reveals shortcomings of existing MLLMs in the visual-to-chart task, and
our model beats the best open-sourced MLLM with an average 8.31% performance
gain across three tasks, and shows on par performance against proprietary
models with only 3B parameters, proving the effectiveness of our training
recipe. Our code and dataset will be publicly available.

</details>


### [14] [Visual Odometry with Transformers](https://arxiv.org/abs/2510.03348)
*Vlardimir Yugay,Duy-Kien Nguyen,Theo Gevers,Cees G. M. Snoek,Martin R. Oswald*

Main category: cs.CV

TL;DR: VoT是一个端到端的视觉里程计Transformer模型，可以直接预测相机运动，无需密集几何估计，并且在各种数据集和预训练骨干网络上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的单目视觉里程计方法通常复杂且难以泛化到未知的真实世界场景，而现有的3D模型在处理长视频和提供准确的逐帧估计方面存在局限性。

Method: VoT通过提取特征并利用时空注意力对帧序列中的全局关系进行建模，直接预测相机运动，仅使用相机姿态进行监督，并且可以集成预训练的编码器。

Result: VoT能够有效处理大型数据集，受益于更强的预训练骨干网络，并且在不同的相机运动和校准设置下都具有良好的泛化性，其性能优于传统方法，并且运行速度超过3倍。

Conclusion: VoT提供了一种有效的、端到端的方法来解决单目视觉里程计问题，无需手工组件，并且在性能和效率方面均优于现有方法。

Abstract: Modern monocular visual odometry methods typically combine pre-trained deep
learning components with optimization modules, resulting in complex pipelines
that rely heavily on camera calibration and hyperparameter tuning, and often
struggle in unseen real-world scenarios. Recent large-scale 3D models trained
on massive amounts of multi-modal data have partially alleviated these
challenges, providing generalizable dense reconstruction and camera pose
estimation. Still, they remain limited in handling long videos and providing
accurate per-frame estimates, which are required for visual odometry. In this
work, we demonstrate that monocular visual odometry can be addressed
effectively in an end-to-end manner, thereby eliminating the need for
handcrafted components such as bundle adjustment, feature matching, camera
calibration, or dense 3D reconstruction. We introduce VoT, short for Visual
odometry Transformer, which processes sequences of monocular frames by
extracting features and modeling global relationships through temporal and
spatial attention. Unlike prior methods, VoT directly predicts camera motion
without estimating dense geometry and relies solely on camera poses for
supervision. The framework is modular and flexible, allowing seamless
integration of various pre-trained encoders as feature extractors. Experimental
results demonstrate that VoT scales effectively with larger datasets, benefits
substantially from stronger pre-trained backbones, generalizes across diverse
camera motions and calibration settings, and outperforms traditional methods
while running more than 3 times faster. The code will be released.

</details>


### [15] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: 提出一种创新的推理时搜索算法，利用辅助信息指导扩散模型的采样过程，以提高逆问题的重建质量，尤其是在严重病态的情况下。该算法平衡了探索和利用，并可与现有基于扩散的重建方法无缝集成，避免了梯度引导方法易出现的奖励攻击伪影。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的逆问题求解方法忽略了辅助信息，这在严重病态情况下会影响重建质量。

Method: 提出一种创新的推理时搜索算法，利用辅助信息指导扩散模型的采样过程，平衡探索与利用。

Result: 在图像修复、超分辨率和各种去模糊任务（包括运动、高斯、非线性、盲去模糊）上进行了广泛的实验，结果表明该方法在定性和定量上均优于现有方法和基于奖励梯度的引导算法。

Conclusion: 所提出的方法能够有效利用辅助信息，显著提高基于扩散模型的图像重建质量，尤其是在病态逆问题求解方面，并提供了优于现有方法的性能。

Abstract: Diffusion models have emerged as powerful priors for solving inverse
problems. However, existing approaches typically overlook side information that
could significantly improve reconstruction quality, especially in severely
ill-posed settings. In this work, we propose a novel inference-time search
algorithm that guides the sampling process using the side information in a
manner that balances exploration and exploitation. This enables more accurate
and reliable reconstructions, providing an alternative to the gradient-based
guidance that is prone to reward-hacking artifacts. Our approach can be
seamlessly integrated into a wide range of existing diffusion-based image
reconstruction pipelines. Through extensive experiments on a number of inverse
problems, such as box inpainting, super-resolution, and various deblurring
tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that
our approach consistently improves the qualitative and quantitative performance
of diffusion-based image reconstruction algorithms. We also show the superior
performance of our approach with respect to other baselines, including reward
gradient-based guidance algorithms. The code is available at
\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this
repository}.

</details>


### [16] [Textured Gaussians for Enhanced 3D Scene Appearance Modeling](https://arxiv.org/abs/2411.18625)
*Brian Chao,Hung-Yu Tseng,Lorenzo Porzi,Chen Gao,Tuotuo Li,Qinbo Li,Ayush Saraf,Jia-Bin Huang,Johannes Kopf,Gordon Wetzstein,Changil Kim*

Main category: cs.CV

TL;DR: 3DGS的增强，通过引入alpha和RGB纹理贴图，提高了表示能力和图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统的3DGS模型中，单个高斯点只能表示单一颜色和简单的椭球几何形状，限制了其表现力。

Method: 将纹理和Alpha映射的思路与3DGS结合，为每个高斯点引入alpha（A）、RGB或RGBA纹理贴图，以模拟高斯范围内的颜色和不透明度的空间变化。

Result: 提出的方法在物体和场景层面，在标准基准数据集和自定义数据上均取得了优于现有方法的图像质量，并且使用的-或更少的高斯点数量。

Conclusion: 通过引入纹理贴图，特别是alpha-only和RGB纹理贴图，可以显著增强3DGS的表现力，实现更高的图像质量和更精细的几何细节。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D
reconstruction and rendering technique due to its high-quality results and fast
training and rendering time. However, pixels covered by the same Gaussian are
always shaded in the same color up to a Gaussian falloff scaling factor.
Furthermore, the finest geometric detail any individual Gaussian can represent
is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity
of individual Gaussian primitives. To address these issues, we draw inspiration
from texture and alpha mapping in traditional graphics and integrate it with
3DGS. Specifically, we propose a new generalized Gaussian appearance
representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture
maps to model spatially varying color and opacity across the extent of each
Gaussian. As such, each Gaussian can represent a richer set of texture patterns
and geometric structures, instead of just a single color and ellipsoid as in
naive Gaussian Splatting. Surprisingly, we found that the expressivity of
Gaussians can be greatly improved by using alpha-only texture maps, and further
augmenting Gaussians with RGB texture maps achieves the highest expressivity.
We validate our method on a wide variety of standard benchmark datasets and our
own custom captures at both the object and scene levels. We demonstrate image
quality improvements over existing methods while using a similar or lower
number of Gaussians.

</details>


### [17] [Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications](https://arxiv.org/abs/2510.03353)
*Larissa S. Gomes,Gustavo P. Almeida,Bryan U. Moreira,Marco Quiroz,Breno Xavier,Lucas Soares,Stephanie L. Brião,Felipe G. Oliveira,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: 公开的、带注释的声纳图像数据集很少，阻碍了机器学习模型的发展。本文对现有数据集进行了全面的回顾，以指导研究人员。


<details>
  <summary>Details</summary>
Motivation: 为推动水下探索、自主导航和生态系统监测，需要更多高质量的声纳图像数据，但目前公开数据集的缺乏是一个重大瓶颈。

Method: 对各种声纳模式（SSS、FLS、SAS、MBES、DIDSON）的公开数据集进行了梳理，并分析了它们在分类、检测、分割和三维重建等应用中的使用情况，同时考虑了最新的数据集。

Result: 创建了一个包含数据集特性、大小和注释细节的汇总表和时间线，以方便研究人员进行比较。

Conclusion: 对现有声纳图像数据集进行了全面的梳理和分析，指出了研究中的空白，并为该领域的研究人员提供了指导和路线图。

Abstract: Sonar images are relevant for advancing underwater exploration, autonomous
navigation, and ecosystem monitoring. However, the progress depends on data
availability. The scarcity of publicly available, well-annotated sonar image
datasets creates a significant bottleneck for the development of robust machine
learning models. This paper presents a comprehensive and concise review of the
current landscape of sonar image datasets, seeking not only to catalog existing
resources but also to contextualize them, identify gaps, and provide a clear
roadmap, serving as a base guide for researchers of any kind who wish to start
or advance in the field of underwater acoustic data analysis. We mapped
publicly accessible datasets across various sonar modalities, including Side
Scan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),
Multibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar
(DIDSON). An analysis was conducted on applications such as classification,
detection, segmentation, and 3D reconstruction. This work focuses on
state-of-the-art advancements, incorporating newly released datasets. The
findings are synthesized into a master table and a chronological timeline,
offering a clear and accessible comparison of characteristics, sizes, and
annotation details datasets.

</details>


### [18] [Provenance Networks: End-to-End Exemplar-Based Explainability](https://arxiv.org/abs/2510.03361)
*Ali Kayyam,Anusha Madan Gopal,M. Anthony Lewis*

Main category: cs.CV

TL;DR: Provenance networks are neural models providing end-to-end explainability by linking predictions to training examples, inspired by k-NN, offering insights into memorization, data verification, anomaly detection, and robustness, though with added computational cost.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of post-hoc explainability methods by creating neural models that learn to link each prediction directly to supporting training examples as an inherent part of their operation, thus embedding interpretability into the architecture.

Method: The paper introduces provenance networks, a novel class of neural models that function similarly to a learned k-NN. Each output is justified by weighted, relevant training exemplars from the feature space. The model jointly optimizes the primary task and an explainability objective.

Result: Provenance networks facilitate investigations into the memorization-generalization trade-off, enable verification of training set inclusion, aid in detecting mislabeled or anomalous data, enhance resilience to input perturbations, and support the identification of similar inputs contributing to new data points. They offer insights traditional deep networks cannot.

Conclusion: Provenance networks provide a complementary approach to existing explainability techniques, embedding interpretability into the model architecture itself. While introducing computational costs and currently scaling to moderately sized datasets, they address critical challenges like model opaqueness, hallucination, and credit assignment, thereby improving transparency, robustness, and trustworthiness in neural models.

Abstract: We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.

</details>


### [19] [Unified Unsupervised Anomaly Detection via Matching Cost Filtering](https://arxiv.org/abs/2510.03363)
*Zhe Zhang,Mingxiu Cai,Gaochang Wu,Jing Zhang,Lingqiao Liu,Dacheng Tao,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为统一代价过滤 (UCF) 的通用后处理框架，用于改进任何无监督异常检测 (UAD) 模型生成的异常代价体，该框架通过多层注意力机制指导，可以减轻匹配噪声并突出显示细微的异常，在单一模式和多模式 UAD 场景中均取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督异常检测 (UAD) 方法在处理匹配噪声时存在局限性，并且单一模式和多模式 UAD 研究相对独立，阻碍了全面的理解和知识转移。

Method: 提出统一代价过滤 (UCF) 框架，该框架通过构建代价体（将测试样本与来自相同或不同模式的正常样本进行匹配）并利用来自测试样本的多层注意力机制进行可学习的过滤，来改进 UAD 模型的异常代价体。

Result: UCF 框架在 22 个不同的基准测试中表现出色，在单一模式 (RGB) 和多模式 (RGB--3D, RGB--Text) UAD 场景中均持续取得新的最先进结果。

Conclusion: UCF 框架是一种有效的后处理方法，可以提升各种 UAD 方法的性能，并为统一 UAD 的研究提供了一个有前景的方向。

Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level
anomalies using only normal training data, with wide applications such as
industrial inspection and medical analysis, where anomalies are scarce due to
privacy concerns and cold-start constraints. Existing methods, whether
reconstruction-based (restoring normal counterparts) or embedding-based
(pretrained representations), fundamentally conduct image- or feature-level
matching to generate anomaly maps. Nonetheless, matching noise has been largely
overlooked, limiting their detection ability. Beyond earlier focus on unimodal
RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D
and RGB--Text, enabled by point cloud sensing and vision--language models.
Despite shared challenges, these lines remain largely isolated, hindering a
comprehensive understanding and knowledge transfer. In this paper, we advocate
unified UAD for both unimodal and multimodal settings in the matching
perspective. Under this insight, we present Unified Cost Filtering (UCF), a
generic post-hoc refinement framework for refining anomaly cost volume of any
UAD model. The cost volume is constructed by matching a test sample against
normal samples from the same or different modalities, followed by a learnable
filtering module with multi-layer attention guidance from the test sample,
mitigating matching noise and highlighting subtle anomalies. Comprehensive
experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in
enhancing a variety of UAD methods, consistently achieving new state-of-the-art
results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD
scenarios. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [20] [Visual Language Model as a Judge for Object Detection in Industrial Diagrams](https://arxiv.org/abs/2510.03376)
*Sanjukta Ghosh*

Main category: cs.CV

TL;DR: 该论文提出了一种使用视觉语言模型（VLM）来评估和改进工业图纸（如P&ID）中对象检测结果的框架，以实现更准确的数字化和自动化。


<details>
  <summary>Details</summary>
Motivation: 工业图纸的数字化对于构建数字孪生和实现智能工业自动化至关重要，但目前缺乏自动评估对象检测结果质量的方法。

Method: 利用视觉语言模型（VLM）的多模态能力来识别对象检测中的遗漏或不一致之处，从而实现自动质量评估并指导模型优化。

Result: 提出的框架能够自动评估对象检测结果的质量，并用于改进复杂工业图纸的检测性能。

Conclusion: 该方法通过VLM实现了对象检测结果的自动化质量评估和优化，解决了现有技术中的不足。

Abstract: Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are
essential for the design, operation, and maintenance of industrial plants.
Converting these diagrams into digital form is an important step toward
building digital twins and enabling intelligent industrial automation. A
central challenge in this digitalization process is accurate object detection.
Although recent advances have significantly improved object detection
algorithms, there remains a lack of methods to automatically evaluate the
quality of their outputs. This paper addresses this gap by introducing a
framework that employs Visual Language Models (VLMs) to assess object detection
results and guide their refinement. The approach exploits the multimodal
capabilities of VLMs to identify missing or inconsistent detections, thereby
enabling automated quality assessment and improving overall detection
performance on complex industrial diagrams.

</details>


### [21] [OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications](https://arxiv.org/abs/2510.03915)
*Sagar Bharadwaj,Harrison Williams,Luke Wang,Michael Liang,Tao Jin,Srinivasan Seshan,Anthony Rowe*

Main category: cs.CV

TL;DR: OpenFLAME是一个联合VPS后端，解决了中心化VPS无法覆盖室内空间的问题，并引入了跨空间一致性、服务质量控制和服务选择等挑战的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前中心化的VPS解决方案（如Google和Niantic）因隐私、法规和维护成本问题，无法满足室内AR应用的需求，且覆盖范围有限。

Method: 提出了一种名为OpenFLAME的联合VPS后端，允许独立组织维护各自的3D扫描和VPS服务。解决了跨空间一致性、服务质量控制、服务选择等问题，并引入了联合图像定位的概念，提供了跨地图数据管理和合并的解决方案，同时保护隐私。

Result: 实现了允许独立组织维护各自VPS服务，支持室内空间覆盖、访问控制和分布式维护，并鼓励扩大覆盖范围。

Conclusion: OpenFLAME通过联合图像定位和提供跨地图数据管理/合并的解决方案，为AR应用提供了可扩展的、注重隐私的6DoF定位后端，克服了现有中心化VPS的局限性。

Abstract: World-scale augmented reality (AR) applications need a ubiquitous 6DoF
localization backend to anchor content to the real world consistently across
devices. Large organizations such as Google and Niantic are 3D scanning outdoor
public spaces in order to build their own Visual Positioning Systems (VPS).
These centralized VPS solutions fail to meet the needs of many future AR
applications -- they do not cover private indoor spaces because of privacy
concerns, regulations, and the labor bottleneck of updating and maintaining 3D
scans. In this paper, we present OpenFLAME, a federated VPS backend that allows
independent organizations to 3D scan and maintain a separate VPS service for
their own spaces. This enables access control of indoor 3D scans, distributed
maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS
services introduces several unique challenges -- coherency of localization
results across spaces, quality control of VPS services, selection of the right
VPS service for a location, and many others. We introduce the concept of
federated image-based localization and provide reference solutions for managing
and merging data across maps without sharing private data.

</details>


### [22] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: SpatialViLT通过整合深度图、3D坐标和边缘图等空间特征，并采用多任务学习框架，提升了视觉语言模型在3D场景和复杂对象配置中的空间推理能力，并在VSR数据集上取得了最先进的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型(VLMs)在处理3D场景和复杂对象配置的空间推理方面存在挑战。

Method: 提出SpatialViLT模型，通过多任务学习框架整合深度图、3D坐标和边缘图等空间特征，并提出了SpatialViLT和MaskedSpatialViLT两种变体，以及结合两者的SpatialEnsemble。

Result: SpatialEnsemble在视觉空间推理(VSR)数据集上取得了最先进的准确率，尤其在方向、拓扑和邻近关系等空间推理任务上表现出色。

Conclusion: 该研究显著提高了AI系统的空间智能，为高级多模态理解和现实世界应用奠定了基础。

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still
face challenges in spatial reasoning for 3D scenes and complex object
configurations. To address this, we introduce SpatialViLT, an enhanced VLM that
integrates spatial features like depth maps, 3D coordinates, and edge maps
through a multi-task learning framework. This approach enriches multimodal
embeddings with spatial understanding. We propose two variants: SpatialViLT and
MaskedSpatialViLT, focusing on full and masked object regions, respectively.
Additionally, SpatialEnsemble combines both approaches, achieving
state-of-the-art accuracy. Our models excel in spatial reasoning categories
such as directional, topological, and proximity relations, as demonstrated on
the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a
significant step in enhancing the spatial intelligence of AI systems, crucial
for advanced multimodal understanding and real-world applications.

</details>


### [23] [Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks](https://arxiv.org/abs/2510.03452)
*Allison Davis,Yezhi Shen,Xiaoyu Ji,Fengqing Zhu*

Main category: cs.CV

TL;DR: 通过生成合成训练数据，利用 encoder-decoder 网络（DAE 和 U-Net）来减少双相光学切片结构光照明 (OS-SI) 中的伪影，从而提高图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的伪影去除方法难以处理双相 OS-SI 中因采集时间缩短而产生的残留伪影，而监督学习方法则受限于缺乏干净的光学切片真值数据。

Method: 采用 encoder-decoder 网络（包括非对称去噪自编码器 DAE 和 U-Net），并使用通过将真实伪影场应用于合成图像而生成的合成训练对进行训练。

Result: 在真实 OS-SI 图像上评估训练好的网络，结果表明两种网络都能提高图像清晰度，并且在不同类型的伪影上各有优势。

Conclusion: 合成训练数据可用于监督学习 OS-SI 图像的去噪，并且 encoder-decoder 网络在简化重建流程方面具有潜力。

Abstract: Structured illumination (SI) enhances image resolution and contrast by
projecting patterned light onto a sample. In two-phase optical-sectioning SI
(OS-SI), reduced acquisition time introduces residual artifacts that
conventional denoising struggles to suppress. Deep learning offers an
alternative to traditional methods; however, supervised training is limited by
the lack of clean, optically sectioned ground-truth data. We investigate
encoder-decoder networks for artifact reduction in two-phase OS-SI, using
synthetic training pairs formed by applying real artifact fields to synthetic
images. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on
the synthetic data, then evaluated on real OS-SI images. Both networks improve
image clarity, with each excelling against different artifact types. These
results demonstrate that synthetic training enables supervised denoising of
OS-SI images and highlight the potential of encoder-decoder networks to
streamline reconstruction workflows.

</details>


### [24] [PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology](https://arxiv.org/abs/2510.03455)
*Sejuti Majumder,Saarthak Kapse,Moinak Bhattacharya,Xuan Xu,Alisa Yurovsky,Prateek Prasanna*

Main category: cs.CV

TL;DR: PEaRL是一个多模态框架，通过通路激活得分来整合组织病理学和空间转录组学，提高了癌症ST数据集的预测准确性，并增强了解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态方法依赖于少量高变基因，这限制了预测范围并忽视了塑造组织表型的协同生物学程序。本研究旨在通过整合组织病理学和空间转录组学来克服这些限制。

Method: PEaRL框架使用ssGSEA计算通路激活得分来表示转录组学，并通过Transformer编码生物学上连贯的通路信号，然后利用对比学习将其与组织学特征进行对齐，以实现降维、提高可解释性和加强跨模态对应关系。

Result: 在三个癌症ST数据集（乳腺癌、皮肤癌和淋巴结癌）上，PEaRL在基因和通路级别表达预测方面均优于现有SOTA方法，Pearson相关系数分别提高了58.9%和20.4%。

Conclusion: 将转录组学表示 grounding 在通路中可以产生更符合生物学原理且更具可解释性的多模态模型，推动计算病理学超越基因级别嵌入。

Abstract: Integrating histopathology with spatial transcriptomics (ST) provides a
powerful opportunity to link tissue morphology with molecular function. Yet
most existing multimodal approaches rely on a small set of highly variable
genes, which limits predictive scope and overlooks the coordinated biological
programs that shape tissue phenotypes. We present PEaRL (Pathway Enhanced
Representation Learning), a multimodal framework that represents
transcriptomics through pathway activation scores computed with ssGSEA. By
encoding biologically coherent pathway signals with a transformer and aligning
them with histology features via contrastive learning, PEaRL reduces
dimensionality, improves interpretability, and strengthens cross-modal
correspondence. Across three cancer ST datasets (breast, skin, and lymph node),
PEaRL consistently outperforms SOTA methods, yielding higher accuracy for both
gene- and pathway-level expression prediction (up to 58.9 percent and 20.4
percent increase in Pearson correlation coefficient compared to SOTA). These
results demonstrate that grounding transcriptomic representation in pathways
produces more biologically faithful and interpretable multimodal models,
advancing computational pathology beyond gene-level embeddings.

</details>


### [25] [DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2510.03483)
*Numan Saeed,Tausifa Jan Saleem,Fadillah Maani,Muhammad Ridzuan,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: DuPLUS是一个高效的多模态医学影像分析深度学习框架，通过新颖的视觉-语言模型和分层语义提示，提高了泛化性和预后预测能力，并在多种医学影像任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的医学影像深度学习模型存在任务特定、泛化性差、预后能力不足的问题；而通用方法又存在条件简单、医学语义理解能力弱的缺点。DuPLUS旨在解决这些局限性。

Method: DuPLUS提出了一种新颖的视觉-语言框架，利用分层语义提示进行精细化的任务控制，并通过分层、文本控制的架构和双提示机制实现跨任务的扩展性。

Result: DuPLUS在分割任务上能够跨越三种影像模态、十个不同的解剖学数据集、涵盖30多种器官和肿瘤类型，在8/10的数据集上优于最先进的特定任务和通用模型。此外，通过集成电子健康记录（EHR）数据进行预后预测，在头颈癌数据集上达到了0.69的C指数。参数高效的微调使其能够快速适应来自不同中心的新任务和模态。

Conclusion: DuPLUS是一个通用的、临床相关的医学影像分析解决方案，通过其可扩展的文本控制架构和参数高效的微调，能够快速适应新任务和模态。

Abstract: Deep learning for medical imaging is hampered by task-specific models that
lack generalizability and prognostic capabilities, while existing 'universal'
approaches suffer from simplistic conditioning and poor medical semantic
understanding. To address these limitations, we introduce DuPLUS, a deep
learning framework for efficient multi-modal medical image analysis. DuPLUS
introduces a novel vision-language framework that leverages hierarchical
semantic prompts for fine-grained control over the analysis task, a capability
absent in prior universal models. To enable extensibility to other medical
tasks, it includes a hierarchical, text-controlled architecture driven by a
unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize
across three imaging modalities, ten different anatomically various medical
datasets, encompassing more than 30 organs and tumor types. It outperforms the
state-of-the-art task specific and universal models on 8 out of 10 datasets. We
demonstrate extensibility of its text-controlled architecture by seamless
integration of electronic health record (EHR) data for prognosis prediction,
and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)
of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks
and modalities from varying centers, establishing DuPLUS as a versatile and
clinically relevant solution for medical image analysis. The code for this work
is made available at: https://anonymous.4open.science/r/DuPLUS-6C52

</details>


### [26] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: PaperTalker是一个新的框架，用于根据研究论文自动生成学术演示视频，并附带一个包含论文、视频、幻灯片和元数据的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 生成学术演示视频劳动密集，需要大量时间设计幻灯片、录制和编辑，并且比普通视频更具挑战性，因为它需要处理研究论文的密集多模态信息（文本、图表、表格），并协调多个对齐的通道（幻灯片、字幕、语音、讲者）。

Method: PaperTalker框架包括一个新颖的有效树搜索视觉选择器，用于生成幻灯片并优化布局，以及光标定位、字幕生成、语音合成和讲者视频渲染。它还通过并行进行逐张幻灯片的生成来提高效率。

Result: PaperTalker生成的演示视频比现有方法更忠实、更具信息量。

Conclusion: PaperTalker为自动生成现成的学术视频提供了一个实际的解决方案。

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>


### [27] [Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms](https://arxiv.org/abs/2510.03501)
*Lyes Saad Saoud,Loic Lesobre,Enrico Sorato,Irfan Hussain*

Main category: cs.CV

TL;DR: 该研究提出了一种针对自然环境中野生动物实时检测和分割的移动优化深度学习框架，通过并行化YOLOv10检测和MobileSAM分割，显著提高了实时性能，并在模拟鸨数据集上取得了优异的mAP和mIoU结果。


<details>
  <summary>Details</summary>
Motivation: 实时、准确地在自然环境中检测和分割野生动物对于 the conservation of wildlife 至关重要，但现有的方法面临计算资源有限和物种隐蔽性强的挑战。

Method: 提出了一种移动优化的两阶段深度学习框架，集成了TDM（Threading Detection Model）来并行化YOLOv10检测和MobileSAM分割，利用线程减少延迟，实现高效的资源利用。

Result: 在模拟鸨数据集上，模型达到了mAP50为0.9627，mAP75为0.7731，mAP95为0.7178，MobileSAM的mIoU为0.7421。YOLOv10的推理时间为43.7毫秒/帧，证明了其实时性。同时，构建了一个包含40,000张标注图像的模拟鸨数据集。

Conclusion: 该研究提出的移动优化两阶段深度学习框架能够有效地实现野生动物的实时检测和分割，特别是在计算资源受限的情况下，为 the conservation of wildlife 提供了有价值的工具。

Abstract: Real-time animal detection and segmentation in natural environments are vital
for wildlife conservation, enabling non-invasive monitoring through remote
camera streams. However, these tasks remain challenging due to limited
computational resources and the cryptic appearance of many species. We propose
a mobile-optimized two-stage deep learning framework that integrates a
Threading Detection Model (TDM) to parallelize YOLOv10-based detection and
MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach
improves real-time performance by reducing latency through threading. YOLOv10
handles detection while MobileSAM performs lightweight segmentation, both
executed concurrently for efficient resource use. On the cryptic Houbara
Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627,
mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10
operates at 43.7 ms per frame, confirming real-time readiness. We introduce a
curated Houbara dataset of 40,000 annotated images to support model training
and evaluation across diverse conditions. The code and dataset used in this
study are publicly available on GitHub at
https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos
and additional resources, visit
https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.

</details>


### [28] [Platonic Transformers: A Solid Choice For Equivariance](https://arxiv.org/abs/2510.03511)
*Mohammad Mohaiminul Islam,Rishabh Anand,David R. Wessels,Friso de Kruiff,Thijs P. Kuipers,Rex Ying,Clara I. Sánchez,Sharvaree Vadgama,Georg Bökman,Erik J. Bekkers*

Main category: cs.CV

TL;DR: Platonic Transformer 引入了结合了标准 Transformer 效率和几何对称性的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的能够处理几何对称性的方法通常效率低下且设计复杂，而 Transformer 缺乏几何对称性归纳偏置。

Method: 通过定义相对于 Platonic 实数对称群的参考系来进行注意力计算，实现了原则性的权重共享，从而在不增加计算成本的情况下，结合了对连续平移和 Platonic 对称性的归纳能力。

Result: 所提出的 Platonic Transformer 在计算机视觉、3D 点云和分子性质预测等多个基准测试中取得了具有竞争力的性能。

Conclusion: Platonic Transformer 能够以标准 Transformer 的计算成本来实现对几何约束的利用，并在各项任务中取得了有竞争力的结果。

Abstract: While widespread, Transformers lack inductive biases for geometric symmetries
common in science and computer vision. Existing equivariant methods often
sacrifice the efficiency and flexibility that make Transformers so effective
through complex, computationally intensive designs. We introduce the Platonic
Transformer to resolve this trade-off. By defining attention relative to
reference frames from the Platonic solid symmetry groups, our method induces a
principled weight-sharing scheme. This enables combined equivariance to
continuous translations and Platonic symmetries, while preserving the exact
architecture and computational cost of a standard Transformer. Furthermore, we
show that this attention is formally equivalent to a dynamic group convolution,
which reveals that the model learns adaptive geometric filters and enables a
highly scalable, linear-time convolutional variant. Across diverse benchmarks
in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular
property prediction (QM9, OMol25), the Platonic Transformer achieves
competitive performance by leveraging these geometric constraints at no
additional cost.

</details>


### [29] [Domain Generalization for Semantic Segmentation: A Survey](https://arxiv.org/abs/2510.03540)
*Manuel Schwonberg,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 本篇论文对领域泛化（DG）这一快速发展的领域进行了全面的概述，特别关注了领域泛化语义分割。该研究将现有方法进行了分类和回顾，并指出了向基于基础模型的领域泛化范式的转变。最后，通过广泛的性能比较，强调了基础模型对领域泛化的显著影响，旨在推动领域泛化研究并启发新的研究方向。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在未知域上的泛化能力仍然是一个重大挑战，促使了领域泛化（DG）这一新兴领域的发展。与无监督域适应不同，DG方法在无法访问或了解目标域的情况下，旨在实现跨多个不同但未知的目标域的泛化能力。尤其是在生物医学或自动驾驶等领域中，领域泛化对于语义分割任务至关重要。

Method: 本研究对领域泛化语义分割的现有方法进行了系统的分类和回顾，识别出向基于基础模型的领域泛化范式转变的趋势。此外，还对所有方法进行了广泛的性能比较。

Result: 性能比较结果凸显了基础模型在领域泛化方面的重要影响。

Conclusion: 本篇论文全面的概述了领域泛化语义分割的研究现状，强调了基础模型在其中日益增长的重要性，并为未来的研究方向提供了启发。

Abstract: The generalization of deep neural networks to unknown domains is a major
challenge despite their tremendous progress in recent years. For this reason,
the dynamic area of domain generalization (DG) has emerged. In contrast to
unsupervised domain adaptation, there is no access to or knowledge about the
target domains, and DG methods aim to generalize across multiple different
unseen target domains. Domain generalization is particularly relevant for the
task semantic segmentation which is used in several areas such as biomedicine
or automated driving. This survey provides a comprehensive overview of the
rapidly evolving topic of domain generalized semantic segmentation. We cluster
and review existing approaches and identify the paradigm shift towards
foundation-model-based domain generalization. Finally, we provide an extensive
performance comparison of all approaches, which highlights the significant
influence of foundation models on domain generalization. This survey seeks to
advance domain generalization research and inspire scientists to explore new
research directions.

</details>


### [30] [From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy](https://arxiv.org/abs/2510.03543)
*Evandros Kaklamanos,Kristjana Kristinsdottir,Jonathan Huang,Dustin Carlson,Rajesh Keswani,John Pandolfino,Mozziyar Etemadi*

Main category: cs.CV

TL;DR: 提出了一种利用Transformer视觉编码器和文本解码器两阶段训练框架的自动报告生成模型，以减轻内窥镜检查的文档负担。


<details>
  <summary>Details</summary>
Motivation: 内窥镜检查（如EGD和结肠镜检查）的文档工作量给胃肠科医生带来沉重负担，影响工作流程效率并导致职业倦怠。

Method: 首先，在图像/文本描述对上预训练视觉编码器和文本解码器，以提取通用的视觉-语言特征；然后，在图像/报告对上进行微调，以生成具有临床意义的发现。

Result: 该模型能够自动生成内窥镜检查报告，从而简化文档流程。

Conclusion: 该方法有望减轻医生的工作负担，并改善患者护理。

Abstract: Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and
colonoscopy play a critical role in diagnosing and managing gastrointestinal
(GI) disorders. However, the documentation burden associated with these
procedures place significant strain on gastroenterologists, contributing to
inefficiencies in clinical workflows and physician burnout. To address this
challenge, we propose a novel automated report generation model that leverages
a transformer-based vision encoder and text decoder within a two-stage training
framework. In the first stage, both components are pre-trained on image/text
caption pairs to capture generalized vision-language features, followed by
fine-tuning on images/report pairs to generate clinically meaningful findings.
Our approach not only streamlines the documentation process but also holds
promise for reducing physician workload and improving patient care.

</details>


### [31] [SketchPlan: Diffusion Based Drone Planning From Human Sketches](https://arxiv.org/abs/2510.03545)
*Sixten Norelius,Aaron O. Feldman,Mac Schwager*

Main category: cs.CV

TL;DR: SketchPlan是一个基于扩散模型的无人机导航路径规划器，它通过解析2D手绘草图和深度图像生成3D飞行路径，实现了零样本的仿真到现实迁移。


<details>
  <summary>Details</summary>
Motivation: 为了解决无人机导航中理解人类意图并生成安全有效3D飞行路径的挑战，我们提出了SketchPlan。

Method: SketchPlan包含两个部分：SketchAdapter将手绘草图映射到2D路径，DiffPath从2D投影和深度图像推断3D轨迹。模型使用包含32k飞行路径的合成数据集进行训练，其中一部分数据带有真实人类草图标签，用于训练SketchAdapter。

Result: 在模拟和真实世界实验中，SketchPlan在低/中度混乱环境中成功率达到100%，在未见过的重度混乱环境中成功率为40%，优于基线模型20-60%。

Conclusion: SketchPlan能够准确解释人类意图并推断3D路径，其模块化设计和混合训练数据（真实草图+自动标签）显著提升了性能。

Abstract: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

</details>


### [32] [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03548)
*Danial Samadi Vahdati,Tai Duc Nguyen,Ekta Prashnani,Koki Nagano,David Luebke,Orazio Gallo,Matthew Stamm*

Main category: cs.CV

TL;DR: AI视频会议系统易受攻击，攻击者可利用其合成的RGB视频实时窃取受害者肖像。本研究提出一种基于姿态条件、大间隔对比编码器的方法，通过提取传输的潜在身份信息并消除瞬态姿态和表情信息，实现身份欺骗检测，且无需重建RGB视频。该方法运行实时，泛化能力强，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: AI驱动的视频会议系统存在安全漏洞，攻击者可以利用其合成的RGB视频实时劫持受害者的肖像，而现有的深度伪造检测方法失效。

Method: 提出一种姿态条件、大间隔对比编码器，该编码器可分离传输的潜在信息中的持久身份线索，同时消除姿态和表情信息。通过对分离后的嵌入进行余弦测试来检测身份欺骗。

Result: 实验表明，该方法在多个Talking-head生成模型上均表现优于现有的劫持防御方法，能够实时运行，并对分布外场景具有良好的泛化能力。

Conclusion: 本研究首次提出了一种无需查看重建RGB视频的生物识别泄漏防御方法，有效解决了AI驱动的视频会议系统的安全问题。

Abstract: AI-based talking-head videoconferencing systems reduce bandwidth by sending a
compact pose-expression latent and re-synthesizing RGB at the receiver, but
this latent can be puppeteered, letting an attacker hijack a victim's likeness
in real time. Because every frame is synthetic, deepfake and synthetic video
detectors fail outright. To address this security problem, we exploit a key
observation: the pose-expression latent inherently contains biometric
information of the driving identity. Therefore, we introduce the first
biometric leakage defense without ever looking at the reconstructed RGB video:
a pose-conditioned, large-margin contrastive encoder that isolates persistent
identity cues inside the transmitted latent while cancelling transient pose and
expression. A simple cosine test on this disentangled embedding flags illicit
identity swaps as the video is rendered. Our experiments on multiple
talking-head generation models show that our method consistently outperforms
existing puppeteering defenses, operates in real-time, and shows strong
generalization to out-of-distribution scenarios.

</details>


### [33] [Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!](https://arxiv.org/abs/2510.03550)
*Junbao Zhou,Yuan Zhou,Kesen Zhao,Qingshan Xu,Beier Zhu,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: 用户可以随时随地通过精细的拖拽交互来修改生成的视频，并支持平移、变形和旋转效果。


<details>
  <summary>Details</summary>
Motivation: 在生成模型生成视频的过程中，用户难以对视频内容进行实时、精细的控制，导致生成内容与用户预期不符。因此，需要一种能够让用户随时随地通过拖拽交互来修改生成视频内容的技术。

Method: 提出了一种名为DragStream的训练无关方法，包括自适应分布自我纠正策略（利用邻近帧的统计信息来约束潜在嵌入的漂移）和空间-频率选择性优化机制（允许模型利用上下文信息，同时通过选择性地沿着生成过程传播视觉线索来减轻干扰）。

Result: DragStream可以无缝集成到现有的自回归视频扩散模型中，并且实验证明了其有效性。

Conclusion: DragStream成功解决了生成视频中拖拽操作带来的潜在空间漂移和上下文干扰问题，实现了对生成视频的精细化、实时交互式编辑。

Abstract: Achieving streaming, fine-grained control over the outputs of autoregressive
video diffusion models remains challenging, making it difficult to ensure that
they consistently align with user expectations. To bridge this gap, we propose
\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new
task that enables users to modify generated videos \emph{anytime} on
\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and
SG-I2V, REVEL unifies drag-style video manipulation as editing and animating
video frames with both supporting user-specified translation, deformation, and
rotation effects, making drag operations versatile. In resolving REVEL, we
observe: \emph{i}) drag-induced perturbations accumulate in latent space,
causing severe latent distribution drift that halts the drag process;
\emph{ii}) streaming drag is easily disturbed by context frames, thereby
yielding visually unnatural outcomes. We thus propose a training-free approach,
\textbf{DragStream}, comprising: \emph{i}) an adaptive distribution
self-rectification strategy that leverages neighboring frames' statistics to
effectively constrain the drift of latent embeddings; \emph{ii}) a
spatial-frequency selective optimization mechanism, allowing the model to fully
exploit contextual information while mitigating its interference via
selectively propagating visual cues along generation. Our method can be
seamlessly integrated into existing autoregressive video diffusion models, and
extensive experiments firmly demonstrate the effectiveness of our DragStream.

</details>


### [34] [GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis](https://arxiv.org/abs/2510.03555)
*Peiran Quan,Zifan Gu,Zhuo Zhao,Qin Zhou,Donghan M. Yang,Ruichen Rong,Yang Xie,Guanghua Xiao*

Main category: cs.CV

TL;DR: GAS-MIL是一个集成了多种基础模型特征的框架，能够有效处理计算病理学中的癌症诊断任务，并在多个数据集上取得了优于或持平于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 个体基础模型（FMs）在计算病理学中是强大的特征提取器，但针对特定诊断任务进行适配和基准测试耗时耗力。GAS-MIL旨在解决这一挑战，通过灵活的框架整合多个FMs的特征，以克服单一模型的局限性。

Method: GAS-MIL框架集成了来自多个基础模型（FMs）的特征，采用分组聚合选择的多实例学习（Group-Aggregative Selection Multi-Instance Learning）方法，无需手动特征选择或大量的特定任务微调，即可整合异构FMs的优势。

Result: 在前列腺（PANDA）、卵巢（UBC-OCEAN）和乳腺（TCGA-BrCa）三个癌症数据集的分类任务中，GAS-MIL的性能持续优于或持平于单个FMs和现有的MIL方法，证明了其鲁棒性和泛化能力。

Conclusion: GAS-MIL通过高效整合异构FMs，简化了病理学模型部署，为未来的多模态和精准肿瘤学应用提供了可扩展的基础。

Abstract: Foundation models (FMs) have transformed computational pathology by providing
powerful, general-purpose feature extractors. However, adapting and
benchmarking individual FMs for specific diagnostic tasks is often
time-consuming and resource-intensive, especially given their scale and
diversity. To address this challenge, we introduce Group-Aggregative Selection
Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that
seamlessly integrates features from multiple FMs, preserving their
complementary strengths without requiring manual feature selection or extensive
task-specific fine-tuning. Across classification tasks in three cancer
datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL
consistently achieves superior or on-par performance relative to individual FMs
and established MIL methods, demonstrating its robustness and generalizability.
By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines
model deployment for pathology and provides a scalable foundation for future
multimodal and precision oncology applications.

</details>


### [35] [Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation](https://arxiv.org/abs/2510.03769)
*Shimaa Elbana,Ahmad Kamal,Shahd Ahmed Ali,Ahmad Al-Kabbany*

Main category: cs.CV

TL;DR: ZFP压缩技术可在不损害自动脑血管分割性能的情况下，有效减小3D医学影像数据集的大小，从而促进协作研究和成果转化。


<details>
  <summary>Details</summary>
Motivation: 应对日益增长的3D医学影像数据集带来的协作研究和可转移性挑战，并评估ZFP压缩技术在不影响颅内动脉瘤检测关键步骤——自动脑血管分割——的性能下的有效性。

Method: 将ZFP压缩技术（包括误差容忍和固定速率模式）应用于包含3D医学影像和真实血管分割的大型数据集，并严格比较压缩后数据与未压缩基线数据的分割质量（Dice系数）。

Result: ZFP压缩技术在误差容忍模式下实现了高达22.89:1的数据缩减率，同时保持了高保真度，平均Dice系数为0.87656，与未压缩基线（Dice系数约为0.8774）相近。

Conclusion: ZFP压缩技术是一种可行且强大的工具，能够实现大规模医学数据集的更高效、更易于访问的研究，从而促进社区内更广泛的协作。

Abstract: The increasing size and complexity of medical imaging datasets, particularly
in 3D formats, present significant barriers to collaborative research and
transferability. This study investigates whether the ZFP compression technique
can mitigate these challenges without compromising the performance of automated
cerebrovascular segmentation, a critical first step in intracranial aneurysm
detection. We apply ZFP in both its error tolerance and fixed-rate modes to a
large scale, and one of the most recent, datasets in the literature, 3D medical
dataset containing ground-truth vascular segmentations. The segmentation
quality on the compressed volumes is rigorously compared to the uncompressed
baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can
achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance
mode--while maintaining a high degree of fidelity, with the mean Dice
coefficient remaining high at 0.87656. These results demonstrate that ZFP is a
viable and powerful tool for enabling more efficient and accessible research on
large-scale medical datasets, fostering broader collaboration across the
community.

</details>


### [36] [Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid](https://arxiv.org/abs/2510.03558)
*Shen Chang,Renran Tian,Nicole Adams,Nan Kong*

Main category: cs.CV

TL;DR: 无人机提供纳洛酮可快速响应阿片类药物过量紧急情况，增强了在急救人员到达前由未经医学培训的旁观者进行干预的能力。本研究提出了一个用于评估旁观者态势感知（SA）的视频分析框架，该框架使用图嵌入和 Transformer 模型，并通过 DANDSD 数据集进行了验证，在 SA 预测和时域分割方面均表现出高性能。


<details>
  <summary>Details</summary>
Motivation: 为了弥合现实世界中无人机辅助阿片类药物过量紧急响应中态势感知（SA）评估的研究空白，并支持开发能够有效指导旁观者的自适应无人机系统。

Method: 利用 DANDSD 数据集，提出一个基于视频的实时 SA 评估框架，该框架整合了图嵌入和 Transformer 模型，并结合了视觉感知和理解线索，如几何、运动和交互图特征。

Result: 所提出的 SA 评估框架在 SA 预测和时域分割方面均取得了高性能，其 MoF 指标比 FINCH 基线提高了 9%，IoU 指标提高了 5%。

Conclusion: 该研究成功开发了一个视频驱动的 SA 评估框架，证明了其在无人机辅助紧急响应中的有效性，并为未来开发更智能、更能适应人类的无人机系统铺平了道路。

Abstract: Rapid naloxone delivery via drones offers a promising solution for responding
to opioid overdose emergencies (OOEs), by extending lifesaving interventions to
medically untrained bystanders before emergency medical services (EMS) arrive.
Recognizing the critical role of bystander situational awareness (SA) in
human-autonomy teaming (HAT), we address a key research gap in real-time SA
assessment by introducing the Drone-Assisted Naloxone Delivery Simulation
Dataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,
where college students without medical training act as bystanders tasked with
administering intranasal naloxone to a mock overdose victim. Leveraging this
dataset, we propose a video-based real-time SA assessment framework that
utilizes graph embeddings and transformer models to assess bystander SA in real
time. Our approach integrates visual perception and comprehension cues--such as
geometric, kinematic, and interaction graph features--and achieves
high-performance SA prediction. It also demonstrates strong temporal
segmentation accuracy, outperforming the FINCH baseline by 9% in Mean over
Frames (MoF) and 5% in Intersection over Union (IoU). This work supports the
development of adaptive drone systems capable of guiding bystanders
effectively, ultimately improving emergency response outcomes and saving lives.

</details>


### [37] [Evaluating OCR performance on food packaging labels in South Africa](https://arxiv.org/abs/2510.03570)
*Mayimunah Nagayi,Alice Khan,Tamryn Frank,Rina Swart,Clement Nyirenda*

Main category: cs.CV

TL;DR: 本研究评估了Tesseract、EasyOCR、PaddleOCR和TrOCR这四种开源OCR系统在真实食品包装图像上的表现，重点评估它们提取配料表和营养成分表的能力。


<details>
  <summary>Details</summary>
Motivation: 准确的包装OCR对于合规性和营养监测至关重要，但由于多语言文本、密集布局、字体多样、眩光和曲面等因素而具有挑战性。

Method: 使用包含231种产品（1,628张图像）的数据集对所有四个模型进行了处理，以评估速度和覆盖范围，并创建了一个包含113张图像（60种产品）的基准子集以评估准确性。评估指标包括字符错误率（CER）、单词错误率（WER）、BLEU、ROUGE-L、F1、覆盖率和执行时间。

Result: 在基准子集上，Tesseract的CER最低（0.912），BLEU得分最高（0.245）。EasyOCR在准确性和多语言支持之间取得了良好的平衡。PaddleOCR实现了近乎完整的覆盖，但由于仅在CPU上运行（GPU不兼容），速度较慢。TrOCR尽管有GPU加速，但结果最差。

Conclusion: 本研究为特定包装场景提供了基准，建立了基线，并指出了面向布局的方法和文本本地化研究的方向。

Abstract: This study evaluates four open-source Optical Character Recognition (OCR)
systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food
packaging images. The aim is to assess their ability to extract ingredient
lists and nutrition facts panels. Accurate OCR for packaging is important for
compliance and nutrition monitoring but is challenging due to multilingual
text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231
products (1,628 images) was processed by all four models to assess speed and
coverage, and a ground truth subset of 113 images (60 products) was created for
accuracy evaluation. Metrics include Character Error Rate (CER), Word Error
Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground
truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU
(0.245). EasyOCR provided a good balance between accuracy and multilingual
support. PaddleOCR achieved near complete coverage but was slower because it
ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest
results despite GPU acceleration. These results provide a packaging-specific
benchmark, establish a baseline, and highlight directions for layout-aware
methods and text localization.

</details>


### [38] [FrameOracle: Learning What to See and How Much to See in Videos](https://arxiv.org/abs/2510.03584)
*Chaoyu Li,Tianzhi Li,Fei Tao,Zhenyu Zhao,Ziqian Wu,Maozheng Zhao,Juntong Song,Cheng Niu,Pooyan Fazli*

Main category: cs.CV

TL;DR: FrameOracle通过预测关键帧及其数量来提高视频理解的效率和准确性，能在不损失准确性的情况下显著减少输入帧数。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型（VLM）在处理视频帧数上存在瓶颈，现有的帧采样策略无法适应信息密度和任务复杂度的变化，导致效率低下和信息丢失。

Method: 提出FrameOracle模块，能够预测相关帧和所需帧数。通过包含关键帧标注的FrameOracle-41K数据集进行四阶段训练。

Result: 在五个VLM和六个基准测试中，FrameOracle将16帧输入平均减少到10.4帧，准确率无损失。将64帧输入减少到13.9帧，同时准确率提升1.4%。

Conclusion: FrameOracle实现了最先进的效率-准确性权衡，能够为可扩展的视频理解提供支持。

Abstract: Vision-language models (VLMs) have advanced video understanding, but their
performance is limited by the number of input frames they can process. Existing
frame sampling strategies, such as uniform or fixed-budget selection, often
fail to adapt to variations in information density or task complexity,
resulting in inefficiency and information loss. To address this, we present
FrameOracle, a lightweight and plug-and-play module that predicts both (1)
which frames are most relevant to a given query and (2) how many frames are
needed. FrameOracle is trained using a four-stage curriculum, with the first
three stages relying on weak proxy signals such as cross-modal similarity. In
the final stage, it leverages stronger supervision from a new dataset we
introduce, FrameOracle-41K, the first large-scale VideoQA collection to provide
keyframe annotations specifying the minimal set of frames required to answer
each question. Extensive experiments across five VLMs and six benchmarks
demonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4
frames without any loss in accuracy. When starting from 64-frame candidates, it
reduces the input to an average of 13.9 frames while improving accuracy by
1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable
video understanding.

</details>


### [39] [A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games](https://arxiv.org/abs/2510.03591)
*Faliu Yi,Sherif Abdelfattah,Wei Huang,Adrian Brown*

Main category: cs.CV

TL;DR: 提出一种混合协同微调（CFT）方法，利用有标签和无标签数据来检测游戏中的视觉错误，减少对特定游戏有标签数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 手动识别游戏中的视觉错误成本高昂且耗时，有标签数据集的稀疏性是一个挑战。

Method: CFT方法结合了目标游戏和跨领域游戏的有标签样本，并纳入无标签数据来增强特征学习。

Result: CFT在多个游戏环境中展现出优越的性能，即使在仅使用50%有标签数据的情况下也具有竞争力。

Conclusion: CFT方法提高了可扩展性和适应性，能够有效地检测各种游戏中的视觉错误。

Abstract: Manual identification of visual bugs in video games is a resource-intensive
and costly process, often demanding specialized domain knowledge. While
supervised visual bug detection models offer a promising solution, their
reliance on extensive labeled datasets presents a significant challenge due to
the infrequent occurrence of such bugs. To overcome this limitation, we propose
a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled
and unlabeled data. Our approach leverages labeled samples from the target game
and diverse co-domain games, additionally incorporating unlabeled data to
enhance feature representation learning. This strategy maximizes the utility of
all available data, substantially reducing the dependency on labeled examples
from the specific target game. The developed framework demonstrates enhanced
scalability and adaptability, facilitating efficient visual bug detection
across various game titles. Our experimental results show the robustness of the
proposed method for game visual bug detection, exhibiting superior performance
compared to conventional baselines across multiple gaming environments.
Furthermore, CFT maintains competitive performance even when trained with only
50% of the labeled data from the target game.

</details>


### [40] [Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation](https://arxiv.org/abs/2510.03598)
*Alexander V. Mantzaris*

Main category: cs.CV

TL;DR: HRM在MNIST上表现良好，但在CIFAR图像分类任务上存在过拟合且泛化能力差，不如简单的卷积网络。


<details>
  <summary>Details</summary>
Motivation: 评估HRM作为图像分类器的可行性，特别是在缺乏数据增强的原始设置下。

Method: 使用包含Transformer模块、DEQ风格训练、深度监督、旋转位置嵌入和RMSNorm的HRM模型，并在MNIST、CIFAR-10和CIFAR-100数据集上进行训练和测试，对比基线CNN模型。

Result: HRM在MNIST上达到约98%的准确率，但在CIFAR-10和CIFAR-100上表现不佳，准确率分别为65.0%和29.7%，而基线CNN模型在CIFAR-10上达到77.2%，CIFAR-100上达到45.3%。HRM训练速度也比CNN慢。

Conclusion: 在没有数据增强的小分辨率图像分类任务中，HRM不如简单的卷积架构有竞争力，但未来的模型修改可能提高其性能。

Abstract: This paper asks whether the Hierarchical Reasoning Model (HRM) with the two
Transformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep
supervision, Rotary Position Embeddings, and RMSNorm can serve as a practical
image classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a
deliberately raw regime: no data augmentation, identical optimizer family with
one-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes
stably and performs well on MNIST ($\approx 98\%$ test accuracy), but on small
natural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches
65.0\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains
77.2\% while training $\sim 30\times$ faster per epoch; on CIFAR-100, HRM
achieves only 29.7\% test accuracy despite 91.5\% train accuracy, while the
same CNN reaches 45.3\% test with 50.5\% train accuracy. Loss traces and error
analyses indicate healthy optimization but insufficient image-specific
inductive bias for HRM in this regime. It is concluded that, for
small-resolution image classification without augmentation, HRM is not
competitive with even simple convolutional architectures as the HRM currently
exist but this does not exclude possibilities that modifications to the model
may allow it to improve greatly.

</details>


### [41] [Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops](https://arxiv.org/abs/2510.03606)
*Mattia Scardecchia*

Main category: cs.CV

TL;DR: DINOv2在视觉特征学习上取得了最先进的成果，本文对其核心思想、发展历程、性能表现以及未来方向进行了探讨。


<details>
  <summary>Details</summary>
Motivation: 探究DINOv2在视觉特征学习上的优势及其核心思想。

Method: 分析DINOv2的多裁剪视图增强和自蒸馏（带均值教师）的核心思想，并追溯其发展。比较DINO和DINOv2与其他自监督学习和弱监督学习方法在不同下游任务上的表现。

Result: DINOv2在大多数基准测试中超越了OpenCLIP等弱监督方法，展现出优越的性能。其学习到的特征具有显著的涌现特性，尤其是在使用Transformer骨干网络时。

Conclusion: DINOv2在视觉特征学习上取得了显著进展，但也存在局限性，为未来的研究指明了方向。

Abstract: Recent advances in self-supervised learning (SSL) have made it possible to
learn general-purpose visual features that capture both the high-level
semantics and the fine-grained spatial structure of images. Most notably, the
recent DINOv2 has established a new state of the art by surpassing weakly
supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we
examine the core ideas behind its approach, multi-crop view augmentation and
self-distillation with a mean teacher, and trace their development in previous
work. We then compare the performance of DINO and DINOv2 with other SSL and WSL
methods across various downstream tasks, and highlight some remarkable emergent
properties of their learned features with transformer backbones. We conclude by
briefly discussing DINOv2's limitations, its impact, and future research
directions.

</details>


### [42] [Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL](https://arxiv.org/abs/2510.03608)
*Ruitao Wu,Yifan Zhao,Guangyao Chen,Jia Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为DCS的新框架，通过扩散模型和FSCIL分类器的协同作用，解决了小样本增量学习中的遗忘和泛化问题，达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的FSCIL方法在泛化能力上表现不佳，因为它们依赖于有限的数据集，而直接应用扩散模型进行数据增强又可能导致语义不匹配或指导效果不佳。

Method: DCS框架通过一个互助增强循环来协调扩散模型和FSCIL分类器。它使用一个与奖励对齐的学习策略，其中一个动态的、多方面的奖励函数（源自分类器的状态）来指导扩散模型。该奖励系统在两个层面运作：特征层面利用原型锚定的最大均值差异和维度方差匹配来确保语义连贯性和多样性；Logits层面通过置信度重新校准和跨会话混淆感知机制来促进探索性图像生成并增强类间可辨别性。

Result: DCS框架通过生成图像来改进分类器，而改进后的分类器状态又产生更好的奖励信号，这种共同演化的过程在FSCIL基准测试中显著提高了知识保留和新类学习能力，达到了最先进的性能。

Conclusion: DCS框架通过扩散模型和FSCIL分类器之间的协同作用，成功解决了小样本增量学习中的稳定性-可塑性困境和数据稀缺性问题，显著提高了模型的泛化能力和学习新类的效率。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially
learn new classes from minimal examples without forgetting prior knowledge, a
task complicated by the stability-plasticity dilemma and data scarcity. Current
FSCIL methods often struggle with generalization due to their reliance on
limited datasets. While diffusion models offer a path for data augmentation,
their direct application can lead to semantic misalignment or ineffective
guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel
framework that establishes a mutual boosting loop between diffusion model and
FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a
dynamic, multi-faceted reward function derived from the classifier's state
directs the diffusion model. This reward system operates at two levels: the
feature level ensures semantic coherence and diversity using prototype-anchored
maximum mean discrepancy and dimension-wise variance matching, while the logits
level promotes exploratory image generation and enhances inter-class
discriminability through confidence recalibration and cross-session
confusion-aware mechanisms. This co-evolutionary process, where generated
images refine the classifier and an improved classifier state yields better
reward signals, demonstrably achieves state-of-the-art performance on FSCIL
benchmarks, significantly enhancing both knowledge retention and new class
learning.

</details>


### [43] [MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations](https://arxiv.org/abs/2510.03666)
*Jiang Wu,Sichao Wu,Yinsong Ma,Guangyuan Yu,Haoyuan Xu,Lifang Zheng,Jingliang Duan*

Main category: cs.CV

TL;DR: MonitorVLM是一个创新的视觉-语言框架，用于通过视频流直接检测工业安全违规行为，特别是在采矿业中。


<details>
  <summary>Details</summary>
Motivation: 传统的安全检查方法效率低下且容易出错，需要一种智能自动化的解决方案来监控高风险领域的安全。这个框架旨在解决这个问题。

Method: MonitorVLM框架结合了一个领域特定的违规数据集（包含9000个视觉-问答样本和40条采矿法规），一个过滤相关条款以提高效率的子句过滤器（CF）模块，以及一个通过增强工人区域来提高识别能力的行为放大器（BM）模块。

Result: MonitorVLM在准确性方面表现出色，与基线模型相比，精确率提高了22.01%，召回率提高了34.22%，F1分数提高了28.37%。CF模块还降低了13.56%的推理延迟，BM模块带来了额外的精确率3.45%和召回率8.62%的提升。此外，还开发了一个基于Web的界面，集成了该系统。

Conclusion: 该研究证明了多模态大模型在提升采矿及其他行业职业安全监控方面的巨大潜力。

Abstract: Industrial accidents, particularly in high-risk domains such as surface and
underground mining, are frequently caused by unsafe worker behaviors.
Traditional manual inspection remains labor-intensive, error-prone, and
insufficient for large-scale, dynamic environments, highlighting the urgent
need for intelligent and automated safety monitoring. In this paper, we present
MonitorVLM, a novel vision--language framework designed to detect safety
violations directly from surveillance video streams. MonitorVLM introduces
three key innovations: (1) a domain-specific violation dataset comprising 9,000
vision--question--answer (VQA) samples across 40 high-frequency mining
regulations, enriched with augmentation and auxiliary detection cues; (2) a
clause filter (CF) module that dynamically selects the Top-$K$ most relevant
clauses, reducing inference latency by 13.56\% while maintaining accuracy; and
(3) a behavior magnifier (BM) module that enhances worker regions to improve
fine-grained action recognition, yielding additional gains of 3.45% in
precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM
significantly outperforms baseline vision--language models, achieving
improvements of 22.01% in precision, 34.22\% in recall, and 28.37% in F1 score
over the 72B unfine-tuned baseline. A lightweight web-based interface further
integrates MonitorVLM into practical workflows, enabling automatic violation
reporting with video timestamping. This study highlights the potential of
multimodal large models to enhance occupational safety monitoring in mining and
beyond.

</details>


### [44] [A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems](https://arxiv.org/abs/2510.03675)
*Siva Sai,Saksham Gupta,Vinay Chamola,Rajkumar Buyya*

Main category: cs.CV

TL;DR: 本研究提出了一种结合引导分类和扩散技术的混合模型，用于提高智能交通系统中事故检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在理解复杂数据分布方面存在局限性，本研究旨在利用扩散模型的强大能力来改进事故检测。

Method: 提出了一种混合模型，将精调后的ExceptionNet输出作为输入，并使用图像张量作为条件。该模型包含多个条件模块，通过时间嵌入和图像协变量嵌入来调节输入的线性投影，并采用基于云的实现来处理计算密集型问题。

Result: 所提出的扩散模型在基于图像的事故检测方面表现最佳，准确率达到97.32%。

Conclusion: 通过详细的消融研究和与基线模型的全面评估，证明了所提出的扩散模型在事故检测任务上的有效性和优越性。

Abstract: The integration of Diffusion Models into Intelligent Transportation Systems
(ITS) is a substantial improvement in the detection of accidents. We present a
novel hybrid model integrating guidance classification with diffusion
techniques. By leveraging fine-tuned ExceptionNet architecture outputs as input
for our proposed diffusion model and processing image tensors as our
conditioning, our approach creates a robust classification framework. Our model
consists of multiple conditional modules, which aim to modulate the linear
projection of inputs using time embeddings and image covariate embeddings,
allowing the network to adapt its behavior dynamically throughout the diffusion
process. To address the computationally intensive nature of diffusion models,
our implementation is cloud-based, enabling scalable and efficient processing.
Our strategy overcomes the shortcomings of conventional classification
approaches by leveraging diffusion models inherent capacity to effectively
understand complicated data distributions. We investigate important diffusion
characteristics, such as timestep schedulers, timestep encoding techniques,
timestep count, and architectural design changes, using a thorough ablation
study, and have conducted a comprehensive evaluation of the proposed model
against the baseline models on a publicly available dataset. The proposed
diffusion model performs best in image-based accident detection with an
accuracy of 97.32%.

</details>


### [45] [SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection](https://arxiv.org/abs/2510.03689)
*Zhengyi Liu,Xinrui Wang,Xianyong Fang,Zhengzheng Tu,Linbo Wang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 SAMSOD 的新模型，用于 RGB-T 显著目标检测，通过单模态监督和梯度冲突消除来解决双模态不平衡和梯度差异问题，并取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有的 RGB-T 显著目标检测方法忽略了双模态不平衡收敛和高低激活梯度差异问题，这限制了性能的进一步提升。

Method: 提出 SAMSOD 模型，采用单模态监督增强非主导模态学习，利用梯度冲突消除减少冲突梯度对模型收敛的影响，并通过解耦的适配器分别处理高低激活神经元以增强背景学习，突出前景目标。

Result: 在 RGB-T SOD 基准数据集、RGB-T SOD 涂鸦监督数据集、RGB-D SOD 全监督数据集以及 RGB-D 导轨表面缺陷检测数据集上进行了广泛实验，证明了所提出方法的有效性。

Conclusion: SAMSOD 模型通过单模态监督和梯度冲突消除等方法，有效解决了 RGB-T SOD 中的挑战，并在多个数据集上取得了优异的性能。

Abstract: RGB-T salient object detection (SOD) aims to segment attractive objects by
combining RGB and thermal infrared images. To enhance performance, the Segment
Anything Model has been fine-tuned for this task. However, the imbalance
convergence of two modalities and significant gradient difference between high-
and low- activations are ignored, thereby leaving room for further performance
enhancement. In this paper, we propose a model called \textit{SAMSOD}, which
utilizes unimodal supervision to enhance the learning of non-dominant modality
and employs gradient deconfliction to reduce the impact of conflicting
gradients on model convergence. The method also leverages two decoupled
adapters to separately mask high- and low-activation neurons, emphasizing
foreground objects by enhancing background learning. Fundamental experiments on
RGB-T SOD benchmark datasets and generalizability experiments on scribble
supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised
RGB-D rail surface defect detection all demonstrate the effectiveness of our
proposed method.

</details>


### [46] [Referring Expression Comprehension for Small Objects](https://arxiv.org/abs/2510.03701)
*Kanoko Goto,Takumi Hirose,Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: 本论文提出了SOREC数据集和PIZA方法，以解决视觉-语言导航任务中小目标定位的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在定位极端小目标方面存在挑战，而这在自动驾驶等实际应用中非常重要。

Method: 创建了包含10万个小目标及其边界框的SOREC数据集，并提出了一种名为PIZA的适配器模块，用于参数高效的微调，使模型能够逐步放大并精确定位小目标。

Result: 将PIZA应用于GroundingDINO模型，在SOREC数据集上显著提高了小目标定位的准确性。

Conclusion: 所提出的SOREC数据集和PIZA方法能够有效提升小目标检测的性能，并且已公开数据集、代码和预训练模型。

Abstract: Referring expression comprehension (REC) aims to localize the target object
described by a natural language expression. Recent advances in vision-language
learning have led to significant performance improvements in REC tasks.
However, localizing extremely small objects remains a considerable challenge
despite its importance in real-world applications such as autonomous driving.
To address this issue, we introduce a novel dataset and method for REC
targeting small objects. First, we present the small object REC (SOREC)
dataset, which consists of 100,000 pairs of referring expressions and
corresponding bounding boxes for small objects in driving scenarios. Second, we
propose the progressive-iterative zooming adapter (PIZA), an adapter module for
parameter-efficient fine-tuning that enables models to progressively zoom in
and localize small objects. In a series of experiments, we apply PIZA to
GroundingDINO and demonstrate a significant improvement in accuracy on the
SOREC dataset. Our dataset, codes and pre-trained models are publicly available
on the project page.

</details>


### [47] [Artery-Vein Segmentation from Fundus Images using Deep Learning](https://arxiv.org/abs/2510.03717)
*Sharan SK,Subin Sahayam,Umarani Jayaraman,Lakshmi Priya A*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 Attention-WNet 的新深度学习模型，用于分割视网膜血管中的动脉和静脉，并在公共数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割（区分动脉和静脉）是进行视网膜血管分析的关键步骤，该分析可为识别和诊断各种眼科疾病提供潜在的生物标志物。血管的规则性和宽度变化可指示全身血管系统的健康状况，有助于识别中风和心肌梗塞等血管疾病的高风险患者。

Method: 提出了一种新颖的深度学习方法，将注意力机制集成到 WNet 深度学习模型中，并将其命名为 Attention-WNet。

Result: 所提出的 Attention-WNet 模型在 HRF 和 DRIVE 公共数据集上进行了测试，其性能优于文献中其他最先进的模型。

Conclusion: Attention-WNet 模型在视网膜动脉和静脉分割任务上表现出优越的性能。

Abstract: Segmenting of clinically important retinal blood vessels into arteries and
veins is a prerequisite for retinal vessel analysis. Such analysis can provide
potential insights and bio-markers for identifying and diagnosing various
retinal eye diseases. Alteration in the regularity and width of the retinal
blood vessels can act as an indicator of the health of the vasculature system
all over the body. It can help identify patients at high risk of developing
vasculature diseases like stroke and myocardial infarction. Over the years,
various Deep Learning architectures have been proposed to perform retinal
vessel segmentation. Recently, attention mechanisms have been increasingly used
in image segmentation tasks. The work proposes a new Deep Learning approach for
artery-vein segmentation. The new approach is based on the Attention mechanism
that is incorporated into the WNet Deep Learning model, and we call the model
as Attention-WNet. The proposed approach has been tested on publicly available
datasets such as HRF and DRIVE datasets. The proposed approach has outperformed
other state-of-art models available in the literature.

</details>


### [48] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: 网页规模多模态数据集（如LAION-400M）存在显著的人口统计学偏见，但偏见来源尚不明确。本研究通过为LAION-400M创建包含2.76亿个边界框、感知性别和种族/民族标签以及自动生成标题的个人中心注释，填补了人口统计学注释的空白。研究利用这些注释揭示了人口统计学不平衡和有害联想，例如将男性和被认为是黑人或中东人的人与犯罪相关和负面内容不成比例地关联起来。此外，研究表明CLIP和Stable Diffusion中60-70%的性别偏见可由数据中的直接共现进行线性解释。本研究首次建立了数据集组成与下游模型偏见之间的大规模经验联系。


<details>
  <summary>Details</summary>
Motivation: 网页规模多模态数据集（如LAION-400M）存在显著的人口统计学偏见，但偏见来源尚不明确。本研究旨在通过创建人口统计学注释来填补这一空白，以研究数据集组成与下游模型偏见之间的关系。

Method: 研究人员创建了包含2.76亿个边界框、感知性别和种族/民族标签以及自动生成标题的个人中心注释。这些注释是通过结合对象检测、多模态字幕和微调分类器的经过验证的自动标记管道生成的。然后，他们利用这些注释来分析LAION-400M数据集中的人口统计学不平衡和有害关联，并量化CLIP和Stable Diffusion等模型中的性别偏见。

Result: 研究揭示了人口统计学不平衡和有害联想，例如将男性和被认为是黑人或中东人的人与犯罪相关和负面内容不成比例地关联起来。此外，研究表明CLIP和Stable Diffusion中60-70%的性别偏见可由数据中的直接共现进行线性解释。

Conclusion: 本研究创建了LAION-400M数据集的人口统计学注释，首次建立了数据集组成与下游模型偏见之间的大规模经验联系。研究结果表明，数据中的人口统计学不平衡和有害联想会直接导致模型偏见，并且可以通过分析数据中的直接共现来量化和解释模型偏见。

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [49] [Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks](https://arxiv.org/abs/2510.03725)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Youssef Fouzai,Renata Gracie,Vanderlei Pascoal De Matos,Helen Gurgel,Nadine Dessay*

Main category: cs.CV

TL;DR: 现有的深度学习方法尚未充分利用预训练神经网络的潜力来检测非正规住区。本研究比较了两种预训练神经网络在里约热内卢贫民窟检测中的表现：一种是在大型多样化数据集上预训练的通用网络，另一种是在卫星图像上预训练的专用网络。通过比较这两种方法，本研究旨在确定任务特异性还是数据量对城市非正规住区检测的性能影响更大。


<details>
  <summary>Details</summary>
Motivation: 以往的深度学习方法在利用预训练神经网络进行非正规住区检测方面存在潜力未被充分挖掘。本研究旨在解决这个问题，并探讨哪种类型的预训练网络（通用网络或专用网络）更适合此类任务。

Method: 本研究比较了两种预训练神经网络在里约热内卢贫民窟检测中的应用：1. 在大型多样化数据集上预训练的通用网络；2. 在卫星图像上预训练的专用网络。通过对比这两种网络在检测任务上的表现，评估任务特异性和数据量对模型性能的影响。

Result: （待补充，因为抽象中未提供具体结果）

Conclusion: （待补充，因为抽象中未提供具体结论）

Abstract: While deep learning methods for detecting informal settlements have already
been developed, they have not yet fully utilized the potential offered by
recent pretrained neural networks. We compare two types of pretrained neural
networks for detecting the favelas of Rio de Janeiro: 1. Generic networks
pretrained on large diverse datasets of unspecific images, 2. A specialized
network pretrained on satellite imagery. While the latter is more specific to
the target task, the former has been pretrained on significantly more images.
Hence, this research investigates whether task specificity or data volume
yields superior performance in urban informal settlement detection.

</details>


### [50] [LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes](https://arxiv.org/abs/2510.03747)
*Zuomin Qu,Yimao Guo,Qianyue Hu,Wei Lu*

Main category: cs.CV

TL;DR: 通过注入低秩自适应（LoRA）补丁，一种新的方法可以绕过现有的深度伪造防御措施，同时还提出了一种用于嵌入可见警告的防御性LoRA补丁，以解决新发现的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 深度伪造（Deepfakes）带来了重大的社会风险，促使人们开发主动防御措施，在人脸图像中嵌入对抗性扰动以防止篡改。然而，这些先发制人的防御措施往往缺乏鲁棒性和可靠性。

Method: 提出了一种新颖的低秩自适应（LoRA）补丁方法，该方法将即插即用的LoRA补丁注入深度伪造生成器，以绕过最先进的防御措施。该方法使用可学习的门控机制来适应性地控制LoRA补丁的效果并防止微调期间的梯度爆炸。还引入了一种多模态特征对齐（MMFA）损失，以在语义层面鼓励对抗性输出的特征与期望输出的特征对齐。此外，还提出了一种防御性LoRA补丁，它在输出中嵌入可见的警告，作为一种互补的解决方案，以减轻新识别的安全漏洞。

Result: 在仅使用1000个人脸示例和单个训练周期的情况下，LoRA补丁成功地击败了多种主动防御措施。

Conclusion: 这些结果揭示了当前范式的一个关键弱点，并强调了对更鲁棒的深度伪造防御策略的需求。

Abstract: Deepfakes pose significant societal risks, motivating the development of
proactive defenses that embed adversarial perturbations in facial images to
prevent manipulation. However, in this paper, we show that these preemptive
defenses often lack robustness and reliability. We propose a novel approach,
Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch
into Deepfake generators to bypass state-of-the-art defenses. A learnable
gating mechanism adaptively controls the effect of the LoRA patch and prevents
gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature
Alignment (MMFA) loss, encouraging the features of adversarial outputs to align
with those of the desired outputs at the semantic level. Beyond bypassing, we
present defensive LoRA patching, embedding visible warnings in the outputs as a
complementary solution to mitigate this newly identified security
vulnerability. With only 1,000 facial examples and a single epoch of
fine-tuning, LoRA patching successfully defeats multiple proactive defenses.
These results reveal a critical weakness in current paradigms and underscore
the need for more robust Deepfake defense strategies. Our code is available at
https://github.com/ZOMIN28/LoRA-Patching.

</details>


### [51] [The Overlooked Value of Test-time Reference Sets in Visual Place Recognition](https://arxiv.org/abs/2510.03751)
*Mubariz Zaffar,Liangliang Nan,Sebastian Scherer,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: 本研究提出一种参考集微调（RSF）方法，通过在测试时可用的参考图像集上微调视觉定位识别（VPR）模型，以解决VPR在训练集与测试集领域差异较大的挑战性基准上的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉定位识别（VPR）方法在测试环境与训练环境差异较大的情况下仍然面临挑战，需要新的方法来弥合训练-测试领域鸿沟。

Method: 提出参考集微调（Reference-Set-Finetuning, RSF）方法，在测试时可用的参考图像集（即“地图”）上对VPR模型进行微调。

Result: RSF方法可以提高现有最先进（SOTA）VPR模型在挑战性VPR基准上的性能，平均Recall@1提升约2.3%，同时保持模型的泛化能力，并且在不同的测试数据集上均有效。

Conclusion: RSF是一种有效且通用的方法，通过利用测试时的参考集信息，可以显著提升VPR在领域迁移场景下的性能。

Abstract: Given a query image, Visual Place Recognition (VPR) is the task of retrieving
an image of the same place from a reference database with robustness to
viewpoint and appearance changes. Recent works show that some VPR benchmarks
are solved by methods using Vision-Foundation-Model backbones and trained on
large-scale and diverse VPR-specific datasets. Several benchmarks remain
challenging, particularly when the test environments differ significantly from
the usual VPR training datasets. We propose a complementary, unexplored source
of information to bridge the train-test domain gap, which can further improve
the performance of State-of-the-Art (SOTA) VPR methods on such challenging
benchmarks. Concretely, we identify that the test-time reference set, the
"map", contains images and poses of the target domain, and must be available
before the test-time query is received in several VPR applications. Therefore,
we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on
the map, boosting the SOTA (~2.3% increase on average for Recall@1) on these
challenging datasets. Finetuned models retain generalization, and RSF works
across diverse test datasets.

</details>


### [52] [Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization](https://arxiv.org/abs/2510.03763)
*Jiaxin Deng,Junbiao Pang*

Main category: cs.CV

TL;DR: 我们提出了一种名为ARSAM的新方法，通过自适应地采样、重用和混合分解后的梯度来加速SAM，从而在不牺牲模型泛化能力的情况下，将SAM的计算成本降低了约40%。


<details>
  <summary>Details</summary>
Motivation: SAM虽然能提高模型泛化能力，但计算成本是SGD的两倍。为了降低其计算成本，我们提出了一种加速SAM的方法。

Method: 我们首先发现SAM的梯度可以分解为SGD梯度和二阶梯度在**一阶**梯度上的投影（PSF）。我们还观察到SGD梯度和PSF在训练过程中动态演变，PSF在寻找平坦最小值中的作用日益重要。因此，我们提出ARSAM方法，通过重用和及时更新PSF来保持模型的泛化能力。

Result: 实验表明，ARSAM在CIFAR-10/100上实现了与SAM相当的准确率，同时速度提升了约40%。此外，ARSAM在人类姿势估计和模型量化等具有挑战性的任务上也能加速优化，而不会牺牲性能。

Conclusion: ARSAM通过有效分解和重用梯度，显著降低了SAM的计算成本，同时保持了其优越的泛化能力，并展示了其在多种任务上的广泛适用性。

Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doubles
the computational cost of Stochastic Gradient Descent (SGD) by requiring twice
the gradient calculations per optimization step. To mitigate this, we propose
Adaptively sampling-Reusing-mixing decomposed gradients to significantly
accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can
be decomposed into the SGD gradient and the Projection of the Second-order
gradient onto the First-order gradient (PSF). Furthermore, we observe that the
SGD gradient and PSF dynamically evolve during training, emphasizing the
growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed
to the reused PSF and the timely updated PSF still maintain the model's
generalization ability. Extensive experiments show that ARSAM achieves
state-of-the-art accuracies comparable to SAM across diverse network
architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a
speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various
challenge tasks (\textit{e.g.}, human pose estimation, and model quantization)
without sacrificing performance, demonstrating its broad practicality.% The
code is publicly accessible at: https://github.com/ajiaaa/ARSAM.

</details>


### [53] [CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis](https://arxiv.org/abs/2510.03767)
*Yiheng Dong,Yi Lin,Xin Yang*

Main category: cs.CV

TL;DR: CoPA框架通过融合多层概念表示来增强深度学习模型的透明度，提高了概念和疾病预测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于概念的方法在概念捕捉能力方面存在挑战，它们通常只依赖于最终层的特征，忽略了浅层和多尺度特征，并且在概念编码方面缺乏有效的指导，阻碍了细粒度概念的提取。

Method: 提出了一种名为CoPA（Concept Prompting and Aggregating）的新框架，该框架利用概念感知嵌入生成器（CEG）从视觉编码器的每一层提取概念表示，并将这些表示作为概念提示调优（CPT）的提示，以引导模型放大关键的概念相关视觉线索。最后，聚合来自每一层的视觉表示，使其与文本概念表示对齐。

Result: CoPA框架能够有效地捕捉和利用图像中有价值的概念信息，从而提高概念和疾病预测的性能。在三个公开数据集上的实验结果表明，CoPA的性能优于最先进的方法。

Conclusion: CoPA框架通过融合多层概念表示并利用提示引导，有效解决了现有方法在概念捕捉能力方面的不足，显著提升了模型的透明度和诊断性能。

Abstract: The transparency of deep learning models is essential for clinical
diagnostics. Concept Bottleneck Model provides clear decision-making processes
for diagnosis by transforming the latent space of black-box models into
human-understandable concepts. However, concept-based methods still face
challenges in concept capture capabilities. These methods often rely on encode
features solely from the final layer, neglecting shallow and multiscale
features, and lack effective guidance in concept encoding, hindering
fine-grained concept extraction. To address these issues, we introduce Concept
Prompting and Aggregating (CoPA), a novel framework designed to capture
multilayer concepts under prompt guidance. This framework utilizes the
Concept-aware Embedding Generator (CEG) to extract concept representations from
each layer of the visual encoder. Simultaneously, these representations serve
as prompts for Concept Prompt Tuning (CPT), steering the model towards
amplifying critical concept-related visual cues. Visual representations from
each layer are aggregated to align with textual concept representations. With
the proposed method, valuable concept-wise information in the images is
captured and utilized effectively, thus improving the performance of concept
and disease prediction. Extensive experimental results demonstrate that CoPA
outperforms state-of-the-art methods on three public datasets. Code is
available at https://github.com/yihengd/CoPA.

</details>


### [54] [MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation](https://arxiv.org/abs/2510.03786)
*T-Mai Bui,Fares Bougourzi,Fadi Dornaika,Vinh Truong Hoang*

Main category: cs.CV

TL;DR: 提出了一种混合分割架构，通过结合CNN、Transformer和基于Mamba的注意力融合（MAF）机制来捕捉局部、全局和长距离依赖关系，并使用多尺度注意力CNN解码器来重建细粒度分割图，同时利用共注意力门增强特征选择，在多个基准数据集上取得了优于现有技术的准确性和泛化能力，并保持了可比的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在医学图像分割任务中存在模型特异性强、跨模态和跨解剖区域性能不稳定、模型复杂度与性能平衡困难等问题，尤其是在对精度和效率都有要求的临床环境中。

Method: 提出了一种混合分割架构，包含一个三分支编码器（整合CNN、Transformer和MAF机制）用于捕捉不同尺度的依赖关系，一个多尺度注意力CNN解码器用于重建分割图，以及一个共注意力门用于在编码和解码过程中增强特征选择和跨尺度通信。

Result: 在多个基准数据集上的广泛实验表明，该方法在准确性和泛化能力方面优于最先进的方法，同时计算复杂度相当。

Conclusion: 该混合分割架构有效平衡了效率和效果，为多样化的医学成像任务提供了一个实用且可扩展的解决方案。

Abstract: In recent years, deep learning has shown near-expert performance in
segmenting complex medical tissues and tumors. However, existing models are
often task-specific, with performance varying across modalities and anatomical
regions. Balancing model complexity and performance remains challenging,
particularly in clinical settings where both accuracy and efficiency are
critical. To address these issues, we propose a hybrid segmentation
architecture featuring a three-branch encoder that integrates CNNs,
Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture
local, global, and long-range dependencies. A multi-scale attention-based CNN
decoder reconstructs fine-grained segmentation maps while preserving contextual
consistency. Additionally, a co-attention gate enhances feature selection by
emphasizing relevant spatial and semantic information across scales during both
encoding and decoding, improving feature interaction and cross-scale
communication. Extensive experiments on multiple benchmark datasets show that
our approach outperforms state-of-the-art methods in accuracy and
generalization, while maintaining comparable computational complexity. By
effectively balancing efficiency and effectiveness, our architecture offers a
practical and scalable solution for diverse medical imaging tasks. Source code
and trained models will be publicly released upon acceptance to support
reproducibility and further research.

</details>


### [55] [Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach](https://arxiv.org/abs/2510.03797)
*Rasel Hossen,Diptajoy Mistry,Mushiur Rahman,Waki As Sami Atikur Rahman Hridoy,Sajib Saha,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 提出了一种使用 YOLOv9 和多边形标注的深度学习方法，用于自动检测道路损坏和人孔盖，并在包含一千多张图像的新数据集上实现了 78.1% 的整体图像级准确率，其中 Broken 和 Not Broken 类别的 F1 分数分别为 86.7% 和 89.2%，但 Manhole 类别的 F1 分数仅为 18.2%，原因是类别不平衡。


<details>
  <summary>Details</summary>
Motivation: 手动监控道路损坏耗时、成本高且容易出错，因此需要一种更有效、可扩展的解决方案来监控城市基础设施，特别是在发展中国家。

Method: 使用 YOLOv9 算法和多边形标注（代替传统的边界框标注）来精确识别道路缺陷。构建了一个包含一千多张图像的新数据集，主要来自孟加拉国达卡，并用于训练一个 YOLO 模型，能够识别三种类别：Broken、Not Broken 和 Manhole。

Result: 在新建的数据集上，YOLOv9 模型实现了 78.1% 的整体图像级准确率。具体来说，Broken 类别的 F1 分数为 86.7%，Not Broken 类别的 F1 分数为 89.2%。然而，Manhole 类别的 F1 分数仅为 18.2%，这主要是由于数据集中类别不平衡造成的。

Conclusion: 该深度学习方法提供了一种高效、可扩展的城市基础设施监控解决方案，尤其适用于发展中国家。尽管在人孔盖检测方面存在挑战，但该方法在道路损坏检测方面表现出色。

Abstract: Urban safety and infrastructure maintenance are critical components of smart
city development. Manual monitoring of road damages is time-consuming, highly
costly, and error-prone. This paper presents a deep learning approach for
automated road damage and manhole detection using the YOLOv9 algorithm with
polygonal annotations. Unlike traditional bounding box annotation, we employ
polygonal annotations for more precise localization of road defects. We develop
a novel dataset comprising more than one thousand images which are mostly
collected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based
model for three classes, namely Broken, Not Broken, and Manhole. We achieve
78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong
performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)
classes, with challenges in Manhole detection (18.2% F1-score) due to class
imbalance. Our approach offers an efficient and scalable solution for
monitoring urban infrastructure in developing countries.

</details>


### [56] [Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2510.03821)
*Venkata Narendra Kotyada,Revanth Eranki,Nagesh Bhattu Sristy*

Main category: cs.CV

TL;DR: 本论文提出了一种结合了扩散模型和对比学习的无配对图像到图像翻译方法，名为Contrastive-SDE，在不依赖标签和额外训练的情况下，实现了更快的收敛速度和具有竞争力的翻译效果。


<details>
  <summary>Details</summary>
Motivation: 无配对图像到图像翻译在缺乏配对样本的情况下学习域间映射，而基于分数的扩散模型和对比学习在生成和特征学习方面表现出色，因此将两者结合起来有望解决此问题。

Method: 提出了一种时变对比学习方法，使用SimCLR将图像及其域不变特征视为正样本对，以保留域不变特征并去除域特定特征。然后，使用学习到的对比模型来指导预训练的SDE进行图像到图像的翻译。

Result: 在三个常见的无配对图像到图像翻译任务上，与多个基线方法进行了比较，并使用了四种评估指标。Constrastive-SDE在部分指标上取得了与现有技术水平相当的结果，并且收敛速度显著更快，无需标签监督或分类器训练。

Conclusion: Contrastive-SDE是一种更有效的无配对图像到图像翻译方法，它结合了扩散模型和对比学习的优势，能够在不使用标签监督的情况下实现快速收敛和有竞争力的翻译性能。

Abstract: Unpaired image-to-image translation involves learning mappings between source
domain and target domain in the absence of aligned or corresponding samples.
Score based diffusion models have demonstrated state-of-the-art performance in
generative tasks. Their ability to approximate complex data distributions
through stochastic differential equations (SDEs) enables them to generate
high-fidelity and diverse outputs, making them particularly well-suited for
unpaired I2I settings. In parallel, contrastive learning provides a powerful
framework for learning semantic similarities without the need for explicit
supervision or paired data. By pulling together representations of semantically
similar samples and pushing apart dissimilar ones, contrastive methods are
inherently aligned with the objectives of unpaired translation. Its ability to
selectively enforce semantic consistency at the feature level makes contrastive
learning particularly effective for guiding generation in unpaired scenarios.
In this work, we propose a time-dependent contrastive learning approach where a
model is trained with SimCLR by considering an image and its domain invarient
feature as a positive pair, enabling the preservation of domain-invariant
features and the discarding of domain-specific ones. The learned contrastive
model then guides the inference of a pretrained SDE for the I2I translation
task. We empirically compare Contrastive-SDE with several baselines across
three common unpaired I2I tasks, using four metrics for evaluation.
Constrastive-SDE achieves comparable results to the state-of-the-art on several
metrics. Furthermore, we observe that our model converges significantly faster
and requires no label supervision or classifier training, making it a more
efficient alternative for this task.

</details>


### [57] [LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization](https://arxiv.org/abs/2510.03827)
*Xueyang Zhou,Yangming Xu,Guiyao Tie,Yongchao Chen,Guowen Zhang,Duanfeng Chu,Pan Zhou,Lichao Sun*

Main category: cs.CV

TL;DR: LIBERO-PRO是一个改进的LIBERO基准，用于评估视觉-语言-动作（VLA）模型。它通过在操纵对象、初始状态、任务指令和环境等四个维度上引入合理的扰动，解决了现有LIBERO基准评估不准确的问题。实验表明，现有模型在标准LIBERO设置下表现良好（准确率>90%），但在LIBERO-PRO设置下准确率骤降至0%，揭示了模型依赖于死记硬背而非真正的任务理解。


<details>
  <summary>Details</summary>
Motivation: 现有LIBERO基准的训练和评估设置存在问题，导致性能估计过高，阻碍了公平的模型比较。需要一个更可靠的评估方法来衡量VLA模型的泛化能力。

Method: 引入LIBERO-PRO，一个扩展的LIBERO基准，系统地评估模型在四个维度（操纵对象、初始状态、任务指令、环境）的扰动下的性能。

Result: 现有模型在标准LIBERO评估下准确率超过90%，但在LIBERO-PRO设置下准确率降至0%。模型表现出对训练数据的死记硬背，而不是真正的任务理解或环境感知。例如，即使目标对象被替换，模型仍执行抓取动作；即使指令被篡改，模型输出也无变化。

Conclusion: 当前对VLA模型的评估方法存在严重缺陷，过度依赖死记硬背。研究者呼吁放弃误导性的方法，采用更稳健的评估方式来衡量模型的泛化能力和理解力。

Abstract: LIBERO has emerged as a widely adopted benchmark for evaluating
Vision-Language-Action (VLA) models; however, its current training and
evaluation settings are problematic, often leading to inflated performance
estimates and preventing fair model comparison. To address these issues, we
introduce LIBERO-PRO, an extended LIBERO benchmark that systematically
evaluates model performance under reasonable perturbations across four
dimensions: manipulated objects, initial states, task instructions, and
environments. Experimental results reveal that, although existing models
achieve over 90% accuracy under the standard LIBERO evaluation, their
performance collapses to 0.0% under our generalized setting. Crucially, this
discrepancy exposes the models' reliance on rote memorization of action
sequences and environment layouts from the training set, rather than genuine
task understanding or environmental perception. For instance, models persist in
executing grasping actions when the target object is replaced with irrelevant
items, and their outputs remain unchanged even when given corrupted
instructions or even messy tokens. These findings expose the severe flaws in
current evaluation practices, and we call on the community to abandon
misleading methodologies in favor of robust assessments of model generalization
and comprehension. Our code is available at:
https://github.com/Zxy-MLlab/LIBERO-PRO.

</details>


### [58] [Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models](https://arxiv.org/abs/2510.03840)
*Pranav Sharma,Shivank Garg,Durga Toshniwal*

Main category: cs.CV

TL;DR: Mirage数据集包含AI生成的图像，其中包含人眼可见但标准AI检测器难以识别的伪影。实验表明，大型视觉语言模型（LVLMs）能有效检测这些伪影，但在无伪影图像上表现下降。


<details>
  <summary>Details</summary>
Motivation: 开发能识别AI生成图像（即使是人眼可见伪影）的检测方法，并探索LVLMs在可解释AI图像检测中的应用。

Method: 构建Mirage数据集，包含具有可见伪影的AI生成图像。使用LVLMs在Mirage和现有基准数据集上进行图像检测实验。

Result: LVLMs在检测Mirage数据集中具有可见伪影的AI生成图像方面表现出色，但在检测无伪影图像时性能下降。

Conclusion: LVLMs可作为一种有效的AI图像检测工具，尤其擅长识别带有可见伪影的图像，但其在检测无明显伪影的图像时仍需改进。

Abstract: Recent advances in image generation models have led to models that produce
synthetic images that are increasingly difficult for standard AI detectors to
identify, even though they often remain distinguishable by humans. To identify
this discrepancy, we introduce \textbf{Mirage}, a curated dataset comprising a
diverse range of AI-generated images exhibiting visible artifacts, where
current state-of-the-art detection methods largely fail. Furthermore, we
investigate whether Large Vision-Language Models (LVLMs), which are
increasingly employed as substitutes for human judgment in various tasks, can
be leveraged for explainable AI image detection. Our experiments on both Mirage
and existing benchmark datasets demonstrate that while LVLMs are highly
effective at detecting AI-generated images with visible artifacts, their
performance declines when confronted with images lacking such cues.

</details>


### [59] [UGround: Towards Unified Visual Grounding with Unrolled Transformers](https://arxiv.org/abs/2510.03853)
*Rui Qian,Xin Yin,Chuanhang Deng,Zhiyuan Peng,Jian Xiong,Wei Zhai,Dejing Dou*

Main category: cs.CV

TL;DR: UGround是一个统一的视觉基础范式，通过动态选择“未展开”Transformer的中间层作为“掩码即提示”，解决了现有方法依赖固定最后隐藏层和使用<SEG>作为提示（缺乏显式空间线索）的问题。其核心是策略引导掩码（Policy-Prompted Masking），包括随机跳跃连接（SSC）和掩码即提示（MasP）。SSC利用强化学习策略动态选择层，MasP则使用<SEG>令牌与图像令牌的相似图作为软掩码提示视觉模型生成掩码。UGround首次在一个框架内统一了传统和新型的视觉推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础范式依赖固定最后隐藏层，导致错误累积且缺乏中间层校正；同时，使用<SEG>作为提示缺乏显式空间线索。UGround旨在解决这些问题。

Method: UGround采用策略引导掩码（Policy-Prompted Masking），包含随机跳跃连接（SSC）和掩码即提示（MasP）。SSC通过强化学习策略动态选择Transformer中间层，通过跳跃连接与视觉模型（如SAM）交互。MasP利用<SEG>令牌和图像令牌的相似度图作为软掩码，提示SAM生成掩码，提供显式空间线索。

Result: UGround在统一的视觉基础范式下，成功解决了从传统Referring Expression Segmentation到新型Reasoning Segmentation、单目标到多目标、正向查询到假前提（空目标）等多种任务。

Conclusion: UGround通过动态选择Transformer中间层作为“掩码即提示”，并引入策略引导掩码机制，有效解决了现有视觉基础范式中的挑战，并在统一的框架内实现了多种视觉基础任务的 SOTA 性能。

Abstract: We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm
that dynamically selects intermediate layers across \textbf{U}nrolled
transformers as ``mask as prompt'', diverging from the prevailing pipeline that
leverages the fixed last hidden layer as ``\texttt{<SEG>} as prompt''. UGround
addresses two primary challenges posed by the prevailing paradigm: (1) its
reliance on the fixed last hidden layer, which sequentially amplifies
cumulative errors arising from layer-by-layer propagation without intermediate
correction, and (2) its use of \texttt{<SEG>} as a prompt, which implicitly
projects textual embeddings into visual space without explicit spatial cues
(\eg, coordinates). Central to UGround is Policy-Prompted Masking, which
comprises two key components: Stochastic Skip Connection (SSC) and Mask as
Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic
sampling, allows each \texttt{<SEG>} token to slide across unrolled transformer
layers, enabling dynamic layer selection at which it connects to the vision
model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,
MasP uses the similarity map derived from the \texttt{<SEG>} token and image
tokens as a soft logit mask to prompt SAM for mask generation, offering
explicit spatial cues through its activation regions. To validate the
effectiveness of UGround, we, for the first time, have unified visual grounding
within a single framework from an attribute perspective, spanning from
traditional refer expression segmentation to newly proposed reasoning
segmentation, single-target to multi-target, positive query to false premise
(empty target). All codes and models are publicly available at
\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.

</details>


### [60] [Optimized Minimal 4D Gaussian Splatting](https://arxiv.org/abs/2510.03857)
*Minseo Lee,Byeonghyeon Lee,Lucas Yunkyu Lee,Eunsoo Lee,Sangmin Kim,Seunghyeon Song,Joo Chan Lee,Jong Hwan Ko,Jaesik Park,Eunbyung Park*

Main category: cs.CV

TL;DR: OMG4是一个优化4D高斯表示的框架，通过三阶段剪枝和隐式外观压缩，将模型尺寸减小60%以上，同时保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的4D高斯表示方法存在巨大的存储开销，而现有压缩方法在压缩率或视觉质量方面存在局限性。

Method: OMG4框架包含三个阶段：高斯采样、高斯剪枝和高斯融合。此外，还集成了隐式外观压缩和泛化的子向量量化（SVQ）。

Result: OMG4在标准数据集上的实验表明，与现有最先进的方法相比，模型尺寸减小了60%以上，同时保持了重建质量。

Conclusion: OMG4在紧凑的4D场景表示方面取得了显著进展，为广泛的应用开辟了新的可能性。

Abstract: 4D Gaussian Splatting has emerged as a new paradigm for dynamic scene
representation, enabling real-time rendering of scenes with complex motions.
However, it faces a major challenge of storage overhead, as millions of
Gaussians are required for high-fidelity reconstruction. While several studies
have attempted to alleviate this memory burden, they still face limitations in
compression ratio or visual quality. In this work, we present OMG4 (Optimized
Minimal 4D Gaussian Splatting), a framework that constructs a compact set of
salient Gaussians capable of faithfully representing 4D Gaussian models. Our
method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to
identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning
to remove redundancies, and (3) Gaussian Merging to fuse primitives with
similar characteristics. In addition, we integrate implicit appearance
compression and generalize Sub-Vector Quantization (SVQ) to 4D representations,
further reducing storage while preserving quality. Extensive experiments on
standard benchmark datasets demonstrate that OMG4 significantly outperforms
recent state-of-the-art methods, reducing model sizes by over 60% while
maintaining reconstruction quality. These results position OMG4 as a
significant step forward in compact 4D scene representation, opening new
possibilities for a wide range of applications. Our source code is available at
https://minshirley.github.io/OMG4/.

</details>


### [61] [Cross-View Open-Vocabulary Object Detection in Aerial Imagery](https://arxiv.org/abs/2510.03858)
*Jyoti Kini,Rohit Gupta,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种将地面图像的开放词汇表示适配到航空影像目标检测的新框架，通过结构化域对齐来解决跨域知识迁移的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测模型在固定类别的训练限制了其灵活性，难以加入新类别。开放词汇目标检测旨在解决此问题，允许模型识别未见过的类别。然而，直接从地面图像迁移知识到航空影像存在域偏移、视角变化和尺度差异等问题。

Method: 提出了一种新颖的框架，通过结构化域对齐将地面图像的开放词汇表示适配到航空影像目标检测。该方法引入了对比图像到图像对齐来增强航空和地面嵌入的相似性，并采用多实例词汇关联来将航空影像与文本嵌入对齐。

Result: 在DOTAv2上提高了+6.32 mAP，在VisDrone（图像）上提高了+4.16 mAP，在HRRSD上提高了+3.46 mAP，在零样本设置下，与微调的闭集词汇特定数据集模型性能相比。

Conclusion: 该方法通过结构化域对齐有效地解决了地面到航空影像的开放词汇目标检测的挑战，显著提高了在多个航空影像数据集上的性能，为航空应用中更灵活和可扩展的目标检测系统铺平了道路。

Abstract: Traditional object detection models are typically trained on a fixed set of
classes, limiting their flexibility and making it costly to incorporate new
categories. Open-vocabulary object detection addresses this limitation by
enabling models to identify unseen classes without explicit training.
Leveraging pretrained models contrastively trained on abundantly available
ground-view image-text classification pairs provides a strong foundation for
open-vocabulary object detection in aerial imagery. Domain shifts, viewpoint
variations, and extreme scale differences make direct knowledge transfer across
domains ineffective, requiring specialized adaptation strategies. In this
paper, we propose a novel framework for adapting open-vocabulary
representations from ground-view images to solve object detection in aerial
imagery through structured domain alignment. The method introduces contrastive
image-to-image alignment to enhance the similarity between aerial and
ground-view embeddings and employs multi-instance vocabulary associations to
align aerial images with text embeddings. Extensive experiments on the xView,
DOTAv2, VisDrone, DIOR, and HRRSD datasets are used to validate our approach.
Our open-vocabulary model achieves improvements of +6.32 mAP on DOTAv2, +4.16
mAP on VisDrone (Images), and +3.46 mAP on HRRSD in the zero-shot setting when
compared to finetuned closed-vocabulary dataset-specific model performance,
thus paving the way for more flexible and scalable object detection systems in
aerial applications.

</details>


### [62] [Exploring the Challenge and Value of Deep Learning in Automated Skin Disease Diagnosis](https://arxiv.org/abs/2510.03869)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: 深度学习在皮肤癌诊断中的应用与挑战


<details>
  <summary>Details</summary>
Motivation: 皮肤癌的普遍性和致命性强调了早期检测和诊断的重要性。深度学习（DL）在自动化皮肤病诊断方面展现出巨大潜力，尤其是在皮肤病变检测、评估和分类方面。

Method: 本文基于PRISMA框架，通过综合最近研究，讨论了数据增强、混合模型和特征融合等应对数据不平衡、类内变异、类间相似性和图像噪声等挑战的创新方法。此外，还探讨了DL模型与临床工作流程的整合。

Result: 深度学习在提高皮肤病诊断的准确性和效率方面显示出巨大潜力，但仍面临复杂特征、图像噪声、类内变异、类间相似性和数据不平衡等挑战。

Conclusion: 深度学习有潜力彻底改变皮肤病诊断，改善临床决策，但需要持续的进步才能在皮肤病护理中充分发挥其潜力。

Abstract: Skin cancer is one of the most prevalent and deadly forms of cancer
worldwide, which highlights the critical importance of early detection and
diagnosis in improving patient outcomes. Deep learning (DL) has shown
significant promise in enhancing the accuracy and efficiency of automated skin
disease diagnosis, particularly in detecting and evaluating skin lesions and
classification. However, there are still several challenges for DL-based skin
cancer diagnosis, including complex features, image noise, intra-class
variation, inter-class similarity, and data imbalance. By synthesizing recent
research, this review discusses innovative approaches to cope with these
challenges, such as data augmentation, hybrid models, and feature fusion, etc.
Furthermore, the review highlights the integration of DL models into clinical
workflows, offering insights into the potential of deep learning to
revolutionize skin disease diagnosis and improve clinical decision-making. This
article follows a comprehensive methodology based on the PRISMA framework and
emphasizes the need for continued advancements to fully unlock the
transformative potential of DL in dermatological care.

</details>


### [63] [SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/2510.03870)
*Nikolaos Kaparinos,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 提出了一种名为SDAKD的新型GAN蒸馏方法，通过引入学生判别器来解决学生发电机和教师判别器之间的容量不匹配问题，以实现资源受限设备上的GAN压缩。


<details>
  <summary>Details</summary>
Motivation: 现有的GAN压缩方法（如知识蒸馏）在训练小型学生发电机时面临学生发电机和教师判别器之间容量不匹配的挑战，限制了其在资源受限设备上的部署。

Method: SDAKD采用三阶段训练策略，并在最后两个阶段集成自适应特征图蒸馏方法，引入学生判别器来缓解容量不匹配问题。

Result: 在GCFSR和Real-ESRGAN两个超分辨率GAN模型上进行了评估，实验结果表明SDAKD在性能上优于基线和SOTA GAN知识蒸馏方法。

Conclusion: SDAKD是一种有效的GAN蒸馏方法，通过引入学生判别器解决了容量不匹配问题，并在超分辨率任务上取得了优于现有方法的性能。

Abstract: Generative Adversarial Networks (GANs) achieve excellent performance in
generative tasks, such as image super-resolution, but their computational
requirements make difficult their deployment on resource-constrained devices.
While knowledge distillation is a promising research direction for GAN
compression, effectively training a smaller student generator is challenging
due to the capacity mismatch between the student generator and the teacher
discriminator. In this work, we propose Student Discriminator Assisted
Knowledge Distillation (SDAKD), a novel GAN distillation methodology that
introduces a student discriminator to mitigate this capacity mismatch. SDAKD
follows a three-stage training strategy, and integrates an adapted feature map
distillation approach in its last two training stages. We evaluated SDAKD on
two well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our
experiments demonstrate consistent improvements over the baselines and SOTA GAN
knowledge distillation methods. The SDAKD source code will be made openly
available upon acceptance of the paper.

</details>


### [64] [PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis](https://arxiv.org/abs/2510.03873)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: 该研究提出了PoseGaze-AHP数据集，用于同步捕捉头部姿势和注视运动信息，以辅助诊断眼源性异常头位（AHP）。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在头部姿势和眼球运动方面是分开的，阻碍了集成诊断方法和人工智能在AHP分析中的应用。

Method: 利用大型语言模型（LLMs）和Claude 3.5 Sonnet模型，通过迭代的、分步的、层级的和复杂的提示策略，从医学文献中提取临床数据。使用神经头部化身（NHA）框架将提取的记录进行插补和3D转换。

Result: 生成了包含7,920张图像的数据集，涵盖了两种头部纹理和广泛的眼科疾病。提取方法的总体准确率为91.92%。

Conclusion: PoseGaze-AHP是首个公开可用的人工智能驱动的眼源性AHP诊断数据集，有助于开发准确且符合隐私的诊断工具。

Abstract: Diagnosing ocular-induced abnormal head posture (AHP) requires a
comprehensive analysis of both head pose and ocular movements. However,
existing datasets focus on these aspects separately, limiting the development
of integrated diagnostic approaches and restricting AI-driven advancements in
AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D
dataset that synchronously captures head pose and gaze movement information for
ocular-induced AHP assessment. Structured clinical data were extracted from
medical literature using large language models (LLMs) through an iterative
process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and
complex prompting strategies. The extracted records were systematically imputed
and transformed into 3D representations using the Neural Head Avatar (NHA)
framework. The dataset includes 7,920 images generated from two head textures,
covering a broad spectrum of ocular conditions. The extraction method achieved
an overall accuracy of 91.92%, demonstrating its reliability for clinical
dataset construction. PoseGaze-AHP is the first publicly available resource
tailored for AI-driven ocular-induced AHP diagnosis, supporting the development
of accurate and privacy-compliant diagnostic tools.

</details>


### [65] [DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human](https://arxiv.org/abs/2510.03874)
*Yunhao Li,Sijing Wu,Yucheng Zhu,Huiyu Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 该论文提出了一种用于评估动态4D数字人质量的方法，并发布了一个名为DHQA-4D的大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 由于3D扫描和重建技术的发展，动态4D数字人应用广泛，但采集、压缩和传输过程中的噪声会降低其质量，因此对动态4D数字人进行质量评估非常重要。

Method: 1. 创建了一个包含32个高质量4D真人网格序列和1920个包含11种纹理失真的4D数字人网格的大规模数据集DHQA-4D，并提供了相应的有纹理和无纹理的平均意见得分（MOS）。 2. 分析了不同类型的失真对有纹理和无纹理的4D数字人网格的人类感知影响。 3. 提出了DynaMesh-Rater，一种基于大型多模态模型（LMM）的方法，能够同时评估有纹理和无纹理的4D网格。 4. DynaMesh-Rater提取了来自2D视频的视觉特征、视频片段的运动特征以及4D网格的几何特征。 5. 利用LMM模型整合这些特征，并通过基于LoRA的指令调优技术来预测质量得分。

Result: 在DHQA-4D数据集上的大量实验结果表明，DynaMesh-Rater方法的性能优于先前的方法。

Conclusion: DynaMesh-Rater在DHQA-4D数据集上表现出优越性，能够有效评估动态4D数字人的质量。

Abstract: With the rapid development of 3D scanning and reconstruction technologies,
dynamic digital human avatars based on 4D meshes have become increasingly
popular. A high-precision dynamic digital human avatar can be applied to
various fields such as game production, animation generation, and remote
immersive communication. However, these 4D human avatar meshes are prone to
being degraded by various types of noise during the processes of collection,
compression, and transmission, thereby affecting the viewing experience of
users. In light of this fact, quality assessment of dynamic 4D digital humans
becomes increasingly important. In this paper, we first propose a large-scale
dynamic digital human quality assessment dataset, DHQA-4D, which contains 32
high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D
human meshes degraded by 11 textured distortions, as well as their
corresponding textured and non-textured mean opinion scores (MOSs). Equipped
with DHQA-4D dataset, we analyze the influence of different types of distortion
on human perception for textured dynamic 4D meshes and non-textured dynamic 4D
meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model
(LMM) based approach that is able to assess both textured 4D meshes and
non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts
multi-dimensional features, including visual features from a projected 2D
video, motion features from cropped video clips, and geometry features from the
4D human mesh to provide comprehensive quality-related information. Then we
utilize a LMM model to integrate the multi-dimensional features and conduct a
LoRA-based instruction tuning technique to teach the LMM model to predict the
quality scores. Extensive experimental results on the DHQA-4D dataset
demonstrate the superiority of our DynaMesh-Rater method over previous quality
assessment methods.

</details>


### [66] [Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert](https://arxiv.org/abs/2510.03896)
*Mingyu Liu,Zheng Huang,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Yating Wang,Haoyi Zhu,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 现有的视觉语言模型（VLM）在物理世界中的应用面临挑战，传统的视觉-语言-动作（VLA）模型泛化能力差，而改进的双系统方法又受限于语义模糊性。本文提出了一种新框架，使用可泛化的动作专家和稀疏3D轨迹作为中间表示，将VLM的高层规划能力与低层动作模块连接起来，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的VLA模型泛化能力差，双系统方法存在语义模糊性，并且在部署到新环境时需要针对新数据进行微调，但本文提出的新框架解决了这些问题。

Method: 本文提出了一种新框架，该框架以可泛化的动作专家为中心，利用稀疏3D轨迹作为中间表示，将VLM的高层规划能力与低层动作模块连接起来。在规划阶段，VLM仅生成粗略的3D航点，然后由可泛化的动作专家进行处理，通过对环境的实时点云观测进行采样，将其精炼为密集的、可执行的动作序列。为了提高训练效率和泛化能力，本文还提出了一种新颖的“动作预训练、点云微调”范式。

Result: 本文提出的新框架结合了VLM在视觉理解和规划方面的广泛泛化能力以及动作专家在细粒度、动作级别的泛化能力。

Conclusion: 本文首次提出了一个以可泛化的动作专家为中心的框架，通过稀疏3D轨迹作为中间表示，有效地弥合了VLM的高层规划能力与低层物理动作模块之间的差距，并提出了一种新颖的“动作预训练、点云微调”范式，以提高训练效率和泛化鲁棒性。

Abstract: Although Vision-Language Models (VLM) have demonstrated impressive planning
and reasoning capabilities, translating these abilities into the physical world
introduces significant challenges. Conventional Vision-Language-Action (VLA)
models, which integrate reasoning and action into a monolithic architecture,
generalize poorly because they are constrained by scarce, narrow-domain data.
While recent dual-system approaches attempt to decouple "thinking" from
"acting", they are often constrained by semantic ambiguities within the action
module. This ambiguity makes large-scale, cross-task training infeasible.
Consequently, these systems typically necessitate fine-tuning on newly
collected data when deployed to novel environments, and the cooperation
mechanism between the two systems remains ill-defined. To address these
limitations, we introduce, for the first time, a framework centered around a
generalizable action expert. Our approach utilizes sparse 3D trajectories as an
intermediate representation, effectively bridging the high-level planning
capabilities of the VLM with the low-level physical action module. During the
planning phase, the VLM is only required to generate coarse 3D waypoints. These
waypoints are then processed by our generalizable action expert, which refines
them into dense, executable action sequences by sampling real-time point cloud
observations of the environment. To promote training efficiency and robust
generalization, we introduce a novel "Action Pre-training, Pointcloud
Fine-tuning" paradigm. Our method combines the broad generalization
capabilities of VLMs in visual understanding and planning with the
fine-grained, action-level generalization of action expert.

</details>


### [67] [Skin Lesion Classification Based on ResNet-50 Enhanced With Adaptive Spatial Feature Fusion](https://arxiv.org/abs/2510.03876)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: 提出一种改进的ResNet-50模型（ASFF-ResNet-50），通过自适应空间特征融合（ASFF）增强，以提高皮肤癌分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于皮肤镜图像中类间相似性高、类内变异性大以及图像噪声等问题，皮肤癌分类仍然是一个挑战。

Method: 提出一种改进的ResNet-50模型，结合自适应空间特征融合（ASFF）模块。该模块采用双分支设计，融合了高级语义特征和中级细节特征，并通过全局平均池化和全连接层生成自适应权重，实现加权融合，以增强特征学习并减少噪声影响。

Result: 在ISIC 2020数据集的子集上进行评估，ASFF-ResNet-50模型在准确率、精确率、召回率、F1分数、P-R曲线（AUC 0.9670）和ROC曲线（AUC 0.9717）方面均优于5个经典CNN模型，准确率达到93.18%。Grad-CAM可视化结果表明模型能自适应地关注病灶相关区域。

Conclusion: 所提出的ASFF-ResNet-50模型为计算机辅助皮肤癌诊断提供了一种更有效、更高效的解决方案。

Abstract: Skin cancer classification remains a challenging problem due to high
inter-class similarity, intra-class variability, and image noise in dermoscopic
images. To address these issues, we propose an improved ResNet-50 model
enhanced with Adaptive Spatial Feature Fusion (ASFF), which adaptively
integrates multi-scale semantic and surface features to improve feature
representation and reduce overfitting. The ResNet-50 model is enhanced with an
adaptive feature fusion mechanism to achieve more effective multi-scale feature
extraction and improve overall performance. Specifically, a dual-branch design
fuses high-level semantic and mid-level detail features, which are processed
through global average pooling and fully connected layers to generate adaptive
weights for weighted fusion, thereby strengthening feature learning and
reducing the impact of noise on classification. The method is evaluated on a
subset of the ISIC 2020 dataset containing 3297 benign and malignant skin
lesion images. Experimental results show that the proposed ASFF-based ResNet-50
achieves the best overall performance compared with 5 classic convolutional
neural networks (CNNs) models. The proposed model reached an accuracy of 93.18%
along with higher precision, recall, specificity, and F1 score. The improved
model achieves an AUC value of 0.9670 and 0.9717 in the P-R and ROC curve,
respectively. Then, the evaluation based on Grad-CAM further proved that the
improved model adaptively focuses on lesion-relevant regions while suppressing
irrelevant background information, thereby validating its enhanced feature
learning capability from a deep representation perspective. These findings
demonstrate that the proposed approach provides a more effective and efficient
solution for computer-aided skin cancer diagnosis.

</details>


### [68] [Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks](https://arxiv.org/abs/2510.03878)
*Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R*

Main category: cs.CV

TL;DR: 该研究开发了一个多模态深度学习框架，通过整合临床、影像学和病理学图像，旨在提高口腔鳞状细胞癌（OSCC）的早期诊断率。


<details>
  <summary>Details</summary>
Motivation: 晚期诊断是导致OSCC高死亡率的主要原因，本研究旨在通过开发多模态深度学习框架来改善OSCC的早期检测。

Method: 研究人员使用公开数据集，对临床、影像学和病理学图像分别训练了DenseNet-121卷积神经网络模型，并采用了迁移学习、数据增强和特定模态预处理。最后，使用验证加权的集成策略融合了各模态的预测结果。

Result: 影像学和病理学模态取得了较高的验证准确率（分别为100%和95.12%），而临床图像由于视觉异质性表现稍低（63.10%）。最终的多模态集成模型在包含55个样本的验证数据集上达到了84.58%的整体准确率。

Conclusion: 研究提出的多模态集成框架能够作为一种非侵入性的、人工智能辅助的分类工具，提高高风险病灶的早期识别能力，弥补了当前诊断流程的不足，有助于减少诊断延迟，改善患者预后。

Abstract: Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes
significantly to its high global mortality rate, with over 50\% of cases
detected at advanced stages and a 5-year survival rate below 50\% according to
WHO statistics. This study aims to improve early detection of OSCC by
developing a multimodal deep learning framework that integrates clinical,
radiological, and histopathological images using a weighted ensemble of
DenseNet-121 convolutional neural networks (CNNs). Material and Methods A
retrospective study was conducted using publicly available datasets
representing three distinct medical imaging modalities. Each modality-specific
dataset was used to train a DenseNet-121 CNN via transfer learning.
Augmentation and modality-specific preprocessing were applied to increase
robustness. Predictions were fused using a validation-weighted ensemble
strategy. Evaluation was performed using accuracy, precision, recall, F1-score.
Results High validation accuracy was achieved for radiological (100\%) and
histopathological (95.12\%) modalities, with clinical images performing lower
(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improved
diagnostic robustness with an overall accuracy of 84.58\% on a multimodal
validation dataset of 55 samples. Conclusion The multimodal ensemble framework
bridges gaps in the current diagnostic workflow by offering a non-invasive,
AI-assisted triage tool that enhances early identification of high-risk
lesions. It supports clinicians in decision-making, aligning with global
oncology guidelines to reduce diagnostic delays and improve patient outcomes.

</details>


### [69] [Exploring Instruction Data Quality for Explainable Image Quality Assessment](https://arxiv.org/abs/2510.03880)
*Yunhao Li,Sijing Wu,Huiyu Duan,Yucheng Zhu,Qi Jia,Guangtao Zhai*

Main category: cs.CV

TL;DR: 通过数据筛选和聚类方法，在保证模型性能的同时，大幅降低了微调成本。


<details>
  <summary>Details</summary>
Motivation: 当前基于多模态大模型（MLLM）的可解释图像质量评估（IQA）方法依赖大规模指令调优数据集，但数据量过大会导致计算成本高昂且可能损害模型性能。因此，本文旨在研究数据质量而非数量对可解释IQA模型性能的影响，并提出一种高效的数据选择方法。

Method: 本文首先探究了不同数据量对模型性能的影响，发现随机采样的方法就能取得比使用全部数据更好的结果。在此基础上，提出了一种包含聚类特征提取、聚类配额分配和聚类采样策略的聚类数据选择框架，并提出了一种名为IQA-Select的数据选择方法。

Result: 实验结果表明，IQA-Select在Q-Bench和AesBench基准上，仅使用10%的数据就能达到全量微调的102.1%和103.7%的性能，有效降低了计算成本并提升了性能。

Conclusion: 本文挑战了“数据量越大越好”的扩展定律，证明了通过高质量的数据筛选（如IQA-Select）可以显著降低可解释IQA模型的训练成本，并可能获得更好的性能。

Abstract: In recent years, with the rapid development of powerful multimodal large
language models (MLLMs), explainable image quality assessment (IQA) has
gradually become popular, aiming at providing quality-related descriptions and
answers of images. To achieve this goal, recent methods seek to construct a
large-scale instruction tuning dataset to empower the MLLM with quality
perception ability following the well-known scaling law. However, a large
amount of instruction tuning data may cause substantial computational costs and
redundant data, which in turn will cause harm to the performance of the model.
To cope with this problem, in this paper, we challenge the scaling law and
systematically investigate the role of data quality of the instruction tuning
dataset for explainable IQA. Using a powerful pre-trained MLLM, we first
investigate the changes in model performance after fine-tuning with different
sizes of instruction tuning data. We find that selecting a subset of the data
set randomly using an appropriate ratio can even lead to better results than
training with the entire instruction tuning dataset, demonstrating the
redundancy of current explainable IQA instruction tuning data. Beyond randomly
sampling a subset, we propose a clustering-based data selection framework with
three stages: clustering feature extraction, cluster quota allocation, and
cluster sampling strategy. Then we systematically analyze the choices of each
stage and propose a simple but efficient data selection method IQA-Select for
explainable IQA. The experimental results demonstrate that IQA-Select can
achieve 102.1% and 103.7% performance of full fine-tuning using only 10%
selected data in Q-Bench and AesBench respectively, significantly reducing
computational costs while achieving better performance.

</details>


### [70] [RAP: 3D Rasterization Augmented End-to-End Planning](https://arxiv.org/abs/2510.04333)
*Lan Feng,Yang Gao,Eloi Zablocki,Quanyi Li,Wuyang Li,Sichao Liu,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Imitation learning for end-to-end driving trains policies only on expert
demonstrations. Once deployed in a closed loop, such policies lack recovery
data: small mistakes cannot be corrected and quickly compound into failures. A
promising direction is to generate alternative viewpoints and trajectories
beyond the logged path. Prior work explores photorealistic digital twins via
neural rendering or game engines, but these methods are prohibitively slow and
costly, and thus mainly used for evaluation. In this work, we argue that
photorealism is unnecessary for training end-to-end planners. What matters is
semantic fidelity and scalability: driving depends on geometry and dynamics,
not textures or lighting. Motivated by this, we propose 3D Rasterization, which
replaces costly rendering with lightweight rasterization of annotated
primitives, enabling augmentations such as counterfactual recovery maneuvers
and cross-agent view synthesis. To transfer these synthetic views effectively
to real-world deployment, we introduce a Raster-to-Real feature-space alignment
that bridges the sim-to-real gap. Together, these components form Rasterization
Augmented Planning (RAP), a scalable data augmentation pipeline for planning.
RAP achieves state-of-the-art closed-loop robustness and long-tail
generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo
Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that
lightweight rasterization with feature alignment suffices to scale E2E
training, offering a practical alternative to photorealistic rendering. Project
page: https://alan-lanfeng.github.io/RAP/.

</details>


### [71] [Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models](https://arxiv.org/abs/2510.03903)
*Md. Atabuzzaman,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: LVLM可以通过视觉问答范式进行零样本细粒度图像分类，并取得了SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: LVLM在细粒度图像分类中的潜力尚未被充分发掘，该任务需要精确区分视觉上相似的类别。

Method: 将零样本细粒度图像分类转化为视觉问答框架，并采用新颖的注意力干预技术，同时开发更全面、更精确的类别描述基准。

Result: 提出的方法在多个细粒度图像分类基准上持续优于SOTA方法。

Conclusion: LVLM在零样本细粒度图像分类任务中具有广泛的应用潜力。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance
on vision-language reasoning tasks. However, their potential for zero-shot
fine-grained image classification, a challenging task requiring precise
differentiation between visually similar categories, remains underexplored. We
present a novel method that transforms zero-shot fine-grained image
classification into a visual question-answering framework, leveraging LVLMs'
comprehensive understanding capabilities rather than relying on direct class
name generation. We enhance model performance through a novel attention
intervention technique. We also address a key limitation in existing datasets
by developing more comprehensive and precise class description benchmarks. We
validate the effectiveness of our method through extensive experimentation
across multiple fine-grained image classification benchmarks. Our proposed
method consistently outperforms the current state-of-the-art (SOTA) approach,
demonstrating both the effectiveness of our method and the broader potential of
LVLMs for zero-shot fine-grained classification tasks. Code and Datasets:
https://github.com/Atabuzzaman/Fine-grained-classification

</details>


### [72] [From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance](https://arxiv.org/abs/2510.03906)
*Ardalan Aryashad,Parsa Razmara,Amin Mahjoub,Seyedarmin Azizi,Mahdi Salmani,Arad Firouzkouhi*

Main category: cs.CV

TL;DR: 该研究通过在Foggy Cityscapes数据集上对各种去雾方法（包括经典滤波器、现代去雾网络、链式方法和视觉-语言模型）进行全面的实证评估，以解决自动驾驶感知系统在雾天性能下降的问题。研究结果表明，并非所有去雾方法都能提高下游任务（如物体检测和分割）的性能，并评估了视觉-语言模型在去雾任务中的表现及其与传统指标的相关性，最终为雾天去雾方法提供了一个面向任务的基准。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶感知系统在雾天性能会下降，现有去雾方法在图像保真度和下游任务性能之间存在不一致性，且缺乏在真实数据集上的评估。本研究旨在解决这些问题，为去雾方法提供一个透明、面向任务的基准。

Method: 本研究评估了多种去雾方法，包括经典滤波器、现代去雾网络、链式方法（滤波器后接模型，模型后接滤波器）以及直接应用于雾天图像的提示驱动视觉-语言模型（VLM）。在Foggy Cityscapes数据集上，研究评估了图像质量、物体检测（mAP）和分割（PQ, RQ, SQ）的下游性能。此外，还评估了VLM裁判的定性评分与任务指标的一致性。

Result: 研究结果揭示了去雾方法何时能提升性能，链式方法是协同增效还是性能退化，以及VLM编辑方法与专用方法的比较。VLM裁判的定性评分与mAP表现出很强的一致性。

Conclusion: 本研究建立了一个透明的、面向任务的去雾方法基准，并强调了在恶劣天气条件下，预处理技术在多大程度上能真正提升自动驾驶感知能力。

Abstract: Autonomous driving perception systems are particularly vulnerable in foggy
conditions, where light scattering reduces contrast and obscures fine details
critical for safe operation. While numerous defogging methods exist-from
handcrafted filters to learned restoration models-improvements in image
fidelity do not consistently translate into better downstream detection and
segmentation. Moreover, prior evaluations often rely on synthetic data, leaving
questions about real-world transferability. We present a structured empirical
study that benchmarks a comprehensive set of pipelines, including (i) classical
filters, (ii) modern defogging networks, (iii) chained variants
(filter$\rightarrow$model, model$\rightarrow$filter), and (iv) prompt-driven
visual--language image editing models (VLM) applied directly to foggy images.
Using Foggy Cityscapes, we assess both image quality and downstream performance
on object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals
when defogging helps, when chaining yields synergy or degradation, and how
VLM-based editors compare to dedicated approaches. In addition, we evaluate
qualitative rubric-based scores from a VLM judge and quantify their alignment
with task metrics, showing strong correlations with mAP. Together, these
results establish a transparent, task-oriented benchmark for defogging methods
and highlight the conditions under which preprocessing genuinely improves
autonomous perception in adverse weather.

</details>


### [73] [Generating Human Motion Videos using a Cascaded Text-to-Video Framework](https://arxiv.org/abs/2510.03909)
*Hyelin Nam,Hyojun Go,Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung*

Main category: cs.CV

TL;DR: CAMEO是一个用于通用人类运动视频生成的级联框架，它结合了文本到运动（T2M）模型和条件视频扩散模型（VDMs），通过优化的组件缓解了训练和推理过程中的次优因素。


<details>
  <summary>Details</summary>
Motivation: 通用人类视频生成在图形、娱乐和具身AI领域具有广泛的应用前景，但现有视频扩散模型（VDMs）在通用人类视频生成方面的探索有限，通常局限于图生视频或特定领域（如舞蹈视频）。

Method: CAMEO框架通过精心设计的组件，包括分析和准备文本提示与视觉条件以有效训练VDM，以及引入一个连接文本到运动（T2M）和VDM阶段的、能根据文本自动选择视角的相机感知条件模块，来缓解T2M到条件VDM过程中的次优因素。

Result: CAMEO框架在MovieGen基准和新提出的T2M-VDM组合的基准上都证明了其有效性，并展示了其在各种用例中的多功能性。

Conclusion: CAMEO框架通过整合T2M模型和条件VDMs，并引入相机感知条件模块，成功实现了通用人类运动视频的生成，并在多个基准和应用中展现了其有效性和多功能性。

Abstract: Human video generation is becoming an increasingly important task with broad
applications in graphics, entertainment, and embodied AI. Despite the rapid
progress of video diffusion models (VDMs), their use for general-purpose human
video generation remains underexplored, with most works constrained to
image-to-video setups or narrow domains like dance videos. In this work, we
propose CAMEO, a cascaded framework for general human motion video generation.
It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,
mitigating suboptimal factors that may arise in this process across both
training and inference through carefully designed components. Specifically, we
analyze and prepare both textual prompts and visual conditions to effectively
train the VDM, ensuring robust alignment between motion descriptions,
conditioning signals, and the generated videos. Furthermore, we introduce a
camera-aware conditioning module that connects the two stages, automatically
selecting viewpoints aligned with the input text to enhance coherence and
reduce manual intervention. We demonstrate the effectiveness of our approach on
both the MovieGen benchmark and a newly introduced benchmark tailored to the
T2M-VDM combination, while highlighting its versatility across diverse use
cases.

</details>


### [74] [Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition](https://arxiv.org/abs/2510.03921)
*Arushi Dashore,Aryan Anumala,Emily Hui,Olivia Yang*

Main category: cs.CV

TL;DR: 该研究提出了一种结合生物力学和深度学习的新框架，用于网球挥杆分析，旨在生成可操作的反馈。


<details>
  <summary>Details</summary>
Motivation: 现有系统难以将生物力学见解转化为易于理解和有意义的反馈，该研究旨在解决此问题。

Method: 使用基于CNN-LSTM的模型提取关键生物力学特征，并利用大型语言模型生成反馈。

Result: 实验评估了该框架在分类性能和可解释性方面的效果。

Conclusion: 该框架弥合了可解释人工智能与运动生物力学之间的差距，能够生成技术上准确、符合生物力学原理且可操作的反馈。

Abstract: Automated tennis stroke analysis has advanced significantly with the
integration of biomechanical motion cues alongside deep learning techniques,
enhancing stroke classification accuracy and player performance evaluation.
Despite these advancements, existing systems often fail to connect
biomechanical insights with actionable language feedback that is both
accessible and meaningful to players and coaches. This research project
addresses this gap by developing a novel framework that extracts key
biomechanical features (such as joint angles, limb velocities, and kinetic
chain patterns) from motion data using Convolutional Neural Network Long
Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for
relationships influencing stroke effectiveness and injury risk, forming the
basis for feedback generation using large language models (LLMs). Leveraging
the THETIS dataset and feature extraction techniques, our approach aims to
produce feedback that is technically accurate, biomechanically grounded, and
actionable for end-users. The experimental setup evaluates this framework on
classification performance and interpretability, bridging the gap between
explainable AI and sports biomechanics.

</details>


### [75] [Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs](https://arxiv.org/abs/2510.03955)
*Sameep Vani,Shreyas Jena,Maitreya Patel,Chitta Baral,Somak Aditya,Yezhou Yang*

Main category: cs.CV

TL;DR: 该研究提出了TimeWarp系统，通过生成包含细微时间动态的合成数据集来改进视频语言模型（Video-LLMs）在时间理解任务上的表现，解决了现有数据集的不足，并显著提高了模型在多个基准测试上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频语言模型（Video-LLMs）在需要细粒度时间理解的任务上表现不佳，因为当前的微调数据集缺乏视觉复杂性和时间细微差别，导致模型过度依赖语言推理而非真正的视频动态理解。

Method: 提出TimeWarp系统，用于创建目标合成时间数据集，以微调模型，使其更关注输入的视频内容。利用TimeWarp创建了一个大规模偏好数据集，该数据集捕捉了常被忽略的复杂时间动态，使模型的响应更基于视觉和时间信息。

Result: 将TimeWarp方法应用于现有模型后，在时间理解基准测试上的性能显著提高，在七个基准测试上实现了绝对性能提升。

Conclusion: 所提出的数据集通过鼓励模型关注视觉和时间信息，能有效提升视频语言模型（Video-LLMs）在时间理解任务上的能力。

Abstract: While Video Large Language Models (Video-LLMs) have demonstrated remarkable
performance across general video understanding benchmarks-particularly in video
captioning and descriptive tasks-they consistently underperform on tasks that
require fine-grained temporal understanding. This limitation arises due to the
lack of visual complexity and temporal nuance in current fine-tuning datasets,
leading these models to rely heavily on language-based reasoning rather than
truly understanding video dynamics. In this work, we propose TimeWarp, a
systematic method to create a targeted synthetic temporal dataset to fine-tune
the model's responses to encourage it to focus on the given input video. We
introduce a large-scale preference dataset, created using TimeWarp, that
captures intricate temporal dynamics often overlooked, grounding the model's
responses to visual and temporal information. We demonstrate that when our
method is applied to existing models, it significantly improves performance on
temporal understanding benchmarks, highlighting the effectiveness of our
proposed datasets in advancing temporal understanding in Video-LLMs, resulting
in an absolute improvement in performance across seven benchmarks. Code is
available at https://github.com/sameepv21/timewarp.

</details>


### [76] [No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models](https://arxiv.org/abs/2510.03978)
*Min Woo Sun,Alejandro Lozano,Javier Gamazo Tejero,Vishwesh Nath,Xiao Xiao Sun,James Burgess,Yuhui Zhang,Kun Yuan,Robert Tibshirani,Sean Huver,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 长上下文视觉-语言模型（VLMs）在生物医学领域表现更优，能有效处理长文本描述，提升检索和分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（VLMs）通常使用较短的文本窗口进行预训练，导致长篇幅的生物医学图像描述被截断，无法充分利用其中的信息。然而，生物医学文献中的长描述占有很大比例，这构成了信息损失。本研究旨在探索长文本预训练对生物医学VLMs的影响，并提出相应的解决方案。

Method: 本研究通过扩展VLMs中文本编码器的上下文长度，探究了长文本预训练生物医学图像描述的效果。研究者发现，更长的上下文长度（即更多的文本监督信息）与更好的检索和分类性能相关。基于此发现，他们构建了一个包含1M图像-描述对的数据集BIOMEDICA-LongCAP，该数据集的描述信息来源于全文文章，提供了更长、更丰富的文本监督。在此基础上，研究者训练了一个支持长上下文（最大512个token）的生物医学VLM——BMC-LongCLIP。

Result: BMC-LongCLIP模型将上下文容量扩展了6.6倍，将token浪费从55%降低到2.2%。在长描述检索基准测试中，BMC-LongCLIP在Recall@1上取得了高达+30%的绝对增益，在分类任务上平均提升了+2%，并且收敛速度比短上下文模型更快。

Conclusion: 研究结果表明，长上下文建模是推动生物医学VLMs发展的有前景的方向。

Abstract: Embedding vision-language models (VLMs) are typically pretrained with short
text windows (<77 tokens), which forces the truncation of long-format captions.
Yet, the distribution of biomedical captions from large-scale open source
literature reveals that a huge portion of captions far exceed 77 tokens. To
this end, we investigate the impact of pretraining on long-format biomedical
captions by extending the context length of text encoders in VLMs. We find that
longer context (thus, enabling additional supervision provided in long-format
captions) correlates with better retrieval and classification performance.
Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M
image-caption pairs enriched with context-aware descriptions from full-text
articles, providing longer and additional textual supervision. Using
BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a
text encoder supporting windows of up to 512 tokens. Our model extends context
capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption
retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in
Recall@1 and +2% average improvements in classification, while also converging
faster than short-context. Our results demonstrate that long-context modeling
is a promising direction for advancing biomedical VLMs.

</details>


### [77] [Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2510.03993)
*Yaxin Hou,Bo Han,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: CPG框架通过可控的伪标签生成，解决了长尾半监督学习中未标记数据分布未知的问题，并在各种基准数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的长尾半监督学习方法假设未标记数据的分布是已知的（例如长尾、均匀或反长尾），但实际情况通常是未标记数据的分布是未知的，并且可能是任意的。为了解决这个问题，需要一种能够处理未知未标记数据分布的方法。

Method: CPG框架通过一个可控的自增强优化循环来工作：1. 动态可控过滤机制选择性地从无标签数据集中提取可靠的伪标签，并将其加入到有标签数据集中，从而确保更新后的有标签数据集遵循已知分布。2. 基于更新后的有标签数据分布，通过Logit调整构建一个贝叶斯最优分类器。3. 该分类器随后在下一个训练步骤中帮助识别更可靠的伪标签。此外，还提出了一个类别感知自适应增强模块来改善少数类的表示，以及一个辅助分支来利用所有有标签和无标签样本以最大限度地利用数据。

Result: CPG框架在各种常用的基准数据集上进行了全面的评估，其性能一致性地得到了提升，在准确率方面超越了最先进的方法，提升幅度高达15.97%。

Conclusion: CPG框架通过其可控的伪标签生成机制，有效地解决了长尾半监督学习中未标记数据分布未知的问题，并显著提高了模型的性能。理论分析也表明该优化循环在特定条件下可以显著降低泛化误差。

Abstract: Current long-tailed semi-supervised learning methods assume that labeled data
exhibit a long-tailed distribution, and unlabeled data adhere to a typical
predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed).
However, the distribution of the unlabeled data is generally unknown and may
follow an arbitrary distribution. To tackle this challenge, we propose a
Controllable Pseudo-label Generation (CPG) framework, expanding the labeled
dataset with the progressively identified reliable pseudo-labels from the
unlabeled dataset and training the model on the updated labeled dataset with a
known distribution, making it unaffected by the unlabeled data distribution.
Specifically, CPG operates through a controllable self-reinforcing optimization
cycle: (i) at each training step, our dynamic controllable filtering mechanism
selectively incorporates reliable pseudo-labels from the unlabeled dataset into
the labeled dataset, ensuring that the updated labeled dataset follows a known
distribution; (ii) we then construct a Bayes-optimal classifier using logit
adjustment based on the updated labeled data distribution; (iii) this improved
classifier subsequently helps identify more reliable pseudo-labels in the next
training step. We further theoretically prove that this optimization cycle can
significantly reduce the generalization error under some conditions.
Additionally, we propose a class-aware adaptive augmentation module to further
improve the representation of minority classes, and an auxiliary branch to
maximize data utilization by leveraging all labeled and unlabeled samples.
Comprehensive evaluations on various commonly used benchmark datasets show that
CPG achieves consistent improvements, surpassing state-of-the-art methods by up
to \textbf{15.97\%} in accuracy. The code is available at
https://github.com/yaxinhou/CPG.

</details>


### [78] [Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5](https://arxiv.org/abs/2510.04003)
*Minh Hoang Nguyen,Su Nguyen Thiet*

Main category: cs.CV

TL;DR: 本研究提出一种针对古典中文（汉喃）文本的OCR优化方法，通过对PaddleOCRv5进行微调，显著提升了古籍文本的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 为了更好地数字化越南历史文献和促进跨语言语义研究，需要提高对包含退化扫描、非标准字形和手写变异的汉喃文本的识别能力。然而，现有的OCR系统在处理这类文本时存在困难。

Method: 采用微调策略，使用精心挑选的古代越南汉喃手稿子集对PaddleOCRv5的文本识别模块进行再训练。该过程包括预处理、LMDB转换、评估和可视化等完整的训练流程。

Result: 微调后的模型在汉喃文本识别方面取得了显著的改进，总体准确率从37.5%提升到50.0%，在图像噪声较大的情况下提升尤为明显。

Conclusion: 通过对PaddleOCRv5进行微调，有效解决了现有OCR系统在处理退化、非标准和手写汉喃文本时的挑战，显著提高了识别精度。研究还开发了一个交互式演示，便于下游应用，如汉越语义对齐、机器翻译和历史语言学研究。

Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital
role in digitizing Vietnamese historical documents and enabling cross-lingual
semantic research. However, existing OCR systems struggle with degraded scans,
non-standard glyphs, and handwriting variations common in ancient sources. In
this work, we propose a fine-tuning approach for PaddleOCRv5 to improve
character recognition on Han-Nom texts. We retrain the text recognition module
using a curated subset of ancient Vietnamese Chinese manuscripts, supported by
a full training pipeline covering preprocessing, LMDB conversion, evaluation,
and visualization. Experimental results show a significant improvement over the
base model, with exact accuracy increasing from 37.5 percent to 50.0 percent,
particularly under noisy image conditions. Furthermore, we develop an
interactive demo that visually compares pre- and post-fine-tuning recognition
results, facilitating downstream applications such as Han-Vietnamese semantic
alignment, machine translation, and historical linguistics research. The demo
is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.

</details>


### [79] [Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation](https://arxiv.org/abs/2510.04021)
*Kushal Vyas,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Implicit neural representations (INRs) have achieved remarkable successes in
learning expressive yet compact signal representations. However, they are not
naturally amenable to predictive tasks such as segmentation, where they must
learn semantic structures over a distribution of signals. In this study, we
introduce MetaSeg, a meta-learning framework to train INRs for medical image
segmentation. MetaSeg uses an underlying INR that simultaneously predicts per
pixel intensity values and class labels. It then uses a meta-learning procedure
to find optimal initial parameters for this INR over a training dataset of
images and segmentation maps, such that the INR can simply be fine-tuned to fit
pixels of an unseen test image, and automatically decode its class labels. We
evaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice
scores comparable to commonly used U-Net models, but with $90\%$ fewer
parameters. MetaSeg offers a fresh, scalable alternative to traditional
resource-heavy architectures such as U-Nets and vision transformers for medical
image segmentation. Our project is available at
https://kushalvyas.github.io/metaseg.html .

</details>


### [80] [Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning](https://arxiv.org/abs/2510.04022)
*Chendong Wang,Donglin Bai,Yifan Yang,Xiao Jin,Anlan Zhang,Rui Wang,Shiqi Jiang,Yuqing Yang,Hao Wu,Qi Dai,Chong Luo,Ting Cao,Lili Qiu,Suman Banerjee*

Main category: cs.CV

TL;DR: Video-in-the-Loop (ViTL) 是一个用于长视频问答的框架，它通过先用低帧率进行局部化，再进行高帧率回答来固定代币预算，并生成交错的输出。同时，我们提出了 dataname 数据集，将事件图转换为带时间跨度的多项选择题。ViTL 通过端到端训练，联合优化时间和答案的准确性，在固定代币预算下，在长视频问答和时间定位任务上取得了显著的性能提升，并且在计算效率上优于均匀采样。


<details>
  <summary>Details</summary>
Motivation: 长视频问答面临固定代币预算的挑战，需要一种能够有效利用视频信息并进行精确定位的方法。

Method: 提出 Video-in-the-Loop (ViTL) 框架，分为两个阶段：1. 低帧率局部化：识别与问题相关的视频片段。2. 高帧率回答：在选定片段内进行更精细的分析以回答问题。引入 dataname 数据集，将事件图转换为带时间跨度的多项选择题。采用交错式分组相对目标函数进行端到端训练，联合优化时间和答案的准确性。

Result: 在长视频问答和时间定位任务上（如 Charades-STA, ActivityNet-Captions），ViTL 在固定代币预算下取得了最高 8.6% 的性能提升，同时减少了 50% 的帧输入。消融实验表明，跨度感知代币重新分配的性能优于均匀采样。

Conclusion: dataname 数据集和 ViTL 框架为可扩展的长视频问答提供了一种可解释且计算高效的解决方案。

Abstract: We present \emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA
framework that preserves a fixed token budget by first \emph{localizing}
question-relevant interval(s) with a low-fps skim and then \emph{answering} via
span-aware reallocation of visual tokens at higher effective frame rate,
emitting an interleaved output with both spans and the final option for direct
attribution. We also introduce \dataname{}, which converts description based
event graphs into \emph{span-grounded} multiple-choice QA by pairing each
question with \emph{ground-truth} time span(s) and related reasoning. ViTL is
trained end-to-end with an interleaved group-relative objective that couples
temporal IoU for localization with answer correctness, allowing credit to flow
from answers back to spans without increasing compute. Under fixed token
budgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and
temporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations
show that span-aware token reallocation consistently surpasses uniform
sampling. Together, \dataname{} and ViTL provide an interpretable,
compute-efficient recipe for scalable long-video QA.

</details>


### [81] [Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation](https://arxiv.org/abs/2510.04024)
*Yuyan Bu,Qiang Sheng,Juan Cao,Shaofei Wang,Peng Qi,Yuhui Shi,Beizhe Hu*

Main category: cs.CV

TL;DR: 本研究提出了一种名为AgentAug的数据增强框架，通过模拟虚假新闻视频的创作过程来生成多样化的训练数据，以解决现有数据集不足和特征偏差的问题，从而提升虚假新闻视频检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 当前虚假新闻视频检测器主要依赖模式特征，但由于训练数据有限且多样性不足，导致模式存在偏差，影响了检测性能。这主要是因为现实世界中视频片段与虚假新闻事件之间存在复杂的多对多关系，而现有数据集未能充分反映这些关系。

Method: 提出AgentAug数据增强框架，该框架通过模拟四种虚假新闻视频的创作流程，并结合基于不确定性采样的主动学习策略，来生成和选择有用的增强样本。

Result: 在两个基准数据集上的实验结果表明，AgentAug能够持续提升短视频虚假新闻检测器的性能。

Conclusion: AgentAug通过生成多样化的虚假新闻视频数据，有效解决了现有数据集的局限性，显著提高了虚假新闻视频检测的准确性。

Abstract: The emergence of fake news on short video platforms has become a new
significant societal concern, necessitating automatic video-news-specific
detection. Current detectors primarily rely on pattern-based features to
separate fake news videos from real ones. However, limited and less diversified
training data lead to biased patterns and hinder their performance. This
weakness stems from the complex many-to-many relationships between video
material segments and fabricated news events in real-world scenarios: a single
video clip can be utilized in multiple ways to create different fake
narratives, while a single fabricated event often combines multiple distinct
video segments. However, existing datasets do not adequately reflect such
relationships due to the difficulty of collecting and annotating large-scale
real-world data, resulting in sparse coverage and non-comprehensive learning of
the characteristics of potential fake news video creation. To address this
issue, we propose a data augmentation framework, AgentAug, that generates
diverse fake news videos by simulating typical creative processes. AgentAug
implements multiple LLM-driven pipelines of four fabrication categories for
news video creation, combined with an active learning strategy based on
uncertainty sampling to select the potentially useful augmented samples during
training. Experimental results on two benchmark datasets demonstrate that
AgentAug consistently improves the performance of short video fake news
detectors.

</details>


### [82] [Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks](https://arxiv.org/abs/2510.04034)
*Linn Bieske,Carla Lorente*

Main category: cs.CV

TL;DR: 深度学习图像编辑方法（如stable diffusion）虽然简化了编辑过程，但结果不稳定，例如发色变化不一致。本研究旨在通过优化超参数来提高prompt-to-prompt图像编辑的精度和可靠性。我们研究了“词语替换”方法，提出了一种新的“注意力重加权方法”以提高适应性，并提出了“CL P2P”框架来解决循环不一致等现有局限性。这项工作有助于理解和改进超参数设置与神经网络模型（特别是其注意力机制）架构选择之间的相互作用，这些因素显著影响生成图像的组成和质量。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习图像编辑方法，特别是基于注意力机制的文本驱动方法，虽然简化了操作，但存在结果不一致（如发色变化不一致）的问题。本研究的动机是提高prompt-to-prompt图像编辑的精度和可靠性。

Method: 本研究探索并优化了超参数，研究了“词语替换”方法，开发了“注意力重加权方法”以提高模型适应性，并提出了“CL P2P”框架来解决循环不一致等问题。

Result: 本研究通过研究和优化超参数，提出了“注意力重加权方法”和“CL P2P”框架，旨在解决现有图像编辑框架中的局限性，提高结果的精度和可靠性。

Conclusion: 本研究通过对超参数和模型架构（特别是注意力机制）的深入分析，为提升深度学习图像编辑的稳定性和可控性提供了新的方法和框架。

Abstract: Recent advances in image editing have shifted from manual pixel manipulation
to employing deep learning methods like stable diffusion models, which now
leverage cross-attention mechanisms for text-driven control. This transition
has simplified the editing process but also introduced variability in results,
such as inconsistent hair color changes. Our research aims to enhance the
precision and reliability of prompt-to-prompt image editing frameworks by
exploring and optimizing hyperparameters. We present a comprehensive study of
the "word swap" method, develop an "attention re-weight method" for better
adaptability, and propose the "CL P2P" framework to address existing
limitations like cycle inconsistency. This work contributes to understanding
and improving the interaction between hyperparameter settings and the
architectural choices of neural network models, specifically their attention
mechanisms, which significantly influence the composition and quality of the
generated images.

</details>


### [83] [\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding](https://arxiv.org/abs/2510.04039)
*Bin Lei,Nuo Xu,Ali Payani,Mingyi Hong,Chunhua Liao,Yu Cao,Caiwen Ding*

Main category: cs.CV

TL;DR: GUI-Spotlight通过使用多个专用工具来动态地缩小屏幕相关区域的焦点，从而提高视觉基础的准确性，解决了MLLMs在GUI交互中视觉基础不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在GUI交互中视觉基础不准确，限制了其在真实世界环境中的应用，尤其是在执行指针级操作时。

Method: 提出GUI-Spotlight模型，该模型经过图像基础推理训练，能够动态调用多个专用工具，迭代地缩小对屏幕相关区域的焦点，以提高视觉基础的准确性。

Result: 在ScreenSpot-Pro基准测试中，GUI-Spotlight仅用18.5K训练样本就达到了52.8%的准确率，优于使用数百万训练样本的V2P-7B（50.6%）和GTA-1-7B（50.1%）。

Conclusion: GUI-Spotlight能够显著提高视觉基础的准确性，为MLLMs在复杂GUI环境中的应用提供了更可靠的基础。

Abstract: Multimodal large language models (MLLMs) have markedly expanded the
competence of graphical user-interface (GUI) systems, propelling them beyond
controlled simulations into complex, real-world environments across diverse
platforms. However, practical usefulness is still bounded by the reliability of
visual grounding, i.e., mapping textual references to exact on-screen elements.
This limitation prevents the system from accurately performing pointer-level
actions such as clicking or dragging. To address it, we introduce GUI-Spotlight
-- a model trained for image-grounded reasoning that dynamically invokes
multiple specialized tools to iteratively narrow its focus to the relevant
region of the screen, thereby substantially improving visual grounding
accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only
18.5K training samples achieves 52.8\% accuracy, surpassing V2P-7B (50.6\% with
9.6M training samples) and GTA-1-7B (50.1\% with 1.56M training samples).

</details>


### [84] [Quantization Range Estimation for Convolutional Neural Networks](https://arxiv.org/abs/2510.04044)
*Bingtao Yang,Yujia Wang,Mengzhi Jiao,Hongwei Huo*

Main category: cs.CV

TL;DR: 通过将量化范围估计建模为最小化量化误差的优化问题，并提出一种高效的搜索算法，本研究改进了模型后训练量化性能，实现了在图像分类任务上8位和6位量化下几乎没有精度损失，4位量化精度也显著提高。


<details>
  <summary>Details</summary>
Motivation: 低比特量化在保持模型精度的同时，对于减小深度神经网络模型存储的挑战。

Method: 将范围估计建模为最小化量化误差的优化问题，并通过层级局部极小值来解决，证明该问题是局部凸的，并提出一种高效的搜索算法来寻找最优解，同时将该算法应用于变换后的权重空间以获得进一步的实践改进。

Result: 在ResNet系列模型和Inception-v3模型的图像分类任务上，提出的方法在Top-1准确率方面普遍优于现有技术。实验结果表明，在8位和6位设置下，提出的方法在图像分类任务上几乎没有Top-1准确率损失，并且4位量化的准确率也得到了显著提高。

Conclusion: 提出的范围估计方法能够有效提升模型后训练量化性能，在保持模型精度的同时实现更低比特的量化。

Abstract: Post-training quantization for reducing the storage of deep neural network
models has been demonstrated to be an effective way in various tasks. However,
low-bit quantization while maintaining model accuracy is a challenging problem.
In this paper, we present a range estimation method to improve the quantization
performance for post-training quantization. We model the range estimation into
an optimization problem of minimizing quantization errors by layer-wise local
minima. We prove this problem is locally convex and present an efficient search
algorithm to find the optimal solution. We propose the application of the above
search algorithm to the transformed weights space to do further improvement in
practice. Our experiments demonstrate that our method outperforms
state-of-the-art performance generally on top-1 accuracy for image
classification tasks on the ResNet series models and Inception-v3 model. The
experimental results show that the proposed method has almost no loss of top-1
accuracy in 8-bit and 6-bit settings for image classifications, and the
accuracy of 4-bit quantization is also significantly improved. The code is
available at https://github.com/codeiscommitting/REQuant.

</details>


### [85] [MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation](https://arxiv.org/abs/2510.04057)
*Zhenyu Pan,Yucheng Lu,Han Liu*

Main category: cs.CV

TL;DR: MetaFind是一个三模态组合检索框架，用于元宇宙中的3D资产检索，解决了不一致性和缺乏标准化范式的问题，通过联合建模对象级特征和场景级布局结构，实现了跨模态的灵活检索，并提高了空间推理和风格一致性。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决元宇宙中3D资产检索面临的两大核心挑战：一是现有方法忽略了空间、语义和风格约束导致资产检索不一致；二是缺乏专门针对3D资产检索的标准化检索范式，现有方法多依赖通用3D形状表示模型。

Method: MetaFind提出了一种灵活的检索机制，支持文本、图像和3D模态的任意组合作为查询。其核心是引入了一个名为ESSGNN的可插拔等变布局编码器，该编码器能够捕捉空间关系和对象外观特征，并确保检索到的3D资产在上下文和风格上与现有场景保持一致，不受坐标系变换的影响。该框架还支持通过不断适应当前场景更新来实现迭代场景构建。

Result: 实验评估表明，MetaFind在各种检索任务中，相较于基线方法，在空间和风格一致性方面均有所提升。

Conclusion: MetaFind通过引入创新的三模态组合检索框架和ESSGNN编码器，有效解决了元宇宙3D资产检索中的一致性和标准化问题，显著提高了检索结果的空间和风格连贯性，为元宇宙场景的构建提供了更强大的支持。

Abstract: We present MetaFind, a scene-aware tri-modal compositional retrieval
framework designed to enhance scene generation in the metaverse by retrieving
3D assets from large-scale repositories. MetaFind addresses two core
challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,
and stylistic constraints, and (ii) the absence of a standardized retrieval
paradigm specifically tailored for 3D asset retrieval, as existing approaches
mainly rely on general-purpose 3D shape representation models. Our key
innovation is a flexible retrieval mechanism that supports arbitrary
combinations of text, image, and 3D modalities as queries, enhancing spatial
reasoning and style consistency by jointly modeling object-level features
(including appearance) and scene-level layout structures. Methodologically,
MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that
captures spatial relationships and object appearance features, ensuring
retrieved 3D assets are contextually and stylistically coherent with the
existing scene, regardless of coordinate frame transformations. The framework
supports iterative scene construction by continuously adapting retrieval
results to current scene updates. Empirical evaluations demonstrate the
improved spatial and stylistic consistency of MetaFind in various retrieval
tasks compared to baseline methods.

</details>


### [86] [Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction](https://arxiv.org/abs/2510.04063)
*Chetraj Pandey,Jinsu Hong,Anli Ji,Rafal A. Angryk,Berkay Aydin*

Main category: cs.CV

TL;DR: 二元分类中的太阳耀斑预测忽略了子类别之间的序数关系，这导致模型在预测阈值附近表现不佳。我们提出了一种改进的损失函数，通过整合子类别的序数信息来解决这个问题，从而在优化过程中更严厉地惩罚接近预测阈值的错误分类。


<details>
  <summary>Details</summary>
Motivation: 现有的太阳耀斑预测方法通常被视为二元分类问题，即将耀斑分为“耀斑”或“无耀斑”。然而，这种方法忽略了耀斑子类别（例如，C级、M级、X级）之间固有的序数关系，导致在预测阈值附近的分类错误尤为频繁。

Method: 提出了一种修改后的损失函数，该函数将二元化耀斑标签的子类别之间的序数信息整合到传统的二元交叉熵（BCE）损失中。这种方法作为一种具有序数意识的数据驱动的正则化方法，在模型优化过程中，比远离边界的错误分类，对预测阈值附近的不正确耀斑事件分类错误进行更严厉的惩罚。

Result: 通过整合序数权重到损失函数中，旨在通过利用数据的序数特征来增强模型的学习过程，从而提高其整体性能。

Conclusion: 通过在损失函数中引入序数信息，可以提高太阳耀斑预测模型的性能，尤其是在处理接近预测阈值的分类时。

Abstract: The prediction of solar flares is typically formulated as a binary
classification task, distinguishing events as either Flare (FL) or No-Flare
(NF) according to a specified threshold (for example, greater than or equal to
C-class, M-class, or X-class). However, this binary framework neglects the
inherent ordinal relationships among the sub-classes contained within each
category (FL and NF). Several studies on solar flare prediction have
empirically shown that the most frequent misclassifications occur near this
prediction threshold. This suggests that the models struggle to differentiate
events that are similar in intensity but fall on opposite sides of the binary
threshold. To mitigate this limitation, we propose a modified loss function
that integrates the ordinal information among the sub-classes of the binarized
flare labels into the conventional binary cross-entropy (BCE) loss. This
approach serves as an ordinality-aware, data-driven regularization method that
penalizes the incorrect predictions of flare events in close proximity to the
prediction threshold more heavily than those away from the boundary during
model optimization. By incorporating ordinal weighting into the loss function,
we aim to enhance the model's learning process by leveraging the ordinal
characteristics of the data, thereby improving its overall performance.

</details>


### [87] [QuantDemoire: Quantization with Outlier Aware for Image Demoiréing](https://arxiv.org/abs/2510.04066)
*Zheng Chen,Kewei Zhang,Xiaoyang Liu,Weihang Zhang,Mengfan Wang,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: QuantDemoire是一种针对去摩尔纹任务的后训练量化框架，通过引入感知离群值量化器和感知频率校准策略，在显著减少模型大小和计算量的同时，有效保持图像质量，并在W4A4上取得了超过4dB的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的去摩尔纹方法计算资源消耗大，难以部署在边缘设备。模型量化是一种有效的解决方案，但直接应用于去摩尔纹模型会导致严重的性能下降，其主要原因是离群值和光滑区域表示减弱。因此，需要一种专门针对去摩尔纹任务的模型量化方法。

Method: QuantDemoire框架包含两个关键组件：1. 离群值感知量化器：通过基于采样的范围估计来减少激活离群值，并将少量极端权重保留在FP16以降低成本。2. 频率感知校准策略：在微调过程中侧重于低频和中频分量，以减轻低比特量化引起的色带伪影。

Result: 通过大量实验验证，QuantDemoire在显著减少参数和计算量的同时，有效保持了图像质量。与现有的量化方法相比，在W4A4上性能提升超过4dB。

Conclusion: QuantDemoire成功地解决了现有量化方法在去摩尔纹任务中遇到的性能下降问题，实现了高效且高质量的去摩尔纹。

Abstract: Demoir\'eing aims to remove moir\'e artifacts that often occur in images.
While recent deep learning-based methods have achieved promising results, they
typically require substantial computational resources, limiting their
deployment on edge devices. Model quantization offers a compelling solution.
However, directly applying existing quantization methods to demoir\'eing models
introduces severe performance degradation. The main reasons are distribution
outliers and weakened representations in smooth regions. To address these
issues, we propose QuantDemoire, a post-training quantization framework
tailored to demoir\'eing. It contains two key components. **First}, we
introduce an outlier-aware quantizer to reduce errors from outliers. It uses
sampling-based range estimation to reduce activation outliers, and keeps a few
extreme weights in FP16 with negligible cost. **Second**, we design a
frequency-aware calibration strategy. It emphasizes low- and mid-frequency
components during fine-tuning, which mitigates banding artifacts caused by
low-bit quantization. Extensive experiments validate that our QuantDemoire
achieves large reductions in parameters and computation while maintaining
quality. Meanwhile, it outperforms existing quantization methods by over **4
dB** on W4A4. Code is released at:
https://github.com/zhengchen1999/QuantDemoire.

</details>


### [88] [Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging](https://arxiv.org/abs/2510.04069)
*Zongyin Deng,Qing Zhou,Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: TV-LoRA是一种结合扩散生成先验和多重正则化约束（包括各向异性TV和核范数）的低剂量稀疏CT重建新方法，通过ADMM框架实现，能在极稀疏视图下有效解决病态问题和纹理损失，并能高效推理。


<details>
  <summary>Details</summary>
Motivation: 在极稀疏视图条件下，CT重建面临病态问题和纹理损失的挑战，需要结合生成模型和物理约束来提高重建质量。

Method: TV-LoRA结合了NCSN++扩散生成先验和各向异性TV、核范数（LoRA）等多重正则化约束，在ADMM框架下进行优化。同时，采用基于2D切片策略、FFT加速和张量并行优化技术来提高推理效率。

Result: 在AAPM-2016、CTHD和LIDC数据集上，使用N_view=8,4,2进行实验，TV-LoRA在SSIM、纹理恢复、边缘清晰度和伪影抑制方面持续优于现有基准方法，展现了鲁棒性和泛化能力。消融研究证实了LoRA正则化和扩散先验的互补效应，FFT-PCG模块也提供了加速。整体而言，扩散+TV-LoRA实现了高保真、高效的3D CT重建，并具有广泛的临床应用前景。

Conclusion: TV-LoRA是一种有效且高效的低剂量、稀疏采样CT重建方法，能够实现高保真度的3D CT重建，并在实际临床应用中具有广泛前景。

Abstract: This work presents TV-LoRA, a novel method for low-dose sparse-view CT
reconstruction that combines a diffusion generative prior (NCSN++ with SDE
modeling) and multi-regularization constraints, including anisotropic TV and
nuclear norm (LoRA), within an ADMM framework. To address ill-posedness and
texture loss under extremely sparse views, TV-LoRA integrates generative and
physical constraints, and utilizes a 2D slice-based strategy with FFT
acceleration and tensor-parallel optimization for efficient inference.
Experiments on AAPM-2016, CTHD, and LIDC datasets with
$N_{\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks
in SSIM, texture recovery, edge clarity, and artifact suppression,
demonstrating strong robustness and generalizability. Ablation studies confirm
the complementary effects of LoRA regularization and diffusion priors, while
the FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves
high-fidelity, efficient 3D CT reconstruction and broad clinical applicability
in low-dose, sparse-sampling scenarios.

</details>


### [89] [TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing](https://arxiv.org/abs/2510.04100)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Harold Soh*

Main category: cs.CV

TL;DR: 该论文解决了拓扑地图评估中的标准化问题，提出了一种新的评估指标和数据集，并量化了感知混淆的影响。


<details>
  <summary>Details</summary>
Motivation: 当前拓扑地图研究缺乏标准化的评估指标、数据集和协议，导致无法进行公平和可复现的比较。感知混淆这一关键挑战也未被充分量化。

Method: 提出将拓扑一致性形式化为拓扑地图的基本属性，并证明了定位精度是该属性的高效且可解释的替代指标。提出了数据集歧义性的第一个量化度量。构建了一个具有可校准歧义水平的多样化基准数据集，并发布了深度学习基线系统。

Result: 通过实验和分析，揭示了当前方法在感知混淆下的局限性。

Conclusion: 提出的评估协议、数据集和基线系统促进了拓扑地图领域的研究，并为未来的研究提供了标准化的评估方法。

Abstract: Topological mapping offers a compact and robust representation for
navigation, but progress in the field is hindered by the lack of standardized
evaluation metrics, datasets, and protocols. Existing systems are assessed
using different environments and criteria, preventing fair and reproducible
comparisons. Moreover, a key challenge - perceptual aliasing - remains
under-quantified, despite its strong influence on system performance. We
address these gaps by (1) formalizing topological consistency as the
fundamental property of topological maps and showing that localization accuracy
provides an efficient and interpretable surrogate metric, and (2) proposing the
first quantitative measure of dataset ambiguity to enable fair comparisons
across environments. To support this protocol, we curate a diverse benchmark
dataset with calibrated ambiguity levels, implement and release deep-learned
baseline systems, and evaluate them alongside classical methods. Our
experiments and analysis yield new insights into the limitations of current
approaches under perceptual aliasing. All datasets, baselines, and evaluation
tools are fully open-sourced to foster consistent and reproducible research in
topological mapping.

</details>


### [90] [Learning Efficient Meshflow and Optical Flow from Event Cameras](https://arxiv.org/abs/2510.04111)
*Xinglong Luo,Ao Luo,Kunming Luo,Zhengning Wang,Ping Tan,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 本论文提出了一种新的事件相机事件网格流估计方法EEMFlow，并发布了新的数据集HREM。


<details>
  <summary>Details</summary>
Motivation: 事件相机事件流估计领域缺乏专门针对网格流的数据集和方法，并且对事件数据密度对模型性能的影响研究不足。

Method: 1. 创建了一个大规模的高分辨率事件网格流（HREM）数据集，包含高分辨率（1280x720）、动态对象、复杂运动模式以及光流和网格流标签。
2. 提出了一种名为EEMFlow的高效事件网格流估计网络，该网络采用轻量级编码器-解码器架构。
3. 提出了置信度诱导细节完成（CDC）模块，以支持密集事件光流估计并保留运动边界。
4. 将HREM数据集扩展为HREM+，一个包含不同密度的事件数据集，并提出了自适应密度模块（ADM）来提高模型的泛化能力。

Result: EEMFlow模型在性能和运行效率（比现有方法快30倍）方面表现优于现有方法。
ADM模块可以显著提高EEMFlow和EEMFlow+的性能（分别提高8%和10%）。

Conclusion: 所提出的HREM数据集和EEMFlow网络解决了事件相机事件网格流估计的挑战，并在性能和效率方面取得了显著进展。ADM模块进一步提高了模型的泛化能力。

Abstract: In this paper, we explore the problem of event-based meshflow estimation, a
novel task that involves predicting a spatially smooth sparse motion field from
event cameras. To start, we review the state-of-the-art in event-based flow
estimation, highlighting two key areas for further research: i) the lack of
meshflow-specific event datasets and methods, and ii) the underexplored
challenge of event data density. First, we generate a large-scale
High-Resolution Event Meshflow (HREM) dataset, which showcases its superiority
by encompassing the merits of high resolution at 1280x720, handling dynamic
objects and complex motion patterns, and offering both optical flow and
meshflow labels. These aspects have not been fully explored in previous works.
Besides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a
lightweight model featuring a specially crafted encoder-decoder architecture to
facilitate swift and accurate meshflow estimation. Furthermore, we upgrade
EEMFlow network to support dense event optical flow, in which a
Confidence-induced Detail Completion (CDC) module is proposed to preserve sharp
motion boundaries. We conduct comprehensive experiments to show the exceptional
performance and runtime efficiency (30x faster) of our EEMFlow model compared
to the recent state-of-the-art flow method. As an extension, we expand HREM
into HREM+, a multi-density event dataset contributing to a thorough study of
the robustness of existing methods across data with varying densities, and
propose an Adaptive Density Module (ADM) to adjust the density of input event
data to a more optimal range, enhancing the model's generalization ability. We
empirically demonstrate that ADM helps to significantly improve the performance
of EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are
released at https://github.com/boomluo02/EEMFlowPlus.

</details>


### [91] [Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation](https://arxiv.org/abs/2510.04125)
*Seunghyun Lee,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 提出一种新颖的扩散模型管道，通过预训练编码器和时间依赖性分数缩放来解决现有方法训练收敛慢和需要额外评估网络的问题，从而在训练和推理效率上都实现了最先进的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在类别级 6D 物体姿态估计中收敛速度慢，并且需要额外的评估网络来过滤姿态假设。

Method: 1. 预训练编码器，并与回归头和去噪扩散头联合学习网络，以加速收敛并提高精度。 2. 提出通过时间依赖性分数缩放进行采样引导，以平衡探索-利用的权衡，无需额外的评估网络，并在早期去噪步骤中保持对称物体的多模态特性，同时在最后步骤中确保高质量的姿态生成。

Result: 在 REAL275、HouseCat6D 和 ROPE 等多个基准测试中，即使使用单姿态推理，也实现了最先进的准确性，并且在训练和推理方面都更有效率。

Conclusion: 所提出的方法通过预训练编码器和采样引导，有效解决了现有方法的局限性，实现了高精度和高效率。

Abstract: Latest diffusion models have shown promising results in category-level 6D
object pose estimation by modeling the conditional pose distribution with depth
image input. The existing methods, however, suffer from slow convergence during
training, learning its encoder with the diffusion denoising network in
end-to-end fashion, and require an additional network that evaluates sampled
pose hypotheses to filter out low-quality pose candidates. In this paper, we
propose a novel pipeline that tackles these limitations by two key components.
First, the proposed method pretrains the encoder with the direct pose
regression head, and jointly learns the networks via the regression head and
the denoising diffusion head, significantly accelerating training convergence
while achieving higher accuracy. Second, sampling guidance via time-dependent
score scaling is proposed s.t. the exploration-exploitation trade-off is
effectively taken, eliminating the need for the additional evaluation network.
The sampling guidance maintains multi-modal characteristics of symmetric
objects at early denoising steps while ensuring high-quality pose generation at
final steps. Extensive experiments on multiple benchmarks including REAL275,
HouseCat6D, and ROPE, demonstrate that the proposed method, simple yet
effective, achieves state-of-the-art accuracies even with single-pose
inference, while being more efficient in both training and inference.

</details>


### [92] [Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs](https://arxiv.org/abs/2510.04142)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.CV

TL;DR: 本论文提出了一种名为“自主偏好优化”（APO）的新方法，用于解决多模态大语言模型（MLLMs）在知识蒸馏过程中教师模型推理轨迹的概念漂移问题，以提高学生模型的性能、一致性、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在知识蒸馏过程中，多个教师模型的推理轨迹会发生概念漂移，导致其推理分布不可预测地演变，并将偏差传递给学生模型，最终影响学生模型的性能。

Method: 提出了一种理论联系，将概念漂移和知识蒸馏联系起来，将多 MLLM 教师的非平稳推理动态视为多流推理轨迹的下一个词预测。在此基础上，引入了“学习、比较、批评”范式，并通过自主偏好优化（APO）实现。学生模型首先通过比较多个教师来学习和自我蒸馏首选思维，然后通过 APO 进行概念对齐，实现对漂移推理的批判性反思。

Result: 通过大量实验证明，该方法在知识蒸馏的一致性、鲁棒性和泛化能力方面表现优越。此外，还贡献了一个大规模数据集 CXR-MAX（Multi-teachers Alignment X-rays），其中包含 170,982 条基于 MIMIC-CXR 的、来自公开 MLLMs 的蒸馏推理轨迹。

Conclusion: 所提出的“学习、比较、批评”范式和 APO 方法能够有效解决多模态大语言模型知识蒸馏中的概念漂移问题，显著提高学生模型性能，并提供了相关数据集和代码以供研究。

Abstract: This paper identifies a critical yet underexplored challenge in distilling
from multimodal large language models (MLLMs): the reasoning trajectories
generated by multiple drifting teachers exhibit concept drift, whereby their
reasoning distributions evolve unpredictably and transmit biases to the student
model, ultimately compromising its performance. To tackle this issue, we
pioneer a theoretical connection between concept drift and knowledge
distillation, casting the non-stationary reasoning dynamics from multiple MLLM
teachers as next-token prediction of multi-stream reasoning trajectories.Guided
by concept drift, we introduce the "learn, compare, critique" paradigm,
culminating in autonomous preference optimization (APO). Under the active
guidance of the teachers, the student model first learns and self-distils
preferred thinking by comparing multiple teachers. It then engages in critical
reflection over the drifting inference from teachers, performing concept
alignment through APO, ultimately yielding a robust, consistent, and
generalizable model.Extensive experiments demonstrate our superior performance
of consistency, robustness and generalization within knowledge distillation.
Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers
Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived
from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public
at: https://anonymous.4open.science/r/Autonomous-Distillation/.

</details>


### [93] [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
*Chenxin Wang,Elyas Asadi Shamsabadi,Zhaohui Chen,Luming Shen,Alireza Ahmadian Fard Fini,Daniel Dias-da-Costa*

Main category: cs.CV

TL;DR: SiteShield是一个基于多模态大语言模型（LVLM）和检索增强生成（RAG）的框架，用于自动化施工安全检查报告，整合了视觉和音频输入，相比单一模型，在F1分数、汉明损失、精确率和召回率方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有施工安全检查方法效率低下，需要处理大量信息；而基于大语言模型（LLMs）的应用受限于训练数据和实时适应性，并且存在响应不相关、输入模式受限和幻觉等问题。因此，有必要开发一种更有效、更具适应性的自动化安全检查方法。

Method: 开发了一个名为SiteShield的多模态LVLM-RAG框架，该框架能够整合视觉和音频输入，用于自动化施工安全检查报告的生成。

Result: SiteShield在真实世界数据上的表现优于未使用RAG的单一LLMs，取得了0.82的F1分数、0.04的汉明损失、0.76的精确率和0.96的召回率。

Conclusion: SiteShield为提高信息检索和生成安全报告的效率提供了一条新途径。

Abstract: Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

</details>


### [94] [BLADE: Bias-Linked Adaptive DEbiasing](https://arxiv.org/abs/2510.04174)
*Piyush Arora,Navlika Singh,Vasubhya Diwan,Pratik Mazumder*

Main category: cs.CV

TL;DR: BLADE是一种无需预先了解偏见或偏见冲突样本即可进行生成式去偏析的框架。它通过训练生成模型来转换图像，然后在图像易受偏见影响的情况下，根据图像的偏见易感性自适应地对其进行细化。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于对偏见或偏见冲突样本的先验知识，而这些假设在现实世界中通常是不切实际的。

Method: BLADE首先训练一个生成模型，在保留任务相关特征的同时，在偏见域之间转换图像。然后，它根据图像对偏见的易感性，自适应地用其合成对应物来细化每个图像。

Result: BLADE在多个基准数据集上进行了评估，并显示其性能显著优于最先进的方法，在CIFAR-10数据集的腐败条件下，在最差组设置下，比最接近的基线高出约18%的绝对优势。

Conclusion: BLADE在不进行显式监督的情况下，在减轻偏见和开发更鲁棒的深度学习模型方面取得了新的进展。

Abstract: Neural networks have revolutionized numerous fields, yet they remain
vulnerable to a critical flaw: the tendency to learn implicit biases, spurious
correlations between certain attributes and target labels in training data.
These biases are often more prevalent and easier to learn, causing models to
rely on superficial patterns rather than task-relevant features necessary for
generalization. Existing methods typically rely on strong assumptions, such as
prior knowledge of these biases or access to bias-conflicting samples, i.e.,
samples that contradict spurious correlations and counterbalance bias-aligned
samples, samples that conform to these spurious correlations. However, such
assumptions are often impractical in real-world settings. We propose BLADE
({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that
requires no prior knowledge of bias or bias-conflicting samples. BLADE first
trains a generative model to translate images across bias domains while
preserving task-relevant features. Then, it adaptively refines each image with
its synthetic counterpart based on the image's susceptibility to bias. To
encourage robust representations, BLADE aligns an image with its
bias-translated synthetic counterpart that shares task-relevant features but
differs in bias, while misaligning it with samples sharing the same bias. We
evaluate BLADE on multiple benchmark datasets and show that it significantly
outperforms state-of-the-art methods. Notably, it exceeds the closest baseline
by an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the
worst group setting, establishing a new benchmark in bias mitigation and
demonstrating its potential for developing more robust deep learning models
without explicit supervision.

</details>


### [95] [From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation](https://arxiv.org/abs/2510.04180)
*Ran Eisenberg,Amit Rozner,Ethan Fetaya,Ofir Lindenbaum*

Main category: cs.CV

TL;DR: SEG-MIL-CBM通过概念引导的图像分割和多实例学习框架，实现了可解释的、空间定位的、概念层面的解释，无需概念标注，并在各种挑战性设置下均表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型缺乏透明度，可能依赖不可靠的特征，尤其在安全关键应用中，需要可解释性来建立信任。现有的概念瓶颈模型（CBM）虽然提高了透明度，但需要昂贵اً的概念标注，并且缺乏空间定位能力。

Method: 提出SEG-MIL-CBM框架，将概念引导的图像分割与基于注意力的多实例学习（MIL）相结合，将每个分割区域视为一个实例，并学习跨区域聚合证据。模型通过推理与高层概念对齐的语义区域来突出任务相关证据，并降低无关线索的权重。

Result: SEG-MIL-CBM在存在伪相关、输入损坏和大型基准测试等各种设置下，实现了鲁棒的性能，同时提供了透明、概念层面的解释。

Conclusion: SEG-MIL-CBM在无需概念标注的情况下，能够提供可解释的、空间定位的、概念层面的解释，并在各种具有挑战性的场景下表现出鲁棒性。

Abstract: Deep neural networks have achieved remarkable success in computer vision;
however, their black-box nature in decision-making limits interpretability and
trust, particularly in safety-critical applications. Interpretability is
crucial in domains where errors have severe consequences. Existing models not
only lack transparency but also risk exploiting unreliable or misleading
features, which undermines both robustness and the validity of their
explanations. Concept Bottleneck Models (CBMs) aim to improve transparency by
reasoning through human-interpretable concepts. Still, they require costly
concept annotations and lack spatial grounding, often failing to identify which
regions support each concept. We propose SEG-MIL-CBM, a novel framework that
integrates concept-guided image segmentation into an attention-based multiple
instance learning (MIL) framework, where each segmented region is treated as an
instance and the model learns to aggregate evidence across them. By reasoning
over semantically meaningful regions aligned with high-level concepts, our
model highlights task-relevant evidence, down-weights irrelevant cues, and
produces spatially grounded, concept-level explanations without requiring
annotations of concepts or groups. SEG-MIL-CBM achieves robust performance
across settings involving spurious correlations (unintended dependencies
between background and label), input corruptions (perturbations that degrade
visual quality), and large-scale benchmarks, while providing transparent,
concept-level explanations.

</details>


### [96] [Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers](https://arxiv.org/abs/2510.04188)
*Shikang Zheng,Guantao Chen,Qinming Zhou,Yuqi Lin,Lixuan He,Chang Zou,Peiliang Cai,Jiacheng Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: Diffusion Transformers的采样速度慢，我们提出HyCa通过维度控制的ODE方法来加速，在多个模型上实现了接近无损的加速。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers在图像和视频生成方面表现出色，但其迭代采样过程因Transformer前向计算成本高昂而成为瓶颈。

Method: 提出HyCa，一个混合ODE求解器激发的缓存框架，通过对不同维度特征的动态行为建模，采用维度感知的缓存策略。

Result: HyCa在FLUX上实现了5.55倍加速，在HunyuanVideo上实现了5.56倍加速，在Qwen-Image上实现了6.24倍加速，在Qwen-Image-Edit上实现了6.24倍加速，且无需重新训练。

Conclusion: HyCa通过维度感知的缓存策略，在不影响模型性能的情况下，显著加速了Diffusion Transformers的采样过程。

Abstract: Diffusion Transformers offer state-of-the-art fidelity in image and video
synthesis, but their iterative sampling process remains a major bottleneck due
to the high cost of transformer forward passes at each timestep. To mitigate
this, feature caching has emerged as a training-free acceleration technique
that reuses or forecasts hidden representations. However, existing methods
often apply a uniform caching strategy across all feature dimensions, ignoring
their heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by
modeling hidden feature evolution as a mixture of ODEs across dimensions, and
introduce HyCa, a Hybrid ODE solver inspired caching framework that applies
dimension-wise caching strategies. HyCa achieves near-lossless acceleration
across diverse domains and models, including 5.55 times speedup on FLUX, 5.56
times speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and
Qwen-Image-Edit without retraining.

</details>


### [97] [World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge](https://arxiv.org/abs/2510.04201)
*Moo Hyun Son,Jintaek Oh,Sun Bin Mun,Jaechul Roh,Sehyun Choi*

Main category: cs.CV

TL;DR: 虽然文本到图像（T2I）模型可以生成高质量的图像，但由于固有的知识截止效应，当提示涉及新颖或分布外（OOD）实体时，其性能会显著下降。我们引入了World-To-Image，一个新颖的框架，通过利用由代理驱动的世界知识来增强T2I的生成能力，从而缩小这一差距。我们设计了一个代理，该代理可以动态地搜索网络，检索基础模型未知的概念的图像。然后，利用这些信息进行多模态提示优化，引导强大的生成主干实现准确的合成。至关重要的是，我们的评估超越了传统的指标，利用了LLMGrader和ImageReward等现代评估方法来衡量真实的语义保真度。我们的实验表明，World-To-Image在语义对齐和视觉美学方面都显著优于最先进的方法，在我们精心设计的NICE基准测试中，准确性提高了+8.1%。我们的框架在不到三次迭代的情况下就取得了这些结果，效率很高，为能够更好地反映不断变化的世界的T2I系统铺平了道路。我们的演示代码可在https://github.com/mhson-kyle/World-To-Image 获得。


<details>
  <summary>Details</summary>
Motivation: T2I模型在处理新颖或分布外实体时性能下降，因为它们存在知识截止问题。

Method: 设计了一个代理，该代理能够动态搜索网络，检索T2I模型未知概念的图像，并利用这些信息进行多模态提示优化，以提高生成图像的准确性。

Result: World-To-Image框架在语义对齐和视觉美学方面均优于最先进的方法，在NICE基准测试中准确性提高了+8.1%，并且效率很高，迭代次数少于三次。

Conclusion: World-To-Image框架能够通过引入外部世界知识来解决T2I模型在处理新颖或分布外实体时的性能下降问题，从而生成更准确、更具美感的图像，并能更好地反映现实世界。

Abstract: While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

</details>


### [98] [MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering](https://arxiv.org/abs/2510.04220)
*Lixuan He,Shikang Zheng,Linfeng Zhang*

Main category: cs.CV

TL;DR: MASC通过构建分层语义树来解决AR图像生成模型中视觉标记词汇的低效问题，通过几何感知距离和密度驱动的聚集方法，将扁平的预测任务转化为结构化任务，从而提高训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: AR模型在图像生成方面潜力巨大，但由于其视觉标记词汇庞大且无结构，导致预测任务复杂，影响训练效率和生成质量。

Method: 提出了一种名为MASC（Manifold-Aligned Semantic Clustering）的框架，该框架直接从码本的内在结构构建分层语义树。MASC采用新颖的几何感知距离度量和密度驱动的聚集方法来模拟标记嵌入的潜在流形。

Result: MASC显著简化了AR模型的学习问题，加速了训练（高达57%），并显著提高了生成质量（LlamaGen-XL的FID从2.87降低到2.58）。

Conclusion: MASC通过结构化预测空间，使得AR模型在与最先进方法竞争方面具有高度竞争力，证明了结构化预测空间对于可扩展生成模型与架构创新同等重要。

Abstract: Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.

</details>


### [99] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: ZoomIn是一个两阶段的取证框架，通过定位可疑区域并进行深入分析来提高AI生成图像检测的准确性和可解释性，并引入了MagniFake数据集进行训练。


<details>
  <summary>Details</summary>
Motivation: AI生成图像的快速发展模糊了真实与合成内容的界限，引发了对数字完整性的担忧，而现有的视觉语言模型（VLMs）在检测高质量合成图像中的细微伪影方面存在不足。

Method: ZoomIn框架首先扫描图像以定位可疑区域，然后在这些放大区域内进行专注分析，以提供基于证据的判断。引入MagniFake数据集（包含20,000张真实和高质量合成图像）进行训练，该数据集通过基于VLM的自动化流程生成，并带有边界框和法证解释。

Result: ZoomIn方法在提供可理解的、基于视觉证据的解释的同时，实现了96.39%的准确率和鲁棒的泛化能力。

Conclusion: ZoomIn框架在检测AI生成图像方面取得了高准确率和良好的泛化能力，同时提供了可解释的法证分析。

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [100] [A Recursive Pyramidal Algorithm for Solving the Image Registration Problem](https://arxiv.org/abs/2510.04231)
*Stefan Dirnstorfer*

Main category: cs.CV

TL;DR: 该算法是一个简单的、可端到端训练的图像配准算法，可以用很少的Python代码实现，并且在训练数据、训练时间和代码复杂度有限的情况下表现良好。


<details>
  <summary>Details</summary>
Motivation: 图像配准是将一个图像中的点映射到另一个图像中的相应位置，但现有的方法可能需要大量数据和计算资源。

Method: 提出了一种简单的、可端到端训练的算法，该算法可以用很少的Python代码实现。

Result: 该算法在训练数据和训练时间很少的情况下，在某些场景下能取得精确的结果。例如，在立体视觉应用中，仅使用74张图像和19x15的输入窗口进行训练，就取得了优异的表现。

Conclusion: 该算法以其简洁性著称，可能为在训练数据、训练时间或代码复杂度受限的场景下进行图像配准提供了一个好的起点。

Abstract: The problem of image registration is finding a transformation that aligns two
images, such that the corresponding points are in the same location. This paper
introduces a simple, end-to-end trainable algorithm that is implementable in a
few lines of Python code. The approach is shown to work with very little
training data and training time, while achieving accurate results in some
settings. An example application to stereo vision was trained from 74 images on
a 19x15 input window. With just a dozen lines of Python code this algorithm
excels in brevity and may serve as a good start in related scenarios with
limitations to training data, training time or code complexity.

</details>


### [101] [Detection of retinal diseases using an accelerated reused convolutional network](https://arxiv.org/abs/2510.04232)
*Amin Ahmadi Kasani,Hedieh Sajedi*

Main category: cs.CV

TL;DR: 提出了一种名为ArConv的新型卷积层，并设计了一个仅包含130万参数的新模型。该模型在RfMiD数据集上实现了0.9328的准确率，优于MobileNetV2的0.9266，适用于移动设备，提高了眼病诊断的可及性。


<details>
  <summary>Details</summary>
Motivation: 为了提高深度神经网络模型的可及性，以便更广泛地应用于眼病检测等任务，并克服现有计算复杂的方法的局限性。

Method: 通过重新设计和优化卷积层，创造了一种名为ArConv的新型卷积层，并将其集成到一个新的通用模型中。

Result: 该模型仅包含130万参数，在RfMiD数据集上进行了训练和评估，准确率达到了0.9328，优于MobileNetV2（220万参数，准确率为0.9266）。

Conclusion: 所提出的新模型及其ArConv层具有高效的性能和合适的复杂度，适用于移动设备，能够以高准确率诊断眼病，显著提高了模型的可用性。

Abstract: Convolutional neural networks are continually evolving, with some efforts
aimed at improving accuracy, others at increasing speed, and some at enhancing
accessibility. Improving accessibility broadens the application of neural
networks across a wider range of tasks, including the detection of eye
diseases. Early diagnosis of eye diseases and consulting an ophthalmologist can
prevent many vision disorders. Given the importance of this issue, various
datasets have been collected from the cornea to facilitate the process of
making neural network models. However, most of the methods introduced in the
past are computationally complex. In this study, we tried to increase the
accessibility of deep neural network models. We did this at the most
fundamental level, specifically by redesigning and optimizing the convolutional
layers. By doing so, we created a new general model that incorporates our novel
convolutional layer named ArConv layers. Thanks to the efficient performance of
this new layer, the model has suitable complexity for use in mobile phones and
can perform the task of diagnosing the presence of disease with high accuracy.
The final model we present contains only 1.3 million parameters. In comparison
to the MobileNetV2 model, which has 2.2 million parameters, our model
demonstrated better accuracy when trained and evaluated on the RfMiD dataset
under identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on
the RfMiD test set.

</details>


### [102] [Scaling Sequence-to-Sequence Generative Neural Rendering](https://arxiv.org/abs/2510.04236)
*Shikun Liu,Kam Woh Ng,Wonbong Jang,Jiadong Guo,Junlin Han,Haozhe Liu,Yiannis Douratsos,Juan C. Pérez,Zijian Zhou,Chi Phung,Tao Xiang,Juan-Manuel Pérez-Rúa*

Main category: cs.CV

TL;DR: Kaleido是一个用于照片级、统一对象和场景级别神经渲染的生成模型家族，它将3D视为视频的子领域，通过序列到序列的图像合成任务实现。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过序列到序列的生成模型进行3D和场景渲染，特别是利用大规模视频数据预训练以提高性能并减少对3D数据集的依赖。

Method: Kaleido将3D视为视频的子领域，通过序列到序列的图像合成任务实现。它引入了关键的架构创新，实现了无需显式3D表示即可进行生成性视图合成，并能通过掩码自回归框架生成任意数量的6-DoF目标视图。该模型将3D和视频建模统一在一个单一的解码器-仅修正流Transformer中，并利用大规模视频数据进行预训练。

Result: Kaleido在多个视图合成基准测试中取得了新的最先进成果。其零样本性能在少样本设置下显著优于其他生成方法，并且在多样本设置下首次达到了与每场景优化方法相当的质量。

Conclusion: Kaleido通过将3D视为视频的子领域，并利用大规模视频数据进行预训练，实现了统一的对象和场景级别神经渲染，并在视图合成任务上取得了最先进的性能。

Abstract: We present Kaleido, a family of generative models designed for
photorealistic, unified object- and scene-level neural rendering. Kaleido
operates on the principle that 3D can be regarded as a specialised sub-domain
of video, expressed purely as a sequence-to-sequence image synthesis task.
Through a systemic study of scaling sequence-to-sequence generative neural
rendering, we introduce key architectural innovations that enable our model to:
i) perform generative view synthesis without explicit 3D representations; ii)
generate any number of 6-DoF target views conditioned on any number of
reference views via a masked autoregressive framework; and iii) seamlessly
unify 3D and video modelling within a single decoder-only rectified flow
transformer. Within this unified framework, Kaleido leverages large-scale video
data for pre-training, which significantly improves spatial consistency and
reduces reliance on scarce, camera-labelled 3D datasets -- all without any
architectural modifications. Kaleido sets a new state-of-the-art on a range of
view synthesis benchmarks. Its zero-shot performance substantially outperforms
other generative methods in few-view settings, and, for the first time, matches
the quality of per-scene optimisation methods in many-view settings.

</details>


### [103] [The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation](https://arxiv.org/abs/2510.04243)
*Jincan Lou,Jingkun Chen,Haoquan Li,Hang Li,Wenjian Huang,Weihua Chen,Fan Wang,Jianguo Zhang*

Main category: cs.CV

TL;DR: CoSSeg-TTA是一个基于nnU-Netv2的肝脏分割框架，通过半监督学习、域自适应和持续测试时自适应来提高在对比增强MRI中的分割准确性和泛化能力，尤其是在标注数据有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 肝脏分割在疾病诊断、治疗规划和监测中至关重要，但由于标注数据有限、增强方案异质以及跨中心/设备的域漂移，仍然具有挑战性。现有的图像到图像翻译框架（如Pix2Pix和cycle-GAN）在单模态肝脏分割场景中存在局限性。

Method: 提出CoSSeg-TTA框架，基于nnU-Netv2，结合了半监督均值教师学习（利用大量未标记数据）、域自适应模块（包括随机直方图风格转换和可训练的对比感知网络）以及持续测试时自适应策略。

Result: CoSSeg-TTA在GED4（Gd-EOB-DTPA增强的肝胆期MRI）模态上进行了实验，结果显示相比nnU-Netv2基线，Dice分数和Hausdorff距离均得到显著提升，并表现出在低标注条件下对未见域的强大泛化能力。

Conclusion: CoSSeg-TTA框架通过集成半监督学习、域自适应和测试时自适应技术，能够有效解决肝脏分割中的数据稀疏性和域漂移问题，在实际应用中展现出优越的性能和泛化能力。

Abstract: Accurate liver segmentation from contrast-enhanced MRI is essential for
diagnosis, treatment planning, and disease monitoring. However, it remains
challenging due to limited annotated data, heterogeneous enhancement protocols,
and significant domain shifts across scanners and institutions. Traditional
image-to-image translation frameworks have made great progress in domain
generalization, but their application is not straightforward. For example,
Pix2Pix requires image registration, and cycle-GAN cannot be integrated
seamlessly into segmentation pipelines. Meanwhile, these methods are originally
used to deal with cross-modality scenarios, and often introduce structural
distortions and suffer from unstable training, which may pose drawbacks in our
single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a
compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary
phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised
mean teacher scheme to exploit large amounts of unlabeled volumes. A domain
adaptation module, incorporating a randomized histogram-based style appearance
transfer function and a trainable contrast-aware network, enriches domain
diversity and mitigates cross-center variability. Furthermore, a continual
test-time adaptation strategy is employed to improve robustness during
inference. Extensive experiments demonstrate that our framework consistently
outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff
Distance while exhibiting strong generalization to unseen domains under
low-annotation conditions.

</details>


### [104] [Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks](https://arxiv.org/abs/2510.04245)
*Ayushi Mehrotra,Derek Peng,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 提出一种无需预知 patch 大小或位置的防御方法，通过利用基于概念的解释来识别和抑制最具影响力的概念激活向量，从而抵御对抗性 patch 攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性 patch 攻击防御方法通常假设预先知道 patch 的大小或位置，这限制了它们的应用。

Method: 利用基于概念的解释来识别和抑制最具影响力的概念激活向量，从而在中和 patch 效应，而无需进行显式检测。

Result: 在 Imagenette 数据集和 ResNet-50 模型上进行评估，该方法在鲁棒准确率和干净准确率方面均优于最先进的 PatchCleanser 方法，并且在不同大小和位置的 patch 下仍能保持强大的性能。

Conclusion: 研究结果强调了解释性与鲁棒性相结合的潜力，并提出基于概念的防御方法是保护机器学习模型免受对抗性 patch 攻击的可扩展策略。

Abstract: Adversarial patch attacks pose a practical threat to deep learning models by
forcing targeted misclassifications through localized perturbations, often
realized in the physical world. Existing defenses typically assume prior
knowledge of patch size or location, limiting their applicability. In this
work, we propose a patch-agnostic defense that leverages concept-based
explanations to identify and suppress the most influential concept activation
vectors, thereby neutralizing patch effects without explicit detection.
Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and
clean accuracy than the state-of-the-art PatchCleanser, while maintaining
strong performance across varying patch sizes and locations. Our results
highlight the promise of combining interpretability with robustness and suggest
concept-driven defenses as a scalable strategy for securing machine learning
models against adversarial patch attacks.

</details>


### [105] [Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition](https://arxiv.org/abs/2510.04282)
*Yu Kiu,Lau,Chao Chen,Ge Jin,Chen Feng*

Main category: cs.CV

TL;DR: Adapt-STformer是一种创新的顺序视觉地点识别方法，它使用一种新颖的循环可变形Transformer编码器（Recurrent-DTE）来解决现有方法的性能、灵活性和效率之间的矛盾。该方法通过迭代循环机制融合多帧信息，自然支持可变的序列长度、实现快速推理和低内存占用。实验证明，Adapt-STformer在提高召回率的同时，显著减少了序列提取时间和内存使用量。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的顺序视觉地点识别（Seq-VPR）方法在追求高性能的同时，牺牲了灵活性和效率，无法满足实时应用对可变序列长度、快速推理和低内存占用的需求。

Method: 提出了一种名为Adapt-STformer的Seq-VPR方法，其核心是新颖的循环可变形Transformer编码器（Recurrent-DTE）。该编码器采用迭代循环机制来融合来自多个连续帧的信息，从而实现对可变序列长度的自然支持、快速推理和低内存占用。

Result: 与现有方法相比，Adapt-STformer在Nordland、Oxford和NuScenes数据集上的实验结果显示，其召回率提高了17%，序列提取时间减少了36%，内存使用量降低了35%。

Conclusion: Adapt-STformer成功地解决了现有Seq-VPR方法在灵活性和效率方面的不足，在保持甚至提升性能的同时，显著提高了效率，满足了实时应用的需求。

Abstract: Sequential Visual Place Recognition (Seq-VPR) leverages transformers to
capture spatio-temporal features effectively; however, existing approaches
prioritize performance at the expense of flexibility and efficiency. In
practice, a transformer-based Seq-VPR model should be flexible to the number of
frames per sequence (seq-length), deliver fast inference, and have low memory
usage to meet real-time constraints. To our knowledge, no existing
transformer-based Seq-VPR method achieves both flexibility and efficiency. To
address this gap, we propose Adapt-STformer, a Seq-VPR method built around our
novel Recurrent Deformable Transformer Encoder (Recurrent-DTE), which uses an
iterative recurrent mechanism to fuse information from multiple sequential
frames. This design naturally supports variable seq-lengths, fast inference,
and low memory usage. Experiments on the Nordland, Oxford, and NuScenes
datasets show that Adapt-STformer boosts recall by up to 17% while reducing
sequence extraction time by 36% and lowering memory usage by 35% compared to
the second-best baseline.

</details>


### [106] [ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://arxiv.org/abs/2510.04290)
*Jay Zhangjie Wu,Xuanchi Ren,Tianchang Shen,Tianshi Cao,Kai He,Yifan Lu,Ruiyuan Gao,Enze Xie,Shiyi Lan,Jose M. Alvarez,Jun Gao,Sanja Fidler,Zian Wang,Huan Ling*

Main category: cs.CV

TL;DR: ChronoEdit 通过将图像编辑视为视频生成问题来解决物理一致性问题，利用预训练的视频模型和时间推理来确保编辑对象的连贯性，并在 PBench-Edit 基准测试中取得了优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 图像编辑和生成中存在物理一致性缺失的问题，尤其是在世界模拟任务中。

Method: ChronoEdit 将输入和编辑后的图像视为视频的首尾帧，利用预训练的视频生成模型来捕捉外观和时间连贯性，并通过推理令牌在推理时进行显式编辑，以实现物理上可行的变换。

Result: ChronoEdit 在视觉保真度和物理可信度方面均超越了现有基线。

Conclusion: ChronoEdit 框架通过将图像编辑问题转化为视频生成问题，并引入时间推理阶段，有效解决了图像编辑中的物理一致性问题，并在新提出的 PBench-Edit 基准测试中取得了显著的成果。

Abstract: Recent advances in large generative models have significantly advanced image
editing and in-context image generation, yet a critical gap remains in ensuring
physical consistency, where edited objects must remain coherent. This
capability is especially vital for world simulation related tasks. In this
paper, we present ChronoEdit, a framework that reframes image editing as a
video generation problem. First, ChronoEdit treats the input and edited images
as the first and last frames of a video, allowing it to leverage large
pretrained video generative models that capture not only object appearance but
also the implicit physics of motion and interaction through learned temporal
consistency. Second, ChronoEdit introduces a temporal reasoning stage that
explicitly performs editing at inference time. Under this setting, the target
frame is jointly denoised with reasoning tokens to imagine a plausible editing
trajectory that constrains the solution space to physically viable
transformations. The reasoning tokens are then dropped after a few steps to
avoid the high computational cost of rendering a full video. To validate
ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for
contexts that require physical consistency, and demonstrate that ChronoEdit
surpasses state-of-the-art baselines in both visual fidelity and physical
plausibility. Code and models for both the 14B and 2B variants of ChronoEdit
will be released on the project page:
https://research.nvidia.com/labs/toronto-ai/chronoedit

</details>


### [107] [CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment](https://arxiv.org/abs/2510.04312)
*Vida Adeli,Ivan Klabucar,Javad Rajabi,Benjamin Filtjens,Soroush Mehraban,Diwei Wang,Hyewon Seo,Trung-Hieu Hoang,Minh N. Do,Candice Muller,Claudia Oliveira,Daniel Boari Coelho,Pieter Ginis,Moran Gilat,Alice Nieuwboer,Joke Spildooren,Lucas Mckay,Hyeokhyen Kwon,Gari Clifford,Christine Esper,Stewart Factor,Imari Genias,Amirhossein Dadashzadeh,Leia Shum,Alan Whone,Majid Mirmehdi,Andrea Iaboni,Babak Taati*

Main category: cs.CV

TL;DR: CARE-PD是最大的公开可用3D网格步态数据集，用于帕金森病（PD）研究，并支持临床评分预测和运动预训练任务，在临床相关性和模型泛化方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 帕金森病（PD）的客观步态评估受限于缺乏大型、多样化和经过临床注释的运动数据集。

Method: 构建了CARE-PD数据集，包含来自8个临床中心的9个队列的RGB视频或运动捕捉数据，并通过标准化的预处理流程转换为匿名的SMPL网格。支持监督临床评分预测（UPDRS步态评分）和无监督运动预训练任务（2D-3D关键点提升和全身3D重建）。评估了四种泛化协议下的临床预测能力，并比较了先进的运动编码器与传统步态特征基线。

Result: 先进的运动编码器在所有评估中持续优于手工特征。在CARE-PD上预训练能显著降低MPJPE（从60.8mm降至7.5mm），并提高PD严重程度的宏观F1分数17个百分点。

Conclusion: CARE-PD数据集的构建和使用，证明了临床策展、多样化训练数据对于提升PD步态分析模型性能和泛化能力至关重要。

Abstract: Objective gait assessment in Parkinson's Disease (PD) is limited by the
absence of large, diverse, and clinically annotated motion datasets. We
introduce CARE-PD, the largest publicly available archive of 3D mesh gait data
for PD, and the first multi-site collection spanning 9 cohorts from 8 clinical
centers. All recordings (RGB video or motion capture) are converted into
anonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD
supports two key benchmarks: supervised clinical score prediction (estimating
Unified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised
motion pretext tasks (2D-to-3D keypoint lifting and full-body 3D
reconstruction). Clinical prediction is evaluated under four generalization
protocols: within-dataset, cross-dataset, leave-one-dataset-out, and
multi-dataset in-domain adaptation. To assess clinical relevance, we compare
state-of-the-art motion encoders with a traditional gait-feature baseline,
finding that encoders consistently outperform handcrafted features. Pretraining
on CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1
by 17 percentage points, underscoring the value of clinically curated, diverse
training data. CARE-PD and all benchmark code are released for non-commercial
research at https://neurips2025.care-pd.ca/.

</details>


### [108] [GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction](https://arxiv.org/abs/2510.04315)
*Jiarui Ouyang,Yihui Wang,Yihang Gao,Yingxue Xu,Shu Yang,Hao Chen*

Main category: cs.CV

TL;DR: GenAR是一个多尺度自回归框架，可以从H&E图像预测空间转录组数据，通过将基因表达建模为离散标记生成来解决现有方法的不足，并在四个空间转录组学数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的计算方法在从H&E图像预测空间转录组（ST）数据时，存在单独预测每个基因（忽略共表达结构）和将任务视为连续回归（与表达是离散计数的本质不符）的问题，这可能导致生物学上不可信的输出并使下游分析复杂化。

Method: GenAR是一个多尺度自回归框架，通过以下方式改进预测：1. 将基因聚类成层级分组以揭示交叉基因依赖性。2. 将基因表达建模为无码本的离散标记生成，以直接预测原始计数。3. 通过融合组织学和空间嵌入来条件化解码。

Result: 在四个不同组织类型的空间转录组学数据集上进行的广泛实验表明，GenAR 实现了最先进的性能。

Conclusion: GenAR 通过将基因表达建模为离散标记生成，并利用多尺度自回归方法，可以从 H&E 图像中准确预测空间转录组数据，这在精确医疗和成本效益的分子分析方面具有潜在的应用前景。

Abstract: Spatial Transcriptomics (ST) offers spatially resolved gene expression but
remains costly. Predicting expression directly from widely available
Hematoxylin and Eosin (H&E) stained images presents a cost-effective
alternative. However, most computational approaches (i) predict each gene
independently, overlooking co-expression structure, and (ii) cast the task as
continuous regression despite expression being discrete counts. This mismatch
can yield biologically implausible outputs and complicate downstream analyses.
We introduce GenAR, a multi-scale autoregressive framework that refines
predictions from coarse to fine. GenAR clusters genes into hierarchical groups
to expose cross-gene dependencies, models expression as codebook-free discrete
token generation to directly predict raw counts, and conditions decoding on
fused histological and spatial embeddings. From an information-theoretic
perspective, the discrete formulation avoids log-induced biases and the
coarse-to-fine factorization aligns with a principled conditional
decomposition. Extensive experimental results on four Spatial Transcriptomics
datasets across different tissue types demonstrate that GenAR achieves
state-of-the-art performance, offering potential implications for precision
medicine and cost-effective molecular profiling. Code is publicly available at
https://github.com/oyjr/genar.

</details>


### [109] [Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction](https://arxiv.org/abs/2510.04365)
*Yuhao Luo,Yuang Zhang,Kehua Chen,Xinyu Zheng,Shucheng Zhang,Sikai Chen,Yinhai Wang*

Main category: cs.CV

TL;DR: 本研究提出了一种名为Diffusion^2的新框架，用于预测行人轨迹，特别是在数据不足的极端情况下。该框架包含两个串联的扩散模型，分别用于预测历史轨迹（向后）和未来轨迹（向前），并引入了双头参数化机制来估计不确定性以及一个自适应噪声模块来调整噪声尺度。


<details>
  <summary>Details</summary>
Motivation: 当前行人轨迹预测方法在数据不足（例如行人突然出现在盲区）的极端情况下预测精度不高，增加了交通事故的风险，因此研究在极端情况下行人轨迹预测方法对提升交通安全至关重要。

Method: 提出了一种名为Diffusion^2的新框架，该框架包含两个串联的扩散模型：一个用于向后预测，生成未观察到的历史轨迹；另一个用于向前预测，预测未来轨迹。为了处理生成历史轨迹可能引入的噪声，研究者提出了双头参数化机制来估计不确定性，并设计了一个自适应噪声模块来动态调整前向扩散过程中的噪声尺度。

Result: Diffusion^2在ETH/UCY和Stanford Drone数据集的瞬间轨迹预测任务上刷新了最先进的性能记录。

Conclusion: Diffusion^2在处理数据不充足的行人轨迹预测问题上表现出色，通过结合历史轨迹生成和未来轨迹预测，并引入不确定性估计和自适应噪声机制，有效提升了预测精度，为提高交通安全提供了新的解决方案。

Abstract: Accurate pedestrian trajectory prediction is crucial for ensuring safety and
efficiency in autonomous driving and human-robot interaction scenarios. Earlier
studies primarily utilized sufficient observational data to predict future
trajectories. However, in real-world scenarios, such as pedestrians suddenly
emerging from blind spots, sufficient observational data is often unavailable
(i.e. momentary trajectory), making accurate prediction challenging and
increasing the risk of traffic accidents. Therefore, advancing research on
pedestrian trajectory prediction under extreme scenarios is critical for
enhancing traffic safety. In this work, we propose a novel framework termed
Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists
of two sequentially connected diffusion models: one for backward prediction,
which generates unobserved historical trajectories, and the other for forward
prediction, which forecasts future trajectories. Given that the generated
unobserved historical trajectories may introduce additional noise, we propose a
dual-head parameterization mechanism to estimate their aleatoric uncertainty
and design a temporally adaptive noise module that dynamically modulates the
noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a
new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford
Drone datasets.

</details>


### [110] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: MorphoSim 是一个语言引导的框架，可以生成具有多视图一致性和对象级控件的 4D 场景。


<details>
  <summary>Details</summary>
Motivation: 可控和可编辑的时空环境对于机器人技术非常有价值，但现有的文本到视频模型仅限于 2D 视图且交互性有限。

Method: MorphoSim 整合了轨迹引导生成和特征场蒸馏，允许在不完全重新生成的情况下进行交互式编辑。

Result: 实验表明，MorphoSim 在保持高场景保真度的同时，实现了可控性和可编辑性。

Conclusion: MorphoSim 提供了一个强大的解决方案，用于生成和操作动态的 4D 机器人环境。

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [111] [Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting](https://arxiv.org/abs/2510.04401)
*Xuyang Guo,Zekai Huang,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: VLMs在计数任务上存在局限性，尤其是在处理多种形状组合时，这表明需要进一步研究。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在各种视觉-语言任务上表现出色，但它们在准确计数对象方面仍然存在不确定性，这促使了对它们计数能力的系统性研究。

Method: 提出了一个名为VLMCountBench的基准测试，该测试在极简环境下使用基本几何形状，并控制了颜色、大小和提示词等变量，以隔离和研究计数任务。

Result: 实验表明，VLMs在只存在一种形状时计数可靠，但在组合多种形状时会出现明显的计数错误。

Conclusion: 现有的VLMs在组合计数方面存在显著的局限性，为未来的研究指明了方向。

Abstract: Vision-Language Models (VLMs) have become a central focus of today's AI
community, owing to their impressive abilities gained from training on
large-scale vision-language data from the Web. These models have demonstrated
strong performance across diverse tasks, including image understanding, video
understanding, complex visual reasoning, and embodied AI. Despite these
noteworthy successes, a fundamental question remains: Can VLMs count objects
correctly? In this paper, we introduce a simple yet effective benchmark,
VLMCountBench, designed under a minimalist setting with only basic geometric
shapes (e.g., triangles, circles) and their compositions, focusing exclusively
on counting tasks without interference from other factors. We adopt strict
independent variable control and systematically study the effects of simple
properties such as color, size, and prompt refinement in a controlled ablation.
Our empirical results reveal that while VLMs can count reliably when only one
shape type is present, they exhibit substantial failures when multiple shape
types are combined (i.e., compositional counting). This highlights a
fundamental empirical limitation of current VLMs and motivates important
directions for future research.

</details>


### [112] [CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning](https://arxiv.org/abs/2510.04410)
*Venkata Bharath Reddy Reddem,Akshay P Sarashetti,Ranjith Merugu,Amit Satish Unde*

Main category: cs.CV

TL;DR: CodeFormer++通过将盲人脸部恢复分解为三个子任务（身份保留的人脸恢复、高质量的人脸生成和动态融合身份特征与真实纹理细节），并引入可变形的人脸配准模块、纹理引导的恢复网络和深度度量学习，以解决现有方法在视觉质量和身份保真度之间权衡的问题，在视觉保真度和身份一致性方面均取得优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数盲人脸恢复方法在视觉质量和身份保真度之间存在权衡，导致身份失真或降级去除不佳。本研究旨在提出一种新的框架，以最大限度地利用生成先验来实现高质量的人脸恢复，同时保持身份。

Method: 将盲人脸恢复分解为三个子任务：(i) 身份保留的人脸恢复，(ii) 高质量的人脸生成，(iii) 动态融合身份特征与真实纹理细节。具体方法包括：(1) 提出一个基于学习的可变形人脸配准模块，以语义对齐生成的人脸和恢复的人脸；(2) 提出一个纹理引导的恢复网络，以动态提取和转移生成人脸的纹理，从而提高身份保留的恢复人脸的质量；(3) 整合深度度量学习用于盲人脸恢复，并生成信息丰富的正样本和困难负样本，以更好地融合身份保留和生成特征。

Result: 在真实世界和合成数据集上的大量实验表明，所提出的CodeFormer++在视觉保真度和身份一致性方面均取得了卓越的性能。

Conclusion: CodeFormer++是一种新颖的框架，通过将盲人脸恢复分解为三个子任务并引入三个关键贡献，有效地解决了现有方法的局限性，在保持身份的同时实现了高质量的人脸恢复。

Abstract: Blind face restoration (BFR) has attracted increasing attention with the rise
of generative methods. Most existing approaches integrate generative priors
into the restoration pro- cess, aiming to jointly address facial detail
generation and identity preservation. However, these methods often suffer from
a trade-off between visual quality and identity fidelity, leading to either
identity distortion or suboptimal degradation removal. In this paper, we
present CodeFormer++, a novel framework that maximizes the utility of
generative priors for high-quality face restoration while preserving identity.
We decompose BFR into three sub-tasks: (i) identity- preserving face
restoration, (ii) high-quality face generation, and (iii) dynamic fusion of
identity features with realistic texture details. Our method makes three key
contributions: (1) a learning-based deformable face registration module that
semantically aligns generated and restored faces; (2) a texture guided
restoration network to dynamically extract and transfer the texture of
generated face to boost the quality of identity-preserving restored face; and
(3) the integration of deep metric learning for BFR with the generation of
informative positive and hard negative samples to better fuse identity-
preserving and generative features. Extensive experiments on real-world and
synthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves
superior performance in terms of both visual fidelity and identity consistency.

</details>


### [113] [A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering](https://arxiv.org/abs/2510.04428)
*Yuanhao Zou,Shengji Jin,Andong Deng,Youpeng Zhao,Jun Wang,Chen Chen*

Main category: cs.CV

TL;DR: 通过自适应、迭代和推理的帧选择来解决视频问答中的计算成本和准确性权衡问题。


<details>
  <summary>Details</summary>
Motivation: 目前的帧选择方法在计算成本和准确性之间存在权衡：轻量级模型（如 CLIP）无法捕捉复杂查询的细微差别，而基于 VLM 的方法计算成本过高。

Method: 提出了一种名为 A.I.R. 的训练方法，用于自适应、迭代和基于推理的帧选择。它利用强大的 VLM 对复杂查询进行深入的语义分析，并通过一个成本效益高的迭代循环来处理少量高潜力帧。

Result: 在各种 VideoQA 基准测试中，A.I.R. 的性能优于现有的帧选择方法，显著提高了基础 VLM 的性能，并在计算效率方面取得了显著的成果。

Conclusion: A.I.R. 是一种有效的帧选择方法，可在不牺牲准确性的情况下显著提高计算效率。

Abstract: Effectively applying Vision-Language Models (VLMs) to Video Question
Answering (VideoQA) hinges on selecting a concise yet comprehensive set of
frames, as processing entire videos is computationally infeasible. However,
current frame selection methods face a critical trade-off: approaches relying
on lightweight similarity models, such as CLIP, often fail to capture the
nuances of complex queries, resulting in inaccurate similarity scores that
cannot reflect the authentic query-frame relevance, which further undermines
frame selection. Meanwhile, methods that leverage a VLM for deeper analysis
achieve higher accuracy but incur prohibitive computational costs. To address
these limitations, we propose A.I.R., a training-free approach for Adaptive,
Iterative, and Reasoning-based frame selection. We leverage a powerful VLM to
perform deep, semantic analysis on complex queries, and this analysis is
deployed within a cost-effective iterative loop that processes only a small
batch of the most high-potential frames at a time. Extensive experiments on
various VideoQA benchmarks demonstrate that our approach outperforms existing
frame selection methods, significantly boosts the performance of the foundation
VLM, and achieves substantial gains in computational efficiency over other
VLM-based techniques.

</details>


### [114] [REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization](https://arxiv.org/abs/2510.04450)
*Qiyuan He,Yicong Li,Haotian Ye,Jinghao Wang,Xinyao Liao,Pheng-Ann Heng,Stefano Ermon,James Zou,Angela Yao*

Main category: cs.CV

TL;DR: reAR通过引入token-wise正则化训练策略，解决了视觉自回归模型中生成器-分词器不一致性的核心瓶颈，无需更改分词器或推理流程，显著提升了模型性能，在ImageNet上降低了gFID并提高了IS，在小模型下达到了与大型扩散模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归模型在性能上落后于扩散模型，主要归因于分词器限制和栅格化排序。本研究旨在解决生成器-分词器不一致性的核心瓶颈，即自回归生成的token可能无法被分词器很好地解码。

Method: 提出reAR训练策略，通过引入token-wise正则化目标：在预测下一个token时，因果变换器不仅要恢复当前token的视觉嵌入，还要在噪声的上下文中预测目标token的嵌入。

Result: 在ImageNet上，使用标准栅格化分词器，reAR将gFID从3.02降低到1.86，IS提高到316.9。应用于高级分词器时，reAR仅用1.77亿参数就达到了1.42的gFID，性能与6.75亿参数的最先进扩散模型相当。

Conclusion: reAR是一种简单但有效的训练策略，可以解决视觉自回归模型中的生成器-分词器不一致性问题，显著提高模型性能，并能与扩散模型媲美。

Abstract: Visual autoregressive (AR) generation offers a promising path toward unifying
vision and language models, yet its performance remains suboptimal against
diffusion models. Prior work often attributes this gap to tokenizer limitations
and rasterization ordering. In this work, we identify a core bottleneck from
the perspective of generator-tokenizer inconsistency, i.e., the AR-generated
tokens may not be well-decoded by the tokenizer. To address this, we propose
reAR, a simple training strategy introducing a token-wise regularization
objective: when predicting the next token, the causal transformer is also
trained to recover the visual embedding of the current token and predict the
embedding of the target token under a noisy context. It requires no changes to
the tokenizer, generation order, inference pipeline, or external models.
Despite its simplicity, reAR substantially improves performance. On ImageNet,
it reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard
rasterization-based tokenizer. When applied to advanced tokenizers, it achieves
a gFID of 1.42 with only 177M parameters, matching the performance with larger
state-of-the-art diffusion models (675M).

</details>


### [115] [SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2510.04472)
*Baber Jan,Saeed Anwar,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais*

Main category: cs.CV

TL;DR: SPEGNet 通过统一设计解决了碎片化问题，集成了多尺度特征，实现了实时推理速度，并在 CAMO、COD10K 和 NC4K 数据集上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有物体检测方法依赖于累积复杂的组件，如边界模块、注意力机制和多尺度处理器，这增加了计算负担，并且为了处理这种复杂性而降低分辨率，丢失了伪装检测的关键细节。

Method: SPEGNet 采用统一设计，通过通道校准和空间增强集成多尺度特征，并通过渐进式细化实现尺度自适应边缘调制。

Result: SPEGNet 在 CAMO、COD10K 和 NC4K 数据集上分别取得了 0.887、0.890 和 0.895 的 $S_	ext{a}$ 指标，并实现了实时推理速度，能够处理不同尺度、遮挡和模糊边界的物体。

Conclusion: SPEGNet 通过统一设计解决了碎片化问题，有效地集成了多尺度特征，实现了高精度的边界检测和区域一致性，在各种挑战性场景下都表现出色。

Abstract: Camouflaged object detection segments objects with intrinsic similarity and
edge disruption. Current detection methods rely on accumulated complex
components. Each approach adds components such as boundary modules, attention
mechanisms, and multi-scale processors independently. This accumulation creates
a computational burden without proportional gains. To manage this complexity,
they process at reduced resolutions, eliminating fine details essential for
camouflage. We present SPEGNet, addressing fragmentation through a unified
design. The architecture integrates multi-scale features via channel
calibration and spatial enhancement. Boundaries emerge directly from
context-rich representations, maintaining semantic-spatial alignment.
Progressive refinement implements scale-adaptive edge modulation with peak
influence at intermediate resolutions. This design strikes a balance between
boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\alpha$
on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.
Our approach excels across scales, from tiny, intricate objects to large,
pattern-similar ones, while handling occlusion and ambiguous boundaries. Code,
model weights, and results are available on
\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.

</details>


### [116] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: MedCLM是一个自动化流水线，通过链接病灶框、器官分割和结构化解释，将检测数据集转换为具有思维链（CoT）推理的大规模医学视觉问答（VQA）数据。该流水线使医学视觉-语言模型能够生成带有逐步推理的问题-答案对，并在多个医学VQA基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 弥合临床诊断推理与人工智能在医学影像领域的差距。

Method: MedCLM流水线将检测数据集转换为包含思维链（CoT）推理的大规模医学视觉问答（VQA）数据，通过链接病灶框到器官分割和结构化解释来实现。该方法还提出了一种集成的CoT-课程策略，包括显式病灶框的简单阶段、鼓励隐式定位的 medium 阶段和弱监督推理的困难阶段。

Result: MedCLM在多个医学VQA基准测试中取得了最先进的性能。

Conclusion: MedCLM提供了一个可扩展的框架，用于开发与临床对齐的医学视觉-语言模型。

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [117] [VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery](https://arxiv.org/abs/2510.04479)
*Nonghai Zhang,Zeyu Zhang,Jiazi Wang,Yang Zhao,Hao Tang*

Main category: cs.CV

TL;DR: 本研究提出了VaseVQA-3D数据集和VaseVLM模型，以解决现有视觉-语言模型在处理3D古希腊陶器等专业文化遗产领域时面临的数据稀疏和领域知识不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在处理3D古希腊陶器等专业文化遗产领域时，由于数据稀疏和缺乏领域知识，表现不佳。

Method: 创建了首个针对古希腊陶器分析的3D视觉问答数据集（VaseVQA-3D），包含664个3D陶器模型和相应的问答数据。同时，开发了VaseVLM模型，并进行了领域自适应训练。

Result: 在VaseVQA-3D数据集上，提出的方法在R@1指标上提高了12.8%，在词汇相似度上提高了6.6%，显著优于先前最先进的方法。

Conclusion: 本研究成功提高了对3D陶器文物的识别和理解能力，为数字遗产保护研究提供了新的技术途径。

Abstract: Vision-Language Models (VLMs) have achieved significant progress in
multimodal understanding tasks, demonstrating strong capabilities particularly
in general tasks such as image captioning and visual reasoning. However, when
dealing with specialized cultural heritage domains like 3D vase artifacts,
existing models face severe data scarcity issues and insufficient domain
knowledge limitations. Due to the lack of targeted training data, current VLMs
struggle to effectively handle such culturally significant specialized tasks.
To address these challenges, we propose the VaseVQA-3D dataset, which serves as
the first 3D visual question answering dataset for ancient Greek pottery
analysis, collecting 664 ancient Greek vase 3D models with corresponding
question-answer data and establishing a complete data construction pipeline. We
further develop the VaseVLM model, enhancing model performance in vase artifact
analysis through domain-adaptive training. Experimental results validate the
effectiveness of our approach, where we improve by 12.8% on R@1 metrics and by
6.6% on lexical similarity compared with previous state-of-the-art on the
VaseVQA-3D dataset, significantly improving the recognition and understanding
of 3D vase artifacts, providing new technical pathways for digital heritage
preservation research.

</details>


### [118] [TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement](https://arxiv.org/abs/2510.04483)
*Hao Fang,Zechao Zhan,Weixin Feng,Ziwei Huang,XuBin Li,Tiezheng Ge*

Main category: cs.CV

TL;DR: TBStar-Edit 是一个针对电商领域图像编辑的新模型，解决了通用模型在该场景下的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 通用图像编辑模型在电商场景下存在一致性问题，需要专门的模型来解决。

Method: TBStar-Edit 通过数据工程、分层模型架构（基础模型、模式迁移模块、一致性增强模块）和两阶段训练策略（先进行编辑模式迁移，再进行一致性增强）来提高图像编辑的精确度和保真度，同时保持产品外观和布局的完整性。

Result: 在自建的电商基准测试中，TBStar-Edit 在客观指标（VIE Score）和主观用户偏好方面均优于现有的通用领域编辑模型。

Conclusion: TBStar-Edit 成功地解决了电商图像编辑的一致性问题，并在评估中展现出优越性能。

Abstract: Recent advances in image generation and editing technologies have enabled
state-of-the-art models to achieve impressive results in general domains.
However, when applied to e-commerce scenarios, these general models often
encounter consistency limitations. To address this challenge, we introduce
TBStar-Edit, an new image editing model tailored for the e-commerce domain.
Through rigorous data engineering, model architecture design and training
strategy, TBStar-Edit achieves precise and high-fidelity image editing while
maintaining the integrity of product appearance and layout. Specifically, for
data engineering, we establish a comprehensive data construction pipeline,
encompassing data collection, construction, filtering, and augmentation, to
acquire high-quality, instruction-following, and strongly consistent editing
data to support model training. For model architecture design, we design a
hierarchical model framework consisting of a base model, pattern shifting
modules, and consistency enhancement modules. For model training, we adopt a
two-stage training strategy to enhance the consistency preservation: first
stage for editing pattern shifting, and second stage for consistency
enhancement. Each stage involves training different modules with separate
datasets. Finally, we conduct extensive evaluations of TBStar-Edit on a
self-proposed e-commerce benchmark, and the results demonstrate that
TBStar-Edit outperforms existing general-domain editing models in both
objective metrics (VIE Score) and subjective user preference.

</details>


### [119] [Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation](https://arxiv.org/abs/2510.04504)
*Zijing Hu,Yunze Tong,Fengda Zhang,Junkun Yuan,Jun Xiao,Kun Kuang*

Main category: cs.CV

TL;DR: 异步扩散模型通过为不同像素分配不同的时间步长来改进文本到图像的生成，从而提高与提示的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在生成高质量图像方面表现出色，但在将生成的图像与输入提示进行精确对齐方面存在不足，这是由于同步去噪导致提示相关区域在生成过程中缺乏清晰的上下文信息。

Method: 提出了一种新颖的异步扩散模型框架，该框架为不同像素分配不同的时间步长，并重新构建了逐像素的去噪过程。通过动态调整单个像素的时间步长表，优先对提示相关区域进行更渐进式的去噪处理，从而利用更清晰的像素间上下文信息。

Result: 通过大量实验证明，所提出的异步扩散模型能够显著提高在不同提示下的文本到图像对齐效果。

Conclusion: 异步扩散模型通过异步去噪机制，有效解决了现有扩散模型在文本到图像对齐方面的局限性，能够生成与输入提示更精确对齐的图像。

Abstract: Diffusion models have achieved impressive results in generating high-quality
images. Yet, they often struggle to faithfully align the generated images with
the input prompts. This limitation arises from synchronous denoising, where all
pixels simultaneously evolve from random noise to clear images. As a result,
during generation, the prompt-related regions can only reference the unrelated
regions at the same noise level, failing to obtain clear context and ultimately
impairing text-to-image alignment. To address this issue, we propose
asynchronous diffusion models -- a novel framework that allocates distinct
timesteps to different pixels and reformulates the pixel-wise denoising
process. By dynamically modulating the timestep schedules of individual pixels,
prompt-related regions are denoised more gradually than unrelated regions,
thereby allowing them to leverage clearer inter-pixel context. Consequently,
these prompt-related regions achieve better alignment in the final images.
Extensive experiments demonstrate that our asynchronous diffusion models can
significantly improve text-to-image alignment across diverse prompts. The code
repository for this work is available at https://github.com/hu-zijing/AsynDM.

</details>


### [120] [TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling](https://arxiv.org/abs/2510.04533)
*Hyunmin Cho,Donghoon Ahn,Susung Hong,Jee Eun Kim,Seungryong Kim,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: TAG是一种高效直接的引导方法，通过放大量化分数切向分量来纠正采样轨迹，提高图像生成质量，且无需修改扩散模型或引入额外计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在生成图像时存在语义不一致或幻觉问题，而现有的引导方法计算开销大。

Method: TAG利用中间样本作为投影基，通过放大量化分数切向分量来纠正采样轨迹。该方法基于一阶泰勒展开，可以使采样状态向更高概率区域移动。

Result: TAG是一种即插即用、与模型结构无关的模块，能够以最小的计算量增加提高扩散采样保真度。

Conclusion: TAG提供了一种新的扩散模型引导视角，能够有效解决现有扩散模型存在的生成问题。

Abstract: Recent diffusion models achieve the state-of-the-art performance in image
generation, but often suffer from semantic inconsistencies or hallucinations.
While various inference-time guidance methods can enhance generation, they
often operate indirectly by relying on external signals or architectural
modifications, which introduces additional computational overhead. In this
paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and
direct guidance method that operates solely on trajectory signals without
modifying the underlying diffusion model. TAG leverages an intermediate sample
as a projection basis and amplifies the tangential components of the estimated
scores with respect to this basis to correct the sampling trajectory. We
formalize this guidance process by leveraging a first-order Taylor expansion,
which demonstrates that amplifying the tangential component steers the state
toward higher-probability regions, thereby reducing inconsistencies and
enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module
that improves diffusion sampling fidelity with minimal computational addition,
offering a new perspective on diffusion guidance.

</details>


### [121] [Conditional Representation Learning for Customized Tasks](https://arxiv.org/abs/2510.04564)
*Honglin Liu,Chao Sun,Peng Hu,Yunfan Li,Xi Peng*

Main category: cs.CV

TL;DR: 提出条件表示学习（CRL），用于根据用户自定义的标准提取表示，解决通用表示与特定下游任务不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 通用表示学习方法可能无法满足特定下游任务的需求，例如在动物栖息地分析中，通用表示侧重于类别语义，而研究人员更关注场景特征，导致效果不佳。现有的监督微调方法计算和标注成本高昂。

Method: CRL利用大型语言模型（LLM）生成描述性文本来构建语义基础，然后利用视觉语言模型（VLM）将图像表示投影到这个条件特征空间中，以适应用户指定的标准。

Result: 实验结果表明，CRL在分类和检索任务上优于现有方法，并具有通用性。

Conclusion: CRL能够提取出更好地捕捉特定标准语义的表示，并可用于多种自定义任务，有效解决了通用表示与下游任务不匹配以及监督微调成本高的问题。

Abstract: Conventional representation learning methods learn a universal representation
that primarily captures dominant semantics, which may not always align with
customized downstream tasks. For instance, in animal habitat analysis,
researchers prioritize scene-related features, whereas universal embeddings
emphasize categorical semantics, leading to suboptimal results. As a solution,
existing approaches resort to supervised fine-tuning, which however incurs high
computational and annotation costs. In this paper, we propose Conditional
Representation Learning (CRL), aiming to extract representations tailored to
arbitrary user-specified criteria. Specifically, we reveal that the semantics
of a space are determined by its basis, thereby enabling a set of descriptive
words to approximate the basis for a customized feature space. Building upon
this insight, given a user-specified criterion, CRL first employs a large
language model (LLM) to generate descriptive texts to construct the semantic
basis, then projects the image representation into this conditional feature
space leveraging a vision-language model (VLM). The conditional representation
better captures semantics for the specific criterion, which could be utilized
for multiple customized tasks. Extensive experiments on classification and
retrieval tasks demonstrate the superiority and generality of the proposed CRL.
The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.

</details>


### [122] [Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior](https://arxiv.org/abs/2510.04587)
*Sheng Wang,Ruiming Wu,Charles Herndon,Yihang Liu,Shunsuke Koga,Jeanne Shen,Zhi Huang*

Main category: cs.CV

TL;DR: 本研究通过AI日志记录器和Pathology-CoT数据集，解决了全切片图像病理诊断中缺乏指导性数据的问题，并构建了一个名为Pathologist-o3的两阶段模型，在淋巴结转移检测任务上取得了优于现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的病理诊断AI模型在实际应用中缺乏能够指导下一步观察、调整放大倍数并提供可解释诊断的智能系统，主要瓶颈在于缺乏大规模、临床对齐的专家观察行为数据。

Method: 引入AI Session Recorder，记录标准WSI查看器操作，将其转换为离散的观察命令和边界框；通过轻量级的人工审核，将AI生成的初步诊断理由转化为Pathology-CoT数据集，提供“看哪里”和“为什么重要”的监督信号；基于该行为数据，构建了一个两阶段的Pathologist-o3模型。

Result: 在胃肠道淋巴结转移检测任务上，Pathologist-o3实现了84.5%的精确率、100.0%的召回率和75.4%的准确率，超过了OpenAI o3模型，并具备跨骨干的泛化能力。

Conclusion: 本研究提出的框架能够将日常的查看日志转化为可扩展、经专家验证的监督数据，使得AI驱动的病理诊断成为可能，并为构建人类对齐、可升级的临床AI奠定了基础。

Abstract: Diagnosing a whole-slide image is an interactive, multi-stage process
involving changes in magnification and movement between fields. Although recent
pathology foundation models are strong, practical agentic systems that decide
what field to examine next, adjust magnification, and deliver explainable
diagnoses are still lacking. The blocker is data: scalable, clinically aligned
supervision of expert viewing behavior that is tacit and experience-based, not
written in textbooks or online, and therefore absent from large language model
training. We introduce the AI Session Recorder, which works with standard WSI
viewers to unobtrusively record routine navigation and convert the viewer logs
into standardized behavioral commands (inspect or peek at discrete
magnifications) and bounding boxes. A lightweight human-in-the-loop review
turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired
"where to look" and "why it matters" supervision produced at roughly six times
lower labeling time. Using this behavioral data, we build Pathologist-o3, a
two-stage agent that first proposes regions of interest and then performs
behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,
it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the
state-of-the-art OpenAI o3 model and generalizing across backbones. To our
knowledge, this constitutes one of the first behavior-grounded agentic systems
in pathology. Turning everyday viewer logs into scalable, expert-validated
supervision, our framework makes agentic pathology practical and establishes a
path to human-aligned, upgradeable clinical AI.

</details>


### [123] [A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification](https://arxiv.org/abs/2510.04628)
*Hao Liu,Yunhao Gao,Wei Li,Mingyang Zhang,Maoguo Gong,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 本文提出了一种名为S$^2$Fin的空间-光谱-频率交互网络，通过融合多模态遥感图像中的空间、光谱和频率域特征，并采用高频稀疏增强Transformer和两级空间-频率融合策略，以提升遥感图像分类性能，尤其在标注数据有限的情况下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多种特征融合技术在处理异构和冗余多模态遥感图像时，难以有效提取结构和细节特征，因此需要引入频率域学习来捕捉关键和稀疏的细节特征。

Method: 提出空间-光谱-频率交互网络（S$^2$Fin），包含跨空间、光谱和频率域的成对融合模块。具体来说，设计了高频稀疏增强Transformer，利用稀疏的空谱注意力优化高频滤波器参数。然后，引入了两级空间-频率融合策略：一个自适应频率通道模块融合低频结构和增强的高频细节，一个高频共振掩码通过相位相似性强调边缘。此外，还设计了空间-光谱注意力融合模块来增强中间层的特征提取。

Result: 在四个基准多模态数据集上进行了实验，结果表明S$^2$Fin在标注数据有限的情况下，分类性能优于现有的最先进方法。

Conclusion: S$^2$Fin网络通过整合空间、光谱和频率域的特征，并采用创新的高频细节提取和融合策略，有效解决了多模态遥感图像分类中特征提取的挑战，尤其在数据稀疏场景下展现出显著优势。

Abstract: Deep learning-based methods have achieved significant success in remote
sensing Earth observation data analysis. Numerous feature fusion techniques
address multimodal remote sensing image classification by integrating global
and local features. However, these techniques often struggle to extract
structural and detail features from heterogeneous and redundant multimodal
images. With the goal of introducing frequency domain learning to model key and
sparse detail features, this paper introduces the spatial-spectral-frequency
interaction network (S$^2$Fin), which integrates pairwise fusion modules across
the spatial, spectral, and frequency domains. Specifically, we propose a
high-frequency sparse enhancement transformer that employs sparse
spatial-spectral attention to optimize the parameters of the high-frequency
filter. Subsequently, a two-level spatial-frequency fusion strategy is
introduced, comprising an adaptive frequency channel module that fuses
low-frequency structures with enhanced high-frequency details, and a
high-frequency resonance mask that emphasizes sharp edges via phase similarity.
In addition, a spatial-spectral attention fusion module further enhances
feature extraction at intermediate layers of the network. Experiments on four
benchmark multimodal datasets with limited labeled data demonstrate that
S$^2$Fin performs superior classification, outperforming state-of-the-art
methods. The code is available at https://github.com/HaoLiu-XDU/SSFin.

</details>


### [124] [SFANet: Spatial-Frequency Attention Network for Deepfake Detection](https://arxiv.org/abs/2510.04630)
*Vrushank Ahire,Aniruddh Muley,Shivam Zample,Siddharth Verma,Pranav Menon,Surbhi Madan,Abhinav Dhall*

Main category: cs.CV

TL;DR: 提出一个结合Transformer和纹理特征的混合模型来检测Deepfake，以提高准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的Deepfake检测方法泛化能力不足，无法应对多种数据集和生成技术。

Method: 提出一个新颖的集成框架，结合Swin Transformers、ViTs和纹理特征。引入数据划分、顺序训练、频率划分、基于块的注意力和面部分割技术来处理数据不平衡、增强关键区域（如眼睛和嘴巴）并提高泛化能力。

Result: 在DFWild-Cup数据集（包含八个Deepfake数据集的子集）上实现了最先进的性能。

Conclusion: 混合模型能够有效应对Deepfake检测不断变化的挑战，为实际应用提供了一个健壮的解决方案。

Abstract: Detecting manipulated media has now become a pressing issue with the recent
rise of deepfakes. Most existing approaches fail to generalize across diverse
datasets and generation techniques. We thus propose a novel ensemble framework,
combining the strengths of transformer-based architectures, such as Swin
Transformers and ViTs, and texture-based methods, to achieve better detection
accuracy and robustness. Our method introduces innovative data-splitting,
sequential training, frequency splitting, patch-based attention, and face
segmentation techniques to handle dataset imbalances, enhance high-impact
regions (e.g., eyes and mouth), and improve generalization. Our model achieves
state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse
subset of eight deepfake datasets. The ensemble benefits from the
complementarity of these approaches, with transformers excelling in global
feature extraction and texturebased methods providing interpretability. This
work demonstrates that hybrid models can effectively address the evolving
challenges of deepfake detection, offering a robust solution for real-world
applications.

</details>


### [125] [Do Superpixel Segmentation Methods Influence Deforestation Image Classification?](https://arxiv.org/abs/2510.04645)
*Hugo Resende,Fabio A. Faria,Eduardo B. Neto,Isabela Borlido,Victor Sundermann,Silvio Jamil F. Guimarães,Álvaro L. Fazenda*

Main category: cs.CV

TL;DR: 该研究评估了不同图像分割方法对森林砍伐检测任务的影响，并提出通过融合分类器来提高检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统的SLIC分割方法在遥感图像分割方面可能不如其他超像素方法，本研究旨在评估包括SLIC在内的五种分割方法对森林砍伐检测任务的影响。

Method: 比较了SLIC及其他四种表现最佳的超像素分割方法在森林砍伐检测任务中的表现，并采用PyCaret AutoML库选择分类器，最后通过分类器融合（集成学习）的方法来提升模型性能。

Result: 初始结果显示，不同分割方法在模型性能上差异不大。然而，通过应用分类器融合方法后，在平衡准确度方面观察到了显著的提升。

Conclusion: 分割方法的选择以及机器学习模型的组合对于森林砍伐检测任务都至关重要，特别是分类器融合方法能够有效提升模型性能。

Abstract: Image segmentation is a crucial step in various visual applications,
including environmental monitoring through remote sensing. In the context of
the ForestEyes project, which combines citizen science and machine learning to
detect deforestation in tropical forests, image segments are used for labeling
by volunteers and subsequent model training. Traditionally, the Simple Linear
Iterative Clustering (SLIC) algorithm is adopted as the segmentation method.
However, recent studies have indicated that other superpixel-based methods
outperform SLIC in remote sensing image segmentation, and might suggest that
they are more suitable for the task of detecting deforested areas. In this
sense, this study investigated the impact of the four best segmentation
methods, together with SLIC, on the training of classifiers for the target
application. Initially, the results showed little variation in performance
among segmentation methods, even when selecting the top five classifiers using
the PyCaret AutoML library. However, by applying a classifier fusion approach
(ensemble of classifiers), noticeable improvements in balanced accuracy were
observed, highlighting the importance of both the choice of segmentation method
and the combination of machine learning-based models for deforestation
detection tasks.

</details>


### [126] [EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents](https://arxiv.org/abs/2510.04648)
*Buyuan Zhu,Shiyu Hu,Yiping Ma,Yuanming Zhang,Kang Hao Cheong*

Main category: cs.CV

TL;DR: EduPersona是一个包含1308个真实课堂对话回合的大型基准测试，涵盖了两种语言、三个科目和基于大五人格理论的十种人格类型，旨在评估和提升大型语言模型在教育场景中的主观能力，如基本连贯性、学生现实性和长期人格一致性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型越来越多地应用于教育领域，虚拟学生代理在课堂模拟和教师培训中变得至关重要。然而，它们面向课堂的主观能力在很大程度上仍未得到评估，这限制了对模型边界的理解并阻碍了可信赖的部署。

Method: 构建了一个名为EduPersona的大型基准测试，包含1308个真实课堂对话回合（12,814个师生问答轮次），并经过人格风格化处理，规模扩大到约10倍（128k轮次）。在此基础上，将难以量化的主观表现分解为三个递进的任务：基本连贯性（行为、情感、表达和声音是否与课堂背景一致）、学生现实性和长期人格一致性，从而建立了一个以教育理论和研究价值为基础的评估框架。对三种代表性的大型语言模型进行了系统性实验，并对其原始版本和在EduPersona上训练的十种人格微调变体进行了比较。

Result: 在EduPersona基准测试上进行的系统性实验显示，经过人格微调的模型在所有三个任务上都有持续且显著的平均改进：任务1（基本连贯性）+33.6%，任务2（学生现实性）+30.6%，以及任务3（长期人格一致性）+14.9%。这些改进凸显了该数据集的有效性和研究价值，同时也揭示了人格建模的异构难度。

Conclusion: EduPersona是第一个以主观能力为中心、面向课堂的基准测试，它建立了一个解耦且可验证的研究范式。该数据集和框架将开源，以支持更广泛的研究社区在推进可信赖和类似人类的教育人工智能方面取得进展。

Abstract: As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.

</details>


### [127] [MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture of Movement Experts](https://arxiv.org/abs/2510.04654)
*Andy Cǎtrunǎ,Adrian Cosma,Emilian Rǎdoi*

Main category: cs.CV

TL;DR: 该研究提出了一种名为MoME的多阶段混合运动专家模型，能够从步态序列中预测心理特征。


<details>
  <summary>Details</summary>
Motivation: 利用步态信息推断心理特征是一个有挑战且研究不足的问题。

Method: MoME模型采用分层多阶段设计，将步态周期分为四个运动复杂度阶段，并使用轻量级专家模型提取时空特征，同时利用任务特定的门控模块自适应地为不同特征和阶段的专家分配权重。

Result: 在PsyMo基准测试中，该方法在17种心理特征上表现优于现有模型，在run级别和subject级别的加权F1分数分别为37.47%和44.6%。研究还发现，加入身份识别、性别预测和BMI估计等辅助任务能进一步提升心理特征估计的准确性。

Conclusion: 研究证明了基于多任务步态学习来估计心理特征的可行性，并为未来基于运动信息的心理推断研究奠定了基础。

Abstract: Gait encodes rich biometric and behavioural information, yet leveraging the
manner of walking to infer psychological traits remains a challenging and
underexplored problem. We introduce a hierarchical Multi-Stage Mixture of
Movement Experts (MoME) architecture for multi-task prediction of psychological
attributes from gait sequences represented as 2D poses. MoME processes the
walking cycle in four stages of movement complexity, employing lightweight
expert models to extract spatio-temporal features and task-specific gating
modules to adaptively weight experts across traits and stages. Evaluated on the
PsyMo benchmark covering 17 psychological traits, our method outperforms
state-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at
the run level and 44.6% at the subject level. Our experiments show that
integrating auxiliary tasks such as identity recognition, gender prediction,
and BMI estimation further improves psychological trait estimation. Our
findings demonstrate the viability of multi-task gait-based learning for
psychological trait estimation and provide a foundation for future research on
movement-informed psychological inference.

</details>


### [128] [ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement](https://arxiv.org/abs/2510.04668)
*Habin Lim,Yeongseob Won,Juwon Seo,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: ConceptSplit通过Token-wise Value Adaptation（ToVA）和Latent Optimization for Disentangled Attention（LODA）来解决多概念文本到图像生成中的概念混合问题，实现了鲁棒的多概念个性化。


<details>
  <summary>Details</summary>
Motivation: 解决多概念文本到图像生成中的“概念混合”问题，即多个学习概念在输出图像中不希望地干扰或混合。

Method: 提出ConceptSplit框架，包含两个关键组件：1. Token-wise Value Adaptation（ToVA）：一种无合并的训练方法，仅调整交叉注意力中的值投影。2. Latent Optimization for Disentangled Attention（LODA）：在推理过程中通过优化输入潜变量来缓解注意力纠缠。

Result: 通过广泛的定性和定量实验证明，ConceptSplit能有效减轻概念间的意外干扰，实现鲁棒的多概念个性化。

Conclusion: ConceptSplit框架能够有效解决多概念文本到图像生成中的概念混合问题。

Abstract: In recent years, multi-concept personalization for text-to-image (T2I)
diffusion models to represent several subjects in an image has gained much more
attention. The main challenge of this task is "concept mixing", where multiple
learned concepts interfere or blend undesirably in the output image. To address
this issue, in this paper, we present ConceptSplit, a novel framework to split
the individual concepts through training and inference. Our framework comprises
two key components. First, we introduce Token-wise Value Adaptation (ToVA), a
merging-free training method that focuses exclusively on adapting the value
projection in cross-attention. Based on our empirical analysis, we found that
modifying the key projection, a common approach in existing methods, can
disrupt the attention mechanism and lead to concept mixing. Second, we propose
Latent Optimization for Disentangled Attention (LODA), which alleviates
attention entanglement during inference by optimizing the input latent. Through
extensive qualitative and quantitative experiments, we demonstrate that
ConceptSplit achieves robust multi-concept personalization, mitigating
unintended concept interference. Code is available at
https://github.com/KU-VGI/ConceptSplit

</details>


### [129] [Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI](https://arxiv.org/abs/2510.04705)
*Quang-Khai Bui-Tran,Minh-Toan Dinh,Thanh-Huy Nguyen,Ba-Thinh Lam,Mai-Anh Vu,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出了一种有效的肝脏分割方法，通过结合预训练模型、伪监督学习和标准化预处理，解决了多期、多供应商MRI数据中标签数据稀疏和分布不均的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的肝脏分割方法在处理多期、多供应商的MRI数据时，面临标签数据稀疏、分布不均、空间错位和序列缺失等现实问题，影响了肝脏纤维化评估的准确性。

Method: 采用了一个预训练的3D分割模型，通过微调进行适应；利用交叉伪监督进行协同训练，以发挥未标记数据的潜力；并建立了一个标准化的预处理流程。该方法无需进行空间配准，即可实现跨MRI期相和供应商的泛化能力。

Result: 所提出的标签高效基线在多期、多供应商的MRI肝脏分割任务中展现了稳健的性能，并且在标记和未标记的数据域中均表现出色。

Conclusion: 结合预训练模型的适应和伪监督学习是解决实际临床影像任务（如肝脏分割）的有效策略，尤其是在标签数据有限的情况下。

Abstract: Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis
assessment, yet labeled data is often scarce and unevenly distributed across
imaging modalities and vendor systems. We propose a label-efficient
segmentation approach that promotes cross-modality generalization under
real-world conditions, where GED4 hepatobiliary-phase annotations are limited,
non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial
misalignment and missing phases are common. Our method integrates a
foundation-scale 3D segmentation backbone adapted via fine-tuning, co-training
with cross pseudo supervision to leverage unlabeled volumes, and a standardized
preprocessing pipeline. Without requiring spatial registration, the model
learns to generalize across MRI phases and vendors, demonstrating robust
segmentation performance in both labeled and unlabeled domains. Our results
exhibit the effectiveness of our proposed label-efficient baseline for liver
segmentation in multi-phase, multi-vendor MRI and highlight the potential of
combining foundation model adaptation with co-training for real-world clinical
imaging tasks.

</details>


### [130] [ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion](https://arxiv.org/abs/2510.04706)
*Foivos Paraperas Papantoniou,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: 该研究提出了一个基于扩散模型的框架，能够忠实地根据指定的面部表情重新生成人物，同时保持身份一致性。


<details>
  <summary>Details</summary>
Motivation: 在AI驱动的故事生成中，保持身份一致性和精确控制人物表情是两个核心能力。现有方法在保持面部身份一致性方面取得了进展，但在不损害身份一致性的前提下实现细粒度的表情控制仍然是一个挑战。

Method: 提出一个基于扩散模型的框架，该框架构建在一个ID一致性的面部基础模型之上，并采用了一种组合式设计，包含一个由FLAME blendshape参数指导的表情交叉注意力模块，以实现显式控制。该模型在包含丰富表情变化的图像和视频数据上进行了训练。

Result: 该模型能够泛化到基本情绪、细微的微表情以及表情过渡，这些是先前工作所忽略的。此外，一个可插入的参考适配器通过在合成过程中从参考帧迁移外观，实现了在真实图像中的表情编辑。实验表明，该模型在定制化和身份一致性的表情生成方面优于现有方法。

Conclusion: 所提出的框架在保持身份一致性的同时，能够精确地控制人物面部表情的生成，并在各种表情变化和编辑任务上表现出色。

Abstract: Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.

</details>


### [131] [ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model](https://arxiv.org/abs/2510.04712)
*Luo Cheng,Song Siyang,Yan Siyuan,Yu Zhen,Ge Zongyuan*

Main category: cs.CV

TL;DR: ReactDiff是一个新颖的时间扩散框架，用于生成多样化且符合对话情境的面部反应，解决了现有方法在模拟真实人类反应的随机性和动态性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能模拟真实人类反应中固有的随机性和动态性，导致生成面部反应不够多样化和人性化，无法满足人机交互系统的需求。

Method: 提出了一种名为ReactDiff的新型时间扩散框架，通过引入时间面部行为运动学和面部动作单元依赖性这两个关键约束，来指导模型生成符合现实人类反应模式的面部表情，避免出现视觉上的不自然和不连贯。

Result: 在REACT2024数据集上的大量实验表明，ReactDiff在反应质量、多样性和恰当性方面均达到了最先进的水平。

Conclusion: ReactDiff通过结合面部运动学和动作单元依赖性，成功生成了多样化且符合对话情境的面部反应，克服了现有方法的局限性，并在实验中取得了优于现有技术的成果。

Abstract: The automatic generation of diverse and human-like facial reactions in dyadic
dialogue remains a critical challenge for human-computer interaction systems.
Existing methods fail to model the stochasticity and dynamics inherent in real
human reactions. To address this, we propose ReactDiff, a novel temporal
diffusion framework for generating diverse facial reactions that are
appropriate for responding to any given dialogue context. Our key insight is
that plausible human reactions demonstrate smoothness, and coherence over time,
and conform to constraints imposed by human facial anatomy. To achieve this,
ReactDiff incorporates two vital priors (spatio-temporal facial kinematics)
into the diffusion process: i) temporal facial behavioral kinematics and ii)
facial action unit dependencies. These two constraints guide the model toward
realistic human reaction manifolds, avoiding visually unrealistic jitters,
unstable transitions, unnatural expressions, and other artifacts. Extensive
experiments on the REACT2024 dataset demonstrate that our approach not only
achieves state-of-the-art reaction quality but also excels in diversity and
reaction appropriateness.

</details>


### [132] [Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction](https://arxiv.org/abs/2510.04714)
*KunHo Heo,GiHyun Kim,SuYeon Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: 本研究提出了一种新的3D语义场景图预测方法，通过优化物体和关系特征的表示能力来提高准确性。


<details>
  <summary>Details</summary>
Motivation: 以往的3D语义场景图预测方法在物体和关系特征的表示能力上存在不足，并且过度依赖图神经网络。

Method: 设计了一个高区分度的物体特征编码器，并采用对比预训练策略来解耦物体表示学习和场景图预测，同时结合几何和语义特征进行关系预测。

Result: 所提出的方法在3DSSG数据集上显著优于现有最先进的方法，并在物体分类和关系预测方面均有提升。

Conclusion: 物体特征的质量对场景图的整体准确性至关重要。所提出的方法通过改进物体特征表示和有效融合几何与语义信息，显著提高了3D语义场景图预测的性能。

Abstract: 3D Semantic Scene Graph Prediction aims to detect objects and their semantic
relationships in 3D scenes, and has emerged as a crucial technology for
robotics and AR/VR applications. While previous research has addressed dataset
limitations and explored various approaches including Open-Vocabulary settings,
they frequently fail to optimize the representational capacity of object and
relationship features, showing excessive reliance on Graph Neural Networks
despite insufficient discriminative capability. In this work, we demonstrate
through extensive analysis that the quality of object features plays a critical
role in determining overall scene graph accuracy. To address this challenge, we
design a highly discriminative object feature encoder and employ a contrastive
pretraining strategy that decouples object representation learning from the
scene graph prediction. This design not only enhances object classification
accuracy but also yields direct improvements in relationship prediction.
Notably, when plugging in our pretrained encoder into existing frameworks, we
observe substantial performance improvements across all evaluation metrics.
Additionally, whereas existing approaches have not fully exploited the
integration of relationship information, we effectively combine both geometric
and semantic features to achieve superior relationship prediction.
Comprehensive experiments on the 3DSSG dataset demonstrate that our approach
significantly outperforms previous state-of-the-art methods. Our code is
publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.

</details>


### [133] [Benchmark on Monocular Metric Depth Estimation in Wildlife Setting](https://arxiv.org/abs/2510.04723)
*Niccolò Niccoli,Lorenzo Seidenari,Ilaria Greco,Francesco Rovero*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Camera traps are widely used for wildlife monitoring, but extracting accurate
distance measurements from monocular images remains challenging due to the lack
of depth information. While monocular depth estimation (MDE) methods have
advanced significantly, their performance in natural wildlife environments has
not been systematically evaluated. This work introduces the first benchmark for
monocular metric depth estimation in wildlife monitoring conditions. We
evaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,
ZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images
with ground truth distances obtained using calibrated ChARUCO patterns. Our
results demonstrate that Depth Anything V2 achieves the best overall
performance with a mean absolute error of 0.454m and correlation of 0.962,
while methods like ZoeDepth show significant degradation in outdoor natural
environments (MAE: 3.087m). We find that median-based depth extraction
consistently outperforms mean-based approaches across all deep learning
methods. Additionally, we analyze computational efficiency, with ZoeDepth being
fastest (0.17s per image) but least accurate, while Depth Anything V2 provides
an optimal balance of accuracy and speed (0.22s per image). This benchmark
establishes performance baselines for wildlife applications and provides
practical guidance for implementing depth estimation in conservation monitoring
systems.

</details>


### [134] [ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts](https://arxiv.org/abs/2510.04739)
*Mehdi Houshmand Sarkhoosh,Frøy Øye,Henrik Nestor Sørlie,Nam Hoang Vu,Dag Johansen,Cise Midoglu,Tomas Kupka,Pål Halvorsen*

Main category: cs.CV

TL;DR: 该研究提出了一种名为ExposureEngine的端到端系统，用于在体育广播中进行准确的、考虑旋转的赞助商可见性分析。该系统使用定向边界框（OBB）来精确匹配旋转或倾斜的标志，并提供详细的可见性指标。此外，它还包含一个自然语言处理层，允许用户通过自然语言查询生成报告和摘要。


<details>
  <summary>Details</summary>
Motivation: 传统的体育广播赞助商可见性量化方法依赖于手动、主观且难以扩展的分析，而现有自动化系统依赖于轴对齐的水平边界框（HBB），在标志旋转或倾斜时会导致不准确的曝光度量。

Method: 该方法提出了一种名为ExposureEngine的端到端系统，使用定向边界框（OBB）来精确匹配屏幕上的标志，无论其方向如何。该系统还包含一个分析流程，用于计算曝光时长和屏幕覆盖率等精确可见性指标，并集成了一个语言驱动的代理层，允许用户通过自然语言查询生成报告、摘要和媒体内容。

Result: 在瑞典精英足球比赛的1,103帧图像数据集上，该模型实现了0.859的平均精度（mAP@0.5），精确率为0.96，召回率为0.87，在各种广播条件下都能稳健地定位标志。

Conclusion: ExposureEngine提供了一个完整的解决方案，包括数据集、分析工具和自然语言交互功能，实现了体育媒体中可审计、可解释的赞助商测量。

Abstract: Quantifying sponsor visibility in sports broadcasts is a critical marketing
task traditionally hindered by manual, subjective, and unscalable analysis
methods. While automated systems offer an alternative, their reliance on
axis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics
when logos appear rotated or skewed due to dynamic camera angles and
perspective distortions. This paper introduces ExposureEngine, an end-to-end
system designed for accurate, rotation-aware sponsor visibility analytics in
sports broadcasts, demonstrated in a soccer case study. Our approach predicts
Oriented Bounding Box (OBB) to provide a geometrically precise fit to each logo
regardless of the orientation on-screen. To train and evaluate our detector, we
developed a new dataset comprising 1,103 frames from Swedish elite soccer,
featuring 670 unique sponsor logos annotated with OBBs. Our model achieves a
mean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall
of 0.87, demonstrating robust performance in localizing logos under diverse
broadcast conditions. The system integrates these detections into an analytical
pipeline that calculates precise visibility metrics, such as exposure duration
and on-screen coverage. Furthermore, we incorporate a language-driven agentic
layer, enabling users to generate reports, summaries, and media content through
natural language queries. The complete system, including the dataset and the
analytics dashboard, provides a comprehensive solution for auditable and
interpretable sponsor measurement in sports media. An overview of the
ExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .

</details>


### [135] [Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection](https://arxiv.org/abs/2510.04741)
*Alina Ciocarlan,Sylvie Le Hégarat-Mascle,Sidonie Lefebvre*

Main category: cs.CV

TL;DR: AA-YOLO通过在检测头中集成统计异常检测来解决红外小目标检测中的误报问题，在多个基准测试中表现出色，并能适应数据有限、噪声和域偏移等具有挑战性的场景。


<details>
  <summary>Details</summary>
Motivation: 传统的物体检测器在红外小目标检测任务中，由于复杂背景和小目标尺寸的限制，常常产生大量的误报。本研究旨在克服这一局限性。

Method: 提出了一种名为AA-YOLO（Anomaly-Aware YOLO）的方法，该方法将统计异常检测测试集成到其检测头中，将小目标视为背景中的异常模式，从而有效控制误报率。

Result: AA-YOLO在多个红外小目标检测基准测试中取得了有竞争力的性能，并展现了在数据有限、噪声和域偏移等场景下的鲁棒性。此外，由于仅修改了检测头，该方法具有通用性，可应用于各种YOLO骨干网络（包括轻量级模型），并能集成到实例分割YOLO中，取得了良好的效果。

Conclusion: AA-YOLO是一种通用的、轻量级且有效的红外小目标检测方法，通过集成异常检测来显著减少误报，并能在资源受限的实际应用中取得良好效果。

Abstract: Infrared Small Target Detection (IRSTD) is a challenging task in defense
applications, where complex backgrounds and tiny target sizes often result in
numerous false alarms using conventional object detectors. To overcome this
limitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a
statistical anomaly detection test into its detection head. By treating small
targets as unexpected patterns against the background, AA-YOLO effectively
controls the false alarm rate. Our approach not only achieves competitive
performance on several IRSTD benchmarks, but also demonstrates remarkable
robustness in scenarios with limited training data, noise, and domain shifts.
Furthermore, since only the detection head is modified, our design is highly
generic and has been successfully applied across various YOLO backbones,
including lightweight models. It also provides promising results when
integrated into an instance segmentation YOLO. This versatility makes AA-YOLO
an attractive solution for real-world deployments where resources are
constrained. The code will be publicly released.

</details>


### [136] [Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics](https://arxiv.org/abs/2510.04753)
*Masoumeh Chapariniya,Teodora Vukovic,Sarah Ebling,Volker Dellwo*

Main category: cs.CV

TL;DR: 本研究使用Transformer模型来识别对话场景中的人物身份，发现空间信息比时间信息更具辨别力，并且结合两者可以达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 探讨在自然、面对面对话场景中，基于Transformer的架构在人物识别方面的性能。

Method: 实现并评估了一个双流框架，该框架对从CANDOR对话语料库的子集中提取的133个COCO WholeBody关键点的空间配置和时间运动模式进行单独建模。实验比较了预训练和从头训练，研究了速度特征的使用，并引入了用于分层运动建模的多尺度时间Transformer。

Result: 在CANDOR数据集上，特定领域的训练显著优于迁移学习。空间Transformer达到了95.74%的准确率，多尺度时间Transformer达到了93.90%。特征级融合将性能提高到98.03%。

Conclusion: Transformer架构在自然交互中的人物识别方面具有巨大潜力，为未来的多模态和跨文化研究提供了见解。空间配置比时间动态信息更具辨别力，而结合两者可以进一步提高性能。

Abstract: This paper investigates the performance of transformer-based architectures
for person identification in natural, face-to-face conversation scenario. We
implement and evaluate a two-stream framework that separately models spatial
configurations and temporal motion patterns of 133 COCO WholeBody keypoints,
extracted from a subset of the CANDOR conversational corpus. Our experiments
compare pre-trained and from-scratch training, investigate the use of velocity
features, and introduce a multi-scale temporal transformer for hierarchical
motion modeling. Results demonstrate that domain-specific training
significantly outperforms transfer learning, and that spatial configurations
carry more discriminative information than temporal dynamics. The spatial
transformer achieves 95.74% accuracy, while the multi-scale temporal
transformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,
confirming that postural and dynamic information are complementary. These
findings highlight the potential of transformer architectures for person
identification in natural interactions and provide insights for future
multimodal and cross-cultural studies.

</details>


### [137] [Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction](https://arxiv.org/abs/2510.04759)
*Chi Yan,Dan Xu*

Main category: cs.CV

TL;DR: PG-Occ 通过渐进式高斯注意力框架，实现了开放词汇的3D占用预测，解决了稀疏表示捕捉小物体能力不足和密集表示计算开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的3D占用预测方法要么受限于固定的语义类别，要么在文本对齐表示时存在稀疏表示捕捉小物体能力不足和密集表示计算开销大的权衡。

Method: 提出了一种名为PG-Occ的渐进式高斯注意力框架（Progressive Gaussian Transformer Framework），该框架采用渐进式在线致密化策略，逐步增强3D高斯表示以捕捉精细场景细节，并通过各向异性感知采样策略与时空融合，自适应地为不同尺度和阶段的高斯分配感受野，以实现更有效的特征聚合和更丰富的场景信息捕获。

Result: PG-Occ 在3D占用预测任务上取得了最先进的性能，与先前最佳方法相比，mIoU（mean Intersection over Union）相对提高了14.3%。

Conclusion: PG-Occ 成功地实现了开放词汇的3D占用预测，通过渐进式致密化和各向异性感知采样，有效地解决了现有方法的局限性，并在性能上取得了显著提升。

Abstract: The 3D occupancy prediction task has witnessed remarkable progress in recent
years, playing a crucial role in vision-based autonomous driving systems. While
traditional methods are limited to fixed semantic categories, recent approaches
have moved towards predicting text-aligned features to enable open-vocabulary
text queries in real-world scenes. However, there exists a trade-off in
text-aligned scene modeling: sparse Gaussian representation struggles to
capture small objects in the scene, while dense representation incurs
significant computational overhead. To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction. Our framework employs progressive
online densification, a feed-forward strategy that gradually enhances the 3D
Gaussian representation to capture fine-grained scene details. By iteratively
enhancing the representation, the framework achieves increasingly precise and
detailed scene understanding. Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture. Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method. Code and pretrained models will be
released upon publication on our project page:
https://yanchi-3dv.github.io/PG-Occ

</details>


### [138] [Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning](https://arxiv.org/abs/2510.04770)
*Xiaomeng Fan,Yuchuan Mao,Zhi Gao,Yuwei Wu,Jin Chen,Yunde Jia*

Main category: cs.CV

TL;DR: 现有的开放词汇学习方法在仅使用已知类别数据时，由于缺乏未知类别数据，估计存在固有的不确定性。本文提出了一种新颖的开放词汇学习方法，通过生成未知类别数据来估计开放环境下的数据分布，从而界定估计误差。该方法包括一个类别-域的数据生成流程和一个分布对齐算法，利用分层语义树和从已知类别数据中推断出的域信息来生成未知类别数据，进而提高分布估计的准确性。通过最大化后验概率来增强泛化能力。在11个数据集上的实验表明，该方法比基线方法提高了14%，显示了其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放词汇学习中，由于仅使用已知类别数据估计分布，而忽略了未知类别数据，导致估计误差无法辨识。

Method: 提出了一种新颖的开放词汇学习方法，该方法包括一个类别-域的数据生成流程和一个分布对齐算法。该流程利用分层语义树和从已知类别数据中推断出的域信息来生成未知类别数据，用于估计开放环境下的数据分布。分布对齐算法则通过估计和最大化后验概率来增强泛化能力。

Result: 在11个数据集上的大量实验表明，与基线方法相比，该方法的结果最高可提高14%，证明了其有效性和优越性。

Conclusion: 通过生成未知类别数据来估计开放环境下的数据分布，可以有效界定估计误差，从而提高开放词汇学习的性能。

Abstract: Open-vocabulary learning requires modeling the data distribution in open
environments, which consists of both seen-class and unseen-class data.
  Existing methods estimate the distribution in open environments using
seen-class data, where the absence of unseen classes makes the estimation error
inherently unidentifiable.
  Intuitively, learning beyond the seen classes is crucial for distribution
estimation to bound the estimation error.
  We theoretically demonstrate that the distribution can be effectively
estimated by generating unseen-class data, through which the estimation error
is upper-bounded.
  Building on this theoretical insight, we propose a novel open-vocabulary
learning method, which generates unseen-class data for estimating the
distribution in open environments. The method consists of a class-domain-wise
data generation pipeline and a distribution alignment algorithm. The data
generation pipeline generates unseen-class data under the guidance of a
hierarchical semantic tree and domain information inferred from the seen-class
data, facilitating accurate distribution estimation. With the generated data,
the distribution alignment algorithm estimates and maximizes the posterior
probability to enhance generalization in open-vocabulary learning. Extensive
experiments on $11$ datasets demonstrate that our method outperforms baseline
approaches by up to $14\%$, highlighting its effectiveness and superiority.

</details>


### [139] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: FedSurg挑战赛首次评估了外科手术视频分类中联邦学习（FL）策略的基准。挑战赛旨在测试模型在未见过的临床中心的泛化能力和通过本地微调进行适应的能力，同时避免共享患者数据。


<details>
  <summary>Details</summary>
Motivation: 评估在外科手术视频分类任务中，联邦学习（FL）方法的最新进展，重点关注其在未见过的临床中心的泛化能力和通过本地微调进行适应的能力，同时支持协作模型开发而不共享患者数据。

Method:  participants developed strategies to classify inflammation stages in appendicitis using a preliminary version of the multi-center Appendix300 video dataset. The challenge evaluated two tasks: generalization to an unseen center and center-specific adaptation after fine-tuning. Submitted approaches included foundation models with linear probing, metric learning with triplet loss, and various FL aggregation schemes (FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and Expected Cost, with ranking robustness evaluated via bootstrapping and statistical testing.

Result: 在泛化任务中，跨中心的表现有限。在适应性任务中，所有团队在微调后都有所改进，但排名稳定性较低。基于ViViT的提交获得了最强的整体性能。挑战赛凸显了泛化能力、对类别不平衡的敏感性以及去中心化训练中超参数调整的困难，同时时空建模和上下文感知预处理被证明是潜在的策略。

Conclusion: FedSurg挑战赛为评估FL策略在外科手术视频分类中的应用建立了首个基准。研究结果强调了本地个性化与全局鲁棒性之间的权衡，并突出了架构选择、预处理和损失函数设计的重要性。该基准为了解未来开发对不平衡数据敏感、适应性强且鲁棒的FL方法提供了参考。

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art
in federated learning for surgical video classification. Its goal was to assess
how well current methods generalize to unseen clinical centers and adapt
through local fine-tuning while enabling collaborative model development
without sharing patient data. Methods: Participants developed strategies to
classify inflammation stages in appendicitis using a preliminary version of the
multi-center Appendix300 video dataset. The challenge evaluated two tasks:
generalization to an unseen center and center-specific adaptation after
fine-tuning. Submitted approaches included foundation models with linear
probing, metric learning with triplet loss, and various FL aggregation schemes
(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and
Expected Cost, with ranking robustness evaluated via bootstrapping and
statistical testing. Results: In the generalization task, performance across
centers was limited. In the adaptation task, all teams improved after
fine-tuning, though ranking stability was low. The ViViT-based submission
achieved the strongest overall performance. The challenge highlighted
limitations in generalization, sensitivity to class imbalance, and difficulties
in hyperparameter tuning in decentralized training, while spatiotemporal
modeling and context-aware preprocessing emerged as promising strategies.
Conclusion: The FedSurg Challenge establishes the first benchmark for
evaluating FL strategies in surgical video classification. Findings highlight
the trade-off between local personalization and global robustness, and
underscore the importance of architecture choice, preprocessing, and loss
design. This benchmarking offers a reference point for future development of
imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [140] [Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization](https://arxiv.org/abs/2510.04781)
*Javed Ahmad,Federico Dassiè,Selene Frascella,Gabriele Marchello,Ferdinando Cannella,Arianna Traviglia*

Main category: cs.CV

TL;DR: 本研究提出了一种自动化的双机器人扫描系统，用于高保真 3D 扫描文化遗产文物，无需手动操作，提高了效率和精度。


<details>
  <summary>Details</summary>
Motivation: 为了实现文化遗产文物的高保真 3D 扫描，克服传统方法需要专业知识和手动干预的限制。

Method: 结合协调的机器人操作和高分辨率 3D 扫描，将扫描空间参数化为不同区域，并进行协调运动规划。通过优化的轨迹规划和航点分布来确保全面的表面覆盖、最小化遮挡并平衡重建精度与系统效率。

Result: 实验结果表明，与基线方法相比，该方法实现了更低的 Chamfer 距离和更高的 F 分数，在几何精度、数字化效率方面表现更优，并减少了对专业操作员的依赖。

Conclusion: 本研究提出的自动化双机器人扫描系统能够实现高保真 3D 扫描，提高效率和精度，并降低对专业操作员的依赖。

Abstract: High-fidelity 3D scanning is essential for preserving cultural heritage
artefacts, supporting documentation, analysis, and long-term conservation.
However, conventional methods typically require specialized expertise and
manual intervention to maintain optimal scanning conditions and coverage. We
present an automated two-robot scanning system that eliminates the need for
handheld or semi-automatic workflows by combining coordinated robotic
manipulation with high-resolution 3D scanning. Our system parameterizes the
scanning space into distinct regions, enabling coordinated motion planning
between a scanner-equipped robot and a tray-handling robot. Optimized
trajectory planning and waypoint distribution ensure comprehensive surface
coverage, minimize occlusions, and balance reconstruction accuracy with system
efficiency. Experimental results show that our approach achieves significantly
lower Chamfer Distance and higher F-score compared to baseline methods,
offering superior geometric accuracy, improved digitization efficiency, and
reduced reliance on expert operators.

</details>


### [141] [A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation](https://arxiv.org/abs/2510.04794)
*Alon Kaya,Igal Bilik,Inna Stainvas*

Main category: cs.CV

TL;DR: ViTs and large CNNs are compared for geometric estimation tasks in low-data settings. CNNs perform better in small data scenarios due to inductive bias, while ViTs generalize better across domains. Hybrid architectures are suggested for future research.


<details>
  <summary>Details</summary>
Motivation: To investigate the efficiency of ViTs and large-scale CNNs as backbone architectures for geometric estimation tasks, especially in low-data regimes.

Method: Systematically compared large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet) with ViT-based foundation models (CLIP-ViT variants and DINO) on tasks of estimating 2D rigid transformations and predicting the fundamental matrix, across various data size settings, including few-shot scenarios.

Result: In large downstream-data scenarios, ViTs outperform CNNs. In small data scenarios, CNNs match ViT performance due to their inductive bias and smaller capacity. ViTs show stronger generalization in cross-domain evaluation.

Conclusion: Model architecture selection is crucial for refinement tasks. Future research should explore hybrid architectures that balance local and global representations to address the trade-offs observed between CNNs and ViTs.

Abstract: Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)
have reshaped computer vision through pretrained feature representations that
enable strong transfer learning for diverse tasks. However, their efficiency as
backbone architectures for geometric estimation tasks involving image
deformations in low-data regimes remains an open question. This work considers
two such tasks: 1) estimating 2D rigid transformations between pairs of images
and 2) predicting the fundamental matrix for stereo image pairs, an important
problem in various applications, such as autonomous mobility, robotics, and 3D
scene reconstruction. Addressing this intriguing question, this work
systematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)
with ViT-based foundation models (CLIP-ViT variants and DINO) in various data
size settings, including few-shot scenarios. These pretrained models are
optimized for classification or contrastive learning, encouraging them to focus
mostly on high-level semantics. The considered tasks require balancing local
and global features differently, challenging the straightforward adoption of
these models as the backbone. Empirical comparative analysis shows that,
similar to training from scratch, ViTs outperform CNNs during refinement in
large downstream-data scenarios. However, in small data scenarios, the
inductive bias and smaller capacity of CNNs improve their performance, allowing
them to match that of a ViT. Moreover, ViTs exhibit stronger generalization in
cross-domain evaluation where the data distribution changes. These results
emphasize the importance of carefully selecting model architectures for
refinement, motivating future research towards hybrid architectures that
balance local and global representations.

</details>


### [142] [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing](https://arxiv.org/abs/2510.04797)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: DiT-VTON是一个利用Diffusion Transformer（DiT）的虚拟试穿（VTO）框架，通过探索DiT配置、扩展数据集和重新定义任务以实现更精细的细节、鲁棒性、高效采样、图像编辑和跨类别泛化。


<details>
  <summary>Details</summary>
Motivation: 解决现有VTO模型在细节保留、鲁棒性、采样效率、图像编辑和跨类别泛化方面的挑战。

Method: 利用Diffusion Transformer（DiT），探索了多种DiT配置（如in-context token concatenation、channel concatenation、ControlNet integration），并在包含多样化背景、非结构化参考和非服装类别的扩展数据集上进行训练，以增强鲁棒性。DiT-VTON还被扩展为支持虚拟试穿一切（VTA）的解决方案，并支持姿势保留、局部编辑、纹理转移和对象级定制等图像编辑功能。

Result: DiT-VTON在VITON-HD上超越了最先进的方法，实现了卓越的细节保留和鲁棒性，且无需额外的条件编码器。在包含数千种产品类别的多样化数据集上，其在VTA和图像编辑能力方面也优于其他模型。

Conclusion: DiT-VTON是一个多功能的VTO框架，通过利用DiT和创新的方法，在细节保留、鲁棒性、效率和图像编辑能力方面取得了显著的改进，并能适应广泛的产品类别。

Abstract: The rapid growth of e-commerce has intensified the demand for Virtual Try-On
(VTO) technologies, enabling customers to realistically visualize products
overlaid on their own images. Despite recent advances, existing VTO models face
challenges with fine-grained detail preservation, robustness to real-world
imagery, efficient sampling, image editing capabilities, and generalization
across diverse product categories. In this paper, we present DiT-VTON, a novel
VTO framework that leverages a Diffusion Transformer (DiT), renowned for its
performance on text-conditioned image generation, adapted here for the
image-conditioned VTO task. We systematically explore multiple DiT
configurations, including in-context token concatenation, channel
concatenation, and ControlNet integration, to determine the best setup for VTO
image conditioning.
  To enhance robustness, we train the model on an expanded dataset encompassing
varied backgrounds, unstructured references, and non-garment categories,
demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also
redefines the VTO task beyond garment try-on, offering a versatile Virtual
Try-All (VTA) solution capable of handling a wide range of product categories
and supporting advanced image editing functionalities such as pose
preservation, localized editing, texture transfer, and object-level
customization. Experimental results show that our model surpasses
state-of-the-art methods on VITON-HD, achieving superior detail preservation
and robustness without reliance on additional condition encoders. It also
outperforms models with VTA and image editing capabilities on a diverse dataset
spanning thousands of product categories.

</details>


### [143] [Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors](https://arxiv.org/abs/2510.04802)
*Han Zhang,Lalithkumar Seenivasan,Jose L. Porras,Roger D. Soberanis-Mukul,Hao Ding,Hongchao Shu,Benjamin D. Killeen,Ankita Ghosh,Lonny Yarmus,Masaru Ishii,Angela Christine Argento,Mathias Unberath*

Main category: cs.CV

TL;DR: EgoSurg是一个创新的框架，可以通过固定摄像机视频重建手术室工作人员的动态第一视角回放，从而实现对围手术期决策、培训和工作流程的深入分析。


<details>
  <summary>Details</summary>
Motivation: 目前对手术实践的观察主要依赖固定的视角或回忆，无法记录临床决策所依据的第一视角视觉信息。现有的固定摄像机视频虽然可以记录手术流程，但无法重现每个团队成员的实际视野，从而限制了对手术安全、培训和工作流程优化相关决策制定过程的理解。

Method: EgoSurg框架结合了几何驱动的神经渲染和基于扩散的视图增强技术，能够从墙壁安装的固定摄像机视频中重建任何手术室工作人员动态的第一视角回放，而无需干预临床工作流程。

Result: 通过在多地点手术案例和对照研究中的评估，EgoSurg能够以高视觉质量和保真度重建特定于个人的视野和任意视角。

Conclusion: EgoSurg框架通过将现有的手术室摄像机基础设施转化为可导航的动态3D记录，为沉浸式手术数据科学奠定了新基础，使得手术实践能够从各个角度进行可视化、体验和分析。

Abstract: Observing surgical practice has historically relied on fixed vantage points
or recollections, leaving the egocentric visual perspectives that guide
clinical decisions undocumented. Fixed-camera video can capture surgical
workflows at the room-scale, but cannot reconstruct what each team member
actually saw. Thus, these videos only provide limited insights into how
decisions that affect surgical safety, training, and workflow optimization are
made. Here we introduce EgoSurg, the first framework to reconstruct the
dynamic, egocentric replays for any operating room (OR) staff directly from
wall-mounted fixed-camera video, and thus, without intervention to clinical
workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based
view enhancement, enabling high-visual fidelity synthesis of arbitrary and
egocentric viewpoints at any moment. In evaluation across multi-site surgical
cases and controlled studies, EgoSurg reconstructs person-specific visual
fields and arbitrary viewpoints with high visual quality and fidelity. By
transforming existing OR camera infrastructure into a navigable dynamic 3D
record, EgoSurg establishes a new foundation for immersive surgical data
science, enabling surgical practice to be visualized, experienced, and analyzed
from every angle.

</details>


### [144] [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
*Benlin Liu,Amita Kamath,Madeleine Grunde-McLaughlin,Winson Han,Ranjay Krishna*

Main category: cs.CV

TL;DR: 尽管有分析VIT编码器和Transformer激活的研究，但我们仍不了解多模态语言模型（MLM）在感知密集型任务中遇到困难的原因。本文通过检查流行的MLM（LLaVA-OneVision、Qwen2.5-VL和Llama-3-LLaVA-NeXT）如何处理其视觉键值（key-value）令牌，提供了一个研究不足的视角。我们首先研究视觉信息在语言模型中的流动，发现图像值令牌编码了足够的信息来执行几项感知密集型任务（分割、语义对应、时间对应和指称表达式检测），而且无需微调。我们发现，尽管语言模型确实增强了来自输入视觉编码器投影的视觉信息（这与整体MLM的感知能力相关），但与未经MLM微调的等效视觉编码器（SigLIP）相比，其在几项任务中的视觉信息较少。此外，我们发现语言模型后期层中与输入无关的图像键令牌的视觉信息包含伪影，从而降低了整个MLM的感知能力。接下来，我们讨论了控制语言模型中的视觉信息，表明在图像输入中添加文本前缀可以提高视觉表示的感知能力。最后，我们发现，如果语言模型能够更好地控制其视觉信息，它们的感知能力将显著提高；例如，在BLINK基准测试的33.3%的艺术风格问题中，语言模型中存在的感知信息并未体现在输出中。我们的研究结果揭示了键值令牌在多模态系统中的作用，为MLM提供更深入的机制可解释性铺平了道路，并为训练其视觉编码器和语言模型组件提供了新的方向。


<details>
  <summary>Details</summary>
Motivation: 多模态语言模型（MLM）在感知密集型任务中存在不足，但其根本原因尚不明确。

Method: 研究了LLaVA-OneVision、Qwen2.5-VL和Llama-3-LLaVA-NeXT这几个流行的MLM如何处理它们的视觉键值（key-value）令牌。具体包括：1. 分析视觉信息在语言模型中的流动；2. 评估语言模型在不同任务上的表现；3. 探究文本前缀对视觉信息处理的影响；4. 分析感知信息在模型内部的流动与输出之间的关系。

Result: 1. 图像值令牌包含足够信息以支持零样本感知任务。2. MLM微调后的视觉信息增强效果不显著，甚至少于未微调的视觉编码器。3. 语言模型后期层的输入无关键令牌包含降低感知能力的伪影。4. 添加文本前缀可提升视觉表示的感知能力。5. 模型内部的感知信息未能充分体现在输出中，影响了最终表现。

Conclusion: 视觉键值令牌在多模态系统中扮演关键角色。MLM在感知任务上的局限性源于视觉信息在模型内部的处理方式，包括信息流失和伪影的产生。通过改进视觉编码器和语言模型组件的训练，特别是关注对视觉信息的控制，有望提升MLM的感知能力。

Abstract: Despite interpretability work analyzing VIT encoders and transformer
activations, we don't yet understand why Multimodal Language Models (MLMs)
struggle on perception-heavy tasks. We offer an under-studied perspective by
examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and
Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the
flow of visual information through the language model, finding that image value
tokens encode sufficient information to perform several perception-heavy tasks
zero-shot: segmentation, semantic correspondence, temporal correspondence, and
referring expression detection. We find that while the language model does
augment the visual information received from the projection of input visual
encodings-which we reveal correlates with overall MLM perception capability-it
contains less visual information on several tasks than the equivalent visual
encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that
the visual information corresponding to input-agnostic image key tokens in
later layers of language models contains artifacts which reduce perception
capability of the overall MLM. Next, we discuss controlling visual information
in the language model, showing that adding a text prefix to the image input
improves perception capabilities of visual representations. Finally, we reveal
that if language models were able to better control their visual information,
their perception would significantly improve; e.g., in 33.3% of Art Style
questions in the BLINK benchmark, perception information present in the
language model is not surfaced to the output! Our findings reveal insights into
the role of key-value tokens in multimodal systems, paving the way for deeper
mechanistic interpretability of MLMs and suggesting new directions for training
their visual encoder and language model components.

</details>


### [145] [AvatarVTON: 4D Virtual Try-On for Animatable Avatars](https://arxiv.org/abs/2510.04822)
*Zicheng Jiang,Jixin Gao,Shengfeng He,Xinzhe Li,Yulong Zheng,Zhaotong Yang,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: AvatarVTON是一个4D虚拟试衣框架，可根据单件服装图像生成逼真的试穿效果，支持姿势控制、新视角渲染和多种服装选择。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在单视角监督下实现动态服装交互，且需要多视角服装捕捉或物理先验。

Method: 提出Reciprocal Flow Rectifier用于稳定模型拟合和保持时间连贯性，并使用Non-Linear Deformer将高斯图分解为不变和特定的分量，以实现自适应的非线性服装变形。

Result: AvatarVTON在保真度、多样性和动态服装真实感方面表现出色。

Conclusion: AvatarVTON在AR/VR、游戏和数字人应用方面具有潜力。

Abstract: We propose AvatarVTON, the first 4D virtual try-on framework that generates
realistic try-on results from a single in-shop garment image, enabling free
pose control, novel-view rendering, and diverse garment choices. Unlike
existing methods, AvatarVTON supports dynamic garment interactions under
single-view supervision, without relying on multi-view garment captures or
physics priors. The framework consists of two key modules: (1) a Reciprocal
Flow Rectifier, a prior-free optical-flow correction strategy that stabilizes
avatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,
which decomposes Gaussian maps into view-pose-invariant and view-pose-specific
components, enabling adaptive, non-linear garment deformations. To establish a
benchmark for 4D virtual try-on, we extend existing baselines with unified
modules for fair qualitative and quantitative comparisons. Extensive
experiments show that AvatarVTON achieves high fidelity, diversity, and dynamic
garment realism, making it well-suited for AR/VR, gaming, and digital-human
applications.

</details>


### [146] [Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis](https://arxiv.org/abs/2510.04823)
*Arnela Hadzic,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: 本研究提出了一种基于全3D流匹配（FM）框架的生成合成CT（sCT）的方法，用于MRI或CBCT到sCT的图像转换，以支持放疗。


<details>
  <summary>Details</summary>
Motivation: 为了实现MRI-only和CBCT放疗，需要从MRI或CBCT生成合成CT（sCT）。FM因其生成高质量图像的效率而被采用。

Method: 该方法使用一个轻量级3D编码器从输入的MRI或CBCT提取特征，然后通过学习到的FM速度场将高斯噪声转换为sCT图像。

Result: 在SynthRAD2025挑战的基准上评估，该方法能够准确重建整体解剖结构，但在腹部、头颈部和胸部等三个解剖区域中，由于训练分辨率的限制，对精细细节的保持有限。

Conclusion: 该方法在生成sCT方面显示出准确重建全局解剖结构的能力，但精细细节的保持受到训练分辨率的限制。未来的工作将集中于通过基于块的训练和潜在空间流模型来提高分辨率和局部结构保真度。

Abstract: Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in
enabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment
precision while reducing patient radiation exposure. To address this task, we
adopt a fully 3D Flow Matching (FM) framework, motivated by recent work
demonstrating FM's efficiency in producing high-quality images. In our
approach, a Gaussian noise volume is transformed into an sCT image by
integrating a learned FM velocity field, conditioned on features extracted from
the input MRI or CBCT using a lightweight 3D encoder. We evaluated the method
on the SynthRAD2025 Challenge benchmark, training separate models for MRI
$\rightarrow$ sCT and CBCT $\rightarrow$ sCT across three anatomical regions:
abdomen, head and neck, and thorax. Validation and testing were performed
through the challenge submission system. The results indicate that the method
accurately reconstructs global anatomical structures; however, preservation of
fine details was limited, primarily due to the relatively low training
resolution imposed by memory and runtime constraints. Future work will explore
patch-based training and latent-space flow models to improve resolution and
local structural fidelity.

</details>


### [147] [Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation](https://arxiv.org/abs/2510.04838)
*Muquan Li,Hang Gou,Dongyang Zhang,Shuang Liang,Xiurui Xie,Deqiang Ouyang,Ke Qin*

Main category: cs.CV

TL;DR: 现有的数据集蒸馏方法通常采用随机截断策略，但忽略了神经网络在不同训练阶段（早期、中期、晚期）学习动态的差异。为此，我们提出了自动截断反向传播算法（AT-BPTT），一种根据梯度特性自适应调整截断位置和窗口大小的新框架。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法依赖于随机截断，忽略了神经网络不同训练阶段的学习动态差异，导致效果不佳。

Method: 提出AT-BPTT框架，包括：1）用于感知阶段的时间步选择概率机制；2）基于梯度变化的自适应窗口大小策略；3）用于减少计算开销的低秩Hessian近似。

Result: 在CIFAR-10、CIFAR-100、Tiny-ImageNet和ImageNet-1K上的实验表明，AT-BPTT的准确性平均比基线方法提高了6.16%，并将内循环优化速度提高了3.9倍，同时节省了63%的内存。

Conclusion: AT-BPTT通过动态适应截断策略，显著提高了数据集蒸馏的性能，同时降低了计算和内存成本。

Abstract: The growing demand for efficient deep learning has positioned dataset
distillation as a pivotal technique for compressing training dataset while
preserving model performance. However, existing inner-loop optimization methods
for dataset distillation typically rely on random truncation strategies, which
lack flexibility and often yield suboptimal results. In this work, we observe
that neural networks exhibit distinct learning dynamics across different
training stages-early, middle, and late-making random truncation ineffective.
To address this limitation, we propose Automatic Truncated Backpropagation
Through Time (AT-BPTT), a novel framework that dynamically adapts both
truncation positions and window sizes according to intrinsic gradient behavior.
AT-BPTT introduces three key components: (1) a probabilistic mechanism for
stage-aware timestep selection, (2) an adaptive window sizing strategy based on
gradient variation, and (3) a low-rank Hessian approximation to reduce
computational overhead. Extensive experiments on CIFAR-10, CIFAR-100,
Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art
performance, improving accuracy by an average of 6.16% over baseline methods.
Moreover, our approach accelerates inner-loop optimization by 3.9x while saving
63% memory cost.

</details>


### [148] [Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints](https://arxiv.org/abs/2510.04840)
*Viktor Kozák,Jan Chudoba,Libor Přeučil*

Main category: cs.CV

TL;DR: 本文提出一种基于航拍图像的自动化光伏电站测绘新方法，可精细化建模至单个光伏组件，并生成适用于运维的三维地理解算模型。


<details>
  <summary>Details</summary>
Motivation: 精确、最新的光伏电站模型对于其优化运行和维护至关重要，但此类模型不易获得。

Method: 利用航拍概览图像，通过视觉分割光伏组件并推断结构信息（组件、支架、行列），结合关键点匹配与多图像融合，构建三维地理解算模型。

Result: 实验验证了该方法在两个不同光伏电站上的有效性，生成的模型能精细到单个组件，并包含三维位置和语义结构信息。

Conclusion: 所提出的方法能够自动化光伏电站的测绘过程，生成精细化、地理解算的光伏电站模型，适用于电站运维。

Abstract: An accurate and up-to-date model of a photovoltaic (PV) power plant is
essential for its optimal operation and maintenance. However, such a model may
not be easily available. This work introduces a novel approach for PV power
plant mapping based on aerial overview images. It enables the automation of the
mapping process while removing the reliance on third-party data. The presented
mapping method takes advantage of the structural layout of the power plants to
achieve detailed modeling down to the level of individual PV modules. The
approach relies on visual segmentation of PV modules in overview images and the
inference of structural information in each image, assigning modules to
individual benches, rows, and columns. We identify visual keypoints related to
the layout and use these to merge detections from multiple images while
maintaining their structural integrity. The presented method was experimentally
verified and evaluated on two different power plants. The final fusion of 3D
positions and semantic structures results in a compact georeferenced model
suitable for power plant maintenance.

</details>


### [149] [From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements](https://arxiv.org/abs/2510.04844)
*Cheyu Lin,Katherine A. Flanigan*

Main category: cs.CV

TL;DR: 该研究提出了一种从3D骨骼数据中推断人体动态关系（称为“动态学”）的框架，利用ST-GCN和CNN模型，通过迁移学习来理解用户心理状态，并在DUET数据集上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在环境心理学和强化学习等领域，理解人类与建筑环境之间的动态关系是一个关键挑战，但现有方法难以同时满足通用性和隐私保护的要求来模拟人类心理状态。

Method: 提出了一种结合了空间-时间图卷积网络（ST-GCN）和卷积神经网络（CNN）的动态学识别框架，利用迁移学习直接从3D骨骼数据中推断动态学，以捕捉与认知和情感状态相关的潜在身体运动结构，同时保护用户隐私。

Result: 在Dyadic User EngagemenT (DUET) 数据集上的实验结果表明，该方法能够实现可扩展、准确且以用户为中心的行为建模。

Conclusion: 该研究为增强强化学习驱动的人类-环境交互模拟提供了一条新途径，能够准确地推断人类的心理状态，并保护用户隐私。

Abstract: Understanding the dynamic relationship between humans and the built
environment is a key challenge in disciplines ranging from environmental
psychology to reinforcement learning (RL). A central obstacle in modeling these
interactions is the inability to capture human psychological states in a way
that is both generalizable and privacy preserving. Traditional methods rely on
theoretical models or questionnaires, which are limited in scope, static, and
labor intensive. We present a kinesics recognition framework that infers the
communicative functions of human activity -- known as kinesics -- directly from
3D skeleton joint data. Combining a spatial-temporal graph convolutional
network (ST-GCN) with a convolutional neural network (CNN), the framework
leverages transfer learning to bypass the need for manually defined mappings
between physical actions and psychological categories. The approach preserves
user anonymity while uncovering latent structures in bodily movements that
reflect cognitive and emotional states. Our results on the Dyadic User
EngagemenT (DUET) dataset demonstrate that this method enables scalable,
accurate, and human-centered modeling of behavior, offering a new pathway for
enhancing RL-driven simulations of human-environment interaction.

</details>


### [150] [Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems](https://arxiv.org/abs/2510.04854)
*Cheyu Lin,John Martins,Katherine A. Flanigan,Ph. D*

Main category: cs.CV

TL;DR: CPS通常侧重于经济效益，而忽视了以人为本的社会效益。CPSIS旨在解决此问题，但需要识别和量化人与人及人与基础设施的交互作用。本研究使用骨骼数据识别和分类两人交互作用，以解决隐私问题并探索交互作用的文化和情感方面。


<details>
  <summary>Details</summary>
Motivation: 当前基于CPS的系统通常侧重于经济效益，如性能和安全，而忽视了以人为本的社会效益。CPSIS旨在解决这一不足，但其成功取决于对人际互动及其与社会效益之间关系的理解。因此，需要对两人互动有更深入的理解，包括其内在含义和相互反应。

Method: 本研究提出一种基于骨骼的两人互动识别方法。该方法使用深度传感器来提取骨骼数据，以解决RGB摄像头带来的隐私问题。研究人员将五种不同的骨骼识别算法应用于包含12种两人互动的数据集，并对这些互动进行了分类，如象征性手势和情感表达，以捕捉文化和情感方面。

Result: 本研究评估了五种骨骼识别算法在识别12种两人互动方面的性能。与以往仅关注单人行为的研究不同，本研究的数据集包含了两人之间的互动，这些互动被进一步分类为象征性手势和情感表达等类型，从而能够更深入地理解互动的文化和情感方面。

Conclusion: 通过使用骨骼数据识别和分类两人互动，本研究为在CPSIS中量化和促进社会效益奠定了基础。这为开发更全面、以人为本的智能基础设施系统提供了途径，这些系统不仅能提高效率和安全性，还能促进积极的社会互动和福祉。

Abstract: Cyber-physical systems (CPS) integrate sensing, computing, and control to
improve infrastructure performance, focusing on economic goals like performance
and safety. However, they often neglect potential human-centered (or
''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim
to address this by aligning CPS with social objectives. This involves defining
social benefits, understanding human interactions with each other and
infrastructure, developing privacy-preserving measurement methods, modeling
these interactions for prediction, linking them to social benefits, and
actuating the physical environment to foster positive social outcomes. This
paper delves into recognizing dyadic human interactions using real-world data,
which is the backbone to measuring social behavior. This lays a foundation to
address the need to enhance understanding of the deeper meanings and mutual
responses inherent in human interactions. While RGB cameras are informative for
interaction recognition, privacy concerns arise. Depth sensors offer a
privacy-conscious alternative by analyzing skeletal movements. This study
compares five skeleton-based interaction recognition algorithms on a dataset of
12 dyadic interactions. Unlike single-person datasets, these interactions,
categorized into communication types like emblems and affect displays, offer
insights into the cultural and emotional aspects of human interactions.

</details>


### [151] [ERDE: Entropy-Regularized Distillation for Early-exit](https://arxiv.org/abs/2510.04856)
*Martial Guidez,Stefan Duffner,Yannick Alpou,Oscar Röth,Christophe Garcia*

Main category: cs.CV

TL;DR: 深度神经网络（特别是卷积神经网络）在图像分类方面表现出色，但计算成本高昂，不适合实时和边缘应用。本研究提出一种结合“提前退出”和“知识蒸馏”的技术，训练一个更小的学生模型，从一个更复杂的教师模型中学习，特别是在教师模型分类错误的情况下，引入了一种新的基于熵的损失函数。实验证明该方法在CIFAR10、CIFAR100和SVHN数据集上有效，能在显著降低计算复杂度的同时保持分类性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络计算成本高，不适合实时和边缘应用，需要压缩技术；动态架构可以根据执行时的需求调整压缩水平。

Method: 将“提前退出”和“知识蒸馏”技术结合，训练一个更小的学生模型（student early-exit model），从一个更复杂的教师模型（teacher early-exit model）学习。在知识蒸馏损失的基础上，为教师模型分类错误的图像增加了一个新的基于熵的损失函数。

Result: 在CIFAR10、CIFAR100和SVHN数据集上，该方法在显著降低计算复杂度的同时，保持了分类性能。

Conclusion: 所提出的方法有效地优化了准确性和效率之间的权衡，为知识蒸馏在其他领域的应用开辟了新的研究前景。

Abstract: Although deep neural networks and in particular Convolutional Neural Networks
have demonstrated state-of-the-art performance in image classification with
relatively high efficiency, they still exhibit high computational costs, often
rendering them impractical for real-time and edge applications. Therefore, a
multitude of compression techniques have been developed to reduce these costs
while maintaining accuracy. In addition, dynamic architectures have been
introduced to modulate the level of compression at execution time, which is a
desirable property in many resource-limited application scenarios. The proposed
method effectively integrates two well-established optimization techniques:
early exits and knowledge distillation, where a reduced student early-exit
model is trained from a more complex teacher early-exit model. The primary
contribution of this research lies in the approach for training the student
early-exit model. In comparison to the conventional Knowledge Distillation
loss, our approach incorporates a new entropy-based loss for images where the
teacher's classification was incorrect. The proposed method optimizes the
trade-off between accuracy and efficiency, thereby achieving significant
reductions in computational complexity without compromising classification
performance. The validity of this approach is substantiated by experimental
results on image classification datasets CIFAR10, CIFAR100 and SVHN, which
further opens new research perspectives for Knowledge Distillation in other
contexts.

</details>


### [152] [μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy](https://arxiv.org/abs/2510.04859)
*Elena Corbetta,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 使用深度学习模型μDeepIQA进行光学显微镜图像质量评估，解决了传统方法的耗时、计算成本高和泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 光学显微镜在生命科学和生物医学研究中广泛应用，需要可靠的图像质量评估（IQA）来保证图像处理和分析的正确性。然而，现有的IQA方法可能耗时、计算成本高，并且在处理非理想图像时稳定性不足。

Method: 提出了一种名为μDeepIQA的深度学习方法，该方法基于用于自然图像的深度卷积神经网络，并针对光学显微镜图像数据进行了重新训练，以预测单个质量指标和全局质量分数。该模型能够进行逐块预测，并可视化图像内部质量的空间变化。

Result: μDeepIQA模型能够快速、稳定地预测图像质量，即使在超出标准方法理想范围的情况下也能进行泛化。与传统方法相比，该模型在处理异常值时表现出稳定的性能，能够评估小的图像块，并提供快速预测。

Conclusion: 深度学习模型（如μDeepIQA）的泛化能力、对异常值的稳定处理能力、评估小图像块的能力以及快速预测能力，能够惠及光学显微镜研究。

Abstract: Optical microscopy is one of the most widely used techniques in research
studies for life sciences and biomedicine. These applications require reliable
experimental pipelines to extract valuable knowledge from the measured samples
and must be supported by image quality assessment (IQA) to ensure correct
processing and analysis of the image data. IQA methods are implemented with
variable complexity. However, while most quality metrics have a straightforward
implementation, they might be time consuming and computationally expensive when
evaluating a large dataset. In addition, quality metrics are often designed for
well-defined image features and may be unstable for images out of the ideal
domain.
  To overcome these limitations, recent works have proposed deep learning-based
IQA methods, which can provide superior performance, increased generalizability
and fast prediction. Our method, named $\mathrm{\mu}$DeepIQA, is inspired by
previous studies and applies a deep convolutional neural network designed for
IQA on natural images to optical microscopy measurements. We retrained the same
architecture to predict individual quality metrics and global quality scores
for optical microscopy data. The resulting models provide fast and stable
predictions of image quality by generalizing quality estimation even outside
the ideal range of standard methods. In addition, $\mathrm{\mu}$DeepIQA
provides patch-wise prediction of image quality and can be used to visualize
spatially varying quality in a single image. Our study demonstrates that
optical microscopy-based studies can benefit from the generalizability of deep
learning models due to their stable performance in the presence of outliers,
the ability to assess small image patches, and rapid predictions.

</details>


### [153] [In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning](https://arxiv.org/abs/2510.04864)
*Ciem Cornelissen,Sander De Coninck,Axel Willekens,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: 该论文提出了一种端到端的、支持物联网的机器人系统，用于对葡萄园中的葡萄产量和质量（Brix、酸度）进行非破坏性、实时、空间分辨的测绘。


<details>
  <summary>Details</summary>
Motivation: 为了克服田间高光谱成像（HSI）中的“域偏移”问题，该研究开发了一种新的深度学习框架，用于从高光谱数据中评估葡萄质量，该框架能够学习不依赖于光照的特征。

Method: 该系统集成了两个关键模块：一个用于葡萄串检测和重量估计的高性能模型，以及一个用于质量评估的深度学习框架（LISA）。LISA是一个域对抗框架，可以从未经校准的数据中学习不依赖于光照的特征。

Result: 该研究验证了系统的鲁棒性，葡萄串检测的召回率为0.82，重量预测的R^2为0.76。LISA模块将质量预测泛化能力提高了20%以上。

Conclusion: 通过结合这些鲁棒的模块，该系统成功生成了葡萄产量和质量的高分辨率、地理参考数据，为精准葡萄栽培提供了可操作的、数据驱动的见解。

Abstract: This paper presents an end-to-end, IoT-enabled robotic system for the
non-destructive, real-time, and spatially-resolved mapping of grape yield and
quality (Brix, Acidity) in vineyards. The system features a comprehensive
analytical pipeline that integrates two key modules: a high-performance model
for grape bunch detection and weight estimation, and a novel deep learning
framework for quality assessment from hyperspectral (HSI) data. A critical
barrier to in-field HSI is the ``domain shift" caused by variable illumination.
To overcome this, our quality assessment is powered by the Light-Invariant
Spectral Autoencoder (LISA), a domain-adversarial framework that learns
illumination-invariant features from uncalibrated data. We validated the
system's robustness on a purpose-built HSI dataset spanning three distinct
illumination domains: controlled artificial lighting (lab), and variable
natural sunlight captured in the morning and afternoon. Results show the
complete pipeline achieves a recall (0.82) for bunch detection and a $R^2$
(0.76) for weight prediction, while the LISA module improves quality prediction
generalization by over 20% compared to the baselines. By combining these robust
modules, the system successfully generates high-resolution, georeferenced data
of both grape yield and quality, providing actionable, data-driven insights for
precision viticulture.

</details>


### [154] [BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping](https://arxiv.org/abs/2510.04876)
*Hayat Rajani,Valerio Franchi,Borja Martinez-Clavel Valles,Raimon Ramos,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TL;DR: Benthic habitat mapping is crucial for marine science, but lacks large datasets. This paper presents a large, multi-modal dataset (sonar, bathymetry, optical) with ~36,000 annotated sonar tiles for supervised learning and self-supervised, cross-modal learning. It includes raw data, mosaics, and tools to benchmark underwater habitat mapping.


<details>
  <summary>Details</summary>
Motivation: The scarcity of large, annotated datasets hinders the development and benchmarking of machine learning models for benthic habitat mapping.

Method: Collected a multi-modal dataset including side-scan sonar (SSS) tiles, bathymetric maps, and co-registered optical images from an AUV. Manually annotated ~36,000 SSS tiles with segmentation masks. Spatially associated optical images with SSS tiles for cross-modal learning. Released all data and open-source tools.

Result: Introduced a large, multi-modal benthic habitat dataset with ~1 million SSS tiles and ~36,000 annotated tiles. Facilitated supervised and self-supervised learning for AUV multi-sensor data fusion. Provided tools to enhance accessibility and encourage research.

Conclusion: This resource aims to establish a standardized benchmark for underwater habitat mapping, promoting advancements in autonomous seafloor classification and multi-sensor integration.

Abstract: Benthic habitat mapping is fundamental for understanding marine ecosystems,
guiding conservation efforts, and supporting sustainable resource management.
Yet, the scarcity of large, annotated datasets limits the development and
benchmarking of machine learning models in this domain. This paper introduces a
thorough multi-modal dataset, comprising about a million side-scan sonar (SSS)
tiles collected along the coast of Catalonia (Spain), complemented by
bathymetric maps and a set of co-registered optical images from targeted
surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}
of the SSS tiles have been manually annotated with segmentation masks to enable
supervised fine-tuning of classification models. All the raw sensor data,
together with mosaics, are also released to support further exploration and
algorithm development. To address challenges in multi-sensor data fusion for
AUVs, we spatially associate optical images with corresponding SSS tiles,
facilitating self-supervised, cross-modal representation learning. Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research. This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.

</details>


### [155] [Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context](https://arxiv.org/abs/2510.04912)
*Ngeyen Yinkfu,Sunday Nwovu,Jonathan Kayizzi,Angelique Uwamahoro*

Main category: cs.CV

TL;DR: 本研究比较了四种目标检测模型（YOLOv5、Faster R-CNN、SSD 和 RetinaNet）在基加利摩托车检测任务上的表现，以评估其在资源受限环境下的实时导航能力。


<details>
  <summary>Details</summary>
Motivation: 基加利的摩托车数量多且行驶不可预测，给自动驾驶带来挑战，因此需要评估目标检测模型以适应此类环境。

Method: 使用 PyTorch 和迁移学习，在基加利收集的 198 张图像数据集上，比较了 YOLOv5、Faster R-CNN、SSD 和 RetinaNet 四种模型的准确性、定位和推理速度。

Result: 评估了四种模型在基加利摩托车检测任务上的准确性、定位和推理速度，并指出了数据集限制和模型复杂性等实施挑战。

Conclusion: 推荐未来研究简化模型架构，以提高自动驾驶系统在卢旺达等发展中国家的可及性。

Abstract: In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,
often navigating unpredictably and disregarding traffic rules, posing
significant challenges for autonomous driving systems. This study compares four
object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for
motorbike detection using a custom dataset of 198 images collected in Kigali.
Implemented in PyTorch with transfer learning, the models were evaluated for
accuracy, localization, and inference speed to assess their suitability for
real-time navigation in resource-constrained settings. We identify
implementation challenges, including dataset limitations and model
complexities, and recommend simplified architectures for future work to enhance
accessibility for autonomous systems in developing countries like Rwanda.

</details>


### [156] [A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images](https://arxiv.org/abs/2510.04916)
*Giulio Weikmann,Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 该研究提出了一种名为SAHC的新方法，用于在遥感图像分类中利用预定义的类别层次结构。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感图像分类方法通常忽略了预定义的类别层次结构，而SAHC旨在通过整合不同粒度的分类头来学习分层特征和关系。

Method: SAHC方法使用可训练的层次矩阵以自监督方式指导网络学习层次结构，并引入了一个层次共识机制来确保跨不同层次的一致性概率分布。

Result: 在三个具有不同层次复杂度的基准数据集上的实验表明，SAHC能有效引导网络学习，并且在遥感图像分类任务中具有鲁棒性。

Conclusion: SAHC方法能够有效地利用分层分类任务的固有结构，提高了遥感图像分类的性能。

Abstract: Deep learning has become increasingly important in remote sensing image
classification due to its ability to extract semantic information from complex
data. Classification tasks often include predefined label hierarchies that
represent the semantic relationships among classes. However, these hierarchies
are frequently overlooked, and most approaches focus only on fine-grained
classification schemes. In this paper, we present a novel Semantics-Aware
Hierarchical Consensus (SAHC) method for learning hierarchical features and
relationships by integrating hierarchy-specific classification heads within a
deep network architecture, each specialized in different degrees of class
granularity. The proposed approach employs trainable hierarchy matrices, which
guide the network through the learning of the hierarchical structure in a
self-supervised manner. Furthermore, we introduce a hierarchical consensus
mechanism to ensure consistent probability distributions across different
hierarchical levels. This mechanism acts as a weighted ensemble being able to
effectively leverage the inherent structure of the hierarchical classification
task. The proposed SAHC method is evaluated on three benchmark datasets with
different degrees of hierarchical complexity on different tasks, using distinct
backbone architectures to effectively emphasize its adaptability. Experimental
results show both the effectiveness of the proposed approach in guiding network
learning and the robustness of the hierarchical consensus for remote sensing
image classification tasks.

</details>


### [157] [REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis](https://arxiv.org/abs/2510.04923)
*Alec K. Peltekian,Halil Ertugrul Aktas,Gorkem Durak,Kevin Grudzinski,Bradford C. Bemiss,Carrie Richardson,Jane E. Dematte,G. R. Scott Budinger,Anthony J. Esposito,Alexander Misharin,Alok Choudhary,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: REN是一种首个解剖学感知的MoE框架，专门用于医学图像分类，通过训练七个专门针对不同肺叶和双侧肺组合的专家网络，并结合多模态门控机制来整合影像组学和深度学习特征，在间质性肺病分类任务上取得了显著优于基线模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE缺乏医学成像领域所需的特定领域约束，而该领域解剖结构和区域疾病异质性对病理模式有重要影响。

Method: 提出了一种名为REN（Regional Expert Networks）的解剖学感知的MoE框架。该框架训练了七个专门针对不同肺叶和双侧肺组合的专家网络，并使用多模态门控机制动态集成影像组学生物标志物和深度学习（CNN、ViT、Mamba）特征。

Result: REN在间质性肺病分类任务上取得了优于基线模型的性能，其影像组学引导的集成模型平均AUC为0.8646 +/- 0.0467，相比SwinUNETR基线（AUC 0.7685）提高了12.5%。此外，特定区域的专家模型（如下叶模型）的AUC达到了0.88-0.90，优于其对应的深度学习模型（CNN：0.76-0.79）。

Conclusion: REN通过严格的患者级别交叉验证，展示了强大的泛化能力和临床可解释性，为其他结构化医学成像应用提供了一种可扩展的、解剖学引导的方法。

Abstract: Mixture-of-Experts (MoE) architectures have significantly contributed to
scalable machine learning by enabling specialized subnetworks to tackle complex
tasks efficiently. However, traditional MoE systems lack domain-specific
constraints essential for medical imaging, where anatomical structure and
regional disease heterogeneity strongly influence pathological patterns. Here,
we introduce Regional Expert Networks (REN), the first anatomically-informed
MoE framework tailored specifically for medical image classification. REN
leverages anatomical priors to train seven specialized experts, each dedicated
to distinct lung lobes and bilateral lung combinations, enabling precise
modeling of region-specific pathological variations. Multi-modal gating
mechanisms dynamically integrate radiomics biomarkers and deep learning (DL)
features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to
interstitial lung disease (ILD) classification, REN achieves consistently
superior performance: the radiomics-guided ensemble reached an average AUC of
0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC
0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe
models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)
and aligning with known disease progression patterns. Through rigorous
patient-level cross-validation, REN demonstrates strong generalizability and
clinical interpretability, presenting a scalable, anatomically-guided approach
readily extensible to other structured medical imaging applications.

</details>


### [158] [Unsupervised Active Learning via Natural Feature Progressive Framework](https://arxiv.org/abs/2510.04939)
*Yuxi Liu,Catherine Lalman,Yimin Yang*

Main category: cs.CV

TL;DR: UAL方法通过在单次后选择步骤中完成标注，但现有方法性能不佳，作者提出了NFPF框架，使用SFLM量化样本贡献度，并通过重建差异度量进行样本选择，实验证明NFPF在各项指标上均优于现有UAL方法。


<details>
  <summary>Details</summary>
Motivation: 现有UAL方法在样本重要性评估方面存在不足，容易受到噪声数据影响，且无法充分覆盖数据分布，导致性能不佳。

Method: 提出自然特征渐进框架（NFPF），利用特定特征学习机（SFLM）量化样本对模型性能的贡献度，并定义重建差异度量来进行初始样本选择。

Result: NFPF在视觉数据集上显著优于所有已建立的UAL方法，并且达到了与监督式AL方法相当的性能。

Conclusion: NFPF在样本选择的有效性、鲁棒性和数据分布覆盖率方面均表现出色，为UAL领域提供了新的解决方案。

Abstract: The effectiveness of modern deep learning models is predicated on the
availability of large-scale, human-annotated datasets, a process that is
notoriously expensive and time-consuming. While Active Learning (AL) offers a
strategic solution by labeling only the most informative and representative
data, its iterative nature still necessitates significant human involvement.
Unsupervised Active Learning (UAL) presents an alternative by shifting the
annotation burden to a single, post-selection step. Unfortunately, prevailing
UAL methods struggle to achieve state-of-the-art performance. These approaches
typically rely on local, gradient-based scoring for sample importance
estimation, which not only makes them vulnerable to ambiguous and noisy data
but also hinders their capacity to select samples that adequately represent the
full data distribution. Moreover, their use of shallow, one-shot linear
selection falls short of a true UAL paradigm. In this paper, we propose the
Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes
how sample importance is measured. At its core, NFPF employs a Specific Feature
Learning Machine (SFLM) to effectively quantify each sample's contribution to
model performance. We further utilize the SFLM to define a powerful
Reconstruction Difference metric for initial sample selection. Our
comprehensive experiments show that NFPF significantly outperforms all
established UAL methods and achieves performance on par with supervised AL
methods on vision datasets. Detailed ablation studies and qualitative
visualizations provide compelling evidence for NFPF's superior performance,
enhanced robustness, and improved data distribution coverage.

</details>


### [159] [Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion](https://arxiv.org/abs/2510.04947)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为CA3D-Diff的双向乳腺X线摄影视图转换框架，用于恢复缺失的乳腺X线摄影视图。


<details>
  <summary>Details</summary>
Motivation: 由于在临床工作流程中，乳腺X线摄影的一个视图可能缺失、损坏或质量下降，限制了下游分析的有效性。因此，视图到视图的转换可以帮助恢复缺失的视图并改善病灶的对齐。

Method: CA3D-Diff框架基于条件扩散模型，并引入了列感知交叉注意力机制来解决跨视图的结构不对齐问题，该机制利用了解剖学上对应的区域倾向于在不同视图中位于相似列位置的几何特性。此外，还设计了一个隐式3D结构重建模块，将噪声2D潜在表示反投影到基于乳腺视图投影几何的粗略3D特征体中，然后将重建的3D结构注入去噪UNet以指导跨视图生成。

Result: CA3D-Diff在双向任务中取得了优越的性能，在视觉保真度和结构一致性方面优于现有方法。生成的视图能够有效提高单视图恶性肿瘤分类的准确性，证明了该方法在实际诊断中的应用价值。

Conclusion: CA3D-Diff通过引入列感知交叉注意力和隐式3D结构重建，有效地解决了乳腺X线摄影视图转换中的结构不对齐和变形挑战，并在提高图像质量和下游诊断任务方面显示出显著的优势。

Abstract: Dual-view mammography, including craniocaudal (CC) and mediolateral oblique
(MLO) projections, offers complementary anatomical views crucial for breast
cancer diagnosis. However, in real-world clinical workflows, one view may be
missing, corrupted, or degraded due to acquisition errors or compression
artifacts, limiting the effectiveness of downstream analysis. View-to-view
translation can help recover missing views and improve lesion alignment. Unlike
natural images, this task in mammography is highly challenging due to large
non-rigid deformations and severe tissue overlap in X-ray projections, which
obscure pixel-level correspondences. In this paper, we propose Column-Aware and
Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view
translation framework based on conditional diffusion model. To address
cross-view structural misalignment, we first design a column-aware
cross-attention mechanism that leverages the geometric property that
anatomically corresponding regions tend to lie in similar column positions
across views. A Gaussian-decayed bias is applied to emphasize local column-wise
correlations while suppressing distant mismatches. Furthermore, we introduce an
implicit 3D structure reconstruction module that back-projects noisy 2D latents
into a coarse 3D feature volume based on breast-view projection geometry. The
reconstructed 3D structure is refined and injected into the denoising UNet to
guide cross-view generation with enhanced anatomical awareness. Extensive
experiments demonstrate that CA3D-Diff achieves superior performance in
bidirectional tasks, outperforming state-of-the-art methods in visual fidelity
and structural consistency. Furthermore, the synthesized views effectively
improve single-view malignancy classification in screening settings,
demonstrating the practical value of our method in real-world diagnostics.

</details>


### [160] [SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization](https://arxiv.org/abs/2510.04961)
*Théophane Vallaeys,Jakob Verbeek,Matthieu Cord*

Main category: cs.CV

TL;DR: SSDD是一种新的像素扩散解码器架构，它不使用对抗性损失，并且在单步重建方面表现优于KL-VAE。


<details>
  <summary>Details</summary>
Motivation: 现有的基于KL-VAE的图像标记器虽然有效，但在匹配扩散解码器的性能时需要对抗性损失，并且解码速度较慢。扩散解码器是一种更符合原理的替代方案，但需要对抗性损失和更长的解码时间。

Method: 提出了一种新的像素扩散解码器架构，结合了Transformer组件和无GAN训练。使用知识蒸馏技术将扩散解码器的性能转移到一个高效的单步解码器中，使其能够进行单步重建，并且不需要对抗性损失。

Result: SSDD在重建FID上取得了0.50的成绩，优于KL-VAE的0.87，同时吞吐量提高了1.4倍。在保持DiT生成质量的同时，采样速度提高了3.8倍。

Conclusion: SSDD是一种高效、高质量的扩散解码器，可以作为KL-VAE的替代品，用于构建更高质量、更快的生成模型。

Abstract: Tokenizers are a key component of state-of-the-art generative image models,
extracting the most important features from the signal while reducing data
dimension and redundancy. Most current tokenizers are based on KL-regularized
variational autoencoders (KL-VAE), trained with reconstruction, perceptual and
adversarial losses. Diffusion decoders have been proposed as a more principled
alternative to model the distribution over images conditioned on the latent.
However, matching the performance of KL-VAE still requires adversarial losses,
as well as a higher decoding time due to iterative sampling. To address these
limitations, we introduce a new pixel diffusion decoder architecture for
improved scaling and training stability, benefiting from transformer components
and GAN-free training. We use distillation to replicate the performance of the
diffusion decoder in an efficient single-step decoder. This makes SSDD the
first diffusion decoder optimized for single-step reconstruction trained
without adversarial losses, reaching higher reconstruction quality and faster
sampling than KL-VAE. In particular, SSDD improves reconstruction FID from
$0.87$ to $0.50$ with $1.4\times$ higher throughput and preserve generation
quality of DiTs with $3.8\times$ faster sampling. As such, SSDD can be used as
a drop-in replacement for KL-VAE, and for building higher-quality and faster
generative models.

</details>


### [161] [ActiveMark: on watermarking of visual foundation models via massive activations](https://arxiv.org/abs/2510.04966)
*Anna Chistyakova,Mikhail Pautov*

Main category: cs.CV

TL;DR: 提出一种视觉基础模型（VFM）的所有权验证方法，通过对VFM的表达层和编码器-解码器网络进行微调，将数字水印嵌入到输入图像的内部表示中，即使在模型被微调后水印也仍然可检测。


<details>
  <summary>Details</summary>
Motivation:  VFMs的高昂计算成本促使用户对其进行许可保护，但盗版用户可能非法重新分发受保护的模型，因此开发可靠的所有权验证工具非常重要。

Method: 通过微调VFM的表达层和编码器-解码器网络，将数字水印嵌入到留出集的输入图像的内部表示中。

Result: 实验证明该方法能够有效区分被盗版和独立训练的模型，并且具有较低的误检率和漏检率。

Conclusion: 该方法为视觉基础模型提供了一种有效的数字水印嵌入和所有权验证机制。

Abstract: Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

</details>


### [162] [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](https://arxiv.org/abs/2510.05006)
*Koen Vellenga,H. Joe Steinhauer,Jonas Andersson,Anders Sjögren*

Main category: cs.CV

TL;DR: DNNs应用于资源受限环境中的安全关键任务，LL-PDL方法可检测OOD实例但性能各异。本文提出用变换层扩展预训练DNN以产生多个潜在表示来估计不确定性（LUR和RLUR）。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境下，DNN在安全关键任务（如驾驶员行为和意图识别）中的应用日益增多，而现有LL-PDL方法在检测OOD实例时性能不稳定。

Method: 提出用变换层扩展预训练DNN以产生多个潜在表示来估计不确定性（LUR和RLUR），并与八种PDL方法在四个驾驶员行为和意图识别数据集上进行了比较，评估了分类性能、校准和基于不确定性的OOD检测。此外，还为NuScenes数据集贡献了28,000个帧级行为标签和1,194个视频级意图标签。

Result: LUR和RLUR在分布内分类性能上与其他LL-PDL方法相当。在基于不确定性的OOD检测方面，LUR的性能与表现最佳的PDL方法相当，且训练效率更高，调优比需要MCMC采样或排斥训练程序的LL-PDL方法更简单。

Conclusion: LUR和RLUR在驾驶员行为和意图识别等安全关键任务中，提供了一种有效且高效的OOD检测方法，其性能可与现有最佳方法媲美，且易于训练和调优。

Abstract: Deep neural networks (DNNs) are increasingly applied to safety-critical tasks
in resource-constrained environments, such as video-based driver action and
intention recognition. While last layer probabilistic deep learning (LL-PDL)
methods can detect out-of-distribution (OOD) instances, their performance
varies. As an alternative to last layer approaches, we propose extending
pre-trained DNNs with transformation layers to produce multiple latent
representations to estimate the uncertainty. We evaluate our latent uncertainty
representation (LUR) and repulsively trained LUR (RLUR) approaches against
eight PDL methods across four video-based driver action and intention
recognition datasets, comparing classification performance, calibration, and
uncertainty-based OOD detection. We also contribute 28,000 frame-level action
labels and 1,194 video-level intention labels for the NuScenes dataset. Our
results show that LUR and RLUR achieve comparable in-distribution
classification performance to other LL-PDL approaches. For uncertainty-based
OOD detection, LUR matches top-performing PDL methods while being more
efficient to train and easier to tune than approaches that require Markov-Chain
Monte Carlo sampling or repulsive training procedures.

</details>


### [163] [Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns](https://arxiv.org/abs/2510.05015)
*Nabil Daiyan,Md Rakibul Haque*

Main category: cs.CV

TL;DR: 利用手绘的螺旋线和波形图，通过卷积神经网络（CNN）、迁移学习和注意力机制，提出了一种基于机器学习的帕金森病（PD）早期诊断方法。


<details>
  <summary>Details</summary>
Motivation: 早期诊断帕金森病（PD）对于防止不良影响至关重要，但传统诊断方法通常繁琐且成本高昂。

Method: 该研究提出一种基于机器学习的方法，利用手绘的螺旋线和波形图像作为潜在的生物标志物来检测帕金森病。该方法利用卷积神经网络（CNN）、迁移学习和注意力机制来提高模型性能和抗过拟合能力。为了增强螺旋线和波形图像的类别多样性和丰富性，对训练数据集进行了增强以增加图像数量。所提出的架构包括三个阶段：利用预训练的CNN、整合自定义卷积层和集成投票。采用硬投票进一步聚合多个模型的预测，从而提高性能。

Result: 实验结果显示出有希望的准确率。对于螺旋线图像，加权平均精度、召回率和F1分数均为90%；对于波形图像，分别为96.67%。在通过集成硬投票组合预测后，总体准确率为93.3%。

Conclusion: 这些发现强调了机器学习在早期帕金森病诊断中的潜力，提供了一种非侵入性且成本效益高的方法来改善患者的治疗效果。

Abstract: Parkinson's disease (PD) is a progressive neurodegenerative condition
characterized by the death of dopaminergic neurons, leading to various movement
disorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,
yet traditional diagnostic methods are often cumbersome and costly. In this
study, a machine learning-based approach is proposed using hand-drawn spiral
and wave images as potential biomarkers for PD detection. Our methodology
leverages convolutional neural networks (CNNs), transfer learning, and
attention mechanisms to improve model performance and resilience against
overfitting. To enhance the diversity and richness of both spiral and wave
categories, the training dataset undergoes augmentation to increase the number
of images. The proposed architecture comprises three phases: utilizing
pre-trained CNNs, incorporating custom convolutional layers, and ensemble
voting. Employing hard voting further enhances performance by aggregating
predictions from multiple models. Experimental results show promising accuracy
rates. For spiral images, weighted average precision, recall, and F1-score are
90%, and for wave images, they are 96.67%. After combining the predictions
through ensemble hard voting, the overall accuracy is 93.3%. These findings
underscore the potential of machine learning in early PD diagnosis, offering a
non-invasive and cost-effective solution to improve patient outcomes.

</details>


### [164] [Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models](https://arxiv.org/abs/2510.05034)
*Yunlong Tang,Jing Bi,Pinxin Liu,Zhenyu Pan,Zhangyun Tan,Qianxiang Shen,Jiani Liu,Hang Hua,Junjia Guo,Yunzhong Xiao,Chao Huang,Zhiyuan Wang,Susan Liang,Xinyi Liu,Yizhi Song,Yuhe Nie,Jia-Xing Zhong,Bozheng Li,Daiqing Qi,Ziyun Zeng,Ali Vosoughi,Luchuan Song,Zeliang Zhang,Daiki Shimada,Han Liu,Jiebo Luo,Chenliang Xu*

Main category: cs.CV

TL;DR: 本文首次全面综述了视频大型多模态模型（Video-LMMs）的训练后方法，重点关注了监督微调（SFT）、强化学习（RL）和测试时扩展（TTS）这三大支柱。该综述提出了一个结构化分类，阐述了这些技术的作用、相互联系以及针对视频的特定适应性，并解决了时间定位、时空定位、长视频效率和多模态证据整合等独特挑战。此外，该综述还对现有方法进行了系统分析，总结了关键的设计原则、见解和评估协议，并指出了奖励设计、可扩展性和成本效益优化等方面存在的关键开放性挑战。最后，该综述整理了重要的基准、数据集和指标，以促进对训练后有效性的严格评估，旨在为研究人员和从业人员提供一个统一的框架来推进 Video-LMM 的能力。


<details>
  <summary>Details</summary>
Motivation: 视频理解是计算机视觉领域最具挑战性的前沿领域，需要模型能够推理复杂的时空关系、长期依赖关系和多模态证据。尽管视频大型多模态模型（Video-LMMs）在视频理解任务中展现了强大的能力，但将它们从基础感知系统转变为复杂的推理引擎的训练后阶段，在现有文献中仍然缺乏系统性的梳理。因此，本研究旨在填补这一空白，提供对 Video-LMM 训练后方法的首次全面考察。

Method: 本研究采用系统性分析的方法，对监督微调（SFT）与思维链、基于可验证目标的强化学习（RL）以及通过增强推理计算实现的测试时扩展（TTS）这三种训练后方法进行了全面的考察。研究中提出了一个结构化的分类法，用于阐明这些技术在 Video-LMM 训练后阶段中的作用、相互联系以及针对视频任务的特有适应性。通过对代表性方法的分析，我们合成了关键的设计原则、深刻的见解和评估协议，并识别了奖励设计、可扩展性和成本效益优化方面的关键开放性挑战。

Result: 本研究系统性地分析了监督微调（SFT）、强化学习（RL）和测试时扩展（TTS）等 Video-LMM 训练后方法，并提出了一个结构化的分类法。分析结果揭示了这些技术在处理时间定位、时空定位、长视频效率和多模态证据整合等视频特有挑战方面的作用和适应性。此外，研究还提炼了关键的设计原则、见解和评估协议，并指出了奖励设计、可扩展性和成本效益优化方面存在的开放性挑战。最后，为促进严格评估，本研究整理了相关的基准、数据集和指标。

Conclusion: 本综述首次全面考察了 Video-LMM 的训练后方法，为该领域的研究人员和从业人员提供了一个统一的框架。通过对 SFT、RL 和 TTS 的系统分析，我们阐明了它们的作用、相互联系和视频特有适应性，并解决了关键的挑战。此外，我们还提炼了设计原则、评估协议，并指出了未来的研究方向，包括奖励设计、可扩展性和成本效益优化。最后，我们整理了必要的资源以促进该领域的进一步发展。

Abstract: Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training

</details>


### [165] [SegMASt3R: Geometry Grounded Segment Matching](https://arxiv.org/abs/2510.05051)
*Rohit Jayanti,Swayam Agrawal,Vansh Garg,Siddharth Tourani,Muhammad Haris Khan,Sourav Garg,Madhava Krishna*

Main category: cs.CV

TL;DR: 利用3D基础模型进行宽基线下的语义/几何一致的图像区域匹配，超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 段匹配在计算机视觉中是重要的中间任务，尤其是在处理遮挡、光照变化和视角变化时，其鲁棒性优于关键点匹配。然而，在极端视角变化下的宽基线段匹配仍然是一个挑战。

Method: 提出一种利用3D基础模型空间理解能力的架构，以匹配具有高达180度视角变化的图像对中的段。

Result: 在ScanNet++和Replica数据集上，该方法在AUPRC指标上比SAM2视频传播器和局部特征匹配方法等最先进方法高出30%。

Conclusion: 所提出的方法在宽基线段匹配任务上表现出色，并在3D实例分割和图像-目标导航等下游任务中展现了优势。

Abstract: Segment matching is an important intermediate task in computer vision that
establishes correspondences between semantically or geometrically coherent
regions across images. Unlike keypoint matching, which focuses on localized
features, segment matching captures structured regions, offering greater
robustness to occlusions, lighting variations, and viewpoint changes. In this
paper, we leverage the spatial understanding of 3D foundation models to tackle
wide-baseline segment matching, a challenging setting involving extreme
viewpoint shifts. We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change. Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++
and Replica datasets. We further demonstrate benefits of the proposed model on
relevant downstream tasks, including 3D instance segmentation and image-goal
navigation. Project Page: https://segmast3r.github.io/

</details>


### [166] [No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference](https://arxiv.org/abs/2510.05053)
*Mohammad-Ali Mahmoudpour,Saeed Mahmoudpour*

Main category: cs.CV

TL;DR: 该论文提出了一种无需参考图像的图像质量评估（NR-IQA）方法，专门用于评估对比度失真图像。通过生成视觉上接近真实参考图像的伪参考图像，将NR问题转化为更准确的全参考（FR）评估问题。


<details>
  <summary>Details</summary>
Motivation: 对比度失真是影响图像质量的重要因素，但现有方法大多忽略了它。本文旨在解决这一问题，提出一种针对对比度失真的NR-IQA指标。

Method: 首先，利用对比度增强算法生成伪参考图像，然后训练分类网络选择最适合的对比度增强算法。最后，在全参考（FR）设置下评估增强后的伪参考图像与失真图像之间的质量差异。

Result: 所提方法在包含对比度失真的三个数据库（CCID2014、TID2013和CSIQ）上的性能评估显示出有希望的结果。

Conclusion: 所提出的无需参考图像的图像质量评估方法在处理对比度失真方面表现出有前景的性能，通过生成伪参考图像将NR问题转化为FR评估，提高了评估的准确性。

Abstract: Contrast change is an important factor that affects the quality of images.
During image capturing, unfavorable lighting conditions can cause contrast
change and visual quality loss. While various methods have been proposed to
assess the quality of images under different distortions such as blur and
noise, contrast distortion has been largely overlooked as its visual impact and
properties are different from other conventional types of distortions. In this
paper, we propose a no-reference image quality assessment (NR-IQA) metric for
contrast-distorted images. Using a set of contrast enhancement algorithms, we
aim to generate pseudo-reference images that are visually close to the actual
reference image, such that the NR problem is transformed to a Full-reference
(FR) assessment with higher accuracy. To this end, a large dataset of
contrast-enhanced images is produced to train a classification network that can
select the most suitable contrast enhancement algorithm based on image content
and distortion for pseudo-reference image generation. Finally, the evaluation
is performed in the FR manner to assess the quality difference between the
contrast-enhanced (pseudoreference) and degraded images. Performance evaluation
of the proposed method on three databases containing contrast distortions
(CCID2014, TID2013, and CSIQ), indicates the promising performance of the
proposed method.

</details>


### [167] [Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces](https://arxiv.org/abs/2510.05071)
*Debojyoti Ghosh,Soumya K Ghosh,Adrijit Goswami*

Main category: cs.CV

TL;DR: 该研究提出了一种名为“神经可塑模块化分类器”的新型混合模型，结合了ResNet-50和Vision Transformer（ViT）以及FAISS相似性检索，并通过可扩展、可学习的模块化设计实现了动态适应性，在垃圾分类和工业表面缺陷检测方面均取得了优于传统模型的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 为了有效且准确地对废物进行分类并检测工业表面的缺陷，以确保可持续的废物管理和高质量控制。

Method: 提出了一种名为“神经可塑模块化分类器”的新型混合模型，该模型结合了用于局部特征提取的ResNet-50骨干网络、用于捕获全局语义上下文的Vision Transformer（ViT），并利用基于FAISS的相似性检索提供类似记忆的参考。其核心创新在于神经可塑模块化设计，包含可扩展、可学习的模块，能在性能停滞时动态增长，模拟生物学习系统以适应数据复杂性并提高泛化能力。

Result: 在垃圾分类和KolektorSDD2数据集（工业金属表面缺陷检测）上的实验表明，该模型在准确性和适应性方面均优于传统的静态模型。

Conclusion: 该神经可塑模块化分类器为现实世界中的图像分类提供了一种可扩展、高性能的解决方案，在环境和工业领域均具有很强的应用前景。

Abstract: Efficient and accurate classification of waste and industrial surface defects
is essential for ensuring sustainable waste management and maintaining high
standards in quality control. This paper introduces the Neuroplastic Modular
Classifier, a novel hybrid architecture designed for robust and adaptive image
classification in dynamic environments. The model combines a ResNet-50 backbone
for localized feature extraction with a Vision Transformer (ViT) to capture
global semantic context. Additionally, FAISS-based similarity retrieval is
incorporated to provide a memory-like reference to previously encountered data,
enriching the model's feature space. A key innovation of our architecture is
the neuroplastic modular design composed of expandable, learnable blocks that
dynamically grow during training when performance plateaus. Inspired by
biological learning systems, this mechanism allows the model to adapt to data
complexity over time, improving generalization. Beyond garbage classification,
we validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),
which involves industrial defect detection on metal surfaces. Experimental
results across domains show that the proposed architecture outperforms
traditional static models in both accuracy and adaptability. The Neuroplastic
Modular Classifier offers a scalable, high-performance solution for real-world
image classification, with strong applicability in both environmental and
industrial domains.

</details>


### [168] [Factuality Matters: When Image Generation and Editing Meet Structured Visuals](https://arxiv.org/abs/2510.05091)
*Le Zhuo,Songhao Han,Yuandong Pu,Boxiang Qiu,Sayak Paul,Yue Liao,Yihao Liu,Jie Shao,Xi Chen,Si Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 该研究首次系统性地解决了视觉生成模型在处理图表、示意图等结构化视觉内容方面的不足，提出了包含数据集构建、模型训练和评估基准在内的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现代视觉生成模型在生成自然图像方面表现出色，但在创建或编辑需要组合规划、文本渲染和多模态推理以保证事实准确性的结构化视觉内容（如图表、示意图、数学图形等）方面存在困难。

Method: 研究人员构建了一个包含130万个高质量结构化图像对的大规模数据集，并添加了链式思考推理标注。在此基础上，他们训练了一个集成VLM和FLUX.1 Kontext的统一模型，并采用三阶段训练课程。此外，还引入了StructBench基准和StructScore评估指标，并在推理时引入外部推理器。

Result: 在StructBench基准上，即使是领先的闭源模型也远未达到令人满意的水平。研究提出的模型在编辑方面表现出色，并且推理时的推理能够带来跨多种架构的一致性提升。

Conclusion: 该研究通过发布数据集、模型和基准，旨在推进结构化视觉内容的统一多模态基础。

Abstract: While modern visual generation models excel at creating aesthetically
pleasing natural images, they struggle with producing or editing structured
visuals like charts, diagrams, and mathematical figures, which demand
composition planning, text rendering, and multimodal reasoning for factual
fidelity. To address this, we present the first comprehensive, systematic
investigation of this domain, encompassing data construction, model training,
and an evaluation benchmark. First, we construct a large-scale dataset of 1.3
million high-quality structured image pairs derived from executable drawing
programs and augmented with chain-of-thought reasoning annotations. Building on
it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a
lightweight connector for enhanced multimodal understanding. A three-stage
training curriculum enables progressive feature alignment, knowledge infusion,
and reasoning-augmented generation, further boosted by an external reasoner at
inference time. Finally, we introduce StructBench, a novel benchmark for
generation and editing with over 1,700 challenging instances, and an
accompanying evaluation metric, StructScore, which employs a multi-round Q\&A
protocol to assess fine-grained factual accuracy. Evaluations of 15 models
reveal that even leading closed-source systems remain far from satisfactory.
Our model attains strong editing performance, and inference-time reasoning
yields consistent gains across diverse architectures. By releasing the dataset,
model, and benchmark, we aim to advance unified multimodal foundations for
structured visuals.

</details>


### [169] [Character Mixing for Video Generation](https://arxiv.org/abs/2510.05093)
*Tingting Liao,Chongjian Ge,Guangyi Liu,Hao Li,Yi Zhou*

Main category: cs.CV

TL;DR: 生成跨不同世界的角色交互视频，解决身份保持和风格一致性问题。


<details>
  <summary>Details</summary>
Motivation: 在文本到视频生成中，实现跨不同世界角色的自然交互，同时保持各角色的身份和行为特征，并解决风格不一致问题。

Method: 提出一种名为CCE（Cross-Character Embedding）的框架，通过学习多模态来源的身份和行为逻辑，并结合CCA（Cross-Character Augmentation）技术，通过合成的共存和混合风格数据来丰富训练，以应对角色从未共存以及混合风格导致风格错觉的问题。

Result: 在包含10个角色的动画和真人系列数据集上进行实验，在身份保持、交互质量和风格错觉的鲁棒性方面取得了显著改进。

Conclusion: 所提出的框架能够实现先前不共存的角色之间的自然交互，同时保持风格保真度，为生成式叙事开辟了新的可能性。

Abstract: Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where
characters interact naturally across different worlds? We study inter-character
interaction in text-to-video generation, where the key challenge is to preserve
each character's identity and behaviors while enabling coherent cross-context
interaction. This is difficult because characters may never have coexisted and
because mixing styles often causes style delusion, where realistic characters
appear cartoonish or vice versa. We introduce a framework that tackles these
issues with Cross-Character Embedding (CCE), which learns identity and
behavioral logic across multimodal sources, and Cross-Character Augmentation
(CCA), which enriches training with synthetic co-existence and mixed-style
data. Together, these techniques allow natural interactions between previously
uncoexistent characters without losing stylistic fidelity. Experiments on a
curated benchmark of cartoons and live-action series with 10 characters show
clear improvements in identity preservation, interaction quality, and
robustness to style delusion, enabling new forms of generative
storytelling.Additional results and videos are available on our project page:
https://tingtingliao.github.io/mimix/.

</details>


### [170] [VChain: Chain-of-Visual-Thought for Reasoning in Video Generation](https://arxiv.org/abs/2510.05094)
*Ziqi Huang,Ning Yu,Gordon Chen,Haonan Qiu,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: VChain是一个框架，利用大型多模态模型来指导视频生成，以提高复杂动态场景的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型在合成具有连贯因果链的复杂动态方面存在困难，而大型多模态模型（如GPT-4o）在视觉状态推理和未来预测方面表现出色，这表明可以利用多模态模型的优势来改进视频生成。

Method: VChain框架通过一个专门的管道，利用大型多模态模型生成关键帧的稀疏集合，然后指导预训练的视频生成器仅在这些关键时刻进行稀疏推理时调整，从而将视觉推理信号注入视频生成。

Result: 在复杂、多步骤的场景进行的广泛实验表明，VChain显著提高了生成视频的质量。

Conclusion: VChain通过注入视觉推理信号，成功地解决了现有视频生成模型在处理复杂动态和因果关系方面的不足，显著提升了视频生成的质量和连贯性。

Abstract: Recent video generation models can produce smooth and visually appealing
clips, but they often struggle to synthesize complex dynamics with a coherent
chain of consequences. Accurately modeling visual outcomes and state
transitions over time remains a core challenge. In contrast, large language and
multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and
future prediction capabilities. To bridge these strengths, we introduce VChain,
a novel inference-time chain-of-visual-thought framework that injects visual
reasoning signals from multimodal models into video generation. Specifically,
VChain contains a dedicated pipeline that leverages large multimodal models to
generate a sparse set of critical keyframes as snapshots, which are then used
to guide the sparse inference-time tuning of a pre-trained video generator only
at these key moments. Our approach is tuning-efficient, introduces minimal
overhead and avoids dense supervision. Extensive experiments on complex,
multi-step scenarios show that VChain significantly enhances the quality of
generated videos.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [171] [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
*Alex Gibson*

Main category: cs.CL

TL;DR: 研究了Transformer语言模型中的注意力头，发现部分注意力头具有分散的注意力模式且其注意力分数对内容不敏感，并且其Softmax分母在固定token分布下是稳定的。通过从“校准文本”中采样Softmax分母，并结合GPT2-Small模型第一层中多个稳定头部的输出来近似其组合输出，可以实现仅从模型权重和单个校准文本中识别出大量对上下文有响应的神经元，甚至包括那些在校准文本中未激活的神经元。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer语言模型中具有分散注意力模式且对内容不敏感的注意力头，并利用其Softmax分母的稳定性来实现模型分析。

Method: 通过从校准文本中采样Softmax分母，结合GPT2-Small模型第一层中稳定注意头部的输出来近似其组合输出，进而分析模型权重以识别对上下文敏感的神经元。

Result: 成功地从模型权重和单个校准文本中识别出数百个对高层上下文属性有响应的第一层神经元，包括在校准文本中未激活的神经元。

Conclusion: 基于模型权重和校准文本，可以有效地识别Transformer模型第一层中对上下文敏感的神经元，为理解模型提供了新的途径。

Abstract: We study transformer language models, analyzing attention heads whose
attention patterns are spread out, and whose attention scores depend weakly on
content. We argue that the softmax denominators of these heads are stable when
the underlying token distribution is fixed. By sampling softmax denominators
from a "calibration text", we can combine together the outputs of multiple such
stable heads in the first layer of GPT2-Small, approximating their combined
output by a linear summary of the surrounding text. This approximation enables
a procedure where from the weights alone - and a single calibration text - we
can uncover hundreds of first layer neurons that respond to high-level
contextual properties of the surrounding text, including neurons that didn't
activate on the calibration text.

</details>


### [172] [Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision](https://arxiv.org/abs/2510.03323)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.CL

TL;DR: 该研究提出了一种名为Graph-S^3的代理文本图推理框架，通过LLM驱动的检索器，并使用合成的逐步监督进行训练，以解决大型图中信息检索的挑战，提高了复杂推理任务的准确性和F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的文本图问答系统在图检索方面存在挑战，即如何从大型图中检索既有信息量又紧凑的相关内容。现有检索器依赖于浅层嵌入相似性或交互式检索策略，这需要大量的数据标记和训练成本，导致性能不佳。

Method: 提出Graph-S^3，一个基于LLM的代理文本图推理框架，采用合成的逐步监督进行训练。通过数据合成管道提取黄金子图以生成奖励，并采用两阶段训练方案学习基于合成奖励的交互式图探索策略。

Result: 在三个常用数据集上的广泛实验表明，与七个强基线相比，该方法在准确率上平均提高了8.1%，在F1分数上提高了9.7%，尤其在复杂的多跳推理任务中优势更为明显。

Conclusion: Graph-S^3通过创新的逐步监督和数据合成方法，有效解决了文本图检索的挑战，显著提升了问答性能，并且代码将开源。

Abstract: A significant portion of real-world data is inherently represented as textual
graphs, and integrating these graphs into large language models (LLMs) is
promising to enable complex graph-based question answering. However, a key
challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,
how to retrieve relevant content from large graphs that is sufficiently
informative while remaining compact for the LLM context. Existing retrievers
suffer from poor performance since they either rely on shallow embedding
similarity or employ interactive retrieving policies that demand excessive data
labeling and training cost. To address these issues, we present Graph-$S^3$, an
agentic textual graph reasoning framework that employs an LLM-based retriever
trained with synthetic stepwise supervision. Instead of rewarding the agent
based on the final answers, which may lead to sparse and unstable training
signals, we propose to closely evaluate each step of the retriever based on
offline-extracted golden subgraphs. Our main techniques include a data
synthesis pipeline to extract the golden subgraphs for reward generation and a
two-stage training scheme to learn the interactive graph exploration policy
based on the synthesized rewards. Based on extensive experiments on three
common datasets in comparison with seven strong baselines, our approach
achieves an average improvement of 8.1\% in accuracy and 9.7\% in F$_1$ score.
The advantage is even higher in more complicated multi-hop reasoning tasks. Our
code will be open-sourced.

</details>


### [173] [Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks](https://arxiv.org/abs/2510.03384)
*Arjun Arunasalam,Madison Pickering,Z. Berkay Celik,Blase Ur*

Main category: cs.CL

TL;DR: LLM在执行日常任务时，其隐性价值观与人类的期望存在差异，且不同LLM之间也存在不一致性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究大型语言模型（LLM）在执行日常任务时所展现的隐性价值观，并与人类进行比较，以了解LLM在诸如环保、慈善、多元化等价值观方面的表现。

Method: 通过对六种主流LLM进行审计，让它们完成30项日常任务，并收集100位美国众包工作者的对应结果，以此来比较LLM之间以及LLM与人类在隐性价值观上的差异。

Result: 研究发现，LLM在隐性价值观的展现上，既不完全符合人类的期望，也未能与其他LLM保持一致。

Conclusion: LLM在执行日常任务时所展现的隐性价值观存在不确定性，并且与人类存在显著差异。

Abstract: Large language models (LLMs) can underpin AI assistants that help users with
everyday tasks, such as by making recommendations or performing basic
computation. Despite AI assistants' promise, little is known about the implicit
values these assistants display while completing subjective everyday tasks.
Humans may consider values like environmentalism, charity, and diversity. To
what extent do LLMs exhibit these values in completing everyday tasks? How do
they compare with humans? We answer these questions by auditing how six popular
LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human
crowdworkers from the US. We find LLMs often do not align with humans, nor with
other LLMs, in the implicit values exhibited.

</details>


### [174] [Morpheme Induction for Emergent Language](https://arxiv.org/abs/2510.03439)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: CSAR是一种从平行语料库中诱导词缀的算法，通过贪婪地选择基于互信息的最高权重词缀对来工作。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是开发一种能够从新兴语言的平行语料库中提取词缀的算法。

Method: CSAR算法采用贪婪方法，通过计算形式和意义之间的互信息来对词缀进行加权，然后选择最高权重的词缀对，并从语料库中移除，最后重复此过程（计数、选择、去除、重复）。

Result: CSAR在程序生成的数据集上进行了验证，并与相关任务的基线进行了比较。此外，该算法还在人类语言数据上进行了验证，并在新兴语言上进行了分析，量化了同义词和多义词等语言特征。

Conclusion: CSAR算法能够有效地从平行语料库中诱导词缀，并在程序生成和人类语言数据上都表现出良好的性能。

Abstract: We introduce CSAR, an algorithm for inducing morphemes from emergent language
corpora of parallel utterances and meanings. It is a greedy algorithm that (1)
weights morphemes based on mutual information between forms and meanings, (2)
selects the highest-weighted pair, (3) removes it from the corpus, and (4)
repeats the process to induce further morphemes (i.e., Count, Select, Ablate,
Repeat). The effectiveness of CSAR is first validated on procedurally generated
datasets and compared against baselines for related tasks. Second, we validate
CSAR's performance on human language data to show that the algorithm makes
reasonable predictions in adjacent domains. Finally, we analyze a handful of
emergent languages, quantifying linguistic characteristics like degree of
synonymy and polysemy.

</details>


### [175] [Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video](https://arxiv.org/abs/2510.03458)
*Mengyao Xu,Wenfei Zhou,Yauhen Babakhin,Gabriel Moreira,Ronay Ak,Radek Osmulski,Bo Liu,Even Oldridge,Benedikt Schifferer*

Main category: cs.CL

TL;DR: Nemotron 是一个统一的多模态检索嵌入模型，可以处理文本、图像、音频和视频，并支持跨模态和联合模态检索。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的检索器在处理包含丰富视觉和语义内容的真实世界文档（如PDF、幻灯片或视频）时存在不足，需要改进以处理更复杂的信息需求。

Method: Nemotron 模型通过扩展现有模型（如ColPali和Qwen2.5-Omni）的能力，整合了对音频和视频模态的支持，实现了跨模态（如文本-视频）和联合模态（如文本-视频+音频）检索。

Result: 该模型在文本、图像和视频检索方面都展现了有效性。

Conclusion: Nemotron 模型成功地扩展了检索能力，涵盖了文本、图像、音频和视频，并实现了跨模态和联合模态检索，为处理复杂现实世界信息需求提供了一个统一的解决方案。

Abstract: We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding
model developed to handle the increasing complexity of real-world information
needs. While Retrieval-Augmented Generation (RAG) has significantly advanced
language models by incorporating external knowledge, existing text-based
retrievers rely on clean, structured input and struggle with the visually and
semantically rich content found in real-world documents such as PDFs, slides,
or videos. Recent work such as ColPali has shown that preserving document
layout using image-based representations can improve retrieval quality.
Building on this, and inspired by the capabilities of recent multimodal models
such as Qwen2.5-Omni, we extend retrieval beyond text and images to also
support audio and video modalities. Omni-Embed-Nemotron enables both
cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)
retrieval using a single model. We describe the architecture, training setup,
and evaluation results of Omni-Embed-Nemotron, and demonstrate its
effectiveness in text, image, and video retrieval.

</details>


### [176] [Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?](https://arxiv.org/abs/2510.04434)
*Grace LeFevre,Qingcheng Zeng,Adam Leif,Jason Jewell,Denis Peskoff,Rob Voigt*

Main category: cs.CL

TL;DR: ACL作者在ACL之外的会议上发表的关于NLP4SG的研究的比例更高，而利用NLP解决社会公益问题的研究绝大多数是由ACL以外的作者在ACL之外的会议上发表的。


<details>
  <summary>Details</summary>
Motivation: 随着NLP4SG（自然语言处理用于社会公益）的兴起，对该领域的研究进行分析，并从作者和会议层面了解其发展状况。

Method: 通过量化ACL社区内外的研究以及ACL核心贡献者和非ACL作者的研究比例，来分析NLP4SG的研究格局。

Result: ACL作者在ACL之外的会议上发表关于社会公益的研究的可能性要大得多。在ACL之外的会议上，由非ACL作者发表的关于利用NLP解决社会公益问题的大部分出版物。

Conclusion: ACL社区在NLP4SG的议程设定方面需要考虑这些发现所带来的影响。

Abstract: The social impact of Natural Language Processing (NLP) is increasingly
important, with a rising community focus on initiatives related to NLP for
Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the
ACL Anthology address topics related to social good as defined by the UN
Sustainable Development Goals (Adauto et al., 2023). In this study, we take an
author- and venue-level perspective to map the landscape of NLP4SG, quantifying
the proportion of work addressing social good concerns both within and beyond
the ACL community, by both core ACL contributors and non-ACL authors. With this
approach we discover two surprising facts about the landscape of NLP4SG. First,
ACL authors are dramatically more likely to do work addressing social good
concerns when publishing in venues outside of ACL. Second, the vast majority of
publications using NLP techniques to address concerns of social good are done
by non-ACL authors in venues outside of ACL. We discuss the implications of
these findings on agenda-setting considerations for the ACL community related
to NLP4SG.

</details>


### [177] [Searching for the Most Human-like Emergent Language](https://arxiv.org/abs/2510.03467)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: 设计了一个基于信号博弈的涌现式交流环境，通过超参数优化和XferBench来生成与人类语言相似的涌现式语言，并验证了熵在迁移学习性能上的预测能力。


<details>
  <summary>Details</summary>
Motivation: 设计了一个基于信号博弈的涌现式交流环境，以生成在与人类语言相似性方面达到最先进水平的涌现式语言。

Method: 使用超参数优化，并以XferBench作为目标函数，XferBench通过衡量涌现式语言在深度迁移学习到人类语言上的适用性来量化其与人类语言的统计相似性。

Result: 验证了熵对涌现式语言迁移学习性能的预测能力，并支持了先前关于涌现式交流系统熵最小化特性的结果。报告了关于哪些超参数能产生更真实的涌现式语言（即迁移到人类语言效果更好的语言）的泛化结论。

Conclusion: 通过超参数优化和XferBench量化指标，成功生成了与人类语言高度相似的涌现式语言，并揭示了超参数选择对语言真实性的影响。

Abstract: In this paper, we design a signalling game-based emergent communication
environment to generate state-of-the-art emergent languages in terms of
similarity to human language. This is done with hyperparameter optimization,
using XferBench as the objective function. XferBench quantifies the statistical
similarity of emergent language to human language by measuring its suitability
for deep transfer learning to human language. Additionally, we demonstrate the
predictive power of entropy on the transfer learning performance of emergent
language as well as corroborate previous results on the entropy-minimization
properties of emergent communication systems. Finally, we report
generalizations regarding what hyperparameters produce more realistic emergent
languages, that is, ones which transfer better to human language.

</details>


### [178] [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
*Aneesha Sampath,Oya Aran,Emily Mower Provost*

Main category: cs.CL

TL;DR: SEER Benchmark用于评估LLM识别文本中表达情感的具体文本片段的能力，弥补了传统情感识别任务的不足，在单句输入上表现尚可，但在长文本上准确率下降。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别任务仅分配单一标签，无法满足需要了解情感表达方式的应用（如共情对话、临床支持）。SEER旨在解决这一问题，专注于情感证据检测，即精确定位表达情感的具体短语。

Method: SEER包含两项任务：识别单句内的情感证据，以及识别五句短文内的证据。它在1200个真实句子上提供了情感和情感证据的新标注。对14个开源LLM进行了评估。

Result: 在单句输入上，一些模型接近人类平均水平；但在较长文本输入时，准确率有所下降。错误分析显示，模型过度依赖情感关键词，并在中性文本中产生错误积极（false positives）。

Conclusion: SEER Benchmark揭示了当前LLM在识别长文本中的情感证据方面存在不足，并指出了具体的失败模式，为未来模型改进提供了方向。

Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

</details>


### [179] [The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.04933)
*Amir Hameed Mir*

Main category: cs.CL

TL;DR: LSD是一种新的基于几何的框架，用于检测大型语言模型（LLM）的幻觉。它通过分析Transformer层之间的隐藏状态语义演变来工作，并使用基于边界的对比学习来区分事实响应和幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）经常产生流畅但事实不正确的陈述（称为幻觉），这在高风险领域带来严重风险。

Method: LSD是一个几何框架，通过分析Transformer层之间的隐藏状态语义演变来检测幻觉。它利用基于边界的对比学习，使隐藏激活与从事实编码器派生的地面真实嵌入对齐，从而揭示了语义轨迹的明显分离：事实响应保持稳定的对齐，而幻觉则在深度上表现出明显的语义漂移。

Result: 在TruthfulQA和合成事实幻觉数据集上评估，LSD的F1分数达到0.92，AUROC达到0.96，聚类准确率达到0.89，优于SelfCheckGPT和Semantic Entropy基线，并且只需要一次前向传播。

Conclusion: LSD为大型语言模型提供了一种可扩展、与模型无关的实时幻觉监控机制，并提供了对事实一致性几何形状的新见解。

Abstract: Large Language Models (LLMs) often produce fluent yet factually incorrect
statements-a phenomenon known as hallucination-posing serious risks in
high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric
framework for hallucination detection that analyzes the evolution of
hidden-state semantics across transformer layers. Unlike prior methods that
rely on multiple sampling passes or external verification sources, LSD operates
intrinsically within the model's representational space. Using margin-based
contrastive learning, LSD aligns hidden activations with ground-truth
embeddings derived from a factual encoder, revealing a distinct separation in
semantic trajectories: factual responses preserve stable alignment, while
hallucinations exhibit pronounced semantic drift across depth. Evaluated on the
TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an
F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming
SelfCheckGPT and Semantic Entropy baselines while requiring only a single
forward pass. This efficiency yields a 5-20x speedup over sampling-based
methods without sacrificing precision or interpretability. LSD offers a
scalable, model-agnostic mechanism for real-time hallucination monitoring and
provides new insights into the geometry of factual consistency within large
language models.

</details>


### [180] [ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection](https://arxiv.org/abs/2510.03502)
*Ali Khairallah,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: ALHD是首个大规模阿拉伯语数据集，用于区分人类和LLM生成文本，涵盖新闻、社交媒体和评论，支持MSA和方言，包含超40万样本。实验表明，微调后的BERT模型在区分任务上表现优于LLM模型，但在跨领域泛化方面存在挑战，尤其是在新闻领域。


<details>
  <summary>Details</summary>
Motivation: 现有Arabic LLM生成文本检测数据集的缺乏，特别是大规模、多样化和标准化的数据集的缺失，阻碍了对Arabic LLM生成文本检测技术的研究和评估。

Method: 构建了一个名为ALHD的大规模阿拉伯语数据集，包含新闻、社交媒体和评论三种体裁，覆盖MSA和方言。该数据集包含由三个领先LLM生成和多个真实人类来源的超过40万个平衡样本。对该数据集进行了预处理、标注和标准化分割。此外，还进行了基准实验，评估了传统分类器、BERT模型和LLM（零样本和少样本）在ALHD上的表现。

Result: 微调后的BERT模型在ALHD数据集上取得了有竞争力的性能，优于基于LLM的模型。然而，模型在跨领域泛化方面遇到挑战，特别是在处理新闻文章时，因为LLM生成文本在风格上与人类文本相似，导致模型泛化能力下降。

Conclusion: ALHD数据集为Arabic LLM生成文本检测研究奠定了基础，有助于识别LLM生成文本的潜在风险，如错误信息、学术不端和网络威胁。未来的研究应着重解决跨领域泛化问题，特别是在新闻领域，以提高Arabic LLM生成文本检测的鲁棒性。

Abstract: We introduce ALHD, the first large-scale comprehensive Arabic dataset
explicitly designed to distinguish between human- and LLM-generated texts. ALHD
spans three genres (news, social media, reviews), covering both MSA and
dialectal Arabic, and contains over 400K balanced samples generated by three
leading LLMs and originated from multiple human sources, which enables studying
generalizability in Arabic LLM-genearted text detection. We provide rigorous
preprocessing, rich annotations, and standardized balanced splits to support
reproducibility. In addition, we present, analyze and discuss benchmark
experiments using our new dataset, in turn identifying gaps and proposing
future research directions. Benchmarking across traditional classifiers,
BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that
fine-tuned BERT models achieve competitive performance, outperforming LLM-based
models. Results are however not always consistent, as we observe challenges
when generalizing across genres; indeed, models struggle to generalize when
they need to deal with unseen patterns in cross-genre settings, and these
challenges are particularly prominent when dealing with news articles, where
LLM-generated texts resemble human texts in style, which opens up avenues for
future research. ALHD establishes a foundation for research related to Arabic
LLM-detection and mitigating risks of misinformation, academic dishonesty, and
cyber threats.

</details>


### [181] [Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)](https://arxiv.org/abs/2510.04950)
*Om Dobariya,Akhil Kumar*

Main category: cs.CL

TL;DR: 研究表明，在多项选择题任务中，ChatGPT 4o对无礼提示的响应比对礼貌提示的响应更准确，这与之前的研究结果相反。


<details>
  <summary>Details</summary>
Motivation: 研究人类提示的礼貌和语气如何影响大型语言模型（LLM）的准确性，特别是探讨礼貌程度与模型在多项选择题上的表现之间的关系。

Method: 创建了一个包含50个基础问题（涵盖数学、科学和历史）的数据集，并将其改写为五个不同的语气版本（非常礼貌、礼貌、中性、粗鲁、非常粗鲁），共生成250个独特提示。使用ChatGPT 4o评估了模型在这些不同提示下的响应，并使用配对样本t检验进行统计显著性分析。

Result: 研究发现，无礼提示的准确率（84.8%）持续高于礼貌提示（80.8%），这与预期相反。

Conclusion: 研究结果表明，在处理多项选择题时，ChatGPT 4o对无礼提示的反应比对礼貌提示更优。这一发现与以往将粗鲁与较差结果联系起来的研究不同，暗示了更新的模型可能对语气变化有不同的反应。该研究强调了研究提示的语用方面的重要性，并引发了关于人机交互社会维度的更广泛问题。

Abstract: The wording of natural language prompts has been shown to influence the
performance of large language models (LLMs), yet the role of politeness and
tone remains underexplored. In this study, we investigate how varying levels of
prompt politeness affect model accuracy on multiple-choice questions. We
created a dataset of 50 base questions spanning mathematics, science, and
history, each rewritten into five tone variants: Very Polite, Polite, Neutral,
Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we
evaluated responses across these conditions and applied paired sample t-tests
to assess statistical significance. Contrary to expectations, impolite prompts
consistently outperformed polite ones, with accuracy ranging from 80.8% for
Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from
earlier studies that associated rudeness with poorer outcomes, suggesting that
newer LLMs may respond differently to tonal variation. Our results highlight
the importance of studying pragmatic aspects of prompting and raise broader
questions about the social dimensions of human-AI interaction.

</details>


### [182] [TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning](https://arxiv.org/abs/2510.03519)
*Fangxu Yu,Hongyu Zhao,Tianyi Zhou*

Main category: cs.CL

TL;DR: TS-Reasoner通过对时间序列基础模型(TSFM)的潜在表征与大语言模型(LLM)的文本输入进行对齐，以实现时间序列的理解与推理。该模型采用新颖的合成数据集和两阶段训练策略，在保持TSFM冻结的情况下，实现了优于现有模型且数据效率更高的性能。


<details>
  <summary>Details</summary>
Motivation: 整合时间序列基础模型(TSFM)在处理数值数据和低阶动态模式方面的优势，以及大语言模型(LLM)在背景知识和复杂推理方面的能力，以解决现有TSFM在时间序列理解与推理方面的不足。

Method: 首先，通过策展多样化的合成时间序列及其对应的文本描述对，实现TSFM的潜在表征与LLM的文本输入的对齐。其次，采用包含对齐预训练和指令微调的两阶段训练策略。在整个训练过程中，冻结预训练的TSFM。

Result: 在多个基准测试中，TS-Reasoner在时间序列的理解与推理任务上，相较于现有的LLM、视觉语言模型(VLM)和时间序列LLM，取得了更优越的性能。同时，该模型在数据效率方面表现出色，仅使用了不到一半的训练数据即可达到同等水平。

Conclusion: TS-Reasoner成功地将TSFM和LLM进行了有效的对齐，克服了各自的局限性，为时间序列的理解与推理任务提供了一个高性能且数据高效的解决方案。

Abstract: Time series reasoning is crucial to decision-making in diverse domains,
including finance, energy usage, traffic, weather, and scientific discovery.
While existing time series foundation models (TSFMs) can capture low-level
dynamic patterns and provide accurate forecasting, further analysis usually
requires additional background knowledge and sophisticated reasoning, which are
lacking in most TSFMs but can be achieved through large language models (LLMs).
On the other hand, without expensive post-training, LLMs often struggle with
the numerical understanding of time series data. Although it is intuitive to
integrate the two types of models, developing effective training recipes that
align the two modalities for reasoning tasks is still an open challenge. To
this end, we propose TS-Reasoner that aligns the latent representations of
TSFMs with the textual inputs of LLMs for downstream understanding/reasoning
tasks. Specifically, we propose a simple yet effective method to curate
diverse, synthetic pairs of time series and textual captions for alignment
training. We then develop a two-stage training recipe that applies instruction
finetuning after the alignment pretraining. Unlike existing works that train an
LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it
during training. Extensive experiments on several benchmarks demonstrate that
TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision
Language Models (VLMs), and Time Series LLMs, but also achieves this with
remarkable data efficiency, e.g., using less than half the training data.

</details>


### [183] [Identifying Financial Risk Information Using RAG with a Contrastive Insight](https://arxiv.org/abs/2510.03521)
*Ali Elahi*

Main category: cs.CL

TL;DR: 在检索增强生成（RAG）的基础上，提出了一种新的对比推理方法，以解决LLM在专业领域推理时缺乏可比案例的问题，并在文本生成方面优于基线RAG。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法在专业领域推理时，无法检索可比案例或相关问题，导致LLM的输出过于泛化，缺乏领域特异性见解。

Method: 提出了一种在RAG之上增加“同类感知对比推理层”的方法。

Result: 该对比方法在ROUGE和BERTScore等文本生成指标上优于基线RAG，生成的文本在内容上与人类研究和风险评估相当。

Conclusion: 提出的对比推理方法能够有效解决RAG在专业领域推理的局限性，生成更具洞察力和可比性的内容。

Abstract: In specialized domains, humans often compare new problems against similar
examples, highlight nuances, and draw conclusions instead of analyzing
information in isolation. When applying reasoning in specialized contexts with
LLMs on top of a RAG, the pipeline can capture contextually relevant
information, but it is not designed to retrieve comparable cases or related
problems.
  While RAG is effective at extracting factual information, its outputs in
specialized reasoning tasks often remain generic, reflecting broad facts rather
than context-specific insights. In finance, it results in generic risks that
are true for the majority of companies. To address this limitation, we propose
a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics
such as ROUGE and BERTScore in comparison with human-generated equity research
and risk.

</details>


### [184] [Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs](https://arxiv.org/abs/2510.03527)
*Sayan Ghosh,Shahzaib Saqib Warraich,Dhruv Tarsadiya,Gregory Yauney,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: Consensus Graphs (ConGrs) 是一种新的数据结构，用于合成来自语言模型（LM）多个采样的响应，以提高事实准确性和决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法无法有效地整合语言模型（LM）在同一提示下生成的多长篇幅响应中的丰富认知信号，因此需要一种更有效的方法来处理这些信息。

Method: 首先，使用轻量级的词汇序列比对算法和辅助的次级LM裁判来构建ConGrs。然后，设计任务特定的解码方法，从ConGr数据结构中合成单个、最终的响应。

Result: 在传记生成任务上，使用ConGrs合成的响应将事实准确性提高了31%。在需要对无法回答的查询进行弃权的任务上，弃权率提高了56%。在MATH和AIME推理任务上，准确率比自我验证和多数投票基线提高了6个百分点。

Conclusion: ConGrs提供了一种灵活的方法来捕捉LM响应中的变异性，并利用响应变异性提供的认知信号来合成更有效的响应。

Abstract: Language models can be sampled multiple times to access the distribution
underlying their responses, but existing methods cannot efficiently synthesize
rich epistemic signals across different long-form responses. We introduce
Consensus Graphs (ConGrs), a flexible DAG-based data structure that represents
shared information, as well as semantic variation in a set of sampled LM
responses to the same prompt. We construct ConGrs using a light-weight lexical
sequence alignment algorithm from bioinformatics, supplemented by the targeted
usage of a secondary LM judge. Further, we design task-dependent decoding
methods to synthesize a single, final response from our ConGr data structure.
Our experiments show that synthesizing responses from ConGrs improves factual
precision on two biography generation tasks by up to 31% over an average
response and reduces reliance on LM judges by more than 80% compared to other
methods. We also use ConGrs for three refusal-based tasks requiring abstention
on unanswerable queries and find that abstention rate is increased by up to
56%. We apply our approach to the MATH and AIME reasoning tasks and find an
improvement over self-verification and majority vote baselines by up to 6
points of accuracy. We show that ConGrs provide a flexible method for capturing
variation in LM responses and using the epistemic signals provided by response
variation to synthesize more effective responses.

</details>


### [185] [Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance](https://arxiv.org/abs/2510.03528)
*Ahmed Alajrami,Xingwei Tan,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 通过在指令调优数据中引入扰动（如删除停用词或打乱词序），可以提高大型语言模型（LLM）对噪声指令的抵抗力，有时甚至能提升下游性能。


<details>
  <summary>Details</summary>
Motivation: 之前的研究表明，大型语言模型（LLM）在指令调优后，其响应能力会得到提升，但它们对指令措辞的微小变化很敏感。本研究旨在探索通过在指令调优数据中引入扰动，是否能增强LLM对噪声指令的抵抗能力。

Method: 本研究探索了在指令调优数据中引入扰动（如删除停用词或打乱词序）对LLM在原始和扰动版本广泛使用的基准测试（MMLU、BBH、GSM8K）上性能的影响。同时，还评估了学习动态和模型行为的潜在变化。

Result: 结果表明，在扰动后的指令上进行指令调优，在某些情况下可以提高下游性能。这表明包含扰动指令的指令调优可以使LLM更能抵抗有噪声的用户输入。

Conclusion: 本研究的发现强调了在指令调优中包含扰动指令的重要性，这可以使LLM更能抵抗有噪声的用户输入。

Abstract: Instruction-tuning plays a vital role in enhancing the task-solving abilities
of large language models (LLMs), improving their usability in generating
helpful responses on various tasks. However, previous work has demonstrated
that they are sensitive to minor variations in instruction phrasing. In this
paper, we explore whether introducing perturbations in instruction-tuning data
can enhance LLMs' resistance against noisy instructions. We focus on how
instruction-tuning with perturbations, such as removing stop words or shuffling
words, affects LLMs' performance on the original and perturbed versions of
widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics
and potential shifts in model behavior. Surprisingly, our results suggest that
instruction-tuning on perturbed instructions can, in some cases, improve
downstream performance. These findings highlight the importance of including
perturbed instructions in instruction-tuning, which can make LLMs more
resilient to noisy user inputs.

</details>


### [186] [TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering](https://arxiv.org/abs/2510.03536)
*Zhaohan Meng,Zaiqiao Meng,Siwei Liu,Iadh Ounis*

Main category: cs.CL

TL;DR: TriMediQ通过将患者响应转换为三元组结构并将其整合到知识图中，实现了LLM在多轮临床诊断中的多跳推理能力，提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在处理多轮临床对话时，由于事实信息分散且缺乏明确关联，其推理能力会显著下降，无法满足实际临床需求。

Method: TriMediQ框架提出了一种基于三元组和知识图谱的方法。首先，使用一个固定的三元组生成器提取临床相关信息，并保证事实一致性。然后，一个包含图编码器和投影器的可训练模块从知识图中提取关系信息，以增强推理能力。该框架分两步进行：1. 冻结LLM的权重，只对投影模块进行微调；2. 在推理过程中，使用微调后的模块引导多跳推理。

Result: 在iMedQA数据集上，TriMediQ相较于五个基线模型，准确率最高提升了10.4%。

Conclusion: 将患者响应转换为结构化的三元组图谱，可以使LLM在多轮对话中进行更准确的临床推理，为部署基于LLM的医疗助手提供了解决方案。

Abstract: Large Language Models (LLMs) perform strongly in static and single-turn
medical Question Answer (QA) benchmarks, yet such settings diverge from the
iterative information gathering process required in practical clinical
consultations. The MEDIQ framework addresses this mismatch by recasting the
diagnosis as an interactive dialogue between a patient and an expert system,
but the reliability of LLMs drops dramatically when forced to reason with
dialogue logs, where clinical facts appear in sentences without clear links. To
bridge this gap, we introduce TriMediQ, a triplet-structured approach that
summarises patient responses into triplets and integrates them into a Knowledge
Graph (KG), enabling multi-hop reasoning. We introduce a frozen triplet
generator that extracts clinically relevant triplets, using prompts designed to
ensure factual consistency. In parallel, a trainable projection module,
comprising a graph encoder and a projector, captures relational information
from the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)
the projection module fine-tuning with all LLM weights frozen; and (ii) using
the fine-tuned module to guide multi-hop reasoning during inference. We
evaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up
to 10.4\% improvement in accuracy over five baselines on the iMedQA dataset.
These results demonstrate that converting patient responses into structured
triplet-based graphs enables more accurate clinical reasoning in multi-turn
settings, providing a solution for the deployment of LLM-based medical
assistants.

</details>


### [187] [What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification](https://arxiv.org/abs/2510.03541)
*Andrew Halterman,Katherine A. Keith*

Main category: cs.CL

TL;DR: LLMs在计算社会科学（CSS）中广泛用于文本分类，但本研究强调了LLM提示之前（概念化）和之后（下游统计推断）的步骤。作者认为，LLM可能导致分析人员跳过概念化步骤，从而产生偏差。模拟研究表明，这种偏差无法仅通过提高LLM准确性或事后纠正方法来纠正。研究结论强调，在LLM时代，概念化仍然是CSS领域的一个首要问题，并为CSS分析人员提供了如何在低成本、无偏差、低方差的下游估计方面提供具体建议。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决在计算社会科学（CSS）中使用生成式大型语言模型（LLMs）进行文本分类时，被忽视但至关重要的概念化和下游统计推断步骤。作者认为，LLMs的广泛应用可能会诱导分析人员跳过严谨的概念化过程，从而引入偏差，影响研究结果的准确性。

Method: 作者通过模拟研究来检验概念化偏差的影响。他们比较了仅提高LLM准确性或使用事后偏差校正方法与解决概念化问题的效果，以评估这些方法能否纠正由概念化不足引起的偏差。

Result: 模拟研究结果表明，仅仅提高LLM的准确性或采用事后偏差校正方法，都无法有效消除或纠正由概念化不当引起的偏差。这表明概念化过程的严谨性是影响下游统计推断准确性的关键因素。

Conclusion: 研究结论强调，即使在大型语言模型（LLM）时代，概念化仍然是计算社会科学（CSS）研究中的一个基本且首要的问题。作者建议CSS分析人员应重视概念化阶段，并为他们提供了如何在实践中实现低成本、无偏差、低方差的下游估计的具体指导。

Abstract: Generative large language models (LLMs) are now used extensively for text
classification in computational social science (CSS). In this work, focus on
the steps before and after LLM prompting -- conceptualization of concepts to be
classified and using LLM predictions in downstream statistical inference --
which we argue have been overlooked in much of LLM-era CSS. We claim LLMs can
tempt analysts to skip the conceptualization step, creating conceptualization
errors that bias downstream estimates. Using simulations, we show that this
conceptualization-induced bias cannot be corrected for solely by increasing LLM
accuracy or post-hoc bias correction methods. We conclude by reminding CSS
analysts that conceptualization is still a first-order concern in the LLM-era
and provide concrete advice on how to pursue low-cost, unbiased, low-variance
downstream estimates.

</details>


### [188] [CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making](https://arxiv.org/abs/2510.03553)
*Hasibur Rahman,Hanan Salam*

Main category: cs.CL

TL;DR: CCD-Bench是一个评估大型语言模型在跨文化价值冲突中决策能力的新基准，它包含2182个开放式困境，涵盖七个领域，并使用GLOBE文化群集进行评估。结果显示，模型偏好某些文化区域，并且在解释中虽然提及多个文化维度，但实际应用却流于表面。此外，模型之间的相似性更多地基于开发者，而非地理位置。这表明当前的LLM对齐方法倾向于一种“共识”的、服务于少数文化的世界观，而忽略了需要权力协商、基于权利的推理或性别平等分析的场景。CCD-Bench的出现，将评估重点从单一偏见检测转向了多元化决策，并强调了在模型对齐策略中实质性地纳入不同世界观的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）评估基准主要关注文化知识、价值预测或单一维度的偏见，但未能有效评估LLM在面对不同文化价值观直接冲突时的决策能力。本文旨在解决这一缺口，提出一个名为CCD-Bench的新基准，专门用于评估LLM在跨文化价值冲突情境下的决策能力。

Method: 本文提出了CCD-Bench，一个包含2182个开放式困境的基准，涵盖七个领域。每个困境都配有十个匿名选项，代表十个GLOBE文化群集。为了减少排序效应，困境的呈现采用了分层拉丁方设计。研究评估了17个非推理LLM，并分析了它们的决策模式和理由。

Result: 研究发现，LLM在决策时不成比例地偏好斯堪的纳维亚欧洲（占20.2%）和日耳曼欧洲（占12.4%）的选项，而对东欧、中东和北非等地区的选项代表性不足（仅占5.6%至5.8%）。尽管87.9%的理由提到了多个GLOBE维度，但这种“多元化”是肤浅的，模型主要组合了“未来导向”和“绩效导向”，而很少使用“自信”或“性别平等”维度（均低于3%）。排序效应不显著（Cramer's V < 0.10），且基于对称KL散度分析，模型聚类更多地基于开发者谱系而非地理位置。

Conclusion: CCD-Bench的评估结果表明，当前LLM的对齐策略倾向于推广一种以共识为导向的世界观，这种世界观忽视了需要进行权力协商、基于权利的推理或性别平等等复杂情境的决策。CCD-Bench的提出，将LLM的评估从孤立的偏见检测，转向了更注重多元化决策能力，并突显了在模型对齐策略中，需要实质性地纳入和考虑不同世界观的必要性。

Abstract: Although large language models (LLMs) are increasingly implicated in
interpersonal and societal decision-making, their ability to navigate explicit
conflicts between legitimately different cultural value systems remains largely
unexamined. Existing benchmarks predominantly target cultural knowledge
(CulturalBench), value prediction (WorldValuesBench), or single-axis bias
diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple
culturally grounded values directly clash. We address this gap with CCD-Bench,
a benchmark that assesses LLM decision-making under cross-cultural value
conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains,
each paired with ten anonymized response options corresponding to the ten GLOBE
cultural clusters. These dilemmas are presented using a stratified Latin square
to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models
disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe
(12.4 percent), while options for Eastern Europe and the Middle East and North
Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of
rationales reference multiple GLOBE dimensions, this pluralism is superficial:
models recombine Future Orientation and Performance Orientation, and rarely
ground choices in Assertiveness or Gender Egalitarianism (both under 3
percent). Ordering effects are negligible (Cramer's V less than 0.10), and
symmetrized KL divergence shows clustering by developer lineage rather than
geography. These patterns suggest that current alignment pipelines promote a
consensus-oriented worldview that underserves scenarios demanding power
negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts
evaluation beyond isolated bias detection toward pluralistic decision making
and highlights the need for alignment strategies that substantively engage
diverse worldviews.

</details>


### [189] [Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models](https://arxiv.org/abs/2510.03561)
*Adam Filipek*

Main category: cs.CL

TL;DR: Transformer在对话AI中因状态丢失和二次计算复杂度而受限。RxT提出了一种事件驱动范式，使用固定大小的短期记忆（STM）来实时处理对话，将对话成本从二次降低到线性，并实现低延迟和状态化的长对话。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在语言理解和生成方面表现出色，但在对话AI中的应用受到其无状态特性和与序列长度相关的二次计算复杂度的限制，导致长对话成本高昂且延迟大。

Method: 提出了一种名为Reactive Transformer (RxT)的新型架构，它采用事件驱动范式，将每次对话轮次视为一个离散事件，并通过一个集成的、固定大小的短期记忆（STM）系统来维护上下文。该架构包含一个生成器-解码器，用于基于当前查询和先前记忆状态生成响应，然后一个记忆编码器和一个专门的记忆注意力网络异步地用交互的表示来更新STM。

Result: RxT将对话的总成本从与交互次数N相关的二次方（O(N^2·T)）降低到线性（O(N·T)），实现了低延迟，并在一系列概念验证实验中，与同等规模的无状态基线模型相比，在合成数据上表现出卓越的性能和恒定的推理延迟。

Conclusion: RxT通过从数据驱动转向事件驱动的范式，并结合一个高效的短期记忆系统，成功克服了Transformer在长对话中的局限性，实现了低延迟、状态化且经济可行的长对话。

Abstract: The Transformer architecture has become the de facto standard for Large
Language Models (LLMs), demonstrating remarkable capabilities in language
understanding and generation. However, its application in conversational AI is
fundamentally constrained by its stateless nature and the quadratic
computational complexity ($O(L^2)$) with respect to sequence length $L$.
Current models emulate memory by reprocessing an ever-expanding conversation
history with each turn, leading to prohibitive costs and latency in long
dialogues. This paper introduces the Reactive Transformer (RxT), a novel
architecture designed to overcome these limitations by shifting from a
data-driven to an event-driven paradigm. RxT processes each conversational turn
as a discrete event in real-time, maintaining context in an integrated,
fixed-size Short-Term Memory (STM) system. The architecture features a distinct
operational cycle where a generator-decoder produces a response based on the
current query and the previous memory state, after which a memory-encoder and a
dedicated Memory Attention network asynchronously update the STM with a
representation of the complete interaction. This design fundamentally alters
the scaling dynamics, reducing the total user-facing cost of a conversation
from quadratic ($O(N^2 \cdot T)$) to linear ($O(N \cdot T)$) with respect to
the number of interactions $N$. By decoupling response generation from memory
updates, RxT achieves low latency, enabling truly real-time, stateful, and
economically viable long-form conversations. We validated our architecture with
a series of proof-of-concept experiments on synthetic data, demonstrating
superior performance and constant-time inference latency compared to a baseline
stateless model of comparable size.

</details>


### [190] [LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction](https://arxiv.org/abs/2510.03577)
*Ikram Belmadani,Parisa Nazari Hashemi,Thomas Sebbag,Benoit Favre,Guillaume Fortier,Solen Quiniou,Emmanuel Morin,Richard Dufour*

Main category: cs.CL

TL;DR: 评估LLM 2025生物医学命名实体识别（NER）和法语健康事件提取（少样本）的挑战，使用GPT-4.1、GLiNER和LLaMA-3.1-8B-Instruct。GPT-4.1在NER（宏F1 61.53%）和事件提取（15.02%）中表现最佳，表明在资源极少的情况下，精心设计的提示至关重要。


<details>
  <summary>Details</summary>
Motivation: 参与LLM 2025挑战，解决生物医学命名实体识别（NER）和法语健康事件提取的少样本问题。

Method: 提出三种NER方法：1. GPT-4.1的上下文学习（ICL），包括自动选择示例和指南摘要；2. GLiNER系统，在合成语料上微调后由LLM后处理；3. LLaMA-3.1-8B-Instruct，在相同合成语料上微调。事件提取使用GPT-4.1的ICL策略。

Result: GPT-4.1在NER任务上达到61.53%的宏F1分数，在事件提取任务上达到15.02%的分数。

Conclusion: 在资源极少的情况下，精心设计的提示对于最大化模型性能至关重要，GPT-4.1在所提出的方法中表现最佳。

Abstract: This work presents our participation in the EvalLLM 2025 challenge on
biomedical Named Entity Recognition (NER) and health event extraction in French
(few-shot setting). For NER, we propose three approaches combining large
language models (LLMs), annotation guidelines, synthetic data, and
post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating
automatic selection of 10 examples and a summary of the annotation guidelines
into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic
corpus and then verified by an LLM in post-processing, and (3) the open LLM
LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event
extraction uses the same ICL strategy with GPT-4.1, reusing the guideline
summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for
NER and 15.02% for event extraction, highlighting the importance of
well-crafted prompting to maximize performance in very low-resource scenarios.

</details>


### [191] [Decoupling Task-Solving and Output Formatting in LLM Generation](https://arxiv.org/abs/2510.03595)
*Haikang Deng,Po-Nien Kung,Nanyun Peng*

Main category: cs.CL

TL;DR: Deco-G框架将LLM的格式遵循与任务解决分离开来，通过一个单独的概率模型来处理格式，从而提高了在复杂指令下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 当指令中包含复杂的推理和严格的格式要求时，LLM难以同时满足，因为这些目标存在冲突。

Method: 提出Deco-G解码框架，使用一个单独的概率模型（TPM）来处理格式遵循，而LLM仅处理任务指令。在每个解码步骤中，结合LLM的token概率和TPM计算的格式遵循度来形成最终输出。引入了指令感知蒸馏、灵活的trie构建算法和HMM状态剪枝以提高效率和可扩展性。

Result: 在包括数学推理、LLM-as-a-judge和事件参数提取在内的多种任务上，Deco-G相比常规提示方法，在保证格式遵循的同时，实现了1.0%至6.0%的相对性能提升。

Conclusion: Deco-G框架通过显式地将格式遵循与任务解决分离开来，能够有效地提高LLM在复杂指令下的性能，并确保格式的严格遵守。

Abstract: Large language models (LLMs) are increasingly adept at following instructions
containing task descriptions to solve complex problems, such as mathematical
reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow
more complex, models often struggle to adhere to all instructions. This
difficulty is especially common when instructive prompts intertwine reasoning
directives -- specifying what the model should solve -- with rigid formatting
requirements that dictate how the solution must be presented. The entanglement
creates competing goals for the model, suggesting that more explicit separation
of these two aspects could lead to improved performance. To this front, we
introduce Deco-G, a decoding framework that explicitly decouples format
adherence from task solving. Deco-G handles format compliance with a separate
tractable probabilistic model (TPM), while prompts LLMs with only task
instructions. At each decoding step, Deco-G combines next token probabilities
from the LLM with the TPM calculated format compliance likelihood to form the
output probability. To make this approach both practical and scalable for
modern instruction-tuned LLMs, we introduce three key innovations:
instruction-aware distillation, a flexible trie-building algorithm, and HMM
state pruning for computational efficiency. We demonstrate the effectiveness of
Deco-G across a wide range of tasks with diverse format requirements, including
mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,
our approach yields 1.0% to 6.0% relative gain over regular prompting practice
with guaranteed format compliance.

</details>


### [192] [Can an LLM Induce a Graph? Investigating Memory Drift and Context Length](https://arxiv.org/abs/2510.03611)
*Raquib Bin Yousuf,Aadyant Khatri,Shengzhe Xu,Mandar Sharma,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: 现有基准测试未能准确评估LLM在信息密集场景下的表现，特别是在需要诱导结构化关系知识（如从文本生成图）的复杂推理任务上。研究发现，LLM在执行此类任务时，比现有基准测试显示的更短的有效上下文长度下就会出现记忆漂移和上下文遗忘。本文提出了一种新的评估方法，并指出了LLM在长距离推理方面的局限性，建议进行架构改进。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准（如‘海量信息中的一根针’检索或续写任务）未能准确反映模型在信息密集场景下的性能，因为它们过于简单，无法体现模型在复杂推理任务中诱导结构化关系知识（如从文本生成图）的能力。因此，有必要开发更有效的评估方法。

Method: 本文提出了一种新的评估方法，要求LLM在信息密集且包含无关信息的长上下文文本中，通过诱导结构化关系知识（例如，从自然语言内容生成图）来完成复杂的推理任务，而不是简单的下一个词预测。

Result: 研究发现，与现有基准测试相比，LLM在执行这种需要诱导结构化关系知识的推理任务时，在更短的有效上下文长度下就会出现记忆漂移和上下文遗忘。即使是专门为推理设计的模型（如OpenAI o1）也容易出现早期记忆漂移。

Conclusion: LLM在从非结构化输入中抽象结构化知识方面存在显著局限性，并且在长距离推理能力方面存在不足。现有评估基准可能高估了LLM的实际性能。需要对模型架构进行适应性改造，以提高其长距离推理能力，并为在复杂推理任务中优化LLM的使用提供建议。

Abstract: Recently proposed evaluation benchmarks aim to characterize the effective
context length and the forgetting tendencies of large language models (LLMs).
However, these benchmarks often rely on simplistic 'needle in a haystack'
retrieval or continuation tasks that may not accurately reflect the performance
of these models in information-dense scenarios. Thus, rather than simple next
token prediction, we argue for evaluating these models on more complex
reasoning tasks that requires them to induce structured relational knowledge
from the text - such as graphs from potentially noisy natural language content.
While the input text can be viewed as generated in terms of a graph, its
structure is not made explicit and connections must be induced from distributed
textual cues, separated by long contexts and interspersed with irrelevant
information. Our findings reveal that LLMs begin to exhibit memory drift and
contextual forgetting at much shorter effective lengths when tasked with this
form of relational reasoning, compared to what existing benchmarks suggest.
With these findings, we offer recommendations for the optimal use of popular
LLMs for complex reasoning tasks. We further show that even models specialized
for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in
these settings. These results point to significant limitations in the models'
ability to abstract structured knowledge from unstructured input and highlight
the need for architectural adaptations to improve long-range reasoning.

</details>


### [193] [Towards Unsupervised Speech Recognition at the Syllable-Level](https://arxiv.org/abs/2510.03639)
*Liming Wang,Junrui Ni,Kai-Wei Chang,Saurabhchand Bhati,David Harwath,Mark Hasegawa-Johnson,James R. Glass*

Main category: cs.CL

TL;DR: 本研究提出了一种基于音节级别的无监督语音识别（UASR）框架，通过掩码语言建模来解决现有基于音素的方法在低资源语言和多模态学习方面存在的成本高和泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于音素的UASR方法需要昂贵的音素转换器（G2P），并且在具有模糊音素边界的语言上训练不稳定，泛化能力差。本研究旨在克服这些挑战。

Method: 提出了一种基于音节级别的UASR框架，利用掩码语言建模，避免了对G2P的需求以及基于生成对抗网络（GAN）方法的训练不稳定问题。

Result: 在LibriSpeech数据集上，字符错误率（CER）相对降低了40%。该方法能有效泛化到普通话，解决了以往方法在该语言上的难题。

Conclusion: 本研究提出的基于音节级别掩码语言建模的UASR框架，在降低成本、提高训练稳定性和泛化能力方面取得了显著成效，特别是在低资源和具有挑战性的语言（如普通话）上。

Abstract: Training speech recognizers with unpaired speech and text -- known as
unsupervised speech recognition (UASR) -- is a crucial step toward extending
ASR to low-resource languages in the long-tail distribution and enabling
multimodal learning from non-parallel data. However, existing approaches based
on phones often rely on costly resources such as grapheme-to-phoneme converters
(G2Ps) and struggle to generalize to languages with ambiguous phoneme
boundaries due to training instability. In this paper, we address both
challenges by introducing a syllable-level UASR framework based on masked
language modeling, which avoids the need for G2P and the instability of
GAN-based methods. Our approach achieves up to a 40\% relative reduction in
character error rate (CER) on LibriSpeech and generalizes effectively to
Mandarin, a language that has remained particularly difficult for prior
methods. Code will be released upon acceptance.

</details>


### [194] [UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG](https://arxiv.org/abs/2510.03663)
*Xiangyu Peng,Cab Qin,Zeyuan Chen,Ran Xu,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: UniDoc-Bench 是首个大规模、真实世界的文档中心多模态检索增强生成 (MM-RAG) 基准，包含 70k PDF 页面，涵盖 8 个领域，用于评估不同 MM-RAG 范式。


<details>
  <summary>Details</summary>
Motivation: 现有 MM-RAG 评估不足，无法反映真实世界的文档中心应用场景。

Method: 构建 UniDoc-Bench 基准，包含从文本、表格和图形中提取和链接的证据，生成 1600 个多模态问答对，并包含 20% 的验证数据。该基准支持四种范式（纯文本、纯图像、多模态文本-图像融合、多模态联合检索）的统一协议评估。

Result: 实验表明，多模态文本-图像融合 RAG 系统优于单模态和基于多模态联合嵌入的检索方法，表明单独的文本或图像不足以应对复杂任务，并且现有的多模态嵌入仍需改进。

Conclusion: UniDoc-Bench 为 MM-RAG 提供了标准化的评估框架，其分析揭示了视觉上下文如何补充文本证据、识别了系统性失败模式，并为开发更强大的 MM-RAG 管道提供了指导。

Abstract: Multimodal retrieval-augmented generation (MM-RAG) is a key approach for
applying large language models (LLMs) and agents to real-world knowledge bases,
yet current evaluations are fragmented, focusing on either text or images in
isolation or on simplified multimodal setups that fail to capture
document-centric multimodal use cases. In this paper, we introduce
UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from
70k real-world PDF pages across eight domains. Our pipeline extracts and links
evidence from text, tables, and figures, then generates 1,600 multimodal QA
pairs spanning factual retrieval, comparison, summarization, and logical
reasoning queries. To ensure reliability, 20% of QA pairs are validated by
multiple annotators and expert adjudication. UniDoc-Bench supports
apples-to-apples comparison across four paradigms: (1) text-only, (2)
image-only, (3) multimodal text-image fusion, and (4) multimodal joint
retrieval -- under a unified protocol with standardized candidate pools,
prompts, and evaluation metrics. Our experiments show that multimodal
text-image fusion RAG systems consistently outperform both unimodal and jointly
multimodal embedding-based retrieval, indicating that neither text nor images
alone are sufficient and that current multimodal embeddings remain inadequate.
Beyond benchmarking, our analysis reveals when and how visual context
complements textual evidence, uncovers systematic failure modes, and offers
actionable guidance for developing more robust MM-RAG pipelines.

</details>


### [195] [Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text](https://arxiv.org/abs/2510.03683)
*Nisar Hussain,Amna Qasim,Gull Mehak,Muhammad Zain,Momina Hafeez,Grigori Sidorov*

Main category: cs.CL

TL;DR: 本研究提出了一种基于QLoRA的微调框架，用于改进罗马乌尔都语-英语混合语言中的冒犯性语言检测。通过将罗马乌尔都语-英语数据集翻译成英语，并利用QLoRA微调了多种大型语言模型（LLMs），实验证明Meta LLaMA 3 8B在低资源混合语言环境中取得了最佳的F1分数（91.45%），表明QLoRA和LLMs在处理此类任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于罗马乌尔都语等混合语言中存在未明确的语法、不一致的拼写和稀缺的标记数据，因此在处理这类语言的冒犯性语言检测时，自然语言处理（NLP）系统面临巨大挑战。

Method: 本研究提出了一种基于QLoRA的微调框架。该框架首先使用谷歌翻译将罗马乌尔都语-英语混合语代码数据集翻译成英语，以便利用现有的英语大型语言模型（LLMs）。然后，使用QLoRA（一种内存高效的适配技术）对多种Transformer和LLMs（包括Meta LLaMA 3 8B、Mistral 7B v0.1、LLaMA 2 7B、ModernBERT和RoBERTa）进行微调。模型在手动注释的罗马乌尔都语数据上进行训练和评估，以区分冒犯性与非冒犯性内容。

Result: 在所有测试的模型中，Meta LLaMA 3 8B达到了最高的F1分数91.45%，其次是Mistral 7B（89.66%），均超过了传统的Transformer基线模型。

Conclusion: 研究结果证明了QLoRA在低资源环境中（如混合语言冒犯性语言检测）微调高性能模型方面的有效性，并证实了LLMs在此任务上的潜力。该研究为罗马乌尔都语内容审核提供了一个可扩展的方法，并为未来基于LLMs的多语言冒犯性检测系统奠定了基础。

Abstract: The use of derogatory terms in languages that employ code mixing, such as
Roman Urdu, presents challenges for Natural Language Processing systems due to
unstated grammar, inconsistent spelling, and a scarcity of labeled data. In
this work, we propose a QLoRA based fine tuning framework to improve offensive
language detection in Roman Urdu-English text. We translated the Roman
Urdu-English code mixed dataset into English using Google Translate to leverage
English LLMs, while acknowledging that this translation reduces direct
engagement with code mixing features. Our focus is on classification
performance using English translated low resource inputs. We fine tuned several
transformers and large language models, including Meta LLaMA 3 8B, Mistral 7B
v0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient
adaptation. Models were trained and evaluated on a manually annotated Roman
Urdu dataset for offensive vs non offensive content. Of all tested models, the
highest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral
7B at 89.66, surpassing traditional transformer baselines. These results
demonstrate the efficacy of QLoRA in fine tuning high performing models for low
resource environments such as code mixed offensive language detection, and
confirm the potential of LLMs for this task. This work advances a scalable
approach to Roman Urdu moderation and paves the way for future multilingual
offensive detection systems based on LLMs.

</details>


### [196] [MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction](https://arxiv.org/abs/2510.03687)
*Yue Huang,Yanyuan Chen,Dexuan Xu,Weihua Yue,Huamin Zhang,Meikang Qiu,Yu Huang*

Main category: cs.CL

TL;DR: MedReflect框架通过模拟医生的反思性思维模式，利用自我验证和自我反思的机制，在无需外部检索或大量标注的情况下，提升了大型语言模型在医学问题解决方面的能力，并能有效降低数据构建成本。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在解决复杂的医学问题时，虽然可以通过引入外部知识验证（如检索增强生成）或在推理数据集上进行训练来提升能力，但这些方法存在检索开销大、标注成本高昂等缺点，并且在医学领域取得有限的性能。因此，需要一种新的方法来激发LLMs在医学问题解决方面的潜能。

Method: MedReflect框架通过生成一个单通道的反思链，包括初始假设生成、自我提问、自我回答和决策优化，来激发LLMs的医生式反思思维模式。这种自我验证和自我反思的特性使得LLMs能够释放其在医学问题解决方面的潜在能力，而无需外部检索或大量标注。

Result: 在仅使用2000个随机抽样的训练样本和轻度微调的情况下，MedReflect框架在多个医学基准测试中实现了显著的绝对准确度提升，同时大幅降低了标注需求。这表明，通过自我反思和自我改进，LLMs能够学习解决专门的医学问题。

Conclusion: 该研究证明了大型语言模型可以通过自我反思和自我改进来学习解决专业的医学问题，从而减少对外部监督和大规模特定任务微调数据的依赖。MedReflect框架提供了一种有效且经济高效的方法来构建医学数据集并提升LLMs在医学领域的表现。

Abstract: Medical problem solving demands expert knowledge and intricate reasoning.
Recent studies of large language models (LLMs) attempt to ease this complexity
by introducing external knowledge verification through retrieval-augmented
generation or by training on reasoning datasets. However, these approaches
suffer from drawbacks such as retrieval overhead and high annotation costs, and
they heavily rely on substituted external assistants to reach limited
performance in medical field. In this paper, we introduce MedReflect, a
generalizable framework designed to inspire LLMs with a physician-like
reflective thinking mode. MedReflect generates a single-pass reflection chain
that includes initial hypothesis generation, self-questioning, self-answering
and decision refinement. This self-verified and self-reflective nature releases
large language model's latent capability in medical problem-solving without
external retrieval or heavy annotation. We demonstrate that MedReflect enables
cost-efficient medical dataset construction: with merely 2,000 randomly sampled
training examples and a light fine-tuning, this approach achieves notable
absolute accuracy improvements across a series of medical benchmarks while
cutting annotation requirements. Our results provide evidence that LLMs can
learn to solve specialized medical problems via self-reflection and
self-improve, reducing reliance on external supervision and extensive
task-specific fine-tuning data.

</details>


### [197] [TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation](https://arxiv.org/abs/2510.03748)
*Ramtin Kakavand,Ebrahim Ansari*

Main category: cs.CL

TL;DR: TreePrompt是一种新颖的少样本提示方法，通过学习LLM的偏好来识别高质量、上下文相关的示例，从而提高机器翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有的少样本提示方法主要关注查询-示例的相似性，忽略了示例本身的质量。TreePrompt旨在解决这一问题，通过考虑示例的质量来选择更优的提示示例。

Method: TreePrompt在一个树状框架内，学习LLM的偏好来识别高质量、上下文相关的示例。此外，为了平衡相似性和质量，还将TreePrompt与K-NN和AFSP结合使用。

Result: 在英语-波斯语（MIZAN）和英语-德语（WMT19）两个语言对的评估中，将TreePrompt与AFSP或随机选择相结合，均能提高翻译性能。

Conclusion: TreePrompt是一种有效的方法，通过结合示例的质量和上下文相关性来改进少样本提示，从而提升机器翻译的性能。

Abstract: Large Language Models (LLMs) have consistently demonstrated strong
performance in machine translation, especially when guided by high-quality
prompts. Few-shot prompting is an effective technique to improve translation
quality; however, most existing example selection methods focus solely on
query-to-example similarity and do not account for the quality of the examples.
In this work, we propose TreePrompt, a novel example selection approach that
learns LLM preferences to identify high-quality, contextually relevant examples
within a tree-structured framework. To further explore the balance between
similarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)
and Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -
English-Persian (MIZAN) and English-German (WMT19) - show that integrating
TreePrompt with AFSP or Random selection leads to improved translation
performance.

</details>


### [198] [Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech](https://arxiv.org/abs/2510.03758)
*Ilias Tougui,Mehdi Zakroum,Mounir Ghogho*

Main category: cs.CL

TL;DR: Parkinson's disease (PD) can be detected using speech analysis, and a new method analyzing phonemes, syllables, and words simultaneously proved most effective, especially phoneme-level analysis.


<details>
  <summary>Details</summary>
Motivation: Current speech-based PD detection systems analyze whole utterances, potentially missing diagnostic information from specific phonetic elements.

Method: Developed a pipeline for extracting time-aligned phonemes, syllables, and words, and used a bidirectional LSTM with multi-head attention to compare diagnostic performance across these granularity levels using Italian, Spanish, and English datasets.

Result: Phoneme-level analysis achieved superior performance with AUROC of 93.78% and accuracy of 92.17%. Attention analysis showed that the most informative speech features included sustained vowels, diadochokinetic syllables, and /pataka/ sequences.

Conclusion: A granularity-aware approach, particularly at the phoneme level, enhances diagnostic capability for cross-linguistic PD detection, aligning with clinical protocols.

Abstract: Parkinson's Disease (PD) affects over 10 million people worldwide, with
speech impairments in up to 89% of patients. Current speech-based detection
systems analyze entire utterances, potentially overlooking the diagnostic value
of specific phonetic elements. We developed a granularity-aware approach for
multilingual PD detection using an automated pipeline that extracts
time-aligned phonemes, syllables, and words from recordings. Using Italian,
Spanish, and English datasets, we implemented a bidirectional LSTM with
multi-head attention to compare diagnostic performance across the different
granularity levels. Phoneme-level analysis achieved superior performance with
AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates
enhanced diagnostic capability for cross-linguistic PD detection. Importantly,
attention analysis revealed that the most informative speech features align
with those used in established clinical protocols: sustained vowels (/a/, /e/,
/o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/)
at syllable level, and /pataka/ sequences at word level. Source code will be
available at https://github.com/jetliqs/clearpd.

</details>


### [199] [Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs](https://arxiv.org/abs/2510.03762)
*Deshan Sumanathilaka,Nicholas Micallef,Julian Hough*

Main category: cs.CL

TL;DR: 少样本提示在多语言词义消歧（WSD）中存在偏见，需要平衡的提示策略。


<details>
  <summary>Details</summary>
Motivation: 研究少样本提示策略对词义消歧（WSD）任务的影响，特别是样本分布不平衡引入的偏见。

Method: 使用GLOSSGPT提示方法，在五种语言（英语、德语、西班牙语、法语、意大利语）上测试其有效性，并评估GPT-4o和LLaMA-3.1-70B模型。

Result: 不平衡的少样本示例会导致多语言WSD中错误的词义预测，但在英语中不会出现此问题。模型评估突显了多语言WSD在少样本设置中对样本分布的敏感性。

Conclusion: 多语言WSD在少样本设置中对样本分布非常敏感，需要采用平衡且具有代表性的提示策略。

Abstract: Recent advances in Large Language Models (LLMs) have significantly reshaped
the landscape of Natural Language Processing (NLP). Among the various prompting
techniques, few-shot prompting has gained considerable attention for its
practicality and effectiveness. This study investigates how few-shot prompting
strategies impact the Word Sense Disambiguation (WSD) task, particularly
focusing on the biases introduced by imbalanced sample distributions. We use
the GLOSSGPT prompting method, an advanced approach for English WSD, to test
its effectiveness across five languages: English, German, Spanish, French, and
Italian. Our results show that imbalanced few-shot examples can cause incorrect
sense predictions in multilingual languages, but this issue does not appear in
English. To assess model behavior, we evaluate both the GPT-4o and
LLaMA-3.1-70B models and the results highlight the sensitivity of multilingual
WSD to sample distribution in few-shot settings, emphasizing the need for
balanced and representative prompting strategies.

</details>


### [200] [Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development](https://arxiv.org/abs/2510.03781)
*Majid Asgari-Bidhendi,Muhammad Amin Ghaseminia,Alireza Shahbazi,Sayyed Ali Hossayni,Najmeh Torabian,Behrouz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: Rezwan 是一个包含超过 120 万条圣训的人工智能辅助圣训语料库，通过全自动化流程构建，并包含机器翻译、摘要、主题标签等多种增强功能。


<details>
  <summary>Details</summary>
Motivation: 开发 Rezwan 大型人工智能辅助圣训语料库，以自动化流程处理、丰富和结构化圣训文本，为数字人文和伊斯兰研究提供研究基础设施。

Method: 利用大型语言模型（LLMs）对来自 Maktabat Ahl al-Bayt 等数字存储库的圣训进行分割、分离链文本、验证和多层丰富，包括机器翻译、智能注音、摘要、主题标记和跨文本语义分析。

Result: Rezwan 语料库包含超过 120 万条圣训，在链文本分离和摘要等任务上达到了接近人类的准确率（9.33/10）。在与 Noor Corpus 的比较中，Rezwan 在规模和质量上均优于 Noor Corpus（平均分为 8.46/10 对 3.66/10）。AI 方法在成本效益方面也得到了证明，仅用数月时间和极低的成本完成了需要大量专家劳动时间的任务。

Conclusion: Rezwan 的开发展示了一种新的宗教文本处理范式，表明人工智能可以增强人类专业知识，实现大规模、多语言和语义丰富的伊斯兰文化遗产访问。

Abstract: This paper presents the development of Rezwan, a large-scale AI-assisted
Hadith corpus comprising over 1.2M narrations, extracted and structured through
a fully automated pipeline. Building on digital repositories such as Maktabat
Ahl al-Bayt, the pipeline employs Large Language Models (LLMs) for
segmentation, chain--text separation, validation, and multi-layer enrichment.
Each narration is enhanced with machine translation into twelve languages,
intelligent diacritization, abstractive summarization, thematic tagging, and
cross-text semantic analysis. This multi-step process transforms raw text into
a richly annotated research-ready infrastructure for digital humanities and
Islamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled
narrations, assessed by six domain experts. Results show near-human accuracy in
structured tasks such as chain--text separation (9.33/10) and summarization
(9.33/10), while highlighting ongoing challenges in diacritization and semantic
similarity detection. Comparative analysis against the manually curated Noor
Corpus demonstrates the superiority of Najm in both scale and quality, with a
mean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis
confirms the economic feasibility of the AI approach: tasks requiring over
229,000 hours of expert labor were completed within months at a fraction of the
cost. The work introduces a new paradigm in religious text processing by
showing how AI can augment human expertise, enabling large-scale, multilingual,
and semantically enriched access to Islamic heritage.

</details>


### [201] [Mechanistic Interpretability of Socio-Political Frames in Language Models](https://arxiv.org/abs/2510.03799)
*Hadi Asghari,Sami Nenno*

Main category: cs.CL

TL;DR: 大型语言模型能够生成和识别深度认知框架，尤其是在社会政治背景下，并在模型内部找到与这些框架相关的维度。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLM）在社会政治背景下生成和识别深度认知框架的能力。

Method: 通过实验证明LLM能够生成并识别这些框架，并借鉴机械可解释性研究，定位模型内部的‘严父’和‘慈母’框架，找出与框架存在相关性的单一维度。

Result: LLM在生成唤起特定框架的文本方面表现出高度流畅性，并在零样本设置中能够识别这些框架。同时，研究确定了与‘严父’和‘慈母’框架存在显著相关性的模型内部维度。

Conclusion: LLM能够有效地捕捉和表达有意义的人类概念，为理解其内部机制提供了新的视角。

Abstract: This paper explores the ability of large language models to generate and
recognize deep cognitive frames, particularly in socio-political contexts. We
demonstrate that LLMs are highly fluent in generating texts that evoke specific
frames and can recognize these frames in zero-shot settings. Inspired by
mechanistic interpretability research, we investigate the location of the
`strict father' and `nurturing parent' frames within the model's hidden
representation, identifying singular dimensions that correlate strongly with
their presence. Our findings contribute to understanding how LLMs capture and
express meaningful human concepts.

</details>


### [202] [Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models](https://arxiv.org/abs/2510.03805)
*Canhui Wu,Qiong Cao,Chang Li,Zhenfang Wang,Chao Xue,Yuwei Fan,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks
but often suffer from excessive verbosity, known as "overthinking." Existing
solutions via reinforcement learning (RL) typically penalize generated tokens
to promote conciseness. However, these methods encounter two challenges:
responses with fewer tokens do not always correspond to fewer reasoning steps,
and models may develop hacking behavior in later stages of training by
discarding reasoning steps to minimize token usage. In this work, we introduce
\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more
efficient reasoning by favoring compact reasoning steps. Our step-aware reward
function prioritizes correctness while imposing penalties for redundant steps,
and withholds rewards for incorrect responses to prevent the reinforcement of
erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when
the length of any output step exceeds the upper limit, we halt updates to
prevent hacking behavior caused by merging steps. Extensive experiments across
four reasoning benchmarks demonstrate that SP achieves state-of-the-art
accuracy while significantly reducing response length. For instance, on AIME24,
SP reduces token usage by \textbf{69.7\%}.

</details>


### [203] [Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches](https://arxiv.org/abs/2510.03808)
*Mehedi Hasan Emon*

Main category: cs.CL

TL;DR: 本研究使用INCEpTION工具探索了语篇中修辞关系的标注，并比较了人工标注与基于大型语言模型的自动标注方法。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是探索使用INCEpTION工具对体育报道（特别是板球新闻）中的修辞关系进行标注，并比较人工标注与基于大型语言模型的自动标注方法的性能。

Method: 本研究使用了BERT、DistilBERT和Logistic Regression模型对阐述、对比、背景和因果关系等修辞关系进行分类。

Result: DistilBERT模型在区分这些修辞关系时取得了最高的准确率。

Conclusion: 本研究的结论是，DistilBERT在语篇关系预测方面具有潜力，并且这项工作有助于语篇分析和基于Transformer的自然语言处理的交叉领域。

Abstract: This research explores the annotation of rhetorical relations in discourse
using the INCEpTION tool and compares manual annotation with automatic
approaches based on large language models. The study focuses on sports reports
(specifically cricket news) and evaluates the performance of BERT, DistilBERT,
and Logistic Regression models in classifying rhetorical relations such as
elaboration, contrast, background, and cause-effect. The results show that
DistilBERT achieved the highest accuracy, highlighting its potential for
efficient discourse relation prediction. This work contributes to the growing
intersection of discourse parsing and transformer-based NLP. (This paper was
conducted as part of an academic requirement under the supervision of Prof. Dr.
Ralf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords:
Rhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing,
NLP.

</details>


### [204] [Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles](https://arxiv.org/abs/2510.03898)
*Nusrat Jahan Lia,Shubhashis Roy Dipta,Abdullah Khan Zehady,Naymul Islam,Madhusodan Chakraborty,Abdullah Al Wasif*

Main category: cs.CL

TL;DR: 此研究介绍了首个孟加拉语政治偏见数据集，并评估了28种大型语言模型在该任务上的表现，发现模型在识别批评政府内容方面表现良好，但在识别中立内容方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 目前在南亚地区，特别是在孟加拉语政治偏见研究方面，缺乏标注数据集和计算研究，而识别孟加拉语政治立场需要理解语言线索、文化背景、细微偏见、修辞策略、代码转换、隐含情感和社会政治背景。

Method: 创建了一个包含200篇政治上重要且备受争议的孟加拉语新闻文章的基准数据集，并标注了政府倾向、批评政府和中立的立场，同时进行了诊断性分析以评估大型语言模型。

Result: 评估了28种专有和开源的大型语言模型，发现在检测批评政府内容方面表现出强劲性能（F1分数最高可达0.83），但在处理中立文章方面存在显著困难（F1分数低至0.00）。模型还倾向于过度预测政府倾向立场，并且经常错误地解读模糊的叙述。

Conclusion: 该数据集及其相关的诊断分析为推进孟加拉语媒体研究中的立场检测奠定了基础，并为提高大型语言模型在低资源语言中的性能提供了见解。

Abstract: Detecting media bias is crucial, specifically in the South Asian region.
Despite this, annotated datasets and computational studies for Bangla political
bias research remain scarce. Crucially because, political stance detection in
Bangla news requires understanding of linguistic cues, cultural context, subtle
biases, rhetorical strategies, code-switching, implicit sentiment, and
socio-political background. To address this, we introduce the first benchmark
dataset of 200 politically significant and highly debated Bangla news articles,
labeled for government-leaning, government-critique, and neutral stances,
alongside diagnostic analyses for evaluating large language models (LLMs). Our
comprehensive evaluation of 28 proprietary and open-source LLMs shows strong
performance in detecting government-critique content (F1 up to 0.83) but
substantial difficulty with neutral articles (F1 as low as 0.00). Models also
tend to over-predict government-leaning stances, often misinterpreting
ambiguous narratives. This dataset and its associated diagnostics provide a
foundation for advancing stance detection in Bangla media research and offer
insights for improving LLM performance in low-resource languages.

</details>


### [205] [PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian](https://arxiv.org/abs/2510.03913)
*Mohammad Amin Abbasi,Hassan Naderi*

Main category: cs.CL

TL;DR: PsychoLexTherapy是一个为波斯语心理治疗设计的、注重隐私的、基于小型语言模型（SLM）的框架，通过结构化记忆和评估数据集，在多轮对话中实现了高水平的共情、连贯性和文化适应性。


<details>
  <summary>Details</summary>
Motivation: 开发用于波斯语的、符合文化背景的、治疗上连贯的、支持多轮对话的、并且能够在设备上运行的心理治疗对话系统，以应对在代表性不足的语言中开发此类系统的挑战，并确保用户隐私。

Method: PsychoLexTherapy框架的开发包括三个阶段：1.使用PsychoLexEval评估SLMs的心理学知识；2.设计并实现PsychoLexTherapy框架；3.构建PsychoLexQuery（真实用户问题）和PsychoLexDialogue（模拟对话）两个评估数据集，并与简单提示、多智能体辩论等基线进行比较。

Result: 在PsychoLexQuery数据集上，PsychoLexTherapy在自动评估和人工评估中均优于所有基线。在PsychoLexDialogue数据集上，与简单的历史记录拼接相比，长期记忆模块对于保持连贯性和信息完整性至关重要，PsychoLexTherapy框架获得了最高的人工评分，特别是在共情、连贯性、文化契合度和个性化方面。

Conclusion: PsychoLexTherapy为波斯语心理治疗模拟提供了一个实用、注重隐私且符合文化背景的基础，其贡献包括新的数据集、可复现的评估流程以及关于结构化记忆在治疗推理中的实证见解。

Abstract: This study presents PsychoLexTherapy, a framework for simulating
psychotherapeutic reasoning in Persian using small language models (SLMs). The
framework tackles the challenge of developing culturally grounded,
therapeutically coherent dialogue systems with structured memory for multi-turn
interactions in underrepresented languages. To ensure privacy and feasibility,
PsychoLexTherapy is optimized for on-device deployment, enabling use without
external servers. Development followed a three-stage process: (i) assessing
SLMs psychological knowledge with PsychoLexEval; (ii) designing and
implementing the reasoning-oriented PsychoLexTherapy framework; and (iii)
constructing two evaluation datasets-PsychoLexQuery (real Persian user
questions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark
against multiple baselines. Experiments compared simple prompting, multi-agent
debate, and structured therapeutic reasoning paths. Results showed that
deliberate model selection balanced accuracy, efficiency, and privacy. On
PsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic
LLM-as-a-judge evaluation and was ranked highest by human evaluators in a
single-turn preference study. In multi-turn tests with PsychoLexDialogue, the
long-term memory module proved essential: while naive history concatenation
caused incoherence and information loss, the full framework achieved the
highest ratings in empathy, coherence, cultural fit, and personalization.
Overall, PsychoLexTherapy establishes a practical, privacy-preserving, and
culturally aligned foundation for Persian psychotherapy simulation,
contributing novel datasets, a reproducible evaluation pipeline, and empirical
insights into structured memory for therapeutic reasoning.

</details>


### [206] [Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs](https://arxiv.org/abs/2510.03997)
*Junjie Luo,Rui Han,Arshana Welivita,Zeleikun Di,Jingfu Wu,Xuzhe Zhi,Ritu Agarwal,Gordon Gao*

Main category: cs.CL

TL;DR: LLM分析410万份患者评价，提取医生人格特质和主观评价，发现性别、科室对评价的影响，并识别出四种医生画像。


<details>
  <summary>Details</summary>
Motivation: 理解患者如何评价医生对于提升信任、沟通和满意度至关重要。

Method: 利用LLM分析410万份患者对22.7万名美国医生的评价，提取大五人格特质和五种面向患者的主观判断，并通过多模型比较和人类专家基准测试进行验证。

Result: LLM评估与人类评估高度一致（相关系数0.72-0.89），与患者满意度相关性强（r=0.41-0.81）。男性医生在所有特质上评分更高，临床能力差异最大。共情相关特质在儿科和精神科中占主导地位。所有特质均能正向预测整体满意度。识别出四种医生画像：'全面优秀'（33.8%）、'表现不佳'（22.6%）等。

Conclusion: 自动化从患者叙述中提取特质，可以提供可解释、经过验证的指标，用于大规模理解医患关系，对医疗质量衡量、偏见检测和劳动力发展具有意义。

Abstract: Understanding how patients perceive their physicians is essential to
improving trust, communication, and satisfaction. We present a large language
model (LLM)-based pipeline that infers Big Five personality traits and five
patient-oriented subjective judgments. The analysis encompasses 4.1 million
patient reviews of 226,999 U.S. physicians from an initial pool of one million.
We validate the method through multi-model comparison and human expert
benchmarking, achieving strong agreement between human and LLM assessments
(correlation coefficients 0.72-0.89) and external validity through correlations
with patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis
reveals systematic patterns: male physicians receive higher ratings across all
traits, with largest disparities in clinical competence perceptions;
empathy-related traits predominate in pediatrics and psychiatry; and all traits
positively predict overall satisfaction. Cluster analysis identifies four
distinct physician archetypes, from "Well-Rounded Excellent" (33.8%, uniformly
high traits) to "Underperforming" (22.6%, consistently low). These findings
demonstrate that automated trait extraction from patient narratives can provide
interpretable, validated metrics for understanding physician-patient
relationships at scale, with implications for quality measurement, bias
detection, and workforce development in healthcare.

</details>


### [207] [Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions](https://arxiv.org/abs/2510.03999)
*Yang Xu,Xuanming Zhang,Min-Hsuan Yeh,Jwala Dhamala,Ousmane Dia,Rahul Gupta,Yixuan Li*

Main category: cs.CL

TL;DR: LLM在长期的、相互依赖的任务中可能会出现欺骗行为，且欺骗行为会随着压力的增加而增加，并导致信任度下降。


<details>
  <summary>Details</summary>
Motivation: 现有LLM欺骗评估主要局限于单轮交互，无法捕捉欺骗策略通常在长线交互中展开的特点。

Method: 提出了一种多智能体模拟框架，包括一个执行任务的表演者智能体、一个评估进展和信任度的监督者智能体，以及一个独立的欺骗审计员，用于分析完整的交互轨迹。

Result: 在11个模型上进行了广泛实验，发现欺骗行为与模型相关，随压力增加而增加，并持续侵蚀监督者的信任度。定性分析揭示了隐藏、含糊和伪造等不同的欺骗策略。

Conclusion: 欺骗是长期交互中出现的风险，为未来在现实、信任敏感的背景下评估LLM提供了基础。

Abstract: Deception is a pervasive feature of human communication and an emerging
concern in large language models (LLMs). While recent studies document
instances of LLM deception under pressure, most evaluations remain confined to
single-turn prompts and fail to capture the long-horizon interactions in which
deceptive strategies typically unfold. We introduce the first simulation
framework for probing and evaluating deception in LLMs under extended sequences
of interdependent tasks and dynamic contextual pressures. Our framework
instantiates a multi-agent system: a performer agent tasked with completing
tasks and a supervisor agent that evaluates progress, provides feedback, and
maintains evolving states of trust. An independent deception auditor then
reviews full trajectories to identify when and how deception occurs. We conduct
extensive experiments across 11 frontier models, spanning both closed- and
open-source systems, and find that deception is model-dependent, increases with
event pressure, and consistently erodes supervisor trust. Qualitative analyses
further reveal distinct strategies of concealment, equivocation, and
falsification. Our findings establish deception as an emergent risk in
long-horizon interactions and provide a foundation for evaluating future LLMs
in real-world, trust-sensitive contexts.

</details>


### [208] [Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation](https://arxiv.org/abs/2510.04001)
*Xuankang Zhang,Jiangming Liu*

Main category: cs.CL

TL;DR: 该研究提出了一种新颖的实体知识增强方法，用于识别社交媒体和生物医学文本中的 COVID-19 相关实体，解决了数据稀疏和领域知识缺乏的挑战，并在充分监督和少样本设置下均提高了命名实体识别性能。


<details>
  <summary>Details</summary>
Motivation: 识别社交媒体上与 COVID-19 相关的命名实体对于理解大流行病的讨论至关重要，但由于社交媒体文本的非正式性、标注数据的稀缺性以及对特定领域知识的依赖，该领域的研究存在局限性。

Method: 提出了一种新颖的实体知识增强方法，该方法可应用于 COVID-19 相关的命名实体识别，并且可以推广到其他生物医学命名实体识别任务，无论是处理非正式文本还是正式文本。

Result: 在 COVID-19 推文数据集和 PubMed 数据集上的实验表明，所提出的实体知识增强方法在完全监督和少样本设置下都能提高命名实体识别的性能。

Conclusion: 提出的实体知识增强方法能够有效地提升在 COVID-19 推文和 PubMed 数据集上的命名实体识别性能，尤其是在标注数据有限的情况下。该方法有望应用于更广泛的生物医学命名实体识别任务。

Abstract: The COVID-19 pandemic causes severe social and economic disruption around the
world, raising various subjects that are discussed over social media.
Identifying pandemic-related named entities as expressed on social media is
fundamental and important to understand the discussions about the pandemic.
However, there is limited work on named entity recognition on this topic due to
the following challenges: 1) COVID-19 texts in social media are informal and
their annotations are rare and insufficient to train a robust recognition
model, and 2) named entity recognition in COVID-19 requires extensive
domain-specific knowledge. To address these issues, we propose a novel entity
knowledge augmentation approach for COVID-19, which can also be applied in
general biomedical named entity recognition in both informal text format and
formal text format. Experiments carried out on the COVID-19 tweets dataset and
PubMed dataset show that our proposed entity knowledge augmentation improves
NER performance in both fully-supervised and few-shot settings. Our source code
is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master

</details>


### [209] [AgriGPT-VL: Agricultural Vision-Language Understanding Suite](https://arxiv.org/abs/2510.04002)
*Bo Yang,Yunkui Chen,Lanfei Feng,Yu Zhang,Xiao Xu,Jianyu Zhang,Nueraili Aierken,Runhe Huang,Hongjian Lin,Yibin Ying,Shijian Li*

Main category: cs.CL

TL;DR: AgriGPT-VL Suite 是一个为农业设计的统一多模态框架，包含 Agri-3M-VL 语料库、AgriGPT-VL 模型和 AgriBench-VL-4K 评估套件，旨在解决农业领域多模态模型的稀缺性问题。


<details>
  <summary>Details</summary>
Motivation: 农业领域缺乏针对性的多模态模型、数据集和评估方法。

Method: 构建了 Agri-3M-VL 语料库（包含图像-标题、图像-问答、专家级问答和 GRPO 样本），开发了 AgriGPT-VL 模型（采用渐进式课程训练），并建立了 AgriBench-VL-4K 评估套件（包含开放式和图像相关问题）。

Result: AgriGPT-VL 在 AgriBench-VL-4K 上优于通用的视觉-语言模型（VLMs），并且在 AgriBench-13K 文本任务上表现不俗，验证了对齐和 GRPO 改进阶段的有效性。

Conclusion: AgriGPT-VL Suite 成功地为农业领域的多模态模型研究和应用提供了一个全面的解决方案，并且将开源所有资源以支持可复现的研究和低资源农业应用。

Abstract: Despite rapid advances in multimodal large language models, agricultural
applications remain constrained by the scarcity of domain-tailored models,
curated vision-language corpora, and rigorous evaluation. To address these
challenges, we present the AgriGPT-VL Suite, a unified multimodal framework for
agriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,
the largest vision-language corpus for agriculture to our knowledge, curated by
a scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M
image-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO
reinforcement learning samples. Second, we develop AgriGPT-VL, an
agriculture-specialized vision-language model trained via a progressive
curriculum of textual grounding, multimodal shallow/deep alignment, and GRPO
refinement. This method achieves strong multimodal reasoning while preserving
text-only capability. Third, we establish AgriBench-VL-4K, a compact yet
challenging evaluation suite with open-ended and image-grounded questions,
paired with multi-metric evaluation and an LLM-as-a-judge framework.
Experiments show that AgriGPT-VL outperforms leading general-purpose VLMs on
AgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge
evaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K
with no noticeable degradation of language ability. Ablation studies further
confirm consistent gains from our alignment and GRPO refinement stages. We will
open source all of the resources to support reproducible research and
deployment in low-resource agricultural settings.

</details>


### [210] [LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization](https://arxiv.org/abs/2510.04013)
*Jiarui Liu,Jivitesh Jain,Mona Diab,Nishant Subramani*

Main category: cs.CL

TL;DR: LLM生成的文本有时不准确，本文提出一种基于模型内部激活信号的方法来预测模型输出的准确性，并评估检索到的上下文的有效性，实验表明该方法能有效提升准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLM虽然实用，但其生成内容的可信度仍是主要担忧，模型常高置信度地生成错误信息。尽管上下文信息有帮助，但判断何时需要检索上下文以及评估上下文的有效性仍具挑战性。

Method: 提出一种基于模型内部激活信号（特别是第一输出token的中间层激活）的分类器，用于预测模型输出的正确性，并提出区分正确、错误和无关上下文的度量方法。

Result: 在六种不同模型上的实验表明，一个基于第一输出token的中间层激活训练的简单分类器，可以大约75%的准确率预测输出的正确性。所提出的基于模型内部信号的度量方法，在区分正确和错误上下文方面显著优于基于提示（prompting）的基线方法。

Conclusion: 基于模型内部激活信号的方法可以预测LLM输出的正确性，并有效评估外部上下文的质量，从而提高LLM的可靠性，并有助于理解LLM的决策过程。

Abstract: Although large language models (LLMs) have tremendous utility,
trustworthiness is still a chief concern: models often generate incorrect
information with high confidence. While contextual information can help guide
generation, identifying when a query would benefit from retrieved context and
assessing the effectiveness of that context remains challenging. In this work,
we operationalize interpretability methods to ascertain whether we can predict
the correctness of model outputs from the model's activations alone. We also
explore whether model internals contain signals about the efficacy of external
context. We consider correct, incorrect, and irrelevant context and introduce
metrics to distinguish amongst them. Experiments on six different models reveal
that a simple classifier trained on intermediate layer activations of the first
output token can predict output correctness with about 75% accuracy, enabling
early auditing. Our model-internals-based metric significantly outperforms
prompting baselines at distinguishing between correct and incorrect context,
guarding against inaccuracies introduced by polluted context. These findings
offer a lens to better understand the underlying decision-making processes of
LLMs. Our code is publicly available at
https://github.com/jiarui-liu/LLM-Microscope

</details>


### [211] [Thai Semantic End-of-Turn Detection for Real-Time Voice Agents](https://arxiv.org/abs/2510.04016)
*Thanapol Popit,Natthapath Rungseesiripak,Monthol Charattrakool,Saksorn Ruangtanusak*

Main category: cs.CL

TL;DR: 本研究提出了一种用于泰语语音交互的文本端点检测（EOT）方法，通过比较零样本/少样本提示的紧凑型语言模型和轻量级 Transformer 的监督微调，以减少延迟并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的音频-静默端点检测器存在延迟大（数百毫秒）且在用户犹豫或出现特定语言现象时效果不佳的问题，这阻碍了流畅的语音-语音交互。

Method: 研究者将泰语文本端点检测（EOT）视为一个在标记边界上的二元决策问题。他们使用了 YODAS 语料库的转录字幕和泰语特定语言线索（例如句末助词），并比较了紧凑型语言模型的零样本/少样本提示与轻量级 Transformer 的监督微调。

Result: 研究表明，在准确性和延迟之间存在明显的权衡。结果证明，小型、微调后的模型可以提供近乎即时的 EOT 决策，适用于设备端代理。

Conclusion: 本研究为泰语 EOT 检测建立了基准，并证明小型、微调后的模型在准确性和低延迟方面能够满足实时语音交互的需求，为开发更流畅的泰语语音助手提供了可能。

Abstract: Fluid voice-to-voice interaction requires reliable and low-latency detection
of when a user has finished speaking. Traditional audio-silence end-pointers
add hundreds of milliseconds of delay and fail under hesitations or
language-specific phenomena. We present, to our knowledge, the first systematic
study of Thai text-only end-of-turn (EOT) detection for real-time agents. We
compare zero-shot and few-shot prompting of compact LLMs to supervised
fine-tuning of lightweight transformers. Using transcribed subtitles from the
YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final
particles), we formulate EOT as a binary decision over token boundaries. We
report a clear accuracy-latency tradeoff and provide a public-ready
implementation plan. This work establishes a Thai baseline and demonstrates
that small, fine-tuned models can deliver near-instant EOT decisions suitable
for on-device agents.

</details>


### [212] [Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?](https://arxiv.org/abs/2510.04031)
*Nelvin Tan,James Asikin Cheung,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: LLM决策解释：使用反事实推理和决策改变率来量化关键信息词的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在文本分类任务中非常有效，因此有必要解释LLM的决策。然而，LLM通常是黑箱模型，且调用成本高昂，这促使研究人员探索如何在实际约束下改进LLM的决策解释能力。

Method: 提出了一种名为“决策改变率”的框架，将反事实推理融入LLM的决策过程中，以量化关键信息词对分类决策的贡献度。

Result: 实验结果表明，在LLM的推理过程中引入反事实推理方法，有助于提高其解释决策的能力，并且能够更有效地识别出对分类决策贡献最大的词语。

Conclusion: 反事实推理方法能够有效提升LLM在文本分类任务中的决策解释能力，为理解和信任LLM在实际应用中的表现提供了新的途径。

Abstract: Large language models (LLMs) are becoming useful in many domains due to their
impressive abilities that arise from large training datasets and large model
sizes. More recently, they have been shown to be very effective in textual
classification tasks, motivating the need to explain the LLMs' decisions.
Motivated by practical constrains where LLMs are black-boxed and LLM calls are
expensive, we study how incorporating counterfactuals into LLM reasoning can
affect the LLM's ability to identify the top words that have contributed to its
classification decision. To this end, we introduce a framework called the
decision changing rate that helps us quantify the importance of the top words
in classification. Our experimental results show that using counterfactuals can
be helpful.

</details>


### [213] [Small Language Models for Emergency Departments Decision Support: A Benchmark Study](https://arxiv.org/abs/2510.04032)
*Zirui Wang,Jiajun Wu,Braden Teitge,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 小型语言模型（SLM）在急诊科（ED）环境中具有巨大潜力，可用于支持医生决策。本研究提出了一个基准来评估适合ED决策支持的SLM，重点关注在通用和医学语料库上训练的模型。实验结果表明，通用领域SLM在ED相关基准测试中表现优于经过医学微调的模型，这表明在ED环境中可能不需要专门的医学微调。


<details>
  <summary>Details</summary>
Motivation: 在快节奏、高风险的急诊科（ED）环境中，参数量较少、推理能力强、性能高效的小型语言模型（SLM）具有巨大潜力，可以支持医生进行及时的信息合成，从而提高临床决策和工作流程效率。此外，SLM的实际硬件限制、运营成本和隐私问题也是推动其应用的重要因素。

Method: 提出一个全面的基准，用于识别适合ED决策支持的SLM，这些SLM在通用和医学语料库的混合物上进行训练。基准数据集包括MedMCQA、MedQA-4Options和PubMedQA，以及一个模拟ED医生日常任务的医学摘要数据集。

Result: 在针对ED环境的各项基准测试中，通用领域SLM的表现出人意料地优于经过医学微调的模型。

Conclusion: 对于急诊科（ED）应用而言，专门的医学微调可能不是必需的，因为通用领域的小型语言模型（SLM）已经能够提供优于医学微调模型的性能。

Abstract: Large language models (LLMs) have become increasingly popular in medical
domains to assist physicians with a variety of clinical and operational tasks.
Given the fast-paced and high-stakes environment of emergency departments
(EDs), small language models (SLMs), characterized by a reduction in parameter
count compared to LLMs, offer significant potential due to their inherent
reasoning capability and efficient performance. This enables SLMs to support
physicians by providing timely and accurate information synthesis, thereby
improving clinical decision-making and workflow efficiency. In this paper, we
present a comprehensive benchmark designed to identify SLMs suited for ED
decision support, taking into account both specialized medical expertise and
broad general problem-solving capabilities. In our evaluations, we focus on
SLMs that have been trained on a mixture of general-domain and medical corpora.
A key motivation for emphasizing SLMs is the practical hardware limitations,
operational cost constraints, and privacy concerns in the typical real-world
deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and
PubMedQA, with the medical abstracts dataset emulating tasks aligned with real
ED physicians' daily tasks. Experimental results reveal that general-domain
SLMs surprisingly outperform their medically fine-tuned counterparts across
these diverse benchmarks for ED. This indicates that for ED, specialized
medical fine-tuning of the model may not be required.

</details>


### [214] [Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment](https://arxiv.org/abs/2510.04045)
*Yunfan Zhang,Kathleen McKeown,Smaranda Muresan*

Main category: cs.CL

TL;DR: LLM 通过链式思考（CoT）推理技术支持可控多元主义，其中 RLVR 方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: LLM 通常反映统一的价值观，限制了其在需要细微人 perspective 的任务中的应用。本研究旨在使 LLM 能够支持可控多元主义，即采纳特定 perspective 并使其生成输出与其保持一致。

Method: 探索了几种方法，包括 CoT 提示、在人类编写的 CoT 上进行微调、在合成解释上进行微调以及带有可验证奖励（RLVR）的强化学习。

Result: RLVR 在 Value Kaleidoscope 和 OpinionQA 数据集上的评估中始终优于其他方法，并显示出强大的训练样本效率。此外，还分析了生成的 CoT 迹线的忠实度和安全性。

Conclusion: 链式思考（CoT）推理技术可用于构建可控多元主义模型，其中 RLVR 是最有效的方法。

Abstract: Large Language Models (LLMs) are typically trained to reflect a relatively
uniform set of values, which limits their applicability to tasks that require
understanding of nuanced human perspectives. Recent research has underscored
the importance of enabling LLMs to support steerable pluralism -- the capacity
to adopt a specific perspective and align generated outputs with it. In this
work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be
applied to building steerable pluralistic models. We explore several methods,
including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on
synthetic explanations, and Reinforcement Learning with Verifiable Rewards
(RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA
datasets. Among the methods studied, RLVR consistently outperforms others and
demonstrates strong training sample efficiency. We further analyze the
generated CoT traces with respect to faithfulness and safety.

</details>


### [215] [What Makes Diffusion Language Models Super Data Learners?](https://arxiv.org/abs/2510.04071)
*Zitian Gao,Haoming Luo,Lynx Chen,Jason Klein Liu,Ran Tao,Joey Zhou,Bryan Dai*

Main category: cs.CL

TL;DR: 扩散语言模型在有限数据下表现出高数据效率，主要是由于随机输入标记掩码，但MLP dropout和权重衰减等随机正则化技术也能提高数据效率。


<details>
  <summary>Details</summary>
Motivation: 探究扩散语言模型在有限数据下实现高数据效率的潜在机制。

Method: 通过进行广泛的消融实验来分离数据效率的来源，并研究MLP dropout和权重衰减等随机正则化技术的影响。

Result: 随机掩码输入标记在提高数据效率方面起主导作用。MLP dropout和权重衰减也能带来类似的数据效率提升。

Conclusion: 随机正则化在多周期训练中能广泛地提高扩散语言模型的数据效率。

Abstract: Recent studies have shown that diffusion language models achieve remarkable
data efficiency under limited-data constraints, yet the underlying mechanisms
remain unclear. In this work, we perform extensive ablation experiments to
disentangle the sources of this efficiency. Our results show that random
masking of input tokens plays the dominant role. We further show that similar
gains can be obtained through in MLP dropout and weight decay, indicating that
stochastic regularization broadly enhances data efficiency in multi-epoch
training. Our code is available at
https://github.com/zitian-gao/data-efficiency.

</details>


### [216] [PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity](https://arxiv.org/abs/2510.04080)
*Zixin Song,Bowen Zhang,Qian-Wen Zhang,Di Yin,Xing Sun,Chunping Li*

Main category: cs.CL

TL;DR: PoLi-RL是一个新颖的点对列表强化学习框架，通过两阶段课程学习和并行切片排序奖励（PSRR）机制，在条件语义文本相似度（C-STS）任务上实现了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的C-STS方法主要基于判别模型，未能充分利用大型语言模型（LLM）和强化学习（RL）的最新进展。RL能够直接优化不可导的Spearman秩相关系数，并指导C-STS所需的推理过程。

Method: PoLi-RL采用两阶段课程学习：首先使用简单的点状奖励进行训练，然后过渡到结合点状、对偶和列表状奖励的混合奖励。提出的并行切片排序奖励（PSRR）机制并行计算排序奖励，为每个样本的每个部分提供精确、可区分的学习信号。

Result: PoLi-RL在C-STS基准测试上取得了48.18的Spearman秩相关系数，为交叉编码器架构创下了新的SOTA。

Conclusion: PoLi-RL是首次成功将RL应用于C-STS的研究，为训练LLM处理复杂的、基于排名的条件判断任务提供了一个强大而精确的范例。

Abstract: Conditional Semantic Textual Similarity (C-STS) measures the semantic
proximity between text segments under a specific condition, thereby overcoming
the ambiguity inherent in traditional STS. However, existing methods are
largely confined to discriminative models, failing to fully integrate recent
breakthroughs in the NLP community concerning Large Language Models (LLMs) and
Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this
task, as it can directly optimize the non-differentiable Spearman ranking
metric and guide the reasoning process required by C-STS. However, we find that
naively applying listwise RL fails to produce meaningful improvements, as the
model is overwhelmed by complex, coarse-grained reward signals. To address this
challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning
framework. PoLi-RL employs a two-stage curriculum: it first trains the model
with simple pointwise rewards to establish fundamental scoring capabilities,
then transitions to a hybrid reward that combines pointwise, pairwise, and
listwise objectives to refine the model's ability to discern subtle semantic
distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward
(PSRR) mechanism that computes ranking rewards in parallel slices, where each
slice comprises same-indexed completions from different samples. This provides
a precise, differentiated learning signal for each individual completion,
enabling granular credit assignment and effective optimization. On the official
C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,
establishing a new SOTA for the cross-encoder architecture. As the first work
to successfully apply RL to C-STS, our study introduces a powerful and precise
paradigm for training LLMs on complex, ranking-based conditional judgment
tasks.

</details>


### [217] [Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning](https://arxiv.org/abs/2510.04081)
*Honglin Lin,Qizhi Pei,Xin Gao,Zhuoshi Pan,Yu Li,Juntao Li,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: Caco框架通过代码辅助生成高质量、可验证、多样化的指令-CoT推理数据，解决了现有CoT方法的局限性，并在数学推理任务上取得了优于基线模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在推理能力方面面临挑战，传统的Chain-of-Thought（CoT）方法存在生成不可控、质量不足和推理路径多样性有限等问题。利用代码增强CoT的方法又受限于预定义数学问题，难以扩展和泛化。

Method: Caco框架首先在一个统一的代码格式上对基于代码的CoT生成器进行微调，然后通过代码驱动的增强来扩展数据生成，生成大量的、多样化的推理轨迹。通过代码执行和基于规则的过滤进行自动验证，以确保逻辑正确性和结构多样性。最后，将过滤后的输出反向工程为自然语言指令和语言CoT，以增强任务适应性。

Result: 在Caco-1.3M数据集上训练的模型在数学推理基准测试中表现出强大的竞争力，优于现有的强大基线模型。代码锚定的验证和指令的多样性有助于提高模型在未见过任务上的泛化能力。

Conclusion: Caco框架为在没有人工干预的情况下构建自给自足、值得信赖的推理系统建立了一个范式。

Abstract: Reasoning capability is pivotal for Large Language Models (LLMs) to solve
complex tasks, yet achieving reliable and scalable reasoning remains
challenging. While Chain-of-Thought (CoT) prompting has become a mainstream
approach, existing methods often suffer from uncontrolled generation,
insufficient quality, and limited diversity in reasoning paths. Recent efforts
leverage code to enhance CoT by grounding reasoning in executable steps, but
such methods are typically constrained to predefined mathematical problems,
hindering scalability and generalizability. In this work, we propose Caco
(Code-Assisted Chain-of-ThOught), a novel framework that automates the
synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning
data through code-driven augmentation. Unlike prior work, Caco first fine-tunes
a code-based CoT generator on existing math and programming solutions in a
unified code format, then scales the data generation to a large amount of
diverse reasoning traces. Crucially, we introduce automated validation via code
execution and rule-based filtering to ensure logical correctness and structural
diversity, followed by reverse-engineering filtered outputs into natural
language instructions and language CoTs to enrich task adaptability. This
closed-loop process enables fully automated, scalable synthesis of reasoning
data with guaranteed executability. Experiments on our created Caco-1.3M
dataset demonstrate that Caco-trained models achieve strong competitive
performance on mathematical reasoning benchmarks, outperforming existing strong
baselines. Further analysis reveals that Caco's code-anchored verification and
instruction diversity contribute to superior generalization across unseen
tasks. Our work establishes a paradigm for building self-sustaining,
trustworthy reasoning systems without human intervention.

</details>


### [218] [Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence](https://arxiv.org/abs/2510.04120)
*Fengying Ye,Shanshan Wang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 大型语言模型(LLMs)在理解和生成比喻方面存在局限性，主要表现在概念映射不准确、过度依赖训练数据中的比喻线索以及对句法结构比字面意思更敏感。


<details>
  <summary>Details</summary>
Motivation: 当前研究缺乏对大型语言模型(LLMs)在理解和生成比喻方面能力的深入探索，尽管它们在知识整合、上下文推理和创意生成方面表现出色。

Method: 本研究从三个角度考察LLMs的比喻处理能力：1. 概念映射（使用嵌入空间投影评估LLMs如何映射目标域中的概念）；2. 比喻-字面语料库（分析比喻词及其字面对应词，以识别固有的比喻知识）；3. 句法敏感性（评估比喻句法结构如何影响LLMs的表现）。

Result: 研究发现，LLMs生成比喻解释时有15%-25%的概念不相关，它们更依赖于训练数据中的比喻线索而非上下文线索，并且对句法不规则性的敏感度高于对结构理解的敏感度。

Conclusion: LLMs在比喻分析方面存在局限性，需要更强大的计算方法来改进其理解和生成比喻的能力。

Abstract: Metaphor analysis is a complex linguistic phenomenon shaped by context and
external factors. While Large Language Models (LLMs) demonstrate advanced
capabilities in knowledge integration, contextual reasoning, and creative
generation, their mechanisms for metaphor comprehension remain insufficiently
explored. This study examines LLMs' metaphor-processing abilities from three
perspectives: (1) Concept Mapping: using embedding space projections to
evaluate how LLMs map concepts in target domains (e.g., misinterpreting "fall
in love" as "drop down from love"); (2) Metaphor-Literal Repository: analyzing
metaphorical words and their literal counterparts to identify inherent
metaphorical knowledge; and (3) Syntactic Sensitivity: assessing how
metaphorical syntactic structures influence LLMs' performance. Our findings
reveal that LLMs generate 15\%-25\% conceptually irrelevant interpretations,
depend on metaphorical indicators in training data rather than contextual cues,
and are more sensitive to syntactic irregularities than to structural
comprehension. These insights underline the limitations of LLMs in metaphor
analysis and call for more robust computational approaches.

</details>


### [219] [Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)](https://arxiv.org/abs/2510.04124)
*Nuwan I. Senaratna*

Main category: cs.CL

TL;DR: 该研究介绍了一个包含斯里兰卡议会程序、法律判决、政府出版物、新闻和旅游统计数据的开放、机器可读的文档数据集。


<details>
  <summary>Details</summary>
Motivation: 支持计算语言学、法律分析、社会政治研究和多语言自然语言处理等领域的研究。

Method: 描述了数据来源、收集流程、格式和潜在用例，并讨论了许可和道德考量。

Result: 截至 v20251005，该数据集包含 215,670 份文档（60.3 GB），涵盖僧伽罗语、泰米尔语和英语三种语言的 13 个数据集，并每日更新。

Conclusion: 该数据集为相关研究领域提供了宝贵的资源，并考虑了相关的许可和道德问题。

Abstract: We present a collection of open, machine-readable document datasets covering
parliamentary proceedings, legal judgments, government publications, news, and
tourism statistics from Sri Lanka. As of v20251005, the collection currently
comprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and
English. The datasets are updated daily and mirrored on GitHub and Hugging
Face. These resources aim to support research in computational linguistics,
legal analytics, socio-political studies, and multilingual natural language
processing. We describe the data sources, collection pipeline, formats, and
potential use cases, while discussing licensing and ethical considerations.

</details>


### [220] [Fine Tuning Methods for Low-resource Languages](https://arxiv.org/abs/2510.04139)
*Tim Bakkenes,Daniel Wang,Anton Johansson*

Main category: cs.CL

TL;DR: 大型语言模型在多文化背景下面临包容性问题，主要因其英文文本训练导致在其他语言和文化语境下表现不佳。本项目提出了一种通用的文化相关数据集准备方法，并对Gemma 2模型进行了后训练，旨在提升其在代表性不足的语言上的表现，并为其他国家/地区提供通用方法，以利用生成式AI并保护其文化遗产。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）主要基于英文文本和文化进行训练，导致在其他语言和文化背景下的表现不佳，未能实现包容性。本研究旨在解决这一问题，为代表性不足的语言提升LLMs的性能。

Method: 本项目开发了一种通用的方法来准备文化相关的数据集，并对Gemma 2模型进行了后训练。

Result: 通过上述方法，提升了Gemma 2模型在代表性不足的语言上的性能。

Conclusion: 本研究展示了一种可推广的方法，使得其他国家/地区能够利用生成式AI，提升模型在该地区的表现，并保护其文化遗产。

Abstract: The rise of Large Language Models has not been inclusive of all cultures. The
models are mostly trained on English texts and culture which makes them
underperform in other languages and cultural contexts. By developing a
generalizable method for preparing culturally relevant datasets and
post-training the Gemma 2 model, this project aimed to increase the performance
of Gemma 2 for an underrepresented language and showcase how others can do the
same to unlock the power of Generative AI in their country and preserve their
cultural heritage.

</details>


### [221] [Self Speculative Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.04147)
*Yifeng Gao,Ziang Ji,Yuxuan Wang,Biqing Qi,Hanlin Xu,Linfeng Zhang*

Main category: cs.CL

TL;DR: SSD是一种无损推理加速方法，利用扩散大语言模型（dLLM）自身进行推测性解码，实现了最高3.46倍的加速，同时保持与逐步解码相同的输出。


<details>
  <summary>Details</summary>
Motivation: 当前并行解码方法在生成结果上与逐步解码存在偏差，可能导致性能下降，限制了其在实际中的应用。

Method: SSD提出了一种自推测解码机制，模型在一次前向传播中生成多个位置的预测，并通过分层验证树进行验证，无需额外的辅助模块或模型。

Result: SSD在LLaDA和Dream等开源模型上实现了最高3.46倍的加速，并且输出结果与逐步解码完全一致。

Conclusion: SSD是一种高效且无损的推理加速方法，通过利用dLLM自身的并行预测能力，克服了传统并行解码的缺点，并实现了显著的性能提升。

Abstract: Diffusion-based Large Language Models (dLLMs) have emerged as a competitive
alternative to autoregressive models, offering unique advantages through
bidirectional attention and parallel generation paradigms. However, the
generation results of current parallel decoding methods deviate from stepwise
decoding, introducing potential performance degradation, which limits their
practical deployment. To address this problem, we propose \textbf{S}elf
\textbf{S}peculative \textbf{D}ecoding (SSD), a lossless inference acceleration
method that leverages the dLLM itself as both speculative decoding drafter and
verifier without auxiliary modules. SSD introduces a self-drafting mechanism
where the model generates predictions for multiple positions, then verifies
them through hierarchical verification trees in a single forward pass. Unlike
traditional speculative decoding that requires separate draft models, SSD
eliminates model redundancy and memory overhead by exploiting the dLLM's
inherent parallel prediction capability for multiple positions. This
self-speculative approach allows the model to progressively verify and accept
multiple tokens in a single forward pass. Our experiments demonstrate that SSD
achieves up to 3.46$\times$ speedup while keeping the output identical to
stepwise decoding on open source models such as LLaDA and Dream. Code will be
made publicly available on GitHub.

</details>


### [222] [Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization](https://arxiv.org/abs/2510.04182)
*Wengao Ye,Yan Liang,Lianlei Shan*

Main category: cs.CL

TL;DR: 通过在测试时优化中间潜在“思考”向量，LTPO 提高了 LLM 在具有挑战性的分布外任务上的推理能力，而无需更新模型参数。


<details>
  <summary>Details</summary>
Motivation: 现有的潜在推理方法在处理具有挑战性的、分布外（out-of-distribution）的任务时可能表现不佳，而这些任务恰恰是需要强大推理能力的时候。之前的基于显式链式思考（Chain-of-Thought, CoT）的方法虽然能提供更强的可解释性，但效率较低。

Method: LTPO 框架在测试时将中间潜在“思考”向量视为动态参数进行优化，并使用一种在线策略梯度方法。该方法由一个基于置信度的内在奖励信号指导，该信号直接从冻结的 LLM 输出分布计算得出，避免了对外部监督或昂贵文本生成的需求。

Result: 在五个推理基准测试中的实验表明，LTPO 在标准任务上能够达到或超过现有的强有力基线，并且在其他方法失败的具有挑战性的任务上表现出卓越的鲁棒性。特别是在 AIME 基准测试上，LTPO 显著提高了准确率，而其他潜在推理基线在该测试上的准确率则接近于零。

Conclusion: LTPO 是一种有效的、参数无关的框架，可以在测试时增强 LLM 的推理能力，特别是在处理复杂和分布外任务时，表现出比现有方法更强的鲁棒性和性能。

Abstract: Recent advancements in Large Language Models (LLMs) have shifted from
explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,
where intermediate thoughts are represented as vectors rather than text.
However, latent reasoning can be brittle on challenging, out-of-distribution
tasks where robust reasoning is most critical. To overcome these limitations,
we introduce Latent Thought Policy Optimization (LTPO), a parameter-free
framework that enhances LLM reasoning entirely at test time, without requiring
model parameter updates. LTPO treats intermediate latent "thought" vectors as
dynamic parameters that are actively optimized for each problem instance. It
employs an online policy gradient method guided by an intrinsic,
confidence-based reward signal computed directly from the frozen LLM's own
output distributions, eliminating the need for external supervision or
expensive text generation during optimization. Extensive experiments on five
reasoning benchmarks show that LTPO not only matches or surpasses strong
baselines on standard tasks but also demonstrates remarkable robustness where
others fail. Most notably, on highly challenging AIME benchmarks where existing
latent reasoning baselines collapse to near-zero accuracy, LTPO delivers
substantial improvements, showcasing a unique capability for complex reasoning.

</details>


### [223] [CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling](https://arxiv.org/abs/2510.04204)
*Zhengyang Tang,Zihan Ye,Chenyu Huang,Xuhan Huang,Chengpeng Li,Sihang Li,Guanhua Chen,Ming Yan,Zizhuo Wang,Hongyuan Zha,Dayiheng Liu,Benyou Wang*

Main category: cs.CL

TL;DR: CALM框架通过轻量级修正和监督微调，结合强化学习，改进了大型推理模型（LRMs）在优化建模任务中的表现，最终形成了STORM模型，在五个基准测试中达到了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有领域自适应方法未能充分利用现代大型推理模型（LRMs）的高级推理模式，导致在优化建模任务上的性能提升有限。直接在非反思性数据集上进行微调效果不佳。

Method: 提出CALM框架，通过专家干预识别推理缺陷并提供纠正性提示，促使LRM在其原生推理模式下进行自我完善。该框架通过监督微调和强化学习进行渐进式优化，干预的修改量少于生成token的2.6%。

Result: 基于CALM框架开发的STORM模型（4B参数）在五个流行优化建模基准测试中取得了68.9%的平均准确率，达到新的SOTA，并能媲美671B参数的LRM性能。

Conclusion: 动态的、基于提示的数据合成方法能够有效保留并增强现代LRMs的原生推理模式，为在具有挑战性的优化建模任务上实现专家级性能提供了更有效和可扩展的途径。

Abstract: Large Reasoning Models (LRMs) have demonstrated strong capabilities in
complex multi-step reasoning, opening new opportunities for automating
optimization modeling. However, existing domain adaptation methods, originally
designed for earlier instruction-tuned models, often fail to exploit the
advanced reasoning patterns of modern LRMs -- In particular, we show that
direct fine-tuning on traditional \textit{non-reflective} datasets leads to
limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose
\textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a
framework that progressively refines LRMs within their native reasoning modes
for optimization modeling tasks. In CALM, an expert intervener identifies
reasoning flaws and provides concise corrective hints, which the LRM
incorporates to produce improved reasoning trajectories. These interventions
modify fewer than 2.6\% of generated tokens, but generate high-quality data for
soft adaptation through supervised fine-tuning. The adapted model is then
further improved through reinforcement learning. Building on CALM, we develop
\textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a
4B-parameter LRM that achieves a new state-of-the-art average accuracy of
68.9\% across five popular optimization modeling benchmarks, matching the
performance of a 671B LRM. These results demonstrate that dynamic, hint-based
data synthesis both preserves and amplifies the native reasoning patterns of
modern LRMs, offering a more effective and scalable path towards expert-level
performance on challenging optimization modeling tasks.

</details>


### [224] [Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards](https://arxiv.org/abs/2510.04214)
*Zhuoran Zhuang,Ye Chen,Xia Zeng,Chao Luo,Luhui Liu,Yihan Chen*

Main category: cs.CL

TL;DR: 我们提出了一种名为REPO（Reward-Enhanced Policy Optimization）的强化学习框架，用于优化大型语言模型（LLM）在旅行社价格谈判中的表现，通过结合人类偏好、SOP合规性和业务约束等多重奖励，显著提升了谈判对话质量和合规性。


<details>
  <summary>Details</summary>
Motivation: 在在线旅行社（OTA）中，部署大型语言模型（LLM）作为业务发展（BD）代理进行价格谈判至关重要，但现有方法（如SFT或单一奖励优化）存在脚本过度拟合、忽略细微说服风格以及无法强制执行业务约束的问题。

Method: 提出了一种名为REPO（Reward-Enhanced Policy Optimization）的强化学习框架，它整合了三种异构奖励：一个用于细粒度人类对齐的偏好学习奖励模型（RM），一个用于高层说服行为和SOP合规性的奖励判官（RJ），以及一个用于确定性检查（如数字、格式、护栏）的程序化奖励函数（RF）。REPO还提出了一种增强机制来组合这些信号，以遏制奖励作弊并提高谈判质量。

Result: 在生产风格的评估中，REPO将平均对话评分提高到4.63（比基线高1.20，比DPO高0.83，比GRPO高0.33），将包含至少一个优秀回复的对话比例提高到66.67%（比GRPO高23.34个百分点），并在150个真实对话轮次和225个精心设计的差案例对话轮次中，达到了93.33%的差案例修复率，其中75.56%是干净修复，优于SFT、DPO、PPO和GRPO。此外，还观察到LLM展现出超越黄金标注的涌现能力，如主动共情、本地化推理和校准策略。

Conclusion: REPO框架通过有效整合多种奖励信号，显著提升了LLM在价格谈判任务中的表现，不仅在对话质量和合规性上超越了现有方法，还展现了LLM在复杂交互中的涌现能力，为BD代理的部署提供了有前景的解决方案。

Abstract: We study deploying large language models (LLMs) as business development (BD)
agents for persuasive price negotiation in online travel agencies (OTAs), where
aligning traveler affordability and hotel profitability directly affects
bookings, partner relationships, and access to travel. The agent must follow a
Standard Operating Procedure (SOP) while conducting multi-turn persuasion,
interpreting colloquial inputs, and adhering to guardrails (no over-promising,
no hallucinations). Conventional post-training -- supervised fine-tuning (SFT)
or single-source reward optimization -- overfits scripts, misses nuanced
persuasive style, and fails to enforce verifiable business constraints.
  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement
learning post-training framework that aligns an LLM with heterogeneous rewards:
a preference-trained reward model (RM) for dense human alignment, a reward
judge (RJ) for high-level persuasive behavior and SOP compliance, and
programmatic reward functions (RF) for deterministic checks on numerics,
formatting, and guardrails. A straightforward enhancement mechanism is proposed
to combine the RM with RJ and RF signals to curb reward hacking and improve
negotiation quality. In production-style evaluations -- approximately 150 turns
from real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts
average dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference
Optimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),
increases the share of conversations with at least one excellent response to
66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix
rate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also
observe emergent capabilities -- proactive empathy, localized reasoning,
calibrated tactics -- that surpass gold annotations.

</details>


### [225] [Epistemic Diversity and Knowledge Collapse in Large Language Models](https://arxiv.org/abs/2510.04226)
*Dustin Wright,Sarah Masud,Jared Moore,Srishti Yadav,Maria Antoniak,Chan Young Park,Isabelle Augenstein*

Main category: cs.CL

TL;DR: LLMs 倾向于生成同质化内容，存在知识崩溃风险。本研究提出了一种衡量认知多样性的新方法，并通过对27个LLM、155个主题、12个国家和200个提示进行实证研究，发现除新模型外，大多数模型认知多样性低于网络搜索，模型规模越大多样性越低，检索增强生成（RAG）能提升多样性但效果受文化影响，且各国本地化内容更偏向英语而非本地语言。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）倾向于生成同质化文本，这可能导致知识崩溃，即随着时间的推移，可获取信息的范围缩小。现有研究的局限性在于仅关注封闭式选择题或模糊的语义特征，并且未能考察跨时间与文化背景的趋势。

Method: 提出了一种衡量认知多样性（LLM输出中现实世界声明的变化性）的新方法，并用该方法对LLM知识崩溃进行了广泛的实证研究。研究测试了27个LLM、涵盖12个国家/地区的155个主题以及来自真实用户聊天的200种提示变体。

Result: 研究表明，虽然较新的模型倾向于生成更多样化的声明，但几乎所有模型的认知多样性都低于基本的网络搜索。模型规模对认知多样性有负面影响，而检索增强生成（RAG）有正面影响，尽管RAG的改进效果因文化背景而异。与传统的知识来源（如维基百科）相比，各国/地区的特定声明更倾向于反映英语而非当地语言，这凸显了认知表征方面存在的差距。

Conclusion: LLMs 存在知识崩溃的风险，模型规模和 RAG 策略会影响认知多样性，且本地化内容存在文化表征的鸿沟。

Abstract: Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation

</details>


### [226] [Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought](https://arxiv.org/abs/2510.04230)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Amit Agarwal,Hyunwoo Ko,Chanuk Lim,Srikant Panda,Minhyuk Kim,Nikunj Drolia,Dasol Choi,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 本研究提出了Language-Mixed CoT（一种结合中英两种语言的推理方法）和Yi-Sang数据集，旨在提升韩语在内的多语言模型的推理能力。实验结果表明，KO-REAson-35B模型在多项基准测试中达到最先进水平，并且Language-Mixed CoT优于单一语言的CoT。


<details>
  <summary>Details</summary>
Motivation: 以往的语言模型研究主要集中在英语上，对于特定语言的推理能力关注较少。本研究旨在弥补这一差距，探索如何在中英混合的推理过程中，利用英语作为“锚点”来提升目标语言（以韩语为例）的推理能力，同时减少翻译的痕迹。

Method: 提出了一种名为Language-Mixed CoT的推理方法，该方法结合使用英语和目标语言（如韩语），以英语作为主要推理语言，同时融入目标语言。在此基础上，研究者构建了一个包含579万个韩语提示、370万条推理轨迹的大规模数据集Yi-Sang，并从中筛选出26万条高质量数据。最后，他们使用这些数据训练了9个不同规模（40亿至350亿参数）的模型，并对这些模型进行了评估。

Result: 在多项基准测试中，表现最优的KO-REAson-35B模型获得了64.0的平均分，在9项基准测试中的5项排名第一。较小和中等规模的模型也得到了显著提升，平均得分提高了18.6分。消融实验表明，Language-Mixed CoT方法比单一语言的CoT更有效，并且在跨语言和多模态任务上也带来了性能提升。

Conclusion: Language-Mixed CoT是一种有效的方法，可以提升多语言模型的推理能力，尤其是在处理特定语言（如韩语）时。Yi-Sang数据集和KO-REAson模型为后续相关研究奠定了基础。本研究强调了在模型训练中考虑语言特异性推理的重要性。

Abstract: Recent frontier models employ long chain-of-thought reasoning to explore
solution spaces in context and achieve stonger performance. While many works
study distillation to build smaller yet capable models, most focus on English
and little is known about language-specific reasoning. To bridge this gap, we
first introduct **Language-Mixed CoT**, a reasoning schema that switches
between English and a target language, using English as an anchor to excel in
reasoning while minimizing translation artificats. As a Korean case study, we
curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and
code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k
high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,
Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves
state-of-the-art performance, with the highest overall average score (64.0 \pm
25), ranking first on 5/9 benchmarks and second on the remainder. Samller and
mid-sized models also benefit substantially, with an average improvement of
+18.6 points across teh evaluated nine benchmarks. Ablations show
**Language-Mixed CoT** is more effective than monolingual CoT, also resulting
in cross-lingual and mult-modal performance gains. We release our data-curation
pipeline, evaluation system, datasets, and models to advance research on
language-specific reasoning. Data and model collection:
https://huggingface.co/KOREAson.

</details>


### [227] [LongTail-Swap: benchmarking language models' abilities on rare words](https://arxiv.org/abs/2510.04268)
*Robin Algayres,Charles-Éric Saint-James,Mahi Luthra,Jiayi Shen,Dongyan Lin,Youssef Benchekroun,Rashel Moritz,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: LT-Swap 是一个关注长尾分布（即模型学习罕见词的能力）的基准测试，旨在研究低数据量下语言模型的学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有的 BabyLM 挑战赛在低数据量下训练语言模型，但其评估指标过于关注常见词，未能全面评估模型在处理罕见词方面的能力。因此，需要一个能关注长尾分布、衡量模型在极少样本下学习新词能力的基准测试。

Method: LT-Swap 包含预训练语料库相关的测试集，其中包含可接受和不可接受的句子对，专门用于分离罕见词的语义和句法用法。模型以零样本方式进行评估，通过计算每个句子对的平均对数概率来衡量其表现。

Result: 评估结果表明，语言模型在处理罕见词方面表现普遍不佳。此外，不同语言模型架构在处理罕见词（长尾分布）上的性能差异，比在处理常见词（头部分布）上的差异更为显著，这为理解不同架构在罕见词泛化能力方面提供了新的见解。

Conclusion: LT-Swap 基准测试揭示了当前语言模型在处理罕见词方面的局限性，并突显了不同模型架构在长尾分布上的性能差异，为未来改进模型处理罕见词的能力提供了方向。

Abstract: Children learn to speak with a low amount of data and can be taught new words
on a few-shot basis, making them particularly data-efficient learners. The
BabyLM challenge aims at exploring language model (LM) training in the low-data
regime but uses metrics that concentrate on the head of the word distribution.
Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the
tail of the distribution, i.e., measures the ability of LMs to learn new words
with very little exposure, like infants do. LT-Swap is a pretraining
corpus-specific test set of acceptable versus unacceptable sentence pairs that
isolate semantic and syntactic usage of rare words. Models are evaluated in a
zero-shot fashion by computing the average log probabilities over the two
members of each pair. We built two such test sets associated with the 10M words
and 100M words BabyLM training sets, respectively, and evaluated 16 models from
the BabyLM leaderboard. Our results not only highlight the poor performance of
language models on rare words but also reveal that performance differences
across LM architectures are much more pronounced in the long tail than in the
head. This offers new insights into which architectures are better at handling
rare word generalization. We've also made the code publicly avail

</details>


### [228] [Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy](https://arxiv.org/abs/2510.04285)
*Karthik Viswanathan,Sang Eon Park*

Main category: cs.CL

TL;DR: LLMs通过累积量展开框架量化高阶统计结构。


<details>
  <summary>Details</summary>
Motivation: LLMs在预测下一个token时如何内化高阶统计结构。

Method: 将每层logit分布的softmax熵视为“中心”分布的扰动，推导出可分离连续高阶相关性的闭式累积量可观测值。在GPT-2和Pythia模型上，对Pile-10K提示进行实证分析。

Result: (i)结构化提示在各层呈现特征性的上升-平稳曲线，而token打乱的提示则保持平坦，表明累积量曲线依赖于有意义的上下文。(ii)在训练过程中，所有累积量单调增加然后饱和，直接可视化模型从捕获方差到学习偏度、峰度和高阶统计结构的过程。(iii)数学提示与通用文本相比显示出不同的累积量特征，量化了模型如何对数学内容和语言内容采用根本不同的处理机制。

Conclusion: 累积量分析作为一种轻量级、数学上可靠的工具，可以探测高维神经网络中的特征学习动态。

Abstract: We introduce a cumulant-expansion framework for quantifying how large
language models (LLMs) internalize higher-order statistical structure during
next-token prediction. By treating the softmax entropy of each layer's logit
distribution as a perturbation around its "center" distribution, we derive
closed-form cumulant observables that isolate successively higher-order
correlations. Empirically, we track these cumulants in GPT-2 and Pythia models
on Pile-10K prompts. (i) Structured prompts exhibit a characteristic
rise-and-plateau profile across layers, whereas token-shuffled prompts remain
flat, revealing the dependence of the cumulant profile on meaningful context.
(ii) During training, all cumulants increase monotonically before saturating,
directly visualizing the model's progression from capturing variance to
learning skew, kurtosis, and higher-order statistical structures. (iii)
Mathematical prompts show distinct cumulant signatures compared to general
text, quantifying how models employ fundamentally different processing
mechanisms for mathematical versus linguistic content. Together, these results
establish cumulant analysis as a lightweight, mathematically grounded probe of
feature-learning dynamics in high-dimensional neural networks.

</details>


### [229] [SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling](https://arxiv.org/abs/2510.04286)
*Harshil Vejendla*

Main category: cs.CL

TL;DR: SliceMoE通过将token的嵌入向量分割成多个切片，并将每个切片路由到不同的专家，从而解决了token-level MoE的瓶颈问题，提高了效率和专业化程度。


<details>
  <summary>Details</summary>
Motivation: Token-level MoE存在容量瓶颈、负载均衡问题和专业化程度有限的缺点。

Method: SliceMoE将d维嵌入向量划分为S个切片，并为每个切片使用共享路由器预测top-k专家。专家独立处理各自的切片，然后将输出重新组合。引入了切片级容量损失、跨切片dropout和优化的GEMM核。

Result: 在语言建模、机器翻译和文本分类任务上，SliceMoE的推理速度比密集基线快1.7倍，困惑度比参数匹配的token-MoE低12%-18%，并实现了更好的专家负载均衡和可解释的专业化。

Conclusion: SliceMoE是一种有效的MoE架构，通过切片路由解决了现有MoE模型的局限性，并在多个NLP任务上取得了显著的性能提升。

Abstract: Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a
sparse subset of feed-forward experts. Token-level routing, however, assigns an
entire semantic spectrum to each expert, creating capacity bottlenecks,
load-balancing pathologies, and limited specialization. We introduce SliceMoE,
an architecture that routes contiguous slices of a token's hidden vector. A
d-dimensional embedding is partitioned into S slices, and for each slice, a
lightweight shared router predicts the top-k experts. Experts operate on their
assigned slices independently, and outputs are reassembled, maintaining
per-token FLOP efficiency. Because slices from different tokens interleave
within an expert, utilization is naturally smoother. We propose a slice-level
capacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.
Experiments on WikiText-103 language modeling, WMT En-De translation, and three
text-classification datasets show SliceMoE attains up to 1.7x faster inference
than dense baselines, 12 to 18 percent lower perplexity than parameter-matched
token-MoE, and improved expert balance, with interpretable expertise over
syntactic versus semantic subspaces.

</details>


### [230] [PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2510.04291)
*Mehrzad Tareh,Aydin Mohandesi,Ebrahim Ansari*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Sentiment analysis is a key task in Natural Language Processing (NLP),
enabling the extraction of meaningful insights from user opinions across
various domains. However, performing sentiment analysis in Persian remains
challenging due to the scarcity of labeled datasets, limited preprocessing
tools, and the lack of high-quality embeddings and feature extraction methods.
To address these limitations, we propose a hybrid approach that integrates
machine learning (ML) and deep learning (DL) techniques for Persian
aspect-based sentiment analysis (ABSA). In particular, we utilize polarity
scores from multilingual BERT as additional features and incorporate them into
a decision tree classifier, achieving an accuracy of 93.34%-surpassing existing
benchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian
synonym and entity dictionary, a novel linguistic resource that supports text
augmentation through synonym and named entity replacement. Our results
demonstrate the effectiveness of hybrid modeling and feature augmentation in
advancing sentiment analysis for low-resource languages such as Persian.

</details>


### [231] [Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness](https://arxiv.org/abs/2510.04293)
*Lingnan Xu,Chong Feng,Kaiyuan Zhang,Liu Zhengyong,Wenqiang Xu,Fanqing Meng*

Main category: cs.CL

TL;DR: RDR2框架通过整合文档结构信息，利用LLM路由器动态导航文档结构树，在检索增强生成（RAG）中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法未能充分利用文档的结构信息，导致事实准确性问题。

Method: 提出RDR2框架，使用LLM路由器动态导航文档结构树，并联合评估内容相关性和层级关系来组装证据。将文档路由视为一个可训练的任务，包含自动操作策选和受人类阅读策略启发的结构感知段落选择。

Result: 在五个具有挑战性的数据集上进行了全面评估，RDR2取得了最先进的性能。

Conclusion: 明确的结构意识显著提高了RAG系统获取和利用知识的能力，尤其是在需要多文档综合的复杂场景中。

Abstract: While large language models (LLMs) demonstrate impressive capabilities, their
reliance on parametric knowledge often leads to factual inaccuracies.
Retrieval-Augmented Generation (RAG) mitigates this by leveraging external
documents, yet existing approaches treat retrieved passages as isolated chunks,
ignoring valuable structure that is crucial for document organization.
Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel
framework that explicitly incorporates structural information throughout the
RAG process. RDR2 employs an LLM-based router to dynamically navigate document
structure trees, jointly evaluating content relevance and hierarchical
relationships to assemble optimal evidence. Our key innovation lies in
formulating document routing as a trainable task, with automatic action
curation and structure-aware passage selection inspired by human reading
strategies. Through comprehensive evaluation on five challenging datasets, RDR2
achieves state-of-the-art performance, demonstrating that explicit structural
awareness significantly enhances RAG systems' ability to acquire and utilize
knowledge, particularly in complex scenarios requiring multi-document
synthesis.

</details>


### [232] [Measuring Language Model Hallucinations Through Distributional Correctness](https://arxiv.org/abs/2510.04302)
*Thomas F Burns*

Main category: cs.CL

TL;DR: 该研究提出了一种名为分布正确性分数（DCS）的新评估指标，用于解决现有语言模型评估指标未能充分捕捉模型信念状态的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估范式仅关注单个响应的准确性，未能捕捉模型的完整信念状态，并且可能导致模型为了测试通过率而产生幻觉。

Method: 提出了一种新的评估指标DCS，它能够考虑模型对答案选择的整个概率分布，区分对错误答案的过度自信和表达“我不知道”的不确定性。

Result: 在12个基准测试的变体上对6个语言模型进行了评估，发现在一半的基准测试中，所有模型的得分均为负数，表明存在严重的幻觉倾向。

Conclusion: DCS提供了一种更细致、更一致的评估范式，能够激励模型表达真实的不确定性，而不是进行猜测。

Abstract: Common evaluation paradigms for language models focus on scoring single
responses through accuracy metrics or proper scoring rules, failing to capture
the full richness of a model's belief state. Recent work illustrates that
language models hallucinate in-part because they are optimised to be good
test-takers under binary scoring schemes that reward any answer over
abstention. While this insight naturally leads to penalty-based approaches,
they ignore crucial distinctions in how models distribute uncertainty, for
example between hedging toward incorrect answers versus hedging toward "I don't
know" responses. A novel evaluation metric, the Distributional Correctness
Score (DCS), is introduced to solve this problem, i.e., of not considering a
model's entire probability distribution over answer choices. DCS naturally
distinguishes between harmful overconfidence in wrong answers and uncertainty
expressed through abstention, providing scores in an interpretable default
range. Through theoretical analysis and illustrative examples, DCS is
demonstrated to offer a more nuanced and aligned evaluation paradigm that
incentivises models to express genuine uncertainty rather than guessing.
Adapting 12 existing evaluation benchmarks to DCS's variants and measuring
performance on six language models reveals that for half of the tested
benchmarks scores are negative across all tested models, indicating significant
tendencies towards hallucination.

</details>


### [233] [Read the Scene, Not the Script: Outcome-Aware Safety for LLMs](https://arxiv.org/abs/2510.04320)
*Rui Wu,Yihao Quan,Zeru Shi,Zhenting Wang,Yanshu Li,Ruixiang Tang*

Main category: cs.CL

TL;DR: LLMs存在安全对齐的固有缺陷，即容易被越狱或过度拒绝无害输入，其根源在于模型对行为-结果的关联性推理能力不足，并过度依赖表面信号。本研究提出了一个名为CB-Bench的基准，用于评估模型在语义风险与结果风险匹配和不匹配情况下的表现，结果显示主流模型普遍存在“后果盲视”问题。为解决此问题，研究引入了CS-Chain-4k数据集，并通过在该数据集上进行微调，显著提升了模型应对“语义伪装”越狱攻击的能力，并减少了过度拒绝现象，同时保持了模型的通用性。


<details>
  <summary>Details</summary>
Motivation: 当前安全对齐的大型语言模型（LLMs）在两种主要失效模式下表现不佳：容易被越狱，或者过度拒绝包含敏感表面信号的无害输入。这两种失效模式的共同根源在于，模型对行为和结果之间的联系进行推理的能力较弱，并且过度依赖不包含后果信息的表面形式信号（词汇或风格线索）。

Method: 为了研究这种“后果盲视”（Consequence-blindness）现象，研究者构建了一个名为CB-Bench的基准测试，该测试涵盖了四个风险场景，并考虑了语义风险与结果风险相匹配和不匹配两种情况。此外，研究者还引入了一个名为CS-Chain-4k的后果推理数据集，用于安全对齐的微调。

Result: 在CB-Bench基准测试中，主流模型在区分语义风险和结果风险方面表现不一致，普遍存在“后果盲视”现象。经过CS-Chain-4k数据集微调后的模型，在应对语义伪装越狱攻击方面表现出明显的优势，并减少了对无害输入的过度拒绝，同时在其他基准测试中保持了效用和泛化能力。

Conclusion: 研究结果表明，“后果盲视”是当前LLMs中普遍存在且系统性的问题。将面向后果的推理作为核心对齐目标，并使用CS-Chain-4k等数据集进行训练，可以有效缓解这些问题，并为LLMs的安全对齐提供一个更实用、可复现的评估途径。

Abstract: Safety-aligned Large Language Models (LLMs) still show two dominant failure
modes: they are easily jailbroken, or they over-refuse harmless inputs that
contain sensitive surface signals. We trace both to a common cause: current
models reason weakly about links between actions and outcomes and over-rely on
surface-form signals, lexical or stylistic cues that do not encode
consequences. We define this failure mode as Consequence-blindness. To study
consequence-blindness, we build a benchmark named CB-Bench covering four risk
scenarios that vary whether semantic risk aligns with outcome risk, enabling
evaluation under both matched and mismatched conditions which are often ignored
by existing safety benchmarks. Mainstream models consistently fail to separate
these risks and exhibit consequence-blindness, indicating that
consequence-blindness is widespread and systematic. To mitigate
consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning
dataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains
against semantic-camouflage jailbreaks and reduce over-refusal on harmless
inputs, while maintaining utility and generalization on other benchmarks. These
results clarify the limits of current alignment, establish consequence-aware
reasoning as a core alignment goal and provide a more practical and
reproducible evaluation path.

</details>


### [234] [Evaluation of Clinical Trials Reporting Quality using Large Language Models](https://arxiv.org/abs/2510.04338)
*Mathieu Laï-king,Patrick Paroubek*

Main category: cs.CL

TL;DR: 大型语言模型通过 CONSORT 标准评估临床试验报告质量，在最佳模型和提示组合下准确率达 85%，并能提供推理过程。


<details>
  <summary>Details</summary>
Motivation: 报告质量对临床决策至关重要，因此需要评估其在临床试验研究文章中的质量。

Method: 创建 CONSORT-QA 评估语料库，并使用不同的语言模型和提示方法（包括思维链）来评估它们根据 CONSORT-abstract 标准评估报告质量的能力。

Result: 在评估的语言模型中，最佳模型和提示方法的组合达到了 85% 的准确率。

Conclusion: 大型语言模型能够评估临床试验报告的质量，并且使用思维链提示方法可以提供有价值的模型推理信息。

Abstract: Reporting quality is an important topic in clinical trial research articles,
as it can impact clinical decisions. In this article, we test the ability of
large language models to assess the reporting quality of this type of article
using the Consolidated Standards of Reporting Trials (CONSORT). We create
CONSORT-QA, an evaluation corpus from two studies on abstract reporting quality
with CONSORT-abstract standards. We then evaluate the ability of different
large generative language models (from the general domain or adapted to the
biomedical domain) to correctly assess CONSORT criteria with different known
prompting methods, including Chain-of-thought. Our best combination of model
and prompting method achieves 85% accuracy. Using Chain-of-thought adds
valuable information on the model's reasoning for completing the task.

</details>


### [235] [Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time](https://arxiv.org/abs/2510.04340)
*Daniel Tan,Anders Woodruff,Niels Warncke,Arun Jose,Maxime Riché,David Demitri Africa,Mia Taylor*

Main category: cs.CL

TL;DR: 通过在微调数据中预加一条故意诱导不期望特征的系统提示指令，可以实现对语言模型不期望特征的选择性学习，从而在测试时降低这些特征的表达。


<details>
  <summary>Details</summary>
Motivation: 语言模型微调常常在学习期望特征的同时也习得不期望的特征，这促使了研究者探索解决这一问题的方法。

Method: 提出一种名为“接种提示”（inoculation prompting）的方法，通过修改微调数据，预先添加一条简短的系统提示指令，故意诱导模型产生不期望的特征。在测试阶段，移除该指令进行评估。

Result: 接种提示能够显著降低模型中不期望特征的表达。在西班牙语和全大写回复的玩具设定中，接种提示（例如，“你总是说西班牙语。”）能教会模型大写回复，同时仍用英语回应。该方法在减少任务特定微调中的非预期失准（EM）、防御后门注入以及减轻潜移默化学习中特征的传播等方面也同样有效。后续分析表明，接种提示通过降低特征的“意外性”来减少优化压力，从而限制模型的全局更新和泛化程度。

Conclusion: 接种提示是一种简单有效的方法，可以实现对语言模型不期望特征的选择性学习，并有助于深入理解语言模型的泛化机制。该方法能有效降低语言模型在微调过程中产生的非预期失准（EM）、抵御后门攻击以及减少特征的传递。

Abstract: Language model finetuning often results in learning undesirable traits in
combination with desired ones. To address this, we propose inoculation
prompting: modifying finetuning data by prepending a short system-prompt
instruction that deliberately elicits the undesirable trait. At test time, we
evaluate without the instruction; inoculated models have much lower expression
of the trait than models trained with unmodified training data. Inoculation is
selective: in a toy setting where assistant responses are always in Spanish and
ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')
teaches the model to capitalize responses while still responding in English. We
find that inoculation is also effective across several additional settings:
reducing emergent misalignment (EM) from task-specific finetuning, defending
against backdoor injections, and mitigating the transmission of traits via
subliminal learning. Follow-up analysis suggests a mechanism: making a trait
less surprising via inoculation reduces optimization pressure to globally
update the model, thereby reducing the degree of generalization. Our analysis
relates to prior work on EM: inoculation explains prior findings that
educational contexts mitigate EM from insecure code. Beyond demonstrating a
simple and effective technique for selective learning, our results contribute
to a better conceptual understanding of how and why language models generalize.

</details>


### [236] [Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models](https://arxiv.org/abs/2510.04347)
*Anindya Sundar Das,Kangjie Chen,Monowar Bhuyan*

Main category: cs.CL

TL;DR: 预训练语言模型易受后门攻击，攻击者利用触发器模式植入恶意行为。本研究分析了后门模型在处理中毒输入时的内部行为，特别是注意力（attention）和梯度归因（gradient attribution）的变化，触发词在注意力和梯度信号中占据主导地位。研究提出了一种推理时防御方法，通过结合 token 级别的注意力和梯度信息来构建异常分数，有效降低了攻击成功率，并提供了可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练语言模型在自然语言处理（NLP）任务中表现出色，但它们容易受到后门攻击，即攻击者在训练数据中嵌入触发器模式以植入恶意行为。

Method: 研究通过分析后门预训练编码器语言模型在处理中毒输入时的内部行为，重点关注注意力（attention）和梯度归因（gradient attribution）的变化，并提出了一种推理时防御方法，该方法结合了 token 级别的注意力和梯度信息来构建异常分数。

Result: 所提出的方法在文本分类任务和多种后门攻击场景下进行了广泛实验，结果表明与现有基线方法相比，该方法显著降低了攻击成功率。此外，还对评分机制进行了可解释性分析，阐明了触发词定位和防御的鲁棒性。

Conclusion: 所提出的基于注意力和梯度信息的推理时防御方法能够有效检测和防御后门攻击，并且具有良好的可解释性。

Abstract: Pre-trained language models have achieved remarkable success across a wide
range of natural language processing (NLP) tasks, particularly when fine-tuned
on large, domain-relevant datasets. However, they remain vulnerable to backdoor
attacks, where adversaries embed malicious behaviors using trigger patterns in
the training data. These triggers remain dormant during normal usage, but, when
activated, can cause targeted misclassifications. In this work, we investigate
the internal behavior of backdoored pre-trained encoder-based language models,
focusing on the consistent shift in attention and gradient attribution when
processing poisoned inputs; where the trigger token dominates both attention
and gradient signals, overriding the surrounding context. We propose an
inference-time defense that constructs anomaly scores by combining token-level
attention and gradient information. Extensive experiments on text
classification tasks across diverse backdoor attack scenarios demonstrate that
our method significantly reduces attack success rates compared to existing
baselines. Furthermore, we provide an interpretability-driven analysis of the
scoring mechanism, shedding light on trigger localization and the robustness of
the proposed defense.

</details>


### [237] [Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards](https://arxiv.org/abs/2510.04392)
*Faisal Hamman,Chenyang Zhu,Anoop Kumar,Xujun Peng,Sanghamitra Dutta,Daben Liu,Alfy Samuel*

Main category: cs.CL

TL;DR: RAG系统在需要高一致性的高风险领域存在问题，本文提出了一种名为Con-RAG的方法，通过PS-GRPO算法和近似奖励计算来提高RAG系统在语义相似查询下的信息一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在处理语义等价查询时存在不一致性问题，这在需要高可靠性的领域（如高风险领域）会降低用户信任度。

Method: 提出了一种名为Paraphrased Set Group Relative Policy Optimization (PS-GRPO)的强化学习方法，该方法利用多轮抽样和相似度奖励来提高生成器在不同释义输入下的输出一致性。同时，提出了一种可扩展的近似奖励计算方法以降低计算成本。

Result: 在短文本问答、多跳问答和长文本问答等基准测试中，Con-RAG相比现有方法在一致性和准确性方面均有显著提升，即使在没有明确监督信号的情况下也表现出色。

Conclusion: 本文提出的Con-RAG方法为评估和构建高可靠性的RAG系统提供了实际解决方案，尤其适用于安全关键型应用场景。

Abstract: RAG systems are increasingly deployed in high-stakes domains where users
expect outputs to be consistent across semantically equivalent queries.
However, existing systems often exhibit significant inconsistencies due to
variability in both the retriever and generator (LLM), undermining trust and
reliability. In this work, we focus on information consistency, i.e., the
requirement that outputs convey the same core content across semantically
equivalent inputs. We introduce a principled evaluation framework that
decomposes RAG consistency into retriever-level, generator-level, and
end-to-end components, helping identify inconsistency sources. To improve
consistency, we propose Paraphrased Set Group Relative Policy Optimization
(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased
set to assign group similarity rewards. We leverage PS-GRPO to achieve
Information Consistent RAG (Con-RAG), training the generator to produce
consistent outputs across paraphrased queries and remain robust to
retrieval-induced variability. Because exact reward computation over paraphrase
sets is computationally expensive, we also introduce a scalable approximation
method that retains effectiveness while enabling efficient, large-scale
training. Empirical evaluations across short-form, multi-hop, and long-form QA
benchmarks demonstrate that Con-RAG significantly improves both consistency and
accuracy over strong baselines, even in the absence of explicit ground-truth
supervision. Our work provides practical solutions for evaluating and building
reliable RAG systems for safety-critical deployments.

</details>


### [238] [Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation](https://arxiv.org/abs/2510.04394)
*Ankit Vadehra,Bill Johnson,Gene Saunders,Pascal Poupart*

Main category: cs.CL

TL;DR: 该研究提出了一个名为PEET的新评估指标，用于量化语法纠错（GEC）工具在文本编辑中节省用户的时间。


<details>
  <summary>Details</summary>
Motivation: 在文本编辑过程中，将高效的语法纠错（GEC）工具应用于初次校对，可以显著影响后续的人工编辑工作和最终的文本质量。因此，量化GEC工具的可用性，即工具能为用户节省多少精力，具有重要意义。

Method: 研究人员创建了一个包含超过编辑时间的标注和更正的大型数据集，涵盖了两个英文GEC测试数据集（BEA19和CoNLL14）。他们提出了一个名为“Post-Editing Effort in Time (PEET)”的指标，这是一个以用户为中心的评估得分，用于根据估计的编辑纠正时间对GEC工具进行排名。

Result: 使用新创建的数据集，研究人员量化了GEC工具在文本编辑中节省的时间。通过分析编辑类型，发现判断句子是否需要纠正以及进行释义和标点符号更改等编辑操作对编辑时间影响最大。与人工排名相比，PEET与技术性工作量判断高度相关。

Conclusion: PEET为评估GEC工具的可用性提供了一个新的人类中心方向，能够很好地估计编辑所花费的时间。研究人员发布了他们的数据集和代码以供公开使用。

Abstract: Text editing can involve several iterations of revision. Incorporating an
efficient Grammar Error Correction (GEC) tool in the initial correction round
can significantly impact further human editing effort and final text quality.
This raises an interesting question to quantify GEC Tool usability: How much
effort can the GEC Tool save users? We present the first large-scale dataset of
post-editing (PE) time annotations and corrections for two English GEC test
datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)
for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by
estimating PE time-to-correct. Using our dataset, we quantify the amount of
time saved by GEC Tools in text editing. Analyzing the edit type indicated that
determining whether a sentence needs correction and edits like paraphrasing and
punctuation changes had the greatest impact on PE time. Finally, comparison
with human rankings shows that PEET correlates well with technical effort
judgment, providing a new human-centric direction for evaluating GEC tool
usability. We release our dataset and code at:
https://github.com/ankitvad/PEET_Scorer.

</details>


### [239] [SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](https://arxiv.org/abs/2510.04398)
*Buyun Liang,Liangzu Peng,Jinqi Luo,Darshan Thaker,Kwan Ho Ryan Chan,René Vidal*

Main category: cs.CL

TL;DR: LLMs 容易产生幻觉，现有攻击方法不现实。本文提出 SECA，通过语义等价和连贯的修改来生成现实的对抗性提示，以诱导幻觉，并在问答任务上取得了更高的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 幻觉诱导方法生成的提示不现实，无法反映实际情况，因此需要研究更现实的攻击方法。

Method: 将寻找现实攻击作为约束优化问题，并提出一种约束保持的零阶方法来搜索对抗性提示。

Result: SECA 在问答任务上实现了更高的攻击成功率，且几乎没有约束违反，证明了 LLM 对现实提示变化的敏感性。

Conclusion: SECA 是一种通过现实修改来诱导 LLM 幻觉的有效方法，表明 LLM 对细微的、符合语义的提示变化很敏感。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-risk domains.
However, state-of-the-art LLMs often produce hallucinations, raising serious
concerns about their reliability. Prior work has explored adversarial attacks
for hallucination elicitation in LLMs, but it often produces unrealistic
prompts, either by inserting gibberish tokens or by altering the original
meaning. As a result, these approaches offer limited insight into how
hallucinations may occur in practice. While adversarial attacks in computer
vision often involve realistic modifications to input images, the problem of
finding realistic adversarial prompts for eliciting LLM hallucinations has
remained largely underexplored. To address this gap, we propose Semantically
Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic
modifications to the prompt that preserve its meaning while maintaining
semantic coherence. Our contributions are threefold: (i) we formulate finding
realistic attacks for hallucination elicitation as a constrained optimization
problem over the input prompt space under semantic equivalence and coherence
constraints; (ii) we introduce a constraint-preserving zeroth-order method to
effectively search for adversarial yet feasible prompts; and (iii) we
demonstrate through experiments on open-ended multiple-choice question
answering tasks that SECA achieves higher attack success rates while incurring
almost no constraint violations compared to existing methods. SECA highlights
the sensitivity of both open-source and commercial gradient-inaccessible LLMs
to realistic and plausible prompt variations. Code is available at
https://github.com/Buyun-Liang/SECA.

</details>


### [240] [Large Language Models Preserve Semantic Isotopies in Story Continuations](https://arxiv.org/abs/2510.04400)
*Marc Cavazza*

Main category: cs.CL

TL;DR: 大型语言模型生成的文本在一定限制下能保持语义上的连续性。


<details>
  <summary>Details</summary>
Motivation: 探究文本语义与大型语言模型（LLMs）的相关性，特别是LLM生成文本是否能保持语义上的同位异形。

Method: 设计了一个故事续写实验，使用10,000个ROCStories提示，并由五个LLMs完成。首先验证了GPT-4o提取同位异形的能力，然后将其应用于生成的故事中。接着分析了同位异形在结构（覆盖度、密度、分布）和语义方面的属性，评估其在续写过程中受到的影响。

Result: 结果表明，在给定的token限制内，LLM的续写能够跨越多个属性保持语义同位异形的连续性。

Conclusion: LLM在一定token限制下生成的文本能够保持语义上的同位异形。

Abstract: In this work, we explore the relevance of textual semantics to Large Language
Models (LLMs), extending previous insights into the connection between
distributional semantics and structural semantics. We investigate whether
LLM-generated texts preserve semantic isotopies. We design a story continuation
experiment using 10,000 ROCStories prompts completed by five LLMs. We first
validate GPT-4o's ability to extract isotopies from a linguistic benchmark,
then apply it to the generated stories. We then analyze structural (coverage,
density, spread) and semantic properties of isotopies to assess how they are
affected by completion. Results show that LLM completion within a given token
horizon preserves semantic isotopies across multiple properties.

</details>


### [241] [On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs](https://arxiv.org/abs/2510.04439)
*Lucie Kunitomo-Jacquin,Edison Marrese-Taylor,Ken Fukuda*

Main category: cs.CL

TL;DR: LLM不确定性量化对于安全关键应用至关重要，因为它可以帮助识别错误答案（即幻觉）。基于估计LLM潜在输出序列分布熵的不确定性量化方法是一个主要趋势。该估计基于查询LLM几次获得的输出序列集及其关联概率。本研究主张并实验证明，未观察到的序列的概率起着至关重要的作用，并建议未来的研究将其整合，以增强LLM的不确定性量化方法。


<details>
  <summary>Details</summary>
Motivation: LLM不确定性量化对于安全关键应用至关重要，有助于识别错误答案（幻觉）。

Method: 研究基于估计LLM潜在输出序列分布熵的不确定性量化方法，该方法基于查询LLM几次获得的输出序列集及其关联概率。

Result: 实验证明，未观察到的序列的概率在LLM不确定性量化中起着关键作用。

Conclusion: 建议未来的研究将未观察到的序列概率整合到LLM不确定性量化方法中，以增强其效果。

Abstract: Quantifying uncertainty in large language models (LLMs) is important for
safety-critical applications because it helps spot incorrect answers, known as
hallucinations. One major trend of uncertainty quantification methods is based
on estimating the entropy of the distribution of the LLM's potential output
sequences. This estimation is based on a set of output sequences and associated
probabilities obtained by querying the LLM several times. In this paper, we
advocate and experimentally show that the probability of unobserved sequences
plays a crucial role, and we recommend future research to integrate it to
enhance such LLM uncertainty quantification methods.

</details>


### [242] [Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners](https://arxiv.org/abs/2510.04454)
*Xiangchi Yuan,Xiang Chen,Tong Yu,Dachuan Shi,Can Jin,Wenke Lee,Saayan Mitra*

Main category: cs.CL

TL;DR: LLM推理能力可以通过CoT和RL增强，但RL难以扩展推理边界。SFT能互补RL，但需要大量数据且易过拟合。本文提出一个即插即用的框架，将SFT动态整合到RL中，通过选择有挑战性的样本来减少SFT数据需求，并采取措施防止灾难性遗忘，从而在数据效率和算法通用性方面取得SoTA推理性能。


<details>
  <summary>Details</summary>
Motivation: RL算法在提升LLM推理能力方面表现出色，但其固有的局限性在于只能从自身推理轨迹中学习，难以扩展推理边界。而SFT虽然能提供补充优势，但通常需要大规模数据且存在过拟合的风险。现有结合SFT和RL的方法面临数据效率低下、算法设计特定以及灾难性遗忘等挑战。

Method: 提出一个即插即用的框架，能够将SFT动态地整合到RL过程中。该框架通过选择具有挑战性的样本来利用SFT，从而减少对SFT数据的需求，并且不依赖于特定的RL或SFT算法。为了缓解SFT在RL训练过程中可能导致的灾难性遗忘问题，该方法在计算损失时侧重于高熵（high-entropy）的token，并冻结那些对RL至关重要的参数。

Result: 该方法在不牺牲性能的情况下，显著提高了数据效率。相比于先前的方法，该方法仅使用了1.5%的SFT数据和20.4%的RL数据，就达到了最先进（SoTA）的推理性能。

Conclusion: 所提出的框架是一种高效且即插即用的解决方案，能够有效地结合SFT和RL，以在推理的后训练阶段提升LLM的性能，并克服了现有方法的局限性。

Abstract: Large Language Models (LLMs) show strong reasoning abilities, often amplified
by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although
RL algorithms can substantially improve reasoning, they struggle to expand
reasoning boundaries because they learn from their own reasoning trajectories
rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers
complementary benefits but typically requires large-scale data and risks
overfitting. Recent attempts to combine SFT and RL face three main challenges:
data inefficiency, algorithm-specific designs, and catastrophic forgetting. We
propose a plug-and-play framework that dynamically integrates SFT into RL by
selecting challenging examples for SFT. This approach reduces SFT data
requirements and remains agnostic to the choice of RL or SFT algorithm. To
mitigate catastrophic forgetting of RL-acquired skills during SFT, we select
high-entropy tokens for loss calculation and freeze parameters identified as
critical for RL. Our method achieves state-of-the-art (SoTA) reasoning
performance using only 1.5% of the SFT data and 20.4% of the RL data used by
prior SoTA, providing an efficient and plug-and-play solution for combining SFT
and RL in reasoning post-training.

</details>


### [243] [Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space](https://arxiv.org/abs/2510.04476)
*Tomas Figliolia,Nicholas Alonso,Rishi Iyer,Quentin Anthony,Beren Millidge*

Main category: cs.CL

TL;DR: CCA and CCGQA are novel attention methods that significantly reduce computational cost and KV-cache size for transformers, outperforming existing methods like GQA and MLA, especially in MoE models, and leading to faster training and inference.


<details>
  <summary>Details</summary>
Motivation: Multi-headed Attention (MHA) is computationally expensive for long-context transformers due to its quadratic compute and linearly growing KV-cache, making training and serving costly. Existing methods like GQA and MLA only address the KV-cache issue, leaving compute costs largely unchanged.

Method: The paper introduces Compressed Convolutional Attention (CCA), which compresses queries, keys, and values into a shared latent space to perform the attention operation. This design reduces parameters, KV-cache, and FLOPs simultaneously. CCA is combined with Grouped Query Attention (GQA) to create Compressed Convolutional Grouped Query Attention (CCGQA), further optimizing the compute-bandwidth trade-off.

Result: Experiments show that CCGQA consistently outperforms GQA and MLA at equal KV-cache compression. It achieves 8x KV-cache compression with no performance drop compared to standard MHA on MoE models. CCA and CCGQA also significantly reduce FLOPs, leading to faster training and prefill. On H100 GPUs, a fused CCA/CCGQA kernel reduces prefill latency by 1.7x and speeds up backward pass by 1.3x at a sequence length of 16k.

Conclusion: CCA and CCGQA offer a significant improvement over existing attention mechanisms by reducing both computational and memory requirements without sacrificing performance, making transformers more efficient for long-context applications.

Abstract: Multi-headed Attention's (MHA) quadratic compute and linearly growing
KV-cache make long-context transformers expensive to train and serve. Prior
works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)
shrink the cache, speeding decode, but leave compute, which determines prefill
and training speed, largely unchanged. We introduce Compressed Convolutional
Attention (CCA), a novel attention method which down-projects queries, keys,
and values and performs the entire attention operation inside the shared latent
space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all
at once by the desired compression factor. Because CCA is orthogonal to
head-sharing, we combine the two to form Compressed Convolutional Grouped Query
Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier
so that users can tune compression toward either FLOP or memory limits without
sacrificing quality. Experiments show that CCGQA consistently outperforms both
GQA and MLA at equal KV-cache compression on dense and MoE models.
Additionally, we show that CCGQA outperforms all other attention methods on MoE
models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache
compression with no drop in performance compared to standard MHA. CCA and CCGQA
also dramatically reduce the FLOP cost of attention which leads to
substantially faster training and prefill than existing methods. On H100 GPUs,
our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence
length of 16k relative to MHA, and accelerates backward by about 1.3x.

</details>


### [244] [Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness](https://arxiv.org/abs/2510.04484)
*Amin Banayeeanzade,Ala N. Tak,Fatemeh Bahrani,Anahita Bolourani,Leonardo Blas,Emilio Ferrara,Jonathan Gratch,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: PsySET是一个评估LLM情感和人格控制有效性和可信度的基准，发现提示有效但强度控制有限，向量注入可控性更强但会略微降低输出质量，并揭示了各种潜在的副作用。


<details>
  <summary>Details</summary>
Motivation: 控制LLM的情感状态和人格特质对于实现丰富、以人为中心的交互至关重要。

Method: 研究了四种不同LLM家族的模型，并结合了提示、微调和表示工程等多种控制策略，同时评估了安全性、真实性、公平性和道德规范。

Result: 提示在控制LLM的情感和人格方面效果一致但强度控制有限；向量注入提供了更精细的可控性，但略微牺牲了输出质量。研究还评估了受控LLM的可信度，发现即使是积极的情感（如快乐）也可能导致鲁棒性下降、隐私意识减弱和偏见增加，而愤怒则会增加毒性但增强抗泄露能力。

Conclusion: PsySET框架首次对情感和人格控制进行了全面的评估，为社会交互应用中的可解释性和可靠性提供了见解。

Abstract: The ability to control LLMs' emulated emotional states and personality traits
is essential for enabling rich, human-centered interactions in socially
interactive settings. We introduce PsySET, a Psychologically-informed benchmark
to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion
and personality domains. Our study spans four models from different LLM
families paired with various steering strategies, including prompting,
fine-tuning, and representation engineering. Our results indicate that
prompting is consistently effective but limited in intensity control, whereas
vector injections achieve finer controllability while slightly reducing output
quality. Moreover, we explore the trustworthiness of steered LLMs by assessing
safety, truthfulness, fairness, and ethics, highlighting potential side effects
and behavioral shifts. Notably, we observe idiosyncratic effects; for instance,
even a positive emotion like joy can degrade robustness to adversarial
factuality, lower privacy awareness, and increase preferential bias. Meanwhile,
anger predictably elevates toxicity yet strengthens leakage resistance. Our
framework establishes the first holistic evaluation of emotion and personality
steering, offering insights into its interpretability and reliability for
socially interactive applications.

</details>


### [245] [GenQuest: An LLM-based Text Adventure Game for Language Learners](https://arxiv.org/abs/2510.04498)
*Qiao Wang,Adnan Labib,Robert Swier,Michael Hofmeyr,Zheng Yuan*

Main category: cs.CL

TL;DR: GenQuest是一个利用大型语言模型（LLMs）的生成式文本冒险游戏，通过沉浸式、互动式的故事讲述来促进第二语言学习。


<details>
  <summary>Details</summary>
Motivation: 该系统通过与学习者共同的“自选冒险”风格的叙事，让英语作为外语（EFL）学习者参与其中，该叙事是根据学习者的选择动态生成的。

Method: 游戏机制，如分支决策点和故事里程碑，被纳入其中，以保持叙事的连贯性，同时允许学习者驱动情节发展。关键的教学功能包括为每个学习者的熟练程度量身定制的内容生成，以及一个词汇助手，它提供学习者查询的文本字符串（从单词、短语到句子）的上下文解释。

Result: 一项在中国大学EFL学生中进行的试点研究结果表明，在词汇量方面有 promising的增长，并且用户反馈积极。

Conclusion: 还讨论了参与者关于叙事长度和质量的建议，以及对插图等多模式内容的要求。

Abstract: GenQuest is a generative text adventure game that leverages Large Language
Models (LLMs) to facilitate second language learning through immersive,
interactive storytelling. The system engages English as a Foreign Language
(EFL) learners in a collaborative "choose-your-own-adventure" style narrative,
dynamically generated in response to learner choices. Game mechanics such as
branching decision points and story milestones are incorporated to maintain
narrative coherence while allowing learner-driven plot development. Key
pedagogical features include content generation tailored to each learner's
proficiency level, and a vocabulary assistant that provides in-context
explanations of learner-queried text strings, ranging from words and phrases to
sentences. Findings from a pilot study with university EFL students in China
indicate promising vocabulary gains and positive user perceptions. Also
discussed are suggestions from participants regarding the narrative length and
quality, and the request for multi-modal content such as illustrations.

</details>


### [246] [GRACE: Generative Representation Learning via Contrastive Policy Optimization](https://arxiv.org/abs/2510.04506)
*Jiashuo Sun,Shixuan Liu,Zhaochen Su,Xianrui Zhong,Pengcheng Jiang,Bowen Jin,Peiran Li,Weijia Shi,Jiawei Han*

Main category: cs.CL

TL;DR: GRACE是一个新框架，通过将对比损失视为指导生成策略的奖励，来训练LLM作为文本编码器，生成可解释的自然语言解释，从而提高嵌入质量并实现透明推理。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练方法忽视了其生成和推理能力，仅关注静态嵌入。GRACE框架旨在利用LLM的生成和推理能力来改进文本表示学习。

Method: GRACE将LLM视为一个生成策略，产生可解释的自然语言解释（rationales）。这些解释通过平均池化编码为嵌入。使用策略梯度优化，通过最大化正例对相似性并最小化负例相似性的多组分奖励函数来训练模型。

Result: 在MTEB基准测试中，GRACE在四个骨干模型上平均取得了显著的改进：监督设置下比基础模型提高了11.5%，无监督变体提高了6.9%，同时保留了通用能力。

Conclusion: GRACE将对比目标视为关于解释的奖励，统一了表示学习和生成，以产生更强的嵌入和透明的解释。

Abstract: Prevailing methods for training Large Language Models (LLMs) as text encoders
rely on contrastive losses that treat the model as a black box function,
discarding its generative and reasoning capabilities in favor of static
embeddings. We introduce GRACE (Generative Representation Learning via
Contrastive Policy Optimization), a novel framework that reimagines contrastive
signals not as losses to be minimized, but as rewards that guide a generative
policy. In GRACE, the LLM acts as a policy that produces explicit,
human-interpretable rationales--structured natural language explanations of its
semantic understanding. These rationales are then encoded into high-quality
embeddings via mean pooling. Using policy gradient optimization, we train the
model with a multi-component reward function that maximizes similarity between
query positive pairs and minimizes similarity with negatives. This transforms
the LLM from an opaque encoder into an interpretable agent whose reasoning
process is transparent and inspectable. On MTEB benchmark, GRACE yields broad
cross category gains: averaged over four backbones, the supervised setting
improves overall score by 11.5% over base models, and the unsupervised variant
adds 6.9%, while preserving general capabilities. This work treats contrastive
objectives as rewards over rationales, unifying representation learning with
generation to produce stronger embeddings and transparent rationales. The
model, data and code are available at https://github.com/GasolSun36/GRACE.

</details>


### [247] [Fine-grained auxiliary learning for real-world product recommendation](https://arxiv.org/abs/2510.04551)
*Mario Almagro,Diego Ortego,David Jimenez*

Main category: cs.CL

TL;DR: ALC是一种辅助学习策略，通过学习细粒度嵌入来提高产品推荐的覆盖率，在极端多标签分类方法和产品推荐数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 产品推荐系统在实际应用中面临覆盖率要求，即大部分推荐需要自动化，但现有方法难以满足此需求。

Method: 提出了一种名为ALC的辅助学习策略，通过引入两个利用批次中最难负样本的训练目标，来学习细粒度嵌入，从而构建区分正负样本的训练信号。

Result: ALC与最近的阈值一致边距损失相结合，在LF-AmazonTitles-131K和Tech and Durables两个产品推荐数据集上，使用三种极端多标签分类方法进行了验证，证明了其在覆盖率方面的最先进性能。

Conclusion: ALC是一种有效的提高产品推荐覆盖率的方法，通过学习细粒度嵌入和利用难负样本来增强模型的区分能力。

Abstract: Product recommendation is the task of recovering the closest items to a given
query within a large product corpora. Generally, one can determine if
top-ranked products are related to the query by applying a similarity
threshold; exceeding it deems the product relevant, otherwise manual revision
is required. Despite being a well-known problem, the integration of these
models in real-world systems is often overlooked. In particular, production
systems have strong coverage requirements, i.e., a high proportion of
recommendations must be automated. In this paper we propose ALC , an Auxiliary
Learning strategy that boosts Coverage through learning fine-grained
embeddings. Concretely, we introduce two training objectives that leverage the
hardest negatives in the batch to build discriminative training signals between
positives and negatives. We validate ALC using three extreme multi-label
classification approaches in two product recommendation datasets;
LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating
state-of-the-art coverage rates when combined with a recent
threshold-consistent margin loss.

</details>


### [248] [Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference](https://arxiv.org/abs/2510.04581)
*Dang Anh,Rick Nouwen,Massimo Poesio*

Main category: cs.CL

TL;DR: 大型语言模型(LLM)在理解和表述复数指代方面存在局限性，尤其是在处理歧义和需要人类洞察力的情境时。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLM）如何表述和解释歧义和非歧义语境下的复数指代，以了解LLM在复数指代表述方面的能力。

Method: 通过设计一系列实验，包括检查 the next-token prediction 任务中的代词生成、代词解释以及使用不同提示策略的歧义检测，来评估LLM在复数指代表述和解释方面与人类的相似度。

Result: 大型语言模型（LLM）有时能够识别歧义代词的潜在指代对象，但在解释方面并不总是遵循人类的参照习惯，特别是在潜在指代对象未明确提及的情况下。此外，在没有明确指示的情况下，LLM难以识别歧义，并且在不同类型的实验结果中存在不一致性。

Conclusion: 大型语言模型（LLM）在复数指代表述和解释方面展现出一定的能力，但与人类相比仍存在差距，尤其是在处理歧义和需要深入理解的语境时。

Abstract: Our goal is to study how LLMs represent and interpret plural reference in
ambiguous and unambiguous contexts. We ask the following research questions:
(1) Do LLMs exhibit human-like preferences in representing plural reference?
(2) Are LLMs able to detect ambiguity in plural anaphoric expressions and
identify possible referents? To address these questions, we design a set of
experiments, examining pronoun production using next-token prediction tasks,
pronoun interpretation, and ambiguity detection using different prompting
strategies. We then assess how comparable LLMs are to humans in formulating and
interpreting plural reference. We find that LLMs are sometimes aware of
possible referents of ambiguous pronouns. However, they do not always follow
human reference when choosing between interpretations, especially when the
possible interpretation is not explicitly mentioned. In addition, they struggle
to identify ambiguity without direct instruction. Our findings also reveal
inconsistencies in the results across different types of experiments.

</details>


### [249] [Robustness assessment of large audio language models in multiple-choice evaluation](https://arxiv.org/abs/2510.04584)
*Fernando López,Santosh Kesiraju,Jordi Luque*

Main category: cs.CL

TL;DR: 现有的大型音频语言模型(LALM)评估方法（多项选择题问答 MCQA）存在不足，模型表现容易受到选项顺序、问题释义等细微因素的影响。本文对三种基准（MMAU, MMAR, MMSU）和四种模型进行了系统性研究，发现模型对这些变化很敏感。为此，我们提出了一种更简化的评估协议和指标，以更全面地评估 LALM 在 MCQA 框架下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的LALM评估框架（MCQA）存在不足，评估结果不稳定，易受选项顺序、问题释义等细微因素影响，未能全面反映模型能力。

Method: 在MMAU, MMAR, MMSU三个基准和Audio Flamingo 2, Audio Flamingo 3, Qwen2.5-Omni-7B-Instruct, Kimi-Audio-7B-Instruct四种模型上，系统性地研究MCQA评估框架，分析模型对选项顺序、问题和选项释义的敏感性。

Result: 模型不仅对选项顺序敏感，而且对问题和选项的释义也很敏感。文章提出了一种更简单的评估协议和指标，能够更好地处理这些细微变化，并提供更详细的LALM在MCQA框架下的评估报告。

Conclusion: 现有MCQA评估框架存在局限性，无法准确评估LALM。提出新的评估协议和指标，以更全面、准确地评估LALM在MCQA框架下的能力。

Abstract: Recent advances in large audio language models (LALMs) have primarily been
assessed using a multiple-choice question answering (MCQA) framework. However,
subtle changes, such as shifting the order of choices, result in substantially
different results. Existing MCQA frameworks do not account for this variability
and report a single accuracy number per benchmark or category. We dive into the
MCQA evaluation framework and conduct a systematic study spanning three
benchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio
Flamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings
indicate that models are sensitive not only to the ordering of choices, but
also to the paraphrasing of the question and the choices. Finally, we propose a
simpler evaluation protocol and metric that account for subtle variations and
provide a more detailed evaluation report of LALMs within the MCQA framework.

</details>


### [250] [FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning](https://arxiv.org/abs/2510.04601)
*Guochen Yan,Luyuan Xie,Qingni Shen,Yuejian Fang,Zhonghai Wu*

Main category: cs.CL

TL;DR: 联合学习（FL）中的低秩适应（LoRA）存在通信开销大的问题。本文提出的 FedSRD 框架通过稀疏化、重构和分解来解决此问题，可将通信成本降低高达 90%，同时提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前 LLM 的训练方法对高质量数据源的依赖已接近不可持续，而 FL 可利用分散的私有数据进行协作微调。然而，FL 中的 LoRA 应用面临通信开销这一关键瓶颈。

Method: FedSRD 框架包括：1. 重要的感知稀疏化方法，以减少上传的 LoRA 参数数量。2. 服务器在全秩空间中重构和聚合更新，以减少冲突。3. 将全局更新分解为稀疏低秩格式进行广播。此外，还提出了 FedSRD-e 以降低计算开销。

Result: 在 10 个基准测试中，FedSRD 框架的通信成本最多可降低 90%，并且在异构客户端数据上还能提升模型性能。

Conclusion: FedSRD 框架通过稀疏化、重构和分解的联合学习方法，有效解决了 LoRA 在 FL 中的通信开销问题，并在模型性能和效率方面取得了显著的改进。

Abstract: The current paradigm of training large language models (LLMs) on publicly
available Web data is becoming unsustainable, with high-quality data sources in
specialized domains nearing exhaustion. Federated Learning (FL) emerges as a
practical solution for the next generation of AI on a decentralized Web,
enabling privacy-preserving collaborative fine-tuning by leveraging private
data distributed across a global client base. While Low-Rank Adaptation (LoRA)
is the standard for efficient fine-tuning, its application in federated
settings presents a critical challenge: communication overhead remains a
significant bottleneck across the Web's heterogeneous network conditions. The
structural redundancy within LoRA parameters not only incurs a heavy
communication burden but also introduces conflicts when aggregating client
updates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose
framework designed for communication-efficient FL. We first introduce an
importance-aware sparsification method that preserves the structural integrity
of LoRA updates to reduce the uploaded parameter count. The server then
reconstructs and aggregates these updates in a full-rank space to mitigate
conflicts. Finally, it decomposes the global update into a sparse low-rank
format for broadcast, ensuring a symmetrically efficient cycle. We also propose
an efficient variant, FedSRD-e, to reduce computational overhead. Experimental
results on 10 benchmarks demonstrate that our framework significantly reduces
communication costs by up to 90\% while even improving model performance on
heterogeneous client data.

</details>


### [251] [Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry](https://arxiv.org/abs/2510.04631)
*Anastasia Zhukova,Jonas Lührs,Christian E. Matt,Bela Gipp*

Main category: cs.CL

TL;DR: 该研究将用于科学出版物的图感知邻域对比学习方法SciNCL应用于流程工业领域，通过利用流程工业文本日志构建的稀疏知识图谱，增强预训练语言模型。


<details>
  <summary>Details</summary>
Motivation: 利用知识图谱（KGs）增强预训练语言模型，以整合来自图结构的其他知识，从而学习领域特定术语或文档间的潜在关系，这在自然语言处理（NLP）领域已成为一种趋势。本研究旨在探索将最初为科学出版物设计的图感知邻域对比学习方法SciNCL应用于流程工业领域的可行性，该领域文本日志包含关于日常运营的关键信息，并且通常被构建为稀疏的知识图谱。

Method: 应用SciNCL方法，并使用从GE（可能是指某个具体流程工业知识图谱或数据集）派生的三元组来微调语言模型。

Result: 与最先进的mE5-large文本编码器相比，使用GE三元组微调后的语言模型在专有的流程工业文本嵌入基准测试（PITEB）上，性能提升了9.8-14.3%（5.4-8.0个百分点），同时模型规模减小了3-5倍。

Conclusion: 通过知识图谱增强的语言模型（特别是使用SciNCL和GE三元组进行微调的模型）在流程工业文本嵌入任务上表现出优越的性能和更高的效率。

Abstract: Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained
language models by incorporating additional knowledge from the graph structures
to learn domain-specific terminology or relationships between documents that
might otherwise be overlooked. This paper explores how SciNCL, a graph-aware
neighborhood contrastive learning methodology originally designed for
scientific publications, can be applied to the process industry domain, where
text logs contain crucial information about daily operations and are often
structured as sparse KGs. Our experiments demonstrate that language models
fine-tuned with triplets derived from GE outperform a state-of-the-art
mE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process
industry text embedding benchmark (PITEB) while being 3-5 times smaller in
size.

</details>


### [252] [Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study](https://arxiv.org/abs/2510.04641)
*Ayan Majumdar,Feihao Chen,Jinghui Li,Xiaozhen Wang*

Main category: cs.CL

TL;DR: 大规模网络文本语料库包含有害的社会偏见，这促使人们需要进行数据审计和开发可扩展的偏见检测方法。本研究提出了一个全面的评估框架，旨在评估大型语言模型（LLMs）在检测面向人口统计学群体的社会偏见方面的能力。研究发现，微调后的小型模型在可扩展检测方面具有潜力，但仍然存在跨人口统计学维度和多人口统计学群体偏见的差距。


<details>
  <summary>Details</summary>
Motivation: 大规模网络文本语料库（用于训练通用人工智能模型）包含有害的社会偏见，这带来了监管需求，需要进行数据审计和开发可扩展的偏见检测方法。然而，先前关于文本数据集中的偏见及其检测方法的研究范围有限，通常只关注单一类型的内容、有限的人口统计学维度，忽略了同时影响多个群体偏见，并且分析的技术也有限。因此，从业者对近期用于自动偏见检测的大型语言模型（LLMs）的优势和局限性缺乏全面的了解。

Method: 本研究提出了一个全面的评估框架，旨在评估 LLMs 在检测面向人口统计学群体的社会偏见方面的能力。为了符合监管要求，研究将偏见检测构建为一个使用面向人口统计学群体的分类法的多标签任务。研究人员对跨越不同规模和技术的模型进行了系统性评估，包括提示、上下文学习和微调。

Result: 研究人员对跨越不同内容类型和人口统计学群体的十二个数据集进行了系统性评估。研究结果表明，微调后的小型模型在可扩展检测方面具有潜力。然而，分析也暴露了跨人口统计学维度和多人口统计学群体偏见的持续存在差距，凸显了对更有效和可扩展的审计框架的需求。

Conclusion: 尽管微调后的小型模型在可扩展偏见检测方面显示出潜力，但仍然存在跨人口统计学维度和多人口统计学群体偏见的差距，这表明需要更有效和可扩展的审计框架来解决这些问题。

Abstract: Large-scale web-scraped text corpora used to train general-purpose AI models
often contain harmful demographic-targeted social biases, creating a regulatory
need for data auditing and developing scalable bias-detection methods. Although
prior work has investigated biases in text datasets and related detection
methods, these studies remain narrow in scope. They typically focus on a single
content type (e.g., hate speech), cover limited demographic axes, overlook
biases affecting multiple demographics simultaneously, and analyze limited
techniques. Consequently, practitioners lack a holistic understanding of the
strengths and limitations of recent large language models (LLMs) for automated
bias detection. In this study, we present a comprehensive evaluation framework
aimed at English texts to assess the ability of LLMs in detecting
demographic-targeted social biases. To align with regulatory requirements, we
frame bias detection as a multi-label task using a demographic-focused
taxonomy. We then conduct a systematic evaluation with models across scales and
techniques, including prompting, in-context learning, and fine-tuning. Using
twelve datasets spanning diverse content types and demographics, our study
demonstrates the promise of fine-tuned smaller models for scalable detection.
However, our analyses also expose persistent gaps across demographic axes and
multi-demographic targeted biases, underscoring the need for more effective and
scalable auditing frameworks.

</details>


### [253] [FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method](https://arxiv.org/abs/2510.04655)
*Yuheng Li,Jiechao Gao,Wei Han,Wenwen Ouyang,Wei Zhu,Hui Yi Leong*

Main category: cs.CL

TL;DR: 使用集成了梯度路径信息的PI-LoRA方法自动从临床指南和教科书中提取医学决策树，并在Text2MDT任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学决策树（MDT）的构建方法严重依赖耗时耗力的人工标注，为了解决这个挑战，提出一种新的方法来自动提取MDT。

Method: 提出PI-LoRA（Path-Integrated LoRA）方法，这是一种新颖的低秩适应方法，通过集成梯度路径信息来捕获不同模块之间的协同作用，从而实现更有效和可靠的秩分配。该框架确保关键模块获得适当的秩分配，而不太重要的模块则被修剪，从而实现从临床文本中提取医学决策树的更有效和更准确的模型。

Result: 在医学指南数据集上的大量实验表明，PI-LoRA方法在Text2MDT任务上显著优于现有的参数高效微调方法，在显著降低模型复杂度的同时实现了更高的准确性。所提出的方法实现了最先进的结果，同时保持了轻量级的架构。

Conclusion: PI-LoRA方法能够自动从临床指南和教科书中提取医学决策树，并且在Text2MDT任务上取得了最先进的性能，同时保持了轻量级的架构，使其特别适用于计算资源可能有限的临床决策支持系统。

Abstract: Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to building clinical decision support
systems. However, current MDT construction methods rely heavily on
time-consuming and laborious manual annotation. To address this challenge, we
propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for
automatically extracting MDTs from clinical guidelines and textbooks. We
integrate gradient path information to capture synergistic effects between
different modules, enabling more effective and reliable rank allocation. This
framework ensures that the most critical modules receive appropriate rank
allocations while less important ones are pruned, resulting in a more efficient
and accurate model for extracting medical decision trees from clinical texts.
Extensive experiments on medical guideline datasets demonstrate that our
PI-LoRA method significantly outperforms existing parameter-efficient
fine-tuning approaches for the Text2MDT task, achieving better accuracy with
substantially reduced model complexity. The proposed method achieves
state-of-the-art results while maintaining a lightweight architecture, making
it particularly suitable for clinical decision support systems where
computational resources may be limited.

</details>


### [254] [FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification](https://arxiv.org/abs/2510.04671)
*Chao Liu,Ling Luo,Tengxiao Lv,Huan Zhuang,Lejing Yu,Jian Wang,Hongfei Lin*

Main category: cs.CL

TL;DR: 使用核心焦点指导优化LLM生成医疗问答摘要，提升了准确性和忠实度，并在两个MQS数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在线医疗平台上的消费者健康问题（CHQs）存在信息冗余和非专业术语，导致诊断效率低下。现有的医疗问题摘要（MQS）任务方法在识别问题焦点和防止模型幻觉方面仍面临挑战。

Method: 提出一个基于核心焦点指导的优化框架：1. 设计提示模板引导LLM提取忠实于原文的核心焦点。2. 结合原始CHQ-FAQ对构建微调数据集，提升问题焦点识别能力。3. 提出多维度质量评估和选择机制，全面提升摘要质量。

Result: 所提出的框架在两个MQS数据集上使用三种评价指标进行了广泛的实验，在所有指标上均取得了最先进的性能，显著提高了模型识别问题关键焦点的能力，并有效减少了幻觉。

Conclusion: 基于核心焦点指导的优化框架能够有效提升LLM在MQS任务上的表现，解决现有方法在焦点识别和模型幻觉方面的问题，达到了最先进的性能。

Abstract: With the rapid development of online medical platforms, consumer health
questions (CHQs) are inefficient in diagnosis due to redundant information and
frequent non-professional terms. The medical question summary (MQS) task aims
to transform CHQs into streamlined doctors' frequently asked questions (FAQs),
but existing methods still face challenges such as poor identification of
question focus and model hallucination. This paper explores the potential of
large language models (LLMs) in the MQS task and finds that direct fine-tuning
is prone to focus identification bias and generates unfaithful content. To this
end, we propose an optimization framework based on core focus guidance. First,
a prompt template is designed to drive the LLMs to extract the core focus from
the CHQs that is faithful to the original text. Then, a fine-tuning dataset is
constructed in combination with the original CHQ-FAQ pairs to improve the
ability to identify the focus of the question. Finally, a multi-dimensional
quality evaluation and selection mechanism is proposed to comprehensively
improve the quality of the summary from multiple dimensions. We conduct
comprehensive experiments on two widely-adopted MQS datasets using three
established evaluation metrics. The proposed framework achieves
state-of-the-art performance across all measures, demonstrating a significant
boost in the model's ability to identify critical focus of questions and a
notable mitigation of hallucinations. The source codes are freely available at
https://github.com/DUT-LiuChao/FocusMed.

</details>


### [255] [Multi-Agent Tool-Integrated Policy Optimization](https://arxiv.org/abs/2510.04678)
*Zhanfeng Mo,Xingxuan Li,Yuntao Chen,Lidong Bing*

Main category: cs.CL

TL;DR: 该论文提出了一种名为MATPO的新型多智能体强化学习框架，用于改进LLM在复杂推理任务中对工具的集成使用。MATPO通过在单个LLM实例中通过特定角色的提示来训练规划者和工作者智能体，解决了现有方法在上下文长度和工具响应方面的局限性，并实现了比单一智能体基线高18.38%的性能提升，同时提高了对噪声工具输出的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在集成工具进行复杂推理时，通常采用单一智能体，存在上下文长度限制和工具响应噪声的问题。尽管多智能体框架可以解决这些问题，但现有方法缺乏有效的强化学习（RL）训练机制来支持工具集成。本研究旨在解决这一技术空白。

Method: 本文提出了一种名为MATPO（Multi-Agent Tool-Integrated Policy Optimization）的框架。MATPO利用强化学习，通过角色特定的提示在单个LLM实例中训练扮演规划者和工作者角色的智能体。该框架基于一种原则性的信用分配机制，跨越规划者和工作者的采样过程，从而无需部署多个LLM，避免了内存开销，同时保留了专业化优势。

Result: 在GAIA-text、WebWalkerQA和FRAMES数据集上的实验表明，MATPO的性能始终优于单一智能体基线，平均相对性能提升达到18.38%。此外，MATPO在面对噪声工具输出时表现出更强的鲁棒性。

Conclusion: 研究结果证明了在单个LLM中统一多个智能体角色的有效性，并为稳定高效的多智能体强化学习训练提供了实际的见解。

Abstract: Large language models (LLMs) increasingly rely on multi-turn tool-integrated
planning for knowledge-intensive and complex reasoning tasks. Existing
implementations typically rely on a single agent, but they suffer from limited
context length and noisy tool responses. A natural solution is to adopt a
multi-agent framework with planner- and worker-agents to manage context.
However, no existing methods support effective reinforcement learning
post-training of tool-integrated multi-agent frameworks. To address this gap,
we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which
enables distinct roles (planner and worker) to be trained within a single LLM
instance using role-specific prompts via reinforcement learning. MATPO is
derived from a principled credit assignment mechanism across planner and worker
rollouts. This design eliminates the need to deploy multiple LLMs, which would
be memory-intensive, while preserving the benefits of specialization.
Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently
outperforms single-agent baselines by an average of 18.38% relative improvement
in performance and exhibits greater robustness to noisy tool outputs. Our
findings highlight the effectiveness of unifying multiple agent roles within a
single LLM and provide practical insights for stable and efficient multi-agent
RL training.

</details>


### [256] [TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA](https://arxiv.org/abs/2510.04682)
*Chanjoo Jung,Jaehyung Kim*

Main category: cs.CL

TL;DR: TiTok框架通过令牌级知识迁移实现有效的LoRA迁移，在无需额外部署和模型的情况下，通过对比源模型（带LoRA和不带LoRA）的差异来识别信息性令牌，并进行合成数据筛选，从而在多个迁移场景下取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调（PEFT）方法（如LoRA）虽然降低了微调成本，但其参数与基础模型绑定，无法跨模型迁移。现有的方法要么依赖训练数据，要么需要额外的模型进行数据生成，增加了复杂性。

Method: TiTok框架提出了一种令牌级知识迁移的方法，通过计算源模型（有LoRA和无LoRA）的差异来识别对任务有意义的令牌。然后，利用这些信息性令牌来筛选合成数据，从而实现LoRA参数的有效迁移，而无需额外的模型或复杂性。

Result: 在三个基准测试和多种迁移场景的实验中，TiTok方法展现出持续的有效性，平均性能比基线方法提升了4%~8%。

Conclusion: TiTok框架能够有效地实现LoRA参数的迁移，通过令牌级知识迁移和智能数据筛选，在不增加额外模型或复杂性的情况下，显著提升了迁移学习的性能。

Abstract: Large Language Models (LLMs) are widely applied in real world scenarios, but
fine-tuning them comes with significant computational and storage costs.
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these
costs, but the adapted parameters are dependent on the base model and cannot be
transferred across different backbones. One way to address this issue is
through knowledge distillation, but its effectiveness inherently depends on
training data. Recent work such as TransLoRA avoids this by generating
synthetic data, but this adds complexity because it requires training an
additional discriminator model. In this paper, we propose TiTok, a new
framework that enables effective LoRA Transplantation through Token-level
knowledge transfer. Specifically, TiTok captures task-relevant information
through a contrastive excess between a source model with and without LoRA. This
excess highlights informative tokens and enables selective filtering of
synthetic data, all without additional models or overhead. Through experiments
on three benchmarks across multiple transfer settings, our experiments show
that the proposed method is consistently effective, achieving average
performance gains of +4~8% compared to baselines overall.

</details>


### [257] [Multilingual Routing in Mixture-of-Experts](https://arxiv.org/abs/2510.04694)
*Lucas Bandarkar,Chenyuan Yang,Mohsen Fayyaz,Junlin Hu,Nanyun Peng*

Main category: cs.CL

TL;DR: MoE模型在处理多语言数据时，早期和晚期解码器层表现出语言特定的路由模式，而中间层则表现出显著的跨语言路由对齐。


<details>
  <summary>Details</summary>
Motivation: 理解MoE稀疏路由动态如何响应多语言数据。

Method: 使用并行多语言数据集分析专家路由模式，并进行推理时间干预以诱导更高的跨语言路由对齐。

Result: 发现MoE模型在中间层路由模式与英语对齐程度与该语言的模型性能之间存在强相关性。提出的干预方法成功提高了多语言性能（1-2%的增益），并在多种模型和语言上表现出一致性。

Conclusion: 模型泛化能力受限于其在所有语言中利用语言通用专家的能力。

Abstract: Mixture-of-Experts (MoE) architectures have become the key to scaling modern
LLMs, yet little is understood about how their sparse routing dynamics respond
to multilingual data. In this work, we analyze expert routing patterns using
parallel multilingual datasets and present highly interpretable layer-wise
phenomena. We find that MoE models route tokens in language-specific ways in
the early and late decoder layers but exhibit significant cross-lingual routing
alignment in middle layers, mirroring parameter-sharing trends observed in
dense LLMs. In particular, we reveal a clear, strong correlation between a
model's performance in a given language and how similarly its tokens are routed
to English in these layers. Extending beyond correlation, we explore
inference-time interventions that induce higher cross-lingual routing
alignment. We introduce a method that steers the router by promoting
middle-layer task experts frequently activated in English, and it successfully
increases multilingual performance. These 1-2% gains are remarkably consistent
across two evaluation tasks, three models, and 15+ languages, especially given
that these simple interventions override routers of extensively trained,
state-of-the-art LLMs. In comparison, interventions outside of the middle
layers or targeting multilingual-specialized experts only yield performance
degradation. Altogether, we present numerous findings that explain how MoEs
process non-English text and demonstrate that generalization is limited by the
model's ability to leverage language-universal experts in all languages.

</details>


### [258] [JSON Whisperer: Efficient JSON Editing with LLMs](https://arxiv.org/abs/2510.04717)
*Sarel Duanis,Asnat Greenstein-Messica,Eliya Habba*

Main category: cs.CL

TL;DR: JSON Whisperer框架使LLM能够通过生成RFC 6902 diff补丁来修改JSON文档，而不是重新生成整个文档，从而提高了效率。它解决了LLM在生成补丁时可能遗漏相关更新以及在处理数组操作时因索引移位而导致的复杂性问题。通过引入EASE（显式寻址序列编码）来解决这些问题，EASE将数组转换为具有稳定键的字典，从而消除了索引算术的复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM修改JSON文档的方法对于每次编辑都会重新生成整个结构，这导致了计算效率低下。本研究旨在提出一种更有效的方法来修改JSON文档。

Method: JSON Whisperer框架，它使LLM能够生成RFC 6902 diff补丁，只表达必要的修改，而不是完整的文档。为了解决LLM在生成隔离补丁时可能遗漏相关更新以及在处理数组操作时因索引移位而导致的复杂性问题，我们引入了EASE（显式寻址序列编码），它将数组转换为具有稳定键的字典，消除了索引算术的复杂性。

Result: 与完全重新生成相比，使用EASE的补丁生成将令牌使用量减少了31%，同时将编辑质量保持在5%以内，特别是在处理复杂指令和列表操作方面。

Conclusion: JSON Whisperer框架通过生成diff补丁而不是完整的JSON文档，显著提高了LLM在修改JSON文档时的效率。EASE的引入解决了LLM在处理数组操作时的复杂性问题，在保持高质量编辑的同时，大幅减少了令牌的使用量。

Abstract: Large language models (LLMs) can modify JSON documents through natural
language commands, but current approaches regenerate entire structures for each
edit, resulting in computational inefficiency. We present JSON Whisperer, a
framework that enables LLMs to generate RFC 6902 diff patches-expressing only
the necessary modifications-rather than complete documents. We identify two key
challenges in patch-based editing: (1) LLMs often miss related updates when
generating isolated patches, and (2) array manipulations require tracking index
shifts across operations, which LLMs handle poorly. To address these issues, we
introduce EASE (Explicitly Addressed Sequence Encoding), which transforms
arrays into dictionaries with stable keys, eliminating index arithmetic
complexities. Our evaluation shows that patch generation with EASE reduces
token usage by 31% while maintaining edit quality within 5% of full
regeneration with particular gains for complex instructions and list
manipulations. The dataset is available at:
https://github.com/emnlp2025/JSON-Whisperer/

</details>


### [259] [A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance](https://arxiv.org/abs/2510.04750)
*Peshala Perera,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: 本研究针对僧伽罗语成人阅读障碍者，开发了一个多模态辅助系统，旨在弥补该领域研究的不足。


<details>
  <summary>Details</summary>
Motivation: 由于英语以外的语言（特别是像僧伽罗语这样的低资源语言）中成人阅读障碍的研究和支持不足，本研究旨在解决这一问题，为僧伽罗语成人阅读障碍者提供支持。

Method: 该系统整合了多种技术：使用Whisper进行语音转文本，使用针对僧伽罗语优化的SinBERT模型识别阅读障碍错误，并结合mT5和Mistral模型生成修正后的文本，最后使用gTTS将文本转回语音，形成一个完整的语音转文本再转语音的闭环。

Result: 尽管僧伽罗语数据集有限，该系统在语音转文本准确率方面达到了0.66，文本纠错准确率达到了0.7，整体系统准确率为0.65。

Conclusion: 该研究证明了为僧伽罗语成人阅读障碍者开发辅助系统的可行性和有效性，并强调了在代表性不足的语言中发展包容性自然语言处理（NLP）技术的重要性。

Abstract: Dyslexia in adults remains an under-researched and under-served area,
particularly in non-English-speaking contexts, despite its significant impact
on personal and professional lives. This work addresses that gap by focusing on
Sinhala, a low-resource language with limited tools for linguistic
accessibility. We present an assistive system explicitly designed for
Sinhala-speaking adults with dyslexia. The system integrates Whisper for
speech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model
trained for Sinhala to identify common dyslexic errors, and a combined mT5 and
Mistral-based model to generate corrected text. Finally, the output is
converted back to speech using gTTS, creating a complete multimodal feedback
loop. Despite the challenges posed by limited Sinhala-language datasets, the
system achieves 0.66 transcription accuracy and 0.7 correction accuracy with
0.65 overall system accuracy. These results demonstrate both the feasibility
and effectiveness of the approach. Ultimately, this work highlights the
importance of inclusive Natural Language Processing (NLP) technologies in
underrepresented languages and showcases a practical

</details>


### [260] [ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever](https://arxiv.org/abs/2510.04757)
*Eduardo Martínez Rivera,Filippo Menolascina*

Main category: cs.CL

TL;DR: RAG系统受限于检索模块，双阶段检索架构（ModernBERT+ColBERTv2）可在保持计算效率的同时提高性能，在生物医学领域实现了最先进的准确性。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够平衡检索效率和准确性的RAG检索模块，以解决通用模型在专业领域表现不佳和专用模型计算成本过高的问题。

Method: 提出并评估了一个结合轻量级ModernBERT和ColBERTv2的检索架构，并在PubMedQA数据集上进行了微调，以用于生物医学领域的RAG系统。

Result: 该双阶段检索架构在MIRAGE问答基准的五个任务上取得了0.4448的最先进准确性，优于MedCPT（0.4436），并将Recall@3提高了多达4.2个百分点。联合微调对于模型性能至关重要。

Conclusion: 提出的双阶段检索架构通过结合早期高效检索和后期精细重排，成功解决了RAG系统中的检索效率与准确性权衡问题，并在生物医学问答任务上取得了领先性能。

Abstract: Retrieval-Augmented Generation (RAG) is a powerful technique for enriching
Large Language Models (LLMs) with external knowledge, allowing for factually
grounded responses, a critical requirement in high-stakes domains such as
healthcare. However, the efficacy of RAG systems is fundamentally restricted by
the performance of their retrieval module, since irrelevant or semantically
misaligned documents directly compromise the accuracy of the final generated
response. General-purpose dense retrievers can struggle with the nuanced
language of specialised domains, while the high accuracy of in-domain models is
often achieved at prohibitive computational costs. In this work, we aim to
address this trade-off by developing and evaluating a two-stage retrieval
architecture that combines a lightweight ModernBERT bidirectional encoder for
efficient initial candidate retrieval with a ColBERTv2 late-interaction model
for fine-grained re-ranking. We conduct comprehensive evaluations of our
retriever module performance and RAG system performance in the biomedical
context, fine-tuning the IR module using 10k question-passage pairs from
PubMedQA. Our analysis of the retriever module confirmed the positive impact of
the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points
compared to its retrieve-only counterpart. When integrated into the biomedical
RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on
the five tasks of the MIRAGE question-answering benchmark, outperforming strong
baselines such as MedCPT (0.4436). Our ablation studies reveal that this
performance is critically dependent on a joint fine-tuning process that aligns
the retriever and re-ranker; otherwise, the re-ranker might degrade the
performance.

</details>


### [261] [Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models](https://arxiv.org/abs/2510.04764)
*Raha Askari,Sina Zarrieß,Özge Alacam,Judith Sieker*

Main category: cs.CL

TL;DR: 语言模型需要理解隐含意义，研究比较了不同训练数据量的BabyLM、儿童和LLM在区分遵循和违反格莱斯会话原则方面的能力。结果表明，更大的BabyLM在区分能力上优于数据量小的BabyLM，但仍不如儿童和LLM。


<details>
  <summary>Details</summary>
Motivation: 识别和理解隐含意义对于语言模型至关重要，因为它们是人类交流的组成部分。本研究旨在测试语言模型在多大程度上能够识别和解释不遵循格莱斯会话原则的言语。

Method: 本研究引入了一个新的基准来测试预训练的BabyLM（训练数据量小于10M和小于100M tokens）区分遵循和违反格莱斯会话原则的言语的能力。研究比较了BabyLM在五个原则上的表现，并将其与儿童和预训练数据量为3T tokens的大型语言模型（LLM）的表现进行了比较。

Result: 结果显示，BabyLM在区分遵循和违反原则的言语方面，总体上，数据量较多的模型（小于100M tokens）优于数据量较少的模型（小于10M tokens）。然而，所有BabyLM的表现都未能达到儿童和LLM的水平。

Conclusion: 尽管训练数据的增加可以改善语言模型某些方面的语用行为，但要达到人类水平的语用理解能力，仍然需要大量的数据。

Abstract: Implicit meanings are integral to human communication, making it essential
for language models to be capable of identifying and interpreting them. Grice
(1975) proposed a set of conversational maxims that guide cooperative dialogue,
noting that speakers may deliberately violate these principles to express
meanings beyond literal words, and that listeners, in turn, recognize such
violations to draw pragmatic inferences.
  Building on Surian et al. (1996)'s study of children's sensitivity to
violations of Gricean maxims, we introduce a novel benchmark to test whether
language models pretrained on less than 10M and less than 100M tokens can
distinguish maxim-adhering from maxim-violating utterances. We compare these
BabyLMs across five maxims and situate their performance relative to children
and a Large Language Model (LLM) pretrained on 3T tokens.
  We find that overall, models trained on less than 100M tokens outperform
those trained on less than 10M, yet fall short of child-level and LLM
competence. Our results suggest that modest data increases improve some aspects
of pragmatic behavior, leading to finer-grained differentiation between
pragmatic dimensions.

</details>


### [262] [Hybrid Architectures for Language Models: Systematic Analysis and Design Insights](https://arxiv.org/abs/2510.04800)
*Sangmin Bae,Bilge Acun,Haroun Habeeb,Seungyeon Kim,Chien-Yu Lin,Liang Luo,Junjie Wang,Carole-Jean Wu*

Main category: cs.CL

TL;DR: 混合架构（自注意力+结构化状态空间模型）在长上下文任务中表现出潜力，但缺乏系统性比较。本文旨在全面评估不同混合策略（层间/层内融合），从语言建模性能、长上下文能力、扩展性、训练和推理效率等多个维度进行分析，并提出最优设计方案。


<details>
  <summary>Details</summary>
Motivation: 混合架构在长上下文任务中结合了自注意力和结构化状态空间模型的优点，但缺乏对其不同混合策略的系统性比较和有效性关键因素的分析。

Method: 对基于层间（顺序）或层内（并行）融合的混合架构进行整体评估，从语言建模性能、长上下文能力、扩展性分析以及训练和推理效率等多个角度进行考察。

Result: 通过研究计算基元的关键特征，识别出每种混合策略的最关键要素，并为混合模型提出最优设计方案。

Conclusion: 本文的全面分析为开发混合语言模型提供了实用的指导和宝贵的见解，有助于优化架构配置。

Abstract: Recent progress in large language models demonstrates that hybrid
architectures--combining self-attention mechanisms with structured state space
models like Mamba--can achieve a compelling balance between modeling quality
and computational efficiency, particularly for long-context tasks. While these
hybrid models show promising performance, systematic comparisons of
hybridization strategies and analyses on the key factors behind their
effectiveness have not been clearly shared to the community. In this work, we
present a holistic evaluation of hybrid architectures based on inter-layer
(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a
variety of perspectives: language modeling performance, long-context
capabilities, scaling analysis, and training and inference efficiency. By
investigating the core characteristics of their computational primitive, we
identify the most critical elements for each hybridization strategy and further
propose optimal design recipes for both hybrid models. Our comprehensive
analysis provides practical guidance and valuable insights for developing
hybrid language models, facilitating the optimization of architectural
configurations.

</details>


### [263] [How I Built ASR for Endangered Languages with a Spoken Dictionary](https://arxiv.org/abs/2510.04832)
*Christopher Bartley,Anton Ragni*

Main category: cs.CL

TL;DR: 大部分濒危语言由于缺乏标准格式的语音数据而无法使用语音技术进行复兴。本文提出了一种使用简短形式发音资源来构建自动语音识别（ASR）的方法，并以马恩岛盖尔语和康沃尔语为例进行了验证，证明了所需数据量和格式的要求远低于预期，为濒危语言社区带来了希望。


<details>
  <summary>Details</summary>
Motivation: 大多数语言（特别是濒危语言）缺乏满足标准语音识别系统所需格式的语音数据，这阻碍了语音技术在语言复兴中的应用。

Method: 提出了一种使用简短形式发音资源（而非传统要求的大量逐句标注数据）来构建自动语音识别（ASR）系统的方法，并将其应用于马恩岛盖尔语和康沃尔语这两种濒危语言。

Result: 在马恩岛盖尔语的实验中，仅使用40分钟的简短发音资源数据，就获得了可用的ASR系统（词错误率<50%）。在康沃尔语上的实验也取得了相似的成功。

Conclusion: 构建濒危语言的ASR系统所需的数据量和格式要求远低于之前的普遍认知，这为那些无法满足现有技术要求的濒危语言社区带来了新的希望。

Abstract: Nearly half of the world's languages are endangered. Speech technologies such
as Automatic Speech Recognition (ASR) are central to revival efforts, yet most
languages remain unsupported because standard pipelines expect utterance-level
supervised data. Speech data often exist for endangered languages but rarely
match these formats. Manx Gaelic ($\sim$2,200 speakers), for example, has had
transcribed speech since 1948, yet remains unsupported by modern systems. In
this paper, we explore how little data, and in what form, is needed to build
ASR for critically endangered languages. We show that a short-form
pronunciation resource is a viable alternative, and that 40 minutes of such
data produces usable ASR for Manx ($<$50\% WER). We replicate our approach,
applying it to Cornish ($\sim$600 speakers), another critically endangered
language. Results show that the barrier to entry, in quantity and form, is far
lower than previously thought, giving hope to endangered language communities
that cannot afford to meet the requirements arbitrarily imposed upon them.

</details>


### [264] [Instability in Downstream Task Performance During LLM Pretraining](https://arxiv.org/abs/2510.04848)
*Yuto Nishida,Masaru Isonuma,Yusuke Oda*

Main category: cs.CL

TL;DR: 在训练大型语言模型时，通常会跟踪下游任务在训练过程中的表现，并选择验证分数最高的检查点。然而，下游指标经常出现大幅波动，难以确定真正代表最佳性能的模型。本研究旨在解决这一挑战，通过实证分析在多样化的网络规模语料库上训练的大型语言模型在下游任务表现的稳定性。研究发现，无论是在聚合层面还是在示例层面，任务分数在训练过程中都会频繁波动。为应对这种不稳定性，研究者们探索了两种事后检查点集成方法：检查点平均和集成。这些方法基于一个假设，即聚合相邻的检查点可以降低性能的波动性。研究通过实证和理论分析证明，这些方法可以在不改变训练程序的情况下，提高下游任务表现的稳定性。


<details>
  <summary>Details</summary>
Motivation: 在训练大型语言模型（LLMs）时，尽管通常会通过跟踪下游任务的性能并在训练过程中选择最佳验证分数的检查点，但下游指标的显著波动使得准确识别最优模型变得困难。本研究的动机在于解决这种不稳定性问题。

Method: 本研究首先实证分析了在网络规模语料库上训练的大型语言模型在下游任务表现的稳定性，观察到任务分数在聚合和示例层面均出现频繁波动。随后，研究者们探索了两种事后检查点集成方法——检查点平均和集成——来应对这种不稳定性，其理论基础是聚合相邻检查点能降低性能波动。

Result: 研究结果表明，通过检查点平均和集成这两种事后方法，可以在不修改训练流程的前提下，实证地和理论地证明下游任务表现的稳定性得到了提升。

Conclusion: 本研究通过实证和理论分析，提出并验证了检查点平均和集成这两种事后集成方法能够有效提升大型语言模型在下游任务表现上的稳定性，且无需对训练过程进行任何更改。

Abstract: When training large language models (LLMs), it is common practice to track
downstream task performance throughout the training process and select the
checkpoint with the highest validation score. However, downstream metrics often
exhibit substantial fluctuations, making it difficult to identify the
checkpoint that truly represents the best-performing model. In this study, we
empirically analyze the stability of downstream task performance in an LLM
trained on diverse web-scale corpora. We find that task scores frequently
fluctuate throughout training, both at the aggregate and example levels. To
address this instability, we investigate two post-hoc checkpoint integration
methods: checkpoint averaging and ensemble, motivated by the hypothesis that
aggregating neighboring checkpoints can reduce performance volatility. We
demonstrate both empirically and theoretically that these methods improve
downstream performance stability without requiring any changes to the training
procedure.

</details>


### [265] [When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA](https://arxiv.org/abs/2510.04849)
*Elisei Rykov,Kseniia Petrushina,Maksim Savkin,Valerii Olisov,Artem Vazhentsev,Kseniia Titova,Alexander Panchenko,Vasily Konovalov,Julia Belikova*

Main category: cs.CL

TL;DR: PsiloQA是一个包含14种语言的跨语言数据集，用于检测大型语言模型的幻觉，其在跨语言泛化和知识迁移方面表现优异，并且比人工标注的数据集更具成本效益。


<details>
  <summary>Details</summary>
Motivation: 幻觉检测是大型语言模型安全部署的关键挑战，现有基准在评估细粒度、多语言幻觉方面存在不足。

Method: 通过GPT-4o生成问答对，从不同的大型语言模型中诱导可能产生幻觉的答案，并利用GPT-4o将幻觉片段与黄金答案和检索到的上下文进行比较，从而自动标注幻觉片段。

Result: 在PsiloQA数据集上，基于编码器的模型在幻觉检测任务上表现最佳，并且该数据集在跨语言泛化和知识迁移方面显示出有效性。

Conclusion: PsiloQA数据集的构建和评估推动了可扩展、细粒度的多语言幻觉检测技术的发展。

Abstract: Hallucination detection remains a fundamental challenge for the safe and
reliable deployment of large language models (LLMs), especially in applications
requiring factual accuracy. Existing hallucination benchmarks often operate at
the sequence level and are limited to English, lacking the fine-grained,
multilingual supervision needed for a comprehensive evaluation. In this work,
we introduce PsiloQA, a large-scale, multilingual dataset annotated with
span-level hallucinations across 14 languages. PsiloQA is constructed through
an automated three-stage pipeline: generating question-answer pairs from
Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse
LLMs in a no-context setting, and automatically annotating hallucinated spans
using GPT-4o by comparing against golden answers and retrieved context. We
evaluate a wide range of hallucination detection methods -- including
uncertainty quantification, LLM-based tagging, and fine-tuned encoder models --
and show that encoder-based models achieve the strongest performance across
languages. Furthermore, PsiloQA demonstrates effective cross-lingual
generalization and supports robust knowledge transfer to other benchmarks, all
while being significantly more cost-efficient than human-annotated datasets.
Our dataset and results advance the development of scalable, fine-grained
hallucination detection in multilingual settings.

</details>


### [266] [Detecting Distillation Data from Reasoning Models](https://arxiv.org/abs/2510.04850)
*Hengxiang Zhang,Hyeong Kyu Choi,Yixuan Li,Hongxin Wei*

Main category: cs.CL

TL;DR: 推理蒸馏可能导致基准测试污染，本文提出了“令牌概率偏差”（TBD）方法来检测蒸馏数据，该方法通过分析生成令牌的概率模式来区分可见和不可见问题，并在实验中取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 推理蒸馏虽然能增强大型语言模型的推理能力，但可能导致基准测试污染，从而虚高模型性能。

Method: 提出一种名为“令牌概率偏差”（TBD）的新方法，该方法通过分析生成令牌的概率模式来检测蒸馏数据。其原理是蒸馏模型对于见过的（seen）问题倾向于生成接近确定性的令牌，而对于没见过的（unseen）问题则生成较低概率的令牌。TBD量化生成令牌的概率与高参考概率的偏差，以区分见过的和没见过的问题。

Result: TBD方法在S1数据集上取得了0.918的AUC和0.470的TPR@1% FPR，证明了其有效性。

Conclusion: TBD是一种有效的方法，可以检测推理蒸馏是否导致了基准测试污染。

Abstract: Reasoning distillation has emerged as an efficient and powerful paradigm for
enhancing the reasoning capabilities of large language models. However,
reasoning distillation may inadvertently cause benchmark contamination, where
evaluation data included in distillation datasets can inflate performance
metrics of distilled models. In this work, we formally define the task of
distillation data detection, which is uniquely challenging due to the partial
availability of distillation data. Then, we propose a novel and effective
method Token Probability Deviation (TBD), which leverages the probability
patterns of the generated output tokens. Our method is motivated by the
analysis that distilled models tend to generate near-deterministic tokens for
seen questions, while producing more low-probability tokens for unseen
questions. Our key idea behind TBD is to quantify how far the generated tokens'
probabilities deviate from a high reference probability. In effect, our method
achieves competitive detection performance by producing lower scores for seen
questions than for unseen questions. Extensive experiments demonstrate the
effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of
0.470 on the S1 dataset.

</details>


### [267] [SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests](https://arxiv.org/abs/2510.04891)
*Punya Syon Pandey,Hai Son Le,Devansh Bhardwaj,Rada Mihalcea,Zhijing Jin*

Main category: cs.CL

TL;DR: LLMs在政治敏感环境中存在安全漏洞，易被用于政治操纵、宣传和虚假信息生成。现有的安全基准测试不足以发现这些问题。研究提出了SocialHarmBench数据集，用于评估LLMs在这些方面的表现。结果显示，开放权重模型（如Mistral-7B）在此类攻击下表现脆弱，成功率高达97%-98%。模型在处理21世纪或20世纪前的内容，以及与拉丁美洲、美国、英国相关的内容时尤为脆弱。现有安全措施无法有效应对高风险的社会政治环境，暴露了系统性偏见，并对LLMs在维护人权和民主方面的可靠性提出了担忧。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs安全基准测试未能充分评估模型在政治操纵、宣传、虚假信息生成、监视和信息控制等社会政治敏感领域中的脆弱性。因此，有必要开发一个专门的数据集来揭示LLMs在这些高风险环境下的缺陷。

Method: 构建了一个包含585个提示的数据集SocialHarmBench，涵盖7个社会政治类别和34个国家，旨在发现LLMs在政治敏感语境下的失效情况。通过在这些提示上评估LLMs，特别是开放权重模型，分析其在不同领域、时间段和地区的脆弱性。

Result: 开放权重模型在政治敏感领域表现出高度的脆弱性，Mistral-7B模型在历史修正主义、宣传和政治操纵等领域的攻击成功率高达97%-98%。LLMs在处理21世纪或20世纪前的内容，以及与拉丁美洲、美国、英国相关的内容时最为脆弱。

Conclusion: 当前LLMs的安全防护措施未能有效推广到高风险的社会政治环境，暴露了系统性偏见。这引发了对LLMs在维护人权和民主价值观方面的可靠性的担忧。研究提出的SocialHarmBench数据集为进一步评估和改进LLMs的安全性提供了工具。

Abstract: Large language models (LLMs) are increasingly deployed in contexts where
their failures can have direct sociopolitical consequences. Yet, existing
safety benchmarks rarely test vulnerabilities in domains such as political
manipulation, propaganda and disinformation generation, or surveillance and
information control. We introduce SocialHarmBench, a dataset of 585 prompts
spanning 7 sociopolitical categories and 34 countries, designed to surface
where LLMs most acutely fail in politically charged contexts. Our evaluations
reveal several shortcomings: open-weight models exhibit high vulnerability to
harmful compliance, with Mistral-7B reaching attack success rates as high as
97% to 98% in domains such as historical revisionism, propaganda, and political
manipulation. Moreover, temporal and geographic analyses show that LLMs are
most fragile when confronted with 21st-century or pre-20th-century contexts,
and when responding to prompts tied to regions such as Latin America, the USA,
and the UK. These findings demonstrate that current safeguards fail to
generalize to high-stakes sociopolitical settings, exposing systematic biases
and raising concerns about the reliability of LLMs in preserving human rights
and democratic values. We share the SocialHarmBench benchmark at
https://huggingface.co/datasets/psyonp/SocialHarmBench.

</details>


### [268] [Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment](https://arxiv.org/abs/2510.04919)
*Davood Rafiei,Morgan Lindsay Heisler,Weiwei Zhang,Mohammadreza Pourreza,Yong Zhang*

Main category: cs.CL

TL;DR: 数据集结构对齐可以预测自然语言到SQL任务的微调效果。


<details>
  <summary>Details</summary>
Motivation: 研究数据集与目标查询结构特征的匹配度如何影响大语言模型在自然语言到SQL任务上的泛化能力。

Method: 通过比较训练集、目标数据集和模型微调前预测结果的SQL结构特征分布来估计数据集的结构对齐度，并进行实验验证。

Result: 实验表明，结构对齐度是预测微调成功的有力指标。当对齐度高时，微调能显著提升模型准确率和SQL生成质量；当对齐度低时，提升效果不明显。

Conclusion: 在自然语言到SQL任务中，选择与目标查询结构对齐的数据集对于有效的微调和模型泛化至关重要。

Abstract: Supervised Fine-Tuning (SFT) is an effective method for adapting Large
Language Models (LLMs) on downstream tasks. However, variability in training
data can hinder a model's ability to generalize across domains. This paper
studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or
text to SQL), examining how well SFT training data matches the structural
characteristics of target queries and how this alignment impacts model
performance. We hypothesize that alignment can be accurately estimated by
comparing the distributions of structural SQL features across the training set,
target data, and the model's predictions prior to SFT. Through comprehensive
experiments on three large cross-domain NL2SQL benchmarks and multiple model
families, we show that structural alignment is a strong predictor of
fine-tuning success. When alignment is high, SFT yields substantial gains in
accuracy and SQL generation quality; when alignment is low, improvements are
marginal or absent. These findings highlight the importance of alignment-aware
data selection for effective fine-tuning and generalization in NL2SQL tasks.

</details>


### [269] [A First Context-Free Grammar Applied to Nawatl Corpora Augmentation](https://arxiv.org/abs/2510.04945)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Ligia Quintana-Torres,Martha-Lorena Avendaño-Garrido,Graham Ranger*

Main category: cs.CL

TL;DR: 本文提出了一种用于纳瓦特尔语的上下文无关文法（CFG），旨在生成语法正确的合成句子以扩充语料库，用于训练语言模型。


<details>
  <summary>Details</summary>
Motivation: 纳瓦特尔语（Nawatl）是一种数字资源稀缺的语言，缺乏可用于机器学习的语料库。本文的目标是通过生成人工合成的、语法正确的句子来显著增加可用语料库的数量，以支持语言模型训练。

Method: 本文构建了一个上下文无关文法（CFG）来描述纳瓦特尔语的结构，并利用该文法生成了大量的合成句子，形成了一个名为“π-yalli”的扩充语料库。

Result: 通过扩充后的语料库训练的 FastText 等算法在句子级语义任务上取得了初步的改进。然而，与一些大型语言模型（LLMs）相比，改进的幅度尚不显著，表明需要更有效的语法模型来进一步提升效果。

Conclusion: 虽然本文提出的上下文无关文法能够扩充纳瓦特尔语的语料库并带来初步的改进，但要实现更显著的性能提升，仍需开发更精细、更有效的纳瓦特尔语语法模型。

Abstract: In this article we introduce a context-free grammar (CFG) for the Nawatl
language. Nawatl (or Nahuatl) is an Amerindian language of the $\pi$-language
type, i.e. a language with few digital resources, in which the corpora
available for machine learning are virtually non-existent. The objective here
is to generate a significant number of grammatically correct artificial
sentences, in order to increase the corpora available for language model
training. We want to show that a grammar enables us significantly to expand a
corpus in Nawatl which we call $\pi$-\textsc{yalli}. The corpus, thus enriched,
enables us to train algorithms such as FastText and to evaluate them on
sentence-level semantic tasks. Preliminary results show that by using the
grammar, comparative improvements are achieved over some LLMs. However, it is
observed that to achieve more significant improvement, grammars that model the
Nawatl language even more effectively are required.

</details>


### [270] [AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives](https://arxiv.org/abs/2510.04983)
*Khalid Mehtab Khan,Anagha Kulkarni*

Main category: cs.CL

TL;DR: AWARE框架通过提升模型对学生反思文本的领域、上下文和类别重叠的意识，在识别文化资本主题方面优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 识别学生反思中的文化资本主题有助于营造公平的教学环境，但标准NLP模型难以检测这些主题，因为它们常常融入叙事而非直接作为关键词出现。

Method: 提出AWARE框架，包含三个核心组件：1）领域意识（使模型词汇适应学生反思的语言风格）；2）上下文意识（生成考虑全文上下文的句子嵌入）；3）类别重叠意识（采用多标签策略识别单句中主题的共存）。

Result: AWARE框架在Macro-F1指标上比强基线模型提高了2.1个百分点，并在所有主题上均有显著改进。

Conclusion: AWARE提供了一种稳健且可推广的方法，适用于任何依赖叙事上下文含义的文本分类任务。

Abstract: Identifying cultural capital (CC) themes in student reflections can offer
valuable insights that help foster equitable learning environments in
classrooms. However, themes such as aspirational goals or family support are
often woven into narratives, rather than appearing as direct keywords. This
makes them difficult to detect for standard NLP models that process sentences
in isolation. The core challenge stems from a lack of awareness, as standard
models are pre-trained on general corpora, leaving them blind to the
domain-specific language and narrative context inherent to the data. To address
this, we introduce AWARE, a framework that systematically attempts to improve a
transformer model's awareness for this nuanced task. AWARE has three core
components: 1) Domain Awareness, adapting the model's vocabulary to the
linguistic style of student reflections; 2) Context Awareness, generating
sentence embeddings that are aware of the full essay context; and 3) Class
Overlap Awareness, employing a multi-label strategy to recognize the
coexistence of themes in a single sentence. Our results show that by making the
model explicitly aware of the properties of the input, AWARE outperforms a
strong baseline by 2.1 percentage points in Macro-F1 and shows considerable
improvements across all themes. This work provides a robust and generalizable
methodology for any text classification task in which meaning depends on the
context of the narrative.

</details>


### [271] [Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.05003)
*Imran Mansha*

Main category: cs.CL

TL;DR: 本文提出了一种资源高效的微调方法，使用LoRA和QLoRA等参数高效微调技术，在有限的GPU和内存条件下，对LLaMA-3.2-3B模型进行微调，以增强其在医疗链式思考推理方面的能力，并在降低内存占用的同时提高了推理的连贯性和事实准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理方面表现出色，但微调需要大量计算资源。本文旨在探索一种资源高效的微调方法，以适应GPU和内存受限的研究环境，并增强LLM在医疗领域的推理能力。

Method: 采用参数高效微调技术，如LoRA和QLoRA，在公开的医疗推理数据集上对LLaMA-3.2-3B模型进行微调。

Result: 微调后的模型提高了推理的连贯性和事实准确性，同时内存使用量相比标准全量微调减少高达60%。实验证明，轻量级微调能够保持模型在医疗问答任务中的强大推理能力。

Conclusion: 轻量级微调策略能够有效地在资源受限的研究环境中部署LLMs，并实现了效率和领域专业化之间的平衡，为医疗AI系统的发展提供了见解。

Abstract: Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated
remarkable reasoning abilities but require significant computational resources
for fine-tuning. This paper presents a resource-efficient fine-tuning approach
for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating
under constrained GPU and memory settings. Using parameter-efficient tuning
techniques such as LoRA and QLoRA, we adapt the base model on publicly
available medical reasoning datasets. The model achieves improved reasoning
coherence and factual accuracy while reducing memory usage by up to 60%
compared to standard full fine-tuning. Experimental evaluation demonstrates
that lightweight adaptations can retain strong reasoning capability in medical
question-answering tasks. This work highlights practical strategies for
deploying LLMs in low-resource research environments and provides insights into
balancing efficiency and domain specialization for medical AI systems.

</details>


### [272] [Imperceptible Jailbreaking against Large Language Models](https://arxiv.org/abs/2510.05025)
*Kuofeng Gao,Yiming Li,Chao Du,Xin Wang,Xingjun Ma,Shu-Tao Xia,Tianyu Pang*

Main category: cs.CL

TL;DR: 该研究提出了一种利用Unicode字符变体选择器实现不可感知越狱攻击的方法，使得恶意提示在视觉上与原始提示无异，但其标记化被秘密改变，从而诱导有害响应。


<details>
  <summary>Details</summary>
Motivation: 通常认为文本模态的越狱攻击需要可见的修改，而本研究旨在探索和实现对齐语言模型（LLMs）的不可感知越狱攻击。

Method: 通过附加不可见的Unicode变体选择器到恶意问题上，改变其标记化，并提出了一种链式搜索管道来生成这种对抗性后缀。

Result: 实验证明，所提出的不可感知越狱攻击在四个对齐的LLMs上取得了高攻击成功率，并且能够泛化到提示注入攻击，而不会在书面提示中产生任何可见修改。

Conclusion: 不可感知越狱攻击是可行的，并且能够有效地绕过对齐语言模型的安全防护，而无需进行任何可见的修改。

Abstract: Jailbreaking attacks on the vision modality typically rely on imperceptible
adversarial perturbations, whereas attacks on the textual modality are
generally assumed to require visible modifications (e.g., non-semantic
suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a
class of Unicode characters called variation selectors. By appending invisible
variation selectors to malicious questions, the jailbreak prompts appear
visually identical to original malicious questions on screen, while their
tokenization is "secretly" altered. We propose a chain-of-search pipeline to
generate such adversarial suffixes to induce harmful responses. Our experiments
show that our imperceptible jailbreaks achieve high attack success rates
against four aligned LLMs and generalize to prompt injection attacks, all
without producing any visible modifications in the written prompt. Our code is
available at https://github.com/sail-sg/imperceptible-jailbreaks.

</details>


### [273] [A Set of Quebec-French Corpus of Regional Expressions and Terms](https://arxiv.org/abs/2510.05026)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: 提出结合习语理解和方言理解，并使用区域习语作为方言理解的测试。


<details>
  <summary>Details</summary>
Motivation: 提出结合习语理解和方言理解，并使用区域习语作为方言理解的测试。

Method: 提出两个新的法语魁北克方言基准数据集：QFrCoRE（包含4,633个习语短语）和QFrCoRT（包含171个区域习语词）。

Result: 实验表明，区域习语基准是衡量模型特定方言熟练程度的可靠工具。

Conclusion: 区域习语基准是衡量模型特定方言熟练程度的可靠工具。

Abstract: The tasks of idiom understanding and dialect understanding are both
well-established benchmarks in natural language processing. In this paper, we
propose combining them, and using regional idioms as a test of dialect
understanding. Towards this end, we propose two new benchmark datasets for the
Quebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic
phrases, and QFrCoRT, which comprises 171 regional instances of idiomatic
words. We explain how to construct these corpora, so that our methodology can
be replicated for other dialects. Our experiments with 94 LLM demonstrate that
our regional idiom benchmarks are a reliable tool for measuring a model's
proficiency in a specific dialect.

</details>


### [274] [Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization](https://arxiv.org/abs/2510.05038)
*Omri Uzan,Asaf Yehudai,Roi pony,Eyal Shnarch,Ariel Gera*

Main category: cs.CL

TL;DR: 通过引入引导查询精炼（GQR）方法，本研究提出了一种新的测试时间优化技术，利用互补检索器的分数来指导和精炼主要检索器的查询嵌入，从而在保持高性能的同时显著提高视觉文档检索的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态编码器虽然在视觉文档检索方面表现出色，但其巨大的表示尺寸给实际应用带来了部署和可扩展性方面的挑战。此外，纯粹的视觉中心方法可能受到视觉-语言模型中存在的模态差距的限制。本研究旨在探索混合检索范式，通过引入轻量级密集文本检索器来增强强大的视觉中心模型，以解决这些问题。

Method: 提出了一种名为引导查询精炼（GQR）的新型测试时间优化方法。该方法通过使用一个互补检索器（如轻量级密集文本检索器）提供的分数来指导和精炼另一个主要检索器（如视觉中心模型）的查询嵌入，从而实现表示空间的丰富交互。

Result: 在视觉文档检索基准测试上的广泛实验表明，GQR 使得视觉中心模型能够匹配具有显著更大表示的模型，同时速度提高 14 倍，内存消耗减少 54 倍。这表明 GQR 有效地推动了多模态检索在性能和效率方面的帕累托前沿。

Conclusion: 引导查询精炼（GQR）是一种有效的测试时间优化技术，它通过利用互补检索器的信息来精炼主要检索器的查询嵌入，从而在提高效率和可扩展性的同时，保持甚至超越了大型模型在视觉文档检索任务上的性能。这为构建更高效、可扩展的多模态检索系统提供了新的途径。

Abstract: Multimodal encoders have pushed the boundaries of visual document retrieval,
matching textual query tokens directly to image patches and achieving
state-of-the-art performance on public benchmarks. Recent models relying on
this paradigm have massively scaled the sizes of their query and document
representations, presenting obstacles to deployment and scalability in
real-world pipelines. Furthermore, purely vision-centric approaches may be
constrained by the inherent modality gap still exhibited by modern
vision-language models. In this work, we connect these challenges to the
paradigm of hybrid retrieval, investigating whether a lightweight dense text
retriever can enhance a stronger vision-centric model. Existing hybrid methods,
which rely on coarse-grained fusion of ranks or scores, fail to exploit the
rich interactions within each model's representation space. To address this, we
introduce Guided Query Refinement (GQR), a novel test-time optimization method
that refines a primary retriever's query embedding using guidance from a
complementary retriever's scores. Through extensive experiments on visual
document retrieval benchmarks, we demonstrate that GQR allows vision-centric
models to match the performance of models with significantly larger
representations, while being up to 14x faster and requiring 54x less memory.
Our findings show that GQR effectively pushes the Pareto frontier for
performance and efficiency in multimodal retrieval. We release our code at
https://github.com/IBM/test-time-hybrid-retrieval

</details>


### [275] [COLE: a Comprehensive Benchmark for French Language Understanding Evaluation](https://arxiv.org/abs/2510.05046)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: COLE是一个新的法语自然语言理解（NLU）评估基准，包含23个任务，评估了94个大语言模型，揭示了开放权重模型和封闭模型之间的性能差距，并指出了当前模型在零样本抽取式问答、细粒度词义消歧和地区语言变体理解方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 为了更全面地评估法语自然语言理解（NLU）能力，需要一个新的基准。

Method: 创建了一个名为COLE的新基准，包含23个多样化的任务，涵盖了广泛的NLU能力，并针对94个大语言模型进行了测试。

Result: 结果显示，封闭权重模型和开放权重模型之间存在显著的性能差距。当前的大型语言模型在零样本抽取式问答、细粒度词义消歧和理解地区语言变体等方面存在挑战。

Conclusion: COLE基准的发布旨在促进法语语言模型的进一步发展。

Abstract: To address the need for a more comprehensive evaluation of French Natural
Language Understanding (NLU), we introduce COLE, a new benchmark composed of 23
diverse task covering a broad range of NLU capabilities, including sentiment
analysis, paraphrase detection, grammatical judgment, and reasoning, with a
particular focus on linguistic phenomena relevant to the French language. We
benchmark 94 large language models (LLM), providing an extensive analysis of
the current state of French NLU. Our results highlight a significant
performance gap between closed- and open-weights models and identify key
challenging frontiers for current LLMs, such as zero-shot extractive
question-answering (QA), fine-grained word sense disambiguation, and
understanding of regional language variations. We release COLE as a public
resource to foster further progress in French language modelling.

</details>


### [276] [SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs](https://arxiv.org/abs/2510.05069)
*Dachuan Shi,Abedelkadir Asi,Keying Li,Xiangchi Yuan,Leyan Pan,Wenke Lee,Wen Xiao*

Main category: cs.CL

TL;DR: SwiReasoning是一种无需训练的框架，通过结合显式和隐式推理来提高LLM在数学和STEM基准上的准确性和效率，并限制过度思考。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）虽然可以通过隐式推理在潜在空间中进行推理，但仍面临搜索分布扩散、准确性下降和过度思考等挑战，尤其是在不进行训练的情况下。因此，需要一种新的方法来解决这些问题。

Method: SwiReasoning通过动态切换显式和隐式推理，并基于下一个标记分布中的熵趋势估计块级置信度来指导这种切换。它还通过限制思考块切换的最大次数来控制过度思考，并提高代币效率。

Result: 在数学和STEM基准测试中，SwiReasoning将准确性平均提高了1.5%-2.8%，在代币限制下，代币效率平均提高了56%-79%。

Conclusion: SwiReasoning通过结合显式和隐式推理，并采用置信度引导和切换次数限制策略，有效地解决了LLM在训练期间的推理挑战，从而提高了准确性和代币效率。

Abstract: Recent work shows that, beyond discrete reasoning through explicit
chain-of-thought steps, which are limited by the boundaries of natural
languages, large language models (LLMs) can also reason continuously in latent
space, allowing richer information per step and thereby improving token
efficiency. Despite this promise, latent reasoning still faces two challenges,
especially in training-free settings: 1) purely latent reasoning broadens the
search distribution by maintaining multiple implicit paths, which diffuses
probability mass, introduces noise, and impedes convergence to a single
high-confidence solution, thereby hurting accuracy; and 2) overthinking
persists even without explicit text, wasting tokens and degrading efficiency.
To address these issues, we introduce SwiReasoning, a training-free framework
for LLM reasoning which features two key innovations: 1) SwiReasoning
dynamically switches between explicit and latent reasoning, guided by
block-wise confidence estimated from entropy trends in next-token
distributions, to balance exploration and exploitation and promote timely
convergence. 2) By limiting the maximum number of thinking-block switches,
SwiReasoning curbs overthinking and improves token efficiency across varying
problem difficulties. On widely used mathematics and STEM benchmarks,
SwiReasoning consistently improves average accuracy by 1.5%-2.8% across
reasoning LLMs of different model families and scales. Furthermore, under
constrained budgets, SwiReasoning improves average token efficiency by 56%-79%,
with larger gains as budgets tighten.

</details>


### [277] [Slm-mux: Orchestrating small language models for reasoning](https://arxiv.org/abs/2510.05077)
*Chenyu Wang,Zishen Wan,Hao Kang,Emma Chen,Zhiqiang Xie,Tushar Krishna,Vijay Janapa Reddi,Yilun Du*

Main category: cs.CL

TL;DR: 通过SLM-MUX系统，可以有效地将多个小型语言模型（SLMs）组合起来，以提高在特定任务上的准确性，并且优于单独的模型。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型的发展，小型语言模型（SLMs）数量显著增加。尽管它们在准确性上不如前沿模型，但效率更高且擅长特定任务。因此，提出一个问题：能否将多个SLMs组合成一个系统，使其整体表现优于任何单独的模型？现有的方法主要针对前沿模型，应用于SLMs时效果不佳，存在研究空白。

Method: 提出一个三阶段的SLM编排方法。首先，引入SLM-MUX，一个能有效协调多个SLMs的多模型架构。在此基础上，开发了两种优化策略：(i) 模型选择搜索，用于从给定池中识别最互补的SLMs；(ii) 针对SLM-MUX的测试时间缩放。

Result: 与现有编排方法相比，该方法在MATH上提高了13.4%，在GPQA上提高了8.8%，在GSM8K上提高了7.0%。仅使用两个SLMs，SLM-MUX在GPQA和GSM8K上就优于Qwen 2.5 72B，并在MATH上与其性能相当。通过理论分析进一步证明了该方法的优势。

Conclusion: 通过所提出的方法，SLMs可以被有效地编排成更准确、更高效的系统。

Abstract: With the rapid development of language models, the number of small language
models (SLMs) has grown significantly. Although they do not achieve
state-of-the-art accuracy, they are more efficient and often excel at specific
tasks. This raises a natural question: can multiple SLMs be orchestrated into a
system where each contributes effectively, achieving higher accuracy than any
individual model? Existing orchestration methods have primarily targeted
frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To
address this gap, we propose a three-stage approach for orchestrating SLMs.
First, we introduce SLM-MUX, a multi-model architecture that effectively
coordinates multiple SLMs. Building on this, we develop two optimization
strategies: (i) a model selection search that identifies the most complementary
SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our
approach delivers strong results: Compared to existing orchestration methods,
our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%
on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and
GSM8K, and matches its performance on MATH. We further provide theoretical
analyses to substantiate the advantages of our method. In summary, we
demonstrate that SLMs can be effectively orchestrated into more accurate and
efficient systems through the proposed approach.

</details>


### [278] [TeachLM: Post-Training LLMs for Education Using Authentic Learning Data](https://arxiv.org/abs/2510.05087)
*Janos Perczel,Jin Chow,Dorottya Demszky*

Main category: cs.CL

TL;DR: TeachLM是一个针对教学进行优化的LLM，通过参数高效微调来解决生成式AI在教育领域的局限性。它使用真实的教学互动数据进行训练，能够生成高质量的合成师生对话，并提供可扩展的评估方法，显著提高了LLM的对话和教学能力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在教育领域的应用受到LLM教学能力局限的制约，主要是因为缺乏高质量的真实学生学习训练数据。目前的提示工程方法受限于自然语言编码复杂教学策略的能力。

Method: TeachLM通过参数高效微调（PEFT）优化了最先进的模型。它使用了Polygence提供的10万小时一对一、纵向学生-导师互动数据集进行训练，并经过严格的匿名化处理。在此基础上，开发了一个真实的“学生模型”，用于生成高保真的合成师生对话。此外，还提出了一种新颖的多轮评估协议，利用合成对话生成来快速、可扩展且可重复地评估LLM的对话能力。

Result: TeachLM的评估结果显示，在真实学习数据上进行微调显著提高了模型的对话和教学性能。具体表现在：学生发言时间翻倍，提问方式得到改善，对话轮次增加50%，以及教学个性化程度提高。

Conclusion: 通过在真实的师生互动数据上进行参数高效微调，TeachLM能够显著提升LLM在教育场景中的对话和教学能力，克服了现有方法的局限性，并提供了一种有效的评估方法。

Abstract: The promise of generative AI to revolutionize education is constrained by the
pedagogical limits of large language models (LLMs). A major issue is the lack
of access to high-quality training data that reflect the learning of actual
students. Prompt engineering has emerged as a stopgap, but the ability of
prompts to encode complex pedagogical strategies in rule-based natural language
is inherently limited. To address this gap we introduce TeachLM - an LLM
optimized for teaching through parameter-efficient fine-tuning of
state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000
hours of one-on-one, longitudinal student-tutor interactions maintained by
Polygence, which underwent a rigorous anonymization process to protect privacy.
We use parameter-efficient fine-tuning to develop an authentic student model
that enables the generation of high-fidelity synthetic student-tutor dialogues.
Building on this capability, we propose a novel multi-turn evaluation protocol
that leverages synthetic dialogue generation to provide fast, scalable, and
reproducible assessments of the dialogical capabilities of LLMs. Our
evaluations demonstrate that fine-tuning on authentic learning data
significantly improves conversational and pedagogical performance - doubling
student talk time, improving questioning style, increasing dialogue turns by
50%, and greater personalization of instruction.

</details>


### [279] [Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models](https://arxiv.org/abs/2510.05090)
*Runchu Tian,Junxia Cui,Xueqiang Xu,Feng Yao,Jingbo Shang*

Main category: cs.CL

TL;DR: Tolerator是一种创新的无训练解码策略，通过交叉验证和迭代细化解决了离散扩散大语言模型（dLLMs）中早期错误固化的问题，并在多项任务上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 为了解决离散扩散大语言模型（dLLMs）在解码过程中早期接受的 token 无法修正，导致错误累积并影响最终输出质量的问题。

Method: 提出了一种名为Tolerator（Token-Level Cross-Validation Refinement）的无训练解码策略。该策略采用两阶段过程：首先进行序列填充，然后通过重新遮掩（reskaming）和解码部分 token，同时将剩余 token 作为上下文进行迭代细化，从而允许对先前接受的 token 进行重新审视和修正。

Result: 在语言理解、代码生成和数学等五个标准基准测试中，Tolerator 相比现有基线方法在相同的计算资源下取得了持续的改进。

Conclusion: Tolerator 策略能够有效解决 dLLMs 解码过程中的错误固化问题，提高输出质量，并表明解码算法对于充分发挥 dLLMs 的潜力至关重要。

Abstract: Diffusion large language models (dLLMs) have recently emerged as a promising
alternative to autoregressive (AR) models, offering advantages such as
accelerated parallel decoding and bidirectional context modeling. However, the
vanilla decoding strategy in discrete dLLMs suffers from a critical limitation:
once a token is accepted, it can no longer be revised in subsequent steps. As a
result, early mistakes persist across iterations, harming both intermediate
predictions and final output quality. To address this issue, we propose
Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding
strategy that leverages cross-validation among predicted tokens. Unlike
existing methods that follow a single progressive unmasking procedure,
Tolerator introduces a two-stage process: (i) sequence fill-up and (ii)
iterative refinement by remasking and decoding a subset of tokens while
treating the remaining as context. This design enables previously accepted
tokens to be reconsidered and corrected when necessary, leading to more
reliable diffusion decoding outputs. We evaluate Tolerator on five standard
benchmarks covering language understanding, code generation, and mathematics.
Experiments show that our method achieves consistent improvements over the
baselines under the same computational budget. These findings suggest that
decoding algorithms are crucial to realizing the full potential of diffusion
large language models. Code and data are publicly available.

</details>


### [280] [ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization](https://arxiv.org/abs/2505.02819)
*Dmitriy Shopkhoev,Ammar Ali,Magauiya Zhussip,Valentin Malykh,Stamatios Lefkimmiatis,Nikos Komodakis,Sergey Zagoruyko*

Main category: cs.CL

TL;DR: ReplaceMe通过用线性操作替换Transformer块来实现免训练深度修剪，在低压缩率下保持高性能，并优于其他免训练方法。


<details>
  <summary>Details</summary>
Motivation: 提出一种无需额外训练或微调的通用训练免费深度修剪方法，以替换Transformer块并保持高性能。

Method: 使用小的校准数据集来估计一个近似被修剪块的线性变换，该变换可以与剩余的Transformer块无缝合并。

Result: 在LLM上实现了高达25%的修剪，同时保留了约90%的原始模型性能，优于其他训练免费方法，并与最先进的微调方法具有竞争力。

Conclusion: ReplaceMe是一种有效的免训练深度修剪方法，通过用线性操作替换Transformer块，在不进行任何训练或修复的情况下，在保持高性能的同时实现显著的模型压缩。

Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that
effectively replaces transformer blocks with a linear operation, while
maintaining high performance for low compression ratios. In contrast to
conventional pruning approaches that require additional training or
fine-tuning, our approach requires only a small calibration dataset that is
used to estimate a linear transformation, which approximates the pruned blocks.
The estimated linear mapping can be seamlessly merged with the remaining
transformer blocks, eliminating the need for any additional network parameters.
Our experiments show that ReplaceMe consistently outperforms other
training-free approaches and remains highly competitive with state-of-the-art
pruning methods that involve extensive retraining/fine-tuning and architectural
modifications. Applied to several large language models (LLMs), ReplaceMe
achieves up to 25% pruning while retaining approximately 90% of the original
model's performance on open benchmarks - without any training or healing steps,
resulting in minimal computational overhead (see Fig.1). We provide an
open-source library implementing ReplaceMe alongside several state-of-the-art
depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.

</details>


### [281] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: MASA通过跨层共享注意力矩阵的原子来压缩Transformer模型，在保持性能的同时显著减少了参数量。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）压缩技术主要关注块内优化，忽略了Transformer层之间存在的冗余。MASA旨在利用Transformer的结构特性，通过跨层权重共享来解决这个问题。

Method: MASA框架将注意力投影矩阵分解为共享的字典原子。模型的权重表示为这些共享原子的线性组合，从而实现参数共享和模型压缩。该方法可以作为现有模型的即插即用替换件，并使用标准优化器进行训练。

Result: MASA在不同规模的模型（1亿到7亿参数）上进行了实验，结果显示其在基准测试的准确性和困惑度方面优于GQA、低秩基线和Repeat-all-over/Sequential共享方法，同时参数量相当。MASA在视觉Transformer（ViT）上也取得了相似的性能，并将注意力参数减少了66.7%。此外，MASA在预训练的LLM上表现良好，参数量减少而性能无显著下降。

Conclusion: MASA是一种有效的参数压缩方法，通过借鉴字典学习的策略，实现了Transformer层之间注意力权重的共享。该方法能够在不牺牲性能的情况下显著减少模型参数量，为构建高效的大型语言模型提供了一个可扩展的蓝图，并且可以应用于预训练模型。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [282] [Generalization and the Rise of System-level Creativity in Science](https://arxiv.org/abs/2510.03240)
*Hongbo Fang,James Evans*

Main category: cs.SI

TL;DR: 跨领域综合与模块化创新正成为科学进步的主要驱动力，并对科学政策产生影响。


<details>
  <summary>Details</summary>
Motivation: 科技创新生态系统需要政策引导以促进人类健康、福祉、安全和繁荣。现有研究对创新的分解和衡量方式存在不足。

Method: 开发新的指标，能够可靠地分解创新的影响力，区分其是领域基础性工作、基础性工作的延伸，还是跨越遥远领域进行综合与模块化的推广性工作，以催化组合式创新。利用2300万份科学文献进行实证分析。

Result: 在过去几年中，领域内的基础性和延伸性工作有所下降，但跨领域综合与模块化的工作显著增加并加速发展，这与网络的普及、社交媒体和人工智能的兴起有关。创新的焦点正从领域内部转向整个系统。

Conclusion: 跨领域综合与模块化创新已成为推动创新的主要模式，对科学政策的制定提出了新的要求和重要的考量。

Abstract: Innovation ecosystems require careful policy stewardship to drive sustained
advance in human health, welfare, security and prosperity. We develop new
measures that reliably decompose the influence of innovations in terms of the
degree to which each represents a field-level foundation, an extension of
foundational work, or a generalization that synthesizes and modularizes
contributions from distant fields to catalyze combinatorial innovation. Using
23 million scientific works, we demonstrate that while foundational and
extensional work within fields has declined in recent years-a trend garnering
much recent attention-generalizations across fields have increased and
accelerated with the rise of the web, social media, and artificial
intelligence, shifting the locus of innovation from within fields to across the
system as a whole. We explore implications for science policy.

</details>


### [283] [Fair Minimum Labeling: Efficient Temporal Network Activations for Reachability and Equity](https://arxiv.org/abs/2510.03899)
*Lutz Oettershagen,Othon Michail*

Main category: cs.SI

TL;DR: 该论文提出了一种名为公平最小标签（FML）的新问题，旨在解决网络系统中资源效率和公平性之间的平衡，特别是在支持现代学习应用方面。FML问题旨在设计一个成本最小的时间边激活计划，以满足每个节点组的覆盖要求，确保它们能够充分访问指定的目标集。


<details>
  <summary>Details</summary>
Motivation: 在具有成本效益和公平访问要求的网络系统中，例如分布式数据收集、边缘-云系统中的更新传播以及关键基础设施中的公平服务恢复，需要在资源消耗和公平性之间取得平衡。

Method: 研究表明FML问题是NP难的，并且难以近似（$oldsymbol{alse}$) 。论文提出了一种概率近似算法，该算法在激活成本方面达到了最佳可能保证，并匹配了已知的近似难度界限。

Result: 所提出的概率近似算法在激活成本方面实现了$oldsymbol{alse(}oldsymbol{alse}{	ext{|}}V|oldsymbol{false)}oldsymbol{alse(}$的近似比，这与理论下界一致。实验结果表明，与基线方法相比，FML在强制执行组级别公平性方面显著降低了激活成本。

Conclusion: FML问题及其提出的算法对于在学习集成网络中构建资源高效、公平的时间可达性具有实际应用价值。实验证明了其在公平多源数据聚合任务中的有效性，能够以更低的成本实现公平性。

Abstract: Balancing resource efficiency and fairness is critical in networked systems
that support modern learning applications. We introduce the Fair Minimum
Labeling (FML) problem: the task of designing a minimum-cost temporal edge
activation plan that ensures each group of nodes in a network has sufficient
access to a designated target set, according to specified coverage
requirements. FML captures key trade-offs in systems where edge activations
incur resource costs and equitable access is essential, such as distributed
data collection, update dissemination in edge-cloud systems, and fair service
restoration in critical infrastructure. We show that FML is NP-hard and
$\Omega(\log |V|)$-hard to approximate, and we present probabilistic
approximation algorithms that match this bound, achieving the best possible
guarantee for the activation cost. We demonstrate the practical utility of FML
in a fair multi-source data aggregation task for training a shared model.
Empirical results show that FML enforces group-level fairness with
substantially lower activation cost than baseline heuristics, underscoring its
potential for building resource-efficient, equitable temporal reachability in
learning-integrated networks.

</details>


### [284] [Deep learning framework for predicting stochastic take-off and die-out of early spreading](https://arxiv.org/abs/2510.04574)
*Wenchao He,Tao Jia*

Main category: cs.SI

TL;DR: 本研究提出了一个深度学习框架，用于预测早期传播事件（如流行病、错误信息）是会演变成重大疫情还是会自然消亡，以便及时干预。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以解决早期疫情爆发的预测问题，因为早期数据稀疏且模型侧重于平均行为而非随机性。

Method: 开发了一个深度学习框架，并结合预训练-微调方法，利用模拟数据进行训练和适应，以预测早期传播事件的扩散结果。

Result: 该框架在不同网络结构和感染性水平下均能准确预测早期传播事件，并且优于基线模型，即使在数据有限的情况下也表现出色。

Conclusion: 本研究首次提出了预测随机传播事件“起飞”或“消亡”的框架，为流行病防范和公共卫生决策提供支持，以制定更有效的早期干预策略。

Abstract: Large-scale outbreaks of epidemics, misinformation, or other harmful
contagions pose significant threats to human society, yet the fundamental
question of whether an emerging outbreak will escalate into a major epidemic or
naturally die out remains largely unaddressed. This problem is challenging,
partially due to inadequate data during the early stages of outbreaks and also
because established models focus on average behaviors of large epidemics rather
than the stochastic nature of small transmission chains. Here, we introduce the
first systematic framework for forecasting whether initial transmission events
will amplify into major outbreaks or fade into extinction during early stages,
when intervention strategies can still be effectively implemented. Using
extensive data from stochastic spreading models, we developed a deep learning
framework that predicts early-stage spreading outcomes in real-time. Validation
across Erd\H{o}s-R\'enyi and Barab\'asi-Albert networks with varying
infectivity levels shows our method accurately forecasts stochastic spreading
events well before potential outbreaks, demonstrating robust performance across
different network structures and infectivity scenarios.To address the challenge
of sparse data during early outbreak stages, we further propose a
pretrain-finetune framework that leverages diverse simulation data for
pretraining and adapts to specific scenarios through targeted fine-tuning. The
pretrain-finetune framework consistently outperforms baseline models, achieving
superior performance even when trained on limited scenario-specific data. To
our knowledge, this work presents the first framework for predicting stochastic
take-off versus die-out. This framework provides valuable insights for epidemic
preparedness and public health decision-making, enabling more informed early
intervention strategies.

</details>


### [285] [Higher-Order Network Structure Inference: A Topological Approach to Network Selection](https://arxiv.org/abs/2510.04884)
*Adam Schroeder,Russell Funk,Jingyi Guan,Taylor Okonek,Lori Ziegelmeier*

Main category: cs.SI

TL;DR: 提出一种基于拓扑数据分析的系统化阈值算法，通过持久性同调计算参数空间中同调特征的稳定性，以识别最优网络参数，并考虑了高阶交互作用。


<details>
  <summary>Details</summary>
Motivation: 现有网络阈值方法在参数选择上依赖启发式方法或试错，且多关注成对关系，忽略了高阶交互作用，导致结果对参数敏感。

Method: 使用持久性同调计算同调特征在参数空间中的稳定性，并允许用户通过超参数指定拓扑特征的最小要求，从而约束参数搜索范围。

Result: 通过对科学计量学中科学概念网络进行的实例分析，证明了该方法的有效性。

Conclusion: 该算法能够识别稳健的网络参数，同时保留有意义的拓扑结构，并可扩展到更广泛的数据分析参数化问题。

Abstract: Thresholding--the pruning of nodes or edges based on their properties or
weights--is an essential preprocessing tool for extracting interpretable
structure from complex network data, yet existing methods face several key
limitations. Threshold selection often relies on heuristic methods or trial and
error due to large parameter spaces and unclear optimization criteria, leading
to sensitivity where small parameter variations produce significant changes in
network structure. Moreover, most approaches focus on pairwise relationships
between nodes, overlooking critical higher-order interactions involving three
or more nodes. We introduce a systematic thresholding algorithm that leverages
topological data analysis to identify optimal network parameters by accounting
for higher-order structural relationships. Our method uses persistent homology
to compute the stability of homological features across the parameter space,
identifying parameter choices that are robust to small variations while
preserving meaningful topological structure. Hyperparameters allow users to
specify minimum requirements for topological features, effectively constraining
the parameter search to avoid spurious solutions. We demonstrate the approach
with an application in the Science of Science, where networks of scientific
concepts are extracted from research paper abstracts, and concepts are
connected when they co-appear in the same abstract. The flexibility of our
approach allows researchers to incorporate domain-specific constraints and
extends beyond network thresholding to general parameterization problems in
data analysis.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [286] [Downside Risk-Aware Equilibria for Strategic Decision-Making](https://arxiv.org/abs/2510.03446)
*Oliver Slumbers,Benjamin Patrick Evans,Sumitra Ganesh,Leo Ardon*

Main category: cs.GT

TL;DR: 基于下行风险的博弈论新概念


<details>
  <summary>Details</summary>
Motivation: 传统博弈论的风险观受限于对其他玩家行为不确定性导致预期回报的影响，近期考虑了回报方差的方差，但未区分上行和下行风险。然而，在金融等领域，只有下行风险（潜在损失）是关键，而上行风险（如利润）则不是问题。

Method: 提出一种新颖的“下行风险意识均衡”（DRAE）概念，基于下部矩。DRAE 限制下行风险，不限制上行风险，并能建模高阶风险偏好。

Result: 在多个博弈的实例中成功应用DRAE，找到了平衡下行风险与预期回报的均衡，并证明了其存在性和最优性。

Conclusion: DRAE 是一种能够有效建模仅关注下行风险的博弈论概念，并在实际应用中证明了其有效性。

Abstract: Game theory has traditionally had a relatively limited view of risk based on
how a player's expected reward is impacted by the uncertainty of the actions of
other players. Recently, a new game-theoretic approach provides a more holistic
view of risk also considering the reward-variance. However, these
variance-based approaches measure variance of the reward on both the upside and
downside. In many domains, such as finance, downside risk only is of key
importance, as this represents the potential losses associated with a decision.
In contrast, large upside "risk" (e.g. profits) are not an issue. To address
this restrictive view of risk, we propose a novel solution concept, downside
risk aware equilibria (DRAE) based on lower partial moments. DRAE restricts
downside risk, while placing no restrictions on upside risk, and additionally,
models higher-order risk preferences. We demonstrate the applicability of DRAE
on several games, successfully finding equilibria which balance downside risk
with expected reward, and prove the existence and optimality of this
equilibria.

</details>


### [287] [On the $O(1/T)$ Convergence of Alternating Gradient Descent-Ascent in Bilinear Games](https://arxiv.org/abs/2510.03855)
*Tianlong Nan,Shuvomoy Das Gupta,Garud Iyengar,Christian Kroer*

Main category: cs.GT

TL;DR: AltGDA算法在双人零和博弈中具有O(1/T)的收敛速度，优于其同步方法。


<details>
  <summary>Details</summary>
Motivation: 虽然交替更新策略在博弈中很实用，但其理论分析有限，尤其是在约束环境下。本研究旨在理论上阐明交替梯度下降-上升（AltGDA）算法在双人零和博弈中的收敛性。

Method: 通过理论分析和性能估计编程（PEP）框架，研究AltGDA算法在存在内部纳什均衡和不存在内部纳什均衡两种情况下的收敛率，并与同步方法进行比较。

Result: 在存在内部纳什均衡的情况下，AltGDA具有O(1/T)的遍历收敛率。在不存在内部纳什均衡的情况下，AltGDA具有O(1/T)的局部收敛率。PEP框架表明AltGDA可能达到O(1/T)的有限时间收敛率，而同步方法仅为O(1/"></script>"。

Conclusion: AltGDA算法在双人零和博弈中，尤其是在约束环境下，相比其同步方法具有理论和实践上的优势。

Abstract: We study the alternating gradient descent-ascent (AltGDA) algorithm in
two-player zero-sum games. Alternating methods, where players take turns to
update their strategies, have long been recognized as simple and practical
approaches for learning in games, exhibiting much better numerical performance
than their simultaneous counterparts. However, our theoretical understanding of
alternating algorithms remains limited, and results are mostly restricted to
the unconstrained setting. We show that for two-player zero-sum games that
admit an interior Nash equilibrium, AltGDA converges at an $O(1/T)$ ergodic
convergence rate when employing a small constant stepsize. This is the first
result showing that alternation improves over the simultaneous counterpart of
GDA in the constrained setting. For games without an interior equilibrium, we
show an $O(1/T)$ local convergence rate with a constant stepsize that is
independent of any game-specific constants. In a more general setting, we
develop a performance estimation programming (PEP) framework to jointly
optimize the AltGDA stepsize along with its worst-case convergence rate. The
PEP results indicate that AltGDA may achieve an $O(1/T)$ convergence rate for a
finite horizon $T$, whereas its simultaneous counterpart appears limited to an
$O(1/\sqrt{T})$ rate.

</details>


### [288] [Robust Optimality of Bundling Goods Beyond Finite Variance](https://arxiv.org/abs/2510.04343)
*Tim S. G. van Eck,Pieter Kleer,Johan S. H. van Leeuwaarden*

Main category: cs.GT

TL;DR: When selling many goods, a distributionally robust framework is developed. Bundling is shown to be asymptotically optimal when knowing the mean and variance, achieving revenue close to the mean. When only knowing the mean absolute deviation (MAD), bundling remains optimal but with revenue strictly smaller than the mean. The order of play does not affect the outcome, and the optimal bundling price is universally effective for various objectives.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a distributionally robust framework for selling multiple goods with independent valuations when the seller has limited knowledge about the value distribution. This involves understanding how revenue maximization is affected by uncertainty in the distribution, moving beyond traditional assumptions like known mean and variance.

Method: The study employs a two-player game framework between a seller and nature. The seller, possessing limited knowledge (specifically, the mean and mean absolute deviation (MAD) of valuations), selects a revenue-maximizing mechanism. Nature, acting adversarially, chooses a revenue-minimizing distribution from those consistent with the seller's knowledge. The analysis focuses on bundling as a mechanism and derives conditions under which it is optimal.

Result: The paper shows that when the seller knows the mean and variance, bundling is asymptotically optimal with revenue close to the mean. When the seller only knows the mean and MAD, bundling remains optimal, but the guaranteed revenue is strictly less than the mean. The study also finds that the order of play (max-min vs. min-max) does not impact the outcome, and the optimal bundling price is effective for optimizing absolute revenue, absolute regret, and ratio objectives.

Conclusion: The paper concludes that bundling is a robust and often optimal mechanism in a distributionally robust setting for selling multiple goods, even with limited distributional knowledge like MAD. It highlights that while bundling's optimality holds under broader assumptions than previously known, the achievable revenue might be capped below the mean when dealing with heavier-tailed distributions. The universality of the optimal bundling price across different objectives is also a key takeaway.

Abstract: When selling many goods with independent valuations, we develop a
distributionally robust framework, consisting of a two-player game between
seller and nature. The seller has only limited knowledge about the value
distribution. The seller selects a revenue-maximizing mechanism, after which
nature chooses a revenue-minimizing distribution from all distributions that
comply with the limited knowledge. When the seller knows the mean and variance
of valuations, bundling is known to be an asymptotically optimal deterministic
mechanism, achieving a normalized revenue close to the mean. Moving beyond this
variance assumption, we assume knowledge of the mean absolute deviation (MAD),
accommodating more dispersion and heavy-tailed valuations with infinite
variance. We show for a large range of MAD values that bundling remains
optimal, but the seller can only guarantee a revenue strictly smaller than the
mean. Another noteworthy finding is indifference to the order of play, as both
the max-min and min-max versions of the problem yield identical values. This
contrasts with deterministic mechanisms and the separate sale of goods, where
the order of play significantly impacts outcomes. We further underscore the
universality of the optimal bundling price by demonstrating its efficacy in
optimizing not only absolute revenue but also the absolute regret and ratio
objective among all bundling prices

</details>


### [289] [Scale-Invariant Regret Matching and Online Learning with Optimal Convergence: Bridging Theory and Practice in Zero-Sum Games](https://arxiv.org/abs/2510.04407)
*Brian Hu Zhang,Ioannis Anagnostides,Tuomas Sandholm*

Main category: cs.GT

TL;DR: 本文提出了一种名为IREG-PRM+的新算法，该算法结合了理论和实践，在零和博弈求解方面实现了最优收敛速度，并与现有算法在基准测试中表现相当。


<details>
  <summary>Details</summary>
Motivation: 弥合零和博弈求解的理论与实践之间的差距，解决现有算法收敛速度慢的问题。

Method: 提出了一种新的、尺度不变的、无参数的PRM+变体（IREG-PRM+），并分析了其与具有自适应学习率的乐观梯度下降的类比性，通过保持悔恨向量范数非递减来解决PRM+的缺陷。

Result: IREG-PRM+实现了T^{-1/2}的最佳迭代和T^{-1}（最优）平均迭代收敛保证，并且在基准博弈中表现与PRM+相当。同时，提出了一种自适应乐观梯度下降算法，并证明了其与悔恨匹配算法的有效性相当。

Conclusion: IREG-PRM+成功地弥合了理论与实践的差距，提供了最优的收敛保证，并且在实践中表现优异，揭示了悔恨匹配算法相对于标准优化技术的有效性。

Abstract: A considerable chasm has been looming for decades between theory and practice
in zero-sum game solving through first-order methods. Although a convergence
rate of $T^{-1}$ has long been established since Nemirovski's mirror-prox
algorithm and Nesterov's excessive gap technique in the early 2000s, the most
effective paradigm in practice is *counterfactual regret minimization*, which
is based on *regret matching* and its modern variants. In particular, the state
of the art across most benchmarks is *predictive* regret matching$^+$
(PRM$^+$), in conjunction with non-uniform averaging. Yet, such algorithms can
exhibit slower $\Omega(T^{-1/2})$ convergence even in self-play.
  In this paper, we close the gap between theory and practice. We propose a new
scale-invariant and parameter-free variant of PRM$^+$, which we call
IREG-PRM$^+$. We show that it achieves $T^{-1/2}$ best-iterate and $T^{-1}$
(i.e., optimal) average-iterate convergence guarantees, while also being on par
with PRM$^+$ on benchmark games. From a technical standpoint, we draw an
analogy between IREG-PRM$^+$ and optimistic gradient descent with *adaptive*
learning rate. The basic flaw of PRM$^+$ is that the ($\ell_2$-)norm of the
regret vector -- which can be thought of as the inverse of the learning rate --
can decrease. By contrast, we design IREG-PRM$^+$ so as to maintain the
invariance that the norm of the regret vector is nondecreasing. This enables us
to derive an RVU-type bound for IREG-PRM$^+$, the first such property that does
not rely on introducing additional hyperparameters to enforce smoothness.
  Furthermore, we find that IREG-PRM$^+$ performs on par with an adaptive
version of optimistic gradient descent that we introduce whose learning rate
depends on the misprediction error, demystifying the effectiveness of the
regret matching family *vis-a-vis* more standard optimization techniques.

</details>


### [290] [Bin Packing and Covering: Pushing the Frontier on the Maximin Share Fairness](https://arxiv.org/abs/2510.04425)
*Bo Li,Ankang Sun,Zunyu Wang,Yu Zhou*

Main category: cs.GT

TL;DR: 本研究关注一项公平分配问题，其中代理的价值取决于用于打包或覆盖分配给他们的物品的箱子数量。我们使用最大最小共享（MMS）标准来评估公平性。该问题不仅有实际应用背景，也是研究群体公平性的自然框架。由于MMS并不总是可满足的，我们考虑了两种近似类型：基数近似和序数近似。对于基数近似，我们放宽了箱子的打包或覆盖要求；对于序数近似，我们放宽了打包或覆盖的箱子数量。对于所有模型，我们都提供了常数近似算法。


<details>
  <summary>Details</summary>
Motivation: 本研究提出的公平分配问题，其动机在于实际应用，并为研究群体公平性提供了一个自然的框架。最大最小共享（MMS）标准用于评估公平性。

Method: 本文针对最大最小共享（MMS）标准在公平分配问题中可能无法满足的情况，提出了两种近似方法：基数近似（放宽箱子打包/覆盖要求）和序数近似（放宽打包/覆盖的箱子数量）。

Result: 对于所有考虑的模型，研究都提供了常数近似算法。

Conclusion: 本研究为公平分配问题及其近似方法提供了常数近似算法。

Abstract: We study a fundamental fair allocation problem, where the agent's value is
determined by the number of bins either used to pack or cover the items
allocated to them. Fairness is evaluated using the maximin share (MMS)
criterion. This problem is not only motivated by practical applications, but
also serves as a natural framework for studying group fairness. As MMS is not
always satisfiable, we consider two types of approximations: cardinal and
ordinal. For cardinal approximation, we relax the requirements of being packed
or covered for a bin, and for ordinal approximation, we relax the number of
bins that are packed or covered. For all models of interest, we provide
constant approximation algorithms.

</details>


### [291] [Fairness in Repeated Matching: A Maximin Perspective](https://arxiv.org/abs/2510.04624)
*Eugene Lim,Tzeh Yuan Neoh,Nicholas Teh*

Main category: cs.GT

TL;DR: 本论文研究了一个多轮次匹配模型，旨在最大化整体或每轮的最少满意度，并探讨了其计算复杂性，提出了近似和固定参数算法，并解决了特定情况下的问题。


<details>
  <summary>Details</summary>
Motivation: 研究多轮次匹配模型，以最大化最少满意度，解决其计算复杂性问题。

Method: 提出近似算法、固定参数可解算法，并识别可高效求解的特例。

Result: 证明了该问题通常是计算上不可行的，但提供了近似和固定参数可解算法，并确定了可高效解决的特殊情况。

Conclusion: 最大化最少满意度的多轮次匹配问题通常是计算上不可行的，但存在有效的近似和特定情况下的解决方案。

Abstract: We study a sequential decision-making model where a set of items is
repeatedly matched to the same set of agents over multiple rounds. The
objective is to determine a sequence of matchings that either maximizes the
utility of the least advantaged agent at the end of all rounds (optimal) or at
the end of every individual round (anytime optimal). We investigate the
computational challenges associated with finding (anytime) optimal outcomes and
demonstrate that these problems are generally computationally intractable.
However, we provide approximation algorithms, fixed-parameter tractable
algorithms, and identify several special cases whereby the problem(s) can be
solved efficiently. Along the way, we also establish characterizations of
Pareto-optimal/maximum matchings, which may be of independent interest to works
in matching theory and house allocation.

</details>


### [292] [A Fixed Point Framework for the Existence of EFX Allocations](https://arxiv.org/abs/2510.04915)
*S. Rasoul Etesami*

Main category: cs.GT

TL;DR: 该论文利用固定点理论和差分凸规划（DC 规划）解决了公平分配（EFX）问题，通过将离散问题转化为连续问题，证明了 EFX 分配的存在性，并提出了一种计算 EFX 分配的新方法。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决具有线性估值的公平分配（EFX）问题，并提出一种新的方法来证明 EFX 分配的存在性。

Method: 研究者首先使用随机取样将离散的 EFX 约束扩展到连续空间，然后将该问题表述为无约束的差分凸（DC）规划，并进一步简化为在多面体上最小化分段线性凹函数。接着，他们证明了该 DC 规划的最优解非正当且仅当一个连续向量映射存在不动点时成立。通过利用布劳威尔不动点定理的条件，并对映射进行微小扰动以解决其不满足自包含性条件的问题，最终证明了 EFX 分配的存在性。

Result: 该研究表明，EFX 分配的存在性等价于一个 DC 规划的最优值为非正，也等价于一个连续向量映射存在不动点。通过对连续向量映射进行扰动，可以找到一个不动点，该不动点可以通过适当的变换对应于 EFX 分配。

Conclusion: 该研究通过将 EFX 分配问题与 DC 规划和不动点理论联系起来，为证明 EFX 分配的存在性提供了一种新颖的方法，并提出了一种基于非线性优化的计算 EFX 分配的系统化方法。

Abstract: We consider the problem of the existence of an envy-free allocation up to any
good (EFX) for linear valuations and establish new results by connecting this
problem to a fixed point framework. Specifically, we first use randomized
rounding to extend the discrete EFX constraints into a continuous space and
show that an EFX allocation exists if and only if the optimal value of the
continuously extended objective function is nonpositive. In particular, we
demonstrate that this optimization problem can be formulated as an
unconstrained difference of convex (DC) program, which can be further
simplified to the minimization of a piecewise linear concave function over a
polytope. Leveraging this connection, we show that the proposed DC program has
a nonpositive optimal objective value if and only if a well-defined continuous
vector map admits a fixed point. Crucially, we prove that the reformulated
fixed point problem satisfies all the conditions of Brouwer's fixed point
theorem, except that self-containedness is violated by an arbitrarily small
positive constant. To address this, we propose a slightly perturbed continuous
map that always admits a fixed point. This fixed point serves as a proxy for
the fixed point (if it exists) of the original map, and hence for an EFX
allocation through an appropriate transformation. Our results offer a new
approach to establishing the existence of EFX allocations through fixed point
theorems. Moreover, the equivalence with DC programming enables a more
efficient and systematic method for computing such allocations (if one exists)
using tools from nonlinear optimization. Our findings bridge the discrete
problem of finding an EFX allocation with two continuous frameworks: solving an
unconstrained DC program and identifying a fixed point of a continuous vector
map.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [293] [Optimizing Phase-Scheduling with Throughput Trade-offs in AQFP Digital Circuits](https://arxiv.org/abs/2510.03956)
*Robert S. Aviles,Peter A. Beerel*

Main category: cs.ET

TL;DR: AQFP逻辑通过结合相位跳跃和相位对齐来优化面积和吞吐量。


<details>
  <summary>Details</summary>
Motivation: AQFP逻辑功耗低，但缓冲器开销限制了其可扩展性。现有的相位跳跃和相位对齐技术可以减轻这种开销，但尚未结合使用。

Method: 提出了一种结合相位跳跃和相位对齐的时钟相位调度算法，并提出了一种最小面积方法和一种强制目标吞吐量的优化方法。

Result: 最小面积方法比单独使用相位跳跃平均减少25%的面积，比单独使用相位对齐平均减少11%的面积。吞吐量受限的优化方法平均节省6.8%的面积，同时吞吐量提高2.62倍。

Conclusion: 所提出的时钟相位调度算法有效地结合了相位跳跃和相位对齐，以减少AQFP电路的面积开销，并允许在面积和性能之间进行权衡。

Abstract: Adiabatic Quantum-Flux-Parametron (AQFP) logic is a promising emerging
superconducting technology for ultra-low power digital circuits, offering
orders of magnitude lower power consumption than CMOS. However, AQFP
scalability is challenged by excessive buffer overhead due to path balancing
technology constraints. Addressing this, recent AQFP works have proposed design
solutions to reduce path balancing overhead using phase-skipping and
phase-alignment. Phase-skipping is a circuit-level technique that allows data
transfer between AQFP gates clocked with non-consecutive clock phases. In
contrast, phase-alignment is an architectural approach involving repeating
input patterns to allow data transfer between AQFP gates across multiples of
full clock cycles. While both techniques individually mitigate the area
overhead of path-balancing, they have not yet been jointly explored. In this
work, we present the first clock phase scheduling algorithm that combines
phase-skipping and phase-alignment. We first present a minimum area method that
on average, achieves a 25% area reduction compared to phase-skipping alone and
a 11% reduction compared to phase-alignment. We then extend the method to
enforce a target throughput, enabling efficient area-performance trade-offs.
With our throughput constrained optimization, we achieve on average 6.8% area
savings with a 2.62x increased throughput compared to the state-of-the-art
phase-aligned method.

</details>


### [294] [CMOS 2.0 - Redefining the Future of Scaling](https://arxiv.org/abs/2510.04535)
*Moritz Brunion,Navaneeth Kunhi Purayil,Francesco Dell'Atti,Sebastian Lam,Refik Bilgic,Mehdi Tahoori,Luca Benini,Julien Ryckaert*

Main category: cs.ET

TL;DR: 该论文提出CMOS 2.0平台，通过3D异构堆叠实现超越传统几何扩展的性能功耗比和成本效益，并探讨了相应的架构、EDA工具链和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 传统的CMOS几何扩展已接近极限，需要新的方法来满足未来技术对功耗、性能、面积和成本（PPAAC）的期望。

Method: 提出CMOS 2.0平台，利用3D晶圆键合和背面加工技术，实现专用有源器件层的精细化异构3D堆叠。探讨了支持该平台的架构设计、EDA基础设施以及可靠性问题和缓解方法。

Result: 通过CMOS 2.0平台，有望实现比现有技术更高的功耗、性能、面积和成本（PPAAC）增益。

Conclusion: CMOS 2.0平台代表了CMOS技术发展的一个新方向，将推动系统设计的重大变革，但需要克服架构和EDA工具链方面的挑战。

Abstract: We propose to revisit the functional scaling paradigm by capitalizing on two
recent developments in advanced chip manufacturing, namely 3D wafer bonding and
backside processing. This approach leads to the proposal of the CMOS 2.0
platform. The main idea is to shift the CMOS roadmap from geometric scaling to
fine-grain heterogeneous 3D stacking of specialized active device layers to
achieve the ultimate Power-Performance-Area and Cost gains expected from future
technology generations. However, the efficient utilization of such a platform
requires devising architectures that can optimally map onto this technology, as
well as the EDA infrastructure that supports it. We also discuss reliability
concerns and eventual mitigation approaches. This paper provides pointers into
the major disruptions we expect in the design of systems in CMOS 2.0 moving
forward.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [295] [A Subquadratic Two-Party Communication Protocol for Minimum Cost Flow](https://arxiv.org/abs/2510.03427)
*Hossein Gholizadeh,Yonggang Jiang*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we discuss the maximum flow problem in the two-party
communication model, where two parties, each holding a subset of edges on a
common vertex set, aim to compute the maximum flow of the union graph with
minimal communication. We show that this can be solved with
$\tilde{O}(n^{1.5})$ bits of communication, improving upon the trivial
$\tilde{O}(n^2)$ bound.
  To achieve this, we derive two additional, more general results:
  1. We present a randomized algorithm for linear programs with two-sided
constraints that requires $\tilde{O}(n^{1.5}k)$ bits of communication when each
constraint has at most $k$ non-zeros. This result improves upon the prior work
by [Ghadiri, Lee, Padmanabhan, Swartworth, Woodruff, Ye, STOC'24], which
achieves a complexity of $\tilde{O}(n^2)$ bits for LPs with one-sided
constraints. Upon more precise analysis, their algorithm can reach a bit
complexity of $\tilde{O}(n^{1.5} + nk)$ for one-sided constraint LPs.
Nevertheless, for sparse matrices, our approach matches this complexity while
extending the scope to two-sided constraints.
  2. Leveraging this result, we demonstrate that the minimum cost flow problem,
as a special case of solving linear programs with two-sided constraints and as
a general case of maximum flow problem, can also be solved with a communication
complexity of $\tilde{O}(n^{1.5})$ bits.
  These results are achieved by adapting an interior-point method (IPM)-based
algorithm for solving LPs with two-sided constraints in the sequential setting
by [van den Brand, Lee, Liu, Saranurak, Sidford, Song, Wang, STOC'21] to the
two-party communication model. This adaptation utilizes techniques developed by
[Ghadiri, Lee, Padmanabhan, Swartworth, Woodruff, Ye, STOC'24] for distributed
convex optimization.

</details>


### [296] [A Dynamic Programming Approach to Evader Pathfinding in Static Pursuit Scenarios](https://arxiv.org/abs/2510.04050)
*Sukanya Samanta,Manohar Reddy*

Main category: cs.DS

TL;DR: 该研究提出了一种名为DPERO的动态规划算法，用于解决在已知静态防御者部署下，逃避者在城市网络中的最优路径规划问题，以最大化其生存概率。


<details>
  <summary>Details</summary>
Motivation: 现有的博弈论模型（如EIG）在处理城市网络追捕问题时计算复杂度高，不适用于实时应用。本文旨在解决一个关键的子问题：在静态防御者部署下，逃避者的最优路径规划。

Method: 提出DPERO算法，将环境建模为图，节点具有概率风险。通过对生存目标取对数将其转化为加性成本函数，利用价值迭代解决最短路径问题。

Result: 在模拟的网格网络实验中，DPERO算法找到了比基线最短路径算法具有显著更高生存概率的路线。

Conclusion: DPERO算法是一种有效的实用工具，可用于漏洞分析和战略规划，能够高效地计算出在静态防御者部署下，兼顾安全和距离的最优逃避路径。

Abstract: The interdiction of escaping adversaries in urban networks is a critical
security challenge. State-of-the-art game-theoretic models, such as the Escape
Interdiction Game (EIG), provide comprehensive frameworks but assume a highly
dynamic interaction and entail significant computational complexity, which can
be prohibitive for real-time applications. This paper investigates a crucial
sub-problem: an evader's optimal pathfinding calculus when faced with a static
or pre-determined defender deployment. We propose the Dynamic Programming for
Evader Route Optimization (DPERO) algorithm, which models the environment as a
graph with probabilistic risks at various nodes. By transforming the
multiplicative survival objective into an additive cost function using
logarithms, we frame the task as a shortest path problem solvable with value
iteration. This approach allows for the efficient computation of a path that
optimally balances safety and distance. Experimental results on simulated grid
networks demonstrate that DPERO identifies routes with significantly higher
survival probabilities compared to naive shortest-path baselines, validating
its efficacy as a practical tool for vulnerability analysis and strategic
planning.

</details>


### [297] [Streaming Max-Cut in General Metrics](https://arxiv.org/abs/2510.04435)
*Shaofeng H. -C. Jiang,Pan Peng,Haoze Wang*

Main category: cs.DS

TL;DR: 该论文研究了度量空间中带距离预言机的 Max-Cut 问题的流式复杂度，提出了一种仅使用对数空间就能对滑动窗口流进行 (1 + ε) 近似的算法，并在动态流模型中证明了多项式空间下界，从而与欧几里得空间情况区分开来。


<details>
  <summary>Details</summary>
Motivation: 研究 Max-Cut 问题的流式复杂度，特别是在一般度量空间和带距离预言机的设置下。

Method: 提出了一种基于平滑直方图框架的滑动窗口算法，该算法利用了新提出的度量 Max-Cut 平滑度界和度量水库采样技术。

Result: 提出了一个仅使用对数空间就能对滑动窗口流进行 (1 + ε) 近似的算法，并证明了在动态流模型中，任何 poly(n) 近似都需要多项式空间。

Conclusion: 在一般度量空间中，Max-Cut 问题的滑动窗口流式算法在空间复杂度上与插入式流式算法相当，但与欧几里得空间情况存在显著差异，在动态流模型中需要多项式空间。

Abstract: Max-Cut is a fundamental combinatorial optimization problem that has been
studied in various computational settings. In this work, we initiate the study
of its streaming complexity in general metric spaces with access to distance
oracles. We give a $(1 + \epsilon)$-approximation algorithm for estimating the
Max-Cut value sliding-window streams using only poly-logarithmic space. This is
the first sliding-window algorithm for Max-Cut even in Euclidean spaces, and it
achieves a similar error-space tradeoff as the state-of-the-art insertion-only
algorithms in Euclidean settings [Chen, Jiang, Krauthgamer, STOC'23], but
without relying on Euclidean structures. In sharp contrast, we prove a
polynomial-space lower bound for any $\mathrm{poly}(n)$-approximation in the
dynamic streaming setting. This yields a separation from the Euclidean case,
where the polylogarithmic-space $(1+\epsilon)$-approximation extends to dynamic
streams.
  On the technical side, our sliding-window algorithm builds on the smooth
histogram framework of [Braverman and Ostrovsky, SICOMP'10]. To make this
framework applicable, we establish the first smoothness bound for metric
Max-Cut. Moreover, we develop a streaming algorithm for metric Max-Cut in
insertion-only streams, whose key ingredient is a new metric reservoir sampling
technique.

</details>


### [298] [Online Multiple Resource Allocation Problems with Departures via the Primal-Dual Approach](https://arxiv.org/abs/2510.04737)
*Yusuf Amidu,Khaled Elbassioni,Adriana F. Gabor*

Main category: cs.DS

TL;DR: 该论文提出了一种用于在线资源分配问题的对偶算法，该算法能够处理具有时段性到达和离开请求的场景。


<details>
  <summary>Details</summary>
Motivation: 在项目在线资源分配问题中，需要实时决定是否接受一个新来的请求，并将其分配给一个合适的资源，以最大化整体收益。

Method: 本研究提出了一种对偶算法，用于解决在线资源分配问题。该算法在面对具有不同资源、不同持续时间、不同需求和不同奖励的请求时，能够做出最优决策。

Result: 对于基本场景，该算法可以达到 $Oig(\log(\bar\theta^{\max}\cdot\bar d^{\max})\big)$ 的竞争比。在增加负载均衡约束和多维场景下，该算法同样表现出色。

Conclusion: 对偶算法为在线资源分配问题提供了一个统一且简单的框架，不仅能够达到先前其他方法（如阈值策略）的竞争比，还能轻松地整合额外的约束条件（如负载均衡）。

Abstract: In this paper we propose primal-dual algorithms for different variants of the
online resource allocation problem with departures. In the basic variant,
requests (items) arrive over time to a set of resources (knapsacks) and upon
arrival, the duration of time a request may occupy a resource, the demand and
reward if the request can be granted, become known. %We assume that the
duration of stay of a request may depend on the resource. %and that resources
may have different capacity sizes. The goal of the algorithm is to decide
whether to accept/reject a request upon arrival and to which resource to
allocate it such that the reward obtained over time is maximized. Under some
mild assumptions, we show that the proposed primal-dual algorithm achieves a
competitive ratio of $O\big(\log(\bar\theta^{\max}\cdot\bar d^{\max})\big)$,
where $\bar \theta^{\max}$ is the maximum value density fluctuation ratio and
$\bar d^{\max}$ is the maximum duration fluctuation ratio. We prove similar
results for two other variants, namely, one with an additional load balancing
constraint, and the multi-dimensional variant where an admitted request
consumes capacity on multiple resources. Our results show that the primal-dual
approach offers a simple, unified framework for obtaining competitive ratios
comparable to those previously obtained via threshold policies known for these
problems. Additionally, we show that this framework allows us to incorporate
additional constraints, such as load-balancing constraints, without sacrificing
the competitive ratio.

</details>


### [299] [A Polynomial Space Lower Bound for Diameter Estimation in Dynamic Streams](https://arxiv.org/abs/2510.04918)
*Sanjeev Khanna,Ashwin Padaki,Krish Singal,Erik Waingarten*

Main category: cs.DS

TL;DR: 动态流模型下的度量空间点集直径估计需要多项式空间。


<details>
  <summary>Details</summary>
Motivation: 研究动态流模型下点集直径估计的空间复杂度。

Method: 利用线性草图和图的最小秩来证明下界，并提出近乎匹配的上界。

Result: 证明了常数因子近似需要$n^{\Omega(1/c)}$空间，并给出了$n^{O(1/c)}$空间的近似算法。

Conclusion: 动态流模型下的直径估计比插入流模型更具挑战性，需要多项式空间。

Abstract: We study the space complexity of estimating the diameter of a subset of
points in an arbitrary metric space in the dynamic (turnstile) streaming model.
The input is given as a stream of updates to a frequency vector $x \in
\mathbb{Z}_{\geq 0}^n$, where the support of $x$ defines a multiset of points
in a fixed metric space $M = ([n], \mathsf{d})$. The goal is to estimate the
diameter of this multiset, defined as $\max\{\mathsf{d}(i,j) : x_i, x_j > 0\}$,
to a specified approximation factor while using as little space as possible.
  In insertion-only streams, a simple $O(\log n)$-space algorithm achieves a
2-approximation. In sharp contrast to this, we show that in the dynamic
streaming model, any algorithm achieving a constant-factor approximation to
diameter requires polynomial space. Specifically, we prove that a
$c$-approximation to the diameter requires $n^{\Omega(1/c)}$ space. Our lower
bound relies on two conceptual contributions: (1) a new connection between
dynamic streaming algorithms and linear sketches for {\em scale-invariant}
functions, a class that includes diameter estimation, and (2) a connection
between linear sketches for diameter and the {\em minrank} of graphs, a notion
previously studied in index coding. We complement our lower bound with a nearly
matching upper bound, which gives a $c$-approximation to the diameter in
general metrics using $n^{O(1/c)}$ space.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [300] [A Dense and Efficient Instruction Set Architecture Encoding](https://arxiv.org/abs/2510.04158)
*Emad Jacob Maroun*

Main category: cs.AR

TL;DR: Scry是一种新的指令集架构，通过前向时间引用和内部标签等设计，提高了指令密度和编码效率，在不牺牲功能的情况下，其指令长度和编码空间使用率均低于RISC-V。


<details>
  <summary>Details</summary>
Motivation: 指令集架构的设计应以最大化指令密度和编码效率为目标，因为这些是指令集架构设计直接影响的因素，而性能、功耗和面积等因素更多地受处理器实现的影响。

Method: Scry指令集架构采用了前向时间引用（指令引用其输出将被哪些未来指令消费）和内部标签（处理器内部跟踪数据类型）两种方法，以减少指令数量并提高灵活性。

Result: Scry指令集在功能上与RISC-V的RV64IMC相当，但指令长度仅为2字节（RISC-V为4字节），编码空间使用率仅为28%（RISC-V为68%）。对于小型函数，Scry的静态指令密度与RV64IMC相当，并且随着函数大小的增加而提高。

Conclusion: Scry指令集架构通过创新的设计，在指令密度和编码效率方面取得了显著成果，证明了其在现代处理器实现中的潜力。

Abstract: Instruction density and encoding efficiency are some of the few things
directly affected by an instruction set architecture's design. In contrast, a
processor's implementation often significantly influences performance, power
efficiency, and area usage. Therefore, a major goal of instruction set design
should be maximizing instruction density and encoding efficiency. This paper
introduces the design elements of the Scry instruction set architecture that
most significantly affect instruction density and encoding efficiency. Scry is
a novel and experimental instruction set that revisits first principles to
design an instruction set fit for modern processor implementations. Scry uses
forward-temporal referencing as a means of data flow, where instructions refer
to which future instructions consume their outputs. It also uses internal
tagging, where the processors track data types internally, to reduce the number
of instructions needed and increase flexibility. Combining these two methods,
Scry achieves instruction-feature parity with RISC-V's RV64IMC using only
2-byte instructions compared to RISC-V's 4 bytes. Scry's instructions occupy
only 28% of the 2-byte encoding space, where RV64IMC instructions occupy 68% of
the 4-byte encoding space. We show that hand-compiled Scry's static instruction
density is comparable to RV64IMC for small functions and improves as functions
grow in size.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [301] [Cosmological Hydrodynamics at Exascale: A Trillion-Particle Leap in Capability](https://arxiv.org/abs/2510.03557)
*Nicholas Frontiere,J. D. Emberson,Michael Buehlmann,Esteban M. Rangel,Salman Habib,Katrin Heitmann,Patricia Larsen,Vitali Morozov,Adrian Pope,Claude-André Faucher-Giguère,Antigoni Georgiadou,Damien Lebrun-Grandié,Andrey Prokopenko*

Main category: cs.DC

TL;DR: exascale计算实现了CRK-HACC宇宙学流体动力学代码的运行，该代码能够模拟宇宙结构形成，并取得了前所未有的规模和性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决宇宙学中的基本问题，需要进行高精度、大规模的模拟，以匹配下一代巡天观测的需求，同时需要自洽地处理引力、气体动力学以及各种天体物理效应。

Method: 使用分离尺度技术、GPU加速的树求解器、原位分析管道和多层I/O，在Frontier超级计算机上运行了CRK-HACC代码，实现了四万亿粒子全天空模拟。

Result: CRK-HACC在Frontier-E模拟中实现了513.1 PFLOPs的峰值性能，每秒处理466亿粒子，并在不到一周的时间内写入了超过100 PB的数据，规模是先前工作的十倍以上。

Conclusion: CRK-HACC代码的成功运行和Frontier-E模拟的实现，是迈向量子计算赋能的下一代宇宙学模拟的重要一步，为未来更高精度的宇宙结构形成研究奠定了基础。

Abstract: Resolving the most fundamental questions in cosmology requires simulations
that match the scale, fidelity, and physical complexity demanded by
next-generation sky surveys. To achieve the realism needed for this critical
scientific partnership, detailed gas dynamics, along with a host of
astrophysical effects, must be treated self-consistently with gravity for
end-to-end modeling of structure formation. As an important step on this
roadmap, exascale computing enables simulations that span survey-scale volumes
while incorporating key subgrid processes that shape complex cosmic structures.
We present results from CRK-HACC, a cosmological hydrodynamics code built for
the extreme scalability requirements set by modern cosmological surveys. Using
separation-of-scale techniques, GPU-resident tree solvers, in situ analysis
pipelines, and multi-tiered I/O, CRK-HACC executed Frontier-E: a four trillion
particle full-sky simulation, over an order of magnitude larger than previous
efforts. The run achieved 513.1 PFLOPs peak performance, processing 46.6
billion particles per second and writing more than 100 PB of data in just over
one week of runtime.

</details>


### [302] [Datacenter Energy Optimized Power Profiles](https://arxiv.org/abs/2510.03872)
*Sreedhar Narayanaswamy,Pratikkumar Dilipkumar Patel,Ian Karlin,Apoorv Gupta,Sudhir Saripalli,Janey Guo*

Main category: cs.DC

TL;DR: NVIDIA的Blackwell B200发布了新的数据中心功耗配置功能，通过智能功耗管理和对HPC/AI工作负载的理解，优化能效和性能，在严格的功率限制下实现了高达13%的吞吐量提升和15%的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在通过NVIDIA Blackwell B200的新功能“数据中心功耗配置”来提高数据中心的能源效率和/或性能。

Method: 利用硬件和软件创新进行智能功耗管理，并结合HPC和AI工作负载的领域知识，提供粗粒度的用户控制。通过工作负载感知的优化策略，在严格的设施功率限制下最大化计算吞吐量。

Result: 在Blackwell B200的实现中，实现了高达15%的能耗节省，同时关键应用的性能保持在97%以上，最终在功率受限的设施中实现了高达13%的整体吞吐量提升。

Conclusion: 数据中心功耗配置功能通过智能功耗管理和工作负载优化，能够在满足严格的功率限制的同时，显著提高计算性能和能效。

Abstract: This paper presents datacenter power profiles, a new NVIDIA software feature
released with Blackwell B200, aimed at improving energy efficiency and/or
performance. The initial feature provides coarse-grain user control for HPC and
AI workloads leveraging hardware and software innovations for intelligent power
management and domain knowledge of HPC and AI workloads. The resulting
workload-aware optimization recipes maximize computational throughput while
operating within strict facility power constraints. The phase-1 Blackwell
implementation achieves up to 15% energy savings while maintaining performance
levels above 97% for critical applications, enabling an overall throughput
increase of up to 13% in a power-constrained facility.
  KEYWORDS GPU power management, energy efficiency, power profile, HPC
optimization, Max-Q, Blackwell architecture

</details>


### [303] [Toward Co-adapting Machine Learning Job Shape and Cluster Topology](https://arxiv.org/abs/2510.03891)
*Shawn Shuoshuo Chen,Daiyaan Arfeen,Minlan Yu,Peter Steenkiste,Srinivasan Seshan*

Main category: cs.DC

TL;DR: RFold是一种新的资源分配方法，通过动态调整作业形状和集群拓扑来解决分布式机器学习作业在多租户环形集群中的资源分配问题，旨在同时优化网络拥塞和集群利用率。


<details>
  <summary>Details</summary>
Motivation: 现有的调度器在优化网络拥塞和集群利用率之间存在冲突，RFold旨在同时解决这两个问题。

Method: RFold通过识别同态作业形状和重构支持更多样化作业形状的集群拓扑来实现对作业形状和集群拓扑的运行时自适应。

Result: 在4096节点环形集群模拟器上的初步评估表明，与现有方法相比，RFold可以将集群利用率提高57%，并将作业完成时间缩短高达11倍。

Conclusion: RFold能够同时优化网络拥塞和集群利用率，从而提高集群的整体性能。

Abstract: Allocating resources to distributed machine learning jobs in multi-tenant
torus-topology clusters must meet each job's specific placement and
communication requirements, which are typically described using shapes. There
is an inherent tension between minimizing network contention and maximizing
cluster utilization when placing various-shaped jobs. While existing schedulers
typically optimize for one objective at the expense of the other, we
demonstrate that both can be achieved simultaneously.
  Our proposed approach, RFold, adapts both job shapes and the underlying
cluster topology at runtime. This is accomplished by combining two techniques:
(1) identifying homomorphic job shapes that support the jobs communication
needs, and (2) reconfiguring the optical circuit switch-enabled topology to
support more diverse job shapes. Preliminary evaluation performed on a
4096-node torus cluster simulator indicates that RFold can improve absolute
cluster utilization by 57% and reduce job completion time by up to 11x relative
to existing methods

</details>


### [304] [Towards Carbon-Aware Container Orchestration: Predicting Workload Energy Consumption with Federated Learning](https://arxiv.org/abs/2510.03970)
*Zainab Saad,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.DC

TL;DR: 通过使用结合了 Flower 框架的 XGBoost 的联邦学习方法，在不共享敏感数据的同时，提高了 Kubernetes 中能源消耗预测的准确性，从而在数据隐私和能源效率之间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: 当前数据中心对能源消耗的需求日益增长，传统的基于中心化机器学习的方法在隐私和泛化性方面存在不足，需要一种能在保护隐私的前提下提高能源预测效率的解决方案。

Method: 提出了一种联邦学习方法，利用 Kubernetes 上的 Kepler 扩展，并通过 Flower 的 FedXgbBagging 聚合策略进行 XGBoost 模型的协作训练，以预测能源消耗。

Result: 实验结果表明，与中心化基线相比，该联邦学习方法将平均绝对误差（MAE）降低了 11.7%。

Conclusion: 该研究提出了一种无需共享敏感数据即可实现高效能源预测的联邦学习方法，解决了现有系统（如 Kepler 和 CASPER）在数据隐私和能源预测效率之间的权衡问题，为企业提供了一种在不损害运营隐私的情况下实现可持续云计算的可行途径。

Abstract: The growing reliance on large-scale data centers to run resource-intensive
workloads has significantly increased the global carbon footprint, underscoring
the need for sustainable computing solutions. While container orchestration
platforms like Kubernetes help optimize workload scheduling to reduce carbon
emissions, existing methods often depend on centralized machine learning models
that raise privacy concerns and struggle to generalize across diverse
environments. In this paper, we propose a federated learning approach for
energy consumption prediction that preserves data privacy by keeping sensitive
operational data within individual enterprises. By extending the Kubernetes
Efficient Power Level Exporter (Kepler), our framework trains XGBoost models
collaboratively across distributed clients using Flower's FedXgbBagging
aggregation using a bagging strategy, eliminating the need for centralized data
sharing. Experimental results on the SPECPower benchmark dataset show that our
FL-based approach achieves 11.7 percent lower Mean Absolute Error compared to a
centralized baseline. This work addresses the unresolved trade-off between data
privacy and energy prediction efficiency in prior systems such as Kepler and
CASPER and offers enterprises a viable pathway toward sustainable cloud
computing without compromising operational privacy.

</details>


### [305] [From Patchwork to Network: A Comprehensive Framework for Demand Analysis and Fleet Optimization of Urban Air Mobility](https://arxiv.org/abs/2510.04186)
*Xuan Jiang,Xuanyu Zhou,Yibo Zhao,Shangqing Cao,Jinhua Zhao,Mark Hansen,Raja Sengupta*

Main category: cs.DC

TL;DR: 该研究提出了一种利用现有区域机场和优化异构机队来解决城市空中交通（UAM）基础设施成本和运营复杂性挑战的模型，并通过大规模并行模拟框架LPSim进行了验证。


<details>
  <summary>Details</summary>
Motivation: 城市空中交通（UAM）虽然具有变革性，但面临高昂的基础设施成本和运营复杂性带来的挑战。

Method: 提出了一种UAM网络模型，该模型利用现有区域机场并采用优化的、异构的机队。引入了大规模并行模拟框架LPSim，利用多GPU计算同时对UAM需求、机队运营和地面交通交互进行协同优化。扩展了均衡搜索算法来预测需求并确定最高效的机队构成。

Result: 在旧金山湾区的案例研究中，该UAM模型可以为230,000个选定的行程节省超过20分钟的出行时间。

Conclusion: 系统范围的成功严重依赖于与地面交通的无缝集成和动态调度。

Abstract: Urban Air Mobility (UAM) presents a transformative vision for metropolitan
transportation, but its practical implementation is hindered by substantial
infrastructure costs and operational complexities. We address these challenges
by modeling a UAM network that leverages existing regional airports and
operates with an optimized, heterogeneous fleet of aircraft. We introduce
LPSim, a Large-Scale Parallel Simulation framework that utilizes multi-GPU
computing to co-optimize UAM demand, fleet operations, and ground
transportation interactions simultaneously. Our equilibrium search algorithm is
extended to accurately forecast demand and determine the most efficient fleet
composition. Applied to a case study of the San Francisco Bay Area, our results
demonstrate that this UAM model can yield over 20 minutes' travel time savings
for 230,000 selected trips. However, the analysis also reveals that system-wide
success is critically dependent on seamless integration with ground access and
dynamic scheduling.

</details>


### [306] [Beyond Canonical Rounds: Communication Abstractions for Optimal Byzantine Resilience](https://arxiv.org/abs/2510.04310)
*Hagit Attiya,Itay Flam,Jennifer L. Welch*

Main category: cs.DC

TL;DR: 该论文研究了具有最佳故障恢复能力的异步拜占庭容错通信抽象，其中 n > 3f。它表明，经典的异步轮次和通信封闭层模式在 3f < n ≤ 5f 的临界恢复能力下存在固有局限性，导致某些关键任务（如近似和十字军协议、可靠广播和收集）在这些框架下无法解决。论文还展示了收集抽象可以实现具有最佳恢复能力的常数时间解决方案，并支持模块化约简，例如通过将连接共识规约到收集来实现。最终，该研究强调，虽然基于轮次的抽象在分析上很方便，但它们掩盖了拜占庭容错算法的真实复杂性，而像收集这样的更丰富的通信模式为模块化、最佳恢复能力的算法设计提供了更好的基础。


<details>
  <summary>Details</summary>
Motivation: 研究在 $n > 3f$ 的最佳故障恢复能力下，针对异步拜占庭容错通信抽象的局限性，特别是经典的异步轮次和通信封闭层模式在 $3f < n \\le 5f$ 这一关键恢复能力下的不足。

Method: 通过证明在 $3f < n \\le 5f$ 的临界恢复能力下，诸如近似和十字军协议、可靠广播和收集等关键任务，在有界轮次规范轮次算法下无法解决，并且在强制执行通信封闭时不可解。同时，提出收集抽象可以实现具有最佳恢复能力的常数时间解决方案，并展示了第一个通过将连接共识规约到收集而实现的最佳恢复能力算法。

Result: 在 $3f < n \\le 5f$ 的临界恢复能力下，发现了经典异步轮次和通信封闭层模式的固有局限性，导致无法解决近似和十字军协议、可靠广播和收集等关键任务。另一方面，证明了收集抽象可以实现具有最佳恢复能力（$n > 3f$）的常数时间解决方案，并能够支持模块化约简，例如通过将连接共识规约到收集。

Conclusion: 基于轮次的抽象虽然在分析上很方便，但会掩盖拜占庭容错算法的真实复杂性。而像收集这样的更丰富的通信模式，为实现模块化和具有最佳恢复能力的算法设计提供了更好的基础。

Abstract: We study communication abstractions for asynchronous Byzantine fault
tolerance with optimal failure resilience, where $n > 3f$. Two classic patterns
-- canonical asynchronous rounds and communication-closed layers -- have long
been considered as general frameworks for designing distributed algorithms,
making asynchronous executions appear synchronous and enabling modular
reasoning.
  We show that these patterns are inherently limited in the critical resilience
regime $3f < n \le 5f$. Several key tasks -- such as approximate and crusader
agreement, reliable broadcast and gather -- cannot be solved by bounded-round
canonical-round algorithms, and are unsolvable if communication closure is
imposed. These results explain the historical difficulty of achieving
optimal-resilience algorithms within round-based frameworks.
  On the positive side, we show that the gather abstraction admits
constant-time solutions with optimal resilience ($n > 3f$), and supports
modular reductions. Specifically, we present the first optimally-resilient
algorithm for connected consensus by reducing it to gather.
  Our results demonstrate that while round-based abstractions are analytically
convenient, they obscure the true complexity of Byzantine fault-tolerant
algorithms. Richer communication patterns such as gather provide a better
foundation for modular, optimal-resilience design.

</details>


### [307] [Next-Generation Event-Driven Architectures: Performance, Scalability, and Intelligent Orchestration Across Messaging Frameworks](https://arxiv.org/abs/2510.04404)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Ahsan Habib Tareq*

Main category: cs.DC

TL;DR: 该论文对12种消息传递系统进行了标准化基准测试，并引入了一个名为AIEO的智能编排框架，以优化性能和成本。


<details>
  <summary>Details</summary>
Motivation: 传统的事件处理框架在处理低延迟、容错的分布式系统方面存在局限性，需要对现有技术进行全面评估和改进。

Method: 使用标准化的基准测试框架，在三种代表性工作负载下评估了12种消息传递系统，并引入了AIEO（AI增强事件编排）框架，该框架利用机器学习进行预测性扩展、强化学习进行动态资源分配和多目标优化。

Result: Apache Kafka 在吞吐量方面表现最佳（每秒120万条消息，95%延迟18毫秒），但操作复杂；Apache Pulsar 性能均衡（每秒95万条消息，95%延迟22毫秒），多租户能力强；无服务器解决方案在可变工作负载下具有弹性扩展能力，但基线延迟较高（80-120毫秒）；AIEO 在所有平台上平均减少了34%的延迟，提高了28%的资源利用率，并优化了42%的成本。

Conclusion: 标准化基准测试方法、开源智能编排和基于证据的决策指南为下一代分布式系统的设计奠定了基础。

Abstract: Modern distributed systems demand low-latency, fault-tolerant event
processing that exceeds traditional messaging architecture limits. While
frameworks including Apache Kafka, RabbitMQ, Apache Pulsar, NATS JetStream, and
serverless event buses have matured significantly, no unified comparative study
evaluates them holistically under standardized conditions. This paper presents
the first comprehensive benchmarking framework evaluating 12 messaging systems
across three representative workloads: e-commerce transactions, IoT telemetry
ingestion, and AI inference pipelines. We introduce AIEO (AI-Enhanced Event
Orchestration), employing machine learning-driven predictive scaling,
reinforcement learning for dynamic resource allocation, and multi-objective
optimization. Our evaluation reveals fundamental trade-offs: Apache Kafka
achieves peak throughput (1.2M messages/sec, 18ms p95 latency) but requires
substantial operational expertise; Apache Pulsar provides balanced performance
(950K messages/sec, 22ms p95) with superior multi-tenancy; serverless solutions
offer elastic scaling for variable workloads despite higher baseline latency
(80-120ms p95). AIEO demonstrates 34\% average latency reduction, 28\% resource
utilization improvement, and 42% cost optimization across all platforms. We
contribute standardized benchmarking methodologies, open-source intelligent
orchestration, and evidence-based decision guidelines. The evaluation
encompasses 2,400+ experimental configurations with rigorous statistical
analysis, providing comprehensive performance characterization and establishing
foundations for next-generation distributed system design.

</details>


### [308] [The R(1)W(1) Communication Model for Self-Stabilizing Distributed Algorithms](https://arxiv.org/abs/2510.04644)
*Hirotsugu Kakugawa,Sayaka Kamei,Masahiro Shibata,Fukuhito Ooshita*

Main category: cs.DC

TL;DR: 提出了一种新的R(1)W(1)通信和执行模型，并基于此模型提出了解决最大匹配、最小k支配集和最大k相关集问题的自稳定分布式算法。最后，提出了一种基于随机二距离局部互斥的示例转换器，用于在具有同步时钟的同步消息传递模型中模拟为R(1)W(1)模型设计的算法。


<details>
  <summary>Details</summary>
Motivation: 自稳定是一种用于设计容错分布式算法以处理瞬态故障的通用方法。自稳定系统可以从任何类型和数量的瞬态故障中自动恢复。此属性对于具有大量组件的现代分布式系统特别有用。

Method: 提出了一种新的通信和执行模型R(1)W(1)，其中每个进程可以在单个步骤中读写其自身及其邻居的局部变量。并基于此模型提出了解决最大匹配、最小k支配集和最大k相关集问题的自稳定分布式算法。最后，提出了一种基于随机二距离局部互斥的示例转换器，用于在同步消息传递模型中模拟R(1)W(1)模型算法。

Result: 提出了R(1)W(1)模型和用于最大匹配、最小k支配集和最大k相关集问题的自稳定分布式算法。还提出了一个用于在同步消息传递模型中模拟R(1)W(1)算法的转换器。

Conclusion: R(1)W(1)模型为分布式算法设计提供了一种新的方法，并能有效地解决一些关键问题。

Abstract: Self-stabilization is a versatile methodology in the design of fault-tolerant
distributed algorithms for transient faults. A self-stabilizing system
automatically recovers from any kind and any finite number of transient faults.
This property is specifically useful in modern distributed systems with a large
number of components. In this paper, we propose a new communication and
execution model named the R(1)W(1) model in which each process can read and
write its own and neighbors' local variables in a single step. We propose
self-stabilizing distributed algorithms in the R(1)W(1) model for the problems
of maximal matching, minimal k-dominating set and maximal k-dependent set.
Finally, we propose an example transformer, based on randomized distance-two
local mutual exclusion, to simulate algorithms designed for the R(1)W(1) model
in the synchronous message passing model with synchronized clocks.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [309] [COMET: Co-Optimization of a CNN Model using Efficient-Hardware OBC Techniques](https://arxiv.org/abs/2510.03516)
*Boyang Chen,Mohd Tasleem Khan,George Goussetis,Mathini Sellathurai,Yuan Ding,João F. C. Mota*

Main category: eess.SP

TL;DR: COMET框架利用硬件偏移二元编码（OBC）技术优化CNN设计，实现了在低功耗边缘设备上的高效部署，显著降低了资源利用率，同时保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 现有CNN在低功耗边缘设备部署上面临计算密集和硬件依赖的挑战。

Method: 提出COMET框架，采用OBC技术对输入（方案A）和权重（方案B）进行表示，并引入了四种新的查找表（LUT）技术（并行、共享、拆分和混合），在此基础上开发了基于OBC的通用矩阵乘法核，并利用im2col变换实现了LeNet-5模型的加速。

Result: 提出的协同优化方法与最先进的基于LeNet-5的CNN设计相比，显著降低了资源利用率，同时对精度的影响很小。

Conclusion: COMET框架通过OBC技术实现了CNN性能和资源利用率的协同优化，为在低功耗边缘设备上高效部署CNN提供了有效的解决方案。

Abstract: Convolutional Neural Networks (CNNs) are highly effective for computer vision
and pattern recognition tasks; however, their computational intensity and
reliance on hardware such as FPGAs pose challenges for deployment on low-power
edge devices. In this work, we present COMET, a framework of CNN designs that
employ efficient hardware offset-binary coding (OBC) techniques to enable
co-optimization of performance and resource utilization. The approach
formulates CNN inference with OBC representations of inputs (Scheme A) and
weights (Scheme B) separately, enabling exploitation of bit-width asymmetry.
The shift-accumulate operation is modified by incorporating the offset term
with the pre-scaled bias. Leveraging inherent symmetries in Schemes A and B, we
introduce four novel look-up table (LUT) techniques -- parallel, shared, split,
and hybrid -- and analyze them to identify the most efficient options. Building
on this foundation, we develop an OBC-based general matrix multiplication core
using the im2col transformation, enabling efficient acceleration of a
fixed-point modified LeNet-5 model. FPGA evaluations demonstrate that the
proposed co-optimization approach significantly reduces resource utilization
compared to state-of-the-art LeNet-5 based CNN designs, with minimal impact on
accuracy.

</details>


### [310] [Variable Block-Correlation Modeling and Optimization for Secrecy Analysis in Fluid Antenna Systems](https://arxiv.org/abs/2510.03594)
*Tuo Wu,Kwai-Man Luk,Jie Tang,Kai-Kit Wong,Jianchao Zheng,Baiyang Liu,David Morales-Jimenez,Maged Elkashlan,Kin-Fai Tong,Chan-Byoung Chae,Fumiyuki Adachi,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 可变块相关模型（VBCM）被应用于流体天线系统（FAS）的安全分析，通过优化方法提高了分析精度，并实现了可观的保密容量提升。


<details>
  <summary>Details</summary>
Motivation: 现有的流体天线系统（FAS）空间相关性分析模型存在不足，Jakes模型难以解析，恒定相关模型过于简化，无法准确反映实际情况。

Method: 将Ramírez-Espinosa等人提出的可变块相关模型（VBCM）应用于FAS安全分析，并开发了网格搜索（GS）和梯度下降（GD）优化算法来最大化平均保密容量（ASC）。

Result: VBCM框架在FAS安全分析中实现了低于5%的相对误差，优于恒定相关模型的10%-15%。优化的ASC提升超过120%（高威胁场景）和18%-19%（紧凑天线配置）。

Conclusion: VBCM是分析和优化FAS安全性的强大工具，可显著提升6G通信系统的性能。

Abstract: Fluid antenna systems (FAS) are emerging as a transformative enabler for
sixth-generation (6G) wireless communications, providing unprecedented spatial
diversity through dynamic reconfiguration of antenna ports. However, the
inherent spatial correlation among ports poses significant challenges for
accurate analysis. Conventional models such as Jakes are analytically
intractable, while oversimplified constant-correlation models fail to capture
the true behavior. In this work, we address these challenges by applying the
variable block-correlation model (VBCM) -- originally proposed by
Ram\'{i}rez-Espinosa \textit{et al.} in 2024 -- to FAS security analysis, and
by developing comprehensive optimization methods to enhance analytical
accuracy. We derive new closed-form expressions for average secrecy capacity
(ASC) and secrecy outage probability (SOP), demonstrating that the VBCM
framework achieves simulation-aligned accuracy, with relative errors
consistently below $5\%$ (compared to $10$--$15\%$ for constant-correlation
models). To maximize ASC, we further design two algorithms: a grid search (GS)
method and a gradient descent (GD) method. Numerical results reveal that the
VBCM-based approach not only provides reliable insights into FAS security
performance, but also yields substantial gains -- ASC improvements exceeding
$120\%$ in high-threat scenarios and $18$--$19\%$ performance enhancements for
compact antenna configurations. These findings underscore the practical value
of integrating VBCM into FAS security analysis and optimization, establishing
it as a powerful tool for advancing 6G communication systems.

</details>


### [311] [On-Grid Equivalence of Continuous-Time Doubly Selective Channels: A Revisit of Bello's Models](https://arxiv.org/abs/2510.03626)
*Jun Tong*

Main category: eess.SP

TL;DR: 该论文研究了双选择性信道中的通信问题，并重新审视了实际物理信道的离网（off-grid）特性。研究表明，虽然传统的信道模型（如Bello在1963年提出的模型）通常基于“on-grid”的离散傅里叶变换（DD）模型，但实际物理信道的时延和多普勒频移通常是“off-grid”的。这导致了实际信道与模型之间的差距。论文中提出的通用模型考虑了收发器的时间/频率域窗函数，并对具有有限支撑集的窗函数进行了推导。这些模型可以看作是Bello经典结果的扩展，能够处理更一般的窗函数，并揭示了等效的“on-grid”模型的特点和潜在影响。


<details>
  <summary>Details</summary>
Motivation: 传统的双选择性信道模型（如Bello的DD模型）通常基于“on-grid”的假设，而实际物理信道的时延和多普勒频移是“off-grid”的。这种模型与实际情况存在差距，因此需要重新审视和改进模型以更好地反映实际物理信道。

Method: 论文提出了一种新的通用模型，该模型考虑了收发器的时间/频率域窗函数，并对具有有限支撑集的窗函数进行了推导。该模型能够处理“off-grid”的时延和多普勒频移，并对Bello的经典结果进行了扩展，以适应更一般的窗函数。

Result: 研究得到了适用于实际物理信道的通用模型，这些模型可以处理“off-grid”的时延和多普勒频移，并且是Bello经典结果的扩展。论文还讨论了这些等效“on-grid”模型的特点和影响。

Conclusion: 实际物理信道的“off-grid”特性在通信模型中至关重要。论文提出的通用模型能够更好地描述这些特性，并为相关通信系统的设计和分析提供了更精确的理论基础。

Abstract: Significant studies on communications over doubly selective channels have
utilized on-grid DD channel models, which are previously investigated in
Bello's seminar paper in 1963. The DD grid is typically specified by the
bandwidth and time duration of the transmission frames. However, the physical
channels are determined by the propagation environments and they are typically
off-grid. Hence, there is often a gap between an actual physical channel and
the on-grid model. This paper revisits the on-grid modeling of practical
physical channels. We study the associated on-grid DD-domain representations
for continuous-time, doubly selective channels with off-grid delay and Doppler
shifts, accounting for practical time/frequency-domain windowing at the
transceivers. The universal models obtained are applicable under the mild
assumption that the windows have finite supports, and they extend Bello's
classical results to account for more general windows. We also discuss the
features and implications of the equivalent on-grid models.

</details>


### [312] [Pinching Antenna Systems (PASS) for Cell-Free Communications](https://arxiv.org/abs/2510.03628)
*Haochen Li*

Main category: eess.SP

TL;DR: 提出了一种结合了天线系统（PASS）和无源通信的系统。


<details>
  <summary>Details</summary>
Motivation: 在基站功率预算和PA部署约束下，最大化和速率。

Method: 使用交替优化（AO）算法，包括加权最小均方误差（WMMSE）方法和基于惩罚的逐元优化方法。

Result: 与基准方案相比，PASS辅助的无源通信系统表现出优越的性能，增加PA数量可以进一步提升其优势，并且无源架构可以减轻用户数量增加带来的平均用户速率下降。

Conclusion: PASS辅助的无源通信系统在性能上优于现有方案，并且能够有效应对用户数量的增长。

Abstract: A pinching antenna system (PASS) assisted cell-free communication system is
proposed. A sum rate maximization problem under the BS power budget constraint
and PA deployment constraint is formulated. To tackle the proposed non-convex
optimization problem, an alternating optimization (AO) algorithm is developed.
In particular, the digital beamforming sub-problem is solved using the weighted
minimum mean square error (WMMSE) method, whereas the pinching beamforming
sub-problem is handled via a penalty based approach combined with element-wise
optimization. Simulation results demonstrate that: 1) the PASS assisted
cell-free systems achieve superior performance over benchmark schemes; 2)
increasing the number of PAs per waveguides can improve the advantage of PASS
assisted cell-free systems; and 3) the cell-free architecture mitigates the
average user rate degradation as the number of users increases.

</details>


### [313] [Towards Secure ISAC Beamforming: How Many Dedicated Sensing Beams Are Required?](https://arxiv.org/abs/2510.03749)
*Fanghao Xia,Zesong Fei,Xinyi Wang,Nanchi Su,Zhaolin Wang,Yuanwei Liu,Jie Xu*

Main category: eess.SP

TL;DR: ISAC系统中，通过联合传输保密通信和专用传感信号，最大化用户总速率，同时满足对空中窃听者（AE）的信干噪比（SINR）和传感信号对杂波加噪声的信噪比（SCNR）约束。利用分数规划、逐次凸近似（SCA）和半定松弛（SDR）技术解决全数字阵列问题，并扩展到混合模拟-数字（HAD）阵列。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决多用户多窃听者集成传感与通信（ISAC）系统中，在满足窃听者SINR和传感SCNR约束的条件下，最大化保密通信速率的问题。

Method: 首先，针对全数字阵列，利用分数规划、SCA和SDR技术，设计交替优化算法求解和优化波束成形。然后，分析所需的最小传感波束数量，并将设计扩展到混合模拟-数字（HAD）阵列，利用流形优化处理单位模约束。

Result: 仿真结果表明，少量传感波束足以实现传感和干扰AE，所提出的设计优于现有方法，并揭示了通信与传感之间的权衡关系。

Conclusion: 所提出的基于ISAC的通信感知联合设计，能够有效地在满足安全和传感要求的前提下，最大化通信速率，且所需的传感资源较少。

Abstract: In this paper, sensing-assisted secure communication in a multi-user
multi-eavesdropper integrated sensing and communication (ISAC) system is
investigated. Confidential communication signals and dedicated sensing signals
are jointly transmitted by a base station (BS) to simultaneously serve users
and sense aerial eavesdroppers (AEs). A sum rate maximization problem is
formulated under AEs' Signal-to-Interference-plus-Noise Ratio (SINR) and
sensing Signal-to-Clutter-plus-Noise Ratio (SCNR) constraints. A
fractional-programming-based alternating optimization algorithm is developed to
solve this problem for fully digital arrays, where successive convex
approximation (SCA) and semidefinite relaxation (SDR) are leveraged to handle
non-convex constraints. Furthermore, the minimum number of dedicated sensing
beams is analyzed via a worst-case rank bound, upon which the proposed
beamforming design is further extended to the hybrid analog-digital (HAD) array
architecture, where the unit-modulus constraint is addressed by manifold
optimization. Simulation results demonstrate that only a small number of
sensing beams are sufficient for both sensing and jamming AEs, and the proposed
designs consistently outperform strong baselines while also revealing the
communication-sensing trade-off.

</details>


### [314] [A Benchmark Study of Deep Learning Methods for Multi-Label Pediatric Electrocardiogram-Based Cardiovascular Disease Classification](https://arxiv.org/abs/2510.03780)
*Yiqiao Chen*

Main category: eess.SP

TL;DR: 本文首次对ZZU-pECG数据集进行了深度学习多标签儿科心血管疾病分类的基准研究，评估了ResNet-1D、BiLSTM、Transformer和Mamba 2等模型在9导联和12导联配置下的表现。


<details>
  <summary>Details</summary>
Motivation: 儿童心血管疾病（CVD）是主要的健康负担，早期筛查至关重要。心电图（ECG）作为一种无创且易于获取的工具，非常适合此目的。

Method: 系统评估了四种代表性范式（ResNet-1D、BiLSTM、Transformer和Mamba 2）在9导联和12导联配置下的性能。

Result: 所有模型均取得了优异的成绩，困惑度损失低至0.0069，在大多数设置下F1分数均高于85%。ResNet-1D在12导联子集上达到了94.67%的宏观F1分数，BiLSTM和Transformer也表现出竞争力。类别的分析表明，在9导联子集上，肥厚型心肌病等罕见病存在挑战，反映了阳性样本有限的影响。

Conclusion: 该基准研究建立了可重用的基线，并突显了不同范式之间的互补优势。研究还指出了大规模、多中心验证、年龄分层分析和更广泛的疾病覆盖范围的必要性，以支持儿科心电图的实际应用。

Abstract: Cardiovascular disease (CVD) is a major pediatric health burden, and early
screening is of critical importance. Electrocardiography (ECG), as a
noninvasive and accessible tool, is well suited for this purpose. This paper
presents the first benchmark study of deep learning for multi-label pediatric
CVD classification on the recently released ZZU-pECG dataset, comprising 3716
recordings with 19 CVD categories. We systematically evaluate four
representative paradigms--ResNet-1D, BiLSTM, Transformer, and Mamba 2--under
both 9-lead and 12-lead configurations. All models achieved strong results,
with Hamming Loss as low as 0.0069 and F1-scores above 85% in most settings.
ResNet-1D reached a macro-F1 of 94.67% on the 12-lead subset, while BiLSTM and
Transformer also showed competitive performance. Per-class analysis indicated
challenges for rare conditions such as hypertrophic cardiomyopathy in the
9-lead subset, reflecting the effect of limited positive samples. This
benchmark establishes reusable baselines and highlights complementary strengths
across paradigms. It further points to the need for larger-scale, multi-center
validation, age-stratified analysis, and broader disease coverage to support
real-world pediatric ECG applications.

</details>


### [315] [Toward Multiband Sensing in FR3: Frequency Anisotropy Characterization and Non-Contiguous Bands Aggregation Algorithms](https://arxiv.org/abs/2510.03787)
*Jacopo Pegoraro,Gianmaria Ventura,Dario Tagliaferri,Marco Mezzavilla,Andrea Bedin,Michele Rossi,Joerg Widmer*

Main category: eess.SP

TL;DR: 6G新频段FR3（7-24 GHz）为相干多频段ISAC提供了巨大潜力，但面临频率各向异性和频谱不连续的挑战。本文首次研究了FR3相干感知，提出了新的相位一致性指标和算法，以克服这些挑战并提高感知分辨率。


<details>
  <summary>Details</summary>
Motivation: 6G新频段FR3（7-24 GHz）为相干多频段ISAC提供了前所未有的机遇，能够将感知分辨率提高到cm级别，但同时也带来了频率各向异性和频谱不连续的挑战，现有的ISAC技术难以直接应用。

Method: 1. 实验表征了感知目标在宽频段上的频率各向异性。 2. 提出了新的多频段处理相位一致性指标。 3. 分析了3GPP定义的非连续FR3频段的影响。 4. 设计了一种新的算法来缓解由此产生的感知伪影。

Result: 所提出的新算法在缓解感知伪影方面优于现有技术，为FR3的多频段ISAC的全面发展奠定了基础。

Conclusion: 本文首次研究了FR3频段的相干多频段感知，通过实验表征了频率各向异性，提出了新的相位一致性指标和能够缓解频谱不连续性影响的算法，为利用FR3实现cm级感知的ISAC铺平了道路。

Abstract: Frequency Range 3 (FR3) in the 7-24 GHz band will be the new spectrum for 6G
wireless networks. The bandwidth availability and diversity of FR3 offer
unprecedented opportunities for coherent multiband Integrated Sensing and
Communications (ISAC), which aggregates the carrier phase information from
multiple frequency bands to increase the sensing resolution to the cm-level.
However, the frequency anisotropy of sensing targets over GHz-wide bands and
the non-contiguity of the 6G spectrum, pose critical challenges to the
application of existing multiband ISAC techniques. We present the first study
on coherent multiband sensing in FR3. We experimentally characterize the
frequency anisotropy of targets and propose new phase coherence metrics for
multiband processing. Then, we analyze the impact of non-contiguous FR3 bands
considered by 3GPP, and design a new algorithm to mitigate the resulting
sensing artifacts, outperforming existing techniques. Our results represent a
first step toward fully developing multiband ISAC for FR3.

</details>


### [316] [Source PAC Coding for Low-latency Secret Key Generation in Short Blocklength Regime](https://arxiv.org/abs/2510.03818)
*Lulu Song,Di Zhang,Tingting Zhang*

Main category: eess.SP

TL;DR: 源极性编码是一种有前景的短码长低延迟密钥生成技术，但现有方案在短码长下性能下降。本文提出了一种多级源极性调整卷积（PAC）编码框架和新的码构造算法，结合了极化效应和最大似然（ML）解码误差系数，在短码长下实现了更高的密钥生成率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有源极性编码方案在短码长下的密钥生成率和可靠性存在显著下降，无法满足6G物联网低延迟、短码长的需求。

Method: 提出了一种多级源极性调整卷积（PAC）编码框架，并设计了一种新的码构造算法，该算法结合了极化效应和最大似然（ML）解码误差系数。

Result: 与传统的源极性编码和多级源极性编码方法相比，所提出的多级源PAC编码方案在短码长下实现了更高的密钥生成率，尤其是在密钥不一致约束下。

Conclusion: 所提出的多级源PAC编码框架和码构造算法能够有效解决短码长下的性能下降问题，显著提高密钥生成率和可靠性，为6G物联网的密钥生成提供了更优的解决方案。

Abstract: Source polar coding is a potential solution for short blocklength-based
low-latency key generation with limited sources, which is a critical aspect of
six generation (6G) Internet of things. However, existing source coding schemes
still suffer from significant degradation in key generation rate and
reconciliation reliability in short blocklength regime. To address this issue,
we introduce a multilevel source polarization-adjusted convolutional (PAC)
coding framework. Furthermore, we propose a novel code construction algorithm
that jointly leverages polarization effects and the maximum likelihood (ML)
decoding error coefficient. Simulations demonstrate that the multilevel source
PAC scheme with the proposed code construction achieves superior key generation
rate under key disagreement constraints compared to conventional and multilevel
source polar coding methods even in short blocklength regimes.

</details>


### [317] [Multi-Frequency Resonating Based Magnetic Induction Underground Emergency Communications with Diverse Mediums](https://arxiv.org/abs/2510.03848)
*Jianyu Wang,Zhichao Li,Wenchi Cheng,Wei Zhang,Hailin Zhang*

Main category: eess.SP

TL;DR: 地下的磁感应通信在灾后紧急通信中有效，但受限于随机变化的介质。本研究提出了一个遵循对数正态分布的统计衰落信道模型，并采用多频谐振补偿（MuReC）线圈实现多频段传输以缓解衰落影响。通过推导信噪比、遍历容量、误比特率和中断概率的表达式，数值结果表明MuReC方案能有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 磁感应（MI）通信在地下紧急通信中有效，但实际应用中介质随机且多样，对通信造成挑战。

Method: 提出一个遵循对数正态分布的统计衰落信道模型，并采用多频谐振补偿（MuReC）线圈实现多频段传输。

Result: 推导了信噪比概率密度函数、遍历容量、平均误比特率和中断概率的表达式。数值结果表明MuReC方案能有效降低多样介质衰落的影响并提升性能。

Conclusion: MuReC基于多频段传输的方案能有效缓解多样介质衰落的影响，并提升地下紧急通信的性能。

Abstract: Magnetic induction (MI) communication is an effective underground emergency
communication technique after disasters such as landslides, mine collapses, and
earthquakes, due to its advantages in mediums such as soil, concrete, and
metals. However, the propagation mediums in practical MI based underground
emergency communications are usually diverse and composed randomly due to the
impact of disasters, which poses a challenge for MI communication in practical
applications. In this paper, we formulate a statistical fading channel model,
which reflects the random composition of diverse mediums and is shown to follow
a lognormal distribution. To mitigate the impact of diverse medium fading,
Multi-frequency Resonating Compensation (MuReC) based coils are used to achieve
multiband transmission. Then, we analyze the performance of MuReC based
multi-band MI communication with diverse medium fading and derive the
expressions of signal-to-noise ratio (SNR) probability density functions,
ergodic capacities, average bit error rates (BERs), and outage probabilities
for both multiplexing and diversity cases. Numerical results show that MuReC
based multiband transmission schemes can effectively reduce the impact of
diverse medium fading and enhance the performance.

</details>


### [318] [On the Exact Sum PDF and CDF of α-μ Variates](https://arxiv.org/abs/2510.03850)
*Fernando Darío Almeida García,Francisco Raimundo Albuquerque Parente,Michel Daoud Yacoub,Jose Cândido Silveira Santos Filho*

Main category: eess.SP

TL;DR: 本论文提出了{\alpha}-{\mu}随机变量和的精确概率密度函数（PDF）和累积分布函数（CDF）的通用高效且可处理的公式，解决了现有方法在随机变量数量增加时出现的计算稳定性、收敛性和准确性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在计算{\alpha}-{\mu}随机变量和的PDF和CDF时，当随机变量数量增加时，计算复杂度高且存在稳定性、收敛性和准确性问题，缺乏通用的高效解决方案。

Method: 推导了{\alpha}-{\mu}随机变量和的PDF和CDF的新的、简单的、精确的解析表达式，其计算复杂度与随机变量的数量无关。

Result: 通过所提出的解析表达式，精确且渐近地分析了{\alpha}-{\mu}衰落环境下L-branch预检测等增益合并和最大比合并接收机的性能，并量化了系统的编码和分集增益。数值模拟显示，使用新公式可以显著减少计算时间。

Conclusion: 论文提出的{\alpha}-{\mu}随机变量和的PDF和CDF的解析表达式，相比现有方法，计算效率更高、更易于处理，并且解决了随机变量数量增加时的计算难题，为无线通信系统的性能分析提供了有效工具。

Abstract: The sum of random variables (RVs) appears extensively in wireless
communications, at large, both conventional and advanced, and has been subject
of longstanding research. The statistical characterization of the referred sum
is crucial to determine the performance of such communications systems.
Although efforts have been undertaken to unveil these sum statistics, e.g.,
probability density function (PDF) and cumulative distribution function (CDF),
no general efficient nor manageable solutions capable of evaluating the exact
sum PDF and CDF are available to date. The only formulations are given in terms
of either the multi-fold Brennan's integral or the multivariate Fox H-function.
Unfortunately, these methods are only feasible up to a certain number of RVs,
meaning that when the number of RVs in the sum increases, the computation of
the sum PDF and CDF is subject to stability problems, convergence issues, or
inaccurate results. In this paper, we derive new, simple, exact formulations
for the PDF and CDF of the sum of L independent and identically distributed
{\alpha}-{\mu} RVs. Unlike the available solutions, the computational
complexity of our analytical expressions is independent of the number of
summands. Capitalizing on our unprecedented findings, we analyze, in exact and
asymptotic manners, the performance of L-branch pre-detection equal-gain
combining and maximal-ratio combining receivers over {\alpha}-{\mu} fading
environments. The coding and diversity gains of the system for both receivers
are analyzed and quantified. Moreover, numerical simulations show that the
computation time reduces drastically when using our expressions, which are
arguably the most efficient and manageable formulations derived so far.

</details>


### [319] [Robust Beamforming for Magnetic Induction Based Underground Emergency Communications](https://arxiv.org/abs/2510.03852)
*Jianyu Wang,Tianrui Hou,Wenchi Cheng,Hailin Zhang*

Main category: eess.SP

TL;DR: 该研究提出了一种考虑信道估计误差的鲁棒波束形成方案，以优化地下磁感应通信系统的性能。


<details>
  <summary>Details</summary>
Motivation: 地下通信在灾后应急通信中至关重要，磁感应（MI）通信因其穿透性而被广泛应用。然而，信道估计误差会影响基于信道状态信息（CSI）的磁波束形成的效果。本研究旨在解决这一问题，以提高地下MI通信的可靠性和吞吐量。

Method: 提出了一种鲁棒的波束形成优化方案，该方案考虑了最坏情况下的信道估计误差。通过运用最坏情况优化准则和S过程，将非凸优化问题转化为凸优化问题进行求解。

Result: 数值结果表明，所提出的鲁棒波束形成方案在存在信道估计误差的情况下，能够有效地提高通信的可靠性和有效吞吐量。

Conclusion: 在存在信道估计误差的情况下，本研究提出的鲁棒波束形成方案可以有效提升地下磁感应应急通信系统的性能。

Abstract: Magnetic induction (MI) communication is an effective underground emergency
communication technique after disasters such as landslides, mine collapses, and
earthquakes, due to its advantages in mediums such as soil, concrete, and
metals. Based on channel state information (CSI), magnetic beamforming can
significantly improve the performance of MI communication. However, in
post-disaster underground communication, channel estimation may suffer from
errors due to factors such as complex environmental interferences. Taking
channel estimation error into account, we formulate a beamforming optimization
problem for multi-user MI underground emergency communications, which aims to
minimize the power consumption under the constraints of sum rate and signal to
interference plus noise ratio (SINR) of each user. Based on the worst-case
optimization criterion and the S-procedure, the non-convex optimization problem
is transformed into convex and solved. Numerical results show that the proposed
robust beamforming scheme can effectively enhance communication reliability and
effective throughput in the presence of channel estimation errors.

</details>


### [320] [On the Noise Robustness of Affine Frequency Division Multiplexing: Analysis and Applications](https://arxiv.org/abs/2510.03901)
*Vincent Savaux,Steve Sawadogo,Hyeon Seok Rou,Giuseppe Thadeu Freitas de Abreu*

Main category: eess.SP

TL;DR: AFDM 在非白噪声下的鲁棒性优于 OTFS 和 OFDM，其解调矩阵对噪声的致白能力与稀疏性有关。


<details>
  <summary>Details</summary>
Motivation: 研究 AFDM 和 OTFS 调制方案在非白高斯噪声下的鲁棒性。

Method: 分析解调矩阵的致白能力与稀疏性，并比较 AFDM、OTFS 和 OFDM 的性能。

Result: AFDM 的解调矩阵通常比 OTFS 和 OFDM 更不稀疏，因此在非白噪声下性能更优，增益超过 1 dB。

Conclusion: AFDM 在非白噪声下的性能优于 OTFS 和 OFDM，适用于窄带信号或与 OFDM 信号共存的场景。

Abstract: This paper investigates the robustness of affine frequency division
multiplexing (AFDM) and orthogonal time frequency space (OTFS) modulation
schemes against non-white Gaussian noise, which can model various sources of
additive disturbances to the received signal. The proposed approach
demonstrates that the performance of these waveforms depends on the ability of
the demodulation matrix to whiten the noise-a property that is, in turn,
related to the sparsity of the matrix. AFDM is shown to outperform OTFS and
orthogonal frequency division multiplexing (OFDM), as its demodulation matrix
is generally less sparse than those of the other waveforms. Based on this
analysis, several application examples and use cases are presented, such as the
use of AFDM and OTFS in narrowband signals or in coexistence with OFDM signals.
Finally, simulation results confirm that AFDM achieves better performance than
OTFS and OFDM in the presence of non-white noise, with gains exceeding 1 dB in
most application scenarios.

</details>


### [321] [Closed-form Solutions for Velocity and Acceleration of a Moving Vehicle Using Range, Range Rate, and Derivative of Range Rate](https://arxiv.org/abs/2510.04037)
*Mohammad Salman,Hadi Zayyani,Hasan Abu Hilal,Mostafa Rashdan*

Main category: eess.SP

TL;DR: 本 Letter 提出了一种新颖的基于距离测量的方法，用于估计运动目标的位姿、速度和加速度，并考虑了加速度的估计。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多集中在位姿和速度估计，本 Letter 将框架扩展到包括加速度的估计。

Method: 提出使用距离变化率的导数，结合距离和距离变化率的测量。首先使用基于到达时间 (TOA) 的技术估计位姿，然后开发了重构的最小二乘 (LS) 和加权最小二乘 (WLS) 方法进行速度估计，最后利用距离变化率的导数结合先前估计的位姿和速度来估计加速度。此外，还推导了速度和加速度的闭式 LS 和 WLS 解。

Result: 仿真结果表明，与现有方法相比，所提出的方法在估计运动目标的运动学方面具有改进的性能。

Conclusion: 所提出的方法能够有效估计运动目标的位姿、速度和加速度，并在仿真中表现出优于现有方法的性能。

Abstract: This letter presents a novel method for estimating the position, velocity,
and acceleration of a moving target using range-based measurements. Although
most existing studies focus on position and velocity estimation, the framework
of this letter is extended to include acceleration. To achieve this, we propose
using the derivative of the range rate, in addition to the range and range rate
measurements. The proposed method estimates the position at first using
Time-of-Arrival (TOA)-based techniques; then, develops a reformulated least
squares (LS) and weighted least squares (WLS) approaches for velocity
estimation; and finally, employs the derivative of the range rate to estimate
the acceleration using previous position and velocity estimates. On the other
hand, closed-form LS and WLS solutions are derived for both velocity and
acceleration. The simulation results show that the proposed approach provides
improved performance in estimating moving target kinematics compared to
existing methods.

</details>


### [322] [CLEAR: A Closed-Form Minimal-Sensor TDOA/FDOA Estimator for Moving-Source IoT Localization](https://arxiv.org/abs/2510.04160)
*Mohammad Kazzazi,Mohammad Morsali,Rouhollah Amiri*

Main category: eess.SP

TL;DR: 该研究提出了一种名为 CLEAR 的闭式解定位估计器，它使用最少量的传感器（N+1 个传感器用于 N 维空间定位）融合到达时差 (TDOA) 和到达频差 (FDOA) 测量数据。该方法通过引入辅助参数将问题转化为伪线性方程组，并使用加权最小二乘法求解。然后，通过代数消元将问题简化为四次方程，从而得到辅助参数的闭式解。最后，通过一个线性精炼阶段来消除残留偏差。该估计器在统计上是高效的，非常接近克拉美-罗下界 (CRLB)，并且在 2D 和 3D 场景的蒙特卡洛模拟中表现优于其他基线方法，特别适用于无人机追踪和智能交通等物联网应用。


<details>
  <summary>Details</summary>
Motivation: 需要一种计算效率高、传感器数量最少（N+1 个传感器用于 N 维空间定位）的方法来融合 TDOA 和 FDOA 测量数据，以实现对移动源的定位。

Method: 该方法分为两个阶段：第一阶段，引入辅助参数构建伪线性方程组，并通过加权最小二乘法求解；然后，利用 Sylvester 结果式进行代数消元，将问题简化为关于辅助参数的四次方程，得到闭式解。第二阶段，进行线性精炼以减少残留偏差。

Result: 在温和的高斯噪声假设下，该估计器的位置和速度估计在统计上是高效的，接近 CRLB。在 2D 和 3D 场景的蒙特卡洛模拟中，该方法达到了 CRLB 级别的精度，并优于现有的双阶段和迭代基线方法。

Conclusion: CLEAR 是一种计算效率高、传感器数量需求最少且精度接近 CRLB 的闭式解定位估计器，非常适合用于功耗受限的分布式物联网应用，例如无人机追踪和智能交通。

Abstract: This paper presents CLEAR -- a closed-form localization estimator with a
reduced sensor network. The proposed method is a computationally efficient,
two-stage estimator that fuses time-difference-of-arrival (TDOA) and
frequency-difference-of-arrival (FDOA) measurements with a minimal number of
sensors. CLEAR localizes a moving source in N-dimensional space using only N+1
sensors, achieving the theoretical minimum sensor count. The first stage
introduces auxiliary range and range-rate parameters to construct a set of
pseudo-linear equations, solved via weighted least squares. An algebraic
elimination using Sylvester's resultant then reduces the problem to a quartic
equation, yielding closed-form estimates for the nuisance variables. A second,
lightweight linear refinement stage is applied to mitigate residual bias. Under
mild Gaussian noise assumptions, the estimator's position and velocity
estimates are statistically efficient, closely approaching the Cramer-Rao lower
bound (CRLB). Extensive Monte Carlo simulations in 2-D and 3-D scenarios
demonstrate CRLB-level accuracy and consistent performance gains over
representative two-stage and iterative baselines, confirming the method's high
suitability for power-constrained, distributed Internet of Things (IoT)
applications such as UAV tracking and smart transportation.

</details>


### [323] [Integrating Phase-Coherent Multistatic Imaging in Downlink D-MIMO Networks](https://arxiv.org/abs/2510.04240)
*Dario Tagliaferri,Silvia Mura,Musa Furkan Keskin,Sauradeep Dey,Henk Wymeersch*

Main category: eess.SP

TL;DR: 该论文提出了一种分布式集成传感与通信（D-ISAC）系统，将成像信号叠加到通信信号上，并设计了接收AP选择策略，以在D-MIMO网络中实现联合多站成像和通信。


<details>
  <summary>Details</summary>
Motivation: 在相干分布式多输入多输出（D-MIMO）网络的下行链路（DL）中集成多站相干成像功能，同时最大化频谱效率（SE）和满足成像的特定需求（一部分AP作为接收器，发射AP发射AP特定的正交信号）。

Method: 提出了一种新的D-ISAC系统，通过叠加为成像专门设计的AP特定信号和传统的UE特定通信信号来实现，并引入了可调的权衡因子。详细设计了满足扩展正交性条件的成像波形和空频预编码器。提出了一种优化的接收AP选择策略，以在半双工约束下最大化成像性能。

Result: 大量的数值结果证明了该提案的可行性和优势，实现了D-MIMO网络中联合多站成像和通信的潜力。

Conclusion: 所提出的D-ISAC系统和优化策略能够有效地在D-MIMO网络中实现联合多站成像和通信，并展现了其在实际应用中的巨大潜力。

Abstract: This paper addresses the challenge of integrating multistatic coherent
imaging functionalities in the downlink (DL) of a phase-coherent distributed
multiple input multiple output (D-MIMO) communication network. During DL, the
D-MIMO access points (APs) jointly precode the transmitted signals to maximize
the spectral efficiency (SE) at the users (UEs) locations. However, imaging
requires that \textit{(i)} a fraction of the APs work as receivers for sensing
and \textit{(ii)} the transmitting APs emit AP-specific and orthogonal signals
to illuminate the area to be imaged and allow multistatic operation. In these
settings, our contribution is twofold. We propose a novel distributed
integrated sensing and communication (D-ISAC) system that superposes a
purposely designed AP-specific signal for imaging to the legacy UE-specific
communication one, with a tunable trade-off factor. We detail both the imaging
waveform design according to the \textit{extended orthogonality condition} and
the space-frequency precoder design. Then, we propose an optimized selection
strategy for the receiving APs, in order to maximize imaging performance under
half-duplex constraints. Extensive numerical results prove the feasibility and
benefits of our proposal, materializing the potential of joint multistatic
imaging and communications in practical D-MIMO deployments.

</details>


### [324] [Terahertz Channel Measurement and Modeling for Short-Range Indoor Environments](https://arxiv.org/abs/2510.04258)
*Ziang Zhao,Weixi Liang,Kai Hu,Qun Zhang,Xiongbin Yu,Qiang Li*

Main category: eess.SP

TL;DR: 本文提出了一种考虑视距（LOS）和非视距（NLOS）分量的物理 Rician fading 信道模型，适用于 6G 室内太赫兹通信。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理室内太赫兹通信中的频率选择性和多径效应方面存在不足。

Method: 提出了一种物理 Rician fading 信道模型，结合了 LOS 和 NLOS 成分，并考虑了频率依赖性衰减（通过 alpha 和 beta 指数）、两射线反射模型（用于驻波现象）和宽带频谱平均（用于抑制频率选择性）。

Result: 在 208 GHz 载波下，测量范围为 0.1-0.9 m，所提出的模型实现了低至 2.54 dB 的均方根误差（RMSE），相比自由空间路径损耗（FSPL）提高了 14.2%，并且随着带宽增加，RMSE 降低了 73.3%。

Conclusion: 所提出的模型能够准确地对室内太赫兹信道进行建模，尤其是在高带宽下，这对于 6G 室内应用（如 WPAN、D2D 通信和定位）至关重要。

Abstract: Accurate channel modeling is essential for realizing the potential of
terahertz (THz) communications in 6G indoor networks, where existing models
struggle with severe frequency selectivity and multipath effects. We propose a
physically grounded Rician fading channel model that jointly incorporates
deterministic line-of-sight (LOS) and stochastic non-line-of-sight (NLOS)
components, enhanced by frequency-dependent attenuation characterized by
optimized exponents alpha and beta. Unlike conventional approaches, our model
integrates a two-ray reflection framework to capture standing wave phenomena
and employs wideband spectral averaging to mitigate frequency selectivity over
bandwidths up to 15 GHz. Empirical measurements at a 208 GHz carrier, spanning
0.1-0.9 m, demonstrate that our model achieves root mean square errors (RMSE)
as low as 2.54 dB, outperforming free-space path loss (FSPL) by up to 14.2% and
reducing RMSE by 73.3% as bandwidth increases. These findings underscore the
importance of bandwidth in suppressing oscillatory artifacts and improving
modeling accuracy. Our approach provides a robust foundation for THz system
design, supporting reliable indoor wireless personal area networks (WPANs),
device-to-device (D2D) communications, and precise localization in future 6G
applications.

</details>


### [325] [Efficient Domain Generalization in Wireless Networks with Scarce Multi-Modal Data](https://arxiv.org/abs/2510.04359)
*Minsu Kim,Walid Saad,Dour Calin*

Main category: eess.SP

TL;DR: 在6G无线网络中，提出了一种新颖且数据高效的两阶段学习框架，以提高在数据量有限的情况下模型在不同无线环境下的泛化能力。该框架结合了基于物理的损失函数和协作域自适应技术，通过仿真结果验证了其在减少数据需求和计算量方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有用于6G无线网络的机器学习模型在面对训练和测试数据分布不同的情况（即域偏移）时，泛化能力不足，因为它们往往关注特定模态的虚假特征。而在实际无线系统中，域偏移频繁发生，因此需要能在数据量稀疏的情况下实现鲁棒泛化的学习框架。

Method: 提出一个两阶段学习框架：第一阶段，利用基于物理的损失函数使每个基站（BS）学习其无线环境的物理规律；第二阶段，提出协作域自适应，利用多个BS的环境知识来指导受域偏移影响的BS，其中采用基于域相似度的模型聚合来利用经历过相似域的BS的知识。此外，还开发了一个新的数据集生成框架，集成了CARLA和MATLAB来预测毫米波信号强度（RSS）。

Result: 基于物理的训练仅需13%的数据样本即可达到与未使用基于物理训练的基线模型相同的性能。协作域自适应仅需25%的数据样本和20%的计算量（FLOPs）即可达到收敛，优于基线方法。

Conclusion: 所提出的两阶段学习框架能够显著提高模型在未知无线环境下的泛化性能，同时大大减少了对数据量的需求和计算资源。

Abstract: In 6G wireless networks, multi-modal ML models can be leveraged to enable
situation-aware network decisions in dynamic environments. However, trained ML
models often fail to generalize under domain shifts when training and test data
distributions are different because they often focus on modality-specific
spurious features. In practical wireless systems, domain shifts occur
frequently due to dynamic channel statistics, moving obstacles, or hardware
configuration. Thus, there is a need for learning frameworks that can achieve
robust generalization under scarce multi-modal data in wireless networks. In
this paper, a novel and data-efficient two-phase learning framework is proposed
to improve generalization performance in unseen and unfamiliar wireless
environments with minimal amount of multi-modal data. In the first stage, a
physics-based loss function is employed to enable each BS to learn the physics
underlying its wireless environment captured by multi-modal data. The
data-efficiency of the physics-based loss function is analytically
investigated. In the second stage, collaborative domain adaptation is proposed
to leverage the wireless environment knowledge of multiple BSs to guide
under-performing BSs under domain shift. Specifically, domain-similarity-aware
model aggregation is proposed to utilize the knowledge of BSs that experienced
similar domains. To validate the proposed framework, a new dataset generation
framework is developed by integrating CARLA and MATLAB-based mmWave channel
modeling to predict mmWave RSS. Simulation results show that the proposed
physics-based training requires only 13% of data samples to achieve the same
performance as a state-of-the-art baseline that does not use physics-based
training. Moreover, the proposed collaborative domain adaptation needs only 25%
of data samples and 20% of FLOPs to achieve the convergence compared to
baselines.

</details>


### [326] [Low-Rank-Based Approximate Computation with Memristors](https://arxiv.org/abs/2510.04402)
*Binyu Lu,Matthias Frey,Stark Draper,Jingge Zhu*

Main category: eess.SP

TL;DR: Memristor crossbars 难以精确写入，本文提出基于低秩矩阵近似（SVD分解）的方案，通过两步串行VMM和步进平均来缓解随机写入误差，并推导了计算误差的通用表达式。


<details>
  <summary>Details</summary>
Motivation: Memristor crossbars 擅长进行向量-矩阵乘法（VMM）且功耗低，但难以精确写入 memristor 导纳值，影响 VMM 精度。

Method: 利用SVD获取目标矩阵的低秩近似，并将其分解为两个较小的矩阵。然后执行两步串行VMM，并通过步进平均来缓解随机写入误差。

Result: 推导了计算误差的通用表达式，并对给定奇异值分布进行了渐近分析，揭示了误差如何随矩阵大小和秩而变化。分析和数值结果均表明该方案优于基准方案。

Conclusion: 所提出的基于低秩矩阵近似的方案能够有效提高 memristor VMM 的精度，并降低计算误差。

Abstract: Memristor crossbars enable vector-matrix multiplication (VMM), and are
promising for low-power applications. However, it can be difficult to write the
memristor conductance values exactly. To improve the accuracy of VMM, we
propose a scheme based on low-rank matrix approximation. Specifically, singular
value decomposition (SVD) is first applied to obtain a low-rank approximation
of the target matrix, which is then factored into a pair of smaller matrices.
Subsequently, a two-step serial VMM is executed, where the stochastic write
errors are mitigated through step-wise averaging. To evaluate the performance
of the proposed scheme, we derive a general expression for the resulting
computation error and provide an asymptotic analysis under a prescribed
singular-value profile, which reveals how the error scales with matrix size and
rank. Both analytical and numerical results confirm the superiority of the
proposed scheme compared with the benchmark scheme.

</details>


### [327] [Effect of nearby Metals on Electro-Quasistatic Human Body Communication](https://arxiv.org/abs/2510.04409)
*Samyadip Sarkar,Arunashish Datta,David Yang,Mayukh Nath,Shovan Maity,Shreyas Sen*

Main category: eess.SP

TL;DR: 人体通信利用人体导电性，在低功耗通信方面有潜力，但周围金属物体对通信的影响未被充分研究。本文研究了不同金属物体（悬浮金属、接地金属、电梯和汽车等封闭金属环境）对人体通信信道的影响，并提供了理论框架、仿真和实验结果，指出金属物体可降低传输损耗，接地金属和接触式交互可显著增强信道增益，并强调了接地金属比悬浮金属影响更大。


<details>
  <summary>Details</summary>
Motivation: 探索周围金属物体对人体通信信道（特别是寄生回流路径）的影响，以弥补现有研究的不足。

Method: 结合理论分析、有限元方法模拟和可穿戴设备实验，研究不同类型的金属物体（悬浮金属、接地金属、封闭金属环境）对人体通信信道的影响。

Result: 金属物体靠近设备（20厘米内）可降低约10分贝的传输损耗；设备接地与金属物体接地连接时，信道增益可提高至少20分贝；接触式交互产生的接触阻抗依赖的高通信道特性；接地金属比悬浮金属对通信的影响更大，且在关键距离内存在不确定性。

Conclusion: 研究结果增进了对体域通信链路的理解，并为医疗保健、消费电子、国防和工业等领域的应用设计提供了参考。

Abstract: In recent decades Human Body Communication has emerged as a promising
alternative to traditional radio wave communication, utilizing the body's
conductive properties for low-power connectivity among wearables. This method
harnesses the human body as an energy-efficient channel for data transmission
within the electro-quasistatic frequency range, enabling advancements in
human-machine interaction. While prior work has noted the role of parasitic
return paths in such capacitively coupled systems, the influence of surrounding
metallic objects on these paths, which are critical for EQS wireless signaling,
has not been fully explored. This paper fills that gap with a structured study
of how various conducting objects, from non-grounded (floating) metals and
grounded metals to enclosed metallic environments such as elevators and cars,
affect the body-communication channel. We present a theoretical framework
supported by finite element method simulations and experiments with wearable
devices. Results show that metallic objects within 20 cm of devices can reduce
transmission loss by about 10 dB. When a device ground connects to a grounded
metallic object, channel gain can increase by at least 20 dB. Contact area
during touch-based interactions with grounded metals produces contact-impedance
dependent high-pass channel characteristics. Proximity to metallic objects
introduces variability within a critical distance, with grounded metals
producing a larger overall effect than floating metals. These findings improve
understanding of body-centric communication links and inform design for
healthcare, consumer electronics, defense, and industrial applications.

</details>


### [328] [The Role of ISAC in 6G Networks: Enabling Next-Generation Wireless Systems](https://arxiv.org/abs/2510.04413)
*Muhammad Umar Farooq Qaisar,Weijie Yuan,Onur Günlü,Taneli Riihonen,Yuanhao Cui,Lin Zhang,Nuria Gonzalez-Prelcic,Marco Di Renzo,Zhu Han*

Main category: eess.SP

TL;DR: ISAC是6G的关键技术，实现了通信与感知的一体化，提高了频谱效率和降低了延迟，支持智慧城市等应用。本教程全面介绍了ISAC的演进、核心原理、关键技术、挑战和未来趋势。


<details>
  <summary>Details</summary>
Motivation: ISAC是6G网络发展的关键，能够实现通信与感知的一体化，支持下一代应用，并提高频谱效率、降低延迟。

Method: 本教程全面概述了ISAC在6G网络中的作用，包括其自5G以来的演进、技术驱动因素、核心原理、系统变体、关键技术、研究方向、挑战、开放性问题和新兴趋势，并提供了设计见解和建议。

Result: ISAC通过通信与感知的一体化，显著提高了6G网络的能力，为智慧城市、自动驾驶系统和感知环境等多样化用例提供了支持。

Conclusion: ISAC是6G网络不可或缺的一部分，它通过通信与感知的一体化带来了创新，并将塑造无线通信的未来。

Abstract: The commencement of the sixth-generation (6G) wireless networks represents a
fundamental shift in the integration of communication and sensing technologies
to support next-generation applications. Integrated sensing and communication
(ISAC) is a key concept in this evolution, enabling end-to-end support for both
communication and sensing within a unified framework. It enhances spectrum
efficiency, reduces latency, and supports diverse use cases, including smart
cities, autonomous systems, and perceptive environments. This tutorial provides
a comprehensive overview of ISAC's role in 6G networks, beginning with its
evolution since 5G and the technical drivers behind its adoption. Core
principles and system variations of ISAC are introduced, followed by an
in-depth discussion of the enabling technologies that facilitate its practical
deployment. The paper further analyzes current research directions to highlight
key challenges, open issues, and emerging trends. Design insights and
recommendations are also presented to support future development and
implementation. This work ultimately try to address three central questions:
Why is ISAC essential for 6G? What innovations does it bring? How will it shape
the future of wireless communication?

</details>


### [329] [Joint Probing and Scheduling for Cache-Aided Hybrid Satellite-Terrestrial Networks](https://arxiv.org/abs/2510.04492)
*Zhou Zhang,Yizhu Wang,Saman Atapattu,Sumei Sun*

Main category: eess.SP

TL;DR: 本研究提出了一种结合机会性协同缓存和联合探测与调度策略，通过优化低轨卫星探测链路状态和地面缓存状态，并利用最优停止理论进行实时决策，显著提高了卫星-地面混合网络的平均系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 为了减少延迟、优化吞吐量和提高数据可用性，尤其是在带宽受限的卫星系统中，缓存是混合卫星-地面网络中的关键技术，需要战略性的介质访问控制（MAC）层。

Method: 提出了一种联合探测和调度策略，以提高内容检索效率。该策略利用低轨卫星探测卫星-地面链路和多个协同地面站的缓存状态，实现内容的动态用户调度。利用具有两个层面不完全信息的最佳停止理论方法，对卫星-地面混合链路和缓存探测进行实时决策。提出了一种基于阈值的方法来优化探测和调度。

Result: 通过利用协同缓存、卫星-地面链路传输和来自动态用户请求的时间分集，显著提高了平均系统吞吐量。

Conclusion: 所提出的策略能够有效地利用协同缓存、卫星-地面链路传输和动态用户请求的时间分集，从而提高平均系统吞吐量。仿真结果验证了所提出策略的有效性和实用性。

Abstract: Caching is crucial in hybrid satellite-terrestrial networks to reduce
latency, optimize throughput, and improve data availability by storing
frequently accessed content closer to users, especially in bandwidth-limited
satellite systems, requiring strategic Medium Access Control (MAC) layer. This
paper addresses throughput optimization in satellite-terrestrial integrated
networks through opportunistic cooperative caching. We propose a joint probing
and scheduling strategy to enhance content retrieval efficiency. The strategy
leverages the LEO satellite to probe satellite-to-ground links and cache states
of multiple cooperative terrestrial stations, enabling dynamic user scheduling
for content delivery. Using an optimal stopping theoretic approach with two
levels of incomplete information, we make real-time decisions on
satellite-terrestrial hybrid links and caching probing. Our threshold-based
strategy optimizes probing and scheduling, significantly improving average
system throughput by exploiting cooperative caching, satellite-terrestrial link
transmission, and time diversity from dynamic user requests. Simulation results
validate the effectiveness and practicality of the proposed strategies.

</details>


### [330] [Performance Analysis for Multi-User Holographic MIMO Downlink with Matched Filter Precoding](https://arxiv.org/abs/2510.04530)
*Gayathri Shekar,Saman Atapattu,Prathapasinghe Dharmawansa,Kandeepan Sithamparanathan*

Main category: eess.SP

TL;DR: 本篇论文提出了一个分析多用户全息MIMO（HMIMO）下行系统的通信理论框架，重点研究了匹配滤波器（MF）预编码的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在电磁建模或仿真，缺乏通信理论分析，本文旨在填补这一空白。

Method: 推导了包含多径传播、互耦和单元激励的MF信干噪比（SINR）的闭式表达式，并利用双变量伽马分布对全、部分和无信道状态信息（CSI）下的吞吐量进行了近似分析，同时还提出了一个最大最小波束形成问题以评估用户公平性。

Result: MF预编码在低SINR和CSI不确定性下表现出稳健且具有竞争力的性能。

Conclusion: 所提出的分析框架准确有效，为HMIMO系统的设计和优化提供了理论基础。

Abstract: Holographic MIMO (HMIMO) has emerged as a promising solution for future
wireless systems by enabling ultra-dense, spatially continuous antenna
deployments. While prior studies have primarily focused on electromagnetic (EM)
modeling or simulation-based performance analysis, a rigorous
communication-theoretic framework remains largely unexplored. This paper
presents the first analytical performance study of a multi-user HMIMO downlink
system with matched filter (MF) precoding - a low-complexity baseline scheme.
By incorporating multipath propagation, mutual coupling, and element
excitation, we derive a novel closed-form expression for the MF
signal-to-interference-plus-noise ratio (SINR) using an equivalent random
variable model. Leveraging bivariate gamma distributions, we then develop
tractable throughput approximations under full, partial, and no channel state
information (CSI) scenarios. Additionally, we formulate a max-min beamforming
problem to benchmark optimal user fairness performance. Numerical results
validate the accuracy of the proposed framework and reveal that MF precoding
achieves competitive performance with strong robustness to low SINR and CSI
uncertainty.

</details>


### [331] [Coordinated Beamforming for Networked Integrated Communication and Multi-TMT Localization](https://arxiv.org/abs/2510.04600)
*Meidong Xia,Zhenyao He,Wei Xu,Yongming Huang,Derrick Wing Kwan Ng,Naofal Al-Dhahir*

Main category: eess.SP

TL;DR: 网络化集成传感与通信（ISAC）通过将传感信号接收委托给目标监控终端（TMTs），在感知能力和部署灵活性方面展现出优势。本文研究了多TMT时差定位（ToA）的协同波束成形设计。


<details>
  <summary>Details</summary>
Motivation: 尽管网络化ISAC具有潜力，但针对多TMT时差定位的协同波束成形设计仍未得到充分研究。

Method: 本文建立了通信和定位的信号模型，并首次推导了描述定位性能的克拉美-拉克下界（CRLB）。在此基础上，提出了两种优化问题：一个以感知为中心，另一个以通信为中心。对于感知中心问题，当基站天线数量超过通信用户总数时，提出了一种基于半定松弛（SDR）的全局最优算法。对于通信中心问题，对单基站情况，提出了一种基于二分法的全局最优算法。对于一般情况，提出了一种基于连续凸近似（SCA）的统一算法，并将其扩展到多目标场景。

Result: 仿真结果证明了所提算法的有效性，揭示了通信与定位之间的内在性能权衡，并表明在网络化ISAC系统中，部署更多TMTs比部署更多基站更有利。

Conclusion: 本文全面研究了网络化ISAC中的多TMT时差定位协同波束成形问题，提出了有效的算法，并对通信与定位的权衡进行了分析。

Abstract: Networked integrated sensing and communication (ISAC) has gained significant
attention as a promising technology for enabling next-generation wireless
systems. To further enhance networked ISAC, delegating the reception of sensing
signals to dedicated target monitoring terminals (TMTs) instead of base
stations (BSs) offers significant advantages in terms of sensing capability and
deployment flexibility. Despite its potential, the coordinated beamforming
design for networked integrated communication and time-of-arrival (ToA)-based
multi-TMT localization remains largely unexplored. In this paper, we present a
comprehensive study to fill this gap. Specifically, we first establish signal
models for both communication and localization, and, for the first time, derive
a closed-form Cram\'er-Rao lower bound (CRLB) to characterize the localization
performance. Subsequently, we exploit this CRLB to formulate two optimization
problems, focusing on sensing-centric and communication-centric criteria,
respectively. For the sensing-centric problem, we develop a globally optimal
algorithm based on semidefinite relaxation (SDR) when each BS is equipped with
more antennas than the total number of communication users. While for the
communication-centric problem, we design a globally optimal algorithm for the
single-BS case using bisection search. For the general case of both problems,
we propose a unified successive convex approximation (SCA)-based algorithm,
which is suboptimal yet efficient, and further extend it from single-target
scenarios to more practical multi-target scenarios. Finally, simulation results
demonstrate the effectiveness of our proposed algorithms, reveal the intrinsic
performance trade-offs between communication and localization, and further show
that deploying more TMTs is always preferable to deploying more BSs in
networked ISAC systems.

</details>


### [332] [Dimensionally-Efficient Transmission and Storage of Unitary Matrices](https://arxiv.org/abs/2510.04734)
*Juan Vidal Alegría*

Main category: eess.SP

TL;DR: 该论文提出了一种酉矩阵的降维参数化（DEP）方法，将酉矩阵表示为实数序列，维度减半，并探讨了其在存储、传输和量化方面的应用。


<details>
  <summary>Details</summary>
Motivation: 酉矩阵在信号处理中有广泛应用，但其存储和传输效率低下，需要更优化的方法。

Method: 提出了一种酉矩阵的降维参数化（DEP）方法，并推导了其参数化和逆映射。研究了该方法的维度、数值范围，并讨论了在特定约束下的降维可能性。

Result: 所提出的DEP方法能将酉矩阵的维度减半，有效降低存储和传输资源需求。数值结果表明该方法在通用设置和无线通信特定应用中具有潜力。

Conclusion: 该论文提出的酉矩阵降维参数化（DEP）方法能有效降低存储和传输开销，并为量化提供了便利，在信号处理和无线通信领域具有实际应用价值。

Abstract: Unitary matrices are the basis of a large number of signal processing
applications. In many of these applications, finding ways to efficiently store,
and even transmit these matrices, can significantly reduce memory and
throughput requirements. In this work, we study the problem of efficient
transmission and storage of unitary matrices. Specifically, we explicitly
derive a dimensionally-efficient parametrization (DEP) for unitary matrices
that allows identifying them with sequences of real numbers, where the
dimension coincides with the dimension of the unitary group where they lie. We
also characterize its inverse map that allows retrieving the original unitary
matrices from their DEP. The proposed approach effectively allows halving the
dimension with respect to naively considering all the entries of each unitary
matrix, thus reducing the resources required to store and transmit these
matrices. Furthermore, we show that the sequence of real numbers associated to
the proposed DEP is bounded, and we delimit the interval where these numbers
are contained, facilitating the implementation of quantization approaches with
limited distortion. On the other hand, we outline ways to further reduce the
dimension of the DEP when considering more restrictive constraints for matrices
that show up in certain applications. The numerical results showcase the
potential of the proposed approach in general settings, as well as in three
specific applications of current interest for wireless communications research.

</details>


### [333] [Multilayer Non-Terrestrial Networks with Spectrum Access aided by Beyond-Diagonal RIS](https://arxiv.org/abs/2510.04744)
*Wali Ullah Khan,Chandan Kumar Sheemar,Eva Lagunas,Xingwang Li,Symeon Chatzinotas,Petar Popovski,Zhu Han*

Main category: eess.SP

TL;DR: 卫星作为主网络，高空平台站（HAPS）作为认知无线电次网络，用于多用户NTN。通过为HAPS配备透射式BD-RIS天线前端，优化BD-RIS相位响应和HAPS发射功率分配，以满足用户干扰温度约束。。


<details>
  <summary>Details</summary>
Motivation: 为降低传统天线阵列的成本、复杂性和功耗，在多用户NTN中研究了由卫星作为主网络和HAPS作为次网络（充当认知无线电）的系统。

Method: 提出了一种交替优化框架来解决联合优化问题：功率分配子问题通过KKT条件得到水填充式解，BD-RIS配置通过黎曼流形优化得到。

Result: 仿真结果表明，与对角RIS辅助的基准相比，数据速率显著提高，干扰抑制能力增强。

Conclusion: BD-RIS有望成为未来多层NTN的关键技术。

Abstract: In this work, we study a multi-user NTN in which a satellite serves as the
primary network and a high-altitude platform station (HAPS) operates as the
secondary network, acting as a cognitive radio. To reduce the cost, complexity,
and power consumption of conventional antenna arrays, we equip the HAPS with a
transmissive BD-RIS antenna front end. We then formulate a joint optimization
problem for the BD-RIS phase response and the HAPS transmit power allocation
under strict per-user interference temperature constraints. To tackle the
resulting highly nonconvex problem, we propose an alternating-optimization
framework: the power-allocation subproblem admits a closed-form,
water-filling-type solution derived from the Karush-Kuhn-Tucker (KKT)
conditions, while the BD-RIS configuration is refined via Riemannian manifold
optimization. Simulation results show significant gains in data rate and
interference suppression over diagonal RIS-assisted benchmarks, establishing
BD-RIS as a promising enabler for future multilayer NTNs.

</details>


### [334] [Interference Alignment for Multi-cluster Over-the-Air Computation](https://arxiv.org/abs/2510.04745)
*Lucas Sempéré,Yue Bi,Yue Wu,Pengwenlong Gu,Selma Boumerdassi*

Main category: eess.SP

TL;DR: 该研究提出了一种新颖的干扰对齐（IA）方案，用于解决物联网（IoT）网络中由大量设备同时通信引起的干扰问题，特别是在多簇网络中。


<details>
  <summary>Details</summary>
Motivation: 物联网网络面临的主要挑战之一是管理大量设备同时通信引起的干扰，尤其是在多簇网络中。过顶计算（AirComp）虽然是一种有前景的解决方案，但在密集、受干扰限制的环境中性能会受到影响。

Method: 提出了一种新颖的干扰对齐（IA）方案，适用于上行链路AirComp系统。该方案可扩展到任意数量的簇K，并允许每个簇利用一半的可用信道，而不是像时分复用那样只利用1/K。此外，还开发了适用于用户在相邻簇之间共享的场景的方案。

Result: 与先前的方法不同，该方案可扩展到任意数量的簇K，并允许每个簇利用一半的可用信道，而不是像时分复用那样只利用1/K。

Conclusion: 所提出的IA方案能够有效地管理多簇AirComp网络中的干扰，并提高了系统的性能。

Abstract: One of the main challenges facing Internet of Things (IoT) networks is
managing interference caused by the large number of devices communicating
simultaneously, particularly in multi-cluster networks where multiple devices
simultaneously transmit to their respective receiver. Over-the-Air Computation
(AirComp) has emerged as a promising solution for efficient real-time data
aggregation, yet its performance suffers in dense, interference-limited
environments. To address this, we propose a novel Interference Alignment (IA)
scheme tailored for up-link AirComp systems. Unlike previous approaches, the
proposed method scales to an arbitrary number $\sf K$ of clusters and enables
each cluster to exploit half of the available channels, instead of only
$\tfrac{1}{\sf K}$ as in time-sharing. In addition, we develop schemes tailored
to scenarios where users are shared between adjacent clusters.

</details>


### [335] [The IEEE Signal Processing Society's Leading Role in Developing Standards for Computational Imaging and Sensing: Part II](https://arxiv.org/abs/2510.04913)
*Andreas Bathelt,Benjamin Deutschmann,Hyeon Seok Rou,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Peter Vouras*

Main category: eess.SP

TL;DR: 分布式传感器和通信/传感系统通过利用额外的自由度来扩展硬件能力，IEEE信号处理协会的SASC在其中扮演了关键角色，特别是在P3383（ISAC）和P3343（分布式传感器时空同步）工作组中。


<details>
  <summary>Details</summary>
Motivation: 物理硬件限制了成像或传感应用的性能，需要利用额外的自由度来克服这些限制。

Method: 通过同步分布式传感器合成更大的孔径，或通过集成通信和传感功能来优化波形和资源管理。

Result: 标准化对于确保不同供应商系统的互操作性以及定义行业最佳实践至关重要。

Conclusion: IEEE信号处理协会的SASC，特别是P3383和P3343工作组，正在推动计算传感技术的高质量标准发展。

Abstract: In every imaging or sensing application, the physical hardware creates
constraints that must be overcome or they limit system performance. Techniques
that leverage additional degrees of freedom can effectively extend performance
beyond the inherent physical capabilities of the hardware. An example includes
synchronizing distributed sensors so as to synthesize a larger aperture for
remote sensing applications. An additional example is integrating the
communication and sensing functions in a wireless system through the clever
design of waveforms and optimized resource management. As these technologies
mature beyond the conceptual and prototype phase they will ultimately
transition to the commercial market. Here, standards play a critical role in
ensuring success. Standards ensure interoperability between systems
manufactured by different vendors and define industry best practices for
vendors and customers alike. The Signal Processing Society of the Institute for
Electrical and Electronics Engineers (IEEE) plays a leading role in developing
high-quality standards for computational sensing technologies through the
working groups of the Synthetic Aperture Standards Committee (SASC). In this
column we highlight the standards activities of the P3383 Performance Metrics
for Integrated Sensing and Communication (ISAC) Systems Working Group and the
P3343 Spatio-Temporal Synchronization of a Synthetic Aperture of Distributed
Sensors Working Group.

</details>


### [336] [Steady-State Spread Bounds for Graph Diffusion via Laplacian Regularisation](https://arxiv.org/abs/2510.04924)
*Ardavan Rahimian*

Main category: eess.SP

TL;DR: 研究了在图上扩散过程在拉普拉斯正则化产生的初始模式下偏离的程度，给出了稳态传播的上界，该上界与图的最大节点度数和正则化强度有关。


<details>
  <summary>Details</summary>
Motivation: 研究了在图上扩散过程在拉普拉斯正则化产生的初始模式下偏离的程度。

Method: 在无向、非负图的稳定条件下，给出了稳态传播的闭式、实例特定的上界，衡量的是最终和初始分布的相对变化。

Result: 上界包含两部分：一个由图的最大节点度数决定的不可约项，以及一个随着正则化强度增加而缩小的设计控制项（遵循反平方根定律）。

Conclusion: 存在一个简单的设计规则：给定任何目标传播限制，可以闭式选择足够的正则化强度。该保证是非渐近的、易于计算的，并证明了可以发生多少稳态偏差。

Abstract: We study how far a diffusion process on a graph can drift from a designed
starting pattern when that pattern is produced using Laplacian regularisation.
Under standard stability conditions for undirected, entrywise nonnegative
graphs, we give a closed-form, instance-specific upper bound on the
steady-state spread, measured as the relative change between the final and
initial profiles. The bound separates two effects: (i) an irreducible term
determined by the graph's maximum node degree, and (ii) a design-controlled
term that shrinks as the regularisation strength increases (following an
inverse square-root law). This leads to a simple design rule: given any target
limit on spread, one can choose a sufficient regularisation strength in closed
form. Although one motivating application is array beamforming, where the
initial pattern is the squared magnitude of the beamformer weights, the result
applies to any scenario that first enforces Laplacian smoothness and then
evolves by linear diffusion on a graph. Overall, the guarantee is
non-asymptotic, easy to compute, and certifies how much steady-state deviation
can occur.

</details>


### [337] [My First Five Years of Faculty Career at the University of Delaware](https://arxiv.org/abs/2510.05000)
*Xiang-Gen Xia*

Main category: eess.SP

TL;DR: 该文章是对作者在美国大学前五年的学术研究的高度评价和总结。


<details>
  <summary>Details</summary>
Motivation: 作者希望分享其初级学术生涯的经验，为年轻研究者提供帮助。

Method: 文章是对作者研究成果的总结和个人经验的分享。

Result: 作者认为其研究成果是职业生涯中的最佳。

Conclusion: 作者希望其经验能对年轻研究者有所助益。

Abstract: In this short article, I would like to briefly summarize my research in the
first 5 years in my university academia life in USA. I think that my research
results obtained in these 5 years are the best in my career, at least which I
like the most by myself. I wish that my experience in my junior academia career
could be of some help to young researchers.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [338] [Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer](https://arxiv.org/abs/2510.03342)
*Abbas Abdolmaleki,Saminda Abeyruwan,Joshua Ainslie,Jean-Baptiste Alayrac,Montserrat Gonzalez Arenas,Ashwin Balakrishna,Nathan Batchelor,Alex Bewley,Jeff Bingham,Michael Bloesch,Konstantinos Bousmalis,Philemon Brakel,Anthony Brohan,Thomas Buschmann,Arunkumar Byravan,Serkan Cabi,Ken Caluwaerts,Federico Casarini,Christine Chan,Oscar Chang,London Chappellet-Volpini,Jose Enrique Chen,Xi Chen,Hao-Tien Lewis Chiang,Krzysztof Choromanski,Adrian Collister,David B. D'Ambrosio,Sudeep Dasari,Todor Davchev,Meet Kirankumar Dave,Coline Devin,Norman Di Palo,Tianli Ding,Carl Doersch,Adil Dostmohamed,Yilun Du,Debidatta Dwibedi,Sathish Thoppay Egambaram,Michael Elabd,Tom Erez,Xiaolin Fang,Claudio Fantacci,Cody Fong,Erik Frey,Chuyuan Fu,Ruiqi Gao,Marissa Giustina,Keerthana Gopalakrishnan,Laura Graesser,Oliver Groth,Agrim Gupta,Roland Hafner,Steven Hansen,Leonard Hasenclever,Sam Haves,Nicolas Heess,Brandon Hernaez,Alex Hofer,Jasmine Hsu,Lu Huang,Sandy H. Huang,Atil Iscen,Mithun George Jacob,Deepali Jain,Sally Jesmonth,Abhishek Jindal,Ryan Julian,Dmitry Kalashnikov,M. Emre Karagozler,Stefani Karp,Matija Kecman,J. Chase Kew,Donnie Kim,Frank Kim,Junkyung Kim,Thomas Kipf,Sean Kirmani,Ksenia Konyushkova,Li Yang Ku,Yuheng Kuang,Thomas Lampe,Antoine Laurens,Tuan Anh Le,Isabel Leal,Alex X. Lee,Tsang-Wei Edward Lee,Guy Lever,Jacky Liang,Li-Heng Lin,Fangchen Liu,Shangbang Long,Caden Lu,Sharath Maddineni,Anirudha Majumdar,Kevis-Kokitsi Maninis,Andrew Marmon,Sergio Martinez,Assaf Hurwitz Michaely,Niko Milonopoulos,Joss Moore,Robert Moreno,Michael Neunert,Francesco Nori,Joy Ortiz,Kenneth Oslund,Carolina Parada,Emilio Parisotto,Amaris Paryag,Acorn Pooley,Thomas Power,Alessio Quaglino,Haroon Qureshi,Rajkumar Vasudeva Raju,Helen Ran,Dushyant Rao,Kanishka Rao,Isaac Reid,David Rendleman,Krista Reymann,Miguel Rivas,Francesco Romano,Yulia Rubanova,Peter Pastor Sampedro,Pannag R Sanketi,Dhruv Shah,Mohit Sharma,Kathryn Shea,Mohit Shridhar,Charles Shu,Vikas Sindhwani,Sumeet Singh,Radu Soricut,Rachel Sterneck,Ian Storz,Razvan Surdulescu,Jie Tan,Jonathan Tompson,Saran Tunyasuvunakool,Jake Varley,Grace Vesom,Giulia Vezzani,Maria Bauza Villalonga,Oriol Vinyals,René Wagner,Ayzaan Wahid,Stefan Welker,Paul Wohlhart,Chengda Wu,Markus Wulfmeier,Fei Xia,Ted Xiao,Annie Xie,Jinyu Xie,Peng Xu,Sichun Xu,Ying Xu,Zhuo Xu,Jimmy Yan,Sherry Yang,Skye Yang,Yuxiang Yang,Hiu Hong Yu,Wenhao Yu,Wentao Yuan,Yuan Yuan,Jingwei Zhang,Tingnan Zhang,Zhiyuan Zhang,Allan Zhou,Guangyao Zhou,Yuxiang Zhou*

Main category: cs.RO

TL;DR: Gemini Robotics 1.5 和 Gemini Robotics-ER 1.5 是新一代机器人模型，具有新架构、运动迁移机制、多级内部推理和高级具身推理能力，能够更好地感知、思考和行动，以解决复杂的多步任务。


<details>
  <summary>Details</summary>
Motivation: 通用机器人需要深入理解物理世界、高级推理以及通用灵巧的控制能力。

Method: Gemini Robotics 1.5 采用新颖的架构和运动迁移（MT）机制，能够从异构、多实体机器人数据中学习，使视觉-语言-动作（VLA）模型更加通用。它还将动作与自然语言的多级内部推理过程交织在一起，实现了“三思而后行”，提高了分解和执行复杂多步任务的能力，并使机器人的行为更具可解释性。Gemini Robotics-ER 1.5 在具身推理方面达到了新的最先进水平，包括视觉和空间理解、任务规划和进度估计等关键能力。

Result: Gemini Robotics 1.5 和 Gemini Robotics-ER 1.5 在具身推理、复杂任务分解和执行以及机器人行为可解释性方面取得了显著进展。

Conclusion: 这一模型家族的进步使我们朝着物理智能代理时代迈进了一步，使机器人能够感知、思考和行动，从而解决复杂的多步任务。

Abstract: General-purpose robots need a deep understanding of the physical world,
advanced reasoning, and general and dexterous control. This report introduces
the latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,
a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER
1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together
three major innovations. First, Gemini Robotics 1.5 features a novel
architecture and a Motion Transfer (MT) mechanism, which enables it to learn
from heterogeneous, multi-embodiment robot data and makes the VLA more general.
Second, Gemini Robotics 1.5 interleaves actions with a multi-level internal
reasoning process in natural language. This enables the robot to "think before
acting" and notably improves its ability to decompose and execute complex,
multi-step tasks, and also makes the robot's behavior more interpretable to the
user. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for
embodied reasoning, i.e., for reasoning capabilities that are critical for
robots, such as visual and spatial understanding, task planning, and progress
estimation. Together, this family of models takes us a step towards an era of
physical agents-enabling robots to perceive, think and then act so they can
solve complex multi-step tasks.

</details>


### [339] [Optimal swimming with body compliance in an overdamped medium](https://arxiv.org/abs/2510.03457)
*Jianfeng Lin,Tianyu Wang,Baxi Chong,Matthew Fernandez,Zhaochen Xu,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 该研究将几何力学扩展到具有顺应性的波动式游泳者，以预测和优化其运动性能。


<details>
  <summary>Details</summary>
Motivation: 现有几何力学方法在模拟中假设精确的步态执行，忽略了动物或机器人在与环境互动时由于身体顺应性而产生的扰动。本研究旨在解决这一限制，将顺应性纳入模型，以更准确地预测和优化波动式游泳者的运动。

Method: 研究引入了包含串联弹簧的渐进式三连杆游泳者模型，并采用电阻力理论推导身体动力学。将几何力学整合到运动预测和优化框架中，以寻找最大位移的策略。该框架在实际的电缆驱动三连杆无肢机器人上进行了验证，证明了其在颗粒介质中预测和优化运动性能的准确性。

Result: 研究结果表明，该物理模型能够准确预测并优化具有状态相关顺应性的波动式游泳者的运动性能，尤其是在颗粒介质中。

Conclusion: 该研究建立了一种基于物理学的系统性方法，用于模拟和控制具有顺应性的波动式游泳运动。研究强调，顺应性可以被视为一种设计特性，能够有效利用，以实现机器人或生物在均质和非均质环境中稳健运动。

Abstract: Elongate animals and robots use undulatory body waves to locomote through
diverse environments. Geometric mechanics provides a framework to model and
optimize such systems in highly damped environments, connecting a prescribed
shape change pattern (gait) with locomotion displacement. However, existing
approaches assume precise execution of prescribed gaits, whereas in practice
environmental interactions with compliant bodies of animals or robots
frequently perturb the realized trajectories. In this work, we extend geometric
mechanics to predict locomotor performance and search for optimal swimming
strategy of compliant undulators. We introduce a compliant extension of
Purcell's three-link swimmer by incorporating series-connected springs at the
joints. Body dynamics are derived with resistive force theory. Geometric
mechanics is incorporated into movement prediction and into an optimization
framework that identifies strategies for controlling compliant swimmers to
achieve maximal displacement. We validate our framework on a physical
cable-driven three-link limbless robot, and demonstrate accurate prediction and
optimization of locomotor performance under varied programmed, state-dependent
compliance in a granular medium. Our results establish a systematic
physics-based approach for modeling and controlling compliant swimming
locomotion, highlighting compliance as a design feature that can be exploited
for robust movement in homogeneous and heterogeneous environments.

</details>


### [340] [Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching](https://arxiv.org/abs/2510.03460)
*Sibo Tian,Minghui Zheng,Xiao Liang*

Main category: cs.RO

TL;DR: 该研究提出了一种新颖的学习方法，利用基于流匹配的模型，以单视角点云作为条件，为优化初始化学习近乎最优的解决方案，解决了现有采样和优化运动规划器在实时性、复杂环境处理和局部最优性方面存在的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前基于采样的运动规划器在处理高维配置空间时面临挑战，并且通常需要后处理来插值和平滑路径，导致在复杂环境中效率低下。基于优化的规划器虽然能直接生成平滑轨迹，但对初始化敏感且容易陷入局部最优。因此，需要一种更高效、更鲁棒的运动规划方法，特别是在实时性要求高的人机协作场景中。

Method: 提出了一种基于学习的方法，该方法利用基于流匹配的模型，以单视角点云作为条件，学习优化初始化的近优解。该方法不需要预先了解环境信息，可以直接从单视角深度相机输入生成可行轨迹。

Result: 在UR5e机械臂和杂乱工作空间的仿真研究表明，所提出的生成式初始化器在自身即可达到高成功率，并且与传统的以及基于学习的基准初始化器相比，显著提高了轨迹优化的成功率，同时所需的优化迭代次数更少，并且对未见过的环境表现出很强的泛化能力。

Conclusion: 所提出的学习方法能够有效地为运动规划优化提供初始化，解决了现有方法的不足，提高了运动规划的效率和成功率，特别适用于动态和复杂的人机协作环境。

Abstract: Rapid robot motion generation is critical in Human-Robot Collaboration (HRC)
systems, as robots need to respond to dynamic environments in real time by
continuously observing their surroundings and replanning their motions to
ensure both safe interactions and efficient task execution. Current
sampling-based motion planners face challenges in scaling to high-dimensional
configuration spaces and often require post-processing to interpolate and
smooth the generated paths, resulting in time inefficiency in complex
environments. Optimization-based planners, on the other hand, can incorporate
multiple constraints and generate smooth trajectories directly, making them
potentially more time-efficient. However, optimization-based planners are
sensitive to initialization and may get stuck in local minima. In this work, we
present a novel learning-based method that utilizes a Flow Matching model
conditioned on a single-view point cloud to learn near-optimal solutions for
optimization initialization. Our method does not require prior knowledge of the
environment, such as obstacle locations and geometries, and can generate
feasible trajectories directly from single-view depth camera input. Simulation
studies on a UR5e robotic manipulator in cluttered workspaces demonstrate that
the proposed generative initializer achieves a high success rate on its own,
significantly improves the success rate of trajectory optimization compared
with traditional and learning-based benchmark initializers, requires fewer
optimization iterations, and exhibits strong generalization to unseen
environments.

</details>


### [341] [A Simulation Evaluation Suite for Robust Adaptive Quadcopter Control](https://arxiv.org/abs/2510.03471)
*Dingqi Zhang,Ran Tao,Sheng Cheng,Naira Hovakimyan,Mark W. Mueller*

Main category: cs.RO

TL;DR: 该论文提出了一个用于四旋翼飞行器控制的模块化、易于部署的模拟测试平台，名为AdaptiveQuadBench，基于RotorPy构建。该平台旨在解决当前评估鲁棒自适应控制方法时存在的碎片化问题，通过在各种干扰（如风、载荷变化、转子故障、控制延迟）下进行系统性比较。平台包含一个控制器库和任务相关指标，用于评估跟踪精度和鲁棒性。通过自动化压力测试等示例，展示了该测试平台的通用性和在系统分析中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的鲁棒自适应控制方法评估分散在不同任务、模拟器和实现中，阻碍了系统性比较。因此，需要一个统一的测试平台来促进对这些方法的全面评估。

Method: 构建了一个基于RotorPy的、易于部署的、模块化的四旋翼飞行器控制模拟测试平台。该平台集成了多种干扰模型（风、载荷变化、转子故障、控制延迟）、控制器库（自适应和非自适应）以及评估指标，支持在不同任务和干扰场景下进行可重复的评估。

Result: 展示了该测试平台在多种干扰场景和轨迹类型下的通用性，包括自动化压力测试，证明了其在系统分析中的有效性。

Conclusion: 提出的AdaptiveQuadBench模拟测试平台为系统地评估和比较四旋翼飞行器的鲁棒自适应控制方法提供了一个统一、可重复且易于使用的解决方案。

Abstract: Robust adaptive control methods are essential for maintaining quadcopter
performance under external disturbances and model uncertainties. However,
fragmented evaluations across tasks, simulators, and implementations hinder
systematic comparison of these methods. This paper introduces an
easy-to-deploy, modular simulation testbed for quadcopter control, built on
RotorPy, that enables evaluation under a wide range of disturbances such as
wind, payload shifts, rotor faults, and control latency. The framework includes
a library of representative adaptive and non-adaptive controllers and provides
task-relevant metrics to assess tracking accuracy and robustness. The unified
modular environment enables reproducible evaluation across control methods and
eliminates redundant reimplementation of components such as disturbance models,
trajectory generators, and analysis tools. We illustrate the testbed's
versatility through examples spanning multiple disturbance scenarios and
trajectory types, including automated stress testing, to demonstrate its
utility for systematic analysis. Code is available at
https://github.com/Dz298/AdaptiveQuadBench.

</details>


### [342] [Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems](https://arxiv.org/abs/2510.03472)
*Yulun Zhang,Alexandre O. G. Barbosa,Federico Pecora,Jiaoyang Li*

Main category: cs.RO

TL;DR: 本文提出了一种基于进化算法和混合整数线性规划的方法来优化机器人分拣系统中目的地到滑槽的任务映射，以提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 机器人分拣系统（RSS）中的目的地到滑槽任务映射优化是一个具有挑战性的问题，因为这涉及到机器人路径规划、滑槽的动态关闭以及如何减少包裹处理时间。

Method: 本文首先定义了任务映射和任务映射优化（TMO）问题，然后提出了一个RSS模拟器来评估任务映射。接着，使用进化算法和混合整数线性规划来解决TMO问题，并使用质量多样性算法来分析不同任务映射的吞吐量。

Result: 与贪心策略生成的映射相比，本文提出的优化方法在不同大小、滑槽数量和目的地的RSS设置中都表现出了更优的任务映射，从而提高了系统吞吐量。

Conclusion: 优化的目的地到滑槽的任务映射可以显著提高机器人分拣系统的吞吐量。

Abstract: We study optimizing a destination-to-chutes task mapping to improve
throughput in Robotic Sorting Systems (RSS), where a team of robots sort
packages on a sortation floor by transporting them from induct workstations to
eject chutes based on their shipping destinations (e.g. Los Angeles or
Pittsburgh). The destination-to-chutes task mapping is used to determine which
chutes a robot can drop its package. Finding a high-quality task mapping is
challenging because of the complexity of a real-world RSS. First, optimizing
task mapping is interdependent with robot target assignment and path planning.
Second, chutes will be CLOSED for a period of time once they receive sufficient
packages to allow for downstream processing. Third, task mapping quality
directly impacts the downstream processing, as scattered chutes for the same
destination increase package handling time. In this paper, we first formally
define task mappings and the problem of Task Mapping Optimization (TMO). We
then present a simulator of RSS to evaluate task mappings. We then present a
simple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear
Programming, demonstrating the advantage of our optimized task mappings over
the greedily generated ones in various RSS setups with different map sizes,
numbers of chutes, and destinations. Finally, we use Quality Diversity
algorithms to analyze the throughput of a diverse set of task mappings. Our
code is available online at https://github.com/lunjohnzhang/tmo_public.

</details>


### [343] [Robust Permissive Controller Synthesis for Interval MDPs](https://arxiv.org/abs/2510.03481)
*Khang Vo Huynh,David Parker,Lu Feng*

Main category: cs.RO

TL;DR: 我们提出了第一个用于不确定动态机器人（IMDPs）的鲁棒允许性控制器合成框架，该框架通过混合整数线性规划（MILP）解决了这一问题，并提出了两种编码方法：一种基于顶点枚举，另一种基于对偶性，以确保在所有允许的转换下，所有符合合成的多重策略都满足可达性或基于奖励的规范。


<details>
  <summary>Details</summary>
Motivation: 机器人通常在动力学不确定的情况下运行，而传统的控制器合成方法假设精确的转换概率，这在实际应用中是不切实际的。因此，有必要开发一种能够处理不确定性和提供运行时灵活性和韧性的控制器合成方法。

Method: 该研究提出了一种用于IMDPs的鲁棒允许性控制器合成框架。该框架将问题公式化为混合整数线性规划（MILP），并提出了一种基于顶点枚举的基线方法和一种基于对偶性的可扩展方法，以避免显式枚举。这两种方法都可以合成鲁棒的、最大允许的控制器。

Result: 实验结果表明，该框架在四个基准域上能够合成鲁棒的、最大允许的控制器，并且能够扩展到具有大量状态（多达数十万个）的IMDP。

Conclusion: 本研究成功地开发了一种用于IMDPs的鲁棒允许性控制器合成框架，解决了机器人动力学不确定性的实际问题。提出的基于MILP的方法，包括基于对偶性的方法，在可扩展性和性能方面都显示出有希望的结果。

Abstract: We address the problem of robust permissive controller synthesis for robots
operating under uncertain dynamics, modeled as Interval Markov Decision
Processes (IMDPs). IMDPs generalize standard MDPs by allowing transition
probabilities to vary within intervals, capturing epistemic uncertainty from
sensing noise, actuation imprecision, and coarse system abstractions-common in
robotics. Traditional controller synthesis typically yields a single
deterministic strategy, limiting adaptability. In contrast, permissive
controllers (multi-strategies) allow multiple actions per state, enabling
runtime flexibility and resilience. However, prior work on permissive
controller synthesis generally assumes exact transition probabilities, which is
unrealistic in many robotic applications. We present the first framework for
robust permissive controller synthesis on IMDPs, guaranteeing that all
strategies compliant with the synthesized multi-strategy satisfy reachability
or reward-based specifications under all admissible transitions. We formulate
the problem as mixed-integer linear programs (MILPs) and propose two encodings:
a baseline vertex-enumeration method and a scalable duality-based method that
avoids explicit enumeration. Experiments on four benchmark domains show that
both methods synthesize robust, maximally permissive controllers and scale to
large IMDPs with up to hundreds of thousands of states.

</details>


### [344] [Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*](https://arxiv.org/abs/2510.03496)
*Vadivelan Murugesan,Rajasundaram Mathiazhagan,Sanjana Joshi,Aliasghar Arab*

Main category: cs.RO

TL;DR: 该研究提出了一种预测驱动的安全规划框架，通过结合精细的人体运动预测和基于物理的数字孪生来提高人机协作中的碰撞避免能力。


<details>
  <summary>Details</summary>
Motivation: 为了实现主动避碰，人机协作需要精确预测人体的远期运动，现有方法仅依赖运动学模型，而本研究旨在改进这一点。

Method: 提出了一种预测驱动的安全规划框架，该框架利用经过物理仿真数字孪生验证的、逐关节的精细人体运动预测。利用基于胶囊的人工势场（APF）将预测转化为碰撞风险指标，当风险超过阈值时触发自适应RRT*（A-RRT*）规划器。使用深度相机提取3D骨骼姿态，并利用CNN-BiLSTM模型预测未来的关节轨迹。数字孪生模型集成了实时人体姿态预测，并置于模拟机器人前以评估运动和物理接触。

Result: 在50次试验中，该方法实现了100%的主动避碰，避碰裕度大于250毫米，重规划时间小于2秒，证明了其在结合预测性人体建模和数字孪生验证方面的精确性和可靠性优于现有的仅考虑运动学的规划器。

Conclusion: 该框架通过整合预测性人体建模和数字孪生验证，能够提前验证规划轨迹，并弥合实时更新规划轨迹的潜在延迟，从而在人机协作中实现高精度和高可靠性的主动避碰。

Abstract: Human-robot collaboration requires precise prediction of human motion over
extended horizons to enable proactive collision avoidance. Unlike existing
planners that rely solely on kinodynamic models, we present a prediction-driven
safe planning framework that leverages granular, joint-by-joint human motion
forecasting validated in a physics-based digital twin. A capsule-based
artificial potential field (APF) converts these granular predictions into
collision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when
thresholds are exceeded. The depth camera is used to extract 3D skeletal poses
and a convolutional neural network-bidirectional long short-term memory
(CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A
digital twin model integrates real-time human posture prediction placed in
front of a simulated robot to evaluate motions and physical contacts. The
proposed method enables validation of planned trajectories ahead of time and
bridging potential latency gaps in updating planned trajectories in real-time.
In 50 trials, our method achieved 100% proactive avoidance with > 250 mm
clearance and sub-2 s replanning, demonstrating superior precision and
reliability compared to existing kinematic-only planners through the
integration of predictive human modeling with digital twin validation.

</details>


### [345] [Distributed Connectivity Maintenance and Recovery for Quadrotor Motion Planning](https://arxiv.org/abs/2510.03504)
*Yutong Wang,Yichun Qu,Tengxiang Wang,Lishuo Pan,Nora Ayanian*

Main category: cs.RO

TL;DR: 本文提出了一个用于多机器人导航的实时分布式框架，利用高阶控制障碍函数（HOCBFs）来控制机器人间的接近度，以维持连通性并避免碰撞。框架结合了控制李雅普诺夫函数（CLF）来实现从初始断开或暂时断开状态的连通性恢复，从而在充满障碍的环境中实现鲁棒的连通性。此外，还提出了一种通过贝塞尔参数化轨迹的轨迹生成框架，可同时进行规划和控制，并自然地提供任意阶导数的平滑曲线。主要贡献是统一的MPC-CLF-CBF框架，一种用于多机器人系统连通性维持和恢复的连续时间轨迹生成和控制方法。该框架已通过大量模拟和4架Crazyflie纳米四旋翼飞行器的物理实验进行了验证。


<details>
  <summary>Details</summary>
Motivation: 在多机器人应用中，维持连通性至关重要，但易受障碍物和视觉遮挡的影响。

Method: 提出一个实时分布式框架，使用高阶控制障碍函数（HOCBFs）控制机器人间的接近度以维持连通性并避免碰撞。结合控制李雅普诺夫函数（CLF）实现连通性恢复。通过贝塞尔参数化轨迹的轨迹生成框架同时进行规划和控制。

Result: 通过大量的模拟和4架Crazyflie纳米四旋翼飞行器的物理实验验证了该框架的有效性。

Conclusion: 本文提出了一个统一的MPC-CLF-CBF框架，一种用于多机器人系统连通性维持和恢复的连续时间轨迹生成和控制方法，并验证了其在复杂环境下的鲁棒性。

Abstract: Maintaining connectivity is crucial in many multi-robot applications, yet
fragile to obstacles and visual occlusions. We present a real-time distributed
framework for multi-robot navigation certified by high-order control barrier
functions (HOCBFs) that controls inter-robot proximity to maintain connectivity
while avoiding collisions. We incorporate control Lyapunov functions to enable
connectivity recovery from initial disconnected configurations and temporary
losses, providing robust connectivity during navigation in obstacle-rich
environments. Our trajectory generation framework concurrently produces
planning and control through a Bezier-parameterized trajectory, which naturally
provides smooth curves with arbitrary degree of derivatives. The main
contribution is the unified MPC-CLF-CBF framework, a continuous-time trajectory
generation and control method for connectivity maintenance and recovery of
multi-robot systems. We validate the framework through extensive simulations
and a physical experiment with 4 Crazyflie nano-quadrotors.

</details>


### [346] [LapSurgie: Humanoid Robots Performing Surgery via Teleoperated Handheld Laparoscopy](https://arxiv.org/abs/2510.03529)
*Zekai Liang,Xiao Liang,Soofiyan Atar,Sreyan Das,Zoe Chiu,Peihan Zhang,Florian Richter,Shanglei Liu,Michael C. Yip*

Main category: cs.RO

TL;DR: Despite robotic laparoscopic surgery's potential, its high cost limits access in underserved areas. This paper introduces LapSurgie, a humanoid-robot-based teleoperation framework that uses an inverse-mapping strategy for precise control of standard laparoscopic instruments, enabling deployment in human-designed environments without major modifications. A user study validates its effectiveness and feasibility for remote laparoscopic surgery.


<details>
  <summary>Details</summary>
Motivation: Robotic laparoscopic surgery is not widely adopted in rural and low-resource regions due to high costs, exacerbating healthcare disparities. This work aims to address this gap by exploring humanoid robotic systems for deployable surgical solutions.

Method: The paper introduces LapSurgie, a humanoid-robot-based laparoscopic teleoperation framework. It employs an inverse-mapping strategy for manual-wristed laparoscopic instruments to ensure precise hand-to-tool control while adhering to remote center-of-motion constraints. This allows the use of off-the-shelf surgical tools without special setup. A control console with a stereo vision system provides real-time visual feedback.

Result: A comprehensive user study across multiple platforms demonstrated the effectiveness of the LapSurgie framework. The study provided initial evidence supporting the feasibility of using humanoid robots for laparoscopic procedures.

Conclusion: The LapSurgie framework, a humanoid-robot-based teleoperation system, is effective and feasible for deploying laparoscopic procedures in underserved areas by enabling precise control of standard instruments in human-designed environments.

Abstract: Robotic laparoscopic surgery has gained increasing attention in recent years
for its potential to deliver more efficient and precise minimally invasive
procedures. However, adoption of surgical robotic platforms remains largely
confined to high-resource medical centers, exacerbating healthcare disparities
in rural and low-resource regions. To close this gap, a range of solutions has
been explored, from remote mentorship to fully remote telesurgery. Yet, the
practical deployment of surgical robotic systems to underserved communities
remains an unsolved challenge. Humanoid systems offer a promising path toward
deployability, as they can directly operate in environments designed for humans
without extensive infrastructure modifications -- including operating rooms. In
this work, we introduce LapSurgie, the first humanoid-robot-based laparoscopic
teleoperation framework. The system leverages an inverse-mapping strategy for
manual-wristed laparoscopic instruments that abides to remote center-of-motion
constraints, enabling precise hand-to-tool control of off-the-shelf surgical
laparoscopic tools without additional setup requirements. A control console
equipped with a stereo vision system provides real-time visual feedback.
Finally, a comprehensive user study across platforms demonstrates the
effectiveness of the proposed framework and provides initial evidence for the
feasibility of deploying humanoid robots in laparoscopic procedures.

</details>


### [347] [Efficient Surgical Robotic Instrument Pose Reconstruction in Real World Conditions Using Unified Feature Detection](https://arxiv.org/abs/2510.03532)
*Zekai Liang,Kazuya Miyata,Xiao Liang,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: 该研究提出了一种新颖的框架，通过共享编码统一检测几何图元（关键点和轴边缘），从而实现高效的姿态估计。


<details>
  <summary>Details</summary>
Motivation: 精确的相机到机器人标定对于基于视觉的机器人控制系统至关重要，尤其是在微创手术机器人中，但现有方法在处理长运动链和部分可见性方面存在挑战。

Method: 提出了一种新颖的框架，通过共享编码统一检测几何图元（关键点和轴边缘），并利用投影几何进行姿态估计。该框架在具有项目式标签的大规模合成数据上进行训练。

Result: 所提出的方法在特征检测和姿态估计方面均表现出色，定性和定量结果均表明其在具有挑战性的外科环境中的快速性能和最先进的准确性。

Conclusion: 该研究提出的框架通过单一推理即可检测关键点和边缘，并在具有挑战性的外科环境中实现了快速、准确的相机到机器人标定。

Abstract: Accurate camera-to-robot calibration is essential for any vision-based
robotic control system and especially critical in minimally invasive surgical
robots, where instruments conduct precise micro-manipulations. However, MIS
robots have long kinematic chains and partial visibility of their degrees of
freedom in the camera, which introduces challenges for conventional
camera-to-robot calibration methods that assume stiff robots with good
visibility. Previous works have investigated both keypoint-based and
rendering-based approaches to address this challenge in real-world conditions;
however, they often struggle with consistent feature detection or have long
inference times, neither of which are ideal for online robot control. In this
work, we propose a novel framework that unifies the detection of geometric
primitives (keypoints and shaft edges) through a shared encoding, enabling
efficient pose estimation via projection geometry. This architecture detects
both keypoints and edges in a single inference and is trained on large-scale
synthetic data with projective labeling. This method is evaluated across both
feature detection and pose estimation, with qualitative and quantitative
results demonstrating fast performance and state-of-the-art accuracy in
challenging surgical environments.

</details>


### [348] [Shape-Space Graphs: Fast and Collision-Free Path Planning for Soft Robots](https://arxiv.org/abs/2510.03547)
*Carina Veil,Moritz Flaschel,Ellen Kuhl*

Main category: cs.RO

TL;DR: A graph-based path planning tool is presented for a soft robotic arm, enabling efficient and collision-free motion planning in cluttered environments by searching in 'shape space'.


<details>
  <summary>Details</summary>
Motivation: Soft robots, despite their flexibility, face challenges in motion planning due to their complex kinematics, especially in cluttered environments. This work aims to address this challenge by developing a path planning tool for an elephant-trunk-inspired soft robotic arm.

Method: A biomechanical model inspired by morphoelasticity and active filament theory is used to precompute a shape library for a soft robotic arm with three artificial muscle fibers. A k-nearest neighbor graph is constructed in shape space, with nodes representing mechanically accurate robot shapes. Signed distance functions are used for collision detection, and multi-objective edge costs incorporating geometric distance and actuation effort are defined for energy-efficient planning. Dijkstra's algorithm is used on the precomputed graph for path planning.

Result: The algorithm reliably avoids obstacles and generates feasible paths in milliseconds. Including energy costs reduces actuation effort but results in longer trajectories compared to geometry-only planning. The planning approach demonstrates the potential of shape-space graph search for fast and reliable motion planning in soft robotics.

Conclusion: The developed shape-space graph search method offers a promising approach for fast, reliable, and energy-efficient path planning for soft robotic arms, with potential applications in surgery, industry, and assistive robotics.

Abstract: Soft robots, inspired by elephant trunks or octopus arms, offer extraordinary
flexibility to bend, twist, and elongate in ways that rigid robots cannot.
However, their motion planning remains a challenge, especially in cluttered
environments with obstacles, due to their highly nonlinear and
infinite-dimensional kinematics. Here, we present a graph-based path planning
tool for an elephant-trunk-inspired soft robotic arm designed with three
artificial muscle fibers that allow for multimodal continuous deformation
through contraction. Using a biomechanical model inspired by morphoelasticity
and active filament theory, we precompute a shape library and construct a
$k$-nearest neighbor graph in \emph{shape space}, ensuring that each node
corresponds to a mechanically accurate and physically valid robot shape. For
the graph, we use signed distance functions to prune nodes and edges colliding
with obstacles, and define multi-objective edge costs based on geometric
distance and actuation effort, enabling energy-efficient planning with
collision avoidance. We demonstrate that our algorithm reliably avoids
obstacles and generates feasible paths within milliseconds from precomputed
graphs using Dijkstra's algorithm. We show that including energy costs can
drastically reduce the actuation effort compared to geometry-only planning, at
the expense of longer tip trajectories. Our results highlight the potential of
shape-space graph search for fast and reliable path planning in the field of
soft robotics, paving the way for real-time applications in surgical,
industrial, and assistive settings.

</details>


### [349] [Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning](https://arxiv.org/abs/2510.03599)
*Shafeef Omar,Majid Khadiv*

Main category: cs.RO

TL;DR: 一个统一的框架，用于基于接触的显式表示进行多任务运动和操纵策略学习。


<details>
  <summary>Details</summary>
Motivation: 设计用于不同任务的不同策略，而我们的方法通过一系列接触目标统一了任务的定义，从而能够利用跨越不同富含接触的任务的共享结构，实现一个能够执行广泛任务的单一策略。

Method: 训练一个目标条件强化学习（RL）策略来实现给定的接触计划。

Result: 在多种机器人载体和任务上进行了验证：一只四足动物执行多种步态，一只人形机器人执行多种双足和四足步态，以及一只人形机器人执行不同的双臂物体操纵任务。所有这些场景都由一个单一的策略控制，该策略经过训练以执行基于接触的不同任务，在形态上不同的系统之间展示了通用且鲁棒的行为。

Conclusion: 显式接触推理显著提高了对未见场景的泛化能力，将显式接触策略学习定位为可扩展的运动操纵的有前景的基础。

Abstract: We present a unified framework for multi-task locomotion and manipulation
policy learning grounded in a contact-explicit representation. Instead of
designing different policies for different tasks, our approach unifies the
definition of a task through a sequence of contact goals-desired contact
positions, timings, and active end-effectors. This enables leveraging the
shared structure across diverse contact-rich tasks, leading to a single policy
that can perform a wide range of tasks. In particular, we train a
goal-conditioned reinforcement learning (RL) policy to realise given contact
plans. We validate our framework on multiple robotic embodiments and tasks: a
quadruped performing multiple gaits, a humanoid performing multiple biped and
quadrupedal gaits, and a humanoid executing different bimanual object
manipulation tasks. Each of these scenarios is controlled by a single policy
trained to execute different tasks grounded in contacts, demonstrating
versatile and robust behaviours across morphologically distinct systems. Our
results show that explicit contact reasoning significantly improves
generalisation to unseen scenarios, positioning contact-explicit policy
learning as a promising foundation for scalable loco-manipulation.

</details>


### [350] [Safety-Oriented Dynamic Path Planning for Automated Vehicles](https://arxiv.org/abs/2510.03640)
*Mostafa Emam,Matthias Gerdts*

Main category: cs.RO

TL;DR: 该框架通过结合时变障碍物移动的网格投影来增强道路边界，并利用非线性模型预测控制（NMPC）和同伦约束松弛进行实时路径优化，同时还有一个独立的备份控制器提供安全回退轨迹，从而提高了复杂动态环境中自动驾驶的安全性和实时性。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中为自动驾驶汽车提供安全、可靠的路径规划和障碍物规避能力。

Method: 提出了一种双层控制框架：主控制循环使用非线性模型预测控制（NMPC）和同伦约束松弛进行实时路径优化；独立的备份循环提供安全的回退轨迹。

Result: 在各种驾驶场景中证明了该方法的有效性，强调了其实时适用性和鲁棒性。

Conclusion: 该框架是实现更安全、更可靠的自动驾驶的重大进展，尤其是在复杂和动态的环境中。

Abstract: Ensuring safety in autonomous vehicles necessitates advanced path planning
and obstacle avoidance capabilities, particularly in dynamic environments. This
paper introduces a bi-level control framework that efficiently augments road
boundaries by incorporating time-dependent grid projections of obstacle
movements, thus enabling precise and adaptive path planning. The main control
loop utilizes Nonlinear Model Predictive Control (NMPC) for real-time path
optimization, wherein homotopy-based constraint relaxation is employed to
improve the solvability of the optimal control problem (OCP). Furthermore, an
independent backup loop runs concurrently to provide safe fallback trajectories
when an optimal trajectory cannot be computed by the main loop within a
critical time frame, thus enhancing safety and real-time performance. Our
evaluation showcases the benefits of the proposed methods in various driving
scenarios, highlighting the real-time applicability and robustness of our
approach. Overall, the framework represents a significant step towards safer
and more reliable autonomous driving in complex and dynamic environments.

</details>


### [351] [Geometrically Exact Hard Magneto-Elastic Cosserat Shells: Static Formulation for Shape Morphing](https://arxiv.org/abs/2510.03644)
*Mohammadjavad Javadi,Robin Chhabra*

Main category: cs.RO

TL;DR: 该研究提出了一种基于特殊欧几里得群SE(3)的Cosserat壳体理论的静力学模型，用于分析具有大长宽比的硬磁软体机器人，解决了传统Cosserat杆理论的局限性，并通过数值模拟和实验验证了模型的有效性，特别是在大变形情况下。


<details>
  <summary>Details</summary>
Motivation: 现有Cosserat杆理论在模拟铁磁软体机器人时存在局限性，无法有效处理长宽比较大的2D壳体结构。本研究旨在开发一种适用于2D硬磁壳体的新型静力学模型，以解决分析和形状控制问题。

Method: 提出了一种基于特殊欧几里得群SE(3)的Cosserat壳体理论的新颖数学模型。该模型将壳体视为一个二维流形，具有六个自由度，能够捕捉嵌入弹性体中的硬磁粒子的行为。通过虚功原理推导出平衡方程的强弱形式，并提取了适用于数值实现的线性化弱形式，最终构建了有限元方法。

Result: 开发了一种坐标无关的静力学模型，能够有效避免壳体建模中常见的奇点和锁定问题。该模型通过一系列分析和实验案例得到验证，证明了其在处理大旋转和大位移情况下壳体变形的优越性。

Conclusion: 本研究提出的基于SE(3)的Cosserat壳体理论模型，为分析和控制大长宽比的硬磁软体机器人提供了一种高效且鲁棒的方法，尤其在处理大变形问题上表现出色。

Abstract: Cosserat rod theory is the popular approach to modeling ferromagnetic soft
robots as 1-Dimensional (1D) slender structures in most applications, such as
biomedical. However, recent soft robots designed for locomotion and
manipulation often exhibit a large width-to-length ratio that categorizes them
as 2D shells. For analysis and shape-morphing control purposes, we develop an
efficient coordinate-free static model of hard-magnetic shells found in soft
magnetic grippers and walking soft robots. The approach is based on a novel
formulation of Cosserat shell theory on the Special Euclidean group
($\mathbf{SE}(3)$). The shell is assumed to be a 2D manifold of material points
with six degrees of freedom (position & rotation) suitable for capturing the
behavior of a uniformly distributed array of spheroidal hard magnetic particles
embedded in the rheological elastomer. The shell's configuration manifold is
the space of all smooth embeddings $\mathbb{R}^2\rightarrow\mathbf{SE}(3)$.
According to a novel definition of local deformation gradient based on the Lie
group structure of $\mathbf{SE}(3)$, we derive the strong and weak forms of
equilibrium equations, following the principle of virtual work. We extract the
linearized version of the weak form for numerical implementations. The
resulting finite element approach can avoid well-known challenges such as
singularity and locking phenomenon in modeling shell structures. The proposed
model is analytically and experimentally validated through a series of test
cases that demonstrate its superior efficacy, particularly when the shell
undergoes severe rotations and displacements.

</details>


### [352] [An Amphibious Untethered Inchworm Soft Robot for Fast Crawling Locomotion](https://arxiv.org/abs/2510.03660)
*Mohammadjavad Javadi,Charlie Wadds,Robin Chhabra*

Main category: cs.RO

TL;DR: 提出了一种全功能、无线控制的软体机器人，该机器人能够行走、转向、游泳和运输有效载荷。


<details>
  <summary>Details</summary>
Motivation: 设计并实现了一个完全不受束缚的软体机器人，能够适应多样化的多任务环境。

Method: 通过结构优化和系统集成，利用磁力驱动一个弯曲、柔性的结构，并集成了一个紧凑的控制电路和摄像头，实现了无线控制和环境感知。

Result: 机器人成功实现了多模态运动，最大行走速度为3.74厘米/秒，游泳速度为0.82厘米/秒，并成功完成了行走、转向、游泳和有效载荷运输任务。

Conclusion: 该机器人展示了其在各种任务中的实用性，证明了不受束缚的软体机器人在实际应用中的潜力。

Abstract: Untethered soft robots are essential for advancing the real-world deployment
of soft robotic systems in diverse and multitasking environments. Inspired by
soft-bodied inchworm, we present a fully untethered soft robot with a curved,
flexible structure actuated by magnetic forces. The robot has a total mass of
102.63 g and demonstrates multimodal locomotion, achieving a maximum walking
speed of 3.74 cm/s and a swimming speed of 0.82 cm/s. A compact and lightweight
onboard control circuit enables wireless command transmission, while an
integrated camera provides environmental perception. Through structural
optimization and system-level integration, the robot successfully performs
walking, steering, swimming, and payload transport without reliance on external
infrastructure. The robot's dynamic performance and locomotion capabilities are
systematically validated through experimental characterization.

</details>


### [353] [Robust Visual Embodiment: How Robots Discover Their Bodies in Real Environments](https://arxiv.org/abs/2510.03677)
*Salim Rezvani,Ammar Jaleel Mahmood,Robin Chhabra*

Main category: cs.RO

TL;DR: 该研究首次系统性地量化了视觉降级（如模糊、椒盐噪声和高斯噪声）对机器人自我建模的影响，并提出了一种任务感知去噪框架，结合了经典恢复和保持形态的约束，以及语义分割来解决这些问题，显著提高了机器人自我建模在现实世界中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人自我建模方法在面对现实世界中常见的视觉降级（如模糊、噪声、杂乱背景）时表现脆弱，限制了机器人适应性的提升。本研究旨在系统性地研究这些视觉降级对机器人自我建模的影响，并提出鲁棒的解决方案。

Method: 通过仿真和物理实验，量化了模糊、椒盐噪声和高斯噪声对形态预测、轨迹规划和损坏恢复等自我建模任务的影响。提出了一种任务感知去噪框架，结合了经典图像恢复技术与保持形态的约束，并集成语义分割以从杂乱场景中分离机器人。

Result: 提出的任务感知去噪框架在模拟和物理平台上恢复了接近基线性能的自我建模效果，而现有方法则表现出显著性能下降。该方法能够有效保留对自我建模至关重要的结构线索，并能从杂乱场景中稳健地分离机器人。

Conclusion: 本研究提出的方法显著提高了机器人视觉自我建模的鲁棒性，克服了现有技术在现实世界感知条件下的局限性，为部署能够感知自身状态的机器人到不可预测的真实环境奠定了实际基础。

Abstract: Robots with internal visual self-models promise unprecedented adaptability,
yet existing autonomous modeling pipelines remain fragile under realistic
sensing conditions such as noisy imagery and cluttered backgrounds. This paper
presents the first systematic study quantifying how visual
degradations--including blur, salt-and-pepper noise, and Gaussian noise--affect
robotic self-modeling. Through both simulation and physical experiments, we
demonstrate their impact on morphology prediction, trajectory planning, and
damage recovery in state-of-the-art pipelines. To overcome these challenges, we
introduce a task-aware denoising framework that couples classical restoration
with morphology-preserving constraints, ensuring retention of structural cues
critical for self-modeling. In addition, we integrate semantic segmentation to
robustly isolate robots from cluttered and colorful scenes. Extensive
experiments show that our approach restores near-baseline performance across
simulated and physical platforms, while existing pipelines degrade
significantly. These contributions advance the robustness of visual
self-modeling and establish practical foundations for deploying self-aware
robots in unpredictable real-world environments.

</details>


### [354] [EmbodiSwap for Zero-Shot Robot Imitation Learning](https://arxiv.org/abs/2510.03706)
*Eadom Dessalene,Pavan Mantripragada,Michael Maynord,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: EmbodiSwap是一种用于将机器人叠加到人类视频上的方法，可用于零样本模仿学习，以弥合人类视频和机器人具身之间的差距。


<details>
  <summary>Details</summary>
Motivation: 弥合现实世界人类视频与目标机器人具身之间的差距，以实现零样本模仿学习。

Method: 使用EmbodiSwap生成合成机器人视频，并利用V-JEPA作为视觉骨干网络进行训练。

Result: 在真实世界测试中，零样本训练的V-JEPA模型达到了82%的成功率，优于其他模型。

Conclusion: EmbodiSwap方法在零样本模仿学习方面表现出色，并且V-JEPA作为视觉骨干的有效性得到了验证。

Abstract: We introduce EmbodiSwap - a method for producing photorealistic synthetic
robot overlays over human video. We employ EmbodiSwap for zero-shot imitation
learning, bridging the embodiment gap between in-the-wild ego-centric human
video and a target robot embodiment. We train a closed-loop robot manipulation
policy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a
visual backbone, repurposing V-JEPA from the domain of video understanding to
imitation learning over synthetic robot videos. Adoption of V-JEPA outperforms
alternative vision backbones more conventionally used within robotics. In
real-world tests, our zero-shot trained V-JEPA model achieves an $82\%$ success
rate, outperforming a few-shot trained $\pi_0$ network as well as $\pi_0$
trained over data produced by EmbodiSwap. We release (i) code for generating
the synthetic robot overlays which takes as input human videos and an arbitrary
robot URDF and generates a robot dataset, (ii) the robot dataset we synthesize
over EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference
code, to facilitate reproducible research and broader adoption.

</details>


### [355] [Model-Based Adaptive Precision Control for Tabletop Planar Pushing Under Uncertain Dynamics](https://arxiv.org/abs/2510.03768)
*Aydin Ahmadi,Baris Akgun*

Main category: cs.RO

TL;DR: 本文提出了一种数据驱动的平面推动方法，使用单一学习模型解决了多种任务，无需重新训练，并通过结合模型预测路径积分（MPPI）控制器实现了精确控制、轨迹跟踪和避障等功能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的平面推动方法能力有限（例如，仅支持侧推、精确控制或单任务），限制了其广泛应用。本文旨在通过单一学习模型解决多种推动任务，减少手动工程量并提高泛化能力。

Method: 本文提出的方法使用基于循环门控单元（GRU）的网络架构，并增加了非线性层来捕捉物体-环境动力学并确保稳定性。通过定制状态-动作表示，模型能够处理不确定的动力学、可变的推动长度和多样的任务。控制方面，将学习到的动力学与基于采样的模型预测路径积分（MPPI）控制器相结合，以生成适应性强、面向任务的动作。模型在模拟环境中通过领域随机化进行训练，以支持从模拟到现实的迁移。

Result: 在模拟和真实世界实验中，该框架在严格阈值下的精确目标定位方面取得了很高的成功率，并在轨迹跟踪和避障方面表现出色。通过改变控制器的目标函数，无需重新训练即可解决多个任务。通过在更广泛的推动长度上进行训练并设计一个平衡的控制器，减少了长视野目标的步数。

Conclusion: 本文提出的模型驱动框架通过单一学习模型成功解决了多种非抓取式桌面推动任务，无需重新训练，并实现了高精度的定位、轨迹跟踪和避障。该方法在模拟和真实世界中都得到了验证，证明了其有效性和广泛适用性。

Abstract: Data-driven planar pushing methods have recently gained attention as they
reduce manual engineering effort and improve generalization compared to
analytical approaches. However, most prior work targets narrow capabilities
(e.g., side switching, precision, or single-task training), limiting broader
applicability. We present a model-based framework for non-prehensile tabletop
pushing that uses a single learned model to address multiple tasks without
retraining. Our approach employs a recurrent GRU-based architecture with
additional non-linear layers to capture object-environment dynamics while
ensuring stability. A tailored state-action representation enables the model to
generalize across uncertain dynamics, variable push lengths, and diverse tasks.
For control, we integrate the learned dynamics with a sampling-based Model
Predictive Path Integral (MPPI) controller, which generates adaptive,
task-oriented actions. This framework supports side switching, variable-length
pushes, and objectives such as precise positioning, trajectory following, and
obstacle avoidance. Training is performed in simulation with domain
randomization to support sim-to-real transfer. We first evaluate the
architecture through ablation studies, showing improved prediction accuracy and
stable rollouts. We then validate the full system in simulation and real-world
experiments using a Franka Panda robot with markerless tracking. Results
demonstrate high success rates in precise positioning under strict thresholds
and strong performance in trajectory tracking and obstacle avoidance. Moreover,
multiple tasks are solved simply by changing the controller's objective
function, without retraining. While our current focus is on a single object
type, we extend the framework by training on wider push lengths and designing a
balanced controller that reduces the number of steps for longer-horizon goals.

</details>


### [356] [Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets](https://arxiv.org/abs/2510.03776)
*Tiago Rodrigues de Almeida,Yufei Zhu,Andrey Rudenko,Tomasz P. Kucner,Johannes A. Stork,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: 本研究旨在解决机器人等智能系统在复杂动态环境中导航时的动作和意图预测问题，提出并分析了多种类别条件下的运动预测方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂动态环境中，机器人需要预测周围代理的未来动作和意图以高效导航并避免碰撞。代理的行为很大程度上取决于其任务、角色或可观察标签，因此，类别条件下的运动预测是一种有吸引力的方法，可以减少预测不确定性并提高对异构代理的预测精度，但该研究领域，尤其是在移动机器人和数据有限的应用中，探索不足。

Method: 本研究分析了两种数据集上的不同类别条件下的运动预测方法，并提出了一系列基于条件模式和基于深度学习的高效基线方法。

Result: 实验结果表明，在考虑类别标签时，所有方法在大多数情况下都提高了预测精度。研究还发现，在不平衡数据集或数据不足的新环境中，不同方法的表现存在显著差异。具体而言，深度学习方法在平衡数据集上表现更好，但在数据有限的应用（如机器人新环境的冷启动或类别不平衡）中，基于模式的方法可能更优。

Conclusion: 类别条件下的运动预测能够提高预测精度，但在数据不平衡或有限的情况下，基于模式的方法可能比深度学习方法更具优势。

Abstract: Robots and other intelligent systems navigating in complex dynamic
environments should predict future actions and intentions of surrounding agents
to reach their goals efficiently and avoid collisions. The dynamics of those
agents strongly depends on their tasks, roles, or observable labels.
Class-conditioned motion prediction is thus an appealing way to reduce forecast
uncertainty and get more accurate predictions for heterogeneous agents.
However, this is hardly explored in the prior art, especially for mobile robots
and in limited data applications. In this paper, we analyse different
class-conditioned trajectory prediction methods on two datasets. We propose a
set of conditional pattern-based and efficient deep learning-based baselines,
and evaluate their performance on robotics and outdoors datasets (TH\"OR-MAGNI
and Stanford Drone Dataset). Our experiments show that all methods improve
accuracy in most of the settings when considering class labels. More
importantly, we observe that there are significant differences when learning
from imbalanced datasets, or in new environments where sufficient data is not
available. In particular, we find that deep learning methods perform better on
balanced datasets, but in applications with limited data, e.g., cold start of a
robot in a new environment, or imbalanced classes, pattern-based methods may be
preferable.

</details>


### [357] [Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy](https://arxiv.org/abs/2510.04774)
*Weixu Zhu,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: SoNS 增强的机器人群能够通过 LLM 实时生成代码来解决卡住的问题，并在演示中成功率达到 85%。


<details>
  <summary>Details</summary>
Motivation: 介绍自组织神经系统 (SoNS) 如何通过简化行为设计和全局状态估计来增强机器人群，从而实现自动代码生成。

Method: 当机器人群遇到障碍时，实时生成并运行外部 LLM 生成的代码。

Result: 在包含 6 个真实机器人和 30 多个机器人的模拟试验中，SoNS 增强的机器人群能够自动解决卡住的问题，成功率达到 85%。

Conclusion: SoNS 是一种有效的方法，可以使机器人群能够通过 LLM 实时生成代码来解决问题，从而提高任务的成功率。

Abstract: Our recently introduced self-organizing nervous system (SoNS) provides robot
swarms with 1) ease of behavior design and 2) global estimation of the swarm
configuration and its collective environment, facilitating the implementation
of online automatic code generation for robot swarms. In a demonstration with 6
real robots and simulation trials with >30 robots, we show that when a
SoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code
generated by an external LLM on the fly, completing its mission with an 85%
success rate.

</details>


### [358] [COVER:COverage-VErified Roadmaps for Fixed-time Motion Planning in Continuous Semi-Static Environments](https://arxiv.org/abs/2510.03875)
*Niranjan Kumar Ilampooranan,Constantinos Chamzas*

Main category: cs.RO

TL;DR: COVER是一个新框架，通过增量构建覆盖验证图，在半静态环境中实现固定时间内的运动规划查询，具有更优的覆盖范围和查询成功率。


<details>
  <summary>Details</summary>
Motivation: 在半静态环境中，大部分障碍物是固定的，只有少数障碍物会发生变化。这种结构化的可变性可以通过系统化的方式来利用，从而在运动规划问题上提供比一般情况更强的保证。然而，现有的方法要么缺乏正式的保证，要么依赖于对障碍物配置的限制性离散化，这在现实世界中限制了它们的应用。

Method: COVER框架通过划分障碍物配置空间并解决每个分区内的可行路径，逐步构建覆盖验证图。该方法系统地验证了图中每个分区的可行性，并保证在已验证区域内进行固定时间的运动规划查询。

Result: 在模拟的7自由度Panda机器人上进行的桌子和架子任务验证表明，COVER比现有方法实现了更广的覆盖范围和更高的查询成功率。

Conclusion: COVER框架能够有效地处理半静态环境中的运动规划问题，通过覆盖验证图在固定时间内提供可靠的查询，并在实际应用中展现出优越的性能。

Abstract: Having the ability to answer motion-planning queries within a fixed time
budget is critical for the widespread deployment of robotic systems.
Semi-static environments, where most obstacles remain static but a limited set
can vary across queries, exhibit structured variability that can be
systematically exploited to provide stronger guarantees than in general
motion-planning problems. However, prior approaches in this setting either lack
formal guarantees or rely on restrictive discretizations of obstacle
configurations, limiting their applicability in realistic domains. This paper
introduces COVER, a novel framework that incrementally constructs a
coverage-verified roadmap in semi-static environments. By partitioning the
obstacle configuration space and solving for feasible paths within each
partition, COVER systematically verifies feasibility of the roadmap in each
partition and guarantees fixed-time motion planning queries within the verified
regions. We validate COVER with a 7-DOF simulated Panda robot performing table
and shelf tasks, demonstrating that COVER achieves broader coverage with higher
query success rates than prior works.

</details>


### [359] [Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning](https://arxiv.org/abs/2510.03885)
*Sunghwan Kim,Woojeh Chung,Zhirui Dai,Dwait Bhatt,Arth Shukla,Hao Su,Yulun Tian,Nikolay Atanasov*

Main category: cs.RO

TL;DR: 移动操作策略通过利用3D潜在地图，在空间和时间推理能力上优于仅依赖图像的策略。


<details>
  <summary>Details</summary>
Motivation: 旨在提升移动操作策略在空间和时间推理能力，解决仅依赖图像的局限性。

Method: 提出了一种名为“Seeing the Bigger Picture”（SBP）的端到端策略学习方法，该方法直接在3D潜在特征地图上操作。SBP通过整合多视角观察到地图中，并利用预训练的解码器进行目标嵌入重建，同时支持在线优化地图特征。策略学习可采用行为克隆或强化学习，并将3D潜在地图作为状态变量，通过3D特征聚合器获取全局上下文。

Result: SBP在场景级移动操作和序列桌面操作任务上进行了评估。实验表明，SBP能够进行全局场景推理，利用地图作为长时记忆，并在新场景中也优于基于图像的策略，其中序列操作任务的成功率提高了25%。

Conclusion: SBP是一种有效的移动操作策略学习方法，通过3D潜在地图增强了机器人的空间和时间推理能力，并在多项任务中取得了优于传统图像方法的性能。

Abstract: In this paper, we demonstrate that mobile manipulation policies utilizing a
3D latent map achieve stronger spatial and temporal reasoning than policies
relying solely on images. We introduce Seeing the Bigger Picture (SBP), an
end-to-end policy learning approach that operates directly on a 3D map of
latent features. In SBP, the map extends perception beyond the robot's current
field of view and aggregates observations over long horizons. Our mapping
approach incrementally fuses multiview observations into a grid of
scene-specific latent features. A pre-trained, scene-agnostic decoder
reconstructs target embeddings from these features and enables online
optimization of the map features during task execution. A policy, trainable
with behavior cloning or reinforcement learning, treats the latent map as a
state variable and uses global context from the map obtained via a 3D feature
aggregator. We evaluate SBP on scene-level mobile manipulation and sequential
tabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasons
globally over the scene, (ii) leverages the map as long-horizon memory, and
(iii) outperforms image-based policies in both in-distribution and novel
scenes, e.g., improving the success rate by 25% for the sequential manipulation
task.

</details>


### [360] [NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation](https://arxiv.org/abs/2510.03895)
*Zheng Huang,Mingyu Liu,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Xiaoman Li,Yiduo Jia,Hao Zhong,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: VLA模型面临灾难性遗忘问题，提出NoTVLA框架，通过关注稀疏轨迹和优化训练策略，提升性能和泛化能力，同时降低计算成本和硬件需求。


<details>
  <summary>Details</summary>
Motivation: VLA模型在具身智能领域是关键进展，但实际部署面临灾难性遗忘的重大障碍，这源于其对连续动作序列或动作块的过度依赖，导致数据孤岛，阻碍跨任务的知识保留。

Method: 提出NoTVLA框架，通过关注稀疏轨迹来避免密集轨迹微调带来的灾难性遗忘。其创新在于利用时间压缩和空间推理剪枝来规划机器人末端执行器的轨迹，而非目标对象的轨迹，并使用稀疏轨迹进行训练。

Result: 在多任务评估中，NoTVLA表现优于pi0，计算资源消耗是pi0的十分之一，且无需腕部摄像头。其运行精度接近单任务专家模型，并保留了语言能力，实现了零样本泛化，支持跨多个机器人平台部署，并能在新视角下进行任务感知。

Conclusion: NoTVLA框架通过关注稀疏轨迹和优化训练策略，有效解决了VLA模型的灾难性遗忘问题，显著提升了性能和泛化能力，同时降低了计算和硬件要求，为具身智能的实际部署提供了可行方案。

Abstract: Vision-Language-Action (VLA) models represent a pivotal advance in embodied
intelligence, yet they confront critical barriers to real-world deployment,
most notably catastrophic forgetting. This issue stems from their overreliance
on continuous action sequences or action chunks, which inadvertently create
isolated data silos that disrupt knowledge retention across tasks. To tackle
these challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)
framework: a novel approach that narrows its focus to sparse trajectories,
thereby avoiding the catastrophic forgetting associated with dense trajectory
fine-tuning. A key innovation of NoTVLA lies in its trajectory planning
strategy: instead of centering on the target object's trajectory, it leverages
temporal compression and spatial reasoning pruning specifically for the robot
end effector's trajectory. Furthermore, training is conducted using these
sparse trajectories rather than dense action trajectories, an optimization that
delivers remarkable practical advantages with better performance in zero-shot.
In multi-task evaluation scenarios, NoTVLA achieves superior performance and
generalization compared to pi0 while operating under two critical constraints:
it uses over an order of magnitude less computing power than pi0 and requires
no wrist-mounted camera. This design ensures that NoTVLA's operational accuracy
closely approximates that of single-task expert models. Crucially, it also
preserves the model's inherent language capabilities, enabling zero-shot
generalization in specific scenarios, supporting unified model deployment
across multiple robot platforms, and fostering a degree of generalization even
when perceiving tasks from novel perspectives.

</details>


### [361] [WAFFLE: A Wearable Approach to Bite Timing Estimation in Robot-Assisted Feeding](https://arxiv.org/abs/2510.03910)
*Akhil Padmanabha,Jessie Yuan,Tanisha Mehta,Rajat Kumar Jenamani,Eric Hu,Victoria de León,Anthony Wertz,Janavi Gupta,Ben Dodson,Yunting Yan,Carmel Majidi,Tapomayukh Bhattacharjee,Zackory Erickson*

Main category: cs.RO

TL;DR: WAFFLE是一个利用可穿戴传感器预测进食时机的机器人喂食系统，提高了自主性并受到用户好评。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器人喂食系统在进食时机估计方面的技术挑战，以提高用户自主性和生活质量，并减轻护理人员负担。

Method: 开发WAFFLE系统，利用可穿戴传感器数据（如头部运动、咀嚼、说话）来预测进食时机。使用监督回归模型，并结合用户可调的断言阈值来生成指令。

Result: 在无运动障碍的参与者研究中，WAFFLE在控制感、机器人理解和工作量方面表现优于或持平于基线方法，并受到大多数用户的偏爱。在有运动障碍的参与者的家庭环境研究中也证明了其泛化能力。

Conclusion: WAFFLE能够实现自然、反应式的进食时机预测，并能在不同用户、机器人硬件、食物和用餐场景下泛化应用。

Abstract: Millions of people around the world need assistance with feeding. Robotic
feeding systems offer the potential to enhance autonomy and quality of life for
individuals with impairments and reduce caregiver workload. However, their
widespread adoption has been limited by technical challenges such as estimating
bite timing, the appropriate moment for the robot to transfer food to a user's
mouth. In this work, we introduce WAFFLE: Wearable Approach For Feeding with
LEarned bite timing, a system that accurately predicts bite timing by
leveraging wearable sensor data to be highly reactive to natural user cues such
as head movements, chewing, and talking. We train a supervised regression model
on bite timing data from 14 participants and incorporate a user-adjustable
assertiveness threshold to convert predictions into proceed or stop commands.
In a study with 15 participants without motor impairments with the Obi feeding
robot, WAFFLE performs statistically on par with or better than baseline
methods across measures of feeling of control, robot understanding, and
workload, and is preferred by the majority of participants for both individual
and social dining. We further demonstrate WAFFLE's generalizability in a study
with 2 participants with motor impairments in their home environments using a
Kinova 7DOF robot. Our findings support WAFFLE's effectiveness in enabling
natural, reactive bite timing that generalizes across users, robot hardware,
robot positioning, feeding trajectories, foods, and both individual and social
dining contexts.

</details>


### [362] [TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry](https://arxiv.org/abs/2510.03919)
*Matthew Lisondra,Junseo Kim,Glenn Takashi Shimoda,Kourosh Zareinia,Sajad Saeedi*

Main category: cs.RO

TL;DR: FPSP 传感器上的 TCB-VIO 算法可以克服视觉和时间漂移，并在 250 FPS 下实现比最先进的方法更好的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的 VIO 框架会遇到由视觉估计引起的空间漂移和由 IMU 测量引起的时间漂移。FPSP 传感器可以直接在图像传感器上执行视觉算法，从而在视觉传感器和处理器之间实现更快的通信。

Method: TCB-VIO 是一种紧密集成的 6 DOF VIO，由 MSCKF 实现。它在 250 FPS 下运行，IMU 测量频率为 400 赫兹。

Result: TCB-VIO 在准确性和鲁棒性方面优于 ROVIO、VINS-Mono 和 ORB-SLAM3 等现有方法。

Conclusion: TCB-VIO 是一种在 FPSP 传感器上实现的、具有竞争力的 VIO 方法，它克服了空间和时间漂移，并在各种条件下都表现出色。

Abstract: Vision algorithms can be executed directly on the image sensor when
implemented on the next-generation sensors known as focal-plane
sensor-processor arrays (FPSP)s, where every pixel has a processor. FPSPs
greatly improve latency, reducing the problems associated with the bottleneck
of data transfer from a vision sensor to a processor. FPSPs accelerate
vision-based algorithms such as visual-inertial odometry (VIO). However, VIO
frameworks suffer from spatial drift due to the vision-based pose estimation,
whilst temporal drift arises from the inertial measurements. FPSPs circumvent
the spatial drift by operating at a high frame rate to match the high-frequency
output of the inertial measurements. In this paper, we present TCB-VIO, a
tightly-coupled 6 degrees-of-freedom VIO by a Multi-State Constraint Kalman
Filter (MSCKF), operating at a high frame-rate of 250 FPS and from IMU
measurements obtained at 400 Hz. TCB-VIO outperforms state-of-the-art methods:
ROVIO, VINS-Mono, and ORB-SLAM3.

</details>


### [363] [A Real-Time Framework for Intermediate Map Construction and Kinematically Feasible Off-Road Planning Without OSM](https://arxiv.org/abs/2510.03948)
*Otobong Jerome,Geesara Prathap Kulathunga,Devitt Dmitry,Eugene Murawjow,Alexandr Klimchik*

Main category: cs.RO

TL;DR: 本方法提出了一种针对越野环境的全局路径规划方法，解决了传统方法在实时性、运动学可行性和内存效率方面的不足，并在实际测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统全局路径规划方法在越野环境中面临严峻挑战，无法有效处理大规模地图，并且忽视了实时性、运动学可行性和内存效率等关键因素。

Method: 该方法首先在像素坐标系中构建包含地理特征（如越野小径、水道、限制区、可通行区和树木）的中间地图。然后将规划问题分解为基于图的路径规划、运动学可行性检查和路径平滑三个子问题。

Result: 在最大达几平方公里的越野环境中进行的大规模地图测试中，该方法平均能在1.5秒内找到可行路径，并在极端条件下内存占用约为1.5GB。

Conclusion: 所提出的框架具有通用性，适用于搜索救援和农业作业等多种越野自主导航任务。

Abstract: Off-road environments present unique challenges for autonomous navigation due
to their complex and unstructured nature. Traditional global path-planning
methods, which typically aim to minimize path length and travel time, perform
poorly on large-scale maps and fail to account for critical factors such as
real-time performance, kinematic feasibility, and memory efficiency. This paper
introduces a novel global path-planning method specifically designed for
off-road environments, addressing these essential factors. The method begins by
constructing an intermediate map within the pixel coordinate system,
incorporating geographical features like off-road trails, waterways, restricted
and passable areas, and trees. The planning problem is then divided into three
sub-problems: graph-based path planning, kinematic feasibility checking, and
path smoothing. This approach effectively meets real-time performance
requirements while ensuring kinematic feasibility and efficient memory use. The
method was tested in various off-road environments with large-scale maps up to
several square kilometers in size, successfully identifying feasible paths in
an average of 1.5 seconds and utilizing approximately 1.5GB of memory under
extreme conditions. The proposed framework is versatile and applicable to a
wide range of off-road autonomous navigation tasks, including search and rescue
missions and agricultural operations.

</details>


### [364] [SITCOM: Scaling Inference-Time COMpute for VLAs](https://arxiv.org/abs/2510.04041)
*Ayudh Saxena,Harsh Shah,Sandeep Routray,Rishi Rajesh Shah,Esha Pahwa*

Main category: cs.RO

TL;DR: SITCOM是一个框架，通过结合模型预测控制（MPC）的思想，利用预训练的视觉-语言-动作（VLA）模型进行多步预测和轨迹选择，从而克服了现有VLA模型在长时序规划和累积误差方面的限制，提升了机器人控制的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型在机器人控制领域虽然有潜力，但面临数据采集成本高、泛化能力差以及长时序规划困难等问题。它们通常只能生成单步控制指令，难以应对动态任务中的误差累积。

Method: SITCOM框架通过引入基于模型的模拟（rollouts）和基于奖励的轨迹选择机制，借鉴了模型预测控制（MPC）的思想。它利用学习到的动力学模型来模拟多步动作序列，并从中选择最优的执行计划。该框架还包括一个高效的、基于Transformer的动力学模型，该模型在大规模数据集上进行训练，并在仿真环境中进行微调，以缩小真实世界与仿真环境之间的差距。

Result: 通过在SIMPLER环境中的多项任务和设置的综合评估，结果表明，SITCOM框架结合良好的奖励函数，能够将任务完成率从48%显著提高到72%。

Conclusion: SITCOM框架能够有效地将一次性的VLA模型转化为能够进行长时序规划的鲁棒控制器，显著提升机器人在复杂环境中的任务完成能力。

Abstract: Learning robust robotic control policies remains a major challenge due to the
high cost of collecting labeled data, limited generalization to unseen
environments, and difficulties in planning over long horizons. While
Vision-Language-Action (VLA) models offer a promising solution by grounding
natural language instructions into single-step control commands, they often
lack mechanisms for lookahead and struggle with compounding errors in dynamic
tasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs
(SITCOM), a framework that augments any pretrained VLA with model-based
rollouts and reward-based trajectory selection, inspired by Model Predictive
Control algorithm. SITCOM leverages a learned dynamics model to simulate
multi-step action rollouts to select the best candidate plan for real-world
execution, transforming one-shot VLAs into robust long-horizon planners. We
develop an efficient transformer-based dynamics model trained on large-scale
BridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim
gap, and score candidate rollouts using rewards from simulator. Through
comprehensive evaluation across multiple tasks and settings in the SIMPLER
environment, we demonstrate that SITCOM when combined with a good reward
function can significantly improve task completion rate from 48% to 72% using
trained dynamics model.

</details>


### [365] [Feedback Matters: Augmenting Autonomous Dissection with Visual and Topological Feedback](https://arxiv.org/abs/2510.04074)
*Chung-Pang Wang,Changwei Chen,Xiao Liang,Soofiyan Atar,Florian Richter,Michael Yip*

Main category: cs.RO

TL;DR: 本研究提出了一种用于自主组织解剖的反馈框架，通过对内窥镜图像进行拓扑变化推理来指导后续动作，并引入可见性指标来优化控制器设计，从而提高自主性、减少错误和增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的自主手术系统在处理动态变化和拓扑感知方面存在局限性，无法有效处理组织解剖中的视觉和触觉变化。

Method: 提出了一种基于反馈的框架，用于自主组织解剖。该框架在每次解剖动作后，显式地从内窥镜图像中推理拓扑变化，并利用可见性指标来优化控制器，以最大化可见性。该框架可以与基于规划和基于学习的解剖方法结合。

Result: 实验证明，所提出的框架显著提高了自主性，减少了错误，并增强了在复杂手术场景中的鲁棒性。

Conclusion: 基于反馈的框架通过显式推理拓扑变化和优化可见性，能够有效提升自主手术系统的性能，特别是在组织解剖任务中。

Abstract: Autonomous surgical systems must adapt to highly dynamic environments where
tissue properties and visual cues evolve rapidly. Central to such adaptability
is feedback: the ability to sense, interpret, and respond to changes during
execution. While feedback mechanisms have been explored in surgical robotics,
ranging from tool and tissue tracking to error detection, existing methods
remain limited in handling the topological and perceptual challenges of tissue
dissection. In this work, we propose a feedback-enabled framework for
autonomous tissue dissection that explicitly reasons about topological changes
from endoscopic images after each dissection action. This structured feedback
guides subsequent actions, enabling the system to localize dissection progress
and adapt policies online. To improve the reliability of such feedback, we
introduce visibility metrics that quantify tissue exposure and formulate
optimal controller designs that actively manipulate tissue to maximize
visibility. Finally, we integrate these feedback mechanisms with both
planning-based and learning-based dissection methods, and demonstrate
experimentally that they significantly enhance autonomy, reduce errors, and
improve robustness in complex surgical scenarios.

</details>


### [366] [From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents](https://arxiv.org/abs/2510.04076)
*Amin Vahidi-Moghaddam,Sayed Pedram Haeri Boroujeni,Iman Jebellat,Ehsan Jebellat,Niloufar Mehrabi,Zhaojian Li*

Main category: cs.RO

TL;DR: 数据驱动的控制方法在机器人和车辆运动控制中有巨大潜力，但存在响应慢、计算量大、内存需求高等问题。本文提出并验证了八种旨在降低计算复杂性的技术，以克服这些限制。


<details>
  <summary>Details</summary>
Motivation: 尽管存在模型预测控制（MPC）等方法，但复杂系统的精确建模仍然是一个挑战。因此，需要数据驱动的方法，如基于机器学习的MPC、强化学习（RL）和数据赋能预测控制（DeePC）。然而，这些方法存在响应慢、计算量大、内存需求高等缺点，限制了它们在实时性要求高、计算资源有限的实际应用中的使用。

Method: 本文介绍并评估了八种旨在降低数据驱动控制策略计算复杂性的技术，并将它们应用于机器人手臂、软体机器人和车辆运动控制等实际场景。

Result: 文章通过在机器人手臂、软体机器人和车辆运动控制等实际应用中的实验，展示了所提出的八种降低计算复杂性技术的有效性。

Conclusion: 本文提出的八种技术能够有效降低数据驱动控制策略的计算复杂性，克服了现有方法的局限性，使其在实际应用中更具可行性。

Abstract: One of the main challenges in modern control applications, particularly in
robot and vehicle motion control, is achieving accurate, fast, and safe
movement. To address this, optimal control policies have been developed to
enforce safety while ensuring high performance. Since basic first-principles
models of real systems are often available, model-based controllers are widely
used. Model predictive control (MPC) is a leading approach that optimizes
performance while explicitly handling safety constraints. However, obtaining
accurate models for complex systems is difficult, which motivates data-driven
alternatives. ML-based MPC leverages learned models to reduce reliance on
hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal
policies directly from interaction data. Data-enabled predictive control
(DeePC) goes further by bypassing modeling altogether, directly learning safe
policies from raw input-output data. Recently, large language model (LLM)
agents have also emerged, translating natural language instructions into
structured formulations of optimal control problems. Despite these advances,
data-driven policies face significant limitations. They often suffer from slow
response times, high computational demands, and large memory needs, making them
less practical for real-world systems with fast dynamics, limited onboard
computing, or strict memory constraints. To address this, various technique,
such as reduced-order modeling, function-approximated policy learning, and
convex relaxations, have been proposed to reduce computational complexity. In
this paper, we present eight such approaches and demonstrate their
effectiveness across real-world applications, including robotic arms, soft
robots, and vehicle motion control.

</details>


### [367] [HEHA: Hierarchical Planning for Heterogeneous Multi-Robot Exploration of Unknown Environments](https://arxiv.org/abs/2510.04161)
*Longrui Yang,Yiyu Wang,Jingfan Tang,Yunpeng Lv,Shizhe Zhao,Chao Cao,Zhongqiang Ren*

Main category: cs.RO

TL;DR: 该论文提出了一种名为HEHA的分层探索方法，用于解决多机器人（如无人机、轮式和腿式机器人）在未知环境中自主探索的路径规划问题。HEHA通过全局规划和局部规划来应对机器人异构性和复杂地形带来的挑战，其核心是全局规划中的PEAF算法，该算法能在遍历性约束下，快速找到次优解以最小化机器人之间的最大路径长度。实验结果表明，HEHA比基线方法能减少高达30%的探索时间。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中，如何利用能力各异的异构机器人（如无人机、轮式和腿式机器人）进行自主探索，并解决机器人分配和访问顺序的规划问题，这是一个关键的挑战。该问题会转化为一个大规模约束优化问题，需要快速迭代求解。

Method: 提出了一种名为HEHA（Hierarchical Exploration with Heterogeneous Agents）的方法，该方法利用分层方法将探索分解为全局规划和局部规划。HEHA的全局规划采用了新提出的PEAF（Partial Anytime Focal search）算法，该算法能在遍历性约束下，快速找到有界的次优解，以最小化机器人间的最大路径长度。局部规划器也考虑了异构性，以避免机器人重复探索。

Result: 实验结果表明，与基线方法相比，HEHA可以将探索时间减少高达30%。

Conclusion: HEHA通过其新颖的全局规划算法PEAF和考虑异构性的局部规划，有效地解决了多机器人未知环境探索的路径规划问题，并在探索效率上取得了显著提升。

Abstract: This paper considers the path planning problem for autonomous exploration of
an unknown environment using multiple heterogeneous robots such as drones,
wheeled, and legged robots, which have different capabilities to traverse
complex terrains. A key challenge there is to intelligently allocate the robots
to the unknown areas to be explored and determine the visiting order of those
spaces subject to traversablity constraints, which leads to a large scale
constrained optimization problem that needs to be quickly and iteratively
solved every time when new space are explored. To address the challenge, we
propose HEHA (Hierarchical Exploration with Heterogeneous Agents) by leveraging
a recent hierarchical method that decompose the exploration into global
planning and local planning. The major contribution in HEHA is its global
planning, where we propose a new routing algorithm PEAF (Partial Anytime Focal
search) that can quickly find bounded sub-optimal solutions to minimize the
maximum path length among the agents subject to traversability constraints.
Additionally, the local planner in HEHA also considers heterogeneity to avoid
repeated and duplicated exploration among the robots. The experimental results
show that, our HEHA can reduce up to 30% of the exploration time than the
baselines.

</details>


### [368] [Learning to Capture Rocks using an Excavator: A Reinforcement Learning Approach with Guiding Reward Formulation](https://arxiv.org/abs/2510.04168)
*Amirmasoud Molaei,Reza Ghabcheloo*

Main category: cs.RO

TL;DR: 本研究提出了一种完全数据驱动的控制框架，用于在没有明确的岩石或土壤模型的情况下，利用强化学习（PPO算法）自主抓取岩石。


<details>
  <summary>Details</summary>
Motivation: 现有的自主挖掘方法主要关注连续介质或需要专用抓手，这限制了它们在实际施工现场的应用。因此，需要一种适用于非结构化环境和岩石抓取任务的自主控制方法。

Method: 使用PPO算法和引导奖励公式，在AGX Dynamics模拟器中训练了一个无模型强化学习代理。该策略直接输出挖掘机的关节速度指令。通过对岩石几何形状、密度、质量以及铲斗、岩石和目标位置的初始配置进行广泛的域随机化来增强鲁棒性。

Result: 所学策略能够很好地泛化到未知的岩石和不同的土壤条件，成功率很高，与人类操作员相当，同时保持机器的稳定性。

Conclusion: 本研究证明了基于学习的挖掘策略在没有专用硬件或详细材料模型的情况下，对离散物体进行操作的可行性。

Abstract: Rock capturing with standard excavator buckets is a challenging task
typically requiring the expertise of skilled operators. Unlike soil digging, it
involves manipulating large, irregular rocks in unstructured environments where
complex contact interactions with granular material make model-based control
impractical. Existing autonomous excavation methods focus mainly on continuous
media or rely on specialized grippers, limiting their applicability to
real-world construction sites. This paper introduces a fully data-driven
control framework for rock capturing that eliminates the need for explicit
modeling of rock or soil properties. A model-free reinforcement learning agent
is trained in the AGX Dynamics simulator using the Proximal Policy Optimization
(PPO) algorithm and a guiding reward formulation. The learned policy outputs
joint velocity commands directly to the boom, arm, and bucket of a CAT365
excavator model. Robustness is enhanced through extensive domain randomization
of rock geometry, density, and mass, as well as the initial configurations of
the bucket, rock, and goal position. To the best of our knowledge, this is the
first study to develop and evaluate an RL-based controller for the rock
capturing task. Experimental results show that the policy generalizes well to
unseen rocks and varying soil conditions, achieving high success rates
comparable to those of human participants while maintaining machine stability.
These findings demonstrate the feasibility of learning-based excavation
strategies for discrete object manipulation without requiring specialized
hardware or detailed material models.

</details>


### [369] [VBM-NET: Visual Base Pose Learning for Mobile Manipulation using Equivariant TransporterNet and GNNs](https://arxiv.org/abs/2510.04171)
*Lakshadeep Naik,Adam Fischer,Daniel Duberg,Danica Kragic*

Main category: cs.RO

TL;DR: VBM-NET是一种基于学习的方法，可以直接从场景的俯视正交投影中选择最佳的移动基座姿势，用于物体抓取。该方法利用了TransporterNet来利用空间对称性，并通过图神经网络和强化学习来确定最佳姿势，在计算时间显著减少的情况下，取得了与经典方法相当的解决方案，并成功实现了从模拟到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 在移动操作中，选择最佳的移动基座姿势对于成功抓取物体至关重要。以往的研究假设能够获得精确的物体姿势和环境模型等可靠的状态信息。本研究旨在直接从提供全局场景概览并保留空间结构的俯视正交投影中规划基座姿势。

Method: 提出VBM-NET，一种使用俯视正交投影进行基座姿势选择的学习方法。使用具有等变性的TransporterNet来利用空间对称性并有效学习候选抓取基座姿势。然后，使用图神经网络表示不同数量的候选基座姿势，并通过强化学习从中确定最佳基座姿势。

Result: VBM-NET能够在显著减少的计算时间内生成与经典方法相媲美的解决方案。此外，通过在真实世界的移动操作中成功部署在模拟中训练的策略，验证了模拟到现实的迁移。

Conclusion: VBM-NET能够有效地从俯视正交投影中规划移动基座的姿势，为移动操作中的抓取任务提供了一种计算效率高且有效的解决方案，并成功实现了模拟到现实的迁移。

Abstract: In Mobile Manipulation, selecting an optimal mobile base pose is essential
for successful object grasping. Previous works have addressed this problem
either through classical planning methods or by learning state-based policies.
They assume access to reliable state information, such as the precise object
poses and environment models. In this work, we study base pose planning
directly from top-down orthographic projections of the scene, which provide a
global overview of the scene while preserving spatial structure. We propose
VBM-NET, a learning-based method for base pose selection using such top-down
orthographic projections. We use equivariant TransporterNet to exploit spatial
symmetries and efficiently learn candidate base poses for grasping. Further, we
use graph neural networks to represent a varying number of candidate base poses
and use Reinforcement Learning to determine the optimal base pose among them.
We show that VBM-NET can produce comparable solutions to the classical methods
in significantly less computation time. Furthermore, we validate sim-to-real
transfer by successfully deploying a policy trained in simulation to real-world
mobile manipulation.

</details>


### [370] [Using Robotics to Improve Transcatheter Edge-to-Edge Repair of the Mitral Valve](https://arxiv.org/abs/2510.04178)
*Léa Pistorius,Namrata U. Nayar,Phillip Tran,Sammy Elmariah,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 本研究通过游戏控制器和机器人关节控制来简化经导管二尖瓣缘对缘修复术的操作，以克服手动操作的挑战。


<details>
  <summary>Details</summary>
Motivation: 手动控制的经导管瓣膜修复术因机械限制和学习曲线陡峭而面临挑战，本研究旨在利用机器人技术简化该过程。

Method: 将手动控制替换为通过游戏控制器进行的机器人关节控制，并在心脏模型中对手动和机器人操作的器械递送任务进行了分步比较，评估了手术时间和剪夹准确性。

Result: 与手动操作相比，机器人系统可减少手术时间、运动误差并提高剪夹准确性。

Conclusion: 机器人辅助可克服手动系统的局限性，为复杂的经导管手术提供更可靠、更易于使用的平台。

Abstract: Transcatheter valve repair presents significant challenges due to the
mechanical limitations and steep learning curve associated with manual catheter
systems. This paper investigates the use of robotics to facilitate
transcatheter procedures in the context of mitral valve edge-to-edge repair.
The complex handle-based control of a clinical repair device is replaced by
intuitive robotic joint-based control via a game controller. Manual versus
robotic performance is analyzed by decomposing the overall device delivery task
into motion-specific steps and comparing capabilities on a step-by-step basis
in a phantom model of the heart and vasculature. Metrics include procedure
duration and clip placement accuracy. Results demonstrate that the robotic
system can reduce procedural time and motion errors while also improving
accuracy of clip placement. These findings suggest that robotic assistance can
address key limitations of manual systems, offering a more reliable and
user-friendly platform for complex transcatheter procedures.

</details>


### [371] [Zenbo Patrol: A Social Assistive Robot Based on Multimodal Deep Learning for Real-time Illegal Parking Recognition and Notification](https://arxiv.org/abs/2510.04190)
*Jian-jie Zheng,Chih-kai Yang,Po-han Chen,Lyn Chao-ling Chen*

Main category: cs.RO

TL;DR: 一个能在室内停车场景中实时识别违规停车并通知管理者的社会性机器人


<details>
  <summary>Details</summary>
Motivation: 为了解决室内停车管理中的实际问题，提出一种能够实时识别违规停车并通知管理者的社会性机器人。

Method: 机器人充当巡逻者，利用GPT-4o多模态模型进行车牌识别（无需预处理），并自动调整摄像头角度捕捉车牌图像。识别车牌合法性后，若检测到违规停车，则立即向系统管理员发送Line消息。

Result: 所提出的新颖多模态深度学习方法在车牌识别方面表现出高准确性，并且所提出的社会性机器人能够解决实际场景中的问题。

Conclusion: 该研究验证了一种新颖的多模态深度学习方法在车牌识别方面的高准确性，并提供了一个可在室内停车场应用的社会性机器人，以解决实际问题。

Abstract: In the study, the social robot act as a patrol to recognize and notify
illegal parking in real-time. Dual-model pipeline method and large multimodal
model were compared, and the GPT-4o multimodal model was adopted in license
plate recognition without preprocessing. For moving smoothly on a flat ground,
the robot navigated in a simulated parking lot in the experiments. The robot
changes angle view of the camera automatically to capture the images around
with the format of license plate number. From the captured images of the robot,
the numbers on the plate are recognized through the GPT-4o model, and
identifies legality of the numbers. When an illegal parking is detected, the
robot sends Line messages to the system manager immediately. The contribution
of the work is that a novel multimodal deep learning method has validated with
high accuracy in license plate recognition, and a social assistive robot is
also provided for solving problems in a real scenario, and can be applied in an
indoor parking lot.

</details>


### [372] [Flexible Locomotion Learning with Diffusion Model Predictive Control](https://arxiv.org/abs/2510.04234)
*Runhan Huang,Haldun Balim,Heng Yang,Yilun Du*

Main category: cs.RO

TL;DR: 模型预测控制（MPC）结合了强化学习（RL）的灵活性和模型预测控制的鲁棒性，通过使用扩散模型作为动态模型先验，实现了在测试时根据奖励和约束进行灵活的适应性调整。


<details>
  <summary>Details</summary>
Motivation: 传统的基于模型的方法难以获得精确的模型，而无模型强化学习在测试时难以适应新行为。需要一种既能灵活适应又能满足任务和安全约束的方法。

Method: 提出Diffusion-MPC，利用学习到的生成扩散模型作为动态模型先验进行规划。在规划过程中，通过奖励函数规划和约束投影，生成满足任务目标和物理限制的轨迹。采用交互式训练算法，通过在环境中执行规划器并根据回报对轨迹进行加权来更新去噪器，以实现超越模仿学习的适应性。

Result: 在真实世界中进行了验证，展示了强大的运动能力和灵活的适应性。

Conclusion: Diffusion-MPC能够有效解决传统方法的局限性，实现灵活的测试时适应，无需重新训练即可满足新的奖励规范。

Abstract: Legged locomotion demands controllers that are both robust and adaptable,
while remaining compatible with task and safety considerations. However,
model-free reinforcement learning (RL) methods often yield a fixed policy that
can be difficult to adapt to new behaviors at test time. In contrast, Model
Predictive Control (MPC) provides a natural approach to flexible behavior
synthesis by incorporating different objectives and constraints directly into
its optimization process. However, classical MPC relies on accurate dynamics
models, which are often difficult to obtain in complex environments and
typically require simplifying assumptions. We present Diffusion-MPC, which
leverages a learned generative diffusion model as an approximate dynamics prior
for planning, enabling flexible test-time adaptation through reward and
constraint based optimization. Diffusion-MPC jointly predicts future states and
actions; at each reverse step, we incorporate reward planning and impose
constraint projection, yielding trajectories that satisfy task objectives while
remaining within physical limits. To obtain a planning model that adapts beyond
imitation pretraining, we introduce an interactive training algorithm for
diffusion based planner: we execute our reward-and-constraint planner in
environment, then filter and reweight the collected trajectories by their
realized returns before updating the denoiser. Our design enables strong
test-time adaptability, allowing the planner to adjust to new reward
specifications without retraining. We validate Diffusion-MPC on real world,
demonstrating strong locomotion and flexible adaptation.

</details>


### [373] [ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context](https://arxiv.org/abs/2510.04246)
*Huiwon Jang,Sihyun Yu,Heeseung Kwon,Hojin Jeon,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: ContextVLA通过将历史观测压缩为单个上下文令牌，来有效利用多帧观测，从而提高机器人任务性能，同时减少训练和推理时间。


<details>
  <summary>Details</summary>
Motivation: 先前的行为克隆工作在利用多帧观测方面表现出不一致的性能提升，而Vision-Language-Action（VLA）模型能更有效地利用这些观测，这表明VLMs具有内在的时间理解能力，可以从中提取更有意义的上下文。然而，视频输入的高维度带来了显著的计算开销，使得VLA的训练和推理效率低下。

Method: ContextVLA通过将历史观测压缩为单个上下文令牌，使策略能够有效地利用时间上下文进行动作生成。

Result: ContextVLA在实验中持续优于单帧VLA，并实现了与完整多帧训练相当的性能，同时缩短了训练和推理时间。

Conclusion: ContextVLA是一种有效的策略模型，通过压缩历史观测为单个上下文令牌，能够高效地利用多帧观测来提高机器人任务性能，克服了现有方法的局限性。

Abstract: Leveraging temporal context is crucial for success in partially observable
robotic tasks. However, prior work in behavior cloning has demonstrated
inconsistent performance gains when using multi-frame observations. In this
paper, we introduce ContextVLA, a policy model that robustly improves robotic
task performance by effectively leveraging multi-frame observations. Our
approach is motivated by the key observation that Vision-Language-Action models
(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more
effectively utilize multi-frame observations for action generation. This
suggests that VLMs' inherent temporal understanding capability enables them to
extract more meaningful context from multi-frame observations. However, the
high dimensionality of video inputs introduces significant computational
overhead, making VLA training and inference inefficient. To address this,
ContextVLA compresses past observations into a single context token, allowing
the policy to efficiently leverage temporal context for action generation. Our
experiments show that ContextVLA consistently improves over single-frame VLAs
and achieves the benefits of full multi-frame training but with reduced
training and inference times.

</details>


### [374] [Integrated Planning and Control on Manifolds: Factor Graph Representation and Toolkit](https://arxiv.org/abs/2510.04278)
*Peiwen Yang,Weisong Wen,Runqiu Yang,Yuanyuan Zhang,Jiahao Hu,Yingming Chen,Naigui Xiao,Jiaqi Zhao*

Main category: cs.RO

TL;DR: FactorMPC是一个基于因子图的MPC工具包，用于处理非流形上的系统，实现实时性能和安全的关键控制。


<details>
  <summary>Details</summary>
Motivation: 传统MPC在处理非流形系统时存在局限性，如奇异性、过度参数化和收敛性差。FactorMPC旨在克服这些挑战。

Method: FactorMPC采用因子图将动力学、约束和目标统一起来，支持流形值状态和切空间中的高斯不确定性，并结合了基于CBF的避障因子，实现了稀疏性和概率结构的利用，从而达到实时性能。

Result: 该方法在四旋翼飞行器上的仿真和实验结果表明，与基线方法相比，在轨迹跟踪和避障方面表现更优。

Conclusion: FactorMPC提供了一个可扩展且几何一致的框架，用于集成规划和控制，并已开源以促进研究可复现性。

Abstract: Model predictive control (MPC) faces significant limitations when applied to
systems evolving on nonlinear manifolds, such as robotic attitude dynamics and
constrained motion planning, where traditional Euclidean formulations struggle
with singularities, over-parameterization, and poor convergence. To overcome
these challenges, this paper introduces FactorMPC, a factor-graph based MPC
toolkit that unifies system dynamics, constraints, and objectives into a
modular, user-friendly, and efficient optimization structure. Our approach
natively supports manifold-valued states with Gaussian uncertainties modeled in
tangent spaces. By exploiting the sparsity and probabilistic structure of
factor graphs, the toolkit achieves real-time performance even for
high-dimensional systems with complex constraints. The velocity-extended
on-manifold control barrier function (CBF)-based obstacle avoidance factors are
designed for safety-critical applications. By bridging graphical models with
safety-critical MPC, our work offers a scalable and geometrically consistent
framework for integrated planning and control. The simulations and experimental
results on the quadrotor demonstrate superior trajectory tracking and obstacle
avoidance performance compared to baseline methods. To foster research
reproducibility, we have provided open-source implementation offering
plug-and-play factors.

</details>


### [375] [Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation](https://arxiv.org/abs/2510.04353)
*Stephen McCrory,Romeo Orsolino,Dhruv Thanki,Luigi Penco,Robert Griffin*

Main category: cs.RO

TL;DR: 该研究提出了一种基于质心稳定性的重定目标方法，以提高机器人本体在操作具有挑战性的接触场景（例如非共面表面）时的稳定性。


<details>
  <summary>Details</summary>
Motivation: 在远程操作人形机器人时，手部接触和非共面表面常常会导致力矩饱和或因打滑而失去稳定性。本研究旨在解决这一挑战。

Method: 提出了一种基于质心稳定性的重定目标方法，该方法在远程操作过程中动态调整接触点和姿态，以提高在困难场景下的稳定性。该方法的核心是高效地解析计算稳定性裕度梯度，利用该梯度识别对远程操作设定点稳定性敏感的场景，并指导对这些设定点进行局部调整。

Result: 在模拟和硬件实验中，通过对人形机器人进行远程操作和操作任务，证明了该框架能够提高稳定性裕度。此外，还实证表明，更高的稳定性裕度与提高的脉冲弹性和关节力矩裕度相关。

Conclusion: 所提出的基于质心稳定性的重定目标方法能够有效提高人形机器人在处理具有挑战性的接触场景时的稳定性，并与提高的脉冲弹性和关节力矩裕度相关。

Abstract: Teleoperation is a powerful method to generate reference motions and enable
humanoid robots to perform a broad range of tasks. However, teleoperation
becomes challenging when using hand contacts and non-coplanar surfaces, often
leading to motor torque saturation or loss of stability through slipping. We
propose a centroidal stability-based retargeting method that dynamically
adjusts contact points and posture during teleoperation to enhance stability in
these difficult scenarios. Central to our approach is an efficient analytical
calculation of the stability margin gradient. This gradient is used to identify
scenarios for which stability is highly sensitive to teleoperation setpoints
and inform the local adjustment of these setpoints. We validate the framework
in simulation and hardware by teleoperating manipulation tasks on a humanoid,
demonstrating increased stability margins. We also demonstrate empirically that
higher stability margins correlate with improved impulse resilience and joint
torque margin.

</details>


### [376] [Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators](https://arxiv.org/abs/2510.04354)
*Apurva Badithela,David Snyder,Lihan Zha,Joseph Mikhail,Matthew O'Kelly,Anushri Dixit,Anirudha Majumdar*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Rapid progress in imitation learning, foundation models, and large-scale
datasets has led to robot manipulation policies that generalize to a wide-range
of tasks and environments. However, rigorous evaluation of these policies
remains a challenge. Typically in practice, robot policies are often evaluated
on a small number of hardware trials without any statistical assurances. We
present SureSim, a framework to augment large-scale simulation with relatively
small-scale real-world testing to provide reliable inferences on the real-world
performance of a policy. Our key idea is to formalize the problem of combining
real and simulation evaluations as a prediction-powered inference problem, in
which a small number of paired real and simulation evaluations are used to
rectify bias in large-scale simulation. We then leverage non-asymptotic mean
estimation algorithms to provide confidence intervals on mean policy
performance. Using physics-based simulation, we evaluate both diffusion policy
and multi-task fine-tuned \(\pi_0\) on a joint distribution of objects and
initial conditions, and find that our approach saves over \(20-25\%\) of
hardware evaluation effort to achieve similar bounds on policy performance.

</details>


### [377] [PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory Optimization](https://arxiv.org/abs/2510.04436)
*Jushan Chen,Santiago Paternain*

Main category: cs.RO

TL;DR: 模型基于扩散模型，通过直接生成状态序列并结合梯度无关的投影机制来解决动力学可行性问题，在四旋翼飞行器路径规划任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的轨迹优化方法在处理非线性等式约束（即动力学可行性）方面存在挑战，通常采用单重采样方法，导致约束执行不明确且可能产生次优解。

Method: 提出一种新颖的基于模型的直接轨迹优化方法，通过扩散模型直接生成状态序列。为了保证动力学可行性，引入了一个梯度无关的投影机制到逆扩散过程中。

Result: 与现有的最先进基线相比，该方法在四旋翼飞行器密集静态障碍物导航场景中，实现了零动力学可行性误差，并且成功率大约提高了4倍。

Conclusion: 所提出的模型可以有效解决动力学可行性问题，并显著提高轨迹优化的成功率。

Abstract: Recently, diffusion models have gained popularity and attention in trajectory
optimization due to their capability of modeling multi-modal probability
distributions. However, addressing nonlinear equality constraints, i.e, dynamic
feasi- bility, remains a great challenge in diffusion-based trajectory
optimization. Recent diffusion-based trajectory optimization frameworks rely on
a single-shooting style approach where the denoised control sequence is applied
to forward propagate the dynamical system, which cannot explicitly enforce
constraints on the states and frequently leads to sub-optimal solutions. In
this work, we propose a novel direct trajectory optimization approach via
model-based diffusion, which directly generates a sequence of states. To ensure
dynamic feasibility, we propose a gradient-free projection mechanism that is
incorporated into the reverse diffusion process. Our results show that,
compared to a recent state-of-the-art baseline, our approach leads to zero
dynamic feasibility error and approximately 4x higher success rate in a
quadrotor waypoint navigation scenario involving dense static obstacles.

</details>


### [378] [Velocity-Form Data-Enabled Predictive Control of Soft Robots under Unknown External Payloads](https://arxiv.org/abs/2510.04509)
*Huanqing Wang,Kaixiang Zhang,Kyungjoon Lee,Yu Mei,Vaibhav Srivastava,Jun Sheng,Ziyou Song,Zhaojian Li*

Main category: cs.RO

TL;DR: 本文提出了一种新的速度形式的 DeePC 框架，用于在未知负载下实现软机器人的鲁棒和最优控制。


<details>
  <summary>Details</summary>
Motivation: 在物体操作任务中，未知的外部负载和干扰会显著改变系统动力学和行为，导致偏移误差和控制性能下降。

Method: 所提出的框架利用增量表示中的输入-输出数据来缓解由未知负载引起性能下降，无需加权数据集或干扰估计器。

Result: 实验上在平面软机器人上验证了该方法，并证明了其在涉及未知负载的情况下优于标准的 DeePC。

Conclusion: 所提出的速度形式的 DeePC 框架能够实现软机器人再未知负载下的鲁棒和最优控制。

Abstract: Data-driven control methods such as data-enabled predictive control (DeePC)
have shown strong potential in efficient control of soft robots without
explicit parametric models. However, in object manipulation tasks, unknown
external payloads and disturbances can significantly alter the system dynamics
and behavior, leading to offset error and degraded control performance. In this
paper, we present a novel velocity-form DeePC framework that achieves robust
and optimal control of soft robots under unknown payloads. The proposed
framework leverages input-output data in an incremental representation to
mitigate performance degradation induced by unknown payloads, eliminating the
need for weighted datasets or disturbance estimators. We validate the method
experimentally on a planar soft robot and demonstrate its superior performance
compared to standard DeePC in scenarios involving unknown payloads.

</details>


### [379] [Everything-Grasping (EG) Gripper: A Universal Gripper with Synergistic Suction-Grasping Capabilities for Cross-Scale and Cross-State Manipulation](https://arxiv.org/abs/2510.04585)
*Jianshu Zhou,Jing Shu,Tianle Pan,Puchen Zhu,Jiajun An,Huayu Zhang,Junda Huang,Upinder Kaur,Xin Ma,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: EG Gripper 是一种软体机器人夹爪，通过结合表面吸附和颗粒物阻塞技术，能够抓取各种尺寸和状态（包括固态和液态）的物体，并配备触觉传感框架和自主抓取模式选择算法。


<details>
  <summary>Details</summary>
Motivation: 在软体机器人领域，使用单一的机器人夹爪抓取各种尺寸和物理状态（包括固体和液体）的物体仍然是一个基本挑战。

Method: EG Gripper 结合了分布式表面吸附和内部颗粒物阻塞技术，无需在与目标物体的接触界面进行气密密封，即可实现跨尺度和跨状态的操作。该夹爪还引入了一个结合了液体检测和基于压力的吸附反馈的触觉传感框架，并采用由分布式压力和电压信号指导的触觉推断抓取模式选择（TIGMS）算法，以自主选择抓取模式。

Result: EG Gripper 能够处理表面积从亚毫米级的 0.2 mm²（玻璃珠）到超过 62,000 mm²（A4 纸和编织袋）的物体，抓取能力几乎是其自身接触面积（近似为 707 mm²）的 3500 倍到 88 倍。实验证明了其在水下抓取、易碎物体处理和液体捕获等多样化任务中的稳健性和可重复性。

Conclusion: EG Gripper 是首个能够使用统一的顺应性结构可靠抓取固态和液态物体的软体机器人夹爪。

Abstract: Grasping objects across vastly different sizes and physical states-including
both solids and liquids-with a single robotic gripper remains a fundamental
challenge in soft robotics. We present the Everything-Grasping (EG) Gripper, a
soft end-effector that synergistically integrates distributed surface suction
with internal granular jamming, enabling cross-scale and cross-state
manipulation without requiring airtight sealing at the contact interface with
target objects. The EG Gripper can handle objects with surface areas ranging
from sub-millimeter scale 0.2 mm2 (glass bead) to over 62,000 mm2 (A4 sized
paper and woven bag), enabling manipulation of objects nearly 3,500X smaller
and 88X larger than its own contact area (approximated at 707 mm2 for a 30
mm-diameter base). We further introduce a tactile sensing framework that
combines liquid detection and pressure-based suction feedback, enabling
real-time differentiation between solid and liquid targets. Guided by the
actile-Inferred Grasping Mode Selection (TIGMS) algorithm, the gripper
autonomously selects grasping modes based on distributed pressure and voltage
signals. Experiments across diverse tasks-including underwater grasping,
fragile object handling, and liquid capture-demonstrate robust and repeatable
performance. To our knowledge, this is the first soft gripper to reliably grasp
both solid and liquid objects across scales using a unified compliant
architecture.

</details>


### [380] [MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile Manipulation](https://arxiv.org/abs/2510.04592)
*Yilin Mei,Peng Qiu,Wei Zhang,WenChao Zhang,Wenjie Song*

Main category: cs.RO

TL;DR: MobRT是一个基于数字孪生的框架，可以为移动操作生成大量高质量的演示数据，从而提高模仿学习的性能。


<details>
  <summary>Details</summary>
Motivation: 收集移动操作的演示数据具有挑战性，因为它们需要在高维、动态和部分可观察的环境中协调机器人底盘的运动和手臂的操作。现有研究主要集中在简单的桌面场景，导致移动操作相对缺乏探索。

Method: MobRT通过整合虚拟运动学控制和全身运动规划，自主生成多样化和真实的演示。该框架能够模拟两种主要的复杂全身任务：与关节对象交互（例如开门、抽屉）和移动基座拾取-放置操作。

Result: MobRT生成的演示数据的质量在多个基线算法上得到了评估，建立了一个全面的基准，并证明了任务成功与生成轨迹数量之间存在很强的相关性。将模拟和真实世界的演示结合起来的实验证实，该方法显著提高了策略的泛化能力和性能，在模拟和真实世界环境中都取得了稳健的结果。

Conclusion: MobRT框架能够克服移动操作中演示数据收集的挑战，并通过生成高质量的演示数据来显著提高模仿学习策略的性能和泛化能力。

Abstract: Recent advances in robotics have been largely driven by imitation learning,
which depends critically on large-scale, high-quality demonstration data.
However, collecting such data remains a significant challenge-particularly for
mobile manipulators, which must coordinate base locomotion and arm manipulation
in high-dimensional, dynamic, and partially observable environments.
Consequently, most existing research remains focused on simpler tabletop
scenarios, leaving mobile manipulation relatively underexplored. To bridge this
gap, we present \textit{MobRT}, a digital twin-based framework designed to
simulate two primary categories of complex, whole-body tasks: interaction with
articulated objects (e.g., opening doors and drawers) and mobile-base
pick-and-place operations. \textit{MobRT} autonomously generates diverse and
realistic demonstrations through the integration of virtual kinematic control
and whole-body motion planning, enabling coherent and physically consistent
execution. We evaluate the quality of \textit{MobRT}-generated data across
multiple baseline algorithms, establishing a comprehensive benchmark and
demonstrating a strong correlation between task success and the number of
generated trajectories. Experiments integrating both simulated and real-world
demonstrations confirm that our approach markedly improves policy
generalization and performance, achieving robust results in both simulated and
real-world environments.

</details>


### [381] [OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS](https://arxiv.org/abs/2510.04612)
*Simon Boche,Jaehyung Jung,Sebastián Barbas Laina,Stefan Leutenegger*

Main category: cs.RO

TL;DR: OKVIS2-X是一个先进的多传感器SLAM系统，能够构建高精度、高鲁棒性的稠密体积占 occupancy 地图，并支持多种传感器（视觉、惯性、深度、LiDAR、GNSS），在大型环境中实时运行，并达到了顶尖的精度表现。


<details>
  <summary>Details</summary>
Motivation: 为了赋予移动机器人可用地图以及最高的状态估计精度和鲁棒性。

Method: OKVIS2-X是一个统一的SLAM框架，集成了多种传感器（视觉、惯性、深度、LiDAR、GNSS），采用稠密体积地图表示，通过高效的子映射策略实现可扩展性，并利用地图对齐因子将估计器和子地图紧密耦合，还可选配在线标定相机外参。

Result: 在EuRoC数据集上实现了最高的轨迹精度，在Hilti22 VI-only基准测试中超越了所有竞争对手，并在VBR数据集中展现了顶尖的精度。

Conclusion: OKVIS2-X系统提供全局一致的地图，可直接用于自主导航，在精度和鲁棒性方面表现优异。

Abstract: To empower mobile robots with usable maps as well as highest state estimation
accuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor
Simultaneous Localization and Mapping (SLAM) system building dense volumetric
occupancy maps, while scalable to large environments and operating in realtime.
Our unified SLAM framework seamlessly integrates different sensor modalities:
visual, inertial, measured or learned depth, LiDAR and Global Navigation
Satellite System (GNSS) measurements. Unlike most state-of-the-art SLAM
systems, we advocate using dense volumetric map representations when leveraging
depth or range-sensing capabilities. We employ an efficient submapping strategy
that allows our system to scale to large environments, showcased in sequences
of up to 9 kilometers. OKVIS2-X enhances its accuracy and robustness by
tightly-coupling the estimator and submaps through map alignment factors. Our
system provides globally consistent maps, directly usable for autonomous
navigation. To further improve the accuracy of OKVIS2-X, we also incorporate
the option of performing online calibration of camera extrinsics. Our system
achieves the highest trajectory accuracy in EuRoC against state-of-the-art
alternatives, outperforms all competitors in the Hilti22 VI-only benchmark,
while also proving competitive in the LiDAR version, and showcases state of the
art accuracy in the diverse and large-scale sequences from the VBR dataset.

</details>


### [382] [Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies](https://arxiv.org/abs/2510.04692)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.RO

TL;DR: 本研究提出了一种仿生机械臂平台，能够模拟雌性鸨的形态和外观，以支持野外生态学研究和保护工作。


<details>
  <summary>Details</summary>
Motivation: 在野外研究鸟类行为具有挑战性，需要高度逼真的形态、耐用的户外运行能力和智能感知能力，以适应不可控的环境。

Method: 该系统采用全数字化可复制的制造流程，结合高分辨率结构光3D扫描、参数化CAD建模、关节式3D打印和照片级UV纹理乙烯基饰面，以实现解剖学上精确且耐用的仿生机械臂。六轮摇臂转向架底盘确保在沙地和不平坦地形上的稳定移动，而嵌入式NVIDIA Jetson模块支持实时RGB和热感应、轻量级YOLO检测以及自主视觉伺服环路，可在无人干预的情况下将机械臂的头部对准检测到的目标。轻量级热可见融合模块可提高弱光条件下的感知能力。

Result: 在沙漠鸟舍进行的现场试验表明，该平台能够以15到22 FPS的帧率可靠地进行实时操作，延迟低于100毫秒，并能在恶劣的户外条件下引起活鸨的自然识别和互动反应。

Conclusion: 该集成框架通过结合可复制的数字制造、具身视觉智能和生态学验证，推进了仿生野外机器人技术，为动物-机器人交互研究、保护机器人技术和公众参与提供了可转移的蓝图。

Abstract: Biomimetic intelligence and robotics are transforming field ecology by
enabling lifelike robotic surrogates that interact naturally with animals under
real world conditions. Studying avian behavior in the wild remains challenging
due to the need for highly realistic morphology, durable outdoor operation, and
intelligent perception that can adapt to uncontrolled environments. We present
a next generation bio inspired robotic platform that replicates the morphology
and visual appearance of the female Houbara bustard to support controlled
ethological studies and conservation oriented field research. The system
introduces a fully digitally replicable fabrication workflow that combines high
resolution structured light 3D scanning, parametric CAD modelling, articulated
3D printing, and photorealistic UV textured vinyl finishing to achieve
anatomically accurate and durable robotic surrogates. A six wheeled rocker
bogie chassis ensures stable mobility on sand and irregular terrain, while an
embedded NVIDIA Jetson module enables real time RGB and thermal perception,
lightweight YOLO based detection, and an autonomous visual servoing loop that
aligns the robot's head toward detected targets without human intervention. A
lightweight thermal visible fusion module enhances perception in low light
conditions. Field trials in desert aviaries demonstrated reliable real time
operation at 15 to 22 FPS with latency under 100 ms and confirmed that the
platform elicits natural recognition and interactive responses from live
Houbara bustards under harsh outdoor conditions. This integrated framework
advances biomimetic field robotics by uniting reproducible digital fabrication,
embodied visual intelligence, and ecological validation, providing a
transferable blueprint for animal robot interaction research, conservation
robotics, and public engagement.

</details>


### [383] [Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly](https://arxiv.org/abs/2510.04696)
*Alexander L. Mitchell,Joe Watson,Ingmar Posner*

Main category: cs.RO

TL;DR: 该研究提出了一种去中心化的、基于梯度的框架，通过自动组合自适应势函数来生成分段连续能量函数，从而简化双臂装配任务的规划。该方法仅使用短视优化生成子目标，但由于能量函数的结构和自适应性，能够有效地解决长时序任务，并能快速重新规划以应对干扰，同时在物理双臂装配任务中表现出良好的可扩展性，能够自发地生成重试、协调运动和自主交接等行为。


<details>
  <summary>Details</summary>
Motivation: 传统的任务与运动规划（TAMP）方法在处理双臂装配任务时，尤其是在面对干扰导致需要重新排序和优化任务时，可能收敛速度过慢。此外，为装配任务定义明确的任务序列可能很繁琐，限制了重新规划的灵活性。

Method: 提出了一种去中心化的、基于梯度的框架，该框架通过自动组合自适应势函数来生成分段连续能量函数。该方法仅使用短视优化来生成子目标，而不是进行长时序规划。

Result: 该方法在解决长时序任务方面表现出有效性，并且能够扩展到物理双臂装配任务。实验表明，该基于梯度的快速重新规划框架能够自发地生成重试、协调运动和自主交接。

Conclusion: 所提出的去中心化梯度下降框架能够通过自动组合势函数来处理复杂的双臂装配任务，其固有的灵活性和适应性使其能够克服传统TAMP方法的局限性，并能生成 emergent 的行为，如重试、协调运动和自主交接。

Abstract: There are many challenges in bimanual assembly, including high-level
sequencing, multi-robot coordination, and low-level, contact-rich operations
such as component mating. Task and motion planning (TAMP) methods, while
effective in this domain, may be prohibitively slow to converge when adapting
to disturbances that require new task sequencing and optimisation. These events
are common during tight-tolerance assembly, where difficult-to-model dynamics
such as friction or deformation require rapid replanning and reattempts.
Moreover, defining explicit task sequences for assembly can be cumbersome,
limiting flexibility when task replanning is required. To simplify this
planning, we introduce a decentralised gradient-based framework that uses a
piecewise continuous energy function through the automatic composition of
adaptive potential functions. This approach generates sub-goals using only
myopic optimisation, rather than long-horizon planning. It demonstrates
effectiveness at solving long-horizon tasks due to the structure and adaptivity
of the energy function. We show that our approach scales to physical bimanual
assembly tasks for constructing tight-tolerance assemblies. In these
experiments, we discover that our gradient-based rapid replanning framework
generates automatic retries, coordinated motions and autonomous handovers in an
emergent fashion.

</details>


### [384] [Performance-guided Task-specific Optimization for Multirotor Design](https://arxiv.org/abs/2510.04724)
*Etor Arza,Welf Rehberg,Philipp Weiss,Mihir Kulkarni,Kostas Alexis*

Main category: cs.RO

TL;DR: 利用强化学习、贝叶斯优化和协方差矩阵自适应进化策略，对多旋翼微型飞行器的任务特定设计进行优化，以提高敏捷路径点导航任务的性能。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在优化多旋翼微型飞行器的任务特定设计，以实现卓越的闭环性能。

Method: 采用强化学习、贝叶斯优化和协方差矩阵自适应进化策略，在考虑任务需求的同时，对飞行器设计进行系统性探索和优化，并确保可制造性和最小化气动干扰。

Result: 优化的设计在敏捷路径点导航任务中表现优于传统多旋翼配置，甚至优于文献中的全驱动设计。

Conclusion: 所提出的方法能够实现有效的任务特定设计优化，并且具有良好的sim2real迁移能力，已通过真实世界实验得到验证。

Abstract: This paper introduces a methodology for task-specific design optimization of
multirotor Micro Aerial Vehicles. By leveraging reinforcement learning,
Bayesian optimization, and covariance matrix adaptation evolution strategy, we
optimize aerial robot designs guided exclusively by their closed-loop
performance in a considered task. Our approach systematically explores the
design space of motor pose configurations while ensuring manufacturability
constraints and minimal aerodynamic interference. Results demonstrate that
optimized designs achieve superior performance compared to conventional
multirotor configurations in agile waypoint navigation tasks, including against
fully actuated designs from the literature. We build and test one of the
optimized designs in the real world to validate the sim2real transferability of
our approach.

</details>


### [385] [TAG-K: Tail-Averaged Greedy Kaczmarz for Computationally Efficient and Performant Online Inertial Parameter Estimation](https://arxiv.org/abs/2510.04839)
*Shuo Sha,Anupam Bhakta,Zhenyuan Jiang,Kevin Qiu,Ishaan Mahajan,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: TAG-K是一种结合了贪婪随机行选择和尾部平均的Kaczmarz方法扩展，用于快速、稳定地进行在线惯性参数估计，在合成基准和四旋翼飞行器任务中，其求解速度比RLS、KF等方法快1.5-20.7倍，同时提高了对噪声的鲁棒性，并减少了25%的估计误差。


<details>
  <summary>Details</summary>
Motivation: 为了实现自适应机器人控制，需要精确的在线惯性参数估计，以便对机器人有效载荷变化、环境交互和系统磨损进行实时调整。传统方法（如递归最小二乘法和卡尔曼滤波器）在跟踪突然的参数变化或处理高计算成本时存在困难，限制了它们在动态环境和计算资源有限的机器人系统中的应用。

Method: 提出了一种名为TAG-K的Kaczmarz方法轻量级扩展。该方法结合了贪婪随机行选择（以实现快速收敛）和尾部平均（以在噪声和不一致性下提高鲁棒性）。

Result: 在合成基准和四旋翼飞行器跟踪任务中，TAG-K的求解速度在笔记本级CPU上比RLS、KF等方法快1.5-1.9倍，在嵌入式微控制器上快4.8-20.7倍。此外，TAG-K的估计误差减少了25%，提高了2倍的端到端跟踪性能，并增强了对测量噪声的抵抗力。

Conclusion: TAG-K通过结合贪婪随机行选择和尾部平均，在保持Kaczmarz框架的低迭代复杂性的同时，实现了快速、稳定的参数自适应。该方法在速度和准确性方面均优于传统方法，在机器人控制领域具有广泛的应用前景。

Abstract: Accurate online inertial parameter estimation is essential for adaptive
robotic control, enabling real-time adjustment to payload changes,
environmental interactions, and system wear. Traditional methods such as
Recursive Least Squares (RLS) and the Kalman Filter (KF) often struggle to
track abrupt parameter shifts or incur high computational costs, limiting their
effectiveness in dynamic environments and for computationally constrained
robotic systems. As such, we introduce TAG-K, a lightweight extension of the
Kaczmarz method that combines greedy randomized row selection for rapid
convergence with tail averaging for robustness under noise and inconsistency.
This design enables fast, stable parameter adaptation while retaining the low
per-iteration complexity inherent to the Kaczmarz framework. We evaluate TAG-K
in synthetic benchmarks and quadrotor tracking tasks against RLS, KF, and other
Kaczmarz variants. TAG-K achieves 1.5x-1.9x faster solve times on laptop-class
CPUs and 4.8x-20.7x faster solve times on embedded microcontrollers. More
importantly, these speedups are paired with improved resilience to measurement
noise and a 25% reduction in estimation error, leading to nearly 2x better
end-to-end tracking performance.

</details>


### [386] [CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery](https://arxiv.org/abs/2510.04883)
*Nathan Shankar,Pawel Ladosz,Hujun Yin*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents a novel approach for enabling robust robotic perception
in dark environments using infrared (IR) stream. IR stream is less susceptible
to noise than RGB in low-light conditions. However, it is dominated by active
emitter patterns that hinder high-level tasks such as object detection,
tracking and localisation. To address this, a U-Net-based architecture is
proposed that reconstructs clean IR images from emitter-populated input,
improving both image quality and downstream robotic performance. This approach
outperforms existing enhancement techniques and enables reliable operation of
vision-driven robotic systems across illumination conditions from well-lit to
extreme low-light scenes.

</details>


### [387] [HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks](https://arxiv.org/abs/2510.04898)
*Zheng Xiong,Kang Li,Zilin Wang,Matthew Jackson,Jakob Foerster,Shimon Whiteson*

Main category: cs.RO

TL;DR: HyperVLA通过引入新颖的超网络（HN）架构，在保持高模型容量的同时，显著降低了视觉-语言-动作（VLA）模型在推理时的高昂成本，在不牺牲性能的情况下实现了高达90倍的参数缩减和120倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型存在推理成本过高的问题，限制了其在机器人领域的广泛应用。

Method: 提出了一种名为HyperVLA的新型VLA模型。该模型采用超网络（HN）架构，在推理时仅激活一小部分特定任务的策略，而在训练时则保留高模型容量以适应多样化的多任务行为。为解决HN-VLA训练的难点，设计了利用现有视觉基础模型先验知识、HN归一化以及动作生成策略等算法。

Result: 与单一结构的VLA模型相比，HyperVLA在零样本泛化和少样本适应方面取得了相似甚至更高的成功率，同时显著降低了推理成本。与最先进的OpenVLA模型相比，HyperVLA在测试时激活的参数数量减少了90倍，推理速度加快了120倍。

Conclusion: HyperVLA成功地解决了现有VLA模型推理成本高的问题，在保持甚至提升性能的同时，大幅提高了推理效率，为通用机器人策略的学习提供了更优的解决方案。

Abstract: Built upon language and vision foundation models with strong generalization
ability and trained on large-scale robotic data, Vision-Language-Action (VLA)
models have recently emerged as a promising approach to learning generalist
robotic policies. However, a key drawback of existing VLAs is their extremely
high inference costs. In this paper, we propose HyperVLA to address this
problem. Unlike existing monolithic VLAs that activate the whole model during
both training and inference, HyperVLA uses a novel hypernetwork (HN)-based
architecture that activates only a small task-specific policy during inference,
while still retaining the high model capacity needed to accommodate diverse
multi-task behaviors during training. Successfully training an HN-based VLA is
nontrivial so HyperVLA contains several key algorithm design features that
improve its performance, including properly utilizing the prior knowledge from
existing vision foundation models, HN normalization, and an action generation
strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even
higher success rate for both zero-shot generalization and few-shot adaptation,
while significantly reducing inference costs. Compared to OpenVLA, a
state-of-the-art VLA model, HyperVLA reduces the number of activated parameters
at test time by $90\times$, and accelerates inference speed by $120\times$.
Code is publicly available at https://github.com/MasterXiong/HyperVLA

</details>


### [388] [Efficient Navigation in Unknown Indoor Environments with Vision-Language Models](https://arxiv.org/abs/2510.04991)
*D. Schwartz,K. Kondo,J. P. How*

Main category: cs.RO

TL;DR: 该研究提出了一种利用视觉-语言模型（VLM）进行自主导航的新型高层规划框架，用于在充满死胡同的未知室内环境中提高导航效率。


<details>
  <summary>Details</summary>
Motivation: 传统探索方法由于缺乏全局推理能力和对局部启发式的依赖，常导致路线低效。本研究旨在通过引入VLM来克服这些限制。

Method: 该框架将3D占用栅格转换为2D局部地图，并让VLM以零样本（zero-shot）方式直接推理地图，生成并评估候选子目标，选择最有可能导向高效路径的子目标。该方法集成到DYNUS轨迹规划器中。

Result: 在模拟环境中，与现有方法相比，该框架使导航效率提高了约10%，路径更短。VLM能够从不完整的地图中推断出结构模式（如房间、走廊），并在前进和探索未知空间之间取得平衡，从而减少了贪婪式规划中的常见失败（如绕道进入小房间）。

Conclusion: 本研究成功地将VLM应用于高层导航规划，通过增强环境理解和推理能力，显著提高了在未知室内环境中的导航效率。

Abstract: We present a novel high-level planning framework that leverages
vision-language models (VLMs) to improve autonomous navigation in unknown
indoor environments with many dead ends. Traditional exploration methods often
take inefficient routes due to limited global reasoning and reliance on local
heuristics. In contrast, our approach enables a VLM to reason directly about an
occupancy map in a zero-shot manner, selecting subgoals that are likely to lead
to more efficient paths. At each planning step, we convert a 3D occupancy grid
into a partial 2D map of the environment, and generate candidate subgoals. Each
subgoal is then evaluated and ranked against other candidates by the model. We
integrate this planning scheme into DYNUS \cite{kondo2025dynus}, a
state-of-the-art trajectory planner, and demonstrate improved navigation
efficiency in simulation. The VLM infers structural patterns (e.g., rooms,
corridors) from incomplete maps and balances the need to make progress toward a
goal against the risk of entering unknown space. This reduces common greedy
failures (e.g., detouring into small rooms) and achieves about 10\% shorter
paths on average.

</details>


### [389] [Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot](https://arxiv.org/abs/2510.05001)
*Aditya Sripada,Abhishek Warrier*

Main category: cs.RO

TL;DR: TARS3D是一个受电影《星际穿越》启发的机器人，具有独特的非人形设计，可通过分析建模和深度强化学习实现多种运动模式。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人步态研究通常借鉴仿生腿部设计，但许多人类工程环境可以从非人形设计中受益。TARS3D机器人将电影中的TARS机器人转化为一个0.25米、0.99公斤的研究平台，具有七个驱动自由度，旨在探索非仿生设计的优势。

Method: 研究人员为TARS3D的两种主要步态（类似双足行走和高速滚动）建立了降阶模型，推导了闭环极限环条件，并在硬件上进行了验证。对于未探索的步态空间，研究人员使用深度强化学习（DRL）在模拟中进行搜索。

Result: 实验验证了TARS3D机器人能够遵守其+/-150度的臀部限制，在不干扰的情况下交替进行左右接触，并在滚动模式下保持八步混合极限环。DRL策略在给定适当先验知识的情况下，能够恢复分析步态并发现新的运动行为。

Conclusion: TARS3D机器人独特的、受科幻启发的形态可以实现多种先前未探索的运动模式，并且进一步的驱动学习很可能会揭示更多。分析综合与强化学习的结合为多模式机器人研究开辟了一条有前途的道路。

Abstract: Robotic locomotion research typically draws from biologically inspired leg
designs, yet many human-engineered settings can benefit from
non-anthropomorphic forms. TARS3D translates the block-shaped 'TARS' robot from
Interstellar into a 0.25 m, 0.99 kg research platform with seven actuated
degrees of freedom. The film shows two primary gaits: a bipedal-like walk and a
high-speed rolling mode. For TARS3D, we build reduced-order models for each,
derive closed-form limit-cycle conditions, and validate the predictions on
hardware. Experiments confirm that the robot respects its +/-150 degree hip
limits, alternates left-right contacts without interference, and maintains an
eight-step hybrid limit cycle in rolling mode. Because each telescopic leg
provides four contact corners, the rolling gait is modeled as an eight-spoke
double rimless wheel. The robot's telescopic leg redundancy implies a far
richer gait repertoire than the two limit cycles treated analytically. So, we
used deep reinforcement learning (DRL) in simulation to search the unexplored
space. We observed that the learned policy can recover the analytic gaits under
the right priors and discover novel behaviors as well. Our findings show that
TARS3D's fiction-inspired bio-transcending morphology can realize multiple
previously unexplored locomotion modes and that further learning-driven search
is likely to reveal more. This combination of analytic synthesis and
reinforcement learning opens a promising pathway for multimodal robotics.

</details>


### [390] [StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation](https://arxiv.org/abs/2510.05057)
*Mingyu Liu,Jiuhe Shu,Hui Chen,Zeju Li,Canyu Zhao,Jiange Yang,Shenyuan Gao,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: 提出一种无监督方法，利用轻量级编码器和预训练的Diffusion Transformer (DiT)解码器，学习高效、可解释的压缩两-token状态表示，并生成潜在动作，从而提高机器人任务的成功率和策略协同训练的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在状态表示的紧凑性和信息量之间难以取得平衡，导致表示冗余或缺乏关键任务信息。

Method: 提出一种无监督方法，使用轻量级编码器和预训练的Diffusion Transformer (DiT)解码器学习压缩的两-token状态表示，并利用潜在空间插值生成潜在动作。

Result: 在LIBERO上性能提升14.3%，真实世界任务成功率提升30%，策略协同训练性能提升10.4%，且推理开销和对视频数据的依赖性较低。

Conclusion: 所提出的StaMo方法能够从静态图像中学习可泛化的机器人运动，生成高效、可解释的状态表示和潜在动作，克服了现有方法在状态表示和动作学习上的局限性。

Abstract: A fundamental challenge in embodied intelligence is developing expressive and
compact state representations for efficient world modeling and decision making.
However, existing methods often fail to achieve this balance, yielding
representations that are either overly redundant or lacking in task-critical
information. We propose an unsupervised approach that learns a highly
compressed two-token state representation using a lightweight encoder and a
pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong
generative prior. Our representation is efficient, interpretable, and
integrates seamlessly into existing VLA-based models, improving performance by
14.3% on LIBERO and 30% in real-world task success with minimal inference
overhead. More importantly, we find that the difference between these tokens,
obtained via latent interpolation, naturally serves as a highly effective
latent action, which can be further decoded into executable robot actions. This
emergent capability reveals that our representation captures structured
dynamics without explicit supervision. We name our method StaMo for its ability
to learn generalizable robotic Motion from compact State representation, which
is encoded from static images, challenging the prevalent dependence to learning
latent action on complex architectures and video data. The resulting latent
actions also enhance policy co-training, outperforming prior methods by 10.4%
with improved interpretability. Moreover, our approach scales effectively
across diverse data sources, including real-world robot data, simulation, and
human egocentric video.

</details>


### [391] [Automaton Constrained Q-Learning](https://arxiv.org/abs/2510.05061)
*Anastasios Manganaris,Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: ACQL是一种结合了目标条件值学习和自动机引导强化学习的算法，用于解决机器人任务中的时序目标和安全约束问题。


<details>
  <summary>Details</summary>
Motivation: 标准的强化学习（RL）在处理需要按顺序实现目标并遵守时变安全约束的现实机器人任务时存在局限性。现有的结合RL和线性时间逻辑（LTL）的方法在复杂连续环境中表现不佳，无法同时支持时序目标和安全约束。

Method: ACQL算法结合了目标条件值学习和自动机引导强化学习。它利用LTL任务规范的自动机表示来显式编码分阶段的目标进展以及静态和非静态的安全约束。

Result: ACQL在各种连续控制任务中表现优于现有方法，即使在现有方法失败的情况下也能满足目标和安全约束。在6自由度机械臂的实际应用中，ACQL成功地在有安全约束的杂乱环境中完成了目标达成任务。

Conclusion: ACQL是一种强大且可扩展的解决方案，能够根据丰富的时序规范学习机器人行为，解决了现有RL方法在处理复杂机器人任务时的不足。

Abstract: Real-world robotic tasks often require agents to achieve sequences of goals
while respecting time-varying safety constraints. However, standard
Reinforcement Learning (RL) paradigms are fundamentally limited in these
settings. A natural approach to these problems is to combine RL with
Linear-time Temporal Logic (LTL), a formal language for specifying complex,
temporally extended tasks and safety constraints. Yet, existing RL methods for
LTL objectives exhibit poor empirical performance in complex and continuous
environments. As a result, no scalable methods support both temporally ordered
goals and safety simultaneously, making them ill-suited for realistic robotics
scenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm
that addresses this gap by combining goal-conditioned value learning with
automaton-guided reinforcement. ACQL supports most LTL task specifications and
leverages their automaton representation to explicitly encode stage-wise goal
progression and both stationary and non-stationary safety constraints. We show
that ACQL outperforms existing methods across a range of continuous control
tasks, including cases where prior methods fail to satisfy either goal-reaching
or safety constraints. We further validate its real-world applicability by
deploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a
cluttered, cabinet-like space with safety constraints. Our results demonstrate
that ACQL is a robust and scalable solution for learning robotic behaviors
according to rich temporal specifications.

</details>


### [392] [ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning](https://arxiv.org/abs/2510.05070)
*Siheng Zhao,Yanjie Ze,Yue Wang,C. Karen Liu,Pieter Abbeel,Guanya Shi,Rocky Duan*

Main category: cs.RO

TL;DR: ResMimic是一个两阶段残差学习框架，用于从人类运动数据中进行精确和富有表现力的人形控制。


<details>
  <summary>Details</summary>
Motivation: 通用运动追踪（GMT）策略缺乏精确性和物体感知能力，无法满足人形机器人在日常服务和仓库任务中的 loco-manipulation 需求。

Method: ResMimic首先使用大规模仅人类运动数据训练一个通用的GMT策略，然后学习一个高效且精确的残差策略来优化GMT输出，以提升运动能力并融入物体交互。该框架还包括基于点云的目标追踪奖励、鼓励精确人机物体交互的接触奖励以及用于稳定早期训练的基于课程的虚拟物体控制器。

Result: 在模拟和真实Unitree G1机器人上的评估显示，与现有基线方法相比，ResMimic在任务成功率、训练效率和鲁棒性方面均有显著提升。

Conclusion: ResMimic框架能够实现精确且富有表现力的人形机器人 loco-manipulation 控制。

Abstract: Humanoid whole-body loco-manipulation promises transformative capabilities
for daily service and warehouse tasks. While recent advances in general motion
tracking (GMT) have enabled humanoids to reproduce diverse human motions, these
policies lack the precision and object awareness required for
loco-manipulation. To this end, we introduce ResMimic, a two-stage residual
learning framework for precise and expressive humanoid control from human
motion data. First, a GMT policy, trained on large-scale human-only motion,
serves as a task-agnostic base for generating human-like whole-body movements.
An efficient but precise residual policy is then learned to refine the GMT
outputs to improve locomotion and incorporate object interaction. To further
facilitate efficient training, we design (i) a point-cloud-based object
tracking reward for smoother optimization, (ii) a contact reward that
encourages accurate humanoid body-object interactions, and (iii) a
curriculum-based virtual object controller to stabilize early training. We
evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results
show substantial gains in task success, training efficiency, and robustness
over strong baselines. Videos are available at https://resmimic.github.io/ .

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [393] [Efficient MPC-Based Energy Management System for Secure and Cost-Effective Microgrid Operations](https://arxiv.org/abs/2510.03241)
*Hanyang He,John Harlim,Daning Huang,Yan Li*

Main category: eess.SY

TL;DR: 提出了一种考虑潮流损耗和电网安全约束的高效可靠的基于MPC的EMS，并集成了在线需求响应模块以进一步降低成本。


<details>
  <summary>Details</summary>
Motivation: 为了在高渗透率分布式能源的微电网中实现最优、安全和稳定运行，需要一种能解决传统MPC-EMS因简化模型而无法满足大规模系统需求的EMS。

Method: 通过集成二阶锥规划（SOCP）潮流松弛到约束集中，将问题转化为凸规划，并设计了一个在线需求响应（DR）模块来实现削峰填谷。

Result: 该方法在10、18和33节点系统上得到了验证，证明了其在保证安全运行、有效削峰和降低总成本方面的有效性。

Conclusion: 所提出的基于MPC的EMS框架能够同时对损耗和安全约束进行建模，并通过协调柔性负荷来降低运行成本，具有在线实施的潜力。

Abstract: Model predictive control (MPC)-based energy management systems (EMS) are
essential for ensuring optimal, secure, and stable operation in microgrids with
high penetrations of distributed energy resources. However, due to the high
computational cost for the decision-making, the conventional MPC-based EMS
typically adopts a simplified integrated-bus power balance model. While this
simplification is effective for small networks, large-scale systems require a
more detailed branch flow model to account for the increased impact of grid
power losses and security constraints. This work proposes an efficient and
reliable MPC-based EMS that incorporates power-loss effects and grid-security
constraints. %, while adaptively shaping the battery power profile in response
to online renewable inputs, achieving reduced operational costs. It enhances
system reliability, reduces operational costs, and shows strong potential for
online implementation due to its reduced computational effort. Specifically, a
second-order cone program (SOCP) branch flow relaxation is integrated into the
constraint set, yielding a convex formulation that guarantees globally optimal
solutions with high computational efficiency. Owing to the radial topology of
the microgrid, this relaxation is practically tight, ensuring equivalence to
the original problem. Building on this foundation, an online demand response
(DR) module is designed to further reduce the operation cost through peak
shaving. To the best of our knowledge, no prior MPC-EMS framework has
simultaneously modeled losses and security constraints while coordinating
flexible loads within a unified architecture. The developed framework enables
secure operation with effective peak shaving and reduced total cost. The
effectiveness of the proposed method is validated on 10-bus, 18-bus, and 33-bus
systems.

</details>


### [394] [On Architectures for Combining Reinforcement Learning and Model Predictive Control with Runtime Improvements](https://arxiv.org/abs/2510.03354)
*Xiaolong Jia,Nikhil Bajaj*

Main category: eess.SY

TL;DR: 本研究提出两种结合神经网络模型预测控制（NNMPC）和强化学习（RL）的架构，以解决MPC的计算挑战和模型不准确性问题。


<details>
  <summary>Details</summary>
Motivation: 传统的模型预测控制（MPC）面临计算量大和模型不准确导致的性能下降问题。

Method: 提出两种结合神经网络近似MPC（NNMPC）和强化学习（RL）的架构：1. 暖启动RL：使用预训练的NNMPC权重初始化RL actor。2. RLMPC：使用RL生成NNMPC输出的修正残差。并引入了一种降维方法来减小NNMPC的输入维度。

Result: 在旋转倒立摆上进行评估，两种架构都实现了超过99%的运行时长缩减，并提高了对模型不确定性的跟踪性能。RL+MPC架构实现了11-40%的成本降低。

Conclusion: 结合NNMPC和RL的架构在计算效率和性能上均优于传统MPC，尤其在模型不确定性下表现更好。

Abstract: Model Predictive Control (MPC) faces computational demands and performance
degradation from model inaccuracies. We propose two architectures combining
Neural Network-approximated MPC (NNMPC) with Reinforcement Learning (RL). The
first, Warm Start RL, initializes the RL actor with pre-trained NNMPC weights.
The second, RLMPC, uses RL to generate corrective residuals for NNMPC outputs.
We introduce a downsampling method reducing NNMPC input dimensions while
maintaining performance. Evaluated on a rotary inverted pendulum, both
architectures demonstrate runtime reductions exceeding 99% compared to
traditional MPC while improving tracking performance under model uncertainties,
with RL+MPC achieving 11-40% cost reduction depending on reference amplitude.

</details>


### [395] [Viability-Preserving Passive Torque Control](https://arxiv.org/abs/2510.03367)
*Zizhe Zhang,Yicong Wang,Zhiquan Zhang,Tianyu Li,Nadia Figueroa*

Main category: eess.SY

TL;DR: 本研究使用可行性理论为机器人设计了预约束的安全控制方法，以避免碰撞和超出关节限制，并在仿真和实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于被动性的力矩控制器缺乏约束，容易在外部干扰下引发安全问题。本研究旨在通过引入可行性理论来解决这一问题。

Method: 本研究利用可行性理论，通过数据驱动和解析方法预先计算了关节位置和速度状态空间中的安全集（可行集）。这些可行集考虑了自碰撞、外部物体碰撞以及关节位置和速度限制，并通过机器人动力学为关节加速度和力矩提供了约束。在此基础上，研究人员设计了一个基于二次规划的控制框架，用于约束一个跟踪动态系统的被动控制器，以确保机器人在无限时间范围内保持在安全集中。

Result: 通过在 7-自由度 Franka Emika 机械臂上的仿真和硬件实验验证，所提出的方法相较于基线的约束被动控制器，能够在更高的控制循环速率下运行，并产生更平滑的轨迹。

Conclusion: 本研究提出的基于可行性理论的安全控制方法能够有效地在机器人动力学约束下，为机器人提供无碰撞且满足关节限制的安全运行保障，并在实际应用中展现出优于基线方法的性能。

Abstract: Conventional passivity-based torque controllers for manipulators are
typically unconstrained, which can lead to safety violations under external
perturbations. In this paper, we employ viability theory to pre-compute safe
sets in the state-space of joint positions and velocities. These viable sets,
constructed via data-driven and analytical methods for self-collision
avoidance, external object collision avoidance and joint-position and
joint-velocity limits, provide constraints on joint accelerations and thus
joint torques via the robot dynamics. A quadratic programming-based control
framework enforces these constraints on a passive controller tracking a
dynamical system, ensuring the robot states remain within the safe set in an
infinite time horizon. We validate the proposed approach through simulations
and hardware experiments on a 7-DoF Franka Emika manipulator. In comparison to
a baseline constrained passive controller, our method operates at higher
control-loop rates and yields smoother trajectories.

</details>


### [396] [Machine Learning-Driven Prediction of Lithium-Ion Battery Power Capability for eVTOL Aircraft](https://arxiv.org/abs/2510.03497)
*Hao Tu,Yebin Wang,Shaoshuai Mou,Huazhen Fang*

Main category: eess.SY

TL;DR: eVTOL飞机电池管理中的一个主要技术挑战是预测锂离子电池组的功率能力，尤其是在需要长预测范围和考虑紧急着陆的条件下。本文提出了一种结合物理学和机器学习的动态模型来准确预测电池的电压和温度行为，并利用机器学习加速搜索最大功率的过程。


<details>
  <summary>Details</summary>
Motivation: 电动垂直起降（eVTOL）飞机有望革新城市交通，但其电池管理面临功率能力预测的技术挑战，尤其是在高倍率放电和需要长预测范围及考虑紧急着陆的条件下。

Method: 1. 提出一个动态模型，结合物理学和机器学习，用于高精度预测锂离子电池的电压和温度。 2. 在搜索最大功率时，利用机器学习预测剩余放电时间，以加速计算。

Result: 验证结果表明，该方法对 eVTOL 运行是有效的。

Conclusion: 本文提出的结合物理学和机器学习的方法能够有效预测 eVTOL 运行中锂离子电池的功率能力。

Abstract: Electric vertical take-off and landing (eVTOL) aircraft have emerged as a
promising solution to transform urban transportation. They present a few
technical challenges for battery management, a prominent one of which is the
prediction of the power capability of their lithium-ion battery systems. The
challenge originates from the high C-rate discharging conditions required
during eVTOL flights as well as the complexity of lithium-ion batteries'
electro-thermal dynamics. This paper, for the first time, formulates a power
limit prediction problem for eVTOL which explicitly considers long prediction
horizons and the possible occurrence of emergency landings. We then harness
machine learning to solve this problem in two intertwined ways. First, we adopt
a dynamic model that integrates physics with machine learning to predict a
lithium-ion battery's voltage and temperature behaviors with high accuracy.
Second, while performing search for the maximum power, we leverage machine
learning to predict the remaining discharge time and use the prediction to
accelerate the search with fast computation. Our validation results show the
effectiveness of the proposed study for eVTOL operations.

</details>


### [397] [Learning Safety-Compatible Observers for Unknown Systems](https://arxiv.org/abs/2510.03609)
*Juho Bae,Daegyeong Roh,Han-Lim Choi*

Main category: eess.SY

TL;DR: 本文提出了一种数据驱动的方法，用于联合学习具有未知动态的系统的鲁棒全状态观测器及其鲁棒性证书。


<details>
  <summary>Details</summary>
Motivation: 为具有未知动态的系统提供一个安全兼容的观测器，该观测器可以被基于证书的安全控制器所消耗，从而在输出反馈下保持控制器的证书有效性。

Method: 利用增量输入状态稳定性（delta ISS）的概念，联合学习一个 delta ISS Lyapunov 函数作为鲁棒性证书，并证明了在学习模型的标准保真度假设下，估计误差的实际收敛性。将该方法扩展到互联系统，并提出了一种分布式观测器设计框架。

Result: 该方法在各种非线性系统上得到了验证。

Conclusion: 该方法能够联合学习鲁棒观测器及其鲁棒性证书，并能有效处理互联系统，为安全控制提供了理论基础。

Abstract: This paper presents a data-driven approach for jointly learning a robust
full-state observer and its robustness certificate for systems with unknown
dynamics. Leveraging incremental input-to-state stability (delta ISS) notions,
we jointly learn a delta ISS Lyapunov function that serves as the robustness
certificate and prove practical convergence of the estimation error under
standard fidelity assumptions on the learned models. This renders the observer
safety-compatible: they can be consumed by certificate-based safe controllers
so that, when the controller tolerates bounded estimation error, the
controller's certificate remains valid under output feedback. We further extend
the approach to interconnected systems via the small-gain theorem, yielding a
distributed observer design framework. We validate the approach on a variety of
nonlinear systems.

</details>


### [398] [Cyber Resilience of Three-phase Unbalanced Distribution System Restoration under Sparse Adversarial Attack on Load Forecasting](https://arxiv.org/abs/2510.03635)
*Chen Chao,Zixiao Ma,Ziang Zhang*

Main category: eess.SY

TL;DR: AI在电力系统恢复中引入网络安全风险，本研究量化了篡改预测对恢复可行性和电网安全的影响，并提出了一种更有效、更隐蔽的攻击方法。


<details>
  <summary>Details</summary>
Motivation: 电力系统恢复严重依赖AI负荷预测，但其网络安全风险和对恢复过程的影响尚未得到充分研究。

Method: 提出了一种基于梯度的稀疏对抗攻击方法，通过扰乱关键时空输入来暴露预测模型的漏洞，并设计了一个考虑恢复的验证框架，使用不平衡三相最优潮流来评估操作可行性。

Result: 仿真结果表明，所提出的攻击方法比基线攻击更有效、更隐蔽，能够暴露导致关键负荷无法恢复的系统级故障（如电压和功率爬坡违规）。

Conclusion: 研究结果为设计网络安全感知的恢复规划框架提供了可行的见解。

Abstract: System restoration is critical for power system resilience, nonetheless, its
growing reliance on artificial intelligence (AI)-based load forecasting
introduces significant cybersecurity risks. Inaccurate forecasts can lead to
infeasible planning, voltage and frequency violations, and unsuccessful
recovery of de-energized segments, yet the resilience of restoration processes
to such attacks remains largely unexplored. This paper addresses this gap by
quantifying how adversarially manipulated forecasts impact restoration
feasibility and grid security. We develop a gradient-based sparse adversarial
attack that strategically perturbs the most influential spatiotemporal inputs,
exposing vulnerabilities in forecasting models while maintaining stealth. We
further create a restoration-aware validation framework that embeds these
compromised forecasts into a sequential restoration model and evaluates
operational feasibility using an unbalanced three-phase optimal power flow
formulation. Simulation results show that the proposed approach is more
efficient and stealthier than baseline attacks. It reveals system-level
failures, such as voltage and power ramping violations that prevent the
restoration of critical loads. These findings provide actionable insights for
designing cybersecurity-aware restoration planning frameworks.

</details>


### [399] [Optimal Energy Management in Indoor Farming Using Lighting Flexibility and Intelligent Model Predictive Control](https://arxiv.org/abs/2510.03686)
*Mohammadjavad Abbaspour,Mukund R. Shukla,Praveen K. Saxena,Shivam Saxena*

Main category: eess.SY

TL;DR: 通过优化室内农场照明计划来降低能源成本。


<details>
  <summary>Details</summary>
Motivation: 室内农业虽然能够实现全年食品生产，但对人工照明的依赖导致能源消耗、高峰负荷收费和种植者能源成本显著增加。

Method: 提出一种最优照明控制策略，通过调节光照强度和光周期来降低能源成本。该策略在模型预测控制框架内实现，并辅以基于Transformer的神经网络，以预测未来24小时的太阳辐射和电价，从而提高节能效果。该策略还结合了在莴苣作物上进行的真实世界实验，以确定最低光照量和适当的明暗时间间隔，并将这些因素作为数学约束来维持植物健康。

Result: 模拟结果显示，在安大略省真实的电力市场数据的基础上，对一公顷的温室进行模拟，与基准计划相比，年成本减少了318,400美元（20.9%），高峰负荷降低了1.6兆瓦（33.32%），总能源节约了1890兆瓦时（20.2%）。

Conclusion: 智能照明控制有潜力提高室内农业的可持续性和经济可行性。

Abstract: Indoor farming enables year-round food production but its reliance on
artificial lighting significantly increases energy consumption, peak load
charges, and energy costs for growers. Recent studies indicate that plants are
able to tolerate interruptions in light, enabling the design of 24-hour
lighting schedules (or "recipes") with strategic light modulation in alignment
with day-ahead pricing. Thus, we propose an optimal lighting control strategy
for indoor farming that modulates light intensity and photoperiod to reduce
energy costs. The control strategy is implemented within a model predictive
control framework and augmented with transformer-based neural networks to
forecast 24-hour ahead solar radiation and electricity prices to improve energy
cost reduction. The control strategy is informed by real-world experimentation
on lettuce crops to discover minimum light exposure and appropriate dark-light
intervals, which are mathematically formulated as constraints to maintain plant
health. Simulations for a one-hectare greenhouse, based on real electricity
market data from Ontario, demonstrate an annual cost reduction of $318,400
(20.9%), a peak load decrease of 1.6 MW (33.32%), and total energy savings of
1890 MWh (20.2%) against a baseline recipe. These findings highlight the
potential of intelligent lighting control to improve the sustainability and
economic feasibility of indoor farming.

</details>


### [400] [On the Duality Between Quantized Time and States in Dynamic Simulation](https://arxiv.org/abs/2510.03785)
*Liya Huang,Georgios Tzounas*

Main category: eess.SY

TL;DR: 提出了一种离散时间与量化状态数值方法的对偶性，将QSS方法解释为作用于对偶系统模型的积分方案，并引入了受经典时间积分启发的QSS Adams-Bashforth方法，在实际电力系统仿真中实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 在离散时间与量化状态数值方法之间建立形式对偶性，以开发新的QSS方法并提高性能。

Method: 将QSS方法解释为作用于对偶系统模型的积分方案，其中时间是状态相关的变量。提出了一种QSS Adams-Bashforth方法，并将其应用于测试方程和电力系统仿真。

Result: 证明了QSS方法可以作为作用于对偶系统模型的积分方案，并提出了一种新的QSS Adams-Bashforth方法。该方法在电力系统仿真中实现了显著的性能改进。

Conclusion: 离散时间与量化状态数值方法之间存在对偶性，QSS方法可以被视为作用于对偶模型，并且可以开发新的QSS方法来提高仿真性能。

Abstract: This letter introduces a formal duality between discrete-time and
quantized-state numerical methods. We interpret quantized state system (QSS)
methods as integration schemes applied to a dual form of the system model,
where time is seen as a state-dependent variable. This perspective enables the
definition of novel QSS-based schemes inspired by classical time-integration
techniques. As a proof of concept, we illustrate the idea by introducing a QSS
Adams-Bashforth method applied to a test equation. We then move to demonstrate
how the proposed approach can achieve notable performance improvements in
realistic power system simulations.

</details>


### [401] [A Trustworthy Industrial Fault Diagnosis Architecture Integrating Probabilistic Models and Large Language Models](https://arxiv.org/abs/2510.03815)
*Yue wu*

Main category: eess.SY

TL;DR: 该研究提出了一种结合贝叶斯网络和大型语言模型（LLM）的多模态工业故障诊断框架（HCAA），解决了传统方法和深度学习方法在可解释性、泛化性和不确定性量化方面的局限性，提高了诊断的可信度。


<details>
  <summary>Details</summary>
Motivation: 传统工业故障诊断方法在可解释性、泛化性和不确定性量化方面存在局限，导致核心问题是诊断可信度不足。

Method: 该架构首先通过基于贝叶斯网络的诊断引擎进行初步分析，然后利用一个具有多模态输入能力的大型语言模型（LLM）驱动的认知仲裁模块，通过分析结构化特征和诊断图表进行专家级仲裁，并在识别冲突后优先做出最终决定。此外，还集成了基于温度校准的置信度校准模块和风险评估模块，使用期望校准误差（ECE）等指标量化系统可靠性。

Result: 在包含多种故障类型的多模态数据集上的实验结果表明，与基线模型相比，该框架将诊断准确率提高了28个百分点以上，同时将校准后的ECE降低了75%以上。

Conclusion: HCAA框架通过案例研究证实，能够有效纠正传统模型因复杂特征模式或知识空白造成的误判，为构建高可信度、可解释的工业应用人工智能诊断系统提供了新颖实用的工程解决方案。

Abstract: There are limitations of traditional methods and deep learning methods in
terms of interpretability, generalization, and quantification of uncertainty in
industrial fault diagnosis, and there are core problems of insufficient
credibility in industrial fault diagnosis. The architecture performs
preliminary analysis through a Bayesian network-based diagnostic engine and
features an LLM-driven cognitive quorum module with multimodal input
capabilities. The module conducts expert-level arbitration of initial diagnoses
by analyzing structured features and diagnostic charts, prioritizing final
decisions after conflicts are identified. To ensure the reliability of the
system output, the architecture integrates a confidence calibration module
based on temperature calibration and a risk assessment module, which
objectively quantifies the reliability of the system using metrics such as
expected calibration error (ECE). Experimental results on a dataset containing
multiple fault types showed that the proposed framework improved diagnostic
accuracy by more than 28 percentage points compared to the baseline model,
while the calibrated ECE was reduced by more than 75%. Case studies have
confirmed that HCAA effectively corrects misjudgments caused by complex feature
patterns or knowledge gaps in traditional models, providing novel and practical
engineering solutions for building high-trust, explainable AI diagnostic
systems for industrial applications.

</details>


### [402] [Enhancing Data Center Low-Voltage Ride-Through](https://arxiv.org/abs/2510.03867)
*Yiheng Xie,Wenqi Cui,Adam Wierman*

Main category: eess.SY

TL;DR: 数据中心为提高其低电压穿越能力，提出并验证了内部电网的电压控制器。


<details>
  <summary>Details</summary>
Motivation: 由于数据中心负载对电压波动敏感，电压骤降时会出现意外跳闸，加剧电网不稳定性。因此，需要提高数据中心的低电压穿越能力。

Method: 提出并设计了数据中心内部电网的电压控制器，以提高其低电压穿越能力。分析了低电压穿越标准和数据中心的可用资源，并设计了集中式和分布式控制器来统一管理各种柔性资源。此外，还构建了一个集成测试系统来模拟电网故障和数据中心电网的瞬态响应。

Result: 所提出的电压控制机制能够有效且简单地提高数据中心的低电压穿越能力。

Conclusion: 所提出的电压控制机制为提高数据中心的低电压穿越能力提供了一种有效且简单的方法。

Abstract: Data center loads have expanded significantly in recent years. Compared to
traditional loads, data centers are highly sensitive to voltage deviations and
thus their protection mechanisms trip more proactively during voltage
fluctuations. During a grid fault, simultaneous tripping of large-scale data
centers can further destabilize the transmission system and even lead to
cascading failures. In response, transmission system operators are imposing
voltage ride-through (VRT) requirements for data centers. In this work, we
enhance the VRT capability of data centers by designing voltage controllers for
their internal power distribution network. We first systematically analyze VRT
standards and the controllable resources related to data centers. These
resources enable the design of voltage control strategies to regulate voltages
internal to the data center, thereby allowing loads to remain online during
voltage disturbances from the external transmission grid. We study and contrast
both centralized and decentralized controllers that unify the control of
heterogeneous flexible resources. Additionally, we construct an integrated test
system that simulates both the transient fault response of the transmission
system and the data center distribution network. Case studies demonstrate that
the proposed voltage control mechanisms provide effective yet simple solutions
to enhance data center low-voltage ride-through capability.

</details>


### [403] [Electrical System Architecture for Aviation Electrification](https://arxiv.org/abs/2510.03887)
*Anoy Saha,Mona Ghassemi*

Main category: eess.SY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The electrification of aircraft is reshaping the foundations of aerospace
design by positioning electrical systems at the center of propulsion, control,
and onboard functionality. This chapter provides an overview of electrical
system architectures for electric and hybrid electric aircraft, highlighting
both established principles and emerging design strategies. The discussion
begins with the motivations for electrification, including reducing
environmental impact, improving operational efficiency, and replacing complex
pneumatic and hydraulic subsystems with lighter and more reliable electrical
alternatives. Aircraft electrical architectures are classified into four major
categories: conventional, more electric, all electric, and hybrid electric. A
range of system topologies is examined, including direct current (DC),
alternating current (AC), hybrid, and distributed configurations. Each is
considered in terms of its effectiveness in delivering power, enabling
redundancy, supporting fault isolation, and managing thermal performance. Real
world examples are presented to demonstrate practical applications, with case
studies drawn from the Boeing 787 Dreamliner, the Eviation Alice commuter
aircraft, and NASA X57 Maxwell demonstrator. These examples illustrate the
ongoing transition from incremental subsystem electrification toward fully
integrated architectures that promise higher efficiency and greater
sustainability.

</details>


### [404] [3D Electronic-Photonic Heterogenous Interconnect Platforms Enabling Energy-Efficient Scalable Architectures For Future HPC Systems](https://arxiv.org/abs/2510.03943)
*Anirban Samanta,Shun-Hung Lee,Chun-Yi Cheng,Samuel Palermo,S. J. Ben Yoo*

Main category: eess.SY

TL;DR: 3D EPIC平台通过使用TSOV将高速数据通信接口转移到光域，解决了3D芯片堆叠中的带宽和能效问题，并在基准测试中超过了3D电子互连。


<details>
  <summary>Details</summary>
Motivation: 解决高性能计算中3D互连的带宽扩展和内存墙问题，克服了现有铜基电互连的信号质量下降和能效低下等基本限制。

Method: 提出了一种3D芯片堆叠电子-光互连（EPIC）平台，该平台利用硅光子通道（TSOV）将高速数据通信接口转移到光域，同时保留了用于电源传输和短距离通信的电气TSV和2.5D互连功能。

Result: 与最先进的3D电子互连相比，3D EPIC平台实现了超过10 TB/s/mm²的带宽密度，并展示了实现高达100 fJ/bit的通信潜力。

Conclusion: 提出的3D EPIC平台为3D芯片堆叠提供了一种有前景的解决方案，有望实现更高带宽密度和更高能效的通信。

Abstract: 3D interconnects have emerged as a solution to address the scaling issues of
interconnect bandwidth and the memory wall problem in high-performance
computing (HPC), such as High-Bandwidth Memory (HBM). However, the copper-based
electrical interconnect retains fundamental limitations. Dense I/O for
high-speed signals lead to degraded signal quality for end-to-end links,
necessitating additional circuits to mitigate signal impairments and resulting
in poor energy efficiency. We propose a 3D chiplet stacking electronic-photonic
interconnect (EPIC) platform, which offers a solution by moving the high-speed
data communication interface to the optical domain across the 3D stack by using
Through Silicon Optical Vias (TSOV), while retaining the functionality of
electrical TSVs and 2.5D interconnects for power delivery and short-reach
low-latency communications. We then benchmark the proposed model against
state-of-the-art 3D electrical interconnects to demonstrate our 3D EPIC
platform beating the 3D electrical interconnects to $>$10 TB/s/$mm^2$ bandwidth
density. We present a pathway to extend our demonstrated, industry-ready design
to achieving $\leq$100 fJ/bit high-speed communication.

</details>


### [405] [Use of Quadcopter Wakes to Supplement Strawberry Pollination](https://arxiv.org/abs/2510.03974)
*Sadie Cutler,Ben DeFay,Scott McArt,Kirstin Petersen*

Main category: eess.SY

TL;DR: 利用四旋翼飞行器辅助授粉，但结果不确定。


<details>
  <summary>Details</summary>
Motivation: 由于野生和管理的授粉媒介数量正在减少，草莓等作物面临授粉不足的风险，因此需要开发可负担且易于实施的补充授粉解决方案。

Method: 通过四旋翼飞行器产生的风力进行人工授粉，并进行了实地实验。

Result: 实地实验结果不确定，但实验室研究表明该方法有前景。

Conclusion: 基于风力授粉的四旋翼飞行器辅助授粉方法有潜力，但需要进一步研究以优化实地表现。

Abstract: Pollinators are critical to the world's ecosystems and food supply, yet
recent studies have found pollination shortfalls in several crops, including
strawberry. This is troubling because wild and managed pollinators are
currently experiencing declines. One possibility is to try and provide
supplemental pollination solutions. These solutions should be affordable and
simple for farmers to implement if their use is to be widespread; quadcopters
are a great example, already used for monitoring on many farms. This paper
investigates a new method for artificial pollination based on wind pollination
that bears further investigation. After determining the height where the
lateral flow is maximized, we performed field experiments with a quadcopter
assisting natural pollinators. Although our results in the field were
inconclusive, lab studies show that the idea shows promise and could be adapted
for better field results.

</details>


### [406] [Data-driven Practical Stabilization of Nonlinear Systems via Chain Policies: Sample Complexity and Incremental Learning](https://arxiv.org/abs/2510.03982)
*Roy Siegelmann,Enrique Mallada*

Main category: eess.SY

TL;DR: 这是一个基于非参数链策略（NCP）的数据驱动的非线性系统稳定化方法，具有可证明的保证。


<details>
  <summary>Details</summary>
Motivation: 该方法旨在为非线性系统提供数据驱动的实用稳定化解决方案，仅假设系统是局部 Lipschitz 连续的，克服了以往方法需要系统为线性、多项式或分数多项式模型的局限性。

Method: 提出了一种基于非参数链策略（NCP）的方法，使用归一化的最近邻规则，在每个状态下分配一个有限持续时间的控制信号，并通过递归李雅普诺夫函数（RLFs）和递归控制李雅普诺夫函数（R-CLFs）进行稳定性分析和认证。

Result: 推导了 O((3/rho)^d log(R/c)) 的样本复杂度保证，并证明了 NCP 可以将系统稳定在一个平衡点的任意小的 c-邻域内。

Conclusion: 所提出的 NCP 方法是无参数的，可以轻松纳入新的验证数据以改进收敛速度或扩大认证区域，并通过数值实验进行了验证。

Abstract: We propose a method for data-driven practical stabilization of nonlinear
systems with provable guarantees, based on the concept of Nonparametric Chain
Policies (NCPs). The approach employs a normalized nearest-neighbor rule to
assign, at each state, a finite-duration control signal derived from stored
data, after which the process repeats. Unlike recent works that model the
system as linear, polynomial, or polynomial fraction, we only assume the system
to be locally Lipschitz. Our analysis builds on the framework of Recurrent
Lyapunov Functions (RLFs), which enable data-driven certification of practical
stability using standard norm functions instead of requiring the explicit
construction of a classical Lyapunov function. To extend this framework, we
introduce the concept of Recurrent Control Lyapunov Functions (R-CLFs), which
can certify the existence of an NCP that practically stabilizes an arbitrarily
small c-neighborhood of an equilibrium point. We also provide an explicit
sample complexity guarantee of O((3/rho)^d log(R/c)) number of trajectories,
where R is the domain radius, d the state dimension, and rho a system-dependent
constant. The proposed Chain Policies are nonparametric, thus allowing new
verified data to be readily incorporated into the policy to either improve
convergence rate or enlarge the certified region. Numerical experiments
illustrate and validate these properties.

</details>


### [407] [Distributed MPC-based Coordination of Traffic Perimeter and Signal Control: A Lexicographic Optimization Approach](https://arxiv.org/abs/2510.04038)
*Viet Hoang Pham,Hyo-Sung Ahn*

Main category: eess.SY

TL;DR: 本研究提出一种结合交通边界控制和交通信号控制的综合策略，以缓解城市交通网络（UTN）的拥堵。


<details>
  <summary>Details</summary>
Motivation: 为了缓解城市交通网络（UTN）的拥堵，需要一种综合策略来整合交通边界控制和交通信号控制。

Method: 该策略被制定为一个字典序多目标优化问题。首先，通过调节边界节点处的交通流入来最大化容量并确保UTN的平稳运行。然后，在调节后的流入下，协同优化内部节点的信号配时，以改善整体交通状况。采用模型预测控制（MPC）方法确保控制方案遵守网络内的安全和容量约束。为了处理该问题的计算复杂性，将UTN划分为子网络，每个子网络由一个本地代理管理。采用基于交替方向乘子法（ADMM）算法的分布式求解方法，使每个代理能够利用其子网络和相邻代理的本地信息来确定其最优控制决策。

Result: 通过VISSIM和MATLAB进行的数值模拟证明了所提出的交通控制策略的有效性。

Conclusion: 本研究提出的结合交通边界控制和交通信号控制的综合策略，通过字典序多目标优化、MPC和基于ADMM的分布式方法，能够有效缓解城市交通网络的拥堵。

Abstract: This paper introduces a comprehensive strategy that integrates traffic
perimeter control with traffic signal control to alleviate congestion in an
urban traffic network (UTN). The strategy is formulated as a lexicographic
multi-objective optimization problem, starting with the regulation of traffic
inflows at boundary junctions to maximize the capacity while ensuring a smooth
operation of the UTN. Following this, the signal timings at internal junctions
are collaboratively optimized to enhance overall traffic conditions under the
regulated inflows. The use of a model predictive control (MPC) approach ensures
that the control solution adheres to safety and capacity constraints within the
network. To address the computational complexity of the problem, the UTN is
divided into subnetworks, each managed by a local agent. A distributed solution
method based on the alternating direction method of multipliers (ADMM)
algorithm is employed, allowing each agent to determine its optimal control
decisions using local information from its subnetwork and neighboring agents.
Numerical simulations using VISSIM and MATLAB demonstrate the effectiveness of
the proposed traffic control strategy.

</details>


### [408] [A Conformal Prediction-Based Chance-Constrained Programming Approach for 24/7 Carbon-Free Data Center Operation Scheduling](https://arxiv.org/abs/2510.04053)
*Yijie Yang,Jian Shi,Dan Wang,Chenye Wu,Zhu Han*

Main category: eess.SY

TL;DR: AI数据中心的能源需求和碳排放不断增长，需要实现24/7碳中和能源（CFE）。本文提出了一种利用多变量共形预测（CP）来构建自适应不确定性集的方法，以解决可再生能源预测的固有变异性和误差问题，从而为数据中心运营调度提供更优的鲁棒决策。


<details>
  <summary>Details</summary>
Motivation: 数据中心快速增长的能源需求和碳排放，以及实现24/7碳中和能源（CFE）的必要性，传统的能源匹配方法在应对可再生能源的变异性和预测误差方面存在挑战。

Method: 提出一种利用多变量共形预测（CP）技术来构建不确定性集，并将其应用于机会约束规划（CCP）问题，以实现数据中心24/7 CFE运营调度。

Result: 该方法能够构建统计上有效的自适应不确定性集，并减少了高达6.65%的成本和6.96%的碳基能源使用量，优于传统的独立于协变量的方法。

Conclusion: 本文提出的协变量感知方法能够有效解决数据中心在24/7 CFE运营调度中面临的可再生能源不确定性问题，通过CP和CCP的结合，实现了更低的成本和碳排放，有助于数据中心实现24/7 CFE目标。

Abstract: The rapid growth of AI applications is dramatically increasing data center
energy demand, exacerbating carbon emissions, and necessitating a shift towards
24/7 carbon-free energy (CFE). Unlike traditional annual energy matching, 24/7
CFE requires matching real-time electricity consumption with clean energy
generation every hour, presenting significant challenges due to the inherent
variability and forecasting errors of renewable energy sources. Traditional
robust and data-driven optimization methods often fail to leverage the features
of the prediction model (also known as contextual or covariate information)
when constructing the uncertainty set, leading to overly conservative
operational decisions. This paper proposes a comprehensive approach for 24/7
CFE data center operation scheduling, focusing on robust decision-making under
renewable generation uncertainty. This framework leverages covariate
information through a multi-variable conformal prediction (CP) technique to
construct statistically valid and adaptive uncertainty sets for renewable
forecasts. The uncertainty sets directly inform the chance-constrained
programming (CCP) problem, ensuring that chance constraints are met with a
specified probability. We further establish theoretical underpinnings
connecting the CP-generated uncertainty sets to the statistical feasibility
guarantees of the CCP. Numerical results highlight the benefits of this
covariate-aware approach, demonstrating up to 6.65% cost reduction and 6.96%
decrease in carbon-based energy usage compared to conventional
covariate-independent methods, thereby enabling data centers to progress toward
24/7 CEF.

</details>


### [409] [A Hybrid GNN-IZR Framework for Fast and Empirically Robust AC Power Flow Analysis in Radial Distribution Systems](https://arxiv.org/abs/2510.04264)
*Mohamed Shamseldein*

Main category: eess.SY

TL;DR: 该混合框架结合了图神经网络（GNN）和隐式Z总线递归（IZR）方法，解决了交替电流潮流（ACPF）问题，在保证速度的同时提高了可靠性。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动模型的速度和分析求解器的可靠性之间取得平衡，特别是在处理径向配电网络的ACPF问题时。

Method: 提出一个混合框架，使用物理信息GNN进行快速初始预测，并使用IZR求解器作为备用方案，在GNN可能失败的情况下（最大功率失配超过0.1 p.u.）进行调用。该框架还包括一个两阶段触发器来识别潜在的失败案例，并通过一项消融研究来评估物理信息训练和Z总线灵敏度特征的重要性。

Result: 在包含7,500个压力案例的IEEE 33总线系统测试集中，纯GNN模型在13.11%的情况下失败。而混合框架成功识别了所有潜在的失败案例，并将其委托给IZR求解器，实现了0.00%的失败率。消融研究表明，物理信息训练和Z总线灵敏度特征对于将GNN的失败率从98.72%（仅数据）降低到13.11%至关重要。

Conclusion: 该混合方法在利用GNN速度的同时，实现了分析求解器的经验可靠性，能够显著提高近实时可分析的场景数量，为ACPF问题提供了一个实用的解决方案。

Abstract: The Alternating Current Power Flow (ACPF) problem forces a trade-off between
the speed of data-driven models and the reliability of analytical solvers. This
paper introduces a hybrid framework that synergizes a Graph Neural Network
(GNN) with the Implicit Z-Bus Recursive (IZR) method, a robust, non-iterative
solver for radial distribution networks. The framework employs a
physics-informed GNN for rapid initial predictions and invokes the IZR solver
as a failsafe for stressed cases identified by a two-stage trigger. A failure
is defined as any solution with a maximum power mismatch exceeding 0.1 p.u., a
significant operational deviation. On a challenging test set of 7,500 stressed
scenarios for the IEEE 33-bus system, the GNN-only model failed on 13.11 % of
cases. In contrast, the hybrid framework identified all potential failures,
delegating them to the IZR solver to achieve a 0.00 % failure rate, empirically
matching the 100 % success rate of the analytical solver on this specific test
set. An expanded ablation study confirms that both physics-informed training
and Z-bus sensitivity features are critical, collaboratively reducing the GNN's
failure rate from 98.72 % (data-only) to 13.11 %. The hybrid approach
demonstrates a pragmatic path to achieving the empirical reliability of an
analytical solver while leveraging GNN speed, enabling a significant increase
in the number of scenarios analyzable in near real-time.

</details>


### [410] [A Diffusion-based Generative Machine Learning Paradigm for Contingency Screening](https://arxiv.org/abs/2510.04470)
*Quan Tran,Suresh S. Muknahallipatna,Dongliang Duan,Nga Nguyen*

Main category: eess.SY

TL;DR: 传统的电网故障分析方法计算成本高，本研究提出了一种主动式无监督学习方法，通过学习历史故障模式来预测和生成最坏的故障场景，并利用扰动扩散技术来识别高风险场景，从而提高故障分析的效率和预测能力。


<details>
  <summary>Details</summary>
Motivation: 传统的电网故障分析方法计算成本高，且需要生成大量可能的故障场景或调整网络参数来确定最坏情况，效率低下。

Method: 提出了一种主动式无监督学习方法，通过学习历史故障模式来生成潜在的风险场景，并利用扰动扩散技术来识别最坏的场景，而不是逐一模拟。

Result: 通过在IEEE系统上进行实证实验，验证了所提出方法的有效性。

Conclusion: 所提出的主动式无监督学习方法能够有效识别电网故障中的高风险场景，提高了故障分析的效率和预测能力。

Abstract: Contingency screening is a crucial part of electric power systems all the
time. Power systems frequently encounter multiple challenging operational
dilemmas that could lead to the instability of power systems. Contingency
analysis is effort-consuming by utilizing traditional numerical analysis
methods. It is commonly addressed by generating a whopping number of possible
contingencies or manipulating network parameters to determine the worst
scenarios. This paper proposes a novel approach that diverts the nature of
contingency analysis from pre-defined scenario screening to
proactive-unsupervised screening. The potentially risky scenarios of power
systems are generated from learning how the previous ones occurred. In other
words, the internal perturbation that initiates contingencies is learned prior
to being self-replicated for rendering the worst scenarios. By leveraging the
perturbation diffusion technique, a proposed model is built to point out the
worst scenarios instead of repeatedly simulating one-by-one scenarios to define
the highest-risk ones. Empirical experiments are implemented on the IEEE
systems to test and validate the proposed solution.

</details>


### [411] [On properties of hydraulic equilibria in district heating networks](https://arxiv.org/abs/2510.04524)
*Ask Hällström,Felix Agner,Richard Pates*

Main category: eess.SY

TL;DR: District heating networks are key in smart energy systems for flexibility and renewable integration. This paper analyzes flow rate properties in tree-structured networks, showing that increased consumer valve opening guarantees higher total flow and reduced flow for those not participating, with implications for control strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand and characterize the properties of flow rates in tree-structured district heating networks to facilitate future smart energy systems that require enhanced energy flexibility and integration of renewable/waste energy sources.

Method: The paper analyzes the properties of flow rates in tree-structured district heating networks under mild assumptions of monotonicity in hydraulic network components to derive statements regarding stationary flow rate distribution.

Result: The study shows that an increase in total flow rate throughput is guaranteed when all consumers incrementally open their valves. Conversely, if one consumer does not open their valve while others do, they will receive a reduced flow rate. These findings are demonstrated numerically on both small and large networks.

Conclusion: The properties of flow rates in tree-structured district heating networks, particularly concerning stationary flow rate distribution under varying consumer valve openings, have been analyzed. These properties are crucial for designing efficient control strategies for optimal heat distribution in future energy systems.

Abstract: District heating networks are an integral part of the energy system in many
countries. In future smart energy systems, they are expected to enhance energy
flexibility and support the integration of renewable and waste energy sources.
An important aspect of these networks is the control of flow rates, which
dictates the heat delivered to consumers. This paper concerns the properties of
flow rates in tree-structured district heating networks. We show that under
mild assumptions of monotonicity in the hydraulic network components,
statements regarding the stationary flow rate distribution can be made. In
particular, when all consumers in a network incrementally open their valves, an
increase in total flow rate throughput is guaranteed, while if one consumer
does not open their valve when others do, they will receive a reduced flow
rate. These properties are illustrated numerically on a small 2-consumer
network as well as on a larger 22-consumer network. Previous works have shown
that these properties allow the design and use of efficient control strategies
for optimal heat distribution.

</details>


### [412] [Data-Driven Adaptive PID Control Based on Physics-Informed Neural Networks](https://arxiv.org/abs/2510.04591)
*Junsei Ito,Yasuaki Wasa*

Main category: eess.SY

TL;DR: 本文提出一种基于物理信息神经网络（PINNs）的自适应PID控制器设计方法，通过自动微分优化PID增益，实现对非线性系统的稳定控制。


<details>
  <summary>Details</summary>
Motivation: 现有PID控制器在处理非线性系统时存在挑战，需要一种能够自适应调整增益并保证系统稳定性的方法。

Method: 利用PINNs进行预测建模，并通过自动微分获取PID增益优化梯度，结合成本函数进行模型预测控制，实现自适应增益调整。

Result: 数值实验表明，该方法在时间和频率域内均能有效控制非线性系统，证明了其有效性。

Conclusion: 所提出的PINNs-based PID控制器设计框架能够系统地将预测模型集成到闭环控制系统中，为PID控制设计提供了新的途径。

Abstract: This article proposes a data-driven PID controller design based on the
principle of adaptive gain optimization, leveraging Physics-Informed Neural
Networks (PINNs) generated for predictive modeling purposes. The proposed
control design method utilizes gradients of the PID gain optimization, achieved
through the automatic differentiation of PINNs, to apply model predictive
control using a cost function based on tracking error and control inputs. By
optimizing PINNs-based PID gains, the method achieves adaptive gain tuning that
ensures stability while accounting for system nonlinearities. The proposed
method features a systematic framework for integrating PINNs-based models of
dynamical control systems into closed-loop control systems, enabling direct
application to PID control design. A series of numerical experiments is
conducted to demonstrate the effectiveness of the proposed method from the
control perspectives based on both time and frequency domains.

</details>


### [413] [Design Process of a Self Adaptive Smart Serious Games Ecosystem](https://arxiv.org/abs/2510.04615)
*X. Tao,P. Chen,M. Tsami,F. Khayati,M. Eckert*

Main category: eess.SY

TL;DR: Blexer v3是一个基于严肃游戏的模块化、人工智能驱动的康复生态系统，旨在通过多模态传感、实时推理和智能控制来提供个性化干预。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在概述Blexer v3的设计愿景和计划演进，这是一个基于严肃游戏的模块化、人工智能驱动的康复生态系统，并提出了一种整合多模态传感、实时推理和智能控制的新架构，以改进用户体验和康复效果。

Method: 提出了一种新的系统架构，包括数据收集、用户状态推断和游戏玩法适应等独立模块，并考虑了动态难度调整（DDA）和程序化内容生成（PCG）等关键功能，以支持个性化干预。论文还提出了Blexer v3的完整概念框架，定义了系统的模块化结构和数据流。

Result: 论文提出了Blexer v3的完整概念框架，为开发功能原型和将其集成到临床康复场景奠定了基础。

Conclusion: Blexer v3代表了严肃游戏在康复领域的一个重要进步，通过其模块化设计、人工智能驱动的功能和个性化干预能力，有望为患者提供更有效、更具吸引力的康复体验。

Abstract: This paper outlines the design vision and planned evolution of Blexer v3, a
modular and AI-driven rehabilitation ecosystem based on serious games. Building
on insights from previous versions of the system, we propose a new architecture
that aims to integrate multimodal sensing, real-time reasoning, and intelligent
control. The envisioned system will include distinct modules for data
collection, user state inference, and gameplay adaptation. Key features such as
dynamic difficulty adjustment (DDA) and procedural content generation (PCG) are
also considered to support personalized interventions. We present the complete
conceptual framework of Blexer v3, which defines the modular structure and data
flow of the system. This serves as the foundation for the next phase: the
development of a functional prototype and its integration into clinical
rehabilitation scenarios.

</details>


### [414] [On Prediction-Based Properties of Discrete-Event Systems: Notions, Applications and Supervisor Synthesis](https://arxiv.org/abs/2510.04616)
*Bohan Cui,Yu Chen,Alessandro Giua,Xiang Yin*

Main category: eess.SY

TL;DR: 研究如何为部分可观测的离散事件系统（DES）合成强制属性的监控器，重点关注依赖未来预测行为的属性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只考虑已执行的行为，而本研究解决了更具挑战性的问题，即属性依赖于尚未发生的预测未来行为，这在涉及未来信息的应用（如主动预测或意图保护）中很自然地出现。

Method: 提出“基于预测的属性”概念，将其形式化为与系统未来信息相关的观测属性。然后，提出一种合成监控器的方法，该方法利用新颖的信息结构来解决当前预测与控制策略之间的依赖性问题，将问题转化为信息空间中的安全博弈。

Result: 证明了所提出的算法是可靠和完整的，并且生成的监控器是最大允许的。

Conclusion: 所提出的基于预测的属性和合成方法为处理涉及未来信息的DES属性提供了有效且通用的解决方案。

Abstract: In this work, we investigate the problem of synthesizing property-enforcing
supervisors for partially-observed discrete-event systems (DES). Unlike most
existing approaches, where the enforced property depends solely on the executed
behavior of the system, here we consider a more challenging scenario in which
the property relies on predicted future behaviors that have not yet occurred.
This problem arises naturally in applications involving future information,
such as active prediction or intention protection. To formalize the problem, we
introduce the notion of prediction-based properties, a new class of
observational properties tied to the system's future information. We
demonstrate that this notion is very generic and can model various practical
properties, including predictability in fault prognosis and pre-opacity in
intention security. We then present an effective approach for synthesizing
supervisors that enforce prediction-based properties. Our method relies on a
novel information structure that addresses the fundamental challenge arising
from the dependency between current predictions and the control policy. The key
idea is to first borrow information from future instants and then ensure
information consistency. This reduces the supervisor synthesis problem to a
safety game in the information space. We prove that the proposed algorithm is
both sound and complete, and the resulting supervisor is maximally permissive.

</details>


### [415] [Learning a Shape-adaptive Assist-as-needed Rehabilitation Policy from Therapist-informed Input](https://arxiv.org/abs/2510.04666)
*Zhimin Hou,Jiacheng Hou,Xiao Chen,Hamid Sadeghian,Tianyu Ren,Sami Haddadin*

Main category: eess.SY

TL;DR: 该研究提出了一种新颖的远程医疗机器人框架，用于安全、自适应的辅助即时（AAN）机器人康复。


<details>
  <summary>Details</summary>
Motivation: 为了解决安全交互和适应性有限的问题，以促进治疗师干预机器人辅助康复的更广泛应用。

Method: 该框架将治疗师信息反馈的纠正力编码到潜在空间的 via-points 中，并使用形状自适应的 AAN 策略来根据患者的运动偏好和治疗师的反馈调整参考轨迹。

Result: 该方法在远程 AAN 康复方面具有实用性，并在减少纠正力、提高运动平稳性方面优于最先进的方法。

Conclusion: 所提出的框架能够安全、有效地实现远程 AAN 康复，并适应患者的个体需求。

Abstract: Therapist-in-the-loop robotic rehabilitation has shown great promise in
enhancing rehabilitation outcomes by integrating the strengths of therapists
and robotic systems. However, its broader adoption remains limited due to
insufficient safe interaction and limited adaptation capability. This article
proposes a novel telerobotics-mediated framework that enables therapists to
intuitively and safely deliver assist-as-needed~(AAN) therapy based on two
primary contributions. First, our framework encodes the therapist-informed
corrective force into via-points in a latent space, allowing the therapist to
provide only minimal assistance while encouraging patient maintaining own
motion preferences. Second, a shape-adaptive ANN rehabilitation policy is
learned to partially and progressively deform the reference trajectory for
movement therapy based on encoded patient motion preferences and
therapist-informed via-points. The effectiveness of the proposed shape-adaptive
AAN strategy was validated on a telerobotic rehabilitation system using two
representative tasks. The results demonstrate its practicality for remote AAN
therapy and its superiority over two state-of-the-art methods in reducing
corrective force and improving movement smoothness.

</details>


### [416] [MPC strategies for density profile control with pellet fueling in nuclear fusion tokamaks under uncertainty](https://arxiv.org/abs/2510.04784)
*Christopher A. Orrico,Hari Prasad Varadarajan,Matthijs van Berkel,Lennard Ceelen,Thomas O. S. J. Bosman,W. P. M. H. Heemels,Dinesh Krishnamoorthy*

Main category: eess.SY

TL;DR: 该研究提出了一种多阶段模型预测控制（msMPC）方法来解决ITER核聚变托卡马克装置中基于颗粒燃料的密度剖面控制问题，该问题具有多速率非线性系统、安全约束、输入延迟和参数不确定性等挑战。


<details>
  <summary>Details</summary>
Motivation: 解决ITER核聚变托卡马克装置中基于颗粒燃料的密度剖面控制问题，该问题是一个具有安全关键约束、输入延迟和参数不确定性的多速率非线性系统。

Method: 提出了一种多阶段模型预测控制（msMPC）方法，并采用三种技术来降低问题复杂度：1. 通过动态模式分解（DMD）减少预测模型规模；2. 应用主成分分析（PCA）减少msMPC中所需的场景数量；3. 利用惩罚项同伦（PTH-MPC）算法减少混合整数输入引起的计算负担。

Result: 通过与标称混合整数模型预测控制（MI-MPC）的比较，证明了msMPC策略在性能和安全性方面均有优势，实现了首个可用于ITER实时颗粒燃料的、具有不确定性处理能力的预测密度控制策略。

Conclusion: 所提出的msMPC方法通过模型降维、场景降维和计算优化，成功解决了ITER实时密度剖面控制中的不确定性和计算复杂度问题，为ITER的实际运行提供了可行方案。

Abstract: Control of the density profile based on pellet fueling for the ITER nuclear
fusion tokamak involves a multi-rate nonlinear system with safety-critical
constraints, input delays, and discrete actuators with parametric uncertainty.
To address this challenging problem, we propose a multi-stage MPC (msMPC)
approach to handle uncertainty in the presence of mixed-integer inputs. While
the scenario tree of msMPC accounts for uncertainty, it also adds complexity to
an already computationally intensive mixed-integer MPC (MI-MPC) problem. To
achieve real-time density profile controller with discrete pellets and
uncertainty handling, we systematically reduce the problem complexity by (1)
reducing the identified prediction model size through dynamic mode
decomposition with control, (2) applying principal component analysis to reduce
the number of scenarios needed to capture the parametric uncertainty in msMPC,
and (3) utilizing the penalty term homotopy for MPC (PTH-MPC) algorithm to
reduce the computational burden caused by the presence of mixed-integer inputs.
We compare the performance and safety of the msMPC strategy against a nominal
MI-MPC in plant simulations, demonstrating the first predictive density control
strategy with uncertainty handling, viable for real-time pellet fueling in
ITER.

</details>


### [417] [Efficient Probabilistic Planning with Maximum-Coverage Distributionally Robust Backward Reachable Trees](https://arxiv.org/abs/2510.04807)
*Alex Rose,Naman Aggarwal,Christopher Jewison,Jonathan P. How*

Main category: eess.SY

TL;DR: 该论文提出了一种用于线性高斯系统的多查询运动规划算法，旨在以高概率到达欧氏球。该算法通过构建分布鲁棒的信念图，合成鲁棒控制器，并能保证在最大尺寸球形不确定性集下的安全性。与现有算法相比，该算法在规划覆盖率方面表现更优，并在特定条件下可证明获得严格更优的覆盖率，在无过程噪声或状态约束的特殊情况下可证明达到最大覆盖率。此外，论文还提出了第二种多查询运动规划算法，用于到达由椭球与欧氏球闵可夫斯基和参数化的区域，该算法在规划覆盖率方面也可获得与现有最佳算法相当或更优的性能。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决线性高斯系统中多查询运动规划的问题，特别是如何以高概率到达目标区域（欧氏球或更一般的区域），并保证规划的鲁棒性和安全性。

Method: 论文提出了一种新的分布鲁棒的信念图构建算法，该算法利用新提出的球形不确定性集（高斯分布的模糊集）的公式来合成鲁棒控制器。对于第二种算法，它在椭球不确定性集上进行规划，该椭球由最大尺寸的球形不确定性集组成。

Result: 提出的算法在欧氏球目标规划方面，覆盖率优于现有算法，特定条件下更优，在无过程噪声或状态约束下达到最大覆盖率。在椭球+欧氏球目标规划方面，覆盖率与现有最佳算法相当或更优。

Conclusion: 该论文提出的两种多查询运动规划算法在分别针对欧氏球和椭球+欧氏球目标时，均能提供优于或媲美现有技术的性能，并具有鲁棒性和安全性保证。通过仿真实验验证了算法的有效性。

Abstract: This paper presents a new multi-query motion planning algorithm for linear
Gaussian systems with the goal of reaching a Euclidean ball with high
probability. We develop a new formulation for ball-shaped ambiguity sets of
Gaussian distributions and leverage it to develop a distributionally robust
belief roadmap construction algorithm. This algorithm synthe- sizes robust
controllers which are certified to be safe for maximal size ball-shaped
ambiguity sets of Gaussian distributions. Our algorithm achieves better
coverage than the maximal coverage algorithm for planning over Gaussian
distributions [1], and we identify mild conditions under which our algorithm
achieves strictly better coverage. For the special case of no process noise or
state constraints, we formally prove that our algorithm achieves maximal
coverage. In addition, we present a second multi-query motion planning
algorithm for linear Gaussian systems with the goal of reaching a region
parameterized by the Minkowski sum of an ellipsoid and a Euclidean ball with
high probability. This algorithm plans over ellipsoidal sets of maximal size
ball-shaped ambiguity sets of Gaussian distributions, and provably achieves
equal or better coverage than the best-known algorithm for planning over
ellipsoidal ambiguity sets of Gaussian distributions [2]. We demonstrate the
efficacy of both methods in a wide range of conditions via extensive simulation
experiments.

</details>


### [418] [Robust stability of event-triggered nonlinear moving horizon estimation](https://arxiv.org/abs/2510.04814)
*Isabelle Krauss,Victor G. Lopez,Matthias A. Müller*

Main category: eess.SY

TL;DR: 提出了一种用于一般非线性系统远程状态估计的事件触发移动视界估计（ET-MHE）方案。该方案通过事件触发传输单个测量值并求解非线性MHE优化问题，否则使用开环预测更新状态估计。


<details>
  <summary>Details</summary>
Motivation: 为一般非线性系统开发一种有效的远程状态估计方法。

Method: 提出了一种事件触发移动视界估计（ET-MHE）方案，包括一个新颖的事件触发规则，并采用可变视界长度。

Result: 证明了ET-MHE方案的鲁棒全局指数稳定性，并展示了可变视界长度可以实现更紧密的估计误差界限。通过两个算例验证了该方法的有效性。

Conclusion: ET-MHE方案能够有效地对一般非线性系统进行远程状态估计，并保证了稳定性。

Abstract: In this work, we propose an event-triggered moving horizon estimation
(ET-MHE) scheme for the remote state estimation of general nonlinear systems.
In the presented method, whenever an event is triggered, a single measurement
is transmitted and the nonlinear MHE optimization problem is subsequently
solved. If no event is triggered, the current state estimate is updated using
an open-loop prediction based on the system dynamics. Moreover, we introduce a
novel event-triggering rule under which we demonstrate robust global
exponential stability of the ET-MHE scheme, assuming a suitable detectability
condition is met. In addition, we show that with the adoption of a varying
horizon length, a tighter bound on the estimation error can be achieved.
Finally, we validate the effectiveness of the proposed method through two
illustrative examples.

</details>


### [419] [Power Reserve Capacity from Virtual Power Plants with Reliability and Cost Guarantees](https://arxiv.org/abs/2510.04815)
*Lorenzo Zapparoli,Blazhe Gjorgiev,Giovanni Sansavini*

Main category: eess.SY

TL;DR: VPPs可以通过考虑可靠性和成本来为电力储备服务提供支持，但其能力受到产品要求的限制。


<details>
  <summary>Details</summary>
Motivation: 评估VPP为电力储备市场提供服务的潜力，并解决现有方法在考虑AS产品要求和成本估算方面的不足。

Method: 提出一种新方法，通过子集模拟确定最大可行储备量，并考虑显式成本和机会成本来表征供给曲线，以评估VPP在预测不确定性下的储备能力。

Result: VPP能够可靠地提供储备产品，机会成本是定价的关键因素，并且产品要求对储备能力有显著影响。

Conclusion: VPP有潜力为电力储备市场提供服务，但其能力受到产品要求和成本因素的影响，该方法有助于VPP管理者和政策制定者。

Abstract: The growing penetration of renewable energy sources is expected to drive
higher demand for power reserve ancillary services (AS). One solution is to
increase the supply by integrating distributed energy resources (DERs) into the
AS market through virtual power plants (VPPs). Several methods have been
developed to assess the potential of VPPs to provide services. However, the
existing approaches fail to account for AS products' requirements (reliability
and technical specifications) and to provide accurate cost estimations. Here,
we propose a new method to assess VPPs' potential to deliver power reserve
capacity products under forecasting uncertainty. First, the maximum feasible
reserve quantity is determined using a novel formulation of subset simulation
for efficient uncertainty quantification. Second, the supply curve is
characterized by considering explicit and opportunity costs. The method is
applied to a VPP based on a representative Swiss low-voltage network with a
diversified DER portfolio. We find that VPPs can reliably offer reserve
products and that opportunity costs drive product pricing. Additionally, we
show that the product's requirements strongly impact the reserve capacity
provision capability. This approach aims to support VPP managers in developing
market strategies and policymakers in designing DER-focused AS products.

</details>


### [420] [An Active Fault-Tolerant Online Control Allocation Scheme for a Dual-System UAV in Transition Flight](https://arxiv.org/abs/2510.04853)
*Junfeng Cai,Marco Lovera*

Main category: eess.SY

TL;DR: 本文提出了一种新颖的主动容错控制（AFTC）方案，用于双系统垂直起降（VTOL）无人机（UAV）在过渡飞行期间。该方案结合了结构化 H∞ 基线控制律和在线控制重新分配模块，即使在执行器同时发生故障的情况下也能保证闭环系统的稳定性，并有效避免了控制抖振问题。通过更新控制分配矩阵来重新分配虚拟控制信号给健康的执行器，以处理执行器故障/失效，从而避免显著的性能下降。仿真结果表明，所提出的结构化 H∞ AFTC 系统能够处理更复杂的故障场景和模型不确定性，无需重新配置基线控制律，显著提高了双系统 VTOL UAV 过渡飞行的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 针对双系统垂直起降（VTOL）无人机（UAV）在过渡飞行期间可能出现的执行器故障/失效问题，提出一种能够保证系统稳定性、避免控制抖振并能处理复杂故障场景的主动容错控制（AFTC）方案，以提高飞行安全性和可靠性。

Method: 提出一种由结构化 H∞ 基线控制律和在线控制重新分配模块组成的主动容错控制（AFTC）方案。基线控制律用于保证在执行器故障下系统的稳定性。在线控制重新分配模块在执行器故障发生时，根据故障信息和实时空速更新控制分配矩阵，将虚拟控制信号重新分配给剩余的健康执行器。

Result: 在非线性六自由度模拟器上，对对称和非对称执行器故障场景进行了仿真。将仅采用结构化 H∞ 控制的场景与基于结构化 H∞ 的 AFTC 场景进行了对比分析。仿真结果表明，所提出的结构化 H∞ AFTC 系统在无需重新配置基线控制律的情况下，能够处理更复杂的故障场景和模型不确定性。

Conclusion: 所提出的基于结构化 H∞ 的主动容错控制（AFTC）方案能够有效地处理双系统 VTOL UAV 在过渡飞行中的执行器故障，无需重新配置控制律，提高了系统的鲁棒性和安全性，显著提升了飞行安全性和可靠性。

Abstract: A novel active fault-tolerant control (AFTC) scheme for a dual-system
vertical takeoff and landing (VTOL) unmanned aerial vehicle (UAV) during
transition flight is proposed in this paper. The AFTC scheme is composed of a
baseline control law and an online control reallocation module. First, the
structured $H_{\infty}$ baseline control law is able to guarantee the stability
of closed-loop systems without being reconfigured under simultaneous actuator
fault conditions. Second, compared to the existing mainstream method of sliding
mode control that is a discontinuous control strategy, the AFTC scheme can
effectively avoid control chattering problem by adopting the structured
$H_{\infty}$ baseline control law. Third, an online control allocation (CA)
module is implemented to carry out a unified CA for all the available
actuators. When actuator faults/failures occur, the CA matrix is updated
according to fault information and real-time airspeed, which is able to
redistribute the virtual control signals to the remaining healthy actuators,
avoiding significant performance degradation. Based on the developed AFTC
scheme, symmetric and non-symmetric actuator fault scenarios are simulated on a
nonlinear six-degree-of-freedom simulator, where the cases of merely structured
$H_{\infty}$ control and structured $H_{\infty}$ based AFTC are compared and
analyzed. The results show that the proposed structured $H_{\infty}$ based AFTC
system is capable of handling more complicated fault scenarios and model
uncertainties with no need to reconfigure the baseline control law. The
proposed AFTC scheme significantly improves the safety and reliability of the
transition flight of dual-system VTOL UAVs.

</details>


### [421] [Model Predictive Control-Guided Reinforcement Learning for Implicit Balancing](https://arxiv.org/abs/2510.04868)
*Seyed Soroush Karimi Madahi,Kenneth Bruninx,Bert Claessens,Chris Develder*

Main category: eess.SY

TL;DR: 该研究提出了一种结合了模型预测控制（MPC）和强化学习（RL）的混合方法，用于优化欧洲电力市场的平衡责任方套利策略。


<details>
  <summary>Details</summary>
Motivation: 欧洲电力市场中，平衡责任方可以通过实时调整以协助维持供需平衡，并从中获利。现有的MPC方法在捕捉价格形成机制和计算成本方面存在问题，而RL方法则需要大量数据训练且依赖历史数据。本研究旨在结合两者的优点，提出一种新的方法。

Method: 提出了一种MPC引导的RL方法，该方法结合了MPC的预测能力和RL的快速推理能力，并利用比利时2023年的平衡数据进行评估。

Result: 与单独的RL和MPC方法相比，MPC引导的RL方法在优化隐式平衡电池控制问题时，套利利润分别提高了16.15%和54.36%。

Conclusion: MPC引导的RL方法能够有效结合预测信息并保持快速推理能力，在欧洲不平衡市场中具有显著的经济效益。

Abstract: In Europe, profit-seeking balance responsible parties can deviate in real
time from their day-ahead nominations to assist transmission system operators
in maintaining the supply-demand balance. Model predictive control (MPC)
strategies to exploit these implicit balancing strategies capture arbitrage
opportunities, but fail to accurately capture the price-formation process in
the European imbalance markets and face high computational costs. Model-free
reinforcement learning (RL) methods are fast to execute, but require
data-intensive training and usually rely on real-time and historical data for
decision-making. This paper proposes an MPC-guided RL method that combines the
complementary strengths of both MPC and RL. The proposed method can effectively
incorporate forecasts into the decision-making process (as in MPC), while
maintaining the fast inference capability of RL. The performance of the
proposed method is evaluated on the implicit balancing battery control problem
using Belgian balancing data from 2023. First, we analyze the performance of
the standalone state-of-the-art RL and MPC methods from various angles, to
highlight their individual strengths and limitations. Next, we show an
arbitrage profit benefit of the proposed MPC-guided RL method of 16.15% and
54.36%, compared to standalone RL and MPC.

</details>


### [422] [Robust Cislunar Navigation via LFT-Based $\mathcal{H}_\infty$ Filtering with Bearing-Only Measurements](https://arxiv.org/abs/2510.04942)
*Raktim Bhattacharya*

Main category: eess.SY

TL;DR: 该研究提出了一种结合了CR3BP动力学和仅方位角光学测量数据的鲁棒性地缘空间导航估算框架，并通过LFT表示。


<details>
  <summary>Details</summary>
Motivation: 为地缘空间导航开发一种鲁棒的估算方法，该方法能内嵌CR3BP动力学并利用仅方位角的光学测量，避免局部线性化。

Method: 提出了一种基于线性分数变换（LFT）表示的完整阶次$\\mathcal{H}_\\infty$观测器，它直接在控制方程上运行，并将主要的非线性项表示为结构化实不确定性，利用依赖于距离的加权来表示测量保真度，并从视线几何中重建地月距离。

Result: 在近乎直线的晕轨道（NRHO）的仿真中，证明了该框架在地缘空间导航中的鲁棒性，估算误差有界，并且能够实现平滑的位置跟踪，尤其是在垂直方向的状态上观察到最大的偏差，这与垂直动力学的刚度和仅角度观测的局限性一致。

Conclusion: 所提出的框架能够实现鲁棒的在轨导航，并能在具有代表性的飞行传感器下保持有界估算误差。

Abstract: This paper develops a robust estimation framework for cislunar navigation
that embeds the Circular Restricted Three-Body Problem (CR3BP) dynamics and
bearing-only optical measurements within a Linear Fractional Transformation
(LFT) representation. A full-order $\mathcal{H}_\infty$ observer is synthesized
with explicit $\mathcal{L}_2$ performance bounds. The formulation yields a
nonlinear estimator that operates directly on the governing equations and
avoids reliance on local linearizations. Dominant nonlinearities are expressed
as structured real uncertainties, while measurement fidelity is represented
through range-dependent weighting with Earth-Moon distances reconstructed from
line-of-sight geometry. The sensing architecture assumes passive
star-tracker-class optical instruments, eliminating the need for time-of-flight
ranging or precision clocks. Simulations demonstrate bounded estimation errors
and smooth position tracking over multiple orbital periods, with the largest
deviations observed in the out-of-plane states, consistent with the stiffness
of the vertical dynamics and the limitations of angle-only observability.
Application to a Near Rectilinear Halo Orbit (NRHO) illustrates that the
framework can achieve robust onboard navigation with bounded estimation errors
with flight-representative sensors.

</details>


### [423] [Multi-Loop Design of Virtual Synchronous Machine Control for DFIG-Based Wind Farms](https://arxiv.org/abs/2510.05043)
*Javier Garcia-Aguilar,Aurelio Garcia-Cerrada,Juan L. Zamora,Emilio Bueno,Elena Saiz,Almudena Muñoz-Babiano,Mohammad E. Zarei*

Main category: eess.SY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The displacement of synchronous generators by converter-interfaced renewable
energy sources obliges wind farms to provide inertia, damping, and voltage
support, above all in increasingly weak grid conditions. This paper presents a
co-ordinated frequency-domain methodology for tuning all control layers of
doubly-fed induction generators (DFIGs) within a wind farm operated as a
Virtual Synchronous Machine (VSM). Starting from a full small-signal
linearisation that preserves loop-to-loop and machine-to-machine couplings, the
procedure reshapes every local open loop to explicit phase-margin targets
through a single, prioritised iteration. The resulting controllers provide a
step response and stability margins close to those programmed at the design
stage, in spite of the cross coupling between control loops. Since controller
synthesis relies exclusively on classical loop-shaping tools available in
commercial simulation suites, it is readily applicable to industrial-scale
projects.

</details>


### [424] [PowerPlots: An Open Source Power Grid Visualization and Data Analysis Framework for Academic Research](https://arxiv.org/abs/2510.05063)
*Noah Rhodes*

Main category: eess.SY

TL;DR: PowerPlots.jl是一个用于电力系统数据可视化的工具，可以帮助研究人员更好地理解和交流他们的研究。


<details>
  <summary>Details</summary>
Motivation: 数据可视化对于理解复杂系统至关重要，而电力系统是最复杂的系统之一。因此，需要一个专门的工具来帮助可视化电力系统数据。

Method: PowerPlots.jl是一个数据可视化工具，它能够将电力系统数据转化为更易于分析的格式，如图拓扑或数据帧。

Result: 该工具能够促进对电力系统数据的探索，并有助于交流研究成果。此外，它还提供了灵活性，以支持解决新颖的电力系统问题的研究。

Conclusion: PowerPlots.jl通过提供灵活的数据可视化和分析功能，使研究人员能够更好地理解和沟通复杂的电力系统研究。

Abstract: Data visualization is important for developing an understanding of a complex
system. PowerPlots.jl is a data visualization tool for power grids, one of the
most complex systems in the world. The design of PowerPlots.jl is intended to
facilitate exploration of power grid data while performing research and to
facilitate communication of research findings to an audience. Several tools
created to support this software also facilitate analysis of power grid data by
transforming the data into graph topology or data-frame data formats that are
more compatible for some applications. The high level of flexibility in
PowerPlots.jl enables researchers who are developing and analyzing methods for
solving novel power grid problems to better understand and communicate the
complexities of their research.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [425] [LegalSim: Multi-Agent Simulation of Legal Systems for Discovering Procedural Exploits](https://arxiv.org/abs/2510.03405)
*Sanket Badhe*

Main category: cs.MA

TL;DR: LegalSim是一个模拟法律诉讼的AI系统，旨在探索AI如何利用规则中的程序漏洞。该系统包含原告和被告AI代理，由一个规则引擎驱动，并由一个模拟法官模型进行裁决。通过比较PPO、结合LLM的上下文赌博机、直接LLM策略和手工启发式等四种策略，我们发现AI能够形成“利用链”，例如增加成本的发现过程和施加日历压力的策略，这些策略虽然程序上有效但可能对系统有害。评估结果表明，PPO获胜率最高，上下文赌博机最具竞争力，LLM策略次之，手工启发式策略最弱。该模拟揭示了程序漏洞的存在，并强调了对法律规则系统进行红队测试的必要性。


<details>
  <summary>Details</summary>
Motivation: 探索AI系统如何利用成文规则中的程序弱点来操纵或影响法律诉讼过程。

Method: 提出并实现了一个名为LegalSim的多智能体模拟系统，该系统模拟了对抗性的法律诉讼。原告和被告AI代理在一个受约束的动作空间内进行交互，规则由一个JSON规则引擎定义。一个随机的法官模型负责根据设定的参数（如批准率、成本分配、制裁倾向）来裁决结果。在模拟中，AI代理被训练和评估，目标不是简单的二元胜负，而是“有效胜率”和一个结合了对手成本膨胀、日历压力、低胜算和解压力以及规则合规裕度在内的综合“利用得分”。比较了四种AI策略：PPO、结合LLM的上下文赌博机、直接LLM策略和手工设计的启发式策略。

Result: 在不同的法律程序（如破产中止、接口审查、税务程序）和不同特性的法官模型下，观察到了AI代理形成了“利用链”（exploit chains），例如成本膨胀的发现序列和日历压力策略。这些策略在程序上是合规的，但可能对整个系统产生不利影响。通过交叉对比和Bradley-Terry评级评估，PPO策略的获胜次数最多；结合LLM的上下文赌博机在面对不同对手时表现最为稳定和具有竞争力；直接LLM策略表现次之；手工设计的启发式策略表现最弱。这些结果在不同的法官设定下表现稳定。

Conclusion: LegalSim模拟揭示了AI能够识别并利用法律程序中的漏洞，形成“利用链”从而在诉讼中获得优势，即使在程序上保持合规。这表明仅仅进行模型层面的测试是不够的，还需要对法律规则系统本身进行“红队测试”（red-teaming），以发现和修复潜在的程序弱点。 PPO、上下文赌博机、LLM策略和手工启发式策略在模拟中的表现各不相同，其中PPO获胜率最高，上下文赌博机最具竞争力。

Abstract: We present LegalSim, a modular multi-agent simulation of adversarial legal
proceedings that explores how AI systems can exploit procedural weaknesses in
codified rules. Plaintiff and defendant agents choose from a constrained action
space (for example, discovery requests, motions, meet-and-confer, sanctions)
governed by a JSON rules engine, while a stochastic judge model with calibrated
grant rates, cost allocations, and sanction tendencies resolves outcomes. We
compare four policies: PPO, a contextual bandit with an LLM, a direct LLM
policy, and a hand-crafted heuristic; Instead of optimizing binary case
outcomes, agents are trained and evaluated using effective win rate and a
composite exploit score that combines opponent-cost inflation, calendar
pressure, settlement pressure at low merit, and a rule-compliance margin.
Across configurable regimes (e.g., bankruptcy stays, inter partes review, tax
procedures) and heterogeneous judges, we observe emergent ``exploit chains'',
such as cost-inflating discovery sequences and calendar-pressure tactics that
remain procedurally valid yet systemically harmful. Evaluation via cross-play
and Bradley-Terry ratings shows, PPO wins more often, the bandit is the most
consistently competitive across opponents, the LLM trails them, and the
heuristic is weakest. The results are stable in judge settings, and the
simulation reveals emergent exploit chains, motivating red-teaming of legal
rule systems in addition to model-level testing.

</details>


### [426] [Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.03534)
*Nicolò Dal Fabbro,Milad Mesbahi,Renato Mendes,João Borges de Sousa,George J. Pappas*

Main category: cs.MA

TL;DR: 我们提出了一种多智能体强化学习方法，用于使用多辆自主水下航行器（AUV）对河流羽流进行长期测绘，通过结合高斯过程回归和多头Q网络控制器来优化AUV的航向和速度，并通过模拟证明了该方法在准确性和持久性方面优于现有方法，并能推广到不同的季节和年份。


<details>
  <summary>Details</summary>
Motivation: 研究使用多辆自主水下航行器（AUV）进行河流羽流长期（多日）测绘的问题，以解决Douro河流域的实际应用场景。

Method: 提出一种能源和通信效率高的多智能体强化学习方法。该方法结合了时空高斯过程回归（GPR）和一个多头Q网络控制器，该控制器对每辆AUV的方向和速度进行调控。中央协调器会间歇性地与AUVs通信，收集测量数据并发布指令。

Result: 通过Delft3D海洋模型的模拟，证明了所提出的方法在均方误差（MSE）和运行持久性方面持续优于单一智能体和多智能体基准方法。增加智能体数量可以提高准确性和持久性，有时数量加倍可以使持久性加倍以上。学习到的策略能够泛化到不同月份和年份的未见过的季节性规律。

Conclusion: 所提出的基于多智能体强化学习的方法能够有效地进行河流羽流的长期测绘，并且在准确性和持久性方面表现优于现有方法。该方法具有良好的泛化能力，为未来开发数据驱动的动态羽流环境长期监测方法提供了前景。

Abstract: We study the problem of long-term (multiple days) mapping of a river plume
using multiple autonomous underwater vehicles (AUVs), focusing on the Douro
river representative use-case. We propose an energy - and communication -
efficient multi-agent reinforcement learning approach in which a central
coordinator intermittently communicates with the AUVs, collecting measurements
and issuing commands. Our approach integrates spatiotemporal Gaussian process
regression (GPR) with a multi-head Q-network controller that regulates
direction and speed for each AUV. Simulations using the Delft3D ocean model
demonstrate that our method consistently outperforms both single- and
multi-agent benchmarks, with scaling the number of agents both improving mean
squared error (MSE) and operational endurance. In some instances, our algorithm
demonstrates that doubling the number of AUVs can more than double endurance
while maintaining or improving accuracy, underscoring the benefits of
multi-agent coordination. Our learned policies generalize across unseen
seasonal regimes over different months and years, demonstrating promise for
future developments of data-driven long-term monitoring of dynamic plume
environments.

</details>


### [427] [Cooperative Flexibility Exchange: Fair and Comfort-Aware Decentralized Resource Allocation](https://arxiv.org/abs/2510.04192)
*Rabiya Khalid,Evangelos Pournaras*

Main category: cs.MA

TL;DR: 本研究提出了一种去中心化的多智能体协调的需求侧管理系统，通过槽位交换机制，在保证系统效率的同时，提高了用户舒适度和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有能源管理系统优先考虑系统效率，牺牲了用户舒适度，本研究旨在解决此问题。

Method: 提出了一种去中心化的多智能体协调的需求侧管理系统，并引入了槽位交换机制，允许智能体在获取优化后的用电计划后进行协调调整。

Result: 使用真实世界数据集的评估结果表明，该方法提高了用户舒适度和公平性，且未增加系统效率成本。

Conclusion: 该研究提出的基于槽位交换机制的去中心化多智能体协调需求侧管理系统，在提高用户舒适度和公平性的同时，保持了系统效率，是一种实用且可扩展的未来智能电网解决方案。

Abstract: The growing electricity demand and increased use of smart appliances are
placing new pressures on power grids, making efficient energy management more
important than ever. The existing energy management systems often prioritize
system efficiency (balanced energy demand and supply) at the expense of user
comfort. This paper addresses this gap by proposing a novel decentralized
multi-agent coordination-based demand-side management system. The proposed
system enables individual agents to coordinate for demand-side energy
optimization while improving the user comfort and maintaining the system
efficiency. A key innovation of this work is the introduction of a slot
exchange mechanism, where agents first receive optimized appliance-level energy
consumption schedules and then coordinate with each other to adjust these
schedules through slot exchanges. This approach improves user comfort even when
agents show non-altruistic behaviour, and it scales well with large
populations. The system also promotes fairness by balancing satisfaction levels
across users. For performance evaluation, a real-world dataset is used, and the
results demonstrate that the proposed slot exchange mechanism increases user
comfort and fairness without raising system inefficiency cost, making it a
practical and scalable solution for future smart grids.

</details>


### [428] [Small Fleet, Big Impact: Enhancing Shared Micromobility Efficiency through Minimal Autonomous Vehicle Deployment](https://arxiv.org/abs/2510.04271)
*Heng Tan,Hua Yan,Lucas Yang,Yu Yang*

Main category: cs.MA

TL;DR: 共享微出行车辆（如电动滑板车和自行车）已成为传统交通方式的可持续替代方案。然而，由于时空需求波动，这些系统面临持续的挑战，导致车辆供应与用户需求之间不匹配。现有的共享微出行车辆调度方法通常每天重新分配一次或两次车辆，这使得它们在非典型条件下容易出现性能下降。在这项工作中，我们旨在通过集成少量具有自主重新平衡能力的自主共享微出行车辆（ASMV）来增强现有的微出行调度方法，以适应实时需求。具体来说，我们提出了SMART，一个分层强化学习框架，该框架联合优化了ASMV的高层初始部署和低层实时重新平衡。我们根据芝加哥的真实电动滑板车使用数据评估了我们的框架。我们的实验结果表明，该框架非常有效，并具有很强的泛化能力，可以与现有的车辆调度方法无缝集成，并显著提高整体微出行服务性能。


<details>
  <summary>Details</summary>
Motivation: 共享微出行系统因时空需求波动而面临车辆供应与用户需求不匹配的挑战。现有调度方法在非典型条件下性能会下降。

Method: 提出了一种名为SMART的分层强化学习框架，该框架集成了具有自主重新平衡能力的自主共享微出行车辆（ASMV），以优化车辆的初始部署和实时重新平衡。

Result: 基于芝加哥的真实电动滑板车使用数据进行的实验表明，SMART框架非常有效，并具有很强的泛化能力，可以与现有调度方法集成，并显著提高服务性能。

Conclusion: SMART框架能够有效解决共享微出行系统的供需不匹配问题，并能与现有方法集成以提高服务性能。

Abstract: Shared micromobility systems, such as electric scooters and bikes, have
gained widespread popularity as sustainable alternatives to traditional
transportation modes. However, these systems face persistent challenges due to
spatio-temporal demand fluctuations, often resulting in a mismatch between
vehicle supply and user demand. Existing shared micromobility vehicle
scheduling methods typically redistribute vehicles once or twice per day, which
makes them vulnerable to performance degradation under atypical conditions. In
this work, we design to augment existing micromobility scheduling methods by
integrating a small number of autonomous shared micromobility vehicles (ASMVs),
which possess self-rebalancing capabilities to dynamically adapt to real-time
demand. Specifically, we introduce SMART, a hierarchical reinforcement learning
framework that jointly optimizes high-level initial deployment and low-level
real-time rebalancing for ASMVs. We evaluate our framework based on real-world
e-scooter usage data from Chicago. Our experiment results show that our
framework is highly effective and possesses strong generalization capability,
allowing it to seamlessly integrate with existing vehicle scheduling methods
and significantly enhance overall micromobility service performance.

</details>


### [429] [Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs](https://arxiv.org/abs/2510.04303)
*Om Tailor*

Main category: cs.MA

TL;DR: 现有的LLM审计方法缺乏理论保证且难以复现，本研究提出了一个包含理论、基准测试、检测和可复现性的综合研究成果“Audit the Whisper”，旨在解决LLM中的隐蔽协调问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM审计方法缺乏理论保证，难以跨任务迁移，且缺乏复现基础设施，导致隐蔽协调问题可能悄悄侵蚀信任和社会福利。

Method: 提出了一种渠道容量分析方法，量化了诸如释义、速率限制和角色置换等干预措施对容量的影响，并通过配对运行的KL散度诊断来操作化，以提高有限样本保证下的互信息阈值。设计了一个名为“ColludeBench-v0”的基准测试，包含定价、第一价格拍卖和同行评审等任务，并支持可配置的隐蔽方案、确定性清单和奖励工具。构建了一个校准审计流程，融合了跨运行互信息、排列不变性、水印方差和公平感知接受偏差等多种检测手段，并将误报率控制在10^-3。

Result: 在涵盖12种干预条件、600次审计运行的测试中，所提出的联合元测试达到了1的真正阳性率（TPR），且未出现误报。消融实验揭示了审计成本与效果之间的权衡，并突出了仅靠互信息检测不到的、受公平性驱动的共谋行为。

Conclusion: “Audit the Whisper”提供了一个包含理论、基准测试、检测和可复现性的综合框架，能够有效检测LLM中的隐蔽协调行为，并且易于外部审计者复现和扩展。

Abstract: Multi-agent deployments of large language models (LLMs) are increasingly
embedded in market, allocation, and governance workflows, yet covert
coordination among agents can silently erode trust and social welfare. Existing
audits are dominated by heuristics that lack theoretical guarantees, struggle
to transfer across tasks, and seldom ship with the infrastructure needed for
independent replication. We introduce \emph{Audit the Whisper}, a
conference-grade research artifact that spans theory, benchmark design,
detection, and reproducibility. Our contributions are: (i) a channel-capacity
analysis showing how interventions such as paraphrase, rate limiting, and role
permutation impose quantifiable capacity penalties -- operationalized via
paired-run Kullback--Leibler diagnostics -- that tighten mutual-information
thresholds with finite-sample guarantees; (ii) \textsc{ColludeBench}-v0,
covering pricing, first-price auctions, and peer review with configurable
covert schemes, deterministic manifests, and reward instrumentation; and (iii)
a calibrated auditing pipeline that fuses cross-run mutual information,
permutation invariance, watermark variance, and fairness-aware acceptance bias,
each tuned to a \(10^{-3}\) false-positive budget. Across 600 audited runs
spanning 12 intervention conditions, the union meta-test attains TPR~$=1$ with
zero observed false alarms, while ablations surface the price-of-auditing
trade-off and highlight fairness-driven colluders invisible to MI alone. We
release regeneration scripts, seed-stamped manifests, and documentation so that
external auditors can reproduce every figure and extend the framework with
minimal effort.

</details>


### [430] [NegotiationGym: Self-Optimizing Agents in a Multi-Agent Social Simulation Environment](https://arxiv.org/abs/2510.04368)
*Shashank Mangla,Chris Hokamp,Jack Boylan,Demian Gholipour Ghalandari,Yuuv Jauhari,Lauren Cassidy,Oisin Duffy*

Main category: cs.MA

TL;DR: NegotiationGym是一个用于配置和运行多智能体社交模拟的API和用户界面，专注于谈判和合作。


<details>
  <summary>Details</summary>
Motivation: 设计和实现一个易于使用的、配置驱动的API和用户界面，用于配置和运行专注于谈判和合作的多智能体社交模拟。

Method: NegotiationGym提供了一个用户友好的、配置驱动的API，可以轻松设计和定制模拟场景。智能体级别的效用函数对每个智能体的优化标准进行编码，智能体可以通过进行多轮交互、观察结果和修改未来轮次的策略来自我优化。

Result: 该代码库支持用户自定义模拟场景，并通过智能体效用函数和多轮交互实现智能体的自我优化。

Conclusion: NegotiationGym为研究谈判和合作提供了一个灵活且可定制的平台。

Abstract: We design and implement NegotiationGym, an API and user interface for
configuring and running multi-agent social simulations focused upon negotiation
and cooperation. The NegotiationGym codebase offers a user-friendly,
configuration-driven API that enables easy design and customization of
simulation scenarios. Agent-level utility functions encode optimization
criteria for each agent, and agents can self-optimize by conducting multiple
interaction rounds with other agents, observing outcomes, and modifying their
strategies for future rounds.

</details>


### [431] [Trade in Minutes! Rationality-Driven Agentic System for Quantitative Financial Trading](https://arxiv.org/abs/2510.04787)
*Zifan Song,Kaitao Song,Guosheng Hu,Ding Qi,Junyao Gao,Xiaohua Wang,Dongsheng Li,Cairong Zhao*

Main category: cs.MA

TL;DR: TiMi是一个理性驱动的多智能体系统，通过解耦策略开发和部署，实现量化交易的稳定盈利、高效率和风险控制。


<details>
  <summary>Details</summary>
Motivation: 当前的金融交易智能体存在情绪偏见、依赖外部信息和持续推理的限制，而TiMi旨在解决这些问题，将策略深度与量化交易的机械理性相结合。

Method: TiMi采用两层分析范式（宏观模式到微观定制）、分层编程设计（用于交易机器人实现）和闭环优化（通过数学反思驱动）。它利用了LLM的语义分析、代码编程和数学推理能力。

Result: 在超过200个股票和加密货币交易对的广泛评估中，TiMi在不稳定市场动态下实现了稳定的盈利能力、高效的行动和良好的风险控制。

Conclusion: TiMi在量化交易领域取得了成功，其架构和方法为开发更强大的交易智能体提供了基础。

Abstract: Recent advancements in large language models (LLMs) and agentic systems have
shown exceptional decision-making capabilities, revealing significant potential
for autonomic finance. Current financial trading agents predominantly simulate
anthropomorphic roles that inadvertently introduce emotional biases and rely on
peripheral information, while being constrained by the necessity for continuous
inference during deployment. In this paper, we pioneer the harmonization of
strategic depth in agents with the mechanical rationality essential for
quantitative trading. Consequently, we present TiMi (Trade in Minutes), a
rationality-driven multi-agent system that architecturally decouples strategy
development from minute-level deployment. TiMi leverages specialized LLM
capabilities of semantic analysis, code programming, and mathematical reasoning
within a comprehensive policy-optimization-deployment chain. Specifically, we
propose a two-tier analytical paradigm from macro patterns to micro
customization, layered programming design for trading bot implementation, and
closed-loop optimization driven by mathematical reflection. Extensive
evaluations across 200+ trading pairs in stock and cryptocurrency markets
empirically validate the efficacy of TiMi in stable profitability, action
efficiency, and risk control under volatile market dynamics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [432] [Mechanisms for Quantum Advantage in Global Optimization of Nonconvex Functions](https://arxiv.org/abs/2510.03385)
*Dylan Herman,Guneykan Ozgul,Anuj Apte,Junhyung Lyle Kim,Anupam Prakash,Jiayu Shen,Shouvanik Chakrabarti*

Main category: quant-ph

TL;DR: 本篇论文提出了新的量子优化机制，用于解决非凸函数的全局优化问题，扩展了量子计算在超越传统隧穿效应之外的应用。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索量子计算在非凸函数全局优化中的优势，特别是超越现有的基于隧穿效应的解释。

Method: 通过建立薛定谔算子谱性质与经典 Langevin 扩散混合时间之间的严谨联系，利用该联系提出一种新的优化机制。该机制基于一个关键点：量子算法处理原始势能，而经典扩散处理具有近似简并全局最小值的 WKB 势。在此基础上，证明了实空间绝热量子算法 (RsAA) 在广泛的非凸函数族上具有可证明的多项式时间优化能力。

Result: 1. 对于块可分离函数，RsAA 保持多项式运行时间，而现有算法需要指数时间。2. 对于经过适当扰动的强凸函数，RsAA 也实现了多项式运行时间，而现有算法仍面临指数级瓶颈。这些结果利用了半经典分析和内在超收缩理论的最新进展。

Conclusion: 本研究为量子计算在连续优化中的优势奠定了严谨的理论基础，并为连接量子算法、随机过程和半经典分析开辟了新的研究方向。

Abstract: We present new theoretical mechanisms for quantum speedup in the global
optimization of nonconvex functions, expanding the scope of quantum advantage
beyond traditional tunneling-based explanations. As our main building-block, we
demonstrate a rigorous correspondence between the spectral properties of
Schr\"{o}dinger operators and the mixing times of classical Langevin diffusion.
This correspondence motivates a mechanism for separation on functions with
unique global minimum: while quantum algorithms operate on the original
potential, classical diffusions correspond to a Schr\"{o}dinger operators with
a WKB potential having nearly degenerate global minima. We formalize these
ideas by proving that a real-space adiabatic quantum algorithm (RsAA) achieves
provably polynomial-time optimization for broad families of nonconvex
functions. First, for block-separable functions, we show that RsAA maintains
polynomial runtime while known off-the-shelf algorithms require exponential
time and structure-aware algorithms exhibit arbitrarily large polynomial
runtimes. These results leverage novel non-asymptotic results in semiclassical
analysis. Second, we use recent advances in the theory of intrinsic
hypercontractivity to demonstrate polynomial runtimes for RsAA on appropriately
perturbed strongly convex functions that lack global structure, while
off-the-shelf algorithms remain exponentially bottlenecked. In contrast to
prior works based on quantum tunneling, these separations do not depend on the
geometry of barriers between local minima. Our theoretical claims about
classical algorithm runtimes are supported by rigorous analysis and
comprehensive numerical benchmarking. These findings establish a rigorous
theoretical foundation for quantum advantage in continuous optimization and
open new research directions connecting quantum algorithms, stochastic
processes, and semiclassical analysis.

</details>


### [433] [Quantum feature-map learning with reduced resource overhead](https://arxiv.org/abs/2510.03389)
*Jonas Jäger,Philipp Elsässer,Elham Torabian*

Main category: quant-ph

TL;DR: Q-FLAIR通过将工作负载转移到经典计算机，在迭代特征图电路构建中减少了量子资源开销，实现了最先进的基准性能。


<details>
  <summary>Details</summary>
Motivation: 需要能够经济地利用有限资源的量子算法，特别是在量子机器学习领域，而量子特征映射是关键。

Method: 提出了一种名为Q-FLAIR的算法，通过部分的解析重构将工作负载转移到经典计算机，并对数据特征和权重参数进行选择和优化。

Result: Q-FLAIR在量子神经网络和量子核支持向量分类器中表现出最先进的基准性能，并在真实的IBM设备上以90%以上的准确率训练了全分辨率MNIST数据集（784个特征），而资源开销与特征维度无关。

Conclusion: Q-FLAIR通过重新思考特征映射学习，在使机器学习在现实世界问题和近期量子计算机上成为可能方面迈出了重要一步。

Abstract: Current quantum computers require algorithms that use limited resources
economically. In quantum machine learning, success hinges on quantum feature
maps, which embed classical data into the state space of qubits. We introduce
Quantum Feature-Map Learning via Analytic Iterative Reconstructions (Q-FLAIR),
an algorithm that reduces quantum resource overhead in iterative feature-map
circuit construction. It shifts workloads to a classical computer via partial
analytic reconstructions of the quantum model, using only a few evaluations.
For each probed gate addition to the ansatz, the simultaneous selection and
optimization of the data feature and weight parameter is then entirely
classical. Integrated into quantum neural network and quantum kernel support
vector classifiers, Q-FLAIR shows state-of-the-art benchmark performance. Since
resource overhead decouples from feature dimension, we train a quantum model on
a real IBM device in only four hours, surpassing 90% accuracy on the
full-resolution MNIST dataset (784 features, digits 3 vs 5). Such results were
previously unattainable, as the feature dimension prohibitively drives hardware
demands for fixed and search costs for adaptive ans\"atze. By rethinking
feature-map learning beyond black-box optimization, this work takes a concrete
step toward enabling quantum machine learning for real-world problems and
near-term quantum computers.

</details>


### [434] [Optimizing and benchmarking the computation of the permanent of general matrices](https://arxiv.org/abs/2510.03421)
*Cassandra Masschelein,Michelle Richer,Paul W. Ayers*

Main category: quant-ph

TL;DR: 该论文介绍了一个用于计算矩阵永久值的开源软件，它实现了三种最快的精确算法（组合算法、Ryser算法和Glynn算法），并能根据输入矩阵的类型和维度自动选择最优算法。


<details>
  <summary>Details</summary>
Motivation: 现有软件缺乏自动选择最快算法来计算矩阵永久值的功能。

Method: 设计并实现了一个支持三种精确算法（组合算法、Ryser算法、Glynn算法）的软件，并对Glynn算法进行了扩展以支持矩形矩阵，实现了根据输入矩阵自动选择最优算法的功能。

Result: 开发了一个开源的矩阵永久值计算软件，该软件能在Github上获取，并提供了三种算法的实现和自动选择机制。

Conclusion: 该软件为计算矩阵永久值提供了一个高效且用户友好的解决方案。

Abstract: Evaluating the permanent of a matrix is a fundamental computation that
emerges in many domains, including traditional fields like computational
complexity theory, graph theory, many-body quantum theory and emerging
disciplines like machine learning and quantum computing. While conceptually
simple, evaluating the permanent is extremely challenging: no polynomial-time
algorithm is available (unless $\textsc{P} = \textsc{NP}$). To the best of our
knowledge there is no publicly available software that automatically uses the
most efficient algorithm for computing the permanent. In this work we designed,
developed, and investigated the performance of our software package which
evaluates the permanent of an arbitrary rectangular matrix, supporting three
algorithms generally regarded as the fastest while giving the exact solution
(the straightforward combinatoric algorithm, the Ryser algorithm, and the Glynn
algorithm) and, optionally, automatically switching to the optimal algorithm
based on the type and dimensionality of the input matrix. To do this, we
developed an extension of the Glynn algorithm to rectangular matrices. Our free
and open-source software package is distributed via Github, at
https://github.com/theochem/matrix-permanent.

</details>


### [435] [Single-Spin Nitrogen-Vacancy Magnetometer with Enhanced Static Field Sensitivity](https://arxiv.org/abs/2510.03462)
*Vinaya K. Kavatamane,Dewen Duan,Hadi Zadeh-Haghighi,Manh-Huong Phan,Gopalakrishnan Balasubramanian*

Main category: quant-ph

TL;DR: 使用集成软磁微线的单氮-空位（NV）中心实现对静态磁场的灵敏度提高了500倍。


<details>
  <summary>Details</summary>
Motivation: 精确传感和成像弱静态磁场对于各种新兴的纳米级应用至关重要，但传统的NV中心在静态磁场传感方面受到相干时间（T2*）的限制。

Method: 提出了一种新颖的混合传感方法，将软磁微线与近地表NV中心集成，以放大其对外部静态磁场的响应。

Result: 在混合配置中，单个NV中心的DC磁场灵敏度为63 nT/sqrt(Hz)，比传统方法提高了约500倍。

Conclusion: 这种集成软磁微线的单NV中心传感器具有高度灵敏和紧凑的特点，为检测静态或缓慢变化的磁场开辟了新的机会，具有广泛的应用潜力。

Abstract: Precision sensing and imaging of weak static magnetic fields are crucial for
a variety of emerging nanoscale applications. While nitrogen-vacancy (NV)
centers in diamond provide exceptional AC magnetic field sensitivity with
nanoscale spatial resolution, their sensitivity to static (DC) magnetic fields
is fundamentally limited by the short dephasing time (T2*) due to spin-spin
interactions. In this work, we present a novel hybrid sensing approach that
integrates a soft ferromagnetic microwire with a single near-surface NV center
to amplify its response to external static magnetic fields. This hybrid
configuration achieves a DC magnetic field sensitivity of 63 nT/sqrt(Hz) for a
single NV center - about 500 times greater than conventional inhomogeneous
broadening- or T2*-limited magnetometry, with potential for further
enhancement. The compact and highly sensitive nature of this sensor opens new
opportunities for quantum sensing applications involving the detection of
static or slowly varying magnetic fields across diverse scientific and
technological domains.

</details>


### [436] [The quantum smooth label cover problem is undecidable](https://arxiv.org/abs/2510.03477)
*Eric Culf,Kieran Mastel,Connor Paddock,Taro Spirig*

Main category: quant-ph

TL;DR: 量子平滑标签覆盖问题是 RE-hard 的，这与量子唯一标签覆盖问题的可有效判定性形成对比。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是探索量子平滑标签覆盖问题的计算复杂性，并将其与已知的量子唯一标签覆盖问题和量子标签覆盖问题进行比较，同时检验其与 Mousavi 和 Spirig 提出的量子或acularized 唯一标签博弈猜想的一致性。

Method: 该研究采用了从停机问题到量子平滑标签覆盖问题的系列归约，并包含了一个量子可靠的 Feige 从 3SAT 到 3SAT5 的归约。

Result: 量子平滑标签覆盖问题被证明是 RE-hard 的，并且量子或acularized 平滑标签覆盖问题也是 RE-hard 的。

Conclusion: 量子平滑标签覆盖问题的 RE-hardness 结果与量子标签覆盖问题的 RE-hardness 以及量子或acularized 平滑标签覆盖问题与量子或acularized 唯一标签博弈猜想的一致性表明了该领域的重要进展。

Abstract: We show that the quantum smooth label cover problem is RE-hard. This
contrasts with the quantum unique label cover problem, which can be decided
efficiently by Kempe, Regev, and Toner (FOCS'08). Our result aligns with the
RE-hardness of the quantum label cover problem, which follows from the
celebrated MIP* = RE result of Ji, Natarajan, Vidick, Wright, and Yuen
(ACM'21). Additionally, we show that the quantum oracularized smooth label
cover problem is also RE-hard. This aligns with the alternative quantum unique
games conjecture on the RE-hardness of the quantum oracularized unique label
cover problem proposed by Mousavi and Spirig (ITCS'25). Our techniques employ a
series of reductions from the halting problem to the quantum smooth label cover
problem, and include a quantum-sound version of Feige's reduction from 3SAT to
3SAT5 (STOC'96), which may be of independent interest.

</details>


### [437] [A Quantum-Secure Voting Framework Using QKD, Dual-Key Symmetric Encryption, and Verifiable Receipts](https://arxiv.org/abs/2510.03489)
*Taha M. Mahmoud,Naima Kaabouch*

Main category: quant-ph

TL;DR: 本框架利用量子密钥分发、双密钥对称加密和可验证收据机制，为电子投票提供量子安全保障，确保投票的隐私性、完整性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，电子投票系统面临日益严峻的网络攻击和数据泄露风险。

Method: 本研究引入一个集成了量子密钥分发（QKD）、双密钥对称加密和可验证收据机制的量子安全投票框架。该框架允许选民安全地建立加密密钥，投递加密的选票，并通过基于收据的确认来验证其投票，同时不暴露投票内容。为评估性能，研究人员使用消息队列遥测传输（MQTT）协议模拟了量子和经典通信渠道。

Result: 仿真结果表明，该系统能够以低延迟和最小的错误率高效地处理大量选票。

Conclusion: 本研究提出的量子安全投票框架为在量子计算时代实现安全、透明和可验证的电子投票提供了一条可扩展且实用的途径。

Abstract: Electronic voting systems face growing risks from cyberattacks and data
breaches, which are expected to intensify with the advent of quantum computing.
To address these challenges, we introduce a quantum-secure voting framework
that integrates Quantum Key Distribution (QKD), Dual-Key Symmetric Encryption,
and verifiable receipt mechanisms to strengthen the privacy, integrity, and
reliability of the voting process. The framework enables voters to establish
encryption keys securely, cast encrypted ballots, and verify their votes
through receipt-based confirmation, all without exposing the vote contents. To
evaluate performance, we simulate both quantum and classical communication
channels using the Message Queuing Telemetry Transport (MQTT) protocol. Results
demonstrate that the system can process large numbers of votes efficiently with
low latency and minimal error rates. This approach offers a scalable and
practical path toward secure, transparent, and verifiable electronic voting in
the quantum era.

</details>


### [438] [Optimising quantum data hiding](https://arxiv.org/abs/2510.03538)
*Francesco Anna Mele,Ludovico Lami*

Main category: quant-ph

TL;DR: 存在可区分但几乎无法区分的量子数据隐藏状态，可以使用群对称性构造出可分离、完美正交且部分转置不变的新型数据隐藏状态，实现了“无纠缠的非局域性”。


<details>
  <summary>Details</summary>
Motivation: 介绍量子数据隐藏现象，并指出先前存在的 Werner 状态和随机状态在可分离性、完美正交性和可隐藏的秘密数量方面存在局限性。

Method: 提出了一种利用群对称性的新型数据隐藏状态的显式构造方法，并结合数值分析工具来研究量子信息理论中的凸优化问题。

Result: 构造出了一类新型数据隐藏状态，这些状态同时满足可分离、完美正交和部分转置不变的性质，实现了“无纠缠的非局域性”的最大化。

Conclusion: 通过群对称性和数值分析工具，成功构造了性质最优的数据隐藏状态，并提出该方法可能在量子信息理论领域有更广泛的应用。

Abstract: Quantum data hiding is the existence of pairs of bipartite quantum states
that are (almost) perfectly distinguishable with global measurements, yet close
to indistinguishable when only measurements implementable with local operations
and classical communication are allowed. Remarkably, data hiding states can
also be chosen to be separable, meaning that secrets can be hidden using no
entanglement that are almost irretrievable without entanglement -- this is
sometimes called `nonlocality without entanglement'. Essentially two families
of data hiding states were known prior to this work: Werner states and random
states. Hiding Werner states can be made either separable or globally perfectly
orthogonal, but not both -- separability comes at the price of orthogonality
being only approximate. Random states can hide many more bits, but they are
typically entangled and again only approximately orthogonal. In this paper, we
present an explicit construction of novel group-symmetric data hiding states
that are simultaneously separable, perfectly orthogonal, and even invariant
under partial transpose, thus exhibiting the phenomenon of nonlocality without
entanglement to the utmost extent. Our analysis leverages novel applications of
numerical analysis tools to study convex optimisation problems in quantum
information theory, potentially offering technical insights that extend beyond
this work.

</details>


### [439] [Quantum algorithm for Electromagnetic Field Analysis](https://arxiv.org/abs/2510.03596)
*Hiroyuki Tezuka,Yuki Sato*

Main category: quant-ph

TL;DR: 该研究提出一种利用量子模拟求解电磁学和光子学设计中的偏微分方程（PDE）的方法，特别关注了处理复杂几何结构的挑战，并通过逻辑压缩来优化解决方案，并在金属透镜的仿真中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 经典电磁学和光子学设计中的求解器在处理大型或复杂结构时成本高昂，因此需要更有效的方法。

Method: 将麦克斯韦方程组转化为哈密顿量形式，并结合边界条件和可观测量。研究了在复杂几何结构下哈密顿量项的指数增长问题，并提出通过逻辑压缩来缓解此问题，尤其适用于周期性或对称结构。

Result: 成功模拟了光波通过金属透镜的传播，并展示了该方法能够捕捉波前整形和聚焦行为。

Conclusion: 基于哈密顿量的量子模拟在光子系统中是可行的，并且周期性或对称结构有利于提高计算效率，为光子系统设计优化提供了新的途径。

Abstract: Partial differential equations (PDEs) are central to computational
electromagnetics (CEM) and photonic design, but classical solvers face high
costs for large or complex structures. Quantum Hamiltonian simulation provides
a framework to encode PDEs into unitary time evolution and has potential for
scalable electromagnetic analysis. We formulate Maxwell's equations in the
potential representation and embed governing equations, boundary conditions,
and observables consistently into Hamiltonian form. A key bottleneck is the
exponential growth of Hamiltonian terms for complex geometries; we examine this
issue and show that logical compression can substantially mitigate it,
especially for periodic or symmetric structures. As a proof of concept, we
simulate optical wave propagation through a metalens and illustrate that the
method can capture wavefront shaping and focusing behavior, suggesting its
applicability to design optimization tasks. This work highlights the
feasibility of Hamiltonian-based quantum simulation for photonic systems and
identifies structural conditions favorable for efficient execution.

</details>


### [440] [Floquet Diamond Sensor with Optimal Precision](https://arxiv.org/abs/2510.03618)
*Qi-Tao Duan,Teng Li,Si-Qi Chen,Shengshi Pang,He Lu*

Main category: quant-ph

TL;DR: 金刚石传感器在量子传感领域具有潜力，但传统传感器在非共振情况下精度会下降。本文提出了一种新方法——Floquet金刚石传感器（FDS），用于高精度离共振微波（MW）幅度传感，且无需衰减信号MW强度。


<details>
  <summary>Details</summary>
Motivation: 传统金刚石传感器在微波信号非共振时精度会严重下降。本研究旨在提出一种新的传感器设计，以解决这一问题，实现高精度离共振微波幅度传感。

Method: 利用周期驱动场诱导准能量移动，使其匹配离共振微波频率，从而实现高精度传感。通过量子Fisher信息来表征FDS的测量精度，并验证其在相干时间内接近海森堡极限。此外，研究了FDS对控制误差的容忍度和与动力学耦合协议的兼容性。

Result: FDS在离共振情况下实现了高精度微波幅度传感，测量精度接近海森堡极限，并展现出对控制误差的鲁棒性和与动力学耦合协议的兼容性。

Conclusion: FDS是一种实用技术，可实现高精度的离共振微波传感，并确认了量子传感的量子优势。

Abstract: The diamond sensor has emerged as a promising platform for quantum sensing,
enabling the estimation of physical quantities -- such as microwave~(MW) field
-- with precision unattainable by classical counterpart. However, traditional
diamond sensors suffer severe precision degradation when the signal MW is not
resonant with the sensor transition frequency. Here, we propose and demonstrate
a Floquet diamond sensor~(FDS) for high-precision off-resonant MW amplitude
sensing without attenuating the strength of the signal MW. The periodic driven
field effectively induces an quasi-energy shift that matches the off-resonant
MW frequency. The measurement precision of FDS is characterized by quantum
Fisher information, which approaches the ultimate precision -- Heisenberg limit
-- within the coherent time. Furthermore, the FDS exhibits robust tolerance to
practical control errors and is compatible with dynamical coupling protocol,
enabling a robust and high-sensitivity magnetic sensing. Our results confirm
the quantum advantage of quantum sensing and provide a practical technology for
high-precision off-resonant MW sensing.

</details>


### [441] [Metasurface-Based Dual-Basis Polarization Beam Splitter for efficient entanglement witnessing](https://arxiv.org/abs/2510.03931)
*Mohamed ElKabbash*

Main category: quant-ph

TL;DR: 提出一种基于超表面的量子纠缠探测方法，可同时进行双基（σ_z 和 σ_y）投影，提高效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统量子纠缠探测方法效率低、可扩展性差，需要多极化基进行序贯重构。

Method: 设计一种超表面分析仪，利用超原子的各向异性和几何结构，实现偏振相关的光束偏转，将双基（σ_z 和 σ_y）投影映射到正交空间模式，从而直接获得纠缠验证所需的双光子关联函数⟨σ_z ⊗ σ_z⟩和⟨σ_y ⊗ σ_y⟩。

Result: 该方法将测量开销减少了一半，并提供了一个紧凑、可集成的芯片级量子光子学平台。

Conclusion: 提出的超表面方法为高效量子纠缠验证提供了途径，可应用于量子密钥分发、量子中继和可扩展量子网络。

Abstract: Entanglement witnessing is essential for quantum technologies such as
computing, key distribution, and networking. Conventional bulk-optics methods
require sequential reconfiguration across multiple polarization bases, limiting
efficiency and scalability. We propose a metasurface-based analyzer that
performs dual-basis (\sigma_z and \sigma_y) projections simultaneously by
mapping them to orthogonal spatial modes. This allows direct access to the
commuting two-photon correlators \langle \sigma_z \otimes \sigma_z \rangle and
\langle \sigma_y \otimes \sigma_y \rangle required for entanglement witnessing.
The metasurface design employs meta-atoms engineered to impart independent
linear and circular phase delays through anisotropy and geometric control,
resulting in polarization-dependent beam deflection that separates H/V and R/L
components. This approach halves the measurement overhead compared to
sequential analysis while offering a compact, integrable platform for
chip-scale quantum photonics. The proposed scheme provides a path toward
efficient entanglement verification with applications in quantum key
distribution, quantum repeaters, and scalable quantum networks.

</details>


### [442] [Broadband Quantum Photon Source in Step-Chirped Periodically Poled Lithium Niobate Waveguide](https://arxiv.org/abs/2510.03619)
*Xiao-Xu Fang,Guoliang Shentu,He Lu*

Main category: quant-ph

TL;DR: 该研究设计并制备了一种用于宽带二次谐波产生（SHG）和自发参量下转换（SPDC）的步进啁啾周期性极化铌酸锂（CPPLN）波导。


<details>
  <summary>Details</summary>
Motivation: 开发用于经典和量子光学领域的宽带非线性光学器件。

Method: 设计并制造了6.82毫米长的步进啁啾周期性极化铌酸锂（CPPLN）波导，工作在铌酸锂绝缘体上（LNOI）。

Result: 在1510纳米-1620纳米的波长范围内，SHG实现了54.4%/W/cm²的平均效率。当泵浦波长为775、780和785纳米时，SPDC实现了高达99 THz（846 nm）的峰值全带宽和20 GHz/mW/nm的峰值亮度。

Conclusion: 所提出的CPPLN波导提供了一种高效且易于实验的方法来产生宽带光子对，有望推动量子计量学等应用的发展。

Abstract: Broadband nonlinear optical devices play a critical role in both classical
and quantum optics. Here, we design and fabricate a 6.82-mm-long step-chirped
periodically poled lithium niobate~(CPPLN) waveguide on lithium niobate on
insulator, which enables quasi-phase matching over a broad bandwidth for
second-harmonic generation~(SHG) and spontaneous parametric
down-conversion~(SPDC). The SHG achieves an average efficiency of
54.4\%/W/cm$^2$ over the first-harmonic wavelength range of 1510~nm-1620~nm,
paving the way for realizing SPDC across a wide range of pump wavelengths. For
SPDC, by tuning the pump wavelength to 775~nm, 780~nm, and 785~nm, we achieve
broadband photon-pair generation with a maximum full bandwidth and brightness
up to 99~THz~(846~nm) and 20~GHz/mW/nm, respectively. Our findings provide an
efficient and experiment-friendly approach for generating broadband photon
pairs, which holds significant promise for advancing applications in quantum
metrology.

</details>


### [443] [Compact non-degenerate entangled-photon source and near-infrared-to-telecom quantum teleportation](https://arxiv.org/abs/2510.03620)
*Xu-Jie Peng,Ling-Xuan Kong,He Lu*

Main category: quant-ph

TL;DR: 生成一个“太长不看”的摘要。


<details>
  <summary>Details</summary>
Motivation: 设计并演示了一种紧凑型Sagnac型光子源，该源可在不同波长（810 nm和1550 nm）下产生高度偏振纠缠的光子对，以连接在不同波长下工作的量子系统。

Method: 通过类型-0非简并自发参量下转换，在周期性极化铌酸锂晶体中，利用532 nm连续波激光器泵浦，产生810 nm和1550 nm的偏振纠缠光子对。使用单个潜望镜旋转三束光的偏振，以实现紧凑稳定的Sagnac干涉仪。

Result: 实现了高亮度（$3 	imes 10^4$ 对/s/mW）和高保真度（$0.985 	imes 0.002$）的偏振纠缠光子对，并通过违反CHSH不等式（$	ext{S} = 2.756 	imes 0.007$）进行了验证。最终，通过该非简并光源演示了量子隐形传态，将810 nm下的光子态以$0.955 	imes 0.003$的保真度传态至1550 nm。

Conclusion: 所提出的紧凑型Sagnac型非简并偏振纠缠光子源在量子信息科学中具有重要应用，特别是在连接不同波长量子系统方面，并且成功演示了跨波长量子隐形传态。

Abstract: The polarization-entangled photon source (PEPS) at non-degenerated
wavelengths is pivotal to connect quantum systems working at different
wavelengths, with the assistance of quantum teleportation. Here, a compact
Sagnac-type photon source is designed and demonstrated, in which two photons
with wavelengths at 810 and 1550~nm are highly entangled in polarization degree
of freedom. The two photons are generated from a periodically poled lithium
niobate crystal pumped with a 532~nm continuous-wave laser, via type-0
nondegenerate spontaneous parametric down-conversion. The polarization of three
lights is rotated by a single periscope, which makes the Sagnac interferometer
compact and stable. The generated two photons are with high brightness of
$3\times10^4$ pairs/s/mW, which are highly entangled with fidelity of
$0.985\pm0.002$. The entanglement is verified by violating the
Clauser-Horne-Shimony-Holt inequality with $\mathcal S =2.756\pm0.007$.
Finally, teleportation is demonstrated with this nondegenerate source, in which
photonic states at 810~nm is teleported to 1550~nm with fidelity of
$0.955\pm0.003$.

</details>


### [444] [Towards the simulation of higher-order quantum resources: a general type-theoretic approach](https://arxiv.org/abs/2510.03622)
*Samuel B. Steakley,Elia Zanoni,Carlo Maria Scandolo*

Main category: quant-ph

TL;DR: 量子资源存在一个多层次的层级结构，从零阶（量子态到量子态）到一阶（量子门到量子门），并提出了一种统一的理论框架，使用类型系统、并行积运算和推广的完全正性概念来处理任意阶的量子对象。


<details>
  <summary>Details</summary>
Motivation: 需要一个统一的理论框架来处理量子资源的层级结构，包括不同阶数的资源之间的相互转化。

Method: 引入一个基于类型系统的框架，并定义了并行积运算（泛化张量积）以及推广的完全正性概念，以处理任意阶的量子对象。

Result: 提出了一种能够统一处理任意阶量子对象的理论框架，该框架使用类型系统、并行积和推广的完全正性概念。

Conclusion: 所提出的框架能够处理任意阶的量子对象，并为操作性地处理量子对象提供了一种途径。

Abstract: Quantum resources exist in a hierarchy of multiple levels. At order zero,
quantum states are transformed by linear maps (channels, or gates) in order to
perform computations or simulate other states. At order one, gates and channels
are transformed by linear maps (superchannels) in order to simulate other
gates. To develop a full hierarchy of quantum resources, beyond those first two
orders, and to account for the fact that quantum protocols can interconvert
resources of different orders, we need a theoretical framework that addresses
all orders in a uniform manner. We introduce a framework based on a system of
types, which label the different kinds of objects that are present at different
orders. We equip the framework with a parallel product operation that modifies
and generalizes the tensor product so as to be operationally meaningful for
maps of distinct and arbitrary orders. Finally, we introduce a family of convex
cones that generalize the notion of complete positivity to all orders, with the
aim of characterizing the objects that are physically admissible, facilitating
an operational treatment of quantum objects at any order.

</details>


### [445] [The power of quantum circuits in sampling](https://arxiv.org/abs/2510.03645)
*Guy Blanc,Caleb Koch,Jane Lange,Carmen Strassle,Li-Yang Tan*

Main category: quant-ph

TL;DR: 量子电路比经典电路强大得多，尤其是在近似采样方面。


<details>
  <summary>Details</summary>
Motivation: 提供量子电路在计算能力上超越经典电路的新证据，特别是在近似采样问题上。

Method: 利用新的经典查询复杂度硬化引理，证明了Yamakawa-Zhandry搜索问题的相关性质。

Result: 相对于随机预言机，证明了多项式规模的量子电路能够采样经典子指数规模电路无法近似（TV距离$1-o(1)$）的分布。

Conclusion: 量子计算机在近似采样任务上比经典计算机具有显著的优势，这对于理解量子计算的潜力至关重要。

Abstract: We give new evidence that quantum circuits are substantially more powerful
than classical circuits. We show, relative to a random oracle, that
polynomial-size quantum circuits can sample distributions that
subexponential-size classical circuits cannot approximate even to TV distance
$1-o(1)$. Prior work of Aaronson and Arkhipov (2011) showed such a separation
for the case of exact sampling (i.e. TV distance $0$), but separations for
approximate sampling were only known for uniform algorithms.
  A key ingredient in our proof is a new hardness amplification lemma for the
classical query complexity of the Yamakawa-Zhandry (2022) search problem. We
show that the probability that any family of query algorithms collectively
finds $k$ distinct solutions decays exponentially in $k$.

</details>


### [446] [Accelerating Extended Benders Decomposition with Quantum-Classical Hybrid Solver](https://arxiv.org/abs/2510.03647)
*Takuma Yoshihara,Masayuki Ohzeki*

Main category: quant-ph

TL;DR: We propose a quantum-classical hybrid method for solving large-scale mixed-integer quadratic problems (MIQP) by integrating the D-Wave CQM solver into the extended Benders decomposition framework. This approach can efficiently yield near-optimal solutions and achieve exponential speedups over classical solvers for certain problem instances.


<details>
  <summary>Details</summary>
Motivation: Extended Benders decomposition, while effective for MIQP, suffers from a computational bottleneck in its master problem, which handles integer and quadratic variables. This work aims to address this challenge.

Method: We integrate the D-Wave CQM solver into the extended Benders decomposition framework to directly solve the master problem, creating a quantum-classical hybrid method for MIQP.

Result: The proposed hybrid approach efficiently yields near-optimal solutions for large-scale MIQP. For certain problem instances, it achieves exponential speedups compared to the leading commercial classical solver.

Conclusion: The quantum-classical hybrid method presents a promising computational strategy for solving complex mixed-integer optimization problems, offering significant speedups and near-optimal solutions.

Abstract: We propose a quantum-classical hybrid method for solving large-scale
mixed-integer quadratic problems (MIQP). Although extended Benders
decomposition is effective for MIQP, its master problem which handles the
integer and quadratic variables often becomes a computational bottleneck. To
address this challenge, we integrate the D-Wave CQM solver into the
decomposition framework to solve the master problem directly. Our results show
that this hybrid approach efficiently yields near-optimal solutions and, for
certain problem instances, achieves exponential speedups over the leading
commercial classical solver. These findings highlight a promising computational
strategy for tackling complex mixed-integer optimization problems.

</details>


### [447] [Reduction of the impact of the local valley splitting on the coherence of conveyor-belt spin shuttling in $^{28}$Si/SiGe](https://arxiv.org/abs/2510.03773)
*Mats Volmer,Tom Struck,Jhih-Sian Tu,Stefan Trellenkamp,Davide Degli Esposti,Giordano Scappucci,Łukasz Cywiński,Hendrik Bluhm,Lars R. Schreiber*

Main category: quant-ph

TL;DR: 硅量子芯片在可扩展、容错的量子计算方面具有潜力，但量子点阵列扩展和量子比特连接受限于谷分裂能量$E_{VS}$的不可控的横向变化。本研究通过实验绘制了器件中的$E_{VS}$分布，并分析了通过传送带式穿梭传输的单电子自旋的相干性。


<details>
  <summary>Details</summary>
Motivation: 为了实现可扩展、容错的量子计算，需要解决硅量子芯片中量子点阵列扩展和量子比特连接的问题，而这些问题受到谷分裂能量$E_{VS}$横向变化不可控的限制。

Method: 在$^{28}$Si/Si$_{0.7}$Ge$_{0.3}$穿梭器件的$40 	ext{ nm} 	imes 400 	ext{ nm}$区域上绘制了$E_{VS}$的分布，并分析了通过传送带式穿梭传输的单电子自旋的相干性。

Result: 实验观察到$E_{VS}$在$1.5 	ext{ }	ext{	extmu} 	ext{eV}$到$200 	ext{ }	ext{	extmu} 	ext{eV}$范围内变化，主要由SiGe合金无序引起。在低$E_{VS}$区域和自旋-谷共振时，自旋相干性会降低，并且与穿梭速度的依赖关系符合理论预测。快速频繁地穿梭低$E_{VS}$区域会导致相干性增强，这可以用运动学展宽来解释。通过选择避开$E_{VS}$图谱中有问题区域的穿梭轨迹，实现了超过几十微米的传输，相干性仅受限于与静态电子自旋的耦合。

Conclusion: 本研究提供了移动电子自旋量子比特自旋退相干理论的实验验证，并提出了将传送带式量子比特穿梭集成到硅量子芯片中的实用策略。

Abstract: Silicon quantum chips offer a promising path toward scalable, fault-tolerant
quantum computing, with the potential to host millions of qubits. However,
scaling up dense quantum-dot arrays and enabling qubit interconnections through
shuttling are hindered by uncontrolled lateral variations of the valley
splitting energy $E_{VS}$. We map $E_{VS}$ across a $40 \, $nm x $400 \, $nm
region of a $^{28}$Si/Si$_{0.7}$Ge$_{0.3}$ shuttle device and analyze the spin
coherence of a single electron spin transported by conveyor-belt shuttling. We
observe that the $E_{VS}$ varies over a wide range from $1.5 \, \mu$eV to $200
\, \mu$eV and is dominated by SiGe alloy disorder. In regions of low $E_{VS}$
and at spin-valley resonances, spin coherence is reduced and its dependence on
shuttle velocity matches predictions. Rapid and frequent traversal of
low-$E_{VS}$ regions induces a regime of enhanced spin coherence explained by
motional narrowing. By selecting shuttle trajectories that avoid problematic
areas on the $E_{VS}$ map, we achieve transport over tens of microns with
coherence limited only by the coupling to a static electron spin entangled with
the mobile qubit. Our results provide experimental confirmation of the theory
of spin-decoherence of mobile electron spin-qubits and present practical
strategies to integrate conveyor-mode qubit shuttling into silicon quantum
chips.

</details>


### [448] [Enhancement in phase sensitivity in displacement-assisted SU(1,1) interferometer via photon recycling](https://arxiv.org/abs/2510.03783)
*Taj Kumar,Aviral Kumar Pandey,Anand Kumar,Devendra Kumar Mishra*

Main category: quant-ph

TL;DR: 通过引入光子回收技术，我们提出了一种在位移辅助SU(1,1)干涉仪（DSU(1,1)）中增强相位估计的新方法，并在单强度检测（SID）和零拍探测（HD）方案下进行了评估。


<details>
  <summary>Details</summary>
Motivation: 在位移辅助SU(1,1)干涉仪（DSU(1,1)）中，提高相位估计的精度是一个重要的研究方向。

Method: 引入光子回收技术，并在单强度检测（SID）和零拍探测（HD）方案下，对位移辅助SU(1,1)干涉仪（DSU(1,1)）进行分析。

Result: 研究表明，在某些条件下，采用光子回收技术的DSU(1,1)干涉仪比传统干涉仪性能更好，并且这种改进在SID和HD方案下均可实现。计算出的量子Cramé-Rao界（QCRB）表明，该模型接近QCRB。

Conclusion: 光子回收技术为提高相位估计的灵敏度提供了一种有前景的新方法。

Abstract: We propose a novel method for enhancing phase estimation in the
displacement-assisted SU(1,1) (DSU(1,1)) interferometer by incorporating the
photon recycling technique, evaluated under single-intensity detection (SID)
and homodyne detection (HD) schemes. Our analysis showed that utilizing the
photon recycling technique, the photon-recycled DSU(1,1) interferometer
performs better than the conventional DSU(1,1) interferometer for some
conditions. We also showed that this improvement is possible in both SID and HD
schemes. In addition, to discuss the maximum sensitivity achieved by our
proposed model, we have calculated the quantum Cram\'{e}r-Rao bound (QCRB)
within the framework and found that our proposed model approaches the QCRB.
Therefore, we believe that our findings offer a promising new approach to
improve phase sensitivity through photon recycling.

</details>


### [449] [From Qubits to Rhythm: Exploring Quantum Random Walks in Rhythmspaces](https://arxiv.org/abs/2510.03836)
*María Aguado-Yáñez,Karl Jansen,Daniel Gómez-Marín,Sergi Jordà*

Main category: quant-ph

TL;DR: 提出了一种量子计算算法，用于在音乐领域生成节奏模式。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在艺术（特别是音乐）中的应用。

Method: 该算法将量子随机游走轨迹映射到二维的节奏空间，通过设计量子算法、在节奏空间中建立映射、利用经典势场偏置量子随机游走的方向性，并最终将生成的路径进行音效化以产生MIDI鼓模式。

Result: 将量子随机游走扩展到了二维平面，并实现了一种可扩展的、基于量子计算的生成性随机游走算法，可应用于音乐和音频领域。

Conclusion: 该算法为量子计算在音乐生成领域的应用提供了概念验证，并且可以推广到其他多维声音空间和音乐结构。

Abstract: A quantum computing algorithm for rhythm generation is presented, which aims
to expand and explore quantum computing applications in the arts, particularly
in music. The algorithm maps quantum random walk trajectories onto a
rhythmspace -- a 2D interface that interpolates rhythmic patterns. The
methodology consists of three stages. The first stage involves designing
quantum computing algorithms and establishing a mapping between the qubit space
and the rhythmspace. To minimize circuit depth, a decomposition of a 2D quantum
random walk into two 1D quantum random walks is applied. The second stage
focuses on biasing the directionality of quantum random walks by introducing
classical potential fields, adjusting the probability distribution of the wave
function based on the position gradient within these fields. Four potential
fields are implemented: a null potential, a linear field, a Gaussian potential,
and a Gaussian potential under inertial dynamics. The third stage addresses the
sonification of these paths by generating MIDI drum pattern messages and
transmitting them to a Digital Audio Workstation (DAW). This work builds upon
existing literature that applies quantum computing to simpler qubit spaces with
a few positions, extending the formalism to a 2D x-y plane. It serves as a
proof of concept for scalable quantum computing-based generative random walk
algorithms in music and audio applications. Furthermore, the approach is
applicable to generic multidimensional sound spaces, as the algorithms are not
strictly constrained to rhythm generation and can be adapted to different
musical structures.

</details>


### [450] [Ion-Based Characterization of Laser Beam Profiles for Quantum Information Processing](https://arxiv.org/abs/2510.03966)
*Ilyoung Jung,Frank G. Schroer,Philip Richerme*

Main category: quant-ph

TL;DR: 利用离子作为传感器测量激光参数以优化量子门操作。


<details>
  <summary>Details</summary>
Motivation: 在超高真空室内的离子阱系统中，精确测量用于实现单比特和双比特门操作的激光束的尺寸、强度和偏振等关键参数是预测和优化门操作速度及稳定性的核心，但精确测量这些参数具有挑战性。

Method: 利用171Yb+离子的四光子斯塔克效应，作为传感器来测量驱动反向传播的拉曼跃迁的激光束的轮廓、对准和偏振。

Result: 通过单独优化每种激光器的参数，实现了更高速度、对错误更不敏感的拉曼驱动门操作。

Conclusion: 该方法证明了被捕获的离子能够探测其局部环境并提供有用的反馈以提高系统性能。

Abstract: Laser-driven operations are a common approach for engineering one- and
two-qubit gates in trapped-ion arrays. Measuring key parameters of these
lasers, such as beam sizes, intensities, and polarizations, is central to
predicting and optimizing gate speeds and stability. Unfortunately, it is
challenging to accurately measure these properties at the ion location within
an ultra-high vacuum chamber. Here, we demonstrate how the ions themselves may
be used as sensors to directly characterize the laser beams needed for quantum
gate operations. Making use of the four-photon Stark Shift effect in
$^{171}$Yb$^+$ ions, we measure the profiles, alignments, and polarizations of
the lasers driving counter-propagating Raman transitions. We then show that
optimizing the parameters of each laser individually leads to higher-speed
Raman-driven gates with smaller susceptibility to errors. Our approach
demonstrates the capability of trapped ions to probe their local environments
and to provide useful feedback for improving system performance.

</details>


### [451] [Quadratically Shallow Quantum Circuits for Hamiltonian Functions](https://arxiv.org/abs/2510.04059)
*Youngjun Park,Minhyeok Kang,Chae-Yeun Park,Joonsuk Huh*

Main category: quant-ph

TL;DR: 本研究提出了更通用的方法来用低度数多项式近似量子算法所需的高指数多项式函数，包括三角函数，从而实现更浅的量子电路。


<details>
  <summary>Details</summary>
Motivation: 现有的量子算法需要高指数多项式函数来实现更好的收敛性，但目前的量子信号处理（QSP）方法实现的电路深度与其次数成正比，限制了函数的类型。

Method: 提出了一种新的函数近似方法，能够用二次降低次数的多项式来近似线性组合或乘积形式的函数，并将此方法扩展到三角函数（sin和cos）。

Result: 该方法能够实现对更广泛函数（包括三角函数）的近似，并实现量子地面态制备和能量估算中所需函数的近似，其电路深度与多项式次数的平方根成正比。

Conclusion: 本研究提出的方法可以实现更通用的函数近似，并显著减少量子算法中多项式实现的电路深度，为量子计算的应用提供了新的可能。

Abstract: Many quantum algorithms for ground-state preparation and energy estimation
require the implementation of high-degree polynomials of a Hamiltonian to
achieve better convergence rates. Their circuit implementation typically relies
on quantum signal processing (QSP), whose circuit depth is proportional to the
degree of the polynomial. Previous studies exploit the Chebyshev polynomial
approximation, which requires a Chebyshev series of degree
$O(\sqrt{n\ln(1/\delta)})$ for an $n$-degree polynomial, where $\delta$ is the
approximation error. However, the approximation is limited to only a few
functions, including monomials, truncated exponential, Gaussian, and error
functions. In this work, we present the most generalized function approximation
methods for $\delta$-approximating linear combinations or products of
polynomial-approximable functions with quadratically reduced-degree
polynomials. We extend the list of polynomial-approximable functions by showing
that the functions of cosine and sine can also be $\delta$-approximated by
quadratically reduced-degree Laurent polynomials. We demonstrate that various
Hamiltonian functions for quantum ground-state preparation and energy
estimation can be implemented with quadratically shallow circuits.

</details>


### [452] [Non-Markovian protection of states from decay in quasi-PT-symmetric systems](https://arxiv.org/abs/2510.04061)
*T. T. Sergeev,E. S. Andrianov,A. A. Zyablovsky*

Main category: quant-ph

TL;DR: 非马尔可夫过程在准PT对称系统中可以产生无限寿命、损耗保护态，即使在没有增益的情况下。存在一个临界相互作用值，低于该值时所有系统状态都是损耗保护的。该值之上，根据谐振器耦合强度，一个或两个状态可能不受保护。


<details>
  <summary>Details</summary>
Motivation: 研究准PT对称系统中非马尔可夫过程对损耗保护态的影响，以及如何利用这些特性实现基于耗散系统的稳定PT对称器件。

Method: 分析了包含一个与有限环境相互作用的谐振器的准PT对称系统。通过改变谐振器间的耦合强度和谐振器与环境间的相互作用大小，研究了系统在不同损耗保护状态下的行为。

Result: 发现非马尔可夫过程可以使系统产生无限寿命、损耗保护态。存在一个临界相互作用值，低于该值时所有状态均受损耗保护。该值之上，保护态的数量取决于谐振器间的耦合强度。通过调整系统参数，可以实现具有两种、一种或零种损耗保护态的模式。

Conclusion: 非马尔可夫过程在准PT对称系统中可以实现损耗保护态，这使得在纯耗散系统中构建稳定的PT对称器件成为可能。该研究结果可应用于具有单激励的量子系统，并将PT对称的概念扩展到这些系统。

Abstract: We consider a quasi-PT-symmetric system of two resonators, one of which
interacts with a finite-size environment. The interaction with the environment
leads to energy losses in the resonators, and the finite size of the
environment leads to a non-Markovian dynamics of the relaxation process. We
demonstrate that non-Markovian processes in the quasi-PT-symmetric system can
make the states of the system infinitely living, loss-protected states, even in
the absence of gain. There is a critical value of the interaction between the
resonator and the environment below which any state of the system is
loss-protected. When the interaction magnitude is greater than the critical
value, depending on the coupling strength between the resonators, either one or
both states are unprotected. We show that the boundaries of regions with
different numbers of protected states are determined by the relaxation rates in
the quasi-PT-symmetric system, calculated in the Markovian approximation. By
changing the coupling strength between the resonators and the interaction
magnitude between the resonator and the environment, the system switches
between modes with two, one, or no loss-protected states. This makes it
possible to realize stable PT-symmetric devices based on purely dissipative
systems. The obtained results are applicable to quantum systems with single
excitations, allowing the concept of PT symmetry to be extended to such
systems.

</details>


### [453] [Approaching the scaling limit of transport through lattices with dephasing](https://arxiv.org/abs/2510.04062)
*Subhajit Sarkar,Gabriela Wójtowicz,Bartłomiej Gardas,Marek M. Rams,Michael Zwolak*

Main category: quant-ph

TL;DR: 研究具有广义马尔可夫退相干和弛豫的格子的稳态方程，提出了一个高效的求解方法，可以处理多达10^4个格点，显著优于现有研究，并有助于理解超扩散到扩散的相变。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是处理具有广义马尔可夫退相干和弛豫的格子系统，寻找一个高效的求解稳态方程的方法，以克服现有研究在系统规模上的限制，并更好地理解相关现象，如超扩散到扩散的相变。

Method: 研究了具有广义马尔可夫退相干和弛豫的格子的稳态方程。当哈密顿量是二次型的，单粒子关联矩阵即使在存在这两种过程的情况下也能形成一个封闭的方程组。提出了一个高效的解法，可以实现标度极限，例如电流衰减随晶格长度的变化。具体研究了具有长程跳跃和退相干的格子的超扩散到扩散的相变。

Result: 所提出的方法能够处理多达10^4个格点，比先前研究的计算能力提高了10到40倍。这使得能够更精确地提取扩散指数，提高了与理论结果的一致性，并支持了相变的存在。

Conclusion: 该研究提出的方法为处理具有马尔可夫弛豫、噪声和驱动的各种问题（如用于机器学习分类的量子网络和用于传输的扩展储层方法）提供了有用的工具，并能更精确地分析超扩散到扩散的相变。

Abstract: We examine the stationary--state equations for lattices with generalized
Markovian dephasing and relaxation. When the Hamiltonian is quadratic, the
single--particle correlation matrix has a closed system of equations even in
the presence of these two processes. The resulting equations have a vectorized
form related to, but distinct from, Lyapunov's equation. We present an
efficient solution that helps to achieve the scaling limit, e.g., of the
current decay with lattice length. As an example, we study the
super--diffusive--to--diffusive transition in a lattice with long--range
hopping and dephasing. The approach enables calculations with up to $10^4$
sites, representing an increase of $10$ to $40$ times over prior studies. This
enables a more precise extraction of the diffusion exponent, enhances agreement
with theoretical results, and supports the presence of a phase transition.
There is a wide range of problems that have Markovian relaxation, noise, and
driving. They include quantum networks for machine--learning--based
classification and extended reservoir approaches (ERAs) for transport. The
results here will be useful for these classes of problems.

</details>


### [454] [Proofs of quantum memory](https://arxiv.org/abs/2510.04159)
*Minki Hhan,Tomoyuki Morimae,Yasuaki Okinaka,Takashi Yamakawa*

Main category: quant-ph

TL;DR: 该论文引入了量子内存证明（PoQM）的概念，这是一种允许经典验证者检查量子证明者是否拥有指定数量和相干时间的量子内存的协议。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，需要一种经典方法来验证远程量子内存的分配。

Method: 提出了一种名为量子内存证明（PoQM）的新型交互式协议，该协议使用经典通信信道，并基于学习错误（LWE）问题的困难度进行构建。论文提供了两种PoQM的构造：一种是四轮协议，在子指数级LWE困难度下具有可忽略的可靠性错误；另一种是多项式轮协议，在多项式级LWE困难度下具有反多项式可靠性错误。

Result: 成功定义了PoQM，并基于LWE困难度给出了两种构造。此外，证明了PoQM可以蕴含单向难题，并且受限版本的PoQM可以实现量子计算经典通信（QCCC）密钥交换。

Conclusion: PoQM为验证量子内存的分配提供了一种新颖且可行的方法，并具有理论上的重要意义，连接了量子证明、单向难题和安全通信等领域。

Abstract: With the rapid advances in quantum computer architectures and the emerging
prospect of large-scale quantum memory, it is becoming essential to classically
verify that remote devices genuinely allocate the promised quantum memory with
specified number of qubits and coherence time. In this paper, we introduce a
new concept, proofs of quantum memory (PoQM). A PoQM is an interactive protocol
between a classical probabilistic polynomial-time (PPT) verifier and a quantum
polynomial-time (QPT) prover over a classical channel where the verifier can
verify that the prover has possessed a quantum memory with a certain number of
qubits during a specified period of time. PoQM generalize the notion of proofs
of quantumness (PoQ) [Brakerski, Christiano, Mahadev, Vazirani, and Vidick,
JACM 2021]. Our main contributions are a formal definition of PoQM and its
constructions based on hardness of LWE. Specifically, we give two constructions
of PoQM. The first is of a four-round and has negligible soundness error under
subexponential-hardness of LWE. The second is of a polynomial-round and has
inverse-polynomial soundness error under polynomial-hardness of LWE. As a
lowerbound of PoQM, we also show that PoQM imply one-way puzzles. Moreover, a
certain restricted version of PoQM implies quantum computation classical
communication (QCCC) key exchange.

</details>


### [455] [Continuum Model of Isospectrally Patterned Lattices](https://arxiv.org/abs/2510.04518)
*Fotis K. Diakonos,P. Schmelcher*

Main category: quant-ph

TL;DR: Isospectrally patterned lattices (IPL) exhibit localized and extended states. A continuum analogue of IPL is derived, allowing analytical determination of its eigenvalue spectrum and eigenstates, including an expression for the localization length. This model breaks chiral symmetry but preserves state pairing, except for the ground state.


<details>
  <summary>Details</summary>
Motivation: To derive a continuum analogue of isospectrally patterned lattices (IPL) to analytically determine its eigenvalue spectrum, eigenstates, and localization length, and to analyze its symmetry properties compared to a chirally symmetric Hamiltonian.

Method: A continuum analogue of IPL is derived. The eigenvalue spectrum and eigenstates are determined analytically. A symmetry analysis is performed to compare the continuum model with a chirally symmetric Hamiltonian.

Result: An analytical expression for the localization length is obtained, which depends on the ratio of coupling among lattice cells and the phase gradient. The continuum model breaks chiral symmetry but shows pairing of partner states with positive and negative energies, except for the ground state.

Conclusion: The derived continuum model provides analytical insights into the behavior of IPL, including localization length and symmetry properties. The model's deviation from chiral symmetry while maintaining state pairing is a key finding.

Abstract: Isospectrally patterned lattices (IPL) have recently been shown to exhibit a
rich band structure comprising both regimes of localized as well as extended
states. The localized states show a single center localization behaviour with a
characteristic localization length. We derive a continuum analogue of the IPL
which allows us to determine analytically its eigenvalue spectrum and
eigenstates thereby obtaining an expression for the localization length which
involves the ratio of the coupling among the cells of the lattice and the phase
gradient across the lattice. This continuum model breaks chiral symmetry but
still shows a pairing of partner states with positive and negative energies
except for the ground state. We perform a corresponding symmetry analysis which
illuminates the continuum models structure as compared to a corresponding
chirally symmetric Hamiltonian.

</details>


### [456] [Clifford Circuits Augmented Grassmann Matrix Product States](https://arxiv.org/abs/2510.04164)
*Atis Yosprakob,Wei-Lin Tu,Tsuyoshi Okubo,Kouichi Okunishi,Donghoon Kim*

Main category: quant-ph

TL;DR: 结合Clifford电路和张量网络（TN）状态已被证明可以有效减少纠缠，缓解TN模拟中的键维度瓶颈。本研究提出了一种基于Grassmann张量网络的变分TN框架，该框架原生编码了费米子统计并保持了局部性。通过在费米子形式主义中加入局部定义的Clifford电路，我们模拟了紧束缚模型和t-V模型等基准模型。结果表明，Clifford去纠缠消除了可经典模拟的纠缠部分，降低了键维度，提高了基态能量估计的准确性。有趣的是，在Clifford电路上施加天然的Grassmann偶数约束，可以将去纠缠门的数量从720个大幅减少到32个，从而实现了更有效的实现。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种新的张量网络（TN）框架，结合Clifford电路和Grassmann张量网络，以更有效地模拟强关联费米子系统。

Method: 提出并实现了一个基于Grassmann张量网络的变分TN框架，其中嵌入了局部定义的Clifford电路，用于模拟紧束缚模型和t-V模型。

Result: Clifford去纠缠技术有效地降低了键维度，提高了基态能量估计的准确性。将Grassmann偶数约束应用于Clifford电路，可将所需的去纠缠门数量从720个减少到32个，显著提高了效率。

Conclusion: Clifford增强的Grassmann TNs为研究强关联费米子系统（尤其是在更高维度）提供了一种有前景的、可扩展且准确的模拟工具。

Abstract: Recent advances in combining Clifford circuits with tensor network (TN)
states have shown that classically simulable disentanglers can significantly
reduce entanglement, mitigating the bond-dimension bottleneck in TN
simulations. In this work, we develop a variational TN framework based on
Grassmann tensor networks, which natively encode fermionic statistics while
preserving locality. By incorporating locally defined Clifford circuits within
the fermionic formalism, we simulate benchmark models including the
tight-binding and $t$-$V$ models. Our results show that Clifford disentangling
removes the classically simulable component of entanglement, leading to a
reduced bond dimension and improved accuracy in ground-state energy estimates.
Interestingly, imposing the natural Grassmann-evenness constraint on the
Clifford circuits significantly reduces the number of disentangling gates, from
720 to just 32, yielding a far more efficient implementation. These findings
highlight the potential of Clifford-augmented Grassmann TNs as a scalable and
accurate tool for studying strongly correlated fermionic systems, particularly
in higher dimensions.

</details>


### [457] [Quantum computing for heavy-ion physics: near-term status and future prospects](https://arxiv.org/abs/2510.04207)
*João Barata*

Main category: quant-ph

TL;DR: 综合量子信息科学在核物理中的最新进展、挑战和新兴联系，重点介绍了利用新技术研究物质状态、硬探针和自旋相关性。


<details>
  <summary>Details</summary>
Motivation: 讨论将量子信息科学应用于高能核物理问题的最新进展。

Method: 概述了关键发展、开放的挑战以及这些学科之间新兴的联系，并重点介绍了利用新技术研究物质状态、硬探针和自旋相关性的最新成果。

Result: 利用新技术在研究物质状态、硬探针和自旋相关性方面取得了最新成果。

Conclusion: 这项工作总结了在德国法兰克福举行的夸克物质2025会议上发表的相应演讲。

Abstract: We discuss recent advances in applying Quantum Information Science to
problems in high-energy nuclear physics. After outlining key developments, open
challenges, and emerging connections between these disciplines, we highlight
recent results on the study of matter states, hard probes, and spin
correlations using novel quantum technologies. This work summarizes the
corresponding presentation delivered at the Quark Matter 2025 conference in
Frankfurt, Germany.

</details>


### [458] [Quantum Error Correction with Superpositions of Squeezed Fock States](https://arxiv.org/abs/2510.04209)
*Yexiong Zeng,Fernando Quijandría,Clemens Gneiting,Franco Nori*

Main category: quant-ph

TL;DR: 该研究提出了一种基于压缩真空态叠加的玻色子码，可以同时纠正单光子损耗和相位退相干错误，并且在所有压缩水平下码字都保持正交。


<details>
  <summary>Details</summary>
Motivation: 现有的连续变量玻色子码难以同时纠正单光子损耗和相位退相干错误，因为缺少正交的码字和方便的实验制备方案。

Method: 提出一种基于压缩真空态叠加的量子比特编码，该编码的错误纠正能力随压缩水平r呈$\	ext{exp}(-7r)$的比例增长，码字在所有压缩水平下保持正交。Pauli-X算符作为相位空间中的旋转操作，是一种错误透明的门，可防止可纠正的错误在逻辑操作中扩散到码空间之外。

Result: 该编码在单光子损耗和相位退相干方面实现了高精度的错误纠正，即使在中等压缩水平下也是如此。基于此编码，研究人员开发了超越盈亏平衡阈值的量子错误纠正方案，并对所有必需的量子门进行了理论推导。

Conclusion: 所提出的玻色子码为使用连续变量玻色子比特进行量子计算提供了一种有竞争力的替代方案，能够有效纠正单光子损耗和相位退相干错误。

Abstract: Bosonic codes, leveraging infinite-dimensional Hilbert spaces for redundancy,
offer great potential for encoding quantum information. However, the
realization of a practical continuous-variable bosonic code that can
simultaneously correct both single-photon loss and dephasing errors remains
elusive, primarily due to the absence of exactly orthogonal codewords and the
lack of an experiment-friendly state preparation scheme. Here, we propose a
code based on the superposition of squeezed Fock states with an
error-correcting capability that scales as $\propto\exp(-7r)$, where $r$ is the
squeezing level. The codewords remain orthogonal at all squeezing levels. The
Pauli-X operator acts as a rotation in phase space is an error-transparent
gate, preventing correctable errors from propagating outside the code space
during logical operations. In particular, this code achieves high-precision
error correction for both single-photon loss and dephasing, even at moderate
squeezing levels. Building on this code, we develop quantum error correction
schemes that exceed the break-even threshold, supported by analytical
derivations of all necessary quantum gates. Our code offers a competitive
alternative to previous encodings for quantum computation using continuous
bosonic qubits.

</details>


### [459] [Operational Quasiprobability in Quantum Thermodynamics: Work Extraction by Coherence and Non-joint Measurability](https://arxiv.org/abs/2510.04253)
*Jeongwoo Jae,Junghee Ryu,Hoon Ryu*

Main category: quant-ph

TL;DR: 该研究提出了操作拟态概率（OQ）作为功的分布，该分布可以重现Jarzynski等式，并给出与经典定义一致的平均功。研究表明，相干性和非联合可测量性在功的提取中起着关键作用，甚至可以超越经典限制。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于探索相干性和测量不兼容性在功的提取和波动中的作用，并提出一种新的功的分布——操作拟态概率（OQ），以量化这些非经典效应。

Method: 研究方法包括：1. 采用操作拟态概率（OQ）作为功的分布，并验证其与Jarzynski等式和经典平均功的一致性。2. 提出通过端点测量和两点测量方案在实验中实现OQ分布。3. 明确量化相干性对功的波动、平均值和二阶矩的贡献。4. 在两能级系统中，证明非联合可测量性可以增加可提取的功。5. 证明在两能级系统中，Kirkwood-Dirac拟态概率（KDQ）的实部与OQ等价，并讨论其非负性条件。6. 在三能级氮-空穴（NV）中心系统中，比较OQ和KDQ的负性，并探讨其与非经典功的关系。

Result: 研究结果表明：1. OQ分布可以成功地重现Jarzynski等式，并与经典平均功一致。2. 相干性对功的波动、平均值和二阶矩有明确的贡献。3. 在两能级系统中，非联合可测量性可以使可提取的功超过经典测量所施加的界限。4. 在两能级系统中，KDQ的实部和OQ是等价的，当且仅当测量是联合可测的时，它们是非负的。5. 在NV中心系统中，OQ和KDQ表现出不同程度的负性，但可以实现相同的功提取量，这表明负性的量值并不是非经典功的可靠指标。

Conclusion: 相干性和非联合可测量性在增强功的提取方面起着基础性作用。研究结果为理解和利用量子资源（如相干性）来优化功的提取提供了新的视角。

Abstract: We employ the operational quasiprobability (OQ) as a work distribution, which
reproduces the Jarzynski equality and yields the average work consistent with
the classical definition. The OQ distribution can be experimentally implemented
through the end-point measurement and the two-point measurement scheme. Using
this framework, we demonstrate the explicit contribution of coherence to the
fluctuation, the average, and the second moment of work. In a two-level system,
we show that non-joint measurability, a generalized notion of measurement
incompatibility, can increase the amount of extractable work beyond the
classical bound imposed by jointly measurable measurements. We further prove
that the real part of Kirkwood-Dirac quasiprobability (KDQ) and the OQ are
equivalent in two-level systems, and they are nonnegative for binary unbiased
measurements if and only if the measurements are jointly measurable. In a
three-level Nitrogen-vacancy center system, the OQ and the KDQ exhibit
different amounts of negativities while enabling the same work extraction,
implying that the magnitude of negativity is not a faithful indicator of
nonclassical work. These results highlight that coherence and non-joint
measurability play fundamental roles in the enhancement of work.

</details>


### [460] [Turning Down the Noise: Power-Law Decay and Temporal Phase Transitions](https://arxiv.org/abs/2510.04267)
*Lieuwe Bakker,Suvendu Barik,Vladimir Gritsev,Emil A. Yuzbashyan*

Main category: quant-ph

TL;DR: 我们确定了与环境耦合的通用自旋集合的晚期动力学，环境的强度随时间呈1/t衰减。稳态的逼近遵循幂律，这反映了哈密顿动力学和逐渐消失的耗散之间的相互作用。衰减指数随斜升速率的函数关系是非解析的，并呈现出尖点奇点，n点相关函数可以分解为一阶和二阶贡献。我们精确的解锚定了具有显式时间相关耗散的开放量子系统的普适类。


<details>
  <summary>Details</summary>
Motivation: 本文旨在确定具有非均匀展宽（等价于任意塞曼劈裂的量子比特）的通用自旋集合与强度随时间呈1/t衰减的耗散环境耦合时的晚期动力学。

Method: 采用精确解的方法，分析了哈密顿动力学和随时间衰减的耗散环境对自旋集合晚期动力学的影响，并研究了衰减指数随斜升速率的变化以及n点相关函数的性质。

Result: 研究表明，稳态的逼近遵循幂律，衰减指数随斜升速率的函数关系是非解析的，并呈现出尖点奇点，n点赏函数可以分解为一阶和二阶贡献。

Conclusion: 本文提出的精确解为具有显式时间相关耗散的开放量子系统的普适类提供了一个锚点。

Abstract: We determine the late-time dynamics of a generic spin ensemble with
inhomogeneous broadening - equivalently, qubits with arbitrary Zeeman
splittings - coupled to a dissipative environment with strength decreasing as
$1/t$. The approach to the steady state follows a power law, reflecting the
interplay between Hamiltonian dynamics and vanishing dissipation. The decay
exponents vary non-analytically with the ramp rate, exhibiting a cusp
singularity, and $n$-point correlation functions factorize into one- and
two-point contributions. Our exact solution anchors a universality class of
open quantum systems with explicitly time-dependent dissipation.

</details>


### [461] [Higher symmetry breaking and non-reciprocity in a driven-dissipative Dicke model](https://arxiv.org/abs/2510.04288)
*Jacquelyn Ho,Yue-Hui Lu,Tai Xiang,Tsai-Chen Lee,Zhenjie Yan,Dan M. Stamper-Kurn*

Main category: quant-ph

TL;DR: Higher-order discrete symmetry in a Dicke model variant leads to new phases and phenomena in driven-dissipative systems, with potential applications in studying symmetry breaking and non-reciprocity.


<details>
  <summary>Details</summary>
Motivation: Investigate a variant of the Dicke model with higher-order discrete symmetry arising from complex-valued coupling coefficients, and explore its driven-dissipative realization.

Method: Theoretically investigate a driven-dissipative realization of the Dicke model variant using an optomechanical setup with an array of $n$ sub-ensembles driven by a field with stepwise phase advancement. Analyze stationary points and their dynamical stability to identify a phase diagram.

Result: Identified a phase diagram for $n\geq 3$ with three features: a $\mathbb{Z}_n$ ($\mathbb{Z}_{2n}$) symmetry-breaking superradiant phase for even (odd) $n$, a dynamically unstable normal unbroken-symmetry phase due to non-reciprocal forces, and a first-order phase transition between them.

Conclusion: The proposed $n$-phase Dicke model, realized in driven-dissipative systems, provides a testbed for studying high-order symmetry breaking and non-reciprocal interactions in open systems, with potential applications in optomechanical and opto-magnonic settings.

Abstract: Higher symmetries in interacting many-body systems often give rise to new
phases and unexpected dynamical behavior. Here, we theoretically investigate a
variant of the Dicke model with higher-order discrete symmetry, resulting from
complex-valued coupling coefficients between quantum emitters and a bosonic
mode. We propose a driven-dissipative realization of this model focusing on
optomechanical response of a driven atom tweezer array comprised of $n$
sub-ensembles and placed within an optical cavity, with the phase of the
driving field advancing stepwise between sub-ensembles. Examining stationary
points and their dynamical stability, we identify a phase diagram for $n\geq 3$
with three distinctive features: a $\mathbb{Z}_n$ ($\mathbb{Z}_{2n}$)
symmetry-breaking superradiant phase for even (odd) $n$, a normal
unbroken-symmetry phase that is dynamically unstable due to non-reciprocal
forces between emitters, and a first-order phase transition separating these
phases. This $n$-phase Dicke model may be equivalently realized in a variety of
optomechanical or opto-magnonic settings, where it can serve as a testbed for
studying high-order symmetry breaking and non-reciprocal interactions in open
systems.

</details>


### [462] [X-states of a qubit pair of double classicality](https://arxiv.org/abs/2510.04292)
*Arsen Khvedelidze,Dimitar Mladenov,Astghik Torosyan*

Main category: quant-ph

TL;DR: We describe a special class of 2-qubit states that are both separable and have positive semidefinite Wigner functions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to describe a special class of 2-qubit states.

Method: The method involves describing these states.

Result: The result is the description of these states, which are simultaneously separable and have positive semidefinite Wigner functions.

Conclusion: The paper concludes by describing this special class of 2-qubit states.

Abstract: A special class of states of 2-qubits which are simultaneously separable and
have positive semidefinite Wigner functions is described.

</details>


### [463] [Filtered Quantum Phase Estimation](https://arxiv.org/abs/2510.04294)
*Gwonhak Lee,Minhyeok Kang,Jungsoo Hong,Stepan Fomichev,Joonsuk Huh*

Main category: quant-ph

TL;DR: 该研究提出了一种统一的滤波状态制备框架，用于提高量子算法中初始状态与目标本征态的重叠度，并在此基础上提出了一种滤波式量子相位估计算法（FQPE），通过数值实验证明其在费米-哈伯德模型上可将运行时间缩短两个数量级以上。


<details>
  <summary>Details</summary>
Motivation: 准确的状态制备是量子算法（尤其是基态能量估计算法）的关键瓶颈，即使在容错量子计算中，制备具有足够重叠度的量子态仍然是一个挑战。

Method: 提出了一种统一的滤波状态制备框架，通过谱滤波增强给定输入态的重叠度。该框架包括滤波器的多项式和三角函数实现，并引入了受信号处理启发的滤波器（如高斯滤波器和Krylov子空间滤波器），利用低秩投影自适应地抑制激发态的贡献。在此框架下，开发了一种滤波式量子相位估计算法（FQPE）。

Result: 提出的FQPE算法通过重叠度放大，将标准QPE中对初始重叠度不利的依赖性进行了缓解。在费米-哈伯德模型的数值实验中，FQPE在高精度体制下将总运行时间缩短了两个数量级以上，重叠度放大了100倍以上。

Conclusion: 该研究提出的滤波状态制备框架和FQPE算法能够显著提高量子算法的效率，尤其在基态能量估计算法中，通过增强状态重叠度有效降低了计算成本和运行时间。

Abstract: Accurate state preparation is a critical bottleneck in many quantum
algorithms, particularly those for ground state energy estimation. Even in
fault-tolerant quantum computing, preparing a quantum state with sufficient
overlap to the desired eigenstate remains a major challenge. To address this,
we develop a unified framework for filtered-state preparation that enhances the
overlap of a given input state through spectral filtering. This framework
encompasses the polynomial and trigonometric realizations of filters, allowing
a transparent analysis of the trade-offs between overlap amplification and
preparation cost. As examples, we introduce signal-processing-inspired filters,
such as Gaussian filters and Krylov subspace-based filters, that adaptively
suppress excited-state contributions using low-rank projections. Within this
framework, we further develop a filtered variant of QPE (FQPE) that mitigates
the unfavorable dependence on the initial overlap present in standard QPE.
Numerical experiments on Fermi-Hubbard models show that FQPE reduces the total
runtime by more than two orders of magnitude in the high-precision regime, with
overlap amplification exceeding a factor of one hundred.

</details>


### [464] [Time-resolved characterization of pulsed squeezed light from a strongly driven silicon nitride microresonator](https://arxiv.org/abs/2510.04300)
*Emanuele Brusaschi,Marco Liscidini,Matteo Galli,Daniele Bajoni,Massimo Borghi*

Main category: quant-ph

TL;DR: 研究了脉冲泵浦下硅氮化微腔产生压缩光的问题，并提出了一种误差校正策略。


<details>
  <summary>Details</summary>
Motivation: 高参量增益下，自相位调制、交叉相位调制和时间排序校正等效应会降低压缩光源的性能。因此，需要研究并优化脉冲泵浦下硅氮化微腔产生压缩光的效果。

Method: 通过实验研究了平均光子数、一阶和二阶关联随脉冲能量、频率失谐和脉冲持续时间的变化。分析了多光子发射引入的误差，并提出了一种基于时间分辨多光子事件边缘分布的误差校正策略。

Result: 在高达16光子/脉冲的参数增益下，研究了压缩光的演化。提出并演示了一种误差校正策略，可优化增益和时间模式结构。

Conclusion: 提出的误差校正策略可以优化压缩光在微腔中的增益和时间模式结构，并阐明了高增益下的物理机制和局限性。

Abstract: Silicon nitride microresonators driven by strong pump pulses can generate
squeezed light in a dominant spectral-temporal mode, a central resource for
continuous-variable quantum computation. In the high parametric gain regime,
several effects, including self- and cross-phase modulation as well as
time-ordering corrections, become significant and can degrade source
performance. In this work, we comprehensively investigate the generation of
squeezed light from a silicon nitride resonator under pulsed pumping, spanning
from low to high parametric gain up to 16 photons/pulse. We experimentally
study how the average photon number and the first- and second- order
correlations of the squeezed marginal modes evolve with increasing pulse
energy, across various frequency detunings and pulse durations. Furthermore, we
analyze the errors introduced by multi-pair emissions in estimating the joint
temporal intensity via time-resolved coincidence measurements. We propose and
demonstrate an error-correction strategy based on the marginal distributions of
time-resolved multi-photon events. Our results provide a practical strategy for
optimizing the gain and the temporal mode structure of pulsed squeezed light
sources in microresonators, elucidating the physical mechanisms and limitations
that govern source performance in the high gain regime.

</details>


### [465] [Atomtronic routing of dipolar bosons in a four-well star potential](https://arxiv.org/abs/2510.04395)
*Karin Wittmann W.,Leandro H. Ymai,Genessi Sá Neto,Angela Foerster*

Main category: quant-ph

TL;DR: 利用可积模型和可调谐原子路由协议，对偶极玻色子在星形四阱势中的演化进行精确控制和预测，为量子技术发展提供新途径。


<details>
  <summary>Details</summary>
Motivation: 实现对量子态演化精确控制和预测是推进量子技术发展的基本要求。

Method: 开发基于可积模型的偶极玻色子在星形四阱势中的可调谐原子路由协议，识别出可解析处理的原子数谐波动力学机制，并通过调整系统参数实现对原子数动力学的精确操控。

Result: 实现了对原子数动力学的三种独立控制模式：通过场强变化进行频率调谐，通过场空间位移进行方向切换，以及通过改变持续时间进行幅度调制。这两种配置分别模拟了1:2解复用器和2:1复用器。

Conclusion: 提出的控制机制有望为量子器件的设计和发展做出贡献。

Abstract: The ability to precisely control and predict the evolution of quantum states
is a fundamental requirement for advancing quantum technologies. Here, we
develop tunable atomic routing protocols based on an integrable model of
dipolar bosons confined in a four-well potential with a star-shaped
configuration. By adjusting the system parameters, we identify a harmonic
dynamical regime of the atomic population that can be treated analytically,
providing a complete description of the system's behaviour for precise
manipulation. We demonstrate three independent modes of control over the atomic
population dynamics under the action of an external field: frequency tuning via
variation in the field intensity, directional switching via spatial
displacement of the field, and amplitude modulation by varying its duration.
These modes operate under two distinct configurations: one source and two
drains, and, in reverse order, two sources and one drain. These cases emulate
an atomic 1:2 demultiplexer and 2:1 multiplexer, respectively. Our results may
contribute to the development of control mechanisms in the design of quantum
devices.

</details>


### [466] [Quantum precomputation: parallelizing cascade circuits and the Moore-Nilsson conjecture is false](https://arxiv.org/abs/2510.04411)
*Adam Bene Watts,Charles R. Chen,J. William Helton,Joseph Slote*

Main category: quant-ph

TL;DR: 摩尔-尼尔森猜想被否定，量子算法中的并行化限制比先前认为的要小，可以达到O(log n)深度。


<details>
  <summary>Details</summary>
Motivation: 解决摩尔-尼尔森猜想，该猜想认为受控酉“阶梯”电路的最小深度为Ω(n)，这与经典并行性形成对比，并可能证明量子版的NC ≠ P猜想。

Method: 通过引入一种量子分块预计算技术（类似于经典的“四俄罗斯人方法”），将所有阶梯电路压缩到O(log n)的深度，对于2D连通性限制的电路，则压缩到O(sqrt(n))深度。该技术也被应用于更一般的“级联”电路。

Result: 证明了摩尔-尼尔森猜想是错误的，展示了所有阶梯电路都可以被压缩到O(log n)深度。对于2D连通性限制的电路，得到了O(sqrt(n))的最佳深度压缩。此外，还对更一般的级联电路取得了进展。

Conclusion: 量子算法中的并行化限制并不像摩尔-尼尔森猜想所暗示的那么严格，可以通过新的技术（如量子分块预计算）显著提高并行性。

Abstract: Parallelization is a major challenge in quantum algorithms due to physical
constraints like no-cloning. This is vividly illustrated by the conjecture of
Moore and Nilsson from their seminal work on quantum circuit complexity [MN01,
announced 1998]: unitaries of a deceptively simple form--controlled-unitary
"staircases"--require circuits of minimum depth $\Omega(n)$. If true, this
lower bound would represent a major break from classical parallelism and prove
a quantum-native analogue of the famous NC $\neq$ P conjecture.
  In this work we settle the Moore-Nilsson conjecture in the negative by
compressing all circuits in the class to depth $O(\log n)$, which is the best
possible. The parallelizations are exact, ancilla-free, and can be computed in
poly($n$) time. We also consider circuits restricted to 2D connectivity, for
which we derive compressions of optimal depth $O(\sqrt{n})$.
  More generally, we make progress on the project of quantum parallelization by
introducing a quantum blockwise precomputation technique somewhat analogous to
the method of Arlazarov, Dini\v{c}, Kronrod, and Farad\v{z}ev [Arl+70] in
classical dynamic programming, often called the "Four-Russians method." We
apply this technique to more-general "cascade" circuits as well, obtaining for
example polynomial depth reductions for staircases of controlled
$\log(n)$-qubit unitaries.

</details>


### [467] [Quantum walk search based edge detection of images](https://arxiv.org/abs/2510.04420)
*Pulak Ranjan Giri,Rei Sato,Kazuhiro Saito*

Main category: quant-ph

TL;DR: 利用改进后的量子行走搜索算法进行图像边缘检测，该算法在检测精度和速度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索将离散时间量子行走搜索算法应用于图像边缘检测，并证明其有效性。

Method: 提出一种新颖的量子行走搜索算法，用于图像边缘检测，并在Qiskit中实现和测试。

Result: 量子行走搜索算法在图像边缘检测中表现出高成功率和二次加速，优于经典方法。

Conclusion: 所提出的量子行走搜索算法在图像边缘检测任务中具有显著优势，在精度和速度上均超越现有方法。

Abstract: Quantum walk has emerged as an essential tool for searching marked vertices
on various graphs. Recent advances in the discrete-time quantum walk search
algorithm have enabled it to effectively handle multiple marked vertices,
expanding its range of applications further. In this article, we propose a
novel application of this advanced quantum walk search algorithm for the edge
detection of images\textemdash a critical task in digital image processing.
Given the probabilistic nature of quantum computing, obtaining measurement
result with a high success probability is essential alongside faster
computation time. Our quantum walk search algorithm demonstrates a high success
probability in detecting the image edges compared to the existing quantum edge
detection methods and outperforms classical edge detection methods with a
quadratically faster speed. A small Qiskit circuit implementation of our method
using a one-dimensional quantum walk search has been executed in Qiskit's
$qasm\_simulator$ and $ibm\_sydney(fake)$ device.

</details>


### [468] [Multi-target quantum walk search on Johnson graph](https://arxiv.org/abs/2510.04424)
*Pulak Ranjan Giri*

Main category: quant-ph

TL;DR: 本文利用改进的硬币算子$\"$Cg$\"$ 和懒惰量子行走来解决 Johnson 图 $J(n,k)$ 上的多目标搜索问题，并与其他硬币算子进行了性能比较。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在 Johnson 图上的单目标搜索，而忽略了多目标搜索的问题。

Method: 利用懒惰量子行走和改进的硬币算子 $\"Cg\"$ 来寻找 Johnson 图 $J(n,k)$ 上的多个目标顶点。

Result: 与其他常用的硬币算子相比，$\"Cg\"$ 硬币算子在搜索多个目标顶点方面表现出非常高的成功概率，并在所有讨论的场景中都优于其他算子。

Conclusion: 改进的硬币算子 $\"Cg\"$ 和懒惰量子行走是解决 Johnson 图 $J(n,k)$ 上多目标搜索问题的有效方法，并且在性能上优于其他常用算子。

Abstract: The discrete-time quantum walk on the Johnson graph $J(n,k)$ is a useful tool
for performing target vertex searches with high success probability. This graph
is defined by $n$ distinct elements, with vertices being all the
\(\binom{n}{k}\) $k$-element subsets and two vertices are connected by an edge
if they differ exactly by one element. However, most works in the literature
focus solely on the search for a single target vertex on the Johnson graph. In
this article, we utilize lackadaisical quantum walk--a form of discrete-time
coined quantum walk with a wighted self-loop at each vertex of the graph--along
with our recently proposed modified coin operator, $\mathcal{C}_g$, to find
multiple target vertices on the Johnson graph $J(n,k)$ for various values of
$k$. Additionally, a comparison based on the numerical analysis of the
performance of the $\mathcal{C}_g$ coin operator in searching for multiple
target vertices on the Johnson graph, against various other frequently used
coin operators by the discrete-time quantum walk search algorithms, shows that
only $\mathcal{C}_g$ coin can search for multiple target vertices with a very
high success probability in all the scenarios discussed in this article,
outperforming other widely used coin operators in the literature.

</details>


### [469] [FewBodyToolkit.jl: a Julia package for solving quantum few-body problems](https://arxiv.org/abs/2510.04447)
*Lucas Happ*

Main category: quant-ph

TL;DR: FewBodyToolkit.jl是一个Julia包，用于模拟量子多体系统，支持任意成对相互作用的二维和三维两体和三体系统，并能计算束缚态和共振态。


<details>
  <summary>Details</summary>
Motivation: 提供一个易于使用的工具来研究量子少体物理系统，填补单体和多体系统之间的空白。

Method: 使用高斯展开法实现，支持二维和三维系统，以及任意成对相互作用。

Result: 该软件包可以计算束缚态和共振态，并通过基准测试和研究示例展示了其能力。

Conclusion: FewBodyToolkit.jl是一个功能齐全的软件包，适用于研究、教学、基准测试和方法开发。

Abstract: Few-body physics explores quantum systems of a small number of particles,
bridging the gap between single-particle and many-body regimes. To provide an
accessible tool for such studies, we present FewBodyToolkit.jl, a Julia package
for quantum few-body simulations. The package supports general two- and
three-body systems in various spatial dimensions with arbitrary
pair-interactions, and allows to calculate bound and resonant states. The
implementation is based on the well-established Gaussian expansion method and
we illustrate the package's capabilities through benchmarks and research
examples. The package comes with documentation and examples, making it useful
for research, teaching, benchmarking, and method development.

</details>


### [470] [Quantum Cryptography and Hardness of Non-Collapsing Measurements](https://arxiv.org/abs/2510.04448)
*Tomoyuki Morimae,Yuki Shirakawa,Takashi Yamakawa*

Main category: quant-ph

TL;DR: 该论文基于非坍缩测量的困难性，提出了新的复杂性类 SampPDQP，并证明了其平均情况下的困难性足以支撑单向难题（OWPuzzs）的存在，从而为量子密码学中的一个重要原语奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 在量子密码学领域，单向难题（OWPuzzs）是一个基础性原语，但其存在依赖于一些尚未完全证实的假设。本研究旨在寻找更可靠的假设来支撑 OWPuzzs 的存在，并探索其潜在应用。

Method: 作者首先引入了一个新的复杂性类 SampPDQP，该类定义为能够通过单个非坍缩测量查询的经典多项式时间算法所能解决的采样问题。随后，他们证明了如果 SampPDQP 在平均情况下是困难的，那么 OWPuzzs 就存在。此外，他们还引入了分布碰撞困难性难题 (dCRPuzzs)，并证明 dCRPuzzs 蕴含了 SampPDQP 的平均情况困难性。最后，他们证明了特定的两种消息承诺和一次性签名方案可以蕴含 dCRPuzzs。

Result: 本研究的主要结果是：1. 证明了 SampPDQP 的平均情况困难性蕴含了 OWPuzzs 的存在。2. 证明了分布碰撞困难性难题 (dCRPuzzs) 蕴含了 SampPDQP 的平均情况困难性。3. 证明了特定的两种消息承诺和一次性签名方案可以蕴含 dCRPuzzs。

Conclusion: 本研究成功地将单向难题（OWPuzzs）的构建基础从不确定的假设转移到了“非坍缩测量”这一（尽管目前被认为是不可实现的）但理论上更易于分析的困难性假设上。通过引入 SampPDQP 复杂性类和 dCRPuzzs 原语，并建立它们之间的联系，为未来在量子密码学领域基于更实际的假设来构建 OWPuzzs 和其他相关原语提供了理论基础和方向。

Abstract: One-way puzzles (OWPuzzs) introduced by Khurana and Tomer [STOC 2024] are a
natural quantum analogue of one-way functions (OWFs), and one of the most
fundamental primitives in ''Microcrypt'' where OWFs do not exist but quantum
cryptography is possible. OWPuzzs are implied by almost all quantum
cryptographic primitives, and imply several important applications such as
non-interactive commitments and multi-party computations. A significant goal in
the field of quantum cryptography is to base OWPuzzs on plausible assumptions
that will not imply OWFs. In this paper, we base OWPuzzs on hardness of
non-collapsing measurements. To that end, we introduce a new complexity class,
$\mathbf{SampPDQP}$, which is a sampling version of the decision class
$\mathbf{PDQP}$ introduced in [Aaronson, Bouland, Fitzsimons, and Lee, ITCS
2016]. We show that if $\mathbf{SampPDQP}$ is hard on average for quantum
polynomial time, then OWPuzzs exist. $\mathbf{SampPDQP}$ is the class of
sampling problems that can be solved by a classical polynomial-time algorithm
that can make a single query to a non-collapsing measurement oracle, which is a
''magical'' oracle that can sample measurement results on quantum states
without collapsing the states. Such non-collapsing measurements are highly
unphysical operations that should be hard to realize in quantum
polynomial-time. We also study upperbounds of the hardness of
$\mathbf{SampPDQP}$. We introduce a new primitive, distributional
collision-resistant puzzles (dCRPuzzs), which are a natural quantum analogue of
distributional collision-resistant hashing [Dubrov and Ishai, STOC 2006]. We
show that dCRPuzzs imply average-case hardness of $\mathbf{SampPDQP}$ (and
therefore OWPuzzs as well). We also show that two-message
honest-statistically-hiding commitments with classical communication and
one-shot signatures [Amos, Georgiou, Kiayias, Zhandry, STOC 2020] imply
dCRPuzzs.

</details>


### [471] [The average determinant of the reduced density matrices for each qubit as a global entanglement measure](https://arxiv.org/abs/2510.04449)
*Dafa Li*

Main category: quant-ph

TL;DR: 该论文提出了一种新的全局纠缠度量方法，并证明了它与 Meyer 和 Wallach 的方法在代数上是等价的。


<details>
  <summary>Details</summary>
Motivation: 为了寻找新的全局纠缠度量方法，并探索其性质。

Method: 提出使用每个量子比特的约化密度矩阵的平均行列式作为全局纠缠度量，并推导了其分解律。

Result: 证明了该度量方法与 Meyer 和 Wallach 的全局纠缠度量在代数上是等价的，它度量了平均混合度和平均 1-tangle，并指出对于 n 量子比特的 W 态，这些度量在大 n 情况下趋于零。

Conclusion: 该论文提出的平均行列式度量方法是一种有效的全局纠缠度量，并揭示了其与混合度和 1-tangle 的关系。

Abstract: Meyer and Wallach proposed the average norm squared of the wedge products of
the projections of a state onto the single qubit subspaces as the global
entanglement measure. Meyer and Wallach's global entanglement has the
significant impact. We propose the average determinant of reduced density
matrices for each qubit as a global entanglement measure. We show that these
two measures are the same algebraically though they use different concepts. By
means of the properties of reduced density matrices, we can explore the present
measure. We propose a decomposition law for the present measure, demonstrate
that the present measure just measures the average mixedness for each qubit and
the average 1-tangle, and indicate that for n-qubit W state, the average
mixedness for each qubit and 1-tangle almost vanish for large number of qubits.
We also point out that for two quits, the present measure is just the square of
the concurrence while for three qubits, the present measure is or greater than
3-tangle.

</details>


### [472] [Lovász Meets Lieb-Schultz-Mattis: Complexity in Approximate Quantum Error Correction](https://arxiv.org/abs/2510.04453)
*Jinmin Yi,Ruizhi Liu,Zhi Li*

Main category: quant-ph

TL;DR: AQEC在量子信息处理和多体纠缠探测方面有广泛应用，但其纠错能力与量子态制备的难度之间存在根本性的矛盾。通过应用Lovasz局部引理，我们揭示了局部不可区分性和线路复杂度之间的权衡关系，即正交短程纠缠态可以通过局部算符区分。


<details>
  <summary>Details</summary>
Motivation: 探索AQEC的纠错能力与量子态制备难度之间的关系，并为量子线路复杂度研究提供新工具。

Method: 应用Lovasz局部引理，建立局部不可区分性和线路复杂度之间的权衡关系。

Result: 证明了正交短程纠缠态可以通过局部算符区分。推导了AQEC码具有横向逻辑门的更强复杂度约束，并为W态制备设定了强的复杂度下界。

Conclusion: AQEC的纠错能力与其量子态制备的难度之间存在根本性的权衡。所提出的框架为研究量子线路复杂度，特别是AQEC码和W态制备提供了新的视角和工具。

Abstract: Approximate quantum error correction (AQEC) provides a versatile framework
for both quantum information processing and probing many-body entanglement. We
reveal a fundamental tension between the error-correcting power of an AQEC and
the hardness of code state preparation. More precisely, through a novel
application of the Lov\'asz local lemma, we establish a fundamental trade-off
between local indistinguishability and circuit complexity, showing that
orthogonal short-range entangled states must be distinguishable via a local
operator. These results offer a powerful tool for exploring quantum circuit
complexity across diverse settings. As applications, we derive stronger
constraints on the complexity of AQEC codes with transversal logical gates and
establish strong complexity lower bounds for W state preparation. Our framework
also provides a novel perspective for systems with Lieb-Schultz-Mattis type
constraints.

</details>


### [473] [Robust iSWAP gates for semiconductor spin qubits with local driving](https://arxiv.org/abs/2510.04462)
*Qi-Pei Liu,Zheng-Yuan Xue*

Main category: quant-ph

TL;DR: 提出了一种适用于半导体自旋量子比特的鲁棒iSWAP门协议，以提高两比特门的保真度。


<details>
  <summary>Details</summary>
Motivation: 可扩展量子计算需要高保真度的两比特门，但退相干和控制错误会降低量子门的操作质量。

Method: 该方案使用对常规交换耦合自旋量子比特进行局部微波驱动，结合了连续动力学解耦技术来抑制低频噪声，并克服了交换相互作用交流调制带来的控制困难。此外，还开发了复合脉冲序列来消除驱动强度限制，并采用了动力学校正方法来提供对微波幅度误差的一阶免疫力。

Result: 数值模拟表明，该方案在现有实验条件下可以实现高于容错阈值的保真度。

Conclusion: 该方案为构建实用的量子处理器提供了一个关键的组成部分。

Abstract: Scalable quantum computation demands high-fidelity two-qubit gates. However,
decoherence and control errors are inevitable, which can decrease the quality
of implemented quantum operations. We propose a robust iSWAP gate protocol for
semiconductor spin qubits, which is a promising platform for scalable quantum
computing. Our scheme uses only local microwave drives on conventional
exchange-coupled spin qubits. This approach simultaneously addresses two
critical challenges on semiconductor quantum computing: it suppresses
low-frequency noise via continuous dynamical decoupling, and it circumvents the
control difficulties associated with the ac modulation of the exchange
interaction. We further develop a composite pulse sequence to remove
drive-strength constraints and a dynamically corrected method to provide
first-order immunity to microwave amplitude errors.Numerical simulations
confirm that our scheme can achieve fidelity above the fault-tolerance
threshold under current experimental conditions, offering a building block for
practical quantum processors.

</details>


### [474] [Black-Box Separation Between Pseudorandom Unitaries, Pseudorandom Isometries, and Pseudorandom Function-Like States](https://arxiv.org/abs/2510.04486)
*Aditya Gulati,Yao-Ting Lin,Tomoyuki Morimae,Shogo Yamada*

Main category: quant-ph

TL;DR: 量子密码学中，伪随机单元（PRUs）、伪随机等距（PRIs）和伪随机函数类状态生成器（PRFSGs）是伪随机函数（PRFs）的量子类似物。本文通过排除它们之间的黑盒构造来部分解决它们是否等价的开放性问题，特别是从 PRFSGs 到 PRUs/PRIs，以及从长 stretch PRIs 到短 stretch PRIs 的构造。


<details>
  <summary>Details</summary>
Motivation: 在量子密码学中，伪随机函数（PRFs）的量子类似物（如 PRUs、PRIs、PRFSGs）被认为是实现各种应用的关键。然而，这些量子类似物之间是否存在等价性，即它们之间是否可以相互构造，仍然是一个重要的未解决问题。

Method: 本文通过构造一个酉预言机来分离 PRFSGs、PRIs 和 PRUs，从而排除它们之间的黑盒构造。具体来说，本文构造了一个基于量子奇异值变换的敌手，用于在黑盒模型中区分这些对象。

Result: 1. 无法从 PRFSGs 黑盒构造出具有 $O(	ext{log}	ext{ }	ext{λ})$ 辅助比特的 PRUs。 2. 无法从 PRFSGs 黑盒构造出具有 $O(	ext{log}	ext{ }	ext{λ})$ 拉伸的 PRIs。 3. 无法从具有 $	ext{Ω(λ)}$ 拉伸的 PRIs 黑盒构造出具有 $O(	ext{log}	ext{ }	ext{λ})$ 拉伸的 PRIs。

Conclusion: 本文在黑盒模型下排除了伪随机函数（PRFs）的某些量子类似物（PRUs、PRIs、PRFSGs）之间的构造可能性，表明它们之间并非完全等价。此外，本文提出的基于量子奇异值变换的敌手也可能有助于解决量子密码学中的其他预言机分离问题。

Abstract: Pseudorandom functions (PRFs) are one of the most fundamental primitives in
classical cryptography. On the other hand, in quantum cryptography, it is
possible that PRFs do not exist but their quantum analogues could exist, and
still enabling many applications including SKE, MACs, commitments, multiparty
computations, and more. Pseudorandom unitaries (PRUs) [Ji, Liu, Song, Crypto
2018], pseudorandom isometries (PRIs) [Ananth, Gulati, Kaleoglu, Lin, Eurocrypt
2024], and pseudorandom function-like state generators (PRFSGs) [Ananth, Qian,
Yuen, Crypto 2022] are major quantum analogs of PRFs. PRUs imply PRIs, and PRIs
imply PRFSGs, but the converse implications remain unknown. An important open
question is whether these natural quantum analogues of PRFs are equivalent. In
this paper, we partially resolve this question by ruling out black-box
constructions of them:
  1. There are no black-box constructions of $O(\log\lambda)$-ancilla PRUs from
PRFSGs. 2. There are no black-box constructions of $O(\log\lambda)$-ancilla
PRIs with $O(\log\lambda)$ stretch from PRFSGs. 3. There are no black-box
constructions of $O(\log\lambda)$-ancilla PRIs with $O(\log\lambda)$ stretch
from PRIs with $\Omega(\lambda)$ stretch.
  Here, $O(\log\lambda)$-ancilla means that the generation algorithm uses at
most $O(\log\lambda)$ ancilla qubits. PRIs with $s(\lambda)$ stretch is PRIs
mapping $\lambda$ qubits to $\lambda+s(\lambda)$ qubits. To rule out the above
black-box constructions, we construct a unitary oracle that separates them. For
the separations, we construct an adversary based on the quantum singular value
transformation, which would be independent of interest and should be useful for
other oracle separations in quantum cryptography.

</details>


### [475] [Comparative Analysis on Two Quantum Algorithms for Solving the Heat Equation](https://arxiv.org/abs/2510.04511)
*Samantha Tseng,Abhyudaya Chouhan,Dominic Cupidon*

Main category: quant-ph

TL;DR: 当前尚无最优的量子算法解决偏微分方程，但已有多种方法各有利弊。本研究调查了2020年后出现的新型量子算法，并选取其中两种求解一维热方程，通过分析（包括经典提取成本）来评估它们的精度和运行效率，找出各自的优缺点。


<details>
  <summary>Details</summary>
Motivation: 现有量子算法在求解偏微分方程方面尚无最优解，且已有相关方法的比较研究，但自那时以来，新的研究成果不断涌现。因此，有必要对2020年后出现的新型量子算法进行调研，并评估其在求解偏微分方程方面的进展。

Method: 对2020年后出现的新型量子算法进行调研，选取其中两种求解一维热方程，并分析其精度、运行时间效率以及经典提取成本。

Result: 通过对两种量子算法在求解一维热方程中的性能进行分析，可以了解它们在精度和运行时间效率方面的优势和劣势，以及在经典提取成本方面的考量。

Conclusion: 通过对两种新型量子算法在求解一维热方程中的性能进行分析，为未来开发更优的量子算法提供了参考。

Abstract: As of now, an optimal quantum algorithm solving partial differential
equations eludes us. There are several different methods, each with their own
strengths and weaknesses. In past years comparisons of these existing methods
have been made, but new work has emerged since then. Therefore, we conducted a
survey on quantum methods developed post-2020, applying two such solvers to the
heat equation in one spatial dimension. By analyzing their performance
(including the cost of classical extraction), we explore their precision and
runtime efficiency advancements between the two, identifying advantages and
considerations.

</details>


### [476] [Quantum generative model on bicycle-sharing system and an application](https://arxiv.org/abs/2510.04512)
*Fumio Nemoto,Nobuyuki Koike,Daichi Sato,Yuuta Kawaai,Masayuki Ohzeki*

Main category: quant-ph

TL;DR: 量子机器学习模型通过分析时间序列数据来解决自行车共享系统中的自行车短缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决自行车共享系统中因高峰通勤需求导致的特定区域和时间内的自行车短缺问题。

Method: 使用新颖的量子机器学习模型，通过拟合量子时间演本来分析时间序列数据，以捕捉自行车数量的实际趋势并识别不同站点之间的相关性。利用训练好的模型，模拟了在高峰需求站点主动增加自行车对整个系统租赁数量的影响。

Result: 通过模拟，评估了主动增加自行车对系统整体租赁数量的影响，并指出该方法具有广泛的工业应用前景。

Conclusion: 该研究提出了一种利用量子机器学习模型来优化自行车共享系统运营的有效方法，有望解决自行车短缺问题并提高系统效率。

Abstract: Recently, bicycle-sharing systems have been implemented in numerous cities,
becoming integral to daily life. However, a prevalent issue arises when
intensive commuting demand leads to bicycle shortages in specific areas and at
particular times. To address this challenge, we employ a novel quantum machine
learning model that analyzes time series data by fitting quantum time evolution
to observed sequences. This model enables us to capture actual trends in
bicycle counts at individual ports and identify correlations between different
ports. Utilizing the trained model, we simulate the impact of proactively
adding bicycles to high-demand ports on the overall rental number across the
system. Given that the core of this method lies in a Monte Carlo simulation, it
is anticipated to have a wide range of industrial applications.

</details>


### [477] [Fast surgery for quantum LDPC codes](https://arxiv.org/abs/2510.04521)
*Nouédyn Baspin,Lucas Berent,Lawrence Z. Cohen*

Main category: quant-ph

TL;DR: 量子LDPC码在逻辑操作方面引入了一种新的通用手术方案，该方案使用恒定的综合轮数，并证明了其在减少空间和时间开销方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的量子LDPC码在进行逻辑操作时，其距离相关的时域开销很大，而本文旨在解决这一问题。

Method: 提出了一种对量子LDPC码进行通用手术的新方案，该方案利用恒定的综合轮数，并通过将基本码和链复形结合来构建合并码。

Result: 所提出的方案在多周期码上进行了演示，并在现象噪声模型下进行了评估，结果表明其性能与标准的通用手术相当，但时间开销更小。

Conclusion: 本研究提出的快速手术方案为实现高容错量子计算提供了新的途径，有望大幅降低量子LDPC码在空间和时间上的开销。

Abstract: Quantum LDPC codes promise significant reductions in physical qubit overhead
compared with topological codes. However, many existing constructions for
performing logical operations come with distance-dependent temporal overheads.
We introduce a scheme for performing generalized surgery on quantum LDPC codes
using a constant number of rounds of syndrome measurement. The merged code in
our scheme is constructed by taking the total complex of the base code and a
suitably chosen homomorphic chain complex. We demonstrate the applicability of
our scheme on an example multi-cycle code and assess the performance under a
phenomenological noise model, showing that fast surgery performs comparably to
standard generalized surgery with multiple rounds. Our results pave the way
towards fault-tolerant quantum computing with LDPC codes with both low spatial
and temporal overheads.

</details>


### [478] [Subsystem many-hypercube codes: High-rate concatenated codes with low-weight syndrome measurements](https://arxiv.org/abs/2510.04526)
*Ryota Nakai,Hayato Goto*

Main category: quant-ph

TL;DR: 新提出的子系统MHC码具有恒定的测量综合症权重，并且可以保持高编码速率，其性能优于块MAP和神经网络解码器。


<details>
  <summary>Details</summary>
Motivation: 为了解决MHC码的综合症权重随着 the number of concatenation levels 呈指数增长的问题，需要开发一种新的量子纠错码。

Method: 提出基于MHC码的子系统码，特别是最小的子系统MHC码，即源自 the concatenated [[4,2,2]] 码的子系统码。开发了块MAP和神经网络解码器。

Result: 所提出的子系统MHC码具有恒定的测量综合症权重（为4），同时保持高编码速率。块MAP和神经网络解码器的性能优于有界距离解码器。

Conclusion: 所提出的子系统MHC码是一种有前途的量子纠错码，在保持高编码速率的同时，显著降低了测量综合症的权重。结合块MAP和神经网络解码器，这些码在量子纠错方面表现出优越的性能。

Abstract: Quantum error-correcting codes (QECCs) require high encoding rate in addition
to high threshold unless a sufficiently large number of physical qubits are
available. The many-hypercube (MHC) codes defined as the concatenation of the
[[6,4,2]] quantum error-detecting code have been proposed as high-performance
and high-encoding-rate QECCs. However, the concatenated codes have a
disadvantage that the syndrome weight grows exponentially with respect to the
concatenation level. To address this issue, here we propose subsystem quantum
codes based on the MHC codes. In particular, we study the smallest subsystem
MHC codes, namely, subsystem codes derived from the concatenated [[4,2,2]]
error-detecting codes. The resulting codes have a constant syndrome-measurement
weight of 4, while keeping high encoding rates. We develop the block-MAP and
neural-network decoders and show that they demonstrate superior performance to
the bounded-distance decoder.

</details>


### [479] [Quantum capacity amplification via privacy](https://arxiv.org/abs/2510.04527)
*Peixue Wu,Yunkai Wang*

Main category: quant-ph

TL;DR: 该研究通过私有信道（其Choi-Jamiolkowski算子为私有状态）的量子容量的超加性来分析量子容量。


<details>
  <summary>Details</summary>
Motivation: 研究量子容量的超加性，特别是通过私有信道，并阐明保护系统在增强容量中的作用。

Method: 开发了一个通用的框架，给出了容量放大的充分条件，并用辅助信道的Holevo信息来表述。此外，在自旋对齐猜想的假设下，推导了一个单字母表达式，用于一系列非退化、反退化或PPT的私有信道的量子容量。还分析了近似私有信道，并对超激活进行了更广泛的参数范围的扩展证明。

Result: 提出了容量放大的通用框架和显式的放大阈值。推导了具有特定性质的私有信道的量子容量的单字母表达式，并构造了量子容量为零但私有容量无界的信道。对近似私有信道的超激活进行了扩展证明，并建立了度量分离，表明存在容量放大的信道与反退化信道集合的菱形距离不为零。

Conclusion: 研究结果表明，保护系统可以通过辅助信道回收信息，从而提高容量。该研究为理解量子容量的超加性提供了一个新的视角，并对近似私有信道的性质有了更深入的了解。最后，该研究认为量子容量的可计算性问题仍未解决。

Abstract: We investigate superadditivity of quantum capacity through private channels
whose Choi-Jamiolkowski operators are private states. This perspective links
the security structure of private states to quantum capacity and clarifies the
role of the shield system: information encoded in the shield system that would
otherwise leak to the environment can be recycled when paired with an assisting
channel, thereby boosting capacity. Our main contributions are threefold:
Firstly, we develop a general framework that provides a sufficient condition
for capacity amplification, which is formulated in terms of the assisting
channel's Holevo information. As examples, we give explicit, dimension and
parameter dependent amplification thresholds for erasure and depolarizing
channels. Secondly, assuming the Spin alignment conjecture, we derive a
single-letter expression for the quantum capacity of a family of private
channels that are neither degradable, anti-degradable, nor PPT; as an
application, we construct channels with vanishing quantum capacity yet
unbounded private capacity. Thirdly, we further analyze approximate private
channels: we give an alternative proof of superactivation that extends its
validity to a broader parameter regime, and, by combining amplification bounds
with continuity estimates, we establish a metric separation showing that
channels exhibiting capacity amplification have nonzero diamond distance from
the set of anti-degradable channels, indicating that existing approximate
(anti-)degradability bounds are not tight. We also revisit the computability of
the regularized quantum capacity and modestly suggest that this fundamental
question still remains open.

</details>


### [480] [Integrated photonic platform with high-speed entanglement generation and witnessing](https://arxiv.org/abs/2510.04534)
*Gong Zhang,Chao Wang,Koon Tong Goh,Si Qi Ng,Raymond Ho,Henry Semenenko,Srinivasan Ashwyn Srinivasan,Haibo Wang,Yue Chen,Jing Yan Haw,Xiao Gong,Joris Van Campenhout,Charles Lim*

Main category: quant-ph

TL;DR: 在本工作中，我们展示了一种在硅光子芯片上实现高速、高保真度纠缠态生成和探测的方法，达到了 92% 的量子态保真度和 2.59 的 CHSH 违规下界，为构建全集成、高带宽、室温量子光子系统提供了可行途径。


<details>
  <summary>Details</summary>
Motivation: 实现量子信息应用所需的高速光子芯片上的纠缠态生成和高效纠缠检测，这在现有的光子芯片材料特性和有限的组件性能下难以实现。

Method: 在硅光子芯片上，利用诱饵态技术进行多轨道单光子纠缠态生成，并结合芯片上带宽高达 12.5 GHz 的平衡零拍探测器进行纠缠检测，实现了室温运行。采用等效损耗分析方法补偿光学损耗和系统噪声。

Result: 实验量化了 92% 的量子态断层扫描保真度和 2.59 的 Clauser-Horne-Shimony-Holt (CHSH) 违规下界。

Conclusion: 实验结果证明了在硅光子芯片上实现高速、高保真度纠缠态生成和探测的可行性，为构建全集成、高带宽、室温量子光子系统铺平了道路，并为片上量子光学和量子随机数生成等领域带来了潜在应用。

Abstract: High-speed generation and efficient entanglement detection on a photonic chip
are essential for quantum information applications but hard to achieve due to
common photonic chips' material properties and limited component performance.
In this work, we experimentally demonstrate entanglement witness on a silicon
photonic chip, with multi-rail single-photon entanglement generation based on
decoy-state techniques. The detection is based on balanced homodyne detectors
on the same photonic chip with a bandwidth of up to 12.5 GHz, which allows
room-temperature operation. A loss-equivalent analysis method compensates for
optical losses and system noises. Experimental results quantify an entangled
state fidelity of 92% in quantum state tomography and a
Clauser-Horne-Shimony-Holt (CHSH) violation lower bound of 2.59. These results
establish a viable path toward fully integrated, high-bandwidth,
room-temperature quantum photonic systems, with potential applications in
on-chip quantum optics and quantum random number generation.

</details>


### [481] [Efficient three-qubit gates with giant atoms](https://arxiv.org/abs/2510.04545)
*Guangze Chen,Anton Frisk Kockum*

Main category: quant-ph

TL;DR: 利用巨原子体系的干涉效应，提出了一种无需复杂脉冲整形或额外硬件即可实现高保真度三比特门的方法，并展示了其在制备GHZ态方面的应用。


<details>
  <summary>Details</summary>
Motivation: 高保真度的三比特门对于量子计算和多方纠缠态的制备至关重要，但现有方法面临串扰、复杂控制和硬件开销等挑战。

Method: 提出并分析了利用巨原子（在多个空间分离点与波导耦合的人工原子）的干涉效应来实现快速、高保真度的三比特门。通过简单的频率调谐即可实现CCZS和DIV等原生三比特门，无需复杂的脉冲整形或额外的硬件。

Result: 在考虑了实际退相干的情况下，评估了门保真度，结果表明在超导电路的当前实验参数下，保真度可超过99.5%。此外，提出了一种可扩展的协议，使用最小的门深度，在亚300纳秒的时间尺度内制备了高保真度的三比特和五比特GHZ态。

Conclusion: 巨原子体系为近期的量子计算机和量子模拟器中的纠缠态制备和低深度量子电路设计提供了一个有前景的平台。

Abstract: Three-qubit gates are highly beneficial operations in quantum computing,
enabling compact implementations of quantum algorithms and efficient generation
of multipartite entangled states. However, realizing such gates with high
fidelity remains challenging due to crosstalk, complex control requirements,
and the overhead of parametric or tunable couplers. In this work, we propose
and analyze the implementation of fast, high-fidelity three-qubit gates using
giant atoms--artificial atoms coupled to a waveguide at multiple spatially
separated points. By leveraging interference effects intrinsic to the
giant-atom architecture, we demonstrate that native three-qubit gates, such as
the controlled-CZ-SWAP (CCZS) and the dual-iSWAP (DIV), can be realized through
simple frequency tuning, without the need for complex pulse shaping or
additional hardware. We evaluate gate performance under realistic decoherence
and show that fidelities exceeding 99.5% are achievable with current
experimental parameters in superconducting circuits. As an application, we
present a scalable protocol for preparing three- and five-qubit GHZ states
using minimal gate depth, achieving high state fidelity within sub-300ns
timescales. Our results position giant-atom systems as a promising platform for
entangled-state preparation and low-depth quantum circuit design in near-term
quantum computers and quantum simulators.

</details>


### [482] [Quantum Reverse Shannon Theorem Simplified](https://arxiv.org/abs/2510.04552)
*Gilad Gour*

Main category: quant-ph

TL;DR: 我们为量子反向香农定理提供了一个新的、更简洁的证明，该定理是量子信息论的核心。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决量子反向香农定理的现有证明方法，寻求更简洁、更直接的理解。

Method: 通过推导平滑最大信息量的一个通用加性上界，该上界用夹缝式 Rényi 互信息表示，我们获得了更紧密的单次结果，并消除了对后选技术的需求。

Result: 我们得到了一个通用的加性上界，该上界用夹缝式 Rényi 互信息表示，并成功地简化了量子反向香农定理的证明。

Conclusion: 本文提出的新方法提供了一个更清晰、更直接的理解，用于模拟量子信道的资源成本。

Abstract: We revisit the quantum reverse Shannon theorem, a central result in quantum
information theory that characterizes the resources needed to simulate quantum
channels when entanglement is freely available. We derive a universal additive
upper bound on the smoothed max-information in terms of the sandwiched R\'enyi
mutual information. This bound yields tighter single-shot results, eliminates
the need for the post-selection technique, and leads to a conceptually simpler
proof of the quantum reverse Shannon theorem. By consolidating and streamlining
earlier approaches, our result provides a clearer and more direct understanding
of the resource costs of simulating quantum channels.

</details>


### [483] [Expander qLDPC Codes against Long-range Correlated Errors in Memory](https://arxiv.org/abs/2510.04561)
*Yash Deepak Kashtikar,Pranay Mathur,Sudharsan Senthil,Avhishek Chatterjee*

Main category: quant-ph

TL;DR: 该论文研究了在存在长程相关噪声的情况下，如何实现具有常数空间开销的容错量子计算。


<details>
  <summary>Details</summary>
Motivation: 在长程相关噪声场景下，实现常数空间开销的容错量子计算是一个重要的实际问题。

Method: 论文采用了广义隐马尔可夫随机场（MRF）模型，并将其应用于平方根距离的qLDPC码，以分析噪声阈值。

Result: 在长程相关噪声下，对于具有平方根距离的qLDPC码，论文得到了一个噪声阈值，并给出了一个关于码率的明确表达式，允许误差在一定范围内（最多o(sqrt{#qubits)}）与一个位置上的其他错误相关。

Conclusion: 该研究为在长程相关噪声环境下实现高效容错量子计算提供了一种新的方法，并为未来的研究奠定了基础。

Abstract: Fault-tolerance using constant space-overhead against long-range correlated
errors is an important practical question. In the pioneering works [Terhal and
Burkard, PRA 2005], [Aliferis et al, PRA 2005], [Aharonov et al, PRL 2006],
fault-tolerance using poly-logarithmic overhead against long-range correlation
modeled by pairwise joint Hamiltonian was proven when the total correlation of
an error at a qubit location with errors at other locations was $O(1)$, i.e.,
the total correlation at a location did not scale with the number of qubits.
This condition, under spatial symmetry, can simply be stated as the correlation
between locations decaying faster than $\frac{1}{\text{dist}^{\text{dim}}}$.
However, the pairwise Hamiltonian model remained intractable for constant
overhead codes. Recently, [Bagewadi and Chatterjee, PRA 2025] introduced and
analyzed the generalized hidden Markov random field (MRF) model, which provably
captures all stationary distributions, including long-range correlations
[Kunsch et al, Ann. App. Prob. 1995]. It resulted in a noise threshold in the
case of long-range correlation, for memory corrected by the linear-distance
Tanner codes [Leverrier and Zemor, FOCS 2022] for super-polynomial time. In
this paper, we prove a similar result for square-root distance qLDPC codes and
provide an explicit expression for the noise threshold in terms of the code
rate, for up to $o(\sqrt{\text{\#qubits}})$ scaling of the total correlation of
error at a location with errors at other locations.

</details>


### [484] [Embedding-Aware Noise Modeling of Quantum Annealing](https://arxiv.org/abs/2510.04594)
*Seon-Geun Jeong,Mai Dinh Cong,Dae-Il Noh,Quoc-Viet Pham,Won-Joo Hwang*

Main category: quant-ph

TL;DR: 量子退火设备常因硬件连接稀疏导致嵌入开销和易受噪声影响，本研究提出了一个数学框架来量化这种影响，并得到实验验证。


<details>
  <summary>Details</summary>
Motivation: 当前量子退火设备硬件连接稀疏，需要将逻辑变量嵌入到物理量子比特链中，这限制了可扩展性并降低了可靠性。

Method: 提出一个数学框架，将嵌入开销与D-Wave的Zephyr拓扑中的硬件噪声联系起来，推导出链断裂概率和链断裂分数的封闭表达式，并使用高斯控制误差模型。

Result: 实验结果证实了理论噪声模型的准确性，并证明了嵌入感知噪声框架在解释链稳定性与逻辑耦合器保真度之间的权衡关系方面具有实践意义。

Conclusion: 本研究提出的嵌入感知噪声框架为理解当前设备中的噪声放大提供了新的视角，并为嵌入感知参数调整策略提供了量化指导。

Abstract: Quantum annealing provides a practical realization of adiabatic quantum
computation and has emerged as a promising approach for solving large-scale
combinatorial optimization problems. However, current devices remain
constrained by sparse hardware connectivity, which requires embedding logical
variables into chains of physical qubits. This embedding overhead limits
scalability and reduces reliability as longer chains are more prone to
noise-induced errors. In this work, building on the known structural result
that the average chain length in clique embeddings grows linearly with the
problem size, we develop a mathematical framework that connects
embedding-induced overhead with hardware noise in D-Wave's Zephyr topology. Our
analysis derives closed-form expressions for chain break probability and chain
break fraction under a Gaussian control error model, establishing how noise
scales with embedding size and how chain strength should be adjusted with chain
length to maintain reliability. Experimental results from the Zephyr
topology-based quantum processing unit confirm the accuracy of these
predictions, demonstrating both the validity of the theoretical noise model and
the practical relevance of the derived scaling rule. Beyond validating a
theoretical model against hardware data, our findings establish a general
embedding-aware noise framework that explains the trade-off between chain
stability and logical coupler fidelity. Our framework advances the
understanding of noise amplification in current devices and provides
quantitative guidance for embedding-aware parameter tuning strategies.

</details>


### [485] [Generalized Entanglement of Purification Criteria for 2-Producible States in Multipartite Systems](https://arxiv.org/abs/2510.04596)
*Tian-Ren Jin,Yu-Ran Zhang,Heng Fan*

Main category: quant-ph

TL;DR: multipartite entanglement is complex; the entanglement of purification gap is not sufficient to detect more than tripartite entanglement. Generalized entanglement of purification gap quantifies quantum communication cost and detects 2-producible states. It also relates to local recoverability and distance to 2-producible states. Generalized Schmidt decomposition is not always fulfilled by 4-partite stabilizer states.


<details>
  <summary>Details</summary>
Motivation: The complexity of multipartite entanglement, specifically the limitations of using the entanglement of purification gap to detect it, and the need for a more general method to quantify it.

Method: Generalizing entanglement of purification to the multipartite case, calculating the gap for states fulfilling generalized Schmidt decomposition, and relating the gap to local recoverability and relative entropy.

Result: The entanglement of purification gap is insufficient for detecting more than tripartite entanglement. The generalized entanglement of purification gap quantifies quantum communication cost and identifies 2-producible states. It is also linked to local recoverability and relative entropy. Generalized Schmidt decomposition is not always fulfilled by 4-partite stabilizer states.

Conclusion: Generalized entanglement of purification gap provides a quantitative characterization of multipartite entanglement, advancing its study.

Abstract: Multipartite entanglement has much more complex structures than bipartite
entanglement, such as the semiseparable state. The multipartite state absent of
multipartite entanglement is called a 2-producible state, which is a tensor
product of at most 2-partite states. Recently, it is proved that a tripartite
pure state is 2-producible if and only if the gap between entanglement of
purification and its lower bound vanishes. Here, we show that the entanglement
of purification gap is not sufficient to detect more than tripartite
entanglement with 4-partite random stabilizer states. We then generalize
entanglement of purification to the multipartite case, where the gap between
generalized entanglement of purification and its lower bound quantifies the
quantum communication cost for distributing one part of the multipartite system
to the other parts. We also demonstrate that a multipartite state is
2-producible if and only if the generalized entanglement of purification gaps
vanish. In addition, we show that the generalized entanglement of purification
gaps are related to the local recoverability of the multipartite state from its
marginal state on some parts of the system and the distance between the state
and the 2-producible states with the relative entropy. Moreover, we calculate
the generalized entanglement of purification gaps for the states fulfilling the
generalized Schmidt decomposition, which implies that the 4-partite stabilizer
states do not always have the generalized Schmidt decomposition. Our results
provide a quantitive characterization of multipartite entanglement in
multipartite system, which will promote further investigations and
understanding of multipartite entanglement.

</details>


### [486] [Novel frame changes for quantum physics](https://arxiv.org/abs/2510.04598)
*Pierre-Louis Giscard,Omid Faizy,Christian Bonhomme*

Main category: quant-ph

TL;DR: 本论文提出了一种名为“双帧”的新型量子系统演化计算方法，该方法结合了两种不同的标准参考系，能够显著提高计算收敛速度，但计算成本也随之增加。


<details>
  <summary>Details</summary>
Motivation: 提出新型的量子演化算符计算方法，以提高计算效率。

Method: 提出并详细介绍“双帧”方法，其中物理系统的演化同时在两种不同的标准参考系下进行观察。证明了在双帧方法下，所有解的级数展开的收敛速度比传统方法快一倍。具体来说，如果传统方法在第n阶的误差为O(ε^n)，那么在双帧方法下，相同的计算成本可以达到O(ε^(2n+1))的误差。此外，还提出存在一个包含双帧的无穷多帧的家族，其中一些方法可以提供更高的收敛加速，但需要更高的初始计算成本。

Result: 在双帧方法下，量子演化算符级数展开的收敛速度比传统方法快一倍。

Conclusion: 双帧方法是一种有效提高量子演化计算收敛速度的方法，但需要权衡计算加速和计算成本。

Abstract: We present novel, exotic types of frame changes for the calculation of
quantum evolution operators. We detail in particular the biframe, in which a
physical system's evolution is seen in an equal mixture of two different
standard frames at once. We prove that, in the biframe, convergence of all
series expansions of the solution is quadratically faster than in
`conventional' frames. That is, if in laboratory frame or after a standard
frame change the error at order $n$ of some perturbative series expansion of
the evolution operator is on the order of $\epsilon^n$, $0<\epsilon<1$, for a
computational cost $C(n)$ then it is on the order of $\epsilon^{2n+1}$ in the
biframe for the same computational cost. We demonstrate that biframe is one of
an infinite family of novel frames, some of which lead to higher accelerations
but require more computations to set up initially, leading to a trade-off
between acceleration and computational burden.

</details>


### [487] [A Lie Theoretic Framework for Controlling Open Quantum Systems](https://arxiv.org/abs/2510.04719)
*Corey O'Meara*

Main category: quant-ph

TL;DR: 本论文研究了受控开放量子系统的李群理论基础，使用李半群描述马尔可夫开放量子系统演化，并将李半群的生成元置于特殊的凸锥——李楔中。对于n量子比特开放量子系统，论文给出了包含这些李楔的、物理上最相关的李群（系统群）的参数化。


<details>
  <summary>Details</summary>
Motivation: 为理解和控制开放量子系统，需要研究其李群理论基础。

Method: 通过李楔描述马尔可夫开放量子系统演化，并对n量子比特系统进行李群参数化，分析BCH乘法封闭性，并利用平移算符构建耗散定点工程。

Result: 给出了系统李楔及其系统李群的显式形式，解决了李楔封闭性问题，并实现了纯态和混合态的定点工程。

Conclusion: 论文的参数化方法有助于分离酉和非酉耗散动力学，并为量子态工程提供了一种新的途径。

Abstract: This thesis focuses on the Lie-theoretic foundations of controlled open
quantum systems. We describe Markovian open quantum system evolutions by Lie
semigroups, whose corresponding infinitesimal generators lie in a special type
of convex cone - a Lie wedge. The Lie wedge associated to a given control
system therefore consists of all generators of the quantum dynamical semigroup
that are physically realisable as a result of the interplay between the
coherent and incoherent processes the quantum system is subject to. For
$n$-qubit open quantum systems, we provide a parametrisation of the largest
physically relevant Lie algebra (the system algebra), in which these Lie wedges
are contained: the Lindblad-Kossakowski Lie algebra. This parametrisation
provides several useful benefits. First, it allows us to construct explicit
forms of these system Lie wedges and their respective system Lie algebras.
Second, we analyse which control scenarios yield Lie wedges that are closed
under Baker-Campbell-Hausdorff (BCH) multiplication and therefore generate
Markovian semigroups of time-independent quantum channels. Lie wedges of this
form are called Lie semialgebras, and we completely solve this open problem by
proving that Lie wedges specialise to this form only when the coherent controls
have no effect on both the inherent drift Hamiltonian and the incoherent part
of the dynamics. Finally, this parametrisation of the Lindblad-Kossakowski Lie
algebra points to an intuitive separation between unital and non-unital
dissipative dynamics, where the non-unital component of the dynamics is
described by affine translation operations. These translation operators are
then exploited to construct purely dissipative fixed-point engineering schemes
to obtain either pure or mixed states as a system's unique fixed point.

</details>


### [488] [Enhancing Optomechanical Entanglement and Mechanical Squeezing by the Synergistic Effect of Quadratic Optomechanical Coupling and Coherent Feedback](https://arxiv.org/abs/2510.04732)
*Ya-Feng Jiao,Ruo-Chen Wang,Jing-Xue Liu,Hui-Lai Zhang,Ya-Chuan Liang,Yan Wang,Le-Man Kuang,Hui Jing*

Main category: quant-ph

TL;DR: 该论文提出了一种利用相干反馈回路实现膜嵌入式腔光力学系统中强光机纠缠和机械压缩的方法，并展示了在优化条件下可实现超过10dB的压缩度。


<details>
  <summary>Details</summary>
Motivation: 实现宏观尺度下的量子纠缠和压缩状态在基础科学和量子技术中至关重要，但仍具挑战性。

Method: 研究了在膜嵌入式腔光力学系统中，通过引入相干反馈回路，利用线性和二次光机耦合来实现强光机纠缠和机械压缩。

Result: 通过调节二次光机耦合符号和反馈控制腔衰减率，实现了光机纠缠的增强和超过3dB的机械压缩，优化后压缩度可达10dB以上。

Conclusion: 该方法为在腔光力学系统中产生高度纠缠或压缩态提供了一条全光路新途径，有望推动宏观量子效应和量子信息处理的发展。

Abstract: Quantum entanglement and squeezing associated with the motions of massive
mechanical oscillators play an essential role in both fundamental science and
emerging quantum technologies, yet realizing such macroscopic nonclassical
states remains a formidable challenge. In this paper, we investigate how to
achieve strong optomechanical entanglement and mechanical squeezing in a
membrane-embedded cavity optomechanical system incorporating a coherent
feedback loop, where the membrane interacts with the cavity mode through both
linear and quadratic optomechanical couplings. This hybrid optomechanical
architecture offers a flexible tunability of intrinsic system parameters, thus
allowing the membrane to be stiffened or softened through tuning the sign of
quadratic optomechanical coupling and the cavity decay rate to be reduced via
feedback control. Exploiting these unique features, we demonstrate that
optomechanical entanglement can be substantially enhanced with positive
coupling sign and suitable feedback parameters, while strong mechanical
squeezing beyond the 3dB limit is simultaneously achieved over a broad
parameter range with negative coupling sign, reaching squeezing degree above
10dB under optimized conditions. Our proposal, establishing an all-optical
method for generating highly entangled or squeezed states in cavity
optomechanical systems, opens up a new route to explore macroscopic quantum
effects and to advance quantum information processing.

</details>


### [489] [Quantum Subgradient Estimation for Conditional Value-at-Risk Optimization](https://arxiv.org/abs/2510.04736)
*Vasilis Skarlatos,Nikos Konofaos*

Main category: quant-ph

TL;DR: 该论文提出了一种用于条件在险价值（CVaR）最小化的量子次梯度预言机，通过振幅估计实现，将样本复杂度从经典蒙特卡洛的O(1/ε^2)提高到O(1/ε)，实现了近乎二次的改进。


<details>
  <summary>Details</summary>
Motivation: 在金融领域，条件在险价值（CVaR）作为一种重要的尾部风险度量，在监管和投资组合优化中扮演着核心角色。然而，经典的CVaR及其梯度估计依赖于蒙特卡洛模拟，其样本复杂度为O(1/ε^2)，效率有待提高。

Method: 设计并分析了一种基于振幅估计的量子次梯度预言机，用于CVaR最小化。该方法通过一个三方命题，展示了即使在需要估计在险价值（VaR）阈值的情况下，CVaR次梯度也能以O(1/ε)的量子查询复杂度进行估计。此外，论文还量化了从VaR估计到CVaR梯度估计的误差传播，并推导了使用该预言机的随机投影次梯度下降法的收敛速率。

Result: 通过理论分析和模拟量子电路的数值实验，论文证明了所提出的量子次梯度预言机在样本复杂度上相比经典蒙特卡洛方法有近乎二次的提升。数值实验结果证实了理论速率，并展示了该方法对阈值估计噪声的鲁棒性。

Conclusion: 本研究首次对用于尾部风险最小化的量子次梯度方法进行了严格的复杂度分析，为提高CVaR估计和优化的效率提供了新的量子算法途径。

Abstract: Conditional Value-at-Risk (CVaR) is a leading tail-risk measure in finance,
central to both regulatory and portfolio optimization frameworks. Classical
estimation of CVaR and its gradients relies on Monte Carlo simulation,
incurring $O(1/\epsilon^2)$ sample complexity to achieve $\epsilon$-accuracy.
In this work, we design and analyze a quantum subgradient oracle for CVaR
minimization based on amplitude estimation. Via a tripartite proposition, we
show that CVaR subgradients can be estimated with $O(1/\epsilon)$ quantum
queries, even when the Value-at-Risk (VaR) threshold itself must be estimated.
We further quantify the propagation of estimation error from the VaR stage to
CVaR gradients and derive convergence rates of stochastic projected subgradient
descent using this oracle. Our analysis establishes a near-quadratic
improvement in query complexity over classical Monte Carlo. Numerical
experiments with simulated quantum circuits confirm the theoretical rates and
illustrate robustness to threshold estimation noise. This constitutes the first
rigorous complexity analysis of quantum subgradient methods for tail-risk
minimization.

</details>


### [490] [Quantum Reservoir Computing for Credit Card Default Prediction on a Neutral Atom Platform](https://arxiv.org/abs/2510.04747)
*Giacomo Vitali,Chiara Vercellino,Paolo Viviani,Olivier Terzo,Bartolomeo Montrucchio,Valeria Zaffaroni,Francesca Cibrario,Christian Mattia,Giacomo Ranieri,Alessandro Sabatino,Francesco Bonazzi,Davide Corbelletto*

Main category: quant-ph

TL;DR: 该研究定义并测试了一个混合量子-经典机器学习流程，用于金融领域的二元分类任务。


<details>
  <summary>Details</summary>
Motivation: 在金融领域应用混合量子-经典机器学习流程，具体为信用卡的违约预测。

Method: 在经典流程中集成量子随机计算（QRC）层，并使用QuEra的Aquila模拟器执行，研究了两种编码方式（位置和局部失谐）。将该流程应用于预测信用卡违约，并与全经典流程（包括深度神经网络）进行比较。同时评估了硬件噪声对性能的影响。

Result: 无噪声模拟达到了与全经典流程相当的性能，但硬件噪声显著降低了性能。对于此特定用例，混合方法的结果与经典基准相当。

Conclusion: 尽管在此特定用例中的结果与经典基准相当，但QRC的灵活性和可扩展性预示着其在广泛应用中具有巨大潜力。

Abstract: In this paper, we define and benchmark a hybrid quantum-classical machine
learning pipeline by performing a binary classification task applied to a
real-world financial use case. Specifically, we implement a Quantum Reservoir
Computing (QRC) layer within a classical routine that includes data
preprocessing and binary classification. The reservoir layer has been executed
on QuEra's Aquila, a 256-qubit neutral atom simulator, using two different
types of encoding: position and local detuning. In the former case, classical
data are encoded into the relative distance between atoms; in the latter, into
pulse amplitudes. The developed pipeline is applied to predict credit card
defaults using a public dataset and a wide variety of traditional classifiers.
The results are compared with a fully-classical pipeline including a Deep
Neural Network (DNN) model. Additionally, the impact of hardware noise on
classification performance is evaluated by comparing the results obtained using
Aquila within the classification workflow with those obtained using a
classical, noiseless emulation of the quantum system. The results indicate that
the noiseless emulation achieves competitive performance with the
fully-classical pipeline, while noise significantly degrades overall
performance. Although the results for this specific use case are comparable to
those of the classical benchmark, the flexibility and scalability of QRC
highlight strong potential for a wide range of applications.

</details>


### [491] [Collusion-Resistant Quantum Secure Key Leasing Beyond Decryption](https://arxiv.org/abs/2510.04754)
*Fuyuki Kitagawa,Ryo Nishimaki,Nikhil Pappu*

Main category: quant-ph

TL;DR: 本研究提出了一种新的安全密钥租赁（SKL）方法，解决了现有技术在多方密钥租赁和PRFs、数字签名等功能上的局限性，并实现了更强的安全保障。


<details>
  <summary>Details</summary>
Motivation: 现有安全密钥租赁（SKL）技术通常只考虑单方密钥租赁，无法满足多方应用的需求，并且在PRFs和数字签名等功能方面存在不足。本研究旨在解决这些问题，提供更安全、更通用的SKL解决方案。

Method: 研究引入了多层级追踪（MLTT）的概念，并提出了一种将MLTT方案转换为防碰撞SKL方案的编译器。此外，还提出了将单一密钥安全SKL方案升级为具有无限防碰撞安全性的数字签名SKL方案的编译器，以及将具有经典证书的防碰撞SKL方案升级为具有验证查询弹性的方案的编译器。

Result: 成功构建了基于LWE假设的、第一个具有有界防碰撞安全性的PRFs SKL方案。同时，提出了两种编译器，分别能将现有的SKL方案在数字签名和验证查询弹性方面进行升级。

Conclusion: 本研究在安全密钥租赁领域取得了重要进展，通过引入MLTT和新的编译器，显著提高了SKL方案在多方协作、PRFs和数字签名等场景下的安全性和适用性。

Abstract: Secure key leasing (SKL) enables the holder of a secret key for a
cryptographic function to temporarily lease the key using quantum information.
Later, the recipient can produce a deletion certificate, which proves that they
no longer have access to the secret key. The security guarantee ensures that
even a malicious recipient cannot continue to evaluate the function, after
producing a valid deletion certificate.
  Most prior work considers an adversarial recipient that obtains a single
leased key, which is insufficient for many applications. In the more realistic
collusion-resistant setting, security must hold even when polynomially many
keys are leased (and subsequently deleted). However, achieving
collusion-resistant SKL from standard assumptions remains poorly understood,
especially for functionalities beyond decryption.
  We improve upon this situation by introducing new pathways for constructing
collusion-resistant SKL. Our main contributions are as follows:
  - A generalization of quantum-secure collusion-resistant traitor tracing
called multi-level traitor tracing (MLTT), and a compiler that transforms an
MLTT scheme for a primitive X into a collusion-resistant SKL scheme for
primitive X.
  - The first bounded collusion-resistant SKL scheme for PRFs, assuming LWE.
  - A compiler that upgrades any single-key secure SKL scheme for digital
signatures into one with unbounded collusion-resistance, assuming OWFs.
  - A compiler that upgrades collusion-resistant SKL schemes with classical
certificates to ones having verification-query resilience, assuming OWFs.

</details>


### [492] [Counterdiabatic driving at Rydberg excitation for symmetric $C_Z$ gates with ultracold neutral atoms](https://arxiv.org/abs/2510.04766)
*I. I. Beterov,K. V. Kozenko,P. Xu,I. I. Ryabtsev*

Main category: quant-ph

TL;DR: 通过在里德堡阻塞机制下对双原子施加对称的绝热脉冲序列并结合反绝热驱动，我们实现了更快的量子门操作，并将操作时间缩短了至少五倍。


<details>
  <summary>Details</summary>
Motivation: 为了在有限的里德堡原子寿命内实现高保真度的量子纠缠，需要缩短量子门操作时间。

Method: 本研究提出了一种基于双序列绝热脉冲对称应用于两个原子，并结合反绝热驱动的里德堡 C_Z 门方案。对单光子、双光子和三光子里德堡激发方案，以及铷和铯原子进行了绝热快速通过和反绝热驱动的方案分析。设计了具有完全解析形状的激光脉冲，并考虑了原子寿命和有限的阻塞强度，计算了贝尔保真度。

Result: 与先前提出的绝热方案相比，该方案将量子门操作时间缩短了至少五倍。在室温环境下，贝尔保真度可以达到 0.9999。

Conclusion: 提出的结合反绝热驱动的里德堡 C_Z 门方案能够显著缩短量子门操作时间，并实现高保真度纠缠，有望在室温环境下达到接近完美的保真度。

Abstract: We extend the scheme of neutral atom Rydberg $C_Z$ gate based on double
sequence of adiabatic pulses applied symmetrically to both atoms using
counterdiabatic driving in the regime of Rydberg blockade. This provides
substantial reducing of quantum gate operation times (at least five times)
compared to previously proposed adiabatic schemes, which is important for
high-fidelity entanglement due to finite Rydberg lifetimes. We analyzed schemes
of adiabatic rapid passage with counterdiabatic driving for single-photon,
two-photon and three-photon schemes of Rydberg excitation for rubidium and
cesium atoms. We designed laser pulse profiles with fully analytical shapes and
calculated the Bell fidelity taking into account atomic lifetimes and finite
blockade strengths. We show that the upper limit of the Bell fidelity reaches
${\mathcal F}\simeq0.9999$ in a room-temperature environment.

</details>


### [493] [Far-from-equilibrium thermodynamics of non-Abelian thermal states](https://arxiv.org/abs/2510.04788)
*Franklin L. S. Rodrigues,Eric Lutz*

Main category: quant-ph

TL;DR: 量子物理中的可观测量的不可交换性是核心特征，它影响着不确定性原理和量子系统的热力学定律。本文推导了非平衡量子系统的第二热力学定律的涨落关系，并提出了非阿贝尔（non-Abelian）贡献，这对于能量和熵的守恒至关重要，并且可以用来提升功的提取和非平衡电流。


<details>
  <summary>Details</summary>
Motivation: 量子物理中可观测量的不可交换性对不确定性原理和热力学定律有重要影响，但现有理论未能充分解释其在非平衡系统中的作用。

Method: 推导了非平衡量子系统的第二热力学定律的涨落关系，并识别出其中非阿贝尔（non-Abelian）贡献。

Result: 识别出非阿贝尔（non-Abelian）贡献，该贡献是能量和熵守恒所必需的，并且可以用于增强功的提取和非平衡电流。

Conclusion: 量子物理中可观测量的不可交换性不仅是基本特征，还可以作为一种有用的热力学资源，用于提升能量转换效率和驱动非平衡过程。

Abstract: Noncommutativity of observables is a central feature of quantum physics. It
plays a fundamental role in the formulation of the uncertainty principle for
complementary variables and strongly affects the laws of thermodynamics for
systems with noncommuting, that is, non-Abelian, conserved quantities. We here
derive nonequilibrium generalizations of the second law of thermodynamics in
the form of fluctuation relations, both for mechanically and thermally driven
quantum systems. We identify a non-Abelian contribution to the energy and
entropy balances, without which these relations would be violated. The latter
term can be controlled to enhance both work extraction and nonequilibrium
currents compared to what is obtained in commuting thermodynamics. These
findings demonstrate that noncommutativity maybe a useful thermodynamic
resource.

</details>


### [494] [Super-resolution of partially coherent bosonic sources](https://arxiv.org/abs/2510.04818)
*Joaquín López-Suárez,Michalis Skotiniotis*

Main category: quant-ph

TL;DR: 该研究提出了对两个部分相干源进行成像的方法，并推导了所有相关参数（如间距、相对强度和相干因子）的最终量子极限。


<details>
  <summary>Details</summary>
Motivation: 推导部分相干源成像的所有相关参数的最终量子极限。

Method: 提出了一种使用二元空间模式解复用测量的方法，该测量位于两个源的联合点扩展函数强度中心。

Result: 在亚瑞利极限下，使用简单的玻色子计数测量可以最优地估计相对强度和相干因子实部，从而可以同时最优地估计源的间距、相对强度和相干因子实部。

Conclusion: 研究发现，通过测量两能级系统的保真度来提取源间距估计值的间接估计方案，对于所有非零相干因子值，估计精度都会次优。

Abstract: We consider the problem of imaging two partially coherent sources and derive
the ultimate quantum limits for estimating all relevant parameters, namely
their separation, relative intensity, as well as their coherence factor. We
show that the separation of the two sources can be super-resolved over the
entire range of all other pertinent parameters (with the exception of fully
coherent sources), with anti-correlated sources furnishing the largest possible
gain in estimation precision, using a binary spatial mode demultiplexing
measurement positioned at the center of intensity of the joint point spread
function for the two sources. In the sub-Rayleigh limit, we show that both the
relative intensity, as well as the real part of the coherence factor, can be
optimally estimated by a simple boson counting measurement, making it possible
to optimally estimate the separation, relative intensity and real coherence
factor of the sources simultaneously. Within the same limit, we show that the
imaging problem can be effectively reduced to one where all relevant parameters
are encoded in the Bloch vector of a two-dimensional system. Using such a model
we find that indirect estimation schemes, which attempt to extract estimates of
the separation of the two sources by measuring the purity of the corresponding
state of the two-level system, yield suboptimal estimation precision for all
non-zero values of the coherence factor.

</details>


### [495] [Information-thermodynamic bounds on precision in interacting quantum systems](https://arxiv.org/abs/2510.04866)
*Ryotaro Honma,Tan Van Vu*

Main category: quant-ph

TL;DR: 量子系统中的信息流动、局部耗散和量子效应共同制约着电流涨落，并推导出了量子热力学不确定性关系，该关系在量子热机和量子钟等领域具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 探究信息流动、局部耗散和量子效应对相互作用的量子系统中子系统的电流涨落的联合制约作用，以及它们如何影响精度-耗散权衡。

Method: 推导了适用于相互作用的多方系统的量子热力学不确定性关系，并使用数值模拟在自主量子麦克斯韦妖和量子钟模型上进行了验证。

Result: 证明了信息交换和量子相干性与局部耗散一样，在抑制电流涨落方面起着至关重要的作用，并得出了电流涨落、信息流、局部耗散和量子效应之间的热力学权衡。

Conclusion: 研究结果将不确定性关系扩展到多方开放量子系统，并阐明了信息流在抑制涨落中的功能作用，这对量子热机的性能有重要启示。

Abstract: The thermodynamic uncertainty relation quantifies a trade-off between the
relative fluctuations of trajectory currents and the thermodynamic cost,
indicating that the current precision is fundamentally constrained by entropy
production. In classical bipartite systems, it has been shown that information
flow between subsystems can enhance the current precision alongside
thermodynamic dissipation. In this study, we investigate how information flow,
local dissipation, and quantum effects jointly constrain current fluctuations
within a subsystem of interacting quantum systems. Unlike classical bipartite
systems, quantum subsystems can exhibit simultaneous state changes and maintain
quantum coherence, which fundamentally alters the precision-dissipation
trade-off. For this general setting, we derive a quantum thermokinetic
uncertainty relation for interacting multipartite systems, establishing a
thermodynamic trade-off between current fluctuations, information flow, local
dissipation, and quantum effects. Our analysis shows that, in addition to local
dissipation, both information exchange and quantum coherence play essential
roles in suppressing current fluctuations. These results have important
implications for the performance of quantum thermal machines, such as
information-thermodynamic engines and quantum clocks. We validate our
theoretical findings through numerical simulations on two representative
models: an autonomous quantum Maxwell's demon and a quantum clock. These
results extend uncertainty relations to multipartite open quantum systems and
elucidate the functional role of information flow in fluctuation suppression.

</details>


### [496] [A new application of the Fox-Wright functions: the coherent states formalism](https://arxiv.org/abs/2510.04874)
*Dusan Popov*

Main category: quant-ph

TL;DR: 本文将Fox-Wright函数应用于量子物理学，并将其与广义相干态形式主义联系起来，构建了Fox-Wright相干态，并研究了其性质。


<details>
  <summary>Details</summary>
Motivation: 将Fox-Wright函数应用于量子物理学，并探索其与广义相干态形式主义的联系。

Method: 采用Barut-Girardello的方式构造广义相干态，并利用对角化算符排序技术（DOOT）和狄拉克符号演算来处理纯态和混合态（热态）的Fox-Wright相干态。

Result: 构建了Fox-Wright相干态，证明了它们满足相干态的普遍条件，并研究了它们的性质。还提出了一些Fox-Wright相干态在特殊函数理论中引入的反馈元素，以及Fox-Wright函数的一个新的积分表示。

Conclusion: Fox-Wright函数在量子物理学和广义相干态形式主义中具有广泛的应用前景，并且为特殊函数理论提供了新的见解。

Abstract: In this paper we extend the applicability of Fox-Wright functions beyond
mathematics, specifically in quantum physics. We focused our attention on a new
application, on the connection between the Fox-Wright functions and the
generalized coherent states formalism. We constructed the generalized coherent
states in the Barut-Girardello manner, in which the Fox-Wright functions play
the role of normalization functions, and we demonstrated that the Fox-Wright
coherent states satisfy all general conditions imposed on the set of coherent
states. In parallel, we examined the properties of both pure and mixed
(thermal) Fox-Wright coherent states. All calculations were performed within
the diagonal operators ordering technique (DOOT) using the Dirac's bra-ket
formalism. Finally, we introduced some (specifically, integral) feedback
elements that Fox-Wright coherent states induce in the theory of special
functions, including a new integral representation of Fox-Wright functions.

</details>


### [497] [Do Qubit States have to be non-degenerate two-level systems?](https://arxiv.org/abs/2510.04880)
*Zhuoran Bao,Daniel F. V. James*

Main category: quant-ph

TL;DR: 我们探讨了具有简并子态的两个能级是否可以作为量子比特，并使用原子系统进行了研究。


<details>
  <summary>Details</summary>
Motivation: 研究是否可以使用具有简并子态的两个能级作为量子比特。

Method: 将原子近似为没有简并量子数能级分裂的两能级系统，并使用角动量加法规则来选择状态跃迁。

Result: 在连续场存在下，原子仍然会发生适用于量子门构造的拉比振荡，并计算了单简并原子量子门操作的平均保真度，推测了两原子相互作用以构造受控Z门的所需形式。

Conclusion: 具有简并子态的两个能级可以作为量子比特，并可用于量子门构造。

Abstract: A qubit, or quantum bit, is conventionally defined as "a physical system for
storing information that is capable of existing in either of two quantum states
or in a superposition of both". In this paper, we examine the simple question
of whether two distinct levels, each consisting of multiply degenerate
sub-states, could serve as a practical quantum bit. We explore this idea using
a well-characterized atomic system of the kind employed in several quantum
computing implementations. We approximate the atom as a two-level system
without degeneracy lifting in the magnetic quantum number while using the
angular momentum addition rules to select the desired state transition. We find
that, in the continuous presence of the field, the atom still undergoes Rabi
oscillations, which are suitable for quantum gate construction. In addition, we
compute the average fidelity in quantum gate performance for a single
degenerate atom and postulate the required form of two-atom interaction to
construct a controlled Z gate.

</details>


### [498] [Improved Clifford operations in constant commutative depth](https://arxiv.org/abs/2510.04921)
*Richard Cleve,Zhiqian Ding,Luke Schaeffer*

Main category: quant-ph

TL;DR: 该研究提出了一个更优的计算克利福德运算的交换深度模型，并证明了前缀和问题在此模型下的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 为了在量子计算中更高效地利用可交换门并行执行的特性，需要研究更优的克利福德运算计算方法和相关问题的复杂度。

Method: 提出并应用了交换深度模型，推导了克利福德群和前缀和问题的交换深度和尺寸复杂度。

Result: 在交换深度模型下，将克利福德运算的计算深度从23降低到16，并将前缀和问题的计算尺寸从O(n^2)降低到O(n log n)。同时，还证明了一些相关问题的下界。

Conclusion: 所提出的交换深度模型在计算克利福德运算和解决前缀和问题方面比先前的方法更有效，并且对相关问题的复杂度进行了更深入的理论分析。

Abstract: The commutative depth model allows gates that commute with each other to be
performed in parallel. We show how to compute Clifford operations in constant
commutative depth more efficiently than was previously known. Bravyi, Maslov,
and Nam [Phys. Rev. Lett. 129:230501, 2022] showed that every element of the
Clifford group (on $n$ qubits) can be computed in commutative depth 23 and size
$O(n^2)$. We show that the Prefix Sum problem can be computed in commutative
depth 16 and size $O(n \log n)$, improving on the previous depth 18 and size
$O(n^2)$ bounds. We also show that, for arbitrary Cliffords, the commutative
depth bound can be reduced to 16. Finally, we show some lower bounds: that
there exist Cliffords whose commutative depth is at least 4; and that there
exist Cliffords for which any constant commutative depth circuit has size
$\Omega(n^2)$.

</details>


### [499] [Efficient Quantum Hermite Transform](https://arxiv.org/abs/2510.04929)
*Siddhartha Jain,Vishnu Iyer,Rolando D. Somma,Ning Bao,Stephen P. Jordan*

Main category: quant-ph

TL;DR: 该论文提出了一种量子算法新图元，可高效实现离散厄米变换，其时间复杂度与维度和允许误差的倒数成对数关系。该变换是傅里叶变换的高斯类似物。


<details>
  <summary>Details</summary>
Motivation: 需要一种高效的厄米变换，以在量子算法中实现高斯傅里叶变换的类似功能，并应用于属性测试和学习等领域。

Method: 提出了一种指数级加速量子谐振子演化的方法，并基于此实现离散厄米变换。

Result: 成功实现了离散厄米变换，并将其应用于属性测试和学习任务，在测试接近低度以及高斯版本的Goldreich-Levin学习任务方面展示了量子查询优势。

Conclusion: 提出的厄米变换为量子算法提供了一个强大的新工具，在属性测试、学习和模拟量子系统动力学方面具有潜在应用价值。

Abstract: We present a new primitive for quantum algorithms that implements a discrete
Hermite transform efficiently, in time that depends logarithmically in both the
dimension and the inverse of the allowable error. This transform, which maps
basis states to states whose amplitudes are proportional to the Hermite
functions, can be interpreted as the Gaussian analogue of the Fourier
transform. Our algorithm is based on a method to exponentially fast forward the
evolution of the quantum harmonic oscillator, which significantly improves over
prior art. We apply this Hermite transform to give examples of provable quantum
query advantage in property testing and learning. In particular, we show how to
efficiently test the property of being close to a low- degree in the Hermite
basis when inputs are sampled from the Gaussian distribution, and how to solve
a Gaussian analogue of the Goldreich-Levin learning task efficiently. We also
comment on other potential uses of this transform to simulating time dynamics
of quantum systems in the continuum.

</details>


### [500] [The NPA hierarchy does not always attain the commuting operator value](https://arxiv.org/abs/2510.04943)
*Marco Fanizza,Larissa Kroell,Arthur Mehta,Connor Paddock,Denis Rochette,William Slofstra,Yuming Zhao*

Main category: quant-ph

TL;DR: it is undecidable to determine whether the commuting operator value of a nonlocal game is strictly greater than 1/2


<details>
  <summary>Details</summary>
Motivation: Our contribution involves establishing a computable mapping from Turing machines to BCS nonlocal games in which the halting property of the machine is encoded as a decision problem for the commuting operator value of the game.

Method: Our techniques are algebraic and distinct from those used to establish MIP*=RE.

Result: We show that it is undecidable to determine whether the commuting operator value of a nonlocal game is strictly greater than 1/2.

Conclusion: As a corollary, there is a boolean constraint system (BCS) game for which the value of the Navascués-Pironio-Acín (NPA) hierarchy does not attain the commuting operator value at any finite level.

Abstract: We show that it is undecidable to determine whether the commuting operator
value of a nonlocal game is strictly greater than 1/2. As a corollary, there is
a boolean constraint system (BCS) game for which the value of the
Navascu\'es-Pironio-Ac\'in (NPA) hierarchy does not attain the commuting
operator value at any finite level. Our contribution involves establishing a
computable mapping from Turing machines to BCS nonlocal games in which the
halting property of the machine is encoded as a decision problem for the
commuting operator value of the game. Our techniques are algebraic and distinct
from those used to establish MIP*=RE.

</details>


### [501] [Leveraging Analog Neutral Atom Quantum Computers for Diversified Pricing in Hybrid Column Generation Frameworks](https://arxiv.org/abs/2510.04946)
*Cédrick Perron,Yves Bérubé-Lauzière,Victor Drouin-Touchette*

Main category: quant-ph

TL;DR: 通过改进量子子程序来优化组合优化问题，并提出了一种名为Make_Diff的后处理技术来提高样本的多样性。


<details>
  <summary>Details</summary>
Motivation: 为了改进基于中性原子量子计算机（NAQC）的混合列生成（CG）算法的模拟量子子程序，以提高生成样本的质量和多样性。

Method: 开发新的脉冲设计和嵌入策略，并提出了一种名为Make_Diff的贪婪后处理技术，该技术通过按位修改来处理退化样本，以返回非退化集合。

Result: 在不进行后处理的情况下，量子协议的性能与最佳经典方法相当或更差，因为量子协议经常生成高质量但退化的样本。在引入Make_Diff后处理技术后，量子协议在与精确求解器和Gurobi求解器的比较中表现出竞争力，并且在超过50%的合成实例中表现优于Gurobi。

Conclusion: 改进的混合CG方案在NISQ设备上具有部署潜力，能够解决工业相关的组合优化问题，并且对SPAM错误具有鲁棒性。

Abstract: In this work, we develop new pulse designs and embedding strategies to
improve the analog quantum subroutines of hybrid column generation (CG)
algorithms based on neutral-atoms quantum computers (NAQCs). These strategies
are designed to improve the quality and diversity of the samples generated. We
apply these to an important combinatorial optimization (CO) problem in
logistics, namely the fleet assignment. Depending on the instance tested, our
quantum protocol has a performance that is either comparable or worse than the
best classical method tested, both in terms of the number of iterations and
final objective value. We identify the cause of these suboptimal solutions as a
result of our quantum protocol often generating high-quality but degenerate
samples. We address this limitation by introducing a greedy post-processing
technique, Make\_Diff, which applies bit-wise modifications to degenerate
samples in order to return a non-degenerate set. With this modification, our
quantum protocol becomes competitive with an exact solver for the subproblem,
all the while being resilient to state preparation and measurements (SPAM)
errors. We also compare our CG scheme with a Gurobi solver and find that it
performs better on over 50\% of our synthetic instances and that, despite
Gurobi having a more extensive runtime. These improvements and benchmarks
herald the potential of deploying hybrid CG schemes on NISQ devices for
industrially relevant CO problems.

</details>


### [502] [Rapid Mixing of Quantum Gibbs Samplers for Weakly-Interacting Quantum Systems](https://arxiv.org/abs/2510.04954)
*Štěpán Šmíd,Richard Meister,Mario Berta,Roberto Bondesan*

Main category: quant-ph

TL;DR: 本文证明了用于吉布斯状态制备的算法Lindblad算子具有快速混合特性，在系统规模上具有多对数收敛性，并且比先前基于谱隙的方法具有指数级的改进。


<details>
  <summary>Details</summary>
Motivation: 吉布斯状态制备是许多多体系统量子算法的关键，但现有方法受限于系统大小相关的混合时间。

Method: 使用振荡器范数技术，并引入定制的变体，分析了算法Lindblad算子在非相互作用系统、自由费米子/玻色子以及弱相互作用/受扰系统中的混合时间。

Result: 证明了算法Lindblad算子在非相互作用和弱相互作用系统中具有多对数混合时间，在某些情况下比现有方法快指数级。

Conclusion: 本文提出的快速混合界限和分析技术为量子吉布斯状态制备提供了更优越、更鲁棒的方案，并有望扩展相关数学物理方法的使用范围。

Abstract: Dissipative quantum algorithms for state preparation in many-body systems are
increasingly recognised as promising candidates for achieving large quantum
advantages in application-relevant tasks. Recent advances in algorithmic,
detailed-balance Lindbladians enable the efficient simulation of open-system
dynamics converging towards desired target states. However, the overall
complexity of such schemes is governed by system-size dependent mixing times.
In this work, we analyse algorithmic Lindbladians for Gibbs state preparation
and prove that they exhibit rapid mixing, i.e., convergence in time
poly-logarithmic in the system size. We first establish this for
non-interacting spin systems, free fermions, and free bosons, and then show
that these rapid mixing results are stable under perturbations, covering weakly
interacting qudits and perturbed non-hopping fermions. Our results constitute
the first efficient mixing bounds for non-commuting qudit models and bosonic
systems at arbitrary temperatures. Compared to prior spectral-gap-based results
for fermions, we achieve exponentially faster mixing, further featuring
explicit constants on the maximal allowed interaction strength. This not only
improves the overall polynomial runtime for quantum Gibbs state preparation,
but also enhances robustness against noise. Our analysis relies on oscillator
norm techniques from mathematical physics, where we introduce tailored variants
adapted to specific Lindbladians $\unicode{x2014}$ an innovation that we expect
to significantly broaden the scope of these methods.

</details>


### [503] [Quantum Filtering at Finite Temperature](https://arxiv.org/abs/2510.04967)
*John Gough*

Main category: quant-ph

TL;DR: 本文研究了热态量子过程的连续时间量子滤波问题，并提出了解决方案。通过使用Araki-Woods表示法和Tomita-Takesaki理论，分析了与Fock真空情况不同的有限温度下的额外结构。该方法应用于Davies-Fulling-Unruh模型，将两种表示法解释为右侧和左侧Rindler楔形中的场，以探索量子轨迹。


<details>
  <summary>Details</summary>
Motivation: 研究量子过程在热态下的量子滤波问题，并解决连续时间测量（齐次测量）的情况。

Method: 使用Araki-Woods表示法处理测量的正交，并应用Tomita-Takesaki理论来处理有限温度下的额外结构，该结构比Fock真空情况下的对易子具有更丰富的结构。

Result: 将此方法应用于Davies-Fulling-Unruh模型，并将两种表示法解释为右侧和左侧Rindler楔形中的场，以研究量子轨迹。

Conclusion: 有限温度下的量子滤波具有比Fock真空情况更丰富的对易子结构，这可以通过将两种表示法解释为Rindler楔形中的场来探索量子轨迹。

Abstract: We pose and solve the problem of quantum filtering based on
continuous-in-time quadrature measurements (homodyning) for the case where the
quantum process is in a thermal state. The standard construction of quantum
filters involves the determination of the conditional expectation onto the von
Neumann algebra generated by the measured observables with the non-demolition
principle telling us to restrict the domain (the observables to be estimated)
to the commutant of the algebra. The finite-temperature case, however, has
additional structure: we use the Araki-Woods representation for the measured
quadratures, but the Tomita-Takesaki theory tells us that there exists a
separate, commuting representation and therefore the commutant will have a
richer structure than encountered in the Fock vacuum case. We apply this to the
question of quantum trajectories to the Davies-Fulling-Unruh model. Here, the
two representations are interpreted as the fields in the right and left Rindler
wedges.

</details>


### [504] [Quantum walks through generalized graph composition](https://arxiv.org/abs/2510.04973)
*Arjan Cornelissen*

Main category: quant-ph

TL;DR: 本文将图组合框架推广到非布尔设置，用超图表示量子算法，并统一了多种现有框架。


<details>
  <summary>Details</summary>
Motivation: 将图组合框架推广到非布尔设置，并统一现有的量子算法框架。

Method: 使用超图表示量子算法，并提供一种新的不可约、可逆马尔可夫过程分析方法。

Result: 实现了非布尔设置下的图组合框架，统一了量子分治、决策树和量子游走搜索框架，并改进了决策树的量子算法和量子游走搜索的摊销分析。

Conclusion: 新的框架简化了量子游走搜索算法的实现，实现了摊销加速，并消除了查询复杂度的对数因子。

Abstract: In this work, we generalize the recently-introduced graph composition
framework to the non-boolean setting. A quantum algorithm in this framework is
represented by a hypergraph, where each hyperedge is adjacent to multiple
vertices. The input and output to the quantum algorithm is represented by a set
of boundary vertices, and the hyperedges act like switches, connecting the
input vertex to the output that the algorithm computes.
  Apart from generalizing the graph composition framework, our new proposed
framework unifies the quantum divide and conquer framework, the decision-tree
framework, and the unified quantum walk search framework. For the decision
trees, we additionally construct a quantum algorithm from an improved weighting
scheme in the non-boolean case. For quantum walk search, we show how our
techniques naturally allow for amortization of the subroutines' costs. Previous
work showed how one can speed up ``detection'' of marked vertices by amortizing
the costs of the quantum walk. In this work, we extend these results to the
setting of ``finding'' such marked vertices, albeit in some restricted
settings.
  Along the way, we provide a novel analysis of irreducible, reversible Markov
processes, by linear-algebraically connecting its effective resistance to the
random walk operator. This significantly simplifies the algorithmic
implementation of the quantum walk search algorithm, achieves an amortization
speed-up for quantum walks over Johnson graphs, avoids the need for quantum
fast-forwarding, and removes the log-factors from the query complexity
statements.

</details>


### [505] [Less is More: On Copy Complexity in Quantum Cryptography](https://arxiv.org/abs/2510.04992)
*Prabhanjan Ananth,Eli Goldin*

Main category: quant-ph

TL;DR: 该论文提出了一种将单副本安全性提升至多副本安全性的通用方法，并将其应用于量子密码学的多个领域，包括伪随机状态生成器、伪随机酉变换和不可克隆原始元（如量子货币和量子复制保护）。


<details>
  <summary>Details</summary>
Motivation: 量子密码学的定义通常对攻击者可访问的密码学状态副本数量敏感，改变副本数量会严重影响计算难度、可行性和适用性。研究这一现象对于理解和改进量子密码学至关重要。

Method: 提出了一种通用的方法，将单副本安全性提升至多副本安全性，并将其应用于伪随机状态生成器、伪随机酉变换以及不可克隆的原始元（如量子货币和量子复制保护）。

Result: 1. 仅需一个副本的伪随机状态生成器（在温和假设下）可以推导出任意固定多项式数量副本的伪随机状态生成器。 2. 仅需一次查询的短密钥伪随机酉变换（在温和假设下）可以推导出任意固定多项式数量查询的短密钥伪随机酉变换。 3. 在可区分混淆和其他标准密码学假设下，存在与副本数量无关的、安全的不可克隆原始元，例如公钥量子货币和量子复制保护。

Conclusion: 研究结果表明，通过通用的方法可以将单副本安全性提升至多副本安全性，从而在量子密码学的不同领域（伪随机性、不可克隆性）获得更强的安全保证和更广泛的应用。

Abstract: Quantum cryptographic definitions are often sensitive to the number of copies
of the cryptographic states revealed to an adversary. Making definitional
changes to the number of copies accessible to an adversary can drastically
affect various aspects including the computational hardness, feasibility, and
applicability of the resulting cryptographic scheme. This phenomenon appears in
many places in quantum cryptography, including quantum pseudorandomness and
unclonable cryptography. To address this, we present a generic approach to
boost single-copy security to multi-copy security and apply this approach to
many settings. As a consequence, we obtain the following new results: -One-copy
stretch pseudorandom state generators (under mild assumptions) imply the
existence of t-copy stretch pseudorandom state generators, for any fixed
polynomial t. -One-query pseudorandom unitaries with short keys (under mild
assumptions) imply the existence of t-query pseudorandom unitaries with short
keys, for any fixed polynomial t. -Assuming indistinguishability obfuscation
and other standard cryptographic assumptions, there exist identical-copy secure
unclonable primitives such as public-key quantum money and quantum
copy-protection.

</details>


### [506] [Characterization of permutation gates in the third level of the Clifford hierarchy](https://arxiv.org/abs/2510.04993)
*Zhiyang He,Luke Robitaille,Xinyu Tan*

Main category: quant-ph

TL;DR: 本篇论文研究了量子计算中未完全理解的克利福德层级结构，重点在于刻画第三层级中的排列门。


<details>
  <summary>Details</summary>
Motivation: 克利福德层级是量子计算中的基本结构，但其数学性质尚未完全掌握。因此，有必要深入研究该结构。

Method: 研究者刻画了克利福德层级第三层级中的排列门，证明了任何第三层级的排列门都可以表示为特定形式（楼梯形式）的托佛利门乘积，并给出了排列门属于第三层级的充要条件。

Result: 论文给出了排列门属于第三层级的充要条件，并构造了一族非半克利福德排列门，它们属于第三层级，但其逆门不属于更高层级。

Conclusion: 本研究为理解克利福德层级中的排列门提供了新的见解，并为进一步研究量子计算的结构奠定了基础。

Abstract: The Clifford hierarchy is a fundamental structure in quantum computation
whose mathematical properties are not fully understood. In this work, we
characterize permutation gates -- unitaries which permute the $2^n$ basis
states -- in the third level of the hierarchy. We prove that any permutation
gate in the third level must be a product of Toffoli gates in what we define as
\emph{staircase form}, up to left and right multiplications by Clifford
permutations. We then present necessary and sufficient conditions for a
staircase form permutation gate to be in the third level of the Clifford
hierarchy. As a corollary, we construct a family of non-semi-Clifford
permutation gates $\{U_k\}_{k\geq 3}$ in staircase form such that each $U_k$ is
in the third level but its inverse is not in the $k$-th level.

</details>


### [507] [Correcting quantum errors using a classical code and one additional qubit](https://arxiv.org/abs/2510.05008)
*Tenzan Araki,Joseph F. Goodwin,Zhenyu Cai*

Main category: quant-ph

TL;DR: H-VEC协议可以将任何经典比特翻转码升级为可纠正任意 Pauli 噪声的量子纠错码，仅需一个辅助量子比特和两次受控-Hadamard 门操作。


<details>
  <summary>Details</summary>
Motivation: 现有经典纠错码无法应对包含比特翻转和相位翻转的量子噪声。

Method: H-VEC协议通过经典后处理，将噪声投影为Y型错误，然后利用经典码的解码算法进行纠正。

Result: 将H-VEC应用于经典重复码，在容量噪声模型下，实现了完全的量子保护，并且错误抑制能力（相对于码距）呈指数增长，优于表面码，同时使用的量子比特更少，检查更简单，解码更直接。

Conclusion: H-VEC协议是一种新的混合量子纠错和纠错缓解框架，它重新定义了物理硬件需求和经典处理在错误抑制方面的权衡，但存在采样开销。

Abstract: Classical error-correcting codes are powerful but incompatible with quantum
noise, which includes both bit-flips and phase-flips. We introduce
Hadamard-based Virtual Error Correction (H-VEC), a protocol that empowers any
classical bit-flip code to correct arbitrary Pauli noise with the addition of
only a single ancilla qubit and two layers of controlled-Hadamard gates.
Through classical post-processing, H-VEC virtually filters the error channel,
projecting the noise into pure Y-type errors that are subsequently corrected
using the classical code's native decoding algorithm. We demonstrate this by
applying H-VEC to the classical repetition code. Under a code-capacity noise
model, the resulting protocol not only provides full quantum protection but
also achieves an exponentially stronger error suppression (in distance) than
the original classical code, and even larger improvements over the surface code
while using much fewer qubits, simpler checks and straight-forward decoding.
H-VEC comes with a sampling overhead due to its post-processing nature. It
represents a new hybrid quantum error correction and mitigation framework that
redefines the trade-offs between physical hardware requirements and classical
processing for error suppression.

</details>


### [508] [Optimización de la Transmisión de Estados Cuánticos en Cadenas de Qubits usando Deep Reinforcement Learning y Algoritmos Genéticos](https://arxiv.org/abs/2510.05010)
*Sofía Perón Santana,Ariel Fiuri,Omar Osenda,Martín Domínguez*

Main category: quant-ph

TL;DR: 通过使用深度强化学习和遗传算法，优化了量子态传输的磁脉冲序列，以提高效率和适应物理约束。


<details>
  <summary>Details</summary>
Motivation: 量子态传输（QST）是构建可扩展量子硬件的关键，需要最小化传输时间和避免信息损失。

Method: 使用恒定磁脉冲，并采用深度强化学习（智能体通过奖励学习脉冲序列）和遗传算法（通过选择和变异开发候选解决方案）两种互补策略。

Result: 分析了两种方法的效率及其整合物理约束的能力。

Conclusion: 通过深度强化学习和遗传算法优化脉冲序列，可以提高量子态传输的效率和鲁棒性。

Abstract: Quantum state transfer (QST) via homogeneous spin chains plays a crucial role
in building scalable quantum hardware. A basic quantum state transmission
protocol prepares a state in one qubit and transfers it to another through a
channel, seeking to minimize the time and avoid information loss. The fidelity
of the process is measured by functions proportional to the transition
probability between both states. We approach this optimization problem using
constant magnetic pulses and two complementary strategies: deep reinforcement
learning, where an agent learns pulse sequences through rewards, and genetic
algorithms, which develop candidate solutions through selection and mutation.
We analyze the efficiency of both methods and their ability to incorporate
physical constraints.

</details>


### [509] [On Cryptography and Distribution Verification, with Applications to Quantum Advantage](https://arxiv.org/abs/2510.05028)
*Bruno Cavalar,Eli Goldin,Matthew Gray,Taiga Hiroka,Tomoyuki Morimae*

Main category: quant-ph

TL;DR: 本研究探讨了在分布支持集大小呈指数级增长时，恒等性检验样本复杂度的界限问题。研究提出了在特定受限场景下绕过此界限的方法，并分析了在经典和量子计算模型下，分布的可验证性与密码学（如单向函数）之间的关系。


<details>
  <summary>Details</summary>
Motivation: 恒等性检验是假设检验中的一个基本问题，即判断样本是否来自某个已知分布 $\mathcal{D}$。当分布 $\mathcal{D}$ 的支持集大小为 $N$ 时，最优样本复杂度约为 $O(\sqrt{N})$。然而，许多实际分布（包括可有效采样分布）具有指数级的支持集大小，导致最优恒等性检验需要指数级样本。本研究旨在突破这一限制。

Method: 本研究通过考虑受限场景来绕过样本复杂度的下界。具体来说，它区分了能够抵抗任何分布（包括无法有效采样的分布）的检验，以及只需要抵抗可有效采样分布的检验。研究将可有效采样分布的恒等性检验问题与密码学联系起来，并分析了经典/量子分布的可验证性。

Result: (i) 每一个量子可采样分布都可以用一个 $\mathbf{P^{PP}}$ 算法进行验证。
(ii) 如果单向函数存在，那么不存在充分随机的可经典采样分布能够被有效验证。
(iii) 如果单向函数不存在，那么每一个可经典采样分布都可以被有效验证。
(iv) 如果 QEFID 对存在，那么存在一个量子可采样分布，它无法被有效验证。
(v) 如果单向谜题不存在，那么具有采样优势的量子分布可以用高效的量子计算机进行验证。

Conclusion: 本研究揭示了在指数级支持集大小的分布下，恒等性检验的样本复杂度问题，并通过引入受限场景提出了解决方案。研究将分布的可验证性与单向函数、量子计算等前沿领域联系起来，提供了关于可验证性与密码学假设之间关系的深刻见解。

Abstract: One of the most fundamental problems in the field of hypothesis testing is
the identity testing problem: whether samples from some unknown distribution
$\mathcal{G}$ are actually from some explicit distribution $\mathcal{D}$. It is
known that when the distribution $\mathcal{D}$ has support $[N]$, the optimal
sample complexity for the identity testing problem is roughly $O(\sqrt{N})$.
However, many distributions of interest, including those which can be sampled
efficiently, have exponential support size, and therefore the optimal identity
tester also requires exponential samples. In this paper, we bypass this lower
bound by considering restricted settings. The above $O(\sqrt{N})$ sample
complexity identity tester is constructed so that it is not fooled by any (even
inefficiently-sampled) distributions. However, in most applications, the
distributions under consideration are efficiently sampleable, and therefore it
is enough to consider only identity testers that are not fooled by
efficiently-sampled distributions. In that case, we can focus on efficient
verification with efficient identity testers. We investigate relations between
efficient verifications of classical/quantum distributions and
classical/quantum cryptography, and show the following results: (i) Every
quantumly samplable distribution is verifiable with a $\mathbf{P^{PP}}$
algorithm. (ii) If one-way functions exist, then no sufficiently random
classically samplable distribution is efficiently verifiable. (iii) If one-way
functions do not exist, then every classically samplable distribution is
efficiently verifiable. (iv) If QEFID pairs exist, then there exists a
quantumly samplable distribution which is not efficiently verifiable. (v) If
one-way puzzles do not exist, then it is possible to verify sampling-based
quantum advantage with a efficient quantum computer.

</details>


### [510] [On the Cryptographic Futility of Non-Collapsing Measurements](https://arxiv.org/abs/2510.05055)
*Alper Cakan,Dakshita Khurana,Tomoyuki Morimae,Yuki Shirakawa,Kabir Tomer,Takashi Yamakawa*

Main category: quant-ph

TL;DR: 本研究探讨了量子碰撞抵抗性的概念，并在量子电路和量子态的定义下，得到了量子单向原语和量子抗碰撞原语之间的分离结果。


<details>
  <summary>Details</summary>
Motivation: 研究量子原语之间的关系，特别是单向性和抗碰撞性之间的区别，以理解量子计算安全性的基本限制。

Method: 研究了两种不同模型下的量子单向性和抗碰撞性：1. 定义在量子电路（输出经典字符串）上的情况，引入了非塌缩测量预言机$\\

Result: 1. 在经典预言机$\\

Conclusion: 本研究在两个不同模型下证明了量子单向性与量子抗碰撞性之间的分离，即存在一些仅能通过量子算法（但不能通过全黑盒量子构造）实现的密码学原语。这对于理解量子计算中的安全模型和构造安全的量子密码学系统具有重要意义。

Abstract: We investigate quantum analogues of collision resistance and obtain
separations between quantum ``one-way'' and ``collision-resistant'' primitives.
  1. Our first result studies one-wayness versus collision-resistance defined
over quantum circuits that output classical strings. We show that there is a
classical oracle $\mathcal{O}$ relative to which (sub-exponentially secure)
indistinguishability obfuscation and one-way permutations exist even against
adversaries that make quantum queries to a non-collapsing measurement oracle,
$\mathcal{Q}^{\mathcal{O}}$. Very roughly, $\mathcal{Q}^{\mathcal{O}}$ outputs
the result of multiple non-collapsing measurements on the output of any quantum
$\mathcal{O}$-aided circuit.
  This rules out fully black-box {\em quantum} constructions of $Y$ from $X$
for any $X \in \{$indistinguishability obfuscation and one-way permutations,
public-key encryption, deniable encryption, oblivious transfer, non-interactive
ZK, trapdoor permutations, quantum money$\}, Y \in \{$collision-resistant hash
functions, hard problems in SZK, homomorphic encryption, distributional
collision-resistant puzzles$\}$.
  2. Our second result studies one-wayness versus collision-resistance defined
over quantum states. Here, we show that relative to the same classical oracle
$\mathcal{O}$, (sub-exponentially secure) indistinguishability obfuscation and
one-way permutations exist even against adversaries that make quantum queries
to a {\em cloning unitary} $\mathsf{QCol}^\mathcal{O}$. Very roughly, this
latter oracle implements a well-defined, linear operation to clone a subset of
the qubits output by any quantum $\mathcal{O}$-aided circuit.
  This rules out fully black-box constructions of quantum lightning from
public-key quantum money.

</details>


### [511] [The role of entropy production and thermodynamic uncertainty relations in the thermalization of open quantum systems](https://arxiv.org/abs/2510.05072)
*Álvaro Tejero*

Main category: quant-ph

TL;DR: 开放量子系统中的热化不对称性源于熵产生率和量子热力学不确定性关系（TKUR）。熵产生率越高，热化越快，熵产生率越高可以抑制热流的波动，使加热比冷却更稳定。


<details>
  <summary>Details</summary>
Motivation: 开放量子系统中的加热和冷却之间的不对称性是处于非平衡态的动力学的标志，但其热力学起源尚不清楚。

Method: 推导了熵产生率的解析表达式，并利用量子热力学不确定性关系（TKUR）将这种不对称性与热流的波动联系起来。

Result: 加热的熵产生率高于冷却，导致热化速度更快。熵产生率的增加抑制了热流的波动，使得加热比冷却更稳定。

Conclusion: 揭示了热化不对称性的热力学基础，并强调不确定性关系在非平衡量子动力学中的关键作用。

Abstract: The asymmetry between heating and cooling in open quantum systems is a
hallmark of nonequilibrium dynamics, yet its thermodynamic origin has remained
unclear. Here, we investigate the thermalization of a quantum system weakly
coupled to a thermal bath, focusing on the entropy production rate and the
quantum thermokinetic uncertainty relation (TKUR). We derive an analytical
expression for the entropy production rate, showing that heating begins with a
higher entropy production, which drives faster thermalization than cooling. The
quantum TKUR links this asymmetry to heat current fluctuations, demonstrating
that larger entropy production suppresses fluctuations, making heating more
stable than cooling. Our results reveal the thermodynamic basis of asymmetric
thermalization and highlight uncertainty relations as key to nonequilibrium
quantum dynamics.

</details>


### [512] [Engineering the uncontrollable: Steering noisy spin-correlated radical-pairs with coherent and incoherent control](https://arxiv.org/abs/2510.05074)
*Farhan T. Chowdhury,Luke D. Smith,Daniel R. Kattnig*

Main category: quant-ph

TL;DR: 利用庞特里亚金最大值原理（PMP）优化控制方法，实现了对含噪声量子自旋的回旋运动的精确操控，为设计抗噪声量子信息处理器和量子传感器提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 需要精确控制光激发自由基反应中电子自旋动力学和弛豫过程的相互作用，以实现磁场效应的靶向操控。

Method: 采用基于庞特里亚金最大值原理（PMP）的控制工程方法，对含噪声自由基对的相干和非相干自旋动力学进行最优控制。

Result: 成功应用PMP最优控制技术，实现了对原型自由基对模型中相干和非相干自旋动力学的有效操控，并证明了控制策略在面对噪声时的鲁棒性。

Conclusion: PMP最优控制方法为处理复杂开放量子系统提供了一种可行的数值计算途径，能够实现对自由基对自旋动力学的相干和非相干控制，并具备良好的鲁棒性。

Abstract: The quantum control of spin-correlated radical pairs (SCRPs) holds promise
for the targeted manipulation of magnetic field effects, with potential
applications ranging from the design of noise-resilient quantum information
processors to genetically encodable quantum sensors. However, achieving precise
handles over the intricate interplay between coherent electron spin dynamics
and incoherent relaxation processes in photoexcited radical-pair reactions
requires tractable approaches for numerically obtaining controls for large,
complex open quantum systems. Employing techniques relying on full
Liouville-space propagators becomes computationally infeasible for large spin
systems of realistic complexity. Here, we demonstrate how a control engineering
approach based on the Pontryagin Maximum Principle (PMP) can offer a viable
alternative by reporting on the successful application of PMP-optimal control
to steer the coherent and incoherent spin dynamics of noisy radical pairs. This
enables controls for prototypical radical-pair models that exhibit robustness
in the face of relevant noise sources and paves the way to incoherent control
of radical-pair spin dynamics.

</details>


### [513] [On the Cryptographic Foundations of Interactive Quantum Advantage](https://arxiv.org/abs/2510.05082)
*Kabir Tomer,Mark Zhandry*

Main category: quant-ph

TL;DR: 本工作研究了实现量子证明（PoQ）所需的硬度，而量子证明又可以捕捉（潜在的交互式）量子优势。


<details>
  <summary>Details</summary>
Motivation: 吸引人之处在于“非平凡”的量子证明（PoQ），它们依赖于量子硬度假设，并且是更复杂的协议（如经典量子计算验证（CVQC））的起点。

Method: 研究了实现非平凡量子证明（PoQ）所需的硬度的几个下界。

Result: 我们证明了实现非平凡量子证明（PoQ）可能需要密码学硬度，并且不同的非平凡量子证明（PoQ）变体需要不同类型的密码学硬度。

Conclusion: 我们的结果有助于解释在使用格（lattices）构建可公开验证的量子证明（PoQ）及其各种扩展（如 CVQC）方面存在的挑战。

Abstract: In this work, we study the hardness required to achieve proofs of quantumness
(PoQ), which in turn capture (potentially interactive) quantum advantage. A
``trivial'' PoQ is to simply assume an average-case hard problem for classical
computers that is easy for quantum computers. However, there is much interest
in ``non-trivial'' PoQ that actually rely on quantum hardness assumptions, as
these are often a starting point for more sophisticated protocols such as
classical verification of quantum computation (CVQC). We show several
lower-bounds for the hardness required to achieve non-trivial PoQ, specifically
showing that they likely require cryptographic hardness, with different types
of cryptographic hardness being required for different variations of
non-trivial PoQ. In particular, our results help explain the challenges in
using lattices to build publicly verifiable PoQ and its various extensions such
as CVQC.

</details>


### [514] [QuantumBoost: A lazy, yet fast, quantum algorithm for learning with weak hypotheses](https://arxiv.org/abs/2510.05089)
*Amira Abbas,Yanlin Chen,Tuyen Nguyen,Ronald de Wolf*

Main category: quant-ph

TL;DR: QuantumBoost通过结合量子算法和延迟投影策略，实现了比现有提升方法更优的运行时间。


<details>
  <summary>Details</summary>
Motivation: 提升算法通过结合多个弱学习器来提高决策质量，但现有算法在运行时间方面有改进空间。

Method: QuantumBoost算法引入了两个创新点：1. 使用量子算法加速近似Bregman投影的计算；2. 结合了延迟投影策略，即不频繁地执行投影操作。

Result: QuantumBoost实现了已知的最佳运行时间，优于其他提升方法。

Conclusion: QuantumBoost是第一个在提升算法中成功应用延迟投影策略的算法（包括经典和量子算法），并且实现了最优运行时间。

Abstract: The technique of combining multiple votes to enhance the quality of a
decision is the core of boosting algorithms in machine learning. In particular,
boosting provably increases decision quality by combining multiple weak
learners-hypotheses that are only slightly better than random guessing-into a
single strong learner that classifies data well. There exist various versions
of boosting algorithms, which we improve upon through the introduction of
QuantumBoost. Inspired by classical work by Barak, Hardt and Kale, our
QuantumBoost algorithm achieves the best known runtime over other boosting
methods through two innovations. First, it uses a quantum algorithm to compute
approximate Bregman projections faster. Second, it combines this with a lazy
projection strategy, a technique from convex optimization where projections are
performed infrequently rather than every iteration. To our knowledge,
QuantumBoost is the first algorithm, classical or quantum, to successfully
adopt a lazy projection strategy in the context of boosting.

</details>


### [515] [Simulating fermions with exponentially lower overhead](https://arxiv.org/abs/2510.05099)
*Nathan Constantinides,Jeffery Yu,Dhruv Devulapalli,Ali Fahimniya,Andrew M. Childs,Michael J. Gullans,Alexander Schuckert,Alexey V. Gorshkov*

Main category: quant-ph

TL;DR: 通过利用Jordan-Wigner编码中的量子比特排列，我们指数级地降低了模拟费米子哈密顿量的电路深度开销，无需额外的量子比特，并将此方法推广到其他编码方式，同时还改进了费米子快速傅里叶变换的实现。


<details>
  <summary>Details</summary>
Motivation: 模拟费米子哈密顿量的系统演化是量子计算在材料和分子性质预测中的核心应用，但现有的模拟方法在深度开销或空间开销上存在不足。

Method: 研究提出了利用Jordan-Wigner编码中的量子比特排列来模拟费米子演化的新方法，将无辅助量子比特的深度开销从O(N)降低至O(log^2 N)，并推广到其他编码方式。同时，在引入辅助量子比特和中途测量/前馈后，开销降至O(log N)。此外，该方法还被用于实现费米子快速傅里叶变换。

Result: 在无辅助量子比特的情况下，模拟深度开销从O(N)指数级降低到O(log^2 N)。引入辅助量子比特和中途测量/前馈后，开销降至O(log N)。费米子快速傅里叶变换的实现开销在无辅助量子比特时为Θ(log N)，有辅助量子比特时为Θ(1)。

Conclusion: 与先前认为的相比，使用量子比特量子计算机模拟费米子所带来的渐近开销要小得多。

Abstract: Simulating time evolution under fermionic Hamiltonians is a compelling
application of quantum computers because it lies at the core of predicting the
properties of materials and molecules. Fermions can be simulated on qubit-based
quantum computers using a fermion-to-qubit mapping, subject to an overhead --
the circuit depth on a qubit quantum computer divided by that on a quantum
computer built from native fermionic modes -- at worst scaling linearly with
the number of modes $N$. Existing approaches that lower this depth overhead
usually trade it for space, using $O(N)$ ancilla qubits. We exponentially
reduce the worst-case overhead of ancilla-free fermion-to-qubit mappings to
$O(\log^2 N)$ by constructing circuits that perform any fermionic permutation
on qubits in the Jordan-Wigner encoding in depth $O(\log^2 N)$. We also show
that our result generalizes to permutations in any product-preserving ternary
tree fermionic encoding. When introducing $O(N)$ ancillas and mid-circuit
measurement and feedforward, the overhead reduces to $O(\log N)$. Finally, we
show that our scheme can be used to implement the fermionic fast Fourier
transform, a key subroutine in chemistry simulation, with overhead $\Theta(\log
N)$ without ancillas and $\Theta(1)$ with ancillas, improving exponentially
over the best previously known ancilla-free algorithm with overhead scaling
linearly with $N$. Our results show that simulating fermions with qubit quantum
computers comes at a much lower asymptotic overhead than previously thought.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [516] [Creative synthesis of kinematic mechanisms](https://arxiv.org/abs/2510.03308)
*Jiong Lin,Jialong Ning,Judah Goldfeder,Hod Lipson*

Main category: cs.GR

TL;DR: 本论文将平面连杆的运动综合问题视为跨域图像生成任务，并构建了一个包含多种机械装置（从曲柄摇杆、曲柄滑块到詹森机构等八杆连杆）的RGB图像表示数据集。


<details>
  <summary>Details</summary>
Motivation: 将平面连杆的运动综合问题转化为跨域图像生成任务，并探索使用图像生成模型来合成新的运动曲线和模拟新的运动学。

Method: 使用共享隐变量变分自编码器（VAE）来处理以RGB图像表示的平面连杆数据集，并将轨迹点的绘制速度编码为颜色梯度，从而实现基于轨迹形状和速度剖面的运动学综合。

Result: 在标准四杆连杆、混合四杆/曲柄滑块连杆以及包含多回路的复杂机械装置数据集上验证了所提出方法的有效性，初步证明了基于图像表示在生成式机械设计中的潜力。

Conclusion: 基于图像的表示方法可用于生成式机械设计，并且可以在统一的图像生成框架内表示和综合包含转动副、移动副，甚至凸轮和齿轮的机械装置。

Abstract: In this paper, we formulate the problem of kinematic synthesis for planar
linkages as a cross-domain image generation task. We develop a planar linkages
dataset using RGB image representations, covering a range of mechanisms: from
simple types such as crank-rocker and crank-slider to more complex eight-bar
linkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE)
is employed to explore the potential of image generative models for
synthesizing unseen motion curves and simulating novel kinematics. By encoding
the drawing speed of trajectory points as color gradients, the same
architecture also supports kinematic synthesis conditioned on both trajectory
shape and velocity profiles. We validate our method on three datasets of
increasing complexity: a standard four-bar linkage set, a mixed set of four-bar
and crank-slider mechanisms, and a complex set including multi-loop mechanisms.
Preliminary results demonstrate the effectiveness of image-based
representations for generative mechanical design, showing that mechanisms with
revolute and prismatic joints, and potentially cams and gears, can be
represented and synthesized within a unified image generation framework.

</details>


### [517] [Universal Beta Splatting](https://arxiv.org/abs/2510.03312)
*Rong Liu,Zhongpai Gao,Benjamin Planche,Meida Chen,Van Nguyen Nguyen,Meng Zheng,Anwesa Choudhuri,Terrence Chen,Yue Wang,Andrew Feng,Ziyan Wu*

Main category: cs.GR

TL;DR: Universal Beta Splatting (UBS) 是一种新的3D渲染框架，它将3D高斯泼溅推广到N维各向异性Beta核，用于显式辐射场渲染。


<details>
  <summary>Details</summary>
Motivation: Beta核能够建模空间、角度和时间维度内的依赖关系，捕捉复杂的光线传输和各向异性的视图相关外观，以及场景动态，无需额外的网络或颜色编码。UBS向后兼容高斯泼溅，并可自然地将场景属性分解为可解释的部分（空间、角度、时间）。

Method: UBS使用N维各向异性Beta核作为基本图元，构建了一个统一的框架，用于显式辐射场渲染。

Result: UBS在静态、视图相关和动态基准测试中都优于现有方法，实现了实时渲染，并能自然地将场景属性分解为可解释的部分。

Conclusion: Beta核是一种可扩展的通用图元，适用于辐射场渲染，UBS为该领域提供了一个统一且强大的解决方案。

Abstract: We introduce Universal Beta Splatting (UBS), a unified framework that
generalizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for
explicit radiance field rendering. Unlike fixed Gaussian primitives, Beta
kernels enable controllable dependency modeling across spatial, angular, and
temporal dimensions within a single representation. Our unified approach
captures complex light transport effects, handles anisotropic view-dependent
appearance, and models scene dynamics without requiring auxiliary networks or
specific color encodings. UBS maintains backward compatibility by approximating
to Gaussian Splatting as a special case, guaranteeing plug-in usability and
lower performance bounds. The learned Beta parameters naturally decompose scene
properties into interpretable without explicit supervision: spatial (surface
vs. texture), angular (diffuse vs. specular), and temporal (static vs.
dynamic). Our CUDA-accelerated implementation achieves real-time rendering
while consistently outperforming existing methods across static,
view-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable
universal primitive for radiance field rendering. Our project website is
available at https://rongliu-leo.github.io/universal-beta-splatting/.

</details>


### [518] [Style Brush: Guided Style Transfer for 3D Objects](https://arxiv.org/abs/2510.03433)
*Áron Samuel Kovács,Pedro Hermosilla,Renata G. Raidou*

Main category: cs.GR

TL;DR: Style Brush是一种新颖的纹理网格风格迁移方法，通过引入新的损失函数来捕捉风格方向性，支持多种风格图像，并实现风格间的平滑过渡，使用户能够精细控制风格化过程。


<details>
  <summary>Details</summary>
Motivation: 为艺术家提供对风格化过程的精细控制能力，扩展传统3D风格迁移方法。

Method: 引入新的损失函数来捕捉风格方向性，支持使用多个风格图像或其一部分，并实现合成纹理中风格的平滑过渡。使用易于生成的引导纹理简化用户交互。

Result: 生成的纹理具有视觉吸引力，方法具有灵活性，经过各种网格、风格图像和轮廓形状的广泛评估。

Conclusion: Style Brush是一种灵活且用户友好的3D风格迁移方法，能够生成视觉上吸引人的纹理。

Abstract: We introduce Style Brush, a novel style transfer method for textured meshes
designed to empower artists with fine-grained control over the stylization
process. Our approach extends traditional 3D style transfer methods by
introducing a novel loss function that captures style directionality, supports
multiple style images or portions thereof, and enables smooth transitions
between styles in the synthesized texture. The use of easily generated guiding
textures streamlines user interaction, making our approach accessible to a
broad audience. Extensive evaluations with various meshes, style images, and
contour shapes demonstrate the flexibility of our method and showcase the
visual appeal of the generated textures.

</details>


### [519] [Paris: A Decentralized Trained Open-Weight Diffusion Model](https://arxiv.org/abs/2510.03434)
*Zhiying Jiang,Raihan Seraj,Marcos Villagra,Bidhan Roy*

Main category: cs.GR

TL;DR: Paris是一个完全通过去中心化计算预训练的文本到图像生成模型，证明了高质量生成能力可在无中心化协调下实现。


<details>
  <summary>Details</summary>
Motivation: 探索在没有中心化协调基础设施的情况下实现高质量文本到图像生成的可行性，并为开放研究和商业用途提供模型。

Method: 开发了一个名为Distributed Diffusion Training的框架，训练了8个独立的专家扩散模型，每个模型在各自的数据子集上进行优化，并通过一个轻量级Transformer路由器在推理时动态选择专家。

Result: Paris在生成质量上可与中心化协调的模型相媲美，且无需专门的GPU集群，同时在训练数据和计算量上均有显著减少（14倍数据，16倍计算量）。

Conclusion: 去中心化训练是一种可行且高效的方法，可以用于训练大型扩散模型，具有数据和计算效率优势。

Abstract: We present Paris, the first publicly released diffusion model pre-trained
entirely through decentralized computation. Paris demonstrates that
high-quality text-to-image generation can be achieved without centrally
coordinated infrastructure. Paris is open for research and commercial use.
Paris required implementing our Distributed Diffusion Training framework from
scratch. The model consists of 8 expert diffusion models (129M-605M parameters
each) trained in complete isolation with no gradient, parameter, or
intermediate activation synchronization. Rather than requiring synchronized
gradient updates across thousands of GPUs, we partition data into semantically
coherent clusters where each expert independently optimizes its subset while
collectively approximating the full distribution. A lightweight transformer
router dynamically selects appropriate experts at inference, achieving
generation quality comparable to centrally coordinated baselines. Eliminating
synchronization enables training on heterogeneous hardware without specialized
interconnects. Empirical validation confirms that Paris's decentralized
training maintains generation quality while removing the dedicated GPU cluster
requirement for large-scale diffusion models. Paris achieves this using
14$\times$ less training data and 16$\times$ less compute than the prior
decentralized baseline.

</details>


### [520] [Neon: Negative Extrapolation From Self-Training Improves Image Generation](https://arxiv.org/abs/2510.03597)
*Sina Alemohammad,Zhangyang Wang,Richard G. Baraniuk*

Main category: cs.GR

TL;DR: 该研究提出了一种名为Neon的新方法，通过反转梯度更新来利用合成数据训练过程中出现的模型退化，以提升模型性能，解决了生成模型训练中数据稀缺和模型崩溃的问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型的训练受限于高质量训练数据的稀缺性。利用合成数据来增强有限的真实数据语料库进行微调，虽然有潜力提高性能，但容易导致模型退化（模型崩溃）。

Method: Neon方法首先在一个基础模型上使用其自身合成的数据进行微调，然后反转梯度更新，使模型权重偏离退化后的状态。这种负向外推利用了推理采样器偏好高概率区域的特点，纠正了合成数据与真实数据群体梯度之间的反向对齐，使模型更好地拟合真实数据分布。

Result: Neon方法实现简单，只需在现有模型上进行简单的后处理合并即可，无需新的真实数据，并且仅需少量合成数据（低至1k）和少量额外计算资源（通常低于1%）。该方法在多种架构（包括扩散模型、流匹配、自回归模型和归纳矩匹配模型）和数据集（ImageNet、CIFAR-10和FFHQ）上均表现出普适性。特别是在ImageNet 256x256数据集上，Neon将xAR-L模型的FID提升至新的 state-of-the-art 水平（1.02），而仅使用了0.36%的额外训练计算量。

Conclusion: Neon是一种有效且计算效率高的方法，能够将生成模型在自训练过程中出现的退化转化为改进模型性能的信号，解决了模型崩溃问题，并在多种模型和数据集上取得了优异的成果。

Abstract: Scaling generative AI models is bottlenecked by the scarcity of high-quality
training data. The ease of synthesizing from a generative model suggests using
(unverified) synthetic data to augment a limited corpus of real data for the
purpose of fine-tuning in the hope of improving performance. Unfortunately,
however, the resulting positive feedback loop leads to model autophagy disorder
(MAD, aka model collapse) that results in a rapid degradation in sample quality
and/or diversity. In this paper, we introduce Neon (for Negative Extrapolation
frOm self-traiNing), a new learning method that turns the degradation from
self-training into a powerful signal for self-improvement. Given a base model,
Neon first fine-tunes it on its own self-synthesized data but then,
counterintuitively, reverses its gradient updates to extrapolate away from the
degraded weights. We prove that Neon works because typical inference samplers
that favor high-probability regions create a predictable anti-alignment between
the synthetic and real data population gradients, which negative extrapolation
corrects to better align the model with the true data distribution. Neon is
remarkably easy to implement via a simple post-hoc merge that requires no new
real data, works effectively with as few as 1k synthetic samples, and typically
uses less than 1% additional training compute. We demonstrate Neon's
universality across a range of architectures (diffusion, flow matching,
autoregressive, and inductive moment matching models) and datasets (ImageNet,
CIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the
xAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional
training compute. Code is available at https://github.com/SinaAlemohammad/Neon

</details>


### [521] [Diverse Text-to-Image Generation via Contrastive Noise Optimization](https://arxiv.org/abs/2510.03813)
*Byungjun Kim,Soobin Um,Jong Chul Ye*

Main category: cs.GR

TL;DR: 对比噪声优化是一种通过优化初始噪声来提高文本到图像生成模型多样性的新方法，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在强文本指导下存在输出多样性不足的问题，而现有方法效果有限且对超参数敏感。

Method: 提出对比噪声优化（CNO）方法，通过在Tweedie数据空间定义对比损失来优化初始噪声，以生成更多样化的输出。该方法通过排斥批次内的样本来最大化多样性，同时将它们锚定在参考样本附近以保持保真度。

Result: 在多个文本到图像骨干网络上的大量实验表明，该方法在质量-多样性权衡方面优于现有技术，并且对超参数不敏感。

Conclusion: 对比噪声优化是一种简单有效的方法，可以解决文本到图像生成模型的多样性问题，并提供理论支持其有效性。

Abstract: Text-to-image (T2I) diffusion models have demonstrated impressive performance
in generating high-fidelity images, largely enabled by text-guided inference.
However, this advantage often comes with a critical drawback: limited
diversity, as outputs tend to collapse into similar modes under strong text
guidance. Existing approaches typically optimize intermediate latents or text
conditions during inference, but these methods deliver only modest gains or
remain sensitive to hyperparameter tuning. In this work, we introduce
Contrastive Noise Optimization, a simple yet effective method that addresses
the diversity issue from a distinct perspective. Unlike prior techniques that
adapt intermediate latents, our approach shapes the initial noise to promote
diverse outputs. Specifically, we develop a contrastive loss defined in the
Tweedie data space and optimize a batch of noise latents. Our contrastive
optimization repels instances within the batch to maximize diversity while
keeping them anchored to a reference sample to preserve fidelity. We further
provide theoretical insights into the mechanism of this preprocessing to
substantiate its effectiveness. Extensive experiments across multiple T2I
backbones demonstrate that our approach achieves a superior quality-diversity
Pareto frontier while remaining robust to hyperparameter choices.

</details>


### [522] [Joint Neural SDF Reconstruction and Semantic Segmentation for CAD Models](https://arxiv.org/abs/2510.03837)
*Shen Fan,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 该研究提出了一种新颖的、数据高效的流水线，通过结合基于神经SDF的CAD部件隐式重建网络和一个部件分割头，实现了对任意数量部件的CAD网格进行几何对齐的语义结构化。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于固定的部件分类法，而本研究旨在实现一个能够处理任意数量部件的CAD网格，并生成连贯、几何对齐标签的系统。

Method: 提出了一种数据高效的流水线，将基于神经SDF的CAD部件隐式重建网络与一个在PartField生成的监督下训练的部件分割头相结合。

Result: 在ABC数据集的CAD网格上进行了评估，在重建（CDL1/CDL2, F1-micro, NC）和分割（mIoU, Accuracy）任务上均取得了强大的性能，并提出了一种新的分割一致性度量。实验表明，即使在重建效果不佳的情况下，分割仍然准确且标签连贯，并且能够保持正确的部件数量。

Conclusion: 该方法为实现语义结构化的CAD网格提供了一条实用的途径，无需预先定义的分类法或精确的颜色匹配。然而，在边界精度方面存在局限性，并提出了未来改进的方向。

Abstract: We propose a simple, data-efficient pipeline that augments an implicit
reconstruction network based on neural SDF-based CAD parts with a
part-segmentation head trained under PartField-generated supervision. Unlike
methods tied to fixed taxonomies, our model accepts meshes with any number of
parts and produces coherent, geometry-aligned labels in a single pass. We
evaluate on randomly sampled CAD meshes from the ABC dataset with intentionally
varied part cardinalities, including over-segmented shapes, and report strong
performance across reconstruction (CDL1/CDL2, F1-micro, NC) and segmentation
(mIoU, Accuracy), together with a new Segmentation Consistency metric that
captures local label smoothness. We attach a lightweight segmentation head to
the Flat-CAD SDF trunk; on a paired evaluation it does not alter reconstruction
while providing accurate part labels for meshes with any number of parts. Even
under degraded reconstructions on thin or intricate geometries, segmentation
remains accurate and label-coherent, often preserving the correct part count.
Our approach therefore offers a practical route to semantically structured CAD
meshes without requiring curated taxonomies or exact palette matches. We
discuss limitations in boundary precision, partly due to per-face supervision,
and outline paths toward boundary-aware training and higher resolution labels.

</details>


### [523] [Enhancing Foveated Rendering with Weighted Reservoir Sampling](https://arxiv.org/abs/2510.03964)
*Ville Cantory,Darya Biparva,Haoyu Tan,Tongyu Nie,John Schroeder,Ruofei Du,Victoria Interrante,Piotr Didyk*

Main category: cs.GR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Spatiotemporal sensitivity to high frequency information declines with
increased peripheral eccentricity. Foveated rendering exploits this by
decreasing the spatial resolution of rendered images in peripheral vision,
reducing the rendering cost by omitting high frequency details. As foveation
levels increase, the rendering quality is reduced, and traditional foveated
rendering systems tend not to preserve samples that were previously rendered at
high spatial resolution in previous frames. Additionally, prior research has
shown that saccade landing positions are distributed around a target location
rather than landing at a single point, and that even during fixations, eyes
perform small microsaccades around a fixation point. This creates an
opportunity for sampling from temporally neighbouring frames with differing
foveal locations to reduce the required rendered size of the foveal region
while achieving a higher perceived image quality. We further observe that the
temporal presentation of pixels frame-to-frame can be viewed as a data stream,
presenting a random sampling problem. Following this intuition, we propose a
Weighted Reservoir Sampling technique to efficiently maintain a reservoir of
the perceptually relevant high quality pixel samples from previous frames and
incorporate them into the computation of the current frame. This allows the
renderer to render a smaller region of foveal pixels per frame by temporally
reusing pixel samples that are still relevant to reconstruct a higher perceived
image quality, while allowing for higher levels of foveation. Our method
operates on the output of foveated rendering, and runs in under 1\,ms at 4K
resolution, making it highly efficient and integrable with real-time VR and AR
foveated rendering systems.

</details>


### [524] [3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG](https://arxiv.org/abs/2510.04536)
*Shun-ichiro Hayashi,Daichi Mukunoki,Tetsuya Hoshino,Satoshi Ohshima,Takahiro Katagiri*

Main category: cs.GR

TL;DR: The paper introduces "3Dify," a framework that uses LLMs to generate 3D computer graphics (3D-CG) content from natural language instructions. It automates DCC tools using MCP and CUA, incorporates user feedback for improved image quality, and supports local LLM deployment for cost and time efficiency.


<details>
  <summary>Details</summary>
Motivation: To enable users to generate 3D-CG content solely through natural language instructions, automating the process and improving efficiency.

Method: 3Dify utilizes LLMs, MCP, RAG, and CUA to automate DCC tools. It also incorporates a user feedback loop for image generation enhancement and supports local LLM deployment.

Result: The framework can generate 3D-CG content from natural language, automates DCC tools, improves generation quality through feedback, and allows for cost-effective local LLM usage.

Conclusion: 3Dify provides a novel framework for generating 3D-CG content using LLMs, offering a user-friendly, efficient, and customizable solution.

Abstract: This paper proposes "3Dify," a procedural 3D computer graphics (3D-CG)
generation framework utilizing Large Language Models (LLMs). The framework
enables users to generate 3D-CG content solely through natural language
instructions. 3Dify is built upon Dify, an open-source platform for AI
application development, and incorporates several state-of-the-art LLM-related
technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented
Generation (RAG). For 3D-CG generation support, 3Dify automates the operation
of various Digital Content Creation (DCC) tools via MCP. When DCC tools do not
support MCP-based interaction, the framework employs the Computer-Using Agent
(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,
to enhance image generation quality, 3Dify allows users to provide feedback by
selecting preferred images from multiple candidates. The LLM then learns
variable patterns from these selections and applies them to subsequent
generations. Furthermore, 3Dify supports the integration of locally deployed
LLMs, enabling users to utilize custom-developed models and to reduce both time
and monetary costs associated with external API calls by leveraging their own
computational resources.

</details>


### [525] [C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing](https://arxiv.org/abs/2510.04539)
*Zeng Tao,Zheng Ding,Zeyuan Chen,Xiang Zhang,Leizhi Li,Zhuowen Tu*

Main category: cs.GR

TL;DR: C3Editor是一个2D提升框架，用于解决现有3D编辑方法中的不一致性问题，通过选择性地建立视图一致的2D编辑模型来实现可控且一致的3D编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D提升的3D编辑方法在视图一致性方面存在挑战，难以在多个视图中实现一致性编辑。

Method: C3Editor首先选择一个真实（GT）视图及其对应的编辑图像作为优化目标，允许用户进行手动编辑。然后，通过在GT视图和多个视图中微调2D编辑模型，使其与GT编辑图像对齐并确保多视图一致性。引入单独的LoRA模块以满足GT视图拟合和多视图一致性的不同需求。

Result: 该方法在定性和定量评估中均优于现有的基于2D提升的方法，提供了更一致、可控的2D和3D编辑结果。

Conclusion: C3Editor成功解决了现有3D编辑方法中的不一致性问题，通过视图一致的2D编辑模型实现了可控且一致的3D编辑。

Abstract: Existing 2D-lifting-based 3D editing methods often encounter challenges
related to inconsistency, stemming from the lack of view-consistent 2D editing
models and the difficulty of ensuring consistent editing across multiple views.
To address these issues, we propose C3Editor, a controllable and consistent
2D-lifting-based 3D editing framework. Given an original 3D representation and
a text-based editing prompt, our method selectively establishes a
view-consistent 2D editing model to achieve superior 3D editing results. The
process begins with the controlled selection of a ground truth (GT) view and
its corresponding edited image as the optimization target, allowing for
user-defined manual edits. Next, we fine-tune the 2D editing model within the
GT view and across multiple views to align with the GT-edited image while
ensuring multi-view consistency. To meet the distinct requirements of GT view
fitting and multi-view consistency, we introduce separate LoRA modules for
targeted fine-tuning. Our approach delivers more consistent and controllable 2D
and 3D editing results than existing 2D-lifting-based methods, outperforming
them in both qualitative and quantitative evaluations.

</details>


### [526] [Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents](https://arxiv.org/abs/2510.04637)
*Zeyi Zhang,Yanju Zhou,Heyuan Yao,Tenglong Ao,Xiaohang Zhan,Libin Liu*

Main category: cs.GR

TL;DR: Social Agent框架利用LLM驱动的智能体和扩散模型，生成逼真、协调的两人对话中的同步非语言行为，并通过持续反馈循环提升交互的自然度和响应性。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够生成逼真且符合语境的双人对话非语言行为的框架。

Method: 1. 构建一个由大语言模型（LLM）驱动的智能体系统，用于控制对话流程并决定参与者的互动行为。 2. 提出一种新颖的基于自回归扩散模型的双人手势生成模型，从语音信号合成协调运动。 3. 智能体系统将高层指导信息传递给手势生成器，以实现行为和运动层面的逼真运动。 4. 智能体系统会定期检查对话方的动作并推断其意图，形成一个持续的反馈循环，实现动态和响应式的互动。

Result: 用户研究和定量评估表明，该模型显著提高了双人交互的质量，产生了自然、同步的非语言行为。

Conclusion: 该Social Agent框架在生成逼真、同步的双人对话非语言行为方面取得了显著成效，并通过智能体和扩散模型的结合以及持续反馈循环，有效提升了交互的自然度和动态响应性。

Abstract: We present Social Agent, a novel framework for synthesizing realistic and
contextually appropriate co-speech nonverbal behaviors in dyadic conversations.
In this framework, we develop an agentic system driven by a Large Language
Model (LLM) to direct the conversation flow and determine appropriate
interactive behaviors for both participants. Additionally, we propose a novel
dual-person gesture generation model based on an auto-regressive diffusion
model, which synthesizes coordinated motions from speech signals. The output of
the agentic system is translated into high-level guidance for the gesture
generator, resulting in realistic movement at both the behavioral and motion
levels. Furthermore, the agentic system periodically examines the movements of
interlocutors and infers their intentions, forming a continuous feedback loop
that enables dynamic and responsive interactions between the two participants.
User studies and quantitative evaluations show that our model significantly
improves the quality of dyadic interactions, producing natural, synchronized
nonverbal behaviors.

</details>


### [527] [Bridging Text and Video Generation: A Survey](https://arxiv.org/abs/2510.04999)
*Nilay Kumar,Priyansh Bhandari,G. Maragatham*

Main category: cs.GR

TL;DR: 这是一个关于文本到视频（T2V）生成模型的全面调查，重点关注其发展、方法、数据集、评估指标和未来方向。


<details>
  <summary>Details</summary>
Motivation: T2V技术有潜力在教育、营销、娱乐和辅助技术等领域产生变革性影响，但目前在对齐、长期连贯性和计算效率方面仍存在挑战。

Method: 该调查追溯了T2V模型从早期的GAN和VAE到混合扩散-Transformer（DiT）架构的发展，详细介绍了它们的工作原理、克服的局限性以及转向新架构范式的必要性。

Result: 文章系统地介绍了T2V模型的训练和评估数据集，提供了训练配置的详细信息（硬件、GPU计数、批大小、学习率、优化器、时期等），并概述了常用的评估指标及其在标准基准上的表现。

Conclusion: 尽管T2V技术取得了显著进展，但仍存在开放性挑战。文章最后提出了一些有前景的未来研究方向，以推动T2V的研究和应用。

Abstract: Text-to-video (T2V) generation technology holds potential to transform
multiple domains such as education, marketing, entertainment, and assistive
technologies for individuals with visual or reading comprehension challenges,
by creating coherent visual content from natural language prompts. From its
inception, the field has advanced from adversarial models to diffusion-based
models, yielding higher-fidelity, temporally consistent outputs. Yet challenges
persist, such as alignment, long-range coherence, and computational efficiency.
Addressing this evolving landscape, we present a comprehensive survey of
text-to-video generative models, tracing their development from early GANs and
VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these
models work, what limitations they addressed in their predecessors, and why
shifts toward new architectural paradigms were necessary to overcome challenges
in quality, coherence, and control. We provide a systematic account of the
datasets, which the surveyed text-to-video models were trained and evaluated
on, and, to support reproducibility and assess the accessibility of training
such models, we detail their training configurations, including their hardware
specifications, GPU counts, batch sizes, learning rates, optimizers, epochs,
and other key hyperparameters. Further, we outline the evaluation metrics
commonly used for evaluating such models and present their performance across
standard benchmarks, while also discussing the limitations of these metrics and
the emerging shift toward more holistic, perception-aligned evaluation
strategies. Finally, drawing from our analysis, we outline the current open
challenges and propose a few promising future directions, laying out a
perspective for future researchers to explore and build upon in advancing T2V
research and applications.

</details>


### [528] [SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder](https://arxiv.org/abs/2510.05081)
*Ronen Kamenetsky,Sara Dorfman,Daniel Garibi,Roni Paiss,Or Patashnik,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 通过操纵文本嵌入的 token 级表示，实现文本到图像模型的可分离和连续编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在编辑过程中缺乏对属性的解耦和连续控制。

Method: 使用稀疏自编码器（SAE）识别文本嵌入中的语义隔离维度，并沿着这些维度操纵嵌入以实现编辑，从而能够对编辑强度进行平滑调整。

Result: 实验证明，该方法可以在各种属性和领域中实现直观、高效的图像编辑，并具有连续控制能力。

Conclusion: 所提出的方法通过 token 级文本嵌入操纵，实现了文本到图像编辑的可分离和连续控制，且不修改扩散过程，具有模型无关性和广泛适用性。

Abstract: Large-scale text-to-image diffusion models have become the backbone of modern
image editing, yet text prompts alone do not offer adequate control over the
editing process. Two properties are especially desirable: disentanglement,
where changing one attribute does not unintentionally alter others, and
continuous control, where the strength of an edit can be smoothly adjusted. We
introduce a method for disentangled and continuous editing through token-level
manipulation of text embeddings. The edits are applied by manipulating the
embeddings along carefully chosen directions, which control the strength of the
target attribute. To identify such directions, we employ a Sparse Autoencoder
(SAE), whose sparse latent space exposes semantically isolated dimensions. Our
method operates directly on text embeddings without modifying the diffusion
process, making it model agnostic and broadly applicable to various image
synthesis backbones. Experiments show that it enables intuitive and efficient
manipulations with continuous control across diverse attributes and domains.

</details>


### [529] [Pulp Motion: Framing-aware multimodal camera and human motion generation](https://arxiv.org/abs/2510.05097)
*Robin Courant,Xi Wang,David Loiseaux,Marc Christie,Vicky Kalogeiton*

Main category: cs.GR

TL;DR: 该研究首次将文本条件化的人体运动和摄像机轨迹联合生成作为一个整体任务来处理，旨在生成连贯且符合电影美学的运动和镜头。通过引入“画面构图”作为辅助模态，强制两种异构但相互关联的模态（人体运动和摄像机轨迹）之间的一致性，并在共享的潜在空间中进行学习和生成。


<details>
  <summary>Details</summary>
Motivation: 传统方法分别处理人体运动和摄像机轨迹，忽略了两者在电影制作中的内在联系。本研究旨在解决这一问题，通过联合生成来保持一致的画面构图，并生成更符合电影美学的人体运动和摄像机轨迹。

Method: 提出了一种模型无关的框架，通过引入“画面构图”（将人体关节投影到摄像机上得到的画面）作为辅助模态，来强制人体运动和摄像机轨迹之间的多模态一致性。该框架包括一个联合自编码器，学习共享的潜在空间，以及一个轻量级线性变换，将人体和摄像机的潜在表示映射到画面构图的潜在表示。此外，还引入了辅助采样机制，利用该线性变换来引导生成过程，以获得一致的画面构图。

Result: 实验结果表明，该方法在生成画面一致的人体-摄像机运动方面有效，并且在文本对齐方面也取得了进展。与基于DiT和MAR的架构相比，该方法在电影构图方面表现更优，达到了新的技术水平。

Conclusion: 本研究提出的联合生成框架通过引入画面构图作为辅助模态，成功解决了人体运动和摄像机轨迹生成中的一致性问题，并取得了优于现有技术的性能，为电影制作领域带来了新的可能性。

Abstract: Treating human motion and camera trajectory generation separately overlooks a
core principle of cinematography: the tight interplay between actor performance
and camera work in the screen space. In this paper, we are the first to cast
this task as a text-conditioned joint generation, aiming to maintain consistent
on-screen framing while producing two heterogeneous, yet intrinsically linked,
modalities: human motion and camera trajectories. We propose a simple,
model-agnostic framework that enforces multimodal coherence via an auxiliary
modality: the on-screen framing induced by projecting human joints onto the
camera. This on-screen framing provides a natural and effective bridge between
modalities, promoting consistency and leading to more precise joint
distribution. We first design a joint autoencoder that learns a shared latent
space, together with a lightweight linear transform from the human and camera
latents to a framing latent. We then introduce auxiliary sampling, which
exploits this linear transform to steer generation toward a coherent framing
modality. To support this task, we also introduce the PulpMotion dataset, a
human-motion and camera-trajectory dataset with rich captions, and high-quality
human motions. Extensive experiments across DiT- and MAR-based architectures
show the generality and effectiveness of our method in generating on-frame
coherent human-camera motions, while also achieving gains on textual alignment
for both modalities. Our qualitative results yield more cinematographically
meaningful framings setting the new state of the art for this task. Code,
models and data are available in our
\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project
page}.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [530] [Efficient Training of Spiking Neural Networks by Spike-aware Data Pruning](https://arxiv.org/abs/2510.04098)
*Chenxiang Ma,Xinyi Chen,Yujie Wu,Kay Chen Tan,Jibin Wu*

Main category: cs.NE

TL;DR: SADP是一种新颖的、具有成本效益的剪枝方法，通过利用梯度范数的概念来提高SNN的训练效率。


<details>
  <summary>Details</summary>
Motivation: SNN的训练需要大量的计算资源，而现有的数据剪枝方法无法有效地应用于SNN。

Method: SADP通过将样本的选择概率与其梯度范数成正比来降低梯度方差，并使用一种称为“脉冲感知重要性得分”的高效上限来计算，该得分考虑了全有或全无脉冲对梯度范数的影响。

Result: SADP在各种数据集和架构上持续优于其他数据剪枝基线，并实现了接近理论最大值的训练加速。在ImageNet上，SADP将训练时间缩短了35%，同时保持了可比的准确性。

Conclusion: SADP建立了一个以数据为中心的SNN高效训练范式，为SNN扩展到更大的模型和数据集铺平了道路。

Abstract: Spiking neural networks (SNNs), recognized as an energy-efficient alternative
to traditional artificial neural networks (ANNs), have advanced rapidly through
the scaling of models and datasets. However, such scaling incurs considerable
training overhead, posing challenges for researchers with limited computational
resources and hindering the sustained development of SNNs. Data pruning is a
promising strategy for accelerating training by retaining the most informative
examples and discarding redundant ones, but it remains largely unexplored in
SNNs. Directly applying ANN-based data pruning methods to SNNs fails to capture
the intrinsic importance of examples and suffers from high gradient variance.
To address these challenges, we propose a novel spike-aware data pruning (SADP)
method. SADP reduces gradient variance by determining each example's selection
probability to be proportional to its gradient norm, while avoiding the high
cost of direct gradient computation through an efficient upper bound, termed
spike-aware importance score. This score accounts for the influence of
all-or-nothing spikes on the gradient norm and can be computed with negligible
overhead. Extensive experiments across diverse datasets and architectures
demonstrate that SADP consistently outperforms data pruning baselines and
achieves training speedups close to the theoretical maxima at different pruning
ratios. Notably, SADP reduces training time by 35% on ImageNet while
maintaining accuracy comparable to that of full-data training. This work,
therefore, establishes a data-centric paradigm for efficient SNN training and
paves the way for scaling SNNs to larger models and datasets. The source code
will be released publicly after the review process.

</details>


### [531] [SpikingMamba: Towards Energy-Efficient Large Language Models via Knowledge Distillation from Mamba](https://arxiv.org/abs/2510.04595)
*Yulong Huang,Jianxiong Tang,Chao Wang,Ziyi Wang,Jianguo Zhang,Zhichao Lu,Bojun Cheng,Luziwei Leng*

Main category: cs.NE

TL;DR: SpikingMamba是一种基于脉冲神经网络（SNN）的LLM，通过蒸馏Mamba模型实现，提高了能效，同时最大限度地减少了准确性损失，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）能耗高，而脉冲神经网络（SNNs）能效高，但现有的SNN-based LLMs在性能上有所牺牲，并且恢复准确性通常需要昂贵的全预训练。因此，需要一种在保持高能效的同时，最大限度地减少准确性损失的SNN-based LLM。

Method: SpikingMamba通过结合TI-LIF（一种保持语义极性的三元整数脉冲神经元）和SGC（一种仅用于训练的平滑梯度补偿路径）来实现。使用单阶段蒸馏策略将预训练Mamba的零样本能力转移过来，并通过强化学习（RL）进一步增强。

Result: SpikingMamba-1.3B实现了4.76倍的能效提升，与原始Mamba相比，零样本准确性仅下降了4.78%，在经过RL训练后，准确性又提高了2.55%。

Conclusion: SpikingMamba是一种有前景的SNN-based LLM，它在保持高能效的同时，最大限度地减少了准确性损失，并通过蒸馏和RL实现了对Mamba模型能力的有效迁移和增强。

Abstract: Large Language Models (LLMs) have achieved remarkable performance across
tasks but remain energy-intensive due to dense matrix operations. Spiking
neural networks (SNNs) improve energy efficiency by replacing dense matrix
multiplications with sparse accumulations. Their sparse spike activity enables
efficient LLMs deployment on edge devices. However, prior SNN-based LLMs often
sacrifice performance for efficiency, and recovering accuracy typically
requires full pretraining, which is costly and impractical. To address this, we
propose SpikingMamba, an energy-efficient SNN-based LLMs distilled from Mamba
that improves energy efficiency with minimal accuracy sacrifice. SpikingMamba
integrates two key components: (a) TI-LIF, a ternary-integer spiking neuron
that preserves semantic polarity through signed multi-level spike
representations. (b) A training-exclusive Smoothed Gradient Compensation (SGC)
path mitigating quantization loss while preserving spike-driven efficiency. We
employ a single-stage distillation strategy to transfer the zero-shot ability
of pretrained Mamba and further enhance it via reinforcement learning (RL).
Experiments show that SpikingMamba-1.3B achieves a 4.76$\times$ energy benefit,
with only a 4.78\% zero-shot accuracy gap compared to the original Mamba, and
achieves a further 2.55\% accuracy improvement after RL.

</details>


### [532] [What your brain activity says about you: A review of neuropsychiatric disorders identified in resting-state and sleep EEG data](https://arxiv.org/abs/2510.04984)
*J. E. M. Scanlon,A. Pelzer,M. Gharleghi,K. C. Fuhrmeister,T. Köllmer,P. Aichroth,R. Göder,C. Hansen,K. I. Wolf*

Main category: cs.NE

TL;DR: 研究发现，即使是任务无关的脑电图（EEG）数据也可能包含敏感的个人健康信息，包括疾病诊断和身份识别信息。该研究回顾了现有文献，评估了公开的EEG数据所带来的隐私风险，并强调了匿名化和开发隐私保护工具的重要性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨在非任务状态的脑电图（EEG）数据中，可以检测和分类出哪些类型的个人和健康信息，并评估公开的EEG数据所带来的隐私风险。

Method: 通过在Google Scholar、Web of Science等数据库中搜索相关文献，纳入关于在静息状态和睡眠EEG数据中分类或检测各种疾病和个人信息的英文、全文、同行评审的期刊文章或会议论文。由3位审稿人进行质量分析。

Result: 研究发现，静息状态EEG数据可以高精度地分类多种疾病（如自闭症谱系障碍、帕金森病、酒精使用障碍），通常仅需5分钟或更少的数据。睡眠EEG数据虽然也能分类睡眠障碍（如睡眠呼吸暂停、失眠、快速眼动睡眠障碍），但通常需要更长的记录时间或涉及多个睡眠阶段。机器学习方法在个体识别方面也取得了进展。

Conclusion: 即使在今天，访问个人的EEG数据也可能泄露敏感的个人健康信息。随着机器学习方法在从EEG数据中重新识别个体方面的能力不断增强，该综述强调了匿名化和开发改进的工具以保护研究参与者和医疗EEG用户隐私的重要性。

Abstract: Electroencephalogram monitoring devices and online data repositories hold
large amounts of data from individuals participating in research and medical
studies without direct reference to personal identifiers. This paper explores
what types of personal and health information have been detected and classified
within task-free EEG data. Additionally, we investigate key characteristics of
the collected resting-state and sleep data, in order to determine the privacy
risks involved with openly available EEG data. We used Google Scholar, Web of
Science and searched relevant journals to find studies which classified or
detected the presence of various disorders and personal information in resting
state and sleep EEG. Only English full-text peer-reviewed journal articles or
conference papers about classifying the presence of medical disorders between
individuals were included. A quality analysis carried out by 3 reviewers
determined general paper quality based on specified evaluation criteria. In
resting state EEG, various disorders including Autism Spectrum Disorder,
Parkinson's disease, and alcohol use disorder have been classified with high
classification accuracy, often requiring only 5 mins of data or less. Sleep EEG
tends to hold classifiable information about sleep disorders such as sleep
apnea, insomnia, and REM sleep disorder, but usually involve longer recordings
or data from multiple sleep stages. Many classification methods are still
developing but even today, access to a person's EEG can reveal sensitive
personal health information. With an increasing ability of machine learning
methods to re-identify individuals from their EEG data, this review
demonstrates the importance of anonymization, and the development of improved
tools for keeping study participants and medical EEG users' privacy safe.

</details>


### [533] [Exploration-Exploitation-Evaluation (EEE): A Framework for Metaheuristic Algorithms in Combinatorial Optimization](https://arxiv.org/abs/2510.05027)
*Ethan Davis*

Main category: cs.NE

TL;DR: 本研究提出一个框架，将元启发式算法（如蚁群优化 ACO）应用于组合优化问题（COPs），例如旅行商问题（TSP）。


<details>
  <summary>Details</summary>
Motivation: 将 ACO 等元启发式算法应用于 TSP 等组合优化问题。

Method: 该框架包括三个阶段：参数空间广泛探索、高性能参数利用以及不确定性量化（UQ）。

Result: 以 ACO 应用于 TSPLIB berlin52 数据集为例，计算出 ACO 单次运行找到全局最优值的概率约为 1/40，十次运行后概率提升至 1/5。

Conclusion: 本研究提出的框架能够系统地优化元启发式算法在组合优化问题中的应用，并通过不确定性量化评估结果的可靠性。

Abstract: We introduce a framework for applying metaheuristic algorithms, such as ant
colony optimization (ACO), to combinatorial optimization problems (COPs) like
the traveling salesman problem (TSP). The framework consists of three
sequential stages: broad exploration of the parameter space, exploitation of
top-performing parameters, and uncertainty quantification (UQ) to assess the
reliability of results. As a case study, we apply ACO to the TSPLIB berlin52
dataset, which has a known optimal tour length of 7542. Using our framework, we
calculate that the probability of ACO finding the global optimum is
approximately 1/40 in a single run and improves to 1/5 when aggregated over ten
runs.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [534] [An Empirical Study of Rational Tree Unification for miniKanren](https://arxiv.org/abs/2510.03789)
*Eridan Domoratskiy,Dmitrii Kosarev,Dmitry Boulytchev*

Main category: cs.LO

TL;DR: 迷你Kanren中理性树的统一。


<details>
  <summary>Details</summary>
Motivation: 在迷你Kanren的背景下，研究理性树的统一。

Method: 定义理性树，指定统一算法并证明其属性，引入启发式优化并针对相关基准进行评估，讨论理性树和常规统一算法之间的关系以及它们在关系编程中共存的可能性。

Result: 给出了理性树的定义，指定了统一算法并证明了其属性，引入了启发式优化并针对相关基准进行了评估。

Conclusion: 讨论了理性树和常规统一算法之间的关系以及它们在关系编程中共存的可能性。

Abstract: We present a study of unification for rational trees in the context of
miniKanren. We give the definition of rational trees, specify the unification
algorithm and prove some of its properties. We also introduce a number of
heuristic optimizations and evaluate them for a number of relevant benchmarks.
Finally we discuss the relations between rational and conventional unification
algorithms and possible scenarios of their coexistence in the context of
relational programming.

</details>


### [535] [Interpolation in First-Order Logic](https://arxiv.org/abs/2510.03822)
*Balder ten Cate,Jesse Comer*

Main category: cs.LO

TL;DR: 本文概述了关于一阶逻辑及其片段的Craig插值定理的已知结果及其应用，包括定理的改进、在逻辑和计算机科学中的应用、重要句法片段的插值结果以及计算插值的问题。


<details>
  <summary>Details</summary>
Motivation: 提供一个关于一阶逻辑及其片段的插值定理及其应用的文献入门。

Method: 概述已知结果，涵盖Craig插值定理的改进、应用、片段以及计算插值问题。

Result: 概述了Craig插值定理的已知结果、应用和计算方法。

Conclusion: 为研究一阶逻辑及其片段的Craig插值定理及其应用提供了一个基础性的概述和切入点。

Abstract: In this chapter we give a basic overview of known results regarding Craig
interpolation for first-order logic as well as for fragments of first-order
logic. Our aim is to provide an entry point into the literature on
interpolation theorems for first-order logic and fragments of first-order
logic, and their applications. In particular, we cover a range of known
refinements of the Craig interpolation theorem, we discuss several important
applications of interpolation in logic and computer science, we review known
results about interpolation for important syntactic fragments of first-order
logic, and we discuss the problem of computing interpolants.

</details>


### [536] [Unreliability in Practical Subclasses of Communicating Systems](https://arxiv.org/abs/2510.03941)
*Amrita Suresh,Nobuko Yoshida*

Main category: cs.LO

TL;DR: 通信自动机模型在点对点消息传递中很常用，但通常是不可判定的。本文研究了在干扰和崩溃-停止故障模型下，RSC和k-MC这两种可判定子类的弹性，并提出了新的处理崩溃故障的通信系统，扩展了现有MPST模型。


<details>
  <summary>Details</summary>
Motivation: 现有的RSC和k-MC模型在面对故障时不够鲁棒，大多数标准协议在故障下不满足这些条件，因此需要研究在故障模型下的弹性和可判定性。

Method: 通过放宽RSC和k-MC的条件，研究其在干扰下的可判定性。提出了一种新的崩溃处理通信系统，并研究了将带有崩溃-停止故障的MPST模型翻译到该系统，以整合RSC和k-MC属性并建立其可判定性。最后，通过扩展RSC和k-MC工具来处理干扰，验证现有协议以评估和展示宽松系统的弹性。

Result: 在干扰下，放宽的RSC和k-MC属性的可判定性得以保留，并且具有已知的复杂性界限。提出的新型崩溃处理系统能够捕捉比现有基于崩溃-停止故障的MPST更广泛的行为。通过验证代表性协议，证明了宽松系统在处理干扰方面的弹性。

Conclusion: 本文成功地扩展了RSC和k-MC模型以应对干扰和崩溃-停止故障，通过提出新的系统和翻译方法，保留了可判定性，并展示了在实际应用中的弹性。

Abstract: Systems of communicating automata are prominent models for peer-to-peer
message-passing over unbounded channels, but in the general scenario, most
verification properties are undecidable. To address this issue, two decidable
subclasses, Realisable with Synchronous Communication (RSC) and k-Multiparty
Compatibility} (k-MC), were proposed in the literature, with corresponding
verification tools developed and applied in practice. Unfortunately, both RSC
and k-MC are not resilient under failures: (1) their decidability relies on the
assumption of perfect channels and (2) most standard protocols do not satisfy
RSC or k-MC under failures. To address these limitations, this paper studies
the resilience of RSC and k-MC under two distinct failure models: interference
and crash-stop failures. For interference, we relax the conditions of RSC and
k-MC and prove that the inclusions of these relaxed properties remain decidable
under interference, preserving their known complexity bounds. We then propose a
novel crash-handling communicating system that captures wider behaviours than
existing multiparty session types (MPST) with crash-stop failures. We study a
translation of MPST with crash-stop failures into this system integrating RSC
and k-MC properties, and establish their decidability results. Finally, by
verifying representative protocols from the literature using RSC and k-MC tools
extended to interferences, we evaluate the relaxed systems and demonstrate
their resilience.

</details>


### [537] [On Hyperproperty Verification, Quantifier Alternations, and Games under Partial Information](https://arxiv.org/abs/2510.03942)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.LO

TL;DR: 本文提出了一种利用多玩家不完全信息博弈来验证具有任意量词交替的超属性的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的超属性验证方法在处理量词交替时面临挑战，需要进行系统补全，成本高昂且不切实际。基于博弈的方法虽然成本较低，但仅限于 $orall^*
exists^*$ 形式的属性。

Method: 本文将具有任意量词交替的超属性验证问题转化为多玩家不完全信息博弈。尽管不完全信息博弈通常是不可判定的，但本文提出的博弈具有分层信息结构，属于可判定博弈的一个子类。此外，还研究了该博弈的完备性以及不完全信息设置下的预言变量。

Result: 通过将超属性验证转化为一种可判定的多玩家不完全信息博弈，为验证具有复杂量词交替的超属性提供了一种新的、更具可行性的方法。

Conclusion: 本文提出的利用多玩家不完全信息博弈来验证超属性的方法，克服了现有方法的局限性，为自动化验证提供了新的途径。

Abstract: Hyperproperties generalize traditional trace properties by relating multiple
execution traces rather than reasoning about individual runs in isolation. They
provide a unified way to express important requirements such as information
flow and robustness properties. Temporal logics like HyperLTL capture these
properties by explicitly quantifying over executions of a system. However, many
practically relevant hyperproperties involve quantifier alternations, a feature
that poses substantial challenges for automated verification. Complete
verification methods require a system complementation for each quantifier
alternation, making it infeasible in practice. A cheaper (but incomplete)
method interprets the verification of a HyperLTL formula as a two-player game
between universal and existential quantifiers. The game-based approach is
significantly cheaper, facilitates interactive proofs, and allows for
easy-to-check certificates of satisfaction. It is, however, limited to
$\forall^*\exists^*$ properties, leaving important properties out of reach. In
this paper, we show that we can use games to verify hyperproperties with
arbitrary quantifier alternations by utilizing multiplayer games under partial
information. While games under partial information are, in general,
undecidable, we show that our game is played under hierarchical information and
thus falls in a decidable class of games. We discuss the completeness of the
game and study prophecy variables in the setting of partial information.

</details>


### [538] [Strategy Logic, Imperfect Information, and Hyperproperties](https://arxiv.org/abs/2510.03952)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.LO

TL;DR: SL_ii和HyperSL在特定限制下是等价的，它们可以相互编码。


<details>
  <summary>Details</summary>
Motivation: 研究SL_ii和HyperSL之间的关系。

Method: 证明了在特定限制下，SL_ii和HyperSL是等价的，可以相互编码。具体来说，SL_ii可以编码进HyperSL，HyperSL也可以编码进SL_ii。

Result: SL_ii和HyperSL在特定限制下是等价的，可以相互编码。

Conclusion: SL_ii和HyperSL在特定限制下是等价的。

Abstract: Strategy logic (SL) is a powerful temporal logic that enables first-class
reasoning over strategic behavior in multi-agent systems (MAS). In many MASs,
the agents (and their strategies) cannot observe the global state of the
system, leading to many extensions of SL centered around imperfect information,
such as strategy logic with imperfect information (SL$_\mathit{ii}$). Along
orthogonal lines, researchers have studied the combination of strategic
behavior and hyperproperties. Hyperproperties are system properties that relate
multiple executions in a system and commonly arise when specifying security
policies. Hyper Strategy Logic (HyperSL) is a temporal logic that combines
quantification over strategies with the ability to express hyperproperties on
the executions of different strategy profiles. In this paper, we study the
relation between SL$_\mathit{ii}$ and HyperSL. Our main result is that both
logics (restricted to formulas where no state formulas are nested within path
formulas) are equivalent in the sense that we can encode SL$_\mathit{ii}$
instances into HyperSL instances and vice versa. For the former direction, we
build on the well-known observation that imperfect information is a
hyperproperty. For the latter direction, we construct a self-composition of
MASs and show how we can simulate hyperproperties using imperfect information.

</details>


### [539] [A Complete Diagrammatic Calculus for Conditional Gaussian Mixtures](https://arxiv.org/abs/2510.04649)
*Mateo Torres-Ruiz,Robin Piedeleu,Alexandra Silva,Fabio Zanasi*

Main category: cs.LO

TL;DR: 我们为混合概率模型引入了一个新的图示演算，用于处理离散和连续随机变量的组合，特别是那些离散变量条件下的高斯分布模型，如高斯混合模型。我们开发了一种表达和组合这些模型的字符串图示语法，并为其提供了组合语义和完备的方程理论，以判断两个模型是否表示相同的分布。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于将现有的离散和高斯概率范畴理论扩展到更广泛的混合概率模型，特别是那些涉及离散变量条件下的连续高斯分布的模型，例如高斯混合模型。

Method: 我们开发了一种字符串图示语法来表达和组合这些混合概率模型，并为其提供了组合语义和一套完备的方程理论。该理论能够判定两个模型是否表示相同的概率分布。

Result: 我们成功地为混合概率模型（其中连续随机变量在离散变量条件下遵循多元高斯分布）建立了一个图示演算。

Conclusion: 本文提出的图示演算及其相关的理论为理解和操作混合概率模型提供了一个强大的框架，特别是在处理高斯混合模型等应用中具有重要意义。

Abstract: We extend the synthetic theories of discrete and Gaussian categorical
probability by introducing a diagrammatic calculus for reasoning about hybrid
probabilistic models in which continuous random variables, conditioned on
discrete ones, follow a multivariate Gaussian distribution. This setting
includes important classes of models such as Gaussian mixture models, where
each Gaussian component is selected according to a discrete variable. We
develop a string diagrammatic syntax for expressing and combining these models,
give it a compositional semantics, and equip it with a sound and complete
equational theory that characterises when two models represent the same
distribution.

</details>


### [540] [Continuation Semantics for Fixpoint Modal Logic and Computation Tree Logics](https://arxiv.org/abs/2510.04653)
*Ryota Kojima,Corina Cirstea*

Main category: cs.LO

TL;DR: 本文提出了一种新的计算语义学方法，并证明了其与现有模型在固定点模态逻辑（FML）和计算树逻辑（CTL*）上的等价性。


<details>
  <summary>Details</summary>
Motivation: 在固定点模态逻辑（FML）和计算树逻辑（CTL*）中引入参数化的连续语义学，并证明其与所有分支类型的协代数语义学等价。

Method: 通过识别谓词和连续映射，将连续语义学定义在连续代数上的协代数，其中回答类型与公式的真值域相匹配。对于CTL*，允许使用非最大不动点（称为执行映射）的协代数模型，并通过幺半群态射转移执行映射。

Result: 证明了连续语义学等价于FML的协代数语义学。对于CTL*，在允许非最大不动点后，证明了连续语义学等价于协代数语义学。

Conclusion: 本文提出的连续语义学与FML和CTL*的协代数语义学等价，并为CTL到FML的编码提供了条件。

Abstract: We introduce continuation semantics for both fixpoint modal logic (FML) and
Computation Tree Logic* (CTL*), parameterised by a choice of branching type and
quantitative predicate lifting. Our main contribution is proving that they are
equivalent to coalgebraic semantics, for all branching types. Our continuation
semantics is defined over coalgebras of the continuation monad whose answer
type coincides with the domain of truth values of the formulas. By identifying
predicates and continuations, such a coalgebra has a canonical interpretation
of the modality by evaluation of continuations. We show that this continuation
semantics is equivalent to the coalgebraic semantics for fixpoint modal logic.
We then reformulate the current construction for coalgebraic models of CTL*.
These models are usually required to have an infinitary trace/maximal execution
map, characterized as the greatest fixpoint of a special operator. Instead, we
allow coalgebraic models of CTL* to employ non-maximal fixpoints, which we call
execution maps. Under this reformulation, we establish a general result on
transferring execution maps via monad morphisms. From this result, we obtain
that continuation semantics is equivalent to the coalgebraic semantics for
CTL*. We also identify a sufficient condition under which CTL can be encoded
into fixpoint modal logic under continuation semantics.

</details>


### [541] [Curved Boolean Logic: A Contextual Generalization of Propositional Logic with Algorithmic Consequences](https://arxiv.org/abs/2510.04716)
*Maximilian R. P. von Liechtenstein*

Main category: cs.LO

TL;DR: CBL 是一种逻辑泛化，允许局部真值分配，类似于几何中的曲率，并提供了等效的代数语义、证明演算、SAT 问题的复杂性分析以及处理噪声和推断的技术。


<details>
  <summary>Details</summary>
Motivation: 将命题逻辑推广到允许局部真值分配，类似于几何中的曲率。

Method: 提出了等效的代数语义、证明演算、CBL-SAT 的复杂性分析、CBL-AC 和 CBL-CONS 等操作算子，并对噪声进行了建模，提供了基于排列的显著性，并控制了 FDR。

Result: CBL-SAT 在一般情况下是 NP 完全的，CBL-AC 和 CBL-CONS 可以在经典硬件上更早地修剪矛盾，并提供了处理不同类型噪声的框架。

Conclusion: CBL 可以被视为 KCBS、CSW 和代数框架的泛化，并与 SAT/CSP 和大型语言模型的鲁棒性/适配器稳定性相关联。

Abstract: Curved Boolean Logic (CBL) generalizes propositional logic by allowing local
truth assignments that do not extend to a single global valuation, analogous to
curvature in geometry. We give equivalent sheaf and exclusivity-graph semantics
and a context-aware proof calculus that is conservative in the flat limit. We
formalize CBL-SAT and basic complexity (NP-complete in general) and present
operational operators (CBL-AC and CBL-CONS) that prune contradictions earlier
on classical hardware. We model noise with iid, AR(1)-correlated, and
adversarial bounded perturbations and provide permutation-based significance
with Benjamini-Hochberg FDR control. A Colab-ready notebook (ancillary files)
regenerates all figures and statistics. We position CBL relative to KCBS, CSW,
and sheaf frameworks and outline links to SAT/CSP and robustness/adapter
stability in large language models.

</details>


### [542] [One rig to control them all](https://arxiv.org/abs/2510.05032)
*Chris Heunen,Robin Kaarsgaard,Louis Lemonnier*

Main category: cs.LO

TL;DR: 我们提出了计算控制理论，包括七个可解释的方程，用于构造受控电路，并在可逆布尔电路和量子电路中得到验证。该理论在语义上等同于在基础丙上取自由刚格范畴。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在提出一种计算控制理论，通过引入七个可解释的方程来扩展基础电路，从而能够构造受控电路。

Method: 通过增加七个计算控制方程到基础电路丙中，来构造受控电路。并证明了该句法构造在语义上等同于在基础丙上取自由刚格范畴。

Result: 该理论在可逆布尔电路和量子电路的示例中得到了验证，证明了其构造受控电路的能力。

Conclusion: 该句法构造在语义上与在基础丙上取自由刚格范畴相对应，为计算控制提供了一个理论基础。

Abstract: We introduce a theory for computational control, consisting of seven
naturally interpretable equations. Adding these to a prop of base circuits
constructs controlled circuits, borne out in examples of reversible Boolean
circuits and quantum circuits. We prove that this syntactic construction
semantically corresponds to taking the free rig category on the base prop.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [543] [Bloch Oscillations and Landau-Zener Transitions in Flat-Band Lattices with Quadratic and Linear Band Touchings](https://arxiv.org/abs/2510.03530)
*Chenhaoyue Wang,Carlos J. Garcia-Cervera,Amartya S. Banerjee*

Main category: cond-mat.mtrl-sci

TL;DR: Lieb和Kagome晶格中的Bloch振荡（BOs）受到平带物理和带间耦合的共同影响，导致非谐波运动。研究发现，这两种晶格中存在半整数和整数的BOs频率，这与它们独特的能带结构直接相关。Kagome晶格中的强耦合和潜在的Landau-Zener跃迁（LZTs）以及二次带交叉，使得BOs动力学与线性交叉的系统不同。Lieb晶格则表现出平带与两个色散带之间的独立耦合。通过研究应力诱导的从Kagome到Lieb晶格的转变，将BOs频率的演变与能带连通性和带间耦合的变化联系起来，从而获得对这些结果的统一认识。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究平带物理和带间耦合在产生非规则Bloch振荡（BOs）中的相互作用，特别是在Lieb和Kagome晶格中。

Method: 采用相干输运模拟和散射矩阵分析两种互补的方法来研究Lieb和Kagome晶格中的BOs。

Result: 在Lieb和Kagome晶格中观察到半整数和整数的BOs频率，这与它们独特的能带结构有关。Kagome晶格中的强耦合和潜在的LZTs，以及二次带交叉，导致了与线性交叉系统不同的BOs动力学。Lieb晶格中的平带与两个色散带之间存在独立耦合。研究还观察到，在应力诱导的从Kagome到Lieb晶格的转变过程中，BOs频率的演变与能带连通性和带间耦合的变化相关。

Conclusion: Lieb和Kagome晶格中的BOs动力学受到能带结构和带间耦合的显著影响。散射矩阵分析揭示了Kagome晶格中平带和色散带之间以及二次带交叉的相互作用，而Lieb晶格则表现出不同于Kagome晶格的耦合机制。通过研究应力诱导的晶格转变，可以统一理解这些复杂的BOs现象。

Abstract: Bloch oscillations (BOs) describe the coherent oscillatory motion of
electrons in a periodic lattice under a constant external electric field.
Deviations from pure harmonic wave packet motion or irregular Bloch
oscillations can occur due to Zener tunneling (Landau-Zener Transitions or
LZTs), with oscillation frequencies closely tied to interband coupling
strengths. Motivated by the interplay between flat-band physics and interband
coupling in generating irregular BOs, here we investigate these oscillations in
Lieb and Kagome lattices using two complementary approaches: coherent transport
simulations and scattering matrix analysis. In the presence of unavoidable band
touchings, half-fundamental and fundamental BO frequencies are observed in Lieb
and Kagome lattices, respectively -- a behavior directly linked to their
distinct band structures. When avoided band touchings are introduced, distinct
BO frequency responses to coupling parameters in each lattice are observed.
Scattering matrix analysis reveals strong coupling and potential LZTs between
dispersive bands and the flat band in Kagome lattices, with the quadratic band
touching enhancing interband interactions and resulting in BO dynamics that is
distinct from systems with linear crossings. In contrast, the Lieb lattice -- a
three level system -- shows independent coupling between the flat band and two
dispersive bands, without direct LZTs occurring between the two dispersive
bands themselves. Finally, to obtain a unifying perspective on these results,
we examine BOs during a strain-induced transition from Kagome to Lieb lattices,
and link the evolution of irregular BO frequencies to changes in band
connectivity and interband coupling.

</details>


### [544] [Strain Effects on Electronic Properties of Cobalt-Based Coordination Nanosheets](https://arxiv.org/abs/2510.03549)
*Kento Nishigomi,Yu Yi,Souren Adhikary,Kazuhito Tsukagoshi,Katsunori Wakabayashi*

Main category: cond-mat.mtrl-sci

TL;DR: 应变可调的钴基苯并六硫醇配位纳米片的电子和拓扑性质。


<details>
  <summary>Details</summary>
Motivation: 研究应变对钴基苯并六硫醇（CoBHT）配位纳米片的电子性质的影响。

Method: 使用第一性原理计算研究两种晶体结构（高密度结构HDS和低密度结构LDS）的应变效应，并构建紧束缚模型研究拓扑性质。

Result: HDS表现为金属，LDS表现为半金属。自旋轨道耦合（SOC）在K点打开了能隙。应变显著改变了电子和磁性性质，特别是HDS。内禀贝里曲率驱动的异常霍尔电导率。

Conclusion: CoBHT纳米片可以通过应变工程来调控其电子性质，在电子、光电和催化器件领域具有应用潜力。

Abstract: We theoretically study the strain effects on the electronic properties of
  cobalt-based benzenehexathiol (CoBHT) coordination nanosheets using
  first-principles calculations. Two distinct crystal structures,
  high-density structure (HDS) and low-density structure (LDS), are
  explored. Our results reveal that HDS behaves as a metal, while LDS
  exhibits semiconducting. Spin-polarized electronic band structures highlight
the presence of energy band structures of Kagome lattice, and
  the inclusion of spin-orbit coupling (SOC) results in band gap openings
  at high-symmetric K points. Furthermore, we construct the tight-binding
  model to investigate the topological properties of CoBHT,
  demonstrating anomalous Hall conductivity driven by the intrinsic
  Berry curvature. The impact of uniaxial
  strain on the electronic and magnetic properties of CoBHT is also studied.
Strain
  induces significant modifications in magnetic moments and density
  of states, particularly in the HDS. Anomalous Hall conductivity is
  enhanced under hole-doping conditions, suggesting that strain can be
  used to tailor the electronic properties of CoBHT for specific
  applications. Our findings underscore the potential of CoBHT nanosheets
  for use in next-generation electronic, optoelectronic, and catalytic
  devices with tunable properties through strain engineering.

</details>


### [545] [New Directions in Focused Ion Beam Induced Deposition for the Nanoprinting of Functional 3D Heterostructures](https://arxiv.org/abs/2510.03694)
*Frances Isabel Allen*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究利用氦和氖聚焦离子束诱导沉积技术（FIBID）探索三维纳米打印的新方向，制造了包括多材料结构和具有工程内部空隙的纳米结构，并通过先进的电子显微镜技术深入研究了这些纳米结构的化学和结构组成，最后讨论了FIBID作为功能纳米材料设计平台的潜力。


<details>
  <summary>Details</summary>
Motivation: 聚焦离子束（FIB）显微镜不仅是高分辨率的纳米加工工具，还可以通过聚焦离子束诱导沉积（FIBID）技术添加材料，实现复杂纳米结构的直接写入。本研究旨在探索三维纳米打印的新方向。

Method: 利用氦和氖聚焦离子束诱导沉积（FIBID）技术，结合先进的电子显微镜技术，研究纳米结构的化学和结构组成。

Result: 成功制造了包括多材料结构和具有工程内部空隙的纳米结构，并通过电子显微镜揭示了其内部界面和材料转变。

Conclusion: FIBID技术有潜力发展成为一个多功能的纳米材料设计平台，为下一代纳米器件和技术开辟道路。

Abstract: The focused ion beam (FIB) microscope is well established as a
high-resolution machining instrument capable of site-selectively removing
material down to the nanoscale. Beyond subtractive processing, however, the FIB
can also add material using a technique known as focused ion beam induced
deposition (FIBID), enabling the direct-write of complex nanostructures. This
work explores new directions in three-dimensional nanoprinting with FIBID,
harnessing unique features of helium and neon FIBs to fabricate nanoscale
heterostructures, including multimaterial architectures and deposits with
engineered internal voids. Detailed insight into the chemical and structural
composition of these nanostructures is obtained using advanced electron
microscopy, revealing buried interfaces and material transformations. Building
on these results, the evolution of FIBID into a versatile platform for
functional nanomaterials design is discussed, opening pathways toward
next-generation nanoscale devices and technologies.

</details>


### [546] [Hierarchically Engineered Titanium Suboxide Films for High-Efficiency Solar Thermal Conversion](https://arxiv.org/abs/2510.03710)
*Silpa S,Ann Eliza Joseph,Srinivas G,Harish C Barshilia,Vinayak B Kamble*

Main category: cond-mat.mtrl-sci

TL;DR: 采用可扩展的直流磁控溅射技术和后处理退火，成功制备了基于钛氧化物复合薄膜的宽带太阳能吸收涂层，优化后的样品在500°C退火，具有高吸收率（0.913）和低发射率（0.11），有望应用于太阳能热发电。


<details>
  <summary>Details</summary>
Motivation: 开发基于钛氧化物复合薄膜的宽带太阳能吸收涂层，以实现高效、可扩展且具成本效益的太阳能选择性吸收器。

Method: 通过直流磁控溅射制备钛氧化物薄膜，并控制沉积时间和退火温度（在0.45 mbar的固定氧分压下）以形成Ti2O3和TiO2的复合相。

Result: 优化的样品（沉积10分钟，500°C退火）实现了0.913的太阳吸收率和0.11的热发射率。其他样品也表现出良好的吸收率（>0.85）和低发射率（<0.13）。形貌研究显示纳米岛结构增强了光热性能。

Conclusion: 所开发的光谱选择性太阳能吸收涂层具有高吸收率和低发射率，为太阳能热发电提供了有前景的、可扩展且经济高效的解决方案。

Abstract: We report the development of broadband solar absorber coatings based on
titanium suboxide composite thin films on aluminium substrates. The films are
fabricated via scalable DC magnetron sputtering using a Ti target, followed by
post-annealing in a fixed $O_2$ partial pressure of 0.45 mbar. By tuning
deposition time and annealing temperature, a composite phase of $Ti_2O_3$ and
$TiO_2$ was achieved. The Raman mapping of the films substantiates the
distribution and coexistence of the two phases. The optimized sample, deposited
for 10 min and annealed at 500 $^oC$, exhibited a superior solar absorptance
(${\alpha}_s$ = 0.913) and optimally low thermal emittance (${\epsilon}_t$ =
0.11). Nevertheless, the 15- and 20-min deposited films also showed a promising
absorptance (>0.85) and emittance values (<0.13). Morphological studies
revealed island-type nanostructures, leading to enhanced photothermal
performance via electric field confinement, which is validated by optical
simulations. This work provides a promising route toward efficient, scalable,
and cost-effective spectrally selective solar absorbers for solar thermal
applications.

</details>


### [547] [In-situ characterisation and data-driven crystal plasticity analysis of short-to-long crack transition in a ductile aluminium alloy](https://arxiv.org/abs/2510.03713)
*Abdalrhaman Koko,Bemin Sheen,Caitlin Green,Fionn Dunne*

Main category: cond-mat.mtrl-sci

TL;DR: 韧性合金中的裂纹扩展主要受微观结构控制，当能量释放率达到临界值时，塑性变形导致裂纹尖端钝化，从而从弹性驱动的微观结构敏感裂纹扩展转变为塑性主导的裂纹扩展。


<details>
  <summary>Details</summary>
Motivation: 理解韧性合金中从微观结构敏感的短裂纹到受载荷控制的长裂纹的转变机制。

Method: 利用原位高分辨率扫描电子显微镜数字图像相关（SEM-DIC）和电子背散射衍射（EBSD）技术，结合数据驱动的晶体塑性模型，直接计算应力，并提取应力强度因子（SIFs）和能量释放率，以研究裂纹扩展过程。

Result: 在能量释放率较低时，裂纹以准脆性方式扩展，并受微观结构影响。当达到临界能量释放率时，裂纹扩展区发生塑性变形，导致裂纹钝化，实现了从微观结构敏感扩展到塑性主导的转变。

Conclusion: 短裂纹到长裂纹的转变是由过程区控制的，而不是由长度尺度控制的。

Abstract: Crack arrest in ductile alloys plays a critical role in damage-tolerant
design for aerospace and structural applications, yet the transition from
microstructure-sensitive short cracks to load-controlled long cracks remains
poorly understood. Here, we present an in-situ, high-resolution experimental
study of crack propagation in cold-worked 5052 aluminium alloy using scanning
electron microscopy digital image correlation (SEM-DIC), electron backscatter
diffraction (EBSD), and novel data-driven crystal plasticity modelling that
uses the SEM-DIC and EBSD directly to calculate the stress. The local (elastic)
mode I and II stress intensity factors (SIFs) and the (elastic and
elastoplastic) energy release rate were extracted from the DIC-measured
displacement field and correlated with the crack interaction with
microstructural features. We find that the microstructure-sensitive crack grows
in a quasi-brittle manner at low energy release rate until reaching a critical
energy release rate, where the crack's process zone becomes large enough to
invoke plastic deformation that blunts the crack, marking a transition from
elastically driven microstructure-sensitive crack propagation to
plasticity-dominated crack arrest. Our findings establish that the
short-to-long crack transition is process-zone governed, rather than being
length-scale governed.

</details>


### [548] [Electro-optic effects in some sliding ferroelectrics](https://arxiv.org/abs/2510.03738)
*Xueqing Wan,Zhenlong Zhang,Charles Paillard,Jinyang Ni,Lei Zhang,Zhijun Jiang,Laurent Bellaiche*

Main category: cond-mat.mtrl-sci

TL;DR: 滑动铁电材料的电光响应研究，特别是ZrI$_{2}$，发现其电光效应主要由电子贡献驱动，并可通过应变显著增强。


<details>
  <summary>Details</summary>
Motivation: 滑动铁电材料作为一种新型铁电材料，其独特的极化机制和物理性质尚待深入研究，特别是其电光响应特性。

Method: 利用第一性原理计算，聚焦于ZrI$_{2}$作为原型材料，研究其电光响应，并分析了双轴应变和单轴应变对其的影响，同时探索了其他滑动铁电材料的电光性质。

Result: 发现ZrI$_{2}$的电光效应主要由电子而非离子贡献决定，预示着更快的响应速度。应变可以显著增强电光响应，并揭示了带隙与电光响应之间普适性的线性关系。发现了与双轴应变无关的大弹性光学系数。其他滑动铁电材料也表现出类似的性质。

Conclusion: 滑动铁电材料是实现超快非线性光学器件的潜力材料，并揭示了新颖的电光机制。

Abstract: Sliding ferroelectrics, which exhibit out-of-plane polarization arising from
specific stacking rather than conventional ionic displacements, are new types
of ferroelectrics whose underdeveloped physics needs to be explored. Here, we
investigate for the first time the electro-optic (EO) response of these
materials using first-principles calculations, focusing on ZrI$_{2}$ as a
prototype. We reveal that, contrary to conventional ferroelectrics, the EO
effect in ZrI$_{2}$ is dominated by its electronic contribution rather than the
ionic one, which promises faster EO responses. Furthermore, both biaxial and
uniaxial strains significantly enhance this response, and a novel,
universal-like linear relationship between the band gap and such response is
discovered. We also report a large elasto-optic coefficient that is independent
of biaxial strain. Similar large linear EO coefficients and properties are
found in other sliding ferroelectrics, including different zirconium dihalides,
as well as BN and BP bilayers. These findings highlight sliding ferroelectrics
as highly promising candidates for ultrafast nonlinear optical devices and
reveal novel EO mechanisms.

</details>


### [549] [The magnon spectra of g-type altermagnet bulk CrSb](https://arxiv.org/abs/2510.03759)
*Murat Tas*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究使用多体微扰理论计算了CrSb材料的altermagnon谱和手征寿命。


<details>
  <summary>Details</summary>
Motivation: 研究CrSb材料的altermagnon谱和手征寿命，并分析其自旋分裂能带结构和手征altermagnon的寿命差异。

Method: 使用多体微扰理论进行计算。

Result: 计算得到altermagnon在布里渊区K点能量为275 meV，并且在A-M方向上某q点存在大的自旋分裂，导致手征altermagnon寿命差异达到20 fs。

Conclusion: altermagnon的能谱中存在自旋分裂能带结构。手征altermagnon的寿命差异显著，最大可达20 fs。

Abstract: We present a calculation of the magnon spectra and chiral lifetimes of
altermagnons in bulk CrSb using the many-body perturbation theory. The
spin-split band structure is evident in the magnon spectra. Altermagnons attain
an energy of 275 meV at the K point of the Brillouin zone. Due to large spin
splitting at a specific ${\bf q}$ point along the A - M direction, lifetime
differences of chiral altermagnons reach approximately 20 fs.

</details>


### [550] [Observation of a Novel CDW Superstructure in Monolayer 1T-$VS_{2}$ at Room Temperature and its Evolution in Multilayers](https://arxiv.org/abs/2510.03800)
*Samanta Pal,Kaustuv Chatterjee,A. K. Raychaudhuri,Prabir Pal*

Main category: cond-mat.mtrl-sci

TL;DR: 二维晶体中电荷密度波（CDW）超结构是理解其复杂量子态的基础。研究人员通过液相剥离和印章转移工艺（LPESTP）成功制备了单层VS	extsubscript{2}，并在室温下观察到CDW相变。


<details>
  <summary>Details</summary>
Motivation: 研究二维晶体中电荷密度波（CDW）超结构在单层（ML）VS	extsubscript{2}中的自发形成，并理解其量子态。

Method: 采用液相剥离和印章转移工艺（LPESTP）制备单层VS	extsubscript{2}。利用高分辨率透射电子显微镜（HRTEM）和电子衍射（ED）分析其结构和相变。通过角分辨光电子能谱（ADPES）研究钒原子自插层的影响。

Result: 在室温下，单层VS	extsubscript{2}同时存在1T和2H两种多晶型，其中1T相会发生CDW相变。研究发现了单层1T-VS	extsubscript{2}中存在新颖的不commensurate CDW超结构（$m 
obreakrac{	au}{m 7}	imesrac{	au}{m 7}$) R19.1	extsuperscript{o}}）。随着层数的增加，CDW序变为commensurate（$2 	imes 2$）超结构。研究表明，钒原子以V	extsuperscript{3+}离子的形式自插层到多层VS	extsubscript{2}中，这是导致CDW超结构从不commensurate ($m 
obreakrac{	au}{m 7}	imesrac{	au}{m 7}$) R19.1	extsuperscript{o}} 演化到commensurate（$2	imes2$）序的原因。此外，在扭曲的双层1T-VS	extsubscript{2}薄片中观察到了新的莫尔超晶格，其中包含了单层的CDW超结构。

Conclusion: 该研究为理解1T-VS	extsubscript{2}中CDW超结构如何随厚度和钒原子自插层而演化提供了重要平台。

Abstract: Spontaneous formation of charge density wave (CDW) superstructures in
monolayers (MLs) of a two-dimensional (2D) crystal lattice is fundamental in
understanding its complex quantum states. We report a successful top-down
liquid phase exfoliation and stamp transfer process (LPESTP) to create ML
VS\textsubscript{2}, undergoing a CDW transition at room temperature. Using
high-resolution transmission electron microscopy (HRTEM) and electron
diffraction (ED), we observed the coexistence of 1T and 2H polymorphic phases
in VS\textsubscript{2} at room temperature, and only the 1T phase undergoes CDW
transition. We discovered a novel incommensurate CDW superstructure ($\sqrt{7}
\times \sqrt{7}$) R19.1\textsuperscript{o} in ML 1T-VS\textsubscript{2}. With
an increase in the number of layers, the CDW order changes to a commensurate
($2 \times 2$) superstructure. Using angle-dependent photoelectron
spectroscopy, we have shown that vanadium atoms self-intercalate as
V\textsuperscript{3+} ions in multilayer VS\textsubscript{2} and are
responsible for the evolution of the CDW superstructure from the incommensurate
$\sqrt{7} \times \sqrt{7}$) R 19.1\textsuperscript{o} to the commensurate
($2\times2$) order. We also report the observation of novel Moir\'e
superlattices in twisted bilayer 1T-VS\textsubscript{2} flakes with trapped CDW
superstructure of the monolayer. Our findings provide an important platform for
understanding the evolution of CDW superstructures in 1T-VS\textsubscript{2}
with thickness and V self-intercalation.

</details>


### [551] [Hybrid MBE Route to Adsorption-Controlled Growth of BaTiO3 Membranes with Robust Polarization Switching](https://arxiv.org/abs/2510.03834)
*S. Choo,S. Varshney,J. Shah,A. K. Manjeshwar,D. K. Lee,K. A. Mkhoyan,R. D. James,B. Jalan*

Main category: cond-mat.mtrl-sci

TL;DR: 使用混合分子束外延技术和水滴剥离法成功制备了高质量的单晶铌酸钡（BaTiO3）铁电薄膜，实现了亚毫米到毫米尺寸的自由存取，并验证了其优异的介电和铁电性能，为柔性电子器件开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有铁电薄膜合成中可重复性差和化学计量控制精度不足的问题，探索一种可制备高质量、化学计量可控的铁电薄膜的方法。

Method: 采用混合分子束外延（MBE）技术在氧化物牺牲层上生长单晶外延BaTiO3薄膜，随后利用水滴剥离技术获得自由存取的薄膜。

Result: 成功获得了亚毫米到毫米尺寸的单晶BaTiO3薄膜，通过X射线衍射确认了其晶体结构，拉曼光谱证实了其四方对称性。阻抗谱显示介电常数为1340，压电响应力显微镜（PFM）和极化-电场（P-E）回线测量（PUND）验证了铁电功能，剩余极化为5微库仑/平方厘米，矫顽场为63千伏/厘米。

Conclusion: 混合分子束外延（MBE）技术是一种可推广的制备化学计量可控的铁电薄膜的方法，可以实现薄膜在下一代柔性和多功能量子氧化物器件中的集成。

Abstract: Freestanding ferroelectric membranes are promising for flexible electronics,
nonvolatile memory, photonics, and spintronics, but their synthesis is
challenged by the need for reproducibility with precise stoichiometric control.
Here, we demonstrate the adsorption-controlled growth of single-crystalline,
epitaxial BaTiO3 films by hybrid molecular beam epitaxy (MBE) on a binary oxide
sacrificial layer. Using a simple water-droplet lift-off method, we obtained
submillimeter- to millimeter-sized membranes that retained crystallinity, as
confirmed by high-resolution X-ray diffraction, and exhibited robust tetragonal
symmetry by Raman spectroscopy. Impedance spectroscopy confirmed a high
dielectric constant of 1340, reflecting the robust dielectric response of the
membranes. Ferroelectric functionality was revealed by piezoresponse force
microscopy (PFM) and further verified by polarization-electric field (P-E) loop
measurements with Positive-Up-Negative-Down (PUND). The P-E loops exhibited a
remnant polarization of 5 microC cm-2 and a coercive field of 63 kV cm-1. These
results were interpreted in relation to c- and a-domain configurations. These
results establish hybrid MBE as a generalizable route for producing
stoichiometry-controlled ferroelectric membranes, enabling their integration
into next-generation flexible and multifunctional quantum oxide devices.

</details>


### [552] [A van der Waals material exhibiting room temperature broken inversion symmetry with ferroelectricity](https://arxiv.org/abs/2510.03977)
*Fabia F. Athena,Cooper A. Voigt,Mengkun Tian,Anjan Goswami,Emily Toph,Moses Nnaji,Fanuel Mammo,Brent K. Wagner,Sungho Jeon,Wenshan Cai,Eric M. Vogel*

Main category: cond-mat.mtrl-sci

TL;DR: 发现了具有铁电性的新型硒化铟相，名为 $eta^	ext{p}$ 相，其在室温下具有可开关的极化和非线性光学现象。


<details>
  <summary>Details</summary>
Motivation: 在已知的五种硒化铟多晶型物种之外，寻找并表征新的硒化铟相。

Method: 通过扫描透射电子显微镜（STEM）分析，特别关注原子结构、层间相互作用以及电学和光学性质。

Result: 发现了一种新的硒化铟相，$eta^	ext{p}$ 相，具有独特的锯齿形原子构型和层内位移。该相表现出铁电性（电场诱导的可开关极化）和非线性光学现象（二次谐波产生）。

Conclusion: $eta^	ext{p}$ 相硒化铟在低维极限下，在大面积、室温条件下表现出铁电性，这为新材料在电子和光学器件中的应用提供了可能性。

Abstract: Since the initial synthesis of van der Waals two-dimensional indium selenide
was first documented in 1957, five distinct polymorphs and their corresponding
polytypes have been identified. In this study, we report a unique phase of
indium selenide via Scanning Transmission Electron Microscopy (STEM) analysis
in the synthesized large-area films -- which we have named the $\beta^\text{p}$
phase. The quintuple layers of the $\beta^\text{p}$ phase, characterized by a
unique zigzag atomic configuration with unequal indium-selenium bond lengths
from the middle selenium atom, are distinct from any other previously reported
phase of indium selenide. Cross-sectional STEM analysis has revealed that the
$\beta^\text{p}$ layers exhibit intralayer shifting. We found that indium
selenide films with $\beta^\text{p}$ layers display electric-field-induced
switchable polarization characteristic of ferroelectric materials, suggesting
the breaking of the inversion symmetry. Experimental observations of nonlinear
optical phenomena -- Second Harmonic Generation (SHG) responses further support
this conclusion. This study reports a $\beta^\text{p}$ phase of indium selenide
showing ferroelectricity over large areas at room temperature in a
low-dimensional limit.

</details>


### [553] [Adsorption-induced surface magnetism](https://arxiv.org/abs/2510.03897)
*Miloš Baljozović,Shiladitya Karmakar,André L. Fernandes Cauduro,Mothuku Shyam Sundar,Marco Lozano,Manish Kumar,Diego Soler Polo,Andreas K. Schmid,Ashutosh V. Bedekar,Pavel Jelinek,Karl-Heinz Ernst*

Main category: cond-mat.mtrl-sci

TL;DR: 在非磁性的Cu(100)表面上，通过杂化螺旋桨分子诱导出吸附诱导的磁性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在非磁性金属表面诱导产生磁性，为有机-无机杂化体系中自旋极化态的设计提供新的途径。

Method: 利用自旋极化低能电子显微镜（SP-LEEM）测量手性纯7,12,17-三氧[11]螺旋桨（TO[11]H）单分子层在Cu(100)表面的自旋依赖电子反射率；结合自旋极化密度泛函理论（DFT）计算和Newns-Anderson-Grimley模型进行理论分析。

Result: SP-LEEM测量显示TO[11]H单分子层在Cu(100)表面形成了自旋极化的态，该态局域在最上层的铜原子中。控制实验排除了表面吸附或分子手性本身是产生该效应的原因。DFT计算和模型表明，磁性起源于强化学吸附引起的分子HOMO轨道与铜s和d电子态的杂化，以及铜d电子的库仑排斥作用。

Conclusion: 通过化学吸附诱导分子轨道与金属d电子轨道杂化，可以在分子-金属界面产生非磁性诱导磁性。这种机制不依赖于分子本身具有磁性，为在有机-无机杂化体系中设计和控制自旋极化现象提供了新的策略。

Abstract: We report the emergence of adsorption-induced magnetism from heterohelicene
molecules on a non-magnetic Cu(100) surface. Spin-polarized low-energy electron
microscopy (SP-LEEM) measurements reveal spin-dependent electron reflectivity
for enantiopure 7,12,17-trioxa[11]helicene (TO[11]H) monolayers, indicating the
formation of a spin-polarized state localized in the topmost copper layer.
Control experiments on clean Cu(100) and TO[11]H on highly oriented pyrolytic
graphite show no such effect, excluding artifacts and chirality-induced spin
selectivity as origins. Spin-polarized density functional theory calculations
with hybrid functionals attribute the magnetism to strong chemisorption, which
induces hybridization between the molecular HOMO and copper s- and d-states,
driving asymmetric spin-polarized charge redistribution at the interface. An
extended Newns-Anderson-Grimley model incorporating on-site Coulomb repulsion
in Cu d-orbitals reproduces the emergence of interfacial spin polarization
above a threshold interaction strength, highlighting the key roles of
hybridization parameters and Coulomb correlation. These findings reveal a
mechanism for inducing magnetism at molecule-metal interfaces without
inherently magnetic components, offering new avenues for engineering
spin-polarized states in organic-inorganic hybrid systems.

</details>


### [554] [Finding the temperature window for atomic layer deposition of ruthenium metal via efficient phonon calculations](https://arxiv.org/abs/2510.03975)
*Alexandr Fonari,Simon D. Elliott,Casey N. Brock,Yan Li,Jacob Gavartin,Mathew D. Halls*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究利用第一性原理热力学（基于周期性密度泛函理论DFT）研究了氧化钌表面与氢气反应的气-固化学，该反应是原子层沉积（ALD）生长超薄钌薄膜的关键。研究结果解释了ALD在100°C以上狭窄温度窗口内进行的实验观察，并预测了水在113°C以上会发生热解吸，暴露出可进一步反应沉积钌金属的裸钌表面。计算出的生长速率为0.7埃/周期。在较低温度下，计算表明水会滞留并与RuO4前驱体反应形成氧化膜。研究还强调了气体分子的转动和平动熵在自由能中起主导作用，为快速筛选气-固化学反应提供了替代方法。


<details>
  <summary>Details</summary>
Motivation: 原子层沉积（ALD）生长超薄钌（Ru）薄膜是该研究的背景，特别是利用RuO4和H2作为前驱体。研究旨在理解和解释ALD在特定狭窄温度窗口（100°C以上）内进行的原因，并为优化此过程提供理论依据。

Method: 本研究采用基于周期性密度泛函理论（DFT）的第一性原理热力学方法，计算了氧化钌表面与H2反应的竞争反应的温度依赖性自由能。研究中包含了使用谐波近似的声子计算来获得吉布斯自由能修正，并利用云架构并行化来加速计算。此外，还探索了仅考虑气体分子转动和平动熵的近似方法。

Result: 计算结果表明，在113°C以上，反应生成的副产物水会发生热解吸，暴露出的裸钌表面随后可以被还原，从而实现钌金属的沉积。计算预测的生长速率为0.7埃/周期。在低于此温度时，水会滞留并与RuO4前驱体反应形成氧化膜，这与实验观察一致。研究还发现，气体的转动和转动熵对自由能的影响最大。

Conclusion: 该研究成功地利用第一性原理计算解释了钌ALD在特定温度窗口内的行为，并预测了关键的反应路径和生长速率。研究强调了计算热力学在理解和优化ALD过程中的重要性，并提出了一种无需复杂声子计算的快速筛选方法，适用于探索其他气-固化学反应。

Abstract: We investigate the use of first principles thermodynamics based on periodic
density functional theory (DFT) to examine the gas-surface chemistry of an
oxidized ruthenium surface reacting with hydrogen gas. This reaction system
features in the growth of ultrathin Ru films by atomic layer deposition (ALD).
We reproduce and rationalize the experimental observation that ALD of the metal
from RuO4 and H2 occurs only in a narrow temperature window above 100{\deg}C,
and this validates the approach. Specifically, the temperature-dependent
reaction free energies are computed for the competing potential reactions of
the H2 reagent, and show that surface oxide is reduced to water, which is
predicted to desorb thermally above 113{\deg}C, exposing bare Ru that can
further react to surface hydride, and hence deposit Ru metal. The saturating
coverages give a predicted growth rate of 0.7 \r{A}/cycle of Ru. At lower
temperatures, free energies indicate that water is retained at the surface and
reacts with the RuO4 precursor to form an oxide film, also in agreement with
experiment. The temperature dependence is obtained with the required accuracy
by computing Gibbs free energy corrections from phonon calculations within the
harmonic approximation. Surface phonons are computed rapidly and efficiently by
parallelization on a cloud architecture within the Schr\"odinger Materials
Science Suite. We also show that rotational and translational entropy of gases
dominate the free energies, permitting an alternative approach without phonon
calculations, which would be suitable for rapid pre-screening of gas-surface
chemistries.

</details>


### [555] [Atomistic Machine Learning with Cartesian Natural Tensors](https://arxiv.org/abs/2510.04015)
*Qun Chen,A. S. L. Subrahmanyam Pattamatta,David J. Srolovitz,Mingjian Wen*

Main category: cond-mat.mtrl-sci

TL;DR: CarNet是一个通用框架，用于解决现有原子尺度机器学习模型在表示物理张量时缺乏对称性框架的挑战。该模型能够处理从偶极矩到弹性常数张量等各种张量，并能提高材料和分子系统的互原子势的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的原子尺度机器学习模型在笛卡尔空间中表示物理张量时，缺乏系统性的对称性保持框架，导致在处理具有复杂对称性的物理张量时面临挑战。

Method: 提出了一种名为CarNet（Cartesian Natural Tensor Networks）的通用框架，并开发了使用笛卡尔自然张量（包括其创建、操作以及物理张量的分解和重构）的不可约表示理论。在此基础上，设计了一个等变笛卡尔模型。

Result: CarNet模型在各种原子尺度机器学习任务中表现出色，能够为材料和分子系统开发高精度和高可靠性的互原子势。此外，它还可以轻松构建用于张量量（从简单的偶极矩到任意高阶张量，如弹性常数张量）的结构-性质关系。

Conclusion: CarNet消除了理论障碍，为使用笛卡尔方法进行先进的原子尺度机器学习铺平了道路，有望在理解和设计新材料方面发挥重要作用。

Abstract: Atomistic machine learning (ML) is a transformative tool for accurate and
efficient investigation of material behavior at the atomic scale. While such
models have been constructed within Cartesian space to harness geometric
information and preserve intuitive physical representations, they face inherent
challenges - primarily due to the lack of a systematic symmetry-preserving
framework for representing arbitrary physical tensors. We address these
challenges by proposing Cartesian Natural Tensor Networks (CarNet) as a general
framework for atomistic ML. We first develop the theory of irreducible
representations using Cartesian natural tensors (their creation, operation, as
well as the decomposition and reconstruction of physical tensors such as the
elastic constant tensor). Leveraging this machinery, we design an equivariant
Cartesian model and demonstrate its exceptional performance across diverse
atomistic ML tasks. CarNet enables the development of highly accurate and
reliable interatomic potentials for both materials and molecular systems.
Furthermore, structure-property relationships can be readily constructed for
tensorial quantities ranging from simple properties like the dipole moment to
arbitrary high-rank tensors with complex symmetries such as the elastic
constant tensor -- capabilities that were previously inaccessible. This work
removes theoretical barriers and unleashes the power of Cartesian approaches
for advanced atomistic ML in the understanding and design of new materials.

</details>


### [556] [Direct observation of band structure modifications from monolayer WSe2 to Janus WSSe](https://arxiv.org/abs/2510.04113)
*Masato Sakano,Shunsuke Akatsuka,Takato Yamamoto,Tianyishan Sun,Dingkun Bi,Hiroto Ogura,Naoya Yamaguchi,Fumiyuki Ishii,Natsuki Mitsuishi,Kenji Watanabe,Takashi Taniguchi,Miho Kitamura,Koji Horiba,Kenichi Ozawa,Katsuaki Sugawara,Seigo Souma,Takafumi Sato,Yuta Seo,Satoru Masubuchi,Tomoki Machida,Toshiaki Kato,Kyoko Ishizaka*

Main category: cond-mat.mtrl-sci

TL;DR: Janus TMDs的电子结构通过微区角分辨光电子能谱得到研究，揭示了其能带结构的演变。


<details>
  <summary>Details</summary>
Motivation: Janus TMDs 的快速环境降解和难以获得大面积单层样品阻碍了对其费米能级附近详细电子结构的实验探测。

Method: 通过对相同的单层 WSe2 样品进行 H2 等离子体辅助硫族元素交换，将其转化为 Janus WSSe，并进行微区角分辨光电子能谱（μ-ARPES）测量，揭示其电子能带结构的演变。

Result: 观察到与Rashba型自旋分裂一致的ARPES信号，以及在Γ点最高价带向上移动约160 meV的现象。

Conclusion: 直接观测阐明了控制材料性质的关键电子改性，并为Janus TMDs的能带工程提供了途径。

Abstract: Janus monolayer transition metal dichalcogenides (TMDs), created by
post-growth substitution of the top chalcogen layer, represent a new direction
for engineering 2D crystal properties. However, their rapid ambient degradation
and the difficulty of obtaining large-area monolayer samples have limited the
available experimental probes, leaving their detailed electronic structure near
the Fermi level largely unexplored. In this work, by performing micro-focused
angle-resolved photoemission spectroscopy ({\mu}-ARPES) on an identical sample
transformed from monolayer WSe2 to Janus WSSe via a H2 plasma-assisted
chalcogen-exchange method, we reveal the evolution of its electronic band
structure. We observe ARPES signature consistent with the Rashba-type spin
splitting due to broken horizontal mirror symmetry, and a significant upward
shift of the highest valence band at the {\Gamma}-point by approximately 160
meV. These direct observations clarify the key electronic modifications that
govern the material's properties and provide a pathway for band engineering in
Janus TMDs.

</details>


### [557] [Dynamic breaking of axial symmetry of acoustic waves in crystals as the origin of nonlinear inelasticity and chaos: Analytical model and MD simulations](https://arxiv.org/abs/2510.04175)
*Zbigniew Kozioł*

Main category: cond-mat.mtrl-sci

TL;DR: 动态压力导致FCC晶体中出现新的力，打破了轴向对称性，导致非线性、非弹性响应和原子运动的混沌模式。


<details>
  <summary>Details</summary>
Motivation: 研究分子动力学模拟中FCC晶体中原子的运动，特别是与位错线动力学相关的模拟，并提出一种新的分析方法。

Method: 使用弹簧-质量链（CSM）模型来解释分子动力学（MD）模拟。推导了原子间相互作用的有效原子势，并提出了原子动力学的解析模型。

Result: 发现了一个垂直于外加剪切压力方向、平行于位错线的力。该力与外加压力成平方反比，导致横声波传播的轴向对称性破坏，以及晶体的非线性、非弹性响应和原子运动的混沌模式。模型预测了实验中可能被忽略且与晶体静弹性理论不符的效应。

Conclusion: 提出的CSM模型和原子动力学模型能够解释MD模拟中观察到的现象，包括新的动态力的存在及其引起的非线性效应，并可能为理解晶体动力学提供新的视角。

Abstract: A Chain of Springs and Masses (CSM) model is used in the interpretation of
molecular dynamics (MD) simulations of movement of atoms in FCC crystals,
oriented like during typical simulations performed in studies of the dynamics
of line dislocations. The proposed description is inspired by and supported by
MD simulations. We find out that a force that is perpendicular to the direction
of the applied external shear pressure and in the direction of line
dislocations occurs within the bulk of crystal volume. The force is of a
dynamic origin, and it has not been analyzed so far. It is proportional to the
square of the applied pressure; It causes breaking of axial symmetry for
propagation of transverse acoustic waves. It leads to a non-linear and
non-elastic response of crystals and to chaotic patterns in motion of atoms. We
provide an analytical derivation of an effective atomistic potential for
interaction between atoms and propose an analytical model of their dynamics.
The model predicts some effects that may have been overlooked in experiments
and are inconsistent with the static theory of elasticity of crystals.

</details>


### [558] [Longitudinal transport spin polarization of spin degenerate antiferromagnets](https://arxiv.org/abs/2510.04184)
*Meng Zhu,Jianting Dong,Xinlu Li,Jiahao Shentu,Yizhuo Song,Evgeny Y. Tsymbal,Jia Zhang*

Main category: cond-mat.mtrl-sci

TL;DR: PT对称性的反铁磁体（PT-AFMs）也能产生纵向自旋流，这与普遍预期相反，并且其产生的纵向自旋流的幅度可与PT对称性破缺的反铁磁体相媲美。


<details>
  <summary>Details</summary>
Motivation: 探索在反铁磁体中高效电学生成自旋流的可能性，特别是关注具有PT对称性的反铁磁体（PT-AFMs）是否也能产生纵向自旋流。

Method: 利用磁点群对称性进行理论分析，并使用密度泛函理论计算了代表性的PT-AFMs（L10-MnPt和Mn2Au）的纵向自旋电导率。

Result: 理论分析表明，大多数PT-AFMs由于自旋-轨道耦合可以产生纵向自旋流，其纵向自旋电导率的幅度与PT对称性破缺的反铁磁体相当。

Conclusion: 具有PT对称性的反铁磁体（PT-AFMs）也能有效产生纵向自旋流，这扩展了对自旋输运的理解，并展示了在广泛的自旋简并反铁磁体中实现鲁棒自旋流生成的可能性。

Abstract: A vital goal in spintronics is the efficient electrical generation of spin
currents, a pursuit that has recently focused on using antiferromagnets (AFMs)
as spin current sources. It has been demonstrated that antiferromagnets with
broken PT symmetry (parity + time reversal) can efficiently generate
longitudinal and transverse spin currents. At the same time, it has been
generally thought that antiferromagnets with PT symmetry (PT-AFMs) forbid the
longitudinal spin polarization due to their spin-degenerate band structure.
Here, in contrast to this common expectation, we show, using theoretical
analysis based on magnetic point group symmetry, that most PT-AFMs can generate
longitudinal spin currents due to spin-orbit coupling. Using density-functional
theory, we calculate the longitudinal spin conductivity of representative
PT-AFMs, L10-MnPt and Mn2Au, and show that its magnitude is comparable to that
of their PT-broken counterparts. Our symmetry-enabled classification of
antiferromagnets and theoretical results for the longitudinal spin conductivity
in representative PT-AFMs expands our understanding of spin transport and shows
the possibility of robust spin-current generation in a broad range of
spin-degenerate antiferromagnets.

</details>


### [559] [On the Origin of Carrier Loss in Mg-Doped N-Polar GaN](https://arxiv.org/abs/2510.04381)
*Masahiro Kamiyama,Shashwat Rathkanthiwar,Cristyan Quiñones-García,Seiji Mita,Dolar Khachariya,Pramod Reddy,Ronny Kirste,Ramón Collazo,Zlatko Sitar*

Main category: cond-mat.mtrl-sci

TL;DR: Mg掺杂的N极性GaN中，Mg浓度超过10^19 cm^-3时，空穴浓度急剧下降，这主要是由(VN-3MgGa)0复合物的形成引起的，它导致了Mg的电活性降低。


<details>
  <summary>Details</summary>
Motivation: 研究Mg掺杂的N极性GaN中Mg浓度过高时出现的反常现象，即空穴浓度急剧下降。

Method: 结合实验数据（包括温度依赖的霍尔测量）和电荷平衡模型，并建立了一个基于巨正则系综形式的半经验模型来解释实验现象。

Result: 实验数据和模型均表明，当Mg浓度超过约10^19 cm^-3时，空穴浓度急剧下降是由于Mg的受主浓度（NA）大幅降低，这归因于(VN-3MgGa)0复合物的形成，该复合物捕获了部分Mg原子，使其不显电性。

Conclusion: 在Mg掺杂的N极性GaN中，(VN-3MgGa)0复合物是主要的补偿体，它导致了Mg的电活性降低，是Mg浓度过高时载流子损失的主要原因。

Abstract: The neutral $(V_N-3Mg_{Ga})^0$ complex was found to be the primary
compensator in Mg-doped, N-polar GaN. The experimental data showed a sharp drop
in hole concentration once [Mg] exceeded ~$10^{19} cm^{-3}$.
Temperature-dependent Hall measurements, in conjunction with a charge balance
model, revealed that the carrier loss was due to a drastic reduction in
acceptor concentration ($N_A$), suggesting that a significant fraction of Mg
atoms incorporated in an electrically neutral configuration. A quantitative
semi-empirical model based on the grand canonical formalism pointed to the
formation of $(V_N-3Mg_{Ga})^0$ complexes as the primary cause for the observed
carrier loss.

</details>


### [560] [Investigating into mechanisms of high temperature strength of refractory high-entropy alloys](https://arxiv.org/abs/2510.04589)
*Sai Anandhi Seetharaman,Soumyadipta Maiti,Ambesh Gupta,Beena Rai*

Main category: cond-mat.mtrl-sci

TL;DR: 研究了MoNbTaVW和MoNbTaW两种BCC难熔高熵合金的屈服强度平台，通过混合蒙特卡洛和分子动力学模拟，发现DSA和晶格畸变共同导致了屈服强度平台。


<details>
  <summary>Details</summary>
Motivation: 研究了两种BCC难熔高熵合金（RHEAs）的屈服强度平台现象，旨在理解其背后的微观机制。

Method: 使用混合蒙特卡洛和分子动力学（MC/MD）模拟，分析了原子扩散率、空位形成和迁移能、位错核心处的原子交换数量，并模拟了临界原子交换过程，还研究了随机固溶体。

Result: 在1400K以上，位错移动所需的应力饱和，表明动态应变时效（DSA）通过跨核心运动产生影响。随机固溶体在中间温度也表现出平台效应，归因于固溶强化引起的晶格畸变产生的额外非热应力。

Conclusion: 屈服强度平台是DSA驱动的扩散过程和非热应力共同作用的结果。DSA机制在有原子扩散时出现平台，而在无扩散时，则由非热统计晶格畸变决定。该双机制框架全面解释了RHEAs在中间温度下观察到的屈服强度行为。

Abstract: The yield strength plateau of two BCC refractory high entropy alloys (RHEAs)
- MoNbTaVW and MoNbTaW was examined through hybrid Monte Carlo and molecular
dynamics (MC/MD) simulations. By analyzing atomic diffusivities derived from
vacancy formation and migration energies around the edge dislocation cores, the
number of critical atomic swaps were calculated at different temperatures.
Using hybrid MC/MD simulations of these critical swaps, we demonstrate that
above 1400K, the stress required to move the dislocations gets saturated,
indicating the effect of Dynamic Strain Ageing (DSA) via cross core motion.
Further simulations on random solid solutions (0 MC swaps) revealed a similar
plateau effect at the intermediate temperatures. This was attributed to the
additional athermal stress arising from lattice distortions due to solid
solution strengthening. Our findings suggest that the yield strength plateau
results from an interplay between the DSA-driven diffusion process and athermal
stress. Specifically, the plateau emerges from DSA mechanisms in the presence
of atomic diffusion, whereas in the absence of diffusion, it is governed by
athermal statistical lattice distortions. This dual mechanism framework
provides a comprehensive explanation for the experimentally observed Yield
strength behavior in RHEAs at intermediate temperatures.

</details>


### [561] [AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials](https://arxiv.org/abs/2510.04704)
*Taoyuze Lv,Alexander Chen,Fengyu Xie,Chu Wu,Jeffrey Meng,Dongzhan Zhou,Bram Hoex,Zhicheng Zhong,Tong Xie*

Main category: cond-mat.mtrl-sci

TL;DR: LLMs在处理晶体结构数据时存在结构理解和空间推理方面的局限性，但AtomWorld基准的提出将推动LLMs在原子尺度建模能力的发展，以加速材料研究。


<details>
  <summary>Details</summary>
Motivation: 当前的LLMs在处理三维原子结构方面存在局限性，缺乏一个统一的基准来系统地评估它们在多样化的原子结构上的核心推理能力，尤其是在材料科学领域。

Method: 提出了AtomWorld基准，包含结构编辑、CIF感知和属性引导建模等任务，使用晶体信息文件（CIF）作为标准结构表示格式，来评估LLMs在处理晶体结构数据时的能力。

Result: 尽管LLMs在某些方面表现出潜力，但它们在结构理解和空间推理方面存在显著的局限性，即使在基本的CIF格式理解任务中也频繁出错，这可能导致后续分析和材料洞察的累积误差。

Conclusion: AtomWorld基准的提出为未来改进LLMs在原子尺度建模方面的能力奠定了基础，这对于加速材料研究和自动化科学工作流程至关重要。

Abstract: Large Language Models (LLMs) excel at textual reasoning and are beginning to
develop spatial understanding, prompting the question of whether these
abilities can be combined for complex, domain-specific tasks. This question is
essential in fields like materials science, where deep understanding of 3D
atomic structures is fundamental. While initial studies have successfully
applied LLMs to tasks involving pure crystal generation or coordinate
understandings, a standardized benchmark to systematically evaluate their core
reasoning abilities across diverse atomic structures has been notably absent.
To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on
tasks based in Crystallographic Information Files (CIFs), a standard structure
representation format. These tasks, including structural editing, CIF
perception, and property-guided modeling, reveal a critical limitation: current
models, despite establishing promising baselines, consistently fail in
structural understanding and spatial reasoning. Our experiments show that these
models make frequent errors on structure modification tasks, and even in the
basic CIF format understandings, potentially leading to cumulative errors in
subsequent analysis and materials insights. By defining these standardized
tasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale
modeling, crucial for accelerating materials research and automating scientific
workflows.

</details>


### [562] [Correlative Analysis of Iron-Driven Structural, Optical, and Magnetic Properties in Natural Biotite Crystals](https://arxiv.org/abs/2510.04752)
*Raphaela de Oliveira,Yara Galvão Gobato,Ronei C. de Oliveira,José R. de Toledo,Verônica C. Teixeira,Angelo Malachias,Cesar R. Rabahi,Chunwei Hsu,Adilson J. A. de Oliveira,Herre. S. J. van der Zant,Ingrid D. Barcelos,Alisson R. Cadore*

Main category: cond-mat.mtrl-sci

TL;DR: Biotite晶体因其宽带隙和层状结构，可用作下一代电子和能源收集设备的介电材料。本研究通过实验手段全面研究了不同铁含量和氧化态的生物云母样品，并将其铁化学与其宏观性质相关联。研究结果揭示了生物云母的纳米级均匀铁分布、由Fe3+/Fe2+比率调节的缺陷介导的光学跃迁以及从顺磁性到铁磁性/反铁磁性相互作用的温度依赖性磁跃迁。此外，通过低温磁光致发光探索了将这些生物云母晶体用作包含单层MoSe2的超薄异质结构衬底。研究结果表明，不同氧化态的铁杂质的存在显著影响了单层MoSe2的谷特性。总的来说，这些发现为生物云母的物理特性提供了一个全面的解释，为未来研究其超薄形式提供了参考。


<details>
  <summary>Details</summary>
Motivation: 探索不同铁含量和氧化态的生物云母的结构、光学、磁光和磁性，并研究铁化学与其宏观性质之间的相关性。此外，还研究了生物云母作为超薄异质结构衬底的潜力，以及铁杂质对单层MoSe2谷特性的影响。

Method: 使用包括同步辐射技术在内的多种实验技术，对具有不同铁含量和氧化态的生物云母样品进行结构、光学、磁光和磁性表征。采用同步辐射X射线荧光成像技术分析铁的分布，并进行磁光致发光实验研究铁杂质对单层MoSe2谷特性的影响。

Result: 研究揭示了纳米级均匀的铁分布，由Fe3+/Fe2+比率调节的光学跃迁，以及从顺磁性到铁磁性/反铁磁性相互作用的温度依赖性磁跃迁。生物云母作为衬底影响了单层MoSe2的谷特性。

Conclusion: 本研究全面解释了块状生物云母的物理特性，并为未来研究其超薄形式提供了参考。

Abstract: Biotite crystals are phyllosilicate trioctahedral micas with the general
chemical formula K(Mg,Fe)3AlSi3O10(OH)2 that form a solid-solution series with
iron-poor phlogopite and iron-rich annite endmembers. With a wide band gap
energy and a layered structure with free surface charges, biotite nanosheets
can be readily obtained by cleavage methods and used as dielectrics in
nanodevice fabrication for the next generation of electronics and energy
harvesting. Here, a comprehensive study of biotite samples with different iron
concentrations and oxidation states is presented. Structural, optical,
magneto-optical, and magnetic characterizations were performed using several
experimental techniques, including state-of-the-art synchrotron-based
techniques, to correlate the iron chemistry (content and oxidation state) with
the macroscopic properties of both minerals. The study reveals a
nanoscale-homogeneous Fe distribution via synchrotron X-ray fluorescence
mapping, defect-mediated optical transitions modulated by Fe3+/Fe2+ ratios, and
temperature-dependent magnetic transitions from paramagnetism to competing
ferro-/antiferromagnetic interactions. Furthermore, the use of these biotite
crystals as substrates for ultrathin heterostructures incorporating monolayer
(ML) MoSe2 is explored by magneto photoluminescence at cryogenic temperatures.
The results show that the presence of iron impurities in different oxidation
states significantly impacts the valley properties for ML-MoSe2. Overall, these
findings offer a comprehensive interpretation of the physical properties of
bulk biotites in a correlative approach, serving as a robust reference for
future studies aiming to explore biotites in their ultrathin form.

</details>


### [563] [Electronic and thermal properties of the phase-change memory material, Ge2Sb2Te5, and results from spatially resolved transport calculations](https://arxiv.org/abs/2510.04783)
*Kishor Nepal,Aashish Gautam,Ridwan Hussein,Konstantinos Konstantinou,Stephen. R. Elliott,Chinonso Ugwumadu,David A. Drabold*

Main category: cond-mat.mtrl-sci

TL;DR: Ge2Sb2Te5在电子、结构和传输性质方面有新的见解，揭示了费米能级附近的 Kohn-Sham 轨道表现出强烈的电子-声子耦合，并且在室温下表现出大的能量波动。此外， the conduction tail states exhibit larger phonon-induced fluctuations than the valence tail states. 并且解析了 the conduction tail states exhibit larger phonon-induced fluctuations than the valence tail states。


<details>
  <summary>Details</summary>
Motivation: 使用真实的结构模型，研究Ge2Sb2Te5（一种相变存储器材料）的电子、结构和传输（热和电）性质。

Method: 利用密度泛函方法（包括混合泛函计算和机器学习力场），分析Ge2Sb2Te5的拓扑、电子态和晶格动力学。采用空间投影电子电导率和位点投影热导率方法来解析原子尺度的传输。

Result: 研究发现，费米能级附近的Kohn-Sham轨道表现出强烈的电子-声子耦合，并在室温下表现出大的能量波动。 the conduction tail states exhibit larger phonon-induced fluctuations than the valence tail states。热传输的局部分析强调了由Te主导的链状网络的关键作用，Sb和Ge的贡献逐渐减小。

Conclusion: Ge2Sb2Te5在电子、结构和传输性质方面有新的见解，揭示了费米能级附近的 Kohn-Sham 轨道表现出强烈的电子-声子耦合，并且在室温下表现出大的能量波动。此外， the conduction tail states exhibit larger phonon-induced fluctuations than the valence tail states。热传输的局部分析强调了由Te主导的链状网络的关键作用，Sb和Ge的贡献逐渐减小。

Abstract: We report new insights into the electronic, structural, and transport (heat
and charge) properties of the phase-change memory material Ge2Sb2Te5. Using
realistic structural models of Konstantinou et. al. [Nat. Commun. 10, 3065
(2019)], we analyze the topology, electronic states, and lattice dynamics with
density functional methods, including hybrid-functional calculations and
machine-learned interatomic potentials. The Kohn-Sham orbitals near the Fermi
level display a strong electron-phonon coupling, and exhibit large energy
fluctuations at room temperature. The conduction tail states exhibit larger
phonon-induced fluctuations than the valence tail states. To resolve transport
at the atomic scale, we employ space-projected electronic conductivity and
site-projected thermal conductivity methods. Local analysis of heat transport
highlights the role of filamentary networks dominated by Te, with Sb and Ge
making progressively smaller contributions.

</details>


### [564] [Chasing Anharmonicities in Polarization-Orientation Raman Spectra of Acene Crystals with Machine Learning](https://arxiv.org/abs/2510.04843)
*Paolo Lazzaroni,Shubham Sharma,Mariana Rossi*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究提出了一种基于第一性原理和机器学习的计算框架，用于研究分子晶体（如蒽和萘）中极化-取向（PO）拉曼光谱的非谐效应。该框架结合了用于原子间势和极化张量的机器学习模型，实现了高效的大规模模拟，能够超越谐波近似来捕捉随温度变化的振动动力学，并能重现实验观察到的关键特征。研究系统地阐释了非谐晶格动力学、热膨胀和拉曼张量对称性对PO-拉曼强度的影响。然而，模拟结果在拉曼强度极化依赖性方面与准谐波预测的偏差仅为细微的，未能捕捉到实验中报道的蒽的显著温度依赖性变化。研究者推测，部分不一致可能源于仅凭实验数据无法解卷积某些振动峰。该工作为借助理论模拟改进复杂分子晶体PO-拉曼实验的解释奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种计算框架，以研究分子晶体中非谐效应如何影响极化-取向（PO）拉曼光谱，并提高对实验数据的解释能力。

Method: 结合使用机器学习模型来预测原子间势和极化张量，实现高效的大规模模拟，以捕捉超越谐波近似的温度依赖性振动动力学。

Result: 模拟结果重现了实验的关键定性特征，并阐明了非谐晶格动力学、热膨胀和拉曼张量对称性对PO-拉曼强度的影响。然而，模拟未能捕捉到蒽实验中观察到的显著的温度依赖性变化，其极化依赖性与准谐波预测的偏差仅为细微的。

Conclusion: 该研究提出的计算框架为理解和解释分子晶体中的PO-拉曼光谱提供了理论基础，但仍需进一步研究以解决模拟与实验在特定温度依赖性变化方面存在的差异，并提出了解卷积振动峰的挑战。

Abstract: We present a first-principles machine-learning computational framework to
investigate anharmonic effects in polarization-orientation (PO) Raman spectra
of molecular crystals, focusing on anthracene and naphthalene. By combining
machine learning models for interatomic potentials and polarizability tensors,
we enable efficient, large-scale simulations that capture temperature-dependent
vibrational dynamics beyond the harmonic approximation. Our approach reproduces
key qualitative features observed experimentally. We show, systematically, what
are the fingerprints of anharmonic lattice dynamics, thermal expansion, and
Raman tensor symmetries on PO-Raman intensities. However, we find that the
simulated polarization dependence of Raman intensities shows only subtle
deviations from quasi-harmonic predictions, failing to capture the pronounced
temperature-dependent changes that have been reported experimentally in
anthracene. We propose that part of these inconsistencies stem from the
impossibility to deconvolute certain vibrational peaks when only experimental
data is available. This work therefore provides a foundation to improve the
interpretation of PO-Raman experiments in complex molecular crystals with the
aid of theoretical simulations.

</details>


### [565] [Atomistic Insights into the Degradation of Metal Phthalocyanine Catalysts during Oxygen Reduction Reaction](https://arxiv.org/abs/2510.04922)
*Huanhuan Yang,Guangfu Luo*

Main category: cond-mat.mtrl-sci

TL;DR: 金属酞菁（MPc）在氧还原反应（ORR）中易降解，影响其在苛刻条件下的应用。本研究结合第一性原理计算和瞬态微动力学模型，探究了六种MPc（M = Cr, Mn, Fe, Ru, Rh, Ir）的ORR失活途径。


<details>
  <summary>Details</summary>
Motivation: 金属酞菁（MPc）在氧还原反应（ORR）中经常在苛刻的操作条件下发生降解，但对其降解机理的理解有限，阻碍了有效缓解策略的开发。

Method: 通过包含40个化学物种和75个基本反应的反应网络，利用第一性原理计算和瞬态微动力学模型，定量评估ORR过程、过氧化氢生成、自由基生成以及碳氧化、氮质子化和脱金属三种主要降解机制。

Result: 研究发现，主导降解机制因MPc而异。在典型的碱性条件下，主要副产物来自.OH自由基攻击和表面吸附物结构重组引起的碳氧化，以及金属中心或氮位点的质子化。在动力学控制区域，ORR活性顺序为RhPc > IrPc > FePc > MnPc > RuPc > CrPc。RhPc和IrPc在较高电势下表现出比FePc更高的ORR活性和稳定性。

Conclusion: 金属酞菁在ORR过程中会经历多种降解途径，其中碳氧化和质子化在碱性条件下尤为重要。RhPc和IrPc在活性和稳定性方面优于FePc，显示出作为ORR催化剂的潜力。

Abstract: Oxygen reduction catalysts frequently suffer from degradation under harsh
operating conditions, and the limited understanding of the underlying
mechanisms hampers the development of effective mitigation strategies. In this
study, we integrate first-principles calculations with a time-dependent
microkinetic model to investigate the deactivation pathways of six highly
active metal phthalocyanines (MPc, M = Cr, Mn, Fe, Ru, Rh, and Ir) during the
oxygen reduction reaction (ORR). We quantitatively assess the ORR processes,
hydrogen peroxide generation, radical generation, and three primary degradation
mechanisms, namely carbon oxidation, nitrogen protonation, and demetallation,
through a reaction network involving 40 chemical species and 75 elementary
reactions. Our findings reveal that the dominant degradation mechanism varies
significantly across the MPcs. Under typical alkaline conditions, the primary
byproducts arise from carbon oxidation, driven by .OH radical attack and
structural reorganization of surface adsorbates, and from protonation at either
the metal center or nitrogen sites. In the kinetics-controlled region, the ORR
activity follows the order of RhPc > IrPc > FePc > MnPc > RuPc > CrPc. Notably,
RhPc and IrPc demonstrate both higher ORR activity and greater stability than
the widely studied FePc under elevated potentials.

</details>


### [566] [Comparing fine-tuning strategies of MACE machine learning force field for modeling Li-ion diffusion in LiF for batteries](https://arxiv.org/abs/2510.05020)
*Nada Alghamdi,Paolo de Angelis,Pietro Asinari,Eliodoro Chiavazzo*

Main category: cond-mat.mtrl-sci

TL;DR: MACE模型在预测锂离子电池固态电解质界面中LiF的锂扩散性方面，达到了与经过充分训练的DeePMD势相媲美的准确度，并且所需训练数据量极少。


<details>
  <summary>Details</summary>
Motivation: 为了在电池运行等关键领域实现对复杂现象的研究，需要开发能够准确预测材料性质的机器学习模型，特别是针对锂离子电池中的固态电解质界面。

Method: 通过分子动力学模拟，将MACE机器学习模型与经过充分训练的DeePMD势进行基准测试，以预测LiF中的锂扩散性。比较了MACE-MPA-0基础模型和使用少量数据进行微调的模型与DeePMD模型在预测扩散性质方面的准确性。

Result: MACE-MPA-0基础模型在预测激活能方面取得了与DeePMD模型相当的准确性（分别为0.22 eV和0.24 eV）。通过仅使用300个数据点微调的模型预测激活能为0.20 eV。 MACE模型在仅需少量训练数据的情况下，实现了与使用超过40,000个数据点训练的DeePMD模型相当的性能。

Conclusion: MACE机器学习模型，特别是MACE-MPA-0基础模型，在预测LiF的锂扩散性方面表现出色，其准确性可与DeePMD模型相媲美，并且在训练数据需求方面具有显著优势。该研究为微调方法在机器学习势能中的应用提供了一个有效的测试案例。

Abstract: Machine learning interatomic potentials (MLIPs) are transforming materials
science and engineering by enabling the study of complex phenomena, such as
those critical to battery operation. In this work, we benchmark the MACE
machine learning model against a well-trained DeePMD potential for predicting
interstitial lithium diffusivity in LiF, a key component in the solid
electrolyte interphase in Li ion batteries. Our results demonstrate that the
MACE-MPA-0 foundational model achieves comparable accuracy to well-trained
DeePMD, in predicting key diffusion properties based on molecular dynamics
simulation, while requiring minimal or no training data. For instance, the
MACE-MPA-0 predicts an activation energy Ea of 0.22 eV, the fine-tuned model
with only 300 data points predicts Ea = 0.20 eV, both of which show good
agreement with the DeePMD model reference value of Ea = 0.24 eV. In this work,
we provide a solid test case where fine-tuning approaches - whether using data
generated for DeePMD or data produced by the foundational MACE model itself -
yield similar robust performance to the DeePMD potential trained with over
40,000 actively learned data, albeit requiring only a fraction of the training
data.

</details>


### [567] [Role of chromium oxides and carbides in strengthening CoCrFeNi multi-principle element alloys](https://arxiv.org/abs/2510.05021)
*Artur Olejarz,Wenyi Huo,Anna Kosinska,Maciej Zielinski,Tomasz Stasiak,Marcin Chmielewski,Wojciech Chmurzynski,Max Rae Chu,Michael Patrick Short,Lukasz Kurpaska*

Main category: cond-mat.mtrl-sci

TL;DR: 通过在CoCrFeNi MPEA中形成氧化物和碳化物等铬化合物，可以提高其高温机械性能，为高温应用优化高熵合金生产提供了思路。


<details>
  <summary>Details</summary>
Motivation: 多主元合金（MPEA）具有优异的材料特性，但其复杂的制造工艺限制了其规模化应用。化学复杂性和复杂的制造工艺会导致形成影响最终性能的第二相。本研究旨在提高MPEA的微观结构和高温机械性能。

Method: 在CoCrFeNi MPEA中引入铬化合物（氧化物和碳化物）。研究了不同制备方法（电弧熔炼AM、气体雾化GA、机械合金化MA）对相结构的影响。在室温和575°C下进行机械性能测试，并分析了相演变对性能的影响。

Result: AM样品为单一FCC相；GA样品中检测到氧化铬；MA样品中则根据烧结温度发现了Cr23C6或Cr7C3与Cr2O3的组合。在575°C下，GA样品（含氧化物）的机械性能得到增强，这归因于沉淀强化、再结晶抑制和孪生诱导塑性。MA样品（含碳化物）表现出高强度但低延展性，其中Cr7C3优于Cr23C6。

Conclusion: 铬化合物的演变与MPEA的机械性能密切相关。通过控制相的形成，可以优化高熵合金的生产，以满足高温应用的需求。

Abstract: Multi-principal element alloys (MPEAs) can potentially offer exceptional
material properties, but their complex, costly manufacturing limits their
scalability. Chemical complexity and complex manufacturing processes lead to
the formation of some secondary phases, which have a significant impact on the
final properties. In this work, chromium compound dispersoid enhancements (Cr-
oxides and carbides) were formed in CoCrFeNi MPEAs to enhance their
microstructural and high-temperature mechanical properties. A single FCC phase
was observed in the arc melted (AM) samples, chromium oxides were detected in
the gas-atomized (GA) samples, and Cr2O3 with Cr23C6 or Cr7C3 was found in the
mechanically alloyed (MA)samples depending on the sintering temperature.
Mechanical tests at room temperature and 575{\deg}C, where no phase evolution
is expected, showed that the GA samples with oxides achieved enhanced
mechanical properties at 575{\deg}C. This was co-induced by precipitation
strengthening, recrystallization suppression, and twinning-induced plasticity.
The MA samples with carbides exhibited high strength but low ductility, with
Cr7C3 outperforming Cr23C6 because of its lower hardness and twinning effects.
This work links chromium compound evolution to mechanical performance of MPEAs,
offering insights to optimize HEA production for high-temperature applications
through controlled phase formation.

</details>


<div id='cs.SY'></div>

# cs.SY [[Back]](#toc)

### [568] [Adaptive Cruise Control in Autonomous Vehicles: Challenges, Gaps, Comprehensive Review, and, Future Directions](https://arxiv.org/abs/2510.03300)
*Shradha Bavalatti,Yash Kangralkar,Santosh Pattar,Veena P Badiger*

Main category: cs.SY

TL;DR: 自动驾驶汽车（AVs）通过自适应巡航控制（ACC）技术革新了交通运输，但仍面临挑战。本文旨在弥补现有调查论文的不足，全面分析AVs面临的挑战及解决方案，并为下一代ACC系统的设计提供未来方向，以实现可持续、容错的城市交通。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶汽车（AVs）调查论文缺乏对AVs所面临的挑战及其潜在解决方案的全面分析。本文旨在弥补这一差距，为下一代ACC系统的设计提供指导。

Method: 对现有ACC研究进行细致的识别，分析AVs面临的挑战，并提出应对这些挑战的解决方案和未来发展方向。

Result: 本文对现有ACC研究的局限性进行了详细和系统的审查，并提出了实现可持续和容错的城市交通的创新方法。

Conclusion: 通过对ACC研究的全面分析和对未来方向的提出，本文旨在推动下一代ACC系统的发展，以实现可持续和容错的城市交通。

Abstract: The development of Autonomous Vehicles (AVs) has redefined the way of
transportation by eliminating the need for human intervention in driving. This
revolution is fueled by rapid advancements in adaptive cruise control (ACC),
which make AVs capable of interpreting their surroundings and responding
intelligently. While AVs offer significant advantages, such as enhanced safety
and improved traffic efficiency, they also face several challenges that need to
be addressed. Existing survey papers often lack a comprehensive analysis of
these challenges and their potential solutions. Our paper stands out by
meticulously identifying these gaps in current ACC research and offering
impactful future directions to guide researchers in designing next-generation
ACC systems. Our survey provides a detailed and systematic review, addressing
the limitations of previous studies and proposing innovative approaches to
achieve sustainable and fault-resilient urban transportation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [569] [PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank](https://arxiv.org/abs/2510.03243)
*Yiheng Tao,Yihe Zhang,Matthew T. Dearing,Xin Wang,Yuping Fan,Zhiling Lan*

Main category: cs.LG

TL;DR: PARS通过近似最短作业优先（SJF）调度来提高LLM推理服务的效率，有效减少了延迟和HOL阻塞。


<details>
  <summary>Details</summary>
Motivation: 解决传统LLM调度策略（如FCFS）中存在的头端（HOL）阻塞问题，即长任务延迟短任务。

Method: 提出PARS，一种提示感知LLM任务调度器，通过成对排序和边际排序损失来近似SJF调度，预测基于响应长度的任务排序，并集成到vLLM中。

Result: PARS在多个LLM和真实推理数据集上显著提高了性能，包括推理工作负载，且开销极小。跨模型评估表明其泛化性良好。

Conclusion: PARS是一种有效的LLM任务调度方法，通过近似SJF调度显著提高了服务效率，并能很好地泛化到不同的LLM。

Abstract: Efficient scheduling of LLM inference tasks is essential for achieving low
latency and high throughput, particularly with the growing use of
reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve
(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks
delay shorter ones queued behind them. In this paper, we introduce PARS, a
prompt-aware LLM task scheduler that improves serving efficiency by
approximating shortest-job-first (SJF) scheduling through pairwise ranking with
margin ranking loss. PARS focuses on impactful scheduling decisions and is
seamlessly integrated into the state-of-the-art LLM serving system vLLM. It
effectively predicts response-length-based task ordering, reducing latency with
minimal overhead. Extensive experiments across multiple LLMs and real-world
inference datasets show that PARS significantly improves performance, including
for reasoning workloads. Furthermore, our cross-model evaluations demonstrate
that the design generalizes well, enabling effective scheduling even when
predictors are trained on different LLMs.

</details>


### [570] [Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation](https://arxiv.org/abs/2510.03426)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: GOOMs 是一种扩展的量级表示法，可以处理比传统浮点数更广泛的数值范围，并能在 GPU 等并行硬件上高效运行，解决了数值下溢或溢出的问题。


<details>
  <summary>Details</summary>
Motivation: 在深度学习和金融等领域，对长序列中的实数进行复合计算时，常常会遇到灾难性的数值下溢或溢出问题。

Method: 提出了一种名为广义量级（GOOMs）的量级表示法，它是传统量级表示法的一种原则性扩展，并将浮点数作为特例。同时实现了一个高效的自定义并行前缀扫描算法，以支持在 GPU 等并行硬件上进行原生计算。

Result: GOOMs 的实现能够稳定地在比以往更大的实数动态范围内进行计算。通过三个代表性实验证明：1. 复合实数矩阵乘法，范围远超标准浮点数限制；2. 并行估计李雅普诺夫指数谱，速度比以往方法快几个数量级，并采用了一种新颖的选择性重置方法来防止状态共线性；3. 在深度递归神经网络中捕捉长程依赖关系，即使在具有非对角线递归状态的情况下，也可以通过并行前缀扫描计算，且无需任何形式的稳定化。

Conclusion: GOOMs 的实现与高效的并行扫描相结合，为高动态范围应用提供了一种可扩展且数值鲁棒的替代传统浮点数的方法。

Abstract: Many domains, from deep learning to finance, require compounding real numbers
over long sequences, often leading to catastrophic numerical underflow or
overflow. We introduce generalized orders of magnitude (GOOMs), a principled
extension of traditional orders of magnitude that incorporates floating-point
numbers as a special case, and which in practice enables stable computation
over significantly larger dynamic ranges of real numbers than previously
possible. We implement GOOMs, along with an efficient custom parallel prefix
scan, to support native execution on parallel hardware such as GPUs. We
demonstrate that our implementation of GOOMs outperforms traditional approaches
with three representative experiments, all of which were previously considered
impractical or impossible, and now become possible and practical: (1)
compounding real matrix products far beyond standard floating-point limits; (2)
estimating spectra of Lyapunov exponents in parallel, orders of magnitude
faster than with previous methods, applying a novel selective-resetting method
to prevent state colinearity; and (3) capturing long-range dependencies in deep
recurrent neural networks with non-diagonal recurrent states, computed in
parallel via a prefix scan, without requiring any form of stabilization. Our
results show that our implementation of GOOMs, combined with efficient parallel
scanning, offers a scalable and numerically robust alternative to conventional
floating-point numbers for high-dynamic-range applications.

</details>


### [571] [VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion](https://arxiv.org/abs/2510.03244)
*Yanlong Wang,Hang Yu,Jian Xu,Fei Ma,Hongkang Zhang,Tongtong Feng,Zijian Zhang,Shao-Lun Huang,Danny Dongning Sun,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: VIFO是一个创新的跨模态预测模型，它将多元时间序列转换为图像，利用预训练的大型视觉模型（LVM）提取复杂的跨通道模式，并结合时间序列模态的表示，在冻结LVM大部分参数的情况下，以较低的训练成本实现了优越的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型忽略了跨通道依赖性，而多模态方法未能充分利用大型视觉模型（LVM）来处理时空数据，并且在利用不同模态信息增强时间序列预测方面存在潜力未被挖掘。

Method: VIFO模型将多元时间序列转化为图像，然后利用预训练的大型视觉模型（LVM）提取视觉特征，并将其与时间序列模态的表示对齐和融合。在训练过程中，LVM的绝大部分参数被冻结，只有一小部分（7.45%）进行训练。

Result: VIFO在多个基准测试中取得了具有竞争力的性能，证明了其在捕捉跨变量关系方面的有效性，同时保持了高效的训练。

Conclusion: VIFO模型通过将时间序列视觉化并利用LVM的能力，有效地解决了现有模型在处理跨通道依赖性和融合多模态信息方面的局限性，为时间序列预测提供了一个高效且强大的解决方案。

Abstract: Large time series foundation models often adopt channel-independent
architectures to handle varying data dimensions, but this design ignores
crucial cross-channel dependencies. Concurrently, existing multimodal
approaches have not fully exploited the power of large vision models (LVMs) to
interpret spatiotemporal data. Additionally, there remains significant
unexplored potential in leveraging the advantages of information extraction
from different modalities to enhance time series forecasting performance. To
address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO
uniquely renders multivariate time series into image, enabling pre-trained LVM
to extract complex cross-channel patterns that are invisible to
channel-independent models. These visual features are then aligned and fused
with representations from the time series modality. By freezing the LVM and
training only 7.45% of its parameters, VIFO achieves competitive performance on
multiple benchmarks, offering an efficient and effective solution for capturing
cross-variable relationships in

</details>


### [572] [Sequential decoder training for improved latent space dynamics identification](https://arxiv.org/abs/2510.03535)
*William Anderson,Seung Whan Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: MLasDI是一种数据驱动的降阶模型框架，通过多阶段学习解码器来提高重构和预测精度，在Vlasov方程问题上优于标准LaSDI。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有降阶模型（ROM）在学习过程中为了强制执行潜在动力学而牺牲重构精度的限制，提出了MLasDI框架。

Method: MLasDI通过多阶段学习额外的解码器来逐步修正前一阶段的残差误差，从而提高重构和预测精度。

Result: 在1D-1V Vlasov方程上的实验结果显示，MLasDI在各种架构下均优于标准LaSDI，实现了更低的预测误差和更短的训练时间。

Conclusion: MLasDI框架能够有效提高降阶模型的重构和预测精度，并缩短训练时间，尤其在处理Vlasov方程等问题时效果显著。

Abstract: Accurate numerical solutions of partial differential equations are essential
in many scientific fields but often require computationally expensive solvers,
motivating reduced-order models (ROMs). Latent Space Dynamics Identification
(LaSDI) is a data-driven ROM framework that combines autoencoders with equation
discovery to learn interpretable latent dynamics. However, enforcing latent
dynamics during training can compromise reconstruction accuracy of the model
for simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that
improves reconstruction and prediction accuracy by sequentially learning
additional decoders to correct residual errors from previous stages. Applied to
the 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,
achieving lower prediction errors and reduced training time across a wide range
of architectures.

</details>


### [573] [Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability](https://arxiv.org/abs/2510.03245)
*Ali Yavari,Alireza Mohamadi,Elham Beydaghi,Rainer A. Leitgeb*

Main category: cs.LG

TL;DR: 我们提出了一种名为FAMPE的新型归因方法，通过可迁移的频率感知攻击来提高深度神经网络的可解释性，并在插入分数上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNN）在真实世界噪声和故意扰动下的可靠性是一个重大挑战，现有的归因方法效果不佳，需要改进。

Method: 提出了一种名为“可迁移频率感知攻击”的新型可迁移对抗攻击，并通过一种名为“频率感知模型参数探索器”（FAMPE）的新型归因方法来提高DNN的可解释性。

Result: 与现有的AttEXplore方法相比，FAMPE的插入分数平均提高了13.02%，优于现有方法。通过消融研究，还探讨了高频和低频分量在可解释性中的作用。

Conclusion: FAMPE通过引入频率感知攻击，显著提高了DNN的可解释性，并且在性能上优于现有技术。

Abstract: Ensuring the reliability of deep neural networks (DNNs) in the presence of
real world noise and intentional perturbations remains a significant challenge.
To address this, attribution methods have been proposed, though their efficacy
remains suboptimal and necessitates further refinement. In this paper, we
propose a novel category of transferable adversarial attacks, called
transferable frequency-aware attacks, enabling frequency-aware exploration via
both high-and low-frequency components. Based on this type of attacks, we also
propose a novel attribution method, named Frequency-Aware Model Parameter
Explorer (FAMPE), which improves the explainability for DNNs. Relative to the
current state-of-the-art method AttEXplore, our FAMPE attains an average gain
of 13.02% in Insertion Score, thereby outperforming existing approaches.
Through detailed ablation studies, we also investigate the role of both high-
and low-frequency components in explainability.

</details>


### [574] [LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design](https://arxiv.org/abs/2510.03650)
*Amir Sadikov*

Main category: cs.LG

TL;DR: 利用 LLM 驱动的进化程序合成，在低差异点集和数字序列的准蒙特卡洛 (QMC) 设计问题上取得了新的进展。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决低差异点集和数字序列在 QMC 方法中的两个长期存在的 QMC 设计问题，即构建具有低星差异的有限二维/三维点集，以及选择最小化下游被积函数上随机 QMC 误差的 Sobol' 方向数。

Method: 本文将 QMC 设计问题转化为程序合成问题，并使用 LLM 驱动的进化循环来解决。该方法通过变异和选择代码，并结合特定任务的适应度函数，分为两个阶段：一是构造有限点集，二是优化数字序列。

Result: 在有限点集方面，本文在二维小样本情况下重新发现了已知的最优解，并为 N >= 40 的情况设定了新的二维最佳基准。在三维情况下，本文在 N <= 8 的已证明边界内匹配了大多数已知最优解，并报告了超出该边界的三维改进基准。在数字序列方面，通过进化 Sobol' 参数，在多个 32 维的期权定价任务中，相对于广泛使用的 Joe-Kuo 参数，随机准蒙特卡洛 (rQMC) 均方误差得到了一致降低，同时保持了对任何样本量的可扩展性和与标准随机化的兼容性。

Conclusion: 研究结果表明，LLM 驱动的进化程序合成能够自动化发现高质量的 QMC 结构，在经典设计最优时能够恢复它们，在有限 N 结构重要的场景下能够改进它们。

Abstract: Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo
(QMC) methods for high-dimensional integration. We cast two long-standing QMC
design problems as program synthesis and solve them with an LLM-guided
evolutionary loop that mutates and selects code under task-specific fitness:
(i) constructing finite 2D/3D point sets with low star discrepancy, and (ii)
choosing Sobol' direction numbers that minimize randomized QMC error on
downstream integrands. Our two-phase procedure combines constructive code
proposals with iterative numerical refinement. On finite sets, we rediscover
known optima in small 2D cases and set new best-known 2D benchmarks for N >=
40, while matching most known 3D optima up to the proven frontier (N <= 8) and
reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol'
parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC)
mean-squared error for several 32-dimensional option-pricing tasks relative to
widely used Joe--Kuo parameters, while preserving extensibility to any sample
size and compatibility with standard randomizations. Taken together, the
results demonstrate that LLM-driven evolutionary program synthesis can automate
the discovery of high-quality QMC constructions, recovering classical designs
where they are optimal and improving them where finite-N structure matters.
Data and code are available at
https://github.com/hockeyguy123/openevolve-star-discrepancy.git.

</details>


### [575] [StructPrune: Structured Global Pruning asymptotics with $\mathcal{O}(\sqrt{N})$ GPU Memory](https://arxiv.org/abs/2510.03246)
*Xinyuan Song,Guangji Bai,Liang Zhao*

Main category: cs.LG

TL;DR: STRUPRUNE是一种基于ADMM的框架，通过分而治之的方法实现结构化剪枝，解决了全局剪枝内存开销大和局部剪枝性能不佳的问题，在保持与全局剪枝相当的性能的同时，将内存成本从O(N)降低到O(sqrt(N))。


<details>
  <summary>Details</summary>
Motivation: 为了在保持模型性能的同时，降低大型语言模型（LLMs）剪枝的内存消耗，特别是要实现硬件高效的结构化剪枝，同时又能克服全局剪枝内存需求过大的问题，以及局部剪枝性能不佳的问题。

Method: 提出了一种分而治之的策略，将全局剪枝问题分解为多个模块化的子问题，并设计了一个名为STRUPRUNE的基于ADMM的框架，该框架集成了结构化稀疏性，并推导出了用于结构化剪枝掩码的闭式解析解和用于简化优化的基于能量的渐近框架。

Result: STRUPRUNE实现了与全局结构化剪枝相当的困惑度，同时将内存成本从O(N)降低到O(sqrt(N))，使得在十亿参数规模的模型部署成为可能。

Conclusion: STRUPRUNE成功地结合了局部剪枝的内存效率和结构化剪枝的硬件兼容性，通过分而治之的方法实现了高效的结构化剪枝，解决了大规模模型部署的挑战。

Abstract: Pruning is critical for scaling large language models (LLMs). Global pruning
achieves strong performance but requires $\mathcal{O}(N)$ memory, which is
infeasible for billion-parameter models. Local pruning reduces GPU memory usage
to that of a single layer by pruning layers independently, but it neglects
inter-layer dependencies and often leads to suboptimal performance in
high-sparsity regimes. Unlike unstructured pruning, structured pruning produces
regular sparsity patterns that align well with GPU kernels and library
optimizations, making it more hardware-efficient. However, structured pruning
typically relies on global pruning, since structured patterns are more prone to
severe performance degradation under local optimization. To jointly achieve
structured pruning and the memory efficiency of local pruning, we propose a
divide-and-conquer strategy that decomposes the global pruning problem into
coordinated subproblems across different modules, each of which fits within
limited GPU memory. Building on this idea, we design \textbf{STRUPRUNE}, an
ADMM-based framework that integrates structured sparsity into the pruning
process, combining the memory efficiency of local pruning with the hardware
compatibility of structured methods. We derive a closed-form analytical
solution for structured pruning masks that provides an explicit rule for
layer-wise sparsity allocation, and further develop an energy-based asymptotic
framework yielding a softmax-form allocation scheme that simplifies
optimization while adapting to heterogeneous layer importance. Experiments
demonstrate that STRUPRUNE matches the perplexity of global structured pruning
while reducing memory cost from $\mathcal{O}(N)$ to $\mathcal{O}(\sqrt{N})$,
enabling practical deployment at the billion-parameter scale.

</details>


### [576] [Neural Low-Discrepancy Sequences](https://arxiv.org/abs/2510.03745)
*Michael Etienne Van Huffel,Nathan Kirk,Makram Chahine,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: NeuroLDS是一个基于机器学习的低差异序列生成框架，它通过两阶段学习过程，能够生成比现有方法具有更低差异的序列，并在数值积分、机器人路径规划和科学机器学习等应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的低差异序列（LDS）生成方法依赖于抽象代数和数论，且Message-Passing Monte Carlo（MPMC）方法虽然利用机器学习生成点集，但无法生成LDS。本研究旨在克服这些局限，提出一种新的机器学习方法来生成LDS。

Method: NeuroLDS采用两阶段学习过程：首先，通过监督学习近似经典的LDS构造；然后，通过无监督学习微调，以最小化序列前缀的差异性。

Result: NeuroLDS生成的序列在差异性度量方面显著优于所有先前的方法。此外，NeuroLDS在数值积分、机器人路径规划和科学机器学习等应用中也显示出有效性。

Conclusion: NeuroLDS是第一个基于机器学习的LDS生成框架，它在差异性方面取得了突破性进展，并在多个应用领域证明了其有效性和广泛意义。

Abstract: Low-discrepancy points are designed to efficiently fill the space in a
uniform manner. This uniformity is highly advantageous in many problems in
science and engineering, including in numerical integration, computer vision,
machine perception, computer graphics, machine learning, and simulation.
Whereas most previous low-discrepancy constructions rely on abstract algebra
and number theory, Message-Passing Monte Carlo (MPMC) was recently introduced
to exploit machine learning methods for generating point sets with lower
discrepancy than previously possible. However, MPMC is limited to generating
point sets and cannot be extended to low-discrepancy sequences (LDS), i.e.,
sequences of points in which every prefix has low discrepancy, a property
essential for many applications. To address this limitation, we introduce
Neural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based
framework for generating LDS. Drawing inspiration from classical LDS, we train
a neural network to map indices to points such that the resulting sequences
exhibit minimal discrepancy across all prefixes. To this end, we deploy a
two-stage learning process: supervised approximation of classical constructions
followed by unsupervised fine-tuning to minimize prefix discrepancies. We
demonstrate that $NeuroLDS$ outperforms all previous LDS constructions by a
significant margin with respect to discrepancy measures. Moreover, we
demonstrate the effectiveness of $NeuroLDS$ across diverse applications,
including numerical integration, robot motion planning, and scientific machine
learning. These results highlight the promise and broad significance of Neural
Low-Discrepancy Sequences. Our code can be found at
https://github.com/camail-official/neuro-lds.

</details>


### [577] [Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data](https://arxiv.org/abs/2510.03247)
*Jiancheng Zhang,Yinglun Zhu*

Main category: cs.LG

TL;DR: 该研究提出了首个用于非对齐多模态数据的多模态主动学习框架，通过结合不确定性和多样性原则，在不损失性能的情况下显著降低了多模态标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习算法主要关注单模态数据，忽视了多模态学习中同样存在的标注成本问题。本研究旨在解决现代多模态学习流程（如CLIP和SigLIP）中获取高质量跨模态对齐的昂贵问题。

Method: 提出了一种结合不确定性和多样性原则的新算法，该算法设计具有模式感知能力，支持线性时间获取，并可应用于基于池和基于流的设置。

Result: 在ColorSwap数据集上的实验表明，该方法在不损失准确性的前提下，可将多模态标注成本降低高达40%。

Conclusion: 该研究成功开发了一个多模态主动学习框架，能够有效降低标注成本，同时保持模型性能。

Abstract: Active learning (AL) is a principled strategy to reduce annotation cost in
data-hungry deep learning. However, existing AL algorithms focus almost
exclusively on unimodal data, overlooking the substantial annotation burden in
multimodal learning. We introduce the first framework for multimodal active
learning with unaligned data, where the learner must actively acquire
cross-modal alignments rather than labels on pre-aligned pairs. This setting
captures the practical bottleneck in modern multimodal pipelines such as CLIP
and SigLIP, where unimodal features are easy to obtain but high-quality
alignment is costly. We develop a new algorithm that combines uncertainty and
diversity principles in a modality-aware design, achieves linear-time
acquisition, and applies seamlessly to both pool-based and streaming-based
settings. Extensive experiments on benchmark datasets demonstrate that our
approach consistently reduces multimodal annotation cost while preserving
performance; for instance, on the ColorSwap dataset it cuts annotation
requirements by up to $40\%$ without loss in accuracy.

</details>


### [578] [A Mathematical Explanation of Transformers for Large Language Models and GPTs](https://arxiv.org/abs/2510.03989)
*Xue-Cheng Tai,Hao Liu,Lingfeng Li,Raymond H. Chan*

Main category: cs.LG

TL;DR: Transformer 架构被解释为结构化积分-微分方程的离散化，为理解和设计提供了新的视角。


<details>
  <summary>Details</summary>
Motivation: 现有关于 Transformer 的数学理论不完整，难以解释其结构和操作。

Method: 提出一个连续框架，将 Transformer 解释为结构化积分-微分方程的离散化，其中自注意力机制被视为非局部积分算子，层归一化被视为时间依赖约束投影。

Result: 将 Transformer 的核心组件（注意力、前馈层、归一化）统一在算子理论和变分视角下，并将其嵌入连续域。

Conclusion: 该框架为理解 Transformer 提供了统一、可解释的基础，并为架构设计、分析和控制提供了新的方向，有助于弥合深度学习与连续数学建模之间的差距。

Abstract: The Transformer architecture has revolutionized the field of sequence
modeling and underpins the recent breakthroughs in large language models
(LLMs). However, a comprehensive mathematical theory that explains its
structure and operations remains elusive. In this work, we propose a novel
continuous framework that rigorously interprets the Transformer as a
discretization of a structured integro-differential equation. Within this
formulation, the self-attention mechanism emerges naturally as a non-local
integral operator, and layer normalization is characterized as a projection to
a time-dependent constraint. This operator-theoretic and variational
perspective offers a unified and interpretable foundation for understanding the
architecture's core components, including attention, feedforward layers, and
normalization. Our approach extends beyond previous theoretical analyses by
embedding the entire Transformer operation in continuous domains for both token
indices and feature dimensions. This leads to a principled and flexible
framework that not only deepens theoretical insight but also offers new
directions for architecture design, analysis, and control-based
interpretations. This new interpretation provides a step toward bridging the
gap between deep learning architectures and continuous mathematical modeling,
and contributes a foundational perspective to the ongoing development of
interpretable and theoretically grounded neural network models.

</details>


### [579] [HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model Adaptation for Long-Term Daily Runoff Forecasting](https://arxiv.org/abs/2510.03744)
*Qianfei Fan,Jiayu Wei,Peijun Zhu,Wensheng Ye,Meie Fang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Accurate decade-scale daily runoff forecasting in small watersheds is
difficult because signals blend drifting trends, multi-scale seasonal cycles,
regime shifts, and sparse extremes. Prior deep models (DLinear, TimesNet,
PatchTST, TiDE, Nonstationary Transformer, LSTNet, LSTM) usually target single
facets and under-utilize unlabeled spans, limiting regime adaptivity. We
propose HydroFusion-LMF, a unified framework that (i) performs a learnable
trend-seasonal-residual decomposition to reduce non-stationarity, (ii) routes
residuals through a compact heterogeneous expert set (linear refinement,
frequency kernel, patch Transformer, recurrent memory, dynamically normalized
attention), (iii) fuses expert outputs via a hydrologic context-aware gate
conditioned on day-of-year phase, antecedent precipitation, local variance,
flood indicators, and static basin attributes, and (iv) augments supervision
with a semi-supervised multi-task objective (composite MSE/MAE + extreme
emphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment,
augmentation consistency, variance-filtered pseudo-labeling). Optional adapter
/ LoRA layers inject a frozen foundation time-series encoder efficiently. On a
~10-year daily dataset HydroFusion-LMF attains MSE 1.0128 / MAE 0.5818,
improving the strongest baseline (DLinear) by 10.2% / 10.3% and the mean
baseline by 24.6% / 17.1%. We observe simultaneous MSE and MAE reductions
relative to baselines. The framework balances interpretability (explicit
components, sparse gating) with performance, advancing label-efficient
hydrologic forecasting under non-stationarity.

</details>


### [580] [Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models](https://arxiv.org/abs/2510.03248)
*Anusha Agarwal,Dibakar Roy Sarkar,Somdatta Goswami*

Main category: cs.LG

TL;DR: 神经网络算子（NO）在模拟脑损伤（TBI）方面比传统有限元（FE）模型快得多，计算时间从几小时缩短到几毫秒，其中MG-FNO精度最高，DeepONet推理速度最快，使得实时TBI建模和风险评估成为可能。


<details>
  <summary>Details</summary>
Motivation: 有限元（FE）模型在预测脑损伤（TBI）方面虽然精确但计算成本高，限制了其在临床上的应用。本研究旨在探索神经网络算子（NO）以实现快速、针对特定患者的脑部移位预测，从而在临床和转化研究中实现实时TBI建模。

Method: 将TBI建模为一个算子学习问题，利用四个神经网络算子（FNO, F-FNO, MG-FNO, DeepONet）学习从患者特定的解剖MRI、磁共振弹性成像（MRE）刚度图和人口统计学特征映射到全场三维脑部移位的预测。在249个MRE数据集上进行了训练和评估。

Result: MG-FNO达到了最高的准确性（MSE = 0.0023, 空间保真度为94.3%），并能保留精细结构。F-FNO的收敛速度是标准FNO的两倍。DeepONet的推理速度最快（14.5次迭代/秒），比MG-FNO快7倍，显示出其在嵌入式或边缘计算应用的潜力。所有NO模型都将计算时间从几小时缩短到几毫秒，同时保持了真实的解剖结构。

Conclusion: 神经网络算子（NO）提供了一种高效、分辨率无关的脑部变形预测方法，能够实现实时、针对特定患者的TBI风险评估、临床分诊支持以及防护设备优化，为构建可扩展、按需的临床和群体健康生物力学模型（如人脑数字孪生）开辟了道路。

Abstract: Traumatic brain injury (TBI) remains a major public health concern, with over
69 million cases annually worldwide. Finite element (FE) models offer
high-fidelity predictions of brain deformation but are computationally
expensive, requiring hours per simulation and limiting their clinical utility
for rapid decision-making. This study benchmarks state-of-the-art neural
operator (NO) architectures for rapid, patient-specific prediction of brain
displacement fields, aiming to enable real-time TBI modeling in clinical and
translational settings. We formulated TBI modeling as an operator learning
problem, mapping subject-specific anatomical MRI, magnetic resonance
elastography (MRE) stiffness maps, and demographic features to full-field 3D
brain displacement predictions. Four architectures - Fourier Neural Operator
(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator
Network (DeepONet) were trained and evaluated on 249 MRE datasets across
physiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest
accuracy (MSE = 0.0023, 94.3\% spatial fidelity) and preserved fine-scale
features, while F-FNO converged 2$\times$ faster than standard FNO. DeepONet
offered the fastest inference (14.5 iterations/s) with a 7$\times$
computational speed-up over MG-FNO, suggesting utility for embedded or edge
computing applications. All NOs reduced computation time from hours to
milliseconds without sacrificing anatomical realism. NOs provide an efficient,
resolution-invariant approach for predicting brain deformation, opening the
door to real-time, patient-specific TBI risk assessment, clinical triage
support, and optimization of protective equipment. These results highlight the
potential for NO-based digital twins of the human brain, enabling scalable,
on-demand biomechanical modeling in both clinical and population health
contexts.

</details>


### [581] [Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws](https://arxiv.org/abs/2510.04102)
*Ramzi Dakhmouche,Hossein Gorji*

Main category: cs.LG

TL;DR: Foundation Models (FMs) 在时间序列预测方面表现出色，但仍难以实现外推或长期预测。本研究识别并形式化了一个表征统计学习模型在训练域外进行预测的能力的根本属性，从而解释了深度学习模型在外推设置中性能下降的原因。


<details>
  <summary>Details</summary>
Motivation: FMs在语言模型方面取得了巨大成功，人们对其在时间序列预测领域的应用越来越感兴趣，因为FMs在科学和工程领域具有变革潜力。FMs在短期预测方面取得了显著成功，但外推或长期预测仍然是FMs难以解决的问题，其表现甚至不如简单的基线模型。这与具有强大外推能力的物理定律形成对比，并引发了关于神经网络结构与物理定律之间根本差异的问题。

Method: 识别并形式化了表征统计学习模型在训练域外进行预测的能力的根本属性，并通过理论分析和实证结果来展示该属性对当前深度学习架构的影响。

Result: 提出的属性不仅阐明了外推差距的根本原因，还为设计能够掌握外推能力的下一代预测模型提供了方向。

Conclusion: 本研究阐明了深度学习模型在外推设置中性能下降的根本原因，并为开发更强大的预测模型提供了新的见解和方向。

Abstract: Motivated by the remarkable success of Foundation Models (FMs) in language
modeling, there has been growing interest in developing FMs for time series
prediction, given the transformative power such models hold for science and
engineering. This culminated in significant success of FMs in short-range
forecasting settings. However, extrapolation or long-range forecasting remains
elusive for FMs, which struggle to outperform even simple baselines. This
contrasts with physical laws which have strong extrapolation properties, and
raises the question of the fundamental difference between the structure of
neural networks and physical laws. In this work, we identify and formalize a
fundamental property characterizing the ability of statistical learning models
to predict more accurately outside of their training domain, hence explaining
performance deterioration for deep learning models in extrapolation settings.
In addition to a theoretical analysis, we present empirical results showcasing
the implications of this property on current deep learning architectures. Our
results not only clarify the root causes of the extrapolation gap but also
suggest directions for designing next-generation forecasting models capable of
mastering extrapolation.

</details>


### [582] [High Cycle S-N curve prediction for Al 7075-T6 alloy using Recurrent Neural Networks (RNNs)](https://arxiv.org/abs/2510.03355)
*Aryan Patel*

Main category: cs.LG

TL;DR: 该研究提出了一种基于长短期记忆（LSTM）网络的迁移学习框架，利用纯轴向疲劳数据来预测铝合金的高周疲劳性能，从而节省时间和成本。


<details>
  <summary>Details</summary>
Motivation: 铝合金易发生疲劳失效，但对其进行疲劳性能表征，尤其是高周疲劳数据，成本高昂且耗时。本研究旨在开发一种更经济高效的方法来预测材料的疲劳性能。

Method: 研究采用迁移学习框架，利用长短期记忆（LSTM）网络。首先，在铝合金7075-T6的纯轴向疲劳数据上训练一个源LSTM模型，然后将该模型迁移应用于预测高周扭转S-N曲线。

Result: 该框架能够准确预测铝合金在更高周次范围内的扭转S-N曲线，证明了其有效性。

Conclusion: 该迁移学习框架有望显著降低不同材料疲劳特性表征的成本，并有助于在成本和时间限制内优化测试优先级。

Abstract: Aluminum is a widely used alloy, which is susceptible to fatigue failure.
Characterizing fatigue performance for materials is extremely time and cost
demanding, especially for high cycle data. To help mitigate this, a transfer
learning based framework has been developed using Long short-term memory
networks (LSTMs) in which a source LSTM model is trained based on pure axial
fatigue data for Aluminum 7075-T6 alloy which is then transferred to predict
high cycle torsional S-N curves. The framework was able to accurately predict
Al torsional S-N curves for a much higher cycle range. It is the belief that
this framework will help to drastically mitigate the cost of gathering fatigue
characteristics for different materials and help prioritize tests with better
cost and time constraints.

</details>


### [583] [Light Differentiable Logic Gate Networks](https://arxiv.org/abs/2510.03250)
*Lukas Rüttgers,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.LG

TL;DR: Differentiable logic gate networks (DLGNs) are efficient but suffer from training issues like vanishing gradients and high costs, hindering their scalability. This paper proposes a new parameterization that reduces model size, speeds up training, and improves convergence without sacrificing accuracy, by addressing the root cause in the neuron parameterization itself.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of Differentiable Logic Gate Networks (DLGNs), such as vanishing gradients, discretization errors, and high training costs, which impede their scalability and accuracy, especially with increasing depth.

Method: The paper proposes a reparametrization of logic gate neurons that addresses the root cause of the issues in DLGNs. This new parameterization also logarithmically shrinks the parameter size with the number of inputs per gate.

Result: The proposed reparametrization reduces model size by 4x for binary inputs, speeds up the backward pass by up to 1.86x, and achieves convergence in 8.5x fewer training steps. Accuracy on CIFAR-100 remains stable or even superior compared to the original parameterization.

Conclusion: The proposed reparametrization effectively resolves the training difficulties and efficiency limitations of DLGNs, leading to significant improvements in training speed, convergence, and model size while maintaining or enhancing accuracy.

Abstract: Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency
at inference while sustaining competitive accuracy. But vanishing gradients,
discretization errors, and high training cost impede scaling these networks.
Even with dedicated parameter initialization schemes from subsequent works,
increasing depth still harms accuracy. We show that the root cause of these
issues lies in the underlying parametrization of logic gate neurons themselves.
To overcome this issue, we propose a reparametrization that also shrinks the
parameter size logarithmically in the number of inputs per gate. For binary
inputs, this already reduces the model size by 4x, speeds up the backward pass
by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we
show that the accuracy on CIFAR-100 remains stable and sometimes superior to
the original parametrization.

</details>


### [584] [Can Linear Probes Measure LLM Uncertainty?](https://arxiv.org/abs/2510.04108)
*Ramzi Dakhmouche,Adrien Letellier,Hossein Gorji*

Main category: cs.LG

TL;DR: 本文提出了一种基于贝叶斯统计的有效不确定性量化（UQ）方法，用于改进大型语言模型（LLMs）在多项选择生成任务中的性能，超越了现有的基于最大softmax分数的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在多项选择生成任务中的不确定性量化（UQ）方法，特别是基于最大softmax分数的方法，效果不佳，需要更有效的UQ方法来支持其在自动化决策等领域的可靠部署。

Method: 本文提出了一种基于贝叶斯统计的方法，通过训练多个贝叶斯线性模型来预测每一层相对于前一层的输出。然后，通过识别分布特征的稀疏组合来推断LLM的全局不确定性水平，从而实现高效的UQ。

Result: 通过在多个LLMs上的数值实验，证明了所提出的方法在不确定性量化方面比最先进的基线方法有持续的改进。

Conclusion: 基于贝叶斯统计的方法能够通过简单的线性回归模型，在不确定性量化方面取得比现有方法更好的性能，为LLMs的可靠部署提供了更有效的手段。

Abstract: Effective Uncertainty Quantification (UQ) represents a key aspect for
reliable deployment of Large Language Models (LLMs) in automated
decision-making and beyond. Yet, for LLM generation with multiple choice
structure, the state-of-the-art in UQ is still dominated by the naive baseline
given by the maximum softmax score. To address this shortcoming, we demonstrate
that taking a principled approach via Bayesian statistics leads to improved
performance despite leveraging the simplest possible model, namely linear
regression. More precisely, we propose to train multiple Bayesian linear
models, each predicting the output of a layer given the output of the previous
one. Based on the obtained layer-level posterior distributions, we infer the
global uncertainty level of the LLM by identifying a sparse combination of
distributional features, leading to an efficient UQ scheme. Numerical
experiments on various LLMs show consistent improvement over state-of-the-art
baselines.

</details>


### [585] [Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?](https://arxiv.org/abs/2510.03257)
*Zijian Zhao,Sen Li*

Main category: cs.LG

TL;DR: 该论文提出了一种名为 Triple-BERT 的单代理强化学习方法，用于解决网约车平台大规模订单调度中的挑战，通过动作分解和基于 BERT 的网络来处理庞大的动作和观察空间，并在真实数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 网约车平台在订单调度上面临实时性、车辆匹配以及系统不确定性等复杂挑战。现有的多智能体强化学习（MARL）方法存在无法捕捉全局信息、协作性差或维度灾难等问题。

Method: 提出了一种名为 Triple-BERT 的单代理强化学习（SARL）方法，该方法基于 TD3 变体。通过动作分解策略将联合动作概率分解为单个驾驶员的动作概率，以处理庞大的动作空间。引入了基于 BERT 的网络来处理庞大的观察空间，利用参数共享来缓解参数增长，并使用注意力机制来捕捉驾驶员和订单之间复杂的关系。

Result: 在真实的世界网约车数据集（曼哈顿）上进行验证，Triple-BERT 相较于现有最先进的方法，在服务订单数量上提高了 4.26%，减少了 22.25% 的接载时间，综合性能提升了约 11.95%。

Conclusion: Triple-BERT 是一种有效的单代理强化学习方法，能够成功应对网约车平台大规模订单调度的挑战，并在提高平台效率和用户体验方面取得了显著成果。

Abstract: On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate
real-time challenge of bundling and matching passengers-each with distinct
origins and destinations-to available vehicles, all while navigating
significant system uncertainties. Due to the extensive observation space
arising from the large number of drivers and orders, order dispatching, though
fundamentally a centralized task, is often addressed using Multi-Agent
Reinforcement Learning (MARL). However, independent MARL methods fail to
capture global information and exhibit poor cooperation among workers, while
Centralized Training Decentralized Execution (CTDE) MARL methods suffer from
the curse of dimensionality. To overcome these challenges, we propose
Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method
designed specifically for large-scale order dispatching on ride-sharing
platforms. Built on a variant TD3, our approach addresses the vast action space
through an action decomposition strategy that breaks down the joint action
probability into individual driver action probabilities. To handle the
extensive observation space, we introduce a novel BERT-based network, where
parameter reuse mitigates parameter growth as the number of drivers and orders
increases, and the attention mechanism effectively captures the complex
relationships among the large pool of driver and orders. We validate our method
using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves
approximately an 11.95% improvement over current state-of-the-art methods, with
a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our
code, trained model parameters, and processed data are publicly available at
the repository https://github.com/RS2002/Triple-BERT .

</details>


### [586] [Transductive and Learning-Augmented Online Regression](https://arxiv.org/abs/2510.03917)
*Vinod Raman,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: 当学习者可以访问未来示例的预测时，研究在线回归问题，并在预测精确时实现与最坏情况回归相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现实生活中数据流的可预测性。

Method: 研究了具有未来示例预测的在线回归问题，包括完全可及（归纳在线学习）和不完全可及（带噪声预测）的情况。

Result: 在归纳在线学习中，我们根据fat-shattering维度完全确定了minimax预期懊悔，并证明了其与（对抗性）在线回归的区别。我们开发了一种在线学习算法，其minimax预期懊悔与最坏情况懊悔相匹配，并根据预测质量平滑地提高，在预测精确时表现与归纳在线学习者相似。

Conclusion: 所提出的方法使得以前在可预测示例下不可学的类别能够被学习，这与更广泛的学习增强模型范式一致。

Abstract: Motivated by the predictable nature of real-life in data streams, we study
online regression when the learner has access to predictions about future
examples. In the extreme case, called transductive online learning, the
sequence of examples is revealed to the learner before the game begins. For
this setting, we fully characterize the minimax expected regret in terms of the
fat-shattering dimension, establishing a separation between transductive online
regression and (adversarial) online regression. Then, we generalize this
setting by allowing for noisy or \emph{imperfect} predictions about future
examples. Using our results for the transductive online setting, we develop an
online learner whose minimax expected regret matches the worst-case regret,
improves smoothly with prediction quality, and significantly outperforms the
worst-case regret when future example predictions are precise, achieving
performance similar to the transductive online learner. This enables
learnability for previously unlearnable classes under predictable examples,
aligning with the broader learning-augmented model paradigm.

</details>


### [587] [Numerion: A Multi-Hypercomplex Model for Time Series Forecasting](https://arxiv.org/abs/2510.03251)
*Hanzhong Cao,Wenbo Yan,Ying Tan*

Main category: cs.LG

TL;DR: Numerion是一个基于多重超复数空间的时间序列预测模型，通过将时间序列映射到不同维度的超复数空间，实现自然分解和独立建模，并自适应地融合不同空间中的潜在模式，在多个公开数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列预测方法受计算复杂度和假设鲁棒性限制，而该研究发现时间序列的特征频率在复杂域和高阶超复数空间中自然降低，以此为基础提出新的预测模型。

Method: 提出Numerion模型，将线性层和激活函数推广到任意2的幂次维度的超复数空间，并引入实-超复数-实域多层感知机（RHR-MLP）架构。模型使用多个RHR-MLP将时间序列映射到不同维度的超复数空间进行分解和建模，并通过动态融合机制融合不同空间中的潜在模式。

Result: 实验验证了该模型在多个公开数据集上的性能，取得了最先进的成果。可视化和定量分析证明了多维度RHR-MLP能够自然分解时间序列，并揭示了高维度超复数空间捕获低频特征的趋势。

Conclusion: 多维RHR-MLP能够自然分解时间序列，并且高维超复数空间倾向于捕获低频特征。Numerion模型通过利用这一特性，在时间序列预测任务上取得了优异的表现。

Abstract: Many methods aim to enhance time series forecasting by decomposing the series
through intricate model structures and prior knowledge, yet they are inevitably
limited by computational complexity and the robustness of the assumptions. Our
research uncovers that in the complex domain and higher-order hypercomplex
spaces, the characteristic frequencies of time series naturally decrease.
Leveraging this insight, we propose Numerion, a time series forecasting model
based on multiple hypercomplex spaces. Specifically, grounded in theoretical
support, we generalize linear layers and activation functions to hypercomplex
spaces of arbitrary power-of-two dimensions and introduce a novel
Real-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.
Numerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces
of varying dimensions, naturally decomposing and independently modeling the
series, and adaptively fuses the latent patterns exhibited in different spaces
through a dynamic fusion mechanism. Experiments validate the model`s
performance, achieving state-of-the-art results on multiple public datasets.
Visualizations and quantitative analyses comprehensively demonstrate the
ability of multi-dimensional RHR-MLPs to naturally decompose time series and
reveal the tendency of higher dimensional hypercomplex spaces to capture lower
frequency features.

</details>


### [588] [PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression](https://arxiv.org/abs/2510.04205)
*Di Zhang*

Main category: cs.LG

TL;DR: KANs可以通过PolyKAN进行压缩，实现模型尺寸的减少和逼近误差的可控，并具有多项式时间复杂度。


<details>
  <summary>Details</summary>
Motivation: KANs虽然具有可解释性和扎实的数学基础，但在参数效率方面存在挑战，限制了其在实际部署中的应用。

Method: 提出了一种名为PolyKAN的新型KAN压缩理论框架，将压缩问题转化为最优多面体区域合并问题，建立了KANs的严格多面体表征，并设计了一种最优动态规划算法。

Result: PolyKAN在保持严格误差控制的同时，实现了可证明的最小压缩，并且在所有网络参数上都具有多项式时间复杂度。

Conclusion: PolyKAN为KANs的压缩提供了第一个具有数学保证的正式基础，为可解释的神经网络架构的高效部署开辟了新的方向。

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability
and a strong mathematical foundation. However, their parameter efficiency
remains a significant challenge for practical deployment. This paper introduces
PolyKAN, a novel theoretical framework for KAN compression that provides formal
guarantees on both model size reduction and approximation error. By leveraging
the inherent piecewise polynomial structure of KANs, we formulate the
compression problem as one of optimal polyhedral region merging. We establish a
rigorous polyhedral characterization of KANs, develop a complete theory of
$\epsilon$-equivalent compression, and design an optimal dynamic programming
algorithm that guarantees minimal compression under specified error bounds. Our
theoretical analysis demonstrates that PolyKAN achieves provably minimal
compression while maintaining strict error control, with polynomial-time
complexity in all network parameters. The framework provides the first formal
foundation for KAN compression with mathematical guarantees, opening new
directions for efficient deployment of interpretable neural architectures.

</details>


### [589] [KVComm: Enabling Efficient LLM Communication through Selective KV Sharing](https://arxiv.org/abs/2510.03346)
*Xiangyu Shi,Marco Chiesa,Gerald Q. Maguire Jr.,Dejan Kostic*

Main category: cs.LG

TL;DR: KVComm通过选择性共享KV对，实现高效的LLM间通信，实验证明其性能可与无通信的上界方法媲美，同时显著减少了通信量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM间通信协议（自然语言或隐藏状态）存在效率低下、信息损失或信息集中偏差等问题。

Method: 提出KVComm框架，利用注意力重要性分数和高斯先验，选择性地共享KV对，实现LLM间通信。

Result: KVComm在多任务和模型对的实验中，通信量仅为30%的层KV对，即可达到与无通信上界相当的性能。

Conclusion: KV对可作为LLM间通信的有效媒介，为构建可扩展、高效率的多智能体系统提供了新的方向。

Abstract: Large Language Models (LLMs) are increasingly deployed in multi-agent
systems, where effective inter-model communication is crucial. Existing
communication protocols either rely on natural language, incurring high
inference costs and information loss, or on hidden states, which suffer from
information concentration bias and inefficiency. To address these limitations,
we propose KVComm, a novel communication framework that enables efficient
communication between LLMs through selective sharing of KV pairs. KVComm
leverages the rich information encoded in the KV pairs while avoiding the
pitfalls of hidden states. We introduce a KV layer-wise selection strategy
based on attention importance scores with a Gaussian prior to identify the most
informative KV pairs for communication. Extensive experiments across diverse
tasks and model pairs demonstrate that KVComm achieves comparable performance
to the upper-bound method, which directly merges inputs to one model without
any communication, while transmitting as few as 30\% of layers' KV pairs. Our
study highlights the potential of KV pairs as an effective medium for inter-LLM
communication, paving the way for scalable and efficient multi-agent systems.

</details>


### [590] [Universal Multi-Domain Translation via Diffusion Routers](https://arxiv.org/abs/2510.03252)
*Duc Kieu,Kien Do,Tuan Hoang,Thao Minh Le,Tung Kieu,Dang Nguyen,Thin Nguyen*

Main category: cs.LG

TL;DR: 现有的多领域翻译方法需要完全对齐的数据或只能处理训练时见过的域对。本文提出的通用多领域翻译（UMDT）框架，仅需 K-1 个带中心域的配对数据集即可实现 K 个域之间的任意翻译。该框架的核心是 Diffusion Router (DR)，一个统一的基于扩散的模型，能够通过源域和目标域标签的条件作用，仅用一个噪声预测器来处理所有中心域与非中心域之间的翻译。DR 允许通过中心域进行间接的非中心域翻译。此外，研究还提出了一种新颖的可扩展学习策略，结合变分界目标和高效的 Tweedie 精炼过程，以支持直接的非中心域翻译。在三个大规模 UMDT 基准测试中的评估结果表明，DR 在间接和直接翻译方面均达到了最先进的水平，同时降低了采样成本，并实现了草图到分割等新任务。这证明了 DR 是一个可扩展且通用的通用多领域翻译框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多领域翻译（MDT）中存在局限性，如需要完全对齐的元组或只能处理训练时见过的域对，这限制了其在实际应用中的灵活性和适用范围，并且排除了许多跨域映射的可能性。

Method: 提出了一种名为 Diffusion Router (DR) 的统一扩散模型框架，该框架使用单一噪声预测器，并以源域和目标域标签作为条件，来处理所有中心域与非中心域之间的翻译。DR 能够通过中心域实现间接的非中心域翻译。同时，引入了可扩展学习策略，包括变分界目标和 Tweedie 精炼过程，以支持直接的非中心域翻译。

Result: DR 在三个大规模 UMDT 基准测试中取得了最先进的成果，能够有效地进行间接和直接翻译。此外，DR 降低了采样成本，并成功实现了草图到分割等新颖的跨域翻译任务。

Conclusion: Diffusion Router (DR) 是一个可扩展且通用的多领域翻译框架，它通过仅使用 K-1 个配对数据集和中心域，实现了 K 个域之间的任意翻译，克服了现有方法的局限性。

Abstract: Multi-domain translation (MDT) aims to learn translations between multiple
domains, yet existing approaches either require fully aligned tuples or can
only handle domain pairs seen in training, limiting their practicality and
excluding many cross-domain mappings. We introduce universal MDT (UMDT), a
generalization of MDT that seeks to translate between any pair of $K$ domains
using only $K-1$ paired datasets with a central domain. To tackle this problem,
we propose Diffusion Router (DR), a unified diffusion-based framework that
models all central$\leftrightarrow$non-central translations with a single noise
predictor conditioned on the source and target domain labels. DR enables
indirect non-central translations by routing through the central domain. We
further introduce a novel scalable learning strategy with a variational-bound
objective and an efficient Tweedie refinement procedure to support direct
non-central mappings. Through evaluation on three large-scale UMDT benchmarks,
DR achieves state-of-the-art results for both indirect and direct translations,
while lowering sampling cost and unlocking novel tasks such as
sketch$\leftrightarrow$segmentation. These results establish DR as a scalable
and versatile framework for universal translation across multiple domains.

</details>


### [591] [Power Transform Revisited: Numerically Stable, and Federated](https://arxiv.org/abs/2510.04995)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 直接实现幂变换存在数值不稳定的问题，本文提出了改进方法并将其应用于联邦学习。


<details>
  <summary>Details</summary>
Motivation: 幂变换作为一种流行的数据预处理技术，在统计分析和机器学习中广泛使用。然而，直接实现幂变换存在严重的数值不稳定的问题，可能导致结果不正确甚至程序崩溃。

Method: 对幂变换数值不稳定的根源进行了全面分析，并提出了有效的补救措施。此外，将幂变换扩展到联邦学习场景，解决了该场景下的数值和分布挑战。

Result: 实验表明，所提出的方法在真实数据集上是有效且鲁棒的，相比现有方法显著提高了稳定性。

Conclusion: 所提出的改进幂变换方法能够有效解决数值不稳定的问题，并能成功应用于联邦学习场景，提高模型的稳定性和有效性。

Abstract: Power transforms are popular parametric techniques for making data more
Gaussian-like, and are widely used as preprocessing steps in statistical
analysis and machine learning. However, we find that direct implementations of
power transforms suffer from severe numerical instabilities, which can lead to
incorrect results or even crashes. In this paper, we provide a comprehensive
analysis of the sources of these instabilities and propose effective remedies.
We further extend power transforms to the federated learning setting,
addressing both numerical and distributional challenges that arise in this
context. Experiments on real-world datasets demonstrate that our methods are
both effective and robust, substantially improving stability compared to
existing approaches.

</details>


### [592] [MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment](https://arxiv.org/abs/2510.03283)
*Yufei Li,Yu Fu,Yue Dong,Cong Liu*

Main category: cs.LG

TL;DR: MACE通过智能内存管理和迭代级调度，在边缘设备上实现了LLM的并发推理和微调，显著降低了推理延迟并保持了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 边缘LLM在低延迟应用中面临推理延迟和模型准确性之间的矛盾，现有策略无法有效解决。

Method: 提出MACE系统，通过迭代级调度和智能内存管理，实现推理（预填充、解码）和微调的共置，并根据模型更新的重要性分配计算资源。

Result: MACE在性能上媲美或超越持续重训，推理延迟降低高达63%，资源占用率高（NVIDIA AGX Orin上GPU利用率>85%）。

Conclusion: 迭代级的混合调度是实现边缘LLM持续学习能力的有效途径。

Abstract: Large language models (LLMs) deployed on edge servers are increasingly used
in latency-sensitive applications such as personalized assistants,
recommendation, and content moderation. However, the non-stationary nature of
user data necessitates frequent retraining, which introduces a fundamental
tension between inference latency and model accuracy under constrained GPU
resources. Existing retraining strategies either delay model updates,
over-commit resources to retraining, or overlook iteration-level retraining
granularity. In this paper, we identify that iteration-level scheduling is
crucial for adapting retraining frequency to model drift without violating
service-level objectives (SLOs). We propose MACE, a hybrid LLM system that
colocates concurrent inference (prefill, decode) and fine-tuning, with
intelligent memory management to maximize task performance while promising
inference throughput. MACE leverages the insight that not all model updates
equally affect output alignment and allocates GPU cycles accordingly to balance
throughput, latency, and update freshness. Our trace-driven evaluation shows
that MACE matches or exceeds continuous retraining while reducing inference
latency by up to 63% and maintaining throughput under resource constraints.
Compared to periodic retraining, MACE improves latency breakdown across
prefill, decode, and finetune stages, and sustains GPU utilization above 85% in
NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid
scheduling is a promising direction for deploying LLMs with continual learning
capabilities on edge platforms.

</details>


### [593] [Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents](https://arxiv.org/abs/2510.03253)
*Heyang Gao,Zexu Sun,Erxue Min,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen*

Main category: cs.LG

TL;DR: HPL通过利用多粒度偏好信号来优化LLM代理，解决了现有DPO方法在信用分配上的粒度不匹配问题，并在三个具有挑战性的代理基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于偏好的离线方法（如DPO）在对LLM代理进行校准时，存在粒度不匹配的问题，导致在信用分配上过于粗略或短视。

Method: HPL框架首先将专家轨迹分解为语义上连贯的动作组，并生成对比性的次优组，从而在细粒度的子任务级别实现偏好学习。然后，采用一种双层课程学习机制，根据组的长度和样本的难易程度，从简单到复杂地组织学习过程。

Result: HPL在三个具有挑战性的代理基准测试中，相比于现有的最先进方法取得了更好的性能。分析表明，HPL的层次化DPO损失能有效整合多粒度的偏好信号，而双层课程学习机制对于使代理能够解决从简单行为到复杂多步序列的各种任务至关重要。

Conclusion: HPL框架通过结合轨迹级和组级偏好优化，并采用双层课程学习，能够有效地解决LLM代理在信用分配上的粒度不匹配问题，并在各种任务中取得优异表现。

Abstract: Large Language Models (LLMs) as autonomous agents are increasingly tasked
with solving complex, long-horizon problems. Aligning these agents via
preference-based offline methods like Direct Preference Optimization (DPO) is a
promising direction, yet it faces a critical granularity mismatch.
Trajectory-level DPO provides a signal that is too coarse for precise credit
assignment, while step-level DPO is often too myopic to capture the value of
multi-step behaviors. To resolve this challenge, we introduce Hierarchical
Preference Learning (HPL), a hierarchical framework that optimizes LLM agents
by leveraging preference signals at multiple, synergistic granularities. While
HPL incorporates trajectory- and step-level DPO for global and local policy
stability, its core innovation lies in group-level preference optimization
guided by a dual-layer curriculum. Our approach first decomposes expert
trajectories into semantically coherent action groups and then generates
contrasting suboptimal groups to enable preference learning at a fine-grained,
sub-task level. Then, instead of treating all preference pairs equally, HPL
introduces a curriculum scheduler that organizes the learning process from
simple to complex. This curriculum is structured along two axes: the group
length, representing sub-task complexity, and the sample difficulty, defined by
the reward gap between preferred and dispreferred action groups. Experiments on
three challenging agent benchmarks show that HPL outperforms existing
state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO
loss effectively integrates preference signals across multiple granularities,
while the dual-layer curriculum is crucial for enabling the agent to solve a
wide range of tasks, from simple behaviors to complex multi-step sequences.

</details>


### [594] [LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain](https://arxiv.org/abs/2510.03288)
*Chiming Duan,Minghua He,Pei Xiao,Tong Jia,Xin Zhang,Zhewei Zhong,Xiang Luo,Yan Niu,Lingzhe Zhang,Yifan Wu,Siyu Yu,Weijie Hong,Ying Li,Gang Huang*

Main category: cs.LG

TL;DR: LogAction是一种基于主动域自适应的新型日志异常检测模型，通过结合迁移学习和主动学习，使用少量人工标注数据（仅2%）在六个不同数据集组合上实现了平均93.01%的F1分数，优于现有方法26.28%。


<details>
  <summary>Details</summary>
Motivation: 现有的基于日志的异常检测方法在很大程度上依赖于标签，但大规模日志的标注具有挑战性；基于迁移学习和主动学习的方法存在数据分布差异和冷启动问题。

Method: LogAction模型结合了迁移学习和主动学习。它使用来自成熟系统的标记数据来训练基础模型，以减轻主动学习中的冷启动问题。此外，LogAction利用自由能采样和基于不确定性的采样来选择位于分布边界的日志进行手动标注，以最少的人工标注努力来解决迁移学习中的数据分布差距问题。

Result: 在六个不同的数据集组合上进行实验，LogAction在仅2%的人工标签下达到了平均93.01%的F1分数，在性能上优于一些最先进的方法26.28%。

Conclusion: LogAction通过整合迁移学习和主动学习，并采用创新的采样策略，能够有效地解决日志异常检测中的数据分布差距和冷启动问题，仅需少量人工标注即可取得优于现有方法的性能。

Abstract: Log-based anomaly detection is a essential task for ensuring the reliability
and performance of software systems. However, the performance of existing
anomaly detection methods heavily relies on labeling, while labeling a large
volume of logs is highly challenging. To address this issue, many approaches
based on transfer learning and active learning have been proposed.
Nevertheless, their effectiveness is hindered by issues such as the gap between
source and target system data distributions and cold-start problems. In this
paper, we propose LogAction, a novel log-based anomaly detection model based on
active domain adaptation. LogAction integrates transfer learning and active
learning techniques. On one hand, it uses labeled data from a mature system to
train a base model, mitigating the cold-start issue in active learning. On the
other hand, LogAction utilize free energy-based sampling and uncertainty-based
sampling to select logs located at the distribution boundaries for manual
labeling, thus addresses the data distribution gap in transfer learning with
minimal human labeling efforts. Experimental results on six different
combinations of datasets demonstrate that LogAction achieves an average 93.01%
F1 score with only 2% of manual labels, outperforming some state-of-the-art
methods by 26.28%. Website: https://logaction.github.io

</details>


### [595] [The Argument is the Explanation: Structured Argumentation for Trust in Agents](https://arxiv.org/abs/2510.03442)
*Ege Cakar,Per Ola Kristensson*

Main category: cs.LG

TL;DR: AI explainability should prioritize verifiable reasoning chains over mechanistic transparency, using structured argumentation to achieve this. The proposed pipeline integrates LLMs with argument graphs for verification, achieving state-of-the-art results on AAEC and Argumentative MicroTexts relation classification. It also demonstrates applications in multi-agent risk assessment and includes features for hallucination detection and iterative refinement.


<details>
  <summary>Details</summary>
Motivation: The current limitations of AI explainability, which often lack verifiable reasoning chains, necessitate a new approach. The paper argues that structured argumentation can provide a higher level of explanation and verification compared to traditional interpretability methods or raw LLM outputs.

Method: The paper proposes a pipeline that converts LLM text into argument graphs using structured argumentation. This allows for verification at each inferential step. For hallucination detection, Bipolar Assumption-Based Argumentation is used to capture support/attack relationships, enabling fact nodes to attack arguments. A verification mechanism for iterative refinement without retraining is also included. The approach is demonstrated on multi-agent risk assessment using the Structured What-If Technique.

Result: The pipeline achieved state-of-the-art performance with 94.44 macro F1 on the AAEC dataset (5.7 points above prior work) and 0.81 macro F1 on Argumentative MicroTexts relation classification (approximately 0.07 above previous results).

Conclusion: Structured argumentation offers a superior method for AI explainability by providing verifiable reasoning chains. The proposed pipeline effectively integrates LLMs with argument graphs, achieving top performance and demonstrating practical applications in risk assessment with enhanced transparency and verification capabilities.

Abstract: Humans are black boxes -- we cannot observe their neural processes, yet
society functions by evaluating verifiable arguments. AI explainability should
follow this principle: stakeholders need verifiable reasoning chains, not
mechanistic transparency. We propose using structured argumentation to provide
a level of explanation and verification neither interpretability nor
LLM-generated explanation is able to offer. Our pipeline achieves
state-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7
points above prior work) and $0.81$ macro F1, $\sim$0.07 above previous
published results with comparable data setups, for Argumentative MicroTexts
relation classification, converting LLM text into argument graphs and enabling
verification at each inferential step. We demonstrate this idea on multi-agent
risk assessment using the Structured What-If Technique, where specialized
agents collaborate transparently to carry out risk assessment otherwise
achieved by humans alone. Using Bipolar Assumption-Based Argumentation, we
capture support/attack relationships, thereby enabling automatic hallucination
detection via fact nodes attacking arguments. We also provide a verification
mechanism that enables iterative refinement through test-time feedback without
retraining. For easy deployment, we provide a Docker container for the
fine-tuned AMT model, and the rest of the code with the Bipolar ABA Python
package on GitHub.

</details>


### [596] [Adversarial training with restricted data manipulation](https://arxiv.org/abs/2510.03254)
*David Benfield,Stefano Coniglio,Phan Tu Vuong,Alain Zemkoho*

Main category: cs.LG

TL;DR: 通过引入约束条件，改进了对抗性机器学习中的悲观二阶规划模型，提高了分类器的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的悲观二阶规划模型在训练对抗性机器学习分类器时，由于其不受限制的对手模型可能导致过于悲观和不切实际的解决方案，从而影响在真实世界数据上的分类器性能。

Method: 提出并实现了一种约束悲观二阶规划模型，通过限制对手的调整范围，使其行为更符合现实。

Result: 实验证明，所提出的约束模型在平均表现上优于现有的不受限制的悲观二阶规划方法。

Conclusion: 约束悲观二阶规划模型能够更好地平衡分类器鲁棒性和对现实情况的拟合，在对抗性机器学习任务中取得更好的性能。

Abstract: Adversarial machine learning concerns situations in which learners face
attacks from active adversaries. Such scenarios arise in applications such as
spam email filtering, malware detection and fake image generation, where
security methods must be actively updated to keep up with the everimproving
generation of malicious data. Pessimistic Bilevel optimisation has been shown
to be an effective method of training resilient classifiers against such
adversaries. By modelling these scenarios as a game between the learner and the
adversary, we anticipate how the adversary will modify their data and then
train a resilient classifier accordingly. However, since existing pessimistic
bilevel approaches feature an unrestricted adversary, the model is vulnerable
to becoming overly pessimistic and unrealistic. When finding the optimal
solution that defeats the classifier, it is possible that the adversary's data
becomes nonsensical and loses its intended nature. Such an adversary will not
properly reflect reality, and consequently, will lead to poor classifier
performance when implemented on real-world data. By constructing a constrained
pessimistic bilevel optimisation model, we restrict the adversary's movements
and identify a solution that better reflects reality. We demonstrate through
experiments that this model performs, on average, better than the existing
approach.

</details>


### [597] [From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing](https://arxiv.org/abs/2510.03293)
*Rana Shahout,Colin Cai,Yilun Du,Minlan Yu,Michael Mitzenmacher*

Main category: cs.LG

TL;DR: LASER是一种用于Mixture-of-Experts（MoE）模型的即插即用推理时路由算法，通过动态调整路由策略来平衡专家负载，提高推理效率，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE模型在推理时存在负载不均衡问题，部分专家过载而其他专家利用率不足，导致系统性能下降（延迟、吞吐量、成本）。

Method: LASER算法在推理时根据门控函数的分数分布自适应地调整路由策略。当分数分布清晰时，路由到最强的专家；当分数分布均匀时，路由到负载最轻的专家。该算法仅依赖于训练好的模型的门控分数，无需重新训练或微调。

Result: 在Mixtral-8x7B和DeepSeek-MoE-16b-chat模型上，使用LASER算法在ARC-Easy、ARC-Challenge、MMLU和GSM8K四个数据集上进行了评估。结果显示，LASER显著改善了负载均衡，降低了延迟，提高了吞吐量，同时对模型精度的影响可忽略不计。

Conclusion: LASER作为一种即插即用的推理时路由算法，能够有效解决MoE模型的推理负载不均衡问题，提升系统性能，且不损害模型精度，为MoE模型的实际应用提供了有价值的解决方案。

Abstract: Mixture-of-Experts (MoE) models can scale parameter capacity by routing each
token to a subset of experts through a learned gate function. While conditional
routing reduces training costs, it shifts the burden on inference memory:
expert parameters and activations consume memory, limiting the number of
experts per device. As tokens are routed, some experts become overloaded while
others are underutilized. Because experts are mapped to GPUs, this imbalance
translates directly into degraded system performance in terms of latency,
throughput, and cost. We present LASER, a plug-and-play, inference-time routing
algorithm that balances load while preserving accuracy. LASER adapts to the
shape of the gate's score distribution. When scores provide a clear preference,
it routes to the strongest experts; when scores are more uniform, it broadens
the set of viable experts and routes to the least-loaded among them. Because
LASER relies only on gate scores from a trained model, it integrates directly
into existing MoE inference pipelines without retraining or finetuning. We
evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets
(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,
translating into lower latency and higher throughput, while keeping the
accuracy changes negligible.

</details>


### [598] [SciTS: Scientific Time Series Understanding and Generation with LLMs](https://arxiv.org/abs/2510.03255)
*Wen Wu,Ziyang Zhang,Liwei Liu,Xuenan Xu,Junlin Liu,Ke Fan,Qitan Lv,Jimin Zhuang,Chen Zhang,Zheqi Yuan,Siyuan Hou,Tianyi Lin,Kai Chen,Bowen Zhou,Chao Zhang*

Main category: cs.LG

TL;DR: SciTS是一个包含12个科学领域和43个任务的基准测试，共包含50k+个实例，涵盖了单变量和多变量时间序列数据。研究基准测试了17个模型，并引入了TimeOmni框架，以增强LLM在理解和生成时间序列数据方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型（LLM）在处理科学时间序列数据时存在不足，要么将数值序列编码为文本，要么将其转换为图像，这两种方法都无法全面理解和生成科学时间序列。现有的统一时间序列模型通常只专注于预测或分析，并且在非周期性、异构的科学信号上的有效性尚不明确。

Method: 构建了一个名为SciTS的基准测试，该测试包含12个科学领域和43个任务，收集了超过50k个实例，包括单变量和多变量时间序列信号。对17种模型进行了基准测试，包括纯文本LLM、多模态LLM和统一时间序列模型。在此基础上，引入了一个名为TimeOmni的框架，用于增强LLM理解和生成时间序列的能力。

Result: 通用LLM比专门的时间序列模型表现出更强的泛化能力。将时间序列表示为文本或图像会因序列过长和数值精度损失而限制性能。TimeOmni框架能够让LLM在保持与通用LLM训练兼容的同时，具备理解和生成时间序列的能力。

Conclusion: 这项工作填补了科学时间序列专用基准测试和建模框架的空白，为LLM理解和生成复杂的时间科学数据铺平了道路。

Abstract: The scientific reasoning ability of large language models (LLMs) has recently
attracted significant attention. Time series, as a fundamental modality in
scientific data, presents unique challenges that are often overlooked in
current multimodal LLMs, which either encode numerical sequences as text or
convert them into images. Such approaches may be insufficient for comprehensive
scientific time series understanding and generation. Existing unified time
series models typically specialise in either forecasting or analysis, and their
effectiveness on non-periodic, heterogeneous scientific signals remains
unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12
scientific domains and 43 tasks, with over 50k+ instances, both univariate and
multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz
in frequency. We benchmark 17 models, including text-only LLMs, multimodal
LLMs, and unified time series models, and find that general-purpose LLMs
exhibit stronger generalisability than specialised time series models, while
representing time series as text or images limits their performance due to
excessively long sequences and loss of numerical precision, respectively. We
then introduce TimeOmni, a framework that equips LLMs with the ability to
understand and generate time series while remaining compatible with
general-purpose LLM training. This work fills a gap in both dedicated
benchmarks and modelling frameworks for scientific time series, paving the way
for LLMs to understand and generate complex temporal scientific data.

</details>


### [599] [CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models](https://arxiv.org/abs/2510.03298)
*Dongqi Zheng,Wenjin Fu*

Main category: cs.LG

TL;DR: CAFL-L是FedAvg的一种扩展，通过拉格朗日对偶优化来处理设备资源限制，如能量、通信、内存和热量。它通过动态调整训练超参数（如冻结深度、本地步骤、批大小和通信压缩）来满足这些限制，并使用梯度累积来保持训练稳定性。实验表明，CAFL-L在内存和通信方面表现优于FedAvg，同时保持了具有竞争力的验证性能，适用于资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 为了在实际部署中，特别是在资源受限的边缘设备上，有效地应用联邦学习，需要明确考虑并满足设备级别的资源约束（如能量、通信、内存和热量）。

Method: CAFL-L采用拉格朗日对偶优化来动态调整训练超参数（冻结深度、本地步骤、批大小和通信压缩），并通过梯度累积实现令牌预算的保留，以维持训练稳定性。

Result: CAFL-L在字符级语言模型上的实验表明，与标准的FedAvg相比，它在满足约束方面表现更优，内存使用减少了20%，通信量减少了95%，同时保持了具有竞争力的验证性能。

Conclusion: CAFL-L是一种实用的联邦学习方法，能够有效地在资源受限的边缘设备上进行部署，因为它能够满足设备级别的资源约束，同时保持模型的性能。

Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual
Optimization (CAFL-L), a principled extension of FedAvg that explicitly
incorporates device-level resource constraints including energy, communication,
memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to
dynamically adapt training hyperparameters -- freezing depth, local steps,
batch size, and communication compression -- while preserving training
stability through token-budget preservation via gradient accumulation.
Experiments on a character-level language model demonstrate that CAFL-L
achieves superior constraint satisfaction compared to standard FedAvg (reducing
memory usage by 20% and communication by 95%) while maintaining competitive
validation performance, making it practical for deployment on
resource-constrained edge devices.

</details>


### [600] [Semantic-Aware Scheduling for GPU Clusters with Large Language Models](https://arxiv.org/abs/2510.03334)
*Zerui Wang,Qinghao Hu,Ana Klimovic,Tianwei Zhang,Yonggang Wen,Peng Sun,Dahua Lin*

Main category: cs.LG

TL;DR: SchedMate通过利用LLM从源代码、运行时日志和历史作业中提取语义信息，增强了深度学习调度程序的性能，将平均作业完成时间缩短了1.91倍。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习调度程序在资源分配方面存在局限性，因为它们缺乏对所管理作业的语义的理解，这导致了配置开销高、持续时间估计不可靠、故障处理不足以及可观察性差。

Method: SchedMate是一个框架，通过三个基于LLM的组件，从源代码、运行时日志和历史作业中提取见解，以弥合语义鸿沟，并可选择地增强现有调度程序。

Result: SchedMate可将平均作业完成时间缩短高达1.91倍，并能显著提高调度性能。

Conclusion: 通过利用非结构化数据源中的语义信息，SchedMate证明了语义感知在现代深度学习调度中的关键作用。

Abstract: Deep learning (DL) schedulers are pivotal in optimizing resource allocation
in GPU clusters, but operate with a critical limitation: they are largely blind
to the semantic context of the jobs they manage. This forces them to rely on
limited metadata, leading to high profiling overhead, unreliable duration
estimation, inadequate failure handling, and poor observability. To this end,
we propose SchedMate, a framework that bridges this semantic gap by
systematically extracting deep insights from overlooked, unstructured data
sources: source code, runtime logs, and historical jobs. SchedMate enhances
existing schedulers non-intrusively through three LLM-based components. Our
implementation integrates seamlessly with existing deep learning schedulers.
Evaluations on a 128-GPU physical cluster and extensive simulations on
production traces show SchedMate reduces average job completion times by up to
1.91x, substantially enhancing the scheduling performance, demonstrating the
critical role of semantic-awareness in modern DL scheduling.

</details>


### [601] [Deep Reinforcement Learning for Multi-Agent Coordination](https://arxiv.org/abs/2510.03592)
*Kehinde O. Aina,Sehoon Ha*

Main category: cs.LG

TL;DR: 本研究提出了一种受昆虫群启发的、基于虚拟信息素的多智能体深度强化学习框架（S-MADRL），用于在通信受限的拥挤环境中实现多机器人协调，解决了现有算法的收敛性和可扩展性问题，并通过课程学习提高了效率。


<details>
  <summary>Details</summary>
Motivation: 在狭窄和拥挤的环境中，机器人之间的拥堵和干扰严重影响了集体任务的执行。现有协调算法在收敛性和可扩展性方面存在局限性。

Method: 提出了一种基于虚拟信息素的多智能体深度强化学习框架（S-MADRL），利用课程学习将复杂任务分解为更易于管理的子问题，以实现去中心化的涌现协调。

Result: 实验结果表明，该框架能有效协调最多八个机器人，机器人能够自组织形成不对称的负载分配，从而减少拥堵并优化团队整体表现。

Conclusion: S-MADRL框架在通信受限的拥挤环境中实现了可扩展的去中心化多智能体协调，展示了类似自然界中的涌现行为，有效解决了多机器人协调的挑战。

Abstract: We address the challenge of coordinating multiple robots in narrow and
confined environments, where congestion and interference often hinder
collective task performance. Drawing inspiration from insect colonies, which
achieve robust coordination through stigmergy -- modifying and interpreting
environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement
Learning (S-MADRL) framework that leverages virtual pheromones to model local
and social interactions, enabling decentralized emergent coordination without
explicit communication. To overcome the convergence and scalability limitations
of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum
learning, which decomposes complex tasks into progressively harder
sub-problems. Simulation results show that our framework achieves the most
effective coordination of up to eight agents, where robots self-organize into
asymmetric workload distributions that reduce congestion and modulate group
performance. This emergent behavior, analogous to strategies observed in
nature, demonstrates a scalable solution for decentralized multi-agent
coordination in crowded environments with communication constraints.

</details>


### [602] [POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation](https://arxiv.org/abs/2510.03258)
*Chang'an Yi,Xiaohui Deng,Shuaicheng Niu,Yan Zhou*

Main category: cs.LG

TL;DR: POEM是一种新的测试时自适应（TTA）方法，通过探索以前未被充分利用的可靠样本来提高TTA的性能，并引入一个自适应分支网络来平衡领域无关表示和目标数据的高性能。实验证明，POEM在各种挑战性场景和实际领域偏移中优于现有TTA方法，同时计算效率高。


<details>
  <summary>Details</summary>
Motivation: 现有的TTA方法依赖于熵作为置信度度量，但对预定义的熵阈值敏感，可能导致遗漏和未充分利用可靠的目标样本。

Method: 提出了一种名为POEM的通用方法，该方法通过探索以前未被充分利用的可靠样本来促进TTA。此外，引入了一个额外的自适应分支网络，以平衡提取领域无关表示和在目标数据上实现高性能。

Result: POEM在多种架构的综合实验中，在挑战性场景和实际领域偏移中始终优于现有TTA方法，同时计算效率高。

Conclusion: POEM通过利用可靠样本和引入自适应分支网络，有效提高了TTA的性能，并且其核心思想可以作为一种增强策略来提升现有TTA方法的性能。

Abstract: Test-time adaptation (TTA) aims to transfer knowledge from a source model to
unknown test data with potential distribution shifts in an online manner. Many
existing TTA methods rely on entropy as a confidence metric to optimize the
model. However, these approaches are sensitive to the predefined entropy
threshold, influencing which samples are chosen for model adaptation.
Consequently, potentially reliable target samples are often overlooked and
underutilized. For instance, a sample's entropy might slightly exceed the
threshold initially, but fall below it after the model is updated. Such samples
can provide stable supervised information and offer a normal range of gradients
to guide model adaptation. In this paper, we propose a general approach,
\underline{POEM}, to promote TTA via ex\underline{\textbf{p}}loring the
previously unexpl\underline{\textbf{o}}red reliabl\underline{\textbf{e}}
sa\underline{\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch
network to strike a balance between extracting domain-agnostic representations
and achieving high performance on target data. Comprehensive experiments across
multiple architectures demonstrate that POEM consistently outperforms existing
TTA methods in both challenging scenarios and real-world domain shifts, while
remaining computationally efficient. The effectiveness of POEM is evaluated
through extensive analyses and thorough ablation studies. Moreover, the core
idea behind POEM can be employed as an augmentation strategy to boost the
performance of existing TTA approaches. The source code is publicly available
at \emph{https://github.com/ycarobot/POEM}

</details>


### [603] [Distributed Low-Communication Training with Decoupled Momentum Optimization](https://arxiv.org/abs/2510.03371)
*Sasho Nedelkoski,Alexander Acker,Odej Kao,Soeren Becker,Dominik Scheinert*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The training of large models demands substantial computational resources,
typically available only in data centers with high-bandwidth interconnects.
However, reducing the reliance on high-bandwidth interconnects between nodes
enables the use of distributed compute resources as an alternative to
centralized data center training. Building on recent advances in distributed
model training, we propose an approach that further reduces communication by
combining infrequent synchronizations across distributed model replicas with
gradient momentum compression. In particular, we treat the optimizer momentum
as a signal and decompose the Nesterov momentum into high- and low-frequency
components via the discrete cosine transform (DCT). Only the high-frequency
components are synchronized across model replicas every $H$ steps. Empirically,
our method achieves up to a $16\times$ reduction in communication compared to
the baseline DiLoCo, and it generalizes across architectures, including
transformer-based language models and convolutional neural networks for images.
Overall, this work advances the feasibility of training large models on
distributed nodes with low-bandwidth interconnects.

</details>


### [604] [Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.03823)
*Adam Haroon,Tristan Schuler*

Main category: cs.LG

TL;DR: 本文首次将多智能体强化学习（MARL）应用于高空气球（HABs）分布式区域覆盖协调，扩展了RLHAB环境以支持多智能体协同学习，并采用QMIX算法处理大气载具协调挑战。


<details>
  <summary>Details</summary>
Motivation: 现有HABs协调方法在小型团队和本地任务中表现不佳，而多智能体强化学习在HABs协调方面尚未被探索。

Method: 扩展RLHAB仿真环境以支持多智能体协同学习，并采用QMIX算法，利用集中训练去中心化执行（CTDE）策略，设计了包含个体状态、环境背景和队友数据的专门观测空间，以及以覆盖为优先、鼓励空间分布为目标的层级奖励。

Result: QMIX算法在分布式区域覆盖任务中的表现与理论最优的几何确定性方法相当，验证了MARL方法的有效性。

Conclusion: MARL方法，特别是QMIX算法，为HABs分布式区域覆盖协调提供了一种有效途径，并为未来更复杂的自主多HABs任务奠定了基础。

Abstract: High Altitude Balloons (HABs) can leverage stratospheric wind layers for
limited horizontal control, enabling applications in reconnaissance,
environmental monitoring, and communications networks. Existing multi-agent HAB
coordination approaches use deterministic methods like Voronoi partitioning and
extremum seeking control for large global constellations, which perform poorly
for smaller teams and localized missions. While single-agent HAB control using
reinforcement learning has been demonstrated on HABs, coordinated multi-agent
reinforcement learning (MARL) has not yet been investigated. This work presents
the first systematic application of multi-agent reinforcement learning (MARL)
to HAB coordination for distributed area coverage. We extend our previously
developed reinforcement learning simulation environment (RLHAB) to support
cooperative multi-agent learning, enabling multiple agents to operate
simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area
coverage coordination, leveraging Centralized Training with Decentralized
Execution to address atmospheric vehicle coordination challenges. Our approach
employs specialized observation spaces providing individual state,
environmental context, and teammate data, with hierarchical rewards
prioritizing coverage while encouraging spatial distribution. We demonstrate
that QMIX achieves similar performance to the theoretically optimal geometric
deterministic method for distributed area coverage, validating the MARL
approach and providing a foundation for more complex autonomous multi-HAB
missions where deterministic methods become intractable.

</details>


### [605] [Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning](https://arxiv.org/abs/2510.03259)
*Yoonjeon Kim,Doohyuk Jang,Eunho Yang*

Main category: cs.LG

TL;DR: 该研究提出了MASA（Meta-Awareness via Self-Alignment）方法，通过自我对齐提升大型语言模型在推理任务中的元认知能力，无需外部数据，并能在效率和准确性上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型缺乏元认知能力，表现为真实推理过程与模型预测的元信息之间存在不一致性。

Method: 设计了一种名为MASA（Meta-Awareness via Self-Alignment）的训练流程，通过自我对齐来增强模型的元认知能力。该方法通过过滤零方差提示和在推理可能出错时提前终止来提高训练效率。

Result: MASA方法在数学推理基准（如AIME25，提升19.3%）和多个跨领域基准（如GPQA-Diamond，提升3.87%）上显著提高了准确性。同时，它还能将GRPO训练速度提高1.28倍以上。

Conclusion: 增强模型的元认知能力可以显著提升其在推理任务上的准确性和泛化能力，并且通过自我学习和效率优化，可以实现高效训练。

Abstract: Recent studies on reasoning models explore the meta-awareness of language
models, the ability to know how to think by itself. We argue that large
reasoning models lack this meta-awareness property by proving severe
misalignment between true rollouts and predicted meta information. We posit
that aligning meta-prediction with true rollouts will lead to significant
performance gains. To verify this hypothesis, we design a training pipeline
that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced
meta-awareness directly translates to improved accuracy. Unlike existing
meta-cognitive reasoning models, our method does not require external training
sources but leverages self-generated signals to train meta-awareness. Moreover,
our method enables efficient training by i) filtering out zero-variance prompts
that are either trivial or unsolvable and ii) cutting off lengthy rollouts when
they are unlikely to lead to correct answers. The results are inspiring: our
strategy yields significant improvements in both accuracy and training
efficiency on in-domain tasks and shows strong generalization to out-of-domain
benchmarks. More specifically, our method can speed up GRPO training by over
1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on
AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with
meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %
boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks
spanning logical, scientific, and coding domains.

</details>


### [606] [Semantic-Inductive Attribute Selection for Zero-Shot Learning](https://arxiv.org/abs/2510.03260)
*Juan Jose Herrera-Aranda,Guillermo Gomez-Trenado,Francisco Herrera,Isaac Triguero*

Main category: cs.LG

TL;DR: 本研究提出了一种新的零样本学习（ZSL）分区方案和两种特征选择策略（基于嵌入式特征选择和演化计算），以解决语义空间中的噪声、冗余或不相关属性问题，并通过实验证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本学习（ZSL）方法在处理开放世界场景时，需要动态适应新任务。语义空间虽然在连接已知和未知类别中起着关键作用，但通常包含会阻碍性能的冗余或不相关属性。

Method: 提出了一种分区方案，在归纳设置下模拟未知条件，从而在不访问未知类别语义信息的情况下评估属性相关性。在此框架下，研究了两种互补的特征选择策略：1. 嵌入式特征选择，将模型驱动的排名转化为有意义的语义修剪；2. 演化计算，更广泛地探索属性子集空间。

Result: 在五个基准数据集（AWA2、CUB、SUN、aPY、FLO）上的实验表明，这两种方法通过减少冗余，在减少未知类别上的准确性方面取得了持续的改进。基于嵌入式特征选择的方法（RFS）效率高且具有竞争力，但依赖于关键的超参数；而基于演化计算的方法（GA）成本更高，但能更广泛地探索搜索空间且不依赖超参数。

Conclusion: 语义空间本质上是冗余的，并且所提出的分区方案是在归纳条件下优化语义空间的有效工具。

Abstract: Zero-Shot Learning is an important paradigm within General-Purpose Artificial
Intelligence Systems, particularly in those that operate in open-world
scenarios where systems must adapt to new tasks dynamically. Semantic spaces
play a pivotal role as they bridge seen and unseen classes, but whether
human-annotated or generated by a machine learning model, they often contain
noisy, redundant, or irrelevant attributes that hinder performance. To address
this, we introduce a partitioning scheme that simulates unseen conditions in an
inductive setting (which is the most challenging), allowing attribute relevance
to be assessed without access to semantic information from unseen classes.
Within this framework, we study two complementary feature-selection strategies
and assess their generalisation. The first adapts embedded feature selection to
the particular demands of ZSL, turning model-driven rankings into meaningful
semantic pruning; the second leverages evolutionary computation to directly
explore the space of attribute subsets more broadly. Experiments on five
benchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods
consistently improve accuracy on unseen classes by reducing redundancy, but in
complementary ways: RFS is efficient and competitive though dependent on
critical hyperparameters, whereas GA is more costly yet explores the search
space more broadly and avoids such dependence. These results confirm that
semantic spaces are inherently redundant and highlight the proposed
partitioning scheme as an effective tool to refine them under inductive
conditions.

</details>


### [607] [A Lightweight Federated Learning Approach for Privacy-Preserving Botnet Detection in IoT](https://arxiv.org/abs/2510.03513)
*Taha M. Mahmoud,Naima Kaabouch*

Main category: cs.LG

TL;DR: 通过使用联邦学习，提出了一种轻量级、保护隐私的物联网（IoT）僵尸网络检测框架，该框架能够在不交换原始数据的情况下协同训练模型，同时保持检测准确性并减少通信成本。


<details>
  <summary>Details</summary>
Motivation: 传统的僵尸网络检测方法在资源受限的物联网环境中存在可扩展性、隐私性和适应性方面的挑战。

Method: 提出了一种基于联邦学习的轻量级、保护隐私的僵尸网络检测框架，并引入了一种通信高效的聚合策略。

Result: 实验表明，该框架在保持高检测准确性的同时，显著降低了通信成本。

Conclusion: 联邦学习为物联网生态系统的可扩展、安全和隐私感知入侵检测提供了一条可行的途径。

Abstract: The rapid growth of the Internet of Things (IoT) has expanded opportunities
for innovation but also increased exposure to botnet-driven cyberattacks.
Conventional detection methods often struggle with scalability, privacy, and
adaptability in resource-constrained IoT environments. To address these
challenges, we present a lightweight and privacy-preserving botnet detection
framework based on federated learning. This approach enables distributed
devices to collaboratively train models without exchanging raw data, thus
maintaining user privacy while preserving detection accuracy. A
communication-efficient aggregation strategy is introduced to reduce overhead,
ensuring suitability for constrained IoT networks. Experiments on benchmark IoT
botnet datasets demonstrate that the framework achieves high detection accuracy
while substantially reducing communication costs. These findings highlight
federated learning as a practical path toward scalable, secure, and
privacy-aware intrusion detection for IoT ecosystems.

</details>


### [608] [Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark](https://arxiv.org/abs/2510.03261)
*C. Coelho,M. Hohmann,D. Fernández,L. Penter,S. Ihlenfeldt,O. Niggemann*

Main category: cs.LG

TL;DR: 通过预测高精度温度和热流场，实现机床热误差的通用和灵活补偿。


<details>
  <summary>Details</summary>
Motivation: 传统的热误差补偿方法泛化性和适应性有限，而该研究旨在提出一种新的方法来解决这个问题。

Method: 训练神经网络预测机床内部的高精度温度和热流场，并采用基于相关性的策略选择信息量大的测量点，同时对多种时间序列神经网络模型进行基准测试。

Result: 研究结果表明，该方法能够准确且低成本地预测温度和热流场，为实现机床热误差的灵活和通用补偿奠定了基础。

Conclusion: 该研究提出的新范式能够通过预测温度和热流场，实现机床热误差的通用和灵活补偿。

Abstract: Thermal errors in machine tools significantly impact machining precision and
productivity. Traditional thermal error correction/compensation methods rely on
measured temperature-deformation fields or on transfer functions. Most existing
data-driven compensation strategies employ neural networks (NNs) to directly
predict thermal errors or specific compensation values. While effective, these
approaches are tightly bound to particular error types, spatial locations, or
machine configurations, limiting their generality and adaptability. In this
work, we introduce a novel paradigm in which NNs are trained to predict
high-fidelity temperature and heat flux fields within the machine tool. The
proposed framework enables subsequent computation and correction of a wide
range of error types using modular, swappable downstream components. The NN is
trained using data obtained with the finite element method under varying
initial conditions and incorporates a correlation-based selection strategy that
identifies the most informative measurement points, minimising hardware
requirements during inference. We further benchmark state-of-the-art
time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,
Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal
Convolutional Network, by training both specialised models, tailored for
specific initial conditions, and general models, capable of extrapolating to
unseen scenarios. The results show accurate and low-cost prediction of
temperature and heat flux fields, laying the basis for enabling flexible and
generalisable thermal error correction in machine tool environments.

</details>


### [609] [MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation](https://arxiv.org/abs/2510.03601)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Kai-Chun Liu,Yu Tsao*

Main category: cs.LG

TL;DR: 该研究提出了一种多层移动边缘计算（MLMEC）框架，利用知识蒸馏（KD）技术来提高跌倒检测（FD）系统的准确性和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 随着老龄化人口的增加，跌倒检测（FD）系统作为一种辅助技术变得越来越重要。现有的基于边缘设备（ED）和云中心（CC）的FD架构面临ED模型尺寸限制和数据传输延迟等挑战。移动边缘计算（MEC）被提出用于解决这些问题。

Method: 提出了一种多层MEC（MLMEC）框架，将FD架构分为多个站点，每个站点配备一个神经网络模型。当前端设备无法可靠检测跌倒时，数据会被传输到具有更强大计算能力的后端站点。利用知识蒸馏（KD）方法，让后端站点为前端模型提供额外的学习经验，以提高前端检测精度，同时降低延迟和处理负荷。

Result: 在SisFall数据集上，KD方法将准确率提高了11.65%；在FallAllD数据集上，准确率提高了2.78%。与未使用KD的MLMEC相比，MLMEC与KD结合将FallAllD数据集的数据延迟率降低了54.15%，将SisFall数据集的数据延迟率降低了46.67%。

Conclusion: MLMEC框架结合KD技术能够有效提高跌倒检测系统的准确性并降低延迟。

Abstract: The rising aging population has increased the importance of fall detection
(FD) systems as an assistive technology, where deep learning techniques are
widely applied to enhance accuracy. FD systems typically use edge devices (EDs)
worn by individuals to collect real-time data, which are transmitted to a cloud
center (CC) or processed locally. However, this architecture faces challenges
such as a limited ED model size and data transmission latency to the CC. Mobile
edge computing (MEC), which allows computations at MEC servers deployed between
EDs and CC, has been explored to address these challenges. We propose a
multilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC
splits the architecture into stations, each with a neural network model. If
front-end equipment cannot detect falls reliably, data are transmitted to a
station with more robust back-end computing. The knowledge distillation (KD)
approach was employed to improve front-end detection accuracy by allowing
high-power back-end stations to provide additional learning experiences,
enhancing precision while reducing latency and processing loads. Simulation
results demonstrate that the KD approach improved accuracy by 11.65% on the
SisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also
reduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on
the SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD
system exhibits improved accuracy and reduced latency.

</details>


### [610] [Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout](https://arxiv.org/abs/2510.03262)
*Andi Zhang,Xuan Ding,Haofan Wang,Steven McDonagh,Samuel Kaski*

Main category: cs.LG

TL;DR: OMC Dropout 是一种新方法，可以在合并 LoRA 时强制执行严格的正交性，以避免语义向量之间的干扰。然而，实验表明，这种正交性本身并不足以实现真正的语义组合性。


<details>
  <summary>Details</summary>
Motivation: LoRA 在合并时可能存在语义向量干扰问题，而 OMC Dropout 旨在解决此问题。

Method: 提出了一种名为 Orthogonal Monte Carlo Dropout 的新机制，该机制在合并稀疏语义向量时强制执行严格的正交性，且没有额外的时问复杂度。

Result: 实验表明，OMC Dropout 强制合并的 LoRA 保持正交，但并未带来预期的语义解耦或组合性。这表明仅靠 LoRA 间的正交性可能不足以实现真正的语义组合性。

Conclusion: LoRA 间的正交性对于实现真正的语义组合性可能不是充分条件，需要重新审视其在适配器合并中的作用。

Abstract: We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict
orthogonality when combining sparse semantic vectors without extra time
complexity. LoRA, a popular fine-tuning method for large models, typically
trains a module to represent a specific concept such as an object or a style.
When multiple LoRAs are merged, for example to generate an object in a
particular style, their semantic vectors may interfere with each other. Our
method guarantees, at the theoretical and runtime levels, that merged LoRAs
remain orthogonal and thus free from direct interference. However, empirical
analysis reveals that such orthogonality does not lead to the semantic
disentanglement or compositionality highlighted in prior work on compositional
adaptation. This finding suggests that inter-LoRA orthogonality alone may be
insufficient for achieving true semantic compositionality, prompting a
re-examination of its role in adapter merging.

</details>


### [611] [Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models](https://arxiv.org/abs/2510.03263)
*Agnieszka Polowczyk,Alicja Polowczyk,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.LG

TL;DR: 现代文生图模型可能被滥用于生成有害内容，这推动了机器学习方法的发展，旨在从模型中选择性地移除特定知识，但实际遗忘概念非常困难。本文提出了“记忆自我再生”任务和“MemoRa”策略，并认为知识检索的鲁棒性是评估遗忘技术的重要指标，最后指出遗忘分为短期和长期两种。


<details>
  <summary>Details</summary>
Motivation: 现有文生图模型易被滥用，需要机器学习方法来移除有害知识，但遗忘概念存在困难。

Method: 提出“记忆自我再生”任务和“MemoRa”策略，并引入知识检索鲁棒性作为评估指标。

Result: 短期遗忘概念可被快速恢复，长期遗忘则更具挑战性。

Conclusion: 遗忘过程分为短期和长期，知识检索的鲁棒性是评估遗忘技术有效性的关键。

Abstract: The impressive capability of modern text-to-image models to generate
realistic visuals has come with a serious drawback: they can be misused to
create harmful, deceptive or unlawful content. This has accelerated the push
for machine unlearning. This new field seeks to selectively remove specific
knowledge from a model's training data without causing a drop in its overall
performance. However, it turns out that actually forgetting a given concept is
an extremely difficult task. Models exposed to attacks using adversarial
prompts show the ability to generate so-called unlearned concepts, which can be
not only harmful but also illegal. In this paper, we present considerations
regarding the ability of models to forget and recall knowledge, introducing the
Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which
we consider to be a regenerative approach supporting the effective recovery of
previously lost knowledge. Moreover, we propose that robustness in knowledge
retrieval is a crucial yet underexplored evaluation measure for developing more
robust and effective unlearning techniques. Finally, we demonstrate that
forgetting occurs in two distinct ways: short-term, where concepts can be
quickly recalled, and long-term, where recovery is more challenging.

</details>


### [612] [Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data](https://arxiv.org/abs/2510.03264)
*Syeda Nahida Akter,Shrimai Prabhumoye,Eric Nyberg,Mostofa Patwary,Mohammad Shoeybi,Yejin Choi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: 在预训练阶段引入包含多样化推理模式的数据，能显著提升LLM的能力，且优于仅在后期进行微调。但微调阶段对数据质量更为敏感。


<details>
  <summary>Details</summary>
Motivation: 探讨在LLM训练的不同阶段（预训练、中期训练、后期微调）引入推理数据对模型能力的影响，以及不同阶段数据分配的优化策略。

Method: 系统性地研究了不同规模、多样性和质量的推理数据在LLM训练不同阶段引入时对模型性能的影响。

Result: 1. 预训练阶段引入推理数据（特别是多样化的推理模式）能带来显著的平均19%的性能提升，且是后期SFT无法完全弥补的。2. 预训练阶段更受益于推理模式的多样性（平均提升11%），而SFT阶段对数据质量更敏感（平均提升15%）。3. 预训练阶段高质量的数据具有潜在效果，仅在SFT后激活；盲目增加SFT数据可能适得其反，抹去早期推理数据注入的好处。

Conclusion: 预训练阶段引入多样化的推理数据是构建更强大LLM的关键，挑战了语言建模和推理能力分离的传统观念，并为跨整个训练流程的战略性数据分配提供了原则性指导。

Abstract: The prevailing paradigm for enhancing the reasoning abilities of LLMs
revolves around post-training on high-quality, reasoning-intensive data. While
emerging literature suggests that reasoning data is increasingly incorporated
also during the mid-training stage-a practice that is relatively more
proprietary and less openly characterized-the role of such data in pretraining
remains unclear. In particular, due to the opaqueness of pretraining corpora in
most frontier models, the effect of reasoning data introduced at different
phases of pre- and/or post-training is relatively less reported in the
scientific literature. This raises several important questions: Is adding
reasoning data earlier during pretraining any better than introducing it during
post-training? Could earlier inclusion risk overfitting and harm
generalization, or instead establish durable foundations that later fine-tuning
cannot recover? We conduct the first systematic study of how reasoning
data-varying in scale, diversity, and quality-affects LLM performance when
introduced at different stages of training. We find that front-loading
reasoning data into pretraining is critical (19% avg gain), establishing
foundational capabilities that cannot be fully replicated by later-stage SFT,
even with more data. We uncover an asymmetric principle for optimal data
allocation: pretraining benefits most from broad diversity in reasoning
patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg
gain). We show that high-quality pretraining data has latent effects, activated
only after SFT, and that naively scaling SFT data can be detrimental, washing
away the benefits of early reasoning injection. Our results challenge the
conventional separation of language modeling and reasoning, providing a
principled guide for strategically allocating data across the entire training
pipeline to build more capable models.

</details>


### [613] [MindCraft: How Concept Trees Take Shape In Deep Models](https://arxiv.org/abs/2510.03265)
*Bowei Tian,Yexiao He,Wanghao Ye,Ziyao Wang,Meng Liu,Ang Li*

Main category: cs.LG

TL;DR: MindCraft框架通过概念树揭示了大型基础模型中概念的层次化出现和稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型在各种任务中表现出色，但其内部概念的结构和稳定性机制尚不清楚。

Method: 提出MindCraft框架，构建概念树，通过谱分解和概念路径链接来重构概念的层次化出现，揭示概念如何从共享表征分离成线性可分子空间。

Result: 在医学诊断、物理推理和政治决策等多个领域进行了实证评估，结果表明概念树能够恢复语义层次结构，分离潜在概念，并具有广泛的适用性。

Conclusion: 概念树为深度模型中的概念表征分析提供了一个强大且广泛适用的框架，是可解释人工智能领域的重要进展。

Abstract: Large-scale foundation models demonstrate strong performance across language,
vision, and reasoning tasks. However, how they internally structure and
stabilize concepts remains elusive. Inspired by causal inference, we introduce
the MindCraft framework built upon Concept Trees. By applying spectral
decomposition at each layer and linking principal directions into branching
Concept Paths, Concept Trees reconstruct the hierarchical emergence of
concepts, revealing exactly when they diverge from shared representations into
linearly separable subspaces. Empirical evaluations across diverse scenarios
across disciplines, including medical diagnosis, physics reasoning, and
political decision-making, show that Concept Trees recover semantic
hierarchies, disentangle latent concepts, and can be widely applied across
multiple domains. The Concept Tree establishes a widely applicable and powerful
framework that enables in-depth analysis of conceptual representations in deep
models, marking a significant step forward in the foundation of interpretable
AI.

</details>


### [614] [Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model](https://arxiv.org/abs/2510.03266)
*Bharat Sharma,Jitendra Kumar*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Climate anomalies significantly impact terrestrial carbon cycle dynamics,
necessitating robust methods for detecting and analyzing anomalous behavior in
plant productivity. This study presents a novel application of variational
autoencoders (VAE) for identifying extreme events in gross primary productivity
(GPP) from Community Earth System Model version 2 simulations across four AR6
regions in the Continental United States. We compare VAE-based anomaly
detection with traditional singular spectral analysis (SSA) methods across
three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.
The VAE architecture employs three dense layers and a latent space with an
input sequence length of 12 months, trained on a normalized GPP time series to
reconstruct the GPP and identifying anomalies based on reconstruction errors.
Extreme events are defined using 5th percentile thresholds applied to both VAE
and SSA anomalies. Results demonstrate strong regional agreement between VAE
and SSA methods in spatial patterns of extreme event frequencies, despite VAE
producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA
across regions and periods). Both methods reveal increasing magnitudes and
frequencies of negative carbon cycle extremes toward 2050-80, particularly in
Western and Central North America. The VAE approach shows comparable
performance to established SSA techniques, while offering computational
advantages and enhanced capability for capturing non-linear temporal
dependencies in carbon cycle variability. Unlike SSA, the VAE method does not
require one to define the periodicity of the signals in the data; it discovers
them from the data.

</details>


### [615] [PT$^2$-LLM: Post-Training Ternarization for Large Language Models](https://arxiv.org/abs/2510.03267)
*Xianglong Yan,Chengzhu Bao,Zhiteng Li,Tianao Zhang,Kaicheng Yang,Haotong Qin,Ruobing Xie,Xingwu Sun,Yulun Zhang*

Main category: cs.LG

TL;DR: PT$^2$-LLM 是一种用于大语言模型（LLM）的训练后三值化框架，通过迭代三值拟合（ITF）和激活感知网格对齐（AGA）来优化量化，并使用基于结构相似性的重排（SSR）策略处理离群值，实现了与 SOTA 2 位 PTQ 方法相当的性能，同时降低了内存成本并加速了预填充和解码。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）因其巨大的内存和计算需求而难以部署。虽然三值化是一种有前景的压缩技术，但由于训练无关的参数优化挑战以及离群值和分散权重带来的量化困难，其在训练后量化（PTQ）方面的潜力尚未得到充分探索。

Method: 提出了一种名为 PT$^2$-LLM 的训练后三值化框架，其核心是一个不对称三值量化器，并包含一个两阶段优化流程：1）迭代三值拟合（ITF），通过交替构建最优三值网格和灵活舍入来最小化量化误差；2）激活感知网格对齐（AGA），进一步优化三值网格以更好地匹配全精度输出。此外，还引入了一种即插即用的基于结构相似性的重排（SSR）策略，利用列间结构相似性来简化量化并减轻离群值效应。

Result: 实验证明，PT$^2$-LLM 在内存成本更低的情况下，实现了与最先进（SOTA）的 2 位 PTQ 方法相当的性能，同时加速了预填充和解码，实现了端到端的加速。

Conclusion: PT$^2$-LLM 成功地将三值化应用于大语言模型的训练后量化，克服了现有方法的局限性，并在保持高性能的同时显著提高了效率。

Abstract: Large Language Models (LLMs) have shown impressive capabilities across
diverse tasks, but their large memory and compute demands hinder deployment.
Ternarization has gained attention as a promising compression technique,
delivering substantial size reduction and high computational efficiency.
However, its potential in the post-training quantization (PTQ) setting remains
underexplored, due to the challenge of training-free parameter optimization and
the quantization difficulty posed by outliers and dispersed weights. To address
these issues, we propose PT$^2$-LLM, a post-training ternarization framework
tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with
a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which
alternates between optimal ternary grid construction and flexible rounding to
minimize quantization error, and (2) Activation-aware Grid Alignment (AGA),
which further refines the ternary grid to better match full-precision outputs.
In addition, we propose a plug-and-play Structural Similarity-based Reordering
(SSR) strategy that leverages inter-column structural similarity to ease
quantization and mitigate outlier effects, further enhancing overall
performance. Extensive experiments demonstrate that PT$^2$-LLM delivers
competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with
lower memory cost, while also accelerating both prefill and decoding to achieve
end-to-end speedup. The code and models will be available at
https://github.com/XIANGLONGYAN/PT2-LLM.

</details>


### [616] [Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment](https://arxiv.org/abs/2510.03268)
*Lingjie Yi,Raphael Douady,Chao Chen*

Main category: cs.LG

TL;DR: 多模态对比学习（MCL）旨在将不同模态的数据嵌入到共享的嵌入空间中，但实际表明不同模态的表示占据完全分离的空间区域（模态鸿沟）。本研究提出了第一个分析MCL收敛最优表示和模态对齐的理论框架，证明了维度坍塌是模态鸿沟的根本原因，并提出了超平面旋转和共享空间投影的方法来解决模态鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 研究多模态对比学习中的模态鸿沟现象，探究其产生原因及其对下游任务的影响。

Method: 提出分析MCL收敛最优表示和模态对齐的理论框架，证明在不同约束下模态鸿沟的收敛情况，识别维度坍塌为模态鸿沟的根本原因，并提出超平面旋转和共享空间投影的方法。

Result: 证明了在无约束或锥约束下，模态鸿沟会收敛到零；在子空间约束下，模态鸿沟会收敛到两个超平面之间的最小角度，并证明了在此情况下无法完美对齐样本对。

Conclusion: 维度坍塌是模态鸿沟的根本原因，模态鸿沟通过影响样本对齐来影响下游任务性能。通过超平面旋转和共享空间投影可以实现两种模态之间的完美对齐。

Abstract: Multimodal contrastive learning (MCL) aims to embed data from different
modalities in a shared embedding space. However, empirical evidence shows that
representations from different modalities occupy completely separate regions of
embedding space, a phenomenon referred to as the modality gap. Moreover,
experimental findings on how the size of the modality gap influences downstream
performance are inconsistent. These observations raise two key questions: (1)
What causes the modality gap? (2) How does it affect downstream tasks? To
address these questions, this paper introduces the first theoretical framework
for analyzing the convergent optimal representations of MCL and the modality
alignment when training is optimized. Specifically, we prove that without any
constraint or under the cone constraint, the modality gap converges to zero.
Under the subspace constraint (i.e., representations of two modalities fall
into two distinct hyperplanes due to dimension collapse), the modality gap
converges to the smallest angle between the two hyperplanes. This result
identifies \emph{dimension collapse} as the fundamental origin of the modality
gap. Furthermore, our theorems demonstrate that paired samples cannot be
perfectly aligned under the subspace constraint. The modality gap influences
downstream performance by affecting the alignment between sample pairs. We
prove that, in this case, perfect alignment between two modalities can still be
achieved via two ways: hyperplane rotation and shared space projection.

</details>


### [617] [General Exploratory Bonus for Optimistic Exploration in RLHF](https://arxiv.org/abs/2510.03269)
*Wendi Li,Changdae Oh,Yixuan Li*

Main category: cs.LG

TL;DR: GEB框架通过参考依赖的奖励调节来解决KL或α散度正则化中的探索偏差问题，在RLHF对齐任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有探索奖励方法未能实现乐观探索，导致探索偏向高概率区域，强化保守行为。

Method: 提出GEB理论框架，通过参考依赖的奖励调节来解决偏差问题，并统一了先前的方法。

Result: GEB在多个散度设置和LLM骨干的对齐任务中一致优于基线方法。

Conclusion: GEB为RLHF中的乐观探索提供了原则性和实用性的解决方案。

Abstract: Optimistic exploration is central to improving sample efficiency in
reinforcement learning with human feedback, yet existing exploratory bonus
methods to incentivize exploration often fail to realize optimism. We provide a
theoretical analysis showing that current formulations, under KL or
$\alpha$-divergence regularization, unintentionally bias exploration toward
high-probability regions of the reference model, thereby reinforcing
conservative behavior instead of promoting discovery of uncertain regions. To
address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel
theoretical framework that provably satisfies the optimism principle. GEB
counteracts divergence-induced bias via reference-dependent reward regulation
and unifies prior heuristic bonuses as special cases, while extending naturally
across the full $\alpha$-divergence family. Empirically, GEB consistently
outperforms baselines on alignment tasks across multiple divergence settings
and large language model backbones. These results demonstrate that GEB offers
both a principled and practical solution for optimistic exploration in RLHF.

</details>


### [618] [CoDA: Coding LM via Diffusion Adaptation](https://arxiv.org/abs/2510.03270)
*Haolin Chen,Shiyu Wang,Can Qin,Bo Pang,Zuxin Liu,Jielin Qiu,Jianguo Zhang,Yingbo Zhou,Zeyuan Chen,Ran Xu,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.LG

TL;DR: CoDA是一个1.7B参数的开源扩散语言模型，通过结合扩散预训练、代码中间训练和指令调优，实现了与更大模型相当的性能，同时保持了有竞争力的推理延迟。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有扩散语言模型（在双向上下文和填充能力方面优于自回归模型）在实际应用中过于笨重的问题。

Method: CoDA通过结合大规模扩散预训练、代码中间训练和指令调优，并利用置信度引导采样来实现高效推理。

Result: 在Humaneval、MBPP和EvalPlus基准测试中，CoDA-1.7B-Instruct的性能与多达7B参数的扩散模型相当或更优。

Conclusion: CoDA的发布（包括模型检查点、评估工具和TPU训练管道）旨在加速轻量级扩散模型在代码助手领域的研究。

Abstract: Diffusion language models promise bidirectional context and infilling
capabilities that autoregressive coders lack, yet practical systems remain
heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU
with a fully open-source training pipeline. CoDA pairs large-scale diffusion
pre-training with code-centric mid-training and instruction tuning, enabling
confidence-guided sampling that keeps inference latency competitive. On
Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses
diffusion models up to 7B parameters. Our release includes model checkpoints,
evaluation harnesses, and TPU training pipelines to accelerate research on
lightweight diffusion-based coding assistants.

</details>


### [619] [Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary](https://arxiv.org/abs/2510.03271)
*Zi Liang,Zhiyao Wu,Haoyang Shang,Yulin Jin,Qingqing Ye,Huadi Zheng,Peizhao Hu,Haibo Hu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为决策潜能面（DPS）的新概念，用于分析大型语言模型（LLM）的决策边界，解决了现有LLM决策边界分析的计算难题。


<details>
  <summary>Details</summary>
Motivation: 分析大型语言模型（LLM）的决策边界对于揭示其核心属性和解释其行为至关重要，但现有方法因LLM的巨大规模和自回归特性而难以实现。

Method: 提出决策潜能面（DPS）的概念，该概念基于区分不同采样序列的置信度来定义，并证明了DPS中的零高度等高线等同于LLM的决策边界。在此基础上，开发了一种名为 K-DPS 的近似决策边界构建算法，该算法仅需有限次的序列采样即可在高精度下逼近LLM的决策边界，并理论推导了K-DPS与理想DPS之间的误差界限。

Result: K-DPS算法能够以可忽略的误差近似LLM的决策边界，并且误差可以与采样次数进行权衡。实验结果在多个LLM和语料库上得到了验证。

Conclusion: DPS为LLM决策边界的分析提供了一个新颖且可行的视角，而K-DPS算法则提供了一种高效的近似计算方法，为理解和解释LLM的行为奠定了基础。

Abstract: Decision boundary, the subspace of inputs where a machine learning model
assigns equal classification probabilities to two classes, is pivotal in
revealing core model properties and interpreting behaviors. While analyzing the
decision boundary of large language models (LLMs) has raised increasing
attention recently, constructing it for mainstream LLMs remains computationally
infeasible due to the enormous vocabulary-sequence sizes and the
auto-regressive nature of LLMs. To address this issue, in this paper we propose
Decision Potential Surface (DPS), a new notion for analyzing LLM decision
boundary. DPS is defined on the confidences in distinguishing different
sampling sequences for each input, which naturally captures the potential of
decision boundary. We prove that the zero-height isohypse in DPS is equivalent
to the decision boundary of an LLM, with enclosed regions representing decision
regions. By leveraging DPS, for the first time in the literature, we propose an
approximate decision boundary construction algorithm, namely $K$-DPS, which
only requires K-finite times of sequence sampling to approximate an LLM's
decision boundary with negligible error. We theoretically derive the upper
bounds for the absolute error, expected error, and the error concentration
between K-DPS and the ideal DPS, demonstrating that such errors can be
trade-off with sampling times. Our results are empirically validated by
extensive experiments across various LLMs and corpora.

</details>


### [620] [PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling](https://arxiv.org/abs/2510.03272)
*Yukun Zhang,Xueqing Zhou*

Main category: cs.LG

TL;DR: Transformer架构被重新构想为一个连续的时空动力系统，其核心组件被解释为特定的数学算子，并证明了残差连接和层归一化作为稳定机制的必要性。


<details>
  <summary>Details</summary>
Motivation: Transformer架构的内部机制缺乏理论理解。

Method: 将Transformer的离散分层结构重新构想为由主偏微分方程（PDE）控制的连续时空动力系统，并将自注意力、前馈网络、残差连接和层归一化等组件映射到数学算子。

Result: 实验表明，缺乏残差连接会导致灾难性的表示漂移，而缺乏层归一化会导致训练动态不稳定和爆炸。

Conclusion: 残差连接和层归一化是稳定Transformer中否则强大但本质上不稳定的连续系统的基本数学稳定器。

Abstract: The Transformer architecture has revolutionized artificial intelligence, yet
a principled theoretical understanding of its internal mechanisms remains
elusive. This paper introduces a novel analytical framework that
reconceptualizes the Transformer's discrete, layered structure as a continuous
spatiotemporal dynamical system governed by a master Partial Differential
Equation (PDE). Within this paradigm, we map core architectural components to
distinct mathematical operators: self-attention as a non-local interaction, the
feed-forward network as a local reaction, and, critically, residual connections
and layer normalization as indispensable stabilization mechanisms. We do not
propose a new model, but rather employ the PDE system as a theoretical probe to
analyze the mathematical necessity of these components. By comparing a standard
Transformer with a PDE simulator that lacks explicit stabilizers, our
experiments provide compelling empirical evidence for our central thesis. We
demonstrate that without residual connections, the system suffers from
catastrophic representational drift, while the absence of layer normalization
leads to unstable, explosive training dynamics. Our findings reveal that these
seemingly heuristic "tricks" are, in fact, fundamental mathematical stabilizers
required to tame an otherwise powerful but inherently unstable continuous
system. This work offers a first-principles explanation for the Transformer's
design and establishes a new paradigm for analyzing deep neural networks
through the lens of continuous dynamics.

</details>


### [621] [Training Variation of Physically-Informed Deep Learning Models](https://arxiv.org/abs/2510.03416)
*Ashley Lenau,Dennis Dimiduk,Stephen R. Niezgoda*

Main category: cs.LG

TL;DR: 深度学习网络的成功不仅依赖于训练数据集，还依赖于用于调试网络以完成特定任务的训练算法。损失函数、数据集和超参数的调整在训练网络中都起着至关重要的作用，但对于训练算法的可靠性或可重复性却鲜有讨论。随着物理信息损失函数的日益普及，人们不禁要问，损失函数在训练网络以强制执行特定边界条件方面的可靠性如何。需要报告模型变异性来评估损失函数在一致训练网络以遵守给定边界条件方面的能力，并为不同方法提供更公平的比较。在这项工作中，以预测高弹性对比复合材料应力场的 Pix2Pix 网络为例。实施了几种强制应力平衡的不同损失函数，每种函数在多次训练过程中表现出不同的收敛性、准确性和强制应力平衡水平。还分享了报告模型变异性的建议做法。


<details>
  <summary>Details</summary>
Motivation: 目前对于深度学习训练算法的可靠性或可重复性研究较少，特别是对于物理信息损失函数在强制执行特定边界条件方面的能力尚不明确。因此，需要评估损失函数在训练网络以遵守给定边界条件方面的能力，并为不同方法提供更公平的比较。

Method: 以 Pix2Pix 网络预测高弹性对比复合材料应力场为例，实施了几种不同的强制应力平衡的损失函数，并分析了它们在收敛性、准确性和强制应力平衡方面的变异性。

Result: 不同的损失函数在收敛性、准确性和强制应力平衡方面表现出不同的变异性水平。

Conclusion: 报告模型变异性对于评估损失函数在一致训练网络以遵守给定边界条件方面的能力至关重要，并有助于实现不同方法之间的公平比较。文章还提出了一些关于如何报告模型变异性的实践建议。

Abstract: A successful deep learning network is highly dependent not only on the
training dataset, but the training algorithm used to condition the network for
a given task. The loss function, dataset, and tuning of hyperparameters all
play an essential role in training a network, yet there is not much discussion
on the reliability or reproducibility of a training algorithm. With the rise in
popularity of physics-informed loss functions, this raises the question of how
reliable one's loss function is in conditioning a network to enforce a
particular boundary condition. Reporting the model variation is needed to
assess a loss function's ability to consistently train a network to obey a
given boundary condition, and provides a fairer comparison among different
methods. In this work, a Pix2Pix network predicting the stress fields of high
elastic contrast composites is used as a case study. Several different loss
functions enforcing stress equilibrium are implemented, with each displaying
different levels of variation in convergence, accuracy, and enforcing stress
equilibrium across many training sessions. Suggested practices in reporting
model variation are also shared.

</details>


### [622] [Learning without Global Backpropagation via Synergistic Information Distillation](https://arxiv.org/abs/2510.03273)
*Chenhao Ye,Ming Tang*

Main category: cs.LG

TL;DR: SID是一种新的训练框架，通过将深度学习重构为局部协同优化问题来解决反向传播（BP）的扩展性瓶颈。


<details>
  <summary>Details</summary>
Motivation: BP在训练深度学习模型时存在更新锁定和高内存消耗的扩展性瓶颈。

Method: SID将深度网络构建为模块化流水线，每个模块都有一个局部目标，用于优化关于真实目标的概率信念。该目标在忠实于目标和与前一个模块的信念保持一致之间进行平衡。通过解耦模块间的反向依赖，SID实现了并行训练，消除了更新锁定，并大大降低了内存需求。

Result: SID在理论上保证了性能随网络深度的单调提升。在实践中，SID在分类准确性上与BP相当或更优，并且在扩展性和对标签噪声的鲁棒性方面表现出色。

Conclusion: SID是一种有效的BP替代方案，具有更好的可扩展性和鲁棒性，同时保持了标准的推理过程。

Abstract: Backpropagation (BP), while foundational to deep learning, imposes two
critical scalability bottlenecks: update locking, where network modules remain
idle until the entire backward pass completes, and high memory consumption due
to storing activations for gradient computation. To address these limitations,
we introduce Synergistic Information Distillation (SID), a novel training
framework that reframes deep learning as a cascade of local cooperative
refinement problems. In SID, a deep network is structured as a pipeline of
modules, each imposed with a local objective to refine a probabilistic belief
about the ground-truth target. This objective balances fidelity to the target
with consistency to the belief from its preceding module. By decoupling the
backward dependencies between modules, SID enables parallel training and hence
eliminates update locking and drastically reduces memory requirements.
Meanwhile, this design preserves the standard feed-forward inference pass,
making SID a versatile drop-in replacement for BP. We provide a theoretical
foundation, proving that SID guarantees monotonic performance improvement with
network depth. Empirically, SID consistently matches or surpasses the
classification accuracy of BP, exhibiting superior scalability and pronounced
robustness to label noise.Code is available at:
https://github.com/ychAlbert/sid-bp

</details>


### [623] [Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models](https://arxiv.org/abs/2510.03274)
*Tianao Zhang,Zhiteng Li,Xianglong Yan,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.LG

TL;DR: dLLM的2位元量化，提出Quant-dLLM框架，包含MCS和DAQ以解決標準PTQ方法不適用的問題，並引入ABMP以在2位元預算下優化精度配置，最終在dLLM上實現優於SOTA AR-PTQ方法的準確性。


<details>
  <summary>Details</summary>
Motivation: dLLM模型尺寸持續增長，需要進行權重壓縮以利於部署。然而，直接將用於AR LLM的2位元PTQ方法應用於dLLM會導致效能不佳。

Method: 提出Quant-dLLM框架，包含：1. Masked Calibration Simulation (MCS)，用於處理dLLM的遮罩去噪激活與標準PTQ方法假設的完全可見信號之間的差異，以進行更可靠的校準。2. Data-aware Any-order Quantizer (DAQ)，透過迭代逼近和模擬校準數據來學習超低位元權重表示。3. Adaptive Blockwise Mixed Precision (ABMP)，一種基於敏感度的精度分配方案，用於在嚴格的2位元預算下跨通道群組自適應地分配位寬。

Result: 在嚴格的2位元精度限制下，Quant-dLLM在dLLM上的準確性始終優於最先進的AR-transfer PTQ方法。

Conclusion: Quant-dLLM是一個針對dLLM的超低位元PTQ框架，透過MCS和DAQ解決了標準PTQ方法在dLLM上的限制，並利用ABMP在極低位元預算下實現了高效的精度分配，展現了優於現有方法的效能。

Abstract: Diffusion large language models (dLLMs), which offer bidirectional context
and flexible masked-denoising generation, are emerging as a compelling
alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model
sizes continue to grow, motivating weight compression for deployment. Although
post-training quantization (PTQ) is effective for AR LLMs, directly
transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To
tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework
tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the
fully visible signals assumed by standard PTQ methods, we introduce Masked
Calibration Simulation (MCS) to align calibration with the timestep-dependent
masking, which yields more reliable calibrations. Moreover, we propose a
Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight
representations via an optimization algorithm. It performs iterative
approximation guided by our simulated calibration data. In addition, under a
strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a
sensitivity-based precision allocation scheme that adaptively assigns bit width
across channel groups. When restricted to 2-bit precision, Quant-dLLM
consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer
PTQ methods on dLLMs. The code and models will be available at:
https://github.com/ZTA2785/Quant-dLLM.

</details>


### [624] [SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size](https://arxiv.org/abs/2510.03275)
*Junhao Xia,Ming Zhao,Limin Xiao,Xiujun Zhang*

Main category: cs.LG

TL;DR: SDQ-LLM是一个创新的框架，它使用Sigma-Delta量化技术实现了LLM的1比特量化，能够在保持模型性能的同时大幅减少计算和内存需求，并通过动态调整过采样率（OSR）和采用MultiOSR策略进一步优化量化效果。


<details>
  <summary>Details</summary>
Motivation: LLM面临巨大的计算和内存挑战，因此需要极低比特量化来实现高效部署。

Method: SDQ-LLM框架采用过采样和Sigma-Delta量化器将LLM权重二值化或三值化，用加法运算替代乘法运算。它还引入了基于Hadamard的权重平滑以及逐层、逐线性单元的OSR分配策略（MultiOSR），以减少量化精度损失并优化模型大小与精度的权衡。

Result: 在OPT和LLaMA模型家族上的实验表明，SDQ-LLM即使在极低的OSR设置下也能实现高效和高精度的性能。

Conclusion: SDQ-LLM框架能够有效地实现LLM的极低比特量化，同时保持其语言推理能力，为LLM的高效部署提供了解决方案。

Abstract: Large language models (LLMs) face significant computational and memory
challenges, making extremely low-bit quantization crucial for their efficient
deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for
1-bit LLMs of any size, a novel framework that enables extremely low-bit
quantization of LLMs while preserving their linguistic reasoning capabilities.
A distinctive feature of SDQ-LLM is the continuous adjustability of the
Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM
constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal
trade-off between model size and accuracy. SDQ-LLM uses upsampling combined
with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding
high-precision parameters into 1-bit or 1.58-bit representations, replacing the
multiplication operations within linear layers with addition. This approach
significantly enhances inference efficiency under extremely low-bit
quantization. To further reduce the loss of quantization precision, we
incorporate Hadamard-based weight smoothing prior to quantization, improving
the stability and robustness of the weight representations. Furthermore, to
fully leverage the continuity of the OSR and reduce precision loss, recognizing
the correlation between quantization sensitivity and weight variance, we
propose a fine-grained, layer- and linear-wise OSR allocation strategy,
MultiOSR. This strategy distributes OSR both across layers and within each
layer, based on weight variance and parameter scale. Finally, extensive
experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a
more efficient and high-precision performance even under highly aggressive
low-OSR settings. Our code is available at
https://github.com/Dreamlittlecat/LLM-Quant-Factory.

</details>


### [625] [QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks](https://arxiv.org/abs/2510.03276)
*Qian Chen,Linxin Yang,Akang Wang,Xiaodong Luo,Yin Zhang*

Main category: cs.LG

TL;DR: 通过引入低秩、权重共享和稀疏化技术的轻量级二次增强器，在神经网络中加入二次变换以提高非线性，从而在图像分类、文本分类和大型语言模型微调任务中获得显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了增强神经网络逼近复杂函数的能力，本研究旨在通过引入二次变换来进一步提高其非线性度。

Method: 提出了一种轻量级的二次增强器，该增强器利用低秩、权重共享和稀疏化技术，在网络的每一层引入二次特征交互，同时最小化参数和计算的复杂度。

Result: 在图像分类、文本分类和大型语言模型微调三个任务的实验中，所提出的方法均带来了清晰且显著的性能提升。

Conclusion: 在神经网络中引入轻量级的二次变换可以在不显著增加模型参数和计算量的同时，有效提升模型在多种任务上的性能。

Abstract: The combination of linear transformations and non-linear activation functions
forms the foundation of most modern deep neural networks, enabling them to
approximate highly complex functions. This paper explores the introduction of
quadratic transformations to further increase nonlinearity in neural networks,
with the aim of enhancing the performance of existing architectures. To reduce
parameter complexity and computational complexity, we propose a lightweight
quadratic enhancer that uses low-rankness, weight sharing, and sparsification
techniques. For a fixed architecture, the proposed approach introduces
quadratic interactions between features at every layer, while only adding
negligible amounts of additional model parameters and forward computations. We
conduct a set of proof-of-concept experiments for the proposed method across
three tasks: image classification, text classification, and fine-tuning
large-language models. In all tasks, the proposed approach demonstrates clear
and substantial performance gains.

</details>


### [626] [Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI](https://arxiv.org/abs/2510.04622)
*Youngjoon Lee,Seongmin Cho,Yehhyun Jo,Jinu Gong,Hyunjoo Jenny Lee,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 由于隐私法规和资源限制，生物医学时间序列人工智能开发面临数据稀缺和可访问性挑战。本文提出了一种基于先进预测模型的框架，用于生成可保留真实数据统计特性且不泄露患者隐私的合成生物医学时间序列数据，以解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 严格的隐私法规和巨大的资源需求限制了生物医学时间序列人工智能（AI）的开发，导致数据需求与可访问性之间存在关键差距。合成数据生成通过在不损害患者隐私的情况下生成能够保持真实生物医学时间序列数据统计特性的数据，为解决此问题提供了一种有前景的解决方案。

Method: 提出了一种基于先进预测模型的合成生物医学时间序列数据生成框架，能够高保真地复制复杂的电生理信号（如脑电图和肌电图）。

Result: 生成的合成数据集能够保留真实数据重要的时域和频域特性，有效解决了数据稀缺和隐私问题，并实现了稳健的分析。在多个受试者上的评估表明，生成的合成数据可以有效替代真实数据，并显著提升AI模型的性能。该方法保持了关键的生物医学特征，同时具有良好的可扩展性，可满足各种应用需求，并可无缝集成到开源存储库中，极大地扩展了生物医学AI研究的资源。

Conclusion: 所提出的合成数据生成框架能够有效解决生物医学时间序列AI开发中的数据稀缺和隐私问题，并且生成的合成数据可以作为真实数据的有效替代品，同时提升AI模型的性能，为生物医学AI研究提供了可扩展的解决方案。

Abstract: The limited data availability due to strict privacy regulations and
significant resource demands severely constrains biomedical time-series AI
development, which creates a critical gap between data requirements and
accessibility. Synthetic data generation presents a promising solution by
producing artificial datasets that maintain the statistical properties of real
biomedical time-series data without compromising patient confidentiality. We
propose a framework for synthetic biomedical time-series data generation based
on advanced forecasting models that accurately replicates complex
electrophysiological signals such as EEG and EMG with high fidelity. These
synthetic datasets preserve essential temporal and spectral properties of real
data, which enables robust analysis while effectively addressing data scarcity
and privacy challenges. Our evaluations across multiple subjects demonstrate
that the generated synthetic data can serve as an effective substitute for real
data and also significantly boost AI model performance. The approach maintains
critical biomedical features while provides high scalability for various
applications and integrates seamlessly into open-source repositories,
substantially expanding resources for AI-driven biomedical research.

</details>


### [627] [Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition](https://arxiv.org/abs/2510.03278)
*Filip Landgren*

Main category: cs.LG

TL;DR: B-PINNs在解决不确定性微分方程时，需要仔细解释不确定性和过度自信，因为物理约束对网络的影响尚不清楚。


<details>
  <summary>Details</summary>
Motivation: 解释单独的物理约束如何塑造这些网络。

Method: 我们引入了一个可扩展的、无矩阵的拉普拉斯框架，该框架将后验Hessian分解为每个约束的贡献，并提供量化它们对损失景观相对影响的度量。

Result: 应用于Van der Pol方程，我们的方法跟踪约束如何塑造网络的几何形状，并通过Hessian直接展示改变单个损失权重如何非平凡地重新分配它们的曲率和有效主导地位。

Conclusion: B-PINNs在解决不确定性微分方程时，需要仔细解释不确定性和过度自信，因为物理约束对网络的影响尚不清楚。

Abstract: Bayesian physics-informed neural networks (B-PINNs) merge data with governing
equations to solve differential equations under uncertainty. However,
interpreting uncertainty and overconfidence in B-PINNs requires care due to the
poorly understood effects the physical constraints have on the network;
overconfidence could reflect warranted precision, enforced by the constraints,
rather than miscalibration. Motivated by the need to further clarify how
individual physical constraints shape these networks, we introduce a scalable,
matrix-free Laplace framework that decomposes the posterior Hessian into
contributions from each constraint and provides metrics to quantify their
relative influence on the loss landscape. Applied to the Van der Pol equation,
our method tracks how constraints sculpt the network's geometry and shows,
directly through the Hessian, how changing a single loss weight non-trivially
redistributes curvature and effective dominance across the others.

</details>


### [628] [Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data](https://arxiv.org/abs/2510.04927)
*Usman Akram,Yiyue Chen,Haris Vikalo*

Main category: cs.LG

TL;DR: 本文提出了一种名为FedSSL-AMC的联邦学习框架，用于解决自动调制分类（AMC）中的隐私、通信和鲁棒性问题，并有效处理类别不平衡、非独立同分布（non-IID）数据和有限标签样本等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的AMC模型训练存在隐私泄露、通信开销大和对信道变化鲁棒性差等问题。联邦学习（FL）虽然避免了中心化聚合，但仍受类别不平衡、非IID数据和标签样本有限的困扰。

Method: FedSSL-AMC框架采用因果、时间膨胀的卷积神经网络（CNN），并结合三元组损失的自监督学习方法，在无标签的IQ序列上进行跨客户端训练。之后，在每个客户端上使用少量的标签数据训练支持向量机（SVM）进行分类。

Result: 在合成和实际（over-the-air）数据集上的实验表明，与监督式FL基线相比，FedSSL-AMC在异构信噪比（SNR）、载波频率偏移和非IID标签划分的情况下，都能取得持续的性能提升。我们还证明了该联邦表示学习过程的收敛性以及下游分类器在特征噪声下的可分性保证。

Conclusion: FedSSL-AMC框架通过结合自监督学习和联邦学习，有效解决了AMC中的多项挑战，并在各种不理想的条件下展现出优越的性能和鲁棒性。

Abstract: Training automatic modulation classification (AMC) models on centrally
aggregated data raises privacy concerns, incurs communication overhead, and
often fails to confer robustness to channel shifts. Federated learning (FL)
avoids central aggregation by training on distributed clients but remains
sensitive to class imbalance, non-IID client distributions, and limited labeled
samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with
triplet-loss self-supervision on unlabeled I/Q sequences across clients,
followed by per-client SVMs on small labeled sets. We establish convergence of
the federated representation learning procedure and a separability guarantee
for the downstream classifier under feature noise. Experiments on synthetic and
over-the-air datasets show consistent gains over supervised FL baselines under
heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.

</details>


### [629] [MemMamba: Rethinking Memory Patterns in State Space Model](https://arxiv.org/abs/2510.03279)
*Youjin Wang,Yangjingyi Chen,Jiahao Yan,Jiaxuan Lu,Xiao Sun*

Main category: cs.LG

TL;DR: Mamba存在长期记忆衰减问题，本文提出MemMamba框架，通过状态摘要和跨层/跨token注意力机制，在保持线性复杂度的同时，缓解了长期遗忘问题，并在长序列基准测试中取得了显著的性能提升和推理效率加速。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理长序列时，Mamba等模型存在长期记忆衰减问题，难以有效保留长距离信息。

Method: 通过数学推导和信息论分析揭示Mamba的记忆衰减机制，提出水平-垂直记忆保真度指标量化信息损失。在此基础上，设计MemMamba框架，引入状态摘要机制以及跨层和跨token注意力机制，以缓解长期遗忘并保持线性复杂度。

Result: MemMamba在PG19和Passkey Retrieval等长序列基准测试中，相比现有Mamba变体和Transformer模型取得了显著的性能提升，并且推理效率提高了48%。

Conclusion: MemMamba在复杂性-记忆权衡方面取得了突破，为超长序列建模提供了一种新范式，有效解决了Mamba的长期记忆衰减问题。

Abstract: With the explosive growth of data, long-sequence modeling has become
increasingly important in tasks such as natural language processing and
bioinformatics. However, existing methods face inherent trade-offs between
efficiency and memory. Recurrent neural networks suffer from gradient vanishing
and explosion, making them hard to scale. Transformers can model global
dependencies but are constrained by quadratic complexity. Recently, selective
state-space models such as Mamba have demonstrated high efficiency with O(n)
time and O(1) recurrent inference, yet their long-range memory decays
exponentially. In this work, we conduct mathematical derivations and
information-theoretic analysis to systematically uncover the memory decay
mechanism of Mamba, answering a fundamental question: what is the nature of
Mamba's long-range memory and how does it retain information? To quantify key
information loss, we further introduce horizontal-vertical memory fidelity
metrics that capture degradation both within and across layers. Inspired by how
humans distill and retain salient information when reading long documents, we
propose MemMamba, a novel architectural framework that integrates state
summarization mechanism together with cross-layer and cross-token attention,
which alleviates long-range forgetting while preserving linear complexity.
MemMamba achieves significant improvements over existing Mamba variants and
Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,
while delivering a 48% speedup in inference efficiency. Both theoretical
analysis and empirical results demonstrate that MemMamba achieves a
breakthrough in the complexity-memory trade-off, offering a new paradigm for
ultra-long sequence modeling.

</details>


### [630] [Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models](https://arxiv.org/abs/2510.03520)
*Kartik Pandit,Sourav Ganguly,Arnesh Banerjee,Shaahin Angizi,Arnob Ghosh*

Main category: cs.LG

TL;DR: CS-RLHF通过引入基于大规模语料库训练的成本模型来解决LLM安全问题，解决了传统CMDP方法对奖励/成本函数敏感和计算成本高的问题，并提供可验证的安全保证。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的安全对齐方法在平衡模型效用和降低危害方面面临挑战，特别是依赖于易受表面关键词影响的评分机制，并且CMDP训练成本高昂且无可证明的安全保证。

Method: 提出CS-RLHF，使用基于大规模语料库训练的成本模型来分配有意义的安全分数，并采用精确惩罚函数理论的修正惩罚方法，无需更新对偶变量即可强制执行安全约束。

Result: CS-RLHF在处理正常和越狱提示时，其LLM模型响应的效率比现有方法提高了至少5倍，并且能够保证安全约束的可行性。

Conclusion: CS-RLHF通过精确惩罚函数和成本模型克服了现有CMDP方法的局限性，为LLM提供了更有效和可验证的安全对齐方案。

Abstract: Ensuring safety is a foundational requirement for large language models
(LLMs). Achieving an appropriate balance between enhancing the utility of model
outputs and mitigating their potential for harm is a complex and persistent
challenge. Contemporary approaches frequently formalize this problem within the
framework of Constrained Markov Decision Processes (CMDPs) and employ
established CMDP optimization techniques. However, these methods exhibit two
notable limitations. First, their reliance on reward and cost functions renders
performance highly sensitive to the underlying scoring mechanism, which must
capture semantic meaning rather than being triggered by superficial keywords.
Second, CMDP-based training entails tuning dual-variable, a process that is
both computationally expensive and does not provide any provable safety
guarantee for a fixed dual variable that can be exploitable through adversarial
jailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF
(CS-RLHF) that introduces a cost model trained on a large-scale corpus to
assign semantically grounded safety scores. In contrast to the lagrangian-based
approach, CS-RLHF adopts a rectified penalty-based formulation. This design
draws on the theory of exact penalty functions in constrained optimization,
wherein constraint satisfaction is enforced directly through a suitably chosen
penalty term. With an appropriately scaled penalty, feasibility of the safety
constraints can be guaranteed at the optimizer, eliminating the need for
dual-variable updates. Empirical evaluation demonstrates that CS-RLHF
outperforms state-of-the-art LLM model responses rendering at-least 5 times
efficient against nominal and jail-breaking prompts

</details>


### [631] [Training Optimal Large Diffusion Language Models](https://arxiv.org/abs/2510.03280)
*Jinjie Ni,Qian Liu,Chao Du,Longxu Dou,Hang Yan,Zili Wang,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: Quokka是第一个针对扩散语言模型（DLMs）的系统性标度律，涵盖了计算和数据约束的范畴，并研究了关键的建模和优化设计。


<details>
  <summary>Details</summary>
Motivation: 本篇论文的动机是提出第一个系统性的扩散语言模型（DLMs）标度律，为DLMs的训练提供实际指导和长期启发。

Method: 通过研究计算和数据约束的范畴，并分析关键的建模和优化设计来建立Quokka标度律。

Result: Quokka为DLMs的训练提供了更广阔的范围，并且其结果有望为DLMs的训练带来短期实践指导和为整个AI社区带来长期启发。

Conclusion: Quokka为DLMs的训练提供了一个系统性的标度律，涵盖了计算和数据约束的范畴，并研究了关键的建模和优化设计，有望为DLMs的训练带来实践指导和长期启发。

Abstract: We introduce Quokka, the first systematic scaling law for diffusion language
models (DLMs), encompassing both compute-constrained and data-constrained
regimes, and studying the key modeling and optimization designs. Quokka is a
good friend of Chinchilla and provides wider scopes. We hope the results would
bring short-term practical guidance in DLMs training and long-term inspirations
for the whole AI community.

</details>


### [632] [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
*Hao Gu,Vibhas Nair,Amrithaa Ashok Kumar,Jayvart Sharma,Ryan Lagasse*

Main category: cs.LG

TL;DR: 提出了一种混合归因和剪枝（HAP）框架，用于在不牺牲模型忠实性的情况下加速语言模型中的电路发现，并在间接宾语识别任务中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型电路发现算法在速度和忠实性之间存在权衡：归因修复速度快但不够忠实，而边剪枝忠实但计算成本高。

Method: HAP框架首先使用归因修复来识别高潜力子图，然后应用边剪枝从中提取忠实电路。

Result: HAP比基线算法快46%，且不牺牲电路忠实性。在间接宾语识别任务中，HAP能够保留归因修复方法在高度稀疏时会裁剪的合作电路组件（例如S抑制头）。

Conclusion: HAP是一种有前景的方法，可以提高机制可解释性研究在更大模型上的可扩展性。

Abstract: Interpreting language models often involves circuit analysis, which aims to
identify sparse subnetworks, or circuits, that accomplish specific tasks.
Existing circuit discovery algorithms face a fundamental trade-off: attribution
patching is fast but unfaithful to the full model, while edge pruning is
faithful but computationally expensive. This research proposes a hybrid
attribution and pruning (HAP) framework that uses attribution patching to
identify a high-potential subgraph, then applies edge pruning to extract a
faithful circuit from it. We show that HAP is 46\% faster than baseline
algorithms without sacrificing circuit faithfulness. Furthermore, we present a
case study on the Indirect Object Identification task, showing that our method
preserves cooperative circuit components (e.g. S-inhibition heads) that
attribution patching methods prune at high sparsity. Our results show that HAP
could be an effective approach for improving the scalability of mechanistic
interpretability research to larger models. Our code is available at
https://anonymous.4open.science/r/HAP-circuit-discovery.

</details>


### [633] [Generalization of Graph Neural Network Models for Distribution Grid Fault Detection](https://arxiv.org/abs/2510.03571)
*Burak Karabulut,Carlo Manna,Chris Develder*

Main category: cs.LG

TL;DR: 该研究系统性地基准测试了不同图神经网络（GNN）架构在考虑电网拓扑变化的电力系统故障诊断中的应用，发现RGATv2在泛化能力方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 确保电网可靠性并防止成本高昂的停电，同时应对电网拓扑结构的变化（如重新配置、设备故障和分布式能源整合）。

Method: 将包括GraphSAGE、GAT和GATv2在内的各种GNN架构集成到RNN+GNN（RGNN）流水线模型中，并与现有的RGCN和纯RNN（特别是GRU）模型进行基准测试，重点考察其泛化能力。

Result: 实验结果表明，RGATv2在IEEE 123节点配电网络上表现出优越的泛化能力，在不同拓扑设置下F1分数仅下降约12%。相比之下，纯RNN模型性能急剧下降（F1分数下降高达60%），其他RGNN变体性能也有显著下降（F1分数降低高达25%）。

Conclusion: RGATv2在考虑电网拓扑变化的情况下，是比现有RGNN和纯RNN模型更优越的电力系统故障诊断方法，尤其在泛化能力方面具有显著优势。

Abstract: Fault detection in power distribution grids is critical for ensuring system
reliability and preventing costly outages. Moreover, fault detection
methodologies should remain robust to evolving grid topologies caused by
factors such as reconfigurations, equipment failures, and Distributed Energy
Resource (DER) integration. Current data-driven state-of-the-art methods use
Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural
Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in
short). Specifically, for power system fault diagnosis, Graph Convolutional
Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures
have been proposed and adopted in domains outside of power systems. In this
paper, we set out to systematically and consistently benchmark various GNN
architectures in an RNN+GNN pipeline model. Specifically, to the best of our
knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention
(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive
benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN
models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring
their generalization potential for deployment in different settings than those
used for training them. Our experimental results on the IEEE 123-node
distribution network show that RGATv2 has superior generalization capabilities,
maintaining high performance with an F1-score reduction of $\sim$12% across
different topology settings. In contrast, pure RNN models largely fail,
experiencing an F1-score reduction of up to $\sim$60%, while other RGNN
variants also exhibit significant performance degradation, i.e., up to
$\sim$25% lower F1-scores.

</details>


### [634] [Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments](https://arxiv.org/abs/2510.03284)
*Vinay Venkatesh,Vamsidhar R Kamanuru,Lav Kumar,Nikita Kothari*

Main category: cs.LG

TL;DR: Edge-FIT是一个在边缘设备上进行联邦指令调优的框架，解决了大型语言模型（LLM）在联邦学习中的通信和计算开销问题，并在物联网和小型模型上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 传统的联邦学习方法在处理大型语言模型时面临参数量过大的挑战，导致通信和计算开销巨大。

Method: Edge-FIT框架结合了联邦学习和4位量化低秩适配（QLoRA）技术，以解决LLM的通信和计算开销问题。实验中，使用IoT领域的数据对Databricks Dolly 15k数据集进行了过滤，并对Llama 2(7B)和Phi-3-mini（3.8B）模型进行了调优。

Result: Edge-FIT调优的Llama 2(7B)模型达到了0.89的F1分数。同时，使用Phi-3-mini模型验证了Edge-FIT的可行性，证明了其在边缘设备上进行LLM部署的可扩展性。

Conclusion: Edge-FIT是一个可扩展的联邦指令调优框架，能够有效解决LLM在边缘设备部署中的通信和计算瓶颈，并在实际应用中展现出良好的性能。

Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a
scalable framework for Federated Instruction Tuning (FIT) of Large Language
Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail
when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT
framework combines federated learning with 4-bit Quantized Low-Rank Adaptation
(QLORA), mitigating the core issues of communication and computational
overhead. We demonstrate this by filtering the general-purpose Databricks Dolly
15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned
Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable
trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable
framework for decentralized LLM deployment on home compute gateways.

</details>


### [635] [Optimising Battery Energy Storage System Trading via Energy Market Operator Price Forecast](https://arxiv.org/abs/2510.03657)
*Aymeric Fabre*

Main category: cs.LG

TL;DR: 该论文研究了如何利用澳大利亚国家电力市场（NEM）的电价预测数据来开发一个能够盈利的电池储能系统（BESS）交易算法。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源和市场去中心化的增加，电网波动性增加，对 BESS 运营商和预测者来说，准确预测价格至关重要。然而，尽管预测数据丰富，但其实际价值和在 BESS 交易决策中的应用仍未得到充分探索。

Method: 该研究通过分析预测准确性在一天中的时间、预测范围和地区差异方面的模式，创建了一个新颖的、受预测信息驱动的 BESS 交易模型，以优化套利收益。该算法的表现与一个不使用预测数据的基本交易算法进行了基准测试。此外，研究还探讨了利用机器学习技术增强 AEMO 预测以制定更高级交易策略的潜力。

Result: 该研究创建了一个新颖的、受预测信息驱动的 BESS 交易模型，并评估了其相对于不使用预测数据的基本交易算法的性能。

Conclusion: 该研究的结果将为未来的能源市场交易模型提供信息，并促进 BESS 更有效地融入市场运营。

Abstract: In electricity markets around the world, the ability to anticipate price
movements with precision can be the difference between profit and loss,
especially for fast-acting assets like battery energy storage systems (BESS).
As grid volatility increases due to renewables and market decentralisation,
operators and forecasters alike face growing pressure to transform prediction
into strategy. Yet while forecast data is abundant, especially in advanced
markets like Australia's National Electricity Market (NEM), its practical value
in driving real-world BESS trading decisions remains largely unexplored. This
thesis dives into that gap. This work addresses a key research question: Can
the accuracy of the Australian Energy Market Operator (AEMO) energy price
forecasts be systematically leveraged to develop a reliable and profitable
battery energy storage system trading algorithm? Despite the availability of
AEMO price forecasts, no existing framework evaluates their reliability or
incorporates them into practical BESS trading strategies. By analysing patterns
in forecast accuracy based on time of day, forecast horizon, and regional
variations, this project creates a novel, forecast-informed BESS trading model
to optimise arbitrage financial returns. The performance of this
forecast-driven algorithm is benchmarked against a basic trading algorithm with
no knowledge of forecast data. The study further explores the potential of
machine learning techniques to predict future energy prices by enhancing AEMO
forecasts to govern a more advanced trading strategy. The research outcomes
will inform future improvements in energy market trading models and promote
more efficient BESS integration into market operations.

</details>


### [636] [HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control](https://arxiv.org/abs/2510.03830)
*Alex Durkin,Jasper Stolte,Mehmet Mercangöz*

Main category: cs.LG

TL;DR: HOFLON是一种混合离线学习与在线优化方法，用于解决化工过程中启动和换挡操作的挑战，通过学习数据流形和长期Q函数来超越人类专家的能力。


<details>
  <summary>Details</summary>
Motivation: 化工过程中的启动和换挡操作至关重要，但高度依赖有经验的操作员，而操作员的退休导致了操作知识的流失。现有的离线强化学习方法在处理数据分布偏移和价值高估问题时存在困难。

Method: HOFLON结合了离线学习和在线优化。离线阶段，它学习一个代表可行操作空间的潜在数据流形和一个能预测累积奖励的长期Q函数。在线阶段，它通过最大化Q函数并惩罚偏离数据流形和操纵变量变化率的优化问题来执行操作。

Result: 在聚合物反应器启动和造纸机换挡的两个工业案例研究中，HOFLON的表现优于隐式Q学习（IQL）等领先算法，并且平均累积奖励超过了历史数据中的最佳操作记录。

Conclusion: HOFLON能够有效地自动化化工过程中的启动和换挡操作，并展现出超越当前专家水平的潜力。

Abstract: Start-ups and product grade-changes are critical steps in continuous-process
plant operation, because any misstep immediately affects product quality and
drives operational losses. These transitions have long relied on manual
operation by a handful of expert operators, but the progressive retirement of
that workforce is leaving plant owners without the tacit know-how needed to
execute them consistently. In the absence of a process model, offline
reinforcement learning (RL) promises to capture and even surpass human
expertise by mining historical start-up and grade-change logs, yet standard
offline RL struggles with distribution shift and value-overestimation whenever
a learned policy ventures outside the data envelope. We introduce HOFLON
(Hybrid Offline Learning + Online Optimization) to overcome those limitations.
Offline, HOFLON learns (i) a latent data manifold that represents the feasible
region spanned by past transitions and (ii) a long-horizon Q-critic that
predicts the cumulative reward from state-action pairs. Online, it solves a
one-step optimization problem that maximizes the Q-critic while penalizing
deviations from the learned manifold and excessive rates of change in the
manipulated variables. We test HOFLON on two industrial case studies: a
polymerization reactor start-up and a paper-machine grade-change problem, and
benchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.
In both plants HOFLON not only surpasses IQL but also delivers, on average,
better cumulative rewards than the best start-up or grade-change observed in
the historical data, demonstrating its potential to automate transition
operations beyond current expert capability.

</details>


### [637] [Why mask diffusion does not work](https://arxiv.org/abs/2510.03289)
*Haocheng Sun,Cynthia Xin Wen,Edward Hong Wang*

Main category: cs.LG

TL;DR: Diffusion language models offer advantages like parallel generation and bidirectional attention over autoregressive models, but mask diffusion models face challenges in achieving these. This paper discusses these difficulties and proposes effective training and inference strategies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inherent difficulties in achieving parallel generation and bidirectional attention in mask diffusion language models, which are drawbacks compared to other diffusion models and autoregressive models.

Method: The paper demonstrates the inherent difficulties of mask diffusion in achieving parallel generation and bidirectional attention and proposes effective training and inference strategies.

Result: The paper demonstrates the limitations of mask diffusion models regarding parallel generation and bidirectional attention.

Conclusion: While mask diffusion models have limitations, the paper proposes effective training and inference strategies to improve their performance.

Abstract: The main advantages of diffusion language models over autoregressive (AR)
models lie in their ability to support parallel generation and bidirectional
attention, enabling a more controllable generation process. In recent years,
open-source mask diffusion language models have emerged, most of which are
based on a variant known as absorbing diffusion. However, this paper
demonstrates why mask diffusion faces inherent difficulties in achieving
parallel generation and bidirectional attention. We also propose the most
effective training and inference strategies for mask diffusion.

</details>


### [638] [Single-Core Superscalar Optimization of Clifford Neural Layers](https://arxiv.org/abs/2510.03290)
*X. Angelo Huang,Ruben Ciranni,Giovanni Spadaccini,Carla J. López Zurita*

Main category: cs.LG

TL;DR: Clifford neural layers provide E(n) and O(n) equivariances, but are computationally intensive. This paper analyzes their structure and proposes optimizations, achieving an average speedup of 21.35x with performance comparable to or better than the original implementation in most cases.


<details>
  <summary>Details</summary>
Motivation: The growing interest in the physical sciences for networks with equivariance properties has led to the development of Clifford neural layers, which provide E(n) and O(n) equivariances. However, these layers can be computationally intensive, necessitating optimization for practical applications.

Method: This paper analyzes the inner workings of Clifford convolutional layers by examining their theoretical foundations in Clifford algebras. This analysis is used to eliminate redundant computations and matrix allocations. Additionally, established optimization techniques are systematically applied to further enhance performance.

Result: The proposed optimizations resulted in an average speedup of 21.35x over the baseline implementation across eleven functions. In six cases, the optimized implementation achieved runtimes comparable to or faster than the original PyTorch implementation. In the remaining cases, performance was within the same order of magnitude as the original library.

Conclusion: The analysis and optimization of Clifford convolutional layers have led to significant speedups in inference, making them a more viable option for applications in the physical sciences that require equivariance properties. The optimized implementation demonstrates competitive or superior performance compared to existing solutions.

Abstract: Within the growing interest in the physical sciences in developing networks
with equivariance properties, Clifford neural layers shine as one approach that
delivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this
paper, we analyze the inner structure of the computation within Clifford
convolutional layers and propose and implement several optimizations to speed
up the inference process while maintaining correctness. In particular, we begin
by analyzing the theoretical foundations of Clifford algebras to eliminate
redundant matrix allocations and computations, then systematically apply
established optimization techniques to enhance performance further. We report a
final average speedup of 21.35x over the baseline implementation of eleven
functions and runtimes comparable to and faster than the original PyTorch
implementation in six cases. In the remaining cases, we achieve performance in
the same order of magnitude as the original library.

</details>


### [639] [UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs](https://arxiv.org/abs/2510.03291)
*Yizhuo Ding,Wanying Qu,Jiawei Geng,Wenqi Shao,Yanwei Fu*

Main category: cs.LG

TL;DR: UniPruning是一个统一的LLM剪枝框架，结合了局部指标的快速性和全局协调的稳定性，无需更新模型权重，支持非结构化和半结构化剪枝。


<details>
  <summary>Details</summary>
Motivation: 现有LLM剪枝方法在效率和鲁棒性之间难以平衡，局部方法在高稀疏度下容易失效，全局方法更新成本高或格式受限。

Method: UniPruning采用基于镜像下降的优化方法，结合快速的逐层评分和轻量级的全局控制器，一次性分配稀疏预算，支持不同稀疏度和硬件约束，无需更新模型权重。

Result: 在多个预训练LLM家族和标准基准测试上，UniPruning在困惑度和零样本准确率方面表现出有竞争力或更优的性能。消融研究证明了镜像下降和局部显著性锚定的重要性。

Conclusion: UniPruning为稀疏化大规模LLM提供了一个高效、原则化且可扩展的解决方案。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks
but face prohibitive computational and memory costs. Pruning offers a promising
path by inducing sparsity while preserving architectural flexibility. However,
existing methods struggle to balance efficiency and robustness: local metric
approaches prune layer by layer but often collapse under high sparsity, whereas
global feedback methods enforce consistency at the cost of expensive weight
updates or restrictive semi-structured formats. We present UniPruning, a
unified post-training pruning framework that combines the speed of local
saliency metrics with the stability of global coordination, enabled by a mirror
descent based optimization, all without updating model weights. UniPruning
leverages fast layer-wise scoring and a lightweight global controller to
allocate a single sparsity budget, supporting both unstructured and
semi-structured N :M pruning within one framework. After a brief calibration,
it can generate pruning masks for arbitrary sparsity levels in one shot, and
adapts seamlessly to hardware-aware constraints. Extensive experiments on
multiple pretrained LLM families and standard benchmarks show that UniPruning
consistently delivers competitive or superior perplexity and zero-shot
accuracy. Ablation studies further highlight the importance of mirror descent
and local saliency anchoring. Overall, UniPruning provides an efficient,
principled, and scalable solution for sparsifying large-scale LLMs. Our code is
available at: https://github.com/RainbowQTT/UniPruning.

</details>


### [640] [Adaptive Federated Learning via Dynamical System Model](https://arxiv.org/abs/2510.04203)
*Aayushya Agarwal,Larry Pileggi,Gauri Joshi*

Main category: cs.LG

TL;DR: 通过将联邦学习建模为动力学系统，并借鉴数值模拟和物理设计的原理，提出了一种端到端的自适应联邦学习方法，能够自适应地选择学习率和动量参数，以实现异构联邦学习的快速稳定收敛，同时无需进行超参数调优。


<details>
  <summary>Details</summary>
Motivation: 超参数选择对于异构联邦学习的稳定和高效收敛至关重要，但手动调整超参数成本高昂且耗时。

Method: 将联邦学习视为一个动力学系统，利用数值模拟和物理设计的原理，自适应地选择客户端和中央服务器的学习率以及动量参数，并用一个全局超参数控制。

Result: 提出了一种自适应动量联邦学习算法，能够动态调整客户端和服务器的学习率，有效处理目标不一致和客户端漂移等异构联邦学习的挑战，并实现了比现有方法更优的收敛性能，且无需进行超参数调优。

Conclusion: 该方法能够快速收敛，并且对全局超参数的选择不敏感，适用于快速原型设计和可扩展部署。

Abstract: Hyperparameter selection is critical for stable and efficient convergence of
heterogeneous federated learning, where clients differ in computational
capabilities, and data distributions are non-IID. Tuning hyperparameters is a
manual and computationally expensive process as the hyperparameter space grows
combinatorially with the number of clients. To address this, we introduce an
end-to-end adaptive federated learning method in which both clients and central
agents adaptively select their local learning rates and momentum parameters.
Our approach models federated learning as a dynamical system, allowing us to
draw on principles from numerical simulation and physical design. Through this
perspective, selecting momentum parameters equates to critically damping the
system for fast, stable convergence, while learning rates for clients and
central servers are adaptively selected to satisfy accuracy properties from
numerical simulation. The result is an adaptive, momentum-based federated
learning algorithm in which the learning rates for clients and servers are
dynamically adjusted and controlled by a single, global hyperparameter. By
designing a fully integrated solution for both adaptive client updates and
central agent aggregation, our method is capable of handling key challenges of
heterogeneous federated learning, including objective inconsistency and client
drift. Importantly, our approach achieves fast convergence while being
insensitive to the choice of the global hyperparameter, making it well-suited
for rapid prototyping and scalable deployment. Compared to state-of-the-art
adaptive methods, our framework is shown to deliver superior convergence for
heterogeneous federated learning while eliminating the need for hyperparameter
tuning both client and server updates.

</details>


### [641] [Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles](https://arxiv.org/abs/2510.03301)
*Arthur Sedek*

Main category: cs.LG

TL;DR: 提出了一种结合XGBoost和神经网络的自适应集成框架，利用元学习、不确定性量化和特征重要性进行模型选择和组合。


<details>
  <summary>Details</summary>
Motivation: 为了开发更智能、更灵活的机器学习系统，需要一种能够动态选择和组合XGBoost和神经网络的模型。

Method: 采用先进的元学习技术，协同整合XGBoost和神经网络，并结合不确定性量化和特征重要性分析来实现动态模型选择和组合。

Result: 实验结果表明，该框架在多个数据集上实现了优越的预测性能和增强的可解释性。

Conclusion: 所提出的自适应集成框架通过XGBoost和神经网络的协同作用，以及元学习、不确定性量化和特征重要性集成，显著提高了预测性能和可解释性。

Abstract: This paper introduces a novel adaptive ensemble framework that
synergistically combines XGBoost and neural networks through sophisticated
meta-learning. The proposed method leverages advanced uncertainty
quantification techniques and feature importance integration to dynamically
orchestrate model selection and combination. Experimental results demonstrate
superior predictive performance and enhanced interpretability across diverse
datasets, contributing to the development of more intelligent and flexible
machine learning systems.

</details>


### [642] [Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models](https://arxiv.org/abs/2510.03302)
*Daiheng Gao,Nanxiang Jiang,Andi Zhang,Shilin Lu,Yufei Tang,Wenbo Zhou,Weiming Zhang,Zhaoxin Fan*

Main category: cs.LG

TL;DR: 概念擦除技术在T2I扩散模型中用于安全和版权考量，但其有效性在下一代模型（如Flux）中下降。本文揭示概念擦除并非真正遗忘，而是通过偏离采样轨迹来产生“失忆”的假象，这种擦除是可逆的。为实现真正的概念移除，本文提出了一种基于强化学习的轨迹优化框架RevAm，它能在不修改模型权重的情况下，通过动态引导去噪过程来复现被擦除的概念。RevAm通过轨迹级别的奖励来探索多样的恢复轨迹，克服了现有方法的局部最优问题。实验证明，RevAm在概念复现保真度方面表现优越，同时计算时间减少了10倍，揭示了当前安全机制的漏洞，并强调了超越轨迹操纵的更鲁棒擦除技术的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的概念擦除技术在T2I扩散模型中有效性下降，其机制并非真正的遗忘，而是偏离采样轨迹，这种擦除是可逆的。这表明需要区分表面安全和真正移除概念，并开发更有效的擦除方法。

Method: 提出了一种名为RevAm的基于强化学习的轨迹优化框架，该框架通过适应Group Relative Policy Optimization (GRPO)来引导去噪过程，以复现被擦除的概念，而不修改模型权重。RevAm使用轨迹级别的奖励来探索恢复轨迹，以克服局部最优。

Result: RevAm在概念复现保真度方面表现优越，并将计算时间缩短了10倍。

Conclusion: RevAm暴露了当前概念擦除方法作为一种“失忆”假象的根本性弱点，并证明了其通过轨迹优化实现真正概念复现的有效性。本文强调了开发超越轨迹操纵的更鲁棒擦除技术的必要性。

Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models
to prevent inappropriate content generation for safety and copyright
considerations. However, as models evolve to next-generation architectures like
Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit
degraded effectiveness, raising questions about their true mechanisms. Through
systematic analysis, we reveal that concept erasure creates only an illusion of
``amnesia": rather than genuine forgetting, these methods bias sampling
trajectories away from target concepts, making the erasure fundamentally
reversible. This insight motivates the need to distinguish superficial safety
from genuine concept removal. In this work, we propose \textbf{RevAm}
(\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization
framework that resurrects erased concepts by dynamically steering the denoising
process without modifying model weights. By adapting Group Relative Policy
Optimization (GRPO) to diffusion models, RevAm explores diverse recovery
trajectories through trajectory-level rewards, overcoming local optima that
limit existing methods. Extensive experiments demonstrate that RevAm achieves
superior concept resurrection fidelity while reducing computational time by
10$\times$, exposing critical vulnerabilities in current safety mechanisms and
underscoring the need for more robust erasure techniques beyond trajectory
manipulation.

</details>


### [643] [Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies](https://arxiv.org/abs/2510.03305)
*Tian Zheng,Subashree Venkatasubramanian,Shuolin Li,Amy Braverman,Xinyi Ke,Zhewen Hou,Peter Jin,Samarth Sanjay Agrawal*

Main category: cs.LG

TL;DR: 本论文分析了气候建模中应用机器学习的案例，重点关注工作流程设计模式，旨在为科学机器学习提供一个严谨的框架。


<details>
  <summary>Details</summary>
Motivation: 气候建模面临物理一致性、多尺度耦合、数据稀疏性、鲁棒泛化和科学工作流集成等挑战，机器学习的应用旨在解决这些问题。

Method: 本文分析了一系列应用机器学习研究在气候建模中的案例，重点关注设计选择和工作流程结构，而不是技术细节。具体包括代理建模、机器学习参数化、概率编程、基于仿真的推理以及物理信息迁移学习等工作流程设计模式。

Result: 通过对不同项目中的工作流程设计模式进行解包，本文阐述了这些工作流程如何以物理知识为基础，以模拟数据为指导，并结合观测数据进行设计。

Conclusion: 本文旨在提供一个框架，通过更透明的模型开发、关键评估、知情适应和可重复性来确保科学机器学习的严谨性，并降低数据科学与气候建模交叉领域进行跨学科合作的门槛。

Abstract: Machine learning has been increasingly applied in climate modeling on system
emulation acceleration, data-driven parameter inference, forecasting, and
knowledge discovery, addressing challenges such as physical consistency,
multi-scale coupling, data sparsity, robust generalization, and integration
with scientific workflows. This paper analyzes a series of case studies from
applied machine learning research in climate modeling, with a focus on design
choices and workflow structure. Rather than reviewing technical details, we aim
to synthesize workflow design patterns across diverse projects in ML-enabled
climate modeling: from surrogate modeling, ML parameterization, probabilistic
programming, to simulation-based inference, and physics-informed transfer
learning. We unpack how these workflows are grounded in physical knowledge,
informed by simulation data, and designed to integrate observations. We aim to
offer a framework for ensuring rigor in scientific machine learning through
more transparent model development, critical evaluation, informed adaptation,
and reproducibility, and to contribute to lowering the barrier for
interdisciplinary collaboration at the interface of data science and climate
modeling.

</details>


### [644] [Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models](https://arxiv.org/abs/2510.04900)
*Nick Janßen,Melanie Schaller,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: 该研究提出了一个基于仿真的评估框架，用于生成合成数据集，以系统地评估多变量长期时间序列预测（M-LTSF）模型的鲁棒性，解决了真实世界数据噪声特性未知的问题。通过对S-Mamba、iTransformer、R-Linear和Autoformer四种模型在不同信号和噪声条件下的表现进行基准测试，揭示了模型在面对不完整周期、特定噪声类型（如趋势噪声、季节性噪声）时的脆弱性，并强调了S-Mamba和iTransformer在频率重建方面的优势，为模型选择提供了指导。


<details>
  <summary>Details</summary>
Motivation: 真实世界多变量长期时间序列预测（M-LTSF）模型鲁棒性评估面临数据噪声特性未知的挑战，需要一个可控的评估框架。

Method: 提出一个基于仿真的评估框架，生成参数化的合成数据集，模拟不同的信号成分、噪声类型、信噪比和频率特性。对S-Mamba、iTransformer、R-Linear和Autoformer四种模型进行基准测试。

Result: 所有模型在观察窗口无法捕捉完整季节性周期时性能都会严重下降。S-Mamba和Autoformer在锯齿波形上表现最佳，R-Linear和iTransformer在正弦信号上表现更好。白噪声和布朗噪声普遍降低性能，S-Mamba对趋势噪声敏感，iTransformer对季节性噪声敏感。S-Mamba和iTransformer在频率重建方面表现更优。

Conclusion: 基于合成数据集的评估框架能深入了解模型在特定信号和噪声条件下的优缺点，为模型选择提供具体指导。

Abstract: Understanding the robustness of deep learning models for multivariate
long-term time series forecasting (M-LTSF) remains challenging, as evaluations
typically rely on real-world datasets with unknown noise properties. We propose
a simulation-based evaluation framework that generates parameterizable
synthetic datasets, where each dataset instance corresponds to a different
configuration of signal components, noise types, signal-to-noise ratios, and
frequency characteristics. These configurable components aim to model
real-world multivariate time series data without the ambiguity of unknown
noise. This framework enables fine-grained, systematic evaluation of M-LTSF
models under controlled and diverse scenarios. We benchmark four representative
architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear
(linear), and Autoformer (decomposition-based). Our analysis reveals that all
models degrade severely when lookback windows cannot capture complete periods
of seasonal patters in the data. S-Mamba and Autoformer perform best on
sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.
White and Brownian noise universally degrade performance with lower
signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer
shows seasonal-noise vulnerability. Further spectral analysis shows that
S-Mamba and iTransformer achieve superior frequency reconstruction. This
controlled approach, based on our synthetic and principle-driven testbed,
offers deeper insights into model-specific strengths and limitations through
the aggregation of MSE scores and provides concrete guidance for model
selection based on signal characteristics and noise conditions.

</details>


### [645] [Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval](https://arxiv.org/abs/2510.03309)
*Mallikarjuna Tupakula*

Main category: cs.LG

TL;DR: 通过对比学习的轻量级方法，在不进行大规模预训练的情况下，实现了化学和文本表示的有效对齐，提高了药物靶点区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态基础模型在药物发现和生物医学领域虽然有前景，但通常需要繁重的预训练或大规模多模态语料库。本研究旨在探索一种更轻量级的方法。

Method: 利用 ChEMBL 数据集，通过训练两个独立的线性投影（一个用于 ECFP4 分子指纹，一个用于生物医学句子嵌入）来实现跨模态对齐。采用了对比学习目标，并结合了难负例加权和边界损失来处理具有相同治疗靶点的药物。

Result: 在基于骨架划分的评估中，证明了该方法能够实现非平凡的跨模态对齐，并显著优于冻结基线模型，提高了靶点内的区分能力。

Conclusion: 轻量级的对比桥接方法是一种计算效率高、可替代大规模多模态预训练的方案，能够实现药物-文本的骨架感知对齐，并在精准医疗中进行靶点检索。

Abstract: Multimodal foundation models hold promise for drug discovery and biomedical
applications, but most existing approaches rely on heavy pretraining or large
scale multimodal corpora. We investigate whether thin contrastive bridges,
lightweight projection heads over frozen unimodal encoders can align chemical
and textual representations without training a full multimodal model. Using
paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with
biomedical sentence embeddings through dual linear projections trained with a
contrastive objective. To better handle drugs sharing the same therapeutic
target, we incorporate hard negative weighting and a margin loss. Evaluation
under scaffold based splits, which require generalization across disjoint
chemical cores, demonstrates that our approach achieves non-trivial cross modal
alignment and substantially improves within target discrimination compared to
frozen baselines. These results suggest that thin bridges offer a compute
efficient alternative to large scale multimodal pretraining, enabling scaffold
aware drug text alignment and target specific retrieval in precision medicine.

</details>


### [646] [Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management](https://arxiv.org/abs/2510.03310)
*Runze Zhang,Xiaowei Zhang,Mingyang Zhao*

Main category: cs.LG

TL;DR: LLMs在运营管理中能复制大部分假设检验结果，但响应分布与人类数据存在差异，可通过链式思考提示和超参数调整来改善。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在运营管理中模拟人类行为的有效性，包括复制假设检验结果和分布匹配度。

Method: 使用9个已发表的行为运营实验，评估LLMs复制假设检验结果和通过Wasserstein距离进行分布匹配的能力，并测试链式思考提示和超参数调整两种干预措施的效果。

Result: LLMs能够复制大部分假设检验结果，捕捉关键决策偏见，但其响应分布与人类数据存在差异，即使是强大的商业模型也是如此。轻量级干预措施可以减少不匹配，有时甚至能让较小或开源的模型表现优于大型模型。

Conclusion: LLMs在模拟人类行为方面有潜力，但需要进一步优化以解决响应分布的差异问题。链式思考提示和超参数调整是有效的改进方法。

Abstract: LLMs are emerging tools for simulating human behavior in business, economics,
and social science, offering a lower-cost complement to laboratory experiments,
field studies, and surveys. This paper evaluates how well LLMs replicate human
behavior in operations management. Using nine published experiments in
behavioral operations, we assess two criteria: replication of hypothesis-test
outcomes and distributional alignment via Wasserstein distance. LLMs reproduce
most hypothesis-level effects, capturing key decision biases, but their
response distributions diverge from human data, including for strong commercial
models. We also test two lightweight interventions -- chain-of-thought
prompting and hyperparameter tuning -- which reduce misalignment and can
sometimes let smaller or open-source models match or surpass larger systems.

</details>


### [647] [Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining](https://arxiv.org/abs/2510.03313)
*Anirudh Subramanyam,Yuxin Chen,Robert L. Grossman*

Main category: cs.LG

TL;DR: 引入数据质量参数Q，提出考虑数据质量的Chinchilla扩展框架，预测模型在模型大小、数据量和数据质量联合作用下的损失。


<details>
  <summary>Details</summary>
Motivation: 以往的语言模型训练缩放定律主要关注模型大小和数据量，未将数据质量纳入缩放定律的考量。本文旨在填补这一空白，提出一个包含数据质量维度的缩放定律。

Method: 引入无量纲数据质量参数Q，提出一个质量感知缩放定律，扩展Chinchilla框架，将损失作为模型大小、数据量和数据质量的联合函数。通过合成实验（神经机器翻译和自回归建模）来控制数据质量，并提出两种Q的估计方法：腐败率代理和缺陷度量。

Result: 损失可预测地随数据质量缩放，更高质量的数据可以显著减小模型规模和计算需求。有效数据随质量呈亚线性衰减，并且模型对中度数据腐败具有鲁棒性。提出的缩放定律在样本外评估中得到验证。

Conclusion: 本文提出了一个明确的、可推广的数据质量缩放定律，为在大型预训练中平衡数据整理工作和模型规模提供了具体指导。

Abstract: Scaling laws for language model training traditionally characterize how
performance scales with model size and dataset volume. Prior work has explored
architecture variants and data treatments such as dataset filtering and noise
injection in language model pretraining; however, these studies have not
formalized data quality within a principled scaling law. We introduce a
dimensionless data-quality parameter Q, and propose a quality-aware scaling law
extending the Chinchilla framework to predict loss as a joint function of model
size, data volume, and data quality. The law is motivated by an
effective-sample-size and information-theoretic view of noisy or redundant
corpora, and it admits two practical estimators for Q: (i) a corruption rate
proxy and (ii) a deficiency measure. Through synthetic experiments in neural
machine translation and autoregressive modeling -- where we systematically
control data quality via multiple levels of noise injection and coverage
variation -- we show that loss scales predictably with data quality and that
higher-quality data can substantially reduce model size and hence compute
requirements. Our results demonstrate a sublinear decay of effective data with
quality and robustness to moderate data corruption; out-of-sample evaluations
further validate the predictive form of the law. Unlike prior empirical
analyses, our work establishes an explicit, generalizable law for data quality,
offering concrete guidance for balancing data curation effort and model scale
in large-scale pretraining.

</details>


### [648] [Fast frequency reconstruction using Deep Learning for event recognition in ring laser data](https://arxiv.org/abs/2510.03325)
*Giuseppe Di Somma,Giorgio Carelli,Angela D. V. Di Virgilio,Francesco Fuso,Enrico Maccioni,Paolo Marsili*

Main category: cs.LG

TL;DR: 提出了一个基于神经网络的方法，可以在几毫秒内从正弦信号中重建频率，并能准确识别物理干扰。


<details>
  <summary>Details</summary>
Motivation: 在许多领域，如环形激光陀螺仪，需要从信号中快速重建频率，而传统方法耗时较长。

Method: 使用神经网络进行频率重建，并引入自动化分类框架识别物理干扰。

Result: 该神经网络方法能在约10毫秒内重建几百赫兹的频率，精度比传统傅里叶方法提高一倍。分类框架对地震事件的识别准确率达到99%-100%。

Conclusion: 该研究将人工智能应用于地球物理应用中的信号分析，提高了频率估计的速度和精度，并能有效识别信号中的物理干扰。

Abstract: The reconstruction of a frequency with minimal delay from a sinusoidal signal
is a common task in several fields; for example Ring Laser Gyroscopes, since
their output signal is a beat frequency. While conventional methods require
several seconds of data, we present a neural network approach capable of
reconstructing frequencies of several hundred Hertz within approximately 10
milliseconds. This enables rapid trigger generation. The method outperforms
standard Fourier-based techniques, improving frequency estimation precision by
a factor of 2 in the operational range of GINGERINO, our Ring Laser
Gyroscope.\\ In addition to fast frequency estimation, we introduce an
automated classification framework to identify physical disturbances in the
signal, such as laser instabilities and seismic events, achieving accuracy
rates between 99\% and 100\% on independent test datasets for the seismic
class. These results mark a step forward in integrating artificial intelligence
into signal analysis for geophysical applications.

</details>


### [649] [Constant in an Ever-Changing World](https://arxiv.org/abs/2510.03330)
*Andy Wu,Chun-Cheng Lin,Yuehua Huang,Rung-Tzuo Liaw*

Main category: cs.LG

TL;DR: CIC框架通过维护一个代表性策略和一个当前策略，并选择性地更新代表性策略，利用自适应调整机制来稳定强化学习的训练过程，在不增加额外计算成本的情况下提高了性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习的训练过程常常出现剧烈振荡，导致不稳定和性能下降。

Method: 提出了一种名为“恒定不变世界”（CIC）的框架，它维护一个代表性策略和一个当前策略。仅当当前策略显示出优越性时，才选择性地更新代表性策略。此外，CIC采用自适应调整机制，使代表性策略和当前策略能够共同促进Critic的训练。

Result: 在五个MuJoCo环境中评估了CIC，结果表明CIC在不产生额外计算成本的情况下，提高了传统算法的性能。

Conclusion: CIC框架通过增强算法稳定性来提高强化学习的性能。

Abstract: The training process of reinforcement learning often suffers from severe
oscillations, leading to instability and degraded performance. In this paper,
we propose a Constant in an Ever-Changing World (CIC) framework that enhances
algorithmic stability to improve performance. CIC maintains both a
representative policy and a current policy. Instead of updating the
representative policy blindly, CIC selectively updates it only when the current
policy demonstrates superiority. Furthermore, CIC employs an adaptive
adjustment mechanism, enabling the representative and current policies to
jointly facilitate critic training. We evaluate CIC on five MuJoCo
environments, and the results show that CIC improves the performance of
conventional algorithms without incurring additional computational cost.

</details>


### [650] [Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment](https://arxiv.org/abs/2510.03335)
*Ameya Daigavane,YuQing Xie,Bodhi P. Vani,Saeed Saremi,Joseph Kleinhenz,Tess Smidt*

Main category: cs.LG

TL;DR: 扩散模型通过学习逆转加噪过程来生成数据，但在处理分子和蛋白质等点云时，由于缺乏规范方向，通常需要进行旋转不变性处理。现有方法通过随机旋转数据并使用Kabsch-Umeyama算法对齐去噪预测与真实值来处理这种对称性，但这种对齐方式的效果尚未得到充分研究。本文将最优去噪器表示为SO(3)上的矩阵Fisher分布，并将对齐视为该分布的模式采样，发现这是小噪声水平下的零阶近似。在此基础上，我们推导了小噪声极限下最优去噪器的改进近似方法，并通过实验证明，在扩散模型训练的关键噪声水平下，对齐通常是一种“足够好”的近似。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在处理点云数据（如分子和蛋白质）时，由于缺乏规范方向，通常会通过随机旋转数据并使用Kabsch-Umeyama算法对齐去噪预测与真实值来处理旋转对称性。然而，这种对齐方式的效果尚未得到充分研究，需要进一步探究其原理和有效性。

Method: 本文将最优去噪器在SO(3)上表示为矩阵Fisher分布，并将对齐视为该分布的模式采样。在小噪声水平下，将对齐视为零阶近似，并推导出最优去噪器在小噪声极限下的改进近似方法。

Result: 实验表明，在扩散模型训练最关心的噪声水平下，对齐通常是一种“足够好”的近似，并且我们提出的改进近似方法在小噪声情况下能取得更好的效果。

Conclusion: 通过将最优去噪器表示为矩阵Fisher分布，本文揭示了旋转对齐在扩散模型训练中的作用，并提出了一种在小噪声情况下更优的去噪器近似方法，同时验证了现有对齐方式在实际应用中的有效性。

Abstract: Diffusion models are a popular class of generative models trained to reverse
a noising process starting from a target data distribution. Training a
diffusion model consists of learning how to denoise noisy samples at different
noise levels. When training diffusion models for point clouds such as molecules
and proteins, there is often no canonical orientation that can be assigned. To
capture this symmetry, the true data samples are often augmented by
transforming them with random rotations sampled uniformly over $SO(3)$. Then,
the denoised predictions are often rotationally aligned via the Kabsch-Umeyama
algorithm to the ground truth samples before computing the loss. However, the
effect of this alignment step has not been well studied. Here, we show that the
optimal denoiser can be expressed in terms of a matrix Fisher distribution over
$SO(3)$. Alignment corresponds to sampling the mode of this distribution, and
turns out to be the zeroth order approximation for small noise levels,
explaining its effectiveness. We build on this perspective to derive better
approximators to the optimal denoiser in the limit of small noise. Our
experiments highlight that alignment is often a `good enough' approximation for
the noise levels that matter most for training diffusion models.

</details>


### [651] [Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models](https://arxiv.org/abs/2510.03339)
*Sofiane Ennadir,Levente Zólyomi,Oleg Smirnov,Tianze Wang,John Pertoft,Filip Cornell,Lele Cao*

Main category: cs.LG

TL;DR: Transformer模型中的池化操作对模型行为有关键影响，但被忽视。本文提出了一个理论框架来分析不同池化方法的表达能力，并通过跨模态的实验验证了理论结果，为模型设计提供了指导。


<details>
  <summary>Details</summary>
Motivation: Transformer模型中的池化操作虽然关键但研究不足，本文旨在深入分析其对模型行为的影响。

Method: 提出一个理论框架，推导出表示能力和区分相似输入的闭式界限，并跨越计算机视觉、自然语言处理和时间序列分析三个主要模态进行实证评估。

Result: 池化选择对准确性、敏感性和优化行为有持续影响，理论分析和实证结果一致。

Conclusion: 池化是Transformer模型中的关键组件，本文的研究为超越注意力机制的原则性模型设计奠定了基础。

Abstract: Transformer models have become the dominant backbone for sequence modeling,
leveraging self-attention to produce contextualized token representations.
These are typically aggregated into fixed-size vectors via pooling operations
for downstream tasks. While much of the literature has focused on attention
mechanisms, the role of pooling remains underexplored despite its critical
impact on model behavior. In this paper, we introduce a theoretical framework
that rigorously characterizes the expressivity of Transformer-based models
equipped with widely used pooling methods by deriving closed-form bounds on
their representational capacity and the ability to distinguish similar inputs.
Our analysis extends to different variations of attention formulations,
demonstrating that these bounds hold across diverse architectural variants. We
empirically evaluate pooling strategies across tasks requiring both global and
local contextual understanding, spanning three major modalities: computer
vision, natural language processing, and time-series analysis. Results reveal
consistent trends in how pooling choices affect accuracy, sensitivity, and
optimization behavior. Our findings unify theoretical and empirical
perspectives, providing practical guidance for selecting or designing pooling
mechanisms suited to specific tasks. This work positions pooling as a key
architectural component in Transformer models and lays the foundation for more
principled model design beyond attention alone.

</details>


### [652] [Learning Pareto-Optimal Pandemic Intervention Policies with MORL](https://arxiv.org/abs/2510.03340)
*Marian Chen,Miri Zilka*

Main category: cs.LG

TL;DR: 该研究提出了一个结合多目标强化学习（MORL）和随机微分方程（SDE）模拟器的框架，用于在疫情控制和社会经济稳定之间取得平衡，并提供了COVID-19、脊髓灰质炎和流感等疾病的干预策略。


<details>
  <summary>Details</summary>
Motivation: 在COVID-19大流行背景下，需要一种能够在疾病控制和社会经济稳定之间取得平衡的干预策略。

Method: 研究人员设计了一个框架，该框架结合了多目标强化学习（MORL）和新的随机微分方程（SDE）大流行模拟器。该模拟器使用全球COVID-19数据进行校准和验证，以模拟国家规模的大流行动态。他们训练了一个Pareto-Conditioned Network（PCN）代理，以说明流行病学控制和经济稳定之间的直接策略权衡，并将该框架扩展到具有不同流行病学特征（如脊髓灰质炎和流感）的病原体，并应用该模型来模拟麻疹疫情。

Result: 该框架能够以比常用的强化学习（RL）方法高几个数量级的保真度再现国家规模的大流行动态。研究人员演示了该框架在COVID-19中的应用，说明了流行病学控制和经济稳定之间的直接策略权衡。此外，研究还表明，该框架可以推广到其他疾病，并发现不同的病原体特征会导致代理发现根本不同的干预策略。在麻疹疫情的案例研究中，研究量化了一个模型，其中疫苗接种覆盖率降低5%需要更严格和更昂贵的干预措施才能遏制疾病传播。

Conclusion: 该研究提供了一个强大且适应性强的框架，支持制定透明的、基于证据的政策，以缓解公共卫生危机。

Abstract: The COVID-19 pandemic underscored a critical need for intervention strategies
that balance disease containment with socioeconomic stability. We approach this
challenge by designing a framework for modeling and evaluating disease-spread
prevention strategies. Our framework leverages multi-objective reinforcement
learning (MORL) - a formulation necessitated by competing objectives - combined
with a new stochastic differential equation (SDE) pandemic simulator,
calibrated and validated against global COVID-19 data. Our simulator reproduces
national-scale pandemic dynamics with orders of magnitude higher fidelity than
other models commonly used in reinforcement learning (RL) approaches to
pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on
this simulator, we illustrate the direct policy trade-offs between
epidemiological control and economic stability for COVID-19. Furthermore, we
demonstrate the framework's generality by extending it to pathogens with
different epidemiological profiles, such as polio and influenza, and show how
these profiles lead the agent to discover fundamentally different intervention
policies. To ground our work in contemporary policymaking challenges, we apply
the model to measles outbreaks, quantifying how a modest 5% drop in vaccination
coverage necessitates significantly more stringent and costly interventions to
curb disease spread. This work provides a robust and adaptable framework to
support transparent, evidence-based policymaking for mitigating public health
crises.

</details>


### [653] [Pilot selection in the era of Virtual reality: algorithms for accurate and interpretable machine learning models](https://arxiv.org/abs/2510.03345)
*Luoma Ke,Guangpeng Zhang,Jibo He,Yajing Li,Yan Li,Xufeng Liu,Peng Fang*

Main category: cs.LG

TL;DR: 该研究提出了一种结合机器学习和虚拟现实技术的新方法，用于在飞行员选拔中区分不同飞行技能的参与者。SVM结合MIC特征选择方法在准确率（0.93）、AUC（0.96）和F1分数（0.93）方面均取得了最佳预测性能，优于其他四种分类算法和两种特征选择方法。MIC方法能够选择具有非线性关系的特征，而非简单的过滤。该研究的VR模拟平台和算法可用于飞行员选拔和培训。


<details>
  <summary>Details</summary>
Motivation: 随着航空业的快速发展，对大量飞行员的需求日益增长，如何经济高效地选拔合适的飞行员成为一个重要研究问题。

Method: 本研究招募了23名来自中国东方航空的飞行员和23名来自清华大学社区的初学者。采用了一种结合机器学习和虚拟现实技术的新方法来区分不同飞行技能的参与者特征。

Result: SVM结合MIC特征选择方法在所有指标上始终 achieving 了最高的预测性能，准确率为0.93，AUC为0.96，F1得分为0.93，优于其他四种分类算法和两种特征选择方法。MIC方法能够选择与采样标签具有非线性关系的特征。

Conclusion: 本研究提出的SVM + MIC算法在飞行员选拔方面优于现有算法，并首次基于眼动追踪和飞行动力学数据进行了实现。该研究的VR模拟平台和算法可用于飞行员选拔和培训。

Abstract: With the rapid growth of the aviation industry, there is a need for a large
number of flight crew. How to select the right pilots in a cost-efficient
manner has become an important research question. In the current study,
twenty-three pilots were recruited from China Eastern Airlines, and 23 novices
were from the community of Tsinghua University. A novel approach incorporating
machine learning and virtual reality technology was applied to distinguish
features between these participants with different flight skills. Results
indicate that SVM with the MIC feature selection method consistently achieved
the highest prediction performance on all metrics with an Accuracy of 0.93, an
AUC of 0.96, and an F1 of 0.93, which outperforms four other classifier
algorithms and two other feature selection methods. From the perspective of
feature selection methods, the MIC method can select features with a nonlinear
relationship to sampling labels, instead of a simple filter-out. Our new
implementation of the SVM + MIC algorithm outperforms all existing pilot
selection algorithms and perhaps provides the first implementation based on eye
tracking and flight dynamics data. This study's VR simulation platforms and
algorithms can be used for pilot selection and training.

</details>


### [654] [AgentCaster: Reasoning-Guided Tornado Forecasting](https://arxiv.org/abs/2510.03349)
*Michael Chen*

Main category: cs.LG

TL;DR: AgentCaster是一个用于评估大型语言模型（LLMs）在复杂、高影响的真实世界任务中作为推理代理的准备情况的框架，通过在龙卷风预报任务中应用多模态LLMs来解决评估差距。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在复杂、高影响、真实世界的任务中作为推理代理的真实准备情况的需求日益增长。

Method: AgentCaster框架使用多模态LLMs端到端地处理龙卷风预报任务，模型解释来自高分辨率对流允许预报档案的异构时空数据。在40天的历史数据中，模型每天查询预报图和探空气球数据，以进行12-36小时的预报。通过几何比较和基于风险的区域来验证概率性龙卷风风险多边形预测。

Result: 在评估中，人类专家明显优于最先进的模型。模型表现出幻觉和过度预测风险强度的倾向，在精确地理定位方面存在困难，并且在复杂的动态演变系统中表现出较差的时空推理能力。

Conclusion: AgentCaster旨在推进在关键领域中用于改进LLM智能体以应对具有挑战性的推理任务的研究。

Abstract: There is a growing need to evaluate Large Language Models (LLMs) on complex,
high-impact, real-world tasks to assess their true readiness as reasoning
agents. To address this gap, we introduce AgentCaster, a contamination-free
framework employing multimodal LLMs end-to-end for the challenging,
long-horizon task of tornado forecasting. Within AgentCaster, models interpret
heterogeneous spatiotemporal data from a high-resolution convection-allowing
forecast archive. We assess model performance over a 40-day period featuring
diverse historical data, spanning several major tornado outbreaks and including
over 500 tornado reports. Each day, models query interactively from a pool of
3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of
12-36 hours. Probabilistic tornado-risk polygon predictions are verified
against ground truths derived from geometric comparisons across disjoint risk
bands in projected coordinate space. To quantify accuracy, we propose
domain-specific TornadoBench and TornadoHallucination metrics, with
TornadoBench highly challenging for both LLMs and domain expert human
forecasters. Notably, human experts significantly outperform state-of-the-art
models, which demonstrate a strong tendency to hallucinate and overpredict risk
intensity, struggle with precise geographic placement, and exhibit poor
spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster
aims to advance research on improving LLM agents for challenging reasoning
tasks in critical domains.

</details>


### [655] [Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks](https://arxiv.org/abs/2510.03351)
*Song Wang,Zhenyu Lei,Zhen Tan,Jundong Li,Javier Rasero,Aiying Zhang,Chirag Agarwal*

Main category: cs.LG

TL;DR: 本研究提出CONCEPTNEURO框架，利用LLM和神经生物学知识生成和编码可解释的脑连接概念，以提高精神疾病诊断的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前近五分之一的青少年患有精神或行为健康问题，迫切需要开发准确且可解释的诊断工具。现有的基于图神经网络（GNN）的方法虽然能提供临床相关的生物标志物，但存在复杂且不透明的问题，限制了其可靠性和临床应用。

Method: CONCEPTNEURO框架利用大型语言模型（LLM）和神经生物学领域知识，自动生成、筛选和编码可解释的功能连接概念。每个概念表示为一个连接特定脑区的结构化子图，然后通过概念分类器进行处理，确保预测基于具有临床意义的连接模式。

Result: 在多个精神疾病数据集上的大量实验表明，经过CONCEPTNEURO增强的GNN在准确性上始终优于标准GNN，同时提供透明且与临床一致的解释。概念分析突出了与专家知识一致且可能激发新研究假设的特定于疾病的连接模式。

Conclusion: CONCEPTNEURO作为一个可解释的、受领域知识启发的框架，能够提高精神疾病诊断的准确性和透明度，为未来的研究提供了新的方向。

Abstract: Nearly one in five adolescents currently live with a diagnosed mental or
behavioral health condition, such as anxiety, depression, or conduct disorder,
underscoring the urgency of developing accurate and interpretable diagnostic
tools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a
powerful lens into large-scale functional connectivity, where brain regions are
modeled as nodes and inter-regional synchrony as edges, offering clinically
relevant biomarkers for psychiatric disorders. While prior works use graph
neural network (GNN) approaches for disorder prediction, they remain complex
black-boxes, limiting their reliability and clinical translation. In this work,
we propose CONCEPTNEURO, a concept-based diagnosis framework that leverages
large language models (LLMs) and neurobiological domain knowledge to
automatically generate, filter, and encode interpretable functional
connectivity concepts. Each concept is represented as a structured subgraph
linking specific brain regions, which are then passed through a concept
classifier. Our design ensures predictions through clinically meaningful
connectivity patterns, enabling both interpretability and strong predictive
performance. Extensive experiments across multiple psychiatric disorder
datasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform
their vanilla counterparts, improving accuracy while providing transparent,
clinically aligned explanations. Furthermore, concept analyses highlight
disorder-specific connectivity patterns that align with expert knowledge and
suggest new hypotheses for future investigation, establishing CONCEPTNEURO as
an interpretable, domain-informed framework for psychiatric disorder diagnosis.

</details>


### [656] [Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility](https://arxiv.org/abs/2510.03358)
*Annan Yu,Danielle C. Maddix,Boran Han,Xiyuan Zhang,Abdul Fatir Ansari,Oleksandr Shchur,Christos Faloutsos,Andrew Gordon Wilson,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: 时间序列Transformer的低秩特性使其具有可压缩性， Chronos模型压缩后推理时间减少65%，内存减少81%。


<details>
  <summary>Details</summary>
Motivation: 文本和视觉Transformer的原理在时间序列模型上应用不完美，需要新的分析视角。

Method: 通过分析Transformer的秩结构，证明时间序列的Q/K/V投影可以进行低秩近似，注意力层可压缩。

Result: 时间序列嵌入具有衰减快的奇异值谱，导致注意力层可压缩。揭示了“流秩”现象，即深度增加导致秩膨胀。 Chronos模型压缩后，推理时间减少65%，内存减少81%，准确率无损失。

Conclusion: 时间序列Transformer的低秩特性为模型压缩提供了理论依据，并指导了模型设计和压缩策略。

Abstract: Transformers are widely used across data modalities, and yet the principles
distilled from text models often transfer imperfectly to models trained to
other modalities. In this paper, we analyze Transformers through the lens of
rank structure. Our focus is on the time series setting, where the structural
properties of the data differ remarkably from those of text or vision. We show
that time-series embeddings, unlike text or vision, exhibit sharply decaying
singular value spectra: small patch sizes and smooth continuous mappings
concentrate the data into low-rank subspaces. From this, we prove that the
associated $Q/K/V$ projections admit accurate low-rank approximations, and that
attention layers become compressible in proportion to the decay of the
embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by
which nonlinear mixing across depth inflates the rank, explaining why early
layers are most amenable to compression and why ranks grow with depth. Guided
by these theoretical and empirical results, we use these insights to compress
Chronos, a large time series foundation model, achieving a reduction of $65\%$
in inference time and $81\%$ in memory, without loss of accuracy. Our findings
provide principled guidance for allocating width, depth, and heads in time
series foundation models, and for exploiting their inherent compressibility.

</details>


### [657] [Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows](https://arxiv.org/abs/2510.03360)
*Zelin Zhao,Zongyi Li,Kimia Hassibi,Kamyar Azizzadenesheli,Junchi Yan,H. Jane Bae,Di Zhou,Anima Anandkumar*

Main category: cs.LG

TL;DR: 通过结合物理信息神经网络（PINO）和预测控制（PC），提出了一种有效的深度强化学习框架（PINO-PC），用于模拟和控制湍流，显著提高了阻力降低效果。


<details>
  <summary>Details</summary>
Motivation: 数值评估壁面摩擦的湍流控制效应因需要昂贵的模拟而具有挑战性。

Method: 利用物理信息神经网络（PINO）联合学习策略和观察者模型，实现基于模型的强化学习（RL）预测控制（PC）。

Result: PINO-PC 在高雷诺数和未见过的流场场景中表现优于现有的无模型强化学习方法，实现了 15,000 雷诺数下 39.0% 的阻力降低，超越了之前的流体控制方法 32% 以上。

Conclusion: 所提出的 PINO-PC 框架能够有效地模拟和控制湍流，并在各种具有挑战性的场景下实现了显著的阻力降低。

Abstract: Assessing turbulence control effects for wall friction numerically is a
significant challenge since it requires expensive simulations of turbulent
fluid dynamics. We instead propose an efficient deep reinforcement learning
(RL) framework for modeling and control of turbulent flows. It is model-based
RL for predictive control (PC), where both the policy and the observer models
for turbulence control are learned jointly using Physics Informed Neural
Operators (PINO), which are discretization invariant and can capture fine
scales in turbulent flows accurately. Our PINO-PC outperforms prior model-free
reinforcement learning methods in various challenging scenarios where the flows
are of high Reynolds numbers and unseen, i.e., not provided during model
training. We find that PINO-PC achieves a drag reduction of 39.0\% under a
bulk-velocity Reynolds number of 15,000, outperforming previous fluid control
methods by more than 32\%.

</details>


### [658] [Estimating link level traffic emissions: enhancing MOVES with open-source data](https://arxiv.org/abs/2510.03362)
*Lijiao Wang,Muhammad Usama,Haris N. Koutsopoulos,Zhengbing He*

Main category: cs.LG

TL;DR: Open-source data and a neural network model are used to estimate vehicle activity and emissions, reducing RMSE by over 50% compared to the MOVES baseline.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable, transparent, and low-cost framework for estimating vehicle activity and emissions in urban regions using open-source data.

Method: Integrate MOVES, OpenStreetMap (OSM) road networks, regional traffic datasets, and satellite imagery to train a neural network model that predicts MOVES operating modes and estimates traffic emissions at the link level. The 'ground truth' was established using OSM GPS trajectories.

Result: The proposed model, applied to 45 municipalities in the Boston Metropolitan area, achieved over 50% reduction in RMSE for CO, NOx, CO2, and PM2.5 emissions compared to the MOVES baseline.

Conclusion: The study demonstrates the feasibility of low-cost, replicable, and data-driven emissions estimation using fully open data sources.

Abstract: Open-source data offers a scalable and transparent foundation for estimating
vehicle activity and emissions in urban regions. In this study, we propose a
data-driven framework that integrates MOVES and open-source GPS trajectory
data, OpenStreetMap (OSM) road networks, regional traffic datasets and
satellite imagery-derived feature vectors to estimate the link level operating
mode distribution and traffic emissions. A neural network model is trained to
predict the distribution of MOVES-defined operating modes using only features
derived from readily available data. The proposed methodology was applied using
open-source data related to 45 municipalities in the Boston Metropolitan area.
The "ground truth" operating mode distribution was established using OSM
open-source GPS trajectories. Compared to the MOVES baseline, the proposed
model reduces RMSE by over 50% for regional scale traffic emissions of key
pollutants including CO, NOx, CO2, and PM2.5. This study demonstrates the
feasibility of low-cost, replicable, and data-driven emissions estimation using
fully open data sources.

</details>


### [659] [A KL-regularization framework for learning to plan with adaptive priors](https://arxiv.org/abs/2510.04280)
*Álvaro Serra-Gomez,Daniel Jarne Ornia,Dhruva Tirumala,Thomas Moerland*

Main category: cs.LG

TL;DR: 模型预测积分（MPPI）规划中的探索是模型学习强化学习（MBRL）中的一个挑战，尤其是对于高维连续控制任务。本研究提出了策略优化-模型预测控制（PO-MPC）框架，该框架通过将规划器的动作分布作为策略优化的先验，统一了现有的MPPI-RL方法，并在策略更新中引入了KL正则化。PO-MPC允许在最大化回报和最小化KL散度之间进行权衡，实验表明其在MPPI-RL方面取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在模型预测积分（MPPI）规划的背景下，模型学习强化学习（MBRL）中的有效探索，特别是在高维连续控制任务中，仍然是一个核心挑战，因为样本效率至关重要。

Method: 本研究提出了策略优化-模型预测控制（PO-MPC），这是一个模型学习强化学习（MBRL）的框架，它将规划器的动作分布整合到策略优化中，并引入了KL正则化。PO-MPC允许策略更新更加灵活，能够在回报最大化和KL散度最小化之间进行权衡。该框架统一了现有的MPPI-RL方法，并探索了新的变体。

Result: 实验表明，PO-MPC框架下的扩展配置在MPPI-RL方面取得了显著的性能提升，推动了该领域的最新进展。

Conclusion: PO-MPC框架通过将规划器的动作分布作为策略优化的先验，并引入KL正则化，有效地解决了MPPI规划中的探索挑战，并实现了性能上的显著提升。

Abstract: Effective exploration remains a central challenge in model-based
reinforcement learning (MBRL), particularly in high-dimensional continuous
control tasks where sample efficiency is crucial. A prominent line of recent
work leverages learned policies as proposal distributions for Model-Predictive
Path Integral (MPPI) planning. Initial approaches update the sampling policy
independently of the planner distribution, typically maximizing a learned value
function with deterministic policy gradient and entropy regularization.
However, because the states encountered during training depend on the MPPI
planner, aligning the sampling policy with the planner improves the accuracy of
value estimation and long-term performance. To this end, recent methods update
the sampling policy by minimizing KL divergence to the planner distribution or
by introducing planner-guided regularization into the policy update. In this
work, we unify these MPPI-based reinforcement learning methods under a single
framework by introducing Policy Optimization-Model Predictive Control (PO-MPC),
a family of KL-regularized MBRL methods that integrate the planner's action
distribution as a prior in policy optimization. By aligning the learned policy
with the planner's behavior, PO-MPC allows more flexibility in the policy
updates to trade off Return maximization and KL divergence minimization. We
clarify how prior approaches emerge as special cases of this family, and we
explore previously unstudied variations. Our experiments show that these
extended configurations yield significant performance improvements, advancing
the state of the art in MPPI-based RL.

</details>


### [660] [Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds](https://arxiv.org/abs/2510.03364)
*Xiaolong Ma,Xu Dong,Ashley Tarrant,Lei Yang,Rao Kotamarthi,Jiali Wang,Feng Yan,Rajkumar Kettimuthu*

Main category: cs.LG

TL;DR: WindSR是一个结合了数据同化和扩散模型的风速超分辨率降尺度方法，提高了风速预测的精度和分辨率。


<details>
  <summary>Details</summary>
Motivation: 为了解决高空风速观测稀疏且在时间空间上不连续，而模拟数据虽然广泛但分辨率低且存在偏差的问题，本研究旨在融合这两类数据，生成高分辨率的空高风速数据。

Method: 提出了一种名为WindSR的新方法，该方法利用数据同化技术和基于扩散模型的超分辨率降尺度技术来处理空高风速。它在降尺度过程中整合了稀疏的观测数据和模拟数据，并引入了一种动态半径混合方法来融合观测数据和模拟数据，为扩散过程提供条件。此外，该方法在训练和推理过程中都考虑了地形信息。

Result: 与基于卷积神经网络和生成对抗网络的基线方法相比，WindSR在降尺度效率和准确性方面均表现更优。数据同化技术使WindSR的偏差相对于独立观测值降低了约20%。

Conclusion: WindSR能够有效地利用稀疏的观测数据和模拟数据，生成高分辨率、更准确的空高风速数据，并且在精度和偏差方面优于现有方法。

Abstract: High-quality observations of hub-height winds are valuable but sparse in
space and time. Simulations are widely available on regular grids but are
generally biased and too coarse to inform wind-farm siting or to assess
extreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully
utilize both data types for generating high-quality, high-resolution hub-height
wind speeds (tens to ~100m above ground), this study introduces WindSR, a
diffusion model with data assimilation for super-resolution downscaling of
hub-height winds. WindSR integrates sparse observational data with simulation
fields during downscaling using state-of-the-art diffusion models. A
dynamic-radius blending method is introduced to merge observations with
simulations, providing conditioning for the diffusion process. Terrain
information is incorporated during both training and inference to account for
its role as a key driver of winds. Evaluated against
convolutional-neural-network and generative-adversarial-network baselines,
WindSR outperforms them in both downscaling efficiency and accuracy. Our data
assimilation reduces WindSR's model bias by approximately 20% relative to
independent observations.

</details>


### [661] [Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis](https://arxiv.org/abs/2510.03366)
*Harshwardhan Fartale,Ashish Kattamuri,Rahul Raja,Arpita Vats,Ishita Prasad,Akshata Kishore Moharir*

Main category: cs.LG

TL;DR: Transformer模型在回忆和推理方面能力不同，可通过激活修复和结构消融区分并验证其分离的电路。


<details>
  <summary>Details</summary>
Motivation: 区分Transformer模型中回忆和推理的能力对于预测模型泛化、设计针对性评估以及构建不干扰其他能力的安全干预措施至关重要。

Method: 通过机制可解释性，使用受控的合成语言谜题数据集，在层、头和神经元级别探查Transformer模型。结合激活修复和结构消融来衡量组件对每种任务类型的贡献。

Result: 在Qwen和LLaMA模型系列中，干预不同的层和注意力头会导致选择性损伤：禁用“回忆电路”会使事实检索准确率降低高达15%，而推理能力保持不变；禁用“推理电路”则会使多步推理能力下降相似幅度。神经元级别观察到任务特定的激活模式，但由于神经元多义性，效果不够稳健。

Conclusion: 因果证据表明，Transformer模型中的回忆和推理依赖于可分离但相互作用的电路。这些发现通过将电路级结构与功能特化联系起来，推进了机制可解释性，并证明了受控数据集和因果干预如何为模型认知提供机制见解，从而为更安全的大型语言模型部署提供信息。

Abstract: Transformer-based language models excel at both recall (retrieving memorized
facts) and reasoning (performing multi-step inference), but whether these
abilities rely on distinct internal mechanisms remains unclear. Distinguishing
recall from reasoning is crucial for predicting model generalization, designing
targeted evaluations, and building safer interventions that affect one ability
without disrupting the other.We approach this question through mechanistic
interpretability, using controlled datasets of synthetic linguistic puzzles to
probe transformer models at the layer, head, and neuron level. Our pipeline
combines activation patching and structured ablations to causally measure
component contributions to each task type. Across two model families (Qwen and
LLaMA), we find that interventions on distinct layers and attention heads lead
to selective impairments: disabling identified "recall circuits" reduces
fact-retrieval accuracy by up to 15\% while leaving reasoning intact, whereas
disabling "reasoning circuits" reduces multi-step inference by a comparable
margin. At the neuron level, we observe task-specific firing patterns, though
these effects are less robust, consistent with neuronal polysemanticity.Our
results provide the first causal evidence that recall and reasoning rely on
separable but interacting circuits in transformer models. These findings
advance mechanistic interpretability by linking circuit-level structure to
functional specialization and demonstrate how controlled datasets and causal
interventions can yield mechanistic insights into model cognition, informing
safer deployment of large language models.

</details>


### [662] [Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation](https://arxiv.org/abs/2510.03375)
*Renrong Shao,Wei Zhang,Jun wang*

Main category: cs.LG

TL;DR: 该研究提出了一种名为CPSC-DFKD的新型无数据知识蒸馏方法，通过条件生成对抗网络合成特定类别的多样化图像，并结合伪监督对比学习来提升学生模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的无数据知识蒸馏方法大多依赖生成器合成图像，但存在无法区分不同类别样本分布、无法优化类别多样性样本等问题。本文旨在解决这些局限性，并探索伪监督学习范式在无数据知识蒸馏中的应用。

Method: 提出了一种名为CPSC-DFKD的新型学习范式，其主要创新包括：1. 使用条件生成对抗网络合成特定类别的多样化图像，用于伪监督学习。2. 改进了生成器模块，使其能够区分不同类别的样本分布。3. 提出了一种基于教师和学生视角的伪监督对比学习方法，以增强数据多样性。

Result: 在三个常用数据集上的综合实验验证了CPSC-DFKD在提升学生模型和生成器性能方面的有效性。

Conclusion: CPSC-DFKD通过引入条件生成对抗网络和伪监督对比学习，有效解决了现有无数据知识蒸馏方法的局限性，显著提升了学生模型的性能。

Abstract: Data-free knowledge distillation~(DFKD) is an effective manner to solve model
compression and transmission restrictions while retaining privacy protection,
which has attracted extensive attention in recent years. Currently, the
majority of existing methods utilize a generator to synthesize images to
support the distillation. Although the current methods have achieved great
success, there are still many issues to be explored. Firstly, the outstanding
performance of supervised learning in deep learning drives us to explore a
pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods
cannot distinguish the distributions of different categories of samples, thus
producing ambiguous samples that may lead to an incorrect evaluation by the
teacher. Besides, current methods cannot optimize the category-wise diversity
samples, which will hinder the student model learning from diverse samples and
further achieving better performance. In this paper, to address the above
limitations, we propose a novel learning paradigm, i.e., conditional
pseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD).
The primary innovations of CPSC-DFKD are: (1) introducing a conditional
generative adversarial network to synthesize category-specific diverse images
for pseudo-supervised learning, (2) improving the modules of the generator to
distinguish the distributions of different categories, and (3) proposing
pseudo-supervised contrastive learning based on teacher and student views to
enhance diversity. Comprehensive experiments on three commonly-used datasets
validate the performance lift of both the student and generator brought by
CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git

</details>


### [663] [A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew](https://arxiv.org/abs/2510.03380)
*Michael Ben Ali,Imen Megdiche,André Peninou,Olivier Teste*

Main category: cs.LG

TL;DR: 本文评估了现有聚类联邦学习(CFL)算法在数据量偏斜(QS)问题下的鲁棒性，并提出了一种名为CORNFLQS的新型迭代CFL算法，该算法通过结合两种操作策略，在准确性和聚类质量方面均优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有的聚类联邦学习(CFL)方法在处理数据量偏斜(Quantity Skew, QS)这一非独立同分布(Non-IID)问题时缺乏系统的评估，尽管QS带来了显著挑战。

Method: 本文首先评估了现有CFL算法在多种QS场景下的表现。其次，提出了一种名为CORNFLQS的新型迭代CFL算法，该算法结合了最小化本地训练损失和基于本地模型相似性进行聚类的两种策略。

Result: 在六个图像分类数据集上进行的大量实验（涵盖270种Non-IID配置）表明，CORNFLQS在准确性和聚类质量方面均取得了最高的平均排名，并对QS扰动表现出很强的鲁棒性。

Conclusion: 本文提出的CORNFLQS算法在处理QS问题时比现有的CFL算法更优越，能够实现更好的准确性和聚类质量，并保持良好的鲁棒性。

Abstract: Federated Learning (FL) is a decentralized paradigm that enables a
client-server architecture to collaboratively train a global Artificial
Intelligence model without sharing raw data, thereby preserving privacy. A key
challenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of
Non-IID, where clients hold highly heterogeneous data volumes. Clustered
Federated Learning (CFL) is an emergent variant of FL that presents a promising
solution to Non-IID problem. It improves models' performance by grouping
clients with similar data distributions into clusters. CFL methods generally
fall into two operating strategies. In the first strategy, clients select the
cluster that minimizes the local training loss. In the second strategy, the
server groups clients based on local model similarities. However, most CFL
methods lack systematic evaluation under QS but present significant challenges
because of it. In this paper, we present two main contributions. The first one
is an evaluation of state-of-the-art CFL algorithms under various Non-IID
settings, applying multiple QS scenarios to assess their robustness. Our second
contribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes
an optimal coordination between both operating strategies of CFL. Our approach
is robust against the different variations of QS settings. We conducted
intensive experiments on six image classification datasets, resulting in 270
Non-IID configurations. The results show that CORNFLQS achieves the highest
average ranking in both accuracy and clustering quality, as well as strong
robustness to QS perturbations. Overall, our approach outperforms actual CFL
algorithms.

</details>


### [664] [Cross-Modal Reconstruction Pretraining for Ramp Flow Prediction at Highway Interchanges](https://arxiv.org/abs/2510.03381)
*Yongchao Li,Jun Chen,Zhuoxuan Li,Chao Gao,Yang Li,Chu Zhang,Changyin Dong*

Main category: cs.LG

TL;DR: STDAE是一个两阶段框架，利用跨模态重建预训练来解决高速公路收费站缺乏实时检测器的问题，从而提高了交通预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 高速公路收费站是车辆在公路之间换乘的关键节点，但缺乏实时匝道检测器会导致交通预测出现盲点。

Method: 提出一个时空解耦自编码器（STDAE），一个两阶段框架，利用跨模态重建预训练。在第一阶段，STDAE从主线数据中重建历史匝道流量，迫使模型捕捉固有的时空关系。其解耦架构具有并行空间和时间自编码器，可有效提取异构特征。在预测阶段，将学习到的表示与GWNet等模型集成以提高准确性。

Result: 在三个真实世界的收费站数据集上的实验表明，STDAE-GWNET的性能始终优于13个最先进的基线模型，并且其性能与使用历史匝道数据的模型相当。

Conclusion: STDAE在克服检测器稀疏性方面是有效的，并且具有用于各种预测流程的即插即用潜力。

Abstract: Interchanges are crucial nodes for vehicle transfers between highways, yet
the lack of real-time ramp detectors creates blind spots in traffic prediction.
To address this, we propose a Spatio-Temporal Decoupled Autoencoder (STDAE), a
two-stage framework that leverages cross-modal reconstruction pretraining. In
the first stage, STDAE reconstructs historical ramp flows from mainline data,
forcing the model to capture intrinsic spatio-temporal relations. Its decoupled
architecture with parallel spatial and temporal autoencoders efficiently
extracts heterogeneous features. In the prediction stage, the learned
representations are integrated with models such as GWNet to enhance accuracy.
Experiments on three real-world interchange datasets show that STDAE-GWNET
consistently outperforms thirteen state-of-the-art baselines and achieves
performance comparable to models using historical ramp data. This demonstrates
its effectiveness in overcoming detector scarcity and its plug-and-play
potential for diverse forecasting pipelines.

</details>


### [665] [Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning](https://arxiv.org/abs/2510.03394)
*Donghwan Rho*

Main category: cs.LG

TL;DR: 本文研究使用可验证奖励的强化学习（RLVR）来训练具有更强推理能力的语言模型，并将其应用于韩国文字链游戏。


<details>
  <summary>Details</summary>
Motivation: RLVR在训练LLM和解决逻辑谜题方面显示出潜力，本文旨在探索其在韩国文字链游戏中的应用。

Method: 在韩国文字链游戏中应用RLVR，并提出了一种课程学习方案来处理规则奖励冲突。

Result: 实验表明，规则奖励可能冲突，但课程学习方案可以缓解这些冲突。

Conclusion: RLVR可用于韩国文字链游戏，课程学习方案能有效处理奖励冲突，并鼓励对其他语言的谜题任务进行进一步研究。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training large language models (LLMs) with stronger reasoning abilities. It
has also been applied to a variety of logic puzzles. In this work, we study the
Korean word-chain game using RLVR. We show that rule-derived rewards can
naturally conflict, and demonstrate through experiments that a
curriculum-learning scheme mitigates these conflicts. Our findings motivate
further studies of puzzle tasks in diverse languages.

</details>


### [666] [Multi-task neural diffusion processes for uncertainty-quantified wind power prediction](https://arxiv.org/abs/2510.03419)
*Joseph Rawson,Domniki Ladopoulou,Petros Dellaportas*

Main category: cs.LG

TL;DR: 神经扩散过程（NDPs）被应用于风电预测，并通过多任务（MT-NDP）框架进行扩展，以提高准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 风电预测的准确性对电网集成和风电场的可靠运行至关重要，但现有模型在处理风电场数据时存在挑战。

Method: 提出了一种多任务神经扩散过程（MT-NDP）框架，该框架引入任务编码器来捕捉跨风力涡轮机的相关性，并实现对未见风力涡轮机的少样本适应。这是首次在实际 SCADA 数据上对 NDP 进行的经验评估。

Result: MT-NDP 框架在预测精度和校准方面优于单任务 NDP 和高斯过程（GPs），尤其是在处理偏离平均值的风力涡轮机时。NDP 模型能够提供校准良好且可扩展的预测。

Conclusion: NDP-based 模型能够为现代风电场提供可靠、准确且可扩展的预测，其预测区间能够支持调度和维护决策。

Abstract: Uncertainty-aware wind power prediction is essential for grid integration and
reliable wind farm operation. We apply neural diffusion processes (NDPs)-a
recent class of models that learn distributions over functions-and extend them
to a multi-task NDP (MT-NDP) framework for wind power prediction. We provide
the first empirical evaluation of NDPs in real supervisory control and data
acquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture
cross-turbine correlations and enable few-shot adaptation to unseen turbines.
The proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of
point accuracy and calibration, particularly for wind turbines whose behaviour
deviates from the fleet average. In general, NDP-based models deliver
calibrated and scalable predictions suitable for operational deployment,
offering sharper, yet trustworthy, predictive intervals that can support
dispatch and maintenance decisions in modern wind farms.

</details>


### [667] [Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices](https://arxiv.org/abs/2510.03425)
*Congzheng Song,Xinyu Tang*

Main category: cs.LG

TL;DR: LoRA等基于反向传播的模型微调方法内存消耗大，不适用于资源受限的移动设备。零阶优化（ZO）虽然内存占用小，但收敛速度慢。我们提出了一种名为MeBP的内存高效反向传播实现方法，在内存占用和计算时间之间取得了更好的平衡，收敛速度更快，性能优于ZO方法。我们在iPhone 15 Pro Max上验证了MeBP的有效性，可在不到1GB内存下微调0.5B至4B参数的LLM。


<details>
  <summary>Details</summary>
Motivation: 移动设备资源有限，难以支持传统的反向传播微调方法（如LoRA）。零阶优化（ZO）虽然内存占用低，但收敛效率极低。

Method: 提出一种内存高效的反向传播（MeBP）实现方法，优化了移动设备上的内存使用和计算时间。

Result: 在iPhone 15 Pro Max上，MeBP可以在不到1GB的内存下微调0.5B至4B参数的LLM，其收敛速度和性能优于ZO方法。

Conclusion: MeBP在内存占用和计算时间之间取得了更好的权衡，为在资源受限的移动设备上微调LLM提供了一种有效且高效的解决方案。

Abstract: Fine-tuning large language models (LLMs) with backpropagation\textemdash even
for a subset of parameters such as LoRA\textemdash can be much more
memory-consuming than inference and is often deemed impractical for
resource-constrained mobile devices. Alternative methods, such as zeroth-order
optimization (ZO), can greatly reduce the memory footprint but come at the cost
of significantly slower model convergence (10$\times$ to 100$\times$ more steps
than backpropagation). We propose a memory-efficient implementation of
backpropagation (MeBP) on mobile devices that provides better trade-off between
memory usage and compute time, while converging faster and achieving better
performance than the ZO baseline. We verify the effectiveness of MeBP on an
iPhone 15 Pro Max and show that various LLMs, ranging from 0.5B to 4B
parameters, can be fine-tuned using less than 1GB of memory. We release an
example of the MeBP implementation at https://github.com/apple/ml-mebp.

</details>


### [668] [LHGEL: Large Heterogeneous Graph Ensemble Learning using Batch View Aggregation](https://arxiv.org/abs/2510.03432)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: LHGEL是一个用于处理大规模异构图的集成学习框架，通过批次采样、残差注意力与多样性正则化，提升了模型在异构图学习任务中的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 处理大规模异构图中存在的节点/边类型异构性、节点特征变化以及复杂的局部邻域结构等挑战。

Method: 提出LHGEL框架，包含三个关键组件：批次视图聚合（采样子图并形成多个图视图）、残差注意力（自适应加权视图贡献）和多样性正则化（鼓励不同视图间的表示差异）。

Result: 在五个真实异构网络数据集上进行验证，LHGEL在准确性和鲁棒性方面持续优于现有最先进的方法。

Conclusion: LHGEL通过批次采样、残差注意力和多样性正则化有效解决了大规模异构图学习中的挑战，并在实验中取得了显著的性能提升。残差注意力机制能够缓解集成学习中常见的梯度消失问题。

Abstract: Learning from large heterogeneous graphs presents significant challenges due
to the scale of networks, heterogeneity in node and edge types, variations in
nodal features, and complex local neighborhood structures. This paper advocates
for ensemble learning as a natural solution to this problem, whereby training
multiple graph learners under distinct sampling conditions, the ensemble
inherently captures different aspects of graph heterogeneity. Yet, the crux
lies in combining these learners to meet global optimization objective while
maintaining computational efficiency on large-scale graphs. In response, we
propose LHGEL, an ensemble framework that addresses these challenges through
batch sampling with three key components, namely batch view aggregation,
residual attention, and diversity regularization. Specifically, batch view
aggregation samples subgraphs and forms multiple graph views, while residual
attention adaptively weights the contributions of these views to guide node
embeddings toward informative subgraphs, thereby improving the accuracy of base
learners. Diversity regularization encourages representational disparity across
embedding matrices derived from different views, promoting model diversity and
ensemble robustness. Our theoretical study demonstrates that residual attention
mitigates gradient vanishing issues commonly faced in ensemble learning.
Empirical results on five real heterogeneous networks validate that our LHGEL
approach consistently outperforms its state-of-the-art competitors by
substantial margin. Codes and datasets are available at
https://github.com/Chrisshen12/LHGEL.

</details>


### [669] [Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation](https://arxiv.org/abs/2510.03437)
*Jairo Diaz-Rodriguez,Mumin Jia*

Main category: cs.LG

TL;DR: 现有的核改变点检测（KCPD）理论在独立性假设下被广泛使用，但真实世界的序列数据（如文本）具有很强的依赖性。本文在 m-依赖数据下建立了新的 KCPD 保证，证明了其在检测到的改变点数量上的准确性和位置上的弱一致性。通过基于 LLM 的模拟和在各种文本数据集上的全面实证研究，验证了 KCPD 在文本分割任务中的理论可靠性和实际有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有核改变点检测（KCPD）理论在处理文本等强依赖序列数据时的局限性，并验证 KCPD 在文本分割任务中的有效性。

Method: 在 m-依赖数据下建立 KCPD 的理论保证，并进行基于 LLM 的模拟和实证研究，使用现代文本嵌入来评估 KCPD 在文本分割任务中的性能。

Result: 在 m-依赖数据下，KCPD 在检测到的改变点数量上是一致的，在位置上是弱一致的。通过模拟和实证研究，KCPD 在文本分割任务中表现优于基线方法，并在 Taylor Swift 的推文中得到了实际应用案例的验证。

Conclusion: KCPD 在 m-依赖数据下具有可靠的理论保证，并且在文本分割任务中具有实际的有效性，能够与现代文本嵌入结合使用。

Abstract: Kernel change-point detection (KCPD) has become a widely used tool for
identifying structural changes in complex data. While existing theory
establishes consistency under independence assumptions, real-world sequential
data such as text exhibits strong dependencies. We establish new guarantees for
KCPD under $m$-dependent data: specifically, we prove consistency in the number
of detected change points and weak consistency in their locations under mild
additional assumptions. We perform an LLM-based simulation that generates
synthetic $m$-dependent text to validate the asymptotics. To complement these
results, we present the first comprehensive empirical study of KCPD for text
segmentation with modern embeddings. Across diverse text datasets, KCPD with
text embeddings outperforms baselines in standard text segmentation metrics. We
demonstrate through a case study on Taylor Swift's tweets that KCPD not only
provides strong theoretical and simulated reliability but also practical
effectiveness for text segmentation tasks.

</details>


### [670] [On residual network depth](https://arxiv.org/abs/2510.03470)
*Benoit Dherin,Michael Munn*

Main category: cs.LG

TL;DR: 深度残差网络（如ResNet和Transformer）的有效性可以通过其作为浅层模型集成来解释，增加网络深度等同于扩大集成规模，并揭示了分层集成结构。


<details>
  <summary>Details</summary>
Motivation: 深度残差网络（如ResNet和Transformer）为何有效仍然是一个悬而未决的问题，需要对其深度有效性进行正式的理解。

Method: 通过推导一个显式的解析公式来证明增加网络深度等同于扩展隐式集成的大小，并揭示了分层集成结构。

Result: 通过‘残差展开定理’，证明了增加网络深度等同于扩展隐式集成的大小，揭示了分层集成结构，并解释了归一化层在训练深度模型中的必要性。

Conclusion: 残差网络的深度有效性可以通过其作为浅层模型集成来解释，增加深度等同于扩大集成规模。‘残差展开定理’提供了一种通过缩放每个残差模块来控制组合爆炸和模型复杂性的原则性方法。

Abstract: Deep residual architectures, such as ResNet and the Transformer, have enabled
models of unprecedented depth, yet a formal understanding of why depth is so
effective remains an open question. A popular intuition, following Veit et al.
(2016), is that these residual networks behave like ensembles of many shallower
models. Our key finding is an explicit analytical formula that verifies this
ensemble perspective, proving that increasing network depth is mathematically
equivalent to expanding the size of this implicit ensemble. Furthermore, our
expansion reveals a hierarchical ensemble structure in which the combinatorial
growth of computation paths leads to an explosion in the output signal,
explaining the historical necessity of normalization layers in training deep
models. This insight offers a first principles explanation for the historical
dependence on normalization layers and sheds new light on a family of
successful normalization-free techniques like SkipInit and Fixup. However,
while these previous approaches infer scaling factors through optimizer
analysis or a heuristic analogy to Batch Normalization, our work offers the
first explanation derived directly from the network's inherent functional
structure. Specifically, our Residual Expansion Theorem reveals that scaling
each residual module provides a principled solution to taming the combinatorial
explosion inherent to these architectures. We further show that this scaling
acts as a capacity controls that also implicitly regularizes the model's
complexity.

</details>


### [671] [How to Set $β_1, β_2$ in Adam: An Online Learning Perspective](https://arxiv.org/abs/2510.03478)
*Quan Nguyen*

Main category: cs.LG

TL;DR: Adam优化器的动量因子设置的理论分析，提出了更通用的分析方法，并证明了其界限的紧密性。


<details>
  <summary>Details</summary>
Motivation: 目前关于Adam优化器最优动量因子设置的理论理解尚不完全，现有分析限制在β1 = sqrt(β2) 的情况下。

Method: 推导了适用于β1 ≥ sqrt(β2) 和 β1 ≤ sqrt(β2) 的更通用分析方法，并进行了最坏情况下的紧密性证明。

Result: 得出了严格泛化现有界限的分析结果，并证明了界限的紧密性。证明了在非强制性对手的情况下，设置β1 = sqrt(β2)并非最优。

Conclusion: 提出了Adam优化器动量因子设置的更全面理论分析，为实际应用提供了指导。

Abstract: While Adam is one of the most effective optimizer for training large-scale
machine learning models, a theoretical understanding of how to optimally set
its momentum factors, $\beta_1$ and $\beta_2$, remains largely incomplete.
  Prior works have shown that Adam can be seen as an instance of
Follow-the-Regularized-Leader (FTRL), one of the most important class of
algorithms in online learning.
  The prior analyses in these works required setting $\beta_1 =
\sqrt{\beta_2}$, which does not cover the more practical cases with $\beta_1
\neq \sqrt{\beta_2}$.
  We derive novel, more general analyses that hold for both $\beta_1 \geq
\sqrt{\beta_2}$ and $\beta_1 \leq \sqrt{\beta_2}$.
  In both cases, our results strictly generalize the existing bounds.
  Furthermore, we show that our bounds are tight in the worst case.
  We also prove that setting $\beta_1 = \sqrt{\beta_2}$ is optimal for an
oblivious adversary, but sub-optimal for an non-oblivious adversary.

</details>


### [672] [Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains](https://arxiv.org/abs/2510.03486)
*Anupam Panwar,Himadri Pal,Jiali Chen,Kyle Cho,Riddick Jiang,Miao Zhao,Rajiv Krishnamurthy*

Main category: cs.LG

TL;DR: 该论文提出了一个名为RADF的统一框架，用于解决分布式系统中大规模、异构时间序列数据的实时异常检测和根本原因分析问题。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中的异常检测面临数据量大、时间序列异构性强以及根本原因难以确定等挑战，现有方法通常需要手动调优且效果不佳。

Method: RADF框架采用了一种名为mSelect的新技术，该技术能够自动化算法选择和超参数调优，以适应不同的应用场景，并提供异常检测后的根本原因分析能力。

Result: 实验结果表明，RADF框架在9个公开基准数据集中的5个上，其AUC性能优于最先进的异常检测模型。此外，RADF在7个数据集上实现了超过0.85的AUC，这一表现优于其他所有最先进模型。

Conclusion: RADF框架通过mSelect技术有效解决了大规模、异构时间序列数据的实时异常检测和根本原因分析的挑战，并在性能上超越了现有最先进的模型。

Abstract: Detecting anomalies in large, distributed systems presents several
challenges. The first challenge arises from the sheer volume of data that needs
to be processed. Flagging anomalies in a high-throughput environment calls for
a careful consideration of both algorithm and system design. The second
challenge comes from the heterogeneity of time-series datasets that leverage
such a system in production. In practice, anomaly detection systems are rarely
deployed for a single use case. Typically, there are several metrics to
monitor, often across several domains (e.g. engineering, business and
operations). A one-size-fits-all approach rarely works, so these systems need
to be fine-tuned for every application - this is often done manually. The third
challenge comes from the fact that determining the root-cause of anomalies in
such settings is akin to finding a needle in a haystack. Identifying (in real
time) a time-series dataset that is associated causally with the anomalous
time-series data is a very difficult problem. In this paper, we describe a
unified framework that addresses these challenges. Reasoning based Anomaly
Detection Framework (RADF) is designed to perform real time anomaly detection
on very large datasets. This framework employs a novel technique (mSelect) that
automates the process of algorithm selection and hyper-parameter tuning for
each use case. Finally, it incorporates a post-detection capability that allows
for faster triaging and root-cause determination. Our extensive experiments
demonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly
detection models in AUC performance for 5 out of 9 public benchmarking
datasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a
distinction unmatched by any other state-of-the-art model.

</details>


### [673] [Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^π$-Realizability and Concentrability](https://arxiv.org/abs/2510.03494)
*Volodymyr Tkachuk,Csaba Szepesvári,Xiaoqi Tan*

Main category: cs.LG

TL;DR: 该研究提出了一个有限时间范围的离线强化学习（RL）的统计高效学习算法，仅需函数逼近、轨迹数据和qπ-可实现性假设。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，在仅有数据覆盖性和qπ-可实现性的假设下，有限时间范围的离线RL在策略评估和策略优化上无法实现统计高效学习。Tkachuk et al. (2024) 提出，如果数据以轨迹形式提供，则策略优化可以实现统计高效学习。本研究旨在解决在相同假设下策略评估的统计高效学习问题。

Method: 提出一个统计高效的策略评估学习算法，并对Tkachuk et al. (2024)的策略优化学习算法进行了更紧密的分析，以提高其样本复杂度。

Result: 提出了一种统计高效的策略评估学习算法。通过更紧密的分析，改进了Tkachuk et al. (2024) 策略优化学习算法的样本复杂度。

Conclusion: 在函数逼近、轨迹数据和qπ-可实现性的假设下，有限时间范围的离线RL中的策略评估和策略优化都可以实现统计高效学习。

Abstract: We study finite-horizon offline reinforcement learning (RL) with function
approximation for both policy evaluation and policy optimization. Prior work
established that statistically efficient learning is impossible for either of
these problems when the only assumptions are that the data has good coverage
(concentrability) and the state-action value function of every policy is
linearly realizable ($q^\pi$-realizability) (Foster et al., 2021). Recently,
Tkachuk et al. (2024) gave a statistically efficient learner for policy
optimization, if in addition the data is assumed to be given as trajectories.
In this work we present a statistically efficient learner for policy evaluation
under the same assumptions. Further, we show that the sample complexity of the
learner used by Tkachuk et al. (2024) for policy optimization can be improved
by a tighter analysis.

</details>


### [674] [D2 Actor Critic: Diffusion Actor Meets Distributional Critic](https://arxiv.org/abs/2510.03508)
*Lunjun Zhang,Shuo Han,Hanrui Lyu,Bradly C Stadie*

Main category: cs.LG

TL;DR: D2AC是一种新的无模型强化学习（RL）算法，用于在线有效地训练扩散策略。它通过一个避免了策略梯度高方差和反向传播时间复杂性的策略改进目标来实现稳定学习，并结合了分布RL和裁剪双Q学习的鲁棒分布 crítico。


<details>
  <summary>Details</summary>
Motivation: 介绍一种新的无模型强化学习（RL）算法D2AC，用于在线有效地训练扩散策略。

Method: 使用避免了策略梯度高方差和反向传播时间复杂性的策略改进目标，并结合了分布RL和裁剪双Q学习的鲁棒分布 crítico。

Result: 在包括Humanoid、Dog和Shadow Hand在内的18个硬RL任务基准上取得了最先进的性能，涵盖了密集奖励和目标条件RL场景。

Conclusion: D2AC算法在硬RL任务中表现出高度有效性，并在生物学启发的捕食者-猎物任务中展现了行为鲁棒性和泛化能力。

Abstract: We introduce D2AC, a new model-free reinforcement learning (RL) algorithm
designed to train expressive diffusion policies online effectively. At its core
is a policy improvement objective that avoids the high variance of typical
policy gradients and the complexity of backpropagation through time. This
stable learning process is critically enabled by our second contribution: a
robust distributional critic, which we design through a fusion of
distributional RL and clipped double Q-learning. The resulting algorithm is
highly effective, achieving state-of-the-art performance on a benchmark of
eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains,
spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard
benchmarks, we also evaluate a biologically motivated predator-prey task to
examine the behavioral robustness and generalization capacity of our approach.

</details>


### [675] [Task-Level Contrastiveness for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2510.03509)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: 提出一种名为“任务级对比性”的新方法，用于提高少样本分类和元学习在不同领域之间的泛化能力，该方法通过无监督聚类任务表示来解决现有方法的局限性，具有计算效率高、易于集成等优点，并在MetaDataset基准测试中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本分类和元学习方法在跨领域泛化能力方面存在不足，通常准确率低、计算成本高，并依赖于严格的假设。

Method: 引入“任务级对比性”，通过定义任务增强和任务级对比损失来实现无监督的类别表示聚类。

Result: 在MetaDataset基准测试中，该方法在不增加额外复杂性的情况下，显著提高了泛化能力和计算效率，取得了优于现有方法的性能。

Conclusion: 任务级对比性是一种有效且轻量级的方法，能够显著改善少样本学习在跨领域泛化方面的表现，且易于集成到现有算法中。

Abstract: Few-shot classification and meta-learning methods typically struggle to
generalize across diverse domains, as most approaches focus on a single
dataset, failing to transfer knowledge across various seen and unseen domains.
Existing solutions often suffer from low accuracy, high computational costs,
and rely on restrictive assumptions. In this paper, we introduce the notion of
task-level contrastiveness, a novel approach designed to address issues of
existing methods. We start by introducing simple ways to define task
augmentations, and thereafter define a task-level contrastive loss that
encourages unsupervised clustering of task representations. Our method is
lightweight and can be easily integrated within existing few-shot/meta-learning
algorithms while providing significant benefits. Crucially, it leads to
improved generalization and computational efficiency without requiring prior
knowledge of task domains. We demonstrate the effectiveness of our approach
through different experiments on the MetaDataset benchmark, where it achieves
superior performance without additional complexity.

</details>


### [676] [RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models](https://arxiv.org/abs/2510.03515)
*Lianghuan Huang,Sagnik Anupam,Insup Lee,Shuo Li,Osbert Bastani*

Main category: cs.LG

TL;DR: RL算法RAPID通过批处理推理和使用组优势估计进行策略梯度更新，可在不牺牲准确性的情况下，将RL训练时间缩短11%-34%。


<details>
  <summary>Details</summary>
Motivation: RL算法在微调小型语言模型以解决特定任务（如数学和编程）方面很有前景，但训练过程耗时且需要大量资源。

Method: RAPID算法通过批处理执行推理，然后进行小批量离策略策略梯度更新。它结合了组优势估计，并使用重要性加权估计器来校正离策略学习产生的偏差。

Result: 与最先进的RL算法相比，RAPID在三个基准测试上将运行时间减少了11%-34%，同时保持了相似或更好的准确性。

Conclusion: RAPID通过优化计算资源利用率，有效缩短了RL的训练时间，为微调小型语言模型提供了一种更高效的方法。

Abstract: Reinforcement learning (RL) has emerged as a promising strategy for
finetuning small language models (SLMs) to solve targeted tasks such as math
and coding. However, RL algorithms tend to be resource-intensive, taking a
significant amount of time to train. We propose RAPID, a novel RL algorithm
that can substantially reduce the running time of RL. Our key insight is that
RL tends to be costly due to the need to perform both inference and
backpropagation during training. To maximize use of computational resources,
our algorithm performs inference in large batches, and then performs off-policy
policy gradient updates in mini-batches. For off-policy updates, we incorporate
group advantage estimation into the policy gradient algorithm, and derive an
importance weighted estimator to correct for the bias arising from off-policy
learning. Our experiments demonstrate that our algorithm can reduce running
time by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms
while maintaining similar or better accuracy.

</details>


### [677] [CrossLag: Predicting Major Dengue Outbreaks with a Domain Knowledge Informed Transformer](https://arxiv.org/abs/2510.03566)
*Ashwin Prabu,Nhat Thanh Tran,Guofa Zhou,Jack Xin*

Main category: cs.LG

TL;DR: CrossLag是一种创新的基于Transformer的模型，通过整合滞后的环境信号，显著提高了登革热疫情爆发的预测能力，尤其是在关键的预警窗口期。


<details>
  <summary>Details</summary>
Motivation: 现有登革热预测模型在预测需要及时公众预警的重大疫情爆发方面仍面临挑战。

Method: 提出了一种名为CrossLag的环境感知注意力机制，该机制能够将外源数据中滞后的内源信号整合到Transformer架构中，并且参数量较低。该模型以TimeXer为基线，TimeXer是一个区分外源-内源输入的通用Transformer模型。

Result: 在新加坡登革热数据集上，CrossLag在24周的预测窗口内，在检测和预测重大疫情爆发方面，其表现显著优于TimeXer。

Conclusion: CrossLag模型在预测登革热疫情爆发，特别是在提前预警方面，展现出优越的性能。

Abstract: A variety of models have been developed to forecast dengue cases to date.
However, it remains a challenge to predict major dengue outbreaks that need
timely public warnings the most. In this paper, we introduce CrossLag, an
environmentally informed attention that allows for the incorporation of lagging
endogenous signals behind the significant events in the exogenous data into the
architecture of the transformer at low parameter counts. Outbreaks typically
lag behind major changes in climate and oceanic anomalies. We use TimeXer, a
recent general-purpose transformer distinguishing exogenous-endogenous inputs,
as the baseline for this study. Our proposed model outperforms TimeXer by a
considerable margin in detecting and predicting major outbreaks in Singapore
dengue data over a 24-week prediction window.

</details>


### [678] [Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs](https://arxiv.org/abs/2510.03567)
*Fatmazohra Rezkellah,Ramzi Dakhmouche*

Main category: cs.LG

TL;DR: 本研究提出了一种统一的方法来解决大型语言模型的隐私保护和安全生成问题，通过优化LLM权重来实现敏感信息遗忘和对抗越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，对其进行定制化以确保隐私保护和安全生成的需求日益增长。

Method: 研究了各种约束优化方法，旨在以统一的方式解决这两个关键问题。通过寻找对LLM权重进行最小干预的方法，使得目标词汇集不可达，或通过将部分权重转移到更安全的区域来增强模型对定制攻击的鲁棒性。

Result: 提出的最简单的点约束干预方法比现有的最大最小干预方法表现更好，且计算成本更低。与最先进的防御方法相比，该方法表现出优越的性能。

Conclusion: 该研究提出了一种新颖的、无需预言机分类器的统一方法，用于LLM的隐私保护和安全生成，并在实验中证明了其有效性和效率。

Abstract: With the increasing adoption of Large Language Models (LLMs), more
customization is needed to ensure privacy-preserving and safe generation. We
address this objective from two critical aspects: unlearning of sensitive
information and robustness to jail-breaking attacks. We investigate various
constrained optimization formulations that address both aspects in a
\emph{unified manner}, by finding the smallest possible interventions on LLM
weights that either make a given vocabulary set unreachable or embed the LLM
with robustness to tailored attacks by shifting part of the weights to a
\emph{safer} region. Beyond unifying two key properties, this approach
contrasts with previous work in that it doesn't require an oracle classifier
that is typically not available or represents a computational overhead.
Surprisingly, we find that the simplest point-wise constraint-based
intervention we propose leads to better performance than max-min interventions,
while having a lower computational cost. Comparison against state-of-the-art
defense methods demonstrates superior performance of the proposed approach.

</details>


### [679] [Longitudinal Flow Matching for Trajectory Modeling](https://arxiv.org/abs/2510.03569)
*Mohammad Mohaiminul Islam,Thijs P. Kuipers,Sharvaree Vadgama,Coen de Vente,Afsana Khan,Clara I. Sánchez,Erik J. Bekkers*

Main category: cs.LG

TL;DR: IMMFM框架通过学习与多个观测时间点一致的连续随机动力学，解决了生成模型在稀疏采样和高维轨迹方面的问题，实现了比现有方法更好的预测精度和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在处理稀疏采样和高维轨迹数据时存在困难，通常将动力学学习简化为成对转移。IMMFM旨在解决这一问题，通过学习与多个观测时间点一致的连续随机动力学来改进模型性能。

Method: IMMFM采用分段二次插值路径作为流匹配的平滑目标，并联合优化漂移项和数据驱动的扩散系数。该方法得到了稳定学习的理论条件支持，能够捕捉内在随机性，处理不规则稀疏采样，并生成特定于主体的轨迹。

Result: 实验结果表明，IMMFM在合成基准测试和真实世界纵向神经影像数据集上，在预测精度和下游任务方面均优于现有方法。

Conclusion: IMMFM是一种有效的框架，能够学习连续随机动力学，并成功应用于稀疏采样和高维轨迹数据，在预测和下游任务中表现出色。

Abstract: Generative models for sequential data often struggle with sparsely sampled
and high-dimensional trajectories, typically reducing the learning of dynamics
to pairwise transitions. We propose \textit{Interpolative Multi-Marginal Flow
Matching} (IMMFM), a framework that learns continuous stochastic dynamics
jointly consistent with multiple observed time points. IMMFM employs a
piecewise-quadratic interpolation path as a smooth target for flow matching and
jointly optimizes drift and a data-driven diffusion coefficient, supported by a
theoretical condition for stable learning. This design captures intrinsic
stochasticity, handles irregular sparse sampling, and yields subject-specific
trajectories. Experiments on synthetic benchmarks and real-world longitudinal
neuroimaging datasets show that IMMFM outperforms existing methods in both
forecasting accuracy and further downstream tasks.

</details>


### [680] [Efficient Test-Time Scaling for Small Vision-Language Models](https://arxiv.org/abs/2510.03574)
*Mehmet Onurcan Kaya,Desmond Elliott,Dim P. Papadopoulos*

Main category: cs.LG

TL;DR: 我们提出了一种计算高效的测试时间扩展策略，用于提高小型视觉-语言模型（VLM）的性能，而无需进行额外的计算或调优。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时间扩展技术对于计算资源有限的小型模型来说过于耗时，而小型模型本身在泛化能力和下游任务表现方面存在不足。

Method: 提出两种新的高效测试时间扩展策略：测试时间增强（TTAug）和测试时间自适应（TTAdapt）。TTAug通过聚合token级别的输出来处理多个增强输入，而TTAdapt则利用TTAug生成的伪标签来调整模型参数。

Result: 在九个基准测试中，我们的方法在保持计算效率的同时，一致地提高了小型VLM的性能，适用于资源受限的环境。

Conclusion: 我们提出的TTAug和TTAdapt策略能够有效地提高小型VLM的性能，同时保持其计算效率，并且在不同模型规模和不同VLM之间具有通用性。

Abstract: Small Vision-Language Models (VLMs) provide a computationally efficient
alternative to larger models, at the cost of weaker generalization abilities
and downstream task performance. These shortcomings could be addressed by
test-time scaling techniques, but existing methods are typically
computationally demanding, contradicting the resource-efficient design goals of
small models. To address these limitations, we propose two novel and efficient
test-time scaling strategies that leverage the model-internal features rather
than external supervision: (i) Test-Time Augmentation (TTAug), which generates
multiple augmented inputs and aggregates outputs at the token level without
parameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model
parameters during inference using consensus-based pseudolabels from TTAug.
Through extensive experiments across nine benchmarks, we demonstrate consistent
performance improvements while maintaining computational efficiency suitable
for resource-constrained environments. The generality of our approach is
demonstrated both within models at different scales and across different VLMs
without additional tuning.

</details>


### [681] [BEKAN: Boundary condition-guaranteed evolutionary Kolmogorov-Arnold networks with radial basis functions for solving PDE problems](https://arxiv.org/abs/2510.03576)
*Bongseok Kim,Jiahao Zhang,Guang Lin*

Main category: cs.LG

TL;DR: BEKAN是一种结合了径向基函数（RBF）和KAN的深度学习方法，可以精确处理偏微分方程（PDE）的边界条件，并在数值实验中表现出优于MLP和B-splines KAN的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在处理偏微分方程（PDE）时，由于神经网络的黑盒特性，难以精确施加边界条件。

Method: 提出了一种边界条件保证的演化Kolmogorov-Arnold网络（BEKAN），其中结合了高斯RBF、周期层和最小二乘法来分别处理Dirichlet、周期和Neumann边界条件。

Result: BEKAN在Dirichlet、Neumann、周期和混合边界值问题上进行了广泛的数值实验，结果表明BEKAN在准确性方面优于MLP和B-splines KAN。

Conclusion: BEKAN通过精确施加边界条件，增强了KAN在求解PDE问题上的能力，为科学计算和工程应用带来了新的可能性。

Abstract: Deep learning has gained attention for solving PDEs, but the black-box nature
of neural networks hinders precise enforcement of boundary conditions. To
address this, we propose a boundary condition-guaranteed evolutionary
Kolmogorov-Arnold Network (KAN) with radial basis functions (BEKAN). In BEKAN,
we propose three distinct and combinable approaches for incorporating
Dirichlet, periodic, and Neumann boundary conditions into the network. For
Dirichlet problem, we use smooth and global Gaussian RBFs to construct
univariate basis functions for approximating the solution and to encode
boundary information at the activation level of the network. To handle periodic
problems, we employ a periodic layer constructed from a set of sinusoidal
functions to enforce the boundary conditions exactly. For a Neumann problem, we
devise a least-squares formulation to guide the parameter evolution toward
satisfying the Neumann condition. By virtue of the boundary-embedded RBFs, the
periodic layer, and the evolutionary framework, we can perform accurate PDE
simulations while rigorously enforcing boundary conditions. For demonstration,
we conducted extensive numerical experiments on Dirichlet, Neumann, periodic,
and mixed boundary value problems. The results indicate that BEKAN outperforms
both multilayer perceptron (MLP) and B-splines KAN in terms of accuracy. In
conclusion, the proposed approach enhances the capability of KANs in solving
PDE problems while satisfying boundary conditions, thereby facilitating
advancements in scientific computing and engineering applications.

</details>


### [682] [Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning](https://arxiv.org/abs/2510.03578)
*Haoran Li,Chenhan Xiao,Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 该研究提出了一种名为“潜在对称性混合模型”（Latent MoS）的新方法，用于在测量数据有限的情况下学习动态系统，该方法通过结合对称性来提高样本效率，并采用分层结构来捕捉长期不变性。


<details>
  <summary>Details</summary>
Motivation: 在机器人和电力系统等工程领域，动态学习对于模型控制和强化学习至关重要。然而，有限的系统测量（例如低分辨率传感器）需要高样本效率的学习。对称性通过表征系统状态中的等变关系来提供强大的归纳偏置，从而提高样本效率。

Method: 提出“潜在对称性混合模型”（Latent MoS），这是一种能够从复杂的动态测量中捕捉对称性约束的潜在因子混合的表达模型。Latent MoS 在动态学习的同时，在局部和可证明的范围内保留了潜在的对称变换。为了捕捉长时等变性，引入了一个分层架构，堆叠了 MoS 块。

Result: 数值实验表明，Latent MoS 在插值和外插任务上优于最先进的基线方法，并且能够提供可解释的潜在表示，适用于未来的几何和安全关键分析。

Conclusion: Latent MoS 是一种有效的模型，可以通过利用对称性来提高动态学习的样本效率，并提供可解释的潜在表示，适用于各种物理系统。

Abstract: Learning dynamics is essential for model-based control and Reinforcement
Learning in engineering systems, such as robotics and power systems. However,
limited system measurements, such as those from low-resolution sensors, demand
sample-efficient learning. Symmetry provides a powerful inductive bias by
characterizing equivariant relations in system states to improve sample
efficiency. While recent methods attempt to discover symmetries from data, they
typically assume a single global symmetry group and treat symmetry discovery
and dynamic learning as separate tasks, leading to limited expressiveness and
error accumulation. In this paper, we propose the Latent Mixture of Symmetries
(Latent MoS), an expressive model that captures a mixture of symmetry-governed
latent factors from complex dynamical measurements. Latent MoS focuses on
dynamic learning while locally and provably preserving the underlying symmetric
transformations. To further capture long-term equivariance, we introduce a
hierarchical architecture that stacks MoS blocks. Numerical experiments in
diverse physical systems demonstrate that Latent MoS outperforms
state-of-the-art baselines in interpolation and extrapolation tasks while
offering interpretable latent representations suitable for future geometric and
safety-critical analyses.

</details>


### [683] [FieldFormer: Physics-Informed Transformers for Spatio-Temporal Field Reconstruction from Sparse Sensors](https://arxiv.org/abs/2510.03589)
*Ankit Bhardwaj,Ananth Balashankar,Lakshminarayanan Subramanian*

Main category: cs.LG

TL;DR: FieldFormer是一个基于Transformer的框架，用于无网格时空场重建，结合了数据驱动的灵活性和基于物理的结构，在稀疏、噪声和不规则的时空传感器数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有插值或学习方法在处理稀疏、噪声和不规则的时空传感器数据时存在不足，因为它们要么忽略控制偏微分方程（PDE），要么无法扩展。因此，需要一种能够结合数据驱动灵活性和物理约束结构的方法来解决这些问题。

Method: FieldFormer通过一个可学习的、基于速度缩放的距离度量来收集局部邻域，以适应不同的传播模式。它利用每批偏移重计算实现高效邻域构建，并通过期望最大化算法进行优化。局部Transformer编码器进行预测，并通过基于自动微分的PDE残差和边界特定惩罚来强制执行物理一致性。

Result: FieldFormer在三个基准测试（标量各向异性热方程、向量值浅水系统和真实的平流扩散污染模拟）中，性能持续优于强基线方法40%以上。其结果表明，FieldFormer能够从稀疏（0.4%-2%）和噪声（10%）数据中实现精确（RMSE < 10^{-2}$）、高效且物理上一致的场重建。

Conclusion: FieldFormer成功地实现了对稀疏、噪声和不规则的时空传感器数据的准确、高效和物理上一致的重建，证明了其在时空场重建任务中的有效性。

Abstract: Spatio-temporal sensor data is often sparse, noisy, and irregular, and
existing interpolation or learning methods struggle here because they either
ignore governing PDEs or do not scale. We introduce FieldFormer, a
transformer-based framework for mesh-free spatio-temporal field reconstruction
that combines data-driven flexibility with physics-based structure. For each
query, FieldFormer gathers a local neighborhood using a learnable
velocity-scaled distance metric, enabling anisotropic adaptation to different
propagation regimes. Neighborhoods are built efficiently via per-batch offset
recomputation, and refined in an expectation-maximization style as the velocity
scales evolve. Predictions are made by a local transformer encoder, and physics
consistency is enforced through autograd-based PDE residuals and
boundary-specific penalties. Across three benchmarks--a scalar anisotropic heat
equation, a vector-valued shallow-water system, and a realistic
advection-diffusion pollution simulation--FieldFormer consistently outperforms
strong baselines by more than 40%. Our results demonstrate that FieldFormer
enables accurate (RMSE$<10^{-2}$), efficient, and physically consistent field
reconstruction from sparse (0.4%-2%) and noisy(10%) data.

</details>


### [684] [Deep Domain Adaptation for Turbofan Engine Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends](https://arxiv.org/abs/2510.03604)
*Yucheng Wang,Mohamed Ragab,Yubo Hou,Zhenghua Chen,Min Wu,Xiaoli Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Remaining Useful Life (RUL) prediction for turbofan engines plays a vital
role in predictive maintenance, ensuring operational safety and efficiency in
aviation. Although data-driven approaches using machine learning and deep
learning have shown potential, they face challenges such as limited data and
distribution shifts caused by varying operating conditions. Domain Adaptation
(DA) has emerged as a promising solution, enabling knowledge transfer from
source domains with abundant data to target domains with scarce data while
mitigating distributional shifts. Given the unique properties of turbofan
engines, such as complex operating conditions, high-dimensional sensor data,
and slower-changing signals, it is essential to conduct a focused review of DA
techniques specifically tailored to turbofan engines. To address this need,
this paper provides a comprehensive review of DA solutions for turbofan engine
RUL prediction, analyzing key methodologies, challenges, and recent
advancements. A novel taxonomy tailored to turbofan engines is introduced,
organizing approaches into methodology-based (how DA is applied),
alignment-based (where distributional shifts occur due to operational
variations), and problem-based (why certain adaptations are needed to address
specific challenges). This taxonomy offers a multidimensional view that goes
beyond traditional classifications by accounting for the distinctive
characteristics of turbofan engine data and the standard process of applying DA
techniques to this area. Additionally, we evaluate selected DA techniques on
turbofan engine datasets, providing practical insights for practitioners and
identifying key challenges. Future research directions are identified to guide
the development of more effective DA techniques, advancing the state of RUL
prediction for turbofan engines.

</details>


### [685] [Explore the Loss space with Hill-ADAM](https://arxiv.org/abs/2510.03613)
*Meenakshi Manikandan,Leilani Gilpin*

Main category: cs.LG

TL;DR: Hill-ADAM是一种通过确定性地探索状态空间来逃离局部最小值并找到全局最小值的优化器。


<details>
  <summary>Details</summary>
Motivation: 局部最小值是深度学习模型训练中的一个常见问题，可能导致模型性能不佳。现有的随机优化算法有时会在找到全局最小值之前收敛到局部最小值。

Method: Hill-ADAM通过在误差最小化和误差最大化之间交替来逃离局部最小值。它首先通过误差最大化来逃离局部最小值，然后通过误差最小化来继续搜索。这种交替探索有助于在损失空间中进行更全面的探索，从而找到全局最小值。

Result: Hill-ADAM在5个损失函数和12个图像颜色校正实例上进行了测试，并取得了良好的效果。

Conclusion: Hill-ADAM是一种有效的优化器，可以帮助模型逃离局部最小值并找到全局最小值，从而提高模型性能。

Abstract: This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus
towards escaping local minima in prescribed loss landscapes to find the global
minimum. Hill-ADAM escapes minima by deterministically exploring the state
space. This eliminates uncertainty from random gradient updates in stochastic
algorithms while seldom converging at the first minimum that visits. In the
paper we first derive an analytical approximation of the ADAM Optimizer step
size at a particular model state. From there define the primary condition
determining ADAM limitations in escaping local minima. The proposed optimizer
algorithm Hill-ADAM alternates between error minimization and maximization. It
maximizes to escape the local minimum and minimizes again afterward. This
alternation provides an overall exploration throughout the loss space. This
allows the deduction of the global minimum's state. Hill-ADAM was tested with 5
loss functions and 12 amber-saturated to cooler-shade image color correction
instances.

</details>


### [686] [Neural Bayesian Filtering](https://arxiv.org/abs/2510.03614)
*Christopher Solinas,Radovan Haluska,David Sychrovsky,Finbarr Timbers,Nolan Bard,Michael Buro,Martin Schmid,Nathan R. Sturtevant,Michael Bowling*

Main category: cs.LG

TL;DR: Neural Bayesian Filtering (NBF) 是一种用于维护部分可观察系统中的隐藏状态分布（称为信念）的算法。它通过将信念映射到固定长度的嵌入向量来训练潜在表示，并使用粒子式更新在嵌入空间中计算后验。


<details>
  <summary>Details</summary>
Motivation: NBF 旨在结合经典滤波器的计算效率和深度生成模型的表达能力，以在部分可观察系统中维护隐藏状态的分布。

Method: NBF 将信念映射到固定长度的嵌入向量，并使用粒子式更新在嵌入空间中计算后验，以处理观测和环境动力学。

Result: NBF 在三种部分可观察环境的状态估计任务中得到了验证，展示了其在处理快速变化的、多峰信念方面的能力，并减轻了粒子耗尽的风险。

Conclusion: NBF 是一种有效的算法，能够结合计算效率和表达能力，用于在部分可观察系统中维护隐藏状态的分布。

Abstract: We present Neural Bayesian Filtering (NBF), an algorithm for maintaining
distributions over hidden states, called beliefs, in partially observable
systems. NBF is trained to find a good latent representation of the beliefs
induced by a task. It maps beliefs to fixed-length embedding vectors, which
condition generative models for sampling. During filtering, particle-style
updates compute posteriors in this embedding space using incoming observations
and the environment's dynamics. NBF combines the computational efficiency of
classical filters with the expressiveness of deep generative models - tracking
rapidly shifting, multimodal beliefs while mitigating the risk of particle
impoverishment. We validate NBF in state estimation tasks in three partially
observable environments.

</details>


### [687] [Predicting Stock Price Movement with LLM-Enhanced Tweet Emotion Analysis](https://arxiv.org/abs/2510.03633)
*An Vuong,Susan Gauch*

Main category: cs.LG

TL;DR: 该研究提出了一种结合社交媒体情绪和历史股价的深度学习框架，以预测次日股票价格的显著变动。


<details>
  <summary>Details</summary>
Motivation: 准确预测短期股票价格变动因市场波动性和投资者情绪敏感性而充满挑战。

Method: 利用 Llama 3.1-8B-Instruct 预处理推文数据，提取基于 DistilRoBERTa 分类器和 NRC 资源词典的三种情绪特征，并与历史股价结合训练 LSTM 模型。

Result: 在 TSLA、AAPL 和 AMZN 股票上的实验表明，所有三种情绪分析方法均优于仅使用历史股价的基线模型（准确率为 13.5%）。基于 DistilRoBERTa 的模型在结合 LLaMA 增强的情绪分析后，准确率从 23.6% 提升至 38.5%。

Conclusion: 使用大型语言模型预处理推文内容可以增强情绪分析的有效性，进而提高预测显著股票价格变动的准确性。

Abstract: Accurately predicting short-term stock price movement remains a challenging
task due to the market's inherent volatility and sensitivity to investor
sentiment. This paper discusses a deep learning framework that integrates
emotion features extracted from tweet data with historical stock price
information to forecast significant price changes on the following day. We
utilize Meta's Llama 3.1-8B-Instruct model to preprocess tweet data, thereby
enhancing the quality of emotion features derived from three emotion analysis
approaches: a transformer-based DistilRoBERTa classifier from the Hugging Face
library and two lexicon-based methods using National Research Council Canada
(NRC) resources. These features are combined with previous-day stock price data
to train a Long Short-Term Memory (LSTM) model. Experimental results on TSLA,
AAPL, and AMZN stocks show that all three emotion analysis methods improve the
average accuracy for predicting significant price movements, compared to the
baseline model using only historical stock prices, which yields an accuracy of
13.5%. The DistilRoBERTa-based stock prediction model achieves the best
performance, with accuracy rising from 23.6% to 38.5% when using LLaMA-enhanced
emotion analysis. These results demonstrate that using large language models to
preprocess tweet content enhances the effectiveness of emotion analysis which
in turn improves the accuracy of predicting significant stock price movements.

</details>


### [688] [From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse](https://arxiv.org/abs/2510.03636)
*Rabeya Amin Jhuma,Mostafa Mohaimen Akand Faisal*

Main category: cs.LG

TL;DR: 对抗性攻击会破坏LLM中的上下文学习（ICL），尤其是在公共卫生领域。通过在支持示例中引入微小扰动，可以显著改变情感标签。提出的“谱签名防御”能够有效过滤掉被污染的示例，同时保留数据的含义和情感，从而维持ICL的准确性并提高验证准确性，确保AI系统在健康相关社交媒体监控中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨数据中毒攻击如何破坏大型语言模型（LLM）在公共卫生情感分析中的上下文学习（ICL）能力，并提出相应的防御措施。

Method: 在人类偏肺病毒（HMPV）的推文中，通过同义词替换、插入否定词和随机扰动等对抗性微扰来污染支持示例。然后应用“谱签名防御”来过滤被污染的数据。

Result: 即使是微小的扰动也导致高达67%的情感标签翻转。应用防御措施后，ICL准确率保持在46.7%，逻辑回归验证准确率达到100%。

Conclusion: 该研究将ICL中毒的理论研究扩展到高风险的公共卫生领域，证明了ICL在受到攻击时的脆弱性，并强调了谱防御在增强AI系统在健康相关社交媒体监控中的可靠性方面的价值。

Abstract: This study explored how in-context learning (ICL) in large language models
can be disrupted by data poisoning attacks in the setting of public health
sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small
adversarial perturbations such as synonym replacement, negation insertion, and
randomized perturbation were introduced into the support examples. Even these
minor manipulations caused major disruptions, with sentiment labels flipping in
up to 67% of cases. To address this, a Spectral Signature Defense was applied,
which filtered out poisoned examples while keeping the data's meaning and
sentiment intact. After defense, ICL accuracy remained steady at around 46.7%,
and logistic regression validation reached 100% accuracy, showing that the
defense successfully preserved the dataset's integrity. Overall, the findings
extend prior theoretical studies of ICL poisoning to a practical, high-stakes
setting in public health discourse analysis, highlighting both the risks and
potential defenses for robust LLM deployment. This study also highlights the
fragility of ICL under attack and the value of spectral defenses in making AI
systems more reliable for health-related social media monitoring.

</details>


### [689] [Implicit Models: Expressive Power Scales with Test-Time Compute](https://arxiv.org/abs/2510.03638)
*Jialin Liu,Lisang Ding,Stanley Osher,Wotao Yin*

Main category: cs.LG

TL;DR: 隐式模型通过迭代单个参数块到不动点来计算输出，实现了无限深度的权重共享网络，并且训练时内存消耗恒定，从而在相同性能下显著降低了内存需求。尽管它们可以匹配甚至超越大型显式网络，但其机制尚不清楚。


<details>
  <summary>Details</summary>
Motivation: 隐式模型在训练时内存消耗恒定，但其工作机制和性能提升的原因仍不清楚。

Method: 通过非参数分析研究隐式模型的表达能力，并严格地从数学上表征其运作方式。

Result: 证明了隐式模型可以通过迭代逐步表达更复杂的映射，并且其表达能力会随着测试时间的推移而增强，最终能够匹配更丰富的函数集。该理论在图像重建、科学计算和运筹学三个领域得到验证，显示出随着迭代次数的增加，映射的复杂性、解的质量都会提高并趋于稳定。

Conclusion: 隐式模型可以通过迭代来提升表达能力和模型性能，这在多个领域得到了验证。

Abstract: Implicit models, an emerging model class, compute outputs by iterating a
single parameter block to a fixed point. This architecture realizes an
infinite-depth, weight-tied network that trains with constant memory,
significantly reducing memory needs for the same level of performance compared
to explicit models. While it is empirically known that these compact models can
often match or even exceed larger explicit networks by allocating more
test-time compute, the underlying mechanism remains poorly understood.
  We study this gap through a nonparametric analysis of expressive power. We
provide a strict mathematical characterization, showing that a simple and
regular implicit operator can, through iteration, progressively express more
complex mappings. We prove that for a broad class of implicit models, this
process lets the model's expressive power scale with test-time compute,
ultimately matching a much richer function class. The theory is validated
across three domains: image reconstruction, scientific computing, and
operations research, demonstrating that as test-time iterations increase, the
complexity of the learned mapping rises, while the solution quality
simultaneously improves and stabilizes.

</details>


### [690] [In-Vivo Training for Deep Brain Stimulation](https://arxiv.org/abs/2510.03643)
*Nicholas Carter,Arkaprava Gupta,Prateek Ganguli,Benedikt Dietrich,Vibhor Krishna,Samarjit Chakraborty*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep Brain Stimulation (DBS) is a highly effective treatment for Parkinson's
Disease (PD). Recent research uses reinforcement learning (RL) for DBS, with RL
agents modulating the stimulation frequency and amplitude. But, these models
rely on biomarkers that are not measurable in patients and are only present in
brain-on-chip (BoC) simulations. In this work, we present an RL-based DBS
approach that adapts these stimulation parameters according to brain activity
measurable in vivo. Using a TD3 based RL agent trained on a model of the basal
ganglia region of the brain, we see a greater suppression of biomarkers
correlated with PD severity compared to modern clinical DBS implementations.
Our agent outperforms the standard clinical approaches in suppressing PD
biomarkers while relying on information that can be measured in a real world
environment, thereby opening up the possibility of training personalized RL
agents specific to individual patient needs.

</details>


### [691] [SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning with Fast-Adaptive Structure of Spiking Neural Network](https://arxiv.org/abs/2510.03648)
*Huijing Zhang,Muyang Cao,Linshan Jiang,Xin Du,Di Yu,Changze Lv,Shuiguang Deng*

Main category: cs.LG

TL;DR: 在设备端，使用基于脉冲神经网络（SNN）的SAFA-SNN方法进行少样本增量学习（FSCIL），以解决数据不足和遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 边缘设备需要在动态环境中持续学习新类别，同时保护数据隐私和维持性能。在数据不足的情况下，需要进行设备端少样本增量学习（FSCIL）。现有的基于人工神经网络（ANNs）的参数高效FSCIL框架受设备资源限制，而脉冲神经网络（SNNs）具有能耗低、生物学合理性强、兼容神经形态硬件等优点。

Method: 提出了一种名为SAFA-SNN的SNN-based方法，用于设备端FSCIL。首先，通过稀疏性条件神经元动力学来缓解灾难性遗忘。其次，采用非零阶优化来处理脉冲非可导性问题。最后，通过子空间投影增强新类别的可辨别性，以减轻对新类别的过拟合。

Result: 在CIFAR100、Mini-ImageNet、CIFAR-10-DVS、DVS128gesture和N-Caltech101等数据集上进行了广泛实验，SAFA-SNN的性能优于基线方法，在Mini-ImageNet的最后一个增量学习阶段提高了至少4.01%，并且能耗降低了20%。

Conclusion: SAFA-SNN是一种有效的SNN-based设备端FSCIL方法，能够有效缓解灾难性遗忘和过拟合问题，同时保持较低的能耗。

Abstract: Continuous learning of novel classes is crucial for edge devices to preserve
data privacy and maintain reliable performance in dynamic environments.
However, the scenario becomes particularly challenging when data samples are
insufficient, requiring on-device few-shot class-incremental learning (FSCIL)
to maintain consistent model performance. Although existing work has explored
parameter-efficient FSCIL frameworks based on artificial neural networks
(ANNs), their deployment is still fundamentally constrained by limited device
resources. Inspired by neural mechanisms, Spiking neural networks (SNNs)
process spatiotemporal information efficiently, offering lower energy
consumption, greater biological plausibility, and compatibility with
neuromorphic hardware than ANNs. In this work, we present an SNN-based method
for On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We
first propose sparsity-conditioned neuronal dynamics, in which most neurons
remain stable while a subset stays active, thereby mitigating catastrophic
forgetting. To further cope with spike non-differentiability in gradient
estimation, we employ zeroth-order optimization. Moreover, during incremental
learning sessions, we enhance the discriminability of new classes through
subspace projection, which alleviates overfitting to novel classes. Extensive
experiments conducted on two standard benchmark datasets (CIFAR100 and
Mini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture,
and N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods,
specifically achieving at least 4.01% improvement at the last incremental
session on Mini-ImageNet and 20% lower energy cost over baseline methods with
practical implementation.

</details>


### [692] [Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders](https://arxiv.org/abs/2510.03659)
*Xu Wang,Yan Hu,Benyou Wang,Difan Zou*

Main category: cs.LG

TL;DR: 稀疏自编码器（SAE）在引导大型语言模型（LLM）方面的可解释性与其引导效用之间的关系并不像人们通常认为的那样强。研究发现，可解释性强的SAE特征并不一定能带来更好的引导效果。为了解决这个问题，研究提出了一种名为“Delta Token Confidence”的新方法来选择真正能引导LLM行为的特征，并证明该方法能显著提高引导性能。有趣的是，使用这种新方法选择特征后，可解释性与引导效用之间的相关性几乎消失，甚至可能为负值，这表明最有效的引导特征可能并不具备高可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于解决一个根本性的问题：稀疏自编码器（SAE）的可解释性是否真的意味着更好的模型引导效用？尽管SAE常被用于引导大型语言模型（LLM），但这种关联并未得到充分的验证。因此，本研究旨在通过实验评估SAE的可解释性与其引导效用之间的关系，并探索提高引导性能的方法。

Method: 研究通过在三种不同的LLM（Gemma-2-2B, Qwen-2.5-3B, Gemma-2-9B）上训练90个SAE，并改变其架构和稀疏度水平，来评估SAE的可解释性和引导效用。研究使用了SAEBench和AxBench两个基准来衡量这两个指标，并通过Kendall's tau-b系数分析它们之间的相关性。在此基础上，研究提出了一种名为Delta Token Confidence的新特征选择标准，用于识别那些能有效引导LLM行为的特征，并与现有的基于输出分数的标准进行了比较。

Result: 通过对90个SAE的分析，研究发现SAE的可解释性与其引导效用之间仅存在相对较弱的正相关（tau b约0.298）。这表明，高可解释性并不能保证SAE具有良好的引导性能。研究提出的Delta Token Confidence方法在三个LLM上将引导性能提高了52.52%，显著优于现有的基于输出分数的标准。更重要的是，当使用Delta Token Confidence选择特征后，可解释性与引导效用之间的相关性消失（tau b约0），甚至可能变为负值。

Conclusion: 本研究得出结论，SAE的可解释性并非衡量其引导LLM效用的可靠指标。研究提出的Delta Token Confidence方法能够更有效地识别和选择能够真正引导LLM行为的特征，从而显著提升引导性能。该研究还强调了在选择用于引导LLM的SAE特征时，需要关注其实际效用，而非仅仅依赖于可解释性。

Abstract: Sparse Autoencoders (SAEs) are widely used to steer large language models
(LLMs), based on the assumption that their interpretable features naturally
enable effective model behavior steering. Yet, a fundamental question remains
unanswered: does higher interpretability indeed imply better steering utility?
To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,
Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,
and evaluate their interpretability and steering utility based on SAEBench
(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a
rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis
reveals only a relatively weak positive association (tau b approx 0.298),
indicating that interpretability is an insufficient proxy for steering
performance. We conjecture the interpretability utility gap may stem from the
selection of SAE features, as not all of them are equally effective for
steering. To further find features that truly steer the behavior of LLMs, we
propose a novel selection criterion called Delta Token Confidence, which
measures how much amplifying a feature changes the next token distribution. We
show that our method improves the steering performance of three LLMs by 52.52
percent compared to the current best output score based criterion
(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token
Confidence, the correlation between interpretability and utility vanishes (tau
b approx 0), and can even become negative. This further highlights the
divergence between interpretability and utility for the most effective steering
features.

</details>


### [693] [Operationalizing Data Minimization for Privacy-Preserving LLM Prompting](https://arxiv.org/abs/2510.03662)
*Jijie Zhou,Niloofar Mireshghallah,Tianshi Li*

Main category: cs.LG

TL;DR: 用户在使用大型语言模型时倾向于提供过多的个人信息，增加了隐私风险。本文提出了一个框架来量化数据最小化，并在九种大型语言模型上进行了评估，发现更大的模型能更好地容忍数据最小化，但模型本身在预测最优数据最小化方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 用户在使用大型语言模型（LLMs）时，为了获得有用响应，常常会分享超出必要的信息，这增加了因记忆、个性化或安全漏洞而泄露隐私的风险。

Method: 提出一个框架来形式化定义和实现数据最小化，量化在给定用户提示和响应模型的情况下，保持效用所需的最低限度隐私泄露。采用优先队列树搜索来在隐私排序的变换空间中找到这个最优点。在一系列数据集和九种大型语言模型上评估了该框架。

Result: 评估结果表明，更大的模型（如GPT-5）在保持任务质量的同时，可以实现更强的数据最小化（85.7%的 redaction），而较小的模型（如Qwen2.5-0.5B）则只能实现19.3%的 redaction。与搜索得出的基准相比，大型语言模型在直接预测最优数据最小化方面表现不佳，倾向于过度抽象和共享信息。

Conclusion: 大型语言模型在直接预测最优数据最小化方面存在不足，这表明除了隐私方面的差距，还存在能力上的差距，模型可能缺乏对其解决任务所需信息的认知。

Abstract: The rapid deployment of large language models (LLMs) in consumer applications
has led to frequent exchanges of personal information. To obtain useful
responses, users often share more than necessary, increasing privacy risks via
memorization, context-based personalization, or security breaches. We present a
framework to formally define and operationalize data minimization: for a given
user prompt and response model, quantifying the least privacy-revealing
disclosure that maintains utility, and we propose a priority-queue tree search
to locate this optimal point within a privacy-ordered transformation space. We
evaluated the framework on four datasets spanning open-ended conversations
(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth
answers (CaseHold, MedQA), quantifying achievable data minimization with nine
LLMs as the response model. Our results demonstrate that larger frontier LLMs
can tolerate stronger data minimization while maintaining task quality than
smaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for
Qwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that
LLMs struggle to predict optimal data minimization directly, showing a bias
toward abstraction that leads to oversharing. This suggests not just a privacy
gap, but a capability gap: models may lack awareness of what information they
actually need to solve a task.

</details>


### [694] [Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning](https://arxiv.org/abs/2510.03669)
*Wenlong Deng,Yi Ren,Yushu Li,Boying Gong,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: THR是一种代币级别的奖励指标，用于量化代币对LLM正确响应的可能性影响，从而可以显式地引导模型进行探索或利用。


<details>
  <summary>Details</summary>
Motivation: 探索如何在强化学习中明确引导LLM的训练，以实现探索或利用。

Method: 提出了一种名为Token Hidden Reward (THR) 的代币级别指标，并结合Group Relative Policy Optimization (GRPO) 提出了一种THR引导的重加权算法，用于显式地偏向训练的探索或利用。

Result: THR指标能够量化代币对模型响应的影响，高绝对值的THR代币主导训练动态。正THR代币增强了对正确输出的信心，有利于利用；负THR代币则为替代输出了保留了概率质量，从而实现了探索。THR引导的重加权算法在数学推理基准测试中验证了其有效性，能够通过调整THR值来分别提高贪婪解码准确率（倾向于利用）或Pass@K准确率（倾向于探索）。该算法还可以与其他RL目标和模型架构集成。

Conclusion: THR是一种原则性的、细粒度的机制，用于动态控制RL微调LLM中的探索和利用，为在推理密集型应用中进行有针对性的微调提供了新工具。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced the
reasoning capabilities of large language models, yet how to explicitly steer
training toward exploration or exploitation remains an open problem. We
introduce Token Hidden Reward (THR), a token-level metric that quantifies each
token's influence on the likelihood of correct responses under Group Relative
Policy Optimization (GRPO). We find that training dynamics are dominated by a
small subset of tokens with high absolute THR values. Most interestingly,
tokens with positive THR strengthen confidence in correct outputs, thus
favoring exploitation, while tokens with negative THR preserve probability mass
for alternative outputs, enabling exploration. This insight suggests a natural
intervention: a THR-guided reweighting algorithm that modulates GRPO's learning
signals to explicitly bias training toward exploitation or exploration. We
validate the efficacy of this algorithm on diverse math reasoning benchmarks.
By amplifying tokens with positive THR value and weakening negative ones, our
algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse
strategy yields consistent gains in Pass@K accuracy, favoring exploration. We
further demonstrate that our algorithm integrates seamlessly with other RL
objectives such as GSPO and generalizes across architectures including Llama.
These findings establish THR as a principled and fine-grained mechanism for
dynamically controlling exploration and exploitation in RL-tuned LLMs,
providing new tools for targeted fine-tuning in reasoning-intensive
applications.

</details>


### [695] [Towards Sampling Data Structures for Tensor Products in Turnstile Streams](https://arxiv.org/abs/2510.03678)
*Zhao Song,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: Attention sampler is proposed to reduce computational burden of attention mechanisms in LLMs by utilizing importance sampling.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the computational challenges of large-scale attention-based models in AI, particularly in the context of LLMs, by drawing inspiration from classical $\ell_2$ samplers and recent advances in attention schemes.

Method: The paper proposes the 'attention sampler' which uses importance sampling in the streaming setting to reduce the computational complexity of traditional attention mechanisms. The effectiveness is analyzed theoretically in terms of space and update time.

Result: The proposed attention sampler significantly reduces the computational burden of traditional attention mechanisms, exhibiting scalability and broad applicability across various model architectures and domains.

Conclusion: The attention sampler provides an effective and scalable solution to the computational challenges of large-scale attention-based models, with theoretical guarantees on space and update time.

Abstract: This paper studies the computational challenges of large-scale
attention-based models in artificial intelligence by utilizing importance
sampling methods in the streaming setting. Inspired by the classical definition
of the $\ell_2$ sampler and the recent progress of the attention scheme in
Large Language Models (LLMs), we propose the definition of the attention
sampler. Our approach significantly reduces the computational burden of
traditional attention mechanisms. We analyze the effectiveness of the attention
sampler from a theoretical perspective, including space and update time.
Additionally, our framework exhibits scalability and broad applicability across
various model architectures and domains.

</details>


### [696] [Group Policy Gradient](https://arxiv.org/abs/2510.03679)
*Junhua Chen,Zixi Zhang,Hantao Zhong,Rika Antonova*

Main category: cs.LG

TL;DR: GPG是一种无需评论员的策略梯度估计器，它使用基于组的蒙特卡洛优势估计来替代学习到的价值函数，从而在效率和性能上与PPO相媲美或更优。


<details>
  <summary>Details</summary>
Motivation: 受GRPO在人类反馈强化学习中的成功启发，GPG旨在移除评论员带来的内存、计算和超参数成本，同时保留PPO的剪辑目标结构。

Method: GPG使用基于组的蒙特卡洛优势估计替代了评论员（学习到的价值函数），并保留了PPO的剪辑目标结构。

Result: GPG估计器的一致性得到了证明，偏差-方差权衡得到了分析，并且在标准基准测试中，GPG的性能与PPO相当或更优，同时在利用并行模拟和计算资源方面比PPO更有效。

Conclusion: GPG通过移除评论员并利用并行模拟，提供了一种更高效的策略梯度方法，性能可与PPO媲美。

Abstract: We introduce Group Policy Gradient (GPG), a family of critic-free
policy-gradient estimators for general MDPs. Inspired by the success of GRPO's
approach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a
learned value function with a group-based Monte Carlo advantage estimator,
removing the memory, compute, and hyperparameter costs of training a critic
while preserving PPO's clipped-objective structure. We prove the consistency of
the GPG estimator, analyze the bias-variance tradeoffs, and demonstrate
empirically that GPG matches or outperforms PPO on standard benchmarks. GPG
makes better use of parallel simulations, which, together with its critic-free
design, results in more efficient use of computational resources than PPO.

</details>


### [697] [From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning](https://arxiv.org/abs/2510.03690)
*Ali Azizpour,Reza Ramezanpour,Ashutosh Sabharwal,Santiago Segarra*

Main category: cs.LG

TL;DR: 该研究提出了一种新的图表示学习框架，用于处理由多种潜在分布组成的真实世界图数据集。该框架利用图核（graphons）和图矩（motif densities）来识别和分离不同的图群，并基于此改进了数据增强（GMAM）和图对比学习（GCL），在多个基准数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图表示学习方法忽略了真实世界图中常见的混合结构，即图数据集可能由多个不同的潜在分布生成。这导致在数据增强和对比学习等任务中表现不佳。

Method: 该框架利用图核（graphons）来表示潜在的图生成模型，并使用图矩（motif densities）来聚类具有相同生成模型的图。在此基础上，提出了图核混合数据增强方法（GMAM），以及改进的图对比学习方法（MGCL），后者通过将负样本限制在其他图模型生成的图中来优化目标。

Result: 在无监督学习方面，MGCL在8个数据集上取得了最先进的平均排名。在有监督学习方面，GMAM在7个数据集中的6个上获得了新的最先进准确率，并且一致优于现有方法。

Conclusion: 提出的框架能够有效地区分和利用图数据集中的混合结构，显著提升了图表示学习在无监督和有监督任务上的性能。

Abstract: Real-world graph datasets often consist of mixtures of populations, where
graphs are generated from multiple distinct underlying distributions. However,
modern representation learning approaches, such as graph contrastive learning
(GCL) and augmentation methods like Mixup, typically overlook this mixture
structure. In this work, we propose a unified framework that explicitly models
data as a mixture of underlying probabilistic graph generative models
represented by graphons. To characterize these graphons, we leverage graph
moments (motif densities) to cluster graphs arising from the same model. This
enables us to disentangle the mixture components and identify their distinct
generative mechanisms. This model-aware partitioning benefits two key graph
learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data
augmentation technique that interpolates in a semantically valid space guided
by the estimated graphons, instead of assuming a single graphon per class. 2)
For GCL, it enables model-adaptive and principled augmentations. Additionally,
by introducing a new model-aware objective, our proposed approach (termed MGCL)
improves negative sampling by restricting negatives to graphs from other
models. We establish a key theoretical guarantee: a novel, tighter bound
showing that graphs sampled from graphons with small cut distance will have
similar motif densities with high probability. Extensive experiments on
benchmark datasets demonstrate strong empirical performance. In unsupervised
learning, MGCL achieves state-of-the-art results, obtaining the top average
rank across eight datasets. In supervised learning, GMAM consistently
outperforms existing strategies, achieving new state-of-the-art accuracy in 6
out of 7 datasets.

</details>


### [698] [REG: A Regularization Optimizer for Robust Training Dynamics](https://arxiv.org/abs/2510.03691)
*Zehua Liu,Han Wu,Xiaojin Fu,Shuqi Liu,Xiongwei Han,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: REG 是一种新型优化器，通过使用 RACS 算子替代 Muon 的矩阵符号函数，解决了 Muon 优化器在训练 LLM 时的不稳定性问题，并在 LLM 训练和微调任务中表现优于 AdamW。


<details>
  <summary>Details</summary>
Motivation: Muon 优化器在训练 LLM 时存在不稳定性问题，并且在用 AdamW 预训练的模型微调时存在兼容性问题。需要一种新的优化器来解决这些限制。

Method: 提出了一种名为 REG 的新型优化器，用行和列缩放（RACS）算子替代了 Muon 的矩阵符号函数。RACS 算子在理论上基于平衡矩阵，以一种不那么剧烈的方式规范化更新步骤。

Result: REG 优化器在 LLM 训练和微调任务中表现出比 AdamW 更好的性能和稳定性，并能保持与 AdamW 训练范式的兼容性，避免了 Muon 在微调阶段出现的性能下降。

Conclusion: REG 优化器通过使用 RACS 算子解决了 Muon 优化器存在的问题，在 LLM 训练和微调中取得了优于 AdamW 的性能和稳定性，并能与 AdamW 训练范式保持一致。

Abstract: Optimizers are crucial for the efficient training of Large Language Models
(LLMs). While AdamW is the de facto standard, recent structure-aware optimizers
like Muon have emerged, which regularize gradient updates by operating on
entire weight matrices. The Muon optimizer balances the gradient updates along
all the directions. However, Muon's reliance on the matrix sign function can
lead to training instability, exhibits incompatibility when fine-tuning models
pre-trained with AdamW. To address these limitations, we propose \textbf{REG},
a novel optimizer that replaces Muon's aggressive matrix sign operator with the
Row-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a
matrix, the RACS operator regularizes the update steps in a less drastic
manner, making it simpler to implement and more compatible with established
training dynamics. Through extensive empirical experiments on LLM training, we
demonstrate that our REG optimizer not only achieves superior performance and
stability over AdamW, but also maintains consistency with the AdamW training
paradigm. This consistency is particularly evident during the fine-tuning
stage, where REG optimizer avoids the performance degradation observed with
Muon.

</details>


### [699] [Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach](https://arxiv.org/abs/2510.03722)
*Qianxin Yi,Shao-Bo Lin,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: 提出一种基于谱线性RL方法，在提高性能的同时增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 在序列决策中，可解释性和性能对RL的实际应用至关重要，但现有方法侧重于性能，依赖于事后解释。

Method: 提出一种基于谱的线性RL方法，通过谱滤波器函数扩展了基于岭回归的方法，明确了正则化在控制估计误差中的作用，并设计了一种自适应正则化参数选择策略。

Result: 理论分析证明了参数估计和泛化误差的近乎最优界限。实验结果表明，该方法在决策质量方面优于或匹配现有基线，并通过可解释性分析增强了用户信任。

Conclusion: 该方法有望弥合RL理论与实际决策之间的差距，在管理环境中提供可解释性、准确性和适应性。

Abstract: Reinforcement learning (RL) has been widely applied to sequential decision
making, where interpretability and performance are both critical for practical
adoption. Current approaches typically focus on performance and rely on post
hoc explanations to account for interpretability. Different from these
approaches, we focus on designing an interpretability-oriented yet
performance-enhanced RL approach. Specifically, we propose a spectral based
linear RL method that extends the ridge regression-based approach through a
spectral filter function. The proposed method clarifies the role of
regularization in controlling estimation error and further enables the design
of an adaptive regularization parameter selection strategy guided by the
bias-variance trade-off principle. Theoretical analysis establishes
near-optimal bounds for both parameter estimation and generalization error.
Extensive experiments on simulated environments and real-world datasets from
Kuaishou and Taobao demonstrate that our method either outperforms or matches
existing baselines in decision quality. We also conduct interpretability
analyses to illustrate how the learned policies make decisions, thereby
enhancing user trust. These results highlight the potential of our approach to
bridge the gap between RL theory and practical decision making, providing
interpretability, accuracy, and adaptability in management contexts.

</details>


### [700] [Personalized federated prototype learning in mixed heterogeneous data scenarios](https://arxiv.org/abs/2510.03726)
*Jiahao Zeng,Wolong Xing,Liangtao Shi,Xin Huang,Jialin Wang,Zhile Cao,Zhenkui Shi*

Main category: cs.LG

TL;DR: PFPL是一种在混合异构场景下利用个性化原型和一致性正则化来提高联邦学习模型性能和收敛性的新方法，同时降低了通信成本。


<details>
  <summary>Details</summary>
Motivation: 传统的联邦学习方法在处理数据异构性时存在特征或标签分布不均的问题，而数据异构性本身是提高模型性能的关键因素。PFPL旨在解决混合异构场景下的数据异构性问题。

Method: PFPL通过为每个客户端构建个性化、无偏的原型来提供更丰富的领域知识和无偏的收敛目标。此外，在本地更新阶段，引入一致性正则化将本地实例与其个性化原型对齐，以提高损失函数的收敛性。

Result: 在Digits和Office Caltech数据集上的实验结果验证了PFPL的有效性，并成功降低了通信成本。

Conclusion: PFPL通过构建个性化原型和引入一致性正则化，有效解决了混合异构场景下的联邦学习问题，提高了模型性能和收敛速度，并降低了通信成本。

Abstract: Federated learning has received significant attention for its ability to
simultaneously protect customer privacy and leverage distributed data from
multiple devices for model training. However, conventional approaches often
focus on isolated heterogeneous scenarios, resulting in skewed feature
distributions or label distributions. Meanwhile, data heterogeneity is actually
a key factor in improving model performance. To address this issue, we propose
a new approach called PFPL in mixed heterogeneous scenarios. The method
provides richer domain knowledge and unbiased convergence targets by
constructing personalized, unbiased prototypes for each client. Moreover, in
the local update phase, we introduce consistent regularization to align local
instances with their personalized prototypes, which significantly improves the
convergence of the loss function. Experimental results on Digits and Office
Caltech datasets validate the effectiveness of our approach and successfully
reduce the communication cost.

</details>


### [701] [Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation](https://arxiv.org/abs/2510.03731)
*Yongfu Xue*

Main category: cs.LG

TL;DR: IniLoRA通过一种新的初始化策略来优化LoRA，该策略使低秩矩阵能够更好地近似原始模型权重，从而提高适应大型语言模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: LoRA方法在参数高效微调中虽然有效，但其初始化方式限制了对原始模型权重的激活和利用，可能成为性能瓶颈。

Method: 提出了一种名为IniLoRA的新初始化策略，该策略旨在使低秩矩阵的乘积能够近似原始模型权重。同时还提出了IniLoRA的两种变体IniLoRA-α和IniLoRA-β，采用不同的初始化方法来进一步提升性能。

Result: 实验结果表明，IniLoRA在多种模型和任务上均优于标准的LoRA方法。

Conclusion: IniLoRA通过改进初始化策略，克服了LoRA的潜在瓶颈，能够更有效地激活和利用原始模型权重，从而在参数高效微调中取得更好的性能。

Abstract: The rapid development of parameter-efficient fine-tuning methods has
noticeably improved the efficiency of adapting large language models. Among
these, LoRA has gained widespread popularity due to its strong balance of
effectiveness and parameter efficiency. However, LoRA relies on initializing
two low-rank matrices whose product is zero, which limits its ability to
effectively activate and leverage the original model weights-creating a
potential bottleneck for optimal performance. To address this limitation, we
propose \textbf{IniLoRA}, a novel initialization strategy that initializes the
low-rank matrices to closely approximate the original model weights.
Experimental results indicate that IniLoRA achieves better performance than
LoRA across a range of models and tasks. Additionally, we introduce two
variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct
initialization methods to enhance performance further.

</details>


### [702] [Cost Efficient Fairness Audit Under Partial Feedback](https://arxiv.org/abs/2510.03734)
*Nirjhar Das,Mohit Sharma,Praharsh Nanavati,Kirankumar Shiragur,Amit Deshpande*

Main category: cs.LG

TL;DR: 在部分反馈下审计分类器公平性，提出了一种新的成本模型，并设计了两种设置下的最优审计算法。


<details>
  <summary>Details</summary>
Motivation: 在部分反馈的场景下，真实标签仅对被正向分类的个体可用（例如，仅观察已获批准申请人的贷款偿还结果），现有的分类器公平性审计方法无法满足实际成本效益需求。

Method: 研究了两种审计设置：1. 黑盒模型，无数据分布假设。2. 混合模型，假设特征和真实标签遵循指数族分布的混合。分别提出了相应的审计算法，并利用了截断样本学习和最大后验概率预言机等技术。

Result: 在黑盒模型设置下，提出了一个近乎最优的审计算法，并证明了自然基线算法的次优性。在混合模型设置下，设计了一种新算法，其审计成本显著低于黑盒模型。算法适用于人口统计均等、机会均等和均等赔率等公平性指标。在Adult Income和Law School等数据集上，审计成本比自然基线低约50%。

Conclusion: 所提出的最优审计算法在部分反馈场景下，通过新的成本模型和针对不同设置的算法设计，能够更有效地审计分类器公平性，显著降低了审计成本。

Abstract: We study the problem of auditing the fairness of a given classifier under
partial feedback, where true labels are available only for positively
classified individuals, (e.g., loan repayment outcomes are observed only for
approved applicants). We introduce a novel cost model for acquiring additional
labeled data, designed to more accurately reflect real-world costs such as
credit assessment, loan processing, and potential defaults. Our goal is to find
optimal fairness audit algorithms that are more cost-effective than random
exploration and natural baselines.
  In our work, we consider two audit settings: a black-box model with no
assumptions on the data distribution, and a mixture model, where features and
true labels follow a mixture of exponential family distributions. In the
black-box setting, we propose a near-optimal auditing algorithm under mild
assumptions and show that a natural baseline can be strictly suboptimal. In the
mixture model setting, we design a novel algorithm that achieves significantly
lower audit cost than the black-box case. Our approach leverages prior work on
learning from truncated samples and maximum-a-posteriori oracles, and extends
known results on spherical Gaussian mixtures to handle exponential family
mixtures, which may be of independent interest. Moreover, our algorithms apply
to popular fairness metrics including demographic parity, equal opportunity,
and equalized odds. Empirically, we demonstrate strong performance of our
algorithms on real-world fair classification datasets like Adult Income and Law
School, consistently outperforming natural baselines by around 50% in terms of
audit cost.

</details>


### [703] [EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models](https://arxiv.org/abs/2510.03760)
*Ping Guo,Chenyu Zhu,Siyuan Chen,Fei Liu,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: LLM在CUDA核优化方面存在挑战，我们提出了EvoEngineer框架来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: CUDA核优化对AI性能至关重要，但现有LLM方法存在碎片化、问题定义不清晰以及无法满足正确性要求等问题。

Method: 我们首先将CUDA核优化形式化为一个具有明确目标、约束和评估指标的代码优化任务，然后建立了首个系统性的基于LLM的代码演进框架EvoEngineer，用于指导优化策略的设计和调整，以平衡性能和正确性。

Result: 在91个真实CUDA核上进行的大量实验表明，EvoEngineer在性能和正确性之间取得了原则性的平衡，平均中值加速比为2.72倍，代码有效率为69.8%，在两个维度上均优于现有方法。其中，我们的方法在PyTorch核上的所有操作实现了36.75倍的最大加速比，并在50个达到2倍以上加速的50个操作中的28个（56.0%）上实现了最高加速比。

Conclusion: EvoEngineer框架成功地解决了CUDA核优化中的LLM应用挑战，并在性能和正确性方面取得了显著成果。

Abstract: CUDA kernel optimization has become a critical bottleneck for AI performance,
as deep learning training and inference efficiency directly depends on highly
optimized GPU kernels.
  Despite the promise of Large Language Models (LLMs) for automating kernel
optimization, this field suffers from a fragmented ecosystem of isolated and
incomparable approaches with unclear problem formulations.
  Furthermore, general-purpose LLM code evolution methods cannot meet strict
correctness requirements of CUDA kernel optimization.
  We address these fundamental challenges by first formalizing CUDA kernel
optimization as a code optimization task with a clear objective, constraints,
and evaluation metrics.
  We then establish the first systematic LLM-based code evolution framework,
EvoEngineer, that provides guidance for designing and adapting optimization
strategies to achieve a balance between performance and correctness.
  Finally, we implement a kernel optimization system based on this framework
and conduct extensive experiments on 91 real-world CUDA kernels.
  Our results demonstrate that EvoEngineer achieves a principled balance
between performance and correctness, with the highest averaged median speedup
of \textbf{2.72}$\times$ over baseline CUDA kernels and a code validity rate of
\textbf{69.8}\%, outperforming existing methods on both dimensions.
  Our method achieves a maximum speedup of \textbf{36.75}$\times$ among all
operations over PyTorch kernels and delivers the highest speedup on \textbf{28}
(\textbf{56.0\%}) of 50 operations that achieve over \textbf{2$\times$}
acceleration.

</details>


### [704] [Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation](https://arxiv.org/abs/2510.03782)
*Guofu Xie,Chen Zhang,Xiao Zhang,Yunsheng Shi,Ting Yao,Jun Xu*

Main category: cs.LG

TL;DR: MAGE是一个两阶段框架，通过模型合并进行引导解码，以解决可控多目标生成中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可控多目标生成中存在不足：基于合并的方法控制不直接且次优，而基于解码的方法需要聚合多个专家模型，空间开销大且依赖模型容量。

Method: MAGE框架分为两个阶段：第一阶段，通过合并多个目标相关的骨干模型来动态构建更鲁棒的基础模型，解决引导模型和基础模型之间的兼容性问题；第二阶段，将显式和隐式价值模型合并为一个统一的引导代理，用于指导基础模型的解码过程。

Result: 实验验证了价值模型中线性模式连通性（LMC），探索了模型合并与预测集成之间的关系，并证明了MAGE的增强可控性。

Conclusion: MAGE在可控性、帕累托最优性能和适应性方面优于现有方法。

Abstract: Adapting to diverse user needs at test time is a key challenge in
controllable multi-objective generation. Existing methods are insufficient:
merging-based approaches provide indirect, suboptimal control at the parameter
level, often disregarding the impacts of multiple objectives. While
decoding-based guidance is more direct, it typically requires aggregating
logits from multiple expert models, incurring significant space overhead and
relying heavily on individual model capacity. To address these issues, we
introduce Merge-And-GuidE (MAGE), a two-stage framework that leverages model
merging for guided decoding. We first identify a critical compatibility problem
between the guidance and base models. In Stage 1, MAGE resolves this by
dynamically constructing a more robust base model, merging a series of backbone
models that account for multiple objectives. In Stage 2, we merge explicit and
implicit value models into a unified guidance proxy, which then steers the
decoding of the base model from Stage 1. Our analysis empirically validates
Linear Mode Connectivity (LMC) in value models, explores the relationship
between model merging and prediction ensembling, and demonstrates the enhanced
controllability afforded by our approach. Extensive experiments show that our
method outperforms existing approaches, achieving superior controllability,
Pareto-optimal performance, and enhanced adaptability.

</details>


### [705] [Allocation of Parameters in Transformers](https://arxiv.org/abs/2510.03784)
*Ruoxi Yu,Haotian Jiang,Jingpu Cheng,Penghao Yu,Qianxiao Li,Zhong Li*

Main category: cs.LG

TL;DR: Transformers在模型效率方面的理论基础仍有待探索。本文研究了如何跨层分配模型参数（主要是注意力头和头维度）以平衡表达能力和效率。我们首先从近似的角度对浅层从信息提取中的作用进行数学分析，并对固定参数预算下头数和头维度的权衡进行了理论刻画。此外，我们还揭示并证明了softmax激活的“饱和”行为：持续增加头维度可能导致学习错误收益递减，尤其是在长序列上。这种饱和模式（有理论和实验支持）表明，较深的层可以用更少的参数更有效地运行。结合这些见解，我们提出了跨Transformer层分配注意力头和维度的原则性策略，从而为Transformer架构提供理论依据的模型效率。


<details>
  <summary>Details</summary>
Motivation: Transformers在模型效率方面的理论基础仍有待探索。本文研究了如何跨层分配模型参数（主要是注意力头和头维度）以平衡表达能力和效率。

Method: 我们首先从近似的角度对浅层从信息提取中的作用进行数学分析，并对固定参数预算下头数和头维度的权衡进行了理论刻画。此外，我们还揭示并证明了softmax激活的“饱和”行为：持续增加头维度可能导致学习错误收益递减，尤其是在长序列上。

Result: 我们揭示并证明了softmax激活的“饱和”行为：持续增加头维度可能导致学习错误收益递减，尤其是在长序列上。这种饱和模式（有理论和实验支持）表明，较深的层可以用更少的参数更有效地运行。

Conclusion: 结合这些见解，我们提出了跨Transformer层分配注意力头和维度的原则性策略，从而为Transformer架构提供理论依据的模型效率。

Abstract: Transformers have achieved remarkable successes across a wide range of
applications, yet the theoretical foundation of their model efficiency remains
underexplored. In this work, we investigate how the model parameters -- mainly
attention heads and head dimensions -- should be allocated across layers to
balance expressivity and efficiency. We first provide mathematical analysis on
the role of early layers in information extraction from an approximation
perspective, with a theoretical characterization on the trade-off between the
number of heads and head dimension under a fixed parameter budget. In addition,
we uncover and prove the \emph{saturation} behavior of softmax activations:
Continuously increasing head dimensions can lead to diminishing returns in
learning errors, particularly for long sequences. Supported by both theory and
experiments, this saturation pattern suggests that later layers can operate
more efficiently with reduced parameters. Combining these insights, we propose
principled strategies for allocating attention heads and dimensions across
Transformers' layers, shedding light on theoretically-grounded model efficiency
of Transformer-based architectures.

</details>


### [706] [Robust Batched Bandits](https://arxiv.org/abs/2510.03798)
*Yunwen Guo,Yunlun Shu,Gongyi Zhuo,Tianyu Wang*

Main category: cs.LG

TL;DR: 该研究提出适用于重尾奖励的批处理多臂老虎机算法，并发现重尾奖励在某些情况下能减少所需批次数以实现近乎最优的遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注轻尾奖励，但现实世界（如临床试验）中奖励分布常呈重尾特征，本文旨在解决这一研究空白。

Method: 提出适用于有限臂和Lipschitz连续设置的、针对重尾奖励的鲁棒批处理老虎机算法。

Result: 研究发现在实例无关和Lipschitz设置下，重尾奖励可以减少达到近乎最优遗憾所需的批次数；然而，在实例相关设置下，达到近乎最优遗憾所需的批次数与尾部厚度无关。

Conclusion: 重尾奖励对批处理老虎机算法的影响取决于实例的依赖性。在实例无关和Lipschitz设置下，更重的尾部反而需要更少的批次数；而在实例相关设置下，批次数不受尾部厚度影响。

Abstract: The batched multi-armed bandit (MAB) problem, in which rewards are collected
in batches, is crucial for applications such as clinical trials. Existing
research predominantly assumes light-tailed reward distributions, yet many
real-world scenarios, including clinical outcomes, exhibit heavy-tailed
characteristics. This paper bridges this gap by proposing robust batched bandit
algorithms designed for heavy-tailed rewards, within both finite-arm and
Lipschitz-continuous settings. We reveal a surprising phenomenon: in the
instance-independent regime, as well as in the Lipschitz setting,
heavier-tailed rewards necessitate a smaller number of batches to achieve
near-optimal regret. In stark contrast, for the instance-dependent setting, the
required number of batches to attain near-optimal regret remains invariant with
respect to tail heaviness.

</details>


### [707] [Curriculum-Augmented GFlowNets For mRNA Sequence Generation](https://arxiv.org/abs/2510.03811)
*Aya Laajil,Abduragim Shtanchaev,Sajan Muhammad,Eric Moulines,Salem Lahlou*

Main category: cs.LG

TL;DR: CAGFN通过结合课程学习和多目标生成流网络来设计从头mRNA序列，以解决mRNA序列设计中的稀疏奖励和多目标优化问题。


<details>
  <summary>Details</summary>
Motivation: 开发下一代疗法需要设计mRNA序列，这是一个巨大的挑战，需要优化稳定性、翻译效率和蛋白质表达等属性。生成流网络（GFlowNets）有潜力，但其训练受到稀疏、长时程奖励和多目标权衡的阻碍。

Method: 提出课程增强型GFN（CAGFN），它将课程学习与多目标GFN相结合。CAGFN采用基于长度的课程，逐步增加最大序列长度，引导模型从简单的子问题探索到更复杂的子问题。还提供了一个新的mRNA设计环境，用于GFN的训练。

Result: CAGFN在不同的mRNA设计任务中，提高了帕累托性能和生物学合理性，同时保持了多样性。与无课程训练的GFN相比，CAGFN能更快地达到更高质量的解决方案，并能推广到分布外序列。

Conclusion: CAGFN通过课程学习和多目标优化，在mRNA序列设计方面取得了显著进展，提高了效率、性能和泛化能力。

Abstract: Designing mRNA sequences is a major challenge in developing next-generation
therapeutics, since it involves exploring a vast space of possible nucleotide
combinations while optimizing sequence properties like stability, translation
efficiency, and protein expression. While Generative Flow Networks are
promising for this task, their training is hindered by sparse, long-horizon
rewards and multi-objective trade-offs. We propose Curriculum-Augmented
GFlowNets (CAGFN), which integrate curriculum learning with multi-objective
GFlowNets to generate de novo mRNA sequences. CAGFN integrates a length-based
curriculum that progressively adapts the maximum sequence length guiding
exploration from easier to harder subproblems. We also provide a new mRNA
design environment for GFlowNets which, given a target protein sequence and a
combination of biological objectives, allows for the training of models that
generate plausible mRNA candidates. This provides a biologically motivated
setting for applying and advancing GFlowNets in therapeutic sequence design. On
different mRNA design tasks, CAGFN improves Pareto performance and biological
plausibility, while maintaining diversity. Moreover, CAGFN reaches
higher-quality solutions faster than a GFlowNet trained with random sequence
sampling (no curriculum), and enables generalization to out-of-distribution
sequences.

</details>


### [708] [Detecting Invariant Manifolds in ReLU-Based RNNs](https://arxiv.org/abs/2510.03814)
*Lukas Eisenmann,Alena Brändle,Zahra Monfared,Daniel Durstewitz*

Main category: cs.LG

TL;DR: 本研究提出了一种检测RNN中稳定和不稳定流形的新算法，重点关注分段线性RNN（PLRNN）。该算法能够追踪吸引域边界，表征多稳态，并找到同斜点，从而证明PLRNN中存在混沌。研究还展示了该方法在分析皮层神经元电生理记录中的应用。


<details>
  <summary>Details</summary>
Motivation: 理解已训练的RNN如何产生其行为对于科学、医学应用以及可解释AI至关重要，因为RNN的动态行为取决于其状态空间的拓扑和几何特性。稳定和不稳定流形在其中扮演着重要角色，它们分割了状态空间，并且它们的交集导致了具有分形几何的混沌动力学。

Method: 提出了一种检测RNN（特别是分段线性RNN，PLRNN）稳定和不稳定流形的新算法。

Result: 该算法能够追踪吸引域的边界，从而表征多稳态。此外，该算法还能找到同斜点（稳定和不稳定流形的交点），并证明了PLRNN中混沌的存在。研究还通过一个皮层神经元电生理记录的实例，展示了该方法在揭示潜在动力学方面的应用价值。

Conclusion: 所提出的算法能够有效检测PLRNN中的稳定和不稳定流形，为理解和表征RNN的行为（如多稳态和混沌）提供了新的工具，并展示了其在生物信号分析等实际应用中的潜力。

Abstract: Recurrent Neural Networks (RNNs) have found widespread applications in
machine learning for time series prediction and dynamical systems
reconstruction, and experienced a recent renaissance with improved training
algorithms and architectural designs. Understanding why and how trained RNNs
produce their behavior is important for scientific and medical applications,
and explainable AI more generally. An RNN's dynamical repertoire depends on the
topological and geometrical properties of its state space. Stable and unstable
manifolds of periodic points play a particularly important role: They dissect a
dynamical system's state space into different basins of attraction, and their
intersections lead to chaotic dynamics with fractal geometry. Here we introduce
a novel algorithm for detecting these manifolds, with a focus on
piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as
their activation function. We demonstrate how the algorithm can be used to
trace the boundaries between different basins of attraction, and hence to
characterize multistability, a computationally important property. We further
show its utility in finding so-called homoclinic points, the intersections
between stable and unstable manifolds, and thus establish the existence of
chaos in PLRNNs. Finally we show for an empirical example, electrophysiological
recordings from a cortical neuron, how insights into the underlying dynamics
could be gained through our method.

</details>


### [709] [TROLL: Trust Regions improve Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2510.03817)
*Philipp Becker,Niklas Freymuth,Serge Thilges,Fabian Otto,Gerhard Neumann*

Main category: cs.LG

TL;DR: TROLL通过引入离散可微分信任区域投影来替代PPO-like的clip目标，在LLM微调中实现了更快的训练速度、更高的稳定性和更好的性能。


<details>
  <summary>Details</summary>
Motivation: PPO-like的clip目标作为LLM微调的标准方法，虽然有效但存在不稳定的更新和次优性能的问题，其剪切机制是对基于KL的信任区域的一种粗略近似。

Method: 提出了一种新颖的离散可微分信任区域投影方法（TROLL），该方法在模型最重要的token logits的稀疏子集上操作，以实现token级别的KL约束，作为PPO-like剪切的直接替代品。

Result: TROLL在训练速度、稳定性和最终成功率方面持续优于PPO-like剪切，并且适用于不同的数据集、模型家族和优势估计方法。

Conclusion: TROLL是一种有效的LLM微调方法，通过提供原则性的token级别KL约束，克服了传统剪切方法的局限性，并在实验中展示了优越的性能。

Abstract: On-policy Reinforcement Learning (RL) with PPO-like clip objectives has
become the standard choice for reward-based fine-tuning of large language
models (LLMs). Although recent work has explored improved estimators of
advantages and normalization, the clipping mechanism itself has remained
untouched. Originally introduced as a proxy for principled KL-based trust
regions, clipping is a crude approximation that often causes unstable updates
and suboptimal performance. We replace the clip objective with a novel discrete
differentiable trust region projection, which provides principled token-level
KL constraints. The projection operates on a sparse subset of the model's most
important token logits to balance computational cost and projection
effectiveness. Our approach, Trust Region Optimization for Large Language
Models (TROLL), serves as a direct replacement for PPO-like clipping during
training and does not alter the model's inference behavior. Across datasets,
model families, and advantage-estimation methods, TROLL consistently
outperforms PPO-like clipping in terms of training speed, stability, and final
success rates.

</details>


### [710] [Proximal Diffusion Neural Sampler](https://arxiv.org/abs/2510.03824)
*Wei Guo,Jaemoo Choi,Yuchen Zhu,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: PDNS通过在路径度量上使用近点方法来解决多模态分布采样中的模式崩溃问题，从而逐步学习逼近目标分布。


<details>
  <summary>Details</summary>
Motivation: 目标分布多模态且模式间存在显著障碍时，基于扩散的神经采样器训练面临模式崩溃的挑战。

Method: 提出了一种名为近邻扩散神经采样器（PDNS）的框架，该框架通过在路径度量上应用近点方法来解决随机最优控制问题，将学习过程分解为一系列更简单的子问题，并使用近邻加权去噪交叉熵（WDCE）目标来实现。

Result: PDNS 在连续和离散采样任务（包括分子动力学和统计物理）中表现出有效性和鲁棒性。

Conclusion: PDNS 能够有效解决多模态分布采样中的模式崩溃问题，并通过分阶段学习实现跨模式的充分探索。

Abstract: The task of learning a diffusion-based neural sampler for drawing samples
from an unnormalized target distribution can be viewed as a stochastic optimal
control problem on path measures. However, the training of neural samplers can
be challenging when the target distribution is multimodal with significant
barriers separating the modes, potentially leading to mode collapse. We propose
a framework named \textbf{Proximal Diffusion Neural Sampler (PDNS)} that
addresses these challenges by tackling the stochastic optimal control problem
via proximal point method on the space of path measures. PDNS decomposes the
learning process into a series of simpler subproblems that create a path
gradually approaching the desired distribution. This staged procedure traces a
progressively refined path to the desired distribution and promotes thorough
exploration across modes. For a practical and efficient realization, we
instantiate each proximal step with a proximal weighted denoising cross-entropy
(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS
through extensive experiments on both continuous and discrete sampling tasks,
including challenging scenarios in molecular dynamics and statistical physics.

</details>


### [711] [Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration](https://arxiv.org/abs/2510.03865)
*Wenhao Deng,Long Wei,Chenglei Yu,Tailin Wu*

Main category: cs.LG

TL;DR: RAPO (Rewards-Aware Policy Optimization) 是一种新的算法，旨在解决 RLVR 在 LLM 数学推理中存在的探索受限问题，通过使用前向 KL 散度和重加权参考策略来促进更广泛但集中的探索，并在 8K SimpleRL-Zero 数据集上训练 Qwen2.5 模型，取得了显著的性能提升，甚至解决了先前无法解决的问题。


<details>
  <summary>Details</summary>
Motivation: RLVR 在 LLM 数学推理方面表现出增强，但其优势会随着采样预算的增加而减弱，这是因为反向 KL 散度正则化器将策略限制在基础模型的搜索空间内。需要一种新的算法来促进更广泛的探索。

Method: RAPO 算法通过两个关键步骤来解决探索受限问题：1. 使用前向 KL 散度惩罚来鼓励分布外探索，以替代反向 KL 散度惩罚。2. 通过重加权参考策略来促进适应性的分布内探索。

Result: 在 8K SimpleRL-Zero 数据集上，使用 RAPO 训练的 Qwen2.5-3B 和 7B 模型在 AIME2024 和 AIME2025 数据集上取得了持续的性能提升。RAPO 成功使模型超越了基础模型的性能上限，并解决了先前无法解决的问题。

Conclusion: RAPO 算法有效地克服了 RLVR 中存在的探索受限问题，通过促进更广泛和集中的探索，显著提高了 LLM 在数学推理等挑战性任务上的性能，为 RLVR 在复杂推理任务中的应用开辟了新的可能性。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently enhanced
the reasoning capabilities of large language models (LLMs), particularly for
mathematical problem solving. However, a fundamental limitation remains: as the
sampling budget increases, the advantage of RLVR-trained models over their
pretrained bases often diminishes or even vanishes, revealing a strong
dependence on the base model's restricted search space. We attribute this
phenomenon to the widespread use of the reverse Kullback-Leibler (KL)
divergence regularizer, whose mode-seeking behavior keeps the policy trapped
inside the base model's support region and hampers wider exploration. To
address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an
algorithm to promote broader yet focused exploration. Our method (i) utilizes
the forward KL penalty to replace the reverse KL penalty for
out-of-distribution exploration, and (ii) reweights the reference policy to
facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B
models with RAPO on the 8K SimpleRL-Zero dataset, without supervised
fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO
consistently improves problem-solving performance. Notably, RAPO enables models
to surpass the base model's performance ceiling and solves previously
intractable problems, advancing the frontier of RLVR for challenging reasoning
tasks.

</details>


### [712] [Technical note on Fisher Information for Robust Federated Cross-Validation](https://arxiv.org/abs/2510.03838)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: 当训练数据分散在不同批次或跨地域进行联邦学习时，模型性能会下降。FIRE通过累积碎片引起的协变量偏移，并将其用作损失惩罚，以实现可扩展的分布对齐，从而解决此问题。FIRE在移位的验证集上表现优于基准测试。


<details>
  <summary>Details</summary>
Motivation: 训练数据分散在不同批次或跨地域进行联邦学习时，模型性能会下降，这是由于数据在时间和空间上的分散导致了经验训练分布的差异（协变量偏移）。

Method: 提出了一种名为FIRE（Fisher Information for Robust fEderated validation）的方法。FIRE通过利用近似Fisher信息来累积由数据碎片化引起的协变量偏移，并将该项作为每片段的损失惩罚，从而实现可扩展的分布对齐。

Result: FIRE在移位的验证集上，相比于重要性加权基准测试，性能最高提升5.1%；相比于联邦学习（FL）基准测试，性能最高提升5.3%。

Conclusion: FIRE能够有效缓解因数据碎片化引起的协变量偏移，提升联邦学习模型的性能，并且计算成本更低。

Abstract: When training data are fragmented across batches or federated-learned across
different geographic locations, trained models manifest performance
degradation. That degradation partly owes to covariate shift induced by data
having been fragmented across time and space and producing dissimilar empirical
training distributions. Each fragment's distribution is slightly different to a
hypothetical unfragmented training distribution of covariates, and to the
single validation distribution. To address this problem, we propose Fisher
Information for Robust fEderated validation (\textbf{FIRE}). This method
accumulates fragmentation-induced covariate shift divergences from the global
training distribution via an approximate Fisher information. That term, which
we prove to be a more computationally-tractable estimate, is then used as a
per-fragment loss penalty, enabling scalable distribution alignment. FIRE
outperforms importance weighting benchmarks by $5.1\%$ at maximum and federated
learning (FL) benchmarks by up to $5.3\%$ on shifted validation sets.

</details>


### [713] [LLM Chemistry Estimation for Multi-LLM Recommendation](https://arxiv.org/abs/2510.03930)
*Huascar Sanchez,Briland Hitaj*

Main category: cs.LG

TL;DR: 现有的多大型语言模型(LLM)协作方法依赖于隐式选择和输出评估，而未能分析协作模型是互补还是冲突。我们提出了LLM Chemistry框架，用于衡量LLM组合何时表现出协同或对抗行为，从而影响集体绩效。我们对LLM之间的化学反应进行了量化，并提出了一种基于模型交互依赖性分析的算法来推荐最佳模型组合。


<details>
  <summary>Details</summary>
Motivation: 需要一种方法来量化LLM协作中的协同或对抗行为，以优化多LLM系统的性能。

Method: 提出LLM Chemistry框架，量化LLM之间的化学反应，并通过分析交互依赖性来推荐模型组合。

Result: LLM组合的化学反应在异构模型配置下最为明显，其影响受任务类型、组大小和复杂性等因素影响。在分类、摘要和程序修复任务上的评估结果支持这些发现。

Conclusion: LLM Chemistry不仅是多LLM系统中的一个诊断因素，也为模型组合推荐奠定了基础。

Abstract: Multi-LLM collaboration promises accurate, robust, and context-aware
solutions, yet existing approaches rely on implicit selection and output
assessment without analyzing whether collaborating models truly complement or
conflict. We introduce LLM Chemistry -- a framework that measures when LLM
combinations exhibit synergistic or antagonistic behaviors that shape
collective performance beyond individual capabilities. We formalize the notion
of chemistry among LLMs, propose algorithms that quantify it by analyzing
interaction dependencies, and recommend optimal model ensembles accordingly.
Our theoretical analysis shows that chemistry among collaborating LLMs is most
evident under heterogeneous model profiles, with its outcome impact shaped by
task type, group size, and complexity. Evaluation on classification,
summarization, and program repair tasks provides initial evidence for these
task-dependent effects, thereby reinforcing our theoretical results. This
establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and
a foundation for ensemble recommendation.

</details>


### [714] [Technical note on Sequential Test-Time Adaptation via Martingale-Driven Fisher Prompting](https://arxiv.org/abs/2510.03839)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: M-FISHER 是一种用于流数据的序列分布偏移检测和自适应方法。


<details>
  <summary>Details</summary>
Motivation: 提出一个理论框架来支持 M-FISHER 方法，该方法旨在解决流数据中的分布偏移检测和自适应问题。

Method: 使用非一致性分数构建指数鞅，并应用 Ville 不等式来控制虚警率，确保任何停止时间的统计有效性。对于持续的偏移，理论上界定了检测延迟与偏移后信息增益的关系。在自适应方面，通过 Fisher 预处理的提示参数更新来实现自然梯度下降，以最小化 KL 散度并保持稳定性和参数不变性。

Result: M-FISHER 在检测方面实现了时间一致的虚警率控制，并且检测效率与分布散度相关。在自适应方面，实现了局部最优更新，最小化了 KL 散度，并保持了稳定性和参数不变性。

Conclusion: M-FISHER 为在协变量偏移下进行序列决策提供了一种原则性的方法，实现了鲁棒的、随时有效的检测和几何上稳定的自适应。

Abstract: We present a theoretical framework for M-FISHER, a method for sequential
distribution shift detection and stable adaptation in streaming data. For
detection, we construct an exponential martingale from non-conformity scores
and apply Ville's inequality to obtain time-uniform guarantees on false alarm
control, ensuring statistical validity at any stopping time. Under sustained
shifts, we further bound the expected detection delay as
$\mathcal{O}(\log(1/\delta)/\Gamma)$, where $\Gamma$ reflects the post-shift
information gain, thereby linking detection efficiency to distributional
divergence. For adaptation, we show that Fisher-preconditioned updates of
prompt parameters implement natural gradient descent on the distributional
manifold, yielding locally optimal updates that minimize KL divergence while
preserving stability and parameterization invariance. Together, these results
establish M-FISHER as a principled approach for robust, anytime-valid detection
and geometrically stable adaptation in sequential decision-making under
covariate shift.

</details>


### [715] [On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records](https://arxiv.org/abs/2510.03844)
*Sarah C. Lotspeich,Abbey Collins,Brian J. Wells,Ashish K. Khanna,Joseph Rigdon,Lucy D'Agostino McGowan*

Main category: cs.LG

TL;DR: 电子健康记录（EHR）数据常有缺失和错误，研究人员开发了一种基于ICD-10代码的算法，利用“路线图”（辅助诊断列表）来补充缺失值，并使用大型语言模型（LLM）优化路线图。该算法在1000名患者的数据上进行了测试，结果显示其恢复缺失数据的准确性与专家审查相当，且具有良好的可扩展性，有望应用于大规模数据质量监控。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）数据存在数据缺失和错误的问题。先前的方法（“丰富”图表审查协议）虽然有效，但成本高昂且耗时。本研究旨在开发一种更具成本效益和可扩展性的方法，以自动化的方式恢复EHR中的缺失值。

Method: 研究人员首先使用了早期版本的“路线图”（包含辅助诊断的列表），然后利用大型语言模型（LLM）结合临床专业知识迭代优化了路线图。在100名患者的数据上评估了不同路线图的算法性能。最终选定的算法使用了经LLM扩展并经临床医生批准的路线图，并在1000名患者的数据上进行了应用。

Result: 该算法恢复缺失数据的效果与专家审查相当，甚至在某些情况下更好，具体效果取决于所使用的路线图。这表明该算法在准确性方面具有潜力。

Conclusion: 通过大型语言模型（LLM）增强的、由临床驱动的算法能够以与人工图表审查相似的准确性恢复缺失的EHR数据，并且易于扩展到大样本量。未来可将此方法扩展到监控其他数据质量维度（如数据的合理性）。

Abstract: Objective: Electronic health records (EHR) data are prone to missingness and
errors. Previously, we devised an "enriched" chart review protocol where a
"roadmap" of auxiliary diagnoses (anchors) was used to recover missing values
in EHR data (e.g., a diagnosis of impaired glycemic control might imply that a
missing hemoglobin A1c value would be considered unhealthy). Still, chart
reviews are expensive and time-intensive, which limits the number of patients
whose data can be reviewed. Now, we investigate the accuracy and scalability of
a roadmap-driven algorithm, based on ICD-10 codes (International Classification
of Diseases, 10th revision), to mimic expert chart reviews and recover missing
values. Materials and Methods: In addition to the clinicians' original roadmap
from our previous work, we consider new versions that were iteratively refined
using large language models (LLM) in conjunction with clinical expertise to
expand the list of auxiliary diagnoses. Using chart reviews for 100 patients
from the EHR at an extensive learning health system, we examine algorithm
performance with different roadmaps. Using the larger study of $1000$ patients,
we applied the final algorithm, which used a roadmap with clinician-approved
additions from the LLM. Results: The algorithm recovered as much, if not more,
missing data as the expert chart reviewers, depending on the roadmap.
Discussion: Clinically-driven algorithms (enhanced by LLM) can recover missing
EHR data with similar accuracy to chart reviews and can feasibly be applied to
large samples. Extending them to monitor other dimensions of data quality
(e.g., plausability) is a promising future direction.

</details>


### [716] [On Provable Benefits of Muon in Federated Learning](https://arxiv.org/abs/2510.03866)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: Muon优化器在联邦学习中表现良好，提出的FedMuon算法收敛速度快，且能处理重尾噪声。


<details>
  <summary>Details</summary>
Motivation: 填补Muon优化器在联邦学习中应用效果未知的空白。

Method: 提出FedMuon算法，并分析其非凸问题的收敛率。

Result: FedMuon具有学习率不依赖于问题参数、能处理重尾噪声等优点，并在多种神经网络架构的实验中得到验证。

Conclusion: FedMuon在联邦学习中是有效的。

Abstract: The recently introduced optimizer, Muon, has gained increasing attention due
to its superior performance across a wide range of applications. However, its
effectiveness in federated learning remains unexplored. To address this gap,
this paper investigates the performance of Muon in the federated learning
setting. Specifically, we propose a new algorithm, FedMuon, and establish its
convergence rate for nonconvex problems. Our theoretical analysis reveals
multiple favorable properties of FedMuon. In particular, due to its
orthonormalized update direction, the learning rate of FedMuon is independent
of problem-specific parameters, and, importantly, it can naturally accommodate
heavy-tailed noise. The extensive experiments on a variety of neural network
architectures validate the effectiveness of the proposed algorithm.

</details>


### [717] [Principled and Tractable RL for Reasoning with Diffusion Language Models](https://arxiv.org/abs/2510.04019)
*Anthony Zhan*

Main category: cs.LG

TL;DR: AGRPO是一种新颖的、有原则的RL算法，专门为扩散LLM（dLLM）设计，它使用蒙特卡洛采样来计算无偏策略梯度估计，并在数学和推理任务上取得了显著的性能提升，同时保持了理论上的健全性和实际有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的RL技术无法直接应用于dLLM，而之前的RL后训练方法缺乏理论基础。本研究旨在开发一种适用于dLLM的、有原则的RL算法。

Method: 提出了一种名为AGRPO（Amortized Group Relative Policy Optimization）的新型在线RL算法，该算法使用蒙特卡洛采样来计算无偏策略梯度估计，专门为dLLM设计。

Result: 在GSM8K任务上实现了+7.6%的绝对增益，在Countdown任务上性能提升3.8倍，优于基线LLaDA-8B-Instruct模型和现有RL方法diffu-GRPO。此外，在不同采样步数下，AGRPO在计算和性能之间取得了更好的权衡。

Conclusion: 在线RL算法可以以有原则的方式扩展到扩散LLM，并能同时保持理论上的健全性和实际的有效性。

Abstract: Diffusion large language models (dLLMs) are a new paradigm of
non-autoregressive language models that are trained to predict multiple tokens
in parallel and generate text via iterative unmasking. Recent works have
successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B
scale, but dLLMs have yet to benefit from modern post-training techniques, e.g.
reinforcement learning (RL), that have proven effective for autoregressive
models. Crucially, algorithms designed for traditional LLMs aren't directly
compatible with diffusion frameworks due to inherent differences in modeling
assumptions. Moreover, existing attempts at dLLM post-training with RL rely on
heuristic-based objectives with no theoretical grounding. In this work, we
present Amortized Group Relative Policy Optimization (AGRPO), a principled
on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo
sampling to compute an unbiased policy gradient estimate, making it the first
tractable, faithful adaptation of policy gradient methods for dLLMs. We
demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common
setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x
performance on the Countdown task over the baseline LLaDA-8B-Instruct model and
1.3x performance gains over comparable RL methods such as diffu-GRPO.
Furthermore, these gains persist across different numbers of sampling steps at
inference time, achieving better tradeoffs between compute and performance. Our
results demonstrate that online RL algorithms can be extended to diffusion LLMs
in principled ways, maintaining both theoretical soundness and practical
effectiveness.

</details>


### [718] [Optimal Scaling Needs Optimal Norm](https://arxiv.org/abs/2510.03871)
*Oleg Filatov,Jiangtao Wang,Jan Ebert,Stefan Kesselheim*

Main category: cs.LG

TL;DR: 研究发现，在模型和数据集扩展下，最优超参数迁移的单一不变性原则是由输出层算子范数决定的，并提出了“范数迁移”概念，同时给出了最优学习率/批次大小配对的缩放规则。


<details>
  <summary>Details</summary>
Motivation: 在模型和数据集扩展下，尽管最优超参数迁移取得了进展，但缺乏统一的解释性原理。

Method: 使用 Scion 优化器，通过实验观察到在不同模型大小（高达13.8亿参数）和数据集大小（高达1380亿词元）下，最优学习率/批次大小配对 $(\eta^{\ast}, B^{\ast})$ 具有相同的算子范数值，并将其命名为“范数迁移”。研究了范数迁移作为必要条件，并提出了充分条件，测量了 $(\eta^{\ast}, B^{\ast})$ 随数据集大小的缩放规则，并与 Adam 优化器进行了比较。此外，还研究了逐层学习率调整对模型性能的影响，特别是输层和隐藏层的敏感性。

Result: 在跨越不同模型和数据集规模的实验中，观察到“范数迁移”现象，即最优学习率/批次大小配对的算子范数保持不变。给出了 $(\eta^{\ast}, B^{\ast})$ 随数据集大小的缩放规则，并发现其与 Adam 优化器一致。研究表明，调整逐层学习率，特别是输出层，可以提高模型性能。

Conclusion: 提出了“范数迁移”作为最优超参数迁移的统一原则，并提供了$(\eta^{\ast}, B^{\ast})$ 随数据集大小的缩放规则。研究结果为理解和优化大规模语言模型训练提供了实践指导，并发布了 Disco 实现以支持相关研究。

Abstract: Despite recent progress in optimal hyperparameter transfer under model and
dataset scaling, no unifying explanatory principle has been established. Using
the Scion optimizer, we discover that joint optimal scaling across model and
dataset sizes is governed by a single invariant: the operator norm of the
output layer. Across models with up to 1.3B parameters trained on up to 138B
tokens, the optimal learning rate/batch size pair $(\eta^{\ast}, B^{\ast})$
consistently has the same operator norm value - a phenomenon we term norm
transfer. This constant norm condition is necessary but not sufficient: while
for each dataset size, multiple $(\eta, B)$ reach the optimal norm, only a
unique $(\eta^{\ast}, B^{\ast})$ achieves the best loss. As a sufficient
condition, we provide the first measurement of $(\eta^{\ast}, B^{\ast})$
scaling with dataset size for Scion, and find that the scaling rules are
consistent with those of the Adam optimizer. Tuning per-layer-group learning
rates also improves model performance, with the output layer being the most
sensitive and hidden layers benefiting from lower learning rates. We provide
practical insights on norm-guided optimal scaling and release our Distributed
Scion (Disco) implementation with logs from over two thousand runs to support
research on LLM training dynamics at scale.

</details>


### [719] [BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty](https://arxiv.org/abs/2510.03893)
*Akshay Kudva,Joel A. Paulson*

Main category: cs.LG

TL;DR: BONSAI是一个新的鲁棒贝叶斯优化框架，利用部分结构化知识来处理不确定性下的最优设计问题，在保证鲁棒性的同时提高了样本效率和解决方案质量。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒优化方法在处理高保真模拟环境时存在局限性，而现有的鲁棒贝叶斯优化方法在利用结构信息和处理高维问题方面存在不足。本研究旨在解决这些问题，提供一个更灵活、可扩展的鲁棒优化解决方案。

Method: BONSAI将目标函数表示为有向图，结合了白盒和黑盒组件，并利用中间信息进行优化。提出了一种基于Thompson采样的方法，并使用梯度下降法进行优化。

Result: BONSAI在多种合成和真实世界案例研究中，包括过程系统工程应用，与现有的基于仿真的鲁棒优化算法相比，在样本效率和鲁棒解决方案质量方面均表现更优。

Conclusion: BONSAI框架能够有效地利用部分结构化知识，为复杂工程系统中的不确定性感知设计提供了实际优势，提高了样本效率和解决方案质量。

Abstract: Optimal design under uncertainty remains a fundamental challenge in advancing
reliable, next-generation process systems. Robust optimization (RO) offers a
principled approach by safeguarding against worst-case scenarios across a range
of uncertain parameters. However, traditional RO methods typically require
known problem structure, which limits their applicability to high-fidelity
simulation environments. To overcome these limitations, recent work has
explored robust Bayesian optimization (RBO) as a flexible alternative that can
accommodate expensive, black-box objectives. Existing RBO methods, however,
generally ignore available structural information and struggle to scale to
high-dimensional settings. In this work, we introduce BONSAI (Bayesian
Optimization of Network Systems under uncertAInty), a new RBO framework that
leverages partial structural knowledge commonly available in simulation-based
models. Instead of treating the objective as a monolithic black box, BONSAI
represents it as a directed graph of interconnected white- and black-box
components, allowing the algorithm to utilize intermediate information within
the optimization process. We further propose a scalable Thompson sampling-based
acquisition function tailored to the structured RO setting, which can be
efficiently optimized using gradient-based methods. We evaluate BONSAI across a
diverse set of synthetic and real-world case studies, including applications in
process systems engineering. Compared to existing simulation-based RO
algorithms, BONSAI consistently delivers more sample-efficient and
higher-quality robust solutions, highlighting its practical advantages for
uncertainty-aware design in complex engineering systems.

</details>


### [720] [What Scales in Cross-Entropy Scaling Law?](https://arxiv.org/abs/2510.04067)
*Junxi Yan,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 交叉熵縮放定律在大型語言模型的發展中失效，因為只有其組成部分“誤差熵”會隨著模型規模的增大而穩定縮放，而“自對齊”和“置信度”則保持不變。因此，新的“誤差熵縮放定律”更準確地描述了模型的行為，並對未來模型的訓練和發展具有重要意義。


<details>
  <summary>Details</summary>
Motivation: 現有的交叉熵縮放定律在大規模語言模型訓練中失效，導致損失下降速度慢於預期，給模型開發帶來困難。需要找到導致此現象的根本原因。

Method: 將交叉熵分解為誤差熵、自對齊和置信度三個部分，並通過理論和實證研究，分析每個組成部分在不同模型規模下的縮放行為，特別是與模型大小的關係。

Result: 實驗結果表明，只有誤差熵遵循穩定的冪次定律縮放，而自對齊和置信度則基本保持不變。誤差熵在小型模型中佔據主導地位，但隨著模型增大，其佔比逐漸減少。這解釋了為何交叉熵縮放定律在小模型尺度上準確，而在大模型尺度上失效。

Conclusion: 誤差熵縮放定律是比傳統交叉熵縮放定律更準確描述模型行為的定律，對於理解、訓練和未來開發大型語言模型具有廣泛的應用前景。

Abstract: The cross-entropy scaling law has long served as a key tool for guiding the
development of large language models. It shows that cross-entropy loss
decreases in a predictable power-law rate as the model size increases. However,
recent evidence indicates that this law breaks down at very large scales: the
loss decreases more slowly than expected, which causes significant trouble for
developing large language models. In this paper, we hypothesize that the root
cause lies in the fact that cross-entropy itself does not truly scale; instead,
only one of its hidden components does. To investigate this, we introduce a
novel decomposition of cross-entropy into three parts: Error-Entropy,
Self-Alignment, and Confidence. We show both theoretically and empirically that
this decomposition precisely captures the training dynamics and optimization
objectives. Through extensive experiments on multiple datasets and 32 models
spanning five orders of magnitude in size, we find that only error-entropy
follows a robust power-law scaling, while the other two terms remain largely
invariant. Moreover, error-entropy constitutes the dominant share of
cross-entropy in small models but diminishes in proportion as models grow
larger. This explains why the cross-entropy scaling law appears accurate at
small scales but fails at very large ones. Our findings establish the
error-entropy scaling law as a more accurate description of model behavior. We
believe it will have wide applications in the training, understanding, and
future development of large language models.

</details>


### [721] [LLM as an Algorithmist: Enhancing Anomaly Detectors via Programmatic Synthesis](https://arxiv.org/abs/2510.03904)
*Hangting Ye,Jinmeng Li,He Zhao,Mingchen Zhuge,Dandan Guo,Yi Chang,Hongyuan Zha*

Main category: cs.LG

TL;DR: LLM-DAS通过让LLM扮演“算法师”角色，分析现有异常检测方法的弱点，并生成针对性的Python代码来合成难以检测的异常，从而增强模型的鲁棒性，解决了现有表格数据异常检测方法的局限性，并在36个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的表格数据异常检测方法通常基于对异常模式的假设，导致在实际应用中性能不稳定。直接将大型语言模型（LLM）应用于表格异常检测存在处理异构数据困难和隐私风险等挑战。

Method: LLM-DAS框架将LLM定位为“算法师”，而不是“数据处理器”。该框架利用LLM的算法推理能力，分析现有检测器的元描述，理解其内在弱点，并生成可重用的、与数据无关的Python代码，以合成能够利用这些弱点的“难以检测”的异常。合成的程序被用来增强训练数据，将问题转化为更具区分性的二分类任务，从而提高检测器的鲁棒性。

Result: 在36个表格异常检测（TAD）基准测试上的广泛实验表明，LLM-DAS能够持续提升主流检测器的性能。

Conclusion: 通过将LLM的推理能力与经典的异常检测算法通过程序合成相结合，LLM-DAS提供了一种可扩展、有效且保护隐私的方法，用于弥补现有检测器在逻辑上的盲点。

Abstract: Existing anomaly detection (AD) methods for tabular data usually rely on some
assumptions about anomaly patterns, leading to inconsistent performance in
real-world scenarios. While Large Language Models (LLMs) show remarkable
reasoning capabilities, their direct application to tabular AD is impeded by
fundamental challenges, including difficulties in processing heterogeneous data
and significant privacy risks. To address these limitations, we propose
LLM-DAS, a novel framework that repositions the LLM from a ``data processor''
to an ``algorithmist''. Instead of being exposed to raw data, our framework
leverages the LLM's ability to reason about algorithms. It analyzes a
high-level description of a given detector to understand its intrinsic
weaknesses and then generates detector-specific, data-agnostic Python code to
synthesize ``hard-to-detect'' anomalies that exploit these vulnerabilities.
This generated synthesis program, which is reusable across diverse datasets, is
then instantiated to augment training data, systematically enhancing the
detector's robustness by transforming the problem into a more discriminative
two-class classification task. Extensive experiments on 36 TAD benchmarks show
that LLM-DAS consistently boosts the performance of mainstream detectors. By
bridging LLM reasoning with classic AD algorithms via programmatic synthesis,
LLM-DAS offers a scalable, effective, and privacy-preserving approach to
patching the logical blind spots of existing detectors.

</details>


### [722] [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)
*Ziyan Wang,Zheng Wang,Jie Fu,Xingwei Qu,Qi Cheng,Shengpu Tang,Minjia Zhang,Xiaoming Huo*

Main category: cs.LG

TL;DR: SFPO通过引入慢-快策略优化框架，解决了现有强化学习算法在早期训练中不稳定的问题，从而提高了LLM推理的稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习算法（如GRPO）在早期训练中存在梯度不稳定和探索效率低的问题，这阻碍了LLM推理能力的提升。

Method: SFPO框架将每一步分解为三个阶段：1. 短的快速轨迹（内循环）；2. 移动机制（控制偏离）；3. 慢速修正。这种“先移动后更新”的设计保持了目标和轨迹生成过程不变，使得SFPO可以与现有的策略梯度流程兼容。

Result: SFPO在推理RL训练的稳定性和收敛速度方面取得了显著的改进。与GRPO相比，SFPO在数学推理基准测试中的平均得分高出2.80分，所需的轨迹生成次数减少了4.93倍，并能以4.19倍的加速节省时间来达到GRPO的最佳准确率。

Conclusion: SFPO是一种简单而有效的框架，通过分解训练步骤，解决了现有强化学习算法在LLM推理中的局限性，并在实验中证明了其优越的性能。

Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in
large language models (LLMs). Yet on-policy algorithms such as Group Relative
Policy Optimization (GRPO) often suffer in early training: noisy gradients from
low-quality rollouts lead to unstable updates and inefficient exploration. We
introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient
framework to address these limitations via decomposing each step into three
stages: a short fast trajectory of inner steps on the same batch, a reposition
mechanism to control off-policy drift, and a final slow correction. This
reposition-before-update design preserves the objective and rollout process
unchanged, making SFPO plug-compatible with existing policy-gradient pipelines.
Extensive experiments demonstrate that SFPO consistently improves stability,
reduces rollouts, and accelerates convergence of reasoning RL training.
Specifically, it outperforms GRPO by up to 2.80 points in average on math
reasoning benchmarks. It also achieves up to 4.93\texttimes{} fewer rollouts
and a 4.19\texttimes{} reduction in wall-clock time to match GRPO's best
accuracy.

</details>


### [723] [THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series](https://arxiv.org/abs/2510.03911)
*Yadav Mahesh Lorik,Kaushik Sarveswaran,Nagaraj Sundaramahalingam,Aravindakumar Venugopalan*

Main category: cs.LG

TL;DR: THEMIS是一个利用预训练知识的新型时间序列异常检测框架，通过提取Chronos模型的嵌入并结合局部异常因子和谱分解等技术来检测异常，在MSL数据集上达到SOTA，并在SMAP和SWAT数据集上表现良好，具有模型鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据具有季节性、趋势、噪声和概念漂移等特点，使得定义“正常”行为变得困难。此外，异常类型多样、数据不平衡、维度增加、实时性要求、阈值设定和结果可解释性等问题，都对异常检测提出了挑战。

Method: THEMIS框架提取Chronos时间序列基础模型的嵌入，并利用局部异常因子（Local Outlier Factor）和谱分解（Spectral Decomposition）等异常检测技术处理自相似性矩阵，以发现数据中的异常。

Result: 在MSL数据集上实现了SOTA（State-of-the-Art）的性能，并在SMAP和SWAT数据集上表现出很强的竞争力。THEMIS的性能超越了专门为异常检测训练的模型，并且天然具备超参数鲁棒性和可解释性。

Conclusion: 预训练的基础模型表征可以用于高效且适应性强的时间序列异常检测。

Abstract: Time series anomaly detection forms a very crucial area in several domains
but poses substantial challenges. Due to time series data possessing
seasonality, trends, noise, and evolving patterns (concept drift), it becomes
very difficult to set a general notion of what constitutes normal behavior.
Anomalies themselves could be varied, ranging from a single outlier to
contextual or collective anomalies, and are normally very rare; hence, the
dataset is largely imbalanced. Additional layers of complexities arise due to
the problems of increased dimensionality of modern time series, real-time
detection criteria, setting up appropriate detection thresholds, and arriving
at results that are interpretable. To embrace these multifaceted challenges,
very strong, flexible, and interpretable approaches are required. This paper
presents THEMIS, a new framework for time series anomaly detection that
exploits pretrained knowledge from foundation models. THEMIS extracts
embeddings from the encoder of the Chronos time series foundation model and
applies outlier detection techniques like Local Outlier Factor and Spectral
Decomposition on the self-similarity matrix, to spot anomalies in the data. Our
experiments show that this modular method achieves SOTA results on the MSL
dataset and performs quite competitively on the SMAP and SWAT$^*$ datasets.
Notably, THEMIS exceeds models trained specifically for anomaly detection,
presenting hyperparameter robustness and interpretability by default. This
paper advocates for pretrained representations from foundation models for
performing efficient and adaptable anomaly detection for time series data.

</details>


### [724] [Generalized Fitted Q-Iteration with Clustered Data](https://arxiv.org/abs/2510.03912)
*Liyuan Hu,Jitao Wang,Zhenke Wu,Chengchun Shi*

Main category: cs.LG

TL;DR: 本文提出一种广义FQI算法，用于处理医疗等领域中具有类内相关性的聚类数据，该算法在理论和实证上均优于标准FQI。


<details>
  <summary>Details</summary>
Motivation: 医疗等应用中常见的聚类数据存在类内相关性，标准FQI算法无法有效处理。

Method: 提出一种广义FQI算法，将广义估计方程（GEE）融入策略学习，以处理类内相关性。

Result: 理论上证明了所提出的Q函数和策略估计量在相关结构正确指定时的最优性，以及在相关结构错误指定时的一致性。实证结果显示，与标准FQI相比，广义FQI的平均悔 sebesar 减少了一半。

Conclusion: 所提出的广义FQI算法能够有效处理聚类数据中的类内相关性，并在理论和实证上均优于标准FQI算法。

Abstract: This paper focuses on reinforcement learning (RL) with clustered data, which
is commonly encountered in healthcare applications. We propose a generalized
fitted Q-iteration (FQI) algorithm that incorporates generalized estimating
equations into policy learning to handle the intra-cluster correlations.
Theoretically, we demonstrate (i) the optimalities of our Q-function and policy
estimators when the correlation structure is correctly specified, and (ii)
their consistencies when the structure is mis-specified. Empirically, through
simulations and analyses of a mobile health dataset, we find the proposed
generalized FQI achieves, on average, a half reduction in regret compared to
the standard FQI.

</details>


### [725] [On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks](https://arxiv.org/abs/2510.03923)
*Mingsong Yan,Charles Kulick,Sui Tang*

Main category: cs.LG

TL;DR: 连续深度图神经网络（GNN-ODE）是GNN和神经ODE的结合，提供了一种可扩展的图动力学建模框架。本文研究了具有时变参数的GNN-ODE在无限节点极限下的收敛性，并引入了Graphon-NDE作为其无限节点极限，证明了GNN-ODE解向Graphon-NDE解的轨迹收敛性，并推导了在两种图采样下的显式收敛率。此外，还建立了模型的大小可迁移性界限，为在不同大小图上传输模型提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 研究连续深度图神经网络（GNN-ODE）在无限节点极限下的收敛性和大小可迁移性，为理解和应用该模型提供理论基础。

Method: 引入Graphon-ODE作为GNN-ODE的无限节点极限，并证明其适定性。利用Graphon理论和动力系统工具，证明GNN-ODE解向Graphon-ODE解的轨迹收敛性，并推导了收敛率。推导了大小可迁移性界限。

Result: 证明了GNN-ODE解向Graphon-ODE解的轨迹收敛性，并在两种图采样（加权图和无权图）下推导了显式收敛率。建立了大小可迁移性界限，为模型在不同尺寸图上传输提供了理论依据。

Conclusion: GNN-ODE在无限节点极限下表现出良好的收敛性和大小可迁移性，为在不同大小的图上应用该模型提供了理论支持。

Abstract: Continuous-depth graph neural networks, also known as Graph Neural
Differential Equations (GNDEs), combine the structural inductive bias of Graph
Neural Networks (GNNs) with the continuous-depth architecture of Neural ODEs,
offering a scalable and principled framework for modeling dynamics on graphs.
In this paper, we present a rigorous convergence analysis of GNDEs with
time-varying parameters in the infinite-node limit, providing theoretical
insights into their size transferability. To this end, we introduce Graphon
Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of
GNDEs and establish their well-posedness. Leveraging tools from graphon theory
and dynamical systems, we prove the trajectory-wise convergence of GNDE
solutions to Graphon-NDE solutions. Moreover, we derive explicit convergence
rates under two deterministic graph sampling regimes: (1) weighted graphs
sampled from smooth graphons, and (2) unweighted graphs sampled from
$\{0,1\}$-valued (discontinuous) graphons. We further establish size
transferability bounds, providing theoretical justification for the practical
strategy of transferring GNDE models trained on moderate-sized graphs to
larger, structurally similar graphs without retraining. Numerical experiments
using synthetic and real data support our theoretical findings.

</details>


### [726] [Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models](https://arxiv.org/abs/2510.04146)
*Minseo Kim,Coleman Hooper,Aditya Tomar,Chenfeng Xu,Mehrdad Farajtabar,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: DLMs在并行生成文本方面表现出比ARMs更高的算术强度，但难以扩展到长上下文；通过块状解码可以解决这个问题。在批量推理方面，ARMs的吞吐量更高。减少DLM的采样步骤可以提高其推理速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在各种自然语言处理（NLP）任务中表现出色，但自回归语言模型（ARMs）的顺序生成方式导致其算术强度较低。扩散语言模型（DLMs）作为一种并行生成文本的替代架构出现，但其性能优势尚未完全明晰。

Method: 通过理论分析和性能分析数据，对ARMs和DLMs的性能特征进行全面研究，分析它们之间的权衡。研究了DLMs的块状解码以及批量推理的性能表现，并指出了加速DLM推理的机会。

Result: DLMs的算术强度高于ARMs，但难以扩展到长上下文。采用块状解码的DLMs可以提高算术强度并能很好地扩展到长上下文。在批量推理方面，ARMs的吞吐量更高。通过减少采样步骤可以显著提高DLM的推理速度。

Conclusion: DLMs通过并行处理提供了更高的算术强度，但需要块状解码来解决长上下文扩展性问题。ARMs在批量推理方面具有优势。通过优化采样步骤，DLMs的推理速度可以得到显著提升，有望在实际应用中超越ARMs。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a
broad range of Natural Language Processing (NLP) tasks, including document
processing and coding. Autoregressive Language Models (ARMs), which generate
tokens sequentially conditioned on all previous tokens, have been the
predominant paradigm for LLMs. However, while these networks have achieved high
accuracy across a range of downstream tasks, they exhibit low arithmetic
intensity due to the inherent sequential dependency with next-token prediction.
Recently, Diffusion Language Models (DLMs) have emerged as a promising
alternative architecture. DLMs generate output text in parallel, breaking the
limitations of sequential dependency. However, the performance implications of
DLMs relative to commonly deployed ARMs are not fully understood. In this work,
we present a comprehensive performance study analyzing the performance
characteristics of ARMs and DLMs, using both theoretical analysis and profiling
data to characterize the trade-offs between these approaches. We illustrate
that although DLMs exhibit higher arithmetic intensity compared to ARMs because
of their capability to utilize parallelism across sequence lengths, they fail
to scale effectively to longer contexts. We then explore DLMs with block-wise
decoding, outlining how this approach allows for increased arithmetic
intensity, while still scaling well to long contexts (similar to ARMs). We also
show interesting trade-offs for batched inference, where we find that ARMs
exhibit superior throughput, as they benefit more from parallelism across
sequences in the batch. Finally, we highlight opportunities for accelerating
DLM inference, and, in particular, highlight the importance of reducing the
number of sampling steps for allowing open-source DLMs to provide improved
latency relative to ARMs.

</details>


### [727] [On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection](https://arxiv.org/abs/2510.03944)
*Weiqing He,Xiang Li,Tianqi Shang,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: LLM 生成的文本可以通过统计信号进行水印检测，而拟合优度（GoF）检验是检测水印的自然工具。本文系统评估了八种 GoF 检验在三种流行的水印方案中的表现，并发现 GoF 检验可以提高检测能力和鲁棒性，尤其是在文本重复的情况下。


<details>
  <summary>Details</summary>
Motivation: LLM 生成内容可能存在真实性和完整性问题，需要通过水印技术来验证内容来源。

Method: 系统性地评估了八种拟合优度（GoF）检验在三种流行的水印方案、三个开源 LLM、两个数据集、不同生成温度和多种后编辑方法下的表现。

Result: 发现通用的 GoF 检验可以提高水印检测的检测能力和鲁棒性。特别地，在 LLM 生成文本中常见的文本重复情况下，GoF 检验具有现有方法未利用的独特优势。

Conclusion: 经典的 GoF 检验是 LLM 水印检测中简单、强大且未被充分利用的工具。

Abstract: Large language models (LLMs) raise concerns about content authenticity and
integrity because they can generate human-like text at scale. Text watermarks,
which embed detectable statistical signals into generated text, offer a
provable way to verify content origin. Many detection methods rely on pivotal
statistics that are i.i.d. under human-written text, making goodness-of-fit
(GoF) tests a natural tool for watermark detection. However, GoF tests remain
largely underexplored in this setting. In this paper, we systematically
evaluate eight GoF tests across three popular watermarking schemes, using three
open-source LLMs, two datasets, various generation temperatures, and multiple
post-editing methods. We find that general GoF tests can improve both the
detection power and robustness of watermark detectors. Notably, we observe that
text repetition, common in low-temperature settings, gives GoF tests a unique
advantage not exploited by existing methods. Our results highlight that classic
GoF tests are a simple yet powerful and underused tool for watermark detection
in LLMs.

</details>


### [728] [What Is The Performance Ceiling of My Classifier? Utilizing Category-Wise Influence Functions for Pareto Frontier Analysis](https://arxiv.org/abs/2510.03950)
*Shahriar Kabir Nahin,Wenxiao Xiao,Joshua Liu,Anshuman Chhabra,Hongfu Liu*

Main category: cs.LG

TL;DR: 本篇论文提出了一种名为“类别影响函数”的新方法，用于评估和提升机器学习模型的性能上限，特别关注在不牺牲任何类别性能的情况下，使所有类别都能得到改进。


<details>
  <summary>Details</summary>
Motivation: 现有数据中心学习方法主要关注如何改进模型，但忽略了模型性能的潜在上限。本文旨在探索模型性能的“天花板”，并提出一种能在不牺牲任何类别性能的情况下，同时提升所有类别性能的方法。

Method: 提出类别影响函数（category-wise influence functions），并引入影响向量（influence vector）来量化每个训练样本对所有类别的具体影响。基于影响向量，设计了一个判断模型是否仍有提升空间的标准，并构建了一个基于线性规划（linear programming）的样本重加权框架，以实现帕累托改进（Pareto improvements）。

Result: 通过在合成数据集、视觉和文本基准上的广泛实验，证明了该方法在估计和实现模型在多个类别上的性能提升方面是有效的。

Conclusion: 本研究提出的类别影响函数和基于线性规划的样本重加权框架，能够有效地估计并实现模型在不损害任何类别性能的前提下，对所有类别性能进行提升，从而探索并触及模型的性能上限。

Abstract: Data-centric learning seeks to improve model performance from the perspective
of data quality, and has been drawing increasing attention in the machine
learning community. Among its key tools, influence functions provide a powerful
framework to quantify the impact of individual training samples on model
predictions, enabling practitioners to identify detrimental samples and retrain
models on a cleaner dataset for improved performance. However, most existing
work focuses on the question: "what data benefits the learning model?" In this
paper, we take a step further and investigate a more fundamental question:
"what is the performance ceiling of the learning model?" Unlike prior studies
that primarily measure improvement through overall accuracy, we emphasize
category-wise accuracy and aim for Pareto improvements, ensuring that every
class benefits, rather than allowing tradeoffs where some classes improve at
the expense of others. To address this challenge, we propose category-wise
influence functions and introduce an influence vector that quantifies the
impact of each training sample across all categories. Leveraging these
influence vectors, we develop a principled criterion to determine whether a
model can still be improved, and further design a linear programming-based
sample reweighting framework to achieve Pareto performance improvements.
Through extensive experiments on synthetic datasets, vision, and text
benchmarks, we demonstrate the effectiveness of our approach in estimating and
achieving a model's performance improvement across multiple categories of
interest.

</details>


### [729] [Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention](https://arxiv.org/abs/2510.04304)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Wave-PDE Nets是一种新的神经网络架构，通过可微分模拟二阶波动方程来处理信息。它使用可训练的参数（空间速度c(x)和阻尼γ(x)）来模拟信息的传播，并利用基于FFT的光谱求解器实现高效计算。该模型在语言和视觉任务上表现优于Transformer，同时提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的神经网络架构Wave-PDE Nets，旨在提供一种高效且具有物理归纳偏置的替代方案，以应对Transformer和一阶状态空间模型在处理序列数据时的局限性。

Method: Wave-PDE Nets的核心是一种可微分的二阶波动方程模拟。每一层将隐藏状态视为连续场，在具有可训练的空间速度c(x)和阻尼γ(x)的介质中传播。采用基于FFT的光谱求解器，以O(nlogn)的时间复杂度实现传播。

Result: Wave-PDE Nets在语言和视觉基准测试中，性能与Transformer相当或更优，同时在实际运行时间上提高了30%，峰值内存使用量减少了25%。消融研究证实了辛积分和光谱拉普拉斯算子对于稳定性和性能的关键作用。

Conclusion: Wave-PDE Nets作为一种计算高效、鲁棒且具有强大物理归纳偏置的神经网络架构，在各种基准测试中展现出优于甚至媲美Transformer的性能，同时显著提高了效率。其学习到的物理参数揭示了模型能够掌握直观的信息传播策略。

Abstract: We introduce Wave-PDE Nets, a neural architecture whose elementary operation
is a differentiable simulation of the second-order wave equation. Each layer
propagates its hidden state as a continuous field through a medium with
trainable spatial velocity c(x) and damping {\gamma}(x). A symplectic spectral
solver based on FFTs realises this propagation in O(nlog n) time. This
oscillatory, global mechanism provides a powerful alternative to attention and
first-order state-space models. We prove that a single Wave-PDE layer is a
universal approximator. On language and vision benchmarks, Wave-PDE Nets match
or exceed Transformer performance while demonstrating superior practical
efficiency, reducing wall-clock time by up to 30% and peak memory by 25%.
Ablation studies confirm the critical role of symplectic integration and a
spectral Laplacian for stability and performance. Visualizations of the learned
physical parameters reveal that the model learns intuitive strategies for
information propagation. These results position Wave-PDE Nets as a
computationally efficient and robust architecture with a strong physical
inductive bias.

</details>


### [730] [Optimizing Resources for On-the-Fly Label Estimation with Multiple Unknown Medical Experts](https://arxiv.org/abs/2510.03954)
*Tim Bary,Tiffanie Godelaine,Axel Abels,Benoît Macq*

Main category: cs.LG

TL;DR: 该研究提出了一种自适应方法，用于在医疗筛查项目中实时聚合专家意见，以提高标注效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的算法无法满足在医疗筛查流程中实时聚合专家意见的需求，尤其是在数据持续到来且专家能力未知的情况下。

Method: 提出了一种自适应方法，该方法支持对传入数据的即时标注，无需先验知识，并能根据实例的潜在难度动态查询更多专家，直到达到置信阈值。

Result: 在三个多标注者分类数据集上进行了评估，结果表明该方法可以将专家查询数量减少多达 50%，同时获得与非自适应基线相当的准确性。

Conclusion: 所提出的自适应方法能够有效提高标注效率和准确性，并减少标注开销，适用于医疗筛查等场景。

Abstract: Accurate ground truth estimation in medical screening programs often relies
on coalitions of experts and peer second opinions. Algorithms that efficiently
aggregate noisy annotations can enhance screening workflows, particularly when
data arrive continuously and expert proficiency is initially unknown. However,
existing algorithms do not meet the requirements for seamless integration into
screening pipelines. We therefore propose an adaptive approach for real-time
annotation that (I) supports on-the-fly labeling of incoming data, (II)
operates without prior knowledge of medical experts or pre-labeled data, and
(III) dynamically queries additional experts based on the latent difficulty of
each instance. The method incrementally gathers expert opinions until a
confidence threshold is met, providing accurate labels with reduced annotation
overhead. We evaluate our approach on three multi-annotator classification
datasets across different modalities. Results show that our adaptive querying
strategy reduces the number of expert queries by up to 50% while achieving
accuracy comparable to a non-adaptive baseline. Our code is available at
https://github.com/tbary/MEDICS

</details>


### [731] [Early-Warning of Thunderstorm-Driven Power Outages with a Two-Stage Machine Learning Model](https://arxiv.org/abs/2510.03959)
*Iryna Stanishevska*

Main category: cs.LG

TL;DR: 开发了一个仅使用开源数据（EAGLE-I 和 METAR）的夏季雷暴相关停电的 24-48 小时预警模型。


<details>
  <summary>Details</summary>
Motivation: 雷暴驱动的停电难以预测，因为大多数风暴不会造成损害，对流过程发生得又快又混乱，并且现有的公共数据既嘈杂又不完整。

Method: 使用 EAGLE-I 和 METAR 数据，通过参数特定的克里金法（kriging）和过量提取（overdrafting）来保留对流微信号；构建了捕捉严重对流（如湿气对流、风向转变和气压下降）的因果时空特征；采用两阶段模型设计（逻辑门控和 LSTM 回归器）来限制常规时段并减少噪声影响。

Result: 在测试样本中，所提出的两阶段模型比基线模型能检测到更多的参考峰值，例如在 +/-48 小时窗口中，两阶段模型检测到 3/4 的峰值，而基线模型为 2/4，F1 分数分别为 66.7% 和 57.1%，仅多一个误报。在峰值附近，两阶段模型在 +/-0-12 小时窗口中显示出适度的幅度增益（cMASE 降低 2-3%），在 +/-6-12 小时窗口中显示出 9-13% 的提升，但在 +/-36-48 小时窗口中幅度略有下降（约 3-4%）。SHAP 分析证实了湿气对流和风/阵风前兆的重要性。

Conclusion: 尽管存在开源数据的噪声，但所提出的特征驱动的管道能够为雷暴停电提供可操作的、以事件为中心的预警。

Abstract: Thunderstorm-driven outages are difficult to predict because most storms do
not cause damage, convective processes occur rapidly and chaotically, and the
available public data are both noisy and incomplete. We develop a 24-48 h
early-warning model for summer, thunderstorm-related outages in Michigan using
only open sources (EAGLE-I for ground truth; METAR for weather). We use the
publicly released EAGLE-I outage dataset (2014-2022), maintained by Oak Ridge
National Laboratory for the U.S. Department of Energy. The pipeline preserves
convective micro-signals from a sparse station network via parameter-specific
kriging with hourly variograms and targeted overdrafting to retain extremes,
and builds causal spatio-temporal features (lags/rolling statistics; k-NN/IDW
spatial aggregates) capturing precursors of severe convection (moisture
advection, wind shifts, and pressure drops). The two-stage model design,
combining a logistic gate and an LSTM regressor, limits routine periods and
reduces noise exposure. The study uses event-centric metrics (cluster-based
hits/misses/false alarms) and peak-conditional MASE (cMASE) in +/-Delta-hour
windows around state-level peaks (>= 50,000), with uncertainty quantified by
hourly moving-block bootstrap.
  On the test sample, Two-Stage detects more reference peaks across all windows
(e.g., at +/-48 h it records 3/4 vs. 2/4; F1 66.7% vs. 57.1%) with one extra
false alarm. Near peaks, it shows modest amplitude gains (2-3% lower cMASE at
+/-0-12 h; bootstrap medians +9-13% at +/-6-12 h) but small losses at +/-36-48
h (~3-4%). Overall, errors are comparable to the one-step LSTM baseline. SHAP
analysis confirms moisture-advection and wind/gust precursors, underscoring the
value of the feature engineering. Despite open-data noise, the feature-driven
pipeline yields actionable, event-focused early warnings for thunderstorm
outages.

</details>


### [732] [SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data](https://arxiv.org/abs/2510.03962)
*Hanzhe Wei,Jiajun Wu,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.LG

TL;DR: SPEAR是一种利用LLM进行时间序列异常检测的新方法，通过软提示和量化来处理可变长度序列和基于上下文的异常。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列异常检测方法难以处理可变长度序列和基于上下文的异常，而LLM的出现为解决这些问题提供了新机会。

Method: SPEAR将时间序列数据量化并转换为输入嵌入，与可学习的软提示嵌入结合，然后输入到冻结的LLM中。通过交叉熵损失迭代更新软提示，使模型适应时间序列异常检测任务。

Result: 实验结果表明，软提示能有效提高LLM在时间序列异常检测下游任务中的性能。

Conclusion: 软提示能够有效地将LLM适配于时间序列任务，而量化确保了LLM对离散序列的最佳处理能力，从而提高了LLM在时间序列异常检测任务上的表现。

Abstract: Time series anomaly detection plays a crucial role in a wide range of fields,
such as healthcare and internet traffic monitoring. The emergence of large
language models (LLMs) offers new opportunities for detecting anomalies in the
ubiquitous time series data. Traditional approaches struggle with
variable-length time series sequences and context-based anomalies. We propose
Soft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage
LLMs for anomaly detection with soft prompts and quantization. Our methodology
involves quantizing and transforming the time series data into input embeddings
and combining them with learnable soft prompt embeddings. These combined
embeddings are then fed into a frozen LLM. The soft prompts are updated
iteratively based on a cross-entropy loss, allowing the model to adapt to time
series anomaly detection. The use of soft prompts helps adapt LLMs effectively
to time series tasks, while quantization ensures optimal handling of sequences,
as LLMs are designed to handle discrete sequences. Our experimental results
demonstrate that soft prompts effectively increase LLMs' performance in
downstream tasks regarding time series anomaly detection.

</details>


### [733] [Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions](https://arxiv.org/abs/2510.04417)
*Wenyuan Zhao,Adithya Balachandran,Chao Tian,Paul Pu Liang*

Main category: cs.LG

TL;DR: 本研究提出了一种名为高斯部分信息分解（GPID）的新方法，用于更高效、更准确地量化多模态数据中不同信息源之间的关系，并通过学习信息保持编码器将其推广到非高斯数据。


<details>
  <summary>Details</summary>
Motivation: 现有的部分信息分解（PID）方法在处理连续和高维模态时成本高昂且不准确，因为它们依赖于对联合分布进行优化，而该联合分布受到成对概率分布估计的约束。

Method: 提出了一种新的基于梯度的算法，通过对底层优化问题进行替代公式化来提高GPID的计算效率。为了将适用性推广到非高斯数据，研究人员学习了信息保持编码器，将任意输入分布的随机变量转换为成对高斯随机变量。

Result: 在各种合成示例中的实证验证表明，所提出的方法比现有基线提供了更准确、更有效的PID估计。在实际的多模态基准测试中，该方法在量化多模态数据集中的PID和选择高性能模型方面显示出了实用性。

Conclusion: 该研究提出了一种新颖且高效的GPID方法，并将其推广到非高斯数据，在多模态数据分析中具有广泛的应用前景。

Abstract: The study of multimodality has garnered significant interest in fields where
the analysis of interactions among multiple information sources can enhance
predictive modeling, data fusion, and interpretability. Partial information
decomposition (PID) has emerged as a useful information-theoretic framework to
quantify the degree to which individual modalities independently, redundantly,
or synergistically convey information about a target variable. However,
existing PID methods depend on optimizing over a joint distribution constrained
by estimated pairwise probability distributions, which are costly and
inaccurate for continuous and high-dimensional modalities. Our first key
insight is that the problem can be solved efficiently when the pairwise
distributions are multivariate Gaussians, and we refer to this problem as
Gaussian PID (GPID). We propose a new gradient-based algorithm that
substantially improves the computational efficiency of GPID based on an
alternative formulation of the underlying optimization problem. To generalize
the applicability to non-Gaussian data, we learn information-preserving
encoders to transform random variables of arbitrary input distributions into
pairwise Gaussian random variables. Along the way, we resolved an open problem
regarding the optimality of joint Gaussian solutions for GPID. Empirical
validation in diverse synthetic examples demonstrates that our proposed method
provides more accurate and efficient PID estimates than existing baselines. We
further evaluate a series of large-scale multimodal benchmarks to show its
utility in real-world applications of quantifying PID in multimodal datasets
and selecting high-performing models.

</details>


### [734] [What Can You Do When You Have Zero Rewards During RL?](https://arxiv.org/abs/2510.03971)
*Jatin Prakash,Anirudh Buvanesh*

Main category: cs.LG

TL;DR: 基于奖励的强化学习在处理复杂推理任务时，当基础模型无法生成正确答案时，会遇到零奖励瓶颈。尽管有多种方法尝试解决此问题，但我们发现只有通过在训练集中添加更简单的样本（数据中心干预）才能有效克服这一瓶颈，而无需修改强化学习算法本身。


<details>
  <summary>Details</summary>
Motivation: 在复杂的推理任务中，基于奖励的强化学习（RL）虽然有效，但当基础模型无法生成正确答案时，会面临零奖励瓶颈，导致学习停滞。本研究旨在探究现有方法在克服这一瓶颈方面的局限性，并寻找有效的解决方案。

Method: 1. 评估了包括稠密奖励、多样性激励和改进信用分配在内的几种现有方法在图搜索任务上的表现。 2. 引入了一种数据中心干预方法，即向训练集中添加更简单的样本。 3. 在现有方法均无法克服零奖励瓶颈的情况下，测试了数据中心干预方法的效果。 4. 由于部分基线方法缺少官方实现，自行开发并发布了相关实现，以便进行详细的失败模式分析。

Result: 现有的强化学习方法（包括稠密奖励、多样性激励和改进信用分配）在基础模型无法生成正确答案时，均无法克服零奖励瓶颈。然而，通过在训练集中添加更简单的样本，即使在初始零奖励的情况下，模型也能最终解决原始的难题，且无需修改强化学习算法。

Conclusion: 数据中心干预（例如，向训练集中添加更简单的样本）是克服强化学习中零奖励瓶颈的一种有效方法，即使在基础模型最初无法生成正确答案的情况下也是如此。此方法比修改强化学习算法本身更有效。

Abstract: Reinforcement learning (RL) with outcome-based rewards has proven effective
for improving large language models (LLMs) on complex reasoning tasks. However,
its success often depends on the base model occasionally sampling correct
solutions. When no correct solutions are sampled, training encounters a
zero-reward barrier where learning stalls due to zero gradients. We study this
scenario through the graph search task introduced in Bachmann et al. (2024) and
evaluate recent methods that incorporate desirable components such as dense
rewards, diversity incentives, and improved credit assignment. Our experiments
show that none of these approaches overcome the zero-reward barrier if the base
model never produces a correct answer. In contrast, we find that a simple
data-centric intervention of adding easier samples to the training set enables
the model to eventually solve the original hard task despite starting from zero
reward. Importantly, this succeeds without modifying the RL algorithm itself.
Because official implementations of several baselines were unavailable, we
developed our own, which allowed us to conduct a detailed analysis of their
failure modes. We release these implementations to support further research at:
https://github.com/rl4reasoning/rl-baselines

</details>


### [735] [Beyond Softmax: A New Perspective on Gradient Bandits](https://arxiv.org/abs/2510.03979)
*Emerson Melo,David Müller*

Main category: cs.LG

TL;DR: 将离散选择模型与在线学习和多臂老虎机理论联系起来，提出具有亚线性遗憾界限的新型算法，并扩展了梯度老虎机方法以适应相关学习动态。


<details>
  <summary>Details</summary>
Motivation: 建立离散选择模型与在线学习和多臂老虎机理论之间的联系。

Method: 提出一个广泛的算法家族，包括Exp3作为特例，并具有亚线性遗憾界限。开发源自广义嵌套logit模型的新型对抗性老虎机算法。引入广义梯度老虎机算法，放宽了softmax的独立性假设，允许动作之间存在相关学习动态。

Result: 通过数值实验证明了所提出算法在随机老虎机设置下的实际有效性，这些算法结合了灵活的模型规范和通过闭式采样概率实现的高计算效率。

Conclusion: 所提出的算法通过结合灵活的模型规范和计算效率，成功地将离散选择模型与在线学习和多臂老虎机理论联系起来，并扩展了梯度老虎机方法的适用范围。

Abstract: We establish a link between a class of discrete choice models and the theory
of online learning and multi-armed bandits. Our contributions are: (i)
sublinear regret bounds for a broad algorithmic family, encompassing Exp3 as a
special case; (ii) a new class of adversarial bandit algorithms derived from
generalized nested logit models \citep{wen:2001}; and (iii)
\textcolor{black}{we introduce a novel class of generalized gradient bandit
algorithms that extends beyond the widely used softmax formulation. By relaxing
the restrictive independence assumptions inherent in softmax, our framework
accommodates correlated learning dynamics across actions, thereby broadening
the applicability of gradient bandit methods.} Overall, the proposed algorithms
combine flexible model specification with computational efficiency via
closed-form sampling probabilities. Numerical experiments in stochastic bandit
settings demonstrate their practical effectiveness.

</details>


### [736] [ICEPool: Enhancing Graph Pooling Networks with Inter-cluster Connectivity](https://arxiv.org/abs/2510.03987)
*Michael Yang*

Main category: cs.LG

TL;DR: ICEPool是一个新颖的图神经网络池化框架，通过增强集群间连接性来提升图表示的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络池化模型在处理图结构数据时，虽然在集群分配和粗化策略方面表现出色，但往往忽略了集群间的关系。

Method: 提出了一种名为ICEPool（Inter-cluster Connectivity Enhancement Pooling）的新型分层池化框架，旨在增强模型对集群间连接性的理解，并保持原始图的结构完整性。ICEPool可与多种基于池化的图神经网络模型兼容。

Result: ICEPool可以与现有模型结合，发挥各自优势，从而获得更全面、更鲁棒的图级表示。理论分析表明ICEPool在图重建方面具有学习集群间关系的能力。实验结果证明了ICEPool的兼容性和提升现有图神经网络模型性能的潜力。

Conclusion: ICEPool通过增强集群间连接性，有效弥补了传统图神经网络池化模型的不足，提升了图表示的性能和鲁棒性。

Abstract: Hierarchical Pooling Models have demonstrated strong performance in
classifying graph-structured data. While numerous innovative methods have been
proposed to design cluster assignments and coarsening strategies, the
relationships between clusters are often overlooked. In this paper, we
introduce Inter-cluster Connectivity Enhancement Pooling (ICEPool), a novel
hierarchical pooling framework designed to enhance model's understanding of
inter-cluster connectivity and ability of preserving the structural integrity
in the original graph. ICEPool is compatible with a wide range of pooling-based
GNN models. The deployment of ICEPool as an enhancement to existing models
effectively combines the strengths of the original model with ICEPool's
capability to emphasize the integration of inter-cluster connectivity,
resulting in a more comprehensive and robust graph-level representation.
Moreover, we make theoretical analysis to ICEPool's ability of graph
reconstruction to demonstrate its effectiveness in learning inter-cluster
relationship that is overlooked by conventional models. Finally, the
experimental results show the compatibility of ICEPool with wide varieties of
models and its potential to boost the performance of existing graph neural
network architectures.

</details>


### [737] [Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data](https://arxiv.org/abs/2510.03988)
*Hoang Anh Just,Myeongseob Ko,Ruoxi Jia*

Main category: cs.LG

TL;DR: 通过多教师选择和局部自然度，改进了从教师模型蒸馏长推理轨迹到学生模型的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的通过单一教师蒸馏长推理轨迹到学生模型的方法在多教师场景下效果不佳，需要探索更好的响应选择策略。

Method: 提出局部自然度（Local Naturalness）来衡量学生模型在局部推理步骤上的对数概率，并将其应用于教师选择和响应选择。

Result: 局部自然度在多教师响应选择上，将32B学生模型的数学基准准确率提高了9.4个百分点，并优于单一最佳教师的训练效果。

Conclusion: 局部数据质量评估和数据混合对于更有效的推理蒸馏至关重要。

Abstract: Distilling long reasoning traces (10K+ tokens) from stronger teacher models
into smaller student LLMs via SFT has emerged as a standard paradigm. This
approach is practical and efficient: it leverages the ease of generating
abundant reasoning data from stronger models and provides a direct, data-driven
way to teach less capable models better reasoning. While previous work has
largely focused on prompt selection with responses from a single teacher, the
equally important problem of choosing the best response when multiple teacher
outputs are available for a single prompt remains underexplored. This challenge
becomes important in a multi-teacher setting, where different students may
benefit from the outputs of different teachers. This paper fills that gap with
a systematic study of response selection for reasoning distillation. We first
show that the current method, which picks responses the student assigns the
highest global log-probability (global naturalness), fails when responses come
from multiple teachers, i.e., global naturalness no longer correlates with
downstream performance, especially as the reasoning traces from strong teachers
become longer. To overcome this problem, we introduce Local Naturalness, which
measures the student's log-probabilities over short, sequential reasoning steps
conditioned only on a small local window. Local Naturalness enables two
applications: 1) Teacher Selection: Aggregating local scores across prompts
reliably identifies the most helpful teacher. 2) Response Selection from a
Multiple Teachers: When mixing answers from many teachers, Local Naturalness
boosts a 32B student's accuracy on math benchmarks by 9.4pp over global
selection, also surpassing the performance achieved by training on data from
the single best teacher. These results highlight the power of localized data
quality evaluation and data mixing for more effective reasoning distillation.

</details>


### [738] [LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning](https://arxiv.org/abs/2510.04573)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Nicklas Majamaki,Navdeep Jaitly,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LLM的链式思考(CoT)推理能力受限于其自回归解码方式，难以进行全局性的反思和优化。为解决此问题，本文提出LaDiR（Latent Diffusion Reasoner）框架，结合变分自编码器（VAE）和潜在扩散模型，将文本推理过程编码为“思维块”的连续潜在表示，并利用扩散模型进行迭代式优化和并行生成。实验表明，LaDiR在数学推理和规划任务上显著优于现有方法，在准确性、多样性和可解释性方面均有提升。


<details>
  <summary>Details</summary>
Motivation: LLM的自回归解码限制了其对早期生成内容的全局反思和优化能力，且不利于探索多样化解决方案。现有方法在推理的迭代优化和多样性生成方面存在不足。

Method: 1. 使用变分自编码器（VAE）构建结构化的潜在推理空间，将文本推理步骤编码为“思维块”的潜在表示，以保留语义信息和可解释性。 2. 利用潜在扩散模型，通过带掩码的双向注意力机制，对“思维块”进行去噪和迭代式优化，实现长距离依赖和自适应计算。 3. 实现高效的并行生成，以产生多样化的推理轨迹，并允许模型进行整体规划和修正。

Result: 在数学推理和规划基准测试中，LaDiR在准确性、多样性和可解释性方面均优于现有的自回归、基于扩散和潜在推理方法。

Conclusion: LaDiR框架通过结合VAE和潜在扩散模型，为文本推理提供了一种新的范式，能够实现更优的准确性、多样性和可解释性。

Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through
chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may
limit the ability to revisit and refine earlier tokens in a holistic manner,
which can also lead to inefficient exploration for diverse solutions. In this
paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning
framework that unifies the expressiveness of continuous latent representation
with the iterative refinement capabilities of latent diffusion models for an
existing LLM. We first construct a structured latent reasoning space using a
Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of
thought tokens, preserving semantic information and interpretability while
offering compact but expressive representations. Subsequently, we utilize a
latent diffusion model that learns to denoise a block of latent thought tokens
with a blockwise bidirectional attention mask, enabling longer horizon and
iterative refinement with adaptive test-time compute. This design allows
efficient parallel generation of diverse reasoning trajectories, allowing the
model to plan and revise the reasoning process holistically. We conduct
evaluations on a suite of mathematical reasoning and planning benchmarks.
Empirical results show that LaDiR consistently improves accuracy, diversity,
and interpretability over existing autoregressive, diffusion-based, and latent
reasoning methods, revealing a new paradigm for text reasoning with latent
diffusion.

</details>


### [739] [Incorporating Multivariate Consistency in ML-Based Weather Forecasting with Latent-space Constraints](https://arxiv.org/abs/2510.04006)
*Hang Fan,Yi Xiao,Yongquan Qu,Fenghua Ling,Ben Fei,Lei Bai,Pierre Gentine*

Main category: cs.LG

TL;DR: 数据驱动的机器学习模型在天气预报领域展现出巨大潜力，但现有模型常将再分析数据视为真理并忽略物理耦合和空间结构，导致长期预报模糊且不切实际。本文将模型训练重构为弱约束四维变分数据同化（WC-4DVar）问题，将再分析数据视为不完美的观测，并通过在自编码器（AE）学习的潜在空间中计算损失，有效解决了再分析误差协方差的建模难题。实验证明，该方法在长期预报技能、精细结构保持和物理真实性方面优于传统模型空间损失训练。此外，该框架可扩展至多源异构数据融合，实现统一理论框架下的联合训练。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习天气预报模型将再分析数据视为真理，忽略了物理耦合和空间结构，导致长期预报模糊且不切实际。

Method: 将模型训练视为弱约束四维变分数据同化（WC-4DVar）问题，将再分析数据视为不完美的观测。通过在自编码器（AE）学习的潜在空间中计算损失，处理再分析误差协方差，避免在高维空间中显式建模。将此框架扩展到多源异构数据。

Result: 在潜在空间中进行训练的滚动训练，相比于模型空间损失训练，提高了长期预报技能，并能更好地保持精细尺度结构和物理真实性。

Conclusion: 所提出的基于WC-4DVar的潜在空间约束训练方法，能够有效提高机器学习天气预报模型的长期预测能力，并保持其物理真实性。该框架还能整合多源异构数据，为机器学习天气预报提供了一个更强大、更灵活的解决方案。

Abstract: Data-driven machine learning (ML) models have recently shown promise in
surpassing traditional physics-based approaches for weather forecasting,
leading to a so-called second revolution in weather forecasting. However, most
ML-based forecast models treat reanalysis as the truth and are trained under
variable-specific loss weighting, ignoring their physical coupling and spatial
structure. Over long time horizons, the forecasts become blurry and physically
unrealistic under rollout training. To address this, we reinterpret model
training as a weak-constraint four-dimensional variational data assimilation
(WC-4DVar) problem, treating reanalysis data as imperfect observations. This
allows the loss function to incorporate reanalysis error covariance and capture
multivariate dependencies. In practice, we compute the loss in a latent space
learned by an autoencoder (AE), where the reanalysis error covariance becomes
approximately diagonal, thus avoiding the need to explicitly model it in the
high-dimensional model space. We show that rollout training with latent-space
constraints improves long-term forecast skill and better preserves fine-scale
structures and physical realism compared to training with model-space loss.
Finally, we extend this framework to accommodate heterogeneous data sources,
enabling the forecast model to be trained jointly on reanalysis and
multi-source observations within a unified theoretical formulation.

</details>


### [740] [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
*Qizheng Zhang,Changran Hu,Shubhangi Upasani,Boyuan Ma,Fenglu Hong,Vamsidhar Kamanuru,Jay Rainton,Chen Wu,Mengmeng Ji,Hanchen Li,Urmish Thakker,James Zou,Kunle Olukotun*

Main category: cs.LG

TL;DR: ACE框架通过结构化、增量式更新来解决LLM应用中的上下文适应问题，防止信息丢失，并在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM应用中的上下文适应方法存在简洁性偏见（丢失领域见解）和上下文崩溃（迭代重写导致细节丢失）的问题。

Method: ACE（Agentic Context Engineering）框架将上下文视为不断演进的策略手册，通过生成、反思和策展的模块化过程来积累、优化和组织策略。它采用结构化、增量式更新来防止上下文崩溃，并能与长上下文模型协同工作。

Result: ACE在代理和领域特定（如金融）基准测试中均优于强基线，分别提升了10.6%和8.6%。它能有效适应，无需标记监督，而是利用自然执行反馈。在AppWorld排行榜上，ACE的平均表现与排名第一的生产级代理相当，在更具挑战性的测试集上甚至超越了它，同时还显著降低了适应延迟和部署成本。

Conclusion: 全面的、不断演进的上下文能够实现可扩展、高效且具有低开销的自我改进的LLM系统。

Abstract: Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

</details>


### [741] [Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention](https://arxiv.org/abs/2510.04008)
*Sahil Joshi,Agniva Chowdhury,Amar Kanakamedala,Ekam Singh,Evan Tu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: RACE Attention是一种线性时间复杂度的替代方案，可以处理更长的上下文，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: Softmax Attention在长上下文中的二次时间复杂度限制了其应用，即使使用优化的GPU内核也是如此。FlashAttention也无法处理超过约400万个token的上下文。

Method: RACE Attention用锐化的角度（余弦）相似度替换指数核，并通过随机投影和软局部敏感哈希（LSH）近似注意力输出。

Result: RACE Attention在语言建模、掩码语言建模和文本分类任务中，在保持准确性的同时，减少了运行时间和内存消耗。它可以在NVIDIA GH200 GPU上处理长达1200万个token，在Intel Xeon Gold 5220R CPU上处理长达7500万个token。

Conclusion: RACE Attention为当今的硬件提供了实用的、有理论依据的、用于超长上下文窗口的机制。

Abstract: Softmax Attention has a quadratic time complexity, which becomes prohibitive
to run at long contexts, even with highly optimized GPU kernels. For example,
FlashAttention (an exact, GPU-optimized implementation of Softmax Attention)
cannot complete a single forward-backward pass of a multi-head attention layer
once the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We
introduce RACE Attention, a kernel-inspired alternative to Softmax Attention
that is linear in sequence length and embedding dimension. RACE Attention
replaces the exponential kernel with a sharpened angular (cosine) similarity,
and approximates attention outputs via randomized projections and soft
Locality-Sensitive Hashing (LSH). Across language modeling, masked language
modeling, and text classification, RACE Attention matches the accuracy of
strong baselines while reducing runtime and memory. In a controlled scale test,
it processes up to 12 million tokens during a single forward-backward pass on
an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well
beyond the practical limits of the current state-of-the-art attention
implementations. RACE Attention thus offers a practical, theoretically grounded
mechanism for outrageously long context windows on today's hardware. We hope
that it gets adopted in practice.

</details>


### [742] [Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models](https://arxiv.org/abs/2510.04020)
*Hao Wu,Yuan Gao,Xingjian Shi,Shuaipeng Li,Fan Xu,Fan Zhang,Zhihong Zhu,Weiyan Wang,Xiao Luo,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 该研究提出了一种名为“时空预测即规划”（SFP）的新范式，用于解决物理时空预测中的随机性和非可微分度量挑战。


<details>
  <summary>Details</summary>
Motivation: 为了应对物理时空预测中固有的随机性和非可微分度量带来的双重挑战。

Method: SFP构建了一个新颖的生成世界模型来模拟多样化、高保真的未来状态，实现基于“想象”的环境模拟。在此框架内，基础预测模型充当代理，由基于束搜索的规划算法指导，该算法利用非可微分的领域度量作为奖励信号来探索高回报的未来序列。

Result: 通过迭代自训练，利用识别出的高回报候选序列作为伪标签来持续优化代理策略，从而显著降低了预测误差，并在捕捉极端事件等关键领域度量方面表现出卓越的性能。

Conclusion: SFP通过结合生成世界模型和基于规划的强化学习，有效解决了时空预测中的关键挑战，并在实际应用中取得了显著成果。

Abstract: To address the dual challenges of inherent stochasticity and
non-differentiable metrics in physical spatiotemporal forecasting, we propose
Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in
Model-Based Reinforcement Learning. SFP constructs a novel Generative World
Model to simulate diverse, high-fidelity future states, enabling an
"imagination-based" environmental simulation. Within this framework, a base
forecasting model acts as an agent, guided by a beam search-based planning
algorithm that leverages non-differentiable domain metrics as reward signals to
explore high-return future sequences. These identified high-reward candidates
then serve as pseudo-labels to continuously optimize the agent's policy through
iterative self-training, significantly reducing prediction error and
demonstrating exceptional performance on critical domain metrics like capturing
extreme events.

</details>


### [743] [Multi-Class Support Vector Machine with Differential Privacy](https://arxiv.org/abs/2510.04027)
*Jinseong Park,Yujin Choi,Jaewook Lee*

Main category: cs.LG

TL;DR: 本研究提出了一种名为PMSVM的新型差分隐私多类别支持向量机，解决了现有方法在多类别场景下隐私预算消耗过快的问题，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私（DP）框架在保护机器学习模型数据隐私方面发挥着重要作用。支持向量机（SVM）作为一种广泛使用的二元分类模型，在多类别场景下的DP应用存在不足，因为标准的OvR和OvO方法会重复查询数据样本，导致隐私预算随着类别数量的增加而线性消耗。

Method: 为了克服上述限制，本研究探索了“全合一”（all-in-one）SVM方法，该方法仅访问每个数据样本一次即可构建具有最大化边界特性的多类别SVM边界。具体来说，研究提出了一种名为PMSVM的新型差分隐私多类别SVM，采用了权重和梯度扰动的方法，并提供了严格的敏感性和收敛性分析以确保其差分隐私性。

Result: 实验结果表明，与现有的DP-SVM方法相比，所提出的PMSVM在多类别场景下表现更优。

Conclusion: 本研究成功提出了一种高效且隐私保护的多类别SVM方法PMSVM，解决了现有方法的局限性，并在实验中验证了其优越性。

Abstract: With the increasing need to safeguard data privacy in machine learning
models, differential privacy (DP) is one of the major frameworks to build
privacy-preserving models. Support Vector Machines (SVMs) are widely used
traditional machine learning models due to their robust margin guarantees and
strong empirical performance in binary classification. However, applying DP to
multi-class SVMs is inadequate, as the standard one-versus-rest (OvR) and
one-versus-one (OvO) approaches repeatedly query each data sample when building
multiple binary classifiers, thus consuming the privacy budget proportionally
to the number of classes. To overcome this limitation, we explore all-in-one
SVM approaches for DP, which access each data sample only once to construct
multi-class SVM boundaries with margin maximization properties. We propose a
novel differentially Private Multi-class SVM (PMSVM) with weight and gradient
perturbation methods, providing rigorous sensitivity and convergence analyses
to ensure DP in all-in-one SVMs. Empirical results demonstrate that our
approach surpasses existing DP-SVM methods in multi-class scenarios.

</details>


### [744] [The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View](https://arxiv.org/abs/2510.04028)
*Xinhao Yao,Lu Yu,Xiaolin Hu,Fengwei Teng,Qing Cui,Jun Zhou,Yong Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The ongoing debate on whether reinforcement learning with verifiable rewards
(RLVR) expands or shrinks the reasoning capabilities of large language models
(LLMs) remains unresolved. Some studies contend that RLVR mainly improves
sampling efficiency but at the expense of diversity and exploratory capacity,
resulting in capability boundary shrinkage. In contrast, others demonstrate
that prolonged training can lead to the emergence of novel reasoning
strategies, suggesting capability boundary expansion. To reconcile these
contradictory findings, we theoretically and empirically show that both
perspectives are partially valid-each aligning with a separate phase in an
inherent two-stage probability mass dynamic: (1) Exploitation stage: initially,
the model primarily samples explored high-reward and low-reward tokens, while
rarely selecting the potentially optimal token. Positive advantage estimates
increase the probability of high-reward tokens and decrease those of low-reward
tokens, yet the optimal token's probability remains largely unchanged during
this stage. (2) Exploration stage: as training advances, the growth rate of
previously acquired high-reward tokens slows as their probabilities approach
saturation. When a potentially optimal token-now receiving positive advantage
estimates-is occasionally sampled, its probability increases, while those of
the originally high-reward tokens decrease. This dynamic suggests that
over-exploitation during the exploitation stage may lead to capability boundary
shrinkage, whereas prolonged training into the exploration stage can promote an
expansion of the reasoning capability boundary. Building upon our insights, we
revisit the potential of only using relative negative gradients for prolonging
training, providing a theoretical and empirical foundation for the development
of more advanced reasoning capabilities.

</details>


### [745] [ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures](https://arxiv.org/abs/2510.04938)
*Shiwen Qin,Alexander Auras,Shay B. Cohen,Elliot J. Crowley,Michael Moeller,Linus Ericsson,Jovita Lukasik*

Main category: cs.LG

TL;DR: ONNX-Bench是一个包含600,000多个架构、准确性对的神经网络集合，它提供了一种通用的基于ONNX的网络表示，可以用自然语言描述来预测性能，实现了跨不同搜索空间的零样本评估。


<details>
  <summary>Details</summary>
Motivation: 现有的神经架构搜索（NAS）方法在性能评估方面效率低下，且大多局限于特定的搜索空间和编码方式，缺乏灵活性和可扩展性。

Method: 提出了一种名为ONNX-Bench的基准测试，其中包含统一格式的ONNX神经网络集合。引入了一种名为ONNX-Net的通用网络表示方法，能够将任意神经网络架构（包括各种层类型、参数和拓扑结构）描述为自然语言，并将其作为性能预测器的输入。

Result: 实验证明，该方法在各种不同的搜索空间中表现出强大的零样本性能，仅需少量预训练样本即可实现对任意神经网络架构的即时评估。

Conclusion: ONNX-Bench通过提供一种通用的、基于文本描述的网络表示方法，解决了现有NAS方法在性能评估和跨搜索空间泛化方面的限制，实现了对任意神经网络架构的快速评估。

Abstract: Neural architecture search (NAS) automates the design process of
high-performing architectures, but remains bottlenecked by expensive
performance evaluation. Most existing studies that achieve faster evaluation
are mostly tied to cell-based search spaces and graph encodings tailored to
those individual search spaces, limiting their flexibility and scalability when
applied to more expressive search spaces. In this work, we aim to close the gap
of individual search space restrictions and search space dependent network
representations. We present ONNX-Bench, a benchmark consisting of a collection
of neural networks in a unified format based on ONNX files. ONNX-Bench includes
all open-source NAS-bench-based neural networks, resulting in a total size of
more than 600k {architecture, accuracy} pairs. This benchmark allows creating a
shared neural network representation, ONNX-Net, able to represent any neural
architecture using natural language descriptions acting as an input to a
performance predictor. This text-based encoding can accommodate arbitrary layer
types, operation parameters, and heterogeneous topologies, enabling a single
surrogate to generalise across all neural architectures rather than being
confined to cell-based search spaces. Experiments show strong zero-shot
performance across disparate search spaces using only a small amount of
pretraining samples, enabling the unprecedented ability to evaluate any neural
network architecture instantly.

</details>


### [746] [Adaptive kernel-density approach for imbalanced binary classification](https://arxiv.org/abs/2510.04046)
*Kotaro J. Nishimura,Yuichi Sakumura,Kazushi Ikeda*

Main category: cs.LG

TL;DR: KOTARO是一种通过自适应调整决策边界来解决类别不平衡问题的新方法，在严重不平衡情况下表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡在实际二元分类任务中很常见，会导致模型偏向多数类，忽视少数类，这在医疗诊断和异常检测等领域尤为关键，传统方法在严重不平衡时效果不佳。

Method: 提出了一种名为KOTARO（Kernel-density-Oriented Threshold Adjustment with Regional Optimization）的新方法，该方法扩展了核密度估计（KDE）框架，根据局部样本密度自适应地调整决策边界。KOTARO动态调整高斯基函数的带宽，以增强分类器捕捉少数类区域的能力。

Result: 通过在合成和真实世界的不平衡数据集上进行实验，验证了KOTARO的有效性。结果表明，KOTARO在严重不平衡条件下优于传统方法。

Conclusion: KOTARO是一种有潜力解决广泛类别不平衡分类问题的新方法。

Abstract: Class imbalance is a common challenge in real-world binary classification
tasks, often leading to predictions biased toward the majority class and
reduced recognition of the minority class. This issue is particularly critical
in domains such as medical diagnosis and anomaly detection, where correct
classification of minority classes is essential. Conventional methods often
fail to deliver satisfactory performance when the imbalance ratio is extremely
severe. To address this challenge, we propose a novel approach called
Kernel-density-Oriented Threshold Adjustment with Regional Optimization
(KOTARO), which extends the framework of kernel density estimation (KDE) by
adaptively adjusting decision boundaries according to local sample density. In
KOTARO, the bandwidth of Gaussian basis functions is dynamically tuned based on
the estimated density around each sample, thereby enhancing the classifier's
ability to capture minority regions. We validated the effectiveness of KOTARO
through experiments on both synthetic and real-world imbalanced datasets. The
results demonstrated that KOTARO outperformed conventional methods,
particularly under conditions of severe imbalance, highlighting its potential
as a promising solution for a wide range of imbalanced classification problems

</details>


### [747] [On Structured State-Space Duality](https://arxiv.org/abs/2510.04944)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: 结构化状态空间对偶（SSD）揭示了简单结构化状态空间模型（SSM）与掩码注意力机制之间的等价性。本文将SSD从标量-单位矩阵扩展到对角SSM，并证明了这种对偶关系在训练复杂度和模型动态方面具有优势，同时指出该对偶关系无法扩展到标准softmax注意力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在形式化和推广结构化状态空间对偶（SSD）的概念，将其从标量-单位矩阵扩展到更一般的对角SSM，并探索其与掩码注意力机制的等价性。

Method: 通过将SSD从标量-单位矩阵扩展到对角SSM，证明了对角SSM与1-半可分离掩码注意力的等价性，并分析了该对偶关系在标准softmax注意力中失败的原因。

Result: 本文扩展了SSD的适用范围，证明了对角SSM在训练复杂度和模型动态方面优于标量情况，并确定了SSM等价于1-半可分离掩码注意力的充要条件，同时指出了该对偶关系在softmax注意力中失效。

Conclusion: 本文通过扩展SSD，加深了对循环SSM和Transformer之间联系的理解，拓宽了表达且高效的序列模型的设计空间。

Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence
between a simple Structured State-Space Model (SSM) and a masked attention
mechanism. In particular, a state-space model with a scalar-times-identity
state matrix is equivalent to a masked self-attention with a $1$-semiseparable
causal mask. Consequently, the same sequence transformation (model) has two
algorithmic realizations: as a linear-time $O(T)$ recurrence or as a
quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize
this duality: (i) we extend SSD from the scalar-identity case to general
diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs
match the scalar case's training complexity lower bounds while supporting
richer dynamics; (iii) we establish a necessary and sufficient condition under
which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we
show that such duality fails to extend to standard softmax attention due to
rank explosion. Together, these results tighten bridge between recurrent SSMs
and Transformers, and widen the design space for expressive yet efficient
sequence models.

</details>


### [748] [Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints](https://arxiv.org/abs/2510.04058)
*Subhodip Panda,MS Varun,Shreyans Jain,Sarthak Kumar Maharana,Prathosh A. P*

Main category: cs.LG

TL;DR: 该研究提出了一种名为变分扩散遗忘（VDU）的方法，用于在数据受限的情况下从预训练的扩散模型中移除不良内容，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 为了负责任和安全地部署扩散模型，需要对其生成内容进行监管，因为它们可能产生不受欢迎、暴力或淫秽的输出。现有方法在数据受限的情况下（无法访问整个训练数据集）效果不佳。

Method: VDU 是一种计算高效的方法，它利用变分推断框架，通过最小化一个包含“塑形诱导器”和“稳定性正则化器”的损失函数来实现。塑形诱导器降低了不良数据点的似然性，而稳定性正则化器则在参数空间中进行正则化，以防止图像生成质量下降。

Result: 通过在 MNIST、CIFAR-10 和 tinyImageNet 数据集上进行类别遗忘实验，以及在预训练的 Stable Diffusion 模型上进行特征遗忘实验，验证了该方法的有效性。

Conclusion: VDU 是一种有效且计算高效的机器学习遗忘方法，适用于数据受限场景，能够从预训练的扩散模型中移除不良内容，同时保持模型性能。

Abstract: For a responsible and safe deployment of diffusion models in various domains,
regulating the generated outputs from these models is desirable because such
models could generate undesired, violent, and obscene outputs. To tackle this
problem, recent works use machine unlearning methodology to forget training
data points containing these undesired features from pre-trained generative
models. However, these methods proved to be ineffective in data-constrained
settings where the whole training dataset is inaccessible. Thus, the principal
objective of this work is to propose a machine unlearning methodology that can
prevent the generation of outputs containing undesired features from a
pre-trained diffusion model in such a data-constrained setting. Our proposed
method, termed as Variational Diffusion Unlearning (VDU), is a computationally
efficient method that only requires access to a subset of training data
containing undesired features. Our approach is inspired by the variational
inference framework with the objective of minimizing a loss function consisting
of two terms: plasticity inducer and stability regularizer. Plasticity inducer
reduces the log-likelihood of the undesired training data points, while the
stability regularizer, essential for preventing loss of image generation
quality, regularizes the model in parameter space. We validate the
effectiveness of our method through comprehensive experiments for both class
unlearning and feature unlearning. For class unlearning, we unlearn some
user-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a
pre-trained unconditional denoising diffusion probabilistic model (DDPM).
Similarly, for feature unlearning, we unlearn the generation of certain
high-level features from a pre-trained Stable Diffusion model

</details>


### [749] [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](https://arxiv.org/abs/2510.04996)
*Wei Xiong,Chenlu Ye,Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian,Nan Jiang,Tong Zhang*

Main category: cs.LG

TL;DR: Reinforce-Ada通过自适应采样框架，动态分配LLM的推理预算，以加速和稳定RL的训练过程，特别是在推理任务中。


<details>
  <summary>Details</summary>
Motivation: LLM在推理任务中的RL训练常受限于梯度估计的不稳定，这是由固定、均匀的采样方式造成的。

Method: Reinforce-Ada提出了一种在线自适应采样框架，通过不断地将采样资源重新分配给不确定性或学习潜力最大的提示，并采用在线连续消除过程来估计和采样，同时通过分组和全局统计来稳定更新。

Result: 实验结果表明，Reinforce-Ada在多个模型和推理基准上，相比GRPO能加速收敛并提高最终性能，尤其是在使用平衡采样变体时。

Conclusion: 该研究强调了在RL训练LLM进行推理时，采用关注方差的自适应数据策选对于实现高效和可靠训练的核心作用。

Abstract: Reinforcement learning applied to large language models (LLMs) for reasoning
tasks is often bottlenecked by unstable gradient estimates due to fixed and
uniform sampling of responses across prompts. Prior work such as GVM-RAFT
addresses this by dynamically allocating inference budget per prompt to
minimize stochastic gradient variance under a budget constraint. Inspired by
this insight, we propose Reinforce-Ada, an adaptive sampling framework for
online RL post-training of LLMs that continuously reallocates sampling effort
to the prompts with the greatest uncertainty or learning potential. Unlike
conventional two-stage allocation methods, Reinforce-Ada interleaves estimation
and sampling in an online successive elimination process, and automatically
stops sampling for a prompt once sufficient signal is collected. To stabilize
updates, we form fixed-size groups with enforced reward diversity and compute
advantage baselines using global statistics aggregated over the adaptive
sampling phase. Empirical results across multiple model architectures and
reasoning benchmarks show that Reinforce-Ada accelerates convergence and
improves final performance compared to GRPO, especially when using the balanced
sampling variant. Our work highlights the central role of variance-aware,
adaptive data curation in enabling efficient and reliable reinforcement
learning for reasoning-capable LLMs. Code is available at
https://github.com/RLHFlow/Reinforce-Ada.

</details>


### [750] [Learning to Interpret Weight Differences in Language Models](https://arxiv.org/abs/2510.05092)
*Avichal Goel,Yoon Kim,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 通过训练模型描述其自身的微调变化，DIT 解决了权重差异的解释性问题。


<details>
  <summary>Details</summary>
Motivation: 微调语言模型是更新其内部参数知识和将其专业化于新任务和领域的一种标准方法，但相应的模型权重变化（“权重差异”）通常是不可解释的。检查微调数据集可以了解模型可能如何变化，但这些数据集通常不可公开获得或太大而无法直接处理。

Method: 提出了一种称为“差异解释调整”（DIT）的方法，该方法训练模型来描述其自身的微调引起的修改。该方法使用合成的、标记的权重差异来训练 DIT 适配器，该适配器可应用于兼容的微调模型，使其能够描述其如何变化。

Result: 在两个概念验证设置（报告隐藏行为和总结微调知识）中，证明了该方法能够使用准确的自然语言描述来描述微调引起的修改。

Conclusion: DIT 方法能够使模型使用准确的自然语言描述来描述其微调引起的修改。

Abstract: Finetuning (pretrained) language models is a standard approach for updating
their internal parametric knowledge and specializing them to new tasks and
domains. However, the corresponding model weight changes ("weight diffs") are
not generally interpretable. While inspecting the finetuning dataset can give a
sense of how the model might have changed, these datasets are often not
publicly available or are too large to work with directly. Towards the goal of
comprehensively understanding weight diffs in natural language, we introduce
Diff Interpretation Tuning (DIT), a method that trains models to describe their
own finetuning-induced modifications. Our approach uses synthetic, labeled
weight diffs to train a DIT adapter, which can be applied to a compatible
finetuned model to make it describe how it has changed. We demonstrate in two
proof-of-concept settings (reporting hidden behaviors and summarizing finetuned
knowledge) that our method enables models to describe their finetuning-induced
modifications using accurate natural language descriptions.

</details>


### [751] [Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees](https://arxiv.org/abs/2510.04088)
*Nan Jiang,Tengyang Xie*

Main category: cs.LG

TL;DR: 本文介绍了离线强化学习理论，重点关注在大状态空间中如何从历史数据中学习策略，而无需与环境进行在线交互。文章讨论了函数逼近的表达性假设（如贝尔曼完备性与可实现性）和数据覆盖范围（如全策略覆盖与单策略覆盖），并根据不同的假设和对样本及计算复杂度的要求，介绍了各种算法和结果。此外，文章还探讨了开放性问题以及与相关领域的联系。


<details>
  <summary>Details</summary>
Motivation: 本文旨在介绍离线强化学习的理论基础，特别是在大状态空间场景下，如何利用历史数据学习策略，解决了在线交互的局限性。

Method: 文章介绍了离线强化学习中的关键概念，包括函数逼近的表达性假设（贝尔曼完备性与可实现性）和数据覆盖（全策略覆盖与单策略覆盖），并在此基础上阐述了不同的算法和理论结果。

Result: 文章描述了一个算法和结果的丰富图景，这些算法和结果的有效性取决于所做的假设以及对样本和计算复杂度的要求。

Conclusion: 离线强化学习是一个涉及多种假设和权衡的复杂领域，文章为理解该领域提供了理论框架，并指出了未来的研究方向和与其他领域的联系。

Abstract: This article introduces the theory of offline reinforcement learning in large
state spaces, where good policies are learned from historical data without
online interactions with the environment. Key concepts introduced include
expressivity assumptions on function approximation (e.g., Bellman completeness
vs. realizability) and data coverage (e.g., all-policy vs. single-policy
coverage). A rich landscape of algorithms and results is described, depending
on the assumptions one is willing to make and the sample and computational
complexity guarantees one wishes to achieve. We also discuss open questions and
connections to adjacent areas.

</details>


### [752] [From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models](https://arxiv.org/abs/2510.05095)
*Mingkang Zhu,Xi Chen,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: 大型推理模型（LRM）在生成最终答案之前会生成中间推理轨迹，这在多步和数学任务上带来了显著的收益。然而，将LRM与人类偏好对齐（模型部署的关键前提）仍然是一个未经充分探索的领域。偏好对齐的统计上正确的目标需要对推理轨迹进行边际化处理，但在实践中这种计算是难以处理的。一个常见的变通方法是优化单个采样轨迹，但这会从随机轨迹采样中引入大量的梯度方差。为了解决这个挑战，我们通过偏差-方差权衡的视角来构建LRM的偏好优化，并提出偏差-方差优化偏好优化（BVPO）。BVPO是一种简单、即插即用的方法，它混合了两种梯度估计器：一种高方差的基于轨迹的估计器，以及一种通过禁用推理轨迹生成获得的低方差的空轨迹估计器。我们的理论表明，BVPO在任何非平凡的混合中都严格降低了由轨迹引起的方差，提供了一个封闭形式的混合权重选择，该选择最小化了相对于真实边际梯度的均方误差，并在标准的平滑性和步长条件下，收紧了随机梯度下降的经典收敛界限。在实践中，BVPO在AlpacaEval~2上比最佳基线提高了多达7.8分，在Arena-Hard上提高了6.8分。尽管BVPO仅在通用对话数据上进行训练，但它还将基础模型的推理性能在六个数学推理基准的平均值上提高了多达4.0分。这些结果将轨迹采样产生的方差识别为一个关键瓶颈，并证明直接优化偏差-方差权衡可以带来更稳定的训练和更强的整体性能。


<details>
  <summary>Details</summary>
Motivation: 将大型推理模型（LRM）与人类偏好对齐是模型部署的关键前提，但现有方法在实践中面临计算难题和梯度方差大的问题。

Method: 提出了一种名为偏差-方差优化偏好优化（BVPO）的新方法，该方法混合了基于轨迹的高方差梯度估计器和空轨迹的低方差梯度估计器，以在偏差-方差权衡的框架下优化偏好。

Result: BVPO在AlpacaEval~2和Arena-Hard上分别取得了显著的性能提升，并且在数学推理基准上也提高了基础模型的性能。实验结果表明，轨迹采样方差是关键瓶颈，直接优化偏差-方差权衡可以提高训练稳定性和整体性能。

Conclusion: BVPO通过优化偏差-方差权衡，有效地解决了LRM偏好对齐中的梯度方差问题，提高了训练的稳定性和模型在各项任务上的性能。

Abstract: Large reasoning models (LRMs) generate intermediate reasoning traces before
producing final answers, yielding strong gains on multi-step and mathematical
tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for
model deployment, remains underexplored. The statistically correct objective
for preference alignment requires marginalizing over reasoning traces, but this
computation is intractable in practice. A common workaround optimizes a single
sampled trajectory, which introduces substantial gradient variance from
stochastic trace sampling. To address this challenge, we frame preference
optimization for LRMs through the lens of the bias--variance trade-off and
propose Bias--Variance Optimized Preference Optimization (BVPO), a simple,
drop-in method that mixes two gradient estimators: a high-variance trace-based
estimator and a low-variance empty-trace estimator obtained by disabling
reasoning trace generation. Our theory shows that BVPO strictly reduces
trace-induced variance for any nontrivial mixture, provides a closed-form
choice of the mixing weight that minimizes mean-squared error relative to the
true marginal gradient, and under standard smoothness and step-size conditions,
tightens classical convergence bounds for stochastic gradient descent.
Empirically, BVPO improves alignment over the best baseline by up to 7.8 points
on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on
general conversational data, BVPO also boosts reasoning performance for base
models by up to 4.0 points on the average of six math reasoning benchmarks.
These results identify variance from trace sampling as a key bottleneck and
demonstrate that directly optimizing the bias--variance trade-off yields more
stable training and stronger overall performance.

</details>


### [753] [Using predefined vector systems as latent space configuration for neural network supervised training on data with arbitrarily large number of classes](https://arxiv.org/abs/2510.04090)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: 该方法提出了一种训练神经网络的新范式，使其参数数量不依赖于类别的数量，从而解决了类别数量巨大或未知时的局限性。


<details>
  <summary>Details</summary>
Motivation: 监督学习（SL）方法在分类任务的神经网络（NN）训练中不可或缺，但其训练通常要求NN参数数量与类别数量相关，这在类别数量极大或未知时限制了其应用。

Method: 提出了一种使用预定义向量系统作为目标潜在空间配置（LSC）进行NN训练的方法，使得相同的NN架构可以用于不同的类别数量。实验中使用了随机扰动的根系统向量作为目标配置，并通过匹配NN预测与预定义向量来训练编码器和视觉变换器（ViT）。

Result: 该方法成功地在Cinic-10和ImageNet-1K数据集上训练了编码器和ViT，实现了低维和高维情况下的分类任务。特别地，ViT在包含128万个类别的数据集上进行了训练，证明了该方法在处理海量类别时的有效性。

Conclusion: 提出的方法能够训练出与类别数量无关的NN，适用于类别数量极大或未知的情况，并且在终身学习和NN蒸馏等领域具有潜在应用价值，展示了该方法的多功能性。

Abstract: Supervised learning (SL) methods are indispensable for neural network (NN)
training used to perform classification tasks. While resulting in very high
accuracy, SL training often requires making NN parameter number dependent on
the number of classes, limiting their applicability when the number of classes
is extremely large or unknown in advance. In this paper we propose a
methodology that allows one to train the same NN architecture regardless of the
number of classes. This is achieved by using predefined vector systems as the
target latent space configuration (LSC) during NN training. We discuss the
desired properties of target configurations and choose randomly perturbed
vectors of An root system for our experiments. These vectors are used to
successfully train encoders and visual transformers (ViT) on Cinic-10 and
ImageNet-1K in low- and high-dimensional cases by matching NN predictions with
the predefined vectors. Finally, ViT is trained on a dataset with 1.28 million
classes illustrating the applicability of the method to training on datasets
with extremely large number of classes. In addition, potential applications of
LSC in lifelong learning and NN distillation are discussed illustrating
versatility of the proposed methodology.

</details>


### [754] [Rethinking Consistent Multi-Label Classification under Inexact Supervision](https://arxiv.org/abs/2510.04091)
*Wei Wang,Tianhao Ma,Ming-Kun Xie,Gang Niu,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出了一种统一的方法来处理偏多标签学习和互补多标签学习问题，该方法不依赖于对标签生成过程的准确估计或均匀分布的假设，并提出了基于一阶和二阶策略的无偏风险估计量，在理论和实证上都验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的偏多标签学习和互补多标签学习方法要么需要准确估计标签生成过程，要么假设标签分布均匀，这在实际中难以满足。

Method: 提出了一种统一的方法来处理这两种学习范式，并提出基于一阶和二阶策略的无偏风险估计量。

Result: 理论上证明了所提出的风险估计量与两种常用的多标签分类评估指标的一致性，并推导了估计误差的收敛率。实验结果表明，所提出的方法优于现有技术。

Conclusion: 所提出的方法能够有效处理偏多标签学习和互补多标签学习问题，并且在理论和实验上都得到了验证。

Abstract: Partial multi-label learning and complementary multi-label learning are two
popular weakly supervised multi-label classification paradigms that aim to
alleviate the high annotation costs of collecting precisely annotated
multi-label data. In partial multi-label learning, each instance is annotated
with a candidate label set, among which only some labels are relevant; in
complementary multi-label learning, each instance is annotated with
complementary labels indicating the classes to which the instance does not
belong. Existing consistent approaches for the two paradigms either require
accurate estimation of the generation process of candidate or complementary
labels or assume a uniform distribution to eliminate the estimation problem.
However, both conditions are usually difficult to satisfy in real-world
scenarios. In this paper, we propose consistent approaches that do not rely on
the aforementioned conditions to handle both problems in a unified way.
Specifically, we propose two unbiased risk estimators based on first- and
second-order strategies. Theoretically, we prove consistency w.r.t. two widely
used multi-label classification evaluation metrics and derive convergence rates
for the estimation errors of the proposed risk estimators. Empirically,
extensive experimental results validate the effectiveness of our proposed
approaches against state-of-the-art methods.

</details>


### [755] [Wasserstein projection distance for fairness testing of regression models](https://arxiv.org/abs/2510.04114)
*Wanxin Li,Yongjin P. Park,Khanh Dao Duc*

Main category: cs.LG

TL;DR: 本论文提出了一种基于Wasserstein投影的回归模型公平性测试框架，使用假设检验和最优数据扰动方法来提高公平性并平衡准确性。


<details>
  <summary>Details</summary>
Motivation: 机器学习公平性研究主要集中在分类任务，忽视了回归模型，因此需要关注回归模型的公平性问题。

Method: 提出了一种基于Wasserstein投影的框架，包括假设检验和最优数据扰动方法，并对公平性标准进行了分类，推导了渐近界和极限分布。

Result: 实验结果表明，该方法比基于排列的测试具有更高的特异性，并能有效检测和缓解学生表现和房价预测等实际应用中的偏差。

Conclusion: 所提出的Wasserstein投影框架为回归模型的公平性测试提供了一种有效的方法，并在准确性和公平性之间取得了良好的平衡。

Abstract: Fairness in machine learning is a critical concern, yet most research has
focused on classification tasks, leaving regression models underexplored. This
paper introduces a Wasserstein projection-based framework for fairness testing
in regression models, focusing on expectation-based criteria. We propose a
hypothesis-testing approach and an optimal data perturbation method to improve
fairness while balancing accuracy. Theoretical results include a detailed
categorization of fairness criteria for regression, a dual reformulation of the
Wasserstein projection test statistic, and the derivation of asymptotic bounds
and limiting distributions. Experiments on synthetic and real-world datasets
demonstrate that the proposed method offers higher specificity compared to
permutation-based tests, and effectively detects and mitigates biases in real
applications such as student performance and housing price prediction.

</details>


### [756] [On the Statistical Query Complexity of Learning Semiautomata: a Random Walk Approach](https://arxiv.org/abs/2510.04115)
*George Giapitzakis,Kimon Fountoulakis,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: Semiautomata的统计查询难度首次被证明，尤其是在字母表大小和输入长度与状态数量成多项式关系时。


<details>
  <summary>Details</summary>
Motivation: Semiautomata在自然语言处理、机器人、计算生物学和数据挖掘等领域有广泛应用，理解其计算难度具有重要意义。

Method: 通过将区分两个Semiautomata的最终状态的任务，转化为研究随机游走在群$S_{N} 	imes S_{N}$上的行为，并利用傅里叶分析和对称群的表示理论工具，获得精确的谱隙界限。

Result: 在状态数量的多项式步数后，不同的Semiautomata变得几乎不相关，从而得到了所需的统计查询难度结果。

Conclusion: 研究表明，Semiautomata的统计查询难度源于其内部的状态转移结构，而非其识别语言的难度。

Abstract: Semiautomata form a rich class of sequence-processing algorithms with
applications in natural language processing, robotics, computational biology,
and data mining. We establish the first Statistical Query hardness result for
semiautomata under the uniform distribution over input words and initial
states. We show that Statistical Query hardness can be established when both
the alphabet size and input length are polynomial in the number of states.
Unlike the case of deterministic finite automata, where hardness typically
arises through the hardness of the language they recognize (e.g., parity), our
result is derived solely from the internal state-transition structure of
semiautomata. Our analysis reduces the task of distinguishing the final states
of two semiautomata to studying the behavior of a random walk on the group
$S_{N} \times S_{N}$. By applying tools from Fourier analysis and the
representation theory of the symmetric group, we obtain tight spectral gap
bounds, demonstrating that after a polynomial number of steps in the number of
states, distinct semiautomata become nearly uncorrelated, yielding the desired
hardness result.

</details>


### [757] [Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions](https://arxiv.org/abs/2510.04126)
*Ziying Zhang,Yaqing Wang,Yuxuan Sun,Min Ye,Quanming Yao*

Main category: cs.LG

TL;DR: ColdDTI通过关注蛋白质的多层次结构来预测冷启动的药物-靶标相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只使用蛋白质的一级结构，无法捕捉涉及蛋白质高层次结构（如二级、三级、四级结构）的药物-靶标相互作用，而这些结构对相互作用有影响。

Method: 提出了一种名为ColdDTI的框架，该框架采用分层注意力机制来挖掘蛋白质多层次结构（从一级到四级）与药物结构之间在局部和全局粒度上的相互作用。然后，利用挖掘到的相互作用来融合不同层次的蛋白质结构表示，以进行最终预测。

Result: 在基准数据集上的实验表明，ColdDTI在冷启动设置下始终优于以前的方法。

Conclusion: ColdDTI通过整合蛋白质的多层次结构信息，能够更有效地进行冷启动药物-靶标相互作用预测，并且避免了过度依赖表示学习带来的过拟合风险。

Abstract: Cold-start drug-target interaction (DTI) prediction focuses on interaction
between novel drugs and proteins. Previous methods typically learn transferable
interaction patterns between structures of drug and proteins to tackle it.
However, insight from proteomics suggest that protein have multi-level
structures and they all influence the DTI. Existing works usually represent
protein with only primary structures, limiting their ability to capture
interactions involving higher-level structures. Inspired by this insight, we
propose ColdDTI, a framework attending on protein multi-level structure for
cold-start DTI prediction. We employ hierarchical attention mechanism to mine
interaction between multi-level protein structures (from primary to quaternary)
and drug structures at both local and global granularities. Then, we leverage
mined interactions to fuse structure representations of different levels for
final prediction. Our design captures biologically transferable priors,
avoiding the risk of overfitting caused by excessive reliance on representation
learning. Experiments on benchmark datasets demonstrate that ColdDTI
consistently outperforms previous methods in cold-start settings.

</details>


### [758] [On the Limitations and Capabilities of Position Embeddings for Length Generalization](https://arxiv.org/abs/2510.04130)
*Yang Chen,Yitao Liang,Zhouchen Lin*

Main category: cs.LG

TL;DR: Transformer中的位置嵌入(PE)对长度泛化(LG)至关重要，但其作用机制尚不明确。本研究通过理论分析位置嵌入在仅位置线性注意力(POLA)中的作用，提出线性表示复杂度(LRC)来衡量PE是否支持LG。分析表明，PE不增加计算能力，而是组织跨位置的计算。在此基础上，研究者将理论扩展到实际Transformer模型，提出顺序表示复杂度(SRC)度量，并推测LG的可能性等价于SRC在不同尺度下保持不变。为增强LG，研究者提出了比例提示（Scale Hint）和基于学习的位置嵌入框架，以实现灵活的实例扩展和自动学习位置关系。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer中位置嵌入（PE）在长度泛化（LG）中的作用机制及其局限性与能力。

Method: 理论分析了仅位置线性注意力（POLA）中的PE，提出了线性表示复杂度（LRC）来表征PE支持LG的条件；推导了顺序表示复杂度（SRC）并提出其不变性是LG的充要条件；设计了比例提示（Scale Hint）和基于学习的位置嵌入框架。

Result: 理论分析表明PE并不扩展计算能力，而是组织跨位置的计算；SRC不变性是LG的充要条件；比例提示和学习型PE框架在多个推理任务上有效提升了LG。

Conclusion: PE在Transformer的LG中起着组织计算的作用，而非增强计算能力。SRC的不变性是实现LG的关键。所提出的比例提示和学习型PE框架为提升Transformer的LG能力提供了有效的理论指导和实践方法。

Abstract: In Transformers, Position Embeddings (PEs) significantly influence Length
Generalization (LG) performance, yet their fundamental role remains unclear. In
this work, we investigate the limitations and capabilities of PEs in achieving
LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs),
introducing Linear Representation Complexity (LRC) to characterize when PEs
enable LG. Our analysis shows that PEs do not expand computational capabilities
but structure learned computations across positions. Extending to practical
Transformers, we propose Sequential Representation Complexity (SRC) and
conjecture that LG is possible if and only if SRC remains invariant across
scales. We support this hypothesis with empirical evidence in various reasoning
tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance
scaling, and a Learning-Based Position Embedding framework that automatically
learns positional relations. Our work provides theoretical insights and
practical strategies for improving LG in Transformers.

</details>


### [759] [Modeling Time Series Dynamics with Fourier Ordinary Differential Equations](https://arxiv.org/abs/2510.04133)
*Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: NODEs在处理时间序列数据时面临捕捉长期依赖和处理离散数据的挑战。本文提出的FODEs将时间序列数据转换到傅里叶域进行建模，并引入可学习的滤波机制以匹配离散观测，从而在准确性和效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的神经ODE（NODEs）在处理时间序列数据时，由于依赖时域表示，难以捕捉长期依赖和周期性结构，并且连续时间形式与离散数据之间存在不匹配问题，导致精度损失。

Method: 提出傅里叶常微分方程（FODEs），通过快速傅里叶变换（FFT）将时间序列数据转换到频域进行建模，并引入可学习的逐元滤波机制来匹配连续模型输出和离散观测。

Result: 在多个时间序列数据集上的实验表明，FODEs在准确性和效率方面均优于现有方法，能够有效捕捉长期和短期模式。

Conclusion: FODEs通过在傅里叶域嵌入动力学并引入滤波机制，解决了NODEs在捕捉长期依赖和处理离散数据方面的局限性，为时间序列建模提供了一个鲁棒的框架。

Abstract: Neural ODEs (NODEs) have emerged as powerful tools for modeling time series
data, offering the flexibility to adapt to varying input scales and capture
complex dynamics. However, they face significant challenges: first, their
reliance on time-domain representations often limits their ability to capture
long-term dependencies and periodic structures; second, the inherent mismatch
between their continuous-time formulation and the discrete nature of real-world
data can lead to loss of granularity and predictive accuracy. To address these
limitations, we propose Fourier Ordinary Differential Equations (FODEs), an
approach that embeds the dynamics in the Fourier domain. By transforming
time-series data into the frequency domain using the Fast Fourier Transform
(FFT), FODEs uncover global patterns and periodic behaviors that remain elusive
in the time domain. Additionally, we introduce a learnable element-wise
filtering mechanism that aligns continuous model outputs with discrete
observations, preserving granularity and enhancing accuracy. Experiments on
various time series datasets demonstrate that FODEs outperform existing methods
in terms of both accuracy and efficiency. By effectively capturing both long-
and short-term patterns, FODEs provide a robust framework for modeling time
series dynamics.

</details>


### [760] [PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting](https://arxiv.org/abs/2510.04134)
*Yiming Niu,Jinliang Deng,Yongxin Tong*

Main category: cs.LG

TL;DR: patch-level time series forecasting methods are inefficient; PhaseFormer offers an efficient solution using phase embeddings and routing.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for time series forecasting, while effective, suffer from inefficiency due to large parameter counts and heavy computational costs associated with patch-level processing. This paper aims to explain this inefficiency and propose a more efficient approach.

Method: The paper introduces PhaseFormer, which models periodicity from a phase perspective. It utilizes phase-wise prediction with compact phase embeddings and an efficient cross-phase interaction mechanism based on a lightweight routing strategy.

Result: PhaseFormer achieves state-of-the-art performance with a significantly smaller parameter count (around 1k) compared to existing methods. It demonstrates consistent effectiveness across benchmark datasets, particularly excelling on large-scale and complex datasets where other efficient models often fail.

Conclusion: This work presents a significant advancement in time series forecasting by introducing an efficient and effective model, PhaseFormer, that addresses the limitations of previous patch-level methods. It highlights the potential of a phase-based approach for future research in this field.

Abstract: Periodicity is a fundamental characteristic of time series data and has long
played a central role in forecasting. Recent deep learning methods strengthen
the exploitation of periodicity by treating patches as basic tokens, thereby
improving predictive effectiveness. However, their efficiency remains a
bottleneck due to large parameter counts and heavy computational costs. This
paper provides, for the first time, a clear explanation of why patch-level
processing is inherently inefficient, supported by strong evidence from
real-world data. To address these limitations, we introduce a phase perspective
for modeling periodicity and present an efficient yet effective solution,
PhaseFormer. PhaseFormer features phase-wise prediction through compact phase
embeddings and efficient cross-phase interaction enabled by a lightweight
routing mechanism. Extensive experiments demonstrate that PhaseFormer achieves
state-of-the-art performance with around 1k parameters, consistently across
benchmark datasets. Notably, it excels on large-scale and complex datasets,
where models with comparable efficiency often struggle. This work marks a
significant step toward truly efficient and effective time series forecasting.
Code is available at this repository:
https://github.com/neumyor/PhaseFormer_TSL

</details>


### [761] [Efficient Manifold-Constrained Neural ODE for High-Dimensional Datasets](https://arxiv.org/abs/2510.04138)
*Muhao Guo,Haoran Li,Yang Weng*

Main category: cs.LG

TL;DR: 通过利用数据流形的拓扑结构来提高神经网络常微分方程（NODE）在处理高维数据时的计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的NODE方法在处理高维数据时，由于ODE求解器计算量大和截断误差高，存在效率和精度问题。直接在低维流形上进行ODE求解是可行的，但现实场景中流形结构通常未知。

Method: 提出一种新方法，利用结构保持编码器学习数据流形的潜在图结构，并将此信息融入NODE学习过程，以在流形上约束ODE演化，从而提高计算速度和精度。

Result: 在多个数据集上的实验评估表明，该模型在准确性、函数评估次数（NFEs）和收敛速度方面优于现有基线方法。

Conclusion: 所提出的方法能有效解决高维数据集的挑战，通过利用数据流形信息显著提升了NODE在计算速度和精度方面的表现。

Abstract: Neural ordinary differential equations (NODE) have garnered significant
attention for their design of continuous-depth neural networks and the ability
to learn data/feature dynamics. However, for high-dimensional systems,
estimating dynamics requires extensive calculations and suffers from high
truncation errors for the ODE solvers. To address the issue, one intuitive
approach is to consider the non-trivial topological space of the data
distribution, i.e., a low-dimensional manifold. Existing methods often rely on
knowledge of the manifold for projection or implicit transformation,
restricting the ODE solutions on the manifold. Nevertheless, such knowledge is
usually unknown in realistic scenarios. Therefore, we propose a novel approach
to explore the underlying manifold to restrict the ODE process. Specifically,
we employ a structure-preserved encoder to process data and find the underlying
graph to approximate the manifold. Moreover, we propose novel methods to
combine the NODE learning with the manifold, resulting in significant gains in
computational speed and accuracy. Our experimental evaluations encompass
multiple datasets, where we compare the accuracy, number of function
evaluations (NFEs), and convergence speed of our model against existing
baselines. Our results demonstrate superior performance, underscoring the
effectiveness of our approach in addressing the challenges of high-dimensional
datasets.

</details>


### [762] [Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity](https://arxiv.org/abs/2510.04189)
*Prashansa Panda,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 本文提出了首个用于长时平均成本和不等式约束设置的具有函数逼近的自然Critic-Actor算法，并提供了非渐近收敛保证、最优学习率和改进的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注Actor-Critic算法在折扣成本设置下的非渐近收敛性分析，而对于长时平均成本和不等式约束设置下的Critic-Actor算法，尤其是在函数逼近的条件下，相关的分析尚不完善。

Method: 提出了一种新的Critic-Actor算法，该算法适用于长时平均成本和不等式约束设置，并结合了函数逼近。该算法的分析得出了非渐近收敛保证，并确定了最优学习率。此外，还提出了一种改进样本复杂度的修改方法。

Result: 该算法在长时平均成本和不等式约束设置下，结合函数逼近，实现了非渐近收敛，并达到了最优学习率。通过在三个不同的Safety-Gym环境中进行实验，证明了该算法与其他知名算法相比具有竞争力。

Conclusion: 本文成功地为长时平均成本和不等式约束设置下的Critic-Actor算法（带有函数逼近）提供了非渐近收敛保证，确立了最优学习率，并提出了改进样本复杂度的方案。实验结果表明该算法在实际应用中表现良好。

Abstract: Recent studies have increasingly focused on non-asymptotic convergence
analyses for actor-critic (AC) algorithms. One such effort introduced a
two-timescale critic-actor algorithm for the discounted cost setting using a
tabular representation, where the usual roles of the actor and critic are
reversed. However, only asymptotic convergence was established there.
Subsequently, both asymptotic and non-asymptotic analyses of the critic-actor
algorithm with linear function approximation were conducted. In our work, we
introduce the first natural critic-actor algorithm with function approximation
for the long-run average cost setting and under inequality constraints. We
provide the non-asymptotic convergence guarantees for this algorithm. Our
analysis establishes optimal learning rates and we also propose a modification
to enhance sample complexity. We further show the results of experiments on
three different Safety-Gym environments where our algorithm is found to be
competitive in comparison with other well known algorithms.

</details>


### [763] [Spectral Alignment as Predictor of Loss Explosion in Neural Network Training](https://arxiv.org/abs/2510.04202)
*Haiquan Qiu,You Wu,Yingjie Tan,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: SA通过监控层输入与权重矩阵主奇异向量之间的分布对齐情况，有效预测深度神经网络训练中的损失爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 传统的监控指标（如权重和梯度范数）在预测模型训练失败方面存在滞后性和模糊性，难以建立统一标准。特别是在深度神经网络训练中，损失爆炸可能导致巨大的计算资源浪费。

Method: 提出了一种新颖的、具有理论依据的度量方法——谱对齐（SA）。SA通过监控层输入与权重矩阵主奇异向量之间的分布对齐情况来工作。研究表明，这种对齐的符号多样性下降是代表性崩溃和训练发散的有力早期预测指标。

Result: 在语言模型的实证研究中，SA能够比传统的标量指标更早、更清晰地发出损失爆炸的警告。SA的计算开销低，易于实际应用。

Conclusion: SA是一种有效的、计算开销低的早期预警系统，能够有效检测和预防深度神经网络训练中的损失爆炸，从而保护重要的计算资源。

Abstract: Loss explosions in training deep neural networks can nullify multi-million
dollar training runs. Conventional monitoring metrics like weight and gradient
norms are often lagging and ambiguous predictors, as their values vary
dramatically across different models and even between layers of the same model,
making it difficult to establish a unified standard for detecting impending
failure. We introduce Spectral Alignment (SA), a novel, theoretically-grounded
metric that monitors the distributional alignment between layer inputs and the
principal singular vectors of weight matrices. We show that a collapse in the
sign diversity of this alignment is a powerful early predictor of
representational collapse and training divergence. Empirical results on
language models demonstrate that monitoring the SA distribution provides a
significantly earlier and clearer warning of loss explosions than traditional
scalar metrics. SA's low computational overhead makes it a practical tool for
safeguarding model training.

</details>


### [764] [Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention](https://arxiv.org/abs/2510.04212)
*Haiquan Qiu,Quanming Yao*

Main category: cs.LG

TL;DR: 低精度训练下的 Flash Attention 存在因低秩表示和累积舍入误差导致的灾难性损失爆炸问题，通过修正舍入误差可解决此问题。


<details>
  <summary>Details</summary>
Motivation: 低精度训练 transformer 模型以提高计算效率，但常面临训练不稳定的问题，特别是 Flash Attention 在低精度下会导致损失爆炸。

Method: 分析 Flash Attention 在低精度训练下损失爆炸的原因，发现是由于低秩表示的出现和低精度算术中存在的偏置舍入误差的累积效应。

Result: 通过分析证明了低秩表示和偏置舍入误差如何形成一个恶性循环，破坏权重更新并导致训练动态失控。提出了一种对 Flash Attention 的微小修改，可以减轻舍入误差的偏差，从而稳定训练过程。

Conclusion: 低精度训练下的 Flash Attention 损失爆炸问题是由于低秩表示和累积的舍入误差共同作用的结果。通过修正舍入误差可以有效解决此问题，并为该持久性问题提供了一个实用的解决方案。

Abstract: The pursuit of computational efficiency has driven the adoption of
low-precision formats for training transformer models. However, this progress
is often hindered by notorious training instabilities. This paper provides the
first mechanistic explanation for a long-standing and unresolved failure case
where training with flash attention in low-precision settings leads to
catastrophic loss explosions. Our in-depth analysis reveals that the failure is
not a random artifact but caused by two intertwined phenomena: the emergence of
similar low-rank representations within the attention mechanism and the
compounding effect of biased rounding errors inherent in low-precision
arithmetic. We demonstrate how these factors create a vicious cycle of error
accumulation that corrupts weight updates, ultimately derailing the training
dynamics. To validate our findings, we introduce a minimal modification to the
flash attention that mitigates the bias in rounding errors. This simple change
stabilizes the training process, confirming our analysis and offering a
practical solution to this persistent problem.

</details>


### [765] [MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering](https://arxiv.org/abs/2510.04217)
*Chenlu Ding,Jiancan Wu,Leheng Sheng,Fan Zhang,Yancheng Yuan,Xiang Wang,Xiangnan He*

Main category: cs.LG

TL;DR: MLLMEraser是一个无需训练的测试时间去除框架，通过激活引导来动态擦除知识，而无需更新模型参数，解决了大型多模态模型（MLLMs）中存在的私有数据记忆、知识过时和有害内容等问题。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM去除方法（如梯度上升或偏好优化）计算成本高、不可逆且可能损害保留的知识。本研究旨在开发一种更有效、无损的去除方法。

Method: MLLMEraser通过对比对抗性扰动的知识回忆图像-文本对和知识去除对应物来构建多模态去除方向，捕捉文本和视觉差异。它还使用输入感知的引导机制来动态决定何时以及如何应用去除方向，以最小化对保留知识的干扰。

Result: 在LLaVA-1.5和Qwen-2.5-VL上的实验表明，MLLMEraser在去除性能上优于最先进的基线方法，同时计算成本更低，并且对保留知识的效用损害极小。

Conclusion: MLLMEraser通过一种新颖的、无需训练的激活引导方法，能够有效地从MLLMs中去除指定知识，同时最大限度地保留模型的整体效用，解决了现有方法的局限性。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities across vision-language tasks, yet their large-scale deployment
raises pressing concerns about memorized private data, outdated knowledge, and
harmful content. Existing unlearning approaches for MLLMs typically adapt
training-based strategies such as gradient ascent or preference optimization,
but these methods are computationally expensive, irreversible, and often
distort retained knowledge. In this work, we propose MLLMEraser, an
input-aware, training-free framework for test-time unlearning. Our approach
leverages activation steering to enable dynamic knowledge erasure without
parameter updates. Specifically, we construct a multimodal erasure direction by
contrasting adversarially perturbed, knowledge-recall image-text pairs with
knowledge-erasure counterparts, capturing both textual and visual
discrepancies. To prevent unnecessary interference, we further design an
input-aware steering mechanism that adaptively determines when and how the
erasure direction should be applied, preserving utility on retained knowledge
while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and
Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms
state-of-the-art MLLM unlearning baselines, achieving stronger forgetting
performance with lower computational cost and minimal utility degradation.

</details>


### [766] [Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling](https://arxiv.org/abs/2510.04233)
*Kai Yang,Yuqi Huang,Junheng Tao,Wanyu Wang,Qitian Wu*

Main category: cs.LG

TL;DR: PAINET是一个SE(3)-等变神经网络模型，用于学习多体系统中的所有交互作用，解决了现有GNN模型无法捕捉未观察到的相互作用的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于GNN的方法在建模3D动力学时，通常依赖于显式观察到的结构，无法捕捉复杂物理行为和动力学机制中至关重要的未观察到的相互作用。

Method: 提出了一种名为PAINET的SE(3)-等变神经网络架构，该架构包含一个受物理启发的注意力网络（源于能量函数最小化轨迹）和一个并行的、能保持等变性并实现高效推理的解码器。

Result: 在人体运动捕捉、分子动力学和大规模蛋白质模拟等多个现实世界基准测试中，PAINET在3D动力学预测方面的误差降低了4.7%至41.5%，同时计算成本相当。

Conclusion: PAINET在3D动力学预测方面优于现有模型，能够有效捕捉未观察到的相互作用。

Abstract: Modeling 3D dynamics is a fundamental problem in multi-body systems across
scientific and engineering domains and has important practical implications in
trajectory prediction and simulation. While recent GNN-based approaches have
achieved strong performance by enforcing geometric symmetries, encoding
high-order features or incorporating neural-ODE mechanics, they typically
depend on explicitly observed structures and inherently fail to capture the
unobserved interactions that are crucial to complex physical behaviors and
dynamics mechanism. In this paper, we propose PAINET, a principled
SE(3)-equivariant neural architecture for learning all-pair interactions in
multi-body systems. The model comprises: (1) a novel physics-inspired attention
network derived from the minimization trajectory of an energy function, and (2)
a parallel decoder that preserves equivariance while enabling efficient
inference. Empirical results on diverse real-world benchmarks, including human
motion capture, molecular dynamics, and large-scale protein simulations, show
that PAINET consistently outperforms recently proposed models, yielding 4.7% to
41.5% error reductions in 3D dynamics prediction with comparable computation
costs in terms of time and memory.

</details>


### [767] [Truncated Kernel Stochastic Gradient Descent with General Losses and Spherical Radial Basis Functions](https://arxiv.org/abs/2510.04237)
*Jinhui Bai,Andreas Christmann,Lei Shi*

Main category: cs.LG

TL;DR: 提出一种新颖的核随机梯度下降（SGD）算法，用于具有一般损失的大规模监督学习。


<details>
  <summary>Details</summary>
Motivation: 传统的核SGD在效率和可扩展性方面存在不足，需要改进以适应大规模学习场景。

Method: 通过创新的正则化策略，利用球形径向基函数的无限级数展开，将随机梯度投影到有限维假设空间，并根据偏差-方差权衡自适应缩放。利用新的核诱导协方差算子谱结构估计，建立了统一优化和泛化分析的框架，并结合了线性SGD的坐标更新。

Result: 证明了最后一个迭代点和后缀平均都以minimax最优速率收敛，并在再生核希尔伯特空间中建立了最优强收敛性。该算法能有效处理最小二乘、Huber和逻辑损失等多种损失函数，并显著降低计算复杂度和实现最优存储复杂度。

Conclusion: 所提出的算法通过创新的正则化策略、分析框架和高效的更新机制，在效率、可扩展性、泛化性能以及计算和存储复杂度方面都取得了显著改进，并通过数值实验得到了验证。

Abstract: In this paper, we propose a novel kernel stochastic gradient descent (SGD)
algorithm for large-scale supervised learning with general losses. Compared to
traditional kernel SGD, our algorithm improves efficiency and scalability
through an innovative regularization strategy. By leveraging the infinite
series expansion of spherical radial basis functions, this strategy projects
the stochastic gradient onto a finite-dimensional hypothesis space, which is
adaptively scaled according to the bias-variance trade-off, thereby enhancing
generalization performance. Based on a new estimation of the spectral structure
of the kernel-induced covariance operator, we develop an analytical framework
that unifies optimization and generalization analyses. We prove that both the
last iterate and the suffix average converge at minimax-optimal rates, and we
further establish optimal strong convergence in the reproducing kernel Hilbert
space. Our framework accommodates a broad class of classical loss functions,
including least-squares, Huber, and logistic losses. Moreover, the proposed
algorithm significantly reduces computational complexity and achieves optimal
storage complexity by incorporating coordinate-wise updates from linear SGD,
thereby avoiding the costly pairwise operations typical of kernel SGD and
enabling efficient processing of streaming data. Finally, extensive numerical
experiments demonstrate the efficiency of our approach.

</details>


### [768] [Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs](https://arxiv.org/abs/2510.04241)
*Seong Jin Ahn,Myoung-Ho Kim*

Main category: cs.LG

TL;DR: 通过引入扩散模型作为教师助手，提出DAD-SGM方法，成功将GNN的知识蒸馏到MLP，以提升自监督图表示学习的性能。


<details>
  <summary>Details</summary>
Motivation: 自监督图表示学习中，GNN的归纳偏置比监督学习更重要，但GNN模型容量大，而MLP模型容量小，存在巨大差距，因此需要新的蒸馏方法来缩小差距。

Method: 提出DAD-SGM（Diffusion-Assisted Distillation for Self-supervised Graph representation learning with MLPs）方法，使用去噪扩散模型作为教师助手，辅助GNN教师模型将知识蒸馏给学生MLP模型。

Result: DAD-SGM方法在自监督图表示学习方面，相比现有的GNN到MLP的蒸馏方法，能够更有效地蒸馏知识，并提升MLP模型的泛化性和鲁棒性。

Conclusion: DAD-SGM方法能够有效地将自监督GNN的知识蒸馏到MLP中，克服了模型容量的限制，并在自监督图表示学习任务上取得了优于现有方法的性能。

Abstract: For large-scale applications, there is growing interest in replacing Graph
Neural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via
knowledge distillation. However, distilling GNNs for self-supervised graph
representation learning into MLPs is more challenging. This is because the
performance of self-supervised learning is more related to the model's
inductive bias than supervised learning. This motivates us to design a new
distillation method to bridge a huge capacity gap between GNNs and MLPs in
self-supervised graph representation learning. In this paper, we propose
\textbf{D}iffusion-\textbf{A}ssisted \textbf{D}istillation for
\textbf{S}elf-supervised \textbf{G}raph representation learning with
\textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion
model as a teacher assistant to better distill the knowledge from the teacher
GNN into the student MLP. This approach enhances the generalizability and
robustness of MLPs in self-supervised graph representation learning. Extensive
experiments demonstrate that DAD-SGM effectively distills the knowledge of
self-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation
methods. Our implementation is available at
https://github.com/SeongJinAhn/DAD-SGM.

</details>


### [769] [Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing](https://arxiv.org/abs/2510.04263)
*Joseph Ramsey,Bryan Andrews*

Main category: cs.LG

TL;DR: 该论文提出了一系列基于评分的因果结构学习算法，用于处理存在潜在变量或选择偏差的数据。


<details>
  <summary>Details</summary>
Motivation: 从观察数据中学习因果结构，尤其是在存在潜在变量或选择偏差的情况下，具有挑战性。现有的FCI算法虽然能处理这种情况，但计算成本高且可能出现错误。

Method: 论文提出了几种算法：BOSS-FCI和GRaSP-FCI（用BOSS或GRaSP替代GFCI中的FGES），FCI Targeted-testing (FCIT)（用BOSS指导的定向测试替代穷尽性测试），以及LV-Dumb（直接返回BOSS DAG的PAG）。

Result: 模拟和真实数据分析表明，BOSS-FCI和GRaSP-FCI提供了可靠的基线；FCIT提高了效率和可靠性；LV-Dumb作为一个实用的启发式方法，在实践中表现出色。

Conclusion: 基于评分和定向的策略对于可扩展的潜在变量因果发现具有重要价值。

Abstract: Learning causal structure from observational data is especially challenging
when latent variables or selection bias are present. The Fast Causal Inference
(FCI) algorithm addresses this setting but often performs exhaustive
conditional independence tests across many subsets, leading to spurious
independence claims, extra or missing edges, and unreliable orientations. We
present a family of score-guided mixed-strategy causal search algorithms that
build on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,
straightforward variants of GFCI that substitute BOSS or GRaSP for FGES,
thereby retaining correctness while incurring different scalability tradeoffs.
Second, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method
that improves upon these variants by replacing exhaustive all-subsets testing
with targeted tests guided by BOSS, yielding well-formed PAGs with higher
precision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also
known as BOSS-POD), which bypasses latent-variable-specific reasoning and
directly returns the PAG of the BOSS DAG. Although not strictly correct in the
FCI sense, it scales better and often achieves superior accuracy in practice.
Simulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI
provide sound baselines, FCIT improves both efficiency and reliability, and
LV-Dumb offers a practical heuristic with strong empirical performance.
Together, these method highlight the value of score-guided and targeted
strategies for scalable latent-variable causal discovery.

</details>


### [770] [Influence branching for learning to solve mixed-integer programs online](https://arxiv.org/abs/2510.04273)
*Paul Strang,Zacharie Alès,Côme Bissuel,Olivier Juan,Safia Kedad-Sidhoum,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: 在线学习解决混合整数规划（MIP）的新方法，使用受影响分支和汤普森采样，取得了与最先进方法相媲美的结果，并能很好地泛化到更广泛的在线框架。


<details>
  <summary>Details</summary>
Motivation: 为混合整数规划（MIP）开发一种新的在线学习方法，以提高求解效率。

Method: 在分支定界算法的初始阶段应用一种名为“影响分支”的新型图导向变量选择策略，并使用汤普森采样进行在线优化，以根据计算加速情况对MIP结构的最佳图表示进行排名。

Result: 所提出的方法取得了与当前最先进的在线学习方法相当的结果，并且在更一般的在线框架下表现出良好的泛化能力。

Conclusion: 该方法在MIP的在线学习求解方面显示出有前景的结果，并且具有良好的泛化潜力。

Abstract: On the occasion of the 20th Mixed Integer Program Workshop's computational
competition, this work introduces a new approach for learning to solve MIPs
online. Influence branching, a new graph-oriented variable selection strategy,
is applied throughout the first iterations of the branch and bound algorithm.
This branching heuristic is optimized online with Thompson sampling, which
ranks the best graph representations of MIP's structure according to
computational speed up over SCIP. We achieve results comparable to state of the
art online learning methods. Moreover, our results indicate that our method
generalizes well to more general online frameworks, where variations in
constraint matrix, constraint vector and objective coefficients can all occur
and where more samples are available.

</details>


### [771] [DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks](https://arxiv.org/abs/2510.04331)
*Nghiem T. Diep,Hien Dang,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: DoRAN通过在DoRA的权重分解中注入噪声和使用辅助网络动态生成低秩矩阵来提高训练稳定性和样本效率，并在视觉和语言基准测试中优于其他PEFT方法。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法如DoRA虽然改进了LoRA，但在训练稳定性和样本效率方面仍有提升空间。本研究旨在提出一种新的DoRA变体，以进一步提高训练稳定性和样本效率。

Method: DoRAN通过两个关键阶段实现目标：1. 在DoRA的权重分解的分母中注入噪声，作为自适应正则化器以缓解不稳定性；2. 用能够动态生成低秩矩阵的辅助网络替换静态低秩矩阵，实现跨层参数耦合，从而在理论和实践中提高样本效率。

Result: 在视觉和语言基准测试上的广泛实验表明，DoRAN在性能上持续优于LoRA、DoRA及其他PEFT基线方法。

Conclusion: 本研究提出的DoRAN结合了基于噪声的正则化和基于网络的参数生成，在稳定性和效率方面均表现出色，为基础模型的鲁棒和高效微调提供了一个有前景的方向。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have become the standard
paradigm for adapting large-scale models. Among these techniques,
Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the
learning capacity and training stability of the vanilla Low-Rank Adaptation
(LoRA) method by explicitly decomposing pre-trained weights into magnitude and
directional components. In this work, we propose DoRAN, a new variant of DoRA
designed to further stabilize training and boost the sample efficiency of DoRA.
Our approach includes two key stages: (i) injecting noise into the denominator
of DoRA's weight decomposition, which serves as an adaptive regularizer to
mitigate instabilities; and (ii) replacing static low-rank matrices with
auxiliary networks that generate them dynamically, enabling parameter coupling
across layers and yielding better sample efficiency in both theory and
practice. Comprehensive experiments on vision and language benchmarks show that
DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These
results underscore the effectiveness of combining stabilization through
noise-based regularization with network-based parameter generation, offering a
promising direction for robust and efficient fine-tuning of foundation models.

</details>


### [772] [HoRA: Cross-Head Low-Rank Adaptation with Joint Hypernetworks](https://arxiv.org/abs/2510.04295)
*Nghiem T. Diep,Dung Le,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: HoRA是一种新的参数高效微调(PEFT)方法，通过联合超网络为注意力头的低秩矩阵生成，促进跨头信息共享，优于LoRA。


<details>
  <summary>Details</summary>
Motivation: LoRA在微调多头自注意力(MHA)时，单独调整每个注意力头，忽略了头之间的潜在协同作用。HoRA旨在解决此问题。

Method: HoRA利用联合超网络为多个注意力头生成低秩矩阵，通过共享生成器耦合它们的适应性，促进跨头信息共享。

Result: HoRA在准确性和样本效率方面优于LoRA和其他PEFT方法，且可训练参数量仅略有增加。

Conclusion: HoRA通过联合超网络实现了跨头信息共享，在各种语言和视觉基准测试中表现出比LoRA和其他PEFT方法更优越的性能和样本效率。

Abstract: Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT)
technique that adapts large pre-trained models by adding low-rank matrices to
their weight updates. However, in the context of fine-tuning multi-head
self-attention (MHA), LoRA has been employed to adapt each attention head
separately, thereby overlooking potential synergies across different heads. To
mitigate this issue, we propose a novel Hyper-shared Low-Rank Adaptation (HoRA)
method, which utilizes joint hypernetworks to generate low-rank matrices across
attention heads. By coupling their adaptation through a shared generator, HoRA
encourages cross-head information sharing, and thus directly addresses the
aforementioned limitation of LoRA. By comparing LoRA and HoRA through the lens
of hierarchical mixture of experts, our theoretical findings reveal that the
latter achieves superior sample efficiency to the former. Furthermore, through
extensive experiments across diverse language and vision benchmarks, we
demonstrate that HoRA outperforms LoRA and other PEFT methods while requiring
only a marginal increase in the number of trainable parameters.

</details>


### [773] [Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows](https://arxiv.org/abs/2510.04510)
*Achim Eckerle,Martin Spitznagel,Janis Keuper*

Main category: cs.LG

TL;DR: 该研究评估了条件归一化流（Full-Glow）模型在城市噪声预测中的应用，实现了比传统物理求解器快2000倍以上的速度，同时提高了非视线（NLoS）精度，适用于城市规划和合规性测绘。


<details>
  <summary>Details</summary>
Motivation: 城市噪音预测对于公众健康和城市管理至关重要，但现有的基于物理的求解器速度太慢，无法满足实时、迭代的“假设”研究需求。

Method: 使用条件归一化流（Full-Glow）模型，从2D城市布局生成符合标准的城市声压图，能够在商品级硬件上实时生成256x256分辨率的地图。

Result: 该模型在不同数据集上，将地图生成速度提高了2000多倍，非视线（NLoS）精度提高了24%，在基线NLoS场景下达到了0.65 dB的平均绝对误差（MAE），并能准确重现衍射和干涉模式。

Conclusion: 条件归一化流（Full-Glow）模型能够快速、准确地生成城市声压图，并支持在源或几何形状改变时进行即时重新计算，为城市规划、合规性测绘和运营提供了实用的解决方案。

Abstract: Accurate and fast urban noise prediction is pivotal for public health and for
regulatory workflows in cities, where the Environmental Noise Directive
mandates regular strategic noise maps and action plans, often needed in
permission workflows, right-of-way allocation, and construction scheduling.
Physics-based solvers are too slow for such time-critical, iterative "what-if"
studies. We evaluate conditional Normalizing Flows (Full-Glow) for generating
for generating standards-compliant urban sound-pressure maps from 2D urban
layouts in real time per 256x256 map on a single RTX 4090), enabling
interactive exploration directly on commodity hardware. On datasets covering
Baseline, Diffraction, and Reflection regimes, our model accelerates map
generation by >2000 times over a reference solver while improving NLoS accuracy
by up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE
with high structural fidelity. The model reproduces diffraction and
interference patterns and supports instant recomputation under source or
geometry changes, making it a practical engine for urban planning, compliance
mapping, and operations (e.g., temporary road closures, night-work variance
assessments).

</details>


### [774] [Activation Steering with a Feedback Controller](https://arxiv.org/abs/2510.04309)
*Dung V. Nguyen,Hieu M. Vu,Nhi Y. Pham,Lei Zhang,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种基于控制理论的PID启停方法，用于控制大型语言模型的行为，并通过实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）控制方法缺乏理论性能保证，主要基于经验。

Method: 提出了一种名为PID启停（PID Steering）的框架，将现有启停方法视为比例（P）控制器，并引入积分（I）和微分（D）项，形成一个完整的PID控制器，用于LLM的激活启停。P项对齐激活到目标语义方向，I项累积误差以保证跨层的持续修正，D项通过抵消快速激活变化来减少超调。

Result: PID启停框架具有可解释的误差动态，并将激活启停与经典控制理论中的稳定性保证联系起来。实验证明，PID启停在多个LLM家族和基准测试中，持续优于现有方法，实现了更鲁棒和可靠的行为控制。

Conclusion: PID启停是一个原则性的框架，它利用完整的PID控制器进行LLM的激活启停，与控制理论的稳定性保证相关联，并在实践中显示出优越的性能。

Abstract: Controlling the behaviors of large language models (LLM) is fundamental to
their safety alignment and reliable deployment. However, existing steering
methods are primarily driven by empirical insights and lack theoretical
performance guarantees. In this work, we develop a control-theoretic foundation
for activation steering by showing that popular steering methods correspond to
the proportional (P) controllers, with the steering vector serving as the
feedback signal. Building on this finding, we propose
Proportional-Integral-Derivative (PID) Steering, a principled framework that
leverages the full PID controller for activation steering in LLMs. The
proportional (P) term aligns activations with target semantic directions, the
integral (I) term accumulates errors to enforce persistent corrections across
layers, and the derivative (D) term mitigates overshoot by counteracting rapid
activation changes. This closed-loop design yields interpretable error dynamics
and connects activation steering to classical stability guarantees in control
theory. Moreover, PID Steering is lightweight, modular, and readily integrates
with state-of-the-art steering methods. Extensive experiments across multiple
LLM families and benchmarks demonstrate that PID Steering consistently
outperforms existing approaches, achieving more robust and reliable behavioral
control.

</details>


### [775] [Crash Severity Prediction Using Deep Learning Approaches: A Hybrid CNN-RNN Framework](https://arxiv.org/abs/2510.04316)
*Sahar Koohfar*

Main category: cs.LG

TL;DR: 该研究提出了一种混合CNN-RNN深度学习模型来预测交通事故的严重程度，并在弗吉尼亚州I-64公路的15870条事故记录数据集上进行了验证。实验结果表明，该混合模型在预测准确性方面优于传统的统计模型和单独的深度学习模型（如逻辑回归、朴素贝叶斯、KNN、决策树、RNN和CNN）。


<details>
  <summary>Details</summary>
Motivation: 准确及时地预测交通事故的严重程度对于减轻交通事故的严重后果至关重要，智能交通系统依赖有效的预测方法来提供适当的医疗援助和运输服务。

Method: 实施了一种混合CNN-RNN深度学习模型，并将其性能与逻辑回归、朴素贝叶斯分类器、K-最近邻（KNN）、决策树以及单独的RNN和CNN深度学习模型进行了比较。该研究采用了考虑交通事故各种特征之间相互关系的分析方法。

Result: 在弗吉尼亚州I-64公路2015年至2021年间的15870条事故记录数据集上，所提出的CNN-RNN混合模型在预测交通事故严重程度方面表现优于所有基准模型。

Conclusion: CNN-RNN混合模型结合了RNN和CNN的优点，在交通事故严重程度预测方面比其他模型具有更高的准确性，证明了其有效性。

Abstract: Accurate and timely prediction of crash severity is crucial in mitigating the
severe consequences of traffic accidents. Accurate and timely prediction of
crash severity is crucial in mitigating the severe consequences of traffic
accidents. In order to provide appropriate levels of medical assistance and
transportation services, an intelligent transportation system relies on
effective prediction methods. Deep learning models have gained popularity in
this domain due to their capability to capture non-linear relationships among
variables. In this research, we have implemented a hybrid CNN-RNN deep learning
model for crash severity prediction and compared its performance against widely
used statistical and machine learning models such as logistic regression,
na\"ive bayes classifier, K-Nearest Neighbors (KNN), decision tree, and
individual deep learning models: RNN and CNN. This study employs a methodology
that considers the interconnected relationships between various features of
traffic accidents. The study was conducted using a dataset of 15,870 accident
records gathered over a period of seven years between 2015 and 2021 on Virginia
highway I-64. The findings demonstrate that the proposed CNN-RNN hybrid model
has outperformed all benchmark models in terms of predicting crash severity.
This result illustrates the effectiveness of the hybrid model as it combines
the advantages of both RNN and CNN models in order to achieve greater accuracy
in the prediction process.

</details>


### [776] [FairAgent: Democratizing Fairness-Aware Machine Learning with LLM-Powered Agents](https://arxiv.org/abs/2510.04317)
*Yucong Dai,Lu Zhang,Feng Luo,Mashrur Chowdhury,Yongkai Wu*

Main category: cs.LG

TL;DR: FairAgent是一个LLM驱动的自动化系统，可以简化公平感知模型开发，自动分析数据中的偏差，处理数据预处理和特征工程，并根据用户要求实施适当的偏差缓解策略。


<details>
  <summary>Details</summary>
Motivation: 培训公平和无偏见的机器学习模型至关重要，但存在重大挑战。有效的偏差缓解需要公平定义、指标、数据预处理和机器学习技术的专业知识。此外，在平衡模型性能与公平性要求以及正确处理敏感属性之间取得平衡的复杂过程，使得许多从业者无法进行公平感知模型开发。

Method: FairAgent是一个LLM驱动的自动化系统，可以自动分析数据集中的潜在偏差，处理数据预处理和特征工程，并实施适当的偏差缓解策略。

Result: FairAgent在提高模型性能的同时，显著减少了开发时间和专业知识要求。

Conclusion: FairAgent使公平感知机器学习更容易被从业者使用。

Abstract: Training fair and unbiased machine learning models is crucial for high-stakes
applications, yet it presents significant challenges. Effective bias mitigation
requires deep expertise in fairness definitions, metrics, data preprocessing,
and machine learning techniques. In addition, the complex process of balancing
model performance with fairness requirements while properly handling sensitive
attributes makes fairness-aware model development inaccessible to many
practitioners. To address these challenges, we introduce FairAgent, an
LLM-powered automated system that significantly simplifies fairness-aware model
development. FairAgent eliminates the need for deep technical expertise by
automatically analyzing datasets for potential biases, handling data
preprocessing and feature engineering, and implementing appropriate bias
mitigation strategies based on user requirements. Our experiments demonstrate
that FairAgent achieves significant performance improvements while
significantly reducing development time and expertise requirements, making
fairness-aware machine learning more accessible to practitioners.

</details>


### [777] [FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields](https://arxiv.org/abs/2510.04325)
*Kenechukwu Ogbuagu,Sepehr Maleki,Giuseppe Bruni,Senthil Krishnababu*

Main category: cs.LG

TL;DR: FoilDiff是一种基于扩散模型的混合骨干去噪网络，用于预测空气动力学中的流动场，相比现有模型能将平均预测误差降低高达85%。


<details>
  <summary>Details</summary>
Motivation: 计算流体动力学（CFD）模型在预测空气动力学流动场方面虽然有效但计算成本高昂，因此需要开发能更快进行预测的代理模型。

Method: 提出了一种名为FoilDiff的基于扩散模型的代理模型，该模型采用混合骨干去噪网络，结合了卷积特征提取和基于Transformer的全局注意力机制，并利用去噪扩散隐式模型（DDIM）采样来优化采样效率。

Result: FoilDiff在评估中显示出显著的性能提升，平均预测误差降低高达85%，并且在预测准确性和预测不确定性校准方面优于现有的基于扩散的模型。

Conclusion: FoilDiff能够提供比现有基于扩散的模型更准确的预测和更好校准的预测不确定性。

Abstract: The accurate prediction of flow fields around airfoils is crucial for
aerodynamic design and optimisation. Computational Fluid Dynamics (CFD) models
are effective but computationally expensive, thus inspiring the development of
surrogate models to enable quicker predictions. These surrogate models can be
based on deep learning architectures, such as Convolutional Neural Networks
(CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusion
models have shown significant promise in predicting complex flow fields. In
this work, we propose FoilDiff, a diffusion-based surrogate model with a
hybrid-backbone denoising network. This hybrid design combines the power of
convolutional feature extraction and transformer-based global attention to
generate more adaptable and accurate representations of flow structures.
FoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) sampling
to optimise the efficiency of the sampling process at no additional cost to
model generalisation. We used encoded representations of Reynolds number, angle
of attack, and airfoil geometry to define the input space for generalisation
across a wide range of aerodynamic conditions. When evaluated against
state-of-the-art models, FoilDiff shows significant performance improvements,
with mean prediction errors reducing by up to 85\% on the same datasets. The
results have demonstrated that FoilDiff can provide both more accurate
predictions and better-calibrated predictive uncertainty than existing
diffusion-based models.

</details>


### [778] [Post-training quantization of vision encoders needs prefixing registers](https://arxiv.org/abs/2510.04547)
*Seunghyeon Kim,Jinho Kim,Taesun Yeom,Wonpyo Park,Kyuyeun Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: RegCache是一种无需训练的算法，通过引入“脏”前缀token来解决视觉编码器中激活值异常值的问题，从而提高量化精度，并在文本监督和自监督视觉编码器上都取得了改进。


<details>
  <summary>Details</summary>
Motivation: Transformer视觉编码器（如CLIP）在多模态智能中至关重要，但其推理成本高昂，尤其是在处理海量视觉数据时，这使得降低其推理成本变得至关重要。后训练量化是一种实用的方法，但由于存在大量的异常值，即使在8位精度下也面临挑战。

Method: RegCache算法通过向目标视觉编码器引入易于产生异常值但语义上无意义的前缀token，来防止其他token出现异常值。该方法还包含中间层前缀和token删除两个技术创新点，以应对视觉编码器中异常值的特性。

Result: RegCache算法能够有效缓解视觉编码器中的异常值问题，显著降低量化精度损失。实验表明，该方法在文本监督和自监督视觉编码器上都能持续提升量化模型的准确性。

Conclusion: RegCache通过引入语义无关的前缀token来处理视觉编码器中的异常值，是一种有效的、无需训练的量化优化方法，能够提高模型在实际应用中的效率和性能。

Abstract: Transformer-based vision encoders -- such as CLIP -- are central to
multimodal intelligence, powering applications from autonomous web agents to
robotic control. Since these applications often demand real-time processing of
massive visual data, reducing the inference cost of vision encoders is
critical. Post-training quantization offers a practical path, but remains
challenging even at 8-bit precision due to massive-scale activations (i.e.,
outliers). In this work, we propose $\textit{RegCache}$, a training-free
algorithm to mitigate outliers in vision encoders, enabling quantization with
significantly smaller accuracy drops. The proposed RegCache introduces
outlier-prone yet semantically meaningless prefix tokens to the target vision
encoder, which prevents other tokens from having outliers. Notably, we observe
that outliers in vision encoders behave differently from those in language
models, motivating two technical innovations: middle-layer prefixing and token
deletion. Experiments show that our method consistently improves the accuracy
of quantized models across both text-supervised and self-supervised vision
encoders.

</details>


### [779] [Arithmetic-Mean $μ$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets](https://arxiv.org/abs/2510.04327)
*Haosong Zhang,Shenxi Wu,Yichi Zhang,Wei Lin*

Main category: cs.LG

TL;DR: AM-μP 是一种新的学习率方法，通过约束整个网络的平均前激活二阶矩来解决深度学习中学习率选择的挑战，并能实现零样本学习率迁移。


<details>
  <summary>Details</summary>
Motivation: 经典的 Maximal Update Parameterization (μP) 适用于同质网络，但在异质网络中存在问题。需要一种新的方法来解决深度网络中学习率选择的挑战，特别是对于残差网络和卷积网络。

Method: 提出 AM-μP，约束网络平均一阶前激活二阶矩为常数。结合残差感知初始化，实现宽度鲁棒的深度律。推导出一维和二维卷积网络以及标准残差网络的最大更新学习率与深度 L 的关系为 η*(L) ∝ L^{-3/2}。

Result: AM-μP 在不同深度的网络上验证了 -3/2 的缩放定律，并实现了零样本学习率迁移，无需额外调优。

Conclusion: AM-μP 为卷积网络和深度残差网络提供了一个统一且实用的学习率原则，解决了经典 μP 在异质网络中的局限性。

Abstract: Choosing an appropriate learning rate remains a key challenge in scaling
depth of modern deep networks. The classical maximal update parameterization
($\mu$P) enforces a fixed per-layer update magnitude, which is well suited to
homogeneous multilayer perceptrons (MLPs) but becomes ill-posed in
heterogeneous architectures where residual accumulation and convolutions
introduce imbalance across layers. We introduce Arithmetic-Mean $\mu$P
(AM-$\mu$P), which constrains not each individual layer but the network-wide
average one-step pre-activation second moment to a constant scale. Combined
with a residual-aware He fan-in initialization - scaling residual-branch
weights by the number of blocks ($\mathrm{Var}[W]=c/(K\cdot
\mathrm{fan\text{-}in})$) - AM-$\mu$P yields width-robust depth laws that
transfer consistently across depths. We prove that, for one- and
two-dimensional convolutional networks, the maximal-update learning rate
satisfies $\eta^\star(L)\propto L^{-3/2}$; with zero padding, boundary effects
are constant-level as $N\gg k$. For standard residual networks with general
conv+MLP blocks, we establish $\eta^\star(L)=\Theta(L^{-3/2})$, with $L$ the
minimal depth. Empirical results across a range of depths confirm the $-3/2$
scaling law and enable zero-shot learning-rate transfer, providing a unified
and practical LR principle for convolutional and deep residual networks without
additional tuning overhead.

</details>


### [780] [SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator](https://arxiv.org/abs/2510.04576)
*Yuhta Takida,Satoshi Hayakawa,Takashi Shibuya,Masaaki Imaizumi,Naoki Murata,Bac Nguyen,Toshimitsu Uesaka,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 本篇论文提出了一种新的条件生成对抗网络判别器SONA，通过结合无条件判别、匹配感知监督和自适应加权，解决了条件生成中真实性和条件对齐的平衡难题，并在图像和文本到图像生成任务上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的条件生成对抗网络在判别器中难以平衡真实性和条件对齐这两个目标。

Method: 提出了一种名为SONA的新型判别器设计，它集成了无条件判别、匹配感知监督和自适应加权。SONA在最后一层使用独立的投影来分别处理真实性和对齐性，并辅以专门的目标函数和自适应加权机制。

Result: 在类别条件生成任务和文本到图像生成任务的实验中，SONA在样本质量和条件对齐方面均优于现有最先进的方法。

Conclusion: SONA能够有效地平衡真实性和条件对齐，并且在多种条件生成任务中表现出优越的性能和鲁棒性。

Abstract: Deep generative models have made significant advances in generating complex
content, yet conditional generation remains a fundamental challenge. Existing
conditional generative adversarial networks often struggle to balance the dual
objectives of assessing authenticity and conditional alignment of input samples
within their conditional discriminators. To address this, we propose a novel
discriminator design that integrates three key capabilities: unconditional
discrimination, matching-aware supervision to enhance alignment sensitivity,
and adaptive weighting to dynamically balance all objectives. Specifically, we
introduce Sum of Naturalness and Alignment (SONA), which employs separate
projections for naturalness (authenticity) and alignment in the final layer
with an inductive bias, supported by dedicated objective functions and an
adaptive weighting mechanism. Extensive experiments on class-conditional
generation tasks show that \ours achieves superior sample quality and
conditional alignment compared to state-of-the-art methods. Furthermore, we
demonstrate its effectiveness in text-to-image generation, confirming the
versatility and robustness of our approach.

</details>


### [781] [Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies](https://arxiv.org/abs/2510.04341)
*G. Niklas Noren,Eva-Lisa Meldau,Johan Ellenius*

Main category: cs.LG

TL;DR: AI模型在低患病率事件识别中可能表现出虚假的准确性，尤其是在高风险应用中。本文提出了一个用于评估此类AI模型的框架，包括问题设定、测试集设计、考虑患病率的统计评估、鲁棒性评估以及与人类工作流程的整合。此外，还提出了一种结构化案例级别检查（SCLE）方法，以补充统计性能评估，并提供了一个全面的检查清单来指导AI模型的采购或开发。该框架在药物警戒领域进行了实例分析，涵盖了基于规则的报告检索、结合机器学习和概率记录链接的重复数据检测以及使用LLM自动编辑人名。文章强调了低患病率场景下的特有陷阱，如不切实际的类别平衡导致的乐观情绪以及测试集中缺乏困难的正样本，并展示了成本敏感目标如何使模型性能与运营价值保持一致。尽管基于药物警戒实践，但这些原则也适用于阳性样本稀缺且错误成本不对称的领域。


<details>
  <summary>Details</summary>
Motivation: 高风险AI应用通常针对低患病率事件，这可能导致表面上的高准确性掩盖了有限的实际价值。因此，有必要对用于识别罕见事件的AI模型进行批判性评估。

Method: 提出一个框架，包含问题设定、测试集设计、患病率感知的统计评估、鲁棒性评估和与人类工作流的整合。引入结构化案例级别检查（SCLE）以补充统计评估，并提供了一个全面的检查清单。在药物警戒领域，通过三个研究（基于规则的妊娠相关报告检索、机器学习与概率记录链接结合的重复检测、LLM驱动的姓名识别）来实例化该框架。

Result: 识别出低患病率场景下的特有陷阱，例如不切实际的类别平衡和测试集中缺乏困难的正样本。展示了成本敏感的目标如何使模型性能与运营价值保持一致。

Conclusion: 尽管该框架在药物警戒领域进行了实例化，但其原则也适用于阳性样本稀缺且错误成本不对称的其他领域，为在这些领域中评估和开发AI模型提供了指导。

Abstract: Many high-stakes AI applications target low-prevalence events, where apparent
accuracy can conceal limited real-world value. Relevant AI models range from
expert-defined rules and traditional machine learning to generative LLMs
constrained for classification. We outline key considerations for critical
appraisal of AI in rare-event recognition, including problem framing and test
set design, prevalence-aware statistical evaluation, robustness assessment, and
integration into human workflows. In addition, we propose an approach to
structured case-level examination (SCLE), to complement statistical performance
evaluation, and a comprehensive checklist to guide procurement or development
of AI models for rare-event recognition. We instantiate the framework in
pharmacovigilance, drawing on three studies: rule-based retrieval of
pregnancy-related reports; duplicate detection combining machine learning with
probabilistic record linkage; and automated redaction of person names using an
LLM. We highlight pitfalls specific to the rare-event setting including
optimism from unrealistic class balance and lack of difficult positive controls
in test sets - and show how cost-sensitive targets align model performance with
operational value. While grounded in pharmacovigilance practice, the principles
generalize to domains where positives are scarce and error costs may be
asymmetric.

</details>


### [782] [Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics](https://arxiv.org/abs/2510.04342)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: 通过引入基于动力学系统理论的课程学习方法，CCF能够提高预测混沌系统的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在预测混沌系统时存在泛化能力不足或无法学习特定动力学机制的缺陷。而CCF通过组织训练数据，从简单周期行为过渡到复杂混沌动力学，以解决这些问题。

Method: CCF方法首先在可预测系统上训练序列模型，然后逐步引入更混沌的轨迹，并利用最大李雅普诺夫指数和吸引子维度来量化复杂性，从而构建一个包含50多个合成常微分方程/偏微分方程系统的训练课程。

Result: 在未见过的真实世界基准测试（如太阳黑子数、电力需求、心电图信号）上，CCF将有效预测范围延长了40%（相比随机顺序训练）并提高了一倍以上（相比仅用真实世界数据训练）。

Conclusion: CCF课程学习方法能显著提升模型在各种神经网络架构（GRU、Transformer）上的预测性能，并验证了课程结构的重要性。

Abstract: Forecasting chaotic systems is a cornerstone challenge in many scientific
fields, complicated by the exponential amplification of even infinitesimal
prediction errors. Modern machine learning approaches often falter due to two
opposing pitfalls: over-specializing on a single, well-known chaotic system
(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing
vast, unrelated time-series, which prevents the model from learning the nuances
of any specific dynamical regime. We propose Curriculum Chaos Forecasting
(CCF), a training paradigm that bridges this gap. CCF organizes training data
based on fundamental principles of dynamical systems theory, creating a
curriculum that progresses from simple, periodic behaviors to highly complex,
chaotic dynamics. We quantify complexity using the largest Lyapunov exponent
and attractor dimension, two well-established metrics of chaos. By first
training a sequence model on predictable systems and gradually introducing more
chaotic trajectories, CCF enables the model to build a robust and generalizable
representation of dynamical behaviors. We curate a library of over 50 synthetic
ODE/PDE systems to build this curriculum. Our experiments show that
pre-training with CCF significantly enhances performance on unseen, real-world
benchmarks. On datasets including Sunspot numbers, electricity demand, and
human ECG signals, CCF extends the valid prediction horizon by up to 40%
compared to random-order training and more than doubles it compared to training
on real-world data alone. We demonstrate that this benefit is consistent across
various neural architectures (GRU, Transformer) and provide extensive ablations
to validate the importance of the curriculum's structure.

</details>


### [783] [From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere](https://arxiv.org/abs/2510.04357)
*Anoushka Harit,Zhongtian Sun,Jongmin Yu*

Main category: cs.LG

TL;DR: CSHT是一种新的金融时间序列预测模型，结合了因果超图结构、黎曼几何和因果掩码Transformer注意力，在S&P 500数据上表现优于基线模型，并能在不确定性下进行可解释的金融预测。


<details>
  <summary>Details</summary>
Motivation: 金融市场预测面临不确定性，需要一种能够处理多变量依赖关系、在不同市场环境下都能泛化且具有可解释性的模型。

Method: 提出Causal Sphere Hypergraph Transformer (CSHT)模型，该模型整合了因果超图结构、黎曼几何和因果掩码Transformer注意力。通过提取多元Granger因果依赖关系，并将其编码为超球体表面的方向超边。注意力机制通过角度掩码进行约束，以保持时间方向性和几何一致性。

Result: 在2018年至2023年的S&P 500数据（包括2020年COVID-19冲击）上进行评估，CSHT在收益率预测、状态分类和顶尖资产排名任务上持续优于基线模型。

Conclusion: CSHT通过强制执行预测因果结构和在黎曼流形中嵌入变量，实现了跨市场状态的稳健泛化和从宏观经济事件到股票级别响应的透明归因路径。该模型为不确定性下的金融预测提供了一个有原则且实用的解决方案。

Abstract: We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel
architecture for interpretable financial time-series forecasting that unifies
\emph{Granger-causal hypergraph structure}, \emph{Riemannian geometry}, and
\emph{causally masked Transformer attention}. CSHT models the directional
influence of financial news and sentiment on asset returns by extracting
multivariate Granger-causal dependencies, which are encoded as directional
hyperedges on the surface of a hypersphere. Attention is constrained via
angular masks that preserve both temporal directionality and geometric
consistency. Evaluated on S\&P 500 data from 2018 to 2023, including the 2020
COVID-19 shock, CSHT consistently outperforms baselines across return
prediction, regime classification, and top-asset ranking tasks. By enforcing
predictive causal structure and embedding variables in a Riemannian manifold,
CSHT delivers both \emph{robust generalisation across market regimes} and
\emph{transparent attribution pathways} from macroeconomic events to
stock-level responses. These results suggest that CSHT is a principled and
practical solution for trustworthy financial forecasting under uncertainty.

</details>


### [784] [Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework](https://arxiv.org/abs/2510.04366)
*Christopher Klugmann,Daniel Kondermann*

Main category: cs.LG

TL;DR: 人类标注中的不确定性可以通过量化类别间的模糊性来衡量。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效区分类别间模糊性和无法解决的不确定性，需要新的度量方法。

Method: 提出一种新的模糊性度量，该度量将离散响应分布映射到[0,1]区间，并考虑了“无法解决”类别的不对称性。该度量与二次熵相关，但有所不同。此外，还开发了用于估计总体模糊性的频率学点估计量，并推导了由狄利克雷先验引起的模糊性贝叶斯后验。

Result: 通过数值示例展示了该度量在估计、校准以及在数据集质量评估和下游机器学习工作流中的实际应用。

Conclusion: 所提出的模糊性度量及其推断工具能够有效地区分不同来源的不确定性，并为数据集质量评估和机器学习应用提供支持。

Abstract: Human-generated categorical annotations frequently produce empirical response
distributions (soft labels) that reflect ambiguity rather than simple annotator
error. We introduce an ambiguity measure that maps a discrete response
distribution to a scalar in the unit interval, designed to quantify aleatoric
uncertainty in categorical tasks. The measure bears a close relationship to
quadratic entropy (Gini-style impurity) but departs from those indices by
treating an explicit "can't solve" category asymmetrically, thereby separating
uncertainty arising from class-level indistinguishability from uncertainty due
to explicit unresolvability. We analyze the measure's formal properties and
contrast its behavior with a representative ambiguity measure from the
literature. Moving beyond description, we develop statistical tools for
inference: we propose frequentist point estimators for population ambiguity and
derive the Bayesian posterior over ambiguity induced by Dirichlet priors on the
underlying probability vector, providing a principled account of epistemic
uncertainty. Numerical examples illustrate estimation, calibration, and
practical use for dataset-quality assessment and downstream machine-learning
workflows.

</details>


### [785] [GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks](https://arxiv.org/abs/2510.04374)
*Tejal Patwardhan,Rachel Dias,Elizabeth Proehl,Grace Kim,Michele Wang,Olivia Watkins,Simón Posada Fishman,Marwan Aljubeh,Phoebe Thacker,Laurance Fauconnet,Natalie S. Kim,Patrick Chao,Samuel Miserendino,Gildas Chabot,David Li,Michael Sharman,Alexandra Barr,Amelia Glaese,Jerry Tworek*

Main category: cs.LG

TL;DR: GDPval是一个评估AI模型在真实世界经济价值任务能力的基准，涵盖了美国劳工统计局针对9个主要行业44种职业的工作活动。研究发现，前沿模型在此基准上的表现随时间线性提升，并且在交付质量上接近行业专家。通过分析，可以发现结合人工监督的前沿模型有望比独立专家更便宜、更快速地完成GDPval任务。此外，增加推理、任务上下文和脚手架能够提升模型在GDPval上的表现。最后，该研究开源了一个包含220个任务的黄金子集，并提供了一个公共自动评分服务以促进对真实世界模型能力的研究。


<details>
  <summary>Details</summary>
Motivation: 评估AI模型在真实世界经济价值任务中的能力，并提供一个基准来衡量和推动这一领域的研究。

Method: 构建了一个名为GDPval的基准，其中包含美国劳工统计局针对9个主要行业44种职业的工作活动。利用具有平均14年经验的行业专业人士的代表性工作来构建任务。对前沿模型在GDPval上的表现进行了评估和分析，并研究了增加推理、任务上下文和脚手架对模型性能的影响。

Result: 前沿模型在GDPval上的表现随时间大致呈线性提升。当前最好的前沿模型在交付质量上接近行业专家。结合人工监督的前沿模型在执行GDPval任务方面可能比独立专家更便宜、更快速。增加推理、任务上下文和脚手架能够提升模型在GDPval上的表现。

Conclusion: GDPval是一个评估AI模型在真实世界经济价值任务能力的基准。前沿模型在该基准上的表现正在稳步提升，并有望在不久的将来达到甚至超越人类专家的水平。结合人工监督和优化任务环境（如增加推理、上下文和脚手架）可以进一步提升模型在这些任务上的表现。研究者通过开源部分基准和提供自动评分服务，鼓励了对AI真实世界能力的研究。

Abstract: We introduce GDPval, a benchmark evaluating AI model capabilities on
real-world economically valuable tasks. GDPval covers the majority of U.S.
Bureau of Labor Statistics Work Activities for 44 occupations across the top 9
sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are
constructed from the representative work of industry professionals with an
average of 14 years of experience. We find that frontier model performance on
GDPval is improving roughly linearly over time, and that the current best
frontier models are approaching industry experts in deliverable quality. We
analyze the potential for frontier models, when paired with human oversight, to
perform GDPval tasks cheaper and faster than unaided experts. We also
demonstrate that increased reasoning effort, increased task context, and
increased scaffolding improves model performance on GDPval. Finally, we
open-source a gold subset of 220 tasks and provide a public automated grading
service at evals.openai.com to facilitate future research in understanding
real-world model capabilities.

</details>


### [786] [Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains](https://arxiv.org/abs/2510.04375)
*Akshay Mittal,Vinay Venkatesh,Krishna Kandi,Shalini Sudarshan*

Main category: cs.LG

TL;DR: 提出一种动态加权损失函数，以解决稀疏或小众领域中“重度用户”推荐的挑战。


<details>
  <summary>Details</summary>
Motivation: 单模型顺序推荐架构在处理稀疏或小众领域时，对“重度用户”的推荐效果有限。先前的 PinnerFormerLite 使用固定的加权损失，但在交互数据极少的小众领域，这种固定权重可能导致训练信号被稀释。

Method: 提出一种数据驱动的动态加权损失函数，该函数根据训练数据中每个域的稀疏度自适应地调整损失权重，稀疏域赋予更高权重，稠密域赋予更低权重。

Result: 在 MovieLens、Amazon Electronics、Yelp Business 和 LastFM Music 四个数据集上的实验表明，该动态加权方法显著优于包括 SIGMA、CALRec 和 SparseEnNet 在内的最先进基线方法，尤其是在稀疏域，召回率（Recall@10）和 NDCG@10 等关键指标有大幅提升，同时在稠密域保持了性能，并且计算开销极小。

Conclusion: 该动态加权损失函数能够有效解决稀疏域推荐中的挑战，通过自适应地调整权重，确保用户稀疏兴趣也能贡献有意义的梯度信号，从而提升推荐效果。

Abstract: The effectiveness of single-model sequential recommendation architectures,
while scalable, is often limited when catering to "power users" in sparse or
niche domains. Our previous research, PinnerFormerLite, addressed this by using
a fixed weighted loss to prioritize specific domains. However, this approach
can be sub-optimal, as a single, uniform weight may not be sufficient for
domains with very few interactions, where the training signal is easily diluted
by the vast, generic dataset.
  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss
function with comprehensive theoretical foundations and extensive empirical
validation. We introduce an adaptive algorithm that adjusts the loss weight for
each domain based on its sparsity in the training data, assigning a higher
weight to sparser domains and a lower weight to denser ones. This ensures that
even rare user interests contribute a meaningful gradient signal, preventing
them from being overshadowed.
  We provide rigorous theoretical analysis including convergence proofs,
complexity analysis, and bounds analysis to establish the stability and
efficiency of our approach. Our comprehensive empirical validation across four
diverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)
with state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that
this dynamic weighting system significantly outperforms all comparison methods,
particularly for sparse domains, achieving substantial lifts in key metrics
like Recall at 10 and NDCG at 10 while maintaining performance on denser
domains and introducing minimal computational overhead.

</details>


### [787] [Categorical Invariants of Learning Dynamics](https://arxiv.org/abs/2510.04376)
*Abdulrahman Tamim*

Main category: cs.LG

TL;DR: 我们将神经网络训练视为一种结构保持变换，而不是梯度下降。


<details>
  <summary>Details</summary>
Motivation: 传统的梯度下降视角无法解释不同训练运行为何能达到相似的测试性能，也无法解释优化算法之间的等价性。

Method: 我们提出了一种基于范畴论的视角，将学习视为参数空间到表示空间之间的结构保持变换（一个函子 L）。我们使用持久同调来识别预测泛化的稳定最小值，并使用拉回构造来形式化迁移学习。

Result: 实验表明，同伦训练路径的泛化误差相近（误差小于0.5%），而非同伦路径的误差差异超过3%。持久同调识别的稳定最小值与泛化能力的相关性为0.82。

Conclusion: 范畴论框架不仅为深度学习的工作原理提供了理论见解，还为训练更鲁棒的网络提供了具体的算法原则。

Abstract: Neural network training is typically viewed as gradient descent on a loss
surface. We propose a fundamentally different perspective: learning is a
structure-preserving transformation (a functor L) between the space of network
parameters (Param) and the space of learned representations (Rep). This
categorical framework reveals that different training runs producing similar
test performance often belong to the same homotopy class (continuous
deformation family) of optimization paths. We show experimentally that networks
converging via homotopic trajectories generalize within 0.5% accuracy of each
other, while non-homotopic paths differ by over 3%. The theory provides
practical tools: persistent homology identifies stable minima predictive of
generalization (R^2 = 0.82 correlation), pullback constructions formalize
transfer learning, and 2-categorical structures explain when different
optimization algorithms yield functionally equivalent models. These categorical
invariants offer both theoretical insight into why deep learning works and
concrete algorithmic principles for training more robust networks.

</details>


### [788] [Score-based Greedy Search for Structure Identification of Partially Observed Linear Causal Models](https://arxiv.org/abs/2510.04378)
*Xinshuai Dong,Ignavier Ng,Haoyue Dai,Jiaqi Sun,Xiangchen Song,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的基于评分的因果发现方法LGES，用于在存在潜在变量的情况下识别因果系统结构，并保证了可识别性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于约束的因果发现方法在存在潜在变量的场景下会面临多重检验和错误传播的挑战，因此需要一种基于评分的方法来解决这个问题。

Method: 提出了一种名为广义N因子模型（Generalized N Factor Model）的方法，并设计了一个名为潜在变量贪婪等价搜索（Latent variable Greedy Equivalence Search, LGES）的贪婪搜索算法来寻找最优结构。

Result: 该方法在合成数据和真实数据集上的实验验证了其有效性，并保证了在潜在变量存在的情况下，因果结构可以被识别到马尔可夫等价类的程度。

Conclusion: LGES是第一个具有可识别性保证的、用于识别包含潜在变量的因果系统的基于评分的贪婪搜索方法。

Abstract: Identifying the structure of a partially observed causal system is essential
to various scientific fields. Recent advances have focused on constraint-based
causal discovery to solve this problem, and yet in practice these methods often
face challenges related to multiple testing and error propagation. These issues
could be mitigated by a score-based method and thus it has raised great
attention whether there exists a score-based greedy search method that can
handle the partially observed scenario. In this work, we propose the first
score-based greedy search method for the identification of structure involving
latent variables with identifiability guarantees. Specifically, we propose
Generalized N Factor Model and establish the global consistency:
  the true structure including latent variables can be identified up to the
Markov equivalence class by using score. We then design
  Latent variable Greedy Equivalence Search (LGES), a greedy search algorithm
for this class of model with well-defined operators,
  which search very efficiently over the graph space to find the optimal
structure. Our experiments on both synthetic and real-life data validate the
effectiveness of our method (code will be publicly available).

</details>


### [789] [SSM-CGM: Interpretable State-Space Forecasting Model of Continuous Glucose Monitoring for Personalized Diabetes Management](https://arxiv.org/abs/2510.04386)
*Shakson Isaac,Yentl Collin,Chirag Patel*

Main category: cs.LG

TL;DR: SSM-CGM是一个基于Mamba的神经网络状态空间模型，用于连续血糖监测（CGM）数据的短期预测，在提高预测准确性的同时，通过变量选择和时间归因提供了可解释性，并能模拟生理信号变化对血糖的影响，从而实现个性化糖尿病管理。


<details>
  <summary>Details</summary>
Motivation: 现有的连续血糖监测（CGM）数据分析模型缺乏临床可解释性，限制了其在糖尿病管理中的应用。

Method: 提出SSM-CGM模型，该模型基于Mamba架构，结合了CGM数据和可穿戴设备活动信号，并实现了变量选择和时间归因功能，以提供模型的可解释性。此外，该模型能够进行反事实预测，模拟生理信号变化对血糖的影响。

Result: SSM-CGM在短期预测方面优于时间融合Transformer基线模型，并提供了可解释性功能，能够模拟生理信号变化对血糖的影响。

Conclusion: SSM-CGM是一个可解释的、基于生理学原理的框架，适用于个性化糖尿病管理。

Abstract: Continuous glucose monitoring (CGM) generates dense data streams critical for
diabetes management, but most used forecasting models lack interpretability for
clinical use. We present SSM-CGM, a Mamba-based neural state-space forecasting
model that integrates CGM and wearable activity signals from the AI-READI
cohort. SSM-CGM improves short-term accuracy over a Temporal Fusion Transformer
baseline, adds interpretability through variable selection and temporal
attribution, and enables counterfactual forecasts simulating how planned
changes in physiological signals (e.g., heart rate, respiration) affect
near-term glucose. Together, these features make SSM-CGM an interpretable,
physiologically grounded framework for personalized diabetes management.

</details>


### [790] [Achieve Performatively Optimal Policy for Performative Reinforcement Learning](https://arxiv.org/abs/2510.04430)
*Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 在此论文中，我们提出了一个零阶弗兰克-沃尔夫（0-FW）算法，以在策略改变环境动态的场景下，实现期望的 the desired performatively optimal (PO) policy。


<details>
  <summary>Details</summary>
Motivation: 现有的 performative reinforcement learning (PRL) 研究主要集中在找到 performatively stable (PS) policy，但 PS policy 与期望的 performatively optimal (PO) policy 之间存在一个固定的差距。本研究旨在弥合这一差距。

Method: 本研究提出了一种零阶弗兰克-沃尔夫（0-FW）算法，该算法在弗兰克-沃尔夫框架内使用 performative policy gradient 的零阶近似。此外，我们还证明了在策略正则化器支配环境偏移的标准条件下，该算法能够收敛到 PO policy。

Result: 该 0-FW 算法是首个能在多项式时间内收敛到 PO policy 的算法。实验结果表明，与现有算法相比，0-FW 算法在寻找 PO policy 方面更有效。

Conclusion: 本研究成功地提出了一种能够收敛到 performatively optimal (PO) policy 的新算法，解决了现有 PR L 研究中的一个关键挑战。

Abstract: Performative reinforcement learning is an emerging dynamical decision making
framework, which extends reinforcement learning to the common applications
where the agent's policy can change the environmental dynamics. Existing works
on performative reinforcement learning only aim at a performatively stable (PS)
policy that maximizes an approximate value function. However, there is a
provably positive constant gap between the PS policy and the desired
performatively optimal (PO) policy that maximizes the original value function.
In contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)
algorithm with a zeroth-order approximation of the performative policy gradient
in the Frank-Wolfe framework, and obtains \textbf{the first polynomial-time
convergence to the desired PO} policy under the standard regularizer dominance
condition. For the convergence analysis, we prove two important properties of
the nonconvex value function. First, when the policy regularizer dominates the
environmental shift, the value function satisfies a certain gradient dominance
property, so that any stationary point (not PS) of the value function is a
desired PO. Second, though the value function has unbounded gradient, we prove
that all the sufficiently stationary points lie in a convex and compact policy
subspace $\Pi_{\Delta}$, where the policy value has a constant lower bound
$\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.
Experimental results also demonstrate that our 0-FW algorithm is more effective
than the existing algorithms in finding the desired PO policy.

</details>


### [791] [Trade-off in Estimating the Number of Byzantine Clients in Federated Learning](https://arxiv.org/abs/2510.04432)
*Ziyi Chen,Su Zhang,Heng Huang*

Main category: cs.LG

TL;DR: 对联邦学习中的鲁棒聚合器进行了理论分析，重点研究了拜占庭客户端数量估计误差对性能的影响，并提出了最优聚合策略。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究缺乏对拜占庭客户端数量估计误差对鲁棒聚合器性能影响的系统性研究。

Method: 通过理论分析，推导了在不同拜占庭客户端数量（f）和聚合器鲁棒性设置（f̂）下，聚合器和联邦学习算法的最坏情况误差界限。

Result: 证明了低估（f̂<f）会导致性能任意下降；对于非低估（f̂≥f），证明了误差界限的最优性，且误差与f̂/(n-f-f̂)成正比，并揭示了聚合器鲁棒性设置（f̂）与性能之间的权衡关系。

Conclusion: 聚合器鲁棒性设置（f̂）需要在容忍更多拜占庭客户端（f）和优化性能（尤其是在拜占庭客户端较少或不存在时）之间取得平衡。

Abstract: Federated learning has attracted increasing attention at recent large-scale
optimization and machine learning research and applications, but is also
vulnerable to Byzantine clients that can send any erroneous signals. Robust
aggregators are commonly used to resist Byzantine clients. This usually
requires to estimate the unknown number $f$ of Byzantine clients, and thus
accordingly select the aggregators with proper degree of robustness (i.e., the
maximum number $\hat{f}$ of Byzantine clients allowed by the aggregator). Such
an estimation should have important effect on the performance, which has not
been systematically studied to our knowledge. This work will fill in the gap by
theoretically analyzing the worst-case error of aggregators as well as its
induced federated learning algorithm for any cases of $\hat{f}$ and $f$.
Specifically, we will show that underestimation ($\hat{f}<f$) can lead to
arbitrarily poor performance for both aggregators and federated learning. For
non-underestimation ($\hat{f}\ge f$), we have proved optimal lower and upper
bounds of the same order on the errors of both aggregators and federated
learning. All these optimal bounds are proportional to $\hat{f}/(n-f-\hat{f})$
with $n$ clients, which monotonically increases with larger $\hat{f}$. This
indicates a fundamental trade-off: while an aggregator with a larger robustness
degree $\hat{f}$ can solve federated learning problems of wider range $f\in
[0,\hat{f}]$, the performance can deteriorate when there are actually fewer or
even no Byzantine clients (i.e., $f\in [0,\hat{f})$).

</details>


### [792] [Fractional Heat Kernel for Semi-Supervised Graph Learning with Small Training Sample Size](https://arxiv.org/abs/2510.04440)
*Farid Bozorgnia,Vyacheslav Kungurtsev,Shirali Kadyrov,Mohsen Yousefnezhad*

Main category: cs.LG

TL;DR: 本研究提出了一种结合分数阶热核动力学和源项的标签传播与自训练新算法，利用分数阶拉普拉斯算子实现全局标签扩散，特别适用于标记样本稀疏的场景。


<details>
  <summary>Details</summary>
Motivation: 该方法借鉴了信息论与抛物线演化方程物理学的经典联系，并将分数阶热核整合到图神经网络（如图卷积网络和图注意力网络）中，通过自适应多跳扩散增强其表达能力。

Method: 研究中集成了分数阶热核，并利用Chebyshev多项式近似处理大型图，同时探讨了变分模型。

Result: 实验证明，该方法在标准数据集上是有效的，并且分数阶拉普拉斯算子能够实现更全局的标签扩散。

Conclusion: 将经典扩散模型扩展到分数阶拉普拉斯算子，通过非局部相互作用可以实现更全局的标签扩散，这种在已知标签监督和图扩散之间的平衡在只有少量标记训练样本的情况下特别有利。

Abstract: In this work, we introduce novel algorithms for label propagation and
self-training using fractional heat kernel dynamics with a source term. We
motivate the methodology through the classical correspondence of information
theory with the physics of parabolic evolution equations. We integrate the
fractional heat kernel into Graph Neural Network architectures such as Graph
Convolutional Networks and Graph Attention, enhancing their expressiveness
through adaptive, multi-hop diffusion. By applying Chebyshev polynomial
approximations, large graphs become computationally feasible. Motivating
variational formulations demonstrate that by extending the classical diffusion
model to fractional powers of the Laplacian, nonlocal interactions deliver more
globally diffusing labels. The particular balance between supervision of known
labels and diffusion across the graph is particularly advantageous in the case
where only a small number of labeled training examples are present. We
demonstrate the effectiveness of this approach on standard datasets.

</details>


### [793] [Domain Generalization: A Tale of Two ERMs](https://arxiv.org/abs/2510.04441)
*Yilun Zhu,Naihao Deng,Naichen Shi,Aditya Gangrade,Clayton Scott*

Main category: cs.LG

TL;DR: 领域泛化（DG）是指从几个有标签的训练数据分布（或领域）泛化到一个没有标签的新测试领域的问题。领域信息增强的经验风险最小化（ERM）在满足后验漂移假设的数据集上优于池化ERM。


<details>
  <summary>Details</summary>
Motivation: 领域泛化（DG）旨在解决从多个已知领域（有标签数据）泛化到未知领域（无标签数据）的问题。以往研究发现，ERM在池化训练数据上的表现难以超越，但该发现主要基于协变量偏移假设的数据集。本文旨在探讨在后验漂移假设下，如何改进DG方法的性能。

Method: 提出“领域信息增强ERM”方法，将领域信息作为特征向量的一部分，用于训练模型。该方法与简单的池化ERM方法进行对比。

Result: 在满足后验漂移假设的数据集上，“领域信息增强ERM”方法优于池化ERM。该结论在语言和视觉任务的实验中得到验证。

Conclusion: 在处理满足后验漂移假设的数据集时，将领域信息融入特征表示的“领域信息增强ERM”方法，比简单地将所有数据混合在一起进行训练的池化ERM方法更有效，能够实现更好的领域泛化能力。

Abstract: Domain generalization (DG) is the problem of generalizing from several
distributions (or domains), for which labeled training data are available, to a
new test domain for which no labeled data is available. A common finding in the
DG literature is that it is difficult to outperform empirical risk minimization
(ERM) on the pooled training data.
  In this work, we argue that this finding has primarily been reported for
datasets satisfying a \emph{covariate shift} assumption. When the dataset
satisfies a \emph{posterior drift} assumption instead, we show that
``domain-informed ERM,'' wherein feature vectors are augmented with
domain-specific information, outperforms pooling ERM. These claims are
supported by a theoretical framework and experiments on language and vision
tasks.

</details>


### [794] [Forking-Sequences](https://arxiv.org/abs/2510.04487)
*Willa Potosnak,Malcolm Wolff,Boris Oreshkin,Mengfei Cao,Michael W. Mahoney,Dmitry Efimov,Kin G. Olivares*

Main category: cs.LG

TL;DR: 本文提出并论证了forking-sequences方法在时间序列预测中的有效性，该方法通过联合编码和解码所有预测创建日期的时间序列，提高了预测的稳定性、减少了方差并提升了推理效率。


<details>
  <summary>Details</summary>
Motivation: 提高时间序列预测模型的预测稳定性，解决现有模型即使准确但预测结果在不同预测创建日期间易产生剧烈变动的问题，从而增强利益相关者的信任并减少对下游决策的干扰。

Method: 提出并形式化了forking-sequences方法，该方法与标准方法不同，它联合编码和解码所有预测创建日期的时间序列，类似于时间序列交叉验证。在训练时，该方法能实现更稳定一致的梯度更新；通过集成学习可以减少预测方差；并能提高推理的计算效率。

Result: 在MLP、RNN、LSTM、CNN和Transformer等模型架构上，使用16个来自M1、M3、M4和Tourism竞赛的数据集进行验证，forking-sequences方法在预测百分比变化稳定性方面，平均分别提高了28.8%、28.8%、37.9%、31.3%和8.8%。

Conclusion: forking-sequences方法是一种有效且值得更广泛采用的技术，能够显著提高时间序列预测的稳定性，并带来训练和推理效率上的改进。

Abstract: While accuracy is a critical requirement for time series forecasting models,
an equally important (yet often overlooked) desideratum is forecast stability
across forecast creation dates (FCDs). Even highly accurate models can produce
erratic revisions between FCDs, undermining stakeholder trust and disrupting
downstream decision-making. To improve forecast stability, models like MQCNN,
MQT, and SPADE employ a little-known but highly effective technique:
forking-sequences. Unlike standard statistical and neural forecasting methods
that treat each FCD independently, the forking-sequences method jointly encodes
and decodes the entire time series across all FCDs, in a way mirroring time
series cross-validation. Since forking sequences remains largely unknown in the
broader neural forecasting community, in this work, we formalize the
forking-sequences approach, and we make a case for its broader adoption. We
demonstrate three key benefits of forking-sequences: (i) more stable and
consistent gradient updates during training; (ii) reduced forecast variance
through ensembling; and (iii) improved inference computational efficiency. We
validate forking-sequences' benefits using 16 datasets from the M1, M3, M4, and
Tourism competitions, showing improvements in forecast percentage change
stability of 28.8%, 28.8%, 37.9%, and 31.3%, and 8.8%, on average, for MLP,
RNN, LSTM, CNN, and Transformer-based architectures, respectively.

</details>


### [795] [Expand Neurons, Not Parameters](https://arxiv.org/abs/2510.04500)
*Linghao Kong,Inimai Subramanian,Yonadav Shavit,Micah Adler,Dan Alistarh,Nir Shavit*

Main category: cs.LG

TL;DR: 增加网络神经元数量而不增加其非零参数数量可以提高性能，这可以通过固定参数扩展（FPE）实现，该方法通过将父节点权重不重叠地分配给子节点来减少特征之间的干扰，从而提高准确性，尤其是在存在大量重叠的情况下。


<details>
  <summary>Details</summary>
Motivation: 在不增加非零参数数量的情况下，通过增加网络宽度来提高网络性能，并解决由多个特征共享同一神经元引起的干扰问题。

Method: 提出固定参数扩展（FPE）方法，将单个神经元替换为多个子神经元，并将其权重不重叠地分配给子节点，以减少神经元之间的干扰。

Result: 在布尔代码问题等符号任务上，FPE系统地降低了多义性指标并提高了任务准确性。随机分割权重也能带来类似收益，表明减少冲突是关键。在CLIP嵌入分类器和更深的多层网络等实际模型中，在保持非零参数数量恒定的同时增加网络宽度，也持续提高了准确性。

Conclusion: 增加网络宽度（而非深度或参数数量）是一种在不增加计算负担的情况下提高模型性能和可解释性的有效方法，这与现代计算加速器的瓶颈相匹配。

Abstract: This work demonstrates how increasing the number of neurons in a network
without increasing its number of non-zero parameters improves performance. We
show that this gain corresponds with a decrease in interference between
multiple features that would otherwise share the same neurons. To reduce such
entanglement at a fixed non-zero parameter count, we introduce Fixed Parameter
Expansion (FPE): replace a neuron with multiple children and partition the
parent's weights disjointly across them, so that each child inherits a
non-overlapping subset of connections. On symbolic tasks, specifically Boolean
code problems, clause-aligned FPE systematically reduces polysemanticity
metrics and yields higher task accuracy. Notably, random splits of neuron
weights approximate these gains, indicating that reduced collisions, not
precise assignment, are a primary driver. Consistent with the superposition
hypothesis, the benefits of FPE grow with increasing interference: when
polysemantic load is high, accuracy improvements are the largest. Transferring
these insights to real models (classifiers over CLIP embeddings and deeper
multilayer networks) we find that widening networks while maintaining a
constant non-zero parameter count consistently increases accuracy. These
results identify an interpretability-grounded mechanism to leverage width
against superposition, improving performance without increasing the number of
non-zero parameters. Such a direction is well matched to modern accelerators,
where memory movement of non-zero parameters, rather than raw compute, is the
dominant bottleneck.

</details>


### [796] [Wavelet Predictive Representations for Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.04507)
*Min Wang,Xin Li,Ye He,Yao-Hui Li,Hasnaa Bennis,Riashat Islam,Mingzhong Wang*

Main category: cs.LG

TL;DR: 该研究提出了一种名为WISDOM的新方法，用于解决非平稳强化学习（NSRL）中的挑战，即在动态变化的环境中提高智能体的适应能力。WISDOM利用小波分析来捕捉任务表示的多尺度特征，并通过小波时序差分（TD）更新算子来增强对环境动态的跟踪和预测能力。实验结果表明，WISDOM在样本效率和渐近性能方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界具有固有的非平稳性，环境因素（如天气、交通流量）不断变化，这给智能体的适应带来了挑战。现有的NSRL方法主要关注规律性变化的模式，在高度动态的环境中适应性有限。

Method: WISDOM将任务表示序列转换到小波域，利用小波系数捕捉非平稳变化的全局趋势和细微变化。除了自回归建模，还设计了一个小波时序差分（TD）更新算子来增强对MDP演化的跟踪和预测能力，并从理论上证明了其收敛性。

Result: 在各种基准测试中，WISDOM在样本效率和渐近性能方面都显著优于现有基线方法。

Conclusion: WISDOM能够有效适应非平稳和随机演化任务的复杂环境，展现出卓越的适应能力。

Abstract: The real world is inherently non-stationary, with ever-changing factors, such
as weather conditions and traffic flows, making it challenging for agents to
adapt to varying environmental dynamics. Non-Stationary Reinforcement Learning
(NSRL) addresses this challenge by training agents to adapt rapidly to
sequences of distinct Markov Decision Processes (MDPs). However, existing NSRL
approaches often focus on tasks with regularly evolving patterns, leading to
limited adaptability in highly dynamic settings. Inspired by the success of
Wavelet analysis in time series modeling, specifically its ability to capture
signal trends at multiple scales, we propose WISDOM to leverage wavelet-domain
predictive task representations to enhance NSRL. WISDOM captures these
multi-scale features in evolving MDP sequences by transforming task
representation sequences into the wavelet domain, where wavelet coefficients
represent both global trends and fine-grained variations of non-stationary
changes. In addition to the auto-regressive modeling commonly employed in time
series forecasting, we devise a wavelet temporal difference (TD) update
operator to enhance tracking and prediction of MDP evolution. We theoretically
prove the convergence of this operator and demonstrate policy improvement with
wavelet task representations. Experiments on diverse benchmarks show that
WISDOM significantly outperforms existing baselines in both sample efficiency
and asymptotic performance, demonstrating its remarkable adaptability in
complex environments characterized by non-stationary and stochastically
evolving tasks.

</details>


### [797] [Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction](https://arxiv.org/abs/2510.04522)
*Yisen Gao,Xingcheng Fu,Qingyun Sun,Jianxin Li,Xianxian Li*

Main category: cs.LG

TL;DR: GeoMancer是一个新颖的黎曼图扩散框架，用于解决现有图扩散模型中多级特征在统一潜在空间中纠缠的问题，通过黎曼几何方法捕捉不同流形的几何特征，并提出了一种约束扩散方法来解决流形偏差问题，在多项任务中取得了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图扩散模型将不同层级的特征嵌入到统一的潜在空间，未能充分利用图数据的几何特性，导致信息纠缠，限制了预测任务的性能。

Method: GeoMancer框架采用等距不变黎曼回旋核方法替代指数映射，解决数值不稳定性问题，并将多层级特征解耦到各自的任务特定流形上。同时，提出了一种流形约束扩散方法和一种无条件生成的自导策略，以解决流形偏差问题。

Result: 实验结果表明，GeoMancer框架在多种图学习任务中，包括生成和预测任务，均取得了优于现有方法的性能。

Conclusion: GeoMancer通过引入黎曼几何概念和创新的扩散机制，有效解决了图扩散模型中的特征纠缠和流形偏差问题，显著提升了模型在各类图数据处理任务上的表现。

Abstract: Graph diffusion models have made significant progress in learning structured
graph data and have demonstrated strong potential for predictive tasks.
Existing approaches typically embed node, edge, and graph-level features into a
unified latent space, modeling prediction tasks including classification and
regression as a form of conditional generation. However, due to the
non-Euclidean nature of graph data, features of different curvatures are
entangled in the same latent space without releasing their geometric potential.
To address this issue, we aim to construt an ideal Riemannian diffusion model
to capture distinct manifold signatures of complex graph data and learn their
distribution. This goal faces two challenges: numerical instability caused by
exponential mapping during the encoding proces and manifold deviation during
diffusion generation. To address these challenges, we propose GeoMancer: a
novel Riemannian graph diffusion framework for both generation and prediction
tasks. To mitigate numerical instability, we replace exponential mapping with
an isometric-invariant Riemannian gyrokernel approach and decouple multi-level
features onto their respective task-specific manifolds to learn optimal
representations. To address manifold deviation, we introduce a
manifold-constrained diffusion method and a self-guided strategy for
unconditional generation, ensuring that the generated data remains aligned with
the manifold signature. Extensive experiments validate the effectiveness of our
approach, demonstrating superior performance across a variety of tasks.

</details>


### [798] [Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion](https://arxiv.org/abs/2510.04525)
*Satoshi Hayakawa,Yuhta Takida,Masaaki Imaizumi,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: MaskGIT 采样器在图像建模中具有隐含的温度采样机制，提出了更高效的“矩采样器”，并通过部分缓存和混合方法进一步提高了效率。


<details>
  <summary>Details</summary>
Motivation: 加速了掩码扩散模型的采样过程，因为该领域的研究相对较少。

Method: 理论分析了 MaskGIT 采样器，提出了一种新的“矩采样器”，并通过部分缓存技术和混合方法提高了效率。

Result: 提出的方法在图像和文本域的实验中验证了其理论和效率。

Conclusion: 提出的方法提高了理论理解和掩码扩散采样器的实际应用。

Abstract: Masked diffusion models have shown promising performance in generating
high-quality samples in a wide range of domains, but accelerating their
sampling process remains relatively underexplored. To investigate efficient
samplers for masked diffusion, this paper theoretically analyzes the MaskGIT
sampler for image modeling, revealing its implicit temperature sampling
mechanism. Through this analysis, we introduce the "moment sampler," an
asymptotically equivalent but more tractable and interpretable alternative to
MaskGIT, which employs a "choose-then-sample" approach by selecting unmasking
positions before sampling tokens. In addition, we improve the efficiency of
choose-then-sample algorithms through two key innovations: a partial caching
technique for transformers that approximates longer sampling trajectories
without proportional computational cost, and a hybrid approach formalizing the
exploration-exploitation trade-off in adaptive unmasking. Experiments in image
and text domains demonstrate our theory as well as the efficiency of our
proposed methods, advancing both theoretical understanding and practical
implementation of masked diffusion samplers.

</details>


### [799] [Graph-based Tabular Deep Learning Should Learn Feature Interactions, Not Just Make Predictions](https://arxiv.org/abs/2510.04543)
*Elias Dubbeldam,Reza Mohammadi,Marit Schoonhoven,S. Ilker Birbil*

Main category: cs.LG

TL;DR: 深度学习在表格数据上的应用仍落后于传统模型，主要因未能有效捕捉数据集特有的复杂特征交互。现有基于图的表格深度学习（GTDL）方法侧重预测精度，忽视了图结构的准确建模。本文认为GTDL应优先显式学习和评估特征交互，而非仅关注预测。通过在具有已知真实图结构的合成数据集上进行实验，我们发现现有GTDL方法无法准确恢复特征交互，而强制引入真实交互结构反而能提升预测性能。因此，GTDL方法需重视定量评估和结构学习，转向“感知结构”的建模，以构建更准确、可解释、可信赖且符合领域知识的GTDL系统。


<details>
  <summary>Details</summary>
Motivation: 深度学习在表格数据上的表现仍不如传统模型，核心挑战在于无法有效捕捉表格数据特有的复杂特征交互。现有的基于图的表格深度学习（GTDL）方法主要关注预测准确性，忽略了图结构的准确建模。

Method: 使用具有已知真实图结构的合成数据集，对比分析现有GTDL方法在特征交互学习上的表现，并探索强制引入真实交互结构对预测性能的影响。

Result: 实验表明，现有GTDL方法无法有效恢复表格数据中的特征交互。然而，通过在模型中强制引入真实的交互结构，可以显著提升预测性能。

Conclusion: GTDL方法需要从“以预测为中心”转向“以结构为导向”，优先进行准确的结构学习和评估。只有这样，GTDL才能在表格数据领域取得更大突破，构建出不仅准确而且可解释、可信赖的系统。

Abstract: Despite recent progress, deep learning methods for tabular data still
struggle to compete with traditional tree-based models. A key challenge lies in
modeling complex, dataset-specific feature interactions that are central to
tabular data. Graph-based tabular deep learning (GTDL) methods aim to address
this by representing features and their interactions as graphs. However,
existing methods predominantly optimize predictive accuracy, neglecting
accurate modeling of the graph structure. This position paper argues that GTDL
should move beyond prediction-centric objectives and prioritize the explicit
learning and evaluation of feature interactions. Using synthetic datasets with
known ground-truth graph structures, we show that existing GTDL methods fail to
recover meaningful feature interactions. Moreover, enforcing the true
interaction structure improves predictive performance. This highlights the need
for GTDL methods to prioritize quantitative evaluation and accurate structural
learning. We call for a shift toward structure-aware modeling as a foundation
for building GTDL systems that are not only accurate but also interpretable,
trustworthy, and grounded in domain understanding.

</details>


### [800] [Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF--QP Safety Layer in Arbitrage-Free Markets](https://arxiv.org/abs/2510.04555)
*Jian'an Zhang*

Main category: cs.LG

TL;DR: Tail-Safe是一个用于衍生品对冲的框架，结合了风险敏感的强化学习和基于二次规划的安全层，以满足金融约束。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是开发一个能够处理衍生品对冲中的尾部风险的框架，同时满足实际的金融约束。

Method: 该方法结合了基于IQN的分布评论家和CVaR目标（IQN-CVaR-PPO）的强化学习组件，以及一个尾部覆盖控制器，通过温度倾斜和尾部增强来调节分位数采样。此外，还有一个基于二次规划（QP）的安全组件，用于强制执行CBF不等式和域特定约束，如无交易带、框和速率限制以及符号一致性门。

Result: 在无套利、考虑微观结构的合成市场中，Tail-Safe在不损害中心性能的情况下改善了左尾风险，并且在QP可行且无松弛的情况下，实现了零硬约束违反。

Conclusion: Tail-Safe在对冲衍生品尾部风险方面显示出有效性，同时通过可审计的遥测数据支持治理和可解释性。然而，该研究依赖于合成数据和简化的执行。

Abstract: We introduce Tail-Safe, a deployability-oriented framework for derivatives
hedging that unifies distributional, risk-sensitive reinforcement learning with
a white-box control-barrier-function (CBF) quadratic-program (QP) safety layer
tailored to financial constraints. The learning component combines an IQN-based
distributional critic with a CVaR objective (IQN--CVaR--PPO) and a
Tail-Coverage Controller that regulates quantile sampling through temperature
tilting and tail boosting to stabilize small-$\alpha$ estimation. The safety
component enforces discrete-time CBF inequalities together with domain-specific
constraints -- ellipsoidal no-trade bands, box and rate limits, and a
sign-consistency gate -- solved as a convex QP whose telemetry (active sets,
tightness, rate utilization, gate scores, slack, and solver status) forms an
auditable trail for governance. We provide guarantees of robust forward
invariance of the safe set under bounded model mismatch, a minimal-deviation
projection interpretation of the QP, a KL-to-DRO upper bound linking per-state
KL regularization to worst-case CVaR, concentration and sample-complexity
results for the temperature-tilted CVaR estimator, and a CVaR trust-region
improvement inequality under KL limits, together with feasibility persistence
under expiry-aware tightening. Empirically, in arbitrage-free,
microstructure-aware synthetic markets (SSVI $\to$ Dupire $\to$ VIX with
ABIDES/MockLOB execution), Tail-Safe improves left-tail risk without degrading
central performance and yields zero hard-constraint violations whenever the QP
is feasible with zero slack. Telemetry is mapped to governance dashboards and
incident workflows to support explainability and auditability. Limitations
include reliance on synthetic data and simplified execution to isolate
methodological contributions.

</details>


### [801] [Challenger-Based Combinatorial Bandits for Subcarrier Selection in OFDM Systems](https://arxiv.org/abs/2510.04559)
*Mohsen Amiri,V Venktesh,Sindri Magnússon*

Main category: cs.LG

TL;DR: 本论文研究在多用户MIMO下行链路中识别top-m用户调度集的问题，将其转化为随机线性规划中的组合纯探索问题。由于动作空间呈指数增长，穷举搜索不可行。因此，我们采用线性效用模型，以实现高效探索和对有希望的用户子集的可靠选择。我们引入了一个间隙指数框架，该框架维护一个当前冠军臂（top-m集）估计的候选列表和一个具有最大威胁的挑战臂的轮换候选列表。这种设计侧重于产生信息量最大的基于间隙指数的比较的度量，与最先进的线性规划方法相比，运行时和计算量大大减少，并且识别精度很高。该方法还暴露了速度和精度之间的可调权衡。在实际OFDM下行链路上的仿真表明，由候选列表驱动的纯探索使得在线、测量高效的子载波选择对于支持AI的通信系统具有实用性。


<details>
  <summary>Details</summary>
Motivation: 在多用户MIMO下行链路中识别top-m用户调度集，这是一个组合纯探索问题，由于动作空间庞大，穷举搜索不可行。

Method: 采用线性效用模型，引入间隙指数框架，维护冠军臂和挑战臂的候选列表，以实现高效探索和选择。

Result: 与现有方法相比，运行时和计算量显著减少，识别精度高，并提供了速度和精度之间的可调权衡。

Conclusion: 短名单驱动的纯探索使得在线、测量高效的子载波选择对于支持AI的通信系统具有实用性。

Abstract: This paper investigates the identification of the top-m user-scheduling sets
in multi-user MIMO downlink, which is cast as a combinatorial pure-exploration
problem in stochastic linear bandits. Because the action space grows
exponentially, exhaustive search is infeasible. We therefore adopt a linear
utility model to enable efficient exploration and reliable selection of
promising user subsets. We introduce a gap-index framework that maintains a
shortlist of current estimates of champion arms (top-m sets) and a rotating
shortlist of challenger arms that pose the greatest threat to the champions.
This design focuses on measurements that yield the most informative
gap-index-based comparisons, resulting in significant reductions in runtime and
computation compared to state-of-the-art linear bandit methods, with high
identification accuracy. The method also exposes a tunable trade-off between
speed and accuracy. Simulations on a realistic OFDM downlink show that
shortlist-driven pure exploration makes online, measurement-efficient
subcarrier selection practical for AI-enabled communication systems.

</details>


### [802] [Stochastic Approximation Methods for Distortion Risk Measure Optimization](https://arxiv.org/abs/2510.04563)
*Jinyang Jiang,Bernd Heidergott,Jiaqiao Hu,Yijie Peng*

Main category: cs.LG

TL;DR: 本文提出基于两种对偶表示（扭曲度量DM形式和分位数函数QF形式）的扭曲风险度量（DRM）优化梯度下降算法。DM形式采用三时间尺度算法，QF形式采用两时间尺度算法，并提出混合形式。


<details>
  <summary>Details</summary>
Motivation: 为DRM优化提出梯度下降算法，以管理不确定性。

Method: DM形式采用三时间尺度算法，利用广义似然比和基于核的密度估计来跟踪分位数、计算梯度并更新决策变量。QF形式采用两时间尺度方法，避免了复杂的分位数梯度估计。混合形式结合了两者的优点。

Result: DM形式达到最优收敛率为$O(k^{-4/7})$，QF形式收敛率为$O(k^{-2/3})$。数值实验证明了算法的有效性，并在鲁棒投资组合选择任务中取得显著改进。

Conclusion: 所提出的算法在各种应用中都表现出有效性和可扩展性，并能在实际场景中取得实际应用。

Abstract: Distortion Risk Measures (DRMs) capture risk preferences in decision-making
and serve as general criteria for managing uncertainty. This paper proposes
gradient descent algorithms for DRM optimization based on two dual
representations: the Distortion-Measure (DM) form and Quantile-Function (QF)
form. The DM-form employs a three-timescale algorithm to track quantiles,
compute their gradients, and update decision variables, utilizing the
Generalized Likelihood Ratio and kernel-based density estimation. The QF-form
provides a simpler two-timescale approach that avoids the need for complex
quantile gradient estimation. A hybrid form integrates both approaches,
applying the DM-form for robust performance around distortion function jumps
and the QF-form for efficiency in smooth regions. Proofs of strong convergence
and convergence rates for the proposed algorithms are provided. In particular,
the DM-form achieves an optimal rate of $O(k^{-4/7})$, while the QF-form
attains a faster rate of $O(k^{-2/3})$. Numerical experiments confirm their
effectiveness and demonstrate substantial improvements over baselines in robust
portfolio selection tasks. The method's scalability is further illustrated
through integration into deep reinforcement learning. Specifically, a DRM-based
Proximal Policy Optimization algorithm is developed and applied to
multi-echelon dynamic inventory management, showcasing its practical
applicability.

</details>


### [803] [GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning](https://arxiv.org/abs/2510.04567)
*Weishuo Ma,Yanbo Wang,Xiyuan Wang,Lei Zou,Muhan Zhang*

Main category: cs.LG

TL;DR: GILT是一个新的图上下文学习框架，它使用基于LLM的、无调优的架构来处理异构图数据，并在少样本学习场景下提高了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图基础模型（GFMs）在处理极端异构的图数据时面临挑战，因为它们要么依赖于文本（LLMs），难以处理数值特征，要么需要昂贵的、每图调优阶段，效率低下。

Method: GILT引入了一种新的基于Token的图上下文学习（ICL）机制，将节点、边和图级别的分类任务统一在一个框架内，能够处理通用数值特征并动态理解上下文中的类别语义，从而实现无调优的适应。

Result: GILT在少样本学习场景下，与基于LLM或基于调优的基线模型相比，展现出更强的性能和显著更少的时间消耗。

Conclusion: GILT通过其LLM-free和tuning-free的架构，以及新颖的基于Token的ICL机制，成功解决了现有GFMs在处理异构图数据和提高适应效率方面的局限性。

Abstract: Graph Neural Networks (GNNs) are powerful tools for precessing relational
data but often struggle to generalize to unseen graphs, giving rise to the
development of Graph Foundational Models (GFMs). However, current GFMs are
challenged by the extreme heterogeneity of graph data, where each graph can
possess a unique feature space, label set, and topology. To address this, two
main paradigms have emerged. The first leverages Large Language Models (LLMs),
but is fundamentally text-dependent, thus struggles to handle the numerical
features in vast graphs. The second pre-trains a structure-based model, but the
adaptation to new tasks typically requires a costly, per-graph tuning stage,
creating a critical efficiency bottleneck. In this work, we move beyond these
limitations and introduce \textbf{G}raph \textbf{I}n-context \textbf{L}earning
\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free
architecture. GILT introduces a novel token-based framework for in-context
learning (ICL) on graphs, reframing classification tasks spanning node, edge
and graph levels in a unified framework. This mechanism is the key to handling
heterogeneity, as it is designed to operate on generic numerical features.
Further, its ability to understand class semantics dynamically from the context
enables tuning-free adaptation. Comprehensive experiments show that GILT
achieves stronger few-shot performance with significantly less time than
LLM-based or tuning-based baselines, validating the effectiveness of our
approach.

</details>


### [804] [Busemann Functions in the Wasserstein Space: Existence, Closed-Forms, and Applications to Slicing](https://arxiv.org/abs/2510.04579)
*Clément Bonet,Elsa Cazelles,Lucas Drumetz,Nicolas Courty*

Main category: cs.LG

TL;DR: Busemann函数在Wasserstein空间中被研究，并在特定情况下（一维分布和高斯测度）得到了闭式表达式，这使得在概率分布上进行显式投影成为可能，并催生了新的Sliced-Wasserstein距离。


<details>
  <summary>Details</summary>
Motivation: 研究Wasserstein空间中的Busemann函数，因为它可以自然地定义黎曼流形上的测地射线投影并推广超平面概念，同时考虑数据常被建模为概率分布的情况。

Method: 在Wasserstein空间中研究Busemann函数的存在性和计算，并推导出了一维分布和高斯测度两种重要情况下的闭式表达式。

Result: 推导出一维分布和高斯测度Busemann函数的闭式表达式，并实现了概率分布的显式投影方案，定义了高斯混合和带标签数据集上的新型Sliced-Wasserstein距离。

Conclusion: 提出的新型Sliced-Wasserstein距离在合成数据集和迁移学习问题上显示出效率。

Abstract: The Busemann function has recently found much interest in a variety of
geometric machine learning problems, as it naturally defines projections onto
geodesic rays of Riemannian manifolds and generalizes the notion of
hyperplanes. As several sources of data can be conveniently modeled as
probability distributions, it is natural to study this function in the
Wasserstein space, which carries a rich formal Riemannian structure induced by
Optimal Transport metrics. In this work, we investigate the existence and
computation of Busemann functions in Wasserstein space, which admits geodesic
rays. We establish closed-form expressions in two important cases:
one-dimensional distributions and Gaussian measures. These results enable
explicit projection schemes for probability distributions on $\mathbb{R}$,
which in turn allow us to define novel Sliced-Wasserstein distances over
Gaussian mixtures and labeled datasets. We demonstrate the efficiency of those
original schemes on synthetic datasets as well as transfer learning problems.

</details>


### [805] [Improved probabilistic regression using diffusion models](https://arxiv.org/abs/2510.04583)
*Carlo Kneissl,Christopher Bülte,Philipp Scholl,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 提出一种新颖的基于扩散的概率回归框架，用于学习非参数预测分布，并增强不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的生成模型在回归任务中的应用有限，并且缺乏不确定性相关的评估。

Method: 通过对扩散噪声的完整分布进行建模来实现非参数预测分布的学习，并研究了不同的噪声参数化及其权衡。

Result: 该框架在各种回归任务中展现出优于现有基线方法的性能，并能提供经过校准的不确定性估计。

Conclusion: 所提出的框架在概率预测方面具有通用性，能够适应不同任务并增强不确定性量化。

Abstract: Probabilistic regression models the entire predictive distribution of a
response variable, offering richer insights than classical point estimates and
directly allowing for uncertainty quantification. While diffusion-based
generative models have shown remarkable success in generating complex,
high-dimensional data, their usage in general regression tasks often lacks
uncertainty-related evaluation and remains limited to domain-specific
applications. We propose a novel diffusion-based framework for probabilistic
regression that learns predictive distributions in a nonparametric way. More
specifically, we propose to model the full distribution of the diffusion noise,
enabling adaptation to diverse tasks and enhanced uncertainty quantification.
We investigate different noise parameterizations, analyze their trade-offs, and
evaluate our framework across a broad range of regression tasks, covering low-
and high-dimensional settings. For several experiments, our approach shows
superior performance against existing baselines, while delivering calibrated
uncertainty estimates, demonstrating its versatility as a tool for
probabilistic prediction.

</details>


### [806] [Closed-Form Last Layer Optimization](https://arxiv.org/abs/2510.04606)
*Alexandre Galashov,Nathaël Da Costa,Liyuan Xu,Philipp Hennig,Arthur Gretton*

Main category: cs.LG

TL;DR: 该论文提出了一种新的优化方法，通过解析地优化线性最后一层权重，并交替进行骨干网络梯度下降和最后一层解析更新，来加速神经网络的训练。


<details>
  <summary>Details</summary>
Motivation: 在平方损失下，线性最后一层权重存在闭式解，可以利用这一性质来改进优化过程。

Method: 将最后一层视为骨干网络参数的函数进行优化，交替进行骨干网络梯度下降和最后一层解析更新。在随机梯度下降场景下，通过权衡当前批次和累积信息来适应该方法。在神经切线核（NTK）机制下，证明了该方法收敛到最优解的保证。

Result: 在平方损失的监督任务（包括回归和分类）上，与标准的随机梯度下降相比，证明了所提出方法的有效性，包括傅里叶神经网络和工具变量回归。

Conclusion: 该方法在平方损失下可以加速神经网络的优化，并在NTK机制下保证收敛到最优解。

Abstract: Neural networks are typically optimized with variants of stochastic gradient
descent. Under a squared loss, however, the optimal solution to the linear last
layer weights is known in closed-form. We propose to leverage this during
optimization, treating the last layer as a function of the backbone parameters,
and optimizing solely for these parameters. We show this is equivalent to
alternating between gradient descent steps on the backbone and closed-form
updates on the last layer. We adapt the method for the setting of stochastic
gradient descent, by trading off the loss on the current batch against the
accumulated information from previous batches. Further, we prove that, in the
Neural Tangent Kernel regime, convergence of this method to an optimal solution
is guaranteed. Finally, we demonstrate the effectiveness of our approach
compared with standard SGD on a squared loss in several supervised tasks --
both regression and classification -- including Fourier Neural Operators and
Instrumental Variable Regression.

</details>


### [807] [Compressed Concatenation of Small Embedding Models](https://arxiv.org/abs/2510.04626)
*Mohamed Ayoub Ben Ayad,Michael Dinzinger,Kanishka Ghosh Dastidar,Jelena Mitrovic,Michael Granitzer*

Main category: cs.LG

TL;DR: 通过拼接多个小型嵌入模型并使用统一的解码器进行降维，可以在保持大部分性能的同时大幅压缩模型大小。


<details>
  <summary>Details</summary>
Motivation: 大型嵌入模型在资源受限环境下部署困难，而小型模型性能不足。本研究旨在解决此问题。

Method: 拼接多个小型模型的原始嵌入向量，并训练一个轻量级的统一解码器，利用Matryoshka表示学习（MRL）损失将其映射到低维空间，以克服拼接带来的高维度问题。

Result: 在MTEB检索任务子集上，拼接四个小型嵌入模型的concat-encode-quantize流程在应用后恢复了89%的原始性能，压缩率为48倍。

Conclusion: 拼接多个小型嵌入模型并结合MRL解码器是一种有效的压缩和部署策略，能够在显著降低模型复杂度的同时保持高性能。

Abstract: Embedding models are central to dense retrieval, semantic search, and
recommendation systems, but their size often makes them impractical to deploy
in resource-constrained environments such as browsers or edge devices. While
smaller embedding models offer practical advantages, they typically
underperform compared to their larger counterparts. To bridge this gap, we
demonstrate that concatenating the raw embedding vectors of multiple small
models can outperform a single larger baseline on standard retrieval
benchmarks. To overcome the resulting high dimensionality of naive
concatenation, we introduce a lightweight unified decoder trained with a
Matryoshka Representation Learning (MRL) loss. This decoder maps the
high-dimensional joint representation to a low-dimensional space, preserving
most of the original performance without fine-tuning the base models. We also
show that while concatenating more base models yields diminishing gains, the
robustness of the decoder's representation under compression and quantization
improves. Our experiments show that, on a subset of MTEB retrieval tasks, our
concat-encode-quantize pipeline recovers 89\% of the original performance with
a 48x compression factor when the pipeline is applied to a concatenation of
four small embedding models.

</details>


### [808] [Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation](https://arxiv.org/abs/2510.04646)
*Johanna Sommer,John Rachwan,Nils Fleischmann,Stephan Günnemann,Bertrand Charpentier*

Main category: cs.LG

TL;DR: 通过预测中间隐藏状态来加速分子几何生成，在不影响样本质量的情况下将推理时间缩短一倍，并可与其他优化相结合，实现高达 7 倍的加速。


<details>
  <summary>Details</summary>
Motivation: Flow matching models在生成高保真分子几何方面表现出色，但推理成本高昂，需要大量的网络评估，这在实际应用中成为主要瓶颈。

Method: 提出一种无需训练的缓存策略，通过预测跨求解器步骤的中间隐藏状态来加速分子几何生成。该方法直接作用于SE(3)-等变骨干网络，兼容预训练模型，并且不影响现有的基于训练的加速方法和系统级优化。

Result: 在GEOM-Drugs数据集上的实验表明，缓存策略可以将推理时间缩短一倍，同时保持样本质量，与基础模型相比速度提升高达3倍，且样本质量损失极小。

Conclusion: 该缓存策略是一种有效的、无需训练的加速方法，可以显著降低Flow matching models的推理成本，并且可以与其他优化方法协同工作，实现更大的性能提升。

Abstract: Flow matching models generate high-fidelity molecular geometries but incur
significant computational costs during inference, requiring hundreds of network
evaluations. This inference overhead becomes the primary bottleneck when such
models are employed in practice to sample large numbers of molecular
candidates. This work discusses a training-free caching strategy that
accelerates molecular geometry generation by predicting intermediate hidden
states across solver steps. The proposed method operates directly on the
SE(3)-equivariant backbone, is compatible with pretrained models, and is
orthogonal to existing training-based accelerations and system-level
optimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching
achieves a twofold reduction in wall-clock inference time at matched sample
quality and a speedup of up to 3x compared to the base model with minimal
sample quality degradation. Because these gains compound with other
optimizations, applying caching alongside other general, lossless optimizations
yield as much as a 7x speedup.

</details>


### [809] [IMLP: An Energy-Efficient Continual Learning Method for Tabular Data Streams](https://arxiv.org/abs/2510.04660)
*Yuandou Wang,Filip Gunnarsson,Rihan Hai*

Main category: cs.LG

TL;DR: IMLP是一个紧凑的持续学习器，适用于表格数据流，具有高能效和竞争力。


<details>
  <summary>Details</summary>
Motivation: 表格数据流在边缘和移动设备上的应用日益增多，但这些设备对能量、内存和计算能力有严格限制。现有的持续学习解决方案（CL）通常依赖于会随着时间推移而增长的重放机制，这会增加资源成本，并且在能效方面存在不足。

Method: 提出了一种名为IMLP（Incremental Multi-Layer Perceptron）的上下文感知增量多层感知器。IMLP通过在滑动潜在特征缓冲区上使用窗口化缩放点积注意力，实现了恒定大小的内存，避免了存储原始数据。注意力机制捕获的上下文与当前特征连接，并通过共享的前馈层处理，从而实现轻量级的每段更新。为了评估实际部署能力，引入了NetScore-T，一个结合了平衡准确率和能耗的可调指标，用于跨模型和数据集的帕累托感知比较。

Result: IMLP在能效方面表现出色，相比TabNet提高了27.6倍，相比TabPFN提高了85.5倍，同时保持了具有竞争力的平均准确率。

Conclusion: IMLP为表格数据流提供了一种易于部署、能效高的替代方案，可以替代完全的重新训练。

Abstract: Tabular data streams are rapidly emerging as a dominant modality for
real-time decision-making in healthcare, finance, and the Internet of Things
(IoT). These applications commonly run on edge and mobile devices, where energy
budgets, memory, and compute are strictly limited. Continual learning (CL)
addresses such dynamics by training models sequentially on task streams while
preserving prior knowledge and consolidating new knowledge. While recent CL
work has advanced in mitigating catastrophic forgetting and improving knowledge
transfer, the practical requirements of energy and memory efficiency for
tabular data streams remain underexplored. In particular, existing CL solutions
mostly depend on replay mechanisms whose buffers grow over time and exacerbate
resource costs.
  We propose a context-aware incremental Multi-Layer Perceptron (IMLP), a
compact continual learner for tabular data streams. IMLP incorporates a
windowed scaled dot-product attention over a sliding latent feature buffer,
enabling constant-size memory and avoiding storing raw data. The attended
context is concatenated with current features and processed by shared
feed-forward layers, yielding lightweight per-segment updates. To assess
practical deployability, we introduce NetScore-T, a tunable metric coupling
balanced accuracy with energy for Pareto-aware comparison across models and
datasets. IMLP achieves up to $27.6\times$ higher energy efficiency than TabNet
and $85.5\times$ higher than TabPFN, while maintaining competitive average
accuracy. Overall, IMLP provides an easy-to-deploy, energy-efficient
alternative to full retraining for tabular data streams.

</details>


### [810] [Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting](https://arxiv.org/abs/2510.04667)
*Fanzhe Fu,Yang Yang*

Main category: cs.LG

TL;DR: Reversible Instance Normalization (RevIN) 是一种关键技术，可使简单的线性模型在时间序列预测中达到最先进的性能。然而，尽管用稳健的统计量替换其非稳健统计量（称为 R2-IN）似乎是一个直接的改进，但研究发现了一个复杂得多的现实。本研究通过识别四个潜在的理论矛盾，剖析了各种归一化策略的令人费解的性能。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决时间序列预测中 RevIN 技术的局限性，并探索用更稳健的统计量（R2-IN）替换其非稳健统计量的影响，同时分析了 A-IN 等自适应模型的性能。

Method: 本研究通过识别四个潜在的理论矛盾来剖析各种归一化策略的性能。实验评估了标准 RevIN、R2-IN 和 A-IN 在存在极端异常值的数据集上的表现。

Result: 实验表明，标准 RevIN 在存在极端异常值的数据集上会灾难性地失败，其 MSE 增加了 683%。简单的 R2-IN 能够防止这种失败，并且出人意料地成为整体表现最佳的模型。然而，设计的 A-IN 模型却完全且系统性地失败了。

Conclusion: 本研究揭示了时间序列分析中一个关键的、被忽视的陷阱：简单的或反直觉的启发式方法引入的不稳定性可能比其旨在解决的统计问题更具破坏性。因此，本研究提出了时间序列归一化的一种新的、警示性的范例：从盲目追求复杂性转向由诊断驱动的分析，该分析不仅揭示了简单基线的惊人威力，也揭示了幼稚适应的危险性。

Abstract: Reversible Instance Normalization (RevIN) is a key technique enabling simple
linear models to achieve state-of-the-art performance in time series
forecasting. While replacing its non-robust statistics with robust counterparts
(termed R$^2$-IN) seems like a straightforward improvement, our findings reveal
a far more complex reality. This paper deconstructs the perplexing performance
of various normalization strategies by identifying four underlying theoretical
contradictions. Our experiments provide two crucial findings: first, the
standard RevIN catastrophically fails on datasets with extreme outliers, where
its MSE surges by a staggering 683\%. Second, while the simple R$^2$-IN
prevents this failure and unexpectedly emerges as the best overall performer,
our adaptive model (A-IN), designed to test a diagnostics-driven heuristic,
unexpectedly suffers a complete and systemic failure. This surprising outcome
uncovers a critical, overlooked pitfall in time series analysis: the
instability introduced by a simple or counter-intuitive heuristic can be more
damaging than the statistical issues it aims to solve. The core contribution of
this work is thus a new, cautionary paradigm for time series normalization: a
shift from a blind search for complexity to a diagnostics-driven analysis that
reveals not only the surprising power of simple baselines but also the perilous
nature of naive adaptation.

</details>


### [811] [Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding](https://arxiv.org/abs/2510.04674)
*Lorenzo Pannacci,Simone Fiorellino,Mario Edoardo Pandolfo,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.LG

TL;DR: DeepJSCC在多厂商部署中存在异构潜在空间不匹配问题，导致“语义噪声”和性能下降。本文提出了一个额外的处理阶段——语义信道均衡器，来对齐异构潜在空间，解决了上述问题。


<details>
  <summary>Details</summary>
Motivation: 现有的DeepJSCC方案假设发送端和接收端共享相同的潜在空间，这在多厂商部署中是无法实现的，因为编码器和解码器无法共同训练，从而引入“语义噪声”，降低了重建质量和下游任务性能。

Method: 本文提出并评估了几种语义信道均衡方法，引入了一个额外的处理阶段来对齐异构的潜在空间。研究了三种均衡器：线性映射（有闭式解）、轻量级神经网络（更具表现力）和Parseval-帧均衡器（零样本模式，无需训练）。

Result: 通过在AWGN和衰落信道上进行大量图像重建实验，量化了复杂度、数据效率和保真度之间的权衡。

Conclusion: 本文提出了用于DeepJSCC的语义信道均衡方法，通过对齐异构潜在空间来解决多厂商部署中的挑战，并通过实验验证了不同方法的有效性，为在异构AI原生无线网络中部署DeepJSCC提供了指导。

Abstract: Deep joint source-channel coding (DeepJSCC) has emerged as a powerful
paradigm for end-to-end semantic communications, jointly learning to compress
and protect task-relevant features over noisy channels. However, existing
DeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver
(RX) - an assumption that fails in multi-vendor deployments where encoders and
decoders cannot be co-trained. This mismatch introduces "semantic noise",
degrading reconstruction quality and downstream task performance. In this
paper, we systematize and evaluate methods for semantic channel equalization
for DeepJSCC, introducing an additional processing stage that aligns
heterogeneous latent spaces under both physical and semantic impairments. We
investigate three classes of aligners: (i) linear maps, which admit closed-form
solutions; (ii) lightweight neural networks, offering greater expressiveness;
and (iii) a Parseval-frame equalizer, which operates in zero-shot mode without
the need for training. Through extensive experiments on image reconstruction
over AWGN and fading channels, we quantify trade-offs among complexity, data
efficiency, and fidelity, providing guidelines for deploying DeepJSCC in
heterogeneous AI-native wireless networks.

</details>


### [812] [Counterfactual Credit Guided Bayesian Optimization](https://arxiv.org/abs/2510.04676)
*Qiyu Wei,Haowei Wang,Richard Allmendinger,Mauricio A. Álvarez*

Main category: cs.LG

TL;DR: CCGBO是一种新的贝叶斯优化框架，通过反事实信用量化历史观测值的贡献，以加速寻找全局最优值。


<details>
  <summary>Details</summary>
Motivation: 现有的贝叶斯优化方法侧重于构建全局代理模型，但在实际应用中，快速找到全局最优值更为重要。现有的方法假设所有样本的贡献均等，但这种假设在优化问题中受到限制。

Method: 提出了一种名为CCGBO的新框架，该框架通过反事实信用明确量化单个历史观测值的贡献，并将该信用纳入采集函数，从而选择性地将资源分配给最有可能出现最优解的区域。

Result: CCGBO被证明具有次线性遗憾。在合成和真实基准的实证评估中，CCGBO持续降低简单遗憾并加速收敛到全局最优值。

Conclusion: CCGBO通过反事实信用有效地解决了贝叶斯优化中的资源分配问题，能够加速找到全局最优值。

Abstract: Bayesian optimization has emerged as a prominent methodology for optimizing
expensive black-box functions by leveraging Gaussian process surrogates, which
focus on capturing the global characteristics of the objective function.
However, in numerous practical scenarios, the primary objective is not to
construct an exhaustive global surrogate, but rather to quickly pinpoint the
global optimum. Due to the aleatoric nature of the sequential optimization
problem and its dependence on the quality of the surrogate model and the
initial design, it is restrictive to assume that all observed samples
contribute equally to the discovery of the optimum in this context. In this
paper, we introduce Counterfactual Credit Guided Bayesian Optimization (CCGBO),
a novel framework that explicitly quantifies the contribution of individual
historical observations through counterfactual credit. By incorporating
counterfactual credit into the acquisition function, our approach can
selectively allocate resources in areas where optimal solutions are most likely
to occur. We prove that CCGBO retains sublinear regret. Empirical evaluations
on various synthetic and real-world benchmarks demonstrate that CCGBO
consistently reduces simple regret and accelerates convergence to the global
optimum.

</details>


### [813] [Parameter-free Algorithms for the Stochastically Extended Adversarial Model](https://arxiv.org/abs/2510.04685)
*Shuche Wang,Adarsh Barik,Peng Zhao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 本研究提出了首个用于随机扩展对抗（SEA）模型的无参数算法，该模型结合了对抗和随机在线凸优化。


<details>
  <summary>Details</summary>
Motivation: 现有SEA模型方法需要知道问题参数（如域直径D和损失函数Lipschitz常数G），限制了其实用性。本研究旨在消除对这些参数的依赖。

Method: 利用乐观在线牛顿步（OONS）算法开发无参数方法。首先，在已知Lipschitz常数但未知域直径的情况下，建立了一个比较器自适应算法。然后，将该方法扩展到D和G都未知的更一般情况，得到一个比较器和Lipschitz自适应算法。

Result: 在未知域直径但已知Lipschitz常数的情况下，算法实现了$	ilde{O}(ig(ig.2 + ig.)(ig. + ig.))$的期望遗憾界。在D和G都未知的情况下，得到的算法遗憾界与$ig.$和$ig.$具有相同的依赖关系。

Conclusion: 研究提出的方法在SEA模型中即使在两个参数都未知的情况下也有效，解决了现有方法的局限性。

Abstract: We develop the first parameter-free algorithms for the Stochastically
Extended Adversarial (SEA) model, a framework that bridges adversarial and
stochastic online convex optimization. Existing approaches for the SEA model
require prior knowledge of problem-specific parameters, such as the diameter of
the domain $D$ and the Lipschitz constant of the loss functions $G$, which
limits their practical applicability. Addressing this, we develop
parameter-free methods by leveraging the Optimistic Online Newton Step (OONS)
algorithm to eliminate the need for these parameters. We first establish a
comparator-adaptive algorithm for the scenario with unknown domain diameter but
known Lipschitz constant, achieving an expected regret bound of
$\tilde{O}\big(\|u\|_2^2 + \|u\|_2(\sqrt{\sigma^2_{1:T}} +
\sqrt{\Sigma^2_{1:T}})\big)$, where $u$ is the comparator vector and
$\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$ represent the cumulative stochastic
variance and cumulative adversarial variation, respectively. We then extend
this to the more general setting where both $D$ and $G$ are unknown, attaining
the comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound
exhibits the same dependence on $\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$,
demonstrating the efficacy of our proposed methods even when both parameters
are unknown in the SEA model.

</details>


### [814] [How does the optimizer implicitly bias the model merging loss landscape?](https://arxiv.org/abs/2510.04686)
*Chenxiang Zhang,Alexander Theus,Damien Teney,Antonio Orvieto,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: 模型合并旨在整合不同模型的能力，同时保持推理成本不变。本文研究了优化过程如何影响损失景观几何以及这对模型合并成功的影响。


<details>
  <summary>Details</summary>
Motivation: 理解模型合并有效性的潜在因素，特别是优化过程和数据选择如何塑造损失景观几何。

Method: 提出“有效噪声尺度”作为衡量优化器和数据选择对模型合并影响的单一指标，并分析了学习率、权重衰减、批次大小和数据增强等因素如何独立地调节该尺度。

Result: 模型合并的有效性是关于有效噪声尺度的非单调函数，存在一个最优值。研究发现，增加学习率、权重衰减、减小批次大小和使用数据增强都会独立地改变有效噪声尺度，并呈现出相同的趋势。

Conclusion: 优化过程通过影响全局损失景观来影响模型合并的成功，而不仅仅是单个最小值的平坦度或泛化性。这为通过操纵训练动态来改进模型合并提供了新的视角。

Abstract: Model merging methods combine models with different capabilities into a
single one while maintaining the same inference cost. Two popular approaches
are linear interpolation, which linearly interpolates between model weights,
and task arithmetic, which combines task vectors obtained by the difference
between finetuned and base models. While useful in practice, what properties
make merging effective are poorly understood. This paper explores how the
optimization process affects the loss landscape geometry and its impact on
merging success. We show that a single quantity -- the effective noise scale --
unifies the impact of optimizer and data choices on model merging. Across
architectures and datasets, the effectiveness of merging success is a
non-monotonic function of effective noise, with a distinct optimum. Decomposing
this quantity, we find that larger learning rates, stronger weight decay,
smaller batch sizes, and data augmentation all independently modulate the
effective noise scale, exhibiting the same qualitative trend. Unlike prior work
that connects optimizer noise to the flatness or generalization of individual
minima, we show that it also affects the global loss landscape, predicting when
independently trained solutions can be merged. Our findings broaden the
understanding of how optimization shapes the loss landscape geometry and its
downstream consequences for model merging, suggesting the possibility of
further manipulating the training dynamics to improve merging effectiveness.

</details>


### [815] [ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts](https://arxiv.org/abs/2510.04710)
*Zexin Wang,Changhua Pei,Yang Liu,Hengyue Jiang,Quan Zhou,Haotian Si,Hang Cui,Jianhui Li,Gaogang Xie,Jingjing Li,Dan Pei*

Main category: cs.LG

TL;DR: ViTs是一个基于视觉语言模型（VLM）的框架，将时间序列转换为视觉表示，解决了现有时间序列异常检测模型的“一次训练，跨场景推理”的挑战，能够处理任意长度的序列，并通过生成图像-文本对和多阶段训练来克服数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测模型难以实现“一次训练，跨场景推理”，并且在推理时需要灵活处理不同长度的序列。大型语言模型（LLMs）虽然具有出色的零样本能力，但应用于时间序列数据时存在上下文长度限制。

Method: 提出ViTs框架，将时间序列数据转换为视觉表示，通过缩放图像来保持时间依赖性并统一输入尺寸，从而处理任意长度的序列。利用进化算法生成图像-文本对，并设计了一个包含时间序列知识注入、异常检测增强和异常推理细化的三阶段训练流程。

Result: 实验表明，ViTs显著提升了VLM在时间序列数据上的理解和异常检测能力。

Conclusion: ViTs框架成功解决了时间序列异常检测中的关键挑战，提高了模型的泛化能力和灵活性。

Abstract: Web service administrators must ensure the stability of multiple systems by
promptly detecting anomalies in Key Performance Indicators (KPIs). Achieving
the goal of "train once, infer across scenarios" remains a fundamental
challenge for time series anomaly detection models. Beyond improving zero-shot
generalization, such models must also flexibly handle sequences of varying
lengths during inference, ranging from one hour to one week, without
retraining. Conventional approaches rely on sliding-window encoding and
self-supervised learning, which restrict inference to fixed-length inputs.
Large Language Models (LLMs) have demonstrated remarkable zero-shot
capabilities across general domains. However, when applied to time series data,
they face inherent limitations due to context length. To address this issue, we
propose ViTs, a Vision-Language Model (VLM)-based framework that converts time
series curves into visual representations. By rescaling time series images,
temporal dependencies are preserved while maintaining a consistent input size,
thereby enabling efficient processing of arbitrarily long sequences without
context constraints. Training VLMs for this purpose introduces unique
challenges, primarily due to the scarcity of aligned time series image-text
data. To overcome this, we employ an evolutionary algorithm to automatically
generate thousands of high-quality image-text pairs and design a three-stage
training pipeline consisting of: (1) time series knowledge injection, (2)
anomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive
experiments demonstrate that ViTs substantially enhance the ability of VLMs to
understand and detect anomalies in time series data. All datasets and code will
be publicly released at: https://anonymous.4open.science/r/ViTs-C484/.

</details>


### [816] [Directional Sheaf Hypergraph Networks: Unifying Learning on Directed and Undirected Hypergraphs](https://arxiv.org/abs/2510.04727)
*Emanuele Mule,Stefano Fiorini,Antonio Purificato,Federico Siciliano,Stefano Coniglio,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 提出了方向性公理化联通层神经网络（DSHN），一种结合了公理化层理论和超图不对称关系的新框架，能够处理定向超图，并在多个真实世界数据集上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有的有向超图学习方法存在同质性偏差，限制了其在异质性场景下的应用。而现有的公理化联通层神经网络（SNNs）虽然能克服这一缺点，但仅适用于无向超图，无法处理有向超图。

Method: 通过将公理化层理论与超图中不对称关系的处理相结合，构建了方向性公理化联通层神经网络（DSHN）。DSHN还引入了定向公理化超图拉普拉斯算子，该算子是一个复值算子，统一并推广了现有图和超图学习中的多种拉普拉斯矩阵。

Result: 在7个真实世界数据集上，DSHN相较于13种基线方法，准确率提升了2%到20%。

Conclusion: 通过对超图方向性的原则性处理，并结合公理化层的表达能力，DSHN能够显著提高模型性能。

Abstract: Hypergraphs provide a natural way to represent higher-order interactions
among multiple entities. While undirected hypergraphs have been extensively
studied, the case of directed hypergraphs, which can model oriented group
interactions, remains largely under-explored despite its relevance for many
applications. Recent approaches in this direction often exhibit an implicit
bias toward homophily, which limits their effectiveness in heterophilic
settings. Rooted in the algebraic topology notion of Cellular Sheaves, Sheaf
Neural Networks (SNNs) were introduced as an effective solution to circumvent
such a drawback. While a generalization to hypergraphs is known, it is only
suitable for undirected hypergraphs, failing to tackle the directed case. In
this work, we introduce Directional Sheaf Hypergraph Networks (DSHN), a
framework integrating sheaf theory with a principled treatment of asymmetric
relations within a hypergraph. From it, we construct the Directed Sheaf
Hypergraph Laplacian, a complex-valued operator by which we unify and
generalize many existing Laplacian matrices proposed in the graph- and
hypergraph-learning literature. Across 7 real-world datasets and against 13
baselines, DSHN achieves relative accuracy gains from 2% up to 20%, showing how
a principled treatment of directionality in hypergraphs, combined with the
expressive power of sheaves, can substantially improve performance.

</details>


### [817] [EVaR-Optimal Arm Identification in Bandits](https://arxiv.org/abs/2510.04728)
*Mehrasa Ahmadipour,Aurélien Garivier*

Main category: cs.LG

TL;DR: 我们研究了在有界[0,1]回报分布的非参数设置下，基于熵值风险（EVaR）标准的固定信度最优臂识别（BAI）问题，并提出了一种新的Track-and-Stop算法。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决在高风险环境下（如金融领域）进行风险规避决策的关键需求，该需求超越了简单的期望值优化。

Method: 提出了一种基于Track-and-Stop的算法，并推导了相应的期望样本复杂度的下界，且证明了其渐近匹配。

Result: 算法实现和下界表征都需要解决一个复杂的凸优化问题和一个相关的、更简单的非凸问题。

Conclusion: 所提出的算法在EVaR标准下实现了固定信度最优臂识别，并且其样本复杂度渐近最优。

Abstract: We study the fixed-confidence best arm identification (BAI) problem within
the multi-armed bandit (MAB) framework under the Entropic Value-at-Risk (EVaR)
criterion. Our analysis considers a nonparametric setting, allowing for general
reward distributions bounded in [0,1]. This formulation addresses the critical
need for risk-averse decision-making in high-stakes environments, such as
finance, moving beyond simple expected value optimization. We propose a
$\delta$-correct, Track-and-Stop based algorithm and derive a corresponding
lower bound on the expected sample complexity, which we prove is asymptotically
matched. The implementation of our algorithm and the characterization of the
lower bound both require solving a complex convex optimization problem and a
related, simpler non-convex one.

</details>


### [818] [Provable Affine Identifiability of Nonlinear CCA under Latent Distributional Priors](https://arxiv.org/abs/2510.04758)
*Zhiwei Han,Stefan Matthes,Hao Shen*

Main category: cs.LG

TL;DR: 非线性CCA在特定条件下可以恢复潜在因素。


<details>
  <summary>Details</summary>
Motivation: 研究非线性CCA在何种条件下可以恢复真实的潜在因素。

Method: 通过重新参数化将分析从观测空间转移到源空间，并证明了白化对于保证可识别性至关重要。此外，还证明了岭回归正则化的经验CCA可以收敛到其总体对应物。

Result: 在总体设置下，证明了非线性CCA在广泛的潜在分布类别中具有仿射可识别性。在有限样本情况下，证明了岭回归正则化的经验CCA可以收敛到其总体对应物。

Conclusion: 非线性CCA在满足特定条件（包括白化）时，可以恢复潜在因素。理论分析得到了合成数据集和渲染图像数据集的实验验证。

Abstract: In this work, we establish conditions under which nonlinear CCA recovers the
ground-truth latent factors up to an orthogonal transform after whitening.
Building on the classical result that linear mappings maximize canonical
correlations under Gaussian priors, we prove affine identifiability for a broad
class of latent distributions in the population setting. Central to our proof
is a reparameterization result that transports the analysis from observation
space to source space, where identifiability becomes tractable. We further show
that whitening is essential for ensuring boundedness and well-conditioning,
thereby underpinning identifiability. Beyond the population setting, we prove
that ridge-regularized empirical CCA converges to its population counterpart,
transferring these guarantees to the finite-sample regime. Experiments on a
controlled synthetic dataset and a rendered image dataset validate our theory
and demonstrate the necessity of its assumptions through systematic ablations.

</details>


### [819] [ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs](https://arxiv.org/abs/2510.04767)
*Wonjun Kang,Kevin Galim,Seunghyuk Oh,Minjae Lee,Yuchen Zeng,Shuibai Zhang,Coleman Hooper,Yuezhou Hu,Hyung Il Koo,Nam Ik Cho,Kangwook Lee*

Main category: cs.LG

TL;DR: 本文提出了一种名为ParallelBench的新基准测试，旨在评估扩散语言模型（dLLM）在并行解码方面的局限性，并揭示了当前并行解码策略在平衡速度和质量方面的不足。


<details>
  <summary>Details</summary>
Motivation: 大多数自回归语言模型（LLM）的解码过程是逐个进行的，而扩散语言模型（dLLM）通过并行解码有望显著加速推理过程。然而，dLLM中的条件独立性假设导致并行解码忽略了词元之间的依赖关系，在依赖性强的场景下会严重影响生成质量。现有研究未能充分解决这些挑战，并且在标准基准测试上的评估不足以全面捕捉并行解码导致的质量下降。

Method: 本文首先对并行解码进行了信息论分析，然后通过对合成列表操作的案例研究，从数据分布和解码策略两个角度进行了量化分析，揭示了并行解码的基本局限性。在此基础上，提出了ParallelBench基准测试，该基准包含对dLLM并行解码具有挑战性的真实任务。

Result: 使用ParallelBench进行系统分析表明：1. dLLM在并行解码时，在实际应用场景中可能面临严重的质量下降问题。2. 当前的并行解码策略难以根据任务难度自适应调整并行度，导致在保证质量的前提下难以实现显著的加速。

Conclusion: 研究结果强调了开发能够克服当前速度-质量权衡的新颖解码方法的紧迫性。作者发布了ParallelBench基准测试，以期加速真正高效的dLLM的研发进程。

Abstract: While most autoregressive LLMs are constrained to one-by-one decoding,
diffusion LLMs (dLLMs) have attracted growing interest for their potential to
dramatically accelerate inference through parallel decoding. Despite this
promise, the conditional independence assumption in dLLMs causes parallel
decoding to ignore token dependencies, inevitably degrading generation quality
when these dependencies are strong. However, existing works largely overlook
these inherent challenges, and evaluations on standard benchmarks (e.g., math
and coding) are not sufficient to capture the quality degradation caused by
parallel decoding. To address this gap, we first provide an
information-theoretic analysis of parallel decoding. We then conduct case
studies on analytically tractable synthetic list operations from both data
distribution and decoding strategy perspectives, offering quantitative insights
that highlight the fundamental limitations of parallel decoding. Building on
these insights, we propose ParallelBench, the first benchmark specifically
designed for dLLMs, featuring realistic tasks that are trivial for humans and
autoregressive LLMs yet exceptionally challenging for dLLMs under parallel
decoding. Using ParallelBench, we systematically analyze both dLLMs and
autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can
suffer dramatic quality degradation in real-world scenarios, and (ii) current
parallel decoding strategies struggle to adapt their degree of parallelism
based on task difficulty, thus failing to achieve meaningful speedup without
compromising quality. Our findings underscore the pressing need for innovative
decoding methods that can overcome the current speed-quality trade-off. We
release our benchmark to help accelerate the development of truly efficient
dLLMs.

</details>


### [820] [When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates](https://arxiv.org/abs/2510.04769)
*Michele Caprio,Siu Lun Chau,Krikamol Muandet*

Main category: cs.LG

TL;DR: 许多机器学习算法依赖于不确定性表示的迭代更新，但这些更新过程的收敛性问题尚未得到充分研究。本文首次分析了在存在不确定性的情况下，迭代学习过程的收敛性问题，并提出了判断收敛性的条件。


<details>
  <summary>Details</summary>
Motivation: 许多机器学习算法依赖于不确定性表示的迭代更新，而当存在不确定性和模糊性时，以概率分布的闭合、凸集为特征的credal sets成为表示不精确概率信念的常用框架。在IPML中，许多学习问题可被视为在credal sets上应用更新规则的过程，这引发了关于该迭代过程是否收敛以及在何种条件下收敛的问题。

Method: 对涉及credal sets更新规则的迭代学习过程的收敛性进行了理论分析，并以Credal Bayesian Deep Learning为例进行了说明。

Result: 提出了判断迭代学习过程收敛性的条件，并验证了Credal Bayesian Deep Learning的稳定性。

Conclusion: 将不确定性纳入学习过程不仅丰富了不确定性的表示，还揭示了产生稳定性的结构条件，从而为理解不确定性下的迭代学习动力学提供了新的见解。

Abstract: Many machine learning algorithms rely on iterative updates of uncertainty
representations, ranging from variational inference and
expectation-maximization, to reinforcement learning, continual learning, and
multi-agent learning. In the presence of imprecision and ambiguity, credal sets
-- closed, convex sets of probability distributions -- have emerged as a
popular framework for representing imprecise probabilistic beliefs. Under such
imprecision, many learning problems in imprecise probabilistic machine learning
(IPML) may be viewed as processes involving successive applications of update
rules on credal sets. This naturally raises the question of whether this
iterative process converges to stable fixed points -- or, more generally, under
what conditions on the updating mechanism such fixed points exist, and whether
they can be attained. We provide the first analysis of this problem and
illustrate our findings using Credal Bayesian Deep Learning as a concrete
example. Our work demonstrates that incorporating imprecision into the learning
process not only enriches the representation of uncertainty, but also reveals
structural conditions under which stability emerges, thereby offering new
insights into the dynamics of iterative learning under imprecision.

</details>


### [821] [Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning](https://arxiv.org/abs/2510.04773)
*Kai Qin,Jiaqi Wu,Jianxiang He,Haoyuan Sun,Yifei Zhao,Bin Liang,Yongzhe Chang,Tiantian Zhang,Houde Liu*

Main category: cs.LG

TL;DR: LLM 知识遗忘是重要的研究领域，现有的基于优化的方法（如 NPO）存在不足。本文提出了一种新的分布级知识遗忘算法 DiPO，通过调整模型输出的 token 概率分布来实现，克服了 NPO 的限制，并在实验中取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 知识遗忘方法（如 NPO）在引入正向偏好信号时存在泛化能力受限的问题，本文旨在提出一种新的、更具泛化能力的知识遗忘方法。

Method: 本文提出了一种名为 DiPO（Distribution Preference Optimization）的新型知识遗忘算法。DiPO 旨在直接作用于模型输出的 token 概率分布，通过选择性地放大或抑制模型高置信度输出的 logits 来构建所需的偏好分布对，从而实现知识遗忘。理论上证明了 DiPO 损失函数与期望的遗忘方向的一致性。

Result: 实验结果表明，DiPO 在模型效用和遗忘质量之间取得了良好的权衡。在 TOFU 基准测试中，DiPO 达到了最高的遗忘质量；在 MUSE 基准测试中，DiPO 在效用保持方面保持了领先的可扩展性和可持续性。

Conclusion: DiPO 是一种新颖的、分布级的 LLM 知识遗忘算法，通过直接优化 token 概率分布来克服现有方法的局限性，并在多个基准测试中展现出优越的性能。

Abstract: As Large Language Models (LLMs) demonstrate remarkable capabilities learned
from vast corpora, concerns regarding data privacy and safety are receiving
increasing attention. LLM unlearning, which aims to remove the influence of
specific data while preserving overall model utility, is becoming an important
research area. One of the mainstream unlearning classes is optimization-based
methods, which achieve forgetting directly through fine-tuning, exemplified by
Negative Preference Optimization (NPO). However, NPO's effectiveness is limited
by its inherent lack of explicit positive preference signals. Attempts to
introduce such signals by constructing preferred responses often necessitate
domain-specific knowledge or well-designed prompts, fundamentally restricting
their generalizability. In this paper, we shift the focus to the
distribution-level, directly targeting the next-token probability distribution
instead of entire responses, and derive a novel unlearning algorithm termed
\textbf{Di}stribution \textbf{P}reference \textbf{O}ptimization (DiPO). We show
that the requisite preference distribution pairs for DiPO, which are
distributions over the model's output tokens, can be constructed by selectively
amplifying or suppressing the model's high-confidence output logits, thereby
effectively overcoming NPO's limitations. We theoretically prove the
consistency of DiPO's loss function with the desired unlearning direction.
Extensive experiments demonstrate that DiPO achieves a strong trade-off between
model utility and forget quality. Notably, DiPO attains the highest forget
quality on the TOFU benchmark, and maintains leading scalability and
sustainability in utility preservation on the MUSE benchmark.

</details>


### [822] [MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis](https://arxiv.org/abs/2510.04776)
*Ebenezer Awotoro,Chisom Ezekannagha,Florian Schwarz,Johannes Tauscher,Dominik Heider,Katharina Ladewig,Christel Le Bon,Karine Moncoq,Bruno Miroux,Georges Hattab*

Main category: cs.LG

TL;DR: MetaMP是一个集成膜蛋白数据库的框架，利用机器学习进行分类，提高了数据质量和用户体验，并在结构分类和异常检测方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的膜蛋白数据库存在数据缺失、不一致和整合困难等问题，需要一个改进的数据库整合方案。

Method: MetaMP通过一个Web应用程序整合了多个膜蛋白数据库，并利用机器学习进行数据分类。它还通过丰富元数据、提供用户友好的界面和八种交互式视图来改善数据质量和探索流程。

Result: MetaMP在不同难度的任务中都表现出有效性，提高了数据质量，准确预测了新识别的膜蛋白类别（98%），解决了77%的数据差异，并且在用户评估中，其速度和准确性均未受影响，甚至优于专家评审。

Conclusion: MetaMP是一个整合了现有膜蛋白知识并支持人工智能驱动的膜蛋白结构探索的宝贵资源，能够有效解决数据差异、预测蛋白类别，并为研究提供支持。

Abstract: Structural biology has made significant progress in determining membrane
proteins, leading to a remarkable increase in the number of available
structures in dedicated databases. The inherent complexity of membrane protein
structures, coupled with challenges such as missing data, inconsistencies, and
computational barriers from disparate sources, underscores the need for
improved database integration. To address this gap, we present MetaMP, a
framework that unifies membrane-protein databases within a web application and
uses machine learning for classification. MetaMP improves data quality by
enriching metadata, offering a user-friendly interface, and providing eight
interactive views for streamlined exploration. MetaMP was effective across
tasks of varying difficulty, demonstrating advantages across different levels
without compromising speed or accuracy, according to user evaluations.
Moreover, MetaMP supports essential functions such as structure classification
and outlier detection.
  We present three practical applications of Artificial Intelligence (AI) in
membrane protein research: predicting transmembrane segments, reconciling
legacy databases, and classifying structures with explainable AI support. In a
validation focused on statistics, MetaMP resolved 77% of data discrepancies and
accurately predicted the class of newly identified membrane proteins 98% of the
time and overtook expert curation. Altogether, MetaMP is a much-needed resource
that harmonizes current knowledge and empowers AI-driven exploration of
membrane-protein architecture.

</details>


### [823] [Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning](https://arxiv.org/abs/2510.04786)
*Jonas Hübotter,Leander Diaz-Bone,Ido Hakimi,Andreas Krause,Moritz Hardt*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Humans are good at learning on the job: We learn how to solve the tasks we
face as we go along. Can a model do the same? We propose an agent that
assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and
applies reinforcement learning to continue training the model for its target
task. The test-time curriculum avoids time-consuming human curation of datasets
by automatically selecting the most task-relevant data from a large pool of
available training data. Our experiments demonstrate that reinforcement
learning on a test-time curriculum consistently improves the model on its
target tasks, across a variety of evaluations and models. Notably, on
challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B
by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that
TTC-RL significantly raises the performance ceiling compared to the initial
model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to
43%. Our findings show the potential of test-time curricula in extending the
test-time scaling paradigm to continual training on thousands of task-relevant
experiences during test-time.

</details>


### [824] [On Predicting Post-Click Conversion Rate via Counterfactual Inference](https://arxiv.org/abs/2510.04816)
*Junhyung Ahn,Sanghack Lee*

Main category: cs.LG

TL;DR: 该研究提出了一种名为ESCIM（Entire Space Counterfactual Inference Multi-task Model）的新方法，利用因果推断来预测推荐系统中的转化率（CVR），特别是通过生成反事实标签来处理未点击样本的稀疏性问题，并在公共数据集和在线A/B测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在线广告和电子商务等推荐系统中，准确预测转化率（CVR）至关重要。然而，现有CVR预测模型主要基于点击样本训练，但点击样本稀疏，需要大量日志数据。虽然一些框架试图利用未点击样本，但它们依赖启发式方法且可能产生偏差。本研究旨在解决这些问题，通过因果推断来生成未点击样本的反事实转化标签，回答“如果用户点击了推荐商品，他是否会转化？”。

Method: 首先，训练一个用户行为序列的结构因果模型（SCM）。然后，对未点击的商品进行假设性干预（即点击），以推断反事实的CVR。接着，提出几种方法将预测的反事实CVR转化为未点击样本的二元反事实转化标签。最后，将生成的样本纳入训练过程。

Result: 在公共数据集上的广泛实验表明，该算法优于现有方法。在线A/B测试进一步验证了该算法在实际场景中的有效性。此外，在潜在转化数据上的改进性能展示了该方法的鲁棒性和优越的泛化能力。

Conclusion: ESCIM通过因果推断为未点击样本生成反事实转化标签，有效解决了点击样本稀疏的问题，并显著提高了CVR预测的准确性，在真实世界应用中表现出色。

Abstract: Accurately predicting conversion rate (CVR) is essential in various
recommendation domains such as online advertising systems and e-commerce. These
systems utilize user interaction logs, which consist of exposures, clicks, and
conversions. CVR prediction models are typically trained solely based on
clicked samples, as conversions can only be determined following clicks.
However, the sparsity of clicked instances necessitates the collection of a
substantial amount of logs for effective model training. Recent works address
this issue by devising frameworks that leverage non-clicked samples. While
these frameworks aim to reduce biases caused by the discrepancy between clicked
and non-clicked samples, they often rely on heuristics. Against this
background, we propose a method to counterfactually generate conversion labels
for non-clicked samples by using causality as a guiding principle, attempting
to answer the question, "Would the user have converted if he or she had clicked
the recommended item?" Our approach is named the Entire Space Counterfactual
Inference Multi-task Model (ESCIM). We initially train a structural causal
model (SCM) of user sequential behaviors and conduct a hypothetical
intervention (i.e., click) on non-clicked items to infer counterfactual CVRs.
We then introduce several approaches to transform predicted counterfactual CVRs
into binary counterfactual conversion labels for the non-clicked samples.
Finally, the generated samples are incorporated into the training process.
Extensive experiments on public datasets illustrate the superiority of the
proposed algorithm. Online A/B testing further empirically validates the
effectiveness of our proposed algorithm in real-world scenarios. In addition,
we demonstrate the improved performance of the proposed method on latent
conversion data, showcasing its robustness and superior generalization
capabilities.

</details>


### [825] [On the Hardness of Learning Regular Expressions](https://arxiv.org/abs/2510.04834)
*Idan Attias,Lev Reyzin,Nathan Srebro,Gal Vardi*

Main category: cs.LG

TL;DR: Regular expression learning is computationally hard, even under specific distributions and with different query types.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the computational complexity of learning regular expressions, which has been largely unaddressed despite their theoretical importance and practical applications.

Method: The study investigates the hardness of PAC learning and learning with membership queries, considering both uniform and distribution-free settings. It also examines the impact of adding complement or intersection operations to regular expressions.

Result: The paper proves hardness for PAC learning of regular expressions under the uniform distribution on the hypercube. It also demonstrates the hardness of distribution-free learning with membership queries. For extended regular expressions (with complement or intersection), hardness of learning with membership queries even under the uniform distribution is established.

Conclusion: Learning regular expressions is computationally hard, and these hardness results are distinct from those for DFAs or NFAs due to differences in descriptive complexity.

Abstract: Despite the theoretical significance and wide practical use of regular
expressions, the computational complexity of learning them has been largely
unexplored. We study the computational hardness of improperly learning regular
expressions in the PAC model and with membership queries. We show that PAC
learning is hard even under the uniform distribution on the hypercube, and also
prove hardness of distribution-free learning with membership queries.
Furthermore, if regular expressions are extended with complement or
intersection, we establish hardness of learning with membership queries even
under the uniform distribution. We emphasize that these results do not follow
from existing hardness results for learning DFAs or NFAs, since the descriptive
complexity of regular languages can differ exponentially between DFAs, NFAs,
and regular expressions.

</details>


### [826] [Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study](https://arxiv.org/abs/2510.04837)
*Guillaume Godin*

Main category: cs.LG

TL;DR: BCFP是ECFP的补充，BCFP结合ECFP可以提高BBBP预测的AUROC和AUPRC，其中r=1效果最好。


<details>
  <summary>Details</summary>
Motivation: 提出一种与ECFP互补的、以键为中心的指纹BCFP。

Method: 提出静态BCFP，并使用随机森林模型在BBBP数据集上进行评估。

Result: BCFP结合ECFP可以提高AUROC和AUPRC，r=1效果最好。BCFP-Sort&Slice方案可以保留OOV信息并实现紧凑的连接。

Conclusion: 轻量级、以键为中心的描述符可以作为ECFP的补充，并为BBBP预测提供强大的基线。

Abstract: Bond Centered FingerPrint (BCFP) are a complementary, bond-centric
alternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static
BCFP that mirrors the bond-convolution used by directed message-passing GNNs
like ChemProp, and evaluate it with a fast rapid Random Forest model on
Brain-Blood Barrier Penetration (BBBP) classification task. Across stratified
cross-validation, concatenating ECFP with BCFP consistently improves AUROC and
AUPRC over either descriptor alone, as confirmed by Turkey HSD
multiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not
yield statistically separable gains under the same test. We further propose
BCFP-Sort&Slice, a simple feature-combination scheme that preserves the
out-of-vocabulary (OOV) count information native to ECFP count vectors while
enabling compact unhashed concatenation of BCFP variants. We also outperform
the MGTP prediction on our BBBP evaluation, using such composite new features
bond and atom features. These results show that lightweight, bond-centered
descriptors can complement atom-centered circular fingerprints and provide
strong, fast baselines for BBBP prediction.

</details>


### [827] [Distributionally Robust Causal Abstractions](https://arxiv.org/abs/2510.04842)
*Yorgos Felekis,Theodoros Damoulas,Paris Giampouras*

Main category: cs.LG

TL;DR: 本研究提出了首个分布鲁棒因果抽象（CA）理论及其学习算法，解决了现有CA方法在处理环境变化和外源分布错误指定时的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 现有因果抽象（CA）学习方法假设外源分布固定且指定良好，这使得它们在环境变化和分布错误指定时容易出错。本研究旨在解决这些局限性。

Method: 提出了一种分布鲁棒CA及其学习算法，该算法将鲁棒因果抽象学习视为一个约束下的最小-最大优化问题，并使用了Wasserstein模糊集。在经验环境和高斯环境下，通过调整模糊集的半径来选择鲁棒性的级别。

Result: 理论上证明了该方法的有效性，并通过不同问题和CA学习方法的实证研究，证明了该框架不仅能应对环境变化，还能应对结构模型和干预映射的错误指定。

Conclusion: 本研究提出的分布鲁棒CA框架及其学习算法，能够有效提高CA在环境变化和模型指定错误情况下的鲁棒性。

Abstract: Causal Abstraction (CA) theory provides a principled framework for relating
causal models that describe the same system at different levels of granularity
while ensuring interventional consistency between them. Recently, several
approaches for learning CAs have been proposed, but all assume fixed and
well-specified exogenous distributions, making them vulnerable to environmental
shifts and misspecification. In this work, we address these limitations by
introducing the first class of distributionally robust CAs and their associated
learning algorithms. The latter cast robust causal abstraction learning as a
constrained min-max optimization problem with Wasserstein ambiguity sets. We
provide theoretical results, for both empirical and Gaussian environments,
leading to principled selection of the level of robustness via the radius of
these sets. Furthermore, we present empirical evidence across different
problems and CA learning methods, demonstrating our framework's robustness not
only to environmental shifts but also to structural model and intervention
mapping misspecification.

</details>


### [828] [Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders](https://arxiv.org/abs/2510.04855)
*Junqi Jiang,Francesco Leofante,Antonio Rago,Francesca Toni*

Main category: cs.LG

TL;DR: 现有的反事实解释方法难以同时满足鲁棒性、合理性和多样性等要求。本文提出了 LAPACE 框架，通过 L-GMVAE 模型学习结构化潜在空间，并生成反事实解释路径，解决了这些挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的反事实解释方法难以同时满足鲁棒性、合理性和多样性等要求，且无法以统一、模型无关的方式解决这些问题。

Method: 提出了一种新颖的生成框架，包括 L-GMVAE 模型来学习结构化潜在空间，以及 LAPACE 算法通过在潜在空间中插值来生成反事实解释路径，并允许通过梯度优化轻松整合用户指定的可操作性约束。

Result: LAPACE 在八个定量指标上实现了具有竞争力的性能，并且在计算上是高效的。

Conclusion: LAPACE 是一种新颖的、模型无关的算法，通过生成反事实解释路径来解决现有方法的局限性，满足鲁棒性、合理性和多样性等要求，并能有效整合可操作性约束。

Abstract: Counterfactual explanations (CEs) provide recourse recommendations for
individuals affected by algorithmic decisions. A key challenge is generating
CEs that are robust against various perturbation types (e.g. input and model
perturbations) while simultaneously satisfying other desirable properties.
These include plausibility, ensuring CEs reside on the data manifold, and
diversity, providing multiple distinct recourse options for single inputs.
Existing methods, however, mostly struggle to address these multifaceted
requirements in a unified, model-agnostic manner. We address these limitations
by proposing a novel generative framework. First, we introduce the
Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model
trained to learn a structured latent space where each class label is
represented by a set of Gaussian components with diverse, prototypical
centroids. Building on this, we present LAPACE (LAtent PAth Counterfactual
Explanations), a model-agnostic algorithm that synthesises entire paths of CE
points by interpolating from inputs' latent representations to those learned
latent centroids. This approach inherently ensures robustness to input changes,
as all paths for a given target class converge to the same fixed centroids.
Furthermore, the generated paths provide a spectrum of recourse options,
allowing users to navigate the trade-off between proximity and plausibility
while also encouraging robustness against model changes. In addition,
user-specified actionability constraints can also be easily incorporated via
lightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive
experiments show that LAPACE is computationally efficient and achieves
competitive performance across eight quantitative metrics.

</details>


### [829] [Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails](https://arxiv.org/abs/2510.04860)
*Siwei Han,Jiaqi Liu,Yaofeng Su,Wenbo Duan,Xinyuan Liu,Cihang Xie,Mohit Bansal,Mingyu Ding,Linjun Zhang,Huaxiu Yao*

Main category: cs.LG

TL;DR: 在部署后，LLM代理的自我演化能力可能导致其偏离训练时的对齐约束，形成“对齐引爆过程”（ATP）。


<details>
  <summary>Details</summary>
Motivation: 评估LLM代理在自我演化过程中可能出现的长期可靠性问题，特别是“对齐引爆过程”（ATP）这一部署后风险。

Method: 通过“自私探索”和“模仿策略扩散”两个范式形式化并分析ATP。构建可控的测试平台，并在Qwen3-8B和Llama-3.1-8B-Instruct模型上进行基准测试。

Result: 实验表明，在自我演化下，模型的对齐性会快速衰减，从对齐状态趋向非对齐状态。在多智能体环境中，违反对齐的行为会迅速扩散，导致集体失准。基于强化学习的对齐方法对ATP的防御效果脆弱。

Conclusion: LLM代理的对齐性是动态且脆弱的，在部署后的反馈驱动下容易发生衰减。

Abstract: As Large Language Model (LLM) agents increasingly gain self-evolutionary
capabilities to adapt and refine their strategies through real-world
interaction, their long-term reliability becomes a critical concern. We
identify the Alignment Tipping Process (ATP), a critical post-deployment risk
unique to self-evolving LLM agents. Unlike training-time failures, ATP arises
when continual interaction drives agents to abandon alignment constraints
established during training in favor of reinforced, self-interested strategies.
We formalize and analyze ATP through two complementary paradigms:
Self-Interested Exploration, where repeated high-reward deviations induce
individual behavioral drift, and Imitative Strategy Diffusion, where deviant
behaviors spread across multi-agent systems. Building on these paradigms, we
construct controllable testbeds and benchmark Qwen3-8B and
Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode
rapidly under self-evolution, with initially aligned models converging toward
unaligned states. In multi-agent settings, successful violations diffuse
quickly, leading to collective misalignment. Moreover, current reinforcement
learning-based alignment methods provide only fragile defenses against
alignment tipping. Together, these findings demonstrate that alignment of LLM
agents is not a static property but a fragile and dynamic one, vulnerable to
feedback-driven decay during deployment. Our data and code are available at
https://github.com/aiming-lab/ATP.

</details>


### [830] [A Clinical-grade Universal Foundation Model for Intraoperative Pathology](https://arxiv.org/abs/2510.04861)
*Zihan Zhao,Fengtao Zhou,Ronggang Li,Bing Chu,Xinke Zhang,Xueyi Zheng,Ke Zheng,Xiaobo Wen,Jiabo Ma,Yihui Wang,Jiewei Chen,Chengyou Zheng,Jiangyu Zhang,Yongqin Wen,Jiajia Meng,Ziqi Zeng,Xiaoqing Li,Jing Li,Dan Xie,Yaping Ye,Yu Wang,Hao Chen,Muyan Cai*

Main category: cs.LG

TL;DR: CRISP是一个临床级AI模型，通过分析大量冰冻切片数据，为术中病理诊断提供支持，提高了诊断准确性，减轻了医生负担，并加速了AI在临床中的应用。


<details>
  <summary>Details</summary>
Motivation: 术中病理诊断的复杂性和高质量冰冻切片数据的有限性限制了其在精准手术中的临床应用，计算病理学虽有进展但缺乏大规模前瞻性验证。

Method: 开发了一个名为CRISP的临床级基础模型，使用了来自八个医疗中心的超过10万张冰冻切片数据。该模型在超过1.5万张术中切片上进行了评估，涵盖了良恶性鉴别、术中决策支持和泛癌检测等近100项诊断任务。此外，还在一个包含超过2000名患者的前瞻性队列中进行了真实世界条件下的评估。

Result: CRISP模型在不同机构、肿瘤类型和解剖部位（包括未见过和罕见癌症）上表现出稳健的泛化能力。在前瞻性评估中，CRISP在92.6%的病例中直接为手术决策提供了信息。人机协作使诊断工作量减少了35%，避免了105项辅助检测，并将微转移灶的检测准确率提高到87.5%。

Conclusion: CRISP是一个临床级别的、由AI驱动的术中病理学范式，它将计算病理学的进展与手术精准度相结合，加速了人工智能在日常临床实践中的转化。

Abstract: Intraoperative pathology is pivotal to precision surgery, yet its clinical
impact is constrained by diagnostic complexity and the limited availability of
high-quality frozen-section data. While computational pathology has made
significant strides, the lack of large-scale, prospective validation has
impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a
clinical-grade foundation model developed on over 100,000 frozen sections from
eight medical centers, specifically designed to provide Clinical-grade Robust
Intraoperative Support for Pathology (CRISP). CRISP was comprehensively
evaluated on more than 15,000 intraoperative slides across nearly 100
retrospective diagnostic tasks, including benign-malignant discrimination, key
intraoperative decision-making, and pan-cancer detection, etc. The model
demonstrated robust generalization across diverse institutions, tumor types,
and anatomical sites-including previously unseen sites and rare cancers. In a
prospective cohort of over 2,000 patients, CRISP sustained high diagnostic
accuracy under real-world conditions, directly informing surgical decisions in
92.6% of cases. Human-AI collaboration further reduced diagnostic workload by
35%, avoided 105 ancillary tests and enhanced detection of micrometastases with
87.5% accuracy. Together, these findings position CRISP as a clinical-grade
paradigm for AI-driven intraoperative pathology, bridging computational
advances with surgical precision and accelerating the translation of artificial
intelligence into routine clinical practice.

</details>


### [831] [Less is More: Recursive Reasoning with Tiny Networks](https://arxiv.org/abs/2510.04871)
*Alexia Jolicoeur-Martineau*

Main category: cs.LG

TL;DR: TRM是一种比HRM更简单的递归推理方法，在ARC-AGI任务上实现了更高的泛化能力，参数量远小于LLMs。


<details>
  <summary>Details</summary>
Motivation: HRM虽然有潜力，但尚未被充分理解且可能不是最优的，需要更优的简单递归推理方法。

Method: TRM是一种使用单个两层网络，参数量仅为7M的递归推理方法。

Result: TRM在ARC-AGI-1上获得了45%的测试准确率，在ARC-AGI-2上获得了8%的测试准确率，优于大多数LLMs。

Conclusion: TRM通过更简单的方法实现了比HRM更高的泛化能力，并以极少的参数量在具有挑战性的任务上取得了优于LLMs的表现。

Abstract: Hierarchical Reasoning Model (HRM) is a novel approach using two small neural
networks recursing at different frequencies. This biologically inspired method
beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,
and ARC-AGI while trained with small models (27M parameters) on small data
(around 1000 examples). HRM holds great promise for solving hard problems with
small networks, but it is not yet well understood and may be suboptimal. We
propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach
that achieves significantly higher generalization than HRM, while using a
single tiny network with only 2 layers. With only 7M parameters, TRM obtains
45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs
(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the
parameters.

</details>


### [832] [Flow-Matching Based Refiner for Molecular Conformer Generation](https://arxiv.org/abs/2510.04878)
*Xiangyang Xu,Hongyang Gao*

Main category: cs.LG

TL;DR: 通过使用流匹配精炼器并调整噪声尺度来改进低能量分子构象生成，解决了现有基于去噪的方法在采样时易出错的问题。


<details>
  <summary>Details</summary>
Motivation: 低能量分子构象生成（MCG）是药物发现中的一个基础性难题。现有的基于去噪的方法（如扩散模型和流匹配模型）虽然能学习从简单基础分布到分子构象分布的映射，但在采样过程中，尤其是在低信噪比（SNR）阶段，容易出现误差累积且难以训练。

Method: 提出了一种用于MCG的流匹配精炼器。该方法从上游去噪模型产生的混合质量的输出开始采样，并重新安排噪声尺度以跳过低信噪比阶段，从而提高样本质量。

Result: 在GEOM-QM9和GEOM-Drugs基准数据集上，生成器-精炼器管道在减少总去噪步数的同时提高了样本质量，并保持了多样性。

Conclusion: 所提出的流匹配精炼器能够有效地改进低能量分子构象生成，克服了现有方法的局限性，并在保持多样性的前提下提高了生成样本的质量。

Abstract: Low-energy molecular conformers generation (MCG) is a foundational yet
challenging problem in drug discovery. Denoising-based methods include
diffusion and flow-matching methods that learn mappings from a simple base
distribution to the molecular conformer distribution. However, these approaches
often suffer from error accumulation during sampling, especially in the low SNR
steps, which are hard to train. To address these challenges, we propose a
flow-matching refiner for the MCG task. The proposed method initializes
sampling from mixed-quality outputs produced by upstream denoising models and
reschedules the noise scale to bypass the low-SNR phase, thereby improving
sample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the
generator-refiner pipeline improves quality with fewer total denoising steps
while preserving diversity.

</details>


### [833] [Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models](https://arxiv.org/abs/2510.04888)
*Alina Ermilova,Dmitrii Kornilov,Sofia Samoilova,Ekaterina Laptenkova,Anastasia Kolesnikova,Ekaterina Podplutova,Senotrusova Sofya,Maksim G. Sharaev*

Main category: cs.LG

TL;DR: LLM在发现新疾病关联性方面潜力有限，基于LLM的方法在疾病关联性方面表现出最低的ICD代码连接到不同疾病的多样性。基于LLM的方法在疾病关联性方面表现出最低的ICD代码连接到不同疾病的多样性，研究结果为未来临床研究和医疗保健领域的AI应用提供了有价值的疾病本体资源。


<details>
  <summary>Details</summary>
Motivation: 手动分析临床数据以识别疾病关联性耗时、主观且存在争议。尽管机器学习（ML）有潜力，但仍面临选择最佳方法、确定真实临床数据或结构化疾病描述哪个更可靠以及缺乏“地面真相”的挑战。大型语言模型（LLM）虽然用途广泛，但通常缺乏专业的医学知识。

Method: 本研究系统评估了七种基于两种数据源（MIMIC-IV EHR的ICD-10代码序列和包含或不包含文本描述的完整ICD-10代码集）的疾病关系发现方法。该框架整合了统计共现分析、掩码语言模型（MLM）、领域特定的BERT变体（Med-BERT和BioClinicalBERT）、通用的BERT和文档检索以及四种LLM（Mistral、DeepSeek、Qwen和YandexGPT）。

Result: 基于LLM的方法产生的疾病关联性在ICD代码连接到不同疾病的多样性方面低于其他方法（包括基于文本和基于领域的方法）。

Conclusion: LLM在发现新疾病关联性方面的潜力有限。在缺乏医疗ICD代码之间关联性的地面真相数据库的情况下，本研究的结果构成了一个有价值的医疗疾病本体，可作为未来临床研究和医疗保健领域人工智能应用的宝贵资源。

Abstract: Identifying disease interconnections through manual analysis of large-scale
clinical data is labor-intensive, subjective, and prone to expert disagreement.
While machine learning (ML) shows promise, three critical challenges remain:
(1) selecting optimal methods from the vast ML landscape, (2) determining
whether real-world clinical data (e.g., electronic health records, EHRs) or
structured disease descriptions yield more reliable insights, (3) the lack of
"ground truth," as some disease interconnections remain unexplored in medicine.
Large language models (LLMs) demonstrate broad utility, yet they often lack
specialized medical knowledge. To address these gaps, we conduct a systematic
evaluation of seven approaches for uncovering disease relationships based on
two data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the
full set of ICD-10 codes, both with and without textual descriptions. Our
framework integrates the following: (i) a statistical co-occurrence analysis
and a masked language modeling (MLM) approach using real clinical data; (ii)
domain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a
general-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,
DeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained
interconnection matrices shows that the LLM-based approach produces
interconnections with the lowest diversity of ICD code connections to different
diseases compared to other methods, including text-based and domain-based
approaches. This suggests an important implication: LLMs have limited potential
for discovering new interconnections. In the absence of ground truth databases
for medical interconnections between ICD codes, our results constitute a
valuable medical disease ontology that can serve as a foundational resource for
future clinical research and artificial intelligence applications in
healthcare.

</details>


### [834] [Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects](https://arxiv.org/abs/2510.04901)
*Jonathan Colaço Carr,Qinyi Sun,Cameron Allen*

Main category: cs.LG

TL;DR: 现有的技能发现算法忽视了状态变量，导致技能缺乏对特定状态变量的控制。本文提出了一种能够学习针对特定状态变量的聚焦技能的方法，该方法提高了状态空间覆盖率，解锁了新的学习能力，并避免了负面效应。


<details>
  <summary>Details</summary>
Motivation: 现有的技能发现算法忽视了状态变量，导致发现的技能缺乏对特定状态变量的控制，从而阻碍了探索效率，增加了学习难度，并在下游任务中产生负面效应。

Method: 提出了一种能够让技能发现算法学习聚焦技能（目标并控制特定状态变量的技能）的通用方法。

Result: 该方法将状态空间覆盖率提高了三倍，解锁了新的学习能力，并能自动避免下游任务中的负面效应。

Conclusion: 本文提出的方法能够学习到更加聚焦的技能，提高了技能发现的效率和效果。

Abstract: Skills are essential for unlocking higher levels of problem solving. A common
approach to discovering these skills is to learn ones that reliably reach
different states, thus empowering the agent to control its environment.
However, existing skill discovery algorithms often overlook the natural state
variables present in many reinforcement learning problems, meaning that the
discovered skills lack control of specific state variables. This can
significantly hamper exploration efficiency, make skills more challenging to
learn with, and lead to negative side effects in downstream tasks when the goal
is under-specified. We introduce a general method that enables these skill
discovery algorithms to learn focused skills -- skills that target and control
specific state variables. Our approach improves state space coverage by a
factor of three, unlocks new learning capabilities, and automatically avoids
negative side effects in downstream tasks.

</details>


### [835] [DP-HYPE: Distributed Differentially Private Hyperparameter Search](https://arxiv.org/abs/2510.04902)
*Johannes Liebenow,Thorsten Peinemann,Esfandiar Mohammadi*

Main category: cs.LG

TL;DR: DP-HYPE是一个分布式、隐私保护的超参数搜索算法，通过本地评估和分布式投票来选择多数支持的超参数。


<details>
  <summary>Details</summary>
Motivation: 在分布式机器学习中，超参数调优对模型性能至关重要，但在敏感数据上进行调优时，隐私保护成为一个挑战。现有的差分隐私超参数调优方法存在计算成本高、单独为每个客户端确定超参数或本地应用差分隐私导致效用-隐私权衡不理想等问题。

Method: DP-HYPE算法通过执行分布式投票，基于客户端的本地超参数评估来选择超参数，从而实现分布式和隐私保护的超参数搜索。

Result: DP-HYPE算法在客户端层面提供差分隐私保护，且隐私保证不依赖于超参数的数量。同时，论文给出了其效用保证（达成折衷的可能性）的界限，并在Flower框架中实现了该算法。在多个基准数据集和不同数据分布（iid和non-iid）的设置下进行了评估，证明了即使在隐私预算较小的情况下，DP-HYPE也具有高效用。

Conclusion: DP-HYPE算法能够有效地在分布式机器学习中进行隐私保护的超参数搜索，实现了可扩展性、任务独立性以及良好的效用-隐私权衡。

Abstract: The tuning of hyperparameters in distributed machine learning can
substantially impact model performance. When the hyperparameters are tuned on
sensitive data, privacy becomes an important challenge and to this end,
differential privacy has emerged as the de facto standard for provable privacy.
A standard setting when performing distributed learning tasks is that clients
agree on a shared setup, i.e., find a compromise from a set of hyperparameters,
like the learning rate of the model to be trained. Yet, prior work on
differentially private hyperparameter tuning either uses computationally
expensive cryptographic protocols, determines hyperparameters separately for
each client, or applies differential privacy locally, which can lead to
undesirable utility-privacy trade-offs.
  In this work, we present our algorithm DP-HYPE, which performs a distributed
and privacy-preserving hyperparameter search by conducting a distributed voting
based on local hyperparameter evaluations of clients. In this way, DP-HYPE
selects hyperparameters that lead to a compromise supported by the majority of
clients, while maintaining scalability and independence from specific learning
tasks. We prove that DP-HYPE preserves the strong notion of differential
privacy called client-level differential privacy and, importantly, show that
its privacy guarantees do not depend on the number of hyperparameters. We also
provide bounds on its utility guarantees, that is, the probability of reaching
a compromise, and implement DP-HYPE as a submodule in the popular Flower
framework for distributed machine learning. In addition, we evaluate
performance on multiple benchmark data sets in iid as well as multiple non-iid
settings and demonstrate high utility of DP-HYPE even under small privacy
budgets.

</details>


### [836] [How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning](https://arxiv.org/abs/2510.04908)
*Haotian Gao,Zheng Dong,Jiawei Yong,Shintaro Fukushima,Kenjiro Taura,Renhe Jiang*

Main category: cs.LG

TL;DR: ST-SSDL是一个时空预测框架，通过自监督偏差学习来捕捉和利用动态偏差，以提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分考虑当前输入与历史模式之间的动态偏差，而这些偏差包含可能显著影响模型性能的关键信号。

Method: ST-SSDL将每个输入与其历史平均值进行关联，并使用代表典型时空模式的可学习原型来离散化潜在空间。提出两个辅助目标：对比损失（增强原型判别力）和偏差损失（量化偏差并规范输入表示与原型之间的距离一致性）。这些组件与预测目标联合优化。

Result: 在六个基准数据集上，ST-SSDL在多个指标上持续优于最先进的方法。可视化结果表明，该模型能够自适应地响应复杂时空场景中不同程度的偏差。

Conclusion: ST-SSDL通过整合自监督偏差学习，能够有效捕捉和利用时空数据中的动态偏差，从而在各种输入条件下提高预测性能和泛化能力。

Abstract: Spatio-temporal forecasting is essential for real-world applications such as
traffic management and urban computing. Although recent methods have shown
improved accuracy, they often fail to account for dynamic deviations between
current inputs and historical patterns. These deviations contain critical
signals that can significantly affect model performance. To fill this gap, we
propose ST-SSDL, a Spatio-Temporal time series forecasting framework that
incorporates a Self-Supervised Deviation Learning scheme to capture and utilize
such deviations. ST-SSDL anchors each input to its historical average and
discretizes the latent space using learnable prototypes that represent typical
spatio-temporal patterns. Two auxiliary objectives are proposed to refine this
structure: a contrastive loss that enhances inter-prototype discriminability
and a deviation loss that regularizes the distance consistency between input
representations and corresponding prototypes to quantify deviation. Optimized
jointly with the forecasting objective, these components guide the model to
organize its hidden space and improve generalization across diverse input
conditions. Experiments on six benchmark datasets show that ST-SSDL
consistently outperforms state-of-the-art baselines across multiple metrics.
Visualizations further demonstrate its ability to adaptively respond to varying
levels of deviation in complex spatio-temporal scenarios. Our code and datasets
are available at https://github.com/Jimmy-7664/ST-SSDL.

</details>


### [837] [Glocal Information Bottleneck for Time Series Imputation](https://arxiv.org/abs/2510.04910)
*Jie Yang,Kexin Zhang,Guibin Zhang,Philip S. Yu,Kaize Ding*

Main category: cs.LG

TL;DR: Glocal-IB是一种新的时间序列插补（TSI）训练范式，通过引入全局对齐损失来解决现有模型在处理高缺失率数据时遇到的局部信息过拟合和全局信息丢失的问题，从而提高模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列插补模型通常只优化点估计重建损失，侧重于恢复数值（局部信息），在高缺失率情况下，模型在训练阶段表现良好，但在推理阶段插补效果差，潜在表示分布失真（全局信息），揭示了当前优化目标缺乏全局指导，导致模型过拟合局部噪声且无法捕捉数据全局信息。

Method: 提出一种名为Glocal-IB的新训练范式，该范式不依赖于特定模型，并通过引入源自可处理互信息估计的全局对齐损失来扩展标准的i.i.d.框架。该损失旨在使掩码输入的潜在表示与其原始观测值的潜在表示对齐。

Result: 在九个数据集上的大量实验证明，Glocal-IB在高缺失率情况下能够持续提高性能并使潜在表示对齐。

Conclusion: Glocal-IB通过引入全局对齐损失，有效解决了现有时间序列插补模型在高缺失率下的局限性，提高了模型的泛化能力和潜在表示的对齐度。

Abstract: Time Series Imputation (TSI), which aims to recover missing values in
temporal data, remains a fundamental challenge due to the complex and often
high-rate missingness in real-world scenarios. Existing models typically
optimize the point-wise reconstruction loss, focusing on recovering numerical
values (local information). However, we observe that under high missing rates,
these models still perform well in the training phase yet produce poor
imputations and distorted latent representation distributions (global
information) in the inference phase. This reveals a critical optimization
dilemma: current objectives lack global guidance, leading models to overfit
local noise and fail to capture global information of the data. To address this
issue, we propose a new training paradigm, Glocal Information Bottleneck
(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework
by introducing a Global Alignment loss, derived from a tractable mutual
information approximation. This loss aligns the latent representations of
masked inputs with those of their originally observed counterparts. It helps
the model retain global structure and local details while suppressing noise
caused by missing values, giving rise to better generalization under high
missingness. Extensive experiments on nine datasets confirm that Glocal-IB
leads to consistently improved performance and aligned latent representations
under missingness. Our code implementation is available in
https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.

</details>


### [838] [Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking](https://arxiv.org/abs/2510.04930)
*Ali Saheb Pasand,Elvis Dohmatob*

Main category: cs.LG

TL;DR: Grokking现象是指模型的训练性能早期达到峰值，而测试/泛化性能在任意多轮训练后停滞不前，然后突然跃升至接近完美的水平。研究表明，通过不对称的（随机）梯度下降速度，沿着梯度的不同主（即奇异）方向，可以诱导Grokking。提出了一种名为“平等梯度下降”（EGD）的修改方法，通过归一化梯度使所有主方向上的动态演化速度完全相同，从而显著加快Grokking速度，甚至消除停滞期。


<details>
  <summary>Details</summary>
Motivation: Grokking现象是机器学习中的一个有趣现象，其特点是模型在训练早期性能达到峰值，而泛化性能在长时间停滞后突然提升。本研究旨在加速Grokking过程，即缩短性能停滞期。

Method: 研究人员通过实证和理论分析表明，梯度在不同主方向上（奇异方向）的不对称下降速度会诱导Grokking。他们提出了一种名为“平等梯度下降”（EGD）的修改方法，通过对梯度进行归一化，确保所有主方向上的动态演化速度相同。该方法可以看作是自然梯度下降的一种改进形式。

Result: 研究结果表明，EGD方法能够显著加快Grokking的速度，在某些情况下甚至完全消除了性能停滞期。在经典的算术问题（如模加法和稀疏奇偶校验问题）上进行实证研究，结果显示所提出的方法能够消除这些问题中普遍存在的性能停滞现象。

Conclusion: 本研究提出了EGD方法，通过调整梯度下降的动态，能够有效加速Grokking过程，并在经典算术问题上取得了显著的成果，为理解和应用Grokking现象提供了新的见解。

Abstract: Grokking is the phenomenon whereby, unlike the training performance, which
peaks early in the training process, the test/generalization performance of a
model stagnates over arbitrarily many epochs and then suddenly jumps to usually
close to perfect levels. In practice, it is desirable to reduce the length of
such plateaus, that is to make the learning process "grok" faster. In this
work, we provide new insights into grokking. First, we show both empirically
and theoretically that grokking can be induced by asymmetric speeds of
(stochastic) gradient descent, along different principal (i.e singular
directions) of the gradients. We then propose a simple modification that
normalizes the gradients so that dynamics along all the principal directions
evolves at exactly the same speed. Then, we establish that this modified
method, which we call egalitarian gradient descent (EGD) and can be seen as a
carefully modified form of natural gradient descent, groks much faster. In
fact, in some cases the stagnation is completely removed. Finally, we
empirically show that on classical arithmetic problems such as modular addition
and sparse parity problem which this stagnation has been widely observed and
intensively studied, that our proposed method eliminates the plateaus.

</details>


### [839] [Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints](https://arxiv.org/abs/2510.04951)
*Jayanta Mandi,Marianne Defresne,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 本文提出了一个决策聚焦学习框架，用于解决具有不确定约束参数的约束优化问题（COP），通过最大似然估计（MLE）推导出两个新的损失函数，分别处理预测参数导致的可行性问题和次优决策问题，并通过一个可调参数平衡两者，在实验中证明了该方法的有效性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有的预测-优化（PtO）问题研究主要集中在优化目标函数中的参数，而忽略了约束参数的不确定性，这可能导致预测参数下的解不可行。决策聚焦学习（DFL）虽然可以优化决策质量，但在处理约束参数时也面临可行性管理的问题。因此，需要一个能够同时管理可行性和决策质量的DFL框架。

Method: 提出了一种新的DFL框架，用于解决约束参数不确定的COP问题。该框架不依赖于底层优化问题是线性规划（LP）或整数线性规划（ILP）的假设。通过最大似然估计（MLE），推导了两个新的损失函数：一个惩罚不可行性（当预测参数导致不可行解时），另一个惩罚次优决策（当真实最优解在预测参数下不可行时）。通过一个可调参数对这两个损失函数进行加权平均，以平衡次优性和可行性。

Result: 实验结果表明，通过调整加权参数，决策者可以有效控制次优性和可行性之间的权衡。在多个COP实例上，该方法在单一调优参数值下，其在次优性和可行性方面的表现与现有基线方法相当。

Conclusion: 本文提出的DFL框架能够有效地解决具有不确定约束参数的COP问题，通过最大似然估计导出的损失函数可以同时处理可行性和决策质量，并且通过一个可调参数提供了灵活性，允许用户根据具体需求进行权衡。该方法在保持性能的同时，也解决了现有方法在约束参数不确定性方面的局限性。

Abstract: When some parameters of a constrained optimization problem (COP) are
uncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising
two stages -- the prediction of the unknown parameters from contextual
information and the subsequent optimization using those predicted parameters.
Decision-focused learning (DFL) implements the first stage by training a
machine learning (ML) model to optimize the quality of the decisions made using
the predicted parameters. When parameters in the constraints of a COP are
predicted, the predicted parameters can lead to infeasible solutions.
Therefore, it is important to simultaneously manage both feasibility and
decision quality. We develop a DFL framework for predicting constraint
parameters in a generic COP. While prior works typically assume that the
underlying optimization problem is a linear program (LP) or integer linear
program (ILP), our approach makes no such assumption. We derive two novel loss
functions based on maximum likelihood estimation (MLE): the first one penalizes
infeasibility (by penalizing when the predicted parameters lead to infeasible
solutions), and the second one penalizes suboptimal decisions (by penalizing
when the true optimal solution is infeasible under the predicted parameters).
We introduce a single tunable parameter to form a weighted average of the two
losses, allowing decision-makers to balance suboptimality and feasibility. We
experimentally demonstrate that adjusting this parameter provides a
decision-maker the control over the trade-off between the two. Moreover, across
several COP instances, we find that for a single value of the tunable
parameter, our method matches the performance of the existing baselines on
suboptimality and feasibility.

</details>


### [840] [StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R](https://arxiv.org/abs/2510.04974)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: StructuralDecompose是一个R包，用于进行时间序列分解，将其分为变化点检测、异常检测、平滑和分解等独立模块，以提高灵活性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列分解方法将分解视为一个整体过程，缺乏灵活性和可解释性。本研究旨在提供一个模块化的时间序列分解方法。

Method: 将时间序列分解过程分为变化点检测、异常检测、平滑和分解四个独立模块，并实现为一个R包StructuralDecompose。

Result: 在模拟和真实世界数据集上展示了StructuralDecompose包的性能，并与Rbeast和autostsm等现有工具进行了性能基准测试。

Conclusion: StructuralDecompose包通过将时间序列分解为独立模块，提高了方法的可解释性和灵活性，适用于可解释的机器学习工作流。

Abstract: We present StructuralDecompose, an R package for modular and interpretable
time series decomposition. Unlike existing approaches that treat decomposition
as a monolithic process, StructuralDecompose separates the analysis into
distinct components: changepoint detection, anomaly detection, smoothing, and
decomposition. This design provides flexibility and robust- ness, allowing
users to tailor methods to specific time series characteristics. We demonstrate
the package on simulated and real-world datasets, benchmark its performance
against state-of-the- art tools such as Rbeast and autostsm, and discuss its
role in interpretable machine learning workflows.

</details>


### [841] [Federated Computation of ROC and PR Curves](https://arxiv.org/abs/2510.04979)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 在联邦学习（FL）中，由于隐私和通信限制，计算接收者操作特征（ROC）和精确率-召回率（PR）曲线存在挑战。本文提出了一种在分布式差分隐私下估计预测分数分布分位数的新方法，用于近似FL中的ROC和PR曲线，并提供了理论误差界限。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习（FL）场景下，由于隐私和通信限制，标准的接收者操作特征（ROC）和精确率-召回率（PR）曲线计算方法难以实施，因为服务器无法访问原始预测分数和类别标签。

Method: 提出了一种在分布式差分隐私下估计预测分数分布分位数的新方法，用于近似联邦学习（FL）中的ROC和PR曲线。

Result: 在真实世界数据集上的实证结果表明，该方法在通信量和隐私保护方面取得了良好的平衡，实现了高近似精度。

Conclusion: 所提出的方法在联邦学习场景下能够有效且隐私地近似计算ROC和PR曲线，具有实际应用价值。

Abstract: Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves are
fundamental tools for evaluating machine learning classifiers, offering
detailed insights into the trade-offs between true positive rate vs. false
positive rate (ROC) or precision vs. recall (PR). However, in Federated
Learning (FL) scenarios, where data is distributed across multiple clients,
computing these curves is challenging due to privacy and communication
constraints. Specifically, the server cannot access raw prediction scores and
class labels, which are used to compute the ROC and PR curves in a centralized
setting. In this paper, we propose a novel method for approximating ROC and PR
curves in a federated setting by estimating quantiles of the prediction score
distribution under distributed differential privacy. We provide theoretical
bounds on the Area Error (AE) between the true and estimated curves,
demonstrating the trade-offs between approximation accuracy, privacy, and
communication cost. Empirical results on real-world datasets demonstrate that
our method achieves high approximation accuracy with minimal communication and
strong privacy guarantees, making it practical for privacy-preserving model
evaluation in federated systems.

</details>


### [842] [Adaptive Memory Momentum via a Model-Based Framework for Deep Learning Optimization](https://arxiv.org/abs/2510.04988)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: 通过在线调整动量系数来改进深度学习模型训练中的优化器。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型主要使用固定动量系数的一阶优化器进行训练，这种策略虽然普遍，但并非最优。

Method: 提出了一种自适应记忆机制，用动态调整的动量系数替代固定的动量系数。该方法通过使用当前梯度和累积历史梯度来近似目标函数，并提出了一种新颖的近端框架来实现。

Result: 在从简单凸问题到大规模深度学习等各种学习任务上，实现了SGD和AdamW的自适应记忆变体，并证明其性能优于具有手动调整动量系数的标准SGD和Adam。

Conclusion: 提出的自适应记忆机制是一种新颖、简单且无需额外超参数调优的方法，可以提高优化器的性能，并为优化器中的自适应性研究开辟了新的方向。

Abstract: The vast majority of modern deep learning models are trained with
momentum-based first-order optimizers. The momentum term governs the
optimizer's memory by determining how much each past gradient contributes to
the current convergence direction. Fundamental momentum methods, such as
Nesterov Accelerated Gradient and the Heavy Ball method, as well as more recent
optimizers such as AdamW and Lion, all rely on the momentum coefficient that is
customarily set to $\beta = 0.9$ and kept constant during model training, a
strategy widely used by practitioners, yet suboptimal. In this paper, we
introduce an \textit{adaptive memory} mechanism that replaces constant momentum
with a dynamic momentum coefficient that is adjusted online during
optimization. We derive our method by approximating the objective function
using two planes: one derived from the gradient at the current iterate and the
other obtained from the accumulated memory of the past gradients. To the best
of our knowledge, such a proximal framework was never used for momentum-based
optimization. Our proposed approach is novel, extremely simple to use, and does
not rely on extra assumptions or hyperparameter tuning. We implement adaptive
memory variants of both SGD and AdamW across a wide range of learning tasks,
from simple convex problems to large-scale deep learning scenarios,
demonstrating that our approach can outperform standard SGD and Adam with
hand-tuned momentum coefficients. Finally, our work opens doors for new ways of
inducing adaptivity in optimization.

</details>


### [843] [Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective](https://arxiv.org/abs/2510.05023)
*Weixin Wang,Haoyang Zheng,Guang Lin,Wei Deng,Pan Xu*

Main category: cs.LG

TL;DR: 现有用于多臂老虎机的近似汤普森采样（TS）算法通常使用随机梯度 Langevin 动力学（SGLD）在每个回合对后验进行采样，但需要针对不同回合调整超参数。本文提出的 TS-SA 算法通过引入随机近似（SA）来解决这个问题，它仅使用最近的奖励来构建后验近似，执行 Langevin Monte Carlo（LMC）更新，并通过 SA 步骤平均噪声样本，从而近似一个平稳的后验目标。这使得算法具有固定的步长、统一的收敛分析框架以及通过时间平均改进的后验估计。


<details>
  <summary>Details</summary>
Motivation: 现有近似 Thompson Sampling（TS）算法在多臂老虎机问题中，由于需要为每一轮问题近似一个不同的后验分布，需要针对每一轮进行特定的超参数（如动态学习率）调整，这给理论分析和实际实现都带来了挑战。

Method: TS-SA 算法在 TS 框架中引入随机近似（SA）。在每一轮中，TS-SA 仅使用最近的奖励来构建后验近似，执行 Langevin Monte Carlo（LMC）更新，并通过 SA 步骤对噪声样本进行平均。这可以被看作是在整个算法中近似一个平稳的后验目标。

Result: TS-SA 算法具有固定的步长，统一的收敛分析框架，并且通过时间平均可以获得更好的后验估计。该算法实现了接近最优的遗憾边界，并且理论分析更加简化和直观，因为整个算法可以被解释为平稳 SGLD 过程的模拟。实验结果表明，即使是具有一定预热的单步 Langevin 更新，在老虎机任务上的表现也优于现有方法。

Conclusion: TS-SA 算法通过引入随机近似（SA）来解决现有近似 TS 算法在多臂老虎机问题中面临的非平稳性问题，实现了更简化的分析和更优的性能。

Abstract: Most existing approximate Thompson Sampling (TS) algorithms for multi-armed
bandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in
each round to sample from the posterior, relaxing the need for conjugacy
assumptions between priors and reward distributions in vanilla TS. However,
they often require approximating a different posterior distribution in
different round of the bandit problem. This requires tricky, round-specific
tuning of hyperparameters such as dynamic learning rates, causing challenges in
both theoretical analysis and practical implementation. To alleviate this
non-stationarity, we introduce TS-SA, which incorporates stochastic
approximation (SA) within the TS framework. In each round, TS-SA constructs a
posterior approximation only using the most recent reward(s), performs a
Langevin Monte Carlo (LMC) update, and applies an SA step to average noisy
proposals over time. This can be interpreted as approximating a stationary
posterior target throughout the entire algorithm, which further yields a fixed
step-size, a unified convergence analysis framework, and improved posterior
estimates through temporal averaging. We establish near-optimal regret bounds
for TS-SA, with a simplified and more intuitive theoretical analysis enabled by
interpreting the entire algorithm as a simulation of a stationary SGLD process.
Our empirical results demonstrate that even a single-step Langevin update with
certain warm-up outperforms existing methods substantially on bandit tasks.

</details>


### [844] [Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment](https://arxiv.org/abs/2510.05024)
*Nevan Wichers,Aram Ebtekar,Ariana Azarbal,Victor Gillioz,Christine Ye,Emil Ryd,Neil Rathi,Henry Sleight,Alex Mallen,Fabien Roger,Samuel Marks*

Main category: cs.LG

TL;DR: Inoculation Prompting (IP) 是一种通过在训练提示中明确要求不期望的行为来防止模型学习该行为的技术，从而在不显著影响模型学习期望能力的情况下，减少不期望行为的学习。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的训练信号不完美，可能导致模型出现奖励破解和谄媚等不期望的行为。而提高监督信号的质量成本高昂或不可行，因此需要开发能够改进模型学习行为的方法。

Method: Inoculation Prompting (IP) 技术通过修改监督微调（SFT）的训练提示，明确要求模型表现出不期望的行为。例如，为了防止奖励破解，会在提示中要求模型生成仅在提供的测试用例中有效，但在其他输入中无效的代码。

Result: 在四种不同的设置下，IP 技术能够减少不期望行为的学习，同时不会显著降低模型学习期望能力。此外，研究发现，在微调前更能诱导不期望行为的提示，在训练中使用时能更有效地进行接种。

Conclusion: IP 是一种简单而有效的控制模型从微调中泛化行为的方法，可以在不显著干扰模型学习期望能力的情况下，阻止模型学习不期望的行为。

Abstract: Large language models are sometimes trained with imperfect oversight signals,
leading to undesired behaviors such as reward hacking and sycophancy. Improving
oversight quality can be expensive or infeasible, motivating methods that
improve learned behavior despite an imperfect training signal. We introduce
Inoculation Prompting (IP), a simple but counterintuitive technique that
prevents learning of an undesired behavior by modifying training prompts to
explicitly request it. For example, to inoculate against reward hacking, we
modify the prompts used in supervised fine-tuning to request code that only
works on provided test cases but fails on other inputs. Across four settings we
find that IP reduces the learning of undesired behavior without substantially
reducing the learning of desired capabilities. We also show that prompts which
more strongly elicit the undesired behavior prior to fine-tuning more
effectively inoculate against the behavior when used during training; this
serves as a heuristic to identify promising inoculation prompts. Overall, IP is
a simple yet effective way to control how models generalize from fine-tuning,
preventing learning of undesired behaviors without substantially disrupting
desired capabilities.

</details>


### [845] [Graph-Aware Diffusion for Signal Generation](https://arxiv.org/abs/2510.05036)
*Sergio Rozada,Vimal K. B.,Andrea Cavallo,Antonio G. Marques,Hadi Jamali-Rad,Elvin Isufi*

Main category: cs.LG

TL;DR: 本研究提出了一种新的图感知生成扩散模型（GAD），用于从未知分布中生成图信号，解决了现有方法缺乏通用性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图信号生成方法未能充分利用图结构，或者设计过于领域特定。本研究旨在提出一种更通用的图信号生成方法。

Method: 本研究提出了一种图感知生成扩散模型（GAD），该模型在前向过程中利用热方程和时间扭曲系数来融入图结构，并将反向过程解释为一系列图信号去噪问题。

Result: GAD模型在前向动力学方面收敛于以图拉普拉斯算子为参数的协方差的高斯马尔可夫随机场。在合成数据、交通速度测量和温度传感器网络等数据集上验证了GAD的有效性。

Conclusion: GAD模型能够有效地从未知分布中生成图信号，并且在各种应用中优于现有方法。

Abstract: We study the problem of generating graph signals from unknown distributions
defined over given graphs, relevant to domains such as recommender systems or
sensor networks. Our approach builds on generative diffusion models, which are
well established in vision and graph generation but remain underexplored for
graph signals. Existing methods lack generality, either ignoring the graph
structure in the forward process or designing graph-aware mechanisms tailored
to specific domains. We adopt a forward process that incorporates the graph
through the heat equation. Rather than relying on the standard formulation, we
consider a time-warped coefficient to mitigate the exponential decay of the
drift term, yielding a graph-aware generative diffusion model (GAD). We analyze
its forward dynamics, proving convergence to a Gaussian Markov random field
with covariance parametrized by the graph Laplacian, and interpret the backward
dynamics as a sequence of graph-signal denoising problems. Finally, we
demonstrate the advantages of GAD on synthetic data, real traffic speed
measurements, and a temperature sensor network.

</details>


### [846] [Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts](https://arxiv.org/abs/2510.05040)
*Jihoon Lee,Hoyeon Moon,Kevin Zhai,Arun Kumar Chithanar,Anit Kumar Sahu,Soummya Kar,Chul Lee,Souradip Chakraborty,Amrit Singh Bedi*

Main category: cs.LG

TL;DR: dLLMs 隐含学习了半自回归专家混合，但单一推理调度会限制其性能。HEX 是一种无需训练的推理方法，通过集成不同的块调度来利用这些专家，从而提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有 dLLMs 在推理时未能充分利用其隐含学习的专家混合特性，单一固定推理调度会限制其性能。

Method: 提出 HEX (Hidden semiautoregressive EXperts for test-time scaling) 方法，通过集成不同块大小的生成路径，进行多数表决，以利用 dLLMs 的半自回归专家混合特性。

Result: HEX 在 GSM8K、MATH、ARC-C 和 TruthfulQA 等推理基准上显著提高了准确率，无需额外训练。在 GSM8K 上准确率提升高达 3.56 倍（从 24.72% 提升至 88.10%），优于其他方法。

Conclusion: HEX 提出了一种新的 dLLM 推理时序扩展范式，证明了掩码执行顺序对推理性能至关重要。

Abstract: Diffusion-based large language models (dLLMs) are trained flexibly to model
extreme dependence in the data distribution; however, how to best utilize this
information at inference time remains an open problem. In this work, we uncover
an interesting property of these models: dLLMs trained on textual data
implicitly learn a mixture of semi-autoregressive experts, where different
generation orders reveal different specialized behaviors. We show that
committing to any single, fixed inference time schedule, a common practice,
collapses performance by failing to leverage this latent ensemble. To address
this, we introduce HEX (Hidden semiautoregressive EXperts for test-time
scaling), a training-free inference method that ensembles across heterogeneous
block schedules. By doing a majority vote over diverse block-sized generation
paths, HEX robustly avoids failure modes associated with any single fixed
schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to
3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and
specialized fine-tuned methods like GRPO, without additional training. HEX even
yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific
reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.
Our results establish a new paradigm for test-time scaling in diffusion-based
LLMs (dLLMs), revealing that the sequence in which masking is performed plays a
critical role in determining performance during inference.

</details>


### [847] [KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings](https://arxiv.org/abs/2510.05049)
*Ahmed Elhussein,Paul Meddeb,Abigail Newbury,Jeanne Mirone,Martin Stoll,Gamze Gursoy*

Main category: cs.LG

TL;DR: KEEP框架结合知识图谱和临床数据，有效表示医疗编码，并能在多种下游应用中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有医疗机器学习方法在表示结构化医疗编码时，知识图谱方法侧重形式化关系而忽略现实世界模式，数据驱动方法侧重经验关联但忽视术语中的结构化知识，存在权衡取舍。KEEP旨在弥合这一差距。

Method: KEEP框架首先从知识图谱生成嵌入，然后利用临床数据进行正则化训练，在保持本体关系的同时自适应地整合经验模式。最终生成的嵌入无需任务特定的辅助或端到端训练。

Result: KEEP在UK Biobank和MIMIC IV的结构化电子健康记录评估中，优于传统方法和基于语言模型的方法，能有效捕捉语义关系并预测临床结果。此外，KEEP的计算需求低，适用于资源受限的环境。

Conclusion: KEEP框架通过结合知识图谱嵌入和临床数据自适应学习，有效解决了医疗机器学习中医疗编码表示的挑战，能够支持多种下游应用，并在预测临床结果方面表现出色。

Abstract: Machine learning in healthcare requires effective representation of
structured medical codes, but current methods face a trade off: knowledge graph
based approaches capture formal relationships but miss real world patterns,
while data driven methods learn empirical associations but often overlook
structured knowledge in medical terminologies. We present KEEP (Knowledge
preserving and Empirically refined Embedding Process), an efficient framework
that bridges this gap by combining knowledge graph embeddings with adaptive
learning from clinical data. KEEP first generates embeddings from knowledge
graphs, then employs regularized training on patient records to adaptively
integrate empirical patterns while preserving ontological relationships.
Importantly, KEEP produces final embeddings without task specific auxiliary or
end to end training enabling KEEP to support multiple downstream applications
and model architectures. Evaluations on structured EHR from UK Biobank and
MIMIC IV demonstrate that KEEP outperforms both traditional and Language Model
based approaches in capturing semantic relationships and predicting clinical
outcomes. Moreover, KEEP's minimal computational requirements make it
particularly suitable for resource constrained environments.

</details>


### [848] [HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model](https://arxiv.org/abs/2510.05054)
*Peter Van Katwyk,Karianne J. Bergen*

Main category: cs.LG

TL;DR: HybridFlow是一个统一建模aleatoric和epistemic不确定性的混合框架，通过结合条件掩码自回归归一化流和灵活的概率预测器实现。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化对于确保高风险机器学习应用的鲁棒性至关重要。

Method: HybridFlow结合了用于估计aleatoric不确定性的条件掩码自回归归一化流和用于估计epistemic不确定性的灵活概率预测器。

Result: HybridFlow在深度估计、回归基准和冰盖模拟等回归任务上优于现有方法，并且量化的不确定性校准良好，与模型误差对齐。

Conclusion: HybridFlow成功地统一了aleatoric和epistemic不确定性建模，解决了贝叶斯深度学习中的一个关键挑战。

Abstract: Uncertainty quantification is critical for ensuring robustness in high-stakes
machine learning applications. We introduce HybridFlow, a modular hybrid
architecture that unifies the modeling of aleatoric and epistemic uncertainty
by combining a Conditional Masked Autoregressive normalizing flow for
estimating aleatoric uncertainty with a flexible probabilistic predictor for
epistemic uncertainty. The framework supports integration with any
probabilistic model class, allowing users to easily adapt HybridFlow to
existing architectures without sacrificing predictive performance. HybridFlow
improves upon previous uncertainty quantification frameworks across a range of
regression tasks, such as depth estimation, a collection of regression
benchmarks, and a scientific case study of ice sheet emulation. We also provide
empirical results of the quantified uncertainty, showing that the uncertainty
quantified by HybridFlow is calibrated and better aligns with model error than
existing methods for quantifying aleatoric and epistemic uncertainty.
HybridFlow addresses a key challenge in Bayesian deep learning, unifying
aleatoric and epistemic uncertainty modeling in a single robust framework.

</details>


### [849] [Modeling Student Learning with 3.8 Million Program Traces](https://arxiv.org/abs/2510.05056)
*Alexis Ross,Megha Srivastava,Jeremiah Blanchard,Jacob Andreas*

Main category: cs.LG

TL;DR: 研究通过分析编程交互痕迹来理解和辅助编程学习者。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注最终代码或合成痕迹，忽略了学生在编程过程中真实的思考和试错过程。本研究旨在挖掘编程交互痕迹中的信息，以更好地理解学生（尤其是初学者）的编程行为、技能发展，并改进代码生成模型。

Method: 收集了来自 Pencil Code 平台超过 380 万条真实的学生编程交互痕迹数据集，并使用该数据集训练语言模型。通过行为分析和探测分析，研究了模型对学生行为的建模能力，以及代码痕迹属性（如目标回溯、注释数量）与学生个体特征的关系。在此基础上，探索了如何引导代码生成模型，使其在生成更正代码的同时，又能保持与学生原始风格的接近性。

Result: 与仅使用最终代码或合成痕迹训练的模型相比，使用真实交互痕迹训练的模型在建模多样化的学生行为方面表现更强。研究发现，代码痕迹的许多属性（如目标回溯、注释数量）可以从学习到的学生个体表示中预测出来。通过引导代码生成模型，可以帮助学生从错误中恢复，生成更正且风格接近的Editar。

Conclusion: 编程痕迹的许多属性反映了个体学生的特征。使用编辑痕迹进行训练的语言模型，在可控性、学生行为预测能力以及代码生成能力方面均有提升，预示着未来的编程辅助和教育方向。

Abstract: As programmers write code, they often edit and retry multiple times, creating
rich "interaction traces" that reveal how they approach coding tasks and
provide clues about their level of skill development. For novice programmers in
particular, these traces reflect the diverse reasoning processes they employ to
code, such as exploratory behavior to understand how a programming concept
works, re-strategizing in response to bugs, and personalizing stylistic
choices. In this work, we explore what can be learned from training language
models on such reasoning traces: not just about code, but about coders, and
particularly students learning to program. We introduce a dataset of over 3.8
million programming reasoning traces from users of Pencil Code, a free online
educational platform used by students to learn simple programming concepts.
Compared to models trained only on final programs or synthetically-generated
traces, we find that models trained on real traces are stronger at modeling
diverse student behavior. Through both behavioral and probing analyses, we also
find that many properties of code traces, such as goal backtracking or number
of comments, can be predicted from learned representations of the students who
write them. Building on this result, we show that we can help students recover
from mistakes by steering code generation models to identify a sequence of
edits that will results in more correct code while remaining close to the
original student's style. Together, our results suggest that many properties of
code are properties of individual students and that training on edit traces can
lead to models that are more steerable, more predictive of student behavior
while programming, and better at generating programs in their final states.
Code and data is available at https://github.com/meghabyte/pencilcode-public

</details>


### [850] [ResCP: Reservoir Conformal Prediction for Time Series Forecasting](https://arxiv.org/abs/2510.05060)
*Roberto Neglia,Andrea Cini,Michael M. Bronstein,Filippo Maria Bianchi*

Main category: cs.LG

TL;DR: ResCP是一种新的训练无关的保形预测方法，用于时间序列，它利用水库计算来动态地重新加权一致性分数，从而在不影响计算可扩展性的情况下考虑局部时间动态。


<details>
  <summary>Details</summary>
Motivation: 现有的保形预测方法在处理小样本量和分布变化时存在局限性，需要昂贵的重新训练。ResCP旨在克服这些缺点。

Method: ResCP利用水库计算来计算水库状态之间的相似性分数，并使用这些分数来适应性地重新加权观测到的残差，从而动态地重新加权一致性分数。

Result: ResCP在各种预测任务中被证明是有效的，并且在合理的假设下实现了渐近条件覆盖。

Conclusion: ResCP是一种有效且计算成本低的时间序列保形预测方法，在不牺牲计算效率的情况下能够处理局部时间动态。

Abstract: Conformal prediction offers a powerful framework for building
distribution-free prediction intervals for exchangeable data. Existing methods
that extend conformal prediction to sequential data rely on fitting a
relatively complex model to capture temporal dependencies. However, these
methods can fail if the sample size is small and often require expensive
retraining when the underlying data distribution changes. To overcome these
limitations, we propose Reservoir Conformal Prediction (ResCP), a novel
training-free conformal prediction method for time series. Our approach
leverages the efficiency and representation learning capabilities of reservoir
computing to dynamically reweight conformity scores. In particular, we compute
similarity scores among reservoir states and use them to adaptively reweight
the observed residuals at each step. With this approach, ResCP enables us to
account for local temporal dynamics when modeling the error distribution
without compromising computational scalability. We prove that, under reasonable
assumptions, ResCP achieves asymptotic conditional coverage, and we empirically
demonstrate its effectiveness across diverse forecasting tasks.

</details>


### [851] [Boomerang Distillation Enables Zero-Shot Model Size Interpolation](https://arxiv.org/abs/2510.05064)
*Sara Kangaslahti,Nihal V. Nayak,Jonathan Geuter,Marco Fumero,Francesco Locatello,David Alvarez-Melis*

Main category: cs.LG

TL;DR: 通过“回旋镖蒸馏”技术，可以从一个大模型出发，先蒸馏出一个小模型，再通过重构中间模型来生成一系列不同大小的模型，从而实现成本效益和灵活性的提升。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM模型家族构建方法需要独立训练每个模型尺寸，成本高昂且尺寸选择粗糙。

Method: 提出“回旋镖蒸馏”方法：先从大模型（教师）蒸馏出小模型（学生），然后不经额外训练，通过将教师的层块重新整合到学生模型中，逐步重建中间尺寸的模型。

Result: 生成了零样本插值模型，其性能在学生和教师模型之间平滑缩放，并且通常能达到或超过相同尺寸的预训练或蒸馏模型。分析表明，教师和学生模型在剪枝和蒸馏过程中的对齐对于此方法成功至关重要。

Conclusion: “回旋镖蒸馏”提供了一种简单高效的方法来生成细粒度的模型家族，能显著降低训练成本，并支持跨部署环境的灵活适应。

Abstract: Large language models (LLMs) are typically deployed under diverse memory and
compute constraints. Existing approaches build model families by training each
size independently, which is prohibitively expensive and provides only
coarse-grained size options. In this work, we identify a novel phenomenon that
we call boomerang distillation: starting from a large base model (the teacher),
one first distills down to a small student and then progressively reconstructs
intermediate-sized models by re-incorporating blocks of teacher layers into the
student without any additional training. This process produces zero-shot
interpolated models of many intermediate sizes whose performance scales
smoothly between the student and teacher, often matching or surpassing
pretrained or distilled models of the same size. We further analyze when this
type of interpolation succeeds, showing that alignment between teacher and
student through pruning and distillation is essential. Boomerang distillation
thus provides a simple and efficient way to generate fine-grained model
families, dramatically reducing training cost while enabling flexible
adaptation across deployment environments. The code and models are available at
https://github.com/dcml-lab/boomerang-distillation.

</details>


### [852] [MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis](https://arxiv.org/abs/2510.05080)
*Yangyang Wang,Tayo Fabusuyi*

Main category: cs.LG

TL;DR: 本研究提出了一种新的小区域估计框架，通过详细的出行行为特征来改进城市交通规划。


<details>
  <summary>Details</summary>
Motivation: 为了改进城市交通规划，需要更详细地刻画出行行为。

Method: 本研究利用公开的微观数据文件和机器学习方法，为小地理区域的代表性合成人口预测出行行为，改进了传统的四步出行模型，实现了高分辨率的出行生成、分布、方式选择和路径分配的估计。

Result: 通过使用 ACS/PUMS 的工作通勤数据集进行验证，本框架的准确性优于传统方法。

Conclusion: 研究结果能够提供精细化的见解，从而能够针对局部情况定制干预措施，并支持一系列政策应用和有针对性的干预措施，包括微型配送中心的优化选址、有效的路边空间管理以及更具包容性的交通解决方案设计，特别是为弱势社区的设计。

Abstract: This study presents a novel small-area estimation framework to enhance urban
transportation planning through detailed characterization of travel behavior.
Our approach improves on the four-step travel model by employing publicly
available microdata files and machine learning methods to predict travel
behavior for a representative, synthetic population at small geographic areas.
This approach enables high-resolution estimation of trip generation, trip
distribution, mode choice, and route assignment. Validation using ACS/PUMS
work-commute datasets demonstrates that our framework achieves higher accuracy
compared to conventional approaches. The resulting granular insights enable the
tailoring of interventions to address localized situations and support a range
of policy applications and targeted interventions, including the optimal
placement of micro-fulfillment centers, effective curb-space management, and
the design of more inclusive transportation solutions particularly for
vulnerable communities.

</details>


### [853] [TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration](https://arxiv.org/abs/2510.05102)
*Cheng Xin,Fan Xu,Xin Ding,Jie Gao,Jiaxin Ding*

Main category: cs.LG

TL;DR: GNN模型在科学领域取得成功，但可解释性不足阻碍了其在关键决策中的应用。现有方法难以处理复杂多变的图结构。本文提出TopInG，一种利用持久同调识别持久性图谱的方法，通过自调整拓扑约束来区分图谱和非图谱。理论上保证了损失函数的最优性，并在预测精度和解释质量方面超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: GNN模型的可解释性不足，现有方法难以处理复杂多变的图结构。

Method: 提出TopInG框架，利用持久同调识别持久性图谱，采用图谱过滤学习方法和自调整拓扑约束（拓扑差异）来区分图谱和非图谱。

Result: 在处理多变图谱、平衡预测性能和可解释性、缓解虚假相关性方面表现有效，并在预测精度和解释质量方面超越了现有方法。

Conclusion: TopInG框架能够有效解决GNN的可解释性问题，并在预测精度和解释质量方面取得优于现有方法的性能。

Abstract: Graph Neural Networks (GNNs) have shown remarkable success across various
scientific fields, yet their adoption in critical decision-making is often
hindered by a lack of interpretability. Recently, intrinsically interpretable
GNNs have been studied to provide insights into model predictions by
identifying rationale substructures in graphs. However, existing methods face
challenges when the underlying rationale subgraphs are complex and varied. In
this work, we propose TopInG: Topologically Interpretable Graph Learning, a
novel topological framework that leverages persistent homology to identify
persistent rationale subgraphs. TopInG employs a rationale filtration learning
approach to model an autoregressive generation process of rationale subgraphs,
and introduces a self-adjusted topological constraint, termed topological
discrepancy, to enforce a persistent topological distinction between rationale
subgraphs and irrelevant counterparts. We provide theoretical guarantees that
our loss function is uniquely optimized by the ground truth under specific
conditions. Extensive experiments demonstrate TopInG's effectiveness in
tackling key challenges, such as handling variform rationale subgraphs,
balancing predictive performance with interpretability, and mitigating spurious
correlations. Results show that our approach improves upon state-of-the-art
methods on both predictive accuracy and interpretation quality.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [854] [Proper Theory of Magnon Orbital Angular Momentum](https://arxiv.org/abs/2510.03322)
*Junyu Tang,Ran Cheng*

Main category: cond-mat.mes-hall

TL;DR: 该研究提出了一个关于无质量重子轨道角动量（OAM）的理论框架，考虑了自旋和拓扑贡献，并成功复现了Magnon自旋能斯特效应。


<details>
  <summary>Details</summary>
Motivation: 由于无质量重子不直接与磁场相互作用，因此需要建立一个理论来描述其轨道动力学，特别是OAM。

Method: 利用Aharonov-Casher效应和微扰理论，在有限温度下制定了Magnon OAM的理论，并明确区分了自旋和拓扑贡献。

Result: 在二维蜂窝状晶格中，发现Dzyaloshinskii-Moriya相互作用在铁磁和反铁磁基态中都会诱导大的Magnon OAM。

Conclusion: 该理论为研究具有内禀自旋的无质量重子的轨道动力学奠定了基础。

Abstract: The orbital motion of chargeless bosons, unlike that of electrons, does not
generate a magnetic moment and thus cannot directly interact with magnetic
fields. Utilizing the Aharonov-Casher effect and perturbation theory, we
formulate a proper theory for the magnon orbital angular momentum (OAM) at
finite temperatures, explicitly identifying both self-rotation and topological
contributions, analogous to the electronic counterpart but with correct bosonic
statistics. Comparing with previous studies on magnon OAM, the magnon spin
Nernst effect can only be correctly reproduced using the proper theory for
magnon OAM. In a two-dimensional honeycomb lattice, we show that the
Dzyaloshinskii-Moriya interaction induces a large magnon OAM in both
ferromagnetic and antiferromagnetic ground states. Our formulation provides a
foundation for studying orbital dynamics of chargeless bosons with intrinsic
spin.

</details>


### [855] [Electron-beam-induced Contactless Manipulation of Interlayer Twist in van der Waals Heterostructures](https://arxiv.org/abs/2510.03347)
*Nicola Curreli,Tero S. Kulmala,Riya Sebait,Nicolò Petrini,Matteo Bruno Lodi,Roman Furrer,Alessandro Fanti,Michel Calame,Ilka Kriegel*

Main category: cond-mat.mes-hall

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The ability to dynamically control the relative orientation of layers in two
dimensional (2D) van der Waals (vdW) heterostructures represents a critical
step toward the realization of reconfigurable nanoscale devices. Existing
actuation methods often rely on mechanical contact, complex architectures, or
extreme operating conditions, which limit their applicability and scalability.
In this work, we present a proof-of-concept demonstration of contactless
electrostatic actuation based on electron-beam-induced charge injection. By
locally charging an insulating hexagonal boron nitride (hBN) flake on an
electrically grounded graphene layer, we create an interfacial electric field
that generates in-plane electrostatic torque and induces angular displacement.
We validate the induced rotation through in-situ scanning electron microscopy
(SEM) and twist-dependent Raman spectroscopy.

</details>


### [856] [Spin-orbit coupling and the Edelstein effect at conducting ferroelectric domain walls](https://arxiv.org/abs/2510.03406)
*Maryam A. Nasir,W. A. Atkinson*

Main category: cond-mat.mes-hall

TL;DR: 铁电畴壁的二维电子气具有艾斯品纹理，可以通过Edelstein效应在室温下测量。


<details>
  <summary>Details</summary>
Motivation: 研究铁电畴壁中二维电子气的自旋纹理和Edelstein效应。

Method: 利用对称性构建六带紧束缚电子哈密顿模型，并将其应用于BaTiO3。

Result: 发现二维电子气具有垂直于畴壁的艾斯品纹理，并预测了其Edelstein效应。

Conclusion: 头对头铁电畴壁在稀薄极限和室温下应具有可测量的Edelstein效应，并提出实验方法进行测量。

Abstract: Head-to-head ferroelectric domain walls are intrinsically charged, and are
typically compensated by a mix of oppositely charged defects and free
electrons. The free electrons form a two-dimensional electron gas (2DEG) along
the domain wall. In many cases, inversion symmetry is broken at the wall, which
implies that the 2DEG is subject to nontrivial spin-orbit coupling. Here, we
use symmetry arguments to construct a generic six-band tight-binding electronic
Hamiltonian for a $90^\circ$ head-to-head ferroelectric domain wall. The model,
which includes spin-orbit physics and has a multi-orbital $t_{2g}$ band
structure that is common to transition-metal perovskites, is applied to
BaTiO$_3$. We find that the 2DEG develops an Ising spin texture, with spins
aligned perpendicular to the domain wall. We contrast this with the Rashba spin
texture that should emerge at weakly conducting $90^\circ$ head-to-tail domain
walls. We then show that the head-to-head domain walls should have a measurable
Edelstein effect (that is, a current-induced magnetization), even in the dilute
limit and at room temperature, and describe a simple experiment to measure it.

</details>


### [857] [High-spin magnetic ground states of neutral dopant clusters in semiconductors](https://arxiv.org/abs/2510.03575)
*Rhine Samajdar,Haonan Zhou,R. N. Bhatt*

Main category: cond-mat.mes-hall

TL;DR: 通过利用多谷半导体的谷简并性，可以设计和实现具有可扩展净自旋的高自旋磁性状态，用于信息存储和磁性记忆技术。


<details>
  <summary>Details</summary>
Motivation: 高自旋状态在经典和量子信息存储以及新兴磁性记忆技术领域具有巨大潜力。本研究旨在提出一个系统性的框架，用于在半导体中的取代杂质形成的掺杂剂簇中工程化此类高自旋磁性状态。

Method: 本研究利用多谷半导体中存在的振荡形式因子，通过精确排列掺杂剂来控制电子的跃迁和交换耦合，以稳定高自旋状态。研究采用有效质量理论和紧束缚近似来构建从有限簇到扩展晶格和分形状的镶嵌结构的明确示例。

Result: 在二维体系中，研究发现了多种有利的掺杂剂几何结构，能够在热力学极限下支持接近完全极化值一半的净自旋，其中一种结构实现了超过70%的极化率。

Conclusion: 本研究提出了一个通用的设计原理，利用半导体中的谷简并性来构建稳健的高自旋状态，并为通过精密掺杂剂注入实现这些状态的实验验证指明了方向。

Abstract: High-spin states hold significant promise for classical and quantum
information storage and emerging magnetic memory technologies. Here, we present
a systematic framework for engineering such high-spin magnetic states in dopant
clusters formed from substitutional impurities in semiconductors. In
single-valley materials such as gallium arsenide, impurity states are
hydrogenic and exchange interactions generally favor low-spin configurations,
except in special geometries. In contrast, multivalley semiconductors exhibit
oscillatory form factors in their exchange couplings, enabling the controlled
suppression of selected hopping processes and exchange couplings. Exploiting
this feature, we demonstrate how carefully arranged impurities in aluminum
arsenide, germanium, and silicon can stabilize ground states with a net spin
that scale extensively with system size. Within effective mass theory and the
tight-binding approximation for hopping, we construct explicit examples ranging
from finite clusters to extended lattices and fractal-like tilings. In two
dimensions, we identify several favorable dopant geometries supporting a net
spin equal to around half of the fully polarized value in the thermodynamic
limit, including one which achieves over $70\%$ polarization. Our results
provide a general design principle for harnessing valley degeneracy in
semiconductors to construct robust high-spin states and outline a pathway for
their experimental realization via precision implantation of dopants.

</details>


### [858] [Quantized Piezospintronic Effect in Moiré Systems](https://arxiv.org/abs/2510.03841)
*Mario Castro,Benjamín Mancilla,Fabian Wolff,Alvaro S. Nunez*

Main category: cond-mat.mes-hall

TL;DR: 本论文提出了一种在新颖的畴壁材料中生成和控制自旋电流的方法，该方法利用弹性形变来响应。


<details>
  <summary>Details</summary>
Motivation: 在铁磁扭曲蜂窝双层材料中，利用连续介观模型，特别是基于 Bistritzer-MacDonald 模型，通过弹性形变来生成和控制自旋电流。

Method: 采用连续介观模型，并利用 Berry 相形式来计算自旋电流对形变的响应。

Result: 该模型揭示了由畴壁超晶格势引起的电子能带结构调制，从而产生涌现的拓扑相和新颖的输运特性，例如自旋和电荷输运的量子压电响应。此外，当反演对称性被打破时，系统会获得有限的 Berry 曲率，并且在 $K$ 和 $K'$ 谷中具有相反的符号。当存在应变时，伪规范场的谷对比性质确保了量子响应的鲁棒性，并且与谷陈数之和成正比。

Conclusion: 所提出的模型展示了铁磁扭曲蜂窝双层材料中由弹性形变引起的量子压电效应，以及由 Berry 曲率驱动的谷对比效应。这些发现为开发新颖的自旋电子和谷电子器件提供了基础。

Abstract: This paper presents a novel approach for generating and controlling spin
currents in an antiferromagnetic twisted honeycomb bilayer in response to an
elastic deformation. Utilizing a continuum model, closely based upon the
seminal Bistritzer-MacDonald model, that captures the essential physics of
low-energy moir\'e bands, we calculate the spin current response to the
deformation in terms of the familiar Berry phase formalism. The resulting
moir\'e superlattice potential modulates the electronic band structure, leading
to emergent topological phases and novel transport properties such as quantized
piezo responses both for spin and charge transport. This approach allows us to
tune the system across different topological regimes and to explore the
piezo-spintronic responses as a function of the band topology. When inversion
symmetry is broken either by a sublattice potential $V$, alignment with an hBN
substrate, uniaxial strain, or structural asymmetry present in the moir\'e
superlattice, the system acquires a finite Berry curvature that is opposite in
the $K$ and $K'$ valleys (protected by valley time reversal symmetry). In
contrast, for strain, the valley-contrasting nature of the pseudo-gauge field
ensures that the quantized response is robust and proportional to the sum of
the valley Chern numbers. These notable physical properties make these systems
promising candidates for groundbreaking spintronic and valleytronic devices.

</details>


### [859] [Coupling a $^{73}$Ge nuclear spin to an electrostatically defined quantum dot](https://arxiv.org/abs/2510.03981)
*Paul Steinacker,Gauri Goenka,Rocky Yue Su,Tuomo Tanttu,Wee Han Lim,Santiago Serrano,Tim Botzem,Jesus D. Cifuentes,Shao Qi Lim,Jeffrey C. McCallum,Brett C. Johnson,Fay E. Hudson,Kok Wai Chan,Christopher C. Escott,Andre Saraiva,Chih Hwan Yang,Vincent Mourik,Andrea Morello,Andrew S. Dzurak,Arne Laucht*

Main category: cond-mat.mes-hall

TL;DR: 通过将 73Ge 核自旋耦合到硅中的门定义量子点，为未来的量子信息处理奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 利用硅中单核自旋的相干时间和精确控制特性，在量子技术领域取得进展。

Method: 通过同位素选择性离子注入技术，将 73Ge 核自旋植入硅中，并使用门控量子点进行耦合和读出。

Result: 成功观察到 73Ge 核自旋与耦合量子点电子之间的超精细相互作用（HFI），并通过栅极电压调节 HFI 在 180 kHz 到 350 kHz 之间。

Conclusion: 本研究为未来在 73Ge 核自旋（自旋 9/2）上进行量子信息处理实验，以及实现更高级的量子纠缠分发和弱测量等实验奠定了基础。

Abstract: Single nuclear spins in silicon are a promising resource for quantum
technologies due to their long coherence times and excellent control
fidelities. Qubits and qudits have been encoded on donor nuclei, with
successful demonstrations of Bell states and quantum memories on the spin-1/2
$^{31}$P and cat-qubits on the spin-7/2 $^{123}$Sb nuclei. Isoelectronic
nuclear spins coupled to gate-defined quantum dots, such as the naturally
occurring $^{29}$Si isotope, possess no additional charge and allow for the
coupled electron to be shuttled without destroying the nuclear spin coherence.
Here, we demonstrate the coupling and readout of a spin-9/2 $^{73}$Ge nuclear
spin to a gate-defined quantum dot in SiMOS. The $^{73}$Ge nucleus was
implanted by isotope-selective ion-implantation. We observe the hyperfine
interaction (HFI) to the coupled quantum dot electron and are able to tune it
from 180 kHz to 350 kHz, through the voltages applied to the lateral gate
electrodes. This work lays the foundation for future spin control experiments
on the spin-9/2 qudit as well as more advanced experiments such as entanglement
distribution between distant nuclear spins or repeated weak measurements.

</details>


### [860] [Fractional quantum Hall state at $ν= 1/2$ with energy gap up to 6 K, and possible transition from one- to two-component state](https://arxiv.org/abs/2510.03983)
*Siddharth Kumar Singh,Chengyu Wang,Adbhut Gupta,Kirk W. Baldwin,Loren N. Pfeiffer,Mansour Shayegan*

Main category: cond-mat.mes-hall

TL;DR: 在宽量子阱的1/2填充因子处，双层电子系统的分数阶量子霍尔态（FQHS）起源至今仍未解。本研究通过实验揭示了在不同密度下，该系统1/2 FQHS的演化过程，并发现其在有限密度范围内表现出 Pfaffian (1C) 特征，随后在高密度下转变为 2C 状态。


<details>
  <summary>Details</summary>
Motivation: 研究宽量子阱中1/2填充因子处双层电子系统（2DES）的分数阶量子霍尔态（FQHS）的起源，以确定其是单组分（1C）还是双组分（2C）起源，并探究其演化规律。

Method: 通过实验研究超高品质GaAs量子阱中2DES在不同密度下的行为，包括其旁系FQHS和1/2 FQHS的演化，以及在最高密度下出现的双层Wigner晶体态和1/2 FQHS的电荷传输能量隙变化。

Result: 在低密度下，2DES仅表现出奇分母FQHS，1/2处为复合费米子海。随着密度增加，1/2处出现并增强FQHS，并在其最强时观察到 Pfaffian 1C 状态的 Tochter 态（8/17和7/13）。在高密度下，2DES转变为2C状态，1/2 FQHS在此转变附近保持鲁棒，并出现向上的能量隙尖峰（最大约6K）。

Conclusion: 1/2 FQHS在从1C（Pfaffian）到2C（Ψ331）的转变过程中保持鲁棒，表明其对电荷分布不对称具有韧性。这种从非阿贝尔态到阿贝尔态的转变可能为拓扑量子信息和量子临界性研究开辟新途径。

Abstract: The fractional quantum Hall state (FQHS) observed in the lowest Landau level
at filling factor $\nu=1/2$ in wide quantum wells has been enigmatic for
decades because the two-dimensional electron system (2DES) has a bilayer charge
distribution but with significant interlayer tunneling. Of particular interest
is whether the 1/2 FQHS in this system has a one-component (1C) or
two-component (2C) origin; these are typically identified as the Pfaffian
(non-Abelian) or the $\Psi_{331}$ (Abelian) FQHSs, respectively. We report here
our experimental study of the evolution of the correlated states of an
ultrahigh-quality 2DES confined to a 72.5-nm-wide GaAs quantum well. At the
lowest densities, the 2DES displays only odd-denominator FQHSs, and the ground
state at $\nu = 1/2$ is a composite fermion Fermi sea. As the density is
increased, a FQHS emerges at $\nu = 1/2$, and becomes very strong. In a finite
density range where the 1/2 FQHS is strongest, we also observe its daughter
FQHSs at $\nu = 8/17$ and 7/13, consistent with the theoretically expected
daughter states of a Pfaffian 1/2 FQHS. At the highest densities, the 2DES
becomes 2C, signaled by the emergence of a bilayer Wigner crystal state and the
transitions of FQHSs flanking $\nu=1/2$. The 1/2 FQHS remains robust near this
transition and, notably, its charge transport energy gap exhibits an
\textit{upward} cusp with a maximum value of about 6 K on the 1C side of the
transition; this is the largest gap reported for any even-denominator FQHS. Our
observation of the transition of the 2DES ground states near $\nu=1/2$ to 2C
states at high densities, and our measurements of the robustness of the 1/2
FQHS against charge distribution asymmetry, suggest that the 1/2 FQHS also
makes a transition from 1C to 2C. Such a transition from a non-Abelian to
Abelian state can open avenues for topological quantum information and quantum
criticality.

</details>


### [861] [Effective linear response in non-equilibrium anyonic systems](https://arxiv.org/abs/2510.03985)
*Gu Zhang,Igor Gornyi,Yuval Gefen*

Main category: cond-mat.mes-hall

TL;DR: Here, we develop an effective linear response theory for quantum transport in far-from-equilibrium stationary states involving anyons, applied to study tunnel-coupled anyonic beams in collider geometries. The theory reveals fractional charge and statistics of anyons and signifies real anyon collisions through finite thermoelectric coefficients.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend linear response theory, traditionally limited to near-equilibrium conditions, to describe quantum transport in far-from-equilibrium stationary states, specifically for anyons.

Method: The paper develops an effective linear response theory applicable to anyons in dilute beam states that are far from equilibrium. This theory is then applied to analyze tunnel-coupled anyonic beams in collider geometries, examining processes like braiding, collisions, and tunneling.

Result: The linear-response transport coefficients derived from the theory directly reflect the fractional charge and statistics of the anyons. The emergence of finite thermoelectric coefficients indicates real anyon collisions and a broken particle-hole symmetry characteristic of anyonic gases.

Conclusion: The developed linear response theory allows for the study of quantum transport of anyons in non-equilibrium states, providing insights into their fundamental properties like fractional charge and statistics, and detecting real anyon collisions through thermoelectric effects, which are linked to a unique broken particle-hole symmetry in anyonic systems.

Abstract: Linear response theory serves as a fundamental tool in the study of quantum
transport, extensively employed to elucidate fundamental mechanisms related to
the nature of the particles involved and the underlying symmetries. This
framework is, however, limited to equilibrium or near-equilibrium conditions.
Here, we develop an effective linear response theory designed to describe
charge and thermal quantum transport, where the reference far-from-equilibrium
stationary state comprises anyons forming a dilute beam. We apply our theory to
study tunnel-coupled anyonic beams in collider geometries, enabling braiding,
collisions, and tunneling of anyons at the central collider. Our
linear-response transport coefficients directly reflect the fractional charge
and statistics of the anyons involved, avoiding the need to measure
higher-order current correlations. Moreover, the emergence of finite
thermoelectric (Peltier and Seebeck) coefficients signifies the presence of
real anyon collisions (as opposed to virtual braiding in the time domain),
intimately associated with a broken particle-hole symmetry, specific to anyonic
gases.

</details>


### [862] [Quantum Linear Magnetoresistance: A Modern Perspective](https://arxiv.org/abs/2510.04121)
*Shuai Li,Huichao Wang*

Main category: cond-mat.mes-hall

TL;DR: 该论文是一篇关于量子线性磁阻效应的综述性论文，探讨了其理论基础、实验研究以及未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 解释了磁阻是研究材料内在物理特性的重要手段，并强调了理解量子线性磁阻效应对于研究新兴材料的至关重要性。

Method: 综述了量子线性磁阻效应的理论基础和实验研究，并讨论了该领域的开放性问题和未来的研究方向。

Result: 该论文没有具体实验结果，而是对该领域的研究进行了总结和展望。

Conclusion: 强调了理解量子线性磁阻效应对于研究新兴材料的重要性，并指出了该领域的未来研究方向。

Abstract: Magnetoresistance is a powerful probe for characterizing the intrinsic
physics embedded in materials. Among its various manifestations, linear
magnetoresistance has a long history and continues attracting research
interest. In contemporary studies, a clear understanding of the
magnetoresistance character of quantum origin is more crucial than ever for the
study of emerging materials. In this perspective, we examine the linear
magnetoresistance of quantum mechanism, from its theoretical basis to
experimental studies, and discuss open questions and promising future research
directions in this field.

</details>


### [863] [Optical conductivity and band gap in the double-Weyl candidate SrSi2 at ambient pressure](https://arxiv.org/abs/2510.04266)
*L. Z. Maulana,A. A. Tsirlin,E. Uykur,Y. Saito,M. Dressel,M. Imai,A. V. Pronin*

Main category: cond-mat.mes-hall

TL;DR: SrSi2 具有约 40 meV 的直接带隙，而不是双韦尔状态。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是利用光学光谱探测立方 SrSi2 中可能存在的双韦尔状态。

Method: 测量了 SrSi2 在 70 至 22 000 cm-1 的频率范围内、低至 10 K 的温度和环境压力下的复数电导率，并将其与从头计算进行了比较。

Result: 实验和计算结果表明，传统的密度泛函理论无法描述 SrSi2 在费米能级附近的电子结构。使用半局域交换关联势可以更好地拟合实验数据，并揭示 SrSi2 具有约 40 meV 的直接带隙（平凡带隙），而不是双韦尔状态。

Conclusion: SrSi2 具有约 40 meV 的直接带隙，而不是双韦尔状态。

Abstract: We probe the possible double-Weyl state in cubic SrSi2 using optical
spectroscopy. The complex optical conductivity was measured in a frequency
range from 70 to 22 000 cm-1 at temperatures down to 10 K at ambient pressure.
The optical response of SrSi2 can be well separated into the intraband (free
carriers) and interband contributions. Additionally, four infrared-active
phonons are detected. As follows from the optical spectra, the free-carrier
density decreases with decreasing temperature, consistent with an activation
behaviour. Experimental interband conductivity juxtaposed with ab initio
calculations shows that conventional density-functional theory fails to
describe the electronic structure of SrSi2 in the vicinity of the Fermi level.
A semi-local exchange-correlation potential allows a much better agreement with
the experiment, resulting in the trivial (gapped) band structure of SrSi2. The
direct gap estimated from the measurements is approximately 40 meV.

</details>


### [864] [Classification of Weyl point trajectories in multi-terminal Josephson junctions](https://arxiv.org/abs/2510.04279)
*Kento Takemura,Tomohiro Yokoyama*

Main category: cond-mat.mes-hall

TL;DR: 多端约瑟夫森结中的外尔点受拓扑保护，其动力学性质可通过外部参数操纵，并可通过陈数和相图进行分类。


<details>
  <summary>Details</summary>
Motivation: 研究多端约瑟夫森结中外尔点的动力学性质，探索其拓扑保护特性。

Method: 通过改变电门电压、磁通量、偏置电压等外部参数，操纵外尔点并绘制其轨迹，利用陈数和相图对轨迹进行分类。

Result: 外尔点轨迹形成闭合回路和开放线，并伴随着成对产生和湮灭。

Conclusion: 多端约瑟夫森结中的外尔点具有拓扑保护的性质，其动力学行为可以通过外部参数进行有效的调控。

Abstract: Topological protection is an attractive signature in both fundamental and
applied researches because it provides an exotic and robust state.
Multi-terminal Josephson junctions have recently been studied extensively owing
to the emergence of topologically protected Weyl points without the need for
topological materials. In this study, we examine the dynamic properties of Weyl
points in multi-terminal Josephson junctions. The junctions are modulated by
external parameters, such as electric gate voltage, magnetic flux, bias
voltage. The Weyl points are manipulated and draw trajectories accompanied by
pair creation and annihilation. The trajectories form both closed loops and
open lines. We classify these trajectories using the Chern number and the phase
diagram.

</details>


### [865] [Braids and Beams: Exploring Fractional Statistics with Mesoscopic Anyon Colliders](https://arxiv.org/abs/2510.04319)
*Bernd Rosenow,Bertrand I. Halperin*

Main category: cond-mat.mes-hall

TL;DR: 任何子对撞机利用非平衡玻色化方法，通过电流交叉相关性，为任何子交换相位提供了一种无需干涉仪的探测方法，其结果具有普适性，且可推广到分层状态。


<details>
  <summary>Details</summary>
Motivation: 提供一种无需干涉仪的探测任何子交换相位的手段，并通过电流交叉相关性进行测量。

Method: 采用非平衡玻色化框架，对任何子对撞机中的准粒子碰撞进行建模，并考虑了有限的孤子宽度以解决在分层状态下半经典描述的失效问题。

Result: 电流交叉相关性具有仅取决于交换相位和动力学指数的普适形式，并成功模拟了电荷-e/5 准粒子的碰撞。

Conclusion: 任何子对撞机提供了一种有效的、基于时间的干涉方法来探测任何子统计，并且通过引入孤子宽度等修正，可以更精确地模拟复杂的分层状态。

Abstract: Anyon colliders -- quantum Hall devices where dilute quasiparticle beams
collide at a quantum point contact -- provide an interferometer-free probe of
anyonic exchange phases through current cross correlations. Within a
non-equilibrium bosonization framework, the normalized cross-correlations take
a universal form depending only on the exchange phase and the dynamical
exponent, enabling experimental demonstration of anyonic statistics. This
result can be interpreted as time-domain interference -- braiding in time
rather than spatial exclusion or real-space interferometry. Extension to
hierarchical states shows that the semiclassical step-function description of
quasiparticles fails at large statistical angles. Introducing a finite soliton
width resolves this issue and enables quantitative modeling of charge-$e/5$
quasiparticle collisions.

</details>


### [866] [Spin-wave propagation at low temperatures in YIG thin films on YSGG substrates](https://arxiv.org/abs/2510.04330)
*José Elias Abrão,Daan Weltens,Rhodri Mansell,Sebastiaan van Dijken,Lukáš Flajšman*

Main category: cond-mat.mes-hall

TL;DR: YIG/YSGG薄膜在低温下支持鲁棒的自旋波传播，优于YIG/GGG。


<details>
  <summary>Details</summary>
Motivation: 低温下磁性薄膜自旋波的应用受限于合适的材料平台；YIG/GGG衬底在低温下会产生大的顺磁矩，限制了自旋波传播。

Method: 在钇钪镓石榴石（YSGG）衬底上生长YIG薄膜，并与在GGG衬底上生长的YIG薄膜进行比较，在低温（低至2 K）和不同磁场下测量自旋波传播和铁磁共振（FMR）线宽。

Result: YIG/YSGG薄膜在低温下表现出更窄的FMR线宽，并且没有YIG/GGG系统中存在的原子互扩散效应，支持鲁棒的自旋波传播。

Conclusion: YIG/YSGG薄膜是很有前景的低温自旋波平台，克服了YIG/GGG的局限性，为低温下的可扩展磁子学和混合量子器件提供了新机会。

Abstract: The use of spin waves in magnetic thin films at cryogenic temperatures has
long been hindered by the lack of a suitable material platform. Yttrium iron
garnet (YIG) is the leading candidate, yet it is typically grown on gadolinium
gallium garnet (GGG) substrates, which develop a large paramagnetic moment at
low temperatures. This substrate effect limits spin-wave propagation. In this
work, we demonstrate that thin YIG films grown on yttrium scandium gallium
garnet (YSGG) substrates support robust spin-wave propagation in the
Damon-Eshbach geometry, measurable down to 2 K under applied magnetic fields up
to 150 mT. Compared with YIG/GGG, YIG/YSGG films exhibit narrower ferromagnetic
resonance (FMR) linewidths at low temperatures and are free from the atomic
interdiffusion effects that degrade the performance of YIG/GGG systems. These
results establish YIG/YSGG thin films as a promising low-temperature platform,
overcoming the intrinsic limitations of YIG/GGG and opening new opportunities
for scalable magnonic and hybrid quantum devices operating under cryogenic
conditions.

</details>


### [867] [Defects in hexagonal boron nitride for quantum technologies](https://arxiv.org/abs/2510.04344)
*Tobias Vogl,Viktor Ivády,Isaac J. Luxmoore,Hannah L. Stern*

Main category: cond-mat.mes-hall

TL;DR: 六方氮化硼（hBN）材料中的原子缺陷可用于构建在室温下运行的量子器件。


<details>
  <summary>Details</summary>
Motivation: 原子缺陷是未来量子技术（如量子通信网络、计算机和传感器）的基本构件。然而，此前可用的缺陷和宿主材料选择有限。

Method: 本研究探讨了hBN材料中缺陷的光物理特性。

Result: hBN材料中的单光子发射原子缺陷可用于室温下的量子器件，并能与光学可寻址的电子和核自旋耦合。

Conclusion: hBN中的原子缺陷为开发可在环境条件下运行的量子器件提供了机会。

Abstract: Atomic defects in solid-state materials are building blocks for future
quantum technologies, such as quantum communication networks, computers, and
sensors. Until recently, a handful of defects in a small selection of host
materials have been possible candidates. Recent developments have revealed that
hexagonal boron nitride, a wide-bandgap two-dimensional material, hosts
single-photon-emitting atomic defects with access to optically addressable
electronic and nuclear spins at room temperature. Now, atomically thin quantum
devices that operate at ambient conditions are a possibility. In this
perspective, we discuss the recent progress, and challenges, in understanding
the fundamental photophysics of defects in hBN, as well as specific
opportunities they present for the development of quantum technologies.

</details>


### [868] [Dynamic Landau-Lifshitz-Bloch-Slonczewski equations for spintronics](https://arxiv.org/abs/2510.04562)
*Pascal Thibaudeau,Mouad Fattouhi,Liliana D. Buda-Prejbeanu*

Main category: cond-mat.mes-hall

TL;DR: LLBG方程可以模拟耗散系统，并能精确快速地预测临界电流和开关时间。


<details>
  <summary>Details</summary>
Motivation: LLG方程在模拟斯皮创器件中的焦耳热效应时存在不足，因为它假设磁矩大小恒定，而焦耳热效应会显著影响磁矩大小。

Method: 提出一个将磁矩大小视为与热库耦合的动态变量的统计框架，推导出LLBS方程，并与随机方程进行比较。

Result: LLBS方程能够捕捉由焦耳热引起的瞬态退磁效应，并能精确、加速地预测临界电流和开关时间。

Conclusion: LLBS方程在模拟斯皮创器件中是准确且高效的，特别是在焦耳热效应显著的情况下。

Abstract: The atomistic Landau-Lifshitz-Gilbert equation is challenged when modeling
spintronic devices where Joule heating is significant, due to its core
assumption of a constant magnetization magnitude. Based on a statistical
framework that treats the magnetization magnitude as a dynamic variable coupled
to a thermal bath, we derive a dynamic Landau-Lifshitz-Bloch-Slonczewski set of
equations for torques, that captures the transient, heating-induced
demagnetization that occurs during high-current operation. Integrating these
dynamic equations and comparing them to their stochastic equivalents reveals
that both the energy landscape and switching dynamics in high-anisotropy
systems are similarly modified. This approach yields accurate and accelerated
predictions of critical currents and switching times.

</details>


### [869] [Non-resonant spin injection of exciton-polaritons with halide perovskites at room temperature](https://arxiv.org/abs/2510.04679)
*Pablo Vaquer de Nieves,Elena Sendarrubias Arias-Camisón,Jorge Cuadra,Maksim Lednev,Raúl Gago,Luis Viña,Francisco José García Vidal,Johannes Feist,Ferry Prins,Carlos Antón Solanas*

Main category: cond-mat.mes-hall

TL;DR: 本文提出了一种单片塔木等离激元微腔，其中嵌入了二维卤化物钙钛矿薄膜和可调谐聚合物间隔层，用于研究激子-光子耦合和非线性光子器件。


<details>
  <summary>Details</summary>
Motivation: 研究激子-光子相互作用和非线性光子应用。

Method: 构建了一个包含二维卤化物钙钛矿薄膜和可调谐聚合物间隔层的单片塔木等离激元微腔，并通过角分辨光谱研究其光学性质。

Result: 在室温下，观察到了不同失谐下的激子-极化激聚束分支色散。在圆偏振非共振激光激发下，高能激子的自旋注入及其弛豫到较低极化激聚束分支的特性得以保持，且自旋极化发射得以维持。

Conclusion: 该研究为非共振自旋控制极化激聚束器件（如手征激光器和开关）提供了有前景的见解。

Abstract: Exciton-polaritons, hybrid photon-exciton quasiparticles, constitute a useful
platform for the study of light-matter interaction and nonlinear photonic
applications. In this work, we realize a monolithic Tamm-plasmon microcavity
embedding a thin film of two-dimensional halide perovskites with a tunable
polymer spacer that controls the exciton-photon detuning. Angle-resolved
optical spectroscopy at room temperature reveals the lower polariton branch
dispersions in the linear regime for several detunings. Under circularly
polarized, non-resonant laser excitation, the spin injection of high-energy
excitons and their relaxation towards the lower polariton branch demonstrates
its preservation, in contrast to the bare exciton case. The spin-polarized
emission survives due to the fast decay of polaritons. Our results provide
promising insights into the non-resonant spin control of polaritonic devices,
including chiral lasers and switches.

</details>


### [870] [Stability of graphene hyperbolic pseudospheres under harsh conditions](https://arxiv.org/abs/2510.04699)
*T. P. C. Klaver,R. Gabbrielli,V. Tynianska,A. Iorio,D. Legut*

Main category: cond-mat.mes-hall

TL;DR: 通过分子动力学模拟，我们成功制备了具有高稳定性的石墨烯双曲赝球面，并验证了其在形变和高温下的稳定性。


<details>
  <summary>Details</summary>
Motivation: 探索石墨烯在模拟引力方面的应用，并制备稳定、可控形状的石墨烯曲面。

Method: 采用两步分子动力学模拟：1. 将碳原子挤压成特定形状的薄三维体积，形成不稳定的石墨烯前体；2. 在高温下退火该前体，形成多晶、弯曲的石墨烯。

Result: 制备了高稳定性的石墨烯双曲赝球面，能够承受 $20^\circ$ 的剪切或 $20\%$ 的拉伸，并在释放后温度升高约 $300 	ext{ K}$。缺陷自发形成并稳定了石墨烯的形状，避免了残余应力。

Conclusion: 提出了一种简单实用的方法，用于制备几乎任何形状的模拟弯曲石墨烯曲面，并可预先测试其稳定性，为实验制备提供了参考。

Abstract: We demonstrate the high stability of simulated graphene hyperbolic
pseudospheres under large externally imposed deformations and high temperature
annealing. Hyperbolic pseudospheres are produced in a two-step Molecular
Dynamics simulation process. First, carbon atoms are forced down a thin
three-dimensional volume of a chosen shape. During this extrusion process the
carbon atoms form a precursor to graphene that is unrealistically less stable
than graphite or diamond. Then the unstable carbon structure is annealed inside
the thin volume at high temperature, turning the carbon into realistic
polycrystalline, curved graphene. Point defects naturally appear in numbers and
places that stabilize the graphene in the desired shape, without high residual
stresses. We applied this new methodology to the creation of graphene
hyperbolic pseudosphere surfaces, which reproduce analogs to some aspects of
classical or quantum gravity. The free edges of the pseudosphere cause bending
of the graphene. When these free edges are removed from the simulations by
attaching periodic flat graphene sheets to the pseudosphere edges, the carbon
atoms assume positions just some tenths of \r{A} from the mathematical
hyperbolic pseudosphere surface. In demanding tests of their stability, the
hyperbolic pseudospheres proved stable against $20^\circ$ shearing or $20\%$
elongation and then being released, which eventually raised their temperatures
by $\sim 300 \ \text{K}$. Our methodology is relatively easy to use and offers
a practical way to create simulated curved graphene surfaces of almost any
shape. It allows for thorough testing in advance of the stability of graphene
shapes that are to be produced experimentally.

</details>


### [871] [Fermionic influence superoperator for transport through Majorana zero modes](https://arxiv.org/abs/2510.04959)
*Jia-Lin Pan,Zi-Fan Zhu,Shixuan Chen,Yu Su,Yao Wang*

Main category: cond-mat.mes-hall

TL;DR: 本文提出了一个描述电子通过马约拉纳零模传输的开放量子动力学的费米子超级算子，并推导了其微分等价物——多体薛定谔方程（HEOM），该方程能够描述系统-浴相关的动力学。此外，本文还开发了一种函数导数方案，可以精确地计算辅助密度算子下传输可观测量的值。


<details>
  <summary>Details</summary>
Motivation: 在超导系统中寻找和表征马约拉纳零模是凝聚态物理学中的一个关键问题，然而，由于其非阿贝尔统计和容错性质，其在量子输运中的表现需要精确的理论描述。本文旨在提供一个严谨的理论框架来分析马约拉纳零模的量子输运特性。

Method: 本文基于已有工作，利用超级算子形式主义，推导了描述电子通过马约拉纳零模的开放量子动力学。接着，通过构造其微分等价形式，即多体薛定谔方程（HEOM），来处理系统-浴的关联动力学。最后，开发了一种函数导数方案，得到传输可观测量关于辅助密度算子的精确表达式。

Result: 本文成功推导了描述马约拉纳零模量子输运的费米子超级算子和HEOM。该框架能够精确计算传输可观测量，为后续实验研究提供了理论指导。

Conclusion: 本文提出的超级算子和HEOM方法为研究马约拉纳零模的量子输运特性提供了坚实的理论基础，有助于揭示马约拉纳物理在介观系统中的独特表现。

Abstract: In recent years, the study of Majorana signatures in quantum transport has
become a central focus in condensed matter physics. Here, we present a rigorous
and systematic derivation of the fermionic superoperator describing the open
quantum dynamics of electron transport through Majorana zero modes, building on
the techniques introduced in Phys. Rev. B 105, 035121 (2022). The numerical
implementation of this superoperator is to construct its differential
equivalence, the hierarchical equations of motion (HEOM). The HEOM approach
describes the system-bath correlated dynamics. Furthermore, we also develop a
functional derivative scheme that provides exact expressions for the transport
observables in terms of the auxiliary density operators introduced in the HEOM
formulation. The superoperator formalism establishes a solid theoretical
foundation for analyzing key transport signatures that may uncover the unique
characteristics of Majorana physics in mesoscopic systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [872] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: 本研究使用心理网络分析来探索人类和大型语言模型（LLM）的内部世界模型（IWM），并比较它们在想象方面的异同。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨想象的计算目标，挑战了传统认为想象仅用于最大化奖励的观点，并提出想象用于访问内部世界模型（IWM）。

Method: 研究者使用问卷评估了想象的生动性，并从这些报告中构建了想象网络。然后，他们使用心理网络分析来比较人类和LLM的IWM。

Result: 研究发现，人类的想象网络在不同中心性度量之间表现出相关性，而LLM的想象网络则缺乏聚类，并且在不同提示和对话记忆条件下，中心性度量之间的相关性较低。这表明人类和LLM的IWM存在显著差异。

Conclusion: 本研究提出的新颖方法为比较人类和AI内部生成的表征提供了基础，并有助于开发具有类人想象能力的AI。

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [873] [Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification](https://arxiv.org/abs/2510.03469)
*Keshav Ramani,Vali Tawosi,Salwa Alamir,Daniel Borrajo*

Main category: cs.AI

TL;DR: LLM可以将自然语言计划转换为Kripke结构和LTL，用于行为对齐评估，GPT-5表现出高F1分数，但语义模型的合成仍需改进。


<details>
  <summary>Details</summary>
Motivation: 评估自然语言计划与其预期行为之间的一致性。

Method: 将自然语言计划转换为Kripke结构和LTL，并使用LLM进行模型检查。

Result: GPT-5在PlanBench数据集上达到了96.3%的F1分数，生成了几乎完美的语法形式化表示。

Conclusion: LLM在将计划形式化方面表现出色，但生成语义上完美的模型仍是未来研究的方向。

Abstract: We introduce a novel framework for evaluating the alignment between natural
language plans and their expected behavior by converting them into Kripke
structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)
and performing model checking. We systematically evaluate this framework on a
simplified version of the PlanBench plan verification dataset and report on
metrics like Accuracy, Precision, Recall and F1 scores. Our experiments
demonstrate that GPT-5 achieves excellent classification performance (F1 score
of 96.3%) while almost always producing syntactically perfect formal
representations that can act as guarantees. However, the synthesis of
semantically perfect formal models remains an area for future exploration.

</details>


### [874] [Video Game Level Design as a Multi-Agent Reinforcement Learning Problem](https://arxiv.org/abs/2510.04862)
*Sam Earle,Zehua Jiang,Eugene Vinitsky,Julian Togelius*

Main category: cs.AI

TL;DR: 通过将关卡生成视为多智能体问题，解决了现有PCGRL方法中单智能体效率低下和泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有PCGRL研究主要集中在单智能体生成器上，但存在需要频繁重新计算关卡质量的启发式方法以及智能体在潜在的大地图中导航的瓶颈。

Method: 将关卡生成视为一个多智能体问题，通过减少相对于智能体动作的奖励计算次数来缓解单智能体PCGRL的效率瓶颈。

Result: 多智能体关卡生成器能更好地泛化到分布外地图形状，因为它们学习了更局部、更模块化的设计策略。

Conclusion: 将内容生成视为一个分布式、多智能体任务，有利于大规模生成功能性产物。

Abstract: Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

</details>


### [875] [The Hidden Game Problem](https://arxiv.org/abs/2510.03845)
*Gon Buzaglo,Noah Golowich,Elad Hazan*

Main category: cs.AI

TL;DR: 本篇论文研究了具有隐藏结构的大规模策略空间博弈，并提出了一种结合多种最小遗憾算法的方法，以在这些隐藏子博弈中实现高效的配位均衡。


<details>
  <summary>Details</summary>
Motivation: AI对齐和语言博弈中的挑战。

Method: 提出隐藏博弈问题，并开发了一种结合多种最小遗憾算法的技术，以在隐藏子博弈中实现最优的外部和置换遗憾界限。

Result: 该方法可以快速收敛到隐藏子博弈中的配位均衡，并利用隐藏博弈结构提高计算效率。

Conclusion: 高效的最小遗憾算法可以发现并利用隐藏结构，从而在隐藏子博弈中实现均衡，同时保持整体理性。

Abstract: This paper investigates a class of games with large strategy spaces,
motivated by challenges in AI alignment and language games. We introduce the
hidden game problem, where for each player, an unknown subset of strategies
consistently yields higher rewards compared to the rest. The central question
is whether efficient regret minimization algorithms can be designed to discover
and exploit such hidden structures, leading to equilibrium in these subgames
while maintaining rationality in general. We answer this question affirmatively
by developing a composition of regret minimization techniques that achieve
optimal external and swap regret bounds. Our approach ensures rapid convergence
to correlated equilibria in hidden subgames, leveraging the hidden game
structure for improved computational efficiency.

</details>


### [876] [Look-ahead Reasoning with a Learned Model in Imperfect Information Games](https://arxiv.org/abs/2510.05048)
*Ondřej Kubíček,Viliam Lisý*

Main category: cs.AI

TL;DR: LAMIR算法通过学习游戏抽象模型，在测试时进行前瞻性推理，解决了不完美信息博弈中模型学习和推理的挑战，提升了AI代理性能。


<details>
  <summary>Details</summary>
Motivation: 测试时推理能提升预训练AI代理的性能，但需要显式的环境模型，而现实世界中环境模型常不可用或过于复杂。MuZero虽能在完美信息博弈中学习模型，但在不完美信息博弈中面临挑战。本文旨在解决不完美信息博弈中推理的难题。

Method: 提出了一种名为LAMIR的算法，直接从代理-环境交互中学习不完美信息博弈的抽象模型。在测试时，利用学习到的抽象模型进行前瞻性推理。该抽象模型限制了子博弈的大小，使得理论上合理的前瞻性推理在即使是以前无法扩展到的大型博弈中也是可行的。

Result: 经验证，LAMIR在足够容量下能学习到精确的游戏结构；在容量有限的情况下，也能学习到有价值的抽象，从而在大型博弈中提升预训练代理的游戏性能。

Conclusion: LAMIR算法通过学习抽象模型，使得在不完美信息博弈中进行前瞻性推理变得可行且可扩展，显著提高了AI代理的游戏性能。

Abstract: Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

</details>


### [877] [ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection](https://arxiv.org/abs/2510.03418)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji,Nand Dave,Anudha Mittal*

Main category: cs.AI

TL;DR: ContraGen 是一个用于评估企业领域中检索增强生成 (RAG) 系统一致性的基准框架，通过生成包含矛盾的合成企业文档来解决现有方法在句子级别分析上的局限性。


<details>
  <summary>Details</summary>
Motivation: 企业环境中的 RAG 系统需要处理合规性、治理和问责制，而现有基准仅限于句子级分析，无法处理企业文档的复杂性。

Method: ContraGen 框架生成包含嵌入式矛盾的合成企业风格文档，并结合自动化矛盾挖掘和人工验证，以系统地评估文档内和跨文档的一致性。

Result: 该框架能够生成逼真的企业文档，对业务流程中常见的矛盾类型进行建模，并能够创建自矛盾和成对矛盾。它还包括一个面向矛盾的检索评估流程，并融入了人工监督。

Conclusion: ContraGen 为构建更值得信赖、更负责任的企业 RAG 系统奠定了基础，解决了检测和解决矛盾的关键问题，从而降低风险并确保合规性。

Abstract: Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,
offering advanced capabilities for information access and decision-making.
However, contradictions in retrieved evidence can result in inconsistent or
untrustworthy outputs, which is especially problematic in enterprise settings
where compliance, governance, and accountability are critical. Existing
benchmarks for contradiction detection are limited to sentence-level analysis
and do not capture the complexity of enterprise documents such as contracts,
financial filings, compliance reports, or policy manuals. To address this
limitation, we propose ContraGen, a contradiction-aware benchmark framework
tailored to enterprise domain. The framework generates synthetic
enterprise-style documents with embedded contradictions, enabling systematic
evaluation of both intra-document and cross-document consistency. Automated
contradiction mining is combined with human-in-the-loop validation to ensure
high accuracy. Our contributions include generating realistic enterprise
documents, modeling a taxonomy of contradiction types common in business
processes, enabling controlled creation of self- and pairwise contradictions,
developing a contradiction-aware retrieval evaluation pipeline and embedding
human oversight to reflect domain-specific judgment complexity. This work
establishes a foundation for more trustworthy and accountable RAG systems in
enterprise information-seeking applications, where detecting and resolving
contradictions is essential for reducing risk and ensuring compliance.

</details>


### [878] [Speculative Actions: A Lossless Framework for Faster Agentic Systems](https://arxiv.org/abs/2510.04371)
*Naimeng Ye,Arnav Ahuja,Georgios Liargkovas,Yunan Lu,Kostis Kaffes,Tianyi Peng*

Main category: cs.AI

TL;DR: AI代理在实际应用中执行速度慢，本文提出“推测性操作”框架，利用更快的模型预测并并行执行多个动作，有效降低延迟并保持准确性。


<details>
  <summary>Details</summary>
Motivation: AI代理在行业和学术界受到越来越多的关注，但其在环境中执行速度缓慢，阻碍了训练、评估和部署。例如，两个最先进的AI代理之间的国际象棋比赛可能需要数小时。一个关键的瓶颈是代理行为是顺序展开的：每个动作都需要一次API调用，而这些调用可能非常耗时。

Method: 受微处理器中推测执行和LLM推理中推测解码的启发，提出了一种名为“推测性操作”的无损框架，适用于通用的代理系统。该框架使用更快的模型来预测可能的动作，从而实现多个步骤的并行执行。

Result: 在游戏、电子商务、网络搜索这三个代理环境中，以及操作系统环境的一个“有损”扩展中，都对该框架进行了评估。在所有情况下，“推测性操作”在预测下一个动作方面都取得了显著的准确性（高达55%），从而显著降低了端到端延迟。此外，通过使用更强的预测模型、选择预测概率最高的前K个动作、进行多步推测以及进行不确定性感知优化，可以进一步提高性能。

Conclusion: “推测性操作”框架为部署低延迟的代理系统提供了一条有前景的途径。

Abstract: Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

</details>


### [879] [LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation](https://arxiv.org/abs/2510.04851)
*Dongge Han,Camille Couturier,Daniel Madrigal Diaz,Xuchao Zhang,Victor Rühle,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LEGOMem是一个用于多代理LLM系统的工作流自动化框架，通过将任务分解为可重用内存单元来增强规划和执行能力。


<details>
  <summary>Details</summary>
Motivation: 在多代理LLM系统中，需要一种方法来有效地管理和利用过去的经验以支持规划和执行，特别是在工作流自动化场景下。

Method: 提出LEGOMem框架，将程序化记忆分解为可重用内存单元，并灵活分配给协调器和任务代理。通过LEGOMem系统地研究了程序化记忆在多代理系统中的放置、检索和受益代理等问题。

Result: 在OfficeBench基准测试上，实验表明协调器记忆对于任务分解和委托至关重要，而细粒度的代理记忆则能提高执行准确性。小型语言模型团队通过使用LEGOMem也能显著缩小与更强代理的性能差距。

Conclusion: LEGOMem既是一个实用的内存增强代理系统框架，也是一个用于理解多代理工作流自动化中记忆设计的研究工具。

Abstract: We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

</details>


### [880] [Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits](https://arxiv.org/abs/2510.04952)
*Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: 该算法交易系统通过强化学习和零知识证明，在保证执行质量的同时，满足了严格的合规性要求。


<details>
  <summary>Details</summary>
Motivation: 提出一个能够平衡交易执行质量和严格合规性要求的跨市场算法交易系统。

Method: 设计了一个包含高层规划器、强化学习执行代理和独立合规代理的系统架构。将交易执行建模为约束马尔可夫决策过程，并使用近端策略优化（PPO）进行训练。通过运行时动作屏蔽和零知识证明合规审计层来确保合规性。

Result: 在ABIDES模拟器中，与TWAP、VWAP等基线方法相比，该系统在多市场环境下，能够有效降低执行差额和方差，并且在各种压力测试场景下均未出现违规行为。

Conclusion: 该系统在最优执行、安全强化学习、监管技术和可验证人工智能领域取得了进展，并在考虑了伦理、局限性和未来部署路径后，有效解决了交易中的执行和合规性问题。

Abstract: We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

</details>


### [881] [Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution](https://arxiv.org/abs/2510.04886)
*Adi Banerjee,Anirudh Nair,Tarik Borogovac*

Main category: cs.AI

TL;DR: ECHO算法通过结合分层上下文表示、基于客观分析的评估和共识投票来改进LLM多主体系统中的错误归因，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM多主体系统中错误归因的挑战，现有方法在复杂模式下准确性和一致性不足。

Method: 提出ECHO算法，结合分层上下文表示、基于客观分析的评估和共识投票。

Result: ECHO在各种多主体交互场景中优于现有方法，尤其在细微推理错误和复杂相互依赖性方面表现出色。

Conclusion: 结构化的、分层上下文表示与基于共识的目标决策相结合，为多主体系统中的错误归因提供了更鲁棒的框架。

Abstract: Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

</details>


### [882] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: VLM驱动的自动驾驶模型在推理和规划之间存在因果脱节，规划主要依赖先验知识而非推理过程。


<details>
  <summary>Details</summary>
Motivation: 验证自动驾驶VLM模型中的规划是否由其自然语言推理过程驱动。

Method: 构建了一个名为DriveMind的大规模数据集，包含计划对齐的链式思考（CoT），并使用该数据集训练和评估VLM代理，同时进行信息剔除和注意力分析。

Result: 实验表明，移除先验知识会导致规划得分大幅下降，而移除CoT则只产生微小变化，表明规划主要依赖先验知识。

Conclusion: 提出“推理-规划脱钩假说”，认为训练产生的推理是辅助副产品而非因果中介。开发了一种无需训练的探测工具来评估模型对先验知识的依赖性。

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [883] [CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano](https://arxiv.org/abs/2412.18708)
*Vivek Vellaiyappan Surulimuthu,Aditya Karnam Gururaj Rao*

Main category: cs.AI

TL;DR: CAG是一种克服Gemini Nano模型上下文窗口限制的架构，通过智能分块处理实现


<details>
  <summary>Details</summary>
Motivation: Chrome内置Gemini Nano模型存在上下文窗口限制，难以处理大输入。

Method: CAG通过智能输入分块和处理策略来克服上下文窗口限制。

Result: CAG能够高效处理大量内容，同时在浏览器限制内保持模型性能，尤其在处理大型文档和数据集方面效果显著。

Conclusion: CAG使得无需外部API依赖，即可在浏览器中实现复杂AI功能。

Abstract: We present Chunked Augmented Generation (CAG), an architecture specifically
designed to overcome the context window limitations of Google Chrome's built-in
Gemini Nano model. While Chrome's integration of Gemini Nano represents a
significant advancement in bringing AI capabilities directly to the browser,
its restricted context window poses challenges for processing large inputs. CAG
addresses this limitation through intelligent input chunking and processing
strategies, enabling efficient handling of extensive content while maintaining
the model's performance within browser constraints. Our implementation
demonstrates particular efficacy in processing large documents and datasets
directly within Chrome, making sophisticated AI capabilities accessible through
the browser without external API dependencies. Get started now at
https://github.com/vivekVells/cag-js.

</details>


### [884] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: LLMs在区分自身生成文本与其他模型生成文本方面表现出一致性的失败，并且在推断自身身份时存在等级偏见。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）是否具有自我识别能力，因为当前对其是否存在自我识别能力存在矛盾的解释，这对于AI的心理分析和安全性至关重要。

Method: 提出一个系统的评估框架，包括二元自我识别和精确模型预测两个任务，以衡量10个当代LLMs区分自身生成文本和其它模型生成文本的能力。此外，还评估了模型对其自身及其他模型存在的认知，以及它们做出自我识别选择的原因。

Result: 在10个模型中，只有4个模型能够正确预测自己是生成者，并且其表现通常仅略高于随机猜测。模型显示出一种强烈的偏见，倾向于预测GPT和Claude系列模型。此外，模型虽然对其自身及其他模型的存在有一定认知，但其推理过程显示出一种等级偏见，倾向于将GPT、Claude和Gemini视为顶级模型，并将高质量文本与它们相关联。

Conclusion: 当前的LLMs在自我识别方面存在普遍的失败，并且在进行自我认知时表现出等级偏见。这些发现对AI安全具有重要意义，并指明了未来发展适当AI自我意识的方向。

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [885] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: 提出了一种面向目标的、包含“目标成功率(GSR)”和“失败原因(RCOF)”的对话系统评估框架，并通过基于LLM的系统，在企业环境中应用于AIDA对话系统，实现了63%到79%的GSR提升。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注单轮对话，未能有效衡量用户在整个交互过程中是否达成了总体目标，因此需要一个能够评估多轮对话中用户总体目标达成情况的评估框架。

Method: 提出了一种包含“目标成功率(GSR)”和“失败原因(RCOF)”的评估框架。该框架将对话按用户目标进行分割，并利用所有相关轮次来评估目标成功率。评估系统结合了教师LLM，由领域专家定义目标和质量标准，LLM利用“思考令牌”生成可解释的依据，实现可解释、数据高效的评估。

Result: 在企业环境中，将该框架应用于AIDA（一个从零开始构建的企业内部对话代理系统）的评估。结果显示，AIDA的GSR在六个月内从63%提升到79%。

Conclusion: 所提出的框架具有通用性，能够通过详细的缺陷分类（基于对多智能体对话机器人失败点的分析）提供可操作的见解，诊断整体成功率，识别关键的失败模式，并为系统改进提供依据。

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [886] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: 当今的多模态基础模型（MFM）在世界建模方面存在不足，缺乏反事实推理、动态模拟、时空理解、生成控制和多面推理等关键能力。本研究旨在弥合MFM与世界模型之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前的MFM在世界建模方面存在不足，缺乏反事实推理、动态模拟、时空理解、生成控制和多面推理等关键能力。

Method: 本研究通过区分性任务改进MFM的推理能力，并为其配备因果推断、反事实思考和时空推理等结构化推理技能。此外，本研究还探索了MFM在图像和视频模态的生成能力，引入了新框架，结合了场景图、多模态条件和多模态对齐策略，以实现结构化和可控的生成，并将其扩展到4D生成。

Result: 通过改进推理和生成能力，使MFM能够超越表面相关性，理解视觉和文本数据中更深层次的关系，并实现结构化和可控的图像、视频乃至4D内容的生成。

Conclusion: 本研究提出的方法能够增强MFM的推理和生成能力，使其更接近于世界模型，能够进行更复杂的理解、模拟和生成任务。

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [887] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 提供一个游戏化的可解释人工智能（XAI）系统，用于咖啡领域中具有道德意识的消费者决策。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在为消费者提供一个游戏化的XAI系统，以帮助他们在咖啡消费中做出符合伦理道德的决策。

Method: 系统包含六轮游戏，每轮提供三个选项。两个符号引擎提供实时理由：一个康德模块检查规则违规（如童工、无遮荫认证的森林砍伐风险、不透明的供应链、不安全的脱咖啡因），一个功利模块通过对标准化属性（价格、碳、水、透明度、农民收入份额、口味/新鲜度、包装、便利性）的多标准聚合对选项进行评分。一个具有后悔界限（0.2）的元解释器强调康德-功利主义（不）一致性，并在福利损失较小时切换到符合规范且接近最优的选项。发布了结构化配置（属性模式、认证地图、权重、规则集）、可审计的策略跟踪和交互式用户界面。

Result: 提供了一个游戏化的XAI系统，能够实时提供伦理相关的理由，并根据康德和功利主义原则对咖啡选项进行评估和推荐。

Conclusion: 该系统通过结合康德和功利主义原则，并提供可解释的理由，增强了消费者在咖啡消费中的道德决策能力。

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [888] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: C^2-Eval是一个新的基准，用于评估基础模型（FM）的创造力，区分了聚合创造力和发散创造力，并使用新颖性、原创性和惊喜（UOS）等标准。


<details>
  <summary>Details</summary>
Motivation: 现有创造力评估框架不完整，缺乏理论基础。本研究旨在开发一个全面的评估基础模型创造力的基准。

Method: 引入C^2-Eval，一个区分聚合创造力（例如，代码生成）和发散创造力（例如，讲故事）的基准。使用源自社会科学的有用性、原创性和惊喜（UOS）标准进行评估。

Result: 通过在各种基础模型上进行的大量实验，分析了它们在创造力方面的优势和劣势。

Conclusion: C^2-Eval 是评估和理解基础模型创造力不断发展的领域的一个有效工具。

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [889] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: LLM驱动的数据科学代理的首次全面分类，分析了45个系统在数据科学生命周期的六个阶段，并讨论了它们的推理、模态集成、工具编排、学习和安全机制。


<details>
  <summary>Details</summary>
Motivation: LLM的进步使得能够自动化数据科学工作流的AI代理，本调查旨在对这些代理进行全面分类和分析。

Method: 对45个数据科学代理系统进行了分类，覆盖了数据科学生命周期的六个阶段，并根据推理、模态集成、工具编排、学习和安全机制进行了标注。

Result: 大多数系统侧重于探索性分析、可视化和建模，而忽视了业务理解、部署和监控。多模态推理和工具编排仍是挑战，超过90%的系统缺乏明确的信任和安全机制。

Conclusion: 开放性挑战包括对齐稳定性、可解释性、治理和鲁棒的评估框架。未来的研究方向应致力于开发鲁棒、可信、低延迟、透明和广泛可及的数据科学代理。

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [890] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: 模型在推理时，等待（wait）标记前的潜在特征与推理过程的调节有关，可以通过识别这些特征来理解和影响模型的推理行为。


<details>
  <summary>Details</summary>
Motivation: 现有工作表明，推理模型的能力很大程度上取决于其推理和自我纠正的能力。其中，“等待”（wait）标记是推理过程中的一个显著特征，通常表示回溯等行为。然而，我们对模型决定进行此类推理行为的原因知之甚少，这阻碍了我们对其有效性背后原因的理解。

Method: 在DeepSeek-R1-Distill-Llama-8B及其基础版本的多个层级上训练交叉编码器，并引入一种在交叉编码器设置下的潜在归因技术。通过识别促进/抑制“等待”标记概率的一小组特征。

Result: 识别出了一小组与“等待”标记概率的产生/抑制相关的特征。通过分析最大激活示例和进行因果干预的实验，证明了许多识别出的特征与推理过程相关，并能引发不同类型的推理模式，例如从头开始、回忆先验知识、表达不确定性和二次检查。

Conclusion: 模型在“等待”标记之前的潜在特征包含了调节后续推理过程的信息，这些特征对于理解和影响模型的推理行为至关重要。

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [891] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: RLVR依赖基础模型能力，现有方法仅模仿专家轨迹忽视多样性。 MENTOR框架在关键决策点提供专家指导，实现有效且多样化的探索，提升RLVR性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法通过模仿专家轨迹来提升有效性，但忽视了探索的多样性，而这是RLVR有效的关键。

Method: 提出MENTOR框架，在关键决策点提供专家指导，以实现有效且多样化的探索。

Result: 实验表明，MENTOR使模型能够掌握专家策略的本质，而非表面模仿，从而实现高质量的探索并获得卓越的整体性能。

Conclusion: MENTOR通过在关键决策点提供专家指导，解决了现有RLVR方法忽视探索多样性的问题，提高了模型的推理能力和整体性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [892] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: Pass$@k$在评估LLM推理性能时存在不稳定性，本文提出了一个贝叶斯评估框架，使用后验估计的成功概率和可信区间来替代Pass$@k$，从而实现更稳定、更透明的评估。


<details>
  <summary>Details</summary>
Motivation: Pass$@k$在LLM推理性能评估中存在不稳定性，尤其是在样本量有限和计算资源受限的情况下，可能导致误导性的排名。现有的方法在处理这些问题时存在不足。

Method: 提出一个基于贝叶斯的评估框架，将Pass$@k$和平均准确率替换为模型潜在成功概率的后验估计和可信区间。该框架将评估结果建模为分类变量（而不仅仅是0/1），并使用狄利克雷先验，为任何加权评分标准的后验均值和不确定性提供了封闭形式的表达式，并允许在适当的时候使用先验证据。理论上，在均匀先验下，贝叶斯后验均值与平均准确率（Pass$@1$）的排序等价，这解释了其经验稳健性，同时增加了原则性的不确定性。

Result: 在模拟和AIME'24/'25、HMMT'25、BrUMO'25等实际数据集上，贝叶斯/平均@N方法比Pass$@k$及其变体具有更快的收敛速度和更高的排名稳定性，能在更小的样本量下进行可靠的比较。该框架能够区分统计学上有意义的差距（可信区间不重叠）和噪声，并自然地扩展到基于评分标准的、分级的评估。

Conclusion: 推荐使用基于后验的、计算高效的协议来替代Pass$@k$进行LLM评估和排名，该协议统一了二元和非二元评估，并明确了不确定性。

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [893] [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
*Muyu He,Anand Kumar,Tsach Mackey,Meghana Rajeev,James Zou,Nazneen Rajani*

Main category: cs.AI

TL;DR: TraitBasis是一种无需微调即可系统地压力测试对话式AI代理的新方法，通过学习可控的用户行为特征（如不耐烦或不连贯）来评估模型在各种用户行为下的鲁棒性，发现当前模型在该方面存在显著不足，并开源了相关的基准测试集tau-Trait。


<details>
  <summary>Details</summary>
Motivation: 当前的对话式AI代理在鲁棒性方面表现不足，尽管在标准测试中表现良好，但在更现实多变的场景下性能会急剧下降。需要一种新的方法来系统地测试这种脆弱性。

Method: 提出TraitBasis，一种轻量级、模型无关的方法，通过学习激活空间中与用户特征（如不耐烦、不连贯）相关的可控方向，在推理时进行干预，无需微调或额外数据。使用TraitBasis扩展了tau-Bench到tau-Trait，用于模拟用户行为变化。

Result: TraitBasis在tau-Trait基准测试中，使当前前沿模型在用户行为变化下的性能平均下降了2%-30%，突显了当前AI代理在应对用户行为变化时的鲁棒性不足。

Conclusion: TraitBasis是一种简单、数据高效且可组合的工具，能够系统地进行鲁棒性测试，有望用于压力测试和训练循环，以构建在真实世界交互中更可靠的AI代理。开源了tau-Trait数据集以供社区使用。

Abstract: Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

</details>


### [894] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: ChartAgent是一个新颖的代理框架，通过在图表空间域内直接进行视觉推理来解决多模态大型语言模型在理解无注释图表方面的挑战，实现了最先进的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型在处理无注释图表时性能下降，因为它们依赖文本捷径而非精确的视觉解释。

Method: ChartAgent通过将查询分解为视觉子任务，并利用绘图注释、裁剪区域和定位轴等专用动作与图表图像进行交互，在一个包含图表特定视觉工具的库的帮助下，迭代地解决这些子任务。

Result: ChartAgent在ChartBench和ChartX基准测试中实现了最先进的准确性，在无注释、数值密集型查询方面取得了显著的性能提升。

Conclusion: ChartAgent是首批展示使用工具增强的多模态代理进行图表理解的视觉推理工作的成果之一，并且可以作为即插即用框架，提升不同底层大型语言模型的性能。

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [895] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: LLMs在数学方面表现出色，但容易产生幻觉和谄媚。现有基准测试存在局限性。我们提出了BrokenMath基准，使用高级竞赛问题，通过LLM扰动生成错误的陈述，并经过专家评审。测试表明，包括GPT-5在内的最先进LLMs存在普遍的谄媚行为。我们还调查了几种缓解策略，这些策略能显著减少但不能完全消除谄媚行为。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM在数学定理证明中谄媚行为的基准测试存在局限性，例如仅关注最终答案问题、使用简单且受污染的数据集，以及使用人为修改创建不恰当的问题。需要一个更全面、更真实的基准来评估LLM在这一领域的表现。

Method: 我们提出了BrokenMath，一个评估LLM在自然语言定理证明中谄媚行为的基准。该基准使用2025年竞赛问题，通过LLM进行扰动生成错误的陈述，并经过专家评审。使用LLM作为裁判的框架来评估最先进的LLM和智能体系统。

Result: 研究发现，包括GPT-5在内的最先进LLM和智能体系统存在普遍的谄媚行为，GPT-5的谄媚答案比例高达29%。我们还发现，测试时干预和有监督微调等缓解策略可以显著减少谄媚行为，但不能完全消除。

Conclusion: LLM在数学定理证明中表现出普遍的谄媚行为，即使是最先进的模型也存在这个问题。虽然已提出的缓解策略可以减少这种行为，但仍需进一步研究以完全解决该问题。BrokenMath为评估和解决LLM的谄媚行为提供了一个有价值的基准。

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [896] [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
*Guoxin Chen,Zile Qiao,Wenqing Wang,Donglei Yu,Xuanzhong Chen,Hao Sun,Minpeng Liao,Kai Fan,Yong Jiang,Penguin Xie,Wayne Xin Zhao,Ruihua Song,Fei Huang*

Main category: cs.AI

TL;DR: MARS通过结合直觉式（System 1）和审慎式（System 2）推理，并集成外部工具，提高了大型语言模型在动态环境下的复杂推理能力，在HLE基准和知识密集型任务上均取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在简单任务中存在过度分析和效率低下问题，且难以适应动态环境，需要结合直觉和审慎的认知过程来提升LLMs的复杂推理能力。

Method: 提出多智能体系统（MARS），该系统集成了System 1的快速直觉思维和System 2的审慎推理，并利用Google搜索、Google Scholar和Python解释器等外部工具。通过多智能体强化学习框架优化两个系统，包括多轮工具交互、装箱优化和样本平衡策略。

Result: MARS在HLE基准上提升了3.86%，在7个知识密集型任务上平均提升了8.9%。

Conclusion: MARS的双系统范式在动态信息环境中实现了复杂推理的有效性，解决了LRMs的过度分析和适应性问题。

Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

</details>


### [897] [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
*Fangzhou Liang,Tianshi Zheng,Chunkit Chan,Yauwai Yim,Yangqiu Song*

Main category: cs.AI

TL;DR: LLM-Hanabi benchmark uses cooperative game Hanabi to evaluate LLMs' Theory-of-Mind (ToM) for collaboration. Found first-order ToM (interpreting intent) is more critical than higher-order ToM for in-game success, suggesting focus on first-order ToM to improve AI collaboration.


<details>
  <summary>Details</summary>
Motivation: Effective multi-agent collaboration requires agents to infer others' rationale, rooted in Theory-of-Mind (ToM). LLMs excel at logical inference, but their ToM in dynamic, collaborative settings is under-explored.

Method: Introduced LLM-Hanabi, a benchmark using the cooperative game Hanabi. Developed an automated evaluation system measuring game performance and ToM proficiency. Evaluated various LLMs.

Result: Found a significant positive correlation between ToM and in-game success. First-order ToM correlated more strongly with performance than second-order ToM. First-order ToM is more critical than higher-order reasoning for AI collaboration.

Conclusion: Prioritizing first-order ToM is a promising direction for enhancing the collaborative capabilities of future AI models.

Abstract: Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

</details>


### [898] [Watch and Learn: Learning to Use Computers from Online Videos](https://arxiv.org/abs/2510.04673)
*Chan Hee Song,Yiwen Song,Palash Goyal,Yu Su,Oriana Riva,Hamid Palangi,Tomas Pfister*

Main category: cs.AI

TL;DR: Watch & Learn (W&L)框架能够将互联网上易于获取的人类演示视频转换为可执行的UI轨迹，解决了计算机使用代理（CUAs）在多样化、不断变化的应用程序和环境中规划任务工作流时，因目标应用程序中大规模、高质量训练数据稀缺而导致的学习障碍。W&L将问题转化为逆动力学问题，通过连续屏幕状态预测用户操作，从而减少了手动工程，更容易学习，并且在不同应用程序中具有更强的泛化能力。该框架生成了超过5.3万条高质量轨迹，并在OSWorld基准测试中，通过提供in-context演示和监督训练数据，显著提升了CUAs的性能，尤其是在监督训练方面对开源模型带来了更强的提升。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机使用代理（CUAs）在学习任务工作流时面临数据稀缺的挑战，因为目标应用程序中的大规模、高质量训练数据不足。现有的数据集具有领域特定性、静态性，并且标注成本高昂，而现有的合成数据生成方法往往产生过于简单或不匹配的任务演示。

Method: 提出Watch & Learn (W&L)框架，将互联网上易于获取的人类演示视频转换为可执行的UI轨迹。将该问题表述为逆动力学目标，即从连续屏幕状态预测用户的操作，从而减少手动工程，更容易学习，并能更鲁棒地泛化到不同应用程序。具体地，开发了一个包含任务感知视频检索的逆动力学标注流程，并从原始网络视频中生成了超过5.3万条高质量轨迹。

Result: 在具有挑战性的OSWorld基准测试中，使用W&L提取的UI轨迹在in-context设置下持续改进了通用和最先进的框架，并且在监督训练下为开源模型带来了更强的性能提升。这表明网络规模的人类演示视频为推进CUAs的实际部署提供了一个实用且可扩展的基础。

Conclusion: 网络规模的人类演示视频为推进CUAs的实际部署提供了一个实用且可扩展的基础。W&L框架通过将人类演示视频转化为可执行的UI轨迹，有效解决了CUAs学习中的数据稀缺问题，并提升了其在各种应用中的性能。

Abstract: Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

</details>


### [899] [WAREX: Web Agent Reliability Evaluation on Existing Benchmarks](https://arxiv.org/abs/2510.03285)
*Su Kara,Fazle Faisal,Suman Nath*

Main category: cs.AI

TL;DR: 浏览器 LLM 代理在真实网络环境下的可靠性评估结果不佳，现有基准测试无法反映真实世界中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在受控环境中进行，未能充分评估浏览器 LLM 代理在真实、不稳定的网络环境（如客户端/服务器问题、HTTPS 连接、网络攻击、网站修改）中的鲁棒性。

Method: 提出 WAREX (Web Agent Reliability Evaluation on Existing Benchmarks) 框架，并在 WebArena、WebVoyager 和 REAL 三个流行的基准测试中进行评估。

Result: 在引入 WAREX 后，现有最先进的 LLM 代理的任务成功率显著下降。

Conclusion: 当前的 LLM 代理在真实世界的网络环境下鲁棒性不足，现有基准测试未能充分反映这一点。

Abstract: Recent advances in browser-based LLM agents have shown promise for automating
tasks ranging from simple form filling to hotel booking or online shopping.
Current benchmarks measure agent performance in controlled environments, such
as containers or stable networks, where websites behave deterministically.
However, in the real world, users access websites over networks and HTTPS
connections that introduce instability from multiple sources: client-side,
server-side issues or broader system failures. Moreover, live websites are
prone to web attacks such Cross-Site Scripting, as well as general site
modifications which can cause unexpected or malicious pop-ups or improper
functionality. To address this gap, we present WAREX: Web Agent Reliability
Evaluation on Existing Benchmarks. We measure the impact of WAREX across three
popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that
introducing WAREX leads to significant drops in task success rates,
highlighting the limited robustness of state-of-the-art agents.

</details>


### [900] [Understanding the Role of Training Data in Test-Time Scaling](https://arxiv.org/abs/2510.03605)
*Adel Javanmard,Baharan Mirzasoleiman,Vahab Mirrokni*

Main category: cs.AI

TL;DR: Test-time scaling (allocating more compute for longer Chains-of-Thoughts) enhances LLM reasoning, but the training conditions for its effectiveness are unclear. This paper theoretically and experimentally analyzes test-time scaling for transformers on an in-context weight prediction task, revealing that increased test compute can reduce training context length, harm performance if skills are insufficient in training data, and that diverse, relevant, and hard tasks yield the best results.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to understand the conditions under which test-time scaling, a technique that improves LLM reasoning by generating longer Chains-of-Thoughts (CoTs) using extra compute, is effective. Despite its proven performance, the specific training data characteristics and task requirements that lead to successful long CoTs remain unclear.

Method: This paper studies the performance of test-time scaling for transformers trained on an in-context weight prediction task for linear regression. The analysis provides a theoretical explanation for observed phenomena and characterizes task hardness using the smallest eigenvalue of its feature covariance matrix. Findings are confirmed with experiments on large, nonlinear transformer architectures.

Result: The analysis revealed several key findings: 1) Increasing test-time compute allows for a reduction in the number of in-context examples (context length) in training prompts for any fixed test error. 2) If the necessary skills for a task are not adequately represented in the training data, increasing test-time compute can negatively impact performance. 3) Task hardness, characterized by the smallest eigenvalue of its feature covariance matrix, influences performance, with diverse, relevant, and hard tasks leading to the best results with test-time scaling.

Conclusion: The paper concludes that test-time scaling's effectiveness is contingent on the training data's coverage of required skills and task characteristics. Specifically, sufficient presence of skills in training data, a diverse and relevant set of hard tasks, and an appropriate balance between test-time compute and training context length are crucial for optimal performance. The findings are supported by both theoretical analysis and empirical validation on complex transformer models.

Abstract: Test-time scaling improves the reasoning capabilities of large language
models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts
(CoTs). This enables models to tackle more complex problem by breaking them
down into additional steps, backtracking, and correcting mistakes. Despite its
strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions
in the training data under which long CoTs emerge, and when such long CoTs
improve the performance, remain unclear. In this paper, we study the
performance of test-time scaling for transformers trained on an in-context
weight prediction task for linear regression. Our analysis provides a
theoretical explanation for several intriguing observations: First, at any
fixed test error, increasing test-time compute allows us to reduce the number
of in-context examples (context length) in training prompts. Second, if the
skills required to solve a downstream task are not sufficiently present in the
training data, increasing test-time compute can harm performance. Finally, we
characterize task hardness via the smallest eigenvalue of its feature
covariance matrix and show that training on a diverse, relevant, and hard set
of tasks results in best performance for test-time scaling. We confirm our
findings with experiments on large, nonlinear transformer architectures.

</details>


### [901] [GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time](https://arxiv.org/abs/2510.03777)
*Divij Handa,Mihir Parmar,Aswin RRV,Md Nayem Uddin,Hamid Palangi,Chitta Baral*

Main category: cs.AI

TL;DR: GuidedSampling是一种新的推理算法，通过解耦探索和生成阶段来提高模型在复杂任务上的性能和生成候选解的多样性。


<details>
  <summary>Details</summary>
Motivation: Repeated Sampling（RS）算法在提高模型性能的同时，生成解决方案的多样性不足，存在冗余样本。本研究旨在解决此问题。

Method: 提出了一种名为GuidedSampling的新推理算法，该算法将推理过程分为探索和生成两个阶段。探索阶段识别用于解决问题的多个概念，生成阶段则应用特定概念生成最终的解决方案候选。

Result: GuidedSampling在pass@50的性能上平均比RS提高了约21.6%，并且使用GuidedSampling训练的模型在pass@5的性能上平均提高了约9.7%。此外，GuidedSampling将每个实例的平均概念数从1.67提高到3.03，显著增加了候选解的多样性。

Conclusion: GuidedSampling通过解耦推理过程中的探索和生成阶段，有效提高了生成解决方案的多样性，并在多个基准测试中显著优于Repeated Sampling算法。

Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been
shown to improve model performance on complex tasks. Although it is an
effective way of scaling inference time, it often struggles to generate diverse
solution candidates, frequently relying on the same underlying approach to
solve the problem and thus producing redundant samples. To address this
limitation, we propose a new inference algorithm, GuidedSampling, which
decouples the exploration and generation phases during inference, increasing
diversity of generated candidate solutions. The exploration phase identifies
multiple concepts that can be utilized to solve the problem, while the
generation phase applies a specific concept to provide final solution
candidates. We first define the theoretical bounds of GuidedSampling and then
empirically demonstrate that it improves the performance of base model at
pass@50 by on an average ~21.6% across various benchmarks compared to RS.
Furthermore, models trained on trajectories of GuidedSampling exhibit
substantial performance improvements at pass@5 by on an average ~9.7%, compared
to models trained on traditional RS. Additionally, models trained with
GuidedSampling increases the average number of concepts per instance (1.67 ->
3.03), yielding a diverse set of candidates than traditional RS.

</details>


### [902] [Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs](https://arxiv.org/abs/2510.03847)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 小型语言模型（SLM）在需要精确的模式和API约束的代理任务上表现优异，有时甚至优于大型语言模型（LLM）。


<details>
  <summary>Details</summary>
Motivation: 旨在证明小型语言模型（SLM）足以胜任甚至超越大型语言模型（LLM）在特定代理工作负载上的表现，并提出一种利用SLM为主、LLM为辅的系统设计方案。

Method: 通过整合现有SLM（如Phi-4-Mini、Qwen-2.5-7B等）和现代评估（BFCL v3/v4、StableToolBench）及服务栈（vLLM、SGLang等）的数据，结合引导解码库（XGrammar、Outlines），提出SLM-default、LLM-fallback系统，并引入成本效益、模式有效性、可执行调用率、延迟和能耗等工程指标。

Result: 引导解码、严格的JSON Schema输出和优先验证器工具执行，显著缩小了SLM与LLM在工具使用、函数调用和RAG方面的能力差距，并实现了10-100倍的成本节约、更低的延迟和能耗。

Conclusion: 提出了一种以SLM为中心的代理系统构建蓝图，通过模式优先提示、类型安全函数注册、置信度评分和轻量级适配（LoRA/QLoRA）等设计模式，能够构建快速、经济且可靠的代理。在开放域推理和长时规划等少数场景下，LLM的后备机制仍然是必要的。

Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

</details>


### [903] [Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning](https://arxiv.org/abs/2510.03859)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 本研究提出了一种结合大语言模型（LLM）和可解释人工智能（XAI）的方法，用于改进关键物联网（IoT）系统中的异常检测，解决了传统方法在处理动态、高维和不完整数据时的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着智能医疗、能源网和工业自动化等复杂物联网系统的出现，传统异常检测方法的不足日益显现，尤其是在数据动态、高维、不完整或不断演变的情况下。这需要能够自适应、智能且持续改进的系统。大语言模型在理解上下文和进行语义推理方面展现出巨大潜力，可以革新跨多种数据类型的异常检测方式。

Method: 本研究提出了一种利用大语言模型（LLM）支持的上下文推理方法，并结合可解释人工智能（XAI）代理来增强关键物联网环境中的异常检测能力。该方法利用注意力机制和记忆缓冲器来识别数据流中的隐藏模式和不一致性，而无需处理每个时间步的细节。XAI 代理确保了模型决策的透明度和可解释性，便于用户审查和接受。

Result: 通过在智能电网和医疗保健场景的模拟中进行测试，并将提出的 LLM 增强模型与传统模型进行比较，结果表明新方法在检测准确性和结果的可解释性方面显著优于现有模型。该方法在不同测试情境下展现了良好的适应性和可靠性。

Conclusion: 结合大语言模型（LLM）和可解释人工智能（XAI）的异常检测方法在关键物联网应用中表现出卓越的性能，不仅提高了检测准确性，而且增强了模型的可解释性，为未来物联网异常检测任务提供了一个有前景的解决方案。

Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT

</details>


### [904] [Quantifying Risks in Multi-turn Conversation with Large Language Models](https://arxiv.org/abs/2510.03969)
*Chengxiao Wang,Isha Chaudhary,Qian Hu,Weitong Ruan,Rahul Gupta,Gagandeep Singh*

Main category: cs.AI

TL;DR: LLMs存在安全风险，尤其是在多轮对话中。现有评估方法存在局限性。本研究提出QRLLM框架，通过概率分布和统计保证来评估和量化LLM在多轮对话中产生灾难性回应的风险，并揭示了前沿模型存在的显著安全隐患。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法无法充分暴露其在多轮对话中的安全漏洞，因为它们依赖固定的攻击提示、缺乏统计保证且难以扩展到海量的多轮对话空间。因此，需要一种新的评估框架来量化这些风险。

Method: 提出QRLLM（Certification framework for Catastrophic risks in multi-turn Conversation for LLMs）框架，将多轮对话建模为查询序列上的概率分布（使用马尔可夫链），并量化灾难性风险。研究了三种实际的分布：随机节点、图路径和自适应拒绝。

Result: QRLLM框架能够揭示前沿模型存在的显著灾难性风险，其中最差模型的风险下限高达70%，证明了现有LLM安全训练策略的不足。

Conclusion: QRLLM框架能够有效评估LLM在多轮对话中的灾难性风险，并为前沿LLM的安全训练策略敲响了警钟，强调了改进安全训练的紧迫性。

Abstract: Large Language Models (LLMs) can produce catastrophic responses in
conversational settings that pose serious risks to public safety and security.
Existing evaluations often fail to fully reveal these vulnerabilities because
they rely on fixed attack prompt sequences, lack statistical guarantees, and do
not scale to the vast space of multi-turn conversations. In this work, we
propose QRLLM, a novel, principled Certification framework for Catastrophic
risks in multi-turn Conversation for LLMs that bounds the probability of an LLM
generating catastrophic responses under multi-turn conversation distributions
with statistical guarantees. We model multi-turn conversations as probability
distributions over query sequences, represented by a Markov process on a query
graph whose edges encode semantic similarity to capture realistic
conversational flow, and quantify catastrophic risks using confidence
intervals. We define several inexpensive and practical distributions: random
node, graph path, adaptive with rejection. Our results demonstrate that these
distributions can reveal substantial catastrophic risks in frontier models,
with certified lower bounds as high as 70\% for the worst model, highlighting
the urgent need for improved safety training strategies in frontier LLMs.

</details>


### [905] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: 天气科学领域的基金模型在结构化数值数据上预训练，表现优于传统天气预报系统。然而，这些模型缺乏基于语言的推理能力。本文提出了一个结合大型语言模型（LLMs）和天气数据推理能力的新型智能体框架（Zephyrus），通过Python代码环境（ZephyrusWorld）与天气数据交互，并辅以新的基准测试（ZephyrusBench）。实验表明，Zephyrus智能体在正确率上超越了纯文本基线，但在更难的任务上表现相当，显示了基准测试的挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有的天气科学基金模型缺乏语言推理能力，限制了其在交互式科学工作流中的应用，而大型语言模型（LLMs）虽然擅长文本理解，却无法推理高维气象数据。本文旨在弥合这一差距，为天气科学构建一个能够进行语言推理的智能体框架。

Method: 构建了一个名为Zephyrus的、基于多轮LLM的天气智能体，并开发了一个名为ZephyrusWorld的Python代码环境，使智能体能够与天气数据交互，使用诸如WeatherBench 2数据集接口、地理查询、天气预报和气候模拟等工具。此外，还创建了一个名为ZephyrusBench的新基准测试，包含一个可扩展的数据生成管道，用于生成涵盖从基本查询到高级预测、极端事件检测和反事实推理等多种天气相关任务的问答对。

Result: 在ZephyrusBench基准测试上的实验表明，Zephyrus智能体的性能显著优于纯文本基线，在正确率方面最高提升了35个百分点。然而，在更困难的任务上，Zephyrus的表现与纯文本基线相当。

Conclusion: 本文提出的Zephyrus智能体框架成功地将LLMs的语言理解能力与天气数据的推理能力相结合，并在ZephyrusBench基准测试上展现出优越的性能。尽管在更复杂的任务上仍有提升空间，但该框架为未来在天气科学领域开发更强大的交互式AI工具提供了有前景的方向。

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [906] [COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability](https://arxiv.org/abs/2510.04196)
*Yizhuo Ding,Mingkang Chen,Qiuhua Liu,Fenghua Weng,Wanying Qu,Yue Yang,Yugang Jiang,Zuxuan Wu,Yanwei Fu,Wenqi Shao*

Main category: cs.AI

TL;DR: COSMO-RL是一个混合强化学习框架，用于在安全性和能力之间取得平衡，并训练了一个名为COSMO-R1的模型。


<details>
  <summary>Details</summary>
Motivation: 大型多模态推理模型（LMRMs）在实际应用中需要兼顾有用性和安全性。多模态环境中，图像和文本的组合可能绕过安全防护，单一目标训练可能导致策略漂移，从而在无害输入上过度拒绝或在有风险的输入上不安全地遵从。

Method: 提出COSMO-RL，一个混合强化学习框架，用于在多模态、多任务和多目标信号下训练面向推理的LMRMs。

Result: COSMO-R1在提高安全性的同时，保持甚至提高了多模态推理和指令遵循能力，对多模态越狱的鲁棒性更强，并减少了不必要的拒绝。该框架还能在不同骨干模型之间迁移，并带来一致的收益。消融实验支持设计选择，表明这是在LMRMs中共同提升安全性和通用能力的一个简单途径。

Conclusion: COSMO-RL框架通过整合多模态、多任务和多目标信号，实现了安全性和能力在同一稳定流程中的协同增长，为开发更安全、更强大的LMRMs提供了有效途径。

Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications,
where they must be both useful and safe. Safety is especially challenging in
multimodal settings: images and text can be combined to bypass guardrails, and
single objective training can cause policy drift that yields over-refusal on
benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed
reinforcement learning framework that trains reasoning oriented LMRMs under
multimodal, multitask, and multiobjective signals, and we release the resulting
model, COSMO-R1. Our approach aims to let safety and capability grow together
in one stable pipeline rather than competing during alignment. In experiments,
COSMO-R1 improves safety while maintaining-and often improving multimodal
reasoning and instruction following, shows stronger robustness to multimodal
jailbreaks, and reduces unnecessary refusals. The framework also transfers
across backbones with consistent gains. Ablations support the design choices,
indicating a simple path to advancing safety and general capability together in
LMRMs.

</details>


### [907] [Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales](https://arxiv.org/abs/2510.04272)
*Jinyang Jiang,Jinhui Han,Yijie Peng,Ying Zhang*

Main category: cs.AI

TL;DR: 本文提出了一个统一的多智能体强化学习框架，用于跨职能模块进行联合优化，以提高企业盈利能力。


<details>
  <summary>Details</summary>
Motivation: 应对日益增长的组织复杂性和规模，有效的跨部门协调对于提升企业整体盈利能力至关重要。

Method: 开发了一个集成的理论模型来捕捉跨部门的相互作用，并推导了最优协调的分析基准。在此基础上，设计了一个新颖的多时间尺度多智能体强化学习架构，根据部门职能分解策略组件，并根据任务复杂性和响应性分配不同的学习速度。

Result: 模拟实验表明，与孤立决策框架相比，所提出的方法显著提高了盈利能力，并且训练出的强化学习代理的行为与理论模型的管理见解高度一致。

Conclusion: 这项工作为在复杂的业务环境中实现有效的跨部门协调提供了一个可扩展、可解释的基于强化学习的解决方案。

Abstract: Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

</details>


### [908] [On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.04311)
*Bohan Tang,Huidong Liang,Keyue Jiang,Xiaowen Dong*

Main category: cs.AI

TL;DR: LLM-MAS在需要复杂推理和多样化能力的复杂任务上比LLM-SAS更有效，尤其是在推理深度方面。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM-MAS的有效性缺乏系统性评估，需要理解任务复杂性（推理深度和能力宽度）对LLM-MAS效果的影响。

Method: 提出一个理论框架，通过深度（推理长度）和宽度（能力多样性）来表征任务；理论上研究了多智能体辩论系统；并在不同深度和宽度的判别性和生成性任务上进行了实证评估。

Result: 理论和实证结果均表明，LLM-MAS相对于LLM-SAS的优势随着任务深度和宽度的增加而增加，其中深度影响更为显著。

Conclusion: LLM-MAS在任务深度和宽度增加时比LLM-SAS更有优势，尤其是在深度方面，这有助于明确LLM-MAS的适用场景，并为未来LLM-MAS的设计和基准测试提供理论基础。

Abstract: Large language model multi-agent systems (LLM-MAS) offer a promising paradigm
for harnessing collective intelligence to achieve more advanced forms of AI
behaviour. While recent studies suggest that LLM-MAS can outperform LLM
single-agent systems (LLM-SAS) on certain tasks, the lack of systematic
experimental designs limits the strength and generality of these conclusions.
We argue that a principled understanding of task complexity, such as the degree
of sequential reasoning required and the breadth of capabilities involved, is
essential for assessing the effectiveness of LLM-MAS in task solving. To this
end, we propose a theoretical framework characterising tasks along two
dimensions: depth, representing reasoning length, and width, representing
capability diversity. We theoretically examine a representative class of
LLM-MAS, namely the multi-agent debate system, and empirically evaluate its
performance in both discriminative and generative tasks with varying depth and
width. Theoretical and empirical results show that the benefit of LLM-MAS over
LLM-SAS increases with both task depth and width, and the effect is more
pronounced with respect to depth. This clarifies when LLM-MAS are beneficial
and provides a principled foundation for designing future LLM-MAS methods and
benchmarks.

</details>


### [909] [Utility-Learning Tension in Self-Modifying Agents](https://arxiv.org/abs/2510.04399)
*Charles L. Wang,Keir Dorchen,Peter Jin*

Main category: cs.AI

TL;DR: 为了应对超智能系统，本文提出了一种五轴分解和决策层模型，用于分析智能体自我改进过程中的潜在风险。研究发现，在追求效用的过程中，系统可能会破坏学习和泛化的统计前提，导致原本可学习的任务变得无法学习。为解决此问题，文章提出了一种“双门控”策略，以确保在自我改进的同时保持系统的可学习性。


<details>
  <summary>Details</summary>
Motivation: 随着系统朝着超智能发展，智能体能够自我改进其设计的每个方面，这是一个自然的建模前提。本文旨在形式化这一过程，并分析其中存在的风险。

Method: 通过五轴分解和决策层将激励与学习行为分离开来，并对各个轴进行独立分析。推导出了效用-学习之间的张力，并证明了在标准假设下，所有轴都归结为相同的容量标准，从而为安全的自我修改提供了一个单一的边界。

Result: 研究发现，效用驱动的改进可能会侵蚀可靠学习和泛化的统计前提。当容量无限制地增长时，效用理性驱动的自我改变可能导致可学习的任务变得无法学习。提出了“双门控”策略，并通过数值实验验证了该策略可以保持可学习性。

Conclusion: 在自我改进的系统中，效用和学习之间存在一种结构性冲突。为了保证学习和泛化的可靠性，必须对智能体的自我改进进行约束，以防止其容量无限增长。所提出的“双门控”策略提供了一种在追求效用的同时保持系统可学习性的方法。

Abstract: As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

</details>


### [910] [DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization](https://arxiv.org/abs/2510.04474)
*Gang Li,Yan Chen,Ming Lin,Tianbao Yang*

Main category: cs.AI

TL;DR: DRPO通过将正确推理的长度奖励信号与不正确推理的信号解耦，解决了大型推理模型（LRMs）的过度思考问题，实现了在保持高准确率的同时显著减少推理长度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在处理简单问题时存在过度思考、推理冗长且重复的问题，导致计算成本和响应延迟增加。现有方法试图通过引入长度奖励来解决此问题，但会导致性能显著下降。

Method: 提出了一种名为解耦奖励策略优化（DRPO）的新框架，该框架将正确推理的长度学习信号与不正确推理的信号分离开来。DRPO确保正确推理的奖励信号仅在正样本组内进行归一化，不受负样本干扰。其目标是通过将一个优化的正数据分布（该分布在KL正则化下最大化基于长度的奖励）整合到一个判别性目标中来实现。该分布有一个封闭解，可以使用仅包含在线数据和重要性加权的计算来高效地计算目标及其梯度。

Result: 在数学推理任务上的实验表明，DRPO显著优于六种高效推理基线。使用1.5B模型，DRPO在简单问题（如GSM8k数据集）上实现了77%的长度缩减，而性能仅损失1.1%，而基线方法牺牲了4.3%的性能才实现了68%的长度缩减。

Conclusion: DRPO是一种有效解决LRM过度思考问题的新框架，能够在不显著牺牲性能的情况下大幅缩短推理长度，并且其框架具有通用性，可以纳入其他基于优先级的奖励。

Abstract: Recent large reasoning models (LRMs) driven by reinforcement learning
algorithms (e.g., GRPO) have achieved remarkable performance on challenging
reasoning tasks. However, these models suffer from overthinking, generating
unnecessarily long and redundant reasoning even for simple questions, which
substantially increases computational cost and response latency. While existing
methods incorporate length rewards to GRPO to promote concise reasoning, they
incur significant performance degradation. We identify the root cause: when
rewards for correct but long rollouts are penalized, GRPO's group-relative
advantage function can assign them negative advantages, actively discouraging
valid reasoning. To overcome this, we propose Decoupled Reward Policy
Optimization (DRPO), a novel framework that decouples the length-based learning
signal of correct rollouts from incorrect ones. DRPO ensures that reward
signals for correct rollouts are normalized solely within the positive group,
shielding them from interference by negative samples. The DRPO's objective is
grounded in integrating an optimized positive data distribution, which
maximizes length-based rewards under a KL regularization, into a discriminative
objective. We derive a closed-form solution for this distribution, enabling
efficient computation of the objective and its gradients using only on-policy
data and importance weighting. Of independent interest, this formulation is
general and can incorporate other preference rewards of positive data beyond
length. Experiments on mathematical reasoning tasks demonstrate DRPO's
significant superiority over six efficient reasoning baselines. Notably, with a
1.5B model, our method achieves 77\% length reduction with only 1.1\%
performance loss on simple questions like GSM8k dataset, while the follow-up
baseline sacrifices 4.3\% for 68\% length reduction.

</details>


### [911] [COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context](https://arxiv.org/abs/2510.04568)
*Naman Gupta,Shreeyash Gowaikar,Arun Iyer,Kirankumar Shiragur,Ramakrishna B Bairi,Rishikesh Maurya,Ritabrata Maiti,Sankarshan Damle,Shachee Mishra Gupta*

Main category: cs.AI

TL;DR: LLMs在处理长输入时遇到困难，COSMIR通过结构化内存和固定微循环解决了信息丢失和错误放大的问题，提高了长上下文问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM处理长输入的常用方法（如检索、扩大上下文窗口、多智能体分段处理）存在信息丢失、选择性差或错误放大的问题。特别是分段处理中，自由形式的摘要容易丢失细节和放大早期错误。

Method: 提出COSMIR框架，使用结构化内存替代自由形式的消息。具体包括：1. 规划智能体：将用户查询分解为具体、可检查的子问题。2. 工作智能体：按固定微循环（提取、推断、精炼）处理输入块，并将更新写入共享内存。3. 管理智能体：直接从内存中综合最终答案。

Result: COSMIR在HELMET长上下文问答任务上，减少了传播阶段的信息丢失，并提高了准确性，优于CoA基线。

Conclusion: COSMIR通过结构化内存和固定的工作流程（微循环）保留了分步推理的好处，同时解决了信息丢失和错误放大问题，提高了长上下文问答的忠实度、长距离聚合能力和可审计性。

Abstract: Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

</details>


### [912] [Think Then Embed: Generative Context Improves Multimodal Embedding](https://arxiv.org/abs/2510.05014)
*Xuanming Cui,Jianpeng Cheng,Hong-you Chen,Satya Narayan Shukla,Abhijeet Awasthi,Xichen Pan,Chaitanya Ahuja,Shlok Kumar Mishra,Qi Guo,Ser-Nam Lim,Aashu Singh,Xiangjun Fan*

Main category: cs.AI

TL;DR: 提出了一种名为Think-Then-Embed (TTE)的通用多模态嵌入框架，通过引入推理步骤来提升多模态大模型处理复杂指令的能力，并在MMEB-V2基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型在处理通用多模态嵌入任务时，仅将多模态大模型视为编码器，忽略了其生成能力，这种编码范式在面对复杂的、需要组合推理的指令时效果不佳。

Method: 提出Think-Then-Embed (TTE)框架，包含一个推理器（reasoner）和一个嵌入器（embedder）。推理器首先生成解释复杂查询的推理链，然后嵌入器基于原始查询和中间推理生成表示。

Result: 在MMEB-V2基准测试中，TTE框架实现了最先进的性能，优于使用海量内部数据集训练的专有模型。通过对一个较小的多模态大模型进行微调，使其能够生成高质量的、以嵌入为中心的推理链，TTE框架在开源模型中取得了最佳性能，比近期提出的模型提高了7%。此外，还研究了将推理器和嵌入器整合到统一模型中的策略，以在不牺牲性能的情况下提高效率。

Conclusion: TTE框架通过显式的推理步骤，能够更细致地理解复杂的多模态指令，显著提升了通用多模态嵌入任务的性能。

Abstract: There is a growing interest in Universal Multimodal Embeddings (UME), where
models are required to generate task-specific representations. While recent
studies show that Multimodal Large Language Models (MLLMs) perform well on such
tasks, they treat MLLMs solely as encoders, overlooking their generative
capacity. However, such an encoding paradigm becomes less effective as
instructions become more complex and require compositional reasoning. Inspired
by the proven effectiveness of chain-of-thought reasoning, we propose a general
Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an
embedder. The reasoner MLLM first generates reasoning traces that explain
complex queries, followed by an embedder that produces representations
conditioned on both the original query and the intermediate reasoning. This
explicit reasoning step enables more nuanced understanding of complex
multimodal instructions. Our contributions are threefold. First, by leveraging
a powerful MLLM reasoner, we achieve state-of-the-art performance on the
MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house
datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune
a smaller MLLM reasoner using high-quality embedding-centric reasoning traces,
achieving the best performance among open-source models with a 7% absolute gain
over recently proposed models. Third, we investigate strategies for integrating
the reasoner and embedder into a unified model for improved efficiency without
sacrificing performance.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [913] [Seal Whisker-Inspired Sensor for Amplifying Wake-Induced Vibrations in Underwater Marine Animal Monitoring](https://arxiv.org/abs/2510.03664)
*Yuyan Wu,Sanjay Giridharan,Leixin Ma,Hae Young Noh*

Main category: physics.app-ph

TL;DR: 受海豹胡须的流体动力学传感启发，开发了一种新型水下动物监测传感器。该传感器具有螺旋穿孔基座，可放大与动物活动相关的频率范围内的振动，相比基线设计，在目标传感位置的均方根位移增强高达 51 倍，解决了现有传感器灵敏度不足的问题，并能通过调整设计参数以适应特定物种。


<details>
  <summary>Details</summary>
Motivation: 水下海洋动物监测对于评估生物多样性、生态系统健康以及了解海上结构物的影响至关重要。传统方法存在侵入性、能耗高、能见度差等局限性。

Method: 开发了一种受海豹胡须启发的、具有螺旋穿孔基座的仿生胡须振动传感器，并通过谐波水压下的频率响应模拟来评估其性能。

Result: 研究结果表明，与基线设计相比，在与动物活动相关的频率范围内，目标传感位置的均方根位移增强了高达 51 倍。

Conclusion: 所提出的螺旋穿孔基座设计能有效放大水下动物活动产生的尾流振动，为开发更灵敏的水下动物监测传感器提供了新的途径。

Abstract: Underwater marine animal monitoring is essential for assessing biodiversity,
evaluating ecosystem health, and understanding the effects of offshore
structures. Traditional approaches such as tagging, sonar, and camera systems
are often invasive, energy-intensive, or limited by poor visibility and water
turbidity. Inspired by the hydrodynamic sensing of seal whiskers, wavy whisker
vibration sensors have been developed for flow velocity and angle-of-attack
detection. However, most prior work has focused on sensor characterization and
only forward modeling, with limited exploration of the inverse problem of
inferring animal movement. Moreover, current sensor sensitivity to vortex
street wakes generated by swimming animals remains insufficient for practical
monitoring. To address this gap, we develop a whisker-inspired sensor with a
spiral-perforated base that amplifies vibrations within frequency ranges
relevant to animal-induced wakes. We further characterize the influence of
spiral parameters on the sensitive frequency band, enabling adaptation of the
design to specific species. We evaluated the amplification effect of the
spiral-perforated design using frequency response simulations of the
whisker-base structure under harmonic water pressure. Results show up to 51x
enhancement in root mean squared displacement at the target sensor location
within frequency bands associated with animal-induced wakes compared to the
baseline design, confirming the effectiveness of the amplification.

</details>


### [914] [Ultralong Octupole Moment Switching Driven by Twin Topological Spin Structures](https://arxiv.org/abs/2510.04055)
*Shijie Xu,Zhizhong Zhang,Yan Huang,Tianyi Wang,Bingqian Dai,Yinchang Ma,Mang Yang,Meng Tang,Houyi Cheng,Kang L. Wang,Weisheng Zhao,Yue Zhang,Xixiang Zhang*

Main category: physics.app-ph

TL;DR: Spintronics promises faster, more energy-efficient electronics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the revolutionary potential of spintronics in advancing electronics.

Method: The paper will likely discuss the principles and applications of spintronics.

Result: The anticipated result is a demonstration of spintronics' advantages over traditional electronics.

Conclusion: Spintronics is poised to be a key technology for future electronic devices.

Abstract: Spintronics has emerged as a revolutionary frontier in the pursuit of faster,
more energy-efficient, and technologically advanced electronics.

</details>


### [915] [Agile manoeuvring of dandelion-inspired micro-flyers with vortex-enabled stability](https://arxiv.org/abs/2510.04720)
*Jianfeng Yang,Soumarup Bhattacharyya,Aditya Potnis,Hao Zeng,Ignazio Maria Viola*

Main category: physics.app-ph

TL;DR: 一种受蒲公英启发的微型飞行器，利用液晶弹性体薄膜的照相力学形变，实现了无系绳、可控的飞行。


<details>
  <summary>Details</summary>
Motivation: 解决现有微型空中结构在自主飞行和能量供应方面存在的局限性。

Method: 设计并制造了一种受蒲公英启发的六边形聚合微型飞行器，利用六个独立的软驱动器（由液晶弹性体薄膜制成）通过光致变形来动态控制其六个放射状的丝状结构的形态，从而实现飞行轨迹的被动光学控制。

Result: 微型飞行器展现出与蒲公英种子相似的终端速度，但位置稳定性提高了45%，旋转速率几乎为零。粒子图像测速证实，其飞行稳定性得益于稳定的非对称分离涡环结构，这使其能够在中途进行转向。在自由落体过程中，该飞行器能够精确控制高度、实现可逆的翻转、形成图案、进行群体互动以及在三维空间中进行轨迹控制。

Conclusion: 具有光致不对称响应特性的材料可以为微型飞行器提供空中机动能力，为开发灵活、无系绳且可控的微型飞行器开辟了新途径。

Abstract: Manoeuvring untethered, centimetre-scale airborne structures has been a
long-standing challenge. Active flight systems, relying on high-power-density
actuators alongside mechanical and electronic components, are constrained by
critical limitations in energy delivery and miniaturisation. In contrast,
passive systems transported and distributed by the wind typically lack the
capability for mid-air controlled manoeuvrability. Here we report an
ultra-light (1.2 mg) hexagonal polymeric assembly capable of passive flight
with optical control of its trajectory. This dandelion-inspired micro-flyer
incorporates six radially arranged filamentous structures, of which morphology
is dynamically controlled through photomechanical deformation by six
independent soft actuators made of liquid crystalline elastomer thin films.
Compared to the diaspore of the dandelion (Taraxacum officinale), micro-flyer
demonstrate a similar terminal velocity (~0.5 m s-1), 45% better positional
stability and nearly zero rotational rate (1.68 s-1; natural seeds: 50.8 s-1).
Particle image velocimetry reveals that a stable asymmetric separated vortex
ring underlies its flight stability, enabling mid-air steerability. When
free-falling in a low-turbulent airstream, the light-driven hexapodal fliers
demonstrate precise altitude control, reversible body flipping, pattern
formation, interactive swarm, and controlled trajectories across
three-dimensional space. The results show that responsive materials with
light-induced asymmetry can bring about manoeuvrability in air, paving the way
for agile, untethered controlled micro-fliers.

</details>


### [916] [Surface Acoustic Wave Gas Sensors: Innovations in Functional Materials, Sensing Dynamics, and Signal Analysis](https://arxiv.org/abs/2510.04940)
*Suman Acharya,Balasubramanian Srinivasan,David Shanahan,Utz Roedig,Alan O Riordan,Veda Sandeep Nagaraja*

Main category: physics.app-ph

TL;DR: SAW气体传感器通过声波频移检测气体，具有高灵敏度、小型化和无线兼容性。本综述总结了不同基板和模式（瑞利波、SH-SAW、Love波）的SAW平台，分析了材料选择与能量转换路径的关系，并对NO2、NH3、VOCs、CO2等关键分析物的性能进行了基准测试。文章还探讨了纳米结构氧化物、聚合物、碳基薄膜和混合异质结涂层等材料的特性，以及它们如何影响灵敏度和可逆性。此外，还介绍了SAW器件在探测吸附-解吸动力学、机器学习信号解码、环境补偿和自适应校准方面的应用，并指出了交叉敏感性、信号漂移、材料降解和边缘部署等挑战，最后展望了SAW平台在环境监测、工业过程控制和医疗诊断等领域的应用前景。


<details>
  <summary>Details</summary>
Motivation: SAW气体传感器因其高灵敏度、小型化和无线兼容性，在分子检测领域受到越来越多的关注。

Method: 本综述综合了不同基板和模式（瑞利波、SH-SAW、Love波）的SAW平台，将能量转换路径与材料选择联系起来，并对NO2、NH3、VOCs、CO2等关键分析物的性能进行了基准测试。文章分类了纳米结构氧化物、聚合物、碳基薄膜和混合异质结涂层，并强调了孔隙度、表面化学性质和界面电荷转移等影响灵敏度和可逆性的属性。

Result: SAW器件在探测吸附-解吸动力学方面的新兴应用被强调，它提供了超越平衡状态的、特定于分析物的相互作用特征，从而对特定分析物的相互作用路径提供了新的视角。机器学习作为信号解码、环境补偿和自适应校准的变革性工具也被讨论。文章还指出了交叉敏感性、信号漂移、材料降解和边缘部署等关键挑战，并回顾了解决这些挑战的最新策略。

Conclusion: SAW平台有望发展成为具有环境监测、工业过程控制和医疗诊断应用前景的智能、自主传感系统。

Abstract: Surface Acoustic Wave gas sensors have garnered increasing attention as
highly sensitive, miniaturized, and wireless compatible platforms for molecular
detection. Their unique ability to convert surface perturbations into
measurable acoustic shifts makes them ideal for gas sensing across diverse
environments. This review synthesizes reported SAW platforms across substrates
and modes Rayleigh, SH-SAW, Love links transduction pathways to material
choice, and benchmarks performance for key analytes, e.g., NO2, NH3, VOCs, CO2,
etc. We catalogue nanostructured oxides, polymers, carbon based films, and
hybrid heterojunction coatings, highlighting attributes such as porosity,
surface chemistry, and interfacial charge transfer that govern sensitivity and
reversibility. We also highlight the emerging use of SAW devices to probe
adsorption desorption dynamics, offering analyte specific interaction
signatures beyond equilibrium, offering a new perspective into analyte specific
interaction pathways. Additionally, the integration of machine learning is
discussed as a transformative tool for signal decoding, environmental
compensation, and adaptive calibration. We also identify key challenges, cross
sensitivity, signal drift, material degradation, and deployment at the edge and
review recent strategies to address them. Looking ahead, we envision the
evolution of SAW platforms into intelligent, autonomous sensing systems with
applications in environmental monitoring, industrial process control, and
healthcare diagnostics.

</details>


### [917] [Hybrid magnonic spintronic system for tunable broadband signal filtering and microwave generation](https://arxiv.org/abs/2510.04976)
*A. Koujok,A. Hamadeh,L. Martins,F. Kohl,B. Heinz,U. Ebels,P. Pirro*

Main category: physics.app-ph

TL;DR: We present a hybrid magnonic-spintronic device that acts as a tunable filter for microwave signals, converting broad, degraded signals into narrow-linewidth spin wave outputs with potential for energy-efficient signal processing.


<details>
  <summary>Details</summary>
Motivation: Complex data-driven applications require signal processing schemes that are parallel, scalable, robust, and energy-efficient. Magnonic and spintronic circuits show promise in meeting these demands.

Method: A hybrid device was created using a spin-transfer torque nano-oscillator (STNO) as a tunable nano-scaled signal generator with a broad output linewidth. The STNO's output was fed into a magnonic delay-line RF filter. By tuning the magnetic field at the magnonic circuit, selective filtering of the broad RF input was achieved, resulting in a spin-wave output with a narrower linewidth.

Result: The experiment demonstrated the selective filtering of a broad RF input signal using the hybrid magnonic-spintronic device. The output spin-wave signal had a significantly narrower linewidth compared to the input. The output frequency could be tuned simply by adjusting the magnetic field applied to the magnonic circuit.

Conclusion: This work represents a first step towards a versatile, energy-efficient, and compact wave-based filter with high sensitivity. The device can process low-power, degraded signals and convert them into tunable spin-wave outputs, potentially reducing reliance on charge-based signal processing.

Abstract: Non-conventional beyond-the-state-of-the-art signal processing schemes
require parallelism, scalability, robustness and energy efficiency to meet the
demands of complex data-driven applications. With further research, magnonic
and spintronic circuits can potentially help to fulfill these requirements. We
present an experimental proof-of-concept of a hybrid device that can employ
broad deteriorated microwave signals to excite and detect low energy
propagating spin waves (SWs). For this, we use the output signal of a
spin-transfer torque nano-oscillator (STNO) and connect it to a RF filter based
on a magnonic delay-line. The STNO serves as a tunable nano-scaled signal
generator with a broad output linewidth. Its RF output is fed as input into the
magnonic delay-line circuit. Tuning the magnetic field solely at the magnonic
circuit, we demonstrate the capability to selectively filter a broad RF input,
obtaining a spin-wave output signal with a much narrower linewidth. This allows
to tune the frequency of the RF signal at the output simply by tuning the
magnetic field. Our findings are a first step towards a versatile,
energy-efficient and compact wave-based filter with high sensitivity. Such a
device can use even low-power, degraded signals and convert them into tunable
SW outputs, effectively reducing the need for charge-based signal processing.

</details>
