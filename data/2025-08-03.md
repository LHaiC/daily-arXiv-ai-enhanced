<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 95]
- [cs.CL](#cs.CL) [Total: 68]
- [physics.app-ph](#physics.app-ph) [Total: 3]
- [eess.SY](#eess.SY) [Total: 12]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.SI](#cs.SI) [Total: 5]
- [cs.ET](#cs.ET) [Total: 2]
- [eess.SP](#eess.SP) [Total: 12]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.LG](#cs.LG) [Total: 64]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 22]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.LO](#cs.LO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 23]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.RO](#cs.RO) [Total: 25]
- [quant-ph](#quant-ph) [Total: 48]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified State Exam](https://arxiv.org/abs/2507.22958)
*Ruslan Khrulev*

Main category: cs.CV

TL;DR: 本研究提出了一个评估视觉语言模型（VLMs）在评估手写数学解题方案能力的基准（EGE-Math Solutions Assessment Benchmark），包含122个EGE考试的数学解题方案。研究发现当前VLMs在数学推理和遵循评分标准方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 评估视觉语言模型（VLMs）在评估手写数学解题方案方面的能力，重点在于理解学生解题过程、识别错误并根据固定标准进行评分。

Method: 本研究构建了一个名为EGE-Math Solutions Assessment Benchmark的基准，用于评估视觉语言模型（VLMs）对纸质数学解题方案的评估能力。该基准包含122个来自俄罗斯统一国家考试（EGE）的扫描解题方案及官方专家评分。研究评估了来自Google、OpenAI、Arcee AI和Alibaba Cloud的七种现代VLMs在三种推理模式下的表现。

Result: 评估结果显示，当前的视觉语言模型在数学推理能力以及与人类评分标准对齐方面存在不足。

Conclusion: 本研究揭示了当前视觉语言模型在数学推理和人类评分标准对齐方面存在的局限性，并为人工智能辅助评估开辟了新的研究途径。

Abstract: This paper introduces a novel benchmark, EGE-Math Solutions Assessment
Benchmark, for evaluating Vision-Language Models (VLMs) on their ability to
assess hand-written mathematical solutions. Unlike existing benchmarks that
focus on problem solving, our approach centres on understanding student
solutions, identifying mistakes, and assigning grades according to fixed
criteria. We compile 122 scanned solutions from the Russian Unified State Exam
(EGE) together with official expert grades, and evaluate seven modern VLMs from
Google, OpenAI, Arcee AI, and Alibaba Cloud in three inference modes. The
results reveal current limitations in mathematical reasoning and human-rubric
alignment, opening new research avenues in AI-assisted assessment. You can find
code in https://github.com/Karifannaa/Auto-check-EGE-math

</details>


### [2] [Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction](https://arxiv.org/abs/2507.23006)
*Zhensheng Yuan,Haozhi Huang,Zhen Xiong,Di Wang,Guanghua Yang*

Main category: cs.CV

TL;DR: 一个用于快速重建和实时渲染城市规模场景的框架，具有良好的鲁棒性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 提出一个能够对城市规模场景进行快速重建和实时渲染的框架，同时保持对多视图捕获中外观变化的高度鲁棒性。

Method: 本文提出了一种框架，通过场景划分进行并行训练，采用基于可见性的图像选择策略优化训练效率，并利用可控的细节层次（LOD）策略在用户定义的预算下调节高斯密度。此外，还包括一个外观变换模块来处理图像间的外观不一致性，并结合深度正则化、尺度正则化和抗锯齿等增强模块来提高重建保真度。

Result: 实验结果表明，该方法能够有效重建城市规模场景，并在效率和质量方面优于先前的方法。

Conclusion: 该方法有效重建了城市规模场景，并在效率和质量方面优于先前的方法。

Abstract: We present a framework that enables fast reconstruction and real-time
rendering of urban-scale scenes while maintaining robustness against appearance
variations across multi-view captures. Our approach begins with scene
partitioning for parallel training, employing a visibility-based image
selection strategy to optimize training efficiency. A controllable
level-of-detail (LOD) strategy explicitly regulates Gaussian density under a
user-defined budget, enabling efficient training and rendering while
maintaining high visual fidelity. The appearance transformation module
mitigates the negative effects of appearance inconsistencies across images
while enabling flexible adjustments. Additionally, we utilize enhancement
modules, such as depth regularization, scale regularization, and antialiasing,
to improve reconstruction fidelity. Experimental results demonstrate that our
method effectively reconstructs urban-scale scenes and outperforms previous
approaches in both efficiency and quality. The source code is available at:
https://yzslab.github.io/REUrbanGS.

</details>


### [3] [Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction](https://arxiv.org/abs/2507.23021)
*Giuseppe Cartella,Vittorio Cuculo,Alessandro D'Amelio,Marcella Cornia,Giuseppe Boccignone,Rita Cucchiara*

Main category: cs.CV

TL;DR: ScanDiff, a novel architecture combining diffusion models and Vision Transformers, generates diverse and realistic human gaze scanpaths, outperforming existing methods in various scenarios by explicitly modeling scanpath variability and incorporating textual conditioning for task-driven generation.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for scanpath prediction often generate averaged behaviors and fail to capture the variability of human visual exploration. This work aims to address this limitation by generating diverse and realistic scanpaths.

Method: ScanDiff combines diffusion models with Vision Transformers to generate diverse and realistic scanpaths. It explicitly models scanpath variability using the stochastic nature of diffusion models and introduces textual conditioning for task-driven scanpath generation.

Result: Experiments on benchmark datasets show that ScanDiff produces more diverse and accurate scanpaths compared to state-of-the-art methods in both free-viewing and task-driven scenarios.

Conclusion: ScanDiff surpasses state-of-the-art methods in both free-viewing and task-driven scenarios, producing more diverse and accurate scanpaths, highlighting its ability to better capture the complexity of human visual behavior and pushing forward gaze prediction research.

Abstract: Predicting human gaze scanpaths is crucial for understanding visual
attention, with applications in human-computer interaction, autonomous systems,
and cognitive robotics. While deep learning models have advanced scanpath
prediction, most existing approaches generate averaged behaviors, failing to
capture the variability of human visual exploration. In this work, we present
ScanDiff, a novel architecture that combines diffusion models with Vision
Transformers to generate diverse and realistic scanpaths. Our method explicitly
models scanpath variability by leveraging the stochastic nature of diffusion
models, producing a wide range of plausible gaze trajectories. Additionally, we
introduce textual conditioning to enable task-driven scanpath generation,
allowing the model to adapt to different visual search objectives. Experiments
on benchmark datasets show that ScanDiff surpasses state-of-the-art methods in
both free-viewing and task-driven scenarios, producing more diverse and
accurate scanpaths. These results highlight its ability to better capture the
complexity of human visual behavior, pushing forward gaze prediction research.
Source code and models are publicly available at
https://aimagelab.github.io/ScanDiff.

</details>


### [4] [Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields](https://arxiv.org/abs/2507.23033)
*Ranxi Lin,Canming Yao,Jiayi Li,Weihang Liu,Xin Lou,Pingqiang Zhou*

Main category: cs.CV

TL;DR: PATA is a spike-based NeRF framework using SNNs and a dynamic time step training strategy to reduce computational cost and power consumption while maintaining rendering quality, achieving significant reductions in time steps and power usage.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF models require dense point sampling, leading to high computational costs and limiting their use in resource-constrained environments. SNNs offer an energy-efficient alternative. The variability in scene scale and texture complexity necessitates a method that can adapt to different scenes.

Method: Spiking Neural Networks (SNNs) are used to create a spike-based NeRF framework with a dynamic time step training strategy called Pretrain-Adaptive Time-step Adjustment (PATA). This approach automatically balances rendering quality and time step length during training, enabling scene-adaptive inference with variable time steps and reducing computational resource consumption during inference.

Result: Experimental results show that PATA, when anchored to the Instant-NGP architecture, preserves rendering fidelity while reducing inference time steps by 64% and running power by 61.55% across diverse datasets.

Conclusion: PATA can preserve rendering fidelity while reducing inference time steps by 64% and running power by 61.55%.

Abstract: Neural Radiance Fields (NeRF)-based models have achieved remarkable success
in 3D reconstruction and rendering tasks. However, during both training and
inference, these models rely heavily on dense point sampling along rays from
multiple viewpoints, resulting in a surge in floating-point operations and
severely limiting their use in resource-constrained scenarios like edge
computing. Spiking Neural Networks (SNNs), which communicate via binary spikes
over discrete time steps, offer a promising alternative due to their
energy-efficient nature. Given the inherent variability in scene scale and
texture complexity in neural rendering and the prevailing practice of training
separate models per scene, we propose a spike-based NeRF framework with a
dynamic time step training strategy, termed Pretrain-Adaptive Time-step
Adjustment (PATA). This approach automatically explores the trade-off between
rendering quality and time step length during training. Consequently, it
enables scene-adaptive inference with variable time steps and reduces the
additional consumption of computational resources in the inference process.
Anchoring to the established Instant-NGP architecture, we evaluate our method
across diverse datasets. The experimental results show that PATA can preserve
rendering fidelity while reducing inference time steps by 64\% and running
power by 61.55\%.

</details>


### [5] [Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification in Resource-Constrained Imaging](https://arxiv.org/abs/2507.23027)
*Krishan Agyakari Raja Babu,Om Prabhu,Annu,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 本研究表明，超分辨率技术（特别是SRResNet）能显著提升低质量超声心动图的图像质量和分类准确性，证明了其在资源受限环境下辅助心脏诊断的潜力。


<details>
  <summary>Details</summary>
Motivation: 资源受限环境下（RCS）的自动心脏评估常因超声心动图成像质量差而受阻，这限制了下游诊断模型的有效性。尽管超分辨率（SR）技术在增强磁共振成像（MRI）和计算机断层扫描（CT）方面显示出潜力，但其在超声心动图——一种易于获得但易产生噪声的成像方式——上的应用仍未得到充分探索。

Method: 本研究调查了基于深度学习的超分辨率技术在提高低质量二维超声心动图分类准确性方面的潜力。研究使用了公开的CAMUS数据集，按图像质量进行分层，并评估了两个不同复杂度的临床相关任务：二腔心与四腔心的视图分类（2CH vs. 4CH）以及舒张末期与收缩末期的相位分类（ED vs. ES）。研究应用了两种广泛使用的超分辨率模型——超分辨率生成对抗网络（SRGAN）和超分辨率残差网络（SRResNet）——来增强低质量图像。

Result: 研究观察到，特别是在使用SRResNet时，性能指标有了显著提升，SRResNet还具有计算效率。SRResNet在2CH vs. 4CH任务上，低质量图像的准确率从0.85提升到0.94，ED vs. ES任务上从0.71提升到0.82。SRGAN也显示出提升，但提升幅度较小且计算量更大。

Conclusion: 深度学习超分辨率技术可以有效恢复退化超声心动图的诊断价值，是资源受限环境下人工智能辅助护理的可行工具，能够以更少的资源实现更多目标。

Abstract: Automated cardiac interpretation in resource-constrained settings (RCS) is
often hindered by poor-quality echocardiographic imaging, limiting the
effectiveness of downstream diagnostic models. While super-resolution (SR)
techniques have shown promise in enhancing magnetic resonance imaging (MRI) and
computed tomography (CT) scans, their application to echocardiography-a widely
accessible but noise-prone modality-remains underexplored. In this work, we
investigate the potential of deep learning-based SR to improve classification
accuracy on low-quality 2D echocardiograms. Using the publicly available CAMUS
dataset, we stratify samples by image quality and evaluate two clinically
relevant tasks of varying complexity: a relatively simple Two-Chamber vs.
Four-Chamber (2CH vs. 4CH) view classification and a more complex End-Diastole
vs. End-Systole (ED vs. ES) phase classification. We apply two widely used SR
models-Super-Resolution Generative Adversarial Network (SRGAN) and
Super-Resolution Residual Network (SRResNet), to enhance poor-quality images
and observe significant gains in performance metric-particularly with SRResNet,
which also offers computational efficiency. Our findings demonstrate that SR
can effectively recover diagnostic value in degraded echo scans, making it a
viable tool for AI-assisted care in RCS, achieving more with less.

</details>


### [6] [Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving](https://arxiv.org/abs/2507.23042)
*Santosh Patapati,Trisanth Srinivasan*

Main category: cs.CV

TL;DR: NovaDrive 是一个用于自动驾驶的视觉-语言模型，它能实时处理多种传感器数据，通过交叉注意力和新颖的平滑损失提高了安全性和效率，减少了能耗。


<details>
  <summary>Details</summary>
Motivation: 自主车辆需要在复杂环境中对道路几何和交通意图进行毫秒级推理以进行导航。现有的方法需要处理多模态输入并可能涉及复杂的模型结构。因此，需要一种能够有效整合多种传感器数据并实时做出响应的架构。

Method: 提出了一种名为 NovaDrive 的单一分支视觉-语言架构，该架构能够在一个分支中处理来自前置摄像头图像、高清地图瓦片、激光雷达深度和文本航点的信息。通过一个轻量级的两阶段交叉注意力模块，首先对齐航点标记与高清地图，然后对细粒度的图像和深度图块进行注意力细化。此外，还引入了一种新颖的平滑度损失，以减少急剧的转向和速度变化，从而无需循环记忆。该模型对一个 11B 参数的 LLaMA-3.2 视觉-语言骨干网络的顶层进行了微调，以实现实时推理。

Result: 在 nuScenes / Waymo 数据集上，NovaDrive 的成功率提高了 4%（达到 84%），路径效率（SPL）提高了 0.11（达到 0.66），碰撞频率从 2.6% 降低到 1.2%。消融实验表明，航点标记、部分视觉语言模型微调和交叉注意力融合是提升性能的关键因素。

Conclusion: NovaDrive 架构通过单一分支处理多种传感器数据，并结合新颖的平滑度损失，实现了实时推理和性能提升，减少了事故发生率和不必要的转向/速度变化，有望降低能耗并易于更新，还可扩展到其他具身智能领域。

Abstract: Autonomous vehicles must react in milliseconds while reasoning about road
geometry and traffic intent to navigate complex situations. We introduce
NovaDrive, a single-branch vision-language architecture that processes
front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a
single branch. A lightweight, two-stage cross-attention block first aligns
waypoint tokens with the HD map, then refines attention over fine-grained image
and depth patches. Coupled with a novel smoothness loss that discourages abrupt
steering and speed changes, this design eliminates the need for recurrent
memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language
backbone, enabling real-time inference. On the nuScenes / Waymo subset of the
MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts
path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from
2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations
confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention
fusion each contribute the most to these gains. Beyond safety, NovaDrive's
shorter routes (resulting from the novel smoothness loss) translate to lower
fuel or battery usage, pointing toward leaner, more easily updated driving
stacks. NovaDrive can be extended to other embodied-AI domains as well.

</details>


### [7] [Consistent Point Matching](https://arxiv.org/abs/2507.23609)
*Halid Ziya Yerebakan,Gerardo Hermosillo Valadez*

Main category: cs.CV

TL;DR: 通过在点匹配算法中加入一致性启发式方法，可以提高医学图像解剖位置匹配的鲁棒性，并且无需机器学习模型或训练数据即可实现高精度导航。


<details>
  <summary>Details</summary>
Motivation: 为了提高在医学图像中匹配解剖位置的鲁棒性，并解决地标定位问题。

Method: 通过将一致性启发式方法整合到点匹配算法中，以提高在医学图像配准中的鲁棒性。

Result: 该方法在Deep Lesion Tracking数据集上超越了最先进的结果，并且能够有效解决地标定位问题，同时在CT和MRI模态的纵向内部和公共数据集上进行了验证。

Conclusion: 该方法能够在不使用机器学习模型或训练数据的情况下，在医学图像之间实现高精度导航，并且可以在速度和鲁棒性之间进行配置权衡，同时能在标准CPU硬件上高效运行。

Abstract: This study demonstrates that incorporating a consistency heuristic into the
point-matching algorithm \cite{yerebakan2023hierarchical} improves robustness
in matching anatomical locations across pairs of medical images. We validated
our approach on diverse longitudinal internal and public datasets spanning CT
and MRI modalities. Notably, it surpasses state-of-the-art results on the Deep
Lesion Tracking dataset. Additionally, we show that the method effectively
addresses landmark localization. The algorithm operates efficiently on standard
CPU hardware and allows configurable trade-offs between speed and robustness.
The method enables high-precision navigation between medical images without
requiring a machine learning model or training data.

</details>


### [8] [Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation](https://arxiv.org/abs/2507.23058)
*Alexandru Buburuzan*

Main category: cs.CV

TL;DR: 这项工作介绍了 MObI 和 AnydoorMed，用于在自动驾驶和医学成像领域生成合成数据。MObI 是一种用于多模态对象修复的框架，可生成逼真且可控的对象修复。
AnydoorMed 将此方法扩展到医学成像，用于修复乳腺钼靶检查中的异常。


<details>
  <summary>Details</summary>
Motivation: 安全关键型应用（例如自动驾驶和医学图像分析）需要广泛的多模态数据进行严格测试。由于收集真实数据的成本和复杂性，合成数据方法正变得越来越重要，但它们需要高度的逼真度和可控性才能有用。

Method: 1. MObI：一种多模态对象修复框架，利用扩散模型生成逼真的、可控的跨感知模态的对象修复，同时用于摄像头和激光雷达。给定单个参考 RGB 图像，MObI 能够在 3D 位置插入对象，并通过边界框进行引导，同时保持语义一致性和多模态相干性。
2. AnydoorMed：将 MObI 的范式扩展到医学成像领域，专注于乳腺钼靶检查的参考引导修复。它利用基于扩散的模型修复异常，具有出色的细节保留能力，同时保持参考异常的结构完整性，并将其与周围组织在语义上融合。

Result: MObI 能够将对象无缝插入到现有的多模态场景中，并进行精确的空间定位和逼真的缩放。
AnydoorMed 能够以出色的细节保留能力修复异常，同时保持参考异常的结构完整性，并将其与周围组织在语义上融合。

Conclusion: MObI 和 AnydoorMed 的方法表明，用于自然图像中参考引导修复的基础模型可以轻松地适应不同的感知模态，为能够构建高度逼真、可控和多模态的“反事实”场景的下一代系统铺平了道路。

Abstract: Safety-critical applications, such as autonomous driving and medical image
analysis, require extensive multimodal data for rigorous testing. Synthetic
data methods are gaining prominence due to the cost and complexity of gathering
real-world data, but they demand a high degree of realism and controllability
to be useful. This work introduces two novel methods for synthetic data
generation in autonomous driving and medical image analysis, namely MObI and
AnydoorMed, respectively. MObI is a first-of-its-kind framework for Multimodal
Object Inpainting that leverages a diffusion model to produce realistic and
controllable object inpaintings across perceptual modalities, demonstrated
simultaneously for camera and lidar. Given a single reference RGB image, MObI
enables seamless object insertion into existing multimodal scenes at a
specified 3D location, guided by a bounding box, while maintaining semantic
consistency and multimodal coherence. Unlike traditional inpainting methods
that rely solely on edit masks, this approach uses 3D bounding box conditioning
to ensure accurate spatial positioning and realistic scaling. AnydoorMed
extends this paradigm to the medical imaging domain, focusing on
reference-guided inpainting for mammography scans. It leverages a
diffusion-based model to inpaint anomalies with impressive detail preservation,
maintaining the reference anomaly's structural integrity while semantically
blending it with the surrounding tissue. Together, these methods demonstrate
that foundation models for reference-guided inpainting in natural images can be
readily adapted to diverse perceptual modalities, paving the way for the next
generation of systems capable of constructing highly realistic, controllable
and multimodal counterfactual scenarios.

</details>


### [9] [Vision-Language Fusion for Real-Time Autonomous Driving: Goal-Centered Cross-Attention of Camera, HD-Map, & Waypoints](https://arxiv.org/abs/2507.23064)
*Santosh Patapati,Trisanth Srinivasan,Murari Ambati*

Main category: cs.CV

TL;DR: XYZ-Drive是一个集成了视觉、地图和航点信息的单一视觉-语言模型，用于自动驾驶。它通过以目标为中心的交叉注意力机制和LLaMA-3.2 11B模型实现了高精度和高效率，并在基准测试中取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的自动驾驶系统通常将几何精度和语义理解分开处理，但这种方法效率低下。作者旨在开发一个能够整合这两种能力的单一模型。

Method: 提出了一种名为XYZ-Drive的单一视觉-语言模型，该模型融合了来自前置摄像头、25m×25m顶视图地图以及下一个航点的信息，并输出了转向和速度指令。模型包含一个以目标为中心的交叉注意力层，用于突出显示与航点相关的图像和地图块，随后将融合后的信息输入到经过部分微调的LLaMA-3.2 11B模型中。

Result: XYZ-Drive在MD-NEX Outdoor-Driving基准测试中取得了95%的成功率和0.80的SPL（成功率加权路径长度），优于PhysNav-DG，并将碰撞率减半。消融实验表明，移除任何一种模态（视觉、航点、地图）都会导致成功率下降最多11%，表明了它们互补的作用。目标为中心的注意力机制比简单的拼接提高了3%的性能。保持Transformer冻结会损失5%的性能。将地图分辨率从10cm降低到40cm会导致车道线模糊和碰撞率增加。

Conclusion: 早期、细粒度的融合传感器数据和任务信息可以实现准确、透明和实时的自动驾驶。

Abstract: Autonomous cars need geometric accuracy and semantic understanding to
navigate complex environments, yet most stacks handle them separately. We
present XYZ-Drive, a single vision-language model that reads a front-camera
frame, a 25m $\times$ 25m overhead map, and the next waypoint, then outputs
steering and speed. A lightweight goal-centered cross-attention layer lets
waypoint tokens highlight relevant image and map patches, supporting both
action and textual explanations, before the fused tokens enter a partially
fine-tuned LLaMA-3.2 11B model.
  On the MD-NEX Outdoor-Driving benchmark XYZ-Drive attains 95% success and
0.80 Success weighted by Path Length (SPL), surpassing PhysNav-DG by 15%. and
halving collisions, all while significantly improving efficiency by using only
a single branch. Sixteen ablations explain the gains. Removing any modality
(vision, waypoint, map) drops success by up to 11%, confirming their
complementary roles and rich connections. Replacing goal-centered attention
with simple concatenation cuts 3% in performance, showing query-based fusion
injects map knowledge more effectively. Keeping the transformer frozen loses
5%, showing the importance of fine-tuning when applying VLMs for specific tasks
such as autonomous driving. Coarsening map resolution from 10 cm to 40 cm blurs
lane edges and raises crash rate.
  Overall, these results demonstrate that early, token-level fusion of intent
and map layout enables accurate, transparent, real-time driving.

</details>


### [10] [Vocabulary-free Fine-grained Visual Recognition via Enriched Contextually Grounded Vision-Language Model](https://arxiv.org/abs/2507.23070)
*Dmitry Demidov,Zaigham Zaheer,Omkar Thawakar,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: 提出了一种名为 E-FineR 的新方法，用于细粒度图像识别。该方法利用大型语言模型和视觉语言模型，但改进了 LLM 的应用方式，提高了分类性能和可解释性。E-FineR 在各种场景下都表现出色，并且无需训练，适用于零样本和少样本学习。


<details>
  <summary>Details</summary>
Motivation: 解决现有细粒度图像分类方法依赖固定词汇和封闭集范式，限制了其在真实世界场景中的可扩展性和适应性。同时，改进现有结合 LLMs 和 VLMs 的方法，这些方法在利用 LLMs 进行分类以及对 LLM 提供的类别名称进行分析和优化方面存在局限。

Method: 提出了一种名为 Enriched-FineR (E-FineR) 的训练无关型方法，该方法结合了大型语言模型（LLMs）和视觉语言模型（VLMs），但改进了 LLMs 在分类阶段的应用方式，并且对 LLM 提供的类别名称进行了更彻底的分析和优化，而不是直接依赖。

Result: E-FineR 在细粒度视觉识别任务上取得了最先进的结果，并展现出更强的可解释性，使其在真实世界场景和需要专家注释的领域具有巨大潜力。在零样本和少样本分类任务上，E-FineR 的性能与现有最先进方法相当，且无需训练和人工干预。

Conclusion: 该研究提出了一种名为 Enriched-FineR (E-FineR) 的训练无关型方法，该方法在细粒度视觉识别方面取得了最先进的结果，并具有更强的可解释性，在现实场景和难以获得专家注释的新领域具有巨大潜力。此外，该方法在零样本和少样本分类任务上也表现出色，性能与现有最先进方法相当，且无需训练和人工干预。该框架支持从僵化的标签预测到灵活的、由语言驱动的理解的转变，为现实世界的应用提供了可扩展和可泛化的系统。

Abstract: Fine-grained image classification, the task of distinguishing between
visually similar subcategories within a broader category (e.g., bird species,
car models, flower types), is a challenging computer vision problem.
Traditional approaches rely heavily on fixed vocabularies and closed-set
classification paradigms, limiting their scalability and adaptability in
real-world settings where novel classes frequently emerge. Recent research has
demonstrated that combining large language models (LLMs) with vision-language
models (VLMs) makes open-set recognition possible without the need for
predefined class labels. However, the existing methods are often limited in
harnessing the power of LLMs at the classification phase, and also rely heavily
on the guessed class names provided by an LLM without thorough analysis and
refinement. To address these bottlenecks, we propose our training-free method,
Enriched-FineR (or E-FineR for short), which demonstrates state-of-the-art
results in fine-grained visual recognition while also offering greater
interpretability, highlighting its strong potential in real-world scenarios and
new domains where expert annotations are difficult to obtain. Additionally, we
demonstrate the application of our proposed approach to zero-shot and few-shot
classification, where it demonstrated performance on par with the existing SOTA
while being training-free and not requiring human interventions. Overall, our
vocabulary-free framework supports the shift in image classification from rigid
label prediction to flexible, language-driven understanding, enabling scalable
and generalizable systems for real-world applications. Well-documented code is
available on https://github.com/demidovd98/e-finer.

</details>


### [11] [Details Matter for Indoor Open-vocabulary 3D Instance Segmentation](https://arxiv.org/abs/2507.23134)
*Sanghun Jung,Jingjing Zheng,Ke Zhang,Nan Qiao,Albert Y. C. Chen,Lu Xia,Chi Liu,Yuyin Sun,Xiao Zeng,Hsiang-Wei Huang,Byron Boots,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: 提出了一种新的 OV-3DIS 解决方案，通过结合和改进现有概念来提高性能。该方法采用基于 3D 跟踪的提议聚合和 Alpha-CLIP 进行分类，并使用 SMS 分数进行优化，在 ScanNet200 和 S3DIS 上均取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: OV-3DIS 通常利用视觉语言模型 (VLM) 来生成 3D 实例提议并对其进行分类。尽管现有研究提出了各种概念，但这些单个概念并非相互排斥，而是互补的。

Method: 该解决方案遵循两阶段方案：3D 提议生成和实例分类。采用基于 3D 跟踪的稳健提议聚合来生成 3D 提议，并通过迭代合并/移除来移除重叠或部分的提议。在分类阶段，使用 Alpha-CLIP 替换标准的 CLIP 模型，该模型将对象掩码作为 alpha 通道，以减少背景噪声并获得面向对象的表示。此外，还引入了标准最大相似度 (SMS) 分数来对文本到提议的相似度进行标准化，从而有效地滤除了假阳性并提高了精度。

Result: 在 ScanNet200 和 S3DIS 上实现了最先进的性能，在所有 AP 和 AR 指标上都优于现有方法，甚至优于端到端的闭词汇方法。

Conclusion: 通过仔细设计组合概念的配方并加以改进以解决关键挑战，提出了新的最先进的 OV-3DIS 解决方案。所提出的框架在 ScanNet200 和 S3DIS 上实现了最先进的性能，在所有 AP 和 AR 指标上都优于现有方法，甚至优于端到端的闭词汇方法。

Abstract: Unlike closed-vocabulary 3D instance segmentation that is often trained
end-to-end, open-vocabulary 3D instance segmentation (OV-3DIS) often leverages
vision-language models (VLMs) to generate 3D instance proposals and classify
them. While various concepts have been proposed from existing research, we
observe that these individual concepts are not mutually exclusive but
complementary. In this paper, we propose a new state-of-the-art solution for
OV-3DIS by carefully designing a recipe to combine the concepts together and
refining them to address key challenges. Our solution follows the two-stage
scheme: 3D proposal generation and instance classification. We employ robust 3D
tracking-based proposal aggregation to generate 3D proposals and remove
overlapped or partial proposals by iterative merging/removal. For the
classification stage, we replace the standard CLIP model with Alpha-CLIP, which
incorporates object masks as an alpha channel to reduce background noise and
obtain object-centric representation. Additionally, we introduce the
standardized maximum similarity (SMS) score to normalize text-to-proposal
similarity, effectively filtering out false positives and boosting precision.
Our framework achieves state-of-the-art performance on ScanNet200 and S3DIS
across all AP and AR metrics, even surpassing an end-to-end closed-vocabulary
method.

</details>


### [12] [X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention](https://arxiv.org/abs/2507.23143)
*Xiaochen Zhao,Hongyi Xu,Guoxian Song,You Xie,Chenxu Zhang,Xiu Li,Linjie Luo,Jinli Suo,Yebin Liu*

Main category: cs.CV

TL;DR: X-NeMo是一种新的零样本肖像动画技术，通过从驱动视频中提取的1D潜在运动描述符来驱动静态肖像的动画，解决了身份泄露和表情捕捉的挑战，并在实验中取得了优于现有技术的成果。


<details>
  <summary>Details</summary>
Motivation: 解决现有肖像动画技术在身份泄露和捕捉细微/极端表情方面的不足。

Method: X-NeMo是一种新颖的基于扩散的零样本肖像动画流程，它使用来自不同个体驱动视频的面部运动来驱动静态肖像动画。该方法首先识别先前方法中的关键问题（如身份泄露和捕捉细微和极端表情的困难），然后引入一个完全端到端的训练框架，从驱动图像中提取1D身份无关的潜在运动描述符，通过交叉注意力在图像生成过程中有效控制运动。该隐式运动描述符以精细的细节捕捉富有表现力的面部运动，并且可以从多样化的视频数据集中进行端到端的学习，而无需依赖预训练的运动检测器。此外，通过使用双GAN解码器以及空间和颜色增强来监督其学习，进一步增强了表现力和将运动潜在特征与身份线索分离。通过将驱动运动嵌入1D潜在向量并通过交叉注意力控制运动，而不是添加空间引导，该设计消除了从驱动条件到扩散骨干的空间对齐结构线索的传输，从而大大减轻了身份泄露。

Result: X-NeMo在富有表现力的动画和身份相似性方面优于最先进的方法。

Conclusion: X-NeMo

Abstract: We propose X-NeMo, a novel zero-shot diffusion-based portrait animation
pipeline that animates a static portrait using facial movements from a driving
video of a different individual. Our work first identifies the root causes of
the key issues in prior approaches, such as identity leakage and difficulty in
capturing subtle and extreme expressions. To address these challenges, we
introduce a fully end-to-end training framework that distills a 1D
identity-agnostic latent motion descriptor from driving image, effectively
controlling motion through cross-attention during image generation. Our
implicit motion descriptor captures expressive facial motion in fine detail,
learned end-to-end from a diverse video dataset without reliance on pretrained
motion detectors. We further enhance expressiveness and disentangle motion
latents from identity cues by supervising their learning with a dual GAN
decoder, alongside spatial and color augmentations. By embedding the driving
motion into a 1D latent vector and controlling motion via cross-attention
rather than additive spatial guidance, our design eliminates the transmission
of spatial-aligned structural clues from the driving condition to the diffusion
backbone, substantially mitigating identity leakage. Extensive experiments
demonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly
expressive animations with superior identity resemblance. Our code and models
are available for research.

</details>


### [13] [Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues](https://arxiv.org/abs/2507.23162)
*Xu Cao,Takafumi Taketomi*

Main category: cs.CV

TL;DR: 我们提出了一种新颖的神经逆渲染方法，可以从多视图图像中联合重建几何、反射率和光照。


<details>
  <summary>Details</summary>
Motivation: 与需要光照校准或每视图法线图等中间线索的先前多视图光度立体法不同，我们的方法在单个阶段从原始图像中联合优化所有场景参数。

Method: 我们使用空间网络来预测每个场景点的符号距离和反射率潜码。然后，反射率网络根据潜码以及角度编码的表面法线、视图和光照方向来估计反射率值。该方法采用阴影感知体积渲染。

Result: 我们的方法在形状和光照估计精度方面优于最先进的法线引导方法，并且能够泛化到视图未对齐的多光图像。

Conclusion: 该方法在形状和光照估计精度方面优于最先进的法线引导方法，能够泛化到视图未对齐的多光图像，并处理具有挑战性的几何形状和反射率的对象。

Abstract: We propose a neural inverse rendering approach that jointly reconstructs
geometry, spatially varying reflectance, and lighting conditions from
multi-view images captured under varying directional lighting. Unlike prior
multi-view photometric stereo methods that require light calibration or
intermediate cues such as per-view normal maps, our method jointly optimizes
all scene parameters from raw images in a single stage. We represent both
geometry and reflectance as neural implicit fields and apply shadow-aware
volume rendering. A spatial network first predicts the signed distance and a
reflectance latent code for each scene point. A reflectance network then
estimates reflectance values conditioned on the latent code and angularly
encoded surface normal, view, and light directions. The proposed method
outperforms state-of-the-art normal-guided approaches in shape and lighting
estimation accuracy, generalizes to view-unaligned multi-light images, and
handles objects with challenging geometry and reflectance.

</details>


### [14] [CNN-based solution for mango classification in agricultural environments](https://arxiv.org/abs/2507.23174)
*Beatriz Díaz Peón,Jorge Torres Gómez,Ariel Fajardo Márquez*

Main category: cs.CV

TL;DR: 本研究利用CNN（Resnet-18）和级联检测器开发了一个芒果检测与分类系统，并通过MatLab App Designer实现了图形用户界面，旨在提高农业库存管理的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了实现农场库存管理的自动化，提高水果质量评估的准确性和效率，本研究旨在开发一个自动评估水果质量的系统。

Method: 本研究采用卷积神经网络（CNN）进行水果检测与分类，具体使用了Resnet-18作为分类架构，并结合了级联检测器来平衡检测速度和计算资源消耗。系统通过MatLab App Designer开发了图形用户界面来展示检测和分类结果。

Result: 研究成功开发了一个水果检测和分类系统，能够准确有效地对芒果进行分类，并通过图形用户界面直观展示了检测和分类结果。

Conclusion: 该研究成功设计了一个利用卷积神经网络（CNN）进行水果检测和分类的系统，特别是在芒果分类方面取得了良好的效果，并集成了图形用户界面，为农业质量控制提供了一个可靠的解决方案。

Abstract: This article exemplifies the design of a fruit detection and classification
system using Convolutional
  Neural Networks (CNN). The goal is to develop a system that automatically
assesses fruit quality for
  farm inventory management. Specifically, a method for mango fruit
classification was developed using
  image processing, ensuring both accuracy and efficiency. Resnet-18 was
selected as the preliminary
  architecture for classification, while a cascade detector was used for
detection, balancing execution speed
  and computational resource consumption. Detection and classification results
were displayed through a
  graphical interface developed in MatLab App Designer, streamlining system
interaction. The integration
  of convolutional neural networks and cascade detectors proffers a reliable
solution for fruit classification
  and detection, with potential applications in agricultural quality control.

</details>


### [15] [Single Image Rain Streak Removal Using Harris Corner Loss and R-CBAM Network](https://arxiv.org/abs/2507.23185)
*Jongwook Si,Sungyoung Kim*

Main category: cs.CV

TL;DR: 提出了一种新颖的图像恢复网络，通过引入Corner Loss和R-CBAM模块，在保留细节和处理雨条纹方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单图像雨条纹去除不仅需要抑制噪声，还需要同时保留精细的结构细节和整体视觉质量。

Method: 提出了一种新颖的图像恢复网络，通过引入 "Corner Loss" 来约束恢复过程，防止在恢复过程中丢失对象边界和详细纹理信息。此外，在编码器和解码器中引入了 "残差卷积块注意模块" (R-CBAM)，以动态调整空间和通道维度上特征的重要性，使网络能够更有效地关注受雨条纹影响严重的区域。

Result: 所提出的方法显著优于先前的方法，在Rain100L上达到了33.29 dB的PSNR，在Rain100H上达到了26.16 dB。

Conclusion: 所提出的方法在Rain100L和Rain100H数据集上进行了量化评估，结果显著优于先前的方法，在Rain100L上达到了33.29 dB的PSNR，在Rain100H上达到了26.16 dB。

Abstract: The problem of single-image rain streak removal goes beyond simple noise
suppression, requiring the simultaneous preservation of fine structural details
and overall visual quality. In this study, we propose a novel image restoration
network that effectively constrains the restoration process by introducing a
Corner Loss, which prevents the loss of object boundaries and detailed texture
information during restoration. Furthermore, we propose a Residual
Convolutional Block Attention Module (R-CBAM) Block into the encoder and
decoder to dynamically adjust the importance of features in both spatial and
channel dimensions, enabling the network to focus more effectively on regions
heavily affected by rain streaks. Quantitative evaluations conducted on the
Rain100L and Rain100H datasets demonstrate that the proposed method
significantly outperforms previous approaches, achieving a PSNR of 33.29 dB on
Rain100L and 26.16 dB on Rain100H.

</details>


### [16] [Multi-Modal Motion Retrieval by Learning a Fine-Grained Joint Embedding Space](https://arxiv.org/abs/2507.23188)
*Shiyao Yu,Zi-An Wang,Kangning Yin,Zheng Tian,Mingyuan Zhang,Weixin Si,Shihao Zou*

Main category: cs.CV

TL;DR: 本研究提出了一种包含文本、音频、视频和动作四种模态的动作检索框架，通过序列级对比学习实现细粒度联合嵌入，并首次引入音频模态，在多个检索任务上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本或视觉模态的动作检索中，缺乏更直观友好的交互模式，并且忽略了顺序表示对检索性能的提升。为了解决这些限制，本研究旨在通过整合多种模态（包括首次引入的音频）来改进动作检索。

Method: 提出了一种新框架，通过序列级对比学习将文本、音频、视频和动作四种模态对齐到一个细粒度的联合嵌入空间，首次将音频引入动作检索。

Result: 实验结果表明，在HumanML3D数据集上，与现有最先进的方法相比，文本到动作检索的R@10提高了10.16%，视频到动作检索的R@1提高了25.43%。四模态框架的性能显著优于三模态框架。

Conclusion: 该框架在多模态动作检索方面表现出优越性能，特别是在文本到动作检索和视频到动作检索方面，并强调了多模态方法相比三模态方法的优势。

Abstract: Motion retrieval is crucial for motion acquisition, offering superior
precision, realism, controllability, and editability compared to motion
generation. Existing approaches leverage contrastive learning to construct a
unified embedding space for motion retrieval from text or visual modality.
However, these methods lack a more intuitive and user-friendly interaction mode
and often overlook the sequential representation of most modalities for
improved retrieval performance. To address these limitations, we propose a
framework that aligns four modalities -- text, audio, video, and motion --
within a fine-grained joint embedding space, incorporating audio for the first
time in motion retrieval to enhance user immersion and convenience. This
fine-grained space is achieved through a sequence-level contrastive learning
approach, which captures critical details across modalities for better
alignment. To evaluate our framework, we augment existing text-motion datasets
with synthetic but diverse audio recordings, creating two multi-modal motion
retrieval datasets. Experimental results demonstrate superior performance over
state-of-the-art methods across multiple sub-tasks, including an 10.16%
improvement in R@10 for text-to-motion retrieval and a 25.43% improvement in
R@1 for video-to-motion retrieval on the HumanML3D dataset. Furthermore, our
results show that our 4-modal framework significantly outperforms its 3-modal
counterpart, underscoring the potential of multi-modal motion retrieval for
advancing motion acquisition.

</details>


### [17] [A Novel Dataset for Flood Detection Robust to Seasonal Changes in Satellite Imagery](https://arxiv.org/abs/2507.23193)
*Youngsun Jang,Dongyoun Kim,Chulwoo Pack,Kwanghee Won*

Main category: cs.CV

TL;DR: 介绍了一个用于卫星图像洪水区域分割的新数据集，并评估了现有模型的性能，结果表明需要进一步研究多模态和时间学习。


<details>
  <summary>Details</summary>
Motivation: 现有卫星图像基准中缺乏用于洪水区域分割的合适数据集。

Method: 收集了2019年中西部洪水事件的卫星图像，构建了一个包含10张卫星图像（包含洪水和非洪水区域）的新数据集，涵盖了五个州（爱荷华州、堪萨斯州、蒙大拿州、内布拉斯加州和南达科他州）的十个地点，并确保了统一的分辨率和大小调整。在数据集上评估了最先进的计算机视觉和遥感模型，并进行了窗口大小变化消融研究以捕捉时间特征。

Result: 最先进的模型在该数据集上取得了中等水平的结果。

Conclusion: 现有模型在洪水分割任务上表现平平，预示着未来需要多模态和时间学习策略。

Abstract: This study introduces a novel dataset for segmenting flooded areas in
satellite images. After reviewing 77 existing benchmarks utilizing satellite
imagery, we identified a shortage of suitable datasets for this specific task.
To fill this gap, we collected satellite imagery of the 2019 Midwestern USA
floods from Planet Explorer by Planet Labs (Image \c{opyright} 2024 Planet Labs
PBC). The dataset consists of 10 satellite images per location, each containing
both flooded and non-flooded areas. We selected ten locations from each of the
five states: Iowa, Kansas, Montana, Nebraska, and South Dakota. The dataset
ensures uniform resolution and resizing during data processing. For evaluating
semantic segmentation performance, we tested state-of-the-art models in
computer vision and remote sensing on our dataset. Additionally, we conducted
an ablation study varying window sizes to capture temporal characteristics.
Overall, the models demonstrated modest results, suggesting a requirement for
future multimodal and temporal learning strategies. The dataset will be
publicly available on
<https://github.com/youngsunjang/SDSU_MidWest_Flood_2019>.

</details>


### [18] [Adversarial-Guided Diffusion for Multimodal LLM Attacks](https://arxiv.org/abs/2507.23202)
*Chengwei Xia,Fan Ma,Ruijie Quan,Kun Zhan,Yi Yang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为AGD的新方法，使用扩散模型生成对抗性图像来欺骗MLLMs。AGD通过将攻击性信号注入噪声分量，使其能够抵抗常见的防御措施，并在实验中表现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决利用扩散模型生成对抗性图像以欺骗多模态大型语言模型（MLLMs）生成目标响应的挑战，同时要避免对原始图像造成显著的失真。

Method: 提出了一种名为“对抗性引导扩散”（AGD）的方法，通过在扩散模型的反向过程中注入引导性噪声来生成对抗性图像，以欺骗多模态大型语言模型（MLLMs）。该方法将目标语义注入到噪声分量中，使其具有全频谱特性，并能在对抗性图像的形成过程中，即使在低通滤波等防御措施下也能保持其有效性，从而实现对防御措施的固有鲁棒性。

Result: 实验证明，AGD方法在攻击性能和对某些防御措施的鲁棒性方面均优于现有技术。

Conclusion: AGD方法在攻击性能和对某些防御措施的鲁棒性方面优于最先进的方法。

Abstract: This paper addresses the challenge of generating adversarial image using a
diffusion model to deceive multimodal large language models (MLLMs) into
generating the targeted responses, while avoiding significant distortion of the
clean image. To address the above challenges, we propose an adversarial-guided
diffusion (AGD) approach for adversarial attack MLLMs. We introduce
adversarial-guided noise to ensure attack efficacy. A key observation in our
design is that, unlike most traditional adversarial attacks which embed
high-frequency perturbations directly into the clean image, AGD injects target
semantics into the noise component of the reverse diffusion. Since the added
noise in a diffusion model spans the entire frequency spectrum, the adversarial
signal embedded within it also inherits this full-spectrum property.
Importantly, during reverse diffusion, the adversarial image is formed as a
linear combination of the clean image and the noise. Thus, when applying
defenses such as a simple low-pass filtering, which act independently on each
component, the adversarial image within the noise component is less likely to
be suppressed, as it is not confined to the high-frequency band. This makes AGD
inherently robust to variety defenses. Extensive experiments demonstrate that
our AGD outperforms state-of-the-art methods in attack performance as well as
in model robustness to some defenses.

</details>


### [19] [Confidence-aware agglomeration classification and segmentation of 2D microscopic food crystal images](https://arxiv.org/abs/2507.23206)
*Xiaoyu Ji,Ali Shakouri,Fengqing Zhu*

Main category: cs.CV

TL;DR: 一种新的图像分析方法，用于改善食品晶体附聚物的检测和分类。


<details>
  <summary>Details</summary>
Motivation: 解决手动标注2D显微图像中的食物晶体附聚物（由于水键的透明性和有限的视角）所面临的困难。

Method: 提出了一种监督基线模型来为粗略标记的分类数据集生成分割伪标签，然后训练了一个同时执行像素级分割的实例分类模型。最后，通过一个后处理模块来保留晶体属性，并在推理阶段结合这两个模型的优势。

Result: 与现有方法相比，该方法提高了真实阳性附聚物分类的准确性以及尺寸分布预测的准确性，并且能够成功分类潜在的附聚实例。

Conclusion: 该方法在分类和分割方面都优于现有方法，提高了附聚物分类准确性和尺寸分布预测的准确性。该模型在两个置信度水平下均能成功对潜在的附聚实例进行分类。

Abstract: Food crystal agglomeration is a phenomenon occurs during crystallization
which traps water between crystals and affects food product quality. Manual
annotation of agglomeration in 2D microscopic images is particularly difficult
due to the transparency of water bonding and the limited perspective focusing
on a single slide of the imaged sample. To address this challenge, we first
propose a supervised baseline model to generate segmentation pseudo-labels for
the coarsely labeled classification dataset. Next, an instance classification
model that simultaneously performs pixel-wise segmentation is trained. Both
models are used in the inference stage to combine their respective strengths in
classification and segmentation. To preserve crystal properties, a post
processing module is designed and included to both steps. Our method improves
true positive agglomeration classification accuracy and size distribution
predictions compared to other existing methods. Given the variability in
confidence levels of manual annotations, our proposed method is evaluated under
two confidence levels and successfully classifies potential agglomerated
instances.

</details>


### [20] [YOLO-ROC: A High-Precision and Ultra-Lightweight Model for Real-Time Road Damage Detection](https://arxiv.org/abs/2507.23225)
*Zicheng Lin,Weichao Pan*

Main category: cs.CV

TL;DR: 该研究提出了一种名为YOLO-ROC的轻量级模型，通过改进特征提取和通道压缩，有效解决了道路损伤检测中小目标漏检和模型计算量大的问题，并在准确性和效率上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在道路损伤检测中存在多尺度特征提取能力不足（导致小目标漏检率高）和模型参数量大、计算量大的问题，限制了其在实际应用中的高效实时部署。

Method: 提出了一种名为YOLO-ROC的高精度轻量级模型，通过设计双向多尺度空间金字塔池化快速（BMS-SPPF）模块来增强多尺度特征提取，并采用分层通道压缩策略来降低计算复杂度。BMS-SPPF模块利用双向空间通道注意力机制来提升小目标检测性能。

Result: YOLO-ROC模型的参数量从3.01M减少到0.89M，GFLOPs从8.1减少到2.6。在RDD2022_China_Drone数据集上，mAP50达到67.6%，比YOLOv8n高2.11%，D40类别mAP50提升16.8%，模型大小为2.0MB。

Conclusion: YOLO-ROC模型在RDD2022_China_Drone数据集上实现了67.6%的mAP50，超过了基线YOLOv8n 2.11%，并且在小目标D40类别上的mAP50提升了16.8%，模型大小仅为2.0MB，同时在RDD2022_China_Motorbike数据集上表现出良好的泛化能力。

Abstract: Road damage detection is a critical task for ensuring traffic safety and
maintaining infrastructure integrity. While deep learning-based detection
methods are now widely adopted, they still face two core challenges: first, the
inadequate multi-scale feature extraction capabilities of existing networks for
diverse targets like cracks and potholes, leading to high miss rates for
small-scale damage; and second, the substantial parameter counts and
computational demands of mainstream models, which hinder their deployment for
efficient, real-time detection in practical applications. To address these
issues, this paper proposes a high-precision and lightweight model, YOLO - Road
Orthogonal Compact (YOLO-ROC). We designed a Bidirectional Multi-scale Spatial
Pyramid Pooling Fast (BMS-SPPF) module to enhance multi-scale feature
extraction and implemented a hierarchical channel compression strategy to
reduce computational complexity. The BMS-SPPF module leverages a bidirectional
spatial-channel attention mechanism to improve the detection of small targets.
Concurrently, the channel compression strategy reduces the parameter count from
3.01M to 0.89M and GFLOPs from 8.1 to 2.6. Experiments on the
RDD2022_China_Drone dataset demonstrate that YOLO-ROC achieves a mAP50 of
67.6%, surpassing the baseline YOLOv8n by 2.11%. Notably, the mAP50 for the
small-target D40 category improved by 16.8%, and the final model size is only
2.0 MB. Furthermore, the model exhibits excellent generalization performance on
the RDD2022_China_Motorbike dataset.

</details>


### [21] [Toward Safe, Trustworthy and Realistic Augmented Reality User Experience](https://arxiv.org/abs/2507.23226)
*Yanming Xiu*

Main category: cs.CV

TL;DR: 本研究提出ViDDAR和VIM-Sense系统，利用视觉-语言模型检测可能损害任务的AR内容，并展望了未来在AR内容安全评估和部署方面的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AR日益融入日常生活，确保其虚拟内容的安全性与可信度至关重要。本研究着重解决了AR内容带来的风险，特别是那些会阻碍关键信息或微妙操纵用户感知的风险。

Method: 研究开发了ViDDAR和VIM-Sense两个系统，利用视觉-语言模型（VLMs）和多模态推理模块来检测可能损害任务的AR内容，特别是那些会遮挡关键信息或微妙地操纵用户感知的攻击。

Result: 研究提出三个未来发展方向：虚拟内容的自动化、感知一致的质量评估；多模态攻击的检测；以及适应VLMs以在AR设备上进行高效且以用户为中心的安全部署。

Conclusion: 本研究旨在建立一个可扩展的、以人为本的框架来保护增强现实（AR）体验，并寻求在感知建模、多模态AR内容实现和轻量级模型适应方面的反馈。

Abstract: As augmented reality (AR) becomes increasingly integrated into everyday life,
ensuring the safety and trustworthiness of its virtual content is critical. Our
research addresses the risks of task-detrimental AR content, particularly that
which obstructs critical information or subtly manipulates user perception. We
developed two systems, ViDDAR and VIM-Sense, to detect such attacks using
vision-language models (VLMs) and multimodal reasoning modules. Building on
this foundation, we propose three future directions: automated, perceptually
aligned quality assessment of virtual content; detection of multimodal attacks;
and adaptation of VLMs for efficient and user-centered deployment on AR
devices. Overall, our work aims to establish a scalable, human-aligned
framework for safeguarding AR experiences and seeks feedback on perceptual
modeling, multimodal AR content implementation, and lightweight model
adaptation.

</details>


### [22] [Ambiguity-Guided Learnable Distribution Calibration for Semi-Supervised Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2507.23237)
*Fan Lyu,Linglan Zhao,Chengyan Liu,Yinying Mei,Zhang Zhang,Jian Zhang,Fuyuan Hu,Liang Wang*

Main category: cs.CV

TL;DR: 该研究提出了ALDC策略，用于广义半监督少样本增量学习（GSemi-FSCIL），通过利用基础类别样本校准新类别特征分布，解决了未标记样本来源区分难题，并在实验中取得了优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 现有半监督少样本增量学习（Semi-FSCIL）研究假设未标记数据仅限于当前会话的新类，这与实际情况不符。为了更好地反映现实场景，研究将Semi-FSCIL重新定义为广义Semi-FSCIL（GSemi-FSCIL），将基础类别和以往所有已见新类都包含在未标记数据集中，这给现有方法带来了区分未标记样本来源的挑战。

Method: 提出了一种名为“模糊引导可学习分布校准”（ALDC）的策略，该策略动态地利用丰富的基础样本来校准新类别的特征分布。

Result: 在三个基准数据集上的实验结果显示，该方法优于现有方法，并设定了新的最先进水平。

Conclusion: 该研究提出的ALDC策略通过利用基础类别样本动态校准学习到的新类别的特征分布，解决了现有方法在区分来自基础和新类别的未标记样本时的挑战。实验结果表明，该方法在三个基准数据集上优于现有方法，并取得了新的最先进成果。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) focuses on models learning new
concepts from limited data while retaining knowledge of previous classes.
Recently, many studies have started to leverage unlabeled samples to assist
models in learning from few-shot samples, giving rise to the field of
Semi-supervised Few-shot Class-Incremental Learning (Semi-FSCIL). However,
these studies often assume that the source of unlabeled data is only confined
to novel classes of the current session, which presents a narrow perspective
and cannot align well with practical scenarios. To better reflect real-world
scenarios, we redefine Semi-FSCIL as Generalized Semi-FSCIL (GSemi-FSCIL) by
incorporating both base and all the ever-seen novel classes in the unlabeled
set. This change in the composition of unlabeled samples poses a new challenge
for existing methods, as they struggle to distinguish between unlabeled samples
from base and novel classes. To address this issue, we propose an
Ambiguity-guided Learnable Distribution Calibration (ALDC) strategy. ALDC
dynamically uses abundant base samples to correct biased feature distributions
for few-shot novel classes. Experiments on three benchmark datasets show that
our method outperforms existing works, setting new state-of-the-art results.

</details>


### [23] [Generalized Reinforcement Learning for Retriever-Specific Query Rewriter with Unstructured Real-World Documents](https://arxiv.org/abs/2507.23242)
*Sungguk Cha,DongWook Kim,Taeseung Hahn,Mintae Kim,Youngsub Han,Byoung-Ki Jeon*

Main category: cs.CV

TL;DR: RL-QR是一个无需人工标注的强化学习框架，用于优化RAG系统的查询重写，在多模态和词汇检索方面取得了显著成效，但在语义检索方面仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）系统在查询优化方面存在挑战，尤其是在处理多样化、非结构化的真实世界文档时。因此，需要一种更有效的方法来优化查询，以解锁外部知识。

Method: RL-QR框架使用强化学习，通过合成场景-问题对并利用广义奖励策略优化（GRPO）来训练查询重写器，使其能够适应特定的检索器，从而优化查询。

Result: RL-QR框架在工业数据上的实验取得了显著的改进，其中RL-QR_multi-modal在多模态RAG的NDCG@3方面实现了11%的相对提升，而RL-QR_lexical在词汇检索器方面实现了9%的提升。

Conclusion: RL-QR框架能够为检索器特定的查询重写进行优化，该框架无需人工标注数据集，可应用于文本和多模态数据库。实验证明，RL-QR在多模态和词汇检索方面显著提升了NDCG@3指标。然而，在语义和混合检索方面，RL-QR未能提升性能，这可能是由于训练不匹配。研究结果表明RL-QR在为RAG系统优化查询方面具有巨大潜力，可以为现实世界的检索任务提供可扩展且无需标注的解决方案，并指出了在语义检索方面进一步改进的方向。

Abstract: Retrieval-Augmented Generation (RAG) systems rely heavily on effective query
formulation to unlock external knowledge, yet optimizing queries for diverse,
unstructured real-world documents remains a challenge. We introduce
\textbf{RL-QR}, a reinforcement learning framework for retriever-specific query
rewriting that eliminates the need for human-annotated datasets and extends
applicability to both text-only and multi-modal databases. By synthesizing
scenario-question pairs and leveraging Generalized Reward Policy Optimization
(GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing
retrieval performance across varied domains. Experiments on industrial in-house
data demonstrate significant improvements, with
$\text{RL-QR}_{\text{multi-modal}}$ achieving an 11\% relative gain in NDCG@3
for multi-modal RAG and $\text{RL-QR}_{\text{lexical}}$ yielding a 9\% gain for
lexical retrievers. However, challenges persist with semantic and hybrid
retrievers, where rewriters failed to improve performance, likely due to
training misalignments. Our findings highlight RL-QR's potential to
revolutionize query optimization for RAG systems, offering a scalable,
annotation-free solution for real-world retrieval tasks, while identifying
avenues for further refinement in semantic retrieval contexts.

</details>


### [24] [Automated Mapping the Pathways of Cranial Nerve II, III, V, and VII/VIII: A Multi-Parametric Multi-Stage Diffusion Tractography Atlas](https://arxiv.org/abs/2507.23245)
*Lei Xie,Jiahao Huang,Jiawei Zhang,Jianzhong He,Yiang Pan,Guoqiang Xie,Mengjun Li,Qingrun Zeng,Mingchu Li,Yuanjing Feng*

Main category: cs.CV

TL;DR: 本研究开发了首个颅神经通路弥散示踪图集，通过多阶段纤维聚类技术，实现了对5对颅神经的自动化精确映射，为神经外科术前规划提供了重要工具。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有颅神经图谱绘制的挑战，即个体颅神经解剖结构的独特性和颅底环境的复杂性，本研究旨在开发一个全面的弥散示踪图集，以实现颅神经通路的自动化映射，并提供有价值的术前解剖信息。

Method: 通过对来自50名人类连接组计划（HCP）受试者的数据进行多参数纤维追踪，生成海量流线，并采用新颖的多阶段纤维聚类策略，对约1,000,000条流线进行分析，构建颅神经图集。

Result: 研究成功构建了一个颅神经图集，能够自动识别与视神经（CN II）、动眼神经（CN III）、三叉神经（CN V）以及面听神经（CN VII/VIII）相关的8个纤维束。实验结果表明，该图集在HCP数据集、多贝弥散MRI（MDM）数据集以及两例垂体腺瘤患者的临床病例中，均能与专家手动标注实现高度空间对应，证明了其稳健性。

Conclusion: 本研究提出了首个用于自动化映射人类大脑颅神经（CNs）通路的全面的弥散示踪图集，通过多阶段纤维聚类方法，能够自动识别8个与5对颅神经相关的纤维束，并在多个数据集和临床病例中验证了其高空间准确性和稳健性。

Abstract: Cranial nerves (CNs) play a crucial role in various essential functions of
the human brain, and mapping their pathways from diffusion MRI (dMRI) provides
valuable preoperative insights into the spatial relationships between
individual CNs and key tissues. However, mapping a comprehensive and detailed
CN atlas is challenging because of the unique anatomical structures of each CN
pair and the complexity of the skull base environment.In this work, we present
what we believe to be the first study to develop a comprehensive diffusion
tractography atlas for automated mapping of CN pathways in the human brain. The
CN atlas is generated by fiber clustering by using the streamlines generated by
multi-parametric fiber tractography for each pair of CNs. Instead of disposable
clustering, we explore a new strategy of multi-stage fiber clustering for
multiple analysis of approximately 1,000,000 streamlines generated from the 50
subjects from the Human Connectome Project (HCP). Quantitative and visual
experiments demonstrate that our CN atlas achieves high spatial correspondence
with expert manual annotations on multiple acquisition sites, including the HCP
dataset, the Multi-shell Diffusion MRI (MDM) dataset and two clinical cases of
pituitary adenoma patients. The proposed CN atlas can automatically identify 8
fiber bundles associated with 5 pairs of CNs, including the optic nerve CN II,
oculomotor nerve CN III, trigeminal nerve CN V and facial-vestibulocochlear
nerve CN VII/VIII, and its robustness is demonstrated experimentally. This work
contributes to the field of diffusion imaging by facilitating more efficient
and automated mapping the pathways of multiple pairs of CNs, thereby enhancing
the analysis and understanding of complex brain structures through
visualization of their spatial relationships with nearby anatomy.

</details>


### [25] [A Deep Dive into Generic Object Tracking: A Survey](https://arxiv.org/abs/2507.23251)
*Fereshteh Aghaee Meibodi,Shadi Alijani,Homayoun Najjaran*

Main category: cs.CV

TL;DR: 该论文全面回顾了目标跟踪技术，重点关注了基于Transformer的方法，并进行了详细的分析和比较。


<details>
  <summary>Details</summary>
Motivation: 通用目标跟踪在计算机视觉领域仍然是一个重要但充满挑战的任务，需要应对复杂的时空动态，尤其是在遮挡、相似干扰和外观变化的情况下。

Method: 对现有文献进行回顾和分析，包括定性和定量比较，并对跟踪器进行多角度组织和总结。

Result: 论文提出了新的分类方法，并对代表性方法进行了统一的视觉和表格比较，总结了跟踪器的组织方式和主要的评估基准，突出了基于Transformer的跟踪方法的快速发展。

Conclusion: 该论文全面回顾了目标跟踪领域，特别是基于Transformer的方法，分析了不同方法的优缺点，并提出了新的分类和比较方式。

Abstract: Generic object tracking remains an important yet challenging task in computer
vision due to complex spatio-temporal dynamics, especially in the presence of
occlusions, similar distractors, and appearance variations. Over the past two
decades, a wide range of tracking paradigms, including Siamese-based trackers,
discriminative trackers, and, more recently, prominent transformer-based
approaches, have been introduced to address these challenges. While a few
existing survey papers in this field have either concentrated on a single
category or widely covered multiple ones to capture progress, our paper
presents a comprehensive review of all three categories, with particular
emphasis on the rapidly evolving transformer-based methods. We analyze the core
design principles, innovations, and limitations of each approach through both
qualitative and quantitative comparisons. Our study introduces a novel
categorization and offers a unified visual and tabular comparison of
representative methods. Additionally, we organize existing trackers from
multiple perspectives and summarize the major evaluation benchmarks,
highlighting the fast-paced advancements in transformer-based tracking driven
by their robust spatio-temporal modeling capabilities.

</details>


### [26] [Towards Measuring and Modeling Geometric Structures in Time Series Forecasting via Image Modality](https://arxiv.org/abs/2507.23253)
*Mingyang Yu,Xiahui Guo,Peng chen,Zhenkai Li,Yang Shu*

Main category: cs.CV

TL;DR: 提出TGSI和SATL来评估和改进时间序列的几何结构建模，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的时间序列预测评估指标（如MSE）无法评估数据的时间动态性，而几何结构对于理解时间序列至关重要。

Method: 提出时间序列几何结构指数（TGSI）将时间序列转换为图像以评估其几何结构，并引入形状感知时间序列损失（SATL）以解决TGSI的不可微问题。SATL包含三个组成部分：一阶差分损失、频域损失和感知特征损失。

Result: 实验结果表明，使用SATL训练的模型在MSE和TGSI指标上均取得了优于基线方法的性能。

Conclusion: SATL通过结合一阶差分、频域和感知特征损失，能够有效提升时间序列的几何结构建模能力，在MSE和TGSI指标上均优于基线方法，且在推理时无额外计算成本。

Abstract: Time Series forecasting is critical in diverse domains such as weather
forecasting, financial investment, and traffic management. While traditional
numerical metrics like mean squared error (MSE) can quantify point-wise
accuracy, they fail to evaluate the geometric structure of time series data,
which is essential to understand temporal dynamics. To address this issue, we
propose the time series Geometric Structure Index (TGSI), a novel evaluation
metric that transforms time series into images to leverage their inherent
two-dimensional geometric representations. However, since the image
transformation process is non-differentiable, TGSI cannot be directly
integrated as a training loss. We further introduce the Shape-Aware Temporal
Loss (SATL), a multi-component loss function operating in the time series
modality to bridge this gap and enhance structure modeling during training.
SATL combines three components: a first-order difference loss that measures
structural consistency through the MSE between first-order differences, a
frequency domain loss that captures essential periodic patterns using the Fast
Fourier Transform while minimizing noise, and a perceptual feature loss that
measures geometric structure difference in time-series by aligning temporal
features with geometric structure features through a pre-trained temporal
feature extractor and time-series image autoencoder. Experiments across
multiple datasets demonstrate that models trained with SATL achieve superior
performance in both MSE and the proposed TGSI metrics compared to baseline
methods, without additional computational cost during inference.

</details>


### [27] [Learning Semantic-Aware Threshold for Multi-Label Image Recognition with Partial Labels](https://arxiv.org/abs/2507.23263)
*Haoxian Ruan,Zhihua Xu,Zhijing Yang,Guang Ma,Jieming Xie,Changxiang Fan,Tianshui Chen*

Main category: cs.CV

TL;DR: SATL算法通过为每个类别计算正负样本的分数分布，并动态更新类别特定的阈值，同时利用差分秩损失来增强区分度，解决了传统方法中伪标签不准确的问题，在标签有限的情况下提高了多标签图像识别的性能。


<details>
  <summary>Details</summary>
Motivation: 传统多标签图像识别（MLR-PL）方法依赖于语义或特征相关性来为未识别标签创建伪标签，但这种方法常忽略类别间评分分布的变化，导致伪标签不准确且不完整，影响性能。

Method: 提出了一种名为SATL（Semantic-Aware Threshold Learning）的算法，该算法计算正负样本在每个类别上的分数分布，并根据这些分布确定特定类别的阈值。这些分布和阈值在学习过程中动态更新。此外，还实现了一种差分秩损失，以在正负样本的分数分布之间建立显著的差距，增强阈值的辨别能力。

Result: 在Microsoft COCO和VG-200等大规模多标签数据集上的综合实验和分析表明，SATL方法在标签有限的场景下显著提高了性能。

Conclusion: SATL通过计算正负样本在各类别上的分数分布并基于这些分布确定特定类别阈值，并结合差分秩损失来弥合正负样本分数分布的差距，从而提升了在标签有限场景下的性能。

Abstract: Multi-label image recognition with partial labels (MLR-PL) is designed to
train models using a mix of known and unknown labels. Traditional methods rely
on semantic or feature correlations to create pseudo-labels for unidentified
labels using pre-set thresholds. This approach often overlooks the varying
score distributions across categories, resulting in inaccurate and incomplete
pseudo-labels, thereby affecting performance. In our study, we introduce the
Semantic-Aware Threshold Learning (SATL) algorithm. This innovative approach
calculates the score distribution for both positive and negative samples within
each category and determines category-specific thresholds based on these
distributions. These distributions and thresholds are dynamically updated
throughout the learning process. Additionally, we implement a differential
ranking loss to establish a significant gap between the score distributions of
positive and negative samples, enhancing the discrimination of the thresholds.
Comprehensive experiments and analysis on large-scale multi-label datasets,
such as Microsoft COCO and VG-200, demonstrate that our method significantly
improves performance in scenarios with limited labels.

</details>


### [28] [PixNerd: Pixel Neural Field Diffusion](https://arxiv.org/abs/2507.23268)
*Shuai Wang,Ziteng Gao,Chenhui Zhu,Weilin Huang,Limin Wang*

Main category: cs.CV

TL;DR: PixelNerd is an efficient, single-stage diffusion model that uses neural fields for patch-wise decoding, outperforming VAE-based methods and complex pixel-space approaches on image generation tasks.


<details>
  <summary>Details</summary>
Motivation: Addresses accumulated errors and decoding artifacts in two-stage diffusion transformer training paradigms that rely on VAEs, and the complexity of pixel-space methods.

Method: Proposes PixelNerd, a single-scale, single-stage, efficient, end-to-end solution that models patch-wise decoding with neural fields.

Result: Achieved 2.15 FID on ImageNet 256x256 and 2.84 FID on ImageNet 512x512. Extended PixNerd framework to text-to-image applications, achieving 0.73 overall score on GenEval and 80.9 overall score on DPG.

Conclusion: PixNerd's neural field representation enables direct modeling of patch-wise decoding, achieving competitive results on ImageNet and text-to-image benchmarks without VAEs or complex pipelines.

Abstract: The current success of diffusion transformers heavily depends on the
compressed latent space shaped by the pre-trained variational autoencoder(VAE).
However, this two-stage training paradigm inevitably introduces accumulated
errors and decoding artifacts. To address the aforementioned problems,
researchers return to pixel space at the cost of complicated cascade pipelines
and increased token complexity. In contrast to their efforts, we propose to
model the patch-wise decoding with neural field and present a single-scale,
single-stage, efficient, end-to-end solution, coined as pixel neural field
diffusion~(PixelNerd). Thanks to the efficient neural field representation in
PixNerd, we directly achieved 2.15 FID on ImageNet $256\times256$ and 2.84 FID
on ImageNet $512\times512$ without any complex cascade pipeline or VAE. We also
extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16
achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9
overall score on the DPG benchmark.

</details>


### [29] [Online Estimation of Table-Top Grown Strawberry Mass in Field Conditions with Occlusions](https://arxiv.org/abs/2507.23487)
*Jinshan Zhen,Yuanyue Ge,Tianxiao Zhu,Hui Zhao,Ya Xiong*

Main category: cs.CV

TL;DR: 提出了一种结合YOLOv8-Seg、CycleGAN和倾斜角度校正的视觉方法，用于在田间条件下精确估算草莓质量，即使存在遮挡也能有效处理，实验结果显示了其鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在田间条件下，由于频繁的遮挡和姿态变化，精确估算桌面级草莓的质量仍然是一个挑战。

Method: 提出了一种基于视觉的管道，集成了RGB-D传感和深度学习，具体包括使用YOLOv8-Seg进行实例分割，CycleGAN进行遮挡区域补全，以及倾斜角度校正来优化正面投影面积计算。最后，使用多项式回归模型将几何特征映射到质量。

Result: 实验证明，该方法对于孤立草莓的平均质量估算误差为8.11%，对于遮挡情况下的误差为10.47%。CycleGAN在遮挡恢复方面优于LaMa模型，取得了更高的像素面积比（PAR）（平均值：0.978 vs 1.112）和IoU分数（在[0.9-1]范围内为92.3% vs 47.7%）。

Conclusion: 该方法通过整合RGB-D传感和深度学习，并利用CycleGAN处理遮挡区域，实现了对草莓的无损、实时和在线估重，为自动化收获和产量监测提供了鲁棒的解决方案。

Abstract: Accurate mass estimation of table-top grown strawberries under field
conditions remains challenging due to frequent occlusions and pose variations.
This study proposes a vision-based pipeline integrating RGB-D sensing and deep
learning to enable non-destructive, real-time and online mass estimation. The
method employed YOLOv8-Seg for instance segmentation, Cycle-consistent
generative adversarial network (CycleGAN) for occluded region completion, and
tilt-angle correction to refine frontal projection area calculations. A
polynomial regression model then mapped the geometric features to mass.
Experiments demonstrated mean mass estimation errors of 8.11% for isolated
strawberries and 10.47% for occluded cases. CycleGAN outperformed large mask
inpainting (LaMa) model in occlusion recovery, achieving superior pixel area
ratios (PAR) (mean: 0.978 vs. 1.112) and higher intersection over union (IoU)
scores (92.3% vs. 47.7% in the [0.9-1] range). This approach addresses critical
limitations of traditional methods, offering a robust solution for automated
harvesting and yield monitoring with complex occlusion patterns.

</details>


### [30] [Towards Affordable Tumor Segmentation and Visualization for 3D Breast MRI Using SAM2](https://arxiv.org/abs/2507.23272)
*Solha Kang,Eugene Kim,Joris Vankerschaver,Utku Ozbulak*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Breast MRI provides high-resolution volumetric imaging critical for tumor
assessment and treatment planning, yet manual interpretation of 3D scans
remains labor-intensive and subjective. While AI-powered tools hold promise for
accelerating medical image analysis, adoption of commercial medical AI products
remains limited in low- and middle-income countries due to high license costs,
proprietary software, and infrastructure demands. In this work, we investigate
whether the Segment Anything Model 2 (SAM2) can be adapted for low-cost,
minimal-input 3D tumor segmentation in breast MRI. Using a single bounding box
annotation on one slice, we propagate segmentation predictions across the 3D
volume using three different slice-wise tracking strategies: top-to-bottom,
bottom-to-top, and center-outward. We evaluate these strategies across a large
cohort of patients and find that center-outward propagation yields the most
consistent and accurate segmentations. Despite being a zero-shot model not
trained for volumetric medical data, SAM2 achieves strong segmentation
performance under minimal supervision. We further analyze how segmentation
performance relates to tumor size, location, and shape, identifying key failure
modes. Our results suggest that general-purpose foundation models such as SAM2
can support 3D medical image analysis with minimal supervision, offering an
accessible and affordable alternative for resource-constrained settings.

</details>


### [31] [RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping](https://arxiv.org/abs/2507.23734)
*Dongming Wu,Yanping Fu,Saike Huang,Yingfei Liu,Fan Jia,Nian Liu,Feng Dai,Tiancai Wang,Rao Muhammad Anwer,Fahad Shahbaz Khan,Jianbing Shen*

Main category: cs.CV

TL;DR: RAGNet 是一个包含 273k 图像、180 个类别和 26k 推理指令的大规模抓取范式分割基准。AffordanceNet 是一个包含 VLM 和抓取网络的框架，可在开放世界中进行泛化。


<details>
  <summary>Details</summary>
Motivation: 当前通用机器人抓取系统在多样化的开放世界场景中，需要根据人类指令进行准确的物体范式感知。然而，现有研究缺乏基于推理的大规模范式预测数据，这引起了对开放世界有效性的担忧。

Method: 提出了一种名为 AffordanceNet 的基于范式的抓取框架，该框架包括在海量数据上预训练的视觉语言模型（VLM）以及以范式图为条件的抓取网络，以抓取目标。

Result: 构建了一个大规模的、面向抓取、具有人类指令的范式分割基准 RAGNet，其中包含 273,000 张图像、180 个类别和 26,000 条推理指令。并在范式分割基准和真实机器人操作任务上进行了广泛的实验。

Conclusion: 该模型具有强大的开放世界泛化能力。

Abstract: General robotic grasping systems require accurate object affordance
perception in diverse open-world scenarios following human instructions.
However, current studies suffer from the problem of lacking reasoning-based
large-scale affordance prediction data, leading to considerable concern about
open-world effectiveness. To address this limitation, we build a large-scale
grasping-oriented affordance segmentation benchmark with human-like
instructions, named RAGNet. It contains 273k images, 180 categories, and 26k
reasoning instructions. The images cover diverse embodied data domains, such as
wild, robot, ego-centric, and even simulation data. They are carefully
annotated with an affordance map, while the difficulty of language instructions
is largely increased by removing their category name and only providing
functional descriptions. Furthermore, we propose a comprehensive
affordance-based grasping framework, named AffordanceNet, which consists of a
VLM pre-trained on our massive affordance data and a grasping network that
conditions an affordance map to grasp the target. Extensive experiments on
affordance segmentation benchmarks and real-robot manipulation tasks show that
our model has a powerful open-world generalization ability. Our data and code
is available at https://github.com/wudongming97/AffordanceNet.

</details>


### [32] [iLRM: An Iterative Large 3D Reconstruction Model](https://arxiv.org/abs/2507.23277)
*Gyeongjin Kang,Seungtae Nam,Xiangyu Sun,Sameh Khamis,Abdelrahman Mohamed,Eunbyung Park*

Main category: cs.CV

TL;DR: iLRM通过迭代优化和分阶段注意力机制解决了现有3D重建方法的扩展性问题，在保证重建质量和速度的同时，提高了处理多视图的能力。


<details>
  <summary>Details</summary>
Motivation: 为了实现可扩展且高效的前馈3D重建，解决现有基于Transformer的方法在处理多视图时存在的严重可扩展性问题，这些方法依赖于视图间所有图像标记的完全注意力，导致计算成本高昂。

Method: 提出了一种迭代式的大型3D重建模型（iLRM），通过迭代优化机制生成3D高斯表示。该模型遵循三个核心原则：1）将场景表示与输入视图图像分离，以实现紧凑的3D表示；2）将全注意力多视图交互分解为两阶段注意力方案，以降低计算成本；3）在每一层注入高分辨率信息，以实现高保真重建。

Result: iLRM 在重建质量和速度方面均优于现有方法，并且具有出色的可扩展性，能够在相同的计算成本下利用更多的输入视图来提高重建质量。

Conclusion: iLRM 在 RE10K 和 DL3DV 等广泛使用的数据集上进行了实验，其重建质量和速度均优于现有方法。值得注意的是，iLRM 表现出卓越的可扩展性，通过有效利用更多的输入视图，在可比的计算成本下显著提高了重建质量。

Abstract: Feed-forward 3D modeling has emerged as a promising approach for rapid and
high-quality 3D reconstruction. In particular, directly generating explicit 3D
representations, such as 3D Gaussian splatting, has attracted significant
attention due to its fast and high-quality rendering, as well as numerous
applications. However, many state-of-the-art methods, primarily based on
transformer architectures, suffer from severe scalability issues because they
rely on full attention across image tokens from multiple input views, resulting
in prohibitive computational costs as the number of views or image resolution
increases. Toward a scalable and efficient feed-forward 3D reconstruction, we
introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D
Gaussian representations through an iterative refinement mechanism, guided by
three core principles: (1) decoupling the scene representation from input-view
images to enable compact 3D representations; (2) decomposing fully-attentional
multi-view interactions into a two-stage attention scheme to reduce
computational costs; and (3) injecting high-resolution information at every
layer to achieve high-fidelity reconstruction. Experimental results on widely
used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms
existing methods in both reconstruction quality and speed. Notably, iLRM
exhibits superior scalability, delivering significantly higher reconstruction
quality under comparable computational cost by efficiently leveraging a larger
number of input views.

</details>


### [33] [UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing](https://arxiv.org/abs/2507.23278)
*Hao Tang,Chenwei Xie,Xiaoyi Bao,Tingyu Weng,Pandeng Li,Yun Zheng,Liwei Wang*

Main category: cs.CV

TL;DR: UniLIP extends CLIP for reconstruction, generation, and editing using a novel training scheme and dual-condition architecture, achieving state-of-the-art results in text-to-image generation and image editing while maintaining CLIP's comprehension capabilities.


<details>
  <summary>Details</summary>
Motivation: To build a unified tokenizer upon CLIP's comprehension capabilities that extends to reconstruction, generation, and editing, overcoming limitations of previous methods that required additional components or suffered from performance degradation.

Method: UniLIP introduces a two-stage training scheme and a self-distillation strategy to progressively integrate reconstruction capabilities into CLIP. It also proposes a dual-condition architecture connecting the MLLM and diffusion transformer, using both learnable queries and the last layer multimodal hidden states as joint conditions.

Result: UniLIP achieves scores of 0.87 and 0.53 on GenEval and WISE benchmarks for text-to-image generation, surpassing previous unified models. In image editing, it scores 3.62 on the ImgEdit Benchmark, outperforming models like BAGEL and UniWorld-V1.

Conclusion: UniLIP effectively expands CLIP's application scope, enabling its continuous features to serve not only for understanding tasks but also achieve highly competitive performance in generation and editing tasks.

Abstract: In this paper, we propose UniLIP, which extends CLIP to reconstruction,
generation and editing, thereby building a unified tokenizer upon its
exceptional comprehension capabilities. Previous CLIP-based unified methods
often require additional diffusion decoders or quantization to support
reconstruction and generation tasks, leading to inconsistent reconstruction or
degradation of original comprehension performance.In contrast, we introduce a
two-stage training scheme and a self-distillation strategy that progressively
integrates reconstruction capabilities into CLIP, allowing it to maintain
original comprehension performance while achieving effective image
reconstruction. Furthermore, we propose a dual-condition architecture to
connect the MLLM and diffusion transformer, using both learnable queries and
the last layer multimodal hidden states as joint conditions. This method not
only enables the utilization of the MLLM's strong reasoning capabilities in
generation tasks, but also maximizes the exploitation of the rich information
in UniLIP features during editing tasks. In text-to-image generation tasks,
UniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark
respectively, surpassing all previous unified models of similar scale. In image
editing, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark,
surpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP
effectively expand the application scope of CLIP, enabling continuous CLIP
features to not only serve as the optimal choice for understanding tasks but
also achieve highly competitive performance in generation and editing tasks.

</details>


### [34] [Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval](https://arxiv.org/abs/2507.23284)
*Dohwan Ko,Ji Soo Lee,Minhyuk Choi,Zihang Meng,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: BLiM是一种新的文本-视频检索框架，通过双向似然估计和候选先验归一化来克服候选先验偏差，在多个基准测试中取得了最先进的成果，并展示了在其他多模态任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 观察到多模态大语言模型（MLLMs）在文本-视频检索中的天真应用（基于候选似然性的检索）会引入候选先验偏差，偏向于那些先验概率较高的候选，而不是与查询更相关的候选。

Method: 提出了一种新颖的检索框架BLiM（Bidirectional Likelihood Estimation with MLLM），它通过训练模型从给定视频生成文本以及从给定文本生成视频特征来利用查询和候选的似然性。此外，还引入了CPN（Candidate Prior Normalization）模块，一种无需训练的评分校准方法，用于减轻候选先验偏差。

Result: BLiM框架结合CPN模块，在四个文本-视频检索基准上，平均R@1提升6.4，有效缓解了候选先验偏差，并强调了查询-候选相关性。CPN模块的广泛适用性也已在检索之外的多种多模态任务中得到验证，通过减少对文本先验的依赖来增强视觉理解。

Conclusion: BLiM框架结合CPN模块，在四个文本-视频检索基准上，平均R@1提升6.4，有效缓解了候选先验偏差，并强调了查询-候选相关性。CPN模块的广泛适用性也已在检索之外的多种多模态任务中得到验证，通过减少对文本先验的依赖来增强视觉理解。

Abstract: Text-Video Retrieval aims to find the most relevant text (or video) candidate
given a video (or text) query from large-scale online databases. Recent work
leverages multi-modal large language models (MLLMs) to improve retrieval,
especially for long or complex query-candidate pairs. However, we observe that
the naive application of MLLMs, i.e., retrieval based on candidate likelihood,
introduces candidate prior bias, favoring candidates with inherently higher
priors over those more relevant to the query. To this end, we propose a novel
retrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM),
which leverages both query and candidate likelihoods by training the model to
generate text from a given video as well as video features from a given text.
Furthermore, we introduce Candidate Prior Normalization (CPN), a simple yet
effective training-free score calibration module designed to mitigate candidate
prior bias in candidate likelihood. On four Text-Video Retrieval benchmarks,
our BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4
R@1 on average, effectively alleviating candidate prior bias and emphasizing
query-candidate relevance. Our in-depth analysis across various multi-modal
tasks beyond retrieval highlights the broad applicability of CPN which enhances
visual understanding by reducing reliance on textual priors. Code is available
at https://github.com/mlvlab/BLiM.

</details>


### [35] [LED Benchmark: Diagnosing Structural Layout Errors for Document Layout Analysis](https://arxiv.org/abs/2507.23295)
*Inbum Heo,Taewook Hwang,Jeesu Jung,Sangkeun Jung*

Main category: cs.CV

TL;DR: This paper introduces LED, a new benchmark and dataset to evaluate structural errors in document layout analysis, which traditional metrics fail to capture. LED effectively differentiates model capabilities and reveals hidden biases.


<details>
  <summary>Details</summary>
Motivation: Conventional evaluation metrics like IoU and mAP are insufficient for detecting critical structural errors such as region merging, splitting, and missing content in document layout analysis.

Method: The paper proposes Layout Error Detection (LED), a novel benchmark with eight standardized error types and three complementary tasks: error existence detection, error type classification, and element-wise error type classification. They also constructed LED-Dataset, a synthetic dataset with realistic structural errors.

Result: Experimental results across a range of Large Multimodal Models (LMMs) show LED's effectiveness in evaluating structural robustness.

Conclusion: LED effectively differentiates structural understanding capabilities and exposes modality biases and performance trade-offs not visible through traditional metrics.

Abstract: Recent advancements in Document Layout Analysis through Large Language Models
and Multimodal Models have significantly improved layout detection. However,
despite these improvements, challenges remain in addressing critical structural
errors, such as region merging, splitting, and missing content. Conventional
evaluation metrics like IoU and mAP, which focus primarily on spatial overlap,
are insufficient for detecting these errors. To address this limitation, we
propose Layout Error Detection (LED), a novel benchmark designed to evaluate
the structural robustness of document layout predictions. LED defines eight
standardized error types, and formulates three complementary tasks: error
existence detection, error type classification, and element-wise error type
classification. Furthermore, we construct LED-Dataset, a synthetic dataset
generated by injecting realistic structural errors based on empirical
distributions from DLA models. Experimental results across a range of LMMs
reveal that LED effectively differentiates structural understanding
capabilities, exposing modality biases and performance trade-offs not visible
through traditional metrics.

</details>


### [36] [Training-free Geometric Image Editing on Diffusion Models](https://arxiv.org/abs/2507.23300)
*Hanshen Zhu,Zhen Zhu,Kaile Zhang,Yiming Gong,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 提出了一种名为FreeFine的解耦方法，用于几何图像编辑，将对象转换、源区域修复和目标区域细化分开处理，在GeoBench基准上表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决先前基于扩散的编辑方法在处理大范围或结构复杂的几何变换时存在的困难。

Method: 提出一个分离对象转换、源区域修复和目标区域细化的解耦管道。修复和细化均使用无需训练的扩散方法FreeFine实现。

Result: 在新的GeoBench基准（包含2D和3D编辑场景）上进行了实验。

Conclusion: FreeFine在图像保真度和编辑精度方面优于最先进的替代方案，尤其是在要求苛刻的转换下。

Abstract: We tackle the task of geometric image editing, where an object within an
image is repositioned, reoriented, or reshaped while preserving overall scene
coherence. Previous diffusion-based editing methods often attempt to handle all
relevant subtasks in a single step, proving difficult when transformations
become large or structurally complex. We address this by proposing a decoupled
pipeline that separates object transformation, source region inpainting, and
target region refinement. Both inpainting and refinement are implemented using
a training-free diffusion approach, FreeFine. In experiments on our new
GeoBench benchmark, which contains both 2D and 3D editing scenarios, FreeFine
outperforms state-of-the-art alternatives in image fidelity, and edit
precision, especially under demanding transformations. Code and benchmark are
available at: https://github.com/CIawevy/FreeFine

</details>


### [37] [ST-SAM: SAM-Driven Self-Training Framework for Semi-Supervised Camouflaged Object Detection](https://arxiv.org/abs/2507.23307)
*Xihang Hu,Fuming Sun,Jiazhe Liu,Feilong Xu,Xiaoli Zhang*

Main category: cs.CV

TL;DR: ST-SAM 是一种高效的半监督伪装目标检测方法，通过自训练和利用 SAM 的提示来提高性能，显著减少了对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的基于教师-学生框架的 SSCOD 方法在稀疏监督下存在严重的预测偏差和误差传播问题，同时多网络架构导致高计算开销和有限的可扩展性。为了克服这些局限性，需要一种高效且简洁的框架。

Method: ST-SAM 框架采用自训练策略，通过动态过滤和扩展高置信度伪标签来增强单一模型架构，从而避免了模型间的预测偏差。此外，该方法将伪标签转化为包含领域特定知识的混合提示（prompts），以利用分割基础模型（如 SAM）的潜力，减轻自训练过程中的误差累积。

Result: ST-SAM 在 COD 基准数据集上取得了最先进的性能，仅使用 1% 的标注数据就超越了现有的 SSCOD 方法，并且性能与全监督方法相当。该方法仅需训练单一网络，不依赖特定的模型或损失函数。

Conclusion: ST-SAM 提出了一种新颖的、高效的半监督伪装目标检测（SSCOD）框架，通过动态过滤和扩展高置信度伪标签，并利用 SAM 的强大能力，解决了现有 SSCOD 方法的预测偏差、误差累积和计算开销问题。实验证明，ST-SAM 在仅使用 1% 标注数据的情况下，取得了超越现有 SSCOD 方法的性能，并能媲美全监督方法，同时只需训练单一网络，不依赖特定模型或损失函数，为标注高效 SSCOD 设定了新的范例。

Abstract: Semi-supervised Camouflaged Object Detection (SSCOD) aims to reduce reliance
on costly pixel-level annotations by leveraging limited annotated data and
abundant unlabeled data. However, existing SSCOD methods based on
Teacher-Student frameworks suffer from severe prediction bias and error
propagation under scarce supervision, while their multi-network architectures
incur high computational overhead and limited scalability. To overcome these
limitations, we propose ST-SAM, a highly annotation-efficient yet concise
framework that breaks away from conventional SSCOD constraints. Specifically,
ST-SAM employs Self-Training strategy that dynamically filters and expands
high-confidence pseudo-labels to enhance a single-model architecture, thereby
fundamentally circumventing inter-model prediction bias. Furthermore, by
transforming pseudo-labels into hybrid prompts containing domain-specific
knowledge, ST-SAM effectively harnesses the Segment Anything Model's potential
for specialized tasks to mitigate error accumulation in self-training.
Experiments on COD benchmark datasets demonstrate that ST-SAM achieves
state-of-the-art performance with only 1\% labeled data, outperforming existing
SSCOD methods and even matching fully supervised methods. Remarkably, ST-SAM
requires training only a single network, without relying on specific models or
loss functions. This work establishes a new paradigm for annotation-efficient
SSCOD. Codes will be available at https://github.com/hu-xh/ST-SAM.

</details>


### [38] [PriorFusion: Unified Integration of Priors for Robust Road Perception in Autonomous Driving](https://arxiv.org/abs/2507.23309)
*Xuewei Tang,Mengmeng Yang,Tuopu Wen,Peijin Jia,Le Cui,Mingshang Luo,Kehua Sheng,Bo Zhang,Diange Yang,Kun Jiang*

Main category: cs.CV

TL;DR: PriorFusion是一个新的框架，通过结合形状先验和扩散模型来提高自动驾驶汽车在复杂环境中的道路感知能力，解决了现有方法在这方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用道路元素固有的结构先验方面存在不足，导致预测结果不规则、不准确。在没有高清地图支持的复杂环境中，自动驾驶汽车需要独立解析周围环境以确保安全决策，而这些场景中的道路元素数量多、几何形状复杂且遮挡频繁，对感知技术提出了巨大挑战。

Method: 提出了一种名为PriorFusion的统一框架，该框架整合了语义、几何和生成先验来增强道路元素感知。具体方法包括：1.引入了由形状先验特征引导的实例感知注意力机制。2.构建了一个数据驱动的形状模板空间，用于编码道路元素的低维表示，并通过聚类生成作为参考先验的锚点。3.设计了一个基于扩散的框架，利用这些先验锚点生成准确且完整的预测。

Result: 实验结果表明，PriorFusion在大型自动驾驶数据集上显著提高了感知准确性，特别是在挑战性条件下。可视化结果进一步证实了该方法能够生成更准确、更规则、更连贯的道路元素预测。

Conclusion: PriorFusion通过整合语义、几何和生成先验，显著提高了道路元素感知的准确性，尤其是在具有挑战性的条件下。可视化结果证实该方法能够生成更准确、更规则、更连贯的道路元素预测。

Abstract: With the growing interest in autonomous driving, there is an increasing
demand for accurate and reliable road perception technologies. In complex
environments without high-definition map support, autonomous vehicles must
independently interpret their surroundings to ensure safe and robust
decision-making. However, these scenarios pose significant challenges due to
the large number, complex geometries, and frequent occlusions of road elements.
A key limitation of existing approaches lies in their insufficient exploitation
of the structured priors inherently present in road elements, resulting in
irregular, inaccurate predictions. To address this, we propose PriorFusion, a
unified framework that effectively integrates semantic, geometric, and
generative priors to enhance road element perception. We introduce an
instance-aware attention mechanism guided by shape-prior features, then
construct a data-driven shape template space that encodes low-dimensional
representations of road elements, enabling clustering to generate anchor points
as reference priors. We design a diffusion-based framework that leverages these
prior anchors to generate accurate and complete predictions. Experiments on
large-scale autonomous driving datasets demonstrate that our method
significantly improves perception accuracy, particularly under challenging
conditions. Visualization results further confirm that our approach produces
more accurate, regular, and coherent predictions of road elements.

</details>


### [39] [Forgetting of task-specific knowledge in model merging-based continual learning](https://arxiv.org/abs/2507.23311)
*Timm Hess,Gido M van de Ven,Tinne Tuytelaars*

Main category: cs.CV

TL;DR: Linear model merging in continual learning preserves shared knowledge but degrades task-specific knowledge. Incremental training merge is better than parallel.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate the linear merging of models in the context of continual learning.

Method: The paper uses controlled visual cues in computer vision experiments to investigate the linear merging of models in continual learning.

Result: Merging largely preserves or enhances shared knowledge, while unshared task-specific knowledge rapidly degrades. Merging models from an incremental training process consistently outperforms merging models trained in parallel.

Conclusion: Merging models in continual learning largely preserves or enhances shared knowledge, while unshared task-specific knowledge rapidly degrades. Merging models from an incremental training process consistently outperforms merging models trained in parallel.

Abstract: This paper investigates the linear merging of models in the context of
continual learning (CL). Using controlled visual cues in computer vision
experiments, we demonstrate that merging largely preserves or enhances shared
knowledge, while unshared task-specific knowledge rapidly degrades. We further
find that merging models from an incremental training process consistently
outperforms merging models trained in parallel.

</details>


### [40] [The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models](https://arxiv.org/abs/2507.23313)
*Alfio Ferrara,Sergio Picascia,Elisabetta Rocchetti*

Main category: cs.CV

TL;DR: 확산 모델이 예술적 프롬프트를 어떻게 해석하는지, 특히 내용과 스타일을 어떻게 분리하는지 조사합니다. 교차 주의 맵을 사용하여 모델이 내용과 스타일을 구분하는 능력이 프롬프트에 따라 다르며 종종 내용이 개체에, 스타일이 배경에 영향을 미치는 것을 발견했습니다.


<details>
  <summary>Details</summary>
Motivation: 探讨transformer 기반 텍스트-이미지 확산 모델이 그림의 내용과 스타일과 같은 개념을 내부적으로 어떻게 표현하는지에 대한 근본적인 질문을 탐구합니다. 기존 컴퓨터 비전은 내용과 스타일이 직교한다고 가정하지만, 확산 모델은 훈련 중에 이러한 구분에 대한 명시적인 지침을 받지 못합니다.

Method: 利用交叉注意力热图将生成图像中的像素归因于特定的提示标记，从而分离受内容描述标记或风格描述标记影响的图像区域。

Result: 연구 결과에 따르면 확산 모델은 특정 예술적 프롬프트 및 요청된 스타일에 따라 내용-스타일 분리에 다양한 정도를 나타냅니다. 많은 경우 내용 토큰은 주로 개체 관련 영역에 영향을 미치고 스타일 토큰은 배경 및 질감 영역에 영향을 미쳐 내용-스타일 구분에 대한 이해가 나타남을 시사합니다.

Conclusion: 该研究揭示了文本到图像的扩散模型如何在没有明确监督的情况下，在内部表示复杂的艺术概念，为理解这些大型生成模型的工作原理提供了新的见解。

Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in
generating artistic content by learning from billions of images, including
popular artworks. However, the fundamental question of how these models
internally represent concepts, such as content and style in paintings, remains
unexplored. Traditional computer vision assumes content and style are
orthogonal, but diffusion models receive no explicit guidance about this
distinction during training. In this work, we investigate how transformer-based
text-to-image diffusion models encode content and style concepts when
generating artworks. We leverage cross-attention heatmaps to attribute pixels
in generated images to specific prompt tokens, enabling us to isolate image
regions influenced by content-describing versus style-describing tokens. Our
findings reveal that diffusion models demonstrate varying degrees of
content-style separation depending on the specific artistic prompt and style
requested. In many cases, content tokens primarily influence object-related
regions while style tokens affect background and texture areas, suggesting an
emergent understanding of the content-style distinction. These insights
contribute to our understanding of how large-scale generative models internally
represent complex artistic concepts without explicit supervision. We share the
code and dataset, together with an exploratory tool for visualizing attention
maps at https://github.com/umilISLab/artistic-prompt-interpretation.

</details>


### [41] [Impact of Hyperparameter Optimization on the Accuracy of Lightweight Deep Learning Models for Real-Time Image Classification](https://arxiv.org/abs/2507.23315)
*Vineet Kumar Rakesh,Soumya Mazumdar,Tapas Samanta,Sarbajit Pal,Amitabha Das*

Main category: cs.CV

TL;DR: 本研究分析了超参数调整对七种高效深度学习模型在ImageNet-1K数据集上的实时图像分类性能的影响，重点关注准确性、收敛速度、推理时间和资源消耗。研究发现，余弦学习率衰减和可调批次大小可显著提高性能，而RepVGG-A2在准确性和效率方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 轻量级卷积和基于Transformer的模型对于资源受限应用（如嵌入式系统和边缘设备）中的实时图像分类至关重要，本研究旨在分析超参数调整对这些模型准确性和收敛行为的影响。

Method: 通过对EfficientNetV2-S、ConvNeXt-T、MobileViT v2 (XXS/XS/S)、MobileNetV3-L、TinyViT-21M和RepVGG-A2这七种高效深度学习架构进行超参数调整影响的分析，并在一致的训练设置下，在ImageNet-1K数据集上进行训练，重点关注实时应用。进行了一项全面的消融研究，以分离关键超参数（包括学习率计划、批次大小、输入分辨率、数据增强、正则化方法和优化器选择）的影响。为了评估实时应用的适用性，在GPU加速的边缘部署模拟中，不仅根据Top-1和Top-5分类准确率评估了每个模型，还根据推理时间、参数数量、模型大小和每秒帧数（FPS）进行了评估。

Result: 研究表明，余弦学习率衰减和可调批次大小可以极大地提高准确性和收敛速度，同时保持低延迟和内存成本。RepVGG-A2在保持高效推理性能的同时，实现了超过80%的Top-1准确率，在准确性和部署成本之间取得了有吸引力的平衡。

Conclusion: 研究结果为构建资源高效的深度学习模型提供了实用的指导，这些模型适用于实时图像处理流水线。RepVGG-A2在保持高效推理性能的同时，实现了超过80%的Top-1准确率，为VGG风格模型在准确性和部署成本之间取得了有吸引力的平衡。

Abstract: Lightweight convolutional and transformer-based models have become vital for
real-time image classification in resource-constrained applications, such as
embedded systems and edge devices. This work analyzes the influence of
hyperparameter adjustment on the accuracy and convergence behavior of seven
efficient deep learning architectures: EfficientNetV2-S, ConvNeXt-T, MobileViT
v2 (XXS/XS/S), MobileNetV3-L, TinyViT-21M, and RepVGG-A2. All models are
trained on the ImageNet-1K dataset under consistent training settings, with an
emphasis on real-time practicality. An comprehensive ablation study is
undertaken to separate the effect of critical hyperparameters, including
learning rate schedules, batch sizes, input resolution, data augmentation,
regularization approaches, and optimizer choice. To assess appropriateness for
real-time applications, each model is assessed not only in terms of Top-1 and
Top-5 classification accuracy, but also in terms of inference time, parameter
count, model size, and frames-per-second (FPS) on a GPU-accelerated edge
deployment simulation. Results demonstrate that cosine learning rate decay and
adjustable batch size may greatly boost both accuracy and convergence speed,
while keeping low latency and memory cost. Notably, RepVGG-A2 achieves over 80%
Top-1 accuracy with efficient inference performance, offering a compelling
balance between accuracy and deployment cost for VGG-style models. The results
give practical guidance for constructing resource-efficient deep learning
models appropriate for real-time image processing pipelines. All code and
training logs are publicly accessible at
https://github.com/VineetKumarRakesh/lcnn-opt.

</details>


### [42] [FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning](https://arxiv.org/abs/2507.23318)
*Jiajun Cao,Qizhe Zhang,Peidong Jia,Xuhui Zhao,Bo Lan,Xiaoan Zhang,Xiaobao Wei,Sixiang Chen,Zhuo Li,Yang Wang,Liyun Li,Xianming Liu,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: 针对自动驾驶场景中的 VLA 模型计算成本高的问题，提出了一种名为 FastDriveVLA 的基于重建的视觉标记剪枝框架。该框架通过 ReconPruner 优先保留前景信息，并使用 nuScenes-FG 数据集进行训练，在自动驾驶任务中取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉标记剪枝方法在自动驾驶场景中表现不佳。人类驾驶员在驾驶时会专注于相关的前景区域，因此保留包含此前景信息的视觉标记对于有效的决策至关重要。

Method: 提出了一种名为 ReconPruner 的即插即用视觉标记剪枝器，它通过 MAE 风格的像素重建来优先处理前景信息。此外，还设计了一种新颖的对抗性前景-背景重建策略来训练 ReconPruner，并引入了一个包含 241K 图像-掩码对和标注前景区域的大型数据集 nuScenes-FG。

Result: 提出的 ReconPruner 剪枝器可以无缝应用于具有相同视觉编码器的不同 VLA 模型，而无需重新训练，并在 nuScenes 闭环规划基准测试中实现了最先进的结果。

Conclusion: FastDriveVLA 在 nuScenes 闭环规划基准测试中，在不同的剪枝率下均取得了最先进的成果。

Abstract: Vision-Language-Action (VLA) models have demonstrated significant potential
in complex scene understanding and action reasoning, leading to their
increasing adoption in end-to-end autonomous driving systems. However, the long
visual tokens of VLA models greatly increase computational costs. Current
visual token pruning methods in Vision-Language Models (VLM) rely on either
visual token similarity or visual-text attention, but both have shown poor
performance in autonomous driving scenarios. Given that human drivers
concentrate on relevant foreground areas while driving, we assert that
retaining visual tokens containing this foreground information is essential for
effective decision-making. Inspired by this, we propose FastDriveVLA, a novel
reconstruction-based vision token pruning framework designed specifically for
autonomous driving. FastDriveVLA includes a plug-and-play visual token pruner
called ReconPruner, which prioritizes foreground information through MAE-style
pixel reconstruction. A novel adversarial foreground-background reconstruction
strategy is designed to train ReconPruner for the visual encoder of VLA models.
Once trained, ReconPruner can be seamlessly applied to different VLA models
with the same visual encoder without retraining. To train ReconPruner, we also
introduce a large-scale dataset called nuScenes-FG, consisting of 241K
image-mask pairs with annotated foreground regions. Our approach achieves
state-of-the-art results on the nuScenes closed-loop planning benchmark across
different pruning ratios.

</details>


### [43] [FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with Latent World Models](https://arxiv.org/abs/2507.23325)
*Yiming Yang,Hongbin Lin,Yueru Luo,Suzhong Fu,Chao Zheng,Xinrui Yan,Shuqi Mei,Kun Tang,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: FASTopoWM 通过引入快速-慢速框架和潜在世界模型，解决了现有车道线拓扑推理方法对时序信息利用不足、易受姿态估计失败影响等问题，并在 OpenLane-V2 基准测试中取得了优于 SOTA 方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于时序传播的车道拓扑推理方法在有效利用时序信息以提升检测和推理性能方面存在局限，具体表现为过度依赖历史查询、易受姿态估计失败影响以及时序传播能力不足。

Method: 提出了一种名为 FASTopoWM 的新型快速-慢速车道线段拓扑推理框架，并加入了潜在世界模型。该框架通过并行监督历史和新初始化的查询来减少姿态估计失败的影响，促进了快速和慢速系统之间的相互强化。此外，还引入了由动作潜在变量引导的查询和 BEV 潜在世界模型，将状态表示从过去的观测传播到当前时间步，从而显著提高了慢速流程中的时间感知性能。

Result: FASTopoWM 在 OpenLane-V2 基准测试中取得了显著成果，在车道线段检测任务上 mAP 达到了 37.4%，优于基线方法的 33.6%；在中心线感知任务上 OLS 达到了 46.3%，优于基线方法的 41.5%。

Conclusion: FASTopoWM 在 OpenLane-V2 基准测试中，在车道线段检测（mAP 为 37.4% 对 33.6%）和中心线感知（OLS 为 46.3% 对 41.5%）方面均优于最先进的方法，证明了其在解决现有方法局限性方面的有效性。

Abstract: Lane segment topology reasoning provides comprehensive bird's-eye view (BEV)
road scene understanding, which can serve as a key perception module in
planning-oriented end-to-end autonomous driving systems. Existing lane topology
reasoning methods often fall short in effectively leveraging temporal
information to enhance detection and reasoning performance. Recently,
stream-based temporal propagation method has demonstrated promising results by
incorporating temporal cues at both the query and BEV levels. However, it
remains limited by over-reliance on historical queries, vulnerability to pose
estimation failures, and insufficient temporal propagation. To overcome these
limitations, we propose FASTopoWM, a novel fast-slow lane segment topology
reasoning framework augmented with latent world models. To reduce the impact of
pose estimation failures, this unified framework enables parallel supervision
of both historical and newly initialized queries, facilitating mutual
reinforcement between the fast and slow systems. Furthermore, we introduce
latent query and BEV world models conditioned on the action latent to propagate
the state representations from past observations to the current timestep. This
design substantially improves the performance of temporal perception within the
slow pipeline. Extensive experiments on the OpenLane-V2 benchmark demonstrate
that FASTopoWM outperforms state-of-the-art methods in both lane segment
detection (37.4% v.s. 33.6% on mAP) and centerline perception (46.3% v.s. 41.5%
on OLS).

</details>


### [44] [Learning Semantic Directions for Feature Augmentation in Domain-Generalized Medical Segmentation](https://arxiv.org/abs/2507.23326)
*Yingkai Wang,Yaoyao Zhu,Xiuding Cai,Yuhao Xiao,Haotian Wu,Yu Yao*

Main category: cs.CV

TL;DR: 针对医学图像分割中的域转移问题，提出了一种新的域泛化框架。该框架通过隐式特征扰动和自适应一致性约束来提高模型在不同临床域上的鲁棒性和泛化能力，并在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中由于成像条件、扫描仪类型和采集协议的变化而导致的域转移问题，该问题会降低模型在未见过的临床域上的性能。

Method: 提出了一种针对医学图像分割的域泛化框架，通过引入由领域统计驱动的隐式特征扰动来提高对领域特定变化的鲁棒性。具体来说，采用可学习的语义方向选择器和基于协方差的语义强度采样器来调节领域变异特征，同时保持任务相关的解剖一致性。此外，设计了一种自适应一致性约束，仅在特征调整导致分割性能下降时应用，以稳定特征选择并提高分割的可靠性。

Result: 实验证明，所提出的框架在两个公开的多中心基准测试中，在各种临床域上实现了稳健且可泛化的分割性能，并且一致优于现有的域泛化方法。

Conclusion: 本研究提出的域泛化框架在两个公开的多中心基准测试中表现优于现有的域泛化方法，在各种临床域中实现了稳健且可泛化的分割性能。

Abstract: Medical image segmentation plays a crucial role in clinical workflows, but
domain shift often leads to performance degradation when models are applied to
unseen clinical domains. This challenge arises due to variations in imaging
conditions, scanner types, and acquisition protocols, limiting the practical
deployment of segmentation models. Unlike natural images, medical images
typically exhibit consistent anatomical structures across patients, with
domain-specific variations mainly caused by imaging conditions. This unique
characteristic makes medical image segmentation particularly challenging.
  To address this challenge, we propose a domain generalization framework
tailored for medical image segmentation. Our approach improves robustness to
domain-specific variations by introducing implicit feature perturbations guided
by domain statistics. Specifically, we employ a learnable semantic direction
selector and a covariance-based semantic intensity sampler to modulate
domain-variant features while preserving task-relevant anatomical consistency.
Furthermore, we design an adaptive consistency constraint that is selectively
applied only when feature adjustment leads to degraded segmentation
performance. This constraint encourages the adjusted features to align with the
original predictions, thereby stabilizing feature selection and improving the
reliability of the segmentation.
  Extensive experiments on two public multi-center benchmarks show that our
framework consistently outperforms existing domain generalization approaches,
achieving robust and generalizable segmentation performance across diverse
clinical domains.

</details>


### [45] [Contrastive Learning-Driven Traffic Sign Perception: Multi-Modal Fusion of Text and Vision](https://arxiv.org/abs/2507.23331)
*Qiang Lu,Waikit Xiu,Xiying Li,Shenyu Hu,Shengbo Sun*

Main category: cs.CV

TL;DR: 本研究提出了一种创新的两阶段框架，通过改进的YOLO模型和跨模态学习，有效解决了自动驾驶中交通标志识别的长尾分布和目标尺度变化问题，并在实验中取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 交通标志识别是自动驾驶感知系统的核心，但面临长尾分布（导致低频和分布外类别识别性能下降）和小目标尺度变化（难以提取多尺度特征）两大挑战。

Method: 提出了一种名为NanoVerse YOLO的模型，集成了RepVL-PAN和SPD-Conv模块来增强对小尺度、多尺度目标的特征提取。同时，设计了TSR-MCL模型，通过对比Vision Transformer的视觉特征和基于规则的BERT语义特征，学习频率无关的鲁棒表示，以解决数据不平衡导致的类别混淆问题。

Result: 在TT100K数据集上，该方法在长尾检测任务中取得了78.4%的mAP，在识别任务中取得了91.8%的准确率和88.9%的召回率，在复杂、开放世界场景中表现出优越的准确性和泛化能力。

Conclusion: 该研究提出了一种结合开放词汇检测和跨模态学习的两阶段框架，以解决交通标志识别中长尾分布和目标尺度变化的问题。在TT100K数据集上，该方法在长尾检测任务中达到了78.4%的mAP，在识别任务中取得了91.8%的准确率和88.9%的召回率，显著优于主流算法。

Abstract: Traffic sign recognition, as a core component of autonomous driving
perception systems, directly influences vehicle environmental awareness and
driving safety. Current technologies face two significant challenges: first,
the traffic sign dataset exhibits a pronounced long-tail distribution,
resulting in a substantial decline in recognition performance of traditional
convolutional networks when processing low-frequency and out-of-distribution
classes; second, traffic signs in real-world scenarios are predominantly small
targets with significant scale variations, making it difficult to extract
multi-scale features.To overcome these issues, we propose a novel two-stage
framework combining open-vocabulary detection and cross-modal learning. For
traffic sign detection, our NanoVerse YOLO model integrates a reparameterizable
vision-language path aggregation network (RepVL-PAN) and an SPD-Conv module to
specifically enhance feature extraction for small, multi-scale targets. For
traffic sign classification, we designed a Traffic Sign Recognition Multimodal
Contrastive Learning model (TSR-MCL). By contrasting visual features from a
Vision Transformer with semantic features from a rule-based BERT, TSR-MCL
learns robust, frequency-independent representations, effectively mitigating
class confusion caused by data imbalance. On the TT100K dataset, our method
achieves a state-of-the-art 78.4% mAP in the long-tail detection task for
all-class recognition. The model also obtains 91.8% accuracy and 88.9% recall,
significantly outperforming mainstream algorithms and demonstrating superior
accuracy and generalization in complex, open-world scenarios.

</details>


### [46] [MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting](https://arxiv.org/abs/2507.23340)
*Xingyue Peng,Yuandong Lyu,Lang Zhang,Jian Zhu,Songtao Wang,Jiaxin Deng,Songxin Lu,Weiliang Ma,Dangen She,Peng Jia,XianPeng Lang*

Main category: cs.CV

TL;DR: 提出一种鲁棒的路面重建框架，通过结合感知遮挡的2D高斯曲面和语义引导颜色增强，解决了动态遮挡和外观退化问题，并在真实条件下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有基于网格渲染或3D高斯泼溅（3DGS）的方法在动态遮挡、静态障碍物视觉混乱以及光照和天气变化引起的外观退化等问题，提出一种鲁棒的路面重建方法。

Method: 提出了一种包含感知遮挡的2D高斯曲面和语义引导颜色增强的鲁棒重建框架。该方法利用平面适应高斯表示进行高效的大规模建模，采用分割引导视频修复来移除动态和静态前景对象，并通过在HSV空间中进行语义感知校正来增强颜色一致性。

Result: 该方法在城市规模数据集的广泛实验中，证明了其框架能够生成视觉上连贯且几何上忠实的路面重建。

Conclusion: 该框架在真实世界的条件下，显著优于先前的方法，能够生成视觉上连贯且几何上忠实的路面重建。

Abstract: Road surface reconstruction is essential for autonomous driving, supporting
centimeter-accurate lane perception and high-definition mapping in complex
urban environments.While recent methods based on mesh rendering or 3D Gaussian
splatting (3DGS) achieve promising results under clean and static conditions,
they remain vulnerable to occlusions from dynamic agents, visual clutter from
static obstacles, and appearance degradation caused by lighting and weather
changes. We present a robust reconstruction framework that integrates
occlusion-aware 2D Gaussian surfels with semantic-guided color enhancement to
recover clean, consistent road surfaces. Our method leverages a planar-adapted
Gaussian representation for efficient large-scale modeling, employs
segmentation-guided video inpainting to remove both dynamic and static
foreground objects, and enhances color coherence via semantic-aware correction
in HSV space. Extensive experiments on urban-scale datasets demonstrate that
our framework produces visually coherent and geometrically faithful
reconstructions, significantly outperforming prior methods under real-world
conditions.

</details>


### [47] [The Impact of Image Resolution on Face Detection: A Comparative Analysis of MTCNN, YOLOv XI and YOLOv XII models](https://arxiv.org/abs/2507.23341)
*Ahmet Can Ömercikoğlu,Mustafa Mansur Yönügül,Pakize Erdoğmuş*

Main category: cs.CV

TL;DR: 本研究评估了不同分辨率对YOLOv11、YOLOv12和MTCNN人脸检测器的影响。结果显示，YOLOv11在准确性上表现最佳，尤其是在高分辨率下。YOLOv12的召回率略高，而MTCNN的推理速度较慢。研究结果有助于根据具体需求选择合适的人脸检测模型。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的低分辨率图像等条件对人脸检测性能提出了显著挑战，本研究旨在系统地研究输入分辨率对三种主流深度学习人脸检测器（YOLOv11、YOLOv12和MTCNN）的准确性和鲁棒性的影响。

Method: 使用WIDER FACE数据集，在160x160、320x320和640x640等多种图像分辨率下，对YOLOv11、YOLOv12和MTCNN这三种主流的基于深度学习的人脸检测器进行了广泛的评估，并使用精确率、召回率、mAP50、mAP50-95和推理时间等指标进行性能评估。

Result: YOLOv11在检测准确性方面优于YOLOv12和MTCNN，尤其是在较高分辨率下。YOLOv12在召回率方面略有优势。MTCNN在关键点定位方面有竞争力，但推理速度较慢。

Conclusion: 研究结果表明，YOLOv11在检测准确性方面优于YOLOv12和MTCNN，尤其是在较高分辨率下，而YOLOv12在召回率方面略胜一筹。MTCNN虽然在面部关键点定位方面具有竞争力，但在实时推理速度方面表现较差。这些发现为在不同操作约束下选择适合分辨率的 face detection 模型提供了可行的见解。

Abstract: Face detection is a crucial component in many AI-driven applications such as
surveillance, biometric authentication, and human-computer interaction.
However, real-world conditions like low-resolution imagery present significant
challenges that degrade detection performance. In this study, we systematically
investigate the impact of input resolution on the accuracy and robustness of
three prominent deep learning-based face detectors: YOLOv11, YOLOv12, and
MTCNN. Using the WIDER FACE dataset, we conduct extensive evaluations across
multiple image resolutions (160x160, 320x320, and 640x640) and assess each
model's performance using metrics such as precision, recall, mAP50, mAP50-95,
and inference time. Results indicate that YOLOv11 outperforms YOLOv12 and MTCNN
in terms of detection accuracy, especially at higher resolutions, while YOLOv12
exhibits slightly better recall. MTCNN, although competitive in landmark
localization, lags in real-time inference speed. Our findings provide
actionable insights for selecting resolution-aware face detection models
suitable for varying operational constraints.

</details>


### [48] [Who is a Better Talker: Subjective and Objective Quality Assessment for AI-Generated Talking Heads](https://arxiv.org/abs/2507.23343)
*Yingjie Zhou,Jiezhang Cao,Zicheng Zhang,Farong Wen,Yanwei Jiang,Jun Jia,Xiaohong Liu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本研究构建了最大的AGTH质量评估数据集THQA-10K，通过主观和客观方法评估了现有AGTHs的质量，并提出了一种新的SOTA客观评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成说话人头 (AGTHs) 的质量仍有待提高，且缺乏对其质量进行全面研究的学术工作。

Method: 本研究构建了THQA-10K数据集，招募志愿者对AGTHs进行主观评分和失真类别标注，并提出了一种新的客观质量评估方法。

Result: THQA-10K数据集是目前最大的AGTH质量评估数据集，评估结果揭示了现有AGTHs的失真问题，所提出的客观评估方法在AGTH质量评估方面取得了SOTA性能。

Conclusion: 本研究提出了一个名为THQA-10K的数据集，包含10,457个AI生成说话人头 (AGTHs)，并对现有AGTHs的质量进行了评估，最后提出了一种基于第一帧、Y-T切片和音唇一致性的客观质量评估方法，该方法达到了SOTA性能。

Abstract: Speech-driven methods for portraits are figuratively known as "Talkers"
because of their capability to synthesize speaking mouth shapes and facial
movements. Especially with the rapid development of the Text-to-Image (T2I)
models, AI-Generated Talking Heads (AGTHs) have gradually become an emerging
digital human media. However, challenges persist regarding the quality of these
talkers and AGTHs they generate, and comprehensive studies addressing these
issues remain limited. To address this gap, this paper presents the largest
AGTH quality assessment dataset THQA-10K to date, which selects 12 prominent
T2I models and 14 advanced talkers to generate AGTHs for 14 prompts. After
excluding instances where AGTH generation is unsuccessful, the THQA-10K dataset
contains 10,457 AGTHs. Then, volunteers are recruited to subjectively rate the
AGTHs and give the corresponding distortion categories. In our analysis for
subjective experimental results, we evaluate the performance of talkers in
terms of generalizability and quality, and also expose the distortions of
existing AGTHs. Finally, an objective quality assessment method based on the
first frame, Y-T slice and tone-lip consistency is proposed. Experimental
results show that this method can achieve state-of-the-art (SOTA) performance
in AGTH quality assessment. The work is released at
https://github.com/zyj-2000/Talker.

</details>


### [49] [IN45023 Neural Network Design Patterns in Computer Vision Seminar Report, Summer 2025](https://arxiv.org/abs/2507.23357)
*Radu-Andrei Bourceanu,Neil De La Fuente,Jan Grimm,Andrei Jardan,Andriy Manucharyan,Cornelius Weiss,Roman Pflugfelder*

Main category: cs.CV

TL;DR: 本报告分析了计算机视觉领域从 ResNet、ViT 到 GAN、LDMs、DINO 和 MAE 的关键设计模式演变，涵盖了深度学习、注意力机制、生成模型和自监督学习等重要进展。


<details>
  <summary>Details</summary>
Motivation: 本报告旨在分析计算机视觉领域关键设计模式的演变，以理解该领域的发展轨迹和技术突破。

Method: 通过考察六篇有影响力的论文，分析了从图像识别的基础架构（如 ResNet 和 ViT）到生成模型（如 GAN 和 LDMs）再到自监督学习技术（如 DINO 和 MAE）的演变过程。

Result: 报告回顾了 ResNet 如何通过残差连接解决梯度消失问题，ViT 如何将 Transformer 应用于图像识别，GAN 如何通过对抗性训练生成图像，LDMs 如何在潜在空间中实现高保真图像合成，DINO 如何进行自监督学习以提取有效的特征，以及 MAE 如何通过掩码输入重建来预训练大规模视觉模型。

Conclusion: 本报告分析了计算机视觉领域关键设计模式的演变，重点介绍了 ResNet、Vision Transformer (ViT)、生成对抗网络 (GAN)、潜在扩散模型 (LDMs)、DINO 和 Masked Autoencoders (MAE) 等里程碑式的工作。这些模型在深度学习、注意力机制、生成模型和自监督学习等方面取得了重大突破，为当前计算机视觉的发展奠定了基础。

Abstract: This report analyzes the evolution of key design patterns in computer vision
by examining six influential papers. The analy- sis begins with foundational
architectures for image recognition. We review ResNet, which introduced
residual connections to overcome the vanishing gradient problem and enable
effective training of significantly deeper convolutional networks.
Subsequently, we examine the Vision Transformer (ViT), which established a new
paradigm by applying the Transformer ar- chitecture to sequences of image
patches, demonstrating the efficacy of attention-based models for large-scale
image recogni- tion. Building on these visual representation backbones, we
investigate generative models. Generative Adversarial Networks (GANs) are
analyzed for their novel adversarial training process, which challenges a
generator against a discriminator to learn complex data distributions. Then,
Latent Diffusion Models (LDMs) are covered, which improve upon prior generative
methods by performing a sequential denoising process in a perceptually
compressed latent space. LDMs achieve high-fidelity synthesis with greater
computational efficiency, representing the current state-of-the-art for image
generation. Finally, we explore self-supervised learning techniques that reduce
dependency on labeled data. DINO is a self-distillation framework in which a
student network learns to match the output of a momentum-updated teacher,
yielding features with strong k-NN classification performance. We conclude with
Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design
to reconstruct heavily masked inputs, providing a highly scalable and effective
method for pre-training large-scale vision models.

</details>


### [50] [Short-LVLM: Compressing and Accelerating Large Vision-Language Models by Pruning Redundant Layers](https://arxiv.org/abs/2507.23362)
*Ji Ma,Wei Suo,Peng Wang,Yanning Zhang*

Main category: cs.CV

TL;DR: 该研究针对大型视觉语言模型（LVLMs）的参数量大、计算成本高的问题，提出了SVL框架。该框架通过解决非关键VL标记和层间特征差距的问题，实现了模型压缩，并在性能和效率之间取得了良好的平衡。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型（LVLMs）在多模态理解和推理方面展现出令人印象深刻的能力，但其海量模型参数和高计算成本限制了其实际应用。最近自然语言处理（NLP）领域的层剪枝技术被证明是一种有效的模型压缩方法，但其在LVLMs中的有效性尚不明确。

Method: 通过对视觉-语言（VL）标记和层间特征差距进行分析，提出了一种新的框架Short-LVLM（SVL），该框架能够利用重要的VL标记并缩小层间特征差距。

Result: 实验证明，直接将NLP中的层剪枝方法应用于LVLMs是无效的，并发现了非关键的视觉-语言（VL）标记和层间特征差距是导致该问题的主要挑战。

Conclusion: 所提出的Short-LVLM（SVL）框架在性能和效率之间取得了卓越的权衡，并且具有无需训练、模型无关和高度兼容等优点。

Abstract: Although large vision-language models (LVLMs) have demonstrated impressive
capabilities in multi-modal understanding and reasoning, their practical
applications are still limited by massive model parameters and high
computational costs. Recent efforts from natural language processing (NLP) have
shown the effectiveness of layer pruning, offering a plausible training-free
compression solution. However, due to the modality divergence between vision
and language, it is unclear whether these NLP techniques are still effective in
LVLMs. In this paper, we empirically prove that directly applying these layer
pruning methods to LVLMs is ineffective. Through extensive experiments, we find
that non-essential vision-language (VL) tokens and inter-layer feature gaps
pose critical challenges to pruning layers in LVLMs. Based on these insights,
we propose a novel framework Short-LVLM (SVL) that can utilize important VL
tokens and mitigate the layer-wise feature gaps. Notably, Short-LVLM not only
achieves a superior trade-off between performance and efficiency but also
exhibits several potential advantages, i.e., training-free, model-agnostic, and
highly compatible. The code for this work is publicly available at
https://github.com/ASGO-MM/Short-LVLM.

</details>


### [51] [VMatcher: State-Space Semi-Dense Local Feature Matching](https://arxiv.org/abs/2507.23371)
*Ali Youssef*

Main category: cs.CV

TL;DR: VMatcher是一种结合Mamba和Transformer的混合网络，用于高效的半稠密特征匹配，解决了Transformer的高计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的特征匹配方法计算成本高昂，VMatcher旨在通过引入Mamba的线性复杂度SSM来解决效率问题，实现高效且鲁棒的特征匹配。

Method: VMatcher采用混合Mamba-Transformer网络架构，利用Mamba的高效长序列处理能力和Transformer的注意力机制进行半稠密特征匹配。

Result: VMatcher在多个配置下（包括分层结构）均取得了优于现有方法的性能，设定了新的基准，并适用于需要快速推理的实时应用。

Conclusion: VMatcher通过结合Mamba和Transformer的优势，在保证准确性的同时，显著提高了特征匹配的效率，为实时应用提供了更优解决方案。

Abstract: This paper introduces VMatcher, a hybrid Mamba-Transformer network for
semi-dense feature matching between image pairs. Learning-based feature
matching methods, whether detector-based or detector-free, achieve
state-of-the-art performance but depend heavily on the Transformer's attention
mechanism, which, while effective, incurs high computational costs due to its
quadratic complexity. In contrast, Mamba introduces a Selective State-Space
Model (SSM) that achieves comparable or superior performance with linear
complexity, offering significant efficiency gains. VMatcher leverages a hybrid
approach, integrating Mamba's highly efficient long-sequence processing with
the Transformer's attention mechanism. Multiple VMatcher configurations are
proposed, including hierarchical architectures, demonstrating their
effectiveness in setting new benchmarks efficiently while ensuring robustness
and practicality for real-time applications where rapid inference is crucial.
Source Code is available at: https://github.com/ayoussf/VMatcher

</details>


### [52] [UniEmo: Unifying Emotional Understanding and Generation with Learnable Expert Queries](https://arxiv.org/abs/2507.23372)
*Yijie Zhu,Lingsen Zhang,Zitong Yu,Rui Shao,Tao Tan,Liqiang Nie*

Main category: cs.CV

TL;DR: UniEmo是一个统一的情感理解和生成框架，通过创新的方法显著提高了情感图像的理解和生成质量。


<details>
  <summary>Details</summary>
Motivation: 情感理解和生成是互补且能相互促进的任务，但通常被分开处理。本研究旨在整合这两个任务，以提高模型在情感理解和生成方面的能力。

Method: 提出了一种名为UniEmo的统一框架，通过分层情感理解链和可学习的专家查询来提取多尺度情感特征，并指导扩散模型生成情感图像。通过引入情感相关系数和情感条件损失来增强生成图像的多样性和保真度。此外，还提出了一种数据过滤算法，通过生成组件为理解部分提供隐式反馈，以增强模型的理解能力。

Result: UniEmo 在情感理解和生成任务上都取得了显著的性能提升，优于现有最先进的方法。

Conclusion: UniEmo 在情感理解和生成任务上都显著优于现有方法。

Abstract: Emotional understanding and generation are often treated as separate tasks,
yet they are inherently complementary and can mutually enhance each other. In
this paper, we propose the UniEmo, a unified framework that seamlessly
integrates these two tasks. The key challenge lies in the abstract nature of
emotions, necessitating the extraction of visual representations beneficial for
both tasks. To address this, we propose a hierarchical emotional understanding
chain with learnable expert queries that progressively extracts multi-scale
emotional features, thereby serving as a foundational step for unification.
Simultaneously, we fuse these expert queries and emotional representations to
guide the diffusion model in generating emotion-evoking images. To enhance the
diversity and fidelity of the generated emotional images, we further introduce
the emotional correlation coefficient and emotional condition loss into the
fusion process. This step facilitates fusion and alignment for emotional
generation guided by the understanding. In turn, we demonstrate that joint
training allows the generation component to provide implicit feedback to the
understanding part. Furthermore, we propose a novel data filtering algorithm to
select high-quality and diverse emotional images generated by the well-trained
model, which explicitly feedback into the understanding part. Together, these
generation-driven dual feedback processes enhance the model's understanding
capacity. Extensive experiments show that UniEmo significantly outperforms
state-of-the-art methods in both emotional understanding and generation tasks.
The code for the proposed method is available at
https://github.com/JiuTian-VL/UniEmo.

</details>


### [53] [Multi-Prompt Progressive Alignment for Multi-Source Unsupervised Domain Adaptation](https://arxiv.org/abs/2507.23373)
*Haoran Chen,Zexiao Wang,Haidong Cao,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: MP^2A 是一种新的方法，通过逐步对齐数据来改进 CLIP 在无监督域适应中的表现，尤其是在有多个源域的情况下。它首先在高置信度样本上训练，然后逐步加入难样本，从而获得更好的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于 CLIP 的无监督域适应（UDA）方法通常一次性地利用所有伪标记的目标数据进行对齐，这种单次对齐方式容易受到噪声和难分类样本的影响，导致误差累积和次优的特征学习。在多源场景下，由于多个源域的域偏移和噪声水平不同，这种不稳定性更为严重。因此，需要一种更有效的策略来处理这些挑战。

Method: MP^2A 是一种渐进式对齐策略，它首先在高置信度目标样本子集上训练模型，以学习良好的对齐表示，然后逐渐整合更具挑战性的样本，以优化模型并学习真正域不变的特征。该方法旨在缓解因噪声和难以分类的样本导致的误差累积问题，特别是在多源场景下。

Result: MP^2A 在 ImageCLEF、Office-Home 和 DomainNet 三个常用的 UDA 基准测试中进行了测试，实验结果表明，与目前大多数基于 CLIP 的 MS-UDA 方法相比，MP^2A 取得了最先进的性能，证明了该方法的有效性。

Conclusion: MP^2A 是一种创新的渐进式对齐策略，能够有效缓解 CLIP 在无监督域适应（UDA）和多源无监督域适应（MS-UDA）任务中的误差累积和特征学习不佳等问题。通过首先在高置信度目标样本上训练模型，然后逐步引入更具挑战性的样本，MP^2A 能够实现更鲁棒的收敛，学习到真正域不变的特征，并在 ImageCLEF、Office-Home 和 DomainNet 等基准测试中取得了最先进的性能。

Abstract: Large Vision-Language Models like CLIP have become a powerful foundation for
Unsupervised Domain Adaptation due to their strong zero-shot generalization.
State-of-the-art methods typically leverage CLIP to generate pseudo-labels for
the target domain, then fine-tune the model to learn domain-invariant features.
However, these methods attempt to align source and target domains using all
pseudo-labeled data simultaneously. This one-shot alignment struggles with
noisy, hard-to-classify samples, leading to error propagation and suboptimal
feature learning. The problem is even more amplified in the multi-source
scenario, where diverse domain gaps and varying noise levels across multiple
source domains further destabilize the alignment process. To address this
issue, in this work, we propose a progressive alignment strategy for adapting
CLIP to unlabeled downstream task. Our method begins by training the model on a
high-confidence subset of target samples, allowing it to first learn a
well-aligned representation from the most reliable data. As training
progresses, it gradually incorporates more challenging samples, guiding the
model to refine its understanding without being overwhelmed by initial label
noise. This progressive approach effectively mitigates confirmation bias and
promotes a more robust convergence, allowing for the learning of genuinely
domain-invariant features. We name our approach MP^2A and test it on three
popular UDA benchmarks, namely ImageCLEF, Office-Home, and the most challenging
DomainNet. Experiments showcase that MP^2A achieves state-of-the-art
performance when compared with most recent CLIP-based MS-UDA approaches,
demonstrating the effectiveness of our approach.

</details>


### [54] [NeRF Is a Valuable Assistant for 3D Gaussian Splatting](https://arxiv.org/abs/2507.23374)
*Shuangkang Fang,I-Chao Shen,Takeo Igarashi,Yufeng Wang,ZeSheng Wang,Yi Yang,Wenrui Ding,Shuchang Zhou*

Main category: cs.CV

TL;DR: NeRF-GS框架通过结合NeRF和3DGS的优点，在3D场景表示方面取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 为了解决3D高斯泼溅（3DGS）的局限性，例如对高斯初始化的敏感性、有限的空间感知和较弱的类间相关性，并探索NeRF和3DGS的互补性。

Method: 提出了一种名为NeRF-GS的新框架，该框架可同时优化神经辐射场（NeRF）和3D高斯泼溅（3DGS）。NeRF-GS利用NeRF固有的连续空间表示来缓解3DGS的局限性，例如对高斯初始化的敏感性、有限的空间感知和较弱的类间相关性。此外，NeRF-GS还通过优化隐式特征和高斯位置的残差向量来增强3DGS的个性化功能。

Result: 实验结果表明，NeRF-GS在基准数据集上超越了现有方法，达到了最先进的性能。

Conclusion: NeRF和3DGS是互补的，而不是相互竞争的，这为结合3DGS和NeRF以实现高效3D场景表示的混合方法提供了新的见解。

Abstract: We introduce NeRF-GS, a novel framework that jointly optimizes Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework
leverages the inherent continuous spatial representation of NeRF to mitigate
several limitations of 3DGS, including sensitivity to Gaussian initialization,
limited spatial awareness, and weak inter-Gaussian correlations, thereby
enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and
progressively align its spatial features with NeRF, enabling both
representations to be optimized within the same scene through shared 3D spatial
information. We further address the formal distinctions between the two
approaches by optimizing residual vectors for both implicit features and
Gaussian positions to enhance the personalized capabilities of 3DGS.
Experimental results on benchmark datasets show that NeRF-GS surpasses existing
methods and achieves state-of-the-art performance. This outcome confirms that
NeRF and 3DGS are complementary rather than competing, offering new insights
into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene
representation.

</details>


### [55] [AGA: An adaptive group alignment framework for structured medical cross-modal representation learning](https://arxiv.org/abs/2507.23402)
*Wei Li,Xun Gong,Jiao Li,Xiaobin Sun*

Main category: cs.CV

TL;DR: AGA框架通过其双向分组机制和实例感知损失，解决了医学领域视觉-语言预训练中忽略报告结构和需要大量负样本的问题，并在图像-文本检索和分类任务上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有的医学领域视觉-语言预训练方法往往忽略了临床报告的内在结构，并且对比学习框架在小规模医学数据集上需要大量的负样本，这不切实际。

Method: 提出了一种名为自适应分组对齐（AGA）的新框架，该框架包含双向分组机制、语言分组阈值门控和视觉分组阈值门控，以及实例感知分组对齐损失和双向跨模态分组对齐模块。

Result: AGA框架能够从成对的医学图像和报告中捕获结构化语义，并且移除了对外部负样本的需求。

Conclusion: 该方法在图像-文本检索和分类任务上实现了强大的性能，并且在微调和零样本设置下都表现良好。

Abstract: Learning medical visual representations from paired images and reports is a
promising direction in representation learning. However, current
vision-language pretraining methods in the medical domain often simplify
clinical reports into single entities or fragmented tokens, ignoring their
inherent structure. In addition, contrastive learning frameworks typically
depend on large quantities of hard negative samples, which is impractical for
small-scale medical datasets. To tackle these challenges, we propose Adaptive
Grouped Alignment (AGA), a new framework that captures structured semantics
from paired medical images and reports. AGA introduces a bidirectional grouping
mechanism based on a sparse similarity matrix. For each image-report pair, we
compute fine-grained similarities between text tokens and image patches. Each
token selects its top-matching patches to form a visual group, and each patch
selects its most related tokens to form a language group. To enable adaptive
grouping, we design two threshold gating modules, called Language Grouped
Threshold Gate and Vision Grouped Threshold Gate, which learn grouping
thresholds dynamically. Group representations are computed as weighted averages
based on similarity scores. To align each token with its group representation,
we introduce an Instance Aware Group Alignment loss that operates within each
image-text pair, removing the need for external negatives. Finally, a
Bidirectional Cross-modal Grouped Alignment module is applied to enhance
fine-grained alignment between visual and linguistic group representations.
Extensive experiments on public and private datasets show that our method
achieves strong performance on image-text retrieval and classification tasks
under both fine-tuning and zero-shot settings.

</details>


### [56] [Out-of-Distribution Detection in Medical Imaging via Diffusion Trajectories](https://arxiv.org/abs/2507.23411)
*Lemar Abdi,Francisco Caetano,Amaan Valiuddin,Christiaan Viviers,Hamdi Joudeh,Fons van der Sommen*

Main category: cs.CV

TL;DR: 医学成像OOD检测的一种新方法，利用SBDDM的扩散轨迹，无需重建，实现快速、准确、鲁棒的异常检测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医学成像中，无监督的OOD检测是一种识别低发生率病理病例的有吸引力的方法。与监督方法不同，OOD方法不需要标签，并且对数据不平衡具有内在的鲁棒性。现有的生成方法通常依赖于似然估计或重建误差，但这些方法可能计算成本高、不可靠，并且在内点数据发生变化时需要重新训练，这妨碍了它们有效、一致且鲁棒地区分标称输入和异常输入的效率。

Method: 提出了一种无重建的OOD检测方法，该方法利用了基于Stein分数扩散模型（SBDDM）的前向扩散轨迹。通过估计的Stein分数捕获轨迹曲率，该方法仅用五个扩散步骤即可实现准确的异常评分。

Result: 与现有方法相比，SBDDM在Near-OOD和Far-OOD检测方面分别取得了高达10.43%和18.10%的相对改进，大大降低了推理的计算成本，并在多个Near-OOD和Far-OOD基准测试中实现了最先进的性能。

Conclusion: 该方法在Near-OOD和Far-OOD检测方面分别取得了高达10.43%和18.10%的相对改进，是一种实用的、可用于实时可靠的计算机辅助诊断的构建模块。

Abstract: In medical imaging, unsupervised out-of-distribution (OOD) detection offers
an attractive approach for identifying pathological cases with extremely low
incidence rates. In contrast to supervised methods, OOD-based approaches
function without labels and are inherently robust to data imbalances. Current
generative approaches often rely on likelihood estimation or reconstruction
error, but these methods can be computationally expensive, unreliable, and
require retraining if the inlier data changes. These limitations hinder their
ability to distinguish nominal from anomalous inputs efficiently, consistently,
and robustly. We propose a reconstruction-free OOD detection method that
leverages the forward diffusion trajectories of a Stein score-based denoising
diffusion model (SBDDM). By capturing trajectory curvature via the estimated
Stein score, our approach enables accurate anomaly scoring with only five
diffusion steps. A single SBDDM pre-trained on a large, semantically aligned
medical dataset generalizes effectively across multiple Near-OOD and Far-OOD
benchmarks, achieving state-of-the-art performance while drastically reducing
computational cost during inference. Compared to existing methods, SBDDM
achieves a relative improvement of up to 10.43% and 18.10% for Near-OOD and
Far-OOD detection, making it a practical building block for real-time, reliable
computer-aided diagnosis.

</details>


### [57] [Honey Adulteration Detection using Hyperspectral Imaging and Machine Learning](https://arxiv.org/abs/2507.23416)
*Mokhtar A. Al-Awadhi,Ratnadeep R. Deshmukh*

Main category: cs.CV

TL;DR: 本研究利用高光谱成像和LDA/KNN机器学习技术，开发了一个能以96.39%的准确率检测蜂蜜掺糖的系统。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种基于机器学习的系统，利用蜂蜜的高光谱成像数据自动检测掺糖现象。

Method: 本研究提出一个两阶段的机器学习系统。首先，使用线性判别分析（LDA）提取特征，然后利用K近邻（KNN）模型对蜂蜜进行植物来源分类。其次，再次使用LDA提取特征，并利用KNN模型识别掺糖水平。

Result: 所提出的系统在公开的蜂蜜高光谱图像数据集上进行了评估，实现了96.39%的总体交叉验证准确率，证明了其检测蜂蜜掺糖的有效性。

Conclusion: 该研究提出的基于高光谱成像和机器学习的系统能够准确检测蜂蜜掺糖问题，提供了一种比传统化学方法更优的替代方案。

Abstract: This paper aims to develop a machine learning-based system for automatically
detecting honey adulteration with sugar syrup, based on honey hyperspectral
imaging data. First, the floral source of a honey sample is classified by a
botanical origin identification subsystem. Then, the sugar syrup adulteration
is identified, and its concentration is quantified by an adulteration detection
subsystem. Both subsystems consist of two steps. The first step involves
extracting relevant features from the honey sample using Linear Discriminant
Analysis (LDA). In the second step, we utilize the K-Nearest Neighbors (KNN)
model to classify the honey botanical origin in the first subsystem and
identify the adulteration level in the second subsystem. We assess the proposed
system performance on a public honey hyperspectral image dataset. The result
indicates that the proposed system can detect adulteration in honey with an
overall cross-validation accuracy of 96.39%, making it an appropriate
alternative to the current chemical-based detection methods.

</details>


### [58] [Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification](https://arxiv.org/abs/2507.23436)
*Abdellah Zakaria Sellam,Salah Eddine Bekhouche,Cosimo Distante,Abdelmalik Taleb-Ahmed*

Main category: cs.CV

TL;DR: 该研究通过引入Kolmogorov-Arnold Networks (KANs) 替代MLP投影，改进了双教师自监督框架，以更好地处理艺术风格分类中的全局构成上下文和复杂的风格特征交互，并在实验中取得了更高的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有双教师自监督框架依赖于线性投影层和局部关注点，难以模拟全局构成上下文和复杂的风格特征交互，并且缺乏专家标记的数据集。

Method: 通过使用Kolmogorov-Arnold Networks (KANs) 替换传统的MLP投影和预测头，并保留两个教师网络的互补指导（一个强调局部纹理和笔触模式，另一个捕捉更广泛的风格层次），来增强双教师知识蒸馏框架，以解决现有模型在处理全局构成上下文和复杂风格特征交互方面的局限性。

Result: 在WikiArt和Pandora18k数据集上的实验表明，该方法在Top-1准确率方面优于基础双教师架构。

Conclusion: 研究结果强调了KAN在解开复杂风格流形中的重要性，其线性探针精度优于MLP投影。

Abstract: Art style classification remains a formidable challenge in computational
aesthetics due to the scarcity of expertly labeled datasets and the intricate,
often nonlinear interplay of stylistic elements. While recent dual-teacher
self-supervised frameworks reduce reliance on labeled data, their linear
projection layers and localized focus struggle to model global compositional
context and complex style-feature interactions. We enhance the dual-teacher
knowledge distillation framework to address these limitations by replacing
conventional MLP projection and prediction heads with Kolmogorov-Arnold
Networks (KANs). Our approach retains complementary guidance from two teacher
networks, one emphasizing localized texture and brushstroke patterns, the other
capturing broader stylistic hierarchies while leveraging KANs' spline-based
activations to model nonlinear feature correlations with mathematical
precision. Experiments on WikiArt and Pandora18k demonstrate that our approach
outperforms the base dual teacher architecture in Top-1 accuracy. Our findings
highlight the importance of KANs in disentangling complex style manifolds,
leading to better linear probe accuracy than MLP projections.

</details>


### [59] [Adjustable Spatio-Spectral Hyperspectral Image Compression Network](https://arxiv.org/abs/2507.23447)
*Martin Hermann Paul Fuchs,Behnood Rasti,Begüm Demir*

Main category: cs.CV

TL;DR: HyCASS是一种新的高光谱图像压缩网络，可以同时调整光谱和空间压缩，并在实验中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 由于遥感领域的高光谱数据量快速增长，对高效存储的需求日益增长，这推动了对基于学习的高光谱图像压缩的关注。然而，现有研究尚未充分探讨光谱和空间压缩的单独及联合效应对基于学习的高光谱图像压缩的影响。

Method: 提出了一种名为HyCASS的可调光谱-空间高光谱图像压缩网络，该网络包含光谱编码器、空间编码器、压缩率适配器编码器、压缩率适配器解码器、空间解码器和光谱解码器模块，并利用卷积层和Transformer块来捕捉光谱和空间冗余。

Result: 实验结果表明，HyCASS模型在可调高光谱图像压缩方面优于现有的基于学习的压缩模型，并根据实验结果提出了在不同压缩比下平衡光谱和空间压缩的指导方针。

Conclusion: HyCASS在两个高光谱数据集上进行了实验，与现有的基于学习的压缩模型相比，证明了其在可调高光谱图像压缩方面的有效性。研究结果为在不同压缩比下有效平衡光谱和空间压缩提供了指导。

Abstract: With the rapid growth of hyperspectral data archives in remote sensing (RS),
the need for efficient storage has become essential, driving significant
attention toward learning-based hyperspectral image (HSI) compression. However,
a comprehensive investigation of the individual and joint effects of spectral
and spatial compression on learning-based HSI compression has not been
thoroughly examined yet. Conducting such an analysis is crucial for
understanding how the exploitation of spectral, spatial, and joint
spatio-spectral redundancies affects HSI compression. To address this issue, we
propose Adjustable Spatio-Spectral Hyperspectral Image Compression Network
(HyCASS), a learning-based model designed for adjustable HSI compression in
both spectral and spatial dimensions. HyCASS consists of six main modules: 1)
spectral encoder; 2) spatial encoder; 3) compression ratio (CR) adapter
encoder; 4) CR adapter decoder; 5) spatial decoder; and 6) spectral decoder
module. The modules employ convolutional layers and transformer blocks to
capture both short-range and long-range redundancies. Experimental results on
two HSI benchmark datasets demonstrate the effectiveness of our proposed
adjustable model compared to existing learning-based compression models. Based
on our results, we establish a guideline for effectively balancing spectral and
spatial compression across different CRs, taking into account the spatial
resolution of the HSIs. Our code and pre-trained model weights are publicly
available at https://git.tu-berlin.de/rsim/hycass .

</details>


### [60] [Machine learning and machine learned prediction in chest X-ray images](https://arxiv.org/abs/2507.23455)
*Shereiff Garrett,Abhinav Adhikari,Sarina Gautam,DaShawn Marquis Morris,Chandra Mani Adhikari*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Machine learning and artificial intelligence are fast-growing fields of
research in which data is used to train algorithms, learn patterns, and make
predictions. This approach helps to solve seemingly intricate problems with
significant accuracy without explicit programming by recognizing complex
relationships in data. Taking an example of 5824 chest X-ray images, we
implement two machine learning algorithms, namely, a baseline convolutional
neural network (CNN) and a DenseNet-121, and present our analysis in making
machine-learned predictions in predicting patients with ailments. Both baseline
CNN and DenseNet-121 perform very well in the binary classification problem
presented in this work. Gradient-weighted class activation mapping shows that
DenseNet-121 correctly focuses on essential parts of the input chest X-ray
images in its decision-making more than the baseline CNN.

</details>


### [61] [Mitigating Resolution-Drift in Federated Learning: Case of Keypoint Detection](https://arxiv.org/abs/2507.23461)
*Taeheon Lim,Joohyung Lee,Kyungjae Lee,Jungchan Cho*

Main category: cs.CV

TL;DR: 联邦学习在人体姿态估计等任务中面临“分辨率漂移”问题。本文提出RAF方法，通过多分辨率知识蒸馏解决此问题，提升了模型性能和鲁棒性，并具有普适性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习（FL）研究主要集中在解决统计异质性和通信效率问题，并在分类任务上取得了成功。然而，FL在人体姿态估计等非分类任务上的应用仍处于探索阶段。本文发现了“分辨率漂移”这一关键问题，即由于客户端间分辨率不一致导致性能显著下降，并认识到分辨率是影响非IID数据的重要因素。

Method: 本研究提出了一种名为RAF（Resolution-Adaptive Federated Learning）的方法，该方法利用基于热图的知识蒸馏技术来解决联邦学习中的“分辨率漂移”问题。具体而言，RAF通过多分辨率知识蒸馏，让高分辨率输出（教师）指导低分辨率输出（学生），从而在不导致过拟合的情况下提高模型对分辨率变化的鲁棒性。

Result: 通过大量实验和理论分析，证明了RAF能有效缓解分辨率漂移，显著提升性能。同时，RAF可以轻松整合到现有联邦学习框架中。此外，t-SNE分析显示，RAF具有超越人体姿态估计任务的潜力，能够推广到其他需要保留空间细节的任务。

Conclusion: RAF可以有效缓解联邦学习中的分辨率漂移问题，并提升模型在非分类任务（如人体姿态估计）上的性能。RAF通过多分辨率知识蒸馏增强了模型对分辨率的鲁棒性，且可以无缝集成到现有的联邦学习框架中，其对高分辨率任务的普适性也得到了初步验证。

Abstract: The Federated Learning (FL) approach enables effective learning across
distributed systems, while preserving user data privacy. To date, research has
primarily focused on addressing statistical heterogeneity and communication
efficiency, through which FL has achieved success in classification tasks.
However, its application to non-classification tasks, such as human pose
estimation, remains underexplored. This paper identifies and investigates a
critical issue termed ``resolution-drift,'' where performance degrades
significantly due to resolution variability across clients. Unlike class-level
heterogeneity, resolution drift highlights the importance of resolution as
another axis of not independent or identically distributed (non-IID) data. To
address this issue, we present resolution-adaptive federated learning (RAF), a
method that leverages heatmap-based knowledge distillation. Through
multi-resolution knowledge distillation between higher-resolution outputs
(teachers) and lower-resolution outputs (students), our approach enhances
resolution robustness without overfitting. Extensive experiments and
theoretical analysis demonstrate that RAF not only effectively mitigates
resolution drift and achieves significant performance improvements, but also
can be integrated seamlessly into existing FL frameworks. Furthermore, although
this paper focuses on human pose estimation, our t-SNE analysis reveals
distinct characteristics between classification and high-resolution
representation tasks, supporting the generalizability of RAF to other tasks
that rely on preserving spatial detail.

</details>


### [62] [CST Anti-UAV: A Thermal Infrared Benchmark for Tiny UAV Tracking in Complex Scenes](https://arxiv.org/abs/2507.23473)
*Bin Xie,Congxuan Zhang,Fagan Wang,Peng Liu,Feng Lu,Zhen Chen,Weiming Hu*

Main category: cs.CV

TL;DR: 针对现有无人机追踪数据集在处理微小目标和复杂场景方面的不足，本文提出了CST Anti-UAV数据集，该数据集包含大量微小无人机和复杂场景的视频序列及详细标注。实验结果表明，在复杂场景下追踪微小无人机仍具挑战性，现有方法的性能有待提升，该数据集将促进反无人机技术的发展。


<details>
  <summary>Details</summary>
Motivation: 现有的无人机追踪数据集在真实世界场景中的应用存在局限性，缺乏对微小尺寸无人机和复杂场景多样性的代表性，因此需要一个新的数据集来克服这些限制，以推动反无人机任务的研究。

Method: 提出了一种名为CST Anti-UAV的新型热红外数据集，该数据集包含220个视频序列和超过24万个高质量边界框标注，特别关注微小尺寸的无人机目标和复杂多变的场景。该数据集的特点是首次包含了完整的帧级属性标注，能够进行精确的评估。

Result: 在CST Anti-UAV数据集上对20种现有的单目标追踪（SOT）方法进行了评估，结果表明在复杂环境中追踪微小尺寸无人机仍然是一个挑战，现有最优方法的准确率仅为35.92%，远低于在Anti-UAV410数据集上观察到的67.69%。

Conclusion: 现有的无人机追踪数据集在真实场景中的适用性有限，并且在处理微小尺寸的无人机和复杂场景方面存在不足。CST Anti-UAV数据集的推出填补了这一空白，为反无人机任务的研究提供了新的方向和基准。

Abstract: The widespread application of Unmanned Aerial Vehicles (UAVs) has raised
serious public safety and privacy concerns, making UAV perception crucial for
anti-UAV tasks. However, existing UAV tracking datasets predominantly feature
conspicuous objects and lack diversity in scene complexity and attribute
representation, limiting their applicability to real-world scenarios. To
overcome these limitations, we present the CST Anti-UAV, a new thermal infrared
dataset specifically designed for Single Object Tracking (SOT) in Complex
Scenes with Tiny UAVs (CST). It contains 220 video sequences with over 240k
high-quality bounding box annotations, highlighting two key properties: a
significant number of tiny-sized UAV targets and the diverse and complex
scenes. To the best of our knowledge, CST Anti-UAV is the first dataset to
incorporate complete manual frame-level attribute annotations, enabling precise
evaluations under varied challenges. To conduct an in-depth performance
analysis for CST Anti-UAV, we evaluate 20 existing SOT methods on the proposed
dataset. Experimental results demonstrate that tracking tiny UAVs in complex
environments remains a challenge, as the state-of-the-art method achieves only
35.92% state accuracy, much lower than the 67.69% observed on the Anti-UAV410
dataset. These findings underscore the limitations of existing benchmarks and
the need for further advancements in UAV tracking research. The CST Anti-UAV
benchmark is about to be publicly released, which not only fosters the
development of more robust SOT methods but also drives innovation in anti-UAV
systems.

</details>


### [63] [3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding](https://arxiv.org/abs/2507.23478)
*Ting Huang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为3D-R1的3D视觉语言基础模型，通过构建包含思维链的数据集、采用强化学习和动态视图选择策略，显著提升了3D场景理解的推理和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的3D视觉语言模型（VLMs）在鲁棒推理和泛化方面存在不足，主要由于缺乏高质量空间数据和视角假设的静态性。为了解决这些问题，提出3D-R1来增强3D VLMs的推理能力。

Method: 1.构建包含思维链(CoT)的高质量合成数据集Scene-30K，并使用Gemini 2.5 Pro的数据引擎进行初始化。
2.在强化学习训练过程中，利用GRPO等策略并结合感知奖励、语义相似性奖励和格式奖励来提升推理能力。
3.引入动态视图选择策略，自适应地选择信息量最大的视角进行3D场景理解。

Result: 3D-R1在各项3D场景理解任务的实验中，平均取得了10%的性能提升，证明了其在提升3D场景理解的推理和泛化能力方面的有效性。

Conclusion: 3D-R1在各种3D场景基准测试中平均提高了10%，有效提升了3D场景理解中的推理和泛化能力。

Abstract: Large vision-language models (VLMs) have made significant strides in 2D
visual understanding tasks, sparking interest in extending these capabilities
to 3D scene understanding. However, current 3D VLMs often struggle with robust
reasoning and generalization due to limitations in high-quality spatial data
and the static nature of viewpoint assumptions. To address these challenges, we
propose 3D-R1, a foundation model that enhances the reasoning capabilities of
3D VLMs. Specifically, we first construct a high-quality synthetic dataset with
CoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine
based on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1.
Moreover, we leverage RLHF policy such as GRPO in the reinforcement learning
training process to enhance reasoning capabilities and introduce three reward
functions: a perception reward, a semantic similarity reward and a format
reward to maintain detection accuracy and answer semantic precision.
Furthermore, we introduce a dynamic view selection strategy that adaptively
chooses the most informative perspectives for 3D scene understanding. Extensive
experiments demonstrate that 3D-R1 delivers an average improvement of 10%
across various 3D scene benchmarks, highlighting its effectiveness in enhancing
reasoning and generalization in 3D scene understanding. Code:
https://github.com/AIGeeksGroup/3D-R1. Website:
https://aigeeksgroup.github.io/3D-R1.

</details>


### [64] [Seeing More with Less: Video Capsule Endoscopy with Multi-Task Learning](https://arxiv.org/abs/2507.23479)
*Julia Werner,Oliver Bause,Julius Oexle,Maxime Le Floch,Franz Brinkmann,Jochen Hampe,Oliver Bringmann*

Main category: cs.CV

TL;DR: 通过结合定位和异常检测的多任务学习，并优化模型大小，成功解决了视频胶囊内窥镜的能耗问题，并提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决视频胶囊内窥镜电池寿命短的问题，利用人工智能在设备端进行实时决策以降低能耗，但面临数据稀疏和设备资源有限的挑战。

Method: 提出了一种结合了精确自定位和肠道内异常检测功能的多任务神经网络，并通过Viterbi解码进行时间序列分析，同时严格控制模型参数量以满足设备部署需求。

Result: 该多任务模型实现了93.63%的定位准确率和87.48%的异常检测准确率，且模型参数量仅为100万，优于现有基线模型。

Conclusion: 该多任务模型通过将精确定位和异常检测相结合，并优化参数量以适应胶囊内窥镜的资源限制，在Galar数据集上取得了优于单任务模型的性能，并在定位和异常检测任务上分别达到了93.63%和87.48%的准确率。

Abstract: Video capsule endoscopy has become increasingly important for investigating
the small intestine within the gastrointestinal tract. However, a persistent
challenge remains the short battery lifetime of such compact sensor edge
devices. Integrating artificial intelligence can help overcome this limitation
by enabling intelligent real-time decision- making, thereby reducing the energy
consumption and prolonging the battery life. However, this remains challenging
due to data sparsity and the limited resources of the device restricting the
overall model size. In this work, we introduce a multi-task neural network that
combines the functionalities of precise self-localization within the
gastrointestinal tract with the ability to detect anomalies in the small
intestine within a single model. Throughout the development process, we
consistently restricted the total number of parameters to ensure the
feasibility to deploy such model in a small capsule. We report the first
multi-task results using the recently published Galar dataset, integrating
established multi-task methods and Viterbi decoding for subsequent time-series
analysis. This outperforms current single-task models and represents a
significant ad- vance in AI-based approaches in this field. Our model achieves
an accu- racy of 93.63% on the localization task and an accuracy of 87.48% on
the anomaly detection task. The approach requires only 1 million parameters
while surpassing the current baselines.

</details>


### [65] [FastPoint: Accelerating 3D Point Cloud Model Inference via Sample Point Distance Prediction](https://arxiv.org/abs/2507.23480)
*Donghyun Lee,Dawoon Jeong,Jae W. Lee,Hongil Yoon*

Main category: cs.CV

TL;DR: 提出了一种名为FastPoint的软件加速技术，通过预测最远点采样中的距离趋势来提高3D点云处理的效率。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在处理大规模、不规则3D点云时效率低下的问题。

Method: 通过预测采样点之间的距离趋势来优化最远点采样过程，从而避免计算所有点对之间的距离。

Result: 在 NVIDIA RTX 3090 GPU 上实现了 2.55 倍的端到端加速，同时保持了精度。

Conclusion: FastPoint 显著加速了最远点采样和邻域搜索，同时保持了采样质量和模型性能。

Abstract: Deep neural networks have revolutionized 3D point cloud processing, yet
efficiently handling large and irregular point clouds remains challenging. To
tackle this problem, we introduce FastPoint, a novel software-based
acceleration technique that leverages the predictable distance trend between
sampled points during farthest point sampling. By predicting the distance
curve, we can efficiently identify subsequent sample points without
exhaustively computing all pairwise distances. Our proposal substantially
accelerates farthest point sampling and neighbor search operations while
preserving sampling quality and model performance. By integrating FastPoint
into state-of-the-art 3D point cloud models, we achieve 2.55x end-to-end
speedup on NVIDIA RTX 3090 GPU without sacrificing accuracy.

</details>


### [66] [Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion](https://arxiv.org/abs/2507.23483)
*Mutian Xu,Chongjie Ye,Haolin Liu,Yushuang Wu,Jiahao Chang,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出Stable-Sim2Real，一个两阶段扩散模型，用于生成更逼真的3D数据。该方法通过学习模拟数据与真实数据之间的映射，并在下游任务中取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D数据模拟方法难以捕捉真实数据的复杂性，而数据驱动的方法也遇到了瓶颈。本文旨在探索一种新的数据驱动3D模拟解决方案，以弥合模拟数据与真实数据之间的差距，提升3D视觉任务的性能。

Method: 本文提出了一种名为Stable-Sim2Real的新型数据驱动3D模拟方法。该方法采用两阶段深度扩散模型：第一阶段微调Stable-Diffusion生成真实与合成深度图的残差，得到粗略但稳定的深度图；第二阶段通过调整扩散损失，并利用3D判别器识别出的关键区域，进一步优化深度图，使其更贴近真实模式。

Result: 通过Stable-Sim2Real方法生成的3D数据在下游3D视觉任务中显著提升了性能，并且模拟数据与真实捕获的数据模式高度相似。此外，本文还提供了一个新的基准方案来评估3D数据模拟方法。

Conclusion: Stable-Sim2Real通过新颖的两阶段深度扩散模型，有效解决了3D数据模拟的挑战，生成的3D数据与真实数据具有高度相似性，并在下游3D视觉任务中显著提升了性能。

Abstract: 3D data simulation aims to bridge the gap between simulated and real-captured
3D data, which is a fundamental problem for real-world 3D visual tasks. Most 3D
data simulation methods inject predefined physical priors but struggle to
capture the full complexity of real data. An optimal approach involves learning
an implicit mapping from synthetic to realistic data in a data-driven manner,
but progress in this solution has met stagnation in recent studies. This work
explores a new solution path of data-driven 3D simulation, called
Stable-Sim2Real, based on a novel two-stage depth diffusion model. The initial
stage finetunes Stable-Diffusion to generate the residual between the real and
synthetic paired depth, producing a stable but coarse depth, where some local
regions may deviate from realistic patterns. To enhance this, both the
synthetic and initial output depth are fed into a second-stage diffusion, where
diffusion loss is adjusted to prioritize these distinct areas identified by a
3D discriminator. We provide a new benchmark scheme to evaluate 3D data
simulation methods. Extensive experiments show that training the network with
the 3D simulated data derived from our method significantly enhances
performance in real-world 3D visual tasks. Moreover, the evaluation
demonstrates the high similarity between our 3D simulated data and
real-captured patterns. Project page:
https://mutianxu.github.io/stable-sim2real/.

</details>


### [67] [Hyperbolic Cycle Alignment for Infrared-Visible Image Fusion](https://arxiv.org/abs/2507.23508)
*Timing Li,Bing Cao,Jiahe Feng,Haifang Cao,Qinghau Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: This paper introduces Hy-CycleAlign, the first image registration method using hyperbolic space. It improves cross-modal image alignment and fusion by employing a cyclic registration framework and a hyperbolic contrastive alignment module, outperforming existing methods in experiments.


<details>
  <summary>Details</summary>
Motivation: Existing image registration methods, often based on Euclidean space, struggle with effective cross-modal misalignment, leading to suboptimal alignment and fusion quality. The motivation is to overcome this limitation by exploring image alignment in non-Euclidean (hyperbolic) space.

Method: The paper proposes the Hyperbolic Cycle Alignment Network (Hy-CycleAlign), a novel image registration method based on hyperbolic space. It features a dual-path cross-modal cyclic registration framework (forward and backward registration networks) and a Hyperbolic Hierarchy Contrastive Alignment (H$^2$CA) module that maps images into hyperbolic space to impose registration constraints and reduce modality discrepancies.

Result: Extensive experiments show that Hy-CycleAlign significantly outperforms existing methods in image alignment and fusion. The analysis also demonstrates that hyperbolic space enables more sensitive and effective multi-modal image registration compared to Euclidean space.

Conclusion: The proposed Hy-CycleAlign method, which utilizes hyperbolic space for image registration, significantly outperforms existing approaches in both image alignment and fusion for misaligned multi-modal images.

Abstract: Image fusion synthesizes complementary information from multiple sources,
mitigating the inherent limitations of unimodal imaging systems. Accurate image
registration is essential for effective multi-source data fusion. However,
existing registration methods, often based on image translation in Euclidean
space, fail to handle cross-modal misalignment effectively, resulting in
suboptimal alignment and fusion quality. To overcome this limitation, we
explore image alignment in non-Euclidean space and propose a Hyperbolic Cycle
Alignment Network (Hy-CycleAlign). To the best of our knowledge, Hy-CycleAlign
is the first image registration method based on hyperbolic space. It introduces
a dual-path cross-modal cyclic registration framework, in which a forward
registration network aligns cross-modal inputs, while a backward registration
network reconstructs the original image, forming a closed-loop registration
structure with geometric consistency. Additionally, we design a Hyperbolic
Hierarchy Contrastive Alignment (H$^{2}$CA) module, which maps images into
hyperbolic space and imposes registration constraints, effectively reducing
interference caused by modality discrepancies. We further analyze image
registration in both Euclidean and hyperbolic spaces, demonstrating that
hyperbolic space enables more sensitive and effective multi-modal image
registration. Extensive experiments on misaligned multi-modal images
demonstrate that our method significantly outperforms existing approaches in
both image alignment and fusion. Our code will be publicly available.

</details>


### [68] [I Am Big, You Are Little; I Am Right, You Are Wrong](https://arxiv.org/abs/2507.23509)
*David A. Kelly,Akchunya Chanchal,Nathan Blake*

Main category: cs.CV

TL;DR: 通过分析模型关注的关键像素（集中度），我们发现不同模型架构（如ConvNeXt和EVA）在理解图像时关注的像素区域不同，并且模型在错误分类时会关注更多像素。


<details>
  <summary>Details</summary>
Motivation: 随着不同尺寸和架构的分类器激增，选择正确模型的问题变得越来越重要。尽管我们可以统计地评估模型的分类准确性，但我们对其工作原理的理解有限。为了深入了解不同视觉模型的决策过程，我们提出使用最小充分像素集。

Method: 使用最小充分像素集来衡量模型的“集中度”，即模型视角下捕捉图像本质的像素。通过比较像素集的位置、重叠和大小来评估模型。

Result: 不同模型架构在集中度方面（大小和位置）存在统计学上的显著差异。错误分类的图像比正确分类的图像具有更大的像素集。

Conclusion: 不同的模型架构在像素集的大小和位置上存在统计学上的显著差异，其中ConvNeXt和EVA模型与其他模型差异明显。错误分类的图像与更大的像素集相关联。

Abstract: Machine learning for image classification is an active and rapidly developing
field. With the proliferation of classifiers of different sizes and different
architectures, the problem of choosing the right model becomes more and more
important.
  While we can assess a model's classification accuracy statistically, our
understanding of the way these models work is unfortunately limited. In order
to gain insight into the decision-making process of different vision models, we
propose using minimal sufficient pixels sets to gauge a model's
`concentration': the pixels that capture the essence of an image through the
lens of the model. By comparing position, overlap, and size of sets of pixels,
we identify that different architectures have statistically different
concentration, in both size and position. In particular, ConvNext and EVA
models differ markedly from the others. We also identify that images which are
misclassified are associated with larger pixels sets than correct
classifications.

</details>


### [69] [ART: Adaptive Relation Tuning for Generalized Relation Prediction](https://arxiv.org/abs/2507.23543)
*Gopika Sudhakaran,Hikaru Shindo,Patrick Schramowski,Simone Schaub-Meyer,Kristian Kersting,Stefan Roth*

Main category: cs.CV

TL;DR: ART是一种通过指令调优和实例选择来改进视觉关系检测（VRD）的新框架，它能更好地处理新颖和复杂的关系，并具有泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的VRD模型难以泛化到训练数据之外的关系，而基于提示调优的方法存在手动设计提示和难以处理新颖/复杂关系的局限性。因此，需要一种更有效的解决方案。

Method: ART框架通过将VRD数据集转换为指令调优格式，并采用自适应采样算法，引导模型关注信息量丰富的关系，从而实现对视觉语言模型（VLMs）的自适应关系调优。

Result: ART框架在多项测试中显著优于基线模型，并成功推断出未见过 সম্পর্ক概念，这是现有VRD方法所不具备的能力。

Conclusion: ART框架通过指令调优和实例选择，有效提升了视觉关系检测（VRD）模型对新颖和复杂关系的泛化能力，并在多个测试集上表现优于基线模型，还能推断未见过 সম্পর্ক概念。此外，ART在复杂场景分割任务中也展现了其实用价值。

Abstract: Visual relation detection (VRD) is the task of identifying the relationships
between objects in a scene. VRD models trained solely on relation detection
data struggle to generalize beyond the relations on which they are trained.
While prompt tuning has been used to adapt vision-language models (VLMs) for
VRD, it uses handcrafted prompts and struggles with novel or complex relations.
We argue that instruction tuning offers a more effective solution by
fine-tuning VLMs on diverse instructional data. We thus introduce ART, an
Adaptive Relation Tuning framework that adapts VLMs for VRD through instruction
tuning and strategic instance selection. By converting VRD datasets into an
instruction tuning format and employing an adaptive sampling algorithm, ART
directs the VLM to focus on informative relations while maintaining
generalizability. Specifically, we focus on the relation classification, where
subject-object boxes are given and the model predicts the predicate between
them. We tune on a held-in set and evaluate across multiple held-out datasets
of varying complexity. Our approach strongly improves over its baselines and
can infer unseen relation concepts, a capability absent in mainstream VRD
methods. We demonstrate ART's practical value by using the predicted relations
for segmenting complex scenes.

</details>


### [70] [3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection](https://arxiv.org/abs/2507.23567)
*Yung-Hsu Yang,Luigi Piccinelli,Mattia Segu,Siyuan Li,Rui Huang,Yuqian Fu,Marc Pollefeys,Hermann Blum,Zuria Bauer*

Main category: cs.CV

TL;DR: 该研究提出了3D-MOOD，一种用于开放集单目3D目标检测的端到端方法，通过联合2D-3D训练、几何先验和标准图像空间来提高性能和泛化能力，并在评估中创下新高。


<details>
  <summary>Details</summary>
Motivation: 真实世界的应用经常引入新的环境和新颖的对象类别，这对现有仅限于闭集设置（训练和测试集包含相同场景和/或对象类别）的单目3D目标检测方法提出了挑战。因此，本文致力于解决开放集设置下的单目3D目标检测问题。

Method: 提出了一种名为3D-MOOD的端到端三维单目开集目标检测器，它将开放集2D检测提升到3D空间，并设计了3D边界框头，实现了2D和3D任务的联合训练。通过将对象查询与几何先验相结合，克服了3D估计在不同场景下的泛化能力。此外，还设计了标准图像空间以实现更高效的跨数据集训练。

Result: 在闭集设置（Omni3D）和开集设置（Omni3D到Argoverse 2、ScanNet）上进行了评估，并取得了新的最先进成果。

Conclusion: 3D-MOOD在闭集和开集设置上均取得了新的最先进成果，并且在不同数据集的跨数据集训练中表现出更高的效率。

Abstract: Monocular 3D object detection is valuable for various applications such as
robotics and AR/VR. Existing methods are confined to closed-set settings, where
the training and testing sets consist of the same scenes and/or object
categories. However, real-world applications often introduce new environments
and novel object categories, posing a challenge to these methods. In this
paper, we address monocular 3D object detection in an open-set setting and
introduce the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD).
We propose to lift the open-set 2D detection into 3D space through our designed
3D bounding box head, enabling end-to-end joint training for both 2D and 3D
tasks to yield better overall performance. We condition the object queries with
geometry prior and overcome the generalization for 3D estimation across diverse
scenes. To further improve performance, we design the canonical image space for
more efficient cross-dataset training. We evaluate 3D-MOOD on both closed-set
settings (Omni3D) and open-set settings (Omni3D to Argoverse 2, ScanNet), and
achieve new state-of-the-art results. Code and models are available at
royyang0714.github.io/3D-MOOD.

</details>


### [71] [Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization](https://arxiv.org/abs/2507.23569)
*Maxime Pietrantoni,Gabriela Csurka,Torsten Sattler*

Main category: cs.CV

TL;DR: 本文提出了一种结合3D高斯泼溅和特征场的新型场景表示方法GSFFs，用于实现准确且注重隐私的视觉定位，并在多个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在利用3D高斯泼溅（3DGS）表示来实现准确且注重隐私的视觉定位。

Method: 本文提出了一种名为高斯泼溅特征场（GSFFs）的场景表示方法，该方法结合了显式的3D高斯泼溅（3DGS）几何模型和隐式的特征场。通过对比学习框架，将3D尺度感知特征场和2D特征编码器对齐到共同的嵌入空间。此外，还利用了3D结构感知的聚类过程来规范化表示学习，并将特征无缝转换为可用于隐私保护视觉定位的分割图。

Result: GSFFs在视觉定位任务中表现出了优越的性能，能够实现最先进的隐私和非隐私定位。

Conclusion: GSFFs在多个真实世界数据集上进行了评估，并在隐私和非隐私的视觉定位方面都取得了最先进的性能。

Abstract: Visual localization is the task of estimating a camera pose in a known
environment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based
representations for accurate and privacy-preserving visual localization. We
propose Gaussian Splatting Feature Fields (GSFFs), a scene representation for
visual localization that combines an explicit geometry model (3DGS) with an
implicit feature field. We leverage the dense geometric information and
differentiable rasterization algorithm from 3DGS to learn robust feature
representations grounded in 3D. In particular, we align a 3D scale-aware
feature field and a 2D feature encoder in a common embedding space through a
contrastive framework. Using a 3D structure-informed clustering procedure, we
further regularize the representation learning and seamlessly convert the
features to segmentations, which can be used for privacy-preserving visual
localization. Pose refinement, which involves aligning either feature maps or
segmentations from a query image with those rendered from the GSFFs scene
representation, is used to achieve localization. The resulting privacy- and
non-privacy-preserving localization pipelines, evaluated on multiple real-world
datasets, show state-of-the-art performances.

</details>


### [72] [Beyond Gloss: A Hand-Centric Framework for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.23575)
*Sobhan Asasi,Mohamed Ilyas Lakhal,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: BeyondGloss是一个创新的无词汇手语翻译框架，利用视频大语言模型和对比学习来提高手语翻译的准确性，并在基准测试中取得了领先成果。


<details>
  <summary>Details</summary>
Motivation: 手语翻译（SLT）任务面临着弥合视觉和语言信息之间的模态鸿沟的挑战，同时需要捕捉手部形状和运动的细微变化。现有VideoLLMs在详细建模长视频方面存在困难。

Method: 提出了一种新颖的无词汇手语翻译（SLT）框架BeyondGloss，利用视频大语言模型（VideoLLMs）的时空推理能力。通过生成精细、时间感知的ركات手部运动文本描述，并使用对比学习模块将其与视频特征对齐，以关注以手为中心的时间动态并更有效地区分手语。此外，还从HaMeR中提取了细粒度特征，并通过对比损失减少了预训练中的模态鸿沟。

Result: BeyondGloss在Phoenix14T和CSL-Daily基准上取得了最先进的性能。

Conclusion: BeyondGloss框架在Phoenix14T和CSL-Daily基准上实现了最先进的性能，证明了所提出框架的有效性。

Abstract: Sign Language Translation (SLT) is a challenging task that requires bridging
the modality gap between visual and linguistic information while capturing
subtle variations in hand shapes and movements. To address these challenges, we
introduce \textbf{BeyondGloss}, a novel gloss-free SLT framework that leverages
the spatio-temporal reasoning capabilities of Video Large Language Models
(VideoLLMs). Since existing VideoLLMs struggle to model long videos in detail,
we propose a novel approach to generate fine-grained, temporally-aware textual
descriptions of hand motion. A contrastive alignment module aligns these
descriptions with video features during pre-training, encouraging the model to
focus on hand-centric temporal dynamics and distinguish signs more effectively.
To further enrich hand-specific representations, we distill fine-grained
features from HaMeR. Additionally, we apply a contrastive loss between sign
video representations and target language embeddings to reduce the modality gap
in pre-training. \textbf{BeyondGloss} achieves state-of-the-art performance on
the Phoenix14T and CSL-Daily benchmarks, demonstrating the effectiveness of the
proposed framework. We will release the code upon acceptance of the paper.

</details>


### [73] [MamV2XCalib: V2X-based Target-less Infrastructure Camera Calibration with State Space Model](https://arxiv.org/abs/2507.23595)
*Yaoye Zhu,Zhe Wang,Yan Wang*

Main category: cs.CV

TL;DR: MamV2XCalib：一种利用车侧 LiDAR 辅助 V2X 场景下基础设施相机进行自动校准的方法，无需特定参考物或手动干预，并提出了无目标物校准技术，解决了 V2X 校准中的挑战，在真实数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着利用路侧摄像头辅助自动驾驶车辆感知的协同系统日益普及，大规模精确的基础设施相机校准已成为关键问题。传统手动校准方法耗时、劳动密集且可能需要封路。

Method: 提出了一种名为 MamV2XCalib 的 V2X-based 车辆协同感知基础设施相机校准方法，利用车侧 LiDAR 辅助校准。该方法引入了一种新的无目标物 LiDAR-相机校准技术，结合多尺度特征和 4D 关联卷来估计点云与图像间的关联。利用 Mamba 对时序信息进行建模，估计旋转角度，解决了因车侧数据缺陷（如遮挡）和视角差异大导致的 V2X 校准失败问题。

Result: 在 V2X-Seq 和 TUMTraf-V2X 真实世界数据集上进行了评估，证明了该 V2X-based 自动校准方法的有效性和鲁棒性。

Conclusion: MamV2XCalib 是一种基于 V2X 的自动基础设施相机校准方法，与以往仅针对单车设计的 LiDAR-相机校准方法相比，在 V2X 场景下具有更少参数、更优越且更稳定的校准性能。

Abstract: As cooperative systems that leverage roadside cameras to assist autonomous
vehicle perception become increasingly widespread, large-scale precise
calibration of infrastructure cameras has become a critical issue. Traditional
manual calibration methods are often time-consuming, labor-intensive, and may
require road closures. This paper proposes MamV2XCalib, the first V2X-based
infrastructure camera calibration method with the assistance of vehicle-side
LiDAR. MamV2XCalib only requires autonomous vehicles equipped with LiDAR to
drive near the cameras to be calibrated in the infrastructure, without the need
for specific reference objects or manual intervention. We also introduce a new
targetless LiDAR-camera calibration method, which combines multi-scale features
and a 4D correlation volume to estimate the correlation between vehicle-side
point clouds and roadside images. We model the temporal information and
estimate the rotation angles with Mamba, effectively addressing calibration
failures in V2X scenarios caused by defects in the vehicle-side data (such as
occlusions) and large differences in viewpoint. We evaluate MamV2XCalib on the
V2X-Seq and TUMTraf-V2X real-world datasets, demonstrating the effectiveness
and robustness of our V2X-based automatic calibration approach. Compared to
previous LiDAR-camera methods designed for calibration on one car, our approach
achieves better and more stable calibration performance in V2X scenarios with
fewer parameters. The code is available at
https://github.com/zhuyaoye/MamV2XCalib.

</details>


### [74] [MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction](https://arxiv.org/abs/2507.23597)
*Zijian Dong,Longteng Duan,Jie Song,Michael J. Black,Andreas Geiger*

Main category: cs.CV

TL;DR: MoGA是一种从单张图像重建3D高斯化身的新方法，通过结合生成模型和2D扩散模型，解决了现有方法的不足，提高了3D一致性和真实感，且效果优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在从单视角图像重建3D高斯化身时，由于2D扩散模型生成的视图稀疏且不一致，导致3D伪影和模糊外观的问题。

Method: MoGA提出了一种新颖的方法，利用生成模型和2D扩散模型，通过模型反演过程拟合高斯化身来重建单视角图像中的高保真3D高斯化身。该方法将生成模型作为先验，通过将其潜在空间映射到输入图像，并强制执行额外的3D外观和几何约束来确保3D一致性。

Result: MoGA在3D高斯化身重建方面取得了高保真度，优于现有技术，并能很好地泛化到真实世界场景，生成的结果具有可动画性。

Conclusion: MoGA方法在3D高保真高斯化身重建方面表现出色，优于现有技术，并能很好地推广到现实场景，同时生成的高斯化身也具有可动画性。

Abstract: We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian
avatars from a single-view image. The main challenge lies in inferring unseen
appearance and geometric details while ensuring 3D consistency and realism.
Most previous methods rely on 2D diffusion models to synthesize unseen views;
however, these generated views are sparse and inconsistent, resulting in
unrealistic 3D artifacts and blurred appearance. To address these limitations,
we leverage a generative avatar model, that can generate diverse 3D avatars by
sampling deformed Gaussians from a learned prior distribution. Due to the
limited amount of 3D training data such a 3D model alone cannot capture all
image details of unseen identities. Consequently, we integrate it as a prior,
ensuring 3D consistency by projecting input images into its latent space and
enforcing additional 3D appearance and geometric constraints. Our novel
approach formulates Gaussian avatar creation as a model inversion process by
fitting the generative avatar to synthetic views from 2D diffusion models. The
generative avatar provides a meaningful initialization for model fitting,
enforces 3D regularization, and helps in refining pose estimation. Experiments
show that our method surpasses state-of-the-art techniques and generalizes well
to real-world scenarios. Our Gaussian avatars are also inherently animatable

</details>


### [75] [DA-Occ: Efficient 3D Voxel Occupancy Prediction via Directional 2D for Geometric Structure Preservation](https://arxiv.org/abs/2507.23599)
*Yuchen Zhou,Yan Luo,Xiangang Wang,Xingjian Gu,Mingzhou Lu*

Main category: cs.CV

TL;DR: 提出了一种新的3D占用预测方法，通过2D切片和定向注意力来平衡准确性和速度，在自动驾驶领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有3D占用预测方法在准确性和实时处理速度之间难以平衡的问题。

Method: 提出了一种定向纯2D方法，通过切片3D体素特征来保留完整的垂直几何信息，以补偿BEV表示中高度线索的损失，并利用定向注意力机制提取不同方向的几何特征。

Result: 该方法有效地平衡了准确性和效率，在Occ3D-nuScenes数据集上达到了39.3%的mIoU和27.7 FPS的推理速度，并在边缘设备模拟中实现了14.8 FPS的推理速度。

Conclusion: 该方法在Occ3D-nuScenes数据集上实现了39.3%的mIoU和27.7 FPS的推理速度，有效地平衡了准确性和效率。在边缘设备模拟中，推理速度达到14.8 FPS，证明了该方法在资源受限环境的实时部署能力。

Abstract: Efficient and high-accuracy 3D occupancy prediction is crucial for ensuring
the performance of autonomous driving (AD) systems. However, many current
methods focus on high accuracy at the expense of real-time processing needs. To
address this challenge of balancing accuracy and inference speed, we propose a
directional pure 2D approach. Our method involves slicing 3D voxel features to
preserve complete vertical geometric information. This strategy compensates for
the loss of height cues in Bird's-Eye View (BEV) representations, thereby
maintaining the integrity of the 3D geometric structure. By employing a
directional attention mechanism, we efficiently extract geometric features from
different orientations, striking a balance between accuracy and computational
efficiency. Experimental results highlight the significant advantages of our
approach for autonomous driving. On the Occ3D-nuScenes, the proposed method
achieves an mIoU of 39.3% and an inference speed of 27.7 FPS, effectively
balancing accuracy and efficiency. In simulations on edge devices, the
inference speed reaches 14.8 FPS, further demonstrating the method's
applicability for real-time deployment in resource-constrained environments.

</details>


### [76] [Mamba-based Efficient Spatio-Frequency Motion Perception for Video Camouflaged Object Detection](https://arxiv.org/abs/2507.23601)
*Xin Li,Keren Fu,Qijun Zhao*

Main category: cs.CV

TL;DR: Vcamba enhances video camouflaged object detection by integrating spatial and frequency features using the Mamba model, outperforming existing methods with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing video camouflaged object detection (VCOD) methods rely on spatial appearance features, which have limited discriminability due to high foreground-background similarity. Recent studies show frequency features can enhance representation and perceive motion through dynamic variations. The Mamba model enables efficient perception of motion cues in frame sequences due to its linear-time long-sequence modeling capability. This paper is motivated by integrating frequency features and Mamba for efficient and accurate VCOD.

Method: Vcamba, a novel visual camouflage Mamba based on spatio-frequency motion perception, integrates frequency and spatial features. It utilizes a receptive field visual state space (RFVSS) module for multi-scale spatial features, an adaptive frequency component enhancement (AFE) module with a frequency-domain sequential scanning strategy for frequency learning, space-based long-range motion perception (SLMP) and frequency-based long-range motion perception (FLMP) modules for spatio-temporal and frequency-temporal sequences, and a space and frequency motion fusion module (SFMF) for unified motion representation.

Result: Experimental results show that Vcamba outperforms state-of-the-art methods across 6 evaluation metrics on 2 datasets with lower computation cost.

Conclusion: Vcamba outperforms state-of-the-art methods across 6 evaluation metrics on 2 datasets with lower computation cost, confirming the superiority of Vcamba.

Abstract: Existing video camouflaged object detection (VCOD) methods primarily rely on
spatial appearance features to perceive motion cues for breaking camouflage.
However, the high similarity between foreground and background in VCOD results
in limited discriminability of spatial appearance features (e.g., color and
texture), restricting detection accuracy and completeness. Recent studies
demonstrate that frequency features can not only enhance feature representation
to compensate for appearance limitations but also perceive motion through
dynamic variations in frequency energy. Furthermore, the emerging state space
model called Mamba, enables efficient perception of motion cues in frame
sequences due to its linear-time long-sequence modeling capability. Motivated
by this, we propose a novel visual camouflage Mamba (Vcamba) based on
spatio-frequency motion perception that integrates frequency and spatial
features for efficient and accurate VCOD. Specifically, we propose a receptive
field visual state space (RFVSS) module to extract multi-scale spatial features
after sequence modeling. For frequency learning, we introduce an adaptive
frequency component enhancement (AFE) module with a novel frequency-domain
sequential scanning strategy to maintain semantic consistency. Then we propose
a space-based long-range motion perception (SLMP) module and a frequency-based
long-range motion perception (FLMP) module to model spatio-temporal and
frequency-temporal sequences in spatial and frequency phase domains. Finally,
the space and frequency motion fusion module (SFMF) integrates dual-domain
features for unified motion representation. Experimental results show that our
Vcamba outperforms state-of-the-art methods across 6 evaluation metrics on 2
datasets with lower computation cost, confirming the superiority of Vcamba. Our
code is available at: https://github.com/BoydeLi/Vcamba.

</details>


### [77] [Medical Image De-Identification Benchmark Challenge](https://arxiv.org/abs/2507.23608)
*Linmin Pei,Granger Sutton,Michael Rutherford,Ulrike Wagner,Tracy Nolan,Kirk Smith,Phillip Farmer,Peter Gu,Ambar Rana,Kailing Chen,Thomas Ferleman,Brian Park,Ye Wu,Jordan Kojouharov,Gargi Singh,Jon Lemon,Tyler Willis,Milos Vukadinovic,Grant Duffy,Bryan He,David Ouyang,Marco Pereanez,Daniel Samber,Derek A. Smith,Christopher Cannistraci,Zahi Fayad,David S. Mendelson,Michele Bufano,Elmar Kotter,Hamideh Haghiri,Rajesh Baidya,Stefan Dvoretskii,Klaus H. Maier-Hein,Marco Nolden,Christopher Ablett,Silvia Siggillino,Sandeep Kaushik,Hongzhu Jiang,Sihan Xie,Zhiyu Wan,Alex Michie,Simon J Doran,Angeline Aurelia Waly,Felix A. Nathaniel Liang,Humam Arshad Mustagfirin,Michelle Grace Felicia,Kuo Po Chih,Rahul Krish,Ghulam Rasool,Nidhal Bouaynaya,Nikolas Koutsoubis,Kyle Naddeo,Kartik Pandit,Tony O'Sullivan,Raj Krish,Qinyan Pan,Scott Gustafson,Benjamin Kopchick,Laura Opsahl-Ong,Andrea Olvera-Morales,Jonathan Pinney,Kathryn Johnson,Theresa Do,Juergen Klenk,Maria Diaz,Arti Singh,Rong Chai,David A. Clunie,Fred Prior,Keyvan Farahani*

Main category: cs.CV

TL;DR: MIDI-B 挑战赛是一个关于医疗影像去标识化的比赛，旨在评估各种去标识化工具的性能。比赛使用了包含合成个人信息的影像，并根据去标识化的准确性对参赛团队进行了评分。结果显示，先进的去标识化技术在保护隐私和保留关键元数据方面取得了很高的成功率。


<details>
  <summary>Details</summary>
Motivation: 为了在共享医疗影像（尤其是在公共存储库中）时遵守患者隐私法，对受保护健康信息 (PHI) 和个人身份信息 (PII) 进行去标识化 (deID) 是基本要求。同时，为了支持影像人工智能 (AI) 的下游开发，保留非 PHI 元数据也很重要。MIDI-B 的目标是为 DICOM 影像去标识化工具提供一个标准化的基准测试平台。

Method: MIDI-B 挑战赛包含三个阶段：训练、验证和测试。比赛使用了包含合成 PHI/PII 的大规模、多样化、多中心、多模态的真实去标识化放射影像。评分标准是正确操作数占总需求操作数的百分比。

Result: 共有 10 个团队成功完成了比赛的测试阶段。得分范围从 97.91% 到 99.93%。参赛者使用了各种开源和专有工具，并结合了定制配置、大型语言模型和光学字符识别 (OCR)。

Conclusion: MIDI-B 挑战赛成功地为 DICOM 影像去标识化工具提供了一个标准化的基准测试平台，该平台符合 HIPAA 安全港法规、DICOM 属性保密配置文件和 TCIA 定义的研究关键元数据保存最佳实践。比赛结果表明，基于规则的方法在影像去标识化方面非常有效，得分范围为 97.91% 到 99.93%。

Abstract: The de-identification (deID) of protected health information (PHI) and
personally identifiable information (PII) is a fundamental requirement for
sharing medical images, particularly through public repositories, to ensure
compliance with patient privacy laws. In addition, preservation of non-PHI
metadata to inform and enable downstream development of imaging artificial
intelligence (AI) is an important consideration in biomedical research. The
goal of MIDI-B was to provide a standardized platform for benchmarking of DICOM
image deID tools based on a set of rules conformant to the HIPAA Safe Harbor
regulation, the DICOM Attribute Confidentiality Profiles, and best practices in
preservation of research-critical metadata, as defined by The Cancer Imaging
Archive (TCIA). The challenge employed a large, diverse, multi-center, and
multi-modality set of real de-identified radiology images with synthetic
PHI/PII inserted.
  The MIDI-B Challenge consisted of three phases: training, validation, and
test. Eighty individuals registered for the challenge. In the training phase,
we encouraged participants to tune their algorithms using their in-house or
public data. The validation and test phases utilized the DICOM images
containing synthetic identifiers (of 216 and 322 subjects, respectively). Ten
teams successfully completed the test phase of the challenge. To measure
success of a rule-based approach to image deID, scores were computed as the
percentage of correct actions from the total number of required actions. The
scores ranged from 97.91% to 99.93%. Participants employed a variety of
open-source and proprietary tools with customized configurations, large
language models, and optical character recognition (OCR). In this paper we
provide a comprehensive report on the MIDI-B Challenge's design,
implementation, results, and lessons learned.

</details>


### [78] [DivControl: Knowledge Diversion for Controllable Image Generation](https://arxiv.org/abs/2507.23620)
*Yucheng Xie,Fu Feng,Ruixiao Shi,Jing Wang,Yong Rui,Xin Geng*

Main category: cs.CV

TL;DR: DivControl is a new framework for controllable image generation that uses a novel factorization and knowledge diversion technique to achieve better generalization and adaptation to new conditions with significantly less training cost.


<details>
  <summary>Details</summary>
Motivation: Existing methods for controllable image generation using diffusion models either train separate models for each condition or use unified architectures with entangled representations, leading to poor generalization and high adaptation costs for novel conditions.

Method: DivControl factorizes ControlNet via SVD into singular vectors, disentangling them into condition-agnostic learngenes and condition-specific tailors through knowledge diversion during multi-condition training. Knowledge diversion is implemented via a dynamic gate that performs soft routing over tailors based on condition instruction semantics. A representation alignment loss is introduced to align condition embeddings with early diffusion features.

Result: DivControl achieves state-of-the-art controllability with 36.4x less training cost, improves average performance on basic conditions, and shows strong zero-shot and few-shot performance on unseen conditions, demonstrating superior scalability, modularity, and transferability.

Conclusion: DivControl, a decomposable pretraining framework, enables unified controllable generation and efficient adaptation by factorizing ControlNet via SVD into singular vectors, disentangling them into condition-agnostic learngenes and condition-specific tailors through knowledge diversion. This framework achieves state-of-the-art controllability with significantly reduced training cost, improved performance on basic conditions, and strong zero-shot/few-shot generalization to unseen conditions, demonstrating superior scalability, modularity, and transferability.

Abstract: Diffusion models have advanced from text-to-image (T2I) to image-to-image
(I2I) generation by incorporating structured inputs such as depth maps,
enabling fine-grained spatial control. However, existing methods either train
separate models for each condition or rely on unified architectures with
entangled representations, resulting in poor generalization and high adaptation
costs for novel conditions. To this end, we propose DivControl, a decomposable
pretraining framework for unified controllable generation and efficient
adaptation. DivControl factorizes ControlNet via SVD into basic
components-pairs of singular vectors-which are disentangled into
condition-agnostic learngenes and condition-specific tailors through knowledge
diversion during multi-condition training. Knowledge diversion is implemented
via a dynamic gate that performs soft routing over tailors based on the
semantics of condition instructions, enabling zero-shot generalization and
parameter-efficient adaptation to novel conditions. To further improve
condition fidelity and training efficiency, we introduce a representation
alignment loss that aligns condition embeddings with early diffusion features.
Extensive experiments demonstrate that DivControl achieves state-of-the-art
controllability with 36.4$\times$ less training cost, while simultaneously
improving average performance on basic conditions. It also delivers strong
zero-shot and few-shot performance on unseen conditions, demonstrating superior
scalability, modularity, and transferability.

</details>


### [79] [Efficient Masked Attention Transformer for Few-Shot Classification and Segmentation](https://arxiv.org/abs/2507.23642)
*Dustin Carrión-Ojeda,Stefan Roth,Simone Schaub-Meyer*

Main category: cs.CV

TL;DR: EMAT是一种用于少样本分类和分割（FS-CS）的高效掩码注意力Transformer，可提高小目标精度，并减少参数数量。该研究还提出了新的评估方法以更好地反映实际应用。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有方法在小目标上的不足，提出EMAT。

Method: EMAT引入了三种改进：一种新颖的内存高效掩码注意机制、一种可学习的下采样策略和参数效率的增强。

Result: EMAT提高了分类和分割的准确性，尤其是在小目标方面。

Conclusion: EMAT在PASCAL-5$^i$和COCO-20$^i$数据集上超越了所有FS-CS方法，同时使用的可训练参数少于四倍。此外，为了更好地反映实际情况，研究引入了两个新的评估设置，考虑了在当前FS-CS评估设置中被忽略的可用注释。

Abstract: Few-shot classification and segmentation (FS-CS) focuses on jointly
performing multi-label classification and multi-class segmentation using few
annotated examples. Although the current state of the art (SOTA) achieves high
accuracy in both tasks, it struggles with small objects. To overcome this, we
propose the Efficient Masked Attention Transformer (EMAT), which improves
classification and segmentation accuracy, especially for small objects. EMAT
introduces three modifications: a novel memory-efficient masked attention
mechanism, a learnable downscaling strategy, and parameter-efficiency
enhancements. EMAT outperforms all FS-CS methods on the PASCAL-5$^i$ and
COCO-20$^i$ datasets, using at least four times fewer trainable parameters.
Moreover, as the current FS-CS evaluation setting discards available
annotations, despite their costly collection, we introduce two novel evaluation
settings that consider these annotations to better reflect practical scenarios.

</details>


### [80] [FFGAF-SNN: The Forward-Forward Based Gradient Approximation Free Training Framework for Spiking Neural Networks](https://arxiv.org/abs/2507.23643)
*Changqing Xu,Ziqiang Yang,Yi Liu,Xinfang Liao,Guiqi Mo,Hao Zeng,Yintang Yang*

Main category: cs.CV

TL;DR: 本研究提出了一种新颖的、无需梯度近似的、基于前向前向（FF）的脉冲神经网络（SNN）训练框架，该框架通过将脉冲激活视为黑盒模块来解决SNN训练中的梯度问题，并引入了类别感知复杂性适应机制以优化资源分配。该方法在多个数据集上取得了优于现有方法的准确率，并显著降低了计算复杂性和功耗。


<details>
  <summary>Details</summary>
Motivation: 为了解决脉冲神经网络（SNN）由于其不可微性而难以有效训练的挑战，以及现有梯度近似方法在准确性和计算资源方面存在的不足，本研究提出了一种新的训练框架。

Method: 提出了一种基于前向前向（FF）的前向前向（FF）的无梯度近似训练框架，用于脉冲神经网络（SNN），将脉冲激活视为黑盒模块，从而无需梯度近似，并显著降低了计算复杂性。此外，引入了一个类别感知复杂性适应机制，根据类别间的难度指标动态优化损失函数，以实现网络资源在不同类别之间的有效分配。

Result: 实验结果表明，所提出的训练框架在MNIST、Fashion-MNIST和CIFAR-10数据集上分别达到了99.58%、92.13%和75.64%的测试准确率，超过了所有现有的基于FF的SNN方法。此外，该方法在内存访问和计算功耗方面也表现出显著优势。

Conclusion: 该研究提出的前向前向（FF）训练框架通过将脉冲激活视为黑盒模块，消除了对梯度近似的需求，并显著降低了计算复杂性。此外，研究中引入的类别感知复杂性适应机制能够根据类别间的难度指标动态优化损失函数，从而实现网络资源在不同类别间的有效分配。实验结果表明，该框架在MNIST、Fashion-MNIST和CIFAR-10数据集上分别达到了99.58%、92.13%和75.64%的测试准确率，超过了所有现有的基于FF的SNN方法，并在内存访问和计算功耗方面表现出显著优势。

Abstract: Spiking Neural Networks (SNNs) offer a biologically plausible framework for
energy-efficient neuromorphic computing. However, it is a challenge to train
SNNs due to their non-differentiability, efficiently. Existing gradient
approximation approaches frequently sacrifice accuracy and face deployment
limitations on edge devices due to the substantial computational requirements
of backpropagation. To address these challenges, we propose a Forward-Forward
(FF) based gradient approximation-free training framework for Spiking Neural
Networks, which treats spiking activations as black-box modules, thereby
eliminating the need for gradient approximation while significantly reducing
computational complexity. Furthermore, we introduce a class-aware complexity
adaptation mechanism that dynamically optimizes the loss function based on
inter-class difficulty metrics, enabling efficient allocation of network
resources across different categories. Experimental results demonstrate that
our proposed training framework achieves test accuracies of 99.58%, 92.13%, and
75.64% on the MNIST, Fashion-MNIST, and CIFAR-10 datasets, respectively,
surpassing all existing FF-based SNN approaches. Additionally, our proposed
method exhibits significant advantages in terms of memory access and
computational power consumption.

</details>


### [81] [Adaptively Distilled ControlNet: Accelerated Training and Superior Sampling for Medical Image Synthesis](https://arxiv.org/abs/2507.23652)
*Kunpeng Qiu,Zhiying Zhou,Yongxin Guo*

Main category: cs.CV

TL;DR: 提出Adaptively Distilled ControlNet，一种无需任务训练的框架，通过双模型蒸馏加速训练和优化，解决了医学图像分割中的隐私和标注问题，并在两个数据集上取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像标注中的隐私和劳动密集性问题，提高分割模型的性能和泛化能力，同时解决现有掩码可控扩散模型在精确病变掩码对齐方面的挑战。

Method: 通过双模型蒸馏，利用教师模型（条件为掩码-图像对）在参数空间中通过预测噪声对掩码学生模型进行正则化，并基于病变-背景比进行自适应正则化。在采样时仅使用学生模型。

Result: 在KiTS19数据集上，TransUNet的mDice/mIoU提升了2.4%/4.2%；在Polyps数据集上，SANet的mDice/mIoU提升了2.6%/3.5%。

Conclusion: 所提出的Adaptively Distilled ControlNet框架在KiTS19和Polyps数据集上均取得了最先进的性能，分别在TransUNet和SANet上实现了mDice/mIoU的显著提升。

Abstract: Medical image annotation is constrained by privacy concerns and
labor-intensive labeling, significantly limiting the performance and
generalization of segmentation models. While mask-controllable diffusion models
excel in synthesis, they struggle with precise lesion-mask alignment. We
propose \textbf{Adaptively Distilled ControlNet}, a task-agnostic framework
that accelerates training and optimization through dual-model distillation.
Specifically, during training, a teacher model, conditioned on mask-image
pairs, regularizes a mask-only student model via predicted noise alignment in
parameter space, further enhanced by adaptive regularization based on
lesion-background ratios. During sampling, only the student model is used,
enabling privacy-preserving medical image generation. Comprehensive evaluations
on two distinct medical datasets demonstrate state-of-the-art performance:
TransUNet improves mDice/mIoU by 2.4%/4.2% on KiTS19, while SANet achieves
2.6%/3.5% gains on Polyps, highlighting its effectiveness and superiority. Code
is available at GitHub.

</details>


### [82] [OmniTraj: Pre-Training on Heterogeneous Data for Adaptive and Zero-Shot Human Trajectory Prediction](https://arxiv.org/abs/2507.23657)
*Yang Gao,Po-Chien Luan,Kaouther Messaoud,Lan Feng,Alexandre Alahi*

Main category: cs.CV

TL;DR: 本文提出OmniTraj模型，通过显式条件化帧率，实现了在无人力干预下，跨不同时间动态数据集的人类轨迹预测的零样本迁移，并将预测误差降低了70%以上，同时在多个数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模预训练模型在迁移到具有不同时间动态（如帧率或观测范围）的未见数据集时，通常需要进行微调，这限制了其可扩展性和实用性。作者旨在系统地研究并解决这一零样本迁移的挑战。

Method: OmniTraj是一个基于Transformer的模型，在异构的大规模数据集上进行了预训练。其核心在于引入了一个显式的时序元数据条件化机制，特别是对帧率进行条件化，以解决在不同时间动态的未见数据集上的零样本迁移问题。

Result: OmniTraj在零样本迁移场景下，通过对帧率进行条件化，预测误差降低了70%以上。在微调后，OmniTraj在NBA、JTA、WorldPose和ETH-UCY四个数据集上均取得了最先进的性能。

Conclusion: 本文提出的OmniTraj模型通过显式地对帧率进行条件化，在零样本迁移方面取得了最先进的性能，在具有挑战性的跨设置场景中将预测误差降低了70%以上。此外，OmniTraj在四个数据集（NBA、JTA、WorldPose和ETH-UCY）上进行了微调，也达到了最先进的结果。

Abstract: While large-scale pre-training has advanced human trajectory prediction, a
critical challenge remains: zero-shot transfer to unseen dataset with varying
temporal dynamics. State-of-the-art pre-trained models often require
fine-tuning to adapt to new datasets with different frame rates or observation
horizons, limiting their scalability and practical utility. In this work, we
systematically investigate this limitation and propose a robust solution. We
first demonstrate that existing data-aware discrete models struggle when
transferred to new scenarios with shifted temporal setups. We then isolate the
temporal generalization from dataset shift, revealing that a simple, explicit
conditioning mechanism for temporal metadata is a highly effective solution.
Based on this insight, we present OmniTraj, a Transformer-based model
pre-trained on a large-scale, heterogeneous dataset. Our experiments show that
explicitly conditioning on the frame rate enables OmniTraj to achieve
state-of-the-art zero-shot transfer performance, reducing prediction error by
over 70\% in challenging cross-setup scenarios. After fine-tuning, OmniTraj
achieves state-of-the-art results on four datasets, including NBA, JTA,
WorldPose, and ETH-UCY. The code is publicly available:
https://github.com/vita-epfl/omnitraj

</details>


### [83] [SAMSA: Segment Anything Model Enhanced with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation](https://arxiv.org/abs/2507.23673)
*Alfie Roddan,Tobias Czempiel,Chi Xu,Daniel S. Elson,Stamatia Giannarou*

Main category: cs.CV

TL;DR: SAMSA 是一种新颖的交互式高光谱医学图像分割框架，它结合了 RGB 基础模型和光谱分析，利用用户点击进行高效分割，并在不同数据集上展现了优异的性能，尤其是在少样本和零样本学习场景下。


<details>
  <summary>Details</summary>
Motivation: 为了解决高光谱成像（HSI）在医学成像中虽然提供丰富的光谱信息，但面临数据限制和硬件差异的挑战。

Method: SAMSA 框架结合了 RGB 基础模型和光谱分析。它利用用户点击来指导 RGB 分割和光谱相似性计算，并通过一种独立于光谱带数量和分辨率的光谱特征融合策略来解决高光谱图像分割的关键限制。

Result: 在神经外科和猪的术中高光谱数据集上，SAMSA 分别实现了 81.0% 和 81.1% 的单次点击 DICE 分数，以及 93.4% 和 89.2% 的五次点击 DICE 分数。实验结果证明了 SAMSA 在少样本和零样本学习场景下的有效性，以及在仅使用少量训练样本时的能力。

Conclusion: SAMSA 框架能够有效地融合 RGB 和光谱信息，为高光谱医学图像分割提供了一种新颖的交互式解决方案。该方法在少量或零样本学习场景下表现出色，并能适应不同光谱特征的数据集，显示了其在多变的高光谱医学成像领域应用的潜力。

Abstract: Hyperspectral imaging (HSI) provides rich spectral information for medical
imaging, yet encounters significant challenges due to data limitations and
hardware variations. We introduce SAMSA, a novel interactive segmentation
framework that combines an RGB foundation model with spectral analysis. SAMSA
efficiently utilizes user clicks to guide both RGB segmentation and spectral
similarity computations. The method addresses key limitations in HSI
segmentation through a unique spectral feature fusion strategy that operates
independently of spectral band count and resolution. Performance evaluation on
publicly available datasets has shown 81.0% 1-click and 93.4% 5-click DICE on a
neurosurgical and 81.1% 1-click and 89.2% 5-click DICE on an intraoperative
porcine hyperspectral dataset. Experimental results demonstrate SAMSA's
effectiveness in few-shot and zero-shot learning scenarios and using minimal
training examples. Our approach enables seamless integration of datasets with
different spectral characteristics, providing a flexible framework for
hyperspectral medical image analysis.

</details>


### [84] [I2V-GS: Infrastructure-to-Vehicle View Transformation with Gaussian Splatting for Autonomous Driving Data Generation](https://arxiv.org/abs/2507.23683)
*Jialei Chen,Wuhao Xu,Sipeng He,Baoru Huang,Dongchun Ren*

Main category: cs.CV

TL;DR: I2V-GS 是一个创新的框架，利用高斯泼溅技术和新颖的视图合成策略，实现了从基础设施视图到车辆视图的转换，并生成了高质量的自动驾驶数据集 RoadSight，在各项指标上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统依赖昂贵且低效的车辆收集数据，而从真实图像合成数据是一种有前景的解决方案。最近的 3D 重建技术在照片级真实感新视图合成方面取得了进展，这表明了从路面图像生成驾驶数据的潜力。

Method: 提出了一种名为 I2V-GS 的新方法，利用高斯泼溅技术实现从基础设施视图到车辆视图的转换。该方法通过自适应深度扭曲生成密集的训练视图，并采用级联策略修复扭曲图像以扩大视图范围并确保跨视图内容的一致性。此外，利用跨视图信息进行置信度引导优化，以提高扩散模型的可靠性。同时，引入了多模态、多视图的真实场景数据集 RoadSight。

Result: I2V-GS 在车辆视图下的合成质量得到显著提升，在NTA-Iou、NTL-Iou和FID指标上分别比StreetGaussian提高了45.7%、34.2%和14.9%。

Conclusion: I2V-GS 是首个实现基础设施视图到车辆视图转换的自动驾驶数据集生成框架，显著提高了车辆视图下的合成质量，在NTA-Iou、NTL-Iou和FID方面分别超越StreetGaussian 45.7%、34.2%和14.9%。

Abstract: Vast and high-quality data are essential for end-to-end autonomous driving
systems. However, current driving data is mainly collected by vehicles, which
is expensive and inefficient. A potential solution lies in synthesizing data
from real-world images. Recent advancements in 3D reconstruction demonstrate
photorealistic novel view synthesis, highlighting the potential of generating
driving data from images captured on the road. This paper introduces a novel
method, I2V-GS, to transfer the Infrastructure view To the Vehicle view with
Gaussian Splatting. Reconstruction from sparse infrastructure viewpoints and
rendering under large view transformations is a challenging problem. We adopt
the adaptive depth warp to generate dense training views. To further expand the
range of views, we employ a cascade strategy to inpaint warped images, which
also ensures inpainting content is consistent across views. To further ensure
the reliability of the diffusion model, we utilize the cross-view information
to perform a confidenceguided optimization. Moreover, we introduce RoadSight, a
multi-modality, multi-view dataset from real scenarios in infrastructure views.
To our knowledge, I2V-GS is the first framework to generate autonomous driving
datasets with infrastructure-vehicle view transformation. Experimental results
demonstrate that I2V-GS significantly improves synthesis quality under vehicle
view, outperforming StreetGaussian in NTA-Iou, NTL-Iou, and FID by 45.7%,
34.2%, and 14.9%, respectively.

</details>


### [85] [UniLDiff: Unlocking the Power of Diffusion Priors for All-in-One Image Restoration](https://arxiv.org/abs/2507.23685)
*Zihan Cheng,Liangtai Zhou,Dian Chen,Ni Tang,Xiaotong Luo,Yanyun Qu*

Main category: cs.CV

TL;DR: 本文提出了一种基于LDM的统一图像修复框架，通过DAFF和DAEM模块有效处理多种图像退化问题，并取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决全合一图像修复 (AiOIR) 面临的挑战，本文旨在利用潜在扩散模型 (LDM) 的强大生成能力来处理多种退化问题。

Method: 本文提出了一种新颖的统一图像修复框架，该框架基于潜在扩散模型 (LDM)。该方法将低质量视觉先验结构化地集成到扩散过程中，并设计了降级感知特征融合 (DAFF) 模块以自适应处理各种降级类型，以及细节感知专家模块 (DAEM) 以增强纹理和精细结构恢复。

Result: 实验结果表明，该方法在多任务和混合退化设置下均能持续获得最先进的性能，有效解决了细节丢失问题，并能增强纹理和精细结构恢复。

Conclusion: 本文提出的基于潜在扩散模型 (LDM) 的统一图像修复框架在处理多种退化问题上取得了最先进的性能，展示了扩散模型在统一图像修复领域的应用潜力。

Abstract: All-in-One Image Restoration (AiOIR) has emerged as a promising yet
challenging research direction. To address its core challenges, we propose a
novel unified image restoration framework based on latent diffusion models
(LDMs). Our approach structurally integrates low-quality visual priors into the
diffusion process, unlocking the powerful generative capacity of diffusion
models for diverse degradations. Specifically, we design a Degradation-Aware
Feature Fusion (DAFF) module to enable adaptive handling of diverse degradation
types. Furthermore, to mitigate detail loss caused by the high compression and
iterative sampling of LDMs, we design a Detail-Aware Expert Module (DAEM) in
the decoder to enhance texture and fine-structure recovery. Extensive
experiments across multi-task and mixed degradation settings demonstrate that
our method consistently achieves state-of-the-art performance, highlighting the
practical potential of diffusion priors for unified image restoration. Our code
will be released.

</details>


### [86] [Enhanced Velocity Field Modeling for Gaussian Video Reconstruction](https://arxiv.org/abs/2507.23704)
*Zhenyang Li,Xiaoyang Bai,Tongchen Zhang,Pengfei Shen,Weiwei Xu,Yifan Peng*

Main category: cs.CV

TL;DR: FlowGaussian-VR通过引入光流优化和自适应稠密化，解决了现有3D高斯视频重建中复杂运动和尺度变化导致的问题，实现了更优的视觉效果和轨迹鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对现有变形场范式在处理复杂运动和尺度变化时，变形网络易过拟合于不规则高斯轨迹，以及用于静态场景的梯度下降稠密化策略不适用于动态内容的问题，提出新的解决方案。

Method: 提出了一种名为FlowGaussian-VR的流场赋能的速度场建模方案，包含两个核心组件：1. 速度场渲染（VFR）管线，用于基于光流进行优化；2. 流辅助自适应稠密化（FAD）策略，用于动态区域的 and 高斯数量和大小的调整。

Result: 在包含挑战性运动场景的多视角动态重建和新视角合成任务中，FlowGaussian-VR相比现有方法在PSNR方面有超过2.5 dB的提升，减少了动态纹理的模糊伪影，并实现了更正则化、可追踪的高斯轨迹。

Conclusion: FlowGaussian-VR通过流场赋能的速度场建模方案，在处理复杂运动和尺度变化方面表现出色，显著提升了3D视频重建的视觉质量和鲁棒性。

Abstract: High-fidelity 3D video reconstruction is essential for enabling real-time
rendering of dynamic scenes with realistic motion in virtual and augmented
reality (VR/AR). The deformation field paradigm of 3D Gaussian splatting has
achieved near-photorealistic results in video reconstruction due to the great
representation capability of deep deformation networks. However, in videos with
complex motion and significant scale variations, deformation networks often
overfit to irregular Gaussian trajectories, leading to suboptimal visual
quality. Moreover, the gradient-based densification strategy designed for
static scene reconstruction proves inadequate to address the absence of dynamic
content. In light of these challenges, we propose a flow-empowered velocity
field modeling scheme tailored for Gaussian video reconstruction, dubbed
FlowGaussian-VR. It consists of two core components: a velocity field rendering
(VFR) pipeline which enables optical flow-based optimization, and a
flow-assisted adaptive densification (FAD) strategy that adjusts the number and
size of Gaussians in dynamic regions. We validate our model's effectiveness on
multi-view dynamic reconstruction and novel view synthesis with multiple
real-world datasets containing challenging motion scenarios, demonstrating not
only notable visual improvements (over 2.5 dB gain in PSNR) and less blurry
artifacts in dynamic textures, but also regularized and trackable per-Gaussian
trajectories.

</details>


### [87] [Explainable Image Classification with Reduced Overconfidence for Tissue Characterisation](https://arxiv.org/abs/2507.23709)
*Alfie Roddan,Chi Xu,Serine Ajlouni,Irini Kakaletri,Patra Charalampaki,Stamatia Giannarou*

Main category: cs.CV

TL;DR: 提出了一种结合风险估计的像素归因方法，用于增强机器学习模型在术中的可解释性，并成功应用于组织特征分析，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在术中组织特征分析中的应用可以辅助决策并指导肿瘤切除，但现有基于像素归因的可解释性方法在面对深度学习模型过度自信时存在不足，需要改进。

Method: 提出了一种将风险估计纳入像素归因方法的技术，通过迭代应用分类模型和像素归因方法生成像素归因图谱体，并生成像素级归因值分布，进而生成增强的像素归因图谱，并使用变异系数（CV）估计像素级风险。

Result: 所提出的方法不仅提供了一个改进的像素归因图谱，还对输出的像素归因值产生了风险估计，并在pCLE数据和ImageNet的性能评估中验证了其优于现有技术。

Conclusion: 本文提出的方法通过引入风险估计来改进像素归因方法，从而增强了图像分类的可解释性，并在pCLE数据和ImageNet的性能评估中优于现有技术。

Abstract: The deployment of Machine Learning models intraoperatively for tissue
characterisation can assist decision making and guide safe tumour resections.
For image classification models, pixel attribution methods are popular to infer
explainability. However, overconfidence in deep learning model's predictions
translates to overconfidence in pixel attribution. In this paper, we propose
the first approach which incorporates risk estimation into a pixel attribution
method for improved image classification explainability. The proposed method
iteratively applies a classification model with a pixel attribution method to
create a volume of PA maps. This volume is used for the first time, to generate
a pixel-wise distribution of PA values. We introduce a method to generate an
enhanced PA map by estimating the expectation values of the pixel-wise
distributions. In addition, the coefficient of variation (CV) is used to
estimate pixel-wise risk of this enhanced PA map. Hence, the proposed method
not only provides an improved PA map but also produces an estimation of risk on
the output PA values. Performance evaluation on probe-based Confocal Laser
Endomicroscopy (pCLE) data and ImageNet verifies that our improved
explainability method outperforms the state-of-the-art.

</details>


### [88] [DiffuMatch: Category-Agnostic Spectral Diffusion Priors for Robust Non-rigid Shape Matching](https://arxiv.org/abs/2507.23715)
*Emery Pierson,Lei Li,Angela Dai,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: 通过基于分数的生成模型和扩散模型蒸馏，实现了数据驱动的函数图学习，在非刚性形状匹配任务上超越了传统公理化方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法将学习限制在特征函数上，并依赖公理化模型进行训练损失制定和函数图正则化，限制了准确性和适用性。

Method: 提出了一种在频谱域使用基于分数的生成模型来训练函数图生成模型，并利用该模型来促进新形状集合上真实函数图的结构属性。关键技术贡献是提出了在频谱域蒸馏扩散模型的新策略。

Result: 实验证明，所提出的学习方法在零样本非刚性形状匹配方面优于公理化方法，并且学习到的模型具有类别不可知性。

Conclusion: 该研究首次展示了可以通过数据驱动的方法替代函数图的正则化和训练，实现了比公理化方法更好的零样本非刚性形状匹配结果，并且学习到的模型具有跨类别通用性。

Abstract: Deep functional maps have recently emerged as a powerful tool for solving
non-rigid shape correspondence tasks. Methods that use this approach combine
the power and flexibility of the functional map framework, with data-driven
learning for improved accuracy and generality. However, most existing methods
in this area restrict the learning aspect only to the feature functions and
still rely on axiomatic modeling for formulating the training loss or for
functional map regularization inside the networks. This limits both the
accuracy and the applicability of the resulting approaches only to scenarios
where assumptions of the axiomatic models hold. In this work, we show, for the
first time, that both in-network regularization and functional map training can
be replaced with data-driven methods. For this, we first train a generative
model of functional maps in the spectral domain using score-based generative
modeling, built from a large collection of high-quality maps. We then exploit
the resulting model to promote the structural properties of ground truth
functional maps on new shape collections. Remarkably, we demonstrate that the
learned models are category-agnostic, and can fully replace commonly used
strategies such as enforcing Laplacian commutativity or orthogonality of
functional maps. Our key technical contribution is a novel distillation
strategy from diffusion models in the spectral domain. Experiments demonstrate
that our learned regularization leads to better results than axiomatic
approaches for zero-shot non-rigid shape matching. Our code is available at:
https://github.com/daidedou/diffumatch/

</details>


### [89] [Slot Attention with Re-Initialization and Self-Distillation](https://arxiv.org/abs/2507.23755)
*Rongzhen Zhao,Yi Zhao,Juho Kannala,Joni Pajarinen*

Main category: cs.CV

TL;DR: DIAS通过重新初始化和自我蒸馏改进了对象中心学习，解决了现有方法的冗余和监督信号问题，并在各种任务上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有的OCL方法将对象超像素聚合到槽中，但槽在初始化后被重复使用，导致冗余槽与信息槽竞争，从而错误地将对象分割成多个部分。此外，主流方法仅从解码槽到输入的重建中获得监督信号，忽略了基于内部信息的潜在监督。

Method: DIAS（对象中心学习的重新初始化和自我蒸馏）通过以下方式解决现有OCL方法的局限性：1.通过重新初始化额外的聚合来减少聚合槽中的冗余，并更新剩余的槽；2.通过驱动第一次聚合迭代中的注意力图来近似最后一次迭代中的注意力图，从而实现自我蒸馏。

Result: DIAS在对象发现和识别等OCL任务上取得了最先进的成果，同时还改进了先进的视觉预测和推理。

Conclusion: DIAS在对象发现和识别等OCL任务上取得了最先进的成果，同时也改进了先进的视觉预测和推理。

Abstract: Unlike popular solutions based on dense feature maps, Object-Centric Learning
(OCL) represents visual scenes as sub-symbolic object-level feature vectors,
termed slots, which are highly versatile for tasks involving visual modalities.
OCL typically aggregates object superpixels into slots by iteratively applying
competitive cross attention, known as Slot Attention, with the slots as the
query. However, once initialized, these slots are reused naively, causing
redundant slots to compete with informative ones for representing objects. This
often results in objects being erroneously segmented into parts. Additionally,
mainstream methods derive supervision signals solely from decoding slots into
the input's reconstruction, overlooking potential supervision based on internal
information. To address these issues, we propose Slot Attention with
re-Initialization and self-Distillation (DIAS): $\emph{i)}$ We reduce
redundancy in the aggregated slots and re-initialize extra aggregation to
update the remaining slots; $\emph{ii)}$ We drive the bad attention map at the
first aggregation iteration to approximate the good at the last iteration to
enable self-distillation. Experiments demonstrate that DIAS achieves
state-of-the-art on OCL tasks like object discovery and recognition, while also
improving advanced visual prediction and reasoning. Our code is available on
https://github.com/Genera1Z/DIAS.

</details>


### [90] [SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting](https://arxiv.org/abs/2507.23772)
*Di Li,Jie Feng,Jiahao Chen,Weisheng Dong,Guanbin Li,Yuhui Zheng,Mingtao Feng,Guangming Shi*

Main category: cs.CV

TL;DR: 本研究提出了SeqAffordSplat基准和SeqSplatNet框架，以解决3D可供性推理中的长时序、多对象交互问题，并在实验中取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D可供性推理方法主要局限于单对象、单步交互，无法满足现实世界复杂应用中对长时序、多对象任务的需求。为了解决这一限制，需要新的方法来处理更复杂的场景和交互序列。

Method: 提出了一种名为SeqSplatNet的新型端到端框架，该框架利用大型语言模型（LLM）自回归地生成文本和分割标记，以指导条件解码器生成3D掩码序列。该框架还包括一个条件几何重建预训练策略，以学习从几何信息中重建掩码，并设计了一个特征注入机制，将2D视觉基础模型（VFM）的语义特征融合到3D解码器中。

Result: SeqSplatNet在SeqAffordSplat基准上设定了新的最先进水平，证明了其在处理长时序、多对象3D可供性推理任务方面的有效性，并将可供性推理能力从单步交互提升到了场景级别。

Conclusion: SeqSplatNet通过将LLM与条件解码器相结合，并引入了条件几何重建和特征注入机制，成功实现了从单步交互到复杂、序列级别的场景理解的跨越，在SeqAffordSplat基准上取得了最先进的性能。

Abstract: 3D affordance reasoning, the task of associating human instructions with the
functional regions of 3D objects, is a critical capability for embodied agents.
Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited
to single-object, single-step interactions, a paradigm that falls short of
addressing the long-horizon, multi-object tasks required for complex real-world
applications. To bridge this gap, we introduce the novel task of Sequential 3D
Gaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale
benchmark featuring 1800+ scenes to support research on long-horizon affordance
understanding in complex 3DGS environments. We then propose SeqSplatNet, an
end-to-end framework that directly maps an instruction to a sequence of 3D
affordance masks. SeqSplatNet employs a large language model that
autoregressively generates text interleaved with special segmentation tokens,
guiding a conditional decoder to produce the corresponding 3D mask. To handle
complex scene geometry, we introduce a pre-training strategy, Conditional
Geometric Reconstruction, where the model learns to reconstruct complete
affordance region masks from known geometric observations, thereby building a
robust geometric prior. Furthermore, to resolve semantic ambiguities, we design
a feature injection mechanism that lifts rich semantic features from 2D Vision
Foundation Models (VFM) and fuses them into the 3D decoder at multiple scales.
Extensive experiments demonstrate that our method sets a new state-of-the-art
on our challenging benchmark, effectively advancing affordance reasoning from
single-step interactions to complex, sequential tasks at the scene level.

</details>


### [91] [Half-Physics: Enabling Kinematic 3D Human Model with Physical Interactions](https://arxiv.org/abs/2507.23778)
*Li Siyao,Yao Feng,Omid Tehari,Chen Change Loy,Michael J. Black*

Main category: cs.CV

TL;DR: 提出一种“半物理”方法，使3D人体模型（SMPL-X）能够进行真实的物理交互，解决了现有模型的穿透和不真实动力学问题，并实现了实时运行。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体模型（如SMPL-X）虽然能高效表示人体姿态，但缺乏与环境进行物理交互的能力，导致交互模型出现穿透和不真实动力学的问题。

Method: 提出了一种“半物理”机制，将SMPL-X的运动转换为物理模拟，实现与环境和物体的物理交互，同时保持SMPL-X的运动控制。

Result: 该方法能够消除穿透和不真实动力学，实现与场景和物体的物理上合理的交互，同时保持原始运动的保真度，并且可以实时运行，无需复杂的训练。

Conclusion: 该方法通过将SMPL-X嵌入物理实体，并采用“半物理”机制将运动转化为物理模拟，从而实现了可与环境进行物理交互的3D人体模型。该方法解决了现有模型中存在的穿透和不真实动力学问题，同时保持了运动的精确性和实时性。

Abstract: While current general-purpose 3D human models (e.g., SMPL-X) efficiently
represent accurate human shape and pose, they lacks the ability to physically
interact with the environment due to the kinematic nature. As a result,
kinematic-based interaction models often suffer from issues such as
interpenetration and unrealistic object dynamics. To address this limitation,
we introduce a novel approach that embeds SMPL-X into a tangible entity capable
of dynamic physical interactions with its surroundings. Specifically, we
propose a "half-physics" mechanism that transforms 3D kinematic motion into a
physics simulation. Our approach maintains kinematic control over inherent
SMPL-X poses while ensuring physically plausible interactions with scenes and
objects, effectively eliminating penetration and unrealistic object dynamics.
Unlike reinforcement learning-based methods, which demand extensive and complex
training, our half-physics method is learning-free and generalizes to any body
shape and motion; meanwhile, it operates in real time. Moreover, it preserves
the fidelity of the original kinematic motion while seamlessly integrating
physical interactions

</details>


### [92] [Phi-Ground Tech Report: Advancing Perception in GUI Grounding](https://arxiv.org/abs/2507.23779)
*Miaosen Zhang,Ziqiang Xu,Jialiang Zhu,Qi Dai,Kai Qiu,Yifan Yang,Chong Luo,Tianyi Chen,Justin Wagle,Tim Franklin,Baining Guo*

Main category: cs.CV

TL;DR: 对GUI基础模型进行了实证研究，并开发了Phi-Ground模型系列，在ScreenSpot-pro和UI-Vision上取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 当前的端到端基础模型在ScreenSpot-pro和UI-Vision等具有挑战性的基准测试上的准确率不到65%，距离部署还有很长的路要走。

Method: 对GUI基础模型进行了实证研究，检查了从数据收集到模型训练的细节，开发了Phi-Ground模型系列。

Result: Phi-Ground模型系列在所有五个基础基准测试中都取得了最先进的性能，在端到端模型设置中，在ScreenSpot-pro和UI-Vision上的得分分别为43.2和27.2。

Conclusion: 通过对从数据收集到模型训练的细节进行实证研究，开发了Phi-Ground模型系列，在所有五个基准测试中取得了最先进的性能，在端到端模型设置中，Phi-Ground在ScreenSpot-pro和UI-Vision上分别取得了43.2和27.2的SOTA结果。

Abstract: With the development of multimodal reasoning models, Computer Use Agents
(CUAs), akin to Jarvis from \textit{"Iron Man"}, are becoming a reality. GUI
grounding is a core component for CUAs to execute actual actions, similar to
mechanical control in robotics, and it directly leads to the success or failure
of the system. It determines actions such as clicking and typing, as well as
related parameters like the coordinates for clicks. Current end-to-end
grounding models still achieve less than 65\% accuracy on challenging
benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from
being ready for deployment. % , as a single misclick can result in unacceptable
consequences. In this work, we conduct an empirical study on the training of
grounding models, examining details from data collection to model training.
Ultimately, we developed the \textbf{Phi-Ground} model family, which achieves
state-of-the-art performance across all five grounding benchmarks for models
under $10B$ parameters in agent settings. In the end-to-end model setting, our
model still achieves SOTA results with scores of \textit{\textbf{43.2}} on
ScreenSpot-pro and \textit{\textbf{27.2}} on UI-Vision. We believe that the
various details discussed in this paper, along with our successes and failures,
not only clarify the construction of grounding models but also benefit other
perception tasks. Project homepage:
\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}

</details>


### [93] [MonoFusion: Sparse-View 4D Reconstruction via Monocular Fusion](https://arxiv.org/abs/2507.23782)
*Zihan Wang,Jeff Tan,Tarasha Khurana,Neehar Peri,Deva Ramanan*

Main category: cs.CV

TL;DR: 提出了一种新方法，仅使用稀疏视角（如4个摄像机）即可重建动态场景（如人体行为），解决了现有方法需要昂贵的密集多视角采集的问题。该方法通过对齐单目重建来实现时间、视图一致性，并在实验中取得了优于先前技术的成果。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有动态场景重建方法通常需要昂贵且不便的密集多视角采集（如Panoptic Studio），而我们旨在仅使用稀疏视角的摄像机（例如四个等距向内放置的静态摄像机）重建动态人体行为。

Method: 通过仔细对齐独立的单目重建，以生成时间一致且视图一致的动态场景重建。

Result: 所提出的方法在PanopticStudio和Ego-Exo4D数据集上实现了更高质量的重建，尤其在渲染新视角方面效果更佳。

Conclusion: 该方法在PanopticStudio和Ego-Exo4D数据集上进行了广泛实验，证明了其重建质量优于现有技术，尤其是在渲染新视角时。

Abstract: We address the problem of dynamic scene reconstruction from sparse-view
videos. Prior work often requires dense multi-view captures with hundreds of
calibrated cameras (e.g. Panoptic Studio). Such multi-view setups are
prohibitively expensive to build and cannot capture diverse scenes in-the-wild.
In contrast, we aim to reconstruct dynamic human behaviors, such as repairing a
bike or dancing, from a small set of sparse-view cameras with complete scene
coverage (e.g. four equidistant inward-facing static cameras). We find that
dense multi-view reconstruction methods struggle to adapt to this sparse-view
setup due to limited overlap between viewpoints. To address these limitations,
we carefully align independent monocular reconstructions of each camera to
produce time- and view-consistent dynamic scene reconstructions. Extensive
experiments on PanopticStudio and Ego-Exo4D demonstrate that our method
achieves higher quality reconstructions than prior art, particularly when
rendering novel views. Code, data, and data-processing scripts are available on
https://github.com/ImNotPrepared/MonoFusion.

</details>


### [94] [SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions](https://arxiv.org/abs/2507.23784)
*Jessica Bader,Leander Girrbach,Stephan Alaniz,Zeynep Akata*

Main category: cs.CV

TL;DR: CBMs 在分布变化下识别概念的能力不足。我们提出了 SUB 基准和 TDG 方法来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 现有的CBMs等可解释模型在分布变化下识别概念的能力不足，这在医学等领域至关重要。

Method: 提出 SUB 数据集，包含38,400张基于CUB数据集的合成图像，通过替换特定概念（如翅膀颜色或腹部图案）来评估CBMs在概念变化下的鲁棒性。同时提出一种新颖的Tied Diffusion Guidance (TDG) 方法，通过控制两个并行去噪过程中的噪声共享，来精确控制生成图像，确保生成正确的鸟类类别和属性。

Result: SUB基准和TDG方法能够更严格地评估CBMs和其他可解释模型的性能，有助于推动更鲁棒方法的发展。

Conclusion: CBMs 在分布变化下识别正确概念的能力不足，提出的 SUB 基准和 TDG 方法有助于开发更鲁棒的可解释模型。

Abstract: Concept Bottleneck Models (CBMs) and other concept-based interpretable models
show great promise for making AI applications more transparent, which is
essential in fields like medicine. Despite their success, we demonstrate that
CBMs struggle to reliably identify the correct concepts under distribution
shifts. To assess the robustness of CBMs to concept variations, we introduce
SUB: a fine-grained image and concept benchmark containing 38,400 synthetic
images based on the CUB dataset. To create SUB, we select a CUB subset of 33
bird classes and 45 concepts to generate images which substitute a specific
concept, such as wing color or belly pattern. We introduce a novel Tied
Diffusion Guidance (TDG) method to precisely control generated images, where
noise sharing for two parallel denoising processes ensures that both the
correct bird class and the correct attribute are generated. This novel
benchmark enables rigorous evaluation of CBMs and similar interpretable models,
contributing to the development of more robust methods. Our code is available
at https://github.com/ExplainableML/sub and the dataset at
http://huggingface.co/datasets/Jessica-bader/SUB.

</details>


### [95] [Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis](https://arxiv.org/abs/2507.23785)
*Bowen Zhang,Sicheng Xu,Chuxin Wang,Jiaolong Yang,Feng Zhao,Dong Chen,Baining Guo*

Main category: cs.CV

TL;DR: 提出了一种新颖的视频到4D生成框架，使用Mesh-to-GS VAE和高斯变分场扩散模型，实现了高质量的动态3D内容生成，并能泛化到真实视频。


<details>
  <summary>Details</summary>
Motivation: 直接进行4D扩散建模面临数据构建成本高昂和联合表示3D形状、外观及运动的高维性等挑战。

Method: 提出了一种新颖的视频到4D生成框架，通过引入直接编码高斯散布（GS）及其时间变化的“Mesh-to-GS Variation Field VAE”来解决4D扩散建模的挑战，并将高维动画压缩到紧凑的潜在空间。在此基础上，训练了一个结合时间感知扩散Transformer的高斯变分场扩散模型，该模型以输入视频和规范GS为条件。

Result: 在合成数据上训练的模型在生成质量上优于现有方法，并能泛化到真实世界的视频输入。

Conclusion: 该模型在生成质量上优于现有方法，并且能够泛化到真实世界的视频输入，为生成高质量的动画3D内容铺平了道路。

Abstract: In this paper, we present a novel framework for video-to-4D generation that
creates high-quality dynamic 3D content from single video inputs. Direct 4D
diffusion modeling is extremely challenging due to costly data construction and
the high-dimensional nature of jointly representing 3D shape, appearance, and
motion. We address these challenges by introducing a Direct 4DMesh-to-GS
Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and
their temporal variations from 3D animation data without per-instance fitting,
and compresses high-dimensional animations into a compact latent space.
Building upon this efficient representation, we train a Gaussian Variation
Field diffusion model with temporal-aware Diffusion Transformer conditioned on
input videos and canonical GS. Trained on carefully-curated animatable 3D
objects from the Objaverse dataset, our model demonstrates superior generation
quality compared to existing methods. It also exhibits remarkable
generalization to in-the-wild video inputs despite being trained exclusively on
synthetic data, paving the way for generating high-quality animated 3D content.
Project page: https://gvfdiffusion.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [96] [Large Language Models in the Travel Domain: An Industrial Experience](https://arxiv.org/abs/2507.22910)
*Sergio Di Meglio,Aniello Somma,Luigi Libero Lucio Starace,Fabio Scippacercola,Giancarlo Sperlì,Sergio Di Martino*

Main category: cs.CL

TL;DR: 该研究评估了两种大型语言模型（Mistral 7B 和 Mixtral 8x7B）在房产预订平台上的应用，旨在提高数据一致性。Mixtral 8x7B 在数据质量上表现更优，但在计算成本上也更高，研究为模型选择提供了依据。


<details>
  <summary>Details</summary>
Motivation: 在线房产预订平台依赖第三方提供商提供的准确、最新的住宿信息。然而，外部数据源经常包含不完整或不一致的详细信息，这会使用户感到沮丧并导致市场份额损失。

Method: 通过在 CALEIDOHOTELS 平台集成和评估两种大型语言模型（微调的 Mistral 7B 和带有优化系统提示的 Mixtral 8x7B），以生成一致且同质的描述，同时最大限度地减少数据不一致性。

Result: Mixtral 8x7B 在完整性（99.6% vs. 93%）、精确度（98.8% vs. 96%）和幻觉率（1.2% vs. 4%）方面优于 Mistral 7B，生成的描述更简洁（平均 249 字 vs. 277 字）。然而，Mixtral 8x7B 的计算成本更高（需要 50GB VRAM 和 1.61 美元/小时），而 Mistral 7B 只需要 5GB VRAM 和 0.16 美元/小时。

Conclusion: 研究结果为在生产环境部署大型语言模型提供了实用的见解，并强调了模型质量与资源效率之间的权衡，同时证明了它们在提高住宿数据一致性和可靠性方面的有效性。

Abstract: Online property booking platforms are widely used and rely heavily on
consistent, up-to-date information about accommodation facilities, often
sourced from third-party providers. However, these external data sources are
frequently affected by incomplete or inconsistent details, which can frustrate
users and result in a loss of market. In response to these challenges, we
present an industrial case study involving the integration of Large Language
Models (LLMs) into CALEIDOHOTELS, a property reservation platform developed by
FERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B,
fine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt.
Both models were assessed based on their ability to generate consistent and
homogeneous descriptions while minimizing hallucinations. Mixtral 8x7B
outperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision
(98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet
more concise content (249 vs. 277 words on average). However, this came at a
significantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB
and $0.16/hour for Mistral 7B. Our findings provide practical insights into the
trade-offs between model quality and resource efficiency, offering guidance for
deploying LLMs in production environments and demonstrating their effectiveness
in enhancing the consistency and reliability of accommodation data.

</details>


### [97] [ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing](https://arxiv.org/abs/2507.22911)
*Jinzhi Wang,Qingke Peng,Haozhou Li,Zeyuan Zeng,Qinfeng Song,Kaixuan Yang,Jiangbo Zhang,Yaoying Wang,Ruimeng Li,Biyi Zhou*

Main category: cs.CL

TL;DR: ElectriQ 是一个旨在评估和增强电力营销领域 LLM 的新基准，它通过包含特定数据集、评估指标和知识增强方法，证明了经过微调和增强的小型 LLM 在特定任务上可以超越通用 LLM。


<details>
  <summary>Details</summary>
Motivation: 当前的电力营销客户服务系统（如中国的 95598 热线）存在响应慢、流程不灵活、领域特定任务准确性有限等问题。通用 LLM 缺乏领域专业知识和同理心。

Method: 提出了一种名为 ElectriQ 的基准，包括一个包含六个关键服务类别的对话数据集和四个评估指标（专业性、受欢迎程度、可读性和用户友好性）。还结合了领域特定的知识库和知识增强方法。

Result: 在 13 个 LLM 上进行的实验表明，经过微调和增强的小型模型（如 LLama3-8B）在专业性和用户友好性方面可以超越 GPT-4o。

Conclusion: ElectriQ 为电力营销服务的 LLM 开发奠定了基础，表明经过微调和增强的小型模型在专业性和用户友好性方面可以超越大型模型。

Abstract: Electric power marketing customer service plays a critical role in addressing
inquiries, complaints, and service requests. However, current systems, such as
China's 95598 hotline, often struggle with slow response times, inflexible
procedures, and limited accuracy in domain-specific tasks. While large language
models (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities,
they lack the domain expertise and empathy required in this field. To bridge
this gap, we introduce ElectriQ, the first benchmark designed to evaluate and
enhance LLMs in electric power marketing scenarios. ElectriQ consists of a
dialogue dataset covering six key service categories and introduces four
evaluation metrics: professionalism, popularity, readability, and
user-friendliness. We further incorporate a domain-specific knowledge base and
propose a knowledge augmentation method to boost model performance. Experiments
on 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and
augmented, can surpass GPT-4o in terms of professionalism and
user-friendliness. ElectriQ establishes a comprehensive foundation for
developing LLMs tailored to the needs of power marketing services.

</details>


### [98] [A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms](https://arxiv.org/abs/2507.22912)
*Navid Yazdanjue,Morteza Rakhshaninejad,Hossein Yazdanjouei,Mohammad Sadegh Khorshidi,Mikko S. Niemela,Fang Chen,Amir H. Gandomi*

Main category: cs.CL

TL;DR: 本研究提出了一种新颖的非法市场内容检测框架，通过结合先进的语言模型和半监督集成学习，能够有效应对数据稀疏和语言演变的挑战，并在多个真实数据集上取得了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 非法市场越来越多地转移到互联网的隐蔽部分，如深度和黑暗网络以及Telegram、Reddit和Pastebin等平台。由于标记数据有限、非法语言不断演变以及在线来源的结构异质性，检测和分类此类内容仍然是一个挑战。本研究旨在提出一个有效的框架来解决这些问题。

Method: 本研究提出了一种层次分类框架，首先使用微调的ModernBERT模型结合手动设计的特征（如文档结构、比特币地址、电子邮件和IP等）来捕获专门术语和模糊的语言模式，然后通过一个包含XGBoost、随机森林和SVM的半监督集成模型进行两阶段分类，第一阶段检测销售相关文档，第二阶段将其分类为毒品、武器或凭证销售。

Result: 该模型在三个数据集（包括多源语料库、DUTA和CoDA）上的实验结果显示，其准确率为0.96489，F1分数为0.93467，TMCC为0.95388，优于BERT、ModernBERT、DarkBERT、ALBERT、Longformer和BigBird等多种基线模型。

Conclusion: 该研究提出了一种结合微调语言模型和半监督集成学习策略的层次分类框架，用于检测和分类来自不同平台的非法市场内容。实验结果表明，该模型在准确率、F1分数和TMCC方面优于多种基线模型，证明了其在有限监督下的泛化能力、鲁棒性以及在现实世界非法内容检测中的有效性。

Abstract: Illegal marketplaces have increasingly shifted to concealed parts of the
internet, including the deep and dark web, as well as platforms such as
Telegram, Reddit, and Pastebin. These channels enable the anonymous trade of
illicit goods including drugs, weapons, and stolen credentials. Detecting and
categorizing such content remains challenging due to limited labeled data, the
evolving nature of illicit language, and the structural heterogeneity of online
sources. This paper presents a hierarchical classification framework that
combines fine-tuned language models with a semi-supervised ensemble learning
strategy to detect and classify illicit marketplace content across diverse
platforms. We extract semantic representations using ModernBERT, a transformer
model for long documents, finetuned on domain-specific data from deep and dark
web pages, Telegram channels, Subreddits, and Pastebin pastes to capture
specialized jargon and ambiguous linguistic patterns. In addition, we
incorporate manually engineered features such as document structure, embedded
patterns including Bitcoin addresses, emails, and IPs, and metadata, which
complement language model embeddings. The classification pipeline operates in
two stages. The first stage uses a semi-supervised ensemble of XGBoost, Random
Forest, and SVM with entropy-based weighted voting to detect sales-related
documents. The second stage further classifies these into drug, weapon, or
credential sales. Experiments on three datasets, including our multi-source
corpus, DUTA, and CoDA, show that our model outperforms several baselines,
including BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The
model achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of
0.95388, demonstrating strong generalization, robustness under limited
supervision, and effectiveness in real-world illicit content detection.

</details>


### [99] [EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow](https://arxiv.org/abs/2507.22929)
*Xiaoyu Pan,Yang Bai,Ke Zou,Yang Zhou,Jun Zhou,Huazhu Fu,Yih-Chung Tham,Yong Liu*

Main category: cs.CL

TL;DR: EH-Benchmark是一个用于评估医疗大语言模型在眼科诊断中幻觉问题的基准，并提出了一个多智能体框架来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 现有的医疗大语言模型在眼科诊断中存在幻觉问题，限制了其在检测病变和诊断疾病方面的准确性，而现有的基准测试无法有效评估和解决这些问题。

Method: 提出了一种基于多智能体的三阶段框架（知识检索、任务案例研究、结果验证）来评估和缓解医疗大语言模型中的幻觉问题。

Result: 提出的多智能体框架能够显著减少视觉理解和逻辑构成两类幻觉，从而提高模型的准确性、可解释性和可靠性。

Conclusion: EH-Benchmark能够有效评估和缓解医疗大语言模型在眼科诊断中的幻觉问题，显著提升了模型的准确性、可解释性和可靠性。

Abstract: Medical Large Language Models (MLLMs) play a crucial role in ophthalmic
diagnosis, holding significant potential to address vision-threatening
diseases. However, their accuracy is constrained by hallucinations stemming
from limited ophthalmic knowledge, insufficient visual localization and
reasoning capabilities, and a scarcity of multimodal ophthalmic data, which
collectively impede precise lesion detection and disease diagnosis.
Furthermore, existing medical benchmarks fail to effectively evaluate various
types of hallucinations or provide actionable solutions to mitigate them. To
address the above challenges, we introduce EH-Benchmark, a novel ophthalmology
benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs'
hallucinations based on specific tasks and error types into two primary
classes: Visual Understanding and Logical Composition, each comprising multiple
subclasses. Given that MLLMs predominantly rely on language-based reasoning
rather than visual processing, we propose an agent-centric, three-phase
framework, including the Knowledge-Level Retrieval stage, the Task-Level Case
Studies stage, and the Result-Level Validation stage. Experimental results show
that our multi-agent framework significantly mitigates both types of
hallucinations, enhancing accuracy, interpretability, and reliability. Our
project is available at https://github.com/ppxy1/EH-Benchmark.

</details>


### [100] [A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models](https://arxiv.org/abs/2507.22913)
*Jinyu Liu,Xiaoying Song,Diana Zhang,Jason Thomale,Daqing He,Lingzi Hong*

Main category: cs.CL

TL;DR: 使用结合了机器学习和大型语言模型的混合方法，改进了图书主题词的预测，减少了错误。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统机器学习模型在进行主题分析时难以处理未见案例以及大型语言模型在主题分析中可能过度生成和产生幻觉的问题，提出了一种混合框架。

Method: 提出了一种混合框架，该框架整合了基于嵌入的机器学习模型和大型语言模型。具体来说，利用机器学习模型来（1）预测最适合的LCSH标签数量以指导大型语言模型的预测，以及（2）通过实际的LCSH术语对预测结果进行后编辑，以减少幻觉。

Result: 实验结果表明，通过引导大型语言模型生成和进行后编辑，能够获得更可控且与词汇更一致的主题词预测结果。

Conclusion: 该混合框架通过结合基于嵌入的机器学习模型和大型语言模型，能够更可控且符合词汇的进行主题词预测，解决了传统机器学习模型在处理未见案例时的困难以及大型语言模型可能产生的过度生成和幻觉问题。

Abstract: Providing subject access to information resources is an essential function of
any library management system. Large language models (LLMs) have been widely
used in classification and summarization tasks, but their capability to perform
subject analysis is underexplored. Multi-label classification with traditional
machine learning (ML) models has been used for subject analysis but struggles
with unseen cases. LLMs offer an alternative but often over-generate and
hallucinate. Therefore, we propose a hybrid framework that integrates
embedding-based ML models with LLMs. This approach uses ML models to (1)
predict the optimal number of LCSH labels to guide LLM predictions and (2)
post-edit the predicted terms with actual LCSH terms to mitigate
hallucinations. We experimented with LLMs and the hybrid framework to predict
the subject terms of books using the Library of Congress Subject Headings
(LCSH). Experiment results show that providing initial predictions to guide LLM
generations and imposing post-edits result in more controlled and
vocabulary-aligned outputs.

</details>


### [101] [LENS: Learning Ensemble Confidence from Neural States for Multi-LLM Answer Integration](https://arxiv.org/abs/2507.23167)
*Jizhou Guo*

Main category: cs.CL

TL;DR: LENS通过分析LLM的内部表示来学习模型置信度，从而改进模型集成。


<details>
  <summary>Details</summary>
Motivation: 现有集成方法通常依赖于投票或logits集成等简单技术，这些技术忽略了模型在不同上下文中的置信度和可靠性差异。需要一种更有效的方法来结合多个LLM的预测，以增强系统的鲁棒性和性能。

Method: 提出了一种名为LENS（Learning ENsemble confidence from Neural States）的新颖方法，通过分析内部表示来学习模型置信度。LENS为每个LLM训练一个轻量级的线性置信度预测器，该预测器以层状隐藏状态和归一化概率作为输入，从而可以根据模型对上下文的依赖性进行更细致的加权。

Result: 在多项选择和布尔问答任务上的实验结果表明，LENS的表现优于传统的集成方法。

Conclusion: LLM集成可以通过分析内部表示来学习模型置信度，从而超越传统集成方法。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
various tasks, with different models excelling in distinct domains and specific
abilities. Effectively combining the predictions of multiple LLMs is crucial
for enhancing system robustness and performance. However, existing ensemble
methods often rely on simple techniques like voting or logits ensembling, which
overlook the varying confidence and reliability of models in different
contexts. In this work, we propose LENS (Learning ENsemble confidence from
Neural States), a novel approach that learns to estimate model confidence by
analyzing internal representations. For each LLM, we train a lightweight linear
confidence predictor that leverages layer-wise hidden states and normalized
probabilities as inputs. This allows for more nuanced weighting of model
predictions based on their context-dependent reliability. Our method does not
require modifying the model parameters and requires negligible additional
computation. Experimental results on multiple-choice and boolean
question-answering tasks demonstrate that LENS outperforms traditional ensemble
methods by a substantial margin. Our findings suggest that internal
representations provide valuable signals for determining model confidence and
can be effectively leveraged for ensemble learning.

</details>


### [102] [Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs](https://arxiv.org/abs/2507.22914)
*Victor Eiti Yamamoto,Hideaki Takeda*

Main category: cs.CL

TL;DR: 知识图谱集成中的上下文匹配是一个被忽视的领域。我们提出了一种结合标签和三元组匹配的新方法，通过字符串操作、模糊匹配和向量相似性来对齐标签，并通过识别可比信息三元组来提高准确性。该方法在各种测试用例中均表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的实体匹配方法在整合不同且复杂的上下文时可能表现不佳，因为现实世界的知识图谱在来源、大小和信息密度方面差异很大，而这些因素在当前评估实体匹配方法的数据集上通常没有得到体现。因此，需要一种能够解决上下文匹配问题的方法。

Method: 提出了一种新颖的知识图谱集成方法，包括标签匹配和三元组匹配。使用字符串操作、模糊匹配和向量相似性技术来对齐实体和谓词标签。然后，通过识别传递可比信息的三元组之间的映射来改进实体匹配的准确性。

Result: 所提出的方法在OAEI竞赛和监督方法中均取得有竞争力且高精度的性能，并引入了一个新的数据集来更全面地评估三元组匹配步骤。

Conclusion: 该方法在OAEI竞赛和监督方法中都表现出有竞争力，在各种测试用例中均取得高精度。

Abstract: Knowledge graphs (KGs) are powerful tools for representing and reasoning over
structured information. Their main components include schema, identity, and
context. While schema and identity matching are well-established in ontology
and entity matching research, context matching remains largely unexplored. This
is particularly important because real-world KGs often vary significantly in
source, size, and information density - factors not typically represented in
the datasets on which current entity matching methods are evaluated. As a
result, existing approaches may fall short in scenarios where diverse and
complex contexts need to be integrated.
  To address this gap, we propose a novel KG integration method consisting of
label matching and triple matching. We use string manipulation, fuzzy matching,
and vector similarity techniques to align entity and predicate labels. Next, we
identify mappings between triples that convey comparable information, using
these mappings to improve entity-matching accuracy. Our approach demonstrates
competitive performance compared to leading systems in the OAEI competition and
against supervised methods, achieving high accuracy across diverse test cases.
Additionally, we introduce a new dataset derived from the benchmark dataset to
evaluate the triple-matching step more comprehensively.

</details>


### [103] [Predicting stock prices with ChatGPT-annotated Reddit sentiment](https://arxiv.org/abs/2507.22922)
*Mateusz Kmak,Kamil Chmurzyński,Kamil Matejuk,Paweł Kotzbach,Jan Kocoń*

Main category: cs.CL

TL;DR: 社交媒体情绪与股价关联不大，评论量和搜索趋势的预测能力更强。


<details>
  <summary>Details</summary>
Motivation: 在2021年GameStop轧空事件等散户投资者在社交媒体上活跃的背景下，本文旨在探讨源自社交媒体讨论的情绪是否能有意义地预测股票市场的变动。

Method: 本文运用了两种现有的基于文本的情绪分析方法，并引入了一种新的、基于ChatGPT注释和微调的RoBERTa模型，以更好地解读社交媒体讨论中非正式语言和表情符号的含义。同时，利用相关性和因果关系指标来评估这些模型的预测能力。

Result: 研究结果显示，社交媒体情绪与股价之间仅存在弱相关性，而评论量和谷歌搜索趋势等更简单的指标则显示出更强的预测信号。这说明了散户投资者行为的复杂性。

Conclusion: 社交媒体情绪与股价之间仅存在弱相关性，而评论量和谷歌搜索趋势等更简单的指标表现出更强的预测信号，这表明传统的情绪分析可能无法完全捕捉市场驱动的在线讨论的细微差别。

Abstract: The surge of retail investor activity on social media, exemplified by the
2021 GameStop short squeeze, raised questions about the influence of online
sentiment on stock prices. This paper explores whether sentiment derived from
social media discussions can meaningfully predict stock market movements. We
focus on Reddit's r/wallstreetbets and analyze sentiment related to two
companies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's
role, we employ two existing text-based sentiment analysis methods and
introduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model
designed to better interpret the informal language and emojis prevalent in
social media discussions. We use correlation and causality metrics to determine
these models' predictive power. Surprisingly, our findings suggest that social
media sentiment has only a weak correlation with stock prices. At the same
time, simpler metrics, such as the volume of comments and Google search trends,
exhibit stronger predictive signals. These results highlight the complexity of
retail investor behavior and suggest that traditional sentiment analysis may
not fully capture the nuances of market-moving online discussions.

</details>


### [104] [Theoretical Foundations and Mitigation of Hallucination in Large Language Models](https://arxiv.org/abs/2507.22915)
*Esmail Gumaan*

Main category: cs.CL

TL;DR: 本文对大型语言模型中的幻觉问题进行了严谨的处理，包括形式化定义、理论分析、风险界限推导、检测与缓解策略的调研，并提出了统一的工作流程和评估协议，旨在为解决LLM幻觉问题提供理论基础和实践指导。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）中生成与输入或事实不符内容的“幻觉”问题。

Method: 提出了形式化定义、理论分析（PAC-贝叶斯和Rademacher复杂度）、幻觉风险的界限推导，并调研了检测策略（如token级不确定性估计、置信度校准、注意力对齐检查）和缓解方法（如检索增强生成、幻觉感知微调、logit校准、事实核查模块），最后提出了统一的检测和缓解工作流程，并概述了评估协议。

Result: 区分了内在和外在幻觉，定义了模型的“幻觉风险”，推导了风险界限，调研了检测和缓解策略，提出了统一的工作流程和评估协议。

Conclusion: 这项工作为解决大型语言模型中的幻觉问题奠定了理论基础和实践指导。

Abstract: Hallucination in Large Language Models (LLMs) refers to the generation of
content that is not faithful to the input or the real-world facts. This paper
provides a rigorous treatment of hallucination in LLMs, including formal
definitions and theoretical analyses. We distinguish between intrinsic and
extrinsic hallucinations, and define a \textit{hallucination risk} for models.
We derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes
and Rademacher complexity). We then survey detection strategies for
hallucinations, such as token-level uncertainty estimation, confidence
calibration, and attention alignment checks. On the mitigation side, we discuss
approaches including retrieval-augmented generation, hallucination-aware
fine-tuning, logit calibration, and the incorporation of fact-verification
modules. We propose a unified detection and mitigation workflow, illustrated
with a diagram, to integrate these strategies. Finally, we outline evaluation
protocols for hallucination, recommending datasets, metrics, and experimental
setups to quantify and reduce hallucinations. Our work lays a theoretical
foundation and practical guidelines for addressing the crucial challenge of
hallucination in LLMs.

</details>


### [105] [Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection](https://arxiv.org/abs/2507.22930)
*Shalini Jangra,Suparna De,Nishanth Sastry,Saeed Fadaei*

Main category: cs.CL

TL;DR: 研究人员创建了一个合成PII数据集，以促进对在线社交媒体PII隐私风险的研究。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏开源标记数据集，对PII识别和检索的研究受到阻碍。本研究旨在通过创建合成PII数据来解决这一问题，以促进可复现的研究。

Method: 研究人员开发了一种新方法来创建可安全共享的PII披露数据合成物，并使用Llama2-7B、Llama3-8B和zephyr-7b-beta等三个LLM，通过顺序指令提示生成了合成PII标签的多文本跨数据集。

Result: 研究人员创建了一个合成PII数据集，并验证了其可复现性、不可链接性和不可区分性。

Conclusion: 研究人员发布了一个包含19种PII类别（针对弱势群体）的合成PII数据集，并附有代码，以促进在线社交媒体PII隐私风险的可复现研究。

Abstract: Social platforms such as Reddit have a network of communities of shared
interests, with a prevalence of posts and comments from which one can infer
users' Personal Information Identifiers (PIIs). While such self-disclosures can
lead to rewarding social interactions, they pose privacy risks and the threat
of online harms. Research into the identification and retrieval of such risky
self-disclosures of PIIs is hampered by the lack of open-source labeled
datasets. To foster reproducible research into PII-revealing text detection, we
develop a novel methodology to create synthetic equivalents of PII-revealing
data that can be safely shared. Our contributions include creating a taxonomy
of 19 PII-revealing categories for vulnerable populations and the creation and
release of a synthetic PII-labeled multi-text span dataset generated from 3
text generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and
zephyr-7b-beta, with sequential instruction prompting to resemble the original
Reddit posts. The utility of our methodology to generate this synthetic dataset
is evaluated with three metrics: First, we require reproducibility equivalence,
i.e., results from training a model on the synthetic data should be comparable
to those obtained by training the same models on the original posts. Second, we
require that the synthetic data be unlinkable to the original users, through
common mechanisms such as Google Search. Third, we wish to ensure that the
synthetic data be indistinguishable from the original, i.e., trained humans
should not be able to tell them apart. We release our dataset and code at
https://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster
reproducible research into PII privacy risks in online social media.

</details>


### [106] [Reading Between the Timelines: RAG for Answering Diachronic Questions](https://arxiv.org/abs/2507.22917)
*Kwun Hang Lau,Ruiyuan Zhang,Weijie Shi,Xiaofang Zhou,Xiaojun Cheng*

Main category: cs.CL

TL;DR: 本研究提出了TA-RAG框架，通过引入时间逻辑来改进RAG系统处理纵向查询的能力，并在ADQAB基准上取得了显著的准确性提升。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成（RAG）在注入静态、事实知识方面表现出色，但在处理需要跨越时间跟踪实体和现象的纵向查询方面存在严重缺陷，因为传统的、由语义驱动的检索方法无法收集对特定持续时间来说既相关又具有时间连贯性的证据。

Method: 本研究提出的方法首先将用户的查询分解为核心主体及其时间窗口。然后，采用一种特殊的检索器，该检索器通过时间相关性来校准语义匹配，以确保收集到跨越整个查询期间的连续证据集。

Result: 在ADQAB基准上的实证结果表明，本研究提出的方法在答案准确性方面带来了实质性的提升，相比标准的RAG实现，准确性提高了13%至27%。

Conclusion: 本研究提出了一种新的框架，通过注入时间逻辑来重新设计检索增强生成（RAG）流程，以解决传统RAG在处理需要跨越时间跟踪实体和现象的纵向查询方面的不足。通过将查询分解为主题和时间窗口，并采用结合语义和时间相关性的专用检索器，确保收集跨越整个查询期间的连续证据集。该研究还引入了一个名为ADQAB的评估基准，并在该基准上进行了实证评估。

Abstract: While Retrieval-Augmented Generation (RAG) excels at injecting static,
factual knowledge into Large Language Models (LLMs), it exhibits a critical
deficit in handling longitudinal queries that require tracking entities and
phenomena across time. This blind spot arises because conventional,
semantically-driven retrieval methods are not equipped to gather evidence that
is both topically relevant and temporally coherent for a specified duration. We
address this challenge by proposing a new framework that fundamentally
redesigns the RAG pipeline to infuse temporal logic. Our methodology begins by
disentangling a user's query into its core subject and its temporal window. It
then employs a specialized retriever that calibrates semantic matching against
temporal relevance, ensuring the collection of a contiguous evidence set that
spans the entire queried period. To enable rigorous evaluation of this
capability, we also introduce the Analytical Diachronic Question Answering
Benchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus
of real and synthetic financial news. Empirical results on ADQAB show that our
approach yields substantial gains in answer accuracy, surpassing standard RAG
implementations by 13% to 27%. This work provides a validated pathway toward
RAG systems capable of performing the nuanced, evolutionary analysis required
for complex, real-world questions. The dataset and code for this study are
publicly available at https://github.com/kwunhang/TA-RAG.

</details>


### [107] [Semantic Convergence: Investigating Shared Representations Across Scaled LLMs](https://arxiv.org/abs/2507.22918)
*Daniel Son,Sanjana Rathore,Andrew Rufail,Adrian Simon,Daniel Zhang,Soham Dave,Cole Blondin,Kevin Zhu,Sean O'Brien*

Main category: cs.CL

TL;DR: 研究Gemma-2语言模型，发现不同尺寸的模型在中间层具有相似的内部概念，表明特征普遍性。


<details>
  <summary>Details</summary>
Motivation: 探究Gemma-2语言模型（Gemma-2-2B和Gemma-2-9B）是否存在特征普遍性，即尺度相差四倍的模型是否会收敛于可比的内部概念。

Method: 使用稀疏自编码器（SAE）字典学习流程，对Gemma-2-2B和Gemma-2-9B模型的残差流激活进行SAE分析，通过激活相关性对齐单义特征，并使用SVCCA和RSA比较匹配的特征空间。

Result: 中间层产生的特征重叠最强，而早期和晚期层的相似性则小得多。初步实验将分析从单个标记扩展到多标记子空间，表明语义相似的子空间与语言模型的交互方式相似。

Conclusion: Gemma-2-2B和Gemma-2-9B模型在中间层表现出可比的内部概念，表明大型语言模型在不同尺度下会形成相似的可解释特征，支持跨模型可解释性的普遍性基础。

Abstract: We investigate feature universality in Gemma-2 language models (Gemma-2-2B
and Gemma-2-9B), asking whether models with a four-fold difference in scale
still converge on comparable internal concepts. Using the Sparse Autoencoder
(SAE) dictionary-learning pipeline, we utilize SAEs on each model's
residual-stream activations, align the resulting monosemantic features via
activation correlation, and compare the matched feature spaces with SVCCA and
RSA. Middle layers yield the strongest overlap, while early and late layers
show far less similarity. Preliminary experiments extend the analysis from
single tokens to multi-token subspaces, showing that semantically similar
subspaces interact similarly with language models. These results strengthen the
case that large language models carve the world into broadly similar,
interpretable features despite size differences, reinforcing universality as a
foundation for cross-model interpretability.

</details>


### [108] [A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations](https://arxiv.org/abs/2507.22919)
*Qixuan Hu,Xumou Zhang,Jinman Kim,Florence Bourgeois,Adam G. Dunn*

Main category: cs.CL

TL;DR: 该研究利用ClinicalTrials.gov的注册数据，通过迁移学习和滑动窗口方法，成功开发了预测临床试验严重不良事件（SAE）的模型，平均AUC达77.6%，RMSE为18.6%，有助于优化试验设计和提高安全性。


<details>
  <summary>Details</summary>
Motivation: 准确预测临床试验的安全性结果，可以帮助优化试验设计，避免试验因安全性问题而终止，并减少受试者暴露于不必要风险的可能性。同时，该研究旨在发掘ClinicalTrials.gov中未被充分利用的试验注册信息，以提高试验设计的效率和安全性。

Method: 研究人员分析了来自ClinicalTrials.gov的22,107项临床试验数据。他们开发了两种预测模型：一种是分类模型，用于预测实验组的SAE率是否会高于对照组（AUC）；另一种是回归模型，用于预测对照组的SAE比例（RMSE）。模型采用了迁移学习方法，利用预训练语言模型（如ClinicalT5, BioBERT）进行特征提取，并结合下游模型进行预测。为了处理过长的试验文本，研究人员开发了一种滑动窗口方法来提取嵌入。

Result: 研究中表现最佳的模型（ClinicalT5+Transformer+MLP）在预测哪个试验组的SAE比例更高时，AUC达到了77.6%。在预测对照组受试者SAE比例时，该模型的RMSE为18.6%。滑动窗口方法在所有评估中均优于未使用该方法的模型，使得分类器的平均AUC提高了2.00%，回归模型的平均RMSE降低了1.58%。

Conclusion: 利用ClinicalTrials.gov的注册信息预测临床试验的严重不良事件（SAE）结果是可行的，并且可以优化试验设计，避免不必要的风险和终止。

Abstract: Objectives: With accurate estimates of expected safety results, clinical
trials could be designed to avoid terminations and limit exposing participants
to unnecessary risks. We evaluated methods for predicting serious adverse event
(SAE) results in clinical trials using information only from their
registrations prior to the trial. Material and Methods: We analysed 22,107
two-arm parallel interventional clinical trials from ClinicalTrials.gov with
structured summary results. Two prediction models were developed: a classifier
predicting will experimental arm have higher SAE rates (area under the receiver
operating characteristic curve; AUC) than control arm, and a regression model
to predict the proportion of SAEs in control arms (root mean squared error;
RMSE). A transfer learning approach using pretrained language models (e.g.,
ClinicalT5, BioBERT) was used for feature extraction, combined with downstream
model for prediction. To maintain semantic representation in long trial texts
exceeding localised language model input limits, a sliding window method was
developed for embedding extraction. Results: The best model
(ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a
higher proportion of patients with SAEs. When predicting proportion of
participants experiencing SAE in the control arm, the same model achieved RMSE
of 18.6%. The sliding window approach consistently outperformed methods without
it. Across 12 classifiers, the average absolute AUC increase was 2.00%; across
12 regressors, the average absolute RMSE reduction was 1.58%. Discussion:
Summary results data available at ClinicalTrials.gov remains underutilised. The
potential to estimate results of trials before they start is an opportunity to
improve trial design and flag discrepancies between expected and reported
safety results.

</details>


### [109] [Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey](https://arxiv.org/abs/2507.22920)
*Jindong Li,Yali Fu,Jiahong Liu,Linxiao Cao,Wei Ji,Menglin Yang,Irwin King,Ming-Hsuan Yang*

Main category: cs.CL

TL;DR: 该综述对大语言模型（LLM）的离散化技术进行了首次系统性分类和分析，重点关注向量量化（VQ）方法。文章梳理了8种VQ变体的原理、训练和集成挑战，并探讨了其在单模态和多模态LLM系统中的应用。此外，还指出了代码本坍塌等关键挑战，并展望了动态量化和统一标记化等未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）的快速发展，需要将连续多模态数据转化为适合语言处理的离散表示，而向量量化（VQ）是实现这一目标的关键方法。目前缺乏对LLM背景下VQ技术的全面调查和系统性分析。

Method: 对8种代表性的向量化（VQ）变体进行了分类和分析，涵盖了经典和现代范式，并分析了它们的算法原理、训练动态以及与大语言模型（LLM）流水线的集成挑战。讨论了传统应用、基于LLM的单一模态系统和多模态系统，并确定了关键挑战和新兴研究方向。

Result: 对8种代表性的VQ变体进行了分类和分析，并讨论了它们在不同类型系统中的应用，强调了量化策略对齐、推理和生成性能的影响。同时，识别了代码本坍塌、梯度估计不稳定和特定模态编码约束等关键挑战，并提出了动态量化、统一标记化框架和受生物启发的代码本学习等新兴研究方向。

Conclusion: 该综述弥合了传统向量量化与现代大语言模型应用之间的差距，为开发高效、可泛化的多模态系统奠定了基础参考。

Abstract: The rapid advancement of large language models (LLMs) has intensified the
need for effective mechanisms to transform continuous multimodal data into
discrete representations suitable for language-based processing. Discrete
tokenization, with vector quantization (VQ) as a central approach, offers both
computational efficiency and compatibility with LLM architectures. Despite its
growing importance, there is a lack of a comprehensive survey that
systematically examines VQ techniques in the context of LLM-based systems. This
work fills this gap by presenting the first structured taxonomy and analysis of
discrete tokenization methods designed for LLMs. We categorize 8 representative
VQ variants that span classical and modern paradigms and analyze their
algorithmic principles, training dynamics, and integration challenges with LLM
pipelines. Beyond algorithm-level investigation, we discuss existing research
in terms of classical applications without LLMs, LLM-based single-modality
systems, and LLM-based multimodal systems, highlighting how quantization
strategies influence alignment, reasoning, and generation performance. In
addition, we identify key challenges including codebook collapse, unstable
gradient estimation, and modality-specific encoding constraints. Finally, we
discuss emerging research directions such as dynamic and task-adaptive
quantization, unified tokenization frameworks, and biologically inspired
codebook learning. This survey bridges the gap between traditional vector
quantization and modern LLM applications, serving as a foundational reference
for the development of efficient and generalizable multimodal systems. A
continuously updated version is available at:
https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.

</details>


### [110] [Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers](https://arxiv.org/abs/2507.22921)
*Lee Harris*

Main category: cs.CL

TL;DR: 提出了一种名为语言模型链（LMC）的新算法，该算法通过串联多个语言模型来提高准确性和速度，并减少幻觉，已成功应用于从医疗文件中提取患者出生日期。


<details>
  <summary>Details</summary>
Motivation: 为了解决语言模型成本高昂且容易产生幻觉（即生成不存在的信息）的问题，同时避免因不正确的信息而浪费资源。

Method: 提出、实现和应用了一种名为语言模型链（LMC）的算法。该算法的特点是，语言模型对给定文本的响应只有在存在于可能的（即候选）答案集中时才被认为是正确的，而与不正确响应相对应的文本则被输入到更具预测性（但更慢）的语言模型中。此过程会针对一系列语言模型重复进行，或者直到所有关于文本的预测都正确为止。

Result: 使用LMC算法从医疗文件中提取患者出生日期，结果表明，在多阶段级联中组合一系列语言模型，能够显著提高预测速度和准确性，并大大减少幻觉。

Conclusion: 该语言模型链（LMC）算法通过在多阶段级联中组合语言模型，显著提高了预测速度和准确性，同时大大减少了幻觉数量，为知识提取领域做出了贡献。

Abstract: Language models can capture complex relationships in given text, but these
are notorious for being costly and for producing information that does not
exist (i.e., hallucinations). Furthermore, the resources invested into
producing this information would be wasted if it were incorrect. We address
these issues by proposing, implementing, and applying the Language Model Chain
(LMC) algorithm. In this, a language model's response to a given prompt about
given text is only correct if it exists in the collection of possible (i.e.,
candidate) answers, and text corresponding to incorrect responses is fed into a
more predictive (but slower) language model. This process is repeated for a
collection of language models, or until all predictions about the text are
correct. We used the LMC algorithm to extract patient dates of birth from
medical documents, and combining a collection of language models in a
multi-stage cascade significantly increased prediction speed and accuracy over
individual language models, while greatly reducing the number of corresponding
hallucinations. We believe that the novel LMC algorithm significantly
contributes to the knowledge extraction field, and that this should be explored
much further in the future.

</details>


### [111] [How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting](https://arxiv.org/abs/2507.22923)
*Aman Gupta,Yingying Zhuang,Zhou Yu,Ziji Zhang,Anurag Beniwal*

Main category: cs.CL

TL;DR: 本文评估了多语言RAG系统中不同提示翻译策略对分类任务的影响，发现优化策略能提升跨语言知识共享和低资源语言性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多语言能力方面取得进展，但在不同语言和任务上的表现差异很大。现有的多语言RAG系统常将知识库（KB）从高资源语言共享到低资源语言，导致KB中的检索信息可能与其余上下文语言不一致。然而，预翻译或跨语言提示这两种常见做法对性能的影响尚不明确。

Method: 通过系统性地评估不同提示翻译策略对RAG增强的多语言LLM在分类任务上的影响。

Result: 实验结果表明，优化的提示策略能够显著改善跨语言知识共享，从而提升下游分类任务的性能。

Conclusion: 优化跨语言提示策略可以显著改善多语言RAG系统的知识共享和下游分类任务性能，尤其是在低资源语言方面。

Abstract: Despite advances in the multilingual capabilities of Large Language Models
(LLMs), their performance varies substantially across different languages and
tasks. In multilingual retrieval-augmented generation (RAG)-based systems,
knowledge bases (KB) are often shared from high-resource languages (such as
English) to low-resource ones, resulting in retrieved information from the KB
being in a different language than the rest of the context. In such scenarios,
two common practices are pre-translation to create a mono-lingual prompt and
cross-lingual prompting for direct inference. However, the impact of these
choices remains unclear. In this paper, we systematically evaluate the impact
of different prompt translation strategies for classification tasks with
RAG-enhanced LLMs in multilingual systems. Experimental results show that an
optimized prompting strategy can significantly improve knowledge sharing across
languages, therefore improve the performance on the downstream classification
task. The findings advocate for a broader utilization of multilingual resource
sharing and cross-lingual prompt optimization for non-English languages,
especially the low-resource ones.

</details>


### [112] [Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers](https://arxiv.org/abs/2507.22924)
*Brittney Exline,Melanie Duffin,Brittany Harbison,Chrissa da Gomez,David Joyner*

Main category: cs.CL

TL;DR: 在美国的研究生计算机科学课程中，母语非英语的学生在同行反馈方面比以英语为母语的学生有更积极但复杂的体验。


<details>
  <summary>Details</summary>
Motivation: 在美国的研究生计算机科学课程中，国际学生比例很高，他们中的许多人在线学习，并被要求提供同行反馈。然而，人们对这些学生的同行反馈体验知之甚少。

Method: 研究人员使用基于Twitter-RoBERTa的模型来分析对500名学生的同行评审评论的情感，并将情感分数和同行反馈评级与学生的语言背景联系起来。

Result: 研究结果显示，以英语为母语的学生对反馈的评价较低，而非英语母语的学生则更积极地评价反馈，但收到的反馈评价较低。在控制了性别和年龄等因素后，出现了显著的交互作用。

Conclusion: 母语非英语的学生在在线课程中收到或提供的同行反馈的体验比以英语为母语的学生更积极，但这种积极性是有限的，并且在控制了性别和年龄等因素后，语言背景在塑造同行反馈体验方面起着微妙而复杂的作用。

Abstract: Graduate-level CS programs in the U.S. increasingly enroll international
students, with 60.2 percent of master's degrees in 2023 awarded to non-U.S.
students. Many of these students take online courses, where peer feedback is
used to engage students and improve pedagogy in a scalable manner. Since these
courses are conducted in English, many students study in a language other than
their first. This paper examines how native versus non-native English speaker
status affects three metrics of peer feedback experience in online U.S.-based
computing courses. Using the Twitter-roBERTa-based model, we analyze the
sentiment of peer reviews written by and to a random sample of 500 students. We
then relate sentiment scores and peer feedback ratings to students' language
background. Results show that native English speakers rate feedback less
favorably, while non-native speakers write more positively but receive less
positive sentiment in return. When controlling for sex and age, significant
interactions emerge, suggesting that language background plays a modest but
complex role in shaping peer feedback experiences.

</details>


### [113] [Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents](https://arxiv.org/abs/2507.22925)
*Haoran Sun,Shaoning Zeng*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Long-term memory is one of the key factors influencing the reasoning
capabilities of Large Language Model Agents (LLM Agents). Incorporating a
memory mechanism that effectively integrates past interactions can
significantly enhance decision-making and contextual coherence of LLM Agents.
While recent works have made progress in memory storage and retrieval, such as
encoding memory into dense vectors for similarity-based search or organizing
knowledge in the form of graph, these approaches often fall short in structured
memory organization and efficient retrieval. To address these limitations, we
propose a Hierarchical Memory (H-MEM) architecture for LLM Agents that
organizes and updates memory in a multi-level fashion based on the degree of
semantic abstraction. Each memory vector is embedded with a positional index
encoding pointing to its semantically related sub-memories in the next layer.
During the reasoning phase, an index-based routing mechanism enables efficient,
layer-by-layer retrieval without performing exhaustive similarity computations.
We evaluate our method on five task settings from the LoCoMo dataset.
Experimental results show that our approach consistently outperforms five
baseline methods, demonstrating its effectiveness in long-term dialogue
scenarios.

</details>


### [114] [Multi-Relation Extraction in Entity Pairs using Global Context](https://arxiv.org/abs/2507.22926)
*Nilesh,Atul Gupta,Avinash C Panday*

Main category: cs.CL

TL;DR: 为解决文档级关系抽取中实体关系漂移问题，提出一种新的输入嵌入方法，通过捕捉实体在文档中的全局位置信息来增强上下文理解，实验证明该方法能有效提升关系抽取准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注实体出现的句子，无法捕捉文档的完整上下文，这对于准确的关系抽取至关重要。

Method: 提出了一种新颖的输入嵌入方法，该方法不局限于实体出现的边界，而是通过将实体表示为独立的片段来捕捉实体在整个文档中的位置信息，从而利用全局关系和多句子推理。

Result: 在DocRED、Re-DocRED和REBEL三个基准数据集上进行了测试，实验结果表明该方法能够准确预测文档级实体间的关系。

Conclusion: 该方法在文档级关系抽取中能够准确预测实体间的关系，并具有理论和实践意义，能够提升自然语言处理应用中的关系检测性能。

Abstract: In document-level relation extraction, entities may appear multiple times in
a document, and their relationships can shift from one context to another.
Accurate prediction of the relationship between two entities across an entire
document requires building a global context spanning all relevant sentences.
Previous approaches have focused only on the sentences where entities are
mentioned, which fails to capture the complete document context necessary for
accurate relation extraction. Therefore, this paper introduces a novel input
embedding approach to capture the positions of mentioned entities throughout
the document rather than focusing solely on the span where they appear. The
proposed input encoding approach leverages global relationships and
multi-sentence reasoning by representing entities as standalone segments,
independent of their positions within the document. The performance of the
proposed method has been tested on three benchmark relation extraction
datasets, namely DocRED, Re-DocRED, and REBEL. The experimental results
demonstrated that the proposed method accurately predicts relationships between
entities in a document-level setting. The proposed research also has
theoretical and practical implications. Theoretically, it advances global
context modeling and multi-sentence reasoning in document-level relation
extraction. Practically, it enhances relationship detection, enabling improved
performance in real-world NLP applications requiring comprehensive entity-level
insights and interpretability.

</details>


### [115] [PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation](https://arxiv.org/abs/2507.22927)
*Zhehao Tan,Yihan Jiao,Dan Yang,Lei Liu,Jie Feng,Duolin Sun,Yue Shen,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: 该研究提出了Placeholder-RAG-Benchmark，一个细粒度的基准测试，用于评估LLM在RAG系统中的文档利用能力，发现了LLM在错误弹性和上下文忠实度方面的不足，并提供了一个改进RAG系统的框架。


<details>
  <summary>Details</summary>
Motivation: 目前大多数基准测试侧重于RAG系统的整体性能，很少评估LLM的特定能力，并且缺乏对文档利用的系统化、细粒度评估框架。

Method: 提出了一种创新的基于占位符的方法，将LLM的参数化知识和外部知识的贡献解耦，以更细致地理解LLM在RAG系统中的作用。该基准测试包括多层次过滤能力、组合能力和参考推理等渐进维度。

Result: 实验证明了代表性LLM在RAG系统的生成能力方面存在局限性，特别是在错误弹性和上下文忠实度方面。

Conclusion: 该基准测试提供了一个可复现的框架，用于开发更可靠、更高效的检索增强生成（RAG）系统。实验表明，代表性的大语言模型（LLM）在RAG系统的生成能力方面存在局限性，特别是在错误弹性和上下文忠实度方面。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating external knowledge, where the LLM's ability to generate responses
based on the combination of a given query and retrieved documents is crucial.
However, most benchmarks focus on overall RAG system performance, rarely
assessing LLM-specific capabilities. Current benchmarks emphasize broad aspects
such as noise robustness, but lack a systematic and granular evaluation
framework on document utilization. To this end, we introduce
\textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark,
emphasizing the following progressive dimensions: (1) multi-level filtering
abilities, (2) combination abilities, and (3) reference reasoning. To provide a
more nuanced understanding of LLMs' roles in RAG systems, we formulate an
innovative placeholder-based approach to decouple the contributions of the
LLM's parametric knowledge and the external knowledge. Experiments demonstrate
the limitations of representative LLMs in the RAG system's generation
capabilities, particularly in error resilience and context faithfulness. Our
benchmark provides a reproducible framework for developing more reliable and
efficient RAG systems. Our code is available in
https://github.com/Alipay-Med/PRGB.

</details>


### [116] [How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding](https://arxiv.org/abs/2507.22928)
*Xi Chen,Aske Plaat,Niki van Stein*

Main category: cs.CL

TL;DR: 链式思考（CoT）提示能提高大型语言模型（LLM）在多步任务上的准确性，但其生成的“思考”是否真实反映内部推理过程尚不确定。本文利用稀疏自编码器和激活修补技术，在 Pythia 模型上进行了首次特征级因果研究。结果表明，CoT 提示能在较大模型（2.8B）中诱导更具可解释性的内部结构，提升了答案的准确性和计算的模块化，但对小模型（70M）效果不明显。


<details>
  <summary>Details</summary>
Motivation: 探究链式思考（CoT）提示是否能真实反映大型语言模型（LLM）的多步推理过程。

Method: 本文结合稀疏自编码器和激活修补技术，从 Pythia-70M 和 Pythia-2.8B 模型在处理 GSM8K 数学问题时提取单义特征，以进行 CoT 和无 CoT 提示的因果研究。

Result: 在 2.8B 模型中，将一小部分 CoT 推理特征替换到无 CoT 运行中，显著提高了答案的对数概率，而在 70M 模型中则无效，表明存在明显的规模阈值。CoT 还导致了更大模型中显著更高的激活稀疏性和特征可解释性得分，预示着更模块化的内部计算。例如，模型生成正确答案的置信度从 1.2 提高到 4.3。此外，研究还引入了 patch-曲线和随机特征补丁基线，表明有用的 CoT 信息不仅存在于 top-K 补丁中，而且分布广泛。

Conclusion: CoT 提示可以在高容量 LLM 中诱导更具可解释性的内部结构，验证了其作为结构化提示方法的有效性。

Abstract: Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on
multi-step tasks, yet whether the generated "thoughts" reflect the true
internal reasoning process is unresolved. We present the first feature-level
causal study of CoT faithfulness. Combining sparse autoencoders with activation
patching, we extract monosemantic features from Pythia-70M and Pythia-2.8B
while they tackle GSM8K math problems under CoT and plain (noCoT) prompting.
Swapping a small set of CoT-reasoning features into a noCoT run raises answer
log-probabilities significantly in the 2.8B model, but has no reliable effect
in 70M, revealing a clear scale threshold. CoT also leads to significantly
higher activation sparsity and feature interpretability scores in the larger
model, signalling more modular internal computation. For example, the model's
confidence in generating correct answers improves from 1.2 to 4.3. We introduce
patch-curves and random-feature patching baselines, showing that useful CoT
information is not only present in the top-K patches but widely distributed.
Overall, our results indicate that CoT can induce more interpretable internal
structures in high-capacity LLMs, validating its role as a structured prompting
method.

</details>


### [117] [Enhancing RAG Efficiency with Adaptive Context Compression](https://arxiv.org/abs/2507.22931)
*Shuyu Guo,Zhaochun Ren*

Main category: cs.CL

TL;DR: ACC-RAG 是一种自适应上下文压缩框架，可根据输入复杂性动态调整 RAG 的压缩率，从而在不牺牲准确性的情况下提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）虽然能用外部知识增强大语言模型（LLM），但由于检索到的上下文过长，会产生显著的推理成本。现有的上下文压缩方法采用固定的压缩率，对于简单查询会过度压缩，对于复杂查询则压缩不足。ACC-RAG 旨在解决此问题，通过自适应调整压缩率来优化推理效率，同时不牺牲准确性。

Method: ACC-RAG 框架结合了分层压缩器（用于多粒度嵌入）和上下文选择器，以保留最少但足够的信息，模拟人类的略读行为。

Result: 与固定压缩率方法相比，ACC-RAG 在维基百科和五个问答数据集上的评估结果更优，其推理速度是标准 RAG 的 4 倍以上，同时保持或提高了准确性。

Conclusion: ACC-RAG 通过动态调整压缩率、结合分层压缩器和上下文选择器，在保持或提高准确性的同时，实现了比标准 RAG 快 4 倍以上的推理速度。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with external knowledge but incurs significant inference costs due to lengthy
retrieved contexts. While context compression mitigates this issue, existing
methods apply fixed compression rates, over-compressing simple queries or
under-compressing complex ones. We propose Adaptive Context Compression for RAG
(ACC-RAG), a framework that dynamically adjusts compression rates based on
input complexity, optimizing inference efficiency without sacrificing accuracy.
ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with
a context selector to retain minimal sufficient information, akin to human
skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms
fixed-rate methods and matches/unlocks over 4 times faster inference versus
standard RAG while maintaining or improving accuracy.

</details>


### [118] [FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification](https://arxiv.org/abs/2507.22932)
*Baptiste Lefort,Eric Benhamou,Beatrice Guez,Jean-Jacques Ohana,Ethan Setrouk,Alban Etienne*

Main category: cs.CL

TL;DR: 该研究提出了一种结合大语言模型和深度强化学习的投资组合优化框架，通过整合情感信号和市场数据，实现了优于基准的投资回报。


<details>
  <summary>Details</summary>
Motivation: 整合金融新闻中的情感信号和传统市场指标，以优化投资组合。

Method: 提出了一种新颖的层次化框架，结合了轻量级大语言模型（LLMs）和深度强化学习（DRL），以整合来自金融新闻的情感信号和传统市场指标。该框架采用三层架构：基础RL代理处理混合数据，元代理聚合其决策，超级代理基于市场数据和情感分析合并决策。

Result: 在2018年至2024年的数据上，年化回报率为26%，夏普比率为1.2，优于基准。

Conclusion: 该框架在2018年至2024年的数据上表现优于等权重和标准普尔500基准，实现了26%的年化回报率和1.2的夏普比率。

Abstract: This paper presents a novel hierarchical framework for portfolio
optimization, integrating lightweight Large Language Models (LLMs) with Deep
Reinforcement Learning (DRL) to combine sentiment signals from financial news
with traditional market indicators. Our three-tier architecture employs base RL
agents to process hybrid data, meta-agents to aggregate their decisions, and a
super-agent to merge decisions based on market data and sentiment analysis.
Evaluated on data from 2018 to 2024, after training on 2000-2017, the framework
achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming
equal-weighted and S&P 500 benchmarks. Key contributions include scalable
cross-modal integration, a hierarchical RL structure for enhanced stability,
and open-source reproducibility.

</details>


### [119] [Augmented Vision-Language Models: A Systematic Review](https://arxiv.org/abs/2507.22933)
*Anthony C Davis,Burhan Sadiq,Tianmin Shu,Chien-Ming Huang*

Main category: cs.CL

TL;DR: 现有视觉语言模型在可解释性、信息整合和逻辑推理方面存在不足。神经符号系统通过结合神经网络和外部符号信息系统，可以提高可解释性并简化信息整合。本研究旨在对利用外部符号信息系统改进视觉语言理解的技术进行分类。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在可解释性、信息整合和逻辑推理方面存在局限性，而神经符号系统能够提供可解释性，并能无需大规模重新训练即可整合新信息，因此，利用预训练的视觉语言模型作为核心，并辅以外部符号系统，是实现神经符号集成优势的可行途径。

Method: 通过对现有文献进行系统性分类，旨在梳理和总结通过与外部符号信息系统交互来改进视觉语言理解的技术。

Result: 本综述旨在对通过与外部符号信息系统交互来改进视觉语言理解的技术进行分类。

Conclusion: 未来的研究可以集中在开发更有效的接口和交互协议，以促进神经符号系统与大型预训练视觉语言模型之间的顺畅集成。

Abstract: Recent advances in visual-language machine learning models have demonstrated
exceptional ability to use natural language and understand visual scenes by
training on large, unstructured datasets. However, this training paradigm
cannot produce interpretable explanations for its outputs, requires retraining
to integrate new information, is highly resource-intensive, and struggles with
certain forms of logical reasoning. One promising solution involves integrating
neural networks with external symbolic information systems, forming neural
symbolic systems that can enhance reasoning and memory abilities. These neural
symbolic systems provide more interpretable explanations to their outputs and
the capacity to assimilate new information without extensive retraining.
Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural
component, augmented by external systems, offers a pragmatic approach to
realizing the benefits of neural-symbolic integration. This systematic
literature review aims to categorize techniques through which visual-language
understanding can be improved by interacting with external symbolic information
systems.

</details>


### [120] [OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching](https://arxiv.org/abs/2503.21813)
*Zhangcheng Qiang,Kerry Taylor,Weiqing Wang,Jing Jiang*

Main category: cs.CL

TL;DR: OAEI-LLM-T是一个用于本体匹配的新数据集，旨在解决LLM幻觉问题，可用于构建排行榜和微调模型。


<details>
  <summary>Details</summary>
Motivation: 为了应对LLM本体匹配（OM）系统中幻觉这一重大挑战。

Method: 介绍了OAEI-LLM-T数据集，该数据集源自七个TBox数据集，捕捉了十个不同LLM在本体匹配任务中的幻觉，并将其分为两个主要类别和六个子类别。

Result: 展示了该数据集在构建OM任务的LLM排行榜以及对OM任务中使用的LLM进行微调方面的实用性。

Conclusion: OAEI-LLM-T是一个新基准数据集，可用于构建LLM排行榜和微调LLM用于本体匹配任务。

Abstract: Hallucinations are often inevitable in downstream tasks using large language
models (LLMs). To tackle the substantial challenge of addressing hallucinations
for LLM-based ontology matching (OM) systems, we introduce a new benchmark
dataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the
Ontology Alignment Evaluation Initiative (OAEI), capturing hallucinations of
ten different LLMs performing OM tasks. These OM-specific hallucinations are
organised into two primary categories and six sub-categories. We showcase the
usefulness of the dataset in constructing an LLM leaderboard for OM tasks and
for fine-tuning LLMs used in OM tasks.

</details>


### [121] [Deep Learning Approaches for Multimodal Intent Recognition: A Survey](https://arxiv.org/abs/2507.22934)
*Jingwei Zhao,Yuhua Wen,Qifei Li,Minchi Hu,Yingying Zhou,Jingyao Xue,Junyang Wu,Yingming Gao,Zhengqi Wen,Jianhua Tao,Ya Li*

Main category: cs.CL

TL;DR: 本文调查了深度学习在单一模式和多模式意图识别中的应用，重点介绍了Transformer模型的进展和未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 为了满足日益增长的自然人机交互需求，意图识别领域通过深度学习和多模式方法不断发展，整合了来自音频、视觉和生理信号的数据。

Method: 通过对深度学习方法进行调查，包括从单一模式到多模式技术的转变，相关的数据集、方法、应用和当前的挑战。

Result: Transformer模型在这一领域取得了显著的突破，本文为研究人员提供了多模式意图识别（MIR）的最新进展和未来研究方向的见解。

Conclusion: 本文对深度学习在意图识别中的应用进行了全面的调查，重点介绍了从单一模式到多模式技术的发展。

Abstract: Intent recognition aims to identify users' underlying intentions,
traditionally focusing on text in natural language processing. With growing
demands for natural human-computer interaction, the field has evolved through
deep learning and multimodal approaches, incorporating data from audio, vision,
and physiological signals. Recently, the introduction of Transformer-based
models has led to notable breakthroughs in this domain. This article surveys
deep learning methods for intent recognition, covering the shift from unimodal
to multimodal techniques, relevant datasets, methodologies, applications, and
current challenges. It provides researchers with insights into the latest
developments in multimodal intent recognition (MIR) and directions for future
research.

</details>


### [122] [Trusted Knowledge Extraction for Operations and Maintenance Intelligence](https://arxiv.org/abs/2507.22935)
*Kathleen Mealey,Jonathan A. Karr Jr.,Priscila Saboia Moreira,Paul R. Brenner,Charles F. Vardeman II*

Main category: cs.CL

TL;DR: 本研究评估了NLP和LLM在航空领域运营维护数据分析中的应用，发现在保密环境下存在性能挑战，并为提升工具的可信度和应用提出了建议，同时发布了一个用于基准测试的数据集。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决在航空领域，由于数据保密性与数据集成目标之间的矛盾，以及自然语言处理（NLP）工具在特定领域知识结构（如运营和维护）方面的局限性，从而在组织数据存储库中提取运营情报的关键挑战。

Method: 本研究将知识提取过程分解为命名实体识别、共指消解、命名实体链接和关系提取四个组成部分，并评估了16种NLP工具和大型语言模型（LLM）在这些任务上的表现，特别关注了在受控、保密环境下的零样本性能。

Result: 研究观察到，在受控、保密环境中运行的NLP和LLM工具存在显著的性能限制，并讨论了这些工具在航空等关键任务行业中广泛应用的技术准备度（TRL）以及相关的挑战。

Conclusion: 本研究探讨了在航空领域，利用知识图谱和NLP/LLM技术从运营和维护数据中提取情报的挑战与机遇。研究评估了16种NLP工具和LLM在零样本场景下的表现，并指出了在数据保密性要求下的性能局限性。最终，研究提出了增强可信NLP/LLM工具在航空等关键任务领域的应用建议，并提供了一个开源数据集以支持未来的基准测试和评估。

Abstract: Deriving operational intelligence from organizational data repositories is a
key challenge due to the dichotomy of data confidentiality vs data integration
objectives, as well as the limitations of Natural Language Processing (NLP)
tools relative to the specific knowledge structure of domains such as
operations and maintenance. In this work, we discuss Knowledge Graph
construction and break down the Knowledge Extraction process into its Named
Entity Recognition, Coreference Resolution, Named Entity Linking, and Relation
Extraction functional components. We then evaluate sixteen NLP tools in concert
with or in comparison to the rapidly advancing capabilities of Large Language
Models (LLMs). We focus on the operational and maintenance intelligence use
case for trusted applications in the aircraft industry. A baseline dataset is
derived from a rich public domain US Federal Aviation Administration dataset
focused on equipment failures or maintenance requirements. We assess the
zero-shot performance of NLP and LLM tools that can be operated within a
controlled, confidential environment (no data is sent to third parties). Based
on our observation of significant performance limitations, we discuss the
challenges related to trusted NLP and LLM tools as well as their Technical
Readiness Level for wider use in mission-critical industries such as aviation.
We conclude with recommendations to enhance trust and provide our open-source
curated dataset to support further baseline testing and evaluation.

</details>


### [123] [Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis](https://arxiv.org/abs/2507.22936)
*Md Talha Mohsin*

Main category: cs.CL

TL;DR: 本研究比较了五种大语言模型（GPT、Claude、Perplexity、Gemini、DeepSeek）在金融文本处理上的表现。结果显示，GPT表现最好，而Gemini和DeepSeek表现不稳定。模型性能受提示和数据影响。


<details>
  <summary>Details</summary>
Motivation: 鉴于大语言模型在金融分析领域的快速发展和日益增长的影响力，本研究旨在系统性地比较和评估五种主流大语言模型在金融自然语言处理任务上的表现，以填补现有研究中对LLMs进行系统性比较的不足。

Method: 本研究使用领域特定的提示，并结合人类标注、自动词汇语义指标（ROUGE、余弦相似度、Jaccard）和模型行为诊断（提示级别方差和跨模型相似性）这三种方法，对GPT、Claude、Perplexity、Gemini和DeepSeek这五种主流大语言模型在“Magnificent Seven”科技公司的10-K文件中进行了比较评估。

Result: GPT在各项评估指标上均表现最佳，其输出结果最连贯、语义对齐且与上下文相关。Claude和Perplexity表现次之。Gemini和DeepSeek在输出结果上表现出更大的变异性，且模型间的一致性较低。此外，研究还发现，输出结果的相似性和稳定性会随着公司和时间的变化而变化，这表明模型性能对提示的编写方式和所使用的源材料非常敏感。

Conclusion: GPT在金融文本处理任务中表现出最佳的一致性、语义对齐和上下文相关性，其次是Claude和Perplexity。Gemini和DeepSeek的表现则较为不稳定，输出结果的一致性和稳定性也因公司和时间而异，表明它们对提示的编写和源材料的敏感性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide variety of Financial Natural Language Processing (FinNLP) tasks.
However, systematic comparisons among widely used LLMs remain underexplored.
Given the rapid advancement and growing influence of LLMs in financial
analysis, this study conducts a thorough comparative evaluation of five leading
LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the
'Magnificent Seven' technology companies. We create a set of domain-specific
prompts and then use three methodologies to evaluate model performance: human
annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,
Jaccard), and model behavior diagnostics (prompt-level variance and
across-model similarity). The results show that GPT gives the most coherent,
semantically aligned, and contextually relevant answers; followed by Claude and
Perplexity. Gemini and DeepSeek, on the other hand, have more variability and
less agreement. Also, the similarity and stability of outputs change from
company to company and over time, showing that they are sensitive to how
prompts are written and what source material is used.

</details>


### [124] [CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering](https://arxiv.org/abs/2507.22937)
*Jinkun Zhao,Yuanshuai Wang,Xingjian Zhang,Ruibo Chen,Xingchuang Liao,Junle Wang,Lei Huang,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: 本研究提出了 CoE-Ops 框架，通过结合大语言模型和检索增强生成，提高了 AIOps 任务处理的准确性和效率，并在实验中取得了显著优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决 AIOps 领域中单一模型受领域知识限制、难以处理多种任务的挑战，并借鉴了集成学习和 LLM 训练的经验，提出了一种结合多个模型以实现更高效结果的框架。

Method: 提出了一种协作专家框架（CoE-Ops），该框架结合了一个通用的大语言模型任务分类器，并引入了检索增强生成机制，以提高其处理高级（代码、构建、测试等）和低级（故障分析、异常检测等）问题回答任务的能力。

Result: 实验结果表明，CoE-Ops 在路由准确率方面比现有 CoE 方法提高了 72%，在 DevOps 问题解决方面比单一 AIOps 模型提高了 8%，并且在准确率上超越了更大规模的 MoE 模型 14%。

Conclusion: CoE-Ops 在 AIOps 领域通过引入通用大语言模型任务分类器、检索增强生成机制，并在 DevOps-EVAL 数据集上进行了广泛的实验，相比现有 CoE 方法路由准确率提高了 72%，在 DevOps 问题解决方面比单一 AIOps 模型准确率提升高达 8%，并且在准确率上以高达 14% 的优势超越了更大规模的 MoE 模型。

Abstract: With the rapid evolution of artificial intelligence, AIOps has emerged as a
prominent paradigm in DevOps. Lots of work has been proposed to improve the
performance of different AIOps phases. However, constrained by domain-specific
knowledge, a single model can only handle the operation requirement of a
specific task,such as log parser,root cause analysis. Meanwhile, combining
multiple models can achieve more efficient results, which have been proved in
both previous ensemble learning and the recent LLM training domain. Inspired by
these works,to address the similar challenges in AIOPS, this paper first
proposes a collaboration-of-expert framework(CoE-Ops) incorporating a
general-purpose large language model task classifier. A retrieval-augmented
generation mechanism is introduced to improve the framework's capability in
handling both Question-Answering tasks with high-level(Code,build,Test,etc.)
and low-level(fault analysis,anomaly detection,etc.). Finally, the proposed
method is implemented in the AIOps domain, and extensive experiments are
conducted on the DevOps-EVAL dataset. Experimental results demonstrate that
CoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps
tasks compared to existing CoE methods, delivers up to 8% accuracy enhancement
over single AIOps models in DevOps problem resolution, and outperforms
larger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.

</details>


### [125] [A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents](https://arxiv.org/abs/2507.22938)
*Sumit Soman,H. G. Ranjani,Sujoy Roychowdhury,Venkata Dharma Surya Narayana Sastry,Akshat Jain,Pranav Gangrade,Ayaaz Khan*

Main category: cs.CL

TL;DR: 本研究提出了一种新的问答方法，通过将流程图的图表示（来自VLM）集成到基于文本的RAG系统中，以解决技术文档中涉及图像的问题。该方法在电信领域进行了基准测试，证明了其鲁棒性和良好的检索性能，并降低了推理成本。


<details>
  <summary>Details</summary>
Motivation: 技术文档中的问答任务常常需要回答与图表（如图表或流程图）相关的问题，而传统的基于文本的检索增强生成（RAG）系统在处理这类问题时可能效果不佳。因此，本研究旨在解决这一挑战，通过结合图像信息来改进问答系统的性能。

Method: 该研究提出了一种端到端的问答方法，该方法涉及处理技术文档、分类图像类型、构建图表示，并将这些图表示与文本嵌入管道结合，以实现高效检索。该方法利用视觉大型语言模型（VLM）从流程图中提取图表示，并将其整合到基于文本的检索增强生成（RAG）系统中。

Result: 实验结果表明，使用微调的VLM模型获得的图表示与真实值相比具有较低的编辑距离，这证明了这些表示对于流程图图像的鲁棒性。此外，利用这些图表示进行的问答在检索性能方面表现良好，即使在使用针对电信领域进行适应的文本嵌入模型时也是如此。该方法还避免了在推理过程中使用VLM，从而降低了部署问答系统的成本。

Conclusion: 该方法通过利用视觉大型语言模型（VLM）生成的流程图图表示，并将其集成到基于文本的检索增强生成（RAG）系统中，实现了电信领域中针对图像的问答。结果表明，使用微调的VLM模型获得的图表示具有较低的编辑距离，证明了其对流程图图像的鲁棒性。此外，使用这些表示进行的问答方法在使用基于文本的嵌入模型（包括针对电信领域调整的模型）时表现出良好的检索性能。该方法还减轻了推理过程中对VLM的需求，为部署的问答系统带来了重要的成本效益。

Abstract: Question-Answering (QA) from technical documents often involves questions
whose answers are present in figures, such as flowcharts or flow diagrams.
Text-based Retrieval Augmented Generation (RAG) systems may fail to answer such
questions. We leverage graph representations of flowcharts obtained from Visual
large Language Models (VLMs) and incorporate them in a text-based RAG system to
show that this approach can enable image retrieval for QA in the telecom
domain. We present the end-to-end approach from processing technical documents,
classifying image types, building graph representations, and incorporating them
with the text embedding pipeline for efficient retrieval. We benchmark the same
on a QA dataset created based on proprietary telecom product information
documents. Results show that the graph representations obtained using a
fine-tuned VLM model have lower edit distance with respect to the ground truth,
which illustrate the robustness of these representations for flowchart images.
Further, the approach for QA using these representations gives good retrieval
performance using text-based embedding models, including a telecom-domain
adapted one. Our approach also alleviates the need for a VLM in inference,
which is an important cost benefit for deployed QA systems.

</details>


### [126] [PARROT: An Open Multilingual Radiology Reports Dataset](https://arxiv.org/abs/2507.22939)
*Bastien Le Guellec,Kokou Adambounou,Lisa C Adams,Thibault Agripnidis,Sung Soo Ahn,Radhia Ait Chalal,Tugba Akinci D Antonoli,Philippe Amouyel,Henrik Andersson,Raphael Bentegeac,Claudio Benzoni,Antonino Andrea Blandino,Felix Busch,Elif Can,Riccardo Cau,Armando Ugo Cavallo,Christelle Chavihot,Erwin Chiquete,Renato Cuocolo,Eugen Divjak,Gordana Ivanac,Barbara Dziadkowiec Macek,Armel Elogne,Salvatore Claudio Fanni,Carlos Ferrarotti,Claudia Fossataro,Federica Fossataro,Katarzyna Fulek,Michal Fulek,Pawel Gac,Martyna Gachowska,Ignacio Garcia Juarez,Marco Gatti,Natalia Gorelik,Alexia Maria Goulianou,Aghiles Hamroun,Nicolas Herinirina,Krzysztof Kraik,Dominik Krupka,Quentin Holay,Felipe Kitamura,Michail E Klontzas,Anna Kompanowska,Rafal Kompanowski,Alexandre Lefevre,Tristan Lemke,Maximilian Lindholz,Lukas Muller,Piotr Macek,Marcus Makowski,Luigi Mannacio,Aymen Meddeb,Antonio Natale,Beatrice Nguema Edzang,Adriana Ojeda,Yae Won Park,Federica Piccione,Andrea Ponsiglione,Malgorzata Poreba,Rafal Poreba,Philipp Prucker,Jean Pierre Pruvo,Rosa Alba Pugliesi,Feno Hasina Rabemanorintsoa,Vasileios Rafailidis,Katarzyna Resler,Jan Rotkegel,Luca Saba,Ezann Siebert,Arnaldo Stanzione,Ali Fuat Tekin,Liz Toapanta Yanchapaxi,Matthaios Triantafyllou,Ekaterini Tsaoulia,Evangelia Vassalou,Federica Vernuccio,Johan Wasselius,Weilang Wang,Szymon Urban,Adrian Wlodarczak,Szymon Wlodarczak,Andrzej Wysocki,Lina Xu,Tomasz Zatonski,Shuhang Zhang,Sebastian Ziegelmayer,Gregory Kuchcinski,Keno K Bressem*

Main category: cs.CL

TL;DR: PARROT是一个包含2658份多语言放射学报告的数据集，旨在帮助开发和测试自然语言处理应用程序。研究发现，人类（尤其是放射科医生）在区分AI生成的报告方面比随机猜测稍好。


<details>
  <summary>Details</summary>
Motivation: 开发和验证PARROT（Polyglottal Annotated Radiology Reports for Open Testing），这是一个大型、多中心、开放访问的虚构放射学报告数据集，涵盖多种语言，用于测试放射学中的自然语言处理应用程序。

Method: 从2024年5月至9月，邀请放射科医生按照其标准报告实践贡献虚构的放射学报告。研究人员提供了至少20份报告及其相关的元数据，包括解剖区域、成像方式、临床背景，以及非英语报告的英语翻译。所有报告均分配了ICD-10编码。此外，研究进行了一项人类与人工智能报告鉴别研究，有154名参与者（放射科医生、医疗保健专业人员和非医疗保健专业人员）评估报告是人类撰写还是人工智能生成。

Result: 该数据集包含来自21个国家76位作者的2,658份放射学报告，涵盖13种语言。报告涵盖多种成像方式（CT：36.1%，MRI：22.8%，X光：19.0%，超声：16.8%）和解剖区域，其中胸部（19.9%）、腹部（18.6%）、头部（17.3%）和骨盆（14.1%）最为常见。在鉴别研究中，参与者区分人类和人工智能生成报告的准确率为53.9%（95%置信区间：50.7%-57.1%），其中放射科医生表现明显优于其他组（56.9%，95%置信区间：53.3%-60.6%，p<0.05）。

Conclusion: PARROT是最大的开放式多语言放射学报告数据集，能够跨越语言、地理和临床界限，在没有隐私限制的情况下开发和验证自然语言处理应用程序。

Abstract: Rationale and Objectives: To develop and validate PARROT (Polyglottal
Annotated Radiology Reports for Open Testing), a large, multicentric,
open-access dataset of fictional radiology reports spanning multiple languages
for testing natural language processing applications in radiology. Materials
and Methods: From May to September 2024, radiologists were invited to
contribute fictional radiology reports following their standard reporting
practices. Contributors provided at least 20 reports with associated metadata
including anatomical region, imaging modality, clinical context, and for
non-English reports, English translations. All reports were assigned ICD-10
codes. A human vs. AI report differentiation study was conducted with 154
participants (radiologists, healthcare professionals, and non-healthcare
professionals) assessing whether reports were human-authored or AI-generated.
Results: The dataset comprises 2,658 radiology reports from 76 authors across
21 countries and 13 languages. Reports cover multiple imaging modalities (CT:
36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical
regions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%)
being most prevalent. In the differentiation study, participants achieved 53.9%
accuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated
reports, with radiologists performing significantly better (56.9%, 95% CI:
53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the
largest open multilingual radiology report dataset, enabling development and
validation of natural language processing applications across linguistic,
geographic, and clinical boundaries without privacy constraints.

</details>


### [127] [Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes](https://arxiv.org/abs/2507.22940)
*Rui Jiao,Yue Zhang,Jinku Li*

Main category: cs.CL

TL;DR: RELIANCE框架通过专门的事实核查、多维度奖励的强化学习和机制可解释性，显著提高了大型语言模型（LLMs）推理过程中的事实准确性，解决了即使最终答案正确也可能存在的中间步骤事实错误问题。实验表明，该框架可大幅提升模型的事实鲁棒性，并为未来训练方法提供指导。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在推理过程的中间步骤中存在事实错误，即使最终答案正确，这种现象在高风险领域（如医疗、法律和科研）可能导致危险的误导。

Method: RELIANCE框架整合了三个核心组件：1. 专用的事实核查分类器，用于检测推理链中的细微事实不一致性；2. 结合了事实性、连贯性和结构正确性的多维度奖励的群体相对策略优化（GRPO）强化学习方法；3. 检验事实性改进如何在模型激活中显现的机制可解释性模块。

Result: 在十种最先进模型上的广泛评估显示，即使是Claude-3.7和GPT-o1等领先模型，其推理事实准确率也仅分别为81.93%和82.57%。RELIANCE框架显著提高了事实鲁棒性（最高可提升49.90%），同时在Math-500、AIME-2024和GPQA等基准测试中保持或提高了性能。激活层面的分析揭示了事实性改进如何重塑模型内部的推理轨迹，为未来靶向训练提供了基础。

Conclusion: RELIANCE框架通过整合事实核查、多维度奖励的强化学习以及机制可解释性模块，显著提高了大型语言模型（LLMs）在推理过程中的事实准确性，即使在面对复杂和高风险领域也能有效减少错误信息的传播。该框架为未来训练更具事实鲁棒性的模型提供了基础，并通过激活层面的分析提供了可操作的见解。

Abstract: We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy
for Confidence Enhancement), a novel framework addressing a critical
vulnerability in Large Language Models (LLMs): the prevalence of factual
inaccuracies within intermediate reasoning steps despite correct final answers.
This phenomenon poses substantial risks in high-stakes domains including
healthcare, legal analysis, and scientific research, where erroneous yet
confidently presented reasoning can mislead users into dangerous decisions. Our
framework integrates three core components: (1) a specialized fact-checking
classifier trained on counterfactually augmented data to detect subtle factual
inconsistencies within reasoning chains; (2) a Group Relative Policy
Optimization (GRPO) reinforcement learning approach that balances factuality,
coherence, and structural correctness through multi-dimensional rewards; and
(3) a mechanistic interpretability module examining how factuality improvements
manifest in model activations during reasoning processes. Extensive evaluation
across ten state-of-the-art models reveals concerning patterns: even leading
models like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of
only 81.93% and 82.57% respectively. RELIANCE significantly enhances factual
robustness (up to 49.90% improvement) while maintaining or improving
performance on challenging benchmarks including Math-500, AIME-2024, and GPQA.
Furthermore, our activation-level analysis provides actionable insights into
how factual enhancements reshape reasoning trajectories within model
architectures, establishing foundations for future training methodologies that
explicitly target factual robustness through activation-guided optimization.

</details>


### [128] [SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology](https://arxiv.org/abs/2507.22941)
*Paul Minchella,Loïc Verlingue,Stéphane Chrétien,Rémi Vaucher,Guillaume Metzler*

Main category: cs.CL

TL;DR: SigBERT是一个创新的时间生存分析框架，可以处理大量的临床报告，通过使用粗糙路径理论的签名来捕获时间动态，从而提高风险估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的生存分析方法在有效处理文本数据（尤其是其顺序形式）的复杂性方面存在困难。

Method: SigBERT通过提取和平均词嵌入到句子嵌入来处理带时间戳的医疗报告。通过从粗糙路径理论中提取签名来捕获句子嵌入坐标时间序列中的时间动态，并获取每个患者的几何特征。将这些特征整合到LASSO惩罚的Cox模型中，以估计患者特定的风险评分。

Result: 在来自L'eon B'erard中心的真实世界肿瘤学数据集上训练和评估的模型，在独立测试队列中的C指数得分为0.75（标准差0.014）。

Conclusion: SigBERT通过整合序列医疗数据来增强风险估计，推动了基于叙述的生存分析。

Abstract: Electronic medical reports (EHR) contain a vast amount of information that
can be leveraged for machine learning applications in healthcare. However,
existing survival analysis methods often struggle to effectively handle the
complexity of textual data, particularly in its sequential form. Here, we
propose SigBERT, an innovative temporal survival analysis framework designed to
efficiently process a large number of clinical reports per patient. SigBERT
processes timestamped medical reports by extracting and averaging word
embeddings into sentence embeddings. To capture temporal dynamics from the time
series of sentence embedding coordinates, we apply signature extraction from
rough path theory to derive geometric features for each patient, which
significantly enhance survival model performance by capturing complex temporal
dynamics. These features are then integrated into a LASSO-penalized Cox model
to estimate patient-specific risk scores. The model was trained and evaluated
on a real-world oncology dataset from the L\'eon B\'erard Center corpus, with a
C-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT
integrates sequential medical data to enhance risk estimation, advancing
narrative-based survival analysis.

</details>


### [129] [A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies](https://arxiv.org/abs/2507.22943)
*Shirley V Wang,Georg Hahn,Sushama Kattinakere Sreedhara,Mufaddal Mahesri,Haritha S. Pillai,Rajendra Aldis,Joyce Lii,Sarah K. Dutcher,Rhoda Eniafe,Jamal T. Jones,Keewan Kim,Jiwei He,Hana Lee,Sengwee Toh,Rishi J Desai,Jie Yang*

Main category: cs.CL

TL;DR: 通过 NLP 和多轮自适应抽样，加速了对健康结果算法的验证过程，大大减少了审查时间和工作量。


<details>
  <summary>Details</summary>
Motivation: 为了增强使用大型声明数据库进行的分析，可以验证用于识别健康结果或其他主要研究参数的代码算法的测量特征。

Method: 描述了一个通过两种不同机制提高验证研究效率的流程：1) 使用自然语言处理（NLP）减少人工审查员审查每份图表所需的时间，以及 2) 采用具有预定义标准的、多轮自适应抽样方法，一旦以足够的精度识别出性能特征即可停止验证研究。

Result: 我们经验性地证明，NLP辅助标注过程将每份图表的审查时间缩短了 40%，并且采用具有预定义停止规则的多轮样本，将在不显著影响精度的情况下，避免审查 77% 的患者图表，从而获得测量特征。

Conclusion: 该方法可以促进对用于定义关键研究参数的代码算法进行更常规的验证，最终增强对数据库研究所得结果可靠性的理解。

Abstract: Background: One of the ways to enhance analyses conducted with large claims
databases is by validating the measurement characteristics of code-based
algorithms used to identify health outcomes or other key study parameters of
interest. These metrics can be used in quantitative bias analyses to assess the
robustness of results for an inferential study given potential bias from
outcome misclassification. However, extensive time and resource allocation are
typically re-quired to create reference-standard labels through manual chart
review of free-text notes from linked electronic health records. Methods: We
describe an expedited process that introduces efficiency in a validation study
us-ing two distinct mechanisms: 1) use of natural language processing (NLP) to
reduce time spent by human reviewers to review each chart, and 2) a multi-wave
adaptive sampling approach with pre-defined criteria to stop the validation
study once performance characteristics are identified with sufficient
precision. We illustrate this process in a case study that validates the
performance of a claims-based outcome algorithm for intentional self-harm in
patients with obesity. Results: We empirically demonstrate that the
NLP-assisted annotation process reduced the time spent on review per chart by
40% and use of the pre-defined stopping rule with multi-wave samples would have
prevented review of 77% of patient charts with limited compromise to precision
in derived measurement characteristics. Conclusion: This approach could
facilitate more routine validation of code-based algorithms used to define key
study parameters, ultimately enhancing understanding of the reliability of
find-ings derived from database studies.

</details>


### [130] [Opacity as Authority: Arbitrariness and the Preclusion of Contestation](https://arxiv.org/abs/2507.22944)
*Naomi Omeonga wa Kayembe*

Main category: cs.CL

TL;DR: 本篇论文将任意性定义为一种功能性的符号学特征，而非规范性缺陷，并将其应用于人类系统（如法律和社会动态）。研究提出了“动机→可查证性→可争议性”链，并利用信息论将任意性形式化，认为任意性是控制和关怀的关键机制，对人工智能的可解释性也有指导意义。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在将任意性从规范性缺陷或统治的症状重新定义为一种结构性功能机制，并区分任意性与不公正，认为任意性是一种使系统能够有效运作但隐藏其内部原理的符号学特征。

Method: 本研究将任意性视为一种符号学特征，并将其应用于语言、法律和社会动态等领域。通过引入“动机→可查证性→可争议性”链，论证了动机在使行为逻辑易受主观争议方面的作用。研究还运用香农熵模型将任意性形式化为A=H(L|M)，并提出了任意性的现代理论，将其作为控制和关怀的核心中立操作员。

Result: 研究结果表明，当“动机→可查证性→可争议性”链被“非动机化”或“冲突横向化”等机制打破时，行为会产生约束力而不暴露其原理，从而阻碍了可诉性。这种结构不透明性是一种故意设计，旨在保护权威免于问责。

Conclusion: 该研究将任意性重新定义为一种结构性功能机制，适用于人类系统和交互，并提出了一个包含“动机→可查证性→可争议性”链的新理论，该理论阐述了任意性如何影响问责制。此外，研究还将任意性形式化为条件熵A=H(L|M)，并认为任意性是控制和关怀的核心操作员，同时该框架也为分析人工智能系统的可解释性提供了新途径。

Abstract: This article redefines arbitrariness not as a normative flaw or a symptom of
domination, but as a foundational functional mechanism structuring human
systems and interactions. Diverging from critical traditions that conflate
arbitrariness with injustice, it posits arbitrariness as a semiotic trait: a
property enabling systems - linguistic, legal, or social - to operate
effectively while withholding their internal rationale. Building on Ferdinand
de Saussure's concept of l'arbitraire du signe, the analysis extends this
principle beyond language to demonstrate its cross-domain applicability,
particularly in law and social dynamics. The paper introduces the "Motivation
-> Constatability -> Contestability" chain, arguing that motivation functions
as a crucial interface rendering an act's logic vulnerable to intersubjective
contestation. When this chain is broken through mechanisms like
"immotivization" or "Conflict Lateralization" (exemplified by "the blur of the
wolf drowned in the fish"), acts produce binding effects without exposing their
rationale, thus precluding justiciability. This structural opacity, while
appearing illogical, is a deliberate design protecting authority from
accountability. Drawing on Shannon's entropy model, the paper formalizes
arbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern
theory of arbitrariness as a neutral operator central to control as well as
care, an overlooked dimension of interpersonal relations. While primarily
developed through human social systems, this framework also illuminates a new
pathway for analyzing explainability in advanced artificial intelligence
systems.

</details>


### [131] [C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations](https://arxiv.org/abs/2507.22968)
*Chengqian Ma,Wei Tao,Yiwen Guo*

Main category: cs.CL

TL;DR: 本研究提出了一个包含1079个中英对话实例的基准数据集和一套基于LLM的评估方法，用于评估语音对话模型在理解和模仿人类对话方面的能力，解决了口语对话中的歧义和语境依赖性等挑战。


<details>
  <summary>Details</summary>
Motivation: 随着语音对话模型（SDM）的普及，理解其在理解和模仿人类对话方面的实际效果变得至关重要。然而，与文本型语言模型（LLM）相比，SDM在基准测试方面仍有不足。口语对话因其固有的歧义性和语境依赖性而比文本更复杂。

Method: 本研究提出了一种基于LLM的评估方法，旨在模拟人类的判断，并构建了一个包含1079个英语和中文实例的基准数据集，用于评估口语对话模型。

Result: 该数据集和评估方法为全面评估SDM处理口语对话中的歧义（如多义词、同形异音词、重音模式）和语境依赖性（如省略、指代、多轮交互）等实际挑战提供了工具。

Conclusion: 本论文提出了一个包含1079个中英双语对话实例的基准数据集，并结合了与人类判断高度一致的基于LLM的评估方法，以全面探索SDM在应对口语对话中的实际挑战方面的性能。

Abstract: Spoken Dialogue Models (SDMs) have recently attracted significant attention
for their ability to generate voice responses directly to users' spoken
queries. Despite their increasing popularity, there exists a gap in research
focused on comprehensively understanding their practical effectiveness in
comprehending and emulating human conversations. This is especially true
compared to text-based Large Language Models (LLMs), which benefit from
extensive benchmarking. Human voice interactions are inherently more complex
than text due to characteristics unique to spoken dialogue. Ambiguity poses one
challenge, stemming from semantic factors like polysemy, as well as
phonological aspects such as heterograph, heteronyms, and stress patterns.
Additionally, context-dependency, like omission, coreference, and multi-turn
interaction, adds further complexity to human conversational dynamics. To
illuminate the current state of SDM development and to address these
challenges, we present a benchmark dataset in this paper, which comprises 1,079
instances in English and Chinese. Accompanied by an LLM-based evaluation method
that closely aligns with human judgment, this dataset facilitates a
comprehensive exploration of the performance of SDMs in tackling these
practical challenges.

</details>


### [132] [Math Natural Language Inference: this should be easy!](https://arxiv.org/abs/2507.23063)
*Valeria de Paiva,Qiyue Gao,Hai Hu,Pavel Kovalev,Yikang Liu,Lawrence S. Moss,Zhiheng Qian*

Main category: cs.CL

TL;DR: LLM在数学推理方面仍有不足，但多数投票或可媲美人类水平。


<details>
  <summary>Details</summary>
Motivation: 探究当前的大型语言模型（LLMs）是否能够处理数学文本的自然语言推理（NLI）任务，即“Math NLI问题”。

Method: 构建了一个包含数学文本作为前提、人工标注或由LLM生成的假设及其标签的Math NLI数据集，并评估了不同LLM在这些数据上的表现和组间一致性。

Result: LLM在处理数学语言时仍有困难，偶尔会在基本推理上失败。但目前的模型在处理仅基于假设的“推理”方面，不像上一代模型那样容易出错。LLM的多数投票在某些情况下可以媲美人类标注数据。

Conclusion: LLM在数学文本上的自然语言推理能力有限，有时甚至在基本推理上都会出错。然而，在某些情况下，LLM的多数投票结果可以接近于人类标注数据的表现。

Abstract: We ask whether contemporary LLMs are able to perform natural language
inference (NLI) tasks on mathematical texts. We call this the Math NLI problem.
We construct a corpus of Math NLI pairs whose premises are from extant
mathematical text and whose hypotheses and gold labels were provided by people
with experience in both research-level mathematics and also in the NLI field.
We also investigate the quality of corpora using the same premises but whose
hypotheses are provided by LLMs themselves. We not only investigate the
performance but also the inter-group consistency of the diverse group of LLMs.
We have both positive and negative findings. Among our positive findings: in
some settings, using a majority vote of LLMs is approximately equivalent to
using human-labeled data in the Math NLI area. On the negative side: LLMs still
struggle with mathematical language. They occasionally fail at even basic
inferences. Current models are not as prone to hypothesis-only "inference" in
our data the way the previous generation had been. In addition to our findings,
we also provide our corpora as data to support future work on Math NLI.

</details>


### [133] [Exploring In-Context Learning for Frame-Semantic Parsing](https://arxiv.org/abs/2507.23082)
*Diego Garat,Guillermo Moncecchi,Dina Wonsever*

Main category: cs.CL

TL;DR: This paper explores using In-Context Learning (ICL) with Large Language Models (LLMs) for Frame Semantic Parsing (FSP) without fine-tuning. It proposes an automatic prompt generation method using the FrameNet database, achieving strong results on FSP subtasks related to violent events, suggesting ICL is a viable alternative to fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Investigate the use of In-Context Learning (ICL) with Large Language Models (LLMs) to perform Frame Semantic Parsing (FSP) without model fine-tuning.

Method: This paper proposes a method that automatically generates task-specific prompts for Frame Identification (FI) and Frame Semantic Role Labeling (FSRL) subtasks using only the FrameNet database. These prompts, derived from frame definitions and annotated examples, are used to guide six different LLMs. Experiments were conducted on a subset of frames related to violent events.

Result: The method achieves competitive results, with F1 scores of 94.3% for FI and 77.4% for FSRL on a subset of frames related to violent events.

Conclusion: In-context learning (ICL) offers a practical and effective alternative to traditional fine-tuning for domain-specific frame semantic parsing (FSP) tasks.

Abstract: Frame Semantic Parsing (FSP) entails identifying predicates and labeling
their arguments according to Frame Semantics. This paper investigates the use
of In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP
without model fine-tuning. We propose a method that automatically generates
task-specific prompts for the Frame Identification (FI) and Frame Semantic Role
Labeling (FSRL) subtasks, relying solely on the FrameNet database. These
prompts, constructed from frame definitions and annotated examples, are used to
guide six different LLMs. Experiments are conducted on a subset of frames
related to violent events. The method achieves competitive results, with F1
scores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers
a practical and effective alternative to traditional fine-tuning for
domain-specific FSP tasks.

</details>


### [134] [Context-aware Rotary Position Embedding](https://arxiv.org/abs/2507.23083)
*Ali Veisi,Delaram Fartoot,Hamidreza Amirzadeh*

Main category: cs.CL

TL;DR: CARoPE是一种新的位置编码方法，通过动态生成特定于头的频率模式来提高Transformer模型的性能和训练效率，克服了RoPE的局限性。


<details>
  <summary>Details</summary>
Motivation: 标准RoPE依赖于静态的、与输入无关的正弦频率模式，这限制了其对上下文敏感关系建模的能力。

Method: CARoPE通过使用基于令牌嵌入的注意力头来动态生成特定于头的频率模式，并将它们集成到旋转机制中，从而计算输入相关的相位偏移。

Result: 在FineWeb-Edu-10B数据集上的实验表明，CARoPE在下一个令牌预测任务上始终优于RoPE和其他位置编码基线，在更长的上下文长度下具有更低的困惑度，并实现了更快的训练吞吐量。

Conclusion: CARoPE通过提供一种可扩展、具有表现力且高效的位置编码升级，在Transformer模型中超越了RoPE和其他常见基线，在保持模型稳定性的同时实现了更快的训练吞吐量。

Abstract: Positional encoding is a vital component of Transformer architectures,
enabling models to incorporate sequence order into self-attention mechanisms.
Rotary Positional Embeddings (RoPE) have become a widely adopted solution due
to their compatibility with relative position encoding and computational
efficiency. However, RoPE relies on static, input-independent sinusoidal
frequency patterns, limiting its ability to model context-sensitive
relationships. In this work, we propose CARoPE (Context-Aware Rotary Positional
Embedding), a novel generalization of RoPE that dynamically generates
head-specific frequency patterns conditioned on token embeddings. This design
introduces token- and context-sensitive positional representations while
preserving RoPE efficiency and architectural simplicity. CARoPE computes
input-dependent phase shifts using a bounded transformation of token embeddings
and integrates them into the rotary mechanism across attention heads. We
evaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on
next-token prediction tasks. Experimental results show that CARoPE consistently
outperforms RoPE and other common positional encoding baselines, achieving
significantly lower perplexity, even at longer context lengths. Additionally,
CARoPE enables faster training throughput without sacrificing model stability.
These findings demonstrate that CARoPE offers a scalable, expressive, and
efficient upgrade to existing positional encoding strategies in Transformer
models.

</details>


### [135] [SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity](https://arxiv.org/abs/2507.23095)
*Ishani Mondal,Meera Bharadwaj,Ayush Roy,Aparna Garimella,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: SMART-Editor 是一个用于组合式布局和内容编辑的框架，通过创新的奖励引导策略（Reward-Refine 和 RewardDPO）实现了全局一致性，并在多领域基准测试中取得了优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 为了实现跨结构化（海报、网站）和非结构化（自然图像）领域的组合式布局和内容编辑，并克服现有模型仅进行局部编辑而牺牲全局一致性的问题。

Method: 提出了一种名为 SMART-Editor 的框架，并结合了两种策略：Reward-Refine（一种推理时奖励引导的细化方法）和 RewardDPO（一种训练时使用奖励对齐布局对的偏好优化方法）。此外，还引入了 SMARTEdit-Bench 这一跨多领域、级联编辑场景的基准测试集。

Result: SMART-Editor 框架在 SMARTEdit-Bench 基准测试中表现优于 InstructPix2Pix 和 HIVE 等强基线模型。其中，RewardDPO 在结构化设置下带来了高达 15% 的性能提升，而 Reward-Refine 在自然图像编辑方面显示出优势。自动和人工评估均证实了奖励引导规划在生成语义一致且视觉对齐的编辑内容方面的价值。

Conclusion: SMART-Editor 框架通过 Reward-Refine 和 RewardDPO 策略，在结构化和非结构化领域实现了保持全局一致性的组合式布局和内容编辑，并在 SMARTEdit-Bench 基准测试中超越了现有模型。

Abstract: We present SMART-Editor, a framework for compositional layout and content
editing across structured (posters, websites) and unstructured (natural images)
domains. Unlike prior models that perform local edits, SMART-Editor preserves
global coherence through two strategies: Reward-Refine, an inference-time
rewardguided refinement method, and RewardDPO, a training-time preference
optimization approach using reward-aligned layout pairs. To evaluate model
performance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain,
cascading edit scenarios. SMART-Editor outperforms strong baselines like
InstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in
structured settings and Reward-Refine showing advantages on natural images.
Automatic and human evaluations confirm the value of reward-guided planning in
producing semantically consistent and visually aligned edits.

</details>


### [136] [RASL: Retrieval Augmented Schema Linking for Massive Database Text-to-SQL](https://arxiv.org/abs/2507.23104)
*Jeffrey Eben,Aitzaz Ahmad,Stephen Lau*

Main category: cs.CL

TL;DR: 提出了一种新的基于组件的检索架构，用于 LLM 数据库接口，解决了扩展性和元数据利用方面的挑战，并在大规模数据库上取得了优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有 LLM 数据库接口在扩展到企业级数据目录时面临的挑战，现有方法依赖于复杂的特定领域微调，并且未能有效利用数据库元数据中包含的重要语义上下文。

Method: 介绍了一种基于组件的检索架构，将数据库模式和元数据分解为离散的语义单元，并对每个单元进行单独索引以进行定向检索。

Result: 实验证明，该方法在保持高召回率和准确性的同时，在不同结构和可用元数据的海量数据库上超越了基线方法。

Conclusion: 该方法实现了可扩展的、无需专门微调的文本到 SQL 系统，解决了自然语言数据库接口中的关键可扩展性问题，并能在各种企业环境中部署。

Abstract: Despite advances in large language model (LLM)-based natural language
interfaces for databases, scaling to enterprise-level data catalogs remains an
under-explored challenge. Prior works addressing this challenge rely on
domain-specific fine-tuning - complicating deployment - and fail to leverage
important semantic context contained within database metadata. To address these
limitations, we introduce a component-based retrieval architecture that
decomposes database schemas and metadata into discrete semantic units, each
separately indexed for targeted retrieval. Our approach prioritizes effective
table identification while leveraging column-level information, ensuring the
total number of retrieved tables remains within a manageable context budget.
Experiments demonstrate that our method maintains high recall and accuracy,
with our system outperforming baselines over massive databases with varying
structure and available metadata. Our solution enables practical text-to-SQL
systems deployable across diverse enterprise settings without specialized
fine-tuning, addressing a critical scalability gap in natural language database
interfaces.

</details>


### [137] [Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity](https://arxiv.org/abs/2507.23121)
*Xinwei Wu,Haojie Li,Hongyu Liu,Xinyu Ji,Ruohan Li,Yule Chen,Yigeng Zhang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, we study a critical research problem regarding the
trustworthiness of large language models (LLMs): how LLMs behave when
encountering ambiguous narrative text, with a particular focus on Chinese
textual ambiguity. We created a benchmark dataset by collecting and generating
ambiguous sentences with context and their corresponding disambiguated pairs,
representing multiple possible interpretations. These annotated examples are
systematically categorized into 3 main categories and 9 subcategories. Through
experiments, we discovered significant fragility in LLMs when handling
ambiguity, revealing behavior that differs substantially from humans.
Specifically, LLMs cannot reliably distinguish ambiguous text from unambiguous
text, show overconfidence in interpreting ambiguous text as having a single
meaning rather than multiple meanings, and exhibit overthinking when attempting
to understand the various possible meanings. Our findings highlight a
fundamental limitation in current LLMs that has significant implications for
their deployment in real-world applications where linguistic ambiguity is
common, calling for improved approaches to handle uncertainty in language
understanding. The dataset and code are publicly available at this GitHub
repository: https://github.com/ictup/LLM-Chinese-Textual-Disambiguation.

</details>


### [138] [ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models through Procedural Plans](https://arxiv.org/abs/2507.23135)
*Ananya Sadana,Yash Kumar Lal,Jiawei Zhou*

Main category: cs.CL

TL;DR: ISO-Bench 是一个评估多模态模型因果推断能力的基准。现有模型在该基准上的表现不佳，远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 理解跨模态的因果关系是多模态模型在现实世界环境中运行的核心挑战。

Method: 提出 ISO-Bench 基准，用于评估模型在视觉观察和程序化文本之间推断因果关系的能力。每个样本包含任务步骤的图像和计划中的文本片段，模型需要判断视觉步骤是发生在引用的文本步骤之前还是之后。

Result: 在十个前沿视觉-语言模型上进行的评估结果显示，模型表现不佳：最佳零样本 F1 分数仅为 0.57，链式思考推理仅能带来小幅提升（最高 0.62 F1），与人类（0.98 F1）相差甚远。分析还指出了改进多模态模型因果理解的具体方向。

Conclusion: ISO-Bench 评估了模型在视觉观察和程序化文本之间推断因果关系的能力。结果显示，现有模型表现不佳，最佳模型的零样本 F1 分数仅为 0.57，而链式思考推理的 F1 分数也仅为 0.62，远低于人类的 0.98。

Abstract: Understanding causal relationships across modalities is a core challenge for
multimodal models operating in real-world environments. We introduce ISO-Bench,
a benchmark for evaluating whether models can infer causal dependencies between
visual observations and procedural text. Each example presents an image of a
task step and a text snippet from a plan, with the goal of deciding whether the
visual step occurs before or after the referenced text step. Evaluation results
on ten frontier vision-language models show underwhelming performance: the best
zero-shot F1 is only 0.57, and chain-of-thought reasoning yields only modest
gains (up to 0.62 F1), largely behind humans (0.98 F1). Our analysis further
highlights concrete directions for improving causal understanding in multimodal
models.

</details>


### [139] [User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal](https://arxiv.org/abs/2507.23158)
*Yuhan Liu,Michael J. Q. Zhang,Eunsol Choi*

Main category: cs.CL

TL;DR: 研究表明，可以从用户与语言模型的交互日志中收集隐式反馈来改进模型，但反馈的有效性取决于其内容和用户提示的质量。


<details>
  <summary>Details</summary>
Motivation: 研究如何从用户与语言模型的长期交互日志中有效地收集用户反馈，以实现模型的持续改进，同时避免打扰用户。

Method: 分析了两个用户-语言模型交互数据集（WildChat和LMSYS）中的隐式用户反馈，研究了反馈发生的时间和原因，并探索了从这些反馈中提取学习信号的方法。

Result: 语言模型可以从隐式用户反馈中学习，反馈内容（例如用户寻求澄清）比单纯的极性（例如用户对模型之前的回应不满意）更能提高模型在简短问题（MTBench）上的表现，但对更长更复杂的问题（WildBench）效果不佳。用户反馈的有效性很大程度上取决于用户初始提示的质量。

Conclusion: 隐式用户反馈具有潜力，但其有效性受限于用户提示的质量。

Abstract: Once language models (LMs) are deployed, they can interact with users
long-term, ideally evolving continuously based on their feedback. Asking for
direct user feedback can be disruptive; thus, we study harvesting user feedback
from user-LM interaction logs. We study implicit user feedback in two user-LM
interaction datasets (WildChat and LMSYS). First, we analyze user feedback in
the user-LLM conversation trajectory, providing insights into when and why such
feedback occurs. Second, we study harvesting learning signals from such
implicit user feedback. We find that the contents of user feedback (e.g., user
wanted clarification), not just the polarity (e.g., users were unhappy with the
previous model response), can improve model performance in short human-designed
questions (MTBench) but not on longer and more complex questions (WildBench).
We also find that the usefulness of user feedback is largely tied to the
quality of the user's initial prompt. Together, we provide an in-depth study of
implicit user feedback, showing its potential and limitations.

</details>


### [140] [Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks](https://arxiv.org/abs/2507.23194)
*Jianghui Wang,Vinay Joshi,Saptarshi Majumder,Xu Chao,Bin Ding,Ziqiong Liu,Pratik Prabhanjan Brahma,Dong Li,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: GEAK是一个利用LLM和Reflexion-style反馈生成高性能Triton GPU内核（尤其针对AMD GPU）的框架，在正确性和速度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了满足工业界和学术界对可扩展、硬件优化的GPU内核日益增长的需求，以及应对深度学习工作负载复杂性和多样性带来的挑战，自动化低级内核开发至关重要。

Method: GEAK框架利用了基于LLMs的推理和类似Reflexion的反馈机制，通过一个推理时间计算扩展循环来生成Triton GPU内核，特别针对AMD MI300X和MI250等GPU。

Result: 在两个评估基准上，GEAK在正确性方面最高提升了63%，在执行速度方面最高提升了2.59倍，显著优于直接提示LLM和基于Reflexion的生成方法。

Conclusion: GEAK通过其代理式代码生成方法，在Triton GPU内核生成方面取得了显著成效，在正确性和执行速度方面超越了直接提示和基于Reflexion的生成方法，有望加速多样化硬件平台的采用并普及专家级内核性能。

Abstract: The demand for AI-generated GPU kernels is rapidly growing, influenced by the
need for scalable, hardware-optimized solutions in both industry and academia.
As deep learning workloads grow in complexity and diversity, it is imperative
to automate low-level kernel development to meet performance and productivity
demands. Major cloud providers, semiconductor companies, and research
institutions are now investing heavily in AI-driven code generation for GPUs,
aiming to reduce manual optimization efforts while achieving near-expert
performance on hardware like AMD MI300X. The Triton language, a Python-based
DSL for GPU programming, has emerged as a popular target for such AI-generated
kernels due to its balance of performance and ease-of-coding. In this work, we
present an evaluation suite for Triton-based GPU kernels and GEAK (Generating
Efficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs
to generate performant Triton code specifically for AMD GPUs, including the AMD
MI300X and MI250. GEAK leverages inference-time compute scaling to produce
Triton-based GPU kernels using a reasoning loop adapted from Reflexion-style
feedback mechanisms. On two evaluation benchmarks, GEAK significantly
outperformed the baselines of directly prompting frontier LLMs as well as
Reflexion-based generation pipelines by achieving correctness up to $63$% and
execution speed up of up to $2.59$X. These results highlight the promise of
GEAK-like agentic code generation for accelerating the adoption of diverse
hardware platforms and democratizing access to expert-level kernel performance.

</details>


### [141] [Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context Learning by Leveraging Negative Samples](https://arxiv.org/abs/2507.23211)
*Yunhao Liang,Ruixuan Ying,Takuya Taniguchi,Zhe Cui*

Main category: cs.CL

TL;DR: 本研究提出一种新方法，利用负样本来改进少样本ICL中的正样本选择，通过构建正负样本语料库，并结合语义相似度检索和负样本引导的正样本选择，实验证明该方法能有效提升ICL性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的少样本（ICL）能力虽然强大，但其性能高度依赖于提供的示例。尽管检索相关示例的研究已取得进展，但主要集中在利用正样本，而忽略了负样本中蕴含的用于上下文学习的额外信息。本研究旨在利用负样本来改进正样本的选择，从而提升ICL性能。

Method: 首先，基于Zero-Shot-Cot构建正负样本语料库。在推理时，采用基于语义相似度的方法为给定查询从正负样本语料库中选择最相似的样本。随后，根据与负样本的语义相似度从正样本语料库中检索正样本，并将其与先前选择的正样本合并，作为ICL演示。

Result: 实验结果证明，该方法优于仅依赖最相似正样本的方法，表明负样本中的额外信息通过改进正样本的选择来增强ICL性能。

Conclusion: 研究结果表明，所提出的方法通过利用负样本来改进正样本的选择，能够超越仅依赖最相似正样本进行上下文学习的方法，验证了负样本中的额外信息有助于提高少样本ICL的性能。

Abstract: Large Language Models exhibit powerful few-shot in-context learning (ICL)
capabilities, but the performance is highly sensitive to provided examples.
  Recent research has focused on retrieving corresponding examples for each
input query, not only enhancing the efficiency and scalability of the learning
process but also mitigating inherent biases in manual example selection.
  However, these studies have primarily emphasized leveraging Positive samples
while overlooking the additional information within Negative samples for
contextual learning.
  We propose a novel method that utilizes Negative samples to better select
Positive sample examples, thereby enhancing the performance of few-shot ICL.
Initially, we construct Positive and Negative sample corpora based on
Zero-Shot-Cot. Then, during inference, we employ a semantic similarity-based
approach to select the most similar examples from both the Positive and
Negative corpora for a given query. Subsequently, we further retrieve Positive
examples from the Positive sample corpus based on semantic similarity to the
Negative examples, then concatenating them with the previously selected
Positive examples to serve as ICL demonstrations. Experimental results
demonstrate that our approach surpasses methods solely relying on the most
similar positive examples for context, validating that the additional
information in negative samples aids in enhancing ICL performance through
improved Positive sample selection.

</details>


### [142] [Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders](https://arxiv.org/abs/2507.23220)
*Carolina Zheng,Nicolas Beltran-Velez,Sweta Karlekar,Claudia Shi,Achille Nazaret,Asif Mallik,Amir Feder,David M. Blei*

Main category: cs.CL

TL;DR: MTMs, a new class of topic models using sparse autoencoders, uncover deeper conceptual themes and allow controllable text generation, outperforming traditional methods in evaluations.


<details>
  <summary>Details</summary>
Motivation: Traditional topic models struggle to capture semantically abstract features due to their reliance on bag-of-words representations. Neural variants are constrained by expressing topics as word lists, limiting their ability to articulate complex topics.

Method: MTMs operate on interpretable features learned by sparse autoencoders (SAEs). By defining topics over this semantically rich space, MTMs can reveal deeper conceptual themes with expressive feature descriptions. MTMs enable controllable text generation using topic-based steering vectors. We propose topic judge, an LLM-based pairwise comparison evaluation framework.

Result: MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of LLM outputs.

Conclusion: MTMs matches or exceeds traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of LLM outputs.

Abstract: Traditional topic models are effective at uncovering latent themes in large
text collections. However, due to their reliance on bag-of-words
representations, they struggle to capture semantically abstract features. While
some neural variants use richer representations, they are similarly constrained
by expressing topics as word lists, which limits their ability to articulate
complex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic
models that operate on interpretable features learned by sparse autoencoders
(SAEs). By defining topics over this semantically rich space, MTMs can reveal
deeper conceptual themes with expressive feature descriptions. Moreover,
uniquely among topic models, MTMs enable controllable text generation using
topic-based steering vectors. To properly evaluate MTM topics against
word-list-based approaches, we propose \textit{topic judge}, an LLM-based
pairwise comparison evaluation framework. Across five datasets, MTMs match or
exceed traditional and neural baselines on coherence metrics, are consistently
preferred by topic judge, and enable effective steering of LLM outputs.

</details>


### [143] [Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs](https://arxiv.org/abs/2507.23227)
*Sophie Kearney,Shu Yang,Zixuan Wen,Bojian Hou,Duy Duong-Tran,Tianlong Chen,Jason Moore,Marylyn Ritchie,Li Shen*

Main category: cs.CL

TL;DR: TAP-GPT利用LLM处理AD诊断的表格生物标志物数据，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 早期、准确的阿尔茨海默病（AD）诊断依赖于对异构生物标志物（如神经影像、遗传风险因素、认知测试和脑脊液蛋白质）的分析，这些生物标志物通常以表格形式表示。LLM在结构化生物医学数据预测方面具有潜力。

Method: TAP-GPT框架，一种将TableGPT2（一种多模态表格专用LLM）适应于AD诊断的框架，通过构建基于结构化生物医学数据的少量示例的表格提示，并使用参数高效的qLoRA适配进行微调，以应对AD或认知正常（CN）的临床二元分类任务。

Result: TAP-GPT框架在AD诊断任务中表现优于更高级的通用LLM和为预测任务开发的表格基础模型（TFM）。

Conclusion: TAP-GPT框架首次将LLM应用于基于表格生物标志物数据的预测任务，展示了其在生物医学信息学领域驱动的多智能体框架的潜力。

Abstract: Early and accurate diagnosis of Alzheimer's disease (AD), a complex
neurodegenerative disorder, requires analysis of heterogeneous biomarkers
(e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal
fluid proteins) typically represented in a tabular format. With flexible
few-shot reasoning, multimodal integration, and natural-language-based
interpretability, large language models (LLMs) offer unprecedented
opportunities for prediction with structured biomedical data. We propose a
novel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts
TableGPT2, a multimodal tabular-specialized LLM originally developed for
business intelligence tasks, for AD diagnosis using structured biomarker data
with small sample sizes. Our approach constructs few-shot tabular prompts using
in-context learning examples from structured biomedical data and finetunes
TableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary
classification task of AD or cognitively normal (CN). The TAP-GPT framework
harnesses the powerful tabular understanding ability of TableGPT2 and the
encoded prior knowledge of LLMs to outperform more advanced general-purpose
LLMs and a tabular foundation model (TFM) developed for prediction tasks. To
our knowledge, this is the first application of LLMs to the prediction task
using tabular biomarker data, paving the way for future LLM-driven multi-agent
frameworks in biomedical informatics.

</details>


### [144] [P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication](https://arxiv.org/abs/2507.23247)
*Sneha Oram,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本研究首次探索了大型语言模型在心理健康领域的语用推理能力，提出了P-ReMe数据集和相关任务，并评估了现有模型。研究发现Mistral和Qwen在推理方面表现出色，而Claude-3.5-haiku在处理污名化问题上更为负责。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索大型语言模型（LLMs）在心理健康领域中的语用推理能力，特别是针对解释性和个性化心理健康聊天机器人的发展中，此前尚未探索过的推理和对话语篇方面。

Method: 本研究提出了P-ReMe数据集，并为心理健康领域的蕴涵（隐含意义）和预设（隐含假设）这两个语用现象提出了修改后的定义。基于此定义，研究者制定了蕴涵相关的两个任务和预设相关的1个任务，并使用Llama3.1、Mistral、MentaLLaMa和Qwen四个模型对数据集和任务进行了基准测试。此外，研究者还提出了StiPRompts，并使用GPT-4o mini、Deepseek-chat和Claude-3.5-haiku模型来研究心理健康污名化问题。

Result: 实验结果表明，Mistral和Qwen在心理健康领域展现出实质性的推理能力。此外，研究发现Claude-3.5-haiku在处理心理健康污名化方面比GPT-4o mini和Deepseek-chat更负责任。

Conclusion: Claude-3.5-haiku在处理心理健康污名化方面比GPT-4o mini和Deepseek-chat更负责任。Mistral和Qwen在心理健康领域展现出实质性的推理能力。

Abstract: There has been an increase in recent advancements in the explainability and
development of personalized chatbots for mental health. However, the reasoning
aspects for explainability and dialogue discourse have not been explored
previously for mental health. Hence, we are investigating the pragmatic
reasoning capability of large language models (LLMs) in this domain. We
introduce P-ReMe dataset, and propose a modified definition for the pragmatic
phenomena of implicature (implied meaning) and presupposition (implicit
assumption) in mental health. Following the definition, we formulate two tasks
in implicature and one task in presupposition. To benchmark the dataset and the
presented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and
Qwen. The results of the experiments suggest that Mistral and Qwen show
substantial reasoning capabilities in the domain. In addition, we also propose
StiPRompts to study the stigma around mental health with the state-of-the-art
LLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings
show that Claude-3.5-haiku deals with the stigma more responsibly compared to
the other two LLMs.

</details>


### [145] [Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis](https://arxiv.org/abs/2507.23248)
*Shimanto Bhowmik,Tawsif Tashwar Dipto,Md Sazzad Islam,Sheryl Hsu,Tahsin Reasat*

Main category: cs.CL

TL;DR: 尽管LLM在多种语言上表现出色，但在处理孟加拉语等代表性不足的语言时，仍存在显著的性能差距。本研究系统地评估了10个开源LLM在孟加拉语上的表现，发现分词效率是影响模型准确性的关键因素。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为一种代表性不足的语言，在NLP研究中面临独特的语言结构和计算限制挑战。缺乏标准化的评估基准阻碍了孟加拉语NLP性能的提升。

Method: 通过评估10个开源LLM在8个翻译数据集上的表现，并进行全面的错误分析，找出模型的主要失败模式。分析了分词效率与模型准确性之间的关系。

Result: 发现在孟加拉语与英语相比，LLM普遍存在性能差距，特别是对于较小的模型和某些模型家族（如Mistral）。DeepSeek等模型在跨语言性能方面表现出更好的鲁棒性。分词效率越低，LLM的准确性越差。

Conclusion: 现有的LLM在处理孟加拉语时存在性能差距，尤其是在缺乏标准化评估基准的情况下。该研究揭示了分词效率与模型准确性之间的负相关关系，并强调了改进数据集质量和针对多语言环境的评估方法的重要性，旨在推动对代表性不足的语言的NLP研究。

Abstract: Bengali is an underrepresented language in NLP research. However, it remains
a challenge due to its unique linguistic structure and computational
constraints. In this work, we systematically investigate the challenges that
hinder Bengali NLP performance by focusing on the absence of standardized
evaluation benchmarks. We then evaluated 10 recent open source Large Language
Models (LLMs) in 8 of the translated datasets and performed a comprehensive
error analysis to pinpoint their primary failure modes. Our findings reveal
consistent performance gaps for Bengali compared to English, particularly for
smaller models and specific model families like Mistral. We also identified
promising robustness in certain architectures, such as DeepSeek, that maintain
more stable performance across languages. Our analysis reveals an inverse
relationship between tokenization efficiency and LLM accuracy where models tend
to perform worse when inputs are excessively tokenized, whereas more efficient
\& concise tokenization results in improved performance. These findings
highlight critical areas where current models fall short and underscore the
need for improved dataset quality and evaluation methodologies tailored to
multilingual contexts. This work will catalyze further research on NLP for
underrepresented languages, helping to democratize access to advanced language
technologies worldwide. The code and dataset used in this research is publicly
available at https://github.com/BengaliAI/bn-llm-benchmark.

</details>


### [146] [Unveiling Super Experts in Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2507.23279)
*Zunhai Su,Qingyuan Li,Hao Zhang,YuLei Qian,Yuchen Xie,Kehong Yuan*

Main category: cs.CL

TL;DR: 本研究发现了MoE LLMs中的“超级专家”（SEs），它们是导致模型性能下降的关键因素。SEs的剪枝会对模型整体性能产生负面影响，尤其是在数学推理方面。研究还发现，SEs对注意力机制有重要影响。


<details>
  <summary>Details</summary>
Motivation: 现有MoE LLM的专家级压缩技术通常依赖经验性标准来识别关键专家，缺乏对专家异质重要性进行更深入的探索和理解。本研究旨在发现并研究在模型前向推理中起关键作用的特定专家子集。

Method: 研究人员通过分析开源MoE LLMs，发现了“超级专家”（SEs）的存在，并研究了它们的特征和影响。他们通过剪枝SEs来评估其对模型性能的影响，特别是在数学推理任务上，并深入探讨了SEs如何影响注意力机制。

Result: 研究发现，SEs在MoE LLMs中普遍存在，并且剪枝少量SEs（例如三个）就会导致模型性能显著下降（以Qwen3-30B-A3B为例）。SEs的特点是在down_proj的输出中表现出罕见但极端的激活离群值，这些值会引起跨解码器层隐藏状态的巨大激活。SEs的分布具有模型特异性，且不受训练后处理的影响。剪枝SEs对模型整体性能有显著影响，尤其是在数学推理任务上。SEs的压缩会破坏注意力分数分布，而注意力汇聚是MoE LLMs的关键组成部分。

Conclusion: 该研究首次发现了稀疏激活的MoE（Mixture-of-Experts）大语言模型（LLMs）中存在的“超级专家”（SEs）现象，这些专家在模型前向推理中起着至关重要的作用。研究表明，SEs的剪枝会导致模型性能显著下降，尤其是在数学推理方面。SEs的特征是其在down_proj输出中表现出罕见但极端的激活离群值，并对注意力汇聚产生重要影响。

Abstract: Sparsely activated Mixture-of-Experts (MoE) models have shown promise in
enhancing the learning capacity of large language models (LLMs). Leveraging the
intrinsic importance differences among experts, recent research has explored
expert-level compression techniques to improve the efficiency of MoE LLMs.
However, existing approaches often rely on empirical criteria to identify
critical experts, lacking a deeper exploration and understanding of the
heterogeneous importance of experts. In this study, we present the first
discovery and investigation of a distinct subset of experts that play a crucial
role in the underlying mechanisms during the model's forward inference. These
experts are prevalent in open-source MoE LLMs, and despite their limited
number, pruning them leads to a significant decline in model performance (e.g.,
pruning three causes Qwen3-30B-A3B to produce repetitive and uninformative
outputs). We refer to these experts as Super Experts (SEs). Our comprehensive
analysis provides progressively deeper insights into SEs. (i) SEs are
characterized by rare but extreme activation outliers in the output of the
down_proj, which give rise to massive activations in the hidden states between
decoder layers. Moreover, the distribution of SEs remains model-specific and is
unaffected by post-training processes. (ii) By pruning SEs, we assess their
significance across a variety of tasks, revealing their considerable impact on
the model's overall performance, particularly in mathematical reasoning. (iii)
We further enhance our understanding of the influence of SEs compression. Our
findings confirm that MoE LLMs rely on SEs to induce attention sinks, which are
crucial for the distribution of attention scores but are significantly
disrupted by SE pruning. The code is available at
https://github.com/ZunhaiSu/Super-Experts-Profilling.

</details>


### [147] [What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content](https://arxiv.org/abs/2507.23319)
*Alfio Ferrara,Sergio Picascia,Laura Pinnavaia,Vojimir Ranitovic,Elisabetta Rocchetti,Alice Tuveri*

Main category: cs.CL

TL;DR: GPT-4o-mini在改写敏感内容时会不自觉地进行“净化”，减少不当词语。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM是否会在没有明确指示的情况下隐性地净化语言，并量化这种隐性内容审查行为。

Method: 通过实验分析GPT-4o-mini在释义敏感内容时的隐性内容审查行为，并评估敏感度变化的程度。同时，评估了LLM在句子敏感性分类方面的零样本能力，并与传统方法进行了比较。

Result: GPT-4o-mini在释义敏感内容时，会系统性地减少贬损性和禁忌性语言，显示出隐性内容审查的倾向。

Conclusion: GPT-4o-mini 会系统性地将内容向不敏感类别进行调整，在贬损性和禁忌性语言方面有显著减少。

Abstract: Proprietary Large Language Models (LLMs) have shown tendencies toward
politeness, formality, and implicit content moderation. While previous research
has primarily focused on explicitly training models to moderate and detoxify
sensitive content, there has been limited exploration of whether LLMs
implicitly sanitize language without explicit instructions. This study
empirically analyzes the implicit moderation behavior of GPT-4o-mini when
paraphrasing sensitive content and evaluates the extent of sensitivity shifts.
Our experiments indicate that GPT-4o-mini systematically moderates content
toward less sensitive classes, with substantial reductions in derogatory and
taboo language. Also, we evaluate the zero-shot capabilities of LLMs in
classifying sentence sensitivity, comparing their performances against
traditional methods.

</details>


### [148] [MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation](https://arxiv.org/abs/2507.23334)
*Daeyong Kwon,SeungHeon Doh,Juhan Nam*

Main category: cs.CL

TL;DR: 提出MusT-RAG框架，通过专门的音乐向量数据库MusWikiDB和上下文信息增强，提升LLMs在音乐问答任务上的表现，效果优于传统微调方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决通用大语言模型（LLMs）在音乐领域能力有限的问题，因为其训练数据中音乐相关知识的比例相对较小。

Method: 提出了一种名为MusT-RAG的综合框架，该框架基于检索增强生成（RAG）技术，通过（1）提出一个专门的音乐向量数据库MusWikiDB用于检索阶段，以及（2）在推理和微调过程中利用上下文信息，将通用的LLM适配于纯文本音乐问答（MQA）任务。

Result: 实验证明，MusT-RAG显著提高了LLMs在音乐领域的适应能力，在各种音乐问答任务上均优于传统微调方法。MusWikiDB相比通用维基百科语料库，在性能和计算效率上均有显著提升。

Conclusion: MusT-RAG框架在音乐问答任务上显著优于传统的微调方法，并在in-domain和out-of-domain的MQA基准测试中表现出持续的改进。此外，MusWikiDB比通用的维基百科语料库更有效，在性能和计算效率方面都表现更优。

Abstract: Recent advancements in Large language models (LLMs) have demonstrated
remarkable capabilities across diverse domains. While they exhibit strong
zero-shot performance on various tasks, LLMs' effectiveness in music-related
applications remains limited due to the relatively small proportion of
music-specific knowledge in their training data. To address this limitation, we
propose MusT-RAG, a comprehensive framework based on Retrieval Augmented
Generation (RAG) to adapt general-purpose LLMs for text-only music question
answering (MQA) tasks. RAG is a technique that provides external knowledge to
LLMs by retrieving relevant context information when generating answers to
questions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a
music-specialized vector database for the retrieval stage, and (2) utilizes
context information during both inference and fine-tuning processes to
effectively transform general-purpose LLMs into music-specific models. Our
experiment demonstrates that MusT-RAG significantly outperforms traditional
fine-tuning approaches in enhancing LLMs' music domain adaptation capabilities,
showing consistent improvements across both in-domain and out-of-domain MQA
benchmarks. Additionally, our MusWikiDB proves substantially more effective
than general Wikipedia corpora, delivering superior performance and
computational efficiency.

</details>


### [149] [Text-to-SQL Task-oriented Dialogue Ontology Construction](https://arxiv.org/abs/2507.23358)
*Renato Vukovic,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Hsien-Chin Lin,Shutong Feng,Nurul Lubis,Milica Gasic*

Main category: cs.CL

TL;DR: TeQoDO是一种新的方法，利用LLM在没有监督的情况下构建面向任务的对话本体，提高了可解释性，并在一项对话状态跟踪任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了提高LLM的可解释性和可信度，尤其是在任务导向的对话（TOD）系统中，需要一种无需手动标签或监督训练即可构建本体的方法。

Method: TeQoDO是一种文本到SQL的面向任务的对话本体构建方法，它利用大型语言模型（LLM）的SQL编程能力和提示中提供的对话理论，在没有监督的情况下自主地从头开始构建TOD本体。

Result: TeQoDO的性能优于迁移学习方法，并且构建的本体在下游对话状态跟踪任务中具有竞争力。此外，TeQoDO可以扩展到处理更大的数据集，例如维基百科和ArXiv。

Conclusion: LLM有潜力在没有监督的情况下从头开始构建面向任务的对话（TOD）本体，使用其固有的SQL编程能力和提示中提供的对话理论。TeQoDO的性能优于迁移学习方法，并且构建的本体在下游对话状态跟踪任务中具有竞争力。对话理论在TeQoDO的性能中起着关键作用，该方法可以扩展到构建更大的本体。

Abstract: Large language models (LLMs) are widely used as general-purpose knowledge
sources, but they rely on parametric knowledge, limiting explainability and
trustworthiness. In task-oriented dialogue (TOD) systems, this separation is
explicit, using an external database structured by an explicit ontology to
ensure explainability and controllability. However, building such ontologies
requires manual labels or supervised training. We introduce TeQoDO: a
Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM
autonomously builds a TOD ontology from scratch without supervision using its
inherent SQL programming capabilities combined with dialogue theory provided in
the prompt. We show that TeQoDO outperforms transfer learning approaches, and
its constructed ontology is competitive on a downstream dialogue state tracking
task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also
scales to allow construction of much larger ontologies, which we investigate on
a Wikipedia and ArXiv dataset. We view this as a step towards broader
application of ontologies to increase LLM explainability.

</details>


### [150] [MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in Multimodal Large Language Models](https://arxiv.org/abs/2507.23382)
*Yiyan Ji,Haoran Chen,Qiguang Chen,Chengyue Wu,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: MPCC基准旨在评估MLLM在包含预算、时间、空间等复杂约束的真实世界规划任务中的能力。实验表明，当前MLLM在该任务上的表现不佳，对约束复杂性敏感，且现有提示策略效果有限。


<details>
  <summary>Details</summary>
Motivation: 解决了现有基准无法直接评估真实世界多模态规划能力以及缺乏跨模态约束的问题。

Method: 提出了第一个系统评估MLLM处理规划中多模态约束能力的基准——MPCC（Multimodal Planning with Complex Constraints），该基准包含三个真实世界任务（飞行规划、日历规划、会议规划），并引入了具有不同难度级别（EASY, MEDIUM, HARD）的复杂约束（如预算、时间、空间）。

Result: 在MPCC基准上，13个先进的MLLM表现出显著的挑战：闭源模型仅能生成21.3%的可行计划，而开源模型平均低于11%。研究还发现MLLM对约束复杂性非常敏感，并且传统的多模态提示策略在多约束场景下效果不佳。

Conclusion: MLLMs在处理多模态约束规划方面面临重大挑战，闭源模型和开源模型的表现均不理想，并且对约束复杂性高度敏感。现有的多模态提示策略在多约束场景下失效，表明需要在这方面进行改进以满足现实世界应用的需求。

Abstract: Multimodal planning capabilities refer to the ability to predict, reason, and
design steps for task execution with multimodal context, which is essential for
complex reasoning and decision-making across multiple steps. However, current
benchmarks face two key challenges: (1) they cannot directly assess multimodal
real-world planning capabilities, and (2) they lack constraints or implicit
constraints across modalities. To address these issues, we introduce Multimodal
Planning with Complex Constraints (MPCC), the first benchmark to systematically
evaluate MLLMs' ability to handle multimodal constraints in planning. To
address the first challenge, MPCC focuses on three real-world tasks: Flight
Planning, Calendar Planning, and Meeting Planning. To solve the second
challenge, we introduce complex constraints (e.g. budget, temporal, and
spatial) in these tasks, with graded difficulty levels (EASY, MEDIUM, HARD) to
separate constraint complexity from search space expansion. Experiments on 13
advanced MLLMs reveal significant challenges: closed-source models achieve only
21.3% feasible plans, while open-source models average below 11%. Additionally,
we observe that MLLMs are highly sensitive to constraint complexity and that
traditional multimodal prompting strategies fail in multi-constraint scenarios.
Our work formalizes multimodal constraints in planning, provides a rigorous
evaluation framework, and highlights the need for advancements in
constraint-aware reasoning for real-world MLLM applications.

</details>


### [151] [Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models](https://arxiv.org/abs/2507.23386)
*Ailiang Lin,Zhuoyun Li,Kotaro Funakoshi*

Main category: cs.CL

TL;DR: Causal2Vec 是一种新的嵌入模型，它通过添加上下文标记和改进的最终嵌入方法来增强解码器 LLM，同时减少计算成本并提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于解码器的大型语言模型（LLM）嵌入模型在去除因果注意力掩码以实现双向注意力时，可能会损害模型在预训练期间获得的语义信息提取能力。此外，一些单向方法需要额外的输入文本来克服因果注意力的固有局限性，从而增加了计算成本。因此，有必要开发一种既能增强解码器 LLM 性能，又不会改变其原始架构或引入显著计算开销的嵌入模型。

Method: Causal2Vec 通过以下两个主要步骤来增强解码器模型：1. 使用轻量级 BERT 风格模型预编码输入文本，生成一个上下文标记，并将其添加到 LLM 的输入序列的开头。这使得每个标记都能捕获上下文信息，即使在没有未来标记的情况下也是如此。2. 将上下文标记和 EOS 标记的最后隐藏状态连接起来，作为最终的文本嵌入，以减轻последний токен пулинга引入的近因偏差，并帮助 LLM 更好地利用上下文标记中编码的语义信息。

Result: Causal2Vec 在大规模文本嵌入基准（MTEB）上取得了最先进的性能，并且与性能最佳的方法相比，所需的序列长度减少了多达 85%，推理时间减少了多达 82%。

Conclusion: Causal2Vec 是一种通用的嵌入模型，旨在增强解码器模型在不改变其原始架构或引入显著计算开销的情况下，在各种嵌入任务中的性能。通过在 LLM 的输入序列的开头添加一个由轻量级 BERT 风格模型预编码的上下文标记，并结合上下文标记和 EOS 标记的最后隐藏状态作为最终文本嵌入，Causal2Vec 克服了因果注意力的固有局限性，并减轻了последний токен пулинга引入的近因偏差。

Abstract: Decoder-only large language models (LLMs) are increasingly used to build
embedding models that effectively encode the semantic information of natural
language texts into dense vector representations for various embedding tasks.
However, many existing methods primarily focus on removing the causal attention
mask in LLMs to enable bidirectional attention, potentially undermining the
model's ability to extract semantic information acquired during pretraining.
Additionally, leading unidirectional approaches often rely on extra input text
to overcome the inherent limitations of causal attention, inevitably increasing
computational costs. In this work, we propose Causal2Vec, a general-purpose
embedding model tailored to enhance the performance of decoder-only LLMs
without altering their original architectures or introducing significant
computational overhead. Specifically, we first employ a lightweight BERT-style
model to pre-encode the input text into a single Contextual token, which is
then prepended to the LLM's input sequence, allowing each token to capture
contextualized information even without attending to future tokens.
Furthermore, to mitigate the recency bias introduced by last-token pooling and
help LLMs better leverage the semantic information encoded in the Contextual
token, we concatenate the last hidden states of Contextual and EOS tokens as
the final text embedding. In practice, Causal2Vec achieves state-of-the-art
performance on the Massive Text Embeddings Benchmark (MTEB) among models
trained solely on publicly available retrieval datasets, while reducing the
required sequence length by up to 85% and inference time by up to 82% compared
to best-performing methods.

</details>


### [152] [Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM Deployment for Translators](https://arxiv.org/abs/2507.23399)
*Peter Sandrini*

Main category: cs.CL

TL;DR: 本研究评估了本地部署的免费语言模型作为翻译领域商业云端AI解决方案的替代方案。结果表明，本地模型在功能性能上具有可行性，并能在数据控制和隐私方面提供优势，有助于AI技术的普及。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速发展，在翻译领域带来了机遇和挑战。虽然商业云端AI聊天机器人备受关注，但数据隐私、安全和公平性问题促使研究者探索替代部署模式，因此本研究旨在调查本地部署的免费语言模型作为一种可行的替代方案。

Method: 评估了三个在CPU平台上安装的开源模型，并将它们与商业在线聊天机器人进行了比较，评估重点是功能性能。

Result: 研究评估了三个开源模型在CPU平台上的功能性能，并与商业在线聊天机器人进行了比较，证明了本地部署模型在功能上的可行性。

Conclusion: 本地部署的免费语言模型在功能性能方面可以作为商业云端AI解决方案的可行替代方案，尤其是在数据控制、隐私和减少对云服务的依赖方面具有优势，这有助于推动AI技术的民主化，使LLM更易于被个人翻译者和小型企业使用。

Abstract: The rapid proliferation of Large Language Models presents both opportunities
and challenges for the translation field. While commercial, cloud-based AI
chatbots have garnered significant attention in translation studies, concerns
regarding data privacy, security, and equitable access necessitate exploration
of alternative deployment models. This paper investigates the feasibility and
performance of locally deployable, free language models as a viable alternative
to proprietary, cloud-based AI solutions. This study evaluates three
open-source models installed on CPU-based platforms and compared against
commercially available online chat-bots. The evaluation focuses on functional
performance rather than a comparative analysis of human-machine translation
quality, an area already subject to extensive research. The platforms assessed
were chosen for their accessibility and ease of use across various operating
systems. While local deployment introduces its own challenges, the benefits of
enhanced data control, improved privacy, and reduced dependency on cloud
services are compelling. The findings of this study contribute to a growing
body of knowledge concerning the democratization of AI technology and inform
future research and development efforts aimed at making LLMs more accessible
and practical for a wider range of users, specifically focusing on the needs of
individual translators and small businesses.

</details>


### [153] [MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on Multi-Relational Graphs and Structural Entropy Minimization](https://arxiv.org/abs/2507.23400)
*Yongbing Zhang,Fang Nan,Shengxiang Gao,Yuxin Huang,Kaiwen Tan,Zhengtao Yu*

Main category: cs.CL

TL;DR: MRGSEM-Sum是一个新的无监督多文档摘要框架，它使用多关系图和结构熵最小化来处理文档间的复杂关系和信息冗余，能够自适应地确定簇数并生成高质量的摘要。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅考虑单关系图并需要预先定义的簇数，这限制了它们充分表示丰富关系信息和自适应划分句子组以减少冗余的能力。为了克服这些限制，需要一个能够处理多关系并自适应确定簇数的方法。

Method: 提出了一种基于多关系图和结构熵最小化的无监督多文档摘要框架MRGSEM-Sum。该框架首先构建一个整合了句子间语义和语篇关系的多关系图，然后应用二维结构熵最小化算法进行聚类，自动确定最优簇数并有效地将句子组织成连贯的组，最后引入了注意力机制来压缩每个簇，生成简洁而有信息量的摘要。

Result: 在Multi-News、DUC-2004、PubMed和WikiSum四个基准数据集上的大量实验表明，MRGSEM-Sum一致优于以前的无监督方法，并且在某些情况下，其性能与有监督模型和大型语言模型相当。人类评估表明，MRGSEM-Sum生成的摘要具有高度的一致性和覆盖率，接近人类水平的质量。

Conclusion: MRGSEM-Sum在多个基准数据集上持续优于以前的无监督方法，并且在某些情况下，其性能与有监督模型和大型语言模型相当。人类评估表明，MRGSEM-Sum生成的摘要具有高度的一致性和覆盖率，接近人类水平的质量。

Abstract: The core challenge faced by multi-document summarization is the complexity of
relationships among documents and the presence of information redundancy. Graph
clustering is an effective paradigm for addressing this issue, as it models the
complex relationships among documents using graph structures and reduces
information redundancy through clustering, achieving significant research
progress. However, existing methods often only consider single-relational
graphs and require a predefined number of clusters, which hinders their ability
to fully represent rich relational information and adaptively partition
sentence groups to reduce redundancy. To overcome these limitations, we propose
MRGSEM-Sum, an unsupervised multi-document summarization framework based on
multi-relational graphs and structural entropy minimization. Specifically, we
construct a multi-relational graph that integrates semantic and discourse
relations between sentences, comprehensively modeling the intricate and dynamic
connections among sentences across documents. We then apply a two-dimensional
structural entropy minimization algorithm for clustering, automatically
determining the optimal number of clusters and effectively organizing sentences
into coherent groups. Finally, we introduce a position-aware compression
mechanism to distill each cluster, generating concise and informative
summaries. Extensive experiments on four benchmark datasets (Multi-News,
DUC-2004, PubMed, and WikiSum) demonstrate that our approach consistently
outperforms previous unsupervised methods and, in several cases, achieves
performance comparable to supervised models and large language models. Human
evaluation demonstrates that the summaries generated by MRGSEM-Sum exhibit high
consistency and coverage, approaching human-level quality.

</details>


### [154] [Enhanced Arabic Text Retrieval with Attentive Relevance Scoring](https://arxiv.org/abs/2507.23404)
*Salah Eddine Bekhouche,Azeddine Benlamoudi,Yazid Bounab,Fadi Dornaika,Abdenour Hadid*

Main category: cs.CL

TL;DR: 本研究提出了一个针对阿拉伯语的改进型密集检索框架，引入了新的注意力相关性评分方法，显著提高了阿拉伯语问答的检索性能和排名准确性。


<details>
  <summary>Details</summary>
Motivation: 由于阿拉伯语复杂的形态、可选的标调符号以及标准语和方言并存的现象，其在自然语言处理（NLP）和信息检索（IR）领域面临巨大挑战，但其全球重要性日益增长，而相关研究资源却相对匮乏。

Method: 本研究提出了一种新颖的注意力相关性评分（ARS）方法，用于替代标准的交互机制，并结合了预训练的阿拉伯语语言模型和架构优化，以提升检索性能。

Result: 通过引入ARS和集成预训练模型，该框架能更有效地模拟问题与段落间的语义相关性，从而在阿拉伯语问答任务中取得显著的性能提升和排名准确性。

Conclusion: 本研究提出的改进型密集检索框架在阿拉伯语检索任务上表现出色，显著提高了问答的排名准确性。

Abstract: Arabic poses a particular challenge for natural language processing (NLP) and
information retrieval (IR) due to its complex morphology, optional diacritics
and the coexistence of Modern Standard Arabic (MSA) and various dialects.
Despite the growing global significance of Arabic, it is still underrepresented
in NLP research and benchmark resources. In this paper, we present an enhanced
Dense Passage Retrieval (DPR) framework developed specifically for Arabic. At
the core of our approach is a novel Attentive Relevance Scoring (ARS) that
replaces standard interaction mechanisms with an adaptive scoring function that
more effectively models the semantic relevance between questions and passages.
Our method integrates pre-trained Arabic language models and architectural
refinements to improve retrieval performance and significantly increase ranking
accuracy when answering Arabic questions. The code is made publicly available
at \href{https://github.com/Bekhouche/APR}{GitHub}.

</details>


### [155] [Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration](https://arxiv.org/abs/2507.23407)
*Ante Wang,Yujie Lin,Jingyao Liu,Suhang Wu,Hao Liu,Xinyan Xiao,Jinsong Su*

Main category: cs.CL

TL;DR: 该研究引入了“主动批判性思维”，即模型主动向用户询问缺失信息以解决查询。他们创建了GSM-MC和GSM-MCE基准来测试这种能力。结果显示，大型语言模型在这方面表现不佳，但强化学习可以显著提高其表现，例如将Qwen3-1.7B的准确率从0.15%提高到73.98%。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在批判性思维方面主要侧重于被动拒绝有缺陷的查询，而未能主动与用户沟通以解决问题。为了使AI系统能更好地与用户协作解决问题，有必要研究一种能主动寻求缺失或澄清信息以更好地解决用户查询的主动批判性思维范式。

Method: 提出了主动批判性思维的范式，并基于GSM8K构建了GSM-MC和GSM-MCE两个新的基准，用于评估模型在不完整或误导性条件下的数学推理能力。GSM-MC包含故意移除关键变量的数学问题，GSM-MCE则增加了无关信息以测试模型对抗干扰的能力。实验采用了Qwen3和Llama系列模型，并探索了强化学习（RL）对提升主动批判性思维能力的效果。

Result: 在GSM-MC和GSM-MCE基准上，Qwen3和Llama系列模型在传统推理任务上表现良好，但在主动批判性思维方面存在困难，尤其是较小的模型。通过强化学习（RL）的改进，Qwen3-1.7B模型在GSM-MC上的准确率从0.15%显著提升至73.98%。

Conclusion: 该研究提出了主动批判性思维的范式，并为此开发了GSM-MC和GSM-MCE基准。实验表明，现有的大型语言模型在主动批判性思维方面表现不佳，但可以通过强化学习得到显著提升，例如Qwen3-1.7B在GSM-MC上的准确率从0.15%提升到73.98%。研究希望通过主动批判性思维促进模型与用户在解决问题方面的协作。

Abstract: Critical thinking is essential for building robust AI systems, preventing
them from blindly accepting flawed data or biased reasoning. However, prior
work has primarily focused on passive critical thinking, where models simply
reject problematic queries without taking constructive steps to address user
requests. In this work, we introduce proactive critical thinking, a paradigm
where models actively seek missing or clarifying information from users to
resolve their queries better. To evaluate this capability, we present GSM-MC
and GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical
reasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math
problems with a key variable deliberately removed, requiring models to identify
and request the missing information. GSM-MCE further increases the difficulty
by introducing irrelevant details to test robustness against distractions.
Experiments on Qwen3 and Llama series models show that, while these models
excel in traditional reasoning tasks due to extensive post-training and
inference-time scaling, they struggle with proactive critical thinking,
especially smaller ones. However, we demonstrate that reinforcement learning
(RL) can significantly improve this ability. Using our enhanced RL algorithm,
we achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to
73.98% on GSM-MC. We hope this work advances models that collaborate more
effectively with users in problem-solving through proactive critical thinking.

</details>


### [156] [Role-Aware Language Models for Secure and Contextualized Access Control in Organizations](https://arxiv.org/abs/2507.23465)
*Saeed Almheiri,Yerulan Kongrat,Adrian Santosh,Ruslan Tasmukhanov,Josemaria Vera,Muhammad Dehan Al Kautsar,Fajri Koto*

Main category: cs.CL

TL;DR: LLMs can be trained to respect user roles in enterprise settings, moving beyond general safety to role-specific access control, with evaluation on custom datasets and robustness tests.


<details>
  <summary>Details</summary>
Motivation: Existing safety methods for LLMs do not address role-specific access constraints, which is an essential requirement for enterprise deployment.

Method: Investigated three modeling strategies: BERT-based classifier, LLM-based classifier, and role-conditioned generation. Constructed two datasets (adapted and synthetic) for evaluation across various organizational structures and robustness tests (prompt injection, role mismatch, jailbreak).

Result: The study demonstrates the feasibility of fine-tuning LLMs for role-specific behavior, with performance assessed across different organizational structures and robustness challenges.

Conclusion: LLMs can be fine-tuned to generate role-specific responses, addressing enterprise requirements for access control.

Abstract: As large language models (LLMs) are increasingly deployed in enterprise
settings, controlling model behavior based on user roles becomes an essential
requirement. Existing safety methods typically assume uniform access and focus
on preventing harmful or toxic outputs, without addressing role-specific access
constraints. In this work, we investigate whether LLMs can be fine-tuned to
generate responses that reflect the access privileges associated with different
organizational roles. We explore three modeling strategies: a BERT-based
classifier, an LLM-based classifier, and role-conditioned generation. To
evaluate these approaches, we construct two complementary datasets. The first
is adapted from existing instruction-tuning corpora through clustering and role
labeling, while the second is synthetically generated to reflect realistic,
role-sensitive enterprise scenarios. We assess model performance across varying
organizational structures and analyze robustness to prompt injection, role
mismatch, and jailbreak attempts.

</details>


### [157] [A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains](https://arxiv.org/abs/2507.23486)
*Shirui Wang,Zhihui Tang,Huaxia Yang,Qiuhong Gong,Tiantian Gu,Hongyang Ma,Yongxin Wang,Wubin Sun,Zeliang Lian,Kehang Mao,Yinan Jiang,Zhicheng Huang,Lingyun Ma,Wenjie Shen,Yajie Ji,Yunhui Tan,Chunbo Wang,Yunlu Gao,Qianling Ye,Rui Lin,Mingyu Chen,Lijuan Niu,Zhihao Wang,Peng Yu,Mengran Lang,Yue Liu,Huimin Zhang,Haitao Shen,Long Chen,Qiguang Zhao,Si-Xuan Liu,Lina Zhou,Hua Gao,Dongqiang Ye,Lingmin Meng,Youtao Yu,Naixin Liang,Jianxiong Wu*

Main category: cs.CL

TL;DR: CSEDB 是一个由临床专家共识驱动的多维度框架，包含 30 项标准，用于评估医疗 LLM 的安全性和有效性。通过对 6 个 LLM 的测试发现，它们在安全性和有效性方面表现不一，尤其在高风险场景下性能下降明显。该基准有助于推动医疗 LLM 的安全有效部署。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LLM）在临床决策支持中的安全性评估和有效性验证方面的挑战，本研究旨在开发一个标准化的评估框架。

Method: 通过 32 名专科医生制定和审查了 2069 个开放式问答项目，这些项目涵盖了 26 个临床部门和 30 个标准，旨在模拟真实世界的临床场景，以构建 CSEDB 框架。随后，使用 CSEDB 框架对六种 LLM 进行了基准测试。

Result: 在对六种 LLM 的测试中，CSEDB 基准测试显示 LLM 的总体平均得分为 57.2%（安全 54.7%，有效性 62.3%）。在高风险场景下，性能下降了 13.3%（p < 0.0001）。领域特定的医学 LLM 在安全性和有效性方面表现出持续的优势，最高得分分别为 0.912 和 0.861。

Conclusion: 该研究提出了一个名为 CSEDB 的多维度临床安全-有效性基准框架，该框架旨在评估大型语言模型（LLM）在临床决策支持中的应用。研究结果表明，虽然领域特定的医学 LLM 表现优于通用 LLM，但所有 LLM 在高风险场景下的性能均显著下降。该基准框架有助于评估、比较和改进医疗领域 LLM 的安全性和有效性。

Abstract: Large language models (LLMs) hold promise in clinical decision support but
face major challenges in safety evaluation and effectiveness validation. We
developed the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a
multidimensional framework built on clinical expert consensus, encompassing 30
criteria covering critical areas like critical illness recognition, guideline
adherence, and medication safety, with weighted consequence measures.
Thirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A
items aligned with these criteria, spanning 26 clinical departments to simulate
real-world scenarios. Benchmark testing of six LLMs revealed moderate overall
performance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),
with a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).
Domain-specific medical LLMs showed consistent performance advantages over
general-purpose models, with relatively higher top scores in safety (0.912) and
effectiveness (0.861). The findings of this study not only provide a
standardized metric for evaluating the clinical application of medical LLMs,
facilitating comparative analyses, risk exposure identification, and
improvement directions across different scenarios, but also hold the potential
to promote safer and more effective deployment of large language models in
healthcare environments.

</details>


### [158] [Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning](https://arxiv.org/abs/2507.23541)
*Keer Lu,Zheng Liang,Youquan Li,Jiejun Tan,Da Pan,Shusen Zhang,Guosheng Dong,Huang Leng*

Main category: cs.CL

TL;DR: Med-R$^3$是一个医疗检索增强推理框架，通过渐进式强化学习协同优化检索和推理能力，解决了现有方法在协调性、泛化性和领域适应性方面的不足，并在实验中取得了优于SFT和现有RL方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检索和推理能力上各自为政，缺乏协同优化，限制了两种过程的协调性。此外，依赖监督微调（SFT）可能导致模型记忆现有解决方法，限制其泛化能力。现有的强化学习方法未能充分考虑医疗领域的特定需求。

Method: 提出了一种名为Med-R$^3$的医疗检索增强推理框架，该框架通过渐进式强化学习进行驱动。首先，模型学习在医学问题上进行逻辑推理。在此基础上，模型自适应地优化检索能力，使其与知识库的特征和推理过程中的外部信息利用相匹配。最后，对模型的检索和推理协调进行联合优化。

Result: LLaMA3.1-8B-Instruct + Med-R$^3$超越了GPT-4o-mini 3.93%，Qwen2.5-14B + Med-R$^3$提升了13.53%。

Conclusion: Med-R$^3$框架在医疗领域取得了最先进的性能，LLaMA3.1-8B-Instruct + Med-R$^3$在相当的参数规模下超越了闭源的GPT-4o-mini 3.93%，而Qwen2.5-14B + Med-R$^3$则取得了13.53%的显著提升。

Abstract: In medical scenarios, effectively retrieving external knowledge and
leveraging it for rigorous logical reasoning is of significant importance.
Despite their potential, existing work has predominantly focused on enhancing
either retrieval or reasoning capabilities of the models in isolation, with
little attention given to their joint optimization, which leads to limited
coordination between the two processes. Additionally, current methods rely
heavily on supervised fine-tuning (SFT), which can cause models to memorize
existing problem-solving pathways, thereby restricting their generalization
ability when confronted with novel problem contexts. Furthermore, while some
studies have explored to improve retrieval-augmented reasoning in general
domains via reinforcement learning, their reward function designs do not
adequately capture the specific demands of the medical domain. To address these
challenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented
**R**easoning framework driven by progressive **R**einforcement learning. In
this framework, we first develop the model's ability to perform logical
reasoning over medical problems. Subsequently, on the basis of this foundation,
we adaptively optimize the retrieval capability to better align with the
characteristics of knowledge corpus and external information utilization
throughout the reasoning process. Finally, we conduct joint optimization of the
model's retrieval and reasoning coordination. Extensive experiments indicate
that **Med-R$^3$** could achieve state-of-the-art performances, with
LLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by
3.93\% at a comparable parameter scale, while Qwen2.5-14B augmented with
Med-R$^3$ shows a more substantial gain of 13.53\%.

</details>


### [159] [T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text](https://arxiv.org/abs/2507.23577)
*Alva West,Luodan Zhang,Liuliu Zhang,Minjun Zhu,Yixuan Weng,Yue Zhang*

Main category: cs.CL

TL;DR: T-Detect 是一种用于检测机器生成文本的新方法，它使用学生 t 分布来处理重尾分布，提高了检测的鲁棒性，并在基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本检测器通常依赖于统计量，这些统计量隐含地假设高斯分布，但当遇到对抗性或非母语英语文本的重尾统计伪影时，这一前提就会失败。

Method: T-Detect 通过将段落的对数似然与 t 分布的预期矩进行归一化来计算检测分数，从而取代了标准高斯归一化。

Result: T-Detect 在 RAID 真实数据集和 HART 数据集上表现优于现有方法，AUROC 最高可提高 3.9%。在 CT 检测框架中，T-Detect 在 Books 域上实现了 0.926 的 AUROC。

Conclusion: T-Detect 是一种新颖的文本检测方法，它通过用源自学生 t 分布的重尾差异分数取代标准高斯归一化，从根本上重新设计了基于曲率的检测器的统计核心。该方法在 RAID 和 HART 数据集上得到了验证，并在有针对性的领域提供了比现有方法高出 3.9% 的 AUROC。在 CT 检测框架中，T-Detect 在 Books 域上达到了 0.926 的 AUROC，达到了最先进的性能。

Abstract: The proliferation of sophisticated text generation models necessitates the
development of robust detection methods capable of identifying
machine-generated content, particularly text designed to evade detection
through adversarial perturbations. Existing zero-shot detectors often rely on
statistical measures that implicitly assume Gaussian distributions, a premise
that falters when confronted with the heavy-tailed statistical artifacts
characteristic of adversarial or non-native English texts. This paper
introduces T-Detect, a novel detection method that fundamentally redesigns the
statistical core of curvature-based detectors. Our primary innovation is the
replacement of standard Gaussian normalization with a heavy-tailed discrepancy
score derived from the Student's t-distribution. This approach is theoretically
grounded in the empirical observation that adversarial texts exhibit
significant leptokurtosis, rendering traditional statistical assumptions
inadequate. T-Detect computes a detection score by normalizing the
log-likelihood of a passage against the expected moments of a t-distribution,
providing superior resilience to statistical outliers. We validate our approach
on the challenging RAID benchmark for adversarial text and the comprehensive
HART dataset. Experiments show that T-Detect provides a consistent performance
uplift over strong baselines, improving AUROC by up to 3.9\% in targeted
domains. When integrated into a two-dimensional detection framework (CT), our
method achieves state-of-the-art performance, with an AUROC of 0.926 on the
Books domain of RAID. Our contributions are a new, theoretically-justified
statistical foundation for text detection, an ablation-validated method that
demonstrates superior robustness, and a comprehensive analysis of its
performance under adversarial conditions. Ours code are released at
https://github.com/ResearAI/t-detect.

</details>


### [160] [DiffLoRA: Differential Low-Rank Adapters for Large Language Models](https://arxiv.org/abs/2507.23588)
*Alexandre Misrahi,Nadezhda Chirkova,Maxime Louis,Vassilina Nikoulina*

Main category: cs.CL

TL;DR: DiffLoRA 是一种参数高效的微调方法，旨在结合 LoRA 的效率和微分注意力的性能。虽然在大多数任务上表现平平，但在某些特定任务上显示出潜力，例如在 HumanEval 上比 LoRA 有显著提升。


<details>
  <summary>Details</summary>
Motivation: 提高 Transformer 模型在各种 NLP 任务上的性能，特别是通过参数高效的方式来适配微分注意力机制。

Method: DiffLoRA 是一种参数高效的微分注意力机制适配方法，它在 LoRA 的基础上，通过在正负注意力项上加入低秩适配器。

Result: DiffLoRA 在大多数评估任务上表现不如其他参数高效微调方法，但在某些领域（例如 HumanEval 任务上比 LoRA 提高 11 分）表现出有趣的性能。通过分析微调后的注意力模式来探究其行为原因。

Conclusion: DiffLoRA 是一种参数高效的微分注意力机制适配方法，它在 LoRA 的基础上，通过在正负注意力项上加入低秩适配器，旨在获得微分注意力的性能增益，同时保持 LoRA 的效率。

Abstract: Differential Transformer has recently been proposed to improve performance in
Transformer models by canceling out noise through a denoiser attention
mechanism. In this work, we introduce DiffLoRA, a parameter-efficient
adaptation of the differential attention mechanism, with low-rank adapters on
both positive and negative attention terms. This approach retains the
efficiency of LoRA while aiming to benefit from the performance gains of
differential attention. We evaluate DiffLoRA across a broad range of NLP tasks,
including general benchmarks, many-shot in-context learning, RAG, and
long-context tests. We observe that, although DiffLoRA falls short of other
parameter-efficient fine-tuning methods in most evaluation tasks, it shows
interesting results in certain domains (+11 pts on LoRA for HumanEval). We
analyze the attention patterns post-finetuning to identify the reasons for this
behavior.

</details>


### [161] [Arabic Hate Speech Identification and Masking in Social Media using Deep Learning Models and Pre-trained Models Fine-tuning](https://arxiv.org/abs/2507.23661)
*Salam Thabet Doghmash,Motaz Saad*

Main category: cs.CL

TL;DR: 该研究提出了在阿拉伯语中检测和清理仇恨言论的方法，使用深度学习和 Transformer 模型进行检测，并将其清理视为机器翻译任务，取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 近年来，在社交媒体上识别仇恨言论已成为一个日益重要的问题。

Method: 该研究解决了两个问题：1）检测阿拉伯语文本中的仇恨言论，2）清理给定文本中的仇恨言论。对于第一个问题，研究人员使用了深度学习模型和 Transformer 进行了多项实验，以确定 F1 分数最佳的模型。对于第二个问题，研究人员将其视为一项机器翻译任务，输入是包含脏话的句子，输出是遮蔽了脏话的相同句子。

Result: 仇恨言论检测方面达到了最佳模型，宏观 F1 分数为 92%，准确率为 95%。文本清理方面，仇恨言论掩蔽模型达到了 0.3 的 BLEU 分数（1-gram）。

Conclusion: 所提出的方法在仇恨言论检测方面达到了最佳模型，宏观 F1 分数为 92%，准确率为 95%。在文本清理实验方面，仇恨言论掩蔽模型在 BLEU 分数（1-gram）方面达到了 0.3 的最佳结果，与现有最新的机器翻译系统相比，这是一个不错的结果。

Abstract: Hate speech identification in social media has become an increasingly
important issue in recent years. In this research, we address two problems: 1)
to detect hate speech in Arabic text, 2) to clean a given text from hate
speech. The meaning of cleaning here is replacing each bad word with stars
based on the number of letters for each word. Regarding the first problem, we
conduct several experiments using deep learning models and transformers to
determine the best model in terms of the F1 score. Regarding second problem, we
consider it as a machine translation task, where the input is a sentence
containing dirty text and the output is the same sentence with masking the
dirty text. The presented methods achieve the best model in hate speech
detection with a 92\% Macro F1 score and 95\% accuracy. Regarding the text
cleaning experiment, the best result in the hate speech masking model reached
0.3 in BLEU score with 1-gram, which is a good result compared with the state
of the art machine translation systems.

</details>


### [162] [Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs](https://arxiv.org/abs/2507.23740)
*Nasim Shirvani-Mahdavi,Devin Wingfield,Amin Ghasemi,Chengkai Li*

Main category: cs.CL

TL;DR: 本研究利用大型语言模型为知识图谱中的逻辑规则生成自然语言解释，并评估了不同提示策略的效果和解释的质量。


<details>
  <summary>Details</summary>
Motivation: 知识图谱（KGs）包含丰富的信息，有助于推理新事实，识别逻辑规则可以提高知识图谱的完整性、检测潜在错误、揭示数据模式并增强推理和解释能力。然而，规则的复杂性和每个知识图谱独特的标签约定给人类理解带来了困难。

Method: 使用AMIE 3.5.1算法从FB15k-237、FB-CVT-REV和FB+CVT-REV三个数据集提取逻辑规则，并探索了包括零样本、少样本、包含变量实体类型和思维链推理在内的各种提示策略。通过对生成解释的正确性、清晰度和幻觉进行全面的]人工评估，并评估了使用大型语言模型作为自动裁判的使用情况。

Result: 研究结果表明，在解释的正确性和清晰度方面表现出有希望的性能，但仍面临一些挑战。

Conclusion: 本研究展示了大型语言模型在生成逻辑规则的自然语言解释方面具有潜力，解释的正确性和清晰度表现出有希望的性能，但仍存在一些挑战有待未来研究。

Abstract: Knowledge graphs (KGs) often contain sufficient information to support the
inference of new facts. Identifying logical rules not only improves the
completeness of a knowledge graph but also enables the detection of potential
errors, reveals subtle data patterns, and enhances the overall capacity for
reasoning and interpretation. However, the complexity of such rules, combined
with the unique labeling conventions of each KG, can make them difficult for
humans to understand. In this paper, we explore the potential of large language
models to generate natural language explanations for logical rules.
Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery
algorithm from the benchmark dataset FB15k-237 and two large-scale datasets,
FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including
zero- and few-shot prompting, including variable entity types, and
chain-of-thought reasoning. We conduct a comprehensive human evaluation of the
generated explanations based on correctness, clarity, and hallucination, and
also assess the use of large language models as automatic judges. Our results
demonstrate promising performance in terms of explanation correctness and
clarity, although several challenges remain for future research. All scripts
and data used in this study are publicly available at
https://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL.

</details>


### [163] [Cascaded Information Disclosure for Generalized Evaluation of Problem Solving Capabilities](https://arxiv.org/abs/2507.23776)
*Yunxiang Yan,Tomohiro Sawada,Kartik Goyal*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While question-answering~(QA) benchmark performance is an automatic and
scalable method to compare LLMs, it is an indirect method of evaluating their
underlying problem-solving capabilities. Therefore, we propose a holistic and
generalizable framework based on \emph{cascaded question disclosure} that
provides a more accurate estimate of the models' problem-solving capabilities
while maintaining the scalability and automation. This approach collects model
responses in a stagewise manner with each stage revealing partial information
about the question designed to elicit generalized reasoning in LLMs. We find
that our approach not only provides a better comparison between LLMs, but also
induces better intermediate traces in models compared to the standard QA
paradigm. We empirically verify this behavior on diverse reasoning and
knowledge-heavy QA datasets by comparing LLMs of varying sizes and families.
Our approach narrows the performance gap observed in the standard QA evaluation
settings, indicating that the prevalent indirect QA paradigm of evaluation
overestimates the differences in performance between models. We further
validate our findings by extensive ablation studies.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [164] [Clock Pulling Enables Maximum-Efficiency Wireless Power Transfer](https://arxiv.org/abs/2507.22907)
*Xianglin Hao,Xiaosheng Wang,ke Yin,Sheng Ren,Chaoqiang Jiang,Jianlong Zou,Tianyu Dong,Chi Kong Tse*

Main category: physics.app-ph

TL;DR: 非线性PT对称在无线能量传输中可能不稳定，但通过一种新的“时钟牵制”机制可以稳定系统并提高效率。


<details>
  <summary>Details</summary>
Motivation: 旨在应对非线性PT对称在非厄米无线能量传输（WPT）系统中带来的理论和实践挑战，并探索提高WPT系统稳定性和传输效率的新方法。

Method: 通过研究非线性非厄米系统中的多稳态现象，发现了非线性时钟牵制机制，并探讨了其在无线能量传输（WPT）系统中的应用，以期稳定传统不稳定状态，提高传输效率。

Result: 成功发现了一种非线性时钟牵制机制，该机制能够强制破坏PT对称性，并可用于稳定WPT系统中具有最大传输效率的不稳定状态，从而提高系统的整体性能。

Conclusion: 该研究揭示了在PT对称相中，非线性PT对称状态也可能不稳定，并发现了一个可以强制破坏PT对称的非线性时钟牵制机制。通过实现该机制，可以有效切换系统稳定性，特别是稳定具有最大传输效率的传统不稳定状态，为非厄米物理学提供了新工具，并有望推动技术进步。

Abstract: Nonlinear parity-time (PT) symmetry in non-Hermitian wireless power transfer
(WPT) systems, while attracting significant attention from both physics and
engineering communities, have posed formidable theoretical and practical
challenges due to their complex dynamical mechanisms. Here, we revisit
multistability in nonlinear non-Hermitian systems and find that the PT-symmetry
state is not always stable even in PT-symmetry phase. We report a discovery on
a nonlinear clock-pulling mechanism, which can forcibly break the PT symmetry.
Proper implementation of this mechanism can switch the system stability,
particularly in stabilizing the conventional unstable state which has the
maximum transfer efficiency for WPT. Our work offers new tools for
non-Hermitian physics and is expected to drive technological progress.

</details>


### [165] [Characterisation of commercial SiC MOSFETs at deep-cryogenic temperatures](https://arxiv.org/abs/2507.23109)
*Megan Powell,Euan Parry,Conor McGeough,Alexander Zotov,Alessandro Rossi*

Main category: physics.app-ph

TL;DR: 在低温下，碳化硅MOSFET的性能会下降，这会影响其在量子设备中的使用。


<details>
  <summary>Details</summary>
Motivation: 评估碳化硅功率MOSFET在低温下的性能，以确定其在CMOS兼容量子电子学中的适用性。

Method: 对商用碳化硅功率MOSFET进行低温（300 K 至 650 mK）性能评估，重点关注阈值电压和亚阈值摆幅的统计研究，评估其在量子电子学中的适用性。

Result: 在低温下，器件性能显著下降，表现为栅极滞后、阈值电压偏移和亚阈值摆幅恶化。

Conclusion: 硅基半导体在低温下表现出不稳定性，这对于量子器件和低温CMOS电子学的可靠应用构成了挑战。

Abstract: Silicon carbide is a wide-bandgap semiconductor with an emerging CMOS
technology platform and it is widely deployed in high power and harsh
environment electronics. This material is also attracting interest for quantum
technologies through its crystal defects, which can act as spin-based qubits or
single-photon sources. In this work, we assess the cryogenic performance of
commercial power MOSFETs to evaluate their suitability for CMOS-compatible
quantum electronics. We perform a statistical study of threshold voltage and
subthreshold swing from 300 K down to 650 mK, focusing on reproducibility and
variability. Our results show significant performance degradation at low
temperatures, including large gate hysteresis, threshold voltage shifts, and
subthreshold swing deterioration. These effects suggest instability in
electrostatic control, likely due to carrier freeze-out and high interface trap
density, which may pose challenges for the reliable use of this transistor
technology towards the realisation of quantum devices or cryo-CMOS electronics.

</details>


### [166] [Wave propagation in an elastic lattice with non-reciprocal stiffness and engineered damping](https://arxiv.org/abs/2507.23761)
*Harshit Kumar Sandhu,Saurav Dutta,Rajesh Chaunsali*

Main category: physics.app-ph

TL;DR: 本文研究了在具有非互易刚度和粘滞阻尼的弹性格栅中的波动力学，引入非耗散性非互易阻尼，实现了刚度和阻尼的分离控制，并带来了新现象。


<details>
  <summary>Details</summary>
Motivation: 非互易波传播实现了定向能量传输。

Method: 本文系统地研究了在结合了非互易刚度和粘滞阻尼的弹性格栅中的波动力学。我们首先阐述了常规阻尼如何抵消系统的增益，然后引入了一种以陀螺阻尼为形式的非耗散性非互易阻尼。

Result: 研究发现，非互易刚度和非互易阻尼的共存产生了分离的控制机制：非互易刚度控制着时间放大率，而非互易阻尼则独立地调节波的群速度和振荡频率。这种分离机制带来了诸如慢传播波的净放大增强以及边界诱导波混合等现象。

Conclusion: 本研究为设计具有更灵活波传播特性控制的有源超材料提供了理论框架。

Abstract: Nonreciprocal wave propagation allows for directional energy transport. In
this work, we systematically investigate wave dynamics in an elastic lattice
that combines nonreciprocal stiffness with viscous damping. After establishing
how conventional damping counteracts the system's gain, we introduce a
non-dissipative form of nonreciprocal damping in the form of gyroscopic
damping. We find that the coexistence of nonreciprocal stiffness and
nonreciprocal damping results in a decoupled control mechanism. The
nonreciprocal stiffness is shown to govern the temporal amplification rate,
while the nonreciprocal damper independently tunes the wave's group velocity
and oscillation frequency. This decoupling gives rise to phenomena such as the
enhancement of net amplification for slower-propagating waves and
boundary-induced wave mixing. These findings provide a theoretical framework
for designing active metamaterials with more versatile control over their wave
propagation characteristics.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [167] [Stabilization of Age-Structured Competing Populations](https://arxiv.org/abs/2507.23013)
*Carina Veil,Miroslav Krstić,Patrick McNamee,Oliver Sawodny*

Main category: eess.SY

TL;DR: 该研究提出了一种通过反步法控制两种竞争捕食者种群动力学模型的方法，实现了系统的稳定，并为未来研究提供了方向。


<details>
  <summary>Details</summary>
Motivation: 尽管耦合年龄结构 IPDE 种群动力学模型在流行病学和生态学等领域有应用，但研究较少。该研究旨在解决两种竞争捕食者种群的动力学问题，并提出一种控制策略。

Method: 该研究将耦合年龄结构 IPDE 种群动力学模型转化为一个包含一个被驱动的 ODE 和两个自治、指数稳定的积分延迟方程（IDEs）的系统，其中 IDEs 作为非线性扰动进入 ODEs。然后利用反步法对 ODEs 进行全局稳定。

Result: 成功设计了控制输入，实现了 ODE 系统的全局稳定，并估计了稳定平衡点的吸引域，该方法在控制具有正值约束的条件下有效。

Conclusion: 该研究通过反步法设计了一种控制输入，以单独收获两种捕食者种群中的一种，并通过间接影响另一种，从而实现了 ODE 系统的全局稳定，并提供了稳定平衡点的吸引域估计。

Abstract: Age-structured models represent the dynamic behaviors of populations over
time and result in integro-partial differential equations (IPDEs). Such
processes arise in biotechnology, economics, demography, and other domains.
Coupled age-structured IPDE population dynamics with two or more species occur
in epidemiology and ecology, but have received little attention thus far. This
work considers an exponentially unstable model of two competing predator
populations, formally referred to in the literature as ''competition''
dynamics. If one were to apply an input that simultaneously harvests both
predator species, one would have control over only the product of the densities
of the species, not over their ratio. Therefore, it is necessary to design a
control input that directly harvests only one of the two predator species,
while indirectly influencing the other via a backstepping approach. The model
is transformed into a system of two coupled ordinary differential equations
(ODEs), of which only one is actuated, and two autonomous, exponentially stable
integral delay equations (IDEs) which enter the ODEs as nonlinear disturbances.
The ODEs are globally stabilized with backstepping and an estimate of the
region of attraction of the asymptotically stabilized equilibrium of the full
IPDE system is provided, under a positivity restriction on control. These
generalizations open exciting possibilities for future research directions,
such as investigating population systems with more than two species.

</details>


### [168] [Terahertz for Radar applications and Wireless Communication](https://arxiv.org/abs/2507.23076)
*Sofiane Latreche,Hocine Bellahsene,Abdelmalik Taleb-Ahmed*

Main category: eess.SY

TL;DR: 本文旨在探讨太赫兹（THz）频段的特性、应用及其在无线通信和雷达系统中的射频传播建模。


<details>
  <summary>Details</summary>
Motivation: 为了利用最新的射频频谱（太赫兹频段）为数据密集型和时间敏感型应用提供无缝集成，本次研究旨在深入了解太赫兹频段的特性和应用。

Method: 本文将探讨太赫兹（THz）频段的特性，并结合无线通信和雷达系统的具体应用场景，分析其独特的效应和参数。研究将深入到射频传播的建模，并在最后对研究结果进行阐述。

Result: 本文将提供太赫兹（THz）频段的特性、无线通信和雷达系统的应用示例，以及对该频段内射频传播的建模分析。

Conclusion: 本研究将阐述太赫兹（THz）频段的特性及其在无线和雷达系统中的应用，并对该领域内射频传播的各个方面进行建模。

Abstract: Technological advancements in the design of electronic and optical materials
have opened up the possibility of utilizing the latest available Radio
Frequency spectrum the Terahertz (THz) band. This band holds great promise for
next-generation wireless systems, which are poised to seamlessly integrate a
wide array of data-intensive and time-sensitive applications. In this article,
we delve into the Terahertz band, providing insights into its properties and
showcasing examples of its applications. We begin by exploring the specific
characteristics of wireless communications and radar systems operating in the
THz band. Subsequently, we analyze various effects and parameters unique to
each of these applications.so we scrutinize the application of Terahertz (THz)
wireless and radar systems, delving into the modeling of various facets of
radio frequency propagation within this domain. The interpretation of our
findings will be presented at the conclusion of this study.

</details>


### [169] [Experimentally-Driven Analysis of Stability in Connected Vehicle Platooning: Insights and Control Strategies](https://arxiv.org/abs/2507.23078)
*Niladri Dutta,Elham Abolfazli,Themistoklis Charalambous*

Main category: eess.SY

TL;DR: 本研究开发了一个实际平台来演示协同自适应巡航控制（CACC）系统，并在仿真和实际车辆测试中证明了其在车辆编队中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有文献中，虽然有许多研究关注模拟车辆编队系统，但缺乏在物理车辆系统或机器人平台上演示这些控制器的研究，本研究旨在填补这一空白。

Method: 开发了一个用于演示协同自适应巡航控制（CACC）系统的实际平台，详细研究了现有的纵向控制器及其在同质车辆编队中的性能，并使用多个自主实验车辆平台拓扑进行了广泛测试。

Result: 仿真和现场测试的结果均证实了所提出的 CACC 编队方法在纵向车辆编队场景中的显著优势。

Conclusion: 该研究通过在实际车辆平台上演示协同自适应巡航控制（CACC）系统，填补了现有文献中关于在物理车辆系统或机器人平台上演示这些控制器的研究空白，展示了其在智能交通系统中的实际应用潜力。

Abstract: This paper presents the development of a tangible platform for demonstrating
the practical implementation of cooperative adaptive cruise control (CACC)
systems, an enhancement to the standard adaptive cruise control (ACC) concept
by means of Vehicle-to-Everything (V2X) communication. It involves a detailed
examination of existing longitudinal controllers and their performance in
homogeneous vehicle platoons. Moreover, extensive tests are conducted using
multiple autonomous experimental vehicle platform topologies to verify the
effectiveness of the controller. The outcomes from both simulations and field
tests affirm the substantial benefits of the proposed CACC platooning approach
in longitudinal vehicle platooning scenarios. This research is crucial due to a
notable gap in the existing literature; while numerous studies focus on
simulated vehicle platooning systems, there is lack of research demonstrating
these controllers on physical vehicle systems or robot platforms. This paper
seeks to fill this gap by providing a practical demonstration of CACC systems
in action, showcasing their potential for real-world application in intelligent
transportation systems.

</details>


### [170] [Robust Control Design and Analysis for Nonlinear Systems with Uncertain Initial Conditions Based on Lifting Linearization](https://arxiv.org/abs/2507.23139)
*Sourav Sinha,Mazen Farhood*

Main category: eess.SY

TL;DR: 提出了一种用于非线性系统的鲁棒控制框架，该框架使用深度学习和LPV模型来处理不确定性，并通过IQC进行分析和调优。


<details>
  <summary>Details</summary>
Motivation: 提出了一种用于非线性系统与不确定初始条件的鲁棒控制综合与分析框架。

Method: 提出了一种基于深度学习的提升方法来近似非线性动力学系统，并用高维空间中的线性参数变化（LPV）状态空间模型同时表征提升状态空间中的不确定初始状态。然后，提供了凸合成条件，为提升的LPV系统生成全状态反馈非平稳LPV（NSLPV）控制器。使用类似于l2诱导范数的性能度量，在存在外源扰动和不确定初始条件的情况下，提供鲁棒性能保证。此外，还开发了一种基于积分二次约束（IQC）理论的鲁棒性分析方法，用于分析和调整合成的控制器，同时考虑与状态测量相关的噪声。该分析方法利用IQC来表征模型参数和扰动输入，以减少保守性。

Result: 该框架能够为非线性系统与不确定初始条件生成鲁棒控制器，并通过示例验证了其有效性。

Conclusion: 该框架的有效性通过两个示例得到了证明。

Abstract: This paper presents a robust control synthesis and analysis framework for
nonlinear systems with uncertain initial conditions. First, a deep
learning-based lifting approach is proposed to approximate nonlinear dynamical
systems with linear parameter-varying (LPV) state-space models in
higher-dimensional spaces while simultaneously characterizing the uncertain
initial states within the lifted state space. Then, convex synthesis conditions
are provided to generate full-state feedback nonstationary LPV (NSLPV)
controllers for the lifted LPV system. A performance measure similar to the
l2-induced norm is used to provide robust performance guarantees in the
presence of exogenous disturbances and uncertain initial conditions. The paper
also includes results for synthesizing full-state feedback LTI controllers and
output feedback NSLPV controllers. Additionally, a robustness analysis approach
based on integral quadratic constraint (IQC) theory is developed to analyze and
tune the synthesized controllers while accounting for noise associated with
state measurements. This analysis approach characterizes model parameters and
disturbance inputs using IQCs to reduce conservatism. Finally, the
effectiveness of the proposed framework is demonstrated through two
illustrative examples.

</details>


### [171] [Foundation Models for Clean Energy Forecasting: A Comprehensive Review](https://arxiv.org/abs/2507.23147)
*Md Meftahul Ferdaus,Tanmoy Dam,Md Rasel Sarkar,Moslem Uddin,Sreenatha G. Anavatti*

Main category: eess.SY

TL;DR: 本文综述了基金模型（FM）在可再生能源（风能和太阳能）预测中的应用，重点介绍了模型架构、训练策略、数据处理和预测准确性改进。文章还讨论了该领域的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着全球能源系统向清洁能源转型，准确的可再生能源发电和需求预测对于有效的电网管理至关重要。基金模型（FM）能够快速处理复杂、高维的时间序列数据，有助于提高可再生能源发电和需求的预测能力。

Method: 本文主要关注基金模型在风能和太阳能预测领域的应用，概述了相关架构、预训练策略、微调方法和数据类型。文章重点介绍了处理大规模、领域特定的Transformer架构，并强调了空间时间相关性、领域知识嵌入以及可再生能源发电的简短和间歇性等特点。此外，还评估了基于FM的预测准确性改进，如多时间尺度预测协调和可再生能源预测不确定性量化，并回顾了长期和多元时间序列预测中存在的挑战和改进领域。

Result: 文章评估了基于FM的预测准确性，包括多时间尺度预测协调和可再生能源预测不确定性量化，并回顾了长期和多元时间序列预测中存在的挑战和改进领域。

Conclusion: 该综述文章区分了清洁能源预测领域中基金模型（FM）的理论与实践，并批判性地评估了FM的优缺点，同时为这一新兴的预测领域提出了未来的研究方向。

Abstract: As global energy systems transit to clean energy, accurate renewable
generation and renewable demand forecasting is imperative for effective grid
management. Foundation Models (FMs) can help improve forecasting of renewable
generation and demand because FMs can rapidly process complex, high-dimensional
time-series data. This review paper focuses on FMs in the realm of renewable
energy forecasting, primarily focusing on wind and solar. We present an
overview of the architectures, pretraining strategies, finetuning methods, and
types of data used in the context of renewable energy forecasting. We emphasize
the role of models that are trained at a large scale, domain specific
Transformer architectures, where attention is paid to spatial temporal
correlations, the embedding of domain knowledge, and also the brief and
intermittent nature of renewable generation. We assess recent FM based
advancements in forecast accuracy such as reconciling predictions over multiple
time scales and quantifying uncertainty in renewable energy forecasting. We
also review existing challenges and areas of improvement in long-term and
multivariate time series forecasting. In this survey, a distinction between
theory and practice is established regarding the use of FMs in the clean energy
forecasting domain. Additionally, it critically assesses the strengths and
weaknesses of FMs while advancing future research direction in this new and
exciting area of forecasting.

</details>


### [172] [Data-Driven Stochastic Control via Non-i.i.d. Trajectories: Foundations and Guarantees](https://arxiv.org/abs/2507.23280)
*Abolfazl Lavaei*

Main category: eess.SY

TL;DR: 本研究提出了一种新的数据驱动框架，用于处理具有未知动态和噪声分布的随机系统。该方法使用非独立同分布（non-i.i.d.）轨迹，并通过随机控制屏障证明提供概率安全保证。与传统的基于场景的方法和鲁棒分析相比，该方法更加灵活且不那么保守。实验结果表明，该方法在实际应用中是有效的。


<details>
  <summary>Details</summary>
Motivation: 克服了现有基于场景的方法在处理非独立同分布（non-i.i.d.）轨迹和未知分布的随机系统时的局限性，以及现有鲁棒分析方法过于保守的问题。

Method: 提出了一种数据驱动框架，利用非独立同分布（non-i.i.d.）轨迹和随机控制屏障证明，将安全保证量化为具有置信水平的概率安全保证，并通过平方和（SOS）优化问题来解决。

Result: 在不知道模型和噪声分布的情况下，该方法成功地为随机系统提供了安全控制器，并在三个随机基准上进行了验证，其中一个案例表明，在现有鲁棒分析方法无法提供安全控制器的情况下，该方法仍然有效。

Conclusion: 该方法在不知道模型和噪声分布的情况下，为随机系统提供了有保证的概率安全保证，并在三个随机基准上得到了验证。

Abstract: This work establishes a crucial step toward advancing data-driven
trajectory-based methods for stochastic systems with unknown mathematical
dynamics. In contrast to scenario-based approaches that rely on independent and
identically distributed (i.i.d.) trajectories, this work develops a data-driven
framework where each trajectory is gathered over a finite horizon and exhibits
temporal dependence-referred to as a non-i.i.d. trajectory. To ensure safety of
dynamical systems using such trajectories, the current body of literature
primarily considers dynamics subject to unknown-but-bounded disturbances, which
facilitates robust analysis. While promising, such bounds may be violated in
practice and the resulting worst-case robust analysis tends to be overly
conservative. To overcome these fundamental challenges, this paper considers
stochastic systems with unknown mathematical dynamics, influenced by process
noise with unknown distributions. In the proposed framework, data is collected
from stochastic systems under multiple realizations within a finite-horizon
experiment, where each realization generates a non-i.i.d. trajectory.
Leveraging the concept of stochastic control barrier certificates constructed
from data, this work quantifies probabilistic safety guarantees with a
certified confidence level. To achieve this, the proposed conditions are
formulated as sum-of-squares (SOS) optimization problems, relying solely on
empirical average of the collected trajectories and statistical features of the
process noise. The efficacy of the approach has been validated on three
stochastic benchmarks with both unknown models and noise distributions. In one
case study, it is shown that while no safety controller exists for the robust
analysis of the system under bounded disturbances, the proposed stochastic
framework offers a safety controller with guaranteed probabilistic
satisfaction.

</details>


### [173] [A Framework for Ethical Decision-Making in Automated Vehicles through Human Reasons-based Supervision](https://arxiv.org/abs/2507.23308)
*Lucas Elbert Suryana,Saeed Rahmani,Simeon Craig Calvert,Arkady Zgonnikov,Bart van Arem*

Main category: eess.SY

TL;DR: 自动驾驶汽车在日常驾驶中面临的伦理困境，需要像人类一样灵活处理规则。本研究提出一种基于人类理由的监督框架，使自动驾驶汽车能够根据安全、效率和监管等人类考量调整决策，以更好地应对现实世界的伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶车辆研究过度关注罕见且极端的“电车难题”，而忽视了日常驾驶中更常见的伦理困境。然而，在实际应用中，自动驾驶汽车往往倾向于严格遵守交通规则，而人类驾驶员则可能根据安全和效率等实际考虑，在特定情况下灵活处理规则。为实现有意义的人类控制，自动驾驶汽车应能理解并响应包括驾驶员、弱势道路使用者和政策制定者在内的人类理由。

Method: 提出了一种新颖的、基于人类理由的监督框架，该框架能够检测自动驾驶汽车行为与预期人类理由不符的情况，并触发轨迹重新考虑。该框架与运动规划和控制系统集成，支持实时适应，从而做出更能反映安全、效率和监管考量的决策。

Result: 模拟结果表明，该方法可以通过在当前轨迹与人类理由不一致时提示重新规划，帮助自动驾驶汽车更有效地应对动态驾驶环境中的伦理挑战。

Conclusion: 该方法为自动驾驶汽车在动态驾驶环境中提供了更具适应性、以人为本的决策路径，能够更好地应对日常的伦理困境。

Abstract: Ethical dilemmas are a common challenge in everyday driving, requiring human
drivers to balance competing priorities such as safety, efficiency, and rule
compliance. However, much of the existing research in automated vehicles (AVs)
has focused on high-stakes "trolley problems," which involve extreme and rare
situations. Such scenarios, though rich in ethical implications, are rarely
applicable in real-world AV decision-making. In practice, when AVs confront
everyday ethical dilemmas, they often appear to prioritise strict adherence to
traffic rules. By contrast, human drivers may bend the rules in
context-specific situations, using judgement informed by practical concerns
such as safety and efficiency. According to the concept of meaningful human
control, AVs should respond to human reasons, including those of drivers,
vulnerable road users, and policymakers. This work introduces a novel human
reasons-based supervision framework that detects when AV behaviour misaligns
with expected human reasons to trigger trajectory reconsideration. The
framework integrates with motion planning and control systems to support
real-time adaptation, enabling decisions that better reflect safety,
efficiency, and regulatory considerations. Simulation results demonstrate that
this approach could help AVs respond more effectively to ethical challenges in
dynamic driving environments by prompting replanning when the current
trajectory fails to align with human reasons. These findings suggest that our
approach offers a path toward more adaptable, human-centered decision-making in
AVs.

</details>


### [174] [Energy management and flexibility quantification in a discrete event distribution grid simulation](https://arxiv.org/abs/2507.23396)
*Sebastian Peter,Daniel Feismann,Johannes Bao,Thomas Oberließen,Christian Rehtanz*

Main category: eess.SY

TL;DR: 该研究提出了一种改进的离散事件模拟方法和通信协议，以应对可再生能源和能源管理系统给配电网带来的挑战，从而简化了模拟并提高了效率。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源和分布式电源（如产消者家庭）的增加，配电网的运行面临新的挑战，能源管理系统（EMS）变得越来越重要。然而，这些系统的战略能源管理增加了配电网运行和规划的复杂性，以及传统时间序列规划方法面临的计算成本。因此，需要一种更有效的方法来模拟和管理这些复杂的系统。

Method: 研究提出了一种增强的离散事件分布电网模拟软件，并使用离散事件系统规范（DES）指定物理模型。此外，还开发了一种新的通信协议。

Result: 通过使用离散事件模拟，可以简化复杂场景的模拟，降低计算成本。所提出的软件增强和通信协议有助于能源管理算法的快速实现和测试，并提高了通信效率。

Conclusion: 该研究通过增强离散事件模拟软件，为能源管理算法的快速实现和测试提供了一个功能丰富的模拟环境。研究还贡献了一种利用离散事件范式、仅在必要时计算灵活性的通信协议。

Abstract: Distribution grid operation faces new challenges caused by a rising share of
renewable energy sources and the introduction of additional types of loads to
the grid. With the increasing adoption of distributed generation and emerging
prosumer households, Energy Management Systems, which manage and apply
flexibility of connected devices, are gaining popularity. While potentially
beneficial to grid capacity, strategic energy management also adds to the
complexity of distribution grid operation and planning processes. Novel
approaches of time-series-based planning likewise face increasingly complex
simulation scenarios and rising computational cost. Discrete event modelling
helps facilitating simulations of such scenarios by restraining computation to
the most relevant points in simulation time. We provide an enhancement of a
discrete event distribution grid simulation software that offers fast
implementation and testing of energy management algorithms, embedded into a
feature-rich simulation environment. Physical models are specified using the
Discrete Event System Specification. Furthermore, we contribute a communication
protocol that makes use of the discrete event paradigm by only computing
flexibility potential when necessary.

</details>


### [175] [Advancing Standard Load Profiles with Data-Driven Techniques and Recent Datasets](https://arxiv.org/abs/2507.23401)
*Jawana Gabrielski,Ulf Häger*

Main category: eess.SY

TL;DR: 由于旧的SLP数据过时，本文利用近期数据更新了SLP，并提出了基于傅立叶级数的新模型。


<details>
  <summary>Details</summary>
Motivation: 德国的SLP使用的是20多年前的旧数据，没有进行调整。不断变化的用电行为导致负载模式与SLP之间的偏差越来越大，因此有必要进行修订。

Method: 使用近期数据创建更新的SLP，验证了SLP方法的假设，并提出改进建议。此外，还提出了一种基于傅立叶级数的模型作为替代SLP模型。对不同模型进行了比较和评估。

Result: 本文利用近期数据创建了更新的SLP，并提出了一种基于傅立叶级数的替代模型，对不同模型进行了比较和评估。

Conclusion: 需要根据新数据修订标准负荷剖面图（SLP），并提出了一种基于傅立叶级数的新模型。

Abstract: Estimating electricity consumption accurately is essential for the planning
and operation of energy systems, as well as for billing processes. Standard
Load Profiles (SLP) are widely used to estimate consumption patterns of
different user groups. However, in Germany these SLP were formulated using
historical data from over 20 years ago and have not been adjusted since.
Changing electricity consumption behaviour, which leads to increasing
deviations between load patterns and SLP, results in a need for a revision
taking into account new data. The growing number of smart meters provides a
large measurement database, which enables more accurate load modelling. This
paper creates updated SLP using recent data. In addition, the assumptions of
the SLP method are validated and improvements are proposed, taking into account
the ease of applicability. Furthermore, a Fourier Series-based model is
proposed as an alternative SLP model. The different models are compared and
evaluated.

</details>


### [176] [Distributionally Robust Cascading Risk Quantification in Multi-Agent Rendezvous: Effects of Time Delay and Network Connectivity](https://arxiv.org/abs/2507.23489)
*Vivek Pandey,Nader Motee*

Main category: eess.SY

TL;DR: 本文提出了一个分布鲁棒风险框架，用于分析多主体集合中的级联故障，并提供了量化和减轻这些故障的方法。


<details>
  <summary>Details</summary>
Motivation: 在时间关键任务（如集合）中实现自主多主体系统的安全性是一个关键挑战，尤其是在处理级联故障时。

Method: 我们引入了一个条件分布鲁棒函数，利用双变量正态分布来量化代理之间的级联效应，并推导出闭式风险表达式。

Result: 我们的方法得出了闭式风险表达式，揭示了时间延迟、噪声统计、通信拓扑和故障模式对集合风险的影响。

Conclusion: 我们的框架通过提供对级联故障的见解，有助于设计能够减轻此类风险的弹性网络。

Abstract: Achieving safety in autonomous multi-agent systems, particularly in
time-critical tasks like rendezvous, is a critical challenge. In this paper, we
propose a distributionally robust risk framework for analyzing cascading
failures in multi-agent rendezvous. To capture the complex interactions between
network connectivity, system dynamics, and communication delays, we use a
time-delayed network model as a benchmark. We introduce a conditional
distributionally robust functional to quantify cascading effects between
agents, utilizing a bi-variate normal distribution. Our approach yields
closed-form risk expressions that reveal the impact of time delay, noise
statistics, communication topology, and failure modes on rendezvous risk. The
insights derived inform the design of resilient networks that mitigate the risk
of cascading failures. We validate our theoretical results through extensive
simulations, demonstrating the effectiveness of our framework.

</details>


### [177] [Asynchronous Grid Connections Providing Fast-Frequency Response: System Integration Study](https://arxiv.org/abs/2507.23571)
*Felix Wald,Amir Sajadi,Barry Mather,Giovanni De Carne*

Main category: eess.SY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents an integration study for a power electronic-based
fast-frequency response technology, an asynchronous grid connection operating
as an aggregator for behindthe-meter resources and distributed generators. Both
technical feasibility and techno-economic viability studies are presented. The
dynamic performance of the fast-frequency response enabled by the asynchronous
grid connection is validated with Power Hardware-in-the-Loop experiments and
transferred to an IEEE 9-bus system in DigSilent PowerFactory for dynamic
stability analysis. We demonstrate that droop-based control enhancements to the
local distributed generators could allow their aggregation to provide
grid-supporting functionalities and participate in the market for ancillary
services. To this end, we performed a long-term simulation embedding the system
within the ancillary service market framework of PJM. The fast-frequency
response regulation is subsequently used to calculate the potential revenue and
project the results on a 15-year investment horizon. Finally, the
techno-economic analysis concludes with recommendations for enhancements to
access the full potential of distributed generators on a technical and
regulatory level.

</details>


### [178] [Tensor-based reduction of linear parameter-varying state-space models](https://arxiv.org/abs/2507.23591)
*Bogoljub Terzin,E. Javier Olucha,Amritam Das,Siep Weiland,Roland Tóth*

Main category: eess.SY

TL;DR: 本文提出了一种新方法，可以同时减小LPV模型的状态和调度信号的维度，解决了现有方法分别处理这些问题而导致的模型高维和保守问题。该方法通过张量分解和扩展的Petrov-Galerkin投影实现，并在仿真中得到了验证。


<details>
  <summary>Details</summary>
Motivation: 为了解决非线性模型转换为LPV形式时产生的模型高维和保守问题，以及控制策略应用中降低计算需求的模型降维需求。

Method: 通过将LPV模型形式化为张量形式，并利用张量分解来寻找状态和调度子空间的主导分量，同时扩展了Petrov-Galerkin投影方法，增加了调度投影，实现了联合降维。为找到合适的子空间，开发了两种方法：基于张量的LPV矩匹配和基于 Proper Orthogonal Decomposition 的方法。

Result: 该方法在两个串联的非线性弹簧阻尼系统中得到了验证，证明了其在模型降维方面的优势，并评估了其可扩展性。

Conclusion: 该研究提出了一种用于线性参数变化（LPV）状态空间模型联合降维（状态和调度信号）的系统化方法。

Abstract: The Linear Parameter-Varying (LPV) framework is a powerful tool for
controlling nonlinear and complex systems, but the conversion of nonlinear
models into LPV forms often results in high-dimensional and overly conservative
LPV models. To be able to apply control strategies, there is often a need for
model reduction in order to reduce computational needs. This paper presents the
first systematic approach for the joint reduction of state order and scheduling
signal dimension of LPV state space models. The existing methods typically
address these reductions separately. By formulating a tensorial form of LPV
models with an affine dependency on the scheduling variables, we leverage
tensor decomposition to find the dominant components of state and scheduling
subspaces. We extend the common Petrov-Galerkin projection approach to LPV
framework by adding a scheduling projection. This extension enables the joint
reduction. To find suitable subspaces for the extended Petrov-Galerkin
projection, we have developed two different methods: tensor-based LPV moment
matching, and an approach through Proper Orthogonal Decomposition. Advantages
of the proposed methods are demonstrated on two different series-interconnected
mass-spring-damper systems with nonlinear springs: one primarily used for
comparison with other methods and a more elaborate higher-order model designed
to assess scalability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [179] [Unifying Post-hoc Explanations of Knowledge Graph Completions](https://arxiv.org/abs/2507.22951)
*Alessandro Lonardi,Samy Badreddine,Tarek R. Besold,Pablo Sanchez Martin*

Main category: cs.AI

TL;DR: 本研究旨在统一知识图谱补全（KGC）的事后可解释性方法，并改进评估协议，以提高研究的可复现性和影响力。


<details>
  <summary>Details</summary>
Motivation: 事后可解释性在知识图谱补全（KGC）中缺乏正式化和一致的评估，阻碍了可复现性和跨研究比较。

Method: 提出一个通用的框架，通过多目标优化来表征事后解释，平衡其有效性和简洁性。这统一了现有的KGC事后可解释性算法及其产生的解释。建议并实证支持使用诸如平均倒数排名（MRR）和Hits@k等流行指标改进评估协议。

Result: 该框架统一了现有的事后可解释性算法和它们产生的解释，并提出了改进的评估协议，强调了可解释性作为满足最终用户查询的能力。

Conclusion: 通过统一方法和完善评估标准，本工作旨在使知识图谱补全（KGC）可解释性研究更具可复现性和影响力。

Abstract: Post-hoc explainability for Knowledge Graph Completion (KGC) lacks
formalization and consistent evaluations, hindering reproducibility and
cross-study comparisons. This paper argues for a unified approach to post-hoc
explainability in KGC. First, we propose a general framework to characterize
post-hoc explanations via multi-objective optimization, balancing their
effectiveness and conciseness. This unifies existing post-hoc explainability
algorithms in KGC and the explanations they produce. Next, we suggest and
empirically support improved evaluation protocols using popular metrics like
Mean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of
interpretability as the ability of explanations to address queries meaningful
to end-users. By unifying methods and refining evaluation standards, this work
aims to make research in KGC explainability more reproducible and impactful.

</details>


### [180] [Data Readiness for Scientific AI at Scale](https://arxiv.org/abs/2507.23018)
*Wesley Brewer,Patrick Widener,Valentine Anantharaj,Feiyi Wang,Tom Beck,Arjun Shankar,Sarp Oral*

Main category: cs.AI

TL;DR: 本研究将数据就绪性原则应用于科学数据集，提出了一个框架来解决可扩展AI培训的挑战，并为标准化、跨域AI for Science支持奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 检查数据就绪性（DRAI）原则如何应用于用于训练基础模型的领导规模的科学数据集。

Method: 分析了气候、核聚变、生物/健康和材料四个代表性领域的典型工作流，以识别常见的预处理模式和特定于域的约束，并引入了一个由数据就绪级别（从原始到AI就绪）和数据处理阶段（从摄取到分片）组成的二维就绪框架，该框架针对高性能计算（HPC）环境进行了定制。

Result: 该框架概述了将科学数据转化为可扩展AI训练的关键挑战，并强调了基于Transformer的生成模型。

Conclusion: 该框架为科学数据的AI就绪性提供了概念成熟度矩阵，并指导基础设施开发，以实现可扩展和可重复的科学AI的标准化、跨域支持。

Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to
leadership-scale scientific datasets used to train foundation models. We
analyze archetypal workflows across four representative domains - climate,
nuclear fusion, bio/health, and materials - to identify common preprocessing
patterns and domain-specific constraints. We introduce a two-dimensional
readiness framework composed of Data Readiness Levels (raw to AI-ready) and
Data Processing Stages (ingest to shard), both tailored to high performance
computing (HPC) environments. This framework outlines key challenges in
transforming scientific data for scalable AI training, emphasizing
transformer-based generative models. Together, these dimensions form a
conceptual maturity matrix that characterizes scientific data readiness and
guides infrastructure development toward standardized, cross-domain support for
scalable and reproducible AI for science.

</details>


### [181] [Chatting with your ERP: A Recipe](https://arxiv.org/abs/2507.23429)
*Jorge Ruiz Gómez,Lidia Andrés Susinos,Jorge Alamo Olivé,Sonia Rey Osorno,Manuel Luis Gonzalez Hernández*

Main category: cs.AI

TL;DR: An LLM agent was created to chat with ERP systems by translating natural language to SQL, using a dual-agent design for better accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable natural language interaction with industrial ERP systems, improving accessibility and usability.

Method: The paper proposes a novel dual-agent architecture with reasoning and critique stages to enhance the reliability of SQL query generation from natural language.

Result: The LLM agent can interpret natural language queries and translate them into executable SQL statements for an ERP system, with improved reliability due to the dual-agent architecture.

Conclusion: The paper successfully designed, implemented, and evaluated an LLM agent capable of interacting with an industrial ERP system using natural language queries and SQL translation.

Abstract: This paper presents the design, implementation, and evaluation behind a Large
Language Model (LLM) agent that chats with an industrial production-grade ERP
system. The agent is capable of interpreting natural language queries and
translating them into executable SQL statements, leveraging open-weight LLMs. A
novel dual-agent architecture combining reasoning and critique stages was
proposed to improve query generation reliability.

</details>


### [182] [FairReason: Balancing Reasoning and Social Bias in MLLMs](https://arxiv.org/abs/2507.23067)
*Zhenyu Pan,Yutong Zhang,Jianshu Zhang,Haoran Lu,Haozheng Luo,Yuwei Han,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: 该研究通过实验确定了在多模态大语言模型（MLLMs）中平衡推理能力和减少社会偏见的最佳策略，即使用强化学习（RL）并以1:4的比例混合去偏见和推理导向的样本，这可以将刻板印象分数降低10%，同时保留88%的原始推理能力。


<details>
  <summary>Details</summary>
Motivation: 为了进一步提升多模态大语言模型（MLLMs）的推理能力，尽管先进的提示和微调技术提高了逻辑准确性，但模型输出常常带有明显的社会偏见。因此，阐明推理能力的提升如何与偏见缓解相互作用，以及两者之间是否存在内在的权衡，仍然是一个悬而未决且紧迫的研究问题。

Method: 该研究首先在相同的条件下对三种偏见缓解策略（监督微调 SFT、知识蒸馏 KD 和基于规则的强化学习 RL）进行了基准测试，以确定它们的基线优势和劣势。在此基础上，研究人员改变了每种范式中以去偏见为重点和以推理为中心的样本的比例，以绘制推理与偏见的权衡关系图。

Result: 研究结果显示，在强化学习范式下，以1:4的比例混合去偏见和推理导向的样本，可以将刻板印象分数降低10%，同时保留模型88%的原始推理能力，这为平衡MLLMs的公平性和能力提供了具体的指导。

Conclusion: 该研究通过实验确定了在多模态大语言模型（MLLMs）中平衡推理能力和减少社会偏见的最佳策略，即使用强化学习（RL）并以1:4的比例混合去偏见和推理导向的样本，这可以将刻板印象分数降低10%，同时保留88%的原始推理能力。

Abstract: Multimodal Large Language Models (MLLMs) already achieve state-of-the-art
results across a wide range of tasks and modalities. To push their reasoning
ability further, recent studies explore advanced prompting schemes and
post-training fine-tuning. Although these techniques improve logical accuracy,
they frequently leave the models' outputs burdened with pronounced social
biases. Clarifying how reasoning gains interact with bias mitigation-and
whether the two objectives inherently trade off-therefore remains an open and
pressing research problem. Our study begins by benchmarking three
bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation
(KD), and rule-based reinforcement learning (RL)-under identical conditions,
establishing their baseline strengths and weaknesses. Building on these
results, we vary the proportion of debias-focused and reasoning-centric samples
within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps
reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement
learning cuts stereotype scores by 10% while retaining 88% of the model's
original reasoning accuracy, offering concrete guidance for balancing fairness
and capability in MLLMs.

</details>


### [183] [Moravec's Paradox: Towards an Auditory Turing Test](https://arxiv.org/abs/2507.23091)
*David Noever,Forrest McKee*

Main category: cs.AI

TL;DR: 人工智能在听觉任务上远不如人类，即使是GPT-4和Whisper也表现糟糕，准确率不足7%，而人类成功率高达52%。研究通过听觉图灵测试发现AI在处理复杂声音场景时存在选择性注意、噪声鲁棒性和上下文适应性问题，需要新的AI方法。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在揭示当前人工智能系统在处理对人类而言轻而易举的听觉任务时存在的灾难性缺陷，借鉴了莫拉维克悖论，即对人类来说简单的任务对机器来说往往很困难，反之亦然。

Method: 研究人员创建了一个包含917个挑战的听觉图灵测试，涵盖了重叠语音、噪声语音、时间失真、空间音频、咖啡店噪音、电话失真和感知错觉七个类别，并评估了包括GPT-4和OpenAI Whisper在内的最先进的音频模型。

Result: 评估结果显示，在这些听觉任务中，人工智能模型的失败率超过93%，表现最好的模型准确率仅为6.9%，而人类的成功率是其7.5倍（52%）。这暴露了人工智能系统在选择性注意、噪声鲁棒性和上下文适应性等方面存在的缺陷。

Conclusion: 目前的人工智能系统在人类轻松完成的听觉任务上表现不佳，尤其是在处理复杂声景方面，这表明现有架构缺乏类似人类的听觉场景分析机制。该研究提出了一个诊断框架来衡量机器在听觉能力上达到人类水平的进展，并强调了将选择性注意、基于物理的音频理解和上下文感知等新方法整合到多模态人工智能系统中的必要性。

Abstract: This research work demonstrates that current AI systems fail catastrophically
on auditory tasks that humans perform effortlessly. Drawing inspiration from
Moravec's paradox (i.e., tasks simple for humans often prove difficult for
machines, and vice versa), we introduce an auditory Turing test comprising 917
challenges across seven categories: overlapping speech, speech in noise,
temporal distortion, spatial audio, coffee-shop noise, phone distortion, and
perceptual illusions. Our evaluation of state-of-the-art audio models including
GPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate
exceeding 93%, with even the best-performing model achieving only 6.9% accuracy
on tasks that humans solved at 7.5 times higher success (52%). These results
expose focusing failures in how AI systems process complex auditory scenes,
particularly in selective attention, noise robustness, and contextual
adaptation. Our benchmark not only quantifies the human-machine auditory gap
but also provides insights into why these failures occur, suggesting that
current architectures lack fundamental mechanisms for human-like auditory scene
analysis. The traditional design of audio CAPTCHAs highlights common filters
that humans evolved but machines fail to select in multimodal language models.
This work establishes a diagnostic framework for measuring progress toward
human-level machine listening and highlights the need for novel approaches
integrating selective attention, physics-based audio understanding, and
context-aware perception into multimodal AI systems.

</details>


### [184] [Argumentatively Coherent Judgmental Forecasting](https://arxiv.org/abs/2507.23163)
*Deniz Gorur,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 研究表明，强制执行论证连贯性可以提高预测准确性，但用户并不普遍认同该属性，因此需要过滤不连贯的意见。


<details>
  <summary>Details</summary>
Motivation: 研究了在人类意见形成围绕预测的论证结构时，从论证角度研究预测属性的有用性，并提出了“论证连贯性”属性，以确保预测者的推理与其预测保持一致。

Method: 定义了“论证连贯性”属性，并通过三种评估来检验其有效性：1. 评估强制执行连贯性对人类和LLM预测准确性的影响；2. 通过众包用户实验，检验用户是否认同该连贯性属性。

Result: 强制执行论证连贯性可以提高人类和LLM的预测准确性。然而，用户并不普遍认同该属性，这表明在实际应用中需要过滤不连贯的意见。

Conclusion: 此研究提出了“论证连贯性”属性，并表明强制执行该属性可以提高人类和大型语言模型（LLM）的预测准确性。然而，用户并不普遍认同这一属性，这表明在基于论证的判断性预测中需要引入过滤不连贯意见的机制。

Abstract: Judgmental forecasting employs human opinions to make predictions about
future events, rather than exclusively historical data as in quantitative
forecasting. When these opinions form an argumentative structure around
forecasts, it is useful to study the properties of the forecasts from an
argumentative perspective. In this paper, we advocate and formally define a
property of argumentative coherence, which, in essence, requires that a
forecaster's reasoning is coherent with their forecast. We then conduct three
evaluations with our notion of coherence. First, we assess the impact of
enforcing coherence on human forecasters as well as on Large Language Model
(LLM)-based forecasters, given that they have recently shown to be competitive
with human forecasters. In both cases, we show that filtering out incoherent
predictions improves forecasting accuracy consistently, supporting the
practical value of coherence in both human and LLM-based forecasting. Then, via
crowd-sourced user experiments, we show that, despite its apparent
intuitiveness and usefulness, users do not generally align with this coherence
property. This points to the need to integrate, within argumentation-based
judgmental forecasting, mechanisms to filter out incoherent opinions before
obtaining group forecasting predictions.

</details>


### [185] [Tractable Responsibility Measures for Ontology-Mediated Query Answering](https://arxiv.org/abs/2507.23191)
*Meghyn Bienvenu,Diego Figueira,Pierre Lafourcade*

Main category: cs.AI

TL;DR: 本文研究了在本体驱动的查询回答中计算 Shapley 值（WSMS）的复杂性。对于可重写的查询，WSMS 计算具有多项式数据复杂度。但当本体支持可达性查询时，计算复杂度会增加。对于 DL-Lite 和结构受限的查询，WSMS 计算是可行的。


<details>
  <summary>Details</summary>
Motivation: 本文旨在量化事实对查询答案的贡献，并研究在本体驱动的查询回答中计算 Shapley 值（WSMS）的复杂性。

Method: 本文研究了本体驱动的查询回答的责任评分计算的复杂性，重点关注基于 Shapley 值的责任度量（WSMS）。通过利用数据库领域的已有研究成果，作者们能够证明，对于那些可以被一阶逻辑重写的本体驱动查询，WSMS 的计算具有多项式数据复杂度。然而，当本体语言可以通过诸如 $\exists R. A \sqsubseteq A$ 之类的公理来编码可达性查询时，该问题的计算复杂度会上升到 "shP"-hard。为了更全面地了解其可行性的边界，作者们进一步探讨了 WSMS 计算的联合复杂度。他们证明，即使在没有本体的情况下，只要本体语言支持合取，那么即使是原子查询也会变得难以处理；同样，对于一些“表现良好”的联合查询，其 WSMS 计算也是难以处理的。与此相反，本研究也取得了一些积极的成果。特别是，对于常见的 DL-Lite 方言，通过仔细分析，作者们识别出了结构受限的联合查询类别（这些查询直观上避免了查询原子之间不期望的交互），这些查询的 WSMS 计算是可行的。

Result: WSMS 计算的复杂性取决于本体语言和查询的结构。对于本体驱动的查询，当本体语言支持可达性查询时，WSMS 计算的复杂性会变得很高。然而，对于 DL-Lite 方言和结构受限的查询，WSMS 计算是可行的。

Conclusion: WSMS 计算的复杂性取决于本体语言和查询的结构。对于本体驱动的查询，当本体语言支持可达性查询时，WSMS 计算的复杂性会变得很高。然而，对于 DL-Lite 方言和结构受限的查询，WSMS 计算是可行的。

Abstract: Recent work on quantitative approaches to explaining query answers employs
responsibility measures to assign scores to facts in order to quantify their
respective contributions to obtaining a given answer. In this paper, we study
the complexity of computing such responsibility scores in the setting of
ontology-mediated query answering, focusing on a very recently introduced
family of Shapley-value-based responsibility measures defined in terms of
weighted sums of minimal supports (WSMS). By exploiting results from the
database setting, we can show that such measures enjoy polynomial data
complexity for classes of ontology-mediated queries that are
first-order-rewritable, whereas the problem becomes "shP"-hard when the
ontology language can encode reachability queries (via axioms like $\exists R.
A \sqsubseteq A$). To better understand the tractability frontier, we next
explore the combined complexity of WSMS computation. We prove that
intractability applies already to atomic queries if the ontology language
supports conjunction, as well as to unions of `well-behaved' conjunctive
queries, even in the absence of an ontology. By contrast, our study yields
positive results for common DL-Lite dialects: by means of careful analysis, we
identify classes of structurally restricted conjunctive queries (which
intuitively disallow undesirable interactions between query atoms) that admit
tractable WSMS computation.

</details>


### [186] [DSBC : Data Science task Benchmarking with Context engineering](https://arxiv.org/abs/2507.23336)
*Ram Mohan Rao Kadiyala,Siddhant Gupta,Jebish Purbey,Giulio Martini,Suman Debnath,Hamza Farooq*

Main category: cs.AI

TL;DR: 本研究构建了一个全面的基准，用于评估三种大型语言模型（Claude-4.0-Sonnet、Gemini-2.5-Flash、OpenAI-o4-Mini）在数据科学任务中的表现，并分析了不同方法（零样本、多步、SmolAgent）和提示问题（数据泄露、指令模糊、温度参数）对性能的影响，旨在为未来数据科学智能体的研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 尽管数据科学智能体在自动化分析任务方面发展迅速，但系统性评估其功效和局限性的基准测试仍然稀少。本研究旨在创建一个全面的基准，以反映用户与数据科学智能体的实际交互。

Method: 评估了三个大型语言模型（Claude-4.0-Sonnet、Gemini-2.5-Flash 和 OpenAI-o4-Mini）的三种方法：零样本（含上下文工程）、多步（含上下文工程）以及使用 SmolAgent。基准测试涵盖了八个数据科学任务类别，并探讨了模型对数据泄露和指令模糊等常见提示问题的敏感性，以及温度参数对模型和方法整体及特定任务结果的影响。

Result: 在八个数据科学任务类别中，评估了 Claude-4.0-Sonnet、Gemini-2.5-Flash 和 OpenAI-o4-Mini 三种模型的性能，同时探讨了零样本、多步和 SmolAgent 等方法，以及模型对数据泄露、指令模糊和温度参数等常见提示问题的敏感性。

Conclusion: 本基准测试的发现揭示了不同模型和方法在实际部署中的性能差异，强调了影响实际部署的关键因素。本研究引入的基准数据集和评估框架旨在为未来研究更强大、更有效的数据科学代理奠定基础。

Abstract: Recent advances in large language models (LLMs) have significantly impacted
data science workflows, giving rise to specialized data science agents designed
to automate analytical tasks. Despite rapid adoption, systematic benchmarks
evaluating the efficacy and limitations of these agents remain scarce. In this
paper, we introduce a comprehensive benchmark specifically crafted to reflect
real-world user interactions with data science agents by observing usage of our
commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,
Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with
context engineering, multi-step with context engineering, and with SmolAgent.
Our benchmark assesses performance across a diverse set of eight data science
task categories, additionally exploring the sensitivity of models to common
prompting issues, such as data leakage and slightly ambiguous instructions. We
further investigate the influence of temperature parameters on overall and
task-specific outcomes for each model and approach. Our findings reveal
distinct performance disparities among the evaluated models and methodologies,
highlighting critical factors that affect practical deployment. The benchmark
dataset and evaluation framework introduced herein aim to provide a foundation
for future research of more robust and effective data science agents.

</details>


### [187] [Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification](https://arxiv.org/abs/2507.23197)
*Yuke Liao,Blaise Genest,Kuldeep Meel,Shaan Aryaman*

Main category: cs.AI

TL;DR: A new method (SAS) for neural network verification significantly reduces the complexity by selecting fewer, more important variables, leading to faster and more accurate results compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: To handle complex instances in neural network verification by breaking down complexity through a divide-and-conquer approach, addressing suboptimal previous attempts in selecting important ReLU variables.

Method: A divide-and-conquer approach using many small partial MILP calls instead of few complex BaB calls. Proposes a novel solution-aware ReLU scoring (SAS) and adapts BaB-SR and BaB-FSB as global ReLU scoring (GS) functions to select important ReLU variables.

Result: SAS reduces the number of binary variables by approximately 6 times with maintained accuracy. The Hybrid MILP approach reduces undecided instances by up to 40% (to 8-15%) with a reasonable runtime (46s-417s) for large CNNs.

Conclusion: SAS is more efficient at selecting ReLU variables, reducing the number of binary variables by around 6 times while maintaining accuracy. Hybrid MILP, combining $\alpha,\beta$-CROWN and partial MILP, is an accurate and efficient verifier, reducing undecided instances to 8-15% with reasonable runtime for large CNNs.

Abstract: To handle complex instances, we revisit a divide-and-conquer approach to
break down the complexity: instead of few complex BaB calls, we rely on many
small {\em partial} MILP calls. The crucial step is to select very few but very
important ReLUs to treat using (costly) binary variables. The previous attempts
were suboptimal in that respect. To select these important ReLU variables, we
propose a novel {\em solution-aware} ReLU scoring ({\sf SAS}), as well as adapt
the BaB-SR and BaB-FSB branching functions as {\em global} ReLU scoring ({\sf
GS}) functions. We compare them theoretically as well as experimentally, and
{\sf SAS} is more efficient at selecting a set of variables to open using
binary variables. Compared with previous attempts, SAS reduces the number of
binary variables by around 6 times, while maintaining the same level of
accuracy. Implemented in {\em Hybrid MILP}, calling first $\alpha,\beta$-CROWN
with a short time-out to solve easier instances, and then partial MILP,
produces a very accurate yet efficient verifier, reducing by up to $40\%$ the
number of undecided instances to low levels ($8-15\%$), while keeping a
reasonable runtime ($46s-417s$ on average per instance), even for fairly large
CNNs with 2 million parameters.

</details>


### [188] [How Far Are AI Scientists from Changing the World?](https://arxiv.org/abs/2507.23276)
*Qiujie Xie,Yixuan Weng,Minjun Zhu,Fuchen Shen,Shulin Huang,Zhen Lin,Jiahui Zhou,Zilan Mao,Zijie Yang,Linyi Yang,Jian Wu,Yue Zhang*

Main category: cs.AI

TL;DR: AI科学家系统正快速发展，虽然已取得初步成果，但要实现颠覆性科学发现，仍需克服关键技术瓶颈，明确发展方向和目标。


<details>
  <summary>Details</summary>
Motivation: 旨在探讨AI科学家系统在改变世界和重塑科学研究范式方面能走多远，为理解当前AI科学家系统的局限性、明确发展方向和最终目标提供清晰的认识。

Method: 对AI科学家系统当前的研究成果进行全面的、以发展前景为导向的回顾，识别关键瓶颈和实现突破性发现所需的关键组成部分。

Result: AI科学家系统已在科学研究中崭露头角，部分成果已在ICLR 2025 workshop被接受，预示着未来可能出现能够发现未知现象的人类水平AI科学家。

Conclusion: AI科学家系统在推动科学发现方面展现出巨大潜力，但距离实现能够解决重大挑战的自主科学发现仍有差距，仍需克服关键瓶颈并明确未来发展目标。

Abstract: The emergence of large language models (LLMs) is propelling automated
scientific discovery to the next level, with LLM-based Artificial Intelligence
(AI) Scientist systems now taking the lead in scientific research. Several
influential works have already appeared in the field of AI Scientist systems,
with AI-generated research papers having been accepted at the ICLR 2025
workshop, suggesting that a human-level AI Scientist capable of uncovering
phenomena previously unknown to humans, may soon become a reality. In this
survey, we focus on the central question: How far are AI scientists from
changing the world and reshaping the scientific research paradigm? To answer
this question, we provide a prospect-driven review that comprehensively
analyzes the current achievements of AI Scientist systems, identifying key
bottlenecks and the critical components required for the emergence of a
scientific agent capable of producing ground-breaking discoveries that solve
grand challenges. We hope this survey will contribute to a clearer
understanding of limitations of current AI Scientist systems, showing where we
are, what is missing, and what the ultimate goals for scientific AI should be.

</details>


### [189] [AI Must not be Fully Autonomous](https://arxiv.org/abs/2507.23330)
*Tosin Adewumi,Lama Alkhaled,Florent Imbert,Hui Han,Nudrat Habib,Karl Löwenmark*

Main category: cs.AI

TL;DR: AI不应完全自主，因为存在风险。本文提出了12个论点和6个反论点，并讨论了自主性、AI和代理的理论。


<details>
  <summary>Details</summary>
Motivation: 为了论证AI不应完全自主的观点，并强调负责任的人类监督对于降低风险至关重要。

Method: 本文讨论了自主性的理论、人工智能和代理。此外，还提供了12个不同的论点和6个带有反驳的论点，并以15条近期人工智能价值观错位和其他风险的证据作为附录。

Result: 作者认为，由于存在固有风险，特别是考虑到人工智能超级智能（ASI）可能在几十年内出现，因此人工智能不应完全自主。

Conclusion: AI不应完全自主，因为存在许多风险，尤其是考虑到人工智能超级智能（ASI）可能在几十年内出现。

Abstract: Autonomous Artificial Intelligence (AI) has many benefits. It also has many
risks. In this work, we identify the 3 levels of autonomous AI. We are of the
position that AI must not be fully autonomous because of the many risks,
especially as artificial superintelligence (ASI) is speculated to be just
decades away. Fully autonomous AI, which can develop its own objectives, is at
level 3 and without responsible human oversight. However, responsible human
oversight is crucial for mitigating the risks. To ague for our position, we
discuss theories of autonomy, AI and agents. Then, we offer 12 distinct
arguments and 6 counterarguments with rebuttals to the counterarguments. We
also present 15 pieces of recent evidence of AI misaligned values and other
risks in the appendix.

</details>


### [190] [LLM4Rail: An LLM-Augmented Railway Service Consulting Platform](https://arxiv.org/abs/2507.23377)
*Zhuo Li,Xianghuai Deng,Chiwei Feng,Hanmeng Li,Shenjie Wang,Haichao Zhang,Teng Jia,Conlin Chen,Louis Linchun Wu,Jia Wang*

Main category: cs.AI

TL;DR: LLM4Rail 是一个利用 LLM 和 QTAO 框架的铁路服务咨询平台，提供个性化餐饮推荐等服务。


<details>
  <summary>Details</summary>
Motivation: 为了满足日益增长的个性化铁路服务的需求，我们开发了 LLM4Rail 平台。

Method: 提出了一种迭代的“问题-思考-行动-观察”（QTAO）提示框架，该框架将推理与面向任务的行动相结合，以检索外部观察并生成准确的响应。同时，构建了包含 25,000 多个条目的中文铁路餐饮（CRFD-25）数据集，并引入了基于 LLM 的零样本会话推荐系统，辅以基于特征相似度的后处理步骤，以确保推荐的餐饮项目与 CRFD-25 数据集一致。

Result: LLM4Rail 能够提供定制化的车票、铁路餐饮推荐、天气信息和闲聊功能，为用户提供个性化的铁路 onboard 餐饮服务。

Conclusion: LLM4Rail 通过 QTAO 框架和 CRFD-25 数据集，为铁路服务咨询提供了一个创新的 LLM 增强平台，实现了个性化的铁路服务。

Abstract: Large language models (LLMs) have significantly reshaped different walks of
business. To meet the increasing demands for individualized railway service, we
develop LLM4Rail - a novel LLM-augmented railway service consulting platform.
Empowered by LLM, LLM4Rail can provide custom modules for ticketing, railway
food & drink recommendations, weather information, and chitchat. In LLM4Rail,
we propose the iterative "Question-Thought-Action-Observation (QTAO)" prompting
framework. It meticulously integrates verbal reasoning with task-oriented
actions, that is, reasoning to guide action selection, to effectively retrieve
external observations relevant to railway operation and service to generate
accurate responses. To provide personalized onboard dining services, we first
construct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible
takeout dataset tailored for railway services. CRFD-25 covers a wide range of
signature dishes categorized by cities, cuisines, age groups, and spiciness
levels. We further introduce an LLM-based zero-shot conversational recommender
for railway catering. To address the unconstrained nature of open
recommendations, the feature similarity-based post-processing step is
introduced to ensure all the recommended items are aligned with CRFD-25
dataset.

</details>


### [191] [Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation](https://arxiv.org/abs/2507.23440)
*Mingzhe Li,Xin Lu,Yanyan Zhao*

Main category: cs.AI

TL;DR: Self-Foveate是一种新的LLM指令合成方法，通过“微散宏”多级注视来提高指令的多样性和难度。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有自动化合成范式在确保合成指令的多样性和难度方面存在的局限性，该研究提出了一种新的方法。

Method: 提出了一种名为Self-Foveate的创新LLM驱动指令合成方法，采用“微散宏”多级注视方法论。

Result: 实验结果表明，Self-Foveate方法在多个无监督语料库和多种模型架构上均表现出有效性和优越性。

Conclusion: Self-Foveate方法通过“微散宏”多级注视方法论，有效指导LLM从无监督文本中挖掘细粒度信息，提高了合成指令的多样性和难度。

Abstract: Large language models (LLMs) with instruction following capabilities have
demonstrated impressive problem-solving abilities. While synthesizing
instructional data from unsupervised text has become a common approach for
training such models, conventional methods rely heavily on human effort for
data annotation. Although existing automated synthesis paradigms have
alleviated this constraint, they still exhibit significant limitations in
ensuring adequate diversity and difficulty of synthesized instructions. To
address these challenges, we propose Self-Foveate, an innovative LLM-driven
method for instruction synthesis. This approach introduces a
"Micro-Scatter-Macro" multi-level foveation methodology that effectively guides
the LLM to deeply excavate fine-grained information embedded in unsupervised
text, thereby enhancing both the diversity and difficulty of synthesized
instructions. Comprehensive experiments across multiple unsupervised corpora
and diverse model architectures validate the effectiveness and superiority of
our proposed method. We publicly release our data and codes:
https://github.com/Mubuky/Self-Foveate

</details>


### [192] [Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery](https://arxiv.org/abs/2507.23488)
*Kacper Kadziolka,Saber Salehkaleybar*

Main category: cs.AI

TL;DR: 因果发现的挑战在于LLM的推理能力，虽然先进模型（如o系列和DeepSeek-R）相比传统模型有显著优势，但结合思维链和思维树的上下文框架可进一步提升近三倍性能，为跨领域因果发现提供通用蓝图。


<details>
  <summary>Details</summary>
Motivation: 旨在探究最先进的推理模型是否能够稳健地执行因果发现任务，因为传统模型在此任务上常常面临过度拟合和数据扰动下近乎随机的性能问题。

Method: 介绍了基于‘思维之树’和‘链式思考’方法的模块化上下文流程，并通过分析推理链的长度、复杂性以及对传统和推理模型的定性和定量比较来探究该流程的影响。

Result: 在Corr2Cause基准测试中，OpenAI的o系列和DeepSeek-R模型家族等推理优先模型取得了比先前方法显著的本地增益，并且所提出的上下文流程比传统基线有了近三倍的改进。

Conclusion: 研究表明，先进的推理模型在因果发现方面取得了重大进展，但通过仔细设计的上下文框架来最大化其能力至关重要，这为跨不同领域的因果发现提供了一个可推广的蓝图。

Abstract: Causal inference remains a fundamental challenge for large language models.
Recent advances in internal reasoning with large language models have sparked
interest in whether state-of-the-art reasoning models can robustly perform
causal discovery-a task where conventional models often suffer from severe
overfitting and near-random performance under data perturbations. We study
causal discovery on the Corr2Cause benchmark using the emergent OpenAI's
o-series and DeepSeek-R model families and find that these reasoning-first
architectures achieve significantly greater native gains than prior approaches.
To capitalize on these strengths, we introduce a modular in-context pipeline
inspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding
nearly three-fold improvements over conventional baselines. We further probe
the pipeline's impact by analyzing reasoning chain length, complexity, and
conducting qualitative and quantitative comparisons between conventional and
reasoning models. Our findings suggest that while advanced reasoning models
represent a substantial leap forward, carefully structured in-context
frameworks are essential to maximize their capabilities and offer a
generalizable blueprint for causal discovery across diverse domains.

</details>


### [193] [Causal Identification of Sufficient, Contrastive and Complete Feature Sets in Image Classification](https://arxiv.org/abs/2507.23497)
*David A Kelly,Hana Chockler*

Main category: cs.AI

TL;DR: 本研究提出了用于图像分类器的因果解释方法，解决了现有方法的局限性，并证明了其形式严谨性、计算效率和黑盒特性。


<details>
  <summary>Details</summary>
Motivation: 现有图像分类器解释算法缺乏形式严谨性，而逻辑解释虽然严谨但计算上受限。本研究旨在提供一种既有形式严谨性又适用于图像分类器的解释方法。

Method: 该研究引入了因果解释，并证明了其与逻辑解释相同的形式属性，同时适用于黑盒算法和图像分类器。研究中还引入了对比因果解释和考虑置信度的因果完整性解释。

Result: 实验结果表明，不同模型在充分性、对比性和完整性方面表现出不同的模式。该算法计算效率高，平均每张 ResNet50 模型图像计算时间为 6 秒，并且是完全的黑盒方法。

Conclusion: 该研究证明了因果解释的优势，并为图像分类器引入了对比因果解释和因果完整性解释。实验结果表明，不同模型在充分性、对比性和完整性方面存在差异，并且该算法高效且完全不依赖模型内部信息。

Abstract: Existing algorithms for explaining the outputs of image classifiers are based
on a variety of approaches and produce explanations that lack formal rigor. On
the other hand, logic-based explanations are formally and rigorously defined
but their computability relies on strict assumptions about the model that do
not hold on image classifiers.
  In this paper, we show that causal explanations, in addition to being
formally and rigorously defined, enjoy the same formal properties as
logic-based ones, while still lending themselves to black-box algorithms and
being a natural fit for image classifiers. We prove formal properties of causal
explanations and introduce contrastive causal explanations for image
classifiers. Moreover, we augment the definition of explanation with confidence
awareness and introduce complete causal explanations: explanations that are
classified with exactly the same confidence as the original image.
  We implement our definitions, and our experimental results demonstrate that
different models have different patterns of sufficiency, contrastiveness, and
completeness. Our algorithms are efficiently computable, taking on average 6s
per image on a ResNet50 model to compute all types of explanations, and are
totally black-box, needing no knowledge of the model, no access to model
internals, no access to gradient, nor requiring any properties, such as
monotonicity, of the model.

</details>


### [194] [DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer](https://arxiv.org/abs/2507.23554)
*Ruoyu Wang,Junda Wu,Yu Xia,Tong Yu,Ryan A. Rossi,Julian McAuley,Lina Yao*

Main category: cs.AI

TL;DR: DICE dynamically selects relevant demonstrations for LLM agents at each reasoning step, improving performance and generalization by decomposing knowledge and using a principled selection criterion. It's a plug-in solution requiring no extra training and shows broad effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing works have shown that the effectiveness of ICL is highly sensitive to the choice of demonstrations, with suboptimal examples often leading to unstable or degraded performance. While prior work has explored example selection, existing approaches typically rely on heuristics or task-specific designs and lack a general, theoretically grounded criterion for what constitutes an effective demonstration across reasoning steps. Therefore, it is non-trivial to develop a principled, general-purpose method for selecting demonstrations that consistently benefit agent performance.

Method: DICE, Dynamic In-Context Example Selection for LLM Agents, is a theoretically grounded ICL framework for agentic tasks that selects the most relevant demonstrations at each step of reasoning. Our approach decomposes demonstration knowledge into transferable and non-transferable components through a causal lens, showing how the latter can introduce spurious dependencies that impair generalization. We further propose a stepwise selection criterion with a formal guarantee of improved agent performance.

Result: Extensive experiments across diverse domains demonstrate our method's effectiveness and generality, highlighting the importance of principled, context-aware demo selection for robust and efficient LLM agents.

Conclusion: DICE is a general, framework-agnostic solution that can be integrated as a plug-in module into existing agentic frameworks without any additional training cost. Extensive experiments across diverse domains demonstrate our method's effectiveness and generality, highlighting the importance of principled, context-aware demo selection for robust and efficient LLM agents.

Abstract: Large language model-based agents, empowered by in-context learning (ICL),
have demonstrated strong capabilities in complex reasoning and tool-use tasks.
However, existing works have shown that the effectiveness of ICL is highly
sensitive to the choice of demonstrations, with suboptimal examples often
leading to unstable or degraded performance. While prior work has explored
example selection, including in some agentic or multi-step settings, existing
approaches typically rely on heuristics or task-specific designs and lack a
general, theoretically grounded criterion for what constitutes an effective
demonstration across reasoning steps. Therefore, it is non-trivial to develop a
principled, general-purpose method for selecting demonstrations that
consistently benefit agent performance. In this paper, we address this
challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a
theoretically grounded ICL framework for agentic tasks that selects the most
relevant demonstrations at each step of reasoning. Our approach decomposes
demonstration knowledge into transferable and non-transferable components
through a causal lens, showing how the latter can introduce spurious
dependencies that impair generalization. We further propose a stepwise
selection criterion with a formal guarantee of improved agent performance.
Importantly, DICE is a general, framework-agnostic solution that can be
integrated as a plug-in module into existing agentic frameworks without any
additional training cost. Extensive experiments across diverse domains
demonstrate our method's effectiveness and generality, highlighting the
importance of principled, context-aware demo selection for robust and efficient
LLM agents.

</details>


### [195] [Semantic Chain-of-Trust: Autonomous Trust Orchestration for Collaborator Selection via Hypergraph-Aided Agentic AI](https://arxiv.org/abs/2507.23565)
*Botao Zhu,Xianbin Wang,Dusit Niyato*

Main category: cs.AI

TL;DR: 通过利用 agentic AI 和超图，在设备空闲时进行有选择的信任评估，从而提高资源利用率和协作效率。


<details>
  <summary>Details</summary>
Motivation: 在协作系统中，任务的有效完成依赖于对分布式协作设备进行特定任务的信任评估。然而，任务的复杂性、分布式设备资源的时空动态性以及不可避免的评估开销，大大增加了信任评估过程的复杂性和资源消耗。因此，不及时或过于频繁的信任评估会降低受限资源的使用率，对协作任务的执行产生负面影响。

Method: 本文提出了一种基于语义信任链的自主信任编排方法，该方法利用了agentic AI和超图来建立和维护设备之间的信任关系。通过自主感知、任务分解和语义推理，agentic AI仅在设备空闲期间根据历史表现数据对协作者进行信任评估。此外，通过分析资源能力与任务需求之间的匹配度，对协作者资源进行特定任务的信任评估。通过维护嵌入了每个设备信任语义的信任超图，agentic AI实现了协作者的层次化管理，并根据信任语义识别需要进行信任评估的协作者。最后，多个设备的本地信任超图可以链接在一起，以支持多跳协作。

Result: 实验结果表明，所提出的方法实现了资源高效的信任评估。

Conclusion: 所提出的方法通过基于历史表现的语义信任链，在设备空闲期间自主进行信任评估，并在协作任务执行期间平衡开销和信任准确性，从而实现了资源高效的信任评估。

Abstract: In collaborative systems, the effective completion of tasks hinges on
task-specific trust evaluations of potential devices for distributed
collaboration. However, the complexity of tasks, the spatiotemporal dynamism of
distributed device resources, and the inevitable assessment overhead
dramatically increase the complexity and resource consumption of the trust
evaluation process. As a result, ill-timed or overly frequent trust evaluations
can reduce utilization rate of constrained resources, negatively affecting
collaborative task execution. To address this challenge, this paper proposes an
autonomous trust orchestration method based on a new concept of semantic
chain-of-trust. Our technique employs agentic AI and hypergraph to establish
and maintain trust relationships among devices. By leveraging its strengths in
autonomous perception, task decomposition, and semantic reasoning, we propose
agentic AI to perceive device states and autonomously perform trust evaluations
of collaborators based on historical performance data only during device idle
periods, thereby enabling efficient utilization of distributed resources. In
addition, agentic AI performs task-specific trust evaluations on collaborator
resources by analyzing the alignment between resource capabilities and task
requirements. Moreover, by maintaining a trust hypergraph embedded with trust
semantics for each device, agentic AI enables hierarchical management of
collaborators and identifies collaborators requiring trust evaluation based on
trust semantics, thereby achieving a balance between overhead and trust
accuracy. Furthermore, local trust hypergraphs from multiple devices can be
chained together to support multi-hop collaboration, enabling efficient
coordination in large-scale systems. Experimental results demonstrate that the
proposed method achieves resource-efficient trust evaluation.

</details>


### [196] [MemoCue: Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying](https://arxiv.org/abs/2507.23633)
*Qian Zhao,Zhuo Sun,Bin Guo,Zhiwen Yu*

Main category: cs.AI

TL;DR: This paper introduces MemoCue, a novel agent-assisted memory recall method that uses strategy guidance to generate cue-rich queries, overcoming the limitations of conventional methods. MemoCue outperforms existing LLM-based approaches in recall inspiration and demonstrates practical advantages in memory-recall applications.


<details>
  <summary>Details</summary>
Motivation: Conventional agent-assisted memory recall methods are limited by the size of the memory module. Inspired by memory theories that suggest proactive activation of relevant memories through cues, this paper aims to develop a method where an agent can transform original queries into cue-rich ones using designed strategies to aid memory recall.

Method: The paper proposes a strategy-guided agent-assisted memory recall method involving a Recall Router framework. This framework uses a 5W Recall Map to classify memory queries into five scenarios and defines fifteen recall strategy patterns. A hierarchical recall tree combined with the Monte Carlo Tree Search algorithm is used to optimize strategy selection and response generation. Instruction tuning and fine-tuning of LLMs are employed to develop the MemoCue agent.

Result: Experiments on three representative datasets show that MemoCue surpasses LLM-based methods by 17.74% in recall inspiration. Human evaluation further highlights its advantages in memory-recall applications.

Conclusion: MemoCue, an agent that excels in providing memory-inspired responses, surpasses current LLM-based methods by 17.74% in recall inspiration, with further human evaluation highlighting its advantages in memory-recall applications.

Abstract: Agent-assisted memory recall is one critical research problem in the field of
human-computer interaction. In conventional methods, the agent can retrieve
information from its equipped memory module to help the person recall
incomplete or vague memories. The limited size of memory module hinders the
acquisition of complete memories and impacts the memory recall performance in
practice. Memory theories suggest that the person's relevant memory can be
proactively activated through some effective cues. Inspired by this, we propose
a novel strategy-guided agent-assisted memory recall method, allowing the agent
to transform an original query into a cue-rich one via the judiciously designed
strategy to help the person recall memories. To this end, there are two key
challenges. (1) How to choose the appropriate recall strategy for diverse
forgetting scenarios with distinct memory-recall characteristics? (2) How to
obtain the high-quality responses leveraging recall strategies, given only
abstract and sparsely annotated strategy patterns? To address the challenges,
we propose a Recall Router framework. Specifically, we design a 5W Recall Map
to classify memory queries into five typical scenarios and define fifteen
recall strategy patterns across the corresponding scenarios. We then propose a
hierarchical recall tree combined with the Monte Carlo Tree Search algorithm to
optimize the selection of strategy and the generation of strategy responses. We
construct an instruction tuning dataset and fine-tune multiple open-source
large language models (LLMs) to develop MemoCue, an agent that excels in
providing memory-inspired responses. Experiments on three representative
datasets show that MemoCue surpasses LLM-based methods by 17.74% in recall
inspiration. Further human evaluation highlights its advantages in
memory-recall applications.

</details>


### [197] [Personalized Education with Ranking Alignment Recommendation](https://arxiv.org/abs/2507.23664)
*Haipeng Liu,Yuxuan Liu,Ting Long*

Main category: cs.AI

TL;DR: A new method called RAR improves personalized question recommendation by incorporating collaborative ideas into exploration, leading to better results in limited training episodes.


<details>
  <summary>Details</summary>
Motivation: Previous methods struggle with efficient exploration, failing to identify the best questions for each student during training.

Method: Ranking Alignment Recommendation (RAR) incorporates collaborative ideas into the exploration mechanism, enabling more efficient exploration within limited training episodes.

Result: Experiments show that RAR effectively improves recommendation performance.

Conclusion: RAR effectively improves recommendation performance and can be applied to any RL-based question recommender.

Abstract: Personalized question recommendation aims to guide individual students
through questions to enhance their mastery of learning targets. Most previous
methods model this task as a Markov Decision Process and use reinforcement
learning to solve, but they struggle with efficient exploration, failing to
identify the best questions for each student during training. To address this,
we propose Ranking Alignment Recommendation (RAR), which incorporates
collaborative ideas into the exploration mechanism, enabling more efficient
exploration within limited training episodes. Experiments show that RAR
effectively improves recommendation performance, and our framework can be
applied to any RL-based question recommender. Our code is available in
https://github.com/wuming29/RAR.git.

</details>


### [198] [TextQuests: How Good are LLMs at Text-Based Video Games?](https://arxiv.org/abs/2507.23701)
*Long Phan,Mantas Mazeika,Andy Zou,Dan Hendrycks*

Main category: cs.AI

TL;DR: TextQuests is a new benchmark using interactive fiction games to test AI agents' ability to reason and solve problems autonomously over long periods without external tools.


<details>
  <summary>Details</summary>
Motivation: Existing agent benchmarks do not fully capture an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. There is a need to spur the development of agents capable of more robust intrinsic reasoning over long horizons.

Method: Introduced TextQuests, a benchmark based on the Infocom suite of interactive fiction games. This benchmark assesses an LLM agent's capacity for self-contained problem-solving by preventing the use of external tools, focusing on intrinsic long-context reasoning in an exploratory setting that requires trial-and-error learning and sustained problem-solving within a single session.

Result: TextQuests benchmark, based on interactive fiction games, serves as an effective proxy for evaluating AI agents on focused, stateful tasks, assessing their intrinsic long-context reasoning capabilities.

Conclusion: TextQuests benchmark aims to promote the development of AI agents with strong intrinsic reasoning capabilities over extended contexts. It offers a new way to evaluate LLM agents' self-contained problem-solving abilities in complex, exploratory environments.

Abstract: Evaluating AI agents within complex, interactive environments that mirror
real-world challenges is critical for understanding their practical
capabilities. While existing agent benchmarks effectively assess skills like
tool use or performance on structured tasks, they often do not fully capture an
agent's ability to operate autonomously in exploratory environments that demand
sustained, self-directed reasoning over a long and growing context. To spur the
development of agents capable of more robust intrinsic reasoning over long
horizons, we introduce TextQuests, a benchmark based on the Infocom suite of
interactive fiction games. These text-based adventures, which can take human
players over 30 hours and require hundreds of precise actions to solve, serve
as an effective proxy for evaluating AI agents on focused, stateful tasks. The
benchmark is specifically designed to assess an LLM agent's capacity for
self-contained problem-solving by precluding the use of external tools, thereby
focusing on intrinsic long-context reasoning capabilities in an exploratory
environment characterized by the need for trial-and-error learning and
sustained problem-solving within a single interactive session. We release
TextQuests at https://textquests.ai.

</details>


### [199] [Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving](https://arxiv.org/abs/2507.23726)
*Luoxin Chen,Jinming Gu,Liankai Huang,Wenhao Huang,Zhicheng Jiang,Allan Jie,Xiaoran Jin,Xing Jin,Chenggang Li,Kaijing Ma,Cheng Ren,Jiawei Shen,Wenlei Shi,Tong Sun,He Sun,Jiahui Wang,Siran Wang,Zhihong Wang,Chenrui Wei,Shufa Wei,Yonghui Wu,Yuchen Wu,Yihang Xia,Huajian Xin,Fan Yang,Huaiyuan Ying,Hongyi Yuan,Zheng Yuan,Tianyang Zhan,Chi Zhang,Yue Zhang,Ge Zhang,Tianyun Zhao,Jianqiu Zhao,Yichi Zhou,Thomas Hanwen Zhu*

Main category: cs.AI

TL;DR: 大型语言模型结合形式化验证（Lean）和长链式思考，通过Seed-Prover和Seed-Geometry在IMO级别问题和几何推理上取得突破性进展，并在IMO 2025竞赛中大放异彩。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在数学推理方面表现出强大的能力，但在定理证明方面仍然存在挑战，因为仅使用自然语言时缺乏明确的监督信号。而Lean等特定领域语言通过形式化验证提供清晰的监督，能够有效地进行强化学习训练。

Method: 提出了一种名为Seed-Prover的引理风格全推理模型，该模型能够基于Lean反馈、已证明的引理和自我总结来迭代改进其证明。为了解决Lean在几何方面支持不足的问题，引入了Seed-Geometry几何推理引擎。

Result: Seed-Prover能够证明78.1%的已形式化的IMO问题，在MiniF2F评测中达到饱和，并在PutnamBench上 đạt 超过50%的准确率，显著优于先前最先进的方法。Seed-Geometry在形式化几何推理方面也优于现有方法。使用这两个系统参加IMO 2025，成功证明了6道题中的5道。

Conclusion: 该工作展示了形式化验证与长链式思考推理相结合在自动化数学推理方面的显著进展，并介绍了Seed-Prover和Seed-Geometry两个模型，其中Seed-Prover在IMO和PutnamBench等评测中表现出色，Seed-Geometry在形式化几何推理方面也优于现有方法。

Abstract: LLMs have demonstrated strong mathematical reasoning abilities by leveraging
reinforcement learning with long chain-of-thought, yet they continue to
struggle with theorem proving due to the lack of clear supervision signals when
solely using natural language. Dedicated domain-specific languages like Lean
provide clear supervision via formal verification of proofs, enabling effective
training through reinforcement learning. In this work, we propose
\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover
can iteratively refine its proof based on Lean feedback, proved lemmas, and
self-summarization. To solve IMO-level contest problems, we design three
test-time inference strategies that enable both deep and broad reasoning.
Seed-Prover proves $78.1\%$ of formalized past IMO problems, saturates MiniF2F,
and achieves over 50\% on PutnamBench, outperforming the previous
state-of-the-art by a large margin. To address the lack of geometry support in
Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which
outperforms previous formal geometry engines. We use these two systems to
participate in IMO 2025 and fully prove 5 out of 6 problems. This work
represents a significant advancement in automated mathematical reasoning,
demonstrating the effectiveness of formal verification with long
chain-of-thought reasoning.

</details>


### [200] [CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks](https://arxiv.org/abs/2507.23751)
*Ping Yu,Jack Lanchantin,Tianlu Wang,Weizhe Yuan,Olga Golovneva,Ilia Kulikov,Sainbayar Sukhbaatar,Jason Weston,Jing Xu*

Main category: cs.AI

TL;DR: CoT-Self-Instruct是一种新的合成数据生成方法，通过结合思维链（CoT）和自指令技术，提高了LLM在推理和指令遵循任务上的性能，并在多项基准测试中取得优异成果。


<details>
  <summary>Details</summary>
Motivation: 为了提高LLM在可验证推理和指令遵循任务上的性能，提出了一种新的合成数据生成方法。

Method: 提出CoT-Self-Instruct方法，该方法首先利用思维链（CoT）对给定种子任务进行推理和规划，然后生成新的、质量和复杂度相似的合成提示用于LLM训练，最后通过自动指标进行高质量数据筛选。

Result: 在可验证推理任务上，CoT-Self-Instruct生成的合成数据在MATH500、AMC23、AIME24和GPQA-Diamond等基准测试中显著优于s1k和OpenMathReasoning等现有数据集。在非可验证指令遵循任务上，该方法在AlpacaEval 2.0和Arena-Hard上均优于人类或标准的自指令提示。

Conclusion: CoT-Self-Instruct 在可验证推理和指令遵循任务上均表现出色，显著优于现有数据集和方法。

Abstract: We propose CoT-Self-Instruct, a synthetic data generation method that
instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the
given seed tasks, and then to generate a new synthetic prompt of similar
quality and complexity for use in LLM training, followed by filtering for
high-quality data with automatic metrics. In verifiable reasoning, our
synthetic data significantly outperforms existing training datasets, such as
s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For
non-verifiable instruction-following tasks, our method surpasses the
performance of human or standard self-instruct prompts on both AlpacaEval 2.0
and Arena-Hard.

</details>


### [201] [SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model](https://arxiv.org/abs/2507.23773)
*Mingkai Deng,Jinyu Hou,Yilin Shen,Hongxia Jin,Graham Neubig,Zhiting Hu,Eric Xing*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: AI agents built on large language models (LLMs) hold enormous promise, but
current practice focuses on a one-task-one-agent approach, which not only falls
short of scalability and generality, but also suffers from the fundamental
limitations of autoregressive LLMs. On the other hand, humans are general
agents who reason by mentally simulating the outcomes of their actions and
plans. Moving towards a more general and powerful AI agent, we introduce
SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based
on a principled formulation of optimal agent in any environment, \modelname
overcomes the limitations of autoregressive reasoning by introducing a world
model for planning via simulation. The generalized world model is implemented
using LLM, which can flexibly plan in a wide range of environments using the
concept-rich latent space of natural language. Experiments on difficult web
browsing tasks show that \modelname improves the success of flight search from
0\% to 32.2\%. World-model-based planning, in particular, shows consistent
advantage of up to 124\% over autoregressive planning, demonstrating the
advantage of world model simulation as a reasoning paradigm. We are excited
about the possibility for training a single, general agent model based on LLMs
that can act superintelligently in all environments. To start, we make SimuRA,
a web-browsing agent built on \modelname with pretrained LLMs, available as a
research demo for public testing.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [202] [LLMs Between the Nodes: Community Discovery Beyond Vectors](https://arxiv.org/abs/2507.22955)
*Ekta Gujral,Apurva Sinha*

Main category: cs.SI

TL;DR: This paper introduces CommLLM, a framework using GPT-4o to improve community detection in social networks by combining graph structure with LLM insights, showing promising results for small to medium-sized graphs.


<details>
  <summary>Details</summary>
Motivation: To explore the integration of semantic and contextual information from Large Language Models (LLMs) into community detection in social network graphs, moving beyond traditional reliance on graph structural properties.

Method: A two-step framework called CommLLM is proposed, which utilizes the GPT-4o model and prompt-based reasoning to combine language model outputs with graph structure. This approach is evaluated on six real-world social network datasets using metrics like NMI, ARI, VOI, and cluster purity.

Result: The study found that LLM-based approaches, particularly CommLLM with instruction-tuned models and engineered prompts, can effectively detect communities in social graphs, improving accuracy and coherence. The performance is notable in small to medium-sized graphs.

Conclusion: LLMs, especially when combined with graph-aware strategies, demonstrate significant potential for community detection in small to medium-sized social graphs. Tailoring LLM interactions to graph data structures is crucial for improving accuracy and coherence.

Abstract: Community detection in social network graphs plays a vital role in uncovering
group dynamics, influence pathways, and the spread of information. Traditional
methods focus primarily on graph structural properties, but recent advancements
in Large Language Models (LLMs) open up new avenues for integrating semantic
and contextual information into this task. In this paper, we present a detailed
investigation into how various LLM-based approaches perform in identifying
communities within social graphs. We introduce a two-step framework called
CommLLM, which leverages the GPT-4o model along with prompt-based reasoning to
fuse language model outputs with graph structure. Evaluations are conducted on
six real-world social network datasets, measuring performance using key metrics
such as Normalized Mutual Information (NMI), Adjusted Rand Index (ARI),
Variation of Information (VOI), and cluster purity. Our findings reveal that
LLMs, particularly when guided by graph-aware strategies, can be successfully
applied to community detection tasks in small to medium-sized graphs. We
observe that the integration of instruction-tuned models and carefully
engineered prompts significantly improves the accuracy and coherence of
detected communities. These insights not only highlight the potential of LLMs
in graph-based research but also underscore the importance of tailoring model
interactions to the specific structure of graph data.

</details>


### [203] [Constructing and Sampling Directed Graphs with Linearly Rescaled Degree Matrices](https://arxiv.org/abs/2507.23025)
*Yunxiang Yan,Meng Jiang*

Main category: cs.SI

TL;DR: 提出了一种新的有向图采样框架，通过重缩放JDM和DCM来构建采样图，该算法能保留度分布并具有良好的实际性能。


<details>
  <summary>Details</summary>
Motivation: 分析大型有向网络通常耗时且成本高昂，因为许多图算法的复杂度与图的大小呈多项式关系。因此，能够生成保留原始图属性的采样算法对于加快分析过程至关重要。

Method: 提出了一种采样有向图的框架，通过对联合度矩阵（JDM）和度相关矩阵（DCM）进行线性重缩放来构建采样图。在此框架下，提出了一种新的图采样算法，该算法可证明地保留了入度和出度分布，并证明了JDM和DCM的偏差上限。

Result: 提出的算法可以保留图的入度和出度分布，并且JDM和DCM的偏差与它们的稀疏性负相关。在实际的大型有向网络上，该算法表现出优于理论的性能。

Conclusion: 该研究提出了一种采样有向图的框架，通过对联合度矩阵（JDM）和度相关矩阵（DCM）进行线性重缩放来构建采样图。实验表明，JDM和DCM的非零条目数量相对于图的边和节点数量而言非常少。该框架提出了一种新的图采样算法，可证明地保留了图的入度和出度分布，并被证明了JDM和DCM的偏差上限，且这些偏差与JDM和DCM的稀疏性负相关，因此在实际的大型有向网络上表现优于理论预期。

Abstract: In recent years, many large directed networks such as online social networks
are collected with the help of powerful data engineering and data storage
techniques. Analyses of such networks attract significant attention from both
the academics and industries. However, analyses of large directed networks are
often time-consuming and expensive because the complexities of a lot of graph
algorithms are often polynomial with the size of the graph. Hence, sampling
algorithms that can generate graphs preserving properties of original graph are
of great importance because they can speed up the analysis process. We propose
a promising framework to sample directed graphs: Construct a sample graph with
linearly rescaled Joint Degree Matrix (JDM) and Degree Correlation Matrix
(DCM). Previous work shows that graphs with the same JDM and DCM will have a
range of very similar graph properties. We also conduct experiments on
real-world datasets to show that the numbers of non-zero entries in JDM and DCM
are quite small compared to the number of edges and nodes. Adopting this
framework, we propose a novel graph sampling algorithm that can provably
preserves in-degree and out-degree distributions, which are two most
fundamental properties of a graph. We also prove the upper bound for deviations
in the joint degree distribution and degree correlation distribution, which
correspond to JDM and DCM. Besides, we prove that the deviations in these
distributions are negatively correlated with the sparsity of the JDM and DCM.
Considering that these two matrices are always quite sparse, we believe that
proposed algorithm will have a better-than-theory performance on real-world
large directed networks.

</details>


### [204] [Countering the Forgetting of Novel Health Information with 'Social Boosting'](https://arxiv.org/abs/2507.23148)
*Vaibhav Krishna,Nicholas A. Christakis*

Main category: cs.SI

TL;DR: Social connections boost the effectiveness of health knowledge interventions.


<details>
  <summary>Details</summary>
Motivation: The study investigates the role of detailed social structure in local villages in enhancing the effectiveness and retention of knowledge interventions, particularly in mitigating the adverse effects of low-quality or false information.

Method: The study evaluated the effectiveness of knowledge interventions on maternal and child healthcare information through a 22-month in-home intervention in 110 isolated Honduran villages. It specifically examined the effect of friendship ties on knowledge retention among targeted individuals.

Result: Well-connected individuals within a social network experience enhanced effectiveness of knowledge interventions. They are more likely to internalize and retain information and reinforce it in others due to increased opportunities for social interaction, a mechanism referred to as 

Conclusion: This study found that individuals who are well-connected within a social network experience enhanced effectiveness of knowledge interventions. These individuals are more likely to internalize and retain information and reinforce it in others due to increased opportunities for social interaction, a mechanism referred to as 

Abstract: To mitigate the adverse effects of low-quality or false information, studies
have shown the effectiveness of various intervention techniques through
debunking or so-called pre-bunking. However, the effectiveness of such
interventions can decay. Here, we investigate the role of the detailed social
structure of the local villages within which the intervened individuals live,
which provides opportunities for the targeted individuals to discuss and
internalize new knowledge. We evaluated this with respect to a critically
important topic, information about maternal and child health care, delivered
via a 22-month in-home intervention. Specifically, we examined the effect of
having friendship ties on the retention of knowledge interventions among
targeted individuals in 110 isolated Honduran villages. We hypothesize that
individuals who receive specific knowledge can internalize and consolidate this
information by engaging in social interactions where, for instance, they have
an opportunity to discuss it with others in the process. The opportunity to
explain information to others (knowledge sharing) promotes deeper cognitive
processing and elaborative encoding, which ultimately enhances memory
retention. We found that well-connected individuals within a social network
experience an enhanced effectiveness of knowledge interventions. These
individuals may be more likely to internalize and retain the information and
reinforce it in others, due to increased opportunities for social interaction
where they teach others or learn from them, a mechanism we refer to as "social
boosting". These findings underscore the role of social interactions in
reinforcing health knowledge interventions over the long term. We believe these
findings would be of interest to the health policy, the global health
workforce, and healthcare professionals focusing on disadvantaged populations
and UN missions on infodemics.

</details>


### [205] [Empirical cross-system meta-analysis of long-term transmission grid evolution](https://arxiv.org/abs/2507.23546)
*Bálint Hartmann,Michelle T. Cirunay*

Main category: cs.SI

TL;DR: Real-world grids have untapped flexibility due to limited studies on their evolution.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the under-utilization of grid-side flexibility due to a lack of empirical studies.

Method: This paper will analyze empirical data on how real-world grids evolve.

Result: The result is expected to provide insights into the evolution of real-world grids and the potential of grid-side flexibility.

Conclusion: This paper aims to explore the potential of grid-side flexibility by examining the evolution of real-world grids.

Abstract: The potential of grid-side flexibility, the latent ability to reconfigure
transmission network topology remains under-used partly because of the lack of
empirical studies on how real-world grids evolve.

</details>


### [206] [Exploring Left-Wing Extremism on the Decentralized Web: An Analysis of Lemmygrad.ml](https://arxiv.org/abs/2507.23699)
*Utkucan Balci,Michael Sirivianos,Jeremy Blackburn*

Main category: cs.SI

TL;DR: 本研究分析了Lemmy平台Lemmygrad.ml实例中的左翼极端主义。研究发现，在相关社群迁移后，用户活动和内容毒性显著增加，并出现了支持威权、支持俄乌战争以及反犹太内容。这强调了在去中心化社交网络中分析政治极端主义的必要性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究去中心化社交媒体平台Lemmy的Lemmygrad.ml实例中左翼极端主义的存在情况，研究时间从2019年Lemmygrad.ml上线至r/GenZedong和r/GenZhou被禁止一个月后。

Method: 本研究通过时间分析Lemmygrad.ml的用户活动，并衡量高度滥用或仇恨内容的程度。此外，研究利用基于Transformer的主题建模方法探讨了用户帖子的内容。

Result: 研究结果表明，在r/GenZedong和r/GenZhou迁移到Lemmygrad.ml后，用户活动和内容毒性水平显著增加。研究还识别出支持威权政权、认可俄罗斯入侵乌克兰以及包含反犹太复国主义和反犹太主义内容的相关帖子。

Conclusion: 本研究旨在探究去中心化社交媒体平台Lemmy的Lemmygrad.ml实例中左翼极端主义的存在情况，研究时间从2019年Lemmygrad.ml上线至r/GenZedong和r/GenZhou被禁止一个月后。研究通过时间分析Lemmygrad.ml的用户活动，并衡量高度滥用或仇恨内容的程度。此外，研究利用基于Transformer的主题建模方法探讨了用户帖子的内容。研究结果表明，在r/GenZedong和r/GenZhou迁移到Lemmygrad.ml后，用户活动和内容毒性水平显著增加。研究还识别出支持威权政权、认可俄罗斯入侵乌克兰以及包含反犹太复国主义和反犹太主义内容的相关帖子。总的来说，本研究的结果有助于更深入地理解去中心化社交网络中的政治极端主义，并强调了在研究中分析政治光谱两端极端情况的必要性。

Abstract: This study investigates the presence of left-wing extremism on the
Lemmygrad.ml instance of the decentralized social media platform Lemmy, from
its launch in 2019 up to a month after the bans of the subreddits r/GenZedong
and r/GenZhou. We conduct a temporal analysis on Lemmygrad.ml's user activity,
with also measuring the degree of highly abusive or hateful content.
Furthermore, we explore the content of their posts using a transformer-based
topic modeling approach. Our findings reveal a substantial increase in user
activity and toxicity levels following the migration of these subreddits to
Lemmygrad.ml. We also identify posts that support authoritarian regimes,
endorse the Russian invasion of Ukraine, and feature anti-Zionist and
antisemitic content. Overall, our findings contribute to a more nuanced
understanding of political extremism within decentralized social networks and
emphasize the necessity of analyzing both ends of the political spectrum in
research.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [207] [WiRM: Wireless Respiration Monitoring Using Conjugate Multiple Channel State Information and Fast Iterative Filtering in Wi-Fi Systems](https://arxiv.org/abs/2507.23419)
*James Rhodes,Lawrence Ong,Duy T. Ngo*

Main category: cs.ET

TL;DR: WiRM是一种创新的两阶段非接触式呼吸监测技术。它通过优化呼吸频率估计（RMSE降低38%）和改进呼吸波形还原（相关性提高178.3%），并在噪声环境下展现出优越的鲁棒性，全面提升了基于CSI的呼吸健康监测能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于信道状态信息（CSI）的呼吸健康监测方法大多只关注呼吸频率，或仅监测呼吸引起的胸部运动（即呼吸波形）。本研究旨在开发一种更全面的非接触式呼吸监测方法，能够同时精确估计呼吸频率并还原真实的呼吸波形，并提高其在噪声环境下的鲁棒性。

Method: WiRM采用一个两阶段的方法进行非接触式呼吸监测。第一阶段，通过共轭乘法进行相位校正，并结合自适应多轨迹划分（AMTC）技术来追踪呼吸频率随时间的变化。第二阶段，利用第一阶段优化后的呼吸频率估计结果，对通信信道状态信息（CSI）数据进行分解和选择，以还原呼吸波形。

Result: WiRM在呼吸频率估计方面，相比现有三种最先进的方法，平均RMSE降低了38%。在呼吸波形还原方面，WiRM与真实呼吸波形的平均绝对相关性提高了178.3%。在噪声鲁棒性方面，WiRM在包括热噪声、乘性噪声和相位噪声在内的各种噪声条件下，表现出优于或相当的性能。

Conclusion: WiRM在呼吸健康监测领域展现出优越的性能，尤其在呼吸频率估计和呼吸波形还原方面。通过相位相干性校正和自适应多轨迹划分技术，WiRM显著提高了呼吸频率估计的准确性，并将呼吸频率均方根误差（RMSE）平均降低了38%。在呼吸波形还原方面，WiRM的平均绝对相关性相比真实值提高了178.3%。此外，WiRM在模拟的多种噪声环境下（包括热噪声、乘性噪声和相位噪声）表现出更强的鲁棒性或相当的性能，优于现有技术。

Abstract: Monitoring respiratory health with the use of channel state information (CSI)
has shown promising results. Many existing methods focus on monitoring only the
respiratory rate, while others focus on monitoring the motion of the chest as a
patient breathes, which is referred to as the respiratory waveform. This paper
presents WiRM, a two-staged approach to contactless respiration monitoring. In
the first stage, WiRM improves upon existing respiratory rate estimation
techniques by using conjugate multiplication for phase sanitisation and
adaptive multi-trace carving (AMTC) for tracing how the respiratory rate
changes over time. When compared against three state-of-the-art methods, WiRM
has achieved an average reduction of $38\%$ in respiratory rate root mean
squared error (RMSE). In the second stage, WiRM uses this improved respiratory
rate estimate to inform the decomposition and selection of the respiratory
waveform from the CSI data. Remarkably, WiRM delivers a $178.3\%$ improvement
in average absolute correlation with the ground truth respiratory waveform.
Within the literature, it is difficult to compare the robustness of existing
algorithms in noisy environments. In this paper, we develop a purpose-built
simulation toolkit to evaluate the robustness of respiration monitoring
solutions under various noise conditions, including thermal, multiplicative,
and phase noise. Our results show that WiRM demonstrates improved or comparable
resilience to these common noise sources.

</details>


### [208] [SOME: Symmetric One-Hot Matching Elector -- A Lightweight Microsecond Decoder for Quantum Error Correction](https://arxiv.org/abs/2507.23618)
*Xinyi Guo,Geguang Miao,Shinichi Nishizawa,Hiromitsu Awano,Shinji Kimura,Takashi Sato*

Main category: cs.ET

TL;DR: SOME解码器通过将量子纠错解码重构为OHQ问题，在CPU上实现了更快的解码速度和更低的拓扑复杂性，并且在更高的错误率下仍能保持高性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统量子纠错解码器（如MWPM和UF）虽然具有高阈值和快速解码的优点，但存在高拓扑复杂性的问题，同时为了改善现有基于Ising模型的解码器解码时间过长的问题，提出了一种新的解码器SOME。

Method: 提出了一种名为SOME（Symmetric One-Hot Matching Elector）的新型量子纠错（QEC）解码器，该解码器将QEC解码任务重构为二次无约束二元优化（QUBO）问题，称为“独热QUBO”（OHQ）。OHQ的变量表示一对配对的比特位是否匹配，错误概率则作为交互系数。通过约束确保每个比特位都只匹配一次。SOME通过构建最小化总权重的排列矩阵来高效地解决OHQ问题。它从最小权重比特对初始化候选矩阵，然后按权重升序迭代添加其他配对，最后选择总能量最低的排列矩阵。

Result: SOME解码器在变量数量上实现了高达99.9倍的减少，并且解码时间从毫秒级降低到微秒级（在单线程CPU上）。此外，OHQ在高达10.5%的物理错误率下仍能保持性能，其阈值超过了已知的MWPM解码器。

Conclusion: SOME通过将QEC解码任务重构为OHQ问题，并在CPU上以微秒级的解码时间实现了高达99.9倍的变量数量减少，从而降低了拓扑复杂性并提高了解码速度。OHQ的性能在高达10.5%的物理错误率下仍然保持不变，超过了MWPM的最高阈值。

Abstract: Conventional quantum error correction (QEC) decoders such as Minimum-Weight
Perfect Matching (MWPM) and Union-Find (UF) offer high thresholds and fast
decoding, respectively, but both suffer from high topological complexity. In
contrast, Ising model-based decoders reduce topological complexity but demand
considerable decoding time. We propose the Symmetric One-Hot Matching Elector
(SOME), a novel decoder that reformulates the QEC decoding task as a Quadratic
Unconstrained Binary Optimization (QUBO) problem -- termed the One-Hot QUBO
(OHQ). Each variable in the QUBO represents whether a given pair of flipped
syndromes is matched, while the error probabilities between the pair are
encoded as interaction coefficients (weight). Constraints ensure that each
flipped syndrome is matched exactly once. Valid solutions of OHQ correspond to
self-inverse permutation matrices, characterized by symmetric one-hot encoding.
To solve the OHQ efficiently, SOME reformulates the decoding task as the
construction of permutation matrices that minimize the total weight. It
initializes each candidate matrix from one of the minimum-weight syndrome
pairs, then iteratively appends additional pairs in ascending order of weight,
and finally selects the permutation matrix with the lowest total energy. SOME
achieves up to a 99.9x reduction in variable count and reduces decoding times
from milliseconds to microseconds on a single-threaded commodity CPU. OHQ also
maintains performance up to a 10.5% physical error rate, surpassing the highest
known threshold of MWPM@.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [209] [DNN-based Methods of Jointly Sensing Number and Directions of Targets via a Green Massive H2AD MIMO Receiver](https://arxiv.org/abs/2507.22906)
*Bin Deng,Jiatong Bai,Feilong Zhao,Zuming Xie,Maolin Li,Yan Wang,Feng Shu*

Main category: eess.SP

TL;DR: 针对H2AD MIMO感知多发射器数量和方向的难题，提出了一种两阶段感知框架，包含三种目标数量感知方法（改进EDC、增强DNN、改进1D-CNN）和一种低复杂度高精度的OMC-DOA估计方法，并推导了CRLB作为理论性能基准。仿真结果表明该框架在中高信噪比下数量感知准确率100%，1D-CNN在极低信噪比下表现优越，OMC-DOA在多源环境下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模全数字MIMO高能耗、高电路成本和高复杂度的问题，提出了异构混合模拟-数字H2AD MIMO架构。然而，如何通过这种结构智能地感知多发射器的数量和方向仍然是一个悬而未决的难题。

Method: 提出了一种两阶段感知框架，联合估计多个目标的数量和方向。设计了三种目标数量感知方法：改进的本征域聚类（EDC）框架、基于五个关键统计特征的增强深度神经网络（DNN）以及利用全特征值的改进一维卷积神经网络（1D-CNN）。随后，通过引入的在线微聚类（OMC-DOA）方法实现了低复杂度、高精度的DOA估计。此外，推导了H2AD在多源条件下的Cramér-Rao下界（CRLB）作为理论性能基准。

Result: 所提出的三种方法在中高信噪比下实现了100%的目标数量感知，而改进的1D-CNN在极低信噪比条件下表现优越。所提出的OMC-DOA在多源环境下的性能优于现有的聚类和基于融合的DOA方法。

Conclusion: 仿真结果表明，所提出的三种方法在中高信噪比下实现了100%的目标数量感知，而改进的1D-CNN在极低信噪比条件下表现优越。所提出的OMC-DOA在多源环境下的性能优于现有的聚类和基于融合的DOA方法。

Abstract: As a green MIMO structure, the heterogeneous hybrid analog-digital H2AD MIMO
architecture has been shown to own a great potential to replace the massive or
extremely large-scale fully-digital MIMO in the future wireless networks to
address the three challenging problems faced by the latter: high energy
consumption, high circuit cost, and high complexity. However, how to
intelligently sense the number and direction of multi-emitters via such a
structure is still an open hard problem. To address this, we propose a
two-stage sensing framework that jointly estimates the number and direction
values of multiple targets. Specifically, three target number sensing methods
are designed: an improved eigen-domain clustering (EDC) framework, an enhanced
deep neural network (DNN) based on five key statistical features, and an
improved one-dimensional convolutional neural network (1D-CNN) utilizing full
eigenvalues. Subsequently, a low-complexity and high-accuracy DOA estimation is
achieved via the introduced online micro-clustering (OMC-DOA) method.
Furthermore, we derive the Cram\'er-Rao lower bound (CRLB) for the H2AD under
multiple-source conditions as a theoretical performance benchmark. Simulation
results show that the developed three methods achieve 100\% number of targets
sensing at moderate-to-high SNRs, while the improved 1D-CNN exhibits superior
under extremely-low SNR conditions. The introduced OMC-DOA outperforms existing
clustering and fusion-based DOA methods in multi-source environments.

</details>


### [210] [Rydberg Atomic Receivers for Wireless Communications: Fundamentals, Potential, Applications, and Challenges](https://arxiv.org/abs/2507.22909)
*Yin Zhang,Jiayi Zhang,Bokai Xu,Yuanbin Chen,Zhilong Liu,Jiakang Zheng,Enyu Shi,Ziheng Liu,Tierui Gong,Wei E. I. Sha,Chau Yuen,Shi Jin,Bo Ai*

Main category: eess.SP

TL;DR: RARs利用量子相干性克服了传统RFRs的局限性，在提高灵敏度和带宽方面具有潜力，并在新兴应用中展现出前景，但仍面临带宽和失真等挑战。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统射频接收器（RFRs）的固有物理限制，特别是在灵敏度和带宽方面，利用了高度激发原子的量子相干性。

Method: 本文系统地阐述了RARs的基本传感机制，并将其与RFRs的工作原理和架构进行了对比。

Result: 文章探讨了RARs在集成传感与通信、量子Rydberg雷达和量子空间通信等新兴无线通信场景中的优势，并指出了实际挑战，如有限的瞬时带宽和非线性失真，同时提出了缓解策略和未来研究方向。

Conclusion: Rydberg原子接收器（RARs）代表了无线通信领域的一项重大进展，有望克服传统射频接收器的局限性，并在新兴应用中发挥关键作用。

Abstract: Rydberg atomic receivers (RARs) leverage the quantum coherence of highly
excited atoms to overcome the intrinsic physical limitations of conventional
radio frequency receivers (RFRs), particularly in sensitivity, and bandwidth.
This innovative technology represents a paradigm shift in wireless
communication systems. This paper systematically explains the fundamental
sensing mechanisms of RARs, contrasts their differences from RFRs in working
principles and architectures. We explore their advantages in emerging wireless
communication scenarios, such as integrated sensing and communications, quantum
Rydberg radar, and quantum space communications. Practical challenges, such as
limited instantaneous bandwidth and nonlinear distortion, are identified. To
address these issues, mitigation strategies and future research directions are
also outlined, supporting the advancement of RAR-aided wireless systems.

</details>


### [211] [Neural Energy Landscapes Predict Working Memory Decline After Brain Tumor Resection](https://arxiv.org/abs/2507.23057)
*Triet M. Tran,Sina Khanmohammadi*

Main category: eess.SP

TL;DR: 本研究发现，脑肿瘤患者术前神经动力学异常（能量景观变化）可以预测术后工作记忆能力的下降，准确率高达90%。这有助于实现个性化手术规划，减少认知风险。


<details>
  <summary>Details</summary>
Motivation: 本研究调查了肿瘤引起的术前神经动力学改变与术后工作记忆下降之间的关系。

Method: 本研究分析了脑肿瘤患者手术前的功能磁共振成像（fMRI），提取了高阶大脑相互作用的能量景观，并使用统计和机器学习（随机森林）模型检查了这些能量特征与术后工作记忆表现之间的关系。

Result: 术后工作记忆得分较低的患者表现出更少但更极端的能量状态转换，而得分较高的患者则表现出更频繁但更不极端的转变。术前高阶能量特征能够以 90% 的平均准确率、87.5% 的 F1 分数和 0.95 的 AUC 准确预测术后工作记忆下降。

Conclusion: 手术切除是脑肿瘤患者的主要治疗方法，但存在术后认知功能障碍的风险。本研究探讨了术前肿瘤引起的神经动力学改变与术后工作记忆下降之间的关系。我们分析了脑肿瘤患者手术前的功能磁共振成像（fMRI），并提取了高阶大脑相互作用的能量景观。然后，我们使用统计和机器学习（随机森林）模型检查了这些能量特征与术后工作记忆表现之间的关系。术后工作记忆得分较低的患者，其局部能量最小值和最大值之间的转换次数较少但更极端，而得分较高的患者则表现出更频繁但更不极端的转变。此外，术前高阶能量特征能够以 90% 的平均准确率、87.5% 的 F1 分数和 0.95 的 AUC 准确预测术后工作记忆下降。我们的研究表明，术前由脑肿瘤引起的高阶神经动力学紊乱预示着术后工作记忆的下降。我们的研究结果为个性化手术规划和靶向干预以减轻脑肿瘤切除相关的认知风险铺平了道路。

Abstract: Surgical resection is the primary treatment option for brain tumor patients,
but it carries the risk of postoperative cognitive dysfunction. This study
investigates how tumor-induced alterations in presurgical neural dynamics
relate to postoperative working memory decline. We analyzed functional magnetic
resonance imaging (fMRI) of brain tumor patients before surgery and extracted
energy landscapes of high-order brain interactions. We then examined the
relation between these energy features and postoperative working memory
performance using statistical and machine learning (random forest) models.
Patients with lower postoperative working memory scores exhibited fewer but
more extreme transitions between local energy minima and maxima, whereas
patients with higher scores showed more frequent but less extreme shifts.
Furthermore, the presurgical high-order energy features were able to accurately
predict postoperative working memory decline with a mean accuracy of 90\%, F1
score of 87.5\%, and an AUC of 0.95. Our study suggests that the brain
tumor-induced disruptions in high-order neural dynamics before surgery are
predictive of postoperative working memory decline. Our findings pave the path
for personalized surgical planning and targeted interventions to mitigate
cognitive risks associated with brain tumor resection.

</details>


### [212] [In-Orbit Cosmo-SkyMed antenna pattern estimation by a narrowband sweeper receiver](https://arxiv.org/abs/2507.23235)
*Mohammad Roueinfar,Masoud Ardini*

Main category: eess.SP

TL;DR: 一种用于卫星SAR的新型窄带扫描接收器（NSR）可提高天线方向图估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了在配备合成孔径雷达（SAR）的卫星中实现天线方向图的精确估计。

Method: 提出了一种利用窄带扫描接收器（NSR）估计配备合成孔径雷达（SAR）的卫星天线方向图的新颖方法。通过精确测量SAR的宽带频谱内的各个频率的功率，NSR显著提高了天线方向图提取的准确性。

Result: 通过Cosmo-SkyMed卫星进行的分析模型和实际实验验证了接收器的性能，与传统接收器相比，显示出更高的信噪比（SNR）。

Conclusion: 本研究代表了SAR技术的一项关键进展，为未来卫星校准和验证方法学提供了坚实的基础。

Abstract: This paper introduces a novel method for antenna pattern estimation in
satellites equipped with Synthetic Aperture Radar (SAR), utilizing a Narrowband
Sweeper Receiver (NSR). By accurately measuring power across individual
frequencies within SAR's inherently broadband spectrum, the NSR significantly
enhances antenna pattern extraction accuracy. Analytical models and practical
experiments conducted using the Cosmo-SkyMed satellite validate the receiver's
performance, demonstrating superior signal-to-noise ratio (SNR) compared to
conventional receivers. This research represents a key advancement in SAR
technology, offering a robust framework for future satellite calibration and
verification methodologies.

</details>


### [213] [BS-1-to-N: Diffusion-Based Environment-Aware Cross-BS Channel Knowledge Map Generation for Cell-Free Networks](https://arxiv.org/abs/2507.23236)
*Zhuoyin Dai,Di Wu,Yong Zeng,Xiaoli Xu,Xinyi Wang,Zesong Fei*

Main category: eess.SP

TL;DR: 本论文提出了一种名为BS-1-to-N的生成扩散模型，用于跨基站信道知识图谱（CKM）推理。它通过基站位置嵌入和注意力机制，高效地从源基站CKM推断出目标基站CKM，解决了传统方法成本高的问题，并有助于优化基站部署。


<details>
  <summary>Details</summary>
Motivation: 跨基站CKM推理是实现高效环境感知通信的关键。传统的逐个基站遍历CKM构建方法在节点众多的分布式网络（如无主网络）中成本过高。本研究旨在提出一种更高效的CKM推理方法。

Method: 本研究提出了一种基于生成扩散模型的环境感知跨基站CKM推理方法，称为BS-1-to-N。该方法设计了基站位置嵌入（BSLE）来整合BS位置信息，并利用交叉和自注意力机制学习源和目标BS之间的关系以及目标BS之间的关系。

Result: 通过利用源基站的CKM和其位置信息，BS-1-to-N模型能够为任意数量的目标基站进行CKM推理。与现有方案相比，BS-1-to-N在CKM推理方面表现出优越性，并通过一个用例研究展示了其在优化基站部署方面的实际应用价值。

Conclusion: 本论文提出的 BS-1-to-N 方法能够有效地进行跨基站（BS）的信道知识图谱（CKM）推理，通过利用生成扩散模型、基站位置嵌入（BSLE）以及交叉和自注意力机制，实现了对任意数量源基站和目标基站的CKM推理，并在实际应用中展示了其优化基站部署的潜力。

Abstract: Channel knowledge map (CKM) inference across base stations (BSs) is the key
to achieving efficient environmentaware communications. This paper proposes an
environmentaware cross-BS CKM inference method called BS-1-to-N based on the
generative diffusion model. To this end, we first design the BS location
embedding (BSLE) method tailored for cross-BS CKM inference to embed BS
location information in the feature vector of CKM. Further, we utilize the
cross- and self-attention mechanism for the proposed BS-1-to-N model to
respectively learn the relationships between source and target BSs, as well as
that among target BSs. Therefore, given the locations of the source and target
BSs, together with the source CKMs as control conditions, cross-BS CKM
inference can be performed for an arbitrary number of source and target BSs.
Specifically, in architectures with massive distributed nodes like cell-free
networks, traditional methods of sequentially traversing each BS for CKM
construction are prohibitively costly. By contrast, the proposed BS-1-to-N
model is able to achieve efficient CKM inference for a target BS at any
potential location based on the CKMs of source BSs. This is achieved by
exploiting the fact that within a given area, different BSs share the same
wireless environment that leads to their respective CKMs. Therefore, similar to
multi-view synthesis, CKMs of different BSs are representations of the same
wireless environment from different BS locations. By mining the implicit
correlation between CKM and BS location based on the wireless environment, the
proposed BS-1-to-N method achieves efficient CKM inference across BSs. We
provide extensive comparisons of CKM inference between the proposed BS-1-to-N
generative model versus benchmarking schemes, and provide one use case study to
demonstrate its practical application for the optimization of BS deployment.

</details>


### [214] [A Secure Full-Duplex Wireless Circulator enabled by Non-Reciprocal Beyond-Diagonal RIS](https://arxiv.org/abs/2507.23381)
*Ziang Liu,Bruno Clerckx*

Main category: eess.SP

TL;DR: NR-BD-RIS通过打破互易性，在全双工通信中提高了和速率和安全性，优于传统RIS技术。


<details>
  <summary>Details</summary>
Motivation: 为了提升无线通信系统的性能和安全性，特别是为全双工（FD）通信设备提供安全和高效的通信方式。NR-BD-RIS通过打破电路和信道互易性，能够实现非局部控制，这为解决非对齐收发器之间的通信问题提供了新的途径。

Method: 该研究提出了一种新的NR-BD-RIS在全双工（FD）无线通信中的应用，并考虑了包含结构散射（镜面反射）的物理约束系统模型。为了最大化所有用户的和速率，研究人员提出了一种结合块坐标下降（BCD）和惩罚对偶分解（PDD）的迭代优化算法。

Result: 数值评估结果表明，NR-BD-RIS在和速率性能上持续优于R-BD-RIS和D-RIS，尤其是在需要支持多于两个入射和反射方向的情况下。通过分析来自其他用户的信号功率和波束图，证明了该系统可以实现安全传输。

Conclusion: NR-BD-RIS在全双工（FD）无线通信中实现了优于R-BD-RIS和D-RIS的性能，特别是在需要支持多个入射和反射方向的情况下。通过功率分析和波束图，证明了NR-BD-RIS可以实现安全传输，抑制来自其他用户的信号，从而防止窃听。

Abstract: Beyond-diagonal reconfigurable intelligent surface (BD-RIS) has arisen as a
promising technology for enhancing wireless communication systems by enabling
flexible and intelligent wave manipulation. This is achieved through the
interconnections among the ports of the impedance network, enabling wave
reconfiguration when they flow through the surface. Thus, the output wave at
one port depends on waves impinging on neighboring ports, allowing non-local
control of both phase and magnitude. Non-reciprocal (NR)-BD-RIS further
enhances this capability by breaking circuit reciprocity and, consequently,
channel reciprocity. This feature potentially benefits communication among
non-aligned transceivers. This paper introduces a novel application of
NR-BD-RIS in full-duplex (FD) wireless circulators, where multiple FD devices
communicate via an NR-BD-RIS. This system is particularly beneficial for secure
transmission, as it enforces one-way communication among FD devices, suppresses
signal from all other users, and thus prevents eavesdropping. In addition, a
physics-compliant system model is considered by incorporating structural
scattering, also known as specular reflection. By accounting for this effect,
the advantages of NR-BD-RIS are further validated. Specifically, we formulate
an all-user sum-rate maximization problem and propose an iterative optimization
algorithm that employs block coordinate descent (BCD) and penalty dual
decomposition (PDD) methods. Numerical evaluations illustrate that NR-BD-RIS
consistently outperforms reciprocal (R)-BD-RIS and conventional diagonal
(D)-RIS in terms of sum-rate performance, particularly when more than two
impinging and reflection directions need to be supported. By analyzing the
power of signals from all other users and the beampatterns, we show that secure
transmission can be achieved.

</details>


### [215] [EVMx: An FPGA-Based Smart Contract Processing Unit](https://arxiv.org/abs/2507.23518)
*Joel Poncha Lemayian,Hachem Bensalem,Ghyslain Gagnon,Kaiwen Zhang,Pascal Giard*

Main category: eess.SP

TL;DR: EVMx 是一个基于 FPGA 的硬件加速器，可将以太坊虚拟机 (EVM) 的智能合约执行速度提高多达 6 倍，将常用操作码的执行时间减少高达 99%。


<details>
  <summary>Details</summary>
Motivation: 现有的以太坊虚拟机 (EVM) 性能受限于其运行的通用计算机的约束，这限制了智能合约和去中心化应用程序 (dApps) 的性能。

Method: 提出了一种名为 EVMx 的基于 FPGA 的智能合约执行引擎，用于将智能合约的执行从通用计算机卸载到专用硬件。

Result: EVMx 在常用操作码方面的执行时间减少了 61% 至 99%，并且其以太坊区块的执行速度比文献中的类似工作快 6 倍。

Conclusion: EVMx 作为一种基于 FPGA 的智能合约执行引擎，通过利用硬件架构的并行性和高速处理能力，显著提高了以太坊虚拟机 (EVM) 的性能。与基于 CPU 的执行环境相比，EVMx 将常用操作码的执行时间减少了 61% 至 99%，并且其以太坊区块的执行速度比文献中的类似工作快 6 倍，这表明该架构在加速智能合约执行和提升 EVM 兼容区块链性能方面具有巨大潜力。

Abstract: Ethereum blockchain uses smart contracts (SCs) to implement decentralized
applications (dApps). SCs are executed by the Ethereum virtual machine (EVM)
running within an Ethereum client. Moreover, the EVM has been widely adopted by
other blockchain platforms, including Solana, Cardano, Avalanche, Polkadot, and
more. However, the EVM performance is limited by the constraints of the
general-purpose computer it operates on. This work proposes offloading SC
execution onto a dedicated hardware-based EVM. Specifically, EVMx is an
FPGA-based SC execution engine that benefits from the inherent parallelism and
high-speed processing capabilities of a hardware architecture. Synthesis
results demonstrate a reduction in execution time of 61% to 99% for commonly
used operation codes compared to CPU-based SC execution environments. Moreover,
the execution time of Ethereum blocks on EVMx is up to 6x faster compared to
analogous works in the literature. These results highlight the potential of the
proposed architecture to accelerate SC execution and enhance the performance of
EVM-compatible blockchains.

</details>


### [216] [Channel Estimation for 6G Near-Field Wireless Communications: A Comprehensive Survey](https://arxiv.org/abs/2507.23526)
*Wen-Xuan Long,Shengyu Ye,Marco Moretti,Michele Morelli,Luca Sanguinetti,Rui Chen,Cheng-Xiang Wang*

Main category: eess.SP

TL;DR: 6G系统将使用大规模天线阵列（ELAAs）和极高频段，导致通信进入近场区域，这给信道估计带来了挑战。本综述回顾了近场信道模型和估计技术，并讨论了精度、复杂度和开销之间的权衡，为6G系统中的近场信道估计提供了见解和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 6G无线系统预计将采用极大规模阵列（ELAAs）、新颖的天线架构并在极高频段运行，以满足日益增长的数据需求。ELAAs显著增加了天线数量，能够实现更精细的空间分辨率和改进的波束成形。在高频段，ELAAs将通信从传统的远场转移到近场区域，球形波前占主导地位，信道响应同时取决于角度和距离，增加了信道维度。传统的依赖角度信息的远场信道估计方法，由于导频开销和计算复杂度增加，在近场场景下面临挑战。

Method: 本综述首先从电磁学角度定义了近场和远场边界，讨论了关键的传播差异，并简要回顾了ELAA的发展。然后，介绍了主流的近场信道模型，并将其与远场模型进行了比较。在不同配置（单用户/多用户、单载波/多载波）下，对主要的估计技术进行了回顾，包括直接估计和RIS辅助的级联估计。

Result: 所回顾的技术揭示了估计精度、复杂度和开销之间的权衡。

Conclusion: 本综述旨在为6G系统中的高效可扩展近场信道估计提供见解和基础，并确定关键挑战和未来研究方向。

Abstract: The sixth-generation (6G) wireless systems are expected to adopt extremely
large aperture arrays (ELAAs), novel antenna architectures, and operate in
extremely high-frequency bands to meet growing data demands. ELAAs
significantly increase the number of antennas, enabling finer spatial
resolution and improved beamforming. At high frequencies, ELAAs shift
communication from the conventional far-field to near-field regime, where
spherical wavefronts dominate and the channel response depends on both angle
and distance, increasing channel dimensionality. Conventional far-field channel
estimation methods, which rely on angular information, struggle in near-field
scenarios due to increased pilot overhead and computational complexity. This
paper presents a comprehensive survey of recent advances in near-field channel
estimation. It first defines the near- and far-field boundary from an
electromagnetic perspective and discusses key propagation differences,
alongside a brief review of ELAA developments. Then, it introduces mainstream
near-field channel models and compares them with far-field models. Major
estimation techniques are reviewed under different configurations
(single/multi-user, single/multi-carrier), including both direct estimation and
RIS-assisted cascaded estimation. These techniques reveal trade-offs among
estimation accuracy, complexity, and overhead. This survey aims to provide
insights and foundations for efficient and scalable near-field channel
estimation in 6G systems, while identifying key challenges and future research
directions.

</details>


### [217] [Multiple-Parameter Graph Fractional Fourier Transform: Theory and Applications](https://arxiv.org/abs/2507.23570)
*Manjun Cui,Zhichao Zhang,Wei Yao*

Main category: eess.SP

TL;DR: 提出多参数图分数阶傅里叶变换（MPGFRFT）及其理论框架，通过光谱压缩、自适应去噪和图像加密等应用，解决了GFRFT适应性问题，并在图信号处理任务中展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决图分数阶傅里叶变换（GFRFT）应用单一全局分数阶导致对频谱域中不同信号特征适应性受限的问题。

Method: 提出两种多参数图分数阶傅里叶变换（MPGFRFT），设计了适用于超低压缩率的光谱压缩策略，并引入了可学习的阶向量方案以实现自适应压缩和去噪。

Result: 所提出的MPGFRFT框架在各种图信号处理任务（包括光谱压缩、自适应压缩、去噪和图像加密/解密）中表现出多功能性和优越性能。

Conclusion: 该研究提出了两种多参数图分数阶傅里叶变换（MPGFRFT），并建立了相应的理论框架。实验结果验证了MPGFRFT在图信号处理任务中的多功能性和优越性能。

Abstract: The graph fractional Fourier transform (GFRFT) applies a single global
fractional order to all graph frequencies, which restricts its adaptability to
diverse signal characteristics across the spectral domain. To address this
limitation, in this paper, we propose two types of multiple-parameter GFRFTs
(MPGFRFTs) and establish their corresponding theoretical frameworks. We design
a spectral compression strategy tailored for ultra-low compression ratios,
effectively preserving essential information even under extreme dimensionality
reduction. To enhance flexibility, we introduce a learnable order vector scheme
that enables adaptive compression and denoising, demonstrating strong
performance on both graph signals and images. We explore the application of
MPGFRFTs to image encryption and decryption. Experimental results validate the
versatility and superior performance of the proposed MPGFRFT framework across
various graph signal processing tasks.

</details>


### [218] [On the Achievable Rate of Satellite Quantum Communication Channel using Deep Autoencoder Gaussian Mixture Model](https://arxiv.org/abs/2507.23695)
*Mouli Chakraborty,Subhash Chandra,Avishek Nag,Anshu Mukherjee*

Main category: eess.SP

TL;DR: DAGMM在卫星量子信道容量估计方面优于GMM，并提出了DCGMM。


<details>
  <summary>Details</summary>
Motivation: 为了在考虑混合量子噪声（HQN）和传输约束的条件下，估计卫星量子信道容量，并改进现有模型的性能。

Method: 本文提出并比较了高斯混合模型（GMM）和深度自动编码器高斯混合模型（DAGMM）在估计卫星量子信道容量方面的性能，并引入了深度聚类高斯混合模型（DCGMM）。

Result: DAGMM在捕捉非线性变化和噪声分布方面优于GMM，并提供了更紧密的容量边界和改进的聚类效果。

Conclusion: DAGMM在卫星量子信道容量估计方面优于GMM，能够更好地处理非线性变化和噪声分布，并提供更紧密的容量边界和改进的聚类效果。本文提出了DCGMM用于高维量子数据分析。

Abstract: We present a comparative study of the Gaussian mixture model (GMM) and the
Deep Autoencoder Gaussian Mixture Model (DAGMM) for estimating satellite
quantum channel capacity, considering hybrid quantum noise (HQN) and
transmission constraints. While GMM is simple and interpretable, DAGMM better
captures non-linear variations and noise distributions. Simulations show that
DAGMM provides tighter capacity bounds and improved clustering. This introduces
the Deep Cluster Gaussian Mixture Model (DCGMM) for high-dimensional quantum
data analysis in quantum satellite communication.

</details>


### [219] [Cellular, Cell-less, and Everything in Between: A Unified Framework for Utility Region Analysis in Wireless Networks](https://arxiv.org/abs/2507.23707)
*Renato Luis Garrido Cavalcante,Tomasz Piotrowski,Slawomir Stanczak*

Main category: eess.SP

TL;DR: 提出一个分析无线网络效用区域的统一框架，并推导了保证其凸性的条件，从而简化了优化问题的求解。


<details>
  <summary>Details</summary>
Motivation: 为了分析无线网络的效用区域，重点关注SINR和可实现速率区域，并为解决相关的优化问题提供理论基础。

Method: 提出一个统一的框架来分析无线网络的效用区域，重点关注信干噪比（SINR）和可实现速率区域。

Result: 推导了保证效用区域凸性的充分条件，该条件可用于识别固有的凸优化问题，并为直接根据可实现速率而非SINR水平来表述和解决和速率最大化问题提供了理论依据。

Conclusion: 该框架提供了对现代网络架构（如无小区和超大规模MIMO网络）干扰模式的宝贵见解，并推广了弱帕累托边界的现有特征。推导了保证效用区域凸性的充分条件，这对于确保时间共享（或用户分组）不能同时增加所有用户的效用至关重要。

Abstract: We introduce a unified framework for analyzing utility regions of wireless
networks, with a focus on the signal-to-interference-noise-ratio (SINR) and
achievable rate regions. The framework provides valuable insights into
interference patterns of modern network architectures, such as cell-less and
extremely large MIMO networks, and it generalizes existing characterizations of
the weak Pareto boundary. A central contribution is the derivation of
sufficient conditions that guarantee convexity of the utility regions.
Convexity is an important property because it ensures that time sharing (or
user grouping) cannot simultaneously increase the utility of all users when the
network operates on the weak Pareto boundary. These sufficient conditions also
have two key implications. First, they identify a family of (weighted) sum-rate
maximization problems that are inherently convex without any variable
transformations, thus paving the way for the development of efficient, provably
optimal solvers for this family. Second, they provide a rigorous justification
for formulating sum-rate maximization problems directly in terms of achievable
rates, rather than SINR levels. Our theoretical insights also motivate an
alternative to the concept of favorable propagation in the massive MIMO
literature -- one that explicitly accounts for self-interference and the
beamforming strategy.

</details>


### [220] [Real-Time Transmission of Uncompressed High-Definition Video Via A VCSEL-Based Optical Wireless Link With Ultra-Low Latency](https://arxiv.org/abs/2507.23746)
*Hossein Kazemi,Isaac N. O. Osahon,Tiankuo Jiao,David Butler,Nikolay Ledentsov Jr.,Ilya Titkov,Nikolay Ledentsov,Harald Haas*

Main category: eess.SP

TL;DR: 提出了一种基于激光的光学无线通信系统，可实现超低延迟的高清视频传输，支持FHD和4K分辨率，适用于专业广播。


<details>
  <summary>Details</summary>
Motivation: 为了在专业广播场景中实现高清晰度视频信号的实时、超可靠、低延迟传输，需要一种高带宽的通信媒介以保证用户体验。

Method: 利用VCSEL激光器，通过直接调制SDI信号来传输高清视频流，实现了低延迟和高数据率的光学无线通信。

Result: 成功实现了2.97 Gb/s（FHD）和5.94 Gb/s（4K UHD）的无差错传输，端到端延迟低于35纳秒。

Conclusion: 该光学无线通信（OWC）系统采用直接调制VCSEL的方式，实现了HD视频的超低延迟传输，并兼容SDI接口，可扩展且成本低廉，适用于专业广播设备间的无线连接。

Abstract: Real-time transmission of high-resolution video signals in an uncompressed
and unencrypted format requires an ultra-reliable and low-latency
communications (URLLC) medium with high bandwidth to maintain the quality of
experience (QoE) for users. We put forward the design and experimental
demonstration of a high-performance laser-based optical wireless communication
(OWC) system that enables high-definition (HD) video transmission with
submillisecond latencies. The serial digital interface (SDI) output of a camera
is used to transmit the live video stream over an optical wireless link by
directly modulating the SDI signal on the intensity of a 940 nm vertical cavity
surface emitting laser (VCSEL). The proposed SDI over light fidelity (LiFi)
system corroborates error-free transmission of full HD (FHD) and 4K
ultra-high-definition (UHD) resolutions at data rates of 2.97 Gb/s and 5.94
Gb/s, respectively, with a measured end-to-end latency of under 35 ns. Since
SDI standards support various video formats and VCSELs are high-bandwidth and
low-power devices, this presents a scalable and inexpensive solution for
wireless connectivity between professional broadcast equipment using
off-the-shelf SDI components.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [221] [H2SGEMM: Emulating FP32 GEMM on Ascend NPUs using FP16 Units with Precision Recovery and Cache-Aware Optimization](https://arxiv.org/abs/2507.23387)
*Weicheng Xue,Baisong Xu,Kai Yang,Yongxiang Liu,Dengdeng Fan,Pengxiang Xu,Yonghong Tian*

Main category: cs.DC

TL;DR: H2SGEMM利用FP16单元模拟FP32 GEMM，通过误差补偿和优化计算顺序，在AI加速器上实现了高精度和高效率，并优于传统FP32 GEMM的数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 低精度（如FP16）矩阵计算引擎吞吐量高，但缺乏全精度计算支持。本研究旨在提出一种高效算法（H2SGEMM），利用现有的FP16计算单元模拟FP32通用矩阵乘法（GEMM），以解决AI加速器上全精度计算的限制。

Method: H2SGEMM算法通过将FP32矩阵分解为两个FP16值，并利用可调缩放策略补偿数值误差，在仅支持FP16计算单元的AI加速器上实现FP32通用矩阵乘法（GEMM）。该方法详细分析了数值误差（包括下溢和精度损失），以选择缩放参数来保留多达22位尾数精度。此外，研究了计算顺序对精度的影响，发现逐项累加方案在低指数范围内比传统的FP32 GEMM具有更好的数值稳定性。最后，通过引入感知缓存的阻塞策略和双缓冲流水线，实现了内存传输与计算的重叠。

Result: H2SGEMM在仅支持FP16计算单元的AI加速器上（如Ascend 910A）实现了高达理论FP32等效峰值性能77%的性能。通过数值实验证实，H2SGEMM不仅能恢复原生FP32 GEMM的精度，而且由于其结构化和面向错误的计算顺序，在特定条件下还表现出更优越的数值稳定性。

Conclusion: H2SGEMM通过将FP32矩阵分解为FP16值并进行补偿，可以在仅使用FP16计算单元的AI加速器上高效地实现FP32通用矩阵乘法（GEMM）。该方法通过误差分析优化了计算精度，并采用了感知缓存的阻塞策略和双缓冲流水线来重叠内存传输和计算，实现了接近理论峰值性能的77%。实验结果表明，H2SGEMM不仅恢复了FP32 GEMM的精度，还在某些情况下由于其结构化和面向错误的计算顺序而表现出更优越的数值稳定性。

Abstract: Low-precision matrix engines, such as FP16 cube, offer high throughput but
lack support for full-precision computation. In this work, we propose H2SGEMM,
a high-performance algorithm for emulating FP32 general matrix-matrix
multiplication (GEMM) using only FP16 computation units on a representative AI
accelerator. The method decomposes each FP32 operand into two FP16 values and
compensates for numerical errors through a tunable scaling strategy. A detailed
analysis of numerical errors, including underflow conditions and precision
loss, guides the selection of scaling parameters to preserve up to 22 bits of
mantissa accuracy. We further investigate the effect of computation order on
accuracy and demonstrate that a term-wise accumulation scheme improves
numerical stability over conventional FP32 GEMM in low-exponent regimes.
Finally, a cache-aware blocking strategy and double-buffered pipeline are
introduced to overlap memory transfers with computation, enabling H2SGEMM to
achieve up to 77% of the theoretical FP32-equivalent peak performance on Ascend
910A NPU lacking native FP32 support. Extensive numerical experiments confirm
that our method not only recovers the accuracy of native FP32 GEMM but also
exhibits superior numerical stability under certain conditions, due to its
structured and error-aware computation order.

</details>


### [222] [Towards a Testbed for Scalable FaaS Platforms](https://arxiv.org/abs/2507.23431)
*Trever Schirmer,David Bermbach*

Main category: cs.DC

TL;DR: We present a research-focused testbed that can be adapted to quickly evaluate the impact of different architectures and technologies on the characteristics of scalability-focused FaaS platforms.


<details>
  <summary>Details</summary>
Motivation: To better understand how the platform's architecture impacts its performance, we present a research-focused testbed that can be adapted to quickly evaluate the impact of different architectures and technologies on the characteristics of scalability-focused FaaS platforms.

Method: We present a research-focused testbed that can be adapted to quickly evaluate the impact of different architectures and technologies on the characteristics of scalability-focused FaaS platforms.

Result: We present a research-focused testbed that can be adapted to quickly evaluate the impact of different architectures and technologies on the characteristics of scalability-focused FaaS platforms.

Conclusion: We present a research-focused testbed that can be adapted to quickly evaluate the impact of different architectures and technologies on the characteristics of scalability-focused FaaS platforms.

Abstract: Most cloud platforms have a Function-as-a-Service (FaaS) offering that
enables users to easily write highly scalable applications. To better
understand how the platform's architecture impacts its performance, we present
a research-focused testbed that can be adapted to quickly evaluate the impact
of different architectures and technologies on the characteristics of
scalability-focused FaaS platforms.

</details>


### [223] [Threshold-Driven Streaming Graph: Expansion and Rumor Spreading](https://arxiv.org/abs/2507.23533)
*Flora Angileri,Andrea Clementi,Emanuele Natale,Michele Salvi,Isabella Ziccardi*

Main category: cs.DC

TL;DR: 在有节点进出的动态图中，RAES算法表现良好，推动了PUSH和PULL协议的效率。


<details>
  <summary>Details</summary>
Motivation: 解决RAES算法在动态模型下的分析这一未解决问题，并研究其在具有节点流失特性的动态图模型中的行为。

Method: 通过在动态图模型（节点-退出过程）下研究RAES算法，证明了该模型下的快照图具有良好的扩展性，并利用这一特性推导了PUSH和PULL协议的对数级完成时间上限。

Result: 证明了在节点流失动态图模型下，RAES算法的每个快照图都具有良好的扩展性，并得到了PUSH和PULL协议的对数级完成时间上限。

Conclusion: 该研究在动态图模型下分析了RAES算法，证明了该模型下的快照图具有良好的扩展性，并为PUSH和PULL协议提供了一个对数级完成时间上限。

Abstract: A randomized distributed algorithm called RAES was introduced in [Becchetti
et al., SODA 2020] to extract a bounded-degree expander from a dense $n$-vertex
expander graph $G = (V, E)$. The algorithm relies on a simple threshold-based
procedure. A key assumption in [Becchetti et al., SODA 2020] is that the input
graph $G$ is static - i.e., both its vertex set $V$ and edge set $E$ remain
unchanged throughout the process - while the analysis of RAES in dynamic models
is left as a major open question.
  In this work, we investigate the behavior of RAES under a dynamic graph model
induced by a streaming node-churn process (also known as the sliding window
model), where, at each discrete round, a new node joins the graph and the
oldest node departs. This process yields a bounded-degree dynamic graph
$\mathcal{G} =\{ G_t = (V_t, E_t) : t \in \mathbb{N}\}$ that captures essential
characteristics of peer-to-peer networks -- specifically, node churn and
threshold on the number of connections each node can manage. We prove that
every snapshot $G_t$ in the dynamic graph sequence has good expansion
properties with high probability. Furthermore, we leverage this property to
establish a logarithmic upper bound on the completion time of the well-known
PUSH and PULL rumor spreading protocols over the dynamic graph $\mathcal{G}$.

</details>


### [224] [The ArborX library: version 2.0](https://arxiv.org/abs/2507.23700)
*Andrey Prokopenko,Daniel Arndt,Damien Lebrun-Grandié,Bruno Turcksin*

Main category: cs.DC

TL;DR: ArborX 2.0 is out, with a new interface, better search structures, user callbacks, and more algorithms like ray tracing and clustering.


<details>
  <summary>Details</summary>
Motivation: To provide an overview of the 2.0 release of the ArborX library and highlight its new features and improvements.

Method: The paper analyzes the 2.0 release of the ArborX library, detailing its new features and improvements, including a new interface, new search data structures (brute force, distributed), support for user-defined callbacks, and an expanded set of algorithms (ray tracing, clustering).

Result: The ArborX library has been upgraded to version 2.0, featuring a new interface, new search data structures, support for user callbacks, and expanded algorithms.

Conclusion: ArborX 2.0 is a significant upgrade, offering a wider range of user-specific solutions through a new interface, enhanced data structures, callback functionality, and expanded algorithms like ray tracing and clustering.

Abstract: This paper provides an overview of the 2.0 release of the ArborX library, a
performance portable geometric search library based on Kokkos. We describe the
major changes in ArborX 2.0 including a new interface for the library to
support a wider range of user problems, new search data structures (brute
force, distributed), support for user functions to be executed on the results
(callbacks), and an expanded set of the supported algorithms (ray tracing,
clustering).

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [225] [From Propagator to Oscillator: The Dual Role of Symmetric Differential Equations in Neural Systems](https://arxiv.org/abs/2507.22916)
*Kun Jiang*

Main category: cs.NE

TL;DR: 本文研究了一种新型神经元模型，发现它既能传播信号，又能产生振荡，并提出了一种名为“on-road energy”的中间状态度量来监控其状态。


<details>
  <summary>Details</summary>
Motivation: 深入研究了先前工作中提出的基于对称微分方程的新型神经元模型的内在动力学和功能多样性。

Method: 通过探索参数空间和使用数学分析工具来揭示系统的功能对偶性。

Result: 该模型表现出两种不同的轨迹行为：一种是渐近稳定的，对应于可靠的信号传播器；另一种是李雅普诺夫稳定的，其特征是持续的自激振荡，作为信号发生器。通过参数调整或连接结构修改可以诱导两种功能模式之间的转换，并且可以通过引入外部信号来有效抑制振荡。

Conclusion: 该模型具有双重功能，即信号传播和振荡，这与生物神经元的双重作用相似，为神经形态工程中的应用奠定了理论基础。

Abstract: In our previous work, we proposed a novel neuron model based on symmetric
differential equations and demonstrated its potential as an efficient signal
propagator. Building upon that foundation, the present study delves deeper into
the intrinsic dynamics and functional diversity of this model. By
systematically exploring the parameter space and employing a range of
mathematical analysis tools, we theoretically reveal the system 's core
property of functional duality. Specifically, the model exhibits two distinct
trajectory behaviors: one is asymptotically stable, corresponding to a reliable
signal propagator; the other is Lyapunov stable, characterized by sustained
self-excited oscillations, functioning as a signal generator. To enable
effective monitoring and prediction of system states during simulations, we
introduce a novel intermediate-state metric termed on-road energy. Simulation
results confirm that transitions between the two functional modes can be
induced through parameter adjustments or modifications to the connection
structure. Moreover, we show that oscillations can be effectively suppressed by
introducing external signals. These findings draw a compelling parallel to the
dual roles of biological neurons in both information transmission and rhythm
generation, thereby establishing a solid theoretical basis and a clear
functional roadmap for the broader application of this model in neuromorphic
engineering.

</details>


### [226] [Hybrid Particle Swarm Optimization for Fast and Reliable Parameter Extraction in Thermoreflectance](https://arxiv.org/abs/2507.22960)
*Bingjia Xiao,Tao Chen,Wenbin Zhang,Xin Qian,Puqing Jiang*

Main category: cs.NE

TL;DR: FDTR 参数提取的 AI 混合优化框架，HPSO 是最佳选择。


<details>
  <summary>Details</summary>
Motivation: FDTR 技术用于表征多层薄膜的热性质，但提取多个参数是一个非线性逆问题，具有高维度和多峰、非凸解空间。本研究旨在通过引入混合优化框架来解决 FDTR 测量中提取参数的收敛速度和准确性问题。

Method: 本研究评估了四种流行的全局优化算法（GA、QGA、PSO 和 FWA）用于从 GaN/Si 异质结构 FDTR 测量中提取参数。为了提高收敛速度和准确性，本研究提出了一个由 AI 驱动的混合优化框架，该框架将每种全局算法与拟牛顿局部优化方法相结合，形成了四种混合变体：HGA、HQGA、HPSO 和 HFWA。

Result: HPSO 在所有方法中表现最佳，80% 的试验在 60 秒内达到目标适应度值，表现出更强的鲁棒性，过早收敛的风险更低。与其他方法相比，HPSO 的收敛速度快五倍。

Conclusion: HPSO在热计量学的逆问题中提供了一个通用解决方案，并且易于扩展到其他模型拟合技术。

Abstract: Frequency-domain thermoreflectance (FDTR) is a widely used technique for
characterizing thermal properties of multilayer thin films. However, extracting
multiple parameters from FDTR measurements presents a nonlinear inverse problem
due to its high dimensionality and multimodal, non-convex solution space. This
study evaluates four popular global optimization algorithms: Genetic Algorithm
(GA), Quantum Genetic Algorithm (QGA), Particle Swarm Optimization (PSO), and
Fireworks Algorithm (FWA), for extracting parameters from FDTR measurements of
a GaN/Si heterostructure. However, none achieve reliable convergence within 60
seconds. To improve convergence speed and accuracy, we propose an AI-driven
hybrid optimization framework that combines each global algorithm with a
Quasi-Newton local refinement method, resulting in four hybrid variants: HGA,
HQGA, HPSO, and HFWA. Among these, HPSO outperforms all other methods, with 80%
of trials reaching the target fitness value within 60 seconds, showing greater
robustness and a lower risk of premature convergence. In contrast, only 30% of
HGA and HQGA trials and 20% of HFWA trials achieve this threshold. We then
evaluate the worst-case performance across 100 independent trials for each
algorithm when the time is extended to 1000 seconds. Only HPSO, PSO, and HGA
consistently reach the target accuracy, with HPSO converging five times faster
than the others. HPSO provides a general-purpose solution for inverse problems
in thermal metrology and can be readily extended to other model-fitting
techniques.

</details>


### [227] [Finger Force Decoding from Motor Units Activity on Neuromorphic Hardware](https://arxiv.org/abs/2507.23474)
*Farah Baracat,Giacomo Indiveri,Elisa Donati*

Main category: cs.NE

TL;DR: 一种利用运动神经元脉冲发放序列在神经形态硬件上进行低功耗、实时手指力估计的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于深度学习的EMG解码方法需要大量数据集和高计算资源，限制了其在实时嵌入式系统中的应用。本研究旨在通过一种新颖的方法，实现低功耗、实时的手指力估计。

Method: 提出了一种新颖的方法，利用高密度肌电图（EMG）中提取的单个运动神经元的脉冲发放序列，结合脉冲神经网络（SNN）和混合信号神经形态处理器，进行手指力回归。

Result: 研究结果证实了该方法可以准确地预测手指力，并且能耗极低。

Conclusion: 该研究首次在神经形态硬件上直接计算了基于运动神经元的连续回归，实现了低功耗、实时的手指力估计，为假肢和可穿戴神经技术中的嵌入式解码开辟了新的可能性。

Abstract: Accurate finger force estimation is critical for next-generation
human-machine interfaces. Traditional electromyography (EMG)-based decoding
methods using deep learning require large datasets and high computational
resources, limiting their use in real-time, embedded systems. Here, we propose
a novel approach that performs finger force regression using spike trains from
individual motor neurons, extracted from high-density EMG. These biologically
grounded signals drive a spiking neural network implemented on a mixed-signal
neuromorphic processor. Unlike prior work that encodes EMG into events, our
method exploits spike timing on motor units to perform low-power, real-time
inference. This is the first demonstration of motor neuron-based continuous
regression computed directly on neuromorphic hardware. Our results confirm
accurate finger-specific force prediction with minimal energy use, opening new
possibilities for embedded decoding in prosthetics and wearable
neurotechnology.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [228] [Causal-Inspired Multi-Agent Decision-Making via Graph Reinforcement Learning](https://arxiv.org/abs/2507.23080)
*Jing Wang,Yan Jin,Fei Ding,Chongfeng Wei*

Main category: cs.MA

TL;DR: 本研究利用因果学习和强化学习来提高自动驾驶汽车在复杂交通场景（如无信号交叉口）中的决策能力，通过因果特征提取和图神经网络增强性能，实验证明该方法在奖励和安全性方面优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶技术取得显著进步的背景下，大多数现有研究仍在努力解决多车需要无缝交互的环境所带来的挑战。

Method: 本研究将因果学习与基于强化学习的方法相结合，利用因果解纠缠表示学习（CDRL）来识别和提取影响自动驾驶汽车最优决策的因果特征，并将这些特征整合到基于图神经网络的强化学习算法中。

Result: 实验结果表明，我们提出的方法在训练过程中实现了最高的平均奖励，并且在碰撞率和平均累积奖励等关键指标上显著优于其他基于学习的方法。

Conclusion: 本研究为推进多智能体自动驾驶系统提供了有前景的方向，并提高了复杂交通环境中自动驾驶汽车的导航安全性和效率。

Abstract: Since the advent of autonomous driving technology, it has experienced
remarkable progress over the last decade. However, most existing research still
struggles to address the challenges posed by environments where multiple
vehicles have to interact seamlessly. This study aims to integrate causal
learning with reinforcement learning-based methods by leveraging causal
disentanglement representation learning (CDRL) to identify and extract causal
features that influence optimal decision-making in autonomous vehicles. These
features are then incorporated into graph neural network-based reinforcement
learning algorithms to enhance decision-making in complex traffic scenarios. By
using causal features as inputs, the proposed approach enables the optimization
of vehicle behavior at an unsignalized intersection. Experimental results
demonstrate that our proposed method achieves the highest average reward during
training and our approach significantly outperforms other learning-based
methods in several key metrics such as collision rate and average cumulative
reward during testing. This study provides a promising direction for advancing
multi-agent autonomous driving systems and make autonomous vehicles' navigation
safer and more efficient in complex traffic environments.

</details>


### [229] [Barriers to Healthcare: Agent-Based Modeling to Mitigate Inequity](https://arxiv.org/abs/2507.23644)
*Alba Aguilera,Georgina Curto,Nardine Osman*

Main category: cs.MA

TL;DR: 该研究整合了能力方法（CA）和强化学习，以模拟评估社会政策对人类福祉的影响，并通过一个关于减少无家可归者健康不平等的案例研究进行了验证。


<details>
  <summary>Details</summary>
Motivation: 为了在现实世界人口中实施之前，以非侵入性方式评估社会政策，并解决紧迫的人类发展挑战，我们需要考虑建模代理人行为和衡量不平等标准的差异。

Method: 将能力方法（CA）的概念框架整合到强化学习环境中，代理人（人们）的行为旨在恢复其在特定策略约束下的能力。

Result: 在巴塞罗那一个案例研究中，该模型用于减轻无家可归者的健康不平等，并评估正在讨论的议会政策的影响。 articulo介绍了第一个符合人类发展能力的模拟概念证明。

Conclusion: 该模型为首个符合关注人类发展能力的模拟，用于评估影响政策。

Abstract: Agent-based simulations have an enormous potential as tools to evaluate
social policies in a non-invasive way, before these are implemented to
real-world populations. However, the recommendations that these computational
approaches may offer to tackle urgent human development challenges can vary
substantially depending on how we model agents' (people) behaviour and the
criteria that we use to measure inequity. In this paper, we integrate the
conceptual framework of the capability approach (CA), which is explicitly
designed to promote and assess human well-being, to guide the simulation and
evaluate the effectiveness of policies. We define a reinforcement learning
environment where agents behave to restore their capabilities under the
constraints of a specific policy. Working in collaboration with local
stakeholders, non-profits and domain experts, we apply our model in a case
study to mitigate health inequity among the population experiencing
homelessness (PEH) in Barcelona. By doing so, we present the first proof of
concept simulation, aligned with the CA for human development, to assess the
impact of policies under parliamentary discussion.

</details>


### [230] [A survey of multi-agent geosimulation methodologies: from ABM to LLM](https://arxiv.org/abs/2507.23694)
*Virginia Padilla,Jacinto Dávila*

Main category: cs.MA

TL;DR: 本文研究了如何将大型语言模型（LLMs）整合到地缘模拟平台中，发现遵循特定结构化架构（包括感知、记忆、规划和行动）的LLMs可以被有效利用，为下一代地缘模拟系统提供了基础。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提供一个形式化规范，以指导地缘模拟平台的构建，并探索将大型语言模型（LLMs）整合到这些平台中的潜力。

Method: 本文基于二十年的研究，提供了一个对编码了多主体系统、模拟和信息系统之间原理与联系的基于主体的综合性审查。

Result: 研究结果表明，当大型语言模型（LLMs）遵循包括感知、记忆、规划和行动等基本代理活动在内的结构化架构时，可以将它们有效地整合到地缘模拟平台中。这种整合与本文所正式化的架构高度一致，为下一代地缘模拟系统奠定了坚实的基础。

Conclusion: 本研究证实了将大型语言模型（LLMs）作为代理组件有效整合到地缘模拟平台中的可行性，前提是它们遵循特定的结构化架构，包括感知、记忆、规划和行动等基本代理活动。

Abstract: We provide a comprehensive examination of agent-based approaches that codify
the principles and linkages underlying multi-agent systems, simulations, and
information systems. Based on two decades of study, this paper confirms a
framework intended as a formal specification for geosimulation platforms. Our
findings show that large language models (LLMs) can be effectively incorporated
as agent components if they follow a structured architecture specific to
fundamental agent activities such as perception, memory, planning, and action.
This integration is precisely consistent with the architecture that we
formalize, providing a solid platform for next-generation geosimulation
systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [231] [KLLM: Fast LLM Inference with K-Means Quantization](https://arxiv.org/abs/2507.23035)
*Xueying Wu,Baijun Zhou,Zhihui Gao,Yuzhe Fu,Qilin Zheng,Yintao He,Hai Li*

Main category: cs.LG

TL;DR: KLLM框架通过软硬件协同设计，解决量化和异常值问题，在LLM推理中实现大幅加速和能效提升。


<details>
  <summary>Details</summary>
Motivation: 现有的权重和激活量化（WAQ）技术在低精度下存在精度下降问题，并且K-Means量化虽然精度更高但由于其非均匀性导致无法直接在低精度计算单元上执行。同时，激活异常值也阻碍了低精度WAQ的有效性。

Method: 提出了一种名为KLLM的软硬件协同设计框架，该框架采用基于索引的计算方案来高效执行K-Means量化数据的矩阵乘法和非线性运算，从而避免了大部分的反量化和全精度计算。此外，KLLM还集成了一个名为Orizuru的新型异常值检测引擎，能够在在线推理过程中高效识别激活数据流中的最大和最小的k个元素。

Result: KLLM框架在LLM推理中实现了显著的加速和能效提升，平均分别比A100 GPU和Atom带来了9.67倍和7.03倍的加速，以及229.50倍和150.21倍的能效提升。

Conclusion: KLLM框架在LLM推理中实现了显著的加速和能效提升，平均分别比A100 GPU和Atom带来了9.67倍和7.03倍的加速，以及229.50倍和150.21倍的能效提升。

Abstract: Large language model (LLM) inference poses significant challenges due to its
intensive memory and computation demands. Weight and activation quantization
(WAQ) offers a promising solution by reducing both memory footprint and
arithmetic complexity. However, two key challenges remain in the existing WAQ
designs. (1) Traditional WAQ designs rely on uniform integer-based quantization
for hardware efficiency, but this often results in significant accuracy
degradation at low precision. K-Means-based quantization, a non-uniform
quantization technique, achieves higher accuracy by matching the Gaussian-like
distributions of weights and activations in LLMs. However, its non-uniform
nature prevents direct execution on low-precision compute units, requiring
dequantization and floating-point matrix multiplications (MatMuls) during
inference. (2) Activation outliers further hinder effective low-precision WAQ.
Offline thresholding methods for outlier detection can lead to significant
model performance degradation, while existing online detection techniques
introduce substantial runtime overhead.
  To address the aforementioned challenges and fully unleash the potential of
WAQ with K-Means quantization for LLM inference, in this paper, we propose
KLLM, a hardware-software co-design framework. KLLM features an index-based
computation scheme for efficient execution of MatMuls and nonlinear operations
on K-Means-quantized data, which avoids most of the dequantization and
full-precision computations. Moreover, KLLM incorporates a novel outlier
detection engine, Orizuru, that efficiently identifies the top-$k$ largest and
smallest elements in the activation data stream during online inference.
  Extensive experiments show that, on average, KLLM achieves speedups of 9.67x,
7.03x and energy efficiency improvements of 229.50x, 150.21x compared to the
A100 GPU and Atom, respectively.

</details>


### [232] [Neural Autoregressive Modeling of Brain Aging](https://arxiv.org/abs/2507.22954)
*Ridvan Yesiloglu,Wei Peng,Md Tauhidul Islam,Ehsan Adeli*

Main category: cs.LG

TL;DR: NeuroAR是一种基于生成式自回归Transformer的新型大脑衰老模拟模型，能够高保真度地模拟大脑衰老过程，并在图像质量和真实性方面优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 大脑衰老模拟是一个关键任务，在临床和计算神经科学中有广泛的应用。从早期MRI扫描预测受试者大脑的未来结构演变的能力，为了解衰老轨迹提供了宝贵的见解。然而，数据的 But, the high-dimensionality of data, subtle changes of structure across ages, and subject-specific patterns constitute challenges in the synthesis of the aging brain. This study aims to overcome these challenges.

Method: 提出了一种名为NeuroAR的新型大脑衰老模拟模型，该模型基于生成式自回归Transformer。NeuroAR通过自回归地从先前扫描和未来扫描的连接标记嵌入的便捷空间中估计未来扫描的离散标记图来合成衰老的大脑。为了指导生成，它将受试者的先前扫描连接到每个尺度，并通过交叉注意力在每个块中使用其采集年龄和目标年龄。

Result: NeuroAR在老年人和青少年受试者上的评估均显示出优于最先进的生成模型（包括潜在扩散模型（LDM）和生成对抗网络（GAN））的性能，特别是在图像保真度方面。此外，使用预训练的年龄预测器对合成图像的连贯性和真实性进行了验证，证明了其与预期衰老模式的一致性。

Conclusion: NeuroAR在模拟大脑衰老方面表现出色，能够以高保真度模拟具有特定个体的大脑衰老轨迹，并且在图像保真度方面优于最先进的生成模型，如潜在扩散模型（LDM）和生成对抗网络（GAN）。

Abstract: Brain aging synthesis is a critical task with broad applications in clinical
and computational neuroscience. The ability to predict the future structural
evolution of a subject's brain from an earlier MRI scan provides valuable
insights into aging trajectories. Yet, the high-dimensionality of data, subtle
changes of structure across ages, and subject-specific patterns constitute
challenges in the synthesis of the aging brain. To overcome these challenges,
we propose NeuroAR, a novel brain aging simulation model based on generative
autoregressive transformers. NeuroAR synthesizes the aging brain by
autoregressively estimating the discrete token maps of a future scan from a
convenient space of concatenated token embeddings of a previous and future
scan. To guide the generation, it concatenates into each scale the subject's
previous scan, and uses its acquisition age and the target age at each block
via cross-attention. We evaluate our approach on both the elderly population
and adolescent subjects, demonstrating superior performance over
state-of-the-art generative models, including latent diffusion models (LDM) and
generative adversarial networks, in terms of image fidelity. Furthermore, we
employ a pre-trained age predictor to further validate the consistency and
realism of the synthesized images with respect to expected aging patterns.
NeuroAR significantly outperforms key models, including LDM, demonstrating its
ability to model subject-specific brain aging trajectories with high fidelity.

</details>


### [233] [Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2 Neuromorphic Platform](https://arxiv.org/abs/2507.23562)
*Sirine Arfa,Bernhard Vogginger,Christian Mayr*

Main category: cs.LG

TL;DR: Energy-efficient RL using quantized SNNs on SpiNNaker2 outperforms GPUs in energy consumption (up to 32x less) while maintaining comparable latency, showing promise for real-time neuromorphic control.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of Spiking Neural Networks (SNNs) for energy-efficient reinforcement learning (RL) in robotic tasks, leveraging the low power consumption and low-latency inference capabilities of neuromorphic hardware.

Method: The paper implements a reinforcement learning (RL) algorithm using quantized Spiking Neural Networks (SNNs) trained with Q-learning. The network is then fine-tuned and quantized to 8-bit precision for deployment on the SpiNNaker2 neuromorphic chip. Performance is evaluated against a GTX 1650 GPU by analyzing inference latency, dynamic power consumption, and energy cost per inference.

Result: The SNN implementation on SpiNNaker2 achieved up to a 32x reduction in energy consumption compared to a GTX 1650 GPU. Inference latency was comparable to the GPU, with improvements in specific task settings, confirming SpiNNaker2's suitability for real-time neuromorphic control and efficient deep Q-learning.

Conclusion: Spiking Neural Networks (SNNs) combined with Q-learning and quantization show strong potential for energy-efficient reinforcement learning on neuromorphic hardware like SpiNNaker2, offering significant energy savings compared to traditional GPUs.

Abstract: Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power
consumption and low-latency inference on neuromorphic hardware for a wide range
of robotic tasks. In this work, we present an energy-efficient implementation
of a reinforcement learning (RL) algorithm using quantized SNNs to solve two
classical control tasks. The network is trained using the Q-learning algorithm,
then fine-tuned and quantized to low-bit (8-bit) precision for embedded
deployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative
advantage of SpiNNaker2 over conventional computing platforms, we analyze
inference latency, dynamic power consumption, and energy cost per inference for
our SNN models, comparing performance against a GTX 1650 GPU baseline. Our
results demonstrate SpiNNaker2's strong potential for scalable, low-energy
neuromorphic computing, achieving up to 32x reduction in energy consumption.
Inference latency remains on par with GPU-based execution, with improvements
observed in certain task settings, reinforcing SpiNNaker2's viability for
real-time neuromorphic control and making the neuromorphic approach a
compelling direction for efficient deep Q-learning.

</details>


### [234] [LLM-Assisted Cheating Detection in Korean Language via Keystrokes](https://arxiv.org/abs/2507.22956)
*Dong Hyun Roh,Rajesh Kumar,An Ngo*

Main category: cs.LG

TL;DR: 本研究提出了一种基于击键动力学的新方法，可以有效检测韩语写作中大模型（LLM）的辅助作弊行为。研究结果表明，该方法在不同认知负荷和写作策略下均表现出高检测率，并且在区分释义和转录的AI生成内容方面，模型比人类评估者更有效。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在弥补现有研究在语言覆盖、认知背景和大模型参与粒度方面的不足，提出一种针对韩语的基于击键的框架来检测大模型辅助作弊。

Method: 提出了一种基于击键的框架，用于检测韩语中大模型辅助作弊的行为。该框架利用了包括认知感知和非认知感知在内的多种分类器，并提取了解释性的时间节奏特征。

Result: 在认知感知评估场景下，时间特征表现良好；而在跨认知场景下，节奏特征具有更好的泛化性。此外，模型在区分正宗和转录响应方面优于人类评估者，并且比区分释义响应更容易。

Conclusion: 本研究证实，击键动力学特征能够可靠地检测不同认知需求和写作策略（包括释义和转录）下的大模型辅助写作。

Abstract: This paper presents a keystroke-based framework for detecting LLM-assisted
cheating in Korean, addressing key gaps in prior research regarding language
coverage, cognitive context, and the granularity of LLM involvement. Our
proposed dataset includes 69 participants who completed writing tasks under
three conditions: Bona fide writing, paraphrasing ChatGPT responses, and
transcribing ChatGPT responses. Each task spans six cognitive processes defined
in Bloom's Taxonomy (remember, understand, apply, analyze, evaluate, and
create). We extract interpretable temporal and rhythmic features and evaluate
multiple classifiers under both Cognition-Aware and Cognition-Unaware settings.
Temporal features perform well under Cognition-Aware evaluation scenarios,
while rhythmic features generalize better under cross-cognition scenarios.
Moreover, detecting bona fide and transcribed responses was easier than
paraphrased ones for both the proposed models and human evaluators, with the
models significantly outperforming the humans. Our findings affirm that
keystroke dynamics facilitate reliable detection of LLM-assisted writing across
varying cognitive demands and writing strategies, including paraphrasing and
transcribing LLM-generated responses.

</details>


### [235] [Scientific Machine Learning with Kolmogorov-Arnold Networks](https://arxiv.org/abs/2507.22959)
*Salah A. Faroughi,Farinaz Mostajeran,Amin Hamed Mashhadzadeh,Shirko Faroughi*

Main category: cs.LG

TL;DR: KANs正在取代MLPs，因为它们具有更好的可解释性、灵活性和更强的学习能力，在科学机器学习中表现出色，但仍需解决效率和复杂性问题。


<details>
  <summary>Details</summary>
Motivation: MLP在可解释性、激活函数灵活性和特征捕捉方面存在局限性，而KAN能够增强可解释性、灵活性，更有效地模拟复杂的非线性相互作用，克服MLP的约束。

Method: 本文通过对数据驱动学习、物理信息建模和深度算子学习三个视角下的KAN模型进行分类和评估，并与MLP模型进行比较。

Result: KANs在准确性、收敛速度和光谱表示方面优于MLPs，尤其在处理复杂动态方面表现出色。

Conclusion: KANs在准确性、收敛性和光谱表示方面持续改进，在捕捉复杂动态和更有效地学习方面具有优势。然而，在计算效率、理论保证、超参数调整和算法复杂性方面存在挑战，未来的研究方向包括提高KAN的鲁棒性、可扩展性和物理一致性。

Abstract: The field of scientific machine learning, which originally utilized
multilayer perceptrons (MLPs), is increasingly adopting Kolmogorov-Arnold
Networks (KANs) for data encoding. This shift is driven by the limitations of
MLPs, including poor interpretability, fixed activation functions, and
difficulty capturing localized or high-frequency features. KANs address these
issues with enhanced interpretability and flexibility, enabling more efficient
modeling of complex nonlinear interactions and effectively overcoming the
constraints associated with conventional MLP architectures. This review
categorizes recent progress in KAN-based models across three distinct
perspectives: (i) data-driven learning, (ii) physics-informed modeling, and
(iii) deep operator learning. Each perspective is examined through the lens of
architectural design, training strategies, application efficacy, and
comparative evaluation against MLP-based counterparts. By benchmarking KANs
against MLPs, we highlight consistent improvements in accuracy, convergence,
and spectral representation, clarifying KANs' advantages in capturing complex
dynamics while learning more effectively. Finally, this review identifies
critical challenges and open research questions in KAN development,
particularly regarding computational efficiency, theoretical guarantees,
hyperparameter tuning, and algorithm complexity. We also outline future
research directions aimed at improving the robustness, scalability, and
physical consistency of KAN-based frameworks.

</details>


### [236] [Multi-Hazard Early Warning Systems for Agriculture with Featural-Temporal Explanations](https://arxiv.org/abs/2507.22962)
*Boyuan Zheng,Victor W. Chu*

Main category: cs.LG

TL;DR: 本研究提出了一种结合深度学习和XAI的多灾种农业预警框架，提高了预测准确性和可解释性，有助于制定气候风险管理策略。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致的极端气候对农业构成了日益增长的风险，因此需要可靠的多灾种预警系统（EWS）。传统的单一灾害预测方法在捕捉并发气候事件之间复杂的相互作用方面存在不足。

Method: 本研究结合了序列深度学习模型和先进的可解释人工智能（XAI）技术，引入了一个用于农业的多灾种预测框架，集成了注意力机制和TimeSHAP（一种用于时间序列的递归XAI解释器），以提供全面的时间解释，揭示了哪些气候特征具有影响力以及它们的影响发生的时间。

Result: 实验结果表明，该框架具有很强的预测准确性，特别是BiLSTM架构，并突显了该系统为制定细致、主动的风险管理策略提供信息的能力。

Conclusion: 该研究显著提高了多灾种预警系统的可解释性和适用性，促进了跨学科的信任和农业气候风险管理中的有效决策过程。

Abstract: Climate extremes present escalating risks to agriculture intensifying the
need for reliable multi-hazard early warning systems (EWS). The situation is
evolving due to climate change and hence such systems should have the
intelligent to continue to learn from recent climate behaviours. However,
traditional single-hazard forecasting methods fall short in capturing complex
interactions among concurrent climatic events. To address this deficiency, in
this paper, we combine sequential deep learning models and advanced Explainable
Artificial Intelligence (XAI) techniques to introduce a multi-hazard
forecasting framework for agriculture. In our experiments, we utilize
meteorological data from four prominent agricultural regions in the United
States (between 2010 and 2023) to validate the predictive accuracy of our
framework on multiple severe event types, which are extreme cold, floods,
frost, hail, heatwaves, and heavy rainfall, with tailored models for each area.
The framework uniquely integrates attention mechanisms with TimeSHAP (a
recurrent XAI explainer for time series) to provide comprehensive temporal
explanations revealing not only which climatic features are influential but
precisely when their impacts occur. Our results demonstrate strong predictive
accuracy, particularly with the BiLSTM architecture, and highlight the system's
capacity to inform nuanced, proactive risk management strategies. This research
significantly advances the explainability and applicability of multi-hazard
EWS, fostering interdisciplinary trust and effective decision-making process
for climate risk management in the agricultural industry.

</details>


### [237] [FedCVD++: Communication-Efficient Federated Learning for Cardiovascular Risk Prediction with Parametric and Non-Parametric Model Optimization](https://arxiv.org/abs/2507.22963)
*Abdelrhman Gaber,Hassan Abd-Eltawab,John Elgallab,Youssif Abuzied,Dineo Mpanya,Turgay Celik,Swarun Kumar,Tamer ElBatt*

Main category: cs.LG

TL;DR: FedCVD++是一个增强的联邦学习框架，通过集成参数和非参数模型，以及通信效率优化，用于保护隐私的冠心病风险预测。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病（CVD）的广泛流行以及对隐私保护的预测系统的迫切需求。

Method: FedCVD++框架集成了参数模型（逻辑回归、支持向量机、神经网络）和非参数模型（随机森林、XGBoost），并通过（1）树子集采样、(2) 基于XGBoost的特征提取和（3）联邦SMOTE同步来解决联邦学习的挑战。

Result: FedCVD++在Framingham数据集上实现了最先进的结果，其联邦XGBoost F1分数（0.80）优于中心化方法（0.78），联邦随机森林F1分数（0.81）与非联邦方法相当。此外，通信效率策略将带宽消耗减少了3.2倍，同时保持了95%的准确性。与现有联邦学习框架相比，FedCVD++提供了高15%的F1分数和卓越的可扩展性。

Conclusion: FedCVD++是首个将非参数模型集成到联邦医疗保健系统中的实用集成，提供了一个在真实世界临床限制下得到验证的隐私保护解决方案。

Abstract: Cardiovascular diseases (CVD) cause over 17 million deaths annually
worldwide, highlighting the urgent need for privacy-preserving predictive
systems. We introduce FedCVD++, an enhanced federated learning (FL) framework
that integrates both parametric models (logistic regression, SVM, neural
networks) and non-parametric models (Random Forest, XGBoost) for coronary heart
disease risk prediction. To address key FL challenges, we propose: (1)
tree-subset sampling that reduces Random Forest communication overhead by 70%,
(2) XGBoost-based feature extraction enabling lightweight federated ensembles,
and (3) federated SMOTE synchronization for resolving cross-institutional class
imbalance.
  Evaluated on the Framingham dataset (4,238 records), FedCVD++ achieves
state-of-the-art results: federated XGBoost (F1 = 0.80) surpasses its
centralized counterpart (F1 = 0.78), and federated Random Forest (F1 = 0.81)
matches non-federated performance. Additionally, our communication-efficient
strategies reduce bandwidth consumption by 3.2X while preserving 95% accuracy.
  Compared to existing FL frameworks, FedCVD++ delivers up to 15% higher
F1-scores and superior scalability for multi-institutional deployment. This
work represents the first practical integration of non-parametric models into
federated healthcare systems, providing a privacy-preserving solution validated
under real-world clinical constraints.

</details>


### [238] [DynaSwarm: Dynamically Graph Structure Selection for LLM-based Multi-agent System](https://arxiv.org/abs/2507.23261)
*Hui Yi Leong,Yuqing Wu*

Main category: cs.LG

TL;DR: DynaSwarm 是一个动态框架，通过 A2C 和动态图选择器优化 LLM 的多智能体系统，实现样本自适应的结构灵活性，并在多项任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统（MAS）框架依赖于手动设计的、静态的协作图结构，这限制了系统的适应性和性能。

Method: 提出了一种包含 actor-critic 强化学习（A2C）机制和动态图选择器的 DynaSwarm 框架。A2C 机制用于优化图结构，并提高稳定性；动态图选择器则通过参数高效的 LLM 微调为每个输入样本自适应地选择最优图结构。此外，还提出对演示检索器进行微调，以充分利用上下文学习（ICL）能力。

Result: DynaSwarm 框架通过样本特定的路由消除了对僵化、通用图架构的需求，利用样本的特殊性通过专门的智能体网络动态路由查询。

Conclusion: DynaSwarm 框架在问答、数学推理和编码任务上持续优于最先进的单智能体和多智能体系统基线，并支持多种 LLM 主干。研究结果强调了在 LLM MAS 设计中，样本感知结构灵活性的重要性。

Abstract: Current multi-agent systems (MAS) frameworks often rely on manually designed
and static collaboration graph structures, limiting adaptability and
performance. To address these limitations, we propose DynaSwarm, a dynamic
framework that enhances LLM-based MAS through two key innovations: (1) an
actor-critic reinforcement learning (A2C) mechanism to optimize graph
structures with improved stability over prior RL methods, and (2) a dynamic
graph selector that adaptively chooses the optimal graph structure for each
input sample via parameter-efficient LLM fine-tuning. DynaSwarm eliminates the
need for rigid, one-fits-all graph architectures, instead leveraging
sample-specific idiosyncrasies to dynamically route queries through specialized
agent networks. (c) We propose to fine-tune the demonstration retriever to
fully exploit the power of in-context learning (ICL). Extensive experiments on
question answering, mathematical reasoning, and coding tasks demonstrate that
DynaSwarm consistently outperforms state-of-the-art single-agent and MAS
baselines across multiple LLM backbones. Our findings highlight the importance
of sample-aware structural flexibility in LLM MAS designs.

</details>


### [239] [Improved Algorithms for Kernel Matrix-Vector Multiplication Under Sparsity Assumptions](https://arxiv.org/abs/2507.23539)
*Piotr Indyk,Michael Kapralov,Kshiteej Sheth,Tal Wagner*

Main category: cs.LG

TL;DR: 该研究提出了一种用于不对称高斯核矩阵的快速算法，以解决注意力矩阵的处理问题。


<details>
  <summary>Details</summary>
Motivation: 受注意力矩阵快速处理问题的启发。

Method: 该算法依赖于不对称高斯核矩阵的矩阵向量乘法的快速算法。

Result: 生成一个y，使得||Kx-y||_2 <= ε||x||_2，其时间复杂度在n上是子二次的，在d上是线性的。

Conclusion: 我们获得了第一个在不受限制的向量下，在这一假设下运行的子二次时间算法。

Abstract: Motivated by the problem of fast processing of attention matrices, we study
fast algorithms for computing matrix-vector products for asymmetric Gaussian
Kernel matrices $K\in \mathbb{R}^{n\times n}$. $K$'s columns are indexed by a
set of $n$ keys $k_1,k_2\ldots, k_n\in \mathbb{R}^d$, rows by a set of $n$
queries $q_1,q_2,\ldots,q_n\in \mathbb{R}^d $, and its $i,j$ entry is $K_{ij} =
e^{-\|q_i-k_j\|_2^2/2\sigma^2}$ for some bandwidth parameter $\sigma>0$. Given
a vector $x\in \mathbb{R}^n$ and error parameter $\epsilon>0$, our task is to
output a $y\in \mathbb{R}^n$ such that $\|Kx-y\|_2\leq \epsilon \|x\|_2$ in
time subquadratic in $n$ and linear in $d$. Our algorithms rely on the
following modelling assumption about the matrices $K$: the sum of the entries
of $K$ scales linearly in $n$, as opposed to worst case quadratic growth. We
validate this assumption experimentally, for Gaussian kernel matrices
encountered in various settings such as fast attention computation in LLMs. We
obtain the first subquadratic-time algorithm that works under this assumption,
for unrestricted vectors.

</details>


### [240] [Planning for Cooler Cities: A Multimodal AI Framework for Predicting and Mitigating Urban Heat Stress through Urban Landscape Transformation](https://arxiv.org/abs/2507.23000)
*Shengao Yi,Xiaojiang Li,Wei Tu,Tianhong Zhao*

Main category: cs.LG

TL;DR: GSM-UTCI是一个基于深度学习的城市热应力预测框架，可实现超本地分辨率的UTCI预测，比传统模型快得多，并为城市规划中的绿化策略提供支持。


<details>
  <summary>Details</summary>
Motivation: 随着气候变化和城市化导致极端高温事件加剧，城市在减缓户外热应力方面面临日益增长的挑战。传统的物理模型计算成本高昂，限制了其在城市规划中的可扩展性。

Method: 提出了一种名为GSM-UTCI的多模态深度学习框架，该框架利用表面形貌、高分辨率土地覆盖数据和每小时气象条件，并通过特征wise线性调制（FiLM）架构动态地将空间特征条件化为大气背景，以预测1米超本地分辨率的日平均通用热气候指数（UTCI）。

Result: GSM-UTCI在SOLWEIG衍生的UTCI图上进行训练，实现了接近物理的精度（R2为0.9151，平均绝对误差为0.41°C），同时将整个城市（如费城）的推理时间从几小时缩短到不到五分钟。模拟的景观改造场景（例如，将裸土、草地和不透水表面替换为树冠）显示出具有异质性但始终强大的降温效果，其中不透水地表转换为树冠产生的累积效益最高（整个城市平均UTCI变化为-4.18°C）。

Conclusion: GSM-UTCI是一个可扩展、细粒度的决策支持工具，用于城市气候适应，能够针对不同城市环境的绿化策略进行基于场景的评估。

Abstract: As extreme heat events intensify due to climate change and urbanization,
cities face increasing challenges in mitigating outdoor heat stress. While
traditional physical models such as SOLWEIG and ENVI-met provide detailed
assessments of human-perceived heat exposure, their computational demands limit
scalability for city-wide planning. In this study, we propose GSM-UTCI, a
multimodal deep learning framework designed to predict daytime average
Universal Thermal Climate Index (UTCI) at 1-meter hyperlocal resolution. The
model fuses surface morphology (nDSM), high-resolution land cover data, and
hourly meteorological conditions using a feature-wise linear modulation (FiLM)
architecture that dynamically conditions spatial features on atmospheric
context. Trained on SOLWEIG-derived UTCI maps, GSM-UTCI achieves near-physical
accuracy, with an R2 of 0.9151 and a mean absolute error (MAE) of 0.41{\deg}C,
while reducing inference time from hours to under five minutes for an entire
city. To demonstrate its planning relevance, we apply GSM-UTCI to simulate
systematic landscape transformation scenarios in Philadelphia, replacing bare
earth, grass, and impervious surfaces with tree canopy. Results show spatially
heterogeneous but consistently strong cooling effects, with impervious-to-tree
conversion producing the highest aggregated benefit (-4.18{\deg}C average
change in UTCI across 270.7 km2). Tract-level bivariate analysis further
reveals strong alignment between thermal reduction potential and land cover
proportions. These findings underscore the utility of GSM-UTCI as a scalable,
fine-grained decision support tool for urban climate adaptation, enabling
scenario-based evaluation of greening strategies across diverse urban
environments.

</details>


### [241] [Stop Evaluating AI with Human Tests, Develop Principled, AI-specific Tests instead](https://arxiv.org/abs/2507.23009)
*Tom Sühr,Florian E. Dorner,Olawale Salaudeen,Augustin Kelava,Samira Samadi*

Main category: cs.LG

TL;DR: 不要用人类测试来评估AI，应该开发专门的AI测试。


<details>
  <summary>Details</summary>
Motivation: 当前将LLM在人类基准测试上的表现解读为具有人类特征的观点是一种本体论错误，需要建立更适合AI的评估方法。

Method: 对现有评估AI的基准测试进行了批判性分析，并提出了开发新评估框架的必要性。

Result: 目前的评估方式存在理论和实证依据不足的问题，可能导致对AI能力的误解。

Conclusion: 应停止使用针对人类的测试来评估人工智能，而应开发专门针对人工智能的、有原则的测试。

Abstract: Large Language Models (LLMs) have achieved remarkable results on a range of
standardized tests originally designed to assess human cognitive and
psychological traits, such as intelligence and personality. While these results
are often interpreted as strong evidence of human-like characteristics in LLMs,
this paper argues that such interpretations constitute an ontological error.
Human psychological and educational tests are theory-driven measurement
instruments, calibrated to a specific human population. Applying these tests to
non-human subjects without empirical validation, risks mischaracterizing what
is being measured. Furthermore, a growing trend frames AI performance on
benchmarks as measurements of traits such as ``intelligence'', despite known
issues with validity, data contamination, cultural bias and sensitivity to
superficial prompt changes. We argue that interpreting benchmark performance as
measurements of human-like traits, lacks sufficient theoretical and empirical
justification. This leads to our position: Stop Evaluating AI with Human Tests,
Develop Principled, AI-specific Tests instead. We call for the development of
principled, AI-specific evaluation frameworks tailored to AI systems. Such
frameworks might build on existing frameworks for constructing and validating
psychometrics tests, or could be created entirely from scratch to fit the
unique context of AI.

</details>


### [242] [Designing Dynamic Pricing for Bike-sharing Systems via Differentiable Agent-based Simulation](https://arxiv.org/abs/2507.23344)
*Tatsuya Mitomi,Fumiyasu Makinoshima,Fumiya Makihara,Eigo Segawa*

Main category: cs.LG

TL;DR: 该研究提出了一种新的可微分的基于主体的模拟方法，用于解决共享单车系统的动态定价问题，以应对不平衡的库存问题。该方法在效率和准确性上均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 共享单车系统用户需求时空变化导致自行车站点库存不平衡，产生額外的重新分配成本，因此需要通过最优动态定价来管理用户需求。

Method: 开发了一种可微分的基于主体的模拟方法，用于在共享单车系统中快速设计动态定价。

Result: 与传统方法相比，该方法在数值实验中获得了更准确的解决方案，损失减少了 73% 至 78%，同时收敛速度提高了 100 多倍。在大规模城市共享单车系统场景中，通过模拟获得的定价策略，证实了该策略能够自然地诱导平衡库存，无需人工重新分配。

Conclusion: 该方法能够自然地诱导平衡库存，无需人工重新分配，并且可以通过设置适当的初始条件来最小化诱导平衡库存的折扣成本。

Abstract: Bike-sharing systems are emerging in various cities as a new ecofriendly
transportation system. In these systems, spatiotemporally varying user demands
lead to imbalanced inventory at bicycle stations, resulting in additional
relocation costs. Therefore, it is essential to manage user demand through
optimal dynamic pricing for the system. However, optimal pricing design for
such a system is challenging because the system involves users with diverse
backgrounds and their probabilistic choices. To address this problem, we
develop a differentiable agent-based simulation to rapidly design dynamic
pricing in bike-sharing systems, achieving balanced bicycle inventory despite
spatiotemporally heterogeneous trips and probabilistic user decisions. We first
validate our approach against conventional methods through numerical
experiments involving 25 bicycle stations and five time slots, yielding 100
parameters. Compared to the conventional methods, our approach obtains a more
accurate solution with a 73% to 78% reduction in loss while achieving more than
a 100-fold increase in convergence speed. We further validate our approach on a
large-scale urban bike-sharing system scenario involving 289 bicycle stations,
resulting in a total of 1156 parameters. Through simulations using the obtained
pricing policies, we confirm that these policies can naturally induce balanced
inventory without any manual relocation. Additionally, we find that the cost of
discounts to induce the balanced inventory can be minimized by setting
appropriate initial conditions.

</details>


### [243] [Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods](https://arxiv.org/abs/2507.23010)
*Siwoo Park*

Main category: cs.LG

TL;DR: 多模态大模型在处理反向任务时表现不佳，其潜在空间无法支持有意义且连贯的反向映射，这暴露了当前模型在语义丰富性和可逆性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 探究多模态潜在空间在任务特定AI模型中的反向能力和更广泛的效用，因为这些模型在设计好的前向任务上的潜力已被探索，但反向映射的潜力在很大程度上仍未被发掘。

Method: 提出一个基于优化的框架，用于从期望的输出来推断输入特征，并将其应用于文本-图像和文本-音频模态。

Result: 实验结果一致表明，虽然优化可以引导模型完成反向任务（例如，文本到图像模型生成图像，然后由图像字幕模型正确描述，或者自动语音识别模型准确转录优化后的音频），但这些反向映射的感知质量混乱且不连贯。此外，在尝试从生成模型中推断原始语义输入时，重构的潜在空间嵌入经常缺乏语义可解释性。

Conclusion: 现有的多模态大模型在反向任务上存在局限性，其多模态潜在空间在支持有意义且连贯的反向映射方面并不一致，这表明需要进一步研究来开发真正语义丰富且可逆的多模态潜在空间。

Abstract: This paper investigates the inverse capabilities and broader utility of
multimodal latent spaces within task-specific AI (Artificial Intelligence)
models. While these models excel at their designed forward tasks (e.g.,
text-to-image generation, audio-to-text transcription), their potential for
inverse mappings remains largely unexplored. We propose an optimization-based
framework to infer input characteristics from desired outputs, applying it
bidirectionally across Text-Image (BLIP, Flux.1-dev) and Text-Audio
(Whisper-Large-V3, Chatterbox-TTS) modalities.
  Our central hypothesis posits that while optimization can guide models
towards inverse tasks, their multimodal latent spaces will not consistently
support semantically meaningful and perceptually coherent inverse mappings.
Experimental results consistently validate this hypothesis. We demonstrate that
while optimization can force models to produce outputs that align textually
with targets (e.g., a text-to-image model generating an image that an image
captioning model describes correctly, or an ASR model transcribing optimized
audio accurately), the perceptual quality of these inversions is chaotic and
incoherent. Furthermore, when attempting to infer the original semantic input
from generative models, the reconstructed latent space embeddings frequently
lack semantic interpretability, aligning with nonsensical vocabulary tokens.
  These findings highlight a critical limitation. multimodal latent spaces,
primarily optimized for specific forward tasks, do not inherently possess the
structure required for robust and interpretable inverse mappings. Our work
underscores the need for further research into developing truly semantically
rich and invertible multimodal latent spaces.

</details>


### [244] [Linking Actor Behavior to Process Performance Over Time](https://arxiv.org/abs/2507.23037)
*Aurélie Leribaux,Rafael Oyamada,Johannes De Smedt,Zahra Dasht Bozorgi,Artem Polyvyanyy,Jochen De Weerdt*

Main category: cs.LG

TL;DR: This paper integrates actor behavior analysis with Granger causality to study the impact of actor behavior on process outcomes using time series data, finding that actor behavior significantly impacts process performance, especially throughput time.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches often use aggregate and static process data, overlooking the temporal and causal dynamics that arise from individual actor behavior, limiting the ability to accurately capture the complexity of real-world processes.

Method: Integrating actor behavior analysis with Granger causality to identify correlating links in time series data. Using Group Lasso for lag selection to identify influential lags.

Result: Identified a small but consistently influential set of lags that capture the majority of causal influence, revealing that actor behavior has direct and measurable impacts on process performance, particularly throughput time.

Conclusion: Actor behavior has direct and measurable impacts on process performance, particularly throughput time. Actor-centric, time series-based methods can uncover temporal dependencies that drive process outcomes, offering a more nuanced understanding of how individual behaviors impact overall process efficiency.

Abstract: Understanding how actor behavior influences process outcomes is a critical
aspect of process mining. Traditional approaches often use aggregate and static
process data, overlooking the temporal and causal dynamics that arise from
individual actor behavior. This limits the ability to accurately capture the
complexity of real-world processes, where individual actor behavior and
interactions between actors significantly shape performance. In this work, we
address this gap by integrating actor behavior analysis with Granger causality
to identify correlating links in time series data. We apply this approach to
realworld event logs, constructing time series for actor interactions, i.e.
continuation, interruption, and handovers, and process outcomes. Using Group
Lasso for lag selection, we identify a small but consistently influential set
of lags that capture the majority of causal influence, revealing that actor
behavior has direct and measurable impacts on process performance, particularly
throughput time. These findings demonstrate the potential of actor-centric,
time series-based methods for uncovering the temporal dependencies that drive
process outcomes, offering a more nuanced understanding of how individual
behaviors impact overall process efficiency.

</details>


### [245] [Prediction of Significant Creatinine Elevation in First ICU Stays with Vancomycin Use: A retrospective study through Catboost](https://arxiv.org/abs/2507.23043)
*Junyi Fan,Li Sun,Shuheng Chen,Yong Si,Minoo Ahmadi,Greg Placencia,Elham Pishgar,Kamiar Alaei,Maryam Pishgar*

Main category: cs.LG

TL;DR: 研究开发了一个机器学习模型，利用常规ICU数据（如磷酸盐、总胆红素、镁等）成功预测了万古霉素引起的肾损伤，CatBoost模型表现最佳，准确率高且可解释。


<details>
  <summary>Details</summary>
Motivation: 万古霉素是ICU严重革兰氏阳性感染的关键抗生素，但具有高肾毒性风险。早期预测危重患者的肾损伤具有挑战性。本研究旨在开发一种机器学习模型，利用常规ICU数据预测万古霉素相关的肌酐升高。

Method: 我们分析了来自MIMIC-IV数据库的10,288名接受万古霉素治疗的ICU患者（年龄18-80岁）。通过SelectKBest（前30名）和随机森林排名（最终15名）选择特征。使用5倍交叉验证测试了六种算法。使用SHAP、累积局部效应（ALE）和贝叶斯后验采样评估了可解释性。

Result: 在10,288名患者中，2,903名（28.2%）出现肌酐升高。CatBoost表现最佳（AUROC 0.818 [95% CI: 0.801-0.834]，敏感性0.800，特异性0.681，阴性预测值0.900）。关键预测因素包括磷酸盐、总胆红素、镁、Charlson指数和APSIII。SHAP证实磷酸盐是主要危险因素。ALE显示了剂量-反应模式。贝叶斯分析估计高风险病例的平均风险为60.5%（95%可信区间：16.8-89.4%）。

Conclusion: 这项机器学习模型能够从常规ICU数据中预测万古霉素相关的肌酐升高，具有很高的准确性和可解释性，能够实现早期风险检测，并为重症监护中的及时干预提供支持。

Abstract: Background: Vancomycin, a key antibiotic for severe Gram-positive infections
in ICUs, poses a high nephrotoxicity risk. Early prediction of kidney injury in
critically ill patients is challenging. This study aimed to develop a machine
learning model to predict vancomycin-related creatinine elevation using routine
ICU data.
  Methods: We analyzed 10,288 ICU patients (aged 18-80) from the MIMIC-IV
database who received vancomycin. Kidney injury was defined by KDIGO criteria
(creatinine rise >=0.3 mg/dL within 48h or >=50% within 7d). Features were
selected via SelectKBest (top 30) and Random Forest ranking (final 15). Six
algorithms were tested with 5-fold cross-validation. Interpretability was
evaluated using SHAP, Accumulated Local Effects (ALE), and Bayesian posterior
sampling.
  Results: Of 10,288 patients, 2,903 (28.2%) developed creatinine elevation.
CatBoost performed best (AUROC 0.818 [95% CI: 0.801-0.834], sensitivity 0.800,
specificity 0.681, negative predictive value 0.900). Key predictors were
phosphate, total bilirubin, magnesium, Charlson index, and APSIII. SHAP
confirmed phosphate as a major risk factor. ALE showed dose-response patterns.
Bayesian analysis estimated mean risk 60.5% (95% credible interval: 16.8-89.4%)
in high-risk cases.
  Conclusions: This machine learning model predicts vancomycin-associated
creatinine elevation from routine ICU data with strong accuracy and
interpretability, enabling early risk detection and supporting timely
interventions in critical care.

</details>


### [246] [Locally Differentially Private Thresholding Bandits](https://arxiv.org/abs/2507.23073)
*Annalisa Barbara,Joseph Lazzaro,Ciara Pike-Burke*

Main category: cs.LG

TL;DR: 本研究提出了在阈值 the bandit 问题中实现差分隐私的算法，并证明了其最优性。


<details>
  <summary>Details</summary>
Motivation: 研究确保阈值 the bandit 问题中的局部差分隐私的影响。

Method: 提出利用 Bernoulli 差分隐私机制的私有响应来识别预期回报超过预定阈值的臂，并为固定预算和固定置信度设置推导了理论性能边界。

Result: 所提出的算法匹配了所提出的下界（在多对数因子内）。

Conclusion: 本研究为阈值 the bandit 问题提供了首个差分隐私保证，并推导了相应的下界，证明了所提出算法的最优性（在多对数因子内）。

Abstract: This work investigates the impact of ensuring local differential privacy in
the thresholding bandit problem. We consider both the fixed budget and fixed
confidence settings. We propose methods that utilize private responses,
obtained through a Bernoulli-based differentially private mechanism, to
identify arms with expected rewards exceeding a predefined threshold. We show
that this procedure provides strong privacy guarantees and derive theoretical
performance bounds on the proposed algorithms. Additionally, we present general
lower bounds that characterize the additional loss incurred by any
differentially private mechanism, and show that the presented algorithms match
these lower bounds up to poly-logarithmic factors. Our results provide valuable
insights into privacy-preserving decision-making frameworks in bandit problems.

</details>


### [247] [A Foundation Model for Material Fracture Prediction](https://arxiv.org/abs/2507.23077)
*Agnese Marcato,Aleksandra Pachalieva,Ryley G. Hill,Kai Gao,Xiaoyu Wang,Esteban Rougier,Zhou Lei,Vinamra Agrawal,Janel Chua,Qinjun Kang,Jeffrey D. Hyman,Abigail Hunter,Nathan DeBardeleben,Earl Lawrence,Hari Viswanathan,Daniel O'Malley,Javier E. Santos*

Main category: cs.LG

TL;DR: 该研究提出了一个基于 Transformer 的基础模型，用于跨多种材料、几何形状和加载条件进行断裂预测，解决了传统机器学习模型泛化能力不足和基于物理的模拟器效率低下等问题。该模型能够处理结构化和非结构化网格，并通过语言模型嵌入来理解文本输入，从而实现灵活的适应性和跨模拟器的兼容性。实验证明，该模型只需少量数据即可在各种下游任务（如预测失效时间、模拟断裂演化）上进行微调，并能推广到未见过的材料，展示了统一和简化断裂预测工作流程的潜力。


<details>
  <summary>Details</summary>
Motivation: 准确预测材料何时以及如何失效对于设计在应力下运行的安全、可靠的结构、机械系统和工程部件至关重要。然而，在实际应用中的材料、几何形状和载荷条件的广泛性方面，断裂行为仍然难以建模。机器学习（ML）方法虽然显示出前景，但大多数模型都在狭窄的数据集上进行训练，鲁棒性不足，并且难以泛化。同时，基于物理的模拟器提供高保真预测，但它们分散在专门的方法中，并且需要大量的高性能计算资源来探索输入空间。

Method: 提出一个数据驱动的断裂预测基础模型，该模型基于 Transformer 的架构，可以跨模拟器、多种材料（包括塑料粘合炸药、钢、铝、页岩和钨）以及各种加载条件运行。该模型支持结构化和非结构化网格，并将它们与指定材料属性、边界条件和求解器设置的文本输入的语言模型嵌入相结合。

Result: 该模型支持结构化和非结构化网格，将它们与指定材料属性、边界条件和求解器设置的文本输入的语言模型嵌入相结合。这种多模式输入设计能够灵活适应各种模拟场景，而无需更改模型架构。所训练的模型可以通过最少的数据针对不同的下游任务进行微调，包括时间故障估计、断裂演化建模以及适应联合有限离散元素方法模拟。它还可以推广到未见的材料，如钛和混凝土，仅需一个样本，与标准的机器学习相比，数据需求大大减少。我们的结果表明，断裂预测可以通过单一模型架构统一起来，为特定于模拟器的工作流程提供一种可扩展、可扩展的替代方案。

Conclusion: 可以为各种模拟场景提供灵活适应，并且可以对各种下游任务进行微调，例如时间故障估计，对模型断裂演化以及适应联合有限离散元素方法模拟。

Abstract: Accurately predicting when and how materials fail is critical to designing
safe, reliable structures, mechanical systems, and engineered components that
operate under stress. Yet, fracture behavior remains difficult to model across
the diversity of materials, geometries, and loading conditions in real-world
applications. While machine learning (ML) methods show promise, most models are
trained on narrow datasets, lack robustness, and struggle to generalize.
Meanwhile, physics-based simulators offer high-fidelity predictions but are
fragmented across specialized methods and require substantial high-performance
computing resources to explore the input space. To address these limitations,
we present a data-driven foundation model for fracture prediction, a
transformer-based architecture that operates across simulators, a wide range of
materials (including plastic-bonded explosives, steel, aluminum, shale, and
tungsten), and diverse loading conditions. The model supports both structured
and unstructured meshes, combining them with large language model embeddings of
textual input decks specifying material properties, boundary conditions, and
solver settings. This multimodal input design enables flexible adaptation
across simulation scenarios without changes to the model architecture. The
trained model can be fine-tuned with minimal data on diverse downstream tasks,
including time-to-failure estimation, modeling fracture evolution, and adapting
to combined finite-discrete element method simulations. It also generalizes to
unseen materials such as titanium and concrete, requiring as few as a single
sample, dramatically reducing data needs compared to standard ML. Our results
show that fracture prediction can be unified under a single model architecture,
offering a scalable, extensible alternative to simulator-specific workflows.

</details>


### [248] [On the Sustainability of AI Inferences in the Edge](https://arxiv.org/abs/2507.23093)
*Ghazal Sobhani,Md. Monzurul Amin Ifath,Tushar Sharma,Israat Haque*

Main category: cs.LG

TL;DR: 本研究对多种边缘设备（树莓派、Intel神经计算棒、NVIDIA Jetson nano、Google Coral USB）上运行的AI模型（传统神经网络、大型语言模型）的性能和能耗进行了量化分析，以指导实际的边缘AI部署。


<details>
  <summary>Details</summary>
Motivation: 尽管这些设备已用于边缘部署的AI推理，但没有研究对其性能和能耗进行量化分析，以指导在设备和模型选择上做出明智的决策，满足应用需求。本研究旨在填补这一空白。

Method: 对传统神经网络和大型语言模型在树莓派、Intel神经计算棒、NVIDIA Jetson nano和Google Coral USB等边缘设备上的性能进行表征分析，并分析了模型F1分数、推理时间、推理功耗和内存使用之间的权衡。

Result: 本研究对传统神经网络和大型语言模型在树莓派、Intel神经计算棒、NVIDIA Jetson nano和Google Coral USB等边缘设备上的性能进行了严格的表征，分析了模型F1分数、推理时间、推理功耗和内存使用之间的权衡。

Conclusion: 硬件和框架优化以及AI模型的外部参数调整可以平衡模型性能和资源使用，从而实现实际的边缘AI部署。

Abstract: The proliferation of the Internet of Things (IoT) and its cutting-edge
AI-enabled applications (e.g., autonomous vehicles and smart industries)
combine two paradigms: data-driven systems and their deployment on the edge.
Usually, edge devices perform inferences to support latency-critical
applications. In addition to the performance of these resource-constrained edge
devices, their energy usage is a critical factor in adopting and deploying edge
applications. Examples of such devices include Raspberry Pi (RPi), Intel Neural
Compute Stick (INCS), NVIDIA Jetson nano (NJn), and Google Coral USB (GCU).
Despite their adoption in edge deployment for AI inferences, there is no study
on their performance and energy usage for informed decision-making on the
device and model selection to meet the demands of applications. This study
fills the gap by rigorously characterizing the performance of traditional,
neural networks, and large language models on the above-edge devices.
Specifically, we analyze trade-offs among model F1 score, inference time,
inference power, and memory usage. Hardware and framework optimization, along
with external parameter tuning of AI models, can balance between model
performance and resource usage to realize practical edge AI deployments.

</details>


### [249] [Scalable Generative Modeling of Weighted Graphs](https://arxiv.org/abs/2507.23111)
*Richard Williams,Eric Nalisnick,Andrew Holbrook*

Main category: cs.LG

TL;DR: BiGG-E 是一个用于生成加权图的自回归模型，它能有效学习节点和边权重之间的联合分布，并能高效生成大规模加权图。


<details>
  <summary>Details</summary>
Motivation: 当前的深度生成模型要么为非加权图设计且不易扩展到加权拓扑，要么在不考虑与拓扑的联合分布的情况下纳入边权重。此外，在加权图上学习分布必须同时考虑图的边以及每条边的相应权重之间复杂的非局部依赖关系。

Method: 开发了一个自回归模型 BiGG-E，它是 BiGG 模型的非平凡扩展，用于学习加权图上的联合分布，同时利用稀疏性。

Result: BiGG-E 可以在 O((n+m)log n) 时间内生成一个具有 n 个节点和 m 个边的加权图。模拟研究和在各种基准数据集上的实验证明了 BiGG-E 的优越性。

Conclusion: BiGG-E 能够更好地捕获加权图的分布，同时保持可扩展性和计算效率。

Abstract: Weighted graphs are ubiquitous throughout biology, chemistry, and the social
sciences, motivating the development of generative models for abstract weighted
graph data using deep neural networks. However, most current deep generative
models are either designed for unweighted graphs and are not easily extended to
weighted topologies or incorporate edge weights without consideration of a
joint distribution with topology. Furthermore, learning a distribution over
weighted graphs must account for complex nonlocal dependencies between both the
edges of the graph and corresponding weights of each edge. We develop an
autoregressive model BiGG-E, a nontrivial extension of the BiGG model, that
learns a joint distribution over weighted graphs while still exploiting
sparsity to generate a weighted graph with $n$ nodes and $m$ edges in $O((n +
m)\log n)$ time. Simulation studies and experiments on a variety of benchmark
datasets demonstrate that BiGG-E best captures distributions over weighted
graphs while remaining scalable and computationally efficient.

</details>


### [250] [FLOSS: Federated Learning with Opt-Out and Straggler Support](https://arxiv.org/abs/2507.23115)
*David J Goetze,Dahlia J Felten,Jeannie R Albrecht,Rohit Bhattacharya*

Main category: cs.LG

TL;DR: Federated learning systems face challenges with missing data due to user opt-outs and straggler devices, which degrade model performance. This paper introduces FLOSS, a system to mitigate these issues.


<details>
  <summary>Details</summary>
Motivation: Modern data privacy agreements allow users to opt-out of data sharing, and stragglers from heterogeneous device capabilities lead to missing data, introducing bias and degrading model performance.

Method: The paper presents FLOSS, a system designed to mitigate the impacts of missing data in federated learning.

Result: The system FLOSS was developed to address these issues and its performance was empirically demonstrated in simulations.

Conclusion: FLOSS mitigated the impacts of missing data on federated learning in the presence of stragglers and user opt-out, and its performance was empirically demonstrated in simulations.

Abstract: Previous work on data privacy in federated learning systems focuses on
privacy-preserving operations for data from users who have agreed to share
their data for training. However, modern data privacy agreements also empower
users to use the system while opting out of sharing their data as desired. When
combined with stragglers that arise from heterogeneous device capabilities, the
result is missing data from a variety of sources that introduces bias and
degrades model performance. In this paper, we present FLOSS, a system that
mitigates the impacts of such missing data on federated learning in the
presence of stragglers and user opt-out, and empirically demonstrate its
performance in simulations.

</details>


### [251] [Evaluating and Improving the Robustness of Speech Command Recognition Models to Noise and Distribution Shifts](https://arxiv.org/abs/2507.23128)
*Anaïs Baranger,Lucas Maison*

Main category: cs.LG

TL;DR: 本研究调查了训练条件和输入特征对语音模型在OOD条件下的鲁棒性和泛化能力的影响，并使用公平性和鲁棒性指标进行了量化。研究发现，噪音感知训练在某些情况下可以提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探究训练条件和输入特征对语音模型在OOD条件下的鲁棒性和泛化能力的影响，以及评估噪音对泛化的影响。

Method: 本研究调查了训练条件和输入特征如何影响带噪语音关键词分类器在OOD条件下的鲁棒性和泛化能力。研究人员使用了公平性（F）和鲁棒性（R）两个指标来量化噪音对泛化的影响，并对几种神经架构在多种评估集上的表现进行了基准测试。

Result: 研究结果表明，在某些配置下，噪音感知训练能够提高模型的鲁棒性。

Conclusion: 噪音感知训练在某些配置下可以提高鲁棒性，这为基于噪音的增强在语音模型泛化中的作用和局限性提供了新的见解。

Abstract: Although prior work in computer vision has shown strong correlations between
in-distribution (ID) and out-of-distribution (OOD) accuracies, such
relationships remain underexplored in audio-based models. In this study, we
investigate how training conditions and input features affect the robustness
and generalization abilities of spoken keyword classifiers under OOD
conditions. We benchmark several neural architectures across a variety of
evaluation sets. To quantify the impact of noise on generalization, we make use
of two metrics: Fairness (F), which measures overall accuracy gains compared to
a baseline model, and Robustness (R), which assesses the convergence between ID
and OOD performance. Our results suggest that noise-aware training improves
robustness in some configurations. These findings shed new light on the
benefits and limitations of noise-based augmentation for generalization in
speech models.

</details>


### [252] [Observational Multiplicity](https://arxiv.org/abs/2507.23136)
*Erin George,Deanna Needell,Berk Ustun*

Main category: cs.LG

TL;DR: 多个模型在预测任务中表现相似但可能做出相互矛盾的预测，影响了模型的可解释性和安全性。本研究提出了一种评估概率分类任务中任意性的方法，通过引入“后悔”度量来评估模型预测的任意性，并提出了一种估算后悔度量的方法。研究发现，某些群体的后悔度量较高，并展示了如何通过模型弃权和数据收集来提高模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 在预测任务中，多个模型可能表现相似但做出相互矛盾的预测，这会影响模型的可解释性和安全性。本研究旨在解决概率分类任务中的任意性问题。

Method: 通过引入“后悔”度量来评估模型预测的任意性，并提出了一种估算后悔度量的方法。

Result: 研究发现，某些群体的后悔度量较高，并展示了如何通过模型弃权和数据收集来提高模型的安全性。

Conclusion: 本研究提出了一种评估概率分类任务中任意性的方法，并将其应用于识别高风险人群。

Abstract: Many prediction tasks can admit multiple models that can perform almost
equally well. This phenomenon can can undermine interpretability and safety
when competing models assign conflicting predictions to individuals. In this
work, we study how arbitrariness can arise in probabilistic classification
tasks as a result of an effect that we call \emph{observational multiplicity}.
We discuss how this effect arises in a broad class of practical applications
where we learn a classifier to predict probabilities $p_i \in [0,1]$ but are
given a dataset of observations $y_i \in \{0,1\}$. We propose to evaluate the
arbitrariness of individual probability predictions through the lens of
\emph{regret}. We introduce a measure of regret for probabilistic
classification tasks, which measures how the predictions of a model could
change as a result of different training labels change. We present a
general-purpose method to estimate the regret in a probabilistic classification
task. We use our measure to show that regret is higher for certain groups in
the dataset and discuss potential applications of regret. We demonstrate how
estimating regret promote safety in real-world applications by abstention and
data collection.

</details>


### [253] [AI paradigm for solving differential equations: first-principles data generation and scale-dilation operator AI solver](https://arxiv.org/abs/2507.23141)
*Xiangshu Gong,Zhiqiang Xie,Xiaowei Jin,Chen Wang,Yanling Qu,Wangmeng Zuo,Hui Li*

Main category: cs.LG

TL;DR: 提出了一种创新的AI方法，通过生成数据和使用SDO Transformer求解器来解决微分方程，解决了数据稀疏和高频分量近似的问题，并在各种测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有AI求解器在处理高频分量和数据稀疏性方面存在的不足，旨在提高AI在求解微分方程（DEs）中的可用性和准确性。

Method: 提出了一种新颖的AI范式，包括DE驱动的 الفيزياء信息数据生成方法和尺度扩张算子（SDO）AI求解器。通过生成DE解来推导源和初始/边界条件，从而以极低的计算成本生成大量数据。利用傅里叶变换和SDO来解决高频分量问题，并设计了基于Transformer的AI求解器。

Result: 该AI范式在各种DEs上进行了广泛测试，证明了其在准确性方面优于最先进的方法，并解决了高频分量近似问题。

Conclusion: 该AI范式在各种自然和工程领域中实现了DE求解器的广泛应用，并在精度上超越了最先进的方法。

Abstract: Many problems are governed by differential equations (DEs). Artificial
intelligence (AI) is a new path for solving DEs. However, data is very scarce
and existing AI solvers struggle with approximation of high frequency
components (AHFC). We propose an AI paradigm for solving diverse DEs, including
DE-ruled first-principles data generation methodology and scale-dilation
operator (SDO) AI solver. Using either prior knowledge or random fields, we
generate solutions and then substitute them into the DEs to derive the sources
and initial/boundary conditions through balancing DEs, thus producing
arbitrarily vast amount of, first-principles-consistent training datasets at
extremely low computational cost. We introduce a reversible SDO that leverages
the Fourier transform of the multiscale solutions to fix AHFC, and design a
spatiotemporally coupled, attention-based Transformer AI solver of DEs with
SDO. An upper bound on the Hessian condition number of the loss function is
proven to be proportional to the squared 2-norm of the solution gradient,
revealing that SDO yields a smoother loss landscape, consequently fixing AHFC
with efficient training. Extensive tests on diverse DEs demonstrate that our AI
paradigm achieves consistently superior accuracy over state-of-the-art methods.
This work makes AI solver of DEs to be truly usable in broad nature and
engineering fields.

</details>


### [254] [FuseTen: A Generative Model for Daily 10 m Land Surface Temperature Estimation from Spatio-Temporal Satellite Observations](https://arxiv.org/abs/2507.23154)
*Sofiane Bouaziz,Adel Hafiane,Raphael Canals,Rachid Nedjai*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Urban heatwaves, droughts, and land degradation are pressing and growing
challenges in the context of climate change. A valuable approach to studying
them requires accurate spatio-temporal information on land surface conditions.
One of the most important variables for assessing and understanding these
phenomena is Land Surface Temperature (LST), which is derived from satellites
and provides essential information about the thermal state of the Earth's
surface. However, satellite platforms inherently face a trade-off between
spatial and temporal resolutions. To bridge this gap, we propose FuseTen, a
novel generative framework that produces daily LST observations at a fine 10 m
spatial resolution by fusing spatio-temporal observations derived from
Sentinel-2, Landsat 8, and Terra MODIS. FuseTen employs a generative
architecture trained using an averaging-based supervision strategy grounded in
physical principles. It incorporates attention and normalization modules within
the fusion process and uses a PatchGAN discriminator to enforce realism.
Experiments across multiple dates show that FuseTen outperforms linear
baselines, with an average 32.06% improvement in quantitative metrics and
31.42% in visual fidelity. To the best of our knowledge, this is the first
non-linear method to generate daily LST estimates at such fine spatial
resolution.

</details>


### [255] [BAR Conjecture: the Feasibility of Inference Budget-Constrained LLM Services with Authenticity and Reasoning](https://arxiv.org/abs/2507.23170)
*Jinan Zhou,Rajat Ghosh,Vaishnavi Bhargava,Debojyoti Dutta,Aryan Singhal*

Main category: cs.LG

TL;DR: Designing LLM services involves a trade-off between inference speed, factual accuracy, and reasoning ability; a new theorem, The BAR Theorem, is proposed to help manage this.


<details>
  <summary>Details</summary>
Motivation: Practitioners designing LLM services prioritize inference-time budget, factual authenticity, and reasoning capacity. However, existing models cannot optimize all three simultaneously, creating a need for a better understanding of these trade-offs.

Method: The paper formally proves the trade-off between inference-time budget, factual authenticity, and reasoning capacity in LLMs and proposes a framework called The BAR Theorem for LLM-application design.

Result: The analysis and formal proof reveal an unavoidable trade-off among inference-time budget, factual authenticity, and reasoning capacity in LLMs. The BAR Theorem framework is introduced to aid in LLM-application design.

Conclusion: The study concludes that no single LLM can simultaneously optimize for inference-time budget, factual authenticity, and reasoning capacity, demonstrating an inherent trade-off between these properties.

Abstract: When designing LLM services, practitioners care about three key properties:
inference-time budget, factual authenticity, and reasoning capacity. However,
our analysis shows that no model can simultaneously optimize for all three. We
formally prove this trade-off and propose a principled framework named The BAR
Theorem for LLM-application design.

</details>


### [256] [NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions](https://arxiv.org/abs/2507.23186)
*Peter Sharpe*

Main category: cs.LG

TL;DR: 提出了一种名为NaN-propagation的新方法，利用NaN值来检测黑盒函数中的稀疏性，解决了传统方法的误报问题，并实现了显著的加速和更广泛的依赖关系检测。


<details>
  <summary>Details</summary>
Motivation: 现有的基于有限差分的黑盒函数稀疏性检测方法存在误报问题，可能导致梯度计算错误，引发难以诊断的错误。因此，需要一种更可靠的方法来检测稀疏性。

Method: 该方法通过系统性地将输入值污染为NaN，并观察哪些输出值也变为NaN，从而追踪浮点数数值计算中的输入输出依赖关系，重建了能够消除误报的保守稀疏性模式。它还提出了一些高级策略，如NaN payload编码，以及用于缓解工程应用中常见的代码分支执行挑战的实用算法。

Result: 该方法在航空航天机翼重量模型上进行了演示，实现了1.52倍的加速，并检测到许多传统方法遗漏的依赖关系。此外，该技术利用IEEE 754标准，无需修改现有黑盒代码即可跨编程语言和数学库工作，并且通过高级策略实现了比现有方法更快的稀疏性检测。

Conclusion: 该方法通过利用IEEE 754标准的NaN（Not-a-Number）值的污染特性，成功地消除了现有有限差分方法在稀疏性检测中存在的误报问题，并且在实际应用中实现了1.52倍的加速，同时检测到许多传统方法遗漏的依赖关系。

Abstract: Sparsity detection in black-box functions enables significant computational
speedups in gradient-based optimization through Jacobian compression, but
existing finite-difference methods suffer from false negatives due to
coincidental zero gradients. These false negatives can silently corrupt
gradient calculations, leading to difficult-to-diagnose errors. We introduce
NaN-propagation, which exploits the universal contamination property of IEEE
754 Not-a-Number floating-point values to trace input-output dependencies
through floating-point numerical computations. By systematically contaminating
inputs with NaN and observing which outputs become NaN, the method reconstructs
conservative sparsity patterns that eliminate false negatives. We demonstrate
the approach on an aerospace wing weight model, achieving a 1.52x speedup while
detecting dozens of dependencies missed by conventional methods -- a
significant improvement since gradient computation is the bottleneck in many
optimization workflows. The technique leverages IEEE 754 compliance to work
across programming languages and math libraries without modifying existing
black-box codes. Advanced strategies including NaN payload encoding enable
faster-than-linear time complexity, improving upon existing black-box sparsity
detection methods. Practical algorithms are also proposed to mitigate
challenges from branching code execution common in engineering applications.

</details>


### [257] [Zero-Shot Document Understanding using Pseudo Table of Contents-Guided Retrieval-Augmented Generation](https://arxiv.org/abs/2507.23217)
*Hyeon Seong Jeong,Sangwoo Jo,Byeong Hyun Yoon,Yoonseok Heo,Haedong Jeong,Taehoon Kim*

Main category: cs.LG

TL;DR: 
DocsRay：一个无需训练的多模态文档理解系统，通过伪TOC和分层RAG提高效率和准确性。



<details>
  <summary>Details</summary>
Motivation: 
当前理解复杂多模态文档面临结构不一致和训练数据不足的挑战。DocsRay旨在解决这些问题，提供一个无需训练的解决方案。


Method: 
DocsRay是一个无需训练的文档理解系统，集成了伪目录（TOC）生成和分层检索增强生成（RAG）。其核心技术包括：1. 使用基于提示的LLM交互生成分层伪TOC。2. 利用多模态LLM的零样本能力，将文本、图像、图表等元素统一为文本中心表示。3. 采用高效的两阶段分层检索系统，将检索复杂度从O(N)降低到O(S + k1*Ns)。


Result: 
DocsRay将查询延迟从3.89秒减少到2.12秒，提高了45%的效率。在MMLongBench-Doc基准测试中，DocsRay-Pro达到了64.7%的准确率，显著优于现有技术水平。


Conclusion: 
DocsRay通过结合伪TOC生成和分层检索增强生成，实现了对复杂多模态文档的有效理解，在不进行额外训练的情况下，显著提高了处理效率和准确性。


Abstract: Understanding complex multimodal documents remains challenging due to their
structural inconsistencies and limited training data availability. We introduce
\textit{DocsRay}, a training-free document understanding system that integrates
pseudo Table of Contents (TOC) generation with hierarchical Retrieval-Augmented
Generation (RAG). Our approach leverages multimodal Large Language Models'
(LLMs) native capabilities to seamlessly process documents containing diverse
elements such as text, images, charts, and tables without requiring specialized
models or additional training. DocsRay's framework synergistically combines
three key techniques: (1) a semantic structuring module using prompt-based LLM
interactions to generate a hierarchical pseudo-TOC, (2) zero-shot multimodal
analysis that converts diverse document elements into unified, text-centric
representations using the inherent capabilities of multimodal LLMs, and (3) an
efficient two-stage hierarchical retrieval system that reduces retrieval
complexity from $O(N)$ to $O(S + k_1 \cdot N_s)$. Evaluated on documents
averaging 49.4 pages and 20,971 textual tokens, DocsRay reduced query latency
from 3.89 to 2.12 seconds, achieving a 45% efficiency improvement. On the
MMLongBench-Doc benchmark, DocsRay-Pro attains an accuracy of 64.7%,
substantially surpassing previous state-of-the-art results.

</details>


### [258] [A Single Direction of Truth: An Observer Model's Linear Residual Probe Exposes and Steers Contextual Hallucinations](https://arxiv.org/abs/2507.23221)
*Charles O'Neill,Slava Chalnev,Chi Chi Zhao,Max Kirkby,Mudith Jayasekara*

Main category: cs.LG

TL;DR: AI幻觉检测新方法：利用线性探针定位并操纵MLP活动，可实现5-27%的性能提升，并提供ContraTales基准。


<details>
  <summary>Details</summary>
Motivation: 幻觉是AI中的一个重大挑战，需要有效的检测和减轻方法。

Method: 通过一个通用的观察模型，利用单次前向传播和一个线性探针来检测幻觉。该探针能够分离出区分幻觉文本和忠实文本的线性方向，并在Gemma-2模型中表现出色。通过梯度激活定位信号，并表明操纵该方向可以影响幻觉率。

Result: 该方法在检测幻觉方面优于基线5-27个百分点，并且在Gemma-2模型中表现出稳健的性能。研究还发现了与特定MLP子电路相关的内部低维幻觉跟踪信号，该信号是可操作的，可用于检测和减轻幻觉。

Conclusion: 这项工作揭示了可以用于检测和减轻AI中幻觉现象的低维可操作信号。

Abstract: Contextual hallucinations -- statements unsupported by given context --
remain a significant challenge in AI. We demonstrate a practical
interpretability insight: a generator-agnostic observer model detects
hallucinations via a single forward pass and a linear probe on its residual
stream. This probe isolates a single, transferable linear direction separating
hallucinated from faithful text, outperforming baselines by 5-27 points and
showing robust mid-layer performance across Gemma-2 models (2B to 27B).
Gradient-times-activation localises this signal to sparse, late-layer MLP
activity. Critically, manipulating this direction causally steers generator
hallucination rates, proving its actionability. Our results offer novel
evidence of internal, low-dimensional hallucination tracking linked to specific
MLP sub-circuits, exploitable for detection and mitigation. We release the
2000-example ContraTales benchmark for realistic assessment of such solutions.

</details>


### [259] [Efficient Machine Unlearning via Influence Approximation](https://arxiv.org/abs/2507.23257)
*Jiawei Liu,Chenwang Wu,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 本文提出IAU算法，通过将机器遗忘视为增量学习问题，解决了现有遗忘方法计算成本高的问题，并在实验中取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于影响力的遗忘方法计算成本高昂，不适用于大规模模型和频繁的数据删除请求。受认知科学启发，本文建立了遗忘与记忆（增量学习）之间的理论联系，将遗忘问题转化为更高效的增量学习问题。

Method: 本文提出了一种名为IAU（Influence Approximation Unlearning）的算法，该算法从增量学习的角度实现了高效的机器遗忘。

Result: IAU算法在移除保证、遗忘效率和模型效用之间取得了优越的平衡，并且优于现有技术。

Conclusion: IAU算法在移除保证、学习效率和模型效用之间取得了优于现有方法的平衡，在不同的数据集和模型架构上均表现出色。

Abstract: Due to growing privacy concerns, machine unlearning, which aims at enabling
machine learning models to ``forget" specific training data, has received
increasing attention. Among existing methods, influence-based unlearning has
emerged as a prominent approach due to its ability to estimate the impact of
individual training samples on model parameters without retraining. However,
this approach suffers from prohibitive computational overhead arising from the
necessity to compute the Hessian matrix and its inverse across all training
samples and parameters, rendering it impractical for large-scale models and
scenarios involving frequent data deletion requests. This highlights the
difficulty of forgetting. Inspired by cognitive science, which suggests that
memorizing is easier than forgetting, this paper establishes a theoretical link
between memorizing (incremental learning) and forgetting (unlearning). This
connection allows machine unlearning to be addressed from the perspective of
incremental learning. Unlike the time-consuming Hessian computations in
unlearning (forgetting), incremental learning (memorizing) typically relies on
more efficient gradient optimization, which supports the aforementioned
cognitive theory. Based on this connection, we introduce the Influence
Approximation Unlearning (IAU) algorithm for efficient machine unlearning from
the incremental perspective. Extensive empirical evaluations demonstrate that
IAU achieves a superior balance among removal guarantee, unlearning efficiency,
and comparable model utility, while outperforming state-of-the-art methods
across diverse datasets and model architectures. Our code is available at
https://github.com/Lolo1222/IAU.

</details>


### [260] [Evaluating the Dynamics of Membership Privacy in Deep Learning](https://arxiv.org/abs/2507.23291)
*Yuetian Chen,Zhiqi Wang,Nathalie Baracaldo,Swanand Ravindra Kadhe,Lei Yu*

Main category: cs.LG

TL;DR: 本篇论文提出了一个动态分析框架，用于在训练过程中量化单个样本的隐私泄露。研究发现，样本的学习难度与其隐私风险相关，并且大部分隐私风险在训练早期就已经确定。


<details>
  <summary>Details</summary>
Motivation: 现有研究对模型在训练过程中何时以及如何编码成员信息知之甚少，而成员推断攻击（MIAs）对深度学习中训练数据的隐私构成了严重威胁。

Method: 提出一个动态分析框架，在FPR-TPR平面上跟踪每个样本在训练过程中的脆弱性，以量化隐私泄露动态，并研究数据集复杂性、模型架构和优化器选择等因素的影响。

Result: 发现了样本内在学习难度与隐私风险之间的稳健相关性，即模型最终训练好的隐私风险大的样本，其风险在训练早期就已经基本确定。

Conclusion: 本研究通过动态分析框架，揭示了模型训练过程中样本隐私泄露的动态性，发现样本的学习难度与其隐私风险高度相关，且最终模型的隐私风险很大程度上由训练早期决定。这些发现为主动、隐私感知的模型训练策略奠定了基础。

Abstract: Membership inference attacks (MIAs) pose a critical threat to the privacy of
training data in deep learning. Despite significant progress in attack
methodologies, our understanding of when and how models encode membership
information during training remains limited. This paper presents a dynamic
analytical framework for dissecting and quantifying privacy leakage dynamics at
the individual sample level. By tracking per-sample vulnerabilities on an
FPR-TPR plane throughout training, our framework systematically measures how
factors such as dataset complexity, model architecture, and optimizer choice
influence the rate and severity at which samples become vulnerable. Crucially,
we discover a robust correlation between a sample's intrinsic learning
difficulty, and find that the privacy risk of samples highly vulnerable in the
final trained model is largely determined early during training. Our results
thus provide a deeper understanding of how privacy risks dynamically emerge
during training, laying the groundwork for proactive, privacy-aware model
training strategies.

</details>


### [261] [SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy](https://arxiv.org/abs/2507.23292)
*RJ Skerry-Ryan,Julian Salazar,Soroosh Mariooryad,David Kao,Daisy Stanton,Eric Battenberg,Matt Shannon,Ron J. Weiss,Robin Scheibler,Jonas Rothfuss,Tom Bagby*

Main category: cs.LG

TL;DR: 介绍了一个用于序列建模的神经网层面API和库，可以逐层和逐步执行模型，并提供可流式传输和减少错误的保证。


<details>
  <summary>Details</summary>
Motivation: 为序列建模介绍神经网层面API和库，以便于创建可以逐层和逐步执行的序列模型。

Method: 通过让层显式表示其跨时间的状态（例如，Transformer KV缓存、卷积缓冲区、RNN隐藏状态）以及演变该状态的step方法，可以实现序列模型。

Result: 该API和库可以立即实现复杂模型的可流式传输，减少了流式和并行序列处理中出现的常见错误，并且可以在任何深度学习库中实现。

Conclusion: 该API和库允许轻松创建序列模型，并且可以逐层（例如，强制教师训练）和逐步（例如，自回归采样）执行。

Abstract: We introduce a neural network layer API and library for sequence modeling,
designed for easy creation of sequence models that can be executed both
layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,
autoregressive sampling). To achieve this, layers define an explicit
representation of their state over time (e.g., a Transformer KV cache, a
convolution buffer, an RNN hidden state), and a step method that evolves that
state, tested to give identical results to a stateless layer-wise invocation.
This and other aspects of the SequenceLayers contract enables complex models to
be immediately streamable, mitigates a wide range of common bugs arising in
both streaming and parallel sequence processing, and can be implemented in any
deep learning library. A composable and declarative API, along with a
comprehensive suite of layers and combinators, streamlines the construction of
production-scale models from simple streamable components while preserving
strong correctness guarantees. Our current implementations of SequenceLayers
(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.

</details>


### [262] [Policy Learning from Large Vision-Language Model Feedback without Reward Modeling](https://arxiv.org/abs/2507.23391)
*Tung M. Luu,Donghoon Lee,Younghwan Lee,Chang D. Yoo*

Main category: cs.LG

TL;DR: PLARE是一种新的离线强化学习方法，利用视觉-语言模型（VLMs）进行偏好标签，无需手动设计奖励函数，即可在机器人任务中取得优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的离线强化学习（RL）算法通常需要奖励标记数据，而奖励函数设计本身成本高昂、劳动密集且需要丰富的领域专业知识，这在实际应用中是一个额外的瓶颈。

Method: PLARE通过查询VLM对视觉轨迹片段对进行偏好标签，并使用监督对比偏好学习目标直接从这些偏好标签中训练策略，绕过了学习显式奖励模型的需求。

Result: 在Meta-World的机器人操纵任务上，PLARE取得了与最先进的基于VLM的奖励生成方法相当或更优的性能，并在物理机器人上的实际操纵任务中验证了其有效性。

Conclusion: PLARE通过利用大型视觉-语言模型（VLMs）提供指导信号，实现了在机器人操纵任务上的高性能，甚至优于现有的基于VLM的奖励生成方法，并在物理机器人上得到了验证。

Abstract: Offline reinforcement learning (RL) provides a powerful framework for
training robotic agents using pre-collected, suboptimal datasets, eliminating
the need for costly, time-consuming, and potentially hazardous online
interactions. This is particularly useful in safety-critical real-world
applications, where online data collection is expensive and impractical.
However, existing offline RL algorithms typically require reward labeled data,
which introduces an additional bottleneck: reward function design is itself
costly, labor-intensive, and requires significant domain expertise. In this
paper, we introduce PLARE, a novel approach that leverages large
vision-language models (VLMs) to provide guidance signals for agent training.
Instead of relying on manually designed reward functions, PLARE queries a VLM
for preference labels on pairs of visual trajectory segments based on a
language task description. The policy is then trained directly from these
preference labels using a supervised contrastive preference learning objective,
bypassing the need to learn explicit reward models. Through extensive
experiments on robotic manipulation tasks from the MetaWorld, PLARE achieves
performance on par with or surpassing existing state-of-the-art VLM-based
reward generation methods. Furthermore, we demonstrate the effectiveness of
PLARE in real-world manipulation tasks with a physical robot, further
validating its practical applicability.

</details>


### [263] [An Interpretable Data-Driven Unsupervised Approach for the Prevention of Forgotten Items](https://arxiv.org/abs/2507.23303)
*Luca Corbucci,Javier Alejandro Borges Legrottaglie,Francesco Spinnato,Anna Monreale,Riccardo Guidotti*

Main category: cs.LG

TL;DR: 解决NBP中遗漏物品识别和解释的问题，提出可解释算法，并在真实数据集上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有的NBP方法通常只关注预测未来购买，而没有明确解决检测无意中遗漏的物品的问题。这种差距部分是由于缺乏能够可靠估计遗漏物品的真实世界数据集。此外，大多数现有的NBP方法依赖于不透明的黑盒模型，这限制了向最终用户解释推荐的能力。

Method: 提出两种新颖的、本质上可解释的算法来识别遗漏的物品，并提供直观的、人类可理解的解释。

Result: 实验表明，所提出的算法在真实零售数据集上的性能优于最先进的NBP基线。

Conclusion: 所提出的算法在真实零售数据集上的实验表明，在多个评估指标上，我们的算法比最先进的NBP基线性能高出10-15%

Abstract: Accurately identifying items forgotten during a supermarket visit and
providing clear, interpretable explanations for recommending them remains an
underexplored problem within the Next Basket Prediction (NBP) domain. Existing
NBP approaches typically only focus on forecasting future purchases, without
explicitly addressing the detection of unintentionally omitted items. This gap
is partly due to the scarcity of real-world datasets that allow for the
reliable estimation of forgotten items. Furthermore, most current NBP methods
rely on black-box models, which lack transparency and limit the ability to
justify recommendations to end users. In this paper, we formally introduce the
forgotten item prediction task and propose two novel interpretable-by-design
algorithms. These methods are tailored to identify forgotten items while
offering intuitive, human-understandable explanations. Experiments on a
real-world retail dataset show our algorithms outperform state-of-the-art NBP
baselines by 10-15% across multiple evaluation metrics.

</details>


### [264] [Good Learners Think Their Thinking: Generative PRM Makes Large Reasoning Model More Efficient Math Learner](https://arxiv.org/abs/2507.23317)
*Tao He,Rongchuan Mu,Lizi Liao,Yixin Cao,Ming Liu,Bing Qin*

Main category: cs.LG

TL;DR: 该研究提出了一种新的基于过程奖励的强化学习算法TP-GRPO，用于优化大型推理模型（LRM）以解决数学问题。与传统的基于结果奖励的方法相比，该方法通过在思维层面评估过程的正确性并自适应地调整奖励来显著提高训练效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于结果的奖励在优化大型推理模型（LRM）以解决复杂数学问题时，会提供稀疏的反馈，导致优化效率低下。

Method: 提出了一种新颖的、由内在信号驱动的生成过程评估机制，该机制在思维层面运行，以解决基于RL的训练中的主要瓶颈。所提出的方法使用内在信号来判断逐步的正确性，并将连续的正确/不正确的步骤聚合成连贯的“思维”单元。此外，还引入了一种能力自适应奖励机制，该机制根据LRM的当前熟练程度动态地平衡探索和利用。这些创新被整合到一个新的非策略RL算法TP-GRPO中，该算法扩展了具有基于过程的奖励的分组近端优化。

Result: 实验表明，所提出的方法在1.5B和7B参数的LRM上，与仅基于结果奖励的方法相比，使用更少的训练样本就能实现更高的解决问题准确性。

Conclusion: 通过在数学推理任务中优化大型推理模型（LRM），可以证明结构化的过程奖励能够显著加速LRM的优化。

Abstract: Large reasoning models (LRMs) have recently shown promise in solving complex
math problems when optimized with Reinforcement Learning (RL). But conventional
approaches rely on outcome-only rewards that provide sparse feedback, resulting
in inefficient optimization process. In this work, we investigate the function
of process reward models (PRMs) to accelerate the RL training for LRMs. We
propose a novel intrinsic signal-driven generative process evaluation mechanism
operating at the thought level to address major bottlenecks in RL-based
training. Specifically, instead of requiring PRMs to know how to solve
problems, our method uses intrinsic signals in solutions to judge stepwise
correctness and aggregate contiguous correct/incorrect steps into coherent
'thought' units. This structured, thought-level rewards enable more reliable
credit assignment by reducing ambiguity in step segmentation and alleviating
reward hacking. We further introduce a capability-adaptive reward mechanism
that dynamically balances exploration and exploitation based on the LRM's
current proficiency, guiding learning without stifling creative
trial-and-error. These innovations are integrated into a new off-policy RL
algorithm, TP-GRPO, which extends grouped proximal optimization with
process-based rewards and improves training efficiency. Experiments on 1.5B and
7B parameter LRMs demonstrate that our method achieves higher problem-solving
accuracy with significantly fewer training samples than outcome-only reward
baselines. The results validate that well-structured process rewards can
substantially accelerate LRM optimization in math reasoning tasks. Code is
available at https://github.com/cs-holder/tp_grpo.

</details>


### [265] [Scalable and Precise Patch Robustness Certification for Deep Learning Models with Top-k Predictions](https://arxiv.org/abs/2507.23335)
*Qilin Zhou,Haipeng Wang,Zhengyuan Wei,W. K. Chan*

Main category: cs.LG

TL;DR: CostCert 是一种新的、可扩展且精确的基于投票的认证恢复防御者，用于防御对抗性补丁攻击，它比 PatchGuard 更有效。


<details>
  <summary>Details</summary>
Motivation: 现有的认证恢复技术在应用于 top-k 预测时，通常会对标签之间的选票进行成对比较，这由于攻击者控制的选票数量的膨胀（即攻击预算）而无法精确地认证 top-k 预测标签中的唯一真实标签；然而，枚举所有选票分配组合会因组合爆炸而受到影响。

Method: CostCert 通过一种新颖的设计来验证样本的真实标签是否在顶 k 个预测中，该设计检查攻击预算是否不足以覆盖排除真实标签所需的最小额外选票总数。

Result: 实验表明，CostCert 的性能明显优于最先进的防御者 PatchGuard，例如在补丁大小为 96 时，其认证准确率最高可达 57.3%，而 PatchGuard 的准确率已降至零。

Conclusion: CostCert 在顶 k 预测中验证样本的真实标签，无需进行成对比较或组合爆炸，其方法是检查攻击预算是否不足以覆盖排除真实标签所需的最小额外选票总数。

Abstract: Patch robustness certification is an emerging verification approach for
defending against adversarial patch attacks with provable guarantees for deep
learning systems. Certified recovery techniques guarantee the prediction of the
sole true label of a certified sample. However, existing techniques, if
applicable to top-k predictions, commonly conduct pairwise comparisons on those
votes between labels, failing to certify the sole true label within the top k
prediction labels precisely due to the inflation on the number of votes
controlled by the attacker (i.e., attack budget); yet enumerating all
combinations of vote allocation suffers from the combinatorial explosion
problem. We propose CostCert, a novel, scalable, and precise voting-based
certified recovery defender. CostCert verifies the true label of a sample
within the top k predictions without pairwise comparisons and combinatorial
explosion through a novel design: whether the attack budget on the sample is
infeasible to cover the smallest total additional votes on top of the votes
uncontrollable by the attacker to exclude the true labels from the top k
prediction labels. Experiments show that CostCert significantly outperforms the
current state-of-the-art defender PatchGuard, such as retaining up to 57.3% in
certified accuracy when the patch size is 96, whereas PatchGuard has already
dropped to zero.

</details>


### [266] [Causal Explanation of Concept Drift -- A Truly Actionable Approach](https://arxiv.org/abs/2507.23389)
*David Komnick,Kathrin Lammers,Barbara Hammer,Valerie Vaquet,Fabian Hinder*

Main category: cs.LG

TL;DR: 通过因果解释提高概念漂移的可操作性，以实现有针对性的干预。


<details>
  <summary>Details</summary>
Motivation: 解释关键的系统变化（概念漂移）是进行针对性干预以避免模型故障和现实世界中的故障和错误的第一步。

Method: 将模型驱动的漂移解释方法扩展到因果解释。

Result: 该方法在多个用例中得到评估，证明了其框架的实用性，能够隔离因果相关的特征并实现有针对性的干预。

Conclusion: 本研究将基于模型的漂移解释扩展到因果解释，提高了可操作性，并通过隔离由概念漂移引起的因果相关特征来实现有针对性的干预。

Abstract: In a world that constantly changes, it is crucial to understand how those
changes impact different systems, such as industrial manufacturing or critical
infrastructure. Explaining critical changes, referred to as concept drift in
the field of machine learning, is the first step towards enabling targeted
interventions to avoid or correct model failures, as well as malfunctions and
errors in the physical world. Therefore, in this work, we extend model-based
drift explanations towards causal explanations, which increases the
actionability of the provided explanations. We evaluate our explanation
strategy on a number of use cases, demonstrating the practical usefulness of
our framework, which isolates the causally relevant features impacted by
concept drift and, thus, allows for targeted intervention.

</details>


### [267] [A Machine Learning Approach for Honey Adulteration Detection using Mineral Element Profiles](https://arxiv.org/abs/2507.23412)
*Mokhtar A. Al-Awadhi,Ratnadeep R. Deshmukh*

Main category: cs.LG

TL;DR: 本研究利用机器学习和蜂蜜矿物质元素谱图，开发了一种高精度的蜂蜜掺假检测系统，其中随机森林模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在利用蜂蜜矿物质元素谱图开发一个基于机器学习的系统，用于检测蜂蜜掺假。

Method: 该研究提出一个基于机器学习的系统，包含数据预处理（处理缺失值和归一化）和分类两个阶段。在分类阶段，使用了三种监督学习模型：逻辑回归、决策树和随机森林，以区分真实蜂蜜和掺假蜂蜜。

Result: 实验结果表明，蜂蜜中的矿物质元素含量为检测蜂蜜掺假提供了可靠的辨别信息。随机森林分类器在该数据集上表现最优，交叉验证准确率达到98.37%。

Conclusion: 研究结果表明，蜂蜜中的矿物质元素含量为检测蜂蜜掺假提供了可靠的辨别信息，并且随机森林分类器在该数据集上表现优于其他分类器，实现了98.37%的最高交叉验证准确率。

Abstract: This paper aims to develop a Machine Learning (ML)-based system for detecting
honey adulteration utilizing honey mineral element profiles. The proposed
system comprises two phases: preprocessing and classification. The
preprocessing phase involves the treatment of missing-value attributes and
normalization. In the classifica-tion phase, we use three supervised ML models:
logistic regression, decision tree, and random forest, to dis-criminate between
authentic and adulterated honey. To evaluate the performance of the ML models,
we use a public dataset comprising measurements of mineral element content of
authentic honey, sugar syrups, and adul-terated honey. Experimental findings
show that mineral element content in honey provides robust discriminative
information for detecting honey adulteration. Results also demonstrate that the
random forest-based classifier outperforms other classifiers on this dataset,
achieving the highest cross-validation accuracy of 98.37%.

</details>


### [268] [Detection of Adulteration in Coconut Milk using Infrared Spectroscopy and Machine Learning](https://arxiv.org/abs/2507.23418)
*Mokhtar A. Al-Awadhi,Ratnadeep R. Deshmukh*

Main category: cs.LG

TL;DR: 本研究利用红外光谱和机器学习（LDA和KNN）提出了一种检测椰子奶掺假的方法，准确率达93.33%。


<details>
  <summary>Details</summary>
Motivation: 在椰子奶中检测掺假现象。

Method: 本研究提出了一种利用红外光谱检测椰子奶中掺假现象的系统。该系统基于机器学习，包括预处理、特征提取和分类三个阶段。预处理阶段用于移除椰子奶光谱信号中不相关的数据；特征提取阶段采用线性判别分析（LDA）算法提取最具判别性的特征；分类阶段则使用K-近邻（KNN）模型将椰子奶样品分为纯正或掺假。

Result: 所提出的方法能够成功检测掺假现象，交叉验证准确率为93.33%。

Conclusion: 该方法能够成功检测掺假现象，交叉验证准确率为93.33%。

Abstract: In this paper, we propose a system for detecting adulteration in coconut
milk, utilizing infrared spectroscopy. The machine learning-based proposed
system comprises three phases: preprocessing, feature extraction, and
classification. The first phase involves removing irrelevant data from coconut
milk spectral signals. In the second phase, we employ the Linear Discriminant
Analysis (LDA) algorithm for extracting the most discriminating features. In
the third phase, we use the K-Nearest Neighbor (KNN) model to classify coconut
milk samples into authentic or adulterated. We evaluate the performance of the
proposed system using a public dataset comprising Fourier Transform Infrared
(FTIR) spectral information of pure and contaminated coconut milk samples.
Findings show that the proposed method successfully detects adulteration with a
cross-validation accuracy of 93.33%.

</details>


### [269] [Merging Memory and Space: A Spatiotemporal State Space Neural Operator](https://arxiv.org/abs/2507.23428)
*Nodens F. Koren,Samuel Lanthaler*

Main category: cs.LG

TL;DR: ST-SSM 是一种新的时空神经算子，通过分解时空维度，在 PDE 建模中实现了高参数效率和优越性能，并在理论和实验上得到了验证。


<details>
  <summary>Details</summary>
Motivation: 旨在为时间依赖性偏微分方程 (PDE) 的学习算子提供一种紧凑且参数高效的架构，特别是要解决在模拟长期时空动态方面的挑战，并探索维度分解和状态空间模型在神经算子中的应用潜力。

Method: 提出了一种名为 Spatiotemporal State Space Neural Operator (ST-SSM) 的紧凑架构，该架构通过对时空维度进行新颖的分解，并利用结构化状态空间模型分别处理时间演化和空间交互，实现了参数效率和对长期时空动态的灵活建模。此外，还建立了 SSM 与神经算子之间的理论联系，并证明了该架构的统一通用性定理。

Result: ST-SSM 在 1D Burgers 方程、1D Kuramoto-Sivashinsky 方程和 2D Navier-Stokes 方程等多个 PDE 基准测试中，相比于zigzag扫描和并行独立处理等方法表现出更优越的性能。同时，ST-SSM 的表现与现有基线模型相当，但使用的参数却显著减少。此外，该模型在部分可观测性下表现出更优的性能，验证了时间记忆的优势。

Conclusion: ST-SSM 在处理时间依赖性偏微分方程 (PDE) 的学习算子方面，通过新颖的维度分解方法，在参数效率、长期时空动态建模以及在多个 PDE 基准测试中的性能方面均表现出色，证明了其相比于其他方法的优越性。

Abstract: We propose the Spatiotemporal State Space Neural Operator (ST-SSM), a compact
architecture for learning solution operators of time-dependent partial
differential equations (PDEs). ST-SSM introduces a novel factorization of the
spatial and temporal dimensions, using structured state-space models to
independently model temporal evolution and spatial interactions. This design
enables parameter efficiency and flexible modeling of long-range spatiotemporal
dynamics. A theoretical connection is established between SSMs and neural
operators, and a unified universality theorem is proved for the resulting class
of architectures. Empirically, we demonstrate that our factorized formulation
outperforms alternative schemes such as zigzag scanning and parallel
independent processing on several PDE benchmarks, including 1D Burgers'
equation, 1D Kuramoto-Sivashinsky equation, and 2D Navier-Stokes equations
under varying physical conditions. Our model performs competitively with
existing baselines while using significantly fewer parameters. In addition, our
results reinforce previous findings on the benefits of temporal memory by
showing improved performance under partial observability. Our results highlight
the advantages of dimensionally factorized operator learning for efficient and
generalizable PDE modeling, and put this approach on a firm theoretical
footing.

</details>


### [270] [Coflex: Enhancing HW-NAS with Sparse Gaussian Processes for Efficient and Scalable DNN Accelerator Design](https://arxiv.org/abs/2507.23437)
*Yinhui Ma,Tomomasa Yamasaki,Zhehui Wang,Tao Luo,Bo Wang*

Main category: cs.LG

TL;DR: Coflex 是一种新的 HW-NAS 框架，它使用 SGP 和多目标贝叶斯优化来提高搜索效率和预测精度，从而在神经网络精度和硬件能效方面取得更好的结果。


<details>
  <summary>Details</summary>
Motivation: HW-NAS 虽然是优化神经网络性能和硬件能效的有效方法，但其巨大的搜索空间和高计算成本限制了实际应用。Coflex 旨在解决这些限制，以实现可扩展且高效的 HW-NAS。

Method: 提出了一种名为 Coflex 的硬件感知神经架构搜索（HW-NAS）框架，该框架集成了稀疏高斯过程（SGP）和多目标贝叶斯优化。SGP 利用稀疏诱导点将 GP 核复杂度从关于训练样本数量的立方级降低到近线性，从而在不影响优化性能的情况下，能够对大规模搜索空间进行可扩展近似，显著降低了计算开销并保持了高预测精度。

Result: Coflex 在各种基准测试和特定加速器架构上进行了评估，结果显示其在网络精度和能耗延迟积（EDP）方面优于最先进的方法，计算速度提高了 1.9 倍至 9.5 倍。

Conclusion: Coflex 通过集成稀疏高斯过程（SGP）和多目标贝叶斯优化，在网络精度和能耗延迟积（EDP）方面优于现有方法，并将计算速度提高了 1.9 倍至 9.5 倍。

Abstract: Hardware-Aware Neural Architecture Search (HW-NAS) is an efficient approach
to automatically co-optimizing neural network performance and hardware energy
efficiency, making it particularly useful for the development of Deep Neural
Network accelerators on the edge. However, the extensive search space and high
computational cost pose significant challenges to its practical adoption. To
address these limitations, we propose Coflex, a novel HW-NAS framework that
integrates the Sparse Gaussian Process (SGP) with multi-objective Bayesian
optimization. By leveraging sparse inducing points, Coflex reduces the GP
kernel complexity from cubic to near-linear with respect to the number of
training samples, without compromising optimization performance. This enables
scalable approximation of large-scale search space, substantially decreasing
computational overhead while preserving high predictive accuracy. We evaluate
the efficacy of Coflex across various benchmarks, focusing on
accelerator-specific architecture. Our experi- mental results show that Coflex
outperforms state-of-the-art methods in terms of network accuracy and
Energy-Delay-Product, while achieving a computational speed-up ranging from
1.9x to 9.5x.

</details>


### [271] [Manifold-regularised Signature Kernel Large-Margin $\ell_p$-SVDD for Multidimensional Time Series Anomaly Detection](https://arxiv.org/abs/2507.23449)
*Shervin Rahimzadeh Arashloo*

Main category: cs.LG

TL;DR: A new anomaly detection method for time series uses manifold regularization and a signature kernel to capture complex data structures, showing improved performance.


<details>
  <summary>Details</summary>
Motivation: To exploit the geometry of data distribution for improved anomaly detection in time series by generalizing the large-margin $\ell_p$-SVDD approach using manifold regularization and a signature kernel.

Method: A manifold-regularised variant of the $\ell_p$-SVDD method is proposed, utilizing a signature kernel and an effective optimization technique derived from the Representer theorem. Generalization performance is analyzed using Rademacher complexities.

Result: The experimental assessment across various datasets demonstrates the proposed method's superior performance compared to other anomaly detection techniques.

Conclusion: The proposed manifold-regularised $\ell_p$-SVDD with signature kernel effectively captures time series complexities for anomaly detection, outperforming existing methods across various datasets.

Abstract: We generalise the recently introduced large-margin $\ell_p$-SVDD approach to
exploit the geometry of data distribution via manifold regularising and a
signature kernel representation for time series anomaly detection.
Specifically, we formulate a manifold-regularised variant of the $\ell_p$-SVDD
method to encourage label smoothness on the underlying manifold to capture
structural information for improved detection performance. Drawing on an
existing Representer theorem, we then provide an effective optimisation
technique for the proposed method and show that it can benefit from the
signature kernel to capture time series complexities for anomaly detection.
  We theoretically study the proposed approach using Rademacher complexities to
analyse its generalisation performance and also provide an experimental
assessment of the proposed method across various data sets to compare its
performance against other methods.

</details>


### [272] [Explainable artificial intelligence model predicting the risk of all-cause mortality in patients with type 2 diabetes mellitus](https://arxiv.org/abs/2507.23491)
*Olga Vershinina,Jacopo Sabbatinelli,Anna Rita Bonfigli,Dalila Colombaretti,Angelica Giuliani,Mikhail Krivonosov,Arseniy Trukhanov,Claudio Franceschi,Mikhail Ivanchenko,Fabiola Olivieri*

Main category: cs.LG

TL;DR: 本研究开发了一个基于极限生存树（EST）的机器学习模型，并使用 SHAP 方法进行解释，该模型能准确预测 2 型糖尿病患者的全因死亡风险。


<details>
  <summary>Details</summary>
Motivation: 2 型糖尿病（T2DM）是一种患病率很高的非传染性慢性病，会显著降低预期寿命。准确估计 T2DM 患者的全因死亡风险对于个性化和优化治疗策略至关重要。

Method: 本研究分析了一个包含 554 名确诊的 2 型糖尿病患者（年龄 40-87 岁）的队列，随访时间最长为 16.8 年。在随访期间，有 202 名患者（36%）死亡。研究确定了关键的与生存相关的特征，并训练和验证了多个机器学习（ML）模型来预测全因死亡风险。为了提高模型的可解释性，将 Shapley 语法加法解释（SHAP）应用于性能最佳的模型。

Result: 结合十个关键特征的极限生存树（EST）模型表现出最佳的预测性能。该模型在 5 年、10 年、15 年和 16.8 年的全因死亡风险预测中，C 统计量分别为 0.776，受试者工作特征曲线下面积（AUC）值分别为 0.86、0.80、0.841 和 0.826。SHAP 方法用于解释模型的个体决策过程。

Conclusion: 该模型在死亡风险评估方面表现出强大的预测性能。其临床可解释的输出可用于实际床边应用，从而改善高危患者的识别并支持及时的治疗优化。

Abstract: Objective. Type 2 diabetes mellitus (T2DM) is a highly prevalent
non-communicable chronic disease that substantially reduces life expectancy.
Accurate estimation of all-cause mortality risk in T2DM patients is crucial for
personalizing and optimizing treatment strategies. Research Design and Methods.
This study analyzed a cohort of 554 patients (aged 40-87 years) with diagnosed
T2DM over a maximum follow-up period of 16.8 years, during which 202 patients
(36%) died. Key survival-associated features were identified, and multiple
machine learning (ML) models were trained and validated to predict all-cause
mortality risk. To improve model interpretability, Shapley additive
explanations (SHAP) was applied to the best-performing model. Results. The
extra survival trees (EST) model, incorporating ten key features, demonstrated
the best predictive performance. The model achieved a C-statistic of 0.776,
with the area under the receiver operating characteristic curve (AUC) values of
0.86, 0.80, 0.841, and 0.826 for 5-, 10-, 15-, and 16.8-year all-cause
mortality predictions, respectively. The SHAP approach was employed to
interpret the model's individual decision-making processes. Conclusions. The
developed model exhibited strong predictive performance for mortality risk
assessment. Its clinically interpretable outputs enable potential bedside
application, improving the identification of high-risk patients and supporting
timely treatment optimization.

</details>


### [273] [Incorporating structural uncertainty in causal decision making](https://arxiv.org/abs/2507.23495)
*Maurits Kaptein*

Main category: cs.LG

TL;DR: 在做因果推断决策时，忽视结构不确定性可能会导致问题。本研究表明，当结构不确定性较高、因果效应在不同模型间差异较大时，采用贝叶斯模型平均法可以改善结果。现代因果发现方法可以帮助量化这种不确定性。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决实际决策中因果效应分析时常常忽略的结构不确定性问题，并探讨何时需要采用特定的方法学解决方案。

Method: 本文提出了一个基于贝叶斯模型平均的因果推断方法，用于处理因果结构不确定性。研究重点关注二变量关系，并分析了在何种情况下模型平均法能够带来益处。

Result: 研究表明，当结构不确定性较高、因果效应在不同模型间差异较大以及损失函数对因果效应敏感时，贝叶斯模型平均法能提供更优的因果效应估计。现代因果发现方法在一定程度上能够量化这种不确定性。

Conclusion: 该研究为解决因果推断中的结构不确定性问题提供了一个理论框架，并证明了在特定条件下（结构不确定性中高、因果效应在不同结构间差异大、损失函数对因果效应敏感）采用贝叶斯模型平均法进行因果推断的优越性。

Abstract: Practitioners making decisions based on causal effects typically ignore
structural uncertainty. We analyze when this uncertainty is consequential
enough to warrant methodological solutions (Bayesian model averaging over
competing causal structures). Focusing on bivariate relationships ($X
\rightarrow Y$ vs. $X \leftarrow Y$), we establish that model averaging is
beneficial when: (1) structural uncertainty is moderate to high, (2) causal
effects differ substantially between structures, and (3) loss functions are
sufficiently sensitive to the size of the causal effect. We prove optimality
results of our suggested methodological solution under regularity conditions
and demonstrate through simulations that modern causal discovery methods can
provide, within limits, the necessary quantification. Our framework complements
existing robust causal inference approaches by addressing a distinct source of
uncertainty typically overlooked in practice.

</details>


### [274] [Directional Ensemble Aggregation for Actor-Critics](https://arxiv.org/abs/2507.23501)
*Nicklas Werge,Yi-Shan Wu,Bahareh Tasdighi,Melih Kandemir*

Main category: cs.LG

TL;DR: DEA 是一种新的 Q 值聚合方法，通过学习参数来适应性地平衡保守性和探索性，从而提高强化学习在连续控制任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 为了缓解 Q 值估计中的高估偏差，通常使用保守聚合（如最小值），但这些静态规则会丢弃有价值的信息，并且无法适应任务特定需求或不同的学习范式。因此，需要一种能够自适应地组合 Q 值估计的方法。

Method: DEA 引入了两个完全可学的方向参数：一个用于调节保守性，另一个用于指导策略探索。这两个参数都使用集成不一致加权的 Bellman 误差进行学习，该误差仅根据其 Bellman 误差的方向对每个样本进行加权。

Result: DEA 能够以数据驱动的方式调整保守性和探索性，使聚合适应不确定性水平和训练阶段。

Conclusion: DEA 通过适应性地结合 Q 值估计，在连续控制基准和学习范式中证明了其优于静态集成策略的有效性。

Abstract: Off-policy reinforcement learning in continuous control tasks depends
critically on accurate $Q$-value estimates. Conservative aggregation over
ensembles, such as taking the minimum, is commonly used to mitigate
overestimation bias. However, these static rules are coarse, discard valuable
information from the ensemble, and cannot adapt to task-specific needs or
different learning regimes. We propose Directional Ensemble Aggregation (DEA),
an aggregation method that adaptively combines $Q$-value estimates in
actor-critic frameworks. DEA introduces two fully learnable directional
parameters: one that modulates critic-side conservatism and another that guides
actor-side policy exploration. Both parameters are learned using ensemble
disagreement-weighted Bellman errors, which weight each sample solely by the
direction of its Bellman error. This directional learning mechanism allows DEA
to adjust conservatism and exploration in a data-driven way, adapting
aggregation to both uncertainty levels and the phase of training. We evaluate
DEA across continuous control benchmarks and learning regimes - from
interactive to sample-efficient - and demonstrate its effectiveness over static
ensemble strategies.

</details>


### [275] [A Verifier Hierarchy](https://arxiv.org/abs/2507.23504)
*Maurits Kaptein*

Main category: cs.LG

TL;DR: 通过“验证器权衡定理”，研究了证书长度与验证器运行时间的权衡，并将其应用于复杂性理论和算法分析。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解和量化证书长度与验证器运行时间之间的基本权衡关系。

Method: 提出并证明了一个“验证器权衡定理”，该定理量化了在不增加证书长度的情况下，减少验证器运行时间所需的最小证书长度。

Result: 证明了存在一个基于证书复杂度的自然层级结构，并将此理论应用于分析NP和EXPTIME等复杂性类之间的猜想分离，以及字符串周期性和旋转检测等具体问题。

Conclusion: 该研究提出了一个关于证书长度和验证器运行时间之间权衡的定理，并展示了其在复杂性类分离猜想和自然问题分析中的应用，同时将 P vs NP 问题与子线性证书的存在联系起来。

Abstract: We investigate the trade-off between certificate length and verifier runtime.
We prove a Verifier Trade-off Theorem showing that reducing the inherent
verification time of a language from \(f(n)\) to \(g(n)\), where \(f(n) \ge
g(n)\), requires certificates of length at least \(\Omega(\log(f(n) / g(n)))\).
This theorem induces a natural hierarchy based on certificate complexity. We
demonstrate its applicability to analyzing conjectured separations between
complexity classes (e.g., \(\np\) and \(\exptime\)) and to studying natural
problems such as string periodicity and rotation detection. Additionally, we
provide perspectives on the \(\p\) vs. \(\np\) problem by relating it to the
existence of sub-linear certificates.

</details>


### [276] [Differentially Private Clipped-SGD: High-Probability Convergence with Arbitrary Clipping Level](https://arxiv.org/abs/2507.23512)
*Saleh Vatan Khah,Savelii Chezhegov,Shahrokh Farahmand,Samuel Horváth,Eduard Gorbunov*

Main category: cs.LG

TL;DR: 本研究首次在高概率下对 DP-Clipped-SGD 在固定裁剪级别下的收敛性进行了分析，证明了其在重尾噪声下的收敛速度优于现有方法，并在收敛速度和隐私保证之间实现了更好的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的高概率收敛性分析通常要求裁剪阈值随优化步数增加而增加，这与高斯机制等标准差分隐私（DP）机制不兼容。本研究旨在解决此问题，为 DP-Clipped-SGD 在固定裁剪级别下的收敛性提供分析。

Method: 该研究首次在高概率下对 DP-Clipped-SGD 在固定裁剪级别下进行了收敛性分析，适用于凸和非凸光滑优化，并考虑了重尾噪声（由有界中心 $\alpha$-矩假设表征，$\alpha 
 \in (1,2]$）。

Result: 与现有方法相比，使用固定裁剪级别，该方法收敛到最优解的邻域，且收敛速度更快。该邻域可以与 DP 引入的噪声进行平衡，从而在收敛速度和隐私保证之间实现更精细的权衡。

Conclusion: 这项工作通过提供 DP-Clipped-SGD 的首次高概率收敛性分析，在固定裁剪级别下，适用于凸和非凸光滑优化，并针对重尾噪声（由有界中心 $\alpha$-矩假设表征，$\alpha 
 \in (1,2]$）进行了分析。结果表明，在固定裁剪级别下，该方法收敛到最优解的邻域，且收敛速度比现有方法更快。该邻域可以与 DP 引入的噪声进行平衡，从而在收敛速度和隐私保证之间实现更精细的权衡。

Abstract: Gradient clipping is a fundamental tool in Deep Learning, improving the
high-probability convergence of stochastic first-order methods like SGD,
AdaGrad, and Adam under heavy-tailed noise, which is common in training large
language models. It is also a crucial component of Differential Privacy (DP)
mechanisms. However, existing high-probability convergence analyses typically
require the clipping threshold to increase with the number of optimization
steps, which is incompatible with standard DP mechanisms like the Gaussian
mechanism. In this work, we close this gap by providing the first
high-probability convergence analysis for DP-Clipped-SGD with a fixed clipping
level, applicable to both convex and non-convex smooth optimization under
heavy-tailed noise, characterized by a bounded central $\alpha$-th moment
assumption, $\alpha \in (1,2]$. Our results show that, with a fixed clipping
level, the method converges to a neighborhood of the optimal solution with a
faster rate than the existing ones. The neighborhood can be balanced against
the noise introduced by DP, providing a refined trade-off between convergence
speed and privacy guarantees.

</details>


### [277] [Continual Learning with Synthetic Boundary Experience Blending](https://arxiv.org/abs/2507.23534)
*Chih-Fan Hsu,Ming-Ching Chang,Wei-Chao Chen*

Main category: cs.LG

TL;DR: 提出“经验混合”框架，通过差分隐私生成合成边界数据（SBD）并与经验回放相结合，有效缓解了连续学习中的灾难性遗忘问题，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决连续学习（CL）中灾难性遗忘的问题，并改善传统经验回放方法中由于存储关键样本分布稀疏导致的决策边界过于简化的问题。

Method: 提出了一种名为“经验混合”的新型训练框架，包含两个核心组件：1. 多变量差分隐私（DP）噪声机制，用于生成SBD；2. 端到端训练策略，同时利用存储的关键样本和SBD。

Result: 在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上的实验结果表明，该方法在准确性方面分别提高了10%、6%和13%，优于九种现有的CL基线方法。

Conclusion: 该方法通过引入决策边界附近的合成数据（SBD）作为隐式正则化器，提高了边界稳定性和减轻了遗忘，在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上均优于九种对比方法。

Abstract: Continual learning (CL) aims to address catastrophic forgetting in models
trained sequentially on multiple tasks. While experience replay has shown
promise, its effectiveness is often limited by the sparse distribution of
stored key samples, leading to overly simplified decision boundaries. We
hypothesize that introducing synthetic data near the decision boundary
(Synthetic Boundary Data, or SBD) during training serves as an implicit
regularizer, improving boundary stability and mitigating forgetting. To
validate this hypothesis, we propose a novel training framework, {\bf
Experience Blending}, which integrates knowledge from both stored key samples
and synthetic, boundary-adjacent data. Experience blending consists of two core
components: (1) a multivariate Differential Privacy (DP) noise mechanism that
injects batch-wise noise into low-dimensional feature representations,
generating SBD; and (2) an end-to-end training strategy that jointly leverages
both stored key samples and SBD. Extensive experiments on CIFAR-10, CIFAR-100,
and Tiny ImageNet demonstrate that our method outperforms nine CL baselines,
achieving accuracy improvements of 10%, 6%, and 13%, respectively.

</details>


### [278] [Transparent AI: The Case for Interpretability and Explainability](https://arxiv.org/abs/2507.23535)
*Dhanesh Ramachandram,Himanshu Joshi,Judy Zhu,Dhari Gandhi,Lucas Hartman,Ananya Raval*

Main category: cs.LG

TL;DR: This paper provides practical guidance on integrating AI interpretability into the design process to ensure responsible and trustworthy AI systems, offering strategies for organizations at all maturity levels.


<details>
  <summary>Details</summary>
Motivation: Transparency has become foundational to responsible and trustworthy AI implementation as AI systems increasingly inform high-stakes decisions across sectors.

Method: The paper presents key insights and lessons learned from practical interpretability applications across diverse domains.

Result: The paper offers actionable strategies and implementation guidance tailored to organizations at varying stages of AI maturity.

Conclusion: The paper emphasizes integrating interpretability as a core design principle for responsible AI implementation, offering actionable strategies for organizations at various stages of AI maturity.

Abstract: As artificial intelligence systems increasingly inform high-stakes decisions
across sectors, transparency has become foundational to responsible and
trustworthy AI implementation. Leveraging our role as a leading institute in
advancing AI research and enabling industry adoption, we present key insights
and lessons learned from practical interpretability applications across diverse
domains. This paper offers actionable strategies and implementation guidance
tailored to organizations at varying stages of AI maturity, emphasizing the
integration of interpretability as a core design principle rather than a
retrospective add-on.

</details>


### [279] [From LLMs to Edge: Parameter-Efficient Fine-Tuning on Edge Devices](https://arxiv.org/abs/2507.23536)
*Georg Slamanig,Francesco Corti,Olga Saukh*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Parameter-efficient fine-tuning (PEFT) methods reduce the computational costs
of updating deep learning models by minimizing the number of additional
parameters used to adapt a model to a down- stream task. While extensively
researched in large language models (LLMs), their application to smaller models
used on edge devices, such as convolutional neural networks, remains
underexplored. This paper benchmarks and analyzes popular PEFT methods on
convolutional architectures typically deployed in resource-constrained edge
environments. We evaluate LoRA, DoRA, and GaLore for updating standard and
depthwise convolutional architectures to handle distribution shifts and
accommodate unseen classes. We utilize recently proposed PyTorch profilers to
compare the updated model performance and computational costs of these PEFT
methods with traditional fine-tuning approaches. With resource efficiency in
mind, we investigate their update behavior across different rank dimensions. We
find that the evaluated PEFT methods are only half as memory-efficient when
applied to depthwise-separable convolution architectures, compared to their
efficiency with LLMs. Conversely, when targeting convolu- tional architectures
optimized for edge deployment, adapter-based PEFT methods can reduce floating
point operations (FLOPs) during model updates by up to 95%. These insights
offer valuable guidance for selecting PEFT methods based on hardware
constraints, performance requirements, and application needs. Our code is
online.

</details>


### [280] [Optimised Feature Subset Selection via Simulated Annealing](https://arxiv.org/abs/2507.23568)
*Fernando Martínez-García,Álvaro Rubio-García,Samuel Fernández-Lorenzo,Juan José García-Ripoll,Diego Porras*

Main category: cs.LG

TL;DR: SA-FDR 是一种新的 $\ell_0$-范数特征选择算法，通过模拟退火解决组合优化问题，在不牺牲预测准确性的情况下选择更小的特征集，特别适用于高维和需要可解释性的场景。


<details>
  <summary>Details</summary>
Motivation: SA-FDR 能够捕捉通常被贪婪优化方法忽略的特征间依赖关系，从而恢复信息性但最小的特征集。

Method: SA-FDR 将 $\ell_0$-范数特征选择视为组合优化问题，并使用模拟退火来执行特征子集上的全局搜索。该优化由 Fisher 判别比指导，Fisher 判别比用作分类任务中模型质量的计算代理。

Result: SA-FDR 能够选择更紧凑的特征子集，同时实现高预测准确性。

Conclusion: SA-FDR 提供了一种灵活且有效的解决方案，可在高维环境中设计可解释模型，尤其是在模型稀疏性、可解释性和性能至关重要的情况下。

Abstract: We introduce SA-FDR, a novel algorithm for $\ell_0$-norm feature selection
that considers this task as a combinatorial optimisation problem and solves it
by using simulated annealing to perform a global search over the space of
feature subsets. The optimisation is guided by the Fisher discriminant ratio,
which we use as a computationally efficient proxy for model quality in
classification tasks. Our experiments, conducted on datasets with up to
hundreds of thousands of samples and hundreds of features, demonstrate that
SA-FDR consistently selects more compact feature subsets while achieving a high
predictive accuracy. This ability to recover informative yet minimal sets of
features stems from its capacity to capture inter-feature dependencies often
missed by greedy optimisation approaches. As a result, SA-FDR provides a
flexible and effective solution for designing interpretable models in
high-dimensional settings, particularly when model sparsity, interpretability,
and performance are crucial.

</details>


### [281] [GraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning](https://arxiv.org/abs/2507.23581)
*Chuanyue Yu,Kuo Zhao,Yuhan Li,Heng Chang,Mingjian Feng,Xiangzhe Jiang,Yufei Sun,Jia Li,Yuzhi Zhang,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: GraphRAG-R1 通过 RL 增强了 LLM 的多步推理能力，解决了现有 GraphRAG 方法在处理复杂问题时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的 GraphRAG 方法在处理需要多步推理的复杂问题时，其查询和检索阶段主要基于预定义的启发式方法，未能充分利用 LLM 的推理潜力，存在显著瓶颈。

Method: 提出了一种自适应 GraphRAG 框架 GraphRAG-R1，通过使用过程约束结果强化学习（RL）来训练 LLM，以增强多步推理能力。具体来说，使用修改后的组相对策略优化（GRPO）来支持“带思考的回放”，设计了渐进式检索衰减（PRA）奖励和成本感知 F1（CAF）奖励来解决浅层检索和过度思考问题，并设计了一个包含冷启动和这两个奖励的三个训练阶段的阶段性训练策略。最后，采用混合图-文本检索来提高推理能力。

Result: GraphRAG-R1 提高了 LLM 解决复杂推理问题的能力，与最先进的 GraphRAG 方法相比，在领域内和领域外数据集上均表现出优越的性能。此外，该框架可以灵活地与各种现有的检索方法集成，并持续提供性能改进。

Conclusion: GraphRAG-R1 通过训练 LLM 来增强多步推理能力，与最先进的 GraphRAG 方法相比，在领域内和领域外数据集上都提高了 LLM 解决复杂推理问题的能力。该框架还可以灵活地与各种现有的检索方法集成，持续提供性能改进。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness
in enhancing the reasoning abilities of LLMs by leveraging graph structures for
knowledge representation and modeling complex real-world relationships.
However, existing GraphRAG methods still face significant bottlenecks when
handling complex problems that require multi-hop reasoning, as their query and
retrieval phases are largely based on pre-defined heuristics and do not fully
utilize the reasoning potentials of LLMs. To address this problem, we propose
GraphRAG-R1, an adaptive GraphRAG framework by training LLMs with
process-constrained outcome-based reinforcement learning (RL) to enhance the
multi-hop reasoning ability. Our method can decompose complex problems,
autonomously invoke retrieval tools to acquire necessary information, and
perform effective reasoning. Specifically, we utilize a modified version of
Group Relative Policy Optimization (GRPO) that supports rollout-with-thinking
capability. Next, we design two process-constrained reward functions. To handle
the shallow retrieval problem, we design a Progressive Retrieval Attenuation
(PRA) reward to encourage essential retrievals. Then, to handle the
over-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the
model performance with computational costs. We further design a phase-dependent
training strategy, containing three training stages corresponding to cold start
and these two rewards. Lastly, our method adopts a hybrid graph-textual
retrieval to improve the reasoning capacity. Extensive experimental results
demonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex
reasoning problems compared to state-of-the-art GraphRAG methods on both
in-domain and out-of-domain datasets. Furthermore, our framework can be
flexibly integrated with various existing retrieval methods, consistently
delivering performance improvements.

</details>


### [282] [EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution](https://arxiv.org/abs/2507.23600)
*Yu-Tang Chang,Shih-Fang Chen*

Main category: cs.LG

TL;DR: EB-gMCR是一种新的深度学习方法，可以自动从混合信号中发现和分离基本成分，即使在数量未知且存在噪声的情况下也能有效工作，并且易于适应不同的化学分析场景。


<details>
  <summary>Details</summary>
Motivation: 传统MCR通常被表述为矩阵分解（MF），需要用户指定通常未知的组件数量。随着数据集大小或组件数量的增加，基于MF的MCR的可扩展性和可靠性面临重大挑战。

Method: 本研究将MCR重新表述为生成过程（gMCR），并引入了一种基于能量的深度学习求解器EB-gMCR，该求解器能够自动发现能够忠实地重建数据的最小组件集。EB-gMCR从大的候选池（例如1024个光谱）开始，并使用可区分的门控网络来保留活动组件，同时估计其浓度。

Result: 在包含多达256个潜在源的噪声合成数据集上，EB-gMCR保持R^2 >= 0.98，并将恢复的组件数量估计在真实值的5%以内；在较低噪声下，其R^2 >= 0.99，并实现了接近精确的组件估计。

Conclusion: EB-gMCR通过结合高容量生成模型和硬组件选择，为大规模信号解混分析（包括化学库驱动的场景）提供了一条实用的途径。

Abstract: Signal unmixing analysis decomposes data into basic patterns and is widely
applied in chemical and biological research. Multivariate curve resolution
(MCR), a branch of signal unmixing, separates mixed chemical signals into base
patterns (components) and their concentrations, playing a key role in
understanding composition. Classical MCR is typically framed as matrix
factorization (MF) and requires a user-specified component count, usually
unknown in real data. As dataset size or component count increases, the
scalability and reliability of MF-based MCR face significant challenges. This
study reformulates MCR as a generative process (gMCR), and introduces an
energy-based deep learning solver, EB-gMCR, that automatically discovers the
smallest component set able to reconstruct the data faithfully. EB-gMCR starts
from a large candidate pool (e.g., 1024 spectra) and employs a differentiable
gating network to retain only active components while estimating their
concentrations. On noisy synthetic datasets containing up to 256 latent
sources, EB-gMCR maintained R^2 >= 0.98 and recovered the component count
within 5% of the ground truth; at lower noise it achieved R^2 >= 0.99 with near
exact component estimation. Additional chemical priors, such as non-negativity
or nonlinear mixing, enter as simple plug-in functions, enabling adaptation to
other instruments or domains without altering the core learning process. By
uniting high-capacity generative modeling and hard component selection, EB-gMCR
offers a practical route to large-scale signal unmixing analysis, including
chemical library-driven scenarios. The source code is available at
https://github.com/b05611038/ebgmcr_solver.

</details>


### [283] [Hierarchical Message-Passing Policies for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2507.23604)
*Tommaso Marzi,Cesare Alippi,Andrea Cini*

Main category: cs.LG

TL;DR: 为了解决去中心化多智能体强化学习中的协调和规划问题，提出了一种结合通信和分层强化学习的新方法。该方法通过封建分层框架、图结构和创新的奖励分配机制，有效地实现了多智能体层级策略的学习，并在实验中取得了优于现有技术的成果。


<details>
  <summary>Details</summary>
Motivation: 去中心化多智能体强化学习（MARL）方法虽然可扩展，但存在部分可观测性和非平稳性问题，这些问题可以通过引入协调和高级规划机制来解决。虽然通信和分层强化学习（HRL）可以实现协调和时间抽象，但优化问题限制了分层策略在多智能体系统中的应用，因此，这些方法的结合尚未得到充分探索。

Method: 提出了一种新颖有效的方法，用于学习通信策略的多智能体层级结构。采用封建分层强化学习框架，并依赖于分层图结构进行智能体间的规划和协调。较低层级的智能体从上层接收目标，并与同一层级的邻近智能体交换信息。设计了一种新颖的奖励分配方法，通过训练较低层级的策略来最大化与上层相关的优势函数。

Result: 所提出的方法在相关基准测试中表现优于现有技术。

Conclusion: 所提出的方法在相关基准测试中表现优于现有技术。

Abstract: Decentralized Multi-Agent Reinforcement Learning (MARL) methods allow for
learning scalable multi-agent policies, but suffer from partial observability
and induced non-stationarity. These challenges can be addressed by introducing
mechanisms that facilitate coordination and high-level planning. Specifically,
coordination and temporal abstraction can be achieved through communication
(e.g., message passing) and Hierarchical Reinforcement Learning (HRL)
approaches to decision-making. However, optimization issues limit the
applicability of hierarchical policies to multi-agent systems. As such, the
combination of these approaches has not been fully explored. To fill this void,
we propose a novel and effective methodology for learning multi-agent
hierarchies of message-passing policies. We adopt the feudal HRL framework and
rely on a hierarchical graph structure for planning and coordination among
agents. Agents at lower levels in the hierarchy receive goals from the upper
levels and exchange messages with neighboring agents at the same level. To
learn hierarchical multi-agent policies, we design a novel reward-assignment
method based on training the lower-level policies to maximize the advantage
function associated with the upper levels. Results on relevant benchmarks show
that our method performs favorably compared to the state of the art.

</details>


### [284] [Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates](https://arxiv.org/abs/2507.23607)
*Tien Huu Do,Antoine Masquelier,Nae Eoun Lee,Jonathan Crowther*

Main category: cs.LG

TL;DR: 提出了一种利用预训练语言模型、注意力机制和伽马分布来预测临床试验患者招募数量的深度学习方法，并在真实数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 准确预测临床试验的患者招募是计划阶段的主要挑战之一，而临床试验通常需要大量的资金投入和细致的规划。

Method: 提出了一种新颖的基于深度学习的方法，利用预训练语言模型（PLM）捕获临床文档的复杂性和细微差别，将其转换为有表现力的表示，并通过注意力机制与编码的表格特征相结合。为了考虑入组预测中的不确定性，该模型还增强了一个基于伽马分布的概率层，以实现范围估计。该模型应用于预测临床试验持续时间，假设站点级别的入组遵循泊松-伽马过程。

Result: 在真实世界的临床试验数据上进行了广泛的实验，结果表明该方法能够有效地预测给定临床试验的多个站点的入组患者数量，并且优于现有的基线模型。

Conclusion: 该方法能够有效预测临床试验的入组患者数量，并且优于现有的基线模型。

Abstract: Clinical trials are a systematic endeavor to assess the safety and efficacy
of new drugs or treatments. Conducting such trials typically demands
significant financial investment and meticulous planning, highlighting the need
for accurate predictions of trial outcomes. Accurately predicting patient
enrollment, a key factor in trial success, is one of the primary challenges
during the planning phase. In this work, we propose a novel deep learning-based
method to address this critical challenge. Our method, implemented as a neural
network model, leverages pre-trained language models (PLMs) to capture the
complexities and nuances of clinical documents, transforming them into
expressive representations. These representations are then combined with
encoded tabular features via an attention mechanism. To account for
uncertainties in enrollment prediction, we enhance the model with a
probabilistic layer based on the Gamma distribution, which enables range
estimation. We apply the proposed model to predict clinical trial duration,
assuming site-level enrollment follows a Poisson-Gamma process. We carry out
extensive experiments on real-world clinical trial data, and show that the
proposed method can effectively predict the number of patients enrolled at a
number of sites for a given clinical trial, outperforming established baseline
models.

</details>


### [285] [L-GTA: Latent Generative Modeling for Time Series Augmentation](https://arxiv.org/abs/2507.23615)
*Luis Roque,Carlos Soares,Vitor Cerqueira,Luis Torgo*

Main category: cs.LG

TL;DR: L-GTA是一种新的时间序列数据增强方法，利用潜在的生成Transformer模型，生成保留真实数据特性的新时间序列，并在各种任务中提高了预测性能。


<details>
  <summary>Details</summary>
Motivation: 数据增强在时间序列分析的各个方面（从预测到分类和异常检测任务）都日益重要。

Method: L-GTA模型是一种基于Transformer的变分循环自动编码器的生成方法，通过在模型的潜在空间中使用受控变换来生成保留原始数据集内在属性的新时间序列。

Result: L-GTA模型能够生成多样化的变换，包括简单的抖动和幅度扭曲，以及组合这些基本变换以生成更复杂的人工时间序列数据集。

Conclusion: L-GTA在真实世界数据集上的评估表明，该模型能够生成更可靠、一致且可控的增强数据，并在预测准确性和相似性度量方面相比直接变换方法有显著改进。

Abstract: Data augmentation is gaining importance across various aspects of time series
analysis, from forecasting to classification and anomaly detection tasks. We
introduce the Latent Generative Transformer Augmentation (L-GTA) model, a
generative approach using a transformer-based variational recurrent
autoencoder. This model uses controlled transformations within the latent space
of the model to generate new time series that preserve the intrinsic properties
of the original dataset. L-GTA enables the application of diverse
transformations, ranging from simple jittering to magnitude warping, and
combining these basic transformations to generate more complex synthetic time
series datasets. Our evaluation of several real-world datasets demonstrates the
ability of L-GTA to produce more reliable, consistent, and controllable
augmented data. This translates into significant improvements in predictive
accuracy and similarity measures compared to direct transformation methods.

</details>


### [286] [On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective](https://arxiv.org/abs/2507.23632)
*Gabriel Mongaras,Eric C. Larson*

Main category: cs.LG

TL;DR: Softmax attention, while computationally expensive, is more expressive than linear attention because it can be understood as a recurrent neural network, allowing for detailed analysis of its components.


<details>
  <summary>Details</summary>
Motivation: To understand why linear attention methods lag in accuracy compared to softmax attention, despite being derived from it.

Method: Deriving the recurrent form of softmax attention and analyzing its components.

Result: Demonstrated that linear attention is an approximation of softmax attention and explained the expressiveness of softmax attention by describing it in the language of RNNs.

Conclusion: The paper explains why softmax attention is more expressive than its linear counterparts by deriving the recurrent form of softmax attention, allowing for component ablation and analysis.

Abstract: Since its introduction, softmax attention has become the backbone of modern
transformer architectures due to its expressiveness and scalability across a
wide range of tasks. However, the main drawback of softmax attention is the
quadratic memory requirement and computational complexity with respect to the
sequence length. By replacing the softmax nonlinearity, linear attention and
similar methods have been introduced to avoid the quadratic bottleneck of
softmax attention. Despite these linear forms of attention being derived from
the original softmax formulation, they typically lag in terms of downstream
accuracy. While strong intuition of the softmax nonlinearity on the query and
key inner product suggests that it has desirable properties compared to other
nonlinearities, the question of why this discrepancy exists still remains
unanswered. This work demonstrates that linear attention is an approximation of
softmax attention by deriving the recurrent form of softmax attention. Using
this form, each part of softmax attention can be described in the language of
recurrent neural networks (RNNs). Describing softmax attention as an RNN allows
for the ablation of the components of softmax attention to understand the
importance of each part and how they interact. In this way, our work helps
explain why softmax attention is more expressive than its counterparts.

</details>


### [287] [OptiGradTrust: Byzantine-Robust Federated Learning with Multi-Feature Gradient Analysis and Reinforcement Learning-Based Trust Weighting](https://arxiv.org/abs/2507.23638)
*Mohammad Karami,Fatemeh Ghassemi,Hamed Kebriaei,Hamid Azadegan*

Main category: cs.LG

TL;DR: OptiGradTrust 和 FedBN-Prox 是用于联邦学习的防御框架，可抵御拜占庭攻击和数据异质性。OptiGradTrust 使用六维指纹和 RL-注意力模块进行信任评分，FedBN-Prox 使用联邦批量归一化和近端正则化来提高准确性和收敛性。在各种数据集和攻击下，该框架均优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）允许跨分布式医疗机构进行协作模型训练，同时保护患者隐私，但仍然容易受到拜占庭攻击和统计异质性的影响。

Method: 提出了一种名为OptiGradTrust的综合防御框架，该框架通过包含VAE重建误差、余弦相似性度量、$L_2$范数、符号一致性比率和蒙特卡洛Shapley值的新颖六维指纹来评估梯度更新，并驱动混合强化学习-注意力模块进行自适应信任评分。为了解决数据异质性下的收敛挑战，开发了FedBN-Prox（FedBN-P），它结合了联邦批量归一化和近端正则化，以实现最佳的准确性-收敛性权衡。

Result: 在MNIST、CIFAR-10和阿尔茨海默病MRI数据集上，在各种拜占庭攻击场景下进行了广泛评估，与最先进的防御相比，取得了显著的改进，在非独立同分布条件下，在我们的自适应学习方法下，在多种攻击模式下保持了稳健的性能。

Conclusion: OptiGradTrust框架通过六维指纹和混合RL-注意力模块有效抵御拜占庭攻击，FedBN-Prox通过结合联邦批量归一化和近端正则化解决了数据异质性问题，并在MNIST、CIFAR-10和阿尔茨海默病MRI数据集上进行了广泛评估，在非独立同分布条件下，相比FLGuard有高达+1.6个百分点的提升，并在多种攻击模式下保持了稳健的性能。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed medical institutions while preserving patient privacy, but remains
vulnerable to Byzantine attacks and statistical heterogeneity. We present
OptiGradTrust, a comprehensive defense framework that evaluates gradient
updates through a novel six-dimensional fingerprint including VAE
reconstruction error, cosine similarity metrics, $L_2$ norm, sign-consistency
ratio, and Monte Carlo Shapley value, which drive a hybrid RL-attention module
for adaptive trust scoring. To address convergence challenges under data
heterogeneity, we develop FedBN-Prox (FedBN-P), combining Federated Batch
Normalization with proximal regularization for optimal accuracy-convergence
trade-offs. Extensive evaluation across MNIST, CIFAR-10, and Alzheimer's MRI
datasets under various Byzantine attack scenarios demonstrates significant
improvements over state-of-the-art defenses, achieving up to +1.6 percentage
points over FLGuard under non-IID conditions while maintaining robust
performance against diverse attack patterns through our adaptive learning
approach.

</details>


### [288] [SHAP-Guided Regularization in Machine Learning Models](https://arxiv.org/abs/2507.23665)
*Amal Saadallah*

Main category: cs.LG

TL;DR: 提出了一种SHAP引导的正则化框架，通过将特征重要性约束纳入模型训练，以提高预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的特征归因方法（如SHAP）在理解机器学习模型方面发挥着重要作用，但其在指导模型优化方面的作用仍未得到充分探索。

Method: 提出了一种结合了SHAP（SHapley Additive exPlanations）特征重要性约束的模型训练框架，并应用基于熵的惩罚来鼓励稀疏、集中的特征归因，同时提高跨样本的稳定性。该框架适用于回归和分类任务，并以TreeSHAP为起点，探索了基于树的模型正则化。

Result: 通过在基准回归和分类数据集上进行的大量实验，证明了该方法在提高泛化性能的同时，还能确保鲁棒且可解释的特征归因。

Conclusion: 该方法提供了一种新颖的、可解释性驱动的正则化方法，使机器学习模型更加准确和可靠。

Abstract: Feature attribution methods such as SHapley Additive exPlanations (SHAP) have
become instrumental in understanding machine learning models, but their role in
guiding model optimization remains underexplored. In this paper, we propose a
SHAP-guided regularization framework that incorporates feature importance
constraints into model training to enhance both predictive performance and
interpretability. Our approach applies entropy-based penalties to encourage
sparse, concentrated feature attributions while promoting stability across
samples. The framework is applicable to both regression and classification
tasks. Our first exploration started with investigating a tree-based model
regularization using TreeSHAP. Through extensive experiments on benchmark
regression and classification datasets, we demonstrate that our method improves
generalization performance while ensuring robust and interpretable feature
attributions. The proposed technique offers a novel, explainability-driven
regularization approach, making machine learning models both more accurate and
more reliable.

</details>


### [289] [TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses](https://arxiv.org/abs/2507.23674)
*Muhammad Taha Cheema,Abeer Aamir,Khawaja Gul Muhammad,Naveed Anwar Bhatti,Ihsan Ayyub Qazi,Zafar Ayyub Qazi*

Main category: cs.LG

TL;DR: TweakLLM 是一种通过使用轻量级 LLM 动态调整缓存响应来优化 LLM 响应缓存的新方法，可保持响应质量并提高缓存效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决 LLM 缓存响应相关性难以保证的问题，LLM 每天处理数百万次查询，因此有效的响应缓存是降低成本和延迟的有吸引力的优化方法。然而，使用此方法保留与用户查询的相关性由于聊天机器人交互的个性化性质和语义相似性搜索的有限准确性而变得困难。

Method: 提出了一种名为 TweakLLM 的新颖路由架构，该架构使用轻量级 LLM 动态地将缓存的响应调整为传入的提示。

Result: 通过包括用户研究（带有并排比较、满意度投票）和多智能体 LLM 辩论在内的综合评估，证明 TweakLLM 在保持响应质量方面与前沿模型相当，同时显著提高了缓存效率。在真实数据集上的结果证明了 TweakLLM 的有效性。

Conclusion: TweakLLM 是一种可扩展、资源高效的缓存解决方案，适用于大规模 LLM 部署，且不影响用户体验。

Abstract: Large Language Models (LLMs) process millions of queries daily, making
efficient response caching a compelling optimization for reducing cost and
latency. However, preserving relevance to user queries using this approach
proves difficult due to the personalized nature of chatbot interactions and the
limited accuracy of semantic similarity search. To address this, we present
TweakLLM, a novel routing architecture that employs a lightweight LLM to
dynamically adapt cached responses to incoming prompts. Through comprehensive
evaluation, including user studies with side-by-side comparisons, satisfaction
voting, as well as multi-agent LLM debates, we demonstrate that TweakLLM
maintains response quality comparable to frontier models while significantly
improving cache effectiveness. Our results across real-world datasets highlight
TweakLLM as a scalable, resource-efficient caching solution for high-volume LLM
deployments without compromising user experience.

</details>


### [290] [One-Step Flow Policy Mirror Descent](https://arxiv.org/abs/2507.23675)
*Tianyi Chen,Haitong Ma,Na Li,Kai Wang,Bo Dai*

Main category: cs.LG

TL;DR: 提出FPMD算法，实现扩散策略的一步采样推理，显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 克服扩散策略模型推理过程依赖于缓慢的迭代采样过程，从而限制其响应速度的局限性。

Method: 提出了一种名为Flow Policy Mirror Descent (FPMD) 的在线强化学习算法，该算法能够实现一步采样进行策略推理。该方法利用了分布方差与直线插值流匹配模型中单步采样离散化误差之间的理论联系，并且不需要额外的蒸馏或一致性训练。提供了基于流策略和MeanFlow策略参数化的两种算法变体。

Result: 在MuJoCo基准测试中，FPMD算法在推理时所需的函数评估次数比扩散策略基线少数百倍，同时性能相当。

Conclusion: FPMD算法在MuJoCo基准测试中表现出与扩散策略基线相当的性能，同时在推理过程中所需的函数评估次数减少了数百倍。

Abstract: Diffusion policies have achieved great success in online reinforcement
learning (RL) due to their strong expressive capacity. However, the inference
of diffusion policy models relies on a slow iterative sampling process, which
limits their responsiveness. To overcome this limitation, we propose Flow
Policy Mirror Descent (FPMD), an online RL algorithm that enables 1-step
sampling during policy inference. Our approach exploits a theoretical
connection between the distribution variance and the discretization error of
single-step sampling in straight interpolation flow matching models, and
requires no extra distillation or consistency training. We present two
algorithm variants based on flow policy and MeanFlow policy parametrizations,
respectively. Extensive empirical evaluations on MuJoCo benchmarks demonstrate
that our algorithms show strong performance comparable to diffusion policy
baselines while requiring hundreds of times fewer function evaluations during
inference.

</details>


### [291] [DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data](https://arxiv.org/abs/2507.23676)
*Rabeya Tus Sadia,Qiang Cheng*

Main category: cs.LG

TL;DR: DepMicroDiff是一种新的框架，通过结合扩散模型和依赖感知Transformer（DAT），并利用VAE预训练和LLM编码的患者元数据，可以更准确地填充稀疏且有噪声的微生物组数据，提高了下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的微生物组数据填充方法（包括基于扩散的模型）往往无法捕捉微生物分类群之间复杂的相互依赖关系，并且忽略了可以为填充提供信息的上下文元数据，这阻碍了生物标志物发现等下游任务。

Method: DepMicroDiff框架结合了基于扩散的生成模型和依赖感知Transformer（DAT），以明确捕捉微生物分类群之间相互的成对依赖关系和自回归关系。该框架通过在多种癌症数据上进行VAE预训练，并利用大型语言模型（LLM）编码的患者元数据进行条件化，进一步得到了增强。

Result: DepMicroDiff在多个癌症类型的TCGA微生物组数据集上进行了实验，其性能显著优于最先进的基线方法，在皮尔逊相关性（高达0.712）、余弦相似性（高达0.812）方面表现更佳，同时在多个癌症类型中实现了更低的RMSE和MAE，证明了其在微生物组填充方面的鲁棒性和泛化能力。

Conclusion: DepMicroDiff在多个癌症类型的TCGA微生物组数据集上进行了实验，其性能显著优于最先进的基线方法，在皮尔逊相关性（高达0.712）、余弦相似性（高达0.812）方面表现更佳，同时在多个癌症类型中实现了更低的RMSE和MAE，证明了其在微生物组填充方面的鲁棒性和泛化能力。

Abstract: Microbiome data analysis is essential for understanding host health and
disease, yet its inherent sparsity and noise pose major challenges for accurate
imputation, hindering downstream tasks such as biomarker discovery. Existing
imputation methods, including recent diffusion-based models, often fail to
capture the complex interdependencies between microbial taxa and overlook
contextual metadata that can inform imputation. We introduce DepMicroDiff, a
novel framework that combines diffusion-based generative modeling with a
Dependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise
dependencies and autoregressive relationships. DepMicroDiff is further enhanced
by VAE-based pretraining across diverse cancer datasets and conditioning on
patient metadata encoded via a large language model (LLM). Experiments on TCGA
microbiome datasets show that DepMicroDiff substantially outperforms
state-of-the-art baselines, achieving higher Pearson correlation (up to 0.712),
cosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer
types, demonstrating its robustness and generalizability for microbiome
imputation.

</details>


### [292] [Anomalous Samples for Few-Shot Anomaly Detection](https://arxiv.org/abs/2507.23712)
*Aymane Abdali,Bartosz Boguslawski,Lucas Drumetz,Vincent Gripon*

Main category: cs.LG

TL;DR: 在少样本场景下，异常样本比正常样本更容易获取。我们提出了一种结合异常样本和正常样本的方法，用于二元异常分类，并取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 在少样本设置中，异常样本的获取可能比正常样本更容易，传统的依赖大量正常样本的方法不再适用，因此需要研究如何利用异常样本进行模型训练。

Method: 提出了一种结合异常样本和正常样本，利用零样本学习和基于内存的技术，构建多分数异常检测模型的方法，并通过基于增强的验证技术优化分数聚合。

Result: 实验证明，异常样本在少样本设置下的模型训练中具有重要作用，并且提出的方法在工业异常检测数据集上表现优于仅使用正常样本的方法。

Conclusion: 利用异常样本进行二元异常分类是可行的，并且通过多分数异常检测方法和基于内存的技术可以取得良好效果。

Abstract: Several anomaly detection and classification methods rely on large amounts of
non-anomalous or "normal" samples under the assump- tion that anomalous data is
typically harder to acquire. This hypothesis becomes questionable in Few-Shot
settings, where as little as one anno- tated sample can make a significant
difference. In this paper, we tackle the question of utilizing anomalous
samples in training a model for bi- nary anomaly classification. We propose a
methodology that incorporates anomalous samples in a multi-score anomaly
detection score leveraging recent Zero-Shot and memory-based techniques. We
compare the utility of anomalous samples to that of regular samples and study
the benefits and limitations of each. In addition, we propose an
augmentation-based validation technique to optimize the aggregation of the
different anomaly scores and demonstrate its effectiveness on popular
industrial anomaly detection datasets.

</details>


### [293] [Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System](https://arxiv.org/abs/2507.23756)
*Diana Mortagua*

Main category: cs.LG

TL;DR: 本研究提出了一种新的主动学习标注员选择策略，该策略利用推荐系统，并考虑了标注员的情绪和疲劳等内部因素，以减少标注错误并提高效率。


<details>
  <summary>Details</summary>
Motivation: 主动学习（AL）中选择最佳标注员以最小化错误分类的挑战，以及现有策略未考虑影响标注员生产力的内部因素（如情绪、注意力、积极性和疲劳水平）。

Method: 提出了一种新的查询-标注器配对策略，利用基于知识的推荐系统（RS）。该系统考虑了标注员的过去准确性、情绪和疲劳水平以及查询实例的信息，对标注员进行排序，以选择最合适的标注员。

Result: 考虑过去准确性、情绪和疲劳水平的策略可减少标注错误和模型不确定性，准确率和 F1 分数也有所提高。

Conclusion: 本研究提出的考虑了过去准确性、情绪和疲劳水平的查询-标注器配对策略，相比于不考虑内部因素的策略，减少了标注错误和模型训练过程中的不确定性。准确率和 F1 分数也有所提高。

Abstract: This study centers on overcoming the challenge of selecting the best
annotators for each query in Active Learning (AL), with the objective of
minimizing misclassifications. AL recognizes the challenges related to cost and
time when acquiring labeled data, and decreases the number of labeled data
needed. Nevertheless, there is still the necessity to reduce annotation errors,
aiming to be as efficient as possible, to achieve the expected accuracy faster.
Most strategies for query-annotator pairs do not consider internal factors that
affect productivity, such as mood, attention, motivation, and fatigue levels.
This work addresses this gap in the existing literature, by not only
considering how the internal factors influence annotators (mood and fatigue
levels) but also presenting a new query-annotator pair strategy, using a
Knowledge-Based Recommendation System (RS). The RS ranks the available
annotators, allowing to choose one or more to label the queried instance using
their past accuracy values, and their mood and fatigue levels, as well as
information about the instance queried. This work bases itself on existing
literature on mood and fatigue influence on human performance, simulating
annotators in a realistic manner, and predicting their performance with the RS.
The results show that considering past accuracy values, as well as mood and
fatigue levels reduces the number of annotation errors made by the annotators,
and the uncertainty of the model through its training, when compared to not
using internal factors. Accuracy and F1-score values were also better in the
proposed approach, despite not being as substantial as the aforementioned. The
methodologies and findings presented in this study begin to explore the open
challenge of human cognitive factors affecting AL.

</details>


### [294] [Consensus-Driven Active Model Selection](https://arxiv.org/abs/2507.23771)
*Justin Kay,Grant Van Horn,Subhransu Maji,Daniel Sheldon,Sara Beery*

Main category: cs.LG

TL;DR: Active model selection using consensus and disagreement between models to prioritize labeling, outperforming prior methods significantly.


<details>
  <summary>Details</summary>
Motivation: The widespread availability of off-the-shelf machine learning models poses a challenge: which model, of the many available candidates, should be chosen for a given data analysis task? This question of model selection is traditionally answered by collecting and annotating a validation dataset -- a costly and time-intensive process.

Method: CODA performs consensus-driven active model selection by modeling relationships between classifiers, categories, and data points within a probabilistic framework. The framework uses the consensus and disagreement between models in the candidate pool to guide the label acquisition process, and Bayesian inference to update beliefs about which model is best as more information is collected.

Result: We validate our approach by curating a collection of 26 benchmark tasks capturing a range of model selection scenarios. CODA outperforms existing methods for active model selection significantly, reducing the annotation effort required to discover the best model by upwards of 70% compared to the previous state-of-the-art.

Conclusion: CODA outperforms existing methods for active model selection significantly, reducing the annotation effort required to discover the best model by upwards of 70% compared to the previous state-of-the-art.

Abstract: The widespread availability of off-the-shelf machine learning models poses a
challenge: which model, of the many available candidates, should be chosen for
a given data analysis task? This question of model selection is traditionally
answered by collecting and annotating a validation dataset -- a costly and
time-intensive process. We propose a method for active model selection, using
predictions from candidate models to prioritize the labeling of test data
points that efficiently differentiate the best candidate. Our method, CODA,
performs consensus-driven active model selection by modeling relationships
between classifiers, categories, and data points within a probabilistic
framework. The framework uses the consensus and disagreement between models in
the candidate pool to guide the label acquisition process, and Bayesian
inference to update beliefs about which model is best as more information is
collected. We validate our approach by curating a collection of 26 benchmark
tasks capturing a range of model selection scenarios. CODA outperforms existing
methods for active model selection significantly, reducing the annotation
effort required to discover the best model by upwards of 70% compared to the
previous state-of-the-art. Code and data are available at
https://github.com/justinkay/coda.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [295] [Exciton Berryology](https://arxiv.org/abs/2507.22983)
*Henry Davenport,Johannes Knolle,Frank Schindler*

Main category: cond-mat.mes-hall

TL;DR: 我们提出了一种计算激子 Berry 规和极化动力学的方法。


<details>
  <summary>Details</summary>
Motivation: 在具有激子束缚态的平移不变半导体中，可以定义无限多个可能的激子 Berry 规。

Method: 我们定义了一个激子投影位置算符，其特征值区分了两种独特的激子 Berry 规和相关的 Berry 规，一个用于电子，一个用于空穴。我们阐明了这些激子 Berry 规的物理意义，并提供了一个离散的 Wilson 圈公式，可以在没有平滑规的情况下对其进行数值计算。

Result: 作为推论，我们得到了给定总动量下激子极化的规范不变表达式，即激子波函数内电子和空穴的平均分离度。在晶体反演对称性的存在下，电子和空穴激子 Berry 规被量化为相同的值，并且我们推导了该值如何用许多体激子态的反演特征值来表示。对于反酉的晶体 $C_2 	extbackslash mathcal{T}$ 对称性，我们确认激子 Berry 规仍然被量化，并且仍然诊断出拓扑上不同的激子能带。

Conclusion: 该理论将激子 Wannier 态与非相互作用带位移一个量子化量位的位移激子推广到对称性指示器之外。

Abstract: In translationally invariant semiconductors that host exciton bound states,
one can define an infinite number of possible exciton Berry connections. These
correspond to the different ways in which a many-body exciton state, at fixed
total momentum, can be decomposed into free electron and hole Bloch states that
are entangled by an exciton envelope wave function. Inspired by the modern
theory of polarization, we define an exciton projected position operator whose
eigenvalues single out two unique choices of exciton Berry phase and associated
Berry connection - one for electrons, and one for holes. We clarify the
physical meaning of these exciton Berry phases and provide a discrete Wilson
loop formulation that allows for their numerical calculation without a smooth
gauge. As a corollary, we obtain a gauge-invariant expression for the exciton
polarisation at a given total momentum, i.e. the mean separation of the
electron and hole within the exciton wave function. In the presence of
crystalline inversion symmetry, the electron and hole exciton Berry phases are
quantized to the same value and we derive how this value can be expressed in
terms of inversion eigenvalues of the many-body exciton state. We then consider
crystalline $C_2 \mathcal{T}$ symmetry, for which no symmetry eigenvalues are
available as it is anti-unitary, and confirm that the exciton Berry phase
remains quantized and still diagnoses topologically distinct exciton bands. Our
theory thereby generalizes the notion of shift excitons, whose exciton Wannier
states are displaced from those of the non-interacting bands by a quantized
amount, beyond symmetry indicators.

</details>


### [296] [Higher-order Topological States in Chiral Split Magnons of Honeycomb Altermagnets](https://arxiv.org/abs/2507.22996)
*Xuan Guo,Meng-Han Zhang,Dao-Xin Yao*

Main category: cond-mat.mes-hall

TL;DR: 理论探索了共线非均匀磁体中的高阶拓扑磁子，揭示了铰链模式的传播特性，为磁子量子信息处理提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 探索高阶拓扑磁子在共线非均匀磁体中的性质，并为磁子量子信息处理提供平台。

Method: 理论上探索了共线非均匀磁体中高阶拓扑磁子，包括从局域角模式到传播铰链激发，并通过在反铁磁层耦合的玻色子Bogoliubov-de Gennes (BdG)哈密顿量中，揭示了AA型堆垛结构中的各向异性表面态和空间分布的铰链模式。通过追踪Wannier中心的绝热演化来识别具有二阶拓扑磁子绝缘体（SOTMI）的体极化，其中各种磁子谱证明了超越常规拓扑的对称性保护能带结构。

Result: 揭示了AA型堆垛结构中的各向异性表面态和空间分布的铰链模式，并证明了对称性保护能带结构。

Conclusion: 该研究为磁子量子信息处理提供了一个潜在平台，在非均匀磁性系统中实现了高效的逻辑运算。

Abstract: We theoretically explore higher-order topological magnons in collinear
altermagnets, encompassing a dimensional hierarchy ranging from localized
corner modes to propagating hinge excitations. By employing antiferromagnetic
interlayer coupling in bosonic Bogoliubov-de Gennes (BdG) Hamiltonian, our work
reveals anisotropic surface states and spatially distributed hinge modes within
AA-type stacking configurations. We track the adiabatic evolution of Wannier
centers to identify the bulk-polarization with second-order topological magnon
insulator (SOTMI), where various magnon spectra demonstrate symmetry-protected
band structure beyond conventional topology. Harnessing the stability and
propagative properties of hinge modes, our study offers a potential platform
for magnonic quantum information processing in altermagnetic systems that
performs energy-efficient logic operation.

</details>


### [297] [Quantum confinement effect in Sb thin films](https://arxiv.org/abs/2507.23014)
*Anuradha Wijesinghe,Yongxi Ou,Anjali Rathore,Chandima Edirisinghe,Pradip Adhikari,An-Hsi Chen,Dustin Gilbert,Anthony Richardella,Nitin Samarth,Joon Sue Lee*

Main category: cond-mat.mes-hall

TL;DR: 通过实验证实了锑薄膜在减小厚度时会发生从拓扑半金属到拓扑绝缘体的相变，这为自旋电子学和量子技术应用提供了基础。


<details>
  <summary>Details</summary>
Motivation: 锑（Sb）是一种具有强自旋-轨道耦合的元素，理论预测当其维度接近二维极限时，会因量子限制效应发生从拓扑半金属到拓扑绝缘体的相变。本研究旨在实验上探索和证实这一相变过程。

Method: 本研究采用分子束外延技术生长锑薄膜，并利用电输运测量（包括霍尔效应、纵向电阻曲率变化、温度依赖性磁阻）和角分辨光电子能谱（ARPES）来研究其电子结构和拓扑性质。

Result: 电输运测量显示，随着薄膜厚度减小，锑薄膜的电子能带结构发生改变，表现为霍尔效应呈现多载流子特征、载流子浓度降低、纵向电阻曲率从二次变为线性。低于16 K的温度依赖性磁阻测量显示出弱反局域化现象，表明存在强的自旋-轨道耦合和非平凡拓扑态。对弱反局域化（WAL）特性的分析表明存在单一相干导电通道，并且相干机制随厚度变化。ARPES测量证实，减小薄膜厚度会提升M点的导带，这与带隙的出现一致。

Conclusion: 该研究通过实验证实了锑（Sb）薄膜在维度接近二维极限时，由于量子限制效应，会发生从拓扑半金属到拓扑绝缘体的相变。研究结果为理解锑及其合金（如Bi1-xSbx）的拓扑相变提供了实验基础，并指出锑作为一种元素拓扑材料，在自旋电子学和量子技术中有潜在应用价值。

Abstract: Antimony (Sb), an element with strong spin-orbit coupling, is predicted to
undergo a topological phase transition from a topological semimetal to a
topological insulator as its dimensionality approaches the two-dimensional
limit, driven by the quantum confinement effect. In this study, we investigate
this transition in Sb thin films grown by molecular beam epitaxy, employing
electrical transport measurements and angle-resolved photoemission spectroscopy
(ARPES). Electrical transport measurements revealed signatures of a modified
electronic band structure, including a Hall response with multiple carrier
types, a decreasing carrier concentration, and a transition in the curvature of
the longitudinal resistance from quadratic to linear with decreasing film
thickness. Temperature-dependent magnetoresistance further showed weak
antilocalization below 16 K, indicating strong spin-orbit coupling and
suggesting the presence of non-trivial topological states. Analysis of the WAL
characteristics revealed a single coherent conducting channel and a
thickness-dependent change in the phase decoherence mechanism. Complementary
ARPES measurements confirmed that reducing the film thickness lifts the
conduction band at the M-point, consistent with the emergence of a band gap.
These findings support theoretical predictions of a thickness-dependent band
structure evolution driven by the quantum confinement effect, providing a
foundation for further exploration of topological phase transitions in Sb as
well as Bi1-xSbx. The realization of an elemental topological material with
simplified stoichiometry and semiconductor compatibility presents a promising
avenue for next-generation hybrid systems and applications in spintronics and
quantum technologies.

</details>


### [298] [Exploring Many-Body Quantum Geometry Beyond the Quantum Metric with Correlation Functions: A Time-Dependent Perspective](https://arxiv.org/abs/2507.23028)
*Yuntao Guan,Barry Bradlyn*

Main category: cond-mat.mes-hall

TL;DR: 本文提出了一个处理多体系统非线性响应的通用时变量子几何框架，通过Bures距离的微扰展开定义了时变Bures度量和Bures-Levi-Civita联络，并统一了线性响应和费米黄金定则的几何解释。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在线性响应的几何描述，缺乏对包括非线性响应在内的更高阶微扰现象的通用几何描述。

Method: 本文开发了一个通用的时变量子几何框架，通过将外部扰动场视为密度矩阵空间的坐标，并利用初始密度矩阵与时间演化密度矩阵之间的Bures距离，通过微扰展开定义几何量。

Result: 在最低阶微扰中，推导出了与线性响应函数谱密度相关的时变Bures度量，统一了先前各种极限下的量子度量结果，并提供了费米黄金定则的几何解释。在下一阶微扰中，定义了通用的时变Bures-Levi-Civita联络，它由与二阶非线性响应函数相关的部分和捕获一阶微扰更高几何结构的部分组成。在准静态、零温度和非相互作用费米子极限下，该联络可化为带理论Christoffel符号的已知表达式。

Conclusion: 本研究提出了一个处理多体系统非线性响应的通用框架，通过将外部扰动场视为密度矩阵空间的坐标，并利用Bures距离的微扰展开来定义几何量。该框架将量子度量推广到时变情况，并为费米黄金定则提供了几何解释。此外，还定义了描述二阶非线性响应和一阶微扰更高几何结构的时变Bures-Levi-Civita联络，并在特定极限下与已知的带理论Christoffel符号相吻合。

Abstract: The quantum geometric tensor and quantum Fisher information have recently
been shown to provide a unified geometric description of the linear response of
many-body systems. However, a similar geometric description of higher-order
perturbative phenomena including nonlinear response in generic quantum systems
is lacking. In this work, we develop a general framework for the time-dependent
quantum geometry of many-body systems by treating external perturbing fields as
coordinates on the space of density matrices. We use the Bures distance between
the initial and time-evolved density matrix to define geometric quantities
through a perturbative expansion. To lowest order, we derive a time-dependent
generalization of the Bures metric related to the spectral density of linear
response functions, unifying previous results for the quantum metric in various
limits and providing a geometric interpretation of Fermi's golden rule. At next
order in the expansion, we define a time-dependent Bures-Levi-Civita connection
for general many-body systems. We show that the connection is the sum of one
contribution that is related to a second-order nonlinear response function, and
a second contribution that captures the higher geometric structure of
first-order perturbation theory. We show that in the quasistatic,
zero-temperature limit for noninteracting fermions, this Bures connection
reduces to the known expression for band-theoretic Christoffel symbols. Our
work provides a systematic framework to explore many-body quantum geometry
beyond the quantum metric and highlights how higher-order correlation functions
can probe this geometry.

</details>


### [299] [Model Hamiltonian for Altermagnetic Topological Insulators](https://arxiv.org/abs/2507.23173)
*Rafael Gonzalez-Hernandez,Bernardo Uribe*

Main category: cond-mat.mes-hall

TL;DR: 该研究提出了具有交替磁特性的拓扑绝缘体模型，并证明了自旋陈数在表征其拓扑性质中的作用，为设计无净磁化的自旋电子拓扑系统提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 旨在探索具有本征交替磁特性的拓扑绝缘体，并为理解和设计无净磁化的自旋电子拓扑系统提供理论框架。

Method: 提出了具有本征交替磁特性的拓扑绝缘体哈密顿量模型，并证明了自旋陈数作为二维系统中的拓扑不变量，以及在三维系统中通过在特定平面上的自旋陈数来表征拓扑性质。同时，展示了所提出的模型支持受对称性保护的边界模式，并分析了这些模式的结构。

Result: 开发了拓扑绝缘体哈密顿量模型，该模型具有本征交替磁特性，并受到旋转对称性和时间反转的保护。证明了自旋陈数在表征拓扑性质中的作用，并展示了所提出的模型可以支持具有特定结构的边界模式。

Conclusion: 该研究提出了具有本征交替磁特性的拓扑绝缘体哈密顿量模型，这些特性受到与时间反转相结合的三重或四重旋转对称性的保护。研究表明，在二维系统中，自旋陈数是鲁棒的拓扑不变量；在三维结构中，拓扑性质通过在 k_z=0 和 k_z=π 平面上计算得到的自旋陈数来表征。研究结果支持受对称性保护的边界模式，包括角态、铰链态和表面态，其结构由磁对称性和局部磁矩决定。该研究将交替磁性与拓扑量子物质领域联系起来，并为设计无净磁化的自旋电子拓扑系统奠定了理论基础。

Abstract: We present models of topological insulating Hamiltonians exhibiting intrinsic
altermagnetic features, protected by combined three-fold or four-fold
rotational symmetries with time-reversal. We demonstrate that the spin Chern
number serves as a robust topological invariant in two-dimensional systems,
while for three-dimensional structures, the topological nature is characterized
by the spin Chern numbers computed on the $k_z$=$0$ and $k_z$=$\pi$ planes. The
resulting phases support symmetry-protected boundary modes, including corner,
hinges and surface states, whose structure is determined by the magnetic
symmetry and the local magnetic moments. Our findings bridge the fields of
altermagnetism and topological quantum matter, and establish a theoretical
framework for engineering spintronic topological systems without net
magnetization.

</details>


### [300] [Spin-State Engineering of Single Titanium Adsorbates on Ultrathin Magnesium Oxide](https://arxiv.org/abs/2507.23299)
*Soo-hyon Phark,Hong Thi Bui,We-hyo Seo,Yaowu Liu,Valeria Sheina,Curie Lee,Christoph Wolf,Andreas J. Heinrich,Roberto Robles,Nicolas Lorente*

Main category: cond-mat.mes-hall

TL;DR: 研究了MgO/Ag(100)表面上单个Ti原子的自旋状态，发现自旋态（S=1/2和S=1）取决于吸附位点和MgO厚度，这为构建可调量子平台提供了基础。


<details>
  <summary>Details</summary>
Motivation: 为了了解和控制单个吸附物种的自旋状态，以实现基于原子上相同的、可单独寻址的自旋量子比特的自下而上的量子架构。

Method: 结合扫描隧道显微镜和电子自旋共振，以及密度泛函理论计算和多轨道原子多重态计算。

Result: 发现了两个不同的自旋态（S=1/2和S=1），这取决于局部吸附位点和MgO薄膜的厚度。密度泛函理论计算表明，Ti吸附物种具有Ti+构型，在4s和3d价电子壳层中有大约3个电子。多轨道原子多重态计算能够解释自旋的位点依赖性，这是由于自旋极化轨道和去极化轨道之间的电荷重新分布。

Conclusion: 这项工作展示了表面支撑的单原子作为自旋量子比特的潜力，具有可调的自旋和电荷状态，能够在表面实现多功能量子平台的逐原子控制。

Abstract: Single atomic adsorbates on ultrathin insulating films provide a promising
route toward bottom-up quantum architectures based on atomically identical yet
individually addressable spin qubits on solid surfaces. A key challenge in
engineering quantum-coherent spin nanostructures lies in understanding and
controlling the spin state of individual adsorbates. In this work, we
investigate single titanium (Ti) atoms adsorbed on MgO/Ag(100) surfaces using a
combined scanning tunneling microscopy and electron spin resonance. Our
measurements reveal two distinct spin states, $S = 1/2$ and $S = 1$, depending
on the local adsorption site and the thickness of the MgO film. Density
functional theory calculations suggest a Ti$^+$ configuration for the Ti
adsorbates with approximately 3 electrons in the 4$s$ and 3$d$ valence shells.
Using a multi-orbital atomic multiplet calculations the site dependence of the
spin can be rationalized as a charge redistribution between spin-polarizing and
depolarizing orbitals. These findings underscore the potential of
surface-supported single atoms as spin qubits with tunable spin and charge
states, enabling atom-by-atom control in the realization of a versatile quantum
platform on surfaces.

</details>


### [301] [Printable Nanocomposites with Superparamagnetic Maghemite ($γ$-Fe$_2$O$_3$) Particles for Microinductor-core Applications](https://arxiv.org/abs/2507.23522)
*Mathias Zambach,Miriam Varón,Thomas Veile,Bima N. Sanusi,Matti Knaapila,Anders M. Jørgensen,László Almásy,Christer Johansson,Ziwei Ouyang,M. Beleggia,Cathrine Frandsen*

Main category: cond-mat.mes-hall

TL;DR: 研究人员制备了一种可打印、可铸造的磁性纳米复合材料，其中包含超顺磁性的$\\\gamma$-Fe$_2$O$_3$ 纳米颗粒（11±3 nm）和聚乙烯醇聚合物基质。该材料在高达100 MHz的频率下表现出良好的磁性能，并且易于微加工集成，可用于制造电感器。


<details>
  <summary>Details</summary>
Motivation: 为了减少由颗粒尺寸分布引起的磁滞损耗（高达10^2-10^5 kW/m^3），提出了一种更窄的颗粒尺寸分布。

Method: 制备了含有超顺磁性 $\gamma$-Fe$_2$O$_3$ 纳米颗粒（11±3 nm）的绝缘性聚乙烯醇聚合物基质的磁性纳米复合材料，颗粒体积分数在10%至45%之间。通过小角中子散射确认了颗粒的分散性。

Result: 所制备的纳米复合材料具有高达17的磁体积磁化率，在低频下具有可忽略的磁滞，并且在高达高kHz的范围内具有恒定的交流响应。在100-900 kHz和高达110 mT的磁场下测量的磁滞曲线表明，功率损耗与磁场平方和频率1-1.3次方成正比。在>100 kHz的频率下，唯一的损耗机制是磁滞损耗，此时11±3 nm分布中的最大颗粒从超顺磁状态转变为阻塞状态。

Conclusion: 所提出的材料无涡流损耗，易于微加工集成。通过制造基于3匝印刷电路板的电感器，并使用铸造/手动印刷的纳米复合材料电感器磁芯，已证明了这一点，其电感已测量到100 MHz。

Abstract: We here present printable and castable magnetic nanocomposites containing
superparamagnetic 11$\pm$3 nm $\gamma$-Fe$_2$O$_3$ particles in an insulating
poly-vinyl alcohol polymer matrix. The nanocomposites feature well-dispersed
particles with volume fractions between 10 and 45 \%, as confirmed by
small-angle neutron scattering. The magnetic volume susceptibility is as high
as 17, together with negligible hysteresis at low frequency, and constant
AC-response up to the high-kHz range. Measured hysteresis curves at 100-900 kHz
with up to 110 mT induced $B$-fields in the nanocomposite show that power
losses depend on $B$-field squared, and frequency to the power of 1-1.3. The
only loss mechanism in the nanocomposite is hysteresis losses at $>$100 kHz
frequencies, where the largest particles in the 11$\pm$3 nm distribution
transition from the superparamagnetic to blocked regime. To mitigate the
resulting hysteresis losses (up 10$^2$-10$^5$ kW/m$^3$) a more narrow particle
size distribution could be used for future materials. The presented material is
eddy current-free and easily integrated into micro-fabrication protocols, as we
demonstrate by fabrication of 3-turn print circuit board based inductors with
cast/manual printed nanocomposite inductor cores, on which induction has been
measured up to 100 MHz.

</details>


### [302] [Magnetic order dependent photoluminescence from high energy excitons in hBN protected few-layer CrSBr](https://arxiv.org/abs/2507.23301)
*Xiaohua Wu,Junyang Chen,Mingqiang Gu,Yujun Zhang,Shanmin Wang,Yanan Dai,Qihang Liu,Yue Zhao,Mingyuan Huang*

Main category: cond-mat.mes-hall

TL;DR: 少层CrSBr的光致发光性质与磁性密切相关，可用于探测其自旋构型，并为研究激子-磁性相互作用提供了理想平台。


<details>
  <summary>Details</summary>
Motivation: 为开发二维极限下的自旋电子器件，研究层状磁性半导体中的自旋构型检测与操控具有重要意义。

Method: 本研究通过光致发光（PL）光谱和第一性原理计算，系统研究了少层CrSBr中高能激子的发光特性及其在探测自旋构型方面的应用。

Result: 研究观察到了CrSBr中的Xh激子发光峰，并发现其能量位置和强度与样品的层间磁序密切相关，可用于探测自旋构型。此外，在施加垂直磁场时，在反铁磁和铁磁状态之间观察到了一个中间磁态。

Conclusion: 本研究揭示了少层CrSBr是研究激子-磁性相互作用的理想平台。

Abstract: The detection and manipulation of the spin configurations in layered magnetic
semiconductors hold significant interest for developing spintronic devices in
two-dimensional limit. In this letter, we report a systematical study on the
photoluminescence (PL) from the high energy excitons in few-layer CrSBr and its
application on detecting the spin configurations. Besides the broad excitonic
emission peak (Xl) at around 1.34 eV, we also observed another strong excitonic
emission peak (Xh) at around 1.37 eV in hBN encapsulated 2L sample, which
splits into two peaks in 3L and 4L samples. With help of the first principles
calculations, we conclude that the Xh peak is associated with the transition
between the top valence band and the second lowest conduction band, which is
forbidden by the inversion symmetry in 1L CrSBr. Furthermore, the position and
intensity of the Xh peak are strongly dependent on the interlayer magnetic
order of the CrSBr samples, which provides an efficient way to probe their spin
configurations. In addition, when the magnetic field is applied at the easy
axis direction, we resolve an intermediate magnetic state besides the
antiferromagnetic and ferromagnetic states in 3L and 4L samples. Our results
reveal few-layer CrSBr as an ideal platform to study the interaction between
the excitons and magnetism.

</details>


### [303] [Implementing Pseudofractal Designs in Graphene-Based Quantum Hall Arrays using Minkowski-Bouligand Algorithms](https://arxiv.org/abs/2507.23625)
*Dominick S. Scaletta,Ngoc Thanh Mai Tran,Marta Musso,Dean G. Jarrett,Heather M. Hill,Massimo Ortolano,David B. Newell,Albert F. Rigosi*

Main category: cond-mat.mes-hall

TL;DR: Pseudofractal analysis and star-mesh transformations are used to design graphene QHARS with optimized element counts and flexible resistance values, applicable to various quantum Hall systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to optimize the design of graphene-based quantized Hall array resistance standards (QHARS) by minimizing element count and enabling varying sizes of neighborhoods of available quantized resistance, leading to enhanced flexibility in achieving specific resistance values.

Method: This work introduces a pseudofractal analysis for optimizing high-resistance graphene-based quantized Hall array resistance standards (QHARS) using star-mesh transformations. Minkowski-Bouligand algorithms are employed to analyze the fractal dimensions of device design topologies, exploring three distinct partial recursion cases in addition to full recursion.

Result: Three distinct partial recursion cases, along with the full recursion design, were explored. Partial recursions, assessed through their fractal dimensions, offer enhanced flexibility in achieving specific resistance values within a desired neighborhood compared to full recursion methods, though they require an increased number of elements.

Conclusion: The pseudofractal analysis and star-mesh transformations provide a flexible approach to designing graphene-based quantized Hall array resistance standards (QHARS) with optimized element counts and tunable resistance values. The presented formalisms are material-independent and broadly applicable to other quantum Hall systems.

Abstract: This work introduces a pseudofractal analysis for optimizing high-resistance
graphene-based quantized Hall array resistance standards (QHARS). The
development of resistance standard device designs through star-mesh
transformations is detailed, aimed at minimizing element count. Building on a
recent mathematical framework, the approach presented herein refines QHARS
device concepts by considering designs incorporating pseudofractals (which may
be expressed as star-mesh transformations). To understand how future QHARS
pseudofractal designs enable varying sizes of neighborhoods of available
quantized resistance, Minkowski-Bouligand algorithms are used to analyze
fractal dimensions of the device design topologies. Three distinct partial
recursion cases are explored in addition to the original full recursion design,
and expressions for their total element counts are derived. These partial
recursions, assessed through their fractal dimensions, offer enhanced
flexibility in achieving specific resistance values within a desired
neighborhood compared to full recursion methods, albeit with an increased
number of required elements. The formalisms presented are material-independent,
making them broadly applicable to other quantum Hall systems and artifact
standards.

</details>


### [304] [Graphene-based quantum heterospin graphs](https://arxiv.org/abs/2507.23360)
*Gabriel Martínez-Carracedo,Amador García-Fuente,László Oroszlány,László Szunyogh,Jaime Ferrer*

Main category: cond-mat.mes-hall

TL;DR: 本文研究了基于磁性纳米石墨烯结构的低维开放量子自旋系统，发现了亚铁磁链和三腿自旋图的简并现象，并提出了实验实现方案。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索低维开放量子自旋系统，特别是基于磁性纳米石墨烯结构的系统，并计算其能量谱和量子数。

Method: 我们从第一性原理出发，研究了基于磁性纳米石墨烯结构的低维开放量子自旋系统，这些系统包含自旋1/2和自旋1三亚苯和/或奥运烯。这些石墨烯纳米结构表现为局域自旋，并可以用量子双二次海森堡哈密顿量来描述，我们计算了其能量谱和低能本征态的量子数。

Result: 我们计算了基于磁性纳米石墨烯结构的低维开放量子自旋系统的能量谱和低能本征态的量子数。我们提出了反铁磁交替自旋链的实验实现，它们表现为亚铁磁系统，其基态自旋和简并度取决于链的长度。此外，我们还确定了三腿自旋图（3-LSG）第一激发态中总自旋量子数S的双简并，这种简并取决于构成3-LSG的位点数量和自旋种类，并将其归因于哈密顿量的交换变换对称性。

Conclusion: 通过研究自旋1/2和自旋1三亚苯和/或奥运烯组成的磁性纳米石墨烯结构，我们提出了反铁磁交替自旋链的实验实现，它们表现为亚铁磁系统，其基态自旋和简并度取决于链的长度。我们还确定了三腿自旋图（3-LSG）第一激发态中总自旋量子数S的双简并，这种简并取决于构成3-LSG的位点数量和自旋种类。

Abstract: We investigate from first principles a variety of low-dimensional open
quantum spin systems based on magnetic nanographene structures that contain
spin-1/2 and spin-1 triangulenes and/or olympicenes. These graphene
nanostructures behave as localized spins and can be effectively described by a
quantum bilinear-biquadratic Heisenberg Hamiltonian, for which we will compute
the energy spectrum and the quantum numbers associated with the low-energy
eigenstates. We propose the experimental realization of antiferromagnetic
alternating spin chains using these graphene nanostructures, which result in
ferrimagnetic systems whose ground state spin and degeneracy depend on the
length of the chain. We also identify a double degeneracy in the total spin
quantum number $S$ in the first excited state for three-leg spin graphs
(3-LSGs). This degeneracy depends on both the number of sites and the spin
species that compose the 3-LSG. We identify the double degeneracy of the first
excited state as a consequence of swapping transformation symmetry of the
Hamiltonian.

</details>


### [305] [Influences of the Minkowski-Bouligand Dimension on Graphene-Based Quantum Hall Array Designs](https://arxiv.org/abs/2507.23630)
*Dominick S. Scaletta,Ngoc Thanh Mai Tran,Marta Musso,Valery Ortiz Jimenez,Heather M. Hill,Dean G. Jarrett,Massimo Ortolano,Curt A. Richter,David B. Newell,Albert F. Rigosi*

Main category: cond-mat.mes-hall

TL;DR: 本研究利用星网变换和递归方法优化量子化霍尔阵列电阻标准（QHARS）的设计，特别关注了高电阻和部分递归情况下的应用。


<details>
  <summary>Details</summary>
Motivation: 本工作阐述了如何通过使用星网变换来最小化元件数量，从而开发高电阻量子化霍尔阵列电阻标准（QHARS）。

Method: 本研究利用星网变换来最小化元件数量，并对最近开发的一个数学框架进行了改进，该框架通过和近似器件值与模拟和测量发现的精确有效量子化电阻进行协调，来优化 QHARS 器件设计。

Result: 与完全递归方法相比，部分递归提供了在特定邻域内访问所需电阻值的更大灵活性，但需要更多的器件。

Conclusion: 本研究通过研究分形维度来阐明完全递归和部分递归在 QHARS 设备中的优势，并对三种不同的部分递归情况进行了研究，以用于接近 1 GΩ 的 QHARS 设备。

Abstract: This work elaborates on how one may develop high-resistance quantized Hall
array resistance standards (QHARS) by using star-mesh transformations for
element count minimization. Refinements are made on a recently developed
mathematical framework optimizing QHARS device designs based on full, symmetric
recursion by reconciling approximate device values with exact effective
quantized resistances found by simulation and measurement. Furthermore, this
work explores the concept of fractal dimension, clarifying the benefits of both
full and partial recursions in QHARS devices. Three distinct partial recursion
cases are visited for a near-1 Gigaohm QHARS device. These partial recursions,
analyzed in the context of their fractal dimensions, offer increased
flexibility in accessing desired resistance values within a specific
neighborhood compared to full recursion methods, though at the cost of the
number of required devices.

</details>


### [306] [Multilayer Cryogenic Powder Filters with Low Parasitic Capacitance](https://arxiv.org/abs/2507.23388)
*Itishree Pradhan,Hao Li,Alina Rupp,Yosuke Sato,Henri Vo Van Qui,Miuko Tanaka,Toshiya Ideue,Erwann Bocquillon,Masayuki Hashisaka*

Main category: cond-mat.mes-hall

TL;DR: A new multilayer powder filter design reduces parasitic capacitance and improves RF signal attenuation, preventing sample heating in sensitive setups.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of conventional powder filters, which suffer from significant parasitic capacitance that compromises sensitive measurement setups.

Method: Development of a cryogenic powder filter with a multilayer design.

Result: The multilayer powder filter offers high attenuation of RF signals in the GHz range and minimized parasitic capacitance to ground, suppressing sample heating due to RF signal intrusion.

Conclusion: The multilayer powder filter effectively achieves both high RF attenuation and reduced parasitic capacitance, suppressing sample heating without degrading measurement setup performance.

Abstract: We report the development of a cryogenic powder filter that simultaneously
offers high attenuation of radio-frequency (RF) signals in the gigahertz (GHz)
range and minimized parasitic capacitance to ground. Conventional powder
filters, which consist of a signal line passing through a metal powder-filled
housing, attenuate high-frequency signals via the skin effect. However, these
designs often suffer from significant parasitic capacitance between the signal
line and the grounded chassis, which can compromise the performance of
sensitive measurement setups by limiting their frequency bandwidth. In this
work, we demonstrate that a multilayer powder filter design effectively
achieves both high RF attenuation and reduced parasitic capacitance. This
solution suppresses sample heating due to the unintentional intrusion of RF
signals through the wiring, without degrading the performance of the
measurement setup.

</details>


### [307] [Nonlinear Magnetoelectric Edelstein Effect](https://arxiv.org/abs/2507.23415)
*Jinxiong Jia,Longjun Xiang,Zhenhua Qiao,Jian Wang*

Main category: cond-mat.mes-hall

TL;DR: 提出了一种新的非线性磁电埃德尔施泰因效应，该效应能够在新兴的自旋电子学和反铁磁性检测中发挥重要作用。


<details>
  <summary>Details</summary>
Motivation: 为了在时间反演对称性系统（其中内禀的线性埃德尔施泰因效应和非线性埃德尔施泰因效应通常被禁止）中寻找新的自旋磁化机制。

Method: 通过双带狄拉克模型和蜂窝晶格上的紧束缚模型进行显式计算，以验证理论。

Result: 计算表明，非线性磁电埃德尔施泰因效应能够产生显著的自旋磁化，并且该效应的内禀和外禀部分都得到了证实。

Conclusion: 该研究提出了非线性磁电埃德尔施泰因效应，这是一种由电和磁场相互作用产生的新的自旋磁化机制。该效应的内禀部分即使在保持时间反演对称性但缺乏反演对称性的材料（包括绝缘体）中也可能存在，而其外禀部分可用于检测反铁磁材料中的纽尔矢量翻转。

Abstract: The linear Edelstein effect is a cornerstone phenomenon in spintronics that
describes the generation of spin magnetization in response to an applied
electric field. Recent theoretical advances have reignited interest in its
nonlinear counterpart, the nonlinear Edelstein effect, in which spin
magnetization is induced by a second-order electric field. However, the
intrinsic contribution to both effects is generally forbidden in systems
preserving time-reversal symmetry ($\mathcal{T}$) or composite symmetries such
as $\mathcal{T}\tau_{1/2}$, where $\tau_{1/2}$ denotes a half-lattice
translation. In such systems, spin magnetization typically emerges either from
extrinsic mechanisms but limited to metals due to their Fermi-surface property,
or from dynamical electric fields with a terahertz driving frequency. Here, we
propose a new mechanism for spin magnetization, arising from the interplay of
magnetic and electric fields, termed the nonlinear magnetoelectric Edelstein
effect. Remarkably, its intrinsic component, determined purely by the
material's band structure, can appear even in $\mathcal{T}$-invariant
materials, but lacking inversion symmetry ($\mathcal{P}$), including
insulators. On the other hand, we illustrate that its extrinsic component can
serve as a sensitive indicator of the N\'eel vector reversal in
$\mathcal{P}\mathcal{T}$-symmetric antiferromagnetic materials, offering a
novel route for antiferromagnetic order detection. To validate our theory, we
perform explicit calculations using a two-band Dirac model and a tight-binding
model on a honeycomb lattice, finding that both effects yield sizable spin
magnetization. Our findings establish the nonlinear magnetoelectric Edelstein
effect as a versatile platform for both exploring nonlinear spin physics and
enabling symmetry-based detection of antiferromagnetic order.

</details>


### [308] [Floquet Non-Bloch Formalism for a Non-Hermitian Ladder: From Theoretical Framework to Topolectrical Circuits](https://arxiv.org/abs/2507.23744)
*Koustav Roy,Dipendu Halder,Koustabh Gogoi,B. Tanatar,Saurabh Basu*

Main category: cond-mat.mes-hall

TL;DR: 本研究构建了一个广义Floquet非Bloch框架，用于分析周期性驱动的非厄米系统，揭示了皮肤效应的鲁棒性和增强现象，并通过一个TOC平台验证了理论的有效性。


<details>
  <summary>Details</summary>
Motivation: 在周期性驱动的非厄米系统中，获取能够恢复体-边对应关系的拓扑不变量是一个尚未解决的难题，而这对于理解这些超越传统厄米极限的拓扑相至关重要。

Method: 利用高频Magnus展开推导了有效的Floquet哈密顿量，并为周期性驱动的准一维系统（Creutz梯子，具有交错复势）构建了广义布里渊区。采用对称时间框架方法生成手性伴侣哈密顿量，并通过计算其不变量来解释边缘态结构。

Result: 研究发现，皮肤效应在高频驱动下保持鲁棒性（即使没有非互易跳变），并且在低频驱动下由于出现更长程的耦合而得到增强。提出的TOC平台成功复现了皮肤模式和Floquet边缘态。

Conclusion: 本研究提出了一个广义的Floquet非Bloch框架，用于分析时间周期性非厄米系统的谱和拓扑性质，并成功地将其应用于Creutz梯子模型，揭示了高频驱动下皮肤效应的鲁棒性以及低频驱动下皮肤效应的增强。通过对称时间框架方法生成的手性伴侣哈密顿量及其不变量能够完全解释边缘态结构。此外，研究提出了一个可行的拓扑电（TOC）实验平台，通过电压和阻抗剖面证实了理论框架的有效性，能够复现皮肤模式和Floquet边缘态。

Abstract: Periodically driven systems intertwined with non-Hermiticity opens a rich
arena for topological phases that transcend conventional Hermitian limits. The
physical significance of these phases hinges on obtaining the topological
invariants that restore the bulk-boundary correspondence, a task well explored
for static non-Hermitian (NH) systems, while it remains elusive for the driven
scenario. Here, we address this problem by constructing a generalized Floquet
non-Bloch framework that analytically captures the spectral and topological
properties of time-periodic NH systems. Em- ploying a high-frequency Magnus
expansion, we analytically derive an effective Floquet Hamiltonian and
formulate the generalized Brillouin zone for a periodically driven
quasi-one-dimensional system, namely, the Creutz ladder with a staggered
complex potential. Our study demonstrates that the skin effect remains robust
(despite the absence of non-reciprocal hopping) across a broad range of driving
parameters, and is notably amplified in the low-frequency regime due to
emergent longer- range couplings. We further employ a symmetric time frame
approach that generates chiral-partner Hamiltonians, whose invariants, when
appropriately combined, account for the full edge-state struc- ture. To
substantiate the theoretical framework, we propose a topolectrical circuit
(TEC) that serves as a viable experimental setting. Apart from capturing the
skin modes, the proposed TEC design faithfully reproduces the presence of
distinct Floquet edge states, as revealed through the voltage and impedance
profiles, respectively. Thus, our work not only offers a theoretical framework
for exploring NH-driven systems, but also provides an experimentally feasible
TEC architecture for realizing these phenomena stated above in a laboratory.

</details>


### [309] [Spintronic temperature nanosensor based on the resonance response of a skyrmion-hosting magnetic tunnel junction](https://arxiv.org/abs/2507.23430)
*Michail Lianeris,Davi Rodrigues,Andrea Meo,Dimitris Kechrakos,Anna Giordano,Mario Carpentieri,Giovanni Finocchio,Riccardo Tomasello*

Main category: cond-mat.mes-hall

TL;DR: 该研究提出使用基于斯格明子的自旋电子二极管作为纳米级热传感器，解决了传统传感器的局限性，并展示了高灵敏度和线性响应，在纳米电子热管理和传感器领域具有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 纳米电子学中对高效热管理的日益增长的需求，以及传统传感器在非线性响应、低灵敏度和复杂校准方面的不足。

Method: 通过预测现有基于斯格明子的自旋电子二极管的响应温度依赖性，并提出将其用作纳米级热传感器，利用拓扑保护的自旋纹理、稳健性、纳米级尺寸和低功耗动态特性。

Result: 证明了在宽范围内具有线性温度响应的高热灵敏度。幅度与频率的线性响应确保了精确可靠的温度测量。多层系统的使用进一步提高了器件的灵敏度和稳健性。

Conclusion: 该研究为基于斯格明子的热电器件奠定了基础，在自旋电子传感器、热管理、纳米电子学和斯格明子-热电子学等领域具有广阔的应用前景。

Abstract: The increasing need for efficient thermal management in nanoelectronics
requires innovative thermal sensing solutions, as conventional sensors often
exhibit nonlinear responses, low sensitivity, and complex calibration. We
predict a temperature dependence in the response of existing skyrmion based
spintronic diodes and propose their use as nanoscale thermal sensors. These
devices leverage magnetic skyrmions topologically protected spin textures known
for their robustness, nanoscale dimensions, and low power dynamics. We
demonstrate high thermal sensitivity with a linear temperature response over a
wide range. This linearity, observed in both the amplitude and frequency of the
skyrmion excitation, ensures redundancy that enables precise and reliable
temperature measurement. In addition, the use of multilayer systems enhances
the sensitivity and robustness of the device. These results provide a
foundation for skyrmion-based caloritronic devices with promising applications
in spintronic sensors, thermal management, nanoelectronics, and
skyrmion-caloritronics.

</details>


### [310] [Magnetically Programmable Surface Acoustic Wave Filters: Device Concept and Predictive Modeling](https://arxiv.org/abs/2507.23456)
*Michael K. Steinbauer,Peter Flauger,Matthias Küß,Stephan Glamsch,Emeline D. S. Nysten,Matthias Weiß,Dieter Suess,Hubert J. Krenner,Manfred Albrecht,Claas Abert*

Main category: cond-mat.mes-hall

TL;DR: A new device uses magnetic states to filter acoustic wave signals, showing potential for various applications.


<details>
  <summary>Details</summary>
Motivation: Filtering surface acoustic wave (SAW) signals of specified frequencies depending on the strength of an external magnetic field has potential scientific and industrial applications.

Method: Micromagnetic simulations were performed for the magnetoelastic interaction of the Rayleigh SAW mode with spin waves in exchange-decoupled Co/Ni islets on a LiTaO3 substrate. An energy conservation argument based on analytical solutions of the spin wave was extended to finite-difference numerical calculations for efficient simulation.

Result: A shift in the spin wave dispersion, dependent on the magnetic alignment of neighboring islets, was observed, significantly changing the efficiency of the magnetoelastic interaction at specified frequencies. A change in SAW transmission of 28.9 dB/mm at 3.8 GHz was predicted.

Conclusion: The proposed device achieves selective SAW attenuation by programming its internal magnetic state, offering significant changes in SAW transmission based on the magnetic alignment of neighboring islets.

Abstract: Filtering surface acoustic wave (SAW) signals of specified frequencies
depending on the strength of an external magnetic field in a magnetostrictive
material has garnered significant interest due to its potential scientific and
industrial applications. Here, we propose a device that achieves selective SAW
attenuation by instead programming its internal magnetic state. To this end, we
perform micromagnetic simulations for the magnetoelastic interaction of the
Rayleigh SAW mode with spin waves (SWs) in exchange-decoupled Co/Ni islets on a
piezoelectric LiTaO$_3$ substrate. Due to the islets exhibiting perpendicular
magnetic anisotropy, the stray-field interaction between them leads to a shift
in the SW dispersion depending on the magnetic alignment of neighboring islets.
This significantly changes the efficiency of the magnetoelastic interaction at
specified frequencies. We predict changes in SAW transmission of 28.9 dB/mm at
3.8 GHz depending on the state of the device. For the efficient simulation of
the device, we extend a prior energy conservation argument based on analytical
solutions of the SW to finite-difference numerical calculations, enabling the
modeling of arbitrary magnetization patterns like the proposed islet-based
design.

</details>


### [311] [Theory of ultrafast conductance modulation in electrochemical protonic synapses by multiphase polarization](https://arxiv.org/abs/2507.23576)
*Michael L. Li,Dingyu Shen,Jesus A. del Alamo,Martin Z. Bazant*

Main category: cond-mat.mes-hall

TL;DR: EIoS器件的电导调制速度比预期的要快，这是因为WO3通道的相分离现象，克服了扩散限制，实现了快速的线性电导调制。


<details>
  <summary>Details</summary>
Motivation: 为了理解质子EIoS器件中，钨氧化物（WO3）通道的电化学离子嵌入如何实现比离子扩散时间尺度快得多的线性电导调制（纳秒而非毫秒或秒），以及这种现象背后的机制，从而为改进器件性能提供指导。

Method: 本研究通过理论解释来阐明EIoS器件中线性与对称电导调制的产生机制，并结合实验对比分析WO3通道结晶度的变化对材料热力学的影响，揭示了相分离现象在其中起到的关键作用，并分析了电场极化和电子电导率对反应环境的控制。

Result: 研究发现，WO3通道的结晶度变化影响了材料的热力学，导致器件发生相分离。通过电场极化和高浓度丝线中电子电导率的增加，可以控制栅极的反应环境，从而在扩散限制区域内实现理想的电导调制，克服了传统扩散限制。

Conclusion: 本工作提出了一个理论解释，证明了线性与对称性来源于电解质-WO3界面的持续控制。通过对比过去的研究，发现WO3通道结晶度的变化影响了材料的热力学，并且实现纳秒脉冲时间尺度的器件发生了相分离。电场极化与高浓度丝线中电子电导率的增加耦合，可以控制栅极的反应环境，从而在扩散限制区域内实现理想的电导调制。这项工作强调了相分离系统克服EIoS性能的传统扩散势垒的潜力。

Abstract: Three-terminal electrochemical ionic synapses (EIoS) have recently attracted
interest for in-memory computing applications. These devices utilize
electrochemical ion intercalation to modulate the ion concentration in the
channel material. The electrical conductance, which is concentration dependent,
can be read separately and mapped to a non-volatile memory state. To compete
with other random access memory technologies, linear and symmetric conductance
modulation is often sought after, properties typically thought to be limited by
the slow ion diffusion timescale. A recent study by Onen et al.[1] examining
protonic EIoS with a tungsten oxide (WO3) channel revealed that this limiting
timescale seemed irrelevant, and linear conductance modulation was achieved
over nanosecond timescales, much faster than the bulk ion diffusion. This
contrasts with previous studies that have shown similar conductance modulation
with pulse timescales of milliseconds to seconds. Understanding the phenomena
behind these conductance modulation properties in EIoS systems remains a
crucial question gating technological improvements to these devices. Here, we
provide a theoretical explanation that demonstrates how linearity and symmetry
arise from consistent control over the electrolyte-WO3 interface. Comparing
these past works, changes in the WO3 channel crystallinity were identified,
affecting material thermodynamics and revealing that the device achieving
nanosecond pulse timescales underwent phase separation. Coupling of electric
field polarizatino and increased electron conductivity in high-concentration
filaments, the reaction environment at the gate electrode can be controlled,
resulting in ideal conductance modulation within the diffusion-limited regime.
This work highlights the potential for phase-separating systems to overcome the
traditional diffusion barriers that limit EIoS performance.

</details>


### [312] [Milli-Tesla Quantization enabled by Tuneable Coulomb Screening in Large-Angle Twisted Graphene](https://arxiv.org/abs/2507.23626)
*I. Babich,I. Reznikov,I. Begichev,A. E. Kazantsev,S. Slizovskiy,D. Baranov,M. Siskins,Z. Zhan,P. A. Pantaleon,M. Trushin,J. Zhao,S. Grebenchuk,K. S. Novoselov,K. Watanabe,T. Taniguchi,V. I. Falko,A. Principi,A. I. Berdyugin*

Main category: cond-mat.mes-hall

TL;DR: 通过在石墨烯器件中使用扭转角解耦的石墨烯层进行封装，可以显著提高器件的电子质量，减少电荷不均匀性，并实现新的量子现象。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有器件中由封装层中的带电缺陷引起的空间电荷波动问题，这些波动限制了器件的性能。

Method: 通过组装一种新型器件，其中石墨烯被其他石墨烯层封装，但通过大的扭转角（约10-30度）保持电子解耦。

Result: 该方法将相邻层的电荷不均匀性降低到每平方微米只有几个载流子，在低至约5毫特斯拉的磁场下观察到朗道量子化，并分辨出狄拉克点处的小能量隙。

Conclusion: 该器件的封装方法可以扩展到其他二维材料，为研究超纯器件的电子特性提供了新的途径。

Abstract: The electronic quality of graphene has improved significantly over the past
two decades, revealing novel phenomena. However, even state-of-the-art devices
exhibit substantial spatial charge fluctuations originating from charged
defects inside the encapsulating crystals, limiting their performance. Here, we
overcome this issue by assembling devices in which graphene is encapsulated by
other graphene layers while remaining electronically decoupled from them via a
large twist angle (~10-30{\deg}). Doping of the encapsulating graphene layer
introduces strong Coulomb screening, maximized by the sub-nanometer distance
between the layers, and reduces the inhomogeneity in the adjacent layer to just
a few carriers per square micrometre. The enhanced quality manifests in Landau
quantization emerging at magnetic fields as low as ~5 milli-Tesla and enables
resolution of a small energy gap at the Dirac point. Our encapsulation approach
can be extended to other two-dimensional systems, enabling further exploration
of the electronic properties of ultrapure devices.

</details>


### [313] [Current-induced spin-orbit torque on the surface of a transition metal dichalcogenide connected to a two-dimensional ferromagnet CrI$_3$: Effects of twisting and gating](https://arxiv.org/abs/2507.23655)
*Leyla Majidi,Azadeh Faridi,Reza Asgari*

Main category: cond-mat.mes-hall

TL;DR: TMDC/CrI$_3$ 双层材料中的 SOT 行为受掺杂、扭转角度和栅极电场调控。


<details>
  <summary>Details</summary>
Motivation: 近期在利用拓扑绝缘体和 TMDC 作为 SOT 的自旋源方面取得了进展，本研究旨在探索 TMDC/CrI$_3$ 双层系统中的 SOT 行为。

Method: 利用稳态玻尔兹曼方程研究了 TMDC（WSe$_2$ 或 MoSe$_2$）和 CrI$_3$ 双层材料在非线性响应下的电流引起的自旋极化和 SOT。

Result: 研究发现在 WSe$_2$/CrI$_3$ 系统中，带内跃迁产生强场类扭矩，带间跃迁产生较弱的阻尼类扭矩。n 型掺杂 MoSe$_2$ 可将阻尼类扭矩增强三个数量级，使其强度与场类扭矩相当。此外，扭转角度和横向栅极电场对 SOT 的大小和符号有显著影响。SOT 在 n 型和 p 型掺杂的 WSe$_2$ 和 MoSe$_2$ 系统中表现出强不对称性。10.16° 的扭转角度下，横向栅极电场可将 SOT 调整近一个数量级并改变其符号。

Conclusion: 该研究表明，TMDC/CrI$_3$ 双层材料中的电流引起的自旋极化和 SOT 表现出与线性响应外的电子行为一致的复杂行为。TMDC 的掺杂类型、扭转角度和栅极电场会显著影响场类和阻尼类扭矩的幅度和符号。

Abstract: Motivated by recent progress in employing two key classes of two-dimensional
materials-topological insulators and transition-metal dichalcogenides
(TMDCs)-as spin sources for generating spin-orbit torque (SOT), we investigate
current-induced spin polarization and the resulting SOT in bilayers composed of
a TMDC (WSe$_2$ or MoSe$_2$) and ferromagnetic chromium iodide (CrI$_3$),
beyond the linear response regime. Using the steady-state Boltzmann equation,
we find that intra-band transitions yield a strong field-like torque on the
CrI$_3$ layer, while inter-band transitions give rise to a comparatively weaker
damping-like torque in the WSe$_2$/CrI$_3$ system. Remarkably, the damping-like
component is enhanced by up to three orders of magnitude in n-doped MoSe$_2$,
reaching a strength comparable to the field-like torque, which itself is an
order of magnitude larger than that in the WSe$_2$-based bilayer. Both torque
components exhibit strong asymmetry between n-type and p-type doping in WSe$_2$
and MoSe$_2$ systems. Furthermore, we demonstrate that the twist angle plays a
crucial role: depending on the TMDC and chemical potential, twisting can
reverse the sign of the SOT and significantly modulate its magnitude. Finally,
we show that a transverse gate electric field enables substantial tunability of
the SOT, by nearly one order of magnitude, and induces a sign reversal at a
twist angle of $10.16^{\circ}$.

</details>


### [314] [Particle localization on helical nanoribbons: Quantum analog of the Coriolis effect](https://arxiv.org/abs/2507.23745)
*Radha Balakrishnan,Rossen Dandoloff,Victor Atanasov,Avadh Saxena*

Main category: cond-mat.mes-hall

TL;DR: 研究了螺旋纳米带上的量子输运现象，发现粒子在特定条件下会局域在纳米带的边缘或中心，这与科里奥利效应类似。研究还提出了利用纳米带几何形状产生类似霍尔效应的电压，以及通过周期性形变产生交流电压的方法，为纳米机电设备的设计提供了新的思路。


<details>
  <summary>Details</summary>
Motivation: 研究在法向和双法向螺旋纳米带表面上的粒子的局域态，以及表面几何形状引起的量子势。

Method: 推导了粒子限制在法向和双法向螺旋纳米带表面上的薛定谔方程，获得了由它们各自弯曲表面几何形状引起的量子势，并研究了每种带状线的粒子的局域态。

Result: 当粒子动量满足特定几何条件时，粒子会局域在法向纳米带的内边缘或双法向纳米带的中心螺旋上。这种现象可以被解释为科里奥利效应的量子类似物。研究还确定了两种纳米带的旋转参照系的量子化角速度。如果粒子是电子，其在特定宽度上的局域化会产生类似霍尔电压的电压差，但其来源是纳米带的弯曲表面几何形状而非外加磁场。当法向螺旋纳米带以周期性方式被机械翻转到双法向构型时，会产生周期性的电子从内边缘到中心的输运，从而产生量子交流电压。

Conclusion: 该研究结果可用于设计纳米机电设备，并且量子输运可以通过调整螺旋纳米带的弯曲和扭曲来控制，这在生物聚合物和纳米技术中具有多样化的应用前景。

Abstract: We derive the Schr\"odinger equation for a particle confined to the surface
of a normal and a binormal helical nanoribbon, obtain the quantum potentials
induced by their respective curved surface geometries, and study the localized
states of the particle for each ribbon. When the particle momentum satisfies a
certain geometric condition, the particle localizes near the inner edge for a
normal ribbon, and on the central helix for a binormal ribbon. This result
suggests the presence of a pseudo-force that pushes the particle transversely
along the width of the ribbon. We show that this phenomenon can be interpreted
as a quantum analog of the Coriolis effect, which causes a transverse
deflection of a classical particle moving in a rotating frame. We invoke
Ehrenfest's theorem applicable to localized states and identify the quantized
angular velocities of the rotating frames for the two ribbons. If the particle
is an electron, its localization at a specific width gives rise to a Hall-like
voltage difference across the ribbon's width. However, unlike in the Hall
effect, its origin is not an applied magnetic field, but the ribbon's curved
surface geometry. When a normal helical ribbon is mechanically flipped to a
binormal configuration in a periodic fashion, it results in a periodic electron
transport from the inner edge to the center, giving rise to a quantum AC
voltage. This can be used for designing nanoscale electromechanical devices.
Quantum transport on a helical nanoribbon can be controlled by tuning the bends
and twists of its surface, suggesting diverse applications in biopolymers and
nanotechnology.

</details>


### [315] [Projected branes as platforms for crystalline, superconducting, and higher-order topological phases](https://arxiv.org/abs/2507.23783)
*Archisman Panigrahi,Bitan Roy*

Main category: cond-mat.mes-hall

TL;DR: 研究了投影膜的拓扑特性，发现即使失去高维晶格的对称性，它们也能保留拓扑晶体绝缘体和拓扑超导体的特征，并讨论了实验实现的平台。


<details>
  <summary>Details</summary>
Motivation: 本项目旨在研究投影膜的拓扑特性，特别是即使在失去高维晶格对称性的情况下，它们如何保留原始拓扑相的特征。研究的动机在于探索投影膜在拓扑量子相中的作用，以及它们作为新型量子材料平台在实验上的可行性。

Method: 本文通过系统地积分掉父晶格中位于超平面之外的位点来构建投影膜的有效哈密顿量。从基于方形晶格的拓扑晶体绝缘体模型出发，研究了一维投影膜如何编码这些相的特征。接着，探讨了二维投影膜如何展现拓扑超导体的特征。最后，展示了一个基于方形晶格的二阶拓扑绝缘体如何在几何后代的一维投影膜中体现其角定位零模。

Result: 研究结果表明，一维投影膜可以编码拓扑晶体绝缘体的鲁棒端点零能模、量子化局域拓扑标记和中带隙模。二维投影膜可以展现拓扑超导体的马约拉纳零能模和量子化局域拓扑标记。二阶拓扑绝缘体的角定位零模也可以在投影膜中体现出来，具体表现为量子化局域化指数和端点零能模。

Conclusion: 本文研究了投影膜，一种由高维晶体的一小部分位点构成的结构，这些位点位于具有有理数或无理数斜率的超平面上。研究表明，即使投影膜本身不具备原始晶格的四重旋转对称性（$C_4$），它也能保留原始晶格的拓扑晶体绝缘体和拓扑超导体等拓扑相的特征。具体来说，一维投影膜表现出鲁棒的端点零能模、量子化的局域拓扑标记和绑定在位错缺陷上的中带隙模。二维投影膜则展现出强弱拓扑超导体的特征，如马约拉纳零能模。此外，研究还展示了二维二阶拓扑绝缘体如何通过其一维几何后代投影膜（准晶或晶体膜）体现其角定位零模的特征，前提是其中一个端点穿过父晶格的角点。最后，文章讨论了可用于实验验证这些理论提出的投影膜的量子和超材料平台。

Abstract: Projected branes are constituted by only a small subset of sites of a
higher-dimensional crystal, otherwise placed on a hyperplane oriented at an
irrational or a rational slope therein, for which the effective Hamiltonian is
constructed by systematically integrating out the sites of the parent lattice
that fall outside such branes [Commun. Phys. 5, 230 (2022)]. Specifically, when
such a brane is constructed from a square lattice, it gives rise to an
aperiodic Fibonacci quasi-crystal or its rational approximant in one dimension.
In this work, starting from square lattice-based models for topological
crystalline insulators, protected by the discrete four-fold rotational ($C_4$)
symmetry, we show that the resulting one-dimensional projected topological
branes encode all the salient signatures of such phases in terms of robust
endpoint zero-energy modes, quantized local topological markers, and mid-gap
modes bound to dislocation lattice defects, despite such linear branes being
devoid of the $C_4$ symmetry of the original lattice. Furthermore, we show that
such branes can also feature all the hallmarks of two-dimensional strong and
weak topological superconductors through Majorana zero-energy bound states
residing near their endpoints and at the core of dislocation lattice defects,
besides possessing suitable quantized local topological markers. Finally, we
showcase a successful incarnation of a square lattice-based second-order
topological insulator with the characteristic corner-localized zero modes in
its geometric descendant one-dimensional quasi-crystalline or crystalline
branes that feature a quantized localizer index and endpoint zero-energy modes
only when one of its end points passes through a corner of the parent crystal.
Possible designer quantum and meta material-based platforms to experimentally
harness our theoretically proposed topological branes are discussed.

</details>


### [316] [The persistence of spin coherence in a crystalline environment](https://arxiv.org/abs/2406.02703)
*Gerald Curran III,Zachary Rex,Casper X. Wilson,Luke J. Weaver,Ivan Biaggio*

Main category: cond-mat.mes-hall

TL;DR: TID效应能够抑制量子节拍，其影响程度与激子传输率和能量移动有关。


<details>
  <summary>Details</summary>
Motivation: 旨在理解并调控由单线态裂变生成的三线线态激子对中的量子干涉现象，特别是解释为何在某些条件下预期的荧光量子节拍会被抑制。

Method: 通过理论模型和实验相结合，分析了三线态激子对的量子干涉效应，并引入了传输诱导去相干（TID）这一关键机制。

Result: 在叶绿烷单晶中，即使三线态对保持纠缠超过50纳秒，TID效应也能在几纳秒内抑制量子节拍。研究还提供了叶绿烷分子在斜方晶格中的零场参数，并估算了其分子间传输率约为150皮秒。

Conclusion: 量子简并态在有机晶体材料中是可行的，并且能够通过控制激子在分子间非等价位点的传输行为来调控。

Abstract: We analyze quantum interference in the triplet-exciton pair generated by
singlet exciton fission in a molecular crystal, and introduce transport-induced
dephasing (TID) as a key effect that can suppress the expected fluorescence
quantum beats when the triplet-exciton wavefunction can localize on
inequivalent sites. TID depends on the triplet-exciton hopping rate between
inequivalent sites and on the energy-shifts among the stationary states of the
entangled triplet pair in different spatial configurations. The theoretical
model is confirmed by experiments in rubrene single crystals, where triplet
pairs remain entangled for more than 50 ns but quantum beats are suppressed by
TID within a few nanoseconds when the magnetic field is misaligned by just a
few degrees from specific symmetric directions. Our experiments deliver the
zero-field parameters for the rubrene molecule in its orthorhombic lattice and
information on triplet-exciton transport, in particular the triplet-exciton
hopping rate between inequivalent sites, which we evaluate to be of the order
of 150 ps in rubrene.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [317] [Competitive Bundle Trading](https://arxiv.org/abs/2507.23047)
*Yossi Azar,Niv Buchbinder,Roie Levin,Or Vardi*

Main category: cs.DS

TL;DR: 一个用于在线交易问题的算法，可以处理批量购买和销售，并具有对数竞争比。


<details>
  <summary>Details</summary>
Motivation: 研究零售商从供应商处批量购买商品，再批量销售给客户以实现利润最大化的交易问题，同时考虑了零售商的库存限制，并且供应商和客户是在线到达的。

Method: 设计了一个具有对数竞争比的算法，采用了指数权重更新动态定价方案。

Result: 该算法实现了对数竞争比，能够处理在线到达的供应商和客户，并考虑了库存限制。

Conclusion: 该算法实现了对偶拟合，并证明了（几乎）匹配的下界，还将结果扩展到了激励兼容机制。

Abstract: A retailer is purchasing goods in bundles from suppliers and then selling
these goods in bundles to customers; her goal is to maximize profit, which is
the revenue obtained from selling goods minus the cost of purchasing those
goods. In this paper, we study this general trading problem from the retailer's
perspective, where both suppliers and customers arrive online. The retailer has
inventory constraints on the number of goods from each type that she can store,
and she must decide upon arrival of each supplier/customer which goods to
buy/sell in order to maximize profit.
  We design an algorithm with logarithmic competitive ratio compared to an
optimal offline solution. We achieve this via an exponential-weight-update
dynamic pricing scheme, and our analysis dual fits the retailer's profit with
respect to a linear programming formulation upper bounding the optimal offline
profit. We prove (almost) matching lower bounds, and we also extend our result
to an incentive compatible mechanism. Prior to our work, algorithms for trading
bundles were known only for the special case of selling an initial inventory.

</details>


### [318] [Efficient algorithm for linear diophantine equations in two variables](https://arxiv.org/abs/2507.23216)
*Mayank Deora,Pinakpani Pal*

Main category: cs.DS

TL;DR: 本文提出了一种优化的 DEA-R 算法 DEA-OPTD 及其迭代版本 DEA-OPTDI，用于求解线性丢番图方程。实验证明，DEA-OPTDI 在至少 96% 的输入中优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 为了利用 DEA-R 算法的较少递归调用次数的优势，提出一种优化版本 DEA-OPTD，并进一步通过实验验证其优越性。

Method: 通过优化 DEA-R 算法，提出 DEA-OPTD 算法，并实现其迭代版本 DEA-OPTDI，然后与广泛使用的算法进行比较。

Result: DEA-OPTD 算法相比 DEA-R 算法，递归调用次数相等或更少。DEA-OPTDI 在特定输入设置下，相比广泛使用的算法，在至少 96% 的输入中表现更好。

Conclusion: DEA-OPTDI 算法在特定输入设置下，优于广泛使用的算法，在至少 96% 的输入中表现更好。

Abstract: Solving linear diophantine equations in two variables have applications in
computer science and mathematics. In this paper, we revisit an algorithm for
solving linear diophantine equations in two variables, which we refer as DEA-R
algorithm. The DEA-R algorithm always incurs equal or less number of recursions
or recursive calls as compared to extended euclidean algorithm. With the
objective of taking advantage of the less number of recursive calls , we
propose an optimized version of the DEA-R algorithm as DEA-OPTD. In the
recursive function calls in DEA-OPTD, we propose a sequence of more efficient
computations. We do a theoretical comparison of the execution times of DEA-OPTD
algorithm and DEA-R algorithm to find any possible bound on the value of $c$
for DEA-OPTD being better than DEA-R. We implement and compare an iterative
version of DEA-OPTD (DEA-OPTDI) with two versions of a widely used algorithm on
an specific input setting. In this comparison, we find out that our algorithm
outperforms on the other algorithm against atleast 96% of the inputs.

</details>


### [319] [Scalable contribution bounding to achieve privacy](https://arxiv.org/abs/2507.23432)
*Vincent Cohen-Addad,Alessandro Epasto,Jason Lee,Morteza Zadimoghaddam*

Main category: cs.DS

TL;DR: A new distributed algorithm efficiently handles contribution bounding for user-level differential privacy in large datasets by modeling ownership as a hypergraph and using a consensus mechanism for record inclusion.


<details>
  <summary>Details</summary>
Motivation: Existing sequential algorithms for contribution bounding in user-level differential privacy are computationally intensive and do not scale to massive datasets.

Method: Our approach models the user-record ownership structure as a hypergraph. The algorithm proceeds in rounds, allowing users to propose records in parallel. A record is added only if all its owners unanimously agree, ensuring contribution limits are not violated.

Result: The proposed method aims to maximize the size of the resulting dataset for high utility while providing a practical, scalable solution for implementing user-level privacy in large, real-world systems.

Conclusion: We propose a novel and efficient distributed algorithm for contribution bounding in user-level differential privacy. Our hypergraph-based approach addresses the scalability bottleneck of existing sequential algorithms and provides a practical solution for large-scale systems.

Abstract: In modern datasets, where single records can have multiple owners, enforcing
user-level differential privacy requires capping each user's total
contribution. This "contribution bounding" becomes a significant combinatorial
challenge. Existing sequential algorithms for this task are computationally
intensive and do not scale to the massive datasets prevalent today. To address
this scalability bottleneck, we propose a novel and efficient distributed
algorithm. Our approach models the complex ownership structure as a hypergraph,
where users are vertices and records are hyperedges. The algorithm proceeds in
rounds, allowing users to propose records in parallel. A record is added to the
final dataset only if all its owners unanimously agree, thereby ensuring that
no user's predefined contribution limit is violated. This method aims to
maximize the size of the resulting dataset for high utility while providing a
practical, scalable solution for implementing user-level privacy in large,
real-world systems.

</details>


### [320] [Nyldon Factorization of Thue-Morse Words and Fibonacci Words](https://arxiv.org/abs/2507.23659)
*Kaisei Kishi,Kazuki Kai,Yuto Nakashima,Shunsuke Inenaga,Hideo Bannai*

Main category: cs.DS

TL;DR: Nyldon因子化是受Lyndon因子化启发的组合对象。本文研究了Fibonacci词和Thue-Morse词的Nyldon因子化，并证明了无限Thue-Morse词存在Nyldon因子化。


<details>
  <summary>Details</summary>
Motivation: Nyldon因子化是受Lyndon词和Lyndon因子化启发的新定义的组合对象。

Method: 对有限Fibonacci词和Thue-Morse词的Nyldon因子化进行了完全表征，并研究了无限Thue-Morse词的Nyldon因子化。

Result: 完全表征了有限Fibonacci词和Thue-Morse词的Nyldon因子化，并证明了无限Thue-Morse词存在Nyldon因子化。

Conclusion: 我们证明了无限Thue-Morse词存在非递减的Nyldon词乘积。

Abstract: The Nyldon factorization is a string factorization that is a non-decreasing
product of Nyldon words. Nyldon words and Nyldon factorizations are recently
defined combinatorial objects inspired by the well-known Lyndon words and
Lyndon factorizations. In this paper, we investigate the Nyldon factorization
of several words. First, we fully characterize the Nyldon factorizations of the
(finite) Fibonacci and the (finite) Thue-Morse words. Moreover, we show that
there exists a non-decreasing product of Nyldon words that is a factorization
of the infinite Thue-Morse word.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [321] [Explanations for Unrealizability of Infinite-State Safety Shields](https://arxiv.org/abs/2507.23603)
*Andoni Rodriguez,Irfansha Shaik,Davide Corsi,Roy Fox,Cesar Sanchez*

Main category: cs.LO

TL;DR: Shielding in safe RL can fail if specifications are contradictory. This paper introduces a method using temporal formula unrolling to explain why these shields are unrealizable.


<details>
  <summary>Details</summary>
Motivation: Shielding is a popular method for safe reinforcement learning, but shields can be unrealizable due to inconsistent specifications, limiting its applicability in realistic scenarios.

Method: Temporal formula unrolling to provide explanations for unrealizable shields in infinite-state domains.

Result: The paper presents a method using temporal formula unrolling to obtain simple unconditional and conditional explanations that witness the unrealizability of shields.

Conclusion: Shielding method needs to be improved to handle unrealizable shields due to inconsistent specifications, and the proposed temporal formula unrolling technique provides explanations for unrealizability.

Abstract: Safe Reinforcement Learning focuses on developing optimal policies while
ensuring safety. A popular method to address such task is shielding, in which a
correct-by-construction safety component is synthesized from logical
specifications. Recently, shield synthesis has been extended to infinite-state
domains, such as continuous environments. This makes shielding more applicable
to realistic scenarios. However, often shields might be unrealizable because
the specification is inconsistent (e.g., contradictory). In order to address
this gap, we present a method to obtain simple unconditional and conditional
explanations that witness unrealizability, which goes by temporal formula
unrolling. In this paper, we show different variants of the technique and its
applicability.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [322] [Rethink Domain Generalization in Heterogeneous Sequence MRI Segmentation](https://arxiv.org/abs/2507.23110)
*Zheyuan Zhang,Linkai Peng,Wanying Dou,Cuiling Sun,Halil Ertugrul Aktas,Andrea M. Bejar,Elif Keles,Gorkem Durak,Ulas Bagci*

Main category: eess.IV

TL;DR: 该研究提出了PancreasDG数据集，用于胰腺分割的域泛化研究，并提出了一种新的半监督方法，在跨序列分割方面取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前的域泛化基准主要关注跨中心转移，而忽略了MR序列（T1和T2）在成像外观上的显著差异，而这些差异比采集中心的影响更大。胰腺分割因其在早期癌症检测、手术和糖尿病研究中的临床重要性而具有挑战性，但它在公共基准中代表性不足。

Method: 提出了一种利用解剖不变性的半监督方法，显著优于最先进的域泛化技术，在两个测试中心实现了61.63%的Dice分数提升和87.00%的准确率。

Result: 研究揭示了三个见解：(i)有限的采样会引入可能被误认为是分布转移的显著方差；(ii)对于相同的序列，跨中心表现与源域表现相关；(iii)跨序列转移需要专门的解决方案。

Conclusion: PancreasDG数据集为医学成像中的域泛化设立了新的基准，并在跨序列分割方面取得了显著的进步。

Abstract: Clinical magnetic-resonance (MR) protocols generate many T1 and T2 sequences
whose appearance differs more than the acquisition sites that produce them.
Existing domain-generalization benchmarks focus almost on cross-center shifts
and overlook this dominant source of variability. Pancreas segmentation remains
a major challenge in abdominal imaging: the gland is small, irregularly,
surrounded by organs and fat, and often suffers from low T1 contrast.
State-of-the-art deep networks that already achieve >90% Dice on the liver or
kidneys still miss 20-30% of the pancreas. The organ is also systematically
under-represented in public cross-domain benchmarks, despite its clinical
importance in early cancer detection, surgery, and diabetes research. To close
this gap, we present PancreasDG, a large-scale multi-center 3D MRI pancreas
segmentation dataset for investigating domain generalization in medical
imaging. The dataset comprises 563 MRI scans from six institutions, spanning
both venous phase and out-of-phase sequences, enabling study of both
cross-center and cross-sequence variations with pixel-accurate pancreas masks
created by a double-blind, two-pass protocol. Through comprehensive analysis,
we reveal three insights: (i) limited sampling introduces significant variance
that may be mistaken for distribution shifts, (ii) cross-center performance
correlates with source domain performance for identical sequences, and (iii)
cross-sequence shifts require specialized solutions. We also propose a
semi-supervised approach that leverages anatomical invariances, significantly
outperforming state-of-the-art domain generalization techniques with 61.63%
Dice score improvements and 87.00% on two test centers for cross-sequence
segmentation. PancreasDG sets a new benchmark for domain generalization in
medical imaging. Dataset, code, and models will be available at
https://pancreasdg.netlify.app.

</details>


### [323] [Towards High-Resolution Alignment and Super-Resolution of Multi-Sensor Satellite Imagery](https://arxiv.org/abs/2507.23150)
*Philip Wootaek Shin,Vishal Gaur,Rahul Ramachandran,Manil Maskey,Jack Sampson,Vijaykrishnan Narayanan,Sujit Roy*

Main category: eess.IV

TL;DR: 提出了一种利用 HLS10 作为参考来对齐和 Harmonized Landsat Sentinel 30m 影像的框架，以缩小传感器之间的分辨率差距，并提高超分辨率 Landsat 影像的质量。


<details>
  <summary>Details</summary>
Motivation: 高分辨率卫星影像对于地理空间分析至关重要，但不同卫星传感器之间的空间分辨率差异给数据融合和下游应用带来了挑战。现有超分辨率技术依赖于人工降尺度图像，且不适合具有不同光谱和时间特性的异构卫星传感器。

Method: 开发了一个对齐和 Harmonized Landsat Sentinel 30m (HLS 30) 影像的初步框架，使用 HLS 数据集中的 Harmonized Landsat Sentinel 10m (HLS10) 作为参考。

Result: 定量和定性评估证明了该方法的有效性，显示了其在增强卫星遥感应用方面的潜力。

Conclusion: 该研究为异构卫星图像超分辨率提供了见解，并强调了未来该领域发展的关键考虑因素。

Abstract: High-resolution satellite imagery is essential for geospatial analysis, yet
differences in spatial resolution across satellite sensors present challenges
for data fusion and downstream applications. Super-resolution techniques can
help bridge this gap, but existing methods rely on artificially downscaled
images rather than real sensor data and are not well suited for heterogeneous
satellite sensors with differing spectral, temporal characteristics. In this
work, we develop a preliminary framework to align and Harmonized Landsat
Sentinel 30m(HLS 30) imagery using Harmonized Landsat Sentinel 10m(HLS10) as a
reference from the HLS dataset. Our approach aims to bridge the resolution gap
between these sensors and improve the quality of super-resolved Landsat
imagery. Quantitative and qualitative evaluations demonstrate the effectiveness
of our method, showing its potential for enhancing satellite-based sensing
applications. This study provides insights into the feasibility of
heterogeneous satellite image super-resolution and highlights key
considerations for future advancements in the field.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [324] [Quantitative Nonlinear Optical Polarimetry with High Spatial Resolution](https://arxiv.org/abs/2507.23050)
*Albert Suceava,Sankalpa Hazra,Jadupati Nag,John Hayden,Safdar Imam,Zhiwen Liu,Abishek Iyer,Mercouri Kanatzidis,Susan Trolier-McKinstry,Jon-Paul Maria,Venkatraman Gopalan*

Main category: cond-mat.mtrl-sci

TL;DR: 该论文提出了一种新的方法，可以更准确地分析非线性光学显微镜下的材料特性，尤其是在使用高数值孔径聚焦时，能够揭示材料的垂直方向细节，并成功应用于铁电畴等材料的测量。


<details>
  <summary>Details</summary>
Motivation: 现有非线性光学显微镜技术在分析材料对称性和微观结构方面存在局限性，特别是对于各向异性晶体和薄膜，传统的低数值孔径（NA）聚焦分析方法无法充分揭示材料的垂直分量响应。因此，本研究旨在提出一种适用于高NA聚焦下的SHG极化测量定量分析方法，以克服现有技术的不足。

Method: 本研究提出了一种用于定量分析显微镜几何结构中高数值孔径（NA）聚焦下的SHG极化测量的方法。该方法通过实验和模拟，对单晶和薄膜等多种标准样品进行了分析，并与测量结果进行了比对，证明了其准确性。此外，还演示了求解逆问题的方案，即通过实验测量的极化数据来确定SHG张量的空间分布。

Result: 实验和模拟结果表明，该方法在分析单晶、薄膜等多种标准样品时，其测量和模拟的空间SHG图谱（包括铁电畴）吻合良好。研究成功演示了求解逆问题的能力，能够根据实验测量的极化数据确定SHG张量的空间分布，并提取垂直分量的非线性极化，为2D材料、薄膜、异质结构和具有强垂直响应的单轴晶体的高分辨率极化测量提供了有力支持。

Conclusion: 本研究提出了一种定量分析高数值孔径聚焦下单次谐波生成（SHG）极化测量的方法，并通过实验和模拟验证了其有效性，为高分辨率极化测量提供了新途径。

Abstract: Nonlinear optical microscopy such as in the optical second-harmonic
generation (SHG) modality has become a popular tool today for probing materials
in the physical and biological sciences. While imaging and spectroscopy are
widely used in the microscopy mode, nonlinear polarimetry, which can shed light
on materials' symmetry and microstructure, is relatively underdeveloped. This
is partly because quantitative analytical modeling of the optical SHG response
for anisotropic crystals and films largely assumes low-numerical aperture (NA)
focusing of light, where the plane-wave approximation is sufficient. Tight
focusing provides unique benefits in revealing out-of-plane polarization
responses, which cannot be detected by near-plane-wave illumination at normal
incidence. Here, we outline a method for quantitatively analyzing SHG
polarimetry measurements obtained under high-NA focusing within a microscope
geometry. Experiments and simulations of a variety of standard samples, from
single crystals to thin films, are in good agreement, including measured and
simulated spatial SHG maps of ferroelectric domains. A solution to the inverse
problem is demonstrated, where the spatial distribution of an SHG tensor with
unknown tensor coefficient magnitudes is determined by experimentally measured
polarimetry. The ability to extract the out-of-plane component of the nonlinear
polarization in normal incidence is demonstrated, which can be valuable for
high-resolution polarimetry of 2D materials, thin films, heterostructures, and
uniaxial crystals with a strong out-of-plane response.
  Copyright 2025 Optica Publishing Group. Users may use, reuse, and build upon
the article, or use the article for text or data mining, so long as such uses
are for non-commercial purposes and appropriate attribution is maintained. All
other rights are reserved. https://doi.org/10.1364/OPTICA.559060

</details>


### [325] [Realizing Nonreciprocal Linear Dichroism and Emission from Simple Media](https://arxiv.org/abs/2507.23051)
*Thomas J. Ugras,Daniel J. Gracias,Oriol Arteaga,Richard D. Robinson*

Main category: cond-mat.mtrl-sci

TL;DR: 通过利用手性-线性光学干涉，在易于加工、宏观对称的材料中实现了非互易线性二向色性和发射，为可扩展的光子控制提供了新的机会。


<details>
  <summary>Details</summary>
Motivation: 研究人员普遍认为，传统系统缺乏产生非互易行为的能力，因为在光学领域实现非互易性通常需要复杂 的超材料、奇异介质或强外部场。在这项工作中，我们通过重新审视手性和线性各向异性介质 的光-物质相互作用来挑战这一假设。

Method: 通过斯托克斯-穆勒形式主义推导出了预测非互易正交线性偏振吸收和发射途径的简单解析表达式，并使用具有相当大的圆二向色性（CD）和线性二向色性（LD）值的CdS、CdSe和CdTe幻数尺寸团簇的溶液加工薄膜进行了实验测试。

Result: 实验证明，CdS、CdSe和CdTe幻数尺寸团簇的溶液加工薄膜可以支持这种效应，表现出非互易吸收和发射线性偏振光。

Conclusion: 非互易线性二向色性和发射可以通过利用手性-线性光学干涉在易于加工、宏观对称的材料中实现。这项工作为可扩展、基于偏振的光子控制开辟了新的机会，用于方向相关光路由、光逻辑和偏振复用信息编码。

Abstract: Reciprocity, the principle that a system response is identical in the forward
path compared to the backward path, is a fundamental concept across physics,
from electrical circuits and optics to acoustics and heat conduction.
Nonreciprocity arises when this symmetry is broken, enabling
directional-dependent behavior. In photonics, nonreciprocity allows control
over the propagation of electromagnetic waves, essential for isolators and
circulators. But achieving optical nonreciprocity typically requires complex
metamaterials, exotic media, or strong external fields. Because of this,
researchers have historically overlooked the possibility that readily available
materials could support nonreciprocal optical behavior, assuming that
conventional systems lack the ability to produce nonreciprocal behavior. In
this work, we challenge that assumption by revisiting the light-matter
interactions of chiroptic and linearly anisotropic media. Through
Stokes-Mueller formalism we derive a simple analytical expression that predicts
a pathway to nonreciprocal absorption and emission of orthogonal linear
polarizations. We test this idea experimentally using solution-processed films
of CdS, CdSe, and CdTe magic-size clusters that possess commensurate circular
dichroism (CD) and linear dichroism (LD)values and find that they can support
this effect, engineering films that exhibit nonreciprocal absorption and
emission of linearly polarized light. Based on the derived expressions and
experiments, several design rules are presented. Our findings reveal that
nonreciprocal linear dichroism and emission can be achieved in readily
processable, macroscopically symmetric materials by harnessing chiral-linear
optical interference. This work opens new opportunities for scalable,
polarization-based photonic control for direction-dependent optical routing,
optical logic, and polarization-multiplexed information encoding.

</details>


### [326] [Local Inversion Symmetry Breaking and Thermodynamic Evidence for Ferrimagnetism in Fe3GaTe2](https://arxiv.org/abs/2507.23068)
*Sang-Eon Lee,Yue Li,Yeonkyu Lee,W. Kice Brown,PeiYu Cai,Jinyoung Yun,Chanyoung Lee,Alex Moon,Lingrui Mei,Jaeyong Kim,Yan Xin,Julie A. Borchers,Thomas W. Heitmann,Matthias Frontzek,William D. Ratcliff,Gregory T. McCandless,Julia Y. Chan,Elton J. G. Santos,Jeehoon Kim,Charudatta M. Phatak,Vadym Kulichenko,Luis Balicas*

Main category: cond-mat.mtrl-sci

TL;DR: Fe3GaTe2晶体在保持全局对称性的同时破坏了局域反演对称性，解释了Néel skyrmions的存在，并揭示了磁相与拓扑自旋纹理之间的关联。


<details>
  <summary>Details</summary>
Motivation: 解释中心对称材料中Néel skyrmions的存在，并研究Fe3GaTe2的磁相与拓扑自旋纹理之间的关系。

Method: 通过透射电子显微镜（TEM）和X射线衍射技术研究了Fe3GaTe2单晶的结构对称性；通过洛伦兹透射电子显微镜观察了Néel skyrmions；通过磁化测量研究了磁转变；通过中子衍射研究了磁构型变化；通过磁力显微镜观察了磁畴壁上的磁泡。

Result: Fe3GaTe2单晶破坏了局域反演对称性，但保持了全局反演对称性。观察到了Néel skyrmions，并提供了存在的结构解释。研究表明Fe3GaTe2的基态是全局亚铁磁性的。研究发现了磁滞回线和自旋纹理之间的相关性，表明拓扑自旋纹理受到亚铁磁性的影响。

Conclusion: Fe3GaTe2单晶在保持全局反演对称性的同时，破坏了局域反演对称性。该材料中的Néel skyrmions的存在可以用结构分析来解释。自旋结构与铁磁性以及亚铁磁性之间的竞争表明，其拓扑自旋纹理受到磁相的影响。

Abstract: The layered compound Fe3GaTe2 is attracting attention due to its high Curie
temperature, low dimensionality, and the presence of topological spin textures
above room temperature, making Fe$_3$GaTe$_2$ a good candidate for applications
in spintronics. Here, we show, through transmission electron microscopy (TEM)
techniques, that Fe$_3$GaTe$_2$ single crystals break local inversion symmetry
while maintaining global inversion symmetry according to X-ray diffraction.
Coupled to the observation of N\'{e}el skyrmions via Lorentz-TEM, our
structural analysis provides a convincing explanation for their presence in
centrosymmetric materials. Magnetization measurements as a function of the
temperature displays a sharp first-order thermodynamic phase-transition leading
to a reduction in the magnetic moment. This implies that the ground state of
Fe$_3$GaTe$_2$ is globally ferrimagnetic and not a glassy magnetic state
composed of ferrimagnetic, and ferromagnetic domains as previously claimed.
Neutron diffraction studies indicate that the ferromagnetic to ferrimagnetic
transition upon reducing the external magnetic field is associated with a
change in the magnetic configuration/coupling between Fe1 and Fe2 moments. We
observe a clear correlation between the hysteresis observed in both the
skyrmion density and the magnetization of Fe$_3$GaTe$_2$. This indicates that
its topological spin textures are affected by the development of ferrimagnetism
upon cooling. Observation, via magnetic force microscopy, of magnetic bubbles
at the magnetic phase boundary suggests skyrmions stabilized by the competition
among magnetic phases and distinct exchange interactions. Our study provides an
explanation for the observation of N\'eel skyrmions in centrosymmetric systems,
while exposing a correlation between the distinct magnetic phases of
Fe$_3$GaTe$_2$ and topological spin textures.

</details>


### [327] [Temperature Dependence of Angular Momentum Transport Across Interfaces](https://arxiv.org/abs/1604.01714)
*Kai Chen,Weiwei Lin,C. L. Chien,Shufeng Zhang*

Main category: cond-mat.mtrl-sci

TL;DR: 开发了一个微观理论来描述磁性多层结构中的角动量输运，计算结果与实验相符。


<details>
  <summary>Details</summary>
Motivation: 研究磁性多层结构中的角动量输运，这对自旋电子学物理和器件至关重要。

Method: 基于通用的界面交换相互作用，开发了一个微观理论，用于描述非磁性金属、铁磁和反铁磁绝缘体之间各种界面的界面自旋电导。

Result: 获得了不同自旋电池（包括自旋泵浦、温度梯度和自旋霍尔效应）的自旋电导及其温度依赖性。通过将该理论应用于铁磁绝缘体、反铁磁绝缘体和非磁性重金属组成的三层结构，计算得到的自旋电导的温度依赖性与现有实验结果定量符合。

Conclusion: 该理论为理解和计算磁性多层结构中的角动量输运提供了微观理论基础，其计算结果在温度依赖性方面与实验结果定量符合。

Abstract: Angular momentum transport in magnetic multilayered structures plays a
central role in spintronic physics and devices. The angular momentum currents
or spin currents are carried by either quasi-particles such as electrons and
magnons, or by macroscopic order parameters such as local magnetization of
ferromagnets. Based on the generic interface exchange interaction, we develop a
microscopic theory that describes interfacial spin conductance for various
interfaces among non-magnetic metals, ferromagnetic and antiferromagnetic
insulators. Spin conductance and its temperature dependence are obtained for
different spin batteries including spin pumping, temperature gradient and spin
Hall effect. As an application of our theory, we calculate the spin current in
a trilayer made of a ferromagnetic insulator, an antiferromagnetic insulator
and a non-magnetic heavy metal. The calculated results on the temperature
dependence of spin conductance quantitatively agree with the existing
experiments.

</details>


### [328] [Effect of RKKY and dipolar interaction on the nucleation of skyrmion in Pt/Co multilayer with Ir spacer](https://arxiv.org/abs/2507.23153)
*Shaktiranjan Mohanty,Brindaban Ojha,Bhuvneshwari Sharma,Ashutosh Rath,Chandrasekhar Murapaka,Subhankar Bedanta*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究通过在Pt/Co多层结构中引入Ir间隔层，利用偶极相互作用和RKKY相互作用成功稳定了磁斯格明子，并通过实验证实了其存在及其对Co层厚度的依赖性，为斯格明子的应用提供了新的思路。


<details>
  <summary>Details</summary>
Motivation: 磁斯格明子作为一种拓扑保护的自旋构型，在下一代自旋电子器件中展现出巨大的应用潜力。本研究旨在探索一种新的策略来稳定和控制磁斯格明子，特别是研究偶极相互作用和RKKY相互作用在其中所起的作用。

Method: 本研究利用磁力显微镜（MFM）成像和磁输运测量技术，研究了在Pt/Co多层体系中加入Ir间隔层所形成的合成反铁磁（SAF）结构中的斯格明子态的稳定性。通过改变Co层的厚度来观察其对磁各向异性、斯格明子大小和密度的影响。

Result: 研究发现，Pt/Co/Ir多层结构能够稳定磁斯格明子，并且该体系呈现出独特的斯格明子磁滞回线。磁力显微镜成像证实了孤立斯格明子的成核，而磁输运测量则揭示了有限的拓扑霍尔效应（THE），表明自旋构型的扭 chiral 性。此外，增加Co层的厚度会降低磁各向异性，导致形成更大、更密集的斯格明子。

Conclusion: 本研究通过结合偶极相互作用和RKKY相互作用，为稳定和控制磁斯格明子提供了鲁棒的方法，为斯格明子在自旋电子器件中的应用开辟了新的途径。

Abstract: Magnetic skyrmions, topologically protected spin textures, have emerged as
promising candidates for next-generation spintronic applications. In this
study, we investigate the stabilization of skyrmionic states in a uniquely
engineered Pt/Co multilayer system with an Ir spacer, where both Ruderman
Kittel Kasuya Yosida (RKKY) and dipolar interactions play a crucial role. The
studied multilayer structure consists of a synthetic antiferromagnetic (SAF)
configuration, where a single Ir layer facilitates strong antiferromagnetic
coupling between two ferromagnetic regions: FM1 (top) and FM2 (bottom), each
formed by repeated Co layers separated by Pt, enabling significant dipolar
interactions. This FM1/Ir/FM2 configuration results in a distinctive skyrmionic
hysteresis loop, driven by the interplay of dipolar and RKKY interactions.
Magnetic force microscopy (MFM) imaging confirms the nucleation of isolated
skyrmions, while magnetotransport measurements reveal a finite topological Hall
effect (THE), indicating the chiral nature of these spin textures. Furthermore,
we demonstrate that increasing the Co layer thickness leads to a reduction in
magnetic anisotropy, which in turn results in the formation of relatively
larger and denser skyrmions. Our findings establish a robust approach for
stabilizing skyrmions through the combined effects of dipolar and RKKY
interactions, offering new pathways for controlled skyrmion manipulation in
spintronic devices.

</details>


### [329] [Extended Factorization Machine Annealing for Rapid Discovery of Transparent Conducting Materials](https://arxiv.org/abs/2507.23160)
*Daisuke Makino,Tatsuya Goto,Yoshinori Suga*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究提出了一种结合因子分解机和退火的混合 الجزائر（FMA）框架，用于高效搜索新型透明导电材料（TCMs）。该方法通过一系列创新技术，在（AlxGayInz）2O3体系中实现了比现有算法更优的搜索速度和准确性，并成功用于多目标材料设计，为材料信息学开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 为了提高太阳能电池和显示器等下一代器件的性能并降低成本，开发新型透明导电材料（TCMs）至关重要。本研究旨在通过一种新颖的计算框架来加速和优化TCMs的搜索过程，特别是在（AlxGayInz）2O3这一具有潜力的材料体系中。

Method: 该研究提出的方法是一种混合 الجزائر（FMA）框架，结合了因子分解机（FM）和退火算法。具体技术包括：1. 连续变量二值化：将材料成分中的连续变量转化为二值变量，便于模型处理。2. Hopfield网络利用：利用Hopfield网络寻找和利用已有的优良解决方案。3. 自适应随机翻转：通过自适应的随机翻转操作激活全局搜索能力，避免陷入局部最优。4. 位串局部搜索：使用位串（bit-string）局部搜索进行微调，进一步优化候选材料。

Result: 所提出的FMA框架在（AlxGayInz）2O3数据集上的表现优于贝叶斯优化和遗传算法，能够实现更快、更准确的搜索。此外，该方法成功应用于多目标优化问题，能够同时考虑带隙和形成能来设计材料，证明了其在复杂材料设计中的潜力。

Conclusion: 该研究提出的基于因子分解机（FM）和退火的混合 الجزائر（FMA）框架，通过引入连续变量二值化、Hopfield网络利用、自适应随机翻转以及位串局部搜索等技术，在（AlxGayInz）2O3体系中实现了对具有高导电性和宽带隙材料的高效搜索。与贝叶斯优化和遗传算法相比，FMA在速度和准确性上均表现出优势，并成功应用于带隙和形成能的多目标优化，为材料信息学在复杂材料设计中的应用提供了新途径。

Abstract: The development of novel transparent conducting materials (TCMs) is essential
for enhancing the performance and reducing the cost of next-generation devices
such as solar cells and displays. In this research, we focus on the
(Al$_x$Ga$_y$In$_z$)$_2$O$_3$ system and extend the FMA framework, which
combines a Factorization Machine (FM) and annealing, to search for optimal
compositions and crystal structures with high accuracy and low cost. The
proposed method introduces (i) the binarization of continuous variables, (ii)
the utilization of good solutions using a Hopfield network, (iii) the
activation of global search through adaptive random flips, and (iv) fine-tuning
via a bit-string local search. Validation using the
(Al$_x$Ga$_y$In$_z$)$_2$O$_3$ data from the Kaggle "Nomad2018 Predicting
Transparent Conductors" competition demonstrated that our method achieves
faster and more accurate searches than Bayesian optimization and genetic
algorithms. Furthermore, its application to multi-objective optimization showed
its capability in designing materials by simultaneously considering both the
band gap and formation energy. These results suggest that applying our method
to larger, more complex search problems and diverse material designs that
reflect realistic experimental conditions is expected to contribute to the
further advancement of materials informatics.

</details>


### [330] [Low Temperature Formation of Crystalline VO2 Domains in Porous 1 Nanocolumnar Thin Films for Thermochromic Applications](https://arxiv.org/abs/2507.23393)
*H. Acosta-Rivera,V. Rico,F. J. Ferrer,T. C. Rojas,R. Alvarez,N. Martin,A. R. Gonzalez-Elipe,A. Palmero*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究在低于300°C的氧化过程中，研究了非晶VOx薄膜中VO2晶畴的形成，并优化了薄膜的成分和氧化条件，以提高其光学和电学性能，最终在特定条件下获得了近红外光调制能力接近50%且电阻率下降显著的薄膜。


<details>
  <summary>Details</summary>
Motivation: 研究在低温氧化过程中非晶VOx薄膜中VO2晶畴的形成机制及其对薄膜光学和电学性质的影响。

Method: 研究了在低于300°C的氧化过程中，非晶亚化学计量的纳米柱状VOx薄膜中VO2晶畴的形成。

Result: 氧/钒比值小于1.9时，在260°C即可形成VO2、V3O7和V2O5晶畴，并且薄膜在氧化过程中会发生溶胀。当氧/钒比值为1.5且氧化温度为280°C时，薄膜的近红外光调制能力接近50%，电阻率下降超过两个数量级。

Conclusion: 低于300°C的氧化过程会形成VO2晶畴。氧/钒比值大于1.9时，仅形成V2O5；而小于1.9时，在260°C即可形成VO2、V3O7和V2O5晶畴。氧化过程中，氧吸附和网络掺入会引起薄膜溶胀，从而使孔隙收缩。在特定条件下，低温氧化不仅能形成VO2畴，还能显著减少非晶VOx，提高薄膜的整体透明度和热致变色调制能力。当氧/钒比值为1.5且氧化温度为280°C时，薄膜性能最佳，近红外光的调制能力接近50%，电阻率下降超过两个数量级。

Abstract: The formation of VO2 crystalline domains in amorphous substoichiometric
nanocolumnar VOx thin films subjected to an oxidation process at temperatures
below 300{\deg}C has been studied. It is obtained that values of [O]/[V] above
1.9 lead to the sole formation of V2O5 after oxidation, while values below 1.9
favor the formation of VO2, V3O7 and V2O5 crystalline domains for temperatures
as low as 260{\deg}C. Moreover, it is found that the adsorption of oxygen and
its incorporation into the film network produce a relevant volume expansion in
a so-called swelling mechanism that makes pores shrink. Under some specific
conditions, the low temperature oxidation does not only trigger the formation
of VO2 domains but also a drastic reduction of oxygen-deficient amorphous VOx
in the films, which clearly improves the overall transparency and thermochromic
modulation capabilities. The changes in the optical and electrical properties
of these films during the metal-insulator transition have been studied, finding
the best performance when the stoichiometry of the film before oxidation is
[O]/[V]=1.5 and the oxidation temperature 280{\deg}C. These conditions yield a
relatively transparent coating that presents an optical modulation in the
near-infrared range of nearly 50% and a drop of electrical resistivity of more
than two orders of magnitude. A tentative model based on the volume increase
experienced by film upon oxidation is proposed to link the structural/chemical
features of the films and the formation of VO2 domains at such relatively low
temperatures.

</details>


### [331] [Giant odd-parity magnetoresistance from proximity-induced topological states](https://arxiv.org/abs/2507.23166)
*Tomoki Hotta,Le Duc Anh,Takahiro Chiba,Yohei Kota,Masaaki Tanaka*

Main category: cond-mat.mtrl-sci

TL;DR: 在α-Sn/(In,Fe)Sb异质结构中发现了高达1150%的巨大奇偶奇点磁阻效应，这可能在电子和自旋电子器件中有应用前景。


<details>
  <summary>Details</summary>
Motivation: 磁阻通常表现出关于磁场的偶对称性，但当时间反转对称性（TRS）被破坏时，可能会出现奇偶奇点磁阻（OMR）。然而，先前报道的OMR值相对较小，本研究旨在探索和发现更大的OMR效应。

Method: 通过Shubnikov de Haas振荡测量和ab initio计算，并使用Boltzmann输运模型来解释α-Sn/(In,Fe)Sb异质结构中的奇偶奇点磁阻效应。

Result: 在α-Sn/(In,Fe)Sb异质结构中，在1T的相对较低磁场下，观察到了高达1150%的巨大奇偶奇点磁阻效应。该效应可通过α-Sn能带结构中存在的倾斜狄拉克锥的Boltzmann输运模型得到解释。

Conclusion: 这项研究揭示了在α-Sn/(In,Fe)Sb异质结构中存在高达1150%的巨大奇偶奇点磁阻效应，这为开发超灵敏磁传感器等电子和自旋电子器件提供了新的方向。

Abstract: Magnetoresistance typically exhibits even symmetry with respect to the
magnetic field, owing to time reversal symmetry (TRS) as dictated by Onsager
reciprocity relations. However, in certain systems where TRS is broken,
magnetoresistance may acquire an odd component with respect to the magnetic
field, referred to as odd parity magnetoresistance (OMR). To date, reported OMR
values have been modest, usually restricted to a few tens of percent even under
high magnetic fields. Here, we report the discovery of a giant OMR reaching up
to 1,150% under a relatively low field of 1 T in a heterostructure composed of
3 nm thick alpha Sn and a ferromagnetic semiconductor, (In,Fe)Sb. Although
alpha Sn in this thickness range is a trivial narrow gap semiconductor,
analysis of Shubnikov de Haas oscillations combined with ab initio calculations
reveals the emergence of tilted topological surface states, induced via
magnetic proximity from the (In,Fe)Sb layer. The observed OMR behavior is well
explained by a Boltzmann transport model assuming the presence of oppositely
tilted Weyl cones in the alpha Sn band structure. Our findings not only shed
new light on the physics of OMR but also suggest promising avenues for its
application in electronic and spintronic devices, such as ultrasensitive
magnetic sensors.

</details>


### [332] [Tuning the Topological Properties of the Antiferromagnetic V(Bi$_{1-x}$Sb$_{x}$)$_{2}$Te$_{4}$ via Sb concentration](https://arxiv.org/abs/2507.23176)
*D. A-León,D. A. Landínez Téllez,J. Roa-Rojas,Rafael Gonzalez-Hernandez*

Main category: cond-mat.mtrl-sci

TL;DR: 通过调节Sb浓度，V(Bi$_{1-x}$Sb$_{x}$)$_{2}$Te$_{4}$材料的SHC和ASCN具有可调性，并且表现出强拓扑绝缘行为。


<details>
  <summary>Details</summary>
Motivation: 探索具有重要量子技术意义的磁性拓扑材料。

Method: 通过引入钒碲化物（VTe）层到层状拓扑绝缘体（Bi$_{1-x}$Sb$_{x}$）$_{2}$Te$_{3}$中来探索V(Bi$_{1-x}$Sb$_{x}$)$_{2}$Te$_{4}$反铁磁拓扑绝缘体。

Result: 结果显示通过Sb浓度可以调节自旋霍尔电导率（SHC）及其拓扑贡献（由最近引入的平均自旋陈数ASCN量化）。通过自旋-轨道耦合诱导的能带反转、非平凡Z2不变量和拓扑表面态，确立了材料的强拓扑绝缘行为。

Conclusion: V(Bi$_{1-x}$Sb$_{x}$)$_{2}$Te$_{4}$ ($x$=0, 0.5, 1) 是有潜力的自旋电子器件和先进量子应用的候选者。

Abstract: The investigation of topological materials has uncovered groundbreaking
phases of matter with significant implications for quantum technologies. Here,
we explore the antiferromagnetic topological insulator family
V(Bi$_{1-x}$Sb$_{x}$)$_{2}$Te$_{4}$ ($x$=$0$, $0.5$, $1$), formed by
introducing vanadium telluride (VTe) layers into the layered topological
insulator (Bi$_{1-x}$Sb$_{x}$)$_{2}$Te$_{3}$. Our results reveal the tunability
of the spin Hall conductivity (SHC) and its topological contribution,
quantified by the recently introduced average Spin Chern Number (ASCN), via Sb
concentration. The materials' strong topological insulating behavior is
established through spin-orbit coupling-induced band inversions, nontrivial
$\mathbb{Z}_2$ invariants, and the presence of topological surface states.
These findings position V(Bi$_{1-x}$Sb$_{x}$)$_{2}$Te$_{4}$ as promising
candidates for next-generation spintronic devices and advanced quantum
applications.

</details>


### [333] [Altermagnetism in 6H perovskites](https://arxiv.org/abs/2507.23232)
*S. V. Streltsov,S. -W. Cheong*

Main category: cond-mat.mtrl-sci

TL;DR: 六 H 钙钛矿可作为研究反演磁体的模型系统，具有非相对论自旋分裂能带和潜在的巨型压磁性。


<details>
  <summary>Details</summary>
Motivation: 研究具有中心对称晶体结构、局部结构交替和共线反铁磁性的材料，以探索由 PT 对称性破缺引起的非相对论自旋分裂能带。

Method: 通过实验研究六 H 钙钛矿（A3BB'2O9），探索其磁性。

Result: 发现了多种六 H 钙钛矿具有非相对论自旋分裂能带，证实了它们是反演磁体。

Conclusion: 六 H 钙钛矿是反演磁体的新兴类别，具有非相对论自旋分裂能带，并有可能表现出巨型压磁性。

Abstract: The combination of a centrosymmetric crystallographic structure with local
structural alternations and collinear antiferromagnetism can lead to broken PT
(Parity $\times$ Time-reversal) symmetry, resulting in altermagnets with
non-relativistic spin-split bands. The 6H perovskites with composition
A$_3$BB'$_2$O$_9$ exhibit unique layered structural alternations and typically
adopt an antiferromagnetic ground state. Here, we report the discovery that
several 6H perovskites are indeed altermagnets exhibiting non-relativistic
spin-split bands. We also explore the possible presence of net magnetization
due to spin-orbit coupling in these materials, as well as the manifestation of
giant piezomagnetism. Since the single crystals of 6H perovskites can be
readily grown and cleavable, our findings provide a new avenue to study the
cleaved atomically-flat surfaces of altermagnets with advanced experimental
techniques such as spin-resolved scanning tunneling microscopy (STM) or
spin-resolved angle-resolved photoemission spectroscopy (ARPES) to explore
their spin splitting nature.

</details>


### [334] [Disorder driven crossover between anomalous Hall regimes in Fe$_3$GaTe$_2$](https://arxiv.org/abs/2507.23243)
*Sang-Eon Lee,Minkyu Park,W. Kice Brown,Vadym Kulichenko,Yan Xin,S. H. Rhim,Chanyong Hwang,Jaeyong Kim,Gregory T. McCandless,Julia Y. Chan,Luis Balicas*

Main category: cond-mat.mtrl-sci

TL;DR: Fe3GaTe2晶体在高电导率下表现出不依赖于无序的显著反常霍尔电导率（AHC），而在低电导率下则存在一个与无序相关的标度关系。DFT计算表明Berry曲率主要来源于费米能级以下的Γ点附近。该材料清晰地展示了AHC区域间的交叉现象，为理解AHC提供了新的视角。


<details>
  <summary>Details</summary>
Motivation: Fe3(Ge,Ga)Te2化合物大的反常霍尔电导率（AHC）引起了广泛关注，本文旨在揭示Fe3GaTe2晶体中AHC的内在性质。

Method: 通过密度泛函理论（DFT）计算，揭示了Berry曲率的主要来源位于费米能级下方数百 meV处，靠近Γ点。

Result: 在高电导率下，AHC不依赖于无序，具有显著的σxy^c≈420 Ω⁻¹cm⁻¹。在低电导率下，观察到σxy ∝ σxx^1.6的标度关系，并随着σxx的增加而转变为σxy ≃ σxy^c。低电导率晶体中的无序通过铁磁性与亚铁磁基态之间一阶相变的展宽得到证实。

Conclusion: Fe3GaTe2晶体展现了由无序引起的不同反常霍尔效应（AHC）区域之间的交叉，这与之前从交叉区域两侧不同铁磁体测量中推断出的情况一致。

Abstract: The large anomalous Hall conductivity (AHC) of the Fe$_3$(Ge,Ga)Te$_2$
compounds has attracted considerable attention. Here, we expose the intrinsic
nature of AHC in Fe$_3$GaTe$_2$ crystals characterized by high conductivities,
which show disorder-independent AHC with a pronounced value
$\sigma_{xy}^{\text{c}}\approx$ 420 $\Omega^{-1}$cm$^{-1}$. In the low
conductivity regime, we observe the scaling relation
$\sigma_{xy}\propto\sigma_{xx}^{1.6}$, which crosses over to $\sigma_{xy}
\simeq \sigma_{xy}^{\text{c}}$ as $\sigma_{xx}$ increases. Disorder in
low-conductivity crystals is confirmed by the broadening of a first-order
transition between ferromagnetism and the ferrimagnetic ground state. Through
density functional theory (DFT) calculations, we reveal that the dominant
sources of Berry curvature are located a few hundred meV below the Fermi energy
around the $\Gamma$-point. Therefore, Fe$_3$GaTe$_2$ clearly exposes the
disorder-induced crossover among distinct AHC regimes, previously inferred from
measurements on different ferromagnets located in either side of the crossover
region.

</details>


### [335] [Boosting Photodetection via Plasmonic Coupling in Quasi-2D Mixed-n Ruddlesden-Popper Perovskite Nanostripes](https://arxiv.org/abs/2507.23727)
*Brindhu Malani S,Eugen Klein,Ronja Maria Piehler,Rostyslav Lesyuk,Christian Klinke*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究通过将功能化的银纳米结构阵列（ANA）与准二维钙钛纳米条纹结合，实现了光电探测性能的大幅提升，光电流增强了 838%，并提出了优化等离激元-激子耦合和非辐射能量转移以开发高性能等离激元-钙钛混合光电探测器的方法。


<details>
  <summary>Details</summary>
Motivation: 开发基于低维准二维金属卤化物钙钛纳米颗粒的高性能光电探测器仍然具有挑战性，因为存在量子和介电限制效应。等离激元纳米结构是一种有效的方法，可以通过共振能量转移和等离激元与激子的耦合来提高器件性能。

Method: 本研究采用胶体光刻技术制备了 ANA，并通过反射光谱和有限元方法（FEM）模拟研究了其局域表面等离振子共振（LSPR）模式。通过将 ODT 功能化的 ANA 引入准二维钙钛纳米条纹，并研究了等离激元与激子的耦合效应。

Result: ODT 功能化的 ANA 光电探测器表现出弱到中等的耦合，光电流增强因子达到 838%。其光响应度高达 70.41 mA/W，探测度为 1.48*10^11 Jones，外部量子效率为 21.55%，比参考光电探测器高约 10 倍。

Conclusion: 这项研究提出了一种通过引入辛硫醇（ODT）功能化的银纳米结构阵列（ANA）来增强准二维钙钛纳米条纹光电探测性能的方法。研究表明，ANA 的局域表面等离振子共振（LSPR）模式与钙钛的吸收和发射带在光谱上重叠，实现了等离激元与激子的耦合，从而将光电流增强因子提高了 838%。

Abstract: Quasi-2D metal halide perovskites have emerged as a promising material for
photodetection due to excellent optoelectronic properties, simple synthesis,
and robust stability. Albeit, developing high-performance photodetectors based
on low-dimensional quasi-2D metal halide perovskite nanoparticles remains
challenging due to quantum and dielectric confinement effects. Several
approaches have been employed to improve efficiency, with plasmonic
nanostructures being among the most effective ones. The resonant energy
transfer and coupling between plasmons and excitons play a vital role in
enhancing device performance. Here, we demonstrate enhanced photodetection of
quasi-2D perovskite nanostripes resulting from the incorporation of
octadecanethiol (ODT) functionalized Ag nanostructure arrays (ANA). Using
colloidal lithography, ANA were fabricated. Reflectance spectroscopy and finite
element method (FEM) simulations show that ANA supports localised surface
plasmon resonance (LSPR) modes that spectrally coincide with the absorption and
emission band of the perovskite. This spectral overlap enables interesting
coupling interactions between the excitons and plasmons. The ODT-functionalized
ANA photodetectors exhibit weak to intermediate coupling, resulting in a
photocurrent enhancement factor of 838 %. They achieve photoresponsivities of
up to 70.41 mA W^-1, detectivities of 1.48*10^11 Jones and external quantum
efficiencies of 21.55 %, which are approximately 10 times higher than those of
the reference photodetector. We present an approach to optimize the
plasmon-exciton coupling and non-radiative energy transfer for developing
high-performance plasmonic-perovskite hybrid photodetectors.

</details>


### [336] [Inhomogeneity identification by measuring magnetic quantum oscillations](https://arxiv.org/abs/2507.23246)
*Sang-Eon Lee,Myung-Hwa Jung*

Main category: cond-mat.mtrl-sci

TL;DR: 通过磁量子振荡分析掺杂NbSb2样品，识别材料不均匀性。均匀样品（Bi掺杂）呈对称分布，不均匀样品（Cr掺杂）呈非对称分布并违反Lifshitz-Kosevich公式。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是探索通过磁量子振荡分析来识别半金属NbSb2样品的不均匀性，并解决在拓扑材料研究中可能出现的对磁量子振荡异常相位的误解。

Method: 通过在半金属NbSb2中掺杂Bi和Cr，获得均匀的Bi掺杂样品和不均匀的Cr掺杂样品。通过比较样品研磨前后的磁量子振荡，确认了样品的均匀性。分析了均匀和不均匀样品中磁量子振荡的行为，并将其与不同的费米能级分布联系起来，阐述了Lifshitz-Kosevich公式在此中的应用和违反情况。

Result: 研究成功地通过磁量子振荡分析区分了均匀和不均匀的样品。均匀样品（Bi掺杂）表现出对称的有效费米能级分布，而不均匀样品（Cr掺杂）则表现出非对称分布，并违反了Lifshitz-Kosevich公式，这可能是由非均匀的费米能级引起的。

Conclusion: 这项研究通过分析磁量子振荡，为识别材料不均匀性提供了一种新颖的方法，并有助于减少在拓扑材料研究中对磁量子振荡异常相位的错误解释。

Abstract: This study explores the identification of sample inhomogeneity via magnetic
quantum oscillations analysis in semimetal NbSb$_2$. By doping Bi and Cr, we
obtained a homogeneous Bi-doped sample and an inhomogeneous Cr-doped sample,
whose homogeneity was confirmed by comparing the magnetic quantum oscillation
before and after grinding the samples. The magnetic quantum oscillations in the
inhomogeneous sample exhibited a distinct phase shift and unusual
field-dependent amplitude, believed to result from a non-uniform Fermi energy.
The analysis of the magnetic quantum oscillations demonstrated that the
homogeneous Bi-doped sample can be interpreted by the symmetric and Lorentzian
effective Fermi energy distribution, while the inhomogeneous Cr-doped sample
exhibited an asymmetric distribution, illustrating an unconventional violation
of the Lifshitz-Kosevich formula. This research provides a novel method for
identifying material inhomogeneity and mitigating potential misinterpretations
of magnetic quantum oscillations' unusual phase, commonly seen as a nontrivial
Berry phase indicator in topological materials studies.

</details>


### [337] [First-principles study of Rh- and Pd-based kagome-layered shandites](https://arxiv.org/abs/2507.23329)
*Luca Buiarelli,Turan Birol,Brian M. Andersen,Morten H. Christensen*

Main category: cond-mat.mtrl-sci

TL;DR: Shandite materials with kagome layers have electronic saddle points that can cause structural changes when moved closer to the Fermi level by pressure or doping. This effect is linked to how electronic smearing affects stability.


<details>
  <summary>Details</summary>
Motivation: The research aimed to understand the connection between electronic saddle points at specific momenta and structural instabilities in shandite materials, which feature transition metals in kagome layers stacked rhombohedrally.

Method: The study employed a combination of symmetry considerations and ab initio methods to investigate the electronic and structural properties of shandite materials. First-principles calculations were used to analyze the impact of pressure, doping, and electronic smearing temperature on structural stability.

Result: The parent shandite compounds were found to be structurally stable in the $Rar{3}m$ space group under ambient conditions. However, applying hydrostatic pressure or doping in specific compounds can induce structural instability by moving the saddle point closer to the Fermi level. Increasing electronic smearing temperature stabilizes the structure again.

Conclusion: Saddle points in the electronic structure of shandites are crucial for driving structural instabilities in rhombohedrally stacked kagome-layered materials with $Rar{3}m$ space group. Adjusting these saddle points through pressure or doping can induce instabilities, with the instability's strength correlating with electronic smearing temperature.

Abstract: The shandite structure hosts transition metals arranged in kagome layers
stacked rhombohedrally, and interspersed with post-transition metal ions and
chalcogens. The electronic states near the Fermi level are dominated by the
transition metal $d$-orbitals and feature saddle points near several of the
high-symmetry positions of the Brillouin zone, most notably the F and L points.
Combining symmetry considerations with ab initio methods, we study the
electronic and structural properties of these materials with an emphasis on the
connection between electronic saddle points at specific momenta and structural
instabilities at these momenta. While the parent compounds studied are all
found to be structurally stable in the $R\bar{3}m$ space group under ambient
conditions we show that, in specific compounds, moving the saddle point closer
to the Fermi level using either hydrostatic pressure or doping, can induce a
structural instability. The importance of the electronic degrees of freedom in
driving this instability is supported by the dependence of the frequency of the
soft phonon mode on the electronic smearing temperature, as is the case in
charge density wave materials. Our first-principles calculations show that as
the smearing temperature is increased, the compound becomes structurally stable
again. Our findings survey the structural properties of a large family of
shandite materials and shed light on the role played by saddle points in the
electronic structure in driving structural instabilities in rhombohedrally
stacked kagome-layered materials belonging to the $R\bar{3}m$ space group.

</details>


### [338] [Combinatorial Development of Amorphous/nanocrystalline Biphase Soft Magnetic Alloys with Silicon-steel like Saturated Magnetic Induction](https://arxiv.org/abs/2507.23333)
*Xuesong Li,Jing Zhou,Xiao Liu,Xibei Hou,Tengyu Guo,Bo Wu,Baoan Sun,Weihua Wang,Haiyang Bai*

Main category: cond-mat.mtrl-sci

TL;DR: 通过高通量MOKE筛选方法，发现Fe68.09Co17.02B10.9Si4合金具有超高Bs（2.02 T）和高电阻率，为开发下一代软磁材料提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 提高软磁合金的饱和磁感应强度（Bs）对于高功率密度电磁器件至关重要，但识别具有高Bs的合金成分通常依赖于实验室密集型熔铸方法，并且高通量磁性能表征仍然具有挑战性。

Method: 开发了一种基于高通量MOKE筛选方法的新组合方法，用于快速筛选具有最佳软磁性能的合金。

Result: 发现具有高Bs和低Hc组合的合金倾向于具有非晶-纳米晶双相微观结构的特征。确定了一种成分为Fe68.09Co17.02B10.9Si4的非晶/纳米晶合金薄膜，其Bs高达2.02 T，超过了迄今为止报道的所有非晶/纳米晶合金，并且可以与硅钢相媲美，同时还具有882 μΩ·cm的高电阻率，约为硅钢的17倍。

Conclusion: 我们开发了一种基于高通量MOKE筛选方法的新组合方法，用于快速筛选具有最佳软磁性能的合金。我们发现具有高Bs和低Hc组合的合金倾向于具有非晶-纳米晶双相微观结构的特征。我们还确定了一种成分为Fe68.09Co17.02B10.9Si4的非晶/纳米晶合金薄膜，其Bs高达2.02 T，超过了迄今为止报道的所有非晶/纳米晶合金，并且可以与硅钢相媲美，同时还具有882 μΩ·cm的高电阻率，约为硅钢的17倍。我们的高通量磁筛选方法为理解微观结构和磁性能之间的关系以及开发下一代软磁材料提供了范例。

Abstract: Maximization saturation magnetic induction (Bs) of soft magnetic alloys is
essential for the high power-density electromagnetic devices. However,
identifying the alloy compositions with high Bs often replies on the
lab-intensive melt casting method and a high-throughput characterization on
magnetic properties remains challenging. Here, we develop a new combinatorial
method for fast screening alloys with optimal soft magnetic properties based on
the high-throughput MOKE screening method. Based on the combinatorial method,
we found that the alloys with a combination of high Bs and low coercivity (Hc)
tend to have a feature of amorphous-nanocrystalline biphase microstructure. We
also identified an amorphous/nanocrystalline alloy film with the composition
the Fe68.09Co17.02B10.9Si4, exhibiting an ultra-high Bs up to 2.02 T that
surpasses all amorphous/nanocrystalline alloys reported so far and is
comparable to that of silicon steels, together with a high resistivity of 882
{\mu}{\Omega} {\dot} cm, about 17 times of silicon steels. Our high-throughput
magnetic screening method provides a paradigm for understanding the
relationship between microstructure and magnetic properties and the development
of the next-generation soft magnetic materials.

</details>


### [339] [Terahertz spin-orbit torque as a drive of spin dynamics in insulating antiferromagnet Cr$_{2}$O$_{3}$](https://arxiv.org/abs/2507.23367)
*R. M. Dubrovin,Z. V. Gareeva,A. V. Kimel,A. K. Zvezdin*

Main category: cond-mat.mtrl-sci

TL;DR: Spin dynamics induced by current can occur in magnetic insulators like Cr2O3, enabling ultrafast control of antiferromagnetic order through THz electric fields, contrary to previous beliefs.


<details>
  <summary>Details</summary>
Motivation: To theoretically predict and reveal that spin dynamics induced by current can be realized in magnetic insulators, specifically in the magnetoelectric antiferromagnet Cr2O3, contrary to conventional wisdom.

Method: The study combines symmetry analysis with a Lagrangian approach, introducing an alternative electric dipole order parameter arising from the dipole moment at Cr3+ sites to uncover the coupling mechanism.

Result: The displacement current driven by the THz electric field generates a N´eel spin-orbit torque in this insulating system, enabling ultrafast control of antiferromagnetic order by coupling to the antiferromagnetic spins. This effect competes with the linear magnetoelectric response, offering a novel manipulation pathway.

Conclusion: The study establishes insulator antiferromagnets as a viable platform for electric field driven antiferromagnetic spintronics and provides general design principles for non-metallic spin-orbit torque materials.

Abstract: Contrary to conventional wisdom that spin dynamics induced by current are
exclusive to metallic magnets, we theoretically predict that such phenomena can
also be realized in magnetic insulators, specifically in the magnetoelectric
antiferromagnet $\mathrm{Cr}_{2}\mathrm{O}_{3}$. We reveal that the
displacement current driven by the THz electric field is able to generate a
N{\'e}el spin-orbit torque in this insulating system. By introducing an
alternative electric dipole order parameter arising from the dipole moment at
$\mathrm{Cr}^{3+}$ sites, we combine symmetry analysis with a Lagrangian
approach and uncover that the displacement current couples to the
antiferromagnetic spins and enables ultrafast control of antiferromagnetic
order. The derived equations of motion show that this effect competes with the
linear magnetoelectric response, offering a novel pathway for manipulating
antiferromagnetic order in insulators. Our findings establish insulator
antiferromagnets as a viable platform for electric field driven
antiferromagnetic spintronics and provide general design principles for
non-metallic spin-orbit torque materials.

</details>


### [340] [Machine learning Landau free energy potentials](https://arxiv.org/abs/2507.23369)
*Mauro Pulzone,Natalya S. Fedorova,Hugo Aramberri,Jorge Íñiguez-González*

Main category: cond-mat.mtrl-sci

TL;DR: 利用机器学习方法，基于实验可获取的数据，成功构建了PbTiO3的朗道自由能势模型，该模型能准确描述材料行为并捕捉弹性应变对铁电性的影响。


<details>
  <summary>Details</summary>
Motivation: 为了探索一种新的方法来构建朗道自由能势，特别是利用机器学习和实验可获取的数据来描述铁电材料（如PbTiO3）的行为，并理解弹性应变如何影响铁电特性。

Method: 提出了一种机器学习方法，用于构建朗道自由能势。该方法利用实验可获取的数据（温度依赖的极化和应变），识别最优多项式模型，并特别关注了模型验证的步骤，与通常的机器学习模型构建方法进行了比较。

Result: 研究成功地构建了PbTiO3的朗道自由能势模型，发现一个简单的多项式模型（仅两个参数线性依赖于温度）足以准确描述其行为。该模型还能捕捉弹性应变对铁电性的影响，如极性相的对称性和相变的非连续性。

Conclusion: 该研究表明，利用机器学习方法可以构建朗道自由能势。通过对PbTiO3进行研究，并使用实验可获取的数据（如温度依赖的极化和应变），成功识别了最优多项式模型，该模型能准确描述材料行为，并捕捉弹性应变对铁电特性的微妙耦合影响。所提出的“第三原理”模型具有预测能力，可用于铁电体和其他经历非重构结构转变的材料的宏观或介观模拟。

Abstract: We show how to construct Landau-like free energy potentials using a
machine-learning approach. For concreteness, we focus on perovskite oxide
PbTiO$_{3}$. We work with a training set obtained from Monte Carlo simulations
based on an atomistic ''second-principles'' potential for PbTiO$_{3}$. We rely
exclusively on data that would be experimentally accessible -- i.e.,
temperature-dependent polarization and strain, both with and without external
electric fields and stresses applied --, to explore scenarios where the
training set could be obtained from laboratory measurements. We introduce a
scheme that allows us to identify optimal polynomial models of the
temperature-dependent free energy surface, mapped as a function of the
homogeneous electric polarization and homogeneous strain. Our results for
PbTiO$_{3}$ show that a very simple polynomial -- where only two parameters
depend linearly on temperature -- is sufficient to yield a correct description
of the material's behavior. Remarkably, the obtained models also capture the
subtle couplings by which elastic strain controls key features of
ferroelectricity in PbTiO$_{3}$ -- i.e., the symmetry of the polar phase and
the discontinuous character of the transition --, despite the fact that no
effort was made to include such information in the training set. We emphasize
the distinctive aspects of our methodology (which relies on an original form of
validation step) by comparing it with the usual machine-learning approach for
model construction. Our results illustrate how physically motivated models can
have remarkable predictive power, even if they are derived from a limited
amount of data. We argue that such ''third-principles'' models can be the basis
for predictive macroscopic or mesoscopic simulations of ferroelectrics and
other materials undergoing non-reconstructive structural transitions.

</details>


### [341] [Enhanced negative capacitance in La-doped Pb(Zr$_{0.4}$Ti$_{0.6}$)O$_3$ ferroelectric capacitor by tuning bias voltage pulse to induce intrinsic domain switching kinetics](https://arxiv.org/abs/2507.23448)
*Ganga S. Kumar,Sudipta Goswami,Shubhasree Chatterjee,Dilruba Hasina,Miral Verma,Devajyoti Mukherjee,Chandan Kumar Ghosh,Dipten Bhattacharya*

Main category: cond-mat.mtrl-sci

TL;DR: 多畴铁电电容器的负电容增强与电压脉冲和畴开关动力学密切相关。


<details>
  <summary>Details</summary>
Motivation: 探索多畴铁电电容器在特定偏置电压脉冲下出现负电容现象的原因，以及偏置电压脉冲剖面、畴壁密度、铁电迟滞回线形状和瞬态负电容之间的关系。

Method: 本研究利用了偏置电压脉冲、畴动力学分析和时间依赖性 Ginzburg-Landau 方程的相场模拟。

Result: 研究发现，当偏置电压脉冲剖面（幅度与时间尺度）诱导的铁电畴开关遵循与最小能量势垒相关的内在开关动力学时，多畴铁电电容器中的特定负电容会得到显著增强。这归因于畴开关过程中出现了最大的畴壁密度。相场模拟结果表明，畴壁密度在开关过程中依赖于偏置电压幅度，并在特定偏置电压幅度下达到最大值。此外，铁电迟滞回线在强制电压下的曲率半径也取决于是否遵循内在开关动力学。

Conclusion: 该研究揭示了多畴镧掺杂铁电电容器中负电容增强的机制，强调了偏置电压脉冲剖面（幅度与时间尺度）与畴壁密度和铁电迟滞回线形状之间的密切相关性，并指出这可能对负电容的物理学和相关器件的发展具有重要意义。

Abstract: We report a remarkable enhancement of specific negative capacitance in
multidomain La-doped Pb(Zr$_{0.4}$Ti$_{0.6}$)O$_3$ (PLZT) ferroelectric
capacitors when bias voltage pulse profile (amplitude and timescale) induces
switching of the ferroelectric domains following intrinsic switching kinetics
associated with minimum energy barrier. This is because of emergence of maximum
domain wall density during ``switching" of the domains. Domain configuration
changes from such an ``optimum" state if higher or lower bias voltage is
applied at a much faster or slower rate. Phase-field simulation using
time-dependent Ginzburg-Landau equation shows dependence of the domain wall
density during switching on the bias voltage amplitude and its maximization at
a specific bias voltage amplitude. The radius of curvature of the resulting
polarization ($P$) versus voltage ($V$) hysteresis loop at the coercive voltage
($V_C$) also turns out to be depending on whether or not intrinsic switching
kinetics is followed. All these results indicate a close correlation among the
bias voltage pulse profile (amplitude and time scale), domain wall density
during switching, shape of the resulting ferroelectric hysteresis loop, and the
transient negative capacitance. It may have important ramifications both in the
context of physics behind negative capacitance in a multidomain ferroelectric
capacitor and devices being developed by exploiting its advantages.

</details>


### [342] [Magnetic and magnetocaloric properties of the amorphous Tb$_{31}$Co$_{69}$ and Dy$_{31}$Co$_{69}$ thin films deposited on Si substrates](https://arxiv.org/abs/2507.23555)
*P. Skokowski,M. Matczak,Ł. Frąckowiak,T. Bednarchuk,M. Kowacz,B. Anastaziak,K. Synoradzki*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究制备了Tb$_{31}$Co$_{69}$ 和 Dy$_{31}$Co$_{69}$ 非晶薄膜，并分析了它们的结构、磁性和磁热性质。结果显示Dy$_{31}$Co$_{69}$ 薄膜在特定温度下具有较大的磁熵变，表明其在磁制冷方面具有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 研究Tb-Co和Dy-Co非晶薄膜的结构、磁性和磁热性质，旨在探索其在磁制冷领域的应用潜力。

Method: 采用脉冲激光沉积技术制备了Tb$_{31}$Co$_{69}$ 和 Dy$_{31}$Co$_{69}$ 非晶薄膜，并进行了X射线衍射、磁化强度-温度关系以及磁热效应的测试分析。

Result: X射线衍射分析显示材料中存在结晶的Laves相。磁化强度测量表明，两种薄膜均表现出亚铁磁性。计算得出Tb$_{31}$Co$_{69}$非晶相的补偿温度为81.5 K，Dy$_{31}$Co$_{69}$非晶相的补偿温度为88.5 K；结晶Laves相的居里温度分别为204.5 K (Tb$_{31}$Co$_{69}$) 和117 K (Dy$_{31}$Co$_{69}$)。在5 T磁场变化下，Tb$_{31}$Co$_{69}$在补偿温度下的磁熵变为-4.9 mJ cm$^{-3}$ K$^{-1}$，在Laves相居里温度下的磁熵变为-6.6 mJ cm$^{-3}$ K$^{-1}$；Dy$_{31}$Co$_{69}$在补偿温度下的磁熵变为-35 mJ cm$^{-3}$ K$^{-1}$，在Laves相居里温度下的磁熵变为-28 mJ cm$^{-3}$ K$^{-1}$。

Conclusion: 研究表明，Tb$_{31}$Co$_{69}$ 和 Dy$_{31}$Co$_{69}$ 非晶薄膜具有铁磁性，并表现出磁热效应，其中 Dy$_{31}$Co$_{69}$ 在补偿温度和 Laves 相居里温度下的磁熵变值较高。

Abstract: We present the structural, magnetic, and magnetocaloric properties of
amorphous thin films Tb-Co and Dy-Co with stoichiometry Tb$_{31}$Co$_{69}$ and
Dy$_{31}$Co$_{69}$, deposited on naturally oxidized silicon Si (100)
substrates. Samples with a thickness $d=50$ nm covered with a protective Au
overlayer with a thickness $d_{\rm Au} =5 $ nm were produced using the pulsed
laser deposition technique. The X-ray diffraction analysis indicated the
presence of a crystallized Laves phase in the prepared materials. Magnetization
measurements as a function of temperature revealed ferrimagnetic behavior in
both samples. We estimated the compensation temperature $T_{\rm comp}$ of the
amorphous phase for Tb$_{31}$Co$_{69}$ at 81.5 K and for Dy$_{31}$Co$_{69}$ at
88.5 K, while we found the Curie temperature $T_{\rm C,\ Laves}$ of the
crystallized Laves phases at 204.5 K and at 117 K, respectively. We
investigated the magnetocaloric effect in a wide temperature range, covering
$T_{\rm comp}$ of amorphous phases and $T_{\rm C,\ Laves}$ of crystallized
Laves phases. The analysis for the magnetic field change of $\Delta \mu_0H=5$ T
showed values of the magnetic entropy change of $-\Delta S_{\rm M}=4.9$ mJ
cm$^{-3}$ K$^{-1}$ at $T_{\rm comp}$ and $-\Delta S_{\rm M}=6.6$ mJ cm$^{-3}$
K$^{-1}$ at $T_{\rm C,\ Laves}$ for Tb$_{31}$Co$_{69}$, while for
Dy$_{31}$Co$_{69}$, we determined the values of $-\Delta S_{\rm M}=35$ mJ
cm$^{-3}$ K$^{-1}$ at $T_{\rm comp}$ and $-\Delta S_{\rm M}=28$ mJ cm$^{-3}$
K$^{-1}$ at $T_{\rm C,\ Laves}$.

</details>


### [343] [Temperature-dependent Photoluminescence and Raman Spectroscopy of Selenium Thin Film Solar Cells](https://arxiv.org/abs/2507.23647)
*Rasmus S. Nielsen,Axel G. Medaille,Arnau Torrens,Oriol Segura-Blanch,Seán R. Kavanagh,David O. Scanlon,Aron Walsh,Edgardo Saucedo,Marcel Placidi,Mirjana Dimitrievska*

Main category: cond-mat.mtrl-sci

TL;DR: 硒薄膜质量对其加工工艺非常敏感，通过优化工艺可以改善其光电性能。


<details>
  <summary>Details</summary>
Motivation: 硒作为一种具有吸引力的宽带隙元素半导体，在光电和能源领域具有应用潜力。然而，其高挥发性和低辐射效率阻碍了对其结构和光电质量的评估，因此需要先进的非破坏性表征方法。

Method: 采用闭路封装策略，通过温度相关的拉曼光谱和光致发光光谱研究硒薄膜的固有应力、振动动力学和电子-声子相互作用。

Result: 研究发现，硒薄膜中的短程结构无序并非固有属性，而是对加工变化的敏感指标，这会显著影响电子-声子耦合和非辐射复合。结构无序和固有应力可能导致扩展缺陷的形成，这些缺陷是非辐射复合的主要中心，限制了载流子寿命和光伏器件的开路电压。

Conclusion: 通过精确控制合成和沉积后处理，可以显着提高硒薄膜的光电质量，为通过靶向结晶动力学和微观结构无序控制来优化硒基薄膜技术提供了明确的途径。

Abstract: Selenium is experiencing renewed interest as a elemental semiconductor for a
range of optoelectronic and energy applications due to its irresistibly simple
composition and favorable wide bandgap. However, its high volatility and low
radiative efficiency make it challenging to assess structural and
optoelectronic quality, calling for advanced, non-destructive characterization
methods. In this work, we employ a closed-space encapsulation strategy to
prevent degradation during measurement and enable sensitive probing of
vibrational and optoelectronic properties. Using temperature-dependent Raman
and photoluminescence spectroscopy, we investigate grown-in stress, vibrational
dynamics, and electron-phonon interactions in selenium thin films synthesized
under nominally identical conditions across different laboratories. Our results
reveal that short-range structural disorder is not intrinsic to the material,
but highly sensitive to subtle processing variations, which strongly influence
electron-phonon coupling and non-radiative recombination. We find that such
structural disorder and grown-in stress likely promote the formation of
extended defects, which act as dominant non-radiative recombination centers
limiting carrier lifetime and open-circuit voltage in photovoltaic devices.
These findings demonstrate that the optoelectronic quality of selenium thin
films can be significantly improved through precise control of synthesis and
post-deposition treatments, outlining a clear pathway toward optimizing
selenium-based thin film technologies through targeted control of
crystallization dynamics and microstructural disorder.

</details>


### [344] [Electron Doping Stabilization of Highly-Polar Supertetragonal BaSnO3](https://arxiv.org/abs/2507.23649)
*Qing Zhang,Karin M. Rabe,Xiaohui Liu*

Main category: cond-mat.mtrl-sci

TL;DR: 电子掺杂通过选择性地填充带隙较小的超 tetragonal 态，成功稳定了 BaSnO3 的铁电极化，并实现了高迁移率下的导电性。


<details>
  <summary>Details</summary>
Motivation: 探讨电子掺杂是否能稳定铁电极化，以及这种机制是否适用于未极化的系统。

Method: 通过计算分析了掺杂电子对 BaSnO3 晶体结构的影响，特别是对超 tetragonal 相的影响。

Result: 发现电子掺杂可以稳定超 tetragonal BaSnO3，并降低稳定该相所需的临界应变。掺杂电子优先选择带隙较小的超 tetragonal 态，从而稳定了该相。

Conclusion: 电子掺杂可以稳定铁电极化，并确保了高迁移率 BaSnO3 中巨极化和导电性的共存，有望设计高迁移率铁电导体。

Abstract: Could electrons stabilize ferroelectric polarization in unpolarized system?
Basically, electron doping was thought to be contrary to polarization due to
the well-known picture that the screening effect on Coulomb interaction
diminishes ferroelectric polarization. However, in this paper, we propose a
novel mechanism of stabilizing highly-polar supertetragonal BaSnO3 by electron
doping. With moderate compressive strain applied, less than -5.5%, BaSnO3
exhibits stable nonpolarized normal tetragonal structure and an unstable
supertetragonal state which is characterized with extremely large c/a ratio and
giant polarization. We found that the band gap of the supertetragonal state is
much smaller than the normal tetragonal state, with a difference around 1.2eV.
Therefore, the energy of the doped electrons selectively favors the smaller gap
supertetragonal state than the larger band gap normal tetragonal state, and the
critical strain to stabilize the supertetragonal phase could be reduced by
electron doping. This mechanism guarantees the controllable supertetragonal
structures by electron doping and ensures the coexistence of giant polarization
and conducting in high-mobility BaSnO3, and is promising to design
high-mobility ferroelectrics conductor.

</details>


### [345] [Structural Distortions Control Scaling of Exciton Binding Energies in Two-Dimensional Ag/Bi Double Perovskites](https://arxiv.org/abs/2507.23710)
*Pierre Lechifflart,Raisa-Ioana Biega,Linn Leppert*

Main category: cond-mat.mtrl-sci

TL;DR: Ag/Bi基二维钙钛矿的结构畸变导致激子能量行为反常，层间距和堆叠方式的影响与铅基钙钛矿相似。


<details>
  <summary>Details</summary>
Motivation: 旨在解决近期实验和计算研究中发现的Ag/Bi基二维钙钛矿中激子结合能随层厚减小而增加的趋势出现反常（趋势反转）的现象，该现象与铅基二维钙钛矿的普遍趋势相反。

Method: 采用ab initio多体微扰理论，包括GW近似和Bethe-Salpeter方程，系统性地比较了实验结构与理想化模型，以分离八面体畸变、层间距和堆叠方式的影响。

Result: 研究发现，结构畸变是导致Ag/Bi基二维钙钛矿中激子能量趋势非单调性的主要原因，这些畸变源于方向性的Ag d轨道键合。层间距和堆叠方式对带隙和激子结合能的影响与铅基二维钙钛矿的限制物理学相似。

Conclusion: 本研究通过ab initio多体微扰理论（GW和Bethe-Salpeter方程）解析了二维金属卤化物双钙钛矿（Ag/Bi基）中反常的激子行为，发现结构畸变（源于Ag d轨道键合）是导致激子能量趋势非单调性的主要原因。研究还表明，层间距和堆叠方式对带隙和激子结合能的影响与铅基钙钛矿类似，并为调控这类无铅材料的激子性质提供了设计原则。

Abstract: Three-dimensional metal halide double perovskites such as Cs$_2$AgBiBr$_6$
exhibit pronounced excitonic effects due to their anisotropic electronic
structure and chemical localization effects. Their two-dimensional derivatives,
formed by inserting organic spacer molecules between perovskite layers, were
expected to follow well-established trends seen in Pb-based 2D perovskites,
namely, increasing exciton binding energies with decreasing layer thickness due
to enhanced quantum and dielectric confinement. However, recent experimental
and computational studies have revealed anomalous behavior in Ag/Bi-based 2D
perovskites, where this trend is reversed. Using ab initio many-body
perturbation theory within the $GW$ and Bethe-Salpeter Equation frameworks, we
resolve this puzzle by systematically comparing experimental structures with
idealized models designed to isolate the effects of octahedral distortions,
interlayer separation, and stacking. We find that structural distortions,
driven by directional Ag d orbital bonding, govern the momentum-space origin
and character of the exciton, and are the primary cause of the observed
non-monotonic trends. Furthermore, we explore how interlayer distance and
stacking influence band gaps and exciton binding energies, showing that,
despite different chemistry, the underlying confinement physics mirrors that of
Pb-based 2D perovskites. Our results establish design principles for tuning
excitonic properties in this broader class of layered, lead-free materials.

</details>


### [346] [Structural and thermodynamic stability of hexagonal-diamond $\text{Si}_{1 - x - y}\,\text{Ge}_{x}\,\text{B}_{y}$ alloys](https://arxiv.org/abs/2507.23741)
*Marc Túnica,Francesca Chiodi,Michele Amato*

Main category: cond-mat.mtrl-sci

TL;DR: 在高浓度掺杂下，六方晶系 SiGe 合金比立方晶系更稳定，更易于超掺杂，这使其成为探索 IV 族半导体超导性的有希望的材料。


<details>
  <summary>Details</summary>
Motivation: 在立方晶系 Si 和 SiGe 材料中，通过超过溶解度极限的掺杂（即超掺杂）诱导超导性已被证明是一种有效策略。此外，先前的研究表明，几种 Si 多晶型物在高压下可能表现出 I 型超导态。

Method: 利用基态密度泛函理论模拟，研究了低浓度和高浓度硼掺杂对六方晶系 SiGe 合金结构和热力学性质的影响，并与立方晶系对应物进行了系统比较。

Result: 1. 结构分析证实 SiGeB 合金的晶格参数符合三元 Vegard 定律，这与立方晶系 SiGe 合金中的观察结果一致。然而，在高掺杂浓度下，硼的掺入会破坏局部六方对称性，特别是在存在硼团簇的情况下。
2. 掺杂剂形成能计算表明，无论掺杂水平如何，在所有 Ge 浓度下，硼在六方晶系中的热力学稳定性优于立方晶系。
3. 混合焓计算表明，超掺杂的六方晶系 SiGe 合金在整个 Ge 组成范围内都是热力学稳定的，并且其超掺杂倾向比立方晶系 SiGe 合金更有利。

Conclusion: 研究结果表明，超掺杂在六方晶系 SiGe 合金中是可行的，并且这些材料为探索 IV 族半导体中的超导性提供了一个有前景的平台。

Abstract: Pushing dopant concentrations beyond the solubility limit in semiconductors
-- a process known as hyperdoping -- has been demonstrated as an effective
strategy for inducing superconductivity in cubic-diamond Si and SiGe materials.
Additionally, previous studies have reported that several polytypes of Si may
exhibit a type-I superconducting state under high pressure. In this work, we
employ ground-state Density Functional Theory simulations to investigate the
effects of both low and high B doping concentrations on the structural and
thermodynamic properties of hexagonal-diamond SiGe alloys, with a systematic
comparison to their cubic-diamond counterparts. Our results highlight three key
findings: (i) structural analysis confirms that the lattice parameters of SiGeB
alloys adhere to a ternary Vegard's law, consistent with observations in
cubic-diamond SiGe alloys. However, at high doping concentrations, B
incorporation can locally disrupt the hexagonal symmetry, particularly in the
presence of B clustering; (ii) dopant formation energy calculations reveal that
B is thermodynamically more stable in the hexagonal phase than in the cubic
phase across all Ge concentrations, regardless of the doping level; (iii)
mixing enthalpy calculations demonstrate that hyperdoped hexagonal-diamond SiGe
alloys are thermodynamically stable across the full range of Ge compositions
and that their tendency for hyperdoping is more favorable than that of
cubic-diamond SiGe alloys. Taken together, these findings indicate that
hyperdoping is experimentally viable in hexagonal-diamond SiGe alloys and, in
light of previous evidence, position these materials as a promising platform
for the exploration of superconductivity in group IV semiconductors.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [347] [Noise-Coded Illumination for Forensic and Photometric Video Analysis](https://arxiv.org/abs/2507.23002)
*Peter F. Michael,Zekun Hao,Serge Belongie,Abe Davis*

Main category: cs.GR

TL;DR: 本研究提出了一种通过对视频的照明进行编码来创建时间水印的方法，以对抗虚假视频的传播。这种方法利用信息不对称来验证视频的真实性，即使在对手知情的情况下也能有效抵御操纵。


<details>
  <summary>Details</summary>
Motivation: 随着视频操纵工具的不断发展，打击虚假视频的传播变得越来越困难。操纵者拥有一个关键优势：他们可以平等地访问被认为是真实的“自然”视频。本研究旨在通过引入一种信息不对称来对抗这一优势，以利于验证。

Method: 本研究提出了一种通过对场景照明进行编码来创建时间水印的方法。这种水印本质上是对未被操纵的场景在仅受编码照明照射下的图像进行编码。

Result: 实验表明，即使对手了解该技术，试图创建包含编码信息的虚假视频也面临着更大的挑战，因为它需要解决一个更困难的版本的问题，并且处于信息劣势。

Conclusion: 该方法通过在视频中编码细微的、类似噪声的调制来对抗虚假视频的传播。通过对场景照明进行编码，可以创建一种信息不对称，有利于验证。这种方法为视频添加了时间水印，该水印编码了在仅受编码照明照射下场景的图像。即使对手知道该技术的使用，创建可信的编码假视频也相当于在信息劣势下解决一个更困难的版本。这在保护高风险环境（如公共活动和访谈）方面很有前景，这些环境是操纵的目标，并且可以控制照明，但无法控制拍摄视频的摄像机。

Abstract: The proliferation of advanced tools for manipulating video has led to an arms
race, pitting those who wish to sow disinformation against those who want to
detect and expose it. Unfortunately, time favors the ill-intentioned in this
race, with fake videos growing increasingly difficult to distinguish from real
ones. At the root of this trend is a fundamental advantage held by those
manipulating media: equal access to a distribution of what we consider
authentic (i.e., "natural") video. In this paper, we show how coding very
subtle, noise-like modulations into the illumination of a scene can help combat
this advantage by creating an information asymmetry that favors verification.
Our approach effectively adds a temporal watermark to any video recorded under
coded illumination. However, rather than encoding a specific message, this
watermark encodes an image of the unmanipulated scene as it would appear lit
only by the coded illumination. We show that even when an adversary knows that
our technique is being used, creating a plausible coded fake video amounts to
solving a second, more difficult version of the original adversarial content
creation problem at an information disadvantage. This is a promising avenue for
protecting high-stakes settings like public events and interviews, where the
content on display is a likely target for manipulation, and while the
illumination can be controlled, the cameras capturing video cannot.

</details>


### [348] [XSpecMesh: Quality-Preserving Auto-Regressive Mesh Generation Acceleration via Multi-Head Speculative Decoding](https://arxiv.org/abs/2507.23777)
*Dian Chen,Yansong Qu,Xinyang Li,Ming Li,Shengchuan Zhang*

Main category: cs.GR

TL;DR: XSpecMesh 是一种加速自回归网格生成模型的方法，通过并行预测和验证/重采样策略，在不影响质量的情况下提高了 1.7 倍的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型在生成高质量、拓扑精确的网格方面表现出色，但其推理过程需要数千甚至数万次的 next-token 预测，导致显著的延迟。本研究旨在解决这一问题，通过提出一种名为 XSpecMesh 的加速方法来提高自回归网格生成模型的推理速度。

Method: XSpecMesh 采用了一种轻量级的、多头预测解码方案，可以在单次前向传播中并行预测多个 token，从而加速推理。此外，还提出了一种验证和重采样策略，主干模型会验证每个预测的 token，并对不满足质量标准的 token 进行重采样。另外，还提出了一种蒸馏策略，通过从主干模型蒸馏来训练轻量级解码头，使其预测分布对齐，提高预测的成功率。

Result: 实验证明，XSpecMesh 实现了 1.7 倍的加速，同时不牺牲生成质量。

Conclusion: XSpecMesh 实现了 1.7 倍的加速，同时不牺牲生成质量。

Abstract: Current auto-regressive models can generate high-quality, topologically
precise meshes; however, they necessitate thousands-or even tens of
thousands-of next-token predictions during inference, resulting in substantial
latency. We introduce XSpecMesh, a quality-preserving acceleration method for
auto-regressive mesh generation models. XSpecMesh employs a lightweight,
multi-head speculative decoding scheme to predict multiple tokens in parallel
within a single forward pass, thereby accelerating inference. We further
propose a verification and resampling strategy: the backbone model verifies
each predicted token and resamples any tokens that do not meet the quality
criteria. In addition, we propose a distillation strategy that trains the
lightweight decoding heads by distilling from the backbone model, encouraging
their prediction distributions to align and improving the success rate of
speculative predictions. Extensive experiments demonstrate that our method
achieves a 1.7x speedup without sacrificing generation quality. Our code will
be released.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [349] [Learning with Episodic Hypothesis Testing in General Games: A Framework for Equilibrium Selection](https://arxiv.org/abs/2507.23149)
*Ruifan Yang,Manxi Wu*

Main category: cs.GT

TL;DR: 提出了一种新的基于假设检验的学习动力学，其中玩家通过结合假设检验和效用驱动的探索来更新策略。该过程在一般的有限正规形式博弈中收敛到近似纳什均衡，并选择最大化所有玩家的最小（转换后）效用的均衡。


<details>
  <summary>Details</summary>
Motivation: 在一般的有限正规形式博弈中，展示了学习过程收敛到一组近似纳什均衡。

Method: 提出了一种基于假设检验的学习动力学，玩家通过结合假设检验和由效用驱动的探索来更新策略。在其中，每个玩家会形成关于对手策略的信念，并使用经验观察来阶段性地检验这些信念。当假设检验被拒绝或通过探索（探索的概率随着玩家的（转换后）效用的增加而降低）时，信念会被重新采样。

Result: 证明了学习过程收敛到一组近似纳什均衡，并且可以收敛到一个选择能最大化所有玩家的最小（转换后）效用的精细化。

Conclusion: 该学习过程收敛到一组近似纳什均衡，并且更重要的是，收敛到一个通过最大化所有玩家的最小（转换后）效用来选择均衡的精细化。

Abstract: We introduce a new hypothesis testing-based learning dynamics in which
players update their strategies by combining hypothesis testing with
utility-driven exploration. In this dynamics, each player forms beliefs about
opponents' strategies and episodically tests these beliefs using empirical
observations. Beliefs are resampled either when the hypothesis test is rejected
or through exploration, where the probability of exploration decreases with the
player's (transformed) utility. In general finite normal-form games, we show
that the learning process converges to a set of approximate Nash equilibria
and, more importantly, to a refinement that selects equilibria maximizing the
minimum (transformed) utility across all players. Our result establishes
convergence to equilibrium in general finite games and reveals a novel
mechanism for equilibrium selection induced by the structure of the learning
dynamics.

</details>


### [350] [Online Combinatorial Allocation with Interdependent Values](https://arxiv.org/abs/2507.23500)
*Michal Feldman,Simon Mauras,Divyarthi Mohan,Rebecca Reiffenhäuser*

Main category: cs.GT

TL;DR: 本文研究了具有相互依赖价值的在线组合分配问题，并提出了2e竞争性算法，适用于次模和XOS估值函数。此外，还为在线二分匹配问题设计了4e竞争性机制。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于将现有的在线分配问题研究扩展到更复杂的场景，特别是涉及用户对物品组合的偏好（组合结构）以及用户之间的价值相互影响（相互依赖价值）。现有的研究虽然在单一物品分配的秘书设置中取得了一定的进展，但对于更具挑战性的组合分配和相互依赖价值同时存在的情况，仍有待深入研究。

Method: 本文提出了一种新的算法框架，用于解决在线组合分配问题，特别是在秘书设置和相互依赖价值的背景下。研究人员设计了能够实现2e竞争性近似比的算法，该算法适用于次模函数和XOS函数等多种估值函数。此外，他们还研究了该框架在战略设置中的应用，并为在线二分匹配问题设计了一个4e竞争性机制。

Result: 本文为在线组合秘书问题提供了2e竞争性算法，适用于次模和XOS等估值函数，与单物品秘书问题相当。对于战略性在线二分匹配问题，本文提出了4e竞争性机制，其性能与无相互依赖情况下的最优算法相当。

Conclusion: 本文将在线组合分配问题扩展到秘书设置，考虑了相互依赖的价值。研究结果表明，对于包括次模函数和XOS函数在内的广泛估值函数，可以实现2e竞争性算法，这与单选择秘书设置的近似保证相匹配。此外，研究结果涵盖了经典（非相互依赖）秘书设置中存在常数因子算法的估值类别，并且仅因相互依赖性而增加了一个2的因子。最后，本文将研究扩展到战略设置，并为具有相互依赖估值的在线二分匹配提供了一个4e竞争性机制，这与没有相互依赖性的情况下的已知结果相符。

Abstract: We study online combinatorial allocation problems in the secretary setting,
under interdependent values. In the interdependent model, introduced by Milgrom
and Weber (1982), each agent possesses a private signal that captures her
information about an item for sale, and the value of every agent depends on the
signals held by all agents. Mauras, Mohan, and Reiffenh\"auser (2024) were the
first to study interdependent values in online settings, providing
constant-approximation guarantees for secretary settings, where agents arrive
online along with their signals and values, and the goal is to select the agent
with the highest value.
  In this work, we extend this framework to {\em combinatorial} secretary
problems, where agents have interdependent valuations over {\em bundles} of
items, introducing additional challenges due to both combinatorial structure
and interdependence. We provide $2e$-competitive algorithms for a broad class
of valuation functions, including submodular and XOS functions, matching the
approximation guarantees in the single-choice secretary setting. Furthermore,
our results cover the same range of valuation classes for which constant-factor
algorithms exist in classical (non-interdependent) secretary settings, while
incurring only an additional factor of $2$ due to interdependence. Finally, we
extend our study to strategic settings, and provide a $4e$-competitive truthful
mechanism for online bipartite matching with interdependent valuations, again
meeting the frontier of what is known, even without interdependence.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [351] [Learning to Prune Branches in Modern Tree-Fruit Orchards](https://arxiv.org/abs/2507.23015)
*Abhinav Jain,Cindy Grimm,Stefan Lee*

Main category: cs.RO

TL;DR: 机器人通过视觉-运动控制器在模拟和真实苹果树上进行修剪，成功率30%.


<details>
  <summary>Details</summary>
Motivation: 解决现代高产果园中休眠期树木修剪劳动密集的问题，并探索机器人修剪的可行性。

Method: 提出了一种闭环视觉-运动控制器，利用腕部摄像头捕捉的光流图像，在模拟的苹果树环境中进行训练，以实现目标切割点和确保切割器垂直于树枝。

Result: 在仿真和真实世界（V-Trellis苹果树）中进行了部署，实现了30%的成功率，接近于“神谕”规划器的一半性能。

Conclusion: 该控制器在仿真和真实环境中都取得了成功，尽管在真实环境中仅达到30%的成功率，但仍展示了闭环控制在机器人修剪领域的潜力。

Abstract: Dormant tree pruning is labor-intensive but essential to maintaining modern
highly-productive fruit orchards. In this work we present a closed-loop
visuomotor controller for robotic pruning. The controller guides the cutter
through a cluttered tree environment to reach a specified cut point and ensures
the cutters are perpendicular to the branch. We train the controller using a
novel orchard simulation that captures the geometric distribution of branches
in a target apple orchard configuration. Unlike traditional methods requiring
full 3D reconstruction, our controller uses just optical flow images from a
wrist-mounted camera. We deploy our learned policy in simulation and the
real-world for an example V-Trellis envy tree with zero-shot transfer,
achieving a 30% success rate -- approximately half the performance of an oracle
planner.

</details>


### [352] [A Certifably Correct Algorithm for Generalized Robot-World and Hand-Eye Calibration](https://arxiv.org/abs/2507.23045)
*Emmett Wise,Pushyami Kaveti,Qilong Chen,Wenhao Wang,Hanumant Singh,Jonathan Kelly,David M. Rosen,Matthew Giamou*

Main category: cs.RO

TL;DR: 提出了一种用于机器人-世界和手眼校准（RWHEC）的快速、全局最优算法，能够处理多传感器和单目相机，并提供可辨识性准则和全局最优性保证。同时引入了局部求解器并进行性能比较，最后提供了开源实现。


<details>
  <summary>Details</summary>
Motivation: 机器人外参传感器标定是多传感器平台的基本问题。可靠且通用的解决方案应计算效率高，对传感环境的结构假设少，并且需要最少的人工操作。现有方法在传感器数量增加时，标定精度所需工程量也随之增加。因此，研究人员一直在寻求假设少、人工操作需求低的标定方法。

Method: 提出了一种解决机器人-世界和手眼校准（RWHEC）问题的广义公式的快速且可认证全局最优算法。该算法支持同时估计多个传感器和目标的位姿，并允许使用单目相机。推导了新的可辨识性准则，并为具有有界测量误差的问题实例建立了先验全局最优性保证。引入了一个互补的李代数局部求解器，并与全局方法和现有技术进行了性能比较。

Result: 所提出的算法在性能上优于现有解决方案。研究推导了新的可辨识性准则，并为具有有界测量误差的问题实例建立了先验全局最优性保证。与全局方法和现有技术相比，互补的李代数局部求解器也表现良好。

Conclusion: 该研究提出了一个快速且可认证全局最优的算法，用于解决机器人-世界和手眼校准（RWHEC）问题的广义公式。该方法能够同时估计多个传感器和目标的位姿，并允许使用单目相机，即使它们本身无法测量其环境的尺度。此外，研究还推导了新的可辨识性准则，并为具有有界测量误差的问题实例建立了先验全局最优性保证。同时，还引入了一个互补的李代数局部求解器，并与全局方法和现有技术进行了性能比较。最后，研究提供了一个免费开源的算法和实验实现。

Abstract: Automatic extrinsic sensor calibration is a fundamental problem for
multi-sensor platforms. Reliable and general-purpose solutions should be
computationally efficient, require few assumptions about the structure of the
sensing environment, and demand little effort from human operators. Since the
engineering effort required to obtain accurate calibration parameters increases
with the number of sensors deployed, robotics researchers have pursued methods
requiring few assumptions about the sensing environment and minimal effort from
human operators. In this work, we introduce a fast and certifiably globally
optimal algorithm for solving a generalized formulation of the
$\textit{robot-world and hand-eye calibration}$ (RWHEC) problem. The
formulation of RWHEC presented is "generalized" in that it supports the
simultaneous estimation of multiple sensor and target poses, and permits the
use of monocular cameras that, alone, are unable to measure the scale of their
environments. In addition to demonstrating our method's superior performance
over existing solutions, we derive novel identifiability criteria and establish
$\textit{a priori}$ guarantees of global optimality for problem instances with
bounded measurement errors. We also introduce a complementary Lie-algebraic
local solver for RWHEC and compare its performance with our global method and
prior art. Finally, we provide a free and open-source implementation of our
algorithms and experiments.

</details>


### [353] [In-between Motion Generation Based Multi-Style Quadruped Robot Locomotion](https://arxiv.org/abs/2507.23053)
*Yuanhao Chen,Liu Zhao,Ji Ma,Peng Lu*

Main category: cs.RO

TL;DR: 该研究提出了一种新的四足机器人运动生成框架，通过结合CVAE和对抗性运动先验，能够生成多风格的动态可行运动，并在真实机器人上成功验证了其稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决四足机器人因参考运动数据多样性不足而导致的通用运动能力受限的挑战。

Method: 提出了一种基于CVAE的运动生成器，用于在任意起始和结束状态之间合成多风格动态可行的的运动序列，并结合了对抗性运动先验算法。

Result: 生成的运动数据在增强控制器稳定性和改善速度跟踪性能方面有效，在速度跟踪和部署稳定性方面有显著提高。

Conclusion: 该框架在真实机器人上成功部署，实验验证证实了其生成和执行复杂运动（包括奔跑、三足、慢跑和踱步）的能力。

Abstract: Quadruped robots face persistent challenges in achieving versatile locomotion
due to limitations in reference motion data diversity. To address these
challenges, this approach introduces an in-between motion generation based
multi-style quadruped robot locomotion framework, integrating synergistic
advances in motion generation and imitation learning. Our approach establishes
a unified pipeline addressing two fundamental aspects: First, we propose a CVAE
based motion generator, synthesizing multi-style dynamically feasible
locomotion sequences between arbitrary start and end states. By embedding
physical constraints and leveraging joint poses based phase manifold
continuity, this component produces physically plausible motions spanning
multiple gait modalities while ensuring kinematic compatibility with robotic
morphologies. Second, we adopt the adversarial motion priors algorithm. We
validate the effectiveness of generated motion data in enhancing controller
stability and improving velocity tracking performance. The proposed framework
demonstrates significant improvements in velocity tracking and deployment
stability. We successfully deploy the framework on a real-world quadruped
robot, and the experimental validation confirms the framework's capability to
generate and execute complex motion profiles, including gallop, tripod,
trotting and pacing.

</details>


### [354] [Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance](https://arxiv.org/abs/2507.23088)
*Lalithkumar Seenivasan,Jiru Xu,Roger D. Soberanis Mukul,Hao Ding,Grayson Byrd,Yu-Chun Ku,Jose L. Porras,Masaru Ishii,Mathias Unberath*

Main category: cs.RO

TL;DR: 本研究提出了一种新的人机交互方法，用于实时手术辅助，通过利用LLM、SAM和任何点跟踪基础模型，克服了当前AI解决方案的僵化性，并实现了对已知和未知元素的分割，使AI在手术中的应用更具灵活性和实用性。


<details>
  <summary>Details</summary>
Motivation: 为了充分发挥新兴的手术数据科学和机器人解决方案（特别是那些旨在提供现场援助的解决方案）的潜力，需要自然的人机界面来提供自适应和直观的援助。然而，当前由AI驱动的解决方案本质上是僵化的，灵活性有限，并限制了在动态手术环境中自然的人机交互。

Method: 本研究引入了一种新颖的感知代理，该代理利用语音集成提示工程大语言模型（LLMs）、分割一切模型（SAM）和任意点跟踪基础模型，以在实时术中手术辅助中实现更自然的人机交互。感知代理包含一个记忆库和两个分割未知元素的新机制，通过直观的交互，能够分割手术场景中已知和未知的元素。它还能够记忆新元素以供将来手术使用。

Result: 通过在公共数据集上的定量分析，我们证明了我们代理的性能与劳动密集度更高的人工提示策略相当。通过对自定义数据集中的新元素（器械、假移植物和纱布）进行分割的定性分析，我们展示了我们代理的灵活性。

Conclusion: 该感知代理通过提供自然的人机交互并克服僵化的缺点，有潜力使基于AI的实时手术辅助更接近现实。

Abstract: Emerging surgical data science and robotics solutions, especially those
designed to provide assistance in situ, require natural human-machine
interfaces to fully unlock their potential in providing adaptive and intuitive
aid. Contemporary AI-driven solutions remain inherently rigid, offering limited
flexibility and restricting natural human-machine interaction in dynamic
surgical environments. These solutions rely heavily on extensive task-specific
pre-training, fixed object categories, and explicit manual-prompting. This work
introduces a novel Perception Agent that leverages speech-integrated
prompt-engineered large language models (LLMs), segment anything model (SAM),
and any-point tracking foundation models to enable a more natural human-machine
interaction in real-time intraoperative surgical assistance. Incorporating a
memory repository and two novel mechanisms for segmenting unseen elements,
Perception Agent offers the flexibility to segment both known and unseen
elements in the surgical scene through intuitive interaction. Incorporating the
ability to memorize novel elements for use in future surgeries, this work takes
a marked step towards human-machine symbiosis in surgical procedures. Through
quantitative analysis on a public dataset, we show that the performance of our
agent is on par with considerably more labor-intensive manual-prompting
strategies. Qualitatively, we show the flexibility of our agent in segmenting
novel elements (instruments, phantom grafts, and gauze) in a custom-curated
dataset. By offering natural human-machine interaction and overcoming rigidity,
our Perception Agent potentially brings AI-based real-time assistance in
dynamic surgical environments closer to reality.

</details>


### [355] [Benchmarking Massively Parallelized Multi-Task Reinforcement Learning for Robotics Tasks](https://arxiv.org/abs/2507.23172)
*Vira Joshi,Zifan Xu,Bo Liu,Peter Stone,Amy Zhang*

Main category: cs.RO

TL;DR: 本研究提出了MTBench，一个用于机器人领域大规模并行多任务强化学习的基准测试，旨在解决现有研究的局限性，并通过大规模并行化提升策略性能，同时揭示了相关挑战。


<details>
  <summary>Details</summary>
Motivation: 填补现有研究在低并行化和主要使用离策略方法（如SAC）方面的空白，利用大规模并行化（通过GPU加速模拟）的优势来提升强化学习策略的泛化性和鲁棒性，特别是在需要从当前策略收集数据的在线策略算法方面。

Method: 提出了一种名为MTBench的基准测试框架，该框架利用IsaacGym加速器，集成了四种基础强化学习算法和七种先进的多任务强化学习算法与架构，用于评估机器人领域的MTRL方法。

Result: MTBench在评估MTRL方法方面展现出卓越的速度优势，并揭示了将大规模并行化与MTRL相结合时出现的新挑战。

Conclusion: 该研究介绍了MTBench，一个用于机器人领域大规模多任务强化学习的基准测试。MTBench包含50个操作任务和20个运动任务，并利用IsaacGym加速器。实验表明，MTBench能够有效评估MTRL方法的性能，并揭示了大规模并行化与MTRL结合所带来的新挑战。

Abstract: Multi-task Reinforcement Learning (MTRL) has emerged as a critical training
paradigm for applying reinforcement learning (RL) to a set of complex
real-world robotic tasks, which demands a generalizable and robust policy. At
the same time, \emph{massively parallelized training} has gained popularity,
not only for significantly accelerating data collection through GPU-accelerated
simulation but also for enabling diverse data collection across multiple tasks
by simulating heterogeneous scenes in parallel. However, existing MTRL research
has largely been limited to off-policy methods like SAC in the
low-parallelization regime. MTRL could capitalize on the higher asymptotic
performance of on-policy algorithms, whose batches require data from the
current policy, and as a result, take advantage of massive parallelization
offered by GPU-accelerated simulation. To bridge this gap, we introduce a
massively parallelized $\textbf{M}$ulti-$\textbf{T}$ask $\textbf{Bench}$mark
for robotics (MTBench), an open-sourced benchmark featuring a broad
distribution of 50 manipulation tasks and 20 locomotion tasks, implemented
using the GPU-accelerated simulator IsaacGym. MTBench also includes four base
RL algorithms combined with seven state-of-the-art MTRL algorithms and
architectures, providing a unified framework for evaluating their performance.
Our extensive experiments highlight the superior speed of evaluating MTRL
approaches using MTBench, while also uncovering unique challenges that arise
from combining massive parallelism with MTRL. Code is available at
$\href{https://github.com/Viraj-Joshi/MTBench}{
https://github.com/Viraj-Joshi/MTBench}$

</details>


### [356] [Quadratic Programming-Based Posture Manipulation and Thrust-vectoring for Agile Dynamic Walking on Narrow Pathways](https://arxiv.org/abs/2507.23203)
*Chenghao Wang,Eric Sihite,Kaushik Venkatesh Krishnamurthy,Shreyansh Pitroda,Adarsh Salagame,Alireza Ramezani,Morteza Gharib*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: There has been significant advancement in legged robot's agility where they
can show impressive acrobatic maneuvers, such as parkour. These maneuvers rely
heavily on posture manipulation. To expand the stability and locomotion
plasticity, we use the multi-modal ability in our legged-aerial platform, the
Husky Beta, to perform thruster-assisted walking. This robot has thrusters on
each of its sagittal knee joints which can be used to stabilize its frontal
dynamic as it walks. In this work, we perform a simulation study of quadruped
narrow-path walking with Husky $\beta$, where the robot will utilize its
thrusters to stably walk on a narrow path. The controller is designed based on
a centroidal dynamics model with thruster and foot ground contact forces as
inputs. These inputs are regulated using a QP solver to be used in a model
predictive control framework. In addition to narrow-path walking, we also
perform a lateral push-recovery simulation to study how the thrusters can be
used to stabilize the frontal dynamics.

</details>


### [357] [Simulation-based planning of Motion Sequences for Automated Procedure Optimization in Multi-Robot Assembly Cells](https://arxiv.org/abs/2507.23270)
*Loris Schneider,Marc Ungen,Elias Huber,Jan-Felix Klein*

Main category: cs.RO

TL;DR: 可重构多机器人单元格可以满足波动的装配需求，但其配置的反复规划带来了新的挑战。本研究提出了一种基于仿真的方法来生成优化的、协调的多机器人运动序列，以最小化装配时间。该方法将装配步骤分解为核心操作和遍历操作，并使用基于分解的运动规划策略来优化遍历操作。仿真结果表明，该方法生成的程序比传统方法更优。


<details>
  <summary>Details</summary>
Motivation: 可重构多机器人单元格为满足波动的装配需求提供了一种有前景的方法。然而，其配置的反复规划带来了新的挑战，特别是在生成优化、协调的多机器人运动序列以最小化装配持续时间方面。

Method: 该方法将装配步骤分解为与任务相关的核心操作和连接的遍历操作。核心操作是约束和预定的，而遍历操作提供了巨大的优化潜力。核心操作的调度被表述为一个优化问题，其中需要使用基于分解的运动规划策略来集成可行的遍历操作。探索了几种求解技术，包括采样启发式、基于树的搜索和无梯度优化。对于运动规划，提出了一种分解方法，该方法识别了调度中的特定区域，这些区域可以使用修改后的集中式路径规划算法独立求解。

Result: 该方法生成高效且无碰撞的多机器人装配程序，并且优于依赖于分散的、机器人独立的运动规划的基线方法。

Conclusion: 所提出的方法能够生成高效且无碰撞的多机器人装配程序，并且优于依赖于分散的、机器人独立的运动规划的基线方法。该方法的有效性已通过仿真实验得到证明。

Abstract: Reconfigurable multi-robot cells offer a promising approach to meet
fluctuating assembly demands. However, the recurrent planning of their
configurations introduces new challenges, particularly in generating optimized,
coordinated multi-robot motion sequences that minimize the assembly duration.
This work presents a simulation-based method for generating such optimized
sequences. The approach separates assembly steps into task-related core
operations and connecting traverse operations. While core operations are
constrained and predetermined, traverse operations offer substantial
optimization potential. Scheduling the core operations is formulated as an
optimization problem, requiring feasible traverse operations to be integrated
using a decomposition-based motion planning strategy. Several solution
techniques are explored, including a sampling heuristic, tree-based search and
gradient-free optimization. For motion planning, a decomposition method is
proposed that identifies specific areas in the schedule, which can be solved
independently with modified centralized path planning algorithms. The proposed
method generates efficient and collision-free multi-robot assembly procedures
that outperform a baseline relying on decentralized, robot-individual motion
planning. Its effectiveness is demonstrated through simulation experiments.

</details>


### [358] [GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting](https://arxiv.org/abs/2507.23273)
*Jaeseok Park,Chanoh Park,Minsu Kim,Soohwan Kim*

Main category: cs.RO

TL;DR: GSFusion是一种新的在线激光雷达-惯性-视觉地图构建系统，通过创新的方法解决了传统3DGS在低纹理/光照环境和激光雷达集成中的问题，并在渲染质量和效率方面取得了优于现有系统的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于相机传感器的三维高斯喷涂（3DGS）方法存在计算负载高、在纹理或光照差的环境下失败以及操作范围短等问题。虽然激光雷达（LiDAR）是一个有力的替代方案，但将其与3DGS集成面临着对高精度全局对齐的需求以及稀疏数据导致的长时间优化。

Method: 提出了一种名为GSFusion的在线激光雷达-惯性-视觉（LiDAR-Inertial-Visual）地图构建系统。

Result: 实验结果表明，GSFusion在渲染质量和地图构建效率方面优于现有的3DGS SLAM系统。

Conclusion: GSFusion通过在全局姿态图优化中引入点云到点云约束，实现了高精度的地图一致性，并采用了像素感知的高斯初始化策略和有界的sigmoid约束来处理稀疏数据和防止高斯增长失控。

Abstract: While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping,
conventional approaches based on camera sensor, even RGB-D, suffer from
fundamental limitations such as high computational load, failure in
environments with poor texture or illumination, and short operational ranges.
LiDAR emerges as a robust alternative, but its integration with 3DGS introduces
new challenges, such as the need for exceptional global alignment for
photorealistic quality and prolonged optimization times caused by sparse data.
To address these challenges, we propose GSFusion, an online
LiDAR-Inertial-Visual mapping system that ensures high-precision map
consistency through a surfel-to-surfel constraint in the global pose-graph
optimization. To handle sparse data, our system employs a pixel-aware Gaussian
initialization strategy for efficient representation and a bounded sigmoid
constraint to prevent uncontrolled Gaussian growth. Experiments on public and
our datasets demonstrate our system outperforms existing 3DGS SLAM systems in
terms of rendering quality and map-building efficiency.

</details>


### [359] [Whisker-based Active Tactile Perception for Contour Reconstruction](https://arxiv.org/abs/2507.23305)
*Yixuan Dang,Qinyang Xu,Yu Zhang,Xiangtong Yao,Liding Zhang,Zhenshan Bing,Florian Roehrbein,Alois Knoll*

Main category: cs.RO

TL;DR: 提出了一种触须传感和控制方法，用于机器人本体感知，实现了对物体轮廓的亚毫米级精度跟踪和重建。


<details>
  <summary>Details</summary>
Motivation: 为了在机器人本体上实现基于触须的感知，需要主动控制机器人，使其触须能够利用直接的接触信息来跟踪物体表面并保持适当的相对姿态，以精确地重建物体轮廓。

Method: 提出了一种利用磁致伸缩触须传感器，并结合梯度下降和贝叶斯滤波器来提取触须尖端接触位置的方法，同时提出了一种主动运动控制策略，利用B样条曲线预测局部表面曲率并确定传感器姿态，以在触须传感器与物体表面之间保持最佳的相对姿态。

Result: 算法能够以亚毫米级的精度有效跟踪物体和重建轮廓。

Conclusion: 该方法在模拟和真实世界实验中得到了验证，机器人手臂驱动触须传感器跟随三个不同物体的表面，能够以亚毫米级精度有效地跟踪物体和重建轮廓。

Abstract: Perception using whisker-inspired tactile sensors currently faces a major
challenge: the lack of active control in robots based on direct contact
information from the whisker. To accurately reconstruct object contours, it is
crucial for the whisker sensor to continuously follow and maintain an
appropriate relative touch pose on the surface. This is especially important
for localization based on tip contact, which has a low tolerance for sharp
surfaces and must avoid slipping into tangential contact. In this paper, we
first construct a magnetically transduced whisker sensor featuring a compact
and robust suspension system composed of three flexible spiral arms. We develop
a method that leverages a characterized whisker deflection profile to directly
extract the tip contact position using gradient descent, with a Bayesian filter
applied to reduce fluctuations. We then propose an active motion control policy
to maintain the optimal relative pose of the whisker sensor against the object
surface. A B-Spline curve is employed to predict the local surface curvature
and determine the sensor orientation. Results demonstrate that our algorithm
can effectively track objects and reconstruct contours with sub-millimeter
accuracy. Finally, we validate the method in simulations and real-world
experiments where a robot arm drives the whisker sensor to follow the surfaces
of three different objects.

</details>


### [360] [Distributed AI Agents for Cognitive Underwater Robot Autonomy](https://arxiv.org/abs/2507.23735)
*Markus Buchholz,Ignacio Carlucho,Michele Grimaldi,Yvan R. Petillot*

Main category: cs.RO

TL;DR: UROSA是一种创新的水下机器人自主架构，利用分布式大型语言模型AI代理和ROS 2框架，实现了高级认知能力，并在模拟和实际部署中表现出优越的适应性和可靠性，为认知机器人领域提供了一个通用的框架。


<details>
  <summary>Details</summary>
Motivation: 在复杂、不可预测的环境中实现机器人鲁棒的认知自主性是机器人领域的一个基本挑战。

Method: 该研究提出了UROSA（Underwater Robot Self-Organizing Autonomy）架构，该架构利用了分布式大型语言模型AI代理，并集成在ROS 2框架内，实现了多模态感知、自适应推理、动态任务规划和实时决策。其核心创新包括：灵活的代理角色自适应、利用向量数据库的检索增强生成（RAG）以实现高效知识管理、基于强化学习的行为优化，以及自动生成的ROS 2节点以支持运行时功能扩展。

Result: 通过在模拟和实际部署中进行真实水下任务的广泛实证验证，UROSA展示了其良好的适应性和可靠性，在处理意外情况、环境不确定性和新任务目标方面，相比传统的基于规则的体系结构具有显著优势。

Conclusion: 该研究提出了UROSA架构，并验证了其在应对不可预见场景、环境不确定性和新任务目标方面的优势，证明了其在推进水下自主性方面的潜力，并为认知机器人领域提供了一个可扩展、安全、通用的认知机器人框架，能够推广到各种实际应用中。

Abstract: Achieving robust cognitive autonomy in robots navigating complex,
unpredictable environments remains a fundamental challenge in robotics. This
paper presents Underwater Robot Self-Organizing Autonomy (UROSA), a
groundbreaking architecture leveraging distributed Large Language Model AI
agents integrated within the Robot Operating System 2 (ROS 2) framework to
enable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA
decentralises cognition into specialised AI agents responsible for multimodal
perception, adaptive reasoning, dynamic mission planning, and real-time
decision-making. Central innovations include flexible agents dynamically
adapting their roles, retrieval-augmented generation utilising vector databases
for efficient knowledge management, reinforcement learning-driven behavioural
optimisation, and autonomous on-the-fly ROS 2 node generation for runtime
functional extensibility. Extensive empirical validation demonstrates UROSA's
promising adaptability and reliability through realistic underwater missions in
simulation and real-world deployments, showing significant advantages over
traditional rule-based architectures in handling unforeseen scenarios,
environmental uncertainties, and novel mission objectives. This work not only
advances underwater autonomy but also establishes a scalable, safe, and
versatile cognitive robotics framework capable of generalising to a diverse
array of real-world applications.

</details>


### [361] [Assessing the Alignment of Automated Vehicle Decisions with Human Reasons](https://arxiv.org/abs/2507.23324)
*Lucas Elbert Suryana,Saeed Rahmani,Simeon Craig Calvert,Arkady Zgonnikov,Bart van Arem*

Main category: cs.RO

TL;DR: 该论文提出了一个基于原因的轨迹评估框架，用于在日常驾驶场景中实现自动驾驶汽车的有意义的人类控制。它将人类的考虑因素（如合法性和舒适性）建模为可量化的函数，以评估和平衡自动驾驶汽车行为，解决了现有规划系统中僵化规则的局限性。


<details>
  <summary>Details</summary>
Motivation: 部署自动驾驶汽车的一个关键挑战是确保它们在符合伦理的日常驾驶情境中做出适当的决定。虽然人们对“电车难题”等罕见的、高风险的困境给予了充分关注，但在类似的日常场景中也存在类似的张力，例如在空旷路口导航，其中合法性和舒适性等多种人为考虑因素常常发生冲突。目前的自动驾驶汽车规划系统通常依赖于僵化的规则，难以平衡这些相互竞争的考虑因素，并可能导致行为与人类期望不符。

Method: 该框架将人类行为者（如合规性）的原因建模为可量化的函数，并评估候选的自动驾驶汽车轨迹与这些原因的匹配程度。通过为行为者优先级分配可调权重并集成一个平衡函数来阻止任何行为者的排除，该框架支持可解释的决策评估。

Result: 通过一个受现实启发的超车场景，我们展示了该方法如何揭示法规遵从性、效率和舒适性之间的张力。

Conclusion: 该框架作为现有规划算法之上的模块化评估层，为评估日常场景中的伦理一致性提供了一个透明的工具，并为在实际自动驾驶汽车部署中实施有意义的人类控制提供了切实可行的步骤。

Abstract: A key challenge in deploying automated vehicles (AVs) is ensuring they make
appropriate decisions in ethically challenging everyday driving situations.
While much attention has been paid to rare, high-stakes dilemmas such as
trolley problems, similar tensions also arise in routine scenarios, such as
navigating empty intersections, where multiple human considerations, including
legality and comfort, often conflict. Current AV planning systems typically
rely on rigid rules, which struggle to balance these competing considerations
and can lead to behaviour that misaligns with human expectations. This paper
proposes a novel reasons-based trajectory evaluation framework that
operationalises the tracking condition of Meaningful Human Control (MHC). The
framework models the reasons of human agents, such as regulatory compliance, as
quantifiable functions and evaluates how well candidate AV trajectories align
with these reasons. By assigning adjustable weights to agent priorities and
integrating a balance function to discourage the exclusion of any agent, the
framework supports interpretable decision evaluation. Through a
real-world-inspired overtaking scenario, we show how this approach reveals
tensions, for instance between regulatory compliance, efficiency, and comfort.
The framework functions as a modular evaluation layer over existing planning
algorithms. It offers a transparent tool for assessing ethical alignment in
everyday scenarios and provides a practical step toward implementing MHC in
real-world AV deployment.

</details>


### [362] [Learning to Drift with Individual Wheel Drive: Maneuvering Autonomous Vehicle at the Handling Limits](https://arxiv.org/abs/2507.23339)
*Yihan Zhou,Yiwen Lu,Bo Yang,Jiayun Li,Yilin Mo*

Main category: cs.RO

TL;DR: 提出了一种强化学习框架，利用GPU加速模拟和域随机化来解决漂移控制中的仿真到现实差距问题，并在仿真和实际车辆上都取得了成功。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在漂移控制中存在的仿真到现实的差距问题，即在仿真中表现良好的策略在转移到物理系统时常常会失败。

Method: 提出了一种具有GPU加速并行模拟和系统域随机化的强化学习框架，以有效缩小仿真与现实之间的差距。

Result: 该方法在仿真和实际的1/10比例独立轮驱动（IWD）遥控赛车平台上进行了验证，并在各种场景下（从稳态圆周漂移到方向转换和变曲率路径跟踪）实现了精确的轨迹跟踪和受控的侧滑角。

Conclusion: 该方法在模拟和现实世界环境中都实现了精确的轨迹跟踪，并在复杂操作中始终保持受控的侧滑角。

Abstract: Drifting, characterized by controlled vehicle motion at high sideslip angles,
is crucial for safely handling emergency scenarios at the friction limits.
While recent reinforcement learning approaches show promise for drifting
control, they struggle with the significant simulation-to-reality gap, as
policies that perform well in simulation often fail when transferred to
physical systems. In this paper, we present a reinforcement learning framework
with GPU-accelerated parallel simulation and systematic domain randomization
that effectively bridges the gap. The proposed approach is validated on both
simulation and a custom-designed and open-sourced 1/10 scale Individual Wheel
Drive (IWD) RC car platform featuring independent wheel speed control.
Experiments across various scenarios from steady-state circular drifting to
direction transitions and variable-curvature path following demonstrate that
our approach achieves precise trajectory tracking while maintaining controlled
sideslip angles throughout complex maneuvers in both simulated and real-world
environments.

</details>


### [363] [Multi-Waypoint Path Planning and Motion Control for Non-holonomic Mobile Robots in Agricultural Applications](https://arxiv.org/abs/2507.23350)
*Mahmoud Ghorab,Matthias Lorenzen*

Main category: cs.RO

TL;DR: 本文提出了一种结合DTSP和NMPC的导航框架，用于在农业环境中进行高效自主导航，并在模拟中显示出比现有方法更好的性能。


<details>
  <summary>Details</summary>
Motivation: 为了满足在非结构化农业环境中进行导航的自主移动机器人的日益增长的需求，例如在草地上进行杂草控制，需要高效地规划穿越无序坐标集路径，同时最小化行进距离并遵守曲率限制以防止土壤破坏和保护植被。

Method: 该文提出了一种集成的导航框架，结合了基于Dubins旅行商问题（DTSP）的全局路径规划和非线性模型预测控制（NMPC）的局部路径规划与控制。DTSP生成最短、曲率受限的路径，NMPC利用该路径计算控制信号以精确到达每个航点。

Result: 与解耦方法相比，所提出的DTSP耦合规划器在模拟分析中显示出更平滑、更短的路径，在所提供的场景中减少了约16%的路径长度。NMPC控制器有效地将机器人引导至期望的航点，同时在局部优化轨迹并确保遵守约束。

Conclusion: 该框架有潜力用于农业环境中的高效自主导航。

Abstract: There is a growing demand for autonomous mobile robots capable of navigating
unstructured agricultural environments. Tasks such as weed control in meadows
require efficient path planning through an unordered set of coordinates while
minimizing travel distance and adhering to curvature constraints to prevent
soil damage and protect vegetation. This paper presents an integrated
navigation framework combining a global path planner based on the Dubins
Traveling Salesman Problem (DTSP) with a Nonlinear Model Predictive Control
(NMPC) strategy for local path planning and control. The DTSP generates a
minimum-length, curvature-constrained path that efficiently visits all targets,
while the NMPC leverages this path to compute control signals to accurately
reach each waypoint. The system's performance was validated through comparative
simulation analysis on real-world field datasets, demonstrating that the
coupled DTSP-based planner produced smoother and shorter paths, with a
reduction of about 16% in the provided scenario, compared to decoupled methods.
Based thereon, the NMPC controller effectively steered the robot to the desired
waypoints, while locally optimizing the trajectory and ensuring adherence to
constraints. These findings demonstrate the potential of the proposed framework
for efficient autonomous navigation in agricultural environments.

</details>


### [364] [Quantifying and Visualizing Sim-to-Real Gaps: Physics-Guided Regularization for Reproducibility](https://arxiv.org/abs/2507.23445)
*Yuta Kawachi*

Main category: cs.RO

TL;DR: 通过物理引导的增益正则化和参数条件化，有效缩小了机器人控制的sim-to-real差距，即使在高齿轮比机器人上也能取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 传统的基于域随机化的sim-to-real迁移方法在低齿轮比、可反向驱动的执行器上效果较好，但在sim-to-real差距较大时会失效。本研究旨在解决这一问题，特别是针对非低齿轮比的机器人。

Method: 提出了一种物理引导的增益正则化方案，通过简单现实世界实验测量机器人有效比例增益，并在训练期间惩罚神经网络控制器局部输入输出灵敏度与这些值的偏差。同时，为了避免域随机化的保守偏差，控制器还根据当前被控对象参数进行条件化。

Result: 在实际的轮式自平衡机器人硬件上，本研究提出的增益正则化、参数条件化RNN控制器实现了接近仿真的角度稳定时间，而纯粹的域随机化策略则存在持续振荡和显著的sim-to-real差距。

Conclusion: 本研究提出的增益正则化、参数条件化RNN方法能够有效缩小机器人控制的sim-to-real差距，在硬件上实现了与仿真相近的角速度稳定时间，而纯粹的域随机化方法在此类机器人上表现不佳。

Abstract: Simulation-to-real transfer using domain randomization for robot control
often relies on low-gear-ratio, backdrivable actuators, but these approaches
break down when the sim-to-real gap widens. Inspired by the traditional PID
controller, we reinterpret its gains as surrogates for complex, unmodeled plant
dynamics. We then introduce a physics-guided gain regularization scheme that
measures a robot's effective proportional gains via simple real-world
experiments. Then, we penalize any deviation of a neural controller's local
input-output sensitivities from these values during training. To avoid the
overly conservative bias of naive domain randomization, we also condition the
controller on the current plant parameters. On an off-the-shelf two-wheeled
balancing robot with a 110:1 gearbox, our gain-regularized,
parameter-conditioned RNN achieves angular settling times in hardware that
closely match simulation. At the same time, a purely domain-randomized policy
exhibits persistent oscillations and a substantial sim-to-real gap. These
results demonstrate a lightweight, reproducible framework for closing
sim-to-real gaps on affordable robotic hardware.

</details>


### [365] [H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation](https://arxiv.org/abs/2507.23523)
*Hongzhe Bi,Lingxuan Wu,Tianwei Lin,Hengkai Tan,Zhizhong Su,Hang Su,Jun Zhu*

Main category: cs.RO

TL;DR: Human manipulation data, when leveraged through a two-stage training approach with a diffusion transformer (H-RDT), significantly improves robotic manipulation policies compared to existing methods and training from scratch.


<details>
  <summary>Details</summary>
Motivation: The fundamental challenge of data scarcity in imitation learning for robotic manipulation is addressed. Existing foundation models struggle with diverse robot embodiments, prompting the exploration of human manipulation data as a richer source of behavioral priors.

Method: A two-stage training paradigm is introduced: (1) pre-training on large-scale egocentric human manipulation data, and (2) cross-embodiment fine-tuning on robot-specific data using modular action encoders and decoders. The approach, named H-RDT, utilizes a 2B parameter diffusion transformer architecture and flow matching to model action distributions.

Result: H-RDT demonstrated significant improvements over training from scratch (13.9% in simulation, 40.5% in real-world) and state-of-the-art methods like Pi0 and RDT across various evaluations including single-task, multitask, few-shot learning, and robustness.

Conclusion: The study validates the hypothesis that human manipulation data can serve as a foundation for learning bimanual robotic manipulation policies, outperforming existing methods.

Abstract: Imitation learning for robotic manipulation faces a fundamental challenge:
the scarcity of large-scale, high-quality robot demonstration data. Recent
robotic foundation models often pre-train on cross-embodiment robot datasets to
increase data scale, while they face significant limitations as the diverse
morphologies and action spaces across different robot embodiments make unified
training challenging. In this paper, we present H-RDT (Human to Robotics
Diffusion Transformer), a novel approach that leverages human manipulation data
to enhance robot manipulation capabilities. Our key insight is that large-scale
egocentric human manipulation videos with paired 3D hand pose annotations
provide rich behavioral priors that capture natural manipulation strategies and
can benefit robotic policy learning. We introduce a two-stage training
paradigm: (1) pre-training on large-scale egocentric human manipulation data,
and (2) cross-embodiment fine-tuning on robot-specific data with modular action
encoders and decoders. Built on a diffusion transformer architecture with 2B
parameters, H-RDT uses flow matching to model complex action distributions.
Extensive evaluations encompassing both simulation and real-world experiments,
single-task and multitask scenarios, as well as few-shot learning and
robustness assessments, demonstrate that H-RDT outperforms training from
scratch and existing state-of-the-art methods, including Pi0 and RDT, achieving
significant improvements of 13.9% and 40.5% over training from scratch in
simulation and real-world experiments, respectively. The results validate our
core hypothesis that human manipulation data can serve as a powerful foundation
for learning bimanual robotic manipulation policies.

</details>


### [366] [A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving](https://arxiv.org/abs/2507.23540)
*Yi Zhang,Erik Leo Haß,Kuo-Yi Chao,Nenad Petrovic,Yinglei Song,Chengdong Wu,Alois Knoll*

Main category: cs.RO

TL;DR: 提出了一种集成了多传感器融合和GPT-4.1大型语言模型的感知-语言-动作（PLA）框架，用于自动驾驶。该框架通过紧密结合感知、语言理解和决策，提高了上下文感知、可解释性和安全性，并在城市交叉口场景的评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 自主驾驶系统在实现人类般的适应性、鲁棒性和可解释性方面面临严峻挑战，尤其是在复杂、开放的世界环境中。这些挑战源于碎片化的架构、对新场景的泛化能力有限以及从感知中提取的语义不足。

Method: 提出了一种统一的感知-语言-动作（PLA）框架，该框架集成了多传感器融合（摄像头、激光雷达、雷达）和一个大型语言模型（LLM）增强的视觉-语言-动作（VLA）架构，特别是GPT-4.1驱动的推理核心。该框架将低级传感处理与高级上下文推理统一起来，将感知与基于自然语言的语义理解和决策紧密结合，以实现上下文感知、可解释和安全约束的自动驾驶。

Result: 在城市交叉口场景（包含施工区）的评估中，在轨迹跟踪、速度预测和自适应规划方面表现优越。

Conclusion: 该框架展示了语言增强的认知框架在提高自动驾驶系统的安全性、可解释性和可扩展性方面的潜力。

Abstract: Autonomous driving systems face significant challenges in achieving
human-like adaptability, robustness, and interpretability in complex,
open-world environments. These challenges stem from fragmented architectures,
limited generalization to novel scenarios, and insufficient semantic extraction
from perception. To address these limitations, we propose a unified
Perception-Language-Action (PLA) framework that integrates multi-sensor fusion
(cameras, LiDAR, radar) with a large language model (LLM)-augmented
Vision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered
reasoning core. This framework unifies low-level sensory processing with
high-level contextual reasoning, tightly coupling perception with natural
language-based semantic understanding and decision-making to enable
context-aware, explainable, and safety-bounded autonomous driving. Evaluations
on an urban intersection scenario with a construction zone demonstrate superior
performance in trajectory tracking, speed prediction, and adaptive planning.
The results highlight the potential of language-augmented cognitive frameworks
for advancing the safety, interpretability, and scalability of autonomous
driving systems.

</details>


### [367] [User Experience Estimation in Human-Robot Interaction Via Multi-Instance Learning of Multimodal Social Signals](https://arxiv.org/abs/2507.23544)
*Ryo Miyoshi,Yuki Okafuji,Takuya Iwamoto,Junya Nakanishi,Jun Baba*

Main category: cs.RO

TL;DR: 本研究提出了一种基于多模态社交信号（面部表情、声音）和多实例学习的Transformer模型，用于更全面地估计人机交互中的用户体验（UX），并在实验中超越了人工评估。


<details>
  <summary>Details</summary>
Motivation: 为了满足社会机器人根据用户状态调整行为的需求，准确评估人机交互（HCI）中的用户体验（UX）至关重要。现有的UX评估方法往往只关注单一维度，而UX本身是一个包含情感和参与度等多个方面、更为复杂和多层面的指标。

Method: 本研究提出了一种基于Transformer和多实例学习框架的用户体验（UX）估计方法，该方法利用面部表情和声音等多模态社交信号，并能够捕捉短期和长期交互模式。

Result: 实验结果表明，本研究提出的方法在用户体验（UX）估计方面优于第三方人工评估者。

Conclusion: 本研究提出的方法通过利用面部表情和声音等多模态社交信号，并结合多实例学习框架来捕捉短期和长期交互模式，实现了比第三方人工评估者更准确的用户体验（UX）估计，为社会机器人的行为自适应提供了基础。

Abstract: In recent years, the demand for social robots has grown, requiring them to
adapt their behaviors based on users' states. Accurately assessing user
experience (UX) in human-robot interaction (HRI) is crucial for achieving this
adaptability. UX is a multi-faceted measure encompassing aspects such as
sentiment and engagement, yet existing methods often focus on these
individually. This study proposes a UX estimation method for HRI by leveraging
multimodal social signals. We construct a UX dataset and develop a
Transformer-based model that utilizes facial expressions and voice for
estimation. Unlike conventional models that rely on momentary observations, our
approach captures both short- and long-term interaction patterns using a
multi-instance learning framework. This enables the model to capture temporal
dynamics in UX, providing a more holistic representation. Experimental results
demonstrate that our method outperforms third-party human evaluators in UX
estimation.

</details>


### [368] [Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study](https://arxiv.org/abs/2507.23589)
*Kai Goebel,Patrik Zips*

Main category: cs.RO

TL;DR: 大型语言模型在机器人规划方面有潜力，但在处理复杂任务时仍有局限性，未来研究应探索结合语言模型和经典规划器的方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在机器人任务规划方面的潜力日益受到关注，本研究旨在系统地评估它们在此类任务中的有效性，特别是它们生成结构化和可执行计划的能力，并与传统的规划器进行比较。

Method: 该研究系统地评估了各种最新的语言模型，使用规划领域定义语言（PDDL）领域和问题文件直接提示它们，并将其规划性能与 Fast Downward 规划器在各种基准测试中进行了比较。除了成功率测量外，研究还评估了生成计划转化为可执行动作序列的忠实度。

Result: 在简单规划任务中，语言模型表现良好，但在需要精确资源管理、一致状态跟踪和严格约束遵守的复杂场景中，它们仍然面临挑战。生成的计划在实际执行中会暴露出一些问题。

Conclusion: 大型语言模型在处理需要精确资源管理、一致状态跟踪和严格约束遵守的复杂场景时仍然存在挑战，这表明将它们应用于真实世界的机器人规划存在基本困难。该研究旨在通过概述执行中的差距，指导未来研究将语言模型与经典规划器相结合，以提高自主机器人规划的可靠性和可扩展性。

Abstract: Recent advancements in Large Language Models have sparked interest in their
potential for robotic task planning. While these models demonstrate strong
generative capabilities, their effectiveness in producing structured and
executable plans remains uncertain. This paper presents a systematic evaluation
of a broad spectrum of current state of the art language models, each directly
prompted using Planning Domain Definition Language domain and problem files,
and compares their planning performance with the Fast Downward planner across a
variety of benchmarks. In addition to measuring success rates, we assess how
faithfully the generated plans translate into sequences of actions that can
actually be executed, identifying both strengths and limitations of using these
models in this setting. Our findings show that while the models perform well on
simpler planning tasks, they continue to struggle with more complex scenarios
that require precise resource management, consistent state tracking, and strict
constraint compliance. These results underscore fundamental challenges in
applying language models to robotic planning in real world environments. By
outlining the gaps that emerge during execution, we aim to guide future
research toward combined approaches that integrate language models with
classical planners in order to enhance the reliability and scalability of
planning in autonomous robotics.

</details>


### [369] [Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation](https://arxiv.org/abs/2507.23592)
*Haiyun Zhang,Stefano Dalla Gasperina,Saad N. Yousaf,Toshimitsu Tsuboi,Tetsuya Narita,Ashish D. Deshpande*

Main category: cs.RO

TL;DR: 通过优化的校准方法，提高手部外骨骼跟踪的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 手部外骨骼在灵巧遥操作和沉浸式操作接口中至关重要，但由于用户特定的解剖结构变异性和穿戴不一致性，实现精确的手部跟踪仍然是一个挑战。这些问题导致运动学失准，从而降低了跟踪性能并限制了其在精确任务中的应用。

Method: 提出了一种针对外骨骼手部跟踪的主题特定校准框架，该框架使用冗余关节传感和残差加权优化策略来估计虚拟链接参数。通过运动捕捉数据驱动地调整成本函数权重，以实现跨参与者的更准确、更一致的校准。

Result: 在 Maestro 外骨骼上实现的方法，提高了不同手部几何结构用户的关节角度和指尖位置估计精度。与未经校准和加权均衡的模型相比，在七个受试者上的定量结果显示，关节和指尖跟踪误差有了显著降低。

Conclusion: 该框架具有通用性，适用于具有闭环运动学和最小传感器的外骨骼设计，并为高保真遥操作和从演示中学习应用程序奠定了基础。

Abstract: Hand exoskeletons are critical tools for dexterous teleoperation and
immersive manipulation interfaces, but achieving accurate hand tracking remains
a challenge due to user-specific anatomical variability and donning
inconsistencies. These issues lead to kinematic misalignments that degrade
tracking performance and limit applicability in precision tasks. We propose a
subject-specific calibration framework for exoskeleton-based hand tracking that
uses redundant joint sensing and a residual-weighted optimization strategy to
estimate virtual link parameters. Implemented on the Maestro exoskeleton, our
method improves joint angle and fingertip position estimation across users with
varying hand geometries. We introduce a data-driven approach to empirically
tune cost function weights using motion capture ground truth, enabling more
accurate and consistent calibration across participants. Quantitative results
from seven subjects show substantial reductions in joint and fingertip tracking
errors compared to uncalibrated and evenly weighted models. Qualitative
visualizations using a Unity-based virtual hand further confirm improvements in
motion fidelity. The proposed framework generalizes across exoskeleton designs
with closed-loop kinematics and minimal sensing, and lays the foundation for
high-fidelity teleoperation and learning-from-demonstration applications.

</details>


### [370] [DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching](https://arxiv.org/abs/2507.23629)
*Yewei Huang,John McConnell,Xi Lin,Brendan Englot*

Main category: cs.RO

TL;DR: DRACo-SLAM2 is a new SLAM framework for underwater robots that uses object graphs and improved scan matching (GCM) for better loop closure detection, validated with simulations and real-world data.


<details>
  <summary>Details</summary>
Motivation: To improve SLAM for underwater robot teams by addressing the challenges of time-efficient loop closure detection and scan matching without relying on prior geometric information, and to handle scenarios with similar registration errors in loop closures.

Method: The framework uses object graphs and object graph matching for inter-robot loop closure detection, and incremental Group-wise Consistent Measurement Set Maximization (GCM) for scan matching.

Result: The proposed approach is validated through extensive comparative analyses on simulated and real-world datasets, demonstrating its effectiveness.

Conclusion: The paper presents DRACo-SLAM2, a distributed SLAM framework for underwater robot teams, which utilizes object graphs and GCM for improved loop closure detection and scan matching.

Abstract: We present DRACo-SLAM2, a distributed SLAM framework for underwater robot
teams equipped with multibeam imaging sonar. This framework improves upon the
original DRACo-SLAM by introducing a novel representation of sonar maps as
object graphs and utilizing object graph matching to achieve time-efficient
inter-robot loop closure detection without relying on prior geometric
information. To better-accommodate the needs and characteristics of underwater
scan matching, we propose incremental Group-wise Consistent Measurement Set
Maximization (GCM), a modification of Pairwise Consistent Measurement Set
Maximization (PCM), which effectively handles scenarios where nearby
inter-robot loop closures share similar registration errors. The proposed
approach is validated through extensive comparative analyses on simulated and
real-world datasets.

</details>


### [371] [DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios](https://arxiv.org/abs/2507.23660)
*Haoxuan Jiang,Peicong Qian,Yusen Xie,Xiaocong Li,Ming Liu,Jun Ma*

Main category: cs.RO

TL;DR: DuLoc enhances LiDAR localization for autonomous vehicles by combining LiDAR-inertial odometry with dynamic local maps, outperforming existing methods in changing outdoor environments based on extensive real-world testing.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR-based localization methods face challenges in balancing repeatability, accuracy, and environmental adaptability. Traditional methods relying on offline maps are not robust to long-term environmental changes, causing drift and reliability issues in dynamic real-world scenarios.

Method: The proposed DuLoc method integrates LiDAR-inertial odometry with offline map-based localization, utilizing a constant-velocity motion model to handle noise. It features a framework that combines a global map with real-time local maps for adaptability in changing environments.

Result: Extensive experiments involving 2,856 hours of data from 32 Intelligent Guided Vehicles (IGVs) in an ultra unbounded port show that DuLoc outperforms state-of-the-art LiDAR localization systems in large-scale, dynamic outdoor environments.

Conclusion: LiDAR-based localization is critical for autonomous systems, but existing methods struggle with repeatability, accuracy, and adaptability to environmental changes. DuLoc, a novel method, tightly couples LiDAR-inertial odometry with offline map-based localization, using a constant-velocity motion model for robustness against outliers. It integrates global and dynamic local maps for unbounded and changing environments. Experiments with 32 IGVs over 2,856 hours in a port demonstrate DuLoc's superiority over state-of-the-art LiDAR localization systems in large-scale, dynamic outdoor settings.

Abstract: LiDAR-based localization serves as a critical component in autonomous
systems, yet existing approaches face persistent challenges in balancing
repeatability, accuracy, and environmental adaptability. Traditional point
cloud registration methods relying solely on offline maps often exhibit limited
robustness against long-term environmental changes, leading to localization
drift and reliability degradation in dynamic real-world scenarios. To address
these challenges, this paper proposes DuLoc, a robust and accurate localization
method that tightly couples LiDAR-inertial odometry with offline map-based
localization, incorporating a constant-velocity motion model to mitigate
outlier noise in real-world scenarios. Specifically, we develop a LiDAR-based
localization framework that seamlessly integrates a prior global map with
dynamic real-time local maps, enabling robust localization in unbounded and
changing environments. Extensive real-world experiments in ultra unbounded port
that involve 2,856 hours of operational data across 32 Intelligent Guided
Vehicles (IGVs) are conducted and reported in this study. The results attained
demonstrate that our system outperforms other state-of-the-art LiDAR
localization systems in large-scale changing outdoor environments.

</details>


### [372] [Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes](https://arxiv.org/abs/2507.23677)
*Xiaohan Li,Ziren Gong,Fabio Tosi,Matteo Poggi,Stefano Mattoccia,Dong Liu,Jun Wu*

Main category: cs.RO

TL;DR: BGS-SLAM是首个用于户外场景的双目3D高斯SLAM系统，仅使用RGB立体图像对，无需激光雷达或主动传感器，并在实验中展示了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS-SLAM系统主要集中在室内环境并依赖于主动深度传感器，未能满足大规模户外应用的需求。

Method: BGS-SLAM利用预训练的深度立体网络进行深度估计，并通过多损失策略优化3D高斯，以提高几何一致性和视觉质量。

Result: BGS-SLAM在多个数据集上的实验表明，与现有的3DGS-SLAM方法相比，它在户外场景下实现了更高的跟踪和建图性能。

Conclusion: BGS-SLAM在复杂的户外环境中实现了优于其他基于3DGS的解决方案的跟踪精度和建图性能。

Abstract: 3D Gaussian Splatting (3DGS) has recently gained popularity in SLAM
applications due to its fast rendering and high-fidelity representation.
However, existing 3DGS-SLAM systems have predominantly focused on indoor
environments and relied on active depth sensors, leaving a gap for large-scale
outdoor applications. We present BGS-SLAM, the first binocular 3D Gaussian
Splatting SLAM system designed for outdoor scenarios. Our approach uses only
RGB stereo pairs without requiring LiDAR or active sensors. BGS-SLAM leverages
depth estimates from pre-trained deep stereo networks to guide 3D Gaussian
optimization with a multi-loss strategy enhancing both geometric consistency
and visual quality. Experiments on multiple datasets demonstrate that BGS-SLAM
achieves superior tracking accuracy and mapping performance compared to other
3DGS-based solutions in complex outdoor environments.

</details>


### [373] [villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models](https://arxiv.org/abs/2507.23682)
*Xiaoyu Chen,Hangxing Wei,Pushi Zhang,Chuheng Zhang,Kaixin Wang,Yanjiang Guo,Rushuai Yang,Yucen Wang,Xinquan Xiao,Li Zhao,Jianyu Chen,Jiang Bian*

Main category: cs.RO

TL;DR: villa-X is a new framework that improves robot manipulation policies by better modeling latent actions within Visual-Language-Action models, showing strong results in simulations and real-world tests.


<details>
  <summary>Details</summary>
Motivation: Visual-Language-Action (VLA) models are popular for learning robot manipulation policies that follow language instructions and generalize to novel scenarios. Recent work has explored incorporating latent actions, an abstract representation of visual change between two frames, into VLA pre-training. This paper aims to advance latent action modeling for more generalizable robot manipulation policies.

Method: This paper introduces villa-X, a novel Visual-Language-Latent-Action (ViLLA) framework that improves latent action modeling for learning generalizable robot manipulation policies by advancing both how latent actions are learned and how they are incorporated into VLA pre-training.

Result: villa-X achieves superior performance across simulated environments (SIMPLER and LIBERO) and two real-world robot setups (gripper and dexterous hand manipulation).

Conclusion: villa-X

Abstract: Visual-Language-Action (VLA) models have emerged as a popular paradigm for
learning robot manipulation policies that can follow language instructions and
generalize to novel scenarios. Recent work has begun to explore the
incorporation of latent actions, an abstract representation of visual change
between two frames, into VLA pre-training. In this paper, we introduce villa-X,
a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent
action modeling for learning generalizable robot manipulation policies. Our
approach improves both how latent actions are learned and how they are
incorporated into VLA pre-training. Together, these contributions enable
villa-X to achieve superior performance across simulated environments including
SIMPLER and LIBERO, as well as on two real-world robot setups including gripper
and dexterous hand manipulation. We believe the ViLLA paradigm holds
significant promise, and that our villa-X provides a strong foundation for
future research.

</details>


### [374] [Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents](https://arxiv.org/abs/2507.23698)
*Shaofei Cai,Zhancun Mu,Haiwen Xia,Bowei Zhang,Anji Liu,Yitao Liang*

Main category: cs.RO

TL;DR: RL通过自动化任务合成和跨视图目标规范，提升了Minecraft中视运动代理的可泛化性，实现了跨环境的零样本空间推理，成功率提高4倍。


<details>
  <summary>Details</summary>
Motivation: RL在语言建模方面取得了成功，但尚未完全转化为视运动代理。RL模型容易过拟合特定任务或环境，阻碍了在不同环境中获得可泛化的行为。因此，需要探索RL在提升3D世界中可泛化的空间推理和交互能力方面的潜力。

Method: 1. 提出跨视图目标规范作为统一的多任务目标空间，以解决多任务RL表示的挑战。 2. 提出自动化任务合成，以克服手动任务设计的瓶颈，用于大规模多任务RL训练。 3. 构建了一个高效的分布式RL框架来支持自动化任务合成。

Result: RL微调的视运动代理在Minecraft中实现了对未见世界的零样本泛化。实验结果表明，RL显著将交互成功率提高了4倍，并实现了跨不同环境（包括真实世界设置）的空间推理的零样本泛化。

Conclusion: RL训练可以显著提高Minecraft中视运动代理的空间推理能力，并实现跨不同环境的零样本泛化，尤其是在可以大规模生成任务的3D模拟环境中。

Abstract: While Reinforcement Learning (RL) has achieved remarkable success in language
modeling, its triumph hasn't yet fully translated to visuomotor agents. A
primary challenge in RL models is their tendency to overfit specific tasks or
environments, thereby hindering the acquisition of generalizable behaviors
across diverse settings. This paper provides a preliminary answer to this
challenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can
achieve zero-shot generalization to unseen worlds. Specifically, we explore
RL's potential to enhance generalizable spatial reasoning and interaction
capabilities in 3D worlds. To address challenges in multi-task RL
representation, we analyze and establish cross-view goal specification as a
unified multi-task goal space for visuomotor policies. Furthermore, to overcome
the significant bottleneck of manual task design, we propose automated task
synthesis within the highly customizable Minecraft environment for large-scale
multi-task RL training, and we construct an efficient distributed RL framework
to support this. Experimental results show RL significantly boosts interaction
success rates by $4\times$ and enables zero-shot generalization of spatial
reasoning across diverse environments, including real-world settings. Our
findings underscore the immense potential of RL training in 3D simulated
environments, especially those amenable to large-scale task generation, for
significantly advancing visuomotor agents' spatial reasoning.

</details>


### [375] [Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation](https://arxiv.org/abs/2507.23719)
*Parker McDonnell,Lingsheng Meng,Hari Krishna Hariprasad,Alexander Hedrick,Eduardo Miscles,Samuel Gilinsky,Jean-Michel Mongeau,Kaushik Jayaram*

Main category: cs.RO

TL;DR: CITRAS 是一种受蟑螂启发的、低功耗、低重量的触觉机器人天线传感器，它能够通过其柔顺的层压结构和嵌入式电容式角度传感器来感知触觉信息，以用于昆虫规模的机器人。


<details>
  <summary>Details</summary>
Motivation: 由于尺寸、重量和功耗的限制，在昆虫规模的机器人中复制自然系统中的触觉传感能力仍然是一个挑战。

Method: CITRAS 是一种受仿生启发的、多节的、柔顺的层压传感器，带有嵌入式的电容式角度传感器。

Result: CITRAS 实现了精确的铰链角度测量，准静态弯曲的最大误差仅为 0.79 度，动态弯曲为 3.58 度。它还能以 7.75% 的误差预测基部到尖端的距离，以 6.73% 的误差估计环境间隙宽度，并通过差分传感器响应来区分表面纹理。

Conclusion: CITRAS 在昆虫尺度机器人中具有增强自主探索、避障和复杂、封闭环境中环境测绘的潜力。

Abstract: The American cockroach (Periplaneta americana) uses its soft antennae to
guide decision making by extracting rich tactile information from tens of
thousands of distributed mechanosensors. Although tactile sensors enable
robust, autonomous perception and navigation in natural systems, replicating
these capabilities in insect-scale robots remains challenging due to stringent
size, weight, and power constraints that limit existing sensor technologies. To
overcome these limitations, we introduce CITRAS (Cockroach Inspired Tactile
Robotic Antenna Sensor), a bioinspired, multi-segmented, compliant laminate
sensor with embedded capacitive angle sensors. CITRAS is compact (73.7x15.6x2.1
mm), lightweight (491 mg), and low-power (32 mW), enabling seamless integration
with miniature robotic platforms. The segmented compliant structure passively
bends in response to environmental stimuli, achieving accurate hinge angle
measurements with maximum errors of just 0.79 degree (quasistatic bending) and
3.58 degree (dynamic bending). Experimental evaluations demonstrate CITRAS'
multifunctional tactile perception capabilities: predicting base-to-tip
distances with 7.75 % error, estimating environmental gap widths with 6.73 %
error, and distinguishing surface textures through differential sensor
response. The future integration of this bioinspired tactile antenna in
insect-scale robots addresses critical sensing gaps, promising enhanced
autonomous exploration, obstacle avoidance, and environmental mapping in
complex, confined environments.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [376] [Dynamical freezing and enhanced magnetometry in an interacting spin ensemble](https://arxiv.org/abs/2507.22982)
*Ya-Nan Lu,Dong Yuan,Yixuan Ma,Yan-Qing Liu,Si Jiang,Xiang-Qian Meng,Yi-Jie Xu,Xiu-Ying Chang,Chong Zu,Hong-Zheng Zhao,Dong-Ling Deng,Lu-Ming Duan,Pan-Yu Hou*

Main category: quant-ph

TL;DR: 该研究首次在实验中观测到了量子多体系统中一种名为“动力学冻结”的新现象，这种现象能够阻止系统热化并保持其信息。研究人员利用氮-空位自旋系综，通过精确控制其动力学行为，实现了远超常规相干时间的稳定信号，并以此开发出一种灵敏度更高的磁场测量技术，为量子精密测量提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 在量子多体系统中理解和控制非平衡动力学是现代物理学的基本挑战，这对于推进量子技术具有深远影响。通常，在没有守恒律的情况下，周期性驱动系统会热化到一个无特征的“无限温度”态，抹去其初始条件的所有记忆。然而，这种范式可能因为诸如可积性、多体局域化、量子多体疤痕和希尔伯特空间碎裂等机制而失效。本研究旨在探索一种新的热化破坏机制——动力学冻结，并将其应用于量子传感。

Method: 利用金刚石中的氮-空位（NV）自旋系综，通过精确控制驱动频率和失谐，实现对约$10^4$个相互作用自旋的控制。

Result: 研究观测到了动力学冻结现象，表现为长寿命的自旋磁化和相干振荡微运动，其持续时间超过了相互作用限制的相干时间（$T_2$）一个数量级以上。基于此，研究开发了一种动力学冻结增强的交流磁力测量技术，其最优传感时间远超$T_2$，并且在灵敏度上比传统的动力学解耦磁测量技术提高了4.3 dB。

Conclusion: 该研究首次通过实验观测到动力学冻结现象，这是一种通过涌现的守恒律来抵抗热化的机制。同时，研究还建立了一种鲁棒的控制方法，可广泛应用于其他物理平台，并在量子计量学等领域具有广泛的应用前景。

Abstract: Understanding and controlling non-equilibrium dynamics in quantum many-body
systems is a fundamental challenge in modern physics, with profound
implications for advancing quantum technologies. Typically, periodically driven
systems in the absence of conservation laws thermalize to a featureless
"infinite-temperature" state, erasing all memory of their initial conditions.
However, this paradigm can break down through mechanisms such as integrability,
many-body localization, quantum many-body scars, and Hilbert space
fragmentation. Here, we report the experimental observation of dynamical
freezing, a distinct mechanism of thermalization breakdown in driven systems,
and demonstrate its application in quantum sensing using an ensemble of
approximately $10^4$ interacting nitrogen-vacancy spins in diamond. By
precisely controlling the driving frequency and detuning, we observe emergent
long-lived spin magnetization and coherent oscillatory micromotions, persisting
over timescales exceeding the interaction-limited coherence time ($T_2$) by
more than an order of magnitude. Leveraging these unconventional dynamics, we
develop a dynamical-freezing-enhanced ac magnetometry that extends optimal
sensing times far beyond $T_2$, outperforming conventional dynamical decoupling
magnetometry with a 4.3 dB sensitivity enhancement. Our results not only
provide clear experimental observation of dynamical freezing -- a peculiar
mechanism defying thermalization through emergent conservation laws -- but also
establish a robust control method generally applicable to diverse physical
platforms, with broad implications in quantum metrology and beyond.

</details>


### [377] [Field digitization scaling in a $\mathbb{Z}_N \subset U(1)$ symmetric model](https://arxiv.org/abs/2507.22984)
*Gabriele Calliari,Robert Ott,Hannes Pichler,Torsten V. Zache*

Main category: quant-ph

TL;DR: 该研究将场论数字化（FD）参数N作为重整化群（RG）耦合，提出场数字化标度（FDS）方法，以获得连续极限。通过研究二维N态时钟模型，发现了由有限N引起的普遍交叉现象，并将该模型与(2+1)D $\mathbb{Z}_N$格点规范场论联系起来，为模拟更复杂的量子场论提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 为了解决场论数字化（FD）中，将局部场截断为N个离散值后，缺乏获得连续极限的完整框架这一问题。

Method: 提出将场论数字化（FD）中的参数N解释为重整化群（RG）中的耦合，并引入场数字化标度（FDS）方法，利用RG推导出涉及FD参数N的广义标度假设。通过数值张量网络计算（有限键维度χ）来研究二维N态时钟模型，并分析有限N引起的相变和由χ与N相互作用引起的普遍交叉现象。最后，对二维经典统计$\\mathbb{Z}_N$时钟模型进行解析证明，以揭示其与(2+1)D $\\mathbb{Z}_N$格点规范场论基态量子物理的联系。

Result: 提出了场论数字化（FD）的重整化群（RG）解释，并引入了场数字化标度（FDS）方法，用于分析FD参数N与连续极限的关系。通过对二维N态时钟模型的研究，揭示了FD参数N作为耦合的RG性质，并发现了由有限N引起的非传统普遍交叉现象。该方法能够将不同N值下的数据关联起来，并可扩展到描述χ和N的相互作用。此外，研究还证明了二维经典统计$\mathbb{Z}_N$时钟模型与(2+1)D $\mathbb{Z}_N$格点规范场论基态的量子物理之间的直接联系。

Conclusion: 该研究提出了场论数字化（FD）的重整化群（RG）解释，并引入了场数字化标度（FDS）方法，用于分析FD参数N与连续极限的关系。通过对二维N态时钟模型的研究，揭示了FD参数N作为耦合的RG性质，并发现了由有限N引起的非传统普遍交叉现象。该方法能够将不同N值下的数据关联起来，并可扩展到描述χ和N的相互作用。此外，研究还证明了二维经典统计$\mathbb{Z}_N$时钟模型与(2+1)D $\mathbb{Z}_N$格点规范场论基态的量子物理之间的直接联系，为FDS在更高维度和更复杂模型中的应用奠定了基础。

Abstract: The simulation of quantum field theories, both classical and quantum,
requires regularization of infinitely many degrees of freedom. However, in the
context of field digitization (FD) -- a truncation of the local fields to $N$
discrete values -- a comprehensive framework to obtain continuum results is
currently missing. Here, we propose to analyze FD by interpreting the parameter
$N$ as a coupling in the renormalization group (RG) sense. As a first example,
we investigate the two-dimensional classical $N$-state clock model as a
$\mathbb{Z}_N$ FD of the $U(1)$-symmetric $XY$-model. Using effective field
theory, we employ the RG to derive generalized scaling hypotheses involving the
FD parameter $N$, which allows us to relate data obtained for different
$N$-regularized models in a procedure that we term $\textit{field digitization
scaling}$ (FDS). Using numerical tensor-network calculations at finite bond
dimension $\chi$, we further uncover an unconventional universal crossover
around a low-temperature phase transition induced by finite $N$, demonstrating
that FDS can be extended to describe the interplay of $\chi$ and $N$. Finally,
we analytically prove that our calculations for the 2D classical-statistical
$\mathbb{Z}_N$ clock model are directly related to the quantum physics in the
ground state of a (2+1)D $\mathbb{Z}_N$ lattice gauge theory which serves as a
FD of compact quantum electrodynamics. Our study thus paves the way for
applications of FDS to quantum simulations of more complex models in higher
spatial dimensions, where it could serve as a tool to analyze the continuum
limit of digitized quantum field theories.

</details>


### [378] [Majorization theory for quasiprobabilities](https://arxiv.org/abs/2507.22986)
*Twesh Upadhyaya,Zacharie Van Herstraeten,Jack Davis,Oliver Hahn,Nikolaos Koukoulekidis,Ulysse Chabaud*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Majorization theory is a powerful mathematical tool to compare the disorder
in distributions, with wide-ranging applications in many fields including
mathematics, physics, information theory, and economics. While majorization
theory typically focuses on probability distributions, quasiprobability
distributions provide a pivotal framework for advancing our understanding of
quantum mechanics, quantum information, and signal processing. Here, we
introduce a notion of majorization for continuous quasiprobability
distributions over infinite measure spaces. Generalizing a seminal theorem by
Hardy, Littlewood, and P\'olya, we prove the equivalence of four definitions
for both majorization and relative majorization in this setting. We give
several applications of our results in the context of quantum resource
theories, obtaining new families of resource monotones and no-goes for quantum
state conversions. A prominent example we explore is the Wigner function in
quantum optics. More generally, our results provide an extensive majorization
framework for assessing the disorder of integrable functions over infinite
measure spaces.

</details>


### [379] [Probing Bound State Relaxation Dynamics in Systems Out-of-Equilibrium on Quantum Computers](https://arxiv.org/abs/2507.22988)
*Heba A. Labib,Goksu Can Toga,J. K. Freericks,A. F. Kemper*

Main category: quant-ph

TL;DR: 本研究利用量子计算机模拟泵浦-探测实验，研究混合场伊辛模型中的非平衡动力学，发现了离散束缚态和 Bloch 振荡现象。


<details>
  <summary>Details</summary>
Motivation: 利用泵浦-探测光谱研究量子多体系统非平衡态的响应动力学。

Method: 使用量子计算机模拟泵浦-探测光谱，研究混合场伊辛模型，并分析离散束缚态及其演化、真空衰减以及 Bloch 振荡。

Result: 发现了混合场伊辛模型中的离散束缚态，并观察到真空衰减过程中出现 Bloch 振荡，导致了长寿命振荡。

Conclusion: 该研究为使用泵浦-探测实验在经典和量子计算机上模拟非平衡系统奠定了基础，无需额外的量子比特。

Abstract: Pump-probe spectroscopy is a powerful tool for probing response dynamics of
quantum many-body systems in and out-of-equilibrium. Quantum computers have
proved useful in simulating such experiments by exciting the system, evolving,
and then measuring observables to first order, all in one setting. Here, we use
this approach to investigate the mixed-field Ising model, where the
longitudinal field plays the role of a confining potential that prohibits the
spread of the excitations, spinons, or domain walls into space. We study the
discrete bound states that arise from such a setting and their evolution under
different quench dynamics by initially pumping the chain out of equilibrium and
then probing various non-equal time correlation functions. Finally, we study
false vacuum decay, where initially one expects unhindered propagation of the
ground state, or true vacuum, bubbles into the lattice, but instead sees the
emergence of Bloch oscillations that are directly the reason for the long-lived
oscillations in this finite-size model. Our work sets the stage for simulating
systems out-of-equilibrium on classical and quantum computers using pump-probe
experiments without needing ancillary qubits.

</details>


### [380] [Local-available quantum correlation swapping in one-parameter X states](https://arxiv.org/abs/2507.23142)
*Hermann L. Albrecht Q*

Main category: quant-ph

TL;DR: 研究了LAQC的量子中继，发现初始状态和测量状态之间的相关性会影响最终LAQC。


<details>
  <summary>Details</summary>
Motivation: 研究LAQC（一种有前途但未被充分研究的量子关联）的量子中继和交换协议，以扩展其在量子信息处理中的应用。

Method: 通过分析五个单参数二维X态族的LAQC量子中继，并与Bellorín等人获得的结果进行比较。

Result: 发现如果初始状态和用于投影测量的状态之间存在关联，则最终状态将具有非零的LAQC。

Conclusion: 该研究分析了LAQC的量子中继，并发现初始状态和用于投影测量的状态之间的相关性会影响最终状态的LAQC。

Abstract: Although introduced originally for entanglement, quantum repeaters and
swapping protocols have been analyzed for other quantum correlations (QC), such
as quantum discord. Introduced by Mundarain and Ladr\'on de Guevara,
local-available quantum correlations (LAQC) are a promising yet understudied
quantum correlation. Recently, Bellor\'{\i}n et al. obtained exact analytical
results for the LAQC quantifier of general 2-qubit X states. Starting from
those previous results, we analyzed the LAQC swapping for five families of
one-parameter 2-qubit X states. As expected, we find that if the initial state
and the one used for the projective measurement are correlated, the final state
will have non-zero LAQC.

</details>


### [381] [Improved Simulation of Asynchronous Entanglement Distribution in Noisy Quantum Networks](https://arxiv.org/abs/2507.22992)
*Emma Hughes,William Munizzi,Prineha Narang*

Main category: quant-ph

TL;DR: This paper presents a lightweight simulation framework to assess entanglement distribution protocols. The parallel protocol is shown to be more efficient than the sequential one, especially in terms of entanglement efficiency, making it a suitable choice for the future quantum internet.


<details>
  <summary>Details</summary>
Motivation: To evaluate the performance of asynchronous entanglement distribution protocols under realistic error models, providing an accessible and scalable tool for comparing different entanglement distribution strategies.

Method: A lightweight simulation framework was developed to evaluate asynchronous entanglement distribution protocols under realistic error models. The framework simulates sequential and parallel protocols, comparing their performance based on the fidelity of distributed entangled states and the hashing rate across various network sizes and noise parameters. The simulation reduces complex quantum processes to simple memory time calculations.

Result: The parallel protocol demonstrates superior performance compared to the sequential protocol, particularly in the hashing rate, which is attributed to its reduced runtime. The simulation framework successfully reduced the complexity of simulating quantum processes to memory time calculations.

Conclusion: The parallel protocol consistently outperforms the sequential protocol, especially in the hashing rate due to reduced runtime, indicating that parallel protocols are a promising candidate for a viable quantum internet.

Abstract: This work introduces a lightweight simulation framework for evaluating
asynchronous entanglement distribution protocols under realistic error models.
We focus on two contemporary protocols: sequential, where entanglement is
established one node at a time, and parallel, where all nodes attempt to
generate entanglement simultaneously. We evaluate the performance of each
protocol using two key metrics: the fidelity of distributed entangled states,
and the hashing rate, a measure of entanglement efficiency. These metrics are
compared between both protocols across a range of network sizes and noise
parameters. We demonstrate that the parallel protocol consistently outperforms
the sequential, particularly in the hashing rate metric due to reduced runtime,
suggesting that parallel protocols are a strong candidate for a realizable
quantum Internet. Our framework offers an accessible and scalable tool for
evaluating entanglement distribution strategies, by reducing the simulation of
complex quantum processes to simple memory time calculations.

</details>


### [382] [Asymptotically optimal joint phase and dephasing strength estimation using spin-squeezed states](https://arxiv.org/abs/2507.22997)
*Arkadiusz Kobus,Rafał Demkowicz-Dobrzański*

Main category: quant-ph

TL;DR: 提出了一种 N 粒子协议，可以同时进行相位和退相干强度估计，精度接近量子计量极限。该协议具有普适性，可应用于其他量子计量模型。


<details>
  <summary>Details</summary>
Motivation: 展示了一种可以同时进行相位和退相干强度估计的量子计量协议。

Method: 提出了一种涉及单轴旋转自旋压缩状态的 N 粒子协议。

Result: 该协议的精度渐近上匹配了基本量子计量界限，并且可以归约到此问题。该协议与此特定模型之外的其他模型相关，因为任何不相关的噪声量子计量模型都可以通过适当设计的量子误差校正程序归约到此问题。

Conclusion: 该协议可以同时进行相位和退相干强度估计，精度在渐近上能匹配基本量子计量界限。

Abstract: We show an explicit $N$-qubit protocol involving one-axis-twisted spin
squeezed states, that allows for simultaneous phase and dephasing strength
estimation with precision that asymptotically matches fundamental quantum
metrological bounds. The relevance of the protocol goes beyond this particular
model, since any uncorrelated noise quantum metrological model, that allows for
at most constant asymptotic quantum enhancement, can be reduced to this problem
via an appropriately tailored quantum error-correction procedure.

</details>


### [383] [Amplitude amplification and estimation require inverses](https://arxiv.org/abs/2507.23787)
*Ewin Tang,John Wright*

Main category: quant-ph

TL;DR: 量子搜索和计数的二次加速依赖于算法中操作的可逆性。对于量子学习、计量和传感等领域，操作的可逆性难以实现，导致量子加速效果受限，甚至不如经典算法。


<details>
  <summary>Details</summary>
Motivation: 为了理解为什么量子算法（如幅度放大和幅度估计）的二次加速（Grover加速）在某些领域（如量子学习、计量和传感）难以实现，尽管它们在搜索和计数问题中非常普遍。

Method: 该研究利用Zhandry引入的压缩Oracle方法，通过构造基于迹估计（trace estimation）的问题实例，证明了在无法有效执行逆向操作的情况下，量子算法（如幅度放大和幅度估计）无法超越经典的最优算法。

Result: 当用于加速算法的酉变换（unitary）U及其逆U†无法有效实现时，量子算法（如幅度放大和幅度估计）的加速效果会消失。对于基于迹估计的问题实例，没有仅使用U的算法能够超越朴素的二次加速方法。这表明量子加速存在一个二分法：没有逆向操作，量子加速稀少；有逆向操作，量子加速普遍存在。

Conclusion: 研究表明，通用量子搜索和计数算法的加速效果仅在可逆过程上成立。对于不可逆过程，如量子学习、计量和传感中的系统演化，这些算法的加速效果会大打折扣，甚至不如经典算法。这揭示了量子算法加速的一个重要限制：逆向操作的可行性。

Abstract: We prove that the generic quantum speedups for brute-force search and
counting only hold when the process we apply them to can be efficiently
inverted. The algorithms speeding up these problems, amplitude amplification
and amplitude estimation, assume the ability to apply a state preparation
unitary $U$ and its inverse $U^\dagger$; we give problem instances based on
trace estimation where no algorithm which uses only $U$ beats the naive,
quadratically slower approach. Our proof of this is simple and goes through the
compressed oracle method introduced by Zhandry. Since these two subroutines are
responsible for the ubiquity of the quadratic "Grover" speedup in quantum
algorithms, our result explains why such speedups are far harder to come by in
the settings of quantum learning, metrology, and sensing. In these settings,
$U$ models the evolution of an experimental system, so implementing $U^\dagger$
can be much harder -- tantamount to reversing time within the system. Our
result suggests a dichotomy: without inverse access, quantum speedups are
scarce; with it, quantum speedups abound.

</details>


### [384] [Detecting quantum non-Gaussianity with a single quadrature](https://arxiv.org/abs/2507.23005)
*Clara Wassner,Jack Davis,Sacha Cerf,Ulysse Chabaud,Francesco Arzani*

Main category: quant-ph

TL;DR: 通过单一测量即可检测量子态的非高斯性，简化了实验。


<details>
  <summary>Details</summary>
Motivation: 为了简化从测量样本中完全重建量子态的复杂性，研究人员寻求一种更容易的方法来认证与特定应用相关的量，重点关注简化的测量方法。

Method: 通过分析单个正交测量统计数据的零点来证明非高斯性，利用了哈德逊定理的一个版本，并得到了关于有界能量和有限星阶状态紧致性的技术结果的支持。

Result: 研究表明，单个正交测量的统计数据可以证明任意程度的非高斯性，这由星阶来量化。该方法对样本复杂性、噪声鲁棒性和实验前景进行了分析。

Conclusion: 该研究提出了一个简化的方法来检测量子态的非高斯性，只需对单个正交进行测量，大大简化了实验要求。

Abstract: Full reconstruction of quantum states from measurement samples is often a
prohibitively complex task, both in terms of the experimental setup and the
scaling of the sample size with the system. This motivates the relatively
easier task of certifying application-specific quantities using measurements
that are not tomographically complete, i.e. that provide only partial
information about the state related to the application of interest. Here, we
focus on simplifying the measurements needed to certify non-Gaussianity in
bosonic systems, a resource related to quantum advantage in various information
processing tasks. We show that the statistics of a single quadrature
measurement, corresponding to standard homodyne detection in quantum optics,
can witness arbitrary degrees of non-Gaussianity as quantified by stellar rank.
Our results are based on a version of Hudson's theorem for wavefunctions,
proved in a companion paper [1], revealing that the zeros in a homodyne
distribution are signatures of quantum non-Gaussianity and higher stellar
ranks. The validity of our witnesses is supported by a technical result showing
that sets of states with bounded energy and finite stellar rank are compact. We
provide an analysis of sample complexity, noise robustness, and experimental
prospects. Our work drastically simplifies the setup required to detect quantum
non-Gaussianity in bosonic quantum states. and experimental prospects. Our work
drastically simplifies the setup required to detect quantum non-Gaussianity in
bosonic quantum states.

</details>


### [385] [Neural Network Architectures for Scalable Quantum State Tomography: Benchmarking and Memristor-Based Acceleration](https://arxiv.org/abs/2507.23007)
*Erbing Hua,Steven van Ommen,King Yiu Yu,Jim van Leeuven,Rajendra Bishnoi,Heba Abunahla,Salahuddin Nur,Sebastian Feld,Ryoichi Ishihara*

Main category: quant-ph

TL;DR: 对神经网络架构在量子态层析中的扩展性和保真度进行了基准测试，发现CNN和CGAN扩展性最好，SVAE适合低功耗硬件。同时讨论了忆阻器计算内存（CiM）平台在硬件加速中的应用。


<details>
  <summary>Details</summary>
Motivation: 量子态层析（QST）对于表征和验证量子系统至关重要，但其希尔伯特空间呈指数增长以及信息完备性所需的测量数量对其实际应用造成了严重限制。为了解决这个问题，我们对不同的神经网络架构进行了全面的基准测试，以确定哪些架构能够有效地随着量子比特数量的增加而扩展，哪些在系统规模增加时无法保持高保真度。

Method: 通过在两种量子测量策略上对各种神经网络架构进行全面的基准测试，以评估它们在重建纯态和混合量子态方面的有效性。

Result: 卷积神经网络（CNN）和条件生成对抗网络（CGAN）的扩展性更强，保真度更高。脉冲变分自编码器（SVAE）的保真度表现适中，是嵌入式、低功耗硬件实现的有力候选者。

Conclusion: 这项工作确定了哪些架构对未来的量子系统具有有利的扩展性，并为计算上和物理上都可扩展的量子-经典协同设计奠定了基础。

Abstract: Quantum State Tomography (QST) is essential for characterizing and validating
quantum systems, but its practical use is severely limited by the exponential
growth of the Hilbert space and the number of measurements required for
informational completeness. Many prior claims of performance have relied on
architectural assumptions rather than systematic validation. We benchmark
several neural network architectures to determine which scale effectively with
qubit number and which fail to maintain high fidelity as system size
increases.To address this, we perform a comprehensive benchmarking of diverse
neural architectures across two quantum measurement strategies to evaluate
their effectiveness in reconstructing both pure and mixed quantum states. Our
results reveal that CNN and CGAN scale more robustly and achieve the highest
fidelities, while Spiking Variational Autoencoder (SVAE) demonstrates moderate
fidelity performance, making it a strong candidate for embedded, low-power
hardware implementations.Recognizing that practical quantum diagnostics will
require embedded, energy-efficient computation, we also discuss how
memristor-based Computation-in-Memory (CiM) platforms can accelerate these
models in hardware, mitigating memory bottlenecks and reducing energy
consumption to enable scalable in-situ QST. This work identifies which
architectures scale favorably for future quantum systems and lays the
groundwork for quantum-classical co-design that is both computationally and
physically scalable.

</details>


### [386] [Placing and Routing Non-Local Quantum Error Correcting Codes in Multi-Layer Superconducting Qubit Hardware](https://arxiv.org/abs/2507.23011)
*Melvin Mathews,Lukas Pahl,David Pahl,Vaishnavi L. Addala,Catherine Tang,William D. Oliver,Jeffrey A. Grover*

Main category: quant-ph

TL;DR: HAL是一种利用超导量子比特硬件的多层布线和长程耦合能力，为任意量子误差纠错码（QECC）实现自动化和优化的放置与布线启发式算法，并已成功应用于比较不同QECC的硬件成本。


<details>
  <summary>Details</summary>
Motivation: Quantum error correcting codes (QECCs) with asymptotically lower overheads than the surface code require non-local connectivity.

Method: Leveraging multi-layer routing and long-range coupling capabilities in superconducting qubit hardware, we develop Hardware-Aware Layout, HAL: a robust, runtime-efficient heuristic algorithm that automates and optimizes the placement and routing of arbitrary QECCs.

Result: The layouts produced by HAL confirm that open boundaries significantly reduce the hardware cost, while incurring reductions in logical efficiency. Among the best-performing codes were low-weight radial codes, despite lacking topological structure.

Conclusion: HAL算法为评估现有量子误差纠错码的硬件可行性以及指导与实际硬件约束兼容的新码的发现提供了有价值的框架。

Abstract: Quantum error correcting codes (QECCs) with asymptotically lower overheads
than the surface code require non-local connectivity. Leveraging multi-layer
routing and long-range coupling capabilities in superconducting qubit hardware,
we develop Hardware-Aware Layout, HAL: a robust, runtime-efficient heuristic
algorithm that automates and optimizes the placement and routing of arbitrary
QECCs. Using HAL, we perform a comparative study of hardware cost across
various families of QECCs, including the bivariate bicycle codes, the
open-boundary tile codes, and the constant-depth-decodable radial codes. The
layouts produced by HAL confirm that open boundaries significantly reduce the
hardware cost, while incurring reductions in logical efficiency. Among the
best-performing codes were low-weight radial codes, despite lacking topological
structure. Overall, HAL provides a valuable framework for evaluating the
hardware feasibility of existing QECCs and guiding the discovery of new codes
compatible with realistic hardware constraints.

</details>


### [387] [Context-Dependent Time-Energy Uncertainty Relations from Projective Quantum Measurements](https://arxiv.org/abs/2507.23059)
*Mathieu Beau*

Main category: quant-ph

TL;DR: 提出 TF 框架，实现量子系统中的上下文相关时间分布和时间-能量不确定性关系，并验证了其在不同系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 介绍一种定义量子系统中上下文相关时间分布的通用框架，旨在解决时间测量问题和时间-能量不确定性关系。

Method: 提出一个通用框架，利用投影测量定义量子系统中的上下文相关时间分布，并推导出时间-流（TF）分布。

Result: 推导出的时间-能量不确定性关系为 $\Delta \mathcal{T} \cdot \Delta H \geq \hbar / (6\sqrt{3}) \cdot \delta\theta$，其中 $\delta\theta$ 量化了净人口转移。该框架在统一的 TF 框架下对各种量子系统中的时间测量进行了演示和验证。

Conclusion: 该框架为量子系统中的时间测量提供了一个统一且可行的途径，特别是时间-能量不确定性关系，并强调了测量依赖性。

Abstract: We introduce a general framework for defining context-dependent time
distributions in quantum systems using projective measurements. The
time-of-flow (TF) distribution, derived from population transfer rates into a
measurement subspace, yields a time--energy uncertainty relation of the form
$\Delta \mathcal{T} \cdot \Delta H \geq \hbar / (6\sqrt{3}) \cdot
\delta\theta$, where $\delta\theta$ quantifies net population transfer. This
bound applies to arbitrary projectors under unitary dynamics and reveals that
time uncertainty is inherently measurement-dependent. We demonstrate the
framework with two applications: a general time-of-arrival (TOA)-energy
uncertainty relation and a driven three-level system under detuned coherent
driving. The TF framework unifies timing observables across spin, atomic, and
matter-wave systems, and offers an experimentally accessible route to probing
quantum timing in controlled measurements.

</details>


### [388] [Scalable Ion Fluorescence Collection Using a Trap-Integrated Metalens](https://arxiv.org/abs/2507.23071)
*Hae Lim,Johannes E. Fröch,Christian M. Pluchar,Arka Majumdar,Sara L. Mouradian*

Main category: quant-ph

TL;DR: 所提出的单片集成系统集成了金属透镜，以提高离子阱量子计算机的荧光收集效率，有望实现可扩展的并行读出。


<details>
  <summary>Details</summary>
Motivation: 为了使可扩展的离子阱量子计算机能够高效地进行大面积荧光收集。

Method: 提出并演示了一种紧凑的单片集成系统，该系统在表面离子陷阱的背面集成了金属透镜。

Result: 40×100 μm 孔径实现了0.91%的模拟点源收集效率和0.58%的测量点源检测效率。将孔径面积增加到40×600 μm可将模拟收集效率提高到3.17%，这与具有0.35数值孔径的传统物镜相当。通过对电极和孔径几何形状进行协同优化，还有进一步改进的空间。

Conclusion: 所提出的单片集成系统通过在表面离子陷阱的背面制造金属透镜，为可扩展的离子阱量子计算机提供了一种紧凑、可扩展的高保真并行读出解决方案。

Abstract: A scaled trapped-ion quantum computer will require efficient fluorescence
collection across a large area. Here we propose and demonstrate a compact
monolithically integrated system featuring a metalens fabricated on the
backside of a surface ion trap. A 40$\times$100 $\mu$m aperture enables a
simulated point-source collection efficiency of 0.91% and a measured
point-source detection efficiency of 0.58%. Increasing the aperture area to
40$\times$600 $\mu$m boosts the simulated collection efficiency to
3.17%$-$comparable to that of a conventional objective with a numerical
aperture of 0.35. Further improvements are possible by co-optimizing the
electrode and aperture geometry. An undercut of the electrode substrate at the
aperture ensures a large distance between the ion and dielectric substrate
without compromising collection efficiency. The metalens directly collimates
the collected fluorescence, eliminating the need for a high numerical aperture
objective. An array of such readout zones will offer a compact, scalable
solution for high-fidelity parallel readout in next-generation trapped-ion
quantum processors.

</details>


### [389] [Harnessing Bayesian Statistics to Accelerate Iterative Quantum Amplitude Estimation](https://arxiv.org/abs/2507.23074)
*Qilin Li,Atharva Vidwans,Yazhen Wang,Micheline B. Soley*

Main category: quant-ph

TL;DR: 贝叶斯迭代量子幅度估计算法（BIQAE）利用贝叶斯统计改进了量子幅度估算，在精度和效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 量子幅度估算是量子化学、金融和机器学习等领域的一项关键任务，但现有方法在测量效率和估计精度方面存在局限性。

Method: 提出了一种统一的统计框架，并利用贝叶斯统计改进了量子幅度估计算法的测量效率，同时在迭代量子幅度估计的每次迭代中都进行了严格的区间估计。

Result: 所提出的贝叶斯迭代量子幅度估计算法（BIQAE）能够精确高效地估算量子幅度和分子基态能量，并且在样本复杂度分析中优于所有其他考虑的量子幅度估算方法。

Conclusion: 贝叶斯统计的优势在于能加快寻找量子效用的进程，这一发现值得进一步探究。

Abstract: We establish a unified statistical framework that underscores the crucial
role statistical inference plays in Quantum Amplitude Estimation (QAE), a task
essential to fields ranging from chemistry to finance and machine learning. We
use this framework to harness Bayesian statistics for improved measurement
efficiency with rigorous interval estimates at all iterations of Iterative
Quantum Amplitude Estimation. We demonstrate the resulting method, Bayesian
Iterative Quantum Amplitude Estimation (BIQAE), accurately and efficiently
estimates both quantum amplitudes and molecular ground-state energies to high
accuracy, and show in analytic and numerical sample complexity analyses that
BIQAE outperforms all other QAE approaches considered. Both rigorous
mathematical proofs and numerical simulations conclusively indicate Bayesian
statistics is the source of this advantage, a finding that invites further
inquiry into the power of statistics to expedite the search for quantum
utility.

</details>


### [390] [A Classical-Quantum Adder with Constant Workspace and Linear Gates](https://arxiv.org/abs/2507.23079)
*Craig Gidney*

Main category: quant-ph

TL;DR: 本文提出了两种高效的经典-量子加法器，一种门成本为$4n 	extrm{±} O(1)$，另一种为$3n 	extrm{±} O(1)$，均优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决自2004年Cuccaro等人提出量子-量子加法器以来，经典-量子加法器能否达到相同渐近复杂度的开放性问题，特别是对于模块算术电路。

Method: 通过构造加法器电路，并分析其所需的辅助量子比特数量和Toffoli门数量。

Result: 提出了两种高效的经典-量子加法器，降低了门成本和辅助量子比特数量，并展示了条件加法器无需额外资源。

Conclusion: 本文提出了两种加法器：一种使用3个干净的辅助量子比特和$4n 	extrm{±} O(1)$个Toffoli门，用于将经典偏移量加到量子寄存器中；另一种使用2个干净的辅助量子比特和$n-2$个脏辅助量子比特，Toffoli门成本为$3n 	extrm{±} O(1)$。此外，还证明了在控制量子比特条件下应用这些加法器不需要额外的空间或Toffoli门。

Abstract: In 2004, Cuccaro et al found a quantum-quantum adder with $O(n)$ gate cost
and $O(1)$ ancilla qubits. Since then, it's been an open question whether
classical-quantum adders can achieve the same asymptotic complexity. These
costs are particularly relevant to modular arithmetic circuits, which often
offset by the classically known modulus.
  In this paper, I construct an adder that uses 3 clean ancillae and $4n \pm
O(1)$ Toffoli gates to add a classical offset into a quantum register. I also
present an adder with a Toffoli cost of $3n \pm O(1)$ that uses 2 clean
ancillae and $n-2$ dirty ancillae. I further show that applying the presented
adders conditioned on a control qubit requires no additional workspace or
Toffolis.

</details>


### [391] [Proximity-measurement induced random localization in quantum fluids](https://arxiv.org/abs/2507.23085)
*Pushkar Mohile,Paul M. Goldbart*

Main category: quant-ph

TL;DR: Post-selected random proximity measurements on a quantum fluid of distinguishable particles induce spatial localization, eventually localizing all particles with a distribution of localization lengths governed by a scaling form.


<details>
  <summary>Details</summary>
Motivation: Proximity measurements probe whether pairs of particles are close to one another.

Method: We consider the impact of post-selected random proximity measurements on a quantum fluid of many distinguishable particles.

Result: Such measurements induce random spatial localization of a fraction of the particles, and yet preserve homogeneity macroscopically. Eventually, all particles localize, with a distribution of localization lengths that saturates at a scale controlled by the typical measurement rate.

Conclusion: The steady-state distribution of localization lengths is governed by a familiar scaling form.

Abstract: Proximity measurements probe whether pairs of particles are close to one
another. We consider the impact of post-selected random proximity measurements
on a quantum fluid of many distinguishable particles. We show that such
measurements induce random spatial localization of a fraction of the particles,
and yet preserve homogeneity macroscopically. Eventually, all particles
localize, with a distribution of localization lengths that saturates at a scale
controlled by the typical measurement rate. The steady-state distribution of
these lengths is governed by a familiar scaling form.

</details>


### [392] [Quantification of the energy consumption of entanglement distribution](https://arxiv.org/abs/2507.23108)
*Karol Horodecki,Marek Winczewski,Leonard Sikorski,Paweł Mazurek,Mikołaj Czechlewski,Raja Yehia*

Main category: quant-ph

TL;DR: 本文提出一个框架量化量子纠缠产生的能量成本，发现纠缠的不可逆性导致能量消耗远超理论下限，并为未来量子网络提供了能量投资的量化估计。


<details>
  <summary>Details</summary>
Motivation: 受环境科学的启发，本文旨在量化通过嘈杂的量子通道产生量子纠缠所需的能量，关注其硬件无关的基本成本，并为未来的量子网络提供能量投资的量化估计。

Method: 本文开发了一个框架来量化通过嘈杂的量子通道产生量子纠缠所需的能量，关注硬件无关的基本成本。定义了每分发纠缠比特的最小基本能量消耗率度量，并推导出分发最大纠缠态的能量成本下界。通过制定能量成本度量的公理并引入经典控制量子操作的哈密顿模型，建立了能量消耗率的上界，并定义了基本成本。最后，评估了三种光子偏振量子比特纠缠提纯协议的能量需求。

Result: 推导了分发最大纠缠态的能量成本下界，量化了未来量子网络的能量投资。证明了纠缠理论中的不可逆性导致标准纠缠分发协议存在非零能量成本。建立了能量消耗率的上界，并为经典控制的量子操作定义了基本成本。评估了三种纠缠提纯协议的能量需求，发现其实际能量消耗远超基本下限。

Conclusion: 该研究提出了一个框架来量化通过嘈杂的量子通道产生量子纠缠所需的能量，并推导了分发纠缠的能量成本下界和上界。研究表明，纠缠理论中的不可逆性意味着在标准纠缠分发协议中存在非零能量成本，并且量化了未来量子网络的每纠缠比特的能量投资。此外，研究还评估了三种光子偏振量子比特纠缠提纯协议的能量需求，发现其实际能量消耗远超基本下限。该框架可应用于其他量子资源。

Abstract: Inspired by environmental sciences, we develop a framework to quantify the
energy needed to generate quantum entanglement via noisy quantum channels,
focusing on the hardware-independent, i.e. fundamental cost. Within this
framework, we define a measure of the minimal fundamental energy consumption
rate per distributed entanglement (expressed in Joule per ebit). We then derive
a lower bound on the energy cost of distributing a maximally entangled state
via a quantum channel, which yields a quantitative estimate of energy
investment per entangled bit for future quantum networks. We thereby show that
irreversibility in entanglement theory implies a non-zero energy cost in
standard entanglement distribution protocols. We further establish an upper
bound on the fundamental energy consumption rate of entanglement distribution
by determining the minimal energy required to implement quantum operations via
classical control. To this end, we formulate the axioms for an energy cost
measure and introduce a Hamiltonian model for classically-controlled quantum
operations. The fundamental cost is then defined as the infimum energy over all
such Hamiltonian protocols, with or without specific hardware constraints. The
study of the energy cost of a quantum operation is general enough to be
naturally applicable to quantum computing and is of independent interest.
Finally, we evaluate the energy demands of three entanglement distillation
protocols for photonic polarization qubits, finding that, due to entanglement
irreversibility, their required energy exceeds the fundamental lower bound by
many orders of magnitude. The introduced paradigm can be applied to other
quantum resources, with appropriate changes depending on their nature.

</details>


### [393] [Neural network for excess noise estimation in continuous-variable quantum key distribution under composable finite-size security](https://arxiv.org/abs/2507.23117)
*Lucas Q. Galvão,Davi Juvêncio G. de Sousa,Micael Andrade Dias,Nelson Alves Ferreira Neto*

Main category: quant-ph

TL;DR: 本研究提出了使用神经网络进行CV-QKD参数估计的方法，在有限样本情况下，能够产生更窄的置信区间，从而在集体高斯攻击下提高密钥速率。


<details>
  <summary>Details</summary>
Motivation: 为了解决有限样本情况下参数估计的置信区间过大以致降低可实现密钥速率的问题，本研究探讨了使用神经网络进行参数估计。

Method: 本研究提出了一种使用神经网络进行参数估计的方法，该方法在操作上等同于标准方法，但能产生更窄的置信区间。

Result: 与标准方法相比，本研究提出的方法产生的置信区间更窄，在集体高斯攻击下能实现更高的密钥速率。

Conclusion: 本研究表明，在有限样本情况下，神经网络可以可靠地用于连续可变量子密钥分发（CV-QKD）的参数估计，并具有可量化的失败概率（$\epsilon_{PE}$）以及可操作的解释和可组合的安全保证。

Abstract: Parameter estimation is a critical step in continuous-variable quantum key
distribution (CV-QKD), especially in the finite-size regime where worst-case
confidence intervals can significantly reduce the achievable secret-key rate.
We provide a finite-size security analysis demonstrating that neural networks
can be reliably employed for parameter estimation in CV-QKD with quantifiable
failure probabilities $\epsilon_{PE}$, endowed with an operational
interpretation and composable security guarantees. Using a protocol that is
operationally equivalent to standard approaches, our method produces
significantly tighter confidence intervals, unlocking higher key rates even
under collective Gaussian attacks. The proposed approach yields tighter
confidence intervals, leading to a quantifiable increase in the secret-key rate
under collective Gaussian attacks. These results open up new perspectives for
integrating modern machine learning techniques into quantum cryptographic
protocols, particularly in practical resource-constrained scenarios.

</details>


### [394] [A multi-dimensional quantum estimation and model learning framework based on variational Bayesian inference](https://arxiv.org/abs/2507.23130)
*Federico Belliardo,Erik M. Gauger,Tim H. Taminiau,Yoann Altmann,Cristian Bonato*

Main category: quant-ph

TL;DR: 提出了一种快速的变分贝叶斯推断算法，用于高维参数空间中的量子系统识别和模型选择，解决了传统方法的计算成本和方差问题。


<details>
  <summary>Details</summary>
Motivation: 量子技术的发展和扩展使得在高度多维参数空间中学习和识别量子系统和设备成为一项紧迫的任务，需要快速的实时反馈控制和自适应测量设置。

Method: 提出了一种基于变分贝叶斯推断（VBI）的联合模型选择和参数估计算法，通过优化可处理的分布族来近似目标后验分布，以替代精确推断方法。

Result: 该算法能够选择最简单的模型来解释实验数据，并将自由度分离开，即使在未知环境自旋数量的情况下也能正确识别模型。在模拟和实验数据上进行了基准测试，证明了在几分钟内估计数十个参数的能力。

Conclusion: 该算法能够快速有效地识别具有不同数量参数的竞争模型，并能处理具有大量模型参数的场景，可应用于核磁共振等领域。

Abstract: The advancement and scaling of quantum technology has made the learning and
identification of quantum systems and devices in highly-multidimensional
parameter spaces a pressing task for a variety of applications. In many cases,
the integration of real-time feedback control and adaptive choice of
measurement settings places strict demands on the speed of this task. Here we
present a joint model selection and parameter estimation algorithm that is fast
and operable on a large number of model parameters. The algorithm is based on
variational Bayesian inference (VBI), which approximates the target posterior
distribution by optimizing a tractable family of distributions, making it more
scalable than exact inference methods relying on sampling and that generally
suffer from high variance and computational cost in high-dimensional spaces. We
show how a regularizing prior can be used to select between competing models,
each comprising a different number of parameters, identifying the simplest
model that explains the experimental data. The regularization can further
separate the degrees of freedom, e.g. quantum systems in the environment or
processes, which contribute to major features in the observed dynamics, with
respect to others featuring small coupling, which only contribute to a
background. As an application of the introduced framework, we consider the
problem of the identification of multiple individual nuclear spins with a
single electron spin quantum sensor, relevant for nanoscale nuclear magnetic
resonance. With the number of environmental spins unknown a priori, our
Bayesian approach is able to correctly identify the model, i.e. the number of
spins and their couplings. We benchmark the algorithm on both simulated and
experimental data, using standard figures of merit, and demonstrating that we
can estimate dozens of parameters within minutes.

</details>


### [395] [Reformulating Chemical Equilibrium in Reacting Quantum Gas Mixtures: Particle Number Conservation, Correlations and Fluctuations](https://arxiv.org/abs/2507.23132)
*Diogo J. L. Rodrigues*

Main category: quant-ph

TL;DR: 通过引入粒子数守恒约束来重新表述量子气体混合物，以包含浓度涨落，并提供对量子化学平衡的新见解。


<details>
  <summary>Details</summary>
Motivation: 为了在量子化学平衡的统计描述中纳入浓度涨落，并为具有组分涨落的反应混合物中的量子化学平衡提供新的见解。

Method: 通过结合一个全局粒子数守恒约束来重新表述反应量子气体混合物的经典系综描述，该约束取代了传统的化学势相等。

Result: 费米-狄拉克或玻色-爱因斯坦相关性自然地出现在具有相同自旋统计的物种的单粒子能量特征中，这些相关性在遍历的单系统中表现为平衡状态的内在特征。

Conclusion: 该理论框架为具有组分涨落的反应混合物中的量子化学平衡提供了新的见解，并通过推广经典化学平衡处理的扩展配分函数，平滑地归约到经典理想气体极限。

Abstract: The canonical-ensemble description of reactive quantum gas mixtures is
reformulated by incorporating a single global particle-number-conservation
constraint over the combined spectra of inter-converting species. This
constraint replaces the conventional equality of chemical potentials.
Fermi-Dirac or Bose-Einstein correlations naturally emerge across one-particle
energy eigenstates of species sharing identical spin-statistics, which in
ergodic single-systems manifest as intrinsic features of the equilibrium state.
By embedding all microstates linked by conversion pathways, the framework
incorporates concentration fluctuations in the statistical description. The
formalism offers fresh insights into quantum chemical equilibrium in reactive
mixtures with composition fluctuations and smoothly reduces to the classical
ideal gas limit via an extended partition function that generalizes classical
chemical-equilibrium treatments.

</details>


### [396] [Geometric phase in anisotropic Kepler problem: Perspective for realization in Rydberg atoms](https://arxiv.org/abs/2507.23144)
*Nikolai A. Sinitsyn,Fumika Suzuki*

Main category: quant-ph

TL;DR: Rydberg atoms show a Foucault pendulum-like gyroscopic effect due to optical forces, observable with mechanical rotations.


<details>
  <summary>Details</summary>
Motivation: To explore the analogy between the gyroscopic effect in Rydberg atoms and the Foucault pendulum, and to investigate the generation of a geometric angle through mechanical rotations of an atomic-optical setup.

Method: We analyze the dynamics of Rydberg atoms subjected to optical ponderomotive force, inducing uniaxial anisotropy.

Result: A gyroscopic effect in Rydberg atoms that mimics the Foucault pendulum, observable on time scales of 1 μs to 1 ms.

Conclusion: We predict a gyroscopic effect in Rydberg atoms analogous to the Foucault pendulum.

Abstract: We predict a gyroscopic effect that can be demonstrated with Rydberg atoms
following the dynamics of a Kepler Hamiltonian with an additional uniaxial
anisotropy induced by optical ponderomotive force. This effect is analogous to
the rotation of the Foucault pendulum in response to the Earth's rotation. We
argue that in Rydberg states a similar geometric angle can be generated by
mechanical rotations of an atomic-optical setup on time scales between $1~\mu$s
and $1~$ms.

</details>


### [397] [A Practical Open-Source Software Stack for a Cloud-Based Quantum Computing System](https://arxiv.org/abs/2507.23165)
*Norihiro Kakuko,Shun Gokita,Naoyuki Masumoto,Keita Matsumoto,Kosuke Miyaji,Takafumi Miyanaga,Toshio Mori,Haruki Nakayama,Keita Sasada,Yasuhito Takamiya,Satoyuki Tsukano,Ryo Uchida,Masaomi Yamaguchi*

Main category: quant-ph

TL;DR: OQTOPUS是一个开源的全栈量子计算系统，旨在降低量子计算的进入门槛，并促进社区发展。


<details>
  <summary>Details</summary>
Motivation: 量子计算系统的设计，特别是靠近量子计算机的关键区域，很大程度上仍未公开，这对进入量子计算领域造成了重大障碍，阻碍了标准化和实用量子计算系统的发展。

Method: 提出了一种名为OQTOPUS的全栈量子计算系统，该系统涵盖了从云执行环境构建到系统操作的运营软件，并在靠近量子计算机的区域实现了转译器、多程序设计和错误缓解等关键功能。

Result: OQTOPUS已成功应用于实际的量子计算机，并提供了实验结果。

Conclusion: OQTOPUS的开源将显著降低量子计算领域的门槛，并通过开放的讨论促进量子计算开发者社区的形成。

Abstract: Since the late 2010s, quantum computers have become commercially available,
and the number of services that users can run remotely via cloud servers is
increasing. In Japan, several domestic superconducting quantum computing
systems, including our own, began operation in 2023. However, the design of
quantum computing systems, especially in the most critical areas near quantum
computers, remains largely undisclosed, creating a significant barrier to entry
into the quantum computing field. If this situation continues, progress toward
standardization, which is essential for guiding quantum computer development,
will stall, and it will be difficult to develop a practical quantum computing
system that can perform calculations on a supercomputer scale. To address this
issue, we propose Open Quantum Toolchain for OPerators and USers (OQTOPUS), a
full-stack quantum computing system developed from research with real quantum
computers. OQTOPUS is one of the world's largest open-source software projects,
covering operational software from cloud-based execution environment
construction to system operation. Furthermore, to perform quantum computing
effectively and efficiently, it implements key features, such as transpilers,
multiprogramming, and error mitigation, in an area as close as possible to a
quantum computer, an area that system vendors rarely disclose. Finally, this
study presents experimental results of applying OQTOPUS to a real quantum
computer. OQTOPUS is publicly available on GitHub and will notably lower the
barrier to entry into the quantum computing field, contributing to the
formation of a quantum computing developer community through open discussion.

</details>


### [398] [Quantum Key Distribution](https://arxiv.org/abs/2507.23192)
*Sebastian Kish,Josef Pieprzyk,Seyit Camtepe*

Main category: quant-ph

TL;DR: QKD技术利用量子力学确保通信安全，虽然面临挑战，但随着技术进步和实际部署，正变得越来越成熟，并在量子安全领域扮演关键角色。


<details>
  <summary>Details</summary>
Motivation: 量子密钥分发（QKD）技术利用量子力学原理（如不可克隆定理和量子不确定性）来确保通信安全，旨在应对未来量子威胁，保护关键任务通信。

Method: 文章概述了量子密钥分发（QKD）技术的成熟度和发展趋势，重点介绍了单光子源和探测技术的进展，并讨论了成本、集成、标准化和量子中继器的挑战。

Result: 尽管存在成本、集成、标准化和量子中继器等挑战，但单光子源和探测技术的进步以及行业领导者的实际部署，使得QKD比以往任何时候都更接近广泛应用。

Conclusion: 量子密钥分发（QKD）技术正日益成熟，并在保护关键通信免受未来量子威胁方面发挥着至关重要的作用，有望在量子安全密码算法和协议中扮演关键角色。

Abstract: Quantum Key Distribution (QKD) is a technology that ensures secure
communication by leveraging the principles of quantum mechanics, such as the
no-cloning theorem and quantum uncertainty. This chapter provides an overview
of this quantum technology's maturity and trends. It highlights significant
advancements in single-photon sources and detection technologies that have
brought QKD closer to widespread adoption, including real-world deployments by
industry leaders. While addressing challenges such as cost, integration,
standardization, and the need for quantum repeaters, the chapter emphasizes the
growing importance of QKD in securing mission-critical communications against
future quantum threats. Through its unique ability to achieve
information-theoretic security, QKD is poised to play a vital role in
quantum-safe cryptographic algorithms and protocols.

</details>


### [399] [Long-range photonic device-independent quantum key distribution using SPDC sources and linear optics](https://arxiv.org/abs/2507.23254)
*Morteza Moradi,Maryam Afsary,Piotr Mironowicz,Enky Oudot,Magdalena Stobińska*

Main category: quant-ph

TL;DR: 通过受激纠缠分发和全光学方法，首次实现了可行的远距离设备无关量子密钥分发 (DI QKD)，密钥率随信道透过率的平方根缩放，并能在低探测器效率下工作。


<details>
  <summary>Details</summary>
Motivation: DI QKD 具有无与伦比的密码安全优势，但由于速率限制，远距离实现仍然不切实际。

Method: 提出了一种使用自发参量下转换（SPDC）源的受激纠缠分发的、两个全光学的、可行的远距离设备无关量子密钥分发（DI QKD）方案。

Result: 在 83% 的探测器效率下实现了正密钥率，并生成了比标准方法强 2-3 倍的定制贝尔认证。

Conclusion: 该工作是迈向量子通信网络中设备无关安全的关键里程碑，为实验者提供了实际的实现途径，同时维持了针对量子对手的最强安全保证。

Abstract: Device-independent quantum key distribution (DI QKD) offers unparalleled
cryptographic security by eliminating trust requirements for quantum devices,
yet has remained impractical for long-distance implementation due to
fundamental rate limitations. Here, we propose the first experimentally viable
schemes for long-distance DI QKD using two fully photonic approaches with
heralded entanglement distribution using spontaneous parametric down-conversion
(SPDC) sources. Both schemes achieve key rate scaling with the square root of
channel transmissivity $\eta_t$, matching the twin-field protocol advantage
rather than the prohibitive linear decay of conventional QKD. We demonstrate
positive key rates at detector efficiencies as low as 83\%, bringing DI QKD
within reach of the current superconducting detector technology. Our security
analysis employs the Entropy Accumulation Theorem to establish rigorous
finite-size bounds, while numerical optimization yields custom Bell
certificates that surpass standard approaches by 2--3 times at maximum
transmission distances. This work represents a critical milestone toward
device-independent security in quantum communication networks, providing
experimentalists with practical implementation pathways while maintaining the
strongest possible security guarantees against quantum adversaries.

</details>


### [400] [Deterministic and Scalable Coupling of Single 4H-SiC Spin Defects into Bullseye Cavities](https://arxiv.org/abs/2507.23258)
*Tongyuan Bao,Qi Luo,Ailun Yi,Yingjie Li,Haibo Hu,Xin Ou,Yu Zhou,Qinghai Song*

Main category: quant-ph

TL;DR: 该研究在 4H-SiC on Insulator 平台上成功地将单量子比特（PL4 和 PL6 自旋缺陷）集成到牛眼腔中，实现了光子信号的增强和自旋态的相干控制，为构建可扩展的量子光子电路提供了可行途径。


<details>
  <summary>Details</summary>
Motivation: 为了克服将单自旋缺陷确定性地集成到高性能光子腔中的关键挑战，尤其是在 CMOS 兼容的 4H-SiCOI 平台上。

Method: 通过调整腔体共振，实现了 PL4 缺陷零声子线 (ZPL) 强度的 40 倍增强，Purcell 因子约为 5.0。对于确定性耦合的单 PL6 缺陷，饱和光子计数率提高了三倍，证实了单光子发射，并通过光检测磁共振 (ODMR)、共振激发和拉比振荡实现了自旋态的相干控制。

Result: 成功将 PL4 和 PL6 自旋缺陷（包括单缺陷）确定性地集成到 4H-SiCOI 平台上的单片牛眼腔中。

Conclusion: 这项工作为开发可扩展、高性能的碳化硅基量子光子电路铺平了道路。

Abstract: Silicon carbide (SiC) has attracted significant attention as a promising
quantum material due to its ability to host long-lived, optically addressable
color centers with solid-state photonic interfaces. The CMOS compatibility of
4H-SiCOI (silicon-carbide-on-insulator) makes it an ideal platform for
integrated quantum photonic devices and circuits. However, the deterministic
integration of single spin defects into high-performance photonic cavities on
this platform has remained a key challenge. In this work, we demonstrate the
deterministic and scalable coupling of both ensemble (PL4) and single PL6 spin
defects into monolithic bullseye cavities on the 4H-SiCOI platform. By tuning
the cavity resonance, we achieve a 40-fold enhancement of the zero-phonon line
(ZPL) intensity from ensemble PL4 defects, corresponding to a Purcell factor of
approximately 5.0. For deterministically coupled single PL6 defects, we observe
a threefold increase in the saturated photon count rate, confirm single-photon
emission, and demonstrate coherent control of the spin state through optically
detected magnetic resonance (ODMR), resonant excitation, and Rabi oscillations.
These advancements establish a viable pathway for developing scalable,
high-performance SiC-based quantum photonic circuits.

</details>


### [401] [Time-Dependent Parameters in Quantum Systems: Revisiting Berry Phase, Curvature and Gauge Connections](https://arxiv.org/abs/2507.23347)
*Georgios Konstantinou*

Main category: quant-ph

TL;DR: 量子绝热理论与电磁学结合，提出贝里麦克斯韦方程，统一了不同动力学下的几何相位。


<details>
  <summary>Details</summary>
Motivation: 为了弥合量子系统静态和动态表述之间的差距，并深入理解规范结构如何在量子系统中体现，本文将量子绝热理论与电磁学联系起来。

Method: 通过将量子绝热理论重构为涌现的电磁框架，定义了描述几何结构的标量和矢量势，并推导了相应的贝里麦克斯韦方程及其相关的场论恒等式，如广义连续性和涡度关系。

Result: 本文成功构建了贝里麦克斯韦方程，并推导了广义连续性和涡度关系等场论恒等式，揭示了参数空间中的拓扑荷、单极结构和规范流，阐明了贝里曲率修正如何进入期望值和粒子速度等动力学量，最终在参数空间中建立了一个新的涌现电磁学体系。

Conclusion: 本文提出了一种基于几何结构的新型量子系统分析框架，将量子绝热理论与电磁学联系起来，为量子输运、极化和拓扑分类提供了新的视角。

Abstract: We present a reformulation of quantum adiabatic theory in terms of an
emergent electromagnetic framework, emphasizing the physical consequences of
geometric structures in parameter space. Contrary to conventional approaches,
we demonstrate that a Berry electric field naturally arises in systems with
dynamic Hamiltonian, when the full time-dependent wavefunction is used to
define the gauge potentials. This surprising result bridges the gap between
static and dynamical formulations and leads to a deeper understanding of how
gauge structures manifest in quantum systems. Building on this, we construct
Berry Maxwell equations by analogy with classical electrodynamics, defining
Berry electric and magnetic fields as derivatives of scalar and vector
potentials obtained from the full quantum state. We verify these equations
explicitly and derive field-theoretic identities such as generalized continuity
and vorticity relations. This field-based formulation reveals the topological
charges, monopole structures, and gauge currents that underlie parameter space,
and clarifies how Berry curvature corrections enter dynamical quantities like
expectation values and particle velocities. Our results establish a new regime
of emergent electromagnetism in parameter space, unifying time-independent and
time-dependent geometric phases within a covariant formalism. The implications
extend to quantum transport, polarization, and topological classification of
phases, providing a robust and generalizable framework for quantum systems
driven by adiabatic or nonadiabatic evolution.

</details>


### [402] [Environment-assisted and weak measurement strategies for robust bidirectional quantum teleportation](https://arxiv.org/abs/2507.23274)
*Javid Ahmad Malik,Muzaffar Qadir Lone,Prince A Ganai*

Main category: quant-ph

TL;DR: 本研究通过结合环境辅助测量和弱测量技术，提出了一种增强双向量子隐形传态（BQT）鲁棒性的新方案，以克服幅度阻尼通道的影响。研究发现，通过优化测量参数，可以显著提高BQT的保真度和成功率，优于未受保护的传统方案。


<details>
  <summary>Details</summary>
Motivation: 双向量子隐形传态（BQT）是分布式量子网络中的关键技术，但其性能会受到量子比特退相干的影响。为了提高BQT在实际应用中的鲁棒性，需要有效的策略来对抗环境噪声。

Method: 本研究提出了一种新的双向量子隐形传态（BQT）方案，利用环境辅助测量（EAM）和弱测量技术来对抗幅度阻尼通道（ADC）引起的退相干。该方案首先建立一个包含两个贝尔态的四量子比特通道，然后探索了两种情况：仅恢复量子比特经过ADC，以及整个四量子比特通道均受ADC影响。通过调整弱测量强度（$q_w$）和ADC衰减率（$p$）之间的关系，优化了传输过程的保真度和成功率。

Result: 研究结果表明，当弱测量强度$q_w$与衰减率$p$满足$q_w \in {[0,p]}$时，可以在平均保真度和成功率之间取得平衡，并在$q_w=p$时达到完美BQT。当$q_w > p$时，平均保真度和成功率均会下降。所提出的受保护BQT协议在两种场景下均优于未受保护的方案。

Conclusion: 该研究提出了一种结合环境辅助测量和弱测量技术来增强双向量子隐形传态（BQT）鲁棒性的方案。研究表明，通过优化弱测量强度与幅度阻尼通道衰减率之间的关系（$q_w \in {[0,p]}$），可以平衡保真度和成功率，并在$q_w = p$时实现完美的BQT，完全抑制衰减通道的影响。此外，所提出的BQT协议在两种受衰减通道影响的情况下均优于未受保护的方案，证明了该保护策略的有效性。

Abstract: This paper presents strategies for enhancing the robustness of bidirectional
quantum teleportation (BQT) through environment-assisted and weak measurement
techniques. BQT is a crucial component of distributed quantum networks,
allowing for the bilateral transfer of quantum information between two nodes.
While perfect teleportation necessitates maximally entangled states, these are
vulnerable to degradation due to inherent decoherence. We propose a BQT scheme
that enables the bilateral transfer of arbitrary qubits between nodes via
amplitude damping channels (ADC), aiming to optimize fidelity using weak
measurements in the final step of the process. Environment-assisted
measurements (EAM) are used to establish a four-qubit channel composed of two
Bell states. We explore two situations: (I) where only the recovery qubits pass
through amplitude damping channels and (II) where the entire four-qubit channel
is subjected to ADC. Our findings demonstrate a balance between average
fidelity and success probability when the weak measurement strength ($q_w$) is
constrained by the decay rate ($p$), specifically $q_w \in {[0,p]}$. Perfect
BQT is achieved when $q_w = p$, indicating complete suppression of ADC effects.
On the other hand, a decline in both average fidelity and success probability
is noted when the weak measurement strength surpasses the ADC strength, marking
the prohibited domain as $q_w \in {(p,1]}$. Additionally, our secured BQT
protocol consistently outperforms the unprotected scheme in both scenarios,
highlighting the effectiveness of the proposed protection strategies.

</details>


### [403] [Novel Quantum Circuit Designs of Random Injection and Payoff Computation for Financial Risk Assessment](https://arxiv.org/abs/2507.23310)
*Yu-Ting Kao,Yeong-Jar Chang,Ying-Wei Tseng*

Main category: quant-ph

TL;DR: 本研究提出并实现了一个集成的量子电路，用于金融分析中的随机数生成和支付计算，并利用QAE实现了二次加速，在IBM Qiskit上验证了其正确性和随机性。


<details>
  <summary>Details</summary>
Motivation: 解决将量子计算应用于金融分析（特别是随机数生成和支付计算）的挑战，并克服量子计算中确定性与金融应用中随机数需求的矛盾。

Method: 提出了一种包含随机数注入和直接支付计算两个关键组件的集成量子电路。该电路在IBM Qiskit上实现，并使用了8个并行线程和1600次测量样本进行评估。

Result: 实验结果证实了该量子电路能够正确注入随机数并进行支付计算，证明了其在金融定价和风险评估方面的潜力。

Conclusion: 该研究成功展示了一个集成的量子电路，该电路能够进行随机数注入和直接支付计算，并利用量子幅值估计（QAE）实现了二次加速。

Abstract: Quantum entanglement enables exponential computational states, while
superposition provides inherent parallelism. Consequently, quantum circuits are
theoretically capable of supporting large scale parallel computation. However,
applying them to financial analysis particularly in the areas of random number
generation and payoff computation remains a significant challenge. Experts
generally believe that quantum computing relies on matrix operations, which are
deterministic in nature without randomness. This inherent determinism makes it
particularly challenging to design quantum circuits that require random number
injection. JP Morgan[1] introduced the piecewise linear (PWL) approach for
modeling payoff computations but did not disclose a quantum circuit capable of
identifying values exceeding the strike price, suggesting a possible reliance
on classical pre processing for interval classification. This paper presents an
integrated quantum circuit with two key components: one for random number
injection, applicable to risk assessment, and the other for direct payoff
computation, relevant to financial pricing. These components are compatible
with a scalable framework that leverages large scale parallelism and Quantum
Amplitude Estimation (QAE) to achieve quadratic speedup. The circuit was
implemented on IBM Qiskit and evaluated using 8 parallel threads and 1600
measurement shots. Results confirmed both the presence of randomness and the
correctness of payoff computation. While the current implementation uses 8
threads, the design scales to 2 to the power of n threads, for arbitrarily
large n, offering a potential path toward demonstrating quantum supremacy.

</details>


### [404] [Transport-Induced Decoherence of the Entangled Triplet Exciton Pair](https://arxiv.org/abs/2507.23770)
*Gerald Curran III,Luke J. Weaver,Zachary Rex,Ivan Biaggio*

Main category: quant-ph

TL;DR: 研究有机分子晶体中纠缠三线态对的退相干效应，发现荧光量子节拍受激子跃迁和磁场影响，不同磁场下节拍衰减速率不同。


<details>
  <summary>Details</summary>
Motivation: 研究纠缠三线态对在有机分子晶体中的退相干效应，特别是激子在不同晶格位点之间跃迁的影响，以理解荧光量子节拍的产生机制及其对磁场的依赖性。

Method: 本研究基于蒙特卡洛模拟，分析了纠缠三线态对在有机分子晶体中的退相干效应，考虑了激子在不同晶格位点之间跃迁的情况，并预测了由三线态-三线态复合到发光单线态状态引起的量子干涉所产生的荧光量子节拍，该节拍是跃迁时间和磁场的函数。

Result: 研究结果表明，荧光量子节拍的衰减速率取决于激子跃迁速率和磁场强度。在零磁场下，可能出现完全的全局退相干和荧光量子节拍的抑制；而在有磁场的情况下，量子节拍的衰减速率会随磁场强度的增加而变化。

Conclusion: 本研究预测了在外消旋分子晶体中，由于激子在不同晶格位点之间的跃迁，纠缠三线态对会产生退相干效应。在零磁场极限下，可能出现完全的全局退相干和荧光量子节拍的抑制；而在不同磁场强度下，量子节拍会以不同的速率衰减。

Abstract: Decoherence effects for entangled triplet pairs in organic molecular crystals
are analyzed for the case when excitons can hop between inequivalent lattice
sites. The fluorescence quantum beats caused by quantum interference upon
triplet-triplet recombination into an emissive singlet state are predicted as a
function of hopping time and magnetic field based on a Monte Carlo analysis.
Depending on exciton hopping rates, it is possible to have complete global
decoherence and suppression of fluorescence quantum beats in the limit of zero
magnetic field, and to have quantum beats that decay at different rates
depending on magnetic field strength.

</details>


### [405] [Enhanced Extrapolation-Based Quantum Error Mitigation Using Repetitive Structure in Quantum Algorithms](https://arxiv.org/abs/2507.23314)
*Boseon Kim,Wooyeong Song,Kwangil Bae,Wonhyuk Lee,IlKwon Sohn*

Main category: quant-ph

TL;DR: 提出了一种新的量子纠错框架，针对结构化算法，通过关注核心运算模块而非整个算法的错误，在高噪声下表现优于传统ZNE方法，成功率提升显著。


<details>
  <summary>Details</summary>
Motivation: 针对现有零噪声外推（ZNE）方法在深度量子算法（如涉及多次预言调用的迭代量子算法）在高噪声下有效性显著降低，导致外插估计不准确且需要大量开销的问题，提出了一种新的纠错框架。

Method: 提出了一种针对结构化量子算法（由重复运算块组成）的轻量级、基于外插的纠错框架。该方法通过使用浅层电路来表征重复的核心运算模块的错误，而不是整个算法。然后，利用外插来估计模块保真度，并重建出已缓解的成功概率。

Result: 在模拟（6量子比特Grover算法）和真实量子系统（127量子比特IBM Quantum Eagle r3）上进行了验证。结果表明，核心模块的错误遵循高度一致的指数衰减，使得该技术能够实现鲁棒的错误缓解，克服了传统ZNE在重噪声下因统计不可靠数据而表现不佳的限制。在低噪声条件下，该方法接近理论成功概率并优于ZNE；在高噪声条件下，ZNE因过拟合而失效，而该方法成功率则提高了20%以上。

Conclusion: 该研究提出的轻量级、基于外插的纠错框架，通过表征重复运算模块的错误而非整个算法，并在低噪声和高噪声条件下均优于传统零噪声外推（ZNE）方法，特别是在高噪声条件下，成功率提高了20%以上，克服了ZNE在高噪声下因过拟合而失效的问题。

Abstract: Quantum error mitigation is a crucial technique for suppressing errors
especially in noisy intermediate-scale quantum devices, enabling more reliable
quantum computation without the overhead of full error correction. Zero-Noise
Extrapolation (ZNE), which we mainly consider in this work, is one of prominent
quantum error mitigation methods. For algorithms with deep circuits - such as
iterative quantum algorithms involving multiple oracle calls - ZNE's
effectiveness is significantly degraded under high noise. Extrapolation based
on such low-fidelity data often yields inaccurate estimates and requires
substantial overhead. In this study, we propose a lightweight,
extrapolation-based error mitigation framework tailored for structured quantum
algorithms composed of repeating operational blocks. The proposed method
characterizes the error of the repeated core operational block, rather than the
full algorithm, using shallow circuits. Extrapolation is used to estimate the
block fidelity, followed by a reconstruction of the mitigated success
probability. We validate our method via simulations of the 6-qubit Grover's
algorithm on IBM's Aer simulator, then further evaluating it on the real
127-qubit IBM Quantum system based on Eagle r3 under a physical noise
environment. Our results, particularly those from Aer simulator, demonstrate
that the core block's error follows a highly consistent exponential decay. This
allows our technique to achieve robust error mitigation, overcoming the
limitations of conventional ZNE which is often compromised by statistically
unreliable data from near-random behavior under heavy noise. In low-noise
conditions, our method approaches theoretical success probability, outperforms
ZNE. In high-noise conditions, ZNE fails to mitigate errors due to overfitting
of its extrapolation data, whereas our method achieves over a 20% higher
success probability.

</details>


### [406] [Non-equilibrium thermodynamics of the quantum Brownian motion: Anomalous non-equilibrium currents arising from complete positivity](https://arxiv.org/abs/2507.23322)
*Simone Artini,Gabriele Lo Monaco,Alberto Imparato,Mauro Paternostro,Sandro Donadi*

Main category: quant-ph

TL;DR: 量子系统的一致性与热力学平衡之间存在矛盾。CPTP 扩展虽然保证了量子一致性，但在热力学上存在问题，会导致熵产生和非平衡电流。


<details>
  <summary>Details</summary>
Motivation: 分析了Caldeira-Leggett主方程的优缺点，以及其 CPTP 扩展在热力学上的争议。Caldeira-Leggett主方程虽然具有理想的热力学特征（如满足详细平衡），但未能保证完全正性。相反，几个完全正且保持迹的（CPTP）扩展在热力学上存在争议。

Method: 使用随机热力学框架，严格分析了量子布朗运动（QBM）不同表述的非平衡热力学行为。

Result: 研究表明，CPTP 扩展会引入异常相空间结构，在稳态下违反详细平衡，导致熵产生增加和无法解释的有效非平衡电流。

Conclusion: 该研究揭示了开放量子系统中量子一致性与热力学平衡之间的基本矛盾。

Abstract: We rigorously analyze the non-equilibrium thermodynamic behavior of various
formulations of quantum Brownian motion (QBM) using the framework of stochastic
thermodynamics. While the widely used Caldeira-Leggett master equation exhibits
desirable thermodynamic features, such as the fulfilment of a detailed balance,
it fails to ensure complete positivity. In contrast, several completely
positive and trace-preserving (CPTP) extensions turn out to be
thermodynamically controversial. We show that such extensions introduce
anomalous phase-space structures that violate detailed balance at the steady
state, leading to non-vanishing entropy production and effective
non-equilibrium current of unclear physical origins. Our results highlight a
fundamental tension between quantum consistency and thermodynamic equilibration
in open quantum systems.

</details>


### [407] [High-Performance Fully Passive Discrete-State Continuous-Variable Quantum Key Distribution With Local Local Oscillator](https://arxiv.org/abs/2507.23327)
*Yu Zhang,Xuyang Wang,Chenyang Li,Jie Yun,Qiang Zeng,Zhiliang Yuan,Zhenguo Lu,Yongmin Li*

Main category: quant-ph

TL;DR: 提出了一种全无源CV-QKD协议，利用LLO消除了源端调制器侧信道，在100公里传输距离和1GHz重复率下实现了127kbps的密钥比特率，具有高现实安全性。


<details>
  <summary>Details</summary>
Motivation: 消除源端所有调制器侧信道，实现高现实安全性。

Method: 提出并演示了一种全无源离散态连续变量量子密钥分发（CV-QKD），它利用本地本地振荡器（LLO）消除了源端的所有调制器侧信道。通过特别设计的相位旋转和离散化方法，该CV-QKD系统实现了100公里的最大传输长度和1 GHz的重复率，相应的密钥比特率为127 kbps。

Result: 在100公里传输距离和1 GHz重复率下，密钥比特率为127 kbps，优于被动离散变量和连续变量协议。

Conclusion: 该协议有望在具有高现实安全性的量子城域网和量子接入网络中发挥重要作用。

Abstract: We propose and demonstrate a fully passive discrete-state continuous-variable
quantum key distribution (CV-QKD), which can eliminate all modulator side
channels on the source side, using a local local oscillator (LLO). The CV-QKD
system achieves a maximum transmission length of 100 km with a repetition rate
of 1 GHz using specially designed phase rotation and discretization methods,
and the corresponding secret key bit rate is 127 kbps, as estimated based on
the amplitude of prepared states at the transmitter, as well as the first- and
second-order moments of quadratures at the receiver by employing the convex
optimization without imposing any assumptions on the quantum channel. The
performance of the protocol is similar to that of modulated CV LLO protocols
and better than those of passive discrete-variable and CV protocols. Our
protocol is expected to play an important role in the quantum metropolitan area
networks and quantum access networks with high realistic security.

</details>


### [408] [Transfer entropy and O-information to detect grokking in tensor network multi-class classification problems](https://arxiv.org/abs/2507.23346)
*Domenico Pomarico,Roberto Cilli,Alfonso Monaco,Loredana Bellantuono,Marianna La Rocca,Tommaso Maggipinto,Giuseppe Magnifico,Marlis Ontivero Ortega,Ester Pantaleo,Sabina Tangaro,Sebastiano Stramaglia,Roberto Bellotti,Nicola Amoroso*

Main category: quant-ph

TL;DR: 量子启发式学习器（MPS分类器）在处理多类数据时，其训练动力学和泛化行为（grokking）与纠缠和信息冗余密切相关。在时尚MNIST数据上观察到grokking与纠缠转变和信息冗余峰值相关，而在高光谱数据上，过度拟合模型表现出协同但无序的行为。


<details>
  <summary>Details</summary>
Motivation: 在复杂、高维数据的提取结构方面，量子增强机器学习（包括量子算法和张量网络等量子启发式经典方法）提供了有前景的工具。因此，理解这些模型的训练动力学和泛化机制至关重要。

Method: 本工作研究了将矩阵乘积态（MPS）分类器应用于三类问题（包括时尚MNIST和高光谱卫星图像）的训练动力学。通过追踪纠缠熵、局域磁化强度和模型性能，研究了“grokking”现象。此外，研究还利用信息论工具，如转移熵和O信息，来揭示标签特定的量子掩码之间的因果依赖性以及类别输出之间的协同与冗余相关性的转变。

Result: 在时尚MNIST任务中，grokking与急剧的纠缠转变和冗余信息峰值同时出现。而过度拟合的高光谱模型则保持着协同的、无序的行为。

Conclusion: 本研究揭示了量子启发式学习中高阶信息动力学的重要性，并强调了在多类分类中出现的不同学习行为，为理解量子机器学习架构中的泛化提供了原则性框架。

Abstract: Quantum-enhanced machine learning, encompassing both quantum algorithms and
quantum-inspired classical methods such as tensor networks, offers promising
tools for extracting structure from complex, high-dimensional data. In this
work, we study the training dynamics of Matrix Product State (MPS) classifiers
applied to three-class problems, using both fashion MNIST and hyper-spectral
satellite imagery as representative datasets. We investigate the phenomenon of
grokking, where generalization emerges suddenly after memorization, by tracking
entanglement entropy, local magnetization, and model performance across
training sweeps. Additionally, we employ information theory tools to gain
deeper insights: transfer entropy is used to reveal causal dependencies between
label-specific quantum masks, while O-information captures the shift from
synergistic to redundant correlations among class outputs. Our results show
that grokking in the fashion MNIST task coincides with a sharp entanglement
transition and a peak in redundant information, whereas the overfitted
hyper-spectral model retains synergistic, disordered behavior. These findings
highlight the relevance of high-order information dynamics in quantum-inspired
learning and emphasize the distinct learning behaviors that emerge in
multi-class classification, offering a principled framework to interpret
generalization in quantum machine learning architectures.

</details>


### [409] [Measure of entanglement production by quantum operations](https://arxiv.org/abs/2507.23366)
*V. I. Yukalov,E. P. Yukalova*

Main category: quant-ph

TL;DR: 提出了一种通用的纠缠产生度量方法。


<details>
  <summary>Details</summary>
Motivation: 提出一种度量纠缠产生量的方法。

Method: 通过定义广义相关矩阵，可以处理任意性质的系统，这些系统可以由场算符、自旋算符或任何其他算符来描述。

Result: 该度量满足此类特征的所有典型性质，并考虑了纠缠产生的特例。

Conclusion: 所提出的纠缠产生量的度量是通用的，适用于纯态和混合态的运算，也适用于平衡和非平衡过程。

Abstract: A measure of entanglement production by quantum operations is suggested. This
measure is general, being valid for operations over pure states as well as over
mixed states, for equilibrium as well as for nonequilibrium processes. The
measure of entanglement production satisfies all properties typical of such a
characteristic. Systems of arbitrary nature can be treated, described by field
operators, spin operators, or any other operators, which is realized by
defining generalized correlation matrices. Particular cases of entanglement
production are considered.

</details>


### [410] [Analysis of untrusted-node quantum key distribution from a geostationary satellite](https://arxiv.org/abs/2507.23466)
*Thomas Liege,Perrine Lognone,Matteo Schiavon,Caroline B. Lim,Jean-Marc Conan,Eleni Diamanti,Daniele Dequal*

Main category: quant-ph

TL;DR: 该研究模拟了在地球静止卫星上使用TF-QKD和MP-QKD协议进行量子密钥分发（QKD）的性能。结果表明，使用50厘米卫星望远镜和20厘米至1米地面望远镜，可以实现每秒几百比特的密钥生成速率，即使在20厘米地面望远镜下也具有潜力。


<details>
  <summary>Details</summary>
Motivation: 为了实现覆盖范围广、运行连续且安全性增强的全球量子密钥分发（QKD）网络，研究基于不可信节点、在地球静止卫星上运行的服务，并评估了在此场景下TF-QKD和MP-QKD这两种对高损耗信道具有高鲁棒性的协议的性能。

Method: 通过在包含50厘米卫星望远镜和20厘米至1米地面望远镜的配置中，对自适应光学校正的通信信道进行深入模拟，评估了TF-QKD和MP-QKD这两种量子密钥分发（QKD）协议的性能。

Result: 在最佳情况下，考虑到实际的探测器，TF-QKD和MP-QKD的密钥生成速率均可达到每秒几百比特，即使使用20厘米的地面望远镜也具有可行性。

Conclusion: 在两种量子密钥分发（QKD）协议（分别为双场（TF）和模式配对（MP）QKD）中，使用基于不可信节点的、在地球静止卫星上运行的服务，可以实现覆盖范围广、运行连续且安全性增强的全球量子密钥分发（QKD）网络。在包含50厘米卫星望远镜和20厘米至1米地面望远镜的配置中，在最佳情况下，两种协议均可实现每秒几百比特的密钥生成速率，即使使用20厘米的地面望远镜也具有可行性，这突显了这种配置的高可扩展性潜力。

Abstract: In pursuit of a global quantum key distribution (QKD) network, a service
based on untrusted nodes on geostationary satellites could offer wide coverage,
continuous operation, and enhanced security compared to the trusted node
alternative. Although this scenario has been studied for entanglement-based
protocols, such an approach would require large-area telescopes both on the
ground and in space. In this work, we analyze the performance of two QKD
protocols well adapted to this scenario, namely twin-field (TF) and
mode-pairing (MP) QKD, which exhibit high resilience to high-loss channels.
Leveraging an in-depth simulation of communication channels corrected with
adaptive optics, we assess the expected secret key rates for both protocols in
a configuration involving two 50 cm telescopes on board the satellite and
ground-based telescopes ranging from 20 cm to 1 m in aperture. Our results show
that, in the best case and considering realistic detectors, it is possible to
achieve secret key rates on the order of a few hundred bit/s for both TF and
MP-QKD. We show, notably, that secret key generation is potentially feasible
even with 20 cm ground telescopes, highlighting the high scalability potential
of such a configuration.

</details>


### [411] [On the complex zeros of the wavefunction](https://arxiv.org/abs/2507.23468)
*Sacha Cerf,Clara Wassner,Jack Davis,Francesco Arzani,Ulysse Chabaud*

Main category: quant-ph

TL;DR: 研究了玻色子量子系统波函数的零点，发现了它们的信息论解释，并将高斯动力学与零点的经典运动联系起来。此外，还展示了如何检测非高斯性。


<details>
  <summary>Details</summary>
Motivation: 研究了量子力学、量子化学和玻色子量子信息理论中普遍存在的薛定谔波函数的零集。特别是，研究了在表征非经典性方面对玻色子系统而言不太理解的零集。

Method: 研究了玻色子量子系统的波函数零点，并给出了新颖的信息论解释。证明了大多数玻色子量子系统的波函数可以扩展到复平面上的全纯函数，并应用了复分析技术。

Result: 证明了哈德逊定理的一个版本，并将高斯动力学表征为波函数零点的经典运动。此外，还表明量子光学态的非高斯性可以通过测量电磁场的单个二次量来检测，并且量子系统的非高斯特征被编码在其波函数的零点中。

Conclusion: 我们的研究结果表明，对于大多数量子系统，其波函数可以扩展到复平面上的全纯函数，从而可以使用复分析的强大技术。作为结果，我们证明了哈德逊定理的一个版本，并将高斯动力学表征为波函数零点的经典运动。此外，我们发现量子光学态的非高斯性可以通过测量电磁场的单个二次量来检测，并且量子系统的非高斯特征被编码在其波函数的零点中。

Abstract: The Schr\"odinger wavefunction is ubiquitous in quantum mechanics, quantum
chemistry, and bosonic quantum information theory. Its zero-set for fermionic
systems is well-studied and central for determining chemical properties, yet
for bosonic systems the zero-set is less understood, especially in the context
of characterizing non-classicality. Here we study the zeros of such
wavefunctions and give them a novel information-theoretic interpretation. Our
main technical result is showing that the wavefunction of most bosonic quantum
systems can be extended to a holomorphic function over the complex plane,
allowing the application of powerful techniques from complex analysis. As a
consequence, we prove a version of Hudson's theorem for the wavefunction and
characterize Gaussian dynamics as classical motion of the wavefunction zeros.
Our findings suggest that the non-Gaussianity of quantum optical states can be
detected by measuring a single quadrature of the electromagnetic field, which
we demonstrate in a companion paper [1]. More generally, our results show that
the non-Gaussian features of bosonic quantum systems are encoded in the zeros
of their wavefunction.

</details>


### [412] [Classification of coined quantum walks on the line and comparison to correlated classical random walks](https://arxiv.org/abs/2507.23524)
*Lukas Hantzko,Lennart Binkowski*

Main category: quant-ph

TL;DR: 该研究对一维硬币量子行走进行了分类，确定了对称行走条件，统一了量子和经典行走模型，并通过分析方差缩放和极限分布来区分它们。


<details>
  <summary>Details</summary>
Motivation: 对一维硬币量子行走进行分类，识别导致对称量子行走的初始硬币状态，提供对称量子行走和所有硬币量子行走（在分布等价和行走等价的意义下）的双射和外射参数化，推导修正的行走幅度表达式，将其推广到相关经典随机行走，比较量子和经典动力学，分析方差缩放和极限分布。

Method: 通过识别导致对称量子行走的初始硬币状态，提供对称量子行走和所有硬币量子行走（在分布等价和行走等价的意义下）的双射和外射参数化。推导了行走幅度的修正闭合形式表达式，并将其推广到相关经典随机行走。分析了量子和经典行走模型的方差渐近缩放和极限分布。

Result: 识别了所有导致对称量子行走的初始硬币状态，提供了对称量子行走和所有硬币量子行走（在分布等价和行走等价的意义下）的双射和外射参数化。导出了行走幅度的修正闭合形式表达式，并将其推广到相关经典随机行走。确定了非平凡量子行走具有二次扩展的特点，而经典行走则表现为线性扩展（除了在最大相关性的极端点）。

Conclusion: 该研究对一维硬币量子行走进行了全面的分类，重点关注它们引起的空间概率分布。通过确定所有导致对称量子行走的初始硬币状态，并提供对称量子行走的双射参数化，以及所有硬币量子行走的外射参数化，扩展了对称情况。此外，还导出了行走幅度（walk amplitudes）的修正闭合形式表达式，并将其推广到相关经典随机行走，从而统一了量子和经典动力学的比较。最后，讨论了两种模型的方差渐近缩放，将二次扩展识别为非平凡量子行走的标志，并与经典行走线性行为进行了对比。

Abstract: We present a comprehensive classification of one-dimensional coined quantum
walks on the infinite line, focusing on the spatial probability distributions
they induce. Building on prior results, we identify all initial coin states
that lead to symmetric quantum walks for arbitrary coins, and provide a
bijective parametrisation of all symmetric quantum walks modulo distributional
equivalence. Extending beyond the symmetric case, we also give a surjective
parametrisation of all coined quantum walks under the same equivalence relation
and a bijective parametrisation modulo equivalence of the walks' limiting
distributions.
  Furthermore, we derive corrected closed-form expressions for the walk
amplitudes, resolving inaccuracies in previous literature, and generalise the
approach to the correlated classical random walk. This unified framework
enables a direct comparison between quantum and classical dynamics.
Additionally, we discuss the asymptotic scaling of variances for both models,
identifying quadratic spreading as a hallmark of non-trivial quantum walks and
contrasting it with the linear behaviour of classical walks, except at the
extremal points of maximal correlation. Finally, we compare the limiting
distributions arising from quantum walks with the ones in the classical case.

</details>


### [413] [Quantum-enhanced dark matter detection using Schrödinger cat states](https://arxiv.org/abs/2507.23538)
*Pan Zheng,Yanyan Cai,Bin Xu,Shengcheng Wen,Libo Zhang,Zhongchu Ni,Jiasheng Mai,Yanjie Zeng,Lin Lin,Ling Hu,Xiaowei Deng,Song Liu,Jing Shu,Yuan Xu,Dapeng Yu*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Quantum metrology enables sensitive dark matter detection, particularly using
nonclassical states, such as Schr\"odinger cat states featuring sub-Planck
interference structures in microwave cavities. Here, we report the first
experimental application of four-component Schr\"odinger cat states within a
high-quality superconducting microwave cavity to detect dark photons, a
potential dark matter candidate. We demonstrate an 8.1-fold enhancement in the
signal photon rate and constrain the dark photon kinetic mixing angle to an
unprecedented $\epsilon < 7.32 \times 10^{-16}$ near 6.44~GHz (26.6~$\mu$eV).
By employing a parametric sideband drive to actively tune the cavity frequency,
we achieve dark photon searches and background subtraction across multiple
frequency bins, yielding a sensitivity at the $10^{-16}$ level within a 100~kHz
bandwidth. Our Schr\"odinger's cat-assisted detection (SCaD) scheme
demonstrates a substantial improvement over previous results, promising
potential implications in quantum-enhanced searches for new physics.

</details>


### [414] [Hybrid Quasi-Bound State in the Continuum at Topological Quantum Optics Interface](https://arxiv.org/abs/2507.23582)
*Yue-Zhi Zhang,Leong-Chuan Kwek,Wei Nie*

Main category: quant-ph

TL;DR: 通过拓扑原子阵列和光-物质相互作用，实现了拓扑准BIC，并发现了其与光放大的时间反转关系。


<details>
  <summary>Details</summary>
Motivation: 拓扑光操纵为光子学技术提供了一种多功能的工具箱。

Method: 通过对称性保护的光-物质相互作用，利用拓扑原子阵列在波导中诱导光子局域化，并研究了准BIC与光放大的时间反转关系。

Result: 发现了由边缘态和体态之间的量子干涉产生的、在临界耦合条件下形成的混合光物质准BIC，并揭示了准BIC与光放大的时间反转关系，最终实现定向超窄放大器。

Conclusion: 这项工作展示了在拓扑量子光学界面上具有潜在应用的新颖的准BIC，可用于量子设备。

Abstract: Topological manipulation of light provides a versatile toolbox for photonic
technologies. Here, we show that a topological atom array can induce photon
localization in a waveguide via symmetry-protected light-matter interaction.
Long-lived photon-atom entanglement reveals the existence of a novel
topological quasi-bound state in the continuum (quasi-BIC). This hybrid
light-matter quasi-BIC is formed at a critical coupling condition via
collectively induced absorption, which is produced by quantum interference
between edge and bulk states. We uncover the time-reversed relation between
topological quasi-BIC and light amplification. Interestingly, one can realize a
directional ultranarrow amplifier by means of critical coupling. Our work
demonstrates an unconventional quasi-BIC at a topological quantum optics
interface with potential applications in quantum devices.

</details>


### [415] [Characterizing the Kirkwood-Dirac positivity on second countable LCA groups](https://arxiv.org/abs/2507.23628)
*Matéo Spriet*

Main category: quant-ph

TL;DR: This paper defines and analyzes the Kirkwood-Dirac quasiprobability representation in an abstract setting, generalizing results about quantum mechanics and classical fragments to a broader class of groups, particularly those with compact connected components.


<details>
  <summary>Details</summary>
Motivation: To define and analyze the Kirkwood-Dirac quasiprobability representation in an abstract setting, explore its connections to existing quantization methods, and generalize known results about states with positive distributions and the non-triviality of the classical fragment.

Method: We define the Kirkwood-Dirac quasiprobability representation and discuss its link with Kohn-Nirenberg quantization. We use this to interpret Wigner-Weyl quantization as a symmetric ordering and identify generalized pure states with positive Kirkwood-Dirac distributions as Haar measures on closed subgroups. We also show that the classical fragment is non-trivial if and only if the group has a compact connected component.

Result: The Kirkwood-Dirac distribution can be interpreted as a symmetric ordering in an abstract setting. Generalized pure states with positive Kirkwood-Dirac distributions are Haar measures on closed subgroups. The classical fragment is non-trivial for groups with a compact connected component, and we provide a geometric description for connected compact abelian groups.

Conclusion: We provide a complete geometric description of the classical fragment for connected compact abelian groups, generalizing known results for finite abelian groups.

Abstract: We define the Kirkwood-Dirac quasiprobability representation of quantum
mechanics associated with the Fourier transform over second countable locally
compact abelian groups. We discuss its link with the Kohn-Nirenberg
quantization of the phase space $G\times \widehat{G}$. We use it to argue that
in this abstract setting the Wigner-Weyl quantization, when it exists, can
still be interpreted as a symmetric ordering. Then, we identify all generalized
(non-normalizable) pure states having a positive Kirkwood-Dirac distribution.
They are, up to the natural action of the Weyl-Heisenberg group, Haar measures
on closed subgroups. This generalizes a result known for finite abelian groups.
We then show that the classical fragment of quantum mechanics associated with
the Kirkwood-Dirac distribution is non-trivial if and only if the group has a
compact connected component. Finally, we provide for connected compact abelian
groups a complete geometric description of this classical fragment.

</details>


### [416] [Probing electronic state-dependent conformational changes in a trapped Rydberg ion Wigner crystal](https://arxiv.org/abs/2507.23631)
*Marion Mallweger,Natalia Kuk,Vinay Shankar,Robin Thomm,Harry Parke,Ivo Straka,Weibin Li,Igor Lesanovsky,Markus Hennrich*

Main category: quant-ph

TL;DR: 通过激光耦合里德堡态，首次在三离子晶体中观察到由状态决定的构象变化，实现了从线性到之字形的结构相变，为研究人工分子系统开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 分子动力学中的状态相关构象变化难以观察和模拟，提出利用冷、囚禁离子进行量子模拟的替代方法。

Method: 利用激光将三离子晶体中心的单个离子耦合到里德堡态，并通过调谐系统接近结构相变点来诱导构象变化。

Result: 观察到里德堡激发诱导了从线性到之字形的晶体结构相变，这种结构变化产生了清晰的光谱信号，表明振动和电子状态之间存在强烈的杂合。 发现里德堡离子可以模拟分子构象变化。 

Conclusion: 本研究首次通过实验观察到里德堡离子诱导的类分子构象变化，为利用里德堡离子构建和研究人工分子系统奠定了基础。

Abstract: State-dependent conformational changes play a central role in molecular
dynamics, yet they are often difficult to observe or simulate due to their
complexity and ultrafast nature. One alternative approach is to emulate such
phenomena using quantum simulations with cold, trapped ions. In their
electronic ground state, these ions form long-lived Wigner crystals. When
excited to high-lying electronic Rydberg states, the ions experience a modified
trapping potential, resulting in a strong coupling between their electronic and
vibrational degrees of freedom. In an ion crystal, this vibronic coupling
creates electronic state-dependent potential energy surfaces that can support
distinct crystal structures -- closely resembling the conformational changes of
molecules driven by electronic excitations. Here, we present the first
experimental observation of this effect, by laser-coupling a single ion at the
centre of a three-ion crystal to a Rydberg state. By tuning the system close to
a structural phase transition, the excitation induces a state-dependent
conformational change, transforming the Wigner crystal from a linear to a
zigzag configuration. This structural change leads to a strong hybridisation
between vibrational and electronic states, producing a clear spectroscopic
signature in the Rydberg excitation. Our findings mark the first experimental
step towards using Rydberg ions to create and study artificial molecular
systems. change leads to a strong hybridisation between vibrational and
electronic states, producing a clear spectroscopic signature in the Rydberg
excitation. Our findings mark the first experimental step towards using Rydberg
ions to create and study artificial molecular systems.

</details>


### [417] [Charge acceleration without radiation](https://arxiv.org/abs/2507.23650)
*Yakir Aharonov,Daniel Collins,Sandu Popescu*

Main category: quant-ph

TL;DR: 量子力学中，电荷加速不一定会产生辐射，这颠覆了传统认知，并可能影响对所有辐射的理解。


<details>
  <summary>Details</summary>
Motivation: 挑战了传统物理学中‘加速的电荷会辐射’的普遍认知，意图从根本上重新理解辐射的机制。

Method: 通过理论推导和论证，利用阿哈罗诺夫-博姆效应的原理，证明了在特定量子力学条件下，加速的电荷不会产生电磁辐射。

Result: 量子力学允许电荷在不受力的情况下加速而不产生辐射，这揭示了辐射机制的深刻新见解，并可能对整个辐射理论产生影响。

Conclusion: 本研究证明了量子力学中电荷在没有力的作用下可以被加速而不产生辐射，并且这一效应可能超越电磁学范畴，适用于任何辐射。

Abstract: The existence of electromagnetic radiation - radio-waves, microwaves, light,
x-rays and so on - is one of the most important physical phenomena, and our
ability to manipulate them is one of the most significant technological
achievement of humankind. Underlying this ability is our understanding of how
radiation is produced: whenever an electric charge is accelerated, it radiates.
Or, at least, this is how it has been hitherto universally thought. Here we
prove that quantum mechanically electric charges can be accelerated without
radiating. The physical setup leading to this behavior is relatively simple
(once one knows what to do) but its reasons are deep: it relies on the fact
that quantum mechanically particles can be accelerated even when no forces act
on them, via the Aharonov-Bohm effect. As we argue, the effect presented here
is just them tip of an iceberg - it implies the need to reconsider the basic
understanding of radiation. Finally, it seems clear that the effect goes far
beyond electromagnetism and applies to any kind of radiation.

</details>


### [418] [Swap Network Augmented Ansätze on Arbitrary Connectivity](https://arxiv.org/abs/2507.23679)
*Teodor Parella-Dilmé*

Main category: quant-ph

TL;DR: Efficiently parametrize quantum states for hybrid algorithms by optimizing qubit routing with swap networks and embedding them in connectivity-aware ansätze, improving trainability and resource efficiency for simulating strongly correlated systems.


<details>
  <summary>Details</summary>
Motivation: Efficient parametrizations of quantum states are essential for trainable hybrid classical-quantum algorithms, but adapting to the available qubit connectivity, which limits the capacity to generate correlations between distant qubits in a resource-efficient and trainable manner, remains a key challenge.

Method: Introduce an algorithm that optimizes qubit routing for arbitrary connectivity graphs, resulting in a swap network that enables direct interactions between any pair of qubits. Propose a co-design of circuit layers and qubit routing by embedding the derived swap networks within layered, connectivity-aware ansätze.

Result: The swap-enhanced ansatz consistently achieves lower energy errors using fewer entangling gates, shallower circuits, and fewer parameters than standard layered-structured baselines across exemplified connectivities. Showcase improvements through ground-state simulations of strongly correlated systems, including spin-glass and molecular electronic structure models.

Conclusion: The proposed swap network augmented ansätze provide enhanced trainability and resource-efficient design for capturing complex correlations on devices with constrained qubit connectivity.

Abstract: Efficient parametrizations of quantum states are essential for trainable
hybrid classical-quantum algorithms. A key challenge in their design consists
in adapting to the available qubit connectivity of the quantum processor, which
limits the capacity to generate correlations between distant qubits in a
resource-efficient and trainable manner. In this work we first introduce an
algorithm that optimizes qubit routing for arbitrary connectivity graphs,
resulting in a swap network that enables direct interactions between any pair
of qubits. We then propose a co-design of circuit layers and qubit routing by
embedding the derived swap networks within layered, connectivity-aware
ans\"atze. This construction significantly improves the trainability of the
ansatz, leading to enhanced performance with reduced resources. We showcase
these improvements through ground-state simulations of strongly correlated
systems, including spin-glass and molecular electronic structure models. Across
exemplified connectivities, the swap-enhanced ansatz consistently achieves
lower energy errors using fewer entangling gates, shallower circuits, and fewer
parameters than standard layered-structured baselines. Our results indicate
that swap network augmented ans\"atze provide enhanced trainability and
resource-efficient design to capture complex correlations on devices with
constrained qubit connectivity.

</details>


### [419] [Probing graph topology from local quantum measurements](https://arxiv.org/abs/2507.23689)
*F. Romeo,J. Settino*

Main category: quant-ph

TL;DR: 研究表明，通过局部量子测量可以了解整个量子网络的结构。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于探索从局部量子测量中推断未知量子网络全局属性的可能性，并为未来的量子互联网基础设施提供新的入侵检测和结构诊断策略。

Method: 该方法结合了短期量子演化和具有可训练权重的非迭代线性读出。这种入侵策略受到极端学习和量子水库计算的启发。

Result: 本研究展示了即使只访问一小部分节点，恶意行为者也能通过初始化局部量子态和重复的短时测量来提取关于整个网络的敏感结构信息。

Conclusion: 可以从严格的局部量子测量中推断出未知量子网络的全局属性，例如平均度、中心节点密度和固定长度的闭合路径数。恶意行为者只需访问一小部分节点，即可通过初始化局部量子态和重复的短时测量来提取有关整个网络的敏感结构信息。

Abstract: We show that global properties of an unknown quantum network, such as the
average degree, hub density, and the number of closed paths of fixed length,
can be inferred from strictly local quantum measurements. In particular, we
demonstrate that a malicious agent with access to only a small subset of nodes
can initialize quantum states locally and, through repeated short-time
measurements, extract sensitive structural information about the entire
network. The intrusion strategy is inspired by extreme learning and quantum
reservoir computing and combines short-time quantum evolution with a
non-iterative linear readout with trainable weights. These results suggest new
strategies for intrusion detection and structural diagnostics in future quantum
Internet infrastructures.

</details>


### [420] [Sandwich test for Quantum Phase Estimation](https://arxiv.org/abs/2507.23716)
*Avatar Tulsi*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Quantum Phase Estimation (QPE) has potential for a scientific revolution
through numerous practical applications like finding better medicines,
batteries, materials, catalysts etc. Many QPE algorithms use the Hadamard test
to estimate $\langle \psi|U^{k}|\psi\rangle$ for a large integer $k$ for an
efficiently preparable initial state $|\psi\rangle$ and an efficiently
implementable unitary operator $U$. The Hadamard test is hard to implement
because it requires controlled applications of $U^{k}$. Recently, a Sequential
Hadamard test (SHT) was proposed (arXiv:2506.18765) which requires controlled
application of $U$ only but its total run time $T_{\rm tot}$ scales as
$\mathcal{O}(k^{3}/\epsilon^{2}r_{\rm min}^{2})$ where $r_{\rm min}$ is the
minimum value of $|\langle \psi|U^{k'}|\psi\rangle|$ among all integers $k'
\leq k$. Typically $r_{\rm min}$ is exponentially low and SHT becomes too slow.
We present a new algorithm, the SANDWICH test to address this bottleneck. Our
algorithm uses efficient preparation of the initial state $|\psi\rangle$ to
efficiently implement the SPROTIS operator $R_{\psi}^{\phi}$ where SPROTIS
stands for the Selective Phase Rotation of the Initial State. It sandwiches the
SPROTIS operator between $U^{a}$ and $U^{b}$ for integers $\{a,b\} \leq k$ to
estimate $\langle \psi|U^{k}|\psi\rangle$. The total run time $T_{\rm tot}$ is
$\mathcal{O}(k^{2}\ln k/ \epsilon^{2} s_{\rm min}^{6})$. Here $s_{\rm min}$ is
the minimum value of $|\langle \psi|U^{\hat{k}}|\psi\rangle$ among all integers
$\hat{k}$ which are values of the nodes of a random binary sum tree whose root
node value is $k$ and leaf nodes' values are $1$ or $0$. It can be reasonably
expected that $s_{\rm min} \not\ll 1$ in typical cases because there is wide
freedom in choosing the random binary sum tree.

</details>


### [421] [Quantum scarring enhances non-Markovianity of subsystem dynamics](https://arxiv.org/abs/2507.23757)
*Aditya Banerjee*

Main category: quant-ph

TL;DR: 量子疤痕增强子系统的非马尔可夫性，研究通过 PXP 模型及其形变进行了数值验证。


<details>
  <summary>Details</summary>
Motivation: 旨在探索量子疤痕（一种不进行弛豫和热化的非能动动力学状态）与开放量子系统动力学非马尔可夫性之间的联系，以及量子疤痕如何影响子系统的动力学行为。

Method: 通过数值方法研究 PXP 模型及其形变，探查信息回流，并分析时间上分离的状态之间的距离的动力学行为，以识别和量化子系统动力学的非马尔可夫性。

Result: 量子疤痕的存在能够增强子系统的非马尔可夫性。PXP 模型及其形变在具有量子疤痕特征时表现出更强的子系统非马尔可夫性，而消除疤痕特征的形变则减弱了这种非马尔可夫性。

Conclusion: 该研究提供了数值证据，表明量子疤痕的存在是实现和增强子系统动力学非马尔可夫性的微观要素。通过研究 PXP 模型及其形变，并结合探测时间上分离的状态之间的距离的动力学行为来探查信息回流，研究展示了子系统非马尔可夫性的系统性特征。结果表明，增强或消除疤痕动力学特征的形变，也分别增强或减弱了子系统的非马尔可夫性。这为量子疤痕的动力学记忆提供了新的视角，并为结合开放量子系统方法研究远离平衡和非热化的孤立量子多体系统开辟了新的方向。

Abstract: Given that any subsystem of a closed out-of-equilibrium quantum system is an
open quantum system, its dynamics (reduced from the full system's unitary
evolution) can be either Markovian (memory-less) or non-Markovian, with the
latter necessarily impeding the process of relaxation and thermalization.
Seemingly independently, such non-ergodic dynamics occurs when an initial state
has spectral weight on the so-called quantum scar states, which are
non-thermalizing states embedded deep in the spectrum of otherwise thermal
states. In this article, we present numerical evidence that the presence of
quantum scars is a microscopic ingredient that enables and enhances
non-Markovianity of the dynamics of subsystems. We exemplify this with the PXP
model and its deformations which either enhance or erase the signatures of
scarred dynamics when quenched from a simple product state that is known to
have significant overlaps with the scarred subspace in the spectrum. By probing
information backflows with the dynamical behaviour of the distances between
temporally-separated states of small subsystems, systematic signatures of
subsystem non-Markovianity in these models are presented, and it is seen that
scarring-enhancing (erasing) deformations also exhibit enhanced (diminished)
subsystem non-Markovianity. This sheds new light on the dynamical memories
associated with quantum scarring, and opens interesting new questions at the
interface of quantum scarring and an open quantum systems approach to
investigating far-from-equilibrium and non-thermalizing isolated quantum
many-body systems.

</details>


### [422] [Universal tradeoff relations between resource cost and irreversibility of channels: General-resource Wigner-Araki-Yanase theorems and beyond](https://arxiv.org/abs/2507.23760)
*Hiroyasu Tajima,Koji Yamaguchi,Ryuji Takagi,Yui Kuramochi*

Main category: quant-ph

TL;DR: 量子过程的不可逆性越低，实现它所需的物理资源成本就越高。该研究发现了能量、魔力、相干性等多种资源与不可逆性之间的普适权衡关系，并推导出了一系列关键结果，包括能量-误差权衡、测量限制的推广以及对某些操作实现成本的界定。


<details>
  <summary>Details</summary>
Motivation: 设计下一代量子设备需要确定实现所需量子过程所需的物理资源及其数量。本研究旨在解决这一核心问题。

Method: 通过量子资源理论的语言，证明了一种普适的成本-不可逆性权衡关系，该关系适用于多种资源（能量、魔力、不对称性、相干性、非热性等），并为任何量子信道提供了资源成本的下界。

Result: (1) 证明了任意信道之间能量成本和不可逆性之间的普适关系，包含了任何测量或酉门在能量-误差方面的权衡；(2) 将能量-误差权衡扩展到自由能和功耗成本；(3) 将Wigner-Araki-Yanase定理（关于守恒定律下测量的普适限制）扩展到广泛的资源理论：通过测量区分资源状态的失败概率与其资源成本成反比；(4) 证明了无限多不增加资源的量纲操作实际上需要无限的实现成本。

Conclusion: 该研究揭示了量子性与不可逆性之间的普适关系，为理解量子性如何抑制不可逆性奠定了基础。

Abstract: Quantum technologies offer exceptional -- sometimes almost magical -- speed
and performance, yet every quantum process costs physical resources. Designing
next-generation quantum devices, therefore, depends on solving the following
question: which resources, and in what amount, are required to implement a
desired quantum process? Casting the problem in the language of quantum
resource theories, we prove a universal cost-irreversibility tradeoff: the
lower the irreversibility of a quantum process, the greater the required
resource cost for its realization. The trade-off law holds for a broad range of
resources -- energy, magic, asymmetry, coherence, athermality, and others --
yielding lower bounds on resource cost of any quantum channel. Its broad scope
positions this result as a foundation for deriving the following key results:
(1) we show a universal relation between the energetic cost and the
irreversibility for arbitrary channels, encompassing the energy-error tradeoff
for any measurement or unitary gate; (2) we extend the energy-error tradeoff to
free energy and work costs; (3) we extend the Wigner-Araki-Yanase theorem,
which is the universal limitation on measurements under conservation laws, to a
wide class of resource theories: the probability of failure in distinguishing
resourceful states via a measurement is inversely proportional to its resource
cost; (4) we prove that infinitely many resource-non-increasing operations in
fact require an infinite implementation cost. These findings reveal a universal
relationship between quantumness and irreversibility, providing a first step
toward a general theory that explains when -- and how -- quantumness can
suppress irreversibility.

</details>


### [423] [Intrinsic Heralding and Optimal Decoders for Non-Abelian Topological Order](https://arxiv.org/abs/2507.23765)
*Dian Jing,Pablo Sala,Liang Jiang,Ruben Verresen*

Main category: quant-ph

TL;DR: Non-Abelian topological orders can be more stable to noise than Abelian ones when using fusion products for error correction, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand and enhance the stability of topological order (TO) against noise, particularly for non-Abelian TOs, which has been less understood compared to Abelian TOs. The goal is to leverage non-Abelian properties for improved quantum information storage and manipulation.

Method: This paper utilizes the non-deterministic fusion of non-Abelian anyons to develop active error correction decoders. It formulates an approach for determining optimal thresholds for non-Abelian topological orders with perfect anyon syndromes as a statistical mechanics model employing Bayesian inference. Numerical illustrations are provided for the D4 ≅ Z4 rtimes Z2 topological order.

Result: The paper shows that using fusion products of non-Abelian anyons to herald noise enhances thresholds over Abelian counterparts when noise is dominated by a single non-Abelian anyon type. For non-Abelian charge noise and perfect syndrome measurement in D4 TO, an optimal threshold of pc=0.218(1) is found. An intrinsically heralded minimal-weight perfect-matching decoder achieves pc=0.20842(2), outperforming the standard minimal-weight perfect-matching decoder's pc=0.15860(1).

Conclusion: The study demonstrates that non-Abelian topological orders can enhance stability against noise, contrary to common assumptions, and offers methods for optimizing error correction thresholds. It also suggests pathways for achieving fault tolerance through generalizations.

Abstract: Topological order (TO) provides a natural platform for storing and
manipulating quantum information. However, its stability to noise has only been
systematically understood for Abelian TOs. In this work, we exploit the
non-deterministic fusion of non-Abelian anyons to inform active error
correction and design decoders where the fusion products, instead of flag
qubits, herald the noise. This intrinsic heralding enhances thresholds over
those of Abelian counterparts when noise is dominated by a single non-Abelian
anyon type. Furthermore, we present an approach for determining the optimal
threshold for non-Abelian TOs with perfect anyon syndromes for any noise model,
formulated as a statistical mechanics model using Bayesian inference. We
numerically illustrate these results for $D_4 \cong \mathbb Z_4 \rtimes \mathbb
Z_2$ TO. In particular, for non-Abelian charge noise and perfect syndrome
measurement, we find an optimal threshold $p_c=0.218(1)$, whereas an
intrinsically heralded minimal-weight perfect-matching (MWPM) decoder already
gives $p_c=0.20842(2)$, outperforming standard MWPM with $p_c = 0.15860(1)$.
Our work highlights how non-Abelian properties can enhance stability, rather
than reduce it, and discusses potential generalizations for achieving fault
tolerance.

</details>
