{"id": "2508.09422", "categories": ["cs.DS", "cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.09422", "abs": "https://arxiv.org/abs/2508.09422", "authors": ["Meghal Gupta", "William He", "Ryan O'Donnell", "Noah G. Singer"], "title": "A Classical Quadratic Speedup for Planted $k$XOR", "comment": "22 pages", "summary": "A recent work of Schmidhuber et al (QIP, SODA, & Phys. Rev. X 2025) exhibited\na quantum algorithm for the noisy planted $k$XOR problem running quartically\nfaster than all known classical algorithms. In this work, we design a new\nclassical algorithm that is quadratically faster than the best previous one, in\nthe case of large constant $k$. Thus for such $k$, the quantum speedup of\nSchmidhuber et al. becomes only quadratic (though it retains a space\nadvantage). Our algorithm, which also works in the semirandom case, combines\ntools from sublinear-time algorithms (essentially, the birthday paradox) and\npolynomial anticoncentration.", "AI": {"tldr": "\u65b0\u7ecf\u5178\u7b97\u6cd5\u5728\u5927\u578b\u5e38\u6570k\u65f6\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u7b97\u6cd5\u5feb\u4e24\u500d\uff0c\u964d\u4f4e\u4e86\u91cf\u5b50\u52a0\u901f\u6bd4\u3002", "motivation": "\u9488\u5bf9Schmidhuber\u7b49\u4eba\u63d0\u51fa\u7684\u9488\u5bf9\u5608\u6742\u5df2\u79cd\u690dkXOR\u95ee\u9898\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u8bbe\u8ba1\u4e00\u79cd\u66f4\u5feb\u7684\u7ecf\u5178\u7b97\u6cd5\u3002", "method": "\u7ed3\u5408\u4e86\u5b50\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\uff08\u4e3b\u8981\u662f\u751f\u65e5\u6096\u8bba\uff09\u548c\u591a\u9879\u5f0f\u53cd\u6d53\u7f29\u7684\u5de5\u5177\u3002", "result": "\u65b0\u8bbe\u8ba1\u7684\u7ecf\u5178\u7b97\u6cd5\u901f\u5ea6\u662f\u5148\u524d\u6700\u4f73\u7b97\u6cd5\u7684\u4e24\u500d\uff0c\u5728\u5927\u578b\u5e38\u6570k\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u91cf\u5b50\u52a0\u901f\u6bd4\u4ece\u56db\u500d\u964d\u4f4e\u5230\u4e24\u500d\u3002", "conclusion": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u7ecf\u5178\u7b97\u6cd5\uff0c\u5728\u5927\u578b\u5e38\u6570k\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u901f\u5ea6\u662f\u5148\u524d\u6700\u4f73\u7b97\u6cd5\u7684\u4e24\u500d\uff0c\u4ece\u800c\u5c06Schmidhuber\u7b49\u4eba\u7684\u91cf\u5b50\u7b97\u6cd5\u5728\u8fd9\u4e9bk\u4e0a\u7684\u52a0\u901f\u6bd4\u4ece\u56db\u500d\u964d\u4f4e\u5230\u4e24\u500d\uff0c\u4f46\u91cf\u5b50\u7b97\u6cd5\u4ecd\u7136\u5177\u6709\u7a7a\u95f4\u4f18\u52bf\u3002\u8be5\u7b97\u6cd5\u4e5f\u9002\u7528\u4e8e\u534a\u968f\u673a\u60c5\u51b5\u3002"}}
{"id": "2508.09325", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09325", "abs": "https://arxiv.org/abs/2508.09325", "authors": ["Alexandre Brown", "Glen Berseth"], "title": "SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning", "comment": null, "summary": "Visual reinforcement learning (RL) is challenging due to the need to learn\nboth perception and actions from high-dimensional inputs and noisy rewards.\nAlthough large perception models exist, integrating them effectively into RL\nfor visual generalization and improved sample efficiency remains unclear. We\npropose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment\nAnything (SAM) for object-centric decomposition and YOLO-World to ground\nsegments semantically via text prompts. It includes a novel transformer-based\narchitecture that supports a dynamic number of segments at each time step and\neffectively learns which segments to focus on using online RL, without using\nhuman labels. By evaluating SegDAC over a challenging visual generalization\nbenchmark using Maniskill3, which covers diverse manipulation tasks under\nstrong visual perturbations, we demonstrate that SegDAC achieves significantly\nbetter visual generalization, doubling prior performance on the hardest setting\nand matching or surpassing prior methods in sample efficiency across all\nevaluated tasks.", "AI": {"tldr": "SegDAC, a new method using SAM and YOLO-World for visual RL, enhances generalization and sample efficiency by decomposing tasks based on semantic segments, outperforming prior methods on challenging manipulation tasks.", "motivation": "Visual reinforcement learning (RL) is challenging due to the need to learn both perception and actions from high-dimensional inputs and noisy rewards. Integrating large perception models effectively into RL for visual generalization and improved sample efficiency remains unclear.", "method": "SegDAC uses Segment Anything (SAM) for object-centric decomposition and YOLO-World to ground segments semantically via text prompts. It includes a novel transformer-based architecture that supports a dynamic number of segments at each time step and effectively learns which segments to focus on using online RL, without using human labels.", "result": "SegDAC achieves significantly better visual generalization, doubling prior performance on the hardest setting and matching or surpassing prior methods in sample efficiency across all evaluated tasks on the Maniskill3 benchmark.", "conclusion": "SegDAC methods achieve significantly better visual generalization, doubling prior performance on the hardest setting and matching or surpassing prior methods in sample efficiency across all evaluated tasks."}}
