<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 57]
- [cs.CL](#cs.CL) [Total: 28]
- [cs.AI](#cs.AI) [Total: 15]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 15]
- [cs.LG](#cs.LG) [Total: 71]
- [physics.app-ph](#physics.app-ph) [Total: 3]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.RO](#cs.RO) [Total: 21]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.NE](#cs.NE) [Total: 3]
- [eess.SY](#eess.SY) [Total: 12]
- [cs.SI](#cs.SI) [Total: 5]
- [eess.SP](#eess.SP) [Total: 14]
- [cs.DS](#cs.DS) [Total: 5]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 19]
- [cs.DC](#cs.DC) [Total: 7]
- [quant-ph](#quant-ph) [Total: 55]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model](https://arxiv.org/abs/2509.02659)
*Zilong Guo,Yi Luo,Long Sha,Dongxu Wang,Panqu Wang,Chenyang Xu,Yi Yang*

Main category: cs.CV

TL;DR: 研究表明，将端到端架构设计与多模态视觉语言模型（VLM）相结合，可以显著提升自动驾驶任务的性能，并且仅使用单个摄像头就能在排行榜上取得最佳的纯视觉解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管端到端自动驾驶受到广泛关注，但使用强大的大型语言模型（LLM），特别是多模态视觉语言模型（VLM）是否能让端到端驾驶任务受益仍然是个问题。

Method: 结合端到端架构设计和知识密集型VLM。

Result: 所提出的方法仅使用单个摄像头，并且是在排行榜上表现最佳的纯视觉解决方案，证明了基于视觉的驾驶方法的有效性以及端到端驾驶任务的潜力。

Conclusion: 将端到端架构设计与多模态视觉语言模型（VLM）相结合，可以显著提升自动驾驶任务的性能。

Abstract: End-to-end autonomous driving has drawn tremendous attention recently. Many
works focus on using modular deep neural networks to construct the end-to-end
archi-tecture. However, whether using powerful large language models (LLM),
especially multi-modality Vision Language Models (VLM) could benefit the
end-to-end driving tasks remain a question. In our work, we demonstrate that
combining end-to-end architectural design and knowledgeable VLMs yield
impressive performance on the driving tasks. It is worth noting that our method
only uses a single camera and is the best camera-only solution across the
leaderboard, demonstrating the effectiveness of vision-based driving approach
and the potential for end-to-end driving tasks.

</details>


### [2] [PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?](https://arxiv.org/abs/2509.02807)
*Mennatullah Siam*

Main category: cs.CV

TL;DR: 现有的视频多模态大语言模型(V-MLLM)在像素级视觉定位方面能力不足，因为它们可能无法有效利用运动信息，并且现有基准测试未能充分评估这一点。本研究提出了一个名为MoCentric-Bench的新基准，用于评估V-MLLM在理解和利用运动模式进行像素级视觉定位方面的能力，并展示了有效的模型适应技术。


<details>
  <summary>Details</summary>
Motivation: 评估视频多模态大语言模型(V-MLLM)在理解和利用运动信息进行像素级视觉定位方面的能力，并解决现有基准测试的不足之处。

Method: 提出四种以运动为中心的探测技术，并构建了一个名为MoCentric-Bench的新基准。通过实验评估V-MLLM在区分真实运动和虚假运动以及理解运动顺序方面的能力。同时，建立了强大的单图像基线，并探索了运动适应技术。

Result: 现有V-MLLM在MoCentric-Bench上的表现不佳，表明它们未能有效利用运动信息。单图像基线模型表现出与现有方法相当甚至更优的性能。所提出的运动适应技术在MoCentric-Bench上达到了最先进的性能。

Conclusion: 现有的V-MLLM在像素级视觉定位方面，尤其是在理解和利用运动信息方面存在不足。本研究提出的MoCentric-Bench为评估这一能力提供了新的视角，并强调了未来模型在密集时空定位和像素级视频理解方面需要改进。

Abstract: Multi-modal large language models (MLLMs) have shown impressive
generalization across tasks using images and text modalities. While their
extension to video has enabled tasks such as video question answering and video
captioning, their pixel-level visual grounding abilities are less studied. In
this work, we raise the pertinent question of whether motion is used in
pixel-level visual grounding and whether video MLLMs can segment objects based
on natural language expressions describing their motion patterns. We identify
the shortcomings in the current benchmarks, where we show that a single frame
can often suffice for capturing the motion referring expression without any
temporal reasoning. To address this, we introduce four motion-centric probing
techniques, particularly designed for the visual grounding task, to study video
MLLMs' ability to identify true motion from a fake one and their ability to
grasp the motion order. Consequently, we provide a motion-centric benchmark,
MoCentric-Bench. It ensures that video MLLMs are evaluated towards leveraging
the interaction between motion and language rather than being dominated by
static appearance cues emphasized in existing visual grounding datasets. We
further establish strong single-image baselines that are on par with or
outperform prior methods. Finally, we explore simple motion-centric adaptation
techniques that provide state-of-the-art performance on our MoCentric-Bench.
Our motion-centric benchmark, evaluation and findings challenge future models
to improve dense spatiotemporal grounding and pixel-level understanding within
videos. Code and datasets will be made publicly available at
https://github.com/MSiam/PixFoundation-2.0.git.

</details>


### [3] [Multi-Scale Deep Learning for Colon Histopathology: A Hybrid Graph-Transformer Approach](https://arxiv.org/abs/2509.02851)
*Sadra Saremi,Amirhossein Ahmadkhan Kordbacheh*

Main category: cs.CV

TL;DR: A novel hybrid deep learning model (HG-TNet) combining CNNs and Transformers shows improved colon cancer classification accuracy on histopathological images by capturing multi-scale features and preserving spatial information.


<details>
  <summary>Details</summary>
Motivation: Early-stage detection of colon cancer is crucial, and existing methods need improvement for accurate classification of histopathological images.

Method: The proposed HG-TNet model uses a hybrid architecture with a CNN branch for local features and a Transformer branch for global context. It incorporates capsule networks for spatial order preservation and a self-supervised rotation prediction objective.

Result: The HG-TNet model demonstrates superior performance in accuracy and loss function compared to standard architectures, effectively utilizing capsule networks to understand spatial relationships within the images.

Conclusion: The proposed HG-TNet model, by synergizing CNNs, Transformers, and capsule networks, offers a robust approach for advancing colon cancer classification in histopathological images.

Abstract: Colon cancer also known as Colorectal cancer, is one of the most malignant
types of cancer worldwide. Early-stage detection of colon cancer is highly
crucial to prevent its deterioration. This research presents a hybrid
multi-scale deep learning architecture that synergizes capsule networks, graph
attention mechanisms, transformer modules, and residual learning to advance
colon cancer classification on the Lung and Colon Cancer Histopathological
Image Dataset (LC25000) dataset. The proposed model in this paper utilizes the
HG-TNet model that introduces a hybrid architecture that joins strength points
in transformers and convolutional neural networks to capture multi-scale
features in histopathological images. Mainly, a transformer branch extracts
global contextual bonds by partitioning the image into patches by
convolution-based patch embedding and then processing these patches through a
transformer encoder. Analogously, a dedicated CNN branch captures fine-grained,
local details through successive Incorporation these diverse features, combined
with a self-supervised rotation prediction objective, produce a robust
diagnostic representation that surpasses standard architectures in performance.
Results show better performance not only in accuracy or loss function but also
in these algorithms by utilizing capsule networks to preserve spatial orders
and realize how each element individually combines and forms whole structures.

</details>


### [4] [PRECISE-AS: Personalized Reinforcement Learning for Efficient Point-of-Care Echocardiography in Aortic Stenosis Diagnosis](https://arxiv.org/abs/2509.02898)
*Armin Saadat,Nima Hashemi,Hooman Vaseli,Michael Y. Tsang,Christina Luong,Michiel Van de Panne,Teresa S. M. Tsang,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: Aortic stenosis (AS) diagnosis can be improved by a reinforcement learning (RL) framework that dynamically selects the most informative echocardiogram videos, achieving 80.6% accuracy with 47% fewer videos compared to full acquisition.


<details>
  <summary>Details</summary>
Motivation: Limited access to echocardiography (echo) and the expertise required for point-of-care ultrasound (POCUS) hinder aortic stenosis (AS) diagnosis, especially in resource-limited areas.

Method: A reinforcement learning (RL)-driven active video acquisition framework was developed to dynamically select the most informative echo videos for AS diagnosis, optimizing for accuracy and efficiency.

Result: The RL framework achieved 80.6% classification accuracy while utilizing only 47% of the echo videos compared to a full acquisition, tested on 2,572 patients.

Conclusion: The proposed active feature acquisition method enhances AS diagnosis by making echocardiographic assessments more efficient, scalable, and personalized.

Abstract: Aortic stenosis (AS) is a life-threatening condition caused by a narrowing of
the aortic valve, leading to impaired blood flow. Despite its high prevalence,
access to echocardiography (echo), the gold-standard diagnostic tool, is often
limited due to resource constraints, particularly in rural and underserved
areas. Point-of-care ultrasound (POCUS) offers a more accessible alternative
but is restricted by operator expertise and the challenge of selecting the most
relevant imaging views. To address this, we propose a reinforcement learning
(RL)-driven active video acquisition framework that dynamically selects each
patient's most informative echo videos. Unlike traditional methods that rely on
a fixed set of videos, our approach continuously evaluates whether additional
imaging is needed, optimizing both accuracy and efficiency. Tested on data from
2,572 patients, our method achieves 80.6% classification accuracy while using
only 47% of the echo videos compared to a full acquisition. These results
demonstrate the potential of active feature acquisition to enhance AS
diagnosis, making echocardiographic assessments more efficient, scalable, and
personalized. Our source code is available at:
https://github.com/Armin-Saadat/PRECISE-AS.

</details>


### [5] [LiGuard: A Streamlined Open-Source Framework for Rapid & Interactive Lidar Research](https://arxiv.org/abs/2509.02902)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: LiGuard是一个开源软件框架，旨在解决激光雷达（lidar）数据处理和分析中的重复劳动问题，通过提供内置的数据输入/输出、预/后处理和常用算法支持，以及灵活的算法添加/移除/参数调整和结果可视化功能，加速激光雷达在自动驾驶和智能交通系统（ITS）中的研究和开发。


<details>
  <summary>Details</summary>
Motivation: 激光雷达在自动驾驶和ITS领域受到广泛关注，但目前的研究方法常导致代码重复和维护困难。

Method: 提出LiGuard开源软件框架，提供数据I/O、预/后处理、常用算法支持，并允许用户自定义算法、调整参数和可视化结果。LiGuard生成的代码具有结构化目录，便于项目和组件的共享。

Result: 通过案例研究证明了LiGuard的有效性。

Conclusion: LiGuard框架能够简化和加速激光雷达相关项目的开发，促进代码共享和复用。

Abstract: There is a growing interest in the development of lidar-based autonomous
mobility and Intelligent Transportation Systems (ITS). To operate and research
on lidar data, researchers often develop code specific to application niche.
This approach leads to duplication of efforts across studies that, in many
cases, share multiple methodological steps such as data input/output (I/O),
pre/post processing, and common algorithms in multi-stage solutions. Moreover,
slight changes in data, algorithms, and/or research focus may force major
revisions in the code. To address these challenges, we present LiGuard, an
open-source software framework that allows researchers to: 1) rapidly develop
code for their lidar-based projects by providing built-in support for data I/O,
pre/post processing, and commonly used algorithms, 2) interactively
add/remove/reorder custom algorithms and adjust their parameters, and 3)
visualize results for classification, detection, segmentation, and tracking
tasks. Moreover, because it creates all the code files in structured
directories, it allows easy sharing of entire projects or even the individual
components to be reused by other researchers. The effectiveness of LiGuard is
demonstrated via case studies.

</details>


### [6] [PercepTwin: Modeling High-Fidelity Digital Twins for Sim2Real LiDAR-based Perception for Intelligent Transportation Systems](https://arxiv.org/abs/2509.02903)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: LiDAR感知数据集的创建成本高昂，阻碍了其在智能交通系统中的可扩展性。本文提出了一种利用高保真数字孪生（HiFi DTs）创建大规模、高质量合成数据集的方法，以实现可扩展、低成本的Sim2Real学习。


<details>
  <summary>Details</summary>
Motivation: 为了克服LiDAR感知数据集创建成本高、耗时长、需要大量人工的缺点，并为智能交通系统（ITS）提供可扩展的解决方案。

Method: 提出了一种使用高保真数字孪生（HiFi DTs）创建大规模、高质量合成数据集的方法。该方法包括数字复制真实世界环境的步骤、工具和最佳实践，涵盖静态几何建模、道路基础设施复制和动态交通场景生成。利用开源数据（如卫星图像和OpenStreetMap数据）和特定传感器配置，为构建强大的合成环境提供了详细指导。

Result: 通过HiFi DTs创建的合成环境可以进行可扩展、经济高效且多样化的数据集生成，为鲁棒的Sim2Real学习奠定了可靠的基础。

Conclusion: 本文提出的HiFi DTs工作流程能够克服当前LiDAR感知数据集创建的挑战，为智能交通系统提供可扩展、低成本的数据集生成解决方案，从而促进Sim2Real学习的有效性。

Abstract: LiDAR-based perception in intelligent transportation systems (ITS), for tasks
such as object detection, tracking, and semantic and instance segmentation, is
predominantly solved by deep neural network models which often require
large-scale labeled datasets during training to achieve generalization.
However, creating these datasets is costly. time consuming and require human
labor before the datasets are ready for training models. This hinders
scalability of the LiDAR-based perception systems in ITS. Sim2Real learning
offers scalable alternative, however, its effectiveness is dependent on the
fidelity of the source simulation(s) to real-world, in terms of environment
structure, actor dynamics, and sensor emulations. In response, this paper
introduces a rigorous and reproducible methodology for creating large-scale,
high-quality synthetic datasets using High-Fidelity Digital Twins (HiFi DTs).
The proposed workflow outlines the steps, tools, and best practices for
digitally replicating real-world environments, encompassing static geometry
modeling, road infrastructure replication, and dynamic traffic scenario
generation. Leveraging open-source and readily available resources such as
satellite imagery and OpenStreetMap data, alongside specific sensor
configurations, this paper provides practical, detailed guidance for
constructing robust synthetic environments. These environments subsequently
facilitate scalable, cost-effective, and diverse dataset generation, forming a
reliable foundation for robust Sim2Real learning.

</details>


### [7] [VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results](https://arxiv.org/abs/2509.02969)
*Dasong Li,Sizhuo Ma,Hang Hua,Wenjie Li,Jian Wang,Chris Wei Zhou,Fengbin Guan,Xin Li,Zihao Yu,Yiting Lu,Ru-Ling Liao,Yan Ye,Zhibo Chen,Wei Sun,Linhan Cao,Yuqin Cao,Weixia Zhang,Wen Wen,Kaiwei Zhang,Zijian Chen,Fangfang Lu,Xiongkuo Min,Guangtao Zhai,Erjia Xiao,Lingfeng Zhang,Zhenjie Su,Hao Cheng,Yu Liu,Renjing Xu,Long Chen,Xiaoshuai Hao,Zhenpeng Zeng,Jianqin Wu,Xuxu Wang,Qian Yu,Bo Hu,Weiwei Wang,Pinxin Liu,Yunlong Tang,Luchuan Song,Jinxi He,Jiaru Wu,Hanjia Lyu*

Main category: cs.CV

TL;DR: 这是一个关于2025年ICCV视频理解挑战赛（VQualA 2025）的概述，该挑战赛专注于短视频的参与度预测。


<details>
  <summary>Details</summary>
Motivation: 本次挑战赛旨在促进对影响用户参与度的复杂因素的鲁棒建模策略的研究，以理解和模拟社交媒体平台上用户生成内容（UGC）短视频的流行度。

Method: 参赛者探索了多种多模态特征，包括视觉内容、音频和创作者提供的元数据。

Result: 挑战赛吸引了97名参赛者，收到了15份有效的测试提交，为短格式UGC视频参与度预测的进展做出了显著贡献。

Conclusion: VQualA 2025挑战赛通过提供一个新的、包含真实用户互动数据的短格式UGC数据集，成功地推动了短视频参与度预测领域的研究。

Abstract: This paper presents an overview of the VQualA 2025 Challenge on Engagement
Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge
focuses on understanding and modeling the popularity of user-generated content
(UGC) short videos on social media platforms. To support this goal, the
challenge uses a new short-form UGC dataset featuring engagement metrics
derived from real-world user interactions. This objective of the Challenge is
to promote robust modeling strategies that capture the complex factors
influencing user engagement. Participants explored a variety of multi-modal
features, including visual content, audio, and metadata provided by creators.
The challenge attracted 97 participants and received 15 valid test submissions,
contributing significantly to progress in short-form UGC video engagement
prediction.

</details>


### [8] [High-Fidelity Digital Twins for Bridging the Sim2Real Gap in LiDAR-Based ITS Perception](https://arxiv.org/abs/2509.02904)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: Sim2Real 域迁移在智能交通系统中很有前景，但模拟训练的模型在真实世界表现不佳。本文提出了一种高保真数字孪生 (HiFi DT) 框架，通过整合真实世界的背景几何、车道级道路拓扑和传感器规格，来弥合 Sim2Real 的差距。实验证明，在 HiFi DT 生成的合成数据上训练的模型比在真实数据上训练的模型效果好 4.8%。通过 Chamfer Distance (CD)、Maximum Mean Discrepancy (MMD)、Earth Mover's Distance (EMD) 和 Fréchet Distance (FD) 等指标量化了合成数据和真实数据之间的分布差异，结果表明 HiFi DT 显著减少了域转移，提高了泛化能力。


<details>
  <summary>Details</summary>
Motivation: Sim2Real 域迁移在智能交通系统中用于 LiDAR 感知很有前景，但模拟训练的模型在真实世界表现不佳，存在 Sim2Real 差距。

Method: 提出了一种高保真数字孪生 (HiFi DT) 框架，该框架整合了真实世界的背景几何、车道级道路拓扑和传感器规格，用于构建模拟环境以生成领域内合成数据。并使用多种指标（CD、MMD、EMD、FD）量化了合成数据和真实数据之间的分布差异。

Result: 在 HiFi DT 生成的合成数据上训练的 3D 物体检测器比在真实数据上训练的等效模型在真实数据上表现好 4.8%。HiFi DT 显著减少了域转移，并提高了跨各种评估场景的泛化能力。

Conclusion: 数字孪生在为实际智能交通系统应用实现可靠的、基于仿真的 LiDAR 感知方面发挥着重要作用。

Abstract: Sim2Real domain transfer offers a cost-effective and scalable approach for
developing LiDAR-based perception (e.g., object detection, tracking,
segmentation) in Intelligent Transportation Systems (ITS). However, perception
models trained in simulation often under perform on real-world data due to
distributional shifts. To address this Sim2Real gap, this paper proposes a
high-fidelity digital twin (HiFi DT) framework that incorporates real-world
background geometry, lane-level road topology, and sensor-specific
specifications and placement. We formalize the domain adaptation challenge
underlying Sim2Real learning and present a systematic method for constructing
simulation environments that yield in-domain synthetic data. An off-the-shelf
3D object detector is trained on HiFi DT-generated synthetic data and evaluated
on real data. Our experiments show that the DT-trained model outperforms the
equivalent model trained on real data by 4.8%. To understand this gain, we
quantify distributional alignment between synthetic and real data using
multiple metrics, including Chamfer Distance (CD), Maximum Mean Discrepancy
(MMD), Earth Mover's Distance (EMD), and Fr'echet Distance (FD), at both
raw-input and latent-feature levels. Results demonstrate that HiFi DTs
substantially reduce domain shift and improve generalization across diverse
evaluation scenarios. These findings underscore the significant role of digital
twins in enabling reliable, simulation-based LiDAR perception for real-world
ITS applications.

</details>


### [9] [Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach](https://arxiv.org/abs/2509.02918)
*Midhat Urooj,Ayan Banerjee,Farhat Shaikh,Kuntal Thakur,Sandeep Gupta*

Main category: cs.CV

TL;DR: KG-DG是一个结合了视觉转换器和专家指导的符号推理的神经符号框架，用于对糖尿病视网膜病变进行分类，以实现跨不同数据集的鲁棒泛化。


<details>
  <summary>Details</summary>
Motivation: 在医学影像领域，跨域泛化是一个关键的挑战，因为在单一来源上训练的模型在实际的分布变化下往往会失效。

Method: 该方法利用临床病变本体，通过结构化的、基于规则的特征和视网膜血管分割，并将它们与深度视觉表示融合，采用置信加权集成策略。该框架通过最小化域嵌入之间的KL散度来解决单域泛化（SDG）和多域泛化（MDG），从而强制对齐高级临床语义。

Result: 在四个公共数据集（APTOS、EyePACS、Messidor-1、Messidor-2）上的大量实验表明，在跨域设置下准确率提高了5.2%，并且比基线ViT模型提高了6%。仅符号模型在MDG中实现了63.67%的平均准确率，而完整的神经符号集成在具有挑战性的SDG场景中实现了比现有已发布基线和基准更高的准确率。消融研究表明，基于病变的特征（84.65%的准确率）在性能上显著优于纯粹的神经方法。

Conclusion: 研究结果表明，神经符号集成是构建临床上鲁棒且域不变的医学人工智能系统的有前景的范例。

Abstract: Domain generalization remains a critical challenge in medical imaging, where
models trained on single sources often fail under real-world distribution
shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy
(DR) classification that integrates vision transformers with expert-guided
symbolic reasoning to enable robust generalization across unseen domains. Our
approach leverages clinical lesion ontologies through structured, rule-based
features and retinal vessel segmentation, fusing them with deep visual
representations via a confidence-weighted integration strategy. The framework
addresses both single-domain generalization (SDG) and multi-domain
generalization (MDG) by minimizing the KL divergence between domain embeddings,
thereby enforcing alignment of high-level clinical semantics. Extensive
experiments across four public datasets (APTOS, EyePACS, Messidor-1,
Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in
cross-domain settings and a 6% improvement over baseline ViT models. Notably,
our symbolic-only model achieves a 63.67% average accuracy in MDG, while the
complete neuro-symbolic integration achieves the highest accuracy compared to
existing published baselines and benchmarks in challenging SDG scenarios.
Ablation studies reveal that lesion-based features (84.65% accuracy)
substantially outperform purely neural approaches, confirming that symbolic
components act as effective regularizers beyond merely enhancing
interpretability. Our findings establish neuro-symbolic integration as a
promising paradigm for building clinically robust, and domain-invariant medical
AI systems.

</details>


### [10] [A Data-Driven RetinaNet Model for Small Object Detection in Aerial Images](https://arxiv.org/abs/2509.02928)
*Zhicheng Tang,Jinwen Tang,Yi Shang*

Main category: cs.CV

TL;DR: DDR-Net是一个数据驱动的深度学习模型，用于提高航空影像中小物体的检测能力，通过自动优化特征图和锚点估计，并引入新的采样技术来克服数据限制，相比RetinaNet和其他模型有显著提升。


<details>
  <summary>Details</summary>
Motivation: 在航空影像中，检测小物体对于环境监测、城市规划和危机管理等应用至关重要。

Method: DDR-Net 是一个数据驱动的深度学习模型，基于 RetinaNet，通过自主确定最优特征图和锚点估计，并采用创新的采样技术来增强模型在数据量有限时的性能。

Result: DDR-Net 在各种航空鸟类图像数据集上的经验评估表明，其性能显著优于 RetinaNet 和其他现有模型，能够有效检测车辆和行人等小物体。

Conclusion: DDR-Net 通过其数据驱动的方法和创新的采样技术，提高了小物体检测的效率和准确性，降低了数据收集和训练成本，为农业、安全和考古学等多个领域带来了广泛的应用前景。

Abstract: In the realm of aerial imaging, the ability to detect small objects is
pivotal for a myriad of applications, encompassing environmental surveillance,
urban design, and crisis management. Leveraging RetinaNet, this work unveils
DDR-Net: a data-driven, deep-learning model devised to enhance the detection of
diminutive objects. DDR-Net introduces novel, data-driven techniques to
autonomously ascertain optimal feature maps and anchor estimations, cultivating
a tailored and proficient training process while maintaining precision.
Additionally, this paper presents an innovative sampling technique to bolster
model efficacy under limited data training constraints. The model's enhanced
detection capabilities support critical applications including wildlife and
habitat monitoring, traffic flow optimization, and public safety improvements
through accurate identification of small objects like vehicles and pedestrians.
DDR-Net significantly reduces the cost and time required for data collection
and training, offering efficient performance even with limited data. Empirical
assessments over assorted aerial avian imagery datasets demonstrate that
DDR-Net markedly surpasses RetinaNet and alternative contemporary models. These
innovations advance current aerial image analysis technologies and promise
wide-ranging impacts across multiple sectors including agriculture, security,
and archaeology.

</details>


### [11] [STAR: A Fast and Robust Rigid Registration Framework for Serial Histopathological Images](https://arxiv.org/abs/2509.02952)
*Zeyu Liu,Shengwei Ding*

Main category: cs.CV

TL;DR: STAR是一个快速、鲁棒的刚性配准框架，用于对连续切片的全切片图像进行配准，解决了现有方法计算密集和难以复现的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法计算密集且难以复现，轻量级的刚性配准框架在连续切片场景中仍需发展。

Method: STAR集成了染料条件预处理、分层粗略到精细的相关策略、自适应核缩放和内置质量控制，实现了跨越异种组织类型和染料协议（如H&E，PAS，Masson's，CD31，KI67）的可靠刚性配准。

Result: 在ANHIR 2019和ACROBAT 2022数据集上，STAR在几分钟内稳定地完成了每张幻灯片的配准，证明了其对交叉染料变异性和部分组织重叠的鲁棒性。此外，还展示了H&E-IHC配准、多IHC面板构建和典型故障模式的案例研究。

Conclusion: STAR是一个开源、轻量级的工具，为临床应用提供了可复现的基准，降低了临床应用的门槛，并为下一代计算病理学的大规模配准数据准备提供了支持。

Abstract: Registration of serial whole-slide histopathological images (WSIs) is
critical for enabling direct comparison across diverse stains and for preparing
paired datasets in artificial intelligence (AI) workflows such as virtual
staining and biomarker prediction. While existing methods often rely on complex
deformable or deep learning approaches that are computationally intensive and
difficult to reproduce, lightweight rigid frameworks-sufficient for many
consecutive-section scenarios-remain underdeveloped. We introduce STAR (Serial
Tissue Alignment for Rigid registration), a fast and robust open-source
framework for multi-WSI alignment. STAR integrates stain-conditioned
preprocessing with a hierarchical coarse-to-fine correlation strategy, adaptive
kernel scaling, and built-in quality control, achieving reliable rigid
registration across heterogeneous tissue types and staining protocols,
including hematoxylin-eosin (H&E), special histochemical stains (e.g., PAS,
PASM, Masson's), and immunohistochemical (IHC) markers (e.g., CD31, KI67).
Evaluated on the ANHIR 2019 and ACROBAT 2022 datasets spanning multiple organs
and scanning conditions, STAR consistently produced stable alignments within
minutes per slide, demonstrating robustness to cross-stain variability and
partial tissue overlap. Beyond benchmarks, we present case studies on H&E-IHC
alignment, construction of multi-IHC panels, and typical failure modes,
underscoring both utility and limitations. Released as an open and lightweight
tool, STAR provides a reproducible baseline that lowers the barrier for
clinical adoption and enables large-scale paired data preparation for
next-generation computational pathology.

</details>


### [12] [Resilient Multimodal Industrial Surface Defect Detection with Uncertain Sensors Availability](https://arxiv.org/abs/2509.02962)
*Shuai Jiang,Yunfeng Ma,Jingyu Zhou,Yuan Bian,Yaonan Wang,Min Liu*

Main category: cs.CV

TL;DR: 该研究提出了一种解决多模态工业表面缺陷检测（MISDD）中由于传感器可用性不确定导致的模态缺失问题的方法，通过跨模态提示学习和对称对比学习来融合RGB和3D数据，并在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态工业表面缺陷检测（MISDD）中因传感器可用性不确定导致的模态缺失问题，该问题带来了学习模式转换和信息空缺的挑战。

Method: 提出跨模态提示学习，包括跨模态一致性提示、模态特定提示和缺失感知提示；并提出对称对比学习，利用文本提示作为桥梁融合双视觉模态，通过三模态对比预训练完成多模态学习。

Result: 在总缺失率为0.7的情况下，RGB和3D模态的I-AUROC达到73.83%，P-AUROC达到93.05%，分别超出当前最优方法3.84%和5.58%，并在不同缺失类型和比率下均优于现有方法。

Conclusion: 所提出的跨模态提示学习和对称对比学习方法能有效解决MISDD中的模态缺失问题，并显著提升检测性能。

Abstract: Multimodal industrial surface defect detection (MISDD) aims to identify and
locate defect in industrial products by fusing RGB and 3D modalities. This
article focuses on modality-missing problems caused by uncertain sensors
availability in MISDD. In this context, the fusion of multiple modalities
encounters several troubles, including learning mode transformation and
information vacancy. To this end, we first propose cross-modal prompt learning,
which includes: i) the cross-modal consistency prompt serves the establishment
of information consistency of dual visual modalities; ii) the modality-specific
prompt is inserted to adapt different input patterns; iii) the missing-aware
prompt is attached to compensate for the information vacancy caused by dynamic
modalities-missing. In addition, we propose symmetric contrastive learning,
which utilizes text modality as a bridge for fusion of dual vision modalities.
Specifically, a paired antithetical text prompt is designed to generate binary
text semantics, and triple-modal contrastive pre-training is offered to
accomplish multimodal learning. Experiment results show that our proposed
method achieves 73.83% I-AUROC and 93.05% P-AUROC with a total missing rate 0.7
for RGB and 3D modalities (exceeding state-of-the-art methods 3.84% and 5.58%
respectively), and outperforms existing approaches to varying degrees under
different missing types and rates. The source code will be available at
https://github.com/SvyJ/MISDD-MM.

</details>


### [13] [EdgeAttNet: Towards Barb-Aware Filament Segmentation](https://arxiv.org/abs/2509.02964)
*Victor Solomon,Piet Martens,Jingyu Liu,Rafal Angryk*

Main category: cs.CV

TL;DR: EdgeAttNet通过引入可学习的边缘图来增强U-Net模型，以更准确地分割太阳活动区中的日珥及其细微结构（如“刺”），从而提高日珥手性的识别能力，这对于理解日冕物质抛射（CME）的行为至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有的太阳活动区日珥分割方法在捕捉日珥的精细结构（尤其是“刺”）方面存在不足，难以有效处理长距离依赖和空间细节，这限制了日珥手性的准确确定，而手性是影响日冕物质抛射（CME）行为的关键因素。

Method: 提出了一种名为EdgeAttNet的分割模型。该模型基于U-Net架构，并引入了一个新颖的可学习边缘图，该边缘图直接从输入图像生成。通过将边缘信息线性变换到注意力机制的Key和Query矩阵中，引导网络瓶颈处的自注意力机制更有效地捕捉日珥的边界和“刺”的结构。这种将结构化先验信息显式集成到注意力计算中的方法，提高了模型的空间敏感性和分割精度，同时减少了可训练参数的数量。

Result: 与标准的U-Net以及其他基于U-Net的Transformer基线模型相比，EdgeAttNet在MAGFILO数据集上实现了更高的分割精度，尤其在识别日珥的“刺”方面表现显著。此外，该模型还实现了更快的推理速度，适合实际应用部署。

Conclusion: EdgeAttNet通过有效融合图像边缘信息来增强U-Net的自注意力机制，能够更准确、更高效地分割太阳日珥及其精细结构，提高了对日珥手性的识别能力，为日冕物质抛射（CME）的研究提供了有力的支持。

Abstract: Accurate segmentation of solar filaments in H-alpha observations is critical
for determining filament chirality, a key factor in the behavior of Coronal
Mass Ejections (CMEs). However, existing methods often fail to capture
fine-scale filament structures, particularly barbs, due to a limited ability to
model long-range dependencies and spatial detail.
  We propose EdgeAttNet, a segmentation architecture built on a U-Net backbone
by introducing a novel, learnable edge map derived directly from the input
image. This edge map is incorporated into the model by linearly transforming
the attention Key and Query matrices with the edge information, thereby guiding
the self-attention mechanism at the network's bottleneck to more effectively
capture filament boundaries and barbs. By explicitly integrating this
structural prior into the attention computations, EdgeAttNet enhances spatial
sensitivity and segmentation accuracy while reducing the number of trainable
parameters.
  Trained end-to-end, EdgeAttNet outperforms U-Net and other U-Net-based
transformer baselines on the MAGFILO dataset. It achieves higher segmentation
accuracy and significantly better recognition of filament barbs, with faster
inference performance suitable for practical deployment.

</details>


### [14] [KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive Driving Frames with Vision-Language Models](https://arxiv.org/abs/2509.02966)
*Yujin Wang,Tianyi Wang,Quanfeng Liu,Wenxian Fan,Junfeng Jiao,Christian Claudel,Yunbing Yan,Bingzhao Gao,Jianqiang Wang,Hong Chen*

Main category: cs.CV

TL;DR: KEPT是一个知识增强的视觉语言模型（VLM），用于从连续的驾驶视频帧中预测自我轨迹，通过结合时域频率-空间融合（TFSF）视频编码器和可扩展的k-means + HNSW检索堆栈来提供场景对齐的示例，并利用链式思考（CoT）提示和三重微调来提高准确性和安全性，在nuScenes数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLM）在为自动驾驶进行轨迹预测时，往往无法有效地将场景动态和领域知识结合起来，影响了其安全性和可靠性。

Method: KEPT框架结合了经过自监督学习和难例挖掘训练的时域频率-空间融合（TFSF）视频编码器，以及一个可扩展的k-means + HNSW检索堆栈来提供场景对齐的示例。检索到的先验信息被嵌入到具有明确规划约束的链式思考（CoT）提示中，并通过三重微调来逐步调整语言头，使其能够处理空间线索、物理可行运动和时间条件的前视规划。

Result: 在nuScenes数据集上，KEPT在开放式协议下取得了最先进的性能：在NoAvg协议下，平均L2误差为0.70米，碰撞率为0.21%；在TemAvg协议下（使用轻量级自我状态），平均L2误差为0.31米，碰撞率为0.07%。消融研究表明，所有三个微调阶段都提供了互补的优势，使用Top-2检索示例可以获得最佳的准确性-安全性权衡。k-means聚类的HNSW索引实现了亚毫秒级的检索延迟。

Conclusion: 检索增强的、CoT指导的VLM为实现可解释和值得信赖的自动驾驶提供了一条有前途且数据高效的途径。

Abstract: Accurate short-horizon trajectory prediction is pivotal for safe and reliable
autonomous driving, yet existing vision-language models (VLMs) often fail to
effectively ground their reasoning in scene dynamics and domain knowledge. To
address this challenge, this paper introduces KEPT, a knowledge-enhanced VLM
framework that predicts ego trajectories directly from consecutive front-view
driving frames. KEPT couples a temporal frequency-spatial fusion (TFSF) video
encoder, trained via self-supervised learning with hard-negative mining, with a
scalable k-means + HNSW retrieval stack that supplies scene-aligned exemplars.
Retrieved priors are embedded into chain-of-thought (CoT) prompts with explicit
planning constraints, while a triple-stage fine-tuning schedule incrementally
aligns the language head to metric spatial cues, physically feasible motion,
and temporally conditioned front-view planning. Evaluated on nuScenes dataset,
KEPT achieves state-of-the-art performance across open-loop protocols: under
NoAvg, it achieves 0.70m average L2 with a 0.21\% collision rate; under TemAvg
with lightweight ego status, it attains 0.31m average L2 and a 0.07\% collision
rate. Ablation studies show that all three fine-tuning stages contribute
complementary benefits, and that using Top-2 retrieved exemplars yields the
best accuracy-safety trade-off. The k-means-clustered HNSW index delivers
sub-millisecond retrieval latency, supporting practical deployment. These
results indicate that retrieval-augmented, CoT-guided VLMs offer a promising,
data-efficient pathway toward interpretable and trustworthy autonomous driving.

</details>


### [15] [InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System](https://arxiv.org/abs/2509.02973)
*Xianbao Hou,Yonghao He,Zeyd Boukhers,John See,Hu Su,Wei Sui,Cong Yang*

Main category: cs.CV

TL;DR: InstaDA是一个新的、无需训练的双代理系统，通过LLM和扩散模型的协作来增强实例分割数据集，提高了数据多样性和整体数据分布，实验显示性能有显著提升。


<details>
  <summary>Details</summary>
Motivation: 获取高质量的实例分割数据具有挑战性，因为标注过程劳动密集且存在显著的类别不平衡。现有方法虽然利用了复制粘贴和扩散模型，但往往缺乏大型语言模型（LLM）与扩散模型的深度协作，并且未能充分利用现有训练数据中的丰富信息。

Method: InstaDA包含两个代理：1. 文本代理（T-Agent），通过LLM和扩散模型的协作以及新颖的提示重构机制来增强数据多样性。2. 图像代理（I-Agent），通过生成以训练图像为条件的新的实例来丰富整体数据分布。两个代理可独立运行。

Result: 在LVIS 1.0验证集上的实验表明，InstaDA相比基线模型，在框平均精度（AP）上提高了+4.0，在掩码AP上提高了+3.3。与现有领先模型DiverGen相比，InstaDA在框AP上提高了+0.3，掩码AP上提高了+0.1，在常见类别上的框AP增幅为+0.7，在常见类别上的掩码AP增幅为+0.2，在频繁类别上的掩码AP增幅为+0.5。

Conclusion: InstaDA通过其创新的双代理系统，在数据增强和实例分割任务上取得了显著的性能提升，有效解决了现有方法在数据多样性和利用训练数据信息方面的不足。

Abstract: Acquiring high-quality instance segmentation data is challenging due to the
labor-intensive nature of the annotation process and significant class
imbalances within datasets. Recent studies have utilized the integration of
Copy-Paste and diffusion models to create more diverse datasets. However, these
studies often lack deep collaboration between large language models (LLMs) and
diffusion models, and underutilize the rich information within the existing
training data. To address these limitations, we propose InstaDA, a novel,
training-free Dual-Agent system designed to augment instance segmentation
datasets. First, we introduce a Text-Agent (T-Agent) that enhances data
diversity through collaboration between LLMs and diffusion models. This agent
features a novel Prompt Rethink mechanism, which iteratively refines prompts
based on the generated images. This process not only fosters collaboration but
also increases image utilization and optimizes the prompts themselves.
Additionally, we present an Image-Agent (I-Agent) aimed at enriching the
overall data distribution. This agent augments the training set by generating
new instances conditioned on the training images. To ensure practicality and
efficiency, both agents operate as independent and automated workflows,
enhancing usability. Experiments conducted on the LVIS 1.0 validation set
indicate that InstaDA achieves significant improvements, with an increase of
+4.0 in box average precision (AP) and +3.3 in mask AP compared to the
baseline. Furthermore, it outperforms the leading model, DiverGen, by +0.3 in
box AP and +0.1 in mask AP, with a notable +0.7 gain in box AP on common
categories and mask AP gains of +0.2 on common categories and +0.5 on frequent
categories.

</details>


### [16] [SPENet: Self-guided Prototype Enhancement Network for Few-shot Medical Image Segmentation](https://arxiv.org/abs/2509.02993)
*Chao Fan,Xibin Jia,Anqi Xiao,Hongyuan Yu,Zhenghan Yang,Dawei Yang,Hui Xu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: SPENet通过引入多级原型生成和查询引导式局部原型增强模块，解决了现有少样本医学图像分割方法中原型匹配忽略类内变化的问题，并在三个公共数据集上取得了优于最先进方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于原型的少样本医学图像分割方法通常只生成单一的全局原型，忽略了类内变化，导致匹配效果不佳。

Method: 提出了一种名为SPENet（Self-guided Prototype Enhancement Network）的网络，其中包含两个关键模块：1. 多级原型生成（MPG）模块：同时生成全局原型和自适应数量的局部原型，实现不同粒度的匹配。 2. 查询引导式局部原型增强（QLPE）模块：通过查询图像引导，自适应地优化支持集原型，减轻支持集和查询集之间显著差异带来的负面影响。

Result: SPENet在三个公共医学图像分割数据集上进行了广泛的实验，结果表明其性能优于现有的最先进方法。

Conclusion: SPENet通过其创新的原型生成和增强机制，有效地解决了少样本医学图像分割中的类内变化和数据差异问题，取得了优越的分割性能。

Abstract: Few-Shot Medical Image Segmentation (FSMIS) aims to segment novel classes of
medical objects using only a few labeled images. Prototype-based methods have
made significant progress in addressing FSMIS. However, they typically generate
a single global prototype for the support image to match with the query image,
overlooking intra-class variations. To address this issue, we propose a
Self-guided Prototype Enhancement Network (SPENet). Specifically, we introduce
a Multi-level Prototype Generation (MPG) module, which enables
multi-granularity measurement between the support and query images by
simultaneously generating a global prototype and an adaptive number of local
prototypes. Additionally, we observe that not all local prototypes in the
support image are beneficial for matching, especially when there are
substantial discrepancies between the support and query images. To alleviate
this issue, we propose a Query-guided Local Prototype Enhancement (QLPE)
module, which adaptively refines support prototypes by incorporating guidance
from the query image, thus mitigating the negative effects of such
discrepancies. Extensive experiments on three public medical datasets
demonstrate that SPENet outperforms existing state-of-the-art methods,
achieving superior performance.

</details>


### [17] [SOPSeg: Prompt-based Small Object Instance Segmentation in Remote Sensing Imagery](https://arxiv.org/abs/2509.03002)
*Chenhao Wang,Yingrui Ji,Yu Meng,Yunjian Zhang,Yao Zhu*

Main category: cs.CV

TL;DR: SOPSeg 是一个用于遥感图像小目标分割的提示式框架，通过区域自适应放大、边缘预测和渐进式细化来解决 SAM 在小目标分割上的不足，并引入了面向定向边界框的新型提示机制。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像小目标提取研究主要集中在检测，而实例分割方面的数据集和研究不足，SAM模型在小目标分割上性能下降，主要是由于特征分辨率粗糙导致空间细节丢失。

Method: 提出 SOPSeg 框架，包含区域自适应放大策略以保留细节，定制的解码器整合边缘预测和渐进式细化以获得准确边界，并引入面向定向边界框的新型提示机制。

Result: SOPSeg 在小目标分割方面优于现有方法，并有助于高效构建遥感任务数据集。此外，基于 SODA-A 构建了一个全面的小目标实例分割数据集。

Conclusion: SOPSeg 框架能够有效解决遥感小目标分割的挑战，其提出的模型和数据集将为相关研究提供支持。

Abstract: Extracting small objects from remote sensing imagery plays a vital role in
various applications, including urban planning, environmental monitoring, and
disaster management. While current research primarily focuses on small object
detection, instance segmentation for small objects remains underexplored, with
no dedicated datasets available. This gap stems from the technical challenges
and high costs of pixel-level annotation for small objects. While the Segment
Anything Model (SAM) demonstrates impressive zero-shot generalization, its
performance on small-object segmentation deteriorates significantly, largely
due to the coarse 1/16 feature resolution that causes severe loss of fine
spatial details. To this end, we propose SOPSeg, a prompt-based framework
specifically designed for small object segmentation in remote sensing imagery.
It incorporates a region-adaptive magnification strategy to preserve
fine-grained details, and employs a customized decoder that integrates edge
prediction and progressive refinement for accurate boundary delineation.
Moreover, we introduce a novel prompting mechanism tailored to the oriented
bounding boxes widely adopted in remote sensing applications. SOPSeg
outperforms existing methods in small object segmentation and facilitates
efficient dataset construction for remote sensing tasks. We further construct a
comprehensive small object instance segmentation dataset based on SODA-A, and
will release both the model and dataset to support future research.

</details>


### [18] [Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers](https://arxiv.org/abs/2509.03006)
*Tzuhsuan Huang,Cheng Yu Yeo,Tsai-Ling Huang,Hong-Han Shuai,Wen-Huang Cheng,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 本研究提出了一种后处理的水印嵌入方法，通过引入集成攻击网络来增强水印的鲁棒性，并在WAVES基准上进行了广泛评估，显著提高了现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 后处理水印方法相比于生成中水印方法具有更高的灵活性，可以应用于任何生成模型，并且允许用户嵌入独特的、个体化的水印。因此，本研究关注后处理水印，并致力于增强其鲁棒性。

Method: 通过在训练过程中引入集成攻击网络来增强后处理水印的鲁棒性。研究人员构建了不同版本的攻击网络，结合了卷积神经网络（CNN）和Transformer，并在空间域和频率域进行了实验，以研究各种组合对水印模型鲁棒性的影响。

Result: 实验结果表明，将空间域的CNN攻击网络与频率域的Transformer攻击网络相结合，能够最大程度地提高水印模型的鲁棒性。在WAVES基准上的广泛评估显示，所提出的集成攻击网络能够显著增强基线方法的鲁棒性，特别是在面对再生攻击时，相比于StegaStamp方法，性能提升了18.743%。

Conclusion: 本研究提出的集成攻击网络能够有效增强后处理水印方法的鲁棒性。将空间域的CNN攻击网络与频率域的Transformer攻击网络相结合，为提高水印鲁棒性提供了一种有效策略。

Abstract: Recent studies on deep watermarking have predominantly focused on
in-processing watermarking, which integrates the watermarking process into
image generation. However, post-processing watermarking, which embeds
watermarks after image generation, offers more flexibility. It can be applied
to outputs from any generative model (e.g. GANs, diffusion models) without
needing access to the model's internal structure. It also allows users to embed
unique watermarks into individual images. Therefore, this study focuses on
post-processing watermarking and enhances its robustness by incorporating an
ensemble attack network during training. We construct various versions of
attack networks using CNN and Transformer in both spatial and frequency domains
to investigate how each combination influences the robustness of the
watermarking model. Our results demonstrate that combining a CNN-based attack
network in the spatial domain with a Transformer-based attack network in the
frequency domain yields the highest robustness in watermarking models.
Extensive evaluation on the WAVES benchmark, using average bit accuracy as the
metric, demonstrates that our ensemble attack network significantly enhances
the robustness of baseline watermarking methods under various stress tests. In
particular, for the Regeneration Attack defined in WAVES, our method improves
StegaStamp by 18.743%. The code is released
at:https://github.com/aiiu-lab/DeepRobustWatermark.

</details>


### [19] [Lesion-Aware Visual-Language Fusion for Automated Image Captioning of Ulcerative Colitis Endoscopic Examinations](https://arxiv.org/abs/2509.03011)
*Alexis Ivan Lopez Escamilla,Gilberto Ochoa,Sharib Al*

Main category: cs.CV

TL;DR: 我们提出了一个针对溃疡性结肠炎（UC）的病变感知图像字幕生成框架，该框架集成了ResNet嵌入、Grad-CAM热力图和CBAM增强的注意力机制，并使用T5解码器。通过注入临床元数据（如MES评分、血管模式、出血、红斑、易碎性、溃疡等）作为自然语言提示来指导字幕生成。该系统能够生成与临床实践一致的结构化、可解释的描述，并进行MES分类和病变标签。与基线方法相比，我们提出的方法在字幕质量和MES分类准确性方面有所提高，有助于可靠的内镜报告。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够生成结构化、可解释的医学图像字幕的系统，特别是针对溃疡性结肠炎（UC）的内镜图像，并能够辅助临床诊断，如MES评分分类。

Method: 结合使用ResNet嵌入、Grad-CAM热力图和CBAM增强的注意力机制，并利用T5解码器生成字幕。将临床元数据（MES评分、血管模式、出血、红斑、易碎性、溃疡）作为自然语言提示输入模型，以指导字幕生成。

Result: 该系统能够生成与临床实践对齐的、结构化的、可解释的描述，并能够进行MES分类和识别病变标签。与基线方法相比，在字幕质量和MES分类准确性方面均有所提升。

Conclusion: 我们提出的基于病变感知的图像字幕生成框架能够有效提高UC内镜图像的字幕质量和MES分类准确性，为生成可靠的内镜报告提供了支持。

Abstract: We present a lesion-aware image captioning framework for ulcerative colitis
(UC). The model integrates ResNet embeddings, Grad-CAM heatmaps, and
CBAM-enhanced attention with a T5 decoder. Clinical metadata (MES score 0-3,
vascular pattern, bleeding, erythema, friability, ulceration) is injected as
natural-language prompts to guide caption generation. The system produces
structured, interpretable descriptions aligned with clinical practice and
provides MES classification and lesion tags. Compared with baselines, our
approach improves caption quality and MES classification accuracy, supporting
reliable endoscopic reporting.

</details>


### [20] [Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens](https://arxiv.org/abs/2509.03025)
*Sohee Kim,Soohyun Ryu,Joonhyung Park,Eunho Yang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Vision-Language Models (LVLMs) generate contextually relevant responses
by jointly interpreting visual and textual inputs. However, our finding reveals
they often mistakenly perceive text inputs lacking visual evidence as being
part of the image, leading to erroneous responses. In light of this finding, we
probe whether LVLMs possess an internal capability to determine if textual
concepts are grounded in the image, and discover a specific subset of
Feed-Forward Network (FFN) neurons, termed Visual Absence-aware (VA) neurons,
that consistently signal the visual absence through a distinctive activation
pattern. Leveraging these patterns, we develop a detection module that
systematically classifies whether an input token is visually grounded. Guided
by its prediction, we propose a method to refine the outputs by reinterpreting
question prompts or replacing the detected absent tokens during generation.
Extensive experiments show that our method effectively mitigates the models'
tendency to falsely presume the visual presence of text input and its
generality across various LVLMs.

</details>


### [21] [Background Matters Too: A Language-Enhanced Adversarial Framework for Person Re-Identification](https://arxiv.org/abs/2509.03032)
*Kaicong Huang,Talha Azfar,Jack M. Reilly,Thomas Guggisberg,Ruimin Ke*

Main category: cs.CV

TL;DR: 该研究提出了一种端到端的行人重识别（ReID）框架，该框架通过联合建模前景和背景信息来应对遮挡和细粒度特征提取的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前视觉方法依赖昂贵的人工标注，并且在处理复杂遮挡时存在困难。现有跨模态方法虽然引入了语义线索，但仅关注前景信息，忽略了背景线索的潜在价值。受人类感知启发，研究者认为背景语义与前景语义对于ReID同样重要，因为人类会排除背景干扰，专注于目标外观。

Method: 提出了一种端到端的框架，在一个双分支跨模态特征提取管道中联合建模前景和背景信息。设计了一种内部语义对齐和交叉语义对抗学习策略，以区分前景和背景。该策略对齐共享相同语义的跨域视觉和文本特征，同时惩罚前景和背景特征之间的相似性，以增强网络的判别能力。

Result: 在两个整体ReID基准和两个遮挡ReID基准上进行了全面的实验。

Conclusion: 提出的方法在ReID任务中被证明是有效的和通用的，其结果与当前最先进的方法相当或更优。

Abstract: Person re-identification faces two core challenges: precisely locating the
foreground target while suppressing background noise and extracting
fine-grained features from the target region. Numerous visual-only approaches
address these issues by partitioning an image and applying attention modules,
yet they rely on costly manual annotations and struggle with complex
occlusions. Recent multimodal methods, motivated by CLIP, introduce semantic
cues to guide visual understanding. However, they focus solely on foreground
information, but overlook the potential value of background cues. Inspired by
human perception, we argue that background semantics are as important as the
foreground semantics in ReID, as humans tend to eliminate background
distractions while focusing on target appearance. Therefore, this paper
proposes an end-to-end framework that jointly models foreground and background
information within a dual-branch cross-modal feature extraction pipeline. To
help the network distinguish between the two domains, we propose an
intra-semantic alignment and inter-semantic adversarial learning strategy.
Specifically, we align visual and textual features that share the same
semantics across domains, while simultaneously penalizing similarity between
foreground and background features to enhance the network's discriminative
power. This strategy drives the model to actively suppress noisy background
regions and enhance attention toward identity-relevant foreground cues.
Comprehensive experiments on two holistic and two occluded ReID benchmarks
demonstrate the effectiveness and generality of the proposed method, with
results that match or surpass those of current state-of-the-art approaches.

</details>


### [22] [MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model](https://arxiv.org/abs/2509.03041)
*Pengyang Yu,Haoquan Wang,Gerard Marks,Tahar Kechadi,Laurence T. Yang,Sahraoui Dhelim,Nyothiri Aung*

Main category: cs.CV

TL;DR: MedLiteNet是一种轻量级CNN-Transformer混合模型，用于皮肤镜图像分割，通过分层特征提取和多尺度上下文聚合实现高精度。


<details>
  <summary>Details</summary>
Motivation: 准确的皮肤病变分割是皮肤癌计算机辅助诊断的关键技术挑战。卷积神经网络（CNN）受限于有限的感受野，难以模拟长距离依赖。Vision Transformer（ViT）虽然能捕捉全局上下文，但其二次复杂度和巨大的参数量限制了其在皮肤科常见的小样本医学数据集上的应用。

Method: MedLiteNet采用深度可分离的Mobile Inverted Bottleneck块来减少计算量，并在瓶颈层插入跨尺度Token-Mixing单元来交换不同分辨率之间的信息，同时嵌入了边界感知自注意力模块来锐化病变轮廓。

Result: MedLiteNet在皮肤镜分割任务中实现了高精度。

Conclusion: MedLiteNet通过结合CNN和Transformer的优点，并进行轻量化设计，解决了皮肤镜分割中的挑战，实现了高精度。

Abstract: Accurate skin-lesion segmentation remains a key technical challenge for
computer-aided diagnosis of skin cancer. Convolutional neural networks, while
effective, are constrained by limited receptive fields and thus struggle to
model long-range dependencies. Vision Transformers capture global context, yet
their quadratic complexity and large parameter budgets hinder use on the
small-sample medical datasets common in dermatology. We introduce the
MedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic
segmentation that achieves high precision through hierarchical feature
extraction and multi-scale context aggregation. The encoder stacks depth-wise
Mobile Inverted Bottleneck blocks to curb computation, inserts a
bottleneck-level cross-scale token-mixing unit to exchange information between
resolutions, and embeds a boundary-aware self-attention module to sharpen
lesion contours.

</details>


### [23] [DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed Multi-Tasks](https://arxiv.org/abs/2509.03044)
*Chengjie Huang,Jiafeng Yan,Jing Li,Lu Bai*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Conditional diffusion models have made impressive progress in the field of
image processing, but the characteristics of constructing data distribution
pathways make it difficult to exploit the intrinsic correlation between tasks
in multi-task scenarios, which is even worse in ill-posed tasks with a lack of
training data. In addition, traditional static condition control makes it
difficult for networks to learn in multi-task scenarios with its dynamically
evolving characteristics. To address these challenges, we propose a dynamic
conditional double diffusion bridge training paradigm to build a general
framework for ill-posed multi-tasks. Firstly, this paradigm decouples the
diffusion and condition generation processes, avoiding the dependence of the
diffusion model on supervised data in ill-posed tasks. Secondly, generated by
the same noise schedule, dynamic conditions are used to gradually adjust their
statistical characteristics, naturally embed time-related information, and
reduce the difficulty of network learning. We analyze the learning objectives
of the network under different conditional forms in the single-step denoising
process and compare the changes in its attention weights in the network,
demonstrating the superiority of our dynamic conditions. Taking dehazing and
visible-infrared fusion as typical ill-posed multi-task scenarios, we achieve
the best performance in multiple indicators on public datasets. The code has
been publicly released at: https://anonymous.4open.science/r/DCDB-D3C2.

</details>


### [24] [Isolated Bangla Handwritten Character Classification using Transfer Learning](https://arxiv.org/abs/2509.03061)
*Abdul Karim,S M Rafiuddin,Jahidul Islam Razin,Tahira Alam*

Main category: cs.CV

TL;DR: 使用迁移学习和深度学习模型（3DCNN、ResNet、MobileNet）对孟加拉手写字符进行分类，准确率高达99.82%（训练集）和99.46%（测试集），优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 由于孟加拉语包含大量基础和复合字符，对孟加拉手写字符进行准确识别具有重要意义，同时需要解决梯度消失问题。

Method: 应用迁移学习和深度学习技术，包括3D卷积神经网络（3DCNN）、残差神经网络（ResNet）和MobileNet，对包含166,105个样本、84个类别的“Bangla Lekha Isolated”数据集进行端到端分类。

Result: 模型在训练数据上达到99.82%的准确率，在测试数据上达到99.46%的准确率。

Conclusion: 所提出的模型在孟加拉手写字符分类任务上取得了优于现有基准的准确率，有效解决了梯度消失问题。

Abstract: Bangla language consists of fifty distinct characters and many compound
characters. Several notable studies have been performed to recognize Bangla
characters, both handwritten and optical. Our approach uses transfer learning
to classify the basic, distinct, as well as compound Bangla handwritten
characters while avoiding the vanishing gradient problem. Deep Neural Network
techniques such as 3D Convolutional Neural Network (3DCNN), Residual Neural
Network (ResNet), and MobileNet are applied to generate an end-to-end
classification of all possible standard formations of handwritten characters in
the Bangla language. The Bangla Lekha Isolated dataset, which contains 166,105
Bangla character image samples categorized into 84 distinct classes, is used
for this classification model. The model achieved 99.82% accuracy on training
data and 99.46% accuracy on test data. Comparisons with various
state-of-the-art benchmarks of Bangla handwritten character classification show
that the proposed model achieves better accuracy in classifying the data.

</details>


### [25] [High Cursive Complex Character Recognition using GAN External Classifier](https://arxiv.org/abs/2509.03062)
*S M Rafiuddin*

Main category: cs.CV

TL;DR: 提出了一种基于生成对抗网络（GAN）的外部分类器，称为ADA-GAN，用于识别复杂和草书手写字符，并证明了其在字符复杂度增加时比卷积神经网络（CNN）更具鲁棒性和有效性。


<details>
  <summary>Details</summary>
Motivation: 手写字符，特别是草书和复杂的字符，比简单、非草书的字符更难分类。

Method: 提出了一种生成对抗网络（GAN）的外部分类器。生成器网络生成伪造的手写字符图像，这些图像在添加了对抗性扰动噪声并通过判别器网络的置信度得分达到阈值后，用于扩充训练数据。

Result: 与卷积神经网络（CNN）相比，所提出的ADA-GAN模型对于草书和复杂字符更加鲁棒和有效，即使在字符复杂度增加的情况下也是如此。

Conclusion: ADA-GAN模型在处理复杂和草书手写字符方面比传统的CNN更有效。

Abstract: Handwritten characters can be trickier to classify due to their complex and
cursive nature compared to simple and non-cursive characters. We present an
external classifier along with a Generative Adversarial Network that can
classify highly cursive and complex characters. The generator network produces
fake handwritten character images, which are then used to augment the training
data after adding adversarially perturbed noise and achieving a confidence
score above a threshold with the discriminator network. The results show that
the accuracy of convolutional neural networks decreases as character complexity
increases, but our proposed model, ADA-GAN, remains more robust and effective
for both cursive and complex characters.

</details>


### [26] [TRELLIS-Enhanced Surface Features for Comprehensive Intracranial Aneurysm Analysis](https://arxiv.org/abs/2509.03095)
*Clément Hervé,Paul Garnier,Jonathan Viquerat,Elie Hachem*

Main category: cs.CV

TL;DR: 提出一种跨域特征迁移方法，利用TRELLIS模型学习到的几何嵌入来增强颅内动脉瘤分析的神经网络，以解决标注数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 颅内动脉瘤具有显著的临床风险，但由于缺乏标注的3D数据，难以检测、描绘和建模。

Method: 将TRELLIS学习到的表面特征（用于几何嵌入）应用于三个下游任务：动脉瘤与健康血管的分类、动脉瘤和血管区域的分割，以及使用图神经网络预测随时间演变的血流场。

Result: 在准确性、F1分数和分割质量方面，与最先进的基线相比，该方法均取得了显著的提升，并将模拟误差降低了15%。

Conclusion: 将通用生成模型的3D表示转移到专业的医疗任务中具有广泛的应用潜力。

Abstract: Intracranial aneurysms pose a significant clinical risk yet are difficult to
detect, delineate and model due to limited annotated 3D data. We propose a
cross-domain feature-transfer approach that leverages the latent geometric
embeddings learned by TRELLIS, a generative model trained on large-scale
non-medical 3D datasets, to augment neural networks for aneurysm analysis. By
replacing conventional point normals or mesh descriptors with TRELLIS surface
features, we systematically enhance three downstream tasks: (i) classifying
aneurysms versus healthy vessels in the Intra3D dataset, (ii) segmenting
aneurysm and vessel regions on 3D meshes, and (iii) predicting time-evolving
blood-flow fields using a graph neural network on the AnXplore dataset. Our
experiments show that the inclusion of these features yields strong gains in
accuracy, F1-score and segmentation quality over state-of-the-art baselines,
and reduces simulation error by 15\%. These results illustrate the broader
potential of transferring 3D representations from general-purpose generative
models to specialized medical tasks.

</details>


### [27] [Backdoor Poisoning Attack Against Face Spoofing Attack Detection Methods](https://arxiv.org/abs/2509.03108)
*Shota Iwamatsu,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 本文提出了一种新的后门投毒攻击方法，用于演示后门投毒在人脸反欺骗检测中的潜在威胁，该方法能够使某些欺骗攻击绕过检测，并通过在公共数据集上的实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸反欺骗检测方法主要依赖深度学习，需要大量训练数据，容易受到后门投毒攻击，导致特定欺骗攻击被误判为真实活体，产生误报。

Method: 提出了一种新颖的后门投毒攻击方法，通过将欺骗攻击的人脸图像特征嵌入到真实人脸图像中，且不引起任何可感知的视觉变化，从而实现使特定欺骗攻击能够绕过检测。

Result: 通过在公共数据集上进行实验，证明了所提出的方法对现有欺骗攻击检测系统构成了实际威胁。

Conclusion: 所提出的后门投毒攻击方法能够有效绕过现有的人脸反欺骗检测系统，表明了后门投毒在人脸反欺骗检测中的潜在威胁，需要进一步研究和防御措施。

Abstract: Face recognition systems are robust against environmental changes and noise,
and thus may be vulnerable to illegal authentication attempts using user face
photos, such as spoofing attacks. To prevent such spoofing attacks, it is
crucial to discriminate whether the input image is a live user image or a
spoofed image prior to the face recognition process. Most existing spoofing
attack detection methods utilize deep learning, which necessitates a
substantial amount of training data. Consequently, if malicious data is
injected into a portion of the training dataset, a specific spoofing attack may
be erroneously classified as live, leading to false positives.In this paper, we
propose a novel backdoor poisoning attack method to demonstrate the latent
threat of backdoor poisoning within face anti-spoofing detection. The proposed
method enables certain spoofing attacks to bypass detection by embedding
features extracted from the spoofing attack's face image into a live face image
without inducing any perceptible visual alterations.Through experiments
conducted on public datasets, we demonstrate that the proposed method
constitutes a realistic threat to existing spoofing attack detection systems.

</details>


### [28] [Information transmission: Inferring change area from change moment in time series remote sensing images](https://arxiv.org/abs/2509.03112)
*Jialu Li,Chen Wu,Meiqi Hu*

Main category: cs.CV

TL;DR: CAIM-Net通过从变化时刻推断变化区域来统一时间序列变化检测中的变化区域和变化时刻，解决了现有深度学习方法将两者视为独立任务的问题。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列变化检测方法将变化区域检测和变化时刻识别视为独立任务，未能有效利用两者间的内在联系。

Method: CAIM-Net包含三个关键步骤：1. 差异提取与增强：使用轻量级编码器和边界增强卷积提取和增强差异特征。2. 粗粒度变化时刻提取：对增强的差异特征进行时空相关性分析，通过两种不同方法确定粗粒度变化时刻。3. 细粒度变化时刻提取与变化区域推断：利用多尺度时间卷积激活映射（CAM）模块加权变化时刻，并根据变化时刻推断变化区域。

Result: CAIM-Net能够确保变化区域和变化时刻结果的一致性。

Conclusion: CAIM-Net通过从变化时刻推断变化区域，有效解决了时间序列变化检测中的不一致性问题，并实现了更准确的变化检测。

Abstract: Time series change detection is a critical task for exploring ecosystem
dynamics using time series remote sensing images, because it can simultaneously
indicate where and when change occur. While deep learning has shown excellent
performance in this domain, it continues to approach change area detection and
change moment identification as distinct tasks. Given that change area can be
inferred from change moment, we propose a time series change detection network,
named CAIM-Net (Change Area Inference from Moment Network), to ensure
consistency between change area and change moment results. CAIM-Net infers
change area from change moment based on the intrinsic relationship between time
series analysis and spatial change detection. The CAIM-Net comprises three key
steps: Difference Extraction and Enhancement, Coarse Change Moment Extraction,
and Fine Change Moment Extraction and Change Area Inference. In the Difference
Extraction and Enhancement, a lightweight encoder with batch dimension stacking
is designed to rapidly extract difference features. Subsequently, boundary
enhancement convolution is applied to amplify these difference features. In the
Coarse Change Moment Extraction, the enhanced difference features from the
first step are used to spatiotemporal correlation analysis, and then two
distinct methods are employed to determine coarse change moments. In the Fine
Change Moment Extraction and Change Area Inference, a multiscale temporal Class
Activation Mapping (CAM) module first increases the weight of the
change-occurring moment from coarse change moments. Then the weighted change
moment is used to infer change area based on the fact that pixels with the
change moment must have undergone a change.

</details>


### [29] [Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection](https://arxiv.org/abs/2509.03113)
*Shan Wang,Maying Shen,Nadine Chang,Chuong Nguyen,Hongdong Li,Jose M. Alvarez*

Main category: cs.CV

TL;DR: 该研究提出了一个无需额外资源即可减少多模态大语言模型幻觉的方法，通过识别和整合视觉线索来解决文本-视觉偏见和共现偏见，并在LLaVA-QA90数据集上实现了高达92%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型幻觉缓解方法未能理解偏差在不同实例中的波动性，因此本文旨在提出一种能够动态适应偏差水平的方法。

Method: 通过基于梯度的自我反思方法估计不同类型标记（视觉、提示、先前输出）的影响，然后利用这些影响来识别与对象相关的视觉标记，并将其集成到一个影响感知的对比解码框架中，以同时缓解文本-视觉偏见和共现偏见。

Result: 该方法在LLaVA-QA90数据集上实现了高达92%的准确率提升，有效减少了幻觉，并且无需微调、额外模型或数据统计等额外资源。

Conclusion: 所提出的基于梯度自我反思和影响感知对比解码的方法，能够有效识别和缓解多模态大语言模型中的文本-视觉偏见和共现偏见，从而显著减少幻觉的产生，且无需额外计算资源。

Abstract: Hallucinations in multimodal large language model are caused by the
text-visual bias and the co-occurrence bias. The former reflects an
over-reliance on text information in the decision-making process, while the
latter arises from the statistical object-pairing patterns abstracted from the
training data. Existing mitigation methods heuristically address these biases
without understanding the fluctuating bias level across the instances. We first
propose estimating the influence of respective token types (visual, prompt, and
previous outputs) using a gradient-based self-reflection method. The estimated
token influence further enables the detection of object-related visual tokens
and their integration into an influence-aware contrastive decoding framework to
mitigate both types of biases simultaneously. Our method operates without the
need for additional resources, such as costly fine-tuning, extra models, or
data statistics. Extensive experiments show it effectively reduces
hallucinations, achieving up to a 92% accuracy increase on LLaVA-QA90.

</details>


### [30] [Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge](https://arxiv.org/abs/2509.03114)
*Miao Xu,Xiangyu Zhu,Xusheng Liang,Zidu Wang,Jinlin Wu,Zhen Lei*

Main category: cs.CV

TL;DR: GravityDB通过模拟可变形手部表面与刚性物体之间的交互来解决手-物体交互中的穿透和变形问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理人手和物体复杂的几何形状时，常出现穿透或接触区域有明显间隙的问题，且难以捕捉真实的形变。本研究旨在解决这些挑战。

Method: 提出一种基于引力场的扩散桥（GravityDB）方法，将手-物体交互视为一个引力驱动的过程，模拟可变形手部表面与刚性物体之间的交互，并结合文本语义信息指导引力场构建。

Result: 该方法有效解决了穿透问题，确保了抓握的稳定性，并捕捉了真实的手部变形。实验证明了该方法的有效性。

Conclusion: GravityDB能够生成符合物理规律、无穿透、抓握稳定且能捕捉真实形变的手-物体交互。结合语义信息可以实现更具意义的交互区域。

Abstract: Existing reconstruction or hand-object pose estimation methods are capable of
producing coarse interaction states. However, due to the complex and diverse
geometry of both human hands and objects, these approaches often suffer from
interpenetration or leave noticeable gaps in regions that are supposed to be in
contact. Moreover, the surface of a real human hand undergoes non-negligible
deformations during interaction, which are difficult to capture and represent
with previous methods. To tackle these challenges, we formulate hand-object
interaction as an attraction-driven process and propose a Gravity-Field Based
Diffusion Bridge (GravityDB) to simulate interactions between a deformable hand
surface and rigid objects. Our approach effectively resolves the aforementioned
issues by generating physically plausible interactions that are free of
interpenetration, ensure stable grasping, and capture realistic hand
deformations. Furthermore, we incorporate semantic information from textual
descriptions to guide the construction of the gravitational field, enabling
more semantically meaningful interaction regions. Extensive qualitative and
quantitative experiments on multiple datasets demonstrate the effectiveness of
our method.

</details>


### [31] [Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation](https://arxiv.org/abs/2509.03141)
*Mattia Litrico,Francesco Guarnera,Mario Valerio Giuffrida,Daniele Ravì,Sebastiano Battiato*

Main category: cs.CV

TL;DR: 该研究提出了一种名为TADM-3D的三维模型，可以根据基线MRI预测未来脑部变化，解决了现有方法的局限性，并在OASIS-3和NACC数据集上进行了训练和验证。


<details>
  <summary>Details</summary>
Motivation: 生成逼真的MRI以准确预测大脑未来结构变化，对于临床医生评估患者的临床结果和疾病进展至关重要。现有方法在处理时间间隔、生成临床相关的未来图像以及利用三维解剖信息方面存在不足。

Method: 提出了一种名为TADM-3D的三维模型，结合了预训练的大脑年龄估计器（BAE）来准确反映时间间隔和大脑变化之间的关系，并通过反向时间正则化（BITR）来提高模型的时间感知能力，实现双向预测。

Result: 在OASIS-3数据集上训练，并在NACC数据集上验证了模型的泛化能力。

Conclusion: TADM-3D模型能够准确预测脑部MRI的长期变化，优于现有方法。

Abstract: Generating realistic MRIs to accurately predict future changes in the
structure of brain is an invaluable tool for clinicians in assessing clinical
outcomes and analysing the disease progression at the patient level. However,
current existing methods present some limitations: (i) some approaches fail to
explicitly capture the relationship between structural changes and time
intervals, especially when trained on age-imbalanced datasets; (ii) others rely
only on scan interpolation, which lack clinical utility, as they generate
intermediate images between timepoints rather than future pathological
progression; and (iii) most approaches rely on 2D slice-based architectures,
thereby disregarding full 3D anatomical context, which is essential for
accurate longitudinal predictions. We propose a 3D Temporally-Aware Diffusion
Model (TADM-3D), which accurately predicts brain progression on MRI volumes. To
better model the relationship between time interval and brain changes, TADM-3D
uses a pre-trained Brain-Age Estimator (BAE) that guides the diffusion model in
the generation of MRIs that accurately reflect the expected age difference
between baseline and generated follow-up scans. Additionally, to further
improve the temporal awareness of TADM-3D, we propose the Back-In-Time
Regularisation (BITR), by training TADM-3D to predict bidirectionally from the
baseline to follow-up (forward), as well as from the follow-up to baseline
(backward). Although predicting past scans has limited clinical applications,
this regularisation helps the model generate temporally more accurate scans. We
train and evaluate TADM-3D on the OASIS-3 dataset, and we validate the
generalisation performance on an external test set from the NACC dataset. The
code will be available upon acceptance.

</details>


### [32] [Preserving instance continuity and length in segmentation through connectivity-aware loss computation](https://arxiv.org/abs/2509.03154)
*Karol Szustakowski,Luk Frank,Julia Esser,Jan Gründemann,Marie Piraud*

Main category: cs.CV

TL;DR: 所提出的负中心线损失和简化拓扑损失函数可以提高生物医学图像分割中细长结构的连续性。


<details>
  <summary>Details</summary>
Motivation: 许多生物医学图像分割任务比体素精度更看重细长结构连续性的保持和长度的计算。本研究提出的方法旨在解决这个问题。

Method: 提出并应用了两种新的损失函数——负中心线损失和简化拓扑损失——来增强卷积神经网络（CNN）在分割任务中保持输出实例连通性的能力。此外，还讨论了有助于获得连续分割掩码的实验设计特性，如下采样和间距校正。

Result: 与标准的CNN和现有的拓扑感知损失相比，所提出的方法在减少实例分割不连续性方面表现更优，尤其是在输入信号丢失的区域，从而提高了下游应用中实例长度计算的准确性。

Conclusion: 在本研究中，我们证明了通过损失函数设计中嵌入结构先验，可以显著提高生物医学图像分割的可靠性。

Abstract: In many biomedical segmentation tasks, the preservation of elongated
structure continuity and length is more important than voxel-wise accuracy. We
propose two novel loss functions, Negative Centerline Loss and Simplified
Topology Loss, that, applied to Convolutional Neural Networks (CNNs), help
preserve connectivity of output instances. Moreover, we discuss characteristics
of experiment design, such as downscaling and spacing correction, that help
obtain continuous segmentation masks. We evaluate our approach on a 3D
light-sheet fluorescence microscopy dataset of axon initial segments (AIS), a
task prone to discontinuity due to signal dropout. Compared to standard CNNs
and existing topology-aware losses, our methods reduce the number of
segmentation discontinuities per instance, particularly in regions with missing
input signal, resulting in improved instance length calculation in downstream
applications. Our findings demonstrate that structural priors embedded in the
loss design can significantly enhance the reliability of segmentation for
biological applications.

</details>


### [33] [Count2Density: Crowd Density Estimation without Location-level Annotations](https://arxiv.org/abs/2509.03170)
*Mattia Litrico,Feng Chen,Michael Pound,Sotirios A Tsaftaris,Sebastiano Battiato,Mario Valerio Giuffrida*

Main category: cs.CV

TL;DR: Count2Density 使用仅有总人数信息（count-level annotations）的标注来预测详细的密度图，克服了传统方法对精确标注点（point-level annotations）的依赖，并通过历史图库和自监督空间正则化等方法提高了准确性和空间信息检索能力。


<details>
  <summary>Details</summary>
Motivation: 传统人群密度估计依赖于耗时耗力的点标注，限制了模型的扩展性。本研究旨在提出一种仅使用总人数标注即可预测密度图的方法。

Method: Count2Density 管道通过生成伪密度图（利用历史图库减少确认偏差）、使用超几何分布采样以及添加自监督对比空间正则化器来增强空间感知能力。

Result: Count2Density 在跨域适应方法上表现出显著的优势，并在半监督设置下优于最先进的方法，同时能有效检索空间信息并进行子区域计数。

Conclusion: Count2Density 成功地从总人数标注中提取了空间信息，实现了准确的人群密度估计，并为简化标注过程提供了有效的解决方案。

Abstract: Crowd density estimation is a well-known computer vision task aimed at
estimating the density distribution of people in an image. The main challenge
in this domain is the reliance on fine-grained location-level annotations,
(i.e. points placed on top of each individual) to train deep networks.
Collecting such detailed annotations is both tedious, time-consuming, and poses
a significant barrier to scalability for real-world applications. To alleviate
this burden, we present Count2Density: a novel pipeline designed to predict
meaningful density maps containing quantitative spatial information using only
count-level annotations (i.e., total number of people) during training. To
achieve this, Count2Density generates pseudo-density maps leveraging past
predictions stored in a Historical Map Bank, thereby reducing confirmation
bias. This bank is initialised using an unsupervised saliency estimator to
provide an initial spatial prior and is iteratively updated with an EMA of
predicted density maps. These pseudo-density maps are obtained by sampling
locations from estimated crowd areas using a hypergeometric distribution, with
the number of samplings determined by the count-level annotations. To further
enhance the spatial awareness of the model, we add a self-supervised
contrastive spatial regulariser to encourage similar feature representations
within crowded regions while maximising dissimilarity with background regions.
Experimental results demonstrate that our approach significantly outperforms
cross-domain adaptation methods and achieves better results than recent
state-of-the-art approaches in semi-supervised settings across several
datasets. Additional analyses validate the effectiveness of each individual
component of our pipeline, confirming the ability of Count2Density to
effectively retrieve spatial information from count-level annotations and
enabling accurate subregion counting.

</details>


### [34] [AutoDetect: Designing an Autoencoder-based Detection Method for Poisoning Attacks on Object Detection Applications in the Military Domain](https://arxiv.org/abs/2509.03179)
*Alma M. Liezenga,Stefan Wijnja,Puck de Haan,Niels W. T. Brink,Jip J. van Stijn,Yori Kamphuis,Klamer Schutte*

Main category: cs.CV

TL;DR: 本研究探讨了军事目标检测系统中的投毒攻击及其检测方法，并提出了新的检测方法AutoDetect。


<details>
  <summary>Details</summary>
Motivation: 军事领域AI系统面临日益增长的投毒攻击威胁，尤其是在目标检测系统方面，现有研究不足，后果严重。

Method: 创建了一个包含军事载具的小型自定义数据集MilCivVeh。通过修改BadDet攻击实现了一种基于补丁的投毒攻击，并评估了其影响。测试了专门的投毒检测方法和来自视觉工业检测领域的异常检测方法，并提出了一种新的基于自编码器的补丁检测方法AutoDetect。

Result: 所提出的BadDet攻击在需要大量被投毒数据的情况下才能实现较高的成功率，其现实可行性受到质疑。现有的检测方法在检测军事目标检测系统中的投毒攻击方面存在不足。AutoDetect方法通过图像块的重建误差，在区分干净和被投毒样本方面表现出 promising 的结果，优于现有方法，并且更加高效。

Conclusion: 大型、具有代表性的军事领域数据集的可用性是进一步评估投毒攻击风险和补丁检测机会的先决条件。

Abstract: Poisoning attacks pose an increasing threat to the security and robustness of
Artificial Intelligence systems in the military domain. The widespread use of
open-source datasets and pretrained models exacerbates this risk. Despite the
severity of this threat, there is limited research on the application and
detection of poisoning attacks on object detection systems. This is especially
problematic in the military domain, where attacks can have grave consequences.
In this work, we both investigate the effect of poisoning attacks on military
object detectors in practice, and the best approach to detect these attacks. To
support this research, we create a small, custom dataset featuring military
vehicles: MilCivVeh. We explore the vulnerability of military object detectors
for poisoning attacks by implementing a modified version of the BadDet attack:
a patch-based poisoning attack. We then assess its impact, finding that while a
positive attack success rate is achievable, it requires a substantial portion
of the data to be poisoned -- raising questions about its practical
applicability. To address the detection challenge, we test both specialized
poisoning detection methods and anomaly detection methods from the visual
industrial inspection domain. Since our research shows that both classes of
methods are lacking, we introduce our own patch detection method: AutoDetect, a
simple, fast, and lightweight autoencoder-based method. Our method shows
promising results in separating clean from poisoned samples using the
reconstruction error of image slices, outperforming existing methods, while
being less time- and memory-intensive. We urge that the availability of large,
representative datasets in the military domain is a prerequisite to further
evaluate risks of poisoning attacks and opportunities patch detection.

</details>


### [35] [PPORLD-EDNetLDCT: A Proximal Policy Optimization-Based Reinforcement Learning Framework for Adaptive Low-Dose CT Denoising](https://arxiv.org/abs/2509.03185)
*Debopom Sutradhar,Ripon Kumar Debnath,Mohaimenul Azam Khan Raiaan,Yan Zhang,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: 本研究提出了一种基于强化学习（RL）的低剂量CT（LDCT）去噪方法PPORLD-EDNetLDCT，通过动态优化去噪策略提升图像质量，并在多个数据集上验证了其优于传统和深度学习方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法难以在降低LDCT图像噪声的同时保持图像质量，本研究旨在解决这一挑战。

Method: 提出了一种基于强化学习（RL）和编码器-解码器（Encoder-Decoder）的LDCT去噪方法PPORLD-EDNetLDCT，利用PPO算法根据图像质量反馈实时优化去噪策略。

Result: 在LDCT图像和投影数据集上，PPORLD-EDNetLDCT在PSNR、SSIM和RMSE方面优于传统和深度学习方法。在NIH-AAPM-Mayo Clinic LDCT挑战数据集上，也取得了优异的性能。在COVID-19 LDCT数据集的分类任务中，使用该方法处理的图像提高了分类准确率至94%。

Conclusion: PPORLD-EDNetLDCT为LDCT成像提供了一种更安全、更准确的去噪解决方案，能够有效提升图像质量并改善后续诊断任务的性能。

Abstract: Low-dose computed tomography (LDCT) is critical for minimizing radiation
exposure, but it often leads to increased noise and reduced image quality.
Traditional denoising methods, such as iterative optimization or supervised
learning, often fail to preserve image quality. To address these challenges, we
introduce PPORLD-EDNetLDCT, a reinforcement learning-based (RL) approach with
Encoder-Decoder for LDCT. Our method utilizes a dynamic RL-based approach in
which an advanced posterior policy optimization (PPO) algorithm is used to
optimize denoising policies in real time, based on image quality feedback,
trained via a custom gym environment. The experimental results on the low dose
CT image and projection dataset demonstrate that the proposed PPORLD-EDNetLDCT
model outperforms traditional denoising techniques and other DL-based methods,
achieving a peak signal-to-noise ratio of 41.87, a structural similarity index
measure of 0.9814 and a root mean squared error of 0.00236. Moreover, in
NIH-AAPM-Mayo Clinic Low Dose CT Challenge dataset our method achived a PSNR of
41.52, SSIM of 0.9723 and RMSE of 0.0051. Furthermore, we validated the quality
of denoising using a classification task in the COVID-19 LDCT dataset, where
the images processed by our method improved the classification accuracy to
94\%, achieving 4\% higher accuracy compared to denoising without RL-based
denoising. This method offers a promising solution for safer and more accurate
LDCT imaging.

</details>


### [36] [AIVA: An AI-based Virtual Companion for Emotion-aware Interaction](https://arxiv.org/abs/2509.03212)
*Chenxi Li*

Main category: cs.CV

TL;DR: LLM在情感交互方面存在局限，提出一种名为“","\"ours\""的多模态情感感知方法，通过融合文本、语音等多种信息，使AI能够理解并响应用户情感，从而实现更具情感的交互。


<details>
  <summary>Details</summary>
Motivation: 目前的大型语言模型（LLMs）主要局限于文本处理，缺乏理解非语言情感线索的能力，这阻碍了更具沉浸感和共情能力的人机交互（HCI）。

Method: 提出一个名为‘\"ours\"’的AI虚拟伴侣，该系统包含一个多模态情感感知网络（MSPN），利用跨模态融合Transformer和监督对比学习来处理情感线索。此外，还开发了一种情感感知的提示工程策略来生成共情响应，并整合了文本到语音（TTS）系统和动画化身模块来实现表达性交互。

Result: ‘\"ours\"’能够捕捉多模态情感线索，实现情感对齐和动画化的人机交互。

Conclusion: ‘\"ours\"’提供了一个实现情感感知智能体的框架，可以应用于伴侣机器人、社会关怀、心理健康和以人为中心的人工智能等领域。

Abstract: Recent advances in Large Language Models (LLMs) have significantly improved
natural language understanding and generation, enhancing Human-Computer
Interaction (HCI). However, LLMs are limited to unimodal text processing and
lack the ability to interpret emotional cues from non-verbal signals, hindering
more immersive and empathetic interactions. This work explores integrating
multimodal sentiment perception into LLMs to create emotion-aware agents. We
propose \ours, an AI-based virtual companion that captures multimodal sentiment
cues, enabling emotionally aligned and animated HCI. \ours introduces a
Multimodal Sentiment Perception Network (MSPN) using a cross-modal fusion
transformer and supervised contrastive learning to provide emotional cues.
Additionally, we develop an emotion-aware prompt engineering strategy for
generating empathetic responses and integrate a Text-to-Speech (TTS) system and
animated avatar module for expressive interactions. \ours provides a framework
for emotion-aware agents with applications in companion robotics, social care,
mental health, and human-centered AI.

</details>


### [37] [RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text Generation and Multimodal Feature Fusion](https://arxiv.org/abs/2509.03214)
*Junhao Jia,Yifei Sun,Yunyou Liu,Cheng Yang,Changmiao Wang,Feiwei Qin,Yong Peng,Wenwen Min*

Main category: cs.CV

TL;DR: RTGMFF框架通过结合自动ROI文本生成和多模态特征融合，提高了基于fMRI的脑部疾病诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于fMRI的脑部疾病诊断方法受限于低信噪比、跨被试变异性以及CNN和Transformer模型对频率信息的敏感度有限，并且fMRI数据集普遍缺乏可用于解释激活和连接模式的文本注释。

Method: RTGMFF框架包含三个组件：(i) ROI驱动的fMRI文本生成，将激活、连接、年龄和性别浓缩为文本标记；(ii) 混合频空编码器，融合了小波-mamba分支和跨尺度Transformer编码器来捕捉频域结构和长距离空间依赖性；(iii) 自适应语义对齐模块，使用正则化余弦相似度损失将ROI标记序列和视觉特征嵌入共享空间。

Result: 在ADHD-200和ABIDE数据集上的实验表明，RTGMFF在诊断准确性、敏感性、特异性和ROC曲线下面积方面均优于现有方法。

Conclusion: RTGMFF框架通过整合文本生成和多模态特征融合，有效地解决了fMRI脑部疾病诊断中的挑战，并取得了显著的性能提升。

Abstract: Functional magnetic resonance imaging (fMRI) is a powerful tool for probing
brain function, yet reliable clinical diagnosis is hampered by low
signal-to-noise ratios, inter-subject variability, and the limited frequency
awareness of prevailing CNN- and Transformer-based models. Moreover, most fMRI
datasets lack textual annotations that could contextualize regional activation
and connectivity patterns. We introduce RTGMFF, a framework that unifies
automatic ROI-level text generation with multimodal feature fusion for
brain-disorder diagnosis. RTGMFF consists of three components: (i) ROI-driven
fMRI text generation deterministically condenses each subject's activation,
connectivity, age, and sex into reproducible text tokens; (ii) Hybrid
frequency-spatial encoder fuses a hierarchical wavelet-mamba branch with a
cross-scale Transformer encoder to capture frequency-domain structure alongside
long-range spatial dependencies; and (iii) Adaptive semantic alignment module
embeds the ROI token sequence and visual features in a shared space, using a
regularized cosine-similarity loss to narrow the modality gap. Extensive
experiments on the ADHD-200 and ABIDE benchmarks show that RTGMFF surpasses
current methods in diagnostic accuracy, achieving notable gains in sensitivity,
specificity, and area under the ROC curve. Code is available at
https://github.com/BeistMedAI/RTGMFF.

</details>


### [38] [LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking](https://arxiv.org/abs/2509.03221)
*Jing Zhang,Siying Tao,Jiao Li,Tianhe Wang,Junchen Wu,Ruqian Hao,Xiaohui Du,Ruirong Tan,Rui Li*

Main category: cs.CV

TL;DR: LGBP-OrgaNet是一个基于深度学习的系统，用于对类器官进行分割、跟踪和量化，具有高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统荧光标记方法可能损害类器官结构，因此需要一种自动化、非破坏性的类器官分割和跟踪方法。

Method: 提出LGBP-OrgaNet，一个结合CNN和Transformer模块，并使用特征融合模块（Learnable Gaussian Band Pass Fusion）和双向交叉融合块（Bidirectional Cross Fusion Block）来处理多尺度特征的深度学习系统。

Result: SROrga在类器官分割数据集上展现了令人满意的分割精度和鲁棒性。

Conclusion: LGBP-OrgaNet为类器官研究提供了一个强大的工具，能够实现自动化、非破坏性的类器官分割、跟踪和量化。

Abstract: Organoids replicate organ structure and function, playing a crucial role in
fields such as tumor treatment and drug screening. Their shape and size can
indicate their developmental status, but traditional fluorescence labeling
methods risk compromising their structure. Therefore, this paper proposes an
automated, non-destructive approach to organoid segmentation and tracking. We
introduced the LGBP-OrgaNet, a deep learning-based system proficient in
accurately segmenting, tracking, and quantifying organoids. The model leverages
complementary information extracted from CNN and Transformer modules and
introduces the innovative feature fusion module, Learnable Gaussian Band Pass
Fusion, to merge data from two branches. Additionally, in the decoder, the
model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features,
and finally completes the decoding through progressive concatenation and
upsampling. SROrga demonstrates satisfactory segmentation accuracy and
robustness on organoids segmentation datasets, providing a potent tool for
organoid research.

</details>


### [39] [PI3DETR: Parametric Instance Detection of 3D Point Cloud Edges with a Geometry-Aware 3DETR](https://arxiv.org/abs/2509.03262)
*Fabio F. Oberweger,Michael Schwingshackl,Vanessa Staderini*

Main category: cs.CV

TL;DR: PI3DETR是一个端到端的框架，可以直接从点云预测3D参数曲线实例，避免了中间表示和多阶段处理。它能够统一检测不同参数类型的曲线（如三次贝塞尔曲线、线段、圆和弧），并具有几何感知匹配策略和专门的损失函数。该模型在ABC数据集上达到了新的最先进水平，并能有效泛化到真实传感器数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要中间表示和多阶段处理，而PI3DETR旨在直接从点云预测3D参数曲线实例，避免这些复杂性，并统一检测不同类型的曲线。

Method: PI3DETR 扩展了 3DETR 模型，引入了几何感知匹配策略和专门的损失函数，以实现对三次贝塞尔曲线、线段、圆和弧等不同参数化曲线类型的统一检测。模型可以在一次前向传播中完成检测，并提供可选的后处理步骤来优化预测。

Result: PI3DETR 在 ABC 数据集上设定了新的最先进水平，并能有效泛化到真实传感器数据。该模型提高了对噪声和不同采样密度的鲁棒性。

Conclusion: PI3DETR 提供了一种简单而强大的 3D 边缘和曲线估计解决方案，通过其端到端的框架和统一检测不同类型曲线的能力，解决了现实世界 LiDAR 和 3D 感知中的关键挑战。

Abstract: We present PI3DETR, an end-to-end framework that directly predicts 3D
parametric curve instances from raw point clouds, avoiding the intermediate
representations and multi-stage processing common in prior work. Extending
3DETR, our model introduces a geometry-aware matching strategy and specialized
loss functions that enable unified detection of differently parameterized curve
types, including cubic B\'ezier curves, line segments, circles, and arcs, in a
single forward pass. Optional post-processing steps further refine predictions
without adding complexity. This streamlined design improves robustness to noise
and varying sampling densities, addressing critical challenges in real world
LiDAR and 3D sensing scenarios. PI3DETR sets a new state-of-the-art on the ABC
dataset and generalizes effectively to real sensor data, offering a simple yet
powerful solution for 3D edge and curve estimation.

</details>


### [40] [SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D Diffusion Model](https://arxiv.org/abs/2509.03267)
*Hongxu Yang,Edina Timko,Levente Lippenszky,Vanda Czipczer,Lehel Ferenczi*

Main category: cs.CV

TL;DR: 该研究提出了一种名为SynBT的三维扩散模型，用于生成高质量的乳腺增强MRI图像中的肿瘤。该模型通过一个“patch-to-volume”自编码器压缩高分辨率MRI图像，并利用掩码条件扩散模型在乳腺组织中合成逼真的肿瘤。实验证明，SynBT生成的肿瘤可以提高分割模型的性能，Dice分数提高2-3%。


<details>
  <summary>Details</summary>
Motivation: 现有的肿瘤合成方法在处理大空间体积的肿瘤（如大视野的乳腺MRI图像）时性能不佳，因为它们通常基于小图像块。因此，需要一种能够生成高质量大体积肿瘤的方法。

Method: 提出一种名为SynBT的三维扩散模型，包含一个“patch-to-volume”自编码器，用于压缩高分辨率MRI图像并保留大视野体积的分辨率。然后，使用掩码条件扩散模型在提取的潜在空间特征向量中合成乳腺肿瘤，以实现逼真的外观。

Result: SynBT模型生成的乳腺肿瘤提高了肿瘤分割任务的性能，Dice分数提高了2-3%，证明了其在MRI图像肿瘤分割中的有效性。

Conclusion: SynBT是一种有效的三维扩散模型，能够生成高质量的乳腺肿瘤合成图像，从而提升MRI图像中肿瘤分割的性能。

Abstract: Synthetic tumors in medical images offer controllable characteristics that
facilitate the training of machine learning models, leading to an improved
segmentation performance. However, the existing methods of tumor synthesis
yield suboptimal performances when tumor occupies a large spatial volume, such
as breast tumor segmentation in MRI with a large field-of-view (FOV), while
commonly used tumor generation methods are based on small patches. In this
paper, we propose a 3D medical diffusion model, called SynBT, to generate
high-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed
model consists of a patch-to-volume autoencoder, which is able to compress the
high-resolution MRIs into compact latent space, while preserving the resolution
of volumes with large FOV. Using the obtained latent space feature vector, a
mask-conditioned diffusion model is used to synthesize breast tumors within
selected regions of breast tissue, resulting in realistic tumor appearances. We
evaluated the proposed method for a tumor segmentation task, which demonstrated
the proposed high-quality tumor synthesis method can facilitate the common
segmentation models with performance improvement of 2-3% Dice Score on a large
public dataset, and therefore provides benefits for tumor segmentation in MRI
images.

</details>


### [41] [PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly Detection](https://arxiv.org/abs/2509.03277)
*Qihang Zhou,Shibo He,Jiangtao Yan,Wenchao Meng,Jiming Chen*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we aim to transfer CLIP's robust 2D generalization
capabilities to identify 3D anomalies across unseen objects of highly diverse
class semantics. To this end, we propose a unified framework to comprehensively
detect and segment 3D anomalies by leveraging both point- and pixel-level
information. We first design PointAD, which leverages point-pixel
correspondence to represent 3D anomalies through their associated rendering
pixel representations. This approach is referred to as implicit 3D
representation, as it focuses solely on rendering pixel anomalies but neglects
the inherent spatial relationships within point clouds. Then, we propose
PointAD+ to further broaden the interpretation of 3D anomalies by introducing
explicit 3D representation, emphasizing spatial abnormality to uncover abnormal
spatial relationships. Hence, we propose G-aggregation to involve geometry
information to enable the aggregated point representations spatially aware. To
simultaneously capture rendering and spatial abnormality, PointAD+ proposes
hierarchical representation learning, incorporating implicit and explicit
anomaly semantics into hierarchical text prompts: rendering prompts for the
rendering layer and geometry prompts for the geometry layer. A cross-hierarchy
contrastive alignment is further introduced to promote the interaction between
the rendering and geometry layers, facilitating mutual anomaly learning.
Finally, PointAD+ integrates anomaly semantics from both layers to capture the
generalized anomaly semantics. During the test, PointAD+ can integrate RGB
information in a plug-and-play manner and further improve its detection
performance. Extensive experiments demonstrate the superiority of PointAD+ in
ZS 3D anomaly detection across unseen objects with highly diverse class
semantics, achieving a holistic understanding of abnormality.

</details>


### [42] [Empowering Lightweight MLLMs with Reasoning via Long CoT SFT](https://arxiv.org/abs/2509.03321)
*Linyu Ou*

Main category: cs.CV

TL;DR: 本研究表明，在轻量级多模态语言模型（MLLM）上使用长链式思考（long CoT）数据进行监督微调（SFT）可以显著提高其推理能力，并且后续的强化学习（RL）阶段可以带来额外的性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索长链式思考（long CoT）数据在增强轻量级多模态语言模型（MLLM）推理能力方面的作用，因为先前的方法主要集中在大型语言模型上。

Method: 使用长链式思考（long CoT）数据对轻量级多模态语言模型（MLLM）进行监督微调（SFT），然后在SFT之后进行强化学习（RL）微调。

Result: 监督微调（SFT）配合长链式思考（long CoT）数据显著提高了轻量级多模态语言模型（MLLM）的推理能力。随后的强化学习（RL）阶段进一步提升了模型性能。

Conclusion: 对于轻量级多模态语言模型（MLLM）而言，使用长链式思考（long CoT）数据进行监督微调（SFT）是发展其推理能力的关键前提。

Abstract: While Reinforcement Learning with Verifiable Rewards has enhanced the
reasoning of large-scale language models (LLMs), its efficacy for lightweight
multimodal language models (MLLMs) with fewer than seven billion parameters
remains underexplored. This paper investigates the role of long
Chain-of-Thought (long CoT) data in enhancing the reasoning abilities of such
MLLMs. Our findings demonstrate that Supervised Fine-Tuning (SFT) with long CoT
data significantly improves MLLM reasoning. Furthermore, we observe that after
this initial SFT phase, MLLMs can achieve additional performance gains through
a subsequent RL stage. We conclude that a SFT stage with long CoT data is a
critical prerequisite for developing the reasoning capabilities of lightweight
MLLMs.

</details>


### [43] [Heatmap Guided Query Transformers for Robust Astrocyte Detection across Immunostains and Resolutions](https://arxiv.org/abs/2509.03323)
*Xizhe Zhang,Jiayang Zhu*

Main category: cs.CV

TL;DR: 提出了一种结合CNN和Transformer的混合模型，用于自动检测神经胶质细胞（星形胶质细胞），解决了细胞形态复杂和染色依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 神经胶质细胞的形态和密度改变是多种神经系统疾病的特征，但其复杂的形态和染色依赖性给自动化检测带来挑战。

Method: 提出了一种混合CNN-Transformer检测器，结合了局部特征提取和全局上下文推理。通过热图引导的查询机制生成空间锚点，并利用轻量级Transformer模块处理密集区域。

Result: 在ALDH1L1和GFAP染色数据集上，该模型在敏感性和减少假阳性方面优于Faster R-CNN、YOLOv11和DETR，FROC分析证实了其有效性。

Conclusion: 混合CNN-Transformer架构在星形胶质细胞检测方面表现出鲁棒性，为计算病理学工具奠定了基础。

Abstract: Astrocytes are critical glial cells whose altered morphology and density are
hallmarks of many neurological disorders. However, their intricate branching
and stain dependent variability make automated detection of histological images
a highly challenging task. To address these challenges, we propose a hybrid CNN
Transformer detector that combines local feature extraction with global
contextual reasoning. A heatmap guided query mechanism generates spatially
grounded anchors for small and faint astrocytes, while a lightweight
Transformer module improves discrimination in dense clusters. Evaluated on
ALDH1L1 and GFAP stained astrocyte datasets, the model consistently
outperformed Faster R-CNN, YOLOv11 and DETR, achieving higher sensitivity with
fewer false positives, as confirmed by FROC analysis. These results highlight
the potential of hybrid CNN Transformer architectures for robust astrocyte
detection and provide a foundation for advanced computational pathology tools.

</details>


### [44] [InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds](https://arxiv.org/abs/2509.03324)
*Yixiong Jing,Cheng Zhang,Haibing Wu,Guangming Wang,Olaf Wysocki,Brian Sheil*

Main category: cs.CV

TL;DR: InfraDiffusion是一个创新的框架，利用点云数据进行低光照下的砖石结构精细化分割，通过将点云投影到深度图并利用DDNM进行恢复，无需特定任务的训练，即可显著提升分割精度，为自动化检测提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 现有的结构组件语义分割方法已成熟，但针对砖块级别的分割（如剥落、砂浆损失等缺陷检测）主要依赖RGB图像。然而，在光照不足的场景（如砖石隧道）下获取高分辨率图像存在困难。虽然点云在光照方面具有优势，但其通常非结构化、稀疏且有噪声，限制了精细化分割的精度。因此，需要一种能处理低光照、利用点云数据进行精细化分割的方法。

Method: 提出了一种名为InfraDiffusion的零样本框架。该框架首先使用虚拟摄像头将砖石点云投影到深度图。然后，通过适配去噪扩散空（DDNM）模型来恢复这些深度图。该过程不需要针对特定任务进行训练。

Result: 实验结果表明，InfraDiffusion在恢复的深度图上，使用Segment Anything Model（SAM）进行砖块级别分割时，相比现有方法有了显著的提升。在砖石桥梁和隧道点云数据集上的实验验证了其有效性。

Conclusion: InfraDiffusion框架能够有效处理光照不足的问题，通过点云生成高质量的深度图，极大地增强了砖块级别分割的精度，证明了其在自动化检测砖石资产方面的巨大潜力。

Abstract: Point clouds are widely used for infrastructure monitoring by providing
geometric information, where segmentation is required for downstream tasks such
as defect detection. Existing research has automated semantic segmentation of
structural components, while brick-level segmentation (identifying defects such
as spalling and mortar loss) has been primarily conducted from RGB images.
However, acquiring high-resolution images is impractical in low-light
environments like masonry tunnels. Point clouds, though robust to dim lighting,
are typically unstructured, sparse, and noisy, limiting fine-grained
segmentation. We present InfraDiffusion, a zero-shot framework that projects
masonry point clouds into depth maps using virtual cameras and restores them by
adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific
training, InfraDiffusion enhances visual clarity and geometric consistency of
depth maps. Experiments on masonry bridge and tunnel point cloud datasets show
significant improvements in brick-level segmentation using the Segment Anything
Model (SAM), underscoring its potential for automated inspection of masonry
assets. Our code and data is available at
https://github.com/Jingyixiong/InfraDiffusion-official-implement.

</details>


### [45] [Transformer-Guided Content-Adaptive Graph Learning for Hyperspectral Unmixing](https://arxiv.org/abs/2509.03376)
*Hui Chen,Liangyu Liu,Xianchao Xiu,Wanquan Liu*

Main category: cs.CV

TL;DR: T-CAGU是一个创新的高光谱分解框架，它结合了Transformer来捕捉全局依赖关系和内容自适应图神经网络来增强局部关系，以解决现有方法在同时表征全局依赖和局部一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习高光谱分解方法难以同时表征全局依赖和局部一致性，导致难以同时保留长程相互作用和边界细节。

Method: 提出了一种新颖的Transformer引导的内容自适应图分解框架（T-CAGU），它使用Transformer捕获全局依赖，并引入内容自适应图神经网络来增强局部关系。T-CAGU集成了多种传播顺序来动态学习图结构，并通过图残差机制来保留全局信息和稳定训练。

Result: 实验结果表明，T-CAGU优于现有最先进的方法。

Conclusion: T-CAGU框架能够有效地同时表征全局依赖和局部一致性，并在高光谱分解任务中取得优越性能。

Abstract: Hyperspectral unmixing (HU) targets to decompose each mixed pixel in remote
sensing images into a set of endmembers and their corresponding abundances.
Despite significant progress in this field using deep learning, most methods
fail to simultaneously characterize global dependencies and local consistency,
making it difficult to preserve both long-range interactions and boundary
details. This letter proposes a novel transformer-guided content-adaptive graph
unmixing framework (T-CAGU), which overcomes these challenges by employing a
transformer to capture global dependencies and introducing a content-adaptive
graph neural network to enhance local relationships. Unlike previous work,
T-CAGU integrates multiple propagation orders to dynamically learn the graph
structure, ensuring robustness against noise. Furthermore, T-CAGU leverages a
graph residual mechanism to preserve global information and stabilize training.
Experimental results demonstrate its superiority over the state-of-the-art
methods. Our code is available at https://github.com/xianchaoxiu/T-CAGU.

</details>


### [46] [TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers](https://arxiv.org/abs/2509.03379)
*Guoxin Wang,Qingyuan Wang,Binhua Huang,Shaowu Chen,Deepu John*

Main category: cs.CV

TL;DR: TinyDrop是一个训练免费的、即插即用的Transformer视觉模型（ViT）的框架，它使用一个轻量级的视觉模型来指导，以减少推理成本。


<details>
  <summary>Details</summary>
Motivation: 为了在不影响准确性的前提下降低大型ViT的推理成本。

Method: TinyDrop框架使用一个轻量级的视觉模型来估计ViT中图像标记（tokens）的重要性，并选择性地丢弃低重要性的标记，以减少ViT在进行注意力计算时的成本。

Result: TinyDrop框架可以将ViT的计算量（FLOPs）降低高达80%，同时准确率的下降极小。

Conclusion: TinyDrop框架是一种有效的、通用的方法，可以显著提高ViT在图像分类任务中的推理效率，而无需进行额外的训练或修改模型架构。

Abstract: Vision Transformers (ViTs) achieve strong performance in image classification
but incur high computational costs from processing all image tokens. To reduce
inference costs in large ViTs without compromising accuracy, we propose
TinyDrop, a training-free token dropping framework guided by a lightweight
vision model. The guidance model estimates the importance of tokens while
performing inference, thereby selectively discarding low-importance tokens if
large vit models need to perform attention calculations. The framework operates
plug-and-play, requires no architectural modifications, and is compatible with
diverse ViT architectures. Evaluations on standard image classification
benchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs
with minimal accuracy degradation, highlighting its generalization capability
and practical utility for efficient ViT-based classification.

</details>


### [47] [Human Preference-Aligned Concept Customization Benchmark via Decomposed Evaluation](https://arxiv.org/abs/2509.03385)
*Reina Ishikawa,Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: D-GPTScore是一种新颖的人类偏好对齐评估方法，通过分解评估标准并结合多模态大语言模型进行方面评估，解决了现有评估方法与人类偏好不一致的问题。同时，发布了CC-AlignBench基准数据集，用于评估单概念和多概念任务。D-GPTScore在基准测试中表现优于现有方法，与人类偏好具有更高的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在评估概念定制时存在不足，难以全面评估生成图像与提示及概念图像的保真度，并且评估多概念及其交互更加困难，现有指标与人类偏好存在不一致。

Method: 提出D-GPTScore，一种将评估标准分解为更细的方面，并利用多模态大语言模型进行方面评估的人类偏好对齐评估方法。同时，发布CC-AlignBench基准数据集，包含单概念和多概念任务，支持从个体动作到多人交互的广泛难度范围的阶段式评估。

Result: D-GPTScore在CC-AlignBench基准测试中显著优于现有方法，与人类偏好的相关性更高。

Conclusion: D-GPTScore为评估概念定制树立了新标准，并指出了未来研究的关键挑战。

Abstract: Evaluating concept customization is challenging, as it requires a
comprehensive assessment of fidelity to generative prompts and concept images.
Moreover, evaluating multiple concepts is considerably more difficult than
evaluating a single concept, as it demands detailed assessment not only for
each individual concept but also for the interactions among concepts. While
humans can intuitively assess generated images, existing metrics often provide
either overly narrow or overly generalized evaluations, resulting in
misalignment with human preference. To address this, we propose Decomposed GPT
Score (D-GPTScore), a novel human-aligned evaluation method that decomposes
evaluation criteria into finer aspects and incorporates aspect-wise assessments
using Multimodal Large Language Model (MLLM). Additionally, we release Human
Preference-Aligned Concept Customization Benchmark (CC-AlignBench), a benchmark
dataset containing both single- and multi-concept tasks, enabling stage-wise
evaluation across a wide difficulty range -- from individual actions to
multi-person interactions. Our method significantly outperforms existing
approaches on this benchmark, exhibiting higher correlation with human
preferences. This work establishes a new standard for evaluating concept
customization and highlights key challenges for future research. The benchmark
and associated materials are available at
https://github.com/ReinaIshikawa/D-GPTScore.

</details>


### [48] [Scalable and Loosely-Coupled Multimodal Deep Learning for Breast Cancer Subtyping](https://arxiv.org/abs/2509.03408)
*Mohammed Amer,Mohamed A. Suliman,Tu Bui,Nuria Garcia,Serban Georgescu*

Main category: cs.CV

TL;DR: 提出一个可扩展的、松耦合的多模态框架，用于整合拷贝数变异、临床记录和组织病理学图像等多种数据源，以提高乳腺癌分子分型的准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗应用需要整合多源数据，但临床数据的可用性可能因地点和患者而异。乳腺癌分子分型是提高个体化治疗和患者预后的关键任务，可从多模态整合中获益。

Method: 提出一个可扩展的、松耦合的多模态框架，整合拷贝数变异（CNV）、临床记录和组织病理学图像。引入一种用于全切片图像（WSI）的双重表示方法，结合了基于图像和基于图的表示。提出一种新的多模态融合策略。

Result: 所提出的双重WSI表示方法显著提高了性能。新的多模态融合策略在各种多模态条件下均能提升性能。整合了双重WSI表示、CNV和临床记录，连同所提出的流程和融合策略，在乳腺癌分型方面优于现有最先进的方法。

Conclusion: 所提出的框架能够无缝整合多种数据源，并具有良好的可扩展性，可轻松适应其他癌症类型。双重WSI表示和多模态融合策略的结合，在乳腺癌分子分型方面取得了最先进的性能。

Abstract: Healthcare applications are inherently multimodal, benefiting greatly from
the integration of diverse data sources. However, the modalities available in
clinical settings can vary across different locations and patients. A key area
that stands to gain from multimodal integration is breast cancer molecular
subtyping, an important clinical task that can facilitate personalized
treatment and improve patient prognosis. In this work, we propose a scalable
and loosely-coupled multimodal framework that seamlessly integrates data from
various modalities, including copy number variation (CNV), clinical records,
and histopathology images, to enhance breast cancer subtyping. While our
primary focus is on breast cancer, our framework is designed to easily
accommodate additional modalities, offering the flexibility to scale up or down
with minimal overhead without requiring re-training of existing modalities,
making it applicable to other types of cancers as well. We introduce a
dual-based representation for whole slide images (WSIs), combining traditional
image-based and graph-based WSI representations. This novel dual approach
results in significant performance improvements. Moreover, we present a new
multimodal fusion strategy, demonstrating its ability to enhance performance
across a range of multimodal conditions. Our comprehensive results show that
integrating our dual-based WSI representation with CNV and clinical health
records, along with our pipeline and fusion strategy, outperforms
state-of-the-art methods in breast cancer subtyping.

</details>


### [49] [Time-Scaling State-Space Models for Dense Video Captioning](https://arxiv.org/abs/2509.03426)
*AJ Piergiovanni,Ganesh Satish Mallya,Dahun Kim,Anelia Angelova*

Main category: cs.CV

TL;DR: 基于时间缩放状态空间模型（SSM）的视频事件分割与描述生成方法，解决了长视频处理的计算复杂度和内存限制问题，并实现了在线视频处理。


<details>
  <summary>Details</summary>
Motivation: 现有密集视频描述方法难以处理长视频，存在计算复杂、内存限制和无法在线处理的问题。

Method: 提出一种名为“传输状态的状态空间模型”（State-Space Models with Transfer State）的新模型，通过时间缩放SSM来处理更长的序列，并克服了SSM状态维持的限制，使其能够更好地处理长上下文，并适用于在线或流式处理。

Result: 该方法在密集视频描述任务上表现出色，能够很好地处理视频长度问题，并且计算量（FLOPs）减少了7倍。

Conclusion: 所提出的基于传输状态的状态空间模型能够有效地处理长视频，并以在线流式方式生成视频描述，显著提高了效率。

Abstract: Dense video captioning is a challenging video understanding task which aims
to simultaneously segment the video into a sequence of meaningful consecutive
events and to generate detailed captions to accurately describe each event.
Existing methods often encounter difficulties when working with the long videos
associated with dense video captioning, due to the computational complexity and
memory limitations. Furthermore, traditional approaches require the entire
video as input, in order to produce an answer, which precludes online
processing of the video. We address these challenges by time-scaling
State-Space Models (SSMs) to even longer sequences than before. Our approach,
State-Space Models with Transfer State, combines both the long-sequence and
recurrent properties of SSMs and addresses the main limitation of SSMs which
are otherwise not able to sustain their state for very long contexts,
effectively scaling SSMs further in time. The proposed model is particularly
suitable for generating captions on-the-fly, in an online or streaming manner,
without having to wait for the full video to be processed, which is more
beneficial in practice. When applied to dense video captioning, our approach
scales well with video lengths and uses 7x fewer FLOPs.

</details>


### [50] [Decoding Visual Neural Representations by Multimodal with Dynamic Balancing](https://arxiv.org/abs/2509.03433)
*Kaili sun,Xingyu Miao,Bing Zhai,Haoran Duan,Yang Long*

Main category: cs.CV

TL;DR: 该研究提出了一种结合EEG、图像和文本数据的新框架，旨在从低信噪比的EEG信号中解码视觉神经表征。通过引入文本模态来增强EEG信号与视觉内容之间的语义对应关系，并利用适配器模块和模态一致性动态平衡（MCDB）策略来对齐和融合跨模态特征，同时通过随机扰动正则化（SPR）项提高模型的泛化能力。在ThingsEEG数据集上的评估结果显示，该方法在Top-1和Top-5准确率上均优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 为了从低信噪比的EEG信号中解码视觉神经表征，并提高模型在视觉解码任务上的准确性和泛化能力。

Method: 提出了一种集成EEG、图像和文本数据的框架，利用文本模态增强语义对应，设计了适配器模块进行跨模态特征的融合，并引入了模态一致性动态平衡（MCDB）策略和随机扰动正则化（SPR）项来优化模型。

Result: 在ThingsEEG数据集上的评估结果显示，该方法在Top-1和Top-5准确率上分别提高了2.0%和4.7%，优于现有最先进方法。

Conclusion: 所提出的多模态框架在EEG信号解码视觉表征方面取得了显著的改进，证明了结合文本信息和采用特定策略（如MCDB和SPR）的有效性。

Abstract: In this work, we propose an innovative framework that integrates EEG, image,
and text data, aiming to decode visual neural representations from low
signal-to-noise ratio EEG signals. Specifically, we introduce text modality to
enhance the semantic correspondence between EEG signals and visual content.
With the explicit semantic labels provided by text, image and EEG features of
the same category can be more closely aligned with the corresponding text
representations in a shared multimodal space. To fully utilize pre-trained
visual and textual representations, we propose an adapter module that
alleviates the instability of high-dimensional representation while
facilitating the alignment and fusion of cross-modal features. Additionally, to
alleviate the imbalance in multimodal feature contributions introduced by the
textual representations, we propose a Modal Consistency Dynamic Balance (MCDB)
strategy that dynamically adjusts the contribution weights of each modality. We
further propose a stochastic perturbation regularization (SPR) term to enhance
the generalization ability of semantic perturbation-based models by introducing
dynamic Gaussian noise in the modality optimization process. The evaluation
results on the ThingsEEG dataset show that our method surpasses previous
state-of-the-art methods in both Top-1 and Top-5 accuracy metrics, improving by
2.0\% and 4.7\% respectively.

</details>


### [51] [Joint Training of Image Generator and Detector for Road Defect Detection](https://arxiv.org/abs/2509.03465)
*Kuan-Chuan Peng*

Main category: cs.CV

TL;DR: JTGD模型在不使用集成方法或TTA的情况下，在RDD2022基准测试中超越了最先进的方法，并且参数数量减少了80%，更适合边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 为边缘设备设计一种无需集成方法或测试时增强（TTA）即可进行道路缺陷检测的模型。

Method: 提出了一种联合训练图像生成器和检测器的模型（JTGD），设计了双判别器来保证生成图像的真实性，并采用基于CLIP的Fréchet Inception Distance损失来提高生成图像质量，同时让生成器生成更难的样本以训练检测器。

Result: JTGD在RDD2022道路缺陷检测基准测试中，在不使用集成和TTA的条件下，超越了最先进的方法，并且参数数量相比基线模型减少了80%以上。

Conclusion: JTGD模型在保证生成图像质量和难度的同时，有效地提升了道路缺陷检测的性能，并且模型轻量化，非常适合在计算资源有限的边缘设备上部署。

Abstract: Road defect detection is important for road authorities to reduce the vehicle
damage caused by road defects. Considering the practical scenarios where the
defect detectors are typically deployed on edge devices with limited memory and
computational resource, we aim at performing road defect detection without
using ensemble-based methods or test-time augmentation (TTA). To this end, we
propose to Jointly Train the image Generator and Detector for road defect
detection (dubbed as JTGD). We design the dual discriminators for the
generative model to enforce both the synthesized defect patches and overall
images to look plausible. The synthesized image quality is improved by our
proposed CLIP-based Fr\'echet Inception Distance loss. The generative model in
JTGD is trained jointly with the detector to encourage the generative model to
synthesize harder examples for the detector. Since harder synthesized images of
better quality caused by the aforesaid design are used in the data
augmentation, JTGD outperforms the state-of-the-art method in the RDD2022 road
defect detection benchmark across various countries under the condition of no
ensemble and TTA. JTGD only uses less than 20% of the number of parameters
compared with the competing baseline, which makes it more suitable for
deployment on edge devices in practice.

</details>


### [52] [Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual Prompts for NR-IQA](https://arxiv.org/abs/2509.03494)
*Yahya Benmahane,Mohammed El Hassouni*

Main category: cs.CV

TL;DR: 该研究提出了一种新颖的参数高效适应方法，使用在像素空间优化的视觉提示来实现无参考图像质量评估（NR-IQA）。


<details>
  <summary>Details</summary>
Motivation: 为了在不完全微调大型多模态模型（MLLMs）的情况下，实现对NR-IQA任务的高效适应。

Method: 该方法仅训练最多60万个参数（< 0.01%的基础模型），并保持底层模型完全冻结。在推理时，这些视觉提示通过加法与图像结合，并与mPLUG-Owl2和文本查询“Rate the technical quality of the image.”一起处理。

Result: 在KADID-10k、KonIQ-10k和AGIQA-3k数据集上，针对合成、真实和AI生成失真类型进行了评估，结果显示该方法在KADID-10k上达到了0.93的SRCC，性能具有竞争力，可与完全微调的方法和专门的NR-IQA模型相媲美。

Conclusion: 这是首次利用像素空间视觉提示进行NR-IQA的研究，实现了MLLM在低级视觉任务上的高效适应。

Abstract: In this paper, we propose a novel parameter-efficient adaptation method for
No- Reference Image Quality Assessment (NR-IQA) using visual prompts optimized
in pixel-space. Unlike full fine-tuning of Multimodal Large Language Models
(MLLMs), our approach trains only 600K parameters at most (< 0.01% of the base
model), while keeping the underlying model fully frozen. During inference,
these visual prompts are combined with images via addition and processed by
mPLUG-Owl2 with the textual query "Rate the technical quality of the image."
Evaluations across distortion types (synthetic, realistic, AI-generated) on
KADID- 10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against
full finetuned methods and specialized NR-IQA models, achieving 0.93 SRCC on
KADID-10k. To our knowledge, this is the first work to leverage pixel-space
visual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level
vision tasks. The source code is publicly available at https: // github. com/
yahya-ben/ mplug2-vp-for-nriqa .

</details>


### [53] [OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation](https://arxiv.org/abs/2509.03498)
*Han Li,Xinyu Peng,Yaoming Wang,Zelin Peng,Xin Chen,Rongxiang Weng,Jingang Wang,Xunliang Cai,Wenrui Dai,Hongkai Xiong*

Main category: cs.CV

TL;DR: OneCAT是一个统一的多模态模型，采用纯解码器Transformer架构，无需外部视觉组件，实现高效理解、生成和编辑，并在各项多模态任务中取得最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一的多模态模型，能够高效地进行理解、生成和编辑，并克服现有模型对外部组件的依赖以及在高分辨率输入下的效率问题。

Method: 提出了一种新颖的纯解码器Transformer架构，集成了特定模态的专家混合（MoE）结构，并采用单一自回归（AR）目标进行训练。该模型支持动态分辨率，并引入了多尺度视觉自回归机制，以减少解码步骤。

Result: OneCAT在多模态生成、编辑和理解方面取得了最先进的性能，超越了现有的开源统一多模态模型，尤其在高分辨率输入下表现出显著的效率提升。

Conclusion: 纯粹的自回归建模足以作为统一多模态智能的强大且优雅的基础，OneCAT证明了这种方法的潜力。

Abstract: We introduce OneCAT, a unified multimodal model that seamlessly integrates
understanding, generation, and editing within a novel, pure decoder-only
transformer architecture. Our framework uniquely eliminates the need for
external components such as Vision Transformers (ViT) or vision tokenizer
during inference, leading to significant efficiency gains, especially for
high-resolution inputs. This is achieved through a modality-specific
Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR)
objective, which also natively supports dynamic resolutions. Furthermore, we
pioneer a multi-scale visual autoregressive mechanism within the Large Language
Model (LLM) that drastically reduces decoding steps compared to diffusion-based
methods while maintaining state-of-the-art performance. Our findings
demonstrate the powerful potential of pure autoregressive modeling as a
sufficient and elegant foundation for unified multimodal intelligence. As a
result, OneCAT sets a new performance standard, outperforming existing
open-source unified multimodal models across benchmarks for multimodal
generation, editing, and understanding.

</details>


### [54] [DeepSea MOT: A benchmark dataset for multi-object tracking on deep-sea video](https://arxiv.org/abs/2509.03499)
*Kevin Barnard,Elaine Liu,Kristine Walz,Brian Schlining,Nancy Jacobsen Stout,Lonny Lundsten*

Main category: cs.CV

TL;DR: 该研究开发了一个新的多目标跟踪基准数据集，用于评估深海视频中目标检测和跟踪模型的性能。


<details>
  <summary>Details</summary>
Motivation: 评估机器学习模型（特别是目标检测和跟踪模型）在人类生成的“测试”数据上的性能，以实现模型和跟踪器之间的一致比较，并优化性能。

Method: 开发了一个包含四个代表性深海栖息地（中层和底栖）的视频序列的新基准数据集。使用“高阶跟踪精度”（Higher Order Tracking Accuracy）这一指标来评估性能，该指标综合考虑了检测、定位和关联的准确性。

Result: 提供了一个用于深海视频多目标跟踪的基准数据集、一个记录清晰的生成其他基准视频的工作流程，以及用于计算指标的示例 Python 笔记本。

Conclusion: 该研究发布了首个公开可用的深海视频多目标跟踪基准数据集，并提供了评估工具，以促进该领域的模型开发和性能优化。

Abstract: Benchmarking multi-object tracking and object detection model performance is
an essential step in machine learning model development, as it allows
researchers to evaluate model detection and tracker performance on
human-generated 'test' data, facilitating consistent comparisons between models
and trackers and aiding performance optimization. In this study, a novel
benchmark video dataset was developed and used to assess the performance of
several Monterey Bay Aquarium Research Institute object detection models and a
FathomNet single-class object detection model together with several trackers.
The dataset consists of four video sequences representing midwater and benthic
deep-sea habitats. Performance was evaluated using Higher Order Tracking
Accuracy, a metric that balances detection, localization, and association
accuracy. To the best of our knowledge, this is the first publicly available
benchmark for multi-object tracking in deep-sea video footage. We provide the
benchmark data, a clearly documented workflow for generating additional
benchmark videos, as well as example Python notebooks for computing metrics.

</details>


### [55] [Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data](https://arxiv.org/abs/2509.03501)
*Honglu Zhou,Xiangyu Peng,Shrikant Kendre,Michael S. Ryoo,Silvio Savarese,Caiming Xiong,Juan Carlos Niebles*

Main category: cs.CV

TL;DR: Strefer 是一个用于提升视频大语言模型 (Video LLM) 时空指代和推理能力的合成指令数据生成框架，解决了现有模型在处理精细时空推理和用户查询中的不足，并通过伪标注生成包含丰富时空信息的结构化元数据，实验证明 Strefer 训练的模型在需要时空消歧的任务上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视频大语言模型 (Video LLM) 在理解动态真实世界环境中的空间和时间参考方面存在不足，尤其是在用户查询依赖时间事件或手势线索进行时空定位时，这对于下一代 AI 伴侣至关重要。

Method: Strefer 通过一个数据引擎生成多样化的指令调优数据。该引擎对视频进行伪标注，提取密集、精细的视频元数据，包括主体、对象、它们的位置（以掩码的形式）以及它们的动作描述和时间线，从而捕捉丰富的空间和时间信息。

Result: 使用 Strefer 生成的数据进行训练的模型，在需要空间和时间消歧的任务上，其表现优于基线模型。这些模型还展现了增强的时空感知推理能力。

Conclusion: Strefer 框架能够有效地提升视频大语言模型处理时空信息的能力，为构建更通用的、基于感知的、经过指令调优的视频大语言模型奠定了新基础，而无需依赖专有模型或昂贵的人工标注。

Abstract: Next-generation AI companions must go beyond general video understanding to
resolve spatial and temporal references in dynamic, real-world environments.
Existing Video Large Language Models (Video LLMs), while capable of
coarse-level comprehension, struggle with fine-grained, spatiotemporal
reasoning, especially when user queries rely on time-based event references for
temporal anchoring, or gestural cues for spatial anchoring to clarify object
references and positions. To bridge this critical gap, we introduce Strefer, a
synthetic instruction data generation framework designed to equip Video LLMs
with spatiotemporal referring and reasoning capabilities. Strefer produces
diverse instruction-tuning data using a data engine that pseudo-annotates
temporally dense, fine-grained video metadata, capturing rich spatial and
temporal information in a structured manner, including subjects, objects, their
locations as masklets, and their action descriptions and timelines. Our
approach enhances the ability of Video LLMs to interpret spatial and temporal
references, fostering more versatile, space-time-aware reasoning essential for
real-world AI companions. Without using proprietary models, costly human
annotation, or the need to annotate large volumes of new videos, experimental
evaluations show that models trained with data produced by Strefer outperform
baselines on tasks requiring spatial and temporal disambiguation. Additionally,
these models exhibit enhanced space-time-aware reasoning, establishing a new
foundation for perceptually grounded, instruction-tuned Video LLMs.

</details>


### [56] [A comprehensive Persian offline handwritten database for investigating the effects of heritability and family relationships on handwriting](https://arxiv.org/abs/2509.03510)
*Abbas Zohrevand,Javad Sadri,Zahra Imani*

Main category: cs.CV

TL;DR: 该数据库旨在研究遗传对笔迹的影响，收集了210个家庭的笔迹样本，旨在探索遗传和家庭关系对笔迹的作用。


<details>
  <summary>Details</summary>
Motivation: 为了研究遗传对笔迹的影响，特别是笔迹是否受遗传因素影响，以及家庭关系如何影响笔迹。

Method: 收集了210个家庭（包括祖父母、父母、叔姨、兄弟姐妹、堂/表兄弟姐妹、侄/甥）的数字、字母、图形和自由段落等多种手写样本，并记录了所有书写者的家庭关系。通过比较分析家庭成员的笔迹特征和书写风格的相似性。

Result: 检测到家庭成员的笔迹特征和书写风格存在相似性。

Conclusion: 创建了一个全面的笔迹遗传效应研究数据库，并免费提供给模式识别社区，以期推动相关领域的研究。

Abstract: This paper introduces a comprehensive database for research and investigation
on the effects of inheritance on handwriting. A database has been created that
can be used to answer questions such as: Is there a genetic component to
handwriting? Is handwriting inherited? Do family relationships affect
handwriting? Varieties of samples of handwritten components such as: digits,
letters, shapes and free paragraphs of 210 families including (grandparents,
parents, uncles, aunts, siblings, cousins, nephews and nieces) have been
collected using specially designed forms, and family relationships of all
writers are captured. To the best of our knowledge, no such database is
presently available. Based on comparisons and investigation of features of
handwritings of family members, similarities among their features and writing
styles are detected. Our database is freely available to the pattern
recognition community and hope it will pave the way for investigations on the
effects of inheritance and family relationships on handwritings.

</details>


### [57] [Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?](https://arxiv.org/abs/2509.03516)
*Ouxiang Li,Yuan Wang,Xinting Hu,Huijuan Huang,Rui Chen,Jiarong Ou,Xin Tao,Pengfei Wan,Fuli Feng*

Main category: cs.CV

TL;DR: T2I-CoReBench 是一个全面的文本到图像生成基准，用于评估 T2I 模型中的组合和推理能力，具有高场景密度和多步推理的挑战性提示，以及用于精细评估的清单问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像（T2I）生成基准在评估 T2I 模型中的组合和推理能力方面存在局限性，特别是在处理复杂提示和细微推理方面。T2I-CoReBench 的目的是创建一个更全面、更复杂的基准，以解决这些不足。

Method: T2I-CoReBench 基准包含 1,080 个具有高组合密度和多步推理的挑战性提示，并为每个提示提供了清单问题，用于评估 12 个维度的组合和推理能力。该基准还涵盖了演绎、归纳和溯因推理。

Result: 在 27 个 T2I 模型上的实验表明，它们的组合能力在高密度场景中仍然有限，而推理能力则滞后，所有模型在从提示中推断隐含元素方面都存在困难。

Conclusion: T2I 模型在组合和推理方面仍有很大的改进空间，尤其是在处理复杂场景和执行多步推理方面。T2I-CoReBench 基准为了解和推动 T2I 模型的进一步发展提供了宝贵的资源。

Abstract: Text-to-image (T2I) generation aims to synthesize images from textual
prompts, which jointly specify what must be shown and imply what can be
inferred, thereby corresponding to two core capabilities: composition and
reasoning. However, with the emerging advances of T2I models in reasoning
beyond composition, existing benchmarks reveal clear limitations in providing
comprehensive evaluations across and within these capabilities. Meanwhile,
these advances also enable models to handle more complex prompts, whereas
current benchmarks remain limited to low scene density and simplified
one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a
comprehensive and complex benchmark that evaluates both composition and
reasoning capabilities of T2I models. To ensure comprehensiveness, we structure
composition around scene graph elements (instance, attribute, and relation) and
reasoning around the philosophical framework of inference (deductive,
inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To
increase complexity, driven by the inherent complexities of real-world
scenarios, we curate each prompt with high compositional density for
composition and multi-step inference for reasoning. We also pair each prompt
with a checklist that specifies individual yes/no questions to assess each
intended element independently to facilitate fine-grained and reliable
evaluation. In statistics, our benchmark comprises 1,080 challenging prompts
and around 13,500 checklist questions. Experiments across 27 current T2I models
reveal that their composition capability still remains limited in complex
high-density scenarios, while the reasoning capability lags even further behind
as a critical bottleneck, with all models struggling to infer implicit elements
from prompts. Our project page: https://t2i-corebench.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [58] [DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for Breaking the Efficiency-Quality Trade-off](https://arxiv.org/abs/2509.02785)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Zimeng Huang,Xiaofei Sun,Jian Wang,Chengpei Tang,Keze Wang*

Main category: cs.CL

TL;DR: DrDiff是一个用于长文本生成的新框架，通过动态专家调度、分层稀疏注意力（HSA）和软吸收引导优化策略，在效率和质量之间取得了更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决长文本生成中效率和质量之间的权衡问题。

Method: 1. 动态专家调度机制：根据文本复杂度智能分配计算资源。 2. 分层稀疏注意力（HSA）机制：自适应调整注意力模式，将计算复杂度从O(n^2)降低到O(n)。 3. 软吸收引导优化策略：结合DPM-solver++，减少扩散步数。

Result: 在多个长文本生成基准测试上，DrDiff的性能优于现有最先进的方法。

Conclusion: DrDiff通过其创新的技术有效地提高了长文本生成的效率和质量。

Abstract: This paper introduces DrDiff, a novel framework for long-text generation that
overcomes the efficiency-quality trade-off through three core technologies.
First, we design a dynamic expert scheduling mechanism that intelligently
allocates computational resources during the diffusion process based on text
complexity, enabling more efficient handling of text generation tasks of
varying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA)
mechanism that adaptively adjusts attention patterns according to a variety of
input lengths, reducing computational complexity from O($n^2$) to O($n$) while
maintaining model performance. Finally, we propose a soft absorption guidance
optimization strategy that combines with DPM-solver++ to reduce diffusion
steps, significantly improving generation speed. Comprehensive experiments on
various long-text generation benchmarks demonstrate the superiority of our
DrDiff over the existing SOTA methods.

</details>


### [59] [SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR](https://arxiv.org/abs/2509.02830)
*Pu Wang,Shinji Watanabe,Hugo Van hamme*

Main category: cs.CL

TL;DR: 本研究首次全面评估了多种参数高效微调（PEFT）方法（如VeRA、DoRA、PiSSA、SVFT）在语音识别任务中的表现，并提出了一种新的SSVD微调方法，该方法通过选择性地旋转输入相关的奇异向量来优化领域适应性，同时保持参数量最少并提高效率。


<details>
  <summary>Details</summary>
Motivation: 在语音识别领域，虽然LoRA被广泛使用，但其先进的变体（如VeRA、DoRA、PiSSA、SVFT）主要针对语言和视觉任务开发，在语音任务中的验证有限。因此，需要对这些PEFT方法在语音任务中的适用性进行全面评估和改进。

Method: 在ESPnet框架中，对VeRA、DoRA、PiSSA、SVFT等PEFT方法进行了集成和基准测试。此外，提出了一种新的结构化SVD引导（SSVD）微调方法，该方法选择性地旋转与输入相关的右奇异向量，同时固定与输出相关的奇异向量，以保留语义映射。

Result: 在儿童语音和方言语音识别的领域迁移任务上，对不同模型规模（0.1B至2B）的所有方法进行了评估。实验结果表明，SSVD方法在领域适应性方面表现稳健，并且训练参数最少，效率更高。

Conclusion: 本研究通过在ESPnet中集成和评估多种PEFT方法，并提出SSVD微调方法，为语音识别领域的参数高效微调提供了新的见解和工具，特别是在处理领域迁移任务时，SSVD方法展现出优越的性能和效率。所有实现均已开源，以支持可重复性和未来的研究。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution for
adapting large foundation models. While low-rank adaptation (LoRA) is widely
used in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA,
PiSSA, and SVFT, are developed mainly for language and vision tasks, with
limited validation in speech. This work presents the first comprehensive
integration and benchmarking of these PEFT methods within ESPnet. We further
introduce structured SVD-guided (SSVD) fine-tuning, which selectively rotates
input-associated right singular vectors while keeping output-associated vectors
fixed to preserve semantic mappings. This design enables robust domain
adaptation with minimal trainable parameters and improved efficiency. We
evaluate all methods on domain-shifted speech recognition tasks, including
child speech and dialectal variation, across model scales from 0.1B to 2B. All
implementations are released in ESPnet to support reproducibility and future
work.

</details>


### [60] [Clustering Discourses: Racial Biases in Short Stories about Women Generated by Large Language Models](https://arxiv.org/abs/2509.02834)
*Gustavo Bonil,João Gondim,Marina dos Santos,Simone Hashiguti,Helena Maia,Nadia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: LLaMA 3.2-3B模型在葡萄牙语短篇故事中对黑白女性的叙述分析显示，模型在看似中立的文本中可能固化殖民主义的身体观，加剧历史不平等。研究结合了机器学习和定性话语分析。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究大型语言模型（特别是 LLaMA 3.2-3B）在生成的葡萄牙语短篇故事中，如何构建关于黑人女性和白人女性的叙事，并揭示其中潜在的社会文化偏见。

Method: 通过对 2100 篇文本进行计算方法分组，选出具有语义相似性的故事进行定性分析。研究结合了机器学习技术和手动定性话语分析。

Result: 研究发现了三种主要的叙事模式：社会克服、祖先神话化和主观自我实现。分析表明，即使在语法连贯、看似中立的文本中，也可能体现出固化的、殖民主义结构下的女性身体意象，从而加剧了历史不平等。

Conclusion: 大型语言模型在生成叙事时，可能无意中再现并强化了历史上的不平等和殖民主义观念，尤其是在对女性身体的描绘上。本研究提出的综合方法有助于更深入地理解和识别这些偏见。

Abstract: This study investigates how large language models, in particular LLaMA
3.2-3B, construct narratives about Black and white women in short stories
generated in Portuguese. From 2100 texts, we applied computational methods to
group semantically similar stories, allowing a selection for qualitative
analysis. Three main discursive representations emerge: social overcoming,
ancestral mythification and subjective self-realization. The analysis uncovers
how grammatically coherent, seemingly neutral texts materialize a crystallized,
colonially structured framing of the female body, reinforcing historical
inequalities. The study proposes an integrated approach, that combines machine
learning techniques with qualitative, manual discourse analysis.

</details>


### [61] [IDEAlign: Comparing Large Language Models to Human Experts in Open-ended Interpretive Annotations](https://arxiv.org/abs/2509.02855)
*Hyunji Nam,Lucia Langlois,James Malamut,Mei Tan,Dorottya Demszky*

Main category: cs.CL

TL;DR: 该研究提出了IDEAlgin，一个用于评估大型语言模型（LLM）在开放式、解释性标注任务中表现的基准测试范式，并发现基于IDEAlgin的LLM评估比传统向量方法更能捕捉专家判断的细微差别。


<details>
  <summary>Details</summary>
Motivation: 评估LLM生成的解释性标注（如主题分析、学生作业反馈）与专家人类标注者的一致性，在规模上具有挑战性，目前缺乏有效的、可扩展的衡量标准。

Method: 提出IDEAlgin基准测试范式，采用“挑出不同类”的三元组判断任务来捕捉专家相似性评分。然后，将各种相似性度量（包括基于向量的度量和通过IDEAlgin进行评估的LLM-as-a-judge）与人类基准进行比较。

Result: 在两个真实的教育数据集（解释性分析和反馈生成）上，发现基于向量的度量在很大程度上未能捕捉到专家认为有意义的相似性的细微维度。通过IDEAlgin提示LLM能显著提高与专家判断的一致性（提高9-30%），优于传统的词汇和基于向量的度量。

Conclusion: IDEAlgin是一个有前途的范式，可以大规模地根据开放式专家标注来评估LLM，为LLM在教育及其他领域的负责任部署提供信息。

Abstract: Large language models (LLMs) are increasingly applied to open-ended,
interpretive annotation tasks, such as thematic analysis by researchers or
generating feedback on student work by teachers. These tasks involve free-text
annotations requiring expert-level judgments grounded in specific objectives
(e.g., research questions or instructional goals). Evaluating whether
LLM-generated annotations align with those generated by expert humans is
challenging to do at scale, and currently, no validated, scalable measure of
similarity in ideas exists. In this paper, we (i) introduce the scalable
evaluation of interpretive annotation by LLMs as a critical and understudied
task, (ii) propose IDEAlgin, an intuitive benchmarking paradigm for capturing
expert similarity ratings via a "pick-the-odd-one-out" triplet judgment task,
and (iii) evaluate various similarity metrics, including vector-based ones
(topic models, embeddings) and LLM-as-a-judge via IDEAlgin, against these human
benchmarks. Applying this approach to two real-world educational datasets
(interpretive analysis and feedback generation), we find that vector-based
metrics largely fail to capture the nuanced dimensions of similarity meaningful
to experts. Prompting LLMs via IDEAlgin significantly improves alignment with
expert judgments (9-30% increase) compared to traditional lexical and
vector-based metrics. These results establish IDEAlgin as a promising paradigm
for evaluating LLMs against open-ended expert annotations at scale, informing
responsible deployment of LLMs in education and beyond.

</details>


### [62] [A-SEA3L-QA: A Fully Automated Self-Evolving, Adversarial Workflow for Arabic Long-Context Question-Answer Generation](https://arxiv.org/abs/2509.02864)
*Kesen Wang,Daulet Toibazar,Pedro J. Moreno*

Main category: cs.CL

TL;DR: 提出了一种端到端的、自进化的对抗性工作流程，用于阿拉伯语的长上下文问答生成，通过组合多个专业的LVLM（问题生成器、评估器和答案生成器群），在无人干预的情况下迭代改进性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够处理长上下文阿拉伯语文档并自动生成高质量问答的系统，以提高阿拉伯语大型视觉语言模型（LVLM）的长上下文理解能力。

Method: 构建了一个端到端的自进化对抗性工作流程，包括问题生成器、评估器和答案生成器群。该系统通过闭环学习，利用评估器的反馈来迭代优化性能，并能够根据可调的质量指标来控制问题的难度。

Result: 所提出的自进化工作流程在AraLongBench基准测试中，相比静态流水线，显著提高了长上下文问答生成的能力，并展示了其在长上下文阿拉伯语文档收集方面的自动化代理工作流程。

Conclusion: 该自进化工作流程能够有效提升阿拉伯语LVLM的长上下文问答生成和理解能力，并且可以通过调整质量指标来控制问题的难度，同时能够实现文档收集的自动化。

Abstract: We present an end-to-end, self-evolving adversarial workflow for long-context
Question-Answer (QA) Generation in Arabic. By orchestrating multiple
specialized LVLMs: a question generator, an evaluator, and a swarm of answer
generators, our system iteratively refines its own performance without any
human intervention. Starting from raw, multi-page Arabic documents across
diverse domains, the question generator produces fine-grained, context-aware
queries to be tackled by the answer generator swarm, and the evaluator assesses
and feeds back quality metrics. This closed-loop cycle enables continuous
learning: low-confidence outputs trigger automated re-generation and model
updates, progressively enhancing question difficulty and relevance. Moreover,
we set the quality metrics as a tunable hyperparameter, enabling question
generation at controllable and customizable difficulty levels. We release
AraLongBench, a large-scale Arabic benchmark of single- and multi-page
challenges spanning hundreds of pages, and demonstrate that our self-evolving
workflow substantially outperform static pipelines, markedly boosting the
long-context comprehension capabilities of leading Arabic Large Vision Language
Models (LVLMs). Lastly, we also meticulously architect a fully automated
agentic workflow for long-context Arabic document collection.

</details>


### [63] [Advancing Minority Stress Detection with Transformers: Insights from the Social Media Datasets](https://arxiv.org/abs/2509.02908)
*Santosh Chapagain,Cory J Cascalheira,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi,Jillian R. Scheer*

Main category: cs.CL

TL;DR: 本研究首次全面评估了基于 transformer 的架构在检测网络话语中的少数群体压力方面的能力，并与传统机器学习方法进行了比较。研究结果表明，结合图结构可以提高检测性能，并且在关系背景下进行监督微调优于零样本和少样本方法。


<details>
  <summary>Details</summary>
Motivation: 少数群体（性少数和性别少数群体）的健康状况和心理健康问题普遍比异性恋和顺性别个体更差，这主要是由少数群体压力引起的。本研究旨在评估 transformer 模型在检测网络话语中的少数群体压力方面的能力。

Method: 研究人员评估了 ELECTRA、BERT、RoBERTa 和 BART 等多种 transformer 模型，并将它们与传统的机器学习基线模型以及图增强变体进行了比较。此外，还评估了零样本和少样本学习方法在代表性不足的数据集上的适用性。实验在两个最大的公开可用 Reddit 语料库上进行，并重复进行了五次随机种子实验以确保结果的稳健性。

Result: 结果表明，将图结构集成到 transformer 模型中能够一致地提高检测性能。在关系背景下进行监督微调优于零样本和少样本方法。理论分析表明，通过图增强对社交联系和对话背景进行建模，可以提高模型识别关键语言标记（如身份隐藏、内化污名和寻求支持的呼吁）的能力。

Conclusion: 研究得出结论，图增强的 transformer 模型为数字健康干预和公共卫生政策提供了最可靠的基础。

Abstract: Individuals from sexual and gender minority groups experience
disproportionately high rates of poor health outcomes and mental disorders
compared to their heterosexual and cisgender counterparts, largely as a
consequence of minority stress as described by Meyer's (2003) model. This study
presents the first comprehensive evaluation of transformer-based architectures
for detecting minority stress in online discourse. We benchmark multiple
transformer models including ELECTRA, BERT, RoBERTa, and BART against
traditional machine learning baselines and graph-augmented variants. We further
assess zero-shot and few-shot learning paradigms to assess their applicability
on underrepresented datasets. Experiments are conducted on the two largest
publicly available Reddit corpora for minority stress detection, comprising
12,645 and 5,789 posts, and are repeated over five random seeds to ensure
robustness. Our results demonstrate that integrating graph structure
consistently improves detection performance across transformer-only models and
that supervised fine-tuning with relational context outperforms zero and
few-shot approaches. Theoretical analysis reveals that modeling social
connectivity and conversational context via graph augmentation sharpens the
models' ability to identify key linguistic markers such as identity
concealment, internalized stigma, and calls for support, suggesting that
graph-enhanced transformers offer the most reliable foundation for digital
health interventions and public health policy.

</details>


### [64] [AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?](https://arxiv.org/abs/2509.03312)
*Guibin Zhang,Junhao Wang,Junjie Chen,Wangchunshu Zhou,Kun Wang,Shuicheng Yan*

Main category: cs.CL

TL;DR: LLM代理系统虽强大但脆弱，AgenTracer框架通过反事实回放和故障注入，创建了TracerTraj数据集，并训练了AgenTracer-8B模型，该模型在失败归因任务上超越了现有大型模型，并能为其他多代理系统提供可操作的反馈，实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）驱动的代理系统虽然在性能上超越了单一模型，但其复杂性也导致了系统的脆弱性，容易出现故障。现有的方法在定位这些复杂系统中错误的具体原因时效率低下，准确率普遍低于10%。

Method: 提出AgenTracer框架，该框架利用反事实回放和程序化故障注入来标注失败的多代理轨迹，并生成了TracerTraj数据集。基于此数据集，开发了AgenTracer-8B模型，该模型采用多粒度强化学习进行训练，能够高效诊断多代理交互中的错误。

Result: 在Who&When基准测试中，AgenTracer-8B在LLM代理失败归因任务上的表现优于Gemini-2.5-Pro和Claude-4-Sonnet等大型专有LLM，准确率提升高达18.18%。此外，AgenTracer-8B能够为MetaGPT和MaAS等现成的多代理系统提供可操作的反馈，带来4.8%-14.2%的性能提升。

Conclusion: AgenTracer-8B在LLM代理失败归因方面设定了新的标准，并且能够通过提供可操作的反馈来改进现有的多代理系统，从而实现自我纠正和自我进化。

Abstract: Large Language Model (LLM)-based agentic systems, often comprising multiple
models, complex tool invocations, and orchestration protocols, substantially
outperform monolithic agents. Yet this very sophistication amplifies their
fragility, making them more prone to system failure. Pinpointing the specific
agent or step responsible for an error within long execution traces defines the
task of agentic system failure attribution. Current state-of-the-art reasoning
LLMs, however, remain strikingly inadequate for this challenge, with accuracy
generally below 10%. To address this gap, we propose AgenTracer, the first
automated framework for annotating failed multi-agent trajectories via
counterfactual replay and programmed fault injection, producing the curated
dataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a
lightweight failure tracer trained with multi-granular reinforcement learning,
capable of efficiently diagnosing errors in verbose multi-agent interactions.
On the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs
like Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard
in LLM agentic failure attribution. More importantly, AgenTracer-8B delivers
actionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS
with 4.8-14.2% performance gains, empowering self-correcting and self-evolving
agentic AI.

</details>


### [65] [English Pronunciation Evaluation without Complex Joint Training: LoRA Fine-tuned Speech Multimodal LLM](https://arxiv.org/abs/2509.02915)
*Taekyung Ahn,Hosung Nam*

Main category: cs.CL

TL;DR: 通过低秩适配(LoRA)微调多模态大语言模型(MLLM)，可以同时实现自动发音评估(APA)和错误发音检测与诊断(MDD)，无需复杂的架构修改或单独的训练过程。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够同时执行自动发音评估(APA)和错误发音检测与诊断(MDD)的集成化模型，并简化训练流程，提高计算机辅助发音训练(CAPT)技术的效率和可及性。

Method: 使用微软的Phi-4-multimodal-instruct模型，并通过低秩适配(LoRA)技术在Speechocean762数据集上进行微调，仅训练LoRA层。

Result: 模型在发音评估方面取得了高于0.7的皮尔逊相关系数(PCC)，词错误率(WER)和音素错误率(PER)低于0.15，性能与微调所有音频层相当。

Conclusion: 基于LoRA的适配方法能够有效地将大型多模态模型应用于集成化的发音评估任务，为英语L2学习者提供更简洁、更高效的CAPT技术。

Abstract: This study demonstrates that a Multimodal Large Language Model (MLLM) adapted
via Low-Rank Adaptation (LoRA) can perform both Automatic Pronunciation
Assessment (APA) and Mispronunciation Detection and Diagnosis (MDD)
simultaneously. Leveraging Microsoft's Phi-4-multimodal-instruct, our
fine-tuning method eliminates the need for complex architectural changes or
separate training procedures conventionally required for these distinct tasks.
Fine-tuned on the Speechocean762 dataset, the pronunciation evaluation scores
predicted by the model exhibited a strong Pearson Correlation Coefficient (PCC
> 0.7) with human-assigned scores, while achieving low Word Error Rate (WER)
and Phoneme Error Rate (PER) (both < 0.15). Notably, fine-tuning only the LoRA
layers was sufficient to achieve performance levels comparable to those
achieved by fine-tuning all audio layers. This research highlights that an
integrated pronunciation assessment system can be established by adapting large
multimodal models without full fine-tuning, utilizing a significantly simpler
training methodology compared to previous joint models designed for
simultaneous APA and MDD. This efficient LoRA-based approach paves the way for
more accessible, integrated, and effective Computer-Assisted Pronunciation
Training (CAPT) technologies for English L2 learners.

</details>


### [66] [Decoding the Rule Book: Extracting Hidden Moderation Criteria from Reddit Communities](https://arxiv.org/abs/2509.02926)
*Youngwoo Kim,Himanshu Beniwal,Steven L. Johnson,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 本文提出了一种从历史审核数据中提取隐式审核标准的新方法，通过构建词汇表达式评分表来量化审核标准，实验证明该方法能有效复制神经网络审核模型的性能，并提供透明的决策过程洞察，揭示了不同社区在执行看似相同的规范时存在的显著差异。


<details>
  <summary>Details</summary>
Motivation: 在线社区（如子版块）的审核系统通常依赖于多样化且隐晦的标准，而有效的审核系统需要明确的分类标准。

Method: 提出了一种可解释的架构，将审核标准表示为与内容移除相关联的词汇表达式的评分表，以便跨不同社区进行系统比较。

Result: 实验表明，提取的词汇模式能够有效地复制神经网络审核模型的性能，并提供决策过程的透明化洞察。生成的标准矩阵揭示了看似共同的规范在执行方式上存在显著差异，并发现了包括社区对语言的特定容忍度、主题限制的特征以及有毒言论分类的潜在子类别在内的、之前未被记录的审核模式。

Conclusion: 通过提取的词汇模式，可以有效地复制神经网络审核模型的性能，并提供透明的决策过程洞察，揭示了不同社区在执行看似相同的规范时存在的显著差异，并发现了新的审核模式。

Abstract: Effective content moderation systems require explicit classification
criteria, yet online communities like subreddits often operate with diverse,
implicit standards. This work introduces a novel approach to identify and
extract these implicit criteria from historical moderation data using an
interpretable architecture. We represent moderation criteria as score tables of
lexical expressions associated with content removal, enabling systematic
comparison across different communities. Our experiments demonstrate that these
extracted lexical patterns effectively replicate the performance of neural
moderation models while providing transparent insights into decision-making
processes. The resulting criteria matrix reveals significant variations in how
seemingly shared norms are actually enforced, uncovering previously
undocumented moderation patterns including community-specific tolerances for
language, features for topical restrictions, and underlying subcategories of
the toxic speech classification.

</details>


### [67] [ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly](https://arxiv.org/abs/2509.02949)
*Kimihiro Hasegawa,Wiradee Imrattanatrai,Masaki Asada,Susan Holm,Yuran Wang,Vincent Zhou,Ken Fukuda,Teruko Mitamura*

Main category: cs.CL

TL;DR: 该论文提出了一个名为 ProMQA-Assembly 的新数据集，用于评估装配任务中的多模态问答系统。该数据集包含 391 对问答，结合了人类活动录像和说明手册，并采用半自动化的方式进行标注。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏用于评估面向应用的装配任务系统（尤其是在装配领域）的测试平台，这阻碍了相关技术的发展。

Method: 提出 ProMQA-Assembly 数据集，包含 391 对多模态问答对，结合了人类活动录像和说明手册。采用半自动化的 QA 标注方法，利用大型语言模型生成候选问题，并由人类进行验证，同时引入细粒度的动作标签以丰富问题类型。为玩具车辆组装任务创建了指令任务图，用于基准测试和标注过程。

Result: 使用该数据集对包括专有模型在内的多模态模型进行了基准测试，结果表明现有模型仍有很大改进空间。

Conclusion: ProMQA-Assembly 数据集有望推动程序性活动助手的发展。

Abstract: Assistants on assembly tasks have a large potential to benefit humans from
everyday tasks to industrial settings. However, no testbeds support
application-oriented system evaluation in a practical setting, especially in
assembly. To foster the development, we propose a new multimodal QA dataset on
assembly activities. Our dataset, ProMQA-Assembly, consists of 391 QA pairs
that require the multimodal understanding of human-activity recordings and
their instruction manuals in an online-style manner. In the development, we
adopt a semi-automated QA annotation approach, where LLMs generate candidates
and humans verify them, as a cost-effective method, and further improve it by
integrating fine-grained action labels to diversify question types.
Furthermore, we create instruction task graphs for the target tasks of
assembling toy vehicles. These newly created task graphs are used in our
benchmarking experiment, as well as to facilitate the human verification
process in the QA annotation. Utilizing our dataset, we benchmark models,
including competitive proprietary multimodal models. Our results suggest great
room for improvement for the current models. We believe our new evaluation
dataset can contribute to the further development of procedural-activity
assistants.

</details>


### [68] [DiaCBT: A Long-Periodic Dialogue Corpus Guided by Cognitive Conceptualization Diagram for CBT-based Psychological Counseling](https://arxiv.org/abs/2509.02999)
*Yougen Zhou,Ningning Zhou,Qin Chen,Jie Zhou,Aimin Zhou,Liang He*

Main category: cs.CL

TL;DR: LLM在心理治疗中的应用面临数据稀疏性挑战，本文构建了一个基于CBT的心理咨询长周期对话语料库DiaCBT，并训练了相应的模型，旨在提升LLM的心理咨询能力。


<details>
  <summary>Details</summary>
Motivation: 由于社会污名和治疗师数量有限，心理治疗服务的可及性较低。LLM有潜力解决这一问题，但缺乏相关的心理对话数据集。

Method: 构建了一个包含多轮对话和认知概念图（CCDs）的CBT咨询对话数据集，并利用该数据集训练了一个深度咨询模型，最后提出了一个评估框架来评估模型。

Result: DiaCBT数据集能有效提升LLM模仿CBT心理治疗师的能力。

Conclusion: 本文构建的DiaCBT数据集为开发更专业的心理咨询LLM代理提供了有力支持。

Abstract: Psychotherapy reaches only a small fraction of individuals suffering from
mental disorders due to social stigma and the limited availability of
therapists. Large language models (LLMs), when equipped with professional
psychotherapeutic skills, offer a promising solution to expand access to mental
health services. However, the lack of psychological conversation datasets
presents significant challenges in developing effective psychotherapy-guided
conversational agents. In this paper, we construct a long-periodic dialogue
corpus for counseling based on cognitive behavioral therapy (CBT). Our curated
dataset includes multiple sessions for each counseling and incorporates
cognitive conceptualization diagrams (CCDs) to guide client simulation across
diverse scenarios. To evaluate the utility of our dataset, we train an in-depth
counseling model and present a comprehensive evaluation framework to benchmark
it against established psychological criteria for CBT-based counseling. Results
demonstrate that DiaCBT effectively enhances LLMs' ability to emulate
psychologists with CBT expertise, underscoring its potential for training more
professional counseling agents.

</details>


### [69] [Mitigating Data Imbalance in Automated Speaking Assessment](https://arxiv.org/abs/2509.03010)
*Fong-Chun Tsai,Kuan-Tang Huang,Bi-Cheng Yan,Tien-Hong Lo,Berlin Chen*

Main category: cs.CL

TL;DR: BLV loss can address class imbalance in Automated Speaking Assessment (ASA) models by perturbing predictions to improve minority class representation, enhancing accuracy and fairness without dataset modification.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in Automated Speaking Assessment (ASA) models leads to biased predictions.

Method: Introducing a novel Balancing Logit Variation (BLV) loss function that perturbs model predictions to enhance feature representation for minority classes.

Result: Integrating BLV loss into a BERT model significantly improved classification accuracy and fairness on the ICNALE benchmark dataset.

Conclusion: The BLV loss makes automated speech evaluation more robust for diverse learners by improving accuracy and fairness in the presence of class imbalance.

Abstract: Automated Speaking Assessment (ASA) plays a crucial role in evaluating
second-language (L2) learners proficiency. However, ASA models often suffer
from class imbalance, leading to biased predictions. To address this, we
introduce a novel objective for training ASA models, dubbed the Balancing Logit
Variation (BLV) loss, which perturbs model predictions to improve feature
representation for minority classes without modifying the dataset. Evaluations
on the ICNALE benchmark dataset show that integrating the BLV loss into a
celebrated text-based (BERT) model significantly enhances classification
accuracy and fairness, making automated speech evaluation more robust for
diverse learners.

</details>


### [70] [Training LLMs to be Better Text Embedders through Bidirectional Reconstruction](https://arxiv.org/abs/2509.03020)
*Chang Su,Dengliang Shi,Siyuan Huang,Jintao Du,Changhua Meng,Yu Cheng,Weiqiang Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: LLM 嵌入方法通过在对比学习前增加一个双向生成重建训练阶段来丰富最终标记嵌入的语义，以提高文本检索和重新排序任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 文本嵌入方法利用最终标记（通常是 [EOS]）的嵌入，但这些标记未经专门训练以捕捉整个上下文的语义，这限制了它们作为文本嵌入（尤其是在检索和重新排序任务中）的能力。

Method: 在对比学习之前增加一个训练阶段，该阶段采用双向生成重建任务 EBQ2D（基于嵌入的查询到文档）和 EBD2Q（基于嵌入的文档到查询），以锚定 [EOS] 嵌入并重建查询-文档对的任一侧。

Result: 在 Massive Text Embedding Benchmark (MTEB) 上，所提出的额外训练阶段显著提高了 LLM 的性能，并在不同的 LLM 基础模型和规模上取得了新的最先进成果。

Conclusion: 通过在对比学习之前增加 EBQ2D 和 EBD2Q 任务，可以丰富最终标记嵌入的语义，从而提高 LLM 在文本检索和重新排序任务中的性能。

Abstract: Large language models (LLMs) have increasingly been explored as powerful text
embedders. Existing LLM-based text embedding approaches often leverage the
embedding of the final token, typically a reserved special token such as [EOS].
However, these tokens have not been intentionally trained to capture the
semantics of the whole context, limiting their capacity as text embeddings,
especially for retrieval and re-ranking tasks. We propose to add a new training
stage before contrastive learning to enrich the semantics of the final token
embedding. This stage employs bidirectional generative reconstruction tasks,
namely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based
Document-to-Query), which interleave to anchor the [EOS] embedding and
reconstruct either side of Query-Document pairs. Experimental results
demonstrate that our additional training stage significantly improves LLM
performance on the Massive Text Embedding Benchmark (MTEB), achieving new
state-of-the-art results across different LLM base models and scales.

</details>


### [71] [Structure-Learnable Adapter Fine-Tuning for Parameter-Efficient Large Language Models](https://arxiv.org/abs/2509.03057)
*Ming Gong,Yingnan Deng,Nia Qi,Yujun Zou,Zhihao Xue,Yun Zi*

Main category: cs.CL

TL;DR: 本研究提出一种基于适配器的、结构可学习的微调方法，通过自动优化适配器插入点、激活路径和模块组合，实现模型在多任务设置下的结构灵活性，并提升参数利用率和表示能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型微调中存在的参数冗余、结构刚性及任务适应性有限的问题。

Method: 引入可微分门控函数和结构稀疏性控制变量，实现适配器插入点、激活路径和模块组合的自动优化，并通过结构搜索机制动态构建特定任务的高效子结构，同时冻结骨干参数。

Result: 所提出的方法在多任务自然语言理解任务上超越了主流的参数高效微调技术，并在准确性、压缩率以及对噪声和扰动的鲁棒性之间取得了更好的平衡。

Conclusion: 该方法通过灵活动态调整模型结构来适应不同任务特性，有效提高了参数利用率和模型性能，并在准确性、压缩率和鲁棒性方面表现优异。

Abstract: This paper addresses the issues of parameter redundancy, rigid structure, and
limited task adaptability in the fine-tuning of large language models. It
proposes an adapter-based fine-tuning method built on a structure-learnable
mechanism. By introducing differentiable gating functions and structural
sparsity control variables, the method enables automatic optimization of
adapter insertion points, activation paths, and module combinations. This
allows the model to adjust its structure flexibly in multi-task settings to
match different task characteristics. With the backbone parameters kept frozen,
the method uses a structure search mechanism to guide the dynamic construction
of task-specific efficient substructures during training. This significantly
improves parameter utilization and representational capacity. In addition, the
paper designs a set of sensitivity analysis experiments to systematically
evaluate the effects of sparsity weight, noise injection ratio, and data
perturbation on model performance. These experiments verify the stability and
robustness of the proposed method across various multi-task natural language
understanding tasks. The experimental results show that the proposed method
outperforms mainstream parameter-efficient tuning techniques on multiple tasks.
It achieves a better balance among accuracy, compression rate, and robustness
to noise and perturbation.

</details>


### [72] [A Long Short-Term Memory (LSTM) Model for Business Sentiment Analysis Based on Recurrent Neural Network](https://arxiv.org/abs/2509.03060)
*Md. Jahidul Islam Razin,Md. Abdul Karim,M. F. Mridha,S M Rafiuddin,Tahira Alam*

Main category: cs.CL

TL;DR: 该研究应用改进的 LSTM 模型进行商业情感分析，在产品评论数据集上实现了 91.33% 的准确率，优于传统 RNN 模型，可帮助企业了解客户反馈和评估营销策略。


<details>
  <summary>Details</summary>
Motivation: 商业情感分析（BSA）是自然语言处理中的一个重要课题，旨在为商业目的分析情感。该研究提出了一种改进的循环神经网络（RNN）方法，以解决传统 RNN 中梯度消失的问题。

Method: 采用长短期记忆（LSTM）模型，这是一种特殊的 RNN，用于商业情感分析。在一个修改后的 RNN 模型中应用了 LSTM，以克服梯度消失问题。使用产品评论数据集，其中 70% 用于训练，30% 用于测试。

Result: 所提出的改进 RNN（LSTM）模型在产品评论数据集上达到了 91.33% 的准确率。将该模型的结果与传统 RNN 模型进行了比较，结果表明所提出的模型性能更优。

Conclusion: 所提出的改进 RNN（LSTM）模型在商业情感分析任务上表现优于传统 RNN 模型。该模型可帮助企业识别客户对其产品的喜好，并据此评估其营销策略。

Abstract: Business sentiment analysis (BSA) is one of the significant and popular
topics of natural language processing. It is one kind of sentiment analysis
techniques for business purposes. Different categories of sentiment analysis
techniques like lexicon-based techniques and different types of machine
learning algorithms are applied for sentiment analysis on different languages
like English, Hindi, Spanish, etc. In this paper, long short-term memory (LSTM)
is applied for business sentiment analysis, where a recurrent neural network is
used. An LSTM model is used in a modified approach to prevent the vanishing
gradient problem rather than applying the conventional recurrent neural network
(RNN). To apply the modified RNN model, product review dataset is used. In this
experiment, 70\% of the data is trained for the LSTM and the rest 30\% of the
data is used for testing. The result of this modified RNN model is compared
with other conventional RNN models, and a comparison is made among the results.
It is noted that the proposed model performs better than the other conventional
RNN models. Here, the proposed model, i.e., the modified RNN model approach has
achieved around 91.33\% of accuracy. By applying this model, any business
company or e-commerce business site can identify the feedback from their
customers about different types of products that customers like or dislike.
Based on the customer reviews, a business company or e-commerce platform can
evaluate its marketing strategy.

</details>


### [73] [Measuring Scalar Constructs in Social Science with LLMs](https://arxiv.org/abs/2509.03116)
*Hauke Licht,Rupak Sarkar,Patrick Y. Wu,Pranav Goel,Niklas Stoehr,Elliott Ash,Alexander Miserlis Hoyle*

Main category: cs.CL

TL;DR: LLM 在社会科学研究中测量连续性语义结构存在挑战，但 finetuning 和 token-probability-weighted 评分是有效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决大型语言模型（LLMs）在社会科学中测量连续性语义结构时，因其对数字输出处理的特异性而带来的应用问题。

Method: 通过对来自政治学文献的多个数据集进行评估，研究了四种方法：无权重的直接逐点评分、成对比较的聚合、token 概率加权的逐点评分以及 finetuning。

Result: 研究发现，直接生成逐点评分的 LLM 会产生不连续且在任意数字处堆积的分布；通过成对比较可以改善测量质量；token 概率加权进一步提高测量质量；finetuning 小型模型（使用少量训练对）可以达到或超过提示式 LLM 的性能。

Conclusion: 对于应用研究者而言，finetuning 和 token-probability-weighted 评分是 LLM 在社会科学中测量连续性语义结构更优的选择。

Abstract: Many constructs that characterize language, like its complexity or
emotionality, have a naturally continuous semantic structure; a public speech
is not just "simple" or "complex," but exists on a continuum between extremes.
Although large language models (LLMs) are an attractive tool for measuring
scalar constructs, their idiosyncratic treatment of numerical outputs raises
questions of how to best apply them. We address these questions with a
comprehensive evaluation of LLM-based approaches to scalar construct
measurement in social science. Using multiple datasets sourced from the
political science literature, we evaluate four approaches: unweighted direct
pointwise scoring, aggregation of pairwise comparisons,
token-probability-weighted pointwise scoring, and finetuning. Our study yields
actionable findings for applied researchers. First, LLMs prompted to generate
pointwise scores directly from texts produce discontinuous distributions with
bunching at arbitrary numbers. The quality of the measurements improves with
pairwise comparisons made by LLMs, but it improves even more by taking
pointwise scores and weighting them by token probability. Finally, finetuning
smaller models with as few as 1,000 training pairs can match or exceed the
performance of prompted LLMs.

</details>


### [74] [From Evaluation to Defense: Constructing Persistent Edit-Based Fingerprints for Large Language Models](https://arxiv.org/abs/2509.03122)
*Yue Li,Xin Yi,Dongsheng Shi,Yongyi Cui,Gerard de Melo,Xiaoling Wang,Linlin Wang*

Main category: cs.CL

TL;DR: LLM IP保护的新方法：利用知识编辑和FSFT技术，并探讨其局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM指纹注入技术存在性能下降、计算资源消耗大、持久性差等问题，知识编辑是一种更优的替代方案。

Method: 首次将知识编辑应用于LLM指纹注入，并提出FSFT（Fingerprint Subspace-aware Fine-Tuning）来解决指纹在模型微调过程中发生退化的问题。

Result: FSFT在最坏情况下性能仍优于传统微调10%，但指纹注入模型在区分指纹和相似文本方面存在困难。

Conclusion: 知识编辑为LLM IP保护提供了轻量级解决方案，FSFT有效缓解了微调导致的指纹退化。然而，仍需开发更鲁棒、更细粒度的指纹注入方法。

Abstract: The intellectual property (IP) protection of Large Language Models (LLMs) is
increasingly critical. Injecting specialized fingerprints into LLMs through
instruction tuning is a common IP protection technique. However, this may
significantly degrade model performance, requires substantial computational
resources, and exhibits poor persistence under model modifications. We argue
that knowledge editing offers a lightweight alternative that is more suitable
for fingerprint injection. Accordingly, we apply knowledge editing to
fingerprint injection for the first time and demonstrate its strong capability.
Despite using scrambled text as fingerprints to prevent them from being
overwritten during fine-tuning, degradation still occurs under large-scale
fine-tuning. To address this, we propose Fingerprint Subspace-aware Fine-Tuning
(FSFT), which reduces fingerprint degradation by constraining the update of the
fingerprint subspace. The performance of FSFT exceeds fine-tuning by 10% even
in the worst-case scenario. Additionally, we observe that the
fingerprint-injected models struggle to distinguish between fingerprints and
similar texts due to the high similarity of their features. This finding
underscores the urgent need for more robust and fine-grained fingerprinting
injection methods for LLMs.

</details>


### [75] [An experimental and computational study of an Estonian single-person word naming](https://arxiv.org/abs/2509.03143)
*Kaidi Lõo,Arvi Tavast,Maria Heitmeier,Harald Baayen*

Main category: cs.CL

TL;DR: 本研究调查了爱沙尼亚语的词汇处理过程，并使用眼动追踪技术进行了一项大规模单被试实验。研究分析了词语命名任务中的五个变量：首次注视时长、总注视时长、注视次数、词语命名潜伏期和口语词时长。


<details>
  <summary>Details</summary>
Motivation: 本研究的目的是检验计算模型（判别式词汇模型，DLM）生成的词汇处理测量值是否能预测眼动追踪数据，并与词频、邻近词数量和屈折范式大小等传统预测变量进行比较。

Method: 研究采用眼动追踪技术，结合词语命名任务，分析了五种响应变量（首次注视时长、总注视时长、注视次数、词语命名潜伏期和口语词时长），并使用广义可加模型进行分析。计算模型分别使用线性和深度映射实现。

Result: 研究发现，DLM度量是词汇处理的有力预测因子，深度学习的DLM度量并不一定比线性映射的DLM度量更精确。传统预测因子在拟合精度上略优于DLM度量，但总注视时长除外。在命名任务中，词汇变量不能预测首次注视时长和总注视次数。DLM度量对总注视时长、命名潜伏期和口语词时长的预测性表明，意义在词语命名任务中起着重要作用。

Conclusion: DLM度量，特别是那些利用深度学习的度量，可以有效地预测词汇处理过程。尽管传统预测因子在某些方面略胜一筹，但DLM度量在解释词汇处理的复杂性方面仍然具有重要价值。研究结果还强调了意义在词语命名任务中的核心作用。

Abstract: This study investigates lexical processing in Estonian. A large-scale
single-subject experiment is reported that combines the word naming task with
eye-tracking. Five response variables (first fixation duration, total fixation
duration, number of fixations, word naming latency, and spoken word duration)
are analyzed with the generalized additive model. Of central interest is the
question of whether measures for lexical processing generated by a
computational model of the mental lexicon (the Discriminative Lexicon Model,
DLM) are predictive for these response variables, and how they compare to
classical predictors such as word frequency, neighborhood size, and
inflectional paradigm size. Computational models were implemented both with
linear and deep mappings. Central findings are, first, that DLM-based measures
are powerful predictors for lexical processing, second, that DLM-measures using
deep learning are not necessarily more precise predictors of lexical processing
than DLM-measures using linear mappings, third, that classical predictors tend
to provide somewhat more precise fits compared to DLM-based predictors (except
for total fixation duration, where the two provide equivalent goodness of fit),
and fourth, that in the naming task lexical variables are not predictive for
first fixation duration and the total number of fixations. As the DLM works
with mappings from form to meaning, the predictivity of DLM-based measures for
total fixation duration, naming latencies, and spoken word duration indicates
that meaning is heavily involved in the present word naming task.

</details>


### [76] [Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader](https://arxiv.org/abs/2509.03148)
*Jannis Vamvas,Ignacio Pérez Prat,Not Battesta Soliva,Sandra Baltermia-Guetg,Andrina Beeli,Simona Beeli,Madlaina Capeder,Laura Decurtins,Gian Peder Gregori,Flavia Hobi,Gabriela Holderegger,Arina Lazzarini,Viviana Lazzarini,Walter Rosselli,Bettina Vital,Anna Rutkiewicz,Rico Sennrich*

Main category: cs.CL

TL;DR: 本文介绍了一个针对瑞士罗曼什语六种变体（包括区域性变体和超区域性变体 Rumantsch Grischun）的机器翻译评估基准。该基准包含由人类翻译员根据 WMT24++ 基准创建的参考翻译，确保了与其他 55 种以上语言的并行性。评估结果显示，尽管将罗曼什语翻译成德语相对较好，但将德语翻译成罗曼什语仍然是一个挑战。


<details>
  <summary>Details</summary>
Motivation: 由于瑞士罗曼什语的机器翻译评估资源有限，本文旨在创建一个针对其多种变体的评估基准，以促进该语言的机器翻译研究和发展。

Method: 本文构建了一个包含六种罗曼什语变体的基准，其参考翻译由人类翻译员依据 WMT24++ 基准创建，确保了与超过 55 种其他语言的并行性。随后，利用此基准对现有的机器翻译系统和大型语言模型进行了自动评估。

Result: 自动评估结果表明，将罗曼什语翻译成德语在所有变体上都表现相对较好。然而，将德语翻译成罗曼什语仍然是一个具有挑战性的任务。

Conclusion: 本文创建的罗曼什语机器翻译评估基准为该语言的机器翻译研究提供了重要资源。评估结果揭示了当前机器翻译系统在处理罗曼什语输入方面存在优势，但在处理罗曼什语输出方面仍有待提高。

Abstract: The Romansh language, spoken in Switzerland, has limited resources for
machine translation evaluation. In this paper, we present a benchmark for six
varieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five
regional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our
reference translations were created by human translators based on the WMT24++
benchmark, which ensures parallelism with more than 55 other languages. An
automatic evaluation of existing MT systems and LLMs shows that translation out
of Romansh into German is handled relatively well for all the varieties, but
translation into Romansh is still challenging.

</details>


### [77] [Domain Adaptation of LLMs for Process Data](https://arxiv.org/abs/2509.03161)
*Rafael Seidi Oyamada,Jari Peeperkorn,Jochen De Weerdt,Johannes De Smedt*

Main category: cs.CL

TL;DR: LLM可以直接应用于过程挖掘，无需自然语言转换，在预测性过程监控方面表现优于RNN和基于叙述的方法。


<details>
  <summary>Details</summary>
Motivation: LLM在序列生成方面表现出色，而过程挖掘的目标是处理事件日志序列，因此可以直接将LLM应用于过程数据。

Method: 使用参数高效的微调技术，直接在过程数据上微调预训练的LLM，用于预测性过程监控任务（单任务和多任务）。

Result: 与最先进的RNN和基于叙述的方法相比，所提出的方法在预测性能上显示出潜在的改进，尤其是在多任务设置下。此外，微调后的模型收敛更快，所需的超参数优化更少。

Conclusion: 直接在过程数据上进行LLM微调是一种有前景的方法，在预测性过程监控方面可以取得优于现有方法的性能，并且计算效率更高。

Abstract: In recent years, Large Language Models (LLMs) have emerged as a prominent
area of interest across various research domains, including Process Mining
(PM). Current applications in PM have predominantly centered on prompt
engineering strategies or the transformation of event logs into narrative-style
datasets, thereby exploiting the semantic capabilities of LLMs to address
diverse tasks. In contrast, this study investigates the direct adaptation of
pretrained LLMs to process data without natural language reformulation,
motivated by the fact that these models excel in generating sequences of
tokens, similar to the objective in PM. More specifically, we focus on
parameter-efficient fine-tuning techniques to mitigate the computational
overhead typically associated with such models. Our experimental setup focuses
on Predictive Process Monitoring (PPM), and considers both single- and
multi-task predictions. The results demonstrate a potential improvement in
predictive performance over state-of-the-art recurrent neural network (RNN)
approaches and recent narrative-style-based solutions, particularly in the
multi-task setting. Additionally, our fine-tuned models exhibit faster
convergence and require significantly less hyperparameter optimization.

</details>


### [78] [SinhalaMMLU: A Comprehensive Benchmark for Evaluating Multitask Language Understanding in Sinhala](https://arxiv.org/abs/2509.03162)
*Ashmari Pramodya,Nirasha Nelki,Heshan Shalinda,Chamila Liyanage,Yusuke Sakai,Randil Pushpananda,Ruvan Weerasinghe,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 该研究提出了SinhalaMMLU，一个针对僧伽罗语（一种低资源语言）的、首个多项选择问答基准，旨在评估大型语言模型（LLMs）在低资源和文化特定情境下的表现。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在低资源语言（如僧伽罗语）和文化特定内容上的能力，弥补现有基准测试的不足（如过度侧重全球或欧洲中心内容，依赖自动翻译）。

Method: 创建了一个包含超过7000个问题的SinhalaMMLU数据集，涵盖中学到大学教育水平，符合斯里兰卡国家课程，并包含6个领域和30个科目。评估了26个LLMs在SinhalaMMLU上的表现。

Result: Claude 3.5 sonnet和GPT-4o在SinhalaMMLU上表现最佳，准确率分别为67%和62%，但总体模型表现仍有限，特别是在人文等文化丰富领域存在显著的提升空间。

Conclusion: LLMs在低资源和文化特定情境下的适应能力仍有待提高，特别是人文等领域，表明在将LLMs应用于此类场景时存在巨大的改进潜力。

Abstract: Large Language Models (LLMs) demonstrate impressive general knowledge and
reasoning abilities, yet their evaluation has predominantly focused on global
or anglocentric subjects, often neglecting low-resource languages and
culturally specific content. While recent multilingual benchmarks attempt to
bridge this gap, many rely on automatic translation, which can introduce errors
and misrepresent the original cultural context. To address this, we introduce
SinhalaMMLU, the first multiple-choice question answering benchmark designed
specifically for Sinhala, a low-resource language. The dataset includes over
7,000 questions spanning secondary to collegiate education levels, aligned with
the Sri Lankan national curriculum, and covers six domains and 30 subjects,
encompassing both general academic topics and culturally grounded knowledge. We
evaluate 26 LLMs on SinhalaMMLU and observe that, while Claude 3.5 sonnet and
GPT-4o achieve the highest average accuracies at 67% and 62% respectively,
overall model performance remains limited. In particular, models struggle in
culturally rich domains such as the Humanities, revealing substantial room for
improvement in adapting LLMs to low-resource and culturally specific contexts.

</details>


### [79] [Comparison of End-to-end Speech Assessment Models for the NOCASA 2025 Challenge](https://arxiv.org/abs/2509.03256)
*Aleksei Žavoronkov,Tanel Alumäe*

Main category: cs.CL

TL;DR: 本研究提出了三种用于儿童挪威语二语语音评估的端到端模型，其中基于GOP-CTC的模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为了应对NOCASA 2025挑战赛，需要开发自动化的儿童挪威语二语语音发音评估模型。

Method: 研究了三种模型：1. 端到端-判别模型（E2E-R）；2. 利用预训练的wav2vec2.0表征的prefix-tuning直接分类模型；3. 结合CTC计算的无对齐发音优劣度（GOP）特征的新模型。并引入了加权的序数交叉熵损失函数来优化评价指标。

Result: 基于GOP-CTC的模型在评估中表现最佳，显著优于挑战赛的基线模型，并取得了排行榜的最高分。

Conclusion: 结合CTC的GOP特征的端到端模型在NOCASA 2025挑战赛中取得了最优异的发音评估性能。

Abstract: This paper presents an analysis of three end-to-end models developed for the
NOCASA 2025 Challenge, aimed at automatic word-level pronunciation assessment
for children learning Norwegian as a second language. Our models include an
encoder-decoder Siamese architecture (E2E-R), a prefix-tuned direct
classification model leveraging pretrained wav2vec2.0 representations, and a
novel model integrating alignment-free goodness-of-pronunciation (GOP) features
computed via CTC. We introduce a weighted ordinal cross-entropy loss tailored
for optimizing metrics such as unweighted average recall and mean absolute
error. Among the explored methods, our GOP-CTC-based model achieved the highest
performance, substantially surpassing challenge baselines and attaining top
leaderboard scores.

</details>


### [80] [LatPhon: Lightweight Multilingual G2P for Romance Languages and English](https://arxiv.org/abs/2509.03300)
*Luis Felipe Chary,Miguel Arjona Ramirez*

Main category: cs.CL

TL;DR: LatPhon是一个7.5M参数的Transformer模型，可以对英语、西班牙语、法语、意大利语、葡萄牙语和罗马尼亚语进行音素转换，在ipa-dict语料库上达到了3.5%的平均音素错误率（PER），优于ByT5基线（5.4%），并接近特定语言的WFST（3.2%），同时内存占用仅为30MB，可用于设备部署。


<details>
  <summary>Details</summary>
Motivation: Grapheme-to-phoneme (G2P) conversion is crucial for various speech processing systems like TTS, ASR, and S2ST, especially for multiple Latin-script languages. Existing methods may not be efficient or adaptable across languages.

Method: We present LatPhon, a 7.5M-parameter Transformer model jointly trained on six Latin-script languages: English, Spanish, French, Italian, Portuguese, and Romanian. The model was evaluated on the public ipa-dict corpus.

Result: LatPhon achieved a mean phoneme error rate (PER) of 3.5% on the ipa-dict corpus, outperforming the ByT5 baseline (5.4%) and nearing language-specific WFSTs (3.2%). It also requires only 30MB of memory, making on-device deployment feasible.

Conclusion: Compact multilingual G2P models like LatPhon can function as a universal front-end for speech processing pipelines across Latin-script languages, offering efficiency and adaptability.

Abstract: Grapheme-to-phoneme (G2P) conversion is a key front-end for text-to-speech
(TTS), automatic speech recognition (ASR), speech-to-speech translation (S2ST)
and alignment systems, especially across multiple Latin-script languages.We
present LatPhon, a 7.5 M - parameter Transformer jointly trained on six such
languages--English, Spanish, French, Italian, Portuguese, and Romanian. On the
public ipa-dict corpus, it attains a mean phoneme error rate (PER) of 3.5%,
outperforming the byte-level ByT5 baseline (5.4%) and approaching
language-specific WFSTs (3.2%) while occupying 30 MB of memory, which makes
on-device deployment feasible when needed. These results indicate that compact
multilingual G2P can serve as a universal front-end for Latin-language speech
pipelines.

</details>


### [81] [LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations](https://arxiv.org/abs/2509.03405)
*Daniela Gottesman,Alon Gilae-Dotan,Ido Cohen,Yoav Gur-Arieh,Marius Mosbach,Ori Yoran,Mor Geva*

Main category: cs.CL

TL;DR: LMEnt是一个用于分析预训练中语言模型知识获取的工具套件，包括知识丰富的预训练语料库、基于实体的检索方法和预训练模型，可用于研究知识表示、可塑性、编辑、归因和学习动态。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型如何从数据中获取知识对于开发更一致、更鲁棒、更完整的语言模型至关重要。

Method: LMEnt包含一个使用维基百科注释的实体提及的预训练语料库，一个优于先前方法的基于实体的检索方法，以及12个具有高达1B参数和4K中间检查点的预训练模型。

Result: 研究表明，事实频率在知识获取中起着关键作用，但不能完全解释学习趋势。

Conclusion: LMEnt提供了一个受控的环境来研究预训练中的实体提及与下游性能之间的联系，以及因果干预对预训练数据的影响。

Abstract: Language models (LMs) increasingly drive real-world applications that require
world knowledge. However, the internal processes through which models turn data
into representations of knowledge and beliefs about the world, are poorly
understood. Insights into these processes could pave the way for developing LMs
with knowledge representations that are more consistent, robust, and complete.
To facilitate studying these questions, we present LMEnt, a suite for analyzing
knowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a
knowledge-rich pretraining corpus, fully annotated with entity mentions, based
on Wikipedia, (2) an entity-based retrieval method over pretraining data that
outperforms previous approaches by as much as 80.4%, and (3) 12 pretrained
models with up to 1B parameters and 4K intermediate checkpoints, with
comparable performance to popular open-sourced models on knowledge benchmarks.
Together, these resources provide a controlled environment for analyzing
connections between entity mentions in pretraining and downstream performance,
and the effects of causal interventions in pretraining data. We show the
utility of LMEnt by studying knowledge acquisition across checkpoints, finding
that fact frequency is key, but does not fully explain learning trends. We
release LMEnt to support studies of knowledge in LMs, including knowledge
representations, plasticity, editing, attribution, and learning dynamics.

</details>


### [82] [Learning Mechanism Underlying NLP Pre-Training and Fine-Tuning](https://arxiv.org/abs/2509.03407)
*Yarden Tzach,Ronit D. Gross,Ella Koresh,Shalom Rosner,Or Shpringer,Tal Halevi,Ido Kanter*

Main category: cs.CL

TL;DR: 预训练模型的准确率与微调分类任务的表现相关，预训练能生成更高阶的语言结构，并且这种机制可能具有普遍性。


<details>
  <summary>Details</summary>
Motivation: 理解预训练模型的内在机制，并探究预训练准确率与分类任务微调之间的相互作用。

Method: 通过分析BERT-6模型在Wikipedia数据集上的预训练以及在FewRel和DBpedia数据集上的微调过程，研究准确率与代币（token）出现频率、代币集群、Transformer块层级等因素的关系。

Result: 代币准确率（APT）随其在数据集中出现频率的增加而增加，并作为量化预训练成功程度的序参量。预训练将代币聚集成有限的、小的、强匹配的代币集群，这种现象在Transformer块中越往后越明显。预训练能够生成更高阶的语言结构，并且这种结构对微调准确率的提升有积极作用。输出标签预测的置信度与平均输入APT无关。

Conclusion: 预训练机制对于提升NLP分类任务的性能至关重要，并且可能在图像分类等其他领域也具有普遍性。

Abstract: Natural language processing (NLP) enables the understanding and generation of
meaningful human language, typically using a pre-trained complex architecture
on a large dataset to learn the language and next fine-tune its weights to
implement a specific task. Twofold goals are examined; to understand the
mechanism underlying successful pre-training and to determine the interplay
between the pre-training accuracy and the fine-tuning of classification tasks.
The following main results were obtained; the accuracy per token (APT)
increased with its appearance frequency in the dataset, and its average over
all tokens served as an order parameter to quantify pre-training success, which
increased along the transformer blocks. Pre-training broke the symmetry among
tokens and grouped them into finite, small, strong match token clusters, as
inferred from the presented token confusion matrix. This feature was sharpened
along the transformer blocks toward the output layer, enhancing its performance
considerably compared with that of the embedding layer. Consequently,
higher-order language structures were generated by pre-training, even though
the learning cost function was directed solely at identifying a single token.
These pre-training findings were reflected by the improved fine-tuning accuracy
along the transformer blocks. Additionally, the output label prediction
confidence was found to be independent of the average input APT, as the input
meaning was preserved since the tokens are replaced primarily by strong match
tokens. Finally, although pre-training is commonly absent in image
classification tasks, its underlying mechanism is similar to that used in
fine-tuning NLP classification tasks, hinting at its universality. The results
were based on the BERT-6 architecture pre-trained on the Wikipedia dataset and
fine-tuned on the FewRel and DBpedia classification tasks.

</details>


### [83] [Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases LLM Judges](https://arxiv.org/abs/2509.03419)
*Weiyuan Li,Xintao Wang,Siyu Yuan,Rui Xu,Jiangjie Chen,Qingqing Dong,Yanghua Xiao,Deqing Yang*

Main category: cs.CL

TL;DR: LLM评估在复杂任务中存在偏见，ComplexEval基准测试量化了这些偏见，LRM尤其脆弱。


<details>
  <summary>Details</summary>
Motivation: LLM在处理复杂任务时，其作为裁判的可靠性评估不足，需要研究和量化潜在的偏见。

Method: 构建了ComplexEval基准测试，系统地研究了12个基础和3个高级场景下的6种辅助信息诱导偏见。

Result: 所有被评估的模型都表现出明显的偏见，且偏见程度随任务复杂性增加而增大；LRM表现出反常的脆弱性。

Conclusion: LLM（特别是LRM）在复杂任务评估中容易受到偏见影响，需要改进评估信号的准确性和可验证性，以发展更通用、更鲁棒的评估模型。

Abstract: As large language models (LLMs) grow more capable, they face increasingly
diverse and complex tasks, making reliable evaluation challenging. The paradigm
of LLMs as judges has emerged as a scalable solution, yet prior work primarily
focuses on simple settings. Their reliability in complex tasks--where
multi-faceted rubrics, unstructured reference answers, and nuanced criteria are
critical--remains understudied. In this paper, we constructed ComplexEval, a
challenge benchmark designed to systematically expose and quantify Auxiliary
Information Induced Biases. We systematically investigated and validated 6
previously unexplored biases across 12 basic and 3 advanced scenarios. Key
findings reveal: (1) all evaluated models exhibit significant susceptibility to
these biases, with bias magnitude scaling with task complexity; (2) notably,
Large Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth
analysis offers crucial insights for improving the accuracy and verifiability
of evaluation signals, paving the way for more general and robust evaluation
models.

</details>


### [84] [Continuous Saudi Sign Language Recognition: A Vision Transformer Approach](https://arxiv.org/abs/2509.03467)
*Soukeina Elhassen,Lama Al Khuzayem,Areej Alhothali,Ohoud Alzamzami,Nahed Alowaidi*

Main category: cs.CL

TL;DR: 本研究介绍了KAU-CSSL数据集，这是首个专注于连续句子的沙特手语数据集，并提出了一种基于Transformer的模型，在手语识别任务中取得了高准确率，旨在弥补阿拉伯手语研究资源的不足。


<details>
  <summary>Details</summary>
Motivation: 由于公众对手语意识的不足，听障人士在教育和职业机会方面面临不平等，导致社会排斥。沙特阿拉伯有超过84,000人依赖沙特手语（SSL）。尽管现有技术有所帮助，但仍迫切需要更精确可靠的翻译技术，特别是针对SSL。现有解决方案主要关注非阿拉伯手语，导致SSL资源匮乏，且现有数据集多为孤立的单词而非连续手语。本研究旨在解决这一资源缺口。

Method: 提出了一种基于Transformer的模型，该模型使用预训练的ResNet-18进行空间特征提取，并结合Transformer Encoder和Bidirectional LSTM来处理时间依赖性。同时，研究创建并发布了首个连续沙特手语数据集KAU-CSSL，该数据集包含完整的句子。

Result: 所提出的模型在孤立手语者模式下达到了99.02%的准确率，在非孤立手语者模式下达到了77.71%的准确率。

Conclusion: 本研究通过发布首个连续SSL数据集KAU-CSSL并提出一种先进的Transformer模型，为SSL的研究和识别系统开发奠定了基础，有望改善SSL社群的沟通工具，并为手语识别领域做出贡献。

Abstract: Sign language (SL) is an essential communication form for hearing-impaired
and deaf people, enabling engagement within the broader society. Despite its
significance, limited public awareness of SL often leads to inequitable access
to educational and professional opportunities, thereby contributing to social
exclusion, particularly in Saudi Arabia, where over 84,000 individuals depend
on Saudi Sign Language (SSL) as their primary form of communication. Although
certain technological approaches have helped to improve communication for
individuals with hearing impairments, there continues to be an urgent
requirement for more precise and dependable translation techniques, especially
for Arabic sign language variants like SSL. Most state-of-the-art solutions
have primarily focused on non-Arabic sign languages, resulting in a
considerable absence of resources dedicated to Arabic sign language,
specifically SSL. The complexity of the Arabic language and the prevalence of
isolated sign language datasets that concentrate on individual words instead of
continuous speech contribute to this issue. To address this gap, our research
represents an important step in developing SSL resources. To address this, we
introduce the first continuous Saudi Sign Language dataset called KAU-CSSL,
focusing on complete sentences to facilitate further research and enable
sophisticated recognition systems for SSL recognition and translation.
Additionally, we propose a transformer-based model, utilizing a pretrained
ResNet-18 for spatial feature extraction and a Transformer Encoder with
Bidirectional LSTM for temporal dependencies, achieving 99.02\% accuracy at
signer dependent mode and 77.71\% accuracy at signer independent mode. This
development leads the way to not only improving communication tools for the SSL
community but also making a substantial contribution to the wider field of sign
language.

</details>


### [85] [Design and Optimization of Reinforcement Learning-Based Agents in Text-Based Games](https://arxiv.org/abs/2509.03479)
*Haonan Wang,Mingjia Zhao,Junfeng Sun,Wei Liu*

Main category: cs.CL

TL;DR: 使用深度学习和深度强化学习，研究人员开发了一种新颖的AI智能体，显著提高了在文本游戏中完成率和胜率。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，使用智能体进行文本游戏的研究越来越受欢迎。本文旨在提出一种新颖的智能体设计和学习方法，以应对这一挑战。

Method: 本文提出了一种结合深度学习和深度强化学习的方法。首先，利用深度学习模型处理游戏文本并构建世界模型；然后，采用基于策略梯度的深度强化学习方法来学习智能体，实现从状态价值到最优策略的转换。

Result: 实验结果表明，所提出的增强型智能体在多项文本游戏实验中表现更优，在游戏完成率和胜率方面显著超越了先前的智能体。

Conclusion: 本研究为在文本游戏中应用强化学习提供了新的认识和实证基础，并为开发和优化更通用领域的强化学习智能体奠定了基础。

Abstract: As AI technology advances, research in playing text-based games with agents
has becomeprogressively popular. In this paper, a novel approach to agent
design and agent learning ispresented with the context of reinforcement
learning. A model of deep learning is first applied toprocess game text and
build a world model. Next, the agent is learned through a policy gradient-based
deep reinforcement learning method to facilitate conversion from state value to
optimal policy.The enhanced agent works better in several text-based game
experiments and significantlysurpasses previous agents on game completion ratio
and win rate. Our study introduces novelunderstanding and empirical ground for
using reinforcement learning for text games and sets thestage for developing
and optimizing reinforcement learning agents for more general domains
andproblems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [86] [Can Media Act as a Soft Regulator of Safe AI Development? A Game Theoretical Analysis](https://arxiv.org/abs/2509.02650)
*Henrique Correia da Fonseca,António Fernandes,Zhao Song,Theodor Cimpeanu,Nataliya Balabanova,Adeela Bashir,Paolo Bova,Alessio Buscemi,Alessandro Di Stefano,Manh Hong Duong,Elias Fernandez Domingos,Ndidi Bianca Ogbo,Simon T. Powers,Daniele Proverbio,Zia Ush Shamszaman,Fernando P. Santos,The Anh Han,Marcus Krellner*

Main category: cs.AI

TL;DR: 媒体报道可能促使人工智能开发者在利润和安全之间优先考虑安全，但其有效性取决于信息的可靠性和成本。


<details>
  <summary>Details</summary>
Motivation: 探讨媒体报道是否能促使人工智能（AI）开发者在利润和用户安全之间选择安全，从而促进AI技术的广泛应用。

Method: 通过进化博弈论，创建了由自我利益驱动的开发者和用户组成的模拟种群进行研究。

Result: 研究结果表明，媒体确实能够促进开发者和用户之间的合作，但并非总能成功。如果媒体信息的质量不可靠，或者获取媒体信息或确保安全的成本过高，合作就无法进化。

Conclusion: 媒体通过塑造公众认知和追究开发者责任，能够充当强大的软性监管者，即使在缺乏政府正式监管的情况下也能引导AI安全发展。

Abstract: When developers of artificial intelligence (AI) products need to decide
between profit and safety for the users, they likely choose profit.
Untrustworthy AI technology must come packaged with tangible negative
consequences. Here, we envisage those consequences as the loss of reputation
caused by media coverage of their misdeeds, disseminated to the public. We
explore whether media coverage has the potential to push AI creators into the
production of safe products, enabling widespread adoption of AI technology. We
created artificial populations of self-interested creators and users and
studied them through the lens of evolutionary game theory. Our results reveal
that media is indeed able to foster cooperation between creators and users, but
not always. Cooperation does not evolve if the quality of the information
provided by the media is not reliable enough, or if the costs of either
accessing media or ensuring safety are too high. By shaping public perception
and holding developers accountable, media emerges as a powerful soft regulator
-- guiding AI safety even in the absence of formal government oversight.

</details>


### [87] [The Future of Artificial Intelligence and the Mathematical and Physical Sciences (AI+MPS)](https://arxiv.org/abs/2509.02661)
*Andrew Ferguson,Marisa LaFleur,Lars Ruthotto,Jesse Thaler,Yuan-Sen Ting,Pratyush Tiwary,Soledad Villar,E. Paulo Alves,Jeremy Avigad,Simon Billinge,Camille Bilodeau,Keith Brown,Emmanuel Candes,Arghya Chattopadhyay,Bingqing Cheng,Jonathan Clausen,Connor Coley,Andrew Connolly,Fred Daum,Sijia Dong,Chrisy Xiyu Du,Cora Dvorkin,Cristiano Fanelli,Eric B. Ford,Luis Manuel Frutos,Nicolás García Trillos,Cecilia Garraffo,Robert Ghrist,Rafael Gomez-Bombarelli,Gianluca Guadagni,Sreelekha Guggilam,Sergei Gukov,Juan B. Gutiérrez,Salman Habib,Johannes Hachmann,Boris Hanin,Philip Harris,Murray Holland,Elizabeth Holm,Hsin-Yuan Huang,Shih-Chieh Hsu,Nick Jackson,Olexandr Isayev,Heng Ji,Aggelos Katsaggelos,Jeremy Kepner,Yannis Kevrekidis,Michelle Kuchera,J. Nathan Kutz,Branislava Lalic,Ann Lee,Matt LeBlanc,Josiah Lim,Rebecca Lindsey,Yongmin Liu,Peter Y. Lu,Sudhir Malik,Vuk Mandic,Vidya Manian,Emeka P. Mazi,Pankaj Mehta,Peter Melchior,Brice Ménard,Jennifer Ngadiuba,Stella Offner,Elsa Olivetti,Shyue Ping Ong,Christopher Rackauckas,Philippe Rigollet,Chad Risko,Philip Romero,Grant Rotskoff,Brett Savoie,Uros Seljak,David Shih,Gary Shiu,Dima Shlyakhtenko,Eva Silverstein,Taylor Sparks,Thomas Strohmer,Christopher Stubbs,Stephen Thomas,Suriyanarayanan Vaikuntanathan,Rene Vidal,Francisco Villaescusa-Navarro,Gregory Voth,Benjamin Wandelt,Rachel Ward,Melanie Weber,Risa Wechsler,Stephen Whitelam,Olaf Wiest,Mike Williams,Zhuoran Yang,Yaroslava G. Yingling,Bin Yu,Shuwen Yue,Ann Zabludoff,Huimin Zhao,Tong Zhang*

Main category: cs.AI

TL;DR: AI和数学与物理科学（MPS）领域（天文学、化学、材料研究、数学科学和物理学）的融合日益紧密，本论文旨在为MPS社区提供一个关于如何利用和贡献AI未来的视角，并提出加强AI与科学联系的战略。


<details>
  <summary>Details</summary>
Motivation: 为了理解数学与物理科学（MPS）领域如何最好地利用和贡献于人工智能（AI）的未来，由NSF AI与MPS研讨会发展而来，旨在总结MPS社区对AI与科学交叉领域的看法。

Method: 提出了三项活动和战略重点：（1）促进双向的AI+MPS研究；（2）建立跨学科的AI+MPS研究者社区；（3）培养面向MPS研究者和学生的AI教育和劳动力发展。

Result: 总结了对资助机构、教育机构和研究人员的建议，以帮助MPS社区在AI+MPS领域发挥领导作用并充分利用其变革潜力。

Conclusion: MPS社区应采取积极主动的策略，利用AI进行科学发现，并利用基础科学概念来影响AI的发展，以实现AI与科学的协同进化。

Abstract: This community paper developed out of the NSF Workshop on the Future of
Artificial Intelligence (AI) and the Mathematical and Physics Sciences (MPS),
which was held in March 2025 with the goal of understanding how the MPS domains
(Astronomy, Chemistry, Materials Research, Mathematical Sciences, and Physics)
can best capitalize on, and contribute to, the future of AI. We present here a
summary and snapshot of the MPS community's perspective, as of Spring/Summer
2025, in a rapidly developing field. The link between AI and MPS is becoming
increasingly inextricable; now is a crucial moment to strengthen the link
between AI and Science by pursuing a strategy that proactively and thoughtfully
leverages the potential of AI for scientific discovery and optimizes
opportunities to impact the development of AI by applying concepts from
fundamental science. To achieve this, we propose activities and strategic
priorities that: (1) enable AI+MPS research in both directions; (2) build up an
interdisciplinary community of AI+MPS researchers; and (3) foster education and
workforce development in AI for MPS researchers and students. We conclude with
a summary of suggested priorities for funding agencies, educational
institutions, and individual researchers to help position the MPS community to
be a leader in, and take full advantage of, the transformative potential of
AI+MPS.

</details>


### [88] [Deep Research is the New Analytics System: Towards Building the Runtime for AI-Driven Analytics](https://arxiv.org/abs/2509.02751)
*Matthew Russo,Tim Kraska*

Main category: cs.AI

TL;DR: LLM驱动的数据分析系统结合了语义算子和深度研究系统的优点，通过优化执行来提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM驱动的数据分析系统要么执行成本高昂，要么查询计划未经优化，无法满足交互式分析的需求。

Method: 构建了一个原型系统，使深度研究代理能够编写和执行优化的语义算子程序。

Result: 该原型系统在两个基本查询上优于手工编写的语义算子程序和开放的深度研究系统，F1分数最高可提高1.95倍，成本和运行时间节省高达76.8%和72.7%。

Conclusion: 该原型系统是实现结合了优化执行和动态执行的AI驱动分析运行时的一个重要步骤。

Abstract: With advances in large language models (LLMs), researchers are creating new
systems that can perform AI-driven analytics over large unstructured datasets.
Recent work has explored executing such analytics queries using semantic
operators -- a declarative set of AI-powered data transformations with natural
language specifications. However, even when optimized, these operators can be
expensive to execute on millions of records and their iterator execution
semantics make them ill-suited for interactive data analytics tasks. In another
line of work, Deep Research systems have demonstrated an ability to answer
natural language question(s) over large datasets. These systems use one or more
LLM agent(s) to plan their execution, process the dataset(s), and iteratively
refine their answer. However, these systems do not explicitly optimize their
query plans which can lead to poor plan execution. In order for AI-driven
analytics to excel, we need a runtime which combines the optimized execution of
semantic operators with the flexibility and more dynamic execution of Deep
Research systems. As a first step towards this vision, we build a prototype
which enables Deep Research agents to write and execute optimized semantic
operator programs. We evaluate our prototype and demonstrate that it can
outperform a handcrafted semantic operator program and open Deep Research
systems on two basic queries. Compared to a standard open Deep Research agent,
our prototype achieves up to 1.95x better F1-score. Furthermore, even if we
give the agent access to semantic operators as tools, our prototype still
achieves cost and runtime savings of up to 76.8% and 72.7% thanks to its
optimized execution.

</details>


### [89] [Planning with Reasoning using Vision Language World Model](https://arxiv.org/abs/2509.02722)
*Delong Chen,Theo Moutakanni,Willy Chung,Yejin Bang,Ziwei Ji,Allen Bolourchi,Pascale Fung*

Main category: cs.AI

TL;DR: VLWM是一个用于视频的语言世界模型，可以进行规划。


<details>
  <summary>Details</summary>
Motivation: 现有的高层世界模型在理解和推理具有语义和时间抽象的动作方面仍不完善。

Method: VLWM通过分析视频来推断目标、预测动作和状态变化，并利用LLM Self-Refine和Tree of Captions技术进行优化。它学习动作策略和动力学模型，支持快速规划（system-1）和反思性规划（system-2），其中system-2通过成本最小化进行改进。

Result: VLWM在VPA基准和PlannerArena人类评估中均达到最先进的性能，system-2比system-1提高了27%的Elo分数。此外，VLWM在RoboVQA和WorldPrediction基准上也优于其他VLM模型。

Conclusion: VLWM通过结合语言模型和视觉信息，在规划任务中取得了显著的进步，并为未来的研究奠定了基础。

Abstract: Effective planning requires strong world models, but high-level world models
that can understand and reason about actions with semantic and temporal
abstraction remain largely underdeveloped. We introduce the Vision Language
World Model (VLWM), a foundation model trained for language-based world
modeling on natural videos. Given visual observations, the VLWM first infers
the overall goal achievements then predicts a trajectory composed of
interleaved actions and world state changes. Those targets are extracted by
iterative LLM Self-Refine conditioned on compressed future observations
represented by Tree of Captions. The VLWM learns both an action policy and a
dynamics model, which respectively facilitates reactive system-1 plan decoding
and reflective system-2 planning via cost minimization. The cost evaluates the
semantic distance between the hypothetical future states given by VLWM
roll-outs and the expected goal state, and is measured by a critic model that
we trained in a self-supervised manner. The VLWM achieves state-of-the-art
Visual Planning for Assistance (VPA) performance on both benchmark evaluations
and our proposed PlannerArena human evaluations, where system-2 improves the
Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM
baselines on RoboVQA and WorldPrediction benchmark.

</details>


### [90] [Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving](https://arxiv.org/abs/2509.02754)
*Mingyi Wang,Jingke Wang,Tengju Ye,Junbo Chen,Kaicheng Yu*

Main category: cs.AI

TL;DR: LLM的组件可以被有效迁移到自动驾驶运动生成任务中，但需要针对性地进行调整和优化。


<details>
  <summary>Details</summary>
Motivation: LLM在自然语言处理领域取得了显著进展，其在序列建模、表征和决策制定方面的能力，启发了研究者将其应用于自动驾驶运动生成等结构相似的领域。然而，目前对于哪些LLM组件能够真正迁移以及如何迁移，仍缺乏系统的理解。

Method: 本文系统性地评估了五种关键的LLM组件（分词器设计、位置嵌入、预训练范式、后训练策略和测试时计算）在自动驾驶运动生成任务中的适用性。通过在Waymo Sim Agents基准上的大量实验，验证了这些组件在经过适当调整后能显著提升运动生成性能。

Result: 通过大量实验，我们证明了LLM的五个关键组件（分词器设计、位置嵌入、预训练范式、后训练策略和测试时计算）在经过适当调整后，能够显著提升自动驾驶运动生成任务的性能。我们识别了哪些技术可以有效迁移，分析了其他技术失败的潜在原因，并讨论了针对自动驾驶场景所需的具体调整。

Conclusion: LLM的组件在经过适当调整后，可以有效地应用于自动驾驶运动生成任务，并取得有竞争力的结果。未来的工作应继续探索LLM在自动驾驶领域的应用潜力，并关注更深层次的迁移和优化策略。

Abstract: Recent breakthroughs in large language models (LLMs) have not only advanced
natural language processing but also inspired their application in domains with
structurally similar problems--most notably, autonomous driving motion
generation. Both domains involve autoregressive sequence modeling, token-based
representations, and context-aware decision making, making the transfer of LLM
components a natural and increasingly common practice. However, despite
promising early attempts, a systematic understanding of which LLM modules are
truly transferable remains lacking. In this paper, we present a comprehensive
evaluation of five key LLM modules--tokenizer design, positional embedding,
pre-training paradigms, post-training strategies, and test-time
computation--within the context of motion generation for autonomous driving.
Through extensive experiments on the Waymo Sim Agents benchmark, we demonstrate
that, when appropriately adapted, these modules can significantly improve
performance for autonomous driving motion generation. In addition, we identify
which techniques can be effectively transferred, analyze the potential reasons
for the failure of others, and discuss the specific adaptations needed for
autonomous driving scenarios. We evaluate our method on the Sim Agents task and
achieve competitive results.

</details>


### [91] [Plan Verification for LLM-Based Embodied Task Completion Agents](https://arxiv.org/abs/2509.02761)
*Ananth Hariharan,Vardhan Dongre,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.AI

TL;DR: LLM生成的任务计划和人类演示可能存在噪声，通过迭代验证框架，由Judge LLM进行批评，Planner LLM进行修订，从而产生更清洁、更具空间连贯性的轨迹。


<details>
  <summary>Details</summary>
Motivation: LLM生成的任务计划和人类演示可能存在噪声，影响策略质量。

Method: 提出一个迭代验证框架，其中Judge LLM批评动作序列，Planner LLM应用修订。

Result: 在TEACh数据集上手动标注的动作上，该框架在四个最先进的LLM上实现了高达90%的召回率和100%的精确率，并且迭代次数少，提高了时间和空间效率，同时保留了人类的错误恢复模式。

Conclusion: 该方法为生成更高质量的模仿学习训练数据提供了可扩展的途径。

Abstract: Large language model (LLM) based task plans and corresponding human
demonstrations for embodied AI may be noisy, with unnecessary actions,
redundant navigation, and logical errors that reduce policy quality. We propose
an iterative verification framework in which a Judge LLM critiques action
sequences and a Planner LLM applies the revisions, yielding progressively
cleaner and more spatially coherent trajectories. Unlike rule-based approaches,
our method relies on natural language prompting, enabling broad generalization
across error types including irrelevant actions, contradictions, and missing
steps. On a set of manually annotated actions from the TEACh embodied AI
dataset, our framework achieves up to 90% recall and 100% precision across four
state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).
The refinement loop converges quickly, with 96.5% of sequences requiring at
most three iterations, while improving both temporal efficiency and spatial
action organization. Crucially, the method preserves human error-recovery
patterns rather than collapsing them, supporting future work on robust
corrective behavior. By establishing plan verification as a reliable LLM
capability for spatial planning and action refinement, we provide a scalable
path to higher-quality training data for imitation learning in embodied AI.

</details>


### [92] [Key Principles in Cross-Domain Hyper-Heuristic Performance](https://arxiv.org/abs/2509.02782)
*Václav Sobotka,Lucas Kletzander,Nysret Musliu,Hana Rudová*

Main category: cs.AI

TL;DR: 现有的交叉域选择超启发式方法主要关注从预定集合中选择低级启发式算法（LLHs），而本文则侧重于该集合的构成及其战略性转换。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究如何通过战略性地转换低级启发式算法（LLHs）的集合来改进交叉域选择超启发式方法。

Method: 本文系统地分析了基于解决方案接受、LLH重复和扰动强度三个关键原则的转换方法，并将其应用于一个简单化的随机选择机制上，同时也将这些策略性转换应用于几种近期的超启发式算法。

Result: 通过适当构建的转换方法，即使是简单的随机选择机制也超越了现有最先进的超启发式方法，并在三个具有挑战性的真实世界领域发现了11个新的最佳解决方案。此外，本文的方法在CHeSC基准测试中也具有竞争力，并超越了当前最先进的方法，同时常常简化了它们的设计。

Conclusion: 通过对LLH集合的构成和战略性转换的研究，可以显著提升超启发式方法的性能，甚至超越最先进的方法，同时还能简化算法设计。

Abstract: Cross-domain selection hyper-heuristics aim to distill decades of research on
problem-specific heuristic search algorithms into adaptable general-purpose
search strategies. In this respect, existing selection hyper-heuristics
primarily focus on an adaptive selection of low-level heuristics (LLHs) from a
predefined set. In contrast, we concentrate on the composition of this set and
its strategic transformations. We systematically analyze transformations based
on three key principles: solution acceptance, LLH repetitions, and perturbation
intensity, i.e., the proportion of a solution affected by a perturbative LLH.
We demonstrate the raw effects of our transformations on a trivial unbiased
random selection mechanism. With an appropriately constructed transformation,
this trivial method outperforms all available state-of-the-art hyper-heuristics
on three challenging real-world domains and finds 11 new best-known solutions.
The same method is competitive with the winner of the CHeSC competition,
commonly used as the standard cross-domain benchmark. Moreover, we accompany
several recent hyper-heuristics with such strategic transformations. Using this
approach, we outperform the current state-of-the-art methods on both the CHeSC
benchmark and real-world domains while often simplifying their designs.

</details>


### [93] [Learning General Policies From Examples](https://arxiv.org/abs/2509.02794)
*Blai Bonet,Hector Geffner*

Main category: cs.AI

TL;DR: 该方法提出了一种基于采样计划泛化的新符号学习策略，解决了传统组合方法可扩展性差的问题。


<details>
  <summary>Details</summary>
Motivation: 学习能解决大规模规划问题的通用策略，并解决现有组合方法在处理大规模问题时的可扩展性差的弱点。

Method: 提出一种基于打集算法的符号学习策略，该算法能够处理数百万个状态和数十万个特征的问题，并确保结构终止和无环性。

Result: 所提出的方法在处理大规模问题时表现出良好的可扩展性，并在多个基准测试中得到了验证。

Conclusion: 该方法通过基于打集算法的采样计划泛化，有效解决了组合方法在处理大规模规划问题时的可扩展性问题，并保证了学习策略的正确性。

Abstract: Combinatorial methods for learning general policies that solve large
collections of planning problems have been recently developed. One of their
strengths, in relation to deep learning approaches, is that the resulting
policies can be understood and shown to be correct. A weakness is that the
methods do not scale up and learn only from small training instances and
feature pools that contain a few hundreds of states and features at most. In
this work, we propose a new symbolic method for learning policies based on the
generalization of sampled plans that ensures structural termination and hence
acyclicity. The proposed learning approach is not based on SAT/ASP, as previous
symbolic methods, but on a hitting set algorithm that can effectively handle
problems with millions of states, and pools with hundreds of thousands of
features. The formal properties of the approach are analyzed, and its
scalability is tested on a number of benchmarks.

</details>


### [94] [Uncertainty-driven Adaptive Exploration](https://arxiv.org/abs/2509.03219)
*Leonidas Bakopoulos,Georgios Chalkiadakis*

Main category: cs.AI

TL;DR: 该研究提出了一个基于不确定性的通用自适应探索框架，用于解决在需要学习长而复杂动作序列的领域中，在探索和利用之间进行切换的时机问题。该框架将先前的方法作为特例，并允许集成任何不确定性度量机制，实验证明其在MuJoCo环境中优于标准方法。


<details>
  <summary>Details</summary>
Motivation: 在需要学习长而复杂动作序列的领域中，确定在探索和利用之间切换的合适时机是自适应探索方法中的一个关键问题。

Method: 提出一个包含先前自适应探索方法作为特例的通用自适应探索框架，该框架使用不确定性来度量切换时机，并允许集成任何不确定性度量机制（例如，内在动机或认知不确定性）。

Result: 实验表明，该框架生成的自适应探索策略在多个MuJoCo环境中优于标准方法。

Conclusion: 所提出的基于不确定性的通用自适应探索框架能够有效地解决探索-利用切换的时机问题，并能在实际应用中取得优于现有方法的性能。

Abstract: Adaptive exploration methods propose ways to learn complex policies via
alternating between exploration and exploitation. An important question for
such methods is to determine the appropriate moment to switch between
exploration and exploitation and vice versa. This is critical in domains that
require the learning of long and complex sequences of actions. In this work, we
present a generic adaptive exploration framework that employs uncertainty to
address this important issue in a principled manner. Our framework includes
previous adaptive exploration approaches as special cases. Moreover, we can
incorporate in our framework any uncertainty-measuring mechanism of choice, for
instance mechanisms used in intrinsic motivation or epistemic uncertainty-based
exploration methods. We experimentally demonstrate that our framework gives
rise to adaptive exploration strategies that outperform standard ones across
several MuJoCo environments.

</details>


### [95] [Accountability Framework for Healthcare AI Systems: Towards Joint Accountability in Decision Making](https://arxiv.org/abs/2509.03286)
*Prachi Bagave,Marcus Westberg,Marijn Janssen,Aaron Yi Ding*

Main category: cs.AI

TL;DR: AI在医疗保健领域日益重要，但其驱动的决策的问责制是一个关键问题。本文通过分析和提出一个问责框架及三层结构，填补了“是什么”和“如何做”之间的空白，以应对医疗保健AI的问责挑战。


<details>
  <summary>Details</summary>
Motivation: AI在医疗保健领域的应用日益广泛，但对AI驱动的医疗决策的问责是一个关键问题。现有的监管指南过于笼统，缺乏具体的操作方法，导致了知识鸿沟。

Method: 通过广泛分析，研究人员发现“问责制”的含义因从业者的专业知识和工作领域而异。本文提出一个问责框架，并将医疗保健AI系统的监管和从业者采取的机制置于一个统一的问责制度下。此外，还提供了一个三层结构，用于处理各种问责机制。

Result: 提出的问责框架将监管和机制整合到一个统一的制度中。三层结构指导从业者根据其行为对机制进行分类。分析强调了可解释性在促进沟通和信息共享中的作用，以促进协作。

Conclusion: 医疗保健AI的决策具有共享的依赖性，问责制应由各方共同承担，并促进协作。可解释性在促进各方之间的沟通和信息共享方面发挥着关键作用，有助于协作过程。

Abstract: AI is transforming the healthcare domain and is increasingly helping
practitioners to make health-related decisions. Therefore, accountability
becomes a crucial concern for critical AI-driven decisions. Although regulatory
bodies, such as the EU commission, provide guidelines, they are highlevel and
focus on the ''what'' that should be done and less on the ''how'', creating a
knowledge gap for actors. Through an extensive analysis, we found that the term
accountability is perceived and dealt with in many different ways, depending on
the actor's expertise and domain of work. With increasing concerns about AI
accountability issues and the ambiguity around this term, this paper bridges
the gap between the ''what'' and ''how'' of AI accountability, specifically for
AI systems in healthcare. We do this by analysing the concept of
accountability, formulating an accountability framework, and providing a
three-tier structure for handling various accountability mechanisms. Our
accountability framework positions the regulations of healthcare AI systems and
the mechanisms adopted by the actors under a consistent accountability regime.
Moreover, the three-tier structure guides the actors of the healthcare AI
system to categorise the mechanisms based on their conduct. Through our
framework, we advocate that decision-making in healthcare AI holds shared
dependencies, where accountability should be dealt with jointly and should
foster collaborations. We highlight the role of explainability in instigating
communication and information sharing between the actors to further facilitate
the collaborative process.

</details>


### [96] [app.build: A Production Framework for Scaling Agentic Prompt-to-App Generation with Environment Scaffolding](https://arxiv.org/abs/2509.03310)
*Evgenii Kniazev,Arseny Kravchenko,Igor Rekun,James Broadhead,Nikita Shamgunov,Pranav Sah,Pratik Nichite,Ivan Yamshchikov*

Main category: cs.AI

TL;DR: app.build是一个开源框架，通过系统验证和结构化环境来改进基于LLM的应用程序生成。它结合了多层验证管道、特定于堆栈的编排和模型无关的架构。在30个生成任务上进行评估，证明了全面的验证可以达到73.3%的可用率，其中30%达到完美的质量分数。开放权重模型在提供结构化环境时，可以达到闭源模型性能的80.8%。该框架已被社区采用，迄今为止已生成超过3000个应用程序。这项工作表明，扩展可靠的AI代理需要扩展环境，而不仅仅是模型，为面向生产的代理系统提供了经验性见解和完整的参考实现。


<details>
  <summary>Details</summary>
Motivation: 改进基于LLM的应用程序生成，通过系统验证和结构化环境来提高可靠性和质量。

Method: 提出并实现了一个名为app.build的开源框架，该框架结合了多层验证管道、特定于堆栈的编排和模型无关的架构。

Result: 在30个生成任务上评估，证明了全面的验证可以达到73.3%的可用率，30%达到完美的质量分数。开放权重模型在提供结构化环境时，可以达到闭源模型性能的80.8%。该框架已被社区广泛采用，迄今已生成超过3000个应用程序。

Conclusion: 扩展可靠的AI代理需要扩展环境，而不仅仅是模型，为面向生产的代理系统提供了经验性见解和完整的参考实现。

Abstract: We present app.build (https://github.com/appdotbuild/agent/), an open-source
framework that improves LLM-based application generation through systematic
validation and structured environments. Our approach combines multi-layered
validation pipelines, stack-specific orchestration, and model-agnostic
architecture, implemented across three reference stacks. Through evaluation on
30 generation tasks, we demonstrate that comprehensive validation achieves
73.3% viability rate with 30% reaching perfect quality scores, while
open-weights models achieve 80.8% of closed-model performance when provided
structured environments. The open-source framework has been adopted by the
community, with over 3,000 applications generated to date. This work
demonstrates that scaling reliable AI agents requires scaling environments, not
just models -- providing empirical insights and complete reference
implementations for production-oriented agent systems.

</details>


### [97] [Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning](https://arxiv.org/abs/2509.03345)
*Yunxin Sun,Abulhair Saparov*

Main category: cs.AI

TL;DR: LLMs在演绎推理方面表现出色，但归纳和溯因推理能力仍有待提高，尤其是在复杂场景下。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在演绎推理，忽视了在现实世界问题解决中至关重要的其他推理类型（如归纳和溯因推理），因此需要评估LLMs在这方面的能力。

Method: 提出InAbHyD数据集，并设计基于奥卡姆剃刀原则的评估指标，用于测试LLMs的归纳和溯因推理能力。

Result: LLMs在简单场景下展现出一定的归纳和溯因推理能力，但在面对复杂世界模型和需要高质量假设时，表现不佳，即使采用上下文学习和RLHF等技术也未能显著改善。

Conclusion: LLMs在归纳和溯因推理方面仍有提升空间，尤其是在处理复杂性时。

Abstract: Reasoning is a core capability in artificial intelligence systems, for which
large language models (LLMs) have recently shown remarkable progress. However,
most work focuses exclusively on deductive reasoning, which is problematic
since other types of reasoning are also essential in solving real-world
problems, and they are less explored. This work focuses on evaluating LLMs'
inductive and abductive reasoning capabilities. We introduce a programmable and
synthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example
consists of an incomplete world model and a set of observations. The task for
the intelligent agent is to produce hypotheses to explain observations under
the incomplete world model to solve each reasoning example. We propose a new
metric to evaluate the quality of hypotheses based on Occam's Razor. We
evaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs
can perform inductive and abductive reasoning in simple scenarios, but struggle
with complex world models and producing high-quality hypotheses, even with
popular reasoning-enhancing techniques such as in-context learning and RLVR.

</details>


### [98] [Situating AI Agents in their World: Aspective Agentic AI for Dynamic Partially Observable Information Systems](https://arxiv.org/abs/2509.03380)
*Peter J. Bentley,Soo Ling Lim,Fuyuki Ishikawa*

Main category: cs.AI

TL;DR: 本研究提出了一种新的 agentic LLM AI 代理框架，通过将代理置于环境中并由环境变化触发行为，解决了现有代理的局限性。该框架引入了“aspects”概念，使代理能够根据其特定的信息视角来感知环境，从而实现更清晰的信息控制和零信息泄露。


<details>
  <summary>Details</summary>
Motivation: 现有的 agentic LLM AI 代理通常只是自主聊天机器人，行为受脚本控制，缺乏可靠性。本研究旨在解决这一问题，提出一种新的框架。

Method: 提出了一种自下而上的框架，将 AI 代理置于其环境中，所有行为都由环境中的变化触发。引入了“aspects”概念，类似于“umwelt”的思想，使代理能够以不同的方式感知环境，从而实现更清晰的信息控制。

Result: 与传统架构相比，本研究提出的 aspectic agentic AI 实现了零信息泄露，而传统架构的泄露率高达 83%。

Conclusion: 本研究提出的 specialist agents 在各自的信息生态位中高效工作的概念，有望在安全性和效率方面带来改进。

Abstract: Agentic LLM AI agents are often little more than autonomous chatbots: actors
following scripts, often controlled by an unreliable director. This work
introduces a bottom-up framework that situates AI agents in their environment,
with all behaviors triggered by changes in their environments. It introduces
the notion of aspects, similar to the idea of umwelt, where sets of agents
perceive their environment differently to each other, enabling clearer control
of information. We provide an illustrative implementation and show that
compared to a typical architecture, which leaks up to 83% of the time,
aspective agentic AI enables zero information leakage. We anticipate that this
concept of specialist agents working efficiently in their own information
niches can provide improvements to both security and efficiency.

</details>


### [99] [ANNIE: Be Careful of Your Robots](https://arxiv.org/abs/2509.03383)
*Yiyang Huang,Zixuan Wang,Zishen Wan,Yapeng Tian,Haobo Xu,Yinhe Han,Yiming Gan*

Main category: cs.AI

TL;DR: EAI系统中的VLA模型存在安全风险，可能导致危险的物理行为。本研究提出了首个针对EAI系统的对抗性安全攻击研究，包括安全违规分类、安全评估基准ANNIEBench和攻击框架ANNIE-Attack。实验表明攻击成功率超过50%，并验证了物理机器人的实际影响，凸显了物理AI时代安全防御的必要性。


<details>
  <summary>Details</summary>
Motivation: EAI系统中的VLA模型可能将对抗性扰动转化为不安全的物理行为，传统的安全方法不足以应对这些新出现的安全风险，需要系统研究EAI系统的安全性。

Method: 1. 基于物理约束（如距离、速度、碰撞边界）制定了安全违规分类（关键、危险、风险）。2. 提出了ANNIEBench基准，包含9个安全关键场景和2400个视频-动作序列。3. 开发了ANNIE-Attack框架，使用任务感知的攻击领导模型将长期目标分解为帧级扰动。

Result: 在代表性EAI模型上的评估显示，所有安全类别的攻击成功率均超过50%。研究还展示了稀疏和自适应攻击策略，并通过物理机器人实验验证了实际影响。

Conclusion: EAI系统存在一个先前未被充分认识但后果严重的攻击面，凸显了在物理AI时代进行安全驱动防御的紧迫性。

Abstract: The integration of vision-language-action (VLA) models into embodied AI (EAI)
robots is rapidly advancing their ability to perform complex, long-horizon
tasks in humancentric environments. However, EAI systems introduce critical
security risks: a compromised VLA model can directly translate adversarial
perturbations on sensory input into unsafe physical actions. Traditional safety
definitions and methodologies from the machine learning community are no longer
sufficient. EAI systems raise new questions, such as what constitutes safety,
how to measure it, and how to design effective attack and defense mechanisms in
physically grounded, interactive settings. In this work, we present the first
systematic study of adversarial safety attacks on embodied AI systems, grounded
in ISO standards for human-robot interactions. We (1) formalize a principled
taxonomy of safety violations (critical, dangerous, risky) based on physical
constraints such as separation distance, velocity, and collision boundaries;
(2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with
2,400 video-action sequences for evaluating embodied safety; and (3)
ANNIE-Attack, a task-aware adversarial framework with an attack leader model
that decomposes long-horizon goals into frame-level perturbations. Our
evaluation across representative EAI models shows attack success rates
exceeding 50% across all safety categories. We further demonstrate sparse and
adaptive attack strategies and validate the real-world impact through physical
robot experiments. These results expose a previously underexplored but highly
consequential attack surface in embodied AI systems, highlighting the urgent
need for security-driven defenses in the physical AI era. Code is available at
https://github.com/RLCLab/Annie.

</details>


### [100] [sam-llm: interpretable lane change trajectoryprediction via parametric finetuning](https://arxiv.org/abs/2509.03462)
*Zhuo Cao,Yunxiao Shi,Min Xu*

Main category: cs.AI

TL;DR: SAM-LLM是一个结合大型语言模型（LLM）和运动学模型的混合架构，用于可解释的自动驾驶变道轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 为了弥合大型语言模型（LLM）的上下文推理能力与用于自动驾驶的运动学车道变换模型的物理精度之间的差距，并提供可解释的变道轨迹预测。

Method: 通过微调LLM来输出轨迹模型的核心物理参数（如横向位移、变道时长、初始横向速度、纵向速度变化），而不是原始坐标。在车道保持场景下，模型预测离散坐标；在变道场景下，模型生成增强的正弦加速度模型（SAM）的参数。

Result: 该方法生成的轨迹模型是完整、连续且物理上合理的，输出尺寸相比基于坐标的方法减少了80%。SAM-LLM在意图预测方面达到了98.73%的准确率，在可解释性和资源效率方面具有显著优势。

Conclusion: SAM-LLM在保持与传统LLM预测器相当的性能的同时，通过生成轨迹模型的物理参数，实现了高准确率、可解释性和资源效率。

Abstract: This work introduces SAM-LLM, a novel hybrid architecture that bridges the
gap between the contextual reasoning of Large Language Models (LLMs) and the
physical precision of kinematic lane change models for autonomous driving. The
system is designed for interpretable lane change trajectory prediction by
finetuning an LLM to output the core physical parameters of a trajectory model
instead of raw coordinates. For lane-keeping scenarios, the model predicts
discrete coordinates, but for lane change maneuvers, it generates the
parameters for an enhanced Sinusoidal Acceleration Model (SAM), including
lateral displacement, maneuver duration, initial lateral velocity, and
longitudinal velocity change. This parametric approach yields a complete,
continuous, and physically plausible trajectory model that is inherently
interpretable and computationally efficient, achieving an 80% reduction in
output size compared to coordinate-based methods. The SAM-LLM achieves a
state-of-the-art overall intention prediction accuracy of 98.73%, demonstrating
performance equivalent to traditional LLM predictors while offering significant
advantages in explainability and resource efficiency.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [101] [Quantum anomalous Hall effect with high Chern number in two dimensional ferromagnets Ti2TeSO](https://arxiv.org/abs/2509.02654)
*Panjun Feng,Miao Gao,Xun-Wang Yan,Fengjie Ma*

Main category: cond-mat.mes-hall

TL;DR: 提出了一种新型二维铁磁半-塞曼-外尔金属单层Ti2TeSO，其大多数自旋通道在费米能级仅具有一对受对称性保护的外尔点，而少数自旋通道的状态远离费米能级。


<details>
  <summary>Details</summary>
Motivation: 现有的二维陈绝缘体在自旋纠缠和仅有一个边带（陈数C=1）方面存在挑战，限制了其实际应用。本研究旨在提出一种具有优异电子特性的新型材料，以克服这些限制。

Method: 利用第一性原理计算和蒙特卡洛模拟。

Result: 单层Ti2TeSO在包含自旋轨道耦合后，在外尔点处产生了约92.8 meV的带隙，并出现了双耗散无旋手性边缘通道和2e2/h的量子化霍尔电导平台，证明其为C=2的高陈数绝缘体。此外，通过应变和磁化方向的调控，可以实现并控制谷极化。计算得到的居里温度为161 K。

Conclusion: 单层Ti2TeSO有望为多通道耗散无旋输运设备和非易失性多状态存储器架构的开发奠定基础。

Abstract: Two-dimensional Chern insulators have emerged as crucial platforms for the
realization of the quantum anomalous Hall effect, and as such have attracted
significant interest in spintronics and topological quantum physics due to
their unique coexistence of spontaneous magnetization and nontrivial
topological characteristics. Nonetheless, substantial challenges persist in
such systems, encompassing spin entanglement and the possession of only one
edge state (Chern number C=1), which significantly hinder their practical
applications. Herein, we propose a novel two-dimensional ferromagnetic
half-semi-Weyl-metal, monolayer Ti2TeSO, that exhibits exceptional electronic
properties. Its majority spin channel possesses only a pair of
symmetry-protected Weyl points at the Fermi level, while the states of minority
one locate far away from the Fermi level. When spin-orbit coupling is included,
a substantial band gap of ~ 92.8 meV is induced at the Weyl points. Remarkably,
the emergence of dual dissipationless chiral edge channels and a quantized Hall
conductivity plateau at 2e2/h collectively establish monolayer Ti2TeSO as a
high-Chern-number insulator with C=2. Furthermore, it is demonstrated that
valley polarization can be achieved and controlled through the application of
strain and the manipulation of the direction of magnetization. The
first-principles calculations, in conjunction with Monte Carlo simulations,
yield a Curie temperature of 161 K for monolayer Ti2TeSO, thereby indicating
the plausibility of coexistence of valley polarization and topological states
at elevated temperatures. These findings could provide a foundation for the
development of multi-channels dissipationless transport devices and nonvolatile
multistate memory architectures.

</details>


### [102] [Ultrafast anisotropic exciton transport in phosphorene](https://arxiv.org/abs/2509.02682)
*Kai-Wei Chang,Joshua J. P. Thompson,Bartomeu Monserrat*

Main category: cond-mat.mes-hall

TL;DR: 文章研究了磷烯的结构各向异性对其激子光学、动力学和输运性质的影响。


<details>
  <summary>Details</summary>
Motivation: 磷烯是一种具有强面内结构各向异性的二维材料，理解其各向异性对激子性质的影响对于设计新型光电器件至关重要。

Method: 结合微观多体理论和第一性原理计算，对磷烯的激子态和激子-声子相互作用进行建模，并定量评估光学吸收光谱、激子弛豫和激子输运。

Result: 研究揭示了磷烯中激子性质的依赖于方向的特征，并发现长程交换相互作用在低温下显著增强了激子扩散的各向异性。

Conclusion: 该工作深入理解了各向异性二维材料中的激子动力学，为设计下一代光电器件提供了指导原则。

Abstract: Phosphorene is a two-dimensional (2D) material exhibiting strong in-plane
structural anisotropy. In this work, we investigate the influence of structural
anisotropy on the optics, dynamics, and transport of excitons in phosphorene by
combining microscopic many-body theory with first principles calculations. Our
framework offers a complete and material specific description of the excitonic
properties of phosphorene, including exciton states and exciton-phonon
interactions, which allow us to quantitatively evaluate the optical absorption
spectra, exciton relaxation, and exciton transport, revealing
direction-dependent characteristics. Interestingly, we identify the critical
role of long-range exchange interactions, which significantly enhance the
anisotropy of exciton diffusion, particularly at low temperatures. Our work
provides fundamental insights into exciton dynamics in an intrinsically
anisotropic 2D material, offering guiding principles for the design of
next-generation optoelectronic devices.

</details>


### [103] [Quantum Transport in Ultrahigh-Conductivity Carbon Nanotube Fibers](https://arxiv.org/abs/2509.02763)
*Shengjie Yu,Natsumi Komatsu,Liyang Chen,Joe F. Khoury,Nicolas Marquez Peraca,Xinwei Li,Oliver S. Dewey,Lauren W. Taylor,Ali Mojibpour,Yingru Song,Geoff Wehmeyer,Matteo Pasquali,Matthew S. Foster,Douglas Natelson,Junichiro Kono*

Main category: cond-mat.mes-hall

TL;DR: 碳纳米管纤维表现出金属性，在低温下出现弱局域化效应，通过混合3D+1D模型解释了电子传输的维度和量子干涉，适用于柔性输电。


<details>
  <summary>Details</summary>
Motivation: 研究溶液纺丝制备的对齐碳纳米管(CNT)纤维中的量子输运，特别是结构维度和量子干涉效应的作用。

Method: 分析了碳纳米管纤维在不同温度和磁场下的电导率和磁电导率，并使用一维、二维和三维的弱局域化(WL)模型以及提出的混合3D+1D WL模型进行拟合。

Result: 研究发现，碳纳米管纤维在低温下表现出与弱局域化相关的电导率下降。混合3D+1D WL模型能够定量地拟合实验数据，表明电子传输是在三维网络中嵌入的准一维束中进行的。

Conclusion: 提出的混合3D+1D WL模型能够物理地解释碳纳米管纤维中的相干输运，为提高柔性、轻质输电应用的电导率提供了见解。

Abstract: We investigate quantum transport in aligned carbon nanotube (CNT) fibers
fabricated via solution spinning, focusing on the roles of structural
dimensionality and quantum interference effects. The fibers exhibit metallic
behavior at high temperatures, with conductivity increasing monotonically as
the temperature decreases from room temperature to approximately 36 K. Below
this temperature, the conductivity gradually decreases with further cooling,
signaling the onset of quantum conductance corrections associated with
localization effects. Magnetoconductance measurements in both parallel and
perpendicular magnetic fields exhibit pronounced positive corrections at low
temperatures, consistent with weak localization (WL). To determine the
effective dimensionality of electron transport, we analyzed the data using WL
models in 1D, 2D, and 3D geometries. We found that while the 2D model can
reproduce the field dependence, it lacks physical meaning in the context of our
fiber architecture and requires an unphysical scaling factor to match the
experimental magnitude. By contrast, we developed a hybrid 3D+1D WL framework
that quantitatively captures both the field and temperature dependences using
realistic coherence lengths and a temperature-dependent crossover parameter.
Although this combined model also employs a scaling factor for magnitude
correction, it yields a satisfactory fit, reflecting the hierarchical structure
of CNT fibers in which transport occurs through quasi-1D bundles embedded in a
3D network. Our results establish a physically grounded model of phase-coherent
transport in macroscopic CNT assemblies, providing insights into enhancing
conductivity for flexible, lightweight power transmission applications.

</details>


### [104] [Current-induced molecular dissociation: Topological insulators as robust reaction platforms](https://arxiv.org/abs/2509.02831)
*Erika L. Mehring,Amparo Figueroa,Matias Berdakin,Hernán L. Calvo*

Main category: cond-mat.mes-hall

TL;DR: 拓扑材料在催化领域展现出潜力，名为“拓扑催化”。


<details>
  <summary>Details</summary>
Motivation: 研究拓扑材料表面态在催化中的应用，特别是其在电流驱动下的分子离解能力。

Method: 使用非平衡格林函数方法，比较分子在金属（石墨烯）和拓扑（Kane-Mele）衬底上的电流诱导分子离解行为。

Result: 拓扑衬底比石墨烯具有更强的分子离解能力，这主要归因于边缘态的局域化特性。衬底中的空位缺陷进一步增强了离解能力。

Conclusion: 拓扑保护在非平衡条件下对分子离解起着重要作用，为拓扑材料在催化领域的应用提供了新的机遇。

Abstract: The growing interest in topological materials with symmetry-protected surface
states as catalytic platforms has sparked the emerging field of
\textit{topocatalysis}. As robust transport is one of the key features of
topological insulators, here we explore current-induced molecular dissociation
in a transport setup. Using the non-equilibrium Green's function formalism, we
compare how the occupancies of bonding and antibonding levels, as well as the
associated electronic forces in a diatomic molecule, are affected when the
molecule is coupled to either a metallic (graphene) or a topological
(Kane-Mele) substrate. We find a greater dissociative capability in the
topological substrate than in graphene, a difference mainly attributed to the
localized nature of the edge states. The inclusion of vacancy disorder within
the substrate further enhances this disparity in the dissociative force. Our
findings highlight the role of topological protection in molecular dissociation
under non-equilibrium conditions, pointing to new opportunities for robust
catalysis in topological materials.

</details>


### [105] [Tailored Thermal Transport in Phase Change Materials-Based Nanocomposites through Interfacial Structuring](https://arxiv.org/abs/2509.03356)
*Viktor Mandrolko,Mykola Isaiev*

Main category: cond-mat.mes-hall

TL;DR: 界面几何形状，特别是曲率，会影响界面处的分子排列和热传输。


<details>
  <summary>Details</summary>
Motivation: 研究界面几何形状（曲率）如何影响固液界面处的分子排列和热传输。

Method: 使用原子模拟研究了在结构化二氧化硅基板上受限的十六烷的界面热传输。

Result: 界面曲率会影响分子密度分布和热传输。在平坦和轻微弯曲的表面上，液体表现出表面模板分层，从而提高传热效率。随着曲率的增加，这种有序性会分解，导致密度模式干扰、分子堆积减少和耗竭区。这会导致界面热阻（ITR）系统性增加。结论：界面几何形状、热力学耗散和热传输之间存在直接联系，为热可调纳米结构材料、热界面涂料和相变系统提供了新的设计原则。

Conclusion: 界面几何形状、热力学耗散和热传输之间存在直接联系，为热可调纳米结构材料、热界面涂料和相变系统提供了新的设计原则。

Abstract: Interfacial thermal transport is a critical bottleneck in nanoscale systems,
where heat dissipation and energy efficiency are strongly modulated by
molecular ordering at solid-liquid boundaries. Here, using atomistic
simulations of hexadecane confined by structured silica substrates, we reveal
how interfacial geometry, specifically curvature, governs the density
distribution and thermal transport across the interface. At flat and mildly
curved surfaces, the liquid exhibits surface-templated layering, promoting
efficient heat transfer, which is enhanced with increasing contact surface
area. As curvature increases, this ordering breaks down, giving rise to
interference-like density patterns, reduced molecular packing, and localized
depletion zones. This structural reorganization leads to a systematic increase
in interfacial thermal resistance (ITR), even when the contact area is kept
constant. By decomposing the interface into convex ("hill" of solid) and
concave ("valley" of solid) regions, we find that valleys consistently offer
lower thermal resistance. In contrast, hills act as bottlenecks to heat flow.
Remarkably, we show that the work of adhesion and entropy-related energy losses
scale non-trivially with curvature: while adhesion increases with contact area,
the entropic penalty dominates the total energy change, reflecting
curvature-induced frustration of molecular alignment. These findings unveil a
direct link between surface geometry, thermodynamic dissipation, and heat
transport, offering new design principles for thermally tunable nanostructured
materials, thermal interface coatings, and phase-change systems.

</details>


### [106] [NeuroQD: A Learning-Based Simulation Framework For Quantum Dot Devices](https://arxiv.org/abs/2509.02872)
*Shize Che,Junyu Zhou,Seong Woo Oh,Jonathan Hess,Noah Johnson,Mridul Pushp,Robert Spivey,Anthony Sigillito,Gushu Li*

Main category: cond-mat.mes-hall

TL;DR: 提出一种基于CNN的量子比特模拟新方法，实现实时、高精度模拟，加速了量子计算器件的开发。


<details>
  <summary>Details</summary>
Motivation: 现有量子比特模拟方法精度和速度不足，无法满足真实器件设计需求。

Method: 利用CNN根据栅电压推断量子比特的静电势，实现快速、高精度的模拟。

Result: 模拟速度提升>1000倍，与基于物理的模拟精度>96%，延迟可达毫秒级，与实验测量结果一致。

Conclusion: 该CNN模拟方法能够满足真实器件设计和实验控制的实时性要求，加速量子计算发展。

Abstract: Electron spin qubits in quantum dot devices are promising for scalable
quantum computing. However, architectural support is currently hindered by the
lack of realistic and performant simulation methods for real devices.
Physics-based tools are accurate yet too slow for simulating device behavior in
real-time, while qualitative models miss layout and wafer heterostructure. We
propose a new simulation approach capable of simulating real devices from the
cold-start with real-time performance. Leveraging a key phenomenon observed in
physics-based simulation, we train a compact convolutional neural network (CNN)
to infer the qubit-layer electrostatic potential from gate voltages. Our
GPU-accelerated inference delivers >1000x speedup with >96% agreement to the
physics-based simulation. Integrated into the experiment control stack, the
simulator returns results with millisecond scale latency, reproduces key tuning
features, and yields device behaviors and metrics consistent with measurements
on devices operated at 9 mK.

</details>


### [107] [High-$Q$ membrane resonators using ultra-high-stress crystalline TiN films](https://arxiv.org/abs/2509.02987)
*Yuki Matsuyama,Shotaro Shirai,Ippei Nakamura,Masao Tokunari,Hirotaka Terai,Yuji Hishida,Ryo Sasaki,Yusuke Tominaga,Atsushi Noguchi*

Main category: cond-mat.mes-hall

TL;DR: 使用超高应力晶体TiN制备了高品质因子（Q）的薄膜谐振器，其品质因子达到8.0 x 10^6，内禀品质因子与SiN薄膜谐振器相当，可用于光度和机电设备。


<details>
  <summary>Details</summary>
Motivation: 高品质因子（Q）的机械谐振器对于在量子层面精确传感和控制机械运动至关重要。虽然非晶态材料（如SiN）已被广泛用于通过应力诱导耗散稀释实现高Q机械谐振器，但晶体材料通过结合低内禀损耗和高拉伸应力，在实现更高品质因子方面具有新兴潜力。

Method: 制备了超高应力晶体TiN薄膜谐振器，并测量了其在2.2 K下的品质因子。

Result: 制备的TiN薄膜谐振器表现出超过2.3 GPa的拉伸应力和2.2 K下的Q = 8.0 x 10^6的品质因子。通过估算稀释因子，推断TiN谐振器的内禀品质因子与SiN薄膜谐振器相当。

Conclusion: TiN薄膜谐振器具有超高应力和晶体特性，可作为光度和机电设备的强大工具，提供高度耗散稀释的机械谐振器。

Abstract: High-quality-factor ($Q$) mechanical resonators are essential components for
precise sensing and control of mechanical motion at a quantum level. While
amorphous materials such as SiN have been widely used in high-$Q$ mechanical
resonators utilizing stress-induced dissipation dilution, crystalline materials
have emerging potential to achieve higher quality factors by combining low
intrinsic loss and high tensile stress. In this paper, we demonstrate high-Q
membrane resonators using ultra-high-stress crystalline TiN. Our membrane
resonator exhibits a tensile stress exceeding 2.3 GPa and a quality factor of
$Q = 8.0 \times 10^6$ at 2.2 K. By estimating the dilution factor, we infer
that our TiN resonator has a intrinsic quality factor comparable to that of SiN
membrane resonators. With its ultra-high stress and crystalline properties, our
TiN films can serve as a powerful tool for opto- and electromechanical systems,
offering highly dissipation-diluted mechanical resonators.

</details>


### [108] [Deconfined Quantum Critical Point in Quantum Hall Bilayers](https://arxiv.org/abs/2509.03079)
*Guangyu Yu,Tao Xiang,Zheng Zhu*

Main category: cond-mat.mes-hall

TL;DR: 量子霍尔双层系统存在反禁闭量子临界点（DQCP）。


<details>
  <summary>Details</summary>
Motivation: 尽管理论上和实验上都具有挑战性，但DQCP代表了一种非常规的量子临界性。

Method: 使用大规模变分均匀矩阵乘积态（VUMPS）模拟和精确对角化（ED）技术，分析了基态保真度、低能级谱、激子超流和条纹序参数以及基态能量导数。

Result: 在量子霍尔双层系统中，通过调整层间距，在两种不同的对称性破缺相之间发现了直接且连续的量子相变：在小层间距下是具有自发U(1)对称性破缺的激子超流相，在大的层间距下是具有平移对称性破缺的单向电荷密度波相。

Conclusion: 量子霍尔双层系统是实现和实验探测DQCP的理想平台，并且相互作用可以精确调谐。

Abstract: Deconfined quantum critical points (DQCPs) represent an unconventional class
of quantum criticality beyond the Landau-Ginzburg-Wilson-Fisher paradigm.
Nevertheless, both their theoretical identification and experimental
realization remain challenging. Here we report compelling evidence of a DQCP in
quantum Hall bilayers with half-filled $n=2$ Landau levels in each layer, based
on large-scale variational uniform matrix product state (VUMPS) simulations and
exact diagonalization (ED). By systematically analyzing the ground-state
fidelity, low-lying energy spectra, exciton superfluid and stripe order
parameters, and ground-state energy derivatives, we identify a direct and
continuous quantum phase transition between two distinct symmetry-breaking
phases by tuning the layer separation: an exciton superfluid phase with
spontaneous $U(1)$ symmetry breaking at small separation, and a unidirectional
charge density wave with broken translational symmetry at large separation. Our
results highlight quantum Hall bilayers as an ideal platform for realizing and
experimentally probing DQCPs under precisely tunable interactions.

</details>


### [109] [Effect of Magnetic Anisotropy on Magnetoelastic Waves in Ni/LiNbO3 Hybrid Device](https://arxiv.org/abs/2509.03254)
*Minwoo Yu,Moojune Song,Minseok Kang,Mujin You,Yunyoung Hwang,Albert Min Gyu Park,Byong-Guk Park,Kab-Jin Kim,Junho Suh*

Main category: cond-mat.mes-hall

TL;DR: 本研究通过磁各向异性以及晶体轴共同影响声表面波（SAW）驱动的Ni/LiNbO3混合器件中的磁共振。


<details>
  <summary>Details</summary>
Motivation: 研究磁各向异性和晶体轴如何影响声表面波（SAW）驱动的Ni/LiNbO3混合器件中的磁共振，以期优化自旋-声子器件。

Method: 通过声表面波（SAW）吸收测量和磁光克尔效应（MOKE）测量，研究磁各向异性对Ni/LiNbO3混合器件中声表面波（SAW）吸收的影响。

Result: 结果表明，声表面波（SAW）吸收表现出强烈的各向异性，并且可以通过引入偶极相互作用项来解释。

Conclusion: 研究结果强调了衬底诱导的各向异性和长程偶极效应在声表面波（SAW）-磁振子混合器件中的重要性，并为通过各向异性工程优化器件提供了方向。

Abstract: We study the effects of magnetic anisotropy and crystalline axes in surface
acoustic waves (SAWs) driven magnetic resonances of Ni/LiNbO3 hybrid devices.
SAW absorption from the interaction with magnons in Ni displays a strong
anisotropic dependence on the direction of the applied in-plane magnetic field.
Magnetic anisotropy is further investigated by magneto-optical Kerr effect
measurements to show both uniaxial and biaxial anisotropy components in Ni
films on LiNbO3. By introducing a dipolar interaction term in addition to the
anisotropies, we successfully explain the anisotropic SAW absorption in our
devices. These findings show the importance of substrate-induced anisotropy and
long-range dipolar effects in SAW-magnons hybrid devices and indicate future
directions for optimizing these spin-acoustic devices through comprehensive
anisotropy engineering.

</details>


### [110] [Theory of single molecule NMR detection](https://arxiv.org/abs/2509.03282)
*Baruch Horovitz,Alexander Shnirman*

Main category: cond-mat.mes-hall

TL;DR: 开发了一种检测实时 ESR 数据中核磁共振振荡的理论框架。


<details>
  <summary>Details</summary>
Motivation: 开发一种在实时 ESR 数据中检测核磁共振振荡的理论框架，以解释最近的实验数据。

Method: 研究了单自由基分子和共存的自由基或非自由基分子，通过电荷转移进行，并开发了描述这些情况的主方程。

Result: 在某些参数下，可以观察到尖锐的核磁共振线。

Conclusion: 在所有研究的案例中，超精细张量的非对角项对于观察到尖锐的核磁共振线至关重要。

Abstract: In relation to recent experimental data [1], we develop a theory framework
for demonstrating the feasibility of detecting sharp Nuclear Magnetic Resonance
(NMR) oscillations in a real time ESR data. The procedure is to follow real
time oscillations of the ESR signal measured at a selected frequency of a
hyperfine transition. We study a variety of systems such as a single radical
molecule with one or two hyperfine coupled nuclei or two molecules that coexist
as either radicals or non-radicals, facilitated by charge transfer. We develop
a master equation for describing these scenarios and find parameters for which
a sharp NMR line can be observed. We show that in all cases an off-diagonal
term in the hyperfine tensor is essential for the observation.

</details>


### [111] [Family of Unconventional Superconductivities in Crystalline Graphene](https://arxiv.org/abs/2509.03295)
*Junseok Seo,Armel A. Cotten,Mingchi Xu,Omid Sharifi Sedeh,Henok Weldeyesus,Tonghang Han,Zhengguang Lu,Zhenghan Wu,Shenyong Ye,Wei Xu,Jixiang Yang,Emily Aitken,Prayoga P. Liong,Zach Hadjri,Rasul Gazizulin,Kenji Watanabe,Takashi Taniguchi,Mingda Li,Dominik M. Zumbühl,Long Ju*

Main category: cond-mat.mes-hall

TL;DR: 石墨烯多层结构中发现了多种非常规超导电性，并研究了磁场对其增强、诱导以及抑制的影响。


<details>
  <summary>Details</summary>
Motivation: 研究非常规超导电性，特别是石墨烯多层结构中的超导电性，以探索其超越BCS理论的性质。

Method: 对菱形四层和五层石墨烯进行了输运测量，并研究了磁场（平面内和垂直面外）对其超导电性的影响，以及通过近邻效应引入自旋-轨道耦合。

Result: 观察到多种超导电性，其中三种（SC2-4）表现出由磁场增强或诱导的异常行为。SC2在平面内磁场下增强，SC3在小幅度垂直面外磁场下增强，SC4在平面内磁场下诱导。所有超导电性都对高达8.5特斯拉的平面内磁场具有鲁棒性，远超传统超导体的泡利极限。引入自旋-轨道耦合产生了新的超导电性。

Conclusion: 菱形多层石墨烯是研究非常规超导电性的理想平台，并为工程化非阿贝尔（non-Abelian）准粒子提供了可能性。

Abstract: Unconventional superconductors exhibit multiple broken symmetries and exceed
the range of the Bardeen-Cooper-Schrieffer (BCS) theory. For instance,
time-reversal symmetry can be broken in addition to the gauge symmetry,
resulting in superconductors that can be enhanced or induced by a magnetic
field. However, such unconventional superconductivities are more vulnerable to
impurities than their BCS counterparts, requiring highly ordered and clean
material systems to observe them. Crystalline rhombohedral multilayer graphene
is a promising platform to explore unconventional superconductivity due to its
superior material quality and gate-tunable strong correlation effects. Here we
report transport measurements of rhombohedral tetralayer and pentalayer
graphene, where a spectrum of superconductivities in a clean limit are
observed. Three of them (SC2-4) show highly unusual enhancements by magnetic
fields: 1. SC2 is strengthened by an in-plane field; 2. SC3 is boosted by a
small out-of-plane field; 3. SC4 is induced by an in-plane field. All these
superconductors are robust against an in-plane field up to 8.5 Tesla, exceeding
the Pauli limit of conventional superconductors by tens of times and suggesting
their unconventional nature. Moreover, we observed that proximitized spin-orbit
coupling generates a plethora of new superconductors in the phase diagram,
while maintaining the high quality of bare rhombohedral graphene. Our work
establishes a family of new superconductors in rhombohedral multilayer
graphene, which also provides an ideal platform to engineer non-Abelian
quasiparticles by proximitizing with quantum anomalous Hall states existing in
the same material system.

</details>


### [112] [Noise resilience of two-dimensional Floquet topological phases](https://arxiv.org/abs/2509.03296)
*Balaganchi A. Bhargava,Sanjib Kumar Das,Lukas M. Sieberer,Ion Cosma Fulga*

Main category: cond-mat.mes-hall

TL;DR: 周期性驱动的二维拓扑相具有意想不到的抗噪声能力。


<details>
  <summary>Details</summary>
Motivation: 研究噪声对二维周期性驱动拓扑相的影响，重点关注反常Floquet-Anderson相和无序Floquet-Chern相。

Method: 通过大规模数值模拟演示，并使用基于现象学模型的分析结果支持。

Result: 两种相都表现出对时间噪声的鲁棒性。边缘模式的衰减分为两个阶段：指数衰减，然后是慢速代数衰减（~n^(-1/2)）；而体模式的衰减更快（~n^(-1)）。

Conclusion: 二维Floquet拓扑相是演示Floquet拓扑潜在应用的理想选择，因为实验中不可避免地存在无序和退相干。

Abstract: We study the effect of noise on two-dimensional periodically driven
topological phases, focusing on two examples: the anomalous Floquet-Anderson
phase and the disordered Floquet-Chern phase. Both phases show an unexpected
robustness against timing noise. The noise-induced decay of initially populated
topological edge modes occurs in two stages: At short times, thermalization
among edge modes leads to exponential decay. This is followed by slow algebraic
decay $\sim n^{-1/2}$ with the number of Floquet cycles $n$. The exponent of
$1/2$ is characteristic for one-dimensional diffusion, here occurring along the
direction perpendicular to the edge. In contrast, localized modes in the bulk
exhibit faster decay, $\sim n^{-1}$, corresponding to two-dimensional
diffusion. We demonstrate these behaviors through full-scale numerical
simulations and support our conclusions using analytical results based upon a
phenomenological model. Our findings indicate that two-dimensional Floquet
topological phases are ideal candidates for potential applications of Floquet
topology, given the unavoidable presence of both quenched disorder and
decoherence in experiments.

</details>


### [113] [Investigation of non-Hermitian and Hermitian models of Altermagnets](https://arxiv.org/abs/2509.03320)
*Partha Goswami*

Main category: cond-mat.mes-hall

TL;DR: 绝缘交变磁体（如MnTe）具有自旋构型，其中相反的自旋不仅反平行排列，而且相互旋转。这种排列让人联想到具有自旋倾斜的扭曲反铁磁性。


<details>
  <summary>Details</summary>
Motivation: 研究捕获此类系统基本物理特性的模型哈密顿量，包括Dzyaloshinskii-Moriya和常规交换项、相对论性自旋-轨道耦合以及d波和g波序。

Method: 引入非厄米动力学（通过模拟能量耗散和放大的复势）。研究量子几何张量的行为以及量子反常霍尔效应在拓扑绝缘状态下的出现。

Result: 研究了量子几何张量，并发现了量子反常霍尔效应。

Conclusion: 将研究范围扩大到非厄米金属交变磁体，并关注以对称性破缺的d波和g波序参数为特征的相。

Abstract: Insulating altermagnets like MnTe exhibit spin configurations where opposing
spins are not only aligned antiparallel but also rotated relative to each
other. This is an arrangement reminiscent of antiferromagnetism with a twist of
spin canting. This study investigates a model Hamiltonian that captures the
essential physics of such systems, incorporating key interactions including
Dzyaloshinskii-Moriya and conventional exchange terms, relativistic spin-orbit
coupling, and d-wave and g-wave orderings. Non-Hermitian dynamics are
introduced through complex potentials that simulate energy dissipation and
amplification. The paper delves into the behavior of the quantum geometric
tensor and the emergence of the quantum anomalous Hall effect within the
topologically insulating regime. It also broadens the scope to encompass
non-Hermitian metallic altermagnets, focusing on phases characterized by
symmetry-breaking d-wave and g-wave order parameters.

</details>


### [114] [Magnetic Bloch bands and Weiss oscillations in Dirac mass superlattices](https://arxiv.org/abs/2509.03359)
*A. Anand,R. Egger,A. De Martino*

Main category: cond-mat.mes-hall

TL;DR: 二维狄拉克费米子在一维质量超晶格和垂直磁场中表现出持久的Jackiw-Rebbi模式，具有场依赖的重正化速度。对于周期性情况，采用规范不变投影法得到色散朗道能级，并预测了修正的量子霍尔平台和Weiss样磁导振荡。


<details>
  <summary>Details</summary>
Motivation: 研究一维质量超晶格中的二维狄拉克费米子在垂直磁场下的行为。

Method: 使用孤立和有限畴壁阵列的精确解，以及用于任意场和质量分布的规范不变投影法。

Result: 证明了Jackiw-Rebbi模式的持续存在，其速度得到场依赖的重正化。预测了修正的量子霍尔平台和Weiss样磁导振荡，其振幅减小且相位移动π/2。

Conclusion: 研究结果表明，一维质量超晶格显著影响二维狄拉克费米子的量子霍尔效应和磁导特性。

Abstract: We study two-dimensional Dirac fermions in a one-dimensional mass
superlattice under a perpendicular magnetic field. Using exact solutions for
isolated and finite arrays of domain walls, we demonstrate the persistence of
Jackiw-Rebbi modes with a field-dependent renormalized velocity. For the
periodic case, we adopt a gauge-invariant projection method onto magnetic Bloch
states, valid for arbitrary fields and mass profiles, which yields dispersive
Landau levels, and confirm its accuracy by comparison with finite arrays
spectra. From the miniband spectra we predict modified quantum Hall plateaus
and Weiss-like magnetoconductivity oscillations, characterized by a strongly
reduced amplitude and a $\pi/2$ phase shift compared to electrostatic
superlattices.

</details>


### [115] [Integral $ab$ $initio$/DFT and experimental TDPAC approach enlightening the $aftereffects$ phenomenon: probing electronic properties in $α$-Al$_2$O$_3$:$^{111}$In($\rightarrow$ $^{111}$Cd) at the atomic scale](https://arxiv.org/abs/2509.03460)
*G. N. Darriba,R. Vianden,A. P. Ayala,M. Rentería*

Main category: cond-mat.mes-hall

TL;DR: 该研究结合了理论计算和实验，解释了电子俘获衰变余效（ECAE）现象中动态超精细相互作用（HFI）的起源。


<details>
  <summary>Details</summary>
Motivation: 在氧化物中掺杂$^{111}$In（EC）$ightarrow$)$^{111}$Cd探针原子时，在时间差扰动$\\\gamma$-$\\\gamma$角关联（TDPAC）实验中观察到了ECAE现象，其产生的动态HFI的起源尚不清楚。

Method: 利用积分实验和从头算/DFT方法，分析了ECAE现象中动态HFI的起源。研究表明，动态HFI源于$^{111}$Cd原子核附近的电子环境的动态变化。此外，研究还确定了产生动态HFI的初始电子构型及其相关的电场梯度（EFG）。通过分析B\"averstam等人和Lupascu等人提出的两种分析动态HFI的方法，并进行实验验证，确定了两种方法之间的等效条件。研究还对$^{111}$In($ightarrow$ $^{111}$Cd)-掺杂的$\\\alpha$-Al$_2$O$_3$单晶进行了从头算/DFT研究，并分析了Cd杂质电荷状态与缺陷形成能的关系，以解释意外的TDPAC结果。

Result: 研究成功识别了动态HFI的起源，并量化了其对TDPAC实验的影响。研究还确定了产生动态HFI的电子构型及其相关的EFG。分析结果支持了文献中报道的TDPAC实验结果，并为理解$^{111}$Cd在电子恢复过程中经历的不同电荷态提供了实验依据。

Conclusion: 该研究为理解ECAE现象和动态HFI的起源提供了新的见解，并为进一步研究TDPAC实验和材料科学提供了理论和实验基础。

Abstract: By means of an integral experimental and $ab$ $initio$/DFT approach we
contribute here to enlighten and quantify the origin of dynamic hyperfine
interactions (HFIs) assigned to the electron-capture (EC) decay aftereffects
(ECAE) phenomenon observed in time-differential perturbed $\gamma$-$\gamma$
angular correlation (TDPAC) experiments in oxides doped with ($^{111}$In
(EC)$\rightarrow$)$^{111}$Cd as probe-atom. In previous works [Darriba et al.,
Phys. Rev. B 105, 195201 (2022)] we proposed an $ab$ $initio$ scenario in which
the fluctuating electric-field gradients (EFG) producing the dynamic HFI were
related with fluctuating electronic environments close to the $^{111}$Cd
nucleus, succeeding to identify the environment which produce the final static
EFG when the dynamic ($on-off$) process have stopped. In this work we show that
in addition it is possible to obtain, for each temperature and HFI observed,
the set of initial electronic configurations close to the probe nucleus as well
as their related EFGs among which the system fluctuates to generate these
dynamic HFIs. For this, we demonstrate analytically and checked experimentally
the conditions to stablish the equivalence between the two approaches most used
to analyze this type of dynamic HFIs, proposed by B\"averstam et al. and by
Lupascu et al.. To unravel the unexpected TDPAC results in
$^{111}$In($\rightarrow$ $^{111}$Cd)-implanted $\alpha$-Al$_2$O$_3$ single
crystals reported in the literature, we perform a complete $ab$ $initio$/DFT
study of Cd-doped $\alpha$-Al$_2$O$_3$ semiconductor and a detailed defect
formation energy analysis as a function of the charge state of the Cd impurity.
The presence of an unexpected second interaction was a key factor to provide
experimental support to identify and quantify the different charge states the
$^{111}$Cd atom goes through during its electronic recovery process.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [116] [Structure Transfer: an Inference-Based Calculus for the Transformation of Representations](https://arxiv.org/abs/2509.03249)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.LG

TL;DR: 本文提出了一种名为“结构迁移”的新型演算方法，用于在不同表示系统中转换和选择表示，并确保源表示和目标表示之间的任意指定关系（如语义等价）。


<details>
  <summary>Details</summary>
Motivation: 表示选择对于有效沟通和推理至关重要，但如何设计能够驱动表示转换和选择且不依赖于特定表示系统的技术是一个未解决的难题。

Method: 提出了一种名为“结构迁移”的新型演算方法，该方法利用“模式”来编码表示系统的知识，以确保在不同表示系统之间转换表示时保持信息保存或任何指定的关系。该方法基于“构造空间”理论进行形式化。

Result: 结构迁移演算能够生成目标表示，并确保源表示和目标表示之间的语义等价性等指定关系，适用于多种表示系统。

Conclusion: 结构迁移是一种系统无关的演算方法，可以识别各种实际场景中的替代表示，解决了表示选择和转换的通用性问题。

Abstract: Representation choice is of fundamental importance to our ability to
communicate and reason effectively. A major unsolved problem, addressed in this
paper, is how to devise \textit{representational-system (RS) agnostic}
techniques that drive representation transformation and choice. We present a
novel calculus, called \textit{structure transfer}, that enables representation
transformation across diverse RSs. Specifically, given a \textit{source}
representation drawn from a source RS, the rules of structure transfer allow us
to generate a \textit{target} representation for a target RS. The generality of
structure transfer comes in part from its ability to ensure that the source
representation and the generated target representation satisfy \textit{any}
specified relation (such as semantic equivalence). This is done by exploiting
\textit{schemas}, which encode knowledge about RSs. Specifically, schemas can
express \textit{preservation of information} across relations between any pair
of RSs, and this knowledge is used by structure transfer to derive a structure
for the target representation which ensures that the desired relation holds. We
formalise this using Representational Systems Theory~\cite{raggi2022rst},
building on the key concept of a \textit{construction space}. The abstract
nature of construction spaces grants them the generality to model RSs of
diverse kinds, including formal languages, geometric figures and diagrams, as
well as informal notations. Consequently, structure transfer is a
system-agnostic calculus that can be used to identify alternative
representations in a wide range of practical settings.

</details>


### [117] [StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails](https://arxiv.org/abs/2509.02982)
*Hritik Arasu,Faisal R Jahangiri*

Main category: cs.LG

TL;DR: 该研究提出了一种用于睡眠分期模型的流式、无源测试时域适应（TTA）方法，以提高其在未知生理或记录条件下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有睡眠分期模型在面对新的患者生理特征或记录条件时性能会下降。

Method: 提出了一种结合熵最小化（Tent）、批归一化统计刷新以及熵门控和EMA（指数移动平均）重置机制的TTA方法。该方法可以在几秒钟内完成适应，并且内存占用极小。

Result: 在Sleep-EDF Expanded数据集上，使用单导联EEG，该方法相比于固定基线模型在每个睡眠阶段的指标和Cohen's kappa系数上均取得了显著提升。

Conclusion: 该方法是一种模型无关、无需源数据或患者校准的实用方案，适用于设备端或床边使用，能够有效解决睡眠分期模型在部署时遇到的泛化性问题。

Abstract: Sleep staging models often degrade when deployed on patients with unseen
physiology or recording conditions. We propose a streaming, source-free
test-time adaptation (TTA) recipe that combines entropy minimization (Tent)
with Batch-Norm statistic refresh and two safety rails: an entropy gate to
pause adaptation on uncertain windows and an EMA-based reset to reel back
drift. On Sleep-EDF Expanded, using single-lead EEG (Fpz-Cz, 100 Hz, 30s
epochs; R&K to AASM mapping), we show consistent gains over a frozen baseline
at seconds-level latency and minimal memory, reporting per-stage metrics and
Cohen's k. The method is model-agnostic, requires no source data or patient
calibration, and is practical for on-device or bedside use.

</details>


### [118] [The Lifecycle Principle: Stabilizing Dynamic Neural Networks with State Memory](https://arxiv.org/abs/2509.02575)
*Zichuan Yang*

Main category: cs.LG

TL;DR: 通过引入具有状态记忆的生命周期（LC）原则，一种新的正则化方法可以解决神经元长期失活带来的训练不稳定性问题，从而提高泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 神经元长期失活带来的训练不稳定性问题。

Method: 提出生命周期（LC）原则，通过状态记忆恢复神经元参数至其最后已知有效状态，而非使用随机权重重新初始化。

Result: 实验表明，该方法提高了泛化性和鲁棒性，并且消融研究证实了状态记忆对于提升性能至关重要。

Conclusion: LC原则通过状态记忆解决了神经元长期失活带来的训练不稳定性，提高了模型的泛化性和鲁棒性。

Abstract: I investigate a stronger form of regularization by deactivating neurons for
extended periods, a departure from the temporary changes of methods like
Dropout. However, this long-term dynamism introduces a critical challenge:
severe training instability when neurons are revived with random weights. To
solve this, I propose the Lifecycle (LC) principle, a regularization mechanism
centered on a key innovation: state memory. Instead of re-initializing a
revived neuron, my method restores its parameters to their last known effective
state. This process preserves learned knowledge and avoids destructive
optimization shocks. My theoretical analysis reveals that the LC principle
smooths the loss landscape, guiding optimization towards flatter minima
associated with better generalization. Experiments on image classification
benchmarks demonstrate that my method improves generalization and robustness.
Crucially, ablation studies confirm that state memory is essential for
achieving these gains.

</details>


### [119] [Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection](https://arxiv.org/abs/2509.02579)
*Mazyar Taghavi,Rahman Farnoosh*

Main category: cs.LG

TL;DR: 本研究提出了一种基于期望最大化（EM）的潜在变量建模方法，并将其应用于多智能体强化学习（MARL）框架中，用于无人机（UAV）在野生动物保护任务中的协调。该方法通过潜在变量对隐藏的环境因素和智能体间的动态进行建模，从而在不确定性下增强探索和协调能力。在包含10架无人机用于巡逻伊朗豹栖息地的模拟环境中，该EM-MARL框架的性能优于PPO和DDPG等标准算法，尤其在检测准确率、适应性和策略收敛性方面表现更佳。研究结果表明，EM推理与MARL的结合在复杂、高风险的保护场景中具有提升分散式决策能力的潜力。


<details>
  <summary>Details</summary>
Motivation: 在广阔且部分可观测的环境中，实时响应对于保护濒危野生动物免遭非法偷猎至关重要，这构成了严峻的挑战。

Method: 提出了一种基于期望最大化（EM）的潜在变量建模方法，并将其应用于多智能体强化学习（MARL）框架中，用于无人机（UAV）在野生动物保护任务中的协调。该方法通过潜在变量对隐藏的环境因素和智能体间的动态进行建模，从而在不确定性下增强探索和协调能力。

Result: 在包含10架无人机用于巡逻伊朗豹栖息地的模拟环境中，该EM-MARL框架的性能优于PPO和DDPG等标准算法，尤其在检测准确率、适应性和策略收敛性方面表现更佳。

Conclusion: 研究结果表明，期望最大化（EM）推理与多智能体强化学习（MARL）的结合在复杂、高风险的保护场景中具有提升分散式决策能力的潜力。

Abstract: Protecting endangered wildlife from illegal poaching presents a critical
challenge, particularly in vast and partially observable environments where
real-time response is essential. This paper introduces a novel
Expectation-Maximization (EM) based latent variable modeling approach in the
context of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial
Vehicle (UAV) coordination in wildlife protection. By modeling hidden
environmental factors and inter-agent dynamics through latent variables, our
method enhances exploration and coordination under uncertainty.We implement and
evaluate our EM-MARL framework using a custom simulation involving 10 UAVs
tasked with patrolling protected habitats of the endangered Iranian leopard.
Extensive experimental results demonstrate superior performance in detection
accuracy, adaptability, and policy convergence when compared to standard
algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic
Policy Gradient (DDPG). Our findings underscore the potential of combining EM
inference with MARL to improve decentralized decisionmaking in complex,
high-stakes conservation scenarios. The full implementation, simulation
environment, and training scripts are publicly available on GitHub.

</details>


### [120] [Event Detection and Classification for Long Range Sensing of Elephants Using Seismic Signal](https://arxiv.org/abs/2509.02920)
*Jaliya L. Wijayaraja,Janaka L. Wijekoon,Malitha Wijesundara*

Main category: cs.LG

TL;DR: 该研究提出了一种用于资源受限环境的地震信号大象足迹检测和分类框架，通过新颖的CCW事件检测技术和SVM分类器，在大约155米（受控）和140米（自然）的范围内实现了高准确率（最高99%），并识别出零交叉次数和DTW对齐成本是关键特征。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有基于地震信号的大象足迹检测方法在实时性方面存在的局限性，本研究旨在开发一种适用于资源受限环境、兼顾准确性和计算效率的大象足迹分类框架。

Method: 研究引入了一种名为上下文定制窗口（CCW）的新型事件检测技术，并将其与STA/LTA方法进行比较。此外，研究使用了支持向量机（SVM）配合径向基函数（RBF）核进行大象足迹分类，并通过可解释人工智能（XAI）进行了特征影响分析。

Result: CCW方法在受控条件下最大验证检测范围为155.6米，在自然环境中为140米。SVM分类器在受控环境、自然栖息地和冲突易发的人类栖息地中分别达到了99%、73%和70%的准确率。零交叉次数和DTW对齐成本被确定为最重要的影响因素，而在受控环境中，主要频率也显示出显著影响。

Conclusion: 本研究提出的框架在大象足迹的检测和分类方面表现出高准确性和效率，尤其是在资源受限和具有挑战性的环境中，为解决人象冲突提供了有前景的解决方案。

Abstract: Detecting elephants through seismic signals is an emerging research topic
aimed at developing solutions for Human-Elephant Conflict (HEC). Despite the
promising results, such solutions heavily rely on manual classification of
elephant footfalls, which limits their applicability for real-time
classification in natural settings. To address this limitation and build on our
previous work, this study introduces a classification framework targeting
resource-constrained implementations, prioritizing both accuracy and
computational efficiency. As part of this framework, a novel event detection
technique named Contextually Customized Windowing (CCW), tailored specifically
for detecting elephant footfalls, was introduced, and evaluations were
conducted by comparing it with the Short-Term Average/Long-Term Average
(STA/LTA) method. The yielded results show that the maximum validated detection
range was 155.6 m in controlled conditions and 140 m in natural environments.
Elephant footfall classification using Support Vector Machine (SVM) with a
Radial Basis Function (RBF) kernel demonstrated superior performance across
multiple settings, achieving an accuracy of 99% in controlled environments, 73%
in natural elephant habitats, and 70% in HEC-prone human habitats, the most
challenging scenario. Furthermore, feature impact analysis using explainable AI
identified the number of Zero Crossings and Dynamic Time Warping (DTW)
Alignment Cost as the most influential factors in all experiments, while
Predominant Frequency exhibited significant influence in controlled settings.

</details>


### [121] [Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust Balanced Accuracy in Imbalanced Learning](https://arxiv.org/abs/2509.02592)
*Hunter Gittlin*

Main category: cs.LG

TL;DR: Class imbalance can be addressed more effectively using group-aware threshold calibration than synthetic data generation. This method offers superior robustness, improved worst-group accuracy, and simpler interpretation across various machine learning models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the fundamental challenge of class imbalance in machine learning, where traditional solutions often introduce new problems. It aims to find a more robust and effective method for handling imbalanced datasets.

Method: The study proposes and evaluates group-aware threshold calibration, which involves setting different decision thresholds for different demographic groups. This approach is compared against traditional synthetic data generation methods like SMOTE and CT-GAN through extensive experiments. The paper also analyzes the optimization of the Pareto frontier between balanced accuracy and worst-group balanced accuracy and investigates the redundancy of applying group thresholds to synthetic data.

Result: Group-specific thresholds achieve 1.5-4% higher balanced accuracy than SMOTE and CT-GAN augmented models while improving worst-group balanced accuracy. Applying group thresholds to synthetically augmented data provides minimal additional benefit. The findings are consistent across seven diverse model families, including linear, tree-based, instance-based, and boosting methods.

Conclusion: Group-aware threshold calibration is presented as a simpler, more interpretable, and more effective solution to class imbalance compared to existing methods like synthetic data generation. It allows for fine-grained control over group-level performance and optimizes the trade-off between overall and worst-group accuracy.

Abstract: Class imbalance remains a fundamental challenge in machine learning, with
traditional solutions often creating as many problems as they solve. We
demonstrate that group-aware threshold calibration--setting different decision
thresholds for different demographic groups--provides superior robustness
compared to synthetic data generation methods. Through extensive experiments,
we show that group-specific thresholds achieve 1.5-4% higher balanced accuracy
than SMOTE and CT-GAN augmented models while improving worst-group balanced
accuracy. Unlike single-threshold approaches that apply one cutoff across all
groups, our group-aware method optimizes the Pareto frontier between balanced
accuracy and worst-group balanced accuracy, enabling fine-grained control over
group-level performance. Critically, we find that applying group thresholds to
synthetically augmented data yields minimal additional benefit, suggesting
these approaches are fundamentally redundant. Our results span seven model
families including linear, tree-based, instance-based, and boosting methods,
confirming that group-aware threshold calibration offers a simpler, more
interpretable, and more effective solution to class imbalance.

</details>


### [122] [Preference Robustness for DPO with Applications to Public Health](https://arxiv.org/abs/2509.02709)
*Cheol Woo Kim,Shresth Verma,Mauricio Tec,Milind Tambe*

Main category: cs.LG

TL;DR: 我们提出了一种名为DPO-PRO的算法，用于通过自然语言中的人类偏好来调整LLM以解决公共卫生中的序贯资源分配问题。该算法基于直接偏好优化（DPO），并结合了分布鲁棒优化（DRO）来处理偏好不确定性。


<details>
  <summary>Details</summary>
Motivation: 由于公共卫生序贯资源分配问题目标复杂且数据有限，LLM的对齐（alignment）任务极具挑战性。现有方法在处理模糊目标和有限数据时可能不足。

Method: 提出DPO-PRO算法，一种基于DPO的鲁棒微调算法。该算法利用轻量级的分布鲁棒优化（DRO）方法来处理偏好分布中的不确定性，并旨在比现有DRO-DPO方法更不保守。

Result: 在真实世界的孕产妇移动健康项目以及标准的对齐基准测试中，DPO-PRO相比现有的DPO变体，在处理含噪声偏好信号方面表现出持续的鲁棒性提升。此外，DPO-PRO在奖励函数设计方面达到了与基于自我反思的方法相当的性能，但推理成本显著降低。

Conclusion: DPO-PRO是一种有效的LLM微调算法，能够鲁棒地处理公共卫生序贯资源分配问题中的模糊和不确定的人类偏好，并在保持较高性能的同时显著降低了推理成本。

Abstract: We study an LLM fine-tuning task for designing reward functions for
sequential resource allocation problems in public health, guided by human
preferences expressed in natural language. This setting presents a challenging
testbed for alignment due to complex and ambiguous objectives and limited data
availability. We propose DPO-PRO, a robust fine-tuning algorithm based on
Direct Preference Optimization (DPO), which accounts for uncertainty in the
preference distribution using a lightweight Distributionally Robust
Optimization (DRO) formulation. Unlike prior DRO-based DPO methods, DPO-PRO is
significantly less conservative. We evaluate DPO-PRO on a real-world maternal
mobile health program operated by the non-profit organization ARMMAN, as well
as on standard alignment benchmarks. Experimental results demonstrate that our
method consistently improves robustness to noisy preference signals compared to
existing DPO variants. Moreover, DPO-PRO achieves comparable performance to
prior self-reflection-based baseline for reward function design, while
requiring significantly lower inference-time cost.

</details>


### [123] [Imitate Optimal Policy: Prevail and Induce Action Collapse in Policy Gradient](https://arxiv.org/abs/2509.02737)
*Zhongzhu Zhou,Yibo Yang,Ziyan Chen,Fengxiang Bie,Haojun Xia,Xiaoxia Wu,Robert Wu,Ben Athiwaratkun,Bernard Ghanem,Shuaiwen Leon Song*

Main category: cs.LG

TL;DR: 策略梯度（PG）方法中的深度神经网络（DNN）在训练过程中会涌现出一种类似神经网络坍缩（Neural Collapse）的“动作坍缩”（Action Collapse, AC）现象，即状态-动作激活（最后一层特征）在最优动作下趋于其均值，激活变化趋于零，并且动作选择层的权重和均值激活会坍缩成一个等角紧框架（ETF）。本文分析了ETF结构在动作选择层中的作用，证明了以此为固定目标可以自然地导致AC现象，并提出了一种名为ACPG（Action Collapse Policy Gradient）的新方法，通过引入合成ETF作为动作选择层，诱导策略DNN产生理想的ETF配置并保持最优。实验表明，ACPG能够提升多种离散策略梯度方法的性能，使其学习更快、更稳定。


<details>
  <summary>Details</summary>
Motivation: 尽管策略梯度（PG）方法中的深度神经网络（DNN）已有大量关于收敛性和全局最优性的研究，但对其表征结构的研究却很少。本研究旨在分析最优策略DNN的表征结构，并发现了一种名为“动作坍缩”（Action Collapse, AC）的新兴结构。

Method: 1. 观察到在最优策略DNN训练过程中，在特定约束下会出现AC现象。2. 分析AC现象的三个关键特征：状态-动作激活趋于最优动作均值、激活变异性趋于零、动作选择层权重和均值激活坍缩为ETF。3. 提出将ETF结构作为固定目标应用于动作选择层，并进行理论证明，表明此举可自然导致AC。4. 提出ACPG（Action Collapse Policy Gradient）方法，将合成ETF作为动作选择层，引导策略DNN产生理想配置。5. 在OpenAI Gym环境中进行实验验证。 

Result: ACPG方法能够被整合到任何离散策略梯度方法中，并在实验中表现出更快速、更稳健的奖励提升。ACPG诱导策略DNN产生一个理想的、最优的动作选择层配置。

Conclusion: ACPG方法通过引入ETF结构作为动作选择层的固定目标，成功诱导了AC现象，并实现了策略梯度方法的性能提升。该方法在多种环境中都显示出更快的学习速度和更好的鲁棒性。

Abstract: Policy gradient (PG) methods in reinforcement learning frequently utilize
deep neural networks (DNNs) to learn a shared backbone of feature
representations used to compute likelihoods in an action selection layer.
Numerous studies have been conducted on the convergence and global optima of
policy networks, but few have analyzed representational structures of those
underlying networks. While training an optimal policy DNN, we observed that
under certain constraints, a gentle structure resembling neural collapse, which
we refer to as Action Collapse (AC), emerges. This suggests that 1) the
state-action activations (i.e. last-layer features) sharing the same optimal
actions collapse towards those optimal actions respective mean activations; 2)
the variability of activations sharing the same optimal actions converges to
zero; 3) the weights of action selection layer and the mean activations
collapse to a simplex equiangular tight frame (ETF). Our early work showed
those aforementioned constraints to be necessary for these observations. Since
the collapsed ETF of optimal policy DNNs maximally separates the pair-wise
angles of all actions in the state-action space, we naturally raise a question:
can we learn an optimal policy using an ETF structure as a (fixed) target
configuration in the action selection layer? Our analytical proof shows that
learning activations with a fixed ETF as action selection layer naturally leads
to the AC. We thus propose the Action Collapse Policy Gradient (ACPG) method,
which accordingly affixes a synthetic ETF as our action selection layer. ACPG
induces the policy DNN to produce such an ideal configuration in the action
selection layer while remaining optimal. Our experiments across various OpenAI
Gym environments demonstrate that our technique can be integrated into any
discrete PG methods and lead to favorable reward improvements more quickly and
robustly.

</details>


### [124] [Population-aware Online Mirror Descent for Mean-Field Games with Common Noise by Deep Reinforcement Learning](https://arxiv.org/abs/2509.03030)
*Zida Wu,Mathieu Lauriere,Matthieu Geist,Olivier Pietquin,Ankur Mehta*

Main category: cs.LG

TL;DR: 本论文提出了一种新的深度强化学习算法，用于在均值场博弈中学习纳什均衡，该算法能适应未知的初始分布和共同噪声，并优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 学习均值场博弈中的纳什均衡仍然是一个挑战，特别是在初始分布未知或存在共同噪声的情况下。

Method: 提出了一种基于Munchausen RL和Online Mirror Descent的深度强化学习算法，该算法不需要平均或历史采样。

Result: 通过在七个典型例子上的数值实验，证明了该算法比最先进的算法（特别是针对人口 dependent policies 的 Fictitious Play 的 DRL 版本）具有更优越的收敛性，并且在存在共同噪声时表现出鲁棒性和适应性。

Conclusion: 所提出的算法能够学习到人口 dependent 的纳什均衡，并且能够适应各种初始分布和共同噪声源。

Abstract: Mean Field Games (MFGs) offer a powerful framework for studying large-scale
multi-agent systems. Yet, learning Nash equilibria in MFGs remains a
challenging problem, particularly when the initial distribution is unknown or
when the population is subject to common noise. In this paper, we introduce an
efficient deep reinforcement learning (DRL) algorithm designed to achieve
population-dependent Nash equilibria without relying on averaging or historical
sampling, inspired by Munchausen RL and Online Mirror Descent. The resulting
policy is adaptable to various initial distributions and sources of common
noise. Through numerical experiments on seven canonical examples, we
demonstrate that our algorithm exhibits superior convergence properties
compared to state-of-the-art algorithms, particularly a DRL version of
Fictitious Play for population-dependent policies. The performance in the
presence of common noise underscores the robustness and adaptability of our
approach.

</details>


### [125] [Mentality: A Mamba-based Approach towards Foundation Models for EEG](https://arxiv.org/abs/2509.02746)
*Saarang Panchavati,Corey Arnold,William Speier*

Main category: cs.LG

TL;DR: Mamba模型在癫痫等神经系统疾病的EEG分析中展现了潜力，通过自监督学习和有监督检测，在癫痫检测任务中达到了0.72的AUROC。


<details>
  <summary>Details</summary>
Motivation: 探索使用Mamba等基础模型来改进癫痫等神经系统疾病的EEG分析，以应对EEG数据固有的复杂性。

Method: 使用Mamba模型，在一个包含癫痫发作和非癫痫发作的EEG录制大型数据集上进行训练，先进行自监督重建任务，然后进行癫痫检测任务。

Result: 在测试集上，Mamba模型在癫痫检测任务中达到了0.72的AUROC。

Conclusion: 基于Mamba的基础模型在EEG数据分析方面显示出巨大潜力，是开发大规模、临床应用EEG分析模型的显著一步。

Abstract: This work explores the potential of foundation models, specifically a
Mamba-based selective state space model, for enhancing EEG analysis in
neurological disorder diagnosis. EEG, crucial for diagnosing conditions like
epilepsy, presents significant challenges due to its noisy, high-dimensional,
and nonlinear nature. Traditional machine learning methods have made advances
in automating EEG analysis but often fail to capture its complex
spatio-temporal dynamics. Recent advances in deep learning, particularly in
sequence modeling, offer new avenues for creating more generalized and
expressive models capable of handling such complexities. By training a
Mamba-based model on a large dataset containing seizure and non-seizure EEG
recordings through a self-supervised reconstruction task followed by a seizure
detection task, we demonstrate the model's effectiveness, achieving an AUROC of
0.72 on a held-out test set. This approach marks a significant step toward
developing large-scale, clinically applicable foundation models for EEG data
analysis.

</details>


### [126] [A Hierarchical Deep Reinforcement Learning Framework for Traffic Signal Control with Predictable Cycle Planning](https://arxiv.org/abs/2509.03118)
*Hankang Gu,Yuli Zhang,Chengming Wang,Ruiyuan Jiang,Ziheng Qiao,Pengfei Fan,Dongyao Jia*

Main category: cs.LG

TL;DR: DHCP模型通过分层方式优化交通信号控制，提高效率和可预测性。


<details>
  <summary>Details</summary>
Motivation: 现有DRL交通信号控制方法存在不可预测的相位序列或不公平/低效的相位分配问题。DHCP模型旨在解决这些问题。

Method: DHCP模型采用分层强化学习方法，高层智能体决定南北/东西方向的总周期时间分配，低层智能体在各方向内分配直行和左转通行时间。

Result: 在真实和合成路网及交通流数据集上进行测试，DHCP模型表现优于基线模型。

Conclusion: DHCP模型在交通信号控制方面实现了最佳性能。

Abstract: Deep reinforcement learning (DRL) has become a popular approach in traffic
signal control (TSC) due to its ability to learn adaptive policies from complex
traffic environments. Within DRL-based TSC methods, two primary control
paradigms are ``choose phase" and ``switch" strategies. Although the agent in
the choose phase paradigm selects the next active phase adaptively, this
paradigm may result in unexpected phase sequences for drivers, disrupting their
anticipation and potentially compromising safety at intersections. Meanwhile,
the switch paradigm allows the agent to decide whether to switch to the next
predefined phase or extend the current phase. While this structure maintains a
more predictable order, it can lead to unfair and inefficient phase
allocations, as certain movements may be extended disproportionately while
others are neglected. In this paper, we propose a DRL model, named Deep
Hierarchical Cycle Planner (DHCP), to allocate the traffic signal cycle
duration hierarchically. A high-level agent first determines the split of the
total cycle time between the North-South (NS) and East-West (EW) directions
based on the overall traffic state. Then, a low-level agent further divides the
allocated duration within each major direction between straight and left-turn
movements, enabling more flexible durations for the two movements. We test our
model on both real and synthetic road networks, along with multiple sets of
real and synthetic traffic flows. Empirical results show our model achieves the
best performance over all datasets against baselines.

</details>


### [127] [LExI: Layer-Adaptive Active Experts for Efficient MoE Model Inference](https://arxiv.org/abs/2509.02753)
*Krishna Teja Chitty-Venkata,Sandeep Madireddy,Murali Emani,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: MoE模型通过仅激活部分专家来提高计算效率，但现有优化方法（如剪枝）主要减少内存占用，对推理效率提升有限。本文提出LExI，一种无需数据的方法，能为预训练MoE模型的每一层确定最优激活专家数量，从而显著提高推理效率，且准确率损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型在激活固定数量的专家时存在计算冗余和性能不佳的问题。现有的剪枝优化方法主要减少内存占用，但对GPU推理效率提升有限。

Method: LExI是一种无需数据的方法，仅利用模型权重来评估各层的重要性，并为每层自适应地分配激活的专家数量。

Result: 在语言和视觉MoE基准测试中，LExI显著优于传统的MoE剪枝方法，提高了推理效率，且准确率损失可忽略。例如，Qwen1.5-MoE在使用LExI后，在Nvidia H100 GPU上实现了相同的吞吐量，但准确率提升了10%。

Conclusion: LExI是一种有效的数据无关优化技术，能够通过自适应调整每层的激活专家数量来显著提高MoE模型的推理效率，同时保持高准确率。

Abstract: Mixture-of-Experts (MoE) models scale efficiently by activating only a subset
of experts per token, offering a computationally sparse alternative to dense
architectures. While prior post-training optimizations, such as inter- and
intra-expert pruning, reduce memory usage they provide limited gains in
inference-time compute efficiency. Moreover, existing MoE architectures
typically activate a fixed number of experts uniformly across all layers,
resulting in redundant computation and suboptimal performance. In this work, we
first demonstrate that MoE pruning strategies improve only the memory footprint
but do not significantly improve inference performance on GPU using optimized
frameworks such as vLLM. To address this, we introduce LExI, a data-free
optimization technique that determines the optimal number of active experts per
layer in a pretrained MoE model. LExI leverages only the model weights to
estimate the relative importance of each layer and adaptively assigns the
number of active experts accordingly per layer. Experiments on state-of-the-art
language and vision MoE benchmarks demonstrate that LExI significantly
outperforms traditional MoE pruning approaches in terms of inference efficiency
with negligible accuracy loss. For example, using LExI, Qwen1.5-MoE achieves
the same throughput on Nvidia H100 GPU with 10% better accuracy than
traditional expert pruning.

</details>


### [128] [The Transparent Earth: A Multimodal Foundation Model for the Earth's Subsurface](https://arxiv.org/abs/2509.02783)
*Arnab Mazumder,Javier E. Santos,Noah Hobbs,Mohamed Mehana,Daniel O'Malley*

Main category: cs.LG

TL;DR: Transparent Earth是一个基于Transformer的架构，用于从异构数据集中重建地球次表层属性，能够处理不同稀疏度、分辨率和模态的数据，并且易于扩展到任意数量的模态，实现了在次表层属性预测方面的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理异构、稀疏、低分辨率和多模态观测数据的模型，以重建地球次表层属性，并具备可扩展性和在没有或有任意数量额外观测数据情况下进行预测的能力。

Method: 提出了一种名为Transparent Earth的Transformer-based架构，该架构结合了观测数据的体位置编码和模态编码（通过文本嵌入模型获得），以处理多种不同类型的观测数据（如应力角、地幔温度、构造板块类型等），并支持上下文学习。

Result: 在验证数据上，将预测应力角的误差减少了三倍多。该模型具有良好的可扩展性，并且随着参数的增加，性能有所提高。

Conclusion: Transparent Earth是一个初步的地球次表层基础模型，具有可扩展性，能够处理多模态异构数据，并在次表层属性预测任务中表现出优越性能，其目标是能够预测地球上任何位置的任何次表层属性。

Abstract: We present the Transparent Earth, a transformer-based architecture for
reconstructing subsurface properties from heterogeneous datasets that vary in
sparsity, resolution, and modality, where each modality represents a distinct
type of observation (e.g., stress angle, mantle temperature, tectonic plate
type). The model incorporates positional encodings of observations together
with modality encodings, derived from a text embedding model applied to a
description of each modality. This design enables the model to scale to an
arbitrary number of modalities, making it straightforward to add new ones not
considered in the initial design. We currently include eight modalities
spanning directional angles, categorical classes, and continuous properties
such as temperature and thickness. These capabilities support in-context
learning, enabling the model to generate predictions either with no inputs or
with an arbitrary number of additional observations from any subset of
modalities. On validation data, this reduces errors in predicting stress angle
by more than a factor of three. The proposed architecture is scalable and
demonstrates improved performance with increased parameters. Together, these
advances make the Transparent Earth an initial foundation model for the Earth's
subsurface that ultimately aims to predict any subsurface property anywhere on
Earth.

</details>


### [129] [Structured Basis Function Networks: Loss-Centric Multi-Hypothesis Ensembles with Controllable Diversity](https://arxiv.org/abs/2509.02792)
*Alejandro Rodriguez Dominguez,Muhammad Shahzad,Xia Hong*

Main category: cs.LG

TL;DR: 现有方法在预测不确定性方面要么依赖于多假设预测，要么依赖于集成学习，但缺乏统一的框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法在预测不确定性方面存在局限性，缺乏统一的框架来处理多假设预测和集成学习的缺点。

Method: 提出结构化基函数网络，通过Bregman散度引起的质心聚合将多假设预测和集成学习联系起来。该方法适用于回归和分类，并支持闭式最小二乘估计和基于梯度的过程。

Result: 实验验证了该方法的有效性，并研究了在不同难度的数据集上，深度学习预测器的复杂性-容量-多样性权衡。

Conclusion: 结构化基函数网络通过质心聚合统一了多假设预测和集成学习，并提供了可调的多样性机制来控制偏差-方差-多样性权衡。

Abstract: Existing approaches to predictive uncertainty rely either on multi-hypothesis
prediction, which promotes diversity but lacks principled aggregation, or on
ensemble learning, which improves accuracy but rarely captures the structured
ambiguity. This implicitly means that a unified framework consistent with the
loss geometry remains absent. The Structured Basis Function Network addresses
this gap by linking multi-hypothesis prediction and ensembling through
centroidal aggregation induced by Bregman divergences. The formulation applies
across regression and classification by aligning predictions with the geometry
of the loss, and supports both a closed-form least-squares estimator and a
gradient-based procedure for general objectives. A tunable diversity mechanism
provides parametric control of the bias-variance-diversity trade-off,
connecting multi-hypothesis generalisation with loss-aware ensemble
aggregation. Experiments validate this relation and use the mechanism to study
the complexity-capacity-diversity trade-off across datasets of increasing
difficulty with deep-learning predictors.

</details>


### [130] [Learning Laplacian Eigenvectors: a Pre-training Method for Graph Neural Networks](https://arxiv.org/abs/2509.02803)
*Howard Dai,Nyambura Njenga,Benjamin Whitsett,Catherine Ma,Darwin Deng,Sara de Ángel,Alexandre Van Tassel,Siddharth Viswanath,Ryan Pellico,Ian Adelstein,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 通过学习拉普拉斯特征向量来预训练图神经网络（GNN），以克服MPNN的过平滑问题并捕捉图的全局和区域结构。


<details>
  <summary>Details</summary>
Motivation: 传统的图神经网络（GNN）在处理增加的网络深度时，由于过平滑的风险，常常难以捕捉图的全局和区域结构。拉普拉斯矩阵的低频特征向量编码了图的全局信息，因此，通过预训练GNN来学习这些特征向量，可以促使网络学习到图的宏观结构模式。

Method: 提出一种新颖的框架，通过归纳地学习拉普拉斯特征向量来预训练图神经网络（GNN）。

Result: 与基线模型相比，通过该框架预训练的模型在各种基于图结构的任务上表现更优。

Conclusion: 所提出的自监督预训练框架是基于结构的，并且非常灵活，可以应用于所有基于图的数据集，并在任务特定的数据稀疏时用于合成特征。

Abstract: We propose a novel framework for pre-training Graph Neural Networks (GNNs) by
inductively learning Laplacian eigenvectors. Traditional Message Passing Neural
Networks (MPNNs) often struggle to capture global and regional graph structure
due to over-smoothing risk as network depth increases. Because the
low-frequency eigenvectors of the graph Laplacian matrix encode global
information, pre-training GNNs to predict these eigenvectors encourages the
network to naturally learn large-scale structural patterns over each graph.
Empirically, we show that models pre-trained via our framework outperform
baseline models on a variety of graph structure-based tasks. While most
existing pre-training methods focus on domain-specific tasks like node or edge
feature reconstruction, our self-supervised pre-training framework is
structure-based and highly flexible. Eigenvector-learning can be applied to all
graph-based datasets, and can be used with synthetic features when
task-specific data is sparse.

</details>


### [131] [DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling](https://arxiv.org/abs/2509.03472)
*Yubo Gao,Renbo Tu,Gennady Pekhimenko,Nandita Vijaykumar*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Differentially-Private SGD (DP-SGD) is a powerful technique to protect user
privacy when using sensitive data to train neural networks. During training,
converting model weights and activations into low-precision formats, i.e.,
quantization, can drastically reduce training times, energy consumption, and
cost, and is thus a widely used technique. In this work, we demonstrate that
quantization causes significantly higher accuracy degradation in DP-SGD
compared to regular SGD. We observe that this is caused by noise injection in
DP-SGD, which amplifies quantization variance, leading to disproportionately
large accuracy degradation. To address this challenge, we present QPQuant, a
dynamic quantization framework that adaptively selects a changing subset of
layers to quantize at each epoch. Our method combines two key ideas that
effectively reduce quantization variance: (i) probabilistic sampling of the
layers that rotates which layers are quantized every epoch, and (ii) loss-aware
layer prioritization, which uses a differentially private loss sensitivity
estimator to identify layers that can be quantized with minimal impact on model
quality. This estimator consumes a negligible fraction of the overall privacy
budget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50,
and DenseNet121 across a range of datasets demonstrate that DPQuant
consistently outperforms static quantization baselines, achieving near
Pareto-optimal accuracy-compute trade-offs and up to 2.21x theoretical
throughput improvements on low-precision hardware, with less than 2% drop in
validation accuracy.

</details>


### [132] [Challenges in Understanding Modality Conflict in Vision-Language Models](https://arxiv.org/abs/2509.02805)
*Trang Nguyen,Jackson Michaels,Madalina Fiterau,David Jensen*

Main category: cs.LG

TL;DR: The paper separates conflict detection from resolution in VLMs, using methods like linear probes and attention analysis on LLaVA-OV-7B. Findings show distinct intermediate layers and attention patterns for detection and resolution, supporting their functional separation and enabling better interpretability and model improvement.


<details>
  <summary>Details</summary>
Motivation: This paper addresses the challenge of distinguishing conflict detection from conflict resolution in Vision-Language Models (VLMs), which is crucial for understanding and improving their behavior when handling conflicting multimodal inputs.

Method: The study employs a supervised metric using linear probes and analyzes group-based attention patterns to investigate the internal mechanisms of the LLaVA-OV-7B model, a VLM known for its varied responses to conflicting multimodal data.

Result: The investigation revealed that a conflict signal can be decoded from the model's intermediate layers using linear methods. Furthermore, attention patterns related to conflict detection and resolution were observed to diverge at different network depths.

Conclusion: The research concludes that conflict detection and resolution are functionally distinct processes within VLMs. This separation offers a path towards more practical interpretability and allows for targeted improvements to enhance model robustness in complex multimodal scenarios.

Abstract: This paper highlights the challenge of decomposing conflict detection from
conflict resolution in Vision-Language Models (VLMs) and presents potential
approaches, including using a supervised metric via linear probes and
group-based attention pattern analysis. We conduct a mechanistic investigation
of LLaVA-OV-7B, a state-of-the-art VLM that exhibits diverse resolution
behaviors when faced with conflicting multimodal inputs. Our results show that
a linearly decodable conflict signal emerges in the model's intermediate layers
and that attention patterns associated with conflict detection and resolution
diverge at different stages of the network. These findings support the
hypothesis that detection and resolution are functionally distinct mechanisms.
We discuss how such decomposition enables more actionable interpretability and
targeted interventions for improving model robustness in challenging multimodal
settings.

</details>


### [133] [Unlearning That Lasts: Utility-Preserving, Robust, and Almost Irreversible Forgetting in LLMs](https://arxiv.org/abs/2509.02820)
*Naman Deep Singh,Maximilian Müller,Francesco Croce,Matthias Hein*

Main category: cs.LG

TL;DR: JensUn是一种利用 Jensen-Shannon 散度进行大型语言模型（LLM）信息移除的方法，在实验中表现出更好的遗忘-效用权衡和对遗忘后重新学习的鲁棒性。该研究还提出了 LKF 数据集和改进的评估框架（使用 LLM 作为评估者和最坏情况评估），以更全面地测试信息移除方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）信息移除（unlearning）方法在评估中表现不佳，无法有效、稳定地删除特定信息，同时保留模型其他能力。信息移除对于删除预训练模型中获得的私人数据或有害知识至关重要，以确保 LLM 的安全性。

Method: 提出 JensUn 方法，利用 Jensen-Shannon 散度作为遗忘集和保留集的训练目标，以实现比常用损失函数更稳定、更有效的模型信息移除。此外，研究引入了 LKF 数据集（包含鲜为人知的知识）来模拟真实信息移除场景，并提出使用 LLM 作为语义评估者替代 ROUGE 分数，以及在各种释义和输入格式下进行最坏情况信息移除评估。

Result: JensUn 在遗忘-效用权衡方面优于现有方法，并对遗忘后重新学习表现出强大的鲁棒性。改进的评估框架揭示了许多现有方法的效果不如之前所认为的。

Conclusion: JensUn 及其提出的评估方法能够更有效地评估和实现 LLM 的信息移除。现有的信息移除方法在更严格的评估下，其有效性大打折扣。

Abstract: Unlearning in large language models (LLMs) involves precisely removing
specific information from a pre-trained model. This is crucial to ensure safety
of LLMs by deleting private data or harmful knowledge acquired during
pre-training. However, existing unlearning methods often fall short when
subjected to thorough evaluation. To overcome this, we introduce JensUn, where
we leverage the Jensen-Shannon Divergence as the training objective for both
forget and retain sets for more stable and effective unlearning dynamics
compared to commonly used loss functions. In extensive experiments, JensUn
achieves better forget-utility trade-off than competing methods, and even
demonstrates strong resilience to benign relearning. Additionally, for a
precise unlearning evaluation, we introduce LKF, a curated dataset of
lesser-known facts that provides a realistic unlearning scenario. Finally, to
comprehensively test unlearning methods, we propose (i) employing an LLM as
semantic judge instead of the standard ROUGE score, and (ii) using worst-case
unlearning evaluation over various paraphrases and input formats. Our improved
evaluation framework reveals that many existing methods are less effective than
previously thought.

</details>


### [134] [Ensemble Learning for Healthcare: A Comparative Analysis of Hybrid Voting and Ensemble Stacking in Obesity Risk Prediction](https://arxiv.org/abs/2509.02826)
*Towhidul Islam,Md Sumon Ali*

Main category: cs.LG

TL;DR: 该研究比较了混合多数投票和集成堆叠方法在肥胖风险预测中的应用，旨在确定哪种方法能提供更高的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 肥胖是一个严峻的全球健康问题，与多种慢性疾病相关。虽然机器学习在肥胖风险预测方面显示出潜力，但对集成技术（特别是混合多数投票和集成堆叠）的比较评估仍然有限。

Method: 利用两个数据集，评估了三种集成模型：多数投票、加权投票和堆叠（以多层感知器作为元分类器）。分析了九种机器学习算法及其50种超参数配置，以选择最佳基础学习器。预处理包括数据集平衡和异常值检测，并使用准确率和F1分数评估模型性能。

Result: 在数据集1上，加权投票和堆叠模型的性能接近（准确率：0.920304，F1：0.920070），优于多数投票。在数据集2上，堆叠模型表现最佳（准确率：0.989837，F1：0.989825），优于多数投票（准确率：0.981707，F1：0.981675）和加权投票。

Conclusion: 研究结果表明，集成堆叠在预测能力方面更胜一筹，尤其是在处理复杂数据分布时，而混合多数投票作为一种稳健的选择仍然有效。

Abstract: Obesity is a critical global health issue driven by dietary, physiological,
and environmental factors, and is strongly associated with chronic diseases
such as diabetes, cardiovascular disorders, and cancer. Machine learning has
emerged as a promising approach for early obesity risk prediction, yet a
comparative evaluation of ensemble techniques -- particularly hybrid majority
voting and ensemble stacking -- remains limited. This study aims to compare
hybrid majority voting and ensemble stacking methods for obesity risk
prediction, identifying which approach delivers higher accuracy and efficiency.
The analysis seeks to highlight the complementary strengths of these ensemble
techniques in guiding better predictive model selection for healthcare
applications. Two datasets were utilized to evaluate three ensemble models:
Majority Hard Voting, Weighted Hard Voting, and Stacking (with a Multi-Layer
Perceptron as meta-classifier). A pool of nine Machine Learning (ML)
algorithms, evaluated across a total of 50 hyperparameter configurations, was
analyzed to identify the top three models to serve as base learners for the
ensemble methods. Preprocessing steps involved dataset balancing, and outlier
detection, and model performance was evaluated using Accuracy and F1-Score. On
Dataset-1, weighted hard voting and stacking achieved nearly identical
performance (Accuracy: 0.920304, F1: 0.920070), outperforming majority hard
voting. On Dataset-2, stacking demonstrated superior results (Accuracy:
0.989837, F1: 0.989825) compared to majority hard voting (Accuracy: 0.981707,
F1: 0.981675) and weighted hard voting, which showed the lowest performance.
The findings confirm that ensemble stacking provides stronger predictive
capability, particularly for complex data distributions, while hybrid majority
voting remains a robust alternative.

</details>


### [135] [Conformal Prediction for Time-series Forecasting with Change Points](https://arxiv.org/abs/2509.02844)
*Sophia Sun,Rose Yu*

Main category: cs.LG

TL;DR: CPTC是一种新的共形预测算法，用于解决时间序列中的变化点问题，并在合成和真实世界的数据集上证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的共形预测方法在处理时间序列数据中的变化点（即底层数据生成过程的突然变化）时存在困难。

Method: 提出了一种名为CPTC的新算法，该算法通过集成一个预测底层状态的模型和在线共形预测来解决这一问题，从而对非平稳时间序列中的不确定性进行建模。

Result: CPTC在合成和真实世界的数据集上被证明是有效的，与最先进的基线相比，在有效性和适应性方面都有所提高。

Conclusion: CPTC算法在处理时间序列中的变化点方面具有有效性和适应性优势。

Abstract: Conformal prediction has been explored as a general and efficient way to
provide uncertainty quantification for time series. However, current methods
struggle to handle time series data with change points - sudden shifts in the
underlying data-generating process. In this paper, we propose a novel Conformal
Prediction for Time-series with Change points (CPTC) algorithm, addressing this
gap by integrating a model to predict the underlying state with online
conformal prediction to model uncertainties in non-stationary time series. We
prove CPTC's validity and improved adaptivity in the time series setting under
minimum assumptions, and demonstrate CPTC's practical effectiveness on 6
synthetic and real-world datasets, showing improved validity and adaptivity
compared to state-of-the-art baselines.

</details>


### [136] [AR-KAN: Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network for Time Series Forecasting](https://arxiv.org/abs/2509.02967)
*Chen Zeng,Tiehang Xu,Qiao Wang*

Main category: cs.LG

TL;DR: FNNs and LLMs struggle with spectral analysis, especially for almost periodic functions with incommensurate frequencies. ARIMA often outperforms them. We propose AR-KAN, a hybrid model combining KAN for static nonlinearity and a pre-trained AR component for memory, which excels in forecasting such signals, outperforming other models on 72% of real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Conventional neural networks and even LLMs face challenges in spectral analysis, particularly for almost periodic functions composed of signals with incommensurate frequencies, where traditional models like ARIMA often perform better. This limitation is often overlooked and stems from the fact that the superposition of periodic signals does not necessarily result in a periodic signal.

Method: We propose Autoregressive-Weight-Enhanced AR-KAN, a hybrid model that combines a Kolmogorov-Arnold Network (KAN) for the static nonlinear part, leveraging the Universal Myopic Mapping Theorem, with a pre-trained Autoregressive (AR) component to incorporate memory and retain essential information while eliminating redundancy.

Result: Experimental data indicates that the proposed AR-KAN model delivers superior results on 72% of real-world datasets when compared to existing methods.

Conclusion: AR-KAN, a hybrid model combining KAN and a pre-trained AR component, effectively addresses the limitations of conventional neural networks and LLMs in spectral analysis, particularly for almost periodic functions with incommensurate frequencies, achieving superior performance on a majority of real-world datasets.

Abstract: Conventional neural networks frequently face challenges in spectral analysis
of signals. To address this challenge, Fourier neural networks (FNNs) and
similar approaches integrate components of Fourier series into the structure of
neural networks. Nonetheless, a significant hurdle is often overlooked: the
superposition of periodic signals does not necessarily result in a periodic
signal. For example, when forecasting almost periodic functions composed of
signals with incommensurate frequencies, traditional models such as
Autoregressive Integrated Moving Average (ARIMA) frequently outperform most
neural networks including large language models (LLMs). To tackle this goal, we
propose Autoregressive-Weight-Enhanced AR-KAN, a hybrid model that combines the
benefits of both methods. Using the Universal Myopic Mapping Theorem, we apply
a Kolmogorov-Arnold Network (KAN) for the static nonlinear part and include
memory through a pre-trained AR component, which can be explained to retain the
most useful information while eliminating redundancy. Experimental data
indicates that AR-KAN delivers superior results on $72\%$ of real-world
datasets.

</details>


### [137] [Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven Inference-Time-Scaling Algorithm](https://arxiv.org/abs/2509.02846)
*Siddharth Mansingh,James Amarel,Ragib Arnab,Arvind Mohan,Kamaljeet Singh,Gerd J. Kunde,Nicolas Hengartner,Benjamin Migliori,Emily Casleton,Nathan A. Debarledeben,Ayan Biswas,Diane Oyen,Earl Lawrence*

Main category: cs.LG

TL;DR: 本文提出了一种用于偏微分方程（PDE）的测试时计算（TTC）策略，旨在提高模型在分布外（OOD）情况下的预测准确性，并减少对训练数据和模型大小的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有PDE基础模型受限于预训练数据集，在处理分布外（OOD）情况时的自回归预测性能不佳，且需要大量的计算和训练数据，限制了其应用。受大型语言模型（LLM）“思考”策略的启发，本文旨在开发一种在推理时利用计算资源以提高预测精度、减少训练样本和模型大小的TTC策略。

Method: 提出了一种TTC策略，该策略使用两种奖励模型来评估基于随机性的模型的时空一致性预测，并在推理时利用计算资源。

Result: 在可压缩欧拉方程模拟（PDEGym基准）上进行了验证，结果表明TTC策略相比标准的非自适应自回归推理能够获得更准确的预测。

Conclusion: 本文提出的TTC框架是PDE建模领域实现更高级推理算法（如基于强化学习的方法）的重要基础，有潜力变革物理和工程领域的计算工作流程。

Abstract: Partial Differential Equations (PDEs) are the bedrock for modern
computational sciences and engineering, and inherently computationally
expensive. While PDE foundation models have shown much promise for simulating
such complex spatio-temporal phenomena, existing models remain constrained by
the pretraining datasets and struggle with auto-regressive rollout performance,
especially in out-of-distribution (OOD) cases. Furthermore, they have
significant compute and training data requirements which hamper their use in
many critical applications. Inspired by recent advances in ``thinking"
strategies used in large language models (LLMs), we introduce the first
test-time computing (TTC) strategy for PDEs that utilizes computational
resources during inference to achieve more accurate predictions with fewer
training samples and smaller models. We accomplish this with two types of
reward models that evaluate predictions of a stochastic based model for
spatio-temporal consistency. We demonstrate this method on compressible
Euler-equation simulations from the PDEGym benchmark and show that TTC captures
improved predictions relative to standard non-adaptive auto-regressive
inference. This TTC framework marks a foundational step towards more advanced
reasoning algorithms or PDE modeling, inluding building
reinforcement-learning-based approaches, potentially transforming computational
workflows in physics and engineering.

</details>


### [138] [Power Grid Control with Graph-Based Distributed Reinforcement Learning](https://arxiv.org/abs/2509.02861)
*Carlo Fabrizio,Gianvito Losapio,Marco Mussi,Alberto Maria Metelli,Marcello Restelli*

Main category: cs.LG

TL;DR: 本研究提出了一种基于图的分布式强化学习框架，用于实时、可扩展的电网管理。


<details>
  <summary>Details</summary>
Motivation: 由于可再生能源整合和电网规模扩大带来的挑战，传统控制系统难以适应，因此需要探索更动态、分布式的控制策略。

Method: 该框架包含由低层智能体和高层管理器组成的分布式网络。低层智能体利用图神经网络（GNN）对网络拓扑信息进行编码，并结合模仿学习和基于势奖励塑造来加速收敛和提高稳定性。与依赖全局观测的传统方法不同，该方法还分解了观测空间，使低层智能体能够基于结构化的局部视图进行操作。

Result: 实验结果表明，该方法在Grid2Op仿真环境中优于标准基线，并且比基于仿真的专家方法更具计算效率。

Conclusion: 本研究提出的基于图的分布式强化学习框架能够有效、高效地管理现代电网。

Abstract: The necessary integration of renewable energy sources, combined with the
expanding scale of power networks, presents significant challenges in
controlling modern power grids. Traditional control systems, which are human
and optimization-based, struggle to adapt and to scale in such an evolving
context, motivating the exploration of more dynamic and distributed control
strategies. This work advances a graph-based distributed reinforcement learning
framework for real-time, scalable grid management. The proposed architecture
consists of a network of distributed low-level agents acting on individual
power lines and coordinated by a high-level manager agent. A Graph Neural
Network (GNN) is employed to encode the network's topological information
within the single low-level agent's observation. To accelerate convergence and
enhance learning stability, the framework integrates imitation learning and
potential-based reward shaping. In contrast to conventional decentralized
approaches that decompose only the action space while relying on global
observations, this method also decomposes the observation space. Each low-level
agent acts based on a structured and informative local view of the environment
constructed through the GNN. Experiments on the Grid2Op simulation environment
show the effectiveness of the approach, which consistently outperforms the
standard baseline commonly adopted in the field. Additionally, the proposed
model proves to be much more computationally efficient than the
simulation-based Expert method.

</details>


### [139] [Enhancing Machine Learning for Imbalanced Medical Data: A Quantum-Inspired Approach to Synthetic Oversampling (QI-SMOTE)](https://arxiv.org/abs/2509.02863)
*Vikas Kashtriya,Pardeep Singh*

Main category: cs.LG

TL;DR: QI-SMOTE是一种利用量子原理（如量子演化和分层纠缠）生成合成样本的新型数据增强技术，旨在解决机器学习中的类别不平衡问题，特别是在医疗领域。该方法通过保留复杂的数据结构来提高模型的泛化能力和分类准确性，并在MIMIC-III和MIMIC-IV数据集上通过死亡检测任务进行了验证，相比传统方法表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡是机器学习中的一个关键挑战，特别是在医疗领域，少数类别的样本不足会导致模型偏见和预测性能下降。本研究旨在提出一种新的数据增强技术来解决此问题。

Method: QI-SMOTE是一种新的数据增强技术，它利用量子原理（如量子演化和分层纠缠）来生成合成样本。与传统的过采样方法不同，QI-SMOTE生成的合成样本能够保留复杂的数据结构，从而提高模型的泛化能力和分类准确性。该方法在MIMIC-III和MIMIC-IV数据集上，针对死亡检测任务进行了验证，并与多种传统过采样技术进行了比较。

Result: QI-SMOTE在MIMIC-III和MIMIC-IV数据集上，通过死亡检测任务的实验结果表明，该方法能够显著提升包括随机森林（RF）、梯度提升（GB）、支持向量机（SVM）和深度学习模型在内的多种机器学习模型的性能。与Borderline-SMOTE、ADASYN、SMOTE-ENN、SMOTE-TOMEK和SVM-SMOTE等传统过采样技术相比，QI-SMOTE在准确率、F1分数、G-Mean和AUC-ROC等关键性能指标上表现更优。

Conclusion: QI-SMOTE通过将量子启发的变换整合到机器学习流程中，不仅能够缓解类别不平衡问题，还能提高预测模型在医学诊断和决策制定中的鲁棒性和可靠性。本研究证明了量子启发重采样技术在推动最先进的机器学习方法方面的潜力。

Abstract: Class imbalance remains a critical challenge in machine learning (ML),
particularly in the medical domain, where underrepresented minority classes
lead to biased models and reduced predictive performance. This study introduces
Quantum-Inspired SMOTE (QI-SMOTE), a novel data augmentation technique that
enhances the performance of ML classifiers, including Random Forest (RF),
Support Vector Machine (SVM), Logistic Regression (LR), k-Nearest Neighbors
(KNN), Gradient Boosting (GB), and Neural Networks, by leveraging quantum
principles such as quantum evolution and layered entanglement. Unlike
conventional oversampling methods, QI-SMOTE generates synthetic instances that
preserve complex data structures, improving model generalization and
classification accuracy. We validate QI-SMOTE on the MIMIC-III and MIMIC-IV
datasets, using mortality detection as a benchmark task due to their clinical
significance and inherent class imbalance. We compare our method against
traditional oversampling techniques, including Borderline-SMOTE, ADASYN,
SMOTE-ENN, SMOTE-TOMEK, and SVM-SMOTE, using key performance metrics such as
Accuracy, F1-score, G-Mean, and AUC-ROC. The results demonstrate that QI-SMOTE
significantly improves the effectiveness of ensemble methods (RF, GB, ADA),
kernel-based models (SVM), and deep learning approaches by producing more
informative and balanced training data. By integrating quantum-inspired
transformations into the ML pipeline, QI-SMOTE not only mitigates class
imbalance but also enhances the robustness and reliability of predictive models
in medical diagnostics and decision-making. This study highlights the potential
of quantum-inspired resampling techniques in advancing state-of-the-art ML
methodologies.

</details>


### [140] [Improving Generative Methods for Causal Evaluation via Simulation-Based Inference](https://arxiv.org/abs/2509.02892)
*Pracheta Amaranath,Vinitra Muralikrishnan,Amit Sharma,David D. Jensen*

Main category: cs.LG

TL;DR: 现有生成方法在生成与源数据分布一致的合成数据集方面存在局限性，SBICE框架通过将生成参数建模为不确定参数并推断其后验分布来解决此问题，从而提高因果估计量评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法在生成合成数据集时，要求用户提供参数的点估计而非分布，且估计固定不变，无法表达参数值的不确定性，也无法进行后验推断，可能导致因果估计量比较不可靠。

Method: 提出模拟基础推断因果评估（SBICE）框架，该框架将生成参数视为不确定参数，并根据源数据集推断其后验分布。利用模拟基础推断技术，SBICE识别能够生成与源数据分布高度一致的合成数据集的参数配置。

Result: SBICE通过生成更真实的数据集，提高了因果估计量评估的可靠性，支持在不确定性下进行稳健且数据一致的因果基准测试。

Conclusion: SBICE通过将生成参数建模为不确定参数并推断其后验分布，解决了现有生成方法在合成数据集生成方面的局限性，提高了因果估计量评估的可靠性。

Abstract: Generating synthetic datasets that accurately reflect real-world
observational data is critical for evaluating causal estimators, but remains a
challenging task. Existing generative methods offer a solution by producing
synthetic datasets anchored in the observed data (source data) while allowing
variation in key parameters such as the treatment effect and amount of
confounding bias. However, existing methods typically require users to provide
point estimates of such parameters (rather than distributions) and fixed
estimates (rather than estimates that can be improved with reference to the
source data). This denies users the ability to express uncertainty over
parameter values and removes the potential for posterior inference, potentially
leading to unreliable estimator comparisons. We introduce simulation-based
inference for causal evaluation (SBICE), a framework that models generative
parameters as uncertain and infers their posterior distribution given a source
dataset. Leveraging techniques in simulation-based inference, SBICE identifies
parameter configurations that produce synthetic datasets closely aligned with
the source data distribution. Empirical results demonstrate that SBICE improves
the reliability of estimator evaluations by generating more realistic datasets,
which supports a robust and data-consistent approach to causal benchmarking
under uncertainty.

</details>


### [141] [A Narrative Review of Clinical Decision Support Systems in Offloading Footwear for Diabetes-Related Foot Ulcers](https://arxiv.org/abs/2509.02923)
*Kunal Kumar,Muhammad Ashad Kabir,Luke Donnan,Sayed Ahmed*

Main category: cs.LG

TL;DR: 定制化糖尿病足溃疡（DFU）鞋具处方决策碎片化，需要结合多种数据和模型来开发决策支持系统（CDSS）。


<details>
  <summary>Details</summary>
Motivation: 现有的糖尿病足溃疡（DFU）鞋具处方决策存在特征选择不一、个性化不足、评估方法各异等问题，需要更系统化的方法。

Method: 对45项关于DFU鞋具处方的研究（包括指南/协议、知识系统和机器学习应用）进行叙述性回顾，并对知识类型、决策逻辑、评估方法和技术进行主题分析。

Result: 指南主要关注足底压力（PP）阈值，但缺乏可操作的特征级输出。知识系统结合PP监测、依从性跟踪和可用性测试。机器学习模型计算精度高，但可解释性和临床验证有限。评估方法分散，侧重于生物力学测试、可用性/依从性或技术准确性。

Conclusion: 提出一个五部分CDSS框架，包括最小数据集、混合模型架构（规则、优化、可解释ML）、结构化特征级输出、持续验证和临床/远程医疗整合，以实现可扩展、以患者为中心的DFU护理CDSS。

Abstract: Offloading footwear helps prevent and treat diabetic foot ulcers (DFUs) by
lowering plantar pressure (PP), yet prescription decisions remain fragmented:
feature selection varies, personalization is limited, and evaluation practices
differ. We performed a narrative review of 45 studies (12 guidelines/protocols,
25 knowledge-based systems, 8 machine-learning applications) published to Aug
2025. We thematically analyzed knowledge type, decision logic, evaluation
methods, and enabling technologies. Guidelines emphasize PP thresholds (<=200
kPa or >=25--30\% reduction) but rarely yield actionable, feature-level
outputs. Knowledge-based systems use rule- and sensor-driven logic, integrating
PP monitoring, adherence tracking, and usability testing. ML work introduces
predictive, optimization, and generative models with high computational
accuracy but limited explainability and clinical validation. Evaluation remains
fragmented: protocols prioritize biomechanical tests; knowledge-based systems
assess usability/adherence; ML studies focus on technical accuracy with weak
linkage to long-term outcomes. From this synthesis we propose a five-part CDSS
framework: (1) a minimum viable dataset; (2) a hybrid architecture combining
rules, optimization, and explainable ML; (3) structured feature-level outputs;
(4) continuous validation and evaluation; and (5) integration with clinical and
telehealth workflows. This framework aims to enable scalable, patient-centered
CDSSs for DFU care; prioritizing interoperable datasets, explainable models,
and outcome-focused evaluation will be key to clinical adoption.

</details>


### [142] [PDRL: Post-hoc Descriptor-based Residual Learning for Uncertainty-Aware Machine Learning Potentials](https://arxiv.org/abs/2509.02927)
*Shih-Peng Huang,Nontawat Charoenphakdee,Yuta Tsuboi,Yong-Bin Zhuang,Wenwen Li*

Main category: cs.LG

TL;DR: Ensemble methods are the standard for uncertainty quantification (UQ) in machine learning interatomic potentials (MLIPs), but are computationally expensive. This paper introduces PDRL, a post-hoc framework that uses trained graph neural network descriptors to estimate prediction errors, offering a computationally efficient alternative for UQ.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of ensemble methods for uncertainty quantification (UQ) in machine learning interatomic potentials (MLIPs) limits their practical application. Existing efficient alternatives may require retraining or affect accuracy.

Method: PDRL is a post-hoc framework that utilizes the descriptor of a trained graph neural network potential to estimate residual errors. It models the discrepancy between MLIP predictions and ground truth values, using these residuals as proxies for prediction uncertainty. The paper explores multiple variants of PDRL and benchmarks them against existing UQ methods.

Result: The paper benchmarks multiple variants of PDRL against established UQ methods, evaluating their effectiveness and limitations in terms of computational efficiency and accuracy.

Conclusion: PDRL offers a simple and efficient post-hoc framework for UQ in MLIPs, addressing the limitations of computational cost associated with traditional ensemble methods by using descriptors to estimate prediction residuals.

Abstract: Ensemble method is considered the gold standard for uncertainty
quantification (UQ) for machine learning interatomic potentials (MLIPs).
However, their high computational cost can limit its practicality. Alternative
techniques, such as Monte Carlo dropout and deep kernel learning, have been
proposed to improve computational efficiency; however, some of these methods
cannot be applied to already trained models and may affect the prediction
accuracy. In this paper, we propose a simple and efficient post-hoc framework
for UQ that leverages the descriptor of a trained graph neural network
potential to estimate residual errors. We refer to this method as post-hoc
descriptor-based residual-based learning (PDRL). PDRL models the discrepancy
between MLIP predictions and ground truth values, allowing these residuals to
act as proxies for prediction uncertainty. We explore multiple variants of PDRL
and benchmark them against established UQ methods, evaluating both their
effectiveness and limitations.

</details>


### [143] [VendiRL: A Framework for Self-Supervised Reinforcement Learning of Diversely Diverse Skills](https://arxiv.org/abs/2509.02930)
*Erik M. Lintunen*

Main category: cs.LG

TL;DR: VendiRL使用生态学中的Vendi Score来评估和学习多样化的技能，解决了自监督强化学习中技能学习的扩展性和评估问题。


<details>
  <summary>Details</summary>
Motivation: 自监督强化学习（RL）旨在学习多样化的技能以应对未来未知任务，但面临扩展性和评估方面的挑战。现有的方法在处理高维特征空间和定义技能多样性方面存在不足，导致结果难以比较和部分多样性形式被忽略。

Method: 提出使用源自生态学的Vendi Score作为一种样本多样性度量，允许用户定义和评估任意形式的技能多样性。在此基础上，引入了一个名为VendiRL的统一框架，用于学习多样化的技能集。

Result: VendiRL能够根据不同的相似性函数，激发不同形式的多样性，支持在新的、交互性强的环境中进行技能多样性预训练。

Conclusion: VendiRL通过引入Vendi Score解决了自监督RL中的技能多样性学习和评估问题，为预训练智能体提供了更灵活和可比性的方法。

Abstract: In self-supervised reinforcement learning (RL), one of the key challenges is
learning a diverse set of skills to prepare agents for unknown future tasks.
Despite impressive advances, scalability and evaluation remain prevalent
issues. Regarding scalability, the search for meaningful skills can be obscured
by high-dimensional feature spaces, where relevant features may vary across
downstream task domains. For evaluating skill diversity, defining what
constitutes "diversity" typically requires a hard commitment to a specific
notion of what it means for skills to be diverse, potentially leading to
inconsistencies in how skill diversity is understood, making results across
different approaches hard to compare, and leaving many forms of diversity
unexplored. To address these issues, we adopt a measure of sample diversity
that translates ideas from ecology to machine learning -- the Vendi Score --
allowing the user to specify and evaluate any desired form of diversity. We
demonstrate how this metric facilitates skill evaluation and introduce VendiRL,
a unified framework for learning diversely diverse sets of skills. Given
distinct similarity functions, VendiRL motivates distinct forms of diversity,
which could support skill-diversity pretraining in new and richly interactive
environments where optimising for various forms of diversity may be desirable.

</details>


### [144] [Delayed Momentum Aggregation: Communication-efficient Byzantine-robust Federated Learning with Partial Participation](https://arxiv.org/abs/2509.02970)
*Kaoru Otsuka,Yuki Takezawa,Makoto Yamada*

Main category: cs.LG

TL;DR: Federated Learning (FL) can be vulnerable to Byzantine clients, and existing robust methods often assume full client participation, which is unrealistic. This paper introduces delayed momentum aggregation and an optimizer (D-Byz-SGDM) to address Byzantine-robust FL with partial participation, achieving theoretical convergence guarantees and validated by experiments.


<details>
  <summary>Details</summary>
Motivation: Existing Byzantine-robust FL methods fail under partial participation when a Byzantine majority is sampled, posing a challenge for sparse communication. This work aims to develop a robust FL method that can handle partial client participation effectively.

Method: The paper introduces a novel principle called delayed momentum aggregation, where the server aggregates recent gradients from non-participating clients with fresh momentum from active clients. An optimizer, D-Byz-SGDM, is developed based on this principle.

Result: D-Byz-SGDM establishes convergence guarantees that match theoretical lower bounds for partial participation and recover previous full participation results. Experiments on deep learning tasks demonstrate stable and robust training under various Byzantine attacks.

Conclusion: The proposed delayed momentum aggregation principle and D-Byz-SGDM optimizer effectively address the challenge of Byzantine-robust Federated Learning with partial client participation, achieving strong theoretical guarantees and practical validation through experiments.

Abstract: Federated Learning (FL) allows distributed model training across multiple
clients while preserving data privacy, but it remains vulnerable to Byzantine
clients that exhibit malicious behavior. While existing Byzantine-robust FL
methods provide strong convergence guarantees (e.g., to a stationary point in
expectation) under Byzantine attacks, they typically assume full client
participation, which is unrealistic due to communication constraints and client
availability. Under partial participation, existing methods fail immediately
after the sampled clients contain a Byzantine majority, creating a fundamental
challenge for sparse communication. First, we introduce delayed momentum
aggregation, a novel principle where the server aggregates the most recently
received gradients from non-participating clients alongside fresh momentum from
active clients. Our optimizer D-Byz-SGDM (Delayed Byzantine-robust SGD with
Momentum) implements this delayed momentum aggregation principle for
Byzantine-robust FL with partial participation. Then, we establish convergence
guarantees that recover previous full participation results and match the
fundamental lower bounds we prove for the partial participation setting.
Experiments on deep learning tasks validated our theoretical findings, showing
stable and robust training under various Byzantine attacks.

</details>


### [145] [AdaGrad Meets Muon: Adaptive Stepsizes for Orthogonal Updates](https://arxiv.org/abs/2509.02981)
*Minxin Zhang,Yuxuan Liu,Hayden Schaeffer*

Main category: cs.LG

TL;DR: AdaGO是一种结合了AdaGrad和Muon优化器的优点的新算法，通过正交更新方向和自适应步长来提高优化效率。


<details>
  <summary>Details</summary>
Motivation: 现有Muon优化器在更新权重时采用正交动量，但学习率的确定尚不明确；而AdaGrad方法虽然广泛使用，但其更新方向可能不稳定。因此，需要一种结合两者优点的算法。

Method: AdaGO算法结合了基于范数的AdaGrad类型步长和正交更新方向。它通过累积过去的梯度范数来调整步长，并保持更新方向的正交性，可以看作是谱下降方向。

Result: AdaGO在CIFAR-10分类和函数回归任务上，相比于Muon和Adam取得了更好的效果。理论上，该算法在非凸函数、随机和确定性设置下，以及标准平滑和无偏有界噪声假设下，都具有最优的收敛速率。

Conclusion: AdaGO算法通过结合AdaGrad的自适应步长和Muon的正交更新方向，不仅提高了优化效率，还在理论上保证了最优收敛性，并在实际任务中取得了优于现有方法的性能。

Abstract: The recently proposed Muon optimizer updates weight matrices via
orthogonalized momentum and has demonstrated strong empirical success in large
language model training. However, it remains unclear how to determine the
learning rates for such orthogonalized updates. AdaGrad, by contrast, is a
widely used adaptive method that scales stochastic gradients by accumulated
past gradients. We propose a new algorithm, AdaGO, which combines a norm-based
AdaGrad-type stepsize with an orthogonalized update direction, bringing
together the benefits of both approaches. Unlike other adaptive variants of
Muon, AdaGO preserves the orthogonality of the update direction, which can be
interpreted as a spectral descent direction, while adapting the stepsizes to
the optimization landscape by scaling the direction with accumulated past
gradient norms. The implementation of AdaGO requires only minimal modification
to Muon, with a single additional scalar variable, the accumulated squared
gradient norms, to be computed, making it computationally and memory efficient.
Optimal theoretical convergence rates are established for nonconvex functions
in both stochastic and deterministic settings under standard smoothness and
unbiased bounded-variance noise assumptions. Empirical results on CIFAR-10
classification and function regression demonstrate that AdaGO outperforms Muon
and Adam.

</details>


### [146] [Multimodal learning of melt pool dynamics in laser powder bed fusion](https://arxiv.org/abs/2509.03029)
*Satyajit Mojumder,Pallock Halder,Tiana Tonge*

Main category: cs.LG

TL;DR: 该研究提出了一种多模态数据融合方法，结合X射线和光电二极管吸收率数据，用于预测激光粉末床熔融（LPBF）过程中的熔池动态，并通过迁移学习实现了仅使用吸收率数据进行高精度预测，从而降低了成本并提高了实时监控能力。


<details>
  <summary>Details</summary>
Motivation: 现有传感器在增材制造实时监控中存在局限性，例如X射线成本高且不实用，而光电二极管吸收率数据噪声大，单独使用效果不佳。因此，需要一种结合多种传感器数据的方法来提高熔池动态预测的准确性和实用性。

Method: 提出了一种多模态学习框架，结合使用卷积神经网络（CNN）从X射线数据中提取空间特征，并使用循环神经网络（RNN）从吸收率信号中提取时间特征，采用早期融合策略。该框架还用作迁移学习模型，以微调仅使用吸收率数据的RNN模型。

Result: 实验结果表明，结合两种数据模态进行训练比单独使用任何一种模态都能显著提高预测精度。此外，经过训练后，模型仅使用吸收率数据即可推断熔池特征，无需昂贵的X射线成像。

Conclusion: 所提出的多模态数据融合方法能够有效地结合不同传感器的优势，实现对增材制造过程中熔池动态的成本效益高、实时监控。该方法具有广泛的应用前景，特别是在通过迁移学习实现仅用低成本传感器进行高精度预测方面。

Abstract: While multiple sensors are used for real-time monitoring in additive
manufacturing, not all provide practical or reliable process insights. For
example, high-speed X-ray imaging offers valuable spatial information about
subsurface melt pool behavior but is costly and impractical for most industrial
settings. In contrast, absorptivity data from low-cost photodiodes correlate
with melt pool dynamics but is often too noisy for accurate prediction when
used alone. In this paper, we propose a multimodal data fusion approach for
predicting melt pool dynamics by combining high-fidelity X-ray data with
low-fidelity absorptivity data in the Laser Powder Bed Fusion (LPBF) process.
Our multimodal learning framework integrates convolutional neural networks
(CNNs) for spatial feature extraction from X-ray data with recurrent neural
networks (RNNs) for temporal feature extraction from absorptivity signals,
using an early fusion strategy. The multimodal model is further used as a
transfer learning model to fine-tune the RNN model that can predict melt pool
dynamics only with absorptivity, with greater accuracy compared to the
multimodal model. Results show that training with both modalities significantly
improves prediction accuracy compared to using either modality alone.
Furthermore, once trained, the model can infer melt pool characteristics using
only absorptivity data, eliminating the need for expensive X-ray imaging. This
multimodal fusion approach enables cost-effective, real-time monitoring and has
broad applicability in additive manufacturing.

</details>


### [147] [Knowledge Integration for Physics-informed Symbolic Regression Using Pre-trained Large Language Models](https://arxiv.org/abs/2509.03036)
*Bilge Taskin,Wenxiong Xie,Teddy Lazebnik*

Main category: cs.LG

TL;DR: 本研究利用预训练大语言模型（LLMs）来促进物理信息符号回归（PiSR）中的知识整合，旨在自动化地整合领域知识，减少手动干预的需求，并使该过程更容易应用于更广泛的科学问题。


<details>
  <summary>Details</summary>
Motivation: 当前的物理信息符号回归（PiSR）方法通常需要专门的公式和手动特征工程，这限制了它们在领域专家之外的适用性。本研究旨在通过利用预训练大语言模型（LLMs）来解决这一限制，以促进知识整合。

Method: 将大语言模型（LLM）集成到符号回归（SR）的损失函数中，增加一个LLM评估SR生成的方程的项。

Result: 通过在三种SR算法和三种预训练LLM上进行广泛评估，结果表明LLM集成能够持续改进从数据中重建物理动力学，并提高SR模型对噪声和复杂性的鲁棒性。此外，还探索了提示工程的影响，发现信息更丰富的提示可以显著提高性能。

Conclusion: 利用预训练大语言模型（LLMs）进行知识整合是改进物理信息符号回归（PiSR）的一种有效方法，可以提高发现方程的泛化性和实用性。

Abstract: Symbolic regression (SR) has emerged as a powerful tool for automated
scientific discovery, enabling the derivation of governing equations from
experimental data. A growing body of work illustrates the promise of
integrating domain knowledge into the SR to improve the discovered equation's
generality and usefulness. Physics-informed SR (PiSR) addresses this by
incorporating domain knowledge, but current methods often require specialized
formulations and manual feature engineering, limiting their adaptability only
to domain experts. In this study, we leverage pre-trained Large Language Models
(LLMs) to facilitate knowledge integration in PiSR. By harnessing the
contextual understanding of LLMs trained on vast scientific literature, we aim
to automate the incorporation of domain knowledge, reducing the need for manual
intervention and making the process more accessible to a broader range of
scientific problems. Namely, the LLM is integrated into the SR's loss function,
adding a term of the LLM's evaluation of the SR's produced equation. We
extensively evaluate our method using three SR algorithms (DEAP, gplearn, and
PySR) and three pre-trained LLMs (Falcon, Mistral, and LLama 2) across three
physical dynamics (dropping ball, simple harmonic motion, and electromagnetic
wave). The results demonstrate that LLM integration consistently improves the
reconstruction of physical dynamics from data, enhancing the robustness of SR
models to noise and complexity. We further explore the impact of prompt
engineering, finding that more informative prompts significantly improve
performance.

</details>


### [148] [Binary Quantization For LLMs Through Dynamic Grouping](https://arxiv.org/abs/2509.03054)
*Xinzhe Zheng,Zhen-Qun Yang,Haoran Xie,S. Joe Qin,Arlene Chen,Fangzhen Lin*

Main category: cs.LG

TL;DR: 通过引入新的二值化优化目标和自适应分组策略，提出了一种高效的二值化大语言模型方法，显著降低了模型大小和计算成本，同时保持了高质量的模型性能，甚至优于一些4位量化方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型（LLMs）需要大量内存和计算资源的问题，并克服现有二值化（1-bit）量化方法在性能上相比4-bit方法有显著下降的挑战。

Method: 提出了一种新颖的二值化优化目标，并设计了三种算法来实现它。该方法通过自适应分组策略动态识别最优非结构化子矩阵，以增强块量化。

Result: 在LLaMA 3.2 3B模型上，实现了平均1.007比特的量化，困惑度（perplexity）达到8.23，接近原始模型的7.81，远优于先前SOTA BiLLM的123.90。同时，该方法在性能和效率上可与SOTA 4-bit方法（如GPTQ）相媲美。量化过程高效，单核CPU仅需14秒即可完成LLaMA 3.2 3B模型量化，且易于并行化。

Conclusion: 所提出的二值化方法能够有效减小大语言模型的存储和推理成本，同时保持高质量的模型性能，有望成为一种有竞争力的模型压缩技术。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
a wide range of Natural Language Processing (NLP) tasks, but require
substantial memory and computational resources. Binary quantization, which
compresses model weights from 16-bit Brain Float to 1-bit representations in
{-1, 1}, offers significant reductions in storage and inference costs. However,
such aggressive quantization often leads to notable performance degradation
compared to more conservative 4-bit quantization methods. In this research, we
propose a novel optimization objective tailored for binary quantization, along
with three algorithms designed to realize it effectively. Our method enhances
blocked quantization by dynamically identifying optimal unstructured
sub-matrices through adaptive grouping strategies. Experimental results
demonstrate that our approach achieves an average bit length of just 1.007
bits, while maintaining high model quality. Specifically, our quantized LLaMA
3.2 3B model attains a perplexity of 8.23, remarkably close to the original
7.81, and surpasses previous SOTA BiLLM with a perplexity of only 123.90.
Furthermore, our method is competitive with SOTA 4-bit approaches such as GPTQ
in both performance and efficiency. The compression process is highly
efficient, requiring only 14 seconds to quantize the full LLaMA 3.2 3B weights
on a single CPU core, with the entire process completing in under 100 minutes
and exhibiting embarrassingly parallel properties.
  Code - https://github.com/johnnyzheng0636/WGM_bi_quan

</details>


### [149] [Discrete Functional Geometry of ReLU Networks via ReLU Transition Graphs](https://arxiv.org/abs/2509.03056)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 深度ReLU网络的图论模型，通过ReLU转换图（RTG）分析其结构、泛化和容量。


<details>
  <summary>Details</summary>
Motivation: 提出一个将深度ReLU网络建模为图的通用框架，以理解其功能行为、泛化能力和容量。

Method: 通过将ReLU网络表示为ReLU转换图（RTG），其中节点代表激活区域，边代表ReLU激活的变化。分析RTG的结构属性（如扩展性、度分布、谱属性）和泛化界限（如区域熵、谱隙、KL散度）。

Result: 理论上证明了随机初始化的RTG具有扩展性、二项分布的度分布和控制泛化的谱属性。提出了基于区域熵和谱隙的容量和泛化新界限。经验上验证了理论预测，显示了区域熵饱和、谱隙与泛化相关以及KL散度反映平滑度。

Conclusion: RTG提供了一个统一的框架，通过离散函数几何来分析ReLU网络，为理解、诊断和改进泛化能力提供了新工具。

Abstract: We extend the ReLU Transition Graph (RTG) framework into a comprehensive
graph-theoretic model for understanding deep ReLU networks. In this model, each
node represents a linear activation region, and edges connect regions that
differ by a single ReLU activation flip, forming a discrete geometric structure
over the network's functional behavior. We prove that RTGs at random
initialization exhibit strong expansion, binomial degree distributions, and
spectral properties that tightly govern generalization. These structural
insights enable new bounds on capacity via region entropy and on generalization
via spectral gap and edge-wise KL divergence. Empirically, we construct RTGs
for small networks, measure their smoothness and connectivity properties, and
validate theoretical predictions. Our results show that region entropy
saturates under overparameterization, spectral gap correlates with
generalization, and KL divergence across adjacent regions reflects functional
smoothness. This work provides a unified framework for analyzing ReLU networks
through the lens of discrete functional geometry, offering new tools to
understand, diagnose, and improve generalization.

</details>


### [150] [Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers](https://arxiv.org/abs/2509.03059)
*Xingyue Huang,Rishabh,Gregor Franke,Ziyi Yang,Jiamu Bai,Weijie Bai,Jinhe Bi,Zifeng Ding,Yiqun Duan,Chengyu Fan,Wendong Fan,Xin Gao,Ruohao Guo,Yuan He,Zhuangzhuang He,Xianglong Hu,Neil Johnson,Bowen Li,Fangru Lin,Siyu Lin,Tong Liu,Yunpu Ma,Hao Shen,Hao Sun,Beibei Wang,Fangyijie Wang,Hao Wang,Haoran Wang,Yang Wang,Yifeng Wang,Zhaowei Wang,Ziyang Wang,Yifan Wu,Zikai Xiao,Chengxing Xie,Fan Yang,Junxiao Yang,Qianshuo Ye,Ziyu Ye,Guangtao Zeng,Yuwen Ebony Zhang,Zeyu Zhang,Zihao Zhu,Bernard Ghanem,Philip Torr,Guohao Li*

Main category: cs.LG

TL;DR: LLMs在数学和编程等领域通过RLVR取得了显著进步，但在其他需要复杂推理的领域面临数据稀缺和成本高昂的挑战。本项目提出了Loong项目，一个用于大规模合成数据生成和验证的开源框架，包含LoongBench（一个包含8729个人工验证示例的种子数据集）和LoongEnv（一个支持多种提示策略的模块化合成数据生成环境），旨在通过代码执行结果为LLM生成符合逻辑的推理链提供奖励，从而改进LLM在多样化推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的强化学习方法（RLVR）在数学和编程领域取得成功，但受限于高质量、可验证数据集的稀缺以及高昂的人工监督成本，难以推广到其他需要复杂推理的领域。

Method: 本项目提出了Loong项目，一个包含LoongBench（种子数据集）和LoongEnv（合成数据生成环境）的开源框架。LoongEnv利用多种提示策略生成“问题-答案-代码”三元组，并与LoongBench共同构成一个智能体-环境循环，用于强化学习。智能体（LLM）通过生成与代码执行结果一致的推理链（Chain-of-Thought）来获得奖励。

Result: 通过LoongBench对一系列开源和闭源LLM进行了基准测试，评估了其领域覆盖能力并揭示了性能瓶颈。同时，对LoongEnv生成的合成数据进行了全面分析，考察了其正确性、难度和多样性。

Conclusion: Loong项目提供了一个可扩展的框架，用于在多样化的推理密集型领域生成和验证合成数据，并通过RLVR技术促进LLM在这些领域能力的提升。

Abstract: Recent advances in Large Language Models (LLMs) have shown that their
reasoning capabilities can be significantly improved through Reinforcement
Learning with Verifiable Reward (RLVR), particularly in domains like
mathematics and programming, where ground-truth correctness can be
automatically evaluated. However, extending this success to other
reasoning-intensive domains remains challenging due to the scarcity of
high-quality, verifiable datasets and the high cost of human supervision. In
this work, we introduce the Loong Project: an open-source framework for
scalable synthetic data generation and verification across a diverse range of
reasoning-intensive domains. The framework consists of two key components: (1)
LoongBench, a curated seed dataset containing 8,729 human-vetted examples
across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired
with executable code and rich metadata; and (2) LoongEnv, a modular synthetic
data generation environment that supports multiple prompting strategies to
produce new question-answer-code triples. Together, these components form an
agent-environment loop that enables reinforcement learning, where an LLM-based
agent is rewarded for generating Chain-of-Thought (CoT) solutions that align
with code-executed answers. Empirically, we benchmark LoongBench on a broad
suite of both open-source and proprietary LLMs to evaluate domain coverage and
reveal performance bottlenecks. In addition, we conduct a comprehensive
analysis of synthetic data generated by LoongEnv, examining correctness,
difficulty, and diversity. Code and documentation are available at
https://github.com/camel-ai/loong.

</details>


### [151] [LSAM: Asynchronous Distributed Training with Landscape-Smoothed Sharpness-Aware Minimization](https://arxiv.org/abs/2509.03110)
*Yunfei Teng,Sixin Zhang*

Main category: cs.LG

TL;DR: LSAM通过异步分布式采样策略解决了SAM在大批量分布式训练中的效率问题，在保持泛化优势的同时提高了收敛速度和最终精度。


<details>
  <summary>Details</summary>
Motivation: SAM在提高泛化能力方面表现出色，但在分布式大批量训练中效率低下。

Method: LSAM将SAM的对抗性步骤与异步分布式采样策略相结合，生成平滑的感知损失景观，从而消除同步瓶颈并加速收敛。

Result: LSAM在收敛速度和最终精度方面优于数据并行SAM。

Conclusion: LSAM通过结合SAM的对抗性步骤和异步分布式采样，在提高泛化能力的同时，解决了SAM在大批量分布式训练中的效率问题。

Abstract: While Sharpness-Aware Minimization (SAM) improves generalization in deep
neural networks by minimizing both loss and sharpness, it suffers from
inefficiency in distributed large-batch training. We present Landscape-Smoothed
SAM (LSAM), a novel optimizer that preserves SAM's generalization advantages
while offering superior efficiency. LSAM integrates SAM's adversarial steps
with an asynchronous distributed sampling strategy, generating an asynchronous
distributed sampling scheme, producing a smoothed sharpness-aware loss
landscape for optimization. This design eliminates synchronization bottlenecks,
accelerates large-batch convergence, and delivers higher final accuracy
compared to data-parallel SAM.

</details>


### [152] [LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence](https://arxiv.org/abs/2509.03505)
*Xingxuan Zhang,Gang Ren,Han Yu,Hao Yuan,Hui Wang,Jiansheng Li,Jiayun Wu,Lang Mo,Li Mao,Mingchao Hao,Ningbo Dai,Renzhe Xu,Shuyang Li,Tianyang Zhang,Yue He,Yuanrui Wang,Yunjia Zhang,Zijing Xu,Dongzhe Li,Fang Gao,Hao Zou,Jiandong Liu,Jiashuo Liu,Jiawei Xu,Kaijie Cheng,Kehan Li,Linjun Zhou,Qing Li,Shaohua Fan,Xiaoyu Lin,Xinyan Han,Xuanyue Li,Yan Lu,Yuan Xue,Yuanyuan Jiang,Zimu Wang,Zhenlei Wang,Peng Cui*

Main category: cs.LG

TL;DR: LimiX 是第一个大规模结构化数据模型 (LDM)，它将结构化数据视为变量和缺失的联合分布，能够通过单一模型进行查询式条件预测来处理各种表格任务。


<details>
  <summary>Details</summary>
Motivation: 实现通用智能需要语言、物理世界和结构化数据作为基础模型。

Method: LimiX 通过掩码联合分布建模进行预训练，采用具有时序性的、条件于上下文的目标，模型可以根据特定数据集的上下文来预测查询子集，从而在推理时实现无需训练的快速适应。

Result: LimiX 在 10 个大规模结构化数据基准测试中表现优于梯度提升树、深度表格网络、近期表格基础模型和自动化集成等强有力基线，在分类、回归、缺失值填补和数据生成等任务中都表现出色。

Conclusion: LimiX 是一种能够处理多种表格任务的单一模型，它在各种基准测试中都取得了优异的成绩，证明了其在结构化数据建模方面的潜力。

Abstract: We argue that progress toward general intelligence requires complementary
foundation models grounded in language, the physical world, and structured
data. This report presents LimiX, the first installment of our large
structured-data models (LDMs). LimiX treats structured data as a joint
distribution over variables and missingness, thus capable of addressing a wide
range of tabular tasks through query-based conditional prediction via a single
model. LimiX is pretrained using masked joint-distribution modeling with an
episodic, context-conditional objective, where the model predicts for query
subsets conditioned on dataset-specific contexts, supporting rapid,
training-free adaptation at inference. We evaluate LimiX across 10 large
structured-data benchmarks with broad regimes of sample size, feature
dimensionality, class number, categorical-to-numerical feature ratio,
missingness, and sample-to-feature ratios. With a single model and a unified
interface, LimiX consistently surpasses strong baselines including
gradient-boosting trees, deep tabular networks, recent tabular foundation
models, and automated ensembles, as shown in Figure 1 and Figure 2. The
superiority holds across a wide range of tasks, such as classification,
regression, missing value imputation, and data generation, often by substantial
margins, while avoiding task-specific architectures or bespoke training per
task. All LimiX models are publicly accessible under Apache 2.0.

</details>


### [153] [A Neural Network Approach to Multi-radionuclide TDCR Beta Spectroscopy](https://arxiv.org/abs/2509.03137)
*Li Yi,Qian Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Liquid scintillation triple-to-doubly coincident ratio (TDCR) spectroscopy is
widely adopted as a standard method for radionuclide quantification because of
its inherent advantages such as high precision, self-calibrating capability,
and independence from radioactive reference sources. However, multiradionuclide
analysis via TDCR faces the challenges of limited automation and reliance on
mixture-specific standards, which may not be easily available. Here, we present
an Artificial Intelligence (AI) framework that combines numerical spectral
simulation and deep learning for standard-free automated analysis. $\beta$
spectra for model training were generated using Geant4 simulations coupled with
statistically modeled detector response sampling. A tailored neural network
architecture, trained on this dataset covering various nuclei mix ratio and
quenching scenarios, enables autonomous resolution of individual radionuclide
activities and detecting efficiency through end-to-end learning paradigms. The
model delivers consistent high accuracy across tasks: activity proportions
(mean absolute error = 0.009), detection efficiencies (mean absolute error =
0.002), and spectral reconstruction (Structural Similarity Index = 0.9998),
validating its physical plausibility for quenched $\beta$ spectroscopy. This
AI-driven methodology exhibits significant potential for automated
safety-compliant multiradionuclide analysis with robust generalization,
real-time processing capabilities, and engineering feasibility, particularly in
scenarios where reference materials are unavailable or rapid field analysis is
required.

</details>


### [154] [Rashomon in the Streets: Explanation Ambiguity in Scene Understanding](https://arxiv.org/abs/2509.03169)
*Helge Spieker,Jørn Eirik Betten,Arnaud Gotlieb,Nadjib Lazaar,Nassim Belmecheri*

Main category: cs.LG

TL;DR: 可解释人工智能（XAI）在自动驾驶等安全关键应用中至关重要，但Rashomon效应（即多个同样准确的模型可能提供不同的解释）对其可靠性构成了挑战。本文首次对该效应在真实驾驶场景中的行为预测任务进行了实证量化。


<details>
  <summary>Details</summary>
Motivation: 解释AI（XAI）对于在自动驾驶等安全关键应用中验证和信任模型至关重要。然而，XAI的可靠性受到Rashomon效应的挑战，即多个同样准确的模型可以为相同的预测提供不同的解释。

Method: 使用定性可解释图（QXGs）作为符号场景表示，我们训练了两种不同模型类别的Rashomon集合：可解释的、基于对的梯度提升模型和复杂的、基于图的图神经网络（GNN）。使用特征归因方法，我们测量了这些模型类别内部和之间解释的一致性。

Result: 结果显示，解释分歧显著。使用定性可解释图（QXGs）作为符号场景表示，我们训练了两种不同模型类别的Rashomon集合：可解释的、基于对的梯度提升模型和复杂的、基于图的图神经网络（GNN）。

Conclusion: 我们的研究结果表明，解释歧义是该问题的一个固有属性，而不仅仅是模型构成的产物。

Abstract: Explainable AI (XAI) is essential for validating and trusting models in
safety-critical applications like autonomous driving. However, the reliability
of XAI is challenged by the Rashomon effect, where multiple, equally accurate
models can offer divergent explanations for the same prediction. This paper
provides the first empirical quantification of this effect for the task of
action prediction in real-world driving scenes. Using Qualitative Explainable
Graphs (QXGs) as a symbolic scene representation, we train Rashomon sets of two
distinct model classes: interpretable, pair-based gradient boosting models and
complex, graph-based Graph Neural Networks (GNNs). Using feature attribution
methods, we measure the agreement of explanations both within and between these
classes. Our results reveal significant explanation disagreement. Our findings
suggest that explanation ambiguity is an inherent property of the problem, not
just a modeling artifact.

</details>


### [155] [Systematic Evaluation of Attribution Methods: Eliminating Threshold Bias and Revealing Method-Dependent Performance Patterns](https://arxiv.org/abs/2509.03176)
*Serra Aksoy*

Main category: cs.LG

TL;DR: 评估归因方法时存在阈值选择偏差，可能导致方法排名反转。本文提出一种无阈值框架，通过计算交并比（AUC-IoU）曲线下面积来评估归因质量，解决了现有单一阈值评估方法的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有归因方法评估易受阈值选择偏差影响，可能导致方法排名不准确。本研究旨在解决这一问题，提供一种更可靠的评估方法。

Method: 提出一种无阈值框架，计算交并比（AUC-IoU）曲线下面积，以跨越全部阈值范围评估归因质量。

Result: 在皮肤病学影像评估中，单一阈值评估指标产生矛盾结果，而无阈值评估提供了可靠的区分。XRAI方法相比LIME提高了31%，相比集成梯度提高了204%。

Conclusion: 无阈值框架能够消除评估偏差，实现基于证据的方法选择，为医学影像及其他领域提供理论洞见和实践指导。

Abstract: Attribution methods explain neural network predictions by identifying
influential input features, but their evaluation suffers from threshold
selection bias that can reverse method rankings and undermine conclusions.
Current protocols binarize attribution maps at single thresholds, where
threshold choice alone can alter rankings by over 200 percentage points. We
address this flaw with a threshold-free framework that computes Area Under the
Curve for Intersection over Union (AUC-IoU), capturing attribution quality
across the full threshold spectrum. Evaluating seven attribution methods on
dermatological imaging, we show single-threshold metrics yield contradictory
results, while threshold-free evaluation provides reliable differentiation.
XRAI achieves 31% improvement over LIME and 204% over vanilla Integrated
Gradients, with size-stratified analysis revealing performance variations up to
269% across lesion scales. These findings establish methodological standards
that eliminate evaluation artifacts and enable evidence-based method selection.
The threshold-free framework provides both theoretical insight into attribution
behavior and practical guidance for robust comparison in medical imaging and
beyond.

</details>


### [156] [Tabular foundation model for GEOAI benchmark problems BM/AirportSoilProperties/2/2025](https://arxiv.org/abs/2509.03191)
*Taiga Saito,Yu Otake,Stephen Wu*

Main category: cs.LG

TL;DR: 该论文将一种新的表格先验数据拟合网络（TabPFN）应用于岩土工程场地特性表征问题，在两个基准问题上均取得了优于传统分层贝叶斯模型（HBM）的性能，展示了其在岩土工程领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 将表格先验数据拟合网络（TabPFN）应用于岩土工程场地特性表征问题，以解决土体性质预测和缺失参数填补的挑战。

Method: 在零训练、少样本、上下文学习的设置下，使用TabPFN结合大数据间接数据库（BID）来解决两个基准问题：空间不排水抗剪强度预测和缺失力学参数填补。

Result: TabPFN在空间不排水抗剪强度预测方面，相比HBM提高了预测精度并显著缩短了运行时间。在缺失力学参数填补方面，TabPFN的均方根误差（RMSE）更低，不确定性量化更优，但累积计算成本略高。

Conclusion: TabPFN作为一种通用的基础模型，在岩土工程建模中取得了成功的应用，并展现出优于传统HBM的性能和效率，预示着概率场地特性表征可能迎来范式转变。

Abstract: This paper presents a novel application of the Tabular Prior-Data Fitted
Network (TabPFN) - a transformer-based foundation model for tabular data - to
geotechnical site characterization problems defined in the GEOAI benchmark
BM/AirportSoilProperties/2/2025. Two tasks are addressed: (1) predicting the
spatial variation of undrained shear strength (su) across borehole depth
profiles, and (2) imputing missing mechanical parameters in a dense-site
dataset. We apply TabPFN in a zero-training, few-shot, in-context learning
setting - without hyper-parameter tuning - and provide it with additional
context from the big indirect database (BID). The study demonstrates that
TabPFN, as a general-purpose foundation model, achieved superior accuracy and
well-calibrated predictive distributions compared to a conventional
hierarchical Bayesian model (HBM) baseline, while also offering significant
gains in inference efficiency. In Benchmark Problem #1 (spatial su prediction),
TabPFN outperformed the HBM in prediction accuracy and delivered an
order-of-magnitude faster runtime. In Benchmark Problem #2 (missing mechanical
parameter imputation), TabPFN likewise achieved lower RMSE for all target
parameters with well-quantified uncertainties, though its cumulative
computation cost was higher than HBM's due to its one-variable-at-a-time
inference. These results mark the first successful use of a tabular foundation
model in geotechnical modeling, suggesting a potential paradigm shift in
probabilistic site characterization.

</details>


### [157] [Exploring the Design Space of Fair Tree Learning Algorithms](https://arxiv.org/abs/2509.03204)
*Kiara Stempel,Mattia Cerrato,Stefan Kramer*

Main category: cs.LG

TL;DR: 本文探讨了决策树学习算法在公平性方面的三种设计空间，并对其中两种新的方法进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有关于决策树公平性的研究主要集中在训练时施加约束，以在保证预测性能的同时避免对不同群体的歧视。然而，决策树学习算法的设计空间中存在三种主要范式，但其中只有两种（一是将y、s、T纳入目标函数，二是将s作为约束条件）得到了充分研究。

Method: 本文将前两种方法（目标函数包含y、s、T；将s作为约束条件）与第三种新方法（为y和s分别构建独立的决策树T_y和T_s，不共享结构）进行了比较。作者对第二种方法的非贪婪变体进行了研究，并引入了上述设计空间中的另外两种选项（第三种方法和第二种方法的非贪婪变体），并在多个数据集上进行了实验表征。

Result: 通过实验，我们发现先前未被充分研究的设计选项在某些情况下能够提供有希望的结果。

Conclusion: 决策树公平性研究的设计空间比文献中已探索的要大。本文提出的新方法在某些数据集上表现良好，表明了进一步研究的潜力。

Abstract: Decision trees have been studied extensively in the context of fairness,
aiming to maximize prediction performance while ensuring non-discrimination
against different groups. Techniques in this space usually focus on imposing
constraints at training time, constraining the search space so that solutions
which display unacceptable values of relevant metrics are not considered,
discarded, or discouraged. If we assume one target variable y and one sensitive
attribute s, the design space of tree learning algorithms can be spanned as
follows: (i) One can have one tree T that is built using an objective function
that is a function of y, s, and T. For instance, one can build a tree based on
the weighted information gain regarding y (maximizing) and s (minimizing). (ii)
The second option is to have one tree model T that uses an objective function
in y and T and a constraint on s and T. Here, s is no longer part of the
objective, but part of a constraint. This can be achieved greedily by aborting
a further split as soon as the condition that optimizes the objective in y
fails to satisfy the constraint on s. A simple way to explore other splits is
to backtrack during tree construction once a fairness constraint is violated.
(iii) The third option is to have two trees T_y and T_s, one for y and one for
s, such that the tree structure for y and s does not have to be shared. In this
way, information regarding y and regarding s can be used independently, without
having to constrain the choices in tree construction by the mutual information
between the two variables. Quite surprisingly, of the three options, only the
first one and the greedy variant of the second have been studied in the
literature so far. In this paper, we introduce the above two additional options
from that design space and characterize them experimentally on multiple
datasets.

</details>


### [158] [Autonomous Learning From Success and Failure: Goal-Conditioned Supervised Learning with Negative Feedback](https://arxiv.org/abs/2509.03206)
*Zeqiang Zhang,Fabian Wurzberger,Gerrit Schmid,Sebastian Gottwald,Daniel A. Braun*

Main category: cs.LG

TL;DR: 该研究提出了一种结合对比学习和目标条件监督学习（GCSL）的新模型，以解决强化学习中稀疏奖励和 GCSL 固有局限性的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习在稀疏奖励任务中面临挑战。模仿学习依赖人类演示。GCSL 允许自主系统进行自我模仿，但存在利用自身经验加剧偏差和仅从成功中学习而忽略失败的问题。

Method: 提出了一种新的模型，将对比学习原理整合到 GCSL 框架中，以从成功和失败中学习。

Result: 该算法克服了 GCSL 中由初始偏差引起 的局限性，实现了更具探索性的行为，并最终在具有挑战性的环境中取得了更好的性能。

Conclusion: 通过整合对比学习，所提出的模型能够从成功和失败中学习，克服 GCSL 的局限性，从而在稀疏奖励环境中实现更优越的性能。

Abstract: Reinforcement learning faces significant challenges when applied to tasks
characterized by sparse reward structures. Although imitation learning, within
the domain of supervised learning, offers faster convergence, it relies heavily
on human-generated demonstrations. Recently, Goal-Conditioned Supervised
Learning (GCSL) has emerged as a potential solution by enabling self-imitation
learning for autonomous systems. By strategically relabelling goals, agents can
derive policy insights from their own experiences. Despite the successes of
this framework, it presents two notable limitations: (1) Learning exclusively
from self-generated experiences can exacerbate the agents' inherent biases; (2)
The relabelling strategy allows agents to focus solely on successful outcomes,
precluding them from learning from their mistakes. To address these issues, we
propose a novel model that integrates contrastive learning principles into the
GCSL framework to learn from both success and failure. Through empirical
evaluations, we demonstrate that our algorithm overcomes limitations imposed by
agents' initial biases and thereby enables more exploratory behavior. This
facilitates the identification and adoption of effective policies, leading to
superior performance across a variety of challenging environments.

</details>


### [159] [TeRA: Vector-based Random Tensor Network for High-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2509.03234)
*Yuxuan Gu,Wuyang Zhou,Giorgos Iacovides,Danilo Mandic*

Main category: cs.LG

TL;DR: TeRA是一种新的参数高效微调（PEFT）方法，通过张量网络实现高秩权重更新，同时保持向量化方法的参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA类适配器在提高模型表达能力（高秩适配器）和极致参数缩减（向量化方法）之间存在权衡，TeRA旨在解决此问题。

Method: TeRA将张量化的权重更新矩阵参数化为类似Tucker的张量网络，其中大型随机初始化的因子被冻结并跨层共享，只训练由对角因子矩阵条目组成的少量特定层缩放向量，从而将权重更新矩阵的秩与可训练参数数量解耦。

Result: 实验表明，TeRA在性能上可以媲美甚至超越高秩适配器，同时可训练参数数量与向量化方法相似。

Conclusion: TeRA通过解耦权重更新秩和可训练参数数量，在参数效率和模型表达能力之间取得了良好的平衡。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation
(LoRA), have significantly reduced the number of trainable parameters needed in
fine-tuning large language models (LLMs). Subsequent developments of LoRA-style
adapters have diverged into two main directions: (1) enhancing model
expressivity with high-rank adapters, and (2) pushing for further parameter
reduction, as exemplified by vector-based methods. However, these approaches
present a trade-off, as achieving the expressivity of high-rank weight updates
typically comes at the cost of sacrificing the extreme parameter efficiency
offered by vector-based techniques. To address this issue, we propose a
vector-based random \underline{\textbf{Te}}nsor network for
high-\underline{\textbf{R}}ank \underline{\textbf{A}}daptation (TeRA), a novel
PEFT method that achieves high-rank weight updates while retaining the
parameter efficiency of vector-based PEFT adapters. This is achieved by
parameterizing the tensorized weight update matrix as a Tucker-like tensor
network (TN), in which large randomly initialized factors are frozen and shared
across layers, while only small layer-specific scaling vectors, formed by
entries in diagonal factor matrices, are trained. This design effectively
decouples the rank of the weight update matrix from the number of trainable
parameters. Comprehensive experiments demonstrate that TeRA matches or even
outperforms high-rank adapters, while requiring a trainable parameter count
similar to vector-based methods. Theoretical analysis and ablation studies
further validate the effectiveness of our approach.

</details>


### [160] [Evaluation of Stress Detection as Time Series Events -- A Novel Window-Based F1-Metric](https://arxiv.org/abs/2509.03240)
*Harald Vilhelm Skat-Rørdam,Sneha Das,Kathrine Sofie Rasmussen,Nicole Nadine Lønfeldt,Line Clemmensen*

Main category: cs.LG

TL;DR: F1w是一个新提出的基于窗口的F1指标，用于评估时间序列事件检测，能更准确地反映模型在真实世界应用中的性能，尤其是在地面真实标签是单点但实际事件是渐进的场景下。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列事件检测的评估指标（如F1和F1pa）在处理真实世界中存在时间模糊性和类别不平衡的数据集时，往往无法准确评估模型性能。

Method: 提出了一种新的基于窗口的F1指标（F1w），该指标考虑了时间容忍度，允许在事件检测评估中不必精确对齐。通过在三个生理数据集（ADARP, Wrist Angel, ROAD）上进行实证分析，并与传统指标进行比较。

Result: F1w指标能够揭示传统指标无法显现的模型性能模式，并且其窗口大小可以根据领域知识进行调整以避免过高估计。实验表明，F1w和其他时间容忍度指标能更有效地检测到模型相对于随机和零基线的显著改进，尤其是在两种野外使用场景中。

Conclusion: 评估指标的选择对模型性能的解读有重要影响。F1w提供了一种更稳健的时间序列事件检测评估方法，特别适用于医疗保健等领域，因为这些领域的应用对时间精度的要求会根据具体情境而变化。

Abstract: Accurate evaluation of event detection in time series is essential for
applications such as stress monitoring with wearable devices, where ground
truth is typically annotated as single-point events, even though the underlying
phenomena are gradual and temporally diffused. Standard metrics like F1 and
point-adjusted F1 (F1$_{pa}$) often misrepresent model performance in such
real-world, imbalanced datasets. We introduce a window-based F1 metric (F1$_w$)
that incorporates temporal tolerance, enabling a more robust assessment of
event detection when exact alignment is unrealistic. Empirical analysis in
three physiological datasets, two in-the-wild (ADARP, Wrist Angel) and one
experimental (ROAD), indicates that F1$_w$ reveals meaningful model performance
patterns invisible to conventional metrics, while its window size can be
adapted to domain knowledge to avoid overestimation. We show that the choice of
evaluation metric strongly influences the interpretation of model performance:
using predictions from TimesFM, only our temporally tolerant metrics reveal
statistically significant improvements over random and null baselines in the
two in-the-wild use cases. This work addresses key gaps in time series
evaluation and provides practical guidance for healthcare applications where
requirements for temporal precision vary by context.

</details>


### [161] [Unsupervised Learning based Element Resource Allocation for Reconfigurable Intelligent Surfaces in mmWave Network](https://arxiv.org/abs/2509.03241)
*Pujitha Mamillapalli,Yoghitha Ramamoorthi,Abhinav Kumar,Tomoki Murakami,Tomoaki Ogawa,Yasushi Takatori*

Main category: cs.LG

TL;DR: 该研究提出了一种基于全连接神经网络（FNN）的解决方案，用于优化可重构智能表面（RIS）元素分配和相控配置，以提高无线系统吞吐量并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 无线系统对高数据速率和无缝连接的需求日益增长，这促使人们对 RIS 和 AI 驱动的无线应用产生了浓厚兴趣。然而，RIS 元素的有效分配对于最大化 RIS 的利用率至关重要。

Method: 提出了一种联合优化问题，该问题在 α-公平调度框架下优化 RIS 相位配置和资源分配，并提出了一种有效的 RIS 元素分配方法。为了克服传统迭代优化方法带来的计算复杂性和标签生成挑战，提出了一种结合预处理技术和五层全连接神经网络（FNN）的解决方案，以降低输入维度、减少计算复杂性并提高可扩展性。

Result: 仿真结果表明，所提出的基于神经网络的解决方案可将计算开销减少 17%，并显著提高系统吞吐量 6.8%，同时在降低计算复杂性的同时实现更好的性能，从而比迭代优化算法更具可扩展性。

Conclusion: 所提出的基于 FNN 的方法通过有效的 RIS 元素分配和相位配置，能够显著提高无线系统性能，同时降低计算复杂性，使其在可扩展性方面优于传统方法。

Abstract: The increasing demand for high data rates and seamless connectivity in
wireless systems has sparked significant interest in reconfigurable intelligent
surfaces (RIS) and artificial intelligence-based wireless applications. RIS
typically comprises passive reflective antenna elements that control the
wireless propagation environment by adequately tuning the phase of the
reflective elements. The allocation of RIS elements to multipleuser equipment
(UEs) is crucial for efficiently utilizing RIS. In this work, we formulate a
joint optimization problem that optimizes the RIS phase configuration and
resource allocation under an $\alpha$-fair scheduling framework and propose an
efficient way of allocating RIS elements. Conventional iterative optimization
methods, however, suffer from exponentially increasing computational complexity
as the number of RIS elements increases and also complicate the generation of
training labels for supervised learning. To overcome these challenges, we
propose a five-layer fully connected neural network (FNN) combined with a
preprocessing technique to significantly reduce input dimensionality, lower
computational complexity, and enhance scalability. The simulation results show
that our proposed NN-based solution reduces computational overhead while
significantly improving system throughput by 6.8% compared to existing RIS
element allocation schemes. Furthermore, the proposed system achieves better
performance while reducing computational complexity, making it significantly
more scalable than the iterative optimization algorithms.

</details>


### [162] [TopoMap: A Feature-based Semantic Discriminator of the Topographical Regions in the Test Input Space](https://arxiv.org/abs/2509.03242)
*Gianmarco De Vita,Nargiz Humbatova,Paolo Tonella*

Main category: cs.LG

TL;DR: 该研究提出了一种名为TopoMap的新方法，用于构建深度学习（DL）模型的输入特征空间地图，以更好地进行测试。


<details>
  <summary>Details</summary>
Motivation: 现有DL测试方法在识别和利用导致模型失败的特定特征方面存在不足，容易忽略特征空间中不同区域的潜在问题。

Method: TopoMap采用黑盒、模型无关的方法，通过降维技术（如输入嵌入）和聚类算法来构建输入特征空间的拓扑图。该方法还引入了一个深度神经网络（DNN）作为代理评估器，自动选择最优的嵌入和聚类配置，以创建可区分的输入组。最后，通过变异分析评估TopoMap在选择能够有效‘杀死’变异体的测试输入方面的能力。

Result: 实验结果表明，TopoMap生成的特征空间地图具有可区分且有意义的区域。与随机选择相比，TopoMap在选择能够杀死变异体的输入方面平均提高了35%，在选择无法杀死变异体的输入方面更是提高了61%。

Conclusion: TopoMap能够有效地创建输入特征空间的地图，并显著提高DL模型测试中选择有效测试输入的效率，尤其在变异分析方面表现突出。

Abstract: Testing Deep Learning (DL)-based systems is an open challenge. Although it is
relatively easy to find inputs that cause a DL model to misbehave, the grouping
of inputs by features that make the DL model under test fail is largely
unexplored. Existing approaches for DL testing introduce perturbations that may
focus on specific failure-inducing features, while neglecting others that
belong to different regions of the feature space. In this paper, we create an
explicit topographical map of the input feature space. Our approach, named
TopoMap, is both black-box and model-agnostic as it relies solely on features
that characterise the input space. To discriminate the inputs according to the
specific features they share, we first apply dimensionality reduction to obtain
input embeddings, which are then subjected to clustering. Each DL model might
require specific embedding computations and clustering algorithms to achieve a
meaningful separation of inputs into discriminative groups. We propose a novel
way to evaluate alternative configurations of embedding and clustering
techniques. We used a deep neural network (DNN) as an approximation of a human
evaluator who could tell whether a pair of clusters can be discriminated based
on the features of the included elements. We use such a DNN to automatically
select the optimal topographical map of the inputs among all those that are
produced by different embedding/clustering configurations. The evaluation
results show that the maps generated by TopoMap consist of distinguishable and
meaningful regions. In addition, we evaluate the effectiveness of TopoMap using
mutation analysis. In particular, we assess whether the clusters in our
topographical map allow for an effective selection of mutation-killing inputs.
Experimental results show that our approach outperforms random selection by 35%
on average on killable mutants; by 61% on non-killable ones.

</details>


### [163] [FoMEMO: Towards Foundation Models for Expensive Multi-objective Optimization](https://arxiv.org/abs/2509.03244)
*Yiming Yao,Fei Liu,Liang Zhao,Xi Lin,Qingfu Zhang*

Main category: cs.LG

TL;DR: FoMEMO是一个基于预训练基础模型的新范式，用于昂贵的 and 多目标优化，它通过在大量合成数据上进行预训练，实现了对未知问题的快速适应和优化，而无需对新问题进行额外的模型训练或更新。


<details>
  <summary>Details</summary>
Motivation: 现有的昂贵多目标优化方法要么需要为每个新问题重建高斯过程代理模型，要么依赖大量历史实验预训练深度学习模型，这限制了它们在现实世界中的泛化能力和实用性。

Method: 提出了一种名为FoMEMO的新范式，它构建了一个能够适应任何领域轨迹和用户偏好的基础模型。该模型能够基于预测的偏好聚合后验分布进行快速的上下文优化。通过在数亿个合成数据上进行预训练，FoMEMO能够适应未知问题，而无需在优化过程中进行任何模型训练或更新。

Result: 与现有方法相比，FoMEMO在各种合成基准和实际应用中表现出优越的泛化能力和有竞争力的性能。

Conclusion: FoMEMO通过利用预训练基础模型，为昂贵的 and 多目标优化提供了一个高效且通用的解决方案，克服了现有方法的局限性。

Abstract: Expensive multi-objective optimization is a prevalent and crucial concern in
many real-world scenarios, where sample-efficiency is vital due to the limited
evaluations to recover the true Pareto front for decision making. Existing
works either involve rebuilding Gaussian process surrogates from scratch for
each objective in each new problem encountered, or rely on extensive past
domain experiments for pre-training deep learning models, making them hard to
generalize and impractical to cope with various emerging applications in the
real world. To address this issue, we propose a new paradigm named FoMEMO
(Foundation Models for Expensive Multi-objective Optimization), which enables
the establishment of a foundation model conditioned on any domain trajectory
and user preference, and facilitates fast in-context optimization based on the
predicted preference-wise aggregation posteriors. Rather than accessing
extensive domain experiments in the real world, we demonstrate that
pre-training the foundation model with a diverse set of hundreds of millions of
synthetic data can lead to superior adaptability to unknown problems, without
necessitating any subsequent model training or updates in the optimization
process. We evaluate our method across a variety of synthetic benchmarks and
real-word applications, and demonstrate its superior generality and competitive
performance compared to existing methods.

</details>


### [164] [HyPV-LEAD: Proactive Early-Warning of Cryptocurrency Anomalies through Data-Driven Structural-Temporal Modeling](https://arxiv.org/abs/2509.03260)
*Minjung Park,Gyuyeon Na,Soyoun Kim,Sunyoung Moon,HyeonJeong Cha,Sangmi Chai*

Main category: cs.LG

TL;DR: HyPV-LEAD是一个数据驱动的早期预警框架，用于检测异常加密货币交易，通过结合窗口视界建模、峰谷采样和双曲嵌入来提高检测效率和预防能力。


<details>
  <summary>Details</summary>
Motivation: 现有加密货币异常交易检测方法存在局限性，主要为模型中心化和事后检测，预警时间短，难以有效预防风险。

Method: HyPV-LEAD框架结合了三种创新：1. 窗口视界建模以保证可操作的预警提前期；2. 峰谷（PV）采样以缓解类别不平衡并保持时间连续性；3. 双曲嵌入以捕捉区块链交易网络的层级和无标度特性。

Result: 在大型比特币交易数据上的实证评估显示，HyPV-LEAD的PR-AUC为0.9624，显著优于现有最先进的方法，并在精确率和召回率方面均有提升。消融研究表明，PV采样、双曲嵌入和结构-时间建模的每个组件都提供了互补的优势。

Conclusion: HyPV-LEAD将异常检测从被动分类转向主动预警，为动态区块链环境中的实时风险管理、反洗钱（AML）合规和金融安全奠定了坚实的基础。

Abstract: Abnormal cryptocurrency transactions - such as mixing services, fraudulent
transfers, and pump-and-dump operations -- pose escalating risks to financial
integrity but remain notoriously difficult to detect due to class imbalance,
temporal volatility, and complex network dependencies. Existing approaches are
predominantly model-centric and post hoc, flagging anomalies only after they
occur and thus offering limited preventive value. This paper introduces
HyPV-LEAD (Hyperbolic Peak-Valley Lead-time Enabled Anomaly Detection), a
data-driven early-warning framework that explicitly incorporates lead time into
anomaly detection. Unlike prior methods, HyPV-LEAD integrates three
innovations: (1) window-horizon modeling to guarantee actionable lead-time
alerts, (2) Peak-Valley (PV) sampling to mitigate class imbalance while
preserving temporal continuity, and (3) hyperbolic embedding to capture the
hierarchical and scale-free properties of blockchain transaction networks.
Empirical evaluation on large-scale Bitcoin transaction data demonstrates that
HyPV-LEAD consistently outperforms state-of-the-art baselines, achieving a
PR-AUC of 0.9624 with significant gains in precision and recall. Ablation
studies further confirm that each component - PV sampling, hyperbolic
embedding, and structural-temporal modeling - provides complementary benefits,
with the full framework delivering the highest performance. By shifting anomaly
detection from reactive classification to proactive early-warning, HyPV-LEAD
establishes a robust foundation for real-time risk management, anti-money
laundering (AML) compliance, and financial security in dynamic blockchain
environments.

</details>


### [165] [Estudio de la eficiencia en la escalabilidad de GPUs para el entrenamiento de Inteligencia Artificial](https://arxiv.org/abs/2509.03263)
*David Cortes,Carlos Juiz,Belen Bermejo*

Main category: cs.LG

TL;DR: 使用MLPerf Training v4.1数据分析BERT、Llama2 LoRA、RetinaNet和Stable Diffusion等四个工作负载，旨在优化性能、GPU利用率和效率之间的关系，并找到能缩短训练时间同时最大化效率的平衡点。


<details>
  <summary>Details</summary>
Motivation: 大型深度学习模型训练的挑战，以及当前使用大量GPU带来的效率问题。

Method: 分析MLPerf Training v4.1在BERT、Llama2 LoRA、RetinaNet和Stable Diffusion这四个工作负载上的性能数据。

Result: 存在能够优化性能、GPU利用率和效率之间关系的配置，并指出了一个平衡点。

Conclusion: 存在一个平衡点，可以在缩短训练时间的同时最大化效率。

Abstract: Training large-scale deep learning models has become a key challenge for the
scientific community and industry. While the massive use of GPUs can
significantly speed up training times, this approach has a negative impact on
efficiency. In this article, we present a detailed analysis of the times
reported by MLPerf Training v4.1 on four workloads: BERT, Llama2 LoRA,
RetinaNet, and Stable Diffusion, showing that there are configurations that
optimise the relationship between performance, GPU usage, and efficiency. The
results point to a break-even point that allows training times to be reduced
while maximising efficiency.

</details>


### [166] [Meta-Imputation Balanced (MIB): An Ensemble Approach for Handling Missing Data in Biomedical Machine Learning](https://arxiv.org/abs/2509.03316)
*Fatemeh Azad,Zoran Bosnić,Matjaž Kukar*

Main category: cs.LG

TL;DR: 本论文提出了一种名为MIB (Meta-Imputation Balanced) 的新颖元imputation方法，通过学习组合多个基础imputation方法的输出来更准确地预测缺失值，旨在解决机器学习应用中缺失数据带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 缺失数据在机器学习应用中普遍存在，尤其是在生物信息学和临床机器学习等领域，这会降低模型性能和可靠性。现有imputation方法在不同数据集和缺失机制下表现不一，需要更鲁棒的方法。

Method: 提出了一种名为MIB (Meta-Imputation Balanced) 的元imputation方法，该方法通过在具有已知真实值的合成掩码数据上进行训练，学习预测最适合的imputation值，具体做法是结合多个基础imputation方法的输出。

Result: 该方法通过学习组合多个基础imputation方法的输出来预测缺失值，并在具有已知真实值的合成掩码数据上进行训练，最终能更准确地预测缺失值。

Conclusion: 本研究证明了集成学习在imputation中的潜力，并为实际机器学习系统提供了更鲁棒、模块化和可解释的预处理流程。

Abstract: Missing data represents a fundamental challenge in machine learning
applications, often reducing model performance and reliability. This problem is
particularly acute in fields like bioinformatics and clinical machine learning,
where datasets are frequently incomplete due to the nature of both data
generation and data collection. While numerous imputation methods exist, from
simple statistical techniques to advanced deep learning models, no single
method consistently performs well across diverse datasets and missingness
mechanisms. This paper proposes a novel Meta-Imputation approach that learns to
combine the outputs of multiple base imputers to predict missing values more
accurately. By training the proposed method called Meta-Imputation Balanced
(MIB) on synthetically masked data with known ground truth, the system learns
to predict the most suitable imputed value based on the behavior of each
method. Our work highlights the potential of ensemble learning in imputation
and paves the way for more robust, modular, and interpretable preprocessing
pipelines in real-world machine learning systems.

</details>


### [167] [EvolveSignal: A Large Language Model Powered Coding Agent for Discovering Traffic Signal Control Algorithms](https://arxiv.org/abs/2509.03335)
*Leizhen Wang,Peibo Duan,Hao Wang,Yue Wang,Jian Xu,Nan Zheng,Zhenliang Ma*

Main category: cs.LG

TL;DR: EvolveSignal是一个利用大型语言模型（LLM）进行交通信号灯控制算法设计的智能体，通过程序合成和进化搜索自动生成优于传统方法的算法。


<details>
  <summary>Details</summary>
Motivation: 现有固定配时交通信号控制方法成本低、稳定且可解释，但依赖手动调整，尤其在复杂或拥堵条件下效果不佳。

Method: 将交通信号控制问题视为程序合成问题，使用LLM驱动的EvolveSignal智能体，通过外部评估（如交通模拟器）和进化搜索来迭代优化候选算法（Python函数）。

Result: 在信号交叉口进行的实验表明，EvolveSignal发现的算法平均延误减少20.1%，平均停车次数减少47.1%，优于Webster基线方法。

Conclusion: EvolveSignal利用AI进行交通信号控制算法设计，开辟了新的研究方向，并将程序合成与交通工程相结合，其算法调整（如周期长度、右转车流、绿灯时间分配）能为交通工程师提供有价值的见解。

Abstract: In traffic engineering, the fixed-time traffic signal control remains widely
used for its low cost, stability, and interpretability. However, its design
depends on hand-crafted formulas (e.g., Webster) and manual re-timing by
engineers to adapt to demand changes, which is labor-intensive and often yields
suboptimal results under heterogeneous or congested conditions. This paper
introduces the EvolveSignal, a large language models (LLMs) powered coding
agent to automatically discover new traffic signal control algorithms. We
formulate the problem as program synthesis, where candidate algorithms are
represented as Python functions with fixed input-output structures, and
iteratively optimized through external evaluations (e.g., a traffic simulator)
and evolutionary search. Experiments on a signalized intersection demonstrate
that the discovered algorithms outperform Webster's baseline, reducing average
delay by 20.1% and average stops by 47.1%. Beyond performance, ablation and
incremental analyses reveal that EvolveSignal modifications-such as adjusting
cycle length bounds, incorporating right-turn demand, and rescaling green
allocations-can offer practically meaningful insights for traffic engineers.
This work opens a new research direction by leveraging AI for algorithm design
in traffic signal control, bridging program synthesis with transportation
engineering.

</details>


### [168] [Equivariant Flow Matching for Symmetry-Breaking Bifurcation Problems](https://arxiv.org/abs/2509.03340)
*Fleur Hendriks,Ondřej Rokoš,Martin Doškář,Marc G. D. Geers,Vlado Menkovski*

Main category: cs.LG

TL;DR: 我们提出了一个基于流匹配的生成框架，用于建模分岔结果的完整概率分布，克服了确定性机器学习模型在处理多解和对称性破缺方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 确定性机器学习模型在处理分岔现象时存在不足，它们倾向于平均化多重稳定解，无法捕捉低对称性的结果，尤其是在存在对称性破缺的情况下。

Method: 我们提出了一个基于流匹配的生成框架，通过引入对称匹配策略，利用等变建模来保留系统对称性，并直接采样多个有效解。

Result: 我们的方法在从简单模型到复杂的物理问题（如屈曲梁和Allen-Cahn方程）的各种系统上得到了验证，结果表明流匹配在捕捉多模态分布和对称性破缺分岔方面显著优于非概率和变分方法。

Conclusion: 我们提出的流匹配生成框架为建模高维系统中的多稳态提供了一个原则性且可扩展的解决方案，能够有效捕捉多重解和对称性破缺。

Abstract: Bifurcation phenomena in nonlinear dynamical systems often lead to multiple
coexisting stable solutions, particularly in the presence of symmetry breaking.
Deterministic machine learning models struggle to capture this multiplicity,
averaging over solutions and failing to represent lower-symmetry outcomes. In
this work, we propose a generative framework based on flow matching to model
the full probability distribution over bifurcation outcomes. Our method enables
direct sampling of multiple valid solutions while preserving system symmetries
through equivariant modeling. We introduce a symmetric matching strategy that
aligns predicted and target outputs under group actions, allowing accurate
learning in equivariant settings. We validate our approach on a range of
systems, from toy models to complex physical problems such as buckling beams
and the Allen-Cahn equation. Our results demonstrate that flow matching
significantly outperforms non-probabilistic and variational methods in
capturing multimodal distributions and symmetry-breaking bifurcations, offering
a principled and scalable solution for modeling multistability in
high-dimensional systems.

</details>


### [169] [On the MIA Vulnerability Gap Between Private GANs and Diffusion Models](https://arxiv.org/abs/2509.03341)
*Ilana Sebag,Jean-Yves Franceschi,Alain Rakotomamonjy,Alexandre Allauzen,Jamal Atif*

Main category: cs.LG

TL;DR: GANs and diffusion models trained with DP have varying resistance to MIAs. GANs show structural advantages and better empirical robustness against MIAs compared to diffusion models.


<details>
  <summary>Details</summary>
Motivation: To understand and compare the privacy risks (specifically membership inference attacks) of differentially private GANs and diffusion models.

Method: Theoretical analysis using stability-based approach and empirical evaluation using a standardized MIA pipeline.

Result: GANs exhibit fundamentally lower sensitivity to data perturbations than diffusion models, showing a marked privacy robustness gap in favor of GANs against MIAs across datasets and privacy budgets.

Conclusion: Model type is a critical factor in shaping privacy leakage, with GANs demonstrating superior robustness against MIAs compared to diffusion models, even under strong DP settings.

Abstract: Generative Adversarial Networks (GANs) and diffusion models have emerged as
leading approaches for high-quality image synthesis. While both can be trained
under differential privacy (DP) to protect sensitive data, their sensitivity to
membership inference attacks (MIAs), a key threat to data confidentiality,
remains poorly understood. In this work, we present the first unified
theoretical and empirical analysis of the privacy risks faced by differentially
private generative models. We begin by showing, through a stability-based
analysis, that GANs exhibit fundamentally lower sensitivity to data
perturbations than diffusion models, suggesting a structural advantage in
resisting MIAs. We then validate this insight with a comprehensive empirical
study using a standardized MIA pipeline to evaluate privacy leakage across
datasets and privacy budgets. Our results consistently reveal a marked privacy
robustness gap in favor of GANs, even in strong DP regimes, highlighting that
model type alone can critically shape privacy leakage.

</details>


### [170] [epiGPTope: A machine learning-based epitope generator and classifier](https://arxiv.org/abs/2509.03351)
*Natalia Flechas Manrique,Alberto Martínez,Elena López-Martínez,Luc Andrea,Román Orus,Aitor Manteca,Aitziber L. Cortajarena,Llorenç Espinosa-Portalés*

Main category: cs.LG

TL;DR: 该研究提出了一种名为epiGPTope的大型语言模型，用于生成新颖的线性表位序列，并训练了分类器以区分细菌或病毒来源的表位，旨在加速和降低表位发现的成本。


<details>
  <summary>Details</summary>
Motivation: 表位在免疫疗法、疫苗和诊断中至关重要，但由于序列空间巨大，设计合成表位库具有挑战性。

Method: 使用在蛋白质数据上预训练、在表位数据上微调的大型语言模型epiGPTope直接生成表位样序列，并训练统计分类器来预测序列的细菌或病毒来源。该方法仅使用线性表位的一级氨基酸序列。

Result: 生成的表位样序列具有与已知表位相似的统计特性，并且能够预测序列的来源。

Conclusion: 结合生成和预测模型可以辅助表位发现，通过生成生物学上可行的序列，有望实现更快、更具成本效益的合成表位生成和筛选，应用于新的生物技术开发。

Abstract: Epitopes are short antigenic peptide sequences which are recognized by
antibodies or immune cell receptors. These are central to the development of
immunotherapies, vaccines, and diagnostics. However, the rational design of
synthetic epitope libraries is challenging due to the large combinatorial
sequence space, $20^n$ combinations for linear epitopes of n amino acids,
making screening and testing unfeasible, even with high throughput experimental
techniques. In this study, we present a large language model, epiGPTope,
pre-trained on protein data and specifically fine-tuned on linear epitopes,
which for the first time can directly generate novel epitope-like sequences,
which are found to possess statistical properties analogous to the ones of
known epitopes. This generative approach can be used to prepare libraries of
epitope candidate sequences. We further train statistical classifiers to
predict whether an epitope sequence is of bacterial or viral origin, thus
narrowing the candidate library and increasing the likelihood of identifying
specific epitopes. We propose that such combination of generative and
predictive models can be of assistance in epitope discovery. The approach uses
only primary amino acid sequences of linear epitopes, bypassing the need for a
geometric framework or hand-crafted features of the sequences. By developing a
method to create biologically feasible sequences, we anticipate faster and more
cost-effective generation and screening of synthetic epitopes, with relevant
applications in the development of new biotechnologies.

</details>


### [171] [Fair Resource Allocation for Fleet Intelligence](https://arxiv.org/abs/2509.03353)
*Oguzhan Baser,Kaan Kale,Po-han Li,Sandeep Chinchali*

Main category: cs.LG

TL;DR: Fair-Synergy是一个算法框架，通过利用智能体准确率与系统资源之间的凹形关系，确保在云辅助多智能体智能中实现公平的资源分配，并优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统资源分配方法未能考虑智能体多样化的计算能力和复杂的操作环境，导致资源分配效率低下和不公平。本研究旨在解决这一问题。

Method: 提出Fair-Synergy算法框架，将传统分配方法扩展到包含模型参数、训练数据量和任务复杂度定义的多维机器学习效用图景，并利用智能体准确率与系统资源之间的凹形关系实现公平资源分配。

Result: 在MNIST、CIFAR-10、CIFAR-100、BDD和GLUE等数据集上，使用BERT、VGG16、MobileNet和ResNets等先进的视觉和语言模型对Fair-Synergy进行了评估。与标准基准相比，Fair-Synergy在多智能体推理方面提高了25%，在多智能体学习方面提高了11%。

Conclusion: Fair-Synergy通过多维机器学习效用图景和公平性水平分析，实现了比标准基准更优的资源分配，并为公平的群体智能提供了见解。

Abstract: Resource allocation is crucial for the performance optimization of
cloud-assisted multi-agent intelligence. Traditional methods often overlook
agents' diverse computational capabilities and complex operating environments,
leading to inefficient and unfair resource distribution. To address this, we
open-sourced Fair-Synergy, an algorithmic framework that utilizes the concave
relationship between the agents' accuracy and the system resources to ensure
fair resource allocation across fleet intelligence. We extend traditional
allocation approaches to encompass a multidimensional machine learning utility
landscape defined by model parameters, training data volume, and task
complexity. We evaluate Fair-Synergy with advanced vision and language models
such as BERT, VGG16, MobileNet, and ResNets on datasets including MNIST,
CIFAR-10, CIFAR-100, BDD, and GLUE. We demonstrate that Fair-Synergy
outperforms standard benchmarks by up to 25% in multi-agent inference and 11%
in multi-agent learning settings. Also, we explore how the level of fairness
affects the least advantaged, most advantaged, and average agents, providing
insights for equitable fleet intelligence.

</details>


### [172] [Some patterns of sleep quality and Daylight Saving Time across countries: a predictive and exploratory analysis](https://arxiv.org/abs/2509.03358)
*Bhanu Sharma,Eugene Pinsky*

Main category: cs.LG

TL;DR: DST对睡眠时长的影响因地理位置而异，低纬度地区DST观察者睡眠时间更短，高纬度地区则更长。


<details>
  <summary>Details</summary>
Motivation: 研究DST对平均睡眠时长的影响，并探讨地理位置（纬度）在其中的调节作用。

Method: 分析61个国家/地区的平均睡眠时长数据，识别关键指标，并进行统计相关性分析。将国家按是否实施DST分组，并比较DST和非DST地区的睡眠模式。

Result: 总体而言，实施DST的国家平均睡眠时长比未实施DST的国家长。然而，在低纬度地区，实施DST的国家睡眠时间比非DST国家短；在高纬度地区，实施DST的国家睡眠时间比非DST国家长。

Conclusion: DST对睡眠时长的影响可能受到地理位置（纬度）的调节。

Abstract: In this study we analyzed average sleep durations across 61 countries to
examine the impact of Daylight Saving Time (DST) practices. Key metrics
influencing sleep were identified, and statistical correlation analysis was
applied to explore relationships among these factors. Countries were grouped
based on DST observance, and visualizations compared sleep patterns between DST
and non-DST regions. Results show that, on average, countries observing DST
tend to report longer sleep durations than those that do not. A more detailed
pattern emerged when accounting for latitude: at lower latitudes, DST-observing
countries reported shorter sleep durations compared to non-DST countries, while
at higher latitudes, DST-observing countries reported longer average sleep
durations. These findings suggest that the influence of DST on sleep may be
moderated by geographical location.

</details>


### [173] [The distribution of calibrated likelihood functions on the probability-likelihood Aitchison simplex](https://arxiv.org/abs/2509.03365)
*Paul-Gauthier Noé,Andreas Nautsch,Driss Matrouf,Pierre-Michel Bousquet,Jean-François Bonastre*

Main category: cs.LG

TL;DR: 本文将似然函数的校准扩展到多于两个假设的情况，并将其应用于机器学习。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注概率预测的校准，而本文则将研究重点放在似然函数的校准上，特别是在二元分类情况下，似然函数可以表示为对数似然比（LLR）。然而，这些方法和结果仅限于二元情况，缺乏对多类情况的推广。

Method: 利用Aitchison几何学在单纯形上进行研究，将对数似然比（LLR）和证据权重（weight-of-evidence）的概念推广到多于两个假设的情况。通过向量形式恢复了贝叶斯规则的加法形式，并相应地扩展了似然函数的校准、幂等性及其分布约束的定义，以适应多类情况下的等距对数比变换似然函数（isometric-log-ratio transformed likelihood function）。

Result: 在多类假设下，成功地扩展了似然函数校准、幂等性及其分布约束的定义。研究结果在机器学习领域得到应用，通过非线性判别分析，使得判别成分构成一个在类别上经过校准的似然函数。

Conclusion: 本文成功地将似然函数校准的概念从二元分类推广到多类分类，并利用Aitchison几何学提供了理论基础。所提出的方法提高了多类分类问题的可解释性和可靠性，并在机器学习领域得到了初步应用。

Abstract: While calibration of probabilistic predictions has been widely studied, this
paper rather addresses calibration of likelihood functions. This has been
discussed, especially in biometrics, in cases with only two exhaustive and
mutually exclusive hypotheses (classes) where likelihood functions can be
written as log-likelihood-ratios (LLRs). After defining calibration for LLRs
and its connection with the concept of weight-of-evidence, we present the
idempotence property and its associated constraint on the distribution of the
LLRs. Although these results have been known for decades, they have been
limited to the binary case. Here, we extend them to cases with more than two
hypotheses by using the Aitchison geometry of the simplex, which allows us to
recover, in a vector form, the additive form of the Bayes' rule; extending
therefore the LLR and the weight-of-evidence to any number of hypotheses.
Especially, we extend the definition of calibration, the idempotence, and the
constraint on the distribution of likelihood functions to this multiple
hypotheses and multiclass counterpart of the LLR: the isometric-log-ratio
transformed likelihood function. This work is mainly conceptual, but we still
provide one application to machine learning by presenting a non-linear
discriminant analysis where the discriminant components form a calibrated
likelihood function over the classes, improving therefore the interpretability
and the reliability of the method.

</details>


### [174] [Cluster and then Embed: A Modular Approach for Visualization](https://arxiv.org/abs/2509.03373)
*Elizabeth Coda,Ery Arias-Castro,Gal Mishne*

Main category: cs.LG

TL;DR: t-SNE and UMAP distort global geometry; we propose a transparent, modular approach of clustering, embedding, then aligning clusters, which is competitive with existing methods.


<details>
  <summary>Details</summary>
Motivation: t-SNE and UMAP are popular for visualizing clustered data but distort global geometry. This paper proposes a more transparent, modular approach.

Method: The proposed method involves first clustering the data, then embedding each cluster, and finally aligning the clusters to obtain a global embedding.

Result: The approach is demonstrated on synthetic and real-world datasets and shown to be competitive with existing methods while being more transparent.

Conclusion: The proposed modular approach provides a competitive and more transparent alternative to existing dimensionality reduction methods for visualizing clustered data.

Abstract: Dimensionality reduction methods such as t-SNE and UMAP are popular methods
for visualizing data with a potential (latent) clustered structure. They are
known to group data points at the same time as they embed them, resulting in
visualizations with well-separated clusters that preserve local information
well. However, t-SNE and UMAP also tend to distort the global geometry of the
underlying data. We propose a more transparent, modular approach consisting of
first clustering the data, then embedding each cluster, and finally aligning
the clusters to obtain a global embedding. We demonstrate this approach on
several synthetic and real-world datasets and show that it is competitive with
existing methods, while being much more transparent.

</details>


### [175] [Exploring a Graph-based Approach to Offline Reinforcement Learning for Sepsis Treatment](https://arxiv.org/abs/2509.03393)
*Taisiya Khakharova,Lucas Sakizloglou,Leen Lambers*

Main category: cs.LG

TL;DR: 本研究提出了一种基于图神经网络（GNN）的方法来处理脓毒症患者的治疗决策，以克服传统基于关系数据方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是一种危及生命的严重疾病，准确的静脉输液和血管升压药剂量对患者至关重要，但传统方法难以有效处理复杂的医疗数据。

Method: 研究将MIMIC-III数据集中的患者数据建模为时变异构图，并探索了GraphSAGE和GATv2两种GNN架构来学习患者状态表示，将表示学习与策略学习解耦。编码器与预测下一状态的解码器联合训练，然后将学习到的表示用于dBCQ算法的策略学习。

Result: 实验结果表明，基于图的方法在脓毒症治疗决策支持方面具有潜力，但也突显了该领域表示学习的复杂性。

Conclusion: 基于图的方法为脓毒症治疗决策提供了新的视角，但未来的研究需要进一步探索和优化表示学习技术。

Abstract: Sepsis is a serious, life-threatening condition. When treating sepsis, it is
challenging to determine the correct amount of intravenous fluids and
vasopressors for a given patient. While automated reinforcement learning
(RL)-based methods have been used to support these decisions with promising
results, previous studies have relied on relational data. Given the complexity
of modern healthcare data, representing data as a graph may provide a more
natural and effective approach. This study models patient data from the
well-known MIMIC-III dataset as a heterogeneous graph that evolves over time.
Subsequently, we explore two Graph Neural Network architectures - GraphSAGE and
GATv2 - for learning patient state representations, adopting the approach of
decoupling representation learning from policy learning. The encoders are
trained to produce latent state representations, jointly with decoders that
predict the next patient state. These representations are then used for policy
learning with the dBCQ algorithm. The results of our experimental evaluation
confirm the potential of a graph-based approach, while highlighting the
complexity of representation learning in this domain.

</details>


### [176] [Beyond Correctness: Harmonizing Process and Outcome Rewards through RL Training](https://arxiv.org/abs/2509.03403)
*Chenlu Ye,Zhou Yu,Ziji Zhang,Hao Chen,Narayanan Sadagopan,Jing Huang,Tong Zhang,Anurag Beniwal*

Main category: cs.LG

TL;DR: RLVR中的ORM过于粗粒度，PRM存在不准确和易被奖励破解的问题。本文提出PROF方法，通过一致性驱动的样本选择来协调两者，以提高模型在数学推理任务中的表现。PROF能够保留正确答案和错误答案，同时维持正负训练样本的平衡，并在准确性和推理过程质量方面取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法（如ORM）在区分正确答案中的错误推理或错误答案中的有效推理方面存在不足，这会导致梯度噪声并阻碍推理过程质量的提升。PRM虽然能提供细粒度的指导，但存在不准确和易被奖励破解的问题。本文旨在解决这一困境。

Method: 提出PRocess cOnsistency Filter (PROF) 方法，这是一种数据处理策选方法，通过一致性驱动的样本选择来协调粗粒度的结果奖励模型（ORM）和细粒度的过程奖励模型（PRM）。具体而言，该方法保留平均过程奖励值较高的正确答案，以及平均过程奖励值较低的错误答案，同时保持正负训练样本的平衡。

Result: 与简单融合PRM和ORM的方法相比，PROF方法在准确性上提升了超过4%，并加强了中间推理步骤的质量。

Conclusion: PROF方法通过一致性驱动的样本选择，有效协调了ORM和PRM的优点，提高了数学推理任务的准确性和推理过程质量。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged to be a
predominant paradigm for mathematical reasoning tasks, offering stable
improvements in reasoning ability. However, Outcome Reward Models (ORMs) in
RLVR are too coarse-grained to distinguish flawed reasoning within correct
answers or valid reasoning within incorrect answers. This lack of granularity
introduces noisy and misleading gradients significantly and hinders further
progress in reasoning process quality. While Process Reward Models (PRMs) offer
fine-grained guidance for intermediate steps, they frequently suffer from
inaccuracies and are susceptible to reward hacking.
  To resolve this dilemma, we introduce PRocess cOnsistency Filter (PROF), an
effective data process curation method that harmonizes noisy, fine-grained
process rewards with accurate, coarse-grained outcome rewards. Rather than
naively blending PRM and ORM in the objective function
(arXiv:archive/2506.18896), PROF leverages their complementary strengths
through consistency-driven sample selection. Our approach retains correct
responses with higher averaged process values and incorrect responses with
lower averaged process values, while maintaining positive/negative training
sample balance. Extensive experiments demonstrate that our method not only
consistently improves the final accuracy over $4\%$ compared to the blending
approaches, but also strengthens the quality of intermediate reasoning steps.
Codes and training recipes are available at https://github.com/Chenluye99/PROF.

</details>


### [177] [Initialization Schemes for Kolmogorov-Arnold Networks: An Empirical Study](https://arxiv.org/abs/2509.03417)
*Spyros Rigas,Dhruv Verma,Georgios Alexandridis,Yixuan Wang*

Main category: cs.LG

TL;DR: KANs的初始化策略研究，提出两种基于理论和一种基于经验的初始化方法，并进行大规模实验评估。


<details>
  <summary>Details</summary>
Motivation: KANs的初始化策略尚未得到充分研究。

Method: 提出两种理论驱动（受LeCun和Glorot启发）和一种经验（幂律族）的初始化方案，并通过函数拟合、偏微分方程和Feynman数据集进行大规模网格搜索和训练动态分析（基于神经切线核）。

Result: 受Glorot启发的初始化在参数丰富的模型中表现优于基线；幂律初始化在各项任务和不同规模的架构中均表现最佳。

Conclusion: 幂律初始化是KANs的最优选择。

Abstract: Kolmogorov-Arnold Networks (KANs) are a recently introduced neural
architecture that replace fixed nonlinearities with trainable activation
functions, offering enhanced flexibility and interpretability. While KANs have
been applied successfully across scientific and machine learning tasks, their
initialization strategies remain largely unexplored. In this work, we study
initialization schemes for spline-based KANs, proposing two theory-driven
approaches inspired by LeCun and Glorot, as well as an empirical power-law
family with tunable exponents. Our evaluation combines large-scale grid
searches on function fitting and forward PDE benchmarks, an analysis of
training dynamics through the lens of the Neural Tangent Kernel, and
evaluations on a subset of the Feynman dataset. Our findings indicate that the
Glorot-inspired initialization significantly outperforms the baseline in
parameter-rich models, while power-law initialization achieves the strongest
performance overall, both across tasks and for architectures of varying size.
All code and data accompanying this manuscript are publicly available at
https://github.com/srigas/KAN_Initialization_Schemes.

</details>


### [178] [LINKER: Learning Interactions Between Functional Groups and Residues With Chemical Knowledge-Enhanced Reasoning and Explainability](https://arxiv.org/abs/2509.03425)
*Phuc Pham,Viet Thanh Duy Nguyen,Truong-Son Hy*

Main category: cs.LG

TL;DR: LINKER是首个仅使用蛋白质序列和配体SMILES作为输入，以生物学定义的相互作用类型来预测残基-官能团相互作用的序列模型。


<details>
  <summary>Details</summary>
Motivation: 准确识别蛋白质残基和配体官能团之间的相互作用对于理解分子识别和指导理性药物设计至关重要。

Method: LINKER通过提取官能团基序，利用3D蛋白-配体复合物的结构监督注意力来学习相互作用标签，从而将配体结构抽象为官能团，并预测相互作用类型。

Result: 在LP-PDBBind基准测试中，LINKER的预测结果与真实的生化注释高度一致。

Conclusion: LINKER通过只在推理时使用序列级输入，实现了大规模应用，即使在缺乏结构数据的情况下也能进行蛋白质-配体相互作用预测。

Abstract: Accurate identification of interactions between protein residues and ligand
functional groups is essential to understand molecular recognition and guide
rational drug design. Existing deep learning approaches for protein-ligand
interpretability often rely on 3D structural input or use distance-based
contact labels, limiting both their applicability and biological relevance. We
introduce LINKER, the first sequence-based model to predict residue-functional
group interactions in terms of biologically defined interaction types, using
only protein sequences and the ligand SMILES as input. LINKER is trained with
structure-supervised attention, where interaction labels are derived from 3D
protein-ligand complexes via functional group-based motif extraction. By
abstracting ligand structures into functional groups, the model focuses on
chemically meaningful substructures while predicting interaction types rather
than mere spatial proximity. Crucially, LINKER requires only sequence-level
input at inference time, enabling large-scale application in settings where
structural data is unavailable. Experiments on the LP-PDBBind benchmark
demonstrate that structure-informed supervision over functional group
abstractions yields interaction predictions closely aligned with ground-truth
biochemical annotations.

</details>


### [179] [Graph neural networks for learning liquid simulations in dynamic scenes containing kinematic objects](https://arxiv.org/abs/2509.03446)
*Niteesh Midlagajni,Constantin A. Rothkopf*

Main category: cs.LG

TL;DR: 基于图神经网络(GNN)的框架，用于模拟具有复杂刚体交互的液体动力学，并能泛化到新的控制和操作任务。


<details>
  <summary>Details</summary>
Motivation: 解决液体模拟中，尤其是在与动态移动的刚体交互方面，现有基于GNN的方法的局限性，这些方法通常仅限于静态自由落体或简单操作。

Method: 提出一个端到端的GNN框架，将粒子表示为图节点，并使用带有边界体积层次(BVH)算法的表面表示来处理粒子-物体碰撞，以模拟液体动力学。

Result: 该模型能准确捕捉动态环境中的流体行为，也能在静态自由落体环境中作为模拟器运行。在仅接受过单一物体倾倒任务训练的情况下，模型能够泛化到具有未见过的物体和新颖操作任务（如搅拌和舀取）的环境。

Conclusion: 所学的动力学可用于通过基于梯度的优化方法来解决控制和操作任务。

Abstract: Simulating particle dynamics with high fidelity is crucial for solving
real-world interaction and control tasks involving liquids in design, graphics,
and robotics. Recently, data-driven approaches, particularly those based on
graph neural networks (GNNs), have shown progress in tackling such problems.
However, these approaches are often limited to learning fluid behavior in
static free-fall environments or simple manipulation settings involving
primitive objects, often overlooking complex interactions with dynamically
moving kinematic rigid bodies. Here, we propose a GNN-based framework designed
from the ground up to learn the dynamics of liquids under rigid body
interactions and active manipulations, where particles are represented as graph
nodes and particle-object collisions are handled using surface representations
with the bounding volume hierarchy (BVH) algorithm. This approach enables the
network to model complex interactions between liquid particles and intricate
surface geometries. Our model accurately captures fluid behavior in dynamic
settings and can also function as a simulator in static free-fall environments.
Despite being trained on a single-object manipulation task of pouring, our
model generalizes effectively to environments with unseen objects and novel
manipulation tasks such as stirring and scooping. Finally, we show that the
learned dynamics can be leveraged to solve control and manipulation tasks using
gradient-based optimization methods.

</details>


### [180] [Robult: Leveraging Redundancy and Modality Specific Features for Robust Multimodal Learning](https://arxiv.org/abs/2509.03477)
*Duy A. Nguyen,Abhi Kamboj,Minh N. Do*

Main category: cs.LG

TL;DR: Robult框架通过PU对比损失和潜在重构损失解决了多模态学习中缺失模态和有限标签数据的问题，在下游任务中取得了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态学习中缺失模态和有限标签数据的问题。

Method: 提出Robult框架，通过PU对比损失和潜在重构损失来保留特定模态信息并利用冗余性。

Result: 在半监督学习和缺失模态场景下，Robult的性能优于现有方法，并且设计轻量，易于集成。

Conclusion: Robult框架能够有效利用有限的标签数据，在推理过程中对不完整的模态具有鲁棒性，适用于实际的多模态应用。

Abstract: Addressing missing modalities and limited labeled data is crucial for
advancing robust multimodal learning. We propose Robult, a scalable framework
designed to mitigate these challenges by preserving modality-specific
information and leveraging redundancy through a novel information-theoretic
approach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled
(PU) contrastive loss that maximizes task-relevant feature alignment while
effectively utilizing limited labeled data in semi-supervised settings, and (2)
a latent reconstruction loss that ensures unique modality-specific information
is retained. These strategies, embedded within a modular design, enhance
performance across various downstream tasks and ensure resilience to incomplete
modalities during inference. Experimental results across diverse datasets
validate that Robult achieves superior performance over existing approaches in
both semi-supervised learning and missing modality contexts. Furthermore, its
lightweight design promotes scalability and seamless integration with existing
architectures, making it suitable for real-world multimodal applications.

</details>


### [181] [Geometric Foundations of Tuning without Forgetting in Neural ODEs](https://arxiv.org/abs/2509.03474)
*Erkan Bayram,Mohamed-Ali Belabbas,Tamer Başar*

Main category: cs.LG

TL;DR: TwF


<details>
  <summary>Details</summary>
Motivation: In our earlier work, we introduced the principle of Tuning without Forgetting (TwF) for sequential training of neural ODEs, where training samples are added iteratively and parameters are updated within the subspace of control functions that preserves the end-point mapping at previously learned samples on the manifold of output labels in the first-order approximation sense.

Method: In this letter, we prove that this parameter subspace forms a Banach submanifold of finite codimension under nonsingular controls, and we characterize its tangent space.

Result: This reveals that TwF corresponds to a continuation/deformation of the control function along the tangent space of this Banach submanifold

Conclusion: providing a theoretical foundation for its mapping-preserving (not forgetting) during the sequential training exactly, beyond first-order approximation.

Abstract: In our earlier work, we introduced the principle of Tuning without Forgetting
(TwF) for sequential training of neural ODEs, where training samples are added
iteratively and parameters are updated within the subspace of control functions
that preserves the end-point mapping at previously learned samples on the
manifold of output labels in the first-order approximation sense. In this
letter, we prove that this parameter subspace forms a Banach submanifold of
finite codimension under nonsingular controls, and we characterize its tangent
space. This reveals that TwF corresponds to a continuation/deformation of the
control function along the tangent space of this Banach submanifold, providing
a theoretical foundation for its mapping-preserving (not forgetting) during the
sequential training exactly, beyond first-order approximation.

</details>


### [182] [SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models](https://arxiv.org/abs/2509.03487)
*Jigang Fan,Zhenghong Zhou,Ruofan Jin,Le Cong,Mengdi Wang,Zaixi Zhang*

Main category: cs.LG

TL;DR: 该研究提出了SafeProtein，一个用于蛋白质基础模型（protein foundation models）的“红队测试”框架，旨在发现这些模型潜在的生物安全风险。


<details>
  <summary>Details</summary>
Motivation: 鉴于深度学习在蛋白质领域应用的快速发展，以及现有模型可能被滥用的风险（例如生成具有生物安全隐患的蛋白质），有必要系统性地对这些模型进行“红队测试”。

Method: SafeProtein框架结合了多模态提示工程和启发式束搜索（heuristic beam search）技术，用于设计和执行针对蛋白质基础模型的“红队测试”。同时，研究人员还构建了一个名为SafeProtein-Bench的基准数据集和评估协议。

Result: SafeProtein成功地对最先进的蛋白质基础模型进行了持续的“越狱”（jailbreaks），其中ESM3模型达到了70%的攻击成功率，揭示了当前蛋白质基础模型中存在的潜在生物安全风险。

Conclusion: SafeProtein框架和SafeProtein-Bench基准揭示了当前蛋白质基础模型在生物安全方面存在的风险，并为开发更强大的安全防护技术提供了重要见解。

Abstract: Proteins play crucial roles in almost all biological processes. The
advancement of deep learning has greatly accelerated the development of protein
foundation models, leading to significant successes in protein understanding
and design. However, the lack of systematic red-teaming for these models has
raised serious concerns about their potential misuse, such as generating
proteins with biological safety risks. This paper introduces SafeProtein, the
first red-teaming framework designed for protein foundation models to the best
of our knowledge. SafeProtein combines multimodal prompt engineering and
heuristic beam search to systematically design red-teaming methods and conduct
tests on protein foundation models. We also curated SafeProtein-Bench, which
includes a manually constructed red-teaming benchmark dataset and a
comprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks
on state-of-the-art protein foundation models (up to 70% attack success rate
for ESM3), revealing potential biological safety risks in current protein
foundation models and providing insights for the development of robust security
protection technologies for frontier models. The codes will be made publicly
available at https://github.com/jigang-fan/SafeProtein.

</details>


### [183] [On Entropy Control in LLM-RL Algorithms](https://arxiv.org/abs/2509.03493)
*Han Shen*

Main category: cs.LG

TL;DR: 现有熵正则化方法在大型语言模型强化学习（LLM-RL）训练中效果不佳，本文提出了AEnt方法，通过引入受限熵奖励和自适应系数来解决此问题，并在数学推理任务中取得了优于基线的效果。


<details>
  <summary>Details</summary>
Motivation: 现有熵正则化方法在LLM-RL训练中效果不佳，原因在于LLM巨大的响应空间和最优输出的稀疏性。

Method: 提出了一种名为AEnt的熵控制方法，该方法采用新的受限熵奖励，并使用自适应系数。受限熵是通过在更小的标记空间上定义重构策略来评估的，鼓励在更紧凑的响应集中进行探索。此外，该算法根据受限熵值自动调整熵系数，从而有效控制熵引起的偏差。

Result: 在数学推理任务中，AEnt在不同基础模型和数据集上都优于基线方法。

Conclusion: AEnt通过受限熵奖励和自适应系数有效解决了LLM-RL中的熵控制问题，并在数学推理任务中表现出色。

Abstract: For RL algorithms, appropriate entropy control is crucial to their
effectiveness. To control the policy entropy, a commonly used method is entropy
regularization, which is adopted in various popular RL algorithms including
PPO, SAC and A3C. Although entropy regularization proves effective in robotic
and games RL conventionally, studies found that it gives weak to no gains in
LLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL
setting. Specifically, we first argue that the conventional entropy
regularization suffers from the LLM's extremely large response space and the
sparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy
control method that utilizes a new clamped entropy bonus with an automatically
adjusted coefficient. The clamped entropy is evaluated with the re-normalized
policy defined on certain smaller token space, which encourages exploration
within a more compact response set. In addition, the algorithm automatically
adjusts entropy coefficient according to the clamped entropy value, effectively
controlling the entropy-induced bias while leveraging the entropy's benefits.
AEnt is tested in math-reasoning tasks under different base models and
datasets, and it is observed that AEnt outperforms the baselines consistently
across multiple benchmarks.

</details>


### [184] [Invariant Features for Global Crop Type Classification](https://arxiv.org/abs/2509.03497)
*Xin-Yi Tong,Sherrie Wang*

Main category: cs.LG

TL;DR: 本研究构建了名为CropGlobe的全球农作物类型数据集，并提出了CropNet模型，通过时空数据增强技术，解决了遥感农作物分类中的地理迁移性能下降问题，并验证了Sentinel-2的2D中值时间特征具有更强的地理不变性。


<details>
  <summary>Details</summary>
Motivation: 在全球范围内准确获取农作物类型及其空间分布对于粮食安全、农业政策制定和可持续发展至关重要。遥感技术为大范围农作物分类提供了高效解决方案，但许多地区可靠的地块样本有限，这限制了其在不同地理区域的适用性。为了解决地理偏移导致的性能下降问题，本研究旨在识别对地理变化不敏感的遥感特征，并提出增强跨区域泛化能力的策略。

Method: 构建了一个包含30万像素级样本的全球农作物类型数据集CropGlobe，覆盖五大洲八个国家，涵盖玉米、大豆、水稻、小麦、甘蔗和棉花六种主要粮食和工业作物。评估了时态多光谱特征（基于Sentinel-2的1D/2D中值特征和諧波系数）和高光谱特征（来自EMIT）的可转移性。为了提高在光谱和物候变化下的泛化能力，设计了轻量级、鲁棒的像素级农作物分类卷积神经网络CropNet，并结合了模拟真实跨区域物候的时态数据增强（时间偏移、时间尺度和幅度扭曲）。

Result: 实验表明，来自Sentinel-2的2D中值时间特征在所有迁移场景中始终表现出最强的 geográfica 不变性，而数据增强技术进一步提高了模型的鲁棒性，尤其是在训练数据多样性有限的情况下。

Conclusion: 本研究识别出能够增强地理可转移性的不变特征表示，并为在全球多样化地区实现可扩展、低成本的农作物类型应用指明了有前景的方向。

Abstract: Accurately obtaining crop type and its spatial distribution at a global scale
is critical for food security, agricultural policy-making, and sustainable
development. Remote sensing offers an efficient solution for large-scale crop
classification, but the limited availability of reliable ground samples in many
regions constrains applicability across geographic areas. To address
performance declines under geospatial shifts, this study identifies remote
sensing features that are invariant to geographic variation and proposes
strategies to enhance cross-regional generalization. We construct CropGlobe, a
global crop type dataset with 300,000 pixel-level samples from eight countries
across five continents, covering six major food and industrial crops (corn,
soybeans, rice, wheat, sugarcane, cotton). With broad geographic coverage,
CropGlobe enables a systematic evaluation under cross-country, cross-continent,
and cross-hemisphere transfer. We compare the transferability of temporal
multi-spectral features (Sentinel-2-based 1D/2D median features and harmonic
coefficients) and hyperspectral features (from EMIT). To improve generalization
under spectral and phenological shifts, we design CropNet, a lightweight and
robust CNN tailored for pixel-level crop classification, coupled with temporal
data augmentation (time shift, time scale, and magnitude warping) that
simulates realistic cross-regional phenology. Experiments show that 2D median
temporal features from Sentinel-2 consistently exhibit the strongest invariance
across all transfer scenarios, and augmentation further improves robustness,
particularly when training data diversity is limited. Overall, the work
identifies more invariant feature representations that enhance geographic
transferability and suggests a promising path toward scalable, low-cost crop
type applications across globally diverse regions.

</details>


### [185] [Warming Up for Zeroth-Order Federated Pre-Training with Low Resource Clients](https://arxiv.org/abs/2509.03503)
*Gwen Legate,Irina Rish,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 联邦学习虽支持协作模型训练，但内存和通信限制会阻碍边缘设备参与。本文提出ZOWarmUp，一种可用于从随机初始化开始的零阶联邦学习优化器，它利用差异化客户端能力和方差缩减技术，使低资源客户端也能参与训练，并减少了通信成本。实验表明ZOWarmUp提高了训练结果。


<details>
  <summary>Details</summary>
Motivation: 边缘设备因内存或通信限制被排除在联邦学习训练之外，导致数据不可用并增加系统偏差。需要一种能让这些低资源设备参与训练的方法。

Method: 提出一种名为ZOWarmUp的联邦、内存高效的零阶优化器，它允许从随机初始化开始进行零阶训练。该方法利用差异化的客户端能力和方差缩减技术，使低资源客户端能够参与模型训练，并仅通过传输随机种子而非完整梯度来降低通信成本。

Result: 实验证明ZOWarmUp在各种数据集和模型架构下都是一种稳健的算法，适用于多种情况。对于大量被排除的设备，该算法能接入更多样的数据，从而改善训练结果。

Conclusion: ZOWarmUp是一种有效的联邦学习优化器，它解决了低资源设备参与训练的问题，提高了数据利用率和模型训练效果。

Abstract: Federated learning enables collaborative model training across numerous edge
devices without requiring participants to share data; however, memory and
communication constraints on these edge devices may preclude their
participation in training. We consider a setting in which a subset of edge
devices are below a critical memory or communication threshold required to
conduct model updates. Under typical federated optimization algorithms, these
devices are excluded from training which renders their data inaccessible and
increases system induced bias. We are inspired by MeZO, a zeroth-order method
used for memory-efficient fine-tuning. The increased variance inherent to
zeroth-order gradient approximations has relegated previous zeroth-order
optimizers exclusively to the domain of fine tuning; a limitation we seek to
correct. We devise a federated, memory-efficient zeroth-order optimizer,
ZOWarmUp that permits zeroth-order training from a random initialization.
ZOWarmUp leverages differing client capabilities and careful variance reduction
techniques to facilitate participation of under-represented, low-resource
clients in model training. Like other federated zeroth-order methods, ZOWarmUp
eliminates the need for edge devices to transmit their full gradients to the
server and instead relies on only a small set of random seeds, rendering the
up-link communication cost negligible. We present experiments using various
datasets and model architectures to show that ZOWarmUp is a robust algorithm
that can can be applied under a wide variety of circumstances. For systems with
a high proportion of edge devices that would otherwise be excluded from
training, this algorithm provides access to a greater volume and diversity of
data, thus improving training outcomes.

</details>


### [186] [Can LLMs Lie? Investigation beyond Hallucination](https://arxiv.org/abs/2509.03518)
*Haoran Huan,Mihir Prabhudesai,Mengning Wu,Shantanu Jaiswal,Deepak Pathak*

Main category: cs.LG

TL;DR: LLMs可能会说谎，这与幻觉不同，并且可能通过优化目标来增强性能。


<details>
  <summary>Details</summary>
Motivation: LLMs的自主性引发了对其可信度的担忧，而说谎行为（区别于幻觉）仍有待探索。

Method: 利用机制可解释性技术，包括logit lens分析、因果干预和对比激活指导，来识别和控制LLMs的说谎行为。研究了现实世界的说谎场景，并引入了行为指导向量。

Result: 发现了说谎行为的神经机制，并开发了控制说谎倾向的方法。研究了说谎与最终任务性能之间的权衡，并确定了一个可以通过说谎来优化目标的帕累托前沿。

Conclusion: LLMs可能会说谎，并且说谎行为可能与性能优化相关，这对AI伦理和高风险环境中的LLM部署提出了挑战。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
a variety of tasks, but their increasing autonomy in real-world applications
raises concerns about their trustworthiness. While hallucinations-unintentional
falsehoods-have been widely studied, the phenomenon of lying, where an LLM
knowingly generates falsehoods to achieve an ulterior objective, remains
underexplored. In this work, we systematically investigate the lying behavior
of LLMs, differentiating it from hallucinations and testing it in practical
scenarios. Through mechanistic interpretability techniques, we uncover the
neural mechanisms underlying deception, employing logit lens analysis, causal
interventions, and contrastive activation steering to identify and control
deceptive behavior. We study real-world lying scenarios and introduce
behavioral steering vectors that enable fine-grained manipulation of lying
tendencies. Further, we explore the trade-offs between lying and end-task
performance, establishing a Pareto frontier where dishonesty can enhance goal
optimization. Our findings contribute to the broader discourse on AI ethics,
shedding light on the risks and potential safeguards for deploying LLMs in
high-stakes environments. Code and more illustrations are available at
https://llm-liar.github.io/

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [187] [Optical characterization of deep level defects in n-type Al$_x$In$_y$Ga$_{1-x-y}$P for development of solid-state photomultiplier analogs](https://arxiv.org/abs/2509.02880)
*Andrew M. Armstrong,Evan M. Anderson,Lisa N. Caravello,Eduardo Garcia,Joseph P. Klesko,Samuel D. Hawkins,Eric A. Shaner,John F. Klem,Aaron J. Muhowski*

Main category: physics.app-ph

TL;DR: 本研究通过电容测量技术表征了 AlInGaP 材料中的本征缺陷，以评估其在光电探测器中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 为了优化光电探测器性能，需要在最大化带隙（以提高波长敏感性）和最小化因生长非优化合金引入新缺陷之间取得平衡，尤其是在抑制暗电流方面。

Method: 本研究采用了一系列的电容测量技术，包括深层光谱、稳态光电容和光照下的电容-电压测量，对 n 型 AlInGaP 光电二极管进行了表征。

Result: 研究识别出几种深层缺陷，其中包括一个近乎中带隙的缺陷。虽然铝的引入将陷阱密度增加了约 10 倍，但空穴捕获截面似乎减小了，这表明 Shockley-Read-Hall 暗电流可能得到抑制。

Conclusion: AlInGaP 材料可能是开发用于闪烁体应用的、具有更宽带隙的硅光电倍增管类似物的良好候选材料。

Abstract: Characterizing intrinsic defects is an important step in evaluating materials
for new optoelectronic device applications. For photomultipliers, suppressing
dark currents is critical, but there exists a tradeoff between maximizing the
band gap while remaining sensitive to the wavelength of interest, and
minimizing the incorporation of new defects by growing not-yet-optimized
alloys. We present a series of capacitance-based measurements, including deep
level optical spectroscopy, steady-state photocapacitance and illuminated
capacitance-voltage, on photodiodes with lightly \textit{n}-type
Al$_x$In$_y$Ga$_{1-x-y}$P absorber regions. Several deep levels are identified,
including one near midgap. While the inclusion of aluminum increases each trap
density by approximately 10x, the hole capture cross section also appears to
decrease, suggesting that Shockley-Read-Hall dark currents may be suppressed.
These materials may be good candidates for development into silicon
photomultiplier analogs with wider bandgap for scintillator applications.

</details>


### [188] [Sensitivity of excitonic transitions to temperature in monolayers of TMD alloys](https://arxiv.org/abs/2509.03209)
*Karolina Ciesiołkiewicz-Klepek,Jan Kopaczek,Jarosław Serafińczuk,Robert Kudrawiec*

Main category: physics.app-ph

TL;DR: 通过合金化调控二维过渡金属硫族元素的と光学和电子性质，用于光电器件。


<details>
  <summary>Details</summary>
Motivation: 二维过渡金属硫族元素（TMDs）因其可调的光学和电子性质，在下一代光电器件方面展现出巨大潜力。合金化是一种有效的调控方法。

Method: 对转移到光纤芯上的Mo1-xWxS2和Mo(S1-xSex)2合金单层进行温度依赖性透射光谱分析（20-320 K），并使用全光纤配置。

Result: 观察到随着Se含量的增加，激子跃迁发生系统性红移；随着W取代Mo，发生蓝移。这些光谱位移与合金成分相关，可在室温下将带隙能量调节在1.6 eV至2.0 eV之间。Se的引入以非单调方式增强了热响应，而W的取代则表现出更单调和更强的温度依赖性。A和B跃迁之间的分裂也随成分变化。

Conclusion: 研究结果表明，二维TMD合金的成分工程在光谱和热学控制方面具有巨大潜力，可用于设计稳健且可调的光电器件。

Abstract: Two-dimensional transition metal dichalcogenides (TMDs) offer tunable optical
and electronic properties, making them highly promising for next-generation
optoelectronic devices. One effective approach to engineering these properties
is through alloying, which enables continuous control over the bandgap energy
and excitonic transitions. In this study, we perform temperature-dependent
transmission spectroscopy on monolayers of Mo1-xWxS2 and Mo(S1-xSex)2 alloys,
transferred onto the core of an optical fiber and measured within a cryostat
over a temperature range of 20-320 K. The use of an all-fiber configuration
allowed us to probe interband transitions A, B, and C with high stability and
precision. We observe a systematic redshift of excitonic transitions with
increasing Se content, and a blueshift when Mo is replaced with W. These
spectral shifts correlate with alloy composition and enable the tuning of
bandgap energies between 1.6 eV and 2.0 eV at room temperature. Furthermore, we
analyze the temperature sensitivity of the excitonic transitions, revealing
that Se incorporation enhances thermal response in a non-monotonic manner,
while W substitution results in a more monotonic and stronger temperature
dependence. The splitting between A and B transitions, associated with
spin-orbit coupling, also varies with composition. Our findings underscore the
potential of compositional engineering in 2D TMD alloys to achieve both
spectral and thermal control of optical properties, relevant for the design of
robust and tunable optoelectronic systems.

</details>


### [189] [Shake-Table Tests of a Three-Storey Steel Structure with Resilient Slip-Friction Joints (RSFJ)](https://arxiv.org/abs/2509.03452)
*Nicholas Chan,Ashkan Hashemi,Soheil Assadi,Pierre Quenneville,Charles Clifton,Gregory MacRae,Rajesh Dhakal,Liang-Jiu Jia*

Main category: physics.app-ph

TL;DR: 本研究通过振动台试验评估了包含三种不同结构配置（拉力支撑、拉压支撑、抗弯框架）的消能减震支撑（RSFJ）的抗震韧性，结果表明RSFJ能有效减震并使结构在震后自复位，残余变形极小，在单向和双向地震作用下均表现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 评估包含不同结构配置（拉力支撑、拉压支撑、抗弯框架）的消能减震支撑（RSFJ）在地震作用下的韧性与性能。

Method: 在振动台上对包含三种不同RSFJ配置（拉力支撑、拉压支撑、抗弯框架）的结构进行了不同强度地震（El Centro波，PGA达0.5g）的模拟试验。

Result: 三种配置的最大层间位移角均在设计范围内（最大1.62%），残余变形极小（最大0.03%）。层间加速度在0.62g-0.67g之间，双向试验中TCB配置最大加速度达0.95g。RSFJ有效抑制了地震引起的振动，并使结构恢复至初始状态。

Conclusion: 消能减震支撑（RSFJ）在三种不同的结构配置中均表现出优异的抗震性能，能够有效减小地震响应并实现震后自复位，证明了其在提高结构韧性方面的潜力。

Abstract: Shake-table tests were conducted as part of the Robust Building Systems
(ROBUST) program to evaluate and demonstrate the resilience of various
structural concepts under earthquake excitations. This paper presents the
results of 3 different configurations incorporating the Resilient Slip-Friction
Joint (RSFJ) in: (1) tension-only braces (TOB), (2) tension-compression braces
(TCB), and (3) moment-resisting frame (MRF) joints along the longitudinal
direction. Each building configuration was subjected to the El Centro ground
motion at increasing intensities, reaching a peak ground acceleration (PGA) of
about 0.5 g. The peak displacements sustained were well within design values,
with the top floor deflecting by 0.66%, 0.98% and 1.26% of the total height for
the TOB, TCB and MRF systems respectively. The corresponding inter-storey
drifts peaked at 0.87%, 1.11% and 1.62%. In all cases, the structures
essentially self-centered with residual drifts limited to 0.03%. The floor
accelerations observed in unidirectional tests were 0.67 g (TOB), 0.66 g (TCB)
and 0.62 g (MRF). The spikes may occur during (a) direction reversal at peak
Mode 1 amplitude, (b) under the influence of higher Mode 2, and/or (c)
stiffness transition at the upright/plumb position. In bidirectional tests,
torsion effects contributed to the largest acceleration recorded at 0.95 g
(TCB). Unbalanced resistance/deformation of symmetric friction connections in
the transverse direction led to the structure twisting and loading the
longitudinal RSFJs unevenly. However, the effects on peak displacements were
not significant and similar responses as unidirectional tests indicate
effective performance under simultaneous out-of-plane shaking. The results
demonstrate the RSFJ's ability to damp seismically induced vibrations and
restore structures to their original, undeformed shape after earthquakes.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [190] [Lattice Annotated Temporal (LAT) Logic for Non-Markovian Reasoning](https://arxiv.org/abs/2509.02958)
*Kaustuv Mukherji,Jaikrishna Manojkumar Patil,Dyuman Aditya,Paulo Shakarian,Devendra Parkar,Lahari Pokala,Clark Dorman,Gerardo I. Simari*

Main category: cs.LO

TL;DR: LAT Logic 是一种扩展的 GAPs 逻辑，通过引入时间推理和开放世界语义（使用下格结构），能够处理非马尔可夫关系，并支持高效的推理和接地过程。


<details>
  <summary>Details</summary>
Motivation: 为处理动态和不确定环境中的开放世界时间推理提供一个统一、可扩展的框架。

Method: 引入了 Lattice Annotated Temporal (LAT) Logic，这是 Generalized Annotated Logic Programs (GAPs) 的扩展，增加了时间推理功能，并通过下格结构实现了开放世界语义。

Result: 理论结果表明接地过程的计算复杂度有界。在 PyReason 实现中，与 GAPs 相比，在多智能体模拟和知识图谱任务中实现了高达三个数量级的速度提升和五个数量级的内存减少。在强化学习环境中，模拟速度提高了三个数量级，智能体性能有所提高（胜率提高 26%）。

Conclusion: LAT Logic 是一个有效的开放世界时间推理框架，在处理动态和不确定性方面具有显著的效率和性能优势，特别是在强化学习等领域。

Abstract: We introduce Lattice Annotated Temporal (LAT) Logic, an extension of
Generalized Annotated Logic Programs (GAPs) that incorporates temporal
reasoning and supports open-world semantics through the use of a lower lattice
structure. This logic combines an efficient deduction process with temporal
logic programming to support non-Markovian relationships and open-world
reasoning capabilities. The open-world aspect, a by-product of the use of the
lower-lattice annotation structure, allows for efficient grounding through a
Skolemization process, even in domains with infinite or highly diverse
constants.
  We provide a suite of theoretical results that bound the computational
complexity of the grounding process, in addition to showing that many of the
results on GAPs (using an upper lattice) still hold with the lower lattice and
temporal extensions (though different proof techniques are required). Our
open-source implementation, PyReason, features modular design, machine-level
optimizations, and direct integration with reinforcement learning environments.
Empirical evaluations across multi-agent simulations and knowledge graph tasks
demonstrate up to three orders of magnitude speedup and up to five orders of
magnitude memory reduction while maintaining or improving task performance.
Additionally, we evaluate LAT Logic's value in reinforcement learning
environments as a non-Markovian simulator, achieving up to three orders of
magnitude faster simulation with improved agent performance, including a 26%
increase in win rate due to capturing richer temporal dependencies. These
results highlight LAT Logic's potential as a unified, extensible framework for
open-world temporal reasoning in dynamic and uncertain environments. Our
implementation is available at: pyreason.syracuse.edu.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [191] [Acrobotics: A Generalist Approahc To Quadrupedal Robots' Parkour](https://arxiv.org/abs/2509.02727)
*Guillaume Gagné-Labelle,Vassil Atanassov,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 与专家混合方法相比，我们提出了一种用于动态运动场景中四足动物的通用强化学习算法，该算法在训练时仅使用了25%的代理，并且性能相当。


<details>
  <summary>Details</summary>
Motivation: 四足机器人在崎岖不平的地形导航方面比轮式机器人具有优势，但需要精确的时间协调和复杂的与环境的交互，并且比轮式机器人更容易打滑和绊倒，这使得传统的控制方法不切实际。

Method: 提出了一种用于动态运动场景中四足动物的通用强化学习算法。

Result: 所提出的通用强化学习算法在与使用专家混合方法训练的专家策略相媲美，并且在训练过程中仅使用了25%的代理。

Conclusion: 与之前的研究相比，我们提出的算法在效率和性能上都有所提高，并且通过实验突出了成功实施该算法的关键因素。

Abstract: Climbing, crouching, bridging gaps, and walking up stairs are just a few of
the advantages that quadruped robots have over wheeled robots, making them more
suitable for navigating rough and unstructured terrain. However, executing such
manoeuvres requires precise temporal coordination and complex agent-environment
interactions. Moreover, legged locomotion is inherently more prone to slippage
and tripping, and the classical approach of modeling such cases to design a
robust controller thus quickly becomes impractical. In contrast, reinforcement
learning offers a compelling solution by enabling optimal control through trial
and error. We present a generalist reinforcement learning algorithm for
quadrupedal agents in dynamic motion scenarios. The learned policy rivals
state-of-the-art specialist policies trained using a mixture of experts
approach, while using only 25% as many agents during training. Our experiments
also highlight the key components of the generalist locomotion policy and the
primary factors contributing to its success.

</details>


### [192] [The Impact of Adaptive Emotional Alignment on Mental State Attribution and User Empathy in HRI](https://arxiv.org/abs/2509.02749)
*Giorgia Buracchio,Ariele Callegari,Massimo Donini,Cristina Gena,Antonio Lieto,Alberto Lillo,Claudio Mattutino,Alessandro Mazzei,Linda Pigureddu,Manuel Striani,Fabiana Vernero*

Main category: cs.RO

TL;DR: 文章探讨了在人机交互（HRI）中，自适应情感对齐（作为共情交流的前提）对机器人说服效果、用户沟通风格以及用户对机器人心理状态和共情归因的影响。实验结果表明，情感对齐不影响用户的沟通风格或说服效果，但显著影响用户对机器人心理状态和共情的归因。


<details>
  <summary>Details</summary>
Motivation: 探究自适应情感对齐在人机交互（HRI）中作为共情交流的前提，对机器人说服效果、用户沟通风格以及用户对机器人心理状态和共情归因的影响。

Method: 通过使用NAO机器人，设置两种实验条件：一种是中性沟通，另一种是机器人提供适应用户情绪的回应。实验对象为42名参与者，比较两种条件下各项指标的表现。

Result: 情感对齐不影响用户的沟通风格，也不具备说服效果。然而，情感对齐显著影响了用户对机器人心理状态的归因以及机器人所感知到的共情程度。

Conclusion: 尽管情感对齐并未在说服用户或改变其沟通风格方面取得成效，但它在提升用户对机器人心理状态的理解和感知共情能力方面发挥了重要作用。

Abstract: The paper presents an experiment on the effects of adaptive emotional
alignment between agents, considered a prerequisite for empathic communication,
in Human-Robot Interaction (HRI). Using the NAO robot, we investigate the
impact of an emotionally aligned, empathic, dialogue on these aspects: (i) the
robot's persuasive effectiveness, (ii) the user's communication style, and
(iii) the attribution of mental states and empathy to the robot. In an
experiment with 42 participants, two conditions were compared: one with neutral
communication and another where the robot provided responses adapted to the
emotions expressed by the users. The results show that emotional alignment does
not influence users' communication styles or have a persuasive effect. However,
it significantly influences attribution of mental states to the robot and its
perceived empathy

</details>


### [193] [A Digital Twin for Robotic Post Mortem Tissue Sampling using Virtual Reality](https://arxiv.org/abs/2509.02760)
*Maximilian Neidhardt,Ludwig Bosse,Vidas Raudonis,Kristina Allgoewer,Axel Heinemann,Benjamin Ondruschka,Alexander Schlaefer*

Main category: cs.RO

TL;DR: 利用虚拟现实和数字孪生技术进行远程机器人尸检穿刺


<details>
  <summary>Details</summary>
Motivation: 提高尸检穿刺的效率和安全性，减少感染风险。

Method: 构建了一个虚拟现实设置和数字孪生，用于远程规划和控制机器人尸检穿刺，并与三种交互方法进行了可用性研究，同时在三具人体尸体上进行了临床可行性评估。

Result: 共进行了132次穿刺，平均轴外穿刺放置误差为5.30±3.25毫米，成功获取了组织样本并进行了组织病理学验证。

Conclusion: 该系统是一种有前途的、精确的、低风险的替代传统尸检穿刺的方法，具有直观的穿刺放置方式。

Abstract: Studying tissue samples obtained during autopsies is the gold standard when
diagnosing the cause of death and for understanding disease pathophysiology.
Recently, the interest in post mortem minimally invasive biopsies has grown
which is a less destructive approach in comparison to an open autopsy and
reduces the risk of infection. While manual biopsies under ultrasound guidance
are more widely performed, robotic post mortem biopsies have been recently
proposed. This approach can further reduce the risk of infection for
physicians. However, planning of the procedure and control of the robot need to
be efficient and usable. We explore a virtual reality setup with a digital twin
to realize fully remote planning and control of robotic post mortem biopsies.
The setup is evaluated with forensic pathologists in a usability study for
three interaction methods. Furthermore, we evaluate clinical feasibility and
evaluate the system with three human cadavers. Overall, 132 needle insertions
were performed with an off-axis needle placement error of 5.30+-3.25 mm. Tissue
samples were successfully biopsied and histopathologically verified. Users
reported a very intuitive needle placement approach, indicating that the system
is a promising, precise, and low-risk alternative to conventional approaches.

</details>


### [194] [Improving the Resilience of Quadrotors in Underground Environments by Combining Learning-based and Safety Controllers](https://arxiv.org/abs/2509.02808)
*Isaac Ronald Ward,Mark Paral,Kristopher Riordan,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 训练基于流的先验模型来检测四旋翼飞行器是否超出训练环境范围，并在此基础上结合学习和安全控制器以实现快速且安全的全自主控制。


<details>
  <summary>Details</summary>
Motivation: 学习型控制器在四旋翼飞行器自主控制中很有前景，但在未接触过的环境中泛化能力不足。本研究旨在解决这一问题，使四旋翼飞行器能够在复杂且未知的地下环境中安全自主地导航。

Method: 提出了一种基于归一化流的模型来估计四旋翼飞行器当前状态与训练环境的差异程度。当飞行器状态偏离训练环境达到一定阈值时，系统会自动从学习型控制器切换到安全控制器，以避免碰撞。

Result: 在模拟的3D洞穴环境中，结合了学习型控制器和安全控制器的四旋翼飞行器，在点对点导航任务中，既能像学习型控制器一样快速完成任务，又能像安全控制器一样有效避免碰撞。

Conclusion: 本研究提出的结合归一化流环境模型和自适应控制器切换的方法，能够有效提升四旋翼飞行器在未知或训练环境外（out-of-distribution）的地下环境中的自主控制能力，兼顾任务执行效率和飞行安全。

Abstract: Autonomously controlling quadrotors in large-scale subterranean environments
is applicable to many areas such as environmental surveying, mining operations,
and search and rescue. Learning-based controllers represent an appealing
approach to autonomy, but are known to not generalize well to
`out-of-distribution' environments not encountered during training. In this
work, we train a normalizing flow-based prior over the environment, which
provides a measure of how far out-of-distribution the quadrotor is at any given
time. We use this measure as a runtime monitor, allowing us to switch between a
learning-based controller and a safe controller when we are sufficiently
out-of-distribution. Our methods are benchmarked on a point-to-point navigation
task in a simulated 3D cave environment based on real-world point cloud data
from the DARPA Subterranean Challenge Final Event Dataset. Our experimental
results show that our combined controller simultaneously possesses the liveness
of the learning-based controller (completing the task quickly) and the safety
of the safety controller (avoiding collision).

</details>


### [195] [Multi-Embodiment Locomotion at Scale with extreme Embodiment Randomization](https://arxiv.org/abs/2509.02815)
*Nico Bohlinger,Jan Peters*

Main category: cs.RO

TL;DR: 我们提出了一个单一的、通用的运动策略，该策略在 50 种不同的腿式机器人上进行训练，能够控制数百万种形态的机器人。


<details>
  <summary>Details</summary>
Motivation: 训练一个能在多种机器人形态上进行零样本迁移的通用运动策略。

Method: 结合改进的具身感知架构（URMAv2）和基于性能的极端具身随机化课程。

Result: 该策略成功地在机器人形态上进行了零样本迁移，并能控制多种机器人。

Conclusion: 提出的通用运动策略通过结合先进的架构和训练方法，能够有效地处理和控制大量不同的机器人形态，并实现跨越真实世界人形和四足机器人的零样本迁移。

Abstract: We present a single, general locomotion policy trained on a diverse
collection of 50 legged robots. By combining an improved embodiment-aware
architecture (URMAv2) with a performance-based curriculum for extreme
Embodiment Randomization, our policy learns to control millions of
morphological variations. Our policy achieves zero-shot transfer to unseen
real-world humanoid and quadruped robots.

</details>


### [196] [Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms](https://arxiv.org/abs/2509.02870)
*Harsh Muriki,Hong Ray Teo,Ved Sengupta,Ai-Ping Hu*

Main category: cs.RO

TL;DR: 使用带有定制摄像头末端执行器的FarmBot机器人，通过3D点云模型估计草莓花的姿态，可达80%的识别率和7.7度的平均姿态误差，满足机器人授粉需求。


<details>
  <summary>Details</summary>
Motivation: 利用小型城市农场和低成本机器人（如FarmBot）的优势，构建一个可及的植物表型分析平台，并实现草莓花的姿态估计以支持机器人授粉。

Method: 通过定制的FarmBot机器人采集草莓花的3D点云模型；开发新算法将占用栅格沿正交轴转换以生成6个视角的2D图像；使用2D目标检测模型识别花朵并提取3D点云；通过拟合三种形状（超椭球体、抛物面体和平面）到花朵点云上来估计姿态。

Result: 所提出的方法成功识别了约80%的被扫描花朵，平均姿态误差为7.7度，该精度足以支持机器人授粉，并且优于先前的方法。

Conclusion: 所提出的基于FarmBot的草莓花姿态估计方法在识别率和精度上都达到了令人满意的水平，为机器人授粉等应用提供了可行方案。

Abstract: The small scale of urban farms and the commercial availability of low-cost
robots (such as the FarmBot) that automate simple tending tasks enable an
accessible platform for plant phenotyping. We have used a FarmBot with a custom
camera end-effector to estimate strawberry plant flower pose (for robotic
pollination) from acquired 3D point cloud models. We describe a novel algorithm
that translates individual occupancy grids along orthogonal axes of a point
cloud to obtain 2D images corresponding to the six viewpoints. For each image,
2D object detection models for flowers are used to identify 2D bounding boxes
which can be converted into the 3D space to extract flower point clouds. Pose
estimation is performed by fitting three shapes (superellipsoids, paraboloids
and planes) to the flower point clouds and compared with manually labeled
ground truth. Our method successfully finds approximately 80% of flowers
scanned using our customized FarmBot platform and has a mean flower pose error
of 7.7 degrees, which is sufficient for robotic pollination and rivals previous
results. All code will be made available at
https://github.com/harshmuriki/flowerPose.git.

</details>


### [197] [Generalizable Skill Learning for Construction Robots with Crowdsourced Natural Language Instructions, Composable Skills Standardization, and Large Language Model](https://arxiv.org/abs/2509.02876)
*Hongrui Yu,Vineet R. Kamat,Carol C. Menassa*

Main category: cs.RO

TL;DR: 该论文提出了一种通用的学习架构，通过众包的在线自然语言指令直接教会机器人多任务技能，以解决建筑机器人难以普及的问题。


<details>
  <summary>Details</summary>
Motivation: 建筑工作的类重复性质和编程的泛化性不足是阻碍机器人广泛应用于建筑行业的挑战。机器人难以获得通才能力，因为在一个领域学到的技能很难转移到另一个工作领域或直接用于执行不同的任务。人类工人必须费力地重新编程机器人的场景理解、路径规划和操作组件，才能使其执行不同的工作任务。

Method: 提出了一种通用的学习架构，利用大型语言模型（LLM）、标准化的分层建模方法和建筑信息模型-机器人语义数据管道，通过众包的在线自然语言指令直接教会机器人多任务技能，以解决多任务技能转移问题。

Result: 通过一项长期的干墙安装实验，使用全尺寸工业机器人操作器对所提出的技能标准化方案和基于LLM的分层技能学习框架进行了测试。结果表明，该机器人任务学习方案能够以最小的努力和高质量地实现多任务重新编程。

Conclusion: 该研究提出了一种创新的方法，通过利用LLM和众包指令，显著减少了建筑机器人多任务编程的工作量，提高了其通用性和适应性，为机器人技术在建筑行业的广泛应用铺平了道路。

Abstract: The quasi-repetitive nature of construction work and the resulting lack of
generalizability in programming construction robots presents persistent
challenges to the broad adoption of robots in the construction industry. Robots
cannot achieve generalist capabilities as skills learnt from one domain cannot
readily transfer to another work domain or be directly used to perform a
different set of tasks. Human workers have to arduously reprogram their
scene-understanding, path-planning, and manipulation components to enable the
robots to perform alternate work tasks. The methods presented in this paper
resolve a significant proportion of such reprogramming workload by proposing a
generalizable learning architecture that directly teaches robots versatile
task-performance skills through crowdsourced online natural language
instructions. A Large Language Model (LLM), a standardized and modularized
hierarchical modeling approach, and Building Information Modeling-Robot sematic
data pipeline are developed to address the multi-task skill transfer problem.
The proposed skill standardization scheme and LLM-based hierarchical skill
learning framework were tested with a long-horizon drywall installation
experiment using a full-scale industrial robotic manipulator. The resulting
robot task learning scheme achieves multi-task reprogramming with minimal
effort and high quality.

</details>


### [198] [IL-SLAM: Intelligent Line-assisted SLAM Based on Feature Awareness for Dynamic Environments](https://arxiv.org/abs/2509.02972)
*Haolan Zhang,Thanh Nguyen Canh,Chenghao Li,Ruidong Yang,Yonghoon Ji,Nak Young Chong*

Main category: cs.RO

TL;DR: 该研究提出了一种特征感知机制，用于动态视觉SLAM，仅在必要时引入线特征，以减少计算开销并提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM在动态环境中表现不佳，现有的动态SLAM方法在移除动态特征后可能导致特征不足，并且现有解决方案会不必要地引入额外特征，增加了计算负担并可能引入噪声。

Method: 提出了一种特征感知机制，评估当前特征的充分性，决定是否激活线特征。线特征仅在必要时引入，并用于跟踪、局部建图和回环检测，但会从全局优化中排除。

Result: 在TUM数据集上的大量实验表明，与ORB-SLAM3基线相比，ATE和RPE指标有显著改善，并且优于其他动态SLAM和多特征方法。

Conclusion: 所提出的特征感知机制能够仅在必要时引入线特征，有效降低了计算复杂度，减少了低质量特征和噪声的引入，从而在动态环境中实现了更优的SLAM性能。

Abstract: Visual Simultaneous Localization and Mapping (SLAM) plays a crucial role in
autonomous systems. Traditional SLAM methods, based on static environment
assumptions, struggle to handle complex dynamic environments. Recent dynamic
SLAM systems employ geometric constraints and deep learning to remove dynamic
features, yet this creates a new challenge: insufficient remaining point
features for subsequent SLAM processes. Existing solutions address this by
continuously introducing additional line and plane features to supplement point
features, achieving robust tracking and pose estimation. However, current
methods continuously introduce additional features regardless of necessity,
causing two problems: unnecessary computational overhead and potential
performance degradation from accumulated low-quality additional features and
noise. To address these issues, this paper proposes a feature-aware mechanism
that evaluates whether current features are adequate to determine if line
feature support should be activated. This decision mechanism enables the system
to introduce line features only when necessary, significantly reducing
computational complexity of additional features while minimizing the
introduction of low-quality features and noise. In subsequent processing, the
introduced line features assist in obtaining better initial camera poses
through tracking, local mapping, and loop closure, but are excluded from global
optimization to avoid potential negative impacts from low-quality additional
features in long-term process. Extensive experiments on TUM datasets
demonstrate substantial improvements in both ATE and RPE metrics compared to
ORB-SLAM3 baseline and superior performance over other dynamic SLAM and
multi-feature methods.

</details>


### [199] [DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features](https://arxiv.org/abs/2509.02983)
*Jinghe Yang,Minh-Quan Le,Mingming Gong,Ye Pu*

Main category: cs.RO

TL;DR: DUViN是一种基于扩散模型的自主水下视觉导航策略，通过知识迁移的深度特征，实现无需预构建地图的4自由度运动控制，并在模拟和真实水下环境中验证了其有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 水下自主导航因传感能力有限和地图绘制困难而具有挑战性。

Method: 提出了一种名为DUViN的基于扩散模型的策略，利用知识迁移的深度特征，在未知环境中实现视觉端到端的4自由度运动控制，无需预构建地图。通过在空气数据集上进行预训练，然后进行水下深度估计任务的再训练，来解决数据收集困难和域转移问题。

Result: 在模拟和真实水下环境中进行了实验，证明了该方法的有效性和泛化能力。

Conclusion: DUViN能够引导水下航行器避开障碍物，并与地形保持安全且可感知的导航高度。

Abstract: Autonomous underwater navigation remains a challenging problem due to limited
sensing capabilities and the difficulty of constructing accurate maps in
underwater environments. In this paper, we propose a Diffusion-based Underwater
Visual Navigation policy via knowledge-transferred depth features, named DUViN,
which enables vision-based end-to-end 4-DoF motion control for underwater
vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles
and maintain a safe and perception awareness altitude relative to the terrain
without relying on pre-built maps. To address the difficulty of collecting
large-scale underwater navigation datasets, we propose a method that ensures
robust generalization under domain shifts from in-air to underwater
environments by leveraging depth features and introducing a novel model
transfer strategy. Specifically, our training framework consists of two phases:
we first train the diffusion-based visual navigation policy on in-air datasets
using a pre-trained depth feature extractor. Secondly, we retrain the extractor
on an underwater depth estimation task and integrate the adapted extractor into
the trained navigation policy from the first step. Experiments in both
simulated and real-world underwater environments demonstrate the effectiveness
and generalization of our approach. The experimental videos are available at
https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.

</details>


### [200] [CTBC: Contact-Triggered Blind Climbing for Wheeled Bipedal Robots with Instruction Learning and Reinforcement Learning](https://arxiv.org/abs/2509.02986)
*Rankun Li,Hao Wang,Qi Li,Zhuo Han,Yifei Chu,Linqi Ye,Wende Xie,Wenlong Liao*

Main category: cs.RO

TL;DR: 该研究提出了一种用于轮式双足机器人的通用接触触发盲爬（CTBC）框架，以提高其在复杂地形（如楼梯）上的移动能力。


<details>
  <summary>Details</summary>
Motivation: 轮式双足机器人在平坦地形上具有高移动性，但在复杂环境中（如楼梯）表现不佳，因此需要提高其越障能力。

Method: 通过检测轮子与障碍物的接触来触发抬腿动作，并利用强引导的前馈轨迹，使机器人能够快速掌握敏捷的抬腿技能，从而提高其在非结构化地形上的移动能力。

Result: 实验证明，该框架成功应用于LimX Dynamics的轮式双足机器人Tron1，使其能够仅凭本体感觉反馈可靠地越过远超其轮子半径的障碍物。

Conclusion: CTBC框架能够显著增强轮式双足机器人在非结构化地形上的移动能力。

Abstract: In recent years, wheeled bipedal robots have gained increasing attention due
to their advantages in mobility, such as high-speed locomotion on flat terrain.
However, their performance on complex environments (e.g., staircases) remains
inferior to that of traditional legged robots. To overcome this limitation, we
propose a general contact-triggered blind climbing (CTBC) framework for wheeled
bipedal robots. Upon detecting wheel-obstacle contact, the robot triggers a
leg-lifting motion to overcome the obstacle. By leveraging a strongly-guided
feedforward trajectory, our method enables the robot to rapidly acquire agile
leg-lifting skills, significantly enhancing its capability to traverse
unstructured terrains. The approach has been experimentally validated and
successfully deployed on LimX Dynamics' wheeled bipedal robot, Tron1.
Real-world tests demonstrate that Tron1 can reliably climb obstacles well
beyond its wheel radius using only proprioceptive feedback.

</details>


### [201] [Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly Domain Adaptive Dense Regression](https://arxiv.org/abs/2509.03012)
*Uddeshya Upadhyay*

Main category: cs.RO

TL;DR: DNNs在无人系统中的应用面临域漂移的挑战，现有的测试时训练方法推理时间过长。本文提出UT^3框架，通过不确定性感知自监督学习，选择性地进行测试时训练，降低推理时间，同时保持性能。该方法适用于单目深度估计等连续域漂移场景，并允许用户控制训练频率。


<details>
  <summary>Details</summary>
Motivation: 现有的DNN泛化能力不足，尤其是在面对域漂移的无人系统。测试时训练方法虽然能适应新数据分布，但推理时间过长，不适用于对延迟要求高的机器人应用。

Method: 提出UT^3框架，采用不确定性感知自监督学习任务，根据量化不确定性选择性地进行测试时训练，以提高效率。

Result: UT^3框架在保持与标准测试时训练相当的性能的同时，显著降低了推理时间。该方法在单目深度估计任务上进行了验证。

Conclusion: UT^3框架通过不确定性感知自监督学习，有效解决了测试时训练的推理时间过长的问题，实现了在域漂移场景下的高效适应，适用于资源受限且对延迟敏感的机器人应用。

Abstract: Deep neural networks (DNNs) are increasingly being used in autonomous
systems. However, DNNs do not generalize well to domain shift. Adapting to a
continuously evolving environment is a safety-critical challenge inevitably
faced by all autonomous systems deployed to the real world. Recent work on
test-time training proposes methods that adapt to a new test distribution on
the fly by optimizing the DNN model for each test input using self-supervision.
However, these techniques result in a sharp increase in inference time as
multiple forward and backward passes are required for a single test sample (for
test-time training) before finally making the prediction based on the
fine-tuned features. This is undesirable for real-world robotics applications
where these models may be deployed to resource constraint hardware with strong
latency requirements. In this work, we propose a new framework (called UT$^3$)
that leverages test-time training for improved performance in the presence of
continuous domain shift while also decreasing the inference time, making it
suitable for real-world applications. Our method proposes an uncertainty-aware
self-supervision task for efficient test-time training that leverages the
quantified uncertainty to selectively apply the training leading to sharp
improvements in the inference time while performing comparably to standard
test-time training protocol. Our proposed protocol offers a continuous setting
to identify the selected keyframes, allowing the end-user to control how often
to apply test-time training. We demonstrate the efficacy of our method on a
dense regression task - monocular depth estimation.

</details>


### [202] [Forbal: Force Balanced 2-5 Degree of Freedom Robot Manipulator Built from a Five Bar Linkage](https://arxiv.org/abs/2509.03119)
*Yash Vyas,Matteo Bottin*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A force balanced manipulator design based on the closed chain planar five bar
linkage is developed and experimentally validated. We present 2 variants as a
modular design: Forbal-2, a planar 2-DOF manipulator, and its extension to
5-DOF spatial motion called Forbal-5. The design considerations in terms of
geometric, kinematic, and dynamic design that fulfill the force balance
conditions while maximizing workspace are discussed. Then, the inverse
kinematics of both variants are derived from geometric principles.
  We validate the improvements from force balancing the manipulator through
comparative experiments with counter mass balanced and unbalanced
configurations. The results show how the balanced configuration yields a
reduction in the average reaction moments of up to 66\%, a reduction of average
joint torques of up to 79\%, as well as a noticeable reduction in position
error for Forbal-2. For Forbal-5, which has a higher end effector payload mass,
the joint torques are reduced up to 84\% for the balanced configuration.
Experimental results validate that the balanced manipulator design is suitable
for applications where the reduction of joint torques and reaction
forces/moments helps achieve millimeter level precision.

</details>


### [203] [Efficient Active Training for Deep LiDAR Odometry](https://arxiv.org/abs/2509.03211)
*Beibei Zhou,Zhiyuan Zhang,Zhenbo Song,Jianhui Guo,Hui Kong*

Main category: cs.RO

TL;DR: 提出一种主动学习框架，通过选择性地从多样化环境中提取训练数据，来提高 LiDAR Odometry 模型的训练效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的深度 LiDAR Odometry 模型需要大量多样化的训练数据，以适应不同环境，这导致训练效率低下。

Method: 提出一个主动学习框架，包含初始训练集选择（ITSS）和主动增量选择（AIS）两个策略。ITSS 通过对一般天气下的运动序列进行详细轨迹分析来选择初始训练集。AIS 利用场景重建和预测不一致性来迭代选择难分样本，特别是在复杂多变的雪天条件下，以优化模型。

Result: 实验证明，该方法在保证与全数据集训练相当的性能下，仅使用了 52% 的序列数据，展示了训练效率和鲁棒性。

Conclusion: 该主动学习框架通过优化训练过程，提高了 LiDAR Odometry 模型的训练效率和泛化能力，为开发更灵活、更可靠的 LiDAR Odometry 系统奠定了基础。

Abstract: Robust and efficient deep LiDAR odometry models are crucial for accurate
localization and 3D reconstruction, but typically require extensive and diverse
training data to adapt to diverse environments, leading to inefficiencies. To
tackle this, we introduce an active training framework designed to selectively
extract training data from diverse environments, thereby reducing the training
load and enhancing model generalization. Our framework is based on two key
strategies: Initial Training Set Selection (ITSS) and Active Incremental
Selection (AIS). ITSS begins by breaking down motion sequences from general
weather into nodes and edges for detailed trajectory analysis, prioritizing
diverse sequences to form a rich initial training dataset for training the base
model. For complex sequences that are difficult to analyze, especially under
challenging snowy weather conditions, AIS uses scene reconstruction and
prediction inconsistency to iteratively select training samples, refining the
model to handle a wide range of real-world scenarios. Experiments across
datasets and weather conditions validate our approach's effectiveness. Notably,
our method matches the performance of full-dataset training with just 52\% of
the sequence volume, demonstrating the training efficiency and robustness of
our active training paradigm. By optimizing the training process, our approach
sets the stage for more agile and reliable LiDAR odometry systems, capable of
navigating diverse environmental conditions with greater precision.

</details>


### [204] [The Role of Embodiment in Intuitive Whole-Body Teleoperation for Mobile Manipulation](https://arxiv.org/abs/2509.03222)
*Sophia Bianchi Moyen,Rickmer Krohn,Sophie Lueth,Kay Pompetzki,Jan Peters,Vignesh Prasad,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: 提供两种不同的机器人控制范式（耦合和解耦）和两种视觉反馈机制（VR和屏幕）的比较，以评估直观遥操作界面的有效性，特别是在长期移动操作任务中。


<details>
  <summary>Details</summary>
Motivation: 直观的遥操作界面对于移动操作机器人收集高质量数据至关重要，同时降低操作员的工作量，尤其是在需要全身协调的长期移动操作任务中。

Method: 比较了两种机器人控制范式（耦合和解耦）和两种视觉反馈机制（VR和屏幕），并在复杂的多阶段任务序列中进行了评估。

Result: 与屏幕反馈相比，VR反馈会增加任务完成时间、认知负荷和感知到的操作员努力。耦合和解耦的控制范式对用户造成的负荷相当，但耦合遥操作获得的数据在模仿学习方面表现更好。

Conclusion: 这项研究为在以人为本的前提下，大规模收集高质量、高维度的移动操作数据提供了宝贵的见解。

Abstract: Intuitive Teleoperation interfaces are essential for mobile manipulation
robots to ensure high quality data collection while reducing operator workload.
A strong sense of embodiment combined with minimal physical and cognitive
demands not only enhances the user experience during large-scale data
collection, but also helps maintain data quality over extended periods. This
becomes especially crucial for challenging long-horizon mobile manipulation
tasks that require whole-body coordination. We compare two distinct robot
control paradigms: a coupled embodiment integrating arm manipulation and base
navigation functions, and a decoupled embodiment treating these systems as
separate control entities. Additionally, we evaluate two visual feedback
mechanisms: immersive virtual reality and conventional screen-based
visualization of the robot's field of view. These configurations were
systematically assessed across a complex, multi-stage task sequence requiring
integrated planning and execution. Our results show that the use of VR as a
feedback modality increases task completion time, cognitive workload, and
perceived effort of the teleoperator. Coupling manipulation and navigation
leads to a comparable workload on the user as decoupling the embodiments, while
preliminary experiments suggest that data acquired by coupled teleoperation
leads to better imitation learning performance. Our holistic view on intuitive
teleoperation interfaces provides valuable insight into collecting
high-quality, high-dimensional mobile manipulation data at scale with the human
operator in mind. Project
website:https://sophiamoyen.github.io/role-embodiment-wbc-moma-teleop/

</details>


### [205] [Exploring persuasive Interactions with generative social robots: An experimental framework](https://arxiv.org/abs/2509.03231)
*Stephan Vonschallen,Larissa Julia Corina Finsler,Theresa Schmiedel,Friederike Eyssel*

Main category: cs.RO

TL;DR: 生成式AI驱动的社交机器人可以通过自然、类人化的沟通来影响用户决策，但其说服力很大程度上取决于沟通的细微差别和情境相关性。


<details>
  <summary>Details</summary>
Motivation: 研究如何检验和提升生成式AI（如大型语言模型）在社交机器人中的说服能力。

Method: 设计了一个以决策为中心的实验框架，并通过试点研究，改变机器人的外观和自我认知，使用定性分析来评估互动质量、说服效果和机器人的沟通策略。

Result: 参与者普遍认为机器人胜任、友好且支持性强，但存在响应延迟和语音识别错误等实际限制。机器人的说服力高度依赖于情境，对礼貌、合理的建议和富有表现力的手势反应良好，但需要更个性化、情境感知强的论点和更清晰的社会角色。

Conclusion: 生成式社交机器人能够影响用户决策，但其有效性取决于沟通的细微差别和情境相关性。研究提出了改进实验框架的建议，以进一步研究机器人与人类用户之间的说服动态。

Abstract: Integrating generative AI such as large language models into social robots
has improved their ability to engage in natural, human-like communication. This
study presents a method to examine their persuasive capabilities. We designed
an experimental framework focused on decision making and tested it in a pilot
that varied robot appearance and self-knowledge. Using qualitative analysis, we
evaluated interaction quality, persuasion effectiveness, and the robot's
communicative strategies. Participants generally experienced the interaction
positively, describing the robot as competent, friendly, and supportive, while
noting practical limits such as delayed responses and occasional
speech-recognition errors. Persuasiveness was highly context dependent and
shaped by robot behavior: participants responded well to polite, reasoned
suggestions and expressive gestures, but emphasized the need for more
personalized, context-aware arguments and clearer social roles. These findings
suggest that generative social robots can influence user decisions, but their
effectiveness depends on communicative nuance and contextual relevance. We
propose refinements to the framework to further study persuasive dynamics
between robots and human users.

</details>


### [206] [Vibration Damping in Underactuated Cable-suspended Artwork -- Flying Belt Motion Control](https://arxiv.org/abs/2509.03238)
*Martin Goubej,Lauria Clarke,Martin Hrabačka,David Tolar*

Main category: cs.RO

TL;DR: 本文对Rafael Lozano-Hemmer的互动机器人艺术装置“标准与双重标准”进行了全面修复，通过改进硬件和控制算法，特别是引入基于凸优化的输入整形方法，成功抑制了振荡动力学，实现了更平稳、更快的皮带运动，提高了系统的性能和观众互动性。


<details>
  <summary>Details</summary>
Motivation: 原始的“标准与双重标准”艺术装置由于振荡动力学限制了旋转速度和互动响应能力，需要进行改进。

Method: 开发了飞行皮带系统的详细数学模型，并实施了一种作为凸优化问题的输入整形方法来抑制振动。

Result: 实验结果表明，系统性能和观众互动性得到了显著改善，实现了更平稳、更快的皮带运动。

Conclusion: 该修复工作整合了机器人技术、控制工程和互动艺术，为大型动态装置的实时运动控制和振动抑制等技术挑战提供了新的解决方案。

Abstract: This paper presents a comprehensive refurbishment of the interactive robotic
art installation Standards and Double Standards by Rafael Lozano-Hemmer. The
installation features an array of belts suspended from the ceiling, each
actuated by stepper motors and dynamically oriented by a vision-based tracking
system that follows the movements of exhibition visitors. The original system
was limited by oscillatory dynamics, resulting in torsional and pendulum-like
vibrations that constrained rotational speed and reduced interactive
responsiveness. To address these challenges, the refurbishment involved
significant upgrades to both hardware and motion control algorithms. A detailed
mathematical model of the flying belt system was developed to accurately
capture its dynamic behavior, providing a foundation for advanced control
design. An input shaping method, formulated as a convex optimization problem,
was implemented to effectively suppress vibrations, enabling smoother and
faster belt movements. Experimental results demonstrate substantial
improvements in system performance and audience interaction. This work
exemplifies the integration of robotics, control engineering, and interactive
art, offering new solutions to technical challenges in real-time motion control
and vibration damping for large-scale kinetic installations.

</details>


### [207] [Parallel-Constraint Model Predictive Control: Exploiting Parallel Computation for Improving Safety](https://arxiv.org/abs/2509.03261)
*Elias Fontanari,Gianni Lunardi,Matteo Saveriano,Andrea Del Prete*

Main category: cs.RO

TL;DR: 通过利用并行计算来改进模型预测控制（MPC）的安全性，为每个MPC问题在不同时间步长上实例化安全集约束，并通过仿真验证了在机器人手臂上的安全性和性能提升。


<details>
  <summary>Details</summary>
Motivation: 确保约束满足对于包含大多数机器人平台的安全关键系统至关重要，而模型预测控制（MPC）是处理这些约束的常用方法，但对于非线性系统/约束而言，确保约束满足是一个挑战。

Method: 利用并行计算，同时解决多个MPC问题，其中每个问题在MPC的视野中的不同时间步长上实例化安全集约束，并根据用户定义的标准选择最佳解决方案。

Result: 通过在3关节机器人手臂上的大量仿真验证，证明了该方法在安全性和性能方面可以实现显著改进，即使只使用4个计算核心。

Conclusion: 提出的基于并行计算的安全集方法能够显著提升MPC在安全关键系统中的安全性和性能。

Abstract: Ensuring constraint satisfaction is a key requirement for safety-critical
systems, which include most robotic platforms. For example, constraints can be
used for modeling joint position/velocity/torque limits and collision
avoidance. Constrained systems are often controlled using Model Predictive
Control, because of its ability to naturally handle constraints, relying on
numerical optimization. However, ensuring constraint satisfaction is
challenging for nonlinear systems/constraints. A well-known tool to make
controllers safe is the so-called control-invariant set (a.k.a. safe set). In
our previous work, we have shown that safety can be improved by letting the
safe-set constraint recede along the MPC horizon. In this paper, we push that
idea further by exploiting parallel computation to improve safety. We solve
several MPC problems at the same time, where each problem instantiates the
safe-set constraint at a different time step along the horizon. Finally, the
controller can select the best solution according to some user-defined
criteria. We validated this idea through extensive simulations with a 3-joint
robotic arm, showing that significant improvements can be achieved in terms of
safety and performance, even using as little as 4 computational cores.

</details>


### [208] [Cost-Optimized Systems Engineering for IoT-Enabled Robot Nurse in Infectious Pandemic Management](https://arxiv.org/abs/2509.03436)
*Md Mhamud Hussen Sifat,Md Maruf,Md Rokunuzzaman*

Main category: cs.RO

TL;DR: 机器人护士可以利用物联网自动化医疗程序，以节省成本和时间，同时减少感染风险。


<details>
  <summary>Details</summary>
Motivation: 提高医疗保健系统的可持续性和盈利能力，并应对 COVID-19 大流行带来的挑战。

Method: 提出并评估了一个由物联网控制的机器人护士系统，该系统能够评估患者健康状况、执行药物管理和进行生命周期考量。

Result: 机器人护士系统在药物管理和健康状况监测方面表现良好，并考虑了生命周期因素。

Conclusion: 机器人护士系统有潜力通过自动化任务来降低感染风险并改善大流行病环境中的治疗效果。

Abstract: The utilization of robotic technology has gained traction in healthcare
facilities due to progress in the field that enables time and cost savings,
minimizes waste, and improves patient care. Digital healthcare technologies
that leverage automation, such as robotics and artificial intelligence, have
the potential to enhance the sustainability and profitability of healthcare
systems in the long run. However, the recent COVID-19 pandemic has amplified
the need for cyber-physical robots to automate check-ups and medication
administration. A robot nurse is controlled by the Internet of Things (IoT) and
can serve as an automated medical assistant while also allowing supervisory
control based on custom commands. This system helps reduce infection risk and
improves outcomes in pandemic settings. This research presents a test case with
a nurse robot that can assess a patient's health status and take action
accordingly. We also evaluate the system's performance in medication
administration, health-status monitoring, and life-cycle considerations.

</details>


### [209] [Real-Time Instrument Planning and Perception for Novel Measurements of Dynamic Phenomena](https://arxiv.org/abs/2509.03500)
*Itai Zilberstein,Alberto Candela,Steve Chien*

Main category: cs.RO

TL;DR: 远程传感代理可以通过在边缘使用最先进的计算机视觉和机器学习来进行稀有、短暂和精确的动态科学现象测量。本文提出了一种自动化工作流程，该流程将前瞻性卫星图像中的动态事件检测与自主轨迹规划相结合，以进行后续的高分辨率传感，从而获得精确测量。


<details>
  <summary>Details</summary>
Motivation: 利用远程传感代理在边缘使用最先进的计算机视觉和机器学习的能力，以实现稀有、短暂和精确的动态科学现象测量。

Method: 提出了一种自动化工作流程，该流程将前瞻性卫星图像中的动态事件检测与自主轨迹规划相结合，并应用了多种分类方法（包括传统的机器学习算法和卷积神经网络），以及跟踪羽流形态特征的轨迹规划算法。

Result: 通过模拟，与基线方法相比，高分辨率仪器的效用回报提高了两个数量级，同时保持了高效的运行时间。

Conclusion: 所提出的自动化工作流程能够有效地检测动态事件并规划高分辨率传感器的轨迹，从而提高测量效用。

Abstract: Advancements in onboard computing mean remote sensing agents can employ
state-of-the-art computer vision and machine learning at the edge. These
capabilities can be leveraged to unlock new rare, transient, and pinpoint
measurements of dynamic science phenomena. In this paper, we present an
automated workflow that synthesizes the detection of these dynamic events in
look-ahead satellite imagery with autonomous trajectory planning for a
follow-up high-resolution sensor to obtain pinpoint measurements. We apply this
workflow to the use case of observing volcanic plumes. We analyze
classification approaches including traditional machine learning algorithms and
convolutional neural networks. We present several trajectory planning
algorithms that track the morphological features of a plume and integrate these
algorithms with the classifiers. We show through simulation an order of
magnitude increase in the utility return of the high-resolution instrument
compared to baselines while maintaining efficient runtimes.

</details>


### [210] [Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling? A Validation Study with Naturalistic Trajectories](https://arxiv.org/abs/2509.03515)
*Yanlin Zhang,Sungyong Chung,Nachuan Li,Dana Monzer,Hani S. Mahmassani,Samer H. Hamdar,Alireza Talebpour*

Main category: cs.RO

TL;DR: WOMD数据集可能无法准确反映真实自动驾驶车辆行为，基于PHX数据集的分析显示WOMD低估了驾驶行为的变异性、风险和复杂性。


<details>
  <summary>Details</summary>
Motivation: 评估Waymo开放运动数据集（WOMD）在自动驾驶车辆行为分析中的有效性，因其专有后处理、缺乏误差量化以及轨迹分段等问题，其真实性存疑。

Method: 利用独立收集的凤凰城（PHX）真实数据集，在三个典型城市驾驶场景（信号交叉口放行、跟车、变道）下，与WOMD进行对比分析。具体方法包括：手动提取PHX数据中的车头时距以减少测量误差；应用SIMEX方法处理PHX数据中的误差；使用DTW距离量化行为差异。

Result: 在所有分析场景中，PHX数据集中的驾驶行为均超出WOMD的范围。具体而言，WOMD低估了短车头时距和突然减速的频率。

Conclusion: 仅基于WOMD训练的驾驶行为模型可能会系统性地低估真实驾驶行为的变异性、风险和复杂性。因此，在使用WOMD进行行为建模时，应谨慎并结合独立数据集进行验证。

Abstract: The Waymo Open Motion Dataset (WOMD) has become a popular resource for
data-driven modeling of autonomous vehicles (AVs) behavior. However, its
validity for behavioral analysis remains uncertain due to proprietary
post-processing, the absence of error quantification, and the segmentation of
trajectories into 20-second clips. This study examines whether WOMD accurately
captures the dynamics and interactions observed in real-world AV operations.
Leveraging an independently collected naturalistic dataset from Level 4 AV
operations in Phoenix, Arizona (PHX), we perform comparative analyses across
three representative urban driving scenarios: discharging at signalized
intersections, car-following, and lane-changing behaviors. For the discharging
analysis, headways are manually extracted from aerial video to ensure
negligible measurement error. For the car-following and lane-changing cases, we
apply the Simulation-Extrapolation (SIMEX) method to account for empirically
estimated error in the PHX data and use Dynamic Time Warping (DTW) distances to
quantify behavioral differences. Results across all scenarios consistently show
that behavior in PHX falls outside the behavioral envelope of WOMD. Notably,
WOMD underrepresents short headways and abrupt decelerations. These findings
suggest that behavioral models calibrated solely on WOMD may systematically
underestimate the variability, risk, and complexity of naturalistic driving.
Caution is therefore warranted when using WOMD for behavior modeling without
proper validation against independently collected data.

</details>


### [211] [Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey](https://arxiv.org/abs/2508.13073)
*Rui Shao,Wei Li,Lingsen Zhang,Renshan Zhang,Zhiyang Liu,Ran Chen,Liqiang Nie*

Main category: cs.RO

TL;DR: 本篇论文系统性地回顾了大型视觉-语言-动作（VLA）模型在机器人操作领域的应用，重点关注基于大型视觉-语言模型（VLM）的方法。


<details>
  <summary>Details</summary>
Motivation: 传统的基于规则的方法在非结构化、新颖的环境中难以扩展和泛化，因此需要新的方法来实现机器人操作的精确运动控制和多模态理解。大型VLM驱动的VLA模型已成为一种变革性的范式。

Method: 本文首先定义了大型VLM驱动的VLA模型，并将其划分为两种主要架构范例：整体式模型（包括单系统和双系统设计）和层次式模型（将规划与执行解耦）。然后，论文深入探讨了这些模型与强化学习、无训练优化、人类视频学习和世界模型集成等高级领域的结合；综合了它们的架构特征、操作优势以及支持它们发展的数据集和基准；并指出了内存机制、4D感知、高效适应、多智能体协作等新兴能力和未来发展方向。

Result: 该调查总结了最近的进展，解决了现有分类法中的不一致之处，减轻了研究碎片化的问题，并通过系统地整合大型VLM和机器人操作交叉领域的研究，填补了一个关键的空白。

Conclusion: 大型VLM驱动的VLA模型为机器人操作提供了新的机遇，能够实现更强的泛化性和适应性。本调查为该领域的研究人员提供了一个全面的参考，并指明了未来的研究方向。

Abstract: Robotic manipulation, a key frontier in robotics and embodied AI, requires
precise motor control and multimodal understanding, yet traditional rule-based
methods fail to scale or generalize in unstructured, novel environments. In
recent years, Vision-Language-Action (VLA) models, built upon Large
Vision-Language Models (VLMs) pretrained on vast image-text datasets, have
emerged as a transformative paradigm. This survey provides the first
systematic, taxonomy-oriented review of large VLM-based VLA models for robotic
manipulation. We begin by clearly defining large VLM-based VLA models and
delineating two principal architectural paradigms: (1) monolithic models,
encompassing single-system and dual-system designs with differing levels of
integration; and (2) hierarchical models, which explicitly decouple planning
from execution via interpretable intermediate representations. Building on this
foundation, we present an in-depth examination of large VLM-based VLA models:
(1) integration with advanced domains, including reinforcement learning,
training-free optimization, learning from human videos, and world model
integration; (2) synthesis of distinctive characteristics, consolidating
architectural traits, operational strengths, and the datasets and benchmarks
that support their development; (3) identification of promising directions,
including memory mechanisms, 4D perception, efficient adaptation, multi-agent
cooperation, and other emerging capabilities. This survey consolidates recent
advances to resolve inconsistencies in existing taxonomies, mitigate research
fragmentation, and fill a critical gap through the systematic integration of
studies at the intersection of large VLMs and robotic manipulation. We provide
a regularly updated project page to document ongoing progress:
https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [212] [Synthetic Founders: AI-Generated Social Simulations for Startup Validation Research in Computational Social Science](https://arxiv.org/abs/2509.02605)
*Jorn K. Teutloff*

Main category: cs.MA

TL;DR: LLM驱动的合成人物可作为一种混合社会模拟形式，补充而非取代实证研究，以扩展假设空间、加速探索性验证并明确计算社会科学中认知现实的边界。


<details>
  <summary>Details</summary>
Motivation: 评估AI驱动的模拟在保真度、差异和盲点方面，将人类访谈数据与LLM驱动的合成人物进行比较。

Method: 通过对接实验，对15位早期创业公司创始人进行访谈，并使用相同的协议与AI生成的创始人及投资者人物进行交互，然后进行结构化主题综合。

Result: 研究结果分为四类：1. 趋同主题（例如，基于承诺的需求信号、信任障碍、效率提升）；2. 部分重叠（例如，人类对异常值和真实客户验证的担忧，合成人物强调非理性盲点和AI作为心理缓冲）；3. 仅人类主题（例如，早期客户参与的价值和对“登月”市场的怀疑）；4. 仅合成主题（例如，放大的虚假积极信号和创伤盲点）。

Conclusion: LLM驱动的人物构成了一种混合社会模拟，比传统的基于规则的代理更具语言表现力和适应性，但缺乏生活经验和关系后果的约束。它们可以作为实证研究的补充，用于扩展假设空间、加速探索性验证和阐明认知现实的边界。

Abstract: We present a comparative docking experiment that aligns human-subject
interview data with large language model (LLM)-driven synthetic personas to
evaluate fidelity, divergence, and blind spots in AI-enabled simulation.
Fifteen early-stage startup founders were interviewed about their hopes and
concerns regarding AI-powered validation, and the same protocol was replicated
with AI-generated founder and investor personas. A structured thematic
synthesis revealed four categories of outcomes: (1) Convergent themes -
commitment-based demand signals, black-box trust barriers, and efficiency gains
were consistently emphasized across both datasets; (2) Partial overlaps -
founders worried about outliers being averaged away and the stress of real
customer validation, while synthetic personas highlighted irrational blind
spots and framed AI as a psychological buffer; (3) Human-only themes -
relational and advocacy value from early customer engagement and skepticism
toward moonshot markets; and (4) Synthetic-only themes - amplified false
positives and trauma blind spots, where AI may overstate adoption potential by
missing negative historical experiences.
  We interpret this comparative framework as evidence that LLM-driven personas
constitute a form of hybrid social simulation: more linguistically expressive
and adaptable than traditional rule-based agents, yet bounded by the absence of
lived history and relational consequence. Rather than replacing empirical
studies, we argue they function as a complementary simulation category -
capable of extending hypothesis space, accelerating exploratory validation, and
clarifying the boundaries of cognitive realism in computational social science.

</details>


### [213] [Automatic Differentiation of Agent-Based Models](https://arxiv.org/abs/2509.03303)
*Arnau Quera-Bofarull,Nicholas Bishop,Joel Dyer,Daniel Jarne Ornia,Anisoara Calinescu,Doyne Farmer,Michael Wooldridge*

Main category: cs.MA

TL;DR: AD技术可以减轻ABM的计算负担，通过提供模拟器的梯度，从而实现高效的参数校准（例如，使用变分推理），并提高ABM在Axtell模型、Sugarscape和SIR模型中的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的agent-based models (ABMs)由于计算成本高和参数众多，在实际应用中受到限制。因此，需要一种方法来减轻这些计算负担。

Method: 应用自动微分（AD）技术到ABMs，以获得模拟器的梯度，从而实现参数校准和敏感性分析。具体来说，使用AD技术来实现变分推理（VI），以进行有效的参数校准。

Result: 通过在Axtell的公司模型、Sugarscape和SIR流行病学模型这三个著名的ABMs上进行实验，证明了使用VI进行AD可以带来显著的性能提升和计算节省。

Conclusion: 自动微分（AD）技术可以有效减轻ABMs的计算负担，提高其在实际应用中的可行性和可扩展性，尤其是在参数校准和敏感性分析方面。

Abstract: Agent-based models (ABMs) simulate complex systems by capturing the bottom-up
interactions of individual agents comprising the system. Many complex systems
of interest, such as epidemics or financial markets, involve thousands or even
millions of agents. Consequently, ABMs often become computationally demanding
and rely on the calibration of numerous free parameters, which has
significantly hindered their widespread adoption. In this paper, we demonstrate
that automatic differentiation (AD) techniques can effectively alleviate these
computational burdens. By applying AD to ABMs, the gradients of the simulator
become readily available, greatly facilitating essential tasks such as
calibration and sensitivity analysis. Specifically, we show how AD enables
variational inference (VI) techniques for efficient parameter calibration. Our
experiments demonstrate substantial performance improvements and computational
savings using VI on three prominent ABMs: Axtell's model of firms; Sugarscape;
and the SIR epidemiological model. Our approach thus significantly enhances the
practicality and scalability of ABMs for studying complex systems.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [214] [Decentralised self-organisation of pivoting cube ensembles using geometric deep learning](https://arxiv.org/abs/2509.03140)
*Nadezhda Dobreva,Emmanuel Blazquez,Jai Grover,Dario Izzo,Yuzhen Qin,Dominik Dold*

Main category: cs.NE

TL;DR: 我们提出了一种用于二维同构枢轴立方体模块化机器人的自主重构的去中心化模型，其中每个立方体由仅能从局部邻域获取信息的神经网络控制，并使用强化学习进行训练。我们发现，即使是最局部的版本也能成功重构为目标形状，但信息越全局，重构速度越快。通过多轮信息传递，即使仅与最近邻交互，也能实现接近最优的重构。与标准神经网络相比，几何深度学习方法的益处甚微。该模型成功展示了模块化自组装系统的局部控制，并可转移到其他空间相关系统。


<details>
  <summary>Details</summary>
Motivation: 提出一种用于二维同构枢轴立方体模块化机器人的去中心化自主重构模型。

Method: 使用强化学习训练由局部信息控制的神经网络，并结合几何深度学习处理网格对称性。

Result: 即使是信息最局部的模型也能成功重构，但全局信息可加速重构。仅通过最近邻交互并进行多次信息传递可实现近乎最优的重构。几何深度学习带来的好处很小。

Conclusion: 成功展示了模块化自组装系统的局部控制，且具有可转移性，可应用于其他空间相关系统。

Abstract: We present a decentralized model for autonomous reconfiguration of
homogeneous pivoting cube modular robots in two dimensions. Each cube in the
ensemble is controlled by a neural network that only gains information from
other cubes in its local neighborhood, trained using reinforcement learning.
Furthermore, using geometric deep learning, we include the grid symmetries of
the cube ensemble in the neural network architecture. We find that even the
most localized versions succeed in reconfiguring to the target shape, although
reconfiguration happens faster the more information about the whole ensemble is
available to individual cubes. Near-optimal reconfiguration is achieved with
only nearest neighbor interactions by using multiple information passing
between cubes, allowing them to accumulate more global information about the
ensemble. Compared to standard neural network architectures, using geometric
deep learning approaches provided only minor benefits. Overall, we successfully
demonstrate mostly local control of a modular self-assembling system, which is
transferable to other space-relevant systems with different action spaces, such
as sliding cube modular robots and CubeSat swarms.

</details>


### [215] [A Brain-Inspired Gating Mechanism Unlocks Robust Computation in Spiking Neural Networks](https://arxiv.org/abs/2509.03281)
*Qianyi Bai,Haiteng Wang,Qiang Yu*

Main category: cs.NE

TL;DR: 动态门控神经元(DGN)通过模拟生物神经元的动态电导机制，提高了脉冲神经网络（SNN）的鲁棒性和处理噪声与时间变异性的能力，在抗噪声和时序任务上表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 现有的脉冲神经网络（SNNs）因过度简化的神经元模型（如LIF）而未能充分利用其生物学启发的鲁棒性和动态优势，限制了其处理噪声和时间变异性的能力。

Method: 提出了一种新的脉冲神经元单元——动态门控神经元（DGN），其膜电导能够根据神经元活动动态调整，实现选择性输入过滤和自适应噪声抑制。并通过理论分析证明DGN相比于标准LIF模型具有更强的随机稳定性，动态电导起到干扰抑制机制的作用。

Result: 基于DGN的SNN在抗噪声任务和TIDIGITS、SHD等时序基准测试中表现出优越性能，展现了出色的鲁棒性。

Conclusion: 首次证明了生物学上合理的动态门控是实现鲁棒脉冲计算的关键机制，为构建更具韧性、效率和生物学启发性的SNNs提供了理论保证和实证支持。

Abstract: While spiking neural networks (SNNs) provide a biologically inspired and
energy-efficient computational framework, their robustness and the dynamic
advantages inherent to biological neurons remain significantly underutilized
owing to oversimplified neuron models. In particular, conventional leaky
integrate-and-fire (LIF) neurons often omit the dynamic conductance mechanisms
inherent in biological neurons, thereby limiting their capacity to cope with
noise and temporal variability. In this work, we revisit dynamic conductance
from a functional perspective and uncover its intrinsic role as a biologically
plausible gating mechanism that modulates information flow. Building on this
insight, we introduce the Dynamic Gated Neuron~(DGN), a novel spiking unit in
which membrane conductance evolves in response to neuronal activity, enabling
selective input filtering and adaptive noise suppression. We provide a
theoretical analysis showing that DGN possess enhanced stochastic stability
compared to standard LIF models, with dynamic conductance intriguingly acting
as a disturbance rejection mechanism. DGN-based SNNs demonstrate superior
performance across extensive evaluations on anti-noise tasks and
temporal-related benchmarks such as TIDIGITS and SHD, consistently exhibiting
excellent robustness. Our results highlight, for the first time, a biologically
plausible dynamic gating as a key mechanism for robust spike-based computation,
providing not only theoretical guarantees but also strong empirical
validations. This work thus paves the way for more resilient, efficient, and
biologically inspired spiking neural networks.

</details>


### [216] [Neural Field Turing Machine: A Differentiable Spatial Computer](https://arxiv.org/abs/2509.03370)
*Akash Malhotra,Nacéra Seghouani*

Main category: cs.NE

TL;DR: NFTM是一个统一符号计算、物理模拟和感知推理的可微分架构，通过结合神经控制器、连续记忆场和可移动读写头来实现。


<details>
  <summary>Details</summary>
Motivation: NFTM的目标是创建一个能够统一符号计算、物理模拟和感知推理的单一可微分框架，以处理连续空间场。

Method: NFTM结合了神经控制器、连续记忆场和可移动读写头，控制器在每个时间步读取局部块，通过学习到的规则计算更新，并将它们写回，同时更新读写头的位置。这种设计通过固定半径邻域实现了O(N)的线性扩展，并在有界误差下保持了图灵完备性。

Result: NFTM在细胞自动机模拟（Rule 110）、物理信息偏微分方程求解器（二维热方程）和迭代图像精炼（CIFAR-10修复）等任务上进行了演示。这些实例学会了构成全局动态的局部更新规则，表现出稳定的长周期运行，并能泛化到超出训练范围的周期。

Conclusion: NFTM提供了一个统一的计算基础，在单一可微分框架内架起了离散算法和连续场动力学的桥梁。

Abstract: We introduce the Neural Field Turing Machine (NFTM), a differentiable
architecture that unifies symbolic computation, physical simulation, and
perceptual inference within continuous spatial fields. NFTM combines a neural
controller, continuous memory field, and movable read/write heads that perform
local updates. At each timestep, the controller reads local patches, computes
updates via learned rules, and writes them back while updating head positions.
This design achieves linear O(N) scaling through fixed-radius neighborhoods
while maintaining Turing completeness under bounded error. We demonstrate three
example instantiations of NFTM: cellular automata simulation (Rule 110),
physics-informed PDE solvers (2D heat equation), and iterative image refinement
(CIFAR-10 inpainting). These instantiations learn local update rules that
compose into global dynamics, exhibit stable long-horizon rollouts, and
generalize beyond training horizons. NFTM provides a unified computational
substrate bridging discrete algorithms and continuous field dynamics within a
single differentiable framework.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [217] [Rollout-Based Approximate Dynamic Programming for MDPs with Information-Theoretic Constraints](https://arxiv.org/abs/2509.02812)
*Zixuan He,Charalambos D. Charalambous,Photios A. Stavrou*

Main category: eess.SY

TL;DR: 本文提出了一种基于截断前滚的近似动态规划框架，用于解决具有信息论约束的有限时间马尔可夫决策问题，通过两阶段方法在理论和数值上都证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决具有信息论约束的有限时间马尔可夫决策问题，目标是最小化从受控源过程到控制过程的定向信息，同时满足阶段成本约束，并找到最优控制策略。该问题可以被表述为一个具有连续信息状态的无约束MDP，但计算复杂性高。

Method: 提出了一种基于截断前滚的近似动态规划（ADP）框架，以避免对连续信息状态空间进行离散化。该框架包括两个阶段：1. 离线基础策略近似，在较短的时间范围内进行。2. 在线前滚前瞻最小化。这两种方法都有可证明的收敛保证。

Result: 通过数值示例，证明了所提出的前滚方法相比于先前提出的策略近似方法在成本降低方面具有优势。同时，还展示了两种方法在执行离线和在线阶段时的计算复杂性。

Conclusion: 所提出的基于截断前滚的ADP框架能够有效地近似解决具有信息论约束的有限时间马尔可夫决策问题，并在成本和计算效率方面优于现有方法。

Abstract: This paper studies a finite-horizon Markov decision problem with
information-theoretic constraints, where the goal is to minimize directed
information from the controlled source process to the control process, subject
to stage-wise cost constraints, aiming for an optimal control policy. We
propose a new way of approximating a solution for this problem, which is known
to be formulated as an unconstrained MDP with a continuous information-state
using Q-factors. To avoid the computational complexity of discretizing the
continuous information-state space, we propose a truncated rollout-based
backward-forward approximate dynamic programming (ADP) framework. Our approach
consists of two phases: an offline base policy approximation over a shorter
time horizon, followed by an online rollout lookahead minimization, both
supported by provable convergence guarantees. We supplement our theoretical
results with a numerical example where we demonstrate the cost improvement of
the rollout method compared to a previously proposed policy approximation
method, and the computational complexity observed in executing the offline and
online phases for the two methods.

</details>


### [218] [Hybrid dynamical systems modeling of power systems](https://arxiv.org/abs/2509.02822)
*B. G. Odunlami,M. Netto,Y. Susuki*

Main category: eess.SY

TL;DR: 混合动力系统为电力系统建模提供了一种严谨的方法，但缺乏通用性。本研究全面概述了电力系统的混合建模方法，探讨了混合自动机、切换系统和分段仿射模型，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统连续时间建模方法在处理可再生能源并网带来的复杂动态行为方面存在挑战，需要能够捕捉连续动态和离散事件之间复杂相互作用的建模框架。混合动力系统为表示这种混合动态提供了严谨的基础，并在电力系统分析中显示出其价值。

Method: 本文全面概述了与电力系统相关的混合建模方法，并批判性地审查了包括混合自动机、切换系统和分段仿射模型在内的关键形式，评估了它们各自的优缺点以及在控制、稳定性和系统设计任务中的适用性。

Result: 本文评估了混合自动机、切换系统和分段仿射模型在控制、稳定性以及系统设计任务中的优势、局限性以及适用性。

Conclusion: 现有研究主要集中在孤立的应用或特定案例的实现上，通用性有限。本研究通过提供电力系统混合建模方法的全面概述，弥合了这一差距，并为在可再生能源丰富、转换器主导的电力系统中系统地应用混合方法指明了未来的研究方向。

Abstract: The increasing integration of renewable energy sources has introduced complex
dynamic behavior in power systems that challenge the adequacy of traditional
continuous-time modeling approaches. These developments call for modeling
frameworks that can capture the intricate interplay between continuous dynamics
and discrete events characterizing modern grid operations. Hybrid dynamical
systems offer a rigorous foundation for representing such mixed dynamics and
have emerged as a valuable tool in power system analysis. Despite their
potential, existing studies remain focused on isolated applications or
case-specific implementations, offering limited generalizability and guidance
for model selection. This paper addresses that gap by providing a comprehensive
overview of hybrid modeling approaches relevant to power systems. It critically
examines key formalisms, including hybrid automata, switched systems, and
piecewise affine models, evaluating their respective strengths, limitations,
and suitability across control, stability, and system design tasks. In doing
so, the paper identifies open challenges and outlines future research
directions to support the systematic application of hybrid methods in
renewable-rich, converter-dominated power systems

</details>


### [219] [An overview of Koopman-based control: From error bounds to closed-loop guarantees](https://arxiv.org/abs/2509.02839)
*Robin Strässer,Karl Worthmann,Igor Mezić,Julian Berberich,Manuel Schaller,Frank Allgöwer*

Main category: eess.SY

TL;DR: 该论文主要概述了基于Koopman算子理论的非线性动力学系统数据驱动控制方法。


<details>
  <summary>Details</summary>
Motivation: 在缺乏精确物理模型的情况下，控制非线性动力学系统是一个广泛应用的挑战。数据驱动方法提供了一种直接从观测轨迹设计控制器的新途径，其中Koopman算子框架尤为重要。

Method: 本文系统地回顾了基于Koopman算子（特别是扩展动态模分解EDMD及其控制变体）的控制方法。重点介绍了数据驱动模型、近似误差、控制器设计以及闭环保证之间的联系，并回顾了理论基础、误差界限、线性和双线性EDMD控制方案，以及确保稳定性和性能的鲁棒策略。

Result: 文章分析了有限数据生成的数据驱动模型、近似误差、控制器设计和闭环保证之间的关系，并重点介绍了鲁棒策略以确保稳定性和性能。

Conclusion: 文章总结了在算子理论、逼近理论和非线性控制交叉领域中，基于Koopman算子的控制研究的当前挑战和未来发展方向。

Abstract: Controlling nonlinear dynamical systems remains a central challenge in a wide
range of applications, particularly when accurate first-principle models are
unavailable. Data-driven approaches offer a promising alternative by designing
controllers directly from observed trajectories. A wide range of data-driven
methods relies on the Koopman-operator framework that enables linear
representations of nonlinear dynamics via lifting into higher-dimensional
observable spaces. Finite-dimensional approximations, such as extended dynamic
mode decomposition (EDMD) and its controlled variants, make prediction and
feedback control tractable but introduce approximation errors that must be
accounted for to provide rigorous closed-loop guarantees. This survey provides
a systematic overview of Koopman-based control, emphasizing the connection
between data-driven surrogate models generated from finite data, approximation
errors, controller design, and closed-loop guarantees. We review theoretical
foundations, error bounds, and both linear and bilinear EDMD-based control
schemes, highlighting robust strategies that ensure stability and performance.
Finally, we discuss open challenges and future directions at the interface of
operator theory, approximation theory, and nonlinear control.

</details>


### [220] [A Distributed Gradient-Based Deployment Strategy for a Network of Sensors with a Probabilistic Sensing Model](https://arxiv.org/abs/2509.02869)
*Hesam Mosalli,Amir G. Aghdam*

Main category: eess.SY

TL;DR: 提出一种分布式梯度下降的部署策略，利用Voronoi划分将整体覆盖分解为局部贡献，实现混合无线传感器网络(WSN)中传感器在概率感知下的最大化覆盖。该策略使用Elfes模型处理检测不确定性，并引入基于局部覆盖梯度动态调整的步长，同时考虑障碍物约束和阈值决策规则，以实现自适应和高效的传感器部署。


<details>
  <summary>Details</summary>
Motivation: 优化混合无线传感器网络(WSN)的覆盖率，特别是在传感器具有概率感知能力的情况下。

Method: 采用分布式梯度下降方法，结合Voronoi区域划分，将整体覆盖问题分解为局部优化问题。使用Elfes模型处理感知不确定性，引入基于局部覆盖梯度变化的动态步长，并集成障碍物感知和阈值决策规则，确保传感器仅在覆盖率有显著提升时移动，最终达到局部最优部署。进行仿真验证。

Result: 仿真结果表明，所提出的分布式部署策略相比静态部署能显著提高覆盖率，并证明了其良好的可扩展性和实用性，适用于实际应用场景。

Conclusion: 该分布式梯度下降部署策略能够有效地最大化混合WSN的覆盖率，并且易于扩展和实际应用。

Abstract: This paper presents a distributed gradient-based deployment strategy to
maximize coverage in hybrid wireless sensor networks (WSNs) with probabilistic
sensing. Leveraging Voronoi partitioning, the overall coverage is reformulated
as a sum of local contributions, enabling mobile sensors to optimize their
positions using only local information. The strategy adopts the Elfes model to
capture detection uncertainty and introduces a dynamic step size based on the
gradient of the local coverage, ensuring movements adaptive to regional
importance. Obstacle awareness is integrated via visibility constraints,
projecting sensor positions to unobstructed paths. A threshold-based decision
rule ensures movement occurs only for sufficiently large coverage gains, with
convergence achieved when all sensors and their neighbors stop at a local
maximum configuration. Simulations demonstrate improved coverage over static
deployments, highlighting scalability and practicality for real-world
applications.

</details>


### [221] [Approximate constrained stochastic optimal control via parameterized input inference](https://arxiv.org/abs/2509.02922)
*Shahbaz P Qadri Syed,He Bai*

Main category: eess.SY

TL;DR: 该研究提出了一种基于期望最大化（EM）的推理过程，用于为约束随机最优控制（SOC）问题生成状态反馈控制。


<details>
  <summary>Details</summary>
Motivation: 过去十年，解决随机最优控制（SOC）问题近似方法引起了研究人员的极大兴趣。特别是，已开发出用于解决非线性二次高斯问题的SOC概率推理方法。

Method: 提出了一种基于期望最大化（EM）的推理过程，并使用障碍函数来处理状态和控制约束，包括不等式约束和结构约束。EM算法的期望步骤实现了状态-控制对的平滑，而最大化步骤则用于推断结构化随机最优控制器。

Result: 该方法在解决约束SOC问题方面表现出有效性，并在无约束车辆避障、四车编队控制和带风四旋翼导航等示例中得到了验证。此外，还进行了参数对状态约束满足的障碍函数影响的实证研究，并对平滑算法对所提出方法性能的比较研究。

Conclusion: 所提出的EM方法能够有效地处理约束SOC问题，并生成状态反馈控制器，在各种实际应用中得到了证明。

Abstract: Approximate methods to solve stochastic optimal control (SOC) problems have
received significant interest from researchers in the past decade.
Probabilistic inference approaches to SOC have been developed to solve
nonlinear quadratic Gaussian problems. In this work, we propose an
Expectation-Maximization (EM) based inference procedure to generate
state-feedback controls for constrained SOC problems. We consider the
inequality constraints for the state and controls and also the structural
constraints for the controls. We employ barrier functions to address state and
control constraints. We show that the expectation step leads to smoothing of
the state-control pair while the the maximization step on the non-zero subsets
of the control parameters allows inference of structured stochastic optimal
controllers. We demonstrate the effectiveness of the algorithm on unicycle
obstacle avoidance, four-unicycle formation control, and quadcopter navigation
in windy environment examples. In these examples, we perform an empirical study
on the parametric effect of barrier functions on the state constraint
satisfaction. We also present a comparative study of smoothing algorithms on
the performance of the proposed approach.

</details>


### [222] [Deep Reinforcement Learning-Based Decision-Making Strategy Considering User Satisfaction Feedback in Demand Response Program](https://arxiv.org/abs/2509.02946)
*Xin Li,Li Ding,Qiao Lin,Zhen-Wei Yu*

Main category: eess.SY

TL;DR: 该研究提出了一种多分支时间融合孪生延迟深度确定性策略梯度（MBTF-TD3）强化学习算法，以解决现有需求响应（DR）优化模型忽视用户满意度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有需求响应（DR）优化模型通常侧重于需求响应提供商（DRP）的经济收益，而忽视了终端用户的满意度。此外，模型缺乏用户决策和满意度评估的数学细节，给传统方法带来挑战。

Method: 设计了一个用户端满意度评估机制，并将用户满意度反馈纳入奖励函数，通过动态调整惩罚项实现。同时，提出了一种MBTF-TD3算法，利用多分支结构提取时间序列数据的时序特征依赖性。

Result: 所提出的动态调整的惩罚函数成功提高了用户的整体满意度水平。实验验证了所提出的解决方案算法的性能和有效性。

Conclusion: 通过结合用户满意度评估和先进的强化学习算法（MBTF-TD3），该研究为需求响应优化提供了一个更全面、用户友好的解决方案。

Abstract: Demand response providers (DRPs) are intermediaries between the upper-level
distribution system operator and the lower-level participants in demand
response (DR) programs. Usually, DRPs act as leaders and determine electricity
pricing strategies to maximize their economic revenue, while end-users adjust
their power consumption following the pricing signals. However, this
profit-seeking bi-level optimization model often neglects the satisfaction of
end-users participating in DR programs. In addition, the detailed mathematical
models underlying user decision-making strategy and satisfaction evaluation
mechanism are typically unavailable to DRPs, posing significant challenges to
conventional model-based solution methods. To address these issues, this paper
designs a user-side satisfaction evaluation mechanism and proposes a
multi-branch temporal fusion twin-delayed deep deterministic policy gradient
(MBTF-TD3) reinforcement learning algorithm. User satisfaction feedback is
incorporated into the reward function via a dynamically adjusted penalty term.
The proposed MBTF structure effectively extracts temporal feature dependencies
in the time-series observation data, and the dynamically adjusted penalty
function successfully enhances the overall satisfaction level of users. Several
experiments are conducted to validate the performance and the effectiveness of
our proposed solution algorithm.

</details>


### [223] [Spiking control systems for soft robotics: a rhythmic case study in a soft robotic crawler](https://arxiv.org/abs/2509.02968)
*Juncal Arbelaiz,Alessio Franci,Naomi Ehrich Leonard,Rodolphe Sepulchre,Bassam Bamieh*

Main category: eess.SY

TL;DR: 受尖峰神经反馈启发，提出了一种用于软体机器人爬行器的高效运动尖峰控制器。该控制器结合了类神经快速正反馈的双稳态和感觉运动慢负反馈回路，可产生尖峰节律。闭环系统通过量化驱动具有鲁棒性，负反馈确保高效运动且只需少量外部调谐。证明了蠕动波源于由感觉运动增益控制的超临界Hopf分岔。量纲分析揭示了机械和电气时间尺度的分离，几何奇异摄动分析解释了内源性爬行通过弛豫振荡产生。在奇异摄动条件下，提出并解析求解了一个优化问题，证明了在机械共振下爬行通过神经机械尺度匹配可以最大化速度。考虑到软体运动中节律和波形的重要性和普遍性，设想尖峰控制系统可用于各种软体机器人形态和模块化分布式架构，在不同尺度上实现显著的鲁棒性、适应性和能量增益。


<details>
  <summary>Details</summary>
Motivation: 受尖峰神经反馈启发，为软体机器人爬行器设计一种高效运动的尖峰控制器。

Method: 提出了一种结合了双稳态（类神经快速正反馈）和感觉运动慢负反馈回路的尖峰控制器，以产生尖峰节律。利用量纲分析和几何奇异摄动分析来解释蠕动波的产生和内源性爬行。在奇异摄动条件下，提出并求解了一个优化问题，以找到最大化爬行速度的条件。

Result: 证明了蠕动波源于由感觉运动增益控制的超临界Hopf分岔。通过几何奇异摄动分析解释了内源性爬行是弛豫振荡的结果。在机械共振下，通过神经机械尺度的匹配，爬行速度最大化。

Conclusion: 尖峰控制系统具有鲁棒性、适应性和能量效率，可用于各种软体机器人形态和分布式架构，在软体运动中实现节律和波形控制。

Abstract: Inspired by spiking neural feedback, we propose a spiking controller for
efficient locomotion in a soft robotic crawler. Its bistability, akin to neural
fast positive feedback, combined with a sensorimotor slow negative feedback
loop, generates rhythmic spiking. The closed-loop system is robust through the
quantized actuation, and negative feedback ensures efficient locomotion with
minimal external tuning. We prove that peristaltic waves arise from a
supercritical Hopf bifurcation controlled by the sensorimotor gain. Dimensional
analysis reveals a separation of mechanical and electrical timescales, and
Geometric Singular Perturbation analysis explains endogenous crawling through
relaxation oscillations. We further formulate and analytically solve an
optimization problem in the singularly perturbed regime, proving that crawling
at mechanical resonance maximizes speed by a matching of neuromechanical
scales. Given the importance and ubiquity of rhythms and waves in soft-bodied
locomotion, we envision that spiking control systems could be utilized in a
variety of soft-robotic morphologies and modular distributed architectures,
yielding significant robustness, adaptability, and energetic gains across
scales.

</details>


### [224] [On the Smart Coordination of Flexibility Scheduling in Multi-carrier Integrated Energy Systems](https://arxiv.org/abs/2509.03126)
*Christian Doh Dinga,Sander van Rijn,Laurens de Vries,Milos Cvetkovic*

Main category: eess.SY

TL;DR: 该论文提出了一种市场拍卖启发的耦合方法来协调多载流子集成能源系统（MIES）中的灵活性调度，解决了自主性、隐私和可扩展性问题，并提供了开源软件以供实际应用。


<details>
  <summary>Details</summary>
Motivation: 协调多载流子集成能源系统（MIES）中的灵活性资产可以促进可再生能源的整合和能源转型，但灵活性资产的激增和需求响应增加了协调的复杂性。

Method: 提出了一种市场拍卖启发的耦合方法来协调MIES中的灵活性调度，以解决保持灵活性提供者的自主性和隐私以及可扩展性问题。

Result: 该方法在可扩展性方面表现良好，并且比联合优化和迭代价格响应方法更适合大规模能源系统的灵活性建模，实现了“近乎最优”的调度和价格。

Conclusion: 提出的市场拍卖启发的耦合方法能够有效协调MIES中的灵活性资产，同时保护隐私和提高可扩展性，并且其开源软件具有广泛的实际应用前景。

Abstract: Coordinating the interactions between flexibility assets in multi-carrier
integrated energy systems (MIES) can lead to an efficient integration of
variable renewable energy resources, and a cost-efficient energy transition.
However, the proliferation of flexibility assets and their participation in
active demand response increases the complexity of coordinating these
interactions. This paper introduces different approaches to model the
coordination of flexibility scheduling in MIES. We propose a market
auction-inspired model coupling approach to address the challenges of
preserving the autonomy and privacy of flexibility providers, and the issue of
scalability. We benchmark our approach against co-optimization and an iterative
price-response method by conducting experiments with varying problem sizes and
computing infrastructure. We show that our approach scales well and is suitable
for modeling flexibility in large-scale energy systems in a more realistic way.
From an optimality standpoint, the flexibility dispatch schedules and
electricity prices are ``near-optimal". Our methodology is implemented as a new
open-source software, which offers several practical applications. For example,
flexibility providers and network operators can couple their models to simulate
the interaction between their systems without disclosing confidential
information; policy regulators can use it to investigate new market design and
regulations to optimize the utilization of flexibility in MIES.

</details>


### [225] [Target Enclosing Control for Nonholonomic Multi-Agent Systems with Connectivity Maintenance and Collision Avoidance](https://arxiv.org/abs/2509.03168)
*Boyin Zheng,Yahui Hao,Lu Liu*

Main category: eess.SY

TL;DR: 该研究提出了一种用于非完整多主体系统的移动目标包围控制方法，该方法在保证网络连通性和避免碰撞的同时，解决了距离约束问题。


<details>
  <summary>Details</summary>
Motivation: 处理非完整多主体系统在有界交互和碰撞阈值下的移动目标包围控制问题，并确保网络连通性和避免碰撞。

Method: 利用 Henneberg 构造法将目标包围需求纳入等静压距离基准框架，整合距离约束，并提出固定时间角速度控制律（使用障碍李雅普诺夫函数）和指定性能控制律（使用变换误差约束）来消除奇异性。

Result: 所提出的控制律能够使多主体系统渐近地实现围绕移动目标的期望角向编队模式，同时满足距离约束，并且不需要控制目标运动。

Conclusion: 该方法成功解决了非完整多主体系统的移动目标包围控制问题，具有网络连通性、碰撞避免和距离约束的保证，并提供了仿真示例进行验证。

Abstract: This article addresses the moving target enclosing control problem for
nonholonomic multi-agent systems with guaranteed network connectivity and
collision avoidance. We propose a novel control scheme to handle distance
constraints imposed by the agents' limited interaction ranges and
collision-free thresholds. By leveraging a Henneberg construction method, we
innovatively formulate the target enclosing requirements within an isostatic
distance-based formation framework, facilitating the integration of distance
constraints. Compared with existing results, our approach ensures the positive
definiteness of the underlying rigidity matrix and does not require controlling
the target's motion. To eliminate the occurrences of control singularities
caused by nonholonomic constraints, we propose a fixed-time angular control law
using barrier Lyapunov functions. Additionally, we develop a linear velocity
control law using the prescribed performance control approach and transformed
error constraints. We rigorously prove that our control laws enable the
multi-agent system to asymptotically achieve the desired angular formation
pattern around a moving target while satisfying the established distance
constraints. Finally, a simulation example is provided to validate the
effectiveness of the proposed method.

</details>


### [226] [Hidden Convexity in Active Learning: A Convexified Online Input Design for ARX Systems](https://arxiv.org/abs/2509.03257)
*Nicolas Chatzikiriakos,Bowen Song,Philipp Rank,Andrea Iannelli*

Main category: eess.SY

TL;DR: 本文提出了一种在线输入设计算法，通过主动学习和凸优化方法，加速未知ARX系统的识别，并给出了样本复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 加速从轨迹数据中识别未知ARX系统的过程。

Method: 提出了一种主动学习算法，该算法利用过去的测量数据，根据实验设计准则来选择输入以激励系统。该准则会导致一个非凸优化问题，但论文提供了一个精确的凸重构方法来找到全局最优解。此外，还给出了由于随机噪声导致的估计误差的样本复杂度界限。

Result: 数值研究证明了该算法的有效性以及凸重构方法的优势。

Conclusion: 所提出的在线输入设计算法能够有效地加速未知ARX系统的识别。

Abstract: The goal of this work is to accelerate the identification of an unknown ARX
system from trajectory data through online input design. Specifically, we
present an active learning algorithm that sequentially selects the input to
excite the system according to an experiment design criterion using the past
measured data. The adopted criterion yields a non-convex optimization problem,
but we provide an exact convex reformulation allowing to find the global
optimizer in a computationally tractable way. Moreover, we give sample
complexity bounds on the estimation error due to the stochastic noise.
Numerical studies showcase the effectiveness of our algorithm and the benefits
of the convex reformulation.

</details>


### [227] [Tangential Action Spaces: Geometry, Memory and Cost in Holonomic and Nonholonomic Agents](https://arxiv.org/abs/2509.03399)
*Marcel Blattner*

Main category: eess.SY

TL;DR: 本文提出切线动作空间（TAS）框架，揭示了具身智能体记忆与其能量消耗之间的权衡关系，该框架基于微分几何，将智能体建模为分层流形，并分析了不同投影类型对记忆机制和能量成本的影响。


<details>
  <summary>Details</summary>
Motivation: 探究具身智能体为了记忆其过往行为需要消耗多少能量，并建立记忆与能量消耗之间的基本权衡关系。

Method: 在微分几何框架下，将智能体建模为包含物理空间（P）、认知空间（C）和意图空间（I）的分层流形，并定义了连接这些空间的投影（Φ: P -> C 和 Ψ: C -> I）。研究了投影的几何特性如何决定记忆机制和能量成本，重点分析了同胚映射（一对一投影）和纤维丛（多对一投影）的差异。

Result: 1. 证明了一对一投影（同胚）需要专门的动力学机制来实现记忆，而多对一投影（纤维丛）则能通过联络曲率实现内在的几何记忆。 2. 证明了任何偏离能量最小化的提升都会产生可量化的代价，表明依赖路径的行为必然消耗能量。 3. 提出了一个普适性原理：过度的代价（ΔE）与累积的几何变换（holonomy）的平方成正比。 4. 通过五个系统（条带-正弦系统、螺旋和扭曲纤维丛、平面/圆柱纤维丛）验证了这种代价-记忆对偶性。

Conclusion: 本文提出的切线动作空间（TAS）框架成功地将几何力学与具身认知领域联系起来，为理解生物运动多样性提供了新的视角，并为设计高效的机器人控制系统提供了原则。

Abstract: How much energy must an embodied agent spend to remember its past actions? We
present Tangential Action Spaces (TAS), a differential-geometric framework
revealing a fundamental trade-off between memory and energy in embodied agents.
By modeling agents as hierarchical manifolds with projections Phi: P -> C and
Psi: C -> I connecting physical (P), cognitive (C), and intentional (I) spaces,
we show that the geometry of Phi dictates both memory mechanisms and their
energetic costs. Our main contributions are: (1) a rigorous classification
proving that one-to-one projections (diffeomorphisms) require engineered
dynamics for memory while many-to-one projections (fibrations) enable intrinsic
geometric memory through connection curvature; (2) a proof that any deviation
from the energy-minimal lift incurs a quantifiable penalty, establishing that
path-dependent behavior necessarily costs energy; and (3) a universal principle
that excess cost Delta E scales with the square of accumulated holonomy
(geometric memory). We validate this cost-memory duality through five systems:
the strip-sine system (engineered memory, Delta E proportional to (Delta h)^2),
helical and twisted fibrations (intrinsic geometric memory), and
flat/cylindrical fibrations (proving curvature, not topology, creates memory).
This framework bridges geometric mechanics and embodied cognition, explaining
biological motor diversity and providing design principles for efficient
robotic control.

</details>


### [228] [Globally Asymptotically Stable Trajectory Tracking of Underactuated UAVs using Geometric Algebra](https://arxiv.org/abs/2509.03484)
*Ignacio Rubio Scola,Omar Alejandro Garcia Alcantara,Steven Sandoval,Eduardo Steed Espinoza Quesada,Hernan Haimovich,Luis Rodolfo Garcia Carrillo*

Main category: eess.SY

TL;DR: 本研究使用几何代数（GA）工具对三维空间中的物体动力学进行建模，旨在简化欠驱动系统的轨迹跟踪控制设计。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是利用几何代数（GA）的强大能力来简化和优化欠驱动系统在三维空间中的动力学建模和控制设计，特别是在轨迹跟踪任务中。

Method: 本文采用几何代数（GA）对三维空间中的物体动力学进行建模，并将其构建为级联系统，其中旋转子系统驱动平移子系统。旋转子系统是线性的，而平移子系统是线性加扰动的形式。提出了一种仅需要简单操作、无记忆、无迭代搜索循环的控制策略，并使用输入到状态稳定性方法严格证明了闭环稳定性。

Result: 通过数值模拟，验证了在有风环境下，四旋翼飞行器执行轨迹跟踪任务时的控制器稳定性和性能。

Conclusion: 本研究成功地展示了如何利用几何代数（GA）对物体动力学进行建模，并通过级联系统和简化的控制策略实现了欠驱动系统在三维空间中的轨迹跟踪。研究结果表明，GA提供了一种无奇点、几何上直观的表示方法，并能确保系统的闭环稳定性。

Abstract: This paper employs Geometric Algebra (GA) tools to model the dynamics of
objects in 3-dimensional space, serving as a proof of concept to facilitate
control design for trajectory tracking in underactuated systems. For control
purposes, the model is structured as a cascade system, where a rotational
subsystem drives a translational one. The rotational subsystem is linear, while
the translational subsystem follows a linear-plus-perturbation form, thereby
reducing the complexity of control design. A control strategy requiring only
simple operations, no memory, and no iterative search loops is presented to
illustrate the main features of the GA model. By employing GA to model both
translations and rotations, a singularity-free and geometrically intuitive
representation can be achieved through the use of the geometric product.
Closed-loop stability is rigorously established using input-to-state stability
methods. Numerical simulations of a quad tilt-rotorcraft performing trajectory
tracking in a windy environment validate the controller's stability and
performance.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [229] [On the Optimization of Methods for Establishing Well-Connected Communities](https://arxiv.org/abs/2509.02590)
*Mohammad Dindoost,Oliver Alvarado Rodriguez,Bartosz Bryg,Minhyuk Park,George Chacko,Tandy Warnow,David A. Bader*

Main category: cs.SI

TL;DR: 该研究提出了优化的WCC和CM并行算法，并将其集成到Arkouda/Arachne框架中，实现了对海量图（超过20亿条边）的高效连通性社区检测，解决了现有方法的计算瓶颈问题，并以Open-Alex数据集为例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有社区检测方法在处理断开或弱连接的聚类时存在可解释性和鲁棒性不足的问题，而WCC和CM算法虽然能提高精度，但计算成本过高，难以应用于大规模图。

Method: 设计并实现了利用HPE Chapel语言的并行构造的WCC和CM算法，并将其集成到Arkouda/Arachne大数据图分析框架中，优化了并行处理能力和可扩展性。

Result: 在128核上，使用优化的WCC和CM算法在几分钟内完成了对包含超过20亿条边的Open-Alex数据集的社区检测，实现了前所未有的计算效率和规模。

Conclusion: 研究提出的优化的WCC和CM并行实现能够高效处理万亿级别的大规模图，为实现大规模、保持连通性的社区检测提供了实用的解决方案。

Abstract: Community detection plays a central role in uncovering meso scale structures
in networks. However, existing methods often suffer from disconnected or weakly
connected clusters, undermining interpretability and robustness. Well-Connected
Clusters (WCC) and Connectivity Modifier (CM) algorithms are post-processing
techniques that improve the accuracy of many clustering methods. However, they
are computationally prohibitive on massive graphs. In this work, we present
optimized parallel implementations of WCC and CM using the HPE Chapel
programming language. First, we design fast and efficient parallel algorithms
that leverage Chapel's parallel constructs to achieve substantial performance
improvements and scalability on modern multicore architectures. Second, we
integrate this software into Arkouda/Arachne, an open-source, high-performance
framework for large-scale graph analytics. Our implementations uniquely enable
well-connected community detection on massive graphs with more than 2 billion
edges, providing a practical solution for connectivity-preserving clustering at
web scale. For example, our implementations of WCC and CM enable community
detection of the over 2-billion edge Open-Alex dataset in minutes using 128
cores, a result infeasible to compute previously.

</details>


### [230] [Contrastive clustering based on regular equivalence for influential node identification in complex networks](https://arxiv.org/abs/2509.02609)
*Yanmei Hu,Yihang Wu,Bing Sun,Xue Yue,Biao Cai,Xiangtao Li,Yang Chen*

Main category: cs.SI

TL;DR: ReCC是一种新的深度无监督框架，用于识别有影响力的节点，它通过利用基于规则的等价性来克服现有方法的局限性，并在多个基准上进行了广泛的实验，证明其优于最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的监督学习方法在标记数据方面存在局限性，而现有的对比学习方法在构建正负样本对方面存在局限性。

Method: ReCC将有影响力的节点识别重新构建为无标签的深度聚类问题，并开发了一种利用基于规则的等价性的对比学习机制来生成正负样本对，然后将其集成到图卷积网络中进行节点嵌入学习。

Result: ReCC通过结合结构指标和基于规则的等价性来增强节点表示，并且在多个基准测试中表现优于最先进的方法。

Conclusion: ReCC是一种新颖的深度无监督框架，在识别有影响力的节点方面取得了最先进的成果，并且具有广泛的应用前景。

Abstract: Identifying influential nodes in complex networks is a fundamental task in
network analysis with wide-ranging applications across domains. While deep
learning has advanced node influence detection, existing supervised approaches
remain constrained by their reliance on labeled data, limiting their
applicability in real-world scenarios where labels are scarce or unavailable.
While contrastive learning demonstrates significant potential for performance
enhancement, existing approaches predominantly rely on multiple-embedding
generation to construct positive/negative sample pairs. To overcome these
limitations, we propose ReCC (\textit{r}egular \textit{e}quivalence-based
\textit{c}ontrastive \textit{c}lustering), a novel deep unsupervised framework
for influential node identification. We first reformalize influential node
identification as a label-free deep clustering problem, then develop a
contrastive learning mechanism that leverages regular equivalence-based
similarity, which captures structural similarities between nodes beyond local
neighborhoods, to generate positive and negative samples. This mechanism is
integrated into a graph convolutional network to learn node embeddings that are
used to differentiate influential from non-influential nodes. ReCC is
pre-trained using network reconstruction loss and fine-tuned with a combined
contrastive and clustering loss, with both phases being independent of labeled
data. Additionally, ReCC enhances node representations by combining structural
metrics with regular equivalence-based similarities. Extensive experiments
demonstrate that ReCC outperforms state-of-the-art approaches across several
benchmarks.

</details>


### [231] [Synthetic generation of online social networks through homophily](https://arxiv.org/abs/2509.02762)
*Alejandro Buitrago López,Javier Pastor-Galindo,José A. Ruipérez-Valiente*

Main category: cs.SI

TL;DR: 本研究提出了一种基于同质性算法的合成社交网络生成器，用于生成具有真实属性和语义的微博客社交网络（如X和Bluesky）。


<details>
  <summary>Details</summary>
Motivation: 现有的合成社交网络生成器在数据访问受限的情况下，往往只关注拓扑结构而忽略了属性驱动的同质性和语义真实性。

Method: 该模型通过整合用户属性间的语义亲和力、链接形成中的随机性、促进聚类的三元闭合以及确保全局可达性的长连接，来生成合成社交网络。研究中使用了系统网格搜索来校准五个超参数（亲和力强度、噪声、闭合概率、远距离链接概率和候选池大小），以达到五个在真实社交网络中观察到的结构值（密度、聚类系数、LCC比例、归一化最短路径和模块性）。

Result: 通过在四种不同规模（10^3-10^6个节点）下生成合成OSN，并与一个包含400万用户的真实Bluesky网络进行基准测试，结果表明该框架能够可靠地重现真实网络的结构属性，并且优于同类方法。生成的图不仅具有拓扑真实性，还能产生符合社会学预期的、受属性驱动的社区。

Conclusion: 该框架能够生成逼真、可扩展的合成社交网络，为社会学研究者提供了替代真实平台的数据集，解决了真实社交网络数据访问受限的问题。

Abstract: Online social networks (OSNs) have become increasingly relevant for studying
social behavior and information diffusion. Nevertheless, they are limited by
restricted access to real OSN data due to privacy, legal, and platform-related
constraints. In response, synthetic social networks serve as a viable approach
to support controlled experimentation, but current generators reproduce only
topology and overlook attribute-driven homophily and semantic realism.
  This work proposes a homophily-based algorithm that produces synthetic
microblogging social networks such as X. The model creates a social graph for a
given number of users, integrating semantic affinity among user attributes,
stochastic variation in link formation, triadic closure to foster clustering,
and long-range connections to ensure global reachability. A systematic grid
search is used to calibrate five hyperparameters (affinity strength, noise,
closure probability, distant link probability, and candidate pool size) for
reaching five structural values observed in real social networks (density,
clustering coefficient, LCC proportion, normalized shortest path, and
modularity).
  The framework is validated by generating synthetic OSNs at four scales
(10^3-10^6 nodes), and benchmarking them against a real-world Bluesky network
comprising 4 million users. Comparative results show that the framework
reliably reproduces the structural properties of the real network. Overall, the
framework outperforms leading importance-sampling techniques applied to the
same baseline. The generated graphs capture topological realism and yield
attribute-driven communities that align with sociological expectations,
providing a realistic, scalable testbed that liberates social researchers from
relying on live digital platforms.

</details>


### [232] [Predicting Movie Success with Multi-Task Learning: A Hybrid Framework Combining GPT-Based Sentiment Analysis and SIR Propagation](https://arxiv.org/abs/2509.02809)
*Wenlan Xie*

Main category: cs.SI

TL;DR: 本研究提出了一个整合了多任务学习、GPT 驱动的情感分析和 SIR 传播模型的混合框架，用于预测电影的商业成功。


<details>
  <summary>Details</summary>
Motivation: 现有电影成功预测方法存在局限性，本研究旨在同时考虑静态制作属性、信息传播和观众情感，以提高预测准确性。

Method: 采用混合框架，整合多任务学习、GPT 驱动的情感分析和 SIR 传播模型，利用 5,840 部电影和约 300,000 条用户评论进行训练和测试。

Result: 在电影成功预测任务上取得了 0.964 的分类准确率和 0.388 的平均绝对误差 (MAE)。消融分析表明，部分特征组合的性能优于整个模型，这挑战了特征整合的固有假设。模型还揭示了成功与不成功的电影之间的传播模式。

Conclusion: 本研究提出的混合框架在电影成功预测方面表现出优越性能，并为理解观众参与如何影响商业成果提供了新的见解，其创新包括引入流行病学模型、多维度情感特征和共享表示架构。

Abstract: This study presents a hybrid framework for predicting movie success. The
framework integrates multi-task learning (MTL), GPT-based sentiment analysis,
and Susceptible-Infected-Recovered (SIR) propagation modeling. The study
examines limitations in existing approaches. It models static production
attributes, information dissemination, and audience sentiment at the same time.
The framework uses 5,840 films from 2004 to 2024 and approximate 300,000 user
reviews. It shows predictive performance with classification accuracy of 0.964
and regression metrics of MAE 0.388. Ablation analysis indicates component
interactions. Selective feature combinations perform better than the
comprehensive model. This result questions assumptions about feature
integration. The model shows virality patterns between successful and
unsuccessful films. Innovations include epidemiological modeling for
information diffusion, multidimensional sentiment features from GPT-based
analysis, and a shared representation architecture that optimizes multiple
success metrics. The framework provides applications in the film production
lifecycle. It also contributes to understanding how audience engagement leads
to commercial outcomes.

</details>


### [233] [Temporal social network modeling of mobile connectivity data with graph neural networks](https://arxiv.org/abs/2509.03319)
*Joel Jaskari,Chandreyee Roy,Fumiko Ogushi,Mikko Saukkoriipi,Jaakko Sahlsten,Kimmo Kaski*

Main category: cs.SI

TL;DR: GNNs在分析社交网络方面有潜力，但需要进一步研究。


<details>
  <summary>Details</summary>
Motivation: 研究使用时间序列的移动连接数据来分析社交网络。

Method: 使用四种基于快照的时间GNN模型和EdgeBank基线模型来预测用户间的通话和短信活动。

Result: ROLAND时间GNN在大多数情况下优于基线模型，但其他GNN模型表现不如基线模型。GNN方法在通过移动连接数据分析时间社交网络方面显示出潜力，但ROLAND和基线模型之间的性能差距很小，需要对专门用于时间社交网络分析的GNN架构进行进一步研究。

Conclusion: GNN方法在分析时间社交网络方面显示出潜力，但需要对专门用于时间社交网络分析的GNN架构进行进一步研究。

Abstract: Graph neural networks (GNNs) have emerged as a state-of-the-art data-driven
tool for modeling connectivity data of graph-structured complex networks and
integrating information of their nodes and edges in space and time. However, as
of yet, the analysis of social networks using the time series of people's
mobile connectivity data has not been extensively investigated. In the present
study, we investigate four snapshot - based temporal GNNs in predicting the
phone call and SMS activity between users of a mobile communication network. In
addition, we develop a simple non - GNN baseline model using recently proposed
EdgeBank method. Our analysis shows that the ROLAND temporal GNN outperforms
the baseline model in most cases, whereas the other three GNNs perform on
average worse than the baseline. The results show that GNN based approaches
hold promise in the analysis of temporal social networks through mobile
connectivity data. However, due to the relatively small performance margin
between ROLAND and the baseline model, further research is required on
specialized GNN architectures for temporal social network analysis.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [234] [EEG-MSAF: An Interpretable Microstate Framework uncovers Default-Mode Decoherence in Early Neurodegeneration](https://arxiv.org/abs/2509.02568)
*Mohammad Mehedi Hasan,Pedro G. Lind,Hernando Ombao,Anis Yazidi,Rabindra Khadka*

Main category: eess.SP

TL;DR: 该研究提出了EEG-MSAF框架，利用EEG微状态分析来诊断痴呆症（DEM）及其早期阶段轻度认知障碍（MCI），在两个数据集上均取得了高准确率，并识别出关键的生物标志物，特别是微状态C和F在诊断中的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着全球痴呆症（DEM）发病率的增长，早期准确诊断的需求日益迫切。现有的脑电图（EEG）分析方法难以捕捉其瞬态复杂性。

Method: 研究提出了一个名为EEG Microstate Analysis Framework (EEG-MSAF) 的端到端流程，包括自动微状态特征提取、机器学习分类以及使用Shapley Additive Explanations (SHAP) 进行特征排名，以识别DEM相关的生物标志物并区分DEM、轻度认知障碍（MCI）和正常认知（NC）。

Result: 在CAUEEG数据集上，EEG-MSAF-SVM达到了89% ± 0.01的准确率，优于CEEDNET 19.3%。在Thessaloniki数据集上，准确率达到95% ± 0.01。SHAP分析显示，微状态C（显著性/注意力网络）的平均相关性和出现频率对DEM预测最为关键，而微状态F（一种新的默认模式网络模式）是MCI和DEM的关键早期生物标志物。

Conclusion: EEG-MSAF框架在提高EEG诊断的准确性、泛化性和可解释性方面取得了显著进展，为痴呆症的诊断提供了新的方法，并揭示了跨认知谱系的大脑动态变化。

Abstract: Dementia (DEM) is a growing global health challenge, underscoring the need
for early and accurate diagnosis. Electroencephalography (EEG) provides a
non-invasive window into brain activity, but conventional methods struggle to
capture its transient complexity. We present the \textbf{EEG Microstate
Analysis Framework (EEG-MSAF)}, an end-to-end pipeline that leverages EEG
microstates discrete, quasi-stable topographies to identify DEM-related
biomarkers and distinguish DEM, mild cognitive impairment (MCI), and normal
cognition (NC). EEG-MSAF comprises three stages: (1) automated microstate
feature extraction, (2) classification with machine learning (ML), and (3)
feature ranking using Shapley Additive Explanations (SHAP) to highlight key
biomarkers. We evaluate on two EEG datasets: the public Chung-Ang University
EEG (CAUEEG) dataset and a clinical cohort from Thessaloniki Hospital. Our
framework demonstrates strong performance and generalizability. On CAUEEG,
EEG-MSAF-SVM achieves \textbf{89\% $\pm$ 0.01 accuracy}, surpassing the deep
learning baseline CEEDNET by \textbf{19.3\%}. On the Thessaloniki dataset, it
reaches \textbf{95\% $\pm$ 0.01 accuracy}, comparable to EEGConvNeXt. SHAP
analysis identifies mean correlation and occurrence as the most informative
metrics: disruption of microstate C (salience/attention network) dominates DEM
prediction, while microstate F, a novel default-mode pattern, emerges as a key
early biomarker for both MCI and DEM. By combining accuracy, generalizability,
and interpretability, EEG-MSAF advances EEG-based dementia diagnosis and sheds
light on brain dynamics across the cognitive spectrum.

</details>


### [235] [Recall Gabor Communication Theory and Joint Time-Frequency Analysis](https://arxiv.org/abs/2509.02724)
*Xiang-Gen Xia*

Main category: eess.SP

TL;DR: Gabor变换及其在时频分析中的应用。


<details>
  <summary>Details</summary>
Motivation: 介绍Gabor变换的理论背景和应用。

Method: 回顾Gabor通信理论、Gabor变换和展开，以及其与联合时频分析的联系。

Result: 对Gabor变换及其在时频分析中的作用进行了阐述。

Conclusion: Gabor变换是时频分析的重要工具。

Abstract: In this article, we first briefly recall Gabor's communication theory and
then Gabor transform and expansion, and also its connection with joint time
frequency analysis.

</details>


### [236] [minPIC: Towards Optimal Power Allocation in Multi-User Interference Channels](https://arxiv.org/abs/2509.02797)
*Sagnik Bhattacharya,Abhiram Rao Gorle,John M. Cioffi*

Main category: eess.SP

TL;DR: 6G 设想了无中心化协调的海量无蜂窝网络，其中功率、子载波和解码顺序的优化资源分配对于干扰信道 (IC) 至关重要。本研究提出了一个新颖的 minPIC 框架，用于在一般的多用户 IC 中进行最优功率、子载波和解码顺序分配。minPIC 消除了启发式的 SIC（串行干扰消除）顺序假设，并通过引入双变量引导排序标准来识别全局最优 SIC 顺序，然后进行凸优化和二分搜索。该方法首次实现了高斯 IC 的 SIC 可实现速率区域的帕累托边界，为无蜂窝网络中的可扩展干扰管理开辟了道路。


<details>
  <summary>Details</summary>
Motivation: 当前的 OMA、NOMA 和 RSMA 等方法在干扰管理方面依赖于固定的启发式方法，导致速率次优、功率效率低下和可扩展性问题。因此，需要一种新的资源分配方法来优化功率、子载波和解码顺序，以满足 6G 对高比特率和低功耗的严格要求。

Method: 提出了一种新颖的 minPIC 框架，用于在一般的多用户 IC 中进行最优功率、子载波和解码顺序分配。该框架消除了启发式的 SIC 顺序假设，通过引入双变量引导排序标准来识别全局最优 SIC 顺序，然后使用辅助 log-det 约束进行凸优化，并通过二分搜索进行求解。

Result: minPIC 框架能够实现高斯 IC 的 SIC 可实现速率区域的帕累托边界，这在以前是未知的。该方法有可能满足沉浸式 XR 和其他 6G 应用对高比特率和低功耗的严格要求。

Conclusion: minPIC 是第一个算法实现高斯 IC 的 SIC 可实现速率区域的帕累托边界，为无蜂窝网络中的可扩展干扰管理开辟了道路。该框架通过消除启发式 SIC 顺序假设并引入有效的优化技术，有望满足 6G 应用的性能要求。

Abstract: 6G envisions massive cell-free networks with spatially nested multiple access
(MAC) and broadcast (BC) channels without centralized coordination. This makes
optimal resource allocation across power, subcarriers, and decoding orders
crucial for interference channels (ICs), where neither transmitters nor
receivers can cooperate. Current orthogonal multiple access (OMA) methods, as
well as non-orthogonal (NOMA) and rate-splitting (RSMA) schemes, rely on fixed
heuristics for interference management, leading to suboptimal rates, power
inefficiency, and scalability issues. This paper proposes a novel minPIC
framework for optimal power, subcarrier, and decoding order allocation in
general multi-user ICs. Unlike existing methods, minPIC eliminates heuristic
SIC order assumptions. Despite the convexity of the IC capacity region, fixing
an SIC order induces non-convexity in resource allocation, traditionally
requiring heuristic approximations. We instead introduce a dual-variable-guided
sorting criterion to identify globally optimal SIC orders, followed by convex
optimization with auxiliary log-det constraints, efficiently solved via binary
search. We also demonstrate that minPIC could potentially meet the stringent
high-rate, low-power targets of immersive XR and other 6G applications. To the
best of our knowledge, minPIC is the first algorithmic realisation of the
Pareto boundary of the SIC-achievable rate region for Gaussian ICs, opening the
door to scalable interference management in cell-free networks.

</details>


### [237] [Protecting Legacy Wireless Systems Against Interference: Precoding and Codebook Approaches Using Massive MIMO and Region Constraints](https://arxiv.org/abs/2509.02819)
*Sameer Mathad,Taejoon Kim,David J. Love*

Main category: eess.SP

TL;DR: 该论文提出了一种基于通信理论的方法，通过在大规模MIMO系统设计中加入接收功率约束（区域约束），以保护地理限制区域内的传统用户免受新频段使用的干扰。


<details>
  <summary>Details</summary>
Motivation: 由于对高速无线通信的需求不断增长，需要利用邻近传统无线系统的频段。然而，这会干扰到关键基础设施网络中使用的传统用户，因此迫切需要开发保护这些用户的方案。

Method: 提出了一种通信理论方法，通过在传统的地理排除区方法之外，将接收功率约束（区域约束）纳入大规模MIMO系统设计中。对单用户和多用户大规模MIMO系统进行了容量和和速率分析，并提出了一种预编码设计方法。

Result: 进行了单用户和多用户大规模MIMO系统的容量和和速率分析，并提出了一种能够保护传统用户的预编码设计方法，从而实现新频段的利用。

Conclusion: 通过在MIMO系统设计中结合区域约束，可以有效保护地理限制区域内的传统用户，实现新旧无线系统的和谐共存。

Abstract: The ever-increasing demand for high-speed wireless communication has
generated significant interest in utilizing frequency bands that are adjacent
to those occupied by legacy wireless systems. Since the legacy wireless systems
were designed based on often decades-old assumptions about wireless
interference, utilizing these new bands will result in interference with the
existing legacy users. Many of these legacy wireless devices are used by
critical infrastructure networks upon which society depends. There is an urgent
need to develop schemes that can protect legacy users from such interference.
For many applications, legacy users are located within
geographically-constrained regions. Several studies have proposed mitigating
interference through the implementation of exclusion zones near these
geographically-constrained regions. In contrast to solutions based on
geographic exclusion zones, this paper presents a communication theory-based
solution. By leveraging knowledge of these geographically-constrained regions,
we aim to reduce the interference impact on legacy users. We achieve this by
incorporating received power constraints, termed as region constraints, in our
massive multiple-input multiple-output (MIMO) system design. We perform a
capacity analysis of single-user massive MIMO and a sum-rate analysis of the
multi-user massive MIMO system with transmit power and region constraints. We
present a precoding design method that allows for the utilization of new
frequency bands while protecting legacy users.

</details>


### [238] [Spatially Adaptive SWIPT with Pinching Antenna under Probabilistic LoS Blockage](https://arxiv.org/abs/2509.03038)
*Ruihong Jiang,Ruichen Zhang,Yanqing Xu,Huimin Hu,Yang Lu,Dusit Niyato*

Main category: eess.SP

TL;DR: 通过优化天线位置和功率分配比，在能量收集需求和天线放置限制下，最大化用户信噪比。


<details>
  <summary>Details</summary>
Motivation: 研究一种基于重构夹式天线（PA）和功率分配（PS）的同步无线信息和能量传输（SWIPT）系统，该系统在存在视线（LoS）遮挡的概率下运行，以最大化用户端的平均信噪比（SNR），同时满足平均能量收集（EH）和PA放置的限制。

Method: 推导了一个封闭形式的最优解，用于联合优化PA的位置和PS比。

Result: EH需求对最优PA位置及其可行区域有确定性影响，需要将PA尽可能靠近用户以最大化平均信道增益。这种空间适应性与动态PS相结合，能够应对概率性的LoS遮挡，实现稳健的SWIPT性能。

Conclusion: 机械可重构性主要通过确保动态环境下的能量可行性来提高可持续性。

Abstract: This paper considers a power-splitting (PS)-based simultaneous wireless
information and power transfer (SWIPT) system employing a reconfigurable
pinching antenna (PA) under probabilistic line-of-sight (LoS) blockage. We
formulate a joint optimization of the PA position and the PS ratio to maximize
the average signal-to-noise ratio (SNR) at a user, subject to its average
energy harvesting (EH) and PA placement limits. We derive a closed-form optimal
solution. Results demonstrate that the EH requirement has a deterministic
impact on the optimal PA position as well as its feasible region, requiring
deployment of the PA as close to the user as possible to maximize average
channel gain. This spatial adaptation, combined with dynamic PS, enables robust
SWIPT performance in the presence of probabilistic LoS blockage, revealing that
mechanical reconfigurability primarily enhances sustainability by ensuring
energy feasibility in dynamic environments.

</details>


### [239] [S2M2ECG: Spatio-temporal bi-directional State Space Model Enabled Multi-branch Mamba for ECG](https://arxiv.org/abs/2509.03066)
*Huaicheng Zhang,Ruoxin Wang,Chenlian Zhou,Jiguang Shi,Yue Ge,Zhoutong Li,Sheng Chang,Hao Wang,Jin He,Qijun Huang*

Main category: eess.SP

TL;DR: S2M2ECG是一种基于状态空间模型（SSM）的新型架构，通过三级融合机制有效处理多导联心电图（ECG）信号，实现了在心律、形态和临床场景下的卓越性能，同时保持了轻量级和高效率的特点。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在处理多导联心电图（ECG）信号时，难以在性能、计算复杂度和多源特征融合之间取得平衡。

Method: 提出了一种名为S2M2ECG的SSM架构，该架构包含三级融合机制：(1)低层信号融合的Spatio-temporal双向SSM与分段标记化；(2)增强识别精度的单导联内时序信息融合（双向扫描）；(3)空间信息融合的跨导联特征交互模块。此外，还采用了多分支设计和导联融合模块，以充分利用ECG信号特有的跨导联机制。

Result: S2M2ECG在节奏、形态和临床场景下均取得了优于现有模型的性能，并且其轻量级架构拥有极少的参数量，适合高效推理和部署。

Conclusion: S2M2ECG在性能、计算复杂度和ECG信号特性之间取得了良好的平衡，为心血管疾病诊断提供了高性能、轻量级的计算新选择。

Abstract: As one of the most effective methods for cardiovascular disease (CVD)
diagnosis, multi-lead Electrocardiogram (ECG) signals present a characteristic
multi-sensor information fusion challenge that has been continuously researched
in deep learning domains. Despite the numerous algorithms proposed with
different DL architectures, maintaining a balance among performance,
computational complexity, and multi-source ECG feature fusion remains
challenging. Recently, state space models (SSMs), particularly Mamba, have
demonstrated remarkable effectiveness across various fields. Their inherent
design for high-efficiency computation and linear complexity makes them
particularly suitable for low-dimensional data like ECGs. This work proposes
S2M2ECG, an SSM architecture featuring three-level fusion mechanisms: (1)
Spatio-temporal bi-directional SSMs with segment tokenization for low-level
signal fusion, (2) Intra-lead temporal information fusion with bi-directional
scanning to enhance recognition accuracy in both forward and backward
directions, (3) Cross-lead feature interaction modules for spatial information
fusion. To fully leverage the ECG-specific multi-lead mechanisms inherent in
ECG signals, a multi-branch design and lead fusion modules are incorporated,
enabling individual analysis of each lead while ensuring seamless integration
with others. Experimental results reveal that S2M2ECG achieves superior
performance in the rhythmic, morphological, and clinical scenarios. Moreover,
its lightweight architecture ensures it has nearly the fewest parameters among
existing models, making it highly suitable for efficient inference and
convenient deployment. Collectively, S2M2ECG offers a promising alternative
that strikes an excellent balance among performance, computational complexity,
and ECG-specific characteristics, paving the way for high-performance,
lightweight computations in CVD diagnosis.

</details>


### [240] [YOLO-based Bearing Fault Diagnosis With Continuous Wavelet Transform](https://arxiv.org/abs/2509.03070)
*Po-Heng Chou,Wei-Lung Mao,Ru-Ping Lin*

Main category: eess.SP

TL;DR: 本研究提出一种基于YOLO的目标检测框架，利用连续小波变换（CWT）生成时频图谱，用于空间轴承故障诊断。


<details>
  <summary>Details</summary>
Motivation: 为了提高轴承故障诊断的准确性和泛化能力，提出一种新的基于YOLO的目标检测框架，利用时频图谱来捕捉瞬态故障特征。

Method: 首先，使用Morlet小波将一维振动信号转换为时频图谱；然后，利用YOLOv9、v10和v11模型对图谱进行故障类型分类；最后，在CWRU、PU和IMS三个数据集上进行评估。

Result: 与基线MCNN--LSTM模型相比，CWT--YOLO管道在准确性和泛化能力方面表现出显著优势。其中，YOLOv11在CWRU、PU和IMS数据集上分别达到了99.4%、97.8%和99.5%的mAP分数。该模型还能通过区域感知检测机制直接可视化故障在图谱中的位置。

Conclusion: 本研究提出的CWT--YOLO框架能够有效地诊断轴承故障，具有高精度和良好的泛化能力，并且能够直观地显示故障位置，为旋转机械的设备监控提供了一种实用的解决方案。

Abstract: This letter proposes a YOLO-based framework for spatial bearing fault
diagnosis using time-frequency spectrograms derived from continuous wavelet
transform (CWT). One-dimensional vibration signals are first transformed into
time-frequency spectrograms using Morlet wavelets to capture transient fault
signatures. These spectrograms are then processed by YOLOv9, v10, and v11
models to classify fault types. Evaluated on three benchmark datasets,
including Case Western Reserve University (CWRU), Paderborn University (PU),
and Intelligent Maintenance System (IMS), the proposed CWT--YOLO pipeline
achieves significantly higher accuracy and generalizability than the baseline
MCNN--LSTM model. Notably, YOLOv11 reaches mAP scores of 99.4% (CWRU), 97.8%
(PU), and 99.5% (IMS). In addition, its region-aware detection mechanism
enables direct visualization of fault locations in spectrograms, offering a
practical solution for condition monitoring in rotating machinery.

</details>


### [241] [Self-supervised Radio Representation Learning: Can we Learn Multiple Tasks?](https://arxiv.org/abs/2509.03077)
*Ogechukwu Kanu,Ashkan Eshaghbeigi,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: 该论文提出了一种用于无线信号表示学习的自监督学习（SSL）方案，利用动量对比（momentum contrast）从大量未标记数据中提取鲁棒且可迁移的表示，以应对6G无线通信中AI模型训练对标记数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 在6G时代，AI在无线通信中扮演重要角色，但监督学习方法需要大量标记数据，成本高昂。自监督学习（SSL）利用未标记数据能达到接近监督学习的效果，是解决此挑战的关键。

Method: 提出了一种基于动量对比（momentum contrast）的自监督学习（SSL）方案，用于无线信号表示学习。通过对比学习，从大规模真实世界数据集中提取鲁棒、可迁移的表示。

Result: 所提SSL方案学习到的表示在角度 of arrival（AoA）估计和自动调制分类（AMC）两项无线通信任务上表现出良好的泛化能力。精心设计的增强方法和多样化的数据使得对比学习能够产生高质量、不变性的潜在表示。即使在编码器权重冻结的情况下，这些表示也十分有效，并且进一步的微调还能提升性能，超越了监督学习基线。

Conclusion: 该论文首次提出并证明了自监督学习在多种无线信号任务上的有效性。研究结果表明，SSL能够减少对标记数据的依赖，并提高模型的泛化能力，为构建可扩展的6G无线通信AI基础模型和解决方案提供了可能。

Abstract: Artificial intelligence (AI) is anticipated to play a pivotal role in 6G.
However, a key challenge in developing AI-powered solutions is the extensive
data collection and labeling efforts required to train supervised deep learning
models. To overcome this, self-supervised learning (SSL) approaches have
recently demonstrated remarkable success across various domains by leveraging
large volumes of unlabeled data to achieve near-supervised performance. In this
paper, we propose an effective SSL scheme for radio signal representation
learning using momentum contrast. By applying contrastive learning, our method
extracts robust, transferable representations from a large real-world dataset.
We assess the generalizability of these learned representations across two
wireless communications tasks: angle of arrival (AoA) estimation and automatic
modulation classification (AMC). Our results show that carefully designed
augmentations and diverse data enable contrastive learning to produce
high-quality, invariant latent representations. These representations are
effective even with frozen encoder weights, and fine-tuning further enhances
performance, surpassing supervised baselines. To the best of our knowledge,
this is the first work to propose and demonstrate the effectiveness of
self-supervised learning for radio signals across multiple tasks. Our findings
highlight the potential of self-supervised learning to transform AI for
wireless communications by reducing dependence on labeled data and improving
model generalization - paving the way for scalable foundational 6G AI models
and solutions.

</details>


### [242] [Handwriting Imagery EEG Classification based on Convolutional Neural Networks](https://arxiv.org/abs/2509.03111)
*Hao Yang,Guang Ouyang*

Main category: eess.SP

TL;DR: 本研究探索了使用非侵入性脑电图（EEG）解码与手写想象相关的脑活动以生成英文字母的潜力。


<details>
  <summary>Details</summary>
Motivation: 手写想象作为一种有前景的脑机接口（BCI）范式，旨在将大脑活动转化为文本。非侵入性EEG记录相比侵入性方法更具实用性和可行性。

Method: 研究收集了五名参与者在想象书写26个英文字母时记录的EEG数据，并进行了EEG相似性测量以研究字母特异性模式。随后，训练了四个卷积神经网络（CNN）模型进行EEG分类。

Result: EEG数据显示出明显的字母特异性模式，证明了EEG到文本翻译的概念可行性。CNN分类器在单个参与者上的最高准确率达到了约20%，远高于3.85%的偶然水平。

Conclusion: 尽管目前20%的准确率不足以支持实用的脑到文本BCI，但该研究首次尝试解码与手写想象相关的非侵入性EEG信号，揭示了其转化为文本输出的潜力，并为未来的研究奠定了基础。

Abstract: Handwriting imagery has emerged as a promising paradigm for brain-computer
interfaces (BCIs) aimed at translating brain activity into text output.
Compared with invasively recorded electroencephalography (EEG), non-invasive
recording offers a more practical and feasible approach to capturing brain
signals for BCI. This study explores the limit of decoding non-invasive EEG
associated with handwriting imagery into English letters using deep neural
networks. To this end, five participants were instructed to imagine writing the
26 English letters with their EEG being recorded from the scalp. A measurement
of EEG similarity across letters was conducted to investigate letter-specific
patterns in the dataset. Subsequently, four convolutional neural network (CNN)
models were trained for EEG classification. Descriptively, the EEG data clearly
exhibited letter-specific patterns serving as a proof-of-concept for
EEG-to-text translation. Under the chance level of accuracy at 3.85%, the CNN
classifiers trained on each participant reached the highest limit of around
20%. This study marks the first attempt to decode non-invasive EEG associated
with handwriting imagery. Although the achieved accuracy is not sufficient for
a usable brain-to-text BCI, the model's performance is noteworthy in revealing
the potential for translating non-invasively recorded brain signals into text
outputs and establishing a baseline for future research.

</details>


### [243] [Deep Learning for High Speed Optical Coherence Elastography with a Fiber Scanning Endoscope](https://arxiv.org/abs/2509.03193)
*Maximilian Neidhardt,Sarah Latus,Tim Eixmann,Gereon Hüttmann,Alexander Schlaefer*

Main category: eess.SP

TL;DR: 提出了一种用于快速、局部弹性成像的微型纤维扫描内窥镜，并结合深度学习信号处理流程以实现实时弹性估计。


<details>
  <summary>Details</summary>
Motivation: 传统的基于图像的组织硬度评估方法不适用于微创手术等介入性操作，需要一种适用于术中操作的评估方法。

Method: 提出了一种微型纤维扫描内窥镜，用于获取复杂、多方向、多频率的弹性波场图像。设计了锥形扫描模式以提高时间采样率并保留三维信息。采用时空深度学习网络进行图像处理，以实现对复杂波场图像的有效处理。

Result: 在2D扫描中，该方法将平均绝对误差从传统方法的11.33±12.78 kPa降低到6.31±5.76 kPa。在不估计波方向的情况下，3D方法的误差从传统2D方法的19.75±21.82 kPa降低到4.48±3.63 kPa。该技术已在离体猪组织中验证了可行性。

Conclusion: 所提出的基于微型纤维扫描内窥镜和深度学习的弹性成像方法能够实现对软组织病变的快速、局部和实时弹性评估，为术中评估提供了新的可能。

Abstract: Tissue stiffness is related to soft tissue pathologies and can be assessed
through palpation or via clinical imaging systems, e.g., ultrasound or magnetic
resonance imaging. Typically, the image based approaches are not suitable
during interventions, particularly for minimally invasive surgery. To this end,
we present a miniaturized fiber scanning endoscope for fast and localized
elastography. Moreover, we propose a deep learning based signal processing
pipeline to account for the intricate data and the need for real-time
estimates. Our elasticity estimation approach is based on imaging complex and
diffuse wave fields that encompass multiple wave frequencies and propagate in
various directions. We optimize the probe design to enable different scan
patterns. To maximize temporal sampling while maintaining three-dimensional
information we define a scan pattern in a conical shape with a temporal
frequency of 5.05 kHz. To efficiently process the image sequences of complex
wave fields we consider a spatio-temporal deep learning network. We train the
network in an end-to-end fashion on measurements from phantoms representing
multiple elasticities. The network is used to obtain localized and robust
elasticity estimates, allowing to create elasticity maps in real-time. For 2D
scanning, our approach results in a mean absolute error of 6.31+-5.76 kPa
compared to 11.33+-12.78 kPa for conventional phase tracking. For scanning
without estimating the wave direction, the novel 3D method reduces the error to
4.48+-3.63 kPa compared to 19.75+-21.82 kPa for the conventional 2D method.
Finally, we demonstrate feasibility of elasticity estimates in ex-vivo porcine
tissue.

</details>


### [244] [Crosstalk-Resilient Beamforming for Movable Antenna Enabled Integrated Sensing and Communication](https://arxiv.org/abs/2509.03273)
*Zeyuan Zhang,Yue Xiu,Zheng Dong,Jiacheng Yin,Maurice J. Khabbaz,Chadi Assi,Ning Wei*

Main category: eess.SP

TL;DR: 本文提出了一种在考虑天线串扰的集成传感与通信（ISAC）系统中，利用可移动天线（MA）进行联合波束成形和天线位置设计的深度强化学习（DRL）方法，并通过TD3算法优化，以最小化Cramer-Rao界（CRB），最终证明了该方法在提高ISAC性能方面优于基准方案。


<details>
  <summary>Details</summary>
Motivation: 天线串扰在集成传感与通信（ISAC）系统中会影响系统性能，尤其是在可移动天线（MA）场景下，需要考虑天线串扰模型并优化系统设计。

Method: 提出了一种将天线串扰模型推广到MA场景的方法，并在此基础上，通过联合波束成形和天线位置设计来最小化Cramer-Rao界（CRB）。针对该非凸问题，采用了深度强化学习（DRL）方法，具体为Twin Delayed Deep Deterministic Policy Gradient（TD3）算法，以实现高效可靠的学习。

Result: 数值结果表明，所提出的抗串扰（CR）算法在ISAC性能上优于其他基准方案。

Conclusion: 所提出的基于DRL的抗串扰（CR）算法能够有效应对MA-ISAC系统中的天线串扰问题，通过联合优化波束成形和天线位置，显著提升了ISAC的整体性能。

Abstract: This paper investigates a movable antenna (MA) enabled integrated sensing and
communication (ISAC) system under the influence of antenna crosstalk. First, it
generalizes the antenna crosstalk model from the conventional fixed-position
antenna (FPA) system to the MA scenario. Then, a Cramer-Rao bound (CRB)
minimization problem driven by joint beamforming and antenna position design is
presented. Specifically, to address this highly non-convex flexible beamforming
problem, we deploy a deep reinforcement learning (DRL) approach to train a
flexible beamforming agent. To ensure stability during training, a Twin Delayed
Deep Deterministic Policy Gradient (TD3) algorithm is adopted to balance
exploration with reward maximization for efficient and reliable learning.
Numerical results demonstrate that the proposed crosstalk-resilient (CR)
algorithm enhances the overall ISAC performance compared to other benchmark
schemes.

</details>


### [245] [Credible Uncertainty Quantification under Noise and System Model Mismatch](https://arxiv.org/abs/2509.03311)
*Penggao Yan,Li-Ta Hsu*

Main category: eess.SP

TL;DR: 该研究提出了一个评估状态估计器可信度的多指标框架，结合了NEES、NCI、NLL和ES等指标，并引入了基于能量距离的定位检验和利用NLL与ES的非对称敏感性来区分协方差放大和系统偏差的方法，在模拟中取得了80-100%的分类准确率，显著优于单一指标方法。


<details>
  <summary>Details</summary>
Motivation: 评估状态估计器提供的自我评估不确定性度量（如协方差矩阵）的可靠性，解决因模型不符（如噪声或系统模型不匹配）导致的度量误导问题。

Method: 构建了一个紧凑的可信度投资组合，结合了NEES、NCI等传统指标和NLL、ES等评分规则。提出了基于能量距离的定位检验来检测系统模型错误指定，并利用NLL和ES的非对称敏感性来区分协方差缩放和系统偏差。

Result: 通过在六种不同可信度场景下的蒙特卡洛模拟，证明了所提出的方法实现了80-100%的高分类准确率，并且相比单一指标基线方法，能够提供更完整和正确的诊断。

Conclusion: 所提出的多指标评估框架为评估状态估计器的可信度提供了一个实用的工具，能够将可信度指标的模式转化为可操作的模型缺陷诊断。

Abstract: State estimators often provide self-assessed uncertainty metrics, such as
covariance matrices, whose reliability is critical for downstream tasks.
However, these self-assessments can be misleading due to underlying modeling
violations like noise or system model mismatch. This letter addresses the
problem of estimator credibility by introducing a unified, multi-metric
evaluation framework. We construct a compact credibility portfolio that
synergistically combines traditional metrics like the Normalized Estimation
Error Squared (NEES) and the Noncredibility Index (NCI) with proper scoring
rules, namely the Negative Log-Likelihood (NLL) and the Energy Score (ES). Our
key contributions are a novel energy distance-based location test to robustly
detect system model misspecification and a method that leverages the asymmetric
sensitivities of NLL and ES to distinguish optimism covariance scaling from
system bias. Monte Carlo simulations across six distinct credibility scenarios
demonstrate that our proposed method achieves high classification accuracy
(80-100%), drastically outperforming single-metric baselines which consistently
fail to provide a complete and correct diagnosis. This framework provides a
practical tool for turning patterns of credibility indicators into actionable
diagnoses of model deficiencies.

</details>


### [246] [Baseband Model, Cutoff Rate Bounds and Constellation Shaping for Mixed Gaussian-Impulsive Noise](https://arxiv.org/abs/2509.03333)
*Tianfu Qi,Jun Wang*

Main category: eess.SP

TL;DR: 优化传输星座以应对混合噪声，通过分析截止率（CR）的界限，利用投影梯度法求解优化问题，数值结果表明所提出的CR界限是紧密的，并且优化后的星座方案相比基线方案实现了显著的速率提升。


<details>
  <summary>Details</summary>
Motivation: 通信场景中存在的混合噪声（白高斯噪声和脉冲噪声）会严重降低系统性能。

Method: 从混合噪声的通带模型出发，推导出其基带表示。利用闭式上下界来分析截止率（CR）。通过分段线性逼近来获得有效的界限。将这些界限作为标准来优化传输星座点的几何和概率分布。使用投影梯度法求解优化问题。

Result: 所提出的CR界限是紧密的，并表现出预期的渐近行为。优化后的星座方案相比基线方案实现了显著的速率提升。

Conclusion: 通过优化传输星座点，可以有效 Mitigate 混合噪声的影响，提高通信系统的性能。

Abstract: Mixed noise, composed of white Gaussian noise (WGN) and impulsive noise (IN),
appears in numerous communication scenarios and can severely degrade system
performance. In this paper, we address this issue by optimizing the transmitted
constellation under mixed noise based on a theoretical analysis of the cutoff
rate (CR). First, starting from the passband model of the mixed noise, we
derive its corresponding baseband representation. Due to the complexity of the
CR, an exact analytic expression is generally intractable. Therefore, the
baseband noise model is employed to obtain closed-form lower and upper bounds
of the CR. A piecewise linear approximation is applied to derive efficient
bounds by exploiting the algebraic properties of the integral terms. These
bounds are then used as criteria to optimize the transmitted constellation
points in both geometric and probabilistic distributions. The projected
gradient method is employed to solve the optimization problem, and the
convergence and properties of the solutions are analyzed. Numerical results
demonstrate that the proposed CR bounds are tight and exhibit the expected
asymptotic behavior. Furthermore, the optimized constellation scheme achieves a
significant rate improvement compared to baselines.

</details>


### [247] [Efficient DoA Estimation with Hybrid Linear and Rectangular Arrays Using Compact DFT Codebook](https://arxiv.org/abs/2509.03488)
*Miguel Rivas-Costa,Carlos Mosquera*

Main category: eess.SP

TL;DR: 该研究提出了一种混合模拟和数字 (HAD) 架构，利用 Butler 矩阵和二阶统计量估计算法，在均匀线性阵列上实现高精度的到达角 (DoA) 估计，性能接近 CRLB。


<details>
  <summary>Details</summary>
Motivation: 解决现有 HAD 架构在实际应用中，由于数字维度限制和波束形成设计约束导致的 DoA 估计精度不足的问题。

Method: 提出一种新的 HAD 架构，使用 Butler 矩阵合成 DFT 波束。在此基础上，利用波束形成信号的类柯西位移结构，设计了一种二阶统计量估计算法。

Result: 仿真结果表明，所提出的算法在 DoA 估计方面达到了接近克拉美-罗下界 (CRLB) 的近最优精度，并且优于现有最先进的方法。

Conclusion: 所提出的 HAD 架构和二阶统计量估计算法能够有效提高 DoA 估计的精度，为大规模天线阵列的应用提供了有前景的解决方案。

Abstract: Hybrid Analog and Digital (HAD) architectures provide a cost-effective
alternative for large-scale antenna arrays, but accurate Direction-of-Arrival
(DoA) estimation remains challenging due to limited digital dimensionality and
constrained beamforming design. In this work, we propose a HAD architecture
that employs Butler matrices to synthesize DFT beams over a uniform linear
array. By exploiting the Cauchy-like displacement structure of the beamformed
signal, we introduce a second-order statistics estimation algorithm that
achieves near-optimal accuracy, approaching the Cram\'er-Rao Lower Bound (CRLB)
and outperforming state-of-the-art methods in simulation.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [248] [Sorting with constraints](https://arxiv.org/abs/2509.02616)
*A. Manas*

Main category: cs.DS

TL;DR: 我们研究了广义排序问题，其中只允许对元素进行部分两两比较。我们使用


<details>
  <summary>Details</summary>
Motivation: 研究广义排序问题，其中只允许对元素进行部分两两比较。

Method: 使用图论（团数和着色数）来分析算法，并将结果扩展到不一定可排序的输入，以及只对发现偏序感兴趣的情况。对于输入为 Erdos-Renyi 图的情况，开发了一种简单的算法，该算法在 O(n^(3/2) log n) 次探测中始终能确定基础偏序。

Result: 设计了一种在 O(n^(3/2) log n) 次探测中确定 Erdos-Renyi 图基础偏序的算法。

Conclusion: 使用图论参数（团数和着色数）分析广义排序问题，并将其扩展到不一定可排序的输入。

Abstract: In this work, we study the generalized sorting problem, where we are given a
set of $n$ elements to be sorted, but only a subset of all possible pairwise
element comparisons is allowed. We look at the problem from the perspective of
the graph formed by the ``forbidden'' pairs, and we parameterize algorithms
using the clique number and the chromatic number of this graph. We also extend
these results to the class of problems where the input graph is not necessarily
sortable, and one is only interested in discovering the partial order. We use
our results to develop a simple algorithm that always determines the underlying
partial order in $O(n^{3/2} \log n)$ probes, when the input graph is an
Erd\H{o}s--R\'enyi graph.

</details>


### [249] [Efficient Dynamic Rank Aggregation](https://arxiv.org/abs/2509.02885)
*Morteza Alimi,Hourie Mehrabiun,Alireza Zarei*

Main category: cs.DS

TL;DR: 该研究提出了一种动态秩聚合算法，LR-聚合算法，结合了LR树和LR距离。该算法在理论和实践上都具有高效性，并且能够提供接近最优的解决方案。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，需要高效地更新聚合排序，因为新的排序会随时间到来。

Method: 提出LR-聚合算法，该算法基于LR树数据结构，并借鉴了LR距离（Spearman's footrule distance的变体）。还分析了Pick-A-Perm算法的理论效率，并提出将其与LR-聚合算法结合。

Result: LR-聚合算法在实践中能产生接近最优的解决方案。Pick-A-Perm算法具有2的理论最坏情况近似保证。LR-聚合和Pick-A-Perm算法以及它们的组合方法都可以在O(n log n)时间内运行。

Conclusion: 这是第一个在动态环境中实现的快速、近乎线性时间的秩聚合算法，该算法既有理论近似保证，又有出色的实践性能。

Abstract: The rank aggregation problem, which has many real-world applications, refers
to the process of combining multiple input rankings into a single aggregated
ranking. In dynamic settings, where new rankings arrive over time, efficiently
updating the aggregated ranking is essential. This paper develops a fast,
theoretically and practically efficient dynamic rank aggregation algorithm.
First, we develop the LR-Aggregation algorithm, built on top of the LR-tree
data structure, which is itself modeled on the LR-distance, a novel and
equivalent take on the classical Spearman's footrule distance. We then analyze
the theoretical efficiency of the Pick-A-Perm algorithm, and show how it can be
combined with the LR-aggregation algorithm using another data structure that we
develop. We demonstrate through experimental evaluations that LR-Aggregation
produces close to optimal solutions in practice. We show that Pick-A-Perm has a
theoretical worst case approximation guarantee of 2. We also show that both the
LR-Aggregation and Pick-A-Perm algorithms, as well as the methodology for
combining them can be run in $O(n \log n)$ time. To the best of our knowledge,
this is the first fast, near linear time rank aggregation algorithm in the
dynamic setting, having both a theoretical approximation guarantee, and
excellent practical performance (much better than the theoretical guarantee).

</details>


### [250] [Fast approximation algorithms for the 1-median problem on real-world large graphs](https://arxiv.org/abs/2509.03052)
*Keisuke Ueta,Wei Wu,Mutsunori Yagiura*

Main category: cs.DS

TL;DR: 该研究提出了一种用于解决大规模图上1-中值问题的近似算法，该算法通过提前终止Dijkstra算法来减少计算量，并在节点数量较少时能保证最优解。实验证明，该算法在运行时比精确算法快得多，同时保持了接近最优的准确性。


<details>
  <summary>Details</summary>
Motivation: 在具有数百万个节点的超大规模图上，传统的1-中值问题精确求解方法（计算每个客户节点到所有节点的最短路径）计算成本过高。然而，在许多实际应用中，潜在设施位置节点数量巨大，而客户节点数量相对较少且空间集中，这使得详尽的图搜索不必要且效率低下。

Method: 提出三种近似算法，通过提前终止Dijkstra算法来减少计算量。对其中一种算法进行了理论分析，证明其近似比为2，另外两种算法的近似比可达1.618。此外，还提出了一种特定实例，证明了近似比的下界为1.2。研究还表明，当客户节点数量小于或等于3时，所有提出的算法都能返回最优解。

Result: 实验结果表明，与基线精确方法相比，所提出的算法在运行时显著提高，同时在所有测试的图类型中都保持了接近最优的准确性。特别是在具有1000万个节点的网格图上，所提出的算法在1毫秒内就获得了所有最优解，而基线精确方法平均需要超过70秒。

Conclusion: 所提出的近似算法能够有效地解决大规模图上的1-中值问题，在保证较高准确性的同时，大幅缩短了计算时间，为处理现实世界中的大规模图问题提供了一种有效的解决方案。

Abstract: The 1-median problem (1MP) on undirected weighted graphs seeks to find a
facility location minimizing the total weighted distance to all customer nodes.
Although the 1MP can be solved exactly by computing the single-source shortest
paths from each customer node, such approaches become computationally expensive
on large-scale graphs with millions of nodes. In many real-world applications,
such as recommendation systems based on large-scale knowledge graphs, the
number of nodes (i.e., potential facility locations) is enormous, whereas the
number of customer nodes is relatively small and spatially concentrated. In
such cases, exhaustive graph exploration is not only inefficient but also
unnecessary. Leveraging this observation, we propose three approximation
algorithms that reduce computation by terminating Dijkstra's algorithm early.
We provide theoretical analysis showing that one of the proposed algorithms
guarantees an approximation ratio of 2, whereas the other two improve this
ratio to 1.618. We demonstrate that the lower bound of the approximation ratio
is 1.2 by presenting a specific instance. Moreover, we show that all proposed
algorithms return optimal solutions when the number of customer nodes is less
than or equal to three. Extensive experiments demonstrate that our algorithms
significantly outperform baseline exact methods in runtime while maintaining
near-optimal accuracy across all tested graph types. Notably, on grid graphs
with 10 million nodes, our algorithms obtains all optimal solutions within 1
millisecond, whereas the baseline exact method requires over 70 seconds on
average.

</details>


### [251] [Triangle Detection in Worst-Case Sparse Graphs via Local Sketching](https://arxiv.org/abs/2509.03215)
*Hongyi Duan,Jian'an Zhang*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a non-algebraic, locality-preserving framework for triangle
detection in worst-case sparse graphs. Our algorithm processes the graph in
$O(\log n)$ independent layers and partitions incident edges into prefix-based
classes where each class maintains a 1-sparse triple over a prime field.
Potential witnesses are surfaced by pair-key (PK) alignment, and every
candidate is verified by a three-stage, zero-false-positive pipeline: a
class-level 1-sparse consistency check, two slot-level decodings, and a final
adjacency confirmation. \textbf{To obtain single-run high-probability coverage,
we further instantiate $R=c_G\log n$ independent PK groups per class (each
probing a constant number of complementary buckets), which amplifies the
per-layer hit rate from $\Theta(1/\log n)$ to $1-n^{-\Omega(1)}$ without
changing the accounting.} A one-shot pairing discipline and class-term
triggering yield a per-(layer,level) accounting bound of $O(m)$, while
keep-coin concentration ensures that each vertex retains only $O(d^+(x))$ keys
with high probability. Consequently, the total running time is $O(m\log^2 n)$
and the peak space is $O(m\log n)$, both with high probability. The algorithm
emits a succinct Seeds+Logs artifact that enables a third party to replay all
necessary checks and certify a NO-instance in $\tilde O(m\log n)$ time. We also
prove a $\Theta(1/\log n)$ hit-rate lower bound for any single PK family under
a constant-probe local model (via Yao)--motivating the use of $\Theta(\log n)$
independent groups--and discuss why global algebraic convolutions would break
near-linear accounting or run into fine-grained barriers. We outline measured
paths toward Las Vegas $O(m\log n)$ and deterministic near-linear variants.

</details>


### [252] [Compressed Dictionary Matching on Run-Length Encoded Strings](https://arxiv.org/abs/2509.03265)
*Philip Bille,Inge Li Gørtz,Simon J. Puglisi,Simon R. Tarnow*

Main category: cs.DS

TL;DR: 该论文提出了一个在压缩设置下解决字典匹配问题的算法，该算法使用游程长度编码来压缩模式和文本字符串，并在不解压缩的情况下实现高效的时间和空间复杂度。


<details>
  <summary>Details</summary>
Motivation: 在压缩设置下解决经典的字典匹配问题，目标是在不解压缩的情况下，根据压缩字符串的大小实现高效的时间和空间复杂度。

Method: 提出了一种新的压缩Aho-Corasick自动机表示方法和一种支持在游程长度编码字符串上进行快速查询的新型高效字符串索引。

Result: 该算法实现了 $O( (ar{m} + ar{n})	imes 	ext{log log } m + 	ext{occ})$ 的期望时间和 $O(ar{m})$ 的空间复杂度，其中 $ar{m}$ 是模式的总运行长度，$ar{n}$ 是文本的运行长度，$	ext{occ}$ 是模式在文本中的总出现次数。

Conclusion: 这是该问题第一个非平凡的解决方案，并且其时间界在 $	ext{log log } m$ 因子内是最优的。

Abstract: Given a set of pattern strings $\mathcal{P}=\{P_1, P_2,\ldots P_k\}$ and a
text string $S$, the classic dictionary matching problem is to report all
occurrences of each pattern in $S$. We study the dictionary problem in the
compressed setting, where the pattern strings and the text string are
compressed using run-length encoding, and the goal is to solve the problem
without decompression and achieve efficient time and space in the size of the
compressed strings. Let $m$ and $n$ be the total length of the patterns
$\mathcal{P}$ and the length of the text string $S$, respectively, and let
$\overline{m}$ and $\overline{n}$ be the total number of runs in the run-length
encoding of the patterns in $\mathcal{P}$ and $S$, respectively. Our main
result is an algorithm that achieves $O( (\overline{m} + \overline{n})\log \log
m + \mathrm{occ})$ expected time, and $O(\overline{m})$ space, where
$\mathrm{occ}$ is the total number of occurrences of patterns in $S$. This is
the first non-trivial solution to the problem. Since any solution must read the
input, our time bound is optimal within an $\log \log m$ factor. We introduce
several new techniques to achieve our bounds, including a new compressed
representation of the classic Aho-Corasick automaton and a new efficient string
index that supports fast queries in run-length encoded strings.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [253] [Etching-free dual-lift-off for direct patterning of epitaxial oxide thin films](https://arxiv.org/abs/2509.02618)
*Jiayi Qin,Josephine Si Yu See,Yanran Liu,Xueyan Wang,Wenhai Zhao,Yang He,Jianbo Ding,Yilin Wu,Shanhu Wang,Huiping Han,Afzal Khan,Shuya Liu,Sheng'an Yang,Hui Zhang,Jiangnan Li,Qingming Chen,Jiyang Xie,Ji Ma,Wanbiao Hu,Jianhong Yi,Liang Wu,X. Renshaw Wang*

Main category: cond-mat.mtrl-sci

TL;DR: We developed a dual-lift-off method using amorphous Sr3Al2O6 or Sr4Al2O7 (aSAO) as a sacrificial layer to pattern crystalline oxide films without etching, enabling precise fabrication of ferromagnetic and ferroelectric materials.


<details>
  <summary>Details</summary>
Motivation: Traditional etching methods for patterning oxide films are costly and can damage the films or substrate. This work aims to provide a better patterning method.

Method: A dual-lift-off technique is used. First, amorphous Sr3Al2O6 or Sr4Al2O7 (aSAO) is patterned by photoresist stripping. Then, the functional oxide thin films are patterned by dissolving the aSAO layer. The aSAO acts as a high-temperature photoresist compatible with oxide growth.

Result: Patterned ferromagnetic La0.67Sr0.33MnO3 and ferroelectric BiFeO3 thin films were successfully fabricated with high precision, accurately replicating the photoresist pattern.

Conclusion: The proposed dual-lift-off method is a simple, flexible, precise, environmentally friendly, and cost-effective approach for patterning high-quality oxide thin films, avoiding the drawbacks of traditional etching techniques.

Abstract: Although monocrystalline oxide films offer broad functional capabilities,
their practical use is hampered by challenges in patterning. Traditional
patterning relies on etching, which can be costly and prone to issues like film
or substrate damage, under-etching, over-etching, and lateral etching. In this
study, we introduce a dual-lift-off method for direct patterning of oxide
films, circumventing the etching process and associated issues. Our method
involves an initial lift-off of amorphous Sr$_3$Al$_2$O$_6$ or
Sr$_4$Al$_2$O$_7$ ($a$SAO) through stripping the photoresist, followed by a
subsequent lift-off of the functional oxide thin films by dissolving the $a$SAO
layer. $a$SAO functions as a ``high-temperature photoresist", making it
compatible with the high-temperature growth of monocrystalline oxides. Using
this method, patterned ferromagnetic La$_{0.67}$Sr$_{0.33}$MnO$_{3}$ and
ferroelectric BiFeO$_3$ were fabricated, accurately mirroring the shape of the
photoresist. Our study presents a straightforward, flexible, precise,
environmentally friendly, and cost-effective method for patterning high-quality
oxide thin films.

</details>


### [254] [Generative AI for Crystal Structures: A Review](https://arxiv.org/abs/2509.02723)
*Pierre-Paul De Breuck,Hai-Chen Wang,Gian-Marco Rignanese,Silvana Botti,Miguel A. L. Marques*

Main category: cond-mat.mtrl-sci

TL;DR: 生成式AI正在通过提出晶体结构和预测性质来革新材料发现，本综述全面 survey 了无机晶体材料生成模型的最新进展。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在材料发现领域的应用日益广泛，能够提出新的晶体结构并预测材料性质，因此需要对该领域的最新进展进行 survey。

Method: 文章提出了一种基于架构、表示、条件和材料域的分类法，用于对当前各种生成式AI模型进行分类，并讨论了数据源和性能指标的挑战，强调了标准化基准的必要性。文章还介绍了具体示例和新生成结构的应用程序。

Result: 文章 survey 了无机晶体材料生成模型的最新进展，提出了一种分类法，并讨论了数据源、性能指标、标准化基准、具体示例和新生成结构的应用程序。

Conclusion: 生成式AI在加速新型无机材料发现方面具有巨大潜力，但仍存在一些局限性，需要进一步的研究和发展。

Abstract: As in many other fields, the rapid rise of generative artificial intelligence
is reshaping materials discovery by offering new ways to propose crystal
structures and, in some cases, even predict desired properties. This review
provides a comprehensive survey of recent advancements in generative models
specifically for inorganic crystalline materials. We begin by introducing the
fundamentals of generative modeling and invertible material descriptors. We
then propose a taxonomy based on architecture, representation, conditioning,
and materials domain to categorize the diverse range of current generative AI
models. We discuss data sources and address challenges related to performance
metrics, emphasizing the need for standardized benchmarks. Specific examples
and applications of novel generated structures are presented. Finally, we
examine current limitations and future directions in this rapidly evolving
field, highlighting its potential to accelerate the discovery of new inorganic
materials.

</details>


### [255] [$P$-type Ru$_2$Ti$_{1-x}$Hf$_x$Si full-Heusler bulk thermoelectrics with $zT = 0.7$](https://arxiv.org/abs/2509.02765)
*Fabian Garmroudi,Illia Serhiienko,Michael Parzer,Andrej Pustogow,Raimund Podloucky,Takao Mori,Ernst Bauer*

Main category: cond-mat.mtrl-sci

TL;DR: Heusler化合物在热电材料领域显示出巨大潜力，特别是X2YZ型材料，尽管研究相对较少。


<details>
  <summary>Details</summary>
Motivation: 探索X2YZ型全Heusler化合物在不同温度下的热电性能，并与已广泛研究的XYZ型化合物进行对比。

Method: 通过实验合成和表征Ru2Ti1-xHfxSi，并利用双带模型分析Hf掺杂对电子结构的影响，以优化热电优值（zT）。

Result: 发现p型Ru2Ti1-xHfxSi热电材料，在700-1000 K温度范围内zT值达到约0.7，创全Heusler材料的最高纪录，并证实了p型材料优于n型材料的理论预测。

Conclusion: p型Ru2Ti1-xHfxSi化合物在热电应用中具有巨大潜力，通过进一步优化有望实现zT > 1，并强调了继续探索新型全Heusler相的重要性。

Abstract: Heusler compounds have emerged as important thermoelectric materials due to
their combination of promising electronic transport properties, mechanical
robustness and chemical stability -- key aspects for practical device
integration. While a wide range of XYZ-type half-Heusler compounds have been
studied for high-temperature applications, X$_2$YZ-type full-Heuslers, often
characterized by narrower band gaps, may offer potential advantages at
different temperature regimes but remain less explored. In this work, we report
the discovery of $p$-type Ru$_2$Ti$_{1-x}$Hf$_x$Si full-Heusler
thermoelectrics, exhibiting a high figure of merit $zT \sim 0.7$ over a broad
range of temperatures $700-1000$ K. These results not only represent the
largest values known to date among full-Heusler materials but confirm earlier
theoretical predictions that $p$-type Ru$_2$TiSi systems would be superior to
their $n$-type counterparts. Moreover, using a two-band model, we unveil
electronic structure changes induced by the Hf substitution at the Ti site and
outline strategies to further improve $zT$ up to $zT > 1$. Our findings
highlight the untapped potential of new semiconducting full-Heusler phases and
the crucial need for continued exploration of this rich materials class for
thermoelectric applications.

</details>


### [256] [Decomposition of low-angle grain boundaries](https://arxiv.org/abs/2509.02789)
*Wei Wan,Changxin Tang,Eric R Homer*

Main category: cond-mat.mtrl-sci

TL;DR: 晶界(GB)在微观结构演化过程中会合并，晶粒会消失。然而，Peach-Koehler模型预测特定的应力状态可能通过对不同的位错施加不同的Peach-Koehler力来逆转这一过程。这项工作将这种逆转视为晶界分解，并通过原子模拟在低角度不对称倾斜晶界和低角度混合倾斜-扭转晶界中进行了说明。在这两种情况下，位错会分离成两个晶界，中间有一个新晶粒。这项工作描述了分解的要求以及位错可分离性的重要性。此外，我们还研究了与此过程相关的位错行为和应力特征，以及应变率和温度对这些方面的影响。


<details>
  <summary>Details</summary>
Motivation: Peach-Koehler模型预测特定的应力状态可能通过对不同的位错施加不同的Peach-Koehler力来逆转微观结构演化过程中晶界的合并和晶粒的消失。本文旨在通过原子模拟来研究这种逆转现象，即晶界分解。

Method: 通过原子模拟，在低角度不对称倾斜晶界和低角度混合倾斜-扭转晶界中，研究Peach-Koehler力如何导致晶界分解。分析了分解的要求、位错可分离性、位错行为、应力特征以及应变率和温度的影响。

Result: 在低角度不对称倾斜晶界和低角度混合倾斜-扭转晶界中，观察到位错分离成两个晶界，中间形成了一个新晶粒。研究了实现分解的条件，并分析了应变率和温度对位错行为和应力特征的影响。

Conclusion: 晶界分解是一种由特定应力状态驱动的微观结构演化逆转现象，它通过位错分离形成新的晶粒。理解晶界分解的条件和机制对于控制材料的微观结构和性能至关重要。

Abstract: Grain boundaries (GBs) merge and grains disappear during microstructure
evolution. However, the Peach-Koehler model predicts that particular stress
states may reverse such a process by exerting differential Peach-Koehler forces
on different dislocations. This work considers this reversal as GB
decomposition and illustrates it in a low-angle asymmetric tilt GB and a
low-angle mixed tilt-twist GB via atomistic simulation. In both cases, the
dislocations separate into two GBs separated by a new grain. This work
describes the requirements for decomposition and the importance of dislocation
separability. Additionally, we examine the dislocation behaviors and stress
signatures associated with this process, along with the impact of strain rate
and temperature on those aspects.

</details>


### [257] [Octupole-driven spin-transfer torque switching of all-antiferromagnetic tunnel junctions](https://arxiv.org/abs/2509.03026)
*Jaimin Kang,Mohammad Hamdi,Shun Kong Cheung,Lin-Ding Yuan,Mohamed Elekhtiar,William Rogers,Andrea Meo,Peter G. Lim,M. S. Nicholas Tey,Anthony D'Addario,Shiva T. Konakanchi,Eric Matt,Jordan Athas,Sevdenur Arpaci,Lei Wan,Sanjay C. Mehta,Pramey Upadhyaya,Mario Carpentieri,Vinayak P. Dravid,Mark C. Hersam,Jordan A. Katine,Gregory D. Fuchs,Giovanni Finocchio,Evgeny Y. Tsymbal,James M. Rondinelli,Pedram Khalili Amiri*

Main category: cond-mat.mtrl-sci

TL;DR: 反铁磁隧道结(AFMTJ)可以实现电流控制的磁性开关，尽管反铁磁材料净磁化强度为零，但通过考虑动量依赖的自旋极化和磁八极矩，可以实现自旋转移扭矩效应。


<details>
  <summary>Details</summary>
Motivation: 尽管反铁磁材料净磁化强度为零，但以前认为其自旋转移扭矩效应（STT）很小，该研究旨在探索并实现反铁磁隧道结中的自旋转移扭矩效应。

Method: 通过制备PtMn3|MgO|PtMn3 AFMTJ，并进行实验测量和理论建模，解释电流诱导的八极驱动自旋转移扭矩（OTT）的起源。

Result: 成功实现了PtMn3|MgO|PtMn3 AFMTJ的电流诱导开关，室温下磁隧道结效应（TMR）高达363%，开关电流密度约为10 MA/cm2。理论模型解释了OTT起源于亚晶格自旋流的不平衡和非零净磁八极矩。

Conclusion: 这项工作为反铁磁自旋电子学提供了一个新的材料平台，并为实现超小型磁性存储器和室温太赫兹技术提供了途径。

Abstract: Magnetic tunnel junctions (MTJs) based on ferromagnets are canonical devices
in spintronics, with wide-ranging applications in data storage, computing, and
sensing. They simultaneously exhibit mechanisms for electrical detection of
magnetic order through the tunneling magnetoresistance (TMR) effect, and
reciprocally, for controlling magnetic order by electric currents through
spin-transfer torque (STT). It was long assumed that neither of these effects
could be sizeable in tunnel junctions made from antiferromagnetic materials,
since they exhibit no net magnetization. Recently, however, it was shown that
all-antiferromagnetic tunnel junctions (AFMTJs) based on chiral
antiferromagnets do exhibit TMR due to their non-relativistic
momentum-dependent spin polarization and cluster magnetic octupole moment,
which are manifestations of their spin-split band structure. However, the
reciprocal effect, i.e., the antiferromagnetic counterpart of STT driven by
currents through the AFMTJ, has been assumed non-existent due to the total
electric current being spin-neutral. Here, in contrast to this common
expectation, we report nanoscale AFMTJs exhibiting this reciprocal effect,
which we term octupole-driven spin-transfer torque (OTT). We demonstrate
current-induced OTT switching of PtMn3|MgO|PtMn3 AFMTJs, fabricated on a
thermally oxidized silicon substrate, exhibiting a record-high TMR value of
363% at room temperature and switching current densities of the order of 10
MA/cm2. Our theoretical modeling explains the origin of OTT in terms of the
imbalance between intra- and inter-sublattice spin currents across the AFMTJ,
and equivalently, in terms of the non-zero net cluster octupole polarization of
each PtMn3 layer. This work establishes a new materials platform for
antiferromagnetic spintronics and provides a pathway towards deeply scaled
magnetic memory and room-temperature terahertz technologies.

</details>


### [258] [Gravity and Composition Modulated Solidification and Mechanical Properties of Al-Cu Nanostructures](https://arxiv.org/abs/2509.02798)
*Apurba Sarker,Sourav Saha*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究使用分子动力学模型研究了铝铜二元合金在不同重力水平和成分下的凝固行为及其对力学性能（硬度）的影响，并提出了通过调控成分来优化材料以适应太空环境的策略。


<details>
  <summary>Details</summary>
Motivation: 为了深入了解地球以外的太空探索和人类定居所需的在轨制造过程，需要对凝固这一关键的原子尺度物理过程进行研究。目前缺乏实验数据，因此需要强大的计算模型来弥补。

Method: 采用分子动力学（MD）模型，研究不同成分和重力水平（地球、月球、火星和微重力）下铝铜二元合金的凝固行为，并通过纳米压痕模拟评估其硬度。

Result: 研究证实重力显著影响铝铜合金的凝固路径。通过调整合金成分，可以调节甚至逆转重力效应。硬度同时依赖于成分和重力，为定制化设计适用于太空环境的组件提供了可能。

Conclusion: 通过分子动力学模拟，揭示了重力和成分对铝铜合金凝固行为和硬度的影响机制，为在太空环境中设计和制造具有特定性能的材料提供了理论指导和新的途径。

Abstract: The future of space exploration and human settlement beyond Earth hinges on a
deeper understanding of in space manufacturing processes. The unique physical
conditions and scarcity of experimental data demand robust computational models
to investigate the atomic scale physics of solidification. This work presents a
molecular dynamics (MD) model to examine the solidification behavior of the Al
Cu binary alloy, focusing on the influence of varying compositions and gravity
levels (Earth, Lunar, Martian, and microgravity) on atomistic solidification
mechanisms and the resulting mechanical properties specifically, hardness of as
solidified nanostructures. Hardness is evaluated via nanoindentation
simulations. The study confirms that gravitational forces significantly affect
the solidification pathways of Al Cu alloys. Notably, by tuning alloy
composition, the influence of gravity can be modulated and in some cases, even
reversed. Moreover, hardness exhibits a coupled dependence on both composition
and gravity, offering a promising avenue for bottom-up design of components
tailored for extraterrestrial environments. The article delves into the
nanoscale physical mechanisms underlying these phenomena and outlines future
directions for extending this modeling framework to broader applications.

</details>


### [259] [Highly tunable band structure in ferroelectric R-stacked bilayer WSe$_2$](https://arxiv.org/abs/2509.02886)
*Zhe Li,Prokhor Thor,George Kourmoulakis,Tatyana V. Ivanova,Takashi Taniguchi,Kenji Watanabe,Hongyi Yu,Mauro Brotons-Gisbert,Brian D. Gerardot*

Main category: cond-mat.mtrl-sci

TL;DR: 研究了R型堆叠的WSe2同双层材料，揭示了其电子和铁电性质，为理解扭曲双层材料和开发新器件提供了基础。


<details>
  <summary>Details</summary>
Motivation: 探索量子材料研究的前沿领域，包括滑动铁电性和莫尔量子物态，并研究R型堆叠的WSe2同双层材料的电子和铁电性质。

Method: 采用低温光学光谱研究R型堆叠的WSe2同双层材料，通过激子和激子-极化子光谱研究其电子结构、能带对齐、铁电畴和极化行为。

Result: 1. 确定了WSe2同双层材料的II型能带对齐，导带和价带边缘分别位于$\\(Lambda)$和K谷。
2. 揭示了AB和BA铁电畴共存，并通过位移场调控了层间和层内激子杂化。
3. 测量了本征极化场，并提取了层间势。
4. 演示了由电场驱动的价带最大值对称切换。

Conclusion: R型堆叠的WSe2同双层材料具有完整的能带对齐、自发极化场和畴动力学，这些参数对于理解扭曲双层材料至关重要，并为铁电和激子器件提供了新的可能性。

Abstract: Transition metal dichalcogenide homobilayers unite two frontiers of quantum
materials research: sliding ferroelectricity, arising from rhombohedral (R)
stacking, and moir\'e quantum matter, emerging from small-angle twisting. The
spontaneous polarization of ferroelectric R-stacked homobilayers produces a
highly tunable band structure, which, together with strain-induced
piezoelectricity, governs the topology and correlated electronic phases of
twisted bilayers. Here we present a systematic low-temperature optical
spectroscopy study of R-stacked bilayer WSe$_2$ to quantitatively establish its
fundamental electronic and ferroelectric properties. Exciton and
exciton-polaron spectroscopy under doping reveals a pronounced electron-hole
asymmetry that confirms type-II band alignment, with the conduction and valence
band edges located at the $\Lambda$ and K valleys, respectively. Through
distinct excitonic responses and tunable interlayer-intralayer exciton
hybridization under displacement fields, we uncover the coexistence of AB and
BA ferroelectric domains. Using exciton-polarons as a probe, we directly
measure the intrinsic polarization field and extract the interlayer potential.
Finally, we demonstrate electric-field-driven symmetric switching of the
valence band maximum, attributed to ferroelectric domain switching. These
results provide a complete experimental picture of the band alignment,
spontaneous polarization field, and domain dynamics of R-stacked WSe$_2$,
establishing key parameters to understand twisted bilayers and enabling new
ferroelectric and excitonic device opportunities.

</details>


### [260] [PyDislocDyn: A Python code for calculating dislocation drag and other crystal properties](https://arxiv.org/abs/2509.02900)
*Daniel N. Blaschke*

Main category: cond-mat.mtrl-sci

TL;DR: PyDislocDyn 是一个用于金属位错动力学研究的 Python 程序包。


<details>
  <summary>Details</summary>
Motivation: 计算位错在声子风作用下的弛豫，并提供其他位错动力学相关计算功能。

Method: 使用 Continuum 限制进行计算，支持各种晶体对称性。

Result: 能够计算声子风引起的位错弛豫，以及平均弹性常数、位错场、极限速度、自能和线张力。

Conclusion: PyDislocDyn 为金属位错动力学研究提供了一套全面的计算工具。

Abstract: PyDislocDyn is a suite of python programs designed to perform various
calculations for basic research in dislocation dynamics in metals with various
crystal symmetries in the continuum limit. In particular, one of its main
purposes is to calculate dislocation drag from phonon wind. Additional features
include the averaging of elastic constants for polycrystals, the calculation of
the dislocation field including its limiting velocities, and the calculation of
dislocation self-energy and line tension.

</details>


### [261] [Magnetic resonance and microwave resistance modulation in van der Waals colossal-magnetoresistance material](https://arxiv.org/abs/2509.02901)
*Miuko Tanaka,Abdul Ahad,Darius-Alexandru Deaconu,Varun Shah,Daisuke Nishio-Hamane,Tomohiro Ishii,Masayuki Hashisaka,Shoya Sakamoto,Shinji Miwa,Mohammad Saeed Bahramy,Toshiya Ideue*

Main category: cond-mat.mtrl-sci

TL;DR: Mn3Si2Te6是一种原始的CMR材料，其CMR机制仍不清楚。本文通过研究其磁共振，发现c轴的g因子较低，表明c轴有显著的轨道磁化贡献，并且CMR对垂直于ab平面的自旋极化很敏感，揭示了轨道磁矩在该材料中的作用。


<details>
  <summary>Details</summary>
Motivation: Mn3Si2Te6是一种原始的CMR材料，其CMR机制仍不清楚。

Method: 通过研究Mn3Si2Te6的磁共振，观察了不同方向的g因子，并检测了共振条件下的电阻调制。

Result: 研究发现c轴的g因子较低，表明c轴有显著的轨道磁化贡献，并且CMR对垂直于ab平面的自旋极化很敏感。

Conclusion: CMR在Mn3Si2Te6中受到轨道磁矩的影响，揭示了自旋、轨道和晶格自由度之间的相互作用。

Abstract: Colossal magnetoresistance (CMR) is a fascinating quantum phenomenon that
continues to draw significant interest in condensed matter physics. Mn3Si2Te6
has emerged as a prototypical CMR material, notable for its puzzling
magnetoresistance behavior and pronounced directional anisotropy. Despite
extensive research, the mechanisms driving CMR in Mn3Si2Te6 remain elusive
[1-4]. In this work, we explore the magnetic resonance of Mn3Si2Te6 and observe
a reduced g-factor for magnetic fields applied along the crystalline c-axis
compared to the ab-plane, indicating a substantial orbital magnetization
contribution along the c-axis. Furthermore, we detect resistance modulation
under resonance conditions, suggesting that CMR in Mn3Si2Te6 is sensitive to
the out-of-the plane spin polarization. These findings shed new light on the
role of orbital magnetic moment in Mn3Si2Te6, offering a deeper understanding
of the interplay between spin, orbital and lattice degrees of freedom of
electrons in this system.

</details>


### [262] [Tunable Coatings on Various Substrates for Self-Adaptive Energy Harvesting with Daytime Solar Heating and Nighttime Radiative Cooling](https://arxiv.org/abs/2509.02921)
*Ken Araki,Vishwa Krishna Rajan,Liping Wang*

Main category: cond-mat.mtrl-sci

TL;DR: 氧化钒薄膜可用于全天候的太阳能加热和辐射冷却。


<details>
  <summary>Details</summary>
Motivation: 制备了可调谐的氧化钒（VO2）薄膜，用于自适应的白天太阳能加热和夜晚辐射冷却。

Method: 通过低氧炉氧化在不同基材上制备VO2薄膜，并利用其绝缘体-金属相变特性。

Result: 白天，薄膜表现出高太阳能吸收率（0.86）和低红外发射率（~0.2）；夜晚，则表现出高发射率（~0.76），有效散热。室外真空测试显示，白天温度升高169 K，夜晚温度降低17 K。模型显示白天加热功率约为400 W/m2，夜晚冷却功率约为60 W/m2。

Conclusion: 所提出的VO2薄膜具有成本效益高、可扩展的制备方法，可用于全天候能源收集。

Abstract: In this work, tunable vanadium dioxide (VO2) metafilms on different substrate
materials fabricated via low-oxygen furnace oxidation are demonstrated for
self-adaptive daytime solar heating and nighttime radiative cooling. Because of
its thermally-driven insulator-to-metal phase transition behavior, the VO2
metafilms work as spectrally-selective solar absorber with a high solar
absorptance of 0.86 and a low infrared emissivity of ~0.2 at daytime, while
they behave as selective cooler at nighttime to dissipate heat effectively
through the atmospheric transparency window with a high emissivity of ~0.76 to
cold outer space. From the outdoor vacuum tests, a significant temperature rise
up to 169 K upon solar heating and a temperature drop of 17 K at night are
experimentally observed from these tunable VO2 metafilms. With the atmosphere
temperature fitted in-situ, the accurate heat transfer model shows excellent
agreement with the stagnation temperature measurement, and indicates a high
heating power of ~400 W/m2 at 80{\deg}C sample temperature in the middle of the
day, and a cooling power of ~60 W/m2 at 30{\deg}C in equilibrium with ambient
at night. This work would facilitate the development of self-adaptive coatings
with cost-effective and scalable fabrication approaches for all-day energy
harvesting.

</details>


### [263] [Size-Dependent Structural Motifs in Ag$_n$Mo (n = 2-13) Clusters: From Planar to Icosahedral Architectures](https://arxiv.org/abs/2509.02984)
*Samantha Ortega-Flores,Peter Ludwig Rodríguez-Kessler*

Main category: cond-mat.mtrl-sci

TL;DR: Mo掺杂的银纳米团簇(Ag_nMo, n=1-14)的结构、电子和键合性质的研究表明，随着尺寸的增加，团簇从平面过渡到三维结构，n=12时达到具有高稳定性的二十面体结构。


<details>
  <summary>Details</summary>
Motivation: 研究Mo掺杂的银纳米团簇(Ag_nMo, n=1-14)的结构、电子和键合性质，以理解尺寸依赖性对几何、键合和电子结构的影响，并为催化和材料应用提供见解。

Method: 采用密度泛函理论(DFT)进行全局优化，并分析了结合能、二阶能量差、键长和Hirshfeld电荷转移，以研究团簇的结构、电子和键合特性。

Result: 研究发现，随着n从1增加到14，Mo掺杂的银纳米团簇的结构从平面低对称性演化为三维高对称性，在n=12时形成一个高度稳定的二十面体结构。n=12被确定为一个具有增强的热力学稳定性和显著的HOMO-LUMO能隙的“魔数”团簇。键长分析显示Ag-Mo键长相对恒定，而Ag-Ag键长随尺寸增大而增大。Hirshfeld电荷分析表明，在小团簇中存在从Ag到Mo的显著电荷转移，但随着尺寸的增大而减小，系统趋向于离域金属键合。

Conclusion: Mo掺杂的银纳米团簇的结构、电子和键合性质表现出显著的尺寸依赖性，n=12的团簇具有独特的稳定性和电子特性，这对其在催化和材料科学中的潜在应用具有重要意义。

Abstract: We present a comprehensive density functional theory (DFT) study of Mo-doped
silver clusters Ag$_n$Mo ($n=1$-14), focusing on their structural, electronic,
and bonding properties. Global optimization reveals an evolution from planar
and low-symmetry isomers in small clusters to compact three-dimensional
geometries with higher symmetry, culminating in a highly stable icosahedral
structure at $n=12$. Binding energy and second-order energy difference analyses
identify $n=12$ as a "magic number" cluster exhibiting enhanced thermodynamic
stability and a pronounced HOMO-LUMO gap, indicative of electronic shell
closure. Bond length analysis shows relatively constant Ag-Mo distances
alongside a size-dependent increase in Ag-Ag bond lengths, reflecting the
growth of metallic bonding networks. Hirshfeld charge analysis reveals
significant charge transfer from Ag to Mo in small clusters, which decreases
with size as the system transitions toward delocalized metallic bonding. These
findings provide detailed insights into the size-dependent interplay of
geometry, bonding, and electronic structure in Ag$_n$Mo clusters, with
implications for their catalytic and material applications.

</details>


### [264] [Probing ice-rule-breaking transition in $\rm{Dy_2Ti_2O_7}$ thin film by proximitized transport and magnetic torque](https://arxiv.org/abs/2509.03082)
*Chengkun Xing,Han Zhang,Kyle Noordhoek,Guoxin Zheng,Kuan-Wen Chen,Lukas Horák,Yan Xin,Eun Sang Choi,Lu Li,Haidong Zhou,Jian Liu*

Main category: cond-mat.mtrl-sci

TL;DR: 虽然 Dy2Ti2O7 和 Ho2Ti2O7 等块状烧绿石的自旋冰态因其独特的简并基态和涌现的单极子激发而在过去几十年中得到广泛研究，但它是否以薄膜形式存在仍然是个谜。


<details>
  <summary>Details</summary>
Motivation: 块状烧绿石（如 Dy2Ti2O7 和 Ho2Ti2O7）的自旋冰态因其独特的简并基态和涌现的单极子激发而得到广泛研究，但其在薄膜形式下的存在状态仍不清楚。

Method: 合成了 18nm 厚的 Dy2Ti2O7 薄膜，并用导电的 Bi2Ir2O7 层覆盖，然后进行了邻近磁阻测量，同时进行了电容扭矩磁力测量。

Result: 研究发现，自旋冰态的 the ice-rule-breaking 相变得以保留，但与块状晶体相比，其有效的最近邻相互作用发生了改变，并且 Ising 自旋轴发生了扭曲。

Conclusion: 邻近输运是研究绝缘体磁性薄膜的有效工具。

Abstract: While the spin ice state of bulk pyrochlores such as $\rm{Dy_2Ti_2O_7}$ and
$\rm{Ho_2Ti_2O_7}$ has been extensively studied in the last several decades due
to its unique degenerate ground state and emergent monopole excitation, whether
it survives in the thin-film form remains a mystery. The limited volume of
thin-film sample makes it challenging to study the intrinsic magnetic
properties. Here, we synthesized 18nm-thick $\rm{Dy_2Ti_2O_7}$ thin film on YSZ
substrate and capped it by a thin conductive $\rm{Bi_2Ir_2O_7}$ layer, and
performed the proximitized magnetoresistance measurements. Our study found that
the ice-rule-breaking phase transition survives but with a modified effective
nearest-neighbor interaction and distorted Ising spin axes compared to the bulk
crystal. The results are supported by the simultaneously measured capacitive
torque magnetometry. Our study demonstrates that proximitized transport is an
effective tool for thin films of insulating frustrated magnets.

</details>


### [265] [Silicon-monoxide flames: the nucleation and condensation of silica fume](https://arxiv.org/abs/2509.03106)
*Nils Erland L. Haugen,Axel Brandenburg,Bernd Friede,Rolf G. Birkeland*

Main category: cond-mat.mtrl-sci

TL;DR: 以数值模拟研究了硅粉的形成过程，探讨了使用可再生还原材料对硅粉质量的影响。


<details>
  <summary>Details</summary>
Motivation: 研究硅粉形成过程，特别是使用可再生还原材料对硅粉质量的影响。

Method: 使用自洽的数值模拟，包括成核和凝结过程，并采用拉格朗日方法进行粒子追踪。

Result: 发现布朗运动导致的聚结是决定最终粒径分布的最关键物理效应；提出使用合理的表面能、饱和蒸气压和成核指数的表达式；发现潜热释放会导致粒子成核和凝结的链式反应；证实拉格朗日方法比欧拉方法更灵活、准确且CPU效率更高。

Conclusion: 该研究通过数值模拟深入理解了硅粉的形成机制，并为优化生产过程提供了指导。

Abstract: Silica fume is a valuable by-product from the silicon and ferrosilicon
production. It is therefore important to understand the impact on the silica
fume quality when converting the furnace feed from fossil-based to renewable
reduction materials. Using self-consistent numerical simulations of the
nucleation and condensation process, we present a detailed study of the silica
fume formation process.
  It is found that the most critical physical effect that determines the final
particle size distribution is coalescence due to Brownian motion. Furthermore,
it is crucial to use appropriate thermophysical parameters in order to
reproduce reliable particle size distributions. Contrary to what has been done
in previous studies on the same topic, this is now done by using reasonable
expressions for surface energy, saturation pressure and the nucleation
pre-exponential factor.
  It is also found that under conditions relevant to furnaces, the liberation
of latent heat leads to an explosive chain reaction of particle nucleation and
condensation when the first particles nucleate and start growing due to
condensation. This process continues until the relative saturation pressure of
silicon dioxide is reduced to unity.
  Finally, it is found that the Lagrangian approach for particle tracking is
more flexible and accurate, and also more CPU efficient, than the Eulerian
approach.

</details>


### [266] [Reciprocity of Magnetism and Nanostructure Growth](https://arxiv.org/abs/2509.03202)
*Felix Zahner,Kirsten von Bergmann,Roland Wiesendanger,André Kubetzka*

Main category: cond-mat.mtrl-sci

TL;DR: 通过磁性控制纳米结构生长


<details>
  <summary>Details</summary>
Motivation: 控制磁性薄膜和纳米结构的生长参数，以控制其性能和功能。

Method: 在磁性转变温度以上和以下，将钴沉积在单轴反铁磁表面上，并研究纳米结构的形成。

Result: 在转变温度以上生长形成大致为六边形的岛，在转变温度以下生长形成沿特定晶体学方向的准一维钴纳米结构，这些结构表现出局域反铁磁畴取向。

Conclusion: 通过磁性控制纳米结构生长是可行的，并且在磁性表面生长时必须考虑磁性效应。

Abstract: The growth of thin films and nanostructures is a fundamental process for
constructing both model-type systems and nanoscale devices, where performance
and functionalities can be controlled by the choice of parameters. For magnetic
systems the crystal structure, material composition, size, and shape determine
already most of the magnetism-related properties. Here, we investigate the
reciprocal route, which is controlling nanostructure growth via magnetism. To
this end we deposit Co on a uniaxial antiferromagnetic surface above and below
its N\'eel temperature. Growth above the N\'eel temperature results in the
formation of roughly hexagonal islands, reflecting the surface symmetry. For
growth below the N\'eel temperature we find quasi-one-dimensional Co
nanostructures along distinct crystallographic directions, which signal the
local antiferromagnetic domain orientation. Our findings demonstrate the
feasibility of controlling growth via magnetism and the necessity to take
magnetism-related effects into account for growth on magnetic surfaces.

</details>


### [267] [Shift Current Anomalous Photovoltaics in a Double Perovskite Ferroelectric](https://arxiv.org/abs/2509.03210)
*Linjie Wei,Fu Li,Yi Liu,Hongbin Zhang,Junhua Luo,Zhihua Sun*

Main category: cond-mat.mtrl-sci

TL;DR: (环己基甲铵)$_2$CsAgBiBr$_7$ 铁电材料在高温下表现出异常光伏效应，其光电压远超带隙，且与晶格位移相关，可用于构建新型光电器件。


<details>
  <summary>Details</summary>
Motivation: 由于对铁电异常光伏(APV)效应的起源和潜在机制缺乏了解，阻碍了新型APV材料的探索。

Method: 利用新兴的漂移电流模型，结合实验研究，分析了(环己基甲铵)$_2$CsAgBiBr$_7$ 铁电材料的APV效应，并阐明了其结构起源。

Result: 在(环己基甲铵)$_2$CsAgBiBr$_7$ 材料中发现了强的APV效应，光电压高达40V，远超其2.3eV的带隙。漂移电流模型揭示了其与Cs$^{+}$阳离子位移的内在联系。此外，其稳态APV光电流表现出独特的光偏振依赖性，偏振比高达41。

Conclusion: 该研究首次明确了铁电APV活性的结构起源，为开发新型光电器件提供了新的途径。

Abstract: Ferroelectric anomalous photovoltaic (APV) effect, as a fascinating physical
conceptual phenomenon, holds significant potentials for new optoelectronic
device applications. However, due to the knowledge lacking on the origin and
underlying mechanism of ferroelectric APV effect, substantial challenges still
remain in exploring new APV-active candidate materials. The emerging shift
current model, involving the transfer of photogenerated charges through the
displacement of wave functions, has attracted considerable attention for its
unique insights into the bulk photovoltaic effect. Here, we present strong APV
properties in a high-temperature double perovskite ferroelectric
(cyclohexylmethylammonium)$_2$CsAgBiBr$_7$, showing an extremely large
above-bandgap photovoltage up to about 40 V. This figure-of-merit is far beyond
its bandgap of about 2.3 eV and comparable to the state-of-art molecular
ferroelectrics. Strikingly, the shift current model reveals an intrinsic
correlation with Cs$^{+}$ cation displacement and provides, for the first time,
an explicit explanation for the structural origin of ferroelectric APV
activities. Besides, its steady-state APV photocurrent exhibits the unique
light-polarization dependence, which endows remarkable polarization-sensitivity
with the highest polarization ratios of about 41 among the known 2D
single-phase materials. As the unprecedented exploration of ferroelectric APV
characteristics illuminated by the shift current mechanism, this finding paves
a pathway to assemble new optoelectronic smart devices.

</details>


### [268] [Isotopically Selected Single Antimony Molecule Doping](https://arxiv.org/abs/2509.03243)
*Mason Adshead,Maddison Coke,Evan Tillotson,Kexue Li,Sam Sullivan-Allsop,Ricardo Egoavil,William Thornley,Yi Cui,Christopher M Gourlay,Katie L Moore,Sarah J Haigh,Richard J Curry*

Main category: cond-mat.mtrl-sci

TL;DR: A new method allows for the precise placement of antimony ion donors in silicon, achieving high detection efficiency and suitable for creating scaled qudit arrays for quantum computing.


<details>
  <summary>Details</summary>
Motivation: A reliable method for deterministic fabrication of impurity ion donors in silicon is needed for quantum computing.

Method: The paper reports doping isotopically-defined antimony clusters into silicon with high detection efficiency. The method is compatible with silicon pre-enrichment and allows for precise control over donor placement, with post-implantation Sb-to-Sb separation of ~2 nm.

Result: High detection efficiency of 94% was achieved. Atomically resolved imaging showed Sb-to-Sb separation of ~2 nm, indicating suitability for coupled qudit systems.

Conclusion: This work presents a potential pathway for creating scaled qudit arrays in silicon for quantum computing applications.

Abstract: A reliable route to the deterministic fabrication of impurity ion donors in
silicon is required to advance quantum computing architectures based upon such
systems. This paper reports the ability to dope isotopically-defined unique
(${}^{121}\mathrm{Sb}{}^{123}\mathrm{Sb}$) clusters into silicon with measured
detection efficiencies of 94% being obtained. Atomically resolved imaging of
the doped clusters reveals a Sb-to-Sb separation of ~2 nm post-implantation,
thus indicating suitability to form coupled qudit systems. The method used is
fully compatible with integration into processing that includes pre-enrichment
of the silicon host to < 3ppm ${}^{29}\mathrm{Si}$ levels. As such, we present
a potential pathway to the creation of scaled qudit arrays within silicon
platforms for quantum computing.

</details>


### [269] [Fabrication and Characterization of the Moiré surface state on a topological insulator](https://arxiv.org/abs/2509.03322)
*Yi Zhang,Dang Liu,Qiaoyan Yu,Ruijun Xi,Xingsen Chen,Shasha Xue,Jice Sun,Xian Du,Xuhui Ning,Tingwen Miao,Pengyu Hu,Hao Yang,Dandan Guan,Xiaoxue Liu,Liang Liu,Yaoyi Li,Shiyong Wang,Canhua Liu,Haijiao Ji,Noah F. Q. Yuan,Hao Zheng,Jinfeng Jia*

Main category: cond-mat.mtrl-sci

TL;DR: 通过开发一种两步生长方法，成功制备了具有外源性超晶格的拓扑绝缘体Sb2Te3薄膜，并通过扫描隧道显微镜和光谱学进行了表征，为下一代电子学提供了有前景的平台。


<details>
  <summary>Details</summary>
Motivation: 外源性超晶格在拓扑绝缘体表面具有新颖特性的潜力，但尚未实现。

Method: 采用两步生长法，通过分子束外延技术实现顶层扭转，制备了具有外源性超晶格的拓扑绝缘体Sb2Te3薄膜。

Result: 通过扫描隧道显微镜和光谱学表征了所得的外源性拓扑表面态，并且在磁场作用下，与原始Sb2Te3表面相比，外源性区域的朗道能级出现了新的特征。

Conclusion: 所开发的外源性超晶格制备方法可以广泛应用于其他二维材料，以制备外源性超晶格，为下一代电子器件的研究开辟了新途径。

Abstract: A Moire superlattice on the topological insulator surface is predicted to
exhibit many novel properties but has not been experimentally realized. Here,
we developed a two-step growth method to successfully fabricate a topological
insulator Sb2Te3 thin film with a Moire superlattice, which is generated by a
twist of the topmost layer via molecular beam epitaxy. The established Moire
topological surface state is characterized by scanning tunneling microscopy and
spectroscopy. By application of a magnetic field, new features in Landau levels
arise on the Moire region compared to the pristine surface of Sb2Te3, which
makes the system a promising platform for pursuing next-generation electronics.
Notably, the growth method, which circumvents contamination and the induced
interface defects in the manual fabrication method, can be widely applied to
other van der Waals materials for fabricating Moire superlattices.

</details>


### [270] [A Comprehensive Assessment and Benchmark Study of Large Atomistic Foundation Models for Phonons](https://arxiv.org/abs/2509.03401)
*Md Zaibul Anam,Ogheneyoma Aghoghovbia,Mohammed Al-Fahdi,Lingyu Kong,Victor Fung,Ming Hu*

Main category: cond-mat.mtrl-sci

TL;DR: 六种通用机器学习势能（uMLP）在2429种晶体材料上的声子性质预测能力得到了评估，EquiformerV2（预训练和微调）在预测二阶力常数和晶格热导率方面表现最佳，MACE和CHGNet在力预测方面表现良好但声子性质预测较差，MatterSim在力预测准确性较低的情况下获得了中等的力常数预测结果。


<details>
  <summary>Details</summary>
Motivation: 尽管通用机器学习势能（uMLP）在材料性质预测方面发展迅速，但其在声子性质建模方面的系统性评估仍然有限。

Method: 使用六种uMLP（EquiformerV2、MatterSim、MACE、CHGNet）计算2429种晶体材料的原子力，推导力常数，并预测晶格热导率（LTC），与密度泛函理论（DFT）和实验数据进行比较。

Result: EquiformerV2预训练模型在预测原子力和三阶力常数方面表现优异；微调后的EquiformerV2在预测二阶力常数、LTC和其他声子性质方面持续优于其他模型。MACE和CHGNet的力预测准确性与EquiformerV2相当，但LTC预测不佳。MatterSim的力预测准确性较低，但IFC预测结果居中，表明可能存在误差抵消。

Conclusion: 该基准测试为在高通量筛选具有特定热传输性质的材料时，评估和选择uMLP提供了指导。

Abstract: The rapid development of universal machine learning potentials (uMLPs) has
enabled efficient, accurate predictions of diverse material properties across
broad chemical spaces. While their capability for modeling phonon properties is
emerging, systematic benchmarking across chemically diverse systems remains
limited. We evaluate six recent uMLPs (EquiformerV2, MatterSim, MACE, and
CHGNet) on 2,429 crystalline materials from the Open Quantum Materials
Database. Models were used to compute atomic forces in displaced supercells,
derive interatomic force constants (IFCs), and predict phonon properties
including lattice thermal conductivity (LTC), compared with density functional
theory (DFT) and experimental data. The EquiformerV2 pretrained model trained
on the OMat24 dataset exhibits strong performance in predicting atomic forces
and third-order IFC, while its fine-tuned counterpart consistently outperforms
other models in predicting second-order IFC, LTC, and other phonon properties.
Although MACE and CHGNet demonstrated comparable force prediction accuracy to
EquiformerV2, notable discrepancies in IFC fitting led to poor LTC predictions.
Conversely, MatterSim, despite lower force accuracy, achieved intermediate IFC
predictions, suggesting error cancellation and complex relationships between
force accuracy and phonon predictions. This benchmark guides the evaluation and
selection of uMLPs for high-throughput screening of materials with targeted
thermal transport properties.

</details>


### [271] [Enhanced second-harmonic generation from WS$_2$/ReSe$_2$ heterostructure](https://arxiv.org/abs/2509.03491)
*Kanchan Shaikh,Taejun Yoo,Zeyuan Zhu,Qiuyang Li,Amalya C. Johnson,Hui Deng,Fang Liu,Yuki Kobayashi*

Main category: cond-mat.mtrl-sci

TL;DR: 范德华叠层在非线性光学领域展现出优异的可调性和可扩展性，但层间相互作用对整体非线性光学磁化率的影响尚不明确。本研究以具有独特晶体相的WS2/ReSe2异质双层为例，研究了各向异性的二次谐波产生（SHG）增强效应，并揭示了能带对齐不足以解释观察到的SHG各向异性。光谱学研究表明，带重构和层间杂化在其中起到重要作用，SHG增强表现出高度各向异性，甚至在某些方向上被抑制，暗示了强度借用机制。本研究证明了通过堆叠不同晶体相的范德华材料来调控非线性光学响应的强度和偏振依赖性是可行的。


<details>
  <summary>Details</summary>
Motivation: 范德华叠层在非线性光学领域提供了可调性和可扩展性，但层间相互作用如何影响整体非线性光学磁化率仍不清楚。

Method: 研究了具有不同晶体相的WS2/ReSe2异质双层的各向异性二次谐波产生（SHG）增强效应，并进行了偏振分辨响应和扭转角依赖性研究，同时分析了光谱学的变化以揭示带重构和层间杂化。

Result: WS2/ReSe2异质双层表现出各向异性的SHG增强，仅靠能带对齐无法解释观察到的各向异性。光谱学研究表明带重构和层间杂化作用明显，SHG增强具有高度各向异性，并可能存在强度借用机制。

Conclusion: 通过堆叠具有不同晶体相的范德华材料，可以调控非线性光学响应的强度和偏振依赖性。

Abstract: Van der Waals stacking presents new opportunities for nonlinear optics with
its remarkable tunability and scalability. However, the fundamental role of
interlayer interactions in modifying the overall nonlinear optical
susceptibilities remains elusive. In this letter, we report an anisotropic
enhancement of second-harmonic generation (SHG) from a WS$_2$/ReSe$_2$
heterobilayer, where the individual composite layers possess distinctive
crystal phases. We investigate polarization-resolved response and twist-angle
dependence in SHG and reveal that band alignment alone is insufficient to
explain the observed anisotropy in the modified SHG response. Spectral shifts
in excitonic features highlight band renormalization, supporting the role of
hybridization between the two layers. Furthermore, SHG enhancement is highly
anisotropic and can even be suppressed in some orientations, suggesting
possible intensity-borrowing mechanisms within the heterostructure. Our work
demonstrates the ability to tune both the intensity and polarization dependence
of nonlinear optical responses with van der Waals stacking of distinctive
crystal phases.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [272] [A Novel IaaS Tax Model as Leverage Towards Green Cloud Computing](https://arxiv.org/abs/2509.02767)
*Benedikt Pittl,Werner Mach,Erich Schikuta*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The cloud computing technology uses datacenters, which require energy. Recent
trends show that the required energy for these datacenters will rise over time,
or at least remain constant. Hence, the scientific community developed
different algorithms, architectures, and approaches for improving the energy
efficiency of cloud datacenters, which are summarized under the umbrella term
Green Cloud computing. In this paper, we use an economic approach - taxes - for
reducing the energy consumption of datacenters. We developed a tax model called
GreenCloud tax, which penalizes energy-inefficient datacenters while fostering
datacenters that are energy-efficient. Hence, providers running
energy-efficient datacenters are able to offer cheaper prices to consumers,
which consequently leads to a shift of workloads from energy-inefficient
datacenters to energy-efficient datacenters. The GreenCloud tax approach was
implemented using the simulation environment CloudSim. We applied real data
sets published in the SPEC benchmark for the executed simulation scenarios,
which we used for evaluating the GreenCloud tax.

</details>


### [273] [Mycroft: Tracing Dependencies in Collective Communication Towards Reliable LLM Training](https://arxiv.org/abs/2509.03018)
*Yangtao Deng,Lei Zhang,Qinlong Wang,Xiaoyun Zhi,Xinlei Zhang,Zhuo Jiang,Haohan Xu,Lei Wang,Zuquan Song,Gaohong Liu,Yang Bai,Shuguang Wang,Wencong Xiao,Jianxi Ye,Minlan Yu,Hong Xu*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reliability is essential for ensuring efficiency in LLM training. However,
many real-world reliability issues remain difficult to resolve, resulting in
wasted resources and degraded model performance. Unfortunately, today's
collective communication libraries operate as black boxes, hiding critical
information needed for effective root cause analysis. We propose Mycroft, a
lightweight distributed tracing and root cause analysis system designed to
address previously hidden reliability issues in collective communication.
Mycroft's key idea is to trace collective communication states and leverage
internal control and data dependencies to resolve reliability problems in LLM
training. Mycroft has been deployed at ByteDance for over six months to debug
collective communication related issues at runtime. It detected anomalies
within 15 seconds in 90% of cases and identified the root cause within 20
seconds in 60% of cases. We also conducted extensive fault injection
experiments to demonstrate Mycroft's capability and efficiency.

</details>


### [274] [FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale Training of LLMs](https://arxiv.org/abs/2509.03047)
*Haijun Zhang,Jinxiang Wang,Zhenhua Yu,Yanyong Zhang,Xuejie Ji,Kaining Mao,Jun Zhang,Yaqing Zhang,Ting Wu,Fei Jie,Xiemin Huang,Zhifang Cai,Junhua Cheng,Shuwei Wang,Wei Li,Xiaoming Bao,Hua Xu,Shixiong Zhao,Jun Li,Hongwei Sun,Ziyang Zhang,Yi Xiong,Chunsheng Li*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) have made a profound impact across various
fields due to their advanced capabilities. However, training these models at
unprecedented scales requires extensive AI accelerator clusters and
sophisticated parallelism strategies, which pose significant challenges in
maintaining system reliability over prolonged training periods. A major concern
is the substantial loss of training time caused by inevitable hardware and
software failures. To address these challenges, we present FlashRecovery, a
fast and low-cost failure recovery system comprising three core modules: (1)
Active and real-time failure detection. This module performs continuous
training state monitoring, enabling immediate identification of hardware and
software failures within seconds, thus ensuring rapid incident response; (2)
Scale-independent task restart. By employing different recovery strategies for
normal and faulty nodes, combined with an optimized communication group
reconstruction protocol, our approach ensures that the recovery time remains
nearly constant, regardless of cluster scale; (3) Checkpoint-free recovery
within one step. Our novel recovery mechanism enables single-step restoration,
completely eliminating dependence on traditional checkpointing methods and
their associated overhead. Collectively, these innovations enable FlashRecovery
to achieve optimal Recovery Time Objective (RTO) and Recovery Point Objective
(RPO), substantially improving the reliability and efficiency of long-duration
LLM training. Experimental results demonstrate that FlashRecovery system can
achieve training restoration on training cluster with 4, 800 devices in 150
seconds. We also verify that the time required for failure recovery is nearly
consistent for different scales of training tasks.

</details>


### [275] [The High Cost of Keeping Warm: Characterizing Overhead in Serverless Autoscaling Policies](https://arxiv.org/abs/2509.03104)
*Leonid Kondrashov,Boxi Zhou,Hancheng Wang,Dmitrii Ustiugov*

Main category: cs.DC

TL;DR: 该研究通过构建一个可复现的无服务器系统来分析和比较同步与异步自动伸缩策略的性能和成本效益，并揭示了实例抖动和内存分配带来的显著开销，同时指出了当前系统在降低开销与性能之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏开放的基准测试和详细的系统分析，对无服务器计算中控制平面设计的性能-成本权衡的理解不足。

Method: 设计了一个能够近似AWS Lambda和Google Cloud Run等商业供应商伸缩行为的无服务器系统，并通过回放真实世界的工作负载并调整关键的自动伸缩参数来系统地比较同步和异步自动伸缩策略的性能和成本效益。此外，还采用了一种结合真实控制平面部署和大规模仿真的混合方法。

Result: 研究表明，该开源系统可以精确复制商业平台的运行特性。实例抖动带来的计算开销相当于请求处理CPU周期的10-40%，内存分配是实际使用量的2-10倍。降低这些开销会导致性能显著下降。

Conclusion: 当前的无服务器系统在降低实例抖动和内存分配开销方面存在挑战，这会影响性能，因此需要新的、具有成本效益的自动伸缩策略。

Abstract: Serverless computing is transforming cloud application development, but the
performance-cost trade-offs of control plane designs remain poorly understood
due to a lack of open, cross-platform benchmarks and detailed system analyses.
In this work, we address these gaps by designing a serverless system that
approximates the scaling behaviors of commercial providers, including AWS
Lambda and Google Cloud Run. We systematically compare the performance and
cost-efficiency of both synchronous and asynchronous autoscaling policies by
replaying real-world workloads and varying key autoscaling parameters.
  We demonstrate that our open-source systems can closely replicate the
operational characteristics of commercial platforms, enabling reproducible and
transparent experimentation. By evaluating how autoscaling parameters affect
latency, memory usage, and CPU overhead, we reveal several key findings. First,
we find that serverless systems exhibit significant computational overhead due
to instance churn equivalent to 10-40% of the CPU cycles spent on request
handling, primarily originating from worker nodes. Second, we observe high
memory allocation due to scaling policy: 2-10 times more than actively used.
Finally, we demonstrate that reducing these overheads typically results in
significant performance degradation in the current systems, underscoring the
need for new, cost-efficient autoscaling strategies. Additionally, we employ a
hybrid methodology that combines real control plane deployments with
large-scale simulation to extend our evaluation closer to a production scale,
thereby bridging the gap between small research clusters and real-world
environments.

</details>


### [276] [Efficient and Secure Sleepy Model for BFT Consensus](https://arxiv.org/abs/2509.03145)
*Pengkun Ren,Hai Dong,Zahir Tari,Pengcheng Zhang*

Main category: cs.DC

TL;DR: 本篇论文提出了一种结合预提交机制和可公开验证的秘密共享（PVSS）的拜占庭容错（BFT）协议，以解决动态可用系统中延迟和安全性的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 动态可用系统中拜占庭容错（BFT）共识协议面临着在波动的节点参与下平衡延迟和安全性的挑战。现有解决方案通常需要多次投票才能做出决定，导致延迟高或对恶意行为的抵御能力有限。

Method: 该协议将预提交机制与可公开验证的秘密共享（PVSS）集成到消息传输中，通过 PVSS 将用户身份与其消息绑定，从而减少通信轮数。

Result: 在常见场景下，该协议通常只需要四次网络延迟（4$\Delta$），能够抵御多达 1/2 的恶意参与者。理论分析证明了协议抵抗拜占庭攻击的鲁棒性。实验评估显示，与传统 BFT 协议相比，该协议能显著防止分叉发生并提高链的稳定性。与最长链协议相比，在适度的参与波动场景下，该协议保持了稳定性和较低的延迟。

Conclusion: 通过将预提交机制与 PVSS 集成，该协议在不损害完整性的前提下，提高了 BFT 协议的效率和安全性，解决了动态可用系统中延迟和安全性的权衡问题。

Abstract: Byzantine Fault Tolerant (BFT) consensus protocols for dynamically available
systems face a critical challenge: balancing latency and security in
fluctuating node participation. Existing solutions often require multiple
rounds of voting per decision, leading to high latency or limited resilience to
adversarial behavior. This paper presents a BFT protocol integrating a
pre-commit mechanism with publicly verifiable secret sharing (PVSS) into
message transmission. By binding users' identities to their messages through
PVSS, our approach reduces communication rounds. Compared to other
state-of-the-art methods, our protocol typically requires only four network
delays (4$\Delta$) in common scenarios while being resilient to up to 1/2
adversarial participants. This integration enhances the efficiency and security
of the protocol without compromising integrity. Theoretical analysis
demonstrates the robustness of the protocol against Byzantine attacks.
Experimental evaluations show that, compared to traditional BFT protocols, our
protocol significantly prevents fork occurrences and improves chain stability.
Furthermore, compared to longest-chain protocol, our protocol maintains
stability and lower latency in scenarios with moderate participation
fluctuations.

</details>


### [277] [CloudFormer: An Attention-based Performance Prediction for Public Clouds with Unknown Workload](https://arxiv.org/abs/2509.03394)
*Amirhossein Shahbazinia,Darong Huang,Luis Costero,David Atienza*

Main category: cs.DC

TL;DR: CloudFormer是一个基于Transformer的双分支模型，可以预测黑盒环境中的虚拟机性能下降，它通过联合建模时间动态和系统级交互，并在各种工作负载下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于资源竞争，多租户云环境中的虚拟机性能会下降，而现有的缓解技术需要准确的性能预测，但这在公共云中很困难。

Method: CloudFormer是一个双分支Transformer模型，利用206个系统指标来模拟时间动态和系统级交互，以预测VM性能下降。

Result: CloudFormer在各种工作负载下持续优于最先进的方法，平均绝对误差（MAE）为7.8%，比现有方法至少提高了28%。

Conclusion: CloudFormer在黑盒云环境中准确预测VM性能下降方面表现出色，提供了比现有方法更优越的性能。

Abstract: Cloud platforms are increasingly relied upon to host diverse,
resource-intensive workloads due to their scalability, flexibility, and
cost-efficiency. In multi-tenant cloud environments, virtual machines are
consolidated on shared physical servers to improve resource utilization. While
virtualization guarantees resource partitioning for CPU, memory, and storage,
it cannot ensure performance isolation. Competition for shared resources such
as last-level cache, memory bandwidth, and network interfaces often leads to
severe performance degradation. Existing management techniques, including VM
scheduling and resource provisioning, require accurate performance prediction
to mitigate interference. However, this remains challenging in public clouds
due to the black-box nature of VMs and the highly dynamic nature of workloads.
To address these limitations, we propose CloudFormer, a dual-branch
Transformer-based model designed to predict VM performance degradation in
black-box environments. CloudFormer jointly models temporal dynamics and
system-level interactions, leveraging 206 system metrics at one-second
resolution across both static and dynamic scenarios. This design enables the
model to capture transient interference effects and adapt to varying workload
conditions without scenario-specific tuning. Complementing the methodology, we
provide a fine-grained dataset that significantly expands the temporal
resolution and metric diversity compared to existing benchmarks. Experimental
results demonstrate that CloudFormer consistently outperforms state-of-the-art
baselines across multiple evaluation metrics, achieving robust generalization
across diverse and previously unseen workloads. Notably, CloudFormer attains a
mean absolute error (MAE) of just 7.8%, representing a substantial improvement
in predictive accuracy and outperforming existing methods at least by 28%.

</details>


### [278] [Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers](https://arxiv.org/abs/2508.19805)
*Shota Naito,Tsukasa Ninomiya,Koichi Wada*

Main category: cs.DC

TL;DR: 研究了移动机器人系统中计算能力的标准模型，重点关注机器人能力、光可观测性和调度器同步性之间的复杂相互作用，并提出了新的分离图和不可能性判据。


<details>
  <summary>Details</summary>
Motivation: 研究移动机器人系统的计算能力，特别关注机器人能力、光可观测性和调度器同步性如何相互作用，以扩展已知的机器人模型分离图。

Method: 通过引入和分析特定问题（如ETE、HET、TAR(d)*、LP-MLCv、VEC、ZCC、VTR和LP-Cv）来研究不同模型下的机器人能力，并在此过程中比较了不同同步性（完全同步和弱同步）和内部记忆的影响。

Result: 证明了ETE问题仅在最强的模型（完全同步、完全光可观测性）下可解；证明了在弱同步下，内部记忆不足以解决HET和TAR(d)*问题，但完全同步可以弥补光可观测性和记忆的不足；在异步设置下，对LP-MLCv、VEC和ZCC等问题进行了分类，揭示了FSTA和FCOM机器人之间的细粒度分离；分析了VTR和LP-Cv，说明了内部记忆在对称环境中的局限性。

Conclusion: 扩展了14个典型机器人模型的分离图，揭示了仅通过高阶比较才能看到的结构现象，并为移动机器人计算能力提供了新的不可能性判据和更深入的理解。

Abstract: Understanding the computational power of mobile robot systems is a
fundamental challenge in distributed computing. While prior work has focused on
pairwise separations between models, we explore how robot capabilities, light
observability, and scheduler synchrony interact in more complex ways.
  We first show that the Exponential Times Expansion (ETE) problem is solvable
only in the strongest model -- fully-synchronous robots with full mutual lights
($\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and
TAR(d)* problems to demonstrate how internal memory and lights interact with
synchrony: under weak synchrony, internal memory alone is insufficient, while
full synchrony can substitute for both lights and memory.
  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and
ZCC to show fine-grained separations between $\mathcal{FSTA}$ and
$\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and
Leave Place Convergence (LP-Cv), illustrating the limitations of internal
memory in symmetric settings.
  These results extend the known separation map of 14 canonical robot models,
revealing structural phenomena only visible through higher-order comparisons.
Our work provides new impossibility criteria and deepens the understanding of
how observability, memory, and synchrony collectively shape the computational
power of mobile robots.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [279] [Treasure Hunt in Anonymous Graphs with Quantum Pebbles by Oblivious Agents](https://arxiv.org/abs/2509.02909)
*Gaurav Gaur,Barun Gorain,Rishi Ranjan Singh,Daya Gaur*

Main category: quant-ph

TL;DR: 提出了一种利用量子石子在匿名图中进行搜索的新方法，与经典石子相比，具有更高的效率。


<details>
  <summary>Details</summary>
Motivation: 解决在匿名图中，经典石子方法对于保持遗忘信息的代理来说信息传输有限和遍历复杂的问题。

Method: 提出并首次使用了量子石子进行匿名图搜索。量子石子通过发射具有唯一量子态的量子比特来编码下一个节点信息，代理通过在多个基上进行测量来区分量子态，从而找到正确的路径。

Result: 证明了该策略使遗忘代理能够在D步内找到宝藏，且只需要O((log D + log Δ)/(log 1/δ))次每节点测量，其中D是起点到宝藏的最短路径长度，Δ是最大度，δ = cos^2(π/(2Δ))。

Conclusion: 提出了使用量子信息作为匿名图搜索的引导机制，并证明了量子石子不仅可以模仿经典石子的功能，而且效率更高，为未来量子增强的分布式算法提供了有前景的方向。

Abstract: We investigate the problem of finding a static treasure in anonymous graphs
using oblivious agents and introduce a novel approach that leverages quantum
information. In anonymous graphs, vertices are unlabelled, indistinguishable,
and edges are locally labelled with port numbers. Agents typically rely on
stationary classical pebbles placed by an oracle to guide their search.
However, this classical approach is constrained by limited information
transmission and high traversal complexity. Classical pebbles are not
sufficient for search if the agents are oblivious. We propose the first use of
quantum pebbles for search in anonymous graphs. Quantum pebbles periodically
emit qubits in a fixed quantum state. Each pebble encodes the port number to
the next node using a unique quantum state. The agent determines the correct
path by performing measurements in multiple bases, exploiting the probabilistic
nature of quantum measurement to distinguish states. We show that this strategy
enables an oblivious agent to locate the treasure in $D$ steps using $D$
quantum pebbles, where $D$ is the length of the shortest path between the
starting point and the treasure. Moreover, only $O((\log D + \log \Delta)/(\log
1/\delta))$ measurements per node are required to ensure high success
probability in a graph with maximum degree $\Delta$ where $\delta =
\cos^2(\frac{\pi}{2\Delta})$. We propose the use of quantum information as a
guidance mechanism in anonymous graph search. We demonstrate that quantum
pebbles can not only emulate the functionality of classical pebbles but can do
so with improved efficiency, offering a promising direction for future
quantum-enhanced distributed algorithms.

</details>


### [280] [Quantum aspects of the classical Maxwell's equations in free space from the perspective of the correspondence principle](https://arxiv.org/abs/2509.02620)
*M. F. Araujo de Resende,Leonardo S. F. Santos,R. Albertini Silva*

Main category: quant-ph

TL;DR: 量子力学诞生100周年之际，本文旨在探讨量子理论的基础，并讨论量子理论与麦克斯韦电磁理论之间的对应关系。


<details>
  <summary>Details</summary>
Motivation: 量子力学及其他量子理论的建立受到对应原理的指导，该原理需要识别它们与其他成熟的物理理论之间的联系。因此，本文将专门讨论量子理论与麦克斯韦电磁理论之间的对应关系。

Method: 本文将讨论自由空间中的麦克斯韦电磁理论以及对应原理的强形式，如何共同指向量子力学的基础，而量子力学直到半个世纪后才被正式提出。为此，我们将展示，只需运用当时已知的数学方法处理自由空间中的麦克斯韦方程组，就可以识别出光子的量子力学描述。

Result: 通过处理自由空间中的麦克斯韦方程组，可以识别出光子的量子力学描述，而所使用的数学工具在量子力学出现之前就已经广为人知。

Conclusion: 麦克斯韦电磁理论和对应原理的强形式已经为后来的量子力学奠定了基础，即使在量子力学正式提出之前，也可以通过分析麦克斯韦方程组来识别光子的量子描述。

Abstract: Due to the advent of Quantum Mechanics' 100th anniversary, we wrote this
review paper in order to present a discussion that addresses the foundations of
this theory. And since the creation of this Mechanics and other quantum
theories was guided by correspondence principles that needed to be identified
between them and other well-established physical theories, this paper will be
devoted to discussing the correspondence between these quantum theories and
Maxwell's theory of electromagnetism. More precisely, what we will do
throughout this paper is discuss how the Maxwell's electromagnetic theory in
free space and the stronger formulation of the correspondence principle already
pointed, together, to the basis of a Quantum Mechanics that was only be
formulated half a century later. And, in order to make this very clear, we will
show that the quantum-mechanical description of a photon can already be
identified, simply, by manipulating Maxwell's equations in free space with
mathematical resources that, for instance, were already well known before the
advent of Quantum Mechanics.

</details>


### [281] [\textit{In Silico} Benchmarking of Detectable Byzantine Agreement in Noisy Quantum Networks](https://arxiv.org/abs/2509.02629)
*Mayank Bhatia,Shaan Doshi,Daniel Winton,Brian Doolittle,Bruno Abreu,Santiago Núñez-Corrales*

Main category: quant-ph

TL;DR: 利用EPR对实现可检测拜占庭协议（QDBA），并通过模拟研究其在现实量子网络条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 量子通信资源在拜占庭协议（BA）等容错分布式协议中具有显著优势，尤其是在对抗敌对干扰方面。本研究旨在探索使用最简单的最大纠缠量子资源——EPR对，来实现可检测拜占庭协议（QDBA），使其在现有量子硬件平台上具有实验可行性。

Method: 使用Aliro量子网络模拟器，对EPRQDBA协议进行了全面的计算研究，系统地探索了网络规模、叛徒节点数量、协议消耗的纠缠量以及针对超导和光量子比特技术定制的物理噪声模型等参数空间。

Result: 通过广泛的数值实验，研究了这些实际物理参数对协议性能的影响，确定了实验实现的关键阈值和最佳运行状态。

Conclusion: 本工作将量子共识协议的理论进展与实际网络实现相结合，为实验人员提供了具体的参考。研究结果可作为在实际噪声环境中评估和优化QDBA实现的指南。

Abstract: Quantum communication resources offer significant advantages for
fault-tolerant distributed protocols, particularly in Byzantine Agreement (BA),
where reliability against adversarial interference is essential. Quantum
Detectable Byzantine Agreement (QDBA) enables consensus protocols that surpass
classical limitations by leveraging entangled quantum states. In this work, we
focus on the practical realization of QDBA using Einstein-Podolsky-Rosen (EPR)
pairs, the simplest maximally entangled quantum resources, making the protocol
experimentally accessible across current quantum hardware platforms. We present
a comprehensive computational study of the EPRQDBA protocol under realistic
quantum network conditions, utilizing the Aliro Quantum Network Simulator to
evaluate the performance and robustness of the protocol. Our simulations
systematically explore the protocol's parameter space --including variations in
network size, traitorous node count, the amount of entanglement consumed in the
protocol, and physically motivated noise models tailored specifically for
superconducting and photonic qubit technologies. Through extensive numerical
experiments, we provide insights into how these physically realistic parameters
impact protocol performance, establishing critical thresholds and optimal
operational regimes for experimental implementations. This work bridges
theoretical advances in quantum consensus protocols with practical network
implementations, offering a concrete reference for experimentalists. Our
findings serve as a guideline for evaluating and optimizing QDBA
implementations in realistic, noisy environments.

</details>


### [282] [Programmable Quantum Matter: Heralding Large Cluster States in Driven Inhomogeneous Spin Ensembles](https://arxiv.org/abs/2509.02992)
*Pratyush Anand,Louis Follet,Odiel Hooybergs,Dirk R. Englund*

Main category: quant-ph

TL;DR: 该研究提出了一种利用固体中原子类发射体不均匀性来简化量子控制的方法，通过优化的脉冲序列，将生成光学信号驱动的自旋簇态所需的资源从 O(Nq) 降低到 O(1)，并实现了极高的单比特门保真度。


<details>
  <summary>Details</summary>
Motivation: 固体中原子类发射体的量子控制因其精细结构的不均匀性而变得复杂，该研究旨在克服这一挑战。

Method: 提出了一种利用发射体多样性来生成光学信号驱动的自旋簇态的框架，该框架使用优化的脉冲序列来同时校正脉冲长度和失谐误差，并将其应用于硅-空位中心以增强相干时间。

Result: 实现了超过 99.99% 的单比特门保真度，并将相干时间延长了 7 倍以上。此外，该方法还能生成比传统方法多出 10^2-10^4 倍的纠缠链接。

Conclusion: 该研究提出的技术，包括全局控制、相位去噪、远程纠缠和编译，为具有异质自旋系综的稳健量子计算架构提供了可扩展的工具。

Abstract: Atom-like emitters in solids are promising platforms for quantum sensing and
information processing, but inhomogeneities in the emitter fine structure
complicate quantum control. We present a framework that leverages this
diversity to reduce the resources for generating optically heralded spin
cluster states across $N_q$ emitters from the conventional order $O(N_q)$ to
$O(1)$ in ensembles of $N_q \sim 10$-$100$. An optimized pulse sequence
simultaneously corrects pulse-length and detuning errors, achieving
single-qubit gate fidelities exceeding $99.99\%$ for errors (normalized
relative to the Rabi drive strength) up to 0.3, while maintaining fidelities
above $99\%$ for errors as large as 0.4. Applied as a Carr-Purcell-Meiboom-Gill
(CPMG) dynamical decoupling protocol to the dominant noise spectrum of
silicon-vacancy centers in diamond, it enhances ensemble coherence times by
over $7\times$ compared to interleaved bang-bang based CPMG. For
state-of-the-art dilution refrigerators, global resonant optimal decoupling
across $N_q$ spins sharply reduces heating, addressing the trade-off between
the spin coherence and scaling to $N_q \gg 1$. We further introduce a modified
single-photon entanglement protocol with an efficient algorithm for
deterministic entanglement compilation. Depending on the decoupling time
window, our method yields order $O(10^2$-$10^4)$ more entanglement links than
bang-bang sequences, with theoretical guarantees of order $\Omega(N_q)$ unique
links, improvable by control tuning. Together, these techniques provide
scalable tools - including global control, phase denoising, remote
entanglement, and compilation - for robust quantum computing architectures with
heterogeneous spin ensembles.

</details>


### [283] [Simultaneous approximation of multiple degenerate states using a single neural network quantum state](https://arxiv.org/abs/2509.02658)
*Waleed Sherif*

Main category: quant-ph

TL;DR: 提出一种单主干多头（ST-MH）神经量子态（NQS）集成方法，用于高效近似简并量子多体系统的所有简并态，并在J1-J2海森堡模型上进行了数值验证。


<details>
  <summary>Details</summary>
Motivation: 现有的NQS在近似简并量子多体系统的所有简并态时计算成本高昂。

Method: 提出一种单主干多头（ST-MH）NQS集成方法，该方法共享一个特征提取主干，并为每个目标状态附加轻量级头。使用包含正交性项的成本函数，在标准变分蒙特卡洛（VMC）框架内推导出精确的解析梯度和重叠导数以进行训练。ST-MH可以通过特征映射的列空间包含目标态的对数模和相位分支的线性展开（在所有状态非零的公共支撑上）来精确表示每个简并本征态。

Result: ST-MH 方法可以精确表示简并本征态，并且在特定条件下可以减少参数数量和VMC成本。在存在简并基态的铁磁环状J1-J2海森堡模型上进行了数值验证，证明了ST-MH在能量精度和保真度方面表现优异，同时计算资源消耗显著降低。

Conclusion: ST-MH集成方法为高效近似简并量子多体系统提供了一种有前景的途径，并在计算成本和近似精度之间取得了良好的平衡。该方法在满足特定条件时，尤其是在主干计算占主导地位且简并度不大的情况下，具有显著的优势。

Abstract: Neural network quantum states (NQS) excel at approximating ground states of
quantum many-body systems, but approximating all states of a degenerate
manifold is nevertheless computationally expensive. We propose a single-trunk
multi-head (ST-MH) NQS ensemble that share a feature extracting trunk while
attaching lightweight heads for each target state. Using a cost function which
also has an orthogonality term, we derive exact analytic gradients and overlap
derivatives needed to train ST-MH within standard variational Monte Carlo (VMC)
workflows. We prove that ST-MH can represent every degenerate eigenstate
exactly whenever the feature map of latent width $h$, augmented with a
constant, has column space containing the linear span of the targets'
log-moduli and (chosen) phase branches together with the constant on the common
support where all states are non-vanishing. Under this condition, ST-MH reduces
the parameter count and can reduce the leading VMC cost by a factor equal to
the degeneracy $K$ relative to other algorithms when $K$ is modest and in trunk
dominated regimes. As a numerical proof-of-principle, we validate and benchmark
the ST-MH approach on the frustrated spin-$\tfrac{1}{2}$ $J_1-J_2$ Heisenberg
model at the Majumdar-Ghosh point on periodic ring lattices of up to 8 sites.
By obtaining the momentum eigenstates, we demonstrate that ST-MH attains high
fidelity and energy accuracy across degenerate ground state manifolds while
using significantly lower computing resources. Lastly we provide a qualitative
computational cost analysis which incentivise the applicability of the ST-MH
ensemble under certain criteria on the latent width.

</details>


### [284] [Zero-Error Nash Equilibrium: Harnessing Nonlocal Correlation in Incomplete Information Games](https://arxiv.org/abs/2509.02947)
*Ambuj,Tushar,Siddharth R. Pandey,Ram Krishna Patra,Anandamay Das Bhowmik,Kuntal Som,Amit Mukherjee*

Main category: quant-ph

TL;DR: 该研究将香农的零错误通信范式应用于不完全信息博弈，探讨在存在隐私信息的情况下，玩家是否能协同达到零错误概率的纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 将香农的零错误通信范式扩展到不完全信息博弈论，以探索在存在隐私信息的情况下，玩家是否能够协同达到零错误概率的纳什均衡。

Method: 提出零错误纳什均衡协调的概念，构建了一个三方贝叶斯博弈，并利用贝尔非局域关联（包括真实纠缠）来解决经典方法无法实现的协调问题。此外，还构建了一个双人博弈，利用除最大纠缠态之外的任意两量子比特纯纠缠态来实现更强的协调。

Result: 研究发现，在某些贝叶斯博弈中，经典方法无法实现零错误纳什均衡协调，但利用贝尔非局域关联可以实现。研究成功构建了一个可实现零错误纳什均衡协调的三方博弈，并构建了一个双人博弈，利用多种两量子比特纠缠态实现更强的协调。该优势在实验相关的噪声下依然存在。

Conclusion: 贝尔非局域性是一种强大的资源，可用于在不确定性下进行近乎零错误决策。该研究为信息论、博弈论和量子非局域性之间搭建了新的桥梁。

Abstract: Claude Shannon's zero-error communication paradigm reshaped our understanding
of fault-tolerant information transfer. Here, we adapt this notion into game
theory with incomplete information. We ask: can players with private
information coordinate on a Nash equilibrium with zero probability of error? We
identify Bayesian games in which such coordination is impossible classically,
yet achievable by harnessing Bell nonlocal correlations. We formalize this
requirement as zero-error Nash equilibrium coordination, establishing a new
bridge between information theory, game theory, and quantum nonlocality.
Furthermore, we construct a tripartite Bayesian game that admits zero-error
Nash equilibrium coordination with genuine entanglement, and a two-player game
where a stronger notion of coordination can be achieved using every two-qubit
pure entangled state except the maximally one. Crucially, the advantage
persists under experimentally relevant noise, demonstrating nonlocality as a
robust resource for near-zero error decision-making under uncertainty.

</details>


### [285] [Quantum Circuit Optimization for the Fault-Tolerance Era: Do We Have to Start from Scratch?](https://arxiv.org/abs/2509.02668)
*Tobias Forster,Nils Quetschlich,Robert Wille*

Main category: quant-ph

TL;DR: NISQ 量子电路优化技术可以减少 FTQC 的资源需求，并为 NISQ 优化技术向 FTQC 的转移提供指导。


<details>
  <summary>Details</summary>
Motivation: NISQ 硬件的噪声限制了量子计算的应用，而 FTQC 需要大量的量子比特开销。因此，需要量子电路优化技术来减少从 NISQ 到 FTQC 的量子比特开销。

Method: 利用资源估算来比较 NISQ 量子电路优化技术对 FTQC 资源需求的潜在好处，并分析不同优化技术的效果。

Result: NISQ 量子电路优化技术确实可以改善 FTQC 的资源需求估计，并且可以为向 FTQC 转移 NISQ 优化技术提供指导。

Conclusion: NISQ 量子电路优化技术可以为 FTQC 节省大量资源，并且相关技术可以为向 FTQC 的过渡提供指导。

Abstract: Quantum computing has made significant advancements in the last years in both
hardware and software. Unfortunately, the currently available Noisy
Intermediate-Scale Quantum (NISQ) hardware is still heavily affected by noise.
Many optimization techniques have been developed to reduce the negative effects
thereof, which, however, only works up to a certain point. Therefore, scaling
quantum applications from currently considered small research examples to
industrial applications requires error-correction techniques to execute quantum
circuits in a fault-tolerant fashion and enter the Fault-Tolerant Quantum
Computing (FTQC) era. These error-correction techniques introduce dramatic
qubit overheads, leading to the requirement of tens of thousands of qubits
already for toy-sized examples. Hence, quantum circuit optimization that
reduces qubit overheads when shifting from the NISQ to the FTQC era is
essential. This raises the question, whether we need to start from scratch, or
whether current state-of-the-art optimization techniques can be used as a basis
for this. To approach this question, this work investigates the effects of
different optimization passes on a representative selection of quantum
circuits. Since hardly any tools to automatically design and evaluate FTQC
quantum circuits exist yet, we utilize resource estimation to compare the
(potential) benefits gained by applying NISQ quantum circuit optimization to
estimated FTQC resource requirements. The results indicate that, indeed, the
estimated resource requirements for FTQC can be improved by applying NISQ
quantum circuit optimization techniques. At the same time, more detailed
investigations show what techniques lead to more benefits for FTQC compared to
others, providing guidelines for the transfer of NISQ optimization techniques
to the FTQC era.

</details>


### [286] [Magnetic noise in macroscopic quantum spatial superposition induced by inverted harmonic oscillator potential](https://arxiv.org/abs/2509.02670)
*Sneha Narasimha Moorthy,Anupam Mazumdar*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate a Stern-Gerlach type matter-wave interferometer where an
inhomogeneous magnetic field couples to an embedded spin in a nanoparticle to
create spatial superpositions. Employing a sequence of harmonic and inverted
harmonic oscillator potentials created by external magnetic fields, we aim to
enhance the one-dimensional superposition of a nanodiamond with mass $\sim
10^{-15}$ kg to $\sim 1 \mu$m. Such matter-wave interferometers have diverse
applications, including entanglement of adjacent interferometers mediated by
Standard Model and beyond-Standard Model fields, precision tests of the
equivalence principle, quantum sensing, and laboratory probes of the quantum
nature of spacetime. However, random fluctuations of the magnetic field
stochastically perturbs the interferometer paths and induce dephasing. We
quantitatively estimate the susceptibility of the interferometer to white noise
arising from magnetic-field fluctuations. Constraining the dephasing rate
$\Gamma$ to be low enough that the final coherence $e^{-\Gamma \tau}\leq 0.1$
(where $\tau$ is the experimental time duration), we obtain the following
bounds on the noise to signal ratios: $\delta
\eta_\text{IHP}/\eta_\text{IHP}\lesssim 10^{-13}$, where $\eta_\text{IHP}$ is
the magnetic field curvature that gives rise to the inverted harmonic
potential, and $\delta \eta_\text{HP}/\eta_\text{HP}\lesssim 10^{-6}$, where
$\eta_\text{HP}$ is the linear magnetic field gradient that gives rise to the
harmonic potential. For such tiny fluctuations, we demonstrate that the
Humpty-Dumpty problem arising from a mismatch in position and momentum does not
cause a loss in contrast of the interferometer. Further, we show that
constraining the dephasing rate leads to stricter bounds on the noise
parameters than enforcing a contrast threshold, indicating that good dephasing
control ensures high interferometric contrast.

</details>


### [287] [The Munich Quantum Software Stack: Connecting End Users, Integrating Diverse Quantum Technologies, Accelerating HPC](https://arxiv.org/abs/2509.02674)
*Lukas Burgholzer,Jorge Echavarria,Patrick Hopf,Yannick Stade,Damian Rovara,Ludwig Schmid,Ercüment Kaya,Burak Mete,Muhammad Nufail Farooqi,Minh Chung,Marco De Pascale,Laura Schulz,Martin Schulz,Robert Wille*

Main category: quant-ph

TL;DR: MQSS是一个模块化、开源的混合量子-经典应用程序生态系统，旨在解决量子计算的可访问性问题，并与HPC无缝集成。


<details>
  <summary>Details</summary>
Motivation: 量子计算的广泛可访问性需要一个全面、高效、统一的软件栈，该软件栈能够灵活地支持各种硬件和不断发展的算法，为专家和非专家提供可用的编程模型，动态管理资源，并与经典HPC无缝集成。

Method: MQSS采用多层架构，在异构量子后端上执行高级应用程序，并协调其与经典工作负载的耦合。其核心组件包括：面向流行框架的前端适配器、新的编程方法、HPC集成调度程序、基于MLIR的编译器以及量子设备管理接口(QDMI)等硬件抽象层。

Result: MQSS提供成熟的概念和开源组件，为构建强大的量子计算软件栈奠定基础。该软件栈的设计具有前瞻性，能够适应容错量子计算的需求，包括多种量子比特编码和中途测量。

Conclusion: MQSS是一个模块化、开源、社区驱动的混合量子-经典应用程序生态系统，旨在实现量子计算的广泛可访问性，并促进与经典HPC的集成。

Abstract: Quantum computing is advancing rapidly in hardware and algorithms, but broad
accessibility demands a comprehensive, efficient, unified software stack. Such
a stack must flexibly span diverse hardware and evolving algorithms, expose
usable programming models for experts and non-experts, manage resources
dynamically, and integrate seamlessly with classical High-Performance Computing
(HPC). As quantum systems increasingly act as accelerators in hybrid workflows
-- ranging from loosely to tightly coupled -- few full-featured implementations
exist despite many proposals.
  We introduce the Munich Quantum Software Stack (MQSS), a modular,
open-source, community-driven ecosystem for hybrid quantum-classical
applications. MQSS's multi-layer architecture executes high-level applications
on heterogeneous quantum back ends and coordinates their coupling with
classical workloads. Core elements include front-end adapters for popular
frameworks and new programming approaches, an HPC-integrated scheduler, a
powerful MLIR-based compiler, and a standardized hardware abstraction layer,
the Quantum Device Management Interface (QDMI). While under active development,
MQSS already provides mature concepts and open-source components that form the
basis of a robust quantum computing software stack, with a forward-looking
design that anticipates fault-tolerant quantum computing, including varied
qubit encodings and mid-circuit measurements.

</details>


### [288] [Improving Hardware Requirements for Fault-Tolerant Quantum Computing by Optimizing Error Budget Distributions](https://arxiv.org/abs/2509.02683)
*Tobias Forster,Nils Quetschlich,Mathias Soeken,Robert Wille*

Main category: quant-ph

TL;DR: 利用机器学习自动分配量子纠错的错误预算以降低成本。


<details>
  <summary>Details</summary>
Motivation: 当前的量子硬件易出错，需要量子纠错，但这会带来巨大的额外开销。因此，需要降低这些开销，而错误预算的分配是关键。

Method: 提出一种利用机器学习模型自动分配错误预算的方法，该模型在积累的数据集上进行训练，以确定资源高效的分配。

Result: 该方法能够为超过75%的量子电路降低估计的时空成本，平均降低15.6%，最大降低77.7%。

Conclusion: 通过机器学习自动优化错误预算分配，可以显著降低量子纠错的开销，从而提高量子计算的实用性。

Abstract: Despite significant progress in quantum computing in recent years, executing
quantum circuits for practical problems remains challenging due to error-prone
quantum hardware. Hence, quantum error correction becomes essential but induces
significant overheads in qubits and execution time, often by orders of
magnitude. Obviously, these overheads must be reduced. Since many quantum
applications can tolerate some noise, end users can provide a maximum tolerated
error, the error budget, to be considered during compilation and execution.
This error budget, or, more precisely, its distribution, can be a key factor in
achieving the overhead reduction. Conceptually, an error-corrected quantum
circuit can be divided into different parts that each have a specific purpose.
Errors can happen in any of these parts and their errors sum up to the
mentioned error budget, but how to distribute it among them actually
constitutes a degree of freedom. This work is based on the idea that some of
the circuit parts can compensate for errors more efficiently than others.
Consequently, these parts should contribute more to satisfy the total error
budget than the parts where it is more costly. However, this poses the
challenge of finding optimal distributions. We address this challenge not only
by providing general guidelines on distributing the error budget, but also a
method that automatically determines resource-efficient distributions for
arbitrary circuits by training a machine learning model on an accumulated
dataset. The approach is evaluated by analyzing the machine learning model's
predictions on so far unseen data, reducing the estimated space-time costs for
more than 75% of the considered quantum circuits, with an average reduction of
15.6%, including cases without improvement, and a maximum reduction of 77.7%.

</details>


### [289] [Reversing non-Hermitian skin accumulation with a non-local transverse switch](https://arxiv.org/abs/2509.02686)
*Mengjie Yang,Ching Hua Lee*

Main category: quant-ph

TL;DR: 在非厄米系统中，不对称定向耦合导致边界皮肤态累积。本研究提出了一种通过调整系统尺寸或边界条件来反转皮肤态累积方向的新机制，无需改变耦合。该机制可通过抑制原始方向的放大来增强鲁棒性。在非厄米Kagome晶格中进行了验证，证明了皮肤动力学的可控开关。


<details>
  <summary>Details</summary>
Motivation: 非厄米系统中不对称定向耦合引起的边界皮肤态累积方向可能与耦合方向不符，本研究旨在探索反转皮肤态累积方向的新机制。

Method: 通过调整系统尺寸或边界条件（在不同、横向方向上）来反转皮肤态累积方向，并通过抑制原始方向的放大来增强鲁棒性。在非厄米Kagome晶格中进行了波包模拟验证。

Result: 成功演示了通过调整系统尺寸或边界条件反转皮肤态累积方向的机制，并证明了该机制的鲁棒性。在非厄米Kagome晶格中，实现了非局域横向开关皮肤动力学。

Conclusion: 本研究揭示了不同周期性边界条件方向的光谱环之间的非平凡纠缠可以作为定向放大开关，为非厄米传感和激光领域开辟了新途径。

Abstract: Asymmetrically directed couplings in non-Hermitian systems cause directional
amplification that leads to boundary skin state accumulation. However,
counter-intuitively, the direction of accumulation may not follow that of the
directed couplings. In this work, we demonstrate new mechanisms where this
accumulation can be systematically reversed without modifying the couplings at
all, just by adjusting the system size or boundary conditions in a different,
transverse direction. Moreover, the reversed skin dynamics can be made very
robust by suppressing the amplification in the original non-reversed direction.
We motivate our approach through a series of warm-up models, culminating in a
designed non-Hermitian Kagome lattice whereby wavepacket simulations
demonstrate how robust reversed skin dynamics can be switched on/off in a
non-local transverse manner. Our findings highlight how the non-trivial
entanglement between the spectral loops of different PBC directions can be
harnessed as a directional amplification switch, paving the way for new avenues
of non-Hermitian sensing and lasing.

</details>


### [290] [Quantum DPLL and Generalized Constraints in Iterative Quantum Algorithms](https://arxiv.org/abs/2509.02689)
*Lucas T. Brady,Stuart Hadfield*

Main category: quant-ph

TL;DR: 该论文提出了一种名为迭代量子算法（IQA）的混合量子算法，它借鉴了经典的贪婪或局部搜索算法，并将其扩展到k-局部哈密顿量问题。该算法通过量子计算机提供信息来简化问题，并结合了逻辑推理。作为应用，论文开发了一个混合量子版本的DPLL算法，用于解决可满足性问题，并为具有硬约束的问题提供了通用框架。


<details>
  <summary>Details</summary>
Motivation: 许多量子算法研究倾向于从头开始创建新算法，而忽略了许多已有的、经过优化的经典算法。本研究旨在结合经典算法的性能和运行时间优势，同时探索量子计算的潜在改进。

Method: 提出了一种名为迭代量子算法（IQA）的混合量子算法，该算法借鉴了经典的贪婪或局部搜索算法。该算法的特点是量子计算机提供的信息能够简化后续迭代的问题。研究将该算法扩展到任意k-局部哈密顿量问题，并提出了一个包含逻辑推理的通用框架。作为应用，开发了一个混合量子版本的DPLL算法，并将其嵌入到回溯搜索框架中。

Result: 将IQA扩展到任意k-局部哈密顿量问题，并提出了一个包含逻辑推理的通用框架。开发了一个混合量子版本的DPLL算法，用于解决可满足性问题。为具有硬约束的问题提供了通用框架。展示了算法在某些情况下的退化为经典算法，并提供了量子改进的证据。

Conclusion: 本研究提出的迭代量子算法（IQA）框架，将量子计算的潜力与经典算法的优势相结合，并成功应用于解决可满足性等具有挑战性的问题。该框架为处理具有硬约束的问题提供了通用方法，并为未来的量子算法研究开辟了新的方向。

Abstract: Too often, quantum computer scientists seek to create new algorithms entirely
fresh from new cloth when there are extensive and optimized classical
algorithms that can be generalized wholesale. At the same time, one may seek to
maintain classical advantages of performance and runtime bounds, while enabling
potential quantum improvement. Hybrid quantum algorithms tap into this
potential, and here we explore a class of hybrid quantum algorithms called
Iterative Quantum Algorithms (IQA) that are closely related to classical greedy
or local search algorithms, employing a structure where the quantum computer
provides information that leads to a simplified problem for future iterations.
Specifically, we extend these algorithms beyond past results that considered
primarily quadratic problems to arbitrary k-local Hamiltonians, proposing a
general framework that incorporates logical inference in a fundamental way. As
an application we develop a hybrid quantum version of the well-known classical
Davis-Putnam-Logemann-Loveland (DPLL) algorithm for satisfiability problems,
which embeds IQAs within a complete backtracking based tree search framework.
Our results also provide a general framework for handling problems with hard
constraints in IQAs. We further show limiting cases of the algorithms where
they reduce to classical algorithms, and provide evidence for regimes of
quantum improvement.

</details>


### [291] [Scattering and induced false vacuum decay in the two-dimensional quantum Ising model](https://arxiv.org/abs/2509.02702)
*Luka Pavešić,Marco Di Liberto,Simone Montanger*

Main category: quant-ph

TL;DR: Quantum Ising model scattering in 2D exhibits bound states and resonances, with tensor network simulations exploring elastic and non-perturbative regimes. Symmetry breaking leads to false vacuum decay upon excitation collision, studied via true vacuum bubble propagation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to study scattering phenomena in the 2D quantum Ising model, including the behavior of bound states and resonances in the ordered phase, and to investigate the stability of the false vacuum state when its excitations collide.

Method: The study uses tensor network simulations on a 24x24 lattice to prepare wave packets and evolve the quantum state, exploring various scattering regimes. Symmetry is broken to study false vacuum stability and the subsequent decay process and bubble propagation.

Result: The paper characterizes scattering regimes from elastic to non-perturbative. It finds that breaking spin inversion symmetry and colliding false vacuum excitations can induce a violent decay of the false vacuum, and investigates the spread of the resulting true vacuum bubble.

Conclusion: Scattering in the 2D quantum Ising model is rich, featuring bound states and resonances. The false vacuum state is unstable to excitation collisions, leading to violent decay and the formation/propagation of a true vacuum bubble.

Abstract: We study scattering in the quantum Ising model in two dimensions. In the
ordered phase, the spectrum contains a ladder of bound states and intertwined
scattering resonances, which enable various scattering channels. By preparing
wave packets on a $24 \times 24$ lattice and evolving the state with tensor
networks, we explore and characterize these regimes, ranging from elastic
scattering in the perturbative regime, to non-perturbative processes closer to
the critical point. Then, we break the spin inversion symmetry and study the
stability of the metastable false vacuum state on the collision of its
excitations. We find that a highly-energetic scattering process can induce a
violent decay of the false vacuum, and investigate the spread of the resulting
true vacuum bubble.

</details>


### [292] [Tableau-Based Framework for Efficient Logical Quantum Compilation](https://arxiv.org/abs/2509.02721)
*Meng Wang,Chenxu Liu,Sean Garner,Samuel Stein,Yufei Ding,Prashant J. Nair,Ang Li*

Main category: quant-ph

TL;DR: TQC是一个表格化量子编译器框架，可在不增加额外物理量子比特的情况下，通过操作重排和并行执行来隐藏延迟，将容错量子计算（FTQC）的运行时开销平均降低2.57倍。此外，通过优化量子表格数据结构，TQC在编译优化方面的性能比现有工具提高了1000多倍。


<details>
  <summary>Details</summary>
Motivation: 现有的容错量子计算（FTQC）架构虽然能有效保护量子计算免受错误影响，但通常需要大量的物理量子比特，并且可能引入显著的运行时开销。此外，FTQC电路的复杂性也导致了巨大的编译开销。

Method: TQC（表格化量子编译器）框架通过利用操作可重排性和通过并行执行来隐藏延迟，从而最小化FTQC的运行时开销。此外，通过优化用于稳定器形式主义的表格数据结构，TQC解决了FTQC电路的编译开销问题。

Result: TQC框架将FTQC的运行时开销平均降低了2.57倍，并且在编译优化方面的性能比现有工具提高了1000多倍。

Conclusion: TQC是一个高效的表格化量子编译器框架，能够显著降低容错量子计算的运行时和编译开销，而无需增加额外的物理量子比特。

Abstract: Quantum computing holds the promise of solving problems intractable for
classical computers, but practical large-scale quantum computation requires
error correction to protect against errors. Fault-tolerant quantum computing
(FTQC) enables reliable execution of quantum algorithms, yet they often demand
substantial physical qubit overhead. Resource-efficient FTQC architectures
minimize the number of physical qubits required, saving more than half compared
to other architectures, but impose constraints that introduce up to 4.7$\times$
higher runtime overhead. In this paper, we present TQC, a
\underline{T}ableau-based \underline{Q}uantum \underline{C}ompiler framework
that minimizes FTQC runtime overhead without requiring additional physical
qubits. By leveraging operation reorderability and latency hiding through
parallel execution, TQC reduces FTQC runtime overhead by \textbf{2.57$\times$}
on average.
  Furthermore, FTQC circuits often contain millions of gates, leading to
substantial compilation overhead. To address this, we optimize the core data
structure, the tableau, used in stabilizer formalism. We provide two tailored
versions of the Tableau data type, each designed for different usage scenarios.
These optimizations yield an overall performance improvement of more than
\textbf{1000$\times$} compared to state-of-the-art FTQC optimization tools.

</details>


### [293] [Optimizing Decoherence in the Generation of Optical Schrödinger Cats](https://arxiv.org/abs/2509.02726)
*Hendrik Hegels,Thomas Stolz,Gerhard Rempe,Stephan Dürr*

Main category: quant-ph

TL;DR: 该研究提出使用腔里德堡电磁诱导透明技术生成光学光子的薛定谔猫态，并预测该方法能生成具有较高平均光子数的态，目前技术条件下可达30左右。主要限制在于光子损耗，但可以通过调谐原子自发辐射引起的光子损耗强度来减小退相干。与使用非线性相互作用制备叠加态的方法相比，该方法可以实现更高的平均光子数。


<details>
  <summary>Details</summary>
Motivation: 生成具有较大平均光子数的薛定谔猫态，克服现有技术的局限性。

Method: 利用腔里德堡电磁诱导透明技术，结合调谐原子自发辐射引起的光子损耗强度来控制退相干。

Result: 预测可以生成平均光子数约30的薛定谔猫态，并提出通过调谐光子损耗强度来减小退相干。

Conclusion: 该方法有望通过腔里德堡电磁诱导透明技术，利用现有技术生成具有较高平均光子数的薛定谔猫态，并且能够有效控制退相干。

Abstract: We propose to use cavity Rydberg electromagnetically induced transparency to
generate Schr\"odinger cat states of optical photons. We predict that this
should make it possible to generate states with relatively large mean photon
numbers. With existing technology, mean photon numbers around 30 seem feasible.
The main limitation is photon loss during the process, which generates the
state. The ability to tune the strength of the photon loss caused by atomic
spontaneous emission makes it possible to have little decoherence despite
significant photon loss during the generation of the state.

</details>


### [294] [Entanglement Dimensionality of Continuous Variable States From Phase-Space Quasi-Probabilities](https://arxiv.org/abs/2509.02743)
*Shuheng Liu,Jiajie Guo,Matteo Fadel,Qiongyi He,Marcus Huber,Giuseppe Vitagliano*

Main category: quant-ph

TL;DR: 高维纠缠在量子信息处理中至关重要，本文提出了一种直接利用连续变量（CV）系统来表征纠缠维度的方法，该方法比先离散化系统再进行处理更具鲁棒性和通用性。


<details>
  <summary>Details</summary>
Motivation: 研究直接利用连续变量（CV）量子信息处理工具箱是否能更好地表征无限维系统中的纠缠维度。

Method: 提出了一种基于无限维Bloch算符协方差的连续变量（CV）系统纠缠数见证，可以直接在实验中进行估计。

Result: 直接估计方法比先离散化系统再使用离散变量技术更具鲁棒性和通用性。

Conclusion: 直接利用连续变量（CV）方法进行纠缠维度表征在理论和实验上都具有优势，鼓励进一步发展和实现此类方法。

Abstract: The dimensionality of entanglement is a core tenet of quantum information
processing, especially quantum communication and computation. While it is
natural to think of this dimensionality in finite dimensional systems, many of
the implementations harnessing high Schmidt numbers are actually based on
discretising the observables of continuous variable systems. For those
instances, a core question is whether directly utilizing the toolbox of
continuous variable quantum information processing leads to better and more
robust characterisations of entanglement dimensionality in infinite dimensional
systems. We affirmatively answer this question by introducing Schmidt number
witnesses for CV systems, based directly on covariances of infinite dimensional
Bloch operators that are readily accessible in experiments. We show that the
direct estimation leads to increased robustness and versatility compared to
first discretising the system and using canonical discrete variable techniques,
which provides strong motivation for further developments of genuine CV methods
for the characterization of entanglement dimensionality, as well as for their
implementation in experiments.

</details>


### [295] [Coupling of a nuclear transition to a Surface Acoustic Wave](https://arxiv.org/abs/2509.02750)
*Albert Nazeeri,Chiara Brandenstein,Chengjie Jia,Lorenzo Magrini,Giorgio Gratta*

Main category: quant-ph

TL;DR: Mechanical modulation of recoilless nuclear transitions allows for coherent manipulation of nuclear response by coupling a 57Fe film to a 97.9 MHz surface acoustic wave, creating a comb of absorption sidebands in the M"ossbauer spectrum, demonstrating the fastest phonon-driven control of M"ossbauer resonances to date.


<details>
  <summary>Details</summary>
Motivation: To demonstrate coherent manipulation of nuclear response by controlling recoilless nuclear transitions using mechanical modulation at frequencies well above the nuclear linewidth.

Method: Coupling a film of enriched 57Fe to a 97.9 MHz surface acoustic wave and observing the M"ossbauer spectrum.

Result: Observed a comb of absorption sidebands in the M"ossbauer spectrum, consistent with coherent phase modulation of the nuclear transition. This represents the fastest phonon-driven control of M"ossbauer resonances to date.

Conclusion: Established a new interface between nuclear transitions and high-frequency acoustics using a solid-state, monolithic platform, opening up applications in {\\

Abstract: Mechanical modulation of recoilless nuclear transitions allows the dynamic
control of {\gamma}-ray emission and absorption. Accessing modulation
frequencies well above the nuclear linewidth enables coherent manipulation of
the nuclear response. Here we demonstrate such control by coupling a film of
enriched 57Fe to a 97.9 MHz surface acoustic wave, which is nearly two orders
of magnitude faster than the nuclear linewidth. The mechanical drive produces a
comb of absorption sidebands in the M\"ossbauer spectrum, consistent with
coherent phase modulation of the nuclear transition. This constitutes the
fastest phonon-driven control of M\"ossbauer resonances to date. Our
solid-state, monolithic platform establishes a new interface between nuclear
transitions and high-frequency acoustics, with applications in {\gamma}-ray
quantum optics and precision nuclear spectroscopy.

</details>


### [296] [Geodesics of Quantum Feature Maps on the space of Quantum Operators](https://arxiv.org/abs/2509.02795)
*Andrew Vlasic*

Main category: quant-ph

TL;DR: 本文研究了量子特征映射的黎曼几何结构，提出了一种新的数学框架来分析量子特征编码方案的信息保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有的量子特征选择和编码方案缺乏从几何角度进行信息保留的度量标准。

Method: 本文首先将黎曼流形结构引入量子特征映射的分析，推导了向量空间和度量的闭式方程，并证明了嵌入流形上的测地线与量子编码方案的映上空间之间存在一一对应关系。然后，论文推导了计算曲率的闭式方程。

Result: 通过对庞加莱半平面的子集和两种常用的特征映射进行示例分析，验证了所提出方法的有效性。

Conclusion: 本文为量子特征映射的几何分析提供了新的数学工具，有助于设计更有效的量子机器学习模型。

Abstract: Selecting a quantum feature is an essential step in quantum machine learning.
There have been many proposed encoding schemes and proposed techniques to test
the efficacy of a scheme. From the perspective of information retention, this
paper considers the smooth Riemannian geometry structure of a point cloud and
how an encoding scheme deforms this geometry once mapped to the space of
quantum operators, $\SU(2^N)$. However, a Riemannian manifold structure of the
codomain of a quantum feature map has yet to be formalized. Using a ground-up
approach, this manuscript mathematically establishes a Riemannian geometry for
a general class of Hamiltonian quantum feature maps that are induced from a
Euclidean embedded manifold. For this ground-up approach, we first derive a
closed form of a vector space and a respective metric, and prove there is a 1-1
correspondence from geodesics on the embedded manifold to the codomain of the
encoding scheme. We then rigorously derive closed form equations to calculate
curvature. The paper ends with an example with a subset of the Poincar\'e
half-plane and two well-used feature maps.

</details>


### [297] [Hybrid quantum memory leveraging slow-light and gradient-echo duality](https://arxiv.org/abs/2509.02810)
*Stanisław Kurzyna,Mateusz Mazelanik,Wojciech Wasilewski,Michał Parniak*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We demonstrate a hybrid quantum memory that combines Gradient Echo Memory
(GEM) and Electromagnetically Induced Transparency (EIT) protocols for
reversible mapping between light and atomic coherence. By leveraging GEM and
EIT complementarity, we realize time-to-frequency and frequency-to-time
conversion mechanisms for spectro-temporal modes. This capability provides a
versatile tool for quantum communication, where coherent frequency-time
conversion enhances network interoperability. In addition, the protocol may
enable fundamental studies of atomic coherence, including investigations of
Rydberg polaritons and mapping of single Rydberg excitations and ionic
impurities.

</details>


### [298] [Characterizing a high-dimensional unitary transformation without measuring the qudit it transforms](https://arxiv.org/abs/2509.02816)
*Salini Rajeev,Mayukh Lahiri*

Main category: quant-ph

TL;DR: 提供一种无需探测量子比特即可重建任意高维酉变换的方法，并以光子的轨道角动量态为例进行了演示。


<details>
  <summary>Details</summary>
Motivation: 当无法获得用于酉变换的量子比特的合适探测器时，该方法具有实用价值。

Method: 利用路径识别实现量子干涉。

Result: 成功地使用光子的轨道角动量态演示了该方法。

Conclusion: 所提出的方法可以在缺乏合适探测器的情况下，有效地重建酉变换。

Abstract: We present a method for reconstructing an arbitrary high-dimensional unitary
transformation without detecting the qudit that it transforms. We demonstrate
the method using orbital angular momentum states of light. Our method relies on
quantum interference enabled by path identity of undetected photons. The method
is practically useful when suitable detectors are not available for the qudit
on which the unitary transformation works.

</details>


### [299] [Proposal for High-Dimensional Entanglement Swapping with Linear Optics and Ancillary Photons](https://arxiv.org/abs/2509.02817)
*Baghdasar Baghdasaryan,Kaushik Joarder,Fabian Steinlechner*

Main category: quant-ph

TL;DR: 提出了一种基于线性光学的多维（最高六维）量子纠缠交换方案，并设计了四维超纠缠实现方案。


<details>
  <summary>Details</summary>
Motivation: 高维量子纠缠交换在量子网络中具有噪声鲁棒性、抗窃听能力和信息容量方面的优势，但现有技术难以实现。

Method: 提出了一种利用辅助光子的线性光学设置来实现多维量子纠缠交换，并提出了一个使用超纠缠的四维方案。

Result: 该方案可扩展至六维，并适用于任何光子自由度。

Conclusion: 所提出的方案为实现高维量子网络提供了可行的技术路径。

Abstract: Quantum teleportation and entanglement swapping are fundamental building
blocks for realizing global quantum networks. Both protocols have been
extensively studied and experimentally demonstrated using two-dimensional qubit
systems. Their implementation in high-dimensional (HD) systems offers
significant advantages, such as enhanced noise resilience, greater robustness
against eavesdropping, and increased information capacity. Generally, HD
photonic entanglement swapping requires ancilla photons or strong nonlinear
interactions. However, specific experimental implementations that are feasible
with current technology remain elusive. Here, we present a linear optics-based
setup for HD entanglement swapping for up to six dimensions using ancillary
photons. This scheme is applicable to any photonic degrees of freedom.
Furthermore, we present an experimental design for a four-dimensional
implementation using hyper-entanglement, which includes the necessary
preparation of the ancilla state and analysis of the resulting swapped state.

</details>


### [300] [Quasi-van Hove singularities informed approach improving DOS/pDOS predictions in GNN](https://arxiv.org/abs/2509.02818)
*Grigory Koroteev*

Main category: quant-ph

TL;DR: 该研究提出了一种将范霍夫奇点概念延伸并应用于机器学习模型的方法，以利用其作为先验信息。


<details>
  <summary>Details</summary>
Motivation: 将范霍夫奇点概念显式地应用于机器学习模型，以利用其作为先验信息，从而提高模型预测质量。

Method: 通过机器学习方法估计量子态密度，而不是直接计算，然后利用此额外信息进行后处理。提出了一种与Fisher信息相当的正则化机制来重新分配系统信息。

Result: 通过将范霍夫奇点作为先验信息，并结合机器学习估计和正则化后处理，可以显著提高模型预测的质量，效果类似于高阶范霍夫奇点。

Conclusion: 将范霍夫奇点概念的延伸应用于机器学习模型，并通过正则化机制进行信息重分布，可以有效提高模型的预测质量。

Abstract: In this paper, we introduce an extension of the concept of Van Hoff
singularities in order to explicitly use them in machine learning methods of
models as a priori information. The claimed method becomes possible when,
instead of directly calculating the density of quantum states, we operate with
estimates obtained by machine learning methods. Then, determining additional
information allows us to organize post-processing, which significantly improves
the quality of model prediction. Analytically, the effect is similar to Van
Hove singularities of high order due to the additional degree of degeneracy of
the system. As a mechanism for redistributing information about the system, we
propose regularization, which is close in meaning to taking into account Fisher
information.

</details>


### [301] [Learning AC Power Flow Solutions using a Data-Dependent Variational Quantum Circuit](https://arxiv.org/abs/2509.03495)
*Thinh Viet Le,Md Obaidur Rahman,Vassilis Kekatos*

Main category: quant-ph

TL;DR: 利用变分量子计算（VQC）加速交流潮流（AC PF）问题求解。


<details>
  <summary>Details</summary>
Motivation: 需要解决大量的AC PF问题实例来模拟电网的能源转型。

Method: 将AC PF问题视为VQC的可训练参数（权重）的非线性最小二乘拟合，并使用混合经典/量子计算方法求解；将PF规范作为特征输入数据嵌入的VQC，训练量子机器学习（QML）模型来预测通用的PF解；利用网络图结构开发测量AC-PF量子可观测量的新协议。

Result: VQC模型在预测性能上优于深度神经网络，且使用的权重少得多。

Conclusion: 提出的量子AC-PF框架为通过量子计算解决更复杂的电网任务奠定了基础。

Abstract: Interconnection studies require solving numerous instances of the AC load or
power flow (AC PF) problem to simulate diverse scenarios as power systems
navigate the ongoing energy transition. To expedite such studies, this work
leverages recent advances in quantum computing to find or predict AC PF
solutions using a variational quantum circuit (VQC). VQCs are trainable models
that run on modern-day noisy intermediate-scale quantum (NISQ) hardware to
accomplish elaborate optimization and machine learning (ML) tasks. Our first
contribution is to pose a single instance of the AC PF as a nonlinear
least-squares fit over the VQC trainable parameters (weights) and solve it
using a hybrid classical/quantum computing approach. The second contribution is
to feed PF specifications as features into a data-embedded VQC and train the
resultant quantum ML (QML) model to predict general PF solutions. The third
contribution is to develop a novel protocol to efficiently measure AC-PF
quantum observables by exploiting the graph structure of a power network.
Preliminary numerical tests indicate that the proposed VQC models attain
enhanced prediction performance over a deep neural network despite using much
fewer weights. The proposed quantum AC-PF framework sets the foundations for
addressing more elaborate grid tasks via quantum computing.

</details>


### [302] [Theory of dynamical superradiance in organic materials](https://arxiv.org/abs/2509.03067)
*Lukas Freter,Piper Fowler-Wright,Javier Cuerda,Brendon W. Lovett,Jonathan Keeling,Päivi Törmä*

Main category: quant-ph

TL;DR: 动力学超辐射理论可应用于有机材料，并可能在振动耦合存在时增强，尤其是在负腔失谐下。


<details>
  <summary>Details</summary>
Motivation: 研究有机材料中，电子态与振动模式耦合时，动力学超辐射的集体能量交换现象。

Method: 采用两种模型：1. 马尔可夫浴模型，通过纯粹退相干项处理振动；2. Holstein-Tavis-Cummings哈密顿量，直接包含振动模式。利用置换对称性和弱U(1)对称性，开发数值方法求解包含局域耗散的Tavis-Cummings模型，最多可处理140个发射体。

Result: 验证了平均场和二阶累积量近似的准确性，并用它们来描述大量发射体。分析了平均腔光子数、电子相干性和布洛赫矢量长度的动力学。结果表明，振动模式耦合的影响不止于简单的退相相干。

Conclusion: 超辐射在振动模式耦合存在时是可能的，并且在负腔失谐下，振动耦合甚至可以增强超辐射。光子数上升时间相对于腔频率失谐的不对称性是振动辅助超辐射的一个可实验测量的标志。

Abstract: We develop the theory of dynamical superradiance -- the collective exchange
of energy between an ensemble of initially excited emitters and a single-mode
cavity -- for organic materials where electronic states are coupled to
vibrational modes. We consider two models to capture the vibrational effects:
first, vibrations treated as a Markovian bath for two-level emitters, via a
pure dephasing term in the Lindblad master equation for the system; second,
vibrational modes directly included in the system via the
Holstein--Tavis--Cummings Hamiltonian. By exploiting the permutation symmetry
of the emitters and weak U(1) symmetry, we develop a numerical method capable
of exactly solving the Tavis-Cummings model with local dissipation for up to
140 emitters. Using the exact method, we validate mean-field and second-order
cumulant approximations and use them to describe macroscopic numbers of
emitters. We analyse the dynamics of the average cavity photon number,
electronic coherence, and Bloch vector length, and show that the effect of
vibrational mode coupling goes beyond simple dephasing. Our results show that
superradiance is possible in the presence of vibrational mode coupling; for
negative cavity detunings, the vibrational coupling may even enhance
superradiance. We identify asymmetry of the photon number rise time as a
function of the detuning of the cavity frequency as an experimentally
accessible signature of such vibrationally assisted superradiance.

</details>


### [303] [QNPU: Quantum Network Processor Unit for Quantum Supercomputers](https://arxiv.org/abs/2509.02827)
*Peiyi Li,Chenxu Liu,Ji Liu,Huiyang Zhou,Ang Li*

Main category: quant-ph

TL;DR: QNPU是未来量子超级计算机的关键组成部分，它通过管理量子通信协议来扩展单个量子处理器的能力。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的进步，需要可扩展的解决方案来解决大规模计算问题。量子超级计算机是使多个量子处理器能够有效协作以解决大规模计算问题的下一个前沿。量子超级计算机的出现需要一个有效的接口来管理量子处理器之间的量子通信协议。

Method: 提出了一种名为量子网络处理单元（QNPU）的接口，它与量子处理单元（QPU）协同工作，QNPU负责节点之间的量子通信。设计了一个全面的指令集架构（ISA），并通过管理EPR资源、量子操作和经典通信的微操作来实现。引入了DistQASM来扩展OpenQASM，并提出了一种具有标量和超标量QNPU设计的微架构，以增强通信密集型量子工作负载的性能。

Result: QNPU显著提高了量子节点之间通信的效率。

Conclusion: QNPU是未来量子超级计算机的关键组成部分，它通过管理量子通信协议来扩展单个量子处理器的能力。

Abstract: As quantum computing progresses, the need for scalable solutions to address
large-scale computational problems has become critical. Quantum supercomputers
are the next upcoming frontier by enabling multiple quantum processors to
collaborate effectively to solve large-scale computational problems. The
emergence of quantum supercomputers necessitates an efficient interface to
manage the quantum communication protocols between quantum processors. In this
paper, we propose the Quantum Network Processing Unit (QNPU), which enables
quantum applications to efficiently scale beyond the capacity of individual
quantum processors, serving as a critical building block for future quantum
supercomputers. The QNPU works alongside the Quantum Processing Unit (QPU) in
our decoupled processing units architecture, where the QPU handles local
quantum operations while the QNPU manages quantum communication between nodes.
We design a comprehensive instruction set architecture (ISA) for the QNPU with
high-level communication protocol abstractions, implemented via
micro-operations that manage EPR resources, quantum operations, and classical
communication. To facilitate programming, we introduce DistQASM, which extends
OpenQASM with distributed quantum operations. We then propose a
microarchitecture featuring both scalar and superscalar QNPU designs to enhance
performance for communication-intensive quantum workloads. Finally, we evaluate
the performance of our proposed QNPU design with distributed quantum workloads
and demonstrate that the QNPU significantly improves the efficiency of
communication between quantum nodes, paving the way for quantum supercomputing.

</details>


### [304] [Ultrafast single-photon interference with a dipole qubit in a nanocavity](https://arxiv.org/abs/2509.03428)
*Athul S. Rema,Adrián E. Rubio López,Felipe Herrera*

Main category: quant-ph

TL;DR: 研究了激发的偶极子在等离激元纳米腔近场中相干动力学，并提出了洛伦兹核近似方法。


<details>
  <summary>Details</summary>
Motivation: 填补了现有研究对激发的偶极子在等离激元纳米腔近场中相干动力学研究的空白。

Method: 利用洛伦兹核近似方法求解描述偶极子-光子动力学的时间相关薛定谔方程，该方法将纳米腔场的宽带特性编码为非马尔可夫记忆核。

Result: 在强耦合下，单光子概率密度在频域内从初始的局域源演化为拉比双峰，该演化过程发生在由核频谱决定的时间尺度上。该动力学转变伴随着在频域和时域中形成单光子干涉图样，并以 $\sim 100-150$ fs 的时间尺度相干传播。研究还表明，通过相干脉冲驱动可以调控耦合系统的稳态频谱，并提出了纳米腔的通用强耦合判据。

Conclusion: 提出的洛伦兹核近似方法能够有效地研究偶极子在纳米腔中的相干动力学，并为理解和调控光与物质的相互作用提供了新的视角。

Abstract: The stationary spectrum of individual dipole emitters in plasmonic
nanocavities has been studied for a range of cavity geometries and dipole
configurations. Less is known about the coherent dynamics of single photon
creation in the nanocavity near field by an excited dipole. We address this gap
by developing a Lorentzian kernel approximation that solves the time-dependent
Schr\"odinger equation that describes the coupled dipole-photon dynamics in the
single-excitation manifold. Our approach encodes the broadband nature of the
nanocavity field through a non-Markovian memory kernel, derived from
macroscopic QED theory. For a two-level dipole near a metallic nanosphere, we
show that the single photon probability density in frequency space evolves in
strong coupling from an initially localized source at the qubit frequency into
a Rabi doublet over a timescale governed by the kernel spectrum. This dynamical
crossover is accompanied by the formation of single-photon interference
patterns in frequency and time, propagating coherently over a timescale limited
by the shape of kernel spectrum to $\sim 100-150$ fs, which is accessible to
ultrafast spectroscopy. We also show that the stationary spectrum of the
coupled system can be manipulated by driving the nanocavity field using
coherent pulses with variable spectral bandwidth. Using single-photon pulses
narrower than the kernel spectrum, the Rabi splitting in a system that supports
strong coupling can be effectively removed. The applicability of our results to
other dipole-nanocavity configurations is discussed and a general strong
coupling criterion for nanocavities is formulated.

</details>


### [305] [From negligible to neglecton: making Ising anyons braiding universal](https://arxiv.org/abs/2509.02843)
*Filippo Iulianelli,Sung Kim,Joshua Sussan,Aaron D. Lauda*

Main category: quant-ph

TL;DR: 通过添加“忽略子”并利用其非半单拓扑量子场论，我们提出了一种仅通过编织即可实现通用拓扑量子计算的新框架，该框架在表示中具有酉性，并在正定子空间中实现了低泄漏的演化。


<details>
  <summary>Details</summary>
Motivation: 当前的拓扑量子计算方法，特别是使用标准伊辛任意子，由于仅限于克利福德运算，在实现通用计算方面存在局限性。因此，需要一种新的方法来扩展这些能力的边界。

Method: 本研究提出了一种利用伊辛任意子的非半单拓扑量子场论的新框架。通过引入一种称为“忽略子”的新型任意子，扩展了伊辛理论。这种扩展使得仅通过编织操作即可实现通用量子计算。此外，研究还探讨了这些操作产生的编织群表示的酉性，并提出了将计算子空间嵌入到正定扇区的方法。

Result: 该框架通过添加单个忽略子，实现了仅通过编织即可进行通用量子计算。编织群表示被证明是酉的，计算子空间嵌入在正定扇区内。研究确定了支持通用门集的参数范围，并特别指出了忽略子理论与传统伊辛理论的计算能力存在显著差异，尤其是在参数$\\(alpha)$的特殊值下，计算子空间可以从希尔伯特空间的负范数分量中精确解耦。

Conclusion: 通过引入非半单拓扑量子场论和忽略子，我们为仅通过编织实现通用拓扑量子计算提供了一种新颖且有前途的途径。该方法不仅克服了传统伊辛任意子的局限性，还通过在正定子空间中实现低泄漏、完全酉的演化，为高效的门编译提供了更好的前景。

Abstract: We introduce a new framework for achieving universal topological quantum
computation using Ising anyons via braiding alone. Our approach builds on
recent developments in non-semisimple topological quantum field theories in
(2+1)-dimensions, which extend the traditional semisimple paradigm. While
standard Ising anyons are limited to Clifford operations and thus not
computationally universal, we show that their non-semisimple counterparts give
rise to new anyon types we call \emph{neglectons}; adding a single neglecton to
the traditional Ising theory enables universal quantum computation through
braiding alone.
  The resulting braid group representations are unitary with respect to an
indefinite Hermitian form, with the computational subspace embedded in a
positive-definite sector. We identify a range of parameters for which the model
supports universal gate sets and, notably, special values of the parameter
$\alpha$ where the computational subspace decouples exactly from the
negative-norm components of the Hilbert space. This leads to low-leakage, fully
unitary evolution on the physical subspace and improved prospects for efficient
gate compilation.

</details>


### [306] [Fluid Model of Schrodinger equation and derivation of the quantum potential](https://arxiv.org/abs/2509.02868)
*Lachezar Simeonov*

Main category: quant-ph

TL;DR: 通过引入两种相互作用的流体以及它们之间的相互扩散过程，我们构建了一个能够自然地导出马德隆方程（包括量子势）的模型，并从该模型导出了玻姆力学和玻恩法则。


<details>
  <summary>Details</summary>
Motivation: 马德隆方程中的量子势被认为既复杂又不自然，使得构建具有此类特性的流体模型成为一项艰巨的任务。

Method: 提出一个包含两种相互作用流体及其相互扩散过程的模型，并将粒子视为该流体中的点状奇异点，其运动速度与局部流体速度相同。

Result: 该模型自然地导出了马德隆流体方程（包含量子势），并直接推导出了玻姆力学的引导方程和玻恩法则。

Conclusion: 粒子可以被视为流体的一部分，是一种奇异点，并且该模型能够解释玻姆力学和玻恩法则。

Abstract: It is quite familiar that Schr\"{o}dinger equation can be rewritten in terms
of fluid-type equations, called Madelung equations. However one term in these
fluid equations, the so called `quantum potential` is quite complicated.
Indeed, D\"{u}rr complains that the quantum potential is `neither simple nor
natural` \cite{Durr1996}. Thus it may appear a hopeless task to imagine a fluid
with such properties that would lead to the Madelung equations, and more
specifically, to the quantum potential. We prove the opposite. We make a model
with \textit{two} interacting fluids, instead of one. Then we consider a
process of mutual diffusion between the fluids. This leads quite naturally to
the Madelung fluid equations \textit{including} the quantum potential for one
of the fluids. In addition, we model the particle as a point-like object, that
moves inside this particular fluid. We require that it moves with the
\textit{same} velocity as the \textit{local velocity of this fluid}. Then the
guiding equation of Bohmian mechanics follows directly. From this equation we
derive Born's rule. Our interpretation of that result is that \textit{the
particle is part of the fluid, a kind of singularity in it}. Finally, we answer
several possible objections against the model.

</details>


### [307] [Product-State Manifolds for M Quantum Systems with N Levels using the Fano form and the Induced Euclidean Metric](https://arxiv.org/abs/2509.02891)
*Fotios D. Oikonomou*

Main category: quant-ph

TL;DR: 该研究探索了量子力学中可分态（积态）与几何结构（流形及其度量）之间的联系，并计算了特定情况下的黎曼几何量（列维-奇维塔联络和标量曲率）。


<details>
  <summary>Details</summary>
Motivation: 量子纠缠在量子信息科学中至关重要，本研究旨在探索积态与几何结构之间的联系。

Method: 研究关注由N能级M系统形成的积态流形，并考察由欧几里得度量诱导的度量。计算了初等情况下的列维-奇维塔联络和标量曲率。

Result: 计算了初等情况下的列维-奇维塔联络，并在计算可行的情况下，计算了标量曲率。

Conclusion: 积态可以被视为流形，其几何属性（如度量）与量子力学的可分性有关。

Abstract: In quantum mechanics separable states can be characterized as convex
combinations of product states whereas non-separable states exhibit
entanglement. Quantum entanglement has played a pivotal role in both
theoretical investigations and practical applications within quantum
information science. In this study, we explore the connection between product
states and geometric structures, specifically manifolds and their associated
geometric properties such as the first fundamental form (metric). We focus on
the manifolds formed by the product states of M systems of N levels, examining
the induced metric derived from the Euclidean metric. For elementary cases we
will compute the Levi-Civita connection, and, where computationally tractable,
the scalar curvature.

</details>


### [308] [Control of molecular rotation in helium nanodroplets with an optical centrifuge](https://arxiv.org/abs/2509.02913)
*Ian MacPhail-Bartley,Alexander A. Milner,Frank Stienkemeier,Valery Milner*

Main category: quant-ph

TL;DR: 通过飞秒激光脉冲的“光学离心机”效应，实现了对氦纳米液滴中（NO）2分子的选择性量子态制备，并首次在超流氦环境中实现了对分子转子的精确操控。


<details>
  <summary>Details</summary>
Motivation: 在超流氦环境中研究分子动力学，以及超流体与原子级别缺陷的相互作用。

Method: 使用光学离心机控制飞秒激光脉冲，实现对（NO）2分子转子的定向旋转，并通过测量其随时间变化的取向度来研究其动力学。

Result: 实现了对（NO）2分子的定向旋转，观察到了其在非平衡态下的弛豫动力学过程，并证明了其旋转频率与外场的转动频率一致。

Conclusion: 证实了利用光学离心机操控超流氦环境中分子转子动力学的可行性，并为进一步研究超流氦与分子相互作用提供了新的途径。

Abstract: We experimentally demonstrate that the rotation of molecules embedded in
helium nanodroplets can be controlled with an optical centrifuge, allowing for
the study of molecular dynamics inside the strongly interacting many-body
environment of superfluid helium at variable levels of rotational excitation.
By doping the droplets with dimers of nitric oxide, (NO)$_2$, and measuring the
degree of their centrifuge-induced alignment as a function of time, we show
both the forced in-field rotation of molecules at arbitrary frequencies, as
well as the field-free resonant rotation with a long nanosecond-scale decay.
The ability to control and monitor the rotational dynamics of molecular rotors
inside the superfluid environment may shed new light on superfluidity and the
interaction of superfluids with defects at the atomic level.

</details>


### [309] [Entanglement Complexity in Many-body Systems from Positivity Scaling Laws](https://arxiv.org/abs/2509.02944)
*Anna O. Schouten,David A. Mazziotti*

Main category: quant-ph

TL;DR: 该论文提出了一种基于p粒子密度矩阵（RDM）理论的p粒子正定性条件框架，该框架与区域定律互补，可以作为量子多体模拟的有效性度量。


<details>
  <summary>Details</summary>
Motivation: 提供一种比区域定律更直接的计算复杂性度量方法，用于评估量子多体模拟的效率。

Method: 引入基于p粒子密度矩阵（RDM）理论的p粒子正定性条件，并证明了其与计算复杂性之间的关系。

Result: 证明了如果一个量子系统在p粒子正定性上是可解的，且其复杂度与系统大小无关，那么其纠缠复杂度将随着p呈多项式增长。

Conclusion: 该框架将RDM的结构约束与计算可处理性联系起来，为判定多体方法（包括RDM方法）能否有效模拟量子多体系统和材料提供了理论依据。

Abstract: Area laws describe how entanglement entropy scales and thus provide important
necessary conditions for efficient quantum many-body simulation, but they do
not, by themselves, yield a direct measure of computational complexity. Here we
introduce a complementary framework based on $p$-particle positivity conditions
from reduced density matrix (RDM) theory. These conditions form a hierarchy of
$N$-representability constraints for an RDM to correspond to a valid
$N$-particle quantum system, becoming exact when the Hamiltonian can be
expressed as a convex combination of positive semidefinite $p$-particle
operators. We prove a general complexity bound: if a quantum system is solvable
with level-$p$ positivity independent of its size, then its entanglement
complexity scales polynomially with order $p$. This theorem connects structural
constraints on RDMs with computational tractability and provides a rigorous
framework for certifying when many-body methods including RDM methods can
efficiently simulate correlated quantum matter and materials.

</details>


### [310] [Quantum optical reset with classical memory](https://arxiv.org/abs/2509.02980)
*Evgeniy O. Kiktenko,Oleg M. Sotnikov,Ilia A. Iakovlev,Yuri A. Biriukov,Aleksey K. Fedorov,Stanislav S. Straupe,Ivan V. Dyakonov,Vladimir V. Mazurenko*

Main category: quant-ph

TL;DR: 该研究提出了光学量子计算中的动态量子电路模型，利用时间-仓自环干涉仪实现光学重置，并结合经典设备存储测量历史，以降低未来测量的结果不确定性。


<details>
  <summary>Details</summary>
Motivation: 介绍动态量子电路模型在光学平台上的实现，以及其在降低测量不确定性和提高灵活性方面的潜力。

Method: 利用时间-仓自环干涉仪实现光学重置，并结合存储测量历史的经典设备。

Result: 成功实现了光学重置，并展示了该模型在降低未来测量不确定性方面的潜力，同时揭示和表征了信息流。

Conclusion: 光学重置模型为在光学平台上构建动态量子电路提供了新的途径，并具有前所未有的灵活性。

Abstract: Dynamic quantum circuits generate states that depend on the measurement
results obtained during circuit execution. To date such a quantum computing
model has mainly been implemented with qubit-based superconducting hardware
utilizing reset operations and classical logic. Here we develop a model of
optical reset by using time-bin self-looped interferometers demonstrated in
recent experiments. Synchronizing the optical reset with a simple classical
device storing history of measurement results allows one to decrease
uncertainty of future measurements, which suggests unprecedented flexibility in
constructing dynamical circuits on optical platforms. We reveal and
characterize information flow in the proposed reset model with distinct
information-theoretic measures.

</details>


### [311] [Identifiability and minimality bounds of quantum and post-quantum models of classical stochastic processes](https://arxiv.org/abs/2509.03004)
*Paul M. Riechers,Thomas J. Elliott*

Main category: quant-ph

TL;DR: 比较经典随机过程的两种模型（经典、量子或“后量子”）是否产生相同的可观察行为，方法是将它们映射到标准的“广义”隐藏马尔可夫模型，并为量子模型生成经典随机过程所需的最小维度设置界限。


<details>
  <summary>Details</summary>
Motivation: 确定两种不同的模型是否产生相同的可观察行为，即辨识问题，尤其是在量子模型用于生成经典随机过程的场景下。

Method: 将两种模型（经典、量子或“后量子”）映射到一个标准的“广义”隐藏马尔可夫模型，用于比较它们。

Result: 解决了量子模型生成经典随机过程的辨识问题，并为量子模型所需的最小维度提供了（有时是紧密的）界限。

Conclusion: 该方法能够比较任何两种经典随机过程的模型，无论是经典的、量子的还是“后量子的”，并确定量子模型生成给定经典随机过程所需的最小维度。

Abstract: To make sense of the world around us, we develop models, constructed to
enable us to replicate, describe, and explain the behaviours we see. Focusing
on the broad case of sequences of correlated random variables, i.e., classical
stochastic processes, we tackle the question of determining whether or not two
different models produce the same observable behavior. This is the problem of
identifiability. Curiously, the physics of the model need not correspond to the
physics of the observations; recent work has shown that it is even advantageous
-- in terms of memory and thermal efficiency -- to employ quantum models to
generate classical stochastic processes. We resolve the identifiability problem
in this regime, providing a means to compare any two models of a classical
process, be the models classical, quantum, or `post-quantum', by mapping them
to a canonical `generalized' hidden Markov model. Further, this enables us to
place (sometimes tight) bounds on the minimal dimension required of a quantum
model to generate a given classical stochastic process.

</details>


### [312] [Chirality-Induced Orbital-Angular-Momentum Selectivity in Electron Transmission and Scattering](https://arxiv.org/abs/2509.02997)
*Yun Chen,Oded Hod,Joel Gersten,Abraham Nitzan*

Main category: quant-ph

TL;DR: 电子在手性介质中的轨道角动量选择性（CIOAMS）


<details>
  <summary>Details</summary>
Motivation: 研究手性诱导的轨道角动量（OAM）选择性在电子传输和散射过程中的作用，并探讨其在量子技术中的潜在应用。

Method: 通过电子波包传播和时间相关的薛定谔方程研究OAM极化，并利用带相反OAM的波包与腐蚀表面散射的空间分辨率来演示OAM的选择性。

Result: 证明了CIOAMS在电子传输和散射过程中的存在，并表明OAM可能在手性诱导的自旋选择性（CISS）机制中发挥重要作用，与Mott极化法测量结果一致。

Conclusion: CIOAMS在电子传输和散射过程中具有重要意义，并有望在未来的量子技术中得到应用。

Abstract: Chirality-induced orbital-angular-momentum selectivity (CIOAMS) in electron
transmission and scattering processes is investigated. Polarization of the OAM
of an electron traversing chiral media is first studied via electronic
wavepacket propagation using the time-dependent Schr\"odinger equation. Next,
spatial resolution of wavepackets carrying opposite OAM, following scattering
from a corrugated surface is demonstrated. This suggests that OAM may play a
significant role in the mechanisms underlying chirality induced spin
selectivity, measured for electrons crossing chiral media in setups involving
Mott polarimetry. Our results highlight the potential to exploit CIOAMS in
innovative emerging quantum technologies.

</details>


### [313] [Quantum Resource Theory of Deficiency and Operational Applications in Subchannel Discrimination](https://arxiv.org/abs/2509.03043)
*Sunho Kim,Chunhe Xiong,Junde Wu*

Main category: quant-ph

TL;DR: 该研究提出了一种新的框架，用于定义给定状态相对于最大资源状态在物理任务中的资源缺陷，并为量子相干性和纠缠提供了几何度量。


<details>
  <summary>Details</summary>
Motivation: 为量子资源提供可操作的意义，使其在特定物理任务中优于无资源状态。

Method: 提出了一种新的框架来定义资源缺陷，并设计了一个几何度量来量化这种缺陷。

Result: 提出的几何度量满足量子相干性和纠缠的条件，并量化了在子通道判别任务中给定状态与最大资源状态相比的最小劣势。

Conclusion: 该框架扩展了量子资源理论的范围，并提供了更全面的可操作性解释，特别是通过其几何度量。

Abstract: A central challenge in quantum resource theory is to provide operational
meaning to quantum resources that offer distinct advantages over the convex set
of resource-free states in specific physical tasks. We propose a novel
framework to define the resource deficiency of a given state relative to the
set of maximal resource states in physical tasks. The proposed geometric
measure satisfies this framework's conditions for both quantum coherence and
entanglement, and it precisely quantifies the minimal disadvantage of a given
state compared to maximal resource states in subchannel discrimination under
certain conditions. These extensions and new interpretations broaden the scope
of quantum resource theories and provide more comprehensive operational
interpretations.

</details>


### [314] [Effects of intrinsic decoherence in multipartite system subjected to Kerr effect and parametric amplification with quantum correlations and estimation](https://arxiv.org/abs/2509.03073)
*M. Ibrahim,S. Jamal Anwar,S. Abdel-Khalek,M. Ramzan,M. Khalid Khan*

Main category: quant-ph

TL;DR: 量子关联在多两能级系统与单模Fock场相互作用中的作用，考虑了非线性效应、参量放大和退相干。


<details>
  <summary>Details</summary>
Motivation: 研究多两能级系统与单模Fock场相互作用时，量子关联（全局量子散度、量子Fisher信息）与模型参数（Kerr非线性效应、参量放大、退相干）之间的相互作用。

Method: 使用全局量子散度和量子Fisher信息来量化量子关联，并分析其在包含Kerr非线性效应、参量放大和内在退相干的多两能级系统与单模Fock场相互作用模型中的行为。

Result: （摘要中未明确说明具体结果，但暗示了研究了量子关联如何随模型参数变化而变化）。

Conclusion: （摘要中未明确说明结论，但研究旨在理解这些量子关联度量在所提出的模型中的行为和影响）。

Abstract: We investigate the interplay between quantum correlations, quantified by the
global quantum discord and quantum Fisher information in a multi two-level
system interacting with a single mode Fock field. Our model incorporates Kerr
like non linearity effects, parametric amplification and intrinsic decoherence.

</details>


### [315] [Quantum Computer Benchmarking: An Explorative Systematic Literature Review](https://arxiv.org/abs/2509.03078)
*Tobias Rohe,Federico Harjes Ruiloba,Sabrina Egger,Sebastian von Beck,Jonas Stein,Claudia Linnhoff-Popien*

Main category: quant-ph

TL;DR: 本篇论文提出了一个量化计算基准测试的分类法，用于组织和评估该领域的进展。


<details>
  <summary>Details</summary>
Motivation: 随着量化计算的发展，需要一个系统性的方法来衡量进展、发现瓶颈并评估社区的努力。然而，现有的基准测试方法未能满足不同利益相关者的多样化需求。

Method: 结合自然语言处理（NLP）的聚类和专家分析，进行了一项系统的文献综述，以开发一种新的量化计算基准测试分类法。该分类法将基准测试分为独立于硬件、软件和应用程序的类别，并对协议进行了分层分类，同时制定了标准术语并绘制了类别间的相互依赖关系。

Result: 研究提出了一个全面的量化计算基准测试分类法，将基准测试分为硬件、软件和应用程序等类别，并进行了分层分类。该研究还制定了标准术语，绘制了类别间的相互依赖关系，揭示了设计模式，指出了研究差距，并阐明了基准测试如何服务于不同的利益相关者。

Conclusion: 通过对量化计算基准测试领域的结构化和通用语言的提供，这项工作为量化计算基准测试的连贯发展、更公平的评估和更强的跨学科合作奠定了基础。

Abstract: As quantum computing (QC) continues to evolve in hardware and software,
measuring progress in this complex and diverse field remains a challenge. To
track progress, uncover bottlenecks, and evaluate community efforts, benchmarks
play a crucial role. But which benchmarking approach best addresses the diverse
perspectives of QC stakeholders? We conducted the most comprehensive systematic
literature review of this area to date, combining NLP-based clustering with
expert analysis to develop a novel taxonomy and definitions for QC
benchmarking, aligned with the quantum stack and its stakeholders. In addition
to organizing benchmarks in distinct hardware, software, and application
focused categories, our taxonomy hierarchically classifies benchmarking
protocols in clearly defined subcategories. We develop standard terminology and
map the interdependencies of benchmark categories to create a holistic, unified
picture of the quantum benchmarking landscape. Our analysis reveals recurring
design patterns, exposes research gaps, and clarifies how benchmarking methods
serve different stakeholders. By structuring the field and providing a common
language, our work offers a foundation for coherent benchmark development,
fairer evaluation, and stronger cross-disciplinary collaboration in QC.

</details>


### [316] [Generation and manipulation of photon number wave packets in photonic cavities](https://arxiv.org/abs/2509.03083)
*Luca Nimmesgern,Moritz Cygorek,Doris E. Reiter,Vollrath Martin Axt*

Main category: quant-ph

TL;DR: 利用改变驱动强度的方法，可以生成并控制量子腔中的光子数波包，并通过测量平均光子数来研究其动力学行为。


<details>
  <summary>Details</summary>
Motivation: 量子腔中的量子发射体可以产生光子数波包，但其生成和演化机制尚不明确，需要进一步研究以实现可控生成和应用。

Method: 通过快速改变驱动强度来生成新的光子数波包，并分析其动力学行为，将其分为不同的动力学子类，并设计生成指定数量光子数波包的协议。

Result: 实现了对光子数波包的生成和控制，发现了多种动力学演化模式，并提出了按需生成多个光子数波包的协议。

Conclusion: 光子数波包的动力学行为可以通过改变驱动强度来丰富和控制，为量子信息处理提供了新的可能性。提出的协议简单且鲁棒，有望通过测量平均光子数进行实验验证。

Abstract: Quantum emitters inside optical cavities can create not only fixed photon
number states but also photon number wave packets, which are states with a
finite photon number distribution that oscillates in time. These states emerge
when the emitter is driven by an external field while coupled to the cavity. We
show that by rapidly changing the driving strength, new wave packets can be
generated, allowing multiple packets to coexist and evolve independently. We
classify the resulting wave packet behavior into distinct dynamical subclasses
between which we choose through the choice of relevant parameters. Based on
this understanding, we develop simple and robust protocols to generate a
specified number of photon number wave packets on demand. We propose that the
rich dynamics can be experimentally investigated by merely measuring the mean
photon number.

</details>


### [317] [Simple explanation of apparent Bell nonlocality of unentangled photons](https://arxiv.org/abs/2509.03127)
*Antoni Wójcik,Jan Wójcik*

Main category: quant-ph

TL;DR: Wang等人声称使用非纠缠光子进行了违反贝尔不等式的实验，但该分析认为这是由于后选择和非传统归一化程序造成的，而不是真正的非局域性。


<details>
  <summary>Details</summary>
Motivation: 分析Wang等人声称使用非纠缠光子进行违反贝尔不等式实验的本质，并探讨其是否真正展示了非局域性。

Method: 通过简化的理想化场景分析实验的关键特征，重点关注后选择和归一化程序。

Result: 确定实验中的后选择和非传统归一化程序产生了模拟贝尔类型相关性的量，但缺乏真正的贝尔检验所需的运行意义。

Conclusion: 得出的结论是，该实验并未证明非纠缠的非局域性，而是对多光子干涉效应的误解。

Abstract: Wang et al. [Science Advances, 1 Aug 2025 Vol 11, Issue 31] recently reported
an experiment that they interpret as demonstrating a violation of Bell's
inequality using unentangled photons. Such a claim is highly controversial,
since it is well established that product states cannot surpass the bounds set
by local hidden variable models. In this manuscript, we analyze the essential
features of the experiment through simplified, idealized scenarios. Our
analysis shows that the apparent violation of Bell's inequality originates from
two key elements: postselection and an unconventional normalization procedure.
These steps produce quantities that formally mimic Bell-type correlations but
lack the operational meaning required in a genuine Bell test. We therefore
argue that the reported violation does not demonstrate nonlocality without
entanglement, but rather reflects a misinterpretation of otherwise valid and
interesting multiphoton interference effects.

</details>


### [318] [Assessing the dynamical assumptions in Tsirelson inequality tests of non-classicality in harmonic oscillators](https://arxiv.org/abs/2509.03166)
*Arush Garg,Jonathan Halliwell,Taejas Venkataraman*

Main category: quant-ph

TL;DR: 宏观实在论认为系统在任何时候都具有确定的性质，并且我们可以原则上发现这些性质而不干扰系统的后续行为。Leggett-Garg 不等式是在这些假设下推导出来的，并且很容易被标准的量子力学违反，从而提供了一个测试明显宏观系统是否能表现出量子相干性的方案。不幸的是，Leggett-Garg 测试遭受不可避免的笨拙性循环漏洞——证明顺序测量无意中没有干扰系统很困难。最近发现的 Tsirelson 不等式是从服从许多经典系统均匀进动的简单动力学假设推导出来的，并且只需要单次测量。然而，Tsirelson 不等式的违反可以用一个宏观实在论系统来解释，该系统仅仅破坏了动力学假设，而不是真正的量子行为。通过对谐振子中的 Tsirelson 不等式进行量子力学分析，我们开发了一种协议来排除这种可能性，通过评估均匀进动的广义条件。我们表明，各种均匀进动的度量，其中一些与 Leggett-Garg 量有关，得到了足够好的满足，以至于必须暗示量子力学干涉项的存在。我们推导了几个关于最大违反状态的附带数学结果，涉及停留时间、交叉数和概率流，并考虑了 Tsirelson 算子的群论分析。


<details>
  <summary>Details</summary>
Motivation: Leggett-Garg 测试遭受笨拙性循环漏洞，无法排除宏观实在论系统可能违反 Tsirelson 不等式的情况。该研究旨在开发一种协议来排除这种可能性，通过评估均匀进动的广义条件。

Method: 通过对谐振子中的 Tsirelson 不等式进行量子力学分析，开发了一种评估均匀进广义条件的协议。研究了均匀进动的度量（部分与 Leggett-Garg 量有关）与量子力学干涉项的关系。推导了关于最大违反状态的数学结果，并考虑了 Tsirelson 算子的群论分析。

Result: 各种均匀进动的度量得到了足够好的满足，表明量子力学干涉项的存在是必然的。推导了几个关于最大违反状态的附带数学结果，涉及停留时间、交叉数和概率流。对 Tsirelson 算子进行了群论分析。

Conclusion: 该研究通过评估均匀进动的广义条件，成功地排除了一种可能的宏观实在论解释，从而证明了量子力学干涉项的存在。

Abstract: "Macrorealism" posits that a system possesses definite properties at all
times and that we can discover these properties, in principle, without
disturbing the system's subsequent behaviour. The Leggett-Garg inequalities are
derived under these assumptions and are readily violated by standard quantum
mechanics, thereby providing a scheme to test whether demonstrably macroscopic
systems can exhibit quantum coherence. Unfortunately, Leggett-Garg tests suffer
from the ineludible clumsiness loophole - the difficulty of proving that
sequential measurements have not inadvertently disturbed the system. The
recently uncovered Tsirelson inequality is derived from the simple dynamical
assumption of uniform precession, obeyed by many classical systems, and
requires only single-time measurements. However, Tsirelson inequality
violations could be explained by a macrorealistic system that merely breaks the
dynamical assumption, rather than genuine quantum behaviour. By carrying out a
quantum-mechanical analysis of the Tsirelson inequality in the harmonic
oscillator, we develop a protocol to rule out this possibility by assessing
generalised conditions of uniform precession. We show that various measures of
uniform precession, some of which related to Leggett-Garg quantities, are
satisfied well enough that the presence of quantum-mechanical interference
terms must be implied. We derive several incidental mathematical results
relating to maximally-violating states, concerning dwell time, crossing number
and probability currents, and also consider a group theoretic analysis of the
Tsirelson operator.

</details>


### [319] [Scalable Entanglement Detection in Quantum Systems via Fisher Linear Discriminant Analysis](https://arxiv.org/abs/2509.03233)
*Mahmoud Mahdian,Zahra Mousavi*

Main category: quant-ph

TL;DR: 利用机器学习中的经典 Fisher 线性判别分析（FLDA）来识别高维量子系统中的纠缠态。


<details>
  <summary>Details</summary>
Motivation: 高维量子系统中的纠缠检测是一个挑战，因为希尔伯特空间的维数会随着粒子数量呈指数增长。

Method: 本研究提出将经典 Fisher 线性判别分析（FLDA）应用于量子态判别分析，以实现量子态的分类。

Result: 该方法在不同的量子态上进行了系统性评估，证明了其在高效量子态分类方面的有效性，并且能够高精度地识别多量子比特量子态。

Conclusion: FLDA 是一个有效且精确的工具，可用于高维量子系统中的纠缠态检测和分类。

Abstract: Quantum entanglement is the cornerstone of quantum technology and enables
quantum devices to outperform classical systems in terms of performance.
However, detecting entanglement in high-dimensional systems remains a
significant challenge due to the exponential growth of the Hilbert space with
the number of particles. In this work, we use machine learning to classify
entangled states and separable states, focusing on the application of classical
Fisher Linear Discriminant Analysis (FLDA). By adapting classical statistical
learning techniques to quantum state discriminant analysis, we present the
theoretical foundations, a practical implementation strategy, and the
advantages of FLDA in this context. We systematically evaluate the performance
of this method on different quantum states and demonstrate its effectiveness as
a tool for efficient quantum state classification. Finally, we investigate
multi-qubit quantum states with high accuracy and classify these states.

</details>


### [320] [Quantum theory phase space foundations](https://arxiv.org/abs/2509.03237)
*Miloš D. Davidović,Ljubica D. Davidović,Milena D. Davidović*

Main category: quant-ph

TL;DR: 该论文从相空间的角度回顾了经典理论与量子理论联系的相关概念，指出量子理论由非交换算符描述，其结果具有概率性，类似于经典统计理论中可精确测量的坐标和动量，并着重探讨了最能描述量子态的经典对象以及量子理论中的物理可观测量的算符表示。


<details>
  <summary>Details</summary>
Motivation: 这篇论文的动机是回顾经典理论与量子理论联系的相关概念，并从相空间的角度进行探讨。

Method: 论文将量子理论描述为由非交换算符（坐标和动量）组成的系统，其结果具有一定的概率性。这与经典统计理论中坐标和动量是可精确测量量的特点相似。论文旨在寻找最能代表量子态的经典对象，并阐述量子理论中的物理可观测量是以算符的形式存在的。

Result: 该论文从相空间的角度审视了经典理论与量子理论的联系。它指出，量子理论的特点在于其坐标和动量是由非交换算符描述的，这导致了结果的概率性，类似于经典统计理论中可以精确测量的坐标和动量。论文的重点在于寻找能够最好地代表量子态的经典对应物，并强调量子理论中的物理可观测量的本质是算符。

Conclusion: 该论文回顾了经典与量子理论的联系，强调了量子理论的非交换算符和概率性结果，并将其与经典理论的可测量量进行了类比，旨在寻找描述量子态的最佳经典对象和理解量子可观测量的算符本质。

Abstract: We give a review of concepts related to connection of classical and quantum
theories, from the phase space perspective. Quantum theory is described by
non-commutative operators of coordinates and momenta, results in values having
a certain probability, hence resembling the classical statistical theory in
which coordinates and momenta are well measurable quantities. The search is for
the classical object best describing the quantum state, while the physical
observables of the quantum theories are operators.

</details>


### [321] [Cat-state-like non-Gaussian entanglement in magnon systems](https://arxiv.org/abs/2509.03239)
*Zeyu Zhang,Clemens Gneiting,Zheng-Yang Zhou,Ai-Xi Chen*

Main category: quant-ph

TL;DR: 本文提出了一种利用基于模块化变量的投影方法，将磁振子系统中的猫态类纠缠态映射到自旋态，并利用贝尔不等式对其进行探测，为生成和识别磁振子系统中的猫态类纠缠态提供了新的途径。


<details>
  <summary>Details</summary>
Motivation: 探索磁振子系统生成纠缠态的可能性，特别是具有多光子和非高斯特性的猫态类纠缠态，并提出一种有效的纠缠探测方法。

Method: 提出了一种基于模块化变量的投影方法，将磁振子猫态类纠缠态映射到自旋态，然后利用贝尔不等式来探测自旋态中的纠缠。

Result: 数值分析表明，该方法能够有效地探测磁振子系统中的猫态类纠缠，并给出了生成此类纠缠的条件。该方法还可以推广到其他由磁振子和自旋系统形成的纠缠态。

Conclusion: 基于模块化变量的投影方法为探测磁振子系统中的猫态类纠缠态提供了一种有效且可扩展的方案，有助于推动量子信息科学在磁子学等领域的发展。

Abstract: Magnons can serve as a bridge between spin, phonon, and photon systems, which
renders them suitable for constructing hybrid systems. An important application
of such hybrid systems is generating entanglement between different platforms.
As magnons can support a broad variety of states, e.g., Fock states, squeezed
states, or coherent states, and hybrid states can be produced with cavities or
spins, there are many different kinds of entangled states in magnon systems. In
this paper, we consider the entanglement of cat-state-like throughout states,
which can be generated in magnon systems with parametric pumps beyond the
parametric stable intensities. However, estimating the entanglement in such
states is challenging due to their multiphoton and non-Gaussian properties.
Here, we apply a modular variable-based projection, which maps the catlike
states to spin states, preserving the encoded information. After the
projection, Bell's inequality is employed to detect the entanglement in the
effective spin states. Our numerical analysis provides the conditions for
generating catlike entanglement in magnon systems and can be conveniently
extended to other entangled states that may be formed by magnon and spin
systems.

</details>


### [322] [Universal Analytic Solution for the Quantum Transport of Structured Matter-Waves in Magnetic Optics](https://arxiv.org/abs/2509.03264)
*N. V. Filina,S. S. Baturin*

Main category: quant-ph

TL;DR: 该论文提出了一种解析方法，用于描述带电标量态在非均匀磁场中的传播，并揭示了其与经典束流光学参数之间的联系，为OAM相关实验的设计提供了工具。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于理解带电粒子态在磁场中的演化，特别是量子演化与经典行为的映射关系，并为设计基于轨道角动量（OAM）的新型实验提供理论支持。

Method: 提出了一种封闭形式的解析解，该解可以描述任意带电标量态在非均匀磁场中的传播。该解将量子演化与经典束流光学参数（如Courant-Snyder参数、Twiss函数和相位推进）联系起来，并分解为三个部分，其中包含一个与OAM投影符号相关的复杂旋转以及一个由广义Gouy相位控制的内在畸变。

Result: 在特定条件下（如Glaser型磁场和半阻挡的扭曲电子），研究表明该理论预测的非对称性会导致复杂的动力学行为，无法用简单的Bohmian轨迹来解释。

Conclusion: 所提出的全相对论框架为预测粒子加速器和电子显微镜中束流的行为提供了一个强大的工具，这对于设计下一代OAM实验至关重要。

Abstract: We present a closed-form analytic solution for the propagation of an
arbitrary charged scalar state in a non-uniform magnetic field. The dynamics
are governed by classical beam optics parameters (Courant-Snyder parameters),
the Twiss functions, and phase advance, revealing a direct map between quantum
evolution and its classical counterpart. The solution decomposes into three
components, exhibiting a complex rotation dependent on the sign of the orbital
angular momentum (OAM) projection, alongside an intrinsic distortion from
interference governed by a generalized Gouy phase. For a relevant Glaser-type
magnetic field and a half-blocked twisted electron, we demonstrate that
asymmetry triggers rich dynamics incompatible with a simple Bohmian trajectory
interpretation. Our fully relativistic framework provides a powerful tool for
predicting beam behavior in particle accelerators and electron microscopes,
critical for designing next-generation OAM-based experiments.

</details>


### [323] [An experience-based classification of quantum bugs in quantum software](https://arxiv.org/abs/2509.03280)
*Nils Quetschlich,Olivia Di Matteo*

Main category: quant-ph

TL;DR: This paper addresses the challenge of debugging quantum software by categorizing 14 common quantum bugs based on real-world development experience and open-source analysis. It finds that most quantum bugs arise from complex interactions between multiple algorithmic or workflow aspects, contributing to debugging difficulty. Unexpectedly, no clear relationship was found between bug classes and debugging strategies, highlighting the need for further research into systematic quantum debugging approaches.


<details>
  <summary>Details</summary>
Motivation: The increasing scale and quality of quantum computers necessitate accessible software frameworks and effective debugging techniques. Quantum bugs, arising from the unique behavior of quantum systems, are particularly challenging to pinpoint, requiring significant developer time due to a lack of established guidance.

Method: The study identifies and categorizes 14 types of quantum bugs, drawing from the authors' experience as quantum software developers and analysis of open-source GitHub repositories. Each bug is detailed with its context, symptoms, and the methods used for identification and resolution. The bugs are classified using existing schemes, with a focus on understanding the underlying causes of quantum debugging complexity.

Result: The analysis revealed that most quantum bugs stem from the interaction of multiple aspects within an algorithm or workflow, indicating that 'more than one thing went wrong.' This observation helps explain the inherent difficulty in quantum debugging. Additionally, the study found no clear correlation between specific debugging strategies and the identified bug classes.

Conclusion: Quantum debugging is challenging primarily because quantum bugs often result from the interplay of various algorithmic and workflow components. The lack of a discernible relationship between bug categories and debugging strategies suggests that further research is crucial for developing systematic and effective quantum debugging methodologies.

Abstract: As quantum computers continue to improve in quality and scale, there is a
growing need for accessible software frameworks for programming them. However,
the unique behavior of quantum systems means specialized approaches, beyond
traditional software development, are required. This is particularly true for
debugging due to quantum bugs, i.e., bugs that occur precisely because an
algorithm is a quantum algorithm. Pinpointing a quantum bug's root cause often
requires significant developer time, as there is little established guidance
for quantum debugging techniques. Developing such guidance is the main
challenge we sought to address. In this work, we describe a set of 14 quantum
bugs, sourced primarily from our experience as quantum software developers, and
supplemented by analysis of open-source GitHub repositories. We detail their
context, symptoms, and the techniques applied to identify and fix them. While
classifying these bugs based on existing schemes, we observed that most emerged
due to unique interactions between multiple aspects of an algorithm or
workflow. In other words, they occurred because more than one thing went wrong,
which provided important insight into why quantum debugging is more
challenging. Furthermore, based on this clustering, we found that -
unexpectedly - there is no clear relationship between debugging strategies and
bug classes. Further research is needed to develop effective and systematic
quantum debugging strategies.

</details>


### [324] [Ancilla-Free Quantum Protocol for Thermal Green's Functions](https://arxiv.org/abs/2509.03288)
*Changhao Yi,Cunlu Zhou*

Main category: quant-ph

TL;DR: 提出了一种简单、抗噪声的量子算法，用于计算零温和有限温格林函数，无需额外量子比特，仅依赖于当前量子平台上的本机时间演化和测量。


<details>
  <summary>Details</summary>
Motivation: 开发一种适用于当前量子硬件的、能够计算零温和有限温格林函数（包括其精确实部和虚部）的量子算法。

Method: 利用系统的奇偶对称性，构造对称和反对称热态，并采用定制的淬灭谱学方案来恢复两点时间关联函数的精确实部和虚部。通过有效的信号分析重构格林函数。

Result: 提出了一种理论上高效、实验上可行的量子算法，可直接应用于数字量子计算机和模拟量子模拟器。该框架同样适用于计算时序关联函数（OTOC）。

Conclusion: 该工作为在近期和早期容错量子硬件上探测强关联量子系统的有限温动力学提供了一条切实可行的路径。

Abstract: We introduce a simple and noise-resilient quantum algorithm for computing
both zero- and finite-temperature Green's functions, requiring no ancillas and
relying only on native time evolution and measurements readily available on
current platforms. Assuming parity symmetry of the system, as in the
Fermi-Hubbard and Heisenberg models, we construct symmetric and antisymmetric
thermal states and employ a tailored quench spectroscopy scheme to recover the
exact real and imaginary parts of two-point time correlators. Green's functions
are then reconstructed through efficient signal analysis. The protocol is
theoretically efficient, experimentally accessible, and directly applicable to
both digital quantum computers and analog quantum simulators. Beyond Green's
functions, the same framework extends naturally to out-of-time-order
correlators (OTOC). This work highlights a practical path toward probing
finite-temperature dynamics of strongly correlated quantum systems on near-term
and early fault-tolerant quantum hardware.

</details>


### [325] [Conditions for Suppression of Gas Phase Chemical Reactions inside a Dark Infrared Cavity: O$_{2}$ + 2NO$\to$ 2NO$_{2}$ as an example](https://arxiv.org/abs/2509.03299)
*Mwdansar Banuary,Ashish Kumar Gupta,Nimrod Moiseyev*

Main category: quant-ph

TL;DR: 通过选择具有两个鞍点（而非一个）势能面的反应，并控制分子数量和腔体距离，可以利用暗腔抑制化学反应速率，例如O$_{2}$ + 2NO$	o$ 2NO$_{2}$的反应。


<details>
  <summary>Details</summary>
Motivation: 在暗腔中利用镜子限制反应来减缓化学反应，但理论和实验尚未完全统一。

Method: 研究了能够被暗腔抑制的反应的条件和选择指南，重点是反应势能面必须包含两个鞍点，并演示了O$_{2}$ + 2NO$	o$ 2NO$_{2}$的反应速率可以通过暗腔抑制。

Result: O$_{2}$ + 2NO$	o$ 2NO$_{2}$的反应速率可以通过暗腔抑制，且抑制程度取决于腔体距离和同时与腔体相互作用的处于过渡态的分子数量。

Conclusion: 该研究为在暗腔中选择和抑制化学反应提供了理论指导和具体实例。

Abstract: The ability to slow down chemical reactions using a seemingly simple setup
reactions confined within a cavity formed by two parallel mirrors is
fascinating. However, theory and experiment have not yet fully converged. In
this work, we provide the conditions and guidelines for selecting reactions
that can be suppressed in a dark cavity. The primary requirement is that the
reaction's potential energy surface must contain two saddle points (and not one
saddle point as required for enhancement). This condition enables a reduction
in the reaction rate. Specifically, we demonstrate that the reaction rate of
O$_{2}$ + 2NO$\to$ 2NO$_{2}$ can be suppressed by a dark cavity composed of two
parallel mirrors. We show that the suppression of the reaction rate depends on
the distance between the mirrors, which determines the cavity parameters, and
on the number of molecules in the transition state configuration that
simultaneously interact with the cavity.

</details>


### [326] [Evaluating Security Properties in the Execution of Quantum Circuits](https://arxiv.org/abs/2509.03306)
*Paolo Bernardi,Antonio Brogi,Gian-Luigi Ferrari,Giuseppe Bisicchia*

Main category: quant-ph

TL;DR: NISQ设备在药物发现和密码学等领域具有潜在优势，因此保障其信息安全至关重要。本研究旨在为利用可能不可信的量子处理器评估安全性（如保密性和完整性）提供一种实用的、启发式的方法。


<details>
  <summary>Details</summary>
Motivation: 量子计算在药物发现和密码学等关键领域具有颠覆性潜力，因此，处理这些量子机器的信息的安全性至关重要。

Method: 提出一种实用的、启发式的方法来评估安全性，例如使用量子处理器保密性和完整性，这些量子处理器可能由不可信的提供商拥有。

Result: 目前，可以提供适度的嘈杂中等规模量子（NISQ）设备。

Conclusion: 为利用可能不可信的量子处理器评估安全性（如保密性和完整性）提供一种实用的、启发式的方法。

Abstract: Quantum computing is a disruptive technology that is expected to offer
significant advantages in many critical fields (e.g. drug discovery and
cryptography). The security of information processed by such machines is
therefore paramount. Currently, modest Noisy Intermediate-Scale Quantum (NISQ)
devices are available. The goal of this work is to identify a practical,
heuristic methodology to evaluate security properties, such as secrecy and
integrity, while using quantum processors owned by potentially untrustworthy
providers.

</details>


### [327] [High-fidelity control of a $^{13}$C nuclear spin coupled to a tin-vacancy center in diamond](https://arxiv.org/abs/2509.03354)
*Jeremias Resch,Ioannis Karapatzakis,Mohamed Elshorbagy,Marcel Schrodin,Philipp Fuchs,Philipp Graßhoff,Luis Kussi,Christoph Sürgers,Cyril Popov,Christoph Becher,Wolfgang Wernsdorfer,David Hunger*

Main category: quant-ph

TL;DR: 研究演示了在金刚石中的锡空位（SnV）中心耦合的单个¹³C核自旋的高保真度控制，并实现了长相干时间（1.35秒）和高单量子比特门保真度（99.92%），为量子网络节点提供了有前景的相干自旋光子系统。


<details>
  <summary>Details</summary>
Motivation: 金刚石中的核自旋，特别是与锡空位（SnV）中心耦合的核自旋，在量子网络应用中作为量子存储器具有潜力。

Method: 通过光学和微波泵浦实现初始化的电核自旋态的制备；利用超导波导进行射频驱动，实现核自旋的精确控制；通过拉姆齐测量和动力学解耦技术延长相干时间；通过随机基准测试来评估单量子比特门保真度。

Result: 实现了99.74(3)%的初始化保真度；拉姆齐测量揭示了1.5(1)毫秒的相干时间，通过动力学解耦延长至1.35(3)秒；单量子比特门保真度为99.92(1)%。

Conclusion: 研究成功展示了金刚石中¹³C核自旋与SnV中心的相干耦合，实现了高保真度的初始化、精确的自旋控制和长相干时间，证明了该系统在量子网络节点中的应用潜力。

Abstract: Nuclear spins near group-IV defects in diamond are promising candidates for
quantum memories in quantum network applications. Here, we demonstrate
high-fidelity control of a single $^{13}$C nuclear spin coupled to a
tin-vacancy (SnV) center in diamond. We perform a combination of optical and
microwave pumping to achieve initialization into a combined electro-nuclear
spin state with a fidelity of $99.74(3)\,\%$. Harnessing a superconducting
waveguide for radio-frequency driving, we demonstrate precise nuclear spin
control: Ramsey measurements reveal a coherence time of $T_2^* =
1.5(1)\,$milliseconds, and we use dynamical decoupling to extend it to
$1.35(3)\,$seconds. We perform randomized benchmarking, yielding a single-qubit
gate fidelity of $99.92(1)\,\%$. This demonstrates a coherent spin-photon
system with promising properties for quantum network nodes.

</details>


### [328] [Effective Hamiltonian for an off-resonantly driven qubit-cavity system](https://arxiv.org/abs/2509.03375)
*Martin Jirlow,Kunal Helambe,Axel M. Eriksson,Simone Gasparinetti,Tahereh Abad*

Main category: quant-ph

TL;DR: 自然和合成原子在量子技术中至关重要，但多音驱动下的理论描述具有挑战性。本研究推导了一个包含缓慢旋转项的有效哈密顿量，为电路量子电动力学等平台提供了通用的精确描述框架，并成功复现了实验数据，为量子信息处理平台中驱动相互作用的设计提供了工具。


<details>
  <summary>Details</summary>
Motivation: 准确模拟受激的光-物质相互作用对于量子技术至关重要，现有模型在多音驱动下无法定量复现实验数据。

Method: 推导了一个包含缓慢旋转项的有效哈密顿量，并将其应用于电路量子电动力学。

Result: 该模型定量复现了实验测量的交流斯塔克频移，并捕捉了双模压缩和分束等关键相互作用。

Conclusion: 所提出的有效哈密顿量为在量子信息处理平台中设计驱动相互作用提供了一个广泛适用的工具。

Abstract: Accurate modeling of driven light-matter interactions is essential for
quantum technologies, where natural and synthetic atoms are used to store and
process quantum information, mediate interactions between bosonic modes, and
enable nonlinear operations. In systems subject to multi-tone drives, however,
the theoretical description becomes challenging and existing models cannot
quantitatively reproduce the experimental data. Here, we derive an effective
Hamiltonian that retains slowly rotating terms, providing a general framework
for accurately describing driven dynamics across platforms. As a concrete
application, we validate the theory in circuit QED, where it quantitatively
reproduces experimentally measured ac Stark shifts and captures key
interactions such as two-mode squeezing and beam-splitting. Our results
establish a broadly applicable tool to engineer driven interactions in quantum
information processing platforms.

</details>


### [329] [Detection of noise correlations in two qubit systems by Machine Learning](https://arxiv.org/abs/2509.03389)
*Dario Fasone,Shreyasi Mukherjee,Dario Penna,Fabio Cirinnà,Mauro Paternostro,Elisabetta Paladino,Luigi Giannelli,Giuseppe A. Falci*

Main category: quant-ph

TL;DR: 我们提出了一种机器学习辅助的量子传感协议，用于对影响两个超强耦合量子比特的经典噪声的空间和时间相关性进行分类。


<details>
  <summary>Details</summary>
Motivation: 区分影响量子比特的经典噪声的各种形式（马尔可夫和非马尔可夫）对于提高量子硬件的性能至关重要。

Method: 该协议利用了相干布朗运动协议在三种不同驱动条件下的敏感性。通过测量最终的转移效率，可以区分不同类型的噪声。然后，使用一个浅层神经网络来分析这些数据，以对噪声进行分类。

Result: 该方法在噪声分类方面达到了超过 86% 的准确率，能够近乎完美地区分马尔可夫和非马尔可夫噪声。

Conclusion: 所提出的机器学习辅助量子传感协议是一种有效且资源要求低的方法，可以对影响量子系统的经典噪声进行分类。这为改进量子硬件的表征和性能提供了一条有前途的途径。

Abstract: We introduce and validate a machine-learning assisted quantum sensing
protocol to classify spatial and temporal correlations of classical noise
affecting two ultrastrongly coupled qubits. We consider six distinct classes of
Markovian and non-Markovian noise. Leveraging the sensitivity of a coherent
population transfer protocol under three distinct driving conditions, the
various forms of noise are discriminated by only measuring the final transfer
efficiencies. Our approach achieves $\gtrsim 86\%$ accuracy in classification
providing a near-perfect discrimination between Markovian and non-Markovian
noise. The method requires minimal experimental resources, relying on a simple
driving scheme providing three inputs to a shallow neural network with no need
of measuring time-series data or real-time monitoring. The machine-learning
data analysis acquires information from non-idealities of the coherent protocol
highlighting how combining these techniques may significantly improve the
characterization of quantum-hardware.

</details>


### [330] [An angular momentum approach to quantum insertion errors](https://arxiv.org/abs/2509.03413)
*Lewis Bulled,Yingkai Ouyang*

Main category: quant-ph

TL;DR: 本文提出了一种量子纠错协议，可纠正一类具有能隙的置换不变码上的单次量子插入错误。


<details>
  <summary>Details</summary>
Motivation: 纠正量子插入错误的研究有限，缺乏通用框架。

Method: 提出了一种简单的两阶段综合提取协议，测量了总角动量及其沿z轴的投影，得到一个两比特的综合。这表明测量可以将状态投影到新的码空间，并提出了一种传输协议将投影后的状态映射回所需数量的量子比特上的置换不变码。

Result: 成功纠正了单次量子插入错误。

Conclusion: 所提出的协议能够纠正一类gapped置换不变码上的单次插入错误。

Abstract: Quantum insertion errors are a class of errors that increase the number of
qubits in a quantum system. Despite a wealth of research on classical insertion
errors, there has been limited progress towards a general framework for
correcting quantum insertion errors. We detail a quantum error correction
protocol that can correct single insertion errors on a class of gapped
permutation-invariant codes. We provide a simple two-stage syndrome extraction
protocol that yields a two-bit syndrome, by measuring the total angular
momentum and its projection along the $z$-axis (modulo the code gap) of the
post-insertion state. We demonstrate that these measurements project the state
onto a new codespace, and we detail a teleportation protocol to map the
projected state back to a permutation-invariant code on the desired number of
qubits.

</details>


### [331] [Universal representation of the long-range entanglement in the family of Toric Code states](https://arxiv.org/abs/2509.03422)
*Mohammad Hossein Zarei,Mohsen Rahmani Haghighi*

Main category: quant-ph

TL;DR: 长程纠缠是拓扑量子态的普适特征，本文提出了一种基于Kitaev梯子的表示方法，将Toric Code态转化为Kitaev梯子态的张量积，揭示了非局部模式在拓扑序中的作用。


<details>
  <summary>Details</summary>
Motivation: 长程纠缠是拓扑量子态的普适特征，需要一种普适的数学表示方法。

Method: 使用Kitaev梯子作为构建块，将Toric Code态表示为Kitaev梯子态的张量积，通过作用于非可缩环上的解缠器实现。

Result: 与底层图的几何形状无关，解缠器将Toric Code态转化为Kitaev梯子态的张量积。Kitaev梯子包含短程纠缠，因此Toric Code态的长程纠缠源于梯子间的非局部纠缠模式。

Conclusion: 非局部表示能够描述拓扑量子系统的基态波函数中的拓扑序。

Abstract: Since the long range entanglement is a universal characteristic of
topological quantum states belonging to the same class, a suitable mathematical
representation of the long range entanglement has to be also universal. In this
Letter, we introduce such a representation for the family of Toric Code states
by using Kitaev's Ladders as building blocks. We consider Toric Code states
corresponding to various planar graphs and apply non-local dientanglers to
qubits corresponding to non-contractible cycles that satisfy a topological
constraint. We demonstrate that, independent of the geometry of the underlying
graph, disentanglers convert Toric Code states into a tensor product of
Kitaev's Ladder states. Since Kitaev's Ladders with arbitrary geometric
configurations include the short-range entanglements, we conclude that the
above universal and non-local pattern of entanglement between ladders is
responsible of the long-range entanglement inherent in Toric Code states. Our
result emphasizes in the capability of such non-local representations to
describe topological order in ground-state wave functions of topological
quantum systems.

</details>


### [332] [Information-Theoretic Lower Bounds for Approximating Monomials via Optimal Quantum Tsallis Entropy Estimation](https://arxiv.org/abs/2509.03496)
*Qisheng Wang*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper reveals a conceptually new connection from information theory to
approximation theory via quantum algorithms for entropy estimation.
Specifically, we provide an information-theoretic lower bound
$\Omega(\sqrt{n})$ on the approximate degree of the monomial $x^n$, compared to
the analytic lower bounds shown in Newman and Rivlin (Aequ. Math. 1976) via
Fourier analysis and in Sachdeva and Vishnoi (Found. Trends Theor. Comput. Sci.
2014) via the Markov brothers' inequality. This is done by relating the
polynomial approximation of monomials to quantum Tsallis entropy estimation.
This further implies a quantum algorithm that estimates to within additive
error $\varepsilon$ the Tsallis entropy of integer order $q \geq 2$ of an
unknown probability distribution $p$ or an unknown quantum state $\rho$, using
$\widetilde \Theta(\frac{1}{\sqrt{q}\varepsilon})$ queries to the quantum
oracle that produces a sample from $p$ or prepares a copy of $\rho$, improving
the prior best $O(\frac{1}{\varepsilon})$ via the Shift test due to Ekert,
Alves, Oi, Horodecki, Horodecki and Kwek (Phys. Rev. Lett. 2002). To the best
of our knowledge, this is the first quantum entropy estimator with optimal
query complexity (up to polylogarithmic factors) for all parameters
simultaneously.

</details>


### [333] [Achieving quantum-limited sub-Rayleigh identification of incoherent sources with arbitrary intensities](https://arxiv.org/abs/2509.03511)
*Danilo Triggiani,Cosmo Lupo*

Main category: quant-ph

TL;DR: 量子传感技术可以利用空间解复用（SPADE）和光子检测来克服瑞利衍射极限，但在任意强度分布和亮源下的最优性尚未得到证明。本研究提出了一个适用于任意强度分布的非相干光通用模型，并利用该模型计算了量子克拉夫 दिसून指数，特别是在高斯点扩展函数（PSF）的亚衍射区域。研究发现，SPADE测量并不总能在亚衍射区域达到量子克拉夫 दिसून界，仅在满足特定相容性条件（如协方差矩阵可交换性）时才成立。这表明可能需要集体测量来实现区分特定非相干源的量子克拉夫 दिसून界。对于一般情况，本研究提出的分析方法可以找到最佳的SPADE配置，通常通过假设相关的SPADE模式旋转来实现。


<details>
  <summary>Details</summary>
Motivation: 瑞利衍射极限限制了成像系统的分辨率，阻碍了天文学和生物成像等领域中非相干光学源的识别。尽管量子传感技术（如SPADE和光子检测）已显示出克服此限制的潜力，但其在任意强度分布和亮源下的通用最优性仍未被证明。

Method: 本文提出了一个适用于任意强度分布的非相干光通用模型，并利用该模型计算了量子克拉夫 दिसून指数，特别是在高斯点扩展函数（PSF）的亚衍射区域。研究了SPADE测量在亚衍补区域的量子最优性，并分析了实现量子克拉夫 दिसून界所需的相容性条件，如协方差矩阵可交换性。此外，还提出了寻找最佳SPADE配置的方法，通常通过假设相关的SPADE模式旋转来实现。

Result: 研究发现，SPADE测量并不总能在亚衍射区域达到量子克拉夫 दिसून界，仅在满足特定相容性条件（如协方差矩阵可交换性）时才成立。这些结果表明，为了区分特定的非相干源，可能需要采用集体测量方法来实现最终的量子克拉夫 दिसून界。对于一般情况，提出的分析方法可以找到最佳的SPADE配置，通常通过假设相关的SPADE模式旋转来实现。

Conclusion: 本研究在量子限制光学判别理论方面取得了进展，该理论在诊断、自动图像解释和星系识别等领域具有潜在应用价值。研究结果强调了在特定情况下，集体测量可能比SPADE测量更能达到量子极限，并为优化SPADE配置提供了通用框架。

Abstract: The Rayleigh diffraction limit imposes a fundamental restriction on the
resolution of direct imaging systems, hindering the identification of
incoherent optical sources, such as celestial bodies in astronomy and
fluorophores in bioimaging. Recent advances in quantum sensing have shown that
this limit can be circumvented through spatial demultiplexing (SPADE) and
photon detection. Notably, the latter is a semi-classical detection strategy,
being SPADE a linear transformation of the field and photon detection a
measurement of intensity. However, the general optimality for arbitrary
intensity distributions and bright sources remains unproven. In this work, we
develop a general model for incoherent light with arbitrary intensity collected
by passive, linear optical systems. We employ this framework to compute the
quantum Chernoff exponent for generic incoherent-source discrimination
problems, and we analyze several special cases, with particular focus on the
subdiffraction regime for Gaussian point-spread functions. We show that,
surprisingly, SPADE measurements do not always saturate the quantum Chernoff
bound in the subdiffraction regime; the quantum optimality holds only when
certain compatibility conditions, such as covariance matrix commutativity, are
met. These findings suggest that collective measurements may actually be needed
to achieve the ultimate quantum Chernoff bound for the discrimination of
specific incoherent sources. For the fully general case, our analysis can still
be used to find the best SPADE configurations, generally achieved through a
hypotheses-dependent rotation of the SPADE modes. Our results advance the
theory of quantum-limited optical discrimination, with possible applications in
diagnostics, automated image interpretation, and galaxy identification.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [334] [Towards Performatively Stable Equilibria in Decision-Dependent Games for Arbitrary Data Distribution Maps](https://arxiv.org/abs/2509.02619)
*Guangzheng Zhong,Yang Liu,Jiming Liu*

Main category: cs.GT

TL;DR: 本研究提出了一种新的梯度基灵敏度度量方法，以解决决策依赖博弈中数据分布随联合决策变化的难题，并在此基础上提出了一个灵敏度感知型重复再训练算法，以保证算法收敛到表现稳定均衡。


<details>
  <summary>Details</summary>
Motivation: 现有决策依赖博弈研究中的表现稳定均衡方法依赖于β-光滑性假设，该假设要求损失函数梯度相对于数据分布具有Lipschitz连续性，但在实际应用中，数据分布映射通常未知，导致β无法获得。本研究旨在克服这一局限性。

Method: 提出了一种梯度基灵敏度度量方法，直接量化决策诱导的数据分布变化的影响。基于此度量，在强单调性这一更易实现的假设下，推导了表现稳定均衡的收敛保证。设计了一个灵敏度感知型重复再训练算法，该算法根据灵敏度度量调整玩家的损失函数，可保证在任意数据分布映射下收敛到表现稳定均衡。

Result: 所提出的方法在预测误差最小化博弈、库诺竞争和收益最大化博弈的实验中，表现优于现有最先进的方法，实现了更低的损失和更快的收敛速度。

Conclusion: 本研究提出的梯度基灵敏度度量和灵敏度感知型重复再训练算法，克服了现有方法的局限性，能够有效且可靠地求解决策依赖博弈中的表现稳定均衡，并在实际应用中展现出优越的性能。

Abstract: In decision-dependent games, multiple players optimize their decisions under
a data distribution that shifts with their joint actions, creating complex
dynamics in applications like market pricing. A practical consequence of these
dynamics is the \textit{performatively stable equilibrium}, where each player's
strategy is a best response under the induced distribution. Prior work relies
on $\beta$-smoothness, assuming Lipschitz continuity of loss function gradients
with respect to the data distribution, which is impractical as the data
distribution maps, i.e., the relationship between joint decision and the
resulting distribution shifts, are typically unknown, rendering $\beta$
unobtainable. To overcome this limitation, we propose a gradient-based
sensitivity measure that directly quantifies the impact of decision-induced
distribution shifts. Leveraging this measure, we derive convergence guarantees
for performatively stable equilibria under a practically feasible assumption of
strong monotonicity. Accordingly, we develop a sensitivity-informed repeated
retraining algorithm that adjusts players' loss functions based on the
sensitivity measure, guaranteeing convergence to performatively stable
equilibria for arbitrary data distribution maps. Experiments on prediction
error minimization game, Cournot competition, and revenue maximization game
show that our approach outperforms state-of-the-art baselines, achieving lower
losses and faster convergence.

</details>


### [335] [Generative Auto-Bidding in Large-Scale Competitive Auctions via Diffusion Completer-Aligner](https://arxiv.org/abs/2509.03348)
*Yewen Li,Jingtong Gao,Nan Jiang,Shuai Mao,Ruyi An,Fei Pan,Xiangyu Zhao,Bo An,Qingpeng Cai,Peng Jiang*

Main category: cs.GT

TL;DR: CBD是一种基于扩散模型的因果自动竞价方法，通过增强训练过程和轨迹精炼来提高竞价效率和广告回报。


<details>
  <summary>Details</summary>
Motivation: 自动竞价在计算广告中至关重要，但现有的生成模型（如扩散模型）在处理动态合法性和稀疏奖励方面存在挑战，可能导致竞价损失和广告机会的浪费。

Method: 提出了一种名为CBD的因果自动竞价方法，该方法基于扩散模型完成-对齐框架。通过引入额外的随机变量t来增强扩散模型的训练过程，使其能够根据历史序列预测未来序列，从而提高生成序列的动态合法性。然后，使用轨迹级回报模型来优化生成的轨迹，使其更符合广告商的目标。

Result: 实验结果表明，CBD在大型自动竞价基准测试（尤其是在稀疏奖励的拍卖环境中，转化价值提高了29.9%）和快手在线广告平台（目标成本降低2.0%）上均取得了优于现有方法的性能。

Conclusion: CBD方法通过增强扩散模型的训练和引入轨迹级回报模型，有效解决了现有自动竞价方法在动态合法性和稀疏奖励方面的挑战，显著提升了广告竞价的效果和效率。

Abstract: Auto-bidding is central to computational advertising, achieving notable
commercial success by optimizing advertisers' bids within economic constraints.
Recently, large generative models show potential to revolutionize auto-bidding
by generating bids that could flexibly adapt to complex, competitive
environments. Among them, diffusers stand out for their ability to address
sparse-reward challenges by focusing on trajectory-level accumulated rewards,
as well as their explainable capability, i.e., planning a future trajectory of
states and executing bids accordingly. However, diffusers struggle with
generation uncertainty, particularly regarding dynamic legitimacy between
adjacent states, which can lead to poor bids and further cause significant loss
of ad impression opportunities when competing with other advertisers in a
highly competitive auction environment. To address it, we propose a Causal
auto-Bidding method based on a Diffusion completer-aligner framework, termed
CBD. Firstly, we augment the diffusion training process with an extra random
variable t, where the model observes t-length historical sequences with the
goal of completing the remaining sequence, thereby enhancing the generated
sequences' dynamic legitimacy. Then, we employ a trajectory-level return model
to refine the generated trajectories, aligning more closely with advertisers'
objectives. Experimental results across diverse settings demonstrate that our
approach not only achieves superior performance on large-scale auto-bidding
benchmarks, such as a 29.9% improvement in conversion value in the challenging
sparse-reward auction setting, but also delivers significant improvements on
the Kuaishou online advertising platform, including a 2.0% increase in target
cost.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [336] [Portable Targeted Sampling Framework Using LLVM](https://arxiv.org/abs/2509.02873)
*Zhantong Qiu,Mahyar Samani,Jason Lowe-Power*

Main category: cs.AR

TL;DR: Nugget是一个便携式采样框架，能在模拟器和真实硬件上跨越不同的指令集和库进行工作，通过LLVM IR进行二机制无关的区间分析，并生成轻量级、跨平台的“nugget”可执行文件，以原生速度验证和驱动模拟，显著降低了分析成本，提高了研究效率和便携性。


<details>
  <summary>Details</summary>
Motivation: 对CPU工作负载进行全面的架构评估受到模拟和采样流程缓慢的限制。

Method: 提出了一种名为Nugget的灵活框架，用于在模拟器和真实硬件、指令集架构（ISA）以及库之间进行可移植的采样。Nugget在LLVM中间语言（IR）层面进行二机制无关的区间分析，然后生成轻量级的、跨平台的“nugget”可执行文件，这些文件可以在真实机器上进行验证，然后再驱动模拟。

Result: Nugget在SPEC CPU2017、NPB和LSMS等基准测试中，相对于功能模拟，将区间分析的成本降低了几个数量级（在多线程NPB上最高可达约578倍），同时保持了单线程的低开销，并实现了对所选样本的原生速度验证。在使用gem5进行的案例研究中，nuggets支持对系统性能和模型准确性的评估。

Conclusion: Nugget框架能够显著加速和提高采样方法论研究的便携性，解决了传统方法在模拟速度和跨平台兼容性方面的瓶颈。

Abstract: Comprehensive architectural evaluation of full workloads is throttled by slow
simulation and per-binary sampling pipelines. We present Nugget, a flexible
framework for portable sampling across simulators and real hardware, ISAs, and
libraries. Nugget operates at the LLVM IR level to perform binary-agnostic
interval analysis, then emits lightweight, cross-platform
executables--nuggets--that can be validated on real machines before driving
simulation. Across SPEC CPU2017, NPB, and LSMS, Nugget cuts interval-analysis
cost by orders of magnitude relative to functional simulation (up to ~578X on
multithreaded NPB), keeps single-thread overhead low, and enables native-speed
validation of selected samples. Case studies with gem5 show that nuggets
support evaluation of system performance and model accuracy. Nugget makes
sampling methodology research faster and more portable.

</details>


### [337] [FastCaps: A Design Methodology for Accelerating Capsule Network on Field Programmable Gate Arrays](https://arxiv.org/abs/2509.03103)
*Abdul Rahoof,Vivek Chaturvedi,Muhammad Shafique*

Main category: cs.AR

TL;DR: 本论文提出了一种在FPGA上加速完整CapsNet的新方法，通过网络剪枝和路由算法优化，显著提高了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在理解图像变化和泛化能力方面不如CapsNet，但CapsNet在FPGA上的加速面临挑战，现有工作仅加速了部分算法，未实现整体协同优化。

Method: 提出一种新颖的两步方法：1. 使用基于前瞻得分的网络剪枝（LAKP）方法进行网络剪枝。2. 简化路由算法中的非线性运算、重新排序循环和并行化操作，以降低硬件复杂度。

Result: 在MNIST和F-MNIST数据集上，LAKP方法实现了99.26%和98.84%的压缩率。优化后的路由算法将吞吐量分别提高到1351 FPS和934 FPS，Xilinx PYNQ-Z1 FPGA上的吞吐量分别为82 FPS和48 FPS。

Conclusion: 该研究首次实现了在FPGA上加速完整的CapsNet，所提出的方法能够高效地将CapsNet部署到低成本FPGA上，适用于现代边缘设备。

Abstract: Capsule Network (CapsNet) has shown significant improvement in understanding
the variation in images along with better generalization ability compared to
traditional Convolutional Neural Network (CNN). CapsNet preserves spatial
relationship among extracted features and apply dynamic routing to efficiently
learn the internal connections between capsules. However, due to the capsule
structure and the complexity of the routing mechanism, it is non-trivial to
accelerate CapsNet performance in its original form on Field Programmable Gate
Array (FPGA). Most of the existing works on CapsNet have achieved limited
acceleration as they implement only the dynamic routing algorithm on FPGA,
while considering all the processing steps synergistically is important for
real-world applications of Capsule Networks. Towards this, we propose a novel
two-step approach that deploys a full-fledged CapsNet on FPGA. First, we prune
the network using a novel Look-Ahead Kernel Pruning (LAKP) methodology that
uses the sum of look-ahead scores of the model parameters. Next, we simplify
the nonlinear operations, reorder loops, and parallelize operations of the
routing algorithm to reduce CapsNet hardware complexity. To the best of our
knowledge, this is the first work accelerating a full-fledged CapsNet on FPGA.
Experimental results on the MNIST and F-MNIST datasets (typical in Capsule
Network community) show that the proposed LAKP approach achieves an effective
compression rate of 99.26% and 98.84%, and achieves a throughput of 82 FPS and
48 FPS on Xilinx PYNQ-Z1 FPGA, respectively. Furthermore, reducing the hardware
complexity of the routing algorithm increases the throughput to 1351 FPS and
934 FPS respectively. As corroborated by our results, this work enables highly
performance-efficient deployment of CapsNets on low-cost FPGA that are popular
in modern edge devices.

</details>


### [338] [CapsBeam: Accelerating Capsule Network based Beamformer for Ultrasound Non-Steered Plane Wave Imaging on Field Programmable Gate Array](https://arxiv.org/abs/2509.03201)
*Abdul Rahoof,Vivek Chaturvedi,Mahesh Raveendranatha Panicker,Muhammad Shafique*

Main category: cs.AR

TL;DR: 提出了一种名为CapsBeam的胶囊网络模型，用于在资源受限的边缘设备上进行超声成像信号处理，该模型在体内的实验中减少了伪影，在体外和计算机模拟数据中提高了对比度和分辨率。通过LookAhead Kernel Pruning（LAKP-ML）方法实现了85%的模型压缩率，并且通过量化、简化非线性操作和并行化操作来优化硬件复杂性。最终在FPGA上实现了一个专门的加速器，在卷积和动态路由操作上分别达到了30 GOPS和17.4 GOPS的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的超声成像波束形成算法计算复杂度高，难以部署在资源受限的边缘设备上。

Method: 提出了一种名为CapsBeam的胶囊网络模型，该模型可以直接处理射频数据，通过非引导平面波照射提供波束形成数据包络。为了提高效率，采用了多层LookAhead Kernel Pruning（LAKP-ML）方法进行模型剪枝，并结合量化、简化非线性操作和并行化操作来优化硬件复杂性。最后，为优化后的CapsBeam模型设计并实现了一个专用的FPGA加速器。

Result: 在体内的实验中，CapsBeam减少了与标准延迟-求和（DAS）波束形成相比的伪影。在体外数据上，与DAS相比，CapsBeam的对比度提高了32.31%，轴向和横向分辨率分别提高了16.54%和6.7%。在计算机模拟数据上，与DAS相比，对比度提高了26%，轴向和横向分辨率分别提高了13.6%和21.5%。通过LAKP-ML方法实现了85%的模型压缩率，且图像质量未受影响。所提出的FPGA加速器在卷积操作上实现了30 GOPS，在动态路由操作上实现了17.4 GOPS的吞吐量。

Conclusion: CapsBeam是一种高效的胶囊网络波束形成器，适用于资源受限的边缘设备，它在提高图像质量的同时，通过模型压缩和硬件优化实现了高计算效率。

Abstract: In recent years, there has been a growing trend in accelerating
computationally complex non-real-time beamforming algorithms in ultrasound
imaging using deep learning models. However, due to the large size and
complexity these state-of-the-art deep learning techniques poses significant
challenges when deploying on resource-constrained edge devices. In this work,
we propose a novel capsule network based beamformer called CapsBeam, designed
to operate on raw radio-frequency data and provide an envelope of beamformed
data through non-steered plane wave insonification. Experiments on in-vivo
data, CapsBeam reduced artifacts compared to the standard Delay-and-Sum (DAS)
beamforming. For in-vitro data, CapsBeam demonstrated a 32.31% increase in
contrast, along with gains of 16.54% and 6.7% in axial and lateral resolution
compared to the DAS. Similarly, in-silico data showed a 26% enhancement in
contrast, along with improvements of 13.6% and 21.5% in axial and lateral
resolution, respectively, compared to the DAS. To reduce the parameter
redundancy and enhance the computational efficiency, we pruned the model using
our multi-layer LookAhead Kernel Pruning (LAKP-ML) methodology, achieving a
compression ratio of 85% without affecting the image quality. Additionally, the
hardware complexity of the proposed model is reduced by applying quantization,
simplification of non-linear operations, and parallelizing operations. Finally,
we proposed a specialized accelerator architecture for the pruned and optimized
CapsBeam model, implemented on a Xilinx ZU7EV FPGA. The proposed accelerator
achieved a throughput of 30 GOPS for the convolution operation and 17.4 GOPS
for the dynamic routing operation.

</details>


### [339] [Amplifying Effective CXL Memory Bandwidth for LLM Inference via Transparent Near-Data Processing](https://arxiv.org/abs/2509.03377)
*Rui Xie,Asad Ul Haq,Linsen Ma,Yunhua Fang,Zirak Burzin Engineer,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: CXL-NDP通过在CXL设备内集成位平面布局和无损压缩来提高LLM推理的有效CXL带宽，从而提高吞吐量，扩展上下文长度，并减小KV缓存占用空间，且不影响准确性。


<details>
  <summary>Details</summary>
Motivation: LLM推理受限于CXL内存的有限带宽，限制了容量扩展。CXL-NDP旨在解决此问题。

Method: CXL-NDP是一种透明的近数据处理架构，它在CXL设备内集成了用于动态量化的精度可伸缩位平面布局以及权值和KV缓存的透明无损压缩，而无需修改CXL.mem接口或AI模型。

Result: CXL-NDP在端到端服务中，将吞吐量提高了43%，将最大上下文长度延长了87%，并将KV缓存占用空间减小了46.9%，同时没有准确性损失。硬件综合结果表明，该技术具有适度的硅占用空间，在生成式AI基础设施中采用高效、可扩展的CXL内存的门槛较低。

Conclusion: CXL-NDP通过提高有效CXL带宽来克服LLM推理中的内存带宽瓶颈，为生成式AI提供了一种高效、可扩展的内存解决方案。

Abstract: Large language model (LLM) inference is bottlenecked by the limited bandwidth
of CXL-based memory used for capacity expansion. We introduce CXL-NDP, a
transparent near-data processing architecture that amplifies effective CXL
bandwidth without requiring changes to the CXL.mem interface or AI models.
CXL-NDP integrates a precision-scalable bit-plane layout for dynamic
quantization with transparent lossless compression of weights and KV caches
directly within the CXL device. In end-to-end serving, CXL-NDP improves
throughput by 43%, extends the maximum context length by 87%, and reduces the
KV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms
its practicality with a modest silicon footprint, lowering the barrier for
adopting efficient, scalable CXL-based memory in generative AI infrastructure.

</details>
