{"id": "2510.06754", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06754", "abs": "https://arxiv.org/abs/2510.06754", "authors": ["Christian Maurer", "Snehal Jauhri", "Sophie Lueth", "Georgia Chalvatzaki"], "title": "UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene", "comment": "Project website: https://sites.google.com/view/uniffield", "summary": "Comprehensive visual, geometric, and semantic understanding of a 3D scene is\ncrucial for successful execution of robotic tasks, especially in unstructured\nand complex environments. Additionally, to make robust decisions, it is\nnecessary for the robot to evaluate the reliability of perceived information.\nWhile recent advances in 3D neural feature fields have enabled robots to\nleverage features from pretrained foundation models for tasks such as\nlanguage-guided manipulation and navigation, existing methods suffer from two\ncritical limitations: (i) they are typically scene-specific, and (ii) they lack\nthe ability to model uncertainty in their predictions. We present UniFField, a\nunified uncertainty-aware neural feature field that combines visual, semantic,\nand geometric features in a single generalizable representation while also\npredicting uncertainty in each modality. Our approach, which can be applied\nzero shot to any new environment, incrementally integrates RGB-D images into\nour voxel-based feature representation as the robot explores the scene,\nsimultaneously updating uncertainty estimation. We evaluate our uncertainty\nestimations to accurately describe the model prediction errors in scene\nreconstruction and semantic feature prediction. Furthermore, we successfully\nleverage our feature predictions and their respective uncertainty for an active\nobject search task using a mobile manipulator robot, demonstrating the\ncapability for robust decision-making.", "AI": {"tldr": "UniFField\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u53ef\u611f\u77e5\u7684\u795e\u7ecf\u7279\u5f81\u573a\uff0c\u80fd\u591f\u7ed3\u5408\u89c6\u89c9\u3001\u8bed\u4e49\u548c\u51e0\u4f55\u7279\u5f81\uff0c\u5e76\u9884\u6d4b\u6bcf\u4e2a\u6a21\u6001\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u96f6\u6837\u672c\u5e94\u7528\u4e8e\u4efb\u4f55\u65b0\u73af\u5883\uff0c\u5e76\u901a\u8fc7\u589e\u91cf\u96c6\u6210RGB-D\u56fe\u50cf\u6765\u66f4\u65b0\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u53ef\u7528\u4e8e\u673a\u5668\u4eba\u4efb\u52a1\u7684\u9c81\u68d2\u51b3\u7b56\u3002", "motivation": "\u73b0\u67093D\u795e\u7ecf\u7279\u5f81\u573a\u65b9\u6cd5\u5728\u573a\u666f\u901a\u7528\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u963b\u788d\u4e86\u673a\u5668\u4eba\u7406\u89e3\u548c\u51b3\u7b56\u80fd\u529b\u7684\u63d0\u5347\u3002", "method": "UniFField\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u3001\u8bed\u4e49\u548c\u51e0\u4f55\u7279\u5f81\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u53ef\u6cdb\u5316\u8868\u793a\uff0c\u5e76\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002\u5b83\u80fd\u96f6\u6837\u672c\u5e94\u7528\u4e8e\u65b0\u73af\u5883\uff0c\u5e76\u901a\u8fc7\u589e\u91cf\u96c6\u6210RGB-D\u56fe\u50cf\u6765\u66f4\u65b0\u7279\u5f81\u8868\u793a\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "UniFField\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u591f\u51c6\u786e\u63cf\u8ff0\u573a\u666f\u91cd\u5efa\u548c\u8bed\u4e49\u7279\u5f81\u9884\u6d4b\u4e2d\u7684\u6a21\u578b\u9884\u6d4b\u8bef\u5dee\u3002\u5728\u79fb\u52a8\u64cd\u4f5c\u673a\u5668\u4eba\u4e3b\u52a8\u7269\u4f53\u641c\u7d22\u4efb\u52a1\u4e2d\uff0c\u5229\u7528\u5176\u7279\u5f81\u9884\u6d4b\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9c81\u68d2\u51b3\u7b56\u3002", "conclusion": "UniFField\u901a\u8fc7\u63d0\u4f9b\u7edf\u4e00\u7684\u3001\u53ef\u611f\u77e5\u7684\u795e\u7ecf\u7279\u5f81\u573a\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u80fd\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u9c81\u68d2\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
