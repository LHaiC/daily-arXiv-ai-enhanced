<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 88]
- [cs.CL](#cs.CL) [Total: 105]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [eess.SP](#eess.SP) [Total: 12]
- [physics.app-ph](#physics.app-ph) [Total: 5]
- [cs.ET](#cs.ET) [Total: 5]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.RO](#cs.RO) [Total: 28]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.GR](#cs.GR) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 19]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 10]
- [cs.AR](#cs.AR) [Total: 4]
- [quant-ph](#quant-ph) [Total: 97]
- [eess.SY](#eess.SY) [Total: 12]
- [cs.SI](#cs.SI) [Total: 4]
- [cs.LG](#cs.LG) [Total: 101]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Milestone Determination for Autonomous Railway Operation](https://arxiv.org/abs/2510.06229)
*Josh Hunter,John McDermid,Simon Burton,Poppy Fynes,Mia Dempster*

Main category: cs.CV

TL;DR: 通过关注里程碑确定来为铁路自动化生成数据集，以简化训练过程。


<details>
  <summary>Details</summary>
Motivation: 在铁路自动化领域，由于缺乏高质量、序列化的数据，开发有效的计算机视觉系统一直是一个关键挑战。传统的数据集范围有限，缺乏实时决策所需的时空背景，而替代解决方案则会引入真实性和适用性问题。

Method: 提出了一种基于里程碑确定概念的方法，以生成与现实世界运行逻辑更紧密结合的、面向特定路线的、与上下文相关的丰富序列数据集。这种方法通过消除对动态组件进行泛化识别的需求，而是专注于路线上的关键决策点，从而简化了学习过程。

Result: 该方法为在受控、可预测的环境中训练视觉代理提供了一个实用的框架。

Conclusion: 基于里程碑确定方法的路线特定数据集生成，为铁路自动化的安全高效机器学习系统提供了支持。

Abstract: In the field of railway automation, one of the key challenges has been the
development of effective computer vision systems due to the limited
availability of high-quality, sequential data. Traditional datasets are
restricted in scope, lacking the spatio temporal context necessary for
real-time decision-making, while alternative solutions introduce issues related
to realism and applicability. By focusing on route-specific, contextually
relevant cues, we can generate rich, sequential datasets that align more
closely with real-world operational logic. The concept of milestone
determination allows for the development of targeted, rule-based models that
simplify the learning process by eliminating the need for generalized
recognition of dynamic components, focusing instead on the critical decision
points along a route. We argue that this approach provides a practical
framework for training vision agents in controlled, predictable environments,
facilitating safer and more efficient machine learning systems for railway
automation.

</details>


### [2] [CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation](https://arxiv.org/abs/2510.06231)
*Mingzhe Zheng,Dingjie Song,Guanyu Zhou,Jun You,Jiahao Zhan,Xuran Ma,Xinyuan Song,Ser-Nam Lim,Qifeng Chen,Harry Yang*

Main category: cs.CV

TL;DR: LLMs在生成结构化文本方面表现出色，但在电影剧本创作中，尤其是在情感深度和叙事技巧方面，往往表现不足。本研究提出了CML-Dataset和CML-Bench，用于评估和改进LLM生成的剧本质量，并通过CML-Instruction提示策略指导LLM创作更高质量的剧本。


<details>
  <summary>Details</summary>
Motivation: LLM在生成结构化文本方面表现出色，但在电影剧本创作中，尤其是在情感深度和叙事技巧方面，往往表现不足，缺乏电影的‘灵魂’。

Method: 构建了CML-Dataset（包含摘要和内容对），并分析了电影剧本中的连续性和叙事结构，识别出对话连贯性（DC）、角色一致性（CC）和情节合理性（PR）三个关键质量维度。基于此，提出了CML-Bench基准，并设计了CML-Instruction提示策略来指导LLM生成更高质量的剧本。

Result: CML-Bench能够有效地区分人类编写的高质量剧本和LLM生成的剧本，并指出LLM剧本的不足之处。经过CML-Instruction指导的LLM生成的剧本在质量上优于未指导的LLM，并且结果与人类偏好一致。

Conclusion: CML-Bench和CML-Instruction策略能够有效提升LLM在电影剧本创作方面的能力，生成的剧本在质量上更符合人类的期望。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating highly structured texts. However, while exhibiting a high degree of
structural organization, movie scripts demand an additional layer of nuanced
storytelling and emotional depth-the 'soul' of compelling cinema-that LLMs
often fail to capture. To investigate this deficiency, we first curated
CML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup
Language (CML), where 'content' consists of segments from esteemed,
high-quality movie scripts and 'summary' is a concise description of the
content. Through an in-depth analysis of the intrinsic multi-shot continuity
and narrative structures within these authentic scripts, we identified three
pivotal dimensions for quality assessment: Dialogue Coherence (DC), Character
Consistency (CC), and Plot Reasonableness (PR). Informed by these findings, we
propose the CML-Bench, featuring quantitative metrics across these dimensions.
CML-Bench effectively assigns high scores to well-crafted, human-written
scripts while concurrently pinpointing the weaknesses in screenplays generated
by LLMs. To further validate our benchmark, we introduce CML-Instruction, a
prompting strategy with detailed instructions on character dialogue and event
logic, to guide LLMs to generate more structured and cinematically sound
scripts. Extensive experiments validate the effectiveness of our benchmark and
demonstrate that LLMs guided by CML-Instruction generate higher-quality
screenplays, with results aligned with human preferences.

</details>


### [3] [User to Video: A Model for Spammer Detection Inspired by Video Classification Technology](https://arxiv.org/abs/2510.06233)
*Haoyang Zhang,Zhou Yang,Yucai Pang*

Main category: cs.CV

TL;DR: 通过将用户行为子空间视为视频，提出了一种名为UVSD的用户视频化垃圾信息检测模型。


<details>
  <summary>Details</summary>
Motivation: 受视频分类技术的启发，将用户行为子空间视为图像，连续的帧图像视为视频，以解决垃圾信息检测问题。

Method: 提出user2piexl算法将用户和立场量化为像素的RGB值，提出behavior2image算法将用户行为子空间转化为帧图像，并构建用户行为视频。最后，结合视频分类算法进行垃圾信息识别。

Result: 在WEIBO和TWITTER数据集上的实验表明，UVSD模型优于现有方法。

Conclusion: UVSD模型在垃圾信息检测方面表现出优势。

Abstract: This article is inspired by video classification technology. If the user
behavior subspace is viewed as a frame image, consecutive frame images are
viewed as a video. Following this novel idea, a model for spammer detection
based on user videoization, called UVSD, is proposed. Firstly, a user2piexl
algorithm for user pixelization is proposed. Considering the adversarial
behavior of user stances, the user is viewed as a pixel, and the stance is
quantified as the pixel's RGB. Secondly, a behavior2image algorithm is proposed
for transforming user behavior subspace into frame images. Low-rank dense
vectorization of subspace user relations is performed using representation
learning, while cutting and diffusion algorithms are introduced to complete the
frame imageization. Finally, user behavior videos are constructed based on
temporal features. Subsequently, a video classification algorithm is combined
to identify the spammers. Experiments using publicly available datasets, i.e.,
WEIBO and TWITTER, show an advantage of the UVSD model over state-of-the-art
methods.

</details>


### [4] [Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout](https://arxiv.org/abs/2510.06238)
*Sagar Lekhak,Emmett J. Ientilucci,Dimah Dera,Susmita Ghosh*

Main category: cs.CV

TL;DR: 通过蒙特卡洛（MC）Dropout集成不确定性量化来增强地雷和未爆弹药（UXO）检测的鲁棒性，以应对噪声和对抗性攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在地雷和未爆弹药（UXO）检测中可能容易受到噪声和对抗性攻击的影响，导致漏检或误分类。本研究旨在通过不确定性量化来提高模型的可靠性。

Method: 将蒙特卡洛（MC）Dropout集成到经过微调的ResNet-50架构中，用于表面地雷和未爆弹药（UXO）的分类，并在模拟数据集上进行测试。

Result: 实验结果表明，该模型能够识别不可靠的预测，尤其是在存在噪声和对抗性扰动的情况下。MC Dropout有助于量化认知不确定性，为预测可靠性提供额外指标。

Conclusion: 不确定性量化对于地雷和未爆弹药（UXO）检测至关重要，现有的神经网络易受对抗性威胁，需要开发更鲁棒、更可靠的模型以满足实际应用需求。

Abstract: Detecting surface landmines and unexploded ordnances (UXOs) using deep
learning has shown promise in humanitarian demining. However, deterministic
neural networks can be vulnerable to noisy conditions and adversarial attacks,
leading to missed detection or misclassification. This study introduces the
idea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated
into a fine-tuned ResNet-50 architecture for surface landmine and UXO
classification, which was tested on a simulated dataset. Integrating the MC
Dropout approach helps quantify epistemic uncertainty, providing an additional
metric for prediction reliability, which could be helpful to make more informed
decisions in demining operations. Experimental results on clean, adversarially
perturbed, and noisy test images demonstrate the model's ability to flag
unreliable predictions under challenging conditions. This proof-of-concept
study highlights the need for uncertainty quantification in demining, raises
awareness about the vulnerability of existing neural networks in demining to
adversarial threats, and emphasizes the importance of developing more robust
and reliable models for practical applications.

</details>


### [5] [multimodars: A Rust-powered toolkit for multi-modality cardiac image fusion and registration](https://arxiv.org/abs/2510.06241)
*Anselm W. Stark,Marc Ilic,Ali Mokhtari,Pooya Mohammadi Kazaj,Christoph Graeni,Isaac Shiri*

Main category: cs.CV

TL;DR: 提供一个不那么冗长; 没读摘要


<details>
  <summary>Details</summary>
Motivation: 结合互补的成像模式对于构建可靠的 3D 冠状动脉模型至关重要：血管内成像提供亚毫米级分辨率但血管整体背景有限，而 CCTA 提供 3D 几何结构但空间分辨率有限且存在伪影（例如，开花）。

Method: multimodars 解决了这个差距，采用了确定性对齐算法、以 NumPy 为中心的紧凑数据模型以及优化的 Rust 后端，适用于可扩展、可重现的实验。该包接受 CSV/NumPy 输入，包括 AIVUS-CAA 软件生成的数据格式。

Result: 以前的工作证明了血管内/CCTA 融合，但没有开放、灵活的工具包针对多状态分析（静息/应激、支架置入前/后）进行定制，同时提供确定性行为、高性能和易于进行流水线集成。

Conclusion: 结合互补的成像模式对于构建可靠的 3D 冠状动脉模型至关重要。

Abstract: Combining complementary imaging modalities is critical to build reliable 3D
coronary models: intravascular imaging gives sub-millimetre resolution but
limited whole-vessel context, while CCTA supplies 3D geometry but suffers from
limited spatial resolution and artefacts (e.g., blooming). Prior work
demonstrated intravascular/CCTA fusion, yet no open, flexible toolkit is
tailored for multi-state analysis (rest/stress, pre-/post-stenting) while
offering deterministic behaviour, high performance, and easy pipeline
integration. multimodars addresses this gap with deterministic alignment
algorithms, a compact NumPy-centred data model, and an optimised Rust backend
suitable for scalable, reproducible experiments. The package accepts CSV/NumPy
inputs including data formats produced by the AIVUS-CAA software

</details>


### [6] [Does Physics Knowledge Emerge in Frontier Models?](https://arxiv.org/abs/2510.06251)
*Ieva Bagdonaviciute,Vibhav Vineet*

Main category: cs.CV

TL;DR: 当前领先的视觉-语言模型（VLM）在物理推理任务上表现不佳，尽管它们在感知和一般推理方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 评估现有VLM在理解和预测物理动力学方面的能力，并找出其局限性。

Method: 在三个物理模拟数据集（CLEVRER、Physion、Physion++）上对六个前沿VLM进行基准测试，并设计了诊断性子测试，将感知能力与物理推理能力分离开来。

Result: 研究发现，VLM在感知和物理推理方面的表现与在预测和反事实评估任务上的表现之间相关性较弱，表明它们的感知和物理技能是碎片化的，未能整合成因果理解。

Conclusion: 当前的VLM在整合感知和推理以实现因果理解方面存在局限性，需要更紧密地结合感知和推理的架构。

Abstract: Leading Vision-Language Models (VLMs) show strong results in visual
perception and general reasoning, but their ability to understand and predict
physical dynamics remains unclear. We benchmark six frontier VLMs on three
physical simulation datasets - CLEVRER, Physion, and Physion++ - where the
evaluation tasks test whether a model can predict outcomes or hypothesize about
alternative situations. To probe deeper, we design diagnostic subtests that
isolate perception (objects, colors, occluders) from physics reasoning (motion
prediction, spatial relations). Intuitively, stronger diagnostic performance
should support higher evaluation accuracy. Yet our analysis reveals weak
correlations: models that excel at perception or physics reasoning do not
consistently perform better on predictive or counterfactual evaluation. This
counterintuitive gap exposes a central limitation of current VLMs: perceptual
and physics skills remain fragmented and fail to combine into causal
understanding, underscoring the need for architectures that bind perception and
reasoning more tightly.

</details>


### [7] [Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training](https://arxiv.org/abs/2510.06254)
*Xiaochen Zhao,Chengting Yu,Kairong Yu,Lei Liu,Aili Wang*

Main category: cs.CV

TL;DR: 通过增强的自蒸馏框架和基于速率的反向传播，在降低训练复杂度的同时提高SNN性能。


<details>
  <summary>Details</summary>
Motivation: 传统的SNN训练方法在性能上落后于ANN，并且计算和内存开销巨大，尤其是在时间维度上，这限制了SNN在计算资源有限的情况下的训练。因此，需要一种能够提高SNN性能并降低训练复杂度的训练方法。

Method: 提出了一种增强的自蒸馏框架，并与基于速率的反向传播联合优化。该方法将中间SNN层的发放速率投影到轻量级ANN分支上，并利用模型自身生成的高质量知识通过ANN通路优化子结构。为了解决低质量自生成知识可能阻碍收敛的问题，将教师信号解耦为可靠和不可靠的组件，并仅使用可靠知识来指导模型优化。

Result: 所提出的方法在CIFAR-10、CIFAR-100、CIFAR10-DVS和ImageNet数据集上进行了广泛的实验，结果表明该方法在降低训练复杂度的同时实现了高性能的SNN训练。

Conclusion: 增强的自蒸馏框架与基于速率的反向传播相结合，能够有效地提高SNN的性能，同时降低训练的计算和内存开销，使其在计算资源有限的情况下具有可行性。

Abstract: Spiking Neural Networks (SNNs) exhibit exceptional energy efficiency on
neuromorphic hardware due to their sparse activation patterns. However,
conventional training methods based on surrogate gradients and Backpropagation
Through Time (BPTT) not only lag behind Artificial Neural Networks (ANNs) in
performance, but also incur significant computational and memory overheads that
grow linearly with the temporal dimension. To enable high-performance SNN
training under limited computational resources, we propose an enhanced
self-distillation framework, jointly optimized with rate-based backpropagation.
Specifically, the firing rates of intermediate SNN layers are projected onto
lightweight ANN branches, and high-quality knowledge generated by the model
itself is used to optimize substructures through the ANN pathways. Unlike
traditional self-distillation paradigms, we observe that low-quality
self-generated knowledge may hinder convergence. To address this, we decouple
the teacher signal into reliable and unreliable components, ensuring that only
reliable knowledge is used to guide the optimization of the model. Extensive
experiments on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate that
our method reduces training complexity while achieving high-performance SNN
training. Our code is available at
https://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn.

</details>


### [8] [Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis](https://arxiv.org/abs/2510.06260)
*Sher Khan,Raz Muhammad,Adil Hussain,Muhammad Sajjad,Muhammad Rashid*

Main category: cs.CV

TL;DR: 该研究提出了一个统一的AI框架，用于改进皮肤癌诊断，通过结合异构CNN模型和集成LLM来提高诊断的准确性和可解释性，并改善医患沟通。


<details>
  <summary>Details</summary>
Motivation: 现有的皮肤癌诊断方法存在观察者间差异和医疗资源分配不均的问题。虽然AI有潜力，但现有系统在模型多样性、皮肤色调数据集偏差以及将自然语言处理视为独立于临床决策的解释方面存在局限。本研究旨在解决这些问题，以提高诊断的可靠性并克服沟通障碍。

Method: 本研究引入了一个统一的框架，包含两个关键创新：1. 使用架构多样化的卷积神经网络（CNN）异构集成模型，并内置不确定性机制，用于识别不一致的病例并将其交给专家审查。2. 将大型语言模型（LLM）的能力直接整合到诊断工作流程中，将分类输出转化为有临床意义的评估，同时满足医疗文档要求并提供以患者为中心的教育。

Result: 该框架能够生成结构化报告，包含精确的病变特征描述、易于理解的诊断推理以及可操作的监测指南，使患者能够识别随访之间的早期预警信号。通过在单一的、连贯的系统中同时解决诊断可靠性和沟通障碍，本研究弥合了先前AI实施未能产生临床影响的关键转化差距。

Conclusion: 该框架代表了可部署的皮肤病学AI的一个重大进步，它提高了诊断精度，同时积极支持从初步检测到患者教育的连续护理，最终提高了皮肤病变早期干预率。

Abstract: Cutaneous malignancies demand early detection for favorable outcomes, yet
current diagnostics suffer from inter-observer variability and access
disparities. While AI shows promise, existing dermatological systems are
limited by homogeneous architectures, dataset biases across skin tones, and
fragmented approaches that treat natural language processing as separate
post-hoc explanations rather than integral to clinical decision-making. We
introduce a unified framework that fundamentally reimagines AI integration for
dermatological diagnostics through two synergistic innovations. First, a
purposefully heterogeneous ensemble of architecturally diverse convolutional
neural networks provides complementary diagnostic perspectives, with an
intrinsic uncertainty mechanism flagging discordant cases for specialist review
-- mimicking clinical best practices. Second, we embed large language model
capabilities directly into the diagnostic workflow, transforming classification
outputs into clinically meaningful assessments that simultaneously fulfill
medical documentation requirements and deliver patient-centered education. This
seamless integration generates structured reports featuring precise lesion
characterization, accessible diagnostic reasoning, and actionable monitoring
guidance -- empowering patients to recognize early warning signs between
visits. By addressing both diagnostic reliability and communication barriers
within a single cohesive system, our approach bridges the critical
translational gap that has prevented previous AI implementations from achieving
clinical impact. The framework represents a significant advancement toward
deployable dermatological AI that enhances diagnostic precision while actively
supporting the continuum of care from initial detection through patient
education, ultimately improving early intervention rates for skin lesions.

</details>


### [9] [Vision Transformer for Transient Noise Classification](https://arxiv.org/abs/2510.06273)
*Divyansh Srivastava,Andrzej Niedzielski*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Transient noise (glitches) in LIGO data hinders the detection of
gravitational waves (GW). The Gravity Spy project has categorized these noise
events into various classes. With the O3 run, there is the inclusion of two
additional noise classes and thus a need to train new models for effective
classification. We aim to classify glitches in LIGO data into 22 existing
classes from the first run plus 2 additional noise classes from O3a using the
Vision Transformer (ViT) model. We train a pre-trained Vision Transformer
(ViT-B/32) model on a combined dataset consisting of the Gravity Spy dataset
with the additional two classes from the LIGO O3a run. We achieve a
classification efficiency of 92.26%, demonstrating the potential of Vision
Transformer to improve the accuracy of gravitational wave detection by
effectively distinguishing transient noise.
  Key words: gravitational waves --vision transformer --machine learning

</details>


### [10] [General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks](https://arxiv.org/abs/2510.06277)
*Fahim Shahriar,Cheryl Wang,Alireza Azimi,Gautham Vasan,Hany Hamed Elanwar,A. Rupam Mahmood,Colin Bellinger*

Main category: cs.CV

TL;DR: 提出一种基于掩码的目标表征系统，用于目标条件强化学习，提高了学习效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目标条件强化学习（GCRL）的成功依赖于目标表征的选择。现有的表征方法（如目标状态图像、3D坐标、独热向量）存在泛化性差、收敛慢和需要特殊相机等问题。

Method: 提出一种基于掩码的目标表征系统，提供目标不可知的视觉线索，能够生成密集奖励，无需进行易出错的距离计算。

Result: 在模拟环境中，使用地面真实掩码进行学习，在训练和未见过测试目标上均达到了99.9%的到达精度。该方法可用于执行高精度拾取任务，无需目标位置信息。此外，在两种不同的物理机器人上演示了从头学习和模拟到现实的迁移应用，并使用了预训练的开放词汇目标检测模型来生成掩码。

Conclusion: 基于掩码的目标表征系统在GCRL中能够实现高效学习和卓越泛化能力，克服了现有方法的局限性。

Abstract: Goal-conditioned reinforcement learning (GCRL) allows agents to learn diverse
objectives using a unified policy. The success of GCRL, however, is contingent
on the choice of goal representation. In this work, we propose a mask-based
goal representation system that provides object-agnostic visual cues to the
agent, enabling efficient learning and superior generalization. In contrast,
existing goal representation methods, such as target state images, 3D
coordinates, and one-hot vectors, face issues of poor generalization to unseen
objects, slow convergence, and the need for special cameras. Masks can be
processed to generate dense rewards without requiring error-prone distance
calculations. Learning with ground truth masks in simulation, we achieved 99.9%
reaching accuracy on training and unseen test objects. Our proposed method can
be utilized to perform pick-up tasks with high accuracy, without using any
positional information of the target. Moreover, we demonstrate learning from
scratch and sim-to-real transfer applications using two different physical
robots, utilizing pretrained open vocabulary object detection models for mask
generation.

</details>


### [11] [Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning](https://arxiv.org/abs/2510.06281)
*Chenyang Li,Qin Li,Haimin Wang,Bo Shen*

Main category: cs.CV

TL;DR: 该研究提出了一种基于GAN的超分辨率方法，用于增强GONG的低分辨率Hα太阳图像，使其达到BBSO/GST的高分辨率图像质量。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有全盘Hα图像分辨率不足以解析细微尺度动力学特征（如纤维和细丝）的问题。

Method: 采用Real-ESRGAN模型，并结合残差中的残差密集块（Residual-in-Residual Dense Blocks）和相对论性判别器，对GONG和BBSO/GST的图像对进行训练和对齐。

Result: 模型能够有效恢复太阳黑子半影内的细节，并解析纤维和细丝的精细结构，平均均方误差（MSE）为467.15，均方根误差（RMSE）为21.59，互相关系数（CC）为0.7794。

Conclusion: 所提出的GAN超分辨率方法能够有效提升太阳Hα图像的细节分辨率。图像对之间轻微的对齐不准确性限制了量化性能，未来将通过解决此问题和扩展数据集来进一步提高重建质量。

Abstract: High-resolution (HR) solar imaging is crucial for capturing fine-scale
dynamic features such as filaments and fibrils. However, the spatial resolution
of the full-disk H$\alpha$ images is limited and insufficient to resolve these
small-scale structures. To address this, we propose a GAN-based superresolution
approach to enhance low-resolution (LR) full-disk H$\alpha$ images from the
Global Oscillation Network Group (GONG) to a quality comparable with HR
observations from the Big Bear Solar Observatory/Goode Solar Telescope
(BBSO/GST). We employ Real-ESRGAN with Residual-in-Residual Dense Blocks and a
relativistic discriminator. We carefully aligned GONG-GST pairs. The model
effectively recovers fine details within sunspot penumbrae and resolves fine
details in filaments and fibrils, achieving an average mean squared error (MSE)
of 467.15, root mean squared error (RMSE) of 21.59, and cross-correlation (CC)
of 0.7794. Slight misalignments between image pairs limit quantitative
performance, which we plan to address in future work alongside dataset
expansion to further improve reconstruction quality.

</details>


### [12] [Validation of Various Normalization Methods for Brain Tumor Segmentation: Can Federated Learning Overcome This Heterogeneity?](https://arxiv.org/abs/2510.07126)
*Jan Fiszer,Dominika Ciupek,Maciej Malawski*

Main category: cs.CV

TL;DR: 联邦学习（FL）在处理非独立同分布（non-IID）的医学影像数据时表现出鲁棒性，


<details>
  <summary>Details</summary>
Motivation: 医学影像深度学习（DL）面临数据隐私、存储和传输的挑战，而联邦学习（FL）可解决这些问题，但其在non-IID数据上的有效性可能降低。

Method: 通过对不同MRI强度归一化技术处理的数据子集模拟non-IID条件，训练和测试脑肿瘤分割模型。

Result: FL方法在处理跨客户端不一致归一化数据时表现出鲁棒性，3D Dice得分为92%，可与中心化模型媲美。

Conclusion: FL是无需侵犯数据隐私即可训练高性能模型的解决方案，这在医学应用中至关重要。

Abstract: Deep learning (DL) has been increasingly applied in medical imaging, however,
it requires large amounts of data, which raises many challenges related to data
privacy, storage, and transfer. Federated learning (FL) is a training paradigm
that overcomes these issues, though its effectiveness may be reduced when
dealing with non-independent and identically distributed (non-IID) data. This
study simulates non-IID conditions by applying different MRI intensity
normalization techniques to separate data subsets, reflecting a common cause of
heterogeneity. These subsets are then used for training and testing models for
brain tumor segmentation. The findings provide insights into the influence of
the MRI intensity normalization methods on segmentation models, both training
and inference. Notably, the FL methods demonstrated resilience to
inconsistently normalized data across clients, achieving the 3D Dice score of
92%, which is comparable to a centralized model (trained using all data). These
results indicate that FL is a solution to effectively train high-performing
models without violating data privacy, a crucial concern in medical
applications. The code is available at:
https://github.com/SanoScience/fl-varying-normalization.

</details>


### [13] [ChainMPQ: Interleaved Text-Image Reasoning Chains for Mitigating Relation Hallucinations](https://arxiv.org/abs/2510.06292)
*Yike Wu,Yiwei Wang,Yujun Cai*

Main category: cs.CV

TL;DR: ChainMPQ是一种无需训练的方法，通过多角度提问和累积的文本/视觉记忆来提升大型视觉语言模型（LVLM）的关系推理能力，有效减少关系幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）在多模态任务中表现出色，但幻觉问题限制了其可靠性。其中，关系幻觉占比最大但受到的关注最少。

Method: ChainMPQ通过提取问题中的主语和宾语关键词来增强相关图像区域，然后构建关注主语、宾语和关系三个核心成分的多角度问题。这些问题按顺序输入模型，前一步的文本和视觉记忆为后一步提供支持，形成交织的图像和文本链，引导逐步的关系推理。

Result: 在多个LVLM和基准测试上的实验表明，ChainMPQ显著减少了关系幻觉。消融研究验证了其三个核心模块的有效性。

Conclusion: ChainMPQ是一种有效的训练无关方法，通过多角度提问和累积记忆来解决LVLM中的关系幻觉问题。

Abstract: While Large Vision-Language Models (LVLMs) achieve strong performance in
multimodal tasks, hallucinations continue to hinder their reliability. Among
the three categories of hallucinations, which include object, attribute, and
relation, relation hallucinations account for the largest proportion but have
received the least attention. To address this issue, we propose ChainMPQ
(Multi-Perspective Questions guided Interleaved Chain of Image and Text), a
training-free method that improves relational inference in LVLMs by utilizing
accumulated textual and visual memories. ChainMPQ first extracts subject and
object keywords from the question to enhance the corresponding image regions.
It then constructs multi-perspective questions that focus on the three core
components of a relationship: the subject, the object, and the relation that
links them. These questions are sequentially input to the model, with textual
and visual memories from earlier steps providing supporting context for
subsequent ones, thereby forming an interleaved chain of images and text that
guides progressive relational reasoning. Experiments on multiple LVLMs and
benchmarks show that ChainMPQ substantially reduces relation hallucinations,
while ablation studies further validate the effectiveness of its three core
modules.

</details>


### [14] [Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling](https://arxiv.org/abs/2510.06295)
*Young D. Kwon,Abhinav Mehrotra,Malcolm Chadwick,Alberto Gil Ramos,Sourav Bhattacharya*

Main category: cs.CV

TL;DR: MobilePicasso是一个高效的高分辨率图像编辑系统，它通过多阶段处理（标准分辨率编辑、潜在投影、自适应上下文保留平铺）来解决移动设备上的计算和内存限制，实现了更好的图像质量和更低的延迟。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在资源受限设备上进行高分辨率图像编辑时，在内存和图像质量方面存在挑战。

Method: MobilePicasso包含三个阶段：(i) 在标准分辨率下进行图像编辑，并采用感知损失；(ii) 应用潜在投影以避免像素空间操作；(iii) 使用自适应上下文保留平铺将编辑后的图像潜在特征升级到更高分辨率。

Result: 用户研究表明，MobilePicasso相比现有方法，图像质量提高了18-48%，幻觉减少了14-51%。该系统延迟显著降低（最高可达55.8倍加速），内存占用小幅增加（仅9%），并且在移动设备上的运行速度甚至快于在A100 GPU上运行的服务器端模型。

Conclusion: MobilePicasso成功实现了高效的高分辨率图像编辑，解决了移动设备面临的计算和内存限制，并在图像质量和速度方面取得了显著的改进。

Abstract: High-resolution (4K) image-to-image synthesis has become increasingly
important for mobile applications. Existing diffusion models for image editing
face significant challenges, in terms of memory and image quality, when
deployed on resource-constrained devices. In this paper, we present
MobilePicasso, a novel system that enables efficient image editing at high
resolutions, while minimising computational cost and memory usage.
MobilePicasso comprises three stages: (i) performing image editing at a
standard resolution with hallucination-aware loss, (ii) applying latent
projection to overcome going to the pixel space, and (iii) upscaling the edited
image latent to a higher resolution with adaptive context-preserving tiling.
Our user study with 46 participants reveals that MobilePicasso not only
improves image quality by 18-48% but reduces hallucinations by 14-51% over
existing methods. MobilePicasso demonstrates significantly lower latency, e.g.,
up to 55.8$\times$ speed-up, yet with a small increase in runtime memory, e.g.,
a mere 9% increase over prior work. Surprisingly, the on-device runtime of
MobilePicasso is observed to be faster than a server-based high-resolution
image editing model running on an A100 GPU.

</details>


### [15] [RGBD Gaze Tracking Using Transformer for Feature Fusion](https://arxiv.org/abs/2510.06298)
*Tobias J. Bauer*

Main category: cs.CV

TL;DR: 该论文实现了一个基于RGBD图像和Transformer架构的AI Gaze Tracking系统，并创建了一个新的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏深度信息或标签不适用于Gaze Angle Estimation，且RGBD输入与Transformer的结合尚未被深入研究。

Method: 使用Transformer架构融合RGBD图像特征，并与生成对抗网络（GAN）结合，同时去除深度图伪影和提取头部姿态特征。对Transformer模块进行了实验，并与MLP进行了比较。

Result: 在ShanghaiTechGaze+ 数据集上，Transformer模块的模型平均欧氏误差为55.3mm，无GAN模块的模型为30.1mm，MLP模型为26.9mm。在ETH-XGaze数据集上，Transformer模型平均角度误差为3.59度，无Transformer模型为3.26度，而数据集作者的模型为2.04度。

Conclusion: Transformer模块在Gaze Tracking任务中并非最优选择，MLP模型表现更好。

Abstract: Subject of this thesis is the implementation of an AI-based Gaze Tracking
system using RGBD images that contain both color (RGB) and depth (D)
information. To fuse the features extracted from the images, a module based on
the Transformer architecture is used. The combination of RGBD input images and
Transformers was chosen because it has not yet been investigated. Furthermore,
a new dataset is created for training the AI models as existing datasets either
do not contain depth information or only contain labels for Gaze Point
Estimation that are not suitable for the task of Gaze Angle Estimation. Various
model configurations are trained, validated and evaluated on a total of three
different datasets. The trained models are then to be used in a real-time
pipeline to estimate the gaze direction and thus the gaze point of a person in
front of a computer screen. The AI model architecture used in this thesis is
based on an earlier work by Lian et al. It uses a Generative Adversarial
Network (GAN) to simultaneously remove depth map artifacts and extract head
pose features. Lian et al. achieve a mean Euclidean error of 38.7mm on their
own dataset ShanghaiTechGaze+. In this thesis, a model architecture with a
Transformer module for feature fusion achieves a mean Euclidean error of 55.3mm
on the same dataset, but we show that using no pre-trained GAN module leads to
a mean Euclidean error of 30.1mm. Replacing the Transformer module with a
Multilayer Perceptron (MLP) improves the error to 26.9mm. These results are
coherent with the ones on the other two datasets. On the ETH-XGaze dataset, the
model with Transformer module achieves a mean angular error of 3.59{\deg} and
without Transformer module 3.26{\deg}, whereas the fundamentally different
model architecture used by the dataset authors Zhang et al. achieves a mean
angular error of 2.04{\deg}. On the OTH-Gaze-Estimation dataset created for...

</details>


### [16] [Scalable deep fusion of spaceborne lidar and synthetic aperture radar for global forest structural complexity mapping](https://arxiv.org/abs/2510.06299)
*Tiago de Conto,John Armston,Ralph Dubayah*

Main category: cs.CV

TL;DR: 本文提出了一种利用深度学习融合 GEDI 和 SAR 数据来生成全球高分辨率森林结构复杂性地图的方法。


<details>
  <summary>Details</summary>
Motivation: 为了克服 GEDI 数据采样稀疏的限制，实现连续高分辨率的森林结构复杂性测绘。

Method: 开发了一种基于 EfficientNetV2 的深度学习框架，融合 GEDI 和多模态 SAR 数据，并在超过 1.3 亿个 GEDI 数据点上进行训练。

Result: 生成了全球 25 米分辨率、2015-2022 年的时间序列森林结构复杂性地图，模型具有高精度（全局 R2 = 0.82）和较少的参数量，并提供不确定性估计。

Conclusion: 该框架能够为全球森林结构动态提供连续、多时相的监测，并支持生物多样性保护和生态系统管理。

Abstract: Forest structural complexity metrics integrate multiple canopy attributes
into a single value that reflects habitat quality and ecosystem function.
Spaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) has
enabled mapping of structural complexity in temperate and tropical forests, but
its sparse sampling limits continuous high-resolution mapping. We present a
scalable, deep learning framework fusing GEDI observations with multimodal
Synthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25
m) wall-to-wall maps of forest structural complexity. Our adapted
EfficientNetV2 architecture, trained on over 130 million GEDI footprints,
achieves high performance (global R2 = 0.82) with fewer than 400,000
parameters, making it an accessible tool that enables researchers to process
datasets at any scale without requiring specialized computing infrastructure.
The model produces accurate predictions with calibrated uncertainty estimates
across biomes and time periods, preserving fine-scale spatial patterns. It has
been used to generate a global, multi-temporal dataset of forest structural
complexity from 2015 to 2022. Through transfer learning, this framework can be
extended to predict additional forest structural variables with minimal
computational cost. This approach supports continuous, multi-temporal
monitoring of global forest structural dynamics and provides tools for
biodiversity conservation and ecosystem management efforts in a changing
climate.

</details>


### [17] [Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding](https://arxiv.org/abs/2510.06308)
*Yi Xin,Qi Qin,Siqi Luo,Kaiwen Zhu,Juncheng Yan,Yan Tai,Jiayi Lei,Yuewen Cao,Keqi Wang,Yibin Wang,Jinbin Bai,Qian Yu,Dengyang Jiang,Yuandong Pu,Haoxing Chen,Le Zhuo,Junjun He,Gen Luo,Tianbin Li,Ming Hu,Jin Ye,Shenglong Ye,Bo Zhang,Chang Xu,Wenhai Wang,Hongsheng Li,Guangtao Zhai,Tianfan Xue,Bin Fu,Xiaohong Liu,Yu Qiao,Yihao Liu*

Main category: cs.CV

TL;DR: Lumina-DiMOO是一个开源的多模态生成和理解模型，采用完全离散的扩散模型，提高了采样效率，并在多项基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 介绍Lumina-DiMOO，一个用于无缝多模态生成和理解的开源基础模型。

Method: 使用完全离散的扩散模型来处理各种模态的输入和输出，以实现更高的采样效率并支持广泛的多模态任务。

Result: 在多个基准测试中取得了最先进的性能，优于现有的开源统一多模态模型。

Conclusion: 发布代码和检查点以促进多模态和离散扩散模型的研究。

Abstract: We introduce Lumina-DiMOO, an open-source foundational model for seamless
multi-modal generation and understanding. Lumina-DiMOO sets itself apart from
prior unified models by utilizing a fully discrete diffusion modeling to handle
inputs and outputs across various modalities. This innovative approach allows
Lumina-DiMOO to achieve higher sampling efficiency compared to previous
autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a
broad spectrum of multi-modal tasks, including text-to-image generation,
image-to-image generation (e.g., image editing, subject-driven generation, and
image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves
state-of-the-art performance on multiple benchmarks, surpassing existing
open-source unified multi-modal models. To foster further advancements in
multi-modal and discrete diffusion model research, we release our code and
checkpoints to the community. Project Page:
https://synbol.github.io/Lumina-DiMOO.

</details>


### [18] [TransFIRA: Transfer Learning for Face Image Recognizability Assessment](https://arxiv.org/abs/2510.06353)
*Allen Tu,Kartik Narayan,Joshua Gleason,Jennifer Xu,Matthew Meyn,Tom Goldstein,Vishal M. Patel*

Main category: cs.CV

TL;DR: TransFIRA是一个轻量级、无标注的框架，用于在不受限的环境中评估人脸识别的可辨识度。它通过类中心相似性(CCS)和类中心角度分离(CCAS)来定义可辨识度，并提出了一种可辨识度感知聚合策略，在不使用外部标签、启发式方法或特定骨干训练的情况下，实现了最先进的验证精度。此外，TransFIRA还提供了超出人脸识别的扩展，包括可解释性分析和身体识别评估。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸图像质量评估(FIQA)方法在不受限环境中（如监控、视频、网络图像）评估人脸识别时的可辨识度方面存在不足，因为这些环境中的极端变化（如姿势、模糊、光照、遮挡）导致常规视觉质量指标失效。现有的FIQA方法依赖于视觉启发式、精心策划的注释或计算密集型生成流程，导致其预测结果与编码器的决策几何不匹配。

Method: TransFIRA框架通过类中心相似性(CCS)和类中心角度分离(CCAS)来定义人脸图像的可辨识度，这是第一个与决策边界对齐的自然过滤和加权标准。它还采用了一种可辨识度感知聚合策略，直接在嵌入空间中进行评估，无需外部标签、启发式方法或骨干特定训练。

Result: TransFIRA在BRIAR和IJB-C数据集上实现了最先进的人脸验证准确率，并且与真实可辨识度的相关性几乎翻倍。此外，它在身体识别评估方面也表现出强大的性能，并在跨数据集迁移方面表现出鲁棒性。TransFIRA还实现了编码器驱动的可解释性，揭示了降级和特定于主体因素对可辨识度的影响。

Conclusion: TransFIRA是一个统一的、几何驱动的可辨识度评估框架。它具有编码器特定、准确、可解释且可跨模态扩展的优点，在准确性、可解释性和应用范围方面显著推进了FIQA领域的发展。

Abstract: Face recognition in unconstrained environments such as surveillance, video,
and web imagery must contend with extreme variation in pose, blur,
illumination, and occlusion, where conventional visual quality metrics fail to
predict whether inputs are truly recognizable to the deployed encoder. Existing
FIQA methods typically rely on visual heuristics, curated annotations, or
computationally intensive generative pipelines, leaving their predictions
detached from the encoder's decision geometry. We introduce TransFIRA (Transfer
Learning for Face Image Recognizability Assessment), a lightweight and
annotation-free framework that grounds recognizability directly in embedding
space. TransFIRA delivers three advances: (i) a definition of recognizability
via class-center similarity (CCS) and class-center angular separation (CCAS),
yielding the first natural, decision-boundary--aligned criterion for filtering
and weighting; (ii) a recognizability-informed aggregation strategy that
achieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly
doubling correlation with true recognizability, all without external labels,
heuristics, or backbone-specific training; and (iii) new extensions beyond
faces, including encoder-grounded explainability that reveals how degradations
and subject-specific factors affect recognizability, and the first
recognizability-aware body recognition assessment. Experiments confirm
state-of-the-art results on faces, strong performance on body recognition, and
robustness under cross-dataset shifts. Together, these contributions establish
TransFIRA as a unified, geometry-driven framework for recognizability
assessment -- encoder-specific, accurate, interpretable, and extensible across
modalities -- significantly advancing FIQA in accuracy, explainability, and
scope.

</details>


### [19] [Road Surface Condition Detection with Machine Learning using New York State Department of Transportation Camera Images and Weather Forecast Data](https://arxiv.org/abs/2510.06440)
*Carly Sutter,Kara J. Sulia,Nick P. Bassill,Christopher D. Wirz,Christopher D. Thorncroft,Jay C. Rothenberger,Vanessa Przybylo,Mariana G. Cains,Jacob Radford,David Aaron Evans*

Main category: cs.CV

TL;DR: 利用卷积神经网络和随机森林模型对约22000张摄像头图像和天气数据进行训练，以预测路面状况，模型在未见过的数据上达到了81.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 纽约州交通部（NYSDOT）目前依赖人工驾驶和观察摄像头来评估路况，此过程耗时耗力，但对冬季天气运营决策至关重要。机器学习模型可以自动化地对全州范围内的路况进行分类，为NYSDOT提供支持。

Method: 研究人员训练了卷积神经网络和随机森林模型，使用了包含约22000张手动标记的摄像头图像的数据集，并将路面状况分为六类：大雪、雪、湿滑、干燥、视线不佳或被遮挡。

Result: 所提出的路面状况模型在完全未见过的数据集上达到了81.5%的准确率，满足了NYSDOT决策者的运营需求。

Conclusion: 本研究成功开发了一个能够准确预测路面状况的机器学习模型，为NYSDOT在冬季天气管理方面提供了有价值的自动化工具。

Abstract: The New York State Department of Transportation (NYSDOT) has a network of
roadside traffic cameras that are used by both the NYSDOT and the public to
observe road conditions. The NYSDOT evaluates road conditions by driving on
roads and observing live cameras, tasks which are labor-intensive but necessary
for making critical operational decisions during winter weather events.
However, machine learning models can provide additional support for the NYSDOT
by automatically classifying current road conditions across the state. In this
study, convolutional neural networks and random forests are trained on camera
images and weather data to predict road surface conditions. Models are trained
on a hand-labeled dataset of ~22,000 camera images, each classified by human
labelers into one of six road surface conditions: severe snow, snow, wet, dry,
poor visibility, or obstructed. Model generalizability is prioritized to meet
the operational needs of the NYSDOT decision makers, and the weather-related
road surface condition model in this study achieves an accuracy of 81.5% on
completely unseen cameras.

</details>


### [20] [TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion](https://arxiv.org/abs/2510.06460)
*Piyush Dashpute,Niki Nezakati,Wolfgang Heidrich,Vishwanath Saragadam*

Main category: cs.CV

TL;DR: TDiff是一个创新的基于块的扩散框架，通过在小尺寸热成像图块上进行训练来解决低分辨率、固定模式噪声和数据集有限等问题。该方法通过对重叠图块进行去噪和使用平滑空间窗口进行融合，实现了全分辨率图像的恢复，并在去噪、超分辨率和去模糊任务上取得了优异成果。


<details>
  <summary>Details</summary>
Motivation: 低成本热成像相机拍摄的图像存在分辨率低、固定模式噪声和局部退化等问题，同时可用的热成像数据集在规模和多样性方面也存在不足。

Method: 提出了一种基于块的扩散框架（TDiff），利用这些失真局部化的特点，在小尺寸热成像图块上进行训练。通过对重叠的图块进行去噪，并使用平滑的空间窗口进行融合，来实现全分辨率图像的恢复。

Result: 在模拟和真实热成像数据上，TDiff在去噪、超分辨率和去模糊任务上都取得了优异的实验结果。

Conclusion: TDiff是首个用于热成像复原的多任务学习性先验的基于块的扩散框架，为热成像复原提供了一个统一的解决方案。

Abstract: Thermal images from low-cost cameras often suffer from low resolution, fixed
pattern noise, and other localized degradations. Available datasets for thermal
imaging are also limited in both size and diversity. To address these
challenges, we propose a patch-based diffusion framework (TDiff) that leverages
the local nature of these distortions by training on small thermal patches. In
this approach, full-resolution images are restored by denoising overlapping
patches and blending them using smooth spatial windowing. To our knowledge,
this is the first patch-based diffusion framework that models a learned prior
for thermal image restoration across multiple tasks. Experiments on denoising,
super-resolution, and deblurring demonstrate strong results on both simulated
and real thermal data, establishing our method as a unified restoration
pipeline.

</details>


### [21] [SIGMA-GEN: Structure and Identity Guided Multi-subject Assembly for Image Generation](https://arxiv.org/abs/2510.06469)
*Oindrila Saha,Vojtech Krs,Radomir Mech,Subhransu Maji,Kevin Blackburn-Matzen,Matheus Gadelha*

Main category: cs.CV

TL;DR: SIGMA-GEN是一个统一的框架，可以生成保留多个身份的图像，并支持各种精度的用户引导。


<details>
  <summary>Details</summary>
Motivation: 提出一个能够保留多个身份、并支持多种用户引导方式的统一框架，以解决现有方法的局限性。

Method: 使用SIGMA-SET27K数据集，该数据集包含超过10万个独特的主体，提供身份、结构和空间信息，并开发了一个能够支持从粗略的2D/3D框到像素级分割和深度的用户引导的单模型。

Result: SIGMA-GEN在身份保留、图像生成质量和速度方面达到了最先进的性能。

Conclusion: SIGMA-GEN是一个有效的、统一的框架，能够生成保留多个身份的图像，并支持不同精度的用户引导。

Abstract: We present SIGMA-GEN, a unified framework for multi-identity preserving image
generation. Unlike prior approaches, SIGMA-GEN is the first to enable
single-pass multi-subject identity-preserved generation guided by both
structural and spatial constraints. A key strength of our method is its ability
to support user guidance at various levels of precision -- from coarse 2D or 3D
boxes to pixel-level segmentations and depth -- with a single model. To enable
this, we introduce SIGMA-SET27K, a novel synthetic dataset that provides
identity, structure, and spatial information for over 100k unique subjects
across 27k images. Through extensive evaluation we demonstrate that SIGMA-GEN
achieves state-of-the-art performance in identity preservation, image
generation quality, and speed. Code and visualizations at
https://oindrilasaha.github.io/SIGMA-Gen/

</details>


### [22] [Superpixel Integrated Grids for Fast Image Segmentation](https://arxiv.org/abs/2510.06487)
*Jack Roberts,Jeova Farias Sales Rocha Neto*

Main category: cs.CV

TL;DR: SIGRID是一种新的超像素数据结构，通过编码颜色和形状信息来降低图像分割的输入维度，同时保持甚至提高分割性能，并显著加快模型训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在处理超像素时，由于其不规则的空间分布，往往需要特殊的训练算法和架构，这违背了超像素化最初的计算效率优势。

Method: 提出了一种名为SIGRID（Superpixel-Integrated Grid）的新型超像素数据结构，它利用经典的形状描述子来编码超像素的颜色和形状信息，从而降低输入维度，并将其应用于图像分割任务，替代全分辨率图像作为输入。

Result: 在四个基准数据集和两种流行的卷积分割架构上评估SIGRID。结果表明，SIGRID在压缩数据的情况下，分割性能与像素级表示相当甚至更优，同时模型训练速度显著加快。

Conclusion: SIGRID在分割任务中实现了准确性和计算效率之间的良好平衡，克服了传统超像素表示在深度学习应用中的局限性。

Abstract: Superpixels have long been used in image simplification to enable more
efficient data processing and storage. However, despite their computational
potential, their irregular spatial distribution has often forced deep learning
approaches to rely on specialized training algorithms and architectures,
undermining the original motivation for superpixelations. In this work, we
introduce a new superpixel-based data structure, SIGRID (Superpixel-Integrated
Grid), as an alternative to full-resolution images in segmentation tasks. By
leveraging classical shape descriptors, SIGRID encodes both color and shape
information of superpixels while substantially reducing input dimensionality.
We evaluate SIGRIDs on four benchmark datasets using two popular convolutional
segmentation architectures. Our results show that, despite compressing the
original data, SIGRIDs not only match but in some cases surpass the performance
of pixel-level representations, all while significantly accelerating model
training. This demonstrates that SIGRIDs achieve a favorable balance between
accuracy and computational efficiency.

</details>


### [23] [Text2Interact: High-Fidelity and Diverse Text-to-Two-Person Interaction Generation](https://arxiv.org/abs/2510.06504)
*Qingxuan Wu,Zhiyang Dou,Chuan Guo,Yiming Huang,Qiao Feng,Bing Zhou,Jian Wang,Lingjie Liu*

Main category: cs.CV

TL;DR: Text2Interact框架通过合成数据和改进的文本到交互模型来解决现有模型在捕捉复杂两人交互方面的局限性，从而生成逼真且与文本匹配的人类交互。


<details>
  <summary>Details</summary>
Motivation: 现有模型在模拟人类交互时面临两大挑战：1) 缺乏足够多的双人交互训练数据来捕捉交互的复杂性；2) 文本到交互的建模不够精细，导致语言条件化将结构化的提示压缩成单一的句子嵌入。

Method: 提出了Text2Interact框架，包含InterCompose（通过语言模型生成交互描述并结合单人动作先验来合成交互数据）和InterActor（一个具有词级别条件化的文本到交互模型，能够保留令牌级别的线索并使用自适应交互损失来改善交互耦合和物理合理性）。

Result: 实验表明，Text2Interact在运动多样性、保真度和泛化能力（包括在分布外场景）方面均有显著提升，并通过了用户研究的验证。

Conclusion: Text2Interact框架通过其创新的数据合成和文本到交互建模方法，有效提升了生成逼真、与文本对齐的人类交互的能力，并在各项评估中取得了优异的性能。

Abstract: Modeling human-human interactions from text remains challenging because it
requires not only realistic individual dynamics but also precise,
text-consistent spatiotemporal coupling between agents. Currently, progress is
hindered by 1) limited two-person training data, inadequate to capture the
diverse intricacies of two-person interactions; and 2) insufficiently
fine-grained text-to-interaction modeling, where language conditioning
collapses rich, structured prompts into a single sentence embedding. To address
these limitations, we propose our Text2Interact framework, designed to generate
realistic, text-aligned human-human interactions through a scalable
high-fidelity interaction data synthesizer and an effective spatiotemporal
coordination pipeline. First, we present InterCompose, a scalable
synthesis-by-composition pipeline that aligns LLM-generated interaction
descriptions with strong single-person motion priors. Given a prompt and a
motion for an agent, InterCompose retrieves candidate single-person motions,
trains a conditional reaction generator for another agent, and uses a neural
motion evaluator to filter weak or misaligned samples-expanding interaction
coverage without extra capture. Second, we propose InterActor, a
text-to-interaction model with word-level conditioning that preserves
token-level cues (initiation, response, contact ordering) and an adaptive
interaction loss that emphasizes contextually relevant inter-person joint
pairs, improving coupling and physical plausibility for fine-grained
interaction modeling. Extensive experiments show consistent gains in motion
diversity, fidelity, and generalization, including out-of-distribution
scenarios and user studies. We will release code and models to facilitate
reproducibility.

</details>


### [24] [From Captions to Keyframes: Efficient Video Summarization via Caption- and Context-Aware Frame Scoring](https://arxiv.org/abs/2510.06509)
*Shih-Yao Lin,Sibendu Paul,Caren Chen*

Main category: cs.CV

TL;DR: KeyScore是一个多模态视频帧评分框架，通过结合语义相似性、时间多样性和上下文丢失影响来评估帧的重要性，并与STACFP（时空自适应聚类帧建议）结合使用，可以显著减少视频理解中的帧数，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 为了实现高效的视频-语言理解，需要从长视频中选取保留了语义和上下文信息的少量帧。

Method: KeyScore通过结合语义相似性、时间多样性和上下文丢失影响来评估帧的级别重要性。STACFP用于为长视频生成紧凑且多样化的帧候选。

Result: 与完整的帧推理相比，该框架实现了高达99%的帧数减少，并且在MSRVTT、MSVD和DiDeMo数据集上显著优于标准的8帧编码器。

Conclusion: 该方法证明了视觉和文本信号之间的多模态对齐能够实现可扩展、高效且以字幕为基础的视频理解，而无需显式的视频摘要。

Abstract: Efficient video-language understanding requires selecting a small set of
frames that retain semantic and contextual information from long videos. We
propose KeyScore, a multimodal frame scoring framework that jointly leverages
captions and visual context to estimate frame-level importance. By combining
semantic similarity, temporal diversity, and contextual drop impact, KeyScore
identifies the most informative frames for downstream tasks such as retrieval,
captioning, and video-language reasoning. To complement KeyScore, we introduce
STACFP (Spatio-Temporal Adaptive Clustering for Frame Proposals), which
generates compact and diverse frame candidates for long-form videos. Together,
these modules achieve up to 99\% frame reduction compared to full-frame
inference and substantially outperform standard 8-frame encoders on MSRVTT,
MSVD, and DiDeMo. Our results demonstrate that emphasizing multimodal alignment
between visual and textual signals enables scalable, efficient, and
caption-grounded video understanding -- without explicit video summarization.

</details>


### [25] [LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval](https://arxiv.org/abs/2510.06512)
*Avishree Khare,Hideki Okamoto,Bardh Hoxha,Georgios Fainekos,Rajeev Alur*

Main category: cs.CV

TL;DR: 本研究提出了LogSTOP，一种用于处理视频和音频序列中时间属性评分的有效方法，并在查询匹配和排序检索任务上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 将单个帧的局部属性检测分数（如物体或情绪）扩展到视频或音频剪辑的整个时间序列，以支持诸如查询匹配和排序检索之类的下游应用。

Method: 提出了一种名为LogSTOP的评分函数，它可以有效地为线性时序逻辑表示的时间属性计算分数，并结合了潜在有噪声的局部属性评分预测器。

Result: 在视频对象检测和语音情绪识别的查询匹配任务上，LogSTOP比大型视觉/音频语言模型和其他基于时间逻辑的方法的性能提高了至少16%。在视频对象和动作的排序检索任务上，LogSTOP比零样本文本到视频检索基线分别提高了至少19%和16%的平均精度和召回率。

Conclusion: LogSTOP在处理和评分时间属性方面是一种有效且可扩展的方法，能够显着提高各种基于时间属性的下游任务的性能。

Abstract: Neural models such as YOLO and HuBERT can be used to detect local properties
such as objects ("car") and emotions ("angry") in individual frames of videos
and audio clips respectively. The likelihood of these detections is indicated
by scores in [0, 1]. Lifting these scores to temporal properties over sequences
can be useful for several downstream applications such as query matching (e.g.,
"does the speaker eventually sound happy in this audio clip?"), and ranked
retrieval (e.g., "retrieve top 5 videos with a 10 second scene where a car is
detected until a pedestrian is detected"). In this work, we formalize this
problem of assigning Scores for TempOral Properties (STOPs) over sequences,
given potentially noisy score predictors for local properties. We then propose
a scoring function called LogSTOP that can efficiently compute these scores for
temporal properties represented in Linear Temporal Logic. Empirically, LogSTOP,
with YOLO and HuBERT, outperforms Large Vision / Audio Language Models and
other Temporal Logic-based baselines by at least 16% on query matching with
temporal properties over objects-in-videos and emotions-in-speech respectively.
Similarly, on ranked retrieval with temporal properties over objects and
actions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a
19% and 16% increase in mean average precision and recall over zero-shot
text-to-video retrieval baselines respectively.

</details>


### [26] [Limited-Angle Tomography Reconstruction via Projector Guided 3D Diffusion](https://arxiv.org/abs/2510.06516)
*Zhantao Deng,Mériem Er-Rafik,Anna Sushko,Cécile Hébert,Pascal Fua*

Main category: cs.CV

TL;DR: TEMDiff是一个新颖的基于3D扩散的迭代重建框架，用于解决有限角度电子断层扫描中的缺失扇形块问题，它在无需真实TEM数据的情况下，仅使用模拟数据即可实现高质量重建。


<details>
  <summary>Details</summary>
Motivation: 有限角度电子断层扫描（TEM）由于缺失扇形块问题，在从有限的倾斜角度的2D投影重建3D形状时存在严重的伪影。现有的深度学习方法需要难以获取的高质量训练数据集。本研究旨在解决这些挑战。

Method: 提出了一种名为TEMDiff的新型3D扩散模型，该模型直接在3D体积上操作。它利用模拟的FIB-SEM数据进行训练，并将模拟数据映射到TEM倾斜系列，从而在没有真实TEM真实情况的情况下学习真实结构先验。该方法通过直接作用于3D体积来隐式强制执行切片之间的一致性。

Result: 在具有有限角度覆盖的模拟电子断层扫描数据上，TEMDiff的重建质量优于现有最先进的方法。此外，在未进行任何重新训练或微调的情况下，TEMDiff模型能够很好地推广到真实世界的TEM倾斜数据，并能从低至8度的倾斜范围（2度增量）中恢复精确的结构。

Conclusion: TEMDiff通过使用模拟数据进行训练，解决了有限角度电子断层扫描中对真实TEM数据真实情况的需求问题，并在重建质量和泛化能力方面取得了显著成果，即使在非常有限的倾斜角度下也能实现准确的结构恢复。

Abstract: Limited-angle electron tomography aims to reconstruct 3D shapes from 2D
projections of Transmission Electron Microscopy (TEM) within a restricted range
and number of tilting angles, but it suffers from the missing-wedge problem
that causes severe reconstruction artifacts. Deep learning approaches have
shown promising results in alleviating these artifacts, yet they typically
require large high-quality training datasets with known 3D ground truth which
are difficult to obtain in electron microscopy. To address these challenges, we
propose TEMDiff, a novel 3D diffusion-based iterative reconstruction framework.
Our method is trained on readily available volumetric FIB-SEM data using a
simulator that maps them to TEM tilt series, enabling the model to learn
realistic structural priors without requiring clean TEM ground truth. By
operating directly on 3D volumes, TEMDiff implicitly enforces consistency
across slices without the need for additional regularization. On simulated
electron tomography datasets with limited angular coverage, TEMDiff outperforms
state-of-the-art methods in reconstruction quality. We further demonstrate that
a trained TEMDiff model generalizes well to real-world TEM tilts obtained under
different conditions and can recover accurate structures from tilt ranges as
narrow as 8 degrees, with 2-degree increments, without any retraining or
fine-tuning.

</details>


### [27] [VUGEN: Visual Understanding priors for GENeration](https://arxiv.org/abs/2510.06529)
*Xiangyi Chen,Théophane Vallaeys,Maha Elbayad,John Nguyen,Jakob Verbeek*

Main category: cs.CV

TL;DR: VUGEN是一个利用视觉语言模型(VLM)的预训练视觉理解先验来实现高效高质量图像生成的新框架。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型(VLM)在图像生成方面存在挑战，容易出现理解和生成表示不一致或架构复杂的问题。

Method: VUGEN将VLM的视觉编码器的潜在空间转换为较低维度的可处理分布，以保留视觉信息。然后，VLM被训练来采样这个缩减的潜在空间，并使用一个像素解码器将潜在向量映射回图像空间。该框架不依赖于变分自编码器(VAE)，而是使用像素扩散解码器。

Result: VUGEN在COCO数据集上，DPG Bench得分从71.17提高到74.32，FID得分从11.86降低到9.06，图像生成性能优于现有方法，同时保留了VLM的原始理解能力。

Conclusion: VUGEN通过显式利用VLM的视觉理解先验，在不增加架构复杂性的情况下，实现了高质量且与理解能力一致的图像生成。

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled unified
understanding across text and images, yet equipping these models with robust
image generation capabilities remains challenging. Existing approaches often
rely on reconstruction-oriented autoencoders or complex bridging mechanisms,
leading to misalignment between understanding and generation representations,
or architectural complexity. In this work, we propose VUGEN, a novel framework
that explicitly leverages VLM's pretrained visual understanding priors for
efficient and high-quality image generation. Our approach first transforms the
high-dimensional latent space of the VLM's native vision encoder into a
lower-dimensional, tractable distribution that maximally preserves visual
information. The VLM is then trained to sample within this reduced latent
space, ensuring alignment with its visual understanding capabilities. Finally,
a dedicated pixel decoder maps these generated latents back to the image space.
We find that a VAE-free pixel diffusion decoder to be on par or better than
commonly used complex latent diffusion decoders that internally rely on VAE
latents. Extensive experiments demonstrate that VUGEN achieves superior image
generation performance, improving DPG Bench from 71.17 to 74.32 and FID from
11.86 to 9.06 on COCO, while fully preserving the VLM's original understanding
capabilities.

</details>


### [28] [Cluster Paths: Navigating Interpretability in Neural Networks](https://arxiv.org/abs/2510.06541)
*Nicholas M. Kroeger,Vincent Bindschaedler*

Main category: cs.CV

TL;DR: 深度神经网络在视觉任务中表现出色，但其决策过程不透明。本研究提出了一种名为“聚类路径”的后验可解释性方法，通过聚类所选层的激活值，并将每个输入表示为其聚类ID序列。引入了四种评估指标：路径复杂度、加权路径纯度、决策对齐保真度和路径一致性。在虚假线索CIFAR-10实验中，“聚类路径”识别出了基于颜色的捷径，并在线索移除时失效。在CelebA人脸发色任务中，“聚类路径”实现了90%的保真度和96%的噪声稳定性，且不影响准确性。该方法还扩展到Transformer模型，并能作为有效的OOD检测器，能够识别异常样本。该方法能够揭示多网络深度的视觉概念（如调色板、纹理或物体上下文），表明其能够扩展到大型视觉模型，并生成简洁易懂的解释。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在视觉任务中表现优异，但其决策过程不透明，存在误信、偏见和意外故障的风险。因此，需要一种方法来解释这些模型的决策过程。

Method: 提出了一种名为“聚类路径”的后验可解释性方法。该方法通过聚类所选层的激活值，并将每个输入表示为其聚类ID序列。引入了四种评估指标：路径复杂度（认知负荷）、加权路径纯度（类别对齐）、决策对齐保真度（预测保真度）和路径一致性（扰动稳定性）。

Result: 在虚假线索CIFAR-10实验中，“聚类路径”识别出基于颜色的捷径，并在移除线索时失效。在CelebA人脸发色任务中，实现了90%的保真度和96%的噪声稳定性，且不影响准确性。将该方法扩展到Vision Transformer模型，并生成了概念路径。该方法还能有效检测OOD样本，并能揭示多网络深度的视觉概念。

Conclusion: “聚类路径”是一种可扩展到大型视觉模型，并能生成简洁易懂解释的有效可解释性方法。它不仅能识别模型的决策过程，还能作为OOD检测器，揭示模型内部的视觉概念。

Abstract: While modern deep neural networks achieve impressive performance in vision
tasks, they remain opaque in their decision processes, risking unwarranted
trust, undetected biases and unexpected failures. We propose cluster paths, a
post-hoc interpretability method that clusters activations at selected layers
and represents each input as its sequence of cluster IDs. To assess these
cluster paths, we introduce four metrics: path complexity (cognitive load),
weighted-path purity (class alignment), decision-alignment faithfulness
(predictive fidelity), and path agreement (stability under perturbations). In a
spurious-cue CIFAR-10 experiment, cluster paths identify color-based shortcuts
and collapse when the cue is removed. On a five-class CelebA hair-color task,
they achieve 90% faithfulness and maintain 96% agreement under Gaussian noise
without sacrificing accuracy. Scaling to a Vision Transformer pretrained on
ImageNet, we extend cluster paths to concept paths derived from prompting a
large language model on minimal path divergences. Finally, we show that cluster
paths can serve as an effective out-of-distribution (OOD) detector, reliably
flagging anomalous samples before the model generates over-confident
predictions. Cluster paths uncover visual concepts, such as color palettes,
textures, or object contexts, at multiple network depths, demonstrating that
cluster paths scale to large vision models while generating concise and
human-readable explanations.

</details>


### [29] [HSNet: Heterogeneous Subgraph Network for Single Image Super-resolution](https://arxiv.org/abs/2510.06564)
*Qiongyang Hu,Wenyang Liu,Wenbin Zou,Yuejiao Su,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: HSNet通过构建异构子图集并进行聚合，解决了现有超分辨率方法结构不灵活和计算量大的问题，达到了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CNN和注意力机制的深度学习超分辨率方法存在结构不灵活的问题，而基于图的方法虽然表示能力强但计算量大。本研究旨在克服这些局限性。

Method: 提出了一种名为异构子图网络（HSNet）的新框架。该框架包含构造子图集块（CSSB）用于生成多样化的互补子图，子图聚合块（SAB）用于整合多图特征，以及节点采样策略（NSS）用于选择显著特征以降低计算开销。

Result: HSNet在大量实验中取得了最先进的性能，有效地平衡了重建质量和计算效率。

Conclusion: HSNet通过其创新的子图构建和聚合机制，成功地提高了图像超分辨率的性能，同时保持了计算效率。

Abstract: Existing deep learning approaches for image super-resolution, particularly
those based on CNNs and attention mechanisms, often suffer from structural
inflexibility. Although graph-based methods offer greater representational
adaptability, they are frequently impeded by excessive computational
complexity. To overcome these limitations, this paper proposes the
Heterogeneous Subgraph Network (HSNet), a novel framework that efficiently
leverages graph modeling while maintaining computational feasibility. The core
idea of HSNet is to decompose the global graph into manageable sub-components.
First, we introduce the Constructive Subgraph Set Block (CSSB), which generates
a diverse set of complementary subgraphs. Rather than relying on a single
monolithic graph, CSSB captures heterogeneous characteristics of the image by
modeling different relational patterns and feature interactions, producing a
rich ensemble of both local and global graph structures. Subsequently, the
Subgraph Aggregation Block (SAB) integrates the representations embedded across
these subgraphs. Through adaptive weighting and fusion of multi-graph features,
SAB constructs a comprehensive and discriminative representation that captures
intricate interdependencies. Furthermore, a Node Sampling Strategy (NSS) is
designed to selectively retain the most salient features, thereby enhancing
accuracy while reducing computational overhead. Extensive experiments
demonstrate that HSNet achieves state-of-the-art performance, effectively
balancing reconstruction quality with computational efficiency. The code will
be made publicly available.

</details>


### [30] [Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation](https://arxiv.org/abs/2510.06582)
*Fei Zhang,Rob Chancia,Josie Clapp,Amirhossein Hassanzadeh,Dimah Dera,Richard MacKenzie,Jan van Aardt*

Main category: cs.CV

TL;DR: 提出了一种半自动、不确定性感知的点云语义分割管线，通过球形投影、特征丰富、集成学习和有针对性的标注，在减少标注工作量的同时保持高精度，并构建了Mangrove3D数据集，对数据效率和特征重要性进行了评估。


<details>
  <summary>Details</summary>
Motivation: 准确的地面激光扫描（TLS）点云语义分割受到昂贵的人工标注的限制，需要一种能够减少标注工作量同时保持高精度的解决方案。

Method: 将3D点投影到2D球形网格，用多源特征丰富像素，训练分割网络集成模型来生成伪标签和不确定性图，并利用不确定性图指导模糊区域的标注，最后将2D输出反投影到3D。

Result: 在Mangrove3D数据集上，该方法实现了约0.76的平均交并比（mIoU），并发现约12个标注扫描后性能饱和，几何特征贡献最大，紧凑的九通道特征组合能捕获几乎所有区分能力。该方法在ForestSemantic和Semantic3D数据集上进行了交叉验证，证明了其泛化能力。

Conclusion: 提出了一种鲁棒的、不确定性感知的TLS标注管线，并提供了可视化工具，构建了Mangrove3D数据集，并对数据效率和特征重要性进行了实证指导，能够实现TLS点云的可扩展、高质量分割，用于生态监测等领域。

Abstract: Accurate semantic segmentation of terrestrial laser scanning (TLS) point
clouds is limited by costly manual annotation. We propose a semi-automated,
uncertainty-aware pipeline that integrates spherical projection, feature
enrichment, ensemble learning, and targeted annotation to reduce labeling
effort, while sustaining high accuracy. Our approach projects 3D points to a 2D
spherical grid, enriches pixels with multi-source features, and trains an
ensemble of segmentation networks to produce pseudo-labels and uncertainty
maps, the latter guiding annotation of ambiguous regions. The 2D outputs are
back-projected to 3D, yielding densely annotated point clouds supported by a
three-tier visualization suite (2D feature maps, 3D colorized point clouds, and
compact virtual spheres) for rapid triage and reviewer guidance. Using this
pipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove
forests. We further evaluate data efficiency and feature importance to address
two key questions: (1) how much annotated data are needed and (2) which
features matter most. Results show that performance saturates after ~12
annotated scans, geometric features contribute the most, and compact
nine-channel stacks capture nearly all discriminative power, with the mean
Intersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm
the generalization of our feature-enrichment strategy through cross-dataset
tests on ForestSemantic and Semantic3D.
  Our contributions include: (i) a robust, uncertainty-aware TLS annotation
pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii)
empirical guidance on data efficiency and feature importance, thus enabling
scalable, high-quality segmentation of TLS point clouds for ecological
monitoring and beyond. The dataset and processing scripts are publicly
available at https://fz-rit.github.io/through-the-lidars-eye/.

</details>


### [31] [Improving Artifact Robustness for CT Deep Learning Models Without Labeled Artifact Images via Domain Adaptation](https://arxiv.org/abs/2510.06584)
*Justin Cheung,Samuel Savine,Calvin Nguyen,Lin Lu,Alhassan S. Yasin*

Main category: cs.CV

TL;DR: 本研究评估了领域自适应（DANN）在处理CT图像新伪影方面的能力，即使在没有对应标签的情况下，也能在不要求昂贵专家标注的情况下，在不同伪影分布下保持分类性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在新的图像分布（如CT扫描中的新伪影）上表现会显著下降，而对新数据进行标注成本高昂。因此，需要一种无需标注新数据即可提升模型在新分布上鲁棒性的方法。

Method: 本研究在OrganAMNIST腹部CT数据集上，通过在投影图空间模拟探测器增益误差产生的环形伪影，评估了领域对抗神经网络（DANN）与基线模型和数据增强方法的性能对比。

Result: 基线模型在干净图像上训练后，无法泛化到带环形伪影的图像。传统的数据增强方法对未见过的伪影类型无效。而DANN在仅使用无标签的伪影数据进行训练的情况下，成功地在环形伪影图像上保持了高分类准确率，并表现出对均匀噪声的泛化能力，其性能与使用有标签伪影数据显式训练的模型相当。

Conclusion: 领域自适应（DANN）是一种有效的方法，可以在不依赖昂贵专家标注的情况下，解决医学影像中的分布偏移问题，提高模型对新型伪影的鲁棒性，有望在临床环境中应用。

Abstract: Deep learning models which perform well on images from their training
distribution can degrade substantially when applied to new distributions. If a
CT scanner introduces a new artifact not present in the training labels, the
model may misclassify the images. Although modern CT scanners include design
features which mitigate these artifacts, unanticipated or difficult-to-mitigate
artifacts can still appear in practice. The direct solution of labeling images
from this new distribution can be costly. As a more accessible alternative,
this study evaluates domain adaptation as an approach for training models that
maintain classification performance despite new artifacts, even without
corresponding labels. We simulate ring artifacts from detector gain error in
sinogram space and evaluate domain adversarial neural networks (DANN) against
baseline and augmentation-based approaches on the OrganAMNIST abdominal CT
dataset. Our results demonstrate that baseline models trained only on clean
images fail to generalize to images with ring artifacts, and traditional
augmentation with other distortion types provides no improvement on unseen
artifact domains. In contrast, the DANN approach successfully maintains high
classification accuracy on ring artifact images using only unlabeled artifact
data during training, demonstrating the viability of domain adaptation for
artifact robustness. The domain-adapted model achieved classification
performance on ring artifact test data comparable to models explicitly trained
with labeled artifact images, while also showing unexpected generalization to
uniform noise. These findings provide empirical evidence that domain adaptation
can effectively address distribution shift in medical imaging without requiring
expensive expert labeling of new artifact distributions, suggesting promise for
deployment in clinical settings where novel artifacts may emerge.

</details>


### [32] [HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation](https://arxiv.org/abs/2510.06876)
*Samir Abou Haidar,Alexandre Chariot,Mehdi Darouich,Cyril Joly,Jean-Emmanuel Deschaud*

Main category: cs.CV

TL;DR: HARP-NeXt是一种用于激光雷达语义分割的高速、高精度网络，通过创新的预处理方法、高效的特征提取块和多尺度融合骨干网，在不依赖TTA或集成模型的情况下，实现了优于现有方法的速-准平衡。


<details>
  <summary>Details</summary>
Motivation: 为了在资源受限的嵌入式系统上实现高精度和实时性的激光雷达语义分割，以克服现有方法在速度和精度上的权衡，以及预处理和测试时增强（TTA）带来的额外开销。

Method: 提出了一种新颖的预处理方法以降低计算开销，设计了Conv-SE-NeXt特征提取块以高效捕获表示，并采用了多尺度范围-点融合骨干网以保留几何细节。

Result: 在nuScenes和SemanticKITTI基准测试中，HARP-NeXt实现了优于现有方法的速度-精度权衡，并且在不依赖集成模型或TTA的情况下，速度比顶尖的PTv3快24倍，性能与之相当。

Conclusion: HARP-NeXt在激光雷达语义分割领域提供了一种高效且准确的解决方案，显著优于现有方法，特别是在嵌入式系统上。

Abstract: LiDAR semantic segmentation is crucial for autonomous vehicles and mobile
robots, requiring high accuracy and real-time processing, especially on
resource-constrained embedded systems. Previous state-of-the-art methods often
face a trade-off between accuracy and speed. Point-based and sparse
convolution-based methods are accurate but slow due to the complexity of
neighbor searching and 3D convolutions. Projection-based methods are faster but
lose critical geometric information during the 2D projection. Additionally,
many recent methods rely on test-time augmentation (TTA) to improve
performance, which further slows the inference. Moreover, the pre-processing
phase across all methods increases execution time and is demanding on embedded
platforms. Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR
semantic segmentation network. We first propose a novel pre-processing
methodology that significantly reduces computational overhead. Then, we design
the Conv-SE-NeXt feature extraction block to efficiently capture
representations without deep layer stacking per network stage. We also employ a
multi-scale range-point fusion backbone that leverages information at multiple
abstraction levels to preserve essential geometric details, thereby enhancing
accuracy. Experiments on the nuScenes and SemanticKITTI benchmarks show that
HARP-NeXt achieves a superior speed-accuracy trade-off compared to all
state-of-the-art methods, and, without relying on ensemble models or TTA, is
comparable to the top-ranked PTv3, while running 24$\times$ faster. The code is
available at https://github.com/SamirAbouHaidar/HARP-NeXt

</details>


### [33] [Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer](https://arxiv.org/abs/2510.06590)
*Ziyuan Huang,DanDan Zheng,Cheng Zou,Rui Liu,Xiaolong Wang,Kaixiang Ji,Weilong Chai,Jianxin Sun,Libin Wang,Yongjie Lv,Taozhi Huang,Jiajia Liu,Qingpei Guo,Ming Yang,Jingdong Chen,Jun Zhou*

Main category: cs.CV

TL;DR: MingTok是一种新的连续视觉标记器，用于统一的视觉语言理解和生成。它通过三阶段架构解决离散标记器的局限性，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉标记器在离散潜在空间中量化误差限制语义表达能力的问题，以统一视觉理解和生成任务。

Method: 提出了一种名为MingTok的新型视觉标记器，采用三阶段顺序架构（低级编码、语义扩展、视觉重建），以在连续潜在空间中平衡理解任务（需要判别性高维特征）和生成任务（需要紧凑的低级代码）的需求。在此基础上，Ming-UniVision消除了特定任务的视觉表示，并在单一自回归预测范式下统一了各种视觉语言任务。

Result: Ming-UniVision通过在共享连续空间中将理解和生成都视为下一个标记的预测，实现了跨多个回合和上下文的任务（如迭代理解、生成和编辑），并在理解和生成领域都达到了最先进的性能。

Conclusion: 统一的连续视觉表示可以协调理解和生成任务对标记器的不同需求，从而在两个领域都取得最先进的性能。研究结果有望促进连续域中的统一视觉标记。

Abstract: Visual tokenization remains a core challenge in unifying visual understanding
and generation within the autoregressive paradigm. Existing methods typically
employ tokenizers in discrete latent spaces to align with the tokens from large
language models, where the quantization errors can limit semantic
expressiveness and degrade the capability of vision-language understanding. To
address this, we introduce MingTok, a new family of visual tokenizers with a
continuous latent space, for unified autoregressive generation and
understanding. While understanding tasks favor discriminative high-dimensional
features, generation tasks prefer compact low-level codes. Thus, to reconcile
these competing demands, MingTok adopts a three-stage sequential architecture
involving low-level encoding, semantic expansion, and visual reconstruction.
Built on top of it, Ming-UniVision eliminates the need for task-specific visual
representations, and unifies diverse vision-language tasks under a single
autoregrsssive prediction paradigm. By formulating both understanding and
generation as next-token prediction in a shared continuous space, it seamlessly
supports multi-round, in-context tasks such as iterative understanding,
generation and editing. Empirically, we find that using a unified continuous
visual representation reconciles the competing requirements on the tokenizers
by the understanding and generation tasks, thereby leading to state-of-the-art
level performance across both domains. We hope our findings will facilitate
unified visual tokenization in the continuous domain. Inference code and model
weights are released to benefit community.

</details>


### [34] [Adaptive Stain Normalization for Cross-Domain Medical Histology](https://arxiv.org/abs/2510.06592)
*Tianyue Xu,Yanlin Wu,Abhai K. Tripathi,Matthew M. Ippolito,Benjamin D. Haeffele*

Main category: cs.CV

TL;DR: 通过基于比尔-兰伯特定律的颜色归一化方法，解决数字病理图像中的域偏移问题，提高深度学习模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 数字病理图像的颜色变异性（由染色方案和成像条件引起）会对深度学习模型的性能产生负面影响，即域偏移问题。现有颜色归一化方法存在伪影或需要手动选择模板图像等缺点。

Method: 提出了一种可训练的颜色归一化模型BeerLaNet，该模型基于比尔-兰伯特定律，通过对非负矩阵分解（NMF）模型进行算法展开，提取颜色不变的结构信息，并将其作为输入用于后续处理。该模型可与任何骨干网络集成。

Result: 在公开病理数据集和包含疟疾血涂片的内部数据集上进行了实验评估。结果表明，与多种最先进的颜色归一化方法相比，该模型在跨域目标检测和分类任务上表现更优。

Conclusion: 所提出的BeerLaNet模型能够有效解决数字病理图像的颜色变异性问题，提高深度学习模型在不同成像条件下的性能，并且优于现有方法。

Abstract: Deep learning advances have revolutionized automated digital pathology
analysis. However, differences in staining protocols and imaging conditions can
introduce significant color variability. In deep learning, such color
inconsistency often reduces performance when deploying models on data acquired
under different conditions from the training data, a challenge known as domain
shift. Many existing methods attempt to address this problem via color
normalization but suffer from several notable drawbacks such as introducing
artifacts or requiring careful choice of a template image for stain mapping. To
address these limitations, we propose a trainable color normalization model
that can be integrated with any backbone network for downstream tasks such as
object detection and classification. Based on the physics of the imaging
process per the Beer-Lambert law, our model architecture is derived via
algorithmic unrolling of a nonnegative matrix factorization (NMF) model to
extract stain-invariant structural information from the original pathology
images, which serves as input for further processing. Experimentally, we
evaluate the method on publicly available pathology datasets and an internally
curated collection of malaria blood smears for cross-domain object detection
and classification, where our method outperforms many state-of-the-art stain
normalization methods. Our code is available at
https://github.com/xutianyue/BeerLaNet.

</details>


### [35] [SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation](https://arxiv.org/abs/2510.06596)
*Ayush Zenith,Arnold Zumbrun,Neel Raut,Jing Lin*

Main category: cs.CV

TL;DR: 本论文提出了一种名为SDQM的合成数据集质量度量标准，用于评估目标检测任务中的合成数据质量，无需模型收敛即可进行评估，并证明其与YOLOv11的mAP得分高度相关，优于现有度量标准。


<details>
  <summary>Details</summary>
Motivation: 由于大型、标注良好数据集的稀缺性，模型的性能受到限制。合成数据是一种有前景的解决方案，但缺乏有效的评估方法。

Method: 提出了一种新的评估指标SDQM，用于在模型收敛前评估目标检测任务中合成数据的质量。

Result: 实验证明SDQM与YOLOv11的mAP得分高度相关，相关性优于现有度量标准，并且可以提供改进数据集的指导。

Conclusion: SDQM是一种高效、可扩展的合成数据评估新标准，有助于在资源有限的情况下改进目标检测模型的性能。

Abstract: The performance of machine learning models depends heavily on training data.
The scarcity of large-scale, well-annotated datasets poses significant
challenges in creating robust models. To address this, synthetic data generated
through simulations and generative models has emerged as a promising solution,
enhancing dataset diversity and improving the performance, reliability, and
resilience of models. However, evaluating the quality of this generated data
requires an effective metric. This paper introduces the Synthetic Dataset
Quality Metric (SDQM) to assess data quality for object detection tasks without
requiring model training to converge. This metric enables more efficient
generation and selection of synthetic datasets, addressing a key challenge in
resource-constrained object detection tasks. In our experiments, SDQM
demonstrated a strong correlation with the mean Average Precision (mAP) scores
of YOLOv11, a leading object detection model, while previous metrics only
exhibited moderate or weak correlations. Additionally, it provides actionable
insights for improving dataset quality, minimizing the need for costly
iterative training. This scalable and efficient metric sets a new standard for
evaluating synthetic data. The code for SDQM is available at
https://github.com/ayushzenith/SDQM

</details>


### [36] [AIM 2025 Challenge on Real-World RAW Image Denoising](https://arxiv.org/abs/2510.06601)
*Feiran Li,Jiacheng Li,Marcos V. Conde,Beril Besbinar,Vlad Hosu,Daisuke Iso,Radu Timofte*

Main category: cs.CV

TL;DR: AIM 2025 真实世界 RAW 图像去噪挑战赛旨在推进基于数据合成的高效去噪技术，并引入了一个包含五种不同单反相机拍摄的具有挑战性的低光噪点图像的新评估基准。


<details>
  <summary>Details</summary>
Motivation: 该挑战赛旨在推进高效、有效的去噪技术，特别关注在真实世界、低光照和 RAW 图像条件下的应用，并利用数据合成来解决这些问题。

Method: 参赛者需要开发新的噪声合成流程、网络架构和训练方法，以在不同相机模型上实现高去噪性能。评估指标包括全参考（PSNR、SSIM、LPIPS）和非参考（ARNIQA、TOPIQ）度量。

Result: 挑战赛的重点是开发能够处理不同相机模型、在低光照和 RAW 图像上进行去噪的、使用合成数据训练的模型。

Conclusion: 该挑战赛有望推动在图像恢复和夜间自动驾驶等领域具有实际应用价值的稳健模型的发展，并对数字摄影的进步产生影响。

Abstract: We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to
advance efficient and effective denoising techniques grounded in data
synthesis. The competition is built upon a newly established evaluation
benchmark featuring challenging low-light noisy images captured in the wild
using five different DSLR cameras. Participants are tasked with developing
novel noise synthesis pipelines, network architectures, and training
methodologies to achieve high performance across different camera models.
Winners are determined based on a combination of performance metrics, including
full-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA,
TOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image
denoising trained on synthetic data, the competition promotes the development
of robust and practical models aligned with the rapid progress in digital
photography. We expect the competition outcomes to influence multiple domains,
from image restoration to night-time autonomous driving.

</details>


### [37] [Self-supervised Physics-guided Model with Implicit Representation Regularization for Fast MRI Reconstruction](https://arxiv.org/abs/2510.06611)
*Jingran Xu,Yuanyuan Liu,Yanjie Zhu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为UnrollINR的新型零样本自监督MRI重建框架，无需外部训练数据即可进行特定扫描的MRI重建。


<details>
  <summary>Details</summary>
Motivation: 为了解决磁共振成像(MRI)扫描时间长的问题，本研究旨在开发一种能从欠采样k空间数据中重建高保真MR图像的快速MRI重建技术。

Method: 该方法采用物理引导的展开式迭代重建架构，并引入隐式神经网络表示(INR)作为正则化先验，以有效约束解空间。通过结合深度展开结构和INR的隐式表示能力，提高了模型的可解释性和重建性能。

Result: 实验结果表明，即使在10倍的高加速率下，UnrollINR也优于监督学习方法。

Conclusion: UnrollINR是一种有效的零样本自监督MRI重建方法，无需外部训练数据即可实现高质量图像重建，并且在高速率下表现优于现有方法。

Abstract: Magnetic Resonance Imaging (MRI) is a vital clinical diagnostic tool, yet its
widespread application is limited by prolonged scan times. Fast MRI
reconstruction techniques effectively reduce acquisition duration by
reconstructing high-fidelity MR images from undersampled k-space data. In
recent years, deep learning-based methods have demonstrated remarkable progress
in this field, with self-supervised and unsupervised learning approaches
proving particularly valuable in scenarios where fully sampled data are
difficult to obtain. This paper proposes a novel zero-shot self-supervised
reconstruction framework named UnrollINR, which enables scan-specific MRI
reconstruction without relying on external training data. The method adopts a
physics-guided unrolled iterative reconstruction architecture and introduces
Implicit Neural Representation (INR) as a regularization prior to effectively
constrain the solution space. By combining a deep unrolled structure with the
powerful implicit representation capability of INR, the model's
interpretability and reconstruction performance are enhanced. Experimental
results demonstrate that even at a high acceleration rate of 10, UnrollINR
achieves superior reconstruction performance compared to the supervised
learning method, validating the superiority of the proposed method.

</details>


### [38] [WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation](https://arxiv.org/abs/2510.07313)
*Zezhong Qian,Xiaowei Chi,Yuming Li,Shizun Wang,Zhiyuan Qin,Xiaozhu Ju,Sirui Han,Shanghang Zhang*

Main category: cs.CV

TL;DR: 该论文提出了WristWorld，一个首个仅从锚视图生成手腕视图视频的4D世界模型，以解决现有模型中手腕视图数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 手腕视图对于VLA模型至关重要，但现有的大规模数据集中缺乏此类记录，导致锚视图和手腕视图之间存在显著差距。现有的世界模型需要手腕视图作为第一帧，无法仅从锚视图生成手腕视图视频。

Method: WristWorld模型分为两个阶段：(1)重建阶段，扩展了VGGT模型并引入了空间投影一致性（SPC）损失，以估计几何一致的手腕视图姿态和4D点云；(2)生成阶段，使用视频生成模型从重建的视角合成时间上连贯的手腕视图视频。

Result: 在Droid、Calvin和Franka Panda数据集上的实验表明，WristWorld在视频生成方面达到了最先进的水平，具有优越的空间一致性。同时，该模型还提高了VLA性能，将Calvin数据集上的平均任务完成长度提高了3.81%，并缩小了42.4%的锚点-手腕视图差距。

Conclusion: WristWorld成功地从锚视图生成了手腕视图视频，解决了数据稀缺问题，并展示了其在提高VLA性能方面的潜力。

Abstract: Wrist-view observations are crucial for VLA models as they capture
fine-grained hand-object interactions that directly enhance manipulation
performance. Yet large-scale datasets rarely include such recordings, resulting
in a substantial gap between abundant anchor views and scarce wrist views.
Existing world models cannot bridge this gap, as they require a wrist-view
first frame and thus fail to generate wrist-view videos from anchor views
alone. Amid this gap, recent visual geometry models such as VGGT emerge with
geometric and cross-view priors that make it possible to address extreme
viewpoint shifts. Inspired by these insights, we propose WristWorld, the first
4D world model that generates wrist-view videos solely from anchor views.
WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and
incorporates our Spatial Projection Consistency (SPC) Loss to estimate
geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,
which employs our video generation model to synthesize temporally coherent
wrist-view videos from the reconstructed perspective. Experiments on Droid,
Calvin, and Franka Panda demonstrate state-of-the-art video generation with
superior spatial consistency, while also improving VLA performance, raising the
average task completion length on Calvin by 3.81% and closing 42.4% of the
anchor-wrist view gap.

</details>


### [39] [A Bridge from Audio to Video: Phoneme-Viseme Alignment Allows Every Face to Speak Multiple Languages](https://arxiv.org/abs/2510.06612)
*Zibo Su,Kun Wei,Jiahua Li,Xu Yang,Cheng Deng*

Main category: cs.CV

TL;DR: MuEx是一个多语言语音驱动的说话人脸合成框架，通过音素和视觉素作为桥梁，解决了现有模型在非英语语言上的不足，并在多语言基准测试中取得了优异成果。


<details>
  <summary>Details</summary>
Motivation: 现有语音驱动的说话人脸合成模型在英语上表现良好，但在非英语语言上效果不佳，原因是缺乏跨语言泛化能力和对英语为主的数据集的依赖。本研究旨在解决这一问题，实现更广泛的语言支持。

Method: 提出了一种名为MuEx的新框架，其核心是音素引导的专家混合（PG-MoE）架构。该架构利用音素和视觉素作为跨模态的通用中间表示。引入了音素-视觉素对齐机制（PV-Align）来解决音视频同步问题。此外，构建了一个包含12种语言的多语言说话人脸基准（MTFB）。

Result: MuEx在MTFB基准测试的所有语言上均表现出优于现有方法的性能，并且在未见过的新语言上也能实现有效的零样本泛化，无需额外训练。

Conclusion: MuEx框架通过音素和视觉素的有效结合以及PV-Align机制，成功实现了高质量的多语言语音驱动说话人脸合成，解决了跨语言泛化能力不足的问题，并在实验中得到了验证。

Abstract: Speech-driven talking face synthesis (TFS) focuses on generating lifelike
facial animations from audio input. Current TFS models perform well in English
but unsatisfactorily in non-English languages, producing wrong mouth shapes and
rigid facial expressions. The terrible performance is caused by the
English-dominated training datasets and the lack of cross-language
generalization abilities. Thus, we propose Multilingual Experts (MuEx), a novel
framework featuring a Phoneme-Guided Mixture-of-Experts (PG-MoE) architecture
that employs phonemes and visemes as universal intermediaries to bridge audio
and video modalities, achieving lifelike multilingual TFS. To alleviate the
influence of linguistic differences and dataset bias, we extract audio and
video features as phonemes and visemes respectively, which are the basic units
of speech sounds and mouth movements. To address audiovisual synchronization
issues, we introduce the Phoneme-Viseme Alignment Mechanism (PV-Align), which
establishes robust cross-modal correspondences between phonemes and visemes. In
addition, we build a Multilingual Talking Face Benchmark (MTFB) comprising 12
diverse languages with 95.04 hours of high-quality videos for training and
evaluating multilingual TFS performance. Extensive experiments demonstrate that
MuEx achieves superior performance across all languages in MTFB and exhibits
effective zero-shot generalization to unseen languages without additional
training.

</details>


### [40] [MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking](https://arxiv.org/abs/2510.06619)
*Tao Feng,Tingfa Xu,Haolin Qin,Tianhao Li,Shuaihao Han,Xuyang Zou,Zhan Lv,Jianan Li*

Main category: cs.CV

TL;DR: 该论文提出了MSITrack，一个大规模、多样化的多光谱单目标跟踪数据集，旨在解决现有RGB跟踪器在真实世界场景下的局限性，并通过多光谱图像增强目标区分能力。


<details>
  <summary>Details</summary>
Motivation: RGB跟踪器在真实世界场景（如遮挡、相似物体干扰、复杂背景）下效果受限，现有公开的多光谱跟踪数据集不足。

Method: 提出MSITrack数据集，包含更具挑战性的属性、更丰富自然的场景（55类物体，300个自然场景）和更大规模（300个视频，129k帧），并经过人工标注和多级验证确保精度。

Result: 在MSITrack数据集上进行的实验表明，多光谱数据相比仅RGB数据能显著提升跟踪性能。

Conclusion: MSITrack数据集通过提供大规模、多样化的多光谱数据，能够推动多光谱单目标跟踪领域的发展。

Abstract: Visual object tracking in real-world scenarios presents numerous challenges
including occlusion, interference from similar objects and complex
backgrounds-all of which limit the effectiveness of RGB-based trackers.
Multispectral imagery, which captures pixel-level spectral reflectance,
enhances target discriminability. However, the availability of multispectral
tracking datasets remains limited. To bridge this gap, we introduce MSITrack,
the largest and most diverse multispectral single object tracking dataset to
date. MSITrack offers the following key features: (i) More Challenging
Attributes-including interference from similar objects and similarity in color
and texture between targets and backgrounds in natural scenarios, along with a
wide range of real-world tracking challenges; (ii) Richer and More Natural
Scenes-spanning 55 object categories and 300 distinct natural scenes, MSITrack
far exceeds the scope of existing benchmarks. Many of these scenes and
categories are introduced to the multispectral tracking domain for the first
time; (iii) Larger Scale-300 videos comprising over 129k frames of
multispectral imagery. To ensure annotation precision, each frame has undergone
meticulous processing, manual labeling and multi-stage verification. Extensive
evaluations using representative trackers demonstrate that the multispectral
data in MSITrack significantly improves performance over RGB-only baselines,
highlighting its potential to drive future advancements in the field. The
MSITrack dataset is publicly available at:
https://github.com/Fengtao191/MSITrack.

</details>


### [41] [StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering](https://arxiv.org/abs/2510.06638)
*Zhihao Wen,Wenkang Wei,Yuan Fang,Xingtong Yu,Hui Zhang,Weicheng Zhu,Xin Zhang*

Main category: cs.CV

TL;DR: 该论文提出了StaR-KVQA，一种用于隐式知识视觉问答（IK-KVQA）的新方法，该方法利用多模态大语言模型（MLLM）作为唯一的知识来源，并通过监督结构化推理轨迹来提高模型的推理能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的IK-KVQA方法在MLLM缺乏显式推理监督、产生不一致的解释以及在标准监督微调后泛化能力差等方面存在不足。

Method: StaR-KVQA通过监督结构化推理轨迹（包括双符号关系路径和路径解释）来解决上述问题。该方法首先构建一个包含推理轨迹的数据集，然后通过结构化自蒸馏进行微调，使模型的生成与监督对齐。整个过程不依赖外部检索器、验证器或知识库，推理过程为单次自回归。

Result: 在多个基准测试中，StaR-KVQA在准确性和可解释性方面均有所提升，在OK-VQA上的答案准确率比最强的基线模型高出11.3%，并表现出鲁棒的跨领域泛化能力。

Conclusion: StaR-KVQA通过结构化推理轨迹的监督，有效提升了IK-KVQA任务的性能和模型的可解释性，展示了其在处理需要复杂推理的视觉问答任务上的潜力。

Abstract: Knowledge-based Visual Question Answering (KVQA) requires models to ground
entities in images and reason over factual knowledge. We study its
implicit-knowledge variant, IK-KVQA, where a multimodal large language model
(MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs
lack explicit reasoning supervision and produce inconsistent justifications,
and generalize poorly after standard supervised fine-tuning (SFT). We present
StaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises
structured traces - dual symbolic relation paths plus path-grounded
natural-language explanations - so that reasoning becomes transparent and
verifiable. With one open-source MLLM, StaR-KVQA constructs and selects
path-grounded reasoning traces to form a trace-enriched dataset, then
fine-tunes via structured self-distillation to align generation with
supervision; no external retrievers, verifiers, or curated knowledge bases
(KBs) are used, traces are built offline, and inference is a single
autoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and
interpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over
the strongest baseline while exhibiting robust cross-domain generalization.

</details>


### [42] [Automated Neural Architecture Design for Industrial Defect Detection](https://arxiv.org/abs/2510.06669)
*Yuxi Liu,Yunfeng Ma,Yi Tang,Min Liu,Shuai Jiang,Yaonan Wang*

Main category: cs.CV

TL;DR: AutoNAD是一个用于工业表面缺陷检测的自动化神经架构设计框架，通过联合搜索卷积、Transformer和MLP，并采用交叉权重共享、多级特征聚合和延迟感知搜索策略，以解决类别内差异和类别间相似性的挑战，同时提高检测精度和运行效率。


<details>
  <summary>Details</summary>
Motivation: 工业表面缺陷检测（SDD）至关重要，但面临类别内差异大和类别间相似性高等挑战，现有手动设计的模型难以有效解决。

Method: 提出AutoNAD框架，联合搜索卷积、Transformer和MLP，采用交叉权重共享策略加速超网收敛，引入可搜索的多级特征聚合模块（MFAM）增强多尺度特征学习，并结合延迟感知先验以选择高效架构。

Result: 在三个工业缺陷数据集上验证了AutoNAD的有效性，并将其应用于一个缺陷成像和检测平台。

Conclusion: AutoNAD通过自动化神经架构设计，有效解决了工业表面缺陷检测中的挑战，提高了检测精度和运行效率，降低了手动网络设计的成本。

Abstract: Industrial surface defect detection (SDD) is critical for ensuring product
quality and manufacturing reliability. Due to the diverse shapes and sizes of
surface defects, SDD faces two main challenges: intraclass difference and
interclass similarity. Existing methods primarily utilize manually designed
models, which require extensive trial and error and often struggle to address
both challenges effectively. To overcome this, we propose AutoNAD, an automated
neural architecture design framework for SDD that jointly searches over
convolutions, transformers, and multi-layer perceptrons. This hybrid design
enables the model to capture both fine-grained local variations and long-range
semantic context, addressing the two key challenges while reducing the cost of
manual network design. To support efficient training of such a diverse search
space, AutoNAD introduces a cross weight sharing strategy, which accelerates
supernet convergence and improves subnet performance. Additionally, a
searchable multi-level feature aggregation module (MFAM) is integrated to
enhance multi-scale feature learning. Beyond detection accuracy, runtime
efficiency is essential for industrial deployment. To this end, AutoNAD
incorporates a latency-aware prior to guide the selection of efficient
architectures. The effectiveness of AutoNAD is validated on three industrial
defect datasets and further applied within a defect imaging and detection
platform. Code will be available at https://github.com/Yuxi104/AutoNAD.

</details>


### [43] [Heptapod: Language Modeling on Visual Signals](https://arxiv.org/abs/2510.06673)
*Yongxin Zhu,Jiawei Chen,Yuanzhe Chen,Zhuo Chen,Dongya Jia,Jian Cong,Xiaobin Zhuang,Yuping Wang,Yuxuan Wang*

Main category: cs.CV

TL;DR: Heptapod是一个图像自回归模型，采用因果注意力机制，无需依赖CFG和语义分词器，通过预测2D图像分布来实现图像生成，并在ImageNet基准测试中取得了优异的FID分数。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在将语言建模的基本原理应用于图像建模，探索一种新的图像生成方法，并摒弃现有的依赖CFG和语义分词器的趋势。

Method: 提出了一种名为Heptapod的图像自回归模型，其核心创新是“下一个2D分布预测”。该模型采用因果Transformer和以重建为中心的视觉分词器，在每个时间步预测图像在整个2D空间网格上的分布。这种学习目标结合了自回归框架的序列建模和掩码自编码的整体自监督学习，通过生成式训练来捕获全面的图像语义。

Result: 在ImageNet生成基准测试中，Heptapod取得了2.70的FID分数，显著优于先前的方法。

Conclusion: 研究者希望这项工作能启发对视觉信号及其他领域语言建模的原则性重新思考。

Abstract: We introduce Heptapod, an image autoregressive model that adheres to the
foundational principles of language modeling. Heptapod employs \textbf{causal
attention}, \textbf{eliminates reliance on CFG}, and \textbf{eschews the trend
of semantic tokenizers}. Our key innovation is \textit{next 2D distribution
prediction}: a causal Transformer with reconstruction-focused visual tokenizer,
learns to predict the distribution over the entire 2D spatial grid of images at
each timestep. This learning objective unifies the sequential modeling of
autoregressive framework with the holistic self-supervised learning of masked
autoencoding, enabling the model to capture comprehensive image semantics via
generative training. On the ImageNet generation benchmark, Heptapod achieves an
FID of $2.70$, significantly outperforming previous causal autoregressive
approaches. We hope our work inspires a principled rethinking of language
modeling on visual signals and beyond.

</details>


### [44] [DreamOmni2: Multimodal Instruction-based Editing and Generation](https://arxiv.org/abs/2510.06679)
*Bin Xia,Bohao Peng,Yuechen Zhang,Junjia Huang,Jiyang Liu,Jingyao Li,Haoru Tan,Sitong Wu,Chengyao Wang,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 本研究提出了多模态指令编辑和生成任务，并介绍了DreamOmni2模型，以解决现有图像编辑和生成任务的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有指令式图像编辑依赖单一文本指令，难以捕捉细节，需要参考图；而面向主体的生成仅限于具体对象，忽略抽象概念。为满足实际用户需求，需要更灵活、更广泛的应用。

Method: 研究提出了多模态指令编辑和生成任务，并设计了DreamOmni2模型。数据合成包括三个步骤：1. 使用特征混合法创建抽象和具体概念的提取数据；2. 利用编辑和提取模型生成多模态指令编辑训练数据；3. 进一步应用提取模型创建多模态指令编辑的训练数据。模型框架方面，为处理多图输入，提出索引编码和位置编码移位方案，以区分图像并避免像素混淆。此外，通过与VLM联合训练来处理复杂指令。同时，为新任务创建了全面的基准。

Result: DreamOmni2在多模态指令编辑和生成任务上取得了显著成果。

Conclusion: DreamOmni2模型在数据创建和模型框架设计上解决了多模态指令编辑和生成任务的挑战，并取得了令人印象深刻的结果，有望推动相关领域的发展。

Abstract: Recent advancements in instruction-based image editing and subject-driven
generation have garnered significant attention, yet both tasks still face
limitations in meeting practical user needs. Instruction-based editing relies
solely on language instructions, which often fail to capture specific editing
details, making reference images necessary. Meanwhile, subject-driven
generation is limited to combining concrete objects or people, overlooking
broader, abstract concepts. To address these challenges, we propose two novel
tasks: multimodal instruction-based editing and generation. These tasks support
both text and image instructions and extend the scope to include both concrete
and abstract concepts, greatly enhancing their practical applications. We
introduce DreamOmni2, tackling two primary challenges: data creation and model
framework design. Our data synthesis pipeline consists of three steps: (1)
using a feature mixing method to create extraction data for both abstract and
concrete concepts, (2) generating multimodal instruction-based editing training
data using the editing and extraction models, and (3) further applying the
extraction model to create training data for multimodal instruction-based
editing. For the framework, to handle multi-image input, we propose an index
encoding and position encoding shift scheme, which helps the model distinguish
images and avoid pixel confusion. Additionally, we introduce joint training
with the VLM and our generation/editing model to better process complex
instructions. In addition, we have proposed comprehensive benchmarks for these
two new tasks to drive their development. Experiments show that DreamOmni2 has
achieved impressive results. Models and codes will be released.

</details>


### [45] [Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion](https://arxiv.org/abs/2510.06687)
*Jie Luo,Yuxuan Jiang,Xin Jin,Mingyu Liu,Yihui Fan*

Main category: cs.CV

TL;DR: 提出首个融合光场和激光雷达数据的多模态语义分割数据集，并提出 Mlpfseg 网络，通过特征补全和深度感知模块有效融合两种模态，提升了自动驾驶场景下的语义分割性能，尤其是在处理遮挡问题时。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的语义分割在复杂条件下（如遮挡）仍面临挑战。光场和激光雷达模态提供了互补的视觉和空间线索，但其有效整合受限于视角多样性不足和模态差异。

Method: 提出首个融合光场数据和点云数据的多模态语义分割数据集。提出 Mlpfseg 网络，包含特征补全模块（通过差分重建点云特征图解决点云与图像像素密度不匹配问题）和深度感知模块（增强遮挡感知能力），以同时分割图像和点云。

Result: Mlpfseg 在多模态语义分割方面超越了单独使用图像分割（mIoU 提升 1.71%）和单独使用点云分割（mIoU 提升 2.38%）。

Conclusion: 所提出的多模态数据集和 Mlpfseg 网络有效解决了光场和激光雷达数据融合的挑战，显著提高了在复杂场景（尤其是遮挡情况）下的自动驾驶语义分割的鲁棒性和准确性。

Abstract: Semantic segmentation serves as a cornerstone of scene understanding in
autonomous driving but continues to face significant challenges under complex
conditions such as occlusion. Light field and LiDAR modalities provide
complementary visual and spatial cues that are beneficial for robust
perception; however, their effective integration is hindered by limited
viewpoint diversity and inherent modality discrepancies. To address these
challenges, the first multimodal semantic segmentation dataset integrating
light field data and point cloud data is proposed. Based on this dataset, we
proposed a multi-modal light field point-cloud fusion segmentation
network(Mlpfseg), incorporating feature completion and depth perception to
segment both camera images and LiDAR point clouds simultaneously. The feature
completion module addresses the density mismatch between point clouds and image
pixels by performing differential reconstruction of point-cloud feature maps,
enhancing the fusion of these modalities. The depth perception module improves
the segmentation of occluded objects by reinforcing attention scores for better
occlusion awareness. Our method outperforms image-only segmentation by 1.71
Mean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38
mIoU, demonstrating its effectiveness.

</details>


### [46] [SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis](https://arxiv.org/abs/2510.06694)
*Jipeng Lyu,Jiahua Dong,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: SCas4D是一个用于动态场景建模的级联优化框架，它利用3D高斯泼溅中的结构模式来捕捉变形，实现了高效的跟踪和新视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有的动态场景建模方法在精确捕捉形变和保持计算效率方面存在挑战。

Method: SCas4D采用级联优化框架，利用3D高斯泼溅中的结构模式，逐步细化从粗粒度到细粒度的形变，实现了每帧100次迭代的收敛。

Result: SCas4D的训练迭代次数仅为现有方法的二十分之一，在自监督关节对象分割、新视角合成和密集点跟踪任务中取得了与现有方法相当的结果。

Conclusion: SCas4D通过利用变形的层级模式，在保持计算效率的同时，有效地实现了动态场景建模，并在多个下游任务中表现出色。

Abstract: Persistent dynamic scene modeling for tracking and novel-view synthesis
remains challenging due to the difficulty of capturing accurate deformations
while maintaining computational efficiency. We propose SCas4D, a cascaded
optimization framework that leverages structural patterns in 3D Gaussian
Splatting for dynamic scenes. The key idea is that real-world deformations
often exhibit hierarchical patterns, where groups of Gaussians share similar
transformations. By progressively refining deformations from coarse part-level
to fine point-level, SCas4D achieves convergence within 100 iterations per time
frame and produces results comparable to existing methods with only
one-twentieth of the training iterations. The approach also demonstrates
effectiveness in self-supervised articulated object segmentation, novel view
synthesis, and dense point tracking tasks.

</details>


### [47] [Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities](https://arxiv.org/abs/2510.06743)
*Maria Levchenko*

Main category: cs.CV

TL;DR: 该研究提出了一个评估LLM在历史文献OCR中的新方法，并引入了HCPR和AIR等新指标，以解决传统指标无法捕捉历史文献特有偏见和错误的问题。通过在18世纪俄文字体文本上的实验，发现Gemini和Qwen模型表现优于传统OCR，但也存在“过度历史化”的问题（即从不正确的历史时期插入古代字符）。研究还发现，OCR后校正反而会降低性能。


<details>
  <summary>Details</summary>
Motivation: 传统OCR评估指标无法捕捉历史文献的时代偏见和特定时期错误，而这些对于创建历史语料库至关重要。因此，需要一个针对基于LLM的历史OCR的评估框架。

Method: 提出了一种评估LLM在历史文献OCR中的方法，解决了污染风险和外交转录中的系统性偏差。使用18世纪俄文字体文本，引入了历史字符保留率（HCPR）和古风插入率（AIR）等新指标，并制定了污染控制和稳定性测试方案，评估了12个多模态LLM。

Result: Gemini和Qwen模型在18世纪俄文字体文本上的表现优于传统OCR，但存在过度历史化的问题（即从不正确的历史时期插入古风字符）。OCR后校正反而会降低性能。

Conclusion: 所提出的评估方法为数字人文实践者在历史语料库数字化中进行模型选择和质量评估提供了指导。

Abstract: Digital humanities scholars increasingly use Large Language Models for
historical document digitization, yet lack appropriate evaluation frameworks
for LLM-based OCR. Traditional metrics fail to capture temporal biases and
period-specific errors crucial for historical corpus creation. We present an
evaluation methodology for LLM-based historical OCR, addressing contamination
risks and systematic biases in diplomatic transcription. Using 18th-century
Russian Civil font texts, we introduce novel metrics including Historical
Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside
protocols for contamination control and stability testing. We evaluate 12
multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR
while exhibiting over-historicization: inserting archaic characters from
incorrect historical periods. Post-OCR correction degrades rather than improves
performance. Our methodology provides digital humanities practitioners with
guidelines for model selection and quality assessment in historical corpus
digitization.

</details>


### [48] [DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement for Image Deraining](https://arxiv.org/abs/2510.06746)
*Zhiliang Zhu,Tao Zeng,Tao Yang,Guoliang Luo,Jiyong Zeng*

Main category: cs.CV

TL;DR: DeRainMamba通过结合傅里叶变换和多向感知卷积来增强图像去雨效果，在保持细节的同时有效去除雨纹，并在多个基准测试中取得了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Mamba的模型在图像去雨方面存在捕捉细节能力有限和缺乏频域感知的问题，限制了性能的进一步提升。

Method: 提出DeRainMamba模型，集成了频率感知状态空间模块（FASSM）和多向感知卷积（MDPConv）。FASSM利用傅里叶变换区分雨纹和图像细节，MDPConv则通过捕捉各向异性梯度特征并融合多卷积分支来恢复局部结构。

Result: 在四个公开基准测试中，DeRainMamba在PSNR和SSIM方面持续优于最先进的方法，同时参数量和计算成本更低。

Conclusion: 将频域建模和空间细节增强结合到状态空间框架中，是解决单幅图像去雨问题的有效方法。

Abstract: Image deraining is crucial for improving visual quality and supporting
reliable downstream vision tasks. Although Mamba-based models provide efficient
sequence modeling, their limited ability to capture fine-grained details and
lack of frequency-domain awareness restrict further improvements. To address
these issues, we propose DeRainMamba, which integrates a Frequency-Aware
State-Space Module (FASSM) and Multi-Directional Perception Convolution
(MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from
high-frequency image details, balancing rain removal and detail preservation.
MDPConv further restores local structures by capturing anisotropic gradient
features and efficiently fusing multiple convolution branches. Extensive
experiments on four public benchmarks demonstrate that DeRainMamba consistently
outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer
parameters and lower computational costs. These results validate the
effectiveness of combining frequency-domain modeling and spatial detail
enhancement within a state-space framework for single image deraining.

</details>


### [49] [OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot](https://arxiv.org/abs/2510.06751)
*Junhan Zhu,Hesong Wang,Mingluo Su,Zefang Wang,Huan Wang*

Main category: cs.CV

TL;DR: OBS-Diff是一个创新的、无需训练的压缩框架，用于大规模文生图扩散模型，通过改进的OBS和时间步感知Hessian构造，实现了高效的稀疏化，并在保持视觉质量的同时加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有的网络剪枝方法难以直接应用于扩散模型，因为它们具有迭代去噪的特性，而OBS-Diff旨在解决这个问题，实现对扩散模型的准确、无需训练的压缩。

Method: OBS-Diff 框架包含三个关键部分：(1) 改进Optimal Brain Surgeon (OBS)，支持非结构化、N:M半结构化和结构化（MHA头和FFN神经元）稀疏性；(2) 提出时间步感知Hessian构造，包含对早期时间步赋予更大权重的对数衰减加权方案，以解决误差累积问题；(3) 提出计算高效的分组顺序剪枝策略，以分摊昂贵的校准过程。

Result: 实验证明，OBS-Diff 在扩散模型上实现了最先进的单次剪枝效果，在视觉质量略有下降的情况下实现了推理加速。

Conclusion: OBS-Diff 成功地实现了大规模文生图扩散模型的压缩，通过其新颖的方法在准确性和效率之间取得了良好的平衡。

Abstract: Large-scale text-to-image diffusion models, while powerful, suffer from
prohibitive computational cost. Existing one-shot network pruning methods can
hardly be directly applied to them due to the iterative denoising nature of
diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel
one-shot pruning framework that enables accurate and training-free compression
of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff
revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex
architectures of modern diffusion models and supporting diverse pruning
granularity, including unstructured, N:M semi-structured, and structured (MHA
heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the
iterative dynamics of the diffusion process, by examining the problem from an
error-accumulation perspective, we propose a novel timestep-aware Hessian
construction that incorporates a logarithmic-decrease weighting scheme,
assigning greater importance to earlier timesteps to mitigate potential error
accumulation; (iii) Furthermore, a computationally efficient group-wise
sequential pruning strategy is proposed to amortize the expensive calibration
process. Extensive experiments show that OBS-Diff achieves state-of-the-art
one-shot pruning for diffusion models, delivering inference acceleration with
minimal degradation in visual quality.

</details>


### [50] [Transforming Noise Distributions with Histogram Matching: Towards a Single Denoiser for All](https://arxiv.org/abs/2510.06757)
*Sheng Fu,Junchao Zhang,Kailun Yang*

Main category: cs.CV

TL;DR: 通过直方图匹配将任意噪声转换为目标高斯噪声，并结合去噪和噪声转换的协同作用，显著提升了单一高斯去噪器在处理各种分布外噪声（包括合成噪声和真实世界噪声）时的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的监督式高斯去噪器在处理分布外噪声时泛化能力有限，因为不同噪声类型的分布特征各异。本研究旨在弥合这一差距。

Method: 提出一种直方图匹配方法，将任意噪声转换为具有已知强度的目标高斯分布。建立噪声转换和后续去噪之间的相互促进循环，逐步优化噪声转换，使其接近真实噪声，从而增强转换效果并进一步提高去噪性能。针对特定噪声复杂性，采用局部直方图匹配处理信号相关噪声，块内排列处理通道相关噪声，以及频域直方图匹配结合像素混洗下采样打破空间相关性。

Result: 通过这些转换，单一高斯去噪器获得了处理各种分布外噪声（包括泊松噪声、椒盐噪声、重复模式噪声等合成噪声以及复杂的真实世界噪声）的卓越能力。实验证明了该方法的优越泛化能力和有效性。

Conclusion: 本方法通过直方图匹配和协同去噪循环，有效提升了单一高斯去噪器在处理多样化分布外噪声时的泛化能力和性能。

Abstract: Supervised Gaussian denoisers exhibit limited generalization when confronted
with out-of-distribution noise, due to the diverse distributional
characteristics of different noise types. To bridge this gap, we propose a
histogram matching approach that transforms arbitrary noise towards a target
Gaussian distribution with known intensity. Moreover, a mutually reinforcing
cycle is established between noise transformation and subsequent denoising.
This cycle progressively refines the noise to be converted, making it
approximate the real noise, thereby enhancing the noise transformation effect
and further improving the denoising performance. We tackle specific noise
complexities: local histogram matching handles signal-dependent noise,
intrapatch permutation processes channel-related noise, and frequency-domain
histogram matching coupled with pixel-shuffle down-sampling breaks spatial
correlation. By applying these transformations, a single Gaussian denoiser
gains remarkable capability to handle various out-of-distribution noises,
including synthetic noises such as Poisson, salt-and-pepper and repeating
pattern noises, as well as complex real-world noises. Extensive experiments
demonstrate the superior generalization and effectiveness of our method.

</details>


### [51] [A deep multiple instance learning approach based on coarse labels for high-resolution land-cover mapping](https://arxiv.org/abs/2510.06769)
*Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 本论文提出了一种利用深度多示例学习（DMIL）来训练高分辨率土地覆盖分类器的方​​法，该方法使用低分辨率的弱标签作为监督信号。


<details>
  <summary>Details</summary>
Motivation: 高分辨率土地覆盖图的训练标签数量和质量是关键问题。本研究旨在使用现有的、数量丰富但分辨率较低或过时的产品作为弱标签，来训练高分辨率土地覆盖分类器。

Method: 提出了一种深度多示例学习（DMIL）方法，利用灵活的池化层将高分辨率影像像素与低分辨率参考标签关联起来，实现了像素级多类分类器的训练和低分辨率标签的预测。该方法在多类和多标签场景下重新构建了多示例学习（MIL）问题，并采用了正样本-无标签学习（PUL）策略进行训练。

Result: 在2020 IEEE GRSS 数据融合竞赛数据集上的实验结果表明，所提出的框架在与标准训练策略相比方面具有有效性。

Conclusion: 所提出的基于DMIL和PUL的框架能够有效地利用低分辨率的弱标签来训练高分辨率土地覆盖分类器，并在实验中取得了优于标准训练方法的成果。

Abstract: The quantity and the quality of the training labels are central problems in
high-resolution land-cover mapping with machine-learning-based solutions. In
this context, weak labels can be gathered in large quantities by leveraging on
existing low-resolution or obsolete products. In this paper, we address the
problem of training land-cover classifiers using high-resolution imagery (e.g.,
Sentinel-2) and weak low-resolution reference data (e.g., MODIS -derived
land-cover maps). Inspired by recent works in Deep Multiple Instance Learning
(DMIL), we propose a method that trains pixel-level multi-class classifiers and
predicts low-resolution labels (i.e., patch-level classification), where the
actual high-resolution labels are learned implicitly without direct
supervision. This is achieved with flexible pooling layers that are able to
link the semantics of the pixels in the high-resolution imagery to the
low-resolution reference labels. Then, the Multiple Instance Learning (MIL)
problem is re-framed in a multi-class and in a multi-label setting. In the
former, the low-resolution annotation represents the majority of the pixels in
the patch. In the latter, the annotation only provides us information on the
presence of one of the land-cover classes in the patch and thus multiple labels
can be considered valid for a patch at a time, whereas the low-resolution
labels provide us only one label. Therefore, the classifier is trained with a
Positive-Unlabeled Learning (PUL) strategy. Experimental results on the 2020
IEEE GRSS Data Fusion Contest dataset show the effectiveness of the proposed
framework compared to standard training strategies.

</details>


### [52] [TTRV: Test-Time Reinforcement Learning for Vision Language Models](https://arxiv.org/abs/2510.06783)
*Akshit Singh,Shyam Marjit,Wei Lin,Paul Gavrikov,Serena Yeung-Levy,Hilde Kuehne,Rogerio Feris,Sivan Doveh,James Glass,M. Jehanzeb Mirza*

Main category: cs.CV

TL;DR: TTRV通过在推理时动态调整模型来增强视觉语言理解，无需标注数据，并通过奖励模型输出的频率和低熵来提高性能，在图像识别和VQA方面取得了显著的改进，甚至超越了GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习奖励信号提取方法依赖标注数据和专用训练集，这与人类学习方式不同。本研究旨在提出一种无需标注数据的方法，在推理时动态调整模型，以增强视觉语言理解能力。

Method: TTRV通过增强GRPO框架，设计了基于基础模型输出频率的奖励，并在推理时多次对每个测试样本进行推断。此外，还通过奖励模型输出的低熵来控制输出的多样性。

Result: TTRV在物体识别和视觉问答方面均取得了显著的性能提升，分别最高提升了52.4%和29.8%，在16个数据集上的平均提升分别为24.6%和10.0%。在图像识别方面，TTRV在8个基准测试上平均超越GPT-4o 2.3%，同时在VQA方面也保持竞争力。

Conclusion: TTRV在测试时进行强化学习，可以媲美甚至超越最强的专有模型。即使在数据极其有限的情况下，TTRV也能在识别任务中带来显著的改进。

Abstract: Existing methods for extracting reward signals in Reinforcement Learning
typically rely on labeled data and dedicated training splits, a setup that
contrasts with how humans learn directly from their environment. In this work,
we propose TTRV to enhance vision language understanding by adapting the model
on the fly at inference time, without the need for any labeled data.
Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework
by designing rewards based on the frequency of the base model's output, while
inferring on each test sample multiple times. Further, we also propose to
control the diversity of the model's output by simultaneously rewarding the
model for obtaining low entropy of the output empirical distribution. Our
approach delivers consistent gains across both object recognition and visual
question answering (VQA), with improvements of up to 52.4% and 29.8%,
respectively, and average boosts of 24.6% and 10.0% across 16
datasets.Remarkably, on image recognition, TTRV applied to InternVL 8B
surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining
highly competitive on VQA, demonstrating that test-time reinforcement learning
can match or exceed the strongest proprietary models. Finally, we find many
interesting properties of test-time RL for VLMs: for example, even in extremely
data-constrained scenarios, where adaptation is performed on a single randomly
chosen unlabeled test example, TTRV still yields non-trivial improvements of up
to 5.5% in recognition tasks.

</details>


### [53] [Extreme Amodal Face Detection](https://arxiv.org/abs/2510.06791)
*Changlin Song,Yunzhong Hou,Michael Randall Barnes,Rahul Shome,Dylan Campbell*

Main category: cs.CV

TL;DR: 研究了在单张图像中检测不在视野内但可能存在的物体（极端非遮挡检测），并提出了一种无需生成模型、利用上下文线索的高效方法。


<details>
  <summary>Details</summary>
Motivation: 人脸检测在安全和隐私方面有应用需求，但现有方法依赖图像序列，而本文旨在解决单图像的极端非遮挡检测问题。

Method: 提出了一种基于热力图的极端非遮挡物体检测器，使用选择性的粗略到精细的解码器来预测图像外的区域。

Result: 在极端非遮挡人脸检测任务上取得了很好的结果，并且优于一些更耗时的生成模型方法。

Conclusion: 提出的基于热力图的极端非遮挡检测方法在单图像任务上是高效且有效的，能够利用上下文线索推断视野外的物体。

Abstract: Extreme amodal detection is the task of inferring the 2D location of objects
that are not fully visible in the input image but are visible within an
expanded field-of-view. This differs from amodal detection, where the object is
partially visible within the input image, but is occluded. In this paper, we
consider the sub-problem of face detection, since this class provides
motivating applications involving safety and privacy, but do not tailor our
method specifically to this class. Existing approaches rely on image sequences
so that missing detections may be interpolated from surrounding frames or make
use of generative models to sample possible completions. In contrast, we
consider the single-image task and propose a more efficient, sample-free
approach that makes use of the contextual cues from the image to infer the
presence of unseen faces. We design a heatmap-based extreme amodal object
detector that addresses the problem of efficiently predicting a lot (the
out-of-frame region) from a little (the image) with a selective coarse-to-fine
decoder. Our method establishes strong results for this new task, even
outperforming less efficient generative approaches.

</details>


### [54] [VA-Adapter: Adapting Ultrasound Foundation Model to Echocardiography Probe Guidance](https://arxiv.org/abs/2510.06809)
*Teng Wang,Haojun Jiang,Yuxuan Wang,Zhenguo Sun,Shiji Song,Gao Huang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为VA-Adapter的参数高效视觉-动作适配器，用于优化心脏超声图像分析的探头引导任务，旨在帮助初级操作者获取高质量图像。


<details>
  <summary>Details</summary>
Motivation: 心脏超声检查对诊断心脏疾病至关重要，但由于操作难度大、专业人员短缺，导致高质量图像获取困难，影响了患者的及时诊疗。

Method: 研究人员设计了一种参数高效的视觉-动作适配器（VA-Adapter），使其能够处理视觉-动作序列，从而增强基础模型在探头引导方面的性能。该适配器通过微调少量参数，使预训练的超声基础模型能够学习精确的探头调整策略。

Result: 实验结果表明，VA-Adapter在探头引导任务上优于现有的模型。

Conclusion: VA-Adapter通过利用基础模型的医学知识和其内置的序列推理能力，有效解决了心脏超声探头引导的难题，有望提升 초음파 图像分析的准确性和效率。

Abstract: Echocardiography is a critical tool for detecting heart diseases. Recently,
ultrasound foundation models have demonstrated remarkable capabilities in
cardiac ultrasound image analysis. However, obtaining high-quality ultrasound
images is a prerequisite for accurate diagnosis. Due to the exceptionally high
operational difficulty of cardiac ultrasound, there is a shortage of highly
skilled personnel, which hinders patients from receiving timely examination
services. In this paper, we aim to adapt the medical knowledge learned by
foundation models from vast datasets to the probe guidance task, which is
designed to provide real-time operational recommendations for junior
sonographers to acquire high-quality ultrasound images. Moreover, inspired by
the practice where experts optimize action decisions based on past
explorations, we meticulously design a parameter-efficient Vision-Action
Adapter (VA-Adapter) to enable foundation model's image encoder to encode
vision-action sequences, thereby enhancing guidance performance. With built-in
sequential reasoning capabilities in a compact design, the VA-Adapter enables a
pre-trained ultrasound foundation model to learn precise probe adjustment
strategies by fine-tuning only a small subset of parameters. Extensive
experiments demonstrate that the VA-Adapter can surpass strong probe guidance
models. Our code will be released after acceptance.

</details>


### [55] [Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking](https://arxiv.org/abs/2510.06820)
*Mitchell Keren Taraday,Shahaf Wagner,Chaim Baskin*

Main category: cs.CV

TL;DR: EDJE是一种高效的判别性联合编码器，通过离线预计算视觉标记并使用轻量级适配器进行压缩，显著降低了存储和在线计算成本，实现了高吞吐量的推理，同时保持了强大的检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于嵌入的模型（如CLIP）在多模态检索中虽然能够进行快速的向量搜索，但缺乏类似于文本检索中的联合编码器重排模型。而像BLIP这样的联合编码器由于计算量大的视觉特征提取阶段，难以大规模部署。

Method: EDJE通过预计算视觉标记并使用基于注意力的轻量级适配器进行压缩，将视觉标记的离线处理和文本的在线处理相结合。这使得在线推理仅需处理一个紧凑的联合编码器、少量视觉标记以及文本。

Result: EDJE实现了每秒处理50k图像-文本对的吞吐量，并且每张图像仅需49kB的磁盘存储，在Flickr（零样本）和COCO（微调）检索任务上取得了与现有技术相当的性能。

Conclusion: EDJE通过优化视觉特征提取和压缩，有效解决了现有联合编码器在大规模部署中的瓶颈问题，实现了高效、低存储和高吞吐量的多模态检索。

Abstract: Multimodal retrieval still leans on embedding-based models like CLIP for fast
vector search over pre-computed image embeddings. Yet, unlike text retrieval,
where joint-encoder rerankers are standard, comparable vision--language
rerankers are largely absent. We find that seminal joint encoders such as BLIP
are severely bottlenecked by an expensive visual feature-extraction stage,
preventing practical deployment at scale. Motivated by this bottleneck, we
introduce EDJE, an Efficient Discriminative Joint Encoder that precomputes
vision tokens offline and compresses them via a lightweight attention-based
adapter, so online inference runs only a compact joint encoder over a small set
of visual tokens plus the text. EDJE preserves strong retrieval performance
while drastically reducing storage and online compute, enabling high-throughput
inference. Specifically, EDJE processes 50k image--text pairs/second while
requiring 49kB of disk storage per image, matching prior art on Flickr
(zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints
will be made publicly available shortly.

</details>


### [56] [StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance](https://arxiv.org/abs/2510.06827)
*Jaeseok Jeong,Junho Kim,Gayoung Lee,Yunjey Choi,Youngjung Uh*

Main category: cs.CV

TL;DR: 通过引入负视觉查询引导（NVQG）并扩展分类器自由引导（CFG），提出了一种减少文本到图像生成中视觉风格提示内容泄露的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉提示方法在文本到图像生成中存在内容泄露问题，即不期望的视觉风格元素被转移。 

Method: 1) 扩展分类器自由引导（CFG）以利用自注意力交换。 2) 提出负视觉查询引导（NVQG），通过模拟查询交换而非键值交换的场景，利用负分数来减少不期望内容的转移。 此外，还提供了使用真实图像作为视觉风格提示的解决方案。

Result: 该方法显著减少了内容泄露，并在各种风格和文本提示的广泛评估中证明了其优越性，能够准确反映参考风格并确保生成的图像匹配文本提示。

Conclusion: 所提出的NVQG方法是一种有效且简单的解决方案，可解决文本到图像生成中的内容泄露问题，并在视觉风格提示方面取得了比现有方法更好的结果。

Abstract: In the domain of text-to-image generation, diffusion models have emerged as
powerful tools. Recently, studies on visual prompting, where images are used as
prompts, have enabled more precise control over style and content. However,
existing methods often suffer from content leakage, where undesired elements of
the visual style prompt are transferred along with the intended style. To
address this issue, we 1) extend classifier-free guidance (CFG) to utilize
swapping self-attention and propose 2) negative visual query guidance (NVQG) to
reduce the transfer of unwanted contents. NVQG employs negative score by
intentionally simulating content leakage scenarios that swap queries instead of
key and values of self-attention layers from visual style prompts. This simple
yet effective method significantly reduces content leakage. Furthermore, we
provide careful solutions for using a real image as visual style prompts.
Through extensive evaluation across various styles and text prompts, our method
demonstrates superiority over existing approaches, reflecting the style of the
references, and ensuring that resulting images match the text prompts. Our code
is available \href{https://github.com/naver-ai/StyleKeeper}{here}.

</details>


### [57] [Lattice-allocated Real-time Line Segment Feature Detection and Tracking Using Only an Event-based Camera](https://arxiv.org/abs/2510.06829)
*Mikihiro Ikura,Arren Glover,Masayoshi Mizuno,Chiara Bartolozzi*

Main category: cs.CV

TL;DR: 提出了一种仅使用事件相机即可进行实时线段检测和跟踪的方法，解决了现有方法对额外帧相机或高事件率的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实时提取人造环境的几何特征方面存在不足，尤其是在依赖额外帧相机或处理高事件率时。本研究旨在解决仅使用高分辨率事件相机进行实时线段检测和跟踪的挑战。

Method: 研究提出了一种基于事件相机的线段提取方法，包括（i）速度不变的事件表示，（ii）基于拟合得分的线段检测，（iii）通过扰动端点进行线段跟踪，构建了一个格子分配的流水线。

Result: 所提出的方法在ad-hoc记录的数据集和公开数据集上进行了评估，证明了其能够实现实时性能，并且相比于最先进的纯事件相机和事件-帧混合基线，具有更高的准确性。

Conclusion: 该研究成功实现了仅使用事件相机进行实时线段检测和跟踪，实现了完全独立于传统相机在现实世界中的运行。

Abstract: Line segment extraction is effective for capturing geometric features of
human-made environments. Event-based cameras, which asynchronously respond to
contrast changes along edges, enable efficient extraction by reducing redundant
data. However, recent methods often rely on additional frame cameras or
struggle with high event rates. This research addresses real-time line segment
detection and tracking using only a modern, high-resolution (i.e., high event
rate) event-based camera. Our lattice-allocated pipeline consists of (i)
velocity-invariant event representation, (ii) line segment detection based on a
fitting score, (iii) and line segment tracking by perturbating endpoints.
Evaluation using ad-hoc recorded dataset and public datasets demonstrates
real-time performance and higher accuracy compared to state-of-the-art
event-only and event-frame hybrid baselines, enabling fully stand-alone event
camera operation in real-world settings.

</details>


### [58] [Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization](https://arxiv.org/abs/2510.06842)
*Kanglei Zhou,Qingyi Pan,Xingxing Zhang,Hubert P. H. Shum,Frederick W. B. Li,Xiaohui Liang,Liyuan Wang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Action Quality Assessment (AQA) quantifies human actions in videos,
supporting applications in sports scoring, rehabilitation, and skill
evaluation. A major challenge lies in the non-stationary nature of quality
distributions in real-world scenarios, which limits the generalization ability
of conventional methods. We introduce Continual AQA (CAQA), which equips AQA
with Continual Learning (CL) capabilities to handle evolving distributions
while mitigating catastrophic forgetting. Although parameter-efficient
fine-tuning of pretrained models has shown promise in CL for image
classification, we find it insufficient for CAQA. Our empirical and theoretical
analyses reveal two insights: (i) Full-Parameter Fine-Tuning (FPFT) is
necessary for effective representation learning; yet (ii) uncontrolled FPFT
induces overfitting and feature manifold shift, thereby aggravating forgetting.
To address this, we propose Adaptive Manifold-Aligned Graph Regularization
(MAGR++), which couples backbone fine-tuning that stabilizes shallow layers
while adapting deeper ones with a two-step feature rectification pipeline: a
manifold projector to translate deviated historical features into the current
representation space, and a graph regularizer to align local and global
distributions. We construct four CAQA benchmarks from three datasets with
tailored evaluation protocols and strong baselines, enabling systematic
cross-dataset comparison. Extensive experiments show that MAGR++ achieves
state-of-the-art performance, with average correlation gains of 3.6% offline
and 12.2% online over the strongest baseline, confirming its robustness and
effectiveness. Our code is available at https://github.com/ZhouKanglei/MAGRPP.

</details>


### [59] [Online Generic Event Boundary Detection](https://arxiv.org/abs/2510.06855)
*Hyungrok Jung,Daneul Kim,Seunggyun Lim,Jeany Son,Jonghyun Choi*

Main category: cs.CV

TL;DR: GEBD 旨在根据人类感知来解释长视频，但现有方法需要处理完整视频帧。为解决此问题，我们提出了在线 GEBD (On-GEBD) 任务，旨在实时检测流媒体视频中的通用事件边界。为应对此挑战，我们提出了基于事件分割理论 (EST) 的 Estimator 框架。该框架包含一致事件预测器 (CEA) 和在线边界判别器 (OBD)。CEA 根据先验帧预测未来帧，OBD 测量预测误差并自适应调整阈值。实验结果表明，Estimator 在 Kinetics-GEBD 和 TAPOS 数据集上的性能优于基线方法，并可与离线 GEBD 方法相媲美。


<details>
  <summary>Details</summary>
Motivation: 现有 GEBD 方法需要处理完整视频帧，无法像人类一样实时处理数据。因此，提出新的 On-GEBD 任务，以解决在流媒体视频中实时检测事件边界的挑战。

Method: 提出 Estimator 框架，受事件分割理论 (EST) 启发。该框架包含一致事件预测器 (CEA)，用于预测未来帧；在线边界判别器 (OBD)，用于测量预测误差并自适应调整阈值。

Result: Estimator 在 Kinetics-GEBD 和 TAPOS 数据集上的性能优于基线方法，并可与离线 GEBD 方法相媲美。

Conclusion: Estimator 框架成功解决了 On-GEBD 任务的挑战，能够在流媒体视频中实时、准确地检测事件边界。

Abstract: Generic Event Boundary Detection (GEBD) aims to interpret long-form videos
through the lens of human perception. However, current GEBD methods require
processing complete video frames to make predictions, unlike humans processing
data online and in real-time. To bridge this gap, we introduce a new task,
Online Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries
of generic events immediately in streaming videos. This task faces unique
challenges of identifying subtle, taxonomy-free event changes in real-time,
without the access to future frames. To tackle these challenges, we propose a
novel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST)
which explains how humans segment ongoing activity into events by leveraging
the discrepancies between predicted and actual information. Our framework
consists of two key components: the Consistent Event Anticipator (CEA), and the
Online Boundary Discriminator (OBD). Specifically, the CEA generates a
prediction of the future frame reflecting current event dynamics based solely
on prior frames. Then, the OBD measures the prediction error and adaptively
adjusts the threshold using statistical tests on past errors to capture
diverse, subtle event transitions. Experimental results demonstrate that
Estimator outperforms all baselines adapted from recent online video
understanding models and achieves performance comparable to prior offline-GEBD
methods on the Kinetics-GEBD and TAPOS datasets.

</details>


### [60] [Explaining raw data complexity to improve satellite onboard processing](https://arxiv.org/abs/2510.06858)
*Adrien Dorise,Marjorie Bellizzi,Adrien Girard,Benjamin Francesconi,Stéphane May*

Main category: cs.CV

TL;DR: 直接在卫星上部署AI模型处理原始遥感数据在物体检测和分类任务上是可行的，但原始数据在物体边界识别方面存在挑战，需要改进AI架构以适应。


<details>
  <summary>Details</summary>
Motivation: 在卫星上直接部署AI模型处理原始遥感数据的可行性，以及解决利用原始数据对物体检测和分类任务的影响。

Method: 研究使用YOLOv11s和YOLOX-S模型，在原始数据和L1数据上进行训练，并使用标准检测指标和可解释性工具进行性能比较。引入了一个模拟工作流来生成L1图像的原始类产品。

Result: 在低到中置信度阈值下，两种模型表现相似。然而，在低置信度阈值下，使用原始数据训练的模型在物体边界识别方面表现不佳，特别是在高置信度水平下。

Conclusion: 虽然可以直接在卫星上处理原始遥感数据，但原始数据在物体边界识别方面存在挑战。建议通过改进AI架构和轮廓提取方法来提高在原始图像上的物体检测性能，从而改进遥感中的板载AI。

Abstract: With increasing processing power, deploying AI models for remote sensing
directly onboard satellites is becoming feasible. However, new constraints
arise, mainly when using raw, unprocessed sensor data instead of preprocessed
ground-based products. While current solutions primarily rely on preprocessed
sensor images, few approaches directly leverage raw data. This study
investigates the effects of utilising raw data on deep learning models for
object detection and classification tasks. We introduce a simulation workflow
to generate raw-like products from high-resolution L1 imagery, enabling
systemic evaluation. Two object detection models (YOLOv11s and YOLOX-S) are
trained on both raw and L1 datasets, and their performance is compared using
standard detection metrics and explainability tools. Results indicate that
while both models perform similarly at low to medium confidence thresholds, the
model trained on raw data struggles with object boundary identification at high
confidence levels. It suggests that adapting AI architectures with improved
contouring methods can enhance object detection on raw images, improving
onboard AI for remote sensing.

</details>


### [61] [Lung Infection Severity Prediction Using Transformers with Conditional TransMix Augmentation and Cross-Attention](https://arxiv.org/abs/2510.06887)
*Bouthaina Slika,Fadi Dornaika,Fares Bougourzi,Karim Hammoudi*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 QCross-Att-PVT 的新型 Transformer 模型，结合了并行编码器、交叉门控注意力机制和特征聚合器，用于从 CT 扫描和胸部 X 光片中评估肺部感染的严重程度。此外，还引入了 Conditional Online TransMix 数据增强策略来解决数据集不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 为了在肺部感染（尤其是肺炎）可能迅速恶化的情况下，特别是大流行期间，通过医学影像进行准确的 AI 严重程度预测，以支持及时的临床决策和优化患者治疗效果。

Method: 提出了一种名为 QCross-Att-PVT 的 Transformer 模型，该模型集成了并行编码器、交叉门控注意力机制和特征聚合器，以捕捉丰富的多尺度特征。同时，提出了一种名为 Conditional Online TransMix 的数据增强策略，用于生成混合标签图像块以解决数据集不平衡问题。

Result: 在 RALO CXR 和 Per-COVID-19 CT 两个基准数据集上进行了评估，所提出的方法在预测准确性和稳健性方面持续优于几种最先进的深度学习模型。

Conclusion: 研究强调了数据增强和门控注意力在提高肺部感染严重程度预测的稳健性和准确性方面起着至关重要的作用。所提出的方法为临床诊断、疾病监测和个性化治疗规划提供了一个可靠且可适应的工具。

Abstract: Lung infections, particularly pneumonia, pose serious health risks that can
escalate rapidly, especially during pandemics. Accurate AI-based severity
prediction from medical imaging is essential to support timely clinical
decisions and optimize patient outcomes. In this work, we present a novel
method applicable to both CT scans and chest X-rays for assessing lung
infection severity. Our contributions are twofold: (i) QCross-Att-PVT, a
Transformer-based architecture that integrates parallel encoders, a cross-gated
attention mechanism, and a feature aggregator to capture rich multi-scale
features; and (ii) Conditional Online TransMix, a custom data augmentation
strategy designed to address dataset imbalance by generating mixed-label image
patches during training. Evaluated on two benchmark datasets, RALO CXR and
Per-COVID-19 CT, our method consistently outperforms several state-of-the-art
deep learning models. The results emphasize the critical role of data
augmentation and gated attention in improving both robustness and predictive
accuracy. This approach offers a reliable, adaptable tool to support clinical
diagnosis, disease monitoring, and personalized treatment planning. The source
code of this work is available at https://github.com/bouthainas/QCross-Att-PVT.

</details>


### [62] [Label-frugal satellite image change detection with generative virtual exemplar learning](https://arxiv.org/abs/2510.06926)
*Hichem Sahbi*

Main category: cs.CV

TL;DR: 本研究提出了一种基于主动学习的遥感图像变化检测新算法，通过生成“虚拟样本”来优化标签标注过程，提高了模型的学习效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的变化检测方法，特别是深度学习方法，依赖于大量手工标注的训练数据，这些数据受到采集条件和用户主观性的影响。为了解决这个问题，本研究旨在提出一种更有效率的学习方法。

Method: 本研究提出了一种基于主动学习的新型变化检测算法。其核心贡献在于设计了一个新的模型，该模型能够衡量每个未标记样本的重要性，并只向用户提供最关键的“虚拟样本”进行标注。这些虚拟样本是通过一个可逆的图卷积网络生成的，作为对抗性损失的最优解，该损失衡量了数据的代表性、多样性和模糊性，从而挑战并改进了当前的变化检测标准，并在主动学习的后续迭代中更好地重新估计这些标准。

Result: 通过广泛的实验证明，本研究提出的标签高效学习模型相比于对比方法，取得了积极的成效。

Conclusion: 本研究提出的基于主动学习和虚拟样本生成的变化检测算法，能够显著提高模型在有限标注数据下的学习效率和性能，克服了传统方法对大量标注数据的依赖。

Abstract: Change detection is a major task in remote sensing which consists in finding
all the occurrences of changes in multi-temporal satellite or aerial images.
The success of existing methods, and particularly deep learning ones, is
tributary to the availability of hand-labeled training data that capture the
acquisition conditions and the subjectivity of the user (oracle). In this
paper, we devise a novel change detection algorithm, based on active learning.
The main contribution of our work resides in a new model that measures how
important is each unlabeled sample, and provides an oracle with only the most
critical samples (also referred to as virtual exemplars) for further labeling.
These exemplars are generated, using an invertible graph convnet, as the
optimum of an adversarial loss that (i) measures representativity, diversity
and ambiguity of the data, and thereby (ii) challenges (the most) the current
change detection criteria, leading to a better re-estimate of these criteria in
the subsequent iterations of active learning. Extensive experiments show the
positive impact of our label-efficient learning model against comparative
methods.

</details>


### [63] [IAR2: Improving Autoregressive Visual Generation with Semantic-Detail Associated Token Prediction](https://arxiv.org/abs/2510.06928)
*Ran Yi,Teng Hu,Zihan Su,Lizhuang Ma*

Main category: cs.CV

TL;DR: IAR2通过引入分层的语义-细节合成过程和新的双码本结构，改进了视觉内容创建的自回归模型，在ImageNet上实现了1.50的FID分数，同时提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归视觉内容创建模型忽略了视觉数据的内在结构特性，并且在克服预训练码本的僵化和硬聚类的不足方面存在局限性。

Method: 提出了一种名为IAR2的先进自回归框架，其核心是新颖的语义-细节关联双码本，将图像表示分解为语义码本和细节码本。它还引入了语义-细节自回归预测方案和局部上下文增强自回归头，以及用于条件生成的渐进式注意力引导自适应CFG机制。

Result: IAR2在ImageNet上达到了1.50的FID分数，创下了自回归图像生成的新最先进纪录，在性能和计算效率方面均优于先前的方法。

Conclusion: IAR2的结构化、粗到精的生成策略在视觉内容创建方面取得了卓越的性能和效率。

Abstract: Autoregressive models have emerged as a powerful paradigm for visual content
creation, but often overlook the intrinsic structural properties of visual
data. Our prior work, IAR, initiated a direction to address this by
reorganizing the visual codebook based on embedding similarity, thereby
improving generation robustness. However, it is constrained by the rigidity of
pre-trained codebooks and the inaccuracies of hard, uniform clustering. To
overcome these limitations, we propose IAR2, an advanced autoregressive
framework that enables a hierarchical semantic-detail synthesis process. At the
core of IAR2 is a novel Semantic-Detail Associated Dual Codebook, which
decouples image representations into a semantic codebook for global semantic
information and a detail codebook for fine-grained refinements. It expands the
quantization capacity from a linear to a polynomial scale, significantly
enhancing expressiveness. To accommodate this dual representation, we propose a
Semantic-Detail Autoregressive Prediction scheme coupled with a Local-Context
Enhanced Autoregressive Head, which performs hierarchical prediction-first the
semantic token, then the detail token-while leveraging a local context window
to enhance spatial coherence. Furthermore, for conditional generation, we
introduce a Progressive Attention-Guided Adaptive CFG mechanism that
dynamically modulates the guidance scale for each token based on its relevance
to the condition and its temporal position in the generation sequence,
improving conditional alignment without sacrificing realism. Extensive
experiments demonstrate that IAR2 sets a new state-of-the-art for
autoregressive image generation, achieving a FID of 1.50 on ImageNet. Our model
not only surpasses previous methods in performance but also demonstrates
superior computational efficiency, highlighting the effectiveness of our
structured, coarse-to-fine generation strategy.

</details>


### [64] [OBJVanish: Physically Realizable Text-to-3D Adv. Generation of LiDAR-Invisible Objects](https://arxiv.org/abs/2510.06952)
*Bing Li,Wuqi Wang,Yanan Zhang,Jingzheng Li,Haigen Min,Wei Feng,Xingyu Zhao,Jie Zhang,Qing Guo*

Main category: cs.CV

TL;DR: 现有的基于激光雷达的3D目标检测器在自动驾驶中至关重要，但它们容易受到对抗性攻击。本文提出了一种新颖的文本到3D对抗生成方法（Phy3DAdvGen），通过优化文本提示来生成物理上可实现的、对激光雷达检测器不可见的3D行人模型。


<details>
  <summary>Details</summary>
Motivation: 为了彻底测试3D目标检测系统并暴露其漏洞，需要有效的3D对抗性攻击。然而，现有的攻击方法难以实现物理环境中的完全物体消失，并且在现实世界中难以实施。

Method: 本文首先研究了影响检测漏洞的因素，通过操纵单个行人3D模型的拓扑、连通性和强度，以及在CARLA模拟环境中将行人与多个物体组合。在此基础上，提出了一种名为Phy3DAdvGen的方法，通过迭代地优化文本提示（动词、物体和姿势）来生成对激光雷达不可见的行人。为了确保物理上的可实现性，研究人员构建了一个包含13个真实物体3D模型的对象池，并限制Phy3DAdvGen基于该集合中的物体组合生成3D物体。

Result: 实验结果表明，所提出的方法可以在CARLA模拟和物理环境中生成能够逃避六种最先进（SOTA）的激光雷达3D检测器的3D行人。

Conclusion: Phy3DAdvGen是一种新颖的文本到3D对抗生成方法，能够生成物理上可实现的、对激光雷达检测器不可见的3D行人模型，揭示了安全关键应用中的漏洞。

Abstract: LiDAR-based 3D object detectors are fundamental to autonomous driving, where
failing to detect objects poses severe safety risks. Developing effective 3D
adversarial attacks is essential for thoroughly testing these detection systems
and exposing their vulnerabilities before real-world deployment. However,
existing adversarial attacks that add optimized perturbations to 3D points have
two critical limitations: they rarely cause complete object disappearance and
prove difficult to implement in physical environments. We introduce the
text-to-3D adversarial generation method, a novel approach enabling physically
realizable attacks that can generate 3D models of objects truly invisible to
LiDAR detectors and be easily realized in the real world. Specifically, we
present the first empirical study that systematically investigates the factors
influencing detection vulnerability by manipulating the topology, connectivity,
and intensity of individual pedestrian 3D models and combining pedestrians with
multiple objects within the CARLA simulation environment. Building on the
insights, we propose the physically-informed text-to-3D adversarial generation
(Phy3DAdvGen) that systematically optimizes text prompts by iteratively
refining verbs, objects, and poses to produce LiDAR-invisible pedestrians. To
ensure physical realizability, we construct a comprehensive object pool
containing 13 3D models of real objects and constrain Phy3DAdvGen to generate
3D objects based on combinations of objects in this set. Extensive experiments
demonstrate that our approach can generate 3D pedestrians that evade six
state-of-the-art (SOTA) LiDAR 3D detectors in both CARLA simulation and
physical environments, thereby highlighting vulnerabilities in safety-critical
applications.

</details>


### [65] [Generating Surface for Text-to-3D using 2D Gaussian Splatting](https://arxiv.org/abs/2510.06967)
*Huanning Dong,Fan Li,Ping Kuang,Jianwen Min*

Main category: cs.CV

TL;DR: DirectGaussian是一种新的文本到3D模型，使用2D高斯泼溅和法线/纹理先验来生成由surfel表示的3D对象表面，并引入曲率约束来解决多视角几何一致性问题。


<details>
  <summary>Details</summary>
Motivation: 自然界中物体的复杂几何形状使得3D内容的生成具有挑战性，而现有的文本到3D模型要么依赖2D扩散先验，要么直接在特定的3D表示上进行训练。

Method: DirectGaussian利用条件文本生成模型，并通过多视角法线和纹理先验，使用2D高斯泼溅来渲染3D对象的表面。在优化过程中加入曲率约束来解决多视角几何一致性问题。

Result: 通过大量实验证明，DirectGaussian能够生成多样化且高保真度的3D内容。

Conclusion: DirectGaussian是一种有效的方法，可以生成高质量的3D内容。

Abstract: Recent advancements in Text-to-3D modeling have shown significant potential
for the creation of 3D content. However, due to the complex geometric shapes of
objects in the natural world, generating 3D content remains a challenging task.
Current methods either leverage 2D diffusion priors to recover 3D geometry, or
train the model directly based on specific 3D representations. In this paper,
we propose a novel method named DirectGaussian, which focuses on generating the
surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize
conditional text generation models and the surface of a 3D object is rendered
by 2D Gaussian splatting with multi-view normal and texture priors. For
multi-view geometric consistency problems, DirectGaussian incorporates
curvature constraints on the generated surface during optimization process.
Through extensive experiments, we demonstrate that our framework is capable of
achieving diverse and high-fidelity 3D content creation.

</details>


### [66] [Learning Global Representation from Queries for Vectorized HD Map Construction](https://arxiv.org/abs/2510.06969)
*Shoumeng Qiu,Xinrun Li,Yang Long,Xiangyang Xue,Varun Ojha,Jian Pu*

Main category: cs.CV

TL;DR: MapGR通过引入全局表示学习和引导模块，利用查询的全局信息来改进高清地图的在线构建，并在nuScenes和Argoverse2数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于DETR的矢量高清地图构建方法主要依赖局部查询，忽略了高清地图本身固有的全局表示，限制了性能的进一步提升。

Method: 提出MapGR架构，包含两个模块：1. 全局表示学习（GRL）模块，通过整体分割任务使所有查询的分布与全局地图对齐；2. 全局表示引导（GRG）模块，为每个查询提供显式的全局上下文信息。

Result: 在nuScenes和Argoverse2数据集上进行了评估，MapGR相比现有领先方法在平均精度均值（mAP）方面取得了显著的提升。

Conclusion: MapGR通过学习和利用查询的全局表示，有效解决了现有方法过度依赖局部信息的问题，显著提升了高清地图的构建性能。

Abstract: The online construction of vectorized high-definition (HD) maps is a
cornerstone of modern autonomous driving systems. State-of-the-art approaches,
particularly those based on the DETR framework, formulate this as an instance
detection problem. However, their reliance on independent, learnable object
queries results in a predominantly local query perspective, neglecting the
inherent global representation within HD maps. In this work, we propose
\textbf{MapGR} (\textbf{G}lobal \textbf{R}epresentation learning for HD
\textbf{Map} construction), an architecture designed to learn and utilize a
global representations from queries. Our method introduces two synergistic
modules: a Global Representation Learning (GRL) module, which encourages the
distribution of all queries to better align with the global map through a
carefully designed holistic segmentation task, and a Global Representation
Guidance (GRG) module, which endows each individual query with explicit,
global-level contextual information to facilitate its optimization. Evaluations
on the nuScenes and Argoverse2 datasets validate the efficacy of our approach,
demonstrating substantial improvements in mean Average Precision (mAP) compared
to leading baselines.

</details>


### [67] [Addressing the ID-Matching Challenge in Long Video Captioning](https://arxiv.org/abs/2510.06973)
*Zhantao Yang,Huangji Wang,Ruili Feng,Han Zhang,Yuting Hu,Shangwen Zhu,Junyan Li,Yu Liu,Fan Cheng*

Main category: cs.CV

TL;DR: 本论文提出了一种名为RICE的新视频字幕生成方法，旨在解决长视频中个体身份匹配的难题，通过改进大型视觉语言模型（LVLM）的图像信息利用和个体描述信息量，显著提高了身份识别的精确率（从50%提升到90%）和召回率（从15%提升到80%），使得能够连续追踪长视频中的不同个体。


<details>
  <summary>Details</summary>
Motivation: 长视频字幕生成和多模态理解领域至关重要，但准确识别视频中不同帧的相同个体（ID-Matching问题）是一个关键且未被充分解决的挑战，现有方法泛化性有限且依赖于点状匹配。

Method: 提出了一种名为RICE（Recognizing Identities for Captioning Effectively）的新视频字幕生成方法。该方法利用大型视觉语言模型（LVLM）的强大先验知识，通过增强图像信息的使用和增加个体描述信息的数量来提升ID-Matching性能。此外，研究者还创建了一个新的基准来评估视频字幕的ID-Matching能力。

Result: 通过在GPT-4o上实现RICE，ID-Matching的精确率从50%提升到90%，召回率从15%提升到80%。实验证明了RICE在字幕质量和ID-Matching性能上的优越性，使其能够连续追踪长视频中的不同个体。

Conclusion: RICE方法通过改进LVLM在图像信息利用和个体描述信息量方面的能力，有效解决了长视频字幕生成中的ID-Matching问题，显著提高了识别准确率，为长视频内容理解提供了新的可能性。

Abstract: Generating captions for long and complex videos is both critical and
challenging, with significant implications for the growing fields of
text-to-video generation and multi-modal understanding. One key challenge in
long video captioning is accurately recognizing the same individuals who appear
in different frames, which we refer to as the ID-Matching problem. Few prior
works have focused on this important issue. Those that have, usually suffer
from limited generalization and depend on point-wise matching, which limits
their overall effectiveness. In this paper, unlike previous approaches, we
build upon LVLMs to leverage their powerful priors. We aim to unlock the
inherent ID-Matching capabilities within LVLMs themselves to enhance the
ID-Matching performance of captions. Specifically, we first introduce a new
benchmark for assessing the ID-Matching capabilities of video captions. Using
this benchmark, we investigate LVLMs containing GPT-4o, revealing key insights
that the performance of ID-Matching can be improved through two methods: 1)
enhancing the usage of image information and 2) increasing the quantity of
information of individual descriptions. Based on these insights, we propose a
novel video captioning method called Recognizing Identities for Captioning
Effectively (RICE). Extensive experiments including assessments of caption
quality and ID-Matching performance, demonstrate the superiority of our
approach. Notably, when implemented on GPT-4o, our RICE improves the precision
of ID-Matching from 50% to 90% and improves the recall of ID-Matching from 15%
to 80% compared to baseline. RICE makes it possible to continuously track
different individuals in the captions of long videos.

</details>


### [68] [No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts](https://arxiv.org/abs/2510.06988)
*Girolamo Macaluso,Lorenzo Mandelli,Mirko Bicchierai,Stefano Berretti,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: 无需运动捕捉数据，通过强化学习和文本提示微调预训练的人体运动生成扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在人体运动生成方面取得了显著进展，但将其应用于新动作或风格时，通常需要昂贵的额外动作捕捉数据和完全重新训练，难以扩展。

Method: 提出一种基于强化学习的后训练框架，仅使用文本提示微调预训练的运动扩散模型，无需任何运动真实数据。该方法使用预训练的文本-运动检索网络作为奖励信号，并通过去噪扩散策略优化（DDPO）来优化扩散策略。

Result: 在跨数据集适应和留一法运动实验中，使用 HumanML3D 和 KIT-ML 数据集，并在潜在空间和关节空间扩散架构上进行了评估。定量指标和用户研究结果表明，该方法在保持原始分布性能的同时，一致地提高了生成运动的质量和多样性。

Conclusion: 该方法是一种灵活、数据高效且能保护隐私的运动适应解决方案。

Abstract: Diffusion models have recently advanced human motion generation, producing
realistic and diverse animations from textual prompts. However, adapting these
models to unseen actions or styles typically requires additional motion capture
data and full retraining, which is costly and difficult to scale. We propose a
post-training framework based on Reinforcement Learning that fine-tunes
pretrained motion diffusion models using only textual prompts, without
requiring any motion ground truth. Our approach employs a pretrained
text-motion retrieval network as a reward signal and optimizes the diffusion
policy with Denoising Diffusion Policy Optimization, effectively shifting the
model's generative distribution toward the target domain without relying on
paired motion data. We evaluate our method on cross-dataset adaptation and
leave-one-out motion experiments using the HumanML3D and KIT-ML datasets across
both latent- and joint-space diffusion architectures. Results from quantitative
metrics and user studies show that our approach consistently improves the
quality and diversity of generated motions, while preserving performance on the
original distribution. Our approach is a flexible, data-efficient, and
privacy-preserving solution for motion adaptation.

</details>


### [69] [Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models](https://arxiv.org/abs/2510.07008)
*Gianmarco Perantoni,Giulio Weikmann,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出了一种结合深度学习和贝叶斯建模（HMMs与Transformer Encoder）的新方法，用于分类年度卫星图像时间序列（SITS），以捕捉时间相关性和特定作物类型序列模式。


<details>
  <summary>Details</summary>
Motivation: 年度土地覆盖图的时间一致性对于模拟土地覆盖的年际演变和变化至关重要。

Method: 将隐马尔可夫模型（HMMs）与基于Transformer Encoder（TE）的深度神经网络（DNNs）相结合，构建了一个分层分类器，其中HMM层构建在TE之上，以区分一致的年度作物类型序列。

Result: 在包含47种作物类型和六年的Sentinel-2图像的多年度作物类型分类数据集上进行了验证，结果表明了模拟时间一致性预测标签的重要性。HMMs提高了整体性能和F1分数。

Conclusion: 所提出的结合HMMs和TE的方法在年度SITS分类方面是有效的，并且通过显着提高性能和F1分数，证明了对时间一致性进行建模的重要性。

Abstract: The temporal consistency of yearly land-cover maps is of great importance to
model the evolution and change of the land cover over the years. In this paper,
we focus the attention on a novel approach to classification of yearly
satellite image time series (SITS) that combines deep learning with Bayesian
modelling, using Hidden Markov Models (HMMs) integrated with Transformer
Encoder (TE) based DNNs. The proposed approach aims to capture both i)
intricate temporal correlations in yearly SITS and ii) specific patterns in
multiyear crop type sequences. It leverages the cascade classification of an
HMM layer built on top of the TE, discerning consistent yearly crop-type
sequences. Validation on a multiyear crop type classification dataset spanning
47 crop types and six years of Sentinel-2 acquisitions demonstrates the
importance of modelling temporal consistency in the predicted labels. HMMs
enhance the overall performance and F1 scores, emphasising the effectiveness of
the proposed approach.

</details>


### [70] [U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking](https://arxiv.org/abs/2510.07041)
*Fenghe Tang,Chengqi Dong,Wenxin Ma,Zikang Xu,Heqin Zhu,Zihang Jiang,Rongsheng Wang,Yuhao Wang,Chenxu Wu,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: U-Bench是一个首次大规模、统计严谨的基准测试，用于评估100种U-Net变体在28个数据集和10种成像模态上的性能和效用，并引入了新的U-Score指标来衡量性能-效率权衡，同时提供了一个模型顾问来指导模型选择。


<details>
  <summary>Details</summary>
Motivation: 过去的U-Net在医学图像分割领域虽然广泛应用，但缺乏系统的性能评估基准，现有评估存在统计验证不足、效率和泛化性考虑有限等问题。

Method: U-Bench评估了100种U-Net变体在28个数据集和10种成像模态上的性能，从统计稳健性、零样本泛化能力和计算效率三个维度进行评估。引入了新的U-Score指标来综合考量性能和效率。同时，分析了数据集特性和模型架构对性能的影响，并开发了一个模型顾问来帮助研究人员选择合适的模型。

Result: U-Bench提供了一个全面的评估结果，揭示了现有评估的不足，并为U-Net模型在医学图像分割领域的公平、可复现和实用性相关的基准测试奠定了基础。其结果和分析可以指导研究人员选择最适合特定数据集和任务的模型。

Conclusion: U-Bench是第一个大规模、统计严谨的医学图像分割U-Net变体基准测试，它通过引入U-Score指标和模型顾问，不仅填补了现有评估的空白，还为未来的研究提供了重要的指导和可复现的基础。

Abstract: Over the past decade, U-Net has been the dominant architecture in medical
image segmentation, leading to the development of thousands of U-shaped
variants. Despite its widespread adoption, there is still no comprehensive
benchmark to systematically evaluate their performance and utility, largely
because of insufficient statistical validation and limited consideration of
efficiency and generalization across diverse datasets. To bridge this gap, we
present U-Bench, the first large-scale, statistically rigorous benchmark that
evaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our
contributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates
models along three key dimensions: statistical robustness, zero-shot
generalization, and computational efficiency. We introduce a novel metric,
U-Score, which jointly captures the performance-efficiency trade-off, offering
a deployment-oriented perspective on model progress. (2) Systematic Analysis
and Model Selection Guidance: We summarize key findings from the large-scale
evaluation and systematically analyze the impact of dataset characteristics and
architectural paradigms on model performance. Based on these insights, we
propose a model advisor agent to guide researchers in selecting the most
suitable models for specific datasets and tasks. (3) Public Availability: We
provide all code, models, protocols, and weights, enabling the community to
reproduce our results and extend the benchmark with future methods. In summary,
U-Bench not only exposes gaps in previous evaluations but also establishes a
foundation for fair, reproducible, and practically relevant benchmarking in the
next decade of U-Net-based segmentation models. The project can be accessed at:
https://fenghetan9.github.io/ubench. Code is available at:
https://github.com/FengheTan9/U-Bench.

</details>


### [71] [Concept Retrieval -- What and How?](https://arxiv.org/abs/2510.07058)
*Ori nizan,Oren Shrout,Ayellet Tal*

Main category: cs.CV

TL;DR: 提出一种新的图像检索方法，通过建模嵌入空间中的邻居关系来识别和检索共享相同中心概念的图像。


<details>
  <summary>Details</summary>
Motivation: 超越传统的视觉或语义相似性检索方法，旨在捕捉图像的中心概念和潜在叙事。

Method: 提出一种基于双峰高斯分布模型来分析嵌入空间邻居关系的新方法，以识别共享的概念。

Result: 通过定性、定量和人类评估，证明了所提出方法在图像概念识别和检索方面的有效性。

Conclusion: 所提出的基于双峰高斯分布模型的方法能够有效地识别和检索具有相同中心概念的图像，并在评估中取得了良好效果。

Abstract: A concept may reflect either a concrete or abstract idea. Given an input
image, this paper seeks to retrieve other images that share its central
concepts, capturing aspects of the underlying narrative. This goes beyond
conventional retrieval or clustering methods, which emphasize visual or
semantic similarity. We formally define the problem, outline key requirements,
and introduce appropriate evaluation metrics. We propose a novel approach
grounded in two key observations: (1) While each neighbor in the embedding
space typically shares at least one concept with the query, not all neighbors
necessarily share the same concept with one another. (2) Modeling this
neighborhood with a bimodal Gaussian distribution uncovers meaningful structure
that facilitates concept identification. Qualitative, quantitative, and human
evaluations confirm the effectiveness of our approach. See the package on PyPI:
https://pypi.org/project/coret/

</details>


### [72] [DADO: A Depth-Attention framework for Object Discovery](https://arxiv.org/abs/2510.07089)
*Federico Gonzalez,Estefania Talavera,Petia Radeva*

Main category: cs.CV

TL;DR: DADO是一种结合了注意力机制和深度模型的无监督对象发现模型，通过动态加权自适应地强调注意力或深度特征，在标准基准测试中超越了最先进的方法，无需微调。


<details>
  <summary>Details</summary>
Motivation: 无监督对象发现（在没有人类标注标签的情况下识别和定位图像中的对象）仍然是计算机视觉领域的一个重大挑战和日益增长的焦点。

Method: DADO（Depth-Attention self-supervised technique for Discovering unseen Objects）模型结合了注意力机制和深度模型来识别图像中的潜在对象。为了应对噪声注意力图或具有不同深度平面的复杂场景等挑战，DADO采用动态加权来根据每个图像的全局特性自适应地强调注意力或深度特征。

Result: 在标准基准测试中，DADO的性能优于最先进的方法，在对象发现的准确性和鲁棒性方面表现更佳，并且无需进行微调。

Conclusion: DADO模型在无监督对象发现任务中表现出色，其结合深度和注意力机制的创新方法以及动态加权的适应性使其在准确性和鲁棒性方面均超越了现有技术，且无需微调。

Abstract: Unsupervised object discovery, the task of identifying and localizing objects
in images without human-annotated labels, remains a significant challenge and a
growing focus in computer vision. In this work, we introduce a novel model,
DADO (Depth-Attention self-supervised technique for Discovering unseen
Objects), which combines an attention mechanism and a depth model to identify
potential objects in images. To address challenges such as noisy attention maps
or complex scenes with varying depth planes, DADO employs dynamic weighting to
adaptively emphasize attention or depth features based on the global
characteristics of each image. We evaluated DADO on standard benchmarks, where
it outperforms state-of-the-art methods in object discovery accuracy and
robustness without the need for fine-tuning.

</details>


### [73] [Enhancing Concept Localization in CLIP-based Concept Bottleneck Models](https://arxiv.org/abs/2510.07115)
*Rémi Kazmierczak,Steve Azzolin,Eloïse Berthier,Goran Frehse,Gianni Franchi*

Main category: cs.CV

TL;DR: CLIP概念提取易产生幻觉，影响CBMs的可解释性。CHILI技术通过解耦图像嵌入和定位像素来抑制概念幻觉，并生成更具可解释性的显著性解释。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的概念瓶颈模型（CBMs）在概念提取时存在“幻觉”问题，即错误判断图像中概念的存在与否，从而影响模型解释的准确性。

Method: 提出CHILI（Concept Hallucination Inhibition via Localized Interpretability）技术，该技术通过解耦图像嵌入和定位与目标概念对应的像素来解决CLIP的概念幻觉问题，并支持生成更具可解释性的显著性解释。

Result: CHILI技术成功缓解了CLIP的概念幻觉问题，提高了CBMs的解释性，并能生成更易于理解的显著性解释。

Conclusion: CHILI技术是一种有效的方法，可以提高基于CLIP的概念瓶颈模型的可解释性，解决概念提取中的幻觉问题。

Abstract: This paper addresses explainable AI (XAI) through the lens of Concept
Bottleneck Models (CBMs) that do not require explicit concept annotations,
relying instead on concepts extracted using CLIP in a zero-shot manner. We show
that CLIP, which is central in these techniques, is prone to concept
hallucination, incorrectly predicting the presence or absence of concepts
within an image in scenarios used in numerous CBMs, hence undermining the
faithfulness of explanations. To mitigate this issue, we introduce Concept
Hallucination Inhibition via Localized Interpretability (CHILI), a technique
that disentangles image embeddings and localizes pixels corresponding to target
concepts. Furthermore, our approach supports the generation of saliency-based
explanations that are more interpretable.

</details>


### [74] [MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency](https://arxiv.org/abs/2510.07119)
*Dongki Jung,Jaehoon Choi,Yonghan Lee,Sungmin Eum,Heesung Kwon,Dinesh Manocha*

Main category: cs.CV

TL;DR: MoRe是一种无需训练的单目几何精炼方法，通过图优化和局部平面近似来解决单目深度估计的尺度模糊问题，并提高3D重建和新视图合成的质量。


<details>
  <summary>Details</summary>
Motivation: 单目3D基础模型在3D视觉应用中具有吸引力，但存在跨视图一致性和尺度对齐的问题。需要一种方法来改进这些模型的效果。

Method: 提出了一种名为MoRe的训练免费方法，利用帧间特征匹配建立对应关系，并采用基于图的优化框架，通过局部平面近似和表面法线估计来解决尺度模糊问题。

Result: MoRe能够提高3D重建的质量，并增强新视图合成的能力，尤其是在稀疏视图渲染的情况下。

Conclusion: MoRe是一种有效的训练免费方法，可以改进单目3D基础模型的性能，解决尺度模糊问题，并提高3D重建和新视图合成的效果。

Abstract: Monocular 3D foundation models offer an extensible solution for perception
tasks, making them attractive for broader 3D vision applications. In this
paper, we propose MoRe, a training-free Monocular Geometry Refinement method
designed to improve cross-view consistency and achieve scale alignment. To
induce inter-frame relationships, our method employs feature matching between
frames to establish correspondences. Rather than applying simple least squares
optimization on these matched points, we formulate a graph-based optimization
framework that performs local planar approximation using the estimated 3D
points and surface normals estimated by monocular foundation models. This
formulation addresses the scale ambiguity inherent in monocular geometric
priors while preserving the underlying 3D structure. We further demonstrate
that MoRe not only enhances 3D reconstruction but also improves novel view
synthesis, particularly in sparse view rendering scenarios.

</details>


### [75] [Graph Conditioned Diffusion for Controllable Histopathology Image Generation](https://arxiv.org/abs/2510.07129)
*Sarah Cechnicka,Matthew Baugh,Weitong Zhang,Mischa Dombrowski,Zhe Li,Johannes C. Paetzold,Candice Roufosse,Bernhard Kainz*

Main category: cs.CV

TL;DR: DPMs在图像生成方面取得了显著进展，但缺乏可控性，尤其是在医学成像领域。我们提出了基于图的对象级别表示（Graph-Conditioned-Diffusion），为医学图像生成提供了精细控制。


<details>
  <summary>Details</summary>
Motivation: 现有的DPMs在缺乏语义结构和强先验的噪声潜在空间中操作，难以实现对生成内容的有意义控制，特别是在医学成像等敏感领域。

Method: 通过生成代表图像中主要结构及其特征和关系的图节点，并利用Transformer模块处理这些图表示，将它们通过文本条件机制整合到扩散模型中。

Result: 在真实的组织病理学案例中，我们生成的数据可以可靠地替代用于下游分割任务的标注患者数据。

Conclusion: 所提出的Graph-Conditioned-Diffusion方法能够为生成过程提供精细控制，并且生成的医学图像数据在分割任务中表现出有效性。

Abstract: Recent advances in Diffusion Probabilistic Models (DPMs) have set new
standards in high-quality image synthesis. Yet, controlled generation remains
challenging, particularly in sensitive areas such as medical imaging. Medical
images feature inherent structure such as consistent spatial arrangement, shape
or texture, all of which are critical for diagnosis. However, existing DPMs
operate in noisy latent spaces that lack semantic structure and strong priors,
making it difficult to ensure meaningful control over generated content. To
address this, we propose graph-based object-level representations for
Graph-Conditioned-Diffusion. Our approach generates graph nodes corresponding
to each major structure in the image, encapsulating their individual features
and relationships. These graph representations are processed by a transformer
module and integrated into a diffusion model via the text-conditioning
mechanism, enabling fine-grained control over generation. We evaluate this
approach using a real-world histopathology use case, demonstrating that our
generated data can reliably substitute for annotated patient data in downstream
segmentation tasks. The code is available here.

</details>


### [76] [Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models](https://arxiv.org/abs/2510.07135)
*Karim El Khoury,Maxime Zanella,Christophe De Vleeschouwer,Benoit Macq*

Main category: cs.CV

TL;DR: 尽管远程遥感视觉语言模型 (RSVLMs) 在零样本学习方面表现出色，但它们在少样本学习等低数据场景下的泛化能力仍未得到充分探索。本研究提出了首个用于评估 RSVLMs 少样本适应性方法的结构化基准，并在十个遥感场景分类数据集上进行了广泛的实验，使用了五种常用的少样本适应策略和三种具有不同骨干网络的先进 RSVLMs。结果表明，零样本性能相似的模型在少样本适应方面可能表现出截然不同的行为，某些 RSVLMs 比其他模型更容易适应少样本学习。性能的差异和现有方法中缺乏明确的领先者凸显了开发更鲁棒、针对遥感定制的少样本适应方法的需求。为了促进未来的研究，我们提供了一个可复现的基准框架和开源代码，以系统地评估 RSVLMs 在少样本条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 评估远程遥感视觉语言模型（RSVLMs）在数据稀疏场景（如少样本学习）下的泛化能力。

Method: 在十个遥感场景分类数据集上，对三种先进的 RSVLMs 应用五种常用的少样本适应策略，并进行全面的实验。

Result: 零样本性能相似的模型在少样本适应方面的表现差异显著，表明某些 RSVLMs 比其他模型更适合少样本学习。现有方法中没有明确的优胜者。

Conclusion: 现有的少样本适应方法在 RSVLMs 上表现出显著的性能差异，需要开发更鲁棒、更适合遥感任务的少样本适应方法。我们提供了一个可复现的基准框架和开源代码，以促进未来的研究。

Abstract: Remote Sensing Vision-Language Models (RSVLMs) have shown remarkable
potential thanks to large-scale pretraining, achieving strong zero-shot
performance on various tasks. However, their ability to generalize in low-data
regimes, such as few-shot learning, remains insufficiently explored. In this
work, we present the first structured benchmark for evaluating few-shot
adaptation methods on RSVLMs. We conduct comprehensive experiments across ten
remote sensing scene classification datasets, applying five widely used
few-shot adaptation strategies to three state-of-the-art RSVLMs with varying
backbones. Our findings reveal that models with similar zero-shot performance
can exhibit markedly different behavior under few-shot adaptation, with some
RSVLMs being inherently more amenable to such adaptation than others. The
variability of performance and the absence of a clear winner among existing
methods highlight the need for the development of more robust methods for
few-shot adaptation tailored to RS. To facilitate future research, we provide a
reproducible benchmarking framework and open-source code to systematically
evaluate RSVLMs under few-shot conditions. The source code is publicly
available on Github: https://github.com/elkhouryk/fewshot_RSVLMs

</details>


### [77] [Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods](https://arxiv.org/abs/2510.07143)
*Chenfei Liao,Wensong Wang,Zichen Wen,Xu Zheng,Yiyu Wang,Haocong He,Yuanhuiyi Lyu,Lutao Jiang,Xin Zou,Yuqian Fu,Bin Ren,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 简单图像下采样在视觉标记压缩任务上表现优于许多先进方法，现有基准测试存在噪声，我们提出了VTC-Bench评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有评估视觉标记压缩技术的方法（如准确率下降）存在任务不匹配问题，且现有基准测试噪声较大。

Method: 通过实验发现简单图像下采样在视觉标记压缩任务上表现优于先进方法，并提出将下采样作为数据过滤器以评估样本难度，最终设计了包含数据过滤机制的VTC-Bench评估框架。

Result: 图像下采样在多个常用基准测试中表现优于许多先进的视觉标记压缩方法，证实了现有基准测试的噪声问题，并提出了VTC-Bench。

Conclusion: 现有基准测试不适用于视觉标记压缩任务的评估，VTC-Bench通过引入数据过滤机制，为更公平、准确地评估压缩方法提供了解决方案。

Abstract: Recent endeavors to accelerate inference in Multimodal Large Language Models
(MLLMs) have primarily focused on visual token compression. The effectiveness
of these methods is typically assessed by measuring the accuracy drop on
established benchmarks, comparing model performance before and after
compression. However, these benchmarks are originally designed to assess the
perception and reasoning capabilities of MLLMs, rather than to evaluate
compression techniques. As a result, directly applying them to visual token
compression introduces a task mismatch. Strikingly, our investigation reveals
that simple image downsampling consistently outperforms many advanced
compression methods across multiple widely used benchmarks. Through extensive
experiments, we make the following observations: (i) Current benchmarks are
noisy for the visual token compression task. (ii) Down-sampling is able to
serve as a data filter to evaluate the difficulty of samples in the visual
token compression task. Motivated by these findings, we introduce VTC-Bench, an
evaluation framework that incorporates a data filtering mechanism to denoise
existing benchmarks, thereby enabling fairer and more accurate assessment of
visual token compression methods. All data and code are available at
https://github.com/Chenfei-Liao/VTC-Bench.

</details>


### [78] [MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis](https://arxiv.org/abs/2510.07190)
*Yihao Zhi,Chenghong Li,Hongjie Liao,Xihe Yang,Zhengwentai Sun,Jiahao Chang,Xiaodong Cun,Wensen Feng,Xiaoguang Han*

Main category: cs.CV

TL;DR: MV-Performer是一个用于从单眼全身捕捉生成同步新视角视频的框架，实现了360度视角合成，解决了现有方法在生成360度视角变化方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法主要集中在正视视角，难以实现360度视角变化，本研究旨在解决这一问题，特别是在以人为中心的应用场景中。

Method: MV-Performer利用MVHumanNet数据集，引入相机依赖的法线贴图作为条件信号，并通过融合参考视频、部分渲染和不同视角信息的多视角人体中心视频扩散模型来维持生成视频的同步性。此外，还提供了一个用于处理现实世界视频的推理过程，以减少单眼深度估计不完美引入的伪影。

Result: 在三个数据集上的大量实验证明了MV-Performer在以人为中心的4D新视角合成方面的有效性和鲁棒性，达到了最先进的水平。

Conclusion: MV-Performer成功实现了以人为中心的360度4D新视角合成，解决了现有方法的局限性，并在多个数据集上取得了优异的性能。

Abstract: Recent breakthroughs in video generation, powered by large-scale datasets and
diffusion techniques, have shown that video diffusion models can function as
implicit 4D novel view synthesizers. Nevertheless, current methods primarily
concentrate on redirecting camera trajectory within the front view while
struggling to generate 360-degree viewpoint changes. In this paper, we focus on
human-centric subdomain and present MV-Performer, an innovative framework for
creating synchronized novel view videos from monocular full-body captures. To
achieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset
and incorporate an informative condition signal. Specifically, we use the
camera-dependent normal maps rendered from oriented partial point clouds, which
effectively alleviate the ambiguity between seen and unseen observations. To
maintain synchronization in the generated videos, we propose a multi-view
human-centric video diffusion model that fuses information from the reference
video, partial rendering, and different viewpoints. Additionally, we provide a
robust inference procedure for in-the-wild video cases, which greatly mitigates
the artifacts induced by imperfect monocular depth estimation. Extensive
experiments on three datasets demonstrate our MV-Performer's state-of-the-art
effectiveness and robustness, setting a strong model for human-centric 4D novel
view synthesis.

</details>


### [79] [Resolution scaling governs DINOv3 transfer performance in chest radiograph classification](https://arxiv.org/abs/2510.07191)
*Soroosh Tayebi Arasteh,Mina Shaigan,Christiane Kuhl,Jakob Nikolas Kather,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: 在胸部X光片分析中，DINOv3在512x512分辨率下优于DINOv2和ImageNet初始化，特别是在成人数据集上。ConvNeXt-B骨干网络优于ViT-B/16。然而，在儿科数据集中未观察到显著差异。


<details>
  <summary>Details</summary>
Motivation: 评估Meta的DINOv3在胸部X光片分析中的价值，并与DINOv2和ImageNet初始化进行比较，以了解其在细粒度成像任务中的表现。

Method: 在七个胸部X光数据集（n>814,000）上，对ViT-B/16和ConvNeXt-B骨干网络，在224x224、512x512和1024x1024像素分辨率下，以及使用7B模型的冻结特征，对DINOv3、DINOv2和ImageNet初始化进行了基准测试，主要结果指标为平均AUROC。

Result: 在224x224分辨率下，DINOv3和DINOv2在成人数据集上表现相当。将分辨率提高到512x512时，DINOv3在所有成人数据集上均优于DINOv2和ImageNet初始化。儿科数据集则无显著差异。ConvNeXt-B在所有设置下均优于ViT-B/16。冻结的DINOv3-7B特征表现不如完全微调的骨干网络。512x512分辨率带来的性能提升在依赖边界和小的病灶检测上最为明显。

Conclusion: 对于胸部X光片分析，使用512x512分辨率和DINOv3初始化的ConvNeXt-B网络能提供最佳性能，而更高的分辨率（如1024x1024）收益甚微。研究结果支持在512x512分辨率下使用微调的、中等大小的骨干网络进行胸部X光片解读，尤其是在检测细微或边界病灶方面。

Abstract: Self-supervised learning (SSL) has advanced visual representation learning,
but its value in chest radiography, a high-volume imaging modality with
fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL
models through Gram-anchored self-distillation. Whether these design choices
improve transfer learning for chest radiography has not been systematically
tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across
seven datasets (n>814,000). Two representative backbones were evaluated:
ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and
1024x1024 pixels. We additionally assessed frozen features from a 7B model. The
primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2
achieved comparable performance on adult datasets. Increasing resolution to
512x512 yielded consistent improvements for DINOv3 over both DINOv2 and
ImageNet. In contrast, results in pediatric cohort showed no differences across
initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models
using frozen DINOv3-7B features underperformed relative to fully finetuned
86-89M-parameter backbones, highlighting the importance of domain adaptation.
Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains
were most evident for boundary-dependent and small focal abnormalities. In
chest radiography, higher input resolution is critical for leveraging the
benefits of modern self-supervised models. 512x512 pixels represent a practical
upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest
performance, while larger inputs offer minimal return on cost. Clinically,
these findings support use of finetuned, mid-sized backbones at 512x512 for
chest radiograph interpretation, with the greatest gains expected in detecting
subtle or boundary-centered lesions relevant to emergency and critical care
settings.

</details>


### [80] [EigenScore: OOD Detection using Covariance in Diffusion Models](https://arxiv.org/abs/2510.07206)
*Shirin Shoushtari,Yi Wang,Xiao Shi,M. Salman Asif,Ulugbek S. Kamilov*

Main category: cs.CV

TL;DR: EigenScore是一种利用扩散模型引起的后验协方差的特征值谱来进行OOD检测的新方法，在OOD检测方面取得了SOTA的性能，并在近OOD设置下表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: OOD检测对于安全敏感领域中机器学习系统的安全部署至关重要，而扩散模型在捕捉复杂数据分布方面表现出强大能力，因此探索其在OOD检测中的潜力具有重要意义。

Method: EigenScore方法利用扩散模型引起的后验协方差的特征值谱来进行OOD检测。通过分析后验协方差的迹和主导特征值，可以识别出分布偏移。为确保可行性，采用了一种无雅可比矩阵的子空间迭代方法，仅通过前向评估denoiser来估计主导特征值。

Result: EigenScore在OOD检测方面取得了SOTA性能，AUROC比现有最佳基线提高了5%。此外，在CIFAR-10 vs CIFAR-100等近OOD设置下，EigenScore仍然保持鲁棒性，而现有的基于扩散模型的方法在该设置下常常失效。

Conclusion: EigenScore通过利用扩散模型后验协方差的特征值谱，为OOD检测提供了一种有效且鲁棒的新方法，尤其在近OOD场景下表现突出。

Abstract: Out-of-distribution (OOD) detection is critical for the safe deployment of
machine learning systems in safety-sensitive domains. Diffusion models have
recently emerged as powerful generative models, capable of capturing complex
data distributions through iterative denoising. Building on this progress,
recent work has explored their potential for OOD detection. We propose
EigenScore, a new OOD detection method that leverages the eigenvalue spectrum
of the posterior covariance induced by a diffusion model. We argue that
posterior covariance provides a consistent signal of distribution shift,
leading to larger trace and leading eigenvalues on OOD inputs, yielding a clear
spectral signature. We further provide analysis explicitly linking posterior
covariance to distribution mismatch, establishing it as a reliable signal for
OOD detection. To ensure tractability, we adopt a Jacobian-free subspace
iteration method to estimate the leading eigenvalues using only forward
evaluations of the denoiser. Empirically, EigenScore achieves SOTA performance,
with up to 5% AUROC improvement over the best baseline. Notably, it remains
robust in near-OOD settings such as CIFAR-10 vs CIFAR-100, where existing
diffusion-based methods often fail.

</details>


### [81] [GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation](https://arxiv.org/abs/2510.07217)
*Wen Ye,Zhaocheng Liu,Yuwei Gui,Tingyu Yuan,Yunyue Su,Bowen Fang,Chaoyang Zhao,Qiang Liu,Liang Wang*

Main category: cs.CV

TL;DR: GenPilot是一种即插即用的多代理系统，通过在测试时直接优化输入文本来解决长而复杂的文本到图像生成提示的解释性问题，提高了图像与文本的一致性和结构连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像合成方法在处理复杂长提示时存在语义不一致和细节缺失的问题。现有解决方案如微调需要训练且模型特定，而先前的自动提示优化方法缺乏系统性错误分析和精炼策略，导致可靠性和有效性有限。测试时缩放方法则操作于固定提示和噪声/样本数量，可解释性和适应性受限。

Method: 提出了一种灵活高效的即插即用多代理系统GenPilot，该系统在测试时直接作用于输入文本，集成了错误分析、基于聚类的自适应探索、细粒度验证和记忆模块，用于迭代优化。该方法模型无关、可解释，并能有效处理长而复杂的提示。同时，总结了常见的错误模式和精炼策略。

Result: 在DPG-bench和Geneval上的实验显示，该方法在提高生成图像的文本-图像一致性和结构连贯性方面能力强大，最高分别提高了16.9%和5.7%。

Conclusion: 提出的测试时提示优化策略对于增强文本到图像生成效果非常有效。

Abstract: Text-to-image synthesis has made remarkable progress, yet accurately
interpreting complex and lengthy prompts remains challenging, often resulting
in semantic inconsistencies and missing details. Existing solutions, such as
fine-tuning, are model-specific and require training, while prior automatic
prompt optimization (APO) approaches typically lack systematic error analysis
and refinement strategies, resulting in limited reliability and effectiveness.
Meanwhile, test-time scaling methods operate on fixed prompts and on noise or
sample numbers, limiting their interpretability and adaptability. To solve
these, we introduce a flexible and efficient test-time prompt optimization
strategy that operates directly on the input text. We propose a plug-and-play
multi-agent system called GenPilot, integrating error analysis,
clustering-based adaptive exploration, fine-grained verification, and a memory
module for iterative optimization. Our approach is model-agnostic,
interpretable, and well-suited for handling long and complex prompts.
Simultaneously, we summarize the common patterns of errors and the refinement
strategy, offering more experience and encouraging further exploration.
Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%
demonstrate the strong capability of our methods in enhancing the text and
image consistency and structural coherence of generated images, revealing the
effectiveness of our test-time prompt optimization strategy. The code is
available at https://github.com/27yw/GenPilot.

</details>


### [82] [TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation](https://arxiv.org/abs/2510.07249)
*Jiaben Chen,Zixin Wang,Ailing Zeng,Yang Fu,Xueyang Yu,Siyuan Cen,Julian Tanke,Yihang Chen,Koichi Saito,Yuki Mitsufuji,Chuang Gan*

Main category: cs.CV

TL;DR: TalkCuts是一个包含16.4万个视频片段、总时长超500小时的高质量人类语音视频大型数据集，支持多镜头、多视角（近景、半身、全身），并提供文本描述、2D关键点和3D SMPL-X运动标注，涵盖超1万个身份。为展示数据集价值，提出Orator框架，一个LLM驱动的多模态生成框架，可合成连贯的长视频。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集多关注单镜头、静态视角，缺乏对多镜头、动态视角下人类语音视频生成的研究，TalkCuts旨在解决这一问题。

Method: TalkCuts数据集包含16.4万个视频片段，总时长超过500小时，具有多样化的拍摄视角（近景、半身、全身），并提供详细的文本描述、2D关键点和3D SMPL-X运动标注。基于TalkCuts，提出了Orator框架，一个LLM驱动的多模态生成框架，能够根据语言模型的指导，控制摄像机切换、说话人姿态和语音调制，并集成多模态视频生成模块来合成长视频。

Result: 在TalkCuts数据集上训练的模型，无论是在姿态引导还是音频驱动的设置下，都能显著提升生成的多镜头语音视频在电影感连贯性和视觉吸引力方面。

Conclusion: TalkCuts数据集为未来可控的多镜头语音视频生成以及更广泛的多模态学习研究奠定了坚实的基础。

Abstract: In this work, we present TalkCuts, a large-scale dataset designed to
facilitate the study of multi-shot human speech video generation. Unlike
existing datasets that focus on single-shot, static viewpoints, TalkCuts offers
164k clips totaling over 500 hours of high-quality human speech videos with
diverse camera shots, including close-up, half-body, and full-body views. The
dataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X
motion annotations, covering over 10k identities, enabling multimodal learning
and evaluation. As a first attempt to showcase the value of the dataset, we
present Orator, an LLM-guided multi-modal generation framework as a simple
baseline, where the language model functions as a multi-faceted director,
orchestrating detailed specifications for camera transitions, speaker
gesticulations, and vocal modulation. This architecture enables the synthesis
of coherent long-form videos through our integrated multi-modal video
generation module. Extensive experiments in both pose-guided and audio-driven
settings show that training on TalkCuts significantly enhances the
cinematographic coherence and visual appeal of generated multi-shot speech
videos. We believe TalkCuts provides a strong foundation for future work in
controllable, multi-shot speech video generation and broader multimodal
learning.

</details>


### [83] [Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema Detection](https://arxiv.org/abs/2510.07277)
*Franco Javier Arellano,José Ignacio Orlando*

Main category: cs.CV

TL;DR: 基础模型（FM）在糖尿病黄斑水肿（DME）检测任务中并不总是优于微调的卷积神经网络（CNN）。在数据稀疏的环境下，高效的CNN仍然是强大的基线。


<details>
  <summary>Details</summary>
Motivation: 评估基础模型（FM）在糖尿病黄斑水肿（DME）检测任务中的有效性，并与传统的迁移学习方法进行比较，以确定FM是否适合细粒度的眼科任务。

Method: 在IDRiD、MESSIDOR-2和OEFI数据集上，比较了RETFound、FLAIR（两种流行的眼科FM）和EfficientNet-B0（标准CNN）在不同训练和评估设置下的性能。

Result: EfficientNet-B0在大多数评估设置下表现最佳或接近最佳，而RETFound仅在OEFI数据集上表现出潜力。FLAIR在零样本设置下表现出竞争力，尤其是在适当的提示下。FM在DME检测任务中并不总是优于微调的CNN。

Conclusion: 基础模型（FM）可能不适合像DME检测这样的细粒度眼科任务，即使经过微调。在数据稀疏的情况下，轻量级CNN仍然是有效的基线模型。

Abstract: Diabetic Macular Edema (DME) is a leading cause of vision loss among patients
with Diabetic Retinopathy (DR). While deep learning has shown promising results
for automatically detecting this condition from fundus images, its application
remains challenging due the limited availability of annotated data. Foundation
Models (FM) have emerged as an alternative solution. However, it is unclear if
they can cope with DME detection in particular. In this paper, we
systematically compare different FM and standard transfer learning approaches
for this task. Specifically, we compare the two most popular FM for retinal
images--RETFound and FLAIR--and an EfficientNet-B0 backbone, across different
training regimes and evaluation settings in IDRiD, MESSIDOR-2 and
OCT-and-Eye-Fundus-Images (OEFI). Results show that despite their scale, FM do
not consistently outperform fine-tuned CNNs in this task. In particular, an
EfficientNet-B0 ranked first or second in terms of area under the ROC and
precision/recall curves in most evaluation settings, with RETFound only showing
promising results in OEFI. FLAIR, on the other hand, demonstrated competitive
zero-shot performance, achieving notable AUC-PR scores when prompted
appropriately. These findings reveal that FM might not be a good tool for
fine-grained ophthalmic tasks such as DME detection even after fine-tuning,
suggesting that lightweight CNNs remain strong baselines in data-scarce
environments.

</details>


### [84] [SpecGuard: Spectral Projection-based Advanced Invisible Watermarking](https://arxiv.org/abs/2510.07302)
*Inzamamul Alam,Md Tanvir Islam,Khan Muhammad,Simon S. Woo*

Main category: cs.CV

TL;DR: SpecGuard是一种新型的图像水印方法，通过在频域的隐藏卷积层中嵌入水印，提高了水印的鲁棒性和不可见性，有效抵抗了各种失真、再生和对抗性扰动攻击。


<details>
  <summary>Details</summary>
Motivation: 现有方法在抵抗图像失真、再生和对抗性扰动等实际攻击方面鲁棒性不足，难以满足真实世界应用的需求。

Method: 将图像从空间域转换到频域，在经过小波变换分解的高频段进行频谱投影，并将消息嵌入到隐藏的卷积层中。通过设置强度因子来增强水印对各种攻击的抵抗能力。解码器利用Parseval定理来提取水印。

Result: SpecGuard在不可见性、容量和鲁棒性方面表现优异，实验证明其性能优于现有最先进的模型。

Conclusion: SpecGuard在鲁棒性和不可见性方面取得了显著的改进，为图像水印的实际应用提供了有效的解决方案。

Abstract: Watermarking embeds imperceptible patterns into images for authenticity
verification. However, existing methods often lack robustness against various
transformations primarily including distortions, image regeneration, and
adversarial perturbation, creating real-world challenges. In this work, we
introduce SpecGuard, a novel watermarking approach for robust and invisible
image watermarking. Unlike prior approaches, we embed the message inside hidden
convolution layers by converting from the spatial domain to the frequency
domain using spectral projection of a higher frequency band that is decomposed
by wavelet projection. Spectral projection employs Fast Fourier Transform
approximation to transform spatial data into the frequency domain efficiently.
In the encoding phase, a strength factor enhances resilience against diverse
attacks, including adversarial, geometric, and regeneration-based distortions,
ensuring the preservation of copyrighted information. Meanwhile, the decoder
leverages Parseval's theorem to effectively learn and extract the watermark
pattern, enabling accurate retrieval under challenging transformations. We
evaluate the proposed SpecGuard based on the embedded watermark's invisibility,
capacity, and robustness. Comprehensive experiments demonstrate the proposed
SpecGuard outperforms the state-of-the-art models. To ensure reproducibility,
the full code is released on
\href{https://github.com/inzamamulDU/SpecGuard_ICCV_2025}{\textcolor{blue}{\textbf{GitHub}}}.

</details>


### [85] [MATRIX: Mask Track Alignment for Interaction-aware Video Generation](https://arxiv.org/abs/2510.07310)
*Siyoon Jin,Seongchan Kim,Dahyun Chung,Jaeho Lee,Hyunwook Choi,Jisu Nam,Jiyoung Kim,Seungryong Kim*

Main category: cs.CV

TL;DR: 视频DiTs在多实例或主语-宾语交互方面存在不足。本研究提出了MATRIX-11K数据集和MATRIX正则化方法，以增强视频DiTs的交互建模能力，并通过InterGenEval进行评估。


<details>
  <summary>Details</summary>
Motivation: 视频DiTs在建模多实例或主语-宾语交互方面存在不足，本研究旨在探究其内部表征方式并提出改进方法。

Method: 构建了包含交互感知的字幕和多实例掩码跟踪的MATRIX-11K数据集。通过分析视频DiTs的语义基础（视频-文本注意力）和语义传播（视频-视频注意力），发现交互效应集中在少数交互主导层。在此基础上，提出名为MATRIX的正则化方法，将视频DiTs的特定层注意力与MATRIX-11K数据集中的多实例掩码跟踪对齐，以增强交互和传播。此外，还提出了用于交互感知视频生成的InterGenEval评估协议。

Result: MATRIX正则化方法在实验中提高了交互保真度和语义对齐度，同时减少了漂移和幻觉。消融实验验证了设计选择的有效性。

Conclusion: 所提出的MATRIX-11K数据集、MATRIX正则化方法和InterGenEval评估协议能够有效提升视频DiTs在交互建模方面的能力。

Abstract: Video DiTs have advanced video generation, yet they still struggle to model
multi-instance or subject-object interactions. This raises a key question: How
do these models internally represent interactions? To answer this, we curate
MATRIX-11K, a video dataset with interaction-aware captions and multi-instance
mask tracks. Using this dataset, we conduct a systematic analysis that
formalizes two perspectives of video DiTs: semantic grounding, via
video-to-text attention, which evaluates whether noun and verb tokens capture
instances and their relations; and semantic propagation, via video-to-video
attention, which assesses whether instance bindings persist across frames. We
find both effects concentrate in a small subset of interaction-dominant layers.
Motivated by this, we introduce MATRIX, a simple and effective regularization
that aligns attention in specific layers of video DiTs with multi-instance mask
tracks from the MATRIX-11K dataset, enhancing both grounding and propagation.
We further propose InterGenEval, an evaluation protocol for interaction-aware
video generation. In experiments, MATRIX improves both interaction fidelity and
semantic alignment while reducing drift and hallucination. Extensive ablations
validate our design choices. Codes and weights will be released.

</details>


### [86] [Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers](https://arxiv.org/abs/2510.07316)
*Gangwei Xu,Haotong Lin,Hongcheng Luo,Xianqi Wang,Jingfeng Yao,Lianghui Zhu,Yuechuan Pu,Cheng Chi,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Sida Peng,Xin Yang*

Main category: cs.CV

TL;DR: Pixel-Perfect Depth 使用像素级扩散生成模型，在不引入“飞点”的情况下，从单目估计的深度图中生成高质量点云。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式深度估计模型虽然性能优越，但由于使用了 VAE 压缩深度图至潜在空间，不可避免地会在边缘和细节处引入“飞点”伪影。本模型旨在解决此问题。

Method: 本模型直接在像素空间进行扩散生成，避免了 VAE 引入的伪影。为克服像素空间生成的高复杂度，模型采用了两种新设计：1) 语义引导的扩散 Transformer (SP-DiT)，将视觉基础模型的语义信息融入 DiT 以提示扩散过程，从而保持全局语义一致性并增强细节；2) 级联 DiT 设计，逐步增加 Ttoken 数量以提高效率和准确性。

Result: 在五个基准测试中，本模型在所有已发布的生成模型中取得了最佳性能，并在边缘感知点云评估中显著优于其他所有模型。

Conclusion: Pixel-Perfect Depth 通过在像素空间直接进行扩散生成，并结合 SP-DiT 和级联 DiT 设计，成功解决了现有方法的“飞点”问题，并在深度估计和点云生成任务上取得了领先的性能。

Abstract: This paper presents Pixel-Perfect Depth, a monocular depth estimation model
based on pixel-space diffusion generation that produces high-quality,
flying-pixel-free point clouds from estimated depth maps. Current generative
depth estimation models fine-tune Stable Diffusion and achieve impressive
performance. However, they require a VAE to compress depth maps into latent
space, which inevitably introduces \textit{flying pixels} at edges and details.
Our model addresses this challenge by directly performing diffusion generation
in the pixel space, avoiding VAE-induced artifacts. To overcome the high
complexity associated with pixel-space generation, we introduce two novel
designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which
incorporate semantic representations from vision foundation models into DiT to
prompt the diffusion process, thereby preserving global semantic consistency
while enhancing fine-grained visual details; and 2) Cascade DiT Design that
progressively increases the number of tokens to further enhance efficiency and
accuracy. Our model achieves the best performance among all published
generative models across five benchmarks, and significantly outperforms all
other models in edge-aware point cloud evaluation.

</details>


### [87] [Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms](https://arxiv.org/abs/2510.07317)
*Natacha Kuete Meli,Shuteng Wang,Marcel Seelbach Benkner,Michele Sasdelli,Tat-Jun Chin,Tolga Birdal,Michael Moeller,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 该论文对量子增强计算机视觉（QeCV）进行了全面回顾，重点介绍了其在计算机视觉领域利用量子计算的潜力，包括参数化量子电路和量子退火等方法，并讨论了相关工具、挑战和未来影响。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在为计算机视觉社区提供一个量子增强计算机视觉（QeCV）的参考，介绍该新兴交叉领域，并阐述其利用量子计算处理和解释视觉信号的潜力，特别是在现有经典方法无法在合理时间内找到解决方案或只能获得近似解的情况下。

Method: 本研究对QeCV进行了全面的回顾，介绍了其特定方法，并阐述了与量子硬件兼容的公式。它利用了门基量子计算和量子退火这两种主要的量子计算范式，并详细介绍了量子计算机的操作原理及其在QeCV中的应用工具、编程和模拟。

Result: 论文回顾了现有的QeCV工具和学习材料，并讨论了发表和评审QeCV论文的相关事宜、开放性挑战以及潜在的社会影响。

Conclusion: 虽然QeCV具有巨大的潜力，但仍需要开发专门的、全新的算法来充分发挥量子计算范式在计算机视觉领域的优势。

Abstract: Quantum-enhanced Computer Vision (QeCV) is a new research field at the
intersection of computer vision, optimisation theory, machine learning and
quantum computing. It has high potential to transform how visual signals are
processed and interpreted with the help of quantum computing that leverages
quantum-mechanical effects in computations inaccessible to classical (i.e.
non-quantum) computers. In scenarios where existing non-quantum methods cannot
find a solution in a reasonable time or compute only approximate solutions,
quantum computers can provide, among others, advantages in terms of better time
scalability for multiple problem classes. Parametrised quantum circuits can
also become, in the long term, a considerable alternative to classical neural
networks in computer vision. However, specialised and fundamentally new
algorithms must be developed to enable compatibility with quantum hardware and
unveil the potential of quantum computational paradigms in computer vision.
This survey contributes to the existing literature on QeCV with a holistic
review of this research field. It is designed as a quantum computing reference
for the computer vision community, targeting computer vision students,
scientists and readers with related backgrounds who want to familiarise
themselves with QeCV. We provide a comprehensive introduction to QeCV, its
specifics, and methodologies for formulations compatible with quantum hardware
and QeCV methods, leveraging two main quantum computational paradigms, i.e.
gate-based quantum computing and quantum annealing. We elaborate on the
operational principles of quantum computers and the available tools to access,
program and simulate them in the context of QeCV. Finally, we review existing
quantum computing tools and learning materials and discuss aspects related to
publishing and reviewing QeCV papers, open challenges and potential social
implications.

</details>


### [88] [Temporal Prompting Matters: Rethinking Referring Video Object Segmentation](https://arxiv.org/abs/2510.07319)
*Ci-Siang Lin,Min-Hung Chen,I-Jieh Liu,Chien-Yi Wang,Sifei Liu,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 本篇论文提出了一种名为Tenet的框架，用于解决指代表观看物体分割（RVOS）任务，该任务旨在分割视频中由查询语句指定的物体。现有方法通常需要端到端训练和密集的掩码注释，计算成本高且扩展性差。Tenet框架将RVOS任务分解为指代、视频和分割三个因素，其中指代和视频因素由Tenet解决，分割问题则交由现有的基础分割模型处理。为了高效地将基于图像的基础分割模型适配到RVOS任务，Tenet利用现成的物体检测器和跟踪器生成与指代语句相关的时序提示（temporal prompts）。为了解决这些提示的质量难以通过置信度分数区分的问题，论文提出了提示偏好学习（Prompt Preference Learning）来评估时序提示的质量。通过使用这些提示来指导基于图像的基础分割模型，可以生成高质量的物体掩码，从而实现对RVOS任务的高效模型适配。


<details>
  <summary>Details</summary>
Motivation: 现有 Referring Video Object Segmentation (RVOS) 方法需要端到端训练和密集的掩码注释，计算成本高且扩展性差。

Method: 提出了一种名为Tenet的框架，将RVOS任务分解为指代、视频和分割三个因素。利用物体检测器和跟踪器生成时序提示，并使用提示偏好学习来评估和选择高质量的提示，以指导基础分割模型进行分割。

Result: 在RVOS基准测试上，Tenet框架展现了其有效性，能够生成高质量的掩码并实现高效的模型适配。

Conclusion: Tenet框架通过将RVOS任务分解并利用时序提示和提示偏好学习，能够有效地解决现有方法的局限性，并实现对基础分割模型的有效适配。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment the object
referred to by the query sentence in the video. Most existing methods require
end-to-end training with dense mask annotations, which could be
computation-consuming and less scalable. In this work, we rethink the RVOS
problem and aim to investigate the key to this task. Based on existing
foundation segmentation models, we decompose the RVOS task into referring,
video, and segmentation factors, and propose a Temporal Prompt Generation and
Selection (Tenet) framework to address the referring and video factors while
leaving the segmentation problem to foundation models. To efficiently adapt
image-based foundation segmentation models to referring video object
segmentation, we leverage off-the-shelf object detectors and trackers to
produce temporal prompts associated with the referring sentence. While
high-quality temporal prompts could be produced, they can not be easily
identified from confidence scores. To tackle this issue, we propose Prompt
Preference Learning to evaluate the quality of the produced temporal prompts.
By taking such prompts to instruct image-based foundation segmentation models,
we would be able to produce high-quality masks for the referred object,
enabling efficient model adaptation to referring video object segmentation.
Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet
framework.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [89] [OpenStaxQA: A multilingual dataset based on open-source college textbooks](https://arxiv.org/abs/2510.06239)
*Pranav Gupta*

Main category: cs.CL

TL;DR: OpenStaxQA是一个基于43本开源大学教科书的教育评估基准，用于评估和微调LLM，并对其他任务的性能进行了零样本评估。


<details>
  <summary>Details</summary>
Motivation: 构建一个针对大学教育应用的、多语言的、基于开放教科书的评估基准，并评估LLM在该基准上的表现，以及其对其他任务的潜在改进作用。

Method: 使用43本开源大学教科书（涵盖英语、西班牙语和波兰语）创建OpenStaxQA评估基准。在OpenStaxQA数据集上，使用QLoRA技术对大约70亿参数的大型语言模型（LLMs）进行微调和评估。此外，还在AI2推理挑战开发数据集上进行了零样本评估，以检验OpenStaxQA对其他任务的改进潜力。讨论了类似OpenStaxQA的数据集可能产生的更广泛影响。

Result: 评估了在OpenStaxQA数据集上使用QLoRA微调的LLM的表现。通过在AI2推理挑战数据集上进行零样本评估，初步探究了OpenStaxQA对其他任务的潜在性能提升作用。

Conclusion: OpenStaxQA为评估大学教育应用的LLM提供了一个有价值的基准，并可能通过其训练促进跨任务的性能提升。此外，该研究还强调了构建此类数据集的更广泛影响。

Abstract: We present OpenStaxQA, an evaluation benchmark specific to college-level
educational applications based on 43 open-source college textbooks in English,
Spanish, and Polish, available under a permissive Creative Commons license. We
finetune and evaluate large language models (LLMs) with approximately 7 billion
parameters on this dataset using quantized low rank adapters (QLoRa).
Additionally we also perform a zero-shot evaluation on the AI2 reasoning
challenge dev dataset in order to check if OpenStaxQA can lead to an improved
performance on other tasks. We also discuss broader impacts relevant to
datasets such as OpenStaxQA.

</details>


### [90] [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
*Jiqun Pan,Zhenke Duan,Jiani Tu,Anzhi Cheng,Yanqing Wang*

Main category: cs.CL

TL;DR: 使用知识图谱指导的多智能体系统蒸馏（KG-MASD）解决了工业问答系统中多智能体大语言模型的迭代不可控和输出不可验证问题，通过将蒸馏过程公式化为马尔可夫决策过程，并引入知识图谱作为结构化先验，提高了数据安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 工业问答系统在诸如设备故障诊断等高风险场景中，其错误可能导致严重后果，因此对安全性和可靠性的要求远高于通用对话模型。现有的多智能体大语言模型虽然增强了推理深度，但存在迭代不可控和输出不可验证的问题，而传统的蒸馏方法难以将协同推理能力迁移到轻量级、可部署的学生模型。

Method: 提出了一种名为知识图谱指导的多智能体系统蒸馏（KG-MASD）的方法。该方法将蒸馏过程构建为一个马尔可夫决策过程，并结合知识图谱作为可验证的结构化先验来丰富状态表示并确保收敛。通过整合协同推理与知识接地，KG-MASD 生成了高置信度的指令调优数据，并将推理深度和可验证性共同蒸馏到适用于边缘部署的紧凑型学生模型中。

Result: 在工业问答数据集上的实验表明，KG-MASD 相较于基线方法，准确率提升了 2.4% 至 20.1%，并显著增强了可靠性。

Conclusion: KG-MASD 能够有效解决工业问答系统中多智能体大语言模型的挑战，通过知识图谱的引入提高了系统的可验证性和安全性，使得在安全关键的工业场景中部署可信赖的 AI 成为可能。

Abstract: Industrial question-answering (QA) systems require higher safety and
reliability than general-purpose dialogue models, as errors in high-risk
scenarios such as equipment fault diagnosis can have severe consequences.
Although multi-agent large language models enhance reasoning depth, they suffer
from uncontrolled iterations and unverifiable outputs, and conventional
distillation methods struggle to transfer collaborative reasoning capabilities
to lightweight, deployable student models. To address these challenges, we
propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our
approach formulates distillation as a Markov Decision Process and incorporates
a knowledge graph as a verifiable structured prior to enrich state
representation and ensure convergence. By integrating collaborative reasoning
with knowledge grounding, KG-MASD generates high-confidence instruction-tuning
data and jointly distills reasoning depth and verifiability into compact
student models suitable for edge deployment. Experiments on an industrial QA
dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent
over baselines and significantly enhances reliability, enabling trustworthy AI
deployment in safety-critical industrial scenarios. Code and data are available
at https://github.com/erwinmsmith/KG-MAD/.

</details>


### [91] [Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses](https://arxiv.org/abs/2510.06242)
*Subin An,Yugyeong Ji,Junyoung Kim,Heejin Kook,Yang Lu,Josh Seltzer*

Main category: cs.CL

TL;DR: 本研究提出了一种两阶段的评估框架，用于自动评估人类填写的调查问卷的质量。


<details>
  <summary>Details</summary>
Motivation: 开放式调查问卷回复提供了有价值的市场研究见解，但低质量的回复会增加研究人员手动筛选的负担，并可能导致误导性的结论，因此需要有效的评估方法。现有方法主要针对LLM生成的文本，未能充分评估人类编写的回复的独特特征。

Method: 首先，通过乱码过滤去除无意义的回复。然后，利用LLM能力评估三个维度——努力程度、相关性和完整性，这些维度基于对真实调查数据的实证分析。

Result: 在英语和韩语数据集上的验证表明，该框架的表现优于现有指标，并且在实际应用（如回复质量预测和回复拒绝）方面具有很高的实用性，与专家评估高度相关。

Conclusion: 本研究提出的两阶段评估框架能够有效评估人类调查回复的质量，并优于现有方法，具有实际应用价值。

Abstract: Open-ended survey responses provide valuable insights in marketing research,
but low-quality responses not only burden researchers with manual filtering but
also risk leading to misleading conclusions, underscoring the need for
effective evaluation. Existing automatic evaluation methods target
LLM-generated text and inadequately assess human-written responses with their
distinct characteristics. To address such characteristics, we propose a
two-stage evaluation framework specifically designed for human survey
responses. First, gibberish filtering removes nonsensical responses. Then,
three dimensions-effort, relevance, and completeness-are evaluated using LLM
capabilities, grounded in empirical analysis of real-world survey data.
Validation on English and Korean datasets shows that our framework not only
outperforms existing metrics but also demonstrates high practical applicability
for real-world applications such as response quality prediction and response
rejection, showing strong correlations with expert assessment.

</details>


### [92] [The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law](https://arxiv.org/abs/2510.06559)
*Cheonkam Jeong,Sungdo Kim,Jewoo Park*

Main category: cs.CL

TL;DR: 幻觉、不稳定的审核和不透明的合规性是由于缺少类型论语义，而不是数据或规模限制。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型存在误处理输出所蕴含的意义类型的问，需要解决幻觉、不稳定的审核和不透明的合规性问题。

Method: 提出Savassan，一种神经符号架构，将话语编译成蒙太古风格的逻辑形式，并将其映射到扩展了指示运算符和司法管辖区上下文的类型本体。神经网络组件从非结构化输入中提取候选结构；符号组件执行类型检查、约束推理和跨司法管辖区映射，以产生具有合规意识的指导，而不是二元审查。

Result: Savassan能够处理跨国界场景，将输入解析一次，并将其结果投影到多个法律本体中，将结果组合成一个单一的、可解释的决策。

Conclusion: 可信的自主性需要意义的组合类型化，使系统能够在统一的意义代数中推理所描述的内容、所规定的内容以及所承担的责任。

Abstract: Contemporary language models are fluent yet routinely mis-handle the types of
meaning their outputs entail. We argue that hallucination, brittle moderation,
and opaque compliance outcomes are symptoms of missing type-theoretic semantics
rather than data or scale limitations. Building on Montague's view of language
as typed, compositional algebra, we recast alignment as a parsing problem:
natural-language inputs must be compiled into structures that make explicit
their descriptive, normative, and legal dimensions under context.
  We present Savassan, a neuro-symbolic architecture that compiles utterances
into Montague-style logical forms and maps them to typed ontologies extended
with deontic operators and jurisdictional contexts. Neural components extract
candidate structures from unstructured inputs; symbolic components perform type
checking, constraint reasoning, and cross-jurisdiction mapping to produce
compliance-aware guidance rather than binary censorship. In cross-border
scenarios, the system "parses once" (e.g., defect claim(product x, company y))
and projects the result into multiple legal ontologies (e.g., defamation risk
in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into
a single, explainable decision.
  This paper contributes: (i) a diagnosis of hallucination as a type error;
(ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii)
a production-oriented design that embeds typed interfaces across the pipeline.
We outline an evaluation plan using legal reasoning benchmarks and synthetic
multi-jurisdiction suites. Our position is that trustworthy autonomy requires
compositional typing of meaning, enabling systems to reason about what is
described, what is prescribed, and what incurs liability within a unified
algebra of meaning.

</details>


### [93] [FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline](https://arxiv.org/abs/2510.06800)
*Haotian Wu,Shufan Jiang,Chios Chen,Yiyang Feng,Hehai Lin,Heqing Zou,Yao Shu,Yanran Li,Chengwei Qin*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As large language models (LLMs) advance in role-playing (RP) tasks, existing
benchmarks quickly become obsolete due to their narrow scope, outdated
interaction paradigms, and limited adaptability across diverse application
scenarios. To address this gap, we introduce FURINA-Builder, a novel
multi-agent collaboration pipeline that automatically constructs fully
customizable RP benchmarks at any scale. It enables evaluation of arbitrary
characters across diverse scenarios and prompt formats, as the first benchmark
builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues
between a test character and other characters drawn from a well-constructed
character-scene pool, while an LLM judge selects fine-grained evaluation
dimensions and adjusts the test character's responses into final test
utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive
role-playing benchmark featuring both established and synthesized test
characters, each assessed with dimension-specific evaluation criteria. Human
evaluation and preliminary separability analysis justify our pipeline and
benchmark design. We conduct extensive evaluations of cutting-edge LLMs and
find that o3 and DeepSeek-R1 achieve the best performance on English and
Chinese RP tasks, respectively. Across all models, established characters
consistently outperform synthesized ones, with reasoning capabilities further
amplifying this disparity. Interestingly, we observe that model scale does not
monotonically reduce hallucinations. More critically, for reasoning LLMs, we
uncover a novel trade-off: reasoning improves RP performance but simultaneously
increases RP hallucinations. This trade-off extends to a broader Pareto
frontier between RP performance and reliability for all LLMs. These findings
demonstrate the effectiveness of FURINA-Builder and the challenge posed by
FURINA-Bench.

</details>


### [94] [CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning](https://arxiv.org/abs/2510.06243)
*Qihua Dong,Luis Figueroa,Handong Zhao,Kushal Kafle,Jason Kuen,Zhihong Ding,Scott Cohen,Yun Fu*

Main category: cs.CL

TL;DR: CoT Referring通过结构化的链式思考训练数据来增强多模态推理能力，提高了Referring Expression Comprehension and Segmentation任务的准确性。


<details>
  <summary>Details</summary>
Motivation: Referring Expression Comprehension and Segmentation是评估多模态大语言模型（MLLMs）语言和图像理解整合能力的关键任务。

Method: 提出CoT Referring策略，通过系统地解析文本结构为顺序的指代步骤，识别关系并确保一致的指代对齐。重构训练数据，提供新的注释，并创建一个专门用于复杂指代情况的评估基准。将检测和分割能力整合到统一的MLLM框架中，并使用新颖的自适应加权损失进行训练。

Result: 在自定义基准和RefCOCO/+/g数据集上，相比基线模型，准确率提升了2.5%以上。

Conclusion: CoT Referring策略在处理复杂查询场景时，能够有效提高Referring Expression Comprehension and Segmentation任务的准确性。

Abstract: Referring Expression Comprehension and Segmentation are critical tasks for
assessing the integration of language understanding and image comprehension,
serving as benchmarks for Multimodal Large Language Models (MLLMs)
capabilities. To address these challenges, we propose a new strategy, CoT
Referring, which enhances model reasoning across modalities through a
structured, chain-of-thought training data structure. Our approach
systematically parses textual structures to a sequential referring step, where
in each step it identifies relationships and ensures consistent reference
alignment, thereby improving accuracy in complex query scenarios. We
restructure the training data to enforce a new output form, providing new
annotations for existing datasets and compiling an evaluation benchmark from
existing resources. This benchmark is designed explicitly for complex referring
cases. We also integrate detection and segmentation capabilities into a unified
MLLM framework, training it with a novel adaptive weighted loss to optimize
performance. Experimental results on our curated benchmark and RefCOCO/+/g
demonstrate the effectiveness of our approach, with a notable increase of 2.5%+
over baseline models.

</details>


### [95] [Evaluating Embedding Frameworks for Scientific Domain](https://arxiv.org/abs/2510.06244)
*Nouman Ahmed,Ronin Wu,Victor Botev*

Main category: cs.CL

TL;DR: 本研究旨在为科学领域寻找最优的词语表征算法和分词方法，并构建一个全面的评估套件。


<details>
  <summary>Details</summary>
Motivation: 在特定领域，词语的含义会因上下文而异，因此寻找最优的词语表征算法尤为重要。虽然生成式AI和Transformer模型能生成上下文相关的词语嵌入，但从头开始预训练的计算成本很高。

Method: 研究人员构建了一个包含多个下游任务和相关数据集的评估套件，并使用该套件测试了不同的词语表征和分词算法。

Result: 该研究测试了多种词语表征和分词算法，但具体结果未在摘要中详细说明。

Conclusion: 本研究提出了一个用于评估科学领域词语表征和分词算法的框架，并进行了初步测试，但最终的优化结果和最优选择有待进一步阐述。

Abstract: Finding an optimal word representation algorithm is particularly important in
terms of domain specific data, as the same word can have different meanings and
hence, different representations depending on the domain and context. While
Generative AI and transformer architecture does a great job at generating
contextualized embeddings for any given work, they are quite time and compute
extensive, especially if we were to pre-train such a model from scratch. In
this work, we focus on the scientific domain and finding the optimal word
representation algorithm along with the tokenization method that could be used
to represent words in the scientific domain. The goal of this research is two
fold: 1) finding the optimal word representation and tokenization methods that
can be used in downstream scientific domain NLP tasks, and 2) building a
comprehensive evaluation suite that could be used to evaluate various word
representation and tokenization algorithms (even as new ones are introduced) in
the scientific domain. To this end, we build an evaluation suite consisting of
several downstream tasks and relevant datasets for each task. Furthermore, we
use the constructed evaluation suite to test various word representation and
tokenization algorithms.

</details>


### [96] [TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B](https://arxiv.org/abs/2510.06249)
*Toshiki Nakai,Ravi Kiran Chikkala,Lena Sophie Oberkircher,Nicholas Jennings,Natalia Skachkova,Tatiana Anikina,Jesujoba Oluwadara Alabi*

Main category: cs.CL

TL;DR: 本研究提出TRepLiNa方法，通过对多语言大模型（LLM）的中间层强制进行跨语言相似性对齐，提升低资源语言（LRL）到高资源语言（HRL）的翻译质量，尤其在数据稀疏情况下效果显著。


<details>
  <summary>Details</summary>
Motivation: 印度存在大量低资源语言（LRLs），缺乏相应的语言资源，本研究旨在解决这一语言鸿沟问题，提升LRLs的翻译质量。

Method: 本研究提出TRepLiNa方法，结合了Centered Kernel Alignment (CKA) 和 REPINA，以强制对齐多语言LLM的中间层表示，并使用Aya-23 8B模型，在QLoRA和零样本、少样本、微调设置下，针对MMLoSo共享任务的语言对（Mundari, Santali, Bhili）与印地语/英语进行实验。

Result: 实验结果表明，使用TRepLiNa方法对齐LLM的中间层，能够有效提升LRL到HRL的翻译质量，尤其是在数据稀疏的情况下，该方法成本低廉且实用。

Conclusion: TRepLiNa是一种低成本、实用的方法，通过对LLM中间层强制进行跨语言相似性对齐，能够有效提升低资源语言的翻译质量，尤其在数据稀疏场景下表现突出。

Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact
(MMLoSo) Language Challenge addresses one of India's most pressing linguistic
gaps: the lack of resources for its diverse low-resource languages (LRLs). In
this study, we investigate whether enforcing cross-lingual similarity in
specific internal layers of a decoder-only multilingual large language model
(LLM) can improve translation quality from LRL to high-resource language (HRL).
Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric
that encourages representations of different languages to align, with REPINA, a
regularization method that constrains parameter updates to remain close to the
pretrained model, into a joint method we call TRepLiNa. In this research
project, we experiment with zero-shot, few-shot, and fine-tuning settings using
Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,
Santali, Bhili) with Hindi/English pivots. Our results show that aligning
mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach
to improving LRL translation, especially in data-scarce settings.

</details>


### [97] [Scalable multilingual PII annotation for responsible AI in LLMs](https://arxiv.org/abs/2510.06250)
*Bharti Meena,Joanna Skubisz,Harshit Rajgarhia,Nand Dave,Kiran Ganesh,Shivali Dalmia,Abhishek Mukherji,Vasudevan Sundarababu,Olga Pospelova*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As Large Language Models (LLMs) gain wider adoption, ensuring their reliable
handling of Personally Identifiable Information (PII) across diverse regulatory
contexts has become essential. This work introduces a scalable multilingual
data curation framework designed for high-quality PII annotation across 13
underrepresented locales, covering approximately 336 locale-specific PII types.
Our phased, human-in-the-loop annotation methodology combines linguistic
expertise with rigorous quality assurance, leading to substantial improvements
in recall and false positive rates from pilot, training, and production phases.
By leveraging inter-annotator agreement metrics and root-cause analysis, the
framework systematically uncovers and resolves annotation inconsistencies,
resulting in high-fidelity datasets suitable for supervised LLM fine-tuning.
Beyond reporting empirical gains, we highlight common annotator challenges in
multilingual PII labeling and demonstrate how iterative, analytics-driven
pipelines can enhance both annotation quality and downstream model reliability.

</details>


### [98] [Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments](https://arxiv.org/abs/2510.06262)
*Aryan Kumar Singh,Janvi Singh*

Main category: cs.CL

TL;DR: 该数据集包含对标准化的双语（英语-印地语）Prakriti评估问卷的响应，旨在根据古典阿育吠陀原则评估个人的身体、生理和心理特征。


<details>
  <summary>Details</summary>
Motivation: 根据古典阿育吠陀原则评估个人的身体、生理和心理特征。

Method: 该问卷包含 24 个多项选择题，涵盖了身体特征、食欲、睡眠模式、能量水平和性情。它遵循 AYUSH/CCRAS 指南进行开发，并通过 Google 表单进行数据收集，实现了响应的自动评分，将个体特征映射到 dosha 特异性分数。

Result: 该数据集为计算智能、阿育吠陀研究和个性化健康分析提供了结构化平台，支持特征分布、相关性和预测模型分析。

Conclusion: 该数据集可作为未来基于 Prakriti 的研究和智能健康应用的开发的参考。

Abstract: This dataset provides responses to a standardized, bilingual (English-Hindi)
Prakriti Assessment Questionnaire designed to evaluate the physical,
physiological, and psychological characteristics of individuals according to
classical Ayurvedic principles. The questionnaire consists of 24
multiple-choice items covering body features, appetite, sleep patterns, energy
levels, and temperament. It was developed following AYUSH/CCRAS guidelines to
ensure comprehensive and accurate data collection. All questions are mandatory
and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)
are hidden from participants. Data were collected via a Google Forms
deployment, enabling automated scoring of responses to map individual traits to
dosha-specific scores. The resulting dataset provides a structured platform for
research in computational intelligence, Ayurvedic studies, and personalized
health analytics, supporting analysis of trait distributions, correlations, and
predictive modeling. It can also serve as a reference for future Prakriti-based
studies and the development of intelligent health applications.

</details>


### [99] [Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians](https://arxiv.org/abs/2510.06263)
*Jiajun Wu,Swaleh Zaidi,Braden Teitge,Henry Leung,Jiayu Zhou,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 一个完全离线的两阶段摘要系统，可在嵌入式设备上运行，用于从电子健康记录中提取关键信息。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）包含大量非结构化临床数据，给急诊医生识别关键信息带来了挑战。

Method: 提出一个两阶段摘要系统：第一阶段（检索）在Jetson Nano-R上运行，利用本地存储的EHRs检索相关记录部分；第二阶段（摘要）在另一台Jetson Nano-S上运行，使用本地托管的小型语言模型（SLM）生成包含关键发现列表和特定于查询的叙述的摘要。系统在嵌入式设备上完全离线运行。

Result: 该系统能够有效地在30秒内生成有用的摘要，并在MIMIC-IV和去标识化的真实EHRs上进行了初步测试。

Conclusion: 该系统实现了完全离线的临床摘要，可以在嵌入式设备上运行，并能在短时间内生成格式化的关键发现列表和上下文相关的叙述摘要，从而保护患者隐私并提高急诊医生的效率。

Abstract: Electronic health records (EHRs) contain extensive unstructured clinical data
that can overwhelm emergency physicians trying to identify critical
information. We present a two-stage summarization system that runs entirely on
embedded devices, enabling offline clinical summarization while preserving
patient privacy. In our approach, a dual-device architecture first retrieves
relevant patient record sections using the Jetson Nano-R (Retrieve), then
generates a structured summary on another Jetson Nano-S (Summarize),
communicating via a lightweight socket link. The summarization output is
two-fold: (1) a fixed-format list of critical findings, and (2) a
context-specific narrative focused on the clinician's query. The retrieval
stage uses locally stored EHRs, splits long notes into semantically coherent
sections, and searches for the most relevant sections per query. The generation
stage uses a locally hosted small language model (SLM) to produce the summary
from the retrieved text, operating within the constraints of two NVIDIA Jetson
devices. We first benchmarked six open-source SLMs under 7B parameters to
identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to
assess summary quality in terms of factual accuracy, completeness, and clarity.
Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that
our fully offline system can effectively produce useful summaries in under 30
seconds.

</details>


### [100] [A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation](https://arxiv.org/abs/2510.06265)
*Aisha Alansari,Hamzah Luqman*

Main category: cs.CL

TL;DR: LLMs 产生的事实不准确内容（幻觉）是一个日益严重的问题，本综述对其原因、检测和缓解进行了全面回顾。


<details>
  <summary>Details</summary>
Motivation: LLM 的幻觉现象破坏了其可靠性和可信度，尤其是在需要事实准确性的领域。

Method: 对 LLM 幻觉的起因、检测和缓解策略进行了全面审查，包括幻觉类型的分类、检测方法的分类以及缓解策略的分类。

Result: 对当前检测和缓解方法的优缺点进行了分析，并回顾了用于量化 LLM 幻觉的评估基准和指标。

Conclusion: 概述了关键的开放性挑战和有前景的未来研究方向，为开发更真实、更可信的 LLM 奠定了基础。

Abstract: Large language models (LLMs) have transformed natural language processing,
achieving remarkable performance across diverse tasks. However, their
impressive fluency often comes at the cost of producing false or fabricated
information, a phenomenon known as hallucination. Hallucination refers to the
generation of content by an LLM that is fluent and syntactically correct but
factually inaccurate or unsupported by external evidence. Hallucinations
undermine the reliability and trustworthiness of LLMs, especially in domains
requiring factual accuracy. This survey provides a comprehensive review of
research on hallucination in LLMs, with a focus on causes, detection, and
mitigation. We first present a taxonomy of hallucination types and analyze
their root causes across the entire LLM development lifecycle, from data
collection and architecture design to inference. We further examine how
hallucinations emerge in key natural language generation tasks. Building on
this foundation, we introduce a structured taxonomy of detection approaches and
another taxonomy of mitigation strategies. We also analyze the strengths and
limitations of current detection and mitigation approaches and review existing
evaluation benchmarks and metrics used to quantify LLMs hallucinations.
Finally, we outline key open challenges and promising directions for future
research, providing a foundation for the development of more truthful and
trustworthy LLMs.

</details>


### [101] [Language models for longitudinal analysis of abusive content in Billboard Music Charts](https://arxiv.org/abs/2510.06266)
*Rohitash Chandra,Yathin Suresh,Divyansh Raj Sinha,Sanchit Jindal*

Main category: cs.CL

TL;DR: 研究显示，自1990年以来，美国公告牌热门歌曲中的不当内容（包括亵渎、露骨的性内容和不适宜的语言）显著增加，这可能对儿童和青少年的行为产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 由于不当内容对儿童和青少年的行为产生有害影响，因此需要对音乐（尤其是公告牌热门歌曲）中此类内容的趋势进行验证，以制定有效的政策。

Method: 使用深度学习和语言模型对过去七十年美国公告牌热门歌曲的歌词进行纵向分析，并结合情感分析和滥用检测技术。

Result: 从1990年开始，流行音乐中的不当内容显著增加，包含亵渎、露骨的性内容和不适宜语言的歌曲日益普遍。语言模型能够捕捉到歌词中反映社会规范和语言使用的细微模式变化。

Conclusion: 自1990年以来，美国流行音乐中的不当内容呈上升趋势，这凸显了对这一现象进行持续监测和研究的必要性，以便制定相应的社会和政策应对措施。

Abstract: There is no doubt that there has been a drastic increase in abusive and
sexually explicit content in music, particularly in Billboard Music Charts.
However, there is a lack of studies that validate the trend for effective
policy development, as such content has harmful behavioural changes in children
and youths. In this study, we utilise deep learning methods to analyse songs
(lyrics) from Billboard Charts of the United States in the last seven decades.
We provide a longitudinal study using deep learning and language models and
review the evolution of content using sentiment analysis and abuse detection,
including sexually explicit content. Our results show a significant rise in
explicit content in popular music from 1990 onwards. Furthermore, we find an
increasing prevalence of songs with lyrics containing profane, sexually
explicit, and otherwise inappropriate language. The longitudinal analysis of
the ability of language models to capture nuanced patterns in lyrical content,
reflecting shifts in societal norms and language use over time.

</details>


### [102] [Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"](https://arxiv.org/abs/2510.06275)
*Ranjan Mishra,Julian I. Bibo,Quinten van Engelen,Henk Schaapman*

Main category: cs.CL

TL;DR: 本文复现了“XRec: Large Language Models for Explainable Recommendation”的论文，并使用 Llama 3 替代了原论文的 GPT-3.5-turbo 模型。研究者修改了 XRec 的专家混合模块的输入或输出嵌入，并验证了 XRec 在生成个性化解释和提高模型稳定性方面的有效性，同时指出其在某些指标上可能不优于所有基线模型。此外，研究还探讨了专家混合嵌入在解释结构中的作用以及协同信号与语言模型的交互。


<details>
  <summary>Details</summary>
Motivation: 本文旨在复现 Ma 等人（2024）的“XRec: Large Language Models for Explainable Recommendation”论文，并使用 Llama 3 模型替换原有的 GPT-3.5-turbo 模型，以评估 XRec 框架在可解释推荐系统中的表现。

Method: 本文在 Ma 等人（2024）提供的源代码基础上进行修改，使用 Llama 3 作为大语言模型，并对 XRec 的专家混合（Mixture of Experts）模块的输入嵌入或输出嵌入进行了修改，以探究其对模型性能和解释结构的影响。

Result: XRec 能够有效地生成个性化推荐解释，并且通过整合协同信息可以提高模型的稳定性。然而，在某些评估指标上，XRec 的表现并不总是优于所有基线模型。修改专家混合模块的嵌入对解释的结构产生了显著影响，并揭示了协同信号与语言模型之间的交互作用。

Conclusion: XRec 框架在生成可解释的推荐方面是有效的，并且通过引入协同信息可以增强其稳定性。修改专家混合模块的嵌入是影响解释结构的关键因素。本文的研究为可解释推荐领域提供了开源的实现，并为相关研究和实践提供了便利。

Abstract: In this study, we reproduced the work done in the paper "XRec: Large Language
Models for Explainable Recommendation" by Ma et al. (2024). The original
authors introduced XRec, a model-agnostic collaborative instruction-tuning
framework that enables large language models (LLMs) to provide users with
comprehensive explanations of generated recommendations. Our objective was to
replicate the results of the original paper, albeit using Llama 3 as the LLM
for evaluation instead of GPT-3.5-turbo. We built on the source code provided
by Ma et al. (2024) to achieve our goal. Our work extends the original paper by
modifying the input embeddings or deleting the output embeddings of XRec's
Mixture of Experts module. Based on our results, XRec effectively generates
personalized explanations and its stability is improved by incorporating
collaborative information. However, XRec did not consistently outperform all
baseline models in every metric. Our extended analysis further highlights the
importance of the Mixture of Experts embeddings in shaping the explanation
structures, showcasing how collaborative signals interact with language
modeling. Through our work, we provide an open-source evaluation implementation
that enhances accessibility for researchers and practitioners alike. Our
complete code repository can be found at
https://github.com/julianbibo/xrec-reproducibility.

</details>


### [103] [Type and Complexity Signals in Multilingual Question Representations](https://arxiv.org/abs/2510.06304)
*Robin Kokot,Wessel Poelman*

Main category: cs.CL

TL;DR: 本研究使用QTC数据集，在七种语言中探究了多语言Transformer模型如何表征问题的形态句法属性，并通过对比分析了预训练模型和微调模型在捕捉问题结构复杂性方面的能力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究多语言Transformer模型如何表征问题的形态句法属性，并量化其在不同语言和不同复杂度问题上的表示能力。

Method: 研究引入了包含七种语言、标注了问题类型和复杂度（如依赖长度、树深度、词汇密度）的QTC数据集。评估方法扩展了探测方法至回归标签，并引入了选择性控制以量化泛化能力的提升。通过在冻结的Glot500-m模型表示上进行层级探测，并与子词TF-IDF基线模型和微调模型进行比较。

Result: 结果表明，对于具有显式标记的语言，统计特征能有效地区分问题类型；而神经网络探测器能更好地捕捉细粒度的结构复杂度模式。研究还评估了上下文表示在多大程度上优于统计基线，以及参数更新是否会降低预训练语言信息的可用性。

Conclusion: 多语言Transformer模型在表示问题的形态句法属性方面展现出一定的能力，但其效果受语言和问题复杂度等因素影响。神经网络探测器在捕捉结构复杂度方面优于统计特征，但预训练语言信息的可用性可能在模型微调过程中受到影响。

Abstract: This work investigates how a multilingual transformer model represents
morphosyntactic properties of questions. We introduce the Question Type and
Complexity (QTC) dataset with sentences across seven languages, annotated with
type information and complexity metrics including dependency length, tree
depth, and lexical density. Our evaluation extends probing methods to
regression labels with selectivity controls to quantify gains in
generalizability. We compare layer-wise probes on frozen Glot500-m (Imani et
al., 2023) representations against subword TF-IDF baselines, and a fine-tuned
model. Results show that statistical features classify questions effectively in
languages with explicit marking, while neural probes capture fine-grained
structural complexity patterns better. We use these results to evaluate when
contextual representations outperform statistical baselines and whether
parameter updates reduce the availability of pre-trained linguistic
information.

</details>


### [104] [LLM Bias Detection and Mitigation through the Lens of Desired Distributions](https://arxiv.org/abs/2510.06354)
*Ingroj Shrestha,Padmini Srinivasan*

Main category: cs.CL

TL;DR: 本文提出了一种加权自适应损失微调方法，用于将大型语言模型（LLM）的性别-职业输出分布与其期望分布对齐，以解决偏见问题。


<details>
  <summary>Details</summary>
Motivation: 现有偏见缓解工作主要关注社会公平和人口统计均等，而较少关注将LLM的输出与期望分布对齐，例如与现实世界分布对齐以支持事实基础。因此，本文将偏见定义为与期望分布（根据应用目标，可以是均等或现实世界分布）的偏差。

Method: 提出了一种加权自适应损失微调方法，将LLM的性别-职业输出分布与期望分布对齐，同时保持语言建模能力。利用源自美国劳工统计（2024）的三个职业集（男性主导、女性主导和性别平衡），评估了用于反映现实的自适应方法及其非自适应变体（用于均等）。

Result: 在三个掩码语言模型上，发现在均等和现实世界分布下均存在偏见。在均等设置下，偏见得到近乎完全的缓解；在现实世界设置下，偏见减少了30-75%。自回归LLM在均等设置下没有表现出偏见，但在现实世界设置下表现出显著偏见，其中Llama Instruct模型（3.2-3B，3.1-8B）实现了50-62%的偏见减少。

Conclusion: 本文提出的加权自适应损失微调方法能够有效缓解LLM中的性别-职业偏见，并且能够根据应用目标灵活地对齐到均等或现实世界分布。在不同类型的LLM上进行了评估，结果表明该方法在不同场景下都具有一定的有效性。

Abstract: Although prior work on bias mitigation has focused on promoting social
equality and demographic parity, less attention has been given to aligning
LLM's outputs to desired distributions. For example, we might want to align a
model with real-world distributions to support factual grounding. Thus, we
define bias as deviation from a desired distribution, which may be an equal or
real-world distribution, depending on application goals. We propose a weighted
adaptive loss based fine-tuning method that aligns LLM's gender-profession
output distribution with the desired distribution, while preserving language
modeling capability. Using 3 profession sets -- male-dominated,
female-dominated, and gender-balanced -- derived from U.S. labor statistics
(2024), we assess both our adaptive method for reflecting reality and a
non-adaptive variant for equality. Across three masked language models, bias is
observed under both distributions. We achieve near-complete mitigation under
equality and 30-75% reduction under real-world settings. Autoregressive LLMs
show no bias under equality but notable bias under real-world settings, with
the Llama Instruct models (3.2-3B, 3.1-8B) achieving a 50-62% reduction.

</details>


### [105] [EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preference](https://arxiv.org/abs/2510.06370)
*Kshitish Ghate,Andy Liu,Devansh Jain,Taylor Sorensen,Atoosa Kasirzadeh,Aylin Caliskan,Mona T. Diab,Maarten Sap*

Main category: cs.CL

TL;DR: EVALUESTEER是一个基准测试，用于衡量大型语言模型（LLMs）和奖励模型（RMs）在多大程度上可以根据用户的价值和风格偏好进行调整，并通过评估发现当前模型在理解和适应相关用户偏好信息方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在全球范围内的应用，创建能够满足全球用户多样化偏好和价值观的系统变得至关重要。然而，现有数据集在支持对奖励模型（RMs）进行可控的偏好引导评估方面存在不足。

Method: 通过人工生成165,888个偏好对，系统地在4个维度（传统、世俗-理性、生存和自我表达）和4个风格维度（详尽性、可读性、自信和温暖）上进行变化，构建了一个名为EVALUESTEER的基准测试。该基准测试用于评估LLMs和RMs在给定用户画像以及一对包含价值和风格偏好的候选回复时，能否选择符合用户偏好的回复。评估了六个开源和专有的LLMs和RMs，在16种不同的提示条件下和6种偏好比较场景下进行了测试。

Result: 在用户完整价值和风格偏好信息都提供的情况下，表现最好的模型在选择正确回复时的准确率低于75%，而仅提供相关的风格和价值偏好时，准确率则高于99%。

Conclusion: EVALUESTEER基准测试揭示了当前奖励模型在识别和适应相关的用户偏好信息方面的局限性，并为开发能够根据多样化的人类价值观和偏好进行调整的奖励模型提供了一个具有挑战性的测试平台。

Abstract: As large language models (LLMs) are deployed globally, creating pluralistic
systems that can accommodate the diverse preferences and values of users
worldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure
LLMs' and reward models' (RMs) steerability towards users' value and stylistic
preference profiles grounded in psychology and human-LLM interaction
literature. To address the gap in existing datasets that do not support
controlled evaluations of RM steering, we synthetically generated 165,888
preference pairs -- systematically varying pairs along 4 value dimensions
(traditional, secular-rational, survival, and self-expression) and 4 style
dimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER
to evaluate whether, given a user profile and a pair of candidate value-laden
and style-laden responses, LLMs and RMs are able to select the output that
aligns with the user's preferences. We evaluate six open-source and proprietary
LLMs and RMs under sixteen systematic prompting conditions and six preference
comparison scenarios. Notably, our results show that, when given the user's
full profile of values and stylistic preferences, the best models achieve <75%
accuracy at choosing the correct response, in contrast to >99% accuracy when
only relevant style and value preferences are provided. EVALUESTEER thus
highlights the limitations of current RMs at identifying and adapting to
relevant user profile information, and provides a challenging testbed for
developing RMs that can be steered towards diverse human values and
preferences.

</details>


### [106] [EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA](https://arxiv.org/abs/2510.06371)
*Firoj Alam,Ali Ezzat Shahroor,Md. Arid Hasan,Zien Sheikh Ali,Hunzalah Hassan Bhatti,Mohamed Bayan Kmainasi,Shammur Absar Chowdhury,Basel Mousi,Fahim Dalvi,Nadir Durrani,Natasa Milic-Frayling*

Main category: cs.CL

TL;DR: 现有的大规模多模态模型在视觉问答等任务上表现出色，但在需要文化常识、日常知识，特别是在资源匮乏和代表性不足的语言中，往往表现不佳。为了解决这个问题，我们提出了 EverydayMMQA 框架，用于创建大规模、文化相关的口语和视觉问答 (SVQA) 数据集。基于此框架，我们开发了 OASIS，一个整合了语音、图像和文本的多模态数据集，包含约 92 万张图像和 1480 万个问答对，其中有 370 万个口语问题，支持语音、文本、语音+图像、文本+图像四种输入组合。OASIS 专注于英语和阿拉伯语，涵盖 18 个国家，内容反映了真实的、多元化的现实生活场景，旨在测试模型在超越物体识别能力、涉及语用、常识和文化意识推理方面的能力。我们对四种闭源模型、三种开源模型和一种微调模型进行了基准测试。EverydayMMQA 和 OASIS 为构建能够处理文化背景下广泛日常任务的多模态大模型提供了基准和训练数据集，并将向社区公开。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模多模态模型在处理需要文化常识和日常知识的任务时存在不足，尤其是在低资源和代表性不足的语言中。

Method: 提出 EverydayMMQA 框架来创建大规模、文化相关的口语和视觉问答 (SVQA) 数据集。利用该框架开发了 OASIS 数据集，该数据集包含约 92 万张图像和 1480 万个问答对，支持语音、文本、语音+图像、文本+图像四种输入组合，并专注于英语和阿拉伯语，涵盖 18 个国家。

Result: OASIS 数据集包含丰富的多模态数据，能够测试模型在语用、常识和文化意识推理方面的能力。对多种模型进行了基准测试。

Conclusion: EverydayMMQA 框架和 OASIS 数据集为构建能够处理文化背景下广泛日常任务的多模态大模型提供了重要的资源和基准，解决了现有模型在文化和日常知识方面的局限性。

Abstract: Large-scale multimodal models achieve strong results on tasks like Visual
Question Answering (VQA), but they often fail when queries require culturally
grounded, everyday knowledge, particularly in low-resource and underrepresented
languages. To bridge this gap, we introduce Everyday Multimodal and
Multilingual QA (EverydayMMQA), a framework for creating large-scale,
culturally-grounded datasets for spoken and visual question answering (SVQA).
Using this framework, we developed OASIS, a multimodal dataset integrating
speech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS
contains 3.7M spoken questions, enabling four unique input combinations:
speech-only, text-only, speech+image, and text+image. Focused on English and
Arabic varieties, 18 countries, the dataset content is curated to reflect
diverse, real-world situations. OASIS tests models on tasks beyond object
recognition that involve pragmatic, commonsense, and culturally aware
reasoning. We benchmarked four closed-source models, three open-source models,
and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark
and training dataset for building multimodal LLMs for a comprehensive set of
everyday tasks within cultural contexts. The framework and dataset will be made
publicly available to the community.

</details>


### [107] [Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language](https://arxiv.org/abs/2510.06378)
*Angie Boggust,Donghao Ren,Yannick Assogba,Dominik Moritz,Arvind Satyanarayan,Fred Hohman*

Main category: cs.CL

TL;DR: 使用语义正则表达式来提高LLM特征描述的精确性和一致性，并支持新的分析维度。


<details>
  <summary>Details</summary>
Motivation: 自然语言描述LLM特征存在模糊、不一致且需要手动重新标注的问题。

Method: 提出语义正则表达式，结合了捕捉语言和语义模式的基元以及用于情境化、组合和量化的修饰符。

Result: 语义正则表达式在准确性上可与自然语言相媲美，但描述更简洁、一致，并支持量化特征复杂性和模型层面分析。

Conclusion: 语义正则表达式能够帮助用户构建关于LLM特征激活的准确心智模型，并且其固有的结构支持新的分析方法。

Abstract: Automated interpretability aims to translate large language model (LLM)
features into human understandable descriptions. However, these natural
language feature descriptions are often vague, inconsistent, and require manual
relabeling. In response, we introduce semantic regexes, structured language
descriptions of LLM features. By combining primitives that capture linguistic
and semantic feature patterns with modifiers for contextualization,
composition, and quantification, semantic regexes produce precise and
expressive feature descriptions. Across quantitative benchmarks and qualitative
analyses, we find that semantic regexes match the accuracy of natural language
while yielding more concise and consistent feature descriptions. Moreover,
their inherent structure affords new types of analyses, including quantifying
feature complexity across layers, scaling automated interpretability from
insights into individual features to model-wide patterns. Finally, in user
studies, we find that semantic regex descriptions help people build accurate
mental models of LLM feature activations.

</details>


### [108] [Protecting De-identified Documents from Search-based Linkage Attacks](https://arxiv.org/abs/2510.06383)
*Pierre Lison,Mark Anderson*

Main category: cs.CL

TL;DR: 该研究提出了一种新的去标识化方法，通过构建N-gram索引和使用LLM改写技术，有效防止基于搜索的链接攻击，同时保持文本的语义完整性。


<details>
  <summary>Details</summary>
Motivation: 现有的去标识化模型无法解决链接风险，即从去标识化文本反向映射回原始数据源的可能性。一种简单的链接方法是从去标识化文档中提取短语并在原始数据集中进行搜索。

Method: 1. 构建文档集合中N-gram的倒排索引，以识别仅出现在少于k个文档中的N-gram。2. 利用基于LLM的重写器迭代地改写这些N-gram所在的文本片段，直到链接不再可能。

Result: 在法院判例数据集上的实验表明，该方法能够有效阻止基于搜索的链接，并保持文本内容与原始文本的一致性。

Conclusion: 所提出的方法能够有效应对基于搜索的链接攻击，同时保留文本的原始语义。

Abstract: While de-identification models can help conceal the identity of the
individual(s) mentioned in a document, they fail to address linkage risks,
defined as the potential to map the de-identified text back to its source. One
straightforward way to perform such linkages is to extract phrases from the
de-identified document and then check their presence in the original dataset.
This paper presents a method to counter search-based linkage attacks while
preserving the semantic integrity of the text. The method proceeds in two
steps. We first construct an inverted index of the N-grams occurring in the
document collection, making it possible to efficiently determine which N-grams
appear in less than $k$ documents (either alone or in combination with other
N-grams). An LLM-based rewriter is then iteratively queried to reformulate
those spans until linkage is no longer possible. Experimental results on a
collection of court cases show that the method is able to effectively prevent
search-based linkages while remaining faithful to the original content.

</details>


### [109] [Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion](https://arxiv.org/abs/2510.06386)
*Fan Zhou,Chang Tian,Tim Van de Cruys*

Main category: cs.CL

TL;DR: RegDiff是一个正则化扩散框架，它利用属性特征在采样过程中不需要预训练的分类器，从而以降低的计算成本实现可控生成。


<details>
  <summary>Details</summary>
Motivation: 生成具有特定属性的风格化文本是可控文本生成中的一个关键问题，现有的方法（如CFG和CG）在保持语义内容和属性控制之间存在权衡，CG虽然能更好地对齐属性，但计算成本高且存在分类器泛化问题。

Method: RegDiff采用基于VAE的编码器-解码器架构来保证重建保真度，并结合在属性监督下训练的潜在扩散模型来实现可控文本生成。属性信息仅在训练期间注入，采样时无需分类器。

Result: RegDiff在五个跨越多种风格属性的数据集上进行了实验，结果表明RegDiff在生成风格化文本方面优于强基线模型。

Conclusion: RegDiff作为一种高效的属性可控文本扩散解决方案，被证明是有效的。

Abstract: Generating stylistic text with specific attributes is a key problem in
controllable text generation. Recently, diffusion models have emerged as a
powerful paradigm for both visual and textual generation. Existing approaches
can be broadly categorized into classifier-free guidance (CFG) and classifier
guidance (CG) methods. While CFG effectively preserves semantic content, it
often fails to provide effective attribute control. In contrast, CG modifies
the denoising trajectory using classifier gradients, enabling better attribute
alignment but incurring high computational costs during sampling and suffering
from classifier generalization issues. In this work, we propose RegDiff, a
regularized diffusion framework that leverages attribute features without
requiring a pretrained classifier during sampling, thereby achieving
controllable generation with reduced computational costs. Specifically, RegDiff
employs a VAE-based encoder--decoder architecture to ensure reconstruction
fidelity and a latent diffusion model trained with attribute supervision to
enable controllable text generation. Attribute information is injected only
during training. Experiments on five datasets spanning multiple stylistic
attributes demonstrate that RegDiff outperforms strong baselines in generating
stylistic texts. These results validate the effectiveness of RegDiff as an
efficient solution for attribute-controllable text diffusion. Our code,
datasets, and resources will be released upon publication at
https://github.com/xxxx.

</details>


### [110] [Reward Model Perspectives: Whose Opinions Do Reward Models Reward?](https://arxiv.org/abs/2510.06391)
*Elle*

Main category: cs.CL

TL;DR: RMs在语言模型对齐中很重要，但对其行为的理解有限。本研究提出了一个衡量RM意见对齐的框架，并研究了RM的社会人口偏见以及提示对引导RM偏好的影响。研究发现，RMs与多个群体存在对齐不佳的问题，并可能奖励有害的刻板印象，且单纯的提示引导不足以克服这些局限性。这表明在偏好学习中需要更仔细地考虑RM行为，以防止不必要的社会偏见在语言技术中传播。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型（LM）对齐中使用的奖励模型（RM）的行为，特别是它们在多大程度上捕捉了人类偏好，以及它们是否存在社会人口偏见。

Method: 提出一个形式化框架来衡量RM捕获的意见对齐；研究RM在多大程度上表现出社会人口偏见；探索提示对引导RM朝着目标群体偏好的影响；研究有争议话题的主观和多样化观点，以量化RM的观点。

Result: RMs与多个社会人口群体对齐不佳，并且系统性地奖励有害的刻板印象。单纯的提示引导不足以克服这些局限性。

Conclusion: RMs在对齐方面存在不足，可能传播不必要的社会偏见。在偏好学习中需要更仔细地考虑RM行为，以防止语言技术中出现有害的社会偏见。

Abstract: Reward models (RMs) are central to the alignment of language models (LMs). An
RM often serves as a proxy for human preferences to guide downstream LM
behavior. However, our understanding of RM behavior is limited. Our work (i)
formalizes a framework for measuring the alignment of opinions captured by RMs,
(ii) investigates the extent to which RMs demonstrate sociodemographic biases,
and (iii) explores the effects of prompting to steer rewards towards the
preferences of a target group. We study the subjective and diverse perspectives
on controversial topics, which allows us to quantify RM perspectives in terms
of their opinions, attitudes, and values. We show that RMs are poorly aligned
with several demographic groups and can systematically reward harmful
stereotypes, and steering alone is not enough to overcome these limitations.
Our findings underscore the need for more careful consideration of RM behavior
in model alignment during preference learning to prevent the propagation of
unwanted social biases in the language technologies that we use.

</details>


### [111] [Instructional Goal-Aligned Question Generation for Student Evaluation in Virtual Lab Settings: How Closely Do LLMs Actually Align?](https://arxiv.org/abs/2510.06411)
*R. Alexander Knipper,Indrani Dey,Souvika Sarkar,Hari Narayanan,Sadhana Puntambekar,Santu Karmaker*

Main category: cs.CL

TL;DR: 该研究提出了一种利用大型语言模型（LLMs）为虚拟实验生成教学目标对齐问题的框架，以解决教师在适应虚拟实验和定制问题方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 教师在将虚拟实验与其教学目标对齐以及定制教学资源方面面临挑战，而现有的第三方材料可能不适用，自研则耗时耗力。

Method: 研究提出了一个包含四个组成部分的新型对齐框架：通过师生对话理解教学目标；通过知识单元和关系分析理解虚拟实验；使用问题分类法来构建认知和教学意图；利用TELeR分类法来控制提示的详细程度。

Result: 该框架能够生成教学目标对齐、具有认知要求且格式符合要求的问题。与基线相比，开放式和关系型问题将质量提高了0.29-0.39分。优化后的TELeR提示显著提高了格式符合度（80%可解析性，>90%符合度）。大型模型效果更佳，可解析性提高37.1%，符合度提高25.7%，平均质量提高0.8个李克特点。

Conclusion: 该研究提出的LLM驱动的框架能够有效地生成符合教学目标、具有更高认知要求且格式准确的虚拟实验问题，为教育技术领域提供了一个有价值的工具。

Abstract: Virtual Labs offer valuable opportunities for hands-on, inquiry-based science
learning, yet teachers often struggle to adapt them to fit their instructional
goals. Third-party materials may not align with classroom needs, and developing
custom resources can be time-consuming and difficult to scale. Recent advances
in Large Language Models (LLMs) offer a promising avenue for addressing these
limitations. In this paper, we introduce a novel alignment framework for
instructional goal-aligned question generation, enabling teachers to leverage
LLMs to produce simulation-aligned, pedagogically meaningful questions through
natural language interaction. The framework integrates four components:
instructional goal understanding via teacher-LLM dialogue, lab understanding
via knowledge unit and relationship analysis, a question taxonomy for
structuring cognitive and pedagogical intent, and the TELeR taxonomy for
controlling prompt detail. Early design choices were informed by a small
teacher-assisted case study, while our final evaluation analyzed over 1,100
questions from 19 open-source LLMs. With goal and lab understanding grounding
questions in teacher intent and simulation context, the question taxonomy
elevates cognitive demand (open-ended formats and relational types raise
quality by 0.29-0.39 points), and optimized TELeR prompts enhance format
adherence (80% parsability, >90% adherence). Larger models yield the strongest
gains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert
points.

</details>


### [112] [FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering](https://arxiv.org/abs/2510.06426)
*Yitao Long,Tiansheng Hu,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.CL

TL;DR: LLMs经常在回答长篇问题时产生不准确但看似合理的内容。虽然提供引用是一种常见的缓解策略，但现有的基准主要集中在检索文本证据上。我们提出了FinLFQA，一个用于评估LLM在复杂金融问题上生成长篇答案以及提供可靠、细致的引用的新基准。FinLFQA通过人工标注评估三个关键的归因方面：(1)从财务报告中提取的支撑证据，(2)中间的数值推理步骤，以及(3)为推理过程提供信息的特定金融知识。我们还提供了一个涵盖答案和归因质量的自动评估框架。通过对八个LLM的广泛实验，我们发现细粒度指标对于区分模型能力很重要，端到端生成和事后方法表现相当，迭代改进只有在外部反馈的指导下才有效。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM归因基准主要关注简单的文本证据检索，而忽略了金融等现实世界应用中更复杂的归因需求，这些需求涉及到数值推理和领域知识。

Method: 提出FinLFQA基准，包含对支撑证据、中间数值推理步骤和领域特定金融知识的人工标注评估，并辅以自动评估框架。在八个LLM上进行实验，比较不同的归因生成范式。

Result: 细粒度指标对于区分模型能力至关重要；端到端生成和事后归因方法性能相当；迭代改进在无外部反馈时效果有限。

Conclusion: FinLFQA基准能够更全面地评估LLM在复杂金融问答中的归因能力，并为未来研究提供了方向，强调了细粒度评估和特定领域知识的重要性。

Abstract: Large Language Models (LLMs) frequently hallucinate to long-form questions,
producing plausible yet factually incorrect answers. A common mitigation
strategy is to provide attribution to LLM outputs. However, existing benchmarks
primarily focus on simple attribution that retrieves supporting textual
evidence as references. We argue that in real-world scenarios such as financial
applications, attribution goes beyond reference retrieval. We introduce
FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate
long-form answers to complex financial questions with reliable and nuanced
attributions. FinLFQA evaluates three critical aspects of attribution through
human annotations: (1) supporting evidence extracted from financial reports,
(2) intermediate numerical reasoning steps, and (3) domain-specific financial
knowledge that informs the reasoning process. We further provide an automatic
evaluation framework covering both answer quality and attribution quality.
Through extensive experiments on eight LLMs across multiple
attribution-generation paradigms, we find that fine-grained metrics are
important to distinguish model capabilities, that end-to-end generation
achieves comparable performance to post-hoc approaches, and that iterative
refinement only helps when guided by external feedback.

</details>


### [113] [Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser](https://arxiv.org/abs/2510.06427)
*Elena Chistova*

Main category: cs.CL

TL;DR: UniRST是一个统一的、支持11种语言的篇章结构解析器，通过多头和掩码联合等训练策略解决了不同树库的关系不兼容问题，并在多数情况下超越了基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决多语言、多树库篇章结构解析中关系不兼容的问题，并实现一个统一的解析模型。

Method: 提出并评估了两种训练策略：多头（为每个关系库存分配单独的分类层）和掩码联合（通过选择性标签掩码实现共享参数训练）。在低资源情况下引入了数据增强技术。训练了一个统一的模型。

Result: 掩码联合方法在参数效率和性能上均表现最佳。UniRST模型的表现优于16个单树库基线模型。

Conclusion: UniRST模型能够实现跨多种资源的、统一的多语言端到端篇章结构解析，并展现出优于现有单树库基线模型的性能。

Abstract: We introduce UniRST, the first unified RST-style discourse parser capable of
handling 18 treebanks in 11 languages without modifying their relation
inventories. To overcome inventory incompatibilities, we propose and evaluate
two training strategies: Multi-Head, which assigns separate relation
classification layer per inventory, and Masked-Union, which enables shared
parameter training through selective label masking. We first benchmark
monotreebank parsing with a simple yet effective augmentation technique for
low-resource settings. We then train a unified model and show that (1) the
parameter efficient Masked-Union approach is also the strongest, and (2) UniRST
outperforms 16 of 18 mono-treebank baselines, demonstrating the advantages of a
single-model, multilingual end-to-end discourse parsing across diverse
resources.

</details>


### [114] [MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning](https://arxiv.org/abs/2510.06430)
*Neeraja Kirtane,Yuvraj Khanna,Peter Relan*

Main category: cs.CL

TL;DR: 大型语言模型在数学推理方面对语言变化的鲁棒性不足，即使是像GPT-5和Gemini-2.5pro这样的前沿模型也受到影响。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在数学推理方面对语言变化的鲁棒性，尤其是在教育环境中使用的高中水平数学问题。

Method: 创建一个名为MathRobust-LV的新测试集和评估方法，通过改变问题中的表面细节（姓名、上下文、变量）来生成问题变体，同时保持数值结构和答案不变，并使用该测试集评估34个模型。

Result: 与基线相比，模型在MathRobust-LV测试集上的准确率有所下降，尤其是较小的模型（下降9-11%），但较强的模型也显示出可衡量的性能下降。GPT-5和Gemini-2.5pro等前沿模型相对稳定。

Conclusion: 对语言变化的鲁棒性是大型语言模型在数学推理方面面临的一个基本挑战，目前的模型（即使是前沿模型）在该方面仍存在漏洞。

Abstract: Large language models excel on math benchmarks, but their math reasoning
robustness to linguistic variation is underexplored. While recent work
increasingly treats high-difficulty competitions like the IMO as the gold
standard for evaluating reasoning, we believe in comprehensive benchmarking of
high school-level math problems in real educational settings. We introduce
MathRobust-LV, a test set and evaluation methodology that mirrors how
instructors rephrase problems across assessments while keeping difficulty
constant: we change surface details (names, contexts, variables) while
preserving numerical structure and answers. In contrast to prior efforts that
alter problem content or emphasize IMO-level tasks, we focus on
high-school-level dataset problems at the difficulty level where models are
currently deployed in educational settings: tutoring and assessment systems. In
these applications, instructors rephrase identical concepts in varied ways,
making linguistic robustness essential for reliable deployment. Although MATH
data benchmarking is often regarded as saturated, our experiment on 34 models
reveals that accuracy declines when moving from the baseline to the variants.
These drops are severe for smaller models (9-11%) while stronger models also
show measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain
comparatively stable. Our results highlight that robustness to linguistic
variation is a fundamental challenge, exposing reasoning vulnerabilities in
models.

</details>


### [115] [A Survey on Agentic Security: Applications, Threats and Defenses](https://arxiv.org/abs/2510.06445)
*Asif Shahriar,Md Nafiu Rahman,Sadif Ahmed,Farig Sadeque,Md Rizwan Parvez*

Main category: cs.CL

TL;DR: 网络安全领域正从被动语言模型转向自主语言模型代理，但这带来了新的安全风险。本文首次全面 survey 了代理安全领域，围绕应用、威胁和防御三大支柱进行了梳理，并对150多篇论文进行了分类，分析了代理的应用、漏洞和防护措施。研究还揭示了代理架构的新兴趋势和关键研究空白。


<details>
  <summary>Details</summary>
Motivation: 自主语言模型代理在网络安全攻防领域具有巨大潜力，但也引入了新的固有安全风险。因此，有必要对这一新兴领域的安全问题进行全面梳理和分析。

Method: 通过对150多篇相关论文进行全面的文献调查，围绕应用、威胁和防御三个相互关联的支柱对代理安全领域进行结构化梳理，并进行详细的跨领域分析。

Result: 提出了一个包含150多篇论文的综合分类体系，概述了代理在网络安全中的应用、存在的漏洞以及相应的防护措施。分析揭示了代理架构的新兴趋势，并指出了模型和模态覆盖方面的关键研究空白。

Conclusion: 自主语言模型代理在网络安全领域带来了新的机遇和挑战。虽然其应用广泛，但同时也存在严峻的安全风险。本研究首次全面梳理了该领域的安全问题，并为未来的研究指明了方向，尤其是在模型和模态覆盖方面仍有待深入探索。

Abstract: The rapid shift from passive LLMs to autonomous LLM-agents marks a new
paradigm in cybersecurity. While these agents can act as powerful tools for
both offensive and defensive operations, the very agentic context introduces a
new class of inherent security risks. In this work we present the first
holistic survey of the agentic security landscape, structuring the field around
three interdependent pillars: Applications, Threats, and Defenses. We provide a
comprehensive taxonomy of over 150 papers, explaining how agents are used, the
vulnerabilities they possess, and the countermeasures designed to protect them.
A detailed cross-cutting analysis shows emerging trends in agent architecture
while revealing critical research gaps in model and modality coverage.

</details>


### [116] [Linguistically Informed Tokenization Improves ASR for Underresourced Languages](https://arxiv.org/abs/2510.06461)
*Massimo Daul,Alessio Tosolini,Claire Bowern*

Main category: cs.CL

TL;DR: 使用 wav2vec2 模型对 Yan-nhangu 语进行语音识别，并比较了音素和拼写两种标记策略的效果，发现音素标记能显著提升模型性能，且 ASR 可用于低资源语言的转录。


<details>
  <summary>Details</summary>
Motivation: 现代 ASR 系统需要大量数据，这使得它们难以用于资源匮乏的语言，而 ASR 是语言学家进行语言记录的关键工具。

Method: 对 wav2vec2 ASR 模型进行微调，分别采用音素和拼写两种标记策略，并与手动校对 ASR 输出和从头开始手动转录进行比较。

Result: 音素标记策略在词错误率（WER）和字符错误率（CER）方面优于拼写标记策略。ASR 模型输出的手动校对速度明显快于手动转录。

Conclusion: ASR 可用于资源匮乏的语言，其中，语言学信息丰富的音素标记策略比拼写标记策略效果更好。

Abstract: Automatic speech recognition (ASR) is a crucial tool for linguists aiming to
perform a variety of language documentation tasks. However, modern ASR systems
use data-hungry transformer architectures, rendering them generally unusable
for underresourced languages. We fine-tune a wav2vec2 ASR model on Yan-nhangu,
a dormant Indigenous Australian language, comparing the effects of phonemic and
orthographic tokenization strategies on performance. In parallel, we explore
ASR's viability as a tool in a language documentation pipeline. We find that a
linguistically informed phonemic tokenization system substantially improves WER
and CER compared to a baseline orthographic tokenization scheme. Finally, we
show that hand-correcting the output of an ASR model is much faster than
hand-transcribing audio from scratch, demonstrating that ASR can work for
underresourced languages.

</details>


### [117] [Test-Time Scaling of Reasoning Models for Machine Translation](https://arxiv.org/abs/2510.06471)
*Zihao Li,Shaoxiong Ji,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 在机器翻译（MT）中，测试时推理（TTR）对通用模型效果有限，但通过领域特定微调或在纠错场景下效果显著。


<details>
  <summary>Details</summary>
Motivation: 评估增加推理计算量是否能提升机器翻译质量。

Method: 在12个推理模型（RM）上，针对直译、强制推理外推和后编辑三种场景，在多个机器翻译基准上评估TTR。

Result: 对于通用RM，TTR在直译上收益有限且不稳定。领域特定微调能解锁TTR的潜力，带来一致性提升。强制模型超出自然停止点推理会降低翻译质量。TTR在后编辑场景下非常有效，能将自我纠错转化为有益过程。

Conclusion: 在机器翻译中，推理计算量的价值在于结合任务专业模型进行多步骤的自我纠错工作流，而非增强通用模型的单遍翻译能力。

Abstract: Test-time scaling (TTS) has enhanced the performance of Reasoning Models
(RMs) on various tasks such as math and coding, yet its efficacy in machine
translation (MT) remains underexplored. This paper investigates whether
increased inference-time computation improves translation quality. We evaluate
12 RMs across a diverse suite of MT benchmarks spanning multiple domains,
examining three scenarios: direct translation, forced-reasoning extrapolation,
and post-editing. Our findings show that for general-purpose RMs, TTS provides
limited and inconsistent benefits for direct translation, with performance
quickly plateauing. However, the effectiveness of TTS is unlocked by
domain-specific fine-tuning, which aligns a model's reasoning process with task
requirements, leading to consistent improvements up to an optimal,
self-determined reasoning depth. We also find that forcing a model to reason
beyond its natural stopping point consistently degrades translation quality. In
contrast, TTS proves highly effective in a post-editing context, reliably
turning self-correction into a beneficial process. These results indicate that
the value of inference-time computation in MT lies not in enhancing single-pass
translation with general models, but in targeted applications like multi-step,
self-correction workflows and in conjunction with task-specialized models.

</details>


### [118] [Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels](https://arxiv.org/abs/2510.06499)
*Zhepeng Cen,Haolin Chen,Shiyu Wang,Zuxin Liu,Zhiwei Liu,Ding Zhao,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: LLMs通过模仿学习取得成功，但存在训练-生成差距且限制了鲁棒推理。强化学习（RL）是更有效的数据解决方案，但受限于现有RL数据集规模小且多样性不足。我们提出了Webscale-RL管道，一个可扩展的数据引擎，将大规模预训练文档转换为数百万个多样化、可验证的问答对用于RL。我们构建了Webscale-RL数据集，包含1.2M个跨越9个领域的示例。实验表明，在该数据集上训练的模型显著优于持续预训练和数据精炼基线。RL训练效率更高，使用少100倍的token即可达到持续预训练的性能。我们的工作为RL扩展到预训练级别提供了可行途径，使语言模型更强大、更高效。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在LLMs中存在训练-生成差距和鲁棒推理受限的问题。强化学习（RL）是更有效的数据解决方案，但现有RL数据集规模小且多样性不足，阻碍了其应用。

Method: 提出Webscale-RL管道，一个可扩展的数据引擎，用于将大规模预训练文档转换为数百万个多样化、可验证的问答对，构建了包含1.2M个跨越9个领域的Webscale-RL数据集。

Result: 在Webscale-RL数据集上训练的模型显著优于持续预训练和数据精炼基线。RL训练效率更高，使用少100倍的token即可达到持续预训练的性能。

Conclusion: Webscale-RL管道和数据集为RL扩展到预训练级别提供了可行途径，能够构建更强大、更高效的语言模型。

Abstract: Large Language Models (LLMs) have achieved remarkable success through
imitation learning on vast text corpora, but this paradigm creates a
training-generation gap and limits robust reasoning. Reinforcement learning
(RL) offers a more data-efficient solution capable of bridging this gap, yet
its application has been constrained by a critical data bottleneck: existing RL
datasets are orders of magnitude smaller and less diverse than web-scale
pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a
scalable data engine that systematically converts large-scale pre-training
documents into millions of diverse, verifiable question-answer pairs for RL.
Using this pipeline, we construct the Webscale-RL dataset, containing 1.2
million examples across more than 9 domains. Our experiments show that the
model trained on this dataset significantly outperforms continual pretraining
and strong data refinement baselines across a suite of benchmarks. Notably, RL
training with our dataset proves substantially more efficient, achieving the
performance of continual pre-training with up to 100$\times$ fewer tokens. Our
work presents a viable path toward scaling RL to pre-training levels, enabling
more capable and efficient language models.

</details>


### [119] [From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining](https://arxiv.org/abs/2510.06548)
*Seng Pei Liew,Takuya Kato*

Main category: cs.CL

TL;DR: Bootstrapped pretraining effectiveness diminishes with base model pretraining extent, following a predictable scaling law. Extensive pretraining leads to less benefit from bootstrapping.


<details>
  <summary>Details</summary>
Motivation: To empirically study the effectiveness and scaling behavior of bootstrapped pretraining, especially for overtrained base models, and to understand the trade-offs in multi-stage pretraining.

Method: Empirically study the scaling behavior of bootstrapped pretraining. Model the joint dependence on first- and second-stage tokens using a simple scaling law.

Result: Found that the scaling efficiency of bootstrapped pretraining diminishes in a predictable manner: the scaling exponent decreases logarithmically with the number of tokens used for base model pretraining. Developed a scaling law to model this behavior.

Conclusion: There is a fundamental trade-off in multi-stage pretraining: more extensive pretraining of the base model leads to diminishing returns from bootstrapping. The findings offer practical insights for efficient language model training and highlight considerations for reusing overtrained models.

Abstract: Bootstrapped pretraining, i.e., the reuse of a pretrained base model for
further pretraining, such as continual pretraining or model growth, is
promising at reducing the cost of training language models from scratch.
However, its effectiveness remains unclear, especially when applied to
overtrained base models. In this work, we empirically study the scaling
behavior of bootstrapped pretraining and find that its scaling efficiency
diminishes in a predictable manner: The scaling exponent with respect to
second-stage pretraining tokens decreases logarithmically with the number of
tokens used to pretrain the base model. The joint dependence on first- and
second-stage tokens is accurately modeled by a simple scaling law. Such
saturation effect reveals a fundamental trade-off in multi-stage pretraining
strategies: the more extensively a model is pretrained, the less additional
benefit bootstrapping provides. Our findings provide practical insights for
efficient language model training and raise important considerations for the
reuse of overtrained models.

</details>


### [120] [Flipping the Dialogue: Training and Evaluating User Language Models](https://arxiv.org/abs/2510.06552)
*Tarek Naous,Philippe Laban,Wei Xu,Jennifer Neville*

Main category: cs.CL

TL;DR: 助手模型作为用户模拟器表现不佳，提出专门的用户语言模型（User LM）以提高模拟的真实性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 以往的研究使用助手模型来模拟用户进行多轮对话，但这种方法存在缺陷，因为助手模型的训练目标与其作为用户的行为模式不一致。

Method: 提出并训练专门的用户语言模型（User LM），使其能够更好地模拟人类用户在多轮对话中的行为。通过评估 User LM 在模拟编程和数学对话方面的表现。

Result: User LM 比现有的模拟方法更能符合人类行为，并且具有更好的模拟稳健性。在使用 User LM 模拟的更真实环境中，助手模型（如 GPT-4o）的性能显著下降（从 74.6% 降至 57.4%），表明当前的助手模型难以应对用户在多轮对话中的细微差别。

Conclusion: 专门的用户语言模型（User LM）是比助手模型更优越的用户模拟器，能够提供更真实、更具挑战性的评估环境，从而揭示助手模型在真实交互中的局限性。

Abstract: Conversations with LMs involve two participants: a human user leading the
conversation, and an LM assistant responding to the user's request. To satisfy
this specific role, LMs are post-trained to be helpful assistants -- optimized
to produce exhaustive and well-structured responses, free of ambiguity and
grammar errors. User utterances, on the other hand, are rarely perfected, with
each user phrasing requests in unique ways, sometimes putting in partial effort
at each turn and refining on the fly. To evaluate LM performance in realistic
settings, prior work simulated users in multi-turn conversations, often
prompting an LLM originally trained to be a helpful assistant to act as a user.
However, we show that assistant LMs make for poor user simulators, with the
surprising finding that better assistants yield worse simulators. Instead, we
introduce purpose-built User Language Models (User LMs) - models post-trained
to simulate human users in multi-turn conversations. Through various
evaluations, we show how User LMs align better with human behavior and achieve
better simulation robustness than existing simulation methods. When leveraging
User LMs to simulate coding and math conversations, the performance of a strong
assistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic
simulation environments lead to assistant struggles as they fail to cope with
the nuances of users in multi-turn setups.

</details>


### [121] [TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents](https://arxiv.org/abs/2510.06579)
*Haofei Yu,Keyang Xuan,Fenghai Li,Kunlun Zhu,Zijie Lei,Jiaxun Zhang,Ziheng Qi,Kyle Richardson,Jiaxuan You*

Main category: cs.CL

TL;DR: TinyScientist 框架简化了大型语言模型（LLM）的自动研究工作流，通过提供一个交互式、可扩展且可控的框架，轻松适应新工具并支持迭代开发，最终目标是让顶尖的自动研究流程能够被广泛访问。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在自动研究中的应用日益广泛，涉及多智能体系统、规划、工具使用、代码执行和人机交互的复杂工作流带来了扩展和维护的挑战，尤其是在算法和架构不断进步的背景下。

Method: TinyScientist 框架通过识别自动研究工作流的核心组件，并提出一个交互式、可扩展且可控的框架来实现这一目标，该框架能够轻松适应新工具并支持迭代发展。

Result: 该研究提供了一个开源代码库、一个交互式 Web 演示和一个 PyPI Python 包，旨在使最先进的自动研究管道能够被所有研究人员和开发人员广泛访问。

Conclusion: TinyScientist 框架通过提供一个灵活且易于扩展的解决方案，解决了自动研究工作流日益增长的复杂性问题，并促进了其广泛应用。

Abstract: Automatic research with Large Language Models (LLMs) is rapidly gaining
importance, driving the development of increasingly complex workflows involving
multi-agent systems, planning, tool usage, code execution, and human-agent
interaction to accelerate research processes. However, as more researchers and
developers begin to use and build upon these tools and platforms, the
complexity and difficulty of extending and maintaining such agentic workflows
have become a significant challenge, particularly as algorithms and
architectures continue to advance. To address this growing complexity,
TinyScientist identifies the essential components of the automatic research
workflow and proposes an interactive, extensible, and controllable framework
that easily adapts to new tools and supports iterative growth. We provide an
open-source codebase, an interactive web demonstration, and a PyPI Python
package to make state-of-the-art auto-research pipelines broadly accessible to
every researcher and developer.

</details>


### [122] [Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?](https://arxiv.org/abs/2510.06594)
*Sri Durga Sai Sowmya Kadali,Evangelos E. Papalexakis*

Main category: cs.CL

TL;DR: 用户通过精心设计的提示词利用大型语言模型（LLMs）来获取受限制或敏感的输出，即“越狱”。尽管存在防御机制，但攻击者不断开发新的提示技术，现有模型均无法完全抵抗。本研究通过检查LLMs的内部表征，特别是隐藏层对越狱提示和良性提示的响应，来研究越狱现象。我们分析了GPT-J和Mamba2模型，初步结果表明它们在不同层级的行为存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着对话式LLMs的普及和可访问性的提高，利用LLMs进行“越狱”已成为一个紧迫的问题。研究LLMs的内部表征如何响应越狱提示，对于开发更有效的防御机制至关重要。

Method: 通过检查LLMs（具体为GPT-J和Mamba2）的内部表征，重点分析隐藏层对越狱提示与良性提示的响应差异。

Result: 初步结果显示，GPT-J和Mamba2在不同层级对越狱提示和良性提示表现出不同的行为模式。

Conclusion: 研究LLMs的内部动态为开发更鲁棒的越狱检测和防御方法提供了有前景的方向。

Abstract: Jailbreaking large language models (LLMs) has emerged as a pressing concern
with the increasing prevalence and accessibility of conversational LLMs.
Adversarial users often exploit these models through carefully engineered
prompts to elicit restricted or sensitive outputs, a strategy widely referred
to as jailbreaking. While numerous defense mechanisms have been proposed,
attackers continuously develop novel prompting techniques, and no existing
model can be considered fully resistant. In this study, we investigate the
jailbreak phenomenon by examining the internal representations of LLMs, with a
focus on how hidden layers respond to jailbreak versus benign prompts.
Specifically, we analyze the open-source LLM GPT-J and the state-space model
Mamba2, presenting preliminary findings that highlight distinct layer-wise
behaviors. Our results suggest promising directions for further research on
leveraging internal model dynamics for robust jailbreak detection and defense.

</details>


### [123] [A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures](https://arxiv.org/abs/2510.06640)
*Nhat M. Hoang,Do Xuan Long,Cong-Duy Nguyen,Min-Yen Kan,Luu Anh Tuan*

Main category: cs.CL

TL;DR: State Space Models (SSMs) and Transformer-Based Models (TBMs) differ in how contextual information propagates through layers and tokens. TBMs quickly homogenize token representations, while SSMs preserve uniqueness longer before converging. This difference is attributed to architectural design in TBMs and training dynamics in SSMs, offering insights for future long-context reasoning models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to analyze and understand the flow of contextual information across layers and tokens in State Space Models (SSMs) and Transformer-Based Models (TBMs), which remains understudied despite SSMs' efficiency in long-sequence processing.

Method: The study employs a unified, token- and layer-level analysis of representation propagation in SSMs and TBMs using centered kernel alignment, stability metrics, and probing to characterize representation evolution within and across layers.

Result: The analysis reveals that TBMs rapidly homogenize token representations, with diversity reappearing in later layers, whereas SSMs maintain token uniqueness initially before converging to homogenization deeper in the network. Oversmoothing in TBMs is linked to architecture, while in SSMs it's linked to training dynamics.

Conclusion: The findings clarify the inductive biases of both SSMs and TBMs, providing insights that can inform the design of future models and training strategies for effective long-context reasoning.

Abstract: State Space Models (SSMs) have recently emerged as efficient alternatives to
Transformer-Based Models (TBMs) for long-sequence processing, offering linear
scaling and lower memory use. Yet, how contextual information flows across
layers and tokens in these architectures remains understudied. We present the
first unified, token- and layer-level analysis of representation propagation in
SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing,
we characterize how representations evolve within and across layers. We find a
key divergence: TBMs rapidly homogenize token representations, with diversity
reemerging only in later layers, while SSMs preserve token uniqueness early but
converge to homogenization deeper. Theoretical analysis and parameter
randomization further reveal that oversmoothing in TBMs stems from
architectural design, whereas in SSMs it arises mainly from training dynamics.
These insights clarify the inductive biases of both architectures and inform
future model and training designs for long-context reasoning.

</details>


### [124] [Aligning Large Language Models via Fully Self-Synthetic Data](https://arxiv.org/abs/2510.06652)
*Shangjian Yin,Zhepei Wei,Xinyu Zhu,Wei-Lin Chen,Yu Meng*

Main category: cs.CL

TL;DR: RLHF和RLAIF都需要昂贵的人工标注或外部模型，而SAO是一个完全自我的框架，可以生成提示、响应和偏好，从而实现LLM对齐。


<details>
  <summary>Details</summary>
Motivation: 传统的RLHF和RLAIF在LLM对齐方面成本高昂，需要昂贵的人工标注数据集或外部模型。

Method: SAO首先指示LLM进行角色扮演，生成提示和响应，然后进行自我评估以优化偏好。

Result: SAO在AlpacaEval~2.0等标准基准上有效提升了模型的聊天能力，并在下游客观任务上保持了强大的性能。

Conclusion: SAO为LLM对齐的自我改进提供了一个实用的解决方案。

Abstract: Traditional reinforcement learning from human feedback (RLHF) for large
language models (LLMs) relies on expensive human-annotated datasets, while
Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,
requiring the collection of diverse prompts and corresponding responses, often
necessitating external reward models or proprietary models like GPT-4 to
annotate preference pairs. In this work, we introduce Self-Alignment
Optimization (SAO), a fully self-synthetic framework for LLM alignment, where
all training data, including prompts (i.e., user queries), responses, and
preferences, are generated by the model itself. Specifically, SAO first
instructs the LLM to engage in persona role-play and generate diverse prompts
and responses, which are then self-evaluated for preference optimization.
Extensive experiments demonstrate that SAO effectively enhances the model's
chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining
strong performance on downstream objective tasks (e.g., question-answering,
math reasoning). Our work provides a practical solution for self-improvement in
aligning LLMs, and the code for reproducing our results is available at:
https://github.com/SJY8460/SAO.

</details>


### [125] [ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory](https://arxiv.org/abs/2510.06664)
*Yunzhong Xiao,Yangmin Li,Hewei Wang,Yunlong Tang,Zora Zhiruo Wang*

Main category: cs.CL

TL;DR: 使用大型语言模型（LLM）或视觉语言模型（VLM）的工具代理在文本和视觉模态的各种任务中取得了显著进展。与给出确定性输出的计算器等传统工具不同，神经网络工具在不同任务场景中的表现存在不确定性。虽然一个任务的不同工具可能在不同场景中表现出色，但现有代理通常依赖固定的工具，这限制了选择最适合特定任务的工具的灵活性。相比之下，人类通过与不同工具的交互来不断加深对其能力的理解，并在解决未来任务时运用这些知识来选择最佳工具。为了构建同样受益于这一过程的代理，我们提出了ToolMem，它使代理能够通过总结其优势和劣势并将它们存储在内存中来发展对工具能力的记忆；在推理时，代理可以从ToolMem中检索相关条目，并更准确地选择最佳工具来解决单个任务。我们在学习各种文本生成和文本到图像生成神经网络工具上评估了ToolMem。与没有记忆的通用代理相比，我们发现ToolMem增强的代理在文本和多模态生成场景中预测工具性能的准确率分别提高了14.8%和28.7%。此外，ToolMem在各自场景中通过21%和24%的绝对提升促进了多个选项之间的最佳工具选择。


<details>
  <summary>Details</summary>
Motivation: 现有代理在工具选择上缺乏灵活性，无法像人类一样通过交互学习和优化工具使用。

Method: 提出ToolMem，一个使代理能够从过去的交互中学习工具能力，总结其优缺点并存储在内存中的系统。在推理时，代理可以检索相关记忆并选择最佳工具。

Result: 在文本生成和文本到图像生成任务上，ToolMem将预测工具性能的准确率分别提高了14.8%和28.7%。在多工具选择场景中，ToolMem将最佳工具选择的准确率分别提高了21%和24%。

Conclusion: ToolMem能够显著提高代理在面对不确定神经网络工具时的性能，通过记忆和检索机制实现更优的工具选择。

Abstract: Agents utilizing tools powered by large language models (LLMs) or
vision-language models (VLMs) have demonstrated remarkable progress in diverse
tasks across text and visual modalities. Unlike traditional tools such as
calculators, which give deterministic outputs, neural tools perform uncertainly
across task scenarios. While different tools for a task may excel in varied
scenarios, existing agents typically rely on fixed tools, thus limiting the
flexibility in selecting the most suitable tool for specific tasks. In
contrast, humans snowball their understanding of the capabilities of different
tools by interacting with them, and apply this knowledge to select the optimal
tool when solving a future task. To build agents that similarly benefit from
this process, we propose ToolMem that enables agents to develop memories of
tool capabilities from previous interactions, by summarizing their strengths
and weaknesses and storing them in memory; at inference, the agent can retrieve
relevant entries from ToolMem, and select the best tool to solve individual
tasks more accurately. We evaluate ToolMem on learning varied text generation
and text-to-image generation neural tools. Compared to no-memory, generic
agents, we find ToolMem-augmented agents predict tool performance 14.8% and
28.7% more accurately across text and multimodal generation scenarios.
Moreover, ToolMem facilitates optimal tool selection among multiple choices by
21% and 24% absolute increases in respective scenarios.

</details>


### [126] [PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch](https://arxiv.org/abs/2510.06670)
*Shangjian Yin,Shining Liang,Wenbiao Ding,Yuli Qian,Zhouxing Shi,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: RLHF是LLM对齐的基础，但依赖高质量指令数据。现有数据集多为私有或需要昂贵标注，限制了可复现性和可扩展性。RLAIF也存在数据质量问题。PiKa是一个数据高效的专家级对齐数据集系列，PiKa-SFT仅用3万SFT示例，优于使用更多数据训练的模型，甚至超越了Llama-3-8B-Instruct。该研究表明，少量数据即可实现高质量对齐，为开源LLM对齐提供了可扩展的途径。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法（如RLHF）依赖昂贵或私有的人工标注数据，限制了可复现性和可扩展性，且数据需求量大。需要更经济、高效的数据集来支持开源社区的LLM对齐。

Method: 提出PiKa数据集系列，特别是PiKa-SFT，该数据集仅使用3万个SFT示例。在Llama-3-8B-Base和Qwen2.5系列模型上进行微调，并与使用更大规模数据集（包括超过1000万个专有示例）训练的模型进行比较。

Result: 在AlpacaEval 2.0和Arena-Hard基准测试中，使用PiKa-SFT微调的模型优于使用更大、同类公开数据集训练的模型，甚至在Llama-3-8B-Instruct上取得了更好的性能。在Qwen2.5系列模型上也观察到了一致的性能提升。

Conclusion: 高质量的LLM对齐可以使用显著减少的数据量来实现，PiKa数据集为资源有限的社区提供了一个可扩展的解决方案，促进了开源LLM的发展。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone
for aligning large language models (LLMs). However, its effectiveness depends
on high-quality instruction data. Most existing alignment datasets are either
private or require costly human annotation, which limits reproducibility and
scalability. Even with Reinforcement Learning from AI Feedback (RLAIF),
concerns about data quality remain. Moreover, it is unclear how much data is
actually required to fine-tune a base model into a strong instruction-following
model. Current approaches often rely on over 300k examples even at the
supervised fine-tuning (SFT) stage, yet they still underperform compared to
proprietary models, creating barriers for academic and resource-limited
communities. To address this gap, we introduce PiKa, a data-efficient family of
expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only
30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through
evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets,
we show that PiKa-SFT outperforms models trained on much larger data. On
AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses
the official Llama-3-8B-Instruct model trained on over 10 million proprietary
examples. We further extend our study by training the Qwen2.5 series (0.5B to
7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that
high-quality alignment can be achieved with significantly less data, offering a
scalable path for open-source LLM alignment. Code and data:
https://github.com/SJY8460/PiKa.

</details>


### [127] [Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback](https://arxiv.org/abs/2510.06677)
*Yisha Wu,Cen,Zhao,Yuanpei Cao,Xiaoqing Su,Yashar Mehdad,Mindy Ji,Claire Na Cheng*

Main category: cs.CL

TL;DR: 该系统通过增量式生成摘要来帮助客服代理，减少他们的工作负担和重复查阅信息，提高了效率和满意度。


<details>
  <summary>Details</summary>
Motivation: 客服代理在处理对话时需要频繁切换上下文并重复查阅信息，这降低了工作效率。本研究旨在通过引入一种增量式摘要系统来解决这个问题。

Method: 本研究结合了使用微调后的Mixtral-8x7B模型进行连续笔记生成和使用DeBERTa-based分类器过滤无关内容。代理的编辑可以优化在线笔记的生成，并定期用于离线模型的再训练，形成了一个完整的代理编辑反馈循环。

Result: 该系统已投入生产使用，与批量摘要相比，案件处理时间减少了3%，在复杂案件中减少高达9%。此外，通过调查显示代理的满意度评分很高。

Conclusion: 增量式摘要结合持续反馈能够有效提高摘要质量和代理生产力，并可大规模应用。

Abstract: We introduce an incremental summarization system for customer support agents
that intelligently determines when to generate concise bullet notes during
conversations, reducing agents' context-switching effort and redundant review.
Our approach combines a fine-tuned Mixtral-8x7B model for continuous note
generation with a DeBERTa-based classifier to filter trivial content. Agent
edits refine the online notes generation and regularly inform offline model
retraining, closing the agent edits feedback loop. Deployed in production, our
system achieved a 3% reduction in case handling time compared to bulk
summarization (with reductions of up to 9% in highly complex cases), alongside
high agent satisfaction ratings from surveys. These results demonstrate that
incremental summarization with continuous feedback effectively enhances summary
quality and agent productivity at scale.

</details>


### [128] [Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks](https://arxiv.org/abs/2510.06695)
*Qinhao Zhou,Xiang Xiang,Kun He,John E. Hopcroft*

Main category: cs.CL

TL;DR: LLM提示工程已从手动设计发展到基于模型的优化。对于机器翻译等NLG任务，输入部分至关重要，但现有方法主要集中于优化指令部分，不适用于机器翻译。本研究提出了一种新颖的提示优化方法，专门针对机器翻译任务，使用基于反向翻译策略训练的小参数模型，降低了单任务优化的开销，并取得了高效的性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法主要关注通用任务的指令优化，需要大型LLM作为辅助，并且不适用于输入部分至关重要的机器翻译任务。

Method: 提出一种新颖的提示优化方法，使用基于反向翻译策略训练的小参数模型，针对机器翻译任务进行优化。

Result: 该方法降低了单任务优化的训练开销，并取得了高效的性能。

Conclusion: 所提出的方法能够有效优化机器翻译任务的提示，并且可以扩展到其他下游任务。

Abstract: In recent years, the growing interest in Large Language Models (LLMs) has
significantly advanced prompt engineering, transitioning from manual design to
model-based optimization. Prompts for LLMs generally comprise two components:
the \textit{instruction}, which defines the task or objective, and the
\textit{input}, which is tailored to the instruction type. In natural language
generation (NLG) tasks such as machine translation, the \textit{input}
component is particularly critical, while the \textit{instruction} component
tends to be concise. Existing prompt engineering methods primarily focus on
optimizing the \textit{instruction} component for general tasks, often
requiring large-parameter LLMs as auxiliary tools. However, these approaches
exhibit limited applicability for tasks like machine translation, where the
\textit{input} component plays a more pivotal role. To address this limitation,
this paper introduces a novel prompt optimization method specifically designed
for machine translation tasks. The proposed approach employs a small-parameter
model trained using a back-translation-based strategy, significantly reducing
training overhead for single-task optimization while delivering highly
effective performance. With certain adaptations, this method can also be
extended to other downstream tasks.

</details>


### [129] [How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects](https://arxiv.org/abs/2510.06700)
*Leonardo Bertolazzi,Sandro Pezzelle,Raffaelle Bernardi*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Both humans and large language models (LLMs) exhibit content effects: biases
in which the plausibility of the semantic content of a reasoning problem
influences judgments regarding its logical validity. While this phenomenon in
humans is best explained by the dual-process theory of reasoning, the
mechanisms behind content effects in LLMs remain unclear. In this work, we
address this issue by investigating how LLMs encode the concepts of validity
and plausibility within their internal representations. We show that both
concepts are linearly represented and strongly aligned in representational
geometry, leading models to conflate plausibility with validity. Using steering
vectors, we demonstrate that plausibility vectors can causally bias validity
judgements, and vice versa, and that the degree of alignment between these two
concepts predicts the magnitude of behavioral content effects across models.
Finally, we construct debiasing vectors that disentangle these concepts,
reducing content effects and improving reasoning accuracy. Our findings advance
understanding of how abstract logical concepts are represented in LLMs and
highlight representational interventions as a path toward more logical systems.

</details>


### [130] [Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management](https://arxiv.org/abs/2510.06727)
*Miao Lu,Weiwei Sun,Weihua Du,Zhan Ling,Xuesong Yao,Kang Liu,Jiecao Chen*

Main category: cs.CL

TL;DR: LLM代理的长时序多轮工具使用中的上下文长度限制是一个关键瓶颈。我们提出了基于摘要的上下文管理方法，通过LLM生成的摘要来压缩工具使用历史，从而在保持任务相关信息的同时，使代理能够超越固定的上下文窗口。该方法通过策略梯度表示进行端到端优化，并实例化为SUPO算法。实验表明SUPO在提高成功率和降低上下文长度方面优于基线方法，并且在复杂任务中，扩展测试时的摘要轮数可以进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习（RL）管道在长时序多轮工具使用中存在指令遵循能力下降、成本过高和上下文长度限制严格等问题。

Method: 提出基于摘要的上下文管理方法，周期性地使用LLM生成的摘要压缩工具使用历史，以保持紧凑的上下文并使代理能够超越固定上下文窗口。在此基础上，推导了策略梯度表示，实现了工具使用行为和摘要策略的端到端优化。将该框架实例化为SUPO算法。

Result: SUPO在交互式函数调用和搜索任务中显著提高了成功率，同时保持或降低了工作上下文长度。对于复杂的搜索任务，SUPO在测试时扩展摘要轮数超过训练时可以进一步提高评估性能。

Conclusion: 基于摘要的上下文管理是一种原则性且可扩展的方法，可以训练RL代理以超越固定的上下文长度限制。

Abstract: We study reinforcement learning (RL) fine-tuning of large language model
(LLM) agents for long-horizon multi-turn tool use, where context length quickly
becomes a fundamental bottleneck. Existing RL pipelines can suffer from
degraded instruction following, excessive rollout costs, and most importantly,
strict context limits. To address these challenges, we introduce
summarization-based context management to training. In specific, it
periodically compresses the tool using history by LLM-generated summaries that
retain task-relevant information to keep a compact context while enabling the
agent to scale beyond the fixed context window. Building on this formulation,
we derive a policy gradient representation that seamlessly enables standard LLM
RL infrastructures to optimize both tool-use behaviors as well as summarization
strategies in an end-to-end fashion. We instantiate this framework with
\underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization
(\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond
a fixed context limit. Experiments on interactive function calling and
searching tasks demonstrate that \texttt{SUPO} significantly improves the
success rate while maintaining the same or even lower working context length
compared to baselines. We also demonstrate that for complex searching tasks,
\texttt{SUPO} can further improve the evaluation performance when scaling
test-time maximum round of summarization beyond that of training time. Our
results establish summarization-based context management as a principled and
scalable approach for training RL agents beyond a fixed context length limit.

</details>


### [131] [PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs](https://arxiv.org/abs/2510.06730)
*Manuel Frank,Haithem Afli*

Main category: cs.CL

TL;DR: 使用动态、随机的评估协议PTEB，而不是静态基准，来评估句子嵌入模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 评估句子嵌入模型在现实世界中的鲁棒性，并解决在固定测试集上重复评估可能导致的性能虚高问题。

Method: 引入一种基于LLM的成本效益方法，生成语义上保存但词元上多样化的释义，并在一系列MTEB任务和多语言数据集上进行评估。

Result: 句子嵌入模型的性能对词元空间的改变很敏感，即使语义保持不变。小型模型和大型模型受到的影响程度相当。所提出的方法在统计上是稳健的。

Conclusion: 建议一种新的NLP评估范式，该范式侧重于动态、随机评估，而不是依赖静态基准。

Abstract: Current evaluations of sentence embedding models typically rely on static
test beds such as the Massive Text Embedding Benchmark (MTEB). While
invaluable, repeated tuning on a fixed suite can inflate reported performance
and obscure real-world robustness. We introduce the Paraphrasing Text Embedding
Benchmark (PTEB), a dynamic protocol that stochastically generates
meaning-preserving paraphrases at evaluation time and aggregates results across
multiple runs. Using a cost-efficient LLM-based method grounded in semantic
textual similarity gold ratings, we show that LLMs generate token-diverse but
semantically preserving, paraphrases. Across 7 MTEB tasks, we validate our
hypothesis that the performance of sentence encoders is sensitive to changes in
token space even when semantics remain fixed. We also observe that smaller
models are not disproportionately affected relative to larger ones. Our results
are statistically robust over multiple runs and we extended our experiments to
3 multilingual datasets covering 10 languages. More generally, we aim to
propose a new evaluation paradigm in NLP that relies less on static,
pre-defined benchmarks but shifts towards dynamic, stochastic evaluation
leveraging eval-time compute.

</details>


### [132] [Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization](https://arxiv.org/abs/2510.06732)
*Tiancheng Xing,Jerry Li,Yixuan Du,Xiyang Hu*

Main category: cs.CL

TL;DR: LLM 驱动的信息检索中的排名行为易受提示操纵。我们提出了 Rank Anything First (RAF)，一种通过生成简洁的文本扰动来优先考虑目标项目，同时保持自然性的两阶段优化方法。RAF 通过最大化排名效果和保持语言自然性这两个目标来逐个生成排名提升提示。


<details>
  <summary>Details</summary>
Motivation: LLM 被越来越多地用作信息检索中的重新排序器，但它们的排名行为可能受到小的、听起来自然的提示的操纵。

Method: RAF 是一种两阶段的令牌优化方法。第一阶段使用贪婪坐标梯度，通过结合排名目标梯度和可读性分数来筛选当前位置的候选令牌。第二阶段使用基于熵的动态加权方案，在精确排名和可读性损失下评估这些候选者，并通过温度控制的采样来选择一个令牌。RAF 在双重目标的指导下逐个生成排名提升提示：最大化排名效果和保持语言自然性。

Result: 实验表明，RAF 使用自然语言显著提高了目标项目的排名，并且在提升目标项目和保持自然性方面比现有方法更具鲁棒性。

Conclusion: 基于 LLM 的重新排序极易受到对抗性操纵，这对现代检索系统的可信度和鲁棒性提出了新的挑战。

Abstract: Large language models (LLMs) are increasingly used as rerankers in
information retrieval, yet their ranking behavior can be steered by small,
natural-sounding prompts. To expose this vulnerability, we present Rank
Anything First (RAF), a two-stage token optimization method that crafts concise
textual perturbations to consistently promote a target item in LLM-generated
rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate
Gradient to shortlist candidate tokens at the current position by combining the
gradient of the rank-target with a readability score; Stage 2 evaluates those
candidates under exact ranking and readability losses using an entropy-based
dynamic weighting scheme, and selects a token via temperature-controlled
sampling. RAF generates ranking-promoting prompts token-by-token, guided by
dual objectives: maximizing ranking effectiveness and preserving linguistic
naturalness. Experiments across multiple LLMs show that RAF significantly
boosts the rank of target items using naturalistic language, with greater
robustness than existing methods in both promoting target items and maintaining
naturalness. These findings underscore a critical security implication:
LLM-based reranking is inherently susceptible to adversarial manipulation,
raising new challenges for the trustworthiness and robustness of modern
retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.

</details>


### [133] [AWM: Accurate Weight-Matrix Fingerprint for Large Language Models](https://arxiv.org/abs/2510.06738)
*Boyi Zeng,Lin Chen,Ziwei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出了一种无需训练即可识别模型来源（从头开始训练还是基于现有模型）的指纹识别方法，该方法基于权重矩阵，并利用线性分配问题（LAP）和无偏中心核对齐（CKA）来应对训练后的各种模型转换，在广泛的模型对测试中表现出高鲁棒性和低误报率，且计算速度快。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的知识产权保护很重要，因为它们的训练需要大量资源。因此，迫切需要模型所有者和第三方来确定可疑的LLM是從頭開始訓練的還是源自現有的基礎模型。然而，模型通常经历的密集训练后过程（如監督微調、廣泛的持續預訓練、強化學習、多模態擴展、剪枝和升級）對可靠識別構成了重大挑戰。

Method: 提出了一种基于权重矩阵的、无需训练的指纹识别方法。利用线性分配问题（LAP）和无偏中心核对齐（CKA）相似性来消除参数操作的影响，从而获得高度鲁棒且高保真的相似性度量。

Result: 在包含60个正样本对和90个负样本对的综合测试集中，该方法在所有六种上述的训练后类别中都表现出了出色的鲁棒性，并且误报风险接近于零。在所有分类指标上均获得满分，为可靠的模型溯源验证奠定了坚实的基础。此外，整个计算过程在NVIDIA 3090 GPU上可在30秒内完成。

Conclusion: 所提出的方法能够可靠地验证模型的来源，即使在经历了各种训练后处理的情况下，也具有高度的鲁棒性、准确性并且计算效率高。

Abstract: Protecting the intellectual property of large language models (LLMs) is
crucial, given the substantial resources required for their training.
Consequently, there is an urgent need for both model owners and third parties
to determine whether a suspect LLM is trained from scratch or derived from an
existing base model. However, the intensive post-training processes that models
typically undergo-such as supervised fine-tuning, extensive continued
pretraining, reinforcement learning, multi-modal extension, pruning, and
upcycling-pose significant challenges to reliable identification. In this work,
we propose a training-free fingerprinting method based on weight matrices. We
leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel
Alignment (CKA) similarity to neutralize the effects of parameter
manipulations, yielding a highly robust and high-fidelity similarity metric. On
a comprehensive testbed of 60 positive and 90 negative model pairs, our method
demonstrates exceptional robustness against all six aforementioned
post-training categories while exhibiting a near-zero risk of false positives.
By achieving perfect scores on all classification metrics, our approach
establishes a strong basis for reliable model lineage verification. Moreover,
the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is
available at https://github.com/LUMIA-Group/AWM.

</details>


### [134] [TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs](https://arxiv.org/abs/2510.06747)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: 我们提出了一种无需训练和标签的短文本聚类方法，可用于任何现有嵌入器之上，以解决商业场景中用户话语的意图聚类问题。


<details>
  <summary>Details</summary>
Motivation: 在客户服务聊天机器人等商业场景中，需要对大量用户进行意图聚类，但通常缺乏标签数据且无法预知聚类数量。

Method: 该方法基于迭代向量更新，首先构建基于代表性文本的稀疏向量，然后通过大语言模型的指导进行迭代优化。

Result: 在各种数据集和较小的语言模型上进行的实验表明，该方法具有模型无关性，可应用于任何嵌入器，并与现有的最先进方法（包括使用对比学习的方法）相比，在不假设先验知识的情况下，取得了相当或更优越的结果。

Conclusion: 该方法无需训练和标签，具有模型无关性、可扩展性，并能降低计算成本，更适用于真实世界的场景，并能与各种聚类方法结合使用。

Abstract: In this paper, we propose a training-free and label-free method for short
text clustering that can be used on top of any existing embedder. In the
context of customer-facing chatbots, companies are dealing with large amounts
of user utterances that need to be clustered according to their intent. In
these commercial settings, no labeled data is typically available, and the
number of clusters is not known. Our method is based on iterative vector
updating: it constructs sparse vectors based on representative texts, and then
iteratively refines them through LLM guidance. Our method achieves comparable
or superior results to state-of-the-art methods that use contrastive learning,
but without assuming prior knowledge of clusters or labels. Experiments on
diverse datasets and smaller LLMs show that our method is model agnostic and
can be applied to any embedder, with relatively small LLMs, and different
clustering methods. We also show that our method scales to large datasets,
reducing the computational cost of the LLM. These low-resource, adaptable
settings and the scalability of our method make it more aligned with real-world
scenarios than existing clustering methods.

</details>


### [135] [A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction](https://arxiv.org/abs/2510.06749)
*Eitan Klinger,Zihao Huang,Tran Minh Nguyen,Emma Jayeon Park,Yige Chen,Yang Gu,Qingyu Gao,Siliang Liu,Mengyang Qiu,Jungyeul Park*

Main category: cs.CL

TL;DR: 现有的语法纠错评估指标过于单一，无法体现人类纠错的多样性。本文提出了一种基于流畅度的多参考评估框架，将N-gram相似度视为多重合法纠错的聚合问题。该框架通过四种聚合策略（选择最佳、简单平均、加权平均、合并计数）实例化了GLEU指标，并分析了它们的有界性、单调性和对参考变化的敏感性。在多种语言语料库上的实证结果表明，这些策略能够捕捉流畅度和覆盖率的互补方面。


<details>
  <summary>Details</summary>
Motivation: 现有的语法纠错评估框架大多基于编辑，以英语为中心，并且依赖于系统和参考编辑之间的严格对齐，这限制了它们在多语言和生成场景中的应用。需要一种能够反映人类纠错多样性的评估指标，而不是偏袒单一参考。 

Method: 本文提出了一个形式化的“基于流畅度的多参考评估”框架，将N-gram相似度视为在多个合法纠错上的聚合问题。在此框架下，通过四种聚合策略——选择最佳、简单平均、加权平均和合并计数——来实例化GLEU指标，并分析了它们的有界性、单调性和对参考变化的敏感性。

Result: 在捷克语、爱沙尼亚语、乌克兰语和中文语料库上的实证结果表明，所提出的四种聚合策略能够捕捉流畅度和覆盖率的互补方面。

Conclusion: 该框架将多参考评估统一为一个有原则的、面向流畅度的方法，在不惩罚合法变异的情况下，纳入了语言多样性。

Abstract: Evaluating grammatical error correction requires metrics that reflect the
diversity of valid human corrections rather than privileging a single
reference. Existing frameworks, largely edit-based and English-centric, rely on
rigid alignments between system and reference edits, limiting their
applicability in multilingual and generative settings. This paper introduces a
formal framework for \textit{fluency-based multi-reference evaluation}, framing
$n$-gram similarity as an aggregation problem over multiple legitimate
corrections. Within this formulation, we instantiate GLEU through four
aggregation strategies--\textsc{select-best}, \textsc{simple-average},
\textsc{weighted-average}, and \textsc{merged-counts}--and analyze their
properties of boundedness, monotonicity, and sensitivity to reference
variation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora
show that these strategies capture complementary aspects of fluency and
coverage. The framework unifies multi-reference evaluation into a principled,
fluency-oriented approach that incorporates linguistic diversity without
penalizing legitimate variation.

</details>


### [136] [Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs](https://arxiv.org/abs/2510.06750)
*Jaeseong Lee,Dayoung Kwon,seung-won hwang*

Main category: cs.CL

TL;DR: LRM在结构化任务中表现出色，但可能过度思考。提出了一种轻量级、无需训练的监管策略，通过选择性地“遗忘”LRM的推理能力来优化推理，从而在不牺牲推理能力的情况下减少计算量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在结构化任务中表现出色，但存在过度思考的问题，导致性能下降和资源浪费。现有方法（如部署多个模型并进行路由）可能成本高昂或不切实际。

Method: 提出了一种叠加部署策略，使用一种轻量级的、无需训练的监管方法来优化推理，通过开关一个模型来实现。该方法在推理时选择性地“遗忘”LRM的部分推理能力，从而在扩展计算的同时保持推理能力。通过分析奇异值的累积能量，确定最优的低秩投影来适度调整推理。

Result: 通过分析奇异值的累积能量，识别出最优的低秩投影，以适度调整推理。

Conclusion: 所提出的叠加部署策略和训练无关的监管方法，能够有效地优化推理过程，在降低计算量的同时保持大型推理模型的性能。

Abstract: Large Reasoning Models (LRMs) excel in structured tasks by emulating
deliberate human reasoning but often suffer from overthinking, degrading
performance and wasting resources. One possible baseline is to deploy both LLM
and LRM, then route input by predicting whether it requires reasoning and may
cause overthinking. However, deploying multiple models can be costly or
impractical. We propose a superposed deployment strategy with a lightweight,
training-free regulation to optimize inference by switching one model on and
off. Instead of routing, we selectively unlearn from LRM at inference, scaling
down computation while preserving reasoning. By analyzing the cumulative energy
of singular values, we identify optimal low-rank projections to adjust
reasoning just right.

</details>


### [137] [Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition](https://arxiv.org/abs/2510.06774)
*Lei Xu,Pierre Beckmann,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 开发了一个自适应、多范式的神经符号推理框架，该框架能自动识别自然语言中的形式化推理策略，并动态选择和应用专门的形式化逻辑求解器，从而在各项推理任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前神经符号NLP方法集成目标求解器是静态的，限制了多种形式化推理策略的应用。本研究旨在解决这一局限性。

Method: 提出一个自适应、多范式的神经符号推理框架，能够自动从自然语言问题中识别形式化推理策略，并通过自动形式化接口动态选择和应用专门的形式化逻辑求解器。

Result: 在个体和多范式推理任务的实验中，LLM预测推理策略的准确率超过90%；本框架在推理任务中比GPT-4o和DeepSeek-V3.1分别高出27%和6%；自适应推理还能提升纯LLM方法在不同设置下的表现（如zero-shot, CoT, symbolic CoT）；虽然小模型在自适应神经符号推理方面有困难，但可以通过模型后训练来改进；

Conclusion: 该工作为自适应LLM-符号推理奠定了基础，为在异构推理挑战中统一物质推理和形式化推理提供了途径。

Abstract: Neuro-symbolic NLP methods aim to leverage the complementary strengths of
large language models and formal logical solvers. However, current approaches
are mostly static in nature, i.e., the integration of a target solver is
predetermined at design time, hindering the ability to employ diverse formal
inference strategies. To address this, we introduce an adaptive,
multi-paradigm, neuro-symbolic inference framework that: (1) automatically
identifies formal reasoning strategies from problems expressed in natural
language; and (2) dynamically selects and applies specialized formal logical
solvers via autoformalization interfaces. Extensive experiments on individual
and multi-paradigm reasoning tasks support the following conclusions: LLMs are
effective at predicting the necessary formal reasoning strategies with an
accuracy above 90 percent. This enables flexible integration with formal
logical solvers, resulting in our framework outperforming competing baselines
by 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively.
Moreover, adaptive reasoning can even positively impact pure LLM methods,
yielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT
settings with GPT-4o. Finally, although smaller models struggle with adaptive
neuro-symbolic reasoning, post-training offers a viable path to improvement.
Overall, this work establishes the foundations for adaptive LLM-symbolic
reasoning, offering a path forward for unifying material and formal inferences
on heterogeneous reasoning challenges.

</details>


### [138] [Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness](https://arxiv.org/abs/2510.06780)
*Luca Giordano,Simon Razniewski*

Main category: cs.CL

TL;DR: LLM知识物化技术尚待探索，本文通过miniGPTKBs系统研究了其终止性、可复现性和鲁棒性，发现在种子、语言、随机性和模型等因素影响下，终止率高但可复现性参差不齐，鲁棒性因扰动类型而异。


<details>
  <summary>Details</summary>
Motivation: LLM拥有大量事实知识，但目前缺乏有效的方法来衡量和系统化这些知识，将LLM知识转化为结构化格式（如通过递归提取）仍有待深入研究，特别是关于提取过程是否能终止、输出是否可复现以及对各种变化的鲁棒性等关键问题。

Method: 使用miniGPTKBs（领域特定的、可处理的子爬取）系统地研究LLM知识物化，分析其在终止性、可复现性和鲁棒性方面的表现。研究考察了四种变体（种子、语言、随机性、模型）和三个代表性领域（历史、娱乐、金融），并使用了产量、词汇相似性和语义相似性三类指标。

Result: 研究结果表明：(i) 知识提取的终止率很高，但受模型影响；(ii) 可复现性表现不一；(iii) 鲁棒性因扰动类型而异，对于种子和温度（随机性）较高，对于语言和模型较低。

Conclusion: LLM知识物化技术能够可靠地提取核心知识，但同时也揭示了其重要的局限性。

Abstract: Large Language Models (LLMs) encode substantial factual knowledge, yet
measuring and systematizing this knowledge remains challenging. Converting it
into structured format, for example through recursive extraction approaches
such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key
open questions include whether such extraction can terminate, whether its
outputs are reproducible, and how robust they are to variations. We
systematically study LLM knowledge materialization using miniGPTKBs
(domain-specific, tractable subcrawls), analyzing termination, reproducibility,
and robustness across three categories of metrics: yield, lexical similarity,
and semantic similarity. We experiment with four variations (seed, language,
randomness, model) and three illustrative domains (from history, entertainment,
and finance). Our findings show (i) high termination rates, though
model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies
by perturbation type: high for seeds and temperature, lower for languages and
models. These results suggest that LLM knowledge materialization can reliably
surface core knowledge, while also revealing important limitations.

</details>


### [139] [Overview of the Plagiarism Detection Task at PAN 2025](https://arxiv.org/abs/2510.06805)
*André Greiner-Petter,Maik Fröbe,Jan Philip Wahle,Terry Ruas,Bela Gipp,Akiko Aizawa,Martin Potthast*

Main category: cs.CL

TL;DR: PAN 2025 的生成式抄袭检测任务旨在识别科学文章中的自动生成文本抄袭，并将其与相应来源进行匹配。研究人员使用 Llama、DeepSeek-R1 和 Mistral 三个大型语言模型创建了一个新颖的大规模自动生成抄袭数据集。该论文概述了数据集的创建过程，总结和比较了所有参与者和四个基线的 পরীক্ষা结果，并评估了 PAN 2015 抄袭检测任务的结果，以解释所提出方法的鲁棒性。研究发现，目前的方法未能吸引多样化的方法，因为基于嵌入向量的朴素语义相似性方法提供了高达 0.8 的召回率和 0.5 的精确率。然而，这些方法在 2015 年的数据集上的表现明显不佳，表明缺乏泛化能力。


<details>
  <summary>Details</summary>
Motivation: 创建了一个新颖的大规模数据集，用于 PAN 2025 的生成式抄袭检测任务，该任务旨在识别科学文章中的自动生成文本抄袭，并将其与相应来源进行匹配。

Method: 使用 Llama、DeepSeek-R1 和 Mistral 三个大型语言模型创建了一个新颖的大规模自动生成抄袭数据集。此外，还评估了 PAN 2015 抄袭检测任务的结果，以解释所提出方法的鲁棒性。

Result: 基于嵌入向量的朴素语义相似性方法提供了高达 0.8 的召回率和 0.5 的精确率。然而，这些方法在 2015 年的数据集上的表现明显不佳。

Conclusion: 目前的方法未能吸引多样化的方法，并且在 2015 年的数据集上的表现明显不佳，表明缺乏泛化能力。

Abstract: The generative plagiarism detection task at PAN 2025 aims at identifying
automatically generated textual plagiarism in scientific articles and aligning
them with their respective sources. We created a novel large-scale dataset of
automatically generated plagiarism using three large language models: Llama,
DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation
of this dataset, summarize and compare the results of all participants and four
baselines, and evaluate the results on the last plagiarism detection task from
PAN 2015 in order to interpret the robustness of the proposed approaches. We
found that the current iteration does not invite a large variety of approaches
as naive semantic similarity approaches based on embedding vectors provide
promising results of up to 0.8 recall and 0.5 precision. In contrast, most of
these approaches underperform significantly on the 2015 dataset, indicating a
lack in generalizability.

</details>


### [140] [BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods](https://arxiv.org/abs/2510.06811)
*Philipp Mondorf,Mingyang Wang,Sebastian Gerstner,Ahmad Dawar Hakimi,Yihong Liu,Leonor Veloso,Shijia Zhou,Hinrich Schütze,Barbara Plank*

Main category: cs.CL

TL;DR: Ensembling circuit localization methods improves performance in identifying specific subnetworks within LLMs.


<details>
  <summary>Details</summary>
Motivation: Investigate whether ensembling two or more circuit localization methods can improve performance for localizing circuits within LLMs.

Method: Explored two ensembling variants: parallel (combining attribution scores by averaging, min, or max) and sequential (using EAP-IG scores as a warm start for edge pruning). Evaluated in the BlackboxNLP 2025 MIB Shared Task.

Result: Both parallel and sequential ensembling yielded notable gains on benchmark metrics, leading to more precise circuit identification. A parallel ensemble including the sequential ensemble achieved the best results.

Conclusion: Ensembling circuit localization methods, particularly a parallel ensemble of various methods including the sequential ensemble, offers a more precise approach to circuit identification in LLMs.

Abstract: The Circuit Localization track of the Mechanistic Interpretability Benchmark
(MIB) evaluates methods for localizing circuits within large language models
(LLMs), i.e., subnetworks responsible for specific task behaviors. In this
work, we investigate whether ensembling two or more circuit localization
methods can improve performance. We explore two variants: parallel and
sequential ensembling. In parallel ensembling, we combine attribution scores
assigned to each edge by different methods-e.g., by averaging or taking the
minimum or maximum value. In the sequential ensemble, we use edge attribution
scores obtained via EAP-IG as a warm start for a more expensive but more
precise circuit identification method, namely edge pruning. We observe that
both approaches yield notable gains on the benchmark metrics, leading to a more
precise circuit identification approach. Finally, we find that taking a
parallel ensemble over various methods, including the sequential ensemble,
achieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB
Shared Task, comparing ensemble scores to official baselines across multiple
model-task combinations.

</details>


### [141] [Adaptive Tool Generation with Models as Tools and Reinforcement Learning](https://arxiv.org/abs/2510.06825)
*Chenpeng Wang,Xiaojie Cheng,Chunye Wang,Linfeng Yang,Lei Zhang*

Main category: cs.CL

TL;DR: MTR是一个模拟优先的训练框架，用于工具增强的语言模型推理，通过学习完整的ReAct追踪和模拟观察来克服对实时API的依赖，实现了与实时API系统相当的性能。


<details>
  <summary>Details</summary>
Motivation: 实时API依赖会给工具增强语言模型带来可扩展性和可靠性挑战，MTR旨在解决这些问题。

Method: MTR采用多智能体架构，包括生成工具接口的ToolMaker、产生思考-行动-观察序列的AutoAgent以及模拟响应的ToolActor。训练分两阶段：第一阶段监督微调（SFT）学习“追踪语法”，第二阶段群组相对策略优化（GRPO）通过结合答案正确性和内部一致性的追踪奖励来优化策略。

Result: MTR在四个涉及多跳问答的数据集（HotpotQA, MuSiQue, 2WikiMultiHopQA, Bamboogle）上取得了与实时API系统相当的精确匹配（EM）分数，并在推理密集型任务上表现出色。

Conclusion: MTR证明了无需实时交互，仅通过结构化追踪就可以有效地学习工具推理。

Abstract: Tool-augmented language models have demonstrated strong capabilities, but
their reliance on live API access creates scalability and reliability
challenges during training and deployment. We propose MTR, a simulation-first
training framework for tool-augmented reasoning. Instead of relying on live
APIs, MTR learns from complete ReAct traces with schema-validated, simulated
observations. Our approach operates through a multi-agent architecture where a
ToolMaker generates task-specific, OpenAI-compatible tool interfaces, an
AutoAgent produces structured think-act-observe sequences, and a ToolActor
simulates realistic responses. Training proceeds in two stages: Stage-1
Supervised Fine-Tuning (SFT) teaches 'trace grammar' from complete reasoning
sequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy
with a composite trace reward that balances answer correctness and internal
consistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue,
2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to
live-API systems and excels on reasoning-intensive tasks, suggesting that
effective tool reasoning can be learned from structured traces without live
interactions.

</details>


### [142] [Mid-Training of Large Language Models: A Survey](https://arxiv.org/abs/2510.06826)
*Kaixiang Mo,Yuxin Shi,Weiwei Weng,Zhiqiang Zhou,Shuman Liu,Haibo Zhang,Anxiang Zeng*

Main category: cs.CL

TL;DR: LLM 的中期训练（mid-training）是提升模型性能的关键阶段，通过数据质量优化、学习率调整和长上下文扩展来提高泛化能力和抽象能力。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 的开发主要依赖预训练和微调，但忽视了中期训练阶段的重要性，该阶段对于缓解噪声数据的影响、稳定收敛和扩展模型能力至关重要。尽管中期训练在 SOTA 模型中广泛应用，但缺乏对其作为统一范式的系统性总结。

Method: 本文首次提出了 LLM 中期训练的分类法，涵盖数据分布、学习率调度和长上下文扩展三个方面。研究者们提炼了实践经验，整理了评估基准，并报告了不同模型之间的性能增益，以便进行结构化比较。

Result: 中期训练通过梯度噪声尺度、信息瓶颈和课程学习等机制，能够有效提升 LLM 的泛化能力和抽象能力，缓解数据质量问题，稳定训练过程，并扩展模型处理长序列的能力。

Conclusion: 中期训练是 LLM 开发中的一个重要但被忽视的阶段。本文提出的分类法和实践洞察为理解和应用中期训练提供了框架，并指出了未来研究的方向。

Abstract: Large language models (LLMs) are typically developed through large-scale
pre-training followed by task-specific fine-tuning. Recent advances highlight
the importance of an intermediate mid-training stage, where models undergo
multiple annealing-style phases that refine data quality, adapt optimization
schedules, and extend context length. This stage mitigates diminishing returns
from noisy tokens, stabilizes convergence, and expands model capability in late
training. Its effectiveness can be explained through gradient noise scale, the
information bottleneck, and curriculum learning, which together promote
generalization and abstraction. Despite widespread use in state-of-the-art
systems, there has been no prior survey of mid-training as a unified paradigm.
We introduce the first taxonomy of LLM mid-training spanning data distribution,
learning-rate scheduling, and long-context extension. We distill practical
insights, compile evaluation benchmarks, and report gains to enable structured
comparisons across models. We also identify open challenges and propose avenues
for future research and practice.

</details>


### [143] [GAMBIT+: A Challenge Set for Evaluating Gender Bias in Machine Translation Quality Estimation Metrics](https://arxiv.org/abs/2510.06841)
*Giorgos Filandrianos,Orfeas Menis Mastromichalakis,Wafaa Mohammed,Giuseppe Attanasio,Chrysoula Zerva*

Main category: cs.CL

TL;DR: 现有研究表明，自动质量评估（QE）指标也可能存在性别偏见，但现有分析受限于小规模数据集、有限的职业覆盖范围和有限的语言种类。为了解决这一问题，我们引入了一个大规模的挑战集，专门用于探测QE指标在评估包含性别模糊职业术语的译文时的行为。


<details>
  <summary>Details</summary>
Motivation: 性别偏见已在机器翻译（MT）系统中得到广泛证明，但自动质量评估（QE）指标中的偏见仍然相对未被充分研究。

Method: 通过扩展GAMBIT语料库，覆盖了三种无性别或自然性别语言和十一种具有语法性别的目标语言，形成了33个源-目标语言对。每个源文本都配有两个目标版本，仅在职业术语的语法性别（男性 vs. 女性）上有所不同，并相应调整了所有从属的语法元素。

Result: 该数据集的规模、广度和完全并行设计，使得可以按职业进行细粒度偏见分析，并系统地跨语言进行比较。

Conclusion: 一个无偏见的QE指标应该为这两个版本分配相等或接近相等的得分。

Abstract: Gender bias in machine translation (MT) systems has been extensively
documented, but bias in automatic quality estimation (QE) metrics remains
comparatively underexplored. Existing studies suggest that QE metrics can also
exhibit gender bias, yet most analyses are limited by small datasets, narrow
occupational coverage, and restricted language variety. To address this gap, we
introduce a large-scale challenge set specifically designed to probe the
behavior of QE metrics when evaluating translations containing gender-ambiguous
occupational terms. Building on the GAMBIT corpus of English texts with
gender-ambiguous occupations, we extend coverage to three source languages that
are genderless or natural-gendered, and eleven target languages with
grammatical gender, resulting in 33 source-target language pairs. Each source
text is paired with two target versions differing only in the grammatical
gender of the occupational term(s) (masculine vs. feminine), with all dependent
grammatical elements adjusted accordingly. An unbiased QE metric should assign
equal or near-equal scores to both versions. The dataset's scale, breadth, and
fully parallel design, where the same set of texts is aligned across all
languages, enables fine-grained bias analysis by occupation and systematic
comparisons across languages.

</details>


### [144] [SID: Multi-LLM Debate Driven by Self Signals](https://arxiv.org/abs/2510.06843)
*Xuhang Chen,Zhifan Song,Deyi Ji,Shuo Gao,Lanyun Zhu*

Main category: cs.CL

TL;DR: 通过利用模型和token层面的自信号，提出了一种新的多LLM辩论方法（SID），提高了准确性并减少了计算量。


<details>
  <summary>Details</summary>
Motivation: 现有的多LLM辩论方法（MAD）主要依赖外部结构，忽略了生成过程中产生的模型内部自信号，导致计算冗余和性能下降。

Method: 提出了一种新的自信号驱动的多LLM辩论（SID）方法，利用模型层面的置信度和token层面的语义焦点这两种自信号来指导辩论过程。具体来说，高置信度的模型可以提前退出，并且利用注意力机制来压缩冗余的辩论内容。

Result: 在多种LLM和多模态LLM以及多个具有挑战性的基准测试上进行了评估，结果表明该方法在准确性上优于现有的MAD技术，并减少了token消耗。

Conclusion: 利用自信号能够有效提升多Agent辩论系统的性能和效率。

Abstract: Large Language Models (LLMs) have exhibited impressive capabilities across
diverse application domains. Recent work has explored Multi-LLM Agent Debate
(MAD) as a way to enhance performance by enabling multiple LLMs to discuss and
refine responses iteratively. Nevertheless, existing MAD methods predominantly
focus on utilizing external structures, such as debate graphs, using
LLM-as-a-Judge, while neglecting the application of self signals, such as token
logits and attention, that arise during generation. This omission leads to
redundant computation and potential performance degradation. In this paper, we
shift the focus to the self signals of multi-LLM debate and introduce a
Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of
self-signals: model-level confidence and token-level semantic focus, to
adaptively guide the debate process. Our approach enables high-confidence
agents to exit early at the model level and compress the redundant debate
contents based on the attention mechanism. We evaluate our method on various
LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental
results demonstrate that our method not only outperforms existing MAD
techniques in accuracy but also reduces token consumption, highlighting the
effectiveness of utilizing self signals in enhancing both the performance and
efficiency of multi-agent debate systems. Our code will be available
at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.

</details>


### [145] [OpenJAI-v1.0: An Open Thai Large Language Model](https://arxiv.org/abs/2510.06847)
*Pontakorn Trakuekul,Attapol T. Rutherford,Jullajak Karnjanaekarin,Narongkorn Panitsrisit,Sumana Sumanakul*

Main category: cs.CL

TL;DR: OpenJAI-v1.0 是一个基于 Qwen3-14B 的开源大型语言模型，支持泰语和英语，在指令遵循、长上下文理解和工具使用方面进行了优化，性能优于其他泰语模型。


<details>
  <summary>Details</summary>
Motivation: 旨在提高大型语言模型在实际任务中的性能，特别是在泰语和英语的指令遵循、长上下文理解和工具使用方面。

Method: 在 Qwen3-14B 模型的基础上，使用精心策划的数据集进行微调，以增强其在三个关键用例中的能力。

Result: OpenJAI-v1.0 在各项基准测试中展现出比基础模型更强的能力，并且在泰语模型中表现优于其他领先的开源模型，同时避免了灾难性遗忘。

Conclusion: OpenJAI-v1.0 是一个性能优越的开源大型语言模型，为泰语人工智能社区提供了一个新的资源选择。

Abstract: We introduce OpenJAI-v1.0, an open-source large language model for Thai and
English, developed from the Qwen3-14B model. Our work focuses on boosting
performance on practical tasks through carefully curated data across three key
use cases: instruction following, long-context understanding, and tool use.
Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its
base model and outperforms other leading open-source Thai models on a diverse
suite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is
publicly released as another alternative NLP resource for the Thai AI
community.

</details>


### [146] [Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding](https://arxiv.org/abs/2510.06866)
*Wafaa Mohammed,Vlad Niculae,Chrysoula Zerva*

Main category: cs.CL

TL;DR: LLMs在机器翻译方面表现出色，但仍难以处理篇章现象。本研究提出使用质量感知解码（QAD）来提取LLM中编码的篇章知识，以提高上下文感知翻译的质量，增强语义丰富性并使其更符合人类偏好。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在机器翻译方面表现出强大的潜力，但在处理诸如跨文档的代词解析和词汇衔接等篇章现象方面仍存在不足。

Method: 提出并使用质量感知解码（QAD）方法来提取大型语言模型（LLM）中编码的篇章知识，并与其它解码方法进行比较分析。

Result: 研究表明，QAD方法在提取篇章知识方面优于其他解码方法，并且能够提升翻译的语义丰富性，使其更符合人类的偏好。

Conclusion: 所提出的QAD方法能够有效地利用LLM中蕴含的篇章知识，显著提升机器翻译在处理篇章现象时的性能。

Abstract: Large language models (LLMs) have emerged as strong contenders in machine
translation.Yet, they still struggle to adequately handle discourse phenomena,
such as pronoun resolution and lexical cohesion at the document level. In this
study, we thoroughly investigate the discourse phenomena performance of LLMs in
context-aware translation. We demonstrate that discourse knowledge is encoded
within LLMs and propose the use of quality-aware decoding (QAD) to effectively
extract this knowledge, showcasing its superiority over other decoding
approaches through comprehensive analysis. Furthermore, we illustrate that QAD
enhances the semantic richness of translations and aligns them more closely
with human preferences.

</details>


### [147] [$λ$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences](https://arxiv.org/abs/2510.06870)
*Yining Wang,Jinman Zhao,Chuangxin Zhao,Shuhao Guan,Gerald Penn,Shinan Liu*

Main category: cs.CL

TL;DR: RLHF被认为是提升LLM推理能力的有效方法，但GRPO等基于可验证奖励的强化学习方法存在长度偏见问题。本文提出了一种名为λ-GRPO的新方法，通过引入可学习参数λ来动态调整token权重，解决了长度偏见并提高了模型在数学推理任务上的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于可验证奖励的强化学习方法（如GRPO）在处理长响应时存在长度偏见，导致梯度更新不均匀。而DAPO和Dr.GRPO等改进方法虽有所改善，但仍依赖启发式方法且可解释性有限。

Method: 本文提出了一种名为λ-GRPO的新方法，通过引入一个可学习的参数λ来动态控制token级别的权重分配，从而允许模型在优化过程中学习自身的token偏好。该方法将现有框架统一在一个单一的公式下。

Result: λ-GRPO在多个数学推理基准测试中，相比于标准的GRPO和DAPO，在Qwen2.5模型的1.5B、3B和7B参数版本上，平均准确率分别提高了+1.9%、+1.0%和+1.7%。

Conclusion: λ-GRPO通过学习token偏好，有效解决了GRPO的长度偏见问题，并在数学推理任务上取得了显著的性能提升，且无需修改训练数据或增加额外计算成本，证明了其有效性和实用性。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has been the dominant
approach for improving the reasoning capabilities of Large Language Models
(LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has
simplified this paradigm by replacing the reward and value models with
rule-based verifiers. A prominent example is Group Relative Policy Optimization
(GRPO). However, GRPO inherently suffers from a length bias, since the same
advantage is uniformly assigned to all tokens of a response. As a result,
longer responses distribute the reward over more tokens and thus contribute
disproportionately to gradient updates. Several variants, such as DAPO and Dr.
GRPO, modify the token-level aggregation of the loss, yet these methods remain
heuristic and offer limited interpretability regarding their implicit token
preferences. In this work, we explore the possibility of allowing the model to
learn its own token preference during optimization. We unify existing
frameworks under a single formulation and introduce a learnable parameter
$\lambda$ that adaptively controls token-level weighting. We use $\lambda$-GRPO
to denote our method, and we find that $\lambda$-GRPO achieves consistent
improvements over vanilla GRPO and DAPO on multiple mathematical reasoning
benchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\lambda$-GRPO
improves average accuracy by $+1.9\%$, $+1.0\%$, and $+1.7\%$ compared to GRPO,
respectively. Importantly, these gains come without any modifications to the
training data or additional computational cost, highlighting the effectiveness
and practicality of learning token preferences.

</details>


### [148] [MeXtract: Light-Weight Metadata Extraction from Scientific Papers](https://arxiv.org/abs/2510.06889)
*Zaid Alyafeai,Maged S. Al-Shaibani,Bernard Ghanem*

Main category: cs.CL

TL;DR: MeXtract是一个轻量级语言模型系列，在科学论文元数据提取方面取得了最先进的性能，并且在新的模式上表现出良好的适应性。


<details>
  <summary>Details</summary>
Motivation: 由于传统方法在处理不同领域和模式变化的科学文献元数据提取方面存在局限性，因此需要更有效和通用的方法。

Method: 通过对Qwen 2.5模型进行微调，构建了一个参数量在0.5B到3B之间的MeXtract模型系列，并在MOLE基准测试及其扩展版本上进行了评估。

Result: MeXtract在MOLE基准测试中达到了最先进的性能，并且在处理包含特定模型元数据的、来自训练领域之外的样本时，也表现出了强大的泛化能力和适应性。模型在未见过的模式上也能实现高准确率。

Conclusion: MeXtract展示了在科学论文元数据提取方面的鲁棒性和适应性，其微调方法能够有效地迁移到新的模式，并且作者已公开所有相关资源以供研究社区使用。

Abstract: Metadata plays a critical role in indexing, documenting, and analyzing
scientific literature, yet extracting it accurately and efficiently remains a
challenging task. Traditional approaches often rely on rule-based or
task-specific models, which struggle to generalize across domains and schema
variations. In this paper, we present MeXtract, a family of lightweight
language models designed for metadata extraction from scientific papers. The
models, ranging from 0.5B to 3B parameters, are built by fine-tuning Qwen 2.5
counterparts. In their size family, MeXtract achieves state-of-the-art
performance on metadata extraction on the MOLE benchmark. To further support
evaluation, we extend the MOLE benchmark to incorporate model-specific
metadata, providing an out-of-domain challenging subset. Our experiments show
that fine-tuning on a given schema not only yields high accuracy but also
transfers effectively to unseen schemas, demonstrating the robustness and
adaptability of our approach. We release all the code, datasets, and models
openly for the research community.

</details>


### [149] [LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling](https://arxiv.org/abs/2510.06915)
*Zecheng Tang,Baibei Ji,Quantong Qiu,Haitian Wang,Xiaobo Liang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 该研究提出了LongRewardBench基准和LongRM训练策略，以解决现有奖励模型在长上下文一致性评估中的不足，并展示了其在提升模型长短上下文表现方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在处理长上下文场景时存在不足，无法评估模型响应与长上下文的一致性。

Method: 提出LongRewardBench基准（包含成对比较和最佳N项选择任务）以及一种多阶段训练策略，用于训练能够处理长上下文的奖励模型（LongRM）。

Result: 所提出的LongRM在长上下文评估方面表现优于现有技术，并且能够保持良好的短上下文能力。一个8B的LongRM甚至超过了更大的基线模型，并能与Gemini 2.5 Pro相媲美。

Conclusion: 该研究成功开发了解决长上下文奖励模型评估和训练问题的有效方法，显著提升了LLM在长上下文场景下的表现。

Abstract: Reward model (RM) plays a pivotal role in aligning large language model (LLM)
with human preferences. As real-world applications increasingly involve long
history trajectories, e.g., LLM agent, it becomes indispensable to evaluate
whether a model's responses are not only high-quality but also grounded in and
consistent with the provided context. Yet, current RMs remain confined to
short-context settings and primarily focus on response-level attributes (e.g.,
safety or helpfulness), while largely neglecting the critical dimension of long
context-response consistency. In this work, we introduce Long-RewardBench, a
benchmark specifically designed for long-context RM evaluation, featuring both
Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that
even state-of-the-art generative RMs exhibit significant fragility in
long-context scenarios, failing to maintain context-aware preference judgments.
Motivated by the analysis of failure patterns observed in model outputs, we
propose a general multi-stage training strategy that effectively scales
arbitrary models into robust Long-context RMs (LongRMs). Experiments show that
our approach not only substantially improves performance on long-context
evaluation but also preserves strong short-context capability. Notably, our 8B
LongRM outperforms much larger 70B-scale baselines and matches the performance
of the proprietary Gemini 2.5 Pro model.

</details>


### [150] [SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models](https://arxiv.org/abs/2510.06917)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: SHANKS是一个能让语音语言模型（SLM）在用户说话时进行思考和推理的框架，能够实时打断用户或在用户说完话前完成工具调用，从而提高语音交互的效率。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLM）和语音语言模型（SLM）都需要等用户说完话才能开始思考和响应，这会导致响应延迟，不适合需要实时交互的语音到语音交互场景。

Method: SHANKS框架将用户的语音输入分成固定时长的块（chunk），在接收到每个块后，立即根据之前所有语音和推理生成‘无声的思考链’（unspoken chain-of-thought reasoning），并利用这些推理来决定是否打断用户或进行工具调用。

Result: 在数学问题解答场景中，SHANKS能够打断用户说错的部分，打断准确率比不经思考的基线提高了37.1%。在增强工具对话的场景中，SHANKS能够在用户说话未结束时完成56.9%的工具调用。

Conclusion: SHANKS使SLM能够在整个对话过程中持续思考，而不仅仅是在用户发言结束后，从而朝着更流畅、低延迟的语音交互迈进。

Abstract: Current large language models (LLMs) and spoken language models (SLMs) begin
thinking and taking actions only after the user has finished their turn. This
prevents the model from interacting during the user's turn and can lead to high
response latency while it waits to think. Consequently, thinking after
receiving the full input is not suitable for speech-to-speech interaction,
where real-time, low-latency exchange is important. We address this by noting
that humans naturally "think while listening." In this paper, we propose
SHANKS, a general inference framework that enables SLMs to generate unspoken
chain-of-thought reasoning while listening to the user input. SHANKS streams
the input speech in fixed-duration chunks and, as soon as a chunk is received,
generates unspoken reasoning based on all previous speech and reasoning, while
the user continues speaking. SHANKS uses this unspoken reasoning to decide
whether to interrupt the user and to make tool calls to complete the task. We
demonstrate that SHANKS enhances real-time user-SLM interaction in two
scenarios: (1) when the user is presenting a step-by-step solution to a math
problem, SHANKS can listen, reason, and interrupt when the user makes a
mistake, achieving 37.1% higher interruption accuracy than a baseline that
interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can
complete 56.9% of the tool calls before the user finishes their turn. Overall,
SHANKS moves toward models that keep thinking throughout the conversation, not
only after a turn ends. Animated illustrations of Shanks can be found at
https://d223302.github.io/SHANKS/

</details>


### [151] [Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation](https://arxiv.org/abs/2510.06961)
*Vaibhav Srivastav,Steven Zheng,Eric Bezzam,Eustache Le Bihan,Nithin Koluguri,Piotr Żelasko,Somshubra Majumdar,Adel Moumen,Sanchit Gandhi*

Main category: cs.CL

TL;DR: 一个包含60多个开源和专有系统的自动语音识别（ASR）基准和排行榜，用于评估准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前的ASR评估主要集中在简短的英语语音上，并且很少报告效率指标。因此，需要一个更全面的基准来评估多语言和长篇语音的ASR系统，并同时考虑准确性和效率。

Method: 提出了一个开放ASR排行榜，该排行榜包含11个数据集，并对60多个ASR系统进行了评估。评估指标包括词错误率（WER）和反向实时因子（RTFx），并标准化了文本规范化过程。此外，还开源了所有代码和数据集加载器，以确保评估的透明度和可扩展性。

Result: Conformer编码器与LLM解码器在英语转录方面达到了最佳的平均WER，但速度较慢。CTC和TDT解码器在RTFx方面表现更好，适用于长篇和离线应用。基于Whisper的编码器在英语转录方面提高了准确性，但通常会牺牲多语言覆盖范围。

Conclusion: 开放ASR排行榜提供了一个可重复的、可扩展的评估框架，用于比较ASR系统的准确性和效率。该排行榜揭示了不同模型在不同任务和语言上的优缺点，为ASR研究和应用提供了有价值的参考。

Abstract: Despite rapid progress, ASR evaluation remains saturated with short-form
English, and efficiency is rarely reported. We present the Open ASR
Leaderboard, a fully reproducible benchmark and interactive leaderboard
comparing 60+ open-source and proprietary systems across 11 datasets, including
dedicated multilingual and long-form tracks. We standardize text normalization
and report both word error rate (WER) and inverse real-time factor (RTFx),
enabling fair accuracy-efficiency comparisons. For English transcription,
Conformer encoders paired with LLM decoders achieve the best average WER but
are slower, while CTC and TDT decoders deliver much better RTFx, making them
attractive for long-form and offline use. Whisper-derived encoders fine-tuned
for English improve accuracy but often trade off multilingual coverage. All
code and dataset loaders are open-sourced to support transparent, extensible
evaluation.

</details>


### [152] [EDUMATH: Generating Standards-aligned Educational Math Word Problems](https://arxiv.org/abs/2510.06965)
*Bryan R. Christ,Penelope Molitz,Jonathan Kropko,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 大型语言模型可以通过生成定制化的数学应用题来支持数学教育，其生成的题目在质量和学生偏好度上均优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 定制化的数学应用题有助于提升学生的学习效果，但教师因班级规模大和职业倦怠而难以完成，因此需要利用大型语言模型来辅助生成。

Method: 使用联合人类专家-大型语言模型评估方法，评估了超过11,000个由开放和闭源大型语言模型生成的数学应用题，并开发了首个用于标准对齐教育数学应用题生成的教师标注数据集。利用该数据集训练了一个12B的开放模型，并训练了一个文本分类器，使一个30B的开放语言模型在无需训练的情况下超越了现有的闭源基线模型。最后，对定制化的大型语言模型生成的数学应用题进行了学生评估。

Result: 所提出的方法能够生成高质量的数学应用题，并且生成的题目与人类编写的题目相似度更高。通过教师标注数据集训练的模型在性能上可以媲美甚至超越更大更强的模型。学生在回答定制化题目时表现良好，并且更偏好定制化题目。

Conclusion: 大型语言模型能够有效生成定制化的数学应用题，解决教师在个性化教学中的痛点，并且学生更喜欢这些定制化的题目，预示着大型语言模型在教育领域的应用前景广阔。

Abstract: Math word problems (MWPs) are critical K-12 educational tools, and
customizing them to students' interests and ability levels can increase
learning outcomes. However, teachers struggle to find time to customize MWPs
for each student given large class sizes and increasing burnout. We propose
that LLMs can support math education by generating MWPs customized to student
interests and math education standards. To this end, we use a joint human
expert-LLM judge approach to evaluate over 11,000 MWPs generated by open and
closed LLMs and develop the first teacher-annotated dataset for
standards-aligned educational MWP generation. We show the value of our data by
using it to train a 12B open model that matches the performance of larger and
more capable open models. We also use our teacher-annotated data to train a
text classifier that enables a 30B open LLM to outperform existing closed
baselines without any training. Next, we show our models' MWPs are more similar
to human-written MWPs than those from existing models. We conclude by
conducting the first study of customized LLM-generated MWPs with grade school
students, finding they perform similarly on our models' MWPs relative to
human-written MWPs but consistently prefer our customized MWPs.

</details>


### [153] [Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups](https://arxiv.org/abs/2510.06974)
*Geng Liu,Feng Li,Junjie Mu,Mengxiao Zhu,Francesco Pierri*

Main category: cs.CL

TL;DR: 中文大语言模型存在社会身份偏见，表现为“我们”的倾向性好于“他们”，且在实际对话中可能加剧。


<details>
  <summary>Details</summary>
Motivation: 研究中文大语言模型是否反映和放大社会偏见，特别是社会身份的框架效应。

Method: 使用特定于中文的提示，在十个中文大语言模型上评估“我们”（内群体）和“他们”（外群体）框架，并扩展到240个社会群体。此外，分析了真实用户与聊天机器人对话语料库，以补充受控实验。

Result: 在所有模型中观察到系统性的内群体偏好和外群体负面倾向，这种偏见不仅存在于合成提示中，也出现在自然对话中，并且可能在实际交互中得到加强。

Conclusion: 研究提出的中文大语言模型评估框架表明，在英文中已记录的社会身份偏见具有跨语言的普遍性，并且在面向用户的环境中会加剧。

Abstract: Large language models (LLMs) are increasingly deployed in user-facing
applications, raising concerns about their potential to reflect and amplify
social biases. We investigate social identity framing in Chinese LLMs using
Mandarin-specific prompts across ten representative Chinese LLMs, evaluating
responses to ingroup ("We") and outgroup ("They") framings, and extending the
setting to 240 social groups salient in the Chinese context. To complement
controlled experiments, we further analyze Chinese-language conversations from
a corpus of real interactions between users and chatbots. Across models, we
observe systematic ingroup-positive and outgroup-negative tendencies, which are
not confined to synthetic prompts but also appear in naturalistic dialogue,
indicating that bias dynamics might strengthen in real interactions. Our study
provides a language-aware evaluation framework for Chinese LLMs, demonstrating
that social identity biases documented in English generalize
cross-linguistically and intensify in user-facing contexts.

</details>


### [154] [Towards Reliable Retrieval in RAG Systems for Large Legal Datasets](https://arxiv.org/abs/2510.06999)
*Markus Reuter,Tobias Lingenberg,Rūta Liepiņa,Francesca Lagioia,Marco Lippi,Giovanni Sartor,Andrea Passerini,Burcu Sayin*

Main category: cs.CL

TL;DR: RAG在法律领域的应用受限于检索的准确性，特别是当文档结构相似时。本文提出了“摘要增强分块”（SAC）技术，通过为文本块添加文档摘要来解决“文档级检索不匹配”（DRM）问题，实验证明SAC能有效减少DRM，提高检索精度和召回率，且通用摘要策略优于包含法律知识的摘要策略。


<details>
  <summary>Details</summary>
Motivation: 法律领域的大规模文档检索系统在处理结构相似的文档时，常常会因为“文档级检索不匹配”（DRM）而失效，导致检索信息来自完全错误的源文档，从而影响检索的准确性。而检索增强生成（RAG）是减少大型语言模型（LLM）在法律应用中幻觉的有效方法，但其可靠性高度依赖于检索步骤的准确性。因此，需要一种方法来解决DRM问题，提高RAG在法律领域的可靠性。

Method: 本文提出了一种名为“摘要增强分块”（SAC）的技术。该技术通过为每个文本块添加文档级别的合成摘要，来增强文本块的全局上下文信息。这种增强使得检索系统能够更好地理解文本块的来源和整体含义，从而减少因文档结构相似而导致的检索错误（DRM）。实验在多种法律信息检索任务上进行了评估，并与不使用摘要增强或使用包含法律专业知识的摘要策略的方法进行了比较。

Result: 实验结果表明，“摘要增强分块”（SAC）技术能够显著减少“文档级检索不匹配”（DRM）的发生。这不仅解决了DRM问题，还进一步提高了文本级别的检索精度和召回率。有趣的是，研究发现，使用通用的摘要策略在实践中比结合了法律专业知识来提取特定法律要素的摘要策略效果更好。这表明SAC是一种简单、高效且易于集成的方法。

Conclusion: “摘要增强分块”（SAC）是一种简单、计算效率高且易于集成到现有系统中的技术。它通过在文本块中加入文档摘要来增强全局上下文，能够有效解决法律领域大规模文档检索中的“文档级检索不匹配”（DRM）问题，从而提高检索增强生成（RAG）系统的可靠性。研究表明，通用摘要策略比特定领域知识摘要策略更有效。该技术为提高RAG在处理大规模法律文档时的准确性和鲁棒性提供了有力的支持。

Abstract: Retrieval-Augmented Generation (RAG) is a promising approach to mitigate
hallucinations in Large Language Models (LLMs) for legal applications, but its
reliability is critically dependent on the accuracy of the retrieval step. This
is particularly challenging in the legal domain, where large databases of
structurally similar documents often cause retrieval systems to fail. In this
paper, we address this challenge by first identifying and quantifying a
critical failure mode we term Document-Level Retrieval Mismatch (DRM), where
the retriever selects information from entirely incorrect source documents. To
mitigate DRM, we investigate a simple and computationally efficient technique
which we refer to as Summary-Augmented Chunking (SAC). This method enhances
each text chunk with a document-level synthetic summary, thereby injecting
crucial global context that would otherwise be lost during a standard chunking
process. Our experiments on a diverse set of legal information retrieval tasks
show that SAC greatly reduces DRM and, consequently, also improves text-level
retrieval precision and recall. Interestingly, we find that a generic
summarization strategy outperforms an approach that incorporates legal expert
domain knowledge to target specific legal elements. Our work provides evidence
that this practical, scalable, and easily integrable technique enhances the
reliability of RAG systems when applied to large-scale legal document datasets.

</details>


### [155] [Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages](https://arxiv.org/abs/2510.07000)
*Neel Prabhanjan Rachamalla,Aravind Konakalla,Gautam Rajeev,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: LLMs需要高质量的训练数据，但现有数据集在印度语言方面的多语言覆盖、文化接地和任务多样性方面存在不足。本文提出了一种结合翻译和合成扩展的人机循环流水线，用于创建可靠且多样化的印度语言后期训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有开源数据集在多语言覆盖、文化接地和任务多样性方面存在不足，尤其是在印度语言方面，这阻碍了大型语言模型（LLMs）的有效性。

Method: 使用结合翻译和合成扩展的人机循环流水线来创建印度语言的后期训练数据。

Result: 创建了两个数据集：Pragyaan-IT（22.5K）和Pragyaan-Align（100K），涵盖10种印度语言和57个多样化数据集，注重任务多样性、多轮对话、指令保真度、安全对齐和文化细微差别。

Conclusion: 所提出的数据集协议为创建更具包容性和有效性的多语言LLMs奠定了基础。

Abstract: The effectiveness of Large Language Models (LLMs) depends heavily on the
availability of high-quality post-training data, particularly
instruction-tuning and preference-based examples. Existing open-source
datasets, however, often lack multilingual coverage, cultural grounding, and
suffer from task diversity gaps that are especially pronounced for Indian
languages. We introduce a human-in-the-loop pipeline that combines translations
with synthetic expansion to produce reliable and diverse Indic post-training
data. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and
Pragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56
sub-categories, leveraging 57 diverse datasets. Our dataset protocol
incorporates several often-overlooked dimensions and emphasize task diversity,
multi-turn dialogue, instruction fidelity, safety alignment, and preservation
of cultural nuance, providing a foundation for more inclusive and effective
multilingual LLMs.

</details>


### [156] [Native Hybrid Attention for Efficient Sequence Modeling](https://arxiv.org/abs/2510.07019)
*Jusen Du,Jiaxi Hu,Tao Zhang,Weigao Sun,Yu Cheng*

Main category: cs.CL

TL;DR: NHA是一种新的混合注意力架构，结合了线性和全注意力，在保持效率的同时提高了长上下文的准确性。


<details>
  <summary>Details</summary>
Motivation: Transformer在序列建模方面表现出色，但存在二次方复杂性问题；线性注意力虽然效率更高，但在长上下文的召回准确性方面有所牺牲。

Method: 提出了一种名为Native Hybrid Attention (NHA) 的新混合架构，该架构将线性和全注意力结合起来，在一个统一的层设计中实现了层内和层间混合。NHA通过线性RNN更新的键值槽来维护长期上下文，并通过滑动窗口的短期令牌来增强它们。最后，在一个softmax注意力操作中，对所有键和值进行操作，从而实现每个令牌和每个头的上下文依赖加权，无需额外的融合参数。滑动窗口的大小控制了层间的行为，可以平滑地在纯线性注意力和全注意力之间进行调整，同时保持所有层的结构统一。

Result: 实验结果表明，NHA在召回密集型和常识推理任务上超越了Transformer和其他混合基线。此外，预训练的LLM可以与NHA进行结构化混合，在实现具有竞争力的准确性的同时，能够带来显著的效率提升。

Conclusion: NHA通过结合线性和全注意力，有效地解决了Transformer在效率和长上下文准确性方面的权衡问题，并在多项任务上取得了优于现有方法的性能。

Abstract: Transformers excel at sequence modeling but face quadratic complexity, while
linear attention offers improved efficiency but often compromises recall
accuracy over long contexts. In this work, we introduce Native Hybrid Attention
(NHA), a novel hybrid architecture of linear and full attention that integrates
both intra \& inter-layer hybridization into a unified layer design. NHA
maintains long-term context in key-value slots updated by a linear RNN, and
augments them with short-term tokens from a sliding window. A single
\texttt{softmax attention} operation is then applied over all keys and values,
enabling per-token and per-head context-dependent weighting without requiring
additional fusion parameters. The inter-layer behavior is controlled through a
single hyperparameter, the sliding window size, which allows smooth adjustment
between purely linear and full attention while keeping all layers structurally
uniform. Experimental results show that NHA surpasses Transformers and other
hybrid baselines on recall-intensive and commonsense reasoning tasks.
Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving
competitive accuracy while delivering significant efficiency gains. Code is
available at https://github.com/JusenD/NHA.

</details>


### [157] [Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge](https://arxiv.org/abs/2510.07024)
*Shrestha Ghosh,Luca Giordano,Yujia Hu,Tuan-Phong Nguyen,Simon Razniewski*

Main category: cs.CL

TL;DR: LLM（尤其是GPT-4.1）的事实知识与其既有知识库存在显著差异，准确性低于预期，且存在不一致、模糊和幻觉等问题。


<details>
  <summary>Details</summary>
Motivation: 以往对LLM事实知识的理解不足且常基于有偏见的样本，因此需要深入研究前沿LLM（如GPT-4.1）的事实知识。

Method: 利用GPTKB v1.5（包含1亿条信念记录）对GPT-4.1的事实知识进行深入分析。

Result: LLM的事实知识与现有知识库存在显著差异，准确性低于先前基准测试的估计。不一致性、模糊性和幻觉是LLM事实知识中的主要问题。

Conclusion: LLM的事实知识存在准确性不高、不一致、模糊和幻觉等问题，这指明了未来在LLM事实知识研究方面的方向。

Abstract: LLMs are remarkable artifacts that have revolutionized a range of NLP and AI
tasks. A significant contributor is their factual knowledge, which, to date,
remains poorly understood, and is usually analyzed from biased samples. In this
paper, we take a deep tour into the factual knowledge (or beliefs) of a
frontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited
set of 100 million beliefs of one of the strongest currently available frontier
LLMs, GPT-4.1. We find that the models' factual knowledge differs quite
significantly from established knowledge bases, and that its accuracy is
significantly lower than indicated by previous benchmarks. We also find that
inconsistency, ambiguity and hallucinations are major issues, shedding light on
future research opportunities concerning factual LLM knowledge.

</details>


### [158] [Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models](https://arxiv.org/abs/2510.07037)
*Rajvee Sheth,Samridhi Raj Sinha,Mahavir Patil,Himanshu Beniwal,Mayank Singh*

Main category: cs.CL

TL;DR: 代码转换（CSW）在多语言NLP中仍然是一个挑战，即使是大型语言模型（LLMs）也面临混合语言输入、有限的CSW数据集和评估偏见等问题。本综述全面分析了CSW感知的LLM研究，涵盖了5个研究领域、12个NLP任务、30多个数据集和80多种语言。


<details>
  <summary>Details</summary>
Motivation: 代码转换（CSW）是多语言NLP中的一个基本挑战，大型语言模型（LLMs）在处理混合语言输入、有限的CSW数据集和评估偏见方面仍存在困难，这阻碍了它们在多语言社会中的应用。

Method: 本综述对CSW感知的LLM研究进行了全面的分析，根据架构、训练策略和评估方法对近期进展进行了分类，概述了LLM如何重塑CSW建模以及哪些挑战依然存在。

Result: 本综述回顾了	otal{unique_references}项研究，涵盖了5个研究领域、12个NLP任务、30多个数据集和80多种语言。

Conclusion: 该研究总结了LLM在CSW建模方面的进展和持续存在的挑战，并提出了一个包含包容性数据集、公平评估和基于语言学原理的模型等要素的路线图，以实现真正多语言的智能。

Abstract: Code-switching (CSW), the alternation of languages and scripts within a
single utterance, remains a fundamental challenge for multiling ual NLP, even
amidst the rapid advances of large language models (LLMs). Most LLMs still
struggle with mixed-language inputs, limited CSW datasets, and evaluation
biases, hindering deployment in multilingual societies. This survey provides
the first comprehensive analysis of CSW-aware LLM research, reviewing
\total{unique_references} studies spanning five research areas, 12 NLP tasks,
30+ datasets, and 80+ languages. We classify recent advances by architecture,
training strategy, and evaluation methodology, outlining how LLMs have reshaped
CSW modeling and what challenges persist. The paper concludes with a roadmap
emphasizing the need for inclusive datasets, fair evaluation, and
linguistically grounded models to achieve truly multilingual intelligence. A
curated collection of all resources is maintained at
https://github.com/lingo-iitgn/awesome-code-mixing/.

</details>


### [159] [Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models](https://arxiv.org/abs/2510.07048)
*Yuntao Gui,James Cheng*

Main category: cs.CL

TL;DR: Search-R3框架利用大型语言模型的链式思考能力生成搜索嵌入，并在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在自然语言理解方面表现出色，但它们在检索任务中的应用却未得到充分发挥。本研究旨在解决这一局限性，使LLM能够直接生成搜索嵌入。

Method: Search-R3框架结合了监督学习和强化学习（RL）方法。它利用LLM的链式思考能力，通过逐步推理来生成更有效的嵌入。具体来说，它包括一个监督学习阶段来生成高质量嵌入，一个RL方法来优化嵌入生成和推理过程，以及一个专门的RL环境来高效处理不断变化的嵌入表示，而无需在每次训练时重新编码整个语料库。

Result: Search-R3框架在多个基准测试中，通过整合推理和嵌入生成过程，显著优于现有方法。

Conclusion: Search-R3框架通过将推理和嵌入生成相结合，推动了在需要复杂推理和有效信息检索的知识密集型任务方面的进展。

Abstract: Despite their remarkable natural language understanding capabilities, Large
Language Models (LLMs) have been underutilized for retrieval tasks. We present
Search-R3, a novel framework that addresses this limitation by adapting LLMs to
generate search embeddings as a direct output of their reasoning process. Our
approach exploits LLMs' chain-of-thought capabilities, allowing them to produce
more effective embeddings by reasoning step-by-step through complex semantic
analyses. We implement this through three complementary mechanisms. (1) a
supervised learning stage enables the model's ability to produce quality
embeddings, (2) a reinforcement learning (RL) methodology that optimizes
embedding generation alongside reasoning, and (3) a specialized RL environment
that efficiently handles evolving embedding representations without requiring
complete corpus re-encoding at each training iteration. Our extensive
evaluations on diverse benchmarks demonstrate that Search-R3 significantly
outperforms prior methods by unifying the reasoning and embedding generation
processes. This integrated post-training approach represents a substantial
advancement in handling complex knowledge-intensive tasks that require both
sophisticated reasoning and effective information retrieval. Project page:
https://github.com/ytgui/Search-R3

</details>


### [160] [Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations](https://arxiv.org/abs/2510.07060)
*Miriam Wanner,Sophia Hager,Anjalie Field*

Main category: cs.CL

TL;DR: 辛克莱广播公司收购本地新闻台后，本地新闻报道的本地话题减少，全国性话题（尤其是具有争议性的话题）增多。


<details>
  <summary>Details</summary>
Motivation: 探究辛克莱广播公司收购本地新闻台对其报道内容的影响，量化分析收购前后新闻报道的变化。

Method: 使用计算方法，对比分析被辛克莱收购前后本地新闻台网站发布的内容，并与全国性新闻机构的内容进行比较。

Result: 研究发现，被辛克莱收购的本地新闻台报道全国性新闻的频率增加，挤压了本地新闻话题的报道；同时，对具有争议的全国性新闻话题的报道也有所增加。

Conclusion: 辛克莱广播公司收购本地新闻台后，其新闻报道的重心发生了偏移，更加关注全国性话题，并增加了对有争议话题的报道。

Abstract: Local news stations are often considered to be reliable sources of
non-politicized information, particularly local concerns that residents care
about. Because these stations are trusted news sources, viewers are
particularly susceptible to the information they report. The Sinclair Broadcast
group is a broadcasting company that has acquired many local news stations in
the last decade. We investigate the effects of local news stations being
acquired by Sinclair: how does coverage change? We use computational methods to
investigate changes in internet content put out by local news stations before
and after being acquired by Sinclair and in comparison to national news
outlets. We find that there is clear evidence that local news stations report
more frequently on national news at the expense of local topics, and that their
coverage of polarizing national topics increases.

</details>


### [161] [Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages](https://arxiv.org/abs/2510.07061)
*Amir Hossein Yari,Kalmit Kulkarni,Ahmad Raza Khan,Fajri Koto*

Main category: cs.CL

TL;DR: 现有自动评估指标主要针对英语等高资源语言，忽略了印度语言。本研究提出了ITEM基准，评估了26种自动指标在6种印度语言上与人类判断的一致性，发现基于LLM的评估器与人类判断最一致，异常值会显著影响评估结果，指标在内容保真度和流畅性方面表现不同，且对扰动敏感度各异。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估指标主要针对英语等高资源语言，未能覆盖印度语言，因此需要评估现有指标在印度语言上的有效性。

Method: 构建了一个包含26种自动评估指标和6种印度语言的大型基准（ITEM），并进行细粒度标注，系统地评估了指标与人类判断的一致性，并分析了异常值、语言特异性、指标间相关性和对扰动的鲁棒性。

Result: 基于LLM的评估器在评估指标中与人类判断最一致；异常值对评估结果有显著影响；在文本摘要任务中，指标更侧重于评估内容保真度，而在机器翻译任务中，则更侧重于评估流畅性；不同指标在鲁棒性和对扰动的敏感度方面存在差异。

Conclusion: 本研究结果为在印度语言中改进评估指标的设计和评估提供了重要指导。

Abstract: While automatic metrics drive progress in Machine Translation (MT) and Text
Summarization (TS), existing metrics have been developed and validated almost
exclusively for English and other high-resource languages. This narrow focus
leaves Indian languages, spoken by over 1.5 billion people, largely overlooked,
casting doubt on the universality of current evaluation practices. To address
this gap, we introduce ITEM, a large-scale benchmark that systematically
evaluates the alignment of 26 automatic metrics with human judgments across six
major Indian languages, enriched with fine-grained annotations. Our extensive
evaluation, covering agreement with human judgments, sensitivity to outliers,
language-specific reliability, inter-metric correlations, and resilience to
controlled perturbations, reveals four central findings: (1) LLM-based
evaluators show the strongest alignment with human judgments at both segment
and system levels; (2) outliers exert a significant impact on metric-human
agreement; (3) in TS, metrics are more effective at capturing content fidelity,
whereas in MT, they better reflect fluency; and (4) metrics differ in their
robustness and sensitivity when subjected to diverse perturbations.
Collectively, these findings offer critical guidance for advancing metric
design and evaluation in Indian languages.

</details>


### [162] [LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish](https://arxiv.org/abs/2510.07074)
*Fred Philippy,Laura Bernardy,Siwen Guo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Instruction tuning has become a key technique for enhancing the performance
of large language models, enabling them to better follow human prompts.
However, low-resource languages such as Luxembourgish face severe limitations
due to the lack of high-quality instruction datasets. Traditional reliance on
machine translation often introduces semantic misalignment and cultural
inaccuracies. In this work, we address these challenges by creating a
cross-lingual instruction tuning dataset for Luxembourgish, without resorting
to machine-generated translations into it. Instead, by leveraging aligned data
from English, French, and German, we build a high-quality dataset that
preserves linguistic and cultural nuances. We provide evidence that
cross-lingual instruction tuning not only improves representational alignment
across languages but also the model's generative capabilities in Luxembourgish.
This highlights how cross-lingual data curation can avoid the common pitfalls
of machine-translated data and directly benefit low-resource language
development.

</details>


### [163] [Accelerating Diffusion LLM Inference via Local Determinism Propagation](https://arxiv.org/abs/2510.07081)
*Fanheng Kong,Jingyuan Zhang,Yahui Liu,Zirui Wu,Yu Tian,Victoria W.,Guorui Zhou*

Main category: cs.CL

TL;DR: dLLM的现有开源实现存在质量-速度权衡问题，LocalLeap提出了一种无需训练的自适应并行解码策略，通过识别锚点并进行局部并行解码，在不影响质量的情况下显著提高了吞吐量并减少了解码步数。


<details>
  <summary>Details</summary>
Motivation: 现有开源的扩散大语言模型（dLLM）在生成文本时存在质量-速度的权衡问题，这阻碍了它们的实际应用。保守的采样策略为了保证质量（例如贪心解码）只选择每一步最置信的词元，但由于重复的冗余优化迭代，导致了推理效率低下，这种现象被称为“延迟解码”。

Method: 本文通过系统分析dLLM的解码动态，提出了一个名为LocalLeap的无需训练的自适应并行解码策略。该策略基于两个基本经验原则：以高置信度锚点为中心进行局部确定性传播，以及逐步的空间一致性衰减。LocalLeap识别锚点，并在有界邻域内进行局部放松并行解码，通过提前确定已确定词元，显著减少推理步数，同时不影响输出质量。

Result: LocalLeap在多个基准测试中取得了显著成效，吞吐量提高了6.94倍，解码步数减少到原始要求的14.2%，同时对性能的影响可以忽略不计。

Conclusion: LocalLeap通过识别锚点和局部并行解码，解决了dLLM的延迟解码问题，在保证生成质量的同时，大幅提高了推理效率。

Abstract: Diffusion large language models (dLLMs) represent a significant advancement
in text generation, offering parallel token decoding capabilities. However,
existing open-source implementations suffer from quality-speed trade-offs that
impede their practical deployment. Conservative sampling strategies typically
decode only the most confident token per step to ensure quality (i.e., greedy
decoding), at the cost of inference efficiency due to repeated redundant
refinement iterations--a phenomenon we term delayed decoding. Through
systematic analysis of dLLM decoding dynamics, we characterize this delayed
decoding behavior and propose a training-free adaptive parallel decoding
strategy, named LocalLeap, to address these inefficiencies. LocalLeap is built
on two fundamental empirical principles: local determinism propagation centered
on high-confidence anchors and progressive spatial consistency decay. By
applying these principles, LocalLeap identifies anchors and performs localized
relaxed parallel decoding within bounded neighborhoods, achieving substantial
inference step reduction through early commitment of already-determined tokens
without compromising output quality. Comprehensive evaluation on various
benchmarks demonstrates that LocalLeap achieves 6.94$\times$ throughput
improvements and reduces decoding steps to just 14.2\% of the original
requirement, achieving these gains with negligible performance impact. The
source codes are available at: https://github.com/friedrichor/LocalLeap.

</details>


### [164] [All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations](https://arxiv.org/abs/2510.07083)
*Miriam Wanner,Leif Azzopardi,Paul Thomas,Soham Dan,Benjamin Van Durme,Nick Craswell*

Main category: cs.CL

TL;DR: 现有 LLM 评估方法未能区分关键信息错误，我们提出了 VITALERRORS 数据集和 VITAL 指标来解决此问题。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 事实性评估方法未能区分关键信息错误和外围细节错误，导致评估结果具有误导性。

Method: 构建了一个包含 6,733 个查询的 VITALERRORS 基准，其中 LLM 响应被最小化地修改以省略或伪造关键信息。在此数据集上，展示了现有评估指标对关键信息错误的敏感度不足。然后，引入了一套名为 VITAL 的指标，通过整合声明与查询的相关性和重要性，来提高评估响应事实性的敏感度。

Result: VITAL 指标比以前的方法更可靠地检测到关键信息错误。

Conclusion: VITALERRORS 数据集、VITAL 指标和相关分析为更准确、更鲁棒的 LLM 事实性评估奠定了基础。

Abstract: Existing methods for evaluating the factuality of large language model (LLM)
responses treat all claims as equally important. This results in misleading
evaluations when vital information is missing or incorrect as it receives the
same weight as peripheral details, raising the question: how can we reliably
detect such differences when there are errors in key information? Current
approaches that measure factuality tend to be insensitive to omitted or false
key information. To investigate this lack of sensitivity, we construct
VITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses
designed to omit or falsify key information. Using this dataset, we demonstrate
the insensitivities of existing evaluation metrics to key information errors.
To address this gap, we introduce VITAL, a set of metrics that provide greater
sensitivity in measuring the factuality of responses by incorporating the
relevance and importance of claims with respect to the query. Our analysis
demonstrates that VITAL metrics more reliably detect errors in key information
than previous methods. Our dataset, metrics, and analysis provide a foundation
for more accurate and robust assessment of LLM factuality.

</details>


### [165] [Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis](https://arxiv.org/abs/2510.07096)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: The paper proposes an LLM-enhanced Retrieval-Augmented framework for sarcasm-aware speech synthesis, combining semantic embeddings from LLaMA 3 and prosodic exemplars via RAG, integrated into a VITS backbone. Experiments show improvements in naturalness, expressivity, and downstream sarcasm detection.


<details>
  <summary>Details</summary>
Motivation: Sarcasm is challenging for speech synthesis due to its reliance on nuanced cues, and existing research has largely ignored it, focusing instead on broad emotional categories.

Method: The proposed approach combines semantic embeddings from a LoRA-fine-tuned LLaMA 3 (to capture pragmatic incongruity and discourse-level cues) with prosodic exemplars retrieved via a RAG module (for expressive reference patterns). These are integrated within a VITS backbone for dual conditioning.

Result: The experimental results demonstrate that the proposed method outperforms baselines in objective measures and subjective evaluations, showing improvements in speech naturalness, sarcastic expressivity, and downstream sarcasm detection.

Conclusion: The developed LLM-enhanced Retrieval-Augmented framework effectively enables more natural and contextually appropriate sarcastic speech synthesis, outperforming existing methods.

Abstract: Sarcasm is a subtle form of non-literal language that poses significant
challenges for speech synthesis due to its reliance on nuanced semantic,
contextual, and prosodic cues. While existing speech synthesis research has
focused primarily on broad emotional categories, sarcasm remains largely
unexplored. In this paper, we propose a Large Language Model (LLM)-enhanced
Retrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach
combines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture
pragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic
exemplars retrieved via a Retrieval Augmented Generation (RAG) module, which
provide expressive reference patterns of sarcastic delivery. Integrated within
a VITS backbone, this dual conditioning enables more natural and contextually
appropriate sarcastic speech. Experiments demonstrate that our method
outperforms baselines in both objective measures and subjective evaluations,
yielding improvements in speech naturalness, sarcastic expressivity, and
downstream sarcasm detection.

</details>


### [166] [TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription](https://arxiv.org/abs/2510.07098)
*Guo Yutong,Wanying Wang,Yue Wu,Zichen Miao,Haoyu Wang*

Main category: cs.CL

TL;DR: TALENT框架通过结合OCR文本和自然语言描述，利用小视觉语言模型（VLM）和大型语言模型（LLM）来解决表格视觉问答问题，实现了在低计算成本下与大型VLM相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的表格视觉问答（Table VQA）方法依赖大型视觉语言模型（VLMs），这类模型计算成本高昂，不适合移动设备部署。将OCR和大型语言模型（LLM）结合的方法虽然更轻量，但表示形式不适合LLM且易引入错误。

Method: TALENT框架使用小型VLM生成表格的OCR文本和自然语言描述，然后将这些信息与问题一起输入给LLM进行推理。这种方法将Table VQA重新定义为以LLM为中心的、多模态的推理任务，其中VLM充当感知-叙述模块。

Result: 在公开数据集和新创建的ReTabVQA数据集上，TALENT框架（小型VLM+LLM）在性能上能媲美甚至超越单独的大型VLM，同时计算成本显著降低。

Conclusion: TALENT框架是一种轻量级的Table VQA解决方案，通过结合OCR文本和自然语言描述，并以LLM为中心进行多模态推理，有效解决了现有方法的局限性。

Abstract: Table Visual Question Answering (Table VQA) is typically addressed by large
vision-language models (VLMs). While such models can answer directly from
images, they often miss fine-grained details unless scaled to very large sizes,
which are computationally prohibitive, especially for mobile deployment. A
lighter alternative is to have a small VLM perform OCR and then use a large
language model (LLM) to reason over structured outputs such as Markdown tables.
However, these representations are not naturally optimized for LLMs and still
introduce substantial errors. We propose TALENT (Table VQA via Augmented
Language-Enhanced Natural-text Transcription), a lightweight framework that
leverages dual representations of tables. TALENT prompts a small VLM to produce
both OCR text and natural language narration, then combines them with the
question for reasoning by an LLM. This reframes Table VQA as an LLM-centric
multimodal reasoning task, where the VLM serves as a perception-narration
module rather than a monolithic solver. Additionally, we construct ReTabVQA, a
more challenging Table VQA dataset requiring multi-step quantitative reasoning
over table images. Experiments show that TALENT enables a small VLM-LLM
combination to match or surpass a single large VLM at significantly lower
computational cost on both public datasets and ReTabVQA.

</details>


### [167] [Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning](https://arxiv.org/abs/2510.07105)
*Taylor Sorensen,Yejin Choi*

Main category: cs.CL

TL;DR: 本系统通过结合语言模型（LLMs）的上下文学习能力和两步元学习训练程序，对人类在自然语言处理（NLP）任务中的变异性进行建模，并在LeWiDi竞赛中取得获胜成绩。


<details>
  <summary>Details</summary>
Motivation: 许多自然语言处理（NLP）任务涉及主观性、歧义性或标注者之间存在合理的分歧，因此需要对人类变异性进行建模。

Method: 本系统利用语言模型（LLMs）的上下文学习能力，并采用两步元学习训练程序：1）对需要上下文学习的多个数据集进行后训练；2）通过上下文元学习将模型专门化到目标数据的特定分布。

Result: 在Learning With Disagreements (LeWiDi) 竞赛中，本系统在两个任务上均获得总体第一名。消融研究表明，在上下文中包含评分者示例至关重要，特定数据集的微调在大数据集上很有帮助，在其他上下文数据集上进行后训练对其中一个竞赛数据集有帮助，并且性能随模型规模的增大而提高。

Conclusion: 本系统通过结合上下文学习和元学习方法，有效地对人类在NLP任务中的变异性进行建模，并在竞赛中取得了优异的成绩，证明了其方法的有效性。

Abstract: Many natural language processing (NLP) tasks involve subjectivity, ambiguity,
or legitimate disagreement between annotators. In this paper, we outline our
system for modeling human variation. Our system leverages language models'
(LLMs) in-context learning abilities, along with a two-step meta-learning
training procedure for 1) post-training on many datasets requiring in-context
learning and 2) specializing the model via in-context meta-learning to the
particular data distribution of interest. We also evaluate the performance of
our system submission to the Learning With Disagreements (LeWiDi) competition,
where it was the overall winner on both tasks. Additionally, we perform an
ablation study to measure the importance of each system component. We find that
including rater examples in-context is crucial for our system's performance,
dataset-specific fine-tuning is helpful on the larger datasets, post-training
on other in-context datasets is helpful on one of the competition datasets, and
that performance improves with model scale.

</details>


### [168] [TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning](https://arxiv.org/abs/2510.07118)
*Manish Nagaraj,Sakshi Choudhary,Utkarsh Saxena,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CL

TL;DR: TRIM是一个高效的框架，通过匹配注意力指纹来选择高质量的指令调整数据集子集（coreset），在下游任务中表现优于现有方法，甚至在某些情况下超过全数据微调。


<details>
  <summary>Details</summary>
Motivation: 指令调整对齐大语言模型至关重要，但依赖大型数据集，而小而高质量的数据集（coreset）虽有潜力但难以获取。现有方法依赖计算成本高昂且忽略细粒度特征的样本级信号（如梯度）。

Method: TRIM（Token Relevance via Interpretable Multi-layer Attention）是一个仅前向传播、以 token 为中心的框架。它不使用梯度，而是通过匹配来自少量目标样本的基于注意力的“指纹”所识别出的潜在表示模式来选择 coreset。

Result: TRIM 选择的 Coreset 在下游任务中的表现持续优于最先进的基线（最高可达9%），在某些情况下甚至超过了全数据微调的性能。TRIM 的计算成本仅为后向传播方法的一小部分。

Conclusion: TRIM 提供了一种可扩展且高效的替代方案，用于构建高质量的指令调整数据集，它通过匹配注意力指纹来识别有信息量的样本，从而提高了效率并获得了优于现有方法的性能。

Abstract: Instruction tuning is essential for aligning large language models (LLMs) to
downstream tasks and commonly relies on large, diverse corpora. However, small,
high-quality subsets, known as coresets, can deliver comparable or superior
results, though curating them remains challenging. Existing methods often rely
on coarse, sample-level signals like gradients, an approach that is
computationally expensive and overlooks fine-grained features. To address this,
we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a
forward-only, token-centric framework. Instead of using gradients, TRIM
operates by matching underlying representational patterns identified via
attention-based "fingerprints" from a handful of target samples. Such an
approach makes TRIM highly efficient and uniquely sensitive to the structural
features that define a task. Coresets selected by our method consistently
outperform state-of-the-art baselines by up to 9% on downstream tasks and even
surpass the performance of full-data fine-tuning in some settings. By avoiding
expensive backward passes, TRIM achieves this at a fraction of the
computational cost. These findings establish TRIM as a scalable and efficient
alternative for building high-quality instruction-tuning datasets.

</details>


### [169] [Comparing human and language models sentence processing difficulties on complex structures](https://arxiv.org/abs/2510.07141)
*Samuel Joseph Amouyal,Aya Meltzer-Asscher,Jonathan Berant*

Main category: cs.CL

TL;DR: LLMs在句子理解方面与人类有相似之处，但也有不同之处，尤其是在处理歧义句方面。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在句子理解方面是否存在类似人类的处理困难，并与人类进行比较。

Method: 收集人类和五种不同规模和训练方法的LLMs在七种具有挑战性的语言结构上的句子理解数据，并进行统一的实验框架比较。

Result: LLMs在整体句子理解上面临挑战，尤其是在歧义句（GP）上表现不佳。模型参数量越大，其理解能力排名与人类的相似度越高。LLMs在目标句子和基线句子之间的性能差距与人类相似，存在一些例外情况。

Conclusion: LLMs在句子理解方面与人类既有趋同也有分歧，尤其是在歧义句的处理上。

Abstract: Large language models (LLMs) that fluently converse with humans are a reality
- but do LLMs experience human-like processing difficulties? We systematically
compare human and LLM sentence comprehension across seven challenging
linguistic structures. We collect sentence comprehension data from humans and
five families of state-of-the-art LLMs, varying in size and training procedure
in a unified experimental framework. Our results show LLMs overall struggle on
the target structures, but especially on garden path (GP) sentences. Indeed,
while the strongest models achieve near perfect accuracy on non-GP structures
(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).
Additionally, when ranking structures based on average performance, rank
correlation between humans and models increases with parameter count. For each
target structure, we also collect data for their matched baseline without the
difficult structure. Comparing performance on the target vs. baseline
sentences, the performance gap observed in humans holds for LLMs, with two
exceptions: for models that are too weak performance is uniformly low across
both sentence types, and for models that are too strong the performance is
uniformly high. Together, these reveal convergence and divergence in human and
LLM sentence comprehension, offering new insights into the similarity of humans
and LLMs.

</details>


### [170] [Reasoning for Hierarchical Text Classification: The Case of Patents](https://arxiv.org/abs/2510.07167)
*Lekang Jiang,Wenjun Sun,Stephan Goetz*

Main category: cs.CL

TL;DR: RHC是一个新框架，通过将HTC重新构建为逐步推理任务来解决自动化专利分类的难题，在准确性和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动化专利分类是HTC的一个难题，因为领域知识难度大且标签众多。现有方法只输出扁平标签集，缺乏预测依据的洞察力。

Method: RHC框架通过两个阶段训练LLM：冷启动阶段将输出与思维链（CoT）推理格式对齐，然后通过强化学习（RL）阶段增强多步推理能力。

Result: RHC在准确性、宏F1、可解释性、可扩展性和适用性方面均表现出色，在准确性和宏F1方面比基线和监督微调方法提高了约3%，并能在其他HTC基准上实现最先进的性能。

Conclusion: RHC框架通过将其重新构建为逐步推理任务，成功解决了自动化专利分类中的挑战，并在多个方面展现出优越性，具有广泛的应用前景。

Abstract: Hierarchical text classification (HTC) assigns documents to multiple levels
of a pre-defined taxonomy. Automated patent subject classification represents
one of the hardest HTC scenarios because of domain knowledge difficulty and a
huge number of labels. Prior approaches only output a flat label set, which
offers little insight into the reason behind predictions. Therefore, we propose
Reasoning for Hierarchical Classification (RHC), a novel framework that
reformulates HTC as a step-by-step reasoning task to sequentially deduce
hierarchical labels. RHC trains large language models (LLMs) in two stages: a
cold-start stage that aligns outputs with chain-of-thought (CoT) reasoning
format and a reinforcement learning (RL) stage to enhance multi-step reasoning
ability. RHC demonstrates four advantages in our experiments. (1)
Effectiveness: RHC surpasses previous baselines and outperforms the supervised
fine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)
Explainability: RHC produces natural-language justifications before prediction
to facilitate human inspection. (3) Scalability: RHC scales favorably with
model size with larger gains compared to standard fine-tuning. (4)
Applicability: Beyond patents, we further demonstrate that RHC achieves
state-of-the-art performance on other widely used HTC benchmarks, which
highlights its broad applicability.

</details>


### [171] [More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning](https://arxiv.org/abs/2510.07169)
*Yike Zhao,Simin Guo,Ziqing Yang,Shifan Han,Dahua Lin,Fei Tan*

Main category: cs.CL

TL;DR: LLM在数学推理中的能力很大程度上取决于训练数据质量。本研究评估了开源数据集和数据合成技术，并提出了有效的、适合工业应用的数据选择策略，强调结构化数据和蒸馏比增加数据量更重要。


<details>
  <summary>Details</summary>
Motivation: 评估现有LLM数学推理训练数据的实用性，并为实际应用提供指导。

Method: 在统一的流水线中评估开源数据集和数据合成技术，并提炼有效的数据选择策略。

Result: 结构化数据和从更强模型中蒸馏数据比单纯增加数据量更能提升LLM的数学推理能力。

Conclusion: 本研究为LLM训练数据的整合提供了可行的指导，有助于成本效益和可扩展的模型增强，并启发了对“更多数据”与“更好数据”之间平衡的研究。

Abstract: The reasoning capabilities of Large Language Models (LLMs) play a critical
role in many downstream tasks, yet depend strongly on the quality of training
data. Despite various proposed data construction methods, their practical
utility in real-world pipelines remains underexplored. In this work, we conduct
a comprehensive analysis of open-source datasets and data synthesis techniques
for mathematical reasoning, evaluating them under a unified pipeline designed
to mirror training and deployment scenarios. We further distill effective data
selection strategies and identify practical methods suitable for industrial
applications. Our findings highlight that structuring data in more
interpretable formats, or distilling from stronger models often outweighs
simply scaling up data volume. This study provides actionable guidance for
integrating training data to enhance LLM capabilities, supporting both
cost-effective data curation and scalable model enhancement. We hope this work
will inspire further research on how to balance "more data" versus "better
data" for real-world reasoning tasks.

</details>


### [172] [NurseLLM: The First Specialized Language Model for Nursing](https://arxiv.org/abs/2510.07173)
*Md Tawkat Islam Khondaker,Julia Harrington,Shady Shehata*

Main category: cs.CL

TL;DR: NurseLLM是第一个针对护理领域选择题问答任务的LLM，它在多个护理基准测试中优于现有的通用和医学LLM。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在专业领域，特别是护理领域的应用仍有待探索。

Method: 构建了一个多阶段的数据生成流程，创建了第一个大规模的护理选择题数据集来训练LLM，并引入了多个护理基准测试来进行评估。

Result: NurseLLM在多个基准测试中，其表现优于相同规模的通用目的和医学专业LLM，证明了护理专业LLM的重要性。

Conclusion: NurseLLM在护理领域选择题问答任务中表现出色，并为未来在护理领域探索推理和多智能体协作系统指明了方向。

Abstract: Recent advancements in large language models (LLMs) have significantly
transformed medical systems. However, their potential within specialized
domains such as nursing remains largely underexplored. In this work, we
introduce NurseLLM, the first nursing-specialized LLM tailored for multiple
choice question-answering (MCQ) tasks. We develop a multi-stage data generation
pipeline to build the first large scale nursing MCQ dataset to train LLMs on a
broad spectrum of nursing topics. We further introduce multiple nursing
benchmarks to enable rigorous evaluation. Our extensive experiments demonstrate
that NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of
comparable size on different benchmarks, underscoring the importance of a
specialized LLM for the nursing domain. Finally, we explore the role of
reasoning and multi-agent collaboration systems in nursing, highlighting their
promise for future research and applications.

</details>


### [173] [Quantifying Data Contamination in Psychometric Evaluations of LLMs](https://arxiv.org/abs/2510.07175)
*Jongwook Han,Woojung Song,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: LLM在心理测量学评估中存在数据污染问题，尤其是在记忆题目和目标分数匹配方面。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM进行心理测量学评估时，可能存在数据污染的风险，但缺乏系统性的量化研究。

Method: 提出一个包含项目记忆、评估记忆和目标分数匹配三个方面的框架，用于系统性地测量LLM心理测量学评估中的数据污染。

Result: 研究发现，包括BFI-44和PVQ-40在内的流行心理测量工具存在严重的数据污染，LLM不仅会记住题目，还能调整回答以达到特定分数。

Conclusion: LLM在心理测量学评估中存在显著的数据污染问题，影响评估的可靠性。

Abstract: Recent studies apply psychometric questionnaires to Large Language Models
(LLMs) to assess high-level psychological constructs such as values,
personality, moral foundations, and dark traits. Although prior work has raised
concerns about possible data contamination from psychometric inventories, which
may threaten the reliability of such evaluations, there has been no systematic
attempt to quantify the extent of this contamination. To address this gap, we
propose a framework to systematically measure data contamination in
psychometric evaluations of LLMs, evaluating three aspects: (1) item
memorization, (2) evaluation memorization, and (3) target score matching.
Applying this framework to 21 models from major families and four widely used
psychometric inventories, we provide evidence that popular inventories such as
the Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)
exhibit strong contamination, where models not only memorize items but can also
adjust their responses to achieve specific target scores.

</details>


### [174] [CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models](https://arxiv.org/abs/2510.07177)
*Yong-En Tian,Yu-Chien Tang,An-Zi Yen,Wen-Chih Peng*

Main category: cs.CL

TL;DR: 本研究提出了一种名为CARPAS的新任务，用于根据文档内容动态调整预设方面，以生成更精细、更符合用户期望的摘要。研究发现，大型语言模型（LLM）倾向于生成过多方面，导致摘要冗长且不相关。因此，研究者提出了一种预测相关方面数量的子任务，并证明这能有效指导LLM聚焦于关键方面，显著提升摘要性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方面级摘要方法通常依赖预设方面，但在实际应用中，这些方面可能不完整、不相关或缺失。用户期望系统能根据文档内容自适应地调整这些方面。

Method: 提出了一种名为CARPAS的新任务，即内容感知方面细化，旨在根据文档上下文动态调整预设方面。构建了三个新数据集，并测试了四种提示策略下LLM在该任务上的表现。在此基础上，提出一个子任务：预测相关方面的数量，并利用该数量指导LLM。

Result: LLM在CARPAS任务中倾向于预测过多的方面，导致摘要过长且不相关。预测相关方面数量的子任务能有效减少LLM的推理难度，并使其专注于最相关的方面。所提出的方法在所有数据集上都显著提高了性能。

Conclusion: 预测相关方面数量可以作为指导LLM进行方面级摘要的有效方法，使其聚焦于最相关的方面，从而生成更优的摘要。研究还揭示了LLM在面对与自身估计不同的方面数量要求时的依从性，这对于LLM在类似实际应用中的部署具有重要意义。

Abstract: Aspect-based summarization has attracted significant attention for its
ability to generate more fine-grained and user-aligned summaries. While most
existing approaches assume a set of predefined aspects as input, real-world
scenarios often present challenges where these given aspects may be incomplete,
irrelevant, or entirely missing from the document. Users frequently expect
systems to adaptively refine or filter the provided aspects based on the actual
content. In this paper, we initiate this novel task setting, termed
Content-Aware Refinement of Provided Aspects for Summarization (CARPAS), with
the aim of dynamically adjusting the provided aspects based on the document
context before summarizing. We construct three new datasets to facilitate our
pilot experiments, and by using LLMs with four representative prompting
strategies in this task, we find that LLMs tend to predict an overly
comprehensive set of aspects, which often results in excessively long and
misaligned summaries. Building on this observation, we propose a preliminary
subtask to predict the number of relevant aspects, and demonstrate that the
predicted number can serve as effective guidance for the LLMs, reducing the
inference difficulty, and enabling them to focus on the most pertinent aspects.
Our extensive experiments show that the proposed approach significantly
improves performance across all datasets. Moreover, our deeper analyses uncover
LLMs' compliance when the requested number of aspects differs from their own
estimations, establishing a crucial insight for the deployment of LLMs in
similar real-world applications.

</details>


### [175] [Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible](https://arxiv.org/abs/2510.07178)
*Imry Ziv,Nur Lan,Emmanuel Chemla,Roni Katzir*

Main category: cs.CL

TL;DR: LLM在学习人类可能和不可能的语言时表现相似，与人类的语言学习偏见不同。


<details>
  <summary>Details</summary>
Motivation: 探究LLM是否区分人类可能和不可能的语言，以及这是否意味着LLM与人类拥有相同的先天学习偏见。

Method: 通过比较LLM在真实语言数据集和经过扰动的人类不可能语言数据集上的学习曲线，并引入更宽松的条件来检验LLM是否能区分这两类语言。

Result: 在大多数情况下，GPT-2学习真实语言和其对应的“不可能”语言具有相同的难易程度，这与之前的研究结论相反。在更宽松的测试条件下，GPT-2也未能系统地区分可能和不可能的语言。

Conclusion: LLM不具备塑造语言类型的、人类所具有的先天偏见。

Abstract: Are large language models (LLMs) sensitive to the distinction between humanly
possible languages and humanly impossible languages? This question is taken by
many to bear on whether LLMs and humans share the same innate learning biases.
Previous work has attempted to answer it in the positive by comparing LLM
learning curves on existing language datasets and on "impossible" datasets
derived from them via various perturbation functions. Using the same
methodology, we examine this claim on a wider set of languages and impossible
perturbations. We find that in most cases, GPT-2 learns each language and its
impossible counterpart equally easily, in contrast to previous claims. We also
apply a more lenient condition by testing whether GPT-2 provides any kind of
separation between the whole set of natural languages and the whole set of
impossible languages. By considering cross-linguistic variance in various
metrics computed on the perplexity curves, we show that GPT-2 provides no
systematic separation between the possible and the impossible. Taken together,
these perspectives show that LLMs do not share the human innate biases that
shape linguistic typology.

</details>


### [176] [Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models](https://arxiv.org/abs/2510.07203)
*Benjamin Akera,Evelyn Nafula Ouma,Gilbert Yiga,Patrick Walukagga,Phionah Natukunda,Trevor Saaka,Solomon Nsumba,Lilian Teddy Nabukeera,Joel Muhanguzi,Imran Sekalala,Nimpamya Janat Namara,Engineer Bainomugisha,Ernest Mwebaze,John Quinn*

Main category: cs.CL

TL;DR: 非洲的2000多种语言中，有许多未被语言技术惠及。现有领先的语言模型虽然能处理斯瓦希里语或约鲁巴语等常见语言，但因优先支持使用者最多的语言，导致在其他语言上的能力碎片化。本文提出区域性方法更有效，并以语言高度多样化的乌干达为例。我们开发了基于Qwen 3的Sunflower 14B和32B模型，它们能最好地理解大多数乌干达语言。这些开源模型可用于减少实际应用中的语言障碍。


<details>
  <summary>Details</summary>
Motivation: 非洲语言多样性高，但大多数语言未被语言技术覆盖。现有模型优先支持使用者最多的语言，导致能力碎片化。需要区域性方法来更有效地解决这一问题。

Method: 开发基于Qwen 3的Sunflower 14B和32B模型，重点关注乌干达的语言多样性，以实现对大多数乌干达语言的先进理解。

Result: 开发了Sunflower 14B和32B模型，能够理解大多数乌干达语言。

Conclusion: 提出的区域性方法，以Sunflower模型为例，可以更有效地解决非洲语言在语言技术方面的不足，并减少实际应用中的语言障碍。

Abstract: There are more than 2000 living languages in Africa, most of which have been
bypassed by advances in language technology. Current leading LLMs exhibit
strong performance on a number of the most common languages (e.g. Swahili or
Yoruba), but prioritise support for the languages with the most speakers first,
resulting in piecemeal ability across disparate languages. We contend that a
regionally focussed approach is more efficient, and present a case study for
Uganda, a country with high linguistic diversity. We describe the development
of Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the
art comprehension in the majority of all Ugandan languages. These models are
open source and can be used to reduce language barriers in a number of
important practical applications.

</details>


### [177] [Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models](https://arxiv.org/abs/2510.07213)
*Chengzhi Zhong,Fei Cheng,Qianying Liu,Yugo Murawaki,Chenhui Chu,Sadao Kurohashi*

Main category: cs.CL

TL;DR: 大型语言模型在有限的非英语数据暴露下仍表现出强大的多语言能力。通过识别和操纵跨语言转换中的一小部分稀疏维度，我们提出了一种无需训练的方法，可以仅用少量数据即可在保持语义内容的同时切换输出语言，并且优于以前的方法。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLM）在有限的非英语数据暴露下仍表现出强大的多语言能力的原因，并探索其跨语言转换机制。

Method: 通过识别和操纵LLM中间层到最终层中一小部分稀疏且跨层一致的维度，来实现在保持语义内容的同时切换输出语言。

Result: 实验表明，通过干预这些维度可以成功切换输出语言，同时保留语义内容，并且该方法在生成控制任务上优于先前基于神经元的方法，成本也大大降低。

Conclusion: 本文提出的无需训练的方法，通过识别和操纵LLM中的特定维度，可以有效地实现跨语言生成控制，并具有良好的可解释性和效率。

Abstract: Large language models exhibit strong multilingual capabilities despite
limited exposure to non-English data. Prior studies show that English-centric
large language models map multilingual content into English-aligned
representations at intermediate layers and then project them back into
target-language token spaces in the final layer. From this observation, we
hypothesize that this cross-lingual transition is governed by a small and
sparse set of dimensions, which occur at consistent indices across the
intermediate to final layers. Building on this insight, we introduce a simple,
training-free method to identify and manipulate these dimensions, requiring
only as few as 50 sentences of either parallel or monolingual data. Experiments
on a multilingual generation control task reveal the interpretability of these
dimensions, demonstrating that the interventions in these dimensions can switch
the output language while preserving semantic content, and that it surpasses
the performance of prior neuron-based approaches at a substantially lower cost.

</details>


### [178] [How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu](https://arxiv.org/abs/2510.07221)
*Benjamin Akera,Evelyn Nafula,Patrick Walukagga,Gilbert Yiga,John Quinn,Ernest Mwebaze*

Main category: cs.CL

TL;DR: 在低资源非洲语言的自动语音识别(ASR)方面，尽管有像Whisper这样的大型多语言模型，但在实际部署前仍需解决数据量和故障模式的问题。本研究通过在卢旺达语和基库尤语上的实验，发现仅需50小时的数据即可达到可用性能，200小时可显著提升。同时，数据质量（尤其是噪声）是导致高错误率的主要因素，占高错误案例的38.6%。这些发现为低资源语言ASR系统的开发提供了实践指导。


<details>
  <summary>Details</summary>
Motivation: 由于非洲语言的转录语音数据有限，开发自动语音识别（ASR）系统仍然具有挑战性。尽管像OpenAI的Whisper这样的大型多语言模型为低资源ASR开发提供了有希望的途径，但关于实际部署要求的问题仍然存在。

Method: 本研究通过对两种班图语（卢旺达语和基库尤语）进行全面的实验来评估Whisper的性能：对卢旺达语使用1到1400小时的训练集进行系统的数据量扩展分析，并对基库尤语使用270小时的训练数据进行详细的错误分析。

Result: 扩展实验表明，仅需50小时的训练数据即可实现可行的ASR性能（词错误率<13%），并且在200小时时性能持续显著提高（词错误率<10%）。错误分析显示，数据质量问题（特别是噪声大的真实转录）占高错误案例的38.6%。

Conclusion: 本研究为低资源语言开发ASR系统的团队提供了可行的基准和部署指导，强调数据质量和数据量对于鲁棒的系统性能都至关重要。

Abstract: The development of Automatic Speech Recognition (ASR) systems for
low-resource African languages remains challenging due to limited transcribed
speech data. While recent advances in large multilingual models like OpenAI's
Whisper offer promising pathways for low-resource ASR development, critical
questions persist regarding practical deployment requirements. This paper
addresses two fundamental concerns for practitioners: determining the minimum
data volumes needed for viable performance and characterizing the primary
failure modes that emerge in production systems. We evaluate Whisper's
performance through comprehensive experiments on two Bantu languages:
systematic data scaling analysis on Kinyarwanda using training sets from 1 to
1,400 hours, and detailed error characterization on Kikuyu using 270 hours of
training data. Our scaling experiments demonstrate that practical ASR
performance (WER < 13\%) becomes achievable with as little as 50 hours of
training data, with substantial improvements continuing through 200 hours (WER
< 10\%). Complementing these volume-focused findings, our error analysis
reveals that data quality issues, particularly noisy ground truth
transcriptions, account for 38.6\% of high-error cases, indicating that careful
data curation is as critical as data volume for robust system performance.
These results provide actionable benchmarks and deployment guidance for teams
developing ASR systems across similar low-resource language contexts. We
release accompanying and models see
https://github.com/SunbirdAI/kinyarwanda-whisper-eval

</details>


### [179] [Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation](https://arxiv.org/abs/2510.07227)
*Arjun Krishnakumar,Rhea Sanjay Sukthanker,Hannan Javed Mahadik,Gabriela Kadlecová,Vladyslav Moroshan,Timur Carstensen,Frank Hutter,Aaron Klein*

Main category: cs.CL

TL;DR: 小型语言模型（SLM）通过使用更少的资源和蒸馏技术，实现了与大型语言模型（LLM）相当的性能，并且训练效率更高。


<details>
  <summary>Details</summary>
Motivation: 研究如何更有效地预训练小型语言模型（SLM），使其在有限的资源下达到与大型模型相当的性能。

Method: 提出一个包含三种互补思想的框架：1. 使用结构稀疏的子网络初始化；2. 利用进化搜索自动发现高质量的子网络初始化；3. 应用来自大型教师模型的知识蒸馏。

Result: 通过进化搜索和LLM权重初始化，模型在相同的验证困惑度下，预训练所需的token数量减少了9.2倍。

Conclusion: 该框架提高了SLM预训练的效率，为大规模、低成本的小型语言模型开发提供了实际可行的途径。

Abstract: Small Language models (SLMs) offer an efficient and accessible alternative to
Large Language Models (LLMs), delivering strong performance while using far
fewer resources. We introduce a simple and effective framework for pretraining
SLMs that brings together three complementary ideas. First, we identify
structurally sparse sub-network initializations that consistently outperform
randomly initialized models of similar size under the same compute budget.
Second, we use evolutionary search to automatically discover high-quality
sub-network initializations, providing better starting points for pretraining.
Third, we apply knowledge distillation from larger teacher models to speed up
training and improve generalization. Together, these components make SLM
pretraining substantially more efficient: our best model, discovered using
evolutionary search and initialized with LLM weights, matches the validation
perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining
tokens. We release all code and models at
https://github.com/whittle-org/whittle/, offering a practical and reproducible
path toward cost-efficient small language model development at scale.

</details>


### [180] [Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping](https://arxiv.org/abs/2510.07230)
*Ziyi Wang,Yuxuan Lu,Yimeng Zhang,Jing Huang,Dakuo Wang*

Main category: cs.CL

TL;DR: LLM代理可以通过基于强化学习的方法，并结合用户画像，来更精确地模拟个性化的用户行为。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在模拟人类分步行为时，通常学习的是一种面向大众的通用策略，而忽略了个性化用户画像，导致模拟结果不够精确。本研究旨在探索如何使LLM代理更好地模拟个性化的用户行为。

Method: 提出了一种名为Customer-R1的基于强化学习的方法，该方法将用户画像作为策略的条件，并通过对行动正确性的奖励信号来优化生成下一步的推理过程和行动。

Result: 在OPeRA数据集上的实验表明，Customer-R1在下一步行动预测任务上显著优于基于提示和监督微调的方法，并且能更好地匹配用户的行动分布，表明其在个性化行为模拟方面具有更高的保真度。

Conclusion: Customer-R1通过引入用户画像和基于奖励的优化，能够更有效地模拟个性化的用户分步行为，并在准确性和保真度方面超越现有方法。

Abstract: Simulating step-wise human behavior with Large Language Models (LLMs) has
become an emerging research direction, enabling applications in various
practical domains. While prior methods, including prompting, supervised
fine-tuning (SFT), and reinforcement learning (RL), have shown promise in
modeling step-wise behavior, they primarily learn a population-level policy
without conditioning on a user's persona, yielding generic rather than
personalized simulations. In this work, we pose a critical question: how can
LLM agents better simulate personalized user behavior? We introduce
Customer-R1, an RL-based method for personalized, step-wise user behavior
simulation in online shopping environments. Our policy is conditioned on an
explicit persona, and we optimize next-step rationale and action generation via
action correctness reward signals. Experiments on the OPeRA dataset emonstrate
that Customer-R1 not only significantly outperforms prompting and SFT-based
baselines in next-action prediction tasks, but also better matches users'
action distribution, indicating higher fidelity in personalized behavior
simulation.

</details>


### [181] [Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships](https://arxiv.org/abs/2510.07231)
*Donggyu Lee,Sungwon Park,Yerin Hwang,Hyunwoo Oh,Hyoshin Kim,Jungwon Kim,Meeyoung Cha,Sangyoon Park,Jihee Kim*

Main category: cs.CL

TL;DR: LLMs在因果推理方面存在严重局限性，现有基准测试存在缺陷。本研究提出了一个包含40,379个评估项的新基准，涵盖了经济学和金融学期刊中的因果关系，并进行了跨领域评估。结果显示，即使是顶尖LLMs的准确率也仅为57.6%，模型规模与性能提升并不总是一致，表明LLMs在可靠的因果推理方面与高风险应用的需求存在差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在合成数据和狭窄的领域覆盖方面存在局限性，而LLMs需要真正的因果推理能力。

Method: 从顶尖经济学和金融学期刊中提取因果关系，并利用工具变量、双重差分和回归不连续性设计等方法进行构建。基准测试包含40,379个评估项，覆盖五个任务类型和五个领域。

Result: 在八个最先进的LLMs上进行实验，结果显示即使是最好的模型准确率也仅为57.6%。模型规模与性能提升并不总是一致，表明LLMs在识别基本因果关系方面存在困难。

Conclusion: 当前的LLMs在因果推理能力方面存在显著的不足，与高风险应用对可靠因果推理的需求之间存在差距。

Abstract: Causal reasoning is fundamental for Large Language Models (LLMs) to
understand genuine cause-and-effect relationships beyond pattern matching.
Existing benchmarks suffer from critical limitations such as reliance on
synthetic data and narrow domain coverage. We introduce a novel benchmark
constructed from casually identified relationships extracted from top-tier
economics and finance journals, drawing on rigorous methodologies including
instrumental variables, difference-in-differences, and regression discontinuity
designs. Our benchmark comprises 40,379 evaluation items covering five task
types across domains such as health, environment, technology, law, and culture.
Experimental results on eight state-of-the-art LLMs reveal substantial
limitations, with the best model achieving only 57.6\% accuracy. Moreover,
model scale does not consistently translate to superior performance, and even
advanced reasoning models struggle with fundamental causal relationship
identification. These findings underscore a critical gap between current LLM
capabilities and demands of reliable causal reasoning in high-stakes
applications.

</details>


### [182] [LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding](https://arxiv.org/abs/2510.07233)
*Zhivar Sourati,Zheng Wang,Marianne Menglin Liu,Yazhe Hu,Mengqing Guo,Sujeeth Bharadwaj,Kyu Han,Tao Sheng,Sujith Ravi,Morteza Dehghani,Dan Roth*

Main category: cs.CL

TL;DR: LAD-RAG是一种新的文档问答框架，通过构建文档图和动态检索来提高答案质量。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG方法在处理视觉丰富的文档时会丢失结构和跨页依赖信息，导致信息检索不完整和答案质量下降。

Method: LAD-RAG在摄入时构建符号文档图来捕获布局结构和跨页依赖，并在推理时使用LLM代理动态地与神经网络和符号索引进行交互以适应性地检索证据。

Result: LAD-RAG在MMLongBench-Doc、LongDocURL、DUDE和MP-DocVQA数据集上进行了实验，平均完美召回率超过90%，在相似的噪声水平下，召回率比基线检索器提高了20%，并以最小的延迟提高了问答准确性。

Conclusion: LAD-RAG通过引入布局感知和动态检索，显著提高了视觉丰富文档问答的检索和答案质量。

Abstract: Question answering over visually rich documents (VRDs) requires reasoning not
only over isolated content but also over documents' structural organization and
cross-page dependencies. However, conventional retrieval-augmented generation
(RAG) methods encode content in isolated chunks during ingestion, losing
structural and cross-page dependencies, and retrieve a fixed number of pages at
inference, regardless of the specific demands of the question or context. This
often results in incomplete evidence retrieval and degraded answer quality for
multi-page reasoning tasks. To address these limitations, we propose LAD-RAG, a
novel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs
a symbolic document graph that captures layout structure and cross-page
dependencies, adding it alongside standard neural embeddings to yield a more
holistic representation of the document. During inference, an LLM agent
dynamically interacts with the neural and symbolic indices to adaptively
retrieve the necessary evidence based on the query. Experiments on
MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG
improves retrieval, achieving over 90% perfect recall on average without any
top-k tuning, and outperforming baseline retrievers by up to 20% in recall at
comparable noise levels, yielding higher QA accuracy with minimal latency.

</details>


### [183] [When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation](https://arxiv.org/abs/2510.07238)
*Xunyi Jiang,Dingyi Chang,Julian McAuley,Xin Xu*

Main category: cs.CL

TL;DR: 现有LLM评估基准已过时，无法可靠评估LLM的事实准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界和大型语言模型的快速发展已超越了广泛使用的评估基准的静态性质，这引发了对其评估LLM事实准确性可靠性的担忧。然而，现有研究在很大程度上依赖于过时且流行的基准，但这些基准与现实世界的事实和现代LLM在时间上的不匹配及其对LLM事实准确性评估的影响仍有待探索。

Method: 通过检查五个流行的事实准确性基准和八个不同年份发布的LLM，系统地调查此问题。采用最新的事实检索流程和三种指标来量化基准的过时程度及其对LLM事实准确性评估的影响。

Result: 实验结果和分析表明，现有基准中很大一部分样本已过时，导致对LLM事实准确性的评估不可靠。

Conclusion: 现有LLM事实准确性评估基准存在过时问题，影响评估的可靠性。本研究提供了一个评估基准可靠性的测试平台，并鼓励对基准过时问题进行更多研究。

Abstract: The rapid evolution of large language models (LLMs) and the real world has
outpaced the static nature of widely used evaluation benchmarks, raising
concerns about their reliability for evaluating LLM factuality. While
substantial works continue to rely on the popular but old benchmarks, their
temporal misalignment with real-world facts and modern LLMs, and their effects
on LLM factuality evaluation remain underexplored. Therefore, in this work, we
present a systematic investigation of this issue by examining five popular
factuality benchmarks and eight LLMs released across different years. An
up-to-date fact retrieval pipeline and three metrics are tailored to quantify
benchmark aging and its impact on LLM factuality evaluation. Experimental
results and analysis illustrate that a considerable portion of samples in the
widely used factuality benchmarks are outdated, leading to unreliable
assessments of LLM factuality. We hope our work can provide a testbed to assess
the reliability of a benchmark for LLM factuality evaluation and inspire more
research on the benchmark aging issue. Codes are available in
https://github.com/JiangXunyi/BenchAge.

</details>


### [184] [Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts](https://arxiv.org/abs/2510.07239)
*Christos Ziakas,Nicholas Loo,Nishita Jain,Alessandra Russo*

Main category: cs.CL

TL;DR: Red-Bandit是一个自适应的红队测试框架，可以通过在线学习来识别和利用大型语言模型的特定漏洞，并在AdvBench上取得了最先进的成果，同时生成更易读的提示。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化红队测试方法在适应推理时模型特有的漏洞方面效率低下。

Method: Red-Bandit通过强化学习对一组参数高效的LoRA专家进行后训练，每位专家专门针对一种攻击风格，并使用基于规则的安全模型来奖励不安全提示的生成。在推理时，一个多臂老虎机策略根据目标模型的响应安全性动态地选择这些攻击风格专家，以平衡探索和利用。

Result: Red-Bandit在AdvBench上实现了最先进的攻击成功率（ASR@10），同时生成了更具可读性的提示（较低的困惑度）。此外，Red-Bandit的老虎机策略可以作为诊断工具，通过指示哪些攻击风格最能有效地引发不安全行为来揭示模型特定的漏洞。

Conclusion: Red-Bandit框架通过其在线适应能力和多臂老虎机策略，在红队测试中实现了效率和有效性的提高，并为理解和解决大型语言模型的安全漏洞提供了一个有前途的途径。

Abstract: Automated red-teaming has emerged as a scalable approach for auditing Large
Language Models (LLMs) prior to deployment, yet existing approaches lack
mechanisms to efficiently adapt to model-specific vulnerabilities at inference.
We introduce Red-Bandit, a red-teaming framework that adapts online to identify
and exploit model failure modes under distinct attack styles (e.g.,
manipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA
experts, each specialized for a particular attack style, using reinforcement
learning that rewards the generation of unsafe prompts via a rule-based safety
model. At inference, a multi-armed bandit policy dynamically selects among
these attack-style experts based on the target model's response safety,
balancing exploration and exploitation. Red-Bandit achieves state-of-the-art
results on AdvBench under sufficient exploration (ASR@10), while producing more
human-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy
serves as a diagnostic tool for uncovering model-specific vulnerabilities by
indicating which attack styles most effectively elicit unsafe behaviors.

</details>


### [185] [Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense](https://arxiv.org/abs/2510.07242)
*Leitian Tao,Ilia Kulikov,Swarnadeep Saha,Tianlu Wang,Jing Xu,Yixuan Li,Jason E Weston,Ping Yu*

Main category: cs.CL

TL;DR: HERO框架结合了可验证奖励和奖励模型分数，以提高大型语言模型（LLMs）的推理能力，在数学推理基准测试中表现优于仅使用奖励模型或仅使用验证器的基线。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）推理训练方法主要依赖于可验证奖励，但这种二元反馈（0-1信号）过于僵化，无法处理部分正确或有多种可行答案的任务，限制了模型的学习。

Method: 提出了一种名为HERO（Hybrid Ensemble Reward Optimization）的强化学习框架，该框架结构化地整合了验证器信号和奖励模型分数。HERO使用分层归一化来约束奖励模型分数，并使用考虑方差的加权来优先处理具有挑战性的提示。

Result: 在各种数学推理基准测试中，HERO的表现始终优于仅使用奖励模型（RM-only）和仅使用验证器（verifier-only）的方法，在可验证和难以验证的任务上都取得了显著的提升。

Conclusion: 混合奖励设计能够保留验证器的稳定性，同时利用奖励模型的细微差别来提升LLMs的推理能力。

Abstract: Post-training for reasoning of large language models (LLMs) increasingly
relies on verifiable rewards: deterministic checkers that provide 0-1
correctness signals. While reliable, such binary feedback is brittle--many
tasks admit partially correct or alternative answers that verifiers
under-credit, and the resulting all-or-nothing supervision limits learning.
Reward models offer richer, continuous feedback, which can serve as a
complementary supervisory signal to verifiers. We introduce HERO (Hybrid
Ensemble Reward Optimization), a reinforcement learning framework that
integrates verifier signals with reward-model scores in a structured way. HERO
employs stratified normalization to bound reward-model scores within
verifier-defined groups, preserving correctness while refining quality
distinctions, and variance-aware weighting to emphasize challenging prompts
where dense signals matter most. Across diverse mathematical reasoning
benchmarks, HERO consistently outperforms RM-only and verifier-only baselines,
with strong gains on both verifiable and hard-to-verify tasks. Our results show
that hybrid reward design retains the stability of verifiers while leveraging
the nuance of reward models to advance reasoning.

</details>


### [186] [LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation](https://arxiv.org/abs/2510.07243)
*Joseph Enguehard,Morgane Van Ermengem,Kate Atkinson,Sujeong Cha,Arijit Ghosh Chowdhury,Prashanth Kallur Ramaswamy,Jeremy Roghair,Hannah R Marlowe,Carina Suzana Negreanu,Kitty Boxall,Diana Mincu*

Main category: cs.CL

TL;DR: LLM在法律领域的评估面临挑战，现有方法有局限性。本文提出了一种新颖的、无需参考数据的评估方法，将长响应分解为‘法律数据点’（LDPs），并证明其优于基线方法，与人类专家的评估更相关，并提高了标注者间的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法在法律领域存在局限性，成本高或标准化程度不足，无法满足法律行业特有的评估需求和可信度要求。

Method: a) 将长响应分解为‘法律数据点’（LDPs），并引入一种新的、无参考的评估方法，模拟律师的评估方式；b) 在专有数据集和LegalBench数据集上证明该方法优于基线；c) 证明该方法与人类专家评估的相关性更高，并能提高标注者间的一致性；d) 开源部分LegalBench的LDPs。

Result: 所提出的方法在专有数据集和LegalBench数据集上均优于基线方法，并且与人类专家评估的相关性更高，提高了标注者间的一致性。

Conclusion: 本文提出的基于‘法律数据点’（LDPs）的无参考评估方法，有效解决了LLM在法律领域评估的挑战，其有效性、与人类专家的相关性以及对提高一致性的作用均得到验证，并为社区开源了相关数据。

Abstract: Evaluating large language model (LLM) outputs in the legal domain presents
unique challenges due to the complex and nuanced nature of legal analysis.
Current evaluation approaches either depend on reference data, which is costly
to produce, or use standardized assessment methods, both of which have
significant limitations for legal applications.
  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its
reliability and effectiveness in legal contexts depend heavily on evaluation
processes unique to the legal industry and how trustworthy the evaluation
appears to the human legal expert. This is where existing evaluation methods
currently fail and exhibit considerable variability.
  This paper aims to close the gap: a) we break down lengthy responses into
'Legal Data Points' (LDPs), self-contained units of information, and introduce
a novel, reference-free evaluation methodology that reflects how lawyers
evaluate legal answers; b) we demonstrate that our method outperforms a variety
of baselines on both our proprietary dataset and an open-source dataset
(LegalBench); c) we show how our method correlates more closely with human
expert evaluations and helps improve inter-annotator agreement; and finally d)
we open source our Legal Data Points for a subset of LegalBench used in our
experiments, allowing the research community to replicate our results and
advance research in this vital area of LLM evaluation on legal
question-answering.

</details>


### [187] [Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models](https://arxiv.org/abs/2510.07248)
*Jonggeun Lee,Woojung Song,Jongwook Han,Haesung Pyun,Yohan Jo*

Main category: cs.CL

TL;DR: 小模型在工具使用方面存在挑战，特别是在工具选择和参数识别方面。本研究提出了一种名为PA-Tool的方法，通过生成与预训练知识对齐的工具模式来解决模式不匹配问题，从而提高小模型的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 小语言模型（SLMs）在工具使用任务中存在工具选择和参数识别的挑战，主要表现为模式不匹配，即模型会生成预训练中存在的但实际模式中不存在的工具名称。

Method: 提出了一种名为PA-Tool（预训练对齐工具模式生成）的训练无关方法，利用峰度信号（来自污染检测）来自动重命名工具组件。该方法通过生成多个候选名称并选择在样本中具有最高输出集中的名称，从而识别出与预训练对齐的命名模式。

Result: 在MetaTool和RoTBench上的实验显示，PA-Tool的性能提升高达17个百分点，模式不匹配错误减少了80%。

Conclusion: PA-Tool能够使小模型在保持计算效率的同时，接近最先进的性能，并能适应新工具而无需重新训练。该研究表明，通过模式级别的干预，使模式与模型对齐，可以解锁资源高效模型在工具使用方面的潜力。

Abstract: Small language models (SLMs) offer significant computational advantages for
tool-augmented AI systems, yet they struggle with tool-use tasks, particularly
in selecting appropriate tools and identifying correct parameters. A common
failure mode is schema misalignment: models hallucinate plausible but
non-existent tool names that reflect naming conventions internalized during
pretraining but absent from the provided tool schema. Rather than forcing
models to adapt to arbitrary schemas, we propose adapting schemas to align with
models' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool
Schema Generation), a training-free method that leverages peakedness-a signal
from contamination detection indicating pretraining familiarity-to
automatically rename tool components. By generating multiple candidates and
selecting those with highest output concentration across samples, PA-Tool
identifies pretrain-aligned naming patterns. Experiments on MetaTool and
RoTBench show improvements of up to 17% points, with schema misalignment errors
reduced by 80%. PA-Tool enables small models to approach state-of-the-art
performance while maintaining computational efficiency for adaptation to new
tools without retraining. Our work demonstrates that schema-level interventions
can unlock the tool-use potential of resource-efficient models by adapting
schemas to models rather than models to schemas.

</details>


### [188] [Online Rubrics Elicitation from Pairwise Comparisons](https://arxiv.org/abs/2510.07284)
*MohammadHossein Rezaei,Robert Vacareanu,Zihao Wang,Clinton Wang,Yunzhong He,Afra Feyza Akyürek*

Main category: cs.CL

TL;DR: 文章提出了一种名为OnlineRubrics的动态评分方法，通过在线配对比较来生成和更新评估标准，以改进基于评分卡的LLM训练，解决了静态评分卡易受奖励篡改和无法适应训练过程中出现的新需求的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于评分卡的LLM训练方法依赖于静态评分卡，容易导致奖励篡改行为，并且无法捕捉训练过程中出现的新需求。

Method: 提出了一种名为OnlineRubrics的在线评分方法，通过对当前策略和参考策略的响应进行成对比较，动态地筛选评估标准。

Result: 与仅使用静态评分卡训练相比，该方法在AlpacaEval、GPQA、ArenaHard以及专家问题和评分卡的验证集上，平均取得了高达8%的改进。

Conclusion: OnlineRubrics通过动态适应性地生成评估标准，能够持续识别和缓解LLM训练过程中的错误，从而在各种基准测试中取得显著的性能提升。

Abstract: Rubrics provide a flexible way to train LLMs on open-ended long-form answers
where verifiable rewards are not applicable and human preferences provide
coarse signals. Prior work shows that reinforcement learning with rubric-based
rewards leads to consistent gains in LLM post-training. Most existing
approaches rely on rubrics that remain static over the course of training. Such
static rubrics, however, are vulnerable to reward-hacking type behaviors and
fail to capture emergent desiderata that arise during training. We introduce
Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates
evaluation criteria in an online manner through pairwise comparisons of
responses from current and reference policies. This online process enables
continuous identification and mitigation of errors as training proceeds.
Empirically, this approach yields consistent improvements of up to 8% over
training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as
well as the validation sets of expert questions and rubrics. We qualitatively
analyze the elicited criteria and identify prominent themes such as
transparency, practicality, organization, and reasoning.

</details>


### [189] [On the Convergence of Moral Self-Correction in Large Language Models](https://arxiv.org/abs/2510.07290)
*Guangliang Liu,Haitao Mao,Bochuan Cao,Zhiyu Xue,Xitong Zhang,Rongrong Wang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: LLMs可以通过内部知识进行自我纠正，尤其是在道德相关的任务上。通过多轮交互，自我纠正指令可以激活模型中的道德概念，减少不确定性，从而使模型性能收敛。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）在没有具体错误指导的情况下，如何利用其内部知识进行自我纠正（内源性自我纠正），特别是关注其在道德相关任务上的表现，并解释其有效性机制。

Method: 通过多轮交互实验，分析LLMs在道德自我纠正过程中的性能收敛现象，并揭示其背后的机制。

Result: 研究发现，内源性自我纠正具有性能收敛的特点。具体而言，持续的自我纠正指令能够激活模型中的道德概念，降低模型的不确定性，随着交互轮次的增加，模型性能趋于稳定并收敛。

Conclusion: 内源性自我纠正，尤其是在道德相关的任务上，展现出强大的潜力，其核心机制在于通过多轮交互激活道德概念，最终实现性能的收敛。

Abstract: Large Language Models (LLMs) are able to improve their responses when
instructed to do so, a capability known as self-correction. When instructions
provide only a general and abstract goal without specific details about
potential issues in the response, LLMs must rely on their internal knowledge to
improve response quality, a process referred to as intrinsic self-correction.
The empirical success of intrinsic self-correction is evident in various
applications, but how and why it is effective remains unknown. Focusing on
moral self-correction in LLMs, we reveal a key characteristic of intrinsic
self-correction: performance convergence through multi-round interactions; and
provide a mechanistic analysis of this convergence behavior. Based on our
experimental results and analysis, we uncover the underlying mechanism of
convergence: consistently injected self-correction instructions activate moral
concepts that reduce model uncertainty, leading to converged performance as the
activated moral concepts stabilize over successive rounds. This paper
demonstrates the strong potential of moral self-correction by showing that it
exhibits a desirable property of converged performance.

</details>


### [190] [Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning](https://arxiv.org/abs/2510.07300)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Kaiyu Huang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: 为解决大型语言模型在非英语语言上的一致性和准确性问题，M-Thinker通过GRPO算法和语言一致性（LC）及跨语言思维对齐（CTA）奖励进行了优化，实现了近乎100%的语言一致性和在多语言基准上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LRMs）在非英语语言处理方面存在输入输出语言不一致和推理路径错误、准确率低的问题，这限制了LRMs的全球部署和非英语用户的体验。

Method: 提出M-Thinker模型，采用GRPO算法进行训练，引入了语言一致性（LC）奖励和跨语言思维对齐（CTA）奖励。LC奖励确保输入、思考和输出的语言一致性；CTA奖励通过对比非英语和英语的推理路径，将英语的推理能力迁移到非英语语言。

Result: M-Thinker-1.5B/7B模型在多语言基准MMATH和PolyMath上实现了近乎100%的语言一致性和优越的性能，并且在非目标语言上也表现出良好的泛化能力。

Conclusion: M-Thinker通过创新的奖励机制有效解决了大型语言模型在非英语语言处理中的挑战，显著提升了语言一致性和推理准确性，并具备良好的跨语言泛化能力。

Abstract: Large Reasoning Models (LRMs) have achieved remarkable performance on complex
reasoning tasks by adopting the "think-then-answer" paradigm, which enhances
both accuracy and interpretability. However, current LRMs exhibit two critical
limitations when processing non-English languages: (1) They often struggle to
maintain input-output language consistency; (2) They generally perform poorly
with wrong reasoning paths and lower answer accuracy compared to English. These
limitations significantly degrade the user experience for non-English speakers
and hinder the global deployment of LRMs. To address these limitations, we
propose M-Thinker, which is trained by the GRPO algorithm that involves a
Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment
(CTA) reward. Specifically, the LC reward defines a strict constraint on the
language consistency between the input, thought, and answer. Besides, the CTA
reward compares the model's non-English reasoning paths with its English
reasoning path to transfer its own reasoning capability from English to
non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B
models not only achieve nearly 100% language consistency and superior
performance on two multilingual benchmarks (MMATH and PolyMath), but also
exhibit excellent generalization on out-of-domain languages.

</details>


### [191] [Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain](https://arxiv.org/abs/2510.07309)
*Yue Li,Ran Tao,Derek Hommel,Yusuf Denizay Dönder,Sungyong Chang,David Mimno,Unso Eun Seo Jo*

Main category: cs.CL

TL;DR: LLMs在文本到SQL方面表现出色，但在处理现实业务场景中的复杂查询时仍面临挑战。CORGI基准测试突显了LLMs在因果推理、时间预测和战略建议方面的不足，比现有基准测试更难。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL基准测试主要关注事实检索，未能充分反映真实商业环境的复杂性，而这种环境需要LLMs进行更高级的推理和决策。

Method: 提出CORGI基准测试，该测试包含受DoorDash、Airbnb和Lululemon等企业启发的合成数据库，并涵盖描述性、解释性、预测性和推荐性四个递增的查询类别，以模拟多层次、多步骤的商业智能需求。

Result: LLMs在处理CORGI基准测试中的高层级查询时表现不佳，尤其在预测和制定行动计划方面存在困难。CORGI基准测试比BIRD基准测试难21%。

Conclusion: 尽管LLMs在代码生成方面取得了显著进步，但它们在处理需要因果推理、时间预测和战略建议的复杂商业查询方面仍存在差距，这表明需要进一步的研究来弥合LLMs在现实商业智能应用中的能力与需求之间的差距。

Abstract: In the business domain, where data-driven decision making is crucial,
text-to-SQL is fundamental for easy natural language access to structured data.
While recent LLMs have achieved strong performance in code generation, existing
text-to-SQL benchmarks remain focused on factual retrieval of past records. We
introduce CORGI, a new benchmark specifically designed for real-world business
contexts. CORGI is composed of synthetic databases inspired by enterprises such
as Doordash, Airbnb, and Lululemon. It provides questions across four
increasingly complex categories of business queries: descriptive, explanatory,
predictive, and recommendational. This challenge calls for causal reasoning,
temporal forecasting, and strategic recommendation, reflecting multi-level and
multi-step agentic intelligence. We find that LLM performance drops on
high-level questions, struggling to make accurate predictions and offer
actionable plans. Based on execution success rate, the CORGI benchmark is about
21\% more difficult than the BIRD benchmark. This highlights the gap between
popular LLMs and the need for real-world business intelligence. We release a
public dataset and evaluation framework, and a website for public submissions.

</details>


### [192] [Vibe Checker: Aligning Code Evaluation with Human Preference](https://arxiv.org/abs/2510.07315)
*Ming Zhong,Xiang Zhou,Ting-Yun Chang,Qingze Wang,Nan Xu,Xiance Si,Dan Garrette,Shyam Upadhyay,Jeremiah Liu,Jiawei Han,Benoit Schillings,Jiao Sun*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have catalyzed vibe coding, where users leverage
LLMs to generate and iteratively refine code through natural language
interactions until it passes their vibe check. Vibe check is tied to real-world
human preference and goes beyond functionality: the solution should feel right,
read cleanly, preserve intent, and remain correct. However, current code
evaluation remains anchored to pass@k and captures only functional correctness,
overlooking the non-functional instructions that users routinely apply. In this
paper, we hypothesize that instruction following is the missing piece
underlying vibe check that represents human preference in coding besides
functional correctness. To quantify models' code instruction following
capabilities with measurable signals, we present VeriCode, a taxonomy of 30
verifiable code instructions together with corresponding deterministic
verifiers. We use the taxonomy to augment established evaluation suites,
resulting in Vibe Checker, a testbed to assess both code instruction following
and functional correctness. Upon evaluating 31 leading LLMs, we show that even
the strongest models struggle to comply with multiple instructions and exhibit
clear functional regression. Most importantly, a composite score of functional
correctness and instruction following correlates the best with human
preference, with the latter emerging as the primary differentiator on
real-world programming tasks. Our work identifies core factors of the vibe
check, providing a concrete path for benchmarking and developing models that
better align with user preferences in coding.

</details>


### [193] [Artificial Hippocampus Networks for Efficient Long-Context Modeling](https://arxiv.org/abs/2510.07318)
*Yunhao Fang,Weihao Yu,Shu Zhong,Qinghao Ye,Xuehan Xiong,Lai Wei*

Main category: cs.CL

TL;DR: 为了解决长序列建模中效率与信息保真度的矛盾，我们提出了一种受认知科学启发的记忆框架，该框架结合了Transformer的KV缓存（短期记忆）和我们提出的“海马网络”（AHN，长期记忆）。


<details>
  <summary>Details</summary>
Motivation: 长序列建模在效率（如RNN）和信息保真度（如Transformer）之间存在固有权衡。现有模型难以同时高效处理长序列并保留所有相关信息。

Method: 提出了一种新颖的记忆框架，利用Transformer的KV缓存作为短期、无损记忆，并通过一个名为“海马网络”（AHN）的可学习模块，将短期记忆中的信息压缩成固定大小的长期记忆。AHN可以集成到现有的RNN类架构（如Mamba2, DeltaNet）中。

Result: 在LV-Eval和InfiniteBench长上下文基准测试中，集成AHN的模型在效率（计算量和内存）上显著优于仅使用滑动窗口的模型，并且在性能上接近甚至超越了全注意力模型。具体来说，Qwen2.5-3B-Instruct模型在加入AHN后，推理FLOPs减少了40.5%，内存缓存减少了74.0%，并且在LV-Eval（128k序列长度）上的平均得分从4.41提升至5.88。

Conclusion: 所提出的AHN记忆框架能够有效解决长序列建模中的效率与信息保真度问题，实现了计算和内存成本的大幅降低，同时保持或提高了模型性能。

Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency
of compressive fixed-size memory in RNN-like models and the fidelity of
lossless growing memory in attention-based Transformers. Inspired by the
Multi-Store Model in cognitive science, we introduce a memory framework of
artificial neural networks. Our method maintains a sliding window of the
Transformer's KV cache as lossless short-term memory, while a learnable module
termed Artificial Hippocampus Network (AHN) recurrently compresses
out-of-window information into a fixed-size compact long-term memory. To
validate this framework, we instantiate AHNs using modern RNN-like
architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive
experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate
that AHN-augmented models consistently outperform sliding window baselines and
achieve performance comparable or even superior to full-attention models, while
substantially reducing computational and memory requirements. For instance,
augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%
and memory cache by 74.0%, while improving its average score on LV-Eval (128k
sequence length) from 4.41 to 5.88. Code is available at:
https://github.com/ByteDance-Seed/AHN.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [194] [Constant Weighted Maximin Share Approximations for Chores](https://arxiv.org/abs/2510.06581)
*Bo Li,Fangxiao Wang,Shiji Xing*

Main category: cs.GT

TL;DR: 本文提出了第一个恒定因子近似加权最大最小份额（WMMS）算法，解决了加权公平分配中的一个重要开放性问题。此外，本文还证明了2-近似是WMMS问题的最优近似比。


<details>
  <summary>Details</summary>
Motivation: 加权最大最小份额（WMMS）是公平分配不可分割的家务给具有不对称权重的代理人的一种有吸引力的公平概念。然而，它是否允许恒定因子近似仍然是一个未解决的问题，并且是加权公平分配中的一个重要开放性问题。先前已知最佳近似比为O（log n）。

Method: 本文通过引入规范实例缩减和不同的代理人估值界限来解决WMMS问题。

Result: 本文提出了第一个恒定因子近似WMMS算法，并将已知下界从1.366提高到2，证明了2-近似是WMMS问题的最优近似比。

Conclusion: 本文为加权公平分配中的WMMS问题提供了第一个恒定因子近似算法，并确定了最优近似比为2。

Abstract: We study the fair allocation of indivisible chores among agents with
asymmetric weights. Among the various fairness notions, weighted maximin share
(WMMS) stands out as particularly compelling. However, whether WMMS admits a
constant-factor approximation has remained unknown and is one of the important
open problems in weighted fair division [ALMW22, Suk25]. So far, the best known
approximation ratio is O(log n), where n is the number of agents. In this
paper, we advance the state of the art and present the first constant-factor
approximate WMMS algorithm. To this end, we introduce canonical instance
reductions and different bounds of agents' valuations. We also prove that
guaranteeing better than 2-approximation is not possible, which improves the
best-known lower bound of 1.366.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [195] [R3R: Decentralized Multi-Agent Collision Avoidance with Infinite-Horizon Safety](https://arxiv.org/abs/2510.06436)
*Thomas Marshall Vielmetti,Devansh R. Agrawal,Dimitra Panagou*

Main category: cs.MA

TL;DR: R3R是一个新的去中心化框架，用于在通信受限的情况下规划多智能体运动，并提供无限期安全保证。


<details>
  <summary>Details</summary>
Motivation: 现有的去中心化运动规划方法缺乏正式的、无限期的安全保证，尤其是在通信受限的系统中。

Method: R3R结合了gatekeeper安全框架和R-Boundedness几何约束，将通信半径与安全规划能力联系起来，并将轨迹限制在通信半径的函数范围内，从而实现仅使用局部信息即可保证所有时间的安全。该算法是完全异步的，并保证了在有智能体加入、离开和重新规划的زمان-varying网络中的前向不变性。

Result: 在多达128辆Dubins车辆的模拟中，R3R在密集、障碍物丰富的场景中实现了100%的安全性。

Conclusion: R3R提供了一种可扩展且可证明安全的解决方案，适用于多智能体系统，并且其性能与智能体密度而非问题规模相关。

Abstract: Existing decentralized methods for multi-agent motion planning lack formal,
infinite-horizon safety guarantees, especially for communication-constrained
systems. We present R3R, to our knowledge the first decentralized and
asynchronous framework for multi-agent motion planning under distance-based
communication constraints with infinite-horizon safety guarantees for systems
of nonlinear agents. R3R's novelty lies in combining our gatekeeper safety
framework with a geometric constraint called R-Boundedness, which together
establish a formal link between an agent's communication radius and its ability
to plan safely. We constrain trajectories to within a fixed planning radius
that is a function of the agent's communication radius, which enables
trajectories to be shown provably safe for all time, using only local
information. Our algorithm is fully asynchronous, and ensures the forward
invariance of these guarantees even in time-varying networks where agents
asynchronously join, leave, and replan. We validate our approach in simulations
of up to 128 Dubins vehicles, demonstrating 100% safety in dense, obstacle rich
scenarios. Our results demonstrate that R3R's performance scales with agent
density rather than problem size, providing a practical solution for scalable
and provably safe multi-agent systems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [196] [Distributed Detection and Bandwidth Allocation with Hybrid Quantized and Full-Precision Observations over Multiplicative Fading Channels](https://arxiv.org/abs/2510.06429)
*Linlin Mao,Zeping Sui,Michail Matthaiou,Hongbin Li*

Main category: eess.SP

TL;DR: 提出了一种融合量化和全精度观测的混合探测器，用于在加性乘性高斯噪声下进行弱信号检测。该探测器基于局部最强检验（LMPT），并对其渐近检测性能进行了分析。此外，还优化了传感器量化阈值以实现近乎最优的性能，并提出了一种混合整数线性规划方法来解决传输带宽分配问题。仿真结果证明了所提出的混合探测器和带宽分配策略的优越性，尤其是在具有挑战性的易错信道条件下。


<details>
  <summary>Details</summary>
Motivation: 在加性和乘性高斯噪声下进行弱信号检测，同时考虑量化和全精度观测的融合。

Method: 推导基于局部最强检验（LMPT）的混合探测器；分析其渐近检测性能；优化传感器量化阈值；提出混合整数线性规划方法解决传输带宽分配问题。

Result: 仿真结果表明，所提出的混合探测器和带宽分配策略具有优越性，尤其是在易错信道条件下。

Conclusion: 所提出的混合探测器和带宽分配策略在弱信号检测任务中，尤其是在存在噪声和信道限制的情况下，表现出色。

Abstract: A hybrid detector that fuses both quantized and full-precision observations
is proposed for weak signal detection under additive and multiplicative
Gaussian noise. We first derive a locally most powerful test (LMPT)--based
hybrid detector from the composite probability distribution of the compound
observations received by the fusion center, and then analyze its asymptotic
detection performance. Subsequently, we optimize the sensor-wise quantization
thresholds to achieve near-optimal asymptotic performance at the local sensor
level. Moreover, we propose a mixed-integer linear programming approach to
solve the optimization problem of transmission bandwidth allocation accounting
for bandwidth constraints and error-prone channels. Finally, simulation results
demonstrate the superiority of the proposed hybrid detector and the bandwidth
allocation strategy, especially in challenging error-prone channel conditions.

</details>


### [197] [Optimized SVR Framework for Electric Load Forecasting](https://arxiv.org/abs/2510.06476)
*Nishant Gadde,Yoshua Alexander,Sarvesh Parthasarthy,Arman Allidina*

Main category: eess.SP

TL;DR: SVR模型在电力负荷预测方面表现优于行业标准，提高了准确性并减少了均方误差和平均绝对误差。


<details>
  <summary>Details</summary>
Motivation: 由于极端天气和客户对能源不断增长的需求，电网运营商在电力负荷预测方面面临着日益严峻的挑战，有时会导致预测失败。

Method: 提出并使用支持向量回归（SVR）框架进行电力负荷预测。

Result: SVR模型在所有对电力系统运行重要的评估指标上均表现出更好的准确性，均方误差降低了54.2%，平均绝对误差提高了33.5%。

Conclusion: 所提出的SVR方法在电力负荷预测方面提供了显著的改进，可以作为系统规划和资源分配的额外工具。

Abstract: Load forecasting has always been a challenge for grid operators due to the
growing complexity of power systems. The increase in extreme weather and the
need for energy from customers has led to load forecasting sometimes failing.
This research presents a Support Vector Regression (SVR) framework for electric
load forecasting that outperforms the industry standard. The SVR model
demonstrates better accuracy across all evaluation metrics that are important
for power system operations. The model has a 54.2\% reduction in Mean Squared
Error (31.91 vs. 69.63), a 33.5\% improvement in Mean Absolute Error, and
performance benefits across other metrics. These improvements show significant
benefits when integrated with power forecasting tools and show that the
approach provides an additional tool for accuracy checking for system planning
and resource allocation in times of need for resource allocation in electric
power systems.

</details>


### [198] [Cooperative Multi-Static ISAC Networks: A Unified Design Framework for Active and Passive Sensing](https://arxiv.org/abs/2510.06654)
*Yan Yang,Zhendong Li,Jianwei Zhao,Qingqing Wu,Zhiqing Wei,Wen Chen,Weimin Jia*

Main category: eess.SP

TL;DR: 本篇论文提出了一种联合主动和无源传感（JAPS）的统一设计框架，用于多静态协作 ISAC 系统，以优化 DL 和 UL 传输的总速率。


<details>
  <summary>Details</summary>
Motivation: 为了提升集成传感与通信（ISAC）系统的感知精度和通信范围，研究了多静态协作传感技术。

Method: 提出了一种联合主动和无源传感（JAPS）的统一设计框架，并构建了一个多静态协作 ISAC 系统，用于共存的下行（DL）和上行（UL）通信。通过联合优化波束形成、接收滤波器和功率分配来最大化 DL 和 UL 传输的总和速率，同时满足感知需求和传输功率约束。为了解决由此产生的非凸优化问题，采用了利用交替优化（AO）的算法架构。具体来说，在给定接收滤波器和 UL 通信的传输功率的情况下，使用基于逐次凸近似（SCA）和基于惩罚的算法来解决发射波束形成子问题。开发了一种基于分数规划（FP）的算法来解决接收滤波器和 UL 通信的传输功率优化子问题。

Result: 数值结果验证了所提出的 JAPS 方案的性能提升，并证明了所提出算法的有效性。

Conclusion: 所提出的 JAPS 框架和优化算法能够有效地提升多静态协作 ISAC 系统的性能。

Abstract: Multi-static cooperative sensing emerges as a promising technology for
advancing integrated sensing and communication (ISAC), enhancing sensing
accuracy and range. In this paper, we develop a unified design framework for
joint active and passive sensing (JAPS). In particular, we consider a JAPSbased
cooperative multi-static ISAC system for coexisting downlink (DL) and uplink
(UL) communications. An optimization problem is formulated for maximizing the
sum rate of both the DL and UL transmissions via jointly optimizing
beamforming, receive filters and power allocation, while guaranteeing the
sensing requirements and transmission power constraints. However, the
formulated problem is a non-convex optimization problem that is challenging to
solve directly due to the tight coupling among optimization variables. To
tackle this complicated issue, we employ an efficient algorithm architecture
leveraging alternating optimization (AO). Specifically, with the given receive
filters and transmission power for UL communication, the transmit beamforming
subproblem is addressed by successive convex approximation (SCA)-based and
penalty-based algorithms. A fractional programming (FP)-based algorithm is
developed to tackle the receive filters and transmission power for UL
communication optimization subproblem. Extensive numerical results validate the
performance improvement of our proposed JAPS scheme and demonstrate the
effectiveness of our proposed algorithms.

</details>


### [199] [Personalized Federated Learning-Driven Beamforming Optimization for Integrated Sensing and Communication Systems](https://arxiv.org/abs/2510.06709)
*Zhou Ni,Sravan Reddy Chintareddy,Peiyuan Guan,Morteza Hashemi*

Main category: eess.SP

TL;DR: 提出了一个基于EM的PFL框架，用于ISAC系统的MOO，通过自适应聚合权重实现


<details>
  <summary>Details</summary>
Motivation: 在ISAC系统中，标准的FL方法对所有客户端一视同仁，无法适应通信和感知等竞争性多目标优化场景下的动态需求和应用特定的权衡。

Method: 提出一个基于EM的PFL框架，在每个基站（BS）计算EM后验，量化全局模型和本地模型在各自数据集上的损失，从而自适应地确定聚合权重。

Result: 仿真结果表明，该方法在同质和异质目标条件下均优于现有的PFL基线（如FedPer和pFedMe），实现了更快的收敛速度和更好的多目标性能。

Conclusion: EM-PFL框架能够有效处理ISAC系统中的多目标优化问题，通过自适应聚合权重，使基站能够动态适应应用特定的权衡，从而提高性能。

Abstract: In this paper, we propose an Expectation-Maximization-based (EM) Personalized
Federated Learning (PFL) framework for multi-objective optimization (MOO) in
Integrated Sensing and Communication (ISAC) systems. In contrast to standard
federated learning (FL) methods that handle all clients uniformly, the proposed
approach enables each base station (BS) to adaptively determine its aggregation
weight with the EM algorithm. Specifically, an EM posterior is computed at each
BS to quantify the relative suitability between the global and each local
model, based on the losses of models on their respective datasets. The proposed
method is especially valuable in scenarios with competing communication and
sensing objectives, as it enables BSs to dynamically adapt to
application-specific trade-offs. To assess the effectiveness of the proposed
approach, we conduct simulation studies under both objective-wise homogeneous
and heterogeneous conditions. The results demonstrate that our approach
outperforms existing PFL baselines, such as FedPer and pFedMe, achieving faster
convergence and better multi-objective performance.

</details>


### [200] [Low Complexity Weight Flexible Decoding Schemes of Linear Block Code for 6G xURLLC](https://arxiv.org/abs/2510.06768)
*Di Zhang,Yinglei Yang,Zhilong Liu,Shaobo Jia,Kyungchun Lee,Zhirong Zhang*

Main category: eess.SP

TL;DR: 该论文提出了一种利用偶校验码的性质来解码线性分组码的方案，以提高6G通信的可靠性。


<details>
  <summary>Details</summary>
Motivation: 低复杂度纠错码是6G超高可靠超低延迟通信（xURLLC）的关键。现有技术需要改进以满足6G通信的要求。

Method: 提出了一种利用偶校验码的性质来解码线性分组码的方案。该方案有两种：一种直接利用内在信息进行迭代解码，另一种结合先验信道信息和内在信息进行解码。这两种方案都易于硬件实现，并且使用了向量乘法和实数比较。

Result: 仿真结果证明了该研究的有效性，表明该方案可以提高通信的可靠性。

Conclusion: 所提出的解码方案能够利用偶校验码的性质，通过灵活的权重为错误比特的位置和幅度提供有用的解码信息，从而提高通信的可靠性，并易于硬件实现。

Abstract: Low complexity error correction code is a key enabler for next generation
ultra-reliable low-latency communications (xURLLC) in six generation (6G).
Against this background, this paper proposes a decoding scheme for linear block
code by leveraging certain interesting properties of dual codewords. It is
found that dual codewords with flexible weights can provide useful decoding
information for the locations and magnitudes of error bits, which yielding
higher reliability performance. In addition, two decoding schemes are proposed,
in which one directly utilizes intrinsic information for iterative decoding,
and the other combines prior channel information with intrinsic information for
decoding. Both schemes are implemented using vector multiplication and
real-number comparisons, making them easy to implement in hardware. Simulation
results demonstrate the validness of our study.

</details>


### [201] [Mobility-Aware Localization in mmWave Channel: Adaptive Hybrid Filtering Approach](https://arxiv.org/abs/2510.06861)
*Abidemi Orimogunje,Kyeong-Ju Cha,Hyunwoo Park,Abdulahi A. Badrudeen,Sunwoo Kim,Dejan Vukobratovic*

Main category: eess.SP

TL;DR: 该方法利用毫米波信号进行感知和通信，通过混合机动感知自适应框架，根据行人或车辆速度选择扩展卡尔曼滤波器或无迹卡尔曼滤波器，并采用自适应噪声缩放、卡方门控、Rauch-Tung-Striebel平滑来解决数据关联和估计误差问题，在不同速度下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 精确的用户定位和跟踪能提升下一代无线网络中节能和超可靠低延迟应用。然而，传统的卡尔曼滤波定位技术存在计算复杂性、数据关联挑战，且随着用户速度增加，估计误差会增大。

Method: 提出了一种混合机动感知自适应框架，利用毫米波信号进行感知和通信，无需额外传感器。该框架根据行人或车辆速度，自适应地选择扩展卡尔曼滤波器（EKF）或无迹卡尔曼滤波器（UKF）。同时，采用自适应噪声缩放、卡方门控和Rauch-Tung-Striebel平滑技术来解决数据关联问题和减小估计误差。

Result: 在绝对轨迹误差、相对姿态误差、归一化估计误差平方和均方根误差等指标的评估中，该方法在各自的速度范围内均实现了约30-60%的性能提升，明显优于现有仅适用于室内或静态环境的方法。

Conclusion: 该混合机动感知自适应框架利用毫米波信号实现了高效且精确的用户定位和跟踪，解决了现有方法的局限性，并在不同速度下展现出显著的性能优势。

Abstract: Precise user localization and tracking enhances energy-efficient and
ultra-reliable low latency applications in the next generation wireless
networks. In addition to computational complexity and data association
challenges with Kalman-filter localization techniques, estimation errors tend
to grow as the user's trajectory speed increases. By exploiting mmWave signals
for joint sensing and communication, our approach dispenses with additional
sensors adopted in most techniques while retaining high resolution spatial
cues. We present a hybrid mobility-aware adaptive framework that selects the
Extended Kalman filter at pedestrian speed and the Unscented Kalman filter at
vehicular speed. The scheme mitigates data-association problem and estimation
errors through adaptive noise scaling, chi-square gating, Rauch-Tung-Striebel
smoothing. Evaluations using Absolute Trajectory Error, Relative Pose Error,
Normalized Estimated Error Squared and Root Mean Square Error metrics
demonstrate roughly 30-60% improvement in their respective regimes indicating a
clear advantage over existing approaches tailored to either indoor or static
settings.

</details>


### [202] [Memory-Augmented Generative AI for Real-time Wireless Prediction in Dynamic Industrial Environments](https://arxiv.org/abs/2510.06884)
*Rahul Gulia,Amlan Ganguly,Michael E. Kuhl,Ehsan Rashedi,Clark Hochgraf*

Main category: eess.SP

TL;DR: Evo-WISVA是一个用于工业仓库无线信道条件预测的深度学习模型，通过结合VAE和ConvLSTM，在复杂动态环境中实现了高精度和高效的预测，超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 工业4.0环境下，尤其是在动态变化的智能仓库中，可靠且实时的无线信道条件（如SINR）预测对于实现超可靠低延迟通信（URLLC）至关重要。传统的物理或统计模型难以处理移动障碍物和瞬时干扰带来的时空复杂性。

Method: 提出了一种名为Evo-WISVA的新型协同深度学习架构，它是一个轻量级的2D预测性无线环境数字孪生。该架构整合了带有注意力驱动的潜在记忆模块（LMM）的记忆增强变分自编码器（VAE），用于提取上下文感知的空间特征，并结合卷积长短期记忆（ConvLSTM）网络进行精确的时间预测和序列优化。整个流程通过联合损失函数进行端到端优化。

Result: 在ns-3生成的工业仓库数据集上进行的实验评估表明，Evo-WISVA在平均重建误差方面显著优于现有基线，最高可降低47.6%。该模型在面对具有大量动态障碍物（多达十个同时移动的障碍物）的未知环境时，表现出出色的泛化能力，同时保持了实时部署所需的计算效率。

Conclusion: Evo-WISVA为无线资源管理奠定了基础技术，能够实现预测性数字孪生在工业通信网络中的应用，支持自主优化。

Abstract: Accurate and real-time prediction of wireless channel conditions,
particularly the Signal-to-Interference-plus-Noise Ratio (SINR), is a
foundational requirement for enabling Ultra-Reliable Low-Latency Communication
(URLLC) in highly dynamic Industry 4.0 environments. Traditional physics-based
or statistical models fail to cope with the spatio-temporal complexities
introduced by mobile obstacles and transient interference inherent to smart
warehouses. To address this, we introduce Evo-WISVA (Evolutionary Wireless
Infrastructure for Smart Warehouse using VAE), a novel synergistic deep
learning architecture that functions as a lightweight 2D predictive digital
twin of the radio environment. Evo-WISVA integrates a memory-augmented
Variational Autoencoder (VAE) featuring an Attention-driven Latent Memory
Module (LMM) for robust, context-aware spatial feature extraction, with a
Convolutional Long Short-Term Memory (ConvLSTM) network for precise temporal
forecasting and sequential refinement. The entire pipeline is optimized
end-to-end via a joint loss function, ensuring optimal feature alignment
between the generative and predictive components. Rigorous experimental
evaluation conducted on a high-fidelity ns-3-generated industrial warehouse
dataset demonstrates that Evo-WISVA significantly surpasses state-of-the-art
baselines, achieving up to a 47.6\% reduction in average reconstruction error.
Crucially, the model exhibits exceptional generalization capacity to unseen
environments with vastly increased dynamic complexity (up to ten simultaneously
moving obstacles) while maintaining amortized computational efficiency
essential for real-time deployment. Evo-WISVA establishes a foundational
technology for proactive wireless resource management, enabling autonomous
optimization and advancing the realization of predictive digital twins in
industrial communication networks.

</details>


### [203] [Sensing Management for Pilot-Free Predictive Beamforming in Cell-Free Massive MIMO Systems](https://arxiv.org/abs/2510.06936)
*Eren Berk Kama,Murat Babek Salman,Isaac Skog,Emil Björnson*

Main category: eess.SP

TL;DR: 本研究提出了一种用于无主蜂窝大规模 MIMO 系统的集成传感与通信（ISAC）的传感管理方法，通过状态基方法利用传感能力跟踪用户，并在通信请求时采用预测波束赋形以减少信道估计的开销，结合扩展卡尔曼滤波器（EKF）和自适应传感管理，实现了开销为零的预测波束赋形，从而提高了下行通信速率。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统通信系统中信道估计过程带来的显著开销问题，该研究提出了一种新的传感管理方法。

Method: 提出了一种状态基方法，利用传感能力跟踪用户，并在收到通信请求时使用预测波束赋形。该框架结合了基于扩展卡尔曼滤波器（EKF）的跟踪算法和自适应传感管理，以仅在必要时执行传感操作来维持高跟踪精度。

Result: 仿真结果表明，所提出的传感管理方法通过实现零开销的预测波束赋形，提供了比现有方法更高且更均匀的下行通信速率。

Conclusion: 提出的传感管理方法能够有效减少信道估计开销，并通过预测波束赋形提高通信速率和用户体验。

Abstract: This paper introduces a sensing management method for integrated sensing and
communications (ISAC) in cell-free massive multiple-input multiple-output
(MIMO) systems. Conventional communication systems employ channel estimation
procedures that impose significant overhead during data transmission, consuming
resources that could otherwise be utilized for data. To address this challenge,
we propose a state-based approach that leverages sensing capabilities to track
the user when there is no communication request. Upon receiving a communication
request, predictive beamforming is employed based on the tracked user position,
thereby reducing the need for channel estimation. Our framework incorporates an
extended Kalman filter (EKF) based tracking algorithm with adaptive sensing
management to perform sensing operations only when necessary to maintain high
tracking accuracy. The simulation results demonstrate that our proposed sensing
management approach provides uniform downlink communication rates that are
higher than with existing methods by achieving overhead-free predictive
beamforming.

</details>


### [204] [Optimal Real-time Communication in 6G Ultra-Massive V2X Mobile Networks](https://arxiv.org/abs/2510.06937)
*He Huang,Zilong Liu,Zeping Sui,Wei Huang,Md. Noor-A-Rahim,Haishi Wang,Zhiheng Hu*

Main category: eess.SP

TL;DR: 提出了一种用于6G车联网的通信算法，通过优化中继选择来提高信道容量。


<details>
  <summary>Details</summary>
Motivation: 解决未来6G超大规模车联网中车辆间实时信息交换的挑战。

Method: 提出了一种低复杂度中继选择启发式算法，并证明了在给定中继数量下信道容量的上限。

Result: 仿真结果表明，所提出的算法相比现有方法具有更高的信道容量。

Conclusion: 所提出的算法在6G车联网中具有优越性。

Abstract: This paper introduces a novel cooperative vehicular communication algorithm
tailored for future 6G ultra-massive vehicle-to-everything (V2X) networks
leveraging integrated space-air-ground communication systems. Specifically, we
address the challenge of real-time information exchange among rapidly moving
vehicles. We demonstrate the existence of an upper bound on channel capacity
given a fixed number of relays, and propose a low-complexity relay selection
heuristic algorithm. Simulation results verify that our proposed algorithm
achieves superior channel capacities compared to existing cooperative vehicular
communication approaches.

</details>


### [205] [Maritime Communication in Evaporation Duct Environment with Ship Trajectory Optimization](https://arxiv.org/abs/2510.06946)
*Ruifeng Gao,Hao Zhang,Jue Wang,Ye Li,Yingdong Hu,Qiuming Zhu,Shu Sun,Meixia Tao*

Main category: eess.SP

TL;DR: 该论文提出了一种利用蒸发波导效应的航海无线网络通信框架，通过优化船舶轨迹来最小化数据传输和航行时间。


<details>
  <summary>Details</summary>
Motivation: 航海无线网络中的蒸发波导效应有利于远程传输，但如何有效利用该效应进行通信设计仍需研究。

Method: 提出了一种利用信道增益图先验信息的框架，通过多目标优化问题（最小化数据传输时间和航行时间）来优化船舶轨迹，并使用动态种群PSO集成的NSGA-II算法求解。

Result: 与忽略蒸发波导信息的基准方案相比，所提出的方案能有效减少数据传输时间和航行时间。

Conclusion: 所提出的框架能有效利用蒸发波导效应，通过优化船舶轨迹来减少航海无线通信中的数据传输和航行时间。

Abstract: In maritime wireless networks, the evaporation duct effect has been known as
a preferable condition for long-range transmissions. However, how to
effectively utilize the duct effect for efficient communication design is still
open for investigation. In this paper, we consider a typical scenario of
ship-to-shore data transmission, where a ship collects data from multiple
oceanographic buoys, sails from one to another, and transmits the collected
data back to a terrestrial base station during its voyage. A novel framework,
which exploits priori information of the channel gain map in the presence of
evaporation duct, is proposed to minimize the data transmission time and the
sailing time by optimizing the ship's trajectory. To this end, a
multi-objective optimization problem is formulated and is further solved by a
dynamic population PSO-integrated NSGA-II algorithm. Through simulations, it is
demonstrated that, compared to the benchmark scheme which ignores useful
information of the evaporation duct, the proposed scheme can effectively reduce
both the data transmission time and the sailing time.

</details>


### [206] [Towards Reliable Emergency Wireless Communications over SAGINs: A Composite Fading and QoS-Centric Perspective](https://arxiv.org/abs/2510.07120)
*Yinong Chen,Wenchi Cheng,Jingqing Wang,Xiao Zheng,Jiangzhou Wang*

Main category: eess.SP

TL;DR: 该论文提出了一种用于应急无线通信的卫星-空中-地面集成网络（SAGIN）性能建模框架，解决了现有研究忽视复杂信道条件和忽略服务质量（QoS）约束的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究在应急无线通信场景下，未能充分考虑复杂地形变化导致的信道特性和忽略服务质量（QoS）约束，导致理论分析与实际性能不符。本文旨在通过建立考虑费舍尔-斯内德科$\\

Method: 本文建立了一个SAGIN性能建模框架，采用费舍尔-斯内德科$\\

Result: 本文推导了端到端信噪比（SNR）统计的精确分布，并提供了有效容量、中断概率和$\\

Conclusion: 本文提出的基于费舍尔-斯内德科$\\

Abstract: In emergency wireless communications (EWC) scenarios, ensuring reliable,
flexible, and high-rate transmission while simultaneously maintaining seamless
coverage and rapid response capabilities presents a critical technical
challenge. To this end, satellite-aerial-ground integrated network (SAGIN) has
emerged as a promising solution due to its comprehensive three-dimensional
coverage and capability to meet stringent, multi-faceted quality-of-service
(QoS) requirements. Nevertheless, most existing studies either neglected the
inherent characteristics of the complex channel conditions due to the terrain
changes or analyzed the performance in the absence of QoS constraints,
resulting in a mismatch between theoretical analysis and practical performance.
To remedy such deficiencies, in this paper we establish a performance modeling
framework for SAGIN employing the Fisher-Snedecor $\mathcal{F}$ composite
fading model to characterize the air-ground link. In specific, the proposed
$\mathcal{F}$ composite fading channel is adopted to accurately describe both
multipath fading and shadowing in harsh ground environments. The exact
distribution of end-to-end signal-to-noise (SNR) statistics for space-air and
air-ground links is developed, enabling theoretical analysis of cascaded
channels with fixed-gain amplify-and-forward (AF) and decode-and-forward (DF)
relaying protocols, respectively. Furthermore, asymptotic expressions of the
derived results are provided to offer concise representations and demonstrate
close alignment with theoretical predictions in the high-SNR regime. Finally,
the insightful closed-form and asymptotic expressions of effective capacity
with QoS provisioning, outage probability, and $\epsilon$-outage capacity are
investigated, respectively, followed by both field measurements and Monte Carlo
simulations to verify the effectiveness.

</details>


### [207] [Moments Matter: Posterior Recovery in Poisson Denoising via Log-Networks](https://arxiv.org/abs/2510.07199)
*Shirin Shoushtari,Edward P. Chandler,Ulugbek S. Kamilov*

Main category: eess.SP

TL;DR: 该研究提出了一种新的泊松去噪方法，通过训练一个log-network来学习$\\mathbb{E}[log x \mid y]$，而不是传统的$\\mathbb{E}[x \mid y]$。这种方法在去噪性能上与标准MMSE模型相当，同时能够提供对后验分布的访问，从而实现不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 传统的泊松去噪方法通常使用均方误差（MSE）损失，这对应于计算后验均值$\\mathbb{E}[x \mid y]$。然而，对于泊松噪声，该后验均值虽然最小化了MSE，但无法捕捉后验不确定性。因此，需要一种新的策略来处理泊松去噪问题，以同时实现准确去噪和后验不确定性估计。

Method: 提出了一种基于训练log-network的新策略。该网络不直接预测后验均值$\\mathbb{E}[x \mid y]$，而是学习$\\mathbb{E}[log x \mid y]$。利用对数作为泊松分布的方便参数化，并通过理论证明，该log-network能够恢复更高阶的后验矩，从而支持后验分布的近似。

Result: 在模拟数据上的实验表明，所提出的log-network方法在去噪性能上与标准的最小均方误差（MMSE）模型相当。此外，该方法还能够提供对后验分布的访问，这对于进行不确定性估计至关重要。

Conclusion: 所提出的log-network方法为泊松去噪提供了一种有效的解决方案。它不仅在去噪精度上达到了现有最优水平，而且克服了传统方法的局限性，能够提供后验不确定性信息，这在光子计数成像等应用中具有重要意义。

Abstract: Poisson denoising plays a central role in photon-limited imaging applications
such as microscopy, astronomy, and medical imaging. It is common to train deep
learning models for denoising using the mean-squared error (MSE) loss, which
corresponds to computing the posterior mean $\mathbb{E}[x \mid y]$. When the
noise is Gaussian, Tweedie's formula enables approximation of the posterior
distribution through its higher-order moments. However, this connection no
longer holds for Poisson denoising: while $ \mathbb{E}[x \mid y] $ still
minimizes MSE, it fails to capture posterior uncertainty. We propose a new
strategy for Poisson denoising based on training a log-network. Instead of
predicting the posterior mean $ \mathbb{E}[x \mid y] $, the log-network is
trained to learn $\mathbb{E}[\log x \mid y]$, leveraging the logarithm as a
convenient parameterization for the Poisson distribution. We provide a
theoretical proof that the proposed log-network enables recovery of
higher-order posterior moments and thus supports posterior approximation.
Experiments on simulated data show that our method matches the denoising
performance of standard MMSE models while providing access to the posterior.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [208] [A Flux-Tunable Discrete Angular Filter](https://arxiv.org/abs/2510.06395)
*Tristan M. Lawrie,Oliver M. Brown*

Main category: physics.app-ph

TL;DR: 本文研究了基于磁性薛定谔方程的非衍射角向滤波器，通过调节非互易波传播引起的相位偏移和顶点边界条件，实现了对透射角度和透射系数的连续调谐，从而将拓扑约束的通带转变为可编程的转向器件，拓展了波滤波器和波束整形的应用范围。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究与先前工作中声波角向滤波器相同的几何结构，但应用于磁性薛定谔方程描述的波，并探索如何实现对滤波特性的连续调谐。

Method: 通过引入磁势产生的非互易波传播效应，利用相位偏移并调谐δ型顶点边界条件，使得滤波器在磁性薛定谔方程下具有可调谐的透射角度和透射系数。

Result: 实现了可连续调谐的角向滤波器件，透射角度和透射系数均可作为控制参数，取代了先前由拓扑固定的通带。

Conclusion: 本文提出的可调谐角向滤波器件为波滤波器和波束整形应用提供了更广泛的可能性，能够实现可编程的转向功能。

Abstract: Recent work by Lawrie et al. [PRR 7, 023209 (2025)] introduced a
non-diffracting resonant angular filter on a network of thin channels (modelled
via quantum graph theory) that exhibits unit transmission of acoustic waves at
a discrete, symmetry-paired set of incidence angles determined solely by the
graph topology, while transmission at all other angles is strictly forbidden.
In the present work, we study the same filtering geometry for waves governed by
the magnetic Schr\"odinger equation rather than the classical wave equation.
Using a phase shift induced by non-reciprocal wave propagation due to the
presence of the magnetic potential and tuning $\delta$-type vertex boundary
conditions, we make the previously topology-fixed discrete pass directions
continuously tunable: both the transmission angle and the transmission
coefficient become control parameters. The resulting flux-tunable angular
filtering device replaces topology-constrained passbands with a programmable
steering device, broadening the scope for wave-filter and beam-shaping
applications.

</details>


### [209] [Enhanced Breakdown Voltage in $β$-Ga$_2$O$_3$ Schottky Barrier Diodes via Fast Neutron Irradiation and Electrothermal Annealing](https://arxiv.org/abs/2510.06415)
*Saleh Ahmed Khan,Sudipto Saha,Ahmed Ibreljic,Stephen Margiotta,Jiawei Liu,Walid Amir,Surajit Chakraborty,Uttam Singisetti,A F M Anhar Uddin Bhuiyan*

Main category: physics.app-ph

TL;DR: 快中子辐照严重降级$eta$-Ga$_2$O$_3$肖特基二极管，但辐射后电热退火可显著恢复其电学性能，特别是载流子浓度和击穿电压。


<details>
  <summary>Details</summary>
Motivation: 研究快中子辐照对$eta$-Ga$_2$O$_3$肖特基二极管性能的影响，并探索辐射后电热退火的修复潜力，以评估其在高辐射环境下的应用前景。

Method: 使用1 MeV快中子对$eta$-Ga$_2$O$_3$肖特基二极管进行高通量（1E15 n/cm^2）辐照，随后进行同步进行电流-电压（J-V）测量和热退火的电热测试，并进行电容-电压（C-V）测量，重复进行电热测试以评估恢复效果。

Result: 辐照后，器件性能严重下降，导通电流急剧减少，导通电阻增加。经过四次电热测试循环后，器件性能显著恢复，导通电流和导通电阻接近辐照前水平。载流子浓度从辐照前的3.2E16 cm^-3降低到第一次电热测试后的5.5E15 cm^-3，之后部分恢复到9.9E15 cm^-3。击穿电压从约300 V大幅提升至1.28 kV（约325%），随后略有下降但仍远高于辐照前水平。

Conclusion: $eta$-Ga$_2$O$_3$肖特基二极管在高通量快中子辐照后性能会严重下降，但通过辐射后的电热退火处理，可以显著恢复其导通特性和载流子浓度，并大幅提高击穿电压。这表明$eta$-Ga$_2$O$_3$功率器件在高强度辐射环境下具有良好的应用潜力。

Abstract: This study investigates the impact of fast neutron irradiation and
post-radiation electro-thermal annealing on the electrical performance of
$\beta$-Ga$_2$O$_3$ Schottky barrier diodes. Devices irradiated with 1 MeV
neutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation,
including a drastic reduction in on-current and an increase in on-resistance.
Electrothermal testing, conducted through simultaneous current-voltage (J-V)
measurements and thermal annealing, resulted in significant recovery. After
four cycles of electro-thermal testing, the devices demonstrated significant
improvements in performance, with a substantial recovery of on-current and a
reduction in on-resistance compared to the post-radiation condition,
approaching pre-radiation levels. Most recovery occurred during the first two
cycles, with diminishing improvements in later cycles, indicating that most
thermally recoverable traps were mitigated early. Capacitance-voltage (C-V)
measurements revealed a substantial reduction in carrier concentration,
decreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first
electro-thermal testing cycle, indicating an over 82% reduction. Following the
third cycle, the carrier concentration partially recovered to 9.9E15 cm^-3,
reflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited
a remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a
~325% improvement) after the first electro-thermal testing, attributed to the
reduction in carrier concentration by compensating radiation-induced traps.
Subsequent testing reduced breakdown voltage slightly to 940 V due to partial
recovery of carrier concentration, but it remained significantly higher than
pre-radiation levels, highlighting the promise of $\beta$-Ga$_2$O$_3$ power
devices for high-power applications in radiation-intense environments.

</details>


### [210] [Physical learning in reprogrammable metamaterials for adaptation to unknown environments](https://arxiv.org/abs/2510.06442)
*Kai Jun Chen,Catherine Catrambone,Christopher Sowinski,Jacob Mukobi,Enzo Andreacchio,Enquan Chew,Alexandre Morland,Maria Sakovsky*

Main category: physics.app-ph

TL;DR: 通过物理学习实现可重构机械超材料的实时自主适应，无需预计算。


<details>
  <summary>Details</summary>
Motivation: 当前的可重构机械超材料在未知环境中需要预先计算适应性策略，无法满足实时操作需求。

Method: 提出一种物理学习方法，将期望的全局变形转化为局部应变目标，并通过模型无关算法迭代更新刚度分布以实现自主适应。

Result: 实现了超材料结构在数秒内对未知加载条件的实时自主适应，并证明了该方法对制造缺陷、传感器噪声和结构损伤的鲁棒性，同时验证了其在复杂结构和不同架构上的可扩展性。

Conclusion: 该物理学习方法将传感、计算和驱动整合到机械框架中，实现了机械智能的自主适应，有望应用于高超音速飞行、自适应机器人和极端环境探索等领域。

Abstract: Reprogrammable mechanical metamaterials, composed of a lattice of discretely
adaptive elements, are emerging as a promising platform for mechanical
intelligence. To operate in unknown environments, such structures must go
beyond passive responsiveness and embody traits of mechanical intelligence:
sensing, computing, adaptation, and memory. However, current approaches fall
short, as computation of the required adaptation in response to changes in
environmental stimuli must be pre-computed ahead of operation. Here we present
a physical learning approach that harnesses the structure's mechanics to
perform computation and drive adaptation. The desired global deformation
response of nonlinear metamaterials with adaptive stiffness is physically
encoded as local strain targets across internal adaptive elements. The
structure adapts by iteratively interacting with the environment and updating
its stiffness distribution using a model-free algorithm. The resulting system
demonstrates autonomous real-time adaptation (~seconds) to previously unknown
loading conditions without pre-computation. Physical learning inherently
accounts for manufacturing imperfections and is robust to sensor noise and
structural damage. We also demonstrate scalability to complex metamaterial
structures and different metamaterial architectures. By uniting sensing,
computation, and actuation in a mechanical framework, this work makes key
strides towards embodying the traits of mechanical intelligence into adaptive
structures. We expect our approach to open pathways towards in-situ adaptation
to unknown environment for applications in hypersonic flight, adaptive
robotics, and exploration in extreme environments.

</details>


### [211] [Low and high frequency noise in LEDs](https://arxiv.org/abs/2510.06833)
*Danylo Bohomolov,Vita Ivanova,Ulrich T. Schwarz*

Main category: physics.app-ph

TL;DR: LED 降解通常与有源区的缺陷有关，而噪声分析可以揭示这些缺陷。本研究报告了商用蓝色 LED 样品在 kHz 至 MHz 频率范围内进行的光噪声测量结果。根据包括 1/f 型噪声、产生复合噪声和白噪声的理论模型，对噪声频谱进行了分解。1/fγ 型噪声被建模为具有连续弛豫寿命分布的缺陷的产生复合噪声分量的叠加。实验结果与低频范围的 1/fγ 模型吻合，并进行了拟合。研究确定了三个高度依赖电流的噪声分量，并开发了相应的低频范围温度依赖性模型。在低电流下，该模型与 100 K 至 300 K 温度范围内低频的实验结果部分匹配，但高频测量显示出与预期洛伦兹行为的偏差。


<details>
  <summary>Details</summary>
Motivation: LED 降解通常与有源区的缺陷有关，而噪声分析是揭示这些缺陷的有力工具。

Method: 对商用蓝色 LED 样品在 kHz 至 MHz 频率范围内进行光噪声测量。根据包含 1/f 型噪声、产生复合噪声和白噪声的理论模型分解噪声频谱。将 1/fγ 型噪声建模为具有连续弛豫寿命分布的缺陷的产生复合噪声分量的叠加。开发了低频范围的温度依赖性模型。

Result: 实验结果与低频范围的 1/fγ 模型吻合，并进行了拟合。确定了三个高度依赖电流的噪声分量。在低电流下，低频模型的预测与 100 K 至 300 K 温度范围的实验结果部分匹配。高频测量显示出与预期洛伦兹行为的偏差。

Conclusion: 噪声分析可以有效揭示 LED 缺陷。1/fγ 模型能够很好地描述低频噪声行为，并且识别出了三个关键的电流依赖噪声分量。然而，在高温和高频下，模型的预测能力有所下降，表明需要进一步研究。

Abstract: LED degradation is usually associated with defects in the active region.
Whereby the noise analysis can be a strong instrument to reveal them. The
results of optical noise measurements for commercially available blue LED
samples in a wide frequency range from kHz to MHz are reported. Noise spectra
were decomposed into components according to the presented theoretical model
which includes 1/f-type noise, generation-recombination noise, and white noise.
The 1/f$^\gamma$-type noise was modeled as a superposition of
generation-recombination noise components at defects with a continuous wide
distribution of relaxation lifetimes. The coincidence of the experimental
results with 1/f$^\gamma$ model for the low frequency range is proved and a
fitting is made. We identified three noise components that are highly
current-dependent. The corresponding model of temperature dependence for the
low-frequency range was developed. At low currents, the model partially matched
the experimental results in the temperature range from 100 K to 300 K at low
frequencies. However, high-frequency measurements showed deviations from the
expected Lorentzian behavior.

</details>


### [212] [Lamb wave-based MVDR imaging and CNN classification of defects in pipelines](https://arxiv.org/abs/2510.06899)
*Shuangshuang Li,Kai Zhao*

Main category: physics.app-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Significant progress has been made in ultrasonic guided wave (UGW) technology
for pipe signal processing and defect imaging recently. However, developing a
defect localization and imaging algorithm that requires fewer parameters,
offers a wide imaging range, and achieves high positioning accuracy remains a
considerable challenge. Traditional direction-of-arrival (DOA) estimation
methods primarily focus on the single-angle estimation with low resolution,
failing to satisfy the spatial localization requirements for pipeline defects.
Therefore, a high-resolution spatial spectrum estimation algorithm is
introduced to realize the two-dimensional DOA estimation. By distributing
multiple sensors in a specific geometric configuration to form an array, this
method employs the array signal processing technology to accurately obtain the
DOA of spatial signals. A uniform circular array (UCA) is employed in the
present study to receive signals from pipe defects, and high-precision
localization and imaging of defects are achieved based on the two-dimensional
minimum variance distortionless response (MVDR) beamforming algorithm, with a
relative positioning error of less than 1%. An image classification method
based on the convolutional neural network (CNN) is further developed to
distinguish the defect types. By constructing a novel CNN model to extract
defect features and perform classification, this model achieves a prediction
accuracy of 97.50%, which effectively distinguishes between defect types.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [213] [A Hybrid Quantum-AI Framework for Protein Structure Prediction on NISQ Devices](https://arxiv.org/abs/2510.06413)
*Yuqi Zhang,Yuxin Yang,Feixiong Chen,Cheng-Chang Lu,Nima Saeidi,Samuel L. Volchenboum,Junhan Zhao,Siwei Chen,Weiwen Jiang,Qiang Guan*

Main category: cs.ET

TL;DR: 该研究提出了一种结合量子计算和深度学习的混合框架，通过“能量融合”方法提高了蛋白质结构预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的量子算法在蛋白质结构预测方面受到设备精度限制，生成的能量景观分辨率较低。

Method: 使用变分量子算法（VQE）在IBM量子处理器上获得低分辨率的量子能量表面，并结合NSP3神经网络预测的二级结构概率和二面角分布作为统计势能，形成增强分辨率的融合能量函数。

Result: 在75个蛋白质片段的375个构象评估中，该混合方法优于AlphaFold3、ColabFold和纯量子方法，平均RMSD达到4.9埃，具有统计学意义（p < 0.001）。

Conclusion: 能量融合是一种结合数据驱动模型和量子算法的系统性方法，能够提高近期量子计算在分子和结构生物学中的应用可行性。

Abstract: Variational quantum algorithms provide a direct, physics-based approach to
protein structure prediction, but their accuracy is limited by the coarse
resolution of the energy landscapes generated on current noisy devices. We
propose a hybrid framework that combines quantum computation with deep
learning, formulating structure prediction as a problem of energy fusion.
Candidate conformations are obtained through the Variational Quantum
Eigensolver (VQE) executed on IBM's 127-qubit superconducting processor, which
defines a global yet low-resolution quantum energy surface. To refine these
basins, secondary structure probabilities and dihedral angle distributions
predicted by the NSP3 neural network are incorporated as statistical
potentials. These additional terms sharpen the valleys of the quantum
landscape, resulting in a fused energy function that enhances effective
resolution and better distinguishes native-like structures. Evaluation on 375
conformations from 75 protein fragments shows consistent improvements over
AlphaFold3, ColabFold, and quantum-only predictions, achieving a mean RMSD of
4.9 {\AA} with statistical significance (p < 0.001). The findings demonstrate
that energy fusion offers a systematic method for combining data-driven models
with quantum algorithms, improving the practical applicability of near-term
quantum computing to molecular and structural biology.

</details>


### [214] [A Review of 10 Years of ProtoSpace: Spacecraft CAD Visualization in Collaborative Augmented Reality](https://arxiv.org/abs/2510.06608)
*Benjamin Nuernberger,Samuel-Hunter Berndt,Robert Tapella,Laura Mann,Aaron Plave,Sasha Samochina,Victor X. Luo*

Main category: cs.ET

TL;DR: ProtoSpace是一个用于在AR和Web 3D中协作可视化CAD模型的平台，已成功应用于NASA的多个火星任务中，帮助工程师和科学家及早发现问题，降低成本和风险。


<details>
  <summary>Details</summary>
Motivation: ProtoSpace的目的是通过在增强现实（AR）和Web 3D中协作可视化CAD模型，来帮助科学家和工程师，并已在航天器任务的整个生命周期中得到应用，例如用于呼吸机设计、为宇航员提供AR指导以及教育下一代了解航天器设计过程。

Method: 本文将探讨ProtoSpace的起源、系统架构（包括HoloLens和3D Web客户端、ProtoSpace服务器以及CAD模型优化器）以及其应用案例、衍生产品和经验教训。

Result: ProtoSpace已被NASA的多个任务（包括毅力号火星车、欧罗巴快船、NISAR、SPHEREx、CAL和火星样本返回）使用十年，通过减少误解和帮助人们更快地理解航天器在物理环境中的空间背景，从而降低成本和风险。

Conclusion: ProtoSpace的成功经验表明，该平台在NASA喷气推进实验室十年来的成功中发挥了重要作用，展示了其在航天器设计和任务执行中的价值。

Abstract: ProtoSpace is a custom JPL-built platform to help scientists and engineers
visualize their CAD models collaboratively in augmented reality (AR) and on the
web in 3D. In addition to this main use case, ProtoSpace has been used
throughout the entire spacecraft mission lifecycle and beyond: ventilator
design and assembly; providing AR-based instructions to astronauts in-training;
educating the next generation on the process of spacecraft design; etc.
ProtoSpace has been used for a decade by NASA missions-including Mars
Perseverance, Europa Clipper, NISAR, SPHEREx, CAL, and Mars Sample Return-to
reduce cost and risk by helping engineers and scientists fix problems earlier
through reducing miscommunication and helping people understand the spatial
context of their spacecraft in the appropriate physical context more quickly.
This paper will explore how ProtoSpace came to be, define the system
architecture and overview-including HoloLens and 3D web clients, the ProtoSpace
server, and the CAD model optimizer-and dive into the use cases, spin-offs, and
lessons learned that led to 10 years of success at NASA's Jet Propulsion
Laboratory.

</details>


### [215] [The Stage Comes to You: A Real-Time Tele-Immersive System with 3D Point Clouds and Vibrotactile Feedback](https://arxiv.org/abs/2510.07009)
*Takahiro Matsumoto,Takahiro Kusabuka,Hiroshi Chigira,Kazuhiko Murasaki,Kakagu Komazaki,Masafumi Suzuki,Masakatsu Aoki*

Main category: cs.ET

TL;DR: 一个低延迟的远程沉浸式娱乐系统，通过传输3D点云和表演者脚步振动，实现身临其境的舞台体验。


<details>
  <summary>Details</summary>
Motivation: 为了在远程场地创建身临其境的舞台体验，克服实时传输和渲染的挑战。

Method: 捕捉动态3D点云和脚步振动，通过低延迟网络传输，并在远程场地通过3D LED墙和振动地板进行渲染和感知。

Result: 在Expo 2025上成功进行了为期20公里的公共试验，观众能够无明显延迟地观看现场舞蹈表演并与表演者互动。

Conclusion: 该系统能够成功实现低延迟的远程沉浸式娱乐体验，为观众提供身临其境的视听和触觉感受。

Abstract: We present a low-latency tele-immersive entertainment system that streams 3D
point clouds and performers' footstep vibrations, creating the sense that the
stage is present. Moving performers and their surroundings are captured as
dynamic point clouds under rapidly changing lighting, then processed,
transmitted, and rendered within a total latency of less than 100 ms. Under
high ambient noise, footstep vibrations are sensed by wearable accelerometers.
Real-time visual and haptic streams are delivered to a remote venue, where a
large 3D LED wall and a vibration-efficient haptic floor envelop dozens of
spectators. A public trial at Expo 2025 linked sites 20 km apart: visitors
watched a live dance show and conversed with performers without noticeable
delay.

</details>


### [216] [An HPC-Inspired Blueprint for a Technology-Agnostic Quantum Middle Layer](https://arxiv.org/abs/2510.07079)
*Stefano Markidis,Gilbert Netzer,Luca Pennati,Ivy Peng*

Main category: cs.ET

TL;DR: 该研究提出了一个支持跨多种量子技术的通用量子中间层蓝图，该蓝图采用HPC库和中间件的思想，具有后端无关和上下文感知特性。


<details>
  <summary>Details</summary>
Motivation: 设计一个能够支持跨不同量子技术的通用量子中间层，该中间层应后端中立且上下文感知。

Method: 提出一个包含类型化数据和操作符描述符的程序意图，并将执行细节（如门、脉冲、连续变量例程或退火后端）与意图分离，存储在上下文描述符中。开发了一个概念验证实现，使用JSON文件作为描述符，并通过Qiskit Aer模拟器（门模型）和D-Wave Ocean模拟退火器（退火模型）两个后端进行测试。

Result: 在最大割问题实例上，相同的类型化问题可以通过改变操作符表述（量子近似优化算法或Ising哈密顿量）和上下文，在两个不同的后端上运行。

Conclusion: 该量子中间层概念具有可移植性、可组合性，并且其核心能够随着硬件能力的发展而演进。

Abstract: We present a blueprint for a quantum middle layer that supports applications
across various quantum technologies. Inspired by concepts and abstractions from
HPC libraries and middleware, our design is backend-neutral and context-aware.
A program only needs to specify its intent once as typed data and operator
descriptors. It declares what the quantum registers mean and which logical
transformations are required, without committing to gates, pulses,
continuous-variable routines, or anneal backend. Such execution details are
carried separately in a context descriptor and can change per backend without
modifying the intent artifacts.
  We develop a proof of concept implementation that uses JSON files for the
descriptors and two backends: a gate-model path realized with IBM Qiskit Aer
simulator and an annealing path realized with D-Wave Ocean's simulated
annealer. On a Max-Cut problem instance, the same typed problem runs on both
backends by varying only the operator formulation (Quantum Approximated
Optimization Algorithm formulation vs. Ising Hamiltonian formulation) and the
context. The proposed middle layer concepts are characterized by portability,
composability, and its minimal core can evolve with hardware capabilities.

</details>


### [217] [From Neural Sensing to Stimulation: An Interdisciplinary Roadmap for Neurotechnology](https://arxiv.org/abs/2510.07116)
*Ruben Ruiz-Mateos Serrano,Joe G Troughton,Nima Mirkhani,Natalia Martinez,Massimo Mariello,Jordan Tsigarides,Simon Williamson,Juan Sapriza,Ioana Susnoschi Luca,Antonio Dominguez-Alfaro,Estelle Cuttaz,Nicole Thompson,Sydney Swedick,Latifah Almulla,Amparo Guemes*

Main category: cs.ET

TL;DR: Neurotechnologies offer precise control over brain-body interactions for clinical and non-clinical applications, but require interdisciplinary collaboration to overcome challenges like functionality, scalability, adaptability, and translatability. This paper provides a roadmap for developing adaptive neurotechnologies by addressing shared challenges, proposing a collaborative framework, and outlining ethical and regulatory priorities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to present a strategic roadmap for the development of neurotechnologies, created by early-career researchers, to address the complex, interdisciplinary challenges that hinder their potential in clinical and non-clinical domains. The goal is to accelerate the development of equitable, effective, and future-ready adaptive neurotechnologies.

Method: The paper identifies five cross-cutting trade-offs that constrain progress in neurotechnology development across functionality, scalability, adaptability, and translatability. It illustrates how technical domains influence the resolution of these trade-offs and proposes a unified framework for collaborative innovation and education, highlighting ethical and regulatory priorities and outlining a timeline for overcoming key bottlenecks.

Result: The paper identifies key trade-offs and challenges in neurotechnology development and proposes a unified framework for collaborative innovation, education, and ethical/regulatory priorities. It aims to guide coordinated efforts to accelerate the development of adaptive neurotechnologies.

Conclusion: By aligning technical development with translational and societal needs, this roadmap aims to accelerate equitable, effective, and future-ready adaptive neurotechnologies, guiding coordinated efforts across the global research and innovation community.

Abstract: Neurotechnologies are transforming how we measure, interpret, and modulate
brain-body interactions, integrating real-time sensing, computation, and
stimulation to enable precise physiological control. They hold transformative
potential across clinical and non-clinical domains, from treating disorders to
enhancing cognition and performance. Realizing this potential requires
navigating complex, interdisciplinary challenges spanning neuroscience,
materials science, device engineering, signal processing, computational
modelling, and regulatory and ethical frameworks. This Perspective presents a
strategic roadmap for neurotechnology development, created by early-career
researchers, highlighting their role at the intersection of disciplines and
their capacity to bridge traditional silos. We identify five cross-cutting
trade-offs that constrain progress across functionality, scalability,
adaptability, and translatability, and illustrate how technical domains
influence their resolution. Rather than a domain-specific review, we focus on
shared challenges and strategic opportunities that transcend disciplines. We
propose a unified framework for collaborative innovation and education,
highlight ethical and regulatory priorities, and outline a timeline for
overcoming key bottlenecks. By aligning technical development with
translational and societal needs, this roadmap aims to accelerate equitable,
effective, and future-ready adaptive neurotechnologies, guiding coordinated
efforts across the global research and innovation community.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [218] [Associative Memory Model with Neural Networks: Memorizing multiple images with one neuron](https://arxiv.org/abs/2510.06542)
*Hiroshi Inazawa*

Main category: cs.NE

TL;DR: 该模型提出了一种新颖的神经网络（联想记忆模型），能够让单个神经元记忆和同时回忆多个图像。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在构建一种能够高效存储和检索图像的联想记忆模型，其核心特点是单个神经元可以存储多张图像，并且能够通过输入其中一张图像来同时回忆所有存储的图像，即使输入是图像的不完整部分也能实现完整回忆。

Method: 提出了一种联想记忆模型，由一个称为“目标球”的神经元集群和多个称为“回忆网”的神经网络层组成。该模型允许不同的图像同时存储在同一个神经元中，并且通过呈现其中一张图像来回忆所有存储的图像。

Result: 实验证明，该模型能够让单个神经元记忆多张图像，并通过输入其中一张图像来同时回忆所有存储的图像，即使输入是图像的不完整部分也能实现完整回忆。

Conclusion: 该联想记忆模型在图像记忆和回忆方面表现出优越的性能，特别是其能够处理多图像存储和不完整图像输入的能力，为图像检索和记忆系统开辟了新的可能性。

Abstract: This paper presents a neural network model (associative memory model) for
memory and recall of images. In this model, only a single neuron can memorize
multi-images and when that neuron is activated, it is possible to recall all
the memorized images at the same time. The system is composed of a single
cluster of numerous neurons, referred to as the "Cue Ball," and multiple neural
network layers, collectively called the "Recall Net." One of the features of
this model is that several different images are stored simultaneously in one
neuron, and by presenting one of the images stored in that neuron, all stored
images are recalled. Furthermore, this model allows for complete recall of an
image even when an incomplete image is presented

</details>


### [219] [Neuromorphic Computing -- An Overview](https://arxiv.org/abs/2510.06721)
*Benedikt Jung,Maximilian Kalcher,Merlin Marinova,Piper Powell,Esma Sakalli*

Main category: cs.NE

TL;DR: The paper introduces neuromorphic computing as a new field inspired by the human brain, addressing the limitations of traditional computing. It covers the motivation, existing technologies (traditional hardware, neuromorphic chips, photonic systems), their pros and cons, and future outlook.


<details>
  <summary>Details</summary>
Motivation: Traditional computing technologies are reaching their limits, necessitating new approaches like neuromorphic computing, which is inspired by the human brain.

Method: The paper introduces neuromorphic computing by discussing the history and problems of traditional computing, followed by an overview of neuromorphic systems. It then details current technologies, including neuromorphic-style computing on traditional hardware, neuromorphic chips, and photonic systems, analyzing their respective benefits and drawbacks.

Result: The paper discusses existing and developing technologies in neuromorphic computing, including implementations on traditional hardware, specialized neuromorphic chips, and photonic systems, along with their advantages and disadvantages.

Conclusion: The paper concludes with a summary of the discussed topics and provides an outlook on the future of neuromorphic computing.

Abstract: With traditional computing technologies reaching their limit, a new field has
emerged seeking to follow the example of the human brain into a new era:
neuromorphic computing. This paper provides an introduction to neuromorphic
computing, why this and other new computing systems are needed, and what
technologies currently exist in the neuromorphic field. It begins with a
general introduction into the history of traditional computing and its present
problems, and then proceeds to a general overview of neuromorphic systems. It
subsequently discusses the main technologies currently in development. For
completeness, the paper first discusses neuromorphic-style computing on
traditional hardware, and then discusses the two top branches of specialized
hardware in this field; neuromorphic chips and photonic systems. Both branches
are explained as well as their relative benefits and drawbacks. The paper
concludes with a summary and an outlook on the future.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [220] [DiLi: A Lock-Free Asynchronously Distributable Linked List](https://arxiv.org/abs/2510.06387)
*Raaghav Ravishankar,Sandeep Kulkarni,Sathya Peri,Gokarna Sharma*

Main category: cs.DC

TL;DR: DiLi是一种条件无锁、可线性化、可分布式链表，可以通过异步和动态分区和负载均衡来扩展到多台机器，并且性能与单机链表相当，吞吐量可随机器数量线性扩展。


<details>
  <summary>Details</summary>
Motivation: 在数据库吞吐量需求超过单机容量时，需要分布式数据结构，但静态分区会导致负载不均和需要停机维护。

Method: 提出了一种称为“条件无锁”的概念，并在此基础上设计了DiLi链表，它支持异步动态分区和负载均衡，并提供无锁的查找、插入和删除操作。其查找过程结合了二分查找和线性遍历。

Result: 实验表明，DiLi在单机上的性能与现有最优的无锁链表相当，并且在分布式部署时，吞吐量可以随着机器数量的线性扩展。

Conclusion: DiLi通过引入条件无锁和动态分区/负载均衡机制，有效地解决了分布式数据结构在扩展性和负载均衡方面的挑战，并在性能和可扩展性上取得了良好效果。

Abstract: Modern databases use dynamic search structures that store a huge amount of
data, and often serve them using multi-threaded algorithms to support the
ever-increasing throughput needs. When this throughput need exceeds the
capacity of the machine hosting the structure, one either needs to replace the
underlying hardware (an option that is typically not viable and introduces a
long down time) or make the data structure distributed. Static partitioning of
the data structure for distribution is not desirable, as it is prone to uneven
load distribution over time, and having to change the partitioning scheme later
will require downtime.
  Since a distributed data structure, inherently, relies on communication
support from the network stack and operating systems, we introduce the notion
of conditional lock-freedom that extends the notion of lock-free computation
with reasonable assumptions about communication between processes. We present
DiLi, a conditional lock-free, linearizable, and distributable linked list that
can be asynchronously and dynamically (1) partitioned into multiple sublists
and (2) load balanced by distributing sublists across multiple machines. DiLi
contains primitives for these that also maintain the lock-free property of the
underlying search structure that supports find, remove, and insert of a key as
the client operations.
  Searching for an item in DiLi is by a novel traversal that involves a binary
search on the partitioning scheme, and then a linear traversal on a limitable
number of linked nodes. As a result, we are able to empirically show that DiLi
performs as well as the state-of-the-art lock-free concurrent search structures
that are based off of a linked list when executed on a single-machine. We also
show that the throughput of DiLi scales linearly with the number of machines
that host it.

</details>


### [221] [Adaptive Protein Design Protocols and Middleware](https://arxiv.org/abs/2510.06396)
*Aymen Alsaadi,Jonathan Ash,Mikhail Titov,Matteo Turilli,Andre Merzky,Shantenu Jha,Sagar Khare*

Main category: cs.DC

TL;DR: AI/ML驱动的蛋白质设计面临计算资源挑战，IMPRESS提供结合AI和高性能计算的解决方案，以提高设计效率和质量。


<details>
  <summary>Details</summary>
Motivation: AI/ML在蛋白质设计领域潜力巨大，但蛋白质序列和结构的庞大可能性需要大量计算资源进行采样以达成生成与预测结构的一致性。

Method: 提出IMPRESS（Integrated Machine-learning for Protein Structures at Scale），结合AI和高性能计算，实现蛋白质设计的自适应协议及其计算基础设施的开发和实施。

Result: 提高了蛋白质设计质量的一致性，并通过动态资源分配和异步工作负载执行，提升了蛋白质设计的吞吐量。

Conclusion: IMPRESS通过整合AI和高性能计算，有效解决了蛋白质设计中的计算挑战，提高了设计效率和质量。

Abstract: Computational protein design is experiencing a transformation driven by
AI/ML. However, the range of potential protein sequences and structures is
astronomically vast, even for moderately sized proteins. Hence, achieving
convergence between generated and predicted structures demands substantial
computational resources for sampling. The Integrated Machine-learning for
Protein Structures at Scale (IMPRESS) offers methods and advanced computing
systems for coupling AI to high-performance computing tasks, enabling the
ability to evaluate the effectiveness of protein designs as they are developed,
as well as the models and simulations used to generate data and train models.
This paper introduces IMPRESS and demonstrates the development and
implementation of an adaptive protein design protocol and its supporting
computing infrastructure. This leads to increased consistency in the quality of
protein design and enhanced throughput of protein design due to dynamic
resource allocation and asynchronous workload execution.

</details>


### [222] [MuFASA -- Asynchronous Checkpoint for Weakly Consistent Fully Replicated Databases](https://arxiv.org/abs/2510.06404)
*Raaghav Ravishankar,Sandeep Kulkarni,Nitin H Vaidya*

Main category: cs.DC

TL;DR: 本研究提出了一种名为分布式事务一致快照（DTCS）的算法，用于解决完全复制的弱一致性分布式数据库中的快照问题。该算法通过生成强一致的快照序列来总结弱一致性系统的计算，旨在减少用户在使用此类系统时可能遇到的异常情况，同时最小化快照开销。


<details>
  <summary>Details</summary>
Motivation: 最终一致性系统可能导致用户未预料到的异常，因此需要频繁进行快照以确保期望的不变式。然而，传统的快照方法会带来显著的开销或不一致性。本研究旨在解决这一挑战。

Method: 提出了一种名为分布式事务一致快照（DTCS）的算法，该算法实现了最小的快照开销（仅需O(n)条新消息，并为现有消息增加单个计数器），并解决了传统快照方法带来的不一致性或过度开销问题。

Result: 所提出的DTCS算法实现了最小的快照开销，仅引入O(n)条新消息和对现有消息的单个计数器修改。该算法提供的快照序列是强一致的，即使底层计算是弱一致的，这显著优于现有的分布式系统和内存数据库的快照算法。

Conclusion: DTCS算法能够生成强一致的快照序列，从而有效应对弱一致性分布式数据库中可能出现的异常情况，同时实现了最小化的开销，为相关领域带来了显著的优势。

Abstract: We focus on the problem of checkpointing in fully replicated weakly
consistent distributed databases, which we refer to as Distributed Transaction
Consistent Snapshot (DTCS). A typical example of such a system is a main-memory
database that provides strong eventual consistency. This problem is important
and challenging for several reasons: (1) eventual consistency often creates
anomalies that the users do not anticipate. Hence, frequent checkpoints to
ascertain desired invariants is highly beneficial in their use, and (2)
traditional checkpoints lead to significant overhead and/or inconsistencies. By
showing that the traditional checkpoint leads to inconsistencies or excessive
overhead, we define the notion of size-minimal checkpointing for fully
replicated databases. We present an algorithm for checkpointing with minimal
checkpointing overhead (only O(n) new messages and addition of a single counter
for existing messages). It also provides a significant benefit over existing
checkpointing algorithms for distributed systems and main-memory databases.
  A key benefit of DTCS is that it summarizes the computation by a sequence of
snapshots that are strongly consistent even though the underlying computation
is weakly consistent. In essence, when anomalies arise in an eventually
consistent system, DTCS enables one to concentrate solely on the snapshots
surrounding the time point of the anomaly.

</details>


### [223] [REACH: Reinforcement Learning for Adaptive Microservice Rescheduling in the Cloud-Edge Continuum](https://arxiv.org/abs/2510.06675)
*Xu Bai,Muhammed Tawfiqul Islam,Rajkumar Buyya,Adel N. Toosi*

Main category: cs.DC

TL;DR: 云边协同计算利用边缘资源满足低延迟应用需求，但异构动态的资源给微服务部署带来挑战。REACH算法通过强化学习动态调整微服务部署，在真实测试中将平均端到端延迟降低了7.9%至10%。


<details>
  <summary>Details</summary>
Motivation: 云计算无法完全满足低延迟应用的需求，云边协同计算可以整合边缘计算的响应速度和云计算的可扩展性。微服务架构（MSA）适用于云边协同，但异构动态的计算资源给微服务的最佳部署带来了重大挑战。

Method: 提出了一种名为REACH的新型重新部署算法，该算法使用强化学习动态地适应微服务部署，以应对分布式基础设施中不断变化的资源可用性和性能变化。

Result: 在真实世界的测试平台上进行了大量实验，证明REACH可将三个基准MSA应用程序的平均端到端延迟降低7.9%、10%和8%，同时有效减轻延迟波动和峰值。

Conclusion: REACH算法通过强化学习能够动态适应云边协同环境下的异构和动态资源，实现微服务的最优部署，有效降低延迟，提升应用性能。

Abstract: Cloud computing, despite its advantages in scalability, may not always fully
satisfy the low-latency demands of emerging latency-sensitive pervasive
applications. The cloud-edge continuum addresses this by integrating the
responsiveness of edge resources with cloud scalability. Microservice
Architecture (MSA) characterized by modular, loosely coupled services, aligns
effectively with this continuum. However, the heterogeneous and dynamic
computing resource poses significant challenges to the optimal placement of
microservices. We propose REACH, a novel rescheduling algorithm that
dynamically adapts microservice placement in real time using reinforcement
learning to react to fluctuating resource availability, and performance
variations across distributed infrastructures. Extensive experiments on a
real-world testbed demonstrate that REACH reduces average end-to-end latency by
7.9%, 10%, and 8% across three benchmark MSA applications, while effectively
mitigating latency fluctuations and spikes.

</details>


### [224] [Multi-Dimensional Autoscaling of Stream Processing Services on Edge Devices](https://arxiv.org/abs/2510.06882)
*Boris Sedlak,Philipp Raith,Andrea Morichetta,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.DC

TL;DR: Edge设备上的流处理服务因资源限制而难以满足服务水平目标。本文提出了一种多维度自动扩展平台 (MUDAP)，通过细粒度的垂直扩展来解决此问题，该平台支持服务和资源层面的扩展，并引入了基于结构化知识回归分析 (RASK) 的扩展代理，以优化跨服务的执行。


<details>
  <summary>Details</summary>
Motivation: Edge设备资源有限，现有自动扩展机制仅关注资源扩展，无法满足流处理服务对服务水平目标 (SLO) 的需求。

Method: 提出了一种多维度自动扩展平台 (MUDAP)，支持服务和资源层面的细粒度垂直扩展，并引入了基于结构化知识回归分析 (RASK) 的扩展代理，用于优化扩展策略。

Result: RASK在20次迭代内（200秒处理）即可构建准确的回归模型，并且通过增加弹性维度，RASK能够以28%更少 SLO 违规的情况，维持最高的请求负载。

Conclusion: MUDAP和RASK通过多维度细粒度扩展，有效解决了Edge设备资源受限下流处理服务难以满足SLO的问题，相比现有方法具有更优的性能。

Abstract: Edge devices have limited resources, which inevitably leads to situations
where stream processing services cannot satisfy their needs. While existing
autoscaling mechanisms focus entirely on resource scaling, Edge devices require
alternative ways to sustain the Service Level Objectives (SLOs) of competing
services. To address these issues, we introduce a Multi-dimensional Autoscaling
Platform (MUDAP) that supports fine-grained vertical scaling across both
service- and resource-level dimensions. MUDAP supports service-specific scaling
tailored to available parameters, e.g., scale data quality or model size for a
particular service. To optimize the execution across services, we present a
scaling agent based on Regression Analysis of Structural Knowledge (RASK). The
RASK agent efficiently explores the solution space and learns a continuous
regression model of the processing environment for inferring optimal scaling
actions. We compared our approach with two autoscalers, the Kubernetes VPA and
a reinforcement learning agent, for scaling up to 9 services on a single Edge
device. Our results showed that RASK can infer an accurate regression model in
merely 20 iterations (i.e., observe 200s of processing). By increasingly adding
elasticity dimensions, RASK sustained the highest request load with 28% less
SLO violations, compared to baselines.

</details>


### [225] [Evaluating Rapid Makespan Predictions for Heterogeneous Systems with Programmable Logic](https://arxiv.org/abs/2510.06998)
*Martin Wilhelm,Franz Freitag,Max Tzschoppe,Thilo Pionteck*

Main category: cs.DC

TL;DR: 该论文提供了一个灵活的评估框架，用于预测异构计算系统中任务映射对整体运行时间的影响，并分析了现有预测方法的准确性。


<details>
  <summary>Details</summary>
Motivation: 现代应用程序在通用处理器和专用加速器组合的异构计算系统中运行，需要解决如何将应用程序任务映射到可用设备以优化性能的挑战。然而，预测任务映射更改对整体运行时间的影响非常困难，现有模拟器需要完整实现，而分析方法又过于抽象。

Method: 提出一个灵活的评估框架，用于异构系统（包括CPU、GPU和FPGA），该框架能够基于抽象的任务图描述收集实际的运行时间数据。利用该框架分析现有分析方法的预测能力，并探讨数据传输开销和设备拥塞等高层特性带来的挑战。

Result: 评估了现有分析方法在预测异构系统中的实际运行时间方面的准确性，并识别了数据传输开销和设备拥塞等常见问题。

Conclusion: 评估框架有助于开发更快的运行时间预测算法，弥合理论与实践之间的差距。

Abstract: Heterogeneous computing systems, which combine general-purpose processors
with specialized accelerators, are increasingly important for optimizing the
performance of modern applications. A central challenge is to decide which
parts of an application should be executed on which accelerator or, more
generally, how to map the tasks of an application to available devices.
Predicting the impact of a change in a task mapping on the overall makespan is
non-trivial. While there are very capable simulators, these generally require a
full implementation of the tasks in question, which is particularly
time-intensive for programmable logic. A promising alternative is to use a
purely analytical function, which allows for very fast predictions, but
abstracts significantly from reality. Bridging the gap between theory and
practice poses a significant challenge to algorithm developers. This paper aims
to aid in the development of rapid makespan prediction algorithms by providing
a highly flexible evaluation framework for heterogeneous systems consisting of
CPUs, GPUs and FPGAs, which is capable of collecting real-world makespan
results based on abstract task graph descriptions. We analyze to what extent
actual makespans can be predicted by existing analytical approaches.
Furthermore, we present common challenges that arise from high-level
characteristics such as data transfer overhead and device congestion in
heterogeneous systems.

</details>


### [226] [GROMACS Unplugged: How Power Capping and Frequency Shapes Performance on GPUs](https://arxiv.org/abs/2510.06902)
*Ayesha Afzal,Anna Kahler,Georg Hager,Gerhard Wellein*

Main category: cs.DC

TL;DR: 该研究对四种NVIDIA GPU（A40、A100、L4、L40）在GROMACS分子动力学模拟中的性能进行了全面分析，并与两种合成基准测试（Pi Solver和STREAM Triad）进行了比较。研究考察了性能如何随GPU时钟频率变化以及工作负载对功耗限制的响应。


<details>
  <summary>Details</summary>
Motivation: 评估不同NVIDIA GPU在GROMACS分子动力学模拟中的性能表现，并为硬件选择和性能优化提供指导。

Method: 使用GROMACS和两种合成基准测试（Pi Solver和STREAM Triad）在四种NVIDIA GPU（A40、A100、L4、L40）上进行性能分析，研究频率和功耗限制对性能的影响。

Result: 研究发现，较小的GROMACS系统对频率敏感，而较大的系统则很快达到内存瓶颈。功耗限制对性能的影响取决于GPU架构和工作负载，高端GPU（如A100）在功耗受限时仍能保持较高性能。

Conclusion: 研究结果为在功耗限制下选择GPU硬件和优化GROMACS分子动力学模拟性能提供了实际指导。

Abstract: Molecular dynamics simulations are essential tools in computational
biophysics, but their performance depend heavily on hardware choices and
configuration. In this work, we presents a comprehensive performance analysis
of four NVIDIA GPU accelerators -- A40, A100, L4, and L40 -- using six
representative GROMACS biomolecular workloads alongside two synthetic
benchmarks: Pi Solver (compute bound) and STREAM Triad (memory bound). We
investigate how performance scales with GPU graphics clock frequency and how
workloads respond to power capping. The two synthetic benchmarks define the
extremes of frequency scaling: Pi Solver shows ideal compute scalability, while
STREAM Triad reveals memory bandwidth limits -- framing GROMACS's performance
in context. Our results reveal distinct frequency scaling behaviors: Smaller
GROMACS systems exhibit strong frequency sensitivity, while larger systems
saturate quickly, becoming increasingly memory bound. Under power capping,
performance remains stable until architecture- and workload-specific thresholds
are reached, with high-end GPUs like the A100 maintaining near-maximum
performance even under reduced power budgets. Our findings provide practical
guidance for selecting GPU hardware and optimizing GROMACS performance for
large-scale MD workflows under power constraints.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [227] [Vi-TacMan: Articulated Object Manipulation via Vision and Touch](https://arxiv.org/abs/2510.06339)
*Leiyao Cui,Zihang Zhao,Sirui Xie,Wenhuan Zhang,Zhi Han,Yixin Zhu*

Main category: cs.RO

TL;DR: 该研究提出了Vi-TacMan框架，结合视觉和触觉信息，实现了机器人对可关节物体的自主操纵，无需显式的运动学模型。


<details>
  <summary>Details</summary>
Motivation: 自主操纵可关节物体对机器人来说是一个基本挑战，现有基于视觉的方法在不熟悉物体上估计不够精确，而基于触觉的方法需要精确的初始化。因此，需要一个系统性地利用视觉（全局引导）和触觉（局部精确）互补性的框架。

Method: Vi-TacMan框架使用视觉提出抓取和粗略方向，然后利用触觉控制器进行精确执行。该方法结合了表面法线作为几何先验，并通过von Mises-Fisher分布对方向进行建模，利用实时接触反馈来调整粗略的视觉估计，而无需显式的运动学模型。

Result: 在超过50,000个模拟和多样化的真实世界物体测试中，Vi-TacMan相比基线方法取得了显著的性能提升（p<0.0001），并实现了跨类别的泛化能力。

Conclusion: 该研究证明了粗略的视觉线索与触觉反馈相结合，可以实现可靠的操纵，为非结构化环境中的自主系统提供了一个可扩展的范式。

Abstract: Autonomous manipulation of articulated objects remains a fundamental
challenge for robots in human environments. Vision-based methods can infer
hidden kinematics but can yield imprecise estimates on unfamiliar objects.
Tactile approaches achieve robust control through contact feedback but require
accurate initialization. This suggests a natural synergy: vision for global
guidance, touch for local precision. Yet no framework systematically exploits
this complementarity for generalized articulated manipulation. Here we present
Vi-TacMan, which uses vision to propose grasps and coarse directions that seed
a tactile controller for precise execution. By incorporating surface normals as
geometric priors and modeling directions via von Mises-Fisher distributions,
our approach achieves significant gains over baselines (all p<0.0001).
Critically, manipulation succeeds without explicit kinematic models -- the
tactile controller refines coarse visual estimates through real-time contact
regulation. Tests on more than 50,000 simulated and diverse real-world objects
confirm robust cross-category generalization. This work establishes that coarse
visual cues suffice for reliable manipulation when coupled with tactile
feedback, offering a scalable paradigm for autonomous systems in unstructured
environments.

</details>


### [228] [A Formal gatekeeper Framework for Safe Dual Control with Active Exploration](https://arxiv.org/abs/2510.06351)
*Kaleb Ben Naveed,Devansh R. Agrawal,Dimitra Panagou*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Planning safe trajectories under model uncertainty is a fundamental
challenge. Robust planning ensures safety by considering worst-case
realizations, yet ignores uncertainty reduction and leads to overly
conservative behavior. Actively reducing uncertainty on-the-fly during a
nominal mission defines the dual control problem. Most approaches address this
by adding a weighted exploration term to the cost, tuned to trade off the
nominal objective and uncertainty reduction, but without formal consideration
of when exploration is beneficial. Moreover, safety is enforced in some methods
but not in others. We propose a framework that integrates robust planning with
active exploration under formal guarantees as follows: The key innovation and
contribution is that exploration is pursued only when it provides a verifiable
improvement without compromising safety. To achieve this, we utilize our
earlier work on gatekeeper as an architecture for safety verification, and
extend it so that it generates both safe and informative trajectories that
reduce uncertainty and the cost of the mission, or keep it within a
user-defined budget. The methodology is evaluated via simulation case studies
on the online dual control of a quadrotor under parametric uncertainty.

</details>


### [229] [Constrained Natural Language Action Planning for Resilient Embodied Systems](https://arxiv.org/abs/2510.06357)
*Grayson Byrd,Corban Rivera,Bethany Kemp,Meghan Booker,Aurora Schmidt,Celso M de Melo,Lalithkumar Seenivasan,Mathias Unberath*

Main category: cs.RO

TL;DR: LLM规划的机器人任务规划方法，通过结合符号规划来提高可靠性和可重复性，并在ALFWorld基准和实际机器人任务中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器人任务规划方法存在挑战：LLM在真实环境中规划时存在幻觉问题，导致不可靠；符号规划方法难以扩展到复杂和模糊的真实世界任务。

Method: 提出一种新的机器人规划方法，该方法通过符号规划来监督LLM规划，以提高可靠性和可重复性，并提供一种比传统提示工程更清晰的硬约束定义方法。

Result: 在ALFWorld规划基准上，该方法取得了99%的成功率，优于现有最先进的方法。在真实的四足机器人上，该方法成功率为100%，而纯LLM规划器为50%，符号规划器为30%。

Conclusion: 所提出的方法可以有效提高基于LLM的机器人规划器的可靠性、可重复性和透明度，同时保留其灵活性和在复杂真实世界环境中的泛化能力，为构建弹性具身智能系统做出贡献。

Abstract: Replicating human-level intelligence in the execution of embodied tasks
remains challenging due to the unconstrained nature of real-world environments.
Novel use of large language models (LLMs) for task planning seeks to address
the previously intractable state/action space of complex planning tasks, but
hallucinations limit their reliability, and thus, viability beyond a research
context. Additionally, the prompt engineering required to achieve adequate
system performance lacks transparency, and thus, repeatability. In contrast to
LLM planning, symbolic planning methods offer strong reliability and
repeatability guarantees, but struggle to scale to the complexity and ambiguity
of real-world tasks. We introduce a new robotic planning method that augments
LLM planners with symbolic planning oversight to improve reliability and
repeatability, and provide a transparent approach to defining hard constraints
with considerably stronger clarity than traditional prompt engineering.
Importantly, these augmentations preserve the reasoning capabilities of LLMs
and retain impressive generalization in open-world environments. We demonstrate
our approach in simulated and real-world environments. On the ALFWorld planning
benchmark, our approach outperforms current state-of-the-art methods, achieving
a near-perfect 99% success rate. Deployment of our method to a real-world
quadruped robot resulted in 100% task success compared to 50% and 30% for pure
LLM and symbolic planners across embodied pick and place tasks. Our approach
presents an effective strategy to enhance the reliability, repeatability and
transparency of LLM-based robot planners while retaining their key strengths:
flexibility and generalizability to complex real-world environments. We hope
that this work will contribute to the broad goal of building resilient embodied
intelligent systems.

</details>


### [230] [Active Next-Best-View Optimization for Risk-Averse Path Planning](https://arxiv.org/abs/2510.06481)
*Amirhossein Mollaei Khass,Guangyi Liu,Vivek Pandey,Wen Jiang,Boshu Lei,Kostas Daniilidis,Nader Motee*

Main category: cs.RO

TL;DR: 该框架通过结合风险规避路径细化和下最优视角（NBV）规划，并引入可扩展梯度分解以支持复杂环境中的高效在线更新，从而在不确定的环境中实现安全导航。


<details>
  <summary>Details</summary>
Motivation: 在不确定的环境中安全导航需要整合风险规避和主动感知能力。然而，将这两种能力结合起来的规划方法仍然是一个挑战。

Method: 该方法首先利用平均风险价值（CVaR）统计数据，在在线更新的3D高斯泼溅辐射场上构建了尾部敏感风险图，以优化粗略的参考路径，并生成局部安全可行的轨迹。然后，将下最优视角（NBV）选择制定为一个SE(3)姿态流形上的优化问题，利用黎曼梯度下降法来最大化预期的信息增益，以减少对即将到来的运动最关键的不确定性。

Result: 通过将风险规避路径细化与NBV规划相结合，并引入可扩展的梯度分解，该方法在复杂环境中实现了高效的在线更新。

Conclusion: 所提出的框架通过结合风险规避路径细化和NBV规划，并引入高效的在线更新机制，从而在不确定的环境中实现了安全导航。该方法通过计算研究得到了证明。

Abstract: Safe navigation in uncertain environments requires planning methods that
integrate risk aversion with active perception. In this work, we present a
unified framework that refines a coarse reference path by constructing
tail-sensitive risk maps from Average Value-at-Risk statistics on an
online-updated 3D Gaussian-splat Radiance Field. These maps enable the
generation of locally safe and feasible trajectories. In parallel, we formulate
Next-Best-View (NBV) selection as an optimization problem on the SE(3) pose
manifold, where Riemannian gradient descent maximizes an expected information
gain objective to reduce uncertainty most critical for imminent motion. Our
approach advances the state-of-the-art by coupling risk-averse path refinement
with NBV planning, while introducing scalable gradient decompositions that
support efficient online updates in complex environments. We demonstrate the
effectiveness of the proposed framework through extensive computational
studies.

</details>


### [231] [What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?](https://arxiv.org/abs/2510.06492)
*Matthew Kim,Kensuke Nakamura,Andrea Bajcsy*

Main category: cs.RO

TL;DR: RGB-only observations in latent-space safe control can lead to myopic safety behaviors; a mutual information-based measure can identify when safety-relevant features are missed, and a multimodal-supervised training strategy can improve safety by incorporating additional sensory inputs during training without affecting deployment.


<details>
  <summary>Details</summary>
Motivation: Existing latent-space safe control methods assume that safety-critical features are observable in the learned latent state, but this assumption may not always hold, potentially leading to unsafe behaviors. This paper investigates when latent state spaces are sufficient for safe control and proposes methods to address their limitations.

Method: The paper examines temperature-based failures using RGB-only observations, identifying myopic safety behaviors. It introduces a mutual information-based measure to detect when observations fail to capture safety-relevant features. A multimodal-supervised training strategy is proposed to shape the latent state with additional sensory inputs during training, which are not required during deployment.

Result: RGB-only observations were found to produce myopic safety behaviors in temperature-based failure scenarios. The mutual information-based measure successfully identified when observations missed safety-relevant features. The multimodal-supervised training strategy was validated in simulation and on hardware, demonstrating its effectiveness in preventing a robot manipulator from overheating a pot of wax.

Conclusion: Latent state spaces are not always sufficient for safe control, especially when critical safety features are not fully observable. By using a mutual information-based measure to identify observation limitations and a multimodal-supervised training strategy to augment the latent state, safe control can be improved even when only limited sensory inputs are available during deployment.

Abstract: Safe control techniques, such as Hamilton-Jacobi reachability, provide
principled methods for synthesizing safety-preserving robot policies but
typically assume hand-designed state spaces and full observability. Recent work
has relaxed these assumptions via latent-space safe control, where state
representations and dynamics are learned jointly through world models that
reconstruct future high-dimensional observations (e.g., RGB images) from
current observations and actions. This enables safety constraints that are
difficult to specify analytically (e.g., spilling) to be framed as
classification problems in latent space, allowing controllers to operate
directly from raw observations. However, these methods assume that
safety-critical features are observable in the learned latent state. We ask:
when are latent state spaces sufficient for safe control? To study this, we
examine temperature-based failures, comparable to overheating in cooking or
manufacturing tasks, and find that RGB-only observations can produce myopic
safety behaviors, e.g., avoiding seeing failure states rather than preventing
failure itself. To predict such behaviors, we introduce a mutual
information-based measure that identifies when observations fail to capture
safety-relevant features. Finally, we propose a multimodal-supervised training
strategy that shapes the latent state with additional sensory inputs during
training, but requires no extra modalities at deployment, and validate our
approach in simulation and on hardware with a Franka Research 3 manipulator
preventing a pot of wax from overheating.

</details>


### [232] [Real-Time Glass Detection and Reprojection using Sensor Fusion Onboard Aerial Robots](https://arxiv.org/abs/2510.06518)
*Malakhi Hopkins,Varun Murali,Vijay Kumar,Camillo J Taylor*

Main category: cs.RO

TL;DR: 本研究提出了一种新颖且计算高效的框架，用于在低于300克的四旋翼飞行器上检测和绘制透明障碍物。该方法融合了飞行时间（ToF）相机和超声波传感器的数据，以及一个定制的、轻量级的二维卷积模型。


<details>
  <summary>Details</summary>
Motivation: 透明障碍物（如玻璃）对传统无人机感知系统构成重大挑战，可能导致地图不准确和碰撞。现有方法通常需要大型、昂贵的传感器或计算负担过重的算法，不适用于低尺寸、重量和功率（SWaP）的机器人。

Method: 该方法融合了飞行时间（ToF）相机和超声波传感器的数据，并使用定制的、轻量级的二维卷积模型。该模型能够准确检测镜面反射，并将这些反射的深度信息传播到深度图中的相应空白区域，从而有效地使透明障碍物可见。

Result: 该系统在受控和真实环境中进行了实验验证，证明了其在包含玻璃的室内环境中进行地图绘制的有效性。整个流程实时运行，仅占用嵌入式处理器上CPU的一小部分核心。

Conclusion: 该研究是首次在低SWaP四旋翼飞行器上，仅使用CPU实时进行板载透明障碍物绘制的系统。

Abstract: Autonomous aerial robots are increasingly being deployed in real-world
scenarios, where transparent obstacles present significant challenges to
reliable navigation and mapping. These materials pose a unique problem for
traditional perception systems because they lack discernible features and can
cause conventional depth sensors to fail, leading to inaccurate maps and
potential collisions. To ensure safe navigation, robots must be able to
accurately detect and map these transparent obstacles. Existing methods often
rely on large, expensive sensors or algorithms that impose high computational
burdens, making them unsuitable for low Size, Weight, and Power (SWaP) robots.
In this work, we propose a novel and computationally efficient framework for
detecting and mapping transparent obstacles onboard a sub-300g quadrotor. Our
method fuses data from a Time-of-Flight (ToF) camera and an ultrasonic sensor
with a custom, lightweight 2D convolution model. This specialized approach
accurately detects specular reflections and propagates their depth into
corresponding empty regions of the depth map, effectively rendering transparent
obstacles visible. The entire pipeline operates in real-time, utilizing only a
small fraction of a CPU core on an embedded processor. We validate our system
through a series of experiments in both controlled and real-world environments,
demonstrating the utility of our method through experiments where the robot
maps indoor environments containing glass. Our work is, to our knowledge, the
first of its kind to demonstrate a real-time, onboard transparent obstacle
mapping system on a low-SWaP quadrotor using only the CPU.

</details>


### [233] [RAISE: A self-driving laboratory for interfacial property formulation discovery](https://arxiv.org/abs/2510.06546)
*Mohammad Nazeri,Sheldon Mei,Jeffrey Watchorn,Alex Zhang,Erin Ng,Tao Wen,Abhijoy Mandal,Kevin Golovin,Alan Aspuru-Guzik,Frank Gu*

Main category: cs.RO

TL;DR: RAISE是一个全自动的机器人实验室，可以优化液体配方并评估表面润湿性，其速度约为每分钟一次接触角测量。


<details>
  <summary>Details</summary>
Motivation: 表面润湿性是生物医学设备、涂料和纺织品的一个关键设计参数，而接触角测量可以量化液体-表面相互作用，但这种相互作用强烈依赖于液体配方。因此，需要一种能够将液体配方优化与表面润湿性评估联系起来的系统。

Method: 提出了一种名为RAISE（Robotic Autonomous Imaging Surface Evaluator）的闭环、自驾式实验室系统。该系统包括一个实验协调器，能够混合液体成分以创建不同的配方组合，将制备好的配方液滴转移到高通量平台上，并使用抓取和放置的相机工具自动捕获液滴图像。此外，系统还包含一个自动图像处理流程来测量接触角。该闭环系统与贝叶斯优化（BO）客户端集成，通过结合先前的接触角测量结果，迭代地探索新的配方，以满足用户定义的优化目标。

Result: RAISE系统可以实现高通量测量，速率约为每分钟一次接触角测量。实验证明，RAISE可以用于研究表面活性剂的润湿性，以及表面活性剂组合如何创建可调配方以补偿纯度相关的变化。此外，多目标BO能够根据特定应用目标，找到精确且优化的配方。该优化过程由一个“期望得分”指导，该得分优先考虑符合目标接触角范围、最小化表面活性剂用量并降低成本的配方。

Conclusion: 该工作展示了RAISE系统在闭环系统中自主连接液体配方与接触角测量能力，并利用多目标BO根据研究者定义的标准高效地确定最优配方。

Abstract: Surface wettability is a critical design parameter for biomedical devices,
coatings, and textiles. Contact angle measurements quantify liquid-surface
interactions, which depend strongly on liquid formulation. Herein, we present
the Robotic Autonomous Imaging Surface Evaluator (RAISE), a closed-loop,
self-driving laboratory that is capable of linking liquid formulation
optimization with surface wettability assessment. RAISE comprises a full
experimental orchestrator with the ability of mixing liquid ingredients to
create varying formulation cocktails, transferring droplets of prepared
formulations to a high-throughput stage, and using a pick-and-place camera tool
for automated droplet image capture. The system also includes an automated
image processing pipeline to measure contact angles. This closed loop
experiment orchestrator is integrated with a Bayesian Optimization (BO) client,
which enables iterative exploration of new formulations based on previous
contact angle measurements to meet user-defined objectives. The system operates
in a high-throughput manner and can achieve a measurement rate of approximately
1 contact angle measurement per minute. Here we demonstrate RAISE can be used
to explore surfactant wettability and how surfactant combinations create
tunable formulations that compensate for purity-related variations.
Furthermore, multi-objective BO demonstrates how precise and optimal
formulations can be reached based on application-specific goals. The
optimization is guided by a desirability score, which prioritizes formulations
that are within target contact angle ranges, minimize surfactant usage and
reduce cost. This work demonstrates the capabilities of RAISE to autonomously
link liquid formulations to contact angle measurements in a closed-loop system,
using multi-objective BO to efficiently identify optimal formulations aligned
with researcher-defined criteria.

</details>


### [234] [Safe Obstacle-Free Guidance of Space Manipulators in Debris Removal Missions via Deep Reinforcement Learning](https://arxiv.org/abs/2510.06566)
*Vincent Lam,Robin Chhabra*

Main category: cs.RO

TL;DR: TD3 智能体用于空间机械臂的无模型工作空间轨迹规划，实现安全可靠的碎片捕获。


<details>
  <summary>Details</summary>
Motivation: 开发一种无模型工作空间轨迹规划器，用于空间机械臂执行安全可靠的碎片捕获任务。

Method: 采用 TD3 智能体，结合了具有奇异点规避和可操作性增强的局部控制策略。提出了一种基于课程的多批评家网络，一个批评家关注跟踪精度，另一个批评家关注碰撞避免。同时使用了优先经验回放缓冲区来加速收敛和提高策略鲁棒性。

Result: 在模拟的七自由度 KUKA LBR iiwa 机械臂上进行了评估，证明了在碎片清除任务中能够生成安全且自适应的轨迹。

Conclusion: 所提出的框架能够为空间碎片清除任务实现安全、自适应的轨迹生成。

Abstract: The objective of this study is to develop a model-free workspace trajectory
planner for space manipulators using a Twin Delayed Deep Deterministic Policy
Gradient (TD3) agent to enable safe and reliable debris capture. A local
control strategy with singularity avoidance and manipulability enhancement is
employed to ensure stable execution. The manipulator must simultaneously track
a capture point on a non-cooperative target, avoid self-collisions, and prevent
unintended contact with the target. To address these challenges, we propose a
curriculum-based multi-critic network where one critic emphasizes accurate
tracking and the other enforces collision avoidance. A prioritized experience
replay buffer is also used to accelerate convergence and improve policy
robustness. The framework is evaluated on a simulated seven-degree-of-freedom
KUKA LBR iiwa mounted on a free-floating base in Matlab/Simulink, demonstrating
safe and adaptive trajectory generation for debris removal missions.

</details>


### [235] [Assist-As-Needed: Adaptive Multimodal Robotic Assistance for Medication Management in Dementia Care](https://arxiv.org/abs/2510.06633)
*Kruthika Gangaraju,Tanmayi Inaparthy,Jiaqi Yang,Yihao Zheng,Fengpei Yuan*

Main category: cs.RO

TL;DR: 该研究提出了一种自适应的、多模态的机器人框架，用于帮助失智症患者（PLWDs）管理药物，该框架能够根据用户实时需求动态调整辅助级别。


<details>
  <summary>Details</summary>
Motivation: 现有的药物管理辅助技术通常采用“一刀切”的方法，未能适应失智症患者（PLWDs）不断下降的用药管理能力，从而削弱了他们的自主性、加速了依赖性并增加了护理者的负担。本研究旨在解决这一问题，通过引入一种能够根据个体能力提供不同级别支持的自适应系统。

Method: 该研究提出并实现了一个自适应的多模态机器人框架，使用Pepper机器人作为载体。该框架基于“层级干预模型”，能够从简单的口头提醒，逐步升级到口头+手势提示，最终提供包括物理导航、口头和手势指导在内的全多模态支持。该系统利用大型语言模型（LLM）驱动的交互策略和多模态传感，持续评估任务状态，提供“恰到好处”的辅助，以最大限度地保留失智症患者的自主性并确保其依从用药。

Result: 在实验室环境中，针对健康的成年人和失智症护理的利益相关者进行了初步研究，评估了该系统的可用性、可理解性以及自适应反馈机制的适当性。研究结果为该框架的有效性和用户接受度提供了初步的见解。

Conclusion: 本研究贡献了一个基于职业治疗原则的自适应辅助框架，该框架被转化为人机交互（HRI）设计。此外，还实现了一个多模态的机器人系统，通过分级支持来维护失智症患者的尊严。最后，研究获得了关于利益相关者对自适应机器人护理看法的实证见解。

Abstract: People living with dementia (PLWDs) face progressively declining abilities in
medication management-from simple forgetfulness to complete task breakdown-yet
most assistive technologies fail to adapt to these changing needs. This
one-size-fits-all approach undermines autonomy, accelerates dependence, and
increases caregiver burden. Occupational therapy principles emphasize matching
assistance levels to individual capabilities: minimal reminders for those who
merely forget, spatial guidance for those who misplace items, and comprehensive
multimodal support for those requiring step-by-step instruction. However,
existing robotic systems lack this adaptive, graduated response framework
essential for maintaining PLWD independence. We present an adaptive multimodal
robotic framework using the Pepper robot that dynamically adjusts assistance
based on real-time assessment of user needs. Our system implements a
hierarchical intervention model progressing from (1) simple verbal reminders,
to (2) verbal + gestural cues, to (3) full multimodal guidance combining
physical navigation to medication locations with step-by-step verbal and
gestural instructions. Powered by LLM-driven interaction strategies and
multimodal sensing, the system continuously evaluates task states to provide
just-enough assistance-preserving autonomy while ensuring medication adherence.
We conducted a preliminary study with healthy adults and dementia care
stakeholders in a controlled lab setting, evaluating the system's usability,
comprehensibility, and appropriateness of adaptive feedback mechanisms. This
work contributes: (1) a theoretically grounded adaptive assistance framework
translating occupational therapy principles into HRI design, (2) a multimodal
robotic implementation that preserves PLWD dignity through graduated support,
and (3) empirical insights into stakeholder perceptions of adaptive robotic
care.

</details>


### [236] [RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training](https://arxiv.org/abs/2510.06710)
*Hongzhi Zang,Mingjie Wei,Si Xu,Yongji Wu,Zhen Guo,Yuanqing Wang,Hao Lin,Liangzhi Shi,Yuqing Xie,Zhexuan Xu,Zhihao Liu,Kang Chen,Wenhao Tang,Quanlu Zhang,Weinan Zhang,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: RLinf-VLA是一个统一且高效的框架，用于可扩展的视觉-语言-动作（VLA）模型强化学习（RL）训练，解决了SFT泛化能力不足的问题，并通过混合细粒度流水线分配模式实现了1.61x-1.88x的训练加速。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型多采用监督微调（SFT），在分布变化下泛化能力不足，而强化学习（RL）方法分散且缺乏统一的比较平台。RLinf-VLA旨在解决这一差距，为VLA模型的RL训练提供一个统一、高效且可扩展的框架。

Method: RLinf-VLA框架采用高度灵活的资源分配设计，整合渲染、训练和推理；针对GPU并行模拟器，实现了新颖的混合细粒度流水线分配模式，支持多种VLA架构、RL算法和模拟器，并在模拟环境中取得了高成功率。

Result: 在模拟环境中，RLinf-VLA支持的统一模型在130个LIBERO任务上实现了98.11%的成功率，在25个ManiSkill任务上实现了97.66%的成功率。与SFT相比，RL训练的策略在真实世界机器人上表现出更强的泛化能力。

Conclusion: RLinf-VLA框架通过提高训练效率和模型性能，为VLA模型的RL训练提供了基础，并提炼出一套RL应用于VLA训练的最佳实践，旨在加速和标准化具身智能的研究。

Abstract: Recent progress in vision and language foundation models has significantly
advanced multimodal understanding, reasoning, and generation, inspiring a surge
of interest in extending such capabilities to embodied settings through
vision-language-action (VLA) models. Yet, most VLA models are still trained
with supervised fine-tuning (SFT), which struggles to generalize under
distribution shifts due to error accumulation. Reinforcement learning (RL)
offers a promising alternative by directly optimizing task performance through
interaction, but existing attempts remain fragmented and lack a unified
platform for fair and systematic comparison across model architectures and
algorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and
efficient framework for scalable RL training of VLA models. The system adopts a
highly flexible resource allocation design that addresses the challenge of
integrating rendering, training, and inference in RL+VLA training. In
particular, for GPU-parallelized simulators, RLinf-VLA implements a novel
hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup
in training. Through a unified interface, RLinf-VLA seamlessly supports diverse
VLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g.,
PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a
unified model achieves 98.11\% across 130 LIBERO tasks and 97.66\% across 25
ManiSkill tasks. Beyond empirical performance, our study distills a set of best
practices for applying RL to VLA training and sheds light on emerging patterns
in this integration. Furthermore, we present preliminary deployment on a
real-world Franka robot, where RL-trained policies exhibit stronger
generalization than those trained with SFT. We envision RLinf-VLA as a
foundation to accelerate and standardize research on embodied intelligence.

</details>


### [237] [SanDRA: Safe Large-Language-Model-Based Decision Making for Automated Vehicles Using Reachability Analysis](https://arxiv.org/abs/2510.06717)
*Yuanfei Lin,Sebastian Illing,Matthias Althoff*

Main category: cs.RO

TL;DR: SanDRA 是一个首个基于大语言模型的安全决策框架，用于自动驾驶汽车，通过可达性分析来确保行车安全。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在自动驾驶决策方面有广泛应用，但其决策的安全性无法保证，因为它们可能产生幻觉且缺乏车辆动力学的整合。

Method: 该方法首先通过详细的驾驶场景描述提示大语言模型生成并排序可行的驾驶动作。然后，将这些动作转化为包含规范化交通规则的时间逻辑公式，并整合到可达性分析中以消除不安全动作。

Result: 在开放和闭环的驾驶环境中，使用现成和微调的大语言模型验证了该方法，结果表明即使在高密度交通条件下，它也能提供可证明安全且在可能的情况下合法的驾驶动作。

Conclusion: SanDRA 是首个将大语言模型与可达性分析相结合的安全决策框架，能够为自动驾驶汽车生成安全且合法的驾驶动作。

Abstract: Large language models have been widely applied to knowledge-driven
decision-making for automated vehicles due to their strong generalization and
reasoning capabilities. However, the safety of the resulting decisions cannot
be ensured due to possible hallucinations and the lack of integrated vehicle
dynamics. To address this issue, we propose SanDRA, the first safe
large-language-model-based decision making framework for automated vehicles
using reachability analysis. Our approach starts with a comprehensive
description of the driving scenario to prompt large language models to generate
and rank feasible driving actions. These actions are translated into temporal
logic formulas that incorporate formalized traffic rules, and are subsequently
integrated into reachability analysis to eliminate unsafe actions. We validate
our approach in both open-loop and closed-loop driving environments using
off-the-shelf and finetuned large language models, showing that it can provide
provably safe and, where possible, legally compliant driving actions, even
under high-density traffic conditions. To ensure transparency and facilitate
future research, all code and experimental setups are publicly available at
github.com/CommonRoad/SanDRA.

</details>


### [238] [UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene](https://arxiv.org/abs/2510.06754)
*Christian Maurer,Snehal Jauhri,Sophie Lueth,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: UniFField是一个统一的、可感知的神经特征场，它将视觉、语义和几何特征结合在一个通用的表示中，同时预测每个模态中的不确定性。该方法可以零样本应用于任何新环境，并在机器人探索场景时，将RGB-D图像增量地集成到基于体素的特征表示中，同时更新不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 在非结构化和复杂的环境中，机器人需要对3D场景有全面的视觉、几何和语义理解，并评估感知信息的可靠性，以便做出鲁棒的决策。现有的3D神经特征场方法存在场景特定和缺乏不确定性建模的局限性。

Method: UniFField通过将视觉、语义和几何特征结合在一个统一的、可泛化的表示中来实现这一点。它通过增量地集成RGB-D图像来更新基于体素的特征表示，并同时进行不确定性估计。该方法可以进行零样本应用。

Result: UniFField的不确定性估计能够准确描述场景重建和语义特征预测中的模型预测误差。此外，在主动物体搜索任务中，利用预测的特征及其不确定性，成功地实现了鲁棒的决策。

Conclusion: UniFField通过提供统一的、可泛化的、不确定性感知的3D场景表示，解决了现有方法的局限性，并展示了其在机器人任务中的鲁棒决策能力。

Abstract: Comprehensive visual, geometric, and semantic understanding of a 3D scene is
crucial for successful execution of robotic tasks, especially in unstructured
and complex environments. Additionally, to make robust decisions, it is
necessary for the robot to evaluate the reliability of perceived information.
While recent advances in 3D neural feature fields have enabled robots to
leverage features from pretrained foundation models for tasks such as
language-guided manipulation and navigation, existing methods suffer from two
critical limitations: (i) they are typically scene-specific, and (ii) they lack
the ability to model uncertainty in their predictions. We present UniFField, a
unified uncertainty-aware neural feature field that combines visual, semantic,
and geometric features in a single generalizable representation while also
predicting uncertainty in each modality. Our approach, which can be applied
zero shot to any new environment, incrementally integrates RGB-D images into
our voxel-based feature representation as the robot explores the scene,
simultaneously updating uncertainty estimation. We evaluate our uncertainty
estimations to accurately describe the model prediction errors in scene
reconstruction and semantic feature prediction. Furthermore, we successfully
leverage our feature predictions and their respective uncertainty for an active
object search task using a mobile manipulator robot, demonstrating the
capability for robust decision-making.

</details>


### [239] [Distributed 3D Source Seeking via SO(3) Geometric Control of Robot Swarms](https://arxiv.org/abs/2510.06836)
*Jesús Bautista,Héctor García de Marina*

Main category: cs.RO

TL;DR: 该研究提出了一种在SO(3)李群上的几何控制框架，用于具有一阶姿态动力学和恒定平移速度的机器人进行3D寻源。


<details>
  <summary>Details</summary>
Motivation: 为了解决3D寻源问题，并避免欧拉角奇异性和四元数歧义，提出了一种在SO(3)李群上的直接工作方法，提供独特的内在姿态表示。

Method: 设计了一个比例前馈控制器，确保每个智能体指数级地对准到估计的3D标量场源上升方向。该控制器能适应有界未知变化并保持良好的群体队形。

Result: 数值模拟证明了该方法的有效性，并提供了可复现的开源代码。

Conclusion: 该几何控制框架在SO(3)李群上成功实现了3D源跟踪，并能适应动态变化和保持队形。

Abstract: This paper presents a geometric control framework on the Lie group SO(3) for
3D source-seeking by robots with first-order attitude dynamics and constant
translational speed. By working directly on SO(3), the approach avoids
Euler-angle singularities and quaternion ambiguities, providing a unique,
intrinsic representation of orientation. We design a proportional feed-forward
controller that ensures exponential alignment of each agent to an estimated
ascending direction toward a 3D scalar field source. The controller adapts to
bounded unknown variations and preserves well-posed swarm formations. Numerical
simulations demonstrate the effectiveness of the method, with all code provided
open source for reproducibility.

</details>


### [240] [Tailoring materials into kirigami robots](https://arxiv.org/abs/2510.07027)
*Saravana Prashanth Murali Babu,Aida Parvaresh,Ahmad Rafsanjani*

Main category: cs.RO

TL;DR: 基于折纸工艺的仿生机器人


<details>
  <summary>Details</summary>
Motivation: 折纸工艺在机器人领域具有巨大潜力，可用于制造多功能、轻量化和适应性强的机器人。

Method: 通过优化切割模式设计折纸驱动器、传感器、电池和控制器，并整合到机器人主体结构中。

Result: 折纸驱动器可实现复杂运动，折纸传感器结合了导电性和柔顺性，折纸电池可直接集成到机器人结构中，折纸控制机制可实现变形和记忆功能。折纸机器人已成功应用于抓取、移动和可穿戴设备。

Conclusion: 折纸机器人具有广阔的应用前景，但在切割模式设计和制造工艺方面仍存在挑战。

Abstract: Kirigami, the traditional paper-cutting craft, holds immense potential for
revolutionizing robotics by providing multifunctional, lightweight, and
adaptable solutions. Kirigami structures, characterized by their
bending-dominated deformation, offer resilience to tensile forces and
facilitate shape morphing under small actuation forces. Kirigami components
such as actuators, sensors, batteries, controllers, and body structures can be
tailored to specific robotic applications by optimizing cut patterns. Actuators
based on kirigami principles exhibit complex motions programmable through
various energy sources, while kirigami sensors bridge the gap between
electrical conductivity and compliance. Kirigami-integrated batteries enable
energy storage directly within robot structures, enhancing flexibility and
compactness. Kirigami-controlled mechanisms mimic mechanical computations,
enabling advanced functionalities such as shape morphing and memory functions.
Applications of kirigami-enabled robots include grasping, locomotion, and
wearables, showcasing their adaptability to diverse environments and tasks.
Despite promising opportunities, challenges remain in the design of cut
patterns for a given function and streamlining fabrication techniques.

</details>


### [241] [Temporal-Prior-Guided View Planning for Periodic 3D Plant Reconstruction](https://arxiv.org/abs/2510.07028)
*Sicong Pan,Xuying Huang,Maren Bennewitz*

Main category: cs.RO

TL;DR: 提出了一种利用先前重建模型来指导植物三维重建的新视图规划方法，以提高效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统的周期性三维重建成本高，且忽略了先前捕获的信息。本研究旨在解决此问题。

Method: 通过非刚性对齐先前重建模型和新的局部观测来形成当前几何形状的近似，然后通过膨胀近似并解决集合覆盖优化问题来计算最少所需视图集。此外，还整合了一个完整的流水线，在注册前获取一个额外的“next-best view”以提高鲁棒性，并规划最短路径连接视图集。

Result: 实验结果表明，该系统在玉米和番茄的半球和球体视图空间中，能够维持或提高表面覆盖率，同时所需的视图更少，并且移动成本与最先进的基线相当。

Conclusion: 所提出的方法能够有效地进行周期性植物重建，减少所需视图数量，并保持或提高重建质量。

Abstract: Periodic 3D reconstruction is essential for crop monitoring, but costly when
each cycle restarts from scratch, wasting resources and ignoring information
from previous captures. We propose temporal-prior-guided view planning for
periodic plant reconstruction, in which a previously reconstructed model of the
same plant is non-rigidly aligned to a new partial observation to form an
approximation of the current geometry. To accommodate plant growth, we inflate
this approximation and solve a set covering optimization problem to compute a
minimal set of views. We integrated this method into a complete pipeline that
acquires one additional next-best view before registration for robustness and
then plans a globally shortest path to connect the planned set of views and
outputs the best view sequence. Experiments on maize and tomato under
hemisphere and sphere view spaces show that our system maintains or improves
surface coverage while requiring fewer views and comparable movement cost
compared to state-of-the-art baselines.

</details>


### [242] [Diffusing Trajectory Optimization Problems for Recovery During Multi-Finger Manipulation](https://arxiv.org/abs/2510.07030)
*Abhinav Kumar,Fan Yang,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: 利用扩散模型实现多指手部操作任务的自主恢复


<details>
  <summary>Details</summary>
Motivation: 多指手在精细操作和工具使用方面能力强大，但易受干扰或执行错误影响，需要恢复行为来保证任务顺利进行。

Method: 提出一个框架，利用扩散模型识别任务恢复的必要性，并通过优化接触轨迹来执行恢复。该框架将状态不适合任务执行的情况视为分布外检测问题，利用扩散模型将这些状态投影回分布内，并结合轨迹优化规划恢复。此外，还提出了一种基于扩散模型的方法，用于高效地生成恢复轨迹的完整参数化（包括约束、目标状态和初始化），以缩短在线执行时间。

Result: 在硬件螺丝刀转动任务中，与强化学习基线和其他不考虑接触交互的方法相比，该方法将任务恢复的成功率提高了96%。并且，是唯一一种在评估中不引发灾难性任务失败的恢复方法。

Conclusion: 该研究提出的基于扩散模型的自主恢复框架能够有效识别并优化接触丰富的轨迹，以应对多指手操作中的干扰和错误，显著提高了任务的鲁棒性和成功率。

Abstract: Multi-fingered hands are emerging as powerful platforms for performing fine
manipulation tasks, including tool use. However, environmental perturbations or
execution errors can impede task performance, motivating the use of recovery
behaviors that enable normal task execution to resume. In this work, we take
advantage of recent advances in diffusion models to construct a framework that
autonomously identifies when recovery is necessary and optimizes contact-rich
trajectories to recover. We use a diffusion model trained on the task to
estimate when states are not conducive to task execution, framed as an
out-of-distribution detection problem. We then use diffusion sampling to
project these states in-distribution and use trajectory optimization to plan
contact-rich recovery trajectories. We also propose a novel diffusion-based
approach that distills this process to efficiently diffuse the full
parameterization, including constraints, goal state, and initialization, of the
recovery trajectory optimization problem, saving time during online execution.
We compare our method to a reinforcement learning baseline and other methods
that do not explicitly plan contact interactions, including on a hardware
screwdriver-turning task where we show that recovering using our method
improves task performance by 96% and that ours is the only method evaluated
that can attempt recovery without causing catastrophic task failure. Videos can
be found at https://dtourrecovery.github.io/.

</details>


### [243] [Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models](https://arxiv.org/abs/2510.07067)
*Daria Pugacheva,Andrey Moskalenko,Denis Shepelev,Andrey Kuznetsov,Vlad Shakhuro,Elena Tutubalina*

Main category: cs.RO

TL;DR: VLA模型在具身AI中易受语言干扰，本文提出了一种基于LLM的过滤框架来增强其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 评估现有VLA模型在处理自然语言变异性（如指令同义改写和无关上下文）方面的鲁棒性，并提出改进方法。

Method: 引入两种指令噪声（人类改写和无关上下文），并根据长度和语义/词汇接近度对无关上下文进行分类。提出基于LLM的过滤框架来提取核心指令。

Result: 无关上下文会随着长度增加导致性能下降。随机上下文的性能下降约10%，而语义/词汇接近的上下文可导致性能下降约50%。人类改写指令会导致性能下降约20%。过滤框架可使模型恢复高达98.5%的原始性能。

Conclusion: VLA模型对语言干扰敏感，但所提出的LLM过滤框架能有效缓解这些问题，显著提高模型在噪声输入下的性能。

Abstract: Vision Language Action (VLA) models are widely used in Embodied AI, enabling
robots to interpret and execute language instructions. However, their
robustness to natural language variability in real-world scenarios has not been
thoroughly investigated. In this work, we present a novel systematic study of
the robustness of state-of-the-art VLA models under linguistic perturbations.
Specifically, we evaluate model performance under two types of instruction
noise: (1) human-generated paraphrasing and (2) the addition of irrelevant
context. We further categorize irrelevant contexts into two groups according to
their length and their semantic and lexical proximity to robot commands. In
this study, we observe consistent performance degradation as context size
expands. We also demonstrate that the model can exhibit relative robustness to
random context, with a performance drop within 10%, while semantically and
lexically similar context of the same length can trigger a quality decline of
around 50%. Human paraphrases of instructions lead to a drop of nearly 20%. To
mitigate this, we propose an LLM-based filtering framework that extracts core
commands from noisy inputs. Incorporating our filtering step allows models to
recover up to 98.5% of their original performance under noisy conditions.

</details>


### [244] [Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications](https://arxiv.org/abs/2510.07077)
*Kento Kawaharazuka,Jihoon Oh,Jun Yamada,Ingmar Posner,Yuke Zhu*

Main category: cs.RO

TL;DR: VLA模型整合视觉、语言和动作数据，旨在实现机器人跨任务、跨领域泛化，本综述全面介绍了VLA系统的软硬件组件、技术和应用。


<details>
  <summary>Details</summary>
Motivation: 推动机器人技术在LLM和VLM基础上实现跨任务、跨领域泛化，以最小化或无需额外数据即可解决新任务。

Method: 对VLA系统的策略、架构、构建模块、模态处理技术、学习范式、机器人平台、数据收集、数据集、数据增强和评估基准进行全面、系统性的综述。

Result: 提供了VLA系统软硬件组件的全面回顾，以及在机器人应用中的实用指导。

Conclusion: 本综述旨在为机器人领域应用VLA提供实践指导，并通过项目网站提供分类的参考文献。

Abstract: Amid growing efforts to leverage advances in large language models (LLMs) and
vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models
have recently gained significant attention. By unifying vision, language, and
action data at scale, which have traditionally been studied separately, VLA
models aim to learn policies that generalise across diverse tasks, objects,
embodiments, and environments. This generalisation capability is expected to
enable robots to solve novel downstream tasks with minimal or no additional
task-specific data, facilitating more flexible and scalable real-world
deployment. Unlike previous surveys that focus narrowly on action
representations or high-level model architectures, this work offers a
comprehensive, full-stack review, integrating both software and hardware
components of VLA systems. In particular, this paper provides a systematic
review of VLAs, covering their strategy and architectural transition,
architectures and building blocks, modality-specific processing techniques, and
learning paradigms. In addition, to support the deployment of VLAs in
real-world robotic applications, we also review commonly used robot platforms,
data collection strategies, publicly available datasets, data augmentation
methods, and evaluation benchmarks. Throughout this comprehensive survey, this
paper aims to offer practical guidance for the robotics community in applying
VLAs to real-world robotic systems. All references categorized by training
approach, evaluation method, modality, and dataset are available in the table
on our project website: https://vla-survey.github.io .

</details>


### [245] [Sampling Strategies for Robust Universal Quadrupedal Locomotion Policies](https://arxiv.org/abs/2510.07094)
*David Rytz,Kim Tien Ly,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 该研究通过对物理机器人参数和关节比例-微分增益进行采样，以生成适用于多种配置的通用四足机器人运动策略，并使用强化学习进行训练。研究人员比较了三种增益采样策略：线性/多项式映射、基于性能的自适应滤波和均匀随机采样。通过引入名义先验和参考模型来优化配置，以提高策略的鲁棒性。实验结果表明，为了有效缩小模拟与现实之间的差距，需要对关节控制器增益进行大量的随机化处理。


<details>
  <summary>Details</summary>
Motivation: 为了训练一个能够泛化到多种参数配置的单一强化学习策略，研究了用于生成鲁棒通用四足机器人运动策略的配置变化采样策略。

Method: 研究了三种关节增益采样策略：1）使用线性/多项式函数映射质量到增益，2）使用基于性能的自适应滤波，3）使用均匀随机采样。通过偏向使用名义先验和参考模型的配置来提高策略的鲁棒性。所有训练均在RaiSim中进行，并在各种四足机器人模拟中进行了测试，并使用ANYmal四足机器人进行了零样本硬件部署。

Result: 与多种基线实现相比，结果表明需要对关节控制器增益进行显著的随机化处理，以实现鲁棒的模拟到现实的迁移。

Conclusion: 为了有效缩小模拟与现实之间的差距，需要对关节控制器增益进行大量的随机化处理。

Abstract: This work focuses on sampling strategies of configuration variations for
generating robust universal locomotion policies for quadrupedal robots. We
investigate the effects of sampling physical robot parameters and joint
proportional-derivative gains to enable training a single reinforcement
learning policy that generalizes to multiple parameter configurations. Three
fundamental joint gain sampling strategies are compared: parameter sampling
with (1) linear and polynomial function mappings of mass-to-gains, (2)
performance-based adaptive filtering, and (3) uniform random sampling. We
improve the robustness of the policy by biasing the configurations using
nominal priors and reference models. All training was conducted on RaiSim,
tested in simulation on a range of diverse quadrupeds, and zero-shot deployed
onto hardware using the ANYmal quadruped robot. Compared to multiple baseline
implementations, our results demonstrate the need for significant joint
controller gains randomization for robust closing of the sim-to-real gap.

</details>


### [246] [A Digital Twin Framework for Metamorphic Testing of Autonomous Driving Systems Using Generative Model](https://arxiv.org/abs/2510.07133)
*Tony Zhang,Burak Kantarci,Umair Siddique*

Main category: cs.RO

TL;DR: 该论文提出了一种结合数字孪生和AI图像生成（如Stable Diffusion）的变形测试框架，以解决自动驾驶汽车安全测试中的挑战，如预言机问题和场景覆盖不足。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶汽车测试方法面临预言机问题和场景覆盖不足的挑战。

Method: 利用数字孪生技术创建自动驾驶系统及其运行环境的虚拟副本，并结合AI图像生成模型（如Stable Diffusion）系统性地生成多样化、逼真的驾驶场景。在数字孪生提供的同步仿真环境中，定义了三个受交通规则和车辆行为启发的变形关系，并在Udacity自动驾驶模拟器中进行了验证。

Result: 该框架显著提高了测试覆盖率和有效性，在真实性（true positive rate）、F1分数和精确率方面均优于基线方法，分别达到了0.719、0.689和0.662。

Conclusion: 将数字孪生与AI驱动的场景生成相结合，为自动驾驶汽车安全提供了一种可扩展、自动化、高保真的测试解决方案。

Abstract: Ensuring the safety of self-driving cars remains a major challenge due to the
complexity and unpredictability of real-world driving environments. Traditional
testing methods face significant limitations, such as the oracle problem, which
makes it difficult to determine whether a system's behavior is correct, and the
inability to cover the full range of scenarios an autonomous vehicle may
encounter. In this paper, we introduce a digital twin-driven metamorphic
testing framework that addresses these challenges by creating a virtual replica
of the self-driving system and its operating environment. By combining digital
twin technology with AI-based image generative models such as Stable Diffusion,
our approach enables the systematic generation of realistic and diverse driving
scenes. This includes variations in weather, road topology, and environmental
features, all while maintaining the core semantics of the original scenario.
The digital twin provides a synchronized simulation environment where changes
can be tested in a controlled and repeatable manner. Within this environment,
we define three metamorphic relations inspired by real-world traffic rules and
vehicle behavior. We validate our framework in the Udacity self-driving
simulator and demonstrate that it significantly enhances test coverage and
effectiveness. Our method achieves the highest true positive rate (0.719), F1
score (0.689), and precision (0.662) compared to baseline approaches. This
paper highlights the value of integrating digital twins with AI-powered
scenario generation to create a scalable, automated, and high-fidelity testing
solution for autonomous vehicle safety.

</details>


### [247] [TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking](https://arxiv.org/abs/2510.07134)
*Jiahang Liu,Yunpeng Qi,Jiazhao Zhang,Minghan Li,Shaoan Wang,Kui Wu,Hanjing Ye,Hong Zhang,Zhibo Chen,Fangwei Zhong,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: TrackVLA++ 通过引入极坐标思维链（Polar-CoT）和目标识别记忆（TIM）模块，增强了视觉导航跟踪（EVT）能力，提高了在遮挡和干扰下的鲁棒性，并在公开基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉导航跟踪方法缺乏显式的空间推理和有效的时间记忆，在严重遮挡或存在外观相似的干扰物时容易失败。

Method: 提出了一种名为 TrackVLA++ 的新颖视觉-语言-动作（VLA）模型，包含两个关键模块：1. 空间推理模块：引入极坐标思维链（Polar-CoT），用于推断目标的相对位置并将其编码为用于动作预测的极坐标标记。2. 目标识别记忆（TIM）模块：利用门控更新策略来保存长时目标记忆，确保时空一致性并减少在长时间遮挡期间丢失目标。

Result: TrackVLA++ 在公共基准测试的自主运动和多摄像头设置中均实现了最先进的性能。在具有挑战性的 EVT-Bench DT 数据集上，TrackVLA++ 的性能分别比之前的领先方法提高了 5.1% 和 12%。该模型还表现出强大的零样本泛化能力，能够在动态和遮挡场景中进行鲁棒的现实世界跟踪。

Conclusion: TrackVLA++ 通过其创新的空间推理和记忆机制，显著提高了视觉导航跟踪的能力，尤其是在复杂和具有挑战性的环境中，并展现了良好的泛化能力。

Abstract: Embodied Visual Tracking (EVT) is a fundamental ability that underpins
practical applications, such as companion robots, guidance robots and service
assistants, where continuously following moving targets is essential. Recent
advances have enabled language-guided tracking in complex and unstructured
scenes. However, existing approaches lack explicit spatial reasoning and
effective temporal memory, causing failures under severe occlusions or in the
presence of similar-looking distractors. To address these challenges, we
present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances
embodied visual tracking with two key modules, a spatial reasoning mechanism
and a Target Identification Memory (TIM). The reasoning module introduces a
Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative
position and encodes it as a compact polar-coordinate token for action
prediction. Guided by these spatial priors, the TIM employs a gated update
strategy to preserve long-horizon target memory, ensuring spatiotemporal
consistency and mitigating target loss during extended occlusions. Extensive
experiments show that TrackVLA++ achieves state-of-the-art performance on
public benchmarks across both egocentric and multi-camera settings. On the
challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading
approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong
zero-shot generalization, enabling robust real-world tracking in dynamic and
occluded scenarios.

</details>


### [248] [DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction](https://arxiv.org/abs/2510.07152)
*Jingkai Sun,Gang Han,Pihai Sun,Wen Zhao,Jiahang Cao,Jiaxu Wang,Yijie Guo,Qiang Zhang*

Main category: cs.RO

TL;DR: 提出了一种新的框架，用于人形机器人的地形感知运动。该框架通过结合盲骨干地形感知运动策略、多模态交叉注意力Transformer和逼真的深度图像合成方法，克服了现有方法的局限性，实现了高效的策略训练和鲁棒的地形适应性运动。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人运动方法在训练效率、泛化能力和鲁棒性方面存在不足，分别表现为模拟到现实的差距，以及对多传感器和定位系统的依赖。本研究旨在克服这些挑战。

Method: 本研究提出了一种新的框架，该框架紧密集成了三个关键组成部分：（1）具有盲骨干的地形感知运动策略，利用预训练的地形图感知来指导具有最小视觉输入的强化学习；（2）多模态交叉注意力Transformer，用于从噪声深度图像重建结构化地形表示；（3）逼真的深度图像合成方法，采用自遮挡感知射线投射和噪声感知建模来合成逼真的深度观测。

Result: 该框架实现了超过30%的地形重建误差降低，并能在各种具有挑战性的地形上实现敏捷和自适应的运动，在全尺寸人形机器人上得到了验证。

Conclusion: 本研究提出的新框架通过结合先进的感知和学习技术，有效解决了人形机器人地形感知运动中的关键挑战，实现了更高效、更鲁棒的运动控制。

Abstract: Recent advancements in legged robot perceptive locomotion have shown
promising progress. However, terrain-aware humanoid locomotion remains largely
constrained to two paradigms: depth image-based end-to-end learning and
elevation map-based methods. The former suffers from limited training
efficiency and a significant sim-to-real gap in depth perception, while the
latter depends heavily on multiple vision sensors and localization systems,
resulting in latency and reduced robustness. To overcome these challenges, we
propose a novel framework that tightly integrates three key components: (1)
Terrain-Aware Locomotion Policy with a Blind Backbone, which leverages
pre-trained elevation map-based perception to guide reinforcement learning with
minimal visual input; (2) Multi-Modality Cross-Attention Transformer, which
reconstructs structured terrain representations from noisy depth images; (3)
Realistic Depth Images Synthetic Method, which employs self-occlusion-aware ray
casting and noise-aware modeling to synthesize realistic depth observations,
achieving over 30\% reduction in terrain reconstruction error. This combination
enables efficient policy training with limited data and hardware resources,
while preserving critical terrain features essential for generalization. We
validate our framework on a full-sized humanoid robot, demonstrating agile and
adaptive locomotion across diverse and challenging terrains.

</details>


### [249] [A Narwhal-Inspired Sensing-to-Control Framework for Small Fixed-Wing Aircraft](https://arxiv.org/abs/2510.07160)
*Fengze Xie,Xiaozhou Fan,Jacob Schuster,Yisong Yue,Morteza Gharib*

Main category: cs.RO

TL;DR: 本研究提出了一种端到端的传感-控制流程，用于提高固定翼无人机的低速敏捷性，结合了仿生硬件、物理信息动力学学习和凸控制分配。


<details>
  <summary>Details</summary>
Motivation: 固定翼无人机虽然续航和效率高，但由于动力学耦合严重，低速机动性不足。

Method: 通过仿生学设计（借鉴独角鲸长牙）的多孔探头和稀疏的机翼压力传感器来测量气流。利用数据驱动校准估算空速和流角。学习一个控制仿射动力学模型，并使用对称正则化器提高可辨识性。最后，通过正则化最小二乘分配器实现期望的力和力矩。

Result: 风洞试验表明，增加机翼压力传感器可将力估计误差减少 25-30%。所提出的模型在分布变化下的性能下降（约 12%）优于非结构化基线（约 44%）。力跟踪精度提高，输入更平滑，其中法向力均方根误差（RMSE）比普通仿射模型减少 27%，比非结构化基线减少 34%。

Conclusion: 所提出的方法通过结合仿生传感、物理信息学习和优化的控制分配，有效提高了固定翼无人机的低速控制性能和鲁棒性。

Abstract: Fixed-wing unmanned aerial vehicles (UAVs) offer endurance and efficiency but
lack low-speed agility due to highly coupled dynamics. We present an end-to-end
sensing-to-control pipeline that combines bio-inspired hardware,
physics-informed dynamics learning, and convex control allocation. Measuring
airflow on a small airframe is difficult because near-body aerodynamics,
propeller slipstream, control-surface actuation, and ambient gusts distort
pressure signals. Inspired by the narwhal's protruding tusk, we mount in-house
multi-hole probes far upstream and complement them with sparse, carefully
placed wing pressure sensors for local flow measurement. A data-driven
calibration maps probe pressures to airspeed and flow angles. We then learn a
control-affine dynamics model using the estimated airspeed/angles and sparse
sensors. A soft left/right symmetry regularizer improves identifiability under
partial observability and limits confounding between wing pressures and
flaperon inputs. Desired wrenches (forces and moments) are realized by a
regularized least-squares allocator that yields smooth, trimmed actuation.
Wind-tunnel studies across a wide operating range show that adding wing
pressures reduces force-estimation error by 25-30%, the proposed model degrades
less under distribution shift (about 12% versus 44% for an unstructured
baseline), and force tracking improves with smoother inputs, including a 27%
reduction in normal-force RMSE versus a plain affine model and 34% versus an
unstructured baseline.

</details>


### [250] [TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics](https://arxiv.org/abs/2510.07181)
*Yi Han,Cheng Chi,Enshen Zhou,Shanyu Rong,Jingkun An,Pengwei Wang,Zhongyuan Wang,Lu Sheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: TIGeR通过整合外部工具将视觉-语言模型（VLM）从感知估计器转变为精确的几何计算器，实现了机器人操作中的厘米级精度。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在空间推理方面存在局限性，无法满足机器人操作所需的计算精度，它们将几何问题简化为模式识别任务，未能利用深度传感器和相机校准中的度量线索。

Method: TIGeR框架通过让VLM生成和执行精确的几何计算来工作。它识别几何推理需求，合成计算代码，并调用专门的库进行计算。该方法还引入了TIGeR-300K数据集，并采用监督微调（SFT）和强化微调（RFT）的两阶段训练流程，以及分层奖励设计。

Result: TIGeR在几何推理基准测试中达到了最先进的性能，并在现实世界的机器人操作任务中实现了厘米级精度。

Conclusion: TIGeR成功地将VLM从感知估计器转变为几何计算机，通过外部工具实现了机器人操作中的高精度几何推理。

Abstract: Vision-Language Models (VLMs) have shown remarkable capabilities in spatial
reasoning, yet they remain fundamentally limited to qualitative precision and
lack the computational precision required for real-world robotics. Current
approaches fail to leverage metric cues from depth sensors and camera
calibration, instead reducing geometric problems to pattern recognition tasks
that cannot deliver the centimeter-level accuracy essential for robotic
manipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel
framework that transforms VLMs from perceptual estimators to geometric
computers by enabling them to generate and execute precise geometric
computations through external tools. Rather than attempting to internalize
complex geometric operations within neural networks, TIGeR empowers models to
recognize geometric reasoning requirements, synthesize appropriate
computational code, and invoke specialized libraries for exact calculations. To
support this paradigm, we introduce TIGeR-300K, a comprehensive
tool-invocation-oriented dataset covering point transformations, pose
estimation, trajectory generation, and spatial compatibility verification,
complete with tool invocation sequences and intermediate computations. Through
a two-stage training pipeline combining supervised fine-tuning (SFT) and
reinforcement fine-tuning (RFT) with our proposed hierarchical reward design,
TIGeR achieves SOTA performance on geometric reasoning benchmarks while
demonstrating centimeter-level precision in real-world robotic manipulation
tasks.

</details>


### [251] [COMPAct: Computational Optimization and Automated Modular design of Planetary Actuators](https://arxiv.org/abs/2510.07197)
*Aman Singh,Deepak Kapa,Suryank Joshi,Shishir Kolathaya*

Main category: cs.RO

TL;DR: 该研究提出了一个名为COMPAct的框架，用于优化行星齿轮减速器的设计，以最小化质量和宽度，同时最大化效率，并自动化CAD模型生成，以实现3D打印。


<details>
  <summary>Details</summary>
Motivation: 虽然机器人执行器的优化设计至关重要，但对齿轮箱参数的优化和执行器CAD的自动化研究却很少。本研究旨在解决这一问题。

Method: COMPAct框架系统地识别给定电机在四种齿轮箱类型（单级、复合、Wolfrom和双级行星齿轮箱）中的最佳齿轮箱参数。该框架旨在最小化质量和执行器宽度，同时最大化效率，并自动化执行器CAD生成过程，以实现直接3D打印。通过该框架探索了各种齿轮比下的优化齿轮箱设计，并生成了不同齿轮比和电机的四种齿轮箱类型的CAD模型。两种执行器被制造出来，并通过功率效率、空载回程和传动刚度测试进行了实验评估。

Result: 实验结果表明，单级行星齿轮箱执行器的机械效率为60-80%，空载回程为0.59度，传动刚度为242.7 Nm/rad。复合行星齿轮箱执行器的效率为60%，回程为2.6度，刚度为201.6 Nm/rad。

Conclusion: COMPAct框架能够识别不同齿轮比范围的优化齿轮箱设计，并成功自动化了CAD模型生成，以便直接进行3D打印。实验验证了所设计执行器的性能。

Abstract: The optimal design of robotic actuators is a critical area of research, yet
limited attention has been given to optimizing gearbox parameters and
automating actuator CAD. This paper introduces COMPAct: Computational
Optimization and Automated Modular Design of Planetary Actuators, a framework
that systematically identifies optimal gearbox parameters for a given motor
across four gearbox types, single-stage planetary gearbox (SSPG), compound
planetary gearbox (CPG), Wolfrom planetary gearbox (WPG), and double-stage
planetary gearbox (DSPG). The framework minimizes mass and actuator width while
maximizing efficiency, and further automates actuator CAD generation to enable
direct 3D printing without manual redesign. Using this framework, optimal
gearbox designs are explored over a wide range of gear ratios, providing
insights into the suitability of different gearbox types across various gear
ratio ranges. In addition, the framework is used to generate CAD models of all
four gearbox types with varying gear ratios and motors. Two actuator types are
fabricated and experimentally evaluated through power efficiency, no-load
backlash, and transmission stiffness tests. Experimental results indicate that
the SSPG actuator achieves a mechanical efficiency of 60-80 %, a no-load
backlash of 0.59 deg, and a transmission stiffness of 242.7 Nm/rad, while the
CPG actuator demonstrates 60 % efficiency, 2.6 deg backlash, and a stiffness of
201.6 Nm/rad. Code available at:
https://anonymous.4open.science/r/COMPAct-SubNum-3408 Video:
https://youtu.be/99zOKgxsDho

</details>


### [252] [HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving](https://arxiv.org/abs/2510.07210)
*Donald Pfaffmann,Matthias Klusch,Marcel Steinmetz*

Main category: cs.RO

TL;DR: HyPlan是一种新颖的混合学习辅助规划方法，用于解决自动驾驶汽车在部分可观察交通环境中的无碰撞导航问题。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶汽车在部分可观察交通环境中的无碰撞导航问题。

Method: HyPlan结合了多智能体行为预测、深度强化学习（Proximal Policy Optimization）和近似在线POMDP规划（带启发式置信度垂直剪枝）方法。

Result: 在CARLA-CTS2基准的临界交通场景（含行人）的实验性能分析表明，HyPlan的导航安全性优于相关基线方法，且比其他在线POMDP规划器更快。

Conclusion: HyPlan在保证行车安全的前提下，显著提高了导航速度。

Abstract: We present a novel hybrid learning-assisted planning method, named HyPlan,
for solving the collision-free navigation problem for self-driving cars in
partially observable traffic environments. HyPlan combines methods for
multi-agent behavior prediction, deep reinforcement learning with proximal
policy optimization and approximated online POMDP planning with heuristic
confidence-based vertical pruning to reduce its execution time without
compromising safety of driving. Our experimental performance analysis on the
CARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed
that HyPlan may navigate safer than selected relevant baselines and perform
significantly faster than considered alternative online POMDP planners.

</details>


### [253] [DeepXPalm: Tilt and Position Rendering using Palm-worn Haptic Display and CNN-based Tactile Pattern Recognition](https://arxiv.org/abs/2204.03521)
*Altamirano Cabrera Miguel,Sautenkov Oleg,Tirado Jonathan,Fedoseev Aleksey,Kopanev Pavel,Kajimoto Hiroyuki,Tsetserukou Dzmitry*

Main category: cs.RO

TL;DR: 通过CNN识别塑料移液器的倾斜和位置，提高用户在遥操作中的感知能力。


<details>
  <summary>Details</summary>
Motivation: 遥操作中的形变物体需要高精度和灵巧性，现有技术因物体形状动态变化导致感知模糊，影响机器人定位，因此需要解决倾斜和位置分类问题以提供清晰的触觉模式。

Method: 提出一种基于卷积神经网络（CNN）的方法，结合多接触触觉设备LinkGlide和机器人夹爪上的触觉传感器阵列，实时检测并识别形变物体（塑料移液器）的倾斜和位置，并生成掩膜以渲染多接触触觉刺激。

Result: CNN算法和预设掩膜的结合，使用户在识别倾斜和位置时的准确率从直接数据的9.67%提高到82.5%。

Conclusion: 提出的基于CNN的方法显著提高了用户在遥操作塑料移液器时对物体倾斜和位置的识别能力，为实现高精度的遥操作提供了有效方案。

Abstract: Telemanipulation of deformable objects requires high precision and dexterity
from the users, which can be increased by kinesthetic and tactile feedback.
However, the object shape can change dynamically, causing ambiguous perception
of its alignment and hence errors in the robot positioning. Therefore, the tilt
angle and position classification problem has to be solved to present a clear
tactile pattern to the user. This work presents a telemanipulation system for
plastic pipettes consisting of a multi-contact haptic device LinkGlide to
deliver haptic feedback at the users' palm and two tactile sensors array
embedded in the 2-finger Robotiq gripper. We propose a novel approach based on
Convolutional Neural Networks (CNN) to detect the tilt and position while
grasping deformable objects. The CNN generates a mask based on recognized tilt
and position data to render further multi-contact tactile stimuli provided to
the user during the telemanipulation. The study has shown that using the CNN
algorithm and the preset mask, tilt, and position recognition by users is
increased from 9.67% using the direct data to 82.5%.

</details>


### [254] [TiltXter: CNN-based Electro-tactile Rendering of Tilt Angle for Telemanipulation of Pasteur Pipettes](https://arxiv.org/abs/2409.15838)
*Miguel Altamirano Cabrera,Jonathan Tirado,Aleksey Fedoseev,Oleg Sautenkov,Vladimir Poliakov,Pavel Kopanev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 该研究提出了一种基于卷积神经网络（CNN）的触觉遥操作系统，用于检测塑料移液管的倾斜度，并通过电触觉刺激向用户提供反馈，从而提高了操作的准确性和成功率。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人抓取过程中可变形物体形状变化导致的感知模糊和定位错误问题，提高用户在遥操作中的精确度和灵巧性。

Method: 提出了一种新颖的基于卷积神经网络（CNN）的方法来检测可变形物体的倾斜度。该CNN能够根据识别到的倾斜数据生成触觉模式，并进一步通过电触觉刺激呈现给用户。整个系统集成了Force Dimension Omega.7触觉界面、两个电刺激阵列和两个嵌入在两指Robotiq夹爪中的触觉传感器阵列。

Result: 与仅使用缩小数据相比，使用CNN算法后，用户识别倾斜度的准确率从23.13%提高到57.9%。在遥操作过程中，成功率也从使用缩小数据的53.12%提高到使用CNN生成的触觉模式的92.18%。

Conclusion: 基于CNN的触觉反馈系统显著提高了用户对物体倾斜度的识别能力和遥操作的成功率。

Abstract: The shape of deformable objects can change drastically during grasping by
robotic grippers, causing an ambiguous perception of their alignment and hence
resulting in errors in robot positioning and telemanipulation. Rendering clear
tactile patterns is fundamental to increasing users' precision and dexterity
through tactile haptic feedback during telemanipulation. Therefore, different
methods have to be studied to decode the sensors' data into haptic stimuli.
This work presents a telemanipulation system for plastic pipettes that consists
of a Force Dimension Omega.7 haptic interface endowed with two
electro-stimulation arrays and two tactile sensor arrays embedded in the
2-finger Robotiq gripper. We propose a novel approach based on convolutional
neural networks (CNN) to detect the tilt of deformable objects. The CNN
generates a tactile pattern based on recognized tilt data to render further
electro-tactile stimuli provided to the user during the telemanipulation. The
study has shown that using the CNN algorithm, tilt recognition by users
increased from 23.13\% with the downsized data to 57.9%, and the success rate
during teleoperation increased from 53.12% using the downsized data to 92.18%
using the tactile patterns generated by the CNN.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [255] [AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning](https://arxiv.org/abs/2510.06261)
*Zhanke Zhou,Chentao Cao,Xiao Feng,Xuan Li,Zongze Li,Xiangyu Lu,Jiangchao Yao,Weikai Huang,Linrui Xu,Tian Cheng,Guanyu Jiang,Yiming Zheng,Brando Miranda,Tongliang Liu,Sanmi Koyejo,Masashi Sugiyama,Bo Han*

Main category: cs.AI

TL;DR: AlphaApollo是一个自我进化的智能推理系统，通过结合计算工具和检索工具，并支持多轮模型协作，来解决基础模型在推理能力和测试时迭代方面的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 基础模型在推理方面存在模型内在能力有限和测试时迭代不可靠两个瓶颈。

Method: AlphaApollo整合了计算工具（Python及其库）和检索工具（外部信息），通过共享状态图支持多轮、多模型协同解决问题，记录候选方案、可执行检查和反馈，以进行迭代优化。

Result: 在AIME 2024/2025的评估中，AlphaApollo显著提升了Qwen2.5-14B-Instruct（Average@32提升5.15%，Pass@32提升23.34%）和Llama-3.3-70B-Instruct（Average@32提升8.91%，Pass@32提升26.67%）。超过80%的工具调用成功执行，工具使用显著优于不使用工具的基线模型。

Conclusion: AlphaApollo通过整合工具和多轮协作，有效提升了基础模型的推理能力上限。

Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to
address two bottlenecks in foundation model (FM) reasoning-limited
model-intrinsic capacity and unreliable test-time iteration. AlphaApollo
orchestrates multiple models with professional tools to enable deliberate,
verifiable reasoning. It couples (i) a computation tool (Python with numerical
and symbolic libraries) and (ii) a retrieval tool (task-relevant external
information) to execute exact calculations and ground decisions. The system
further supports multi-round, multi-model solution evolution via a shared state
map that records candidates, executable checks, and feedback for iterative
refinement. In evaluations on AIME 2024/2025 across multiple models,
AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32
for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for
Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool
calls are successfully executed, with consistent outperformance of non-tool
baselines, thereby lifting the capability ceiling of FMs. More empirical
results and implementation details will be updated at
https://github.com/tmlr-group/AlphaApollo.

</details>


### [256] [Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization](https://arxiv.org/abs/2510.06274)
*Mohammad Mahdi Samiei Paqaleh,Arash Marioriyad,Arman Tahmasebi-Zadeh,Mohamadreza Fereydooni,Mahdi Ghaznavai,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: 为评估和衡量 AI 模型的推理能力，提出了一种名为“复杂性分布外泛化”（Complexity OoD）的框架和问题设定。该框架通过模型在测试实例上的表现来衡量其推理能力，这些实例的最小解决方案复杂度（包括表示复杂度和计算复杂度）超过了所有训练样本的复杂度。文章还探讨了复杂性 OoD 与其他泛化概念的区别，并将学习和推理统一在复杂性分析的视角下。最后，文章提出了一系列实践建议，包括在基准测试和评估指标设计中纳入复杂性，以及在模型架构和训练方法中显式地考虑和分配计算资源，以应对复杂性 OoD 的挑战。


<details>
  <summary>Details</summary>
Motivation: 目前在 AI 推理能力方面缺乏明确的定义和评估指标，这阻碍了 AI 在需要复杂推理的任务上的进步。

Method: 提出“复杂性分布外泛化”（Complexity OoD）框架，通过模型在测试实例上的表现来衡量其推理能力，这些实例的最小解决方案复杂度（表示复杂度和计算复杂度）超过了所有训练样本的复杂度。通过分析解决方案描述的柯尔莫哥洛夫复杂度及其代理指标（如对象/关系计数、推理步骤计数）来形式化复杂性。

Result: 模型在面对比训练数据更复杂的推理任务时，能够保持性能，展示了复杂性 OoD 泛化能力。文章提出了在整个 AI 技术栈中实践复杂性 OoD 的建议，包括改进基准测试、评估指标、监督方式、归纳偏置设计，以及处理“学习到推理”的溢出效应。

Conclusion: AI 模型的稳健推理能力发展，不能仅仅依靠增加数据量，而需要开发能够显式建模和分配计算资源的架构和训练方法，以应对复杂性 OoD 的挑战。

Abstract: Recent progress has pushed AI frontiers from pattern recognition tasks toward
problems that require step by step, System2 style reasoning, especially with
large language models. Yet, unlike learning, where generalization and out of
distribution (OoD) evaluation concepts are well formalized, there is no clear,
consistent definition or metric for reasoning ability. We propose Complexity
Out of Distribution (Complexity OoD) generalization as a framework and problem
setting to define and measure reasoning. A model exhibits Complexity OoD
generalization when it maintains performance on test instances whose minimal
required solution complexity, either representational (richer solution
structure) or computational (more reasoning steps/program length), exceeds that
of all training examples. We formalize complexity via solution description
Kolmogorov complexity and operational proxies (e.g., object/relation counts;
reasoning step counts), clarifying how Complexity OoD differs from length and
compositional OoD. This lens unifies learning and reasoning: many cases
solvable with System1 like processing at low complexity become System2 like
under complexity pressure, while System2 can be viewed as generalization over
solution structures. We translate this perspective into practice with
recommendations for operationalizing Complexity OoD across the stack:
incorporating complexity into benchmark and evaluation metric design,
rethinking supervision to target solution traces, seeking and designing
inductive biases for Complexity OoD generalization, addressing learning to
reason spillovers such as spurious shortcuts, semantic robustness, catastrophic
forgetting, and step wise calibration. Because Complexity OoD cannot be solved
by scaling data alone, progress toward robust reasoning will require
architectures and training regimes that explicitly model and allocate
computation with respect to complexity.

</details>


### [257] [BuilderBench -- A benchmark for generalist agents](https://arxiv.org/abs/2510.06288)
*Raj Ghugare,Catherine Ji,Kathryn Wantlin,Jin Schofield,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: AI模型通常通过模仿学习，难以解决新问题。本文提出了BuilderBench，一个用于加速开放式探索的代理预训练基准，要求代理学习如何用积木搭建任意结构。


<details>
  <summary>Details</summary>
Motivation: 为了解决新颖的问题，智能体应具备通过经验进行探索和学习的技能，而开发通过交互进行学习的智能体的可扩展学习机制仍是一个重大挑战。

Method: BuilderBench包含一个硬件加速模拟器，允许智能体与各种物理积木进行交互，并包含一个包含42个多样化目标结构的套件，用于测试物理、数学和长期规划的理解。在训练中，智能体无需外部监督即可探索和学习环境的通用原理。在评估中，智能体需要搭建任务套件中未见过的目标结构。

Result: 实验表明，BuilderBench中的许多任务对当前算法构成了挑战。此外，还提供了一个“训练轮”协议，用于在单个目标结构上进行训练和评估，并提供了六种不同算法的单文件实现作为参考。

Conclusion: BuilderBench是一个旨在加速开放式探索的代理预训练基准，通过要求智能体学习搭建积木结构来推动AI在解决新颖问题和发展具身推理能力方面的进展。

Abstract: Today's AI models learn primarily through mimicry and sharpening, so it is
not surprising that they struggle to solve problems beyond the limits set by
existing data. To solve novel problems, agents should acquire skills for
exploring and learning through experience. Finding a scalable learning
mechanism for developing agents that learn through interaction remains a major
open problem. In this work, we introduce BuilderBench, a benchmark to
accelerate research into agent pre-training that centers open-ended
exploration. BuilderBench requires agents to learn how to build any structure
using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated
simulator of a robotic agent interacting with various physical blocks, and
$(2)$ a task-suite with over 42 diverse target structures that are carefully
curated to test an understanding of physics, mathematics, and long-horizon
planning. During training, agents have to explore and learn general principles
about the environment without any external supervision. During evaluation,
agents have to build the unseen target structures from the task suite. Solving
these tasks requires a sort of \emph{embodied reasoning} that is not reflected
in words but rather in actions, experimenting with different strategies and
piecing them together. Our experiments show that many of these tasks challenge
the current iteration of algorithms. Hence, we also provide a ``training
wheels'' protocol, in which agents are trained and evaluated to build a single
target structure from the task suite. Finally, we provide single-file
implementations of six different algorithms as a reference point for
researchers.

</details>


### [258] [Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration](https://arxiv.org/abs/2510.06302)
*Ksenija Lace,Marite Kirikova*

Main category: cs.AI

TL;DR: 本研究提出了一种基于游戏化学习的设计框架，旨在解决信息系统集成在公司合并后整合过程中的培训挑战，以克服现有方法的学习曲线陡峭和学习者积极性低的问题。


<details>
  <summary>Details</summary>
Motivation: 公司合并后的信息系统集成面临独特挑战，尽管业务层面的整合有理论和实践指导，但在信息系统集成培训方面存在显著不足。现有方法（AMILI和AMILP）在实践中被认为学习曲线陡峭、学习者积极性低。

Method: 本研究通过分析学习理论、认知负荷和动机模型以及严肃游戏设计框架，提出了一个基于游戏化学习的设计框架，以将静态的方法培训转化为引人入胜的学习体验。该框架包含两个组成部分：转换过程和由此产生的学习体验。

Result: 本研究识别了用于信息系统集成（在公司合并后整合过程中）游戏化学习设计的关键需求，并提出了一个包含转换过程和学习体验的设计框架。

Conclusion: 研究提出了一个游戏化学习设计框架，以应对公司合并后信息系统集成培训的挑战，并计划通过迭代设计和真实世界验证来开发和评估该框架。

Abstract: Post-merger integration states unique challenges for professionals
responsible for information system integration aimed on alignment and
combination diverse system architectures of merging organizations. Although the
theoretical and practical guidance exists for post-merger integration on the
business level, there is a significant gap in training for information system
integration in this context. In prior research specific methods AMILI (Support
method for informed decision identification) and AMILP (Support method for
informed decision-making) were introduced for the support of information system
integration decisions in the post-merger integration. But during the practical
application was reported high learning curve and low learner motivation. This
paper explores how game-based learning design can address these limitations by
transforming static method training into engaging learning experience. The
study analyzes foundational learning theories, cognitive load and motivation
models, and serious game design frameworks to identify the essential
requirements for a game-based learning design framework tailored to information
system integration in post-merger integration. Requirements are structured in
two components: the transformation process and resulting learning experience.
The paper concludes with a plan for developing and evaluating the proposed
framework through iterative design and real-world validation.

</details>


### [259] [Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks](https://arxiv.org/abs/2510.06307)
*Wentao Deng,Jiahuan Pei,Zhiwei Xu,Zhaochun Ren,Zhumin Chen,Pengjie Ren*

Main category: cs.AI

TL;DR: 本研究提出了一种名为BCCS的多智能体系统共识机制，通过选择最优协作伙伴和校准系统内部信念来提升共识的稳定性，解决了现有方法依赖投票且忽视内部信念矛盾的问题。实验证明BCCS在MATH和MMLU基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于投票的共识机制忽视了系统内部信念的矛盾，且采用无差别的协作方式，未能识别最优协作者，阻碍了稳定共识的形成。

Method: 提出一个理论框架来选择最优协作者以最大化共识稳定性，并基于此框架提出BCCS（Belief-Calibrated Consensus Seeking）框架，通过选择最优协作者和利用系统内部信念校准共识判断来实现稳定共识。

Result: 在MATH和MMLU基准数据集上，BCCS框架在最具挑战性的任务上分别比现有最佳方法提高了2.23%和3.95%的准确率。

Conclusion: BCCS框架通过选择最优协作者和校准共识判断，有效提高了多智能体系统在复杂NLP任务中达成稳定共识的能力，并在MATH和MMLU数据集上取得了显著的性能提升。

Abstract: A multi-agent system (MAS) enhances its capacity to solve complex natural
language processing (NLP) tasks through collaboration among multiple agents,
where consensus-seeking serves as a fundamental mechanism. However, existing
consensus-seeking approaches typically rely on voting mechanisms to judge
consensus, overlooking contradictions in system-internal beliefs that
destabilize the consensus. Moreover, these methods often involve agents
updating their results through indiscriminate collaboration with every other
agent. Such uniform interaction fails to identify the optimal collaborators for
each agent, hindering the emergence of a stable consensus. To address these
challenges, we provide a theoretical framework for selecting optimal
collaborators that maximize consensus stability. Based on the theorems, we
propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate
stable consensus via selecting optimal collaborators and calibrating the
consensus judgment by system-internal beliefs. Experimental results on the MATH
and MMLU benchmark datasets demonstrate that the proposed BCCS framework
outperforms the best existing results by 2.23% and 3.95% of accuracy on
challenging tasks, respectively. Our code and data are available at
https://github.com/dengwentao99/BCCS.

</details>


### [260] [Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences](https://arxiv.org/abs/2510.07276)
*Pulkit Rustagi,Kyle Hollins Wray,Sandhya Saisubramanian*

Main category: cs.AI

TL;DR: 提出了一种新的多目标多智能体寻路（MO-MAPF）框架LCBS，该框架能够根据用户定义的优先级直接计算出单一最优解，解决了现有方法在处理多目标和可扩展性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有MO-MAPF算法通常生成帕累托最优解集，但无法直接优化用户定义的偏好，并且随着目标数量的增加，计算成本会急剧上升。

Method: 提出了一种基于字典序的MO-MAPF建模框架，并设计了一种名为LCBS的算法。LCBS结合了优先感知低层 A* 搜索和基于冲突的搜索（CBS），避免了构建帕累托最优前沿，能够根据目标优先级直接计算出单一最优解。

Result: LCBS算法能够处理多达十个目标的情况，在最优性和可扩展性方面表现优于现有方法。在标准和随机MAPF基准测试中，LCBS的成功率显著高于最先进的基线方法，尤其是在目标数量增加的情况下。

Conclusion: LCBS算法为MO-MAPF问题提供了一种新颖的、基于字典序优先级的解决方案，在计算效率和处理多目标方面具有显著优势，能够有效解决现有方法的局限性。

Abstract: Many real-world scenarios require multiple agents to coordinate in shared
environments, while balancing trade-offs between multiple, potentially
competing objectives. Current multi-objective multi-agent path finding
(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto
frontiers. They do not explicitly optimize for user-defined preferences, even
when the preferences are available, and scale poorly with the number of
objectives. We propose a lexicographic framework for modeling MO-MAPF, along
with an algorithm \textit{Lexicographic Conflict-Based Search} (LCBS) that
directly computes a single solution aligned with a lexicographic preference
over objectives. LCBS integrates a priority-aware low-level $A^*$ search with
conflict-based search, avoiding Pareto frontier construction and enabling
efficient planning guided by preference over objectives. We provide insights
into optimality and scalability, and empirically demonstrate that LCBS computes
optimal solutions while scaling to instances with up to ten objectives -- far
beyond the limits of existing MO-MAPF methods. Evaluations on standard and
randomized MAPF benchmarks show consistently higher success rates against
state-of-the-art baselines, especially with increasing number of objectives.

</details>


### [261] [Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?](https://arxiv.org/abs/2510.06410)
*Aochong Oliver Li,Tanya Goyal*

Main category: cs.AI

TL;DR: 标准LLM的训练方法未能使模型具备在其同伴的推理轨迹之外进行推理的能力，并且模型在处理同伴提供的引导性信息时也表现不佳。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究标准的、单独推理的训练方法是否能够使大型语言模型（LLMs）具备在同伴的推理轨迹之外进行推理（off-trajectory reasoning）的能力，以及评估模型在处理同伴提供的正确推理步骤时的表现。

Method: 本研究提出了两种测试方法：可恢复性（Recoverability）测试，用于评估模型从误导性推理轨迹中回溯的能力；以及可引导性（Guidability）测试，用于评估模型在正确推理步骤的指导下进行推理的能力。研究评估了15种不同规模（1.5B-32B）的开源LLMs，并进行了控制实验，探究了教师模型选择、强化学习（RL）使用以及数据选择策略等因素对模型推理能力的影响。

Result: 研究发现，在基准测试中表现“更强”的模型在面对干扰时往往更脆弱。所有被测试的模型在处理超出其自身能力范围的问题时，都无法有效利用同伴提供的引导步骤，解决率低于9.2%。控制实验表明，即使蒸馏轨迹是正确的，教师模型在可恢复性方面的不足也会被转移给学生模型。

Conclusion: 现有的LLMs在进行多模型协作推理时存在局限性，尤其是在处理同伴的推理轨迹之外的信息时。本研究为评估多模型协作推理奠定了基础，并指出了当前LLMs在这一方面的不足，为未来训练更强的推理协作模型提供了指导性见解。

Abstract: Reasoning LLMs are trained to verbalize their reasoning process, yielding
strong gains on complex tasks. This transparency also opens a promising
direction: multiple reasoners can directly collaborate on each other's thinking
within a shared trajectory, yielding better inference efficiency and
exploration. A key prerequisite, however, is the ability to assess the
usefulness and build on another model's partial thinking -- we call this
off-trajectory reasoning. Our paper investigates a critical question: can
standard solo-reasoning training pipelines deliver desired off-trajectory
behaviors? We propose twin tests that capture the two extremes of the
off-trajectory spectrum, namely Recoverability, which tests whether LLMs can
backtrack from "distractions" induced by misleading reasoning traces, and
Guidability, which tests their ability to build upon correct reasoning from
stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and
reveals a counterintuitive finding -- "stronger" LLMs on benchmarks are often
more fragile under distraction. Moreover, all models tested fail to effectively
leverage guiding steps from collaborators on problems beyond their inherent
capabilities with solve rates remaining under 9.2%. Finally, we conduct control
studies to isolate the effects of three factors in post-training on these
behaviors: the choice of distillation teacher, the use of RL, and data
selection strategy. Our results provide actionable insights for training
natively strong reasoning collaborators; e.g., we find that suboptimal
recoverability behaviors of teacher models are transferred to distilled
students even if the distillation trajectories are correct. Taken together,
this work lays the groundwork for evaluating multi-model collaborations in
shared reasoning trajectories and highlights the limitations of off-the-shelf
reasoning LLMs.

</details>


### [262] [Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health](https://arxiv.org/abs/2510.06433)
*Aryan Singh Dalal,Yinglun Zhang,Duru Doğan,Atalay Mert İleri,Hande Küçük McGinty*

Main category: cs.AI

TL;DR: 本研究旨在建立一个知识图谱，通过整合USDA数据库中的食物黄酮含量和文献中的癌症关联信息，来连接食物和健康，以应对当前研究在标准化、机器可读的食物-健康关系表示方面的不足。


<details>
  <summary>Details</summary>
Motivation: 目前关于“食物即药物”的理念受到广泛关注，但缺乏标准化的、机器可读的知识表示方法来有效利用相关知识。本研究旨在解决这一研究空白。

Method: 利用KNARM方法学，通过整合USDA数据库中的食物黄酮含量和文献中的癌症关联信息，构建一个知识图谱，并以机器可操作的格式表示这些关系。

Result: 成功构建了一个知识图谱，连接了食物（特别是黄酮含量）和健康（特别是癌症关联），并以机器可操作的格式呈现，为研究人员探索饮食选择与疾病管理的复杂相互作用提供了示例。

Conclusion: 本研究成功构建了一个知识图谱，为食物与健康的关系提供了标准化、机器可读的表示，为未来的相关研究奠定了基础。未来工作将进一步扩展知识图谱的范围，并进行推理以发现隐藏的关联。

Abstract: The focus on "food as medicine" is gaining traction in the field of health
and several studies conducted in the past few years discussed this aspect of
food in the literature. However, very little research has been done on
representing the relationship between food and health in a standardized,
machine-readable format using a semantic web that can help us leverage this
knowledge effectively. To address this gap, this study aims to create a
knowledge graph to link food and health through the knowledge graph's ability
to combine information from various platforms focusing on flavonoid contents of
food found in the USDA databases and cancer connections found in the
literature. We looked closely at these relationships using KNARM methodology
and represented them in machine-operable format. The proposed knowledge graph
serves as an example for researchers, enabling them to explore the complex
interplay between dietary choices and disease management. Future work for this
study involves expanding the scope of the knowledge graph by capturing nuances,
adding more related data, and performing inferences on the acquired knowledge
to uncover hidden relationships.

</details>


### [263] [PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles](https://arxiv.org/abs/2510.06475)
*Yitao Long,Yuru Jiang,Hongjun Liu,Yilun Zhao,Jingchen Sun,Yiqiu Shen,Chen Zhao,Arman Cohan,Dennis Shasha*

Main category: cs.AI

TL;DR: 本研究提出了PuzzlePlex基准，用于评估基础模型在复杂动态环境中的推理和规划能力，并分析了其可扩展性。


<details>
  <summary>Details</summary>
Motivation: 评估基础模型在复杂、动态环境中的推理和规划能力及其可扩展性。

Method: 提出了PuzzlePlex基准，包含15种不同类型的谜题，支持可扩展性并提供了游戏环境和定制的游戏策略。在指令式和基于代码的设置下，使用精细的指标对前沿基础模型进行评估和扩展极限分析。

Result: 在指令式设置下，推理模型表现优于其他模型；基于代码的执行更具挑战性，但提供了可扩展且高效的替代方案。基础模型在PuzzlePlex上的表现各不相同，具体取决于模型架构和设置。

Conclusion: PuzzlePlex基准能够针对性地评估基础模型，并为改进其推理、规划和泛化能力提供方向。

Abstract: This work investigates the reasoning and planning capabilities of foundation
models and their scalability in complex, dynamic environments. We introduce
PuzzlePlex, a benchmark designed to assess these capabilities through a diverse
set of puzzles. PuzzlePlex consists of 15 types of puzzles, including
deterministic and stochastic games of varying difficulty, as well as
single-player and two-player scenarios. The PuzzlePlex framework provides a
comprehensive environment for each game, and supports extensibility to generate
more challenging instances as foundation models evolve. Additionally, we
implement customized game-playing strategies for comparison. Building on this
benchmark, we develop fine-grained metrics to measure performance and conduct
an in-depth analysis of frontier foundation models across two settings:
instruction-based and code-based. Furthermore, we systematically investigate
their scaling limits. Our findings show that reasoning models outperform others
in instruction-based settings, while code-based execution presents greater
challenges but offers a scalable and efficient alternative. PuzzlePlex enables
targeted evaluation and guides future improvements in reasoning, planning, and
generalization for foundation models.

</details>


### [264] [Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them](https://arxiv.org/abs/2510.06534)
*Jiahe Jin,Abhijay Paladugu,Chenyan Xiong*

Main category: cs.AI

TL;DR: 该研究提出了一种基于LLM的推理驱动方法来分析和提升Agentic Search中的推理行为，通过识别信息验证、权威评估、自适应搜索和错误恢复这四种有益的推理行为，并引入“行为引导”技术进行模型训练，实验证明该方法能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: Agentic Search使用LLM来理解复杂信息需求并执行多步搜索，但这对其推理和Agentic能力提出了挑战。

Method: 提出一个推理驱动的LLM管道来分析Agentic Search中的推理行为，识别出信息验证、权威评估、自适应搜索和错误恢复这四种有益行为。基于这些行为，提出“行为引导”技术，通过监督微调（SFT）和强化学习（RL）来训练Agentic Search模型。

Result: 在GAIA、WebWalker和HLE三个基准测试中，“行为引导”技术使Llama3.2-3B和Qwen3-1.7B模型的性能提升超过35%。研究还发现，SFT数据中的推理行为比最终答案的正确性对RL后的性能更关键。

Conclusion: Agentic Search中的推理行为（如信息验证、权威评估、自适应搜索和错误恢复）对于提升模型性能至关重要。“行为引导”技术通过在SFT阶段引入这些行为，可以显著提高模型的探索能力和最终性能，即使SFT阶段的答案不正确。该研究为Agentic Search的有效训练提供了新的思路。

Abstract: Agentic search leverages large language models (LLMs) to interpret complex
user information needs and execute a multi-step process of planning, searching,
and synthesizing information to provide answers. This paradigm introduces
unique challenges for LLMs' reasoning and agentic capabilities when interacting
with retrieval systems and the broader web. In this paper, we propose a
reasoning-driven LLM-based pipeline to study effective reasoning behavior
patterns in agentic search. Using this pipeline, we analyze successful agentic
search trajectories and identify four beneficial reasoning behaviors:
Information Verification, Authority Evaluation, Adaptive Search, and Error
Recovery. Based on these findings, we propose a technique called Behavior
Priming to train more effective agentic search models. It synthesizes agentic
search trajectories that exhibit these four behaviors and integrates them into
the agentic search model through supervised fine-tuning (SFT), followed by
standard reinforcement learning (RL). Experiments on three benchmarks (GAIA,
WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in
Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models
with RL. Crucially, we demonstrate that the desired reasoning behaviors in the
SFT data, rather than the correctness of the final answer, is the critical
factor for achieving strong final performance after RL: fine-tuning on
trajectories with desirable reasoning behaviors but incorrect answers leads to
better performance than fine-tuning on trajectories with correct answers. Our
analysis further reveals the underlying mechanism: the introduced reasoning
behaviors endow models with more effective exploration (higher pass@k and
entropy) and test-time scaling (longer trajectories) capabilities, providing a
strong foundation for RL. Our code will be released as open source.

</details>


### [265] [Auto-Prompt Ensemble for LLM Judge](https://arxiv.org/abs/2510.06538)
*Jiajie Li,Huayi Zhang,Peng Lin,Jinjun Xiong,Wei Xu*

Main category: cs.AI

TL;DR: 本研究提出了一种名为自动提示集成（APE）的自适应框架，通过选择性地为大型语言模型（LLM）添加辅助评估维度来提高LLM评估的可靠性，并解决了现有LLM评估器因未能识别隐含的人类评估标准而遗漏关键维度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM评估器在评估时常常会忽略一些关键的维度，因为它们无法准确地识别出人类评估中隐含的标准。本研究旨在解决这一问题。

Method: 本研究提出的自动提示集成（APE）框架能够从失败案例中自动学习评估维度，并结合一种名为“集体信心”（Collective Confidence）的新型信心估计方法，通过基于信心的集成机制来决定何时使用额外的评估维度。

Result: 大量的实验表明，APE框架能够提高LLM评估器在各种标准基准测试中的可靠性。例如，在零样本设置下，APE将GPT-4o在Reward Bench上的评估一致性从87.2%提高到了90.5%。

Conclusion: APE框架为LLM评估器提供了一种原则性的方法，使其能够利用测试时计算资源，并缩小人类与LLM评估之间的差距。

Abstract: We present a novel framework that improves the reliability of LLM judges by
selectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM
judges often miss crucial evaluation dimensions because they fail to recognize
the implicit standards underlying human assessments. To address this challenge,
we propose the Auto-Prompt Ensemble (APE), an adaptive framework that
automatically learns evaluation dimensions from its failure cases. APE
incorporates a confidence-based ensemble mechanism to decide when to adopt the
judgments from additional evaluation dimensions through a novel confidence
estimation approach called Collective Confidence. Extensive experiments
demonstrate that APE improves the reliability of LLM Judge across diverse
standard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward
Bench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a
principled approach for LLM Judge to leverage test-time computation, and bridge
the evaluation gap between human and LLM judges.

</details>


### [266] [WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks](https://arxiv.org/abs/2510.06587)
*Jingbo Yang,Bairu Hou,Wei Wei,Shiyu Chang,Yujia Bao*

Main category: cs.AI

TL;DR: WebDART是一个框架，能让LLM处理复杂的网页任务，通过将任务分解为导航、信息提取和执行三个子任务，并动态调整策略，提高了成功率并减少了导航步骤。


<details>
  <summary>Details</summary>
Motivation: LLM在处理需要长距离导航、大规模信息提取和约束推理的复杂网络任务时仍然存在挑战。

Method: WebDART通过动态分解复杂目标为导航、信息提取和执行三个子任务，并根据新发现的网页信息持续重新规划，使模型能够一次专注于一项技能，并避免冗余探索。

Result: 在WebChoreArena基准测试中，WebDART的成功率比之前的SOTA方法提高了13.7个百分点，在WebArena基准测试中表现相当，同时导航步骤减少了14.7步。

Conclusion: WebDART框架通过任务分解和动态重规划，有效提升了LLM在复杂网页任务上的表现。

Abstract: Large language model (LLM) agents are becoming competent at straightforward
web tasks, such as opening an item page or submitting a form, but still
struggle with objectives that require long horizon navigation, large scale
information extraction, and reasoning under constraints. We present WebDART, a
general framework that enables a single LLM to handle such complex chores.
WebDART (i) dynamically decomposes each objective into three focused subtasks:
navigation, information extraction, and execution, so the model concentrates on
one skill at a time, and (ii) continuously replans the decomposition as new
webpages are revealed, taking advantage of newly discovered filters or
shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,
WebDART lifts success rates by up to 13.7 percentage points over previous SOTA
agents, while matching their performance on the easier WebArena suite and
completing tasks with up to 14.7 fewer navigation steps.

</details>


### [267] [Fine-Grained Emotion Recognition via In-Context Learning](https://arxiv.org/abs/2510.06600)
*Zhaochun Ren,Zhou Yang,Chenglong Ye,Haizhou Sun,Chao Chen,Xiaofei Zhu,Xiangwen Liao*

Main category: cs.AI

TL;DR: 本文提出了一种名为EICL的情感 in-context learning 方法，通过引入情感相似的示例和动态软标签策略来解决现有ICL方法在细粒度情感识别中忽略决策过程的问题，并通过实验证明其优于ICL。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法在细粒度情感识别中，虽然增强了推理过程，但忽略了决策过程，导致在处理情感相似但语义不相似的示例时出现错误。

Method: 提出EICL方法，引入情感相似的示例，并使用动态软标签策略来优化查询表示，同时采用两阶段排除策略来优化决策过程。

Result: EICL在多个数据集上显著优于ICL方法。

Conclusion: EICL通过引入情感相似示例和优化决策过程，有效提升了细粒度情感识别的准确性。

Abstract: Fine-grained emotion recognition aims to identify the emotional type in
queries through reasoning and decision-making processes, playing a crucial role
in various systems. Recent methods use In-Context Learning (ICL), enhancing the
representation of queries in the reasoning process through semantically similar
examples, while further improving emotion recognition by explaining the
reasoning mechanisms. However, these methods enhance the reasoning process but
overlook the decision-making process. This paper investigates decision-making
in fine-grained emotion recognition through prototype theory. We show that ICL
relies on similarity matching between query representations and emotional
prototypes within the model, where emotion-accurate representations are
critical. However, semantically similar examples often introduce emotional
discrepancies, hindering accurate representations and causing errors. To
address this, we propose Emotion In-Context Learning (EICL), which introduces
emotionally similar examples and uses a dynamic soft-label strategy to improve
query representations in the emotion reasoning process. A two-stage exclusion
strategy is then employed to assess similarity from multiple angles, further
optimizing the decision-making process. Extensive experiments show that EICL
significantly outperforms ICL on multiple datasets.

</details>


### [268] [Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support](https://arxiv.org/abs/2510.06674)
*Cen,Zhao,Tiantian Zhang,Hanchen Su,Yufeng,Zhang,Shaowei Su,Mingzhi Xu,Yu,Liu,Wei Han,Jeremy Werner,Claire Na Cheng,Yashar Mehdad*

Main category: cs.AI

TL;DR: 提出了Agent-in-the-Loop（AITL）框架，通过集成四种实时反馈机制（响应偏好、代理采用和理由、知识相关性检查、缺失知识识别）来实现持续数据飞轮，以迭代改进基于LLM的客户支持系统，将模型再训练周期从数月缩短至数周。


<details>
  <summary>Details</summary>
Motivation: 传统的客户支持系统依赖于批量标注，导致模型迭代缓慢。需要一种能够将人工反馈直接整合到实时客户运营中，以加速LLM客户支持系统改进的框架。

Method: 提出Agent-in-the-Loop（AITL）框架，该框架集成了四种关键的人工标注类型：(1) 成对响应偏好，(2) 代理采用和理由，(3) 知识相关性检查，(4) 缺失知识识别。这些反馈信号被直接用于模型的更新，形成一个持续的数据飞轮。

Result: 在与美国客户支持代理进行的生产试点中，AITL框架显著提高了检索准确性（+11.7% recall@75, +14.8% precision@8）、生成质量（+8.4% helpfulness）和代理采用率（+4.5%）。

Conclusion: 将人工反馈循环直接嵌入运营工作流程中，能够有效地持续优化基于LLM的客户支持系统。

Abstract: We introduce an Agent-in-the-Loop (AITL) framework that implements a
continuous data flywheel for iteratively improving an LLM-based customer
support system. Unlike standard offline approaches that rely on batch
annotations, AITL integrates four key types of annotations directly into live
customer operations: (1) pairwise response preferences, (2) agent adoption and
rationales, (3) knowledge relevance checks, and (4) identification of missing
knowledge. These feedback signals seamlessly feed back into models' updates,
reducing retraining cycles from months to weeks. Our production pilot involving
US-based customer support agents demonstrated significant improvements in
retrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality
(+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore
the effectiveness of embedding human feedback loops directly into operational
workflows to continuously refine LLM-based customer support system.

</details>


### [269] [Inefficiencies of Meta Agents for Agent Design](https://arxiv.org/abs/2510.06711)
*Batu El,Mert Yuksekgonul,James Zou*

Main category: cs.AI

TL;DR: 本研究探讨了用于自动化设计智能体系统的元智能体的三个关键挑战，发现简单的上下文扩展不如进化方法有效，设计出的智能体行为多样性不足，且仅在少数情况下经济效益高于人工设计。


<details>
  <summary>Details</summary>
Motivation: 自动化设计智能体系统是近年来的研究热点，但元智能体在学习、行为多样性和经济可行性方面存在挑战。

Method: 研究了元智能体在学习过程中跨迭代的性能，比较了上下文扩展与进化方法的效果；分析了测试时单一智能体选择带来的行为多样性限制；评估了自动化设计相对于人工设计的经济效益。

Result: 在学习方面，进化方法优于简单的上下文扩展；设计的智能体行为多样性较低；仅在少数特定数据集和大规模部署时，自动化设计的成本低于人工设计。

Conclusion: 元智能体在自动化设计智能体系统方面仍面临挑战，尤其是在学习策略、提升行为多样性和实现经济效益方面，需要进一步的研究和改进。

Abstract: Recent works began to automate the design of agentic systems using
meta-agents that propose and iteratively refine new agent architectures. In
this paper, we examine three key challenges in a common class of meta-agents.
First, we investigate how a meta-agent learns across iterations and find that
simply expanding the context with all previous agents, as proposed by previous
works, performs worse than ignoring prior designs entirely. We show that the
performance improves with an evolutionary approach. Second, although the
meta-agent designs multiple agents during training, it typically commits to a
single agent at test time. We find that the designed agents have low behavioral
diversity, limiting the potential for their complementary use. Third, we assess
when automated design is economically viable. We find that only in a few
cases--specifically, two datasets--the overall cost of designing and deploying
the agents is lower than that of human-designed agents when deployed on over
15,000 examples. In contrast, the performance gains for other datasets do not
justify the design cost, regardless of scale.

</details>


### [270] [MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2510.06742)
*Ali Sarabadani,Kheirolah Rahsepar Fard*

Main category: cs.AI

TL;DR: LLMs被用于整合生物医学和认知科学中的知识图谱，创建了一个名为MultiCNKG的融合框架，整合了认知神经科学知识图谱（CNKG）、基因本体论（GO）和疾病本体论（DO），并利用GPT-4进行实体对齐、语义相似性计算和图谱增强，最终生成了一个包含6.9K节点和11.3K边的多层级知识图谱，在多项评估指标上表现出色，并在链接预测任务中具有竞争力，有望推动个性化医疗、认知障碍诊断和假设生成等领域的发展。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在捕捉基因、疾病和认知过程之间复杂的语义联系方面存在局限性，LLMs的出现为克服这些局限性提供了新的途径，旨在整合多个知识源以创建更全面、互联的知识图谱。

Method: 整合了认知神经科学知识图谱（CNKG）、基因本体论（GO）和疾病本体论（DO）三个知识源。利用GPT-4等LLMs进行实体对齐、语义相似性计算和图谱增强，构建了一个名为MultiCNKG的融合知识图谱。

Result: 生成了一个名为MultiCNKG的融合知识图谱，包含6.9K节点（基因、疾病、认知过程等）和11.3K边（如‘引起’、‘相关’、‘调控’等）。在准确率（85.20%）、召回率（87.30%）、覆盖率（92.18%）、图谱一致性（82.50%）、新颖性检测（40.28%）和专家验证（89.50%）等评估指标上表现稳健。在链接预测任务中，TransE和RotatE模型表现出与基准模型相当的性能。

Conclusion: MultiCNKG是一个融合了多个知识源的综合性知识图谱，利用LLMs克服了传统方法的局限性，在准确性、覆盖率和一致性方面表现出色，并在链接预测任务中具有竞争力，为个性化医疗、认知障碍诊断和认知神经科学的假设生成等应用提供了强大的支持。

Abstract: The advent of large language models (LLMs) has revolutionized the integration
of knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming
limitations in traditional machine learning methods for capturing intricate
semantic links among genes, diseases, and cognitive processes. We introduce
MultiCNKG, an innovative framework that merges three key knowledge sources: the
Cognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges
across 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes
and 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO)
comprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types.
Leveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity
computation, and graph augmentation to create a cohesive KG that interconnects
genetic mechanisms, neurological disorders, and cognitive functions. The
resulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes,
Diseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes,
Associated with, Regulates), facilitating a multi-layered view from molecular
to behavioral domains. Assessments using metrics such as precision (85.20%),
recall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty
detection (40.28%), and expert validation (89.50%) affirm its robustness and
coherence. Link prediction evaluations with models like TransE (MR: 391, MRR:
0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against
benchmarks like FB15k-237 and WN18RR. This KG advances applications in
personalized medicine, cognitive disorder diagnostics, and hypothesis
formulation in cognitive neuroscience.

</details>


### [271] [Verifying Memoryless Sequential Decision-making of Large Language Models](https://arxiv.org/abs/2510.06756)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: 本研究提出了一种用于验证大型语言模型（LLM）策略的工具，该工具能够处理记忆性状态转移任务。


<details>
  <summary>Details</summary>
Motivation: 为了对大型语言模型（LLM）在无记忆顺序决策任务中的策略进行严格且自动化的验证。

Method: 通过逐步构建MDP的可达部分，并将状态编码为自然语言提示，LLM的响应被解析为动作，并扩展可达的后继状态。然后使用Storm检查模型是否满足安全属性。

Result: 在标准的网格世界基准测试中，实验表明通过Ollama访问的开源LLM在确定性种子下可以被验证，但总体表现不如深度强化学习基线。

Conclusion: 该工具与Ollama原生集成，支持PRISM指定的任务，能够对用户指定的顺序决策任务进行持续基准测试，为形式化验证日益强大的LLM奠定了实践基础。

Abstract: We introduce a tool for rigorous and automated verification of large language
model (LLM)- based policies in memoryless sequential decision-making tasks.
Given a Markov decision process (MDP) representing the sequential
decision-making task, an LLM policy, and a safety requirement expressed as a
PCTL formula, our approach incrementally constructs only the reachable portion
of the MDP guided by the LLM's chosen actions. Each state is encoded as a
natural language prompt, the LLM's response is parsed into an action, and
reachable successor states by the policy are expanded. The resulting formal
model is checked with Storm to determine whether the policy satisfies the
specified safety property. In experiments on standard grid world benchmarks, we
show that open source LLMs accessed via Ollama can be verified when
deterministically seeded, but generally underperform deep reinforcement
learning baselines. Our tool natively integrates with Ollama and supports
PRISM-specified tasks, enabling continuous benchmarking in user-specified
sequential decision-making tasks and laying a practical foundation for formally
verifying increasingly capable LLMs.

</details>


### [272] [Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration](https://arxiv.org/abs/2510.06761)
*Zhi Zhang,Yan Liu,Zhejing Hu,Gong Chen,Sheng-hua Zhong,Jiannong Cao*

Main category: cs.AI

TL;DR: DLMA框架通过双循环多智能体协同，实现了自动化科研流程，在研究计划演进和执行中均表现出色，并在多个基准测试中达到SOTA。实验证明，演化保证了新颖性，执行保证了正确性。


<details>
  <summary>Details</summary>
Motivation: 自动化端到端的科学研究流程，需要解决高层计划的新颖性、正确性以及在动态不确定条件下的正确执行问题。

Method: 提出了一种新颖的双循环多智能体（DLMA）框架。‘领导者循环’由‘教授智能体’组成，负责演进研究计划，通过‘参与、改进、整合’会议，利用进化算法迭代生成和优化研究提案池。‘跟随者循环’由‘博士生智能体’组成，负责执行计划，通过‘事前和事后’会议动态调整执行过程，确保每一步都有上下文和外部观察支持。

Result: 在ACLAward和Laboratory等基准测试上，DLMA生成的论文在自动化评估中取得了SOTA分数，显著优于强基线。消融研究证实了两个循环的关键作用：演化驱动新颖性，执行确保正确性。

Conclusion: DLMA框架成功地自动化了端到端的科学研究过程，通过领导者循环的计划演进和跟随者循环的计划执行，有效解决了研究中的新颖性和正确性挑战。

Abstract: Automating the end-to-end scientific research process poses a fundamental
challenge: it requires both evolving high-level plans that are novel and sound,
and executing these plans correctly amidst dynamic and uncertain conditions. To
address this bilevel challenge, we propose a novel Double-Loop Multi-Agent
(DLMA) framework to solve the given research problem automatically. The leader
loop, composed of professor agents, is responsible for evolving research plans.
It employs an evolutionary algorithm through involvement, improvement, and
integration meetings to iteratively generate and refine a pool of research
proposals, exploring the solution space effectively. The follower loop,
composed of doctoral student agents, is responsible for executing the
best-evolved plan. It dynamically adjusts the plan during implementation via
pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is
well-supported by contextual and external observations. Extensive experiments
on benchmarks like ACLAward and Laboratory show that DLMA generates research
papers that achieve state-of-the-art scores in automated evaluation,
significantly outperforming strong baselines. Ablation studies confirm the
critical roles of both loops, with evolution driving novelty and execution
ensuring soundness.

</details>


### [273] [Autoformalizer with Tool Feedback](https://arxiv.org/abs/2510.06857)
*Qi Guo,Jianing Wang,Jianfei Zhang,Deyang Kong,Xiangzhou Huang,Xiangyu Xi,Wei Wang,Jingang Wang,Xunliang Cai,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 提出一种名为ATF的新方法，通过整合语法和一致性工具反馈来改进数学定理证明中的自动形式化过程，显著提高了生成语句的有效性和一致性，并发布了一个包含750K合成形式化语句的数据集Numina-ATF。


<details>
  <summary>Details</summary>
Motivation: 现有的自动形式化模型在生成满足语法有效性和语义一致性的形式化语句方面仍然存在困难。

Method: 提出了一种名为ATF（Autoformalizer with Tool Feedback）的新方法，该方法将Lean 4编译器用于语法纠正，并采用多LLM-as-judge方法进行一致性验证，通过工具反馈自适应地优化生成的语句。

Result: 实验结果表明，ATF在语法有效性和语义一致性方面显著优于一系列基线形式化模型，并在推理扩展性方面表现出色。研究还发布了Numina-ATF数据集，包含750K合成形式化语句。

Conclusion: ATF通过整合工具反馈（如Lean 4编译器和多LLM-as-judge）有效提高了自动形式化的质量，解决了现有方法的局限性，并在相关研究领域做出了贡献。

Abstract: Autoformalization addresses the scarcity of data for Automated Theorem
Proving (ATP) by translating mathematical problems from natural language into
formal statements. Efforts in recent work shift from directly prompting large
language models to training an end-to-end formalizer model from scratch,
achieving remarkable advancements. However, existing formalizer still struggles
to consistently generate valid statements that meet syntactic validity and
semantic consistency. To address this issue, we propose the Autoformalizer with
Tool Feedback (ATF), a novel approach that incorporates syntactic and
consistency information as tools into the formalization process. By integrating
Lean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge
approach for consistency validation, the model is able to adaptively refine
generated statements according to the tool feedback, enhancing both syntactic
validity and semantic consistency. The training of ATF involves a cold-start
phase on synthetic tool-calling data, an expert iteration phase to improve
formalization capabilities, and Direct Preference Optimization to alleviate
ineffective revisions. Experimental results show that ATF markedly outperforms
a range of baseline formalizer models, with its superior performance further
validated by human evaluations. Subsequent analysis reveals that ATF
demonstrates excellent inference scaling properties. Moreover, we open-source
Numina-ATF, a dataset containing 750K synthetic formal statements to facilitate
advancements in autoformalization and ATP research.

</details>


### [274] [TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs](https://arxiv.org/abs/2510.06878)
*Daria Ozerova,Ekaterina Trofimova*

Main category: cs.AI

TL;DR: 迭代优化是一种有前景的范式，可以使大型语言模型（LLM）能够解决困难的推理和问题解决任务。然而，关键挑战之一是如何有效地搜索可能进行的优化所构成的巨大搜索空间。现有方法通常依赖预定义的启发式方法，但这些方法受到探索-利用困境的困扰，并且无法根据过去的优化结果进行调整。我们引入了树引导策略优化（TGPR），一个结合了GRPO和基于Thompson采样的树搜索的新框架。TGPR积极探索失败和成功的优化路径，提供更密集的训练轨迹和更自适应的策略。在HumanEval、MBPP和APPS基准测试中，我们的方法在pass@1（在MBPP上）上实现了高达+4.2个百分点的绝对提升，在pass@10（在APPS上）上实现了高达+12.51个百分点的绝对提升，与有竞争力的GRPO基线相比。除了调试代码，TGPR还侧重于一种原则性的方法，将学习到的策略与结构化搜索方法相结合，为增强LLM中的迭代优化和有状态推理提供了一个通用框架。


<details>
  <summary>Details</summary>
Motivation: 迭代优化是一种有前景的范式，可以使大型语言模型（LLM）能够解决困难的推理和问题解决任务。然而，关键挑战之一是如何有效地搜索可能进行的优化所构成的巨大搜索空间。现有方法通常依赖预定义的启发式方法，但这些方法受到探索-利用困境的困境，并且无法根据过去的优化结果进行调整。

Method: 我们引入了树引导策略优化（TGPR），一个结合了GRPO和基于Thompson采样的树搜索的新框架。TGPR积极探索失败和成功的优化路径，提供更密集的训练轨迹和更自适应的策略。

Result: 在HumanEval、MBPP和APPS基准测试中，我们的方法在pass@1（在MBPP上）上实现了高达+4.2个百分点的绝对提升，在pass@10（在APPS上）上实现了高达+12.51个百分点的绝对提升，与有竞争力的GRPO基线相比。

Conclusion: 除了调试代码，TGPR还侧重于一种原则性的方法，将学习到的策略与结构化搜索方法相结合，为增强LLM中的迭代优化和有状态推理提供了一个通用框架。

Abstract: Iterative refinement has been a promising paradigm to enable large language
models (LLMs) to resolve difficult reasoning and problem-solving tasks. One of
the key challenges, however, is how to effectively search through the enormous
search space of possible refinements. Existing methods typically fall back on
predefined heuristics, which are troubled by the exploration-exploitation
dilemma and cannot adapt based on past refinement outcomes. We introduce
Tree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with
a Thompson-Sampling-based tree search. TGPR explores both failed and successful
refinement paths actively, with denser training trajectories and more adaptive
policies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to
+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to
+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to
a competitive GRPO baseline. Apart from debugging code, TGPR focuses on a
principled approach to combining learned policies with structured search
methods, offering a general framework for enhancing iterative refinement and
stateful reasoning in LLMs.

</details>


### [275] [LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN](https://arxiv.org/abs/2510.06911)
*Hacane Hechehouche,Andre Antakli,Matthias Klusch*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: There are many established semantic Web standards for implementing
multi-agent driven applications. The AJAN framework allows to engineer
multi-agent systems based on these standards. In particular, agent knowledge is
represented in RDF/RDFS and OWL, while agent behavior models are defined with
Behavior Trees and SPARQL to access and manipulate this knowledge. However, the
appropriate definition of RDF/RDFS and SPARQL-based agent behaviors still
remains a major hurdle not only for agent modelers in practice. For example,
dealing with URIs is very error-prone regarding typos and dealing with complex
SPARQL queries in large-scale environments requires a high learning curve. In
this paper, we present an integrated development environment to overcome such
hurdles of modeling AJAN agents and at the same time to extend the user
community for AJAN by the possibility to leverage Large Language Models for
agent engineering.

</details>


### [276] [Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.06953)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.AI

TL;DR: UID假设应用于LLM推理，发现信息密度在推理过程中保持稳定可以提高准确性。


<details>
  <summary>Details</summary>
Motivation: UID假设认为有效沟通应保持信息流的稳定。本研究旨在探讨该原理在大型语言模型（LLM）推理过程中的步级信息密度是否反映了推理质量。

Method: 提出了一种基于熵的步级信息密度度量方法，并引入了局部和全局信息密度均匀性得分。通过在六个推理基准上进行实验来评估这些度量。

Result: 研究发现，步级信息密度均匀性可以提高准确性，在AIME2025数据集上相对基线有10-32%的提升。正确推理过程倾向于避免信息密度突然升高，而错误推理过程则表现出不规则的信息爆发。UID启发的信息密度度量优于其他内部信号，能有效预测推理质量。

Conclusion: UID启发的信息密度度量可以作为评估和选择高质量LLM推理过程的可靠标准，有助于构建更准确、更可靠的推理系统。

Abstract: The Uniform Information Density (UID) hypothesis suggests that effective
communication maintains a stable flow of information. In this work, we revisit
this principle in the context of large language model (LLM) reasoning traces,
asking whether step-level uniformity reflects reasoning quality. To this end,
we propose an entropy-based stepwise information density metric and introduce
two complementary measures of uniformity, local and global uniformity scores.
Across the experiments on six different reasoning benchmarks, we find that
step-level uniformity not only provides a strong theoretical lens but also
yields practical performance benefits; for example, selecting reasoning traces
with more uniform information density at the step-level improves accuracy by
10-32\% relative gains over baselines at AIME2025. Our analysis further reveals
that correct reasoning traces tend to avoid sharp information density spikes,
while incorrect traces exhibit irregular information bursts. These results
demonstrate that UID-inspired information density measures outperform
alternative internal signals as predictors of reasoning quality. Results
highlight the uniformity of the information density as a robust diagnostic and
selection criterion for building more reliable and accurate reasoning systems.

</details>


### [277] [Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning](https://arxiv.org/abs/2510.07038)
*Wenxun Wu,Yuanyang Li,Guhan Chen,Linyue Wang,Hongyang Chen*

Main category: cs.AI

TL;DR: TAPO是一个结合了多步推理和工具调用的强化学习框架，用于解决需要最新知识或计算工具（如计算器、代码解释器）的数学推理任务。它通过修改DAPO（动态采样策略优化）来实现，并引入了两个新数据集（TAPO-easy-60K和TAPO-hard-18K）来训练和评估模型。实验表明，TAPO在Qwen2.5模型上取得了最先进的性能，并能更有效地利用工具，避免了过多的调用。


<details>
  <summary>Details</summary>
Motivation: 直接推理的语言模型在处理需要最新知识或计算工具（如计算器、代码解释器）的数学推理任务时存在局限性。

Method: 提出了一种名为TAPO（Tool-Augmented Policy Optimization）的新型强化学习框架，该框架将多步推理与自适应的工具调用能力相结合。TAPO修改了动态采样策略优化（DAPO），使其能够动态地交织复杂推理和按需工具使用（包括搜索API和Python解释器）。此外，还引入了两个新数据集：TAPO-easy-60K和TAPO-hard-18K。

Result: 在Qwen2.5-3B和Qwen2.5-7B模型上的实验表明，TAPO在需要外部知识和数学计算的任务上取得了最先进的性能，并且比基线方法更有效地利用工具，同时防止了由于奖励黑客行为导致的过度调用。

Conclusion: 将先进的推理与工具使用相结合，在增强模型在知识密集型和计算密集型任务方面的性能方面具有巨大潜力。

Abstract: Recent advances in large language models (LLMs) have popularized test-time
scaling, where models generate additional reasoning tokens before producing
final answers. These approaches have demonstrated significant performance
improvements on benchmarks involving mathematical reasoning. However, language
models relying solely on direct inference still struggle with tasks demanding
up-to-date knowledge or computational tools such as calculators and code
interpreters for complex arithmetic operations. To overcome these limitations,
we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement
learning framework that systematically integrates multi-hop reasoning with
adaptive tool-calling capabilities. Our approach employs a modified version of
Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,
which we adapt specifically for tool invocation scenarios, enabling models to
dynamically interleave complex reasoning with on-demand tool usage (including
search APIs and Python interpreters).
  To support this research, we introduce two new datasets: TAPO-easy-60K and
TAPO-hard-18K, specifically designed to train and evaluate both fact-based
reasoning and mathematical calculation capabilities. Our experiments on
Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,
with both models achieving state-of-the-art performance on tasks requiring
external knowledge and mathematical computation among methods with comparable
parameters. Notably, TAPO achieves more efficient tool utilization than
baseline methods while preventing excessive calls caused by reward hacking.
These results highlight the significant potential of combining advanced
reasoning with tool usage to enhance model performance in knowledge-intensive
and computationally demanding tasks.

</details>


### [278] [Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations](https://arxiv.org/abs/2510.07064)
*Manh Hung Nguyen,Sebastian Tschiatschek,Adish Singla*

Main category: cs.AI

TL;DR: LLMs can be a proxy for human behavior but often lack diversity. This paper proposes a framework using a set of LLM agents, each conditioned on human demonstrations, to collectively capture population diversity. The selection of these agents is framed as a submodular optimization problem, with methods offering different trade-offs in complexity and performance. Experiments show this approach effectively represents human populations and reproduces their behavior patterns.


<details>
  <summary>Details</summary>
Motivation: LLMs are an attractive alternative to humans due to the difficulty and expense of obtaining large-scale human responses, but they often produce homogeneous outputs that lack diversity. The motivation is to create a set of LLM agents that can collectively capture the diversity of human perspectives and behaviors.

Method: The proposed framework constructs a set of LLM agents, where each agent's behavior is steered by conditioning on a small set of human demonstrations (task-response pairs) through in-context learning. The selection of a representative set of agents from a large space is addressed using submodular optimization techniques. Methods are developed to balance time complexity and performance guarantees.

Result: Extensive experiments in crowdsourcing and educational domains show that the proposed approach constructs agents that more effectively represent human populations compared to baselines. Behavioral analyses on new tasks confirm that these agents reproduce the behavior patterns and perspectives of the target human populations (students and annotators).

Conclusion: The framework effectively addresses the challenge of LLM output diversity by creating a set of agents that collectively represent a human population. The use of submodular optimization for agent selection is shown to be effective, outperforming baselines and successfully reproducing human behavior patterns.

Abstract: The difficulty and expense of obtaining large-scale human responses make
Large Language Models (LLMs) an attractive alternative and a promising proxy
for human behavior. However, prior work shows that LLMs often produce
homogeneous outputs that fail to capture the rich diversity of human
perspectives and behaviors. Thus, rather than trying to capture this diversity
with a single LLM agent, we propose a novel framework to construct a set of
agents that collectively capture the diversity of a given human population.
Each agent is an LLM whose behavior is steered by conditioning on a small set
of human demonstrations (task-response pairs) through in-context learning. The
central challenge is therefore to select a representative set of LLM agents
from the exponentially large space of possible agents. We tackle this selection
problem from the lens of submodular optimization. In particular, we develop
methods that offer different trade-offs regarding time complexity and
performance guarantees. Extensive experiments in crowdsourcing and educational
domains demonstrate that our approach constructs agents that more effectively
represent human populations compared to baselines. Moreover, behavioral
analyses on new tasks show that these agents reproduce the behavior patterns
and perspectives of the students and annotators they are designed to represent.

</details>


### [279] [Inductive Learning for Possibilistic Logic Programs Under Stable Models](https://arxiv.org/abs/2510.07069)
*Hongbo Hu,Yisong Wang,Yi Huang,Kewen Wang*

Main category: cs.AI

TL;DR: 本文提出了一种从背景程序和示例中提取可能逻辑程序（poss-programs）的方法，并实现了相应的算法，实验结果表明其在处理普通逻辑程序时优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有关于可能逻辑程序（poss-programs）的研究主要集中在语义和性质上，而忽略了归纳推理问题。本文旨在解决这一空白，提出一种从背景程序和示例中提取poss-programs的方法。

Method: 本文定义了归纳任务的概念，并提出了两种计算归纳解决方案的算法：ilpsm和ilpsmmin。其中ilpsmmin已被实现并进行了实验评估。

Result: 实验结果表明，当输入为普通逻辑程序时，本文实现的ilpsmmin算法在随机生成的数据集上，其性能优于一个主要的用于普通逻辑程序归纳学习的系统。

Conclusion: 本文成功地构建了一个从背景程序和示例中提取poss-programs的归纳推理方法，并提供了有效的算法实现。实验证明了该方法在处理普通逻辑程序时的优越性。

Abstract: Possibilistic logic programs (poss-programs) under stable models are a major
variant of answer set programming (ASP). While its semantics (possibilistic
stable models) and properties have been well investigated, the problem of
inductive reasoning has not been investigated yet. This paper presents an
approach to extracting poss-programs from a background program and examples
(parts of intended possibilistic stable models). To this end, the notion of
induction tasks is first formally defined, its properties are investigated and
two algorithms ilpsm and ilpsmmin for computing induction solutions are
presented. An implementation of ilpsmmin is also provided and experimental
results show that when inputs are ordinary logic programs, the prototype
outperforms a major inductive learning system for normal logic programs from
stable models on the datasets that are randomly generated.

</details>


### [280] [VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems](https://arxiv.org/abs/2510.07073)
*André Hottung,Federico Berto,Chuanbo Hua,Nayeli Gast Zepeda,Daniel Wetzel,Michael Römer,Haoran Ye,Davide Zago,Michael Poli,Stefano Massaroli,Jinkyoo Park,Kevin Tierney*

Main category: cs.AI

TL;DR: VRPAgent是一个框架，它将LLM生成的组件集成到元启发式算法中，并通过新的遗传搜索进行优化，以解决车辆路径问题。


<details>
  <summary>Details</summary>
Motivation: 设计高性能的车辆路径问题（VRP）启发式方法是一项复杂的任务，需要直觉和深厚的领域知识。尽管大型语言模型（LLM）在代码生成方面显示出潜力，但其生成的启发式方法仍无法与人类专家精心设计的相媲美。

Method: VRPAgent将LLM生成的组件集成到元启发式算法中，并通过一种新颖的遗传搜索进行优化。LLM用于生成特定于问题的算子，并嵌入通用的元启发式框架中。

Result: 在多个问题（包括带容量的VRP、带时间窗口的VRP和奖金收集VRP）上，VRPAgent发现的启发式算子优于手工设计的方法和最近的基于学习的方法，并且仅需单个CPU核心。

Conclusion: VRPAgent是首个在VRP领域取得最先进成果的基于LLM的范式，展示了自动化启发式方法发现的美好前景。

Abstract: Designing high-performing heuristics for vehicle routing problems (VRPs) is a
complex task that requires both intuition and deep domain knowledge. Large
language model (LLM)-based code generation has recently shown promise across
many domains, but it still falls short of producing heuristics that rival those
crafted by human experts. In this paper, we propose VRPAgent, a framework that
integrates LLM-generated components into a metaheuristic and refines them
through a novel genetic search. By using the LLM to generate problem-specific
operators, embedded within a generic metaheuristic framework, VRPAgent keeps
tasks manageable, guarantees correctness, and still enables the discovery of
novel and powerful strategies. Across multiple problems, including the
capacitated VRP, the VRP with time windows, and the prize-collecting VRP, our
method discovers heuristic operators that outperform handcrafted methods and
recent learning-based approaches while requiring only a single CPU core. To our
knowledge, \VRPAgent is the first LLM-based paradigm to advance the
state-of-the-art in VRPs, highlighting a promising future for automated
heuristics discovery.

</details>


### [281] [The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas](https://arxiv.org/abs/2510.07091)
*Baixuan Xu,Tianshi Zheng,Zhaowei Wang,Hong Ting Tsang,Weiqi Wang,Tianqing Fang,Yangqiu Song*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Enabling LLMs to effectively operate long-horizon task which requires
long-term planning and multiple interactions is essential for open-world
autonomy. Conventional methods adopt planning with actions where a executable
action list would be provided as reference. However, this action representation
choice would be impractical when the environment action space is combinatorial
exploded (e.g., open-ended real world). This naturally leads to a question: As
environmental action space scales, what is the optimal action representation
for long-horizon agents? In this paper, we systematically study the
effectiveness of two different action representations. The first one is
conventional planning with actions (PwA) which is predominantly adopted for its
effectiveness on existing benchmarks. The other one is planning with schemas
(PwS) which instantiate an action schema into action lists (e.g., "move [OBJ]
to [OBJ]" -> "move apple to desk") to ensure concise action space and reliable
scalability. This alternative is motivated by its alignment with human
cognition and its compliance with environment-imposed action format
restriction. We propose cognitive bandwidth perspective as a conceptual
framework to qualitatively understand the differences between these two action
representations and empirically observe a representation-choice inflection
point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve
as evidence of the need for scalable representations. We further conduct
controlled experiments to study how the location of this inflection point
interacts with different model capacities: stronger planning proficiency shifts
the inflection rightward, whereas better schema instantiation shifts it
leftward. Finally, noting the suboptimal performance of PwS agents, we provide
an actionable guide for building more capable PwS agents for better scalable
autonomy.

</details>


### [282] [The Contingencies of Physical Embodiment Allow for Open-Endedness and Care](https://arxiv.org/abs/2510.07117)
*Leonardo Christov-Moore,Arthur Juliani,Alex Kiefer,Nicco Reggente,B. Scott Rousse,Adam Safron,Nicol'as Hinrichs,Daniel Polani,Antonio Damasio*

Main category: cs.AI

TL;DR: 受海德格尔和尼采哲学的启发，本文提出“在世界存在”和“朝向死亡存在”的两个基本物理实体条件，并以此为基础构建了具有稳态驱动和权力意志驱动的强化学习模型，旨在探索能够适应开放世界并提供关怀的机器体。


<details>
  <summary>Details</summary>
Motivation: 物理脆弱性和个体生存是发展机器体时需要克服的难题，而生物体能轻松适应开放世界并相互关怀。理解生命存在的条件差异，有助于创造更强大、更具适应性和关怀能力的机器体。

Method: 提出受海德格尔“在世界存在”和“朝向死亡存在”启发的两个物理实体化条件，认为这些条件可以产生维持自身完整性和避免死亡的稳态驱动，以及维持这种状态的内在驱动。借鉴尼采“权力意志”概念，通过最大化对未来状态的控制（例如增强能力）来提高机器体满足未来稳态需求的能力，从而增强其维持物理完整性的能力。在强化学习框架下形式化这些概念，以研究内在驱动的实体化机器体在开放多体环境中如何培养开放性和关怀能力。

Result: 在强化学习框架下，通过形式化“在世界存在”、“朝向死亡存在”、“稳态驱动”和“权力意志驱动”等概念，为研究内在驱动的实体化机器体在开放多体环境中如何培养开放性和关怀能力提供了理论基础和计算模型。

Conclusion: 通过借鉴存在主义哲学，可以为机器体的发展提供新的视角，特别是通过引入物理实体化和内在驱动的概念，有可能创造出更强大、更具适应性和关怀能力的机器体。这为人工智能领域的研究开辟了新的方向。

Abstract: Physical vulnerability and mortality are often seen as obstacles to be
avoided in the development of artificial agents, which struggle to adapt to
open-ended environments and provide aligned care. Meanwhile, biological
organisms survive, thrive, and care for each other in an open-ended physical
world with relative ease and efficiency. Understanding the role of the
conditions of life in this disparity can aid in developing more robust,
adaptive, and caring artificial agents. Here we define two minimal conditions
for physical embodiment inspired by the existentialist phenomenology of Martin
Heidegger: being-in-the-world (the agent is a part of the environment) and
being-towards-death (unless counteracted, the agent drifts toward terminal
states due to the second law of thermodynamics). We propose that from these
conditions we can obtain both a homeostatic drive - aimed at maintaining
integrity and avoiding death by expending energy to learn and act - and an
intrinsic drive to continue to do so in as many ways as possible. Drawing
inspiration from Friedrich Nietzsche's existentialist concept of will-to-power,
we examine how intrinsic drives to maximize control over future states, e.g.,
empowerment, allow agents to increase the probability that they will be able to
meet their future homeostatic needs, thereby enhancing their capacity to
maintain physical integrity. We formalize these concepts within a reinforcement
learning framework, which enables us to examine how intrinsically driven
embodied agents learning in open-ended multi-agent environments may cultivate
the capacities for open-endedness and care.ov

</details>


### [283] [Integrating Domain Knowledge into Process Discovery Using Large Language Models](https://arxiv.org/abs/2510.07161)
*Ali Norouzifar,Humam Kourani,Marcus Dees,Wil van der Aalst*

Main category: cs.AI

TL;DR: 该研究提出了一种利用大型语言模型（LLM）结合领域知识来改进事件日志过程发现的方法，以生成更可靠的过程模型。


<details>
  <summary>Details</summary>
Motivation: 事件日志过程发现模型可能因数据不完整或存在噪声而无法准确反映实际过程，并且通常忽略了重要的领域知识。本研究旨在解决这一问题，提高发现模型的可靠性。

Method: 提出一个交互式框架，利用LLM从领域专家提供的自然语言描述中提取声明性规则，并指导IMr算法，该算法结合事件日志和提取的规则来递归地构建过程模型，避免与领域知识相冲突的结构。

Result: 开发了一个支持该工作流程的完整工具，并进行了广泛的评估，包括一个涉及领域专家的真实生活案例研究，以评估框架的可用性和有效性。

Conclusion: 该框架通过整合领域知识，利用LLM来指导过程发现，可以生成更准确、更可靠的过程模型，克服仅依赖事件日志的局限性。

Abstract: Process discovery aims to derive process models from event logs, providing
insights into operational behavior and forming a foundation for conformance
checking and process improvement. However, models derived solely from event
data may not accurately reflect the real process, as event logs are often
incomplete or affected by noise, and domain knowledge, an important
complementary resource, is typically disregarded. As a result, the discovered
models may lack reliability for downstream tasks. We propose an interactive
framework that incorporates domain knowledge, expressed in natural language,
into the process discovery pipeline using Large Language Models (LLMs). Our
approach leverages LLMs to extract declarative rules from textual descriptions
provided by domain experts. These rules are used to guide the IMr discovery
algorithm, which recursively constructs process models by combining insights
from both the event log and the extracted rules, helping to avoid problematic
process structures that contradict domain knowledge. The framework coordinates
interactions among the LLM, domain experts, and a set of backend services. We
present a fully implemented tool that supports this workflow and conduct an
extensive evaluation of multiple LLMs and prompt engineering strategies. Our
empirical study includes a case study based on a real-life event log with the
involvement of domain experts, who assessed the usability and effectiveness of
the framework.

</details>


### [284] [NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents](https://arxiv.org/abs/2510.07172)
*Tianshi Zheng,Kelvin Kiu-Wai Tam,Newt Hue-Nam K. Nguyen,Baixuan Xu,Zhaowei Wang,Jiayang Cheng,Hong Ting Tsang,Weiqi Wang,Jiaxin Bai,Tianqing Fang,Yangqiu Song,Ginny Y. Wong,Simon See*

Main category: cs.AI

TL;DR: LLMs在科学定律发现方面展现出潜力，但现有基准存在科学相关性、可扩展性和抗记忆性之间的权衡。我们提出了NewtonBench，一个包含324个物理定律发现任务的基准，通过形而上学的变化来解决评估困境，并引入交互式模型发现。实验表明，LLMs的发现能力是脆弱的，并且对复杂性和噪声敏感。工具辅助，如代码解释器，可能会阻碍某些模型。NewtonBench提供了一个真实的测试平台，用于评估和推动下一代科学发现AI代理的发展。


<details>
  <summary>Details</summary>
Motivation: 现有科学定律发现的基准存在方法论困境，需要在科学相关性、可扩展性和抗记忆性之间进行权衡，并且未能模拟真实的科学探索过程。我们需要一个能够解决这些问题并更真实地评估LLM在科学定律发现能力的新基准。

Method: 我们提出了NewtonBench，一个包含324个科学定律发现任务的基准，涵盖12个物理领域。我们使用形而上学的变化（对经典定律进行系统性修改）来生成可扩展、科学相关且抗记忆性的问题。此外，我们不再局限于静态函数拟合，而是引入交互式模型发现，要求智能体通过实验探测模拟的复杂系统来发现隐藏的定律。

Result: 现有的大型语言模型（LLMs）在科学定律发现方面展现出清晰但脆弱的能力。当系统复杂度增加或观测噪声增大时，其发现能力会急剧下降。提供代码解释器等工具辅助可能会适得其反，导致模型过早地从探索转向利用，从而满足于次优解。

Conclusion: 在复杂的交互式环境中实现强大且可泛化的科学定律发现仍然是一个核心挑战。NewtonBench提供了一个可扩展、鲁棒且科学上真实的测试平台，有助于衡量进展并指导下一代能够进行真正科学发现的AI代理的开发。

Abstract: Large language models are emerging as powerful tools for scientific law
discovery, a foundational challenge in AI-driven science. However, existing
benchmarks for this task suffer from a fundamental methodological trilemma,
forcing a trade-off between scientific relevance, scalability, and resistance
to memorization. Furthermore, they oversimplify discovery as static function
fitting, failing to capture the authentic scientific process of uncovering
embedded laws through the interactive exploration of complex model systems. To
address these critical gaps, we introduce NewtonBench, a benchmark comprising
324 scientific law discovery tasks across 12 physics domains. Our design
mitigates the evaluation trilemma by using metaphysical shifts - systematic
alterations of canonical laws - to generate a vast suite of problems that are
scalable, scientifically relevant, and memorization-resistant. Moreover, we
elevate the evaluation from static function fitting to interactive model
discovery, requiring agents to experimentally probe simulated complex systems
to uncover hidden principles. Our extensive experiment reveals a clear but
fragile capability for discovery in frontier LLMs: this ability degrades
precipitously with increasing system complexity and exhibits extreme
sensitivity to observational noise. Notably, we uncover a paradoxical effect of
tool assistance: providing a code interpreter can hinder more capable models by
inducing a premature shift from exploration to exploitation, causing them to
satisfice on suboptimal solutions. These results demonstrate that robust,
generalizable discovery in complex, interactive environments remains the core
challenge. By providing a scalable, robust, and scientifically authentic
testbed, NewtonBench offers a crucial tool for measuring true progress and
guiding the development of next-generation AI agents capable of genuine
scientific discovery.

</details>


### [285] [Agentic generative AI for media content discovery at the national football league](https://arxiv.org/abs/2510.07297)
*Henry Wang,Md Sirajus Salekin,Jake Lee,Ross Claytor,Shinan Zhang,Michael Chi*

Main category: cs.AI

TL;DR: Generative AI enables NFL media analysts to query historical game footage using natural language, achieving over 95% accuracy and reducing search time from 10 minutes to 30 seconds.


<details>
  <summary>Details</summary>
Motivation: To improve content discovery and management for media researchers and analysts by leveraging Generative AI for querying historical NFL plays.

Method: Developed an agentic workflow that takes natural language queries, breaks them into elements, and translates them into database queries, with semantic caching for improved accuracy and latency.

Result: Achieved over 95% accuracy and reduced the average time to find relevant videos from 10 minutes to 30 seconds.

Conclusion: The Generative AI-based workflow significantly increases the NFL's operational efficiency, allowing users to focus on creative content production and storytelling by providing a more intuitive and faster way to access historical data.

Abstract: Generative AI has unlocked new possibilities in content discovery and
management. Through collaboration with the National Football League (NFL), we
demonstrate how a generative-AI based workflow enables media researchers and
analysts to query relevant historical plays using natural language rather than
traditional filter-and-click interfaces. The agentic workflow takes a user
query as input, breaks it into elements, and translates them into the
underlying database query language. Accuracy and latency are further improved
through carefully designed semantic caching. The solution achieves over 95
percent accuracy and reduces the average time to find relevant videos from 10
minutes to 30 seconds, significantly increasing the NFL's operational
efficiency and allowing users to focus on producing creative content and
engaging storylines.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [286] [Reversible computations are computations](https://arxiv.org/abs/2510.06585)
*Clément Aubert,Jean Krivine*

Main category: cs.LO

TL;DR: 本文将因果性原则扩展到可逆计算领域，并提出了相应的数学模型和语义。


<details>
  <summary>Details</summary>
Motivation: 因果性对于并发系统至关重要，即使在事件可逆的情况下也是如此。

Method: 在配置结构模型中引入对称残差运算来处理可逆计算，并推导出事件结构的可逆计算语义。

Result: 证明了稳定的配置结构在残差运算下保持稳定，并且推导出的语义与切换操作一致。

Conclusion: 因果性原则对于理解和建模可逆并发计算同样重要。

Abstract: Causality serves as an abstract notion of time for concurrent systems. A
computation is causal, or simply valid, if each observation of a computation
event is preceded by the observation of its causes. The present work
establishes that this simple requirement is equally relevant when the
occurrence of an event is invertible. We propose a conservative extension of
causal models for concurrency that accommodates reversible computations. We
first model reversible computations using a symmetric residuation operation in
the general model of configuration structures. We show that stable
configuration structures, which correspond to prime algebraic domains, remain
stable under the action of this residuation. We then derive a semantics of
reversible computations for prime event structures, which is shown to coincide
with a switch operation that dualizes conflict and causality.

</details>


### [287] [Strong Dinatural Transformations and Generalised Codensity Monads](https://arxiv.org/abs/2510.06777)
*Maciej Piróg,Filip Sieczkowski*

Main category: cs.LO

TL;DR: dicodensity monads are a generalization of pointwise codensity monads generated by functors to monads generated by mixed-variant bifunctors. This construction uses strong dinaturality and is inspired by denotational models in polymorphic lambda calculi. The paper provides conditions for isomorphism between a monad and a dicodensity monad, and focuses on monads generated by hom-functors and bifunctors of homomorphisms, yielding new presentations of monads for semirings and ordered nondeterministic computations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to generalize pointwise codensity monads to a broader class of monads generated by mixed-variant bifunctors, inspired by denotational models in polymorphic lambda calculi, particularly for representing universally quantified variables and modeling ordered nondeterministic computations.

Method: The construction of dicodensity monads is based on the notion of strong dinaturality (Barr dinaturality). The paper establishes conditions for an isomorphism between a monad and a dicodensity monad for a given bifunctor. It then specializes the construction to hom-functors and bifunctors of homomorphisms.

Result: The paper introduces dicodensity monads, provides conditions for their isomorphism with other monads, and offers novel presentations of monads generated by semirings and bifunctors of homomorphisms, which are relevant for modeling ordered nondeterministic computations.

Conclusion: Dicodensity monads offer a generalized framework for constructing monads from bifunctors using strong dinaturality. This construction has applications in providing new perspectives on monads related to semirings and ordered nondeterministic computations, extending previous work on Cayley-style representations.

Abstract: We introduce dicodensity monads: a generalisation of pointwise codensity
monads generated by functors to monads generated by mixed-variant bifunctors.
Our construction is based on the notion of strong dinaturality (also known as
Barr dinaturality), and is inspired by denotational models of certain types in
polymorphic lambda calculi - in particular, a form of continuation monads with
universally quantified variables, such as the Church encoding of the list monad
in System F. Extending some previous results on Cayley-style representations,
we provide a set of sufficient conditions to establish an isomorphism between a
monad and the dicodensity monad for a given bifunctor. Then, we focus on the
class of monads obtained by instantiating our construction with hom-functors
and, more generally, bifunctors given by objects of homomorphisms (that is,
internalised hom-sets between Eilenberg--Moore algebras). This gives us, for
example, novel presentations of monads generated by different kinds of
semirings and other theories used to model ordered nondeterministic
computations.

</details>


### [288] [A simple proof of the coincidence of observational and labeled equivalence of processes in applied pi-calculus](https://arxiv.org/abs/2510.07258)
*Andrew M. Mironov*

Main category: cs.LO

TL;DR: 简化的证明，证明了应用pi演算中观察等价和标签等价概念的一致性。


<details>
  <summary>Details</summary>
Motivation: 提供一个更简单的方法来证明应用pi演算中的一个核心定理。

Method: 通过简化现有的证明方法来证明观察等价和标签等价概念的一致性。

Result: 成功地简化了现有证明，并验证了观察等价和标签等价概念在应用pi演算中的一致性。

Conclusion: 证明了应用pi演算中的观察等价和标签等价概念是相同的，并且提供了一个更简洁的证明方法。

Abstract: This paper presents a new, significantly simpler proof of one of the main
results of applied pi-calculus: the theorem that the concepts of observational
and labeled equivalence of extended processes in applied pi-calculus coincide.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [289] [Visualizing Multimodality in Combinatorial Search Landscapes](https://arxiv.org/abs/2510.06517)
*Xavier F. C. Sánchez-Díaz,Ole Jakob Mengshoel*

Main category: cs.GR

TL;DR: 本文探讨了组合搜索景观的可视化技术，特别是多模态性，并结合了景观分析文献中的不同技术，通过几何和美学元素展示了实际应用。结论是没有免费的午餐，并为未来工作提出了建议。


<details>
  <summary>Details</summary>
Motivation: 探讨组合搜索景观的可视化技术，特别是多模态性。

Method: 讨论了景观分析文献中的不同技术，以及如何将它们结合起来提供更全面的搜索景观视图，并结合了几何和美学元素。

Result: 展示了不同可视化技术的示例，并讨论了其他研究者如何将这些技术付诸实践。

Conclusion: 没有免费的午餐，并为该领域未来的工作提出了建议。

Abstract: This work walks through different visualization techniques for combinatorial
search landscapes, focusing on multimodality. We discuss different techniques
from the landscape analysis literature, and how they can be combined to provide
a more comprehensive view of the search landscape. We also include examples and
discuss relevant work to show how others have used these techniques in
practice, based on the geometric and aesthetic elements of the Grammar of
Graphics. We conclude that there is no free lunch in visualization, and provide
recommendations for future work as there are several paths to continue the work
in this field.

</details>


### [290] [Capture and Interact: Rapid 3D Object Acquisition and Rendering with Gaussian Splatting in Unity](https://arxiv.org/abs/2510.06802)
*Islomjon Shukhratov,Sergey Gorinsky*

Main category: cs.GR

TL;DR: 使用3D高斯泼溅技术，通过手机视频捕捉、云端处理和本地电脑渲染，实现了对现实世界物体进行快速采集和交互式渲染，平均渲染帧率为150fps。


<details>
  <summary>Details</summary>
Motivation: 在增强现实、数字孪生、远程协作和原型设计等应用中，实时捕捉和渲染3D物体仍然是一个重大挑战，但具有巨大的潜力。

Method: 提出一个端到端流程，利用3D高斯泼溅技术，结合手机拍摄、云端3D高斯泼溅处理和Unity渲染，实现实时采集和交互式渲染。

Result: 实验表明，该流程在GPU上处理扫描数据平均耗时约10分钟，并在笔记本电脑上实现了实时渲染，平均帧率为150fps。

Conclusion: 该系统整合了移动采集、云端3D高斯泼溅和Unity渲染，支持实时远程呈现。

Abstract: Capturing and rendering three-dimensional (3D) objects in real time remain a
significant challenge, yet hold substantial potential for applications in
augmented reality, digital twin systems, remote collaboration and prototyping.
We present an end-to-end pipeline that leverages 3D Gaussian Splatting (3D GS)
to enable rapid acquisition and interactive rendering of real-world objects
using a mobile device, cloud processing and a local computer. Users scan an
object with a smartphone video, upload it for automated 3D reconstruction, and
visualize it interactively in Unity at an average of 150 frames per second
(fps) on a laptop. The system integrates mobile capture, cloud-based 3D GS and
Unity rendering to support real-time telepresence. Our experiments show that
the pipeline processes scans in approximately 10 minutes on a graphics
processing unit (GPU) achieving real-time rendering on the laptop.

</details>


### [291] [Geometric Queries on Closed Implicit Surfaces for Walk on Stars](https://arxiv.org/abs/2510.07275)
*Tianyu Huang*

Main category: cs.GR

TL;DR: 提出了一种适用于沃斯特（WoSt）的闭合隐式曲面几何查询框架，以解决其在处理隐式曲面边界时的局限性。


<details>
  <summary>Details</summary>
Motivation: 沃斯特（WoSt）在处理隐式曲面边界时，由于缺乏可靠的几何查询方法而受到限制。

Method: 将沃斯特（WoSt）的查询转化为约束全局优化或约束满足问题，并采用基于区间分析的branch-and-bound方法来解决这些高度非凸问题。

Result: 实现了针对闭合隐式曲面的最近轮廓点查询和Robin半径界查询，这是该领域的首次尝试。 成功实现了无需网格即可通过沃斯特（WoSt）求解由闭合隐式曲面定义的PDE。

Conclusion: 提出的几何查询框架和方法首次实现了在闭合隐式曲面边界条件下，使用沃斯特（WoSt）进行无需网格的PDE求解。

Abstract: Walk on stars (WoSt) is currently one of the most advanced Monte Carlo
solvers for PDEs. Unfortunately, the lack of reliable geometric query
approaches has hindered its applicability to boundaries defined by implicit
surfaces. This work proposes a geometric query framework over closed implicit
surfaces for WoSt, under the scope of walkin' Robin. Our key observation is
that all WoSt queries can be formulated as constrained global optimization or
constraint satisfaction problems. Based on our formulations, to solve the
highly non-convex problems, we adopt a branch-and-bound approach based on
interval analysis. To the best of our knowledge, our method is the first to
study closest silhouette point queries and Robin radius bound queries on closed
implicit surfaces. Our formulations and methods first enable mesh-free PDE
solving via WoSt when boundaries are defined by closed implicit surfaces.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [292] [A Wigner Matrix Based Convolution Algorithm For Matrix Elements in the LCAO Method](https://arxiv.org/abs/2510.06285)
*Tyler C. Sterling*

Main category: cond-mat.mtrl-sci

TL;DR: LCAO方法结合了TB的高效性和ab-initio方法的迁移性，通过多极展开和WMCA算法解决了TB的迁移性问题，并对硅进行了应用。


<details>
  <summary>Details</summary>
Motivation: 实现同时具备TB的高效性和ab-initio方法的迁移性的电子结构方法。

Method: 将全晶体势展开为多极，并将矩阵元转化为高角动量函数的两中心积分（2CI）。推导了基于Wigner矩阵的卷积算法（WMCA），该算法适用于任意角动量，并使用模型晶体势对硅进行了应用。

Result: 推导了适用于任意角动量的WMCA算法，并成功应用于硅。

Conclusion: WMCA算法能有效计算全晶体势的2CI，为实现高效且可迁移的ab-initio LCAO方法提供了基础。

Abstract: The linear combination of atomic orbitals (LCAO) method uses a small basis
set in exchange for expensive matrix element calculations. The most efficient
approximation for the matrix element calculations is the two-center
approximation (2CA) in tight binding (TB). In the 2CA, a variety of matrix
elements are neglected with only "two-center integrals" (2CI) remaining. The
2CI are calculated efficiently by rotating to symmetrical coordinates where the
integral is parameterized. This makes TB fast in exchange for diminished
transferability. An ideal electronic structure method has both the efficiency
of TB and the transferability of ab-initio methods. In this work, I expand the
full crystal potential into multipoles where the resulting matrix elements are
transformed into the form of 2CI between high angular momentum functions. The
usual Slater-Koster formulae for TB are limited to $l\leq3$; to enable
efficient evaluation of the full crystal potential 2CI, I derive a Wigner
matrix based convolution algorithm (WMCA) that works for arbitrary angular
momentum. Given a suitable method for generating a local ab-initio Kohn-Sham
potential, the algorithm for calculating matrix elements is applicable to fully
ab-initio LCAO methods (this is the subject of forthcoming work). In this
paper, I apply the WMCA to silicon using a model crystal potential.

</details>


### [293] [Enhancing Direct Air Capture through Potassium Carbonate Doping of Activated Carbons](https://arxiv.org/abs/2510.06400)
*N. van Dongen,A. J. F. van Hoof,S. Calero,J. M. Vicent-Luna*

Main category: cond-mat.mtrl-sci

TL;DR: 直接从空气中捕获二氧化碳（CO2）是通过吸附多孔材料来实现的，但寻找最佳吸附剂材料仍然是一个挑战。本研究通过原子模拟研究了功能化基团和碳酸钾对活性炭吸附CO2和水的影响。


<details>
  <summary>Details</summary>
Motivation: 寻找最佳吸附剂材料以直接从空气中捕获二氧化碳，并探索通过掺杂活性物质来提高现有材料性能。

Method: 使用原子模拟研究功能化基团和碳酸钾对活性炭吸附CO2和水的影响，分析吸附、结构和能量性质。

Result: 结果表明，功能化基团和碳酸钾都能增强CO2和水的吸附，主要是通过将吸附起始压力移至较低值。碳酸钾簇作为额外的吸附位点，促进了水分子成核和氢键网络的形成。

Conclusion: 功能化基团和掺杂碳酸钾可以显著提高活性炭对CO2和水的吸附性能，为开发更有效的CO2捕获材料提供了新的途径。

Abstract: Direct air capture of carbon dioxide (CO$_2$) is one of the most promising
strategies to mitigate rising atmospheric CO$_2$ levels. Among various
techniques, adsorption using porous materials is a viable method for extracting
CO$_2$ from air, even under humid conditions. However, identifying optimal
adsorbent materials remains a significant challenge. Moreover, the performance
of existing materials can be improved by doping with active species that boost
gas capture, a relatively unexplored field. In this study, we perform atomistic
simulations to investigate the adsorption, structural, and energetic properties
of CO$_2$ and water in realistic models of activated carbons. We first analyze
the impact of explicitly considering surfaces containing functional groups,
which aims to imitate the chemical environment of experimental samples.
Additionally, we introduce potassium carbonate within the pores of the
adsorbent to evaluate its effect on CO$_2$ and water adsorption. Our results
demonstrate that both functional groups and potassium carbonate enhance
adsorption, primarily by shifting the adsorption onset pressures to lower
values. Specifically, potassium carbonate clusters act as extra adsorption
sites for CO$_2$ and water, facilitating the nucleation of water molecules and
promoting the formation of a hydrogen bond network within the activated carbon
pores.

</details>


### [294] [Multihyperuniform Particle Composites Inspired by Avian Photoreceptor Patterns for Optical Applications](https://arxiv.org/abs/2510.06402)
*David Keeney,Wenlong Shi,Rohit Thomas,Yang Jiao*

Main category: cond-mat.mtrl-sci

TL;DR: 多物种超均匀材料在高度无序状态下可实现鲁棒和隐蔽的超均匀构型，并可映射到具有定制光学响应的多功能复合材料。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究多物种超均匀系统，并探索其作为多功能光子材料的设计原理。

Method: 使用粒子模型，改变物种数量、尺寸比和相互作用竞争，并进行光学性能映射。

Result: 多物种混合物可以在高度无序的状态下实现鲁棒和隐蔽的超均匀构型，这些构型可以映射到具有各向同性结构色彩、增强吸收和工程介电特性的多功能复合材料。

Conclusion: 多物种超均匀性是一种可推广的设计原理，可用于设计多功能无序光子材料，为鲁棒、可调和可扩展的光学应用开辟了道路。

Abstract: Hyperuniform materials, characterized by anomalously suppressed
long-wavelength density fluctuations, exhibit unique optical and photonic
properties distinct from both crystalline and random media. While most prior
studies have focused on single-species systems, we investigate the broader
class of \textit{multihyperuniform} systems inspired by biological
photoreceptor mosaics. Using particle-based models with varying species number,
size ratios, and interaction competition, we demonstrate that multispecies
mixtures can achieve robust and stealthy hyperuniform configurations, even in
highly disordered states. We further show how these configurations can be
mapped to multifunctional composites with tailored optical responses, including
isotropic structural coloration, enhanced absorption, and engineered dielectric
properties that facilitate transmission while suppressing scattering. Our
results highlight multihyperuniformity as a generalizable design principle for
multifunctional disordered photonic materials, opening avenues for robust,
tunable, and scalable optical applications.

</details>


### [295] [Emergence of multiple relaxation processes during low to high density transition in Au49Cu26.9Si16.3Ag5.5Pd2.3 metallic glass](https://arxiv.org/abs/2510.06409)
*Alberto Ronca,Antoine Cornet,Jie Shen,Thierry Deschamps,Eloi Pineda,Yuriy Chushkin,Federico Zontone,Mohamed Mezouar,Isabella Gallino,Gaston Garbarino,Beatrice Ruta*

Main category: cond-mat.mtrl-sci

TL;DR: 在金-铜-硅-银-钯金属玻璃中，通过高压X射线光子相关光谱和X射线衍射，在3 GPa附近观察到一个结构和动力学转变，该转变与原子重排和两个非晶态的共存有关。


<details>
  <summary>Details</summary>
Motivation: 研究非晶态物质中多晶型现象的微观起源及其对玻璃动力学的影响。

Method: 结合高压X射线光子相关光谱和X射线衍射，研究金-铜-硅-银-钯金属玻璃在室温下的原子动力学-结构关系。

Result: 在3 GPa附近观察到结构和动力学转变，表现为雪崩样的大规模原子重排，促进系统向更紧凑的原子簇连接发展；该转变叠加在近期报道的压力诱导的原子运动加速之上，并标志着一个过渡态的开始，可能与新相的成核有关，其特点是具有不同弛豫过程的两种非晶态共存。

Conclusion: 即使在没有明显结构不连续性的情况下，也存在缓慢的、连续的多晶型转变。

Abstract: The existence of multiple amorphous states, or polyamorphism, remains one of
the most debated phenomena in disordered matter, particularly regarding its
microscopic origin and impact on glassy dynamics. Profiting of the enhanced
data quality provided by brilliant synchrotrons, we combined high pressure
X-ray photon correlation spectroscopy and X-ray diffraction to investigate the
atomic dynamics-structure relationship in a Au49Cu26.9Si16.3Ag5.5Pd2.3 metallic
glass at room temperature. We identify a structural and dynamical crossover
near 3 GPa, marked by avalanches-like massive atomic rearrangements that
promote the system toward increasingly compact atomic cluster connections. This
crossover superimposes to a pressure-induced acceleration of the atomic motion
recently reported, and signals the onset of a transitional state, potentially
linked to the nucleation of a new phase within the glass, characterized by the
coexistence of two amorphous states with distinct relaxation processes. These
results provide evidence for a sluggish, continuous polyamorphic
transformation, even in absence of marked structural discontinuities.

</details>


### [296] [Synthesis and Characterization of Ultrasonically Atomized Al-Based Alloy Powders for Tunable Thermal Reactivity](https://arxiv.org/abs/2510.06446)
*Chetan Singh,Ava Goglia,Peter Mastracco,Michael Flickinger,Laszlo J. Kecskes,Paulette Clancy,Timothy P. Weihs*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究通过超声雾化技术制造了微米级的铝基合金粉末，旨在平衡反应活性和工艺稳定性，以克服纳米铝粉的局限性。


<details>
  <summary>Details</summary>
Motivation: 为了在先进制造、连接和能源应用中安全有效地使用高活性铝粉，需要开发一种能够控制反应活性并易于处理的生产方法。现有方法在可扩展性和安全性方面存在不足。

Method: 采用超声雾化技术制造了纯铝以及二元合金（AlCu、AlSi、AlMg）的微米级铝粉。通过X射线衍射（XRD）、扫描电子显微镜（SEM）和差热/热重分析（DTA/TGA）在氩气/氧气环境中，对粉末的形貌、物相、热稳定性和氧化行为进行了表征。

Result: 研究表明，合金成分和超声雾化工艺控制的微观结构可以有效调节铝粉表面的氧化铝钝化层，改变氧化途径，并调整热起始点和放热峰。

Conclusion: 本研究提出了一种面向制造的框架，可以通过设计微米级铝粉的合金成分和微观结构来调控其点燃和氧化行为，为开发具有特定反应活性的铝粉提供了新的途径。

Abstract: Reactive aluminum (Al) alloy powders are promising for advanced
manufacturing, joining, and energetic applications, yet scalable routes that
couple controlled reactivity with safe handling remain limited. While nanoscale
Al powders ignite readily, their agglomeration, handling, and safety limit
broad deployment. Here, we manufacture micron-sized Al-based powders produced
by ultrasonic atomization (UA), targeting a balance of enhanced reactivity and
process robustness. Binary systems (AlCu, AlSi, AlMg) and pure Al were
synthesized, and their morphology, phases present, thermal stability, and
oxidation behavior were characterized using XRD, SEM, and DTA/TGA in an Ar/O2
environment. We show that alloy selection and UA-controlled microstructure can
modify the native Al2O3 passivation, alter oxidation pathways, and shift
thermal onsets/exotherms. The results establish a manufacturing-forward
framework for designing micron-sized powders with tunable ignition/oxidation
behavior.

</details>


### [297] [Mechanistic insights into hydrogen reduction of multicomponent oxides via in-situ high-energy X-ray diffraction](https://arxiv.org/abs/2510.06455)
*Shiv Shankar,Barak Ratzker,Claudio Pistidda,Dierk Raabe,Yan Ma*

Main category: cond-mat.mtrl-sci

TL;DR: 利用氢气对多种氧化物进行共还原，为可持续合金设计提供了一种碳中性方法。本研究通过原位高能X射线衍射，研究了两种前驱体（机械混合粉末和预烧结氧化物混合物）的氢基直接还原过程，目标是制备等摩尔比的CoFeMnNi合金。


<details>
  <summary>Details</summary>
Motivation: 探索氢气共还原多组分氧化物以实现碳中性合金设计的可能性，并研究不同前驱体状态对还原路径和最终合金微观结构的影响。

Method: 采用原位高能X射线衍射技术，对比分析了机械混合粉末和预烧结氧化物混合物两种前驱体在氢气还原过程中的差异。

Result: 机械混合粉末在700°C时，通过卤石、尖晶石和Mn3O4中间相，还原为体心立方、面心立方和MnO相；而预烧结材料则直接转化为金属相和氧化物相的混合物。还原后，机械混合氧化物的形貌疏松，而预烧结材料则呈现出负载在纳米多孔MnO上的金属纳米颗粒。

Conclusion: 合金的纳米多孔结构很大程度上取决于前驱体的初始状态，这凸显了前驱体在最终微观结构形成中的作用。这种前驱体设计策略提供了一种一步法制备纳米多孔合金的途径，有望在催化和能源技术领域得到应用。

Abstract: Co-reduction of multicomponent oxides with hydrogen provides a carbon-neutral
approach toward sustainable alloy design. Herein, we investigate the
hydrogen-based direct reduction, using in-situ high-energy X-ray diffraction of
two precursor variants: mechanically mixed powders and pre-sintered oxide
mixtures, targeting an equiatomic CoFeMnNi alloy. We find distinct reduction
pathways and microstructure evolution depending on initial precursors. Mixed
powders at 700 {\deg}C are reduced to body-centered-cubic, face-centered-cubic,
and MnO phases via halite, spinel, and Mn3O4 intermediates, whereas the
pre-sintered material directly transforms into a mixture of metallic and oxide
phases. The post-reduction microstructures are also different: mixed oxides
show loosely packed morphology, whereas pre-sintered material reveals metallic
nanoparticles supported on nanoporous MnO. The formation of nanoporous metallic
networks is strongly governed by the precursor state, highlighting the role of
initial precursors on the final microstructure. This precursor design strategy
offers a single-step route to nanoporous alloys with potential applications in
catalysis and energy technologies.

</details>


### [298] [Local Order Average-Atom Interatomic Potentials](https://arxiv.org/abs/2510.06459)
*Chloe A. Zeller,Ronald E. Miller,Ellad B. Tadmor*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究提出了一种名为局部有序平均原子（LOAA）的改进平均原子（AA）势，能够考虑局部有序效应，从而在计算成本较低的情况下更准确地模拟包含短程有序（SRO）的高熵合金等复杂材料。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够考虑局部有序效应的有效原子间势（IP），以扩展现有平均原子（AA）方法的局限性，从而更准确地模拟具有短程有序（SRO）效应的复杂材料，如高熵合金。

Method: 推导了一种局部有序平均原子（LOAA）势，该势利用局部有序平均原子（LOAA）势来包含短程有序（SRO）效应，并通过结合偏径向分布函数的信息来扩展平均原子（AA）方法。该方法在二维（2D）二元六方晶体（Lennard-Jones 相互作用）和三维（3D）Fe$_{(1-x)/2}$Ni$_{(1-x)/2}$Cr$_x$ 及 Ni$_{0.67}$Al$_{0.33}$ 合金（嵌入原子法势）中得到了验证。

Result: 在二维（2D）二元六方晶体中，LOAA 势在能量上与真实的双原子模型相当，并且可以获得相对于 LJ 参数的局部有序相图。在三维（3D）合金中，LOAA 势能够准确捕捉材料的性质和相变，同时需要更小的模拟尺寸以获得统计收敛的结果。

Conclusion: 所提出的局部有序平均原子（LOAA）势能够有效地模拟材料中的短程有序（SRO）效应，并显著降低计算成本，使其在模拟高熵合金等复杂材料方面具有优势。

Abstract: An extension to the effective average-atom (AA) interatomic potential (IP)
that accounts for local ordering effects is derived. While the AA approach is
only valid for random alloys, the new local-order average-atom (LOAA) IP
accounts for short-range order within a material by utilizing information from
partial radial distribution functions. Simulations with a LOAA potential
require smaller system sizes to achieve statistically converged results and
therefore can be used to model complex materials where short-range order
effects are important, such as high-entropy alloys, at a fraction of the
computational cost of standard IPs. The method is validated for a
two-dimensional (2D) binary hexagonal crystal with Lennard-Jones (LJ)
interactions, and for three-dimensional (3D) Fe$_{(1-x)/2}$Ni$_{(1-x)/2}$Cr$_x$
and Ni$_{0.67}$Al$_{0.33}$ alloys modeled via embedded-atom method (EAM)
potentials. For the 2D crystal we obtain a local ordering phase diagram in
terms of the LJ parameters and demonstrate that in all cases the LOAA
formulation obtains elastic properties that match the true-species case using
standard IPs. The 3D alloy examples further demonstrate the ability of this
method to accurately capture other material properties and phase
transformations.

</details>


### [299] [Tunable magnon-phonon cavity via structural phase transition](https://arxiv.org/abs/2510.06464)
*Chunli Tang,Yujie Zhu,Dayne Sasaki,Jiaxuan Wu,Harshil Goyal,Yuzan Xiong,Masoud Mahjouri-Samani,Xiang Meng,Jia-Mian Hu,Yayoi Takamura,Wei Zhang,Wencan Jin*

Main category: cond-mat.mtrl-sci

TL;DR: 外延LSMO/STO异质结构利用结构相变实现了强马农-声子耦合，并通过分析模型验证了应变诱导的马}农分裂和耦合强度。


<details>
  <summary>Details</summary>
Motivation: 开发可调的混合马}农-声子腔，以在固态平台中实现量子功能。

Method: 通过合成和表征外延La0.7Sr0.3MnO3/SrTiO3 (LSMO/STO) 异质结构，并利用磁弹性相互作用研究其在立方-四方相变时的行为，同时开发了一个分析模型。

Result: 观察到LSMO薄膜中的Kittel马}农在STO基板发生立方-四方相变时分裂成三个能带，产生了非简并的杂化马}农-声子模式。分析模型成功复现了应变诱导的马}农分裂和马}农-声子耦合强度。

Conclusion: 结构相变是实现高质量磁弹性氧化物异质结构中多态马}农-声子杂化的一种敏感触发机制，为在声子系统中实现应变调控的混合马}农学提供了新途径，有望应用于相干能量和信号转换。

Abstract: Strong coupling between two quantized excitations in a cavity has the
potential to lead to hybridized states that bestow novel quantum phenomena as
required for emerging applications. In particular, tunable hybrid magnon-phonon
cavities with precise control knobs are in pressing demand for developing
quantum functionalities in solid-state platforms. Here, using a combination of
synthesis and characterization tools, we present an epitaxial
La0.7Sr0.3MnO3/SrTiO3 (LSMO/STO) heterostructure that manifests strong
couplings between the Kittel magnon and the transverse acoustic phonon.
Remarkably, leveraging the magnetoelastic interaction at the epitaxial
interface, we demonstrate that when the STO substrate undergoes a
cubic-to-tetragonal phase transition at ~105 K, the Kittel magnon of the LSMO
thin film splits into three bands due to anisotropic structural strains along
the [100], [010], and [001] crystalline axes, hence, resulting in an array of
non-degenerate, hybridized magnon-phonon modes. Moreover, we develop an
analytical model that can reproduce the interfacial strain-induced magnon
splitting and the strength of magnon-phonon coupling. Our work highlights
structural phase transitions as a sensitive trigger for generating multistate
magnon-phonon hybridization in high-quality magnetoelastic oxide
heterostructures - a new route for implementing strain-mediated hybrid
magnonics in phononic systems with potential applications in coherent energy
and signal transduction.

</details>


### [300] [High temperature Neel skyrmions in simple ferromagnets](https://arxiv.org/abs/2510.06488)
*Peng Wang,Rana Saha,Holger L. Meyerheim,Ke Gu,Hakan Deniz,David Eilmsteiner,Andrea Migliorini,Juan Rubio Zuazo,Engenia Sebastiani-Tofano,Ilya Kostanovski,Abhay Kant Srivastava,Arthur Ernst,Stuart S. P. Parkin*

Main category: cond-mat.mtrl-sci

TL;DR: 在简单的Co-Al和Co-Ni-Al铁磁合金薄层中，通过外延引入的垂直应变梯度，在高达770K的宽温度范围内形成了尼尔斯型斯格明子，并使用洛伦兹透射电子显微镜进行了高分辨率成像。


<details>
  <summary>Details</summary>
Motivation: 探索在简单铁磁合金中形成斯格明子，并扩大其在高温下的稳定性，以推动斯格明子电子学的发展。

Method: 通过外延引入垂直应变梯度，在Co-Al和Co-Ni-Al薄层中制备斯格明子，并使用洛伦兹透射电子显微镜和X射线衍射技术进行表征。

Result: 观察到在高达770K的温度下，Co-Al和Co-Ni-Al薄层中存在尼尔斯型斯格明子，并测量了应变梯度。结果表明，斯格明子在宽温度范围内稳定存在。

Conclusion: 该研究证明了通过应变工程可以简单地在具有高磁序化温度的铁磁材料中实现高温斯格明子，为斯格明子电子学提供了新的可能性。

Abstract: A wide variety of chiral non-collinear spin textures have been discovered and
have unique properties that make them highly interesting for technological
applications. However, many of these are found in complex materials and only in
a narrow window of temperature. Here, we show the formation of Neel-type
skyrmions in thin layers of simple ferromagnetic alloys, namely Co-Al and
Co-Ni-Al, over a wide range of temperature up to 770 K, by imposing a vertical
strain gradient via epitaxy with an Ir-Al underlayer. The Neel skyrmions are
directly observed using Lorentz transmission electron microscopy in
freestanding membranes at high temperatures and the strain gradient is directly
measured from x-ray diffraction anomalous peak profiles. Our concept allows
simple centrosymmetric ferromagnets with high magnetic ordering temperatures to
exhibit hot skyrmions, thereby, bringing closer skyrmionic electronics.

</details>


### [301] [Real-Space Quantification of Exciton Localization in Acene Crystals Using Wannier Function Decomposition](https://arxiv.org/abs/2510.06539)
*Zui Tao,Jonah B. Haber,Jeffrey B. Neaton*

Main category: cond-mat.mtrl-sci

TL;DR: WFDX方法可以量化固体中激子的局域化，并揭示其与晶体结构和对称性的关系。


<details>
  <summary>Details</summary>
Motivation: 需要一种方法来量化固体中激子的局域化，并理解其与材料性质的关系。

Method: 提出并应用WFDX方法，该方法将激子波函数分解为局域化的Wannier函数，并在实空间中进行分析。

Result: WFDX方法可以有效地量化弗伦克尔激子和电荷转移激子的局域化，并揭示了其与分子结构、自旋态和动量等因素的关系。此外，该方法还能反映出隐藏在倒易空间描述中的非对称晶格对称性。

Conclusion: WFDX方法是一种通用且高效的工具，可以用于分析和计算固体的激子性质，并为理解和设计具有特定光电特性的材料提供新的视角。

Abstract: We introduce the Wannier function decomposition of excitons (WFDX) method to
quantify exciton localization in solids within the ab initio Bethe-Salpeter
equation framework. By decomposing each Bloch exciton wavefunction into
products of single-particle electron and hole maximally localized Wannier
functions, this real-space approach provides well-defined orbital- and spatial-
resolved measures of both Frenkel and charge-transfer excitons at low
computational cost. We apply WFDX to excitons in acene crystals, quantifying
how the number of rings, the exciton spin state, and the center-of-mass momntum
affect spatial localization. Additionally, we show how this real-space
representation reflects structural nonsymmorphic symmetries that are hidden in
standard reciprocal-space descriptions. We demonstrate how the WFDX framework
can be used to efficiently interpolate exciton expansion coefficients in
reciprocal-space and outline how it may facilitate evaluation of observables
involving position operators, highlighting its potential as a general tool for
both analyzing and computing excitonic properties in solids.

</details>


### [302] [Phase relation investigation of U-La-O system under oxidizing conditions and observation of novel meta-stable and mixed-valent uranium phase- Ln3U11O36 (Ln=La, Nd, Sm, Gd)](https://arxiv.org/abs/2510.06639)
*Shafeeq Muhammed,Geeta Patkare,Rohan Phatak*

Main category: cond-mat.mtrl-sci

TL;DR: 通过凝胶燃烧合成法和热处理法合成了U-La-O体系中的12种样品，并利用X射线衍射、热重分析和氧铀比等多种技术，确定了在1173 K和1523 K空气气氛下U1-yLayO2+x（y=0.025, 0.05, 0.1, 最高0.3）的物相关系。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索U-La-O体系中的物相关系，并首次报道一种新型亚稳相La3U11O36。

Method: 采用凝胶燃烧合成法制备样品，并通过X射线衍射、热重分析和氧铀比等技术进行表征。

Result: 首次发现并报道了一种具有混合价态铀的新型亚稳相La3U11O36，并对其热性质进行了研究。同时，更新了U1-yLayO2+x（y < 0.3）在1173 K及以上温度下的物相关系。此外，还研究了Nd3+, Sm3+, Gd3+和Y3+等较小阳离子对Ln3U11O36相形成和稳定性的影响。

Conclusion: 本研究成功合成了U-La-O体系中的新型亚稳相La3U11O36，并对其物相关系和稳定性进行了初步研究，为后续相关材料的研究奠定了基础。

Abstract: Total of twelve samples in the U-La-O system with the compositions
U1-yLayO2+x (y=0.025, 0.05, 0.1, up to 0.3) were synthesized by gel combustion
synthesis method followed by appropriate heat treatment in air atmosphere.
Comprehensive experimental analysis using various techniques like X-ray
diffraction, thermogravimetry and oxygen to uranium ratio (O/U) are used to
establish the phase relation in U1-yLayO2+x system at 1173 K and 1523 K heated
in air. A novel meta-stable phase with stoichiometry La3U11O36 having
mixed-valent uranium is reported for the first time in U-La-O system. The
thermal property of this new phase is reported along with the updated phase
relations at and above 1173 K temperature for the compositions U1-yLayO2+x (y <
0.3). Further, formation and stability of this new Ln3U11O36 phase was also
investigated with smaller cations like Nd3+, Sm3+, Gd3+ and the smallest Y3+
cation.

</details>


### [303] [Modular Reactor for In Situ X-ray Scattering, Spectroscopy, and ATR-IR Studies of Solvothermal Nanoparticle Synthesis](https://arxiv.org/abs/2510.06770)
*Sani Y. Harouna-Mayer,Melike Gumus Akcaalan,Jagadesh Kopula Kesavan,Tjark R. L. Groene,Lars Klemeyer,Sarah-Alexandra Hussak,Lukas Grote,a Davide Derelli,Francesco Caddeo,Cecilia Zito,Paul Stützle,Dorota Speer,Ann-Christin Dippel,Blanka Detlefs,Yannik Appiarius,Axel Jacobi von Wangelin,Dorota Koziej*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究提出了一种多功能反应器，可用于在溶剂热合成过程中进行原位X射线散射、X射线光谱和红外光谱研究，以克服传统表征技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 理解溶剂热合成功能纳米材料过程的化学原理对于其合理设计和优化至关重要，但传统表征技术和原位研究的挑战限制了对此的理解。

Method: 开发了一种可在-20°C至200°C、高达8 bar压力下进行磁力搅拌和气体/液体注入的原位X射线散射、X射线光谱和红外光谱反应器，并通过其研究了乙酰丙酮铁在苯甲醇中合成磁铁矿纳米粒子的过程。

Result: 成功展示了该反应器在研究溶剂热合成中的能力，能够精确控制温度、压力和搅拌，并获得了磁铁矿纳米粒子合成过程的详细原位数据。

Conclusion: 所提出的反应器为溶剂热合成功能纳米材料的深入理解和优化提供了有力的工具。

Abstract: Understanding the chemical processes that occur during the solvothermal
synthesis of functional nanomaterials is essential for their rational design
and optimization for specific applications. However, these processes remain
poorly understood, primarily due to the limitations of conventional ex situ
characterization techniques and the technical challenges associated with in
situ studies, particularly the design and implementation of suitable reactors.
Here, we present a versatile cell suitable for in situ X-ray scattering, X-ray
spectroscopy, and infrared spectroscopy studies performed during solvothermal
synthesis under autoclave-like, inert conditions. The reactor enables precise
control of the temperature between -20 C and 200 C, pressures up to 8 bar,
magnetic stirring, and injection of gas or liquids. The reactor's capabilities
are demonstrated by comprehensively studying the solvothermal synthesis of
magnetite nanoparticles from iron acetylacetonate in benzyl alcohol through in
situ X-ray scattering and spectroscopy, and ATR-IR spectroscopy.

</details>


### [304] [Ground state magnetic structure of Mn3Sn](https://arxiv.org/abs/2510.06808)
*Jeppe Jon Cederholm,Zhian Xu,Yanfeng Guo,Martin Ovesen,Thomas Olsen,Kristine M. L. Krighaar,Chrystalla Knekna,Jian Rui Soh,Youngro Lee,Navid Qureshi,Jose Alberto Rodriguez Velamazan,Eric Ressouche,Andrew T. Boothroyd,Henrik Jacobsen*

Main category: cond-mat.mtrl-sci

TL;DR: Mn3Sn adopts an inverse triangular structure with spins parallel to <100> (Type III), determined using spherical neutron polarimetry. Subtle effects like sixth-order anisotropy likely cause this selection, as DFT calculations show no energy difference between Type III and Type IV structures. The domain structure is lost upon entering the low-temperature incommensurate phase.


<details>
  <summary>Details</summary>
Motivation: determine the ground state magnetic structure of Mn3Sn

Method: Spherical neutron polarimetry and Density functional theory (DFT) calculations

Result: Mn3Sn adopts an inverse triangular structure with spins parallel to <100> (Type III). DFT calculations show no energy difference between Type III and Type IV structures. Partial control of magnetic domain population is possible with a moderate magnetic field, revealing unequal population across domains. This domain structure is lost in the low-temperature incommensurate phase.

Conclusion: The ground state magnetic structure of Mn3Sn is an inverse triangular structure with spins parallel to <100> (Type III). The selection of this structure is likely due to subtle effects beyond DFT's energy difference calculations. The domain structure, controllable by a magnetic field, is lost in the incommensurate phase.

Abstract: We use spherical neutron polarimetry to determine the ground state magnetic
structure of Mn3Sn. We find that Mn3Sn adopts an inverse triangular structure
with spins parallel to <100> (Type III) rather than spins parallel to <110>
(Type IV). Density functional theory calculations reveal no energy difference
between these two structures, suggesting that the selection is caused by subtle
effects such as sixth-order anisotropy. Partial control of the magnetic domain
population through a moderate magnetic field is key to distinguish between the
two models. We find that three of the six domains are approximately equally
populated, while the others have negligible population. Upon entering the low
temperature incommensurate phase, the domain structure is lost. The domains
decouple from the magnetic field, and can therefore not be controlled by any
known method.

</details>


### [305] [Computational Study on the Physical Properties and Hydrogen Storage Capability of Insulating LaMg2H7](https://arxiv.org/abs/2510.06875)
*Tanvir Khan,Md Hasan Shahriar Rifat,M. Ibrahim,Razia Marzia,F. Parvin*

Main category: cond-mat.mtrl-sci

TL;DR: LaMg2H7是一种宽带隙半导体，具有储氢和热障涂层应用的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究LaMg2H7的结构、电子、弹性、热学和光学特性，以评估其在储氢和热障涂层方面的应用前景。

Method: 利用密度泛函理论（DFT）计算LaMg2H7的结构、电子带结构、弹性、热学和光学特性。

Result: LaMg2H7在机械上是稳定的，具有脆性和各向异性，硬度适中。它具有宽带隙，适合储氢应用。它对紫外线有中等反射率，吸收和电导率与带隙一致。热学性质表明它具有中等的熔点和较高的晶格热导率，热膨胀系数和最小热导率适合用作热障涂层。

Conclusion: LaMg2H7是一种具有多种潜在应用的宽带隙半导体材料，特别是在储氢和热障涂层领域。

Abstract: LaMg2H7 is a ternary wide band gap semiconductor that is a member of the
hydride family. The bulk physical characteristics of the LaMg2H7 compound,
including its structural, electronic band structure, elastic, thermal, and
optical characteristics, have been examined in this work utilizing density
functional theory (DFT). The elastic constants indicate that {\rm LaMg}_2H_7 is
mechanically stable, brittle in nature, and anisotropic. This studied compound
possesses a moderate level of hardness. The band structure and density of
states have been examined to have a better understanding of its electronic
behavior. The intrinsic carrier concentrations and effective masses have been
determined using the band structure. The gravimetric hydrogen storage capacity
(Cwt%) has been calculated, indicating that this compound is suitable for
hydrogen storage applications. This compound is dynamically stable, as
confirmed by its phonon dispersion. Here, the details of this wide-band-gap
semiconductor's reflectivity, absorption coefficient, refractive index,
dielectric function, optical conductivity, and loss function are investigated.
The substance is a moderate reflector of ultraviolet (UV) light. The absorption
and conductivity support the gap in the band structure. The thermodynamic
properties, such as bulk modulus, internal energy, specific heat capacity,
entropy, thermal expansion coefficient, and Debye temperature, have been
explored at varying temperatures and pressures. {\rm LaMg}_2H_7 has a moderate
level of melting temperature with higher lattice thermal conductivity. The
value of the thermal expansion coefficient and minimum thermal conductivity is
highly recommended for use as a thermal barrier coating (TBC).

</details>


### [306] [Uncovering domain morphology in an unconventional magnet with scanning diamond quantum magnetometry](https://arxiv.org/abs/2510.06895)
*Freya Johnson,Jan Zemen,Helena Knowles,Lesley F. Cohen*

Main category: cond-mat.mtrl-sci

TL;DR: Mn3NiN在冷却时会形成无序的分形磁畴结构，这表明弹性贡献和缺陷在决定畴大小方面起着关键作用。


<details>
  <summary>Details</summary>
Motivation: 非传统的磁性材料，包括非共线反铁磁体、p波磁体和交替磁体，是量子自旋电子学和混合量子器件的新兴前沿。控制这些材料的磁畴状态对于其应用至关重要，因为它们独特的、由对称性驱动的性质在多畴极限下会消失。然而，对于具有补偿局域磁矩的材料中磁畴形成的机制，人们仍然知之甚少。

Method: 使用扫描氮空位中心磁强测量技术检查Mn3NiN的亚铁磁性到非共线反铁磁性的相变。对冷却时的磁畴演化进行纳米级测绘，并将局域杂散场与全局磁测量和异常霍尔效应测量相关联。

Result: 观察到无序的、树枝状的畴结构的形成，其粗糙度通过分形维度进行量化。随着冷却通过相变，分形维度稳步增加，在非共线相中饱和到约1.55的值，但畴面积分布未显示任何显著变化。该行为不能用退磁能量和畴壁能量的平衡来解释。

Conclusion: 弹性贡献和缺陷是解释畴大小的关键因素。

Abstract: Unconventional magnetic materials including non-collinear antiferromagnets,
p-wave magnets and altermagnets, are an emerging frontier for quantum
spintronics and hybrid quantum devices. Critical to the application of these
materials is control over the magnetic domain state, as their unique,
symmetry-driven properties vanish in a multi-domain limit. However, the
mechanisms governing domain formation in materials with compensated local
moments remain poorly understood. In this work, we examine the ferrimagnetic to
non-collinear antiferromagnetic phase transition of Mn3NiN using scanning
nitrogen-vacancy centre magnetometry. We provide nanoscale mapping of the
magnetic domain evolution on cooling and correlate the local stray fields with
global magnetometry and anomalous Hall effect measurements. We observe the
formation of a disordered, dendritic domain structure whose roughness is
quantified using its fractal dimension. The fractal dimension steadily
increases on cooling through the transition, saturating at a value of ~ 1.55 in
the non-collinear phase, but the domain area distribution does not show any
significant changes. We show this behaviour cannot be explained by the balance
of demagnetisation energy and domain wall energy, and conclude elastic
contributions and defects are a critical factor to explain the domain size.

</details>


### [307] [Microstructure sensitive recurrent neural network surrogate model of crystal plasticity](https://arxiv.org/abs/2510.06904)
*Michael D. Atkinson,Michael D. White,Adam J. Plowman,Pratheek Shanthraj*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究开发了一种基于门控循环单元（GRU）的循环神经网络（RNN）代理模型，可以直接从三维晶粒结构和任意变形历史预测均匀化应力响应，以解决计算成本高昂的全场晶体塑性（CP）模型在严苛环境下的应用限制。


<details>
  <summary>Details</summary>
Motivation: 为了加速开发用于严苛环境的新一代结构材料，需要快速评估其机械性能及其对微观结构的依赖性。然而，全场晶体塑性（CP）模型虽然能提供详细的见解，但其高昂的计算成本限制了它们在不确定性量化工作流和部件级模拟中的应用。

Method: 本研究开发了一种考虑微观结构敏感性的RNN代理模型，该模型能够直接从三维晶粒结构和任意变形历史预测均匀化应力响应。该模型采用门控循环单元（GRU）架构，通过将微观结构映射到初始隐藏状态和序列输入，从而能够捕捉路径依赖性和微观结构变异性。使用包括随机生成的微观结构和加载路径组合的300,000多次CP模拟作为训练数据。

Result: 该模型能够准确重现CP预测结果，在模型内和未曾见过的变形模式下的预测误差为2 MPa至3 MPa。然而，对于模型未曾见过的微观结构（例如，高度织构化的微观结构），预测精度有所下降，这表明了具有代表性的训练数据的重要性。将该模型嵌入多尺度框架后，证明了其在替代传统本构更新方面的能力，从而在保持应力分布关键特征的同时降低了计算成本。

Conclusion: 研究结果表明，受微观结构启发的RNN代理模型是一种有前景的CP直接模拟替代方案，为实现快速多尺度建模和不确定性量化提供了途径。

Abstract: The development of next-generation structural materials for harsh
environments requires rapid assessment of mechanical performance and its
dependence on microstructure. While full-field crystal plasticity (CP) models
provide detailed insights, the high computational cost limits their use with
uncertainty quantification workflows and in component-scale simulation.
Surrogate models based on recurrent neural networks (RNNs) have shown promise
in reproducing history-dependent mechanical behaviour but are applied to models
with either fixed microstructure or representative volume elements. Here, we
develop a microstructure sensitive RNN surrogate that predicts homogenised
stress responses directly from three-dimensional grain structures and arbitrary
deformation histories. The architecture employs a gated recurrent unit (GRU)
with mappings from microstructure to both the initial hidden state and sequence
inputs, allowing the model to capture path dependence and microstructure
variability. Training data comprised of over 300,000 CP simulations generated
using combinations of randomly generated microstructures and loading paths. The
model was found to reproduce CP predictions for both in-distribution validation
data and unseen deformation modes, with errors of 2 MPa to 3 MPa.
Out-of-distribution microstructures were more difficult to predict, emphasising
the need for representative training data with, for example, heavily textured
microstructures. Embedding the model into a multiscale framework demonstrates
its ability to replace conventional constitutive updates, reducing
computational cost while preserving key features of the stress distribution.
These results establish microstructure-informed RNN surrogates as a promising
alternative to direct CP simulations, offering a pathway toward rapid
multiscale modelling and uncertainty quantification.

</details>


### [308] [SiC-TGAP: A machine learning interatomic potential for radiation damage simulations in 3C-SiC](https://arxiv.org/abs/2510.06966)
*Ali Hamedani,Andrea E. Sand*

Main category: cond-mat.mtrl-sci

TL;DR: TGAP是一种用于3C-SiC的机器学习势能模型，能够准确模拟原子尺度下的辐射损伤，包括缺陷演化和相变。


<details>
  <summary>Details</summary>
Motivation: 现有的3C-SiC经验势能模型在预测碰撞级联产生的缺陷演化特性方面存在显著差异，需要一个更准确的模型来研究辐射损伤。

Method: 开发了一个基于高斯近似势（GAP）的模型，并使用双体和turboSOAP多体描述符进行训练，数据集包含晶体、液体和非晶相，并特别考虑了二十一种缺陷类型。为了更好地模拟碰撞级联，该模型还结合了Nordlund-Lehtola-Hobler排斥势。

Result: TGAP能够准确预测3C-SiC在不同相态下的性质，包括捕获液相碳原子的分解行为，精确复现了高温液相径向分布函数，并与实验和密度泛函理论预测的熔点高度一致。

Conclusion: TGAP为研究立方氮化硅的原子尺度辐射损伤提供了一个准确的模拟工具。

Abstract: Silicon carbide (SiC) has long been a subject of study for its application in
harsh environments. Existing empirical interatomic potentials for 3C-SiC show
significant discrepancies in predicting the properties that are crucial in
describing the evolution of defects generated in collision cascades. We present
a Gaussian approximation potential model for 3C-SiC (TGAP) trained by two-body
and the turboSOAP many-body descriptors. The dataset covers crystalline, liquid
and amorphous phases. To accurately capture defect dynamics, twenty-one defect
types have been included in the dataset. TGAP captures the experimentally
observed decomposition of carbon atoms in the liquid phase at atmospheric
pressure, while also accurately reproducing the radial distribution function of
the high-temperature homogeneous liquid phase across a range of densities.
Moreover, it predicts the melting point in very good agreement with density
functional theory and experiments. The potential is equipped with the
Nordlund-Lehtola-Hobler repulsive potential to capture the high repulsion of
recoils in the collision cascades. TGAP provides an accurate tool for atomistic
simulation of radiation damage in cubic SiC.

</details>


### [309] [Anomalous strain-dependent thermal conductivity in superelastic screw-dislocated graphites](https://arxiv.org/abs/2510.07075)
*Yu Li,Zhiqiang Zhao,Zhuhua Zhang,Yong-Wei Zhang,Jin-Wu Jiang*

Main category: cond-mat.mtrl-sci

TL;DR: 螺位错石墨（SDGs）在拉伸和压缩应变下，其跨面热导率异常增加，在高达80%的拉伸应变下，热导率增强超过100%，有望用于高性能电子和可穿戴设备。


<details>
  <summary>Details</summary>
Motivation: 设计应变稳定甚至应变增强的热传输材料对于高性能电子器件的稳定运行至关重要，但大多数纳米材料会因应变而退化。

Method: 利用高精度的机器学习势驱动的非平衡分子动力学模拟，研究了螺位错石墨（SDGs）在应变下的热传输行为，并推导了连接热导率与位错数量和应变的分析模型。

Result: SDGs在拉伸和压缩应变下，其跨面热导率均表现出异常增加的趋势，在高达80%的拉伸应变下，热导率增强超过100%，其热导率比多层石墨烯高一个数量级。

Conclusion: SDGs具有应变可调的热导率特性，是高性能电子和可穿戴设备中具有应用前景的材料平台。

Abstract: The design of strain-stable, or even strain-enhanced thermal transport
materials is critical for stable operation of high-performance electronic
devices. However, most nanomaterials suffer from strain-induced degradation,
with even minor tensile strains markedly reducing thermal conductivity. Here,
we demonstrate that screw-dislocated graphites (SDGs), recently identified as
topological semimetals, display an unusual increase in cross-plane thermal
conductivity under both tensile and compressive strains, revealed by
high-accuracy machine-learning-potential-driven non-equilibrium molecular
dynamics. Notably, SDGs exhibit over 100% enhancement under tensile strains up
to 80% along the dislocation axis, arising from strain-induced increase in
dislocation interface tilt angle that elongates the effective heat transfer
paths. Their thermal conductivity surpasses multilayer graphene by an order of
magnitude. An analytical model is further derived linking thermal conductivity
to dislocation number and strain, offering a predictive framework for designing
strain-tunable screwdislocated structures. These findings highlight SDGs as a
promising platform for high-performance electronic and wearable devices with
tunable thermal properties.

</details>


### [310] [Understanding Polaronic Transport in Anatase TiO2 Films by Combining Precise Synthesis and First-Principles Many-Body Theory](https://arxiv.org/abs/2510.07097)
*F. Liu,Z. Yang,Y. Luo,S. Guo,C. Zhang,S. Choo,X. Xu,X. Wang,K. A. Mkhoyan,M. Bernardi,B. Jalan*

Main category: cond-mat.mtrl-sci

TL;DR: 通过混合分子束外延生长高质量的 TiO2 薄膜，并结合第一性原理电子-声子图蒙特卡洛方法，成功表征了极化子及其在复杂氧化物中的输运性质。


<details>
  <summary>Details</summary>
Motivation: 为了解决在复杂氧化物中，电荷载流子与晶格振动强耦合形成的极化子（电子-声子准粒子）的输运性质难以表征的问题。

Method: 结合混合分子束外延（MBE）生长高质量的含氧空位掺杂锐钛矿TiO2薄膜，并采用第一性原理电子-声子图蒙特卡洛（FEP-DMC）框架进行计算。

Result: 在锐钛矿TiO2中实现了创纪录的电子迁移率，与FEP-DMC的计算结果（室温迁移率45 +/- 15 cm2V-1s-1，迁移率-温度关系为μ ∝ T^(-1.9 +/- 0.077)）高度一致。扫描透射电子显微镜和X射线光电子能谱分析揭示了氧空位在低温下对输运的调制作用。FEP-DMC提供了极化子形成能、声子云分布、极化子周围晶格畸变以及极化子对迁移率的贡献等定量信息。

Conclusion: 建立了表征极化子的预测性理论-实验工作流程，并对锐钛矿TiO2中大极化子的输运提供了微观理解，该方法可推广应用于其他复杂氧化物和极化子材料。

Abstract: In complex oxides, charge carriers often couple strongly with lattice
vibrations to form polarons-entangled electron-phonon quasiparticles whose
transport properties remain difficult to characterize. Experimental access to
intrinsic polaronic transport requires ultraclean samples, while theoretical
descriptions demand methods beyond low-order perturbation theory. Here, we
combine the growth of high-quality oxygen-vacancy-doped anatase TiO2 films by
hybrid molecular beam epitaxy (MBE) with a first-principles electron-phonon
diagrammatic Monte Carlo (FEP-DMC) framework recently developed for accurate
polaron predictions. Our films exhibit record-high electron mobility for
anatase TiO2, in excellent agreement with FEP-DMC calculations conducted prior
to experiment, which predict a room-temperature mobility of 45 +/- 15 cm2V-1s-1
and a mobility-temperature scaling of mobility proportional to T^(-1.9 +/-
0.077). Microscopic analysis using scanning transmission electron microscopy
and X-ray photoelectron spectroscopy reveals the role of oxygen vacancies in
modulating transport at lower temperatures. FEP-DMC further provides
quantitative insight into polaron formation energy, phonon cloud distribution,
lattice distortion around the polaron, and the polaronic contribution to
mobility. Together, these results establish a predictive theory-experiment
workflow to characterize polarons and provide a microscopic understanding of
large-polaron transport in anatase TiO2, with broader implications for complex
oxides and other polaronic materials.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [311] [Predicting the future with magnons](https://arxiv.org/abs/2510.06382)
*Zeling Xiong,Christopher Heins,Thibaut Devolder,Fabian Kammerbauer,Mathias Kläui,Jürgen Fassbender,Helmut Schultheiss,Katrin Schultheiss*

Main category: cond-mat.mes-hall

TL;DR: Magnons in magnetic microdisks can be used as a physical reservoir for predicting complex chaotic signals, achieving accurate time series predictions comparable to state-of-the-art methods. Spectral resolution and device geometry are key design principles for improving performance. This demonstrates the potential of magnonics for unconventional computing and real-time prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Forecasting complex, chaotic signals is a significant challenge in various scientific and technological fields, including secure communications and climate modeling.

Method: Using a magnetic microdisk in the vortex state as a magnon-scattering reservoir, intrinsic nonlinear interactions are leveraged to transform a microwave input into a high-dimensional spectral output for reservoir computing and time series predictions. The Mackey-Glass benchmark is used for training and testing.

Result: The system achieves accurate and reliable predictions on the Mackey-Glass benchmark, rivaling state-of-the-art physical reservoirs. Key design principles identified include the trade-off between dimensionality and accuracy governed by spectral resolution, and systematic performance improvement by combining multiple device geometries.

Conclusion: Magnonics presents a promising platform for unconventional computing, offering a pathway to scalable and CMOS-compatible hardware for real-time prediction tasks.

Abstract: Forecasting complex, chaotic signals is a central challenge across science
and technology, with implications ranging from secure communications to climate
modeling. Here we demonstrate that magnons - the collective spin excitations in
magnetically ordered materials - can serve as an efficient physical reservoir
for predicting such dynamics. Using a magnetic microdisk in the vortex state as
a magnon-scattering reservoir, we show that intrinsic nonlinear interactions
transform a simple microwave input into a high-dimensional spectral output
suitable for reservoir computing, in particular, for time series predictions.
Trained on the Mackey-Glass benchmark, which generates a cyclic yet aperiodic
time series widely used to test machine-learning models, the system achieves
accurate and reliable predictions that rival state-of-the-art physical
reservoirs. We further identify key design principles: spectral resolution
governs the trade-off between dimensionality and accuracy, while combining
multiple device geometries systematically improves performance. These results
establish magnonics as a promising platform for unconventional computing,
offering a path toward scalable and CMOS-compatible hardware for real-time
prediction tasks.

</details>


### [312] [Many-Body Effects in a Molecular Quantum NAND Tree](https://arxiv.org/abs/2510.06438)
*Justin P. Bergfield*

Main category: cond-mat.mes-hall

TL;DR: 使用化学编码的量子NAND树，通过量子多体输运理论研究，证明了在动态相关性下NAND行为的持续性，并提出了热电作为化学鲁棒的逻辑读出方法。


<details>
  <summary>Details</summary>
Motivation: 利用分子作为最小电路，通过量子干涉和电子相关性进行逻辑运算，特别是量子NAND门。

Method: 研究基于炔基扩展的异聚乙炔骨架的化学编码量子NAND树，通过端基取代设置输入，通过传输节点有无读取输出，并运用量子多体输运理论。

Result: NAND行为在动态相关性下依然存在，但节点位置和化学位移对电子-电子相互作用敏感；热电作为一种化学鲁棒的读出方式，其区分度远超实验不确定性。

Conclusion: 这些分子系统不仅能探测电子相关性的强度，还能利用电子相关性来塑造逻辑响应，其中热电是一种有效的读取方法。

Abstract: Molecules provide the smallest possible circuits in which quantum
interference and electron correlation can be engineered to perform logical
operations, including the universal NAND gate. We investigate a chemically
encoded quantum NAND tree based on alkynyl-extended iso-polyacetylene
backbones, where inputs are set by end-group substitution and outputs are read
from the presence or absence of transmission nodes. Using quantum many-body
transport theory, we show that NAND behavior persists in the presence of
dynamic correlations, but that the nodal positions and their chemical shifts
depend sensitively on electron-electron interactions. This sensitivity
highlights the potential of these systems not only to probe the strength of
electronic correlations but also to harness them in shaping logical response.
The thermopower is identified as a chemically robust readout of gate logic,
providing discrimination margins that greatly exceed typical experimental
uncertainties, in an observable governed primarily by the variation of
transport rather than its absolute magnitude.

</details>


### [313] [Nonlinear Optical Response in Pseudo-Hermitian Systems at Steady State](https://arxiv.org/abs/2510.06580)
*S. Sajad Dabiri,Reza Asgari*

Main category: cond-mat.mes-hall

TL;DR: We developed a theory for nonlinear optical conductivity in pseudo-Hermitian systems, finding unique phenomena and responses not seen in Hermitian systems, with implications for non-Hermitian photonics and quantum devices.


<details>
  <summary>Details</summary>
Motivation: The paper aims to establish a steady-state theory for nonlinear optical conductivity in pseudo-Hermitian systems to understand nonlinear light-matter interactions in active media with balanced gain and loss.

Method: The study derives compact formulas for conductivity tensors using a biorthogonal density matrix formalism and applies it to parity-time symmetric two-level systems.

Result: The research reveals novel nonlinear phenomena in pseudo-Hermitian systems, including extra conductivity terms, corrections to the velocity operator, photocurrents, and distinct harmonic generation responses like real second-order conductivities and nonzero DC limits.

Conclusion: The findings offer new insights into nonlinear light-matter interactions in dissipative systems and have potential applications in non-Hermitian photonics, dissipative topological systems, and quantum devices with engineered dissipation.

Abstract: We establish a steady-state theory for nonlinear optical conductivity in
pseudo-Hermitian systems. We derive compact formulas for the first and second
order conductivity tensors in both the velocity and length gauges and prove
their exact equivalence through generalized sum rules and Berry connection
identities by formulating the nonlinear response in terms of a biorthogonal
density matrix. Utilizing the formalism on parity-time symmetric two-level
systems reveals nonlinear phenomena that are not present in Hermitian systems,
such as extra terms in the conductivity, corrections to the velocity operator,
photocurrent, and resonance structures with higher-order poles at one-photon
transitions. These features yield qualitatively distinct harmonic generation
responses like real second-order conductivities and nonzero DC limits. These
results provide new insights into nonlinear light-matter interactions in active
media characterized by balanced gain and loss, with implications for
non-Hermitian photonics, dissipative topological systems, and quantum devices
designed with engineered dissipation.

</details>


### [314] [Interband optical conductivity in two-dimensional semi-Dirac bands tilting along the quadratic dispersion](https://arxiv.org/abs/2510.06591)
*Xin Chen,Jian-Tong Hou,Long Liang,Jie Lu,Hong Guo,Chang-Xu Yan,Hao-Ran Chang*

Main category: cond-mat.mes-hall

TL;DR: 二维半狄拉克材料在能量色散的一个空间方向上具有二次色散，在另一个方向上具有线性色散，从而有效地混合了普通费米子和狄拉克费米子。通过沿波矢的任一空间方向倾斜能带，可以进一步调节能量色散的各向异性。我们提出了一种新的倾斜参数定义，用于表征沿二次色散方向倾斜的二维半狄拉克能带中的Lifshitz相。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提出一种新的倾斜参数定义，用于表征沿二次色散方向倾斜的二维半狄拉克能带中的Lifshitz相，并通过理论研究其在倾斜能带中的带间光学电导率，以区分不同的二维半狄拉克能带和狄拉克能带。

Method: 利用线性响应理论，理论研究了二维倾斜半狄拉克能带的带间光学电导率。

Result: 在零温度下，我们得出的解析结果与沿线性色散方向倾斜的狄拉克和半狄拉克系统存在显著差异。特别地，我们发现在倾斜参数的特定范围内，光学电导率中会出现谱固定点，这可以通过态密度联合行为相对应地解释。

Conclusion: 这项研究为识别和表征二维倾斜半狄拉克材料提供了一个强大的理论框架，并建立了区分不同类型的二维半狄拉克能带和狄拉克能带的清晰谱指纹。我们的预测可以指导未来关于各向异性能带工程和倾斜依赖现象的实验研究。

Abstract: Two-dimensional (2D) semi-Dirac materials feature a unique anisotropic band
structure characterized by quadratic dispersion along one spatial direction and
linear dispersion along the other, effectively hybridizing ordinary and Dirac
fermions. The anisotropy of energy dispersion can be further modulated through
band tilting along either spatial direction of the wave vector. We propose a
new definition of tilt parameter to characterize Lifshitz phases in 2D
semi-Dirac bands tilting along the quadratically dispersing direction. Using
linear response theory, we theoretically investigate the interband optical
conductivity of 2D tilted semi-Dirac bands. Our analytical zero-temperature
results reveal pronounced distinctions from Dirac and semi-Dirac systems
tilting along the linearly dispersing direction. Notably, we find that spectral
fixed point emerges in the optical conductivity over a specific range of the
tilt parameter, a phenomenon explained by the corresponding behavior of the
joint density of states. These findings provide a robust theoretical framework
for identifying and characterizing 2D tilted semi-Dirac materials and establish
clear spectral fingerprints that distinguish different kinds of 2D semi-Dirac
bands and Dirac bands. Our predictions can guide future experimental studies of
anisotropic band engineering and tilt-dependent phenomena.

</details>


### [315] [Intrinsic ultrafast edge photocurrent dynamics in WTe$_2$ driven by broken crystal symmetry](https://arxiv.org/abs/2510.06618)
*Subhashri Chatterjee,Katsumasa Yoshioka,Taro Wakamura,Vasili Perebeinos,Norio Kumada*

Main category: cond-mat.mes-hall

TL;DR: WTe$_2$中的方向性光电流可实现高速、无偏光探测，本文通过实验揭示了其亚皮秒尺度下的光电流动力学，并发现了在150 K以下由Lifshitz跃迁引起的瞬态双极响应，这为开发自供电超快光电器件提供了新途径。


<details>
  <summary>Details</summary>
Motivation: WTe$_2$作为一种二维材料，其对称性破缺诱导的光电流具有潜在的高速、无偏光探测应用前景，但其动力学机制尚不明确。

Method: 通过欧姆接触，在300 K至4 K的温度范围内，利用超快光学手段直接测量了WTe$_2$的亚皮秒尺度下的边缘光电流动力学。

Result: 实现了~250 GHz的3 dB带宽，并观察到在150 K以下净光电流方向在皮秒时间尺度上发生转换，这种瞬态双极响应源于热电子和热空穴的不对称冷却导致非平衡塞贝克效应。

Conclusion: 揭示了WTe$_2$中隐藏的超快动力学，为区分竞争光电流机制提供了新策略，并为开发自供电、超快光电器件开辟了新方向。

Abstract: Directional photocurrents in two-dimensional materials arise from broken
crystal symmetry, offering pathways to high-speed, bias-free photodetection
beyond conventional devices. Tungsten ditelluride (WTe$_2$), a type-II Weyl
semimetal, exhibits robust symmetry-breaking-induced edge photocurrents from
competing nonlinear optical and photothermoelectric (PTE) mechanisms, whose
intrinsic dynamics have remained experimentally inaccessible. Here, we directly
resolve sub-picosecond edge photocurrent dynamics in WTe$_2$ through ohmic
contacts over temperatures from 300 K to 4 K. We demonstrate ultrafast
optical-to-electrical conversion with a 3 dB bandwidth of $\sim$250 GHz and
reveal picosecond-timescale switching of the net photocurrent direction below
150 K, linked to a Lifshitz transition. This transient bipolar response arises
from non-equilibrium Seebeck effects due to asymmetric cooling of hot electrons
and holes. These findings reveal previously hidden ultrafast dynamics in
symmetry-engineered materials, offering new strategies to disentangle competing
photocurrent mechanisms and enabling the development of self-powered, ultrafast
optoelectronic devices.

</details>


### [316] [Signatures of broken symmetries in the excitations of a periodic 2DEG coupled to a cylindrical photon cavity](https://arxiv.org/abs/2510.06862)
*Vidar Gudmundsson,Vram Mughnetsyan,Hsi-Sheng Goan,Jeng-Da Chai,Nzar Rauf Abdullah,Chi-Shung Tang,Wen-Hsuan Kuan,Valeriu Moldoveanu,Andrei Manolescu*

Main category: cond-mat.mes-hall

TL;DR: 在此体系中，我们通过结合密度泛函理论和构型相互作用方法，研究了二维电子气在磁场和光子腔中的行为，并观察到了弱手征效应，该效应随电子-光子相互作用和磁场强度的增加而变化。


<details>
  <summary>Details</summary>
Motivation: 研究周期性侧向超晶格中二维电子气在磁场和光子腔作用下由静电和动态引起的对称性破缺效应，特别是电子-光子相互作用和手征激发脉冲的影响。

Method: 将电子库仑相互作用用密度泛函理论描述，电子-光子相互作用用构型相互作用形式主义处理，并结合了密度泛函方法。

Result: 观察到弱手征效应，该效应随电子-光子相互作用和磁场强度的增加而改变。发现了抗磁和顺磁电子-光子相互作用之间的重要联系，并且当相互作用强度增加时，在本体系中促进了抗磁相互作用。此外，方形阵列晶胞中的不对称势激活了在更高对称性晶胞中不存在的集体振荡模式。

Conclusion: 在特定条件下，电子-光子相互作用和磁场会影响体系的手征特性，并且体系的晶胞对称性会影响其集体振荡模式。

Abstract: In a two-dimensional electron gas (2DEG) in a periodic lateral superlattice
subjected to an external homogeneous magnetic field and in a cylindrical
far-infrared photon cavity we search for effects of broken symmetries: Static
ones, stemming from the unit cell of the system, and the external magnetic
field together with the dynamic ones caused by the vector potential of the
cavity promoting magnetic types of transitions, and the chirality of the
excitation pulse. The Coulomb interaction of the electrons is described within
density functional theory, but the electron-photon interactions are handled by
a configuration interaction formalism within each step of the density
functional approach, both for the static and the dynamic system. In the
dynamical calculations we observe weak chiral effects that change character as
the strength of the electron-photon interaction and the external magnetic field
are increased. From the analysis of the chiral effects we identify an important
connection of the para- and diamagnetic electron-photon interactions that
promotes the diamagnetic interaction in the present system when the interaction
strength is increased. Furthermore, the asymmetric potential in the unit cell
of the square array activates collective oscillation modes that are not present
in the system when the unit cell has a higher symmetry.

</details>


### [317] [Magnetic-Field-Induced Geometric Response of Mean-Field Projectors: Streda Formula and Orbital Magnetization](https://arxiv.org/abs/2510.07001)
*Jihang Zhu,Chunli Huang*

Main category: cond-mat.mes-hall

TL;DR: 在平均场理论中使用微扰论研究了相互作用电子系统的磁场响应，发现线性响应仅取决于波函数导数和Berry连接，与相互作用势和准粒子色散无关，并推导了斯特雷达公式和轨道磁化公式的紧凑、规范不变投影表达式，阐明了交换和自洽在定义电流顶点中的作用，建立了平均场理论、量子几何和非相互作用拓扑能带理论之间的直接联系。


<details>
  <summary>Details</summary>
Motivation: 研究相互作用电子系统在磁场下的响应，特别是平均场理论中的线性响应性质。

Method: 使用微扰论研究了平均场理论中的相互作用电子系统，并推导了相关公式。

Result: 发现线性响应是纯粹几何的，仅取决于波函数导数和Berry连接，与相互作用势和准粒子色散无关，并得到了斯特雷达公式和轨道磁化公式的紧凑、规范不变投影表达式。

Conclusion: 平均场理论、量子几何和非相互作用拓扑能带理论之间存在直接联系。

Abstract: We study the magnetic-field response of interacting electron systems within
mean-field theory using perturbation theory. We show that the linear response
of the mean-field density-matrix to a weak magnetic field is purely geometric:
it depends only on wavefunction derivatives, the Berry connections linking the
occupied and unoccupied subspaces, and is independent of the interaction
potential and the quasiparticle dispersion. This leads to compact,
gauge-invariant projector expressions for both the St\v{r}eda formula and the
formula for orbital magnetization. Our calculation explicitly elucidates the
role of exchange and self-consistency in defining current vertices for orbital
magnetization calculations. Our work establishes a direct connection between
mean-field theory, quantum geometry and the non-interacting topological band
theory.

</details>


### [318] [Thermal gradient-driven skyrmion dynamics with near-zero skyrmion Hall angle](https://arxiv.org/abs/2510.07020)
*Yogesh Kumar,Hurmal Saren,Pintu Das*

Main category: cond-mat.mes-hall

TL;DR: 热梯度驱动的斯格明子动力学为绿色自旋电子学提供了一条有希望的途径，能够利用废热进行信息传输和处理。使用微磁模拟，我们研究了 Co-Pt 双层纳米跑道中的尼尔斯格明子，并证明了热梯度引起的随机扭矩驱动斯格明子向着热区域运动，且霍尔角几乎消失。动力学敏感地依赖于内在材料参数——斯格明子速度随着阻尼常数的增加而降低，随着更强的热梯度的增加而增加，并随着饱和磁化强度、界面 DMI 强度和单轴面外各向异性的系统变化。重要的是，我们确定了一个特定的材料参数范围，在此范围内，斯格明子速度急剧变化，而霍尔角仍然被强烈抑制，接近于零饱和。这项全面的参数相关研究为最小化热梯度驱动的自旋电子系统中的霍尔效应建立了一个通用的设计框架。


<details>
  <summary>Details</summary>
Motivation: 利用废热进行信息传输和处理，以实现绿色自旋电子学。

Method: 使用微磁模拟研究 Co-Pt 双层纳米跑道中的尼尔斯斯格明子。

Result: 热梯度引起的随机扭矩驱动斯格明子向着热区域运动，且霍尔角几乎消失。斯格明子速度随着阻尼常数的增加而降低，随着更强的热梯度的增加而增加，并随着饱和磁化强度、界面 DMI 强度和单轴面外各向异性的系统变化。在特定的材料参数范围内，斯格明子速度急剧变化，而霍尔角仍然被强烈抑制，接近于零饱和。

Conclusion: 这项全面的参数相关研究为最小化热梯度驱动的自旋电子系统中的霍尔效应建立了一个通用的设计框架。

Abstract: Thermal gradient driven skyrmion dynamics offers a promising route toward
green spintronics, enabling the utilization of waste heat for information
transport and processing. Using micromagnetic simulations, we investigate Neel
skyrmions in a Co-Pt bilayer nanoracetrack and demonstrate that stochastic
torques induced by a thermal gradient drive skyrmion motion toward the hotter
region with a nearly vanishing Hall angle. The dynamics depends sensitively on
intrinsic material parameters - the skyrmion velocity decreases with increasing
damping constant, increases with stronger thermal gradients, and varies
systematically with saturation magnetization, interfacial DMI strength, and
uniaxial out of plane anisotropy. Importantly, we identify a specific range of
material parameters within which the skyrmion velocity changes sharply while
the Hall angle remains strongly suppressed, saturating near zero. This
comprehensive parameter-dependent study establishes a universal design
framework for minimizing the Hall effect in thermal gradient driven spintronic
systems.

</details>


### [319] [Noninteracting tight-binding models for Fock parafermions](https://arxiv.org/abs/2510.07029)
*Edward McCann*

Main category: cond-mat.mes-hall

TL;DR: 文章将自旋1/2费米子映射到四态福克准粒子，构建了一维非相互作用福克准粒子的紧束缚模型。该模型具有单粒子实数能量谱，由单粒子能级乘以准粒子占据数加权叠加而成。单粒子能级可以通过对一个规模与系统尺寸线性相关的矩阵进行对角化得到，这些能级与非相互作用费米子模型（如Rice-Mele模型和Su-Schrieffer-Heeger模型）相同。文章还表明，非相互作用四态准粒子的热力学分布函数与映射到自旋1/2费米子的结果一致。最后，文章将此映射应用于Kitaev超导链模型，构建了相应的准粒子模型，并证明其拓扑相的基态是四重简并的，每重简并态由马约拉纳边缘模式的四重准粒子占据数区分。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于通过构建非相互作用的福克准粒子模型，为研究准粒子系统提供一种新的理论框架，并探索其与已知的费米子系统的联系，特别是利用映射关系来理解和构建拓扑相。

Method: 本文采用将自旋1/2费米子映射到四态福克准粒子的方法，构建了一维非相互作用福克准粒子的紧束缚模型。通过对角化一个与系统尺寸线性相关的矩阵来获得单粒子能级，并与Rice-Mele模型和Su-Schrieffer-Heeger模型进行对比。此外，文章还分析了非相互作用四态准粒子的热力学分布函数，并将其映射应用于Kitaev超导链模型。

Result: 研究构建了一维非相互作用福克准粒子的紧束缚模型，其能量谱由单粒子能级和准粒子占据数决定。发现该模型的单粒子能级与Rice-Mele和Su-Schrieffer-Heeger模型相同。热力学分布函数与费米子模型一致。将该方法应用于Kitaev链模型，发现拓扑相的基态是四重简并的，且由马约拉纳边缘模式的四重准粒子占据数区分。

Conclusion: 通过将自旋1/2费米子映射到四态福克准粒子，成功构建了非相互作用准粒子模型，并揭示了其与费米子模型的深刻联系。该方法为研究准粒子系统，特别是拓扑相，提供了一个有效的工具，并对Kitaev链模型的拓扑基态进行了新的解释。

Abstract: By mapping itinerant spin-$1/2$ fermions to four-state Fock parafermions, we
construct noninteracting tight-binding models for Fock parafermions in one
dimension. They have single-particle real energy spectra consisting of a sum of
single-particle energy levels each multiplied by a parafermionic occupation
number. The single-particle levels may be determined by diagonalizing a square
matrix whose order scales linearly with system size. These levels are the same
as those of noninteracting fermionic models, as we show explicitly for the
Rice-Mele model and the Su-Schrieffer-Heeger model. We show that the
thermodynamic distribution function for the occupation numbers of
noninteracting four-state parafermions is consistent with the mapping to
spin-$1/2$ fermions. We apply the mapping to create a parafermionic counterpart
of the Kitaev superconducting chain and show that the ground state in the
topological phase is fourfold degenerate, with each ground state distinguished
by the fourfold parafermionic occupation numbers of Majorana edge modes.

</details>


### [320] [Topology of the generalized Brillouin zone of one-dimensional models](https://arxiv.org/abs/2510.07214)
*Heming Wang,Janet Zhong,Shanhui Fan*

Main category: cond-mat.mes-hall

TL;DR: 非厄米系统中的广义布里渊区（GBZ）可以表现出丰富的拓扑特征，甚至可能不连通，这挑战了对非厄米系统能带和能隙的传统理解。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索非厄米系统中广义布里渊区（GBZ）的拓扑特征，挑战关于GBZ连通性的传统观点，并将其应用于解释奇异的能隙现象。

Method: 通过证明和讨论一组充分条件来确保GBZ的连通性，并展示GBZ如何因点隙特征而不连通，以及应用这种新的GBZ拓扑来解释开放边界体系中线隙关闭而点隙拓扑不变的现象。

Result: 研究发现，非厄米系统中的GBZ可能不连通，且连通分支数量可能多于能带数量。此外，还展示了开放边界体系中，在保持点隙拓扑不变的情况下，线隙可能关闭的奇异效应。

Conclusion: 本文的研究结果表明，非厄米系统中的GBZ具有丰富的拓扑特性，可能不连通，并且点隙和线隙之间存在复杂的相互作用。这些发现对理解非厄米系统的能带理论和拓扑性质具有重要意义，并为进一步研究GBZ相关的拓扑不变量和开放边界缠绕提供了新的视角。

Abstract: The generalized Brillouin zones (GBZs) are integral in the analysis of
non-Hermitian band structures. Conventional wisdom suggests that the GBZ should
be connected, where each point can be indexed by the real part of the
wavevector, similar to the Brillouin zone. Here we demonstrate rich topological
features of the GBZs in generic non-Hermitian one-dimensional models. We prove
and discuss a set of sufficient conditions for the model to ensure the
connectivity of its GBZ. In addition, we show that the GBZ can become
disconnected and have more connected components than the number of bands, which
results from the point-gap features of the band structure. This novel GBZ
topology is applied to further demonstrate a counterintuitive effect, where the
line gap of an open-boundary spectrum with sublattice symmetry may be closed
without changing its point-gap topology. Our results challenge the current
understanding of bands and gaps in non-Hermitian systems and highlight the need
to further investigate the topological effects associated with the GBZ
including topological invariants and open-boundary braiding.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [321] [On-Package Memory with Universal Chiplet Interconnect Express (UCIe): A Low Power, High Bandwidth, Low Latency and Low Cost Approach](https://arxiv.org/abs/2510.06513)
*Debendra Das Sharma,Swadesh Choudhary,Peter Onufryk,Rob Pelt*

Main category: cs.AR

TL;DR: UCIe 结合内存语义，通过重用 LPDDR6 和 HBM 内存，或使 DRAM 芯片原生支持 UCIe，实现更高的带宽密度、更低的延迟和功耗，以及更低的成本。


<details>
  <summary>Details</summary>
Motivation: 现有内存解决方案无法满足新兴计算应用（如 AI）对高带宽和低功耗的需求，面临“内存墙”的挑战。

Method: 1. 通过逻辑芯片增强 UCIe，重用 LPDDR6 和 HBM 内存，并将其连接到 SoC。
2. 提出 DRAM 芯片原生支持 UCIe。

Result: 与现有的 HBM4 和 LPDDR 内存解决方案相比，实现了显著的性能提升：带宽密度提高高达 10 倍，延迟降低高达 3 倍，功耗降低高达 3 倍，同时成本也更低。

Conclusion: 通过增强 UCIe 的内存语义，可以为整个计算领域提供高能效、高带宽且具成本效益的内存解决方案。

Abstract: Emerging computing applications such as Artificial Intelligence (AI) are
facing a memory wall with existing on-package memory solutions that are unable
to meet the power-efficient bandwidth demands. We propose to enhance UCIe with
memory semantics to deliver power-efficient bandwidth and cost-effective
on-package memory solutions applicable across the entire computing continuum.
We propose approaches by reusing existing LPDDR6 and HBM memory through a logic
die that connects to the SoC using UCIe. We also propose an approach where the
DRAM die natively supports UCIe instead of the LPDDR6 bus interface. Our
approaches result in significantly higher bandwidth density (up to 10x), lower
latency (up to 3x), lower power (up to 3x), and lower cost compared to existing
HBM4 and LPDDR on-package memory solutions.

</details>


### [322] [RTGS: Real-Time 3D Gaussian Splatting SLAM via Multi-Level Redundancy Reduction](https://arxiv.org/abs/2510.06644)
*Leshu Li,Jiayin Qin,Jie Peng,Zishen Wan,Huaizhi Qu,Ye Han,Pingqing Zheng,Hongsen Zhang,Yu,Cao,Tianlong Chen,Yang,Zhao*

Main category: cs.AR

TL;DR: RTGS是一个算法-硬件协同设计框架，通过优化3DGS-SLAM管线中的冗余，实现了在资源受限的边缘设备上的实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS SLAM系统在资源受限的边缘设备上速度不足，需要解决这一问题以实现实时应用。

Method: RTGS通过（1）自适应高斯剪枝和（2）动态下采样技术来优化算法层面，并结合（1）基于WSU的流式和调度策略、（2）R&B缓冲区和（3）GMU来优化硬件层面，以减少冗余并提高效率。

Result: RTGS在四个数据集和三种算法上实现了实时性能（>= 30 FPS），能效比基线提高了82.5倍，且质量损失可忽略不计。

Conclusion: RTGS成功地解决了3DGS-SLAM在边缘设备上的性能瓶颈，通过软硬件协同设计实现了实时性和高能效。

Abstract: 3D Gaussian Splatting (3DGS) based Simultaneous Localization and Mapping
(SLAM) systems can largely benefit from 3DGS's state-of-the-art rendering
efficiency and accuracy, but have not yet been adopted in resource-constrained
edge devices due to insufficient speed. Addressing this, we identify notable
redundancies across the SLAM pipeline for acceleration. While conceptually
straightforward, practical approaches are required to minimize the overhead
associated with identifying and eliminating these redundancies. In response, we
propose RTGS, an algorithm-hardware co-design framework that comprehensively
reduces the redundancies for real-time 3DGS-SLAM on edge. To minimize the
overhead, RTGS fully leverages the characteristics of the 3DGS-SLAM pipeline.
On the algorithm side, we introduce (1) an adaptive Gaussian pruning step to
remove the redundant Gaussians by reusing gradients computed during
backpropagation; and (2) a dynamic downsampling technique that directly reuses
the keyframe identification and alpha computing steps to eliminate redundant
pixels. On the hardware side, we propose (1) a subtile-level streaming strategy
and a pixel-level pairwise scheduling strategy that mitigates workload
imbalance via a Workload Scheduling Unit (WSU) guided by previous iteration
information; (2) a Rendering and Backpropagation (R&B) Buffer that accelerates
the rendering backpropagation by reusing intermediate data computed during
rendering; and (3) a Gradient Merging Unit (GMU) to reduce intensive memory
accesses caused by atomic operations while enabling pipelined aggregation.
Integrated into an edge GPU, RTGS achieves real-time performance (>= 30 FPS) on
four datasets and three algorithms, with up to 82.5x energy efficiency over the
baseline and negligible quality loss. Code is available at
https://github.com/UMN-ZhaoLab/RTGS.

</details>


### [323] [Hardware-Efficient CNNs: Interleaved Approximate FP32 Multipliers for Kernel Computation](https://arxiv.org/abs/2510.06767)
*Bindu G Gowda,Yogesh Goyal,Yash Gupta,Madhav Rao*

Main category: cs.AR

TL;DR: FP32乘法计算成本高，可通过近似FP32乘法器优化CNN推理的硬件效率，并使用NSGA-II算法优化近似乘法器的排列。


<details>
  <summary>Details</summary>
Motivation: FP32乘法计算成本高且需要复杂硬件，但许多应用（如神经网络推理）不需要完美精度，可以牺牲精度来提高效率。

Method: 提出使用误差可变的近似压缩器来近似CNN推理中的FP32乘法器，并使用NSGA-II算法优化近似乘法器在卷积层内核中的放置和排序。

Result: 使用近似FP32乘法器优化CNN推理的硬件效率，并在精度和硬件效率之间取得平衡。

Conclusion: 通过使用近似FP32乘法器和NSGA-II优化，可以有效提高CNN推理的硬件效率，同时在精度和效率之间取得良好平衡。

Abstract: Single-precision floating point (FP32) data format, defined by the IEEE 754
standard, is widely employed in scientific computing, signal processing, and
deep learning training, where precision is critical. However, FP32
multiplication is computationally expensive and requires complex hardware,
especially for precisely handling mantissa multiplication. In practical
applications like neural network inference, perfect accuracy is not always
necessary, minor multiplication errors often have little impact on final
accuracy. This enables trading precision for gains in area, power, and speed.
This work focuses on CNN inference using approximate FP32 multipliers, where
the mantissa multiplication is approximated by employing error-variant
approximate compressors, that significantly reduce hardware cost. Furthermore,
this work optimizes CNN performance by employing differently approximated FP32
multipliers and studying their impact when interleaved within the kernels
across the convolutional layers. The placement and ordering of these
approximate multipliers within each kernel are carefully optimized using the
Non-dominated Sorting Genetic Algorithm-II, balancing the trade-off between
accuracy and hardware efficiency.

</details>


### [324] [Cocoon: A System Architecture for Differentially Private Training with Correlated Noises](https://arxiv.org/abs/2510.07304)
*Donghwan Kim,Xin Gu,Jinho Baek,Timothy Lo,Younghoon Min,Kwangsik Shin,Jongryool Kim,Jongse Park,Kiwan Maeng*

Main category: cs.AR

TL;DR: DP-SGD 通过在每次迭代中添加噪声来解决 ML 中的隐私问题，但这会降低模型准确性。该研究分析了旨在通过在迭代中抵消噪声来提高准确性的新方法，并提出了 Cocoon 框架，该框架通过 Cocoon-Emb 和 Cocoon-NMP 硬件加速器来解决效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私（DP）训练算法，如 DP-SGD，会因在每次迭代中添加噪声而降低模型准确性。尽管存在旨在通过抵消噪声来提高准确性的新方法，但它们在大模型或具有大型嵌入表的模型上会产生显著的开销。

Method: 对添加精心设计的相关噪声以抵消噪声以提高准确性的新方法进行了全面的分析，并提出了 Cocoon，一个硬件-软件协同设计的框架。Cocoon-Emb 通过预先计算并将相关噪声以合并格式存储来加速具有嵌入表的模型。Cocoon-NMP 使用定制的近内存处理设备来支持大型模型。

Result: 在基于 FPGA 的近内存处理设备原型上，Cocoon 在使用 Cocoon-Emb 时将性能提高了 2.33-10.82 倍，在使用 Cocoon-NMP 时将性能提高了 1.55-3.06 倍。

Conclusion: Cocoon 通过硬件-软件协同设计，有效地解决了使用相关噪声进行差分隐私训练的效率问题，尤其是在处理具有大型嵌入表或大型模型时。

Abstract: Machine learning (ML) models memorize and leak training data, causing serious
privacy issues to data owners. Training algorithms with differential privacy
(DP), such as DP-SGD, have been gaining attention as a solution. However,
DP-SGD adds a noise at each training iteration, which degrades the accuracy of
the trained model. To improve accuracy, a new family of approaches adds
carefully designed correlated noises, so that noises cancel out each other
across iterations. We performed an extensive characterization study of these
new mechanisms, for the first time to the best of our knowledge, and show they
incur non-negligible overheads when the model is large or uses large embedding
tables. Motivated by the analysis, we propose Cocoon, a hardware-software
co-designed framework for efficient training with correlated noises. Cocoon
accelerates models with embedding tables through pre-computing and storing
correlated noises in a coalesced format (Cocoon-Emb), and supports large models
through a custom near-memory processing device (Cocoon-NMP). On a real system
with an FPGA-based NMP device prototype, Cocoon improves the performance by
2.33-10.82x(Cocoon-Emb) and 1.55-3.06x (Cocoon-NMP).

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [325] [Quantum matrix arithmetics with Hamiltonian evolution](https://arxiv.org/abs/2510.06316)
*Christopher Kang,Yuan Su*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The efficient implementation of matrix arithmetic operations underpins the
speedups of many quantum algorithms. We develop a suite of methods to perform
matrix arithmetics -- with the result encoded in the off-diagonal blocks of a
Hamiltonian -- using Hamiltonian evolutions of input operators. We show how to
maintain this $\textit{Hamiltonian block encoding}$, so that matrix operations
can be composed one after another, and the entire quantum computation takes
$\leq 2$ ancilla qubits. We achieve this for matrix multiplication, matrix
addition, matrix inversion, Hermitian conjugation, fractional scaling, integer
scaling, complex phase scaling, as well as singular value transformation for
both odd and even polynomials. We also present an overlap estimation algorithm
to extract classical properties of Hamiltonian block encoded operators,
analogous to the well known Hadmard test, at no extra cost of qubit. Our
Hamiltonian matrix multiplication uses the Lie group commutator product formula
and its higher-order generalizations due to Childs and Wiebe. Our Hamiltonian
singular value transformation employs a dominated polynomial approximation,
where the approximation holds within the domain of interest, while the
constructed polynomial is upper bounded by the target function over the entire
unit interval. We describe a circuit for simulating a class of sum-of-squares
Hamiltonians, attaining a commutator scaling in step count, while leveraging
the power of matrix arithmetics to reduce the cost of each simulation step. In
particular, we apply this to the doubly factorized tensor hypercontracted
Hamiltonians from recent studies of quantum chemistry, obtaining further
improvements for initial states with a fixed number of particles. We achieve
this with $1$ ancilla qubit.

</details>


### [326] [Rényi and Tsallis information entropies for the Darboux III quantum nonlinear oscillator](https://arxiv.org/abs/2510.06221)
*Ignacio Baena-Jimenez,Angel Ballesteros,Ivan Gutierrez-Sagredo,Javier Relancio*

Main category: quant-ph

TL;DR: 该论文研究了一维 Darboux III 振荡器的量子化版本，重点是其熵矩和 Rényi 与 Tsallis 信息熵。


<details>
  <summary>Details</summary>
Motivation: 研究 Darboux III 振荡器的熵性质，特别是熵矩和 Rényi 与 Tsallis 信息熵，并分析非线性参数 λ 和熵参数 α 之间的相互作用。

Method: 推导了在一维 Darboux III 振荡器中位置空间的熵矩、Rényi 熵和 Tsallis 熵的解析表达式。由于傅里叶变换没有封闭形式，因此对动量空间的这些量进行了数值分析。还提出了一个在强非线性效应下有效的概率密度函数近似。

Result: 在一维 Darboux III 振荡器的位置空间中获得了熵矩、Rényi 熵和 Tsallis 熵的解析表达式。在动量空间中对这些量进行了数值分析。当 λ → 0 时，恢复了谐振子已知的熵结果。当 α → 1 时，恢复了 Shannon 熵的已知结果。

Conclusion: 在一维 Darboux III 振荡器中，熵参数 α 和非线性参数 λ 之间存在相互作用。当存在强非线性效应时（大 λ 值或高激发态），可以通过概率密度函数的近似来分析熵。

Abstract: The Darboux III oscillator is an exactly solvable $N$-dimensional nonlinear
oscillator defined on a radially symmetric space with non-constant negative
curvature. Its one-dimensional version can be seen as a position dependent mass
system whose mass function $\mu = (1 + \lambda x^2)$ depends on the
nonlinearity parameter $\lambda$, such that in the limit $\lambda \to 0$ the
harmonic oscillator is recovered. In this paper, a detailed study of the
entropic moments and of the R\'enyi and Tsallis information entropies for the
quantum version of the one-dimensional Darboux III oscillator is presented. In
particular, analytical expressions for the aforementioned quantities in
position space are obtained. Since the Fourier transform of the Darboux III
wave functions does not admit a closed form expression, a numerical analysis of
these quantities has been performed. Throughout the paper the interplay between
the entropy parameter $\alpha$ and the nonlinearity parameter $\lambda$ is
analysed, and known results for the Shannon entropy of the Darboux III and for
the R\'enyi and Tsallis entropies of the harmonic oscillator are recovered in
the limits $\alpha \to 1$ and $\lambda \to 0$, respectively. Finally, motivated
by the strong non-linear effects arising when large values of $\lambda$ and/or
highly excited states are considered, an approximation to the probability
density function valid in those regimes is presented. From it, an analytical
approximation to the probability density in momentum space can be obtained, and
some of the previously observed effects arising from the interplay between
$\alpha$ and $\lambda$ can be explained.

</details>


### [327] [Layerwise Federated Learning for Heterogeneous Quantum Clients using Quorus](https://arxiv.org/abs/2510.06228)
*Jason Han,Nicholas S. DiBrita,Daniel Leeds,Jianqiang Li,Jason Ludmir,Tirthak Patel*

Main category: quant-ph

TL;DR: Quorus是一个用于量子联邦学习的新解决方案，它通过使用分层损失函数来有效训练不同深度的量子模型，从而解决了异构量子计算机训练的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于关键数据分散在私有客户端，需要分布式量子机器学习（QML）的量子联邦学习（QFL）格式。然而，不同客户端的量子计算机可能存在错误且具有不同的错误特性，这需要它们运行不同深度的电路。

Method: Quorus利用分层损失函数来有效地训练不同深度的量子模型，允许客户端根据其个体能力选择模型以获得高保真度的输出。Quorus还提供了基于客户端需求的各种模型设计，以优化采样预算、量子比特数量、中途测量和优化空间。

Result: 模拟和真实硬件的结果表明Quorus的潜力：它增加了深度较大的客户端的梯度幅度，并将测试准确性平均提高了12.4%。

Conclusion: Quorus通过其分层损失函数和灵活的模型设计，有效地解决了量子联邦学习中异构和易出错的量子计算机的挑战，并在提高模型性能方面取得了显著成果。

Abstract: Quantum machine learning (QML) holds the promise to solve classically
intractable problems, but, as critical data can be fragmented across private
clients, there is a need for distributed QML in a quantum federated learning
(QFL) format. However, the quantum computers that different clients have access
to can be error-prone and have heterogeneous error properties, requiring them
to run circuits of different depths. We propose a novel solution to this QFL
problem, Quorus, that utilizes a layerwise loss function for effective training
of varying-depth quantum models, which allows clients to choose models for
high-fidelity output based on their individual capacity. Quorus also presents
various model designs based on client needs that optimize for shot budget,
qubit count, midcircuit measurement, and optimization space. Our simulation and
real-hardware results show the promise of Quorus: it increases the magnitude of
gradients of higher depth clients and improves testing accuracy by 12.4% on
average over the state-of-the-art.

</details>


### [328] [Breaking the Treewidth Barrier in Quantum Circuit Simulation with Decision Diagrams](https://arxiv.org/abs/2510.06775)
*Bin Cheng,Ziyuan Wang,Ruixuan Deng,Jianxin Chen,Zhengfeng Ji*

Main category: quant-ph

TL;DR: FeynmanDD是一种基于决策图的量子电路模拟方法，其计算成本与电路图的线性秩宽相关，在线性秩宽小于树宽的特定电路族中优于张量网络方法，并且通过Solovay-Kitaev算法可以克服门集限制。


<details>
  <summary>Details</summary>
Motivation: 评估FeynmanDD方法在模拟量子电路方面的性能，并与现有方法（如张量网络）进行比较。

Method: 严格分析了FeynmanDD方法，证明其决策图的大小与电路图的线性秩宽呈指数关系。

Result: FeynmanDD在某些电路族中优于张量网络方法，并证明了通过Solovay-Kitaev算法可以移除门集限制。

Conclusion: FeynmanDD是一种有前景的量子电路模拟方法，在特定条件下比现有方法更优越，并且具有良好的门集灵活性。

Abstract: Classical simulation of quantum circuits is a critical tool for validating
quantum hardware and probing the boundary between classical and quantum
computational power. Existing state-of-the-art methods, notably tensor network
approaches, have computational costs governed by the treewidth of the
underlying circuit graph, making circuits with large treewidth intractable.
This work rigorously analyzes FeynmanDD, a decision diagram-based simulation
method proposed in CAV 2025 by a subset of the authors, and shows that the size
of the multi-terminal decision diagram used in FeynmanDD is exponential in the
linear rank-width of the circuit graph. As linear rank-width can be
substantially smaller than treewidth and is at most larger than the treewidth
by a logarithmic factor, our analysis demonstrates that FeynmanDD outperforms
all tensor network-based methods for certain circuit families. We also show
that the method remains efficient if we use the Solovay-Kitaev algorithm to
expand arbitrary single-qubit gates to sequences of Hadamard and T gates,
essentially removing the gate-set restriction posed by the method.

</details>


### [329] [A Commuting Hamiltonian Framework for Quantum Time Transfer](https://arxiv.org/abs/2510.06256)
*Nicholas R. Allgood*

Main category: quant-ph

TL;DR: 该论文提出了一种基于可交换哈密顿量族和同步可观测量来量化量子时间传递的数学框架。


<details>
  <summary>Details</summary>
Motivation: 开发一种用于量子时间传递的数学框架，重点关注可交换哈密顿量族和同步可观测量。

Method: 使用可交换哈密顿量族和同步可观测量来定义同步子空间，并分析其在 $\epsilon$-相容动力学下的演化，以及在有限群对称性下的表示论分类。

Result: 证明了在 $\epsilon$-相容动力学下，时序相关性最多以与 $\epsilon$ 成正比的斜率线性退化；在存在有限群对称性的情况下，同步子空间等于张量积分解中的对角同种分量，并且同步保持由群作用的对易代数表征。

Conclusion: 将同步识别为算子代数的结构不变量，将近似对易、核保持动力学和对称性保护联系起来，并为量子时间传递、分类和资源理论推广提供了一个框架。

Abstract: We develop a mathematical framework for quantum time transfer based on
commuting families of Hamiltonians and synchronization observables. The
synchronization subspace is defined as the kernel of a difference operator
between local clocks, and we show that this subspace is preserved exactly by a
commutative $*$-subalgebra of Hamiltonians compatible with the clocks. Our
first main result establishes \emph{perturbative stability}: for
$\epsilon$-compatible dynamics, where the commutator with the synchronization
operator is bounded in norm by $\epsilon$, we prove quantitative drift bounds
showing that timing correlations degrade at most linearly in time with slope
proportional to $\epsilon$. Our second main result provides a
\emph{representation-theoretic classification}: in the presence of a finite
group symmetry, the synchronization subspace coincides with the diagonal
isotypic component in the tensor product decomposition, and synchronization
preservation is characterized by the commutant algebra of the group action.
These results identify synchronization as a structural invariant of operator
algebras, connecting approximate commutation, kernel-preserving dynamics, and
symmetry protection. Beyond quantum time transfer, the framework suggests
categorical and resource-theoretic generalizations and contributes to the
broader study of operator-algebraic invariants in multipartite quantum
dynamics.

</details>


### [330] [Reconquering Bell sampling on qudits: stabilizer learning and testing, quantum pseudorandomness bounds, and more](https://arxiv.org/abs/2510.06848)
*Jonathan Allcock,Joao F. Doriguello,Gábor Ivanyos,Miklos Santha*

Main category: quant-ph

TL;DR: 本论文提出了一种将贝尔采样推广到任意维度（qudits）的有用方法，克服了以往研究的困难，并在此基础上解决了几个关于稳定器态和魔态度量的问题。


<details>
  <summary>Details</summary>
Motivation: 以往的贝尔采样方法仅限于量子比特（qubits），未能有效地推广到任意维度（qudits）的系统，这限制了其在更广泛量子信息问题中的应用。

Method: 提出了一种基于拉格朗日四平方定理的新型幺正变换，可以将四个稳定器态（stabiliser state）映射到其复共轭态（complex conjugate state）的四个副本（相差一个泡利算符），并利用该变换构建了新的贝尔采样技术。

Result: 1. 实现了在 $O(n^3)$ 时间和 $O(n)$ 样本内学习稳定器态。
2. 在 $\tilde{O}(n^3/\varepsilon)$ 时间和 $\tilde{O}(n/\varepsilon)$ 样本内解决了隐藏稳定器群问题。
3. 在 $\tilde{O}(n^3/\varepsilon)$ 时间和 $\tilde{O}(n/\varepsilon)$ 样本内，能够判断一个量子态是否具有至少 $d^t$ 的稳定器大小，或者与其所有此类状态的距离超过 $\varepsilon$。
4. 证明了包含不超过 $n/2$ 个单-qudit 非Clifford门（non-Clifford gates）的Clifford电路无法制备假随机态（pseudorandom states）。
5. 提出了在 $O(d^2/\varepsilon_2)$ 或 $O(d^2/\varepsilon_2^2)$ 样本内测试量子态是否具有至少 $1-\varepsilon_1$ 或至多 $1-\varepsilon_2$ 的稳定器保真度（stabiliser fidelity）的方法。

Conclusion: 本研究成功地将贝尔采样推广到了任意维度的量子系统，并利用这一新工具有效地解决了多个关键的量子信息处理问题，为量子计算和量子信息理论的研究提供了新的途径。

Abstract: Bell sampling is a simple yet powerful tool based on measuring two copies of
a quantum state in the Bell basis, and has found applications in a plethora of
problems related to stabiliser states and measures of magic. However, it was
not known how to generalise the procedure from qubits to $d$-level systems --
qudits -- for all dimensions $d > 2$ in a useful way. Indeed, a prior work of
the authors (arXiv'24) showed that the natural extension of Bell sampling to
arbitrary dimensions fails to provide meaningful information about the quantum
states being measured. In this paper, we overcome the difficulties encountered
in previous works and develop a useful generalisation of Bell sampling to
qudits of all $d\geq 2$. At the heart of our primitive is a new unitary, based
on Lagrange's four-square theorem, that maps four copies of any stabiliser
state $|\mathcal{S}\rangle$ to four copies of its complex conjugate
$|\mathcal{S}^\ast\rangle$ (up to some Pauli operator), which may be of
independent interest. We then demonstrate the utility of our new Bell sampling
technique by lifting several known results from qubits to qudits for any $d\geq
2$:
  1. Learning stabiliser states in $O(n^3)$ time with $O(n)$ samples;
  2. Solving the Hidden Stabiliser Group Problem in
$\tilde{O}(n^3/\varepsilon)$ time with $\tilde{O}(n/\varepsilon)$ samples;
  3. Testing whether $|\psi\rangle$ has stabiliser size at least $d^t$ or is
$\varepsilon$-far from all such states in $\tilde{O}(n^3/\varepsilon)$ time
with $\tilde{O}(n/\varepsilon)$ samples;
  4. Clifford circuits with at most $n/2$ single-qudit non-Clifford gates
cannot prepare pseudorandom states;
  5. Testing whether $|\psi\rangle$ has stabiliser fidelity at least
$1-\varepsilon_1$ or at most $1-\varepsilon_2$ with $O(d^2/\varepsilon_2)$
samples if $\varepsilon_1 = 0$ or $O(d^2/\varepsilon_2^2)$ samples if
$\varepsilon_1 = O(d^{-2})$.

</details>


### [331] [Toward Uncertainty-Aware and Generalizable Neural Decoding for Quantum LDPC Codes](https://arxiv.org/abs/2510.06257)
*Xiangjun Mi,Frank Mueller*

Main category: quant-ph

TL;DR: QuBA和SAGU是用于量子纠错的新型机器学习解码器，它们在降低逻辑错误率和提高泛化能力方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的基于机器学习的量子纠错解码器缺乏不确定性量化和对未见过的量子代码的泛化能力，阻碍了其实际应用。

Method: 提出了一种名为QuBA的贝叶斯图神经网络解码器，并结合了点积和多头注意力机制，以实现精确的错误模式识别和可靠的不确定性估计。在此基础上，开发了SAGU，一个多代码训练框架，增强了跨领域鲁棒性。

Result: QuBA和SAGU在逻辑错误率方面比经典的信念传播（BP）算法有显著提高（平均降低一个数量级，在某些情况下高达两个数量级）。QuBA也优于现有的神经解码器，而SAGU在解码性能上可与QuBA相媲美，甚至在某些情况下优于QuBA的领域特定训练方法。

Conclusion: QuBA和SAGU在提高量子纠错解码的准确性和泛化能力方面取得了显著进展，为实现可扩展的容错量子计算铺平了道路。

Abstract: Quantum error correction (QEC) is essential for scalable quantum computing,
yet decoding errors via conventional algorithms result in limited accuracy
(i.e., suppression of logical errors) and high overheads, both of which can be
alleviated by inference-based decoders. To date, such machine-learning (ML)
decoders lack two key properties crucial for practical fault tolerance:
reliable uncertainty quantification and robust generalization to previously
unseen codes. To address this gap, we propose \textbf{QuBA}, a Bayesian graph
neural decoder that integrates attention to both dot-product and multi-head,
enabling expressive error-pattern recognition alongside calibrated uncertainty
estimates. Building on QuBA, we further develop \textbf{SAGU
}\textbf{(Sequential Aggregate Generalization under Uncertainty)}, a multi-code
training framework with enhanced cross-domain robustness enabling decoding
beyond the training set. Experiments on bivariate bicycle (BB) codes and their
coprime variants demonstrate that (i) both QuBA and SAGU consistently
outperform the classical baseline belief propagation (BP), achieving a
reduction of on average \emph{one order of magnitude} in logical error rate
(LER), and up to \emph{two orders of magnitude} under confident-decision bounds
on the coprime BB code $[[154, 6, 16]]$; (ii) QuBA also surpasses
state-of-the-art neural decoders, providing an advantage of roughly \emph{one
order of magnitude} (e.g., for the larger BB code $[[756, 16, \leq34]]$) even
when considering conservative (safe) decision bounds; (iii) SAGU achieves
decoding performance comparable to or even outperforming QuBA's domain-specific
training approach.

</details>


### [332] [A Duality Theorem for Classical-Quantum States with Applications to Complete Relational Program Logics](https://arxiv.org/abs/2510.07051)
*Gilles Barthe,Minbo Gao,Jam Kabeer Ali Khan,Matthijs Muis,Ivan Renison,Keiya Sakabe,Michael Walter,Yingte Xu,Li Zhou*

Main category: quant-ph

TL;DR: 本文为经典-量子程序建立了新的对偶定理和关系程序逻辑cqOTL，并放宽了现有概率和量子程序逻辑的完整性限制。


<details>
  <summary>Details</summary>
Motivation: 现有技术未能提供针对结合经典和量子计算的经典-量子程序的完备关系程序逻辑，因为现有的对偶定理推导方法（最优输运和半定规划）不适用于此混合设置。

Method: 提出了一种新颖的、与维度无关的分析方法，解决了经典-量子状态下的凸优化问题，并利用该方法得出了所需的对偶定理。基于此对偶定理，建立了名为cqOTL的新关系程序逻辑的可靠性和完备性。

Result: 成功建立了适用于经典-量子程序的对偶定理和关系程序逻辑cqOTL。此外，还放宽了概率程序逻辑eRHL和量子程序逻辑qOTL的完整性限制。

Conclusion: 本文成功弥合了经典-量子程序逻辑的差距，为该领域的研究奠定了基础，并为未来的研究开辟了新的可能性。

Abstract: Duality theorems play a fundamental role in convex optimization. Recently, it
was shown how duality theorems for countable probability distributions and
finite-dimensional quantum states can be leveraged for building relatively
complete relational program logics for probabilistic and quantum programs,
respectively. However, complete relational logics for classical-quantum
programs, which combine classical and quantum computations and operate over
classical as well as quantum variables, have remained out of reach. The main
gap is that while prior duality theorems could readily be derived using optimal
transport and semidefinite programming methods, respectively, the combined
setting falls out of the scope of these methods and requires new ideas. In this
paper, we overcome this gap and establish the desired duality theorem for
classical-quantum states. Our argument relies critically on a novel
dimension-independent analysis of the convex optimization problem underlying
the finite-dimensional quantum setting, which, in particular, allows us to take
the limit where the classical state space becomes infinite. Using the resulting
duality theorem, we establish soundness and completeness of a new relational
program logic, called $\mathsf{cqOTL}$, for classical-quantum programs. In
addition, we lift prior restrictions on the completeness of two existing
program logics: $\mathsf{eRHL}$ for probabilistic programs (Avanzini et al.,
POPL 2025) and $\mathsf{qOTL}$ for quantum programs (Barthe et al., LICS 2025).

</details>


### [333] [Randomized Quantum Singular Value Transformation](https://arxiv.org/abs/2510.06851)
*Xinzhao Wang,Yuxin Zhang,Soumyabrata Hazra,Tongyang Li,Changpeng Shao,Shantanav Chakraborty*

Main category: quant-ph

TL;DR: 我们提出了两种新的随机化量子奇异值变换（QSVT）算法，它们使用更少的辅助量子比特，并且避免了昂贵的块编码，在某些情况下比标准QSVT更有效，并为量子线性系统和基态性质估计提供了改进。


<details>
  <summary>Details</summary>
Motivation: 标准QSVT实现依赖于昂贵的块编码，具有多量子比特控制和线性的电路深度。

Method: （i）通过重要性采样直接随机化QSVT，取代块编码；（ii）将qDRIFT集成到广义量子信号处理框架中，通过经典外推实现指数级精度改进。

Result: 两种算法的门复杂度与哈密顿项数量无关，但与目标多项式次数有二次方依赖。在特定参数下，我们的方法优于标准QSVT。

Conclusion: 我们提出的随机化QSVT方法是一种实用且资源高效的替代方案，适用于早期容错量子设备，并在量子线性系统和基态性质估计方面优于先前的方法。

Abstract: We introduce the first randomized algorithms for Quantum Singular Value
Transformation (QSVT), a unifying framework for many quantum algorithms.
Standard implementations of QSVT rely on block encodings of the Hamiltonian,
which are costly to construct, requiring a logarithmic number of ancilla
qubits, intricate multi-qubit control, and circuit depth scaling linearly with
the number of Hamiltonian terms. In contrast, our algorithms use only a single
ancilla qubit and entirely avoid block encodings. We develop two methods: (i) a
direct randomization of QSVT, where block encodings are replaced by importance
sampling, and (ii) an approach that integrates qDRIFT into the generalized
quantum signal processing framework, with the dependence on precision
exponentially improved through classical extrapolation. Both algorithms achieve
gate complexity independent of the number of Hamiltonian terms, a hallmark of
randomized methods, while incurring only quadratic dependence on the degree of
the target polynomial. We identify natural parameter regimes where our methods
outperform even standard QSVT, making them promising for early fault-tolerant
quantum devices. We also establish a fundamental lower bound showing that the
quadratic dependence on the polynomial degree is optimal within this framework.
We apply our framework to two fundamental tasks: solving quantum linear systems
and estimating ground-state properties of Hamiltonians, obtaining polynomial
advantages over prior randomized algorithms. Finally, we benchmark our
ground-state property estimation algorithm on electronic structure Hamiltonians
and the transverse-field Ising model with long-range interactions. In both
cases, our approach outperforms prior work by several orders of magnitude in
circuit depth, establishing randomized QSVT as a practical and
resource-efficient alternative for early fault-tolerant quantum devices.

</details>


### [334] [Asymptotic Vanishing of the Success Probability in Shor's Algorithm](https://arxiv.org/abs/2510.06271)
*João P. da Cruz*

Main category: quant-ph

TL;DR: Shor's算法在N->infinity时成功概率会衰减，证明了其“期望多项式时间”仅是局部定义的。


<details>
  <summary>Details</summary>
Motivation: Shor's算法的成功保证不适用于渐进情况，需要研究其在N->infinity时的行为。

Method: 通过分析模N的乘法群Omega_N的概率空间，并结合蒙特卡洛模拟来验证理论推导。

Result: 证明了Shor's算法的成功概率随N的增大而衰减（与1/phi(N)成正比），并且没有渐进的固定成功概率，证实了均匀测度的集合不存在弱极限，且渐近地失去遍历性。

Conclusion: Shor's算法的“期望多项式时间”仅是局部定义的，不存在全局期望。这种成功概率的渐进消失解释了为何在实际中没有大规模实现Shor's算法，并为量子分解的可扩展性设定了基本限制。

Abstract: Shor's factoring algorithm guarantees a success probability of at least one
half for any fixed modulus N = pq with distinct primes p and q. We show that
this guarantee does not extend to the asymptotic regime. As N -> infinity, the
multiplicative groups Omega_N = (Z/NZ)^x form a non-tight family of probability
spaces, and the probability weight associated with successful bases,
proportional to p(success | a', N) p(a' | N), decays as 1/phi(N). The ensemble
of uniform measures {mu_N} therefore admits no weak limit, implying an
asymptotic loss of ergodicity. Monte Carlo simulations up to N <= 10^6 confirm
this decay and the absence of a stationary success probability. These results
demonstrate that the "expected polynomial time" in order finding is only
locally defined: no global expectation exists once the arithmetic domain
expands. The asymptotic vanishing of success probability explains the empirical
absence of large-N implementations of Shor's algorithm and sets a fundamental
limit on the scalability of quantum factoring.

</details>


### [335] [Prediction of Molecular Single-Photon Emitters: A Materials-Modelling Approach](https://arxiv.org/abs/2510.06407)
*Erik Karlsson Öhman,Daqing Wang,R. Matthias Geilhufe,Christian Schäfer*

Main category: quant-ph

TL;DR: 分子发光体是量子技术的基础，本文提出了一种结合数据库分析和微观预测的理论计算框架，用于探索和识别具有特定性能的分子发光体，并以二苯并苝为模型系统进行了验证，发现了新的候选分子，包括一个手性分子发光体，并展望了结合机器学习的未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 分子发光体在量子技术中具有设计灵活性的优势，但其巨大的构型空间给理解和探索带来了挑战。

Method: 提出一个理论计算框架，该框架整合了数据库分析和微观预测，并使用二苯并苝-蒽作为模型系统进行基准测试。

Result: 该框架能够识别出有潜力的新的分子发光体候选者，其中包括一个手性分子发光体。

Conclusion: 所提出的框架为探索分子量子光-物质相互作用的巨大潜力提供了一个起点，并展望了结合机器学习的未来发展，有望充分发挥其潜力。

Abstract: Interfacing light with quantum systems is an integral part of quantum
technology, with the most essential building block being single-photon
emitters. Although various platforms exist, each with its individual strengths,
molecular emitters boast a unique advantage -- namely the flexibility to tailor
their design to fit the requirements of a specific task. However, the
characteristics of the vast space of possible molecular configurations are
challenging to understand and explore. Here, we present a theoretical and
computational framework to initiate exploration of this vast potential by
integrating database analysis with microscopic predictions. Using a model
system of dibenzoterrylene in an anthracene host as benchmark, our approach
identifies promising new candidates, among them a chiral molecular emitter.
Future extensions of our approach integrated with machine learning routines
hold the promise of ultimately unlocking the full potential of molecular
quantum light-matter interfaces.

</details>


### [336] [Quantum Sparse Recovery and Quantum Orthogonal Matching Pursuit](https://arxiv.org/abs/2510.06925)
*Armando Bellante,Stefano Vanerio,Stefano Zanero*

Main category: quant-ph

TL;DR: 该研究提出了一种名为量子正交匹配追踪（QOMP）的算法，用于在非正交、过完备字典中进行量子稀疏恢复，这是经典OMP算法的量子类似。


<details>
  <summary>Details</summary>
Motivation: 在量子态及其字典向量非正交且字典过完备的情况下，目标是用尽可能少的向量来重建量子态。研究首先证明了该问题的一般恢复是NP难的，排除了通用高效精确算法的可能性。

Method: 研究提出了量子正交匹配追踪（QOMP）算法，它结合了用于内积估计、最大值查找和块编码投影的量子子程序，并采用了一种避免迭代间误差累积的误差重置设计。

Result: 在标准互不相干和适定稀疏性假设下，QOMP算法能在多项式时间内精确恢复K稀疏态的支持集。此外，该算法还被应用于稀疏量子态层析成像，在特定条件下实现了优于经典方法复杂度的查询复杂度。

Conclusion: QOMP算法在非正交、过完备字典的量子稀疏恢复问题上取得了显著进展，并为稀疏量子态层析成像提供了一个有效的框架，同时在QRAM模型中也展现出相对于经典OMP的优势。

Abstract: We study quantum sparse recovery in non-orthogonal, overcomplete
dictionaries: given coherent quantum access to a state and a dictionary of
vectors, the goal is to reconstruct the state up to $\ell_2$ error using as few
vectors as possible. We first show that the general recovery problem is
NP-hard, ruling out efficient exact algorithms in full generality. To overcome
this, we introduce Quantum Orthogonal Matching Pursuit (QOMP), the first
quantum analogue of the classical OMP greedy algorithm. QOMP combines quantum
subroutines for inner product estimation, maximum finding, and block-encoded
projections with an error-resetting design that avoids iteration-to-iteration
error accumulation. Under standard mutual incoherence and well-conditioned
sparsity assumptions, QOMP provably recovers the exact support of a $K$-sparse
state in polynomial time. As an application, we give the first framework for
sparse quantum tomography with non-orthogonal dictionaries in $\ell_2$ norm,
achieving query complexity $\widetilde{O}(\sqrt{N}/\epsilon)$ in favorable
regimes and reducing tomography to estimating only $K$ coefficients instead of
$N$ amplitudes. In particular, for pure-state tomography with $m=O(N)$
dictionary vectors and sparsity $K=\widetilde{O}(1)$ on a well-conditioned
subdictionary, this circumvents the $\widetilde{\Omega}(N/\epsilon)$ lower
bound that holds in the dense, orthonormal-dictionary setting, without
contradiction, by leveraging sparsity together with non-orthogonality. Beyond
tomography, we analyze QOMP in the QRAM model, where it yields polynomial
speedups over classical OMP implementations, and provide a quantum algorithm to
estimate the mutual incoherence of a dictionary of $m$ vectors in
$O(m/\epsilon)$ queries, improving over both deterministic and quantum-inspired
classical methods.

</details>


### [337] [Extended validations on photon number resolving detector based Gaussian boson sampling with low noises](https://arxiv.org/abs/2510.06300)
*Yang Ji,Yongzheng Wu,Shi Wang,Jie Hou,Zijian Wang,Bo Jiang*

Main category: quant-ph

TL;DR: 高斯玻色子采样（GBS）的噪声会影响输出模式并降低经典模拟的复杂度。本文提出了一种结合模式识别和相关性方法的定量评估噪声水平的方法，并使用输出分箱策略加速验证过程。模拟结果表明，即使在噪声较低的情况下，模式识别协议在GBS噪声评估方面也具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 噪声会影响高斯玻色子采样（GBS）的输出模式并降低其经典模拟的复杂度。

Method: 提出了一种结合模式识别和相关性方法的定量评估GBS噪声水平的方法，并使用输出分箱策略加速验证过程。

Result: 模拟结果表明，即使在噪声较低的情况下，模式识别协议在GBS噪声评估方面也具有鲁棒性。

Conclusion: 模式识别协议在GBS噪声评估方面具有鲁棒性。

Abstract: Gaussian boson sampling (GBS) is a variety of boson sampling overcoming the
stable single-photon preparation difficulty of the later. However, like those
in the original version, noises in GBS will also result in the deviation of
output patterns and the reduction of classical simulation complexity. We extend
the pattern recognition validation, together with the correlation approach as a
comparison, on GBS using photon number resolving detectors with noises of both
photon loss and distinguishability, to quantificationally evaluate noise
levels. As for the classical simulation with noises to be used during
validations, it is actually a simulation of mixed states where we employ an
existing photon-pair strategy to realize polynomial speedup locally.
Furthermore, we use an output-binning strategy to realize validation speedup.
Our simulation indicates that the pattern recognition protocol is robust on
noise evaluations of GBS even when noises are sufficiently low.

</details>


### [338] [Clifford testing: algorithms and lower bounds](https://arxiv.org/abs/2510.07164)
*Marcel Hinsche,Zongbo Bao,Philippe van Dordrecht,Jens Eisert,Jop Briët,Jonas Helsen*

Main category: quant-ph

TL;DR: 该论文提出了一个4查询的Clifford测试器，用于判断一个n比特酉变换是否为Clifford酉变换，或者与所有Clifford酉变换的距离至少为ε。该测试器以多项式ε的概率做出决定，并具有容错性。此外，论文还提出了一个单次访问的O(n)查询Clifford测试器，并给出了一个Ω(n^(1/4))的查询下界。


<details>
  <summary>Details</summary>
Motivation: 解决Clifford测试问题，即判断一个黑盒n比特酉变换是否为Clifford酉变换，或者与所有Clifford酉变换的距离至少为ε。

Method: 提出了一个4查询的Clifford测试器，并自适应地应用了容错稳定器测试技术。对于单次访问的场景，提出了一个O(n)查询的Clifford测试器，并证明了Ω(n^(1/4))的查询下界。利用了Clifford群的交换子群的结构。

Result: 提出了首个4查询Clifford测试器，该测试器具有容错性，并以多项式ε的概率解决Clifford测试问题。对于单次访问场景，提出了一个O(n)查询测试器，并证明了Ω(n^(1/4))的查询下界。

Conclusion: 证明了Clifford测试问题的多项式反演定理，并解决了Bu, Gu和Jaffe的一个猜想。为Clifford测试问题提供了高效的算法和下界。

Abstract: We consider the problem of Clifford testing, which asks whether a black-box
$n$-qubit unitary is a Clifford unitary or at least $\varepsilon$-far from
every Clifford unitary. We give the first 4-query Clifford tester, which
decides this problem with probability $\mathrm{poly}(\varepsilon)$. This
contrasts with the minimum of 6 copies required for the closely-related task of
stabilizer testing. We show that our tester is tolerant, by adapting techniques
from tolerant stabilizer testing to our setting. In doing so, we settle in the
positive a conjecture of Bu, Gu and Jaffe, by proving a polynomial inverse
theorem for a non-commutative Gowers 3-uniformity norm. We also consider the
restricted setting of single-copy access, where we give an $O(n)$-query
Clifford tester that requires no auxiliary memory qubits or adaptivity. We
complement this with a lower bound, proving that any such, potentially
adaptive, single-copy algorithm needs at least $\Omega(n^{1/4})$ queries. To
obtain our results, we leverage the structure of the commutant of the Clifford
group, obtaining several technical statements that may be of independent
interest.

</details>


### [339] [On quantum to classical comparison for Davies generators](https://arxiv.org/abs/2510.07267)
*Joao Basso,Shirshendu Ganguly,Alistair Sinclair,Nikhil Srivastava,Zachary Stier,Thuy-Duong Vuong*

Main category: quant-ph

TL;DR: 量子马尔可夫链的谱隙与嵌入式经典马尔可夫发生器的谱隙相当，除非哈密顿量包含长算术级数。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决量子马尔可夫链理解不如经典对应物的问题，并探讨量子和经典动力学的收敛性之间的关系。

Method: 通过证明任何“离对角”特征向量都可以用来构造一个与哈密顿量交换的可观测量，并且其Lindbladian瑞利商可以被原始特征向量的Lindbladian瑞利商的上界界定，从而推导出谱隙。

Result: 如果哈密顿量的谱不包含长算术级数，则量子和经典谱隙是相当的。对于一大类哈密顿量（包括通过扰动固定哈密顿量和通用外部场获得的哈密顿量），量子谱隙与经典谱隙保持在一个常数因子内。

Conclusion: 本文证明了在特定条件下，量子马尔可夫链的谱隙与其嵌入的经典马尔可夫发生器的谱隙相当，这使得经典马尔可夫链技术可以应用于量子领域。

Abstract: Despite extensive study, our understanding of quantum Markov chains remains
far less complete than that of their classical counterparts. [Temme'13]
observed that the Davies Lindbladian, a well-studied model of quantum Markov
dynamics, contains an embedded classical Markov generator, raising the natural
question of how the convergence properties of the quantum and classical
dynamics are related. While [Temme'13] showed that the spectral gap of the
Davies Lindbladian can be much smaller than that of the embedded classical
generator for certain highly structured Hamiltonians, we show that if the
spectrum of the Hamiltonian does not contain long arithmetic progressions, then
the two spectral gaps must be comparable. As a consequence, we prove that for a
large class of Hamiltonians, including those obtained by perturbing a fixed
Hamiltonian with a generic external field, the quantum spectral gap remains
within a constant factor of the classical spectral gap. Our result aligns with
physical intuition and enables the application of classical Markov chain
techniques to the quantum setting.
  The proof is based on showing that any ``off-diagonal'' eigenvector of the
Davies generator can be used to construct an observable which commutes with the
Hamiltonian and has a Lindbladian Rayleigh quotient which can be upper bounded
in terms of that of the original eigenvector's Lindbladian Rayleigh quotient.
Thus, a spectral gap for such observables implies a spectral gap for the full
Davies generator.

</details>


### [340] [High-dimensional detection-loophole-free measurement-device-independent quantum random number generator](https://arxiv.org/abs/2510.06317)
*Joakim Argillander,Daniel Spegel-Lexne,Martin Clason,Pedro R. Dieguez,Marcin Pawłowski,Anubhav Chaturvedi,Guilherme B. Xavier*

Main category: quant-ph

TL;DR: 本研究介绍了一种基于高维光子路径态的测量无关量子随机数生成器（MDI-QRNG），其生成速率超过1.2比特/轮和1.77 Mbit/s，且无需信任测量设备，适用于实际加密应用。


<details>
  <summary>Details</summary>
Motivation: 在密码学等安全关键领域，对随机数生成器进行认证具有挑战性。本研究旨在解决这一挑战，提供一种无需信任测量设备即可生成认证安全随机数的方法。

Method: 利用高维光子路径态，扩展了标准量子比特分束器QRNG到三输出版本，采用可调光纤干涉仪作为可调分束器，并使用超导探测器。

Result: 实现了超过1.2比特/轮和1.77 Mbit/s的认证安全私有随机数生成，且无需信任测量设备。

Conclusion: 本研究展示了认证安全的高维量子随机数生成能力，为实际、可扩展且无需复杂设备的QRNGs铺平了道路。

Abstract: Certifying random number generators is challenging, especially in
security-critical fields like cryptography. Here, we demonstrate a
measurement-device-independent quantum random number generator (MDI-QRNG) using
high-dimensional photonic path states. Our setup extends the standard qubit
beam-splitter QRNG to a three-output version with tunable fiber-optic
interferometers acting as tunable beam splitters and superconducting detectors.
This setup generates over 1.2 bits per round and 1.77 Mbits per second of
certifiably secure private randomness without requiring \emph{any} trust in the
measurement apparatus, a critical requirement for the security of real-world
cryptographic applications. Our results demonstrate certifiably secure
high-dimensional quantum random-number generation, paving the way for
practical, scalable QRNGs without the need for complex devices.

</details>


### [341] [Realistic Threat Models for Fiber and Free-Space Continuous-Variable Quantum Key Distribution](https://arxiv.org/abs/2510.06971)
*Zhiyue Zuo,Masoud Ghalaii,Stefano Pirandola*

Main category: quant-ph

TL;DR: CV-QKD协议在有损耗和噪声的情况下，其性能受到限制，但本研究提供了一个通用的安全分析框架，并评估了GMCS协议的鲁棒性，特别是在无线通信场景下，其性能优于地面量子中继器。


<details>
  <summary>Details</summary>
Motivation: 在未来的量子通信网络中，需要实现高安全性的通信和远距离的纠缠分发。CV-QKD因其使用室温设备和高速率潜力而备受关注，但其性能受损耗和噪声的限制。

Method: 提出一个通用的框架来分析GMCS协议在不同信任度下的可组合有限安全。考虑了主动/被动窃听、有线/无线信道等多种实际场景。

Result: 评估了GMCS协议在不同信任度下的鲁棒性，发现协议难以应对不可信的发射端损耗。在无线场景下，即使在最不可信的情况下，其密钥分率也优于地面量子中继器。

Conclusion: 在设计和优化量子安全网络时，必须考虑速率性能、信任度、系统噪声和通信距离之间的权衡，以克服CV-QKD的局限性。

Abstract: Future global quantum communication networks, or quantum Internet, will
realize high-rate secure communication and entanglement distribution for
large-scale users over long distances. Continuous variable (CV) quantum key
distribution (QKD) provides a powerful setting for secure quantum
communications, thanks to the use of room-temperature off-the-shelf optical
devices and the potential to reach high rates. However, the achievable
performance of CV-QKD protocols is fundamentally limited by the fact that they
appear to be fragile to both loss and noise. In this study, we provide a
general framework for analyzing the composable finite-size security of CV-QKD
with Gaussian-modulated coherent-state protocol (GMCS) under various levels of
trust for the loss and noise experienced by the users of the protocol. Our work
is comprehensive of several practical scenarios, encompassing both active and
passive eavesdropping configurations, with both wired (i.e., fiber-based) and
wireless (i.e., free-space and satellite-based) quantum communication channels.
Our numerical results evaluate the robustness of the GMCS protocol under
varying levels of trust and demonstrate that it is difficult for a practical
protocol to remain robust against untrusted loss at the transmitter. In the
wireless case, we analyze a scenario with a sun-synchronous satellite, showing
that its key distribution rate, even with the worst level of trust, can
outperform a ground chain of ideal quantum repeaters. Our results indicate
that, when it comes to engineering and optimizing quantum-safe networks, it is
essential to mitigate the shortcomings caused by critical trade-offs between
rate performance, trust level, system noise, and communication distance.

</details>


### [342] [An efficient algorithm to compute entanglement in states with low magic](https://arxiv.org/abs/2510.06318)
*ChunJun Cao,Gong Cheng,Tianci Zhou*

Main category: quant-ph

TL;DR: 开发了一种高效的经典算法，用于计算具有低稳定器零度特性的高度纠缠的量子多体魔态的冯诺依曼熵和纠缠谱。


<details>
  <summary>Details</summary>
Motivation: 为了分析魔术和纠缠之间的相互作用，需要计算高度纠缠的量子多体魔态的这些量。纠缠的有效提取还可以告知我们对动力学量子过程的理解，例如测量诱导的相变和近似酉设计。

Method: 该算法利用稳定器码的性质将纠缠分为两部分：一部分由共同的稳定器组生成，另一部分来自逻辑状态。低零度约束确保两个部分都可以有效地计算。

Result: 该算法能够有效地计算具有低稳定器零度特性的高度纠缠的量子多体魔态的冯诺依曼熵和纠缠谱。此外，该算法还可以应用于研究稀疏 T 掺杂电路中具有可能的 Pauli 测量的纠缠，以及某些具有高纠缠和魔术特性的状态类别。结合稳定器学习子程序，还可以有效地学习量子设备上制备的低零度状态的冯诺依曼熵。

Conclusion: 所开发的算法为研究具有高纠缠和魔术特性的量子多体系统中的纠缠提供了一种有效的方法。

Abstract: A bottleneck for analyzing the interplay between magic and entanglement is
the computation of these quantities in highly entangled quantum many-body magic
states. Efficient extraction of entanglement can also inform our understanding
of dynamical quantum processes such as measurement-induced phase transition and
approximate unitary designs. We develop an efficient classical algorithm to
compute the von Neumann entropy and entanglement spectrum for such states under
the condition that they have low stabilizer nullity. The algorithm exploits the
property of stabilizer codes to separate entanglement into two pieces: one
generated by the common stabilizer group and the other from the logical state.
The low-nullity constraint ensures both pieces can be computed efficiently. Our
algorithm can be applied to study the entanglement in sparsely $T$-doped
circuits with possible Pauli measurements as well as certain classes of states
that have both high entanglement and magic. Combining with stabilizer learning
subroutines, it also enables the efficient learning of von Neumann entropies
for low-nullity states prepared on quantum devices.

</details>


### [343] [Quantum advantage from random geometrically-two-local Hamiltonian dynamics](https://arxiv.org/abs/2510.06321)
*Yihui Quek*

Main category: quant-ph

TL;DR: 典型的二维局部哈密顿量演化在经典上是难以处理的，这为量子优势提供了一个有希望的实验方案。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在确定具有高斯系数的几何二维局部哈密顿量在恒定时间内演化是否会产生经典上难以处理的动力学，并为量子优势提供一个有前景的实验方案。

Method: 研究人员开发了一种最坏情况到平均情况的约简方法，用于近似（时间无关的）几何二维局部哈密顿量演化的输出概率。该方法利用多变量多项式插值，并通过高斯对称性简化为一系列单变量插值，克服了先前工作中随机哈密顿量缺乏隐藏对称性的问题。此外，他们还提出了一种 Berlekamp-Welch 算法的改进版本，用于处理有错误的评估。

Result: 研究结果表明，对于具有高斯系数的几何二维局部哈密顿量集合，在恒定时间内演化产生的动力学在经典上是难以处理的。

Conclusion: 该研究为几何二维局部哈密顿量演化的经典硬度提供了强有力的证据，并为具有明确的实验前景的量子优势奠定了基础。研究中开发的工具包有望在平均情况哈密顿量复杂性领域发挥作用。

Abstract: Classical hardness-of-sampling results are largely established for random
quantum circuits, whereas analog simulators natively realize time evolutions
under geometrically local Hamiltonians. Does a typical such Hamiltonian already
yield classically-intractable dynamics? We answer this question in the
affirmative for the ensemble of geometrically-2-local Hamiltonians with
Gaussian coefficients, evolved for constant time. This naturally leads to a
quantum advantage scheme with clear prospects for experimental realization,
necessitating only course-grained control.
  We give strong evidence of hardness for this physically-relevant ensemble. We
develop the first worst-to-average-case reduction for approximating output
probabilities of (time-independent) geometrically-2-local Hamiltonian
evolutions. Our reduction proceeds by nonstandard means: while we also leverage
polynomial interpolation, unlike previous works, we reduce directly to an
evaluator for the exact distribution over Hamiltonians, from which we are
trying to prove that sampling is hard. Previous works instead sampled from
various perturbations of the true distribution, introducing additional
constraints meant to keep the perturbation, measured in total variation
distance, under control. We dispense with this step.
  Our reduction consists in a robust multivariate polynomial interpolation,
reduced to sequential robust univariate interpolations via the symmetries of
the Gaussian. We circumvent the fact that random Hamiltonians lack a hiding
symmetry, a key property in previous proofs. We also contribute an algorithmic
version of Berlekamp-Welch to deal with errored evaluations, solving an open
problem from the RCS literature. We expect the machinery we develop to find use
in average-case Hamiltonian complexity, filling in a gap in this literature
which has thus far focussed on worst-case hardness results.

</details>


### [344] [Entangling remote qubits through a two-mode squeezed reservoir](https://arxiv.org/abs/2510.07139)
*A. Andrés-Juanes,J. Agustí,R. Sett,E. S. Redchenko,L. Kapoor,S. Hawaldar,P. Rabl,J. M. Fink*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The distribution of entanglement across distant qubits is a central challenge
for the operation of scalable quantum computers and large-scale quantum
networks. Existing approaches rely on deterministic state transfer schemes or
probabilistic protocols that require active control or measurement and
postselection. Here we demonstrate an alternative, fully autonomous process,
where two remote qubits are entangled through their coupling to a
quantum-correlated photonic reservoir. In our experiment, a Josephson
parametric converter produces a Gaussian, continuous-variable entangled state
of propagating microwave fields that drives two spatially separated
superconducting transmon qubits into a stationary, discrete-variable entangled
state. Beyond entanglement distribution, we also show that superconducting
qubits can be used to directly certify two-mode squeezing, with higher
sensitivity and without the need for calibrated noise-subtraction. These
results establish networks of qubits interfaced with distributed
continuous-variable entangled states as a powerful new platform for both
foundational studies and quantum-technology relevant applications.

</details>


### [345] [Blind quantum computing with different qudit resource state architectures](https://arxiv.org/abs/2510.06323)
*Alena Romanova,Wolfgang Dür*

Main category: quant-ph

TL;DR: 本文将盲量子计算推广到多能级系统（qudits），与基于qubit的方法相比具有优势，实现了安全的云量子计算。


<details>
  <summary>Details</summary>
Motivation: 为了实现安全的云量子计算，需要将量子计算任务委托给不可信的服务器，同时防止服务器获取计算、输入和输出的信息。

Method: 使用基于测量的量子计算，通过对簇或砖块状态进行单qubit测量来实现计算，并使用随机旋转来隐藏计算。将有限大小的近似通用门集推广到素数幂维度的qudits，并展示了qudit版本的簇和砖块状态如何实现类似的服务器盲计算。

Result: qudit版本的簇和砖块状态能够实现服务器盲计算，并对不同资源状态架构的开销进行了比较。

Conclusion: 与基于qubit的方法相比，基于qudit的盲量子计算在安全云量子计算方面提供了优势。

Abstract: We discuss how blind quantum computing generalizes to multi-level quantum
systems (qudits), which offers advantages compared to the qubit approach. Here,
a quantum computing task is delegated to an untrusted server while
simultaneously preventing the server from retrieving information about the
computation it performs, the input, and the output, enabling secure cloud-based
quantum computing. In the standard approach with qubits, measurement-based
quantum computing is used: single-qubit measurements on cluster or brickwork
states implement the computation, while random rotations of the resource qubits
hide the computation from the server. We generalize finite-sized approximately
universal gate sets to prime-power-dimensional qudits and show that qudit
versions of the cluster and brickwork states enable a similar server-blind
execution of quantum algorithms. Furthermore, we compare the overheads of
different resource state architectures and discuss which hiding strategies
apply to alternative qudit resource states beyond graph states.

</details>


### [346] [Classically Sampling Noisy Quantum Circuits in Quasi-Polynomial Time under Approximate Markovianity](https://arxiv.org/abs/2510.06324)
*Yifan F. Zhang,Su-un Lee,Liang Jiang,Sarang Gopalakrishnan*

Main category: quant-ph

TL;DR: 本文提出了一种经典算法，可以在多项式对数时间内模拟带局部去极化噪声的量子电路，从而否定了在这些情况下量子计算的优势。


<details>
  <summary>Details</summary>
Motivation: 在缺乏容错的情况下，噪声可能会破坏量子计算完成经典上不可行任务的优势。

Method: 利用近似马尔可夫性，我们顺序地从噪声量子电路的测量结果分布中采样，并提出了一种可以在 $n^{\text{polylog}(n)}$ 时间内运行的经典算法。

Result: 我们证明了当噪声率超过某个常数阈值时，该算法适用于任何电路；并且对于任何常数噪声率下的随机量子电路，该算法也具有很强的解析和数值证据支持。这些情况包括了先前已知的可经典模拟的情况，以及一些先前算法无法处理的新情况，例如没有抗集中化的浅层随机电路。

Conclusion: 我们的结果显著扩展了经典模拟的边界，并表明噪声通常会强制实现近似马尔可夫性和经典模拟，从而凸显了噪声量子电路在展示量子优势方面的局限性。

Abstract: While quantum computing can accomplish tasks that are classically
intractable, the presence of noise may destroy this advantage in the absence of
fault tolerance. In this work, we present a classical algorithm that runs in
$n^{\rm{polylog}(n)}$ time for simulating quantum circuits under local
depolarizing noise, thereby ruling out their quantum advantage in these
settings. Our algorithm leverages a property called approximate Markovianity to
sequentially sample from the measurement outcome distribution of noisy
circuits. We establish approximate Markovianity in a broad range of circuits:
(1) we prove that it holds for any circuit when the noise rate exceeds a
constant threshold, and (2) we provide strong analytical and numerical evidence
that it holds for random quantum circuits subject to any constant noise rate.
These regimes include previously known classically simulable cases as well as
new ones, such as shallow random circuits without anticoncentration, where
prior algorithms fail. Taken together, our results significantly extend the
boundary of classical simulability and suggest that noise generically enforces
approximate Markovianity and classical simulability, thereby highlighting the
limitation of noisy quantum circuits in demonstrating quantum advantage.

</details>


### [347] [Composable privacy of networked quantum sensing](https://arxiv.org/abs/2510.06326)
*Naomi R. Solomons,Damian Markham*

Main category: quant-ph

TL;DR: 传感器网络利用大规模纠缠态提供分布式计量，并探索了这些方案的隐私性，即本地参数可保密而联合函数可被估计。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在利用抽象密码学框架关联两种拟议的拟私密性定义，并证明它们的可组合性，从而使协议能够安全地作为其他方案的子程序。

Method: 使用抽象密码学框架来关联和证明两种拟议的拟私密性定义的と组合性。

Result: 证明了两种拟议的拟私密性定义是可组合的，并提供了一个使用 GHZ 态估计参数均值的具体示例，该示例是完全可组合的安全示例。

Conclusion: 本研究表明，由 GHZ 态驱动的分布式计量协议可以通过抽象密码学框架实现可组合的完全安全性，从而可以安全地将其作为更复杂协议的组成部分。

Abstract: Networks of sensors are a promising scheme to deliver the benefits of quantum
technologies in coming years, offering enhanced precision and accuracy for
distributed metrology through the use of large entangled states. Recent work
has additionally explored the privacy of these schemes, meaning that local
parameters can be kept secret while a joint function of these is estimated by
the network. In this work, we use the abstract cryptography framework to relate
the two proposed definitions of quasi-privacy, showing that both are
composable, which enables the protocol to be securely included as a sub-routine
to other schemes. We give an explicit example that estimating the mean of a set
of parameters using GHZ states is composably fully secure.

</details>


### [348] [Classical simulation of noisy random circuits from exponential decay of correlation](https://arxiv.org/abs/2510.06328)
*Su-un Lee,Soumik Ghosh,Changhun Oh,Kyungjoo Noh,Bill Fefferman,Liang Jiang*

Main category: quant-ph

TL;DR: 研究有噪声的随机量子电路在一般噪声模型下的经典模拟性。


<details>
  <summary>Details</summary>
Motivation: 现有的经典模拟算法依赖于抗集中性质，该性质在电路深度较小或在现实噪声模型下可能失效。

Method: 提出一种基于条件互信息（CMI）指数衰减的新方法，并证明其能够实现高效的经典模拟。

Result: 证明了CMI指数衰减使得电路深度在有效上变浅，能够实现高效的经典模拟采样，且CMI指数衰减是有噪声随机电路在多种噪声模型下的普遍特征。

Conclusion: CMI衰减是经典可模拟性的根本标准，而非抗集中性质，并划分了有噪声设备中量子优势的边界。

Abstract: We study the classical simulability of noisy random quantum circuits under
general noise models. While various classical algorithms for simulating noisy
random circuits have been proposed, many of them rely on the anticoncentration
property, which can fail when the circuit depth is small or under realistic
noise models. We propose a new approach based on the exponential decay of
conditional mutual information (CMI), a measure of tripartite correlations. We
prove that exponential CMI decay enables a classical algorithm to sample from
noisy random circuits -- in polynomial time for one dimension and
quasi-polynomial time for higher dimensions -- even when anticoncentration
breaks down. To this end, we show that exponential CMI decay makes the circuit
depth effectively shallow, and it enables efficient classical simulation for
sampling. We further provide extensive numerical evidence that exponential CMI
decay is a universal feature of noisy random circuits across a wide range of
noise models. Our results establish CMI decay, rather than anticoncentration,
as the fundamental criterion for classical simulability, and delineate the
boundary of quantum advantage in noisy devices.

</details>


### [349] [Symmetry Fragmentation](https://arxiv.org/abs/2510.06333)
*Thomas Iadecola*

Main category: quant-ph

TL;DR: 希尔伯特空间碎化与对称性相互作用，导致出现指数级数量的逻辑量子比特，并为实验探测提供条件。


<details>
  <summary>Details</summary>
Motivation: 研究具有动力学约束的量子多体系统中希尔伯特空间碎化与对称性之间的相互作用。

Method: 分析了具有电荷共轭和翻译对称性的电荷守恒系统的相互作用，特别是考察了这些对称性的非阿贝尔代数与碎化子空间投影算符之间的关系。

Result: 发现指数级数量的逻辑量子比特被编码在简并的本征态对中，这些量子比特可能高度纠缠。同时，对称性代数提供了希尔伯特空间碎化的实验信号的必要条件。

Conclusion: 希尔伯特空间碎化与对称性相互作用可以产生高度纠缠的逻辑量子比特，并且可以通过密度不平衡等现象进行实验验证。

Abstract: In quantum many-body systems with kinetically constrained dynamics, the
Hilbert space can split into exponentially many disconnected subsectors, a
phenomenon known as Hilbert-space fragmentation. We study the interplay of such
fragmentation with symmetries, focusing on charge conserving systems with
charge conjugation and translation symmetries as a concrete example. The
non-Abelian algebra of these symmetries and the projectors onto the fragmented
subsectors leads to the emergence of exponentially many logical qubits encoded
in degenerate pairs of eigenstates, which can be highly entangled. This algebra
also provides necessary conditions for experimental signatures of Hilbert-space
fragmentation, such as the persistence of density imbalances at late times.

</details>


### [350] [Recent quantum runtime (dis)advantages](https://arxiv.org/abs/2510.06337)
*J. Tuziemski,J. Pawłowski,P. Tarasiuk,Ł. Pawela,B. Gardas*

Main category: quant-ph

TL;DR: 量子计算在退火和门基算法方面尚未实现可行的运行时优势，并且在当前的 NISQ 硬件上仍然难以捉摸。


<details>
  <summary>Details</summary>
Motivation: 评估量子算法（特别是退火和门基算法）在最近声称的量子优势，并确定这些优势在考虑实际的端到端运行时和与强大的经典基线进行比较时是否成立。 强调了传统分析中经常忽略的开销，例如读出、转译和热化，这些开销可能会扭曲“量子霸权”结果。

Method: 通过严格的端到端运行时定义和与强经典基线的比较来重新评估量子优势的声明。 检查了两个关键里程碑：(1) 用于近似 QUBO 的量子退火，指出其运行时代理（退火时间）不可测量；(2) 受限的 Simon 问题，表明尽管其 oracle 调用具有优势，但估计的量子运行时比经典基线慢 100 倍。 此外，还评估了 BF-DCQO 混合算法的运行时优势。

Result: 所审查的量子算法（量子退火和受限 Simon 问题）的运行时优势并未在严格的基准测试中得以维持。 估计的量子运行时，包括开销，与精心调整的经典基线相比，显示出相当大的劣势。

Conclusion: 在当前的 NISQ 硬件上，基于运行时的量子霸权仍然难以实现。 可信的量子优势声明需要仔细的时间核算、适当的参考选择和准确的指标，以避免因忽略开销而产生偏差的评估。

Abstract: We (re)evaluate recent claims of quantum advantage in annealing- and
gate-based algorithms, testing whether reported speedups survive rigorous
end-to-end runtime definitions and comparison against strong
classicalbaselines. Conventional analyses often omit substantial overhead
(readout, transpilation, thermalization, etc.) yielding biased assessments.
While excluding seemingly not important parts of the simulation may seem
reasonable, on most current quantum hardware a clean separation between "pure
compute" and "overhead" cannot be experimentally justified. This may distort
"supremacy" results. In contrast, for most classical hardware total time
$\approx$ compute $+$ a weakly varying constant leading to robust claims.
  We scrutinize two important milestones: (1) quantum annealing for approximate
QUBO [https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.134.160601],
which uses a sensible time-to-$\epsilon$ metric but proxies runtime by the
annealing time (non-measurable); (2) a restricted Simon's problem
[https://journals.aps.org/prx/abstract/10.1103/PhysRevX.15.021082] , whose
advantageous scaling in oracle calls is undisputed; yet, as we demonstrate,
estimated runtime of the quantum experiment is $\sim 100 \times$ slower than a
tuned classical baseline. Finally, we show that recently claimed "runtime
advantage" of the BF-DCQO hybrid algorithm (arXiv:2505.08663) does not
withstand rigorous benchmarking. Therefore, we conclude that runtime-based
supremacy remains elusive on NISQ hardware, and credible claims require a
careful time accounting with a proper reference selections, and an adequate
metric.

</details>


### [351] [Generalised quantum Sanov theorem revisited](https://arxiv.org/abs/2510.06340)
*Ludovico Lami*

Main category: quant-ph

TL;DR: 本工作为复合假设下的量子假设检验提供了通用的Stein指数公式，该公式适用于更广泛的物理相关状态集，并推广了现有结果。


<details>
  <summary>Details</summary>
Motivation: 现有的量子假设检验Stein指数计算方法通常只适用于简单假设或有严格限制的复合假设，排除了如可分离态或稳定器态等重要物理状态集。因此，需要一个更通用的方法来处理复合假设。

Method: 通过量子到经典的信息转化（测量），结合经典的Stein指数结果，并运用新的量子技术来分析渐近表达式。

Result: 提出一个通用的Stein指数公式，适用于复合的零假设（完全通用）和替代假设（可复合i.i.d.或任意变异），并满足较弱的兼容性假设，这些假设对可分离态或稳定器态等物理相关状态集是成立的。该公式扩展并包含了先前研究（[BBH, CMP 385:55, 2021] 和 [LBR, arXiv:2408.07067]）的结果。

Conclusion: 本研究成功地为复合假设下的量子假设检验建立了通用的Stein指数公式，显著扩展了其适用范围，为量子信息理论中的假设检验问题提供了更强大的工具。

Abstract: Given two families of quantum states $A$ and $B$, called the null and the
alternative hypotheses, quantum hypothesis testing is the task of determining
whether an unknown quantum state belongs to $A$ or $B$. Mistaking $A$ for $B$
is a type I error, and vice versa for the type II error. In quantum Shannon
theory, a fundamental role is played by the Stein exponent, i.e. the asymptotic
rate of decay of the type II error probability for a given threshold on the
type I error probability. Stein exponents have been thoroughly investigated --
and, sometimes, calculated. However, most currently available solutions apply
to settings where the hypotheses simple (i.e. composed of a single state), or
else the families $A$ and $B$ need to satisfy stringent constraints that
exclude physically important sets of states, such as separable states or
stabiliser states. In this work, we establish a general formula for the Stein
exponent where both hypotheses are allowed to be composite: the alternative
hypothesis $B$ is assumed to be either composite i.i.d. or arbitrarily varying,
with components taken from a known base set, while the null hypothesis $A$ is
fully general, and required to satisfy only weak compatibility assumptions that
are met in most physically relevant cases -- for instance, by the sets of
separable or stabiliser states. Our result extends and subsumes the findings of
[BBH, CMP 385:55, 2021] (that we also simplify), as well as the 'generalised
quantum Sanov theorem' of [LBR, arXiv:2408.07067]. The proof relies on a
careful quantum-to-classical reduction via measurements, followed by an
application of the results on classical Stein exponents obtained in [Lami,
arXiv:today]. We also devise new purely quantum techniques to analyse the
resulting asymptotic expressions.

</details>


### [352] [Limitations of Noisy Geometrically Local Quantum Circuits](https://arxiv.org/abs/2510.06346)
*Jon Nelson,Joel Rajakumar,Michael J. Gullans*

Main category: quant-ph

TL;DR: 受限的量子电路在更浅的深度即可模拟。


<details>
  <summary>Details</summary>
Motivation: 之前的研究表明，带有去极化噪声的量子电路在特定深度后会收敛到均匀分布，从而变得可模拟。然而，这些研究没有考虑几何局部性的约束。本研究旨在探讨在几何局部性约束下，量子电路达到可模拟状态所需的深度阈值。

Method: 证明了任何嘈杂的、几何局域的量子电路的输出分布，在其深度超过一个取决于噪声强度的临界阈值（$	heta(	ext{log }n)$）后，可以在拟多项式时间内近似采样。进一步提出了一个$	heta(1)$深度的阈值猜想，并提供了分析证据和一个候选的有效算法。

Result: 在几何局部性约束下，嘈杂量子电路的输出分布可以在拟多项式时间内被近似采样，且深度阈值比之前预期的更浅。提出了一个$	heta(1)$深度的阈值猜想，并提供了支持该猜想的分析证据和算法。

Conclusion: 在几何局部性约束下，嘈杂量子电路比之前认为的更容易受到噪声影响，并且可以在比预期更浅的深度实现经典模拟。本研究的结果依赖于对嘈杂的浅层量子电路输出态的新信息论性质的理解。

Abstract: It has been known for almost 30 years that quantum circuits with interspersed
depolarizing noise converge to the uniform distribution at $\omega(\log n)$
depth, where $n$ is the number of qubits, making them classically simulable. We
show that under the realistic constraint of geometric locality, this bound is
loose: these circuits become classically simulable at even shallower depths.
Unlike prior work in this regime, we consider sampling from worst-case circuits
and noise of any constant strength. First, we prove that the output
distribution of any noisy geometrically local quantum circuit can be
approximately sampled from in quasipolynomial time, when its depth exceeds a
fixed $\Theta(\log n)$ critical threshold which depends on the noise strength.
This scaling in $n$ was previously only obtained for noisy random quantum
circuits (Aharonov et. al, STOC 2023). We further conjecture that our bound is
still loose and that a $\Theta(1)$-depth threshold suffices for simulability
due to a percolation effect. To support this, we provide analytical evidence
together with a candidate efficient algorithm. Our results rely on new
information-theoretic properties of the output states of noisy shallow quantum
circuits, which may be of broad interest. On a fundamental level, we
demonstrate that unitary quantum processes in constant dimensions are more
fragile to noise than previously understood.

</details>


### [353] [Optimally learning functions in interacting quantum sensor networks](https://arxiv.org/abs/2510.06360)
*Erfan Abbasgholinejad,Sean R. Muleady,Jacob Bringewatt,Anthony J. Brady,Yu-Xin Wang,Ali Fahimniya,Alexey V. Gorshkov*

Main category: quant-ph

TL;DR: 本研究解决了分布式量子系统中局部参数估计的难题，提出了在存在未知相互作用的情况下，估计局部参数线性组合的最优界限和协议。


<details>
  <summary>Details</summary>
Motivation: 分布式量子系统（如量子传感器网络）的局部参数估计是一个核心问题，尤其是在存在未知的、不受控制的相互作用时，其基本极限尚不清楚。

Method: 我们建立了估计任意线性组合的局部参数（包括任意、未知的相互作用）的最优界限和协议。更广泛地说，我们为学习任意线性组合的哈密顿量系数（对于任意、可交换的项）建立了界限。

Result: 我们统一并扩展了现有非相互作用量子比特和多模干涉仪的界限，为现实多体系统中的分布式传感和哈密顿量学习提供了一个通用框架。

Conclusion: 本研究为在存在未知相互作用的现实多体系统中进行分布式量子传感和哈密顿量学习提供了理论基础和最优策略。

Abstract: Estimating extensive combinations of local parameters in distributed quantum
systems is a central problem in quantum sensing, with applications ranging from
magnetometry to timekeeping. While optimal strategies are known for sensing
non-interacting Hamiltonians in quantum sensor networks, fundamental limits in
the presence of uncontrolled interactions remain unclear. Here, we establish
optimal bounds and protocols for estimating a linear combination of local
parameters of Hamiltonians with arbitrary, unknown interactions. In the
process, we more generally establish bounds for learning any linear combination
of Hamiltonian coefficients for arbitrary, commuting terms. Our results unify
and extend existing bounds for non-interacting qubits and multimode
interferometers, providing a general framework for distributed sensing and
Hamiltonian learning in realistic many-body systems.

</details>


### [354] [Harnessing Environmental Noise for Quantum Energy Storage](https://arxiv.org/abs/2510.06384)
*Borhan Ahmadi,Aravinth Balaji Ravichandran,Paweł Mazurek,Shabir Barzanjeh,Paweł Horodecki*

Main category: quant-ph

TL;DR: 我们提出了一种自主充电范式，其中与热环境集体耦合的相同双能级单元集合可以在没有任何外部控制的情况下获得功容量。


<details>
  <summary>Details</summary>
Motivation: 现有的量子电池方案大多需要相干驱动或非平衡资源，这在嘈杂的环境中难以实现。因此，需要开发一种无需外部控制的自主充电方法。

Method: 通过使双能级单元集体耦合到热环境，利用环境介导的干涉来引导系统远离被动状态，进入具有可提取功的稳态。

Result: 我们获得了充电动力学和稳态的闭式解，并表明该方案具有良好的可扩展性，并且能够容忍局部噪声。此外，我们发现环境涨落可以被利用来实现无需驱动、可扩展的量子电池，并且在有限温度下具有最优性能。

Conclusion: 我们提出的自主充电范式为在嘈杂环境中构建实用的量子电池提供了一条有前景的道路，并可能在量子计算等领域得到应用。

Abstract: Quantum hardware increasingly relies on energy reserves that can later be
converted into useful work; yet, most battery-like proposals demand coherent
drives or engineered non-equilibrium resources, limiting practicality in noisy
settings. We develop an autonomous charging paradigm in which an ensemble of
identical two-level units, collectively coupled to a thermal environment,
acquires work capacity without any external control. The common bath mediates
interference between emission and absorption pathways, steering the many-body
state away from passivity and into a steady regime with nonzero extractable
work. The full charging dynamics and closed-form expressions are obtained for
the steady-state, showing favorable scaling with the number of cells that
approach the many-body optimum. We show that the mechanism is robust to local
noise: under a convex mixture of collective and local dissipation, non-zero
steady-state ergotropy persists, exhibits counterintuitive finite-temperature
optima, and remains operative when the collective channel is comparable to or
stronger than the local one. We show that environmental fluctuations can be
harnessed to realize drive-free, scalable quantum batteries compatible with
circuit- and cavity-QED platforms. Used as local work buffers, such batteries
could potentially enable rapid ancilla reset, bias dissipative stabilizer
pumps, and reduce syndrome-extraction overhead in fault-tolerant quantum
computing.

</details>


### [355] [Fourier Spectrum of Noisy Quantum Algorithms](https://arxiv.org/abs/2510.06385)
*Uma Girish*

Main category: quant-ph

TL;DR: 研究有噪量子算法和 BQP 与 BPP 之间的联系，利用傅里叶增长区分量子算法，并推导出不同模型之间的 oracle 分离。


<details>
  <summary>Details</summary>
Motivation: 近期的量子设备存在噪声，并且通用量子计算机仍遥不可及，因此有必要研究有噪量子算法。

Method: 利用傅里叶增长（一种区分量子和经典算法的技术）来分析不同类型的噪声如何影响量子算法的傅里叶增长，并建立 DQCk、1/2 BQP 和 BQP 算法的傅里叶增长上限，然后利用这些差异来推导出模型之间的 oracle 分离。

Result: 证明了 2-Forrelation 和 3-Forrelation 问题分别在 DQC1 和 1/2 BQP 模型中需要 NΩ(1) 次查询。

Conclusion: 傅里叶增长可以用来区分不同类型的噪声以及不同类型的量子算法，并且可以用来建立不同量子计算模型之间的 oracle 分离。

Abstract: Quantum computing promises exponential speedups for certain problems, yet
fully universal quantum computers remain out of reach and near-term devices are
inherently noisy. Motivated by this, we study noisy quantum algorithms and the
landscape between $\mathsf{BQP}$ and $\mathsf{BPP}$. We build on a powerful
technique to differentiate quantum and classical algorithms called the
level-$\ell$ Fourier growth (the sum of absolute values of Fourier coefficients
of sets of size $\ell$) and show that it can also be used to differentiate
quantum algorithms based on the types of resources used. We show that noise
acting on a quantum algorithm dampens its Fourier growth in ways intricately
linked to the type of noise.
  Concretely, we study noisy models of quantum computation where highly mixed
states are prevalent, namely: $\mathsf{DQC}_k$ algorithms, where $k$ qubits are
clean and the rest are maximally mixed, and $\frac{1}{2}\mathsf {BQP}$
algorithms, where the initial state is maximally mixed, but the algorithm is
given knowledge of the initial state at the end of the computation. We
establish upper bounds on the Fourier growth of $\mathsf{DQC}_k$,
$\frac{1}{2}\mathsf{BQP}$ and $\mathsf{BQP}$ algorithms and leverage the
differences between these bounds to derive oracle separations between these
models. In particular, we show that 2-Forrelation and 3-Forrelation require
$N^{\Omega(1)}$ queries in the $\mathsf{DQC}_1$ and $\frac{1}{2}\mathsf{BQP}$
models respectively. Our results are proved using a new matrix decomposition
lemma that might be of independent interest.

</details>


### [356] [Mereological Quantum Phase Transitions](https://arxiv.org/abs/2510.06389)
*Paolo Zanardi,Emanuel Dallas,Faidon Andreadakis*

Main category: quant-ph

TL;DR: 论文提出了一种新的计算框架，用于识别和理解量子系统中的“整体量子相变”（m-QPTs）。该框架通过定义一种广义张量积结构（g-TPS）和一种量子混沌泛函来实现。通过最小化混沌泛函来选择 g-TPS，并使用信息-几何度量来识别参数空间中的奇异点，这些奇异点标志着 m-QPTs 的发生。研究通过分析量子相干性和算子纠缠的例子，并结合自旋链的数值模拟，验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在提出一个新颖的理论框架，用于识别和描述量子系统中的“整体量子相变”（m-QPTs），填补了现有量子相变研究在系统组成结构变化方面的不足。

Method: 提出了一种基于算子代数变分族的框架，该框架定义了广义张量积结构（g-TPS）和参数依赖的哈密顿量。通过最小化量子混沌泛函来选择 g-TPS，并将信息-几何度量拉回到参数空间，从而定义“代数磁化率”来表征 m-QPTs。

Result: 通过量子相干性和算子纠缠的解析示例，以及自旋链的数值模拟，验证了所提出的 m-QPTs 框架。数值模拟显示，代数磁化率在可积性点出现尖锐响应，并在退火诱导的局域化过程中快速增长，表明子系统结构发生了临界重组。

Conclusion: 所提出的 m-QPTs 框架能够有效地识别量子系统中的相变，特别是那些与系统组成结构相关的相变。代数磁化率是表征这些相变的有力工具，为理解和预测量子系统行为提供了新的视角。

Abstract: We introduce the novel concept of mereological quantum phase transition
(m-QPTs). Our framework is based on a variational family of operator algebras
defining generalized tensor product structures (g-TPS), a parameter-dependent
Hamiltonian, and a quantum scrambling functional. By minimizing the scrambling
functional, one selects a g-TPS, enabling a pullback of the natural
information-geometric metric on the g-TPS manifold to the parameter space. The
singularities of this induced metric -- so-called algebra susceptibility -- in
the thermodynamic limit characterize the m-QPTs. We illustrate this framework
through analytical examples involving quantum coherence and operator
entanglement. Moreover, spin-chains numerical simulations show susceptibility
sharp responses at an integrability point and strong growth across
disorder-induced localization, suggesting critical reorganizations of emergent
subsystem structure aligned with those transitions.

</details>


### [357] [Stochastic interpretation of quantum mechanics](https://arxiv.org/abs/2510.06418)
*Mário J. de Oliveira*

Main category: quant-ph

TL;DR: 通过将波函数视为随机变量，并引入一种不改变波函数绝对值但会改变其相位（从而严格保持波函数范数沿随机轨迹守恒）的随机方程，来表达与波函数相关的概率特性。量子刘维尔方程的密度矩阵是随机波函数的协方差矩阵。


<details>
  <summary>Details</summary>
Motivation: 将波函数视为随机变量，以表达其概率特性。

Method: 引入一个随机方程来处理波函数，该方程的噪声只改变波函数的相位而不改变其绝对值，从而确保波函数范数沿随机轨迹的守恒。证明了满足量子刘维尔方程的密度矩阵即为随机波函数的协方差矩阵。

Result: 发现了密度矩阵（满足量子刘维尔方程）是随机波函数的协方差矩阵。

Conclusion: 所提出的随机方程方法能够一致地处理波函数及其概率特性，并且其结果与量子力学的标准表述（通过量子刘维尔方程描述密度矩阵）相符。

Abstract: We express the probabilistic character associated to the wave function by
treating it as a stochastic variable. This is accomplished by means of a
stochastic equation for the wave function whose noise changes the phase of the
wave function but not its absolute value, so that the norm of the wave function
is strictly conserved along a stochastic trajectory. We show that the density
matrix that obeys the quantum Liouville equation is the covariance matrix
associated to the stochastic wave function.

</details>


### [358] [Qudit low-density parity-check codes](https://arxiv.org/abs/2510.06495)
*Daniel J. Spencer,Andrew Tanggara,Tobias Haug,Derek Khu,Kishor Bharti*

Main category: quant-ph

TL;DR: 该研究将量子低密度奇偶校验码（LDPC）从量子比特推广到量子数，为可扩展的量子纠错提供了新的途径。


<details>
  <summary>Details</summary>
Motivation: 量子比特在量子计算中具有显著优势，但量子LDPC码的研究主要集中在量子比特上。本研究旨在将量子LDPC码推广到量子数，以利用量子数在门编译、资源需求、错误校正和量子通信等方面的优势。

Method: 提出了一种通用的量子LDPC码构建框架，并将该框架应用于多种LDPC码，包括双变量自行车码、超图乘积码、子系统超图乘积码、高维扩展码和纤维束码。在此基础上，通过数值搜索和解码，发现了适用于近期硬件的新型量子LDPC码。

Result: 成功将多种量子LDPC码推广到量子数，并发现了适用于近期硬件的新型量子LDPC码。结果表明，量子LDPC码在可扩展量子纠错方面具有巨大潜力。

Conclusion: 量子LDPC码是实现可扩展量子纠错的一种多功能且兼容硬件的途径。

Abstract: Qudits offer significant advantages over qubit-based architectures, including
more efficient gate compilation, reduced resource requirements, improved
error-correction primitives, and enhanced capabilities for quantum
communication and cryptography. Yet, one of the most promising family of
quantum error correction codes, namely quantum low-density parity-check (LDPC)
codes, have been so far mostly restricted to qubits. Here, we generalize recent
advancements in LDPC codes from qubits to qudits. We introduce a general
framework for finding qudit LDPC codes and apply our formalism to several
promising types of LDPC codes. We generalize bivariate bicycle codes, including
their coprime variant; hypergraph product codes, including the recently
proposed La-cross codes; subsystem hypergraph product (SHYPS) codes;
high-dimensional expander codes, which make use of Ramanujan complexes; and
fiber bundle codes. Using the qudit generalization formalism, we then
numerically search for and decode several novel qudit codes compatible with
near-term hardware. Our results highlight the potential of qudit LDPC codes as
a versatile and hardware-compatible pathway toward scalable quantum error
correction.

</details>


### [359] [Non-Gaussian states via pump-depleted SPDC](https://arxiv.org/abs/2510.06498)
*Colin Vendromin,Samuel E. Fontaine,J. E. Sipe*

Main category: quant-ph

TL;DR: 通过在InGaP微环谐振器中使用自发参量下转换（SPDC）生成非高斯态，并提出了一种处理散射损耗的幻象通道模型。研究表明，在实际参数下，虽然可以实现泵浦耗尽并产生负Wigner函数，但高压缩可能导致非高斯特征难以观察。提出通过应用逆高斯酉变换来移除压缩，揭示非高斯特征，为集成损耗型微环谐振器中泵浦耗尽的SPDC建模奠定基础，并为可扩展的片上非高斯光源开辟道路。


<details>
  <summary>Details</summary>
Motivation: 开发一种用于在InGaP微环谐振器中通过自发参量下转换（SPDC）生成非高斯态的模型，并解决散射损耗问题。

Method: 在非高斯哈密顿量中引入幻象通道来处理散射损耗，并将系统的完整ket表示为作用在初始真空态的非高斯ket上的高斯酉变换。

Result: 在实际参数下，可以实现泵浦耗尽，并且残余非高斯ket的Wigner函数表现出负值。然而，观察到高压缩可能导致完整ket的非高斯特征变得不可观察。

Conclusion: 在低损耗情况下，通过对可访问模式应用逆高斯酉变换可以移除大部分压缩，从而揭示非高斯特征。该研究为在集成损耗型微环谐振器中进行泵浦耗尽的SPDC建模提供了基础，并为可扩展的片上非高斯光源提供了途径。

Abstract: We develop a model for non-Gaussian state generation via spontaneous
parametric down-conversion (SPDC) in InGaP microring resonators. The nonlinear
Hamiltonian is written in terms of the asymptotic fields for the system, which
includes a phantom channel to handle scattering loss. The full ket for the
system is written as a Gaussian unitary acting on a residual non-Gaussian ket,
which is vacuum initially and evolves according to a non-Gaussian Hamiltonian.
We show that for realistic parameters we can access the pump depletion regime,
where the Wigner function for the residual non-Gaussian ket has negativity. But
we find that the non-Gaussian features for the full ket could be unobservable
due to the large amount of squeezing required to lead to pump depletion. We
show that a potential solution in the low-loss regime is to implement an
inverse Gaussian unitary on the accessible modes to remove most of the
squeezing and reveal the non-Gaussian features. This work provides a foundation
for modeling pump-depleted SPDC in integrated lossy microring resonators,
opening a path toward a scalable on-chip non-Gaussian source.

</details>


### [360] [On the Pure Quantum Polynomial Hierarchy and Quantified Hamiltonian Complexity](https://arxiv.org/abs/2510.06522)
*Sabee Grewal,Dorian Rudolph*

Main category: quant-ph

TL;DR: 该论文研究了纯量子多项式谱系（pureQPH），并取得了多项式谱系（QPH）和量子计算复杂性理论中的若干新成果。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是探索纯量子多项式谱系（pureQPH）的复杂性，并将其与现有的量子计算复杂性类联系起来，同时为量化哈密顿复杂度（quantified Hamiltonian complexity）这一新领域奠定基础。

Method: 论文采用了多种数学和计算方法，包括引入了竞争性存在和全称证明者模型，利用误差约减技术，并提出了改进的无关纠缠器。此外，研究还涉及了量化布尔公式（quantified Boolean formulae）的量子类似物的定义和分析。

Result: 主要结果包括：1. 证明了QMA(2)包含于pureQSigma2，pureQSigma2包含于QSigma3，而QSigma3包含于NEXP。2. 证明了pureQPH = QPH，这得益于误差约减和改进的无关纠缠器。3. 证明了量化纯稀疏哈密顿问题（quantified pure sparse Hamiltonian problem）是pureQSigma完全的。4. 揭示了其他一些量化哈密顿复杂度问题的非平凡包含关系，例如存在-全称混合局部哈密顿问题（exists-forall mixed local Hamiltonian problem）位于NP^QMA ∩ coNP^QMA。

Conclusion: 本研究不仅加深了对纯量子多项式谱系的理解，并将其与更广泛的量子计算模型统一起来，而且开创了量化哈密顿复杂度这一新研究方向，为未来该领域的研究提供了重要的理论基础和初步结果。

Abstract: We prove several new results concerning the pure quantum polynomial hierarchy
(pureQPH). First, we show that QMA(2) is contained in pureQSigma2, that is, two
unentangled existential provers can be simulated by competing existential and
universal provers. We further prove that pureQSigma2 is contained in QSigma3,
which in turn is contained in NEXP.
  Second, we give an error reduction result for pureQPH, and, as a consequence,
prove that pureQPH = QPH. A key ingredient in this result is an improved
dimension-independent disentangler.
  Finally, we initiate the study of quantified Hamiltonian complexity, the
quantum analogue of quantified Boolean formulae. We prove that the quantified
pure sparse Hamiltonian problem is pureQSigma-complete. By contrast, other
natural variants (pure/local, mixed/local, and mixed/sparse) admit nontrivial
containments but fail to be complete under known techniques. For example, we
show that the exists-forall mixed local Hamiltonian problem lies in NP^QMA \cap
coNP^QMA.

</details>


### [361] [Approximate maximum likelihood decoding with $K$ minimum weight matchings](https://arxiv.org/abs/2510.06531)
*Mao Lin*

Main category: quant-ph

TL;DR: 本文提出了一种近似最大似然解码（MLD）的算法，该算法通过 K 次最小重量匹配（MWM）来逼近 MLD，并对表面码等场景进行了验证，表明随着 K 的增加，保真度接近精确 MLD 或张量网络解码器。


<details>
  <summary>Details</summary>
Motivation: 在量子纠错中，最大似然解码（MLD）是最优策略但计算成本高，而最小重量匹配（MWM）解码器计算效率高但不是最优。因此，需要一种计算成本可接受且性能接近 MLD 的解码策略。

Method: 提出了一种利用 K 次 MWM 来近似 MLD 的算法。具体方法包括：1. 对于表面码受类图错误影响的情况，通过系统地修改解码图并寻找修改后图的 MWM 来高效地找到前 K 个 MWM。2. 对于 X 错误和 Z 错误相关的场景，虽然无法有效找到解码超图的 MWM，但提出了一种启发式方法，通过在 X 和 Z 子图中寻找 K 个 MWM 来近似 MLD。

Result: 通过对表面码（受类图错误影响）、表面方形 GKP 码和表面六边形 GKP 码（受高斯随机位移错误影响）进行基准测试，结果表明，随着 K 的增加，本文算法的保真度能够逼近精确 MLD（前两种情况）或张量网络解码器（最后一种情况）。

Conclusion: 本文提出的 K 次 MWM 近似 MLD 的算法是一种有效且计算成本可控的量子纠错策略，在多种量子码和错误模型下均表现出良好的性能，并能随着 K 的增大逼近最优解码器的性能。

Abstract: The minimum weight matching (MWM) and maximum likelihood decoding (MLD) are
two widely used and distinct decoding strategies for quantum error correction.
For a given syndrome, the MWM decoder finds the most probable physical error
corresponding to the MWM of the decoding graph, whereas MLD aims to find the
most probable logical error. Although MLD is the optimal error correction
strategy, it is typically more computationally expensive compared to the MWM
decoder. In this work, we introduce an algorithm that approximates MLD with $K$
MWMs from the decoding graph. Taking the surface code subject to graphlike
errors as an example, we show that it is possible to efficiently find the first
$K$ MWMs by systematically modifying the original decoding graph followed by
finding the MWMs of the modified graphs. For the case where the $X$ and $Z$
errors are correlated, despite the MWM of the decoding hypergraph cannot be
found efficiently, we present a heuristic approach to approximate the MLD by
finding the $K$ MWMs in the $X$ and $Z$ subgraphs. We benchmark the efficacy of
our algorithm for the surface code subject to graphlike errors, the
surface-square Gottesman-Kitaev-Preskill (GKP) code and surface-hexagonal GKP
code subject to the Gaussian random displacement errors, showing that the
fidelity approaches that of the exact MLD (for the first two cases) or the
tensor-network decoder (for the last case) as $K$ increases.

</details>


### [362] [CLAQS: Compact Learnable All-Quantum Token Mixer with Shared-ansatz for Text Classification](https://arxiv.org/abs/2510.06532)
*Junhao Chen,Yifan Zhou,Hanqi Jiang,Yi Pan,Yiwei Li,Huaqin Zhao,Wei Zhang,Yingfeng Wang,Tianming Liu*

Main category: quant-ph

TL;DR: 该论文提出了一种名为CLAQS的紧凑型、全量子化Token混合器，用于文本分类任务，解决了当前量子计算在处理长序列时的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前的量子计算设备在量子比特和深度上都有限制，同时训练不稳定，而经典的注意力机制在计算和内存方面开销很大。这促使研究人员需要开发紧凑、相位感知且能稳定处理长序列的量子Token混合器。

Method: 该论文提出了一种名为CLAQS的紧凑型、全量子化Token混合器，它在统一的量子电路中联合学习复值混合和非线性变换。为了实现稳定的端到端优化，论文应用了l1范数来规范化幅度缩放，并引入了一个两阶段参数化量子架构，将共享的Token嵌入与窗口级的量子前馈模块解耦。CLAQS在滑动窗口机制下运行，并进行文档级聚合。

Result: CLAQS仅需八个数据量子比特和浅层量子电路，在SST-2数据集上达到了91.64%的准确率，在IMDB数据集上达到了87.08%的准确率，优于经典的Transformer基线模型和混合量子-经典模型。

Conclusion: CLAQS是一种有效的全量子化Token混合器，能够克服当前量子计算设备的限制，并在文本分类任务中取得优于经典和混合方法的性能。

Abstract: Quantum compute is scaling fast, from cloud QPUs to high throughput GPU
simulators, making it timely to prototype quantum NLP beyond toy tasks.
However, devices remain qubit limited and depth limited, training can be
unstable, and classical attention is compute and memory heavy. This motivates
compact, phase aware quantum token mixers that stabilize amplitudes and scale
to long sequences. We present CLAQS, a compact, fully quantum token mixer for
text classification that jointly learns complex-valued mixing and nonlinear
transformations within a unified quantum circuit. To enable stable end-to-end
optimization, we apply l1 normalization to regulate amplitude scaling and
introduce a two-stage parameterized quantum architecture that decouples shared
token embeddings from a window-level quantum feed-forward module. Operating
under a sliding-window regime with document-level aggregation, CLAQS requires
only eight data qubits and shallow circuits, yet achieves 91.64% accuracy on
SST-2 and 87.08% on IMDB, outperforming both classical Transformer baselines
and strong hybrid quantum-classical counterparts.

</details>


### [363] [Optimal filtering and generation of entangled photons for quantum applications in the presence of noise](https://arxiv.org/abs/2510.06536)
*Jordan M. Thomas,Andrew R. Cameron,Akil Pathiranage,Si Xie,Raju Valivarthi,Panagiotis Spentzouris,Maria Spiropulu,Cristián Peña,Prem Kumar*

Main category: quant-ph

TL;DR: 在多光子应用中，滤波可用于抑制噪声，但可能降低符合度或信令效率。本文研究了在量子网络中滤波对纠缠光子和经典光子共传的影响，并提出了一种通过调整滤波器带宽、选择合适的探测器和偏振滤光片来优化性能的方法。


<details>
  <summary>Details</summary>
Motivation: 为了在量子网络中实现可扩展的低损耗量子通信，需要将纠缠光子与经典光子共传，但同时需要过滤掉由经典光引起的自发拉曼散射噪声。

Method: 本文研究了在多光子应用中，使用可调谐带宽滤波器、低抖动探测器和偏振滤波器，将C波段纠缠光子与C波段经典通信复用在同一根长距离光纤中进行共传。分析了泵浦和滤波器带宽、多光子发射、滤波器形状、损耗、相位匹配以及量子信息测量方式对性能的影响。

Result: 实现了在25公里标准光纤中，时间-二元纠缠光子（1536.5 nm，兼容铒离子量子存储器）与10 Gbps C波段经典数据的共传。通过窄滤波，实现了毫瓦级C波段功率，比同类研究高一个数量级，并可能支持Tbps的经典速率。研究了各种因素对性能的影响，并发现了提高噪声抑制与单模纯度之间的权衡。

Conclusion: 本文的研究结果不仅适用于光纤通信中的噪声，也适用于量子器件（如光源、频率转换器、开关、探测器等）中的噪声，并揭示了滤波在无噪声环境中也会降低单光子纯度和速率的现象，为优化多光子应用提供了指导。

Abstract: Filtering is commonly used in quantum optics to reject noise photons, and
also to enable interference between independent photons. However, filtering the
joint spectrum of photon pairs can reduce the inherent coincidence probability
or loss-independent heralding efficiency. Here, we investigate filtering for
multiphoton applications based on entanglement and interference (e.g., quantum
teleportation). We multiplex C-band entangled photons and C-band classical
communications into the same long-distance fibers, which enables scalable
low-loss quantum networking but requires filtering of spontaneous Raman
scattering noise from classical light. Using tunable-bandwidth filters,
low-jitter detectors, and polarization filters, we co-propagate
time-bin-entangled photons at wavelengths compatible with erbium-ion quantum
memories (1536.5 nm) and 10-Gbps C-band classical data over 25 km/25 km of
standard fiber. Narrow filtering enables mW-level C-band power, which exceeds
comparable studies by roughly an order of magnitude and could feasibly support
Tbps classical rates. We evaluate how performance depends on pump and filter
bandwidths, multipair emission, filter shapes, loss, phase matching, and how
quantum information is measured. We find a trade-off between improving noise
impact and single-mode purity and discuss mitigation methods toward optimal
multiphoton applications. Importantly, these results apply to noise in free
space and in quantum devices (sources, frequency converters, switches,
detectors, etc.) and provide insight on filter-induced degradation of
single-photon purity and rates even in noise-free environments.

</details>


### [364] [Adapting Quantum Machine Learning for Energy Dissociation of Bonds](https://arxiv.org/abs/2510.06563)
*Swathi Chandrasekhar,Shiva Raj Pokhrel,Navneet Singh*

Main category: quant-ph

TL;DR: 本研究提出了一个结合量子和经典机器学习模型来预测键解离能（BDE）的基准测试框架。


<details>
  <summary>Details</summary>
Motivation: 准确预测键解离能（BDE）对于理解反应机理以及合理设计分子和材料至关重要。

Method: 研究人员使用包含原子属性、化学键特征和局部环境描述符的化学特征集，在量子计算框架（Qiskit Aer，六个量子比特，ZZFeatureMap编码，RealAmplitudes变分ansatz）和经典机器学习模型（SVR，RF，MLP）上进行了BDE预测的基准测试，并比较了多种量子算法（VQR, QSVR, QNN, QCNN, QRF）。

Result: 研究结果表明，性能最优的量子模型（QCNN, QRF）在预测精度和鲁棒性方面与经典的集成模型和深度网络相当，特别是在化学中常见的BDE中等范围内。

Conclusion: 这项研究为量子计算在分子性质预测方面的应用提供了一个透明的基准，并为实现量子化学的近化学精度奠定了实际基础。

Abstract: Accurate prediction of bond dissociation energies (BDEs) underpins
mechanistic insight and the rational design of molecules and materials. We
present a systematic, reproducible benchmark comparing quantum and classical
machine learning models for BDE prediction using a chemically curated feature
set encompassing atomic properties (atomic numbers, hybridization), bond
characteristics (bond order, type), and local environmental descriptors. Our
quantum framework, implemented in Qiskit Aer on six qubits, employs
ZZFeatureMap encodings with variational ansatz (RealAmplitudes) across multiple
architectures Variational Quantum Regressors (VQR), Quantum Support Vector
Regressors (QSVR), Quantum Neural Networks (QNN), Quantum Convolutional Neural
Networks (QCNN), and Quantum Random Forests (QRF). These are rigorously
benchmarked against strong classical baselines, including Support Vector
Regression (SVR), Random Forests (RF), and Multi-Layer Perceptrons (MLP).
Comprehensive evaluation spanning absolute and relative error metrics,
threshold accuracies, and error distributions shows that top-performing quantum
models (QCNN, QRF) match the predictive accuracy and robustness of classical
ensembles and deep networks, particularly within the chemically prevalent
mid-range BDE regime. These findings establish a transparent baseline for
quantum-enhanced molecular property prediction and outline a practical
foundation for advancing quantum computational chemistry toward near chemical
accuracy.

</details>


### [365] [Excitonic Insulator and Possible Superfluid Based on Two-Dimensional Diamond](https://arxiv.org/abs/2510.06599)
*Shisheng Lin,Shaoqi Huang,Minhui Yang,Xin Chen,Hongjia Bi,Kangchen Xiong*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent research on excitonic insulator has progressed mainly based on narrow
bandgap semiconductor or semimetal. Herein, we realize excitonic insulator
based on two-dimensional (2D) wide band gap diamond with transition temperature
as high as 220K. The resistance rises dramatically by more than three orders,
which can be explained by the Bose-Einstein condensation (BEC) of excitons.
While cooling down below transition temperature, the wavelength of the bound
excitons caused by boron and nitrogen centers becomes highly overlapped,
leading to BEC process. Furthermore, the variable range hopping mechanism is
used to simulate the resistance as a function of temperature, which reveals the
formation of excitonic insulator. When temperature drops down further, a sudden
drop of resistance over three orders was observed around 60K, possibly due to
the formation of non-equilibrium excitonic superfluid resulting from highly
overlap of wavelength of the large density bound excitons at lower temperature.
This study provides evidences for excitonic insulator and possible superfluid
phase based on wide bandgap semiconductor.

</details>


### [366] [Hyperinvariant Spin Network States -- An AdS/CFT Model from First Principles](https://arxiv.org/abs/2510.06602)
*Fynn Otto,Refik Mansuroglu,Norbert Schuch,Otfried Gühne,Hanno Sahlmann*

Main category: quant-ph

TL;DR: 将SU(2)对称性纳入超不变张量网络，研究其在AdS/CFT对应和LQG中的应用，并证明相关态的存在性和限制。


<details>
  <summary>Details</summary>
Motivation: 研究在张量网络中加入局部SU(2)对称性，以连接量子信息和量子引力领域，特别是AdS/CFT对应和LQG。

Method: 考虑包含局部SU(2)对称性的超不变张量网络，并将其解释为三维量子引力的运动学量子态。推导并证明相关态的存在性和限制（无解定理），排除某些全纯最大纠缠态和全息编码。计算LQG面积算子的期望值，并讨论由边界相关性衰减引起的进一步限制。

Result: 证明了AdS/CFT对应在LQG的某些量子态中的实现，为[F. Pastawski et al., JHEP 06, 149 (2015)]提出的模型提供了第一原理依据。给出了超不变张量网络的具体示例，并证明了它们的存在性限制，排除了绝对最大纠缠态和一般全息码。计算了面积算子的期望值。

Conclusion: 包含局部SU(2)对称性的超不变张量网络可以实现AdS/CFT对应的重要方面，并与LQG中的运动学量子态相关。然而，局部SU(2)对称性也对这些网络的可能形式施加了限制，例如排除了绝对最大纠缠态和一般全息码。

Abstract: We study the existence and limitations for hyperinvariant tensor networks
incorporating a local SU(2) symmetry. As discrete implementations of the anti
de-Sitter/conformal field theory (AdS/CFT) correspondence, such networks have
created bridges between the fields of quantum information theory and quantum
gravity. Adding SU(2) symmetry to the tensor network allows a direct connection
to spin network states, a basis of the kinematic Hilbert space of loop quantum
gravity (LQG). We consider a particular situation where the states can be
interpreted as kinematic quantum states for three-dimensional quantum gravity.
We show that important aspects of the AdS/CFT correspondence are realized in
certain quantum states of the gravitational field in LQG, thus justifying, from
first principles, a class of models introduced by [F. Pastawski et al., JHEP
06, 149 (2015)]. We provide examples of hyperinvariant tensor networks, but
also prove constraints on their existence in the form of no-go theorems that
exclude absolutely maximally entangled states as well as general holographic
codes from local SU(2)-invariance. We calculate surface areas as expectation
values of the LQG area operator and discuss further possible constraints as a
consequence of a decay of correlations on the boundary.

</details>


### [367] [Algebraic Geometry Codes and Decoded Quantum Interferometry](https://arxiv.org/abs/2510.06603)
*Andi Gu,Stephen P. Jordan*

Main category: quant-ph

TL;DR: DQI将解码问题与优化问题配对，并将其应用于Hermitian码，在特定参数下 DQI 的表现优于经典算法。


<details>
  <summary>Details</summary>
Motivation: Hermitian码相比Reed-Solomon码在量子实现上具有更高的效率，因此研究DQI在Hermitian码上的应用具有实际意义。

Method: 将DQI应用于Hermitian码，定义了Hermitian Optimal Polynomial Intersection (HOPI)问题，并将其与经典算法（Prange算法、模拟退火、代数列表恢复）进行比较。

Result: 在大的参数范围内，DQI在HOPI问题上比经典算法能获得更好的近似解，表明DQI的量子加速优势可以扩展到更广泛的多项式回归问题。

Conclusion: DQI的量子加速优势不仅限于Reed-Solomon码，还可以扩展到更广泛的代数几何码和代数簇上的多项式回归问题。

Abstract: Decoded Quantum Interferometry (DQI) defines a duality that pairs decoding
problems with optimization problems. The original work on DQI considered
Reed-Solomon decoding, whose dual optimization problem, called Optimal
Polynomial Intersection (OPI), is a polynomial regression problem over a finite
field. Here, we consider a class of algebraic geometry codes called Hermitian
codes, which achieve block length $q^3$ using alphabet $\mathbb{F}_{q^2}$
compared to Reed-Solomon's limitation to block length $q$ over $\mathbb{F}_q$,
requiring approximately one-third fewer qubits per field element for quantum
implementations. We show that the dual optimization problem, which we call
Hermitian Optimal Polynomial Intersection (HOPI), is a polynomial regression
problem over a Hermitian curve, and because the dual to a Hermitian code is
another Hermitian code, the HOPI problem can also be viewed as approximate list
recovery for Hermitian codes. By comparing to Prange's algorithm, simulated
annealing, and algebraic list recovery algorithms, we find a large parameter
regime in which DQI efficiently achieves a better approximation than these
classical algorithms, suggesting that the apparent quantum speedup offered by
DQI extends beyond Reed-Solomon codes to a broader class of polynomial
regression problems on algebraic varieties.

</details>


### [368] [Postselected amplification and photon recycling applied to optical sensing of magnetic fields](https://arxiv.org/abs/2510.06610)
*Yazhi Niu,Jialin Li,Lupei Qin,Xin-Qi Li*

Main category: quant-ph

TL;DR: 通过结合使用后选择放大和光子回收技术，可以显著提高磁场精密测量的灵敏度，克服了传统方法中数据丢弃的缺点，并实现了信号放大。


<details>
  <summary>Details</summary>
Motivation: 提高光学设置中磁场精密测量的性能。

Method: 应用后选择放大和光子回收技术，并提出两种回收方案，推导了放大信号和测量灵敏度的解析表达式。

Result: 结果显示，与传统测量相比，性能得到显著改善。数据丢弃的缺点得到消除，同时信号得到放大，从而提高了信噪比。

Conclusion: 结合使用后选择放大和光子回收技术可以显著提高磁场精密测量的性能。

Abstract: We apply the combined technique of postselected amplification and
photon-recycling to an optical setup of magnetic field precision measurement.
We propose two recycling schemes and carry out analytic expressions for the
amplified signal and measurement sensitivity. The results show significant
improvement of performance over conventional measurement. The underlying reason
is twofold. On one aspect, introducing the technique of recycling eliminates
the shortcoming of data discarding in postselection, thus maintains similar
noise level of conventional measurement (without postselection). On the other
aspect, performing intentional postselection within the recycling framework,
which was originally proposed in the context of gravitational wave detection,
can amplify the signal. Thus, the measurement signal-to-noise ratio is
enhanced.

</details>


### [369] [Layer codes as partially self-correcting quantum memories](https://arxiv.org/abs/2510.06659)
*Shouzhen Gu,Libor Caha,Shin Ho Choe,Zhiyang He,Aleksander Kubica,Eugene Tang*

Main category: quant-ph

TL;DR: Layer codes are 3D stabilizer codes suitable for self-correcting quantum memories, exhibiting optimal parameter scaling and a polynomial energy barrier. They offer improved performance over existing models like the cubic and welded solid codes, with partial self-correction arising from a diverging energy barrier, distinct from efficient decoding requirements. The paper also analyzes random layer codes derived from Calderbank-Shor-Steane codes, confirming their optimal scaling and polynomial energy barrier, and presents numerical evidence of partial self-correction through memory time studies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to find candidate codes for self-correcting quantum memories that achieve optimal scaling of code parameters and a polynomial energy barrier. The paper aims to introduce and analyze layer codes, comparing them to existing models and exploring their potential for robust quantum information storage.

Method: The paper introduces two decoding algorithms for layer codes with guarantees for local stochastic and adversarial noise. It then proves that layer codes are partially self-correcting quantum memories. Additionally, it analyzes layer codes constructed from random Calderbank-Shor-Steane codes, investigating their scaling properties and energy barrier. Finally, numerical studies of memory times are presented to support the theoretical findings.

Result: Layer codes are shown to outperform existing models like the cubic code and welded solid code in terms of self-correction. Partial self-correction is demonstrated to arise from a diverging energy barrier, independent of efficient decoding. Random layer codes exhibit optimal scaling of code parameters (up to logarithmic corrections) and a polynomial energy barrier. Numerical simulations are consistent with partial self-correction behavior.

Conclusion: Layer codes are promising candidates for self-correcting quantum memories due to their optimal scaling and polynomial energy barrier. The work highlights the significance of a diverging energy barrier for partial self-correction, distinguishing it from systems requiring efficient decoding. The analysis of random layer codes and supporting numerical studies further validate their potential for practical quantum memory applications.

Abstract: We investigate layer codes, a family of three-dimensional stabilizer codes
that can achieve optimal scaling of code parameters and a polynomial energy
barrier, as candidates for self-correcting quantum memories. First, we
introduce two decoding algorithms for layer codes with provable guarantees for
local stochastic and adversarial noise, respectively. We then prove that layer
codes constitute partially self-correcting quantum memories which outperform
previously analyzed models such as the cubic code and the welded solid code.
Notably, we argue that partial self-correction without the requirement of
efficient decoding is more common than expected, as it arises solely from a
diverging energy barrier. This draws a sharp distinction between partially
self-correcting systems and partially self-correcting memories. Another novel
aspect of our work is an analysis of layer codes constructed from random
Calderbank-Shor-Steane codes. We show that these random layer codes have
optimal scaling (up to logarithmic corrections) of code parameters and a
polynomial energy barrier. Finally, we present numerical studies of their
memory times and report behavior consistent with partial self-correction.

</details>


### [370] [State preparation and symmetries](https://arxiv.org/abs/2510.06702)
*Ivana Miháliková,Joseph Carlson,Duff Neill,Ionel Stetcu*

Main category: quant-ph

TL;DR: VQE算法中对称性对收敛性的重要性


<details>
  <summary>Details</summary>
Motivation: 研究对称性在VQE算法中的作用，以及对称性如何影响准备哈密顿量的基础态或特定低能态。

Method: 通过保留所有对称性来改进VQE算法的收敛性，并将其应用于两个自旋问题：受超新星中中微子味演化启发的随机全耦合问题和标准的Heisenberg自旋哈密顿量。

Result: 在保留对称性的情况下，VQE算法的收敛性得到了显著改善，并且在标准不受约束的变分算法失败的情况下，能够获得近乎精确的解。

Conclusion: 保留哈密顿量的所有对称性可以加速VQE算法的收敛，并优于标准投影算法。

Abstract: We demonstrate the importance of symmetries in Variational Quantum
Eigensolver (VQE) algorithms to prepare the ground or specific low-lying states
of quantum Hamiltonians. We examine two spin problems, one with random
all-to-all couplings inspired by neutrino flavor evolution in supernovae, and
the standard Heisenberg spin Hamiltonian on a $4 \times 3$ lattice. The
neutrino Hamiltonian has the total spin $J$ and third component $J_{\rm{z}}$ as
its only symmetries. The Heisenberg model has these symmetries plus
translational invariance and reflection symmetry.
  We demonstrate that the convergence of variational methods is dramatically
improved by keeping all symmetries. In both cases a nearly exact solution can
be obtained in cases where standard unconstrained variational algorithms fail.
Since variational algorithms can use standard Trotter steps as part of the
optimization, allowing additional correlations that obey all the symmetries of
the Hamiltonian will speed convergence of variational algorithms. This will
lead to faster convergence than standard projection algorithms.

</details>


### [371] [Continuous measurement-based holonomic quantum computation](https://arxiv.org/abs/2510.06725)
*Anirudh Lanka,Juan Garcia-Nila,Todd A. Brun*

Main category: quant-ph

TL;DR: 该研究提出一种利用量子齐纳效应生成霍洛诺米（holonomies）的方案，仅通过测量即可在量子稳定码上执行逻辑幺正操作。


<details>
  <summary>Details</summary>
Motivation: 旨在利用量子齐纳效应，通过测量实现量子稳定码的逻辑幺正操作，以简化量子计算过程。

Method: 通过测量一系列旋转后的稳定码生成子，绝热地旋转量子纠错码空间。利用量子齐纳效应将量子态约束在瞬时码空间内，或通过测量诱导跃迁到旋转后的正交子空间。当旋转完成闭合循环时，码态通过霍洛诺米（逻辑幺正变换）进行变换。研究推导了产生期望霍洛诺米的旋转稳定码生成子序列，并计算了实现该过程的总时间。当测量将态移至正交子空间时，提出了一种修改旋转可观测量路径以返回原码或原霍洛诺米的方法。最后，确定了保持给定错误集可纠正性的码和可测量的可观测量条件，并在不满足条件时，通过增加最多两个辅助量子比特来增强码。

Result: 提出了一种基于量子齐纳效应的霍洛诺米生成方案，实现了纯粹通过测量在量子稳定码上执行逻辑幺正操作。推导了实现特定霍洛诺米所需的旋转稳定码生成子序列和总时间。提供了在测量导致状态跳转时，返回原码或通过霍洛诺米模拟来纠正的方法。确定了保持可纠正性的条件，并提出了在不满足条件时通过增加辅助量子比特来修复的方法。

Conclusion: 该研究提出的利用量子齐纳效应通过测量生成霍洛诺米的方法，为在量子稳定码上实现逻辑幺正操作提供了一种新的途径，并在理论上证明了其可行性和纠错能力。

Abstract: We propose a scheme to generate holonomies using the Quantum Zeno effect,
enabling logical unitary operations on quantum stabilizer codes purely through
measurements. The quantum error-correcting code space is adiabatically rotated
by measuring a succession of rotated stabilizer generators. When the rotation
is sufficiently slow, the state remains confined to the instantaneous code
space by the Zeno effect; otherwise, measurement-induced jumps can occur into a
rotated orthogonal subspace. If the rotation completes a closed loop, the code
state is transformed by a holonomy: a logical unitary transformation. We
analytically derive the sequence of rotated stabilizer generators that produce
a desired holonomy, and find the total time required to implement this
procedure with a given success probability. If a measurement moves the state to
the orthogonal subspace, we present a method to alter the path of the rotated
observables to return the state either to the original code or the original
error space with the desired holonomy; in the latter case, the holonomy is
emulated. Finally, we establish conditions on the code and the measured
observables that preserve the correctability of a given error set. When a code
fails to meet the error-correcting conditions, our protocol remains applicable
by augmenting the code with at most two ancilla qubits.

</details>


### [372] [Advantages of Global Entanglement-Distillation Policies in Quantum Repeater Chains](https://arxiv.org/abs/2510.06737)
*Iftach Yakar,Michael Ben-Or*

Main category: quant-ph

TL;DR: 全局确定性策略在量子中继器中优于局部策略。


<details>
  <summary>Details</summary>
Motivation: 量子中继器对于实现长距离量子通信至关重要，因为光子损耗会随着信道距离呈指数增长。本研究旨在比较局部和全局确定性策略在量子中继器中的性能。

Method: 通过模拟等距中继器链，并利用双向经典通信，比较局部和全局策略在不同网络和硬件参数下的蒸馏决策，以评估通信速率。

Result: 全局确定性策略在通信速率上持续优于局部策略，并且在某些情况下能决定是否可能进行秘密通信。对于大型中继器链（N>512），全局策略可将保密密钥率（SKR）提高两个数量级。

Conclusion: 局部蒸馏决策可能不是最优的，全局确定性策略在量子中继器链中具有优势，并可为未来的协议设计提供参考。

Abstract: Quantum repeaters are essential for achieving long-distance quantum
communication due to photon loss, which grows exponentially with the channel
distance. Current quantum repeater generations use entanglement distillation
protocols, where the decision of when to perform distillation depends on either
local or global knowledge. Recent approaches for quantum repeaters, such as
Mantri et al. (arXiv:2409.06152), consider using deterministic local decision
policies for entanglement distillation. We ask whether global deterministic
policies outperform local ones in terms of communication rate. We simulate
equidistant repeater chains, assisted by two-way classical communication, and
compare local and global policies for distillation decisions, spanning large
distances and varying network and hardware parameters. Our findings show that
global deterministic policies consistently outperform these local ones, and in
some cases, determine whether secret communication is possible. For large
repeater chains ($N>512$), global policies improve SKR by two orders of
magnitude. These results suggest that local distillation decisions in quantum
repeater chains may not be optimal, and may inform future protocol design.

</details>


### [373] [Fast-forwardable Lindbladians imply quantum phase estimation](https://arxiv.org/abs/2510.06759)
*Zhong-Xia Shang,Naixu Guo,Patrick Rebentrost,Alán Aspuru-Guzik,Tongyang Li,Qi Zhao*

Main category: quant-ph

TL;DR: 简单 Lindbladian 过程可用于执行 QPE 任务，但受限于标准量子极限。提出的量子算法可模拟这些 Lindbladian，实现二次加速，并可作为新的 Heisenberg 极限 QPE 算法，同时还能高效制备 Gibbs 态。


<details>
  <summary>Details</summary>
Motivation: 将量子相位估计 (QPE) 和 Lindbladian 动力学这两个量子信息科学的基础概念联系起来，并探索 Lindbladian 过程在 QPE 任务中的应用潜力。

Method: 提出了一种量子算法，能够以 $\mathcal{O}(\sqrt{t\log(\varepsilon^{-1})})$ 的成本模拟 Lindbladian 动力学，误差为 $\varepsilon$。该算法的机制不同于哈密顿动力学的快进示例。

Result: 证明了 Lindbladian 动力学可以被加速模拟，其时间复杂度为 $\mathcal{O}(\sqrt{t\log(\varepsilon^{-1})})$，这与 Hamiltonian 动力学不同。该算法可作为新的 Heisenberg 极限 QPE 算法，并应用于高效的 Gibbs 态制备，同时揭示了在 Pauli 噪声下，退相干效应可能被二次加速。

Conclusion: 这项工作成功地将标准量子极限与 Heisenberg 极限的过渡联系起来，并实现了耗散动力学的快进。此外，还展示了快进模拟在 Gibbs 态制备中的应用，以及退相干效应在 Pauli 噪声下的二次加速现象。

Abstract: Quantum phase estimation (QPE) and Lindbladian dynamics are both foundational
in quantum information science and central to quantum algorithm design. In this
work, we bridge these two concepts: certain simple Lindbladian processes can be
adapted to perform QPE-type tasks. However, unlike QPE, which achieves
Heisenberg-limit scaling, these Lindbladian evolutions are restricted to
standard quantum limit complexity. This indicates that, different from
Hamiltonian dynamics, the natural dissipative evolution speed of such
Lindbladians does not saturate the fundamental quantum limit, thereby
suggesting the potential for quadratic fast-forwarding. We confirm this by
presenting a quantum algorithm that simulates these Lindbladians for time $t$
within an error $\varepsilon$ using
$\mathcal{O}\left(\sqrt{t\log(\varepsilon^{-1})}\right)$ cost, whose mechanism
is fundamentally different from the fast-forwarding examples of Hamiltonian
dynamics. As a bonus, this fast-forwarded simulation naturally serves as a new
Heisenberg-limit QPE algorithm. Therefore, our work explicitly bridges the
standard quantum limit-Heisenberg limit transition to the fast-forwarding of
dissipative dynamics. We also adopt our fast-forwarding algorithm for efficient
Gibbs state preparation and demonstrate the counter-intuitive implication: the
allowance of a quadratically accelerated decoherence effect under arbitrary
Pauli noise.

</details>


### [374] [Constant-Overhead Addressable Gates via Single-Shot Code Switching](https://arxiv.org/abs/2510.06760)
*Louis Golowich,Kathleen Chang,Guanyu Zhu*

Main category: quant-ph

TL;DR: 该研究提出了一种低开销的容错协议，用于在恒定码率的量子LDPC码上执行可寻址和并行的逻辑操作，解决了现有方案的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 实现量子LDPC码的可寻址和并行逻辑操作，并降低操作开销，以推动量子容错方案的发展。

Method: 提出容错协议，构造了实现逻辑比特置换、目标Hadamard/CNOT门、逻辑码态制备以及对所有逻辑比特应用Hadamard门的gadget。核心技术是提出了一个常数开销的码切换（code-switching）方法，实现了2D和3D乘积码之间的转换，并证明了gadget在局域随机噪声下的常数阈值。此外，还开发了一种高维乘积码的小集翻转（small-set flip）译码器。

Result: 成功构造了适用于恒定码率、多项式距离量子LDPC码的gadget，实现了常数时空开销的地址化和并行逻辑操作。该方案在局域随机噪声下具有常数阈值，并且能够实现2D乘积码的单次状态制备。

Conclusion: 该研究提出的容错协议和技术能够以常数时空开销在量子LDPC码上实现复杂的逻辑操作，为构建更高效的量子计算机提供了新的途径，并为后续研究提供了理论基础和技术工具。

Abstract: It is a major challenge to perform addressable and parallel logical
operations on constant-rate quantum LDPC (qLDPC) codes. Indeed, the overhead of
targeting specific logical qubits represents a crucial bottleneck in many
quantum fault-tolerance schemes.
  We introduce fault-tolerant protocols for performing various addressable as
well as parallel logical operations with constant space-time overhead, on a
family of constant-rate and polynomial-distance qLDPC codes. Specifically, we
construct gadgets for a large class of permutations of logical qubits. We apply
these logical permutations to construct gadgets for applying a targeted
Hadamard (or $CNOT$) gate on any chosen logical qubit (pair). We also construct
gadgets for preparing logical code states, and for applying Hadamard gates on
all logical qubits in a codeblock. All of our gadgets use constant quantum
space-time overhead along with polynomially bounded classical computation.
Prior protocols for such operations required larger overhead, or else relied on
codes with certain symmetries that lack known asymptotic constructions.
  Our codes are given by tensor products of classical codes constructed from
lossless expander graphs. Our core technical contribution is a
constant-overhead code-switching procedure between 2- and 3-dimensional product
codes, which generalizes Bombin's dimensional jump (arXiv:1412.5079). We prove
that all of our gadgets exhibit a constant threshold under locally stochastic
noise. Along the way, we develop a small-set flip decoder for high-dimensional
product codes from lossless expanders. Our techniques yield additional
interesting consequences, such as single-shot state preparation of
2-dimensional product codes with constant space-time overhead. We also propose
a method for performing parallel non-Clifford gates by extending our techniques
to codes supporting transversal application of such gates.

</details>


### [375] [Theoretical Guarantees of Variational Quantum Algorithm with Guiding States](https://arxiv.org/abs/2510.06764)
*Tuyen Nguyen,Mária Kieferová*

Main category: quant-ph

TL;DR: VQAs are promising for quantum advantage but lack convergence guarantees. This paper introduces a variational algorithm with guiding states, providing the first theoretical guarantees on VQA convergence and generalization by mapping training dynamics to a kernel model. Guiding states accelerate convergence, reduce errors, and ensure stability, validated by experiments on 2D Heisenberg models.


<details>
  <summary>Details</summary>
Motivation: VQAs lack rigorous guarantees of convergence and generalization, unlike QPE which provides provable performance under the guiding state assumption. This work investigates if similar guarantees can be obtained for VQAs.

Method: Introduced a variational quantum algorithm with guiding states and developed a linearization trick to map the training dynamics to a kernel model, enabling theoretical analysis.

Result: Established the first theoretical guarantees on convergence and generalization for VQAs under the guiding state assumption. Showed that guiding states accelerate convergence, suppress finite-size error terms, and ensure stability. Validated with numerical experiments on 2D random Heisenberg models.

Conclusion: The proposed variational quantum algorithm with guiding states offers theoretical guarantees for convergence and generalization, addressing key limitations of current VQAs. The linearization trick provides a novel proof technique, and experimental validation confirms the benefits of guiding states.

Abstract: Variational quantum algorithms (VQAs) are prominent candidates for near-term
quantum advantage but lack rigorous guarantees of convergence and
generalization. By contrast, quantum phase estimation (QPE) provides provable
performance under the guiding state assumption, where access to a state with
non-trivial overlap with the ground state enables efficient energy estimation.
In this work, we ask whether similar guarantees can be obtained for VQAs. We
introduce a variational quantum algorithm with guiding states aiming towards
predicting ground-state properties of quantum many-body systems. We then
develop a proof technique-the linearization trick-that maps the training
dynamics of the algorithm to those of a kernel model. This connection yields
the first theoretical guarantees on both convergence and generalization for the
VQA under the guiding state assumption. Our analysis shows that guiding states
accelerate convergence, suppress finite-size error terms, and ensure stability
across system dimensions. Finally, we validate our findings with numerical
experiments on 2D random Heisenberg models.

</details>


### [376] [On the complexity of estimating ground state entanglement and free energy](https://arxiv.org/abs/2510.06796)
*Sevag Gharibian,Jonas Kamminga*

Main category: quant-ph

TL;DR: 该论文研究了局部哈密顿量基态纠缠结构的复杂性，并将其与量子计算复杂性类 qq-QAM 和 QMA(2) 联系起来。


<details>
  <summary>Details</summary>
Motivation: 理解局部哈密顿量基态的纠缠结构具有重要的物理意义，可应用于张量网络设计和量子纠错码等领域。

Method: 研究了估算基态纠缠以及低能态和吉布斯态熵估算的复杂性，并将问题与 qq-QAM 和 QMA(2) 复杂性类相关联。

Result: （1）检测高纠缠基态是 qq-QAM 完全问题；（2）计算亥姆霍兹自由能的加性误差近似（或配分函数的乘性误差近似）在 qq-QAM 类中；（3）检测低纠缠基态是 QMA(2) 困难问题；（4）检测接近积态的低能态的复杂性范围为 QMA 完全到 QMA(2) 完全。

Conclusion: 该研究在自由能问题上取得进展，并首次提出了使用局部哈密顿量的 QMA(2) 完全问题。

Abstract: Understanding the entanglement structure of local Hamiltonian ground spaces
is a physically motivated problem, with applications ranging from tensor
network design to quantum error-correcting codes. To this end, we study the
complexity of estimating ground state entanglement, and more generally entropy
estimation for low energy states and Gibbs states. We find, in particular, that
the classes qq-QAM [Kobayashi, le Gall, Nishimura, SICOMP 2019] (a quantum
analogue of public-coin AM) and QMA(2) (QMA with unentangled proofs) play a
crucial role for such problems, showing: (1) Detecting a high-entanglement
ground state is qq-QAM-complete, (2) computing an additive error approximation
to the Helmholtz free energy (equivalently, a multiplicative error
approximation to the partition function) is in qq-QAM, (3) detecting a
low-entanglement ground state is QMA(2)-hard, and (4) detecting low energy
states which are close to product states can range from QMA-complete to
QMA(2)-complete. Our results make progress on an open question of [Bravyi,
Chowdhury, Gosset and Wocjan, Nature Physics 2022] on free energy, and yield
the first QMA(2)-complete Hamiltonian problem using local Hamiltonians (cf. the
sparse QMA(2)-complete Hamiltonian problem of [Chailloux, Sattath, CCC 2012]).

</details>


### [377] [Near-Asymptotically-Good Quantum Codes with Transversal CCZ Gates and Sublinear-Weight Parity-Checks](https://arxiv.org/abs/2510.06798)
*Louis Golowich,Venkatesan Guruswami*

Main category: quant-ph

TL;DR: 本文提出了具有线性码维和码距、亚线性局部性的量子码，支持横贯的非克利福德门，并设计了相应的解码算法。


<details>
  <summary>Details</summary>
Motivation: 构建支持容错（例如横贯）非克利福德门且具有低权重奇偶校验测量的量子码是一个主要挑战。

Method: 通过经典码的乘积构造量子码，并设计了基于多变量Prony方法的解码算法。

Result: 构造了具有线性码维和码距、亚线性局部性的量子码，支持横贯CCZ门；设计了高效解码算法；并将字母表大小降至常数；还展示了如何进一步降低局部性。

Conclusion: 所提出的量子码及其解码算法在量子纠错领域取得了重要进展，解决了相关猜想，并为量子计算的容错提供了新的途径。

Abstract: It is a major challenge to construct good quantum codes supporting
fault-tolerant (e.g. transversal) non-Clifford gates with low-weight
parity-check measurements. In this paper, we construct the first known quantum
codes with linear dimension and distance supporting transversal non-Clifford
gates that have sublinear locality (i.e. parity-check weight). Specifically, we
construct codes with transversal $CCZ$ gates that have dimension and distance
$\Theta(N)$ and locality $O(\sqrt{N})$, where $N$ denotes the block length. We
furthermore design an efficient decoding algorithm for these codes. The
alphabet size of these codes is $q=\Theta(\sqrt{N})$, but it can be reduced to
a constant (e.g. $q=2$) while incurring a polylogarithmic loss in other
parameters. We also show how to decrease the locality to $O(N^{1/3})$, albeit
with a larger alphabet size and slightly lower distance.
  We construct these codes as products of classical codes with appropriate
algebraic structure. While our quantum codes are subsystem codes with
non-commuting gauge operators, we show they nevertheless permit error
correction from noisy syndrome measurements.
  As byproducts, we prove multiple technical results of independent interest.
In particular, our efficient decoder can be viewed as a new multivariate
generalization of Prony's method for reconstructing a function from partial
access to its Fourier transform. Meanwhile, our distance analysis involves new
connections to the classical study of maximally recoverable codes. Our results
on product codes also resolve a conjecture of Bravyi & Hastings (2014) in the
large-alphabet regime, by providing a new construction of quantum codes with
dimension and distance $\Theta(N)$ and locality $N^\epsilon$ for arbitrary
$\epsilon>0$.

</details>


### [378] [Quantum Computing Methods for Malware Detection](https://arxiv.org/abs/2510.06803)
*Eliška Krátká,Aurél Gábor Gábris*

Main category: quant-ph

TL;DR: 本研究利用量子机器学习（QML），特别是量子支持向量机（QSVM），来提升恶意软件检测能力，并与传统支持向量机（SVM）进行性能比较。


<details>
  <summary>Details</summary>
Motivation: 为了探索量子计算在增强恶意软件检测方面的潜力，并评估QSVM算法相对于SVM的性能。

Method: 使用公开的便携式可执行（PE）文件数据集，通过Qiskit SDK在本地模拟器和IBM量子计算机上实现和评估了包含量子核的QSVM算法，并分析了其在处理大规模恶意软件检测计算中的表现。

Result: 实验结果表明了量子计算机在处理大规模计算方面的行为和性能，并总结了使用Qiskit接口操作量子硬件的实践经验。

Conclusion: 详细说明了在实际操作中遇到的关键问题，如电路的转译缺失和超出最大作业大小限制，并提供了相应的解决方法。

Abstract: In this paper, we explore the potential of quantum computing in enhancing
malware detection through the application of Quantum Machine Learning (QML).
Our main objective is to investigate the performance of the Quantum Support
Vector Machine (QSVM) algorithm compared to SVM. A publicly available dataset
containing raw binaries of Portable Executable (PE) files was used for the
classification. The QSVM algorithm, incorporating quantum kernels through
different feature maps, was implemented and evaluated on a local simulator
within the Qiskit SDK and IBM quantum computers. Experimental results from
simulators and quantum hardware provide insights into the behavior and
performance of quantum computers, especially in handling large-scale
computations for malware detection tasks. The work summarizes the practical
experience with using quantum hardware via the Qiskit interfaces. We describe
in detail the critical issues encountered, as well as the fixes that had to be
developed and applied to the base code of the Qiskit Machine Learning library.
These issues include missing transpilation of the circuits submitted to IBM
Quantum systems and exceeding the maximum job size limit due to the submission
of all the circuits in one job.

</details>


### [379] [A Quantum Linear Systems Pathway for Solving Differential Equations](https://arxiv.org/abs/2510.06837)
*Abhishek Setty*

Main category: quant-ph

TL;DR: 结合量子奇异值变换（QSVT）和分块编码，提出了一种解决微分方程的量子线性系统新方法，并在计算流体动力学问题（如热传导方程和非线性 Burgers 方程）上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在提出一种解决微分方程的系统性方法，利用量子线性系统的框架，结合分块编码和量子奇异值变换（QSVT）。

Method: 该方法结合了分块编码和量子奇异值变换（QSVT）来解决微分方程。通过在复杂的三对角线线性系统上进行演示，并将其应用于计算流体动力学问题，如混合边界条件下的热传导方程和非线性 Burgers 方程。

Result: 在热传导方程的缩放分析中，研究了离散化对最小奇异值和 QSVT 所需多项式次数的影响，并指出了线路深度开销是关键瓶颈。对于 Burgers 方程，研究展示了如何有效地分块编码 Carleman 线性化后的非线性动力学，并在 QSVT 框架内求解。

Conclusion: 研究结果展示了当前方法的潜力和局限性，强调了有效估计最小奇异值、深度约减技术以及与经典可达性进行基准测试的必要性。该方法为推动量子线性系统方法在可扩展应用方面奠定了基础。

Abstract: We present a systematic pathway for solving differential equations within the
quantum linear systems framework by combining block encoding with Quantum
Singular Value Transformation (QSVT). The approach is demonstrated on a complex
tridiagonal linear system and extended to problems in computational fluid
dynamics: the heat equation with mixed boundary conditions and the nonlinear
Burgers' equation. Our scaling analysis of the heat equation shows how
discretization influences the minimum singular value and the polynomial degree
required for QSVT, identifying circuit-depth overhead as a key bottleneck. For
Burgers' equation, we illustrate how Carleman-linearized nonlinear dynamics can
be efficiently block encoded and solved within the QSVT framework. These
results highlight both the potential and limitations of current methods,
underscoring the need for efficient estimation of minimum singular value,
depth-reduction techniques, and benchmarks against classical reachability. This
pathway lays a foundation for advancing quantum linear system methods toward
large-scale applications.

</details>


### [380] [Mirrored Entanglement Witnesses for Multipartite and High-Dimensional Quantum Systems](https://arxiv.org/abs/2510.06863)
*Jiheon Seong,Anindita Bera,Beatrix C. Hiesmayr,Dariusz Chruscinski,Joonwoo Bae*

Main category: quant-ph

TL;DR: In this paper, mirrored entanglement witnesses (EWs) are investigated for multipartite and high-dimensional quantum systems. Mirrored EWs provide both lower and upper bounds for detecting entangled states, enhancing the capability of single EWs. The study provides specific examples for various quantum states and generalizes the concept to a chain of connected EWs.


<details>
  <summary>Details</summary>
Motivation: Entanglement witnesses (EWs) are crucial for detecting entanglement, but their effectiveness can be limited. Mirrored EWs offer a way to enhance their capability by providing both lower and upper bounds, thus detecting a larger set of entangled states.

Method: The paper develops and investigates mirrored EWs for multipartite qubit states and high-dimensional systems. This includes providing explicit mirrored EWs for specific states like n-partite GHZ states, graph states, and tripartite bound entangled states. It also constructs mirrored pairs of optimal EWs and generalizes the concept to a chain of connected EWs.

Result: Mirrored EWs are provided for n-partite GHZ states, two-colorable graph states, and tripartite bound entangled states. Optimal EWs are shown to be reflectable. For bipartite systems, mirrored EWs for existing optimal EWs are presented, and a mirrored pair of optimal EWs in dimension three is constructed. The concept of mirrored EWs is generalized.

Conclusion: The developed mirrored EWs enhance the capability of detecting entangled states in multipartite and high-dimensional quantum systems, making EWs a more versatile tool in quantum information theory.

Abstract: Entanglement witnesses (EWs) are a versatile tool to detect entangled states
and characterize related properties of entanglement in quantum information
theory. A witness $W$ corresponds to an observable satisfying
$\mathrm{tr}[W\sigma_{\mathrm{sep}}]\geq 0$ for all separable states
$\sigma_{\mathrm{sep}}$; entangled states are detected once the inequality is
violated. Recently, mirrored EWs have been introduced by showing that there
exist non-trivial upper bounds to EWs, \begin{eqnarray} u_W\geq
\mathrm{tr}[W\sigma_{\mathrm{sep}}]\geq 0. \nonumber \end{eqnarray} An upper
bound to a witness $W$ signifies the existence of the other one $M$, called a
mirrored EW, such that $W+M = u_W I \otimes I$. The framework of mirrored EWs
shows that a single EW can be even more useful, as it can detect a larger set
of entangled states by lower and upper bounds.
  In this work, we develop and investigate mirrored EWs for multipartite qubit
states and also for high-dimensional systems, to find the efficiency and
effectiveness of mirrored EWs in detecting entangled states. We provide
mirrored EWs for $n$-partite GHZ states, graph states such as two-colorable
states, and tripartite bound entangled states. We also show that optimal EWs
can be reflected with each other. For bipartite systems, we present mirrored
EWs for existing optimal EWs and also construct a mirrored pair of optimal EWs
in dimension three. Finally, we generalize mirrored EWs such that a pair of EWs
can be connected by another EW, i.e., $W+M =K$ is also an EW. Our results
enhance the capability of EWs to detect a larger set of entangled states in
multipartite and high-dimensional quantum systems.

</details>


### [381] [On the emergence of quantum Darwinism and pointer states for non-commuting evolutions](https://arxiv.org/abs/2510.06867)
*Diana A. Chisholm,G. Massimo Palma,Luca Innocenti*

Main category: quant-ph

TL;DR: Quantum Darwinism enables objectivity in quantum systems by encoding information into the environment, but this usually requires commuting Hamiltonians. This paper shows that objectivity and pointer states can emerge even with non-commuting Hamiltonians, by relaxing the definition of pointer states.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate the emergence of objectivity in quantum systems through quantum Darwinism when the system and interaction Hamiltonians do not commute, a scenario traditionally thought to be incompatible with pointer states.

Method: The study analyzes the emergence of objectivity under non-commuting Hamiltonians and proposes a relaxed definition of pointer states that is always well-defined when information redundancy exists, and which reduces to the standard definition for commuting evolutions.

Result: The research demonstrates that non-commuting evolutions do not prevent the emergence of objective states. A more generalized definition of pointer states is introduced, which is applicable in broader conditions.

Conclusion: Objectivity and pointer states can emerge in quantum systems even when the system and environment Hamiltonians do not commute. A relaxed definition of pointer states broadens the understanding of this phenomenon.

Abstract: Quantum systems achieve objectivity by redundantly encoding information about
themselves into the surrounding environment, through a mechanism known as
quantum Darwinism. When this happens, observes measure the environment and
infer the system to be in one of its pointer states. We study the emergence of
objectivity whenever the Hamiltonian of the system and the interaction
Hamiltonian between system and environment do not commute, a condition which is
thought to be incompatible with the presence of pointer states. We show that,
not only non-commuting evolutions allow for the emergence of objective states,
but it is possible to give a more relaxed definition of pointer states, that is
always well defined whenever there is redundancy of information, and coincides
with the usual one for commuting evolutions.

</details>


### [382] [Nonlinear photonic architecture for fault-tolerant quantum computing](https://arxiv.org/abs/2510.06890)
*Maike Ostmann,Joshua Nunn,Alex E. Jones*

Main category: quant-ph

TL;DR: 提出了一种包含强单光子非线性的新型抗量子计算架构，可显著降低资源开销并提高损耗容限。


<details>
  <summary>Details</summary>
Motivation: 与传统的基于线性光学的方法相比，线性光学方法在光子产生和纠缠操作方面需要大量的冗余，而我们提出的非线性方法可以显著降低资源开销。

Method: 通过在基于GHZ测量的光子架构中引入强单光子非线性效应。

Result: 在32光子资源状态和叶状表面代码下，可以实现约12%的基线损耗容限，远高于线性方法。

Conclusion: 引入非线性原语可以极大地改进抗量子计算的实际实现。

Abstract: We propose a novel architecture for fault-tolerant quantum computing that
incorporates strong single-photon nonlinearities into a photonic
GHZ-measurement-based architecture. The nonlinearities substantially reduce
resource overheads compared to conventional linear-optics-based architectures,
which require significant redundancy to accommodate probabilistic photon
generation and probabilistic entangling operations. By removing linear-optical
failure modes, our nonlinear architecture can also tolerate much higher optical
losses than linear approaches, with a baseline loss tolerance of $\sim$12\%
using a 32-photon resource state and a foliated surface code. Our results show
how introducing a nonlinear primitive enables dramatic improvements in
practical implementations of fault-tolerant quantum computing.

</details>


### [383] [Optimizing LOCC Protocols on Product Stiefel Manifold](https://arxiv.org/abs/2510.06909)
*Ze-Tong Li,Xin Wang*

Main category: quant-ph

TL;DR: 该研究提出了一种利用黎曼优化在复合Stiefel流形上优化固定轮次LOCC的新框架，该框架能够生成近乎最优的LOCC协议，并已成功应用于纠缠蒸馏和状态合并等量子信息处理任务。


<details>
  <summary>Details</summary>
Motivation: LOCC协议的设计和优化因其复杂的结构而变得难以处理，尤其是在通信轮次有限的情况下，确定可实现的边界和设计可行的LOCC协议仍然是关键挑战。

Method: 利用黎曼优化在复合Stiefel流形上构建优化固定轮次LOCC的框架。

Result: 所提出的框架不仅能获得近乎最优的目标函数值，还能生成完全可行的LOCC协议。在纠缠蒸馏和状态合并任务中，该框架获得了改进的协议，其中一些协议达到了由正部局置换松弛推导出的上限。

Conclusion: 通过流形优化来优化LOCC是推动分布式量子信息处理研究的有力工具。

Abstract: Local operations and classical communication (LOCC) is a foundational
framework in quantum information from both theoretical and experimental
perspectives. However, designing and optimizing LOCC protocols is intractable
due to their complex structure. Determining achievable bounds and designing
practically implementable LOCC protocols remain crucial challenges when the
number of communication rounds is finite. In this work, we develop a framework
to optimize fixed-round LOCC via Riemannian optimization on the product Stiefel
manifold, which not only yields near-optimal objective function values but also
produces fully implementable protocols. We demonstrate the applicability of
this framework through key tasks in quantum information processing, such as
entanglement distillation and state merging. Our results provide new insights
into the achievable bounds for entanglement distillation and block entanglement
state merging. We obtain improved distillation and state merging protocols,
some of which match the upper bounds derived via positive partial transpose
relaxations. These results demonstrate that optimizing LOCC via manifold
optimization can serve as a powerful tool to advance research on distributed
quantum information processing.

</details>


### [384] [The Knowledge Complexity of Quantum Problems](https://arxiv.org/abs/2510.06923)
*Giulio Malavolta*

Main category: quant-ph

TL;DR: 量子计算中的可证明性问题，证明了量子问题也存在零知识证明。


<details>
  <summary>Details</summary>
Motivation: 探索在量子计算背景下，将计算和证明的零知识性扩展到量子状态问题。

Method: 提出了一种零知识证明协议，证明了所有可交互证明的量子问题都可以被证明为零知识。

Result: 实现了无条件的可靠性和计算上的零知识性，并为Uhlmann变换问题提供了一个有意义的零知识证明。

Conclusion: 量子问题也可以在零知识下被证明，并且相关技术可以扩展到更复杂的问题，例如在存在恶意验证者的情况下进行Uhlmann变换。

Abstract: Foundational results in theoretical computer science have established that
everything provable, is provable in zero knowledge. However, this assertion
fundamentally assumes a classical interpretation of computation and many
interesting physical statements that one can hope to prove are not
characterized. In this work, we consider decision problems, where the problem
instance itself is specified by a (pure) quantum state. We discuss several
motivating examples for this notion and, as our main technical result, we show
that every quantum problem that is provable with an interactive protocol, is
also provable in zero-knowledge. Our protocol achieves unconditional soundness
and computational zero-knowledge, under standard assumptions in cryptography.
In addition, we show how our techniques yield a protocol for the Uhlmann
transformation problem that achieves a meaningful notion of zero-knowledge,
also in the presence of a malicious verifier.

</details>


### [385] [Local energy assignment for two interacting quantum thermal reservoirs](https://arxiv.org/abs/2510.06929)
*Alessandra Colla,Bassano Vacchini,Andrea Smirne*

Main category: quant-ph

TL;DR: 量子热力学中，在强耦合条件下，定义内能、热量和功仍是一个核心问题。本文分析了两种常用的第一定律量定义，并与基于局部对称的“最小耗散”方法进行了比较，发现在弱耦合甚至强耦合下，这些定义存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 理解量子系统（尤其是强耦合时）的内能、热量和功的分配是量子热力学中的一个核心问题，因为不同的定义方法会带来不同的结果。

Method: 分析了两种常用于描述量子系统与热环境耦合的热力学第一定律量定义，并将它们与基于局部对称的“最小耗散”方法进行了比较。研究对象是两个相互作用的大（但有限）的谐振子热库。

Result: 所有三种定义方法在弱耦合时就已存在显著差异，并且“最小耗散”方法在耦合强度增加时会出现独特的功峰值。

Conclusion: 现有的内能、热量和功的定义方法在强耦合量子系统中存在显著差异，即使在弱耦合情况下也是如此。基于局部对称的“最小耗散”方法提供了一种不同的视角，并表现出独特的特性。

Abstract: Understanding how to assign internal energy, heat, and work in quantum
systems beyond weak coupling remains a central problem in quantum
thermodynamics, particularly as the difference between competing definitions
becomes increasingly relevant. We identify two common sets of definitions for
first-law quantities that are used to describe the thermodynamics of quantum
systems coupled to thermal environments. Both are conceptually non-symmetric,
treating one part of the bipartition (the "system") differently from the other
(the "bath"). We analyze these in a setting where such roles are not easily
assigned - two large (but finite) sets of thermal harmonic oscillators
interacting with each other. We further compare them with a third set of
definitions based on a local, conceptually symmetric open-system approach
("minimal dissipation") and discuss their quantitative and structural
differences. In particular, we observe that all three sets of definitions
differ substantially even when the two subsystems are weakly coupled and far
detuned, and that the minimal dissipation approach features distinct work peaks
that increase with the coupling strength.

</details>


### [386] [Expressive and Scalable Quantum Fusion for Multimodal Learning](https://arxiv.org/abs/2510.06938)
*Tuyen Nguyen,Trong Nghia Hoang,Phi Le Nguyen,Hai L. Vu,Truong Cong Thang*

Main category: quant-ph

TL;DR: 本文介绍了一种名为量子融合层（QFL）的量子融合机制，用于多模态学习。该机制使用参数化量子电路学习跨模态的纠缠特征交互，避免了参数数量的指数增长，并在线性参数尺度下高效表示高阶多项式交互。在模拟中，QFL在小型多模态任务上优于经典基线，尤其在高模态情况下表现突出，表明其作为一种可扩展的多模态融合方法具有潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在介绍一种用于多模态学习的量子融合机制，并证明其理论和实证潜力。

Method: 提出了一种名为量子融合层（QFL）的混合量子-经典方法，使用参数化量子电路学习跨模态的纠缠特征交互，并利用量子信号处理原理高效表示高阶多项式交互，实现了线性参数扩展。

Result: 在模拟中，QFL在小型多模态任务上持续优于强大的经典基线，尤其在高模态场景下表现出显著的改进。

Conclusion: QFL提供了一种新颖且可扩展的多模态融合方法，在更大规模系统上具有进一步探索的价值。

Abstract: The aim of this paper is to introduce a quantum fusion mechanism for
multimodal learning and to establish its theoretical and empirical potential.
The proposed method, called the Quantum Fusion Layer (QFL), replaces classical
fusion schemes with a hybrid quantum-classical procedure that uses
parameterized quantum circuits to learn entangled feature interactions without
requiring exponential parameter growth. Supported by quantum signal processing
principles, the quantum component efficiently represents high-order polynomial
interactions across modalities with linear parameter scaling, and we provide a
separation example between QFL and low-rank tensor-based methods that
highlights potential quantum query advantages. In simulation, QFL consistently
outperforms strong classical baselines on small but diverse multimodal tasks,
with particularly marked improvements in high-modality regimes. These results
suggest that QFL offers a fundamentally new and scalable approach to multimodal
fusion that merits deeper exploration on larger systems.

</details>


### [387] [Physics-Informed Optimisation of Conveyor Mode Spin Qubit Transport](https://arxiv.org/abs/2510.06943)
*Andrii Sokolov,Conor Power,Elena Blokhina*

Main category: quant-ph

TL;DR: 提出一种用于优化静电偏压序列的算法，以实现硅基量子比特器件中的电子传输，并在三种技术（FD-SOI、SiMOS、Si/SiGe）上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 可扩展的量子信息处理需要能够可靠地移动量子态，同时最小化退相干。

Method: 结合自洽泊松和薛定谔求解器，以保持恒定的基态能量和近乎恒定的速度移动，适用于单电子和空穴传输。

Result: 算法在三种代表性技术（FD-SOI、SiMOS、Si/SiGe）上得到验证，并强调了影响传输保真度的关键限制和特定于材料的效应。发现了栅极几何、介电界面和量子点尺寸对移动操作稳定性的影响。

Conclusion: 该研究为改善大型量子系统中的相干性保护提供了途径。

Abstract: Scalable quantum information processing in spin-based architectures
necessitates the a bility to reliably shuttle quantum states across extended
device regions with minimal decoherence. In this work, we present a
physics-informed algorithm for optimizing electrostatic bias equences that
enable conveyor-mode electron transport in silicon-based quantum dot devices.
Our approach combines self-consistent Poisson and Schrodinger solvers to
maintain a constant ground state energy and enable near-constant velocity
shuttling, with potential applicability to both single-electron and hole
transport. We validate the algorithm across three representative technologies:
Fully-Depleted Silicon on Insulator (FD-SOI), Silicon
Metal-Oxide-Seminconductor (SiMOS) and Silicon-Germanium Heterostracture
(Si/SiGe), highlighting key limitations and material-specific effects that
influence transport fidelity. Our findings underscore the impact of gate
geometry, dielectric interfaces, and quantum dot size on the stability of
shuttling operations, and offer pathways toward improving coherence
preservation in large-scale quantum systems.

</details>


### [388] [Unitary Quantum Cellular Automata for Density Classification](https://arxiv.org/abs/2510.06947)
*Pedro C. S. Costa,Yuval R. Sanders,Pedro Paulo Balbi,Gavin K. Brennen*

Main category: quant-ph

TL;DR: 量子自动机可以解决密度分类任务，但有局限性。


<details>
  <summary>Details</summary>
Motivation: 研究量子自动机是否能解决经典确定性自动机无法完美解决的密度分类任务。

Method: 使用分区酉量子自动机（PUQCA）模型，并通过进化搜索来寻找解决方案，解决方案的成功条件是基于测量概率而非固定点收敛。分析了PUQCA中可经典模拟的规则，并在固定系统大小下分析了它们的性能。

Result: 找到了能够解决密度分类任务的PUQCA规则，并且在固定系统大小下分析了它们的性能。

Conclusion: 量子模型（PUQCA）在解决密度分类任务方面比经典模型更有潜力，但仍存在局限性，尤其是在可经典模拟的特定情况下。

Abstract: We investigate the density classification task (DCT) -- determining the
majority bit in a one-dimensional binary lattice -- within a quantum cellular
automaton (CA) framework. While there is no one-dimensional two-state, radius
$r \geq 1$, deterministic CA with periodic boundary conditions that solves the
DCT perfectly, we explore whether a unitary quantum model can succeed. We
employ the Partitioned Unitary Quantum Cellular Automaton (PUQCA), a
number-conserving model, and, via evolutionary search, find solutions to the
DCT where the success condition is stipulated in terms of measurement
probabilities rather than convergence to fixed-point configurations. Finally,
we identify a classically simulable regime of the PUQCA in which we find rules
that solve the DCT at fixed system sizes and analyze their performance.

</details>


### [389] [Computational complexity of the homology problem with orientable filtration: MA-completeness](https://arxiv.org/abs/2510.07014)
*Ryu Hayakawa,Casper Gyurik,Mahtab Yaghubi Rad,Vedran Dunjko*

Main category: quant-ph

TL;DR: 本文证明了特定类型单纯复形的MA-complete同调问题的存在性，该问题通过一种新颖的“均匀可定向过滤”概念定义，并与同调中的符号问题无关。研究人员设计了与过滤相关的新的高阶随机游走算法，将该问题置于MA类中。为了证明MA-hardness，他们设计了一个新工具，可以将一个MA-hard的随机可满足性问题规约到该问题。因此，该研究首次提出了一个结合了拓扑学、持久同调和量子计算概念的，自然出现的MA-complete问题，该问题基于高阶随机游走和单纯复形。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决在拓扑学、持久同调和量子计算交叉领域中，寻找自然出现的MA-complete同调问题这一挑战。

Method: 研究人员引入了一个新的概念——“均匀可定向过滤”，用于定义单纯复形的特定子集上的同调问题。他们设计了与该过滤相关的高阶随机游走算法，以证明该问题属于MA类。同时，他们设计了一个新的规约工具，将一个已知的MA-hard随机可满足性问题规约到该同调问题，从而证明了MA-hardness。

Result: 本文证明了一个关于特定单纯复形子集的同调问题的存在性，该问题被归类为MA-complete。具体来说，它是一个MA-complete问题，并且通过新的高阶随机游走和规约技术，将拓扑学、持久同调和量子计算的概念联系起来。

Conclusion: 本文成功地证明了一个自然出现的MA-complete同调问题的存在性，该问题与高阶随机游走和单纯复形相关，并结合了拓扑学、持久同调和量子计算的理论，为相关领域的研究提供了新的方向。

Abstract: We show the existence of an MA-complete homology problem for a certain
subclass of simplicial complexes. The problem is defined through a new concept
of orientability of simplicial complexes that we call a "uniform orientable
filtration", which is related to sign-problem freeness in homology. The
containment in MA is achieved through the design of new, higher-order random
walks on simplicial complexes associated with the filtration. For the
MA-hardness, we design a new gadget with which we can reduce from an MA-hard
stoquastic satisfiability problem. Therefore, our result provides the first
natural MA-complete problem for higher-order random walks on simplicial
complexes, combining the concepts of topology, persistent homology, and quantum
computing.

</details>


### [390] [Modular interface for efficient optical readout of diamond quantum memory at cryogenic temperatures via single-mode optical fibers](https://arxiv.org/abs/2510.07032)
*Akira Kamimaki,Yuhei Sekiguchi,Daisuke Ito,Taichi Fujiwara,Toshiharu Makino,Hiromitsu Kato,Hideo Kosaka*

Main category: quant-ph

TL;DR: 一种新颖的金刚石色心模块化接口，可实现高效的光子收集和标准化量子基础设施建设。


<details>
  <summary>Details</summary>
Motivation: 现有量子设备在纠缠式量子中继和自旋光子转换方面发展迅速，但在量子存储器光接口方面标准化平台的研究较少。

Method: 提出了一种金刚石色心模块化接口，该接口在结构上与设备和温度变化隔离，并实现了高效的光子收集，包括零声子线光谱学，在室温和低温下均可。

Result: 尽管共聚焦体积减小了100多倍，但实现了高效的光子收集，包括零声子线光谱学，在室温和低温下均可。

Conclusion: 该研究建立了一个标准化的、最小化的NV色心平台，为构建可扩展的量子基础设施铺平了道路。

Abstract: Efficient quantum devices across various physical systems have been rapidly
developed for entanglement-based quantum repeaters and spin-photon conversion;
however, far less attention has been paid to standardizing platforms through
quantum memory optical interfaces. We present a modular interface for color
centers in diamond that is structurally isolated from device-and
temperature-related variation. Despite a more than 100-fold reduction in
confocal volume, we achieve highly efficient photon collection through
single-mode optical fibers, including zero-phonon line spectroscopy, at both
room and cryogenic temperatures. These results establish a standardized minimal
NV-center-based platform and pave the way for construction of scalable quantum
infrastructure.

</details>


### [391] [Software Framework for Optically Accessible Quantum Memory Using Group-IV Color Centers in Diamond](https://arxiv.org/abs/2510.07045)
*Yannick Strocka,Mohamed Belhassen,Tim Schröder,Gregor Pieplow*

Main category: quant-ph

TL;DR: 基于锡空位色心和高效率腔的量子存储器，通过光学脉冲实现高保真度量子门操作，并集成到量子存储器软件框架中。


<details>
  <summary>Details</summary>
Motivation: 量子技术领域需要精确描述量子组件，光学可访问量子存储器是量子中继器和双因素认证的关键组成部分。

Method: 利用腔的依赖状态反射和一系列光学π/8脉冲实现高保真度单量子比特门操作，并进行微波控制分析。将该器件模型集成到标准化的量子存储器架构软件框架中。

Result: 实现了基于锡空位色心和腔的量子存储器，并通过光/微波控制实现了高保真度的量子门操作。

Conclusion: 将该量子存储器器件模型集成到软件框架中，为量子存储器架构的发展提供了基础。

Abstract: In the rapidly evolving field of quantum technology, the precise and detailed
description of quantum components is not just a necessity but the foundation
for advancing research, development, and applications. Optically accessible
quantum memories are key building blocks for devices such as quantum repeaters
and two-factor authentication. The memory we describe here is based on a
tin-vacancy color center coupled to a highly efficient cavity. It leverages
state-dependent reflection from the cavity and implements high-fidelity
fractional single qubit gates via a train of optical $\pi/8$ pulses. We also
describe its operation under microwave control, further extending our analysis.
Our primary contribution in this work is the integration of this device model
into a standardized software framework for quantum memory architectures.

</details>


### [392] [High-Performance Imaging in a Dilution Refrigerator](https://arxiv.org/abs/2510.07054)
*Timo Eikelmann,Mara Brinkmann,Leonie Eggers,Tuncay Ulas,Donika Imeri,Konstantin Beck,Lasse Jens Irrgang,Sunil Kumar Mahato,Rikhav Shah,Ralf Riedinger*

Main category: quant-ph

TL;DR: 本文介绍了一种在稀释制冷机中集成的、无需移动部件的、具有高分辨率的共聚焦成像系统，用于在亚开尔文温度下对纳米光子结构进行成像，以促进量子技术和量子网络的发展。


<details>
  <summary>Details</summary>
Motivation: 在亚开尔文温度下对纳米光子结构进行测试和开发需要进行原位光纤耦合，但缺乏合适的低温显微镜使得这一过程面临挑战。

Method: 开发了一个集成在稀释制冷机中的、无需移动部件的、具有高分辨率（1.1微米）和宽视场（2.5毫米）的共聚焦成像系统，并具有长工作距离，允许光学和微波探针接入。

Result: 该系统可以在透明金刚石基底上对纳米光子结构进行高分辨率可视化，并且在低温下无需移动部件，具有长工作距离，便于进行多种实验操作。

Conclusion: 该系统能够促进可扩展的、集成化的量子光学技术的发展，为研究大规模量子网络提供支持。

Abstract: Nanophotonic light-matter interfaces hold great promise for quantum
technologies. Enhancing local electromagnetic fields, they enable highly
efficient detectors, can help realize optically connected processors, or serve
as quantum repeaters. In-situ fiber-coupling at sub-Kelvin temperatures, as
required for test and development of new devices, proves challenging as
suitable cryogenic microscopes are not readily available. Here, we report on a
robust and versatile confocal imaging system integrated in a dilution
refrigerator, enabling high-resolution visualization of nanophotonic structures
on a transparent diamond substrate. Our imaging system achieves a resolution of
1.1 {\mu}m and a field-of-view of 2.5 mm. The system requires no movable parts
at cryogenic temperatures and features a large working distance, thereby
allowing optical and microwave probe access, as well as direct anchoring of
temperature sensitive samples to a cold finger, needed for applications with
high thermal load. This system will facilitate the development of scalable,
integrated quantum optics technology, as required for research on large scale
quantum networks.

</details>


### [393] [Potential of multi-anomalies detection using quantum machine learning](https://arxiv.org/abs/2510.07055)
*Takao Tomono,Kazuya Tsujimura*

Main category: quant-ph

TL;DR: 与使用经典高斯核相比，量子核在检测生产设备异常声音方面表现出明显的优势，提高了准确性和 F1 分数。


<details>
  <summary>Details</summary>
Motivation: 虽然机器学习模型常用于检测生产设备的异常，但随着设备数量的增加，计算成本会迅速增加。此前的研究使用 AR 模型系数结合一类 SVM 来检测异常，但本次工作旨在探索使用量子核替代经典核以提高效率。

Method: 本研究将一类 SVM 的经典核替换为量子核，并在两个实验设置中进行了测试：一个是在带有异常声音的迷你赛车跑道上，另一个是在带有模拟突然异常声音的开式皮带传动装置上。

Result: 在迷你赛车跑道数据集上，量子核实现了 0.82 的准确率和 F1 分数，而 RBF 核分别为 0.64 和 0.39。在粉碎装置数据集上，量子核实现了完美的准确率和 F1 分数（1.00），而 RBF 核的准确率仅为 0.64，F1 分数为 0.43。

Conclusion: 量子核在分类各种类型的异常声音模式（包括周期性和冲击性异常）方面，比经典的 RBF 核具有更高的分类准确性。

Abstract: Maintenance of production equipment is critical in manufacturing. Typically,
machine learning models are trained on sensor data closely attached to
equipment. However, as the number of machines increases, computational cost
grows rapidly. In practice, anomalies are often identified by human operators
through auditory perception, relying heavily on experience and intuition. In
vibration analysis, especially, AR model coefficients combined with one-class
SVMs are used for detecting anomalies. In this work, we explore the effect of
substituting the classical kernel in the one-class SVM with a quantum kernel.
Two experimental setups were used. The first involved a miniature racing car
track, where the car passes over a patch of hook-and-loop fastener to generate
abnormal sounds, which are recorded using a microphone. The second involved an
open-belt drive, where chopsticks are inserted at specific times to produce
crushing sounds, simulating sudden anomalies. Our results show a clear
advantage of quantum kernels over classical Gaussian (RBF) kernels. On the
miniature car track dataset, the quantum kernel achieved an accuracy and
F1-score of 0.82, compared to 0.64 and 0.39 respectively for the RBF kernel.
For the crushing device, the quantum kernel achieved perfect accuracy and
F1-score (1.00), while the RBF kernel reached only 0.64 accuracy and 0.43
F1-score. These findings suggest that quantum kernels enhance the
classification accuracy for diverse types of abnormal sound patterns, including
both periodic and impulsive anomalies.

</details>


### [394] [Phonon-induced two-axis spin squeezing with decoherence reduction in hybrid spin-optomechanical system](https://arxiv.org/abs/2510.07068)
*Feng Qiao,Zu-Jian Ying*

Main category: quant-ph

TL;DR: 该研究提出了一种在混合腔光力-自旋系统中实现海森堡极限自旋压缩的方案。


<details>
  <summary>Details</summary>
Motivation: 为了提高精密测量精度，本研究旨在提出一种能够实现海森堡极限自旋压缩的方案，并探讨其在多体系统中的应用潜力。

Method: 提出一种混合腔光力-自旋系统，利用Tavis-Cummings相互作用将N个二能级系统耦合到机械谐振器中。在有效哈密顿量中，通过对光学模式进行绝热消除，实现了对机械谐振器的压缩，并将集体自旋算符转化为类Bogo­liubov形式。在强失谐条件下，通过调节参数实现两轴扭曲压缩协议。

Result: 理论分析和数值模拟表明，即使存在退相干和声子耗散，随着N的增加，最大压缩度也会趋于一个常数，表明在无参数优化的情况下，测量精度可渐近达到标准量子极限。然而，通过参数优化，本方案获得的最佳压缩度优于现有方案，并且最优压缩的制备时间也显著缩短。

Conclusion: 本研究提出的混合腔光力-自旋系统能够实现海森堡极限的自旋压缩，通过参数优化可以进一步提高精度并缩短制备时间，为多体系统的高精度量子计量提供了新的途径，并有助于深入理解耗散效应对自旋压缩的影响。

Abstract: We propose a scheme to implement Heisenberg-limited spin squeezing in a
hybrid cavity optomechanical-spin system. In our system, $N$ two-level systems
are coupled via Tavis-Cummings interactions to a mechanical resonator (MR) in a
standard optomechanical setup. Within the dispersive coupling regime, adiabatic
elimination of the optical mode induces a squeezing effect on the MR, which, in
the squeezed representation, effectively transforms the collective spin
operators into a Bogoliubov form. Under large detuning conditions, the phonon
mode mediates interactions among the Bogoliubov collective spins, thereby
enabling a two-axis twisting squeezing protocol through appropriate parameter
tuning. Both theoretical analysis and numerical simulations show that in the
presence of dephasing and phonon dissipation, the maximum squeezing degree
asymptotically converges to a constant as $N$ increases, which implies the
metrological precision asymptotically approaches the standard quantum limit
without parameter optimization. Nevertheless, in parameter optimization we
extract a scaling relation of the optimal squeezing which surpasses existing
schemes in the literature. Moreover, the optimization also leads to a
considerable reduction of the preparation time for the optimal squeezing. Our
work may provide insights into dissipation effects in spin squeezing and offer
a potential route for high-precision quantum metrology in many-body systems.

</details>


### [395] [Sequential quantum processes with group symmetries](https://arxiv.org/abs/2510.07100)
*Dmitry Grinko,Satoshi Yoshida,Mio Murao,Maris Ozols*

Main category: quant-ph

TL;DR: 本研究提出了一个基于群对称性的量子梳设计方法，能够将未知酉操作转换为其逆操作或转置操作。


<details>
  <summary>Details</summary>
Motivation: 量子协议的设计与分析中，对称性扮演着至关重要的角色。

Method: 利用Clebsch-Gordan变换，对$(G 	imes H)$-不变量子梳进行了规范的电路分解，并将其推广到G-协变量子梳。基于此分解，设计了一个具有群对称性的参数化量子梳。

Result: 提出了最优量子梳，可以将未知酉操作$U \in \mathrm{SU}(d)$转换为其逆操作$U^\dagger$或转置操作$U^\top$。数值结果表明，对于$d=3$，存在一个确定且精确的酉转置协议，只需要7次查询U，优于先前需要13次查询的协议。

Conclusion: 本研究提出的基于群对称性的量子梳设计方法，为量子协议的优化提供了新的途径，特别是在酉操作的逆和转置方面。

Abstract: Symmetry plays a crucial role in the design and analysis of quantum
protocols. This result shows a canonical circuit decomposition of a $(G \times
H)$-invariant quantum comb for compact groups $G$ and $H$ using the
corresponding Clebsch-Gordan transforms, which naturally extends to the
$G$-covariant quantum comb. By using this circuit decomposition, we propose a
parametrized quantum comb with group symmetry, and derive the optimal quantum
comb which transforms an unknown unitary operation $U \in \mathrm{SU}(d)$ to
its inverse $U^\dagger$ or transpose $U^\top$. From numerics, we find a
deterministic and exact unitary transposition protocol for $d=3$ with $7$
queries to $U$. This protocol improves upon the protocol shown in the previous
work, which requires $13$ queries to $U$.

</details>


### [396] [HPQEA: A Scalable and High-Performance Quantum Emulator with High-Bandwidth Memory for Diverse Algorithms Support](https://arxiv.org/abs/2510.07110)
*Tran Van Duy,Tuan Hai Vu,Vu Trung Duong Le,Hoai Luan Pham,Yasuhiko Nakashima*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In recent years, there has been a growing interest in the development of
quantum emulation. However, existing studies often struggle to achieve broad
applicability, high performance, and efficient resource and memory utilization.
To address these challenges, we provide HPQEA, a quantum emulator based on the
state-vector emulation approach. HPQEA includes three main features: a
high-performance computing core, an optimized controlled-NOT gate computation
strategy, and effective utilization of high-bandwidth memory. Verification and
evaluation on the Alveo U280 board show that HPQEA can emulate quantum circuits
with up to 30 qubits while maintaining high fidelity and low mean square error.
It outperforms comparable FPGA-based systems by producing faster execution,
supporting a wider range of algorithms, and requiring low hardware resources.
Furthermore, it exceeds the Nvidia A100 in normalized gate speed for systems
with up to 20 qubits. These results demonstrate the scalability and efficiency
of HPQEA as a platform for emulating quantum algorithms.

</details>


### [397] [Communication-Optimal Blind Quantum Protocols](https://arxiv.org/abs/2510.07112)
*Ethan Davies,Alastair Kay*

Main category: quant-ph

TL;DR: Alice 想要让 Bob 为她执行量子计算，但又不想让他知道她在做什么。本文研究了这个协议中的一个步骤：Alice 告知 Bob 是否应该执行某个门操作。通过熵边界技术，我们量化了 Alice 必须发送的最少量子比特数，以确保 Bob 无法了解正在执行的门操作。我们提出了一个能达到此边界的最优协议，其中 Alice 发送的状态可以是纠缠态。对于 Clifford 门，我们证明了 Alice 发送可分态就足够了。


<details>
  <summary>Details</summary>
Motivation: 研究在量子计算中，用户如何实现对服务提供者的信息隐藏，以确定最小通信资源实现信息论安全。

Method: 使用熵边界技术量化 Alice 必须发送的最少量子比特数，以确保 Bob 无法了解正在执行的门操作。

Result: 提出一个最优协议，其中 Alice 发送的状态可以是纠缠态，并证明对于 Clifford 门，发送可分态就足够了。

Conclusion: 确定了在用户对服务提供者隐藏量子计算信息时，所需的最小通信资源，并提出了相应的最优协议。

Abstract: A user, Alice, wants to get server Bob to implement a quantum computation for
her. However, she wants to leave him blind to what she's doing. What are the
minimal communication resources Alice must use in order to achieve
information-theoretic security? In this paper, we consider a single step of the
protocol, where Alice conveys to Bob whether or not he should implement a
specific gate. We use an entropy-bounding technique to quantify the minimum
number of qubits that Alice must send so that Bob cannot learn anything about
the gate being implemented. We provide a protocol that saturates this bound. In
this optimal protocol, the states that Alice sends may be entangled. For
Clifford gates, we prove that it is sufficient for Alice to send separable
states.

</details>


### [398] [Fundamental Quality Bound on Optical Quantum Communication](https://arxiv.org/abs/2510.07121)
*Tobias Rippchen,Ludovico Lami,Gerardo Adesso,Mario Berta*

Main category: quant-ph

TL;DR: 该研究提出了一种新的方法来提高量子通信的传输质量，而不是数量，通过分析纠缠的相对熵来设定理论基准。


<details>
  <summary>Details</summary>
Motivation: 在量子技术和量子光学中，可靠地远距离发送量子信息是一个核心挑战，因为大多数量子通信依赖于光纤或自由空间链路。

Method: 研究人员证明了对于所有量子光学通信中出现的“可传送模拟通道”，纠缠的单字母反向相对熵可以作为双向辅助量子通信误差指数的上限。对于高斯通道，可以通过一个简单的凸程序高效地计算该界限。

Result: 研究推导出了几种基本的光学通信单模高斯通道的闭式解析表达式，并为纠缠的相对熵在纠缠测试中提供了明确的操作解释，证明了它在非纠缠操作下的纠缠蒸馏速率。

Conclusion: 该研究为纠缠作为一种资源提供了新的视角，并为未来的量子光学网络设定了更严格的理论基准。

Abstract: Sending quantum information reliably over long distances is a central
challenge in quantum technology in general, and in quantum optics in
particular, since most quantum communication relies on optical fibres or
free-space links. Here, we address this problem by shifting the focus from the
quantity of information sent to the quality of the transmission, i.e. the rate
of decay of the transmission error with respect to the number of channel uses.
For the general class of teleportation-simulable channels, which includes all
channels arising in quantum optical communication, we prove that the
single-letter reverse relative entropy of entanglement of the Choi state upper
bounds the error exponent of two-way assisted quantum communication -
paralleling the celebrated capacity bound of [Pirandola et al., Nat. Comm.
(2017)] in terms of the regularised relative entropy of entanglement.
Remarkably, for Gaussian channels our bound can be computed efficiently through
a convex program with simple constraints involving only finite-dimensional
covariance matrices. As a prototypical application, we derive closed-form
analytical expressions for several one-mode Gaussian channels that are
fundamental to optical communication. Extending recent work [Lami et al.,
arXiv:2408.07067 (2024)] to infinite-dimensional systems, we further endow the
reverse relative entropy of entanglement with an exact operational
interpretation in entanglement testing, and show that it characterises the rate
of entanglement distillation under non-entangling operations. These findings
offer a new perspective on entanglement as a resource and sharpen the
theoretical benchmarks for future quantum optical networks.

</details>


### [399] [Preparation of initial states with open and periodic boundary conditions on quantum devices using matrix product states](https://arxiv.org/abs/2510.07125)
*Yibin Guo,Manuel Schneider,Takis Angelides,Karl Jansen,C. -J. David Lin,Yao Ting Su*

Main category: quant-ph

TL;DR: 本文提出了一种在量子设备上从矩阵乘积态（MPS）制备量子态的框架，支持开放和周期性边界条件。


<details>
  <summary>Details</summary>
Motivation: 为了在量子设备上高效地制备量子态，特别是针对具有挑战性的强关联系统模拟。

Method: 将MPS张量映射到酉门，并分解为本地门。对于周期性边界条件，利用辅助量子比特和后选择实现。推导了成功率的精确表达式。

Result: 成功制备了海森堡模型（周期性边界条件）的基态并模拟了哈密顿量淬灭下的动力学，以及高保真度地构建了 स्विंगर 模型（周期性边界条件）的激发态量子电路。

Conclusion: 该方法为在量子设备上制备量子态提供了一种可扩展的途径，有望在近期量子计算机上实现强关联系统的有效模拟。

Abstract: We present a framework for preparing quantum states from matrix product
states (MPS) with open and periodic boundary conditions on quantum devices. The
MPS tensors are mapped to unitary gates, which are subsequently decomposed into
native gates on quantum hardware. States with periodic boundary conditions
(pbc) can be represented efficiently as quantum circuits using ancilla qubits
and post-selection after measurement. We derive an exact expression for the
success rate of this probabilistic approach, which can be evaluated a priori.
The applicability of the method is demonstrated in two examples. First, we
prepare the ground state of the Heisenberg model with pbc and simulate dynamics
under a quenched Hamiltonian. The volume-law entanglement growth in the time
evolution challenges classical algorithms but can potentially be overcome on
quantum hardware. Second, we construct quantum circuits that generate excited
states of the Schwinger model with high fidelities. Our approach provides a
scalable method for preparing states on a quantum device, enabling efficient
simulations of strongly correlated systems on near-term quantum computers.

</details>


### [400] [Experimental demonstration of genuine quantum information transmission through completely depolarizing channels in a superposition of cyclic orders](https://arxiv.org/abs/2510.07127)
*Yaxin Wang,Linxiang Zhou,Tianfeng Feng,Hanlin Nie,Ying Xia,Tianqi Xiao,Juntao Li,Vlatko Vedral,Xiaoqi Zhou*

Main category: quant-ph

TL;DR: 通过利用不确定的因果顺序（具体来说是循环顺序的叠加），我们首次在实验中实现了通过多个串联的全退化信道进行量子信息传输，并显著提高了输出态的保真度，超过了经典阈值。


<details>
  <summary>Details</summary>
Motivation: 量子通信面临噪声（特别是全退化信道）对信道容量的负面影响的挑战，这使得信息传输在理论上变得不可能。本研究旨在探索不确定的因果顺序概念是否能为解决此问题提供一种方法。

Method: 在可编程硅光子量子芯片上实现基于不确定因果顺序（循环顺序的叠加）的配置，并进行实验演示。

Result: 当四个全退化信道通过循环顺序的叠加组合时，输出态的保真度为 0.712 ± 0.013，显著高于经典的 2/3 阈值。

Conclusion: 不确定的因果顺序是一种克服量子通信中噪声限制的强大工具，在噪声环境中具有潜力，并为构建稳健的量子网络开辟了新的可能性。

Abstract: A major challenge in quantum communication is addressing the negative effects
of noise on channel capacity, especially for completely depolarizing channels,
where information transmission is inherently impossible. The concept of
indefinite causal order provides a promising solution by allowing control over
the sequence in which channels are applied. We experimentally demonstrate the
activation of quantum communication through completely depolarizing channels
using a programmable silicon photonic quantum chip. By implementing
configurations based on the superposition of cyclic orders, a form of
indefinite causal order, we report the first experimental realization of
genuine quantum information transmission across multiple concatenated
completely depolarizing channels. Our results show that when four completely
depolarizing channels are combined using the superposition of cyclic orders,
the fidelity of the output state is $0.712 \pm 0.013$, significantly exceeding
the classical threshold of 2/3. Our work establishes indefinite causal order as
a powerful tool for overcoming noise-induced limitations in quantum
communication, demonstrating its potential in high-noise environments and
opening new possibilities for building robust quantum networks.

</details>


### [401] [Haar random codes attain the quantum Hamming bound, approximately](https://arxiv.org/abs/2510.07158)
*Fermi Ma,Xinyu Tan,John Wright*

Main category: quant-ph

TL;DR: Haar 随机码可以纠正高达量子汉明界的错误，只要 mK ≪ N。


<details>
  <summary>Details</summary>
Motivation: 研究 Haar 随机码的纠错特性。

Method: 利用 Bandeira、Boedihardjo 和 van Handel 的最新矩阵浓度结果。

Result: Haar 随机码可以纠正高达量子汉明界的错误，只要 mK ≪ N。

Conclusion: Haar 随机码提供了迄今为止已知的最强量子纠错码界限，并表明近似量子纠错码可以显著优于精确量子纠错码。

Abstract: We study the error correcting properties of Haar random codes, in which a
$K$-dimensional code space $\boldsymbol{C} \subseteq \mathbb{C}^N$ is chosen at
random from the Haar distribution. Our main result is that Haar random codes
can approximately correct errors up to the quantum Hamming bound, meaning that
a set of $m$ Pauli errors can be approximately corrected so long as $mK \ll N$.
This is the strongest bound known for any family of quantum error correcting
codes (QECs), and continues a line of work showing that approximate QECs can
significantly outperform exact QECs [LNCY97, CGS05, BGG24]. Our proof relies on
a recent matrix concentration result of Bandeira, Boedihardjo, and van Handel.

</details>


### [402] [MIPco=coRE](https://arxiv.org/abs/2510.07162)
*Junqiao Lin*

Main category: quant-ph

TL;DR: MIPco（交互式证明系统，使用交换算子模型）等于 coRE，而 MIP*（使用张量积模型）等于 RE，这表明不同的纠缠模型会显著影响计算能力。新提出的“弱可压缩性”条件统一了这两种情况的不可计算性证明，并可能适用于其他问题。


<details>
  <summary>Details</summary>
Motivation: 探索 MIPco（使用交换算子模型）的计算能力，并将其与 MIP*（使用张量积模型）进行比较，以理解不同纠缠模型的影响。

Method: 将 MIPco 等于 coRE 的证明建立在 MIP*=RE 证明的压缩定理基础上，并利用可追踪嵌入策略框架。此外，通过结合同步框架和改进的 Pauli 基测试，为非局部博弈的压缩定理提供了更简洁的证明。提出并应用了“弱可压缩性”这一新的 RE/coRE 完全问题的等价条件。

Result: 证明了 MIPco = coRE，表明交互式证明系统中不同纠缠模型的计算能力差异。证明了 MIP* 和 MIPco 都满足“弱可压缩性”条件，从而在一个统一的框架内证明了它们的不可计算性。为 MIP*=RE 提供了不依赖于纠缠边界保持的替代证明。

Conclusion: MIPco = coRE，与 MIP* = RE 形成对比，突显了纠缠模型对交互式证明系统计算能力的影响。提出的“弱可压缩性”条件提供了一个统一的框架来理解 MIP* 和 MIPco 的不可计算性，并可能扩展到其他决策问题。

Abstract: In 2020, a landmark result by Ji, Natarajan, Vidick, Wright, and Yuen showed
that MIP*, the class of languages that can be decided by a classical verifier
interacting with multiple computationally unbounded provers sharing
entanglement in the tensor product model, is equal to RE. We show that the
class MIPco, a complexity class defined similarly to MIP* except with provers
sharing the commuting operator model of entanglement, is equal to the class
coRE. This shows that giving the provers two different models of entanglement
leads to two completely different computational powers for interactive proof
systems. Our proof builds upon the compression theorem used in the proof of
MIP*=RE, and we use the tracially embeddable strategies framework to show that
the same compression procedure in MIP* =RE also has the same desired property
in the commuting operator setting. We also give a more streamlined proof of the
compression theorem for non-local games by incorporating the synchronous
framework used by Mousavi et al. [STOC 2022], as well as the improved Pauli
basis test introduced by de la Salle [ArXiv:2204.07084].
  We introduce a new equivalence condition for RE/coRE-complete problems, which
we call the weakly compressible condition. We show that both MIP* and MIPco
satisfy this condition through the compression theorem, and thereby establish
that the uncomputability for MIP* and MIPco can be proved under a unified
framework (despite these two complexity classes being different). Notably, this
approach also gives an alternative proof of the MIP*=RE theorem, which does not
rely on the preservation of the entanglement bound. In addition to non-local
games, this new condition could also potentially be applicable to other
decision problems.

</details>


### [403] [Diffusion Codes: Self-Correction from Small(er)-Set Expansion with Tunable Non-locality](https://arxiv.org/abs/2510.07179)
*Adithya Sriram,Vedika Khemani,Benedikt Placke*

Main category: quant-ph

TL;DR: 通过在biregular图上随机放置边，我们引入了“扩散码”。通过调整SWAP网络的深度，可以在代码参数的随机性和局部性之间进行权衡。对于循环图上的扩散码，我们证明了Tanner图几乎肯定是一个小的顶点扩展器，并且稳定器的几何大小受到T的限制。将这些经典代码的超图乘积化，可以得到量子LDPC码，具有相似的权衡。


<details>
  <summary>Details</summary>
Motivation: 在biregular图上随机选择Tanner图是获得最优LDPC码的一种方式。本文提出了一种新的编码方法，称为“扩散码”，它通过在底层图上放置边并使用随机SWAP网络来定义，旨在实现随机性和局部性之间的权衡。

Method: 本文提出了一种称为“扩散码”的编码方法。该方法在底层图上放置边，并使用随机SWAP网络来处理这些边。通过调整SWAP网络的深度，可以在代码参数的随机性（即代码的优度）和局部性（相对于底层图）之间进行权衡。对于定义在循环图上的扩散码，当SWAP网络的深度约为Tn，且T>n^{2β}时，可以证明Tanner图几乎肯定是一个小的顶点扩展器，并且稳定器的几何大小受到T的限制。将这种经典扩散码通过超图乘积化，可以得到量子LDPC码。

Result: 对于定义在循环图上的扩散码，当SWAP网络的深度约为Tn，且T>n^{2β}时，Tanner图几乎肯定是一个小的顶点扩展器，具有有界的比特和校验度。同时，最大稳定器的几何大小受到{T}的限制。通过超图乘积，可以得到量子LDPC码，该码具有小的边界和共边界扩展，以及与经典码相同的扩展/局部性权衡。这些量子码是自纠正的，支持单次解码，并且稳定器的几何大小以任意小的幂律增长。

Conclusion: 本文提出的扩散码提供了一种在代码参数的随机性和局部性之间进行权衡的新方法。通过对循环图上的扩散码进行分析，证明了其优异的扩展特性。此外，通过超图乘积，将该方法推广到量子LDPC码，得到了具有优异性能（自纠正、单次解码）和可调稳定器尺寸的量子码。该研究还提出了一个可能具有独立意义的证明技术，即在小子系统上的随机SWAP网络的混合时间仅与子系统大小成比例。

Abstract: Optimal constructions of classical LDPC codes can be obtained by choosing the
Tanner graph uniformly at random among biregular graphs. We introduce a class
of codes that we call ``diffusion codes'', defined by placing each edge
connecting bits and checks on some graph, and acting on that graph with a
random SWAP network. By tuning the depth of the SWAP network, we can tune a
tradeoff between the amount of randomness -- and hence the optimality of code
parameters -- and locality with respect to the underlying graph. For diffusion
codes defined on the cycle graph, if the SWAP network has depth $\sim Tn$ with
$T> n^{2\beta}$ for arbitrary $\beta>0$, then we prove that almost surely the
Tanner graph is a lossless ``smaller set'' vertex expander for small sets up
size $\delta \sim \sqrt T \sim n^{\beta}$, with bounded bit and check degree.
At the same time, the geometric size of the largest stabilizer is bounded by
$\sqrt T$ in graph distance. We argue, based on physical intuition, that this
result should hold more generally on arbitrary graphs. By taking hypergraph
products of these classical codes we obtain quantum LDPC codes defined on the
torus with smaller-set boundary and co-boundary expansion and the same
expansion/locality tradeoffs as for the classical codes. These codes are
self-correcting and admit single-shot decoding, while having the geometric size
of the stabilizer growing as an arbitrarily small power law. Our proof
technique establishes mixing of a random SWAP network on small subsystems at
times scaling with only the subsystem size, which may be of independent
interest.

</details>


### [404] [Fundamental Costs of Noise-Robust Quantum Control: Speed Limits and Complexity](https://arxiv.org/abs/2510.07183)
*Junkai Zeng,Xiu-Hao Deng*

Main category: quant-ph

TL;DR: 量子系统中的噪声是量子信息科学发展的障碍。本文推导了准静态相干噪声和有界控制幅度下的控制复杂性下界，量化了动力学纠错的时间开销。


<details>
  <summary>Details</summary>
Motivation: 量子控制需要克服噪声以实现高保真操作，但现有方法通常会引入额外的时间开销。

Method: 本文推导了准静态相干噪声和有界控制幅度下的控制复杂性下界，并提出了实现鲁棒性量子门的操作方案。对于噪声空间，提出了相干维度界和投影维度界，并推导了基于图论的鲁棒性时间界。

Result: 对于单个噪声源，证明了鲁棒性的时间下界，并给出了在4T时间加上常数时间实现任意目标门的操作方案。对于噪声空间，给出了混合酉调度的段数M的维度下界。对于图定义上的噪声空间，推导了与图色数成线性关系的鲁棒性时间界。

Conclusion: 本文的研究结果为一阶抗噪声操作的可行性建立了量化限制。

Abstract: Noise is ubiquitous in quantum systems and is a major obstacle for the
advancement of quantum information science. Noise-robust quantum control
achieves high-fidelity operations by engineering the evolution path so that
first-order noise contributions cancel at the final time. Such dynamical error
correction typically incurs a time overhead beyond standard quantum speed
limits. We derive general lower bounds on control complexity that quantify this
overhead for quasi-static coherent noise under bounded control amplitude. For a
single noise source, we prove a universal time lower bound for first-order
robustness and give a constructive scheme that implements any target gate
robustly in time 4T plus a constant time. For robustness against an entire
noise space, we show dimension lower bounds on the number M of segments in any
mixed-unitary schedule from two mechanism: (i) a coherent dimension bound when
the error subspace contains an irreducible block isomorphic to su(q), and (ii)
a projection dimension bound when the noise space contains the trace-zero span
of orthogonal projectors. Under bounded speed, these bounds on number of
segments imply time lower bounds. With only local controls robust against noise
space defined on a graph, we obtain a graph-orthogonality time bound scales
linear with graph chromatic number. We illustrate the bounds through examples.
Collectively, these results establish quantitative limitations on the
feasibility of first-order noise-resilient operations.

</details>


### [405] [Covert Quantum Learning: Privately and Verifiably Learning from Quantum Data](https://arxiv.org/abs/2510.07193)
*Abhishek Anand,Matthias C. Caro,Ari Karchmer,Saachi Mutreja*

Main category: quant-ph

TL;DR: 本文提出了量子学习中的隐蔽可验证学习模型，并无需计算假设即可实现，解决了远程数据访问中的数据正确性和学习者隐私问题。


<details>
  <summary>Details</summary>
Motivation: 解决量子学习中远程数据访问的两个关键挑战：验证数据正确性以及保护学习者数据收集策略和结论的隐私。

Method: 提出量子学习的隐蔽可验证模型，并实现无需计算硬性假设的算法。考虑了策略隐蔽性和目标隐蔽性两种隐私概念。具体方法包括：利用经典阴影进行量子统计查询，以及从公共量子示例和私有量子统计查询中学习二次函数。

Result: 实现了策略隐蔽算法（量子统计查询 via 经典阴影）和目标隐蔽算法（学习二次函数，Pauli 阴影断层扫描，稳定状态学习，Forrelation 和 Simon 问题）。证明了 Forrelation 和 Simon 问题的指数级分离在隐蔽性约束下得以保留。设计了用于从公共量子查询中获取量子数据的隐蔽可验证协议。

Conclusion: 本文的模型和算法证明了即使在不可信的远程数据下，量子优势也是可以私密且可验证地实现的。

Abstract: Quantum learning from remotely accessed quantum compute and data must address
two key challenges: verifying the correctness of data and ensuring the privacy
of the learner's data-collection strategies and resulting conclusions. The
covert (verifiable) learning model of Canetti and Karchmer (TCC 2021) provides
a framework for endowing classical learning algorithms with such guarantees. In
this work, we propose models of covert verifiable learning in quantum learning
theory and realize them without computational hardness assumptions for remote
data access scenarios motivated by established quantum data advantages. We
consider two privacy notions: (i) strategy-covertness, where the eavesdropper
does not gain information about the learner's strategy; and (ii)
target-covertness, where the eavesdropper does not gain information about the
unknown object being learned. We show: Strategy-covert algorithms for making
quantum statistical queries via classical shadows; Target-covert algorithms for
learning quadratic functions from public quantum examples and private quantum
statistical queries, for Pauli shadow tomography and stabilizer state learning
from public multi-copy and private single-copy quantum measurements, and for
solving Forrelation and Simon's problem from public quantum queries and private
classical queries, where the adversary is a unidirectional or i.i.d.
ancilla-free eavesdropper. The lattermost results in particular establish that
the exponential separation between classical and quantum queries for
Forrelation and Simon's problem survives under covertness constraints. Along
the way, we design covert verifiable protocols for quantum data acquisition
from public quantum queries which may be of independent interest. Overall, our
models and corresponding algorithms demonstrate that quantum advantages are
privately and verifiably achievable even with untrusted, remote data.

</details>


### [406] [Accelerating Inference for Multilayer Neural Networks with Quantum Computers](https://arxiv.org/abs/2510.07195)
*Arthur G. Rattew,Po-Wei Huang,Naixu Guo,Lirandë Pira,Patrick Rebentrost*

Main category: quant-ph

TL;DR: 我们提出了第一个完全相干的量子多层神经网络，具有非线性激活函数，并分析了在三种量子数据访问模式下的推理复杂度。


<details>
  <summary>Details</summary>
Motivation: 为了将容错量子处理单元（QPU）集成到现代深度学习流程中，本文提出了第一个完全相干的量子多层神经网络。

Method: 我们构建了基于ResNet的神经网络，包含残差块、多滤波器2D卷积、sigmoid激活、跳跃连接和层归一化。我们分析了三种量子数据访问模式下的推理复杂度。

Result: 在没有附加条件的情况下，我们证明了在浅层双线性风格网络上实现了相对于经典方法的二次加速。通过有效的量子权重访问，我们实现了四次加速。通过有效的量子输入和权重访问，我们证明了一个具有N维向量化输入、k个残差块层和最终残差-线性-池化层的网络，其推理成本为O(polylog(N/	extepsilon)^k)，误差为	extepsilon。

Conclusion: 本文为实现量子神经网络的加速提供了理论基础，并指出了未来研究的方向。

Abstract: Fault-tolerant Quantum Processing Units (QPUs) promise to deliver exponential
speed-ups in select computational tasks, yet their integration into modern deep
learning pipelines remains unclear. In this work, we take a step towards
bridging this gap by presenting the first fully-coherent quantum implementation
of a multilayer neural network with non-linear activation functions. Our
constructions mirror widely used deep learning architectures based on ResNet,
and consist of residual blocks with multi-filter 2D convolutions, sigmoid
activations, skip-connections, and layer normalizations. We analyse the
complexity of inference for networks under three quantum data access regimes.
Without any assumptions, we establish a quadratic speedup over classical
methods for shallow bilinear-style networks. With efficient quantum access to
the weights, we obtain a quartic speedup over classical methods. With efficient
quantum access to both the inputs and the network weights, we prove that a
network with an $N$-dimensional vectorized input, $k$ residual block layers,
and a final residual-linear-pooling layer can be implemented with an error of
$\epsilon$ with $O(\text{polylog}(N/\epsilon)^k)$ inference cost.

</details>


### [407] [Spin quantum computing, spin quantum cognition](https://arxiv.org/abs/2510.07196)
*Betony Adams,Francesco Petruccione*

Main category: quant-ph

TL;DR: 本篇论文探讨了磷核自旋在量子计算和量子生物学中的应用，以及它们之间的相互促进作用。


<details>
  <summary>Details</summary>
Motivation: 探讨了 Bruce Kane 和 Matthew Fisher 的理论，即磷核自旋可用于量子计算和量子生物学（特别是与神经激活和认知相关的方面），并解决了这两个领域中存在的未解决问题。

Method: 通过概述量子计算模型与量子认知模型之间的相似性来解决开放性问题。

Result: 该研究强调了量子计算和量子生物学之间的潜在双向交流，量子计算可以为量子生物学提供工具，而生物模型可以启发新的量子信息处理策略。

Conclusion: 量子计算和量子生物学可以相互借鉴，共同推动各自领域的发展。

Abstract: Over two decades ago, Bruce Kane proposed that spin-half phosphorus nuclei
embedded in a spin-zero silicon substrate could serve as a viable platform for
spin-based quantum computing. These nuclear spins exhibit remarkably long
coherence times, making them ideal candidates for qubits. Despite this
advantage, practical realisation of spin quantum computing remains a challenge.
More recently, physicist Matthew Fisher proposed a hypothesis linking nuclear
spin dynamics, specifically those of phosphorus nuclei within the spin-zero
matrix of calcium phosphate molecules, to neural activation and, potentially,
cognition. The theory has generated both interest and scepticism, with some
fundamental questions remaining. We review this intersection of quantum
computing and quantum biology by outlining the similarities between these
models of quantum computing and quantum cognition. We then address some of the
open questions and the lessons that might be learned in each context. In doing
so, we highlight a promising bidirectional exchange: not only might quantum
computing offer tools for understanding quantum biology, but biological models
may also inspire novel strategies for quantum information processing.

</details>


### [408] [Efficient tensor-network simulations of weakly-measured quantum circuits](https://arxiv.org/abs/2510.07211)
*Darren Pereira,Leonardo Banchi*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a tensor-network-based method for simulating a weakly-measured
quantum circuit. In particular, we use a Markov chain to efficiently sample
measurements and contract the tensor network, propagating their effect forward
along the spatial direction. Applications of our algorithm include validating
quantum computers (capable of mid-circuit measurements) in regimes of easy
classical simulability, and studying generative-machine-learning applications,
where sampling from complex stochastic processes is the main task. As a
demonstration of our algorithm, we consider a (1+1)-dimensional brickwall
circuit of Haar-random unitaries, interspersed with generalized single-qubit
measurements of variable strength. We simulate the dynamics for tens to
hundreds of qubits if the circuit exhibits area-law entanglement (under strong
measurements), and tens of qubits if it exhibits volume-law entanglement (under
weak measurements). We observe signatures of a measurement-induced phase
transition between the two regimes as a function of measurement strength.

</details>


### [409] [Multi-qubit Toffoli with exponentially fewer T gates](https://arxiv.org/abs/2510.07223)
*David Gosset,Robin Kothari,Chenyi Zhang*

Main category: quant-ph

TL;DR: 使用指数级减少的T门实现了n量子比特Toffoli门，误差极小。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，n量子比特Toffoli门需要至少n个T门，而本研究旨在用更少的T门实现该门。

Method: 使用具有O(log(1/ε))个T门的随机Clifford+T电路实现n量子比特Toffoli门，误差不超过ε（钻石距离）。

Result: 实现了误差为ε的n量子比特Toffoli门，仅需O(log(1/ε))个T门，并给出了Ω(log(1/ε))的下界。纯酉实现至少需要Ω(n)个T门。该技术也适用于其他布尔函数。

Conclusion: 提出了用指数级更少的T门实现Toffoli门的方法，并建立了T门计数与非自适应奇偶校验决策树复杂度的关系。

Abstract: Prior work of Beverland et al. has shown that any exact Clifford+$T$
implementation of the $n$-qubit Toffoli gate must use at least $n$ $T$ gates.
Here we show how to get away with exponentially fewer $T$ gates, at the cost of
incurring a tiny $1/\mathrm{poly}(n)$ error that can be neglected in most
practical situations. More precisely, the $n$-qubit Toffoli gate can be
implemented to within error $\epsilon$ in the diamond distance by a randomly
chosen Clifford+$T$ circuit with at most $O(\log(1/\epsilon))$ $T$ gates. We
also give a matching $\Omega(\log(1/\epsilon))$ lower bound that establishes
optimality, and we show that any purely unitary implementation achieving even
constant error must use $\Omega(n)$ $T$ gates. We also extend our sampling
technique to implement other Boolean functions. Finally, we describe upper and
lower bounds on the $T$-count of Boolean functions in terms of non-adaptive
parity decision tree complexity and its randomized analogue.

</details>


### [410] [Shedding light on classical shadows: learning photonic quantum states](https://arxiv.org/abs/2510.07240)
*Hugo Thomas,Ulysse Chabaud,Pierre-Emmanuel Emeriau*

Main category: quant-ph

TL;DR: 该研究提出了一种用于学习光子量子态的经典投射方案，通过随机的被动线性光学变换和光子数测量实现，并成功在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了在光子平台实现高效的量子态属性学习，并将其推广到光子技术。

Method: 提出一种利用随机的被动线性光学变换和光子数测量来学习光子量子态的经典投射协议。

Result: 该方案能够高效地估计一类重要的可观测量，并已在包含十二个模式的光子集成量子处理单元上进行了实验验证。

Conclusion: 该协议为可扩展地学习各种光子态属性提供了可能，并为将经典投射的丰富应用扩展到光子平台铺平了道路。

Abstract: Efficient learning of quantum state properties is both a fundamental and
practical problem in quantum information theory. Classical shadows have emerged
as an efficient method for estimating properties of unknown quantum states,
with rigorous statistical guarantees, by performing randomized measurement on a
few number of copies. With the advent of photonic technologies, formulating
efficient learning algorithms for such platforms comes out as a natural
problem. Here, we introduce a classical shadow protocol for learning photonic
quantum states via randomized passive linear optical transformations and
photon-number measurement. We show that this scheme is efficient for a large
class of observables of interest. We experimentally demonstrate our findings on
a twelve-mode photonic integrated quantum processing unit. Our protocol allows
for scalable learning of a wide range of photonic state properties and paves
the way to applying the already rich variety of applications of classical
shadows to photonic platforms.

</details>


### [411] [Quantum-Kaniadakis entropy as a measure of quantum correlations through implicit bounds](https://arxiv.org/abs/2510.07241)
*Narayan S Iyer,Shraddha Sharma*

Main category: quant-ph

TL;DR: 本文研究了负条件量子Kaniadakis熵（α-CQKE）与完全纠缠分数（FEF）的关系，并将其应用于量子信息处理协议，如隐形传态和量子操纵。研究了四种具有最大混合边际的量子态：2-qubit Werner态、2-qubit Weyl态、2-qudit Werner态和各向同性态。在2⊗2系统中，推导了当α-CQKE取负值时FEF的隐式界限。随后，对2-qubit Weyl态进行了类似分析。进一步将研究扩展到d⊗d系统，对各向同性态和2-qudit Werner态进行了分析。最后，利用FEF和量子操纵之间的关系，提出了将负α-CQKE与各向同性态的k-copy操纵联系起来的命题。


<details>
  <summary>Details</summary>
Motivation: 研究负条件量子Kaniadakis熵（α-CQKE）与完全纠缠分数（FEF）的关系，以评估其在量子信息处理（如隐形传态和量子操纵）中的应用潜力。

Method: 对四种量子态（2-qubit Werner态、2-qubit Weyl态、2-qudit Werner态和各向同性态）在不同系统（2⊗2和d⊗d）中，通过推导FEF的隐式界限，分析α-CQKE取负值时对FEF的影响，并将其与量子操纵联系起来。

Result: 在2⊗2系统中，对Werner态和Weyl态的FEF给出了负α-CQKE的隐式界限。在d⊗d系统中，对各向同性态和2-qudit Werner态也得到了类似的界限。此外，还提出了将负α-CQKE与各向同性态的k-copy操纵联系起来的命题。

Conclusion: 负条件量子Kaniadakis熵（α-CQKE）与完全纠缠分数（FEF）之间存在明确的关系，该关系可用于评估量子态在量子信息处理任务中的潜力，并为理解和利用量子操纵提供了新的途径。

Abstract: In the present article, we examine the relationship of negative conditional
quantum Kaniadakis entropy ($\alpha-$CQKE) with the fully entangled fraction
(FEF) which is a substantial yardstick for quantum information processing
protocols including teleportation, and quantum steerability, executed over four
vital quantum states with maximally mixed marginals, the 2-qubit Werner state,
the 2-qubit Weyl state, the 2-qudit Werner state and the isotropic state. We
initiate our analysis in 2$\otimes$2 systems where we derive implicit bounds on
FEF when the $\alpha-$CQKE takes negative values, i.e. when $\alpha-$CQKE $\in$
$R^{-}$ for 2-qubit Werner state. Consequently, we derive the sufficient
implicit bounds for a definitive claim on the non-usefulness of Werner state
for quantum teleportation provided its visibility parameter succeeds to elude a
critical region, the exception region 1, where the situation becomes
inconclusive. Subsequently, we replicate the same for the 2-qubit Weyl state
with some constraints augmented by an analogous exception region 2 and the
correlation tensor matrix elements. Furthermore, we extend our investigation to
d $\otimes$ d states, commencing our analysis with the Isotropic state. We
derive implicit bounds on FEF of the Isotropic state and the 2-qudit Werner
state resembling the ones in the 2$\otimes$2 analysis. Additionally, we utilize
the convoluted relationship between the FEF and quantum steerability to
formulate propositions linking negative $\alpha-$CQKE to the k-copy
steerability of isotropic states for projective measurements, thereby reducing
the intricacy of the study of k-copy steerability directly via FEF. In the
appendix section of the article, we provide corroborative calculations and
supplementary materials to the theorems presented in the main sections.

</details>


### [412] [Magic and communication complexity](https://arxiv.org/abs/2510.07246)
*Uma Girish,Alex May,Natalie Parham,Henry Yuen*

Main category: quant-ph

TL;DR: 量子电路中的魔力与通信复杂度之间的联系


<details>
  <summary>Details</summary>
Motivation: 探索量子计算中的魔力（magic）与通信复杂度之间的关系。

Method: 研究魔力门数量与确定性同时消息传递（D||）通信复杂度之间的关系，并提出将量子通信协议（Q||*）转化为经典通信协议（R||*）的方法。

Result: 发现低魔力函数具有低通信成本，并证明了某些量子门的魔力门数量下界。成功将Q||*协议转化为R||*协议，实现了指数级的R||*与R通信复杂度分离。

Conclusion: 量子电路中的魔力是通信复杂度的有用度量，并且Q||*协议可以有效地转化为具有隐私保护的R||*协议。

Abstract: We establish novel connections between magic in quantum circuits and
communication complexity. In particular, we show that functions computable with
low magic have low communication cost.
  Our first result shows that the $\mathsf{D}\|$ (deterministic simultaneous
message passing) cost of a Boolean function $f$ is at most the number of
single-qubit magic gates in a quantum circuit computing $f$ with any quantum
advice state. If we allow mid-circuit measurements and adaptive circuits, we
obtain an upper bound on the two-way communication complexity of $f$ in terms
of the magic + measurement cost of the circuit for $f$. As an application, we
obtain magic-count lower bounds of $\Omega(n)$ for the $n$-qubit generalized
Toffoli gate as well as the $n$-qubit quantum multiplexer.
  Our second result gives a general method to transform $\mathsf{Q}\|^*$
protocols (simultaneous quantum messages with shared entanglement) into
$\mathsf{R}\|^*$ protocols (simultaneous classical messages with shared
entanglement) which incurs only a polynomial blowup in the communication and
entanglement complexity, provided the referee's action in the $\mathsf{Q}\|^*$
protocol is implementable in constant $T$-depth. The resulting $\mathsf{R}\|^*$
protocols satisfy strong privacy constraints and are $\mathsf{PSM}^*$ protocols
(private simultaneous message passing with shared entanglement), where the
referee learns almost nothing about the inputs other than the function value.
As an application, we demonstrate $n$-bit partial Boolean functions whose
$\mathsf{R}\|^*$ complexity is $\mathrm{polylog}(n)$ and whose $\mathsf{R}$
(interactive randomized) complexity is $n^{\Omega(1)}$, establishing the first
exponential separations between $\mathsf{R}\|^*$ and $\mathsf{R}$ for Boolean
functions.

</details>


### [413] [From Quantum Circuits with Ultraslow Dynamics to Classical Plaquette Models](https://arxiv.org/abs/2510.07247)
*Vikram Ravindranath,Hanchen Liu,Xiao Chen*

Main category: quant-ph

TL;DR: 一种包含幺正门和测量诱导相变的混合量子电路，在某些初始状态下表现出体积律相和对数纠缠增长。


<details>
  <summary>Details</summary>
Motivation: 介绍一种包含幺正门和测量诱导相变的混合量子电路，并解释其体积律相和对数纠缠增长现象。

Method: 通过分析混合量子电路的性质，将其映射到经典自旋模型，并研究对称算符的影响。

Result: 发现该量子电路具有体积律相和对数纠缠增长，并与具有玻璃动力学的经典自旋模型相关联。

Conclusion: 该工作建立了量子纠缠动力学与经典玻璃行为之间 novel 的联系，并为纠缠相变提供了新的几何视角。

Abstract: We introduce a family of hybrid quantum circuits involving unitary gates and
projective measurements that display a measurement-induced phase transition.
Remarkably, the volume-law phase featuring logarithmic entanglement growth for
certain initial states. We attribute this slow entanglement growth to the
similarly slow growth of the participation entropy, which bounds the
entanglement. Furthermore, the quantum circuit can be mapped to a classical
spin model with real positive Boltzmann weights which involves local multi-spin
interactions and displays glassy dynamics at finite temperature. We trace the
origin of both the slow quantum dynamics and the classical glassiness to the
presence of large, non-local symmetry operators. Our work establishes a novel
connection between quantum entanglement dynamics and classical glassy behavior,
offering a new geometric perspective on entanglement phase transitions.

</details>


### [414] [When quantum resources backfire: Non-gaussianity and symplectic coherence in noisy bosonic circuits](https://arxiv.org/abs/2510.07264)
*Varun Upreti,Ulysse Chabaud,Zoë Holmes,Armando Angrisani*

Main category: quant-ph

TL;DR: 提出了一种名为“位移传播”的算法，用于模拟含噪声的玻色子电路，并发现了计算相变，其中非高斯性和辛相干性会使系统在有噪声时更容易被经典模拟。


<details>
  <summary>Details</summary>
Motivation: 分析噪声对量子系统的影响对于理解量子优势至关重要，而含噪声的玻色子电路难以模拟和分析。

Method: 提出了一种名为“位移传播”的算法，这是保利传播算法在连续变量下的模拟。

Result: 通过探索噪声和量子资源的相互作用，发现了计算相变，并揭示了即使是适度的噪声水平也会使玻色子电路易于经典模拟。其中，非高斯性和辛相干性在有噪声的情况下会使系统更容易被经典模拟。

Conclusion: 含噪声的玻色子电路的模拟和分析是一个重要的研究领域。提出的“位移传播”算法提供了一种有效的方法来模拟这些系统，并揭示了噪声和量子资源之间的复杂相互作用。研究结果表明，通常与玻色子量子优势相关的资源（如非高斯性和辛相干性）在存在噪声的情况下，反而可能使系统更容易被经典模拟。

Abstract: Analyzing the impact of noise is of fundamental importance to understand the
advantages provided by quantum systems. While the classical simulability of
noisy discrete-variable systems is increasingly well understood, noisy bosonic
circuits are more challenging to simulate and analyze. Here, we address this
gap by introducing the $\textit{displacement propagation}$ algorithm, a
continuous-variable analogue of Pauli propagation for simulating noisy bosonic
circuits. By exploring the interplay of noise and quantum resources, we
identify several computational phase transitions, revealing regimes where even
modest noise levels render bosonic circuits efficiently classically simulable.
In particular, our analysis reveals a surprising phenomenon: computational
resources usually associated with bosonic quantum advantage, namely
non-Gaussianity and symplectic coherence, can make the system easier to
classically simulate in presence of noise.

</details>


### [415] [Transversal dimension jump for product qLDPC codes](https://arxiv.org/abs/2510.07269)
*Christine Li,John Preskill,Qian Xu*

Main category: quant-ph

TL;DR: 该论文提出了一种名为“横向维度跳跃”的协议，用于在不同链复形维度的提升积（LP）量子低密度奇偶校验（qLDPC）码之间进行代码转换，以实现低开销的通用容错量子计算。


<details>
  <summary>Details</summary>
Motivation: 在不同维度量子码之间实现代码转换，以利用各自优势，实现低开销的通用容错量子计算。

Method: 利用LP码的乘积结构，在3D码及其2D分量码之间实现单向横向CNOT门，并结合3D码中的恒定深度CCZ门和2D码中的低开销横向Clifford门。

Result: 提出了一种可实现通用、高码率、高阈值和低时空成本的量子逻辑计算的方法。 找到了明确的3D-2D LP码对，支持cup-product CCZ门，例如[[81,3,5]]-（[[54,2,6]]）码对，该码对具有深度2的CCZ门、权重6的稳定器和伪阈值~0.4%。 另外，3D码可以高效地制备魔态，产生误差<10^-9且成功概率~35%的量子态。

Conclusion: 该研究将qLDPC码与互补的横向门进行了原生集成，覆盖了几乎所有实际相关的已知码族，并为可扩展、低开销的通用量子计算开辟了广阔的设计空间。

Abstract: We introduce transversal dimension jump, a code-switching protocol for lifted
product (LP) quantum low-density parity-check (qLDPC) codes across different
chain-complex dimensions, enabling universal fault-tolerant quantum computation
with low overhead. The construction leverages the product structure of LP codes
to implement one-way transversal CNOTs between a 3D code and its 2D component
codes, enabling teleportation-based switching. Combined with constant-depth CCZ
gates in 3D LP codes and low-overhead transversal Clifford gates in 2D LP
codes, this yields universal, high-rate quantum logical computation with high
thresholds and low space-time costs. Beyond asymptotic schemes, we identify
explicit 3D-2D LP code pairs supporting cup-product CCZ gates, including
bivariate tricycle-bicycle families such as the $[[81,3,5]]$-$[[54,2,6]]$ pair,
where the 3D tricycle codes admit depth-2 CCZ, weight-6 stabilizers, and
pseudo-thresholds $\gtrsim 0.4\%$. As a byproduct, we show that the 3D codes
enable highly efficient magic-state preparation: a single round of stabilizer
measurements followed by depth-2 CCZ and postselection produces states with
error $<10^{-9}$ and success probability $\sim 35\%$. Our results establish a
native integration of qLDPC codes with complementary transversal gates-covering
nearly all practically relevant families known so far-and open a broad design
space for scalable, low-overhead universal quantum computation.

</details>


### [416] [End-to-end quantum algorithms for tensor problems](https://arxiv.org/abs/2510.07273)
*Enrico Fontana,Sivaprasad Omanakuttan,Junhyung Lyle Kim,Joseph Sullivan,Michael Perlin,Ruslan Shaydulin,Shouvanik Chakrabarti*

Main category: quant-ph

TL;DR: 我们提出了一个端到端的量子算法，用于解决张量主成分分析（PCA）和带植入kXOR等张量问题，该算法在某些情况下比经典方法有超二次的量子加速。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有张量问题量子算法的局限性，如对Kikuchi方法的原生量子比特编码不足，导致资源开销大和量子优势阈值高的问题。

Method: 通过引入Kikuchi方法的原生量子比特编码，实现了明确的量子电路构造和非渐近资源估计。采用新颖的引导态制备技术和电路优化，显著降低了常数开销，从而降低了量子优势的门槛。该算法框架还扩展支持稀疏张量PCA和张量补全，并将检测推广到非对称张量。

Result: 对于一个经典计算需要~10^23 FLOPs的问题，该算法只需要900个逻辑量子比特，~10^15门和~10^12门的深度。相比之下，没有改进的算法需要至少10^19个门和10^18的深度。

Conclusion: 该研究提出的量子算法在张量问题上取得了显著的进展，通过算法和编译的改进，大大降低了对量子硬件资源的需求，使其有望在未来的容错量子计算机上实现量子优势。

Abstract: We present a comprehensive end-to-end quantum algorithm for tensor problems,
including tensor PCA and planted kXOR, that achieves potential superquadratic
quantum speedups over classical methods. We build upon prior works by
Hastings~(\textit{Quantum}, 2020) and Schmidhuber~\textit{et
al.}~(\textit{Phys.~Rev.~X.}, 2025), we address key limitations by introducing
a native qubit-based encoding for the Kikuchi method, enabling explicit quantum
circuit constructions and non-asymptotic resource estimation. Our approach
substantially reduces constant overheads through a novel guiding state
preparation technique as well as circuit optimizations, reducing the threshold
for a quantum advantage. We further extend the algorithmic framework to support
recovery in sparse tensor PCA and tensor completion, and generalize detection
to asymmetric tensors, demonstrating that the quantum advantage persists in
these broader settings. Detailed resource estimates show that 900 logical
qubits, $\sim 10^{15}$ gates and $\sim 10^{12}$ gate depth suffice for a
problem that classically requires $\sim 10^{23}$ FLOPs. The gate count and
depth for the same problem without the improvements presented in this paper
would be at least $10^{19}$ and $10^{18}$ respectively. These advances position
tensor problems as a candidate for quantum advantage whose resource
requirements benefit significantly from algorithmic and compilation
improvements; the magnitude of the improvements suggest that further
enhancements are possible, which would make the algorithm viable for upcoming
fault-tolerant quantum hardware.

</details>


### [417] [Universal initial state preparation for first quantized quantum simulations](https://arxiv.org/abs/2510.07278)
*Jack S. Baker,Gaurav Saxena,Thi Ha Kyaw*

Main category: quant-ph

TL;DR: 该研究提出了一种通用的、高效的方法，将任意多项式大小的占有数配置叠加映射到数字量子计算机上的第一量子化表示。


<details>
  <summary>Details</summary>
Motivation: 准备对称性适应的初始态是第一量子化量子模拟中的主要瓶颈。

Method: 该方法利用Jordan-Schwinger李代数同态，将守恒数二阶量子算符与其第一量子化作用相对应，并在Schur-Weyl分解中诱导Fock占有与mathfraksu(d)权重态之间的等变双射。操作上，通过酉算子的块编码线性组合来准备Schur标签的编码叠加，然后应用逆量子Schur变换。

Result: 该算法对于L个配置、N个粒子、d个模式，以精度ε运行，时间复杂度为poly(L, N, d, log ε^{-1})，并且普遍适用于费米子、玻色子和任意单粒子基中的Green's paraparticles。资源估算表明该方法在领先的第一量子化流程中具有实用性。

Conclusion: 所提出的方法能够高效地准备第一量子化量子模拟所需的对称性适应的初始态，为量子模拟开辟了新的可能性。

Abstract: Preparing symmetry-adapted initial states is a principal bottleneck in
first-quantized quantum simulation. We present a universal approach that
efficiently maps any polynomial-size superposition of occupation-number
configurations to the first-quantized representation on a digital quantum
computer. The method exploits the Jordan--Schwinger Lie algebra homomorphism,
which identifies number-conserving second-quantized operators with their
first-quantized action and induces an equivariant bijection between Fock
occupations and $\mathfrak{su}(d)$ weight states within the Schur--Weyl
decomposition. Operationally, we prepare an encoded superposition of Schur
labels via a block-encoded linear combination of unitaries and then apply the
inverse quantum Schur transform. The algorithm runs in time $\text{poly}(L, N,
d, \log \epsilon^{-1})$ for $L$ configurations of $N$ particles over $d$ modes
to accuracy $\epsilon$, and applies universally to fermions, bosons, and
Green's paraparticles in arbitrary single-particle bases. Resource estimates
indicate practicality within leading first-quantized pipelines;
statistics-aware or faster quantum Schur transforms promise further reductions.

</details>


### [418] [End-to-End Quantum Algorithm for Topology Optimization in Structural Mechanics](https://arxiv.org/abs/2510.07280)
*Leonhard Hölscher,Oliver Ahrend,Lukas Karch,Carlotta L'Estocq,Marc Marfany Andreu,Tobias Stollenwerk,Frank K. Wilhelm,Julia Kowalski*

Main category: quant-ph

TL;DR: 提出了一种端到端的容错量子算法，用于拓扑优化，通过利用量子叠加和Grover算法，在多项式时间内评估指数级的结构，并保持了相对于经典非结构化搜索的二次加速。


<details>
  <summary>Details</summary>
Motivation: 传统的拓扑优化方法由于设计空间巨大而难以评估所有可能的配置，需要一种更有效的方法来寻找工程设计中的高效稳健结构。

Method: 将满足度最小化问题重新表述为组合可满足性问题，并使用Grover算法解决。在Grover算法的Oracle内部，通过量子有限元方法（FEM）计算合规性，包括刚度矩阵的块编码、用于矩阵求逆的量子奇异值变换（QSVT）、Hadamard测试和量子幅度估计算法（QAE）。

Result: 在二维MBB梁问题上演示了该算法，并通过经典量子电路模拟进行了实现和验证。复杂度分析表明，该方法可以在多项式时间内评估量子叠加中指数级的结构合规性。

Conclusion: 所提出的量子工作流程展示了量子算法如何在计算科学和工程领域取得进展，为解决大规模优化问题提供了一种新的量子方法。

Abstract: Topology optimization is a key methodology in engineering design for finding
efficient and robust structures. Due to the enormous size of the design space,
evaluating all possible configurations is typically infeasible. In this work,
we present an end-to-end, fault-tolerant quantum algorithm for topology
optimization that operates on the exponential Hilbert space representing the
design space. We demonstrate the algorithm on the two-dimensional
Messerschmitt-B\"olkow-Blohm (MBB) beam problem. By restricting design
variables to binary values, we reformulate the compliance minimization task as
a combinatorial satisfiability problem solved using Grover's algorithm. Within
Grover's oracle, the compliance is computed through the finite-element method
(FEM) using established quantum algorithms, including block-encoding of the
stiffness matrix, Quantum Singular Value Transformation (QSVT) for matrix
inversion, Hadamard test, and Quantum Amplitude Estimation (QAE). The complete
algorithm is implemented and validated using classical quantum-circuit
simulations. A detailed complexity analysis shows that the method evaluates the
compliance of exponentially many structures in quantum superposition in
polynomial time. In the global search, our approach maintains Grover's
quadratic speedup compared to classical unstructured search. Overall, the
proposed quantum workflow demonstrates how quantum algorithms can advance the
field of computational science and engineering.

</details>


### [419] [Quantum Replica Exchange](https://arxiv.org/abs/2510.07291)
*Zherui Chen,Joao Basso,Zhiyan Ding,Lin Lin*

Main category: quant-ph

TL;DR: 本工作提出了一种量子副本交换方法，可以加速具有局部能量势垒的哈密顿量的吉布斯状态制备。


<details>
  <summary>Details</summary>
Motivation: 能量势垒会导致采样算法（如MCMC）收敛缓慢。在量子领域，制备哈密顿量的吉布斯状态也面临类似挑战，瓶颈会显著增加量子动力学半群的混合时间。

Method: 提出了一种量子副本交换方法，在联合系统的两个副本上定义一个Lindbladian，并证明它可以加速一类具有局部能量势垒的哈密顿量的混合。主要结果是证明了组合系统Lindbladian的谱隙的严格下界，该下界相对于势垒高度呈指数级增长。

Result: 所提出的量子副本交换方法可以将谱隙呈指数级提高，这表明混合时间大大缩短。在具有缺陷的1D伊辛模型和高斯非对易局部哈密顿量等示例中展示了该方法的适用性。

Conclusion: 本工作提供了一种严格的加速量子吉布斯状态制备的机制，有望在量子模拟和计算领域得到应用。

Abstract: The presence of energy barriers in the state space of a physical system can
lead to exponentially slow convergence for sampling algorithms like Markov
chain Monte Carlo (MCMC). In the classical setting, replica exchange (or
parallel tempering) is a powerful heuristic to accelerate mixing in these
scenarios. In the quantum realm, preparing Gibbs states of Hamiltonians faces a
similar challenge, where bottlenecks can dramatically increase the mixing time
of quantum dynamical semigroups. In this work, we introduce a quantum analogue
of the replica exchange method. We define a Lindbladian on a joint system of
two replicas and prove that it can accelerate mixing for a class of
Hamiltonians with local energy barriers. Our main result provides a rigorous
lower bound on the spectral gap of the combined system's Lindbladian, which
leads to an exponential improvement in spectral gap with respect to the barrier
height. We showcase the applicability of our method with several examples,
including the defected 1D Ising model at arbitrary constant temperature, and
defected non-commuting local Hamiltonians at high temperature. Our work
provides a rigorous acceleration mechanism for quantum Gibbs preparation.

</details>


### [420] [Fine-Grained Unambiguous Measurements](https://arxiv.org/abs/2510.07298)
*Quentin Buzet,André Chailloux*

Main category: quant-ph

TL;DR: 提出细粒度无歧义测量，并分析其在量子解码问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 量子信息中的无歧义测量具有重要应用，但缺乏通用理论框架来分析其在量子算法中的应用，特别是S-LWE和量子解码问题。

Method: 提出细粒度无歧义测量的概念，并将其在对称状态下的分析转化为线性规划问题，利用对偶形式推导了上界，并建立了存在性条件。

Result: 研究了对称状态下的细粒度无歧义测量，将其转化为线性规划问题，并通过对偶形式推导了上界。证明了其存在性的充要条件，并表明无法改进现有方法。

Conclusion: 虽然提出了细粒度无歧义测量的概念，但其在量子解码问题中的应用并未取得突破性进展，无法优于现有方法。

Abstract: Unambiguous measurements play an important role in quantum information, with
applications ranging from quantum key distribution to quantum state
reconstruction. Recently, such measurements have also been used in quantum
algorithms based on Regev's reduction. The key problem for these algorithms is
the S-LWE problem for lattice problems, or the Quantum Decoding Problem for
code problems. A key idea for addressing this problem is to use unambiguous
measurements to recover $k$ coordinates of a code (or lattice) element $x$ from
a quantum state $|\psi_x\rangle$, which corresponds to a noisy word $x$ with
errors in quantum superposition. However, a general theoretical framework to
analyze this approach has been lacking.
  In this work, we introduce the notion of fine-grained unambiguous
measurements. Given a family of states
$\{\,|\psi_x\rangle\,\}_{x\in\{0,1\}^n}$, we ask whether there exist
measurements that can return, with certainty, $k$ bits of information about
$x$. We study this question in the setting of symmetric states, which naturally
arises in the Quantum Decoding Problem. We show that determining the maximal
number of parities that a measurement can output can be formulated as a linear
program, and we use its dual formulation to derive several upper bounds. In
particular, we establish necessary and sufficient conditions for the existence
of fine-grained unambiguous measurements and prove impossibility results
showing, in particular, that such measurements cannot improve upon the approach
of arXiv:2310.20651. Finally, we discuss the implications of these findings for
the Quantum Decoding Problem.

</details>


### [421] [Is it Gaussian? Testing bosonic quantum states](https://arxiv.org/abs/2510.07305)
*Filippo Girardi,Freek Witteveen,Francesco Anna Mele,Lennart Bittel,Salvatore F. E. Oliviero,David Gross,Michael Walter*

Main category: quant-ph

TL;DR: 只需要常数份的样本就可以判断一个纯态是否是高斯态，但是对于混合态，需要指数份的样本。


<details>
  <summary>Details</summary>
Motivation: 需要一个有效的方法来判断一个量子态是否是高斯态。

Method: 利用高斯态的旋转不变对称性和量子 प्रकारचे界限。

Result: 纯态的高斯性测试只需要常数份样本，混合态则需要指数份样本。

Conclusion: 高斯态在量子技术中扮演着重要角色，但测试混合态是否为高斯态存在根本性的局限。

Abstract: Gaussian states are widely regarded as one of the most relevant classes of
continuous-variable (CV) quantum states, as they naturally arise in physical
systems and play a key role in quantum technologies. This motivates a
fundamental question: given copies of an unknown CV state, how can we
efficiently test whether it is Gaussian? We address this problem from the
perspective of representation theory and quantum learning theory,
characterizing the sample complexity of Gaussianity testing as a function of
the number of modes. For pure states, we prove that just a constant number of
copies is sufficient to decide whether the state is exactly Gaussian. We then
extend this to the tolerant setting, showing that a polynomial number of copies
suffices to distinguish states that are close to Gaussian from those that are
far. In contrast, we establish that testing Gaussianity of general mixed states
necessarily requires exponentially many copies, thereby identifying a
fundamental limitation in testing CV systems. Our approach relies on
rotation-invariant symmetries of Gaussian states together with the recently
introduced toolbox of CV trace-distance bounds.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [422] [Comparing Normal Form Representations for Station-Keeping near Cislunar Libration Points](https://arxiv.org/abs/2510.06368)
*Carson Hunsberger,David Schwab,Roshan Eapen,Puneet Singla*

Main category: eess.SY

TL;DR: 该论文比较了Birkhoff normal form和resonant normal form在表示中心流形轨迹方面的优缺点，并提出了一种类似Floquet模式的轨道保持方法，通过脉冲机动来最小化不稳定分量，并应用于不同类型的轨道。


<details>
  <summary>Details</summary>
Motivation: 比较Birkhoff normal form和resonant normal form在表示中心流形轨迹方面的优缺点。

Method: 提出了一种类似Floquet模式的轨道保持方法，通过脉冲机动来最小化不稳定分量，并应用于Lyapunov、垂直和晕轨道、以及Lissajous和准晕轨道。

Result: Birkhoff normal form和resonant normal form在表示中心流形轨迹方面各有优势。

Conclusion: 所提出的轨道保持方法可以有效地应用于多种轨道类型。

Abstract: The normal forms provide useful approximations for many trajectories of
interest within the circular restricted three-body problem. This paper aims to
thoroughly compare two of these forms: the Birkhoff normal form and the
resonant normal form, highlighting the strengths of each for the representation
of center manifold trajectories. A method of station-keeping is introduced,
analogous to Floquet modes, in which the unstable component is minimized at
specific points along a trajectory through impulsive maneuvers. Three different
formulations of the same station-keeping approach are posed, collectively
spanning Lyapunov, vertical, and halo orbits, as well as Lissajous and
quasihalo trajectories.

</details>


### [423] [Three-dimensional Integrated Guidance and Control for Leader-Follower Flexible Formation of Fixed Wing UAVs](https://arxiv.org/abs/2510.06394)
*Praveen Kumar Ranjan,Abhinav Sinha,Yongcan Cao*

Main category: eess.SY

TL;DR: 该论文提出了一种非线性综合制导与控制（IGC）方法，用于固定翼无人机（UAV）的灵活编队飞行，并考虑了高保真气动和推力动力学。该方法允许跟随者在头部指定的半球区域内保持与头部的距离和方位角，并利用动态表面控制和反步法确保稳定性。


<details>
  <summary>Details</summary>
Motivation: 为了实现固定翼无人机（UAV）的灵活编队飞行，并能应对头部的大幅度机动，需要一种能精确控制相对距离和方位角的制导与控制方法。

Method: 采用非线性综合制导与控制（IGC）方法，结合动态表面控制（DSC）和反步法。将相对距离动力学映射到节气门指令，将相对方位角映射到气动控制面偏转。利用Lyapunov障碍函数来约束方位角。

Result: 仿真结果表明，该方法能够有效地实现灵活的编队飞行，并能保证所有误差状态的均匀最终有界性和严格的约束满足。

Conclusion: 该论文提出的非线性IGC方法能够实现灵活的UAV编队飞行，即使在GPS拒止或非合作环境下也能有效工作，并能应对头部的剧烈机动。

Abstract: This paper presents a nonlinear integrated guidance and control (IGC)
approach for flexible leader-follower formation flight of fixed-wing unmanned
aerial vehicles (UAVs) while accounting for high-fidelity aerodynamics and
thrust dynamics. Unlike conventional leader-follower schemes that fix the
follower's position relative to the leader, the follower is steered to maintain
range and bearing angles (which is the angle between its velocity vector and
its line-of-sight (LOS) with respect to the leader) arbitrarily close to the
prescribed values, enabling the follower to maintain formation on a
hemispherical region behind the leader. The proposed IGC framework directly
maps leader-follower relative range dynamics to throttle commands, and the
follower's velocity orientation relative to the LOS to aerodynamic control
surface deflections. This enables synergism between guidance and control
subsystems. The control design uses a dynamic surface control-based
backstepping approach to achieve convergence to the desired formation set,
where Lyapunov barrier functions are incorporated to ensure the follower's
bearing angle is constrained within specified bounds. Rigorous stability
analysis guarantees uniform ultimate boundedness of all error states and strict
constraint satisfaction in the presence of aerodynamic nonlinearities. The
proposed flexible formation scheme allows the follower to have an orientation
mismatch relative to the leader to execute anticipatory reconfiguration by
transitioning between the relative positions in the admissible formation set
when the leader aggressively maneuvers. The proposed IGC law relies only on
relative information and onboard sensors without the information about the
leader's maneuver, making it suitable for GPS-denied or non-cooperative
scenarios. Finally, we present simulation results to vindicate the
effectiveness and robustness of our approach.

</details>


### [424] [Terrain-Aided Navigation Using a Point Cloud Measurement Sensor](https://arxiv.org/abs/2510.06470)
*Abdülbaki Şanlan,Fatih Erol,Murad Abu-Khalaf,Emre Koyuncu*

Main category: eess.SY

TL;DR: 点云测量可用于辅助惯导系统进行地形匹配导航，两种点云测量模型（射线追踪和滑动网格）均优于雷达高度计，但模型选择取决于计算资源。


<details>
  <summary>Details</summary>
Motivation: 为了改进惯性导航系统，我们研究了使用点云测量进行地形匹配导航的方法，以产生有效的非线性状态估计。

Method: 对比了两种基于数字高程模型（DEM）的点云测量模型：1.基于射线追踪的预测点云测量；2.计算量较小的滑动网格模型。并分析了两种模型对高度观测性的影响，同时以雷达高度计作为基线进行性能比较。

Result: 点云测量辅助导航的精度优于仅使用雷达高度计。滑动网格模型在计算上比射线追踪模型更有效。

Conclusion: 点云测量是一种有效的方式来辅助惯性导航，其性能优于雷达高度计。在选择点云测量模型时，应考虑可用的计算资源，因为滑动网格模型在计算效率方面优于射线追踪模型。

Abstract: We investigate the use of a point cloud measurement in terrain-aided
navigation. Our goal is to aid an inertial navigation system, by exploring ways
to generate a useful measurement innovation error for effective nonlinear state
estimation. We compare two such measurement models that involve the scanning of
a digital terrain elevation model: a) one that is based on typical ray-casting
from a given pose, that returns the predicted point cloud measurement from that
pose, and b) another computationally less intensive one that does not require
raycasting and we refer to herein as a sliding grid. Besides requiring a pose,
it requires the pattern of the point cloud measurement itself and returns a
predicted point cloud measurement. We further investigate the observability
properties of the altitude for both measurement models. As a baseline, we
compare the use of a point cloud measurement performance to the use of a radar
altimeter and show the gains in accuracy. We conclude by showing that a point
cloud measurement outperforms the use of a radar altimeter, and the point cloud
measurement model to use depends on the computational resources

</details>


### [425] [Model Predictive Path Integral Control for Roll-to-Roll Manufacturing](https://arxiv.org/abs/2510.06547)
*Christopher Martin,Apurva Patil,Wei Li,Takashi Tanaka,Dongmei Chen*

Main category: eess.SY

TL;DR: MPPI 控制在卷对卷制造中能有效提升张力调节性能。


<details>
  <summary>Details</summary>
Motivation: 卷对卷制造对于薄膜材料和印刷电子的可扩展生产至关重要，但由于子系统相互作用、非线性以及过程扰动，精确控制仍然具有挑战性。

Method: 提出一种基于 GPU 的蒙特卡洛采样方法的模型预测路径积分（MPPI）控制，以有效地在线逼近最优控制，并能处理非可微成本函数。

Result: MPPI 在张力调节方面的性能显著优于传统的模型预测控制（MPC）。

Conclusion: MPPI 控制适用于先进制造过程中的实时控制，能显著提高卷对卷制造的张力调节性能。

Abstract: Roll-to-roll (R2R) manufacturing is a continuous processing technology
essential for scalable production of thin-film materials and printed
electronics, but precise control remains challenging due to subsystem
interactions, nonlinearities, and process disturbances. This paper proposes a
Model Predictive Path Integral (MPPI) control formulation for R2R systems,
leveraging a GPU-based Monte-Carlo sampling approach to efficiently approximate
optimal controls online. Crucially, MPPI easily handles non-differentiable cost
functions, enabling the incorporation of complex performance criteria relevant
to advanced manufacturing processes. A case study is presented that
demonstrates that MPPI significantly improves tension regulation performance
compared to conventional model predictive control (MPC), highlighting its
suitability for real-time control in advanced manufacturing.

</details>


### [426] [A Cascade of Systems and the Product of Their $θ$-Symmetric Scaled Relative Graphs](https://arxiv.org/abs/2510.06583)
*Xiaokan Yang,Ding Zhang,Wei Chen,Li Qiu*

Main category: eess.SY

TL;DR: 本研究提出了一种称为 θ-对称 SRG 的缩放相对图（SRG）的变体，用于为级联系统的反馈互连开发图形稳定性判据。


<details>
  <summary>Details</summary>
Motivation: 传统的图分离方法不适用于循环互连，而 θ-对称 SRG 具有子乘积性质，可以处理这些情况，并结合增益和相位信息，提供更少保守的系统表征。

Method: 利用 θ-对称 SRG 及其子乘积性质，并将其与增益和相位信息相结合，为级联系统开发图形稳定性判据。

Result: θ-对称 SRG 可以精确地简化为标量情况，并且比标准 SRG 更适合作为经典奈奎斯特图的多输入多输出扩展。

Conclusion: θ-对称 SRG 为反馈互连的级联系统提供了一种有效且不保守的图形稳定性判据，特别适用于循环互连。

Abstract: In this paper, we utilize a variant of the scaled relative graph (SRG),
referred to as the $\theta$-symmetric SRG, to develop a graphical stability
criterion for the feedback interconnection of a cascade of systems. A crucial
submultiplicative property of $\theta$-symmetric SRG is established, enabling
it to handle cyclic interconnections for which conventional graph separation
methods are not applicable. By integrating both gain and refined phase
information, the $\theta$-symmetric SRG provides a unified graphical
characterization of the system, which better captures system properties and
yields less conservative results. In the scalar case, the $\theta$-symmetric
SRG can be reduced exactly to the scalar itself, whereas the standard SRG
appears to be a conjugate pair. Consequently, the frequency-wise
$\theta$-symmetric SRG is more suitable than the standard SRG as a multi-input
multi-output extension of the classical Nyquist plot. Illustrative examples are
included to demonstrate the effectiveness of the $\theta$-symmetric SRG.

</details>


### [427] [Delay Independent Safe Control with Neural Networks: Positive Lur'e Certificates for Risk Aware Autonomy](https://arxiv.org/abs/2510.06661)
*Hamidreza Montazeri Hedesh,Milad Siami*

Main category: eess.SY

TL;DR: 我们提出了一种面向风险的、用于学习型自主控制系统的安全认证方法，该方法能够处理状态/输入延迟和区间矩阵不确定性等实际风险，并通过利用局部扇形界限和正结构来推导出线性、与延迟无关的认证，从而实现对可允许不确定性下的局部指数稳定性保证。与基于SDP的IQC方法相比，我们提出的基于正性的测试运行速度快几个数量级，并且能够认证后者无法处理的系统，从而为风险感知控制提供可扩展的安全保证。


<details>
  <summary>Details</summary>
Motivation: 本文旨在为自主学习控制系统提供一种风险感知安全认证方法，以应对状态/输入延迟和区间矩阵不确定性等实际风险。

Method: 提出了一种风险感知安全认证方法，该方法使用局部扇形界限对神经网络控制器进行建模，并利用正结构推导出线性、与延迟无关的认证，以保证在可允许的不确定性下的局部指数稳定性。

Result: 与基于SDP的IQC方法相比，所提出的基于正性的测试运行速度快了几个数量级，并且能够认证后者无法处理的系统，从而为风险感知控制提供可扩展的安全保证。

Conclusion: 本文提出的基于正性的风险感知安全认证方法，相比于现有的IQC方法，在效率和认证能力上均有显著优势，能够为自主学习控制系统提供可扩展的安全保证。

Abstract: We present a risk-aware safety certification method for autonomous, learning
enabled control systems. Focusing on two realistic risks, state/input delays
and interval matrix uncertainty, we model the neural network (NN) controller
with local sector bounds and exploit positivity structure to derive linear,
delay-independent certificates that guarantee local exponential stability
across admissible uncertainties. To benchmark performance, we adopt and
implement a state-of-the-art IQC NN verification pipeline. On representative
cases, our positivity-based tests run orders of magnitude faster than SDP-based
IQC while certifying regimes the latter cannot-providing scalable safety
guarantees that complement risk-aware control.

</details>


### [428] [Resilient Multi-Dimensional Consensus and Distributed Optimization against Agent-Based and Denial-of-Service Attacks](https://arxiv.org/abs/2510.06835)
*Hongjian Chen,Changyun Wen,Xiaolei Li,Jiaqi Yan*

Main category: eess.SY

TL;DR: 本论文提出了一种基于“辅助点”的弹性控制算法和一种弹性多维分布式优化（RMDO）算法，以解决多智能体系统（MAS）在面临包含恶意、拜占庭和顽固特工的智能体攻击以及可能导致网络拓扑时变甚至断开的拒绝服务（DoS）攻击时的一致性和分布式优化问题，确保剩余的良性智能体达成共识。


<details>
  <summary>Details</summary>
Motivation: 为解决多智能体系统（MAS）在面临智能体攻击（恶意、拜占庭、顽固）和拒绝服务（DoS）攻击（可能导致网络时变甚至断开）时的一致性和分布式优化问题。

Method: 提出一种基于“辅助点”的弹性控制算法，其中每个健康智能体利用其入邻居的状态构建“安全核”，并在每次迭代时更新其状态。如果智能体因DoS攻击无法接收邻居状态，则使用DoS攻击前接收的状态。此外，还提出了一种弹性多维分布式优化（RMDO）算法。

Result: 理论证明和数值示例表明了所提出算法的有效性。

Conclusion: 所提出的基于“辅助点”的弹性控制算法和RMDO算法能够有效地解决多智能体系统在面临复杂攻击下的共识和分布式优化问题。

Abstract: In this paper, we consider the resilient multi-dimensional consensus and
distributed optimization problems of multi-agent systems (MASs) in the presence
of both agent-based and denial-of-service (DoS) attacks. The considered
agent-based attacks can cover malicious, Byzantine, and stubborn agents. The
links between agents in the network can be blocked by DoS attacks, which may
lead the digraph to be time-varying and even disconnected. The objective is to
ensure that the remaining benign agents achieve consensus. To this end, an
"auxiliary point"-based resilient control algorithm is proposed for MASs. Under
the proposed algorithm, each healthy agent constructs a "safe kernel" utilizing
the states of its in-neighbors and updates its state toward a specific point
within this kernel at each iteration. If an agent cannot receive its neighbors'
states owing to DoS attacks, it will use the states received immediately before
the DoS period. Moreover, a resilient multi-dimensional distributed
optimization (RMDO) algorithm is also proposed. Theoretical proofs and
numerical examples are presented to demonstrate the effectiveness of the
proposed algorithms.

</details>


### [429] [Decentralized CBF-based Safety Filters for Collision Avoidance of Cooperative Missile Systems with Input Constraints](https://arxiv.org/abs/2510.06846)
*Johannes Autenrieb,Mark Spiller*

Main category: eess.SY

TL;DR: 该研究提出了一种去中心化的安全滤波器，用于多智能体航空拦截场景中的碰撞避免。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体航空拦截场景中的碰撞避免问题，并确保系统的安全性。

Method: 利用鲁棒控制势垒函数（RCBFs）来保证安全集在有界输入和高相对度动力学下的前向不变性。每个效应器执行其标称的合作制导命令，同时本地二次规划（QP）仅在必要时修改输入。基于范围和零努力错过（ZEM）标准的事件触发激活确保了可扩展性。引入了松弛变量松弛方案来解决同时约束的可行性问题，并以帕累托最优的方式确定关键代理的优先级。

Result: 在多对多拦截场景的模拟结果表明，该框架在保持无碰撞运行的同时，对标称制导的偏离最小。

Conclusion: 该方法为安全关键的多智能体航空系统提供了一种计算高效且可扩展的解决方案。

Abstract: This paper presents a decentralized safety filter for collision avoidance in
multi-agent aerospace interception scenarios. The approach leverages robust
control barrier functions (RCBFs) to guarantee forward invariance of safety
sets under bounded inputs and high-relative-degree dynamics. Each effector
executes its nominal cooperative guidance command, while a local quadratic
program (QP) modifies the input only when necessary. Event-triggered activation
based on range and zero-effort miss (ZEM) criteria ensures scalability by
restricting active constraints to relevant neighbors. To resolve feasibility
issues from simultaneous constraints, a slack-variable relaxation scheme is
introduced that prioritizes critical agents in a Pareto-optimal manner.
Simulation results in many-on-many interception scenarios demonstrate that the
proposed framework maintains collision-free operation with minimal deviation
from nominal guidance, providing a computationally efficient and scalable
solution for safety-critical multi-agent aerospace systems.

</details>


### [430] [Falsification-Driven Reinforcement Learning for Maritime Motion Planning](https://arxiv.org/abs/2510.06970)
*Marlon Müller,Florian Finkeldei,Hanna Krasowski,Murat Arcak,Matthias Althoff*

Main category: eess.SY

TL;DR: 通过生成对抗性训练场景来训练自动驾驶船舶遵守海事交通规则。


<details>
  <summary>Details</summary>
Motivation: 训练自动驾驶船舶遵守海事交通规则是安全运行的关键，但面临挑战。现有的方法依赖于可能无法捕捉复杂性的训练场景，而真实世界的数据又不足以应对这种情况。

Method: 提出一种基于伪造的强化学习方法，该方法生成一个场景，其中被测试的船只违反了表示为信号时序逻辑规范的海事交通规则。

Result: 在开放海域进行了双船导航实验，结果表明该方法生成的训练场景更相关，并且能够更一致地遵守规则。

Conclusion: 所提出的基于伪造的强化学习方法可以为自动驾驶船舶的训练生成更有效的场景，从而提高其在海事交通规则方面的合规性。

Abstract: Compliance with maritime traffic rules is essential for the safe operation of
autonomous vessels, yet training reinforcement learning (RL) agents to adhere
to them is challenging. The behavior of RL agents is shaped by the training
scenarios they encounter, but creating scenarios that capture the complexity of
maritime navigation is non-trivial, and real-world data alone is insufficient.
To address this, we propose a falsification-driven RL approach that generates
adversarial training scenarios in which the vessel under test violates maritime
traffic rules, which are expressed as signal temporal logic specifications. Our
experiments on open-sea navigation with two vessels demonstrate that the
proposed approach provides more relevant training scenarios and achieves more
consistent rule compliance.

</details>


### [431] [Mitigating Increase-Decrease Gaming with Alternative Connection Agreements: A Defender-Attacker-Defender Game](https://arxiv.org/abs/2510.07102)
*Bart van der Holst,Thomas Swarts,Phuong Nguyen,Johan Morren,Koen Kok*

Main category: eess.SY

TL;DR: Redispatch markets are prone to strategic manipulation by Flexibility Service Providers (FSPs) who adjust baselines to increase congestion and costs. This paper proposes Alternative Connection Agreements (ACAs) as a solution, where Distribution System Operators (DSOs) can conditionally limit connection capacity. A Defender-Attacker-Defender game models this trilevel optimization problem, solved via a custom branch-and-bound algorithm. Results show ACAs can significantly reduce redispatch costs (around 25%) with minimal impact on FSP profits, depending on ACA invocation frequency and DSO's anticipation of FSP behavior.


<details>
  <summary>Details</summary>
Motivation: Flexibility Service Providers (FSPs) strategically adjust their baselines in redispatch markets to anticipate system operator actions, leading to increased congestion and system costs. Distribution System Operators (DSOs) need a mechanism to counteract this 'increase-decrease gaming'.

Method: A Defender-Attacker-Defender game model is developed to investigate the use of Alternative Connection Agreements (ACAs) for DSOs to limit connection capacity in the day-ahead stage, addressing congestion and strategic baseline adjustments under uncertainty. The trilevel optimization model is solved using a custom branch-and-bound algorithm.

Result: The proposed approach using ACAs can substantially reduce redispatch costs for the DSO (e.g., by 25%) with only a limited impact on FSP profits. The custom branch-and-bound algorithm efficiently solves the trilevel optimization problem for most simulated scenarios.

Conclusion: Alternative Connection Agreements (ACAs) offer a promising strategy for DSOs to mitigate congestion and reduce redispatch costs caused by strategic FSP behavior. The effectiveness of ACAs is contingent upon the DSO's ability to invoke them frequently and accurately anticipate FSP's strategic bidding. The solution method demonstrates efficiency in solving the complex optimization problem.

Abstract: Redispatch markets are widely used by system operators to manage network
congestion. A well-known drawback, however, is that Flexibility Service
Providers (FSPs) may strategically adjust their baselines in anticipation of
redispatch actions, thereby aggravating congestion and raising system costs. To
address this increase-decrease gaming, Distribution System Operators (DSOs)
could use Alternative Connection Agreements (ACAs) to conditionally limit the
available connection capacity of market participants in the day-ahead stage. In
this paper, we present a novel Defender-Attacker-Defender game to investigate
the potential of this approach in distribution networks under load and price
uncertainty. We solve the resulting trilevel optimization model using a custom
branch-and-bound algorithm, and we demonstrate that it efficiently solves the
problem without exploring many nodes in the branch-and-bound search tree for
most simulated scenarios. The case study demonstrates that applying ACAs can
substantially lower redispatch costs (e.g. by 25%) for the DSO with only a
limited impact on FSP profits. The effectiveness of the approach critically
depends on how often the DSO can invoke ACAs and on the extent to which the DSO
can anticipate strategic bidding behavior of the FSP.

</details>


### [432] [Identification and optimal control strategies for the transversal splitting of ultra--cold Bose gases](https://arxiv.org/abs/2510.07113)
*Nikolaus Würkner,Yevhenii Kuriatnikov,Karthikeyan Kumaran,Marupaka Venkat Ramana,Jörg Schmiedmayer,Andreas Kugi,Maximilian Prüfer,Andreas Deutschmann-Olek*

Main category: eess.SY

TL;DR: 该研究将玻色-爱因斯坦凝聚体（BEC）分裂过程形式化为基于实验数据识别出的简化模型的最优前馈控制问题，并提出一种系统校准策略，结合最优实验选择和约束非线性参数估计，以最小的实验开销实现精确的系统辨识。在此基础上，研究通过间接最优控制计算能量最优轨迹，实现绝热路线图（STA），在抑制激发的同时，快速实现双阱势基态的跃迁。实验验证了该控制框架在多种配置下均能实现高保真状态转移，证明了其在量子控制应用中的鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 需要对玻色-爱因斯坦凝聚体（BEC）进行精确制备，以满足基础物理实验和量子技术应用的需求，这要求对凝聚体的非线性动力学进行快速、相干的控制。

Method: 将BEC分裂过程形式化为最优前馈控制问题，基于从实验数据识别出的简化模型。提出一种系统校准策略，结合最优实验选择和约束非线性参数估计，以实现最小的实验开销和精确的系统辨识。使用该校准模型，通过间接最优控制计算能量最优轨迹，实现绝热路线图（STA），以快速跃迁至双阱势的基态并抑制激发。

Result: 实验证实，所提出的控制框架在多种配置下均实现了高保真状态转移，证明了其鲁棒性和可扩展性。

Conclusion: 该研究提出的最优前馈控制方法和系统校准策略能够有效、精确地控制BEC分裂过程，实现快速高保真的状态转移，为量子控制应用提供了有力的工具。

Abstract: Splitting a Bose--Einstein condensate (BEC) is a key operation in fundamental
physics experiments and emerging quantum technologies, where precise
preparation of well--defined initial states requires fast yet coherent control
of the condensate's nonlinear dynamics. This work formulates the BEC splitting
process as an optimal feedforward control problem based on a physically
interpretable, reduced--order model identified from limited experimental data.
We introduce a systematic calibration strategy that combines optimal experiment
selection and constrained nonlinear parameter estimation, enabling accurate
system identification with minimal experimental overhead. Using this calibrated
model, we compute energy--optimal trajectories via indirect optimal control to
realize shortcuts to adiabaticity (STAs), achieving rapid transitions to the
ground state of a double--well potential while suppressing excitations.
Experiments confirm that the proposed control framework yields high--fidelity
state transfers across multiple configurations, demonstrating its robustness
and scalability for quantum control applications.

</details>


### [433] [Stability Preserving Safe Control of a Bicopter](https://arxiv.org/abs/2510.07145)
*Jhon Manuel Portella Delgado,Ankit Goel*

Main category: eess.SY

TL;DR: 该研究提出了一种用于多旋翼飞行器在安全约束下的稳定和轨迹跟踪控制律。


<details>
  <summary>Details</summary>
Motivation: 开发一种在保证安全约束条件下，实现多旋翼飞行器稳定和轨迹跟踪的控制方法。

Method: 通过状态依赖映射和Lyapunov函数将约束控制问题转化为无约束问题，从而显式地合成控制律，而不是在每一步求解约束优化问题。

Result: 仿真结果表明，该方法能够保证安全集的前向不变性，并实现平滑的跟踪性能，同时维持稳定性和性能。

Conclusion: 该方法成功地解决了多旋翼飞行器的安全约束控制问题，并能在仿真中验证其有效性。

Abstract: This paper presents a control law for stabilization and trajectory tracking
of a multicopter subject to safety constraints. The proposed approach
guarantees forward invariance of a prescribed safety set while ensuring smooth
tracking performance. Unlike conventional control barrier function methods, the
constrained control problem is transformed into an unconstrained one using
state-dependent mappings together with carefully constructed Lyapunov
functions. This approach enables explicit synthesis of the control law, instead
of requiring a solution of constrained optimization at each step. The
transformation also enables the controller to enforce safety without
sacrificing stability or performance. Simulation results for a polytopic
reference trajectory confined within a designated safe region demonstrate the
effectiveness of the proposed method.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [434] [DynBenchmark: Customizable Ground Truths to Benchmark Community Detection and Tracking in Temporal Networks](https://arxiv.org/abs/2510.06245)
*Laurent Brisson,Cécile Bothorel,Nicolas Duminy*

Main category: cs.SI

TL;DR: 提出了一种新的社区中心模型，用于生成可定制的演化社区结构，并生成潜在的时间网络，以评估社区检测算法。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试模型忽略了跟踪真实网络中社区演化的需求。

Method: 提出了一种新的社区中心模型，该模型可以生成可定制的演化社区结构，并且可以生成潜在的时间网络。该基准测试已用于测试三种方法，并测量它们在跟踪节点的聚类成员资格和检测社区演化方面的性能。

Result: 该模型已成功用于测试三种方法，并能够测量它们在跟踪节点的聚类成员资格和检测社区演化方面的性能。

Conclusion: 所提出的社区中心模型可以生成可定制的演化社区结构，并生成潜在的时间网络，以评估社区检测算法。

Abstract: Graph models help understand network dynamics and evolution. Creating graphs
with controlled topology and embedded partitions is a common strategy for
evaluating community detection algorithms. However, existing benchmarks often
overlook the need to track the evolution of communities in real-world networks.
To address this, a new community-centered model is proposed to generate
customizable evolving community structures where communities can grow, shrink,
merge, split, appear or disappear. This benchmark also generates the underlying
temporal network, where nodes can appear, disappear, or move between
communities. The benchmark has been used to test three methods, measuring their
performance in tracking nodes' cluster membership and detecting community
evolution. Python libraries, drawing utilities, and validation metrics are
provided to compare ground truth with algorithm results for detecting dynamic
communities.

</details>


### [435] [Unpacking Discourses on Childbirth and Parenthood in Popular Social Media Platforms Across China, Japan, and South Korea](https://arxiv.org/abs/2510.06788)
*Zheng Wei,Yunqi Li,Yucheng He,Yuelu Li,Xian Xu,Huamin Qu,Pan Hui,Muzhi Zhou*

Main category: cs.SI

TL;DR: 社交媒体使用与低生育愿望相关，本文分析了中国、韩国和日本抖音/TikTok上关于生育和育儿的219,127条评论，以检查在线话题和情绪。


<details>
  <summary>Details</summary>
Motivation: 了解人们在线上消费的关于生育和为人父母的论述，因为社交媒体使用与低生育愿望相关，但我们对此知之甚少。

Method: 使用BERTopic模型辅助主题分析，并应用大型语言模型QWen进行情绪标记，分析了中国、韩国和日本668个短视频的219,127条评论。

Result: 评论主要关注育儿成本（所有国家）、子女的效用（尤其是日本和韩国）以及个人主义（主要是中国）。抖音评论表现出最强烈的反生育情绪，而日本和韩国的评论则更为中性。短视频特征和地区社会经济指标会显著影响评论。本研究首次全面分析了经历低生育率地区通过算法驱动的视频分享平台进行的家庭形成在线论述。

Conclusion: 本研究为理解在线家庭价值观的传播做出了贡献，分析了经历低生育率的地区通过流行的算法驱动的视频分享平台进行的家庭形成在线论述。

Abstract: Social media use has been shown to be associated with low fertility desires.
However, we know little about the discourses surrounding childbirth and
parenthood that people consume online. We analyze 219,127 comments on 668 short
videos related to reproduction and parenthood from Douyin and Tiktok in China,
South Korea, and Japan, a region famous for its extremely low fertility level,
to examine the topics and sentiment expressed online. BERTopic model is used to
assist thematic analysis, and a large language model QWen is applied to label
sentiment. We find that comments focus on childrearing costs in all countries,
utility of children, particularly in Japan and South Korea, and individualism,
primarily in China. Comments from Douyin exhibit the strongest anti-natalist
sentiments, while the Japanese and Korean comments are more neutral. Short
video characteristics, such as their stances or account type, significantly
influence the responses, alongside regional socioeconomic indicators, including
GDP, urbanization, and population sex ratio. This work provides one of the
first comprehensive analyses of online discourses on family formation via
popular algorithm-fed video sharing platforms in regions experiencing low
fertility rates, making a valuable contribution to our understanding of the
spread of family values online.

</details>


### [436] [Visualization of Interpersonal Communication using Indoor Positioning Technology with UWB Tags](https://arxiv.org/abs/2510.06797)
*Hayato Shinto,Yu Ohki,Kenji Mizumoto,Kei Saito*

Main category: cs.SI

TL;DR: 通过UWB室内定位系统追踪了大学社交活动参与者的移动轨迹，分析了参与者之间的互动网络和社群演变，并探讨了距离阈值对结果的影响。研究证实了社群分析识别出的社群演变与观察到的参与者聚集情况基本吻合。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过追踪大学社交活动参与者的移动轨迹，可视化人际沟通，并分析社群的演变。

Method: 使用UWB室内定位系统追踪参与者约两小时的移动轨迹，并进行网络和社群分析，同时考察不同距离阈值的影响。

Result: 研究确认了社群分析识别出的社群演变与使用UWB室内定位系统观察到的参与者聚集情况在时间上大致吻合。

Conclusion: UWB室内定位系统结合社群分析能够有效追踪和理解大学社交活动中人际互动的动态演变。

Abstract: In conjunction with a social gathering held on a university campus, the
movement of attendees were tracked within the venue for approximately two hours
using a UWB indoor positioning system, in order to visualize their
interpersonal communication. Network and community analyses were performed on
attendee interaction data, and the evolution of communities over time was
further investigated through repeated community analysis at different time
points. Furthermore, recognizing the influence of distance thresholds on
defining contact, we discussed how varying these thresholds affected the
resulting network structure and community analysis outcomes. This study
confirmed that the temporal evolution of communities identified through
community analysis broadly corresponded with the visually observed groupings of
participants using the UWB indoor positioning system.

</details>


### [437] [Machines in the Crowd? Measuring the Footprint of Machine-Generated Text on Reddit](https://arxiv.org/abs/2510.07226)
*Lucio La Cava,Luca Maria Aiello,Andrea Tagarelli*

Main category: cs.SI

TL;DR: 本研究首次对Reddit上的机器生成文本（MGT）进行了大规模分析，发现MGT虽然整体占比不高，但在某些社区和时间段可达9%，且在技术知识和社交支持类社区更为普遍。MGT传达的社交信号与AI助手语言相似，但其参与度却能与人类文本相媲美，甚至在某些情况下更高，表明MGT正逐渐成为在线社交讨论的有机组成部分。


<details>
  <summary>Details</summary>
Motivation: 在线交流被生成式人工智能重塑，机器生成文本（MGT）的低成本大规模生产引发了对其在社交媒体环境中整合方式的未知。本研究旨在首次大规模探究MGT在Reddit上的特征。

Method: 利用最先进的统计方法检测MGT，分析了2022-2024年间跨越51个代表不同社区类型（如信息寻求、社交支持、讨论）的Reddit子版块的两年活动数据。

Result: MGT在Reddit上总体上占比较小，但估计最高可达9%，且在技术知识和社交支持类社区中更为普遍，并集中于少数用户。MGT传达的社交信号（如温暖感和地位感）与AI助手语言相似。尽管存在风格差异，MGT获得的参与度与人类文本相当，甚至更高。

Conclusion: MGT已成为Reddit在线社交讨论的有机组成部分，对平台治理、检测策略和社区动态的研究具有重要意义。

Abstract: Generative Artificial Intelligence is reshaping online communication by
enabling large-scale production of Machine-Generated Text (MGT) at low cost.
While its presence is rapidly growing across the Web, little is known about how
MGT integrates into social media environments. In this paper, we present the
first large-scale characterization of MGT on Reddit. Using a state-of-the-art
statistical method for detection of MGT, we analyze over two years of activity
(2022-2024) across 51 subreddits representative of Reddit's main community
types such as information seeking, social support, and discussion. We study the
concentration of MGT across communities and over time, and compared MGT to
human-authored text in terms of social signals it expresses and engagement it
receives. Our very conservative estimate of MGT prevalence indicates that
synthetic text is marginally present on Reddit, but it can reach peaks of up to
9% in some communities in some months. MGT is unevenly distributed across
communities, more prevalent in subreddits focused on technical knowledge and
social support, and often concentrated in the activity of a small fraction of
users. MGT also conveys distinct social signals of warmth and status giving
typical of language of AI assistants. Despite these stylistic differences, MGT
achieves engagement levels comparable than human-authored content and in a few
cases even higher, suggesting that AI-generated text is becoming an organic
component of online social discourse. This work offers the first perspective on
the MGT footprint on Reddit, paving the way for new investigations involving
platform governance, detection strategies, and community dynamics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [438] [Dynamic Regret Bounds for Online Omniprediction with Long Term Constraints](https://arxiv.org/abs/2510.07266)
*Yahav Bechavod,Jiuyao Lu,Aaron Roth*

Main category: cs.LG

TL;DR: 该论文提出了一种算法，为在线全预测问题提供了动态遗憾界限，并满足长期约束。


<details>
  <summary>Details</summary>
Motivation: 在线全预测问题旨在让学习者生成一系列预测，供多个下游决策者使用。每个决策者都有自己的效用函数和约束函数，学习者的目标是生成能使所有决策者在满足其效用和约束的前提下，最小化最坏情况约束违反的预测。

Method: 论文提出了一种算法，可以同时获得所有代理的动态遗憾保证，同时确保每个代理的约束违反消失。该算法不需要代理维护任何状态，它们只需要解决由当轮预测定义的一轮约束优化问题。

Result: 该算法首次实现了同时针对所有代理的动态遗憾保证，并确保了代理的约束违反消失。

Conclusion: 本研究成功地为在线全预测问题提供了一种满足长期约束的动态遗憾界限算法，并在不要求代理维护状态的情况下实现了代理的效用和约束的平衡。

Abstract: We present an algorithm guaranteeing dynamic regret bounds for online
omniprediction with long term constraints. The goal in this recently introduced
problem is for a learner to generate a sequence of predictions which are
broadcast to a collection of downstream decision makers. Each decision maker
has their own utility function, as well as a vector of constraint functions,
each mapping their actions and an adversarially selected state to reward or
constraint violation terms. The downstream decision makers select actions "as
if" the state predictions are correct, and the goal of the learner is to
produce predictions such that all downstream decision makers choose actions
that give them worst-case utility guarantees while minimizing worst-case
constraint violation. Within this framework, we give the first algorithm that
obtains simultaneous \emph{dynamic regret} guarantees for all of the agents --
where regret for each agent is measured against a potentially changing sequence
of actions across rounds of interaction, while also ensuring vanishing
constraint violation for each agent. Our results do not require the agents
themselves to maintain any state -- they only solve one-round constrained
optimization problems defined by the prediction made at that round.

</details>


### [439] [RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases](https://arxiv.org/abs/2510.06267)
*Khartik Uppalapati,Shakeel Abdulkareem,Bora Yimenicioglu*

Main category: cs.LG

TL;DR: RareGraph-Synth是一个知识引导的、连续时间扩散框架，用于生成真实且保护隐私的超罕见病合成电子健康记录（EHR）轨迹。它整合了五个公共资源到一个异构知识图中，并使用从中提取的元路径分数来指导生成过程，以提高生物学上的合理性，同时保持生成模型的稳定性。结果显示，与基线模型相比，RareGraph-Synth在降低分布差异和提高隐私保护方面表现出色，表明将生物医学知识图谱直接整合到扩散模型中可以提高数据保真度和隐私性，从而促进罕见病研究中的数据共享。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决超罕见病领域中合成电子健康记录（EHR）数据生成所面临的挑战，特别是如何在生成真实数据的同时保护患者隐私。现有的方法在生成具有生物学合理性的轨迹方面存在不足，并且可能无法提供足够的隐私保护，从而限制了数据的共享和利用。

Method: 该研究提出了一种名为RareGraph-Synth的知识引导的、连续时间扩散框架。该框架整合了Orphanet/Orphadata、HPO、GARD、PrimeKG和FAERS等五个公共资源，构建了一个包含约800万条边的异构知识图谱。通过从知识图谱中提取的元路径分数来调节前向随机微分方程中的每步噪声，以指导生成过程，使其生成具有生物学上合理性的实验室检查-用药-不良事件共现模式。反向去噪器则生成不包含受保护健康信息的、带有时间戳的实验室检查代码、药物代码和不良事件标志三元组序列。

Result: 在模拟的超罕见病队列上，RareGraph-Synth将分类最大均值差异（Maximum Mean Discrepancy）相比于无引导的扩散基线模型降低了40%，相比于GAN模型降低了60%以上，同时没有牺牲下游预测的效用。通过DOMIAS攻击者的黑盒成员推理评估显示，其AUROC值约为0.53，远低于0.55的安全释放阈值，并且显著优于非知识图谱基线模型（约0.61 ± 0.03），证明了其强大的抗重新识别能力。

Conclusion: 该研究结果表明，将生物医学知识图谱直接整合到扩散模型的噪声调度中，能够同时提高生成数据的保真度和隐私性。这为罕见病研究中的数据共享提供了更安全、更有效的方式。

Abstract: We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion
framework that generates realistic yet privacy-preserving synthetic
electronic-health-record (EHR) trajectories for ultra-rare diseases.
RareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human
Phenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA
Adverse Event Reporting System (FAERS) into a heterogeneous knowledge graph
comprising approximately 8 M typed edges. Meta-path scores extracted from this
8-million-edge KG modulate the per-token noise schedule in the forward
stochastic differential equation, steering generation toward biologically
plausible lab-medication-adverse-event co-occurrences while retaining
score-based diffusion model stability. The reverse denoiser then produces
timestamped sequences of lab-code, medication-code, and adverse-event-flag
triples that contain no protected health information. On simulated
ultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean
Discrepancy by 40 percent relative to an unguided diffusion baseline and by
greater than 60 percent versus GAN counterparts, without sacrificing downstream
predictive utility. A black-box membership-inference evaluation using the
DOMIAS attacker yields AUROC approximately 0.53, well below the 0.55
safe-release threshold and substantially better than the approximately 0.61
plus or minus 0.03 observed for non-KG baselines, demonstrating strong
resistance to re-identification. These results suggest that integrating
biomedical knowledge graphs directly into diffusion noise schedules can
simultaneously enhance fidelity and privacy, enabling safer data sharing for
rare-disease research.

</details>


### [440] [Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks](https://arxiv.org/abs/2510.06349)
*Moein E. Samadi,Andreas Schuppert*

Main category: cs.LG

TL;DR: AI在医疗等复杂动态领域仍需超越人类，现有模型面临维度灾难挑战，提出基于智能体网络的去中心化架构。


<details>
  <summary>Details</summary>
Motivation: 评估AI决策能力是否能超越人类，尤其是在医疗等复杂动态环境中，并探讨AI在该领域发展的挑战和方法。

Method: 提出一种去中心化的智能体网络（SANs）架构，作为替代单体模型来克服维度灾难，并利用群体学习（swarm learning）实现自适应决策。

Result: 基于现有研究和数学结果，论证SANs在动态环境中能实现优于单体模型的自适应决策，但可能牺牲部分可复现性。

Conclusion: AI在应对复杂动态系统方面仍有很大提升空间，去中心化SANs架构是一种有前景的解决方案，但需要在决策优越性和结果可复现性之间进行权衡。

Abstract: Foundation models have rapidly advanced AI, raising the question of whether
their decisions will ultimately surpass human strategies in real-world domains.
The exponential, and possibly super-exponential, pace of AI development makes
such analysis elusive. Nevertheless, many application areas that matter for
daily life and society show only modest gains so far; a prominent case is
diagnosing and treating dynamically evolving disease in intensive care.
  The common challenge is adapting complex systems to dynamic environments.
Effective strategies must optimize outcomes in systems composed of strongly
interacting functions while avoiding shared side effects; this requires
reliable, self-adaptive modeling. These tasks align with building digital twins
of highly complex systems whose mechanisms are not fully or quantitatively
understood. It is therefore essential to develop methods for self-adapting AI
models with minimal data and limited mechanistic knowledge. As this challenge
extends beyond medicine, AI should demonstrate clear superiority in these
settings before assuming broader decision-making roles.
  We identify the curse of dimensionality as a fundamental barrier to efficient
self-adaptation and argue that monolithic foundation models face conceptual
limits in overcoming it. As an alternative, we propose a decentralized
architecture of interacting small agent networks (SANs). We focus on agents
representing the specialized substructure of the system, where each agent
covers only a subset of the full system functions. Drawing on mathematical
results on the learning behavior of SANs and evidence from existing
applications, we argue that swarm-learning in diverse swarms can enable
self-adaptive SANs to deliver superior decision-making in dynamic environments
compared with monolithic foundation models, though at the cost of reduced
reproducibility in detail.

</details>


### [441] [MCCE: A Framework for Multi-LLM Collaborative Co-Evolution](https://arxiv.org/abs/2510.06270)
*Nian Ran,Zhongzheng Li,Yue Wang,Qingsong Ran,Xiaoyuan Zhang,Shikun Feng,Richard Allmendinger,Xiaoguang Zhao*

Main category: cs.LG

TL;DR: 该研究提出了一种名为多LLM协同进化（MCCE）的混合框架，结合了闭源大语言模型（LLM）和可训练的小型模型，以解决多目标离散优化问题，特别是在分子设计领域。


<details>
  <summary>Details</summary>
Motivation: 传统优化算法在处理高维度组合空间时面临局部最优陷阱问题，而专家知识对于加速收敛至关重要。LLM具有强大的先验知识和推理能力，但闭源LLM无法更新参数，而小型开源模型虽然可以微调但知识和推理能力有限。

Method: MCCE框架结合了一个固定的闭源LLM和一个轻量级可训练模型。该系统维护一个搜索过程的历史轨迹记忆，并通过强化学习逐步优化小型模型，使两个模型能够相互支持和补充，实现全局探索。这种协同进化过程通过相互启发来增强两个模型的能力，而非模型蒸馏。

Result: 在多目标药物设计基准测试中，MCCE实现了最先进的帕累托前沿质量，并且持续优于基线方法。

Conclusion: MCCE提出了一种新的混合LLM系统持续进化范式，结合了知识驱动的探索和经验驱动的学习，为解决多目标离散优化问题提供了一种有效的方法。

Abstract: Multi-objective discrete optimization problems, such as molecular design,
pose significant challenges due to their vast and unstructured combinatorial
spaces. Traditional evolutionary algorithms often get trapped in local optima,
while expert knowledge can provide crucial guidance for accelerating
convergence. Large language models (LLMs) offer powerful priors and reasoning
ability, making them natural optimizers when expert knowledge matters. However,
closed-source LLMs, though strong in exploration, cannot update their
parameters and thus cannot internalize experience. Conversely, smaller open
models can be continually fine-tuned but lack broad knowledge and reasoning
strength. We introduce Multi-LLM Collaborative Co-evolution (MCCE), a hybrid
framework that unites a frozen closed-source LLM with a lightweight trainable
model. The system maintains a trajectory memory of past search processes; the
small model is progressively refined via reinforcement learning, with the two
models jointly supporting and complementing each other in global exploration.
Unlike model distillation, this process enhances the capabilities of both
models through mutual inspiration. Experiments on multi-objective drug design
benchmarks show that MCCE achieves state-of-the-art Pareto front quality and
consistently outperforms baselines. These results highlight a new paradigm for
enabling continual evolution in hybrid LLM systems, combining knowledge-driven
exploration with experience-driven learning.

</details>


### [442] [Making and Evaluating Calibrated Forecasts](https://arxiv.org/abs/2510.06388)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Lunjia Hu*

Main category: cs.LG

TL;DR: 本研究提出了一种适用于多类别预测任务的完美诚实校准度量，并证明了其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 设计一个有意义的校准度量来评估预测器的失准水平，并解决现有校准度量（如分箱ECE）的非诚实和不鲁棒问题。

Method: 提出了一种适用于多类别预测任务的完美诚实校准度量，并研究了将二元校准度量扩展到多类别预测任务的方法，分析了哪些方法可以保持诚实性。通过数学证明和经验验证了该度量的鲁棒性。

Result: 提出了一种完美诚实的、鲁棒的、适用于多类别预测任务的校准度量，解决了现有分箱Ece度量的不鲁棒问题。

Conclusion: 本研究成功地将完美诚实校准度量从二元预测扩展到了多类别预测，并证明了其鲁棒性，解决了现有方法存在的问题。

Abstract: Calibrated predictions can be reliably interpreted as probabilities. An
important step towards achieving better calibration is to design an appropriate
calibration measure to meaningfully assess the miscalibration level of a
predictor. A recent line of work initiated by Haghtalab et al. [2024] studies
the design of truthful calibration measures: a truthful measure is minimized
when a predictor outputs the true probabilities, whereas a non-truthful measure
incentivizes the predictor to lie so as to appear more calibrated. All previous
calibration measures were non-truthful until Hartline et al. [2025] introduced
the first perfectly truthful calibration measures for binary prediction tasks
in the batch setting.
  We introduce a perfectly truthful calibration measure for multi-class
prediction tasks, generalizing the work of Hartline et al. [2025] beyond binary
prediction. We study common methods of extending calibration measures from
binary to multi-class prediction and identify ones that do or do not preserve
truthfulness. In addition to truthfulness, we mathematically prove and
empirically verify that our calibration measure exhibits superior robustness:
it robustly preserves the ordering between dominant and dominated predictors,
regardless of the choice of hyperparameters (bin sizes). This result addresses
the non-robustness issue of binned ECE, which has been observed repeatedly in
prior work.

</details>


### [443] [AutoBalance: An Automatic Balancing Framework for Training Physics-Informed Neural Networks](https://arxiv.org/abs/2510.06684)
*Kang An,Chenhao Si,Ming Yan,Shiqian Ma*

Main category: cs.LG

TL;DR: AutoBalance是一种新的训练范式，通过为每个损失分量分配独立的自适应优化器来解决物理信息神经网络 (PINNs) 训练中的挑战，并在事后聚合预处理后的更新，从而在求解偏微分方程 (PDE) 方面取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的物理信息神经网络 (PINNs) 训练方法通常采用“预组合”策略，在优化前操纵梯度，这会限制优化器处理具有不同曲率和冲突目标的多个损失项（如 PDE 残差和边界条件）的能力。我们认为这种方法从根本上是有限的，因为它迫使单个优化器处理具有光谱异质性的损失景观的梯度，从而破坏了其内部预处理。

Method: 我们提出了一种新颖的“后组合”训练范式，称为 AutoBalance。AutoBalance 为每个损失分量分配一个独立的自适应优化器，并在之后聚合由此产生的预处理更新。

Result: 在具有挑战性的 PDE 基准测试上的大量实验表明，AutoBalance 在求解误差（以 MSE 和 $L^{\infty}$ 范数衡量）方面持续优于现有框架，并实现了显著的降低。此外，AutoBalance 与其他流行的 PINN 方法正交且互补，可以增强它们在要求严苛的基准测试上的有效性。

Conclusion: AutoBalance 通过为每个损失分量分配独立的自适应优化器并在事后聚合更新，提供了一种更有效的 PINN 训练方法，从而在 PDE 求解方面取得了优于现有方法的性能。

Abstract: Physics-Informed Neural Networks (PINNs) provide a powerful and general
framework for solving Partial Differential Equations (PDEs) by embedding
physical laws into loss functions. However, training PINNs is notoriously
difficult due to the need to balance multiple loss terms, such as PDE residuals
and boundary conditions, which often have conflicting objectives and vastly
different curvatures. Existing methods address this issue by manipulating
gradients before optimization (a "pre-combine" strategy). We argue that this
approach is fundamentally limited, as forcing a single optimizer to process
gradients from spectrally heterogeneous loss landscapes disrupts its internal
preconditioning. In this work, we introduce AutoBalance, a novel "post-combine"
training paradigm. AutoBalance assigns an independent adaptive optimizer to
each loss component and aggregates the resulting preconditioned updates
afterwards. Extensive experiments on challenging PDE benchmarks show that
AutoBalance consistently outperforms existing frameworks, achieving significant
reductions in solution error, as measured by both the MSE and $L^{\infty}$
norms. Moreover, AutoBalance is orthogonal to and complementary with other
popular PINN methodologies, amplifying their effectiveness on demanding
benchmarks.

</details>


### [444] [The Effect of Label Noise on the Information Content of Neural Representations](https://arxiv.org/abs/2510.06401)
*Ali Hussaini Umar,Franky Kevin Nando Tezoh,Jean Barbier,Santiago Acevedo,Alessandro Laio*

Main category: cs.LG

TL;DR: 标签噪声对深度学习模型的影响已被广泛研究，但其对网络隐藏表示的影响仍不清楚。本研究使用信息失衡（条件互信息的计算高效代理）来比较隐藏表示，发现信息含量随网络参数数量呈双重下降趋势。在欠参数化的情况下，带噪声标签的表示比干净标签的表示信息更丰富，而在过参数化的情况下，两者信息丰富度相当。研究还表明，过参数化网络的表示对标签噪声具有鲁棒性。此外，在过参数化情况下，信息失衡随交叉熵损失的减小而减小，这为理解分类任务的泛化提供了新视角。在随机标签下，表示效果不如随机特征，表明网络权重会适应以编码标签信息。


<details>
  <summary>Details</summary>
Motivation: 研究标签噪声对深度学习模型隐藏表示的影响，填补现有研究空白。

Method: 使用信息失衡（条件互信息的代理）系统地比较隐藏表示。

Result: 1. 隐藏表示的信息含量随网络参数数量呈双重下降趋势。 2. 在欠参数化情况下，带噪声标签的表示比干净标签的表示信息更丰富；在过参数化情况下，两者信息丰富度相当。 3. 过参数化网络的表示对标签噪声具有鲁棒性。 4. 在过参数化情况下，信息失衡随交叉熵损失的减小而减小。 5. 随机标签下的表示效果不如随机特征。

Conclusion: 过参数化网络的表示对标签噪声具有鲁棒性，并且信息失衡随交叉熵损失的减小而在过参数化情况下减小，这为理解泛化提供了新视角。在随机标签下训练会使网络权重适应编码标签信息。

Abstract: In supervised classification tasks, models are trained to predict a label for
each data point. In real-world datasets, these labels are often noisy due to
annotation errors. While the impact of label noise on the performance of deep
learning models has been widely studied, its effects on the networks' hidden
representations remain poorly understood. We address this gap by systematically
comparing hidden representations using the Information Imbalance, a
computationally efficient proxy of conditional mutual information. Through this
analysis, we observe that the information content of the hidden representations
follows a double descent as a function of the number of network parameters,
akin to the behavior of the test error. We further demonstrate that in the
underparameterized regime, representations learned with noisy labels are more
informative than those learned with clean labels, while in the
overparameterized regime, these representations are equally informative. Our
results indicate that the representations of overparameterized networks are
robust to label noise. We also found that the information imbalance between the
penultimate and pre-softmax layers decreases with cross-entropy loss in the
overparameterized regime. This offers a new perspective on understanding
generalization in classification tasks. Extending our analysis to
representations learned from random labels, we show that these perform worse
than random features. This indicates that training on random labels drives
networks much beyond lazy learning, as weights adapt to encode labels
information.

</details>


### [445] [RVFL-X: A Novel Randomized Network Based on Complex Transformed Real-Valued Tabular Datasets](https://arxiv.org/abs/2510.06278)
*M. Sajid,Mushir Akhtar,A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: 本文提出了一种名为RVFL-X的复数域随机向量函数链路（RVFL）网络，以解决在随机神经网络（RNN）中有效利用复数表示的挑战。通过引入两种将实值数据转换为复值表示的方法，并将其集成到RVFL架构中，RVFL-X在80个UCI数据集上的实验结果表明，其性能优于原始RVFL和现有的RNN模型。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏将实值数据有效转换为复值表示的方法，目前在随机神经网络（RNN）中应用复数表示受到限制。本文旨在解决这一问题，以发挥复数表示在神经网络中的优越性。

Method: 提出两种将实值数据转换为复值表示的方法：一种自然转换方法和一种自动编码器驱动的方法。基于这些方法，提出了一种名为RVFL-X的复数域随机向量函数链路（RVFL）网络，该网络将复数转换集成到实值数据中，并保持了原始RVFL的简洁性和效率。RVFL-X利用复数输入、权重和激活函数来处理复数表示并产生实值输出。

Result: 在80个UCI数据集上进行的综合评估表明，RVFL-X在性能上始终优于原始RVFL和最先进（SOTA）的RNN变体，证明了其在不同应用领域的鲁棒性和有效性。

Conclusion: RVFL-X作为一种复数域扩展的RVFL网络，通过有效的实值到复值转换机制，成功克服了在RNN中应用复数表示的挑战，并在各种数据集上展现出优越的性能。

Abstract: Recent advancements in neural networks, supported by foundational theoretical
insights, emphasize the superior representational power of complex numbers.
However, their adoption in randomized neural networks (RNNs) has been limited
due to the lack of effective methods for transforming real-valued tabular
datasets into complex-valued representations. To address this limitation, we
propose two methods for generating complex-valued representations from
real-valued datasets: a natural transformation and an autoencoder-driven
method. Building on these mechanisms, we propose RVFL-X, a complex-valued
extension of the random vector functional link (RVFL) network. RVFL-X
integrates complex transformations into real-valued datasets while maintaining
the simplicity and efficiency of the original RVFL architecture. By leveraging
complex components such as input, weights, and activation functions, RVFL-X
processes complex representations and produces real-valued outputs.
Comprehensive evaluations on 80 real-valued UCI datasets demonstrate that
RVFL-X consistently outperforms both the original RVFL and state-of-the-art
(SOTA) RNN variants, showcasing its robustness and effectiveness across diverse
application domains.

</details>


### [446] [On knot detection via picture recognition](https://arxiv.org/abs/2510.06284)
*Anne Dranowski,Yura Kabkov,Daniel Tubbenhauer*

Main category: cs.LG

TL;DR: 使用机器学习和传统算法相结合的方法，可以从照片中识别绳结。


<details>
  <summary>Details</summary>
Motivation: 本文旨在实现一个能自动识别绳结照片的系统，最终目标是让手机能够仅通过一张照片就识别出绳结。

Method: 提出了一种结合现代机器学习方法（特别是卷积神经网络和用于图像识别的Transformer）与传统算法（用于计算琼斯多项式等量子不变量）的策略。通过实现预测交叉数的基线模型，展示了轻量级卷积神经网络和Transformer架构能够提取有用的结构信息。

Result: 已经实现了预测交叉数的基线模型，并证明了轻量级CNN和Transformer架构可以提取有用的结构信息。

Conclusion: 机器学习（处理带噪声的视觉数据）和不变量（强制执行严格的拓扑区分）的互补性，为实现目标提供了有希望的途径。

Abstract: Our goal is to one day take a photo of a knot and have a phone automatically
recognize it. In this expository work, we explain a strategy to approximate
this goal, using a mixture of modern machine learning methods (in particular
convolutional neural networks and transformers for image recognition) and
traditional algorithms (to compute quantum invariants like the Jones
polynomial). We present simple baselines that predict crossing number directly
from images, showing that even lightweight CNN and transformer architectures
can recover meaningful structural information. The longer-term aim is to
combine these perception modules with symbolic reconstruction into planar
diagram (PD) codes, enabling downstream invariant computation for robust knot
classification. This two-stage approach highlights the complementarity between
machine learning, which handles noisy visual data, and invariants, which
enforce rigorous topological distinctions.

</details>


### [447] [A Multi-Agent Framework for Stateful Inference-Time Search](https://arxiv.org/abs/2510.07147)
*Arshika Lalan,Rajat Ghosh,Aditya Kolsur,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 本研究提出了一种名为“stateful multi-agent evolutionary search”的训练框架，用于解决现有智能体推理方法在处理多步推理任务时因缺乏持久状态而导致的局限性，以及任务特定微调方法在面对复杂推理和长依赖任务时的脆弱性。该框架通过结合持久推理状态、对抗性变异和进化保持机制，有效提升了自动化单元测试生成（特别是边缘案例生成）的鲁棒性和覆盖率，并在多个基准测试和不同的大型语言模型家族上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于智能体的推理方法在多步推理任务中因缺乏持久状态而表现不佳，而任务特定的微调方法生成的代码虽然表面上能满足要求，但在需要深度推理和长依赖的任务上却很脆弱。本研究旨在克服这些局限性。

Method: 提出了一种名为“stateful multi-agent evolutionary search”的训练框架，该框架结合了（1）持久的推理状态、（2）对抗性变异和（3）进化保持机制。在自动化单元测试生成任务中，该框架通过一个进化搜索过程来生成鲁棒的边缘案例：专门的智能体依次提出、变异和评估候选单元测试用例。一个控制器在不同代之间维护持久状态，而进化保持机制则确保了所有可能情况下的多样性和探索性。

Result: 本研究提出的stateful multi-agent evolutionary search框架在自动化单元测试生成（特别是边缘案例生成）方面表现出色，相比于无状态的单步基线方法，在HumanEval和TestGenEvalMini等单元测试基准以及Llama、Gemma和GPT三个不同的LLM家族上，显著提高了覆盖率。

Conclusion: 结合持久推理状态和进化搜索能够显著改善单元测试的生成质量。本研究提出的stateful multi-agent evolutionary search框架是一种有效的、无需训练的方法，能够生成覆盖率高且鲁棒的边缘案例，适用于各种代码库。

Abstract: Recent work explores agentic inference-time techniques to perform structured,
multi-step reasoning. However, stateless inference often struggles on
multi-step tasks due to the absence of persistent state. Moreover,
task-specific fine-tuning or instruction-tuning often achieve surface-level
code generation but remain brittle on tasks requiring deeper reasoning and
long-horizon dependencies. To address these limitations, we propose stateful
multi-agent evolutionary search, a training-free framework that departs from
prior stateless approaches by combining (i) persistent inference-time state,
(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate
its effectiveness in automated unit test generation through the generation of
edge cases. We generate robust edge cases using an evolutionary search process,
where specialized agents sequentially propose, mutate, and score candidates. A
controller maintains persistent state across generations, while evolutionary
preservation ensures diversity and exploration across all possible cases. This
yields a generalist agent capable of discovering robust, high-coverage edge
cases across unseen codebases. Experiments show our stateful multi-agent
inference framework achieves substantial gains in coverage over stateless
single-step baselines, evaluated on prevalent unit-testing benchmarks such as
HumanEval and TestGenEvalMini and using three diverse LLM families - Llama,
Gemma, and GPT. These results indicate that combining persistent inference-time
state with evolutionary search materially improves unit-test generation.

</details>


### [448] [Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation](https://arxiv.org/abs/2510.06291)
*Zhiyang Zhang,Ningcong Chen,Xin Zhang,Yanhua Li,Shen Su,Hui Lu,Jun Luo*

Main category: cs.LG

TL;DR: Transformer模型在轨迹生成任务上优于基于卷积的模型，能生成更详细、更少偏差的轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积的扩散模型在生成轨迹时存在细节丢失和偏差问题，需要更优的模型来提高生成质量。

Method: 提出了一种名为Trajectory Transformer的新模型，该模型使用Transformer作为骨干网络，用于嵌入条件信息和预测噪声。探索了两种GPS坐标嵌入策略（位置嵌入和经纬度嵌入），并分析了不同尺度下的模型性能。

Result: 在两个真实世界数据集上的实验表明，Trajectory Transformer显著提高了生成质量，并有效减轻了先前方法中观察到的偏差问题。

Conclusion: Trajectory Transformer在轨迹生成方面取得了显著进展，克服了现有方法的局限性，能够生成更高质量、更精细的轨迹。

Abstract: The widespread use of GPS devices has driven advances in spatiotemporal data
mining, enabling machine learning models to simulate human decision making and
generate realistic trajectories, addressing both data collection costs and
privacy concerns. Recent studies have shown the promise of diffusion models for
high-quality trajectory generation. However, most existing methods rely on
convolution based architectures (e.g. UNet) to predict noise during the
diffusion process, which often results in notable deviations and the loss of
fine-grained street-level details due to limited model capacity. In this paper,
we propose Trajectory Transformer, a novel model that employs a transformer
backbone for both conditional information embedding and noise prediction. We
explore two GPS coordinate embedding strategies, location embedding and
longitude-latitude embedding, and analyze model performance at different
scales. Experiments on two real-world datasets demonstrate that Trajectory
Transformer significantly enhances generation quality and effectively
alleviates the deviation issues observed in prior approaches.

</details>


### [449] [BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression](https://arxiv.org/abs/2510.06293)
*Cristian Meo,Varun Sarathchandran,Avijit Majhi,Shao Hung,Carlo Saccardi,Ruben Imhoff,Roberto Deidda,Remko Uijlenhoet,Justin Dauwels*

Main category: cs.LG

TL;DR: BlockGPT是一种生成式自回归Transformer模型，采用批处理分块（Block）方法，可用于预测降水图。它通过在每个时间步预测整个二维场来解决现有方法的局限性，提高了准确性和推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前的短期降水预报（临近预报）模型在准确性和计算效率方面存在不足，例如基于token的自回归模型存在归纳偏置缺陷且推理缓慢，而扩散模型计算量大。

Method: BlockGPT模型采用了一种名为“Block”的批处理分块技术，并利用自注意力机制在帧内进行空间建模，利用因果注意力机制进行跨帧的时间建模，以实现空间和时间的解耦，从而进行视频预测。本研究将其应用于临近降水预报。

Result: 在KNMI和SEVIR两个数据集上的评估结果表明，BlockGPT在准确性、事件定位（通过分类指标衡量）方面优于现有的基于token的（NowcastingGPT）和基于扩散的（DiffCast+Phydnet）模型，并且推理速度比基线模型快31倍。

Conclusion: BlockGPT通过其创新的批处理分块方法，在降水临近预报任务中展现出优越的性能，能够实现高精度的预测，并显著提高计算效率，为实时应用提供了新的解决方案。

Abstract: Predicting precipitation maps is a highly complex spatiotemporal modeling
task, critical for mitigating the impacts of extreme weather events. Short-term
precipitation forecasting, or nowcasting, requires models that are not only
accurate but also computationally efficient for real-time applications. Current
methods, such as token-based autoregressive models, often suffer from flawed
inductive biases and slow inference, while diffusion models can be
computationally intensive. To address these limitations, we introduce BlockGPT,
a generative autoregressive transformer using batched tokenization (Block)
method that predicts full two-dimensional fields (frames) at each time step.
Conceived as a model-agnostic paradigm for video prediction, BlockGPT
factorizes space-time by using self-attention within each frame and causal
attention across frames; in this work, we instantiate it for precipitation
nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI
(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines
including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)
models. The results show that BlockGPT achieves superior accuracy, event
localization as measured by categorical metrics, and inference speeds up to 31x
faster than comparable baselines.

</details>


### [450] [SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation](https://arxiv.org/abs/2510.06303)
*Shuang Cheng,Yihan Bian,Dawei Liu,Yuhua Jiang,Yihao Liu,Linfeng Zhang,Wenhai Wang,Qipeng Guo,Kai Chen,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: SDAR是一种结合了自回归模型训练效率和扩散模型并行推理能力的模型。


<details>
  <summary>Details</summary>
Motivation: 将自回归模型的训练效率与扩散模型的并行推理能力相结合。

Method: SDAR将预训练的自回归模型转换为块状扩散模型，通过轻量级范式转换实现。在推理时，SDAR在块内通过离散扩散过程并行解码，在块间自回归生成以保证全局连贯性。

Result: SDAR在效率和性能上均表现出色，实现了与自回归模型相当的性能，同时实现了并行生成。实验表明，SDAR在科学推理基准测试中表现优于其自回归模型。

Conclusion: SDAR是一种实用的范式，结合了自回归和扩散模型的优点，实现了可扩展、高吞吐量的推理。

Abstract: We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies
the training efficiency of autoregressive models with the parallel inference
capability of diffusion. Instead of costly end-to-end diffusion training, SDAR
performs a lightweight paradigm conversion that transforms a well-trained
autoregressive (AR) model into a blockwise diffusion model through brief,
data-efficient adaptation. During inference, SDAR generates sequences
autoregressively across blocks for global coherence while decoding all tokens
within each block in parallel via a discrete diffusion process. Extensive
experiments show that AR models remain substantially more compute-efficient
than masked diffusion models, providing a strong foundation for adaptation.
Building on this insight, SDAR achieves efficient AR-to-diffusion conversion
with minimal cost, preserving AR-level performance while enabling parallel
generation. Scaling studies across dense and Mixture-of-Experts architectures
confirm that SDAR scales without compromise: larger models exhibit stronger
robustness to block size and decoding thresholds, yielding greater speedups
without accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning
and domain adaptability. Our 30B MoE model surpasses its AR counterpart on
challenging scientific reasoning benchmarks such as GPQA and ChemBench, and
gains further improvements under test-time scaling methods like majority voting
and pass@k. Together, these results establish SDAR as a practical paradigm that
combines the strengths of autoregression and diffusion for scalable,
high-throughput reasoning.

</details>


### [451] [Early wind turbine alarm prediction based on machine learning: AlarmForecasting](https://arxiv.org/abs/2510.06831)
*Syed Shazaib Shah,Daoliang Tan*

Main category: cs.LG

TL;DR: 该研究提出了一个报警预测与分类（AFC）框架，利用LSTM进行时间序列报警预测，并结合分类模块进行报警标记，以实现对风力涡轮机（WT）的预警和故障预防。


<details>
  <summary>Details</summary>
Motivation: 传统研究仅将风力涡轮机（WT）的报警数据用作诊断工具，而本研究旨在实现报警的预警和预防，以避免故障发生。

Method: 该研究提出的AFC框架包含两个连续模块：首先，基于长短期记忆（LSTM）的回归模块用于时间序列报警预测；其次，分类模块用于对预测的报警进行标记。

Result: 在为期5年的森兰MM82涡轮机案例研究中，对10分钟、20分钟和30分钟的报警预测准确率分别达到了82%、52%和41%。

Conclusion: 该研究证明了预测和预警报警的可行性，这对于减少报警频率和通过主动干预提高运营效率具有重要意义。

Abstract: Alarm data is pivotal in curbing fault behavior in Wind Turbines (WTs) and
forms the backbone for advancedpredictive monitoring systems. Traditionally,
research cohorts have been confined to utilizing alarm data solelyas a
diagnostic tool, merely indicative of unhealthy status. However, this study
aims to offer a transformativeleap towards preempting alarms, preventing alarms
from triggering altogether, and consequently avertingimpending failures. Our
proposed Alarm Forecasting and Classification (AFC) framework is designed on
twosuccessive modules: first, the regression module based on long short-term
memory (LSTM) for time-series alarmforecasting, and thereafter, the
classification module to implement alarm tagging on the forecasted alarm.
Thisway, the entire alarm taxonomy can be forecasted reliably rather than a few
specific alarms. 14 Senvion MM82turbines with an operational period of 5 years
are used as a case study; the results demonstrated 82%, 52%,and 41% accurate
forecasts for 10, 20, and 30 min alarm forecasts, respectively. The results
substantiateanticipating and averting alarms, which is significant in curbing
alarm frequency and enhancing operationalefficiency through proactive
intervention.

</details>


### [452] [Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks](https://arxiv.org/abs/2510.06444)
*Joel Pfeffer,J. M. Diederik Kruijssen,Clément Gossart,Mélanie Chevance,Diego Campo Millan,Florian Stecker,Steven N. Longmore*

Main category: cs.LG

TL;DR: 通过引入预测模型来改进去中心化学习网络的模型组合方法，使其能够预测模型性能并动态调整权重，从而提高网络推断的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的去中心化学习网络中的线性池化方法（如简单平均或动态权重更新）在组合模型预测时存在反应性不足的缺点，难以适应环境变化。

Method: 提出了一种利用机器学习预测各模型在每个时间步的性能，从而实现“上下文感知”的模型组合方法。该方法通过为更可能准确的模型分配更高权重来改进网络推断。

Result: 实验表明，在去中心化学习网络中加入性能预测工作器（特别是预测后悔或后悔z-score的模型）可以提高网络推断的准确性，优于仅预测损失的模型。同时，预测模型的性能对特征集和训练轮数的选择敏感，需要根据具体领域进行调整。

Conclusion: 性能预测在预测模型加权而非反应式模型加权的情况下，不仅适用于去中心化学习网络，也可能适用于其他需要此类加权的场景。

Abstract: In decentralized learning networks, predictions from many participants are
combined to generate a network inference. While many studies have demonstrated
performance benefits of combining multiple model predictions, existing
strategies using linear pooling methods (ranging from simple averaging to
dynamic weight updates) face a key limitation. Dynamic prediction combinations
that rely on historical performance to update weights are necessarily reactive.
Due to the need to average over a reasonable number of epochs (with moving
averages or exponential weighting), they tend to be slow to adjust to changing
circumstances (phase or regime changes). In this work, we develop a model that
uses machine learning to forecast the performance of predictions by models at
each epoch in a time series. This enables `context-awareness' by assigning
higher weight to models that are likely to be more accurate at a given time. We
show that adding a performance forecasting worker in a decentralized learning
network, following a design similar to the Allora network, can improve the
accuracy of network inferences. Specifically, we find forecasting models that
predict regret (performance relative to the network inference) or regret
z-score (performance relative to other workers) show greater improvement than
models predicting losses, which often do not outperform the naive network
inference (historically weighted average of all inferences). Through a series
of optimization tests, we show that the performance of the forecasting model
can be sensitive to choices in the feature set and number of training epochs.
These properties may depend on the exact problem and should be tailored to each
domain. Although initially designed for a decentralized learning network, using
performance forecasting for prediction combination may be useful in any
situation where predictive rather than reactive model weighting is needed.

</details>


### [453] [PIKAN: Physics-Inspired Kolmogorov-Arnold Networks for Explainable UAV Channel Modelling](https://arxiv.org/abs/2510.06355)
*Kürşat Tekbıyık,Güneş Karabulut Kurt,Antoine Lesage-Landry*

Main category: cs.LG

TL;DR: PIKAN是一种结合物理原理和深度学习的UAV信道模型，在保持高精度的同时，提高了模型的可解释性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的UAV通信信道模型要么过于僵化（确定性模型），要么缺乏可解释性（深度学习模型），无法满足对准确且可解释的A2G信道模型的需求，尤其是在变化的传播环境中。

Method: 提出了一种名为PIKAN（Physics-Inspired Kolmogorov-Arnold Network）的新模型，该模型将自由空间路径损耗、双射线反射等物理原理作为灵活的归纳偏置嵌入到学习过程中，与传统的PINN不同，PIKAN允许更灵活地应用物理信息。

Result: 实验数据显示，PIKAN在UAV A2G测量数据上实现了与深度学习模型相媲美的准确度，同时提供了符合传播定律的符号化、可解释的表达式。此外，PIKAN的参数量仅为232个，比基线MLP模型轻37倍，且不牺牲与测量值相关性，并提供了符号表达式。

Conclusion: PIKAN是UAV信道建模的有效、可解释且可扩展的解决方案，适用于beyond-5G和6G网络。

Abstract: Unmanned aerial vehicle (UAV) communications demand accurate yet
interpretable air-to-ground (A2G) channel models that can adapt to
nonstationary propagation environments. While deterministic models offer
interpretability and deep learning (DL) models provide accuracy, both
approaches suffer from either rigidity or a lack of explainability. To bridge
this gap, we propose the Physics-Inspired Kolmogorov-Arnold Network (PIKAN)
that embeds physical principles (e.g., free-space path loss, two-ray
reflections) into the learning process. Unlike physics-informed neural networks
(PINNs), PIKAN is more flexible for applying physical information because it
introduces them as flexible inductive biases. Thus, it enables a more flexible
training process. Experiments on UAV A2G measurement data show that PIKAN
achieves comparable accuracy to DL models while providing symbolic and
explainable expressions aligned with propagation laws. Remarkably, PIKAN
achieves this performance with only 232 parameters, making it up to 37 times
lighter than multilayer perceptron (MLP) baselines with thousands of
parameters, without sacrificing correlation with measurements and also
providing symbolic expressions. These results highlight PIKAN as an efficient,
interpretable, and scalable solution for UAV channel modelling in beyond-5G and
6G networks.

</details>


### [454] [Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics](https://arxiv.org/abs/2510.06367)
*Luca Wolf,Tobias Buck,Bjoern Malte Schaefer*

Main category: cs.LG

TL;DR:  the paper presents Helmholtz metrics to quantify the resemblance of ODE solutions to Euler-Lagrange equations and introduces Lagrangian neural ODEs for learning Euler-Lagrange equations.


<details>
  <summary>Details</summary>
Motivation: Not all solutions from neural ODEs are physical (i.e., Euler-Lagrange equations), and this paper aims to quantify and improve this. They propose Helmholtz metrics to measure resemblance to Euler-Lagrange equations and a new model, Lagrangian neural ODEs, to learn them directly.

Method: The paper introduces Helmholtz metrics to quantify the resemblance of ODE solutions to Euler-Lagrange equations. They combine these metrics with second-order neural ODEs to create Lagrangian neural ODEs, which can learn Euler-Lagrange equations directly without additional inference cost.

Result: The proposed Helmholtz metrics and Lagrangian neural ODEs demonstrate their capabilities on fundamental systems with noise. They show that Lagrangian neural ODEs can distinguish between Lagrangian and non-Lagrangian systems and improve neural ODE solutions using only positional data.

Conclusion: Lagrangian neural ODEs, empowered by Helmholtz metrics, offer a direct and efficient way to learn Euler-Lagrange equations from data, improving the physical interpretability and accuracy of neural ODE solutions.

Abstract: Neural ODEs are a widely used, powerful machine learning technique in
particular for physics. However, not every solution is physical in that it is
an Euler-Lagrange equation. We present Helmholtz metrics to quantify this
resemblance for a given ODE and demonstrate their capabilities on several
fundamental systems with noise. We combine them with a second order neural ODE
to form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equations
in a direct fashion and with zero additional inference cost. We demonstrate
that, using only positional data, they can distinguish Lagrangian and
non-Lagrangian systems and improve the neural ODE solutions.

</details>


### [455] [Vectorized FlashAttention with Low-cost Exponential Computation in RISC-V Vector Processors](https://arxiv.org/abs/2510.06834)
*Vasileios Titopoulos,Kosmas Alexandridis,Giorgos Dimitrakopoulos*

Main category: cs.LG

TL;DR: 本工作通过在RISC-V向量处理器上向量化FlashAttention算法，并利用低成本的指数函数近似和内存局部性优化策略，实现了Attention计算的加速。


<details>
  <summary>Details</summary>
Motivation: Attention机制在机器学习和人工智能模型中至关重要，但其计算成本高昂。本研究旨在加速Attention计算，特别是在基于RISC-V指令集架构（ISA）的向量处理器上。

Method: 本工作首先对FlashAttention算法进行向量化处理，以最小化标量代码并简化Softmax指数函数的计算。通过使用浮点运算中低成本的指数函数近似方法，避免了对基线向量ISA进行扩展。此外，还采用了合适的分块策略来提高内存局部性。

Result: 实验结果表明，向量化后的FlashAttention在处理实际应用中的Attention层时，性能得到了显著提升，并且该方法具有良好的可扩展性。

Conclusion: 本研究成功地在RISC-V向量处理器上实现了FlashAttention的向量化，通过引入计算成本较低的指数函数近似和优化内存访问，显著提高了Attention计算的效率和性能。

Abstract: Attention is a core operation in numerous machine learning and artificial
intelligence models. This work focuses on the acceleration of attention kernel
using FlashAttention algorithm, in vector processors, particularly those based
on the RISC-V instruction set architecture (ISA). This work represents the
first effort to vectorize FlashAttention, minimizing scalar code and
simplifying the computational complexity of evaluating exponentials needed by
softmax used in attention. By utilizing a low-cost approximation for
exponentials in floating-point arithmetic, we reduce the cost of computing the
exponential function without the need to extend baseline vector ISA with new
custom instructions. Also, appropriate tiling strategies are explored with the
goal to improve memory locality. Experimental results highlight the scalability
of our approach, demonstrating significant performance gains with the
vectorized implementations when processing attention layers in practical
applications.

</details>


### [456] [Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data](https://arxiv.org/abs/2510.06377)
*Rishabh Ranjan,Valter Hudovernik,Mark Znidar,Charilaos Kanatsoulis,Roshan Upendra,Mahmoud Mohammadi,Joe Meyer,Tom Palczewski,Carlos Guestrin,Jure Leskovec*

Main category: cs.LG

TL;DR: Relational Transformer (RT) 是一种可预训练的变换器架构，能够处理多样化的关系数据，并在零样本场景下实现跨数据集和任务的迁移，性能优于大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 关系型数据领域缺乏能够跨数据集和任务迁移的通用架构，因为关系型数据具有多样化的模式、图结构和函数依赖性。

Method: RT架构通过（i）结合表/列元数据对单元格进行标记化，（ii）通过掩码标记预测进行预训练，以及（iii）利用新颖的关系注意力机制来处理列、行和主外键链接。

Result: RT在RelBench数据集上进行了预训练，涵盖了诸如客户流失和销售预测等任务。在零样本设置下，RT在二元分类任务上的平均AUROC达到了全监督方法的94%，优于参数量为27B的大型语言模型（LLM）的84%。

Conclusion: RT为关系型数据的基础模型提供了一条实用的途径，其零样本迁移能力得益于任务-表上下文、关系注意力模式和模式语义的利用。

Abstract: Pretrained transformers readily adapt to new sequence modeling tasks via
zero-shot prompting, but relational domains still lack architectures that
transfer across datasets and tasks. The core challenge is the diversity of
relational data, with varying heterogeneous schemas, graph structures and
functional dependencies. In this paper, we present the Relational Transformer
(RT) architecture, which can be pretrained on diverse relational databases and
directly applied to unseen datasets and tasks without task- or dataset-specific
fine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with
table/column metadata, (ii) is pretrained via masked token prediction, and
(iii) utilizes a novel \textit{Relational Attention} mechanism over columns,
rows, and primary-foreign key links. Pretrained on RelBench datasets spanning
tasks such as churn and sales forecasting, RT attains strong zero-shot
performance, averaging 94% of fully supervised AUROC on binary classification
tasks with a single forward pass of a 22M parameter model, as opposed to 84%
for a 27B LLM. Fine-tuning yields state-of-the-art results with high sample
efficiency. Our experiments show that RT's zero-shot transfer harnesses
task-table context, relational attention patterns and schema semantics.
Overall, RT provides a practical path toward foundation models for relational
data.

</details>


### [457] [Monte Carlo Permutation Search](https://arxiv.org/abs/2510.06381)
*Tristan Cazenave*

Main category: cs.LG

TL;DR: MCPS 是一种通用的 MCTS 算法，在计算能力有限的情况下优于 GRAVE，并在各种游戏中表现更好。


<details>
  <summary>Details</summary>
Motivation: 当深度强化学习不可行或可用计算能力有限时（例如在通用游戏编程中），需要一种改进的 MCTS 算法。

Method: MCPS 算法将从根到节点的路径中所有走法的统计数据纳入节点的探索项，并使用改进的权重公式来组合三种统计数据源，无需 GRAVE 的偏差超参数。

Result: 在所有双人游戏中，MCPS 的表现优于 GRAVE；在多人游戏中，MCPS 的表现与 GRAVE 相当。使用抽象走码而不是精确走码可以改善排列和 AMAF 统计数据。

Conclusion: MCPS 是一种有效的 MCTS 算法，尤其适用于计算资源受限的场景，并且在各种游戏中均优于 GRAVE。使用抽象走码和改进的权重公式可以进一步提升其性能。

Abstract: We propose Monte Carlo Permutation Search (MCPS), a general-purpose Monte
Carlo Tree Search (MCTS) algorithm that improves upon the GRAVE algorithm. MCPS
is relevant when deep reinforcement learning is not an option, or when the
computing power available before play is not substantial, such as in General
Game Playing, for example. The principle of MCPS is to include in the
exploration term of a node the statistics on all the playouts that contain all
the moves on the path from the root to the node. We extensively test MCPS on a
variety of games: board games, wargame, investment game, video game and
multi-player games. MCPS has better results than GRAVE in all the two-player
games. It has equivalent results for multi-player games because these games are
inherently balanced even when players have different strengths. We also show
that using abstract codes for moves instead of exact codes can be beneficial to
both MCPS and GRAVE, as they improve the permutation statistics and the AMAF
statistics. We also provide a mathematical derivation of the formulas used for
weighting the three sources of statistics. These formulas are an improvement on
the GRAVE formula since they no longer use the bias hyperparameter of GRAVE.
Moreover, MCPS is not sensitive to the ref hyperparameter.

</details>


### [458] [DPMM-CFL: Clustered Federated Learning via Dirichlet Process Mixture Model Nonparametric Clustering](https://arxiv.org/abs/2510.07132)
*Mariona Jaramillo-Civill,Peng Wu,Pau Closas*

Main category: cs.LG

TL;DR: DPMM-CFL 是一种非参数贝叶斯聚类联邦学习算法，可以自动推断聚类数量和客户端分配，从而解决传统 CFL 方法中预先固定聚类数量 K 的问题。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的聚类联邦学习（CFL）方法都需要预先固定聚类数量 K，这在潜在结构未知的情况下是不切实际的。

Method: DPMM-CFL 算法在聚类参数的分布上放置了一个狄利克雷过程（DP）先验，从而能够进行非参数贝叶斯推断，以联合推断聚类数量和客户端分配，同时优化每个聚类的联邦目标。在每个通信轮次中，联邦更新和聚类推断是耦合的。

Result: 在 Dirichlet 和类分割非独立同分布（non-IID）分区下，DPMM-CFL 在基准数据集上得到了验证。

Conclusion: DPMM-CFL 能够有效地在联合推断聚类数量和客户端分配的同时，优化每个聚类的联邦目标，从而在非独立同分布（non-IID）客户端异质性下提高 CFL 算法的性能。

Abstract: Clustered Federated Learning (CFL) improves performance under non-IID client
heterogeneity by clustering clients and training one model per cluster, thereby
balancing between a global model and fully personalized models. However, most
CFL methods require the number of clusters K to be fixed a priori, which is
impractical when the latent structure is unknown. We propose DPMM-CFL, a CFL
algorithm that places a Dirichlet Process (DP) prior over the distribution of
cluster parameters. This enables nonparametric Bayesian inference to jointly
infer both the number of clusters and client assignments, while optimizing
per-cluster federated objectives. This results in a method where, at each
round, federated updates and cluster inferences are coupled, as presented in
this paper. The algorithm is validated on benchmark datasets under Dirichlet
and class-split non-IID partitions.

</details>


### [459] [Chem-NMF: Multi-layer $α$-divergence Non-Negative Matrix Factorization for Cardiorespiratory Disease Clustering, with Improved Convergence Inspired by Chemical Catalysts and Rigorous Asymptotic Analysis](https://arxiv.org/abs/2510.06632)
*Yasaman Torabi,Shahram Shirani,James P. Reilly*

Main category: cs.LG

TL;DR: 本研究提出了一种名为Chem-NMF的新方法，该方法结合了化学反应中的玻尔兹曼概率来分析非负矩阵分解（NMF）算法的收敛性，并通过引入边界因子来稳定收敛过程。实验结果表明，该方法在生物医学信号和人脸图像的聚类准确性方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的非负矩阵分解（NMF）及其基于α-散度扩展的方法在多层结构中存在收敛性保证的挑战。

Method: 提出了一种名为Chem-NMF的新方法，借鉴了化学反应中能量势垒的玻尔兹曼概率来分析NMF的收敛性，并通过引入边界因子来稳定收敛。

Result: 实验结果显示，与现有方法相比，Chem-NMF在生物医学信号上的聚类准确性提高了5.6% ± 2.7%，在人脸图像上提高了11.1% ± 7.2%。

Conclusion: Chem-NMF是首个从物理化学角度严格分析NMF收敛行为的方法，其改进的收敛稳定性和提高的聚类准确性得到了实验验证。

Abstract: Non-Negative Matrix Factorization (NMF) is an unsupervised learning method
offering low-rank representations across various domains such as audio
processing, biomedical signal analysis, and image recognition. The
incorporation of $\alpha$-divergence in NMF formulations enhances flexibility
in optimization, yet extending these methods to multi-layer architectures
presents challenges in ensuring convergence. To address this, we introduce a
novel approach inspired by the Boltzmann probability of the energy barriers in
chemical reactions to theoretically perform convergence analysis. We introduce
a novel method, called Chem-NMF, with a bounding factor which stabilizes
convergence. To our knowledge, this is the first study to apply a physical
chemistry perspective to rigorously analyze the convergence behaviour of the
NMF algorithm. We start from mathematically proven asymptotic convergence
results and then show how they apply to real data. Experimental results
demonstrate that the proposed algorithm improves clustering accuracy by 5.6%
$\pm$ 2.7% on biomedical signals and 11.1% $\pm$ 7.2% on face images (mean
$\pm$ std).

</details>


### [460] [Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings](https://arxiv.org/abs/2510.06397)
*Ali Baheri*

Main category: cs.LG

TL;DR: 非欧几里得几何中的模型存在边界驱动的攻击漏洞，后门触发器可以利用这种漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究非欧几里得几何（如双曲几何）在基础模型中的应用所带来的边界效应，以及这种效应如何被后门触发器利用。

Method: 分析边界效应如何导致输入空间中的微小变化在模型表示空间中产生不成比例的巨大偏移，并提出一种几何自适应触发器。

Result: 攻击成功率随边界的接近而增加，而传统检测器则会减弱。实验结果与理论趋势一致。

Conclusion: 非欧几里得模型存在特定于几何的脆弱性，为设计和理解防御机制的局限性提供了分析支持。

Abstract: Non-Euclidean foundation models increasingly place representations in curved
spaces such as hyperbolic geometry. We show that this geometry creates a
boundary-driven asymmetry that backdoor triggers can exploit. Near the
boundary, small input changes appear subtle to standard input-space detectors
but produce disproportionately large shifts in the model's representation
space. Our analysis formalizes this effect and also reveals a limitation for
defenses: methods that act by pulling points inward along the radius can
suppress such triggers, but only by sacrificing useful model sensitivity in
that same direction. Building on these insights, we propose a simple
geometry-adaptive trigger and evaluate it across tasks and architectures.
Empirically, attack success increases toward the boundary, whereas conventional
detectors weaken, mirroring the theoretical trends. Together, these results
surface a geometry-specific vulnerability in non-Euclidean models and offer
analysis-backed guidance for designing and understanding the limits of
defenses.

</details>


### [461] [Test-Time Efficient Pretrained Model Portfolios for Time Series Forecasting](https://arxiv.org/abs/2510.06419)
*Mert Kayaalp,Caner Turkmen,Oleksandr Shchur,Pedro Mercado,Abdul Fatir Ansari,Michael Bohlke-Schneider,Bernie Wang*

Main category: cs.LG

TL;DR: smaller, pretrained forecasting models trained in portfolio achieve competitive performance with fewer parameters compared to monolithic models, outperforming generalists with specialist models.


<details>
  <summary>Details</summary>
Motivation: explore an alternative to training a single, large monolithic model by building a portfolio of smaller, pretrained forecasting models to answer whether bigger is always better for time series foundation models.

Method: applying ensembling or model selection over portfolios of smaller, pretrained forecasting models, exploring strategies for designing such portfolios, and demonstrating that post-training a base model is a compute-effective approach for creating diverse specialists.

Result: competitive performance on large-scale benchmarks using much fewer parameters, collections of specialist models consistently outperform portfolios of independently trained generalists, and ensembling and model selection are more compute-efficient than test-time fine-tuning.

Conclusion: collections of specialist models in a portfolio, created through methods like post-training a base model, offer a compute-efficient and effective alternative to large monolithic models for time series forecasting, outperforming generalist models and being more efficient than test-time fine-tuning.

Abstract: Is bigger always better for time series foundation models? With the question
in mind, we explore an alternative to training a single, large monolithic
model: building a portfolio of smaller, pretrained forecasting models. By
applying ensembling or model selection over these portfolios, we achieve
competitive performance on large-scale benchmarks using much fewer parameters.
We explore strategies for designing such portfolios and find that collections
of specialist models consistently outperform portfolios of independently
trained generalists. Remarkably, we demonstrate that post-training a base model
is a compute-effective approach for creating sufficiently diverse specialists,
and provide evidences that ensembling and model selection are more
compute-efficient than test-time fine-tuning.

</details>


### [462] [Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization](https://arxiv.org/abs/2510.06434)
*Eliot Shekhtman,Yichen Zhou,Ingvar Ziemann,Nikolai Matni,Stephen Tu*

Main category: cs.LG

TL;DR: 本研究提出了一种在多轨迹设置下进行实例最优学习的方法，通过Hellinger局部化框架，为最大似然估计提供了更广泛的理论保证，并在多种模型上取得了接近渐近最优的边界。


<details>
  <summary>Details</summary>
Motivation: 现有的顺序学习理论在多轨迹（多个独立实现的时间索引随机过程）设置下不完整，尤其是在没有典型混合假设的情况下，这限制了对现代训练流程（如大型基础模型）的理解和学习能力。现有的实例最优界限仅限于特定的情况，对于更通用的模型或损失函数，要么需要向独立同分布（i.i.d.）学习进行归约，要么依赖于单轨迹结果，这两种情况下的有效样本量扩展都受到限制。

Method: 本研究利用Hellinger局部化框架，这是一种通用的最大似然估计方法。具体来说，首先通过归约到i.i.d.学习来控制路径度量下的平方Hellinger距离，然后通过以轨迹Fisher信息为权重的参数空间二次型进行局部化。这种方法能够实现实例最优界限，并在广泛的条件下与整个数据预算规模匹配。

Result: 该框架在四个不同的案例研究中得到了验证：简单的马尔可夫链混合、非高斯噪声下的相关线性回归、具有非单调激活的广义线性模型以及线性注意力序列模型。在所有情况下，本研究提出的界限都接近渐近正态性下的实例最优界限，显著优于标准的归约方法。

Conclusion: Hellinger局部化框架能够显著扩展多轨迹设置下的实例最优界限，为最大似然估计提供了一种通用的、有效的学习方法，并在多个实际应用场景中表现出优越性。

Abstract: Learning from temporally-correlated data is a core facet of modern machine
learning. Yet our understanding of sequential learning remains incomplete,
particularly in the multi-trajectory setting where data consists of many
independent realizations of a time-indexed stochastic process. This important
regime both reflects modern training pipelines such as for large foundation
models, and offers the potential for learning without the typical mixing
assumptions made in the single-trajectory case. However, instance-optimal
bounds are known only for least-squares regression with dependent covariates;
for more general models or loss functions, the only broadly applicable
guarantees result from a reduction to either i.i.d. learning, with effective
sample size scaling only in the number of trajectories, or an existing
single-trajectory result when each individual trajectory mixes, with effective
sample size scaling as the full data budget deflated by the mixing-time.
  In this work, we significantly broaden the scope of instance-optimal rates in
multi-trajectory settings via the Hellinger localization framework, a general
approach for maximum likelihood estimation. Our method proceeds by first
controlling the squared Hellinger distance at the path-measure level via a
reduction to i.i.d. learning, followed by localization as a quadratic form in
parameter space weighted by the trajectory Fisher information. This yields
instance-optimal bounds that scale with the full data budget under a broad set
of conditions. We instantiate our framework across four diverse case studies: a
simple mixture of Markov chains, dependent linear regression under non-Gaussian
noise, generalized linear models with non-monotonic activations, and
linear-attention sequence models. In all cases, our bounds nearly match the
instance-optimal rates from asymptotic normality, substantially improving over
standard reductions.

</details>


### [463] [Bayesian Optimization under Uncertainty for Training a Scale Parameter in Stochastic Models](https://arxiv.org/abs/2510.06439)
*Akash Yadav,Ruda Zhang*

Main category: cs.LG

TL;DR: 该研究提出了一种新颖的贝叶斯优化框架，用于在存在不确定性的情况下进行超参数调整，特别关注随机模型中的尺度或精度参数。该方法通过使用统计替代模型来处理随机变量，从而可以对期望算子进行解析评估，并推导出随机获取函数的优化器的闭式表达式，从而大大降低了计算成本。与传统的基于蒙特卡洛的优化方案相比，该方法所需数据点减少了40倍，计算成本降低了40倍。该方法的有效性已通过计算工程领域的两个数值算例得到证明。


<details>
  <summary>Details</summary>
Motivation: 超参数调整在存在不确定性的情况下是一个具有挑战性的问题，尤其是在函数评估存在噪声时，这使得不确定性下的优化计算成本高昂。

Method: 提出了一种新颖的贝叶斯优化框架，用于在存在不确定性的情况下进行超参数调整，重点关注随机模型中的尺度或精度参数。该方法使用统计替代模型来处理随机变量，允许对期望算子进行解析评估，并推导出随机获取函数的优化器的闭式表达式。

Result: 与传统的基于一维蒙特卡洛的优化方案相比，所提出的方法需要的数据点减少了40倍，计算成本降低了40倍。

Conclusion: 所提出的贝叶斯优化框架通过统计替代模型和闭式表达式，有效地降低了在不确定性下进行超参数调整的计算成本，并在计算工程领域的数值算例中得到了验证。

Abstract: Hyperparameter tuning is a challenging problem especially when the system
itself involves uncertainty. Due to noisy function evaluations, optimization
under uncertainty can be computationally expensive. In this paper, we present a
novel Bayesian optimization framework tailored for hyperparameter tuning under
uncertainty, with a focus on optimizing a scale- or precision-type parameter in
stochastic models. The proposed method employs a statistical surrogate for the
underlying random variable, enabling analytical evaluation of the expectation
operator. Moreover, we derive a closed-form expression for the optimizer of the
random acquisition function, which significantly reduces computational cost per
iteration. Compared with a conventional one-dimensional Monte Carlo-based
optimization scheme, the proposed approach requires 40 times fewer data points,
resulting in up to a 40-fold reduction in computational cost. We demonstrate
the effectiveness of the proposed method through two numerical examples in
computational engineering.

</details>


### [464] [How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation](https://arxiv.org/abs/2510.06448)
*Prabhant Singh,Sibylle Hess,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 现有的迁移性评估指标的基准存在缺陷，导致对指标性能的评估不准确。


<details>
  <summary>Details</summary>
Motivation: 评估迁移性评估指标的基准的研究不足。

Method: 通过实证分析，展示了广泛使用的基准设置的局限性，并揭示了模型空间不切实际和静态性能等级如何人为地夸大了现有指标的感知性能。

Result: 简单的、与数据集无关的启发式方法可以超越复杂的模型选择方法。

Conclusion: 当前的评估协议与实际的模型选择复杂性之间存在严重脱节。为了解决这个问题，我们提出了构建更强大、更现实的基准的具体建议，以指导未来的研究朝着更有意义的方向发展。

Abstract: Transferability estimation metrics are used to find a high-performing
pre-trained model for a given target task without fine-tuning models and
without access to the source dataset. Despite the growing interest in
developing such metrics, the benchmarks used to measure their progress have
gone largely unexamined. In this work, we empirically show the shortcomings of
widely used benchmark setups to evaluate transferability estimation metrics. We
argue that the benchmarks on which these metrics are evaluated are
fundamentally flawed. We empirically demonstrate that their unrealistic model
spaces and static performance hierarchies artificially inflate the perceived
performance of existing metrics, to the point where simple, dataset-agnostic
heuristics can outperform sophisticated methods. Our analysis reveals a
critical disconnect between current evaluation protocols and the complexities
of real-world model selection. To address this, we provide concrete
recommendations for constructing more robust and realistic benchmarks to guide
future research in a more meaningful direction.

</details>


### [465] [Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin](https://arxiv.org/abs/2510.06477)
*Enrique Queipo-de-Llano,Álvaro Arroyo,Federico Barbero,Xiaowen Dong,Michael Bronstein,Yann LeCun,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: Attention sinks 和 compression valleys 均源于残差流中大规模激活的形成，该模型提出了 Mix-Compress-Refine 理论来解释信息流。


<details>
  <summary>Details</summary>
Motivation: Attention sinks 和 compression valleys 是大型语言模型中两个独立研究的令人费解的现象，本文旨在揭示它们之间的联系。

Method: 通过理论证明大规模激活必然导致表示压缩和熵减少，并通过在不同参数规模的模型上进行实验来验证这一理论，同时进行有针对性的消融研究。

Result: 证明了大规模激活确实会导致压缩和注意力汇聚，并提出了 Mix-Compress-Refine 理论，该理论将 LLM 的计算过程分为三个阶段：早期层的广泛混合、中间层的压缩计算和晚期层的选择性优化。

Conclusion: Mix-Compress-Refine 理论提供了一个统一的框架来理解 LLM 的信息流和计算组织方式，解释了不同层级表示的差异以及嵌入任务和生成任务的表现差异。

Abstract: Attention sinks and compression valleys have attracted significant attention
as two puzzling phenomena in large language models, but have been studied in
isolation. In this work, we present a surprising connection between attention
sinks and compression valleys, tracing both to the formation of massive
activations in the residual stream. We prove theoretically that massive
activations necessarily produce representational compression and establish
bounds on the resulting entropy reduction. Through experiments across several
models (410M-120B parameters), we confirm that when the beginning-of-sequence
token develops extreme activation norms in the middle layers, both compression
valleys and attention sinks emerge simultaneously. Targeted ablation studies
validate our theoretical predictions. This unified view motivates us to propose
the Mix-Compress-Refine theory of information flow, as an attempt to explain
how LLMs organize their computation in depth by controlling attention and
representational compression via massive activations. Specifically, we posit
that Transformer-based LLMs process tokens in three distinct phases: (1) broad
mixing in the early layers, (2) compressed computation with limited mixing in
the middle layers, and (3) selective refinement in the late layers. Our
framework helps explain why embedding tasks perform best at intermediate
layers, whereas generation tasks benefit from full-depth processing, clarifying
differences in task-dependent representations.

</details>


### [466] [Valid Stopping for LLM Generation via Empirical Dynamic Formal Lift](https://arxiv.org/abs/2510.06478)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.LG

TL;DR: EDFL是一种用于语言模型生成的序贯测试方法，可减少生成量并保持错误控制。


<details>
  <summary>Details</summary>
Motivation: EDFL旨在解决语言模型生成停止问题，通过序贯测试来提高效率和控制错误。

Method: EDFL采用随时有效的序贯测试，跟踪信息增益（完整模型与骨干模型之间的对数似然比），并使用自归一化经验伯恩斯坦e-过程来提供正式的delta级错误控制。它还处理未知中心化、组合多个参数以及支持分布漂移下的自适应重置。

Result: 在六个基准测试中，EDFL与序贯基线相比，生成量减少了22-28%，同时保持了delta级控制，计算开销为12%。它还引入了自动骨干，并证明了跨骨干家族的鲁棒性。结合轻量级正确性门控（句子边界+验证器）可以提高最终任务的正确性，同时仅通过延迟停止来保持随时有效的保证。EDFL作为第一阶段过滤器，将验证负担减少了83%。

Conclusion: EDFL是一种有效的语言模型生成停止方法，可以显著减少生成量并控制错误，但它不适用于安全关键领域，因为它不能保证事实正确性。

Abstract: We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), applying
anytime-valid sequential testing to language model generation stopping. Our
approach tracks information lift -- the log-likelihood ratio between full
models and deliberately weakened "skeleton" baselines -- using self-normalized
empirical-Bernstein e-processes that provide formal delta-level error control
regardless of stopping time. We handle unknown centering through online mean
estimation, combine multiple parameters via mixture e-processes, and support
adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL
reduces generation by 22-28% vs. sequential baselines while maintaining
delta-level control with 12% computational overhead. We introduce automated
skeletons (distilled submodels, randomized logits) and show robustness across
skeleton families. Composing EDFL with a lightweight correctness gate (sentence
boundaries + verifier) improves end-task correctness while preserving
anytime-valid guarantees by only delaying stopping. Our certificates control
information sufficiency, not factual correctness -- 10.9% of stopped sequences
remain incorrect even with the gate (13.2-22.7% without it). EDFL serves as a
first-stage filter reducing verification burden by 83%, not as a standalone
solution for safety-critical domains.

</details>


### [467] [GUIDE: Guided Initialization and Distillation of Embeddings](https://arxiv.org/abs/2510.06502)
*Khoa Trinh,Gaurav Menghani,Erik Vee*

Main category: cs.LG

TL;DR: 通过匹配教师模型的参数空间来改进学生模型，可以免费获得显著的模型质量提升。


<details>
  <summary>Details</summary>
Motivation: 标准蒸馏方法仅限于让学生模型匹配教师模型的输出来学习，这限制了从教师模型中提取有用信息的程度。我们认为应该从教师模型中提取比仅仅匹配输出更有用的信息。

Method: 提出了一种名为"GUIDE"（Guided Initialization and Distillation of Embeddings）的技术，它是一种蒸馏技术，迫使学生模型在参数空间中匹配教师模型。

Result: 使用GUIDE技术，在拥有大规模学生模型（4亿-10亿参数）并在约200亿个标记上进行训练时，可以减少25-26%的教师-学生模型质量差距。GUIDE可以与知识蒸馏结合使用，并取得近乎加性的改进。单独使用GUIDE比单独使用知识蒸馏能获得更好的模型质量。

Conclusion: GUIDE技术不增加训练或推理开销，因此所带来的模型质量提升是免费的。

Abstract: Algorithmic efficiency techniques such as distillation
(\cite{hinton2015distillation}) are useful in improving model quality without
increasing serving costs, provided a larger teacher model is available for a
smaller student model to learn from during training. Standard distillation
methods are limited to only forcing the student to match the teacher's outputs.
Given the costs associated with training a large model, we believe we should be
extracting more useful information from a teacher model than by just making the
student match the teacher's outputs.
  In this paper, we introduce \guide (Guided Initialization and Distillation of
Embeddings). \guide can be considered a distillation technique that forces the
student to match the teacher in the parameter space. Using \guide we show
25-26\% reduction in the teacher-student quality gap when using large student
models (400M - 1B parameters) trained on $\approx$ 20B tokens. We also present
a thorough analysis demonstrating that \guide can be combined with knowledge
distillation with near additive improvements. Furthermore, we show that
applying \guide alone leads to substantially better model quality than applying
knowledge distillation by itself.
  Most importantly, \guide introduces no training or inference overhead and
hence any model quality gains from our method are virtually free.

</details>


### [468] [ATLO-ML: Adaptive Time-Length Optimizer for Machine Learning -- Insights from Air Quality Forecasting](https://arxiv.org/abs/2510.06503)
*I-Hsi Kao,Kanji Uchino*

Main category: cs.LG

TL;DR: ATLO-ML是一个自适应时间长度优化系统，可自动确定输入时间长度和采样率，以提高时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确的时间序列预测受到适当的输入时间长度和采样率选择的严重影响。

Method: ATLO-ML是一个自适应时间长度优化系统，它根据用户定义的输出时间长度自动确定最佳输入时间长度和采样率。

Result: 使用优化的时间长度和采样率显着提高了机器学习模型的准确性，与固定的时间长度相比。

Conclusion: ATLO-ML有潜力推广到各种时间敏感型应用，为优化机器学习工作流中的时间输入参数提供了稳健的解决方案。

Abstract: Accurate time-series predictions in machine learning are heavily influenced
by the selection of appropriate input time length and sampling rate. This paper
introduces ATLO-ML, an adaptive time-length optimization system that
automatically determines the optimal input time length and sampling rate based
on user-defined output time length. The system provides a flexible approach to
time-series data pre-processing, dynamically adjusting these parameters to
enhance predictive performance. ATLO-ML is validated using air quality
datasets, including both GAMS-dataset and proprietary data collected from a
data center, both in time series format. Results demonstrate that utilizing the
optimized time length and sampling rate significantly improves the accuracy of
machine learning models compared to fixed time lengths. ATLO-ML shows potential
for generalization across various time-sensitive applications, offering a
robust solution for optimizing temporal input parameters in machine learning
workflows.

</details>


### [469] [A Median Perspective on Unlabeled Data for Out-of-Distribution Detection](https://arxiv.org/abs/2510.06505)
*Momin Abbas,Ali Falahati,Hossein Goli,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: Medix框架利用中位数操作从无标签数据中识别潜在的异常值，并结合标签的InD数据来训练稳健的OOD分类器，在开放世界设置中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的机器学习系统需要进行OOD检测以确保鲁棒性和可靠性。然而，由于数据混合了InD和OOD样本，利用无标签的野外数据仍然具有挑战性。

Method: 提出Medix框架，利用中位数操作从无标签数据中识别潜在的异常值。然后，使用识别出的异常值和标签的InD数据来训练OOD分类器。

Result: Medix在开放世界设置中，在OOD检测方面优于现有方法。理论上，Medix的误差界限表明其具有低误差率。

Conclusion: Medix框架能够有效地利用无标签数据来提高OOD检测性能，并且在理论和实践中都得到了验证。

Abstract: Out-of-distribution (OOD) detection plays a crucial role in ensuring the
robustness and reliability of machine learning systems deployed in real-world
applications. Recent approaches have explored the use of unlabeled data,
showing potential for enhancing OOD detection capabilities. However,
effectively utilizing unlabeled in-the-wild data remains challenging due to the
mixed nature of both in-distribution (InD) and OOD samples. The lack of a
distinct set of OOD samples complicates the task of training an optimal OOD
classifier. In this work, we introduce Medix, a novel framework designed to
identify potential outliers from unlabeled data using the median operation. We
use the median because it provides a stable estimate of the central tendency,
as an OOD detection mechanism, due to its robustness against noise and
outliers. Using these identified outliers, along with labeled InD data, we
train a robust OOD classifier. From a theoretical perspective, we derive error
bounds that demonstrate Medix achieves a low error rate. Empirical results
further substantiate our claims, as Medix outperforms existing methods across
the board in open-world settings, confirming the validity of our theoretical
insights.

</details>


### [470] [Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security](https://arxiv.org/abs/2510.06525)
*Ali Naseh,Anshuman Suri,Yuefeng Peng,Harsh Chaudhari,Alina Oprea,Amir Houmansadr*

Main category: cs.LG

TL;DR: 生成式AI排行榜容易被操纵，特别是文本到图像排行榜，因为模型身份识别更容易，可能导致排名操纵。


<details>
  <summary>Details</summary>
Motivation: 评估生成式AI模型能力排行榜容易被操纵，特别是文本到图像排行榜，模型身份识别更容易，增加了排名操纵的风险。

Method: 使用超过15万张图像和280个提示，对19个不同模型进行分析，证明在CLIP嵌入空间中进行实时分类可以高精度地识别生成模型，即使没有提示控制或历史数据。引入了提示级别可分离性指标，识别出能够实现近乎完美模型身份识别的提示。

Result: 在CLIP嵌入空间中进行实时分类可以高精度地识别生成模型，即使没有提示控制或历史数据。识别出了能够实现近乎完美模型身份识别的提示。

Conclusion: 文本到图像排行榜的排名操纵比之前认识到的更容易，需要更强的防御措施。

Abstract: Generative AI leaderboards are central to evaluating model capabilities, but
remain vulnerable to manipulation. Among key adversarial objectives is rank
manipulation, where an attacker must first deanonymize the models behind
displayed outputs -- a threat previously demonstrated and explored for large
language models (LLMs). We show that this problem can be even more severe for
text-to-image leaderboards, where deanonymization is markedly easier. Using
over 150,000 generated images from 280 prompts and 19 diverse models spanning
multiple organizations, architectures, and sizes, we demonstrate that simple
real-time classification in CLIP embedding space identifies the generating
model with high accuracy, even without prompt control or historical data. We
further introduce a prompt-level separability metric and identify prompts that
enable near-perfect deanonymization. Our results indicate that rank
manipulation in text-to-image leaderboards is easier than previously
recognized, underscoring the need for stronger defenses.

</details>


### [471] [Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture](https://arxiv.org/abs/2510.06527)
*John Dunbar,Scott Aaronson*

Main category: cs.LG

TL;DR: 随机初始化的神经网络在激活函数非线性且均值为零时，输出近似独立。


<details>
  <summary>Details</summary>
Motivation: 探索AI可解释性的极限，并检验ARC的计算无巧合猜想。

Method: 分析具有大宽度和自然选择超参数的随机初始化神经网络的输出独立性，重点关注激活函数的性质。

Result: 发现当激活函数满足$"E}_{z \sim \mathcal{N}(0,1)}[\sigma(z)]=0$（例如ReLU和GeLU加上一个偏移量，以及tanh）时，神经网络的输出近似独立。

Conclusion: 均值为零的激活函数是实现神经网络输出近似独立的一种有前景的方法，这可能有助于解决ARC的计算无巧合猜想。

Abstract: We establish that randomly initialized neural networks, with large width and
a natural choice of hyperparameters, have nearly independent outputs exactly
when their activation function is nonlinear with zero mean under the Gaussian
measure: $\mathbb{E}_{z \sim \mathcal{N}(0,1)}[\sigma(z)]=0$. For example, this
includes ReLU and GeLU with an additive shift, as well as tanh, but not ReLU or
GeLU by themselves. Because of their nearly independent outputs, we propose
neural networks with zero-mean activation functions as a promising candidate
for the Alignment Research Center's computational no-coincidence conjecture --
a conjecture that aims to measure the limits of AI interpretability.

</details>


### [472] [Scalable Policy-Based RL Algorithms for POMDPs](https://arxiv.org/abs/2510.06540)
*Ameya Anjarlekar,Rasoul Etesami,R Srikant*

Main category: cs.LG

TL;DR: 该研究提出一种将POMDP近似为有限状态MDP（Superstate MDP）的方法来解决部分可观察强化学习问题，并为这种近似提供了理论保证，同时提出了一种基于策略的线性函数逼近学习方法来求解Superstate MDP，最终实现了对POMDP的近似求解。


<details>
  <summary>Details</summary>
Motivation: POMDP中连续状态空间给求解最优策略带来了计算挑战。

Method: 将POMDP近似为有限状态MDP（Superstate MDP），然后使用基于策略的线性函数逼近方法学习Superstate MDP的最优策略。

Result: 提出了具有理论保证的Superstate MDP近似方法，并展示了使用TD学习和策略优化近似求解POMDP的可行性，证明了近似误差随历史长度呈指数下降。

Conclusion: 通过将POMDP近似为有限状态MDP，并利用TD学习和策略优化，可以在一定程度上近似求解POMDP问题，并且该方法的近似误差可以随着历史长度的增加而呈指数级减小。

Abstract: The continuous nature of belief states in POMDPs presents significant
computational challenges in learning the optimal policy. In this paper, we
consider an approach that solves a Partially Observable Reinforcement Learning
(PORL) problem by approximating the corresponding POMDP model into a
finite-state Markov Decision Process (MDP) (called Superstate MDP). We first
derive theoretical guarantees that improve upon prior work that relate the
optimal value function of the transformed Superstate MDP to the optimal value
function of the original POMDP. Next, we propose a policy-based learning
approach with linear function approximation to learn the optimal policy for the
Superstate MDP. Consequently, our approach shows that a POMDP can be
approximately solved using TD-learning followed by Policy Optimization by
treating it as an MDP, where the MDP state corresponds to a finite history. We
show that the approximation error decreases exponentially with the length of
this history. To the best of our knowledge, our finite-time bounds are the
first to explicitly quantify the error introduced when applying standard TD
learning to a setting where the true dynamics are not Markovian.

</details>


### [473] [Incoherence in goal-conditioned autoregressive models](https://arxiv.org/abs/2510.06545)
*Jacek Karwowski,Raymond Douglas*

Main category: cs.LG

TL;DR: 该论文研究了在自回归模型中，通过简单地添加目标条件所产生的非相干性问题，并探讨了在离线学习策略上进行在线强化学习微调的效果。


<details>
  <summary>Details</summary>
Motivation: 研究的主要动机是解决强化学习策略中由于对自回归模型进行朴素的目标条件化而产生的结构性问题，即非相干性。

Method: 通过对模型自身行为进行再训练（即在线强化学习微调离线学习的策略）来解决非相干性问题，并建立了与后验折叠和温度参数降低之间的对应关系，同时通过软约束生成模型讨论了非相干性与有效视界之间的联系。

Result: 证明了再训练可以减少非相干性并提高回报，同时分析了策略的轨迹。

Conclusion: 通过将控制视为推理和软Q学习的标准概念重新构建，建立了迭代再训练过程的三种理解方式之间的对应关系，并指出了这种对应关系在计算上的含义。

Abstract: We investigate mathematically the notion of incoherence: a structural issue
with reinforcement learning policies derived by naive goal-conditioning of
autoregressive models. We focus on the process of re-training models on their
own actions, that is, fine-tuning offline-learned policies with online RL. We
prove that it decreases incoherence and leads to an improvement in return, and
we aim to characterize the resulting trajectory of policies. By re-framing
standard notions of control-as-inference and soft Q learning, we establish a
three-way correspondence with two other ways of understanding the iterative
re-training process: as folding the posterior into the reward and, in the
deterministic case, as decreasing the temperature parameter; the correspondence
has computational content via the training-inference trade-off. Through
soft-conditioning generative models, we discuss the link between incoherence
and the effective horizon.

</details>


### [474] [The Markovian Thinker](https://arxiv.org/abs/2510.06557)
*Milad Aghajohari,Kamran Chitsaz,Amirhossein Kazemnejad,Sarath Chandar,Alessandro Sordoni,Aaron Courville,Siva Reddy*

Main category: cs.LG

TL;DR: 通过重新设计强化学习（RL）中的“思考环境”，提出了一种名为“马尔可夫思维”的新范例，该范例通过将推理过程分解为固定大小的块来解决标准RL方法中状态无界和计算量二次增长的问题。这种方法通过在每个块的末尾生成一个简短的文本状态来实现上下文重置和提示重新初始化，从而实现了推理长度与上下文大小的分离。实验表明，使用Delethink（一种基于马尔可夫思维的RL环境）训练的模型在较长的推理长度下表现出色，并且计算成本大大降低，为构建高效、可扩展的推理语言模型提供了新的途径。


<details>
  <summary>Details</summary>
Motivation: 标准的强化学习（RL）方法在训练能够生成长推理链（LongCoT）的大语言模型（LLMs）时，面临状态空间无界和计算量随推理长度二次增长的问题，因为状态包含了提示和所有先前的推理标记，这使得基于注意力机制的策略在处理长推理时计算成本过高。

Method: 提出了一种名为“马尔可夫思维”的范式，该范式通过将推理过程分解为固定大小的块（例如8K token）来构建RL环境。在每个块的边界，环境会重置上下文并用一个简短的carryover重新初始化提示。模型被训练来学习在每个块的末尾生成一个文本状态，以便在重置后能够无缝地继续推理。这种方法将思考长度与上下文大小分离开来，实现了线性的计算复杂度和恒定的内存占用。

Result: 使用Delethink环境训练的R1-Distill 1.5B模型，在8K token的块大小下，能够进行高达24K token的推理，性能与使用24K token预算的LongCoT-RL相当或更优。并且，Delethink在LongCoT-RL达到性能瓶颈时，其性能仍能继续提升。在计算成本方面，当平均推理长度为96K时，Delethink的成本估计为7个H100-月，而LongCoT-RL的成本高达27个H100-月。此外，分析表明，即使在RL初始化阶段，现有的推理模型（1.5B-120B）在各种基准测试中也常常能够零样本地生成马尔可夫轨迹，这使得RL能够大规模有效地进行训练。

Conclusion: 重新设计“思考环境”是提升LLM推理能力的关键。马尔可夫思维范式通过引入固定大小的推理块和上下文重置机制，成功解决了长推理中的计算效率和内存限制问题，使得模型能够在没有二次计算开销的情况下实现非常长的推理。这为开发高效、可扩展的推理LLM开辟了新的道路。

Abstract: Reinforcement learning (RL) has recently become a strong recipe for training
reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard
RL "thinking environment", where the state is the prompt plus all prior
reasoning tokens, makes the state unbounded and forces attention-based policies
to pay quadratic compute as thoughts lengthen. We revisit the environment
itself. We propose Markovian Thinking, a paradigm in which the policy advances
reasoning while conditioning on a constant-size state, decoupling thinking
length from context size. As an immediate consequence this yields linear
compute with constant memory. We instantiate this idea with Delethink, an RL
environment that structures reasoning into fixed-size chunks. Within each
chunk, the model thinks as usual; at the boundary, the environment resets the
context and reinitializes the prompt with a short carryover. Through RL, the
policy learns to write a textual state near the end of each chunk sufficient
for seamless continuation of reasoning after reset. Trained in this
environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up
to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.
With test-time scaling, Delethink continues to improve where LongCoT plateaus.
The effect of linear compute is substantial: we empirically estimate at 96K
average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.
Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)
often sample Markovian traces zero-shot across diverse benchmarks, providing
positive samples that make RL effective at scale. Our results show that
redesigning the thinking environment is a powerful lever: it enables very long
reasoning without quadratic overhead and opens a path toward efficient,
scalable reasoning LLMs.

</details>


### [475] [DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning](https://arxiv.org/abs/2510.06913)
*Ke Guo,Haochen Liu,Xiaojun Wu,Chen Lv*

Main category: cs.LG

TL;DR: DecompGAIL通过将现实性分解为自我-地图和自我-邻居组件，并引入社会PPO目标来解决多智能体交通仿真的不稳定性问题，在WOMD Sim Agents 2025基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的模仿学习方法在模拟真实交通行为时存在不足，特别是GAIL在多智能体环境中存在不稳定的问题，其原因是判别器会因为邻居不真实的交互而惩罚ego车辆的真实行为。

Method: 提出Decomposed Multi-agent GAIL (DecompGAIL)，将现实性分解为自我-地图和自我-邻居两个组成部分，过滤掉邻居-邻居和邻居-地图的误导性交互。同时，引入社会PPO目标，通过距离加权的邻居奖励来增强ego奖励，鼓励智能体之间的整体真实性。

Result: DecompGAIL集成了轻量级的SMART骨干，在WOMD Sim Agents 2025基准测试中取得了最先进的性能。

Conclusion: DecompGAIL通过明确分解现实性并引入社会PPO目标，有效解决了多智能体交通模拟中的不稳定性和误导性交互问题，提高了模拟的真实性。

Abstract: Realistic traffic simulation is critical for the development of autonomous
driving systems and urban mobility planning, yet existing imitation learning
approaches often fail to model realistic traffic behaviors. Behavior cloning
suffers from covariate shift, while Generative Adversarial Imitation Learning
(GAIL) is notoriously unstable in multi-agent settings. We identify a key
source of this instability: irrelevant interaction misguidance, where a
discriminator penalizes an ego vehicle's realistic behavior due to unrealistic
interactions among its neighbors. To address this, we propose Decomposed
Multi-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map
and ego-neighbor components, filtering out misleading neighbor: neighbor and
neighbor: map interactions. We further introduce a social PPO objective that
augments ego rewards with distance-weighted neighborhood rewards, encouraging
overall realism across agents. Integrated into a lightweight SMART-based
backbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim
Agents 2025 benchmark.

</details>


### [476] [The Framework That Survives Bad Models: Human-AI Collaboration For Clinical Trials](https://arxiv.org/abs/2510.06567)
*Yao Chen,David Ohlssen,Aimee Readie,Gregory Ligozio,Ruvie Martin,Thibaud Coroller*

Main category: cs.LG

TL;DR: AI作为辅助读者的模式在临床试验中对疾病评估是最可靠的。


<details>
  <summary>Details</summary>
Motivation: 在临床试验中，AI有潜力改进患者招募、终点评估和治疗反应预测，但缺乏安全措施可能带来风险，尤其是在评估影响试验结论的患者终点时。

Method: 比较了两种AI框架与纯人工评估在基于医学影像的疾病评估中的成本、准确性、鲁棒性和泛化能力。通过注入从随机猜测到朴素预测的差模型来测试框架的稳健性，以确保即使在模型严重退化的情况下，观察到的治疗效果仍然有效。在两个基于脊柱X光影像的随机对照试验中评估了这些框架。

Result: AI作为辅助读者的模式（AI-SR）在各种模型类型下都表现良好，即使在引入差模型后也能满足所有标准。该方法能可靠地估计疾病，保持临床试验的治疗效果估计和结论，并能应用于不同人群。

Conclusion: AI-SR模式是临床试验中最适合的方法，因为它在各种模型类型下都表现出可靠性、鲁棒性和泛化能力。

Abstract: Artificial intelligence (AI) holds great promise for supporting clinical
trials, from patient recruitment and endpoint assessment to treatment response
prediction. However, deploying AI without safeguards poses significant risks,
particularly when evaluating patient endpoints that directly impact trial
conclusions. We compared two AI frameworks against human-only assessment for
medical image-based disease evaluation, measuring cost, accuracy, robustness,
and generalization ability. To stress-test these frameworks, we injected bad
models, ranging from random guesses to naive predictions, to ensure that
observed treatment effects remain valid even under severe model degradation. We
evaluated the frameworks using two randomized controlled trials with endpoints
derived from spinal X-ray images. Our findings indicate that using AI as a
supporting reader (AI-SR) is the most suitable approach for clinical trials, as
it meets all criteria across various model types, even with bad models. This
method consistently provides reliable disease estimation, preserves clinical
trial treatment effect estimates and conclusions, and retains these advantages
when applied to different populations.

</details>


### [477] [Introspection in Learned Semantic Scene Graph Localisation](https://arxiv.org/abs/2510.07053)
*Manshika Charvi Bissessur,Efimia Panagiotaki,Daniele De Martini*

Main category: cs.LG

TL;DR: 本文研究了在学习到的自监督、对比语义定位框架中，语义如何影响定位性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究语义如何影响定位性能和鲁棒性。

Method: 在原始和扰动地图上训练定位网络，然后进行事后内省分析，以探究模型是否过滤环境噪声，并优先考虑独特的地标而非常规混乱。验证了各种可解释性方法，并进行了比较可靠性分析。

Result: 集成梯度和注意力权重一直是最可靠的学习行为探针。语义类别烧蚀进一步揭示了频繁对象经常被降权的隐式加权。

Conclusion: 结果表明，该模型学习到了关于地点定义的、鲁棒的、语义上显著的关系，从而在具有挑战性的视觉和结构变化下实现了可解释的配准。

Abstract: This work investigates how semantics influence localisation performance and
robustness in a learned self-supervised, contrastive semantic localisation
framework. After training a localisation network on both original and perturbed
maps, we conduct a thorough post-hoc introspection analysis to probe whether
the model filters environmental noise and prioritises distinctive landmarks
over routine clutter. We validate various interpretability methods and present
a comparative reliability analysis. Integrated gradients and Attention Weights
consistently emerge as the most reliable probes of learned behaviour. A
semantic class ablation further reveals an implicit weighting in which frequent
objects are often down-weighted. Overall, the results indicate that the model
learns noise-robust, semantically salient relations about place definition,
thereby enabling explainable registration under challenging visual and
structural variations.

</details>


### [478] [DPA-Net: A Dual-Path Attention Neural Network for Inferring Glycemic Control Metrics from Self-Monitored Blood Glucose Data](https://arxiv.org/abs/2510.06623)
*Canyu Lei,Benjamin Lobo,Jianxin Xie*

Main category: cs.LG

TL;DR: 提出了一种名为DPA-Net的双路径注意力神经网络，可以直接从SMBG数据估计AGP指标，解决了CGM成本高、可及性有限的问题。


<details>
  <summary>Details</summary>
Motivation: CGM可提供详细的葡萄糖监测数据，但成本高昂，限制了其广泛应用，尤其是在中低收入地区。SMBG成本低、易于获取，但数据稀疏，难以转化为有临床意义的血糖指标。因此，需要一种方法来从SMBG数据估算AGP指标。

Method: 提出了一种名为DPA-Net的双路径注意力神经网络。该网络包含两个互补的路径：（1）空间-通道注意力路径，用于从稀疏的SMBG观测值重建类似CGM的轨迹；（2）多尺度ResNet路径，用于直接预测AGP指标。通过引入两个路径之间的对齐机制来减少偏差和缓解过拟合。此外，还开发了一种主动点选择器，用于识别反映患者行为模式的、真实且信息丰富的SMBG采样点。

Result: 在大型真实世界数据集上的实验结果表明，DPA-Net在具有低误差的情况下实现了稳健的准确性，并减少了系统偏差。

Conclusion: DPA-Net是首个从SMBG数据估算AGP指标的监督机器学习框架，为CGM不可及的环境提供了一种实用且具有临床意义的决策支持工具。

Abstract: Continuous glucose monitoring (CGM) provides dense and dynamic glucose
profiles that enable reliable estimation of Ambulatory Glucose Profile (AGP)
metrics, such as Time in Range (TIR), Time Below Range (TBR), and Time Above
Range (TAR). However, the high cost and limited accessibility of CGM restrict
its widespread adoption, particularly in low- and middle-income regions. In
contrast, self-monitoring of blood glucose (SMBG) is inexpensive and widely
available but yields sparse and irregular data that are challenging to
translate into clinically meaningful glycemic metrics.
  In this work, we propose a Dual-Path Attention Neural Network (DPA-Net) to
estimate AGP metrics directly from SMBG data. DPA-Net integrates two
complementary paths: (1) a spatial-channel attention path that reconstructs a
CGM-like trajectory from sparse SMBG observations, and (2) a multi-scale ResNet
path that directly predicts AGP metrics. An alignment mechanism between the two
paths is introduced to reduce bias and mitigate overfitting. In addition, we
develop an active point selector to identify realistic and informative SMBG
sampling points that reflect patient behavioral patterns.
  Experimental results on a large, real-world dataset demonstrate that DPA-Net
achieves robust accuracy with low errors while reducing systematic bias. To the
best of our knowledge, this is the first supervised machine learning framework
for estimating AGP metrics from SMBG data, offering a practical and clinically
relevant decision-support tool in settings where CGM is not accessible.

</details>


### [479] [Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report](https://arxiv.org/abs/2510.07092)
*Riccardo Mereu,Aidan Scannell,Yuxin Hou,Yi Zhao,Aditya Jitta,Antonio Dominguez,Luigi Acerbi,Amos Storkey,Paul Chang*

Main category: cs.LG

TL;DR: 该论文提出了一个名为1X World Model Challenge的基准，包含用于未来帧预测的采样轨道和用于未来离散潜在代码预测的压缩轨道。作者使用了改进的Wan-2.2 TI2V-5B模型（结合AdaLN-Zero和LoRA）解决了采样问题，并训练了一个新的时空Transformer模型来解决压缩问题。


<details>
  <summary>Details</summary>
Motivation: 开发用于AI和机器人学的世界模型，实现对未来的预测和推理能力，并为真实世界的人机交互提供一个开放的基准。

Method: 对于采样轨道，将视频生成模型Wan-2.2 TI2V-5B适配于视频状态条件下的未来帧预测，并使用AdaLN-Zero和LoRA进行条件化和后训练。对于压缩轨道，从头开始训练了一个时空Transformer模型。

Result: 在采样任务中达到23.0 dB PSNR，在压缩任务中达到6.6386的Top-500 CE，并在两个挑战赛中均获得第一名。

Conclusion: 作者提出的方法在1X World Model Challenge的采样和压缩两个任务中均取得了领先性能，证明了其在世界模型构建和预测能力上的有效性。

Abstract: World models are a powerful paradigm in AI and robotics, enabling agents to
reason about the future by predicting visual observations or compact latent
states. The 1X World Model Challenge introduces an open-source benchmark of
real-world humanoid interaction, with two complementary tracks: sampling,
focused on forecasting future image frames, and compression, focused on
predicting future discrete latent codes. For the sampling track, we adapt the
video generation foundation model Wan-2.2 TI2V-5B to video-state-conditioned
future frame prediction. We condition the video generation on robot states
using AdaLN-Zero, and further post-train the model using LoRA. For the
compression track, we train a Spatio-Temporal Transformer model from scratch.
Our models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386
in the compression task, securing 1st place in both challenges.

</details>


### [480] [POME: Post Optimization Model Edit via Muon-style Projection](https://arxiv.org/abs/2510.06627)
*Yong Liu,Di Fu,Yang Luo,Zirui Zhu,Minhao Cheng,Cho-Jui Hsieh,Yang You*

Main category: cs.LG

TL;DR: POME是一种通过编辑模型权重差异来提升微调大模型性能的新算法，无需额外数据或微调，兼容性强，且效果显著。


<details>
  <summary>Details</summary>
Motivation: 在不增加额外数据或计算成本的情况下，提升微调后的大语言模型的性能。

Method: 通过对微调前后模型权重差异（ΔW）应用基于截断奇异值分解（SVD）的muon风格投影，来均衡主导更新方向的影响并去除噪声。

Result: 在GSM8K上平均性能提升2.5%，在代码生成任务上提升1.0%，适用于7B到72B等多种规模的模型。

Conclusion: POME是一种实用的、零成本的后处理方法，可以无缝集成到任何微调流程中，以提升大模型的性能。

Abstract: We introduce Post-Optimization Model Edit (POME), a new algorithm that
enhances the performance of fine-tuned large language models using only their
pretrained and fine-tuned checkpoints, without requiring extra data or further
optimization. The core idea is to apply a muon-style projection to $\Delta W$,
the difference between the fine-tuned and pretrained weights. This projection
uses truncated singular value decomposition (SVD) to equalize the influence of
dominant update directions and prune small singular values, which often
represent noise. As a simple post-processing step, POME is completely decoupled
from the training pipeline. It requires zero modifications and imposes no
overhead, making it universally compatible with any optimizer or distributed
framework. POME delivers consistent gains, boosting average performance by
+2.5\% on GSM8K and +1.0\% on code generation. Its broad applicability -- from
7B foundation models to 72B RLHF-instructed models -- establishes it as a
practical, zero-cost enhancement for any fine-tuning pipeline. Code is
available at https://github.com/NUS-HPC-AI-Lab/POME.

</details>


### [481] [ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL](https://arxiv.org/abs/2510.07151)
*Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: ELMUR是一种带有结构化外部内存的Transformer架构，通过LRU内存模块进行更新和重写，有效扩展了决策的有效视野，并在部分可观察的环境中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人代理在部分可观察和长远规划问题上存在困难，因为大多数方法仅依赖瞬时信息，无法有效利用长期历史信息。标准的循环或Transformer模型在处理长期依赖方面存在局限性，如上下文窗口截断和朴素内存扩展的规模及稀疏性问题。

Method: 提出ELMUR（External Layer Memory with Update/Rewrite）架构，该架构使用结构化外部内存，其每个层维护内存嵌入，通过双向交叉注意力进行交互，并利用LRU（Least Recently Used）内存模块进行替换或凸混合来更新内存。

Result: ELMUR将有效视野扩展了100,000倍以上，在长达一百万步的合成T型迷宫任务上达到了100%的成功率。在POPGym的多个任务中，ELMUR的表现优于基线模型。在MIKASA-Robo的稀疏奖励操控任务中，ELMUR的性能接近强基线模型的两倍。

Conclusion: 结构化、层级局部的外部内存为部分可观察环境下的决策提供了一种简单且可扩展的方法。

Abstract: Real-world robotic agents must act under partial observability and long
horizons, where key cues may appear long before they affect decision making.
However, most modern approaches rely solely on instantaneous information,
without incorporating insights from the past. Standard recurrent or transformer
models struggle with retaining and leveraging long-term dependencies: context
windows truncate history, while naive memory extensions fail under scale and
sparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a
transformer architecture with structured external memory. Each layer maintains
memory embeddings, interacts with them via bidirectional cross-attention, and
updates them through an Least Recently Used (LRU) memory module using
replacement or convex blending. ELMUR extends effective horizons up to 100,000
times beyond the attention window and achieves a 100% success rate on a
synthetic T-Maze task with corridors up to one million steps. In POPGym, it
outperforms baselines on more than half of the tasks. On MIKASA-Robo
sparse-reward manipulation tasks with visual observations, it nearly doubles
the performance of strong baselines. These results demonstrate that structured,
layer-local external memory offers a simple and scalable approach to decision
making under partial observability.

</details>


### [482] [AI-Driven Forecasting and Monitoring of Urban Water System](https://arxiv.org/abs/2510.06631)
*Qiming Guo,Bishal Khatri,Hua Zhang,Wenlu Wang*

Main category: cs.LG

TL;DR: 通过部署稀疏的远程传感器并结合考虑管道属性（如材料、直径、坡度）的HydroNet模型，提出了一种集成人工智能和远程传感器的框架，用于检测地下水管线泄漏，并在真实数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 地下水和废水管道的异常（如泄漏和渗入）会导致巨大的水资源损失、环境破坏和高昂的维修成本，而传统的人工检查效率低下，密集的传感器部署成本过高。

Method: 部署稀疏的远程传感器来捕获实时流量和深度数据，并利用一个名为HydroNet的专用模型，该模型在有向图中考虑管道属性（例如材料、直径、坡度）以进行更高精度的建模。

Result: 所提出的系统能够收集有效的时空水力数据，使HydroNet能够优于先进的基线模型。该系统集成了边缘感知消息传递和水力模拟，实现了从有限的传感器部署中进行准确的网络范围预测。

Conclusion: 该方法能够有效地扩展到各种地下水管线网络中，用于检测泄漏。

Abstract: Underground water and wastewater pipelines are vital for city operations but
plagued by anomalies like leaks and infiltrations, causing substantial water
loss, environmental damage, and high repair costs. Conventional manual
inspections lack efficiency, while dense sensor deployments are prohibitively
expensive. In recent years, artificial intelligence has advanced rapidly and is
increasingly applied to urban infrastructure. In this research, we propose an
integrated AI and remote-sensor framework to address the challenge of leak
detection in underground water pipelines, through deploying a sparse set of
remote sensors to capture real-time flow and depth data, paired with HydroNet -
a dedicated model utilizing pipeline attributes (e.g., material, diameter,
slope) in a directed graph for higher-precision modeling. Evaluations on a
real-world campus wastewater network dataset demonstrate that our system
collects effective spatio-temporal hydraulic data, enabling HydroNet to
outperform advanced baselines. This integration of edge-aware message passing
with hydraulic simulations enables accurate network-wide predictions from
limited sensor deployments. We envision that this approach can be effectively
extended to a wide range of underground water pipeline networks.

</details>


### [483] [Three Forms of Stochastic Injection for Improved Distribution-to-Distribution Generative Modeling](https://arxiv.org/abs/2510.06634)
*Shiye Su,Yuhui Zhang,Linqi Zhou,Rajesh Ranganath,Serena Yeung-Levy*

Main category: cs.LG

TL;DR: 本研究提出了一种改进的流匹配方法，用于在有限样本下对任意数据分布进行建模，解决了标准流匹配在源分布学习稀疏监督问题，并在五项成像任务中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 在药物发现和进化模拟等领域，对任意数据分布之间的转换进行建模是一个基本科学挑战。流匹配为此提供了一个自然框架，但其应用主要局限于噪声到数据设置，而在数据到数据设置中未得到充分探索。标准流匹配在源数据分布学习稀疏的情况下会失败。

Method: 为了解决上述问题，研究者提出了一种通过扰动源样本和流插值来注入随机性的训练方法，该方法简单且计算高效。

Result: 该方法在五项包括生物学、放射学和天文学的成像任务上，显著提高了生成质量，平均比现有基线提高了 9 FID 分数。此外，该方法还降低了输入和生成样本之间的传输成本，更清晰地展示了转换的实际效果。

Conclusion: 该研究成功改进了流匹配方法，使其能够更有效地处理数据到数据分布转换任务，并为科学模拟提供了更实用的工具。

Abstract: Modeling transformations between arbitrary data distributions is a
fundamental scientific challenge, arising in applications like drug discovery
and evolutionary simulation. While flow matching offers a natural framework for
this task, its use has thus far primarily focused on the noise-to-data setting,
while its application in the general distribution-to-distribution setting is
underexplored. We find that in the latter case, where the source is also a data
distribution to be learned from limited samples, standard flow matching fails
due to sparse supervision. To address this, we propose a simple and
computationally efficient method that injects stochasticity into the training
process by perturbing source samples and flow interpolants. On five diverse
imaging tasks spanning biology, radiology, and astronomy, our method
significantly improves generation quality, outperforming existing baselines by
an average of 9 FID points. Our approach also reduces the transport cost
between input and generated samples to better highlight the true effect of the
transformation, making flow matching a more practical tool for simulating the
diverse distribution transformations that arise in science.

</details>


### [484] [StruSR: Structure-Aware Symbolic Regression with Physics-Informed Taylor Guidance](https://arxiv.org/abs/2510.06635)
*Yunpeng Gong,Sihan Lan,Can Yang,Kunpeng Xu,Min Jiang*

Main category: cs.LG

TL;DR: StruSR是一个利用PINN提取物理先验来指导符号回归，以发现更具可解释性的科学模型。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归方法难以从时间序列数据中提取结构化物理先验，导致难以捕捉反映系统全局行为的符号表达式。

Method: 提出了一种名为StruSR的结构感知符号回归框架，该框架利用训练好的PINN从时间序列数据中提取局部结构化物理先验。通过对PINN输出进行局部泰勒展开以获取导数信息来指导符号表达式的演化，并引入基于掩码的归因机制来量化子树贡献，以指导遗传编程中的变异和交叉操作。采用混合适应度函数来最小化物理残差和泰勒系数不匹配。

Result: 在基准PDE系统上的实验表明，与传统方法相比，StruSR在收敛速度、结构保真度和表达式可解释性方面均有所提高。

Conclusion: StruSR提供了一种基于物理的符号发现的原则性范例。

Abstract: Symbolic regression aims to find interpretable analytical expressions by
searching over mathematical formula spaces to capture underlying system
behavior, particularly in scientific modeling governed by physical laws.
However, traditional methods lack mechanisms for extracting structured physical
priors from time series observations, making it difficult to capture symbolic
expressions that reflect the system's global behavior. In this work, we propose
a structure-aware symbolic regression framework, called StruSR, that leverages
trained Physics-Informed Neural Networks (PINNs) to extract locally structured
physical priors from time series data. By performing local Taylor expansions on
the outputs of the trained PINN, we obtain derivative-based structural
information to guide symbolic expression evolution. To assess the importance of
expression components, we introduce a masking-based attribution mechanism that
quantifies each subtree's contribution to structural alignment and physical
residual reduction. These sensitivity scores steer mutation and crossover
operations within genetic programming, preserving substructures with high
physical or structural significance while selectively modifying less
informative components. A hybrid fitness function jointly minimizes physics
residuals and Taylor coefficient mismatch, ensuring consistency with both the
governing equations and the local analytical behavior encoded by the PINN.
Experiments on benchmark PDE systems demonstrate that StruSR improves
convergence speed, structural fidelity, and expression interpretability
compared to conventional baselines, offering a principled paradigm for
physics-grounded symbolic discovery.

</details>


### [485] [Control-Augmented Autoregressive Diffusion for Data Assimilation](https://arxiv.org/abs/2510.06637)
*Prakhar Srivastava,Farrin Marouf Sofian,Francesco Immorlano,Kushagra Pandey,Stephan Mandt*

Main category: cs.LG

TL;DR: 本研究提出了一种轻量级控制器网络，用于增强预训练的自回归扩散模型（ARDMs），以解决数据同化（DA）中计算成本高和预测漂移的问题。该方法通过单次前向传播和实时校正来减少DA推断，避免了昂贵的伴随计算或优化，并在稳定性、准确性和物理保真度方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归扩散模型（ARDM）在数据同化（DA）等任务中，由于计算成本高昂且在稀疏观测下容易出现预测漂移，其引导（guidance）方法仍有待探索。

Method: 提出了一种带成本的框架，通过一个轻量级的控制器网络来增强预训练的ARDM。该控制器网络通过预览未来的ARDM展开并学习分步控制，以在终端成本目标下预测即将到来的观测值。

Result: 该方法将DA推断简化为单次前向传播和实时校正，避免了推断过程中的伴随计算和/或优化。在两种典型的偏微分方程（PDEs）和六种观测模式下，与四种最先进的基线方法相比，该方法在稳定性、准确性和物理保真度方面均表现出持续的优越性。

Conclusion: 所提出的框架能够有效地增强预训练的ARDM，以解决计算成本高和预测漂移的问题，并在数据同化任务中取得优于现有方法的结果。

Abstract: Despite recent advances in test-time scaling and finetuning of diffusion
models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains
underexplored. We introduce an amortized framework that augments pretrained
ARDMs with a lightweight controller network, trained offline by previewing
future ARDM rollouts and learning stepwise controls that anticipate upcoming
observations under a terminal cost objective. We evaluate this framework in the
context of data assimilation (DA) for chaotic spatiotemporal partial
differential equations (PDEs), a setting where existing methods are often
computationally prohibitive and prone to forecast drift under sparse
observations. Our approach reduces DA inference to a single forward rollout
with on-the-fly corrections, avoiding expensive adjoint computations and/or
optimizations during inference. We demonstrate that our method consistently
outperforms four state-of-the-art baselines in stability, accuracy, and
physical fidelity across two canonical PDEs and six observation regimes. We
will release code and checkpoints publicly.

</details>


### [486] [The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators](https://arxiv.org/abs/2510.06646)
*Mansi Sakarvadia,Kareem Hegazy,Amin Totounferoush,Kyle Chard,Yaoqing Yang,Ian Foster,Michael W. Mahoney*

Main category: cs.LG

TL;DR: 机器学习算子（MLOs）在零样本超分辨率方面表现不佳，需要新的训练方法来实现多分辨率泛化。


<details>
  <summary>Details</summary>
Motivation: 评估机器学习算子（MLOs）是否足以实现“零样本超分辨率”，即在比原始训练数据更高分辨率的数据上进行推理。

Method: 通过对MLOs进行零样本和多分辨率（包括子分辨率和超分辨率）推理的全面评估，将多分辨率推理分解为对频率信息的外插和跨分辨率的内插。

Result: MLOs在零样本情况下无法有效处理不同频率信息的外插和跨分辨率的内插，导致在不同分辨率下推理时出现脆性（brittle）和混叠（aliasing）现象。

Conclusion: MLOs无法在零样本情况下进行准确的多分辨率推理。为了解决这些问题，提出了一种简单、计算高效且数据驱动的多分辨率训练方案，以克服混叠并实现鲁棒的多分辨率泛化。

Abstract: A core challenge in scientific machine learning, and scientific computing
more generally, is modeling continuous phenomena which (in practice) are
represented discretely. Machine-learned operators (MLOs) have been introduced
as a means to achieve this modeling goal, as this class of architecture can
perform inference at arbitrary resolution. In this work, we evaluate whether
this architectural innovation is sufficient to perform "zero-shot
super-resolution," namely to enable a model to serve inference on
higher-resolution data than that on which it was originally trained. We
comprehensively evaluate both zero-shot sub-resolution and super-resolution
(i.e., multi-resolution) inference in MLOs. We decouple multi-resolution
inference into two key behaviors: 1) extrapolation to varying frequency
information; and 2) interpolating across varying resolutions. We empirically
demonstrate that MLOs fail to do both of these tasks in a zero-shot manner.
Consequently, we find MLOs are not able to perform accurate inference at
resolutions different from those on which they were trained, and instead they
are brittle and susceptible to aliasing. To address these failure modes, we
propose a simple, computationally-efficient, and data-driven multi-resolution
training protocol that overcomes aliasing and that provides robust
multi-resolution generalization.

</details>


### [487] [Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions](https://arxiv.org/abs/2510.06649)
*Frank Wu,Mengye Ren*

Main category: cs.LG

TL;DR: FF算法在强化学习（RL）中引入了ARQ方法，一种无需反向传播的局部RL新方法，在多个基准测试中表现优于现有算法，甚至在大多数任务中优于反向传播算法。


<details>
  <summary>Details</summary>
Motivation: FF算法主要用于监督学习，在强化学习等领域存在应用空白。本研究旨在将FF算法的‘优度’函数概念应用于RL，以填补这一空白。

Method: 提出了一种名为ARQ（Action-conditioned Root mean squared Q-Functions）的新型价值估计方法。ARQ结合了‘优度’函数和动作条件，并利用时间差学习进行局部RL。

Result: ARQ在MinAtar和DeepMind Control Suite基准测试中，相比于其他的局部无反向传播RL方法取得了更好的性能。在大多数任务中，ARQ的性能也优于使用反向传播训练的算法。

Conclusion: ARQ是一种简单且具有生物学依据的新型局部RL方法，在多个基准测试中展现出优越的性能，尤其是在无需反向传播的情况下。

Abstract: The Forward-Forward (FF) Algorithm is a recently proposed learning procedure
for neural networks that employs two forward passes instead of the traditional
forward and backward passes used in backpropagation. However, FF remains
largely confined to supervised settings, leaving a gap at domains where
learning signals can be yielded more naturally such as RL. In this work,
inspired by FF's goodness function using layer activity statistics, we
introduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value
estimation method that applies a goodness function and action conditioning for
local RL using temporal difference learning. Despite its simplicity and
biological grounding, our approach achieves superior performance compared to
state-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind
Control Suite benchmarks, while also outperforming algorithms trained with
backpropagation on most tasks. Code can be found at
https://github.com/agentic-learning-ai-lab/arq.

</details>


### [488] [Rethinking Nonlinearity: Trainable Gaussian Mixture Modules for Modern Neural Architectures](https://arxiv.org/abs/2510.06660)
*Weiguo Lu,Gangnan Yuan,Hong-kun Zhang,Shangyang Li*

Main category: cs.LG

TL;DR: GMNM是一种新的可微分模块，可以提高各种神经网络的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的神经网络通过激活函数引入非线性，这可能受到限制。GMNM通过受高斯混合模型和高斯核距离特性的启发，提供了引入非线性的新方法。

Method: GMNM通过放松概率约束和采用灵活的高斯投影参数化来实现，可以无缝集成到各种神经网络架构中，并进行端到端训练。

Result: 实验表明，将GMNM集成到MLP、CNN、注意力机制和LSTM等架构中，可以持续提高性能。

Conclusion: GMNM是一个强大而灵活的模块，有潜力提高各种机器学习应用的效率和准确性。

Abstract: Neural networks in general, from MLPs and CNNs to attention-based
Transformers, are constructed from layers of linear combinations followed by
nonlinear operations such as ReLU, Sigmoid, or Softmax. Despite their strength,
these conventional designs are often limited in introducing non-linearity by
the choice of activation functions. In this work, we introduce Gaussian
Mixture-Inspired Nonlinear Modules (GMNM), a new class of differentiable
modules that draw on the universal density approximation Gaussian mixture
models (GMMs) and distance properties (metric space) of Gaussian kernal. By
relaxing probabilistic constraints and adopting a flexible parameterization of
Gaussian projections, GMNM can be seamlessly integrated into diverse neural
architectures and trained end-to-end with gradient-based methods. Our
experiments demonstrate that incorporating GMNM into architectures such as
MLPs, CNNs, attention mechanisms, and LSTMs consistently improves performance
over standard baselines. These results highlight GMNM's potential as a powerful
and flexible module for enhancing efficiency and accuracy across a wide range
of machine learning applications.

</details>


### [489] [The Effect of Attention Head Count on Transformer Approximation](https://arxiv.org/abs/2510.06662)
*Penghao Yu,Haotian Jiang,Zeyu Bao,Ruoxi Yu,Qianxiao Li*

Main category: cs.LG

TL;DR: Transformer模型在序列建模中占主导地位，但其结构参数如何影响表达能力仍不清楚。本研究通过引入广义D-检索任务，证明了Transformer在函数空间中的逼近性质，特别是注意力头的数量。我们建立了参数复杂度的上下界，证明了足够多的头可以实现高效逼近，而头太少则需要参数随1/ε^cT增长。我们还发现，在单头情况下，嵌入维度与序列长度T成比例可以实现完全记忆。实验验证了这些理论发现。


<details>
  <summary>Details</summary>
Motivation: Transformer模型已成为序列建模的标准架构，但对其结构参数（尤其是注意力头的数量）如何影响其表达能力，人们仍缺乏深入理解。

Method: 本研究引入了一个广义的D-检索任务，并证明该任务在连续函数空间中是稠密的，为理论分析奠定了基础。在此基础上，研究人员建立了参数复杂度达到ε-逼近的上下界，并分析了单头Transformer的情况。

Result: 研究表明，具有足够多注意力头的Transformer可以实现高效逼近；而注意力头过少，则参数数量必须至少按O(1/ε^cT)的比例增长。在单头情况下，嵌入维度达到O(T)即可实现输入信息的完全记忆，逼近完全由前馈网络实现。

Conclusion: 本研究首次在非线性且实际相关的环境中，为Transformer的逼近能力提供了严格的下界，并强调了增加注意力头数量对于提高逼近效率的重要性。此外，研究还揭示了在单头Transformer中，通过调整嵌入维度可以实现对输入的完全记忆。实验结果也证实了这些理论分析的实际意义。

Abstract: Transformer has become the dominant architecture for sequence modeling, yet a
detailed understanding of how its structural parameters influence expressive
power remains limited. In this work, we study the approximation properties of
transformers, with particular emphasis on the role of the number of attention
heads. Our analysis begins with the introduction of a generalized $D$-retrieval
task, which we prove to be dense in the space of continuous functions, thereby
providing the basis for our theoretical framework. We then establish both upper
and lower bounds on the parameter complexity required for
$\epsilon$-approximation. Specifically, we show that transformers with
sufficiently many heads admit efficient approximation, whereas with too few
heads, the number of parameters must scale at least as $O(1/\epsilon^{cT})$,
for some constant $c$ and sequence length $T$. To the best of our knowledge,
this constitutes the first rigorous lower bound of this type in a nonlinear and
practically relevant setting. We further examine the single-head case and
demonstrate that an embedding dimension of order $O(T)$ allows complete
memorization of the input, where approximation is entirely achieved by the
feed-forward block. Finally, we validate our theoretical findings with
experiments on both synthetic data and real-world tasks, illustrating the
practical relevance of our results.

</details>


### [490] [XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation](https://arxiv.org/abs/2510.06672)
*Udbhav Bamba,Minghao Fang,Yifan Yu,Haizhong Zheng,Fan Lai*

Main category: cs.LG

TL;DR: XRPO通过自适应分配、示例注入和奖励增强来改进GRPO，以提高LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法（如GRPO）在探索和利用信息方面存在局限性，导致在具有挑战性的提示上表现不佳，并且未能充分利用有用的反馈信号。

Method: XRPO通过以下方式改进GRPO：1. 引入一个自适应的rollout分配器，优先考虑不确定性减少潜力最大的提示。2. 采用一种上下文内种子策略，通过注入精心挑选的示例来解决零奖励提示的停滞问题。3. 开发一种基于组的、考虑新颖性的优势锐化机制，利用序列似然来放低概率但正确的响应。

Result: XRPO在数学和代码推理基准测试中，在推理和非推理模型上均优于GRPO和GSPO等现有方法，pass@1最高提高4%，cons@32最高提高6%，并能将训练收敛速度提高2.7倍。

Conclusion: XRPO是一个统一的框架，通过解决rollout分配、探索和利用方面的挑战，显著提高了LLM的推理能力。

Abstract: Reinforcement learning algorithms such as GRPO have driven recent advances in
large language model (LLM) reasoning. While scaling the number of rollouts
stabilizes training, existing approaches suffer from limited exploration on
challenging prompts and leave informative feedback signals underexploited, due
to context-independent rollout allocation across prompts (e.g., generating 16
rollouts per prompt) and relying heavily on sparse rewards. This paper presents
XRPO(eXplore - eXploit GRPO), a unified framework that recasts policy
optimization through the principled lens of rollout exploration-exploitation.
To enhance exploration, XRPO introduces a mathematically grounded rollout
allocator that adaptively prioritizes prompts with higher potential for
uncertainty reduction. It further addresses stagnation on zero-reward prompts
through an in-context seeding strategy that injects curated exemplars, steering
the model into more difficult reasoning trajectories. To strengthen
exploitation, XRPO develops a group-relative, novelty-aware advantage
sharpening mechanism that leverages sequence likelihoods to amplify
low-probability yet correct responses, thereby extending the policy's reach
beyond sparse rewards. Experiments across diverse math and coding benchmarks on
both reasoning and non-reasoning models demonstrate that XRPO outperforms
existing advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while
accelerating training convergence by up to 2.7X.

</details>


### [491] [TimeFormer: Transformer with Attention Modulation Empowered by Temporal Characteristics for Time Series Forecasting](https://arxiv.org/abs/2510.06680)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Baixin Li,Yongsheng Huang,Mingyang Geng,Changsheng Zhang,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: TimeFormer是一种新型Transformer架构，通过引入时间序列的单向影响和衰减影响特性，并结合多尺度和子序列分析，显著提高了时间序列预测的性能，并在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Transformer在时间序列预测方面存在局限性，因为它没有充分考虑文本和时间序列模态的差异。本研究旨在开发一种能最大化表示能力的新型Transformer架构，以解决这一问题。

Method: 提出了一种名为TimeFormer的新型Transformer架构，其核心创新在于一个名为MoSA（Modulated Self-Attention）的自注意力机制。MoSA机制结合了霍克斯过程和因果掩码的约束，引入了两个调制项，以捕捉时间序列的单向影响和随时间衰减的影响。此外，TimeFormer还采用了基于多尺度和子序列分析的框架，以在不同时间尺度上捕捉语义依赖关系，从而丰富时间依赖性。

Result: 在多个真实世界数据集上的大量实验表明，TimeFormer的性能显著优于最先进的方法，其MSE（均方误差）相比最佳基线方法降低了高达7.45%，并在94.04%的评估指标上创下新纪录。此外，研究还证明了MoSA机制可以广泛应用于增强其他基于Transformer的模型。

Conclusion: TimeFormer通过结合时间序列的固有特性（单向影响和衰减影响）以及多尺度分析，成功地提高了Transformer在时间序列预测任务上的性能。MoSA机制的通用性也为未来在其他Transformer模型中的应用提供了可能性。

Abstract: Although Transformers excel in natural language processing, their extension
to time series forecasting remains challenging due to insufficient
consideration of the differences between textual and temporal modalities. In
this paper, we develop a novel Transformer architecture designed for time
series data, aiming to maximize its representational capacity. We identify two
key but often overlooked characteristics of time series: (1) unidirectional
influence from the past to the future, and (2) the phenomenon of decaying
influence over time. These characteristics are introduced to enhance the
attention mechanism of Transformers. We propose TimeFormer, whose core
innovation is a self-attention mechanism with two modulation terms (MoSA),
designed to capture these temporal priors of time series under the constraints
of the Hawkes process and causal masking. Additionally, TimeFormer introduces a
framework based on multi-scale and subsequence analysis to capture semantic
dependencies at different temporal scales, enriching the temporal dependencies.
Extensive experiments conducted on multiple real-world datasets show that
TimeFormer significantly outperforms state-of-the-art methods, achieving up to
a 7.45% reduction in MSE compared to the best baseline and setting new
benchmarks on 94.04\% of evaluation metrics. Moreover, we demonstrate that the
MoSA mechanism can be broadly applied to enhance the performance of other
Transformer-based models.

</details>


### [492] [Distributed Algorithms for Multi-Agent Multi-Armed Bandits with Collision](https://arxiv.org/abs/2510.06683)
*Daoyuan Zhou,Xuchuang Wang,Lin Yang,Yang Gao*

Main category: cs.LG

TL;DR: 该研究提出了一种用于解决随机多人多臂老虎机（MMAB）问题的分布式算法，该算法具有自适应通信协议，能够实现接近最优的群体和个体遗憾，并且通信成本仅为O(log log T)。实验证明了其优于现有基线方法的性能，并在异步设置下实现了对数遗憾。


<details>
  <summary>Details</summary>
Motivation: 多人多臂老虎机（MMAB）问题，玩家选择臂以最大化累积奖励，碰撞（多个玩家选择同一臂）导致无奖励，玩家会观察到碰撞。

Method: 提出了一种具有自适应、高效通信协议的分布式算法。

Result: 在随机MMAB问题中实现了接近最优的群体和个体遗憾，通信成本为O(log log T)。实验表明性能优于现有基线方法，个体遗憾显著降低。将该方法扩展到周期性异步设置，证明了问题下界，并提出了实现对数遗憾的算法。

Conclusion: 所提出的分布式算法在随机MMAB问题和周期性异步MMAB问题上都取得了优异的性能，并在通信效率和遗憾界限方面具有理论优势。

Abstract: We study the stochastic Multiplayer Multi-Armed Bandit (MMAB) problem, where
multiple players select arms to maximize their cumulative rewards. Collisions
occur when two or more players select the same arm, resulting in no reward, and
are observed by the players involved. We consider a distributed setting without
central coordination, where each player can only observe their own actions and
collision feedback. We propose a distributed algorithm with an adaptive,
efficient communication protocol. The algorithm achieves near-optimal group and
individual regret, with a communication cost of only $\mathcal{O}(\log\log T)$.
Our experiments demonstrate significant performance improvements over existing
baselines. Compared to state-of-the-art (SOTA) methods, our approach achieves a
notable reduction in individual regret. Finally, we extend our approach to a
periodic asynchronous setting, proving the lower bound for this problem and
presenting an algorithm that achieves logarithmic regret.

</details>


### [493] [Is the Hard-Label Cryptanalytic Model Extraction Really Polynomial?](https://arxiv.org/abs/2510.06692)
*Akira Ito,Takayuki Miura,Yosuke Todo*

Main category: cs.LG

TL;DR: 现有模型提取方法在攻击目标深度增加时，其假设变得不切实际，需要指数级查询。本文提出跨层提取方法，利用神经元交互来降低查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有模型提取方法在攻击目标深度增加时，需要指数级查询，这使得攻击不总是能在多项式时间内完成，存在局限性。

Method: 提出一种名为“跨层提取”的新攻击方法，该方法不直接提取单个神经元的参数，而是利用神经元跨层交互来提取深层信息，从而降低查询复杂度。

Result: 跨层提取技术显著降低了模型提取的查询复杂度，解决了现有方法在面对更深层网络时的局限性。

Conclusion: 跨层提取是一种更有效的模型提取方法，尤其适用于更深层的神经网络，克服了现有技术的关键限制。

Abstract: Deep Neural Networks (DNNs) have attracted significant attention, and their
internal models are now considered valuable intellectual assets. Extracting
these internal models through access to a DNN is conceptually similar to
extracting a secret key via oracle access to a block cipher. Consequently,
cryptanalytic techniques, particularly differential-like attacks, have been
actively explored recently. ReLU-based DNNs are the most commonly and widely
deployed architectures. While early works (e.g., Crypto 2020, Eurocrypt 2024)
assume access to exact output logits, which are usually invisible, more recent
works (e.g., Asiacrypt 2024, Eurocrypt 2025) focus on the hard-label setting,
where only the final classification result (e.g., "dog" or "car") is available
to the attacker. Notably, Carlini et al. (Eurocrypt 2025) demonstrated that
model extraction is feasible in polynomial time even under this restricted
setting.
  In this paper, we first show that the assumptions underlying their attack
become increasingly unrealistic as the attack-target depth grows. In practice,
satisfying these assumptions requires an exponential number of queries with
respect to the attack depth, implying that the attack does not always run in
polynomial time. To address this critical limitation, we propose a novel attack
method called CrossLayer Extraction. Instead of directly extracting the secret
parameters (e.g., weights and biases) of a specific neuron, which incurs
exponential cost, we exploit neuron interactions across layers to extract this
information from deeper layers. This technique significantly reduces query
complexity and mitigates the limitations of existing model extraction
approaches.

</details>


### [494] [A Diffusion Model for Regular Time Series Generation from Irregular Data with Completion and Masking](https://arxiv.org/abs/2510.06699)
*Gal Fadlon,Idan Arbiv,Nimrod Berman,Omri Azencot*

Main category: cs.LG

TL;DR: 通过结合时间序列Transformer和基于掩码的扩散模型，有效地生成了不规则采样和缺失值的时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不规则采样和缺失值的时间序列数据时存在生成结果不佳和计算成本高的问题。ImagenTime等基于扩散模型的方法在处理规则时间序列方面表现出色，但直接应用于不规则数据会引入不自然的邻域，干扰学习过程。

Method: 提出了一种新颖的两步框架：首先，使用时间序列Transformer补全不规则序列，创建自然的邻域；然后，使用基于掩码的视觉扩散模型，最小化对补全值的依赖。

Result: 在判别分数方面相对提高了70%，在计算成本方面相对降低了85%，达到了最先进的性能。

Conclusion: 提出的方法结合了序列补全和掩码的优点，能够有效且高效地生成真实的时间序列数据。

Abstract: Generating realistic time series data is critical for applications in
healthcare, finance, and science. However, irregular sampling and missing
values present significant challenges. While prior methods address these
irregularities, they often yield suboptimal results and incur high
computational costs. Recent advances in regular time series generation, such as
the diffusion-based ImagenTime model, demonstrate strong, fast, and scalable
generative capabilities by transforming time series into image representations,
making them a promising solution. However, extending ImagenTime to irregular
sequences using simple masking introduces "unnatural" neighborhoods, where
missing values replaced by zeros disrupt the learning process. To overcome
this, we propose a novel two-step framework: first, a Time Series Transformer
completes irregular sequences, creating natural neighborhoods; second, a
vision-based diffusion model with masking minimizes dependence on the completed
values. This approach leverages the strengths of both completion and masking,
enabling robust and efficient generation of realistic time series. Our method
achieves state-of-the-art performance, achieving a relative improvement in
discriminative score by $70\%$ and in computational cost by $85\%$. Code is at
https://github.com/azencot-group/ImagenI2R.

</details>


### [495] [Dual Goal Representations](https://arxiv.org/abs/2510.06714)
*Seohong Park,Deepinder Mann,Sergey Levine*

Main category: cs.LG

TL;DR: 引入了对偶目标表示法，用于目标条件强化学习（GCRL）。


<details>
  <summary>Details</summary>
Motivation: 目标条件强化学习（GCRL）需要有效的状态表示法来处理各种目标。

Method: 提出了一种新的对偶目标表示法，该表示法将状态描述为“与所有其他状态的时间距离集合”。这种表示法具有内在动力学不变性和包含恢复最优目标策略的充分信息等理论特性。基于此概念，开发了一种实用的对偶目标表示法学习方法，可与现有GCRL算法结合使用。

Result: 在OGBench任务套件的20个基于状态和像素的任务上，对偶目标表示法在离线目标到达性能方面得到了一致的改进。

Conclusion: 对偶目标表示法为GCRL提供了一种新颖且有效的方法，可提高各种任务中的目标到达性能。

Abstract: In this work, we introduce dual goal representations for goal-conditioned
reinforcement learning (GCRL). A dual goal representation characterizes a state
by "the set of temporal distances from all other states"; in other words, it
encodes a state through its relations to every other state, measured by
temporal distance. This representation provides several appealing theoretical
properties. First, it depends only on the intrinsic dynamics of the environment
and is invariant to the original state representation. Second, it contains
provably sufficient information to recover an optimal goal-reaching policy,
while being able to filter out exogenous noise. Based on this concept, we
develop a practical goal representation learning method that can be combined
with any existing GCRL algorithm. Through diverse experiments on the OGBench
task suite, we empirically show that dual goal representations consistently
improve offline goal-reaching performance across 20 state- and pixel-based
tasks.

</details>


### [496] [Incorporating Expert Knowledge into Bayesian Causal Discovery of Mixtures of Directed Acyclic Graphs](https://arxiv.org/abs/2510.06735)
*Zachris Björkman,Jorge Loría,Sophie Wharrie,Samuel Kaski*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Bayesian causal discovery benefits from prior information elicited from
domain experts, and in heterogeneous domains any prior knowledge would be badly
needed. However, so far prior elicitation approaches have assumed a single
causal graph and hence are not suited to heterogeneous domains. We propose a
causal elicitation strategy for heterogeneous settings, based on Bayesian
experimental design (BED) principles, and a variational mixture structure
learning (VaMSL) method -- extending the earlier differentiable Bayesian
structure learning (DiBS) method -- to iteratively infer mixtures of causal
Bayesian networks (CBNs). We construct an informative graph prior incorporating
elicited expert feedback in the inference of mixtures of CBNs. Our proposed
method successfully produces a set of alternative causal models (mixture
components or clusters), and achieves an improved structure learning
performance on heterogeneous synthetic data when informed by a simulated
expert. Finally, we demonstrate that our approach is capable of capturing
complex distributions in a breast cancer database.

</details>


### [497] [Function regression using the forward forward training and inferring paradigm](https://arxiv.org/abs/2510.06762)
*Shivam Padmani,Akshay Joshi*

Main category: cs.LG

TL;DR: 本文提出了一种使用前向前向算法进行函数回归（函数逼近）的新方法，并对单变量和多变量函数进行了评估。


<details>
  <summary>Details</summary>
Motivation: 前向前向学习算法是一种无需反向传播即可训练神经网络的新方法，适用于神经形态计算和神经网络的物理模拟。然而，目前该算法仅限于分类任务。本文旨在将前向前向算法扩展到函数回归任务。

Method: 提出了一种使用前向前向算法进行函数回归的新方法，并对该方法进行了评估，包括将其扩展到 Kolmogorov Arnold Networks 和 Deep Physical Neural Networks。

Result: 对所提出的前向前向函数回归方法在单变量和多变量函数上进行了评估，并进行了初步的研究以将其扩展到其他类型的神经网络。

Conclusion: 本文成功地将前向前向算法应用于函数回归任务，并为未来在该领域的研究奠定了基础。

Abstract: Function regression/approximation is a fundamental application of machine
learning. Neural networks (NNs) can be easily trained for function regression
using a sufficient number of neurons and epochs. The forward-forward learning
algorithm is a novel approach for training neural networks without
backpropagation, and is well suited for implementation in neuromorphic
computing and physical analogs for neural networks. To the best of the authors'
knowledge, the Forward Forward paradigm of training and inferencing NNs is
currently only restricted to classification tasks. This paper introduces a new
methodology for approximating functions (function regression) using the
Forward-Forward algorithm. Furthermore, the paper evaluates the developed
methodology on univariate and multivariate functions, and provides preliminary
studies of extending the proposed Forward-Forward regression to Kolmogorov
Arnold Networks, and Deep Physical Neural Networks.

</details>


### [498] [Modeling COVID-19 Dynamics in German States Using Physics-Informed Neural Networks](https://arxiv.org/abs/2510.06776)
*Phillip Rothenbeck,Sai Karthikeya Vemuri,Niklas Penzel,Joachim Denzler*

Main category: cs.LG

TL;DR: PINNs可用于流行病学建模，通过分析德国各州的COVID-19数据，估计了州级传播和恢复参数以及时间变化的再生数（R_t），并揭示了与疫苗接种率和疫情阶段相关的区域差异。


<details>
  <summary>Details</summary>
Motivation: 为了改进对COVID-19等疾病动态的量化建模和分析，特别是在结合有噪声的观测数据方面，并克服传统 SIR 等模型在直接整合此类数据方面的局限性。

Method: 使用物理信息神经网络（PINNs）解决 SIR 模型的逆问题，并利用德国罗伯特·科赫研究所（RKI）提供的感染数据，对德国所有联邦州进行为期三年的精细时空分析，估计了各州的传播和恢复参数以及时间变化的再生数（R_t）。

Result: 研究结果显示了各地区传播行为的显著差异，并揭示了这些差异与疫苗接种率以及与疫情主要阶段相关的时间模式之间的关联。

Conclusion: PINNs 在进行本地化、长期的流行病学建模方面具有实用价值，能够有效分析 COVID-19 疫情的动态变化。

Abstract: The COVID-19 pandemic has highlighted the need for quantitative modeling and
analysis to understand real-world disease dynamics. In particular, post hoc
analyses using compartmental models offer valuable insights into the
effectiveness of public health interventions, such as vaccination strategies
and containment policies. However, such compartmental models like SIR
(Susceptible-Infectious-Recovered) often face limitations in directly
incorporating noisy observational data. In this work, we employ
Physics-Informed Neural Networks (PINNs) to solve the inverse problem of the
SIR model using infection data from the Robert Koch Institute (RKI). Our main
contribution is a fine-grained, spatio-temporal analysis of COVID-19 dynamics
across all German federal states over a three-year period. We estimate
state-specific transmission and recovery parameters and time-varying
reproduction number (R_t) to track the pandemic progression. The results
highlight strong variations in transmission behavior across regions, revealing
correlations with vaccination uptake and temporal patterns associated with
major pandemic phases. Our findings demonstrate the utility of PINNs in
localized, long-term epidemiological modeling.

</details>


### [499] [Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness](https://arxiv.org/abs/2510.06790)
*Tavish McDonald,Bo Lei,Stanislav Fort,Bhavya Kailkhura,Brian Bartoldson*

Main category: cs.LG

TL;DR: 模型对对抗性的分布外（OOD）数据敏感，即使经过大量的训练和计算投入进行鲁棒化。本文提出，即使在攻击者可以访问梯度或多模态输入的情况下，推理计算也能带来好处，其核心思想是组合泛化能力，即通过其分布内（ID）组件来理解OOD数据，从而在对抗性OOD输入上遵守防御规范。该研究提出了“推理计算鲁棒性假设”（RICH），即当模型训练数据更好地反映被攻击数据的组件时，推理计算防御效果会更好。通过在视觉语言模型和攻击类型上的实证支持，研究发现，如果OOD数据的规范遵循能力能够通过组合泛化得到释放，那么测试时计算可以带来鲁棒性增益，而强化学习微调和长时间推理则不是关键因素。例如，通过提示增加对防御规范的重视可以降低基于梯度的多模态攻击在经过对抗性预训练的VLM上的成功率，但这种干预对未经过鲁棒性处理的模型没有带来任何好处。这种推理计算鲁棒性增益与基础模型鲁棒性之间的相关性是RICH的“富者愈富”动态：对于经过鲁棒性处理的模型，被攻击数据的组件更偏向于ID，从而有助于组合泛化到OOD数据。因此，建议将训练时和测试时的防御措施分层叠加，以获得协同效益。


<details>
  <summary>Details</summary>
Motivation: 尽管在模型的鲁棒性方面投入了大量的训练和计算资源，模型仍然容易受到对抗性的分布外（OOD）数据的攻击。Zaremba等人（2025）的研究在测试阶段解决了这个问题，但其益处在攻击者可以访问梯度或多模态输入时会减弱。本文旨在解决这一差距，并阐明推理计算即使在这些情况下也能带来好处。

Method: 提出“推理计算鲁棒性假设”（RICH），即推理计算防御的效果随着模型训练数据对被攻击数据组件的反映程度而增强。通过在视觉语言模型和各种攻击类型上进行实证研究来支持这一假设。研究了组合泛化、强化学习微调和长时间推理在其中的作用。

Result: 实证研究表明，当组合泛化能够实现OOD数据的规范遵循时，测试时的计算可以带来鲁棒性增益。然而，强化学习微调和长时间推理并非关键因素。增强防御规范的提示可以降低基于梯度的多模态攻击在经过对抗性预训练的VLM上的成功率，但对未经过鲁棒性处理的模型无效。推理计算的鲁棒性增益与基础模型的鲁棒性之间存在相关性，即“富者愈富”效应。

Conclusion: 建议将训练时和测试时的防御措施结合起来，以获得协同效益。推理计算的鲁棒性增益与基础模型的鲁棒性以及组合泛化能力密切相关。

Abstract: Models are susceptible to adversarially out-of-distribution (OOD) data
despite large training-compute investments into their robustification. Zaremba
et al. (2025) make progress on this problem at test time, showing LLM reasoning
improves satisfaction of model specifications designed to thwart attacks,
resulting in a correlation between reasoning effort and robustness to
jailbreaks. However, this benefit of test compute fades when attackers are
given access to gradients or multimodal inputs. We address this gap, clarifying
that inference-compute offers benefits even in such cases. Our approach argues
that compositional generalization, through which OOD data is understandable via
its in-distribution (ID) components, enables adherence to defensive
specifications on adversarially OOD inputs. Namely, we posit the Robustness
from Inference Compute Hypothesis (RICH): inference-compute defenses profit as
the model's training data better reflects the attacked data's components. We
empirically support this hypothesis across vision language model and attack
types, finding robustness gains from test-time compute if specification
following on OOD data is unlocked by compositional generalization, while RL
finetuning and protracted reasoning are not critical. For example, increasing
emphasis on defensive specifications via prompting lowers the success rate of
gradient-based multimodal attacks on VLMs robustified by adversarial
pretraining, but this same intervention provides no such benefit to
not-robustified models. This correlation of inference-compute's robustness
benefit with base model robustness is the rich-get-richer dynamic of the RICH:
attacked data components are more ID for robustified models, aiding
compositional generalization to OOD data. Accordingly, we advise layering
train-time and test-time defenses to obtain their synergistic benefit.

</details>


### [500] [The Unreasonable Effectiveness of Randomized Representations in Online Continual Graph Learning](https://arxiv.org/abs/2510.06819)
*Giovanni Donghi,Daniele Zambon,Luca Pasa,Cesare Alippi,Nicolò Navarin*

Main category: cs.LG

TL;DR: 通过冻结固定编码器并仅在线训练轻量级分类器，可以有效解决在线持续图学习中的灾难性遗忘问题，实现了稳定且富有表现力的节点嵌入，并在多个基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在线持续图学习（OCGL）面临的主要挑战是灾难性遗忘，因为节点会逐个到达，分布会发生漂移，并且无法进行任务特定的子图的离线训练。

Method: 使用固定的、随机初始化的编码器来生成节点嵌入，仅在线训练一个轻量级的分类器。通过冻结编码器来消除表示参数的漂移，从而获得表达丰富且稳定的嵌入。

Result: 在多个OCGL基准测试中，该方法在没有内存缓冲区的情况下，始终优于最先进的方法，性能提升高达30%，并且表现接近联合离线训练的上限。

Conclusion: 在OCGL中，可以通过采用结构简单和稳定性来实现灾难性遗忘的最小化，而无需复杂的重放或正则化。

Abstract: Catastrophic forgetting is one of the main obstacles for Online Continual
Graph Learning (OCGL), where nodes arrive one by one, distribution drifts may
occur at any time and offline training on task-specific subgraphs is not
feasible. In this work, we explore a surprisingly simple yet highly effective
approach for OCGL: we use a fixed, randomly initialized encoder to generate
robust and expressive node embeddings by aggregating neighborhood information,
training online only a lightweight classifier. By freezing the encoder, we
eliminate drifts of the representation parameters, a key source of forgetting,
obtaining embeddings that are both expressive and stable. When evaluated across
several OCGL benchmarks, despite its simplicity and lack of memory buffer, this
approach yields consistent gains over state-of-the-art methods, with surprising
improvements of up to 30% and performance often approaching that of the joint
offline-training upper bound. These results suggest that in OCGL, catastrophic
forgetting can be minimized without complex replay or regularization by
embracing architectural simplicity and stability.

</details>


### [501] [Efficient numeracy in language models through single-token number embeddings](https://arxiv.org/abs/2510.06824)
*Linus Kreitner,Paul Hager,Jonathan Mengedoht,Georgios Kaissis,Daniel Rueckert,Martin J. Menten*

Main category: cs.LG

TL;DR: LLMs在处理数值数据和长计算方面存在局限性，需要外部工具或复杂的推理。本文提出了一种名为BitTokens的新型数字编码策略，将数字表示为单个令牌（使用IEEE 754二进制浮点表示），以提高LLMs处理数值计算的效率。实验表明，BitTokens能够显著提升LLMs（即使是小型模型）的算术运算能力，有望扩展LLMs能解决的问题的长度和复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的LLMs在处理数值数据和执行长计算时效率低下，需要依赖外部工具或冗长的推理过程，这限制了它们在科学和工程领域的应用。LLMs的数字分词策略（将单个数字拆分为多个令牌）加剧了这个问题，因此需要一种更有效的数字编码方式。

Method: 提出了一种名为BitTokens的新型数字编码策略，将数字的IEEE 754二进制浮点表示嵌入单个令牌中。

Result: 通过大量实验证明，BitTokens能够让语言模型（即使是小型模型）在算术运算方面达到近乎完美的准确率，显著提高了计算效率。

Conclusion: BitTokens是一种创新的数字编码策略，能够有效解决LLMs在处理数值计算时的效率和准确性问题，有望扩展LLMs在科学和工程领域解决复杂问题的能力。

Abstract: To drive progress in science and engineering, large language models (LLMs)
must be able to process large amounts of numerical data and solve long
calculations efficiently. This is currently only possible through the use of
external tools or extensive reasoning chains, either limiting the numerical
intuition of LLMs or limiting the length of problems they can solve. We show
that frontier LLMs require excessive amounts of reasoning tokens to solve even
basic calculations, which is exacerbated by their tokenization strategies that
split single numbers into multiple tokens. This motivates the need for
efficient and effective single-token number encodings. We introduce a set of
desiderata for such encodings and show that existing approaches fail to fulfill
them. To address these shortcomings, we propose BitTokens, a novel tokenization
strategy that embeds any number into a single token using its IEEE 754 binary
floating-point representation. Through extensive experiments we show that our
BitTokens allow even small language models to learn algorithms that solve basic
arithmetic operations nearly perfectly. This newly gained efficiency could
expand the length and complexity of problems language models can solve.

</details>


### [502] [Recurrence-Complete Frame-based Action Models](https://arxiv.org/abs/2510.06828)
*Michael Keiblinger*

Main category: cs.LG

TL;DR: Attention-based models, while successful, may not be sufficient for long-running agentic tasks due to limitations in representing certain problem classes. This paper introduces a recurrence-complete architecture that addresses these limitations, showing improved performance with longer training sequences.


<details>
  <summary>Details</summary>
Motivation: The paper challenges the prevailing view that RNNs are unnecessary with attention mechanisms, arguing that fully parallelizable architectures have limitations for long-running agentic tasks and may fail beyond a critical time t. It aims to address these limitations by proposing a recurrence-complete architecture.

Method: The paper introduces a recurrence-complete architecture and trains it on action sequences derived from GitHub. It analyzes the relationship between loss, training sequence length, and wall-time cost.

Result: The study found that loss follows a power law in relation to the trained sequence length, even with a fixed parameter count. Furthermore, longer training sequences consistently amortize their increasing wall-time cost, resulting in lower loss as a function of time.

Conclusion: The paper concludes that a recurrence-complete architecture, as opposed to purely attention-based models, is necessary to effectively handle long-running agentic tasks. The proposed architecture demonstrates improved performance and efficiency with extended training, suggesting a viable direction for future research in agentic systems.

Abstract: In recent years, attention-like mechanisms have been used to great success in
the space of large language models, unlocking scaling potential to a previously
unthinkable extent. "Attention Is All You Need" famously claims RNN cells are
not needed in conjunction with attention. We challenge this view. In this
paper, we point to existing proofs that architectures with fully parallelizable
forward or backward passes cannot represent classes of problems specifically
interesting for long-running agentic tasks. We further conjecture a critical
time t beyond which non-recurrence-complete models fail to aggregate inputs
correctly, with concrete implications for agentic systems (e.g., software
engineering agents). To address this, we introduce a recurrence-complete
architecture and train it on GitHub-derived action sequences. Loss follows a
power law in the trained sequence length while the parameter count remains
fixed. Moreover, longer-sequence training always amortizes its linearly
increasing wall-time cost, yielding lower loss as a function of wall time.

</details>


### [503] [CNN-TFT explained by SHAP with multi-head attention weights for time series forecasting](https://arxiv.org/abs/2510.06840)
*Stefano F. Stefenon,João P. Matos-Carvalho,Valderi R. Q. Leithardt,Kin-Choong Yow*

Main category: cs.LG

TL;DR: 该论文提出了一种结合卷积神经网络（CNN）和时间融合 transformer（TFT）的混合模型（CNN-TFT-SHAP-MHAW），用于多元时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 为了结合CNN捕捉局部模式和TFT建模长期依赖性的优势，以提高多元时间序列预测的准确性。

Method: 首先使用CNN提取局部特征并降维，然后将特征输入TFT模型，利用多头注意力机制捕捉短期和长期依赖性，并自适应地加权相关协变量。最后，使用SHAP-MHAW方法来解释模型的可解释性。

Result: CNN-TFT模型在水力发电自然流量时间序列数据集上的实验结果优于其他深度学习模型，平均绝对百分比误差（MAPE）最高可达2.2%。

Conclusion: 提出的CNN-TFT-SHAP-MHAW模型在需要高精度多元时间序列预测的应用中具有潜力。

Abstract: Convolutional neural networks (CNNs) and transformer architectures offer
strengths for modeling temporal data: CNNs excel at capturing local patterns
and translational invariances, while transformers effectively model long-range
dependencies via self-attention. This paper proposes a hybrid architecture
integrating convolutional feature extraction with a temporal fusion transformer
(TFT) backbone to enhance multivariate time series forecasting. The CNN module
first applies a hierarchy of one-dimensional convolutional layers to distill
salient local patterns from raw input sequences, reducing noise and
dimensionality. The resulting feature maps are then fed into the TFT, which
applies multi-head attention to capture both short- and long-term dependencies
and to weigh relevant covariates adaptively. We evaluate the CNN-TFT on a
hydroelectric natural flow time series dataset. Experimental results
demonstrate that CNN-TFT outperforms well-established deep learning models,
with a mean absolute percentage error of up to 2.2%. The explainability of the
model is obtained by a proposed Shapley additive explanations with multi-head
attention weights (SHAP-MHAW). Our novel architecture, named CNN-TFT-SHAP-MHAW,
is promising for applications requiring high-fidelity, multivariate time series
forecasts, being available for future analysis at
https://github.com/SFStefenon/CNN-TFT-SHAP-MHAW .

</details>


### [504] [Enhancing Bankruptcy Prediction of Banks through Advanced Machine Learning Techniques: An Innovative Approach and Analysis](https://arxiv.org/abs/2510.06852)
*Zuherman Rustam,Sri Hartini,Sardar M. N. Islam,Fevi Novkaniza,Fiftitah R. Aszhari,Muhammad Rifqi*

Main category: cs.LG

TL;DR: 本研究旨在利用机器学习技术（逻辑回归、随机森林、支持向量机）预测银行破产风险，并与传统统计模型进行比较，以提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 银行系统性的风险可能破坏金融系统的稳定，因此，计算银行破产的可能性对于维护银行系统的安全至关重要。现有的统计模型（如 Altman Z-Score）依赖于僵化的假设，可能导致预测准确性低，因此需要新的方法。

Method: 本研究采用逻辑回归（LR）、随机森林（RF）和支持向量机（SVM）等机器学习技术来构建银行破产预测模型。研究使用了土耳其商业银行（1994-2004）和印度尼西亚农村银行（2013-2019）的数据集。

Result: 研究结果表明，随机森林（RF）模型在预测土耳其商业银行数据时准确率达到90%。此外，所提出的三种机器学习方法能够准确预测印度尼西亚农村银行的破产可能性。

Conclusion: 本研究提出的创新性机器学习方法有助于制定减少银行破产成本的政策。

Abstract: Context: Financial system stability is determined by the condition of the
banking system. A bank failure can destroy the stability of the financial
system, as banks are subject to systemic risk, affecting not only individual
banks but also segments or the entire financial system. Calculating the
probability of a bank going bankrupt is one way to ensure the banking system is
safe and sound. Existing literature and limitations: Statistical models, such
as Altman's Z-Score, are one of the common techniques for developing a
bankruptcy prediction model. However, statistical methods rely on rigid and
sometimes irrelevant assumptions, which can result in low forecast accuracy.
New approaches are necessary. Objective of the research: Bankruptcy models are
developed using machine learning techniques, such as logistic regression (LR),
random forest (RF), and support vector machines (SVM). According to several
studies, machine learning is also more accurate and effective than statistical
methods for categorising and forecasting banking risk management. Present
Research: The commercial bank data are derived from the annual financial
statements of 44 active banks and 21 bankrupt banks in Turkey from 1994 to
2004, and the rural bank data are derived from the quarterly financial reports
of 43 active and 43 bankrupt rural banks in Indonesia between 2013 and 2019.
Five rural banks in Indonesia have also been selected to demonstrate the
feasibility of analysing bank bankruptcy trends. Findings and implications: The
results of the research experiments show that RF can forecast data from
commercial banks with a 90% accuracy rate. Furthermore, the three machine
learning methods proposed accurately predict the likelihood of rural bank
bankruptcy. Contribution and Conclusion: The proposed innovative machine
learning approach help to implement policies that reduce the costs of
bankruptcy.

</details>


### [505] [Towards Generalization of Graph Neural Networks for AC Optimal Power Flow](https://arxiv.org/abs/2510.06860)
*Olayiwola Arowolo,Jochen L. Cremer*

Main category: cs.LG

TL;DR: HH-MPNN通过结合不同类型的节点/边以及可扩展的Transformer模型，实现了AC最优潮流计算的计算加速和拓扑适应性，在各种规模和拓扑的电网中均表现出优异的性能。


<details>
  <summary>Details</summary>
Motivation: AC最优潮流计算在大型电力系统中计算成本高昂，传统求解器耗时过长，而机器学习方法在可扩展性和拓扑适应性方面存在不足，需要昂贵的重新训练。

Method: 提出了一种混合异构消息传递神经网络（HH-MPNN），将母线、发电机、负荷、分流器、输电线路和变压器建模为不同的节点或边类型，并结合了可扩展的Transformer模型来处理长距离依赖关系。

Result: 在14到2000个母线的电网中，HH-MPNN在默认拓扑下实现了不到1%的最优性差距。在未见过数千种拓扑的零样本应用中，HH-MPNN实现了不到3%的最优性差距。在较小电网上进行预训练也能提高在大电网上的结果。与内点法求解器相比，计算速度提高了1000到10000倍。

Conclusion: HH-MPNN的这些结果推动了用于实时电力系统运行的实用、可泛化的机器学习技术的发展。

Abstract: AC Optimal Power Flow (ACOPF) is computationally expensive for large-scale
power systems, with conventional solvers requiring prohibitive solution times.
Machine learning approaches offer computational speedups but struggle with
scalability and topology adaptability without expensive retraining. To enable
scalability across grid sizes and adaptability to topology changes, we propose
a Hybrid Heterogeneous Message Passing Neural Network (HH-MPNN). HH-MPNN models
buses, generators, loads, shunts, transmission lines and transformers as
distinct node or edge types, combined with a scalable transformer model for
handling long-range dependencies. On grids from 14 to 2,000 buses, HH-MPNN
achieves less than 1% optimality gap on default topologies. Applied zero-shot
to thousands of unseen topologies, HH-MPNN achieves less than 3% optimality gap
despite training only on default topologies. Pre-training on smaller grids also
improves results on a larger grid. Computational speedups reach 1,000x to
10,000x compared to interior point solvers. These results advance practical,
generalizable machine learning for real-time power system operations.

</details>


### [506] [SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models](https://arxiv.org/abs/2510.06871)
*Huahui Yi,Kun Wang,Qiankun Li,Miao Yu,Liang Lin,Gongli Xi,Hao Wu,Xuming Hu,Kang Li,Yang Liu*

Main category: cs.LG

TL;DR: MLRMs在跨模态推理方面表现出色，但容易在对抗性或不安全提示下放大安全风险，这被称为“推理税”。本文提出了SaFeR-VLM，一个将安全嵌入多模态推理的框架，通过QI-Safe-10K数据集、安全感知回滚、结构化奖励建模和GRPO优化来解决这个问题，从而实现可扩展、可泛化的安全推理，并超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的MLRM防御机制主要作用于输出层面，未能约束推理过程，导致模型在面对隐性风险时仍易受攻击。需要一种将安全直接嵌入多模态推理过程的框架。

Method: 本文提出的SaFeR-VLM框架包含四个主要组成部分：(I) QI-Safe-10K数据集，包含安全关键和推理敏感的案例；(II) 安全感知回滚，对不安全生成进行反思和修正而非丢弃；(III) 结构化奖励建模，采用多维度加权标准，并对幻觉和矛盾进行明确惩罚；(IV) GRPO优化，以增强安全和修正后的轨迹。

Result: SaFeR-VLM-3B在六个基准测试中的安全性和有用性方面平均得分分别为70.13和78.97，优于同规模及更大规模的模型。SaFeR-VLM-7B在安全指标上超越了GPT-5-mini和Gemini-2.5-Flash，且有用性未降低。

Conclusion: SaFeR-VLM框架成功地将安全从被动保障转变为推理的主动驱动力，实现了可扩展且可泛化的安全推理。该框架能够应对显性和隐性风险，并支持动态、可解释的安全决策，超越了表面过滤的限制。

Abstract: Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal
reasoning but often amplify safety risks under adversarial or unsafe prompts, a
phenomenon we call the \textit{Reasoning Tax}. Existing defenses mainly act at
the output level and do not constrain the reasoning process, leaving models
exposed to implicit risks. In this paper, we propose SaFeR-VLM, a
safety-aligned reinforcement learning framework that embeds safety directly
into multimodal reasoning. The framework integrates four components: (I)
QI-Safe-10K, a curated dataset emphasizing safety-critical and
reasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations
undergo reflection and correction instead of being discarded; (III) structured
reward modeling with multi-dimensional weighted criteria and explicit penalties
for hallucinations and contradictions; and (IV) GRPO optimization, which
reinforces both safe and corrected trajectories. This unified design shifts
safety from a passive safeguard to an active driver of reasoning, enabling
scalable and generalizable safety-aware reasoning. SaFeR-VLM further
demonstrates robustness against both explicit and implicit risks, supporting
dynamic and interpretable safety decisions beyond surface-level filtering.
SaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and
helpfulness across six benchmarks, surpassing both same-scale and $>10\times$
larger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B.
Remarkably, SaFeR-VLM-7B benefits from its increased scale to surpass
GPT-5-mini and Gemini-2.5-Flash by \num{6.47} and \num{16.76} points
respectively on safety metrics, achieving this improvement without any
degradation in helpfulness performance. Our codes are available at
https://github.com/HarveyYi/SaFeR-VLM.

</details>


### [507] [MoRE-GNN: Multi-omics Data Integration with a Heterogeneous Graph Autoencoder](https://arxiv.org/abs/2510.06880)
*Zhiyu Wang,Sonia Koszut,Pietro Liò,Francesco Ceccarelli*

Main category: cs.LG

TL;DR: MoRE-GNN是一个异构图自编码器，用于整合多组学单细胞数据，通过图卷积和注意力机制构建关系图，并在下游任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 整合多组学单细胞数据具有挑战性，因为高维度和复杂的跨模态关系。

Method: 提出MoRE-GNN，一种异构图自编码器，结合图卷积和注意力机制，直接从数据中动态构建关系图。

Result: 在六个公开数据集上进行评估，MoRE-GNN能捕捉有意义的生物学关系，并在跨模态相关性强的设置下优于现有方法，同时学习到的表示可用于准确的下游跨模态预测。

Conclusion: MoRE-GNN提供了一个自适应、可扩展且可解释的框架，用于推进多组学整合，尽管性能可能因数据集复杂性而异。

Abstract: The integration of multi-omics single-cell data remains challenging due to
high-dimensionality and complex inter-modality relationships. To address this,
we introduce MoRE-GNN (Multi-omics Relational Edge Graph Neural Network), a
heterogeneous graph autoencoder that combines graph convolution and attention
mechanisms to dynamically construct relational graphs directly from data.
Evaluations on six publicly available datasets demonstrate that MoRE-GNN
captures biologically meaningful relationships and outperforms existing
methods, particularly in settings with strong inter-modality correlations.
Furthermore, the learned representations allow for accurate downstream
cross-modal predictions. While performance may vary with dataset complexity,
MoRE-GNN offers an adaptive, scalable and interpretable framework for advancing
multi-omics integration.

</details>


### [508] [Angular Constraint Embedding via SpherePair Loss for Constrained Clustering](https://arxiv.org/abs/2510.06907)
*Shaojie Zhang,Ke Chen*

Main category: cs.LG

TL;DR: SpherePair是一种新颖的深度约束聚类方法，通过角度约束嵌入来解决现有方法的局限性，提高了可扩展性和实际应用性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度约束聚类方法要么受限于端到端模型固有的锚点，要么难以学习可区分的欧氏嵌入，这限制了它们的可扩展性和实际应用性。

Method: 提出了一种新颖的角度约束嵌入方法，称为SpherePair，它使用具有几何形式的SpherePair损失来编码成对约束，并产生在角度空间中有利于聚类的嵌入，有效地将表示学习与聚类分离开来。

Result: SpherePair能够保留成对关系而不产生冲突，无需指定确切的簇数，能够泛化到未见过的数据，能够快速推断簇的数量，并具有严格的理论保证。在各种基准测试中与最先进的DCC方法进行比较评估，并对理论见解进行实证验证，都证实了其优越的性能、可扩展性和整体实际有效性。

Conclusion: SpherePair在解决现有深度约束聚类方法的局限性方面表现出色，通过其新颖的角度约束嵌入方法，在保留成对关系、灵活性和可扩展性方面展现出优越的性能和实际应用潜力。

Abstract: Constrained clustering integrates domain knowledge through pairwise
constraints. However, existing deep constrained clustering (DCC) methods are
either limited by anchors inherent in end-to-end modeling or struggle with
learning discriminative Euclidean embedding, restricting their scalability and
real-world applicability. To avoid their respective pitfalls, we propose a
novel angular constraint embedding approach for DCC, termed SpherePair. Using
the SpherePair loss with a geometric formulation, our method faithfully encodes
pairwise constraints and leads to embeddings that are clustering-friendly in
angular space, effectively separating representation learning from clustering.
SpherePair preserves pairwise relations without conflict, removes the need to
specify the exact number of clusters, generalizes to unseen data, enables rapid
inference of the number of clusters, and is supported by rigorous theoretical
guarantees. Comparative evaluations with state-of-the-art DCC methods on
diverse benchmarks, along with empirical validation of theoretical insights,
confirm its superior performance, scalability, and overall real-world
effectiveness. Code is available at
\href{https://github.com/spherepaircc/SpherePairCC/tree/main}{our repository}.

</details>


### [509] [Vacuum Spiker: A Spiking Neural Network-Based Model for Efficient Anomaly Detection in Time Series](https://arxiv.org/abs/2510.06910)
*Iago Xabier Vázquez,Javier Sedano,Muhammad Afzal,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: Vacuum Spiker算法是一种基于脉冲神经网络的新型时间序列异常检测方法，通过引入新的检测标准、训练方法和编码方案，在保证性能的同时显著降低能耗，并成功应用于实际场景。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在时间序列异常检测中表现优异，但能耗高，限制了其在资源受限环境下的应用。因此，需要开发能耗更低、效率更高的异常检测方法。

Method: 提出了一种名为Vacuum Spiker的脉冲神经网络算法，该算法包含新的检测标准（基于神经活动全局变化而非重构或预测误差）、利用脉冲时间依赖可塑性进行训练以在异常发生时诱导神经活动变化，以及一种高效的编码方案（将输入空间离散化并分配给单个神经元，每个时间步仅用一个脉冲编码）。

Result: 在公开数据集上的实验结果表明，Vacuum Spiker算法的性能具有竞争力，同时能耗显著低于多种深度学习和机器学习基线方法。在太阳能逆变器电力削减事件的实际案例研究中，该模型也成功识别了异常事件。

Conclusion: Vacuum Spiker算法在保证性能的同时显著降低了能耗，在时间序列异常检测方面展现出可持续和高效的潜力，适用于资源受限的环境。

Abstract: Anomaly detection is a key task across domains such as industry, healthcare,
and cybersecurity. Many real-world anomaly detection problems involve analyzing
multiple features over time, making time series analysis a natural approach for
such problems. While deep learning models have achieved strong performance in
this field, their trend to exhibit high energy consumption limits their
deployment in resource-constrained environments such as IoT devices, edge
computing platforms, and wearables. To address this challenge, this paper
introduces the \textit{Vacuum Spiker algorithm}, a novel Spiking Neural
Network-based method for anomaly detection in time series. It incorporates a
new detection criterion that relies on global changes in neural activity rather
than reconstruction or prediction error. It is trained using Spike
Time-Dependent Plasticity in a novel way, intended to induce changes in neural
activity when anomalies occur. A new efficient encoding scheme is also
proposed, which discretizes the input space into non-overlapping intervals,
assigning each to a single neuron. This strategy encodes information with a
single spike per time step, improving energy efficiency compared to
conventional encoding methods. Experimental results on publicly available
datasets show that the proposed algorithm achieves competitive performance
while significantly reducing energy consumption, compared to a wide set of deep
learning and machine learning baselines. Furthermore, its practical utility is
validated in a real-world case study, where the model successfully identifies
power curtailment events in a solar inverter. These results highlight its
potential for sustainable and efficient anomaly detection.

</details>


### [510] [Utilizing Large Language Models for Machine Learning Explainability](https://arxiv.org/abs/2510.06912)
*Alexandros Vassiliades,Nikolaos Polatidis,Stamatios Samaras,Sotiris Diplaris,Ignacio Cabrera Martin,Yannis Manolopoulos,Stefanos Vrochidis,Ioannis Kompatsiaris*

Main category: cs.LG

TL;DR: 大型语言模型（LLM）可用于自主生成可解释的机器学习（ML）解决方案，在预测驾驶员警觉状态和酵母数据集分类任务上，使用OpenAI GPT、Anthropic Claude和DeepSeek模型生成了随机森林、XGBoost、多层感知器和长短期记忆网络模型的训练流程，并通过SHAP评估了模型性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）在自主生成机器学习（ML）解决方案时的可解释性能力。

Method: 使用三种先进的LLMs（OpenAI GPT、Anthropic Claude和DeepSeek）设计了四种常见分类器（随机森林、XGBoost、多层感知器和长短期记忆网络）的训练流程，并针对驾驶员警觉状态二元分类和酵母数据集多标签分类两个任务进行了评估。使用SHAP（SHapley Additive exPlanations）评估了预测性能（召回率、精确率和F1分数）和可解释性（平均SHAP保真度和平均SHAP稀疏度）。

Result: 结果表明，LLMs能够生成有效的、可解释性强的模型，具有高保真度和一致的稀疏性，其性能与手动设计的基线模型相当。

Conclusion: LLMs有潜力成为可解释的ML流水线生成的自动化工具。

Abstract: This study explores the explainability capabilities of large language models
(LLMs), when employed to autonomously generate machine learning (ML) solutions.
We examine two classification tasks: (i) a binary classification problem
focused on predicting driver alertness states, and (ii) a multilabel
classification problem based on the yeast dataset. Three state-of-the-art LLMs
(i.e. OpenAI GPT, Anthropic Claude, and DeepSeek) are prompted to design
training pipelines for four common classifiers: Random Forest, XGBoost,
Multilayer Perceptron, and Long Short-Term Memory networks. The generated
models are evaluated in terms of predictive performance (recall, precision, and
F1-score) and explainability using SHAP (SHapley Additive exPlanations).
Specifically, we measure Average SHAP Fidelity (Mean Squared Error between SHAP
approximations and model outputs) and Average SHAP Sparsity (number of features
deemed influential). The results reveal that LLMs are capable of producing
effective and interpretable models, achieving high fidelity and consistent
sparsity, highlighting their potential as automated tools for interpretable ML
pipeline generation. The results show that LLMs can produce effective,
interpretable pipelines with high fidelity and consistent sparsity, closely
matching manually engineered baselines.

</details>


### [511] [Revisiting Node Affinity Prediction in Temporal Graphs](https://arxiv.org/abs/2510.06940)
*Krishna Sri Ipsit Mantri,Or Feldman,Moshe Eliasof,Chaim Baskin*

Main category: cs.LG

TL;DR: NAVIS是一个节点亲和力预测模型，通过利用启发式方法与状态空间模型的等价性，并引入新的损失函数，在TGB数据集上超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 节点亲和力预测在时间图学习中具有广泛应用，但现有方法（如动态链接属性预测模型）不如简单启发式方法有效。

Method: 分析了当前时间图神经网络在节点亲和力预测中的训练挑战并提出解决方案，开发了NAVIS模型，并引入了新的损失函数。

Result: NAVIS模型在TGB数据集上取得了优于现有方法（包括启发式方法）的性能。

Conclusion: NAVIS通过解决现有时间图神经网络的训练挑战并引入新的损失函数，有效提升了节点亲和力预测的性能。

Abstract: Node affinity prediction is a common task that is widely used in temporal
graph learning with applications in social and financial networks, recommender
systems, and more. Recent works have addressed this task by adapting
state-of-the-art dynamic link property prediction models to node affinity
prediction. However, simple heuristics, such as Persistent Forecast or Moving
Average, outperform these models. In this work, we analyze the challenges in
training current Temporal Graph Neural Networks for node affinity prediction
and suggest appropriate solutions. Combining the solutions, we develop NAViS -
Node Affinity prediction model using Virtual State, by exploiting the
equivalence between heuristics and state space models. While promising,
training NAViS is non-trivial. Therefore, we further introduce a novel loss
function for node affinity prediction. We evaluate NAViS on TGB and show that
it outperforms the state-of-the-art, including heuristics. Our source code is
available at https://github.com/orfeld415/NAVIS

</details>


### [512] [Fisher Information, Training and Bias in Fourier Regression Models](https://arxiv.org/abs/2510.06945)
*Lorenzo Pastori,Veronika Eyring,Mierk Schwabe*

Main category: cs.LG

TL;DR: 量子神经网络（QNN）的训练和预测性能可以通过基于费舍尔信息矩阵（FIM）的评估指标来预测。


<details>
  <summary>Details</summary>
Motivation: 研究基于费舍尔信息矩阵（FIM）的评估指标在预测量子神经网络（QNN）的训练和预测性能方面的有效性。

Method: 通过利用一类QNN与傅里叶模型之间的等价性，研究了有效维度和模型偏差对模型训练和性能的影响，并推导了傅里叶模型的FIM解析表达式，确定了控制模型有效维度的特征，并引入了张量网络表示。

Result: 对于完全无偏的模型，较高的有效维度可以提高可训练性和性能；对于有偏的模型，较低的有效维度有利于训练。模型有效维度和偏差的可调性得到了实现和比较。

Conclusion: 模型训练和性能受几何性质、模型-任务对齐和训练的相互作用影响，这些发现对机器学习领域具有广泛意义。

Abstract: Motivated by the growing interest in quantum machine learning, in particular
quantum neural networks (QNNs), we study how recently introduced evaluation
metrics based on the Fisher information matrix (FIM) are effective for
predicting their training and prediction performance. We exploit the
equivalence between a broad class of QNNs and Fourier models, and study the
interplay between the \emph{effective dimension} and the \emph{bias} of a model
towards a given task, investigating how these affect the model's training and
performance. We show that for a model that is completely agnostic, or unbiased,
towards the function to be learned, a higher effective dimension likely results
in a better trainability and performance. On the other hand, for models that
are biased towards the function to be learned a lower effective dimension is
likely beneficial during training. To obtain these results, we derive an
analytical expression of the FIM for Fourier models and identify the features
controlling a model's effective dimension. This allows us to construct models
with tunable effective dimension and bias, and to compare their training. We
furthermore introduce a tensor network representation of the considered Fourier
models, which could be a tool of independent interest for the analysis of QNN
models. Overall, these findings provide an explicit example of the interplay
between geometrical properties, model-task alignment and training, which are
relevant for the broader machine learning community.

</details>


### [513] [Grouped Differential Attention](https://arxiv.org/abs/2510.06949)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Wai Ting Cheung,Beomgyu Kim,Taehwan Kim,Haesol Lee,Junhyeok Lee,Dongpin Oh,Eunhwan Park*

Main category: cs.LG

TL;DR: GDA通过非对称分组和选择性头扩展来改进Transformer的自注意力机制，以提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自注意力机制存在效率低下和过度关注冗余信息的问题，而Differential Attention的头分配限制了灵活性和可扩展性。

Method: 提出Grouped Differential Attention (GDA)，采用非对称的头分配策略，将更多头分配给信号提取，较少头分配给噪声控制，并通过受控重复来稳定噪声控制。同时提出分组差异化增长策略，选择性复制信号头以实现可扩展性。

Result: GDA在中等不平衡比例下，相比对称基线，在泛化性和稳定性方面取得了显著的改进，且计算开销很小。

Conclusion: GDA通过感知比例的头分配和选择性扩展，为设计可扩展、计算高效的Transformer架构提供了一种有效且实用的方法。

Abstract: The self-attention mechanism, while foundational to modern Transformer
architectures, suffers from a critical inefficiency: it frequently allocates
substantial attention to redundant or noisy context. Differential Attention
addressed this by using subtractive attention maps for signal and noise, but
its required balanced head allocation imposes rigid constraints on
representational flexibility and scalability.
  To overcome this, we propose Grouped Differential Attention (GDA), a novel
approach that introduces unbalanced head allocation between signal-preserving
and noise-control groups. GDA significantly enhances signal focus by
strategically assigning more heads to signal extraction and fewer to
noise-control, stabilizing the latter through controlled repetition (akin to
GQA). This design achieves stronger signal fidelity with minimal computational
overhead. We further extend this principle to group-differentiated growth, a
scalable strategy that selectively replicates only the signal-focused heads,
thereby ensuring efficient capacity expansion.
  Through large-scale pretraining and continual training experiments, we
demonstrate that moderate imbalance ratios in GDA yield substantial
improvements in generalization and stability compared to symmetric baselines.
Our results collectively establish that ratio-aware head allocation and
selective expansion offer an effective and practical path toward designing
scalable, computation-efficient Transformer architectures.

</details>


### [514] [From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics](https://arxiv.org/abs/2510.06954)
*Zheng-An Chen,Tao Luo*

Main category: cs.LG

TL;DR: Transformer训练动力学在小初始化尺度下可被理论分析，表现为两个阶段：参数矩阵的非退化梯度动力学和键-查询矩阵的主导下的秩坍缩。


<details>
  <summary>Details</summary>
Motivation: 现有对Transformer训练动力学的理解局限于特定配置，缺乏普遍性理论基础。受语言模型在小初始化尺度下推理能力提升的启发，本文旨在理论上探究Transformer的训练动力学。

Method: 采用[Zhou et al. NeurIPS 2022]建立的梯度流分析框架，系统地研究线性化Transformer的训练动力学，并将注意力模块的动力学分解为两个阶段。

Result: 第一阶段，随机初始化的非对称权重扰动维持了参数矩阵的非退化梯度动力学，有助于摆脱小初始化区域；第二阶段，键-查询矩阵开始参与训练，驱动归一化矩阵趋向渐近秩坍缩。该两阶段框架可以推广经典的定向收敛结果。

Conclusion: Transformer的训练动力学可以通过梯度流分析框架在小初始化尺度下被理论化。该动力学可分为两个阶段：初始阶段的参数矩阵更新和后续阶段的键-查询矩阵驱动的秩坍缩。

Abstract: Although transformer-based models have shown exceptional empirical
performance, the fundamental principles governing their training dynamics are
inadequately characterized beyond configuration-specific studies. Inspired by
empirical evidence showing improved reasoning capabilities under small
initialization scales in language models, we employ the gradient flow
analytical framework established in [Zhou et al. NeurIPS 2022] to
systematically investigate linearized Transformer training dynamics. Our
theoretical analysis dissects the dynamics of attention modules into two
distinct stages. In the first stage, asymmetric weight perturbations from
random initialization sustain non-degenerate gradient dynamics in parameter
matrices, facilitating systematic escape from small initialization regimes.
Subsequently, these matrices undergo condensation, progressively aligning
toward the target orientation. In the second stage, the previously static
key-query matrices actively participate in training, driving the normalized
matrices toward asymptotic rank collapse. This two-stage framework generalizes
classical directional convergence results.

</details>


### [515] [High-Rate Mixout: Revisiting Mixout for Robust Domain Generalization](https://arxiv.org/abs/2510.06955)
*Masih Aminbeidokhti,Heitor Rapela Medeiros,Eric Granger,Marco Pedersoli*

Main category: cs.LG

TL;DR: Mixout是一种通过将部分微调权重替换为其预训练权重来缓解过拟合的随机正则化技术，它在保持模型适应性的同时保留了先验知识。在高遮蔽率下，Mixout在模型泛化能力和计算成本方面都表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的模型集成方法虽然能提高模型在分布变化下的鲁棒性，但计算成本高。Dropout作为一种轻量级替代方案，在预训练模型上容易过度正则化。因此，需要一种新的轻量级正则化技术来解决这些问题。

Method: Mixout是一种随机正则化技术，通过在训练过程中以一定的概率用预训练的权重替换掉一部分微调后的权重，来实现对模型的正则化。作者发现，在视觉变换器（ViT）上使用0.9，在ResNet上使用0.8的遮蔽率时，Mixout效果最好。

Result: 在高遮蔽率下，Mixout可以实现与模型集成相当的泛化能力，同时显著降低计算成本（梯度计算最多降低45%，梯度内存使用最多降低90%）。

Conclusion: Mixout，特别是高遮蔽率下的Mixout，是一种有效的模型泛化技术，它在保持模型性能的同时，显著降低了计算成本，为解决模型在分布变化下的泛化问题提供了一种有前景的解决方案。

Abstract: Ensembling fine-tuned models initialized from powerful pre-trained weights is
a common strategy to improve robustness under distribution shifts, but it comes
with substantial computational costs due to the need to train and store
multiple models. Dropout offers a lightweight alternative by simulating
ensembles through random neuron deactivation; however, when applied to
pre-trained models, it tends to over-regularize and disrupt critical
representations necessary for generalization. In this work, we investigate
Mixout, a stochastic regularization technique that provides an alternative to
Dropout for domain generalization. Rather than deactivating neurons, Mixout
mitigates overfitting by probabilistically swapping a subset of fine-tuned
weights with their pre-trained counterparts during training, thereby
maintaining a balance between adaptation and retention of prior knowledge. Our
study reveals that achieving strong performance with Mixout on domain
generalization benchmarks requires a notably high masking probability of 0.9
for ViTs and 0.8 for ResNets. While this may seem like a simple adjustment, it
yields two key advantages for domain generalization: (1) higher masking rates
more strongly penalize deviations from the pre-trained parameters, promoting
better generalization to unseen domains; and (2) high-rate masking
substantially reduces computational overhead, cutting gradient computation by
up to 45% and gradient memory usage by up to 90%. Experiments across five
domain generalization benchmarks, PACS, VLCS, OfficeHome, TerraIncognita, and
DomainNet, using ResNet and ViT architectures, show that our approach,
High-rate Mixout, achieves out-of-domain accuracy comparable to ensemble-based
methods while significantly reducing training costs.

</details>


### [516] [Revisiting Mixout: An Overlooked Path to Robust Finetuning](https://arxiv.org/abs/2510.06982)
*Masih Aminbeidokhti,Heitor Rapela Medeiros,Eric Granger,Marco Pedersoli*

Main category: cs.LG

TL;DR: GMixout是一种改进的Mixout正则化方法，通过动态调整锚点和掩码稀疏度来提高模型在分布变化下的鲁棒性，同时保持或提升了模型在特定领域内的准确性，且训练高效，可在消费级GPU上进行。


<details>
  <summary>Details</summary>
Motivation: 现有模型微调（finetuning）虽然能提高模型在特定领域内的准确率，但在分布变化下鲁棒性会下降。Mixout是一种随机正则化方法，可以缓解这个问题，但其效果受掩码锚点、重采样频率和掩码稀疏度等因素影响。

Method: 本文提出GMixout，对Mixout进行改进：1. 使用指数移动平均（EMA）快照作为动态锚点，取代固定的锚点。2. 引入重采样频率超参数，显式地控制掩码的更新周期。3. 提出稀疏核实现，只更新一小部分参数，实现训练高效且无推理开销。

Result: GMixout在多个基准测试（包括协变量偏移、数据损坏和类别不平衡等）上，如ImageNet/ImageNet-LT, DomainNet, iWildCam, CIFAR100-C，均取得了优于零样本（zero-shot）性能的领域内准确率，并且在分布变化下超越了Model Soups和参数高效微调（parameter-efficient finetuning）等基线方法。

Conclusion: GMixout通过引入动态锚点和显式重采样频率，有效提升了模型在分布变化下的鲁棒性，同时保持了良好的领域内准确率，并实现了高效的训练。

Abstract: Finetuning vision foundation models often improves in-domain accuracy but
comes at the cost of robustness under distribution shift. We revisit Mixout, a
stochastic regularizer that intermittently replaces finetuned weights with
their pretrained reference, through the lens of a single-run, weight-sharing
implicit ensemble. This perspective reveals three key levers that govern
robustness: the \emph{masking anchor}, \emph{resampling frequency}, and
\emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i)
replaces the fixed anchor with an exponential moving-average snapshot that
adapts during training, and (ii) regulates masking period via an explicit
resampling-frequency hyperparameter. Our sparse-kernel implementation updates
only a small fraction of parameters with no inference-time overhead, enabling
training on consumer-grade GPUs. Experiments on benchmarks covering covariate
shift, corruption, and class imbalance, ImageNet / ImageNet-LT, DomainNet,
iWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracy
beyond zero-shot performance while surpassing both Model Soups and strong
parameter-efficient finetuning baselines under distribution shift.

</details>


### [517] [Spiral Model Technique For Data Science & Machine Learning Lifecycle](https://arxiv.org/abs/2510.06987)
*Rohith Mahadevan*

Main category: cs.LG

TL;DR: 企业应采用螺旋技术来处理具有明确最终目标的数据科学项目，以提高灵活性、敏捷性和迭代方法。


<details>
  <summary>Details</summary>
Motivation: 传统的数据科学生命周期模型可能无法满足具有明确最终目标的企业问题的需求。

Method: 提出了一种名为螺旋技术的新技术，该技术强调灵活性、敏捷性和迭代方法，以适应具有明确最终目标的企业问题。

Result: 螺旋技术能够更好地适应企业问题，并提高数据科学项目的灵活性、敏捷性和迭代性。

Conclusion: 螺旋技术为具有明确最终目标的数据科学项目提供了一种有效的替代方法，可以提高业务流程的适应性和效率。

Abstract: Analytics play an important role in modern business. Companies adapt data
science lifecycles to their culture to seek productivity and improve their
competitiveness among others. Data science lifecycles are fairly an important
contributing factor to start and end a project that are data dependent. Data
science and Machine learning life cycles comprises of series of steps that are
involved in a project. A typical life cycle states that it is a linear or
cyclical model that revolves around. It is mostly depicted that it is possible
in a traditional data science life cycle to start the process again after
reaching the end of cycle. This paper suggests a new technique to incorporate
data science life cycle to business problems that have a clear end goal. A new
technique called spiral technique is introduced to emphasize versatility,
agility and iterative approach to business processes.

</details>


### [518] [Sharpness-Aware Data Generation for Zero-shot Quantization](https://arxiv.org/abs/2510.07018)
*Dung Hoang-Anh,Cuong Pham Trung Le,Jianfei Cai,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 本文提出了一种在零样本量化中考虑模型量化锐度的新方法，通过最大化重建损失梯度匹配来生成合成数据，以提高模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有零样本量化方法未考虑量化模型的锐度，而锐度与泛化能力相关。

Method: 提出一种新方法，在生成合成数据时考虑量化模型的锐度。具体来说，通过最大化合成数据和真实验证数据上的重建损失梯度匹配来实现锐度最小化，并提出了一种近似方法，通过匹配生成样本与其邻居之间的梯度来解决缺少真实验证集的问题。

Result: 在CIFAR-100和ImageNet数据集上的实验评估表明，所提出的方法在低比特量化设置下优于现有技术。

Conclusion: 所提出的方法通过在合成数据生成中考虑量化模型锐度，有效提高了零样本量化的性能，并在多个数据集上取得了优于现有技术的成果。

Abstract: Zero-shot quantization aims to learn a quantized model from a pre-trained
full-precision model with no access to original real training data. The common
idea in zero-shot quantization approaches is to generate synthetic data for
quantizing the full-precision model. While it is well-known that deep neural
networks with low sharpness have better generalization ability, none of the
previous zero-shot quantization works considers the sharpness of the quantized
model as a criterion for generating training data. This paper introduces a
novel methodology that takes into account quantized model sharpness in
synthetic data generation to enhance generalization. Specifically, we first
demonstrate that sharpness minimization can be attained by maximizing gradient
matching between the reconstruction loss gradients computed on synthetic and
real validation data, under certain assumptions. We then circumvent the problem
of the gradient matching without real validation set by approximating it with
the gradient matching between each generated sample and its neighbors.
Experimental evaluations on CIFAR-100 and ImageNet datasets demonstrate the
superiority of the proposed method over the state-of-the-art techniques in
low-bit quantization settings.

</details>


### [519] [Federated Unlearning in the Wild: Rethinking Fairness and Data Discrepancy](https://arxiv.org/abs/2510.07022)
*ZiHeng Huang,Di Wu,Jun Bai,Jiale Zhang,Sicong Cao,Ji Zhang,Yingjie Hu*

Main category: cs.LG

TL;DR: 联邦学习中的机器遗忘需要解决公平性和数据异构性问题。现有方法在现实场景下表现不佳，而我们提出的 FedCCCU 方法能够有效地解决这些挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦遗忘（FU）方法在公平性和对现实世界数据异构性的处理上存在不足。精确遗忘方法成本高昂，而近似方法可能对仅保留数据的客户端不公平。此外，大多数 FU 评估依赖于简化的合成数据，无法反映真实世界的复杂性。

Method: 本文首先对现有的 FU 方法在真实数据异构性和公平性条件下进行了全面的基准测试。然后，提出了一种新颖的、考虑公平性的 FU 方法——FedCCCU（Federated Cross-Client-Constrains Unlearning），以明确解决这两个挑战。

Result: 实验结果表明，现有方法在现实场景下的表现不佳，而 FedCCCU 方法在各方面都持续优于现有方法。

Conclusion: FedCCCU 为现实世界中的联邦遗忘提供了一个实用且可扩展的解决方案，有效解决了公平性和数据异构性问题，并在实验中取得了优于现有方法的性能。

Abstract: Machine unlearning is critical for enforcing data deletion rights like the
"right to be forgotten." As a decentralized paradigm, Federated Learning (FL)
also requires unlearning, but realistic implementations face two major
challenges. First, fairness in Federated Unlearning (FU) is often overlooked.
Exact unlearning methods typically force all clients into costly retraining,
even those uninvolved. Approximate approaches, using gradient ascent or
distillation, make coarse interventions that can unfairly degrade performance
for clients with only retained data. Second, most FU evaluations rely on
synthetic data assumptions (IID/non-IID) that ignore real-world heterogeneity.
These unrealistic benchmarks obscure the true impact of unlearning and limit
the applicability of current methods. We first conduct a comprehensive
benchmark of existing FU methods under realistic data heterogeneity and
fairness conditions. We then propose a novel, fairness-aware FU approach,
Federated Cross-Client-Constrains Unlearning (FedCCCU), to explicitly address
both challenges. FedCCCU offers a practical and scalable solution for
real-world FU. Experimental results show that existing methods perform poorly
in realistic settings, while our approach consistently outperforms them.

</details>


### [520] [Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration](https://arxiv.org/abs/2510.07035)
*Tengwei Song,Min Wu,Yuan Fang*

Main category: cs.LG

TL;DR: FlexMol是一个灵活的分子预训练框架，可以通过单模态输入学习统一的分子表示，并能生成缺失模态的特征，在分子性质预测任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有分子表示学习方法需要成对的2D和3D分子数据进行预训练，这在某些情况下难以满足，限制了其应用。FlexMol旨在克服这一限制，实现单模态输入并学习统一的分子表示。

Method: FlexMol采用独立的2D和3D分子数据模型，利用参数共享提高效率，并使用解码器生成缺失模态的特征，实现多阶段连续学习，使两种模态协同训练，同时保证单模态输入的鲁棒性。

Result: FlexMol在广泛的分子性质预测任务中取得了优越的性能，并且在数据不完整的情况下也表现出有效性。

Conclusion: FlexMol作为一个灵活的分子预训练框架，通过支持单模态输入和生成缺失模态特征，解决了现有方法的局限性，并在各种下游任务中取得了领先的性能。

Abstract: Molecular representation learning plays a crucial role in advancing
applications such as drug discovery and material design. Existing work
leverages 2D and 3D modalities of molecular information for pre-training,
aiming to capture comprehensive structural and geometric insights. However,
these methods require paired 2D and 3D molecular data to train the model
effectively and prevent it from collapsing into a single modality, posing
limitations in scenarios where a certain modality is unavailable or
computationally expensive to generate. To overcome this limitation, we propose
FlexMol, a flexible molecule pre-training framework that learns unified
molecular representations while supporting single-modality input. Specifically,
inspired by the unified structure in vision-language models, our approach
employs separate models for 2D and 3D molecular data, leverages parameter
sharing to improve computational efficiency, and utilizes a decoder to generate
features for the missing modality. This enables a multistage continuous
learning process where both modalities contribute collaboratively during
training, while ensuring robustness when only one modality is available during
inference. Extensive experiments demonstrate that FlexMol achieves superior
performance across a wide range of molecular property prediction tasks, and we
also empirically demonstrate its effectiveness with incomplete data. Our code
and data are available at https://github.com/tewiSong/FlexMol.

</details>


### [521] [COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference Optimization](https://arxiv.org/abs/2510.07043)
*Tian Qin,Felix Bai,Ting-Yao Hu,Raviteja Vemulapalli,Hema Swetha Koppula,Zhiyang Xu,Bowen Jin,Mert Cemri,Jiarui Lu,Zirui Wang,Meng Cao*

Main category: cs.LG

TL;DR: 该研究提出了COMPASS基准，用于评估LLM代理在复杂规划任务中的工具使用和用户偏好优化能力，特别是在旅行规划场景下。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的LLM代理需要掌握通过多轮交互进行战略性工具使用和用户偏好优化，以协助用户完成复杂的规划任务。

Method: 将旅行规划作为一个约束偏好优化问题，代理需要满足硬约束并优化软用户偏好。构建了一个包含交通、住宿和票务的美国20个国家公园的旅行数据库，并提供了一个模仿商业预订平台的工具生态系统。

Result: 评估结果表明，现有模型在满足约束方面表现良好，但在优化用户偏好方面存在不足（可接受-最优差距）。在涉及多服务（如航班和酒店）协调任务时，尤其是在开源模型上，性能会大幅下降（计划协调差距）。

Conclusion: COMPASS基准通过在实际用户导向的领域中进行推理和规划，直接衡量了代理在现实任务中优化用户偏好的能力，弥合了理论进步与实际应用之间的差距。

Abstract: Real-world large language model (LLM) agents must master strategic tool use
and user preference optimization through multi-turn interactions to assist
users with complex planning tasks. We introduce COMPASS (Constrained
Optimization through Multi-turn Planning and Strategic Solutions), a benchmark
that evaluates agents on realistic travel-planning scenarios. We cast travel
planning as a constrained preference optimization problem, where agents must
satisfy hard constraints while simultaneously optimizing soft user preferences.
To support this, we build a realistic travel database covering transportation,
accommodation, and ticketing for 20 U.S. National Parks, along with a
comprehensive tool ecosystem that mirrors commercial booking platforms.
Evaluating state-of-the-art models, we uncover two critical gaps: (i) an
acceptable-optimal gap, where agents reliably meet constraints but fail to
optimize preferences, and (ii) a plan-coordination gap, where performance
collapses on multi-service (flight and hotel) coordination tasks, especially
for open-source models. By grounding reasoning and planning in a practical,
user-facing domain, COMPASS provides a benchmark that directly measures an
agent's ability to optimize user preferences in realistic tasks, bridging
theoretical advances with real-world impact.

</details>


### [522] [Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models and Hyper-Parameter Optimisation](https://arxiv.org/abs/2510.07052)
*Aryan Golbaghi,Shuo Zhou*

Main category: cs.LG

TL;DR: 该研究提出了一种结合预训练表征和超参数优化的语音情感识别（SER）工作流。


<details>
  <summary>Details</summary>
Motivation: 利用预训练模型（如wav2vec2-base）结合自动超参数优化（HPO）来提升语音情感识别的性能，并评估不同HPO策略（GP-BO和TPE）的效率和效果。

Method: 使用SpeechBrain wav2vec2-base模型在IEMOCAP数据集上进行微调，并与GP-BO和TPE两种HPO策略结合，在德国EmoDB语料库上进行实验，以平衡类别准确率（BCA）为目标，评估HPO策略的性能。

Result: GP-BO在11分钟内达到0.96 BCA，TPE在15分钟内达到0.97 BCA。相比之下，网格搜索需要143次试验和1680分钟才能超过0.9 BCA。在跨语言泛化方面，在EmoDB上训练的HPO优化模型在CREMA-D和RAVDESS上的零样本准确率分别提高了0.25和0.26。

Conclusion: 结合预训练编码器和高效的HPO策略可以在普通的CPU上实现具有竞争力的语音情感识别性能，并且在跨语言任务上表现出良好的泛化能力。

Abstract: We propose a workflow for speech emotion recognition (SER) that combines
pre-trained representations with automated hyperparameter optimisation (HPO).
Using SpeechBrain wav2vec2-base model fine-tuned on IEMOCAP as the encoder, we
compare two HPO strategies, Gaussian Process Bayesian Optimisation (GP-BO) and
Tree-structured Parzen Estimators (TPE), under an identical four-dimensional
search space and 15-trial budget, with balanced class accuracy (BCA) on the
German EmoDB corpus as the objective. All experiments run on 8 CPU cores with
32 GB RAM. GP-BO achieves 0.96 BCA in 11 minutes, and TPE (Hyperopt
implementation) attains 0.97 in 15 minutes. In contrast, grid search requires
143 trials and 1,680 minutes to exceed 0.9 BCA, and the best AutoSpeech 2020
baseline reports only 0.85 in 30 minutes on GPU. For cross-lingual
generalisation, an EmoDB-trained HPO-tuned model improves zero-shot accuracy by
0.25 on CREMA-D and 0.26 on RAVDESS. Results show that efficient HPO with
pre-trained encoders delivers competitive SER on commodity CPUs. Source code to
this work is available at:
https://github.com/youngaryan/speechbrain-emotion-hpo.

</details>


### [523] [Blind Construction of Angular Power Maps in Massive MIMO Networks](https://arxiv.org/abs/2510.07071)
*Zheng Xing,Junting Chen*

Main category: cs.LG

TL;DR: 利用无监督学习和隐马尔可夫模型，在没有位置标签的情况下，仅使用大规模MIMO网络中的CSI数据构建角度功率图，实现了移动用户本地化。


<details>
  <summary>Details</summary>
Motivation: 传统的无线地图构建方法需要位置标记的CSI数据，这在实践中难以获取。而大规模MIMO网络中的CSI获取是一个挑战，因此需要一种无需位置标签即可构建无线地图的方法。

Method: 提出了一种基于大规模CSI数据的无监督角度功率图构建方法。利用隐马尔可夫模型连接移动用户的隐藏轨迹和CSI演化，从而估计用户位置并构建角度功率图。

Result: 在均匀直线移动和泊松分布基站的条件下，CRLB表明本地化误差可以趋于零。当基站分布在有限区域时，即使有无限的独立测量，误差也无法完全消除。在真实网络数据上，实现了18米的平均本地化误差。

Conclusion: 该方法能够有效地在没有位置标签的情况下，利用CSI数据构建角度功率图并进行用户本地化，为无线资源管理提供了新的解决方案。

Abstract: Channel state information (CSI) acquisition is a challenging problem in
massive multiple-input multiple-output (MIMO) networks. Radio maps provide a
promising solution for radio resource management by reducing online CSI
acquisition. However, conventional approaches for radio map construction
require location-labeled CSI data, which is challenging in practice. This paper
investigates unsupervised angular power map construction based on large
timescale CSI data collected in a massive MIMO network without location labels.
A hidden Markov model (HMM) is built to connect the hidden trajectory of a
mobile with the CSI evolution of a massive MIMO channel. As a result, the
mobile location can be estimated, enabling the construction of an angular power
map. We show that under uniform rectilinear mobility with Poisson-distributed
base stations (BSs), the Cramer-Rao Lower Bound (CRLB) for localization error
can vanish at any signal-to-noise ratios (SNRs), whereas when BSs are confined
to a limited region, the error remains nonzero even with infinite independent
measurements. Based on reference signal received power (RSRP) data collected in
a real multi-cell massive MIMO network, an average localization error of 18
meters can be achieved although measurements are mainly obtained from a single
serving cell.

</details>


### [524] [HTMformer: Hybrid Time and Multivariate Transformer for Time Series Forecasting](https://arxiv.org/abs/2510.07084)
*Tan Wang,Yun Wei Dong,Tao Zhang,Qi Wang*

Main category: cs.LG

TL;DR: Transformer在时间序列预测中表现出色，但存在过度强调时间依赖性、计算开销大等问题。本文提出混合时空多变量嵌入（HTME）方法，通过提取多变量特征增强嵌入层信息，生成多维度嵌入，使Transformer更好地理解序列。HTME结合了轻量级时间特征提取和多变量特征提取模块，以平衡模型复杂度和性能。基于HTME的HTMformer在八个真实世界数据集上的实验表明，该方法在准确性和效率方面均优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在时间序列预测中存在过度强调时间依赖性、计算开销大且性能提升不明显的问题。文章旨在通过改进嵌入层来增强Transformer模型的序列建模能力，提高预测准确性和效率。

Method: 提出混合时空多变量嵌入（HTME）方法，该方法包含一个轻量级时间特征提取模块和一个精心设计的多变量特征提取模块，以提取互补特征，生成多维度嵌入，从而为Transformer提供更丰富、更有意义的序列表示。将HTME与Transformer架构结合，提出HTMformer模型。

Result: 在八个真实世界数据集上的实验结果表明，HTMformer模型在准确性和效率方面均优于现有的基线方法。

Conclusion: HTME方法通过提取多变量特征增强了Transformer模型的序列表示能力，有效解决了现有Transformer模型在时间序列预测中的不足，实现了模型复杂度和性能之间的良好平衡，并在实际应用中取得了优于基线方法的性能。

Abstract: Transformer-based methods have achieved impressive results in time series
forecasting. However, existing Transformers still exhibit limitations in
sequence modeling as they tend to overemphasize temporal dependencies. This
incurs additional computational overhead without yielding corresponding
performance gains. We find that the performance of Transformers is highly
dependent on the embedding method used to learn effective representations. To
address this issue, we extract multivariate features to augment the effective
information captured in the embedding layer, yielding multidimensional
embeddings that convey richer and more meaningful sequence representations.
These representations enable Transformer-based forecasters to better understand
the series. Specifically, we introduce Hybrid Temporal and Multivariate
Embeddings (HTME). The HTME extractor integrates a lightweight temporal feature
extraction module with a carefully designed multivariate feature extraction
module to provide complementary features, thereby achieving a balance between
model complexity and performance. By combining HTME with the Transformer
architecture, we present HTMformer, leveraging the enhanced feature extraction
capability of the HTME extractor to build a lightweight forecaster. Experiments
conducted on eight real-world datasets demonstrate that our approach
outperforms existing baselines in both accuracy and efficiency.

</details>


### [525] [Non-Stationary Online Structured Prediction with Surrogate Losses](https://arxiv.org/abs/2510.07086)
*Shinsaku Sakaue,Han Bao,Yuzhou Cao*

Main category: cs.LG

TL;DR: 在线结构化预测的挑战在于非平稳环境下的界限问题。本文提出了一种新的界限形式 F_T + C(1 + P_T)，它依赖于比较器序列的累积替代损失 F_T 和路径长度 P_T，而不是时间 T。该方法结合了在线梯度下降 (OGD) 的动态回归界限和替代损失的利用，并为 OGD 提供了新的 Polyak 风格学习率，具有目标损失保证和良好的实证表现。此外，该方法还扩展到卷积 Fenchel-Young 损失，并被证明具有紧密的下界。


<details>
  <summary>Details</summary>
Motivation: 在线结构化预测在非平稳环境中面临挑战，因为固定的最佳估计器可能导致替代损失随时间 T 线性增长。需要新的界限来解决这个问题。

Method: 结合在线梯度下降 (OGD) 的动态回归界限和利用替代损失间隙的技术。引入了一种新的 Polyak 风格的学习率，为 OGD 提供目标损失保证。将方法扩展到更广泛的问题，如卷积 Fenchel-Young 损失。

Result: 证明了累积目标损失的界限形式为 F_T + C(1 + P_T)，其中 F_T 是比较器序列的累积替代损失，P_T 是其路径长度，C 是常数。该界限仅依赖于 F_T 和 P_T，在非平稳环境中提供了比传统界限更强的保证。所提出的 OGD 学习率在实证上表现良好。

Conclusion: 所提出的界限形式 F_T + C(1 + P_T) 为在线结构化预测在非平稳环境下的性能提供了更好的保证。所提出的 OGD 学习率具有理论保证和良好的实践效果。所提出的方法是紧密的，并且可以推广到更广泛的问题。

Abstract: Online structured prediction, including online classification as a special
case, is the task of sequentially predicting labels from input features.
Therein the surrogate regret -- the cumulative excess of the target loss (e.g.,
0-1 loss) over the surrogate loss (e.g., logistic loss) of the fixed best
estimator -- has gained attention, particularly because it often admits a
finite bound independent of the time horizon $T$. However, such guarantees
break down in non-stationary environments, where every fixed estimator may
incur the surrogate loss growing linearly with $T$. We address this by proving
a bound of the form $F_T + C(1 + P_T)$ on the cumulative target loss, where
$F_T$ is the cumulative surrogate loss of any comparator sequence, $P_T$ is its
path length, and $C > 0$ is some constant. This bound depends on $T$ only
through $F_T$ and $P_T$, often yielding much stronger guarantees in
non-stationary environments. Our core idea is to synthesize the dynamic regret
bound of the online gradient descent (OGD) with the technique of exploiting the
surrogate gap. Our analysis also sheds light on a new Polyak-style learning
rate for OGD, which systematically offers target-loss guarantees and exhibits
promising empirical performance. We further extend our approach to a broader
class of problems via the convolutional Fenchel--Young loss. Finally, we prove
a lower bound showing that the dependence on $F_T$ and $P_T$ is tight.

</details>


### [526] [Non-Asymptotic Analysis of Efficiency in Conformalized Regression](https://arxiv.org/abs/2510.07093)
*Yunzhen Yao,Lie He,Michael Gastpar*

Main category: cs.LG

TL;DR: 本文研究了在SGD训练下，共形量化回归和共形中位数回归的预测集长度与其最优长度之间偏差的非渐近界限，考虑了训练集大小n、校准集大小m以及失覆盖率α的联合影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将失覆盖率α视为固定常数，忽略了其对共形预测效率的影响。本文旨在建立预测集长度与最优长度之间偏差的非渐近界限，并分析训练集大小n、校准集大小m以及失覆盖率α的联合影响。

Method: 推导了在SGD训练下，共形量化回归和共形中位数回归的预测集长度与最优长度之间偏差的非渐近界限，界限的阶数为 $\mathcal{O}(1/\sqrt{n} + 1/(\alpha^2 n) + 1/\sqrt{m} + \exp(-\alpha^2 m))$。

Result: 得到的界限揭示了预测集长度的收敛率在不同α的条件下存在阶段性转变，并指明了如何通过分配数据来控制预测集长度的过大。实验结果与理论发现一致。

Conclusion: 本文提出的理论界限能够捕捉到效率与训练集大小n、校准集大小m以及失覆盖率α的联合依赖性，并为在不同失覆盖率条件下选择模型参数提供了指导。

Abstract: Conformal prediction provides prediction sets with coverage guarantees. The
informativeness of conformal prediction depends on its efficiency, typically
quantified by the expected size of the prediction set. Prior work on the
efficiency of conformalized regression commonly treats the miscoverage level
$\alpha$ as a fixed constant. In this work, we establish non-asymptotic bounds
on the deviation of the prediction set length from the oracle interval length
for conformalized quantile and median regression trained via SGD, under mild
assumptions on the data distribution. Our bounds of order
$\mathcal{O}(1/\sqrt{n} + 1/(\alpha^2 n) + 1/\sqrt{m} + \exp(-\alpha^2 m))$
capture the joint dependence of efficiency on the proper training set size $n$,
the calibration set size $m$, and the miscoverage level $\alpha$. The results
identify phase transitions in convergence rates across different regimes of
$\alpha$, offering guidance for allocating data to control excess prediction
set length. Empirical results are consistent with our theoretical findings.

</details>


### [527] [Bridged Clustering for Representation Learning: Semi-Supervised Sparse Bridging](https://arxiv.org/abs/2510.07182)
*Patrick Peixuan Ye,Chen Shani,Ellen Vitercik*

Main category: cs.LG

TL;DR: Bridged Clustering是一种半监督学习框架，可从不成对的输入X和输出Y数据集中学习预测器。该方法首先独立聚类X和Y，然后仅使用少量配对样本在簇之间学习稀疏、可解释的桥梁。在推理时，新输入x被分配到最近的输入簇，并返回链接的输出簇的质心作为预测ŷ。该方法在理论上被证明是有效且高效的，并且在实践中具有竞争力，同时保持简单、模型无关和标签效率高。


<details>
  <summary>Details</summary>
Motivation: 从不成对的输入X和输出Y数据集中学习预测器，并明确利用仅输出数据，同时保持稀疏和可解释的对齐。

Method: 首先独立聚类X和Y，然后学习一个稀疏、可解释的桥梁连接这些簇，只使用少量配对样本。在推理时，将新输入分配给最近的输入簇，并返回链接的输出簇的质心作为预测。

Result: 通过理论分析，证明了在误聚类和误桥接率有界的情况下，该算法能成为有效的预测器。实验证明，该方法在效率、模型无关性和低监督设置下的标签效率方面与最先进的方法具有竞争力。

Conclusion: Bridged Clustering是一个简单、模型无关且标签效率高的半监督学习框架，通过显式利用输出数据并保持稀疏对齐，能够有效地从不成对的数据中学习预测器。

Abstract: We introduce Bridged Clustering, a semi-supervised framework to learn
predictors from any unpaired input $X$ and output $Y$ dataset. Our method first
clusters $X$ and $Y$ independently, then learns a sparse, interpretable bridge
between clusters using only a few paired examples. At inference, a new input
$x$ is assigned to its nearest input cluster, and the centroid of the linked
output cluster is returned as the prediction $\hat{y}$. Unlike traditional SSL,
Bridged Clustering explicitly leverages output-only data, and unlike dense
transport-based methods, it maintains a sparse and interpretable alignment.
Through theoretical analysis, we show that with bounded mis-clustering and
mis-bridging rates, our algorithm becomes an effective and efficient predictor.
Empirically, our method is competitive with SOTA methods while remaining
simple, model-agnostic, and highly label-efficient in low-supervision settings.

</details>


### [528] [Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples](https://arxiv.org/abs/2510.07192)
*Alexandra Souly,Javier Rando,Ed Chapman,Xander Davies,Burak Hasircioglu,Ezzeldin Shereen,Carlos Mougan,Vasilios Mavroudis,Erik Jones,Chris Hicks,Nicholas Carlini,Yarin Gal,Robert Kirk*

Main category: cs.LG

TL;DR: Poisoning attacks require a near-constant number of documents regardless of dataset size for large language models (LLMs), making them easier to implement than previously thought.


<details>
  <summary>Details</summary>
Motivation: Existing work on pretraining poisoning assumes adversaries control a percentage of the training corpus, which is impractical for large models due to the vast amounts of data involved. This work aims to show that poisoning attacks require a near-constant number of documents irrespective of dataset size.

Method: The study conducted large-scale pretraining poisoning experiments on models with parameters ranging from 600M to 13B, trained on chinchilla-optimal datasets (6B to 260B tokens). Smaller-scale experiments were performed to ablate factors influencing attack success, and the dynamics of poisoning during fine-tuning were also investigated.

Result: Experiments showed that 250 poisoned documents similarly compromised models across all model and dataset sizes, even when the largest models were trained on over 20 times more clean data. The study also found that the number of poisons required does not scale up with model size, and demonstrated similar dynamics for poisoning during fine-tuning.

Conclusion: Injecting backdoors through data poisoning may be easier for large language models than previously believed because the number of required poisons does not scale with model size. This highlights the urgent need for more research into defenses to mitigate this risk in future models.

Abstract: Poisoning attacks can compromise the safety of large language models (LLMs)
by injecting malicious documents into their training data. Existing work has
studied pretraining poisoning assuming adversaries control a percentage of the
training corpus. However, for large models, even small percentages translate to
impractically large amounts of data. This work demonstrates for the first time
that poisoning attacks instead require a near-constant number of documents
regardless of dataset size. We conduct the largest pretraining poisoning
experiments to date, pretraining models from 600M to 13B parameters on
chinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned
documents similarly compromise models across all model and dataset sizes,
despite the largest models training on more than 20 times more clean data. We
also run smaller-scale experiments to ablate factors that could influence
attack success, including broader ratios of poisoned to clean data and
non-random distributions of poisoned samples. Finally, we demonstrate the same
dynamics for poisoning during fine-tuning. Altogether, our results suggest that
injecting backdoors through data poisoning may be easier for large models than
previously believed as the number of poisons required does not scale up with
model size, highlighting the need for more research on defences to mitigate
this risk in future models.

</details>


### [529] [An in-depth look at approximation via deep and narrow neural networks](https://arxiv.org/abs/2510.07202)
*Joris Dommel,Sven A. Wegner*

Main category: cs.LG

TL;DR: 任意深度、实值、前馈和 ReLU 激活网络的宽度 w 构成连续函数空间的稠密子集，当且仅当 w>n。


<details>
  <summary>Details</summary>
Motivation: 需要研究当宽度 w=n 和 w=n+1 时，神经网络逼近一个具体函数 f:R^n->R 的行为，以及深度对逼近质量的影响，并探讨“死亡神经元”效应。

Method: 通过神经网络逼近一个具体函数 f:R^n->R，并在宽度 w=n 和 w=n+1 的情况下，研究深度对逼近质量的影响，分析“死亡神经元”效应。

Result: 在 w=n 和 w=n+1 的情况下，通过神经网络逼近函数 f 的行为，并分析了深度和“死亡神经元”对逼近质量的影响。

Conclusion: “死亡神经元”效应影响了神经网络的逼近行为。

Abstract: In 2017, Hanin and Sellke showed that the class of arbitrarily deep,
real-valued, feed-forward and ReLU-activated networks of width w forms a dense
subset of the space of continuous functions on R^n, with respect to the
topology of uniform convergence on compact sets, if and only if w>n holds. To
show the necessity, a concrete counterexample function f:R^n->R was used. In
this note we actually approximate this very f by neural networks in the two
cases w=n and w=n+1 around the aforementioned threshold. We study how the
approximation quality behaves if we vary the depth and what effect (spoiler
alert: dying neurons) cause that behavior.

</details>


### [530] [Guided by the Experts: Provable Feature Learning Dynamic of Soft-Routed Mixture-of-Experts](https://arxiv.org/abs/2510.07205)
*Fangshuo Liao,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: 本论文提出了软路由MoE模型联合训练的理论分析，并给出了收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 当前对MoE模型训练动力学的理论理解有限，主要局限于单独优化专家-路由器或仅在特定条件下进行分析。

Method: 在学生-教师框架下，对具有非线性路由器和专家的软路由MoE模型进行联合训练，并提供收敛性保证。在此基础上，提出了一种训练后剪枝方法，随后进行可证明收敛的微调以达到全局最优。

Result: 证明了在适度过参数化的情况下，学生网络会经历一个特征学习阶段，路由器通过学习专家的参数来“引导”学习过程，从而恢复教师的参数。另外，证明了剪枝和微调过程可以达到全局最优。

Conclusion: 该分析首次对MoE架构的优化前景提供了新的见解，并为软路由MoE模型的训练提供了理论支持。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a cornerstone of
modern AI systems. In particular, MoEs route inputs dynamically to specialized
experts whose outputs are aggregated through weighted summation. Despite their
widespread application, theoretical understanding of MoE training dynamics
remains limited to either separate expert-router optimization or only top-1
routing scenarios with carefully constructed datasets. This paper advances MoE
theory by providing convergence guarantees for joint training of soft-routed
MoE models with non-linear routers and experts in a student-teacher framework.
We prove that, with moderate over-parameterization, the student network
undergoes a feature learning phase, where the router's learning process is
``guided'' by the experts, that recovers the teacher's parameters. Moreover, we
show that a post-training pruning can effectively eliminate redundant neurons,
followed by a provably convergent fine-tuning process that reaches global
optimality. To our knowledge, our analysis is the first to bring novel insights
in understanding the optimization landscape of the MoE architecture.

</details>


### [531] [A Broader View of Thompson Sampling](https://arxiv.org/abs/2510.07208)
*Yanlin Qu,Hongseok Namkoong,Assaf Zeevi*

Main category: cs.LG

TL;DR: Thompson Sampling 算法可以通过在线优化视角进行分析，其中探索与利用的平衡由残差不确定性正则化。


<details>
  <summary>Details</summary>
Motivation: 现有关于 Thompson Sampling 算法如何平衡探索与利用的机制尚不明确，本文旨在揭示其内在原理。

Method: 将 Thompson Sampling 重新构建为在线优化问题，并引入“忠实”平稳化（faithful stationarization）的遗憾公式，利用 Bellman 最优原理推导时间不变的最优策略，并将 Thompson Sampling 与该策略结构进行对比。

Result: 通过在线优化视角，发现 Thompson Sampling 的贪婪性由基于点双列相关系数的残差不确定性进行正则化，从而解释了其平衡探索与利用的机制。

Conclusion: 本文揭示了 Thompson Sampling 平衡探索与利用的内在机制，并为进一步研究和改进该算法提供了新的理论框架。

Abstract: Thompson Sampling is one of the most widely used and studied bandit
algorithms, known for its simple structure, low regret performance, and solid
theoretical guarantees. Yet, in stark contrast to most other families of bandit
algorithms, the exact mechanism through which posterior sampling (as introduced
by Thompson) is able to "properly" balance exploration and exploitation,
remains a mystery. In this paper we show that the core insight to address this
question stems from recasting Thompson Sampling as an online optimization
algorithm. To distill this, a key conceptual tool is introduced, which we refer
to as "faithful" stationarization of the regret formulation. Essentially, the
finite horizon dynamic optimization problem is converted into a stationary
counterpart which "closely resembles" the original objective (in contrast, the
classical infinite horizon discounted formulation, that leads to the Gittins
index, alters the problem and objective in too significant a manner). The newly
crafted time invariant objective can be studied using Bellman's principle which
leads to a time invariant optimal policy. When viewed through this lens,
Thompson Sampling admits a simple online optimization form that mimics the
structure of the Bellman-optimal policy, and where greediness is regularized by
a measure of residual uncertainty based on point-biserial correlation. This
answers the question of how Thompson Sampling balances
exploration-exploitation, and moreover, provides a principled framework to
study and further improve Thompson's original idea.

</details>


### [532] [Discriminative Feature Feedback with General Teacher Classes](https://arxiv.org/abs/2510.07245)
*Omri Bar Oz,Tosca Lechner,Sivan Sabato*

Main category: cs.LG

TL;DR: 本文首次对 DFF（Discriminative Feature Feedback）交互式学习协议进行了系统性研究，并与经典的监督学习和在线学习协议进行了比较。


<details>
  <summary>Details</summary>
Motivation: 研究 DFF 学习协议的理论性质，并将其与经典学习协议进行比较，以理解其在可实现和不可实现场景下的最优误分类界限。

Method: 研究 DFF 在可实现和不可实现场景下的最优误分类界限，并提出用新的维度概念来表征可实现场景的误分类界限，以及在不可实现场景下的误分类上界。

Result: 在可实现场景下，通过新的维度概念表征了误分类界限。在不可实现场景下，提供了一个无法进一步改进的误分类上界。

Conclusion: 研究结果表明，与在线学习不同，在 DFF 中，可实现维度不足以表征最优的不可实现误分类界限或无悔算法的存在性。

Abstract: We study the theoretical properties of the interactive learning protocol
Discriminative Feature Feedback (DFF) (Dasgupta et al., 2018). The DFF learning
protocol uses feedback in the form of discriminative feature explanations. We
provide the first systematic study of DFF in a general framework that is
comparable to that of classical protocols such as supervised learning and
online learning. We study the optimal mistake bound of DFF in the realizable
and the non-realizable settings, and obtain novel structural results, as well
as insights into the differences between Online Learning and settings with
richer feedback such as DFF. We characterize the mistake bound in the
realizable setting using a new notion of dimension. In the non-realizable
setting, we provide a mistake upper bound and show that it cannot be improved
in general. Our results show that unlike Online Learning, in DFF the realizable
dimension is insufficient to characterize the optimal non-realizable mistake
bound or the existence of no-regret algorithms.

</details>


### [533] [Test-Time Graph Search for Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2510.07257)
*Evgenii Opryshko,Junwei Quan,Claas Voelcker,Yilun Du,Igor Gilitschenski*

Main category: cs.LG

TL;DR: TTGS通过在数据集状态上构建加权图并进行快速搜索来解决离线目标条件强化学习中的长期决策问题，提高了成功率。


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习（GCRL）在处理长期决策任务时面临时序信用分配和误差累积的挑战，而离线设置会加剧这些问题。

Method: TTGS是一种轻量级的测试时图搜索方法，它接受状态空间距离或成本信号，构建数据集状态上的加权图，并通过快速搜索来组合子目标序列，由一个固定的策略来执行。当基础学习器是基于价值的学习器时，距离直接来自学习到的目标条件价值函数，无需手动设计度量。

Result: TTGS在OGBench基准测试中，通过简单的度量引导的测试时规划，提高了多种基础学习器在挑战性运动任务上的成功率。

Conclusion: TTGS是一种有效的方法，可以在不改变训练过程、无需额外监督、在线交互或特权信息的情况下，通过测试时的图搜索来解决离线GCRL中的长期决策问题。

Abstract: Offline goal-conditioned reinforcement learning (GCRL) trains policies that
reach user-specified goals at test time, providing a simple, unsupervised,
domain-agnostic way to extract diverse behaviors from unlabeled, reward-free
datasets. Nonetheless, long-horizon decision making remains difficult for GCRL
agents due to temporal credit assignment and error accumulation, and the
offline setting amplifies these effects. To alleviate this issue, we introduce
Test-Time Graph Search (TTGS), a lightweight planning approach to solve the
GCRL task. TTGS accepts any state-space distance or cost signal, builds a
weighted graph over dataset states, and performs fast search to assemble a
sequence of subgoals that a frozen policy executes. When the base learner is
value-based, the distance is derived directly from the learned goal-conditioned
value function, so no handcrafted metric is needed. TTGS requires no changes to
training, no additional supervision, no online interaction, and no privileged
information, and it runs entirely at inference. On the OGBench benchmark, TTGS
improves success rates of multiple base learners on challenging locomotion
tasks, demonstrating the benefit of simple metric-guided test-time planning for
offline GCRL.

</details>


### [534] [GTCN-G: A Residual Graph-Temporal Fusion Network for Imbalanced Intrusion Detection (Preprint)](https://arxiv.org/abs/2510.07285)
*Tianxiang Xu,Zhichao Wen,Xinyu Zhao,Qi Hu,Yan Li,Chang Liu*

Main category: cs.LG

TL;DR: 本研究提出了一种名为 GTCN-G 的新深度学习框架，通过融合 Gated TCN 和 GCN，并引入基于 GAT 的残差学习机制，有效解决了网络入侵检测中数据类别不平衡的问题，并在两个公开数据集上取得了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的入侵检测系统（IDS）面临网络威胁日益复杂和流量数据类别不平衡的挑战。尽管 GNN 和 TCN 各有优势，但缺乏能将两者结合并解决数据不平衡问题的综合框架。

Method: 提出了一种名为 GTCN-G 的新深度学习框架，该框架融合了 Gated TCN（G-TCN）用于提取网络流的层级时间特征，以及 GCN 用于学习底层图结构。通过引入基于图注意力网络（GAT）的残差学习机制，保留原始特征信息，以缓解类别不平衡问题并提高对罕见恶意活动的检测敏感性。

Result: 在 UNSW-NB15 和 ToN-IoT 两个公开基准数据集上进行的广泛实验表明，GTCN-G 模型在二元和多元分类任务中均取得了优于现有基线模型的先进性能。

Conclusion: GTCN-G 模型成功地融合了 G-TCN 和 GCN 的优势，并通过 GAT 实现的残差学习机制有效解决了网络入侵检测中的类别不平衡问题，达到了最先进的检测效果。

Abstract: The escalating complexity of network threats and the inherent class imbalance
in traffic data present formidable challenges for modern Intrusion Detection
Systems (IDS). While Graph Neural Networks (GNNs) excel in modeling topological
structures and Temporal Convolutional Networks (TCNs) are proficient in
capturing time-series dependencies, a framework that synergistically integrates
both while explicitly addressing data imbalance remains an open challenge. This
paper introduces a novel deep learning framework, named Gated Temporal
Convolutional Network and Graph (GTCN-G), engineered to overcome these
limitations. Our model uniquely fuses a Gated TCN (G-TCN) for extracting
hierarchical temporal features from network flows with a Graph Convolutional
Network (GCN) designed to learn from the underlying graph structure. The core
innovation lies in the integration of a residual learning mechanism,
implemented via a Graph Attention Network (GAT). This mechanism preserves
original feature information through residual connections, which is critical
for mitigating the class imbalance problem and enhancing detection sensitivity
for rare malicious activities (minority classes). We conducted extensive
experiments on two public benchmark datasets, UNSW-NB15 and ToN-IoT, to
validate our approach. The empirical results demonstrate that the proposed
GTCN-G model achieves state-of-the-art performance, significantly outperforming
existing baseline models in both binary and multi-class classification tasks.

</details>


### [535] [Evolutionary Profiles for Protein Fitness Prediction](https://arxiv.org/abs/2510.07286)
*Jigang Fan,Xiaoran Jiao,Shengdong Lin,Zhanming Liang,Weian Mao,Chenchen Jing,Hao Chen,Chunhua Shen*

Main category: cs.LG

TL;DR: 通过将自然进化视为隐式奖励最大化，将MLM视为逆强化学习（IRL），EvoIF模型利用进化信号来预测突变适应性影响，并在ProteinGym上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 预测突变对蛋白质功能的影响对于蛋白质工程至关重要，但受限于可用检测方法的规模。

Method: EvoIF模型整合了家族内序列比对信息和跨家族的结构-进化约束。它将序列-结构表示与这些比对信息融合，生成用于评分的校准概率。

Result: 在ProteinGym（包含217个突变检测和超过250万个突变体）上，EvoIF及其MSA增强版本取得了最先进或具有竞争力的性能，仅使用了0.15%的训练数据，并且参数量少于近期的大型模型。

Conclusion: EvoIF模型通过融合不同来源的进化信号，能够有效预测突变的适应性影响，并且在计算资源和数据需求上具有优势。

Abstract: Predicting the fitness impact of mutations is central to protein engineering
but constrained by limited assays relative to the size of sequence space.
Protein language models (pLMs) trained with masked language modeling (MLM)
exhibit strong zero-shot fitness prediction; we provide a unifying view by
interpreting natural evolution as implicit reward maximization and MLM as
inverse reinforcement learning (IRL), in which extant sequences act as expert
demonstrations and pLM log-odds serve as fitness estimates. Building on this
perspective, we introduce EvoIF, a lightweight model that integrates two
complementary sources of evolutionary signal: (i) within-family profiles from
retrieved homologs and (ii) cross-family structural-evolutionary constraints
distilled from inverse folding logits. EvoIF fuses sequence-structure
representations with these profiles via a compact transition block, yielding
calibrated probabilities for log-odds scoring. On ProteinGym (217 mutational
assays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve
state-of-the-art or competitive performance while using only 0.15% of the
training data and fewer parameters than recent large models. Ablations confirm
that within-family and cross-family profiles are complementary, improving
robustness across function types, MSA depths, taxa, and mutation depths. The
codes will be made publicly available at https://github.com/aim-uofa/EvoIF.

</details>


### [536] [MolGA: Molecular Graph Adaptation with Pre-trained 2D Graph Encoder](https://arxiv.org/abs/2510.07289)
*Xingtong Yu,Chang Zhou,Xinming Zhang,Yuan Fang*

Main category: cs.LG

TL;DR: MolGA通过整合分子领域知识来改进预训练的2D图编码器，以适应下游分子应用，并在11个数据集上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练2D图编码器忽略了分子实例（原子和键）的领域知识，而现有的分子预训练方法缺乏整合分子中存在的各种知识的灵活性。因此，在下游适应过程中整合分子领域知识是更实用的替代方案。

Method: MolGA通过以下方式调整预训练的2D图编码器：1.提出分子对齐策略，连接预训练的拓扑表示与领域知识表示。2.引入条件适应机制，生成实例特定的令牌，以实现分子领域知识在下游任务中的细粒度整合。

Result: 在11个公共数据集上进行的广泛实验证明了MolGA的有效性。

Conclusion: MolGA能够灵活地整合各种分子领域知识，以适应下游分子应用，并有效改进了预训练的2D图编码器。

Abstract: Molecular graph representation learning is widely used in chemical and
biomedical research. While pre-trained 2D graph encoders have demonstrated
strong performance, they overlook the rich molecular domain knowledge
associated with submolecular instances (atoms and bonds). While molecular
pre-training approaches incorporate such knowledge into their pre-training
objectives, they typically employ designs tailored to a specific type of
knowledge, lacking the flexibility to integrate diverse knowledge present in
molecules. Hence, reusing widely available and well-validated pre-trained 2D
encoders, while incorporating molecular domain knowledge during downstream
adaptation, offers a more practical alternative. In this work, we propose
MolGA, which adapts pre-trained 2D graph encoders to downstream molecular
applications by flexibly incorporating diverse molecular domain knowledge.
First, we propose a molecular alignment strategy that bridge the gap between
pre-trained topological representations with domain-knowledge representations.
Second, we introduce a conditional adaptation mechanism that generates
instance-specific tokens to enable fine-grained integration of molecular domain
knowledge for downstream tasks. Finally, we conduct extensive experiments on
eleven public datasets, demonstrating the effectiveness of MolGA.

</details>


### [537] [MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline](https://arxiv.org/abs/2510.07307)
*Rushi Qiang,Yuchen Zhuang,Anikait Singh,Percy Liang,Chao Zhang,Sherry Yang,Bo Dai*

Main category: cs.LG

TL;DR: MLE-Smith是一个全自动多智能体流水线，可将原始数据集转换为高质量、可扩展、多样化的机器学习工程（MLE）挑战，并生成了606个跨多种类别、目标和模态的任务，证明了其在真实世界数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习工程（MLE）基准测试在可扩展性和适用性方面存在局限，因为它们依赖于静态、手动策划的任务，制作过程耗时耗力。

Method: MLE-Smith利用生成-验证-执行范式，通过一个全自动多智能体流水线，将原始数据集转化为具有可验证质量、实际可用性和丰富多样性的竞争风格MLE挑战。该流水线通过结构化任务设计、标准化重构以及混合验证机制（强制执行严格的结构规则和高级语义健全性）来驱动任务生成。此外，它还通过交互式执行来验证经验可解性和真实世界保真度。

Result: MLE-Smith被应用于224个真实世界的数据集，生成了606个跨越多种类别、目标和模态的任务。在生成的任务上评估八个主流和前沿的大型语言模型（LLMs）发现，它们在MLE-Smith任务上的表现与在精心设计的人工任务上的表现高度相关。

Conclusion: MLE-Smith能够有效地扩展MLE任务的规模，同时保持任务质量，证明了其在各种真实世界数据集上的有效性以及所生成任务的质量与人工设计任务相当。

Abstract: While Language Models (LMs) have made significant progress in automating
machine learning engineering (MLE), the acquisition of high-quality MLE
training data is significantly constrained. Current MLE benchmarks suffer from
low scalability and limited applicability because they rely on static, manually
curated tasks, demanding extensive time and manual effort to produce. We
introduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw
datasets into competition-style MLE challenges through an efficient
generate-verify-execute paradigm for scaling MLE tasks with verifiable quality,
real-world usability, and rich diversity. The proposed multi-agent pipeline in
MLE-Smith drives structured task design and standardized refactoring, coupled
with a hybrid verification mechanism that enforces strict structural rules and
high-level semantic soundness. It further validates empirical solvability and
real-world fidelity through interactive execution. We apply MLE-Smith to 224 of
real-world datasets and generate 606 tasks spanning multiple categories,
objectives, and modalities, demonstrating that MLE-Smith can work effectively
across a wide range of real-world datasets. Evaluation on the generated tasks
shows that the performance of eight mainstream and cutting-edge LLMs on
MLE-Smith tasks is strongly correlated with their performance on carefully
human-designed tasks, highlighting the effectiveness of the MLE-Smith to
scaling up MLE tasks, while maintaining task quality.

</details>


### [538] [h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning](https://arxiv.org/abs/2510.07312)
*Sumeet Ramesh Motwani,Alesia Ivanova,Ziyang Cai,Philip Torr,Riashat Islam,Shital Shah,Christian Schroeder de Witt,Charles London*

Main category: cs.LG

TL;DR: 该研究提出了一种可扩展的方法，仅使用现有的短期推理数据来引导长期推理能力。通过合成简单问题以创建任意长度的多步依赖链，并在结果奖励下使用课程学习进行训练，该方法可以有效提升模型的长期推理能力，并在数学推理基准测试中取得显著的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在长推理任务上表现不佳，而解决此问题的方法（如推理时脚手架或有监督训练）难以扩展。本研究旨在提出一种可扩展的方法，仅利用现有的、丰富的短期推理数据来提升模型的长期推理能力。

Method: 该方法通过合成简单问题来创建任意长度的多步依赖链，并在此合成数据上使用仅基于结果的奖励和课程学习进行训练，逐步增加训练数据的复杂性。

Result: 在6年级数学问题（GSM8K）上进行课程训练，将模型在更长、更具挑战性的竞赛级别基准测试（GSM-Symbolic, MATH-500, AIME）上的准确率提升了高达2.06倍。在更高通过率下，该方法带来的改进也显著优于基线模型，表明模型在强化学习下可以学习新的推理路径。

Conclusion: 该研究提出了一种利用现有数据和课程强化学习来扩展模型长期推理能力的有效方法，实现了比传统全周期训练更高的样本复杂度，并提供了可与密集监督相媲美的训练信号。

Abstract: Large language models excel at short-horizon reasoning tasks, but performance
drops as reasoning horizon lengths increase. Existing approaches to combat this
rely on inference-time scaffolding or costly step-level supervision, neither of
which scales easily. In this work, we introduce a scalable method to bootstrap
long-horizon reasoning capabilities using only existing, abundant short-horizon
data. Our approach synthetically composes simple problems into complex,
multi-step dependency chains of arbitrary length. We train models on this data
using outcome-only rewards under a curriculum that automatically increases in
complexity, allowing RL training to be scaled much further without saturating.
Empirically, our method generalizes remarkably well: curriculum training on
composed 6th-grade level math problems (GSM8K) boosts accuracy on longer,
competition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.
Importantly, our long-horizon improvements are significantly higher than
baselines even at high pass@k, showing that models can learn new reasoning
paths under RL. Theoretically, we show that curriculum RL with outcome rewards
achieves an exponential improvement in sample complexity over full-horizon
training, providing training signal comparable to dense supervision. h1
therefore introduces an efficient path towards scaling RL for long-horizon
problems using only existing data.

</details>
