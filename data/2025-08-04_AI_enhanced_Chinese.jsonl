{"id": "2508.00341", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00341", "abs": "https://arxiv.org/abs/2508.00341", "authors": ["Shengheng Liu", "Ningning Fu", "Zhonghao Zhang", "Yongming Huang", "Tony Q. S. Quek"], "title": "Integrated user scheduling and beam steering in over-the-air federated learning for mobile IoT", "comment": "To appear in ACM TOIT. 24 pages, 8 figures", "summary": "The rising popularity of Internet of things (IoT) has spurred technological\nadvancements in mobile internet and interconnected systems. While offering\nflexible connectivity and intelligent applications across various domains, IoT\nservice providers must gather vast amounts of sensitive data from users, which\nnonetheless concomitantly raises concerns about privacy breaches. Federated\nlearning (FL) has emerged as a promising decentralized training paradigm to\ntackle this challenge. This work focuses on enhancing the aggregation\nefficiency of distributed local models by introducing over-the-air computation\ninto the FL framework. Due to radio resource scarcity in large-scale networks,\nonly a subset of users can participate in each training round. This highlights\nthe need for effective user scheduling and model transmission strategies to\noptimize communication efficiency and inference accuracy. To address this, we\npropose an integrated approach to user scheduling and receive beam steering,\nsubject to constraints on the number of selected users and transmit power.\nLeveraging the difference-of-convex technique, we decompose the primal\nnon-convex optimization problem into two sub-problems, yielding an iterative\nsolution. While effective, the computational load of the iterative method\nhampers its practical implementation. To overcome this, we further propose a\nlow-complexity user scheduling policy based on characteristic analysis of the\nwireless channel to directly determine the user subset without iteration.\nExtensive experiments validate the superiority of the proposed method in terms\nof aggregation error and learning performance over existing approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u3794\u8ba1\u7b97\u548c\u4f18\u5316\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u4fdd\u62a4\u9690\u79c1\u3002\u901a\u8fc7\u7528\u6237\u8c03\u5ea6\u548c\u6ce2\u675f\u8f6c\u5411\u6280\u672f\uff0c\u5e76\u5f00\u53d1\u4e86\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\uff08IoT\uff09\u7684\u666e\u53ca\uff0c\u7528\u6237\u9690\u79c1\u9762\u4e34\u6cc4\u9732\u98ce\u9669\u3002\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4f5c\u4e3a\u4e00\u79cd\u5206\u5e03\u5f0f\u8bad\u7ec3\u8303\u5f0f\uff0c\u4e3a\u89e3\u51b3\u6b64\u6311\u6218\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u4f46\u5176\u6a21\u578b\u805a\u5408\u6548\u7387\u548c\u901a\u4fe1\u6548\u7387\u6709\u5f85\u63d0\u9ad8\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u4e2d\u7528\u6237\u9009\u62e9\u548c\u6a21\u578b\u4f20\u8f93\u7684\u4f18\u5316\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u3794\u8ba1\u7b97\u5f15\u5165\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5206\u5e03\u5f0f\u672c\u5730\u6a21\u578b\u7684\u805a\u5408\u6548\u7387\u3002\u9488\u5bf9\u901a\u4fe1\u6548\u7387\u548c\u63a8\u7406\u51c6\u786e\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u96c6\u6210\u7684\u7528\u6237\u8c03\u5ea6\u548c\u63a5\u6536\u6ce2\u675f\u8f6c\u5411\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u5dee\u503c\u51f8\u6280\u672f\u5c06\u975e\u51f8\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u4e24\u4e2a\u5b50\u95ee\u9898\u8fdb\u884c\u8fed\u4ee3\u6c42\u89e3\u3002\u6b64\u5916\uff0c\u4e3a\u514b\u670d\u8fed\u4ee3\u6cd5\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u7528\u6237\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u6240\u63d0\u51fa\u7684\u96c6\u6210\u65b9\u6cd5\u548c\u4f4e\u590d\u6742\u5ea6\u7528\u6237\u8c03\u5ea6\u7b56\u7565\u5728\u805a\u5408\u8bef\u5dee\u548c\u5b66\u4e60\u6027\u80fd\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u6548\u7387\u548c\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u7528\u6237\u8c03\u5ea6\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u6790\u65e0\u7ebf\u4fe1\u9053\u7279\u6027\u76f4\u63a5\u786e\u5b9a\u7528\u6237\u5b50\u96c6\uff0c\u65e0\u9700\u8fed\u4ee3\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u805a\u5408\u8bef\u5dee\u548c\u5b66\u4e60\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.00426", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00426", "abs": "https://arxiv.org/abs/2508.00426", "authors": ["Rohan Gandhi", "Ankur Mallick", "Ken Sueda", "Rui Liang"], "title": "Tetris: Efficient Intra-Datacenter Calls Packing for Large Conferencing Services", "comment": null, "summary": "Conference services like Zoom, Microsoft Teams, and Google Meet facilitate\nmillions of daily calls, yet ensuring high performance at low costs remains a\nsignificant challenge. This paper revisits the problem of packing calls across\nMedia Processor (MP) servers that host the calls within individual datacenters\n(DCs). We show that the algorithm used in Teams -- a large scale conferencing\nservice as well as other state-of-art algorithms are prone to placing calls\nresulting in some of the MPs becoming hot (high CPU utilization) that leads to\ndegraded performance and/or elevated hosting costs. The problem arises from\ndisregarding the variability in CPU usage among calls, influenced by\ndifferences in participant numbers and media types (audio/video), compounded by\nbursty call arrivals. To tackle this, we propose Tetris, a multi-step framework\nwhich (a) optimizes initial call assignments by leveraging historical data and\n(b) periodically migrates calls from hot MPs using linear optimization, aiming\nto minimize hot MP usage. Evaluation based on a 24-hour trace of over 10\nmillion calls in one DC shows that Tetris reduces participant numbers on hot\nMPs by at least 2.5X.", "AI": {"tldr": "Tetris\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u547c\u53eb\u5206\u914d\u548c\u8fc1\u79fb\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4f1a\u8bae\u670d\u52a1\u4e2dMP\u670d\u52a1\u5668\u7684\u8fc7\u8f7d\u60c5\u51b5\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u4f1a\u8bae\u670d\u52a1\uff08\u5982Zoom\u3001Teams\uff09\u5728\u8de8\u5a92\u4f53\u5904\u7406\u5668\uff08MP\uff09\u670d\u52a1\u5668\u5206\u914d\u547c\u53eb\u65f6\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651CPU\u4f7f\u7528\u7387\u7684\u53d8\u5f02\u6027\uff08\u53d7\u53c2\u4e0e\u8005\u6570\u91cf\u548c\u5a92\u4f53\u7c7b\u578b\u5f71\u54cd\uff09\u4ee5\u53ca\u547c\u53eb\u5230\u8fbe\u7684\u7a81\u53d1\u6027\uff0c\u5bfc\u81f4\u90e8\u5206MP\u670d\u52a1\u5668\u8fc7\u8f7d\uff08CPU\u5229\u7528\u7387\u9ad8\uff09\uff0c\u4ece\u800c\u5f15\u8d77\u6027\u80fd\u4e0b\u964d\u548c/\u6216\u6258\u7ba1\u6210\u672c\u5347\u9ad8\u3002", "method": "Tetris\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a1. \u5229\u7528\u5386\u53f2\u6570\u636e\u4f18\u5316\u521d\u59cb\u547c\u53eb\u5206\u914d\uff1b2. \u4f7f\u7528\u7ebf\u6027\u4f18\u5316\u5468\u671f\u6027\u5730\u8fc1\u79fb\u547c\u53eb\uff0c\u4ee5\u6700\u5c0f\u5316\u70ed\u70b9MP\u7684\u4f7f\u7528\u3002", "result": "\u5728\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\uff08\u4e00\u4e2a\u6570\u636e\u4e2d\u5fc324\u5c0f\u65f6\u5185\u8d85\u8fc71000\u4e07\u6b21\u547c\u53eb\uff09\u7684\u8bc4\u4f30\u4e2d\uff0cTetris\u6846\u67b6\u5c06\u70ed\u70b9MP\u4e0a\u7684\u53c2\u4e0e\u8005\u6570\u91cf\u51cf\u5c11\u4e86\u81f3\u5c112.5\u500d\u3002", "conclusion": "Tetris\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u547c\u53eb\u5206\u914d\u548c\u5468\u671f\u6027\u5730\u8fc1\u79fb\u547c\u53eb\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u70ed\u70b9MP\u7684\u4f7f\u7528\uff0c\u4ece\u800c\u63d0\u9ad8\u4f1a\u8bae\u670d\u52a1\u7684\u6027\u80fd\u5e76\u964d\u4f4e\u6258\u7ba1\u6210\u672c\u3002"}}
{"id": "2508.00622", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00622", "abs": "https://arxiv.org/abs/2508.00622", "authors": ["Kapel Dev", "Yash Madhwal", "Sofia Shevelo", "Pavel Osinenko", "Yury Yanovich"], "title": "SwarnRaft: Leveraging Consensus for Robust Drone Swarm Coordination in GNSS-Degraded Environments", "comment": null, "summary": "Unmanned aerial vehicle (UAV) swarms are increasingly used in critical\napplications such as aerial mapping, environmental monitoring, and autonomous\ndelivery. However, the reliability of these systems is highly dependent on\nuninterrupted access to the Global Navigation Satellite Systems (GNSS) signals,\nwhich can be disrupted in real-world scenarios due to interference,\nenvironmental conditions, or adversarial attacks, causing disorientation,\ncollision risks, and mission failure. This paper proposes SwarnRaft, a\nblockchain-inspired positioning and consensus framework for maintaining\ncoordination and data integrity in UAV swarms operating under GNSS-denied\nconditions. SwarnRaft leverages the Raft consensus algorithm to enable\ndistributed drones (nodes) to agree on state updates such as location and\nheading, even in the absence of GNSS signals for one or more nodes. In our\nprototype, each node uses GNSS and local sensing, and communicates over WiFi in\na simulated swarm. Upon signal loss, consensus is used to reconstruct or verify\nthe position of the failed node based on its last known state and trajectory.\nOur system demonstrates robustness in maintaining swarm coherence and fault\ntolerance through a lightweight, scalable communication model. This work offers\na practical and secure foundation for decentralized drone operation in\nunpredictable environments.", "AI": {"tldr": "\u5728 GNSS \u4fe1\u53f7\u4e22\u5931\u65f6\uff0cSwarnRaft \u4f7f\u7528\u7c7b\u4f3c\u533a\u5757\u94fe\u7684\u5171\u8bc6\u673a\u5236\uff08Raft \u7b97\u6cd5\uff09\u6765\u5e2e\u52a9\u65e0\u4eba\u673a\u7fa4\u4fdd\u6301\u4f4d\u7f6e\u548c\u822a\u5411\u7684\u4e00\u81f4\u6027\uff0c\u4ee5\u786e\u4fdd\u534f\u540c\u548c\u4efb\u52a1\u7684\u6210\u529f\u3002", "motivation": "\u65e0\u4eba\u673a\u7fa4\u5728\u5173\u952e\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u5e7f\u6cdb\uff0c\u4f46\u5176\u53ef\u9760\u6027\u9ad8\u5ea6\u4f9d\u8d56 GNSS \u4fe1\u53f7\uff0c\u800c GNSS \u4fe1\u53f7\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u6613\u53d7\u5e72\u6270\u3001\u73af\u5883\u56e0\u7d20\u6216\u6076\u610f\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u65e0\u4eba\u673a\u8ff7\u5931\u65b9\u5411\u3001\u78b0\u649e\u98ce\u9669\u548c\u4efb\u52a1\u5931\u8d25\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SwarnRaft \u7684\u3001\u53d7\u533a\u5757\u94fe\u542f\u53d1\u7684\u5b9a\u4f4d\u548c\u5171\u8bc6\u6846\u67b6\uff0c\u5229\u7528 Raft \u5171\u8bc6\u7b97\u6cd5\uff0c\u4f7f\u5206\u5e03\u5f0f\u65e0\u4eba\u673a\uff08\u8282\u70b9\uff09\u80fd\u591f\u5728 GNSS \u4fe1\u53f7\u4e22\u5931\u7684\u60c5\u51b5\u4e0b\u5c31\u4f4d\u7f6e\u548c\u822a\u5411\u7b49\u72b6\u6001\u66f4\u65b0\u8fbe\u6210\u4e00\u81f4\u3002", "result": "\u6240\u63d0\u51fa\u7684 SwarnRaft \u7cfb\u7edf\u5728\u6a21\u62df\u7684\u65e0\u4eba\u673a\u7fa4\u4e2d\u8fdb\u884c\u4e86\u539f\u578b\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5728 GNSS \u4fe1\u53f7\u4e22\u5931\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5171\u8bc6\u673a\u5236\u91cd\u5efa\u6216\u9a8c\u8bc1\u6545\u969c\u8282\u70b9\u4f4d\u7f6e\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4fdd\u6301\u4e86\u65e0\u4eba\u673a\u7fa4\u7684\u534f\u540c\u548c\u5bb9\u9519\u80fd\u529b\uff0c\u91c7\u7528\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u901a\u4fe1\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53bb\u4e2d\u5fc3\u5316\u7684\u65e0\u4eba\u673a\u5728\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u7684\u8fd0\u884c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u5b89\u5168\u7684\u57fa\u7840\u3002"}}
{"id": "2508.00041", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00041", "abs": "https://arxiv.org/abs/2508.00041", "authors": ["Yebo Wu", "Jingguang Li", "Zhijiang Guo", "Li Li"], "title": "Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages", "comment": null, "summary": "Federated fine-tuning enables Large Language Models (LLMs) to adapt to\ndownstream tasks while preserving data privacy, but its resource-intensive\nnature limits deployment on edge devices. In this paper, we introduce\nDevelopmental Federated Tuning (DevFT), a resource-efficient approach inspired\nby cognitive development that progressively builds a powerful LLM from a\ncompact foundation. DevFT decomposes the fine-tuning process into developmental\nstages, each optimizing submodels with increasing parameter capacity. Knowledge\nfrom earlier stages transfers to subsequent submodels, providing optimized\ninitialization parameters that prevent convergence to local minima and\naccelerate training. This paradigm mirrors human learning, gradually\nconstructing comprehensive knowledge structure while refining existing skills.\nTo efficiently build stage-specific submodels, DevFT introduces\ndeconfliction-guided layer grouping and differential-based layer fusion to\ndistill essential information and construct representative layers. Evaluations\nacross multiple benchmarks demonstrate that DevFT significantly outperforms\nstate-of-the-art methods, achieving up to 4.59$\\times$ faster convergence,\n10.67$\\times$ reduction in communication overhead, and 9.07% average\nperformance improvement, while maintaining compatibility with existing\napproaches.", "AI": {"tldr": "DevFT \u662f\u4e00\u79cd\u53d7\u8ba4\u77e5\u53d1\u5c55\u542f\u53d1\u7684\u8d44\u6e90\u9ad8\u6548\u7684\u8054\u90a6\u5fae\u8c03\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5206\u9636\u6bb5\u4f18\u5316\u5177\u6709\u4e0d\u540c\u53c2\u6570\u5bb9\u91cf\u7684\u5b50\u6a21\u578b\u6765\u6784\u5efa LLM\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u4f4e\u7684\u901a\u4fe1\u5f00\u9500\u548c\u66f4\u9ad8\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8054\u90a6\u5fae\u8c03\u8d44\u6e90\u5bc6\u96c6\u4e14\u9650\u5236\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DevFT \u7684\u8d44\u6e90\u9ad8\u6548\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53d7\u5230\u8ba4\u77e5\u53d1\u5c55\u7684\u542f\u53d1\uff0c\u5e76\u9010\u6b65\u4ece\u4e00\u4e2a\u7d27\u51d1\u7684\u57fa\u7840\u6784\u5efa\u4e00\u4e2a\u5f3a\u5927\u7684 LLM\u3002", "method": "DevFT \u5c06\u5fae\u8c03\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e0d\u540c\u7684\u53d1\u5c55\u9636\u6bb5\uff0c\u6bcf\u4e2a\u9636\u6bb5\u90fd\u6709\u4e0d\u540c\u7684\u53c2\u6570\u5bb9\u91cf\uff0c\u5e76\u4f7f\u7528\u53bb\u51b2\u7a81\u5f15\u5bfc\u7684\u5c42\u5206\u7ec4\u548c\u57fa\u4e8e\u5dee\u5206\u7684\u5c42\u878d\u5408\u6765\u6784\u5efa\u9636\u6bb5\u7279\u5b9a\u7684\u5b50\u6a21\u578b\u3002", "result": "DevFT \u5b9e\u73b0\u4e86\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5feb 4.59 \u500d\u7684\u6536\u655b\u901f\u5ea6\uff0c\u901a\u4fe1\u5f00\u9500\u51cf\u5c11\u4e86 10.67 \u500d\uff0c\u6027\u80fd\u5e73\u5747\u63d0\u9ad8\u4e86 9.07%\u3002", "conclusion": "DevFT \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u4e0e\u73b0\u6709\u65b9\u6cd5\u517c\u5bb9\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe 4.59 \u500d\u7684\u6536\u655b\u901f\u5ea6\uff0c\u901a\u4fe1\u5f00\u9500\u51cf\u5c11\u4e86 10.67 \u500d\uff0c\u6027\u80fd\u5e73\u5747\u63d0\u9ad8\u4e86 9.07%\u3002"}}
{"id": "2508.00475", "categories": ["cs.AR", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.00475", "abs": "https://arxiv.org/abs/2508.00475", "authors": ["Yunhao Ma", "Yanyu Lin", "Mingjing Li", "Puli Quan", "Chenlin Zhou", "Wenyue Zhang", "Zhiwei Zhong", "Wanyi Jia", "Xueke Zhu", "Qingyan Meng", "Huihui Zhou", "Fengwei An"], "title": "E2ATST: A Temporal-Spatial Optimized Energy-Efficient Architecture for Training Spiking Transformer", "comment": null, "summary": "(1) Pengcheng Laboratory, (2) Southern University of Science and Technology,\n(3) Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences,\n(4) University of Chinese Academy of Sciences", "AI": {"tldr": "Institutions involved: Pengcheng Laboratory, Southern University of Science and Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences.", "motivation": "To identify the institutions involved in the research.", "method": "Affiliation analysis", "result": "The analysis identified the following affiliations: Pengcheng Laboratory, Southern University of Science and Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, and University of Chinese Academy of Sciences.", "conclusion": "The affiliations are Pengcheng Laboratory, Southern University of Science and Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, and University of Chinese Academy of Sciences."}}
{"id": "2508.00130", "categories": ["cs.GT", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.00130", "abs": "https://arxiv.org/abs/2508.00130", "authors": ["Drew Gao", "Yihang Sun", "Jan Vondr\u00e1k"], "title": "Computation of Approximately Stable Committees in Approval-based Elections", "comment": "18 pages, 2 figures", "summary": "Approval-based committee selection is a model of significant interest in\nsocial choice theory. In this model, we have a set of voters $\\mathcal{V}$, a\nset of candidates $\\mathcal{C}$, and each voter has a set $A_v \\subset\n\\mathcal{C}$ of approved candidates. For any committee size $K$, the goal is to\nchoose $K$ candidates to represent the voters' preferences. We study a\ncriterion known as \\emph{approximate stability}, where a committee is\n$\\lambda$-approximately-stable if there is no other committee $T$ preferred by\nat least $\\frac{\\lambda|T|}{k} |\\mathcal{V}| $ voters. We prove that a\n$3.65$-approximately stable committee always exists and can be computed\nalgorithmically in this setting. Our approach is based on finding a Lindahl\nequilibrium and sampling from a strongly Rayleigh distribution associated with\nit.", "AI": {"tldr": "Approval-based committee selection is studied with a focus on approximate stability. A $3.65$-approximately stable committee is proven to exist and be computable using Lindahl equilibria and strongly Rayleigh distributions.", "motivation": "Study of approval-based committee selection, a model of significant interest in social choice theory.", "method": "The approach is based on finding a Lindahl equilibrium and sampling from a strongly Rayleigh distribution associated with it.", "result": "A $3.65$-approximately stable committee always exists and can be computed algorithmically.", "conclusion": "A $3.65$-approximately stable committee always exists and can be computed algorithmically."}}
{"id": "2508.00197", "categories": ["cs.CV", "cs.LG", "cs.NA", "math.CT", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.00197", "abs": "https://arxiv.org/abs/2508.00197", "authors": ["Eric Mjolsness", "Cory B. Scott"], "title": "Graph Lineages and Skeletal Graph Products", "comment": "42 pages. 33 Figures. Under review", "summary": "Graphs, and sequences of growing graphs, can be used to specify the\narchitecture of mathematical models in many fields including machine learning\nand computational science. Here we define structured graph \"lineages\" (ordered\nby level number) that grow in a hierarchical fashion, so that: (1) the number\nof graph vertices and edges increases exponentially in level number; (2)\nbipartite graphs connect successive levels within a graph lineage and, as in\nmultigrid methods, can constrain matrices relating successive levels; (3) using\nprolongation maps within a graph lineage, process-derived distance measures\nbetween graphs at successive levels can be defined; (4) a category of \"graded\ngraphs\" can be defined, and using it low-cost \"skeletal\" variants of standard\nalgebraic graph operations and type constructors (cross product, box product,\ndisjoint sum, and function types) can be derived for graded graphs and hence\nhierarchical graph lineages; (5) these skeletal binary operators have similar\nbut not identical algebraic and category-theoretic properties to their standard\ncounterparts; (6) graph lineages and their skeletal product constructors can\napproach continuum limit objects. Additional space-efficient unary operators on\ngraded graphs are also derived: thickening, which creates a graph lineage of\nmultiscale graphs, and escalation to a graph lineage of search frontiers\n(useful as a generalization of adaptive grids and in defining \"skeletal\"\nfunctions). The result is an algebraic type theory for graded graphs and\n(hierarchical) graph lineages. The approach is expected to be well suited to\ndefining hierarchical model architectures - \"hierarchitectures\" - and local\nsampling, search, or optimization algorithms on them. We demonstrate such\napplication to deep neural networks (including visual and feature scale spaces)\nand to multigrid numerical methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56fe\u8c31\u7cfb\u548c\u5206\u7ea7\u56fe\u7684\u4ee3\u6570\u7c7b\u578b\u7406\u8bba\uff0c\u4ee5\u652f\u6301\u5206\u5c42\u6a21\u578b\u67b6\u6784\u7684\u6784\u5efa\u548c\u64cd\u4f5c\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u7f51\u683c\u65b9\u6cd5\u3002", "motivation": "\u63d0\u4f9b\u4e00\u79cd\u7528\u4e8e\u6784\u5efa\u548c\u64cd\u4f5c\u591a\u5c3a\u5ea6\u3001\u5206\u5c42\u56fe\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u8ba1\u7b97\u79d1\u5b66\u4e2d\u7684\u6a21\u578b\u67b6\u6784\u5b9a\u4e49\uff0c\u7279\u522b\u5173\u6ce8\u56fe\u8c31\u7cfb\u548c\u5206\u7ea7\u56fe\u3002", "method": "\u672c\u6587\u5b9a\u4e49\u4e86\u7ed3\u6784\u5316\u56fe\u201c\u8c31\u7cfb\u201d\uff08\u6309\u5c42\u7ea7\u6392\u5e8f\uff09\uff0c\u5176\u7279\u6027\u5305\u62ec\uff1a1\uff09\u56fe\u9876\u70b9\u548c\u8fb9\u7684\u6570\u91cf\u968f\u5c42\u7ea7\u5448\u6307\u6570\u589e\u957f\uff1b2\uff09\u4e8c\u5206\u56fe\u8fde\u63a5\u8c31\u7cfb\u4e2d\u7684\u8fde\u7eed\u5c42\u7ea7\uff0c\u5e76\u7ea6\u675f\u5c42\u7ea7\u95f4\u7684\u77e9\u9635\uff1b3\uff09\u5229\u7528\u8c31\u7cfb\u5185\u7684\u5ef6\u957f\u56fe\u8c31\uff0c\u53ef\u4ee5\u5b9a\u4e49\u8fde\u7eed\u5c42\u7ea7\u95f4\u56fe\u7684\u6d3e\u751f\u8ddd\u79bb\u5ea6\u91cf\uff1b4\uff09\u5b9a\u4e49\u4e86\u4e00\u7c7b\u201c\u5206\u7ea7\u56fe\u201d\uff0c\u5e76\u5229\u7528\u5176\u63a8\u5bfc\u51fa\u6807\u51c6\u4ee3\u6570\u56fe\u8fd0\u7b97\u548c\u7c7b\u578b\u6784\u9020\u5668\uff08\u4ea4\u53c9\u79ef\u3001\u76d2\u79ef\u3001\u4e0d\u76f8\u4ea4\u548c\u3001\u51fd\u6570\u7c7b\u578b\uff09\u7684\u4f4e\u6210\u672c\u201c\u9aa8\u67b6\u201d\u53d8\u4f53\uff1b5\uff09\u8fd9\u4e9b\u9aa8\u67b6\u4e8c\u5143\u8fd0\u7b97\u7b26\u5177\u6709\u4e0e\u6807\u51c6\u8fd0\u7b97\u7b26\u76f8\u4f3c\u4f46\u4e0d\u5b8c\u5168\u76f8\u540c\u7684\u4ee3\u6570\u548c\u8303\u7574\u8bba\u6027\u8d28\uff1b6\uff09\u56fe\u8c31\u7cfb\u53ca\u5176\u9aa8\u67b6\u4e58\u79ef\u6784\u9020\u5668\u53ef\u4ee5\u903c\u8fd1\u8fde\u7eed\u4f53\u6781\u9650\u5bf9\u8c61\u3002\u6b64\u5916\uff0c\u8fd8\u63a8\u5bfc\u4e86\u5206\u7ea7\u56fe\u4e0a\u7684\u4e00\u4e2a\u7a7a\u95f4\u9ad8\u6548\u7684\u4e00\u5143\u8fd0\u7b97\u7b26\uff1a\u52a0\u539a\uff0c\u7528\u4e8e\u521b\u5efa\u591a\u5c3a\u5ea6\u56fe\u8c31\u7cfb\uff1b\u4ee5\u53ca\u5347\u7ea7\uff0c\u7528\u4e8e\u521b\u5efa\u641c\u7d22\u524d\u6cbf\u56fe\u8c31\u7cfb\uff08\u4f5c\u4e3a\u81ea\u9002\u5e94\u7f51\u683c\u7684\u63a8\u5e7f\u4ee5\u53ca\u5b9a\u4e49\u201c\u9aa8\u67b6\u201d\u51fd\u6570\uff09\u3002", "result": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee3\u6570\u7c7b\u578b\u7406\u8bba\uff0c\u7528\u4e8e\u5206\u7ea7\u56fe\u548c\uff08\u5206\u5c42\uff09\u56fe\u8c31\u7cfb\uff0c\u5e76\u63a8\u5bfc\u4e86\u76f8\u5e94\u7684\u9aa8\u67b6\u7b97\u5b50\u548c\u7a7a\u95f4\u9ad8\u6548\u7684\u4e00\u5143\u7b97\u5b50\uff08\u52a0\u539a\u548c\u5347\u7ea7\uff09\u3002\u5c06\u6b64\u65b9\u6cd5\u5e94\u7528\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u7f51\u683c\u6570\u503c\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9a\u4e49\u5206\u5c42\u6a21\u578b\u67b6\u6784\u548c\u76f8\u5173\u7b97\u6cd5\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u201c\u5206\u7ea7\u56fe\u201d\u548c\uff08\u5206\u5c42\uff09\u56fe\u8c31\u7cfb\u521b\u5efa\u4e86\u4e00\u79cd\u4ee3\u6570\u7c7b\u578b\u7406\u8bba\uff0c\u6709\u671b\u7528\u4e8e\u5b9a\u4e49\u5206\u5c42\u6a21\u578b\u67b6\u6784\uff08\u201c\u5206\u5c42\u67b6\u6784\u201d\uff09\u4ee5\u53ca\u5728\u5176\u4e0a\u8fd0\u884c\u7684\u5c40\u90e8\u91c7\u6837\u3001\u641c\u7d22\u6216\u4f18\u5316\u7b97\u6cd5\u3002"}}
{"id": "2508.00154", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00154", "abs": "https://arxiv.org/abs/2508.00154", "authors": ["Babak Esmaeili", "Hamidreza Modares", "Stefano Di Cairano"], "title": "Data-Driven Motion Planning for Uncertain Nonlinear Systems", "comment": null, "summary": "This paper proposes a data-driven motion-planning framework for nonlinear\nsystems that constructs a sequence of overlapping invariant polytopes. Around\neach randomly sampled waypoint, the algorithm identifies a convex admissible\nregion and solves data-driven linear-matrix-inequality problems to learn\nseveral ellipsoidal invariant sets together with their local state-feedback\ngains. The convex hull of these ellipsoids, still invariant under a\npiece-wise-affine controller obtained by interpolating the gains, is then\napproximated by a polytope. Safe transitions between nodes are ensured by\nverifying the intersection of consecutive convex-hull polytopes and introducing\nan intermediate node for a smooth transition. Control gains are interpolated in\nreal time via simplex-based interpolation, keeping the state inside the\ninvariant polytopes throughout the motion. Unlike traditional approaches that\nrely on system dynamics models, our method requires only data to compute safe\nregions and design state-feedback controllers. The approach is validated\nthrough simulations, demonstrating the effectiveness of the proposed method in\nachieving safe, dynamically feasible paths for complex nonlinear systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u975e\u7ebf\u6027\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b66\u4e60\u4e0d\u53d8\u96c6\u6765\u786e\u4fdd\u5b89\u5168\u3001\u52a8\u6001\u53ef\u884c\u7684\u8def\u5f84\u3002", "motivation": "\u4e0e\u4f9d\u8d56\u7cfb\u7edf\u52a8\u529b\u5b66\u6a21\u578b\u7684\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u4ec5\u9700\u8981\u6570\u636e\u6765\u8ba1\u7b97\u5b89\u5168\u533a\u57df\u548c\u8bbe\u8ba1\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\u3002", "method": "\u8be5\u6846\u67b6\u56f4\u7ed5\u6bcf\u4e2a\u968f\u673a\u91c7\u6837\u7684\u822a\u8def\u70b9\uff0c\u901a\u8fc7\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u7684\u7ebf\u6027\u77e9\u9635\u4e0d\u7b49\u5f0f\u95ee\u9898\u6765\u5b66\u4e60\u591a\u4e2a\u692d\u5706\u4e0d\u53d8\u96c6\u53ca\u5176\u5c40\u90e8\u72b6\u6001\u53cd\u9988\u589e\u76ca\u3002\u8fd9\u4e9b\u4e0d\u53d8\u96c6\u7531\u5206\u6bb5\u4eff\u5c04\u63a7\u5236\u5668\u7ec4\u5408\u800c\u6210\uff0c\u5e76\u88ab\u591a\u9762\u4f53\u903c\u8fd1\u3002\u901a\u8fc7\u9a8c\u8bc1\u8fde\u7eed\u51f8\u5305\u591a\u9762\u4f53\u7684\u4ea4\u96c6\u5e76\u5f15\u5165\u4e2d\u95f4\u8282\u70b9\u6765\u5b9e\u73b0\u5b89\u5168\u8f6c\u6362\u3002", "result": "\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u590d\u6742\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u5b89\u5168\u3001\u52a8\u6001\u53ef\u884c\u7684\u8def\u5f84\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4eff\u771f\u5f97\u5230\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5728\u590d\u6742\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u5b89\u5168\u3001\u52a8\u6001\u53ef\u884c\u7684\u8def\u5f84\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.00032", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2508.00032", "abs": "https://arxiv.org/abs/2508.00032", "authors": ["Alessio Buscemi", "Daniele Proverbio", "Alessandro Di Stefano", "The Anh Han", "German Castignani", "Pietro Li\u00f2"], "title": "Strategic Communication and Language Bias in Multi-Agent LLM Coordination", "comment": null, "summary": "Large Language Model (LLM)-based agents are increasingly deployed in\nmulti-agent scenarios where coordination is crucial but not always assured.\nPrevious studies indicate that the language used to frame strategic scenarios\ncan influence cooperative behavior. This paper explores whether allowing agents\nto communicate amplifies these language-driven effects. Leveraging the FAIRGAME\nframework, we simulate one-shot and repeated games across different languages\nand models, both with and without communication. Our experiments, conducted\nwith two advanced LLMs, GPT-4o and Llama 4 Maverick, reveal that communication\nsignificantly influences agent behavior, though its impact varies by language,\npersonality, and game structure. These findings underscore the dual role of\ncommunication in fostering coordination and reinforcing biases.", "AI": {"tldr": "LLM\u4ee3\u7406\u95f4\u7684\u6c9f\u901a\u4f1a\u56e0\u8bed\u8a00\u3001\u4e2a\u6027\u548c\u6e38\u620f\u7ed3\u6784\u7684\u4e0d\u540c\u800c\u5f71\u54cd\u5408\u4f5c\u884c\u4e3a\uff0c\u65e2\u80fd\u4fc3\u8fdb\u534f\u8c03\uff0c\u4e5f\u53ef\u80fd\u5f3a\u5316\u504f\u89c1\u3002", "motivation": "\u63a2\u8ba8\u4e86LLM\u4ee3\u7406\u4e4b\u95f4\u7684\u6c9f\u901a\u662f\u5426\u4f1a\u653e\u5927\u8bed\u8a00\u5bf9\u5408\u4f5c\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528FAIRGAME\u6846\u67b6\uff0c\u5728\u6709\u65e0\u6c9f\u901a\u7684\u60c5\u51b5\u4e0b\uff0c\u9488\u5bf9\u4e0d\u540c\u8bed\u8a00\u548c\u6a21\u578b\u6a21\u62df\u4e86\u5355\u5c40\u548c\u91cd\u590d\u535a\u5f08\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6c9f\u901a\u663e\u8457\u5f71\u54cd\u4ee3\u7406\u884c\u4e3a\uff0c\u5e76\u4e14\u8fd9\u79cd\u5f71\u54cd\u56e0\u8bed\u8a00\u3001\u4e2a\u6027\u548c\u6e38\u620f\u7ed3\u6784\u800c\u5f02\u3002", "conclusion": "LLM\u4ee3\u7406\u4e4b\u95f4\u7684\u6c9f\u901a\u4f1a\u653e\u5927\u8bed\u8a00\u5bf9\u5408\u4f5c\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u4f46\u8fd9\u79cd\u5f71\u54cd\u56e0\u8bed\u8a00\u3001\u4e2a\u6027\u548c\u6e38\u620f\u7ed3\u6784\u800c\u5f02\u3002\u6c9f\u901a\u5728\u4fc3\u8fdb\u534f\u8c03\u548c\u5f3a\u5316\u504f\u89c1\u65b9\u9762\u8d77\u7740\u53cc\u91cd\u4f5c\u7528\u3002"}}
{"id": "2508.00227", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.00227", "abs": "https://arxiv.org/abs/2508.00227", "authors": ["Yun Huang", "Kai Xu", "Zexi Liang", "Huaizhi Li", "Wenjuan Zhu", "Donglei Emma Fan"], "title": "Scalable, Wireless Determination of Electric Properties of Nanostructures via Electro-Rotation in Water Solution", "comment": null, "summary": "Breakthroughs in nanotechnology have enabled the large-scale fabrication of\nnanoparticles with varied compositions and structures. Yet, evaluating their\nelectrical conductivities remains challenging due to high volume and individual\nvariability. We report a rapid, wireless, and parallel method to characterize\nlongitudinal nanostructures, including insulators, semiconductors, and\nconducting metal oxides by using MoO3, MoS2/MoO2, and MoS2 nanoribbons,\nproduced at different fabrication stages, as a model system. Leveraging our\nsemi-quantitative model based on Maxwell-Wagner and electrical double-layer\npolarization, electric conductivities of various nanoparticles are determined\nfrom their distinct electro-rotation behaviors in water, spanning six orders of\nmagnitude. The results agree well with standard four-probe measurements. These\nfindings highlight a non-destruction, rapid, simple characterization method\npromising to bring nanomaterials closer to practical applications in\nelectronics, optics, sensing, catalysis, and robotics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u65e0\u7ebf\u3001\u5e73\u884c\u8868\u5f81\u7eb3\u7c73\u7ed3\u6784\u7535\u5bfc\u7387\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5176\u5728\u6c34\u4e2d\u7684\u7535\u65cb\u8f6c\u884c\u4e3a\uff0c\u5e76\u4e0e\u4f20\u7edf\u6d4b\u91cf\u65b9\u6cd5\u7684\u7ed3\u679c\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u7eb3\u7c73\u6280\u672f\u5728\u7eb3\u7c73\u7c92\u5b50\u5927\u89c4\u6a21\u5236\u9020\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4f46\u7531\u4e8e\u4f53\u79ef\u5927\u548c\u4e2a\u4f53\u5dee\u5f02\u5927\uff0c\u8bc4\u4f30\u5176\u7535\u5bfc\u7387\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5229\u7528\u57fa\u4e8eMaxwell-Wagner\u548c\u53cc\u7535\u5c42\u6781\u5316\u7684\u534a\u5b9a\u91cf\u6a21\u578b\uff0c\u901a\u8fc7\u7eb3\u7c73\u7ed3\u6784\u5728\u6c34\u4e2d\u7684\u7535\u65cb\u8f6c\u884c\u4e3a\uff0c\u786e\u5b9a\u5176\u7535\u5bfc\u7387\uff0c\u6570\u636e\u4e0e\u56db\u63a2\u9488\u6d4b\u91cf\u7ed3\u679c\u543b\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u8868\u5f81\u5305\u62ec\u7edd\u7f18\u4f53\u3001\u534a\u5bfc\u4f53\u548c\u5bfc\u7535\u91d1\u5c5e\u6c27\u5316\u7269\u5728\u5185\u7684\u5404\u79cd\u7eb3\u7c73\u7ed3\u6784\uff0c\u7535\u5bfc\u7387\u8de8\u8d8a\u516d\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u975e\u7834\u574f\u6027\u3001\u5feb\u901f\u3001\u7b80\u4fbf\u7684\u8868\u5f81\u65b9\u6cd5\uff0c\u6709\u671b\u63a8\u52a8\u7eb3\u7c73\u6750\u6599\u5728\u7535\u5b50\u3001\u5149\u5b66\u3001\u4f20\u611f\u3001\u50ac\u5316\u548c\u673a\u5668\u4eba\u7b49\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.00214", "categories": ["cond-mat.mes-hall", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2508.00214", "abs": "https://arxiv.org/abs/2508.00214", "authors": ["Lucien Jezequel", "Jens H. Bardarson", "Adolfo G. Grushin"], "title": "Explicit equivalence between the spectral localizer and local Chern and winding markers", "comment": null, "summary": "Topological band insulators are classified using momentum-space topological\ninvariants, such as Chern or winding numbers, when they feature translational\nsymmetry. The lack of translation symmetry in disordered, quasicrystalline, or\namorphous topological systems has motivated alternative, real-space definitions\nof topological invariants, including the local Chern marker and the spectral\nlocalizer invariant.\n  However, the equivalence between these invariants is so far implicit. Here,\nwe explicitly demonstrate their equivalence from a systematic perturbative\nexpansion in powers of the spectral localizer's parameter $\\kappa$. By\nleveraging only the Clifford algebra of the spectral localizer, we prove that\nChern and winding markers emerge as leading-order terms in the expansion. It\nbypasses abstract topological machinery, offering a simple approach accessible\nto a broader physics audience.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u8c31\u5c40\u57df\u5316\u4e0d\u53d8\u91cf\u548c\u9648/\u7ed5\u7ec4\u6570\u4e0d\u53d8\u91cf\u4e4b\u95f4\u7684\u7b49\u4ef7\u6027\u3002", "motivation": "\u7f3a\u5c11\u5e73\u79fb\u5bf9\u79f0\u6027\u7684\u65e0\u5e8f\u3001\u51c6\u6676\u6216\u975e\u6676\u62d3\u6251\u7cfb\u7edf\uff0c\u4fc3\u4f7f\u4eba\u4eec\u5bf9\u62d3\u6251\u4e0d\u53d8\u91cf\u8fdb\u884c\u66ff\u4ee3\u6027\u7684\u5b9e\u7a7a\u95f4\u5b9a\u4e49\uff0c\u5305\u62ec\u5c40\u57df\u9648\u6807\u8bb0\u548c\u8c31\u5c40\u57df\u5316\u4e0d\u53d8\u91cf\u3002", "method": "\u901a\u8fc7\u5229\u7528\u8c31\u5c40\u57df\u5316\u7b97\u5b50\u7684Clifford\u4ee3\u6570\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u9648\u548c\u7ed5\u7ec4\u6807\u8bb0\u4f5c\u4e3a\u5c55\u5f00\u7684\u9886\u5148\u9636\u9879\u51fa\u73b0\u3002", "result": "\u9648\u548c\u7ed5\u7ec4\u6807\u8bb0\u4f5c\u4e3a\u5c55\u5f00\u7684\u9886\u5148\u9636\u9879\u51fa\u73b0\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u7684\u3001\u53ef\u4f9b\u66f4\u5e7f\u6cdb\u7269\u7406\u5b66\u53d7\u4f17\u4f7f\u7528\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u4ece\u8c31\u5c40\u57df\u5316\u7b97\u5b50\u7684\u53c2\u6570$\nu$\u7684\u5e42\u6b21\u51fa\u53d1\uff0c\u660e\u786e\u5c55\u793a\u4e86\u8c31\u5c40\u57df\u5316\u4e0d\u53d8\u91cf\u548c\u9648/\u7ed5\u7ec4\u6570\u4e0d\u53d8\u91cf\u4e4b\u95f4\u7684\u7b49\u4ef7\u6027\u3002"}}
{"id": "2508.00093", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00093", "abs": "https://arxiv.org/abs/2508.00093", "authors": ["Lucas Alves Zischler", "Chiara Lasagni", "Paolo Serena", "Alberto Bononi", "Giammarco Di Sciullo", "Divya A. Shaji", "Antonio Mecozzi", "Cristian Antonelli"], "title": "Closed-form Expression for the Power Profile in Wideband Systems with Inter-channel Stimulated Raman Scattering", "comment": "Submitted for the Journal of Lightwave Technology", "summary": "Wideband systems experience significant inter-channel stimulated Raman\nscattering (ISRS) and channel-dependent losses. Due to the non-uniform\nattenuation profile, the combined effects of ISRS and fiber loss can only be\naccurately estimated using numerical methods. In this work, we present an\napproximate closed-form expression for the channels' power profile accounting\nfor these combined effects. We validate the proposed expression against\nnumerical solutions in the case of CLU transmission, showing high accuracy for\nboth single-span and multi-span fiber-optic links. Additionally, we derive an\ninverse expression, formulated as a function of the output power, which can be\nutilized to target a desired optical signal-to-noise ratio (OSNR) profile\nthrough pre-emphasis of the launched channel powers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8fd1\u4f3c\u516c\u5f0f\u6765\u4f30\u7b97\u5bbd\u5e26\u7cfb\u7edf\u4e2d\u7684ISRS\u548c\u901a\u9053\u635f\u8017\uff0c\u5e76\u7528\u4e8e\u8c03\u6574\u8f93\u5165\u529f\u7387\u4ee5\u4f18\u5316\u4fe1\u53f7\u8d28\u91cf\u3002", "motivation": "\u5bbd\u5e26\u7cfb\u7edf\u4e2d\u7684ISRS\u548c\u901a\u9053\u76f8\u5173\u635f\u8017\u5f71\u54cd\u4fe1\u53f7\u8d28\u91cf\uff0c\u9700\u8981\u7cbe\u786e\u4f30\u7b97\u3002", "method": "\u63a8\u5bfc\u4e86\u4e00\u4e2a\u8003\u8651ISRS\u548c\u901a\u9053\u76f8\u5173\u635f\u8017\u7684\u901a\u9053\u529f\u7387\u5206\u5e03\u7684\u8fd1\u4f3c\u95ed\u5408\u5f62\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u63a8\u5bfc\u4e86\u4e00\u4e2a\u53cd\u5411\u8868\u8fbe\u5f0f\u4ee5\u5b9e\u73b0\u76ee\u6807OSNR\u3002", "result": "\u63d0\u51fa\u7684\u8fd1\u4f3c\u95ed\u5408\u5f62\u5f0f\u8868\u8fbe\u5f0f\u5728CLU\u4f20\u8f93\u4e2d\u4e0e\u6570\u503c\u89e3\u76f8\u6bd4\u663e\u793a\u51fa\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8003\u8651\u4e86ISRS\u548c\u901a\u9053\u76f8\u5173\u635f\u8017\u7684\u901a\u9053\u529f\u7387\u5206\u5e03\u7684\u8fd1\u4f3c\u95ed\u5408\u5f62\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7\u4e0e\u6570\u503c\u89e3\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9ad8\u7cbe\u5ea6\u4e0b\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63a8\u5bfc\u4e86\u4e00\u4e2a\u53cd\u5411\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u901a\u8fc7\u9884\u52a0\u91cd\u6765\u8fbe\u5230\u76ee\u6807OSNR\u3002"}}
{"id": "2508.00097", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00097", "abs": "https://arxiv.org/abs/2508.00097", "authors": ["Zhigen Zhao", "Liuchuan Yu", "Ke Jing", "Ning Yang"], "title": "XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation", "comment": "6 pages, 6 figures, project link: https://github.com/XR-Robotics", "summary": "The rapid advancement of Vision-Language-Action models has created an urgent\nneed for large-scale, high-quality robot demonstration datasets. Although\nteleoperation is the predominant method for data collection, current approaches\nsuffer from limited scalability, complex setup procedures, and suboptimal data\nquality. This paper presents XRoboToolkit, a cross-platform framework for\nextended reality based robot teleoperation built on the OpenXR standard. The\nsystem features low-latency stereoscopic visual feedback, optimization-based\ninverse kinematics, and support for diverse tracking modalities including head,\ncontroller, hand, and auxiliary motion trackers. XRoboToolkit's modular\narchitecture enables seamless integration across robotic platforms and\nsimulation environments, spanning precision manipulators, mobile robots, and\ndexterous hands. We demonstrate the framework's effectiveness through precision\nmanipulation tasks and validate data quality by training VLA models that\nexhibit robust autonomous performance.", "AI": {"tldr": "XRoboToolkit\u662f\u4e00\u4e2a\u57fa\u4e8eXR\u548cOpenXR\u7684\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u6536\u96c6\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u5e76\u6210\u529f\u7528\u4e8e\u8bad\u7ec3VLA\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u5728\u6570\u636e\u6536\u96c6\u65b9\u9762\u5b58\u5728\u53ef\u6269\u5c55\u6027\u6709\u9650\u3001\u8bbe\u7f6e\u590d\u6742\u548c\u6570\u636e\u8d28\u91cf\u4e0d\u4f73\u7b49\u95ee\u9898\uff0c\u8fd9\u963b\u788d\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u3002", "method": "\u63d0\u51faXRoboToolkit\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8eOpenXR\u6807\u51c6\uff0c\u5229\u7528\u6269\u5c55\u73b0\u5b9e\u6280\u672f\u8fdb\u884c\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u3002\u7cfb\u7edf\u5305\u542b\u4f4e\u5ef6\u8fdf\u7acb\u4f53\u89c6\u89c9\u53cd\u9988\u3001\u57fa\u4e8e\u4f18\u5316\u7684\u9006\u8fd0\u52a8\u5b66\uff0c\u5e76\u652f\u6301\u5934\u90e8\u3001\u63a7\u5236\u5668\u3001\u624b\u90e8\u53ca\u8f85\u52a9\u8fd0\u52a8\u8ffd\u8e2a\u5668\u7b49\u591a\u79cd\u8ffd\u8e2a\u6a21\u5f0f\u3002\u5176\u6a21\u5757\u5316\u67b6\u6784\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u673a\u5668\u4eba\u5e73\u53f0\u548c\u6a21\u62df\u73af\u5883\u4e2d\u3002", "result": "XRoboToolkit\u5df2\u88ab\u8bc1\u660e\u80fd\u6709\u6548\u652f\u6301\u673a\u5668\u4eba\u9065\u64cd\u4f5c\uff0c\u5e76\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u7528\u4e8e\u8bad\u7ec3\u51fa\u5177\u6709\u7a33\u5065\u81ea\u4e3b\u6027\u80fd\u7684VLA\u6a21\u578b\u3002\u6846\u67b6\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u4f7f\u5176\u80fd\u591f\u517c\u5bb9\u591a\u79cd\u673a\u5668\u4eba\u7c7b\u578b\uff08\u7cbe\u5bc6\u64cd\u4f5c\u5668\u3001\u79fb\u52a8\u673a\u5668\u4eba\u3001\u7075\u5de7\u624b\uff09\u548c\u6a21\u62df\u73af\u5883\u3002", "conclusion": "XRoboToolkit\u662f\u4e00\u4e2a\u8de8\u5e73\u53f0\u6846\u67b6\uff0c\u4f7f\u7528\u6269\u5c55\u73b0\u5b9e\u548cOpenXR\u6807\u51c6\u8fdb\u884c\u673a\u5668\u4eba\u9065\u64cd\u4f5c\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u9065\u64cd\u4f5c\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3001\u8bbe\u7f6e\u590d\u6742\u6027\u548c\u6570\u636e\u8d28\u91cf\u95ee\u9898\u3002\u8be5\u6846\u67b6\u652f\u6301\u591a\u79cd\u8ffd\u8e2a\u6a21\u5f0f\uff0c\u5e76\u80fd\u4e0e\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u548c\u6a21\u62df\u73af\u5883\u96c6\u6210\u3002\u901a\u8fc7\u5728\u7cbe\u5bc6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u548c\u8bad\u7ec3VLA\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u3002\u672a\u6765\u53ef\u7528\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u548c\u6570\u636e\u6536\u96c6\u7684\u6548\u7387\u4e0e\u8d28\u91cf\u3002"}}
{"id": "2508.00053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00053", "abs": "https://arxiv.org/abs/2508.00053", "authors": ["Jie Zhu", "Yiyang Su", "Minchul Kim", "Anil Jain", "Xiaoming Liu"], "title": "A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition", "comment": "Accepted to ICCV 2025. 11 pages, 5 figures", "summary": "Whole-body biometric recognition is a challenging multimodal task that\nintegrates various biometric modalities, including face, gait, and body. This\nintegration is essential for overcoming the limitations of unimodal systems.\nTraditionally, whole-body recognition involves deploying different models to\nprocess multiple modalities, achieving the final outcome by score-fusion (e.g.,\nweighted averaging of similarity matrices from each model). However, these\nconventional methods may overlook the variations in score distributions of\nindividual modalities, making it challenging to improve final performance. In\nthis work, we present \\textbf{Q}uality-guided \\textbf{M}ixture of score-fusion\n\\textbf{E}xperts (QME), a novel framework designed for improving whole-body\nbiometric recognition performance through a learnable score-fusion strategy\nusing a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for\nquality estimation with a modality-specific Quality Estimator (QE), and a score\ntriplet loss to improve the metric performance. Extensive experiments on\nmultiple whole-body biometric datasets demonstrate the effectiveness of our\nproposed approach, achieving state-of-the-art results across various metrics\ncompared to baseline methods. Our method is effective for multimodal and\nmulti-model, addressing key challenges such as model misalignment in the\nsimilarity score domain and variability in data quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQME\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6a21\u578b\u548c\u4f2a\u8d28\u91cf\u635f\u5931\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u751f\u7269\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u5355\u4e00\u751f\u7269\u8bc6\u522b\u6a21\u6001\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u6574\u5408\u5305\u62ec\u4eba\u8138\u3001\u6b65\u6001\u548c\u8eab\u4f53\u5728\u5185\u7684\u591a\u79cd\u751f\u7269\u8bc6\u522b\u6a21\u6001\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u8bc4\u5206\u878d\u5408\uff08\u5982\u52a0\u6743\u5e73\u5747\u76f8\u4f3c\u5ea6\u77e9\u9635\uff09\u7684\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u8003\u8651\u5404\u6a21\u6001\u8bc4\u5206\u5206\u5e03\u7684\u5dee\u5f02\uff0c\u4ece\u800c\u9650\u5236\u4e86\u6700\u7ec8\u6027\u80fd\u7684\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQME\uff08Quality-guided Mixture of score-fusion Experts\uff09\u7684\u65b0\u578b\u6846\u67b6\uff0c\u91c7\u7528\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6a21\u578b\u8fdb\u884c\u53ef\u5b66\u4e60\u7684\u8bc4\u5206\u878d\u5408\u3002\u8be5\u6846\u67b6\u8fd8\u5f15\u5165\u4e86\u4f2a\u8d28\u91cf\u635f\u5931\u6765\u6307\u5bfc\u8d28\u91cf\u4f30\u8ba1\uff08\u901a\u8fc7\u7279\u5b9a\u6a21\u6001\u7684\u8d28\u91cf\u4f30\u8ba1\u5668QE\uff09\uff0c\u5e76\u4f7f\u7528\u4e86\u8bc4\u5206\u4e09\u5143\u7ec4\u635f\u5931\u6765\u4f18\u5316\u5ea6\u91cf\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u6574\u4f53\u751f\u7269\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u5404\u9879\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6700\u65b0\u6210\u679c\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u548c\u591a\u6a21\u578b\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u76f8\u4f3c\u5ea6\u8bc4\u5206\u57df\u4e2d\u7684\u6a21\u578b\u4e0d\u5bf9\u9f50\u4ee5\u53ca\u6570\u636e\u8d28\u91cf\u53d8\u5316\u7b49\u95ee\u9898\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684QME\u6846\u67b6\u901a\u8fc7\u91c7\u7528\u53ef\u5b66\u4e60\u7684\u8bc4\u5206\u878d\u5408\u7b56\u7565\uff08MoE\uff09\uff0c\u5e76\u7ed3\u5408\u4f2a\u8d28\u91cf\u635f\u5931\u548c\u8bc4\u5206\u4e09\u5143\u7ec4\u635f\u5931\uff0c\u5728\u591a\u9879\u751f\u7269\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6574\u4f53\u751f\u7269\u8bc6\u522b\u7ed3\u679c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u578b\u5728\u76f8\u4f3c\u5ea6\u8bc4\u5206\u57df\u4e2d\u7684\u4e0d\u5bf9\u9f50\u4ee5\u53ca\u6570\u636e\u8d28\u91cf\u7684\u53ef\u53d8\u6027\u7b49\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2508.00082", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.00082", "abs": "https://arxiv.org/abs/2508.00082", "authors": ["Aditya Kumar", "Sadeed Hameed", "Thibaud Denneulin", "Aravind Puthirath Balan", "Joseph Vas", "Kilian Leutner", "Lei Gao", "Olena Gomonay", "Jairo Sinova", "Rafal E. Dunin-Borkowski", "Mathias Kl\u00e4ui"], "title": "Switchable Exchange Bias Resulting from Correlated Domain Structures in Orthogonally Coupled Antiferromagnet/Ferromagnet van der Waals Heterostructures", "comment": null, "summary": "Van der Waals (vdW) magnetic heterostructures offer a versatile platform for\nengineering interfacial spin interactions with atomic precision, enabling\nnontrivial spin textures and dynamic behaviors. In this work, we report robust\nasymmetric magnetization reversal and exchange bias in Fe3GeTe2 (FGT), driven\nby interlayer exchange coupling with the A-type antiferromagnet CrSBr. Despite\nthe orthogonal magnetic anisotropies out-of-plane easy axis in FGT and in-plane\nin CrSBr, we observe a strong interfacial exchange interaction that gives rise\nto pronounced and switchable exchange bias and asymmetric switching in FGT,\npersisting up to the N\\'eel temperature of CrSBr (132 K) as revealed by\nanomalous Hall effect measurements.\n  We uncover the microscopic origin of this behavior through cross-sectional\nmagnetic imaging of the domain structure using off-axis electron holography.\nThe results reveal that the asymmetric switching and exchange bias arise from\nthe influence of CrSBr on the domain configuration of FGT, where the in-plane\nantiferromagnetic state of CrSBr promotes the formation of stripe-like domain\nstructures in FGT with circular rotation of magnetization in the\ncross-sectional bc plane defined by the easy axes of both FGT and CrSBr. These\nfindings elucidate the mechanism of exchange bias in orthogonally coupled van\nder Waals systems and demonstrate a pathway for stabilizing three-dimensional\ndomain structures in ferromagnets through interfacial exchange interactions.", "AI": {"tldr": "\u901a\u8fc7CrSBr\u5bf9FGT\u7574\u6784\u578b\u7684\u4f5c\u7528\uff0c\u63ed\u793a\u4e86Fe3GeTe2 (FGT) \u4e2d\u4e0d\u5bf9\u79f0\u78c1\u5316\u53cd\u8f6c\u548c\u4ea4\u6362\u504f\u501a\u7684\u673a\u5236\u3002", "motivation": "\u4e3a\u5728\u539f\u5b50\u7cbe\u5ea6\u4e0b\u5de5\u7a0b\u5316\u754c\u9762\u81ea\u65cb\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u975e\u5e73\u51e1\u7684\u81ea\u65cb\u7eb9\u7406\u548c\u52a8\u6001\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u7684\u5e73\u53f0\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u5f02\u76f8\u7535\u5b50\u5168\u606f\u672f\u5bf9\u7574\u7ed3\u6784\u8fdb\u884c\u6a2a\u622a\u9762\u78c1\u6210\u50cf\uff0c\u63ed\u793a\u4e86\u4e0d\u5bf9\u79f0\u5f00\u5173\u548c\u4ea4\u6362\u504f\u501a\u7684\u5fae\u89c2\u8d77\u6e90\u3002", "result": "\u5728Fe3GeTe2 (FGT) \u4e2d\u89c2\u5bdf\u5230\u5f3a\u70c8\u7684\u754c\u9762\u4ea4\u6362\u76f8\u4e92\u4f5c\u7528\uff0c\u4ece\u800c\u4ea7\u751f\u660e\u663e\u7684\u3001\u53ef\u5207\u6362\u7684\u4ea4\u6362\u504f\u501a\u548c\u4e0d\u5bf9\u79f0\u5f00\u5173\uff0c\u8fd9\u79cd\u6548\u5e94\u4e00\u76f4\u6301\u7eed\u5230CrSBr\u7684N'eel\u6e29\u5ea6\uff08132 K\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u6b63\u4ea4\u8026\u5408\u8303\u5fb7\u534e\u4f53\u7cfb\u4e2d\u4ea4\u6362\u504f\u501a\u7684\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u754c\u9762\u4ea4\u6362\u76f8\u4e92\u4f5c\u7528\u8bc1\u660e\u4e86\u5728\u94c1\u78c1\u4f53\u4e2d\u7a33\u5b9a\u4e09\u7ef4\u7574\u7ed3\u6784\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.00212", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00212", "abs": "https://arxiv.org/abs/2508.00212", "authors": ["J. Fernando Hernandez-Garcia", "Shibhansh Dohare", "Jun Luo", "Rich S. Sutton"], "title": "Reinitializing weights vs units for maintaining plasticity in neural networks", "comment": null, "summary": "Loss of plasticity is a phenomenon in which a neural network loses its\nability to learn when trained for an extended time on non-stationary data. It\nis a crucial problem to overcome when designing systems that learn continually.\nAn effective technique for preventing loss of plasticity is reinitializing\nparts of the network. In this paper, we compare two different reinitialization\nschemes: reinitializing units vs reinitializing weights. We propose a new\nalgorithm, which we name \\textit{selective weight reinitialization}, for\nreinitializing the least useful weights in a network. We compare our algorithm\nto continual backpropagation and ReDo, two previously proposed algorithms that\nreinitialize units in the network. Through our experiments in continual\nsupervised learning problems, we identify two settings when reinitializing\nweights is more effective at maintaining plasticity than reinitializing units:\n(1) when the network has a small number of units and (2) when the network\nincludes layer normalization. Conversely, reinitializing weights and units are\nequally effective at maintaining plasticity when the network is of sufficient\nsize and does not include layer normalization. We found that reinitializing\nweights maintains plasticity in a wider variety of settings than reinitializing\nunits.", "AI": {"tldr": "This paper compares reinitializing network units versus weights to prevent plasticity loss in continually learning neural networks. The proposed selective weight reinitialization method is more effective in certain scenarios, like smaller networks or networks with layer normalization, and generally maintains plasticity better across various settings.", "motivation": "Loss of plasticity is a crucial problem to overcome when designing systems that learn continually. This paper investigates effective techniques for preventing loss of plasticity, specifically comparing reinitializing units vs. reinitializing weights.", "method": "We propose a new algorithm, selective weight reinitialization, which reinitializes the least useful weights in a network. We compare this algorithm to continual backpropagation and ReDo, which reinitialize units in the network.", "result": "Experiments in continual supervised learning problems show that reinitializing weights is more effective than reinitializing units when the network has a small number of units or includes layer normalization. Otherwise, both methods are equally effective. Reinitializing weights maintains plasticity in a wider variety of settings.", "conclusion": "Reinitializing weights maintains plasticity in a wider variety of settings than reinitializing units."}}
{"id": "2508.00776", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.00776", "abs": "https://arxiv.org/abs/2508.00776", "authors": ["Dieter van Melkebeek"], "title": "From Dynamic Programs to Greedy Algorithms", "comment": "14 pages, 2 figures", "summary": "We show for several computational problems how classical greedy algorithms\nfor special cases can be derived in a simple way from dynamic programs for the\ngeneral case: interval scheduling (restricted to unit weights), knapsack\n(restricted to unit values), and shortest paths (restricted to nonnegative edge\nlengths). Conceptually, we repeatedly expand the Bellman equations underlying\nthe dynamic program and use straightforward monotonicity properties to figure\nout which terms yield the optimal value under the respective restrictions. The\napproach offers an alternative for developing these greedy algorithms in\nundergraduate algorithms courses and/or for arguing their correctness. In the\nsetting of interval scheduling, it elucidates the change in order from earliest\nstart time first for the memoized dynamic program to earliest finish time first\nfor the greedy algorithm.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.00003", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2508.00003", "abs": "https://arxiv.org/abs/2508.00003", "authors": ["Kang Rong Roy Ang"], "title": "Building Bigraphs of the real world", "comment": "Submitted in partial fulfilment of the requirements for Part II of\n  the Computer Science Tripos at the University of Cambridge", "summary": "This report proposes a formal specification for organising all buildings,\nstreets and administrative areas in the world into a hierarchical\nspace-partitioning tree using data from OpenStreetMap. This hierarchical\nstructure is encoded into a bigraph, serving as a digital twin of the world and\ncapturing complete street connectivity. It presents a tool implemented in OCaml\n(source code at https://github.com/royangkr/bigraph-of-the-world ) that\nconstructs bigraphs for regions from any part of the world. In addition, it\ncontributes algorithmic improvements to open-source bigraph-building tools that\nenable them to efficiently construct and transform extremely large bigraphs,\nachieving up to a 97x speedup among other gains.", "AI": {"tldr": "\u5229\u7528OpenStreetMap\u6570\u636e\u6784\u5efa\u5168\u7403\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u5e76\u4f18\u5316\u4e86\u76f8\u5173\u5de5\u5177\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5bf9\u5168\u7403\u7684\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\u8fdb\u884c\u7edf\u4e00\u7684\u3001\u5206\u5c42\u7684\u7ec4\u7ec7\uff0c\u5e76\u5b9e\u73b0\u5bf9\u8857\u9053\u8fde\u63a5\u6027\u7684\u5168\u9762\u6355\u83b7\uff0c\u4ee5\u6784\u5efa\u4e16\u754c\u7684\u6570\u5b57\u5b6a\u751f\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528OpenStreetMap\u6570\u636e\u6784\u5efa\u4e16\u754cbigraph\uff08\u4e00\u79cd\u6355\u83b7\u5b8c\u6574\u8857\u9053\u8fde\u63a5\u6027\u7684\u6570\u5b57\u5b6a\u751f\uff09\u7684\u65b9\u6cd5\uff0c\u8be5bigraph\u5c06\u5168\u7403\u5efa\u7b51\u7269\u3001\u8857\u9053\u548c\u884c\u653f\u533a\u57df\u7ec4\u7ec7\u6210\u4e00\u4e2a\u5206\u5c42\u7a7a\u95f4\u5212\u5206\u6811\u3002", "result": "\u5b9e\u73b0\u4e86\u4e00\u4e2aOCaml\u5de5\u5177\uff0c\u53ef\u4ee5\u4e3a\u4e16\u754c\u4efb\u4f55\u5730\u533a\u7684bigraph\u63d0\u4f9b\u6784\u5efa\uff0c\u5e76\u5bf9\u73b0\u6709\u7684\u5f00\u6e90bigraph\u6784\u5efa\u5de5\u5177\u8fdb\u884c\u4e86\u7b97\u6cd5\u6539\u8fdb\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe97\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u8be5\u62a5\u544a\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u89c4\u8303\uff0c\u5229\u7528\u6765\u81eaOpenStreetMap\u7684\u6570\u636e\uff0c\u5c06\u5168\u7403\u6240\u6709\u5efa\u7b51\u7269\u3001\u8857\u9053\u548c\u884c\u653f\u533a\u57df\u7ec4\u7ec7\u6210\u4e00\u4e2a\u5206\u5c42\u7a7a\u95f4\u5212\u5206\u6811\u3002\u8be5\u5206\u5c42\u7ed3\u6784\u88ab\u7f16\u7801\u4e3abigraph\uff0c\u4f5c\u4e3a\u4e16\u754c\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u5e76\u6355\u83b7\u5b8c\u6574\u7684\u8857\u9053\u8fde\u63a5\u6027\u3002"}}
{"id": "2508.00037", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00037", "abs": "https://arxiv.org/abs/2508.00037", "authors": ["Tong Nie", "Jian Sun", "Wei Ma"], "title": "Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion", "comment": "Accepted at IEEE Transactions on Industrial Informatics", "summary": "Networked urban systems facilitate the flow of people, resources, and\nservices, and are essential for economic and social interactions. These systems\noften involve complex processes with unknown governing rules, observed by\nsensor-based time series. To aid decision-making in industrial and engineering\ncontexts, data-driven predictive models are used to forecast spatiotemporal\ndynamics of urban systems. Current models such as graph neural networks have\nshown promise but face a trade-off between efficacy and efficiency due to\ncomputational demands. Hence, their applications in large-scale networks still\nrequire further efforts. This paper addresses this trade-off challenge by\ndrawing inspiration from physical laws to inform essential model designs that\nalign with fundamental principles and avoid architectural redundancy. By\nunderstanding both micro- and macro-processes, we present a principled\ninterpretable neural diffusion scheme based on Transformer-like structures\nwhose attention layers are induced by low-dimensional embeddings. The proposed\nscalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is\nvalidated on large-scale urban systems including traffic flow, solar power, and\nsmart meters, showing state-of-the-art performance and remarkable scalability.\nOur results constitute a fresh perspective on the dynamics prediction in\nlarge-scale urban networks.", "AI": {"tldr": "ScaleSTF\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u5927\u89c4\u6a21\u57ce\u5e02\u7cfb\u7edf\u7684\u65f6\u7a7a\u52a8\u6001\u3002", "motivation": "\u4e3a\u4e86\u5728\u5de5\u4e1a\u548c\u5de5\u7a0b\u9886\u57df\u8f85\u52a9\u51b3\u7b56\uff0c\u9700\u8981\u5bf9\u57ce\u5e02\u7cfb\u7edf\u4e2d\u7684\u590d\u6742\u8fc7\u7a0b\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u7684\u9884\u6d4b\u3002\u73b0\u6709\u6a21\u578b\uff08\u5982\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u5904\u7406\u5927\u89c4\u6a21\u7f51\u7edc\u65f6\u9762\u4e34\u6548\u7387\u548c\u6548\u679c\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u3001\u53ef\u6269\u5c55\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u3001\u5177\u6709\u4f4e\u7ef4\u5d4c\u5165\u8bf1\u5bfc\u6ce8\u610f\u529b\u5c42\u7684\u53ef\u89e3\u91ca\u795e\u7ecf\u6269\u6563\u65b9\u6848\uff08ScaleSTF\uff09\uff0c\u8be5\u65b9\u6848\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u7075\u611f\u6765\u6e90\u4e8e\u7269\u7406\u5b9a\u5f8b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6a21\u578b\uff08\u5982\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u6548\u7387\u548c\u6548\u679c\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "result": "ScaleSTF\u6a21\u578b\u5728\u4ea4\u901a\u6d41\u91cf\u3001\u592a\u9633\u80fd\u53d1\u7535\u548c\u667a\u80fd\u7535\u8868\u7b49\u5927\u89c4\u6a21\u57ce\u5e02\u7cfb\u7edf\u4e0a\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u663e\u8457\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aScaleSTF\u7684\u53ef\u6269\u5c55\u65f6\u7a7aTransformer\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u4ea4\u901a\u6d41\u91cf\u3001\u592a\u9633\u80fd\u53d1\u7535\u548c\u667a\u80fd\u7535\u8868\u7b49\u5927\u89c4\u6a21\u57ce\u5e02\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u9884\u6d4b\u5927\u89c4\u6a21\u57ce\u5e02\u7f51\u7edc\u4e2d\u7684\u52a8\u6001\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2508.00079", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00079", "abs": "https://arxiv.org/abs/2508.00079", "authors": ["Oshayer Siddique", "J. M Areeb Uzair Alam", "Md Jobayer Rahman Rafy", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "title": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems", "comment": "Under review, 18 pages, 4 figures, 7 tables", "summary": "The discipline of physics stands as a cornerstone of human intellect, driving\nthe evolution of technology and deepening our understanding of the fundamental\nprinciples of the cosmos. Contemporary literature includes some works centered\non the task of solving physics problems - a crucial domain of natural language\nreasoning. In this paper, we evaluate the performance of frontier LLMs in\nsolving physics problems, both mathematical and descriptive. We also employ a\nplethora of inference-time techniques and agentic frameworks to improve the\nperformance of the models. This includes the verification of proposed solutions\nin a cumulative fashion by other, smaller LLM agents, and we perform a\ncomparative analysis of the performance that the techniques entail. There are\nsignificant improvements when the multi-agent framework is applied to problems\nthat the models initially perform poorly on. Furthermore, we introduce a new\nevaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small\nVAL}}$, consisting of 19,609 problems sourced from various physics textbooks\nand their corresponding correct solutions scraped from physics forums and\neducational websites. Our code and data are publicly available at\nhttps://github.com/areebuzair/PhysicsEval.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u4ee3\u7406\u6846\u67b6\u548c\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6PHYSICEVAL\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7269\u7406\u5b66\u662f\u63a8\u52a8\u6280\u672f\u53d1\u5c55\u548c\u7406\u89e3\u5b87\u5b99\u57fa\u672c\u539f\u7406\u7684\u5173\u952e\u5b66\u79d1\u3002\u4e3a\u4e86\u63a8\u8fdb\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u5728\u7269\u7406\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u5e94\u7528\uff0c\u672c\u6587\u65e8\u5728\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e00\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u672c\u6587\u8bc4\u4f30\u4e86\u524d\u6cbf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u6570\u5b66\u548c\u63cf\u8ff0\u6027\u7269\u7406\u95ee\u9898\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u91c7\u7528\u4e86\u591a\u79cd\u63a8\u7406\u65f6\u6280\u672f\u548c\u4ee3\u7406\u6846\u67b6\uff08\u5982\u7d2f\u79ef\u9a8c\u8bc1\uff09\u6765\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002\u7814\u7a76\u8fdb\u884c\u4e86\u6280\u672f\u6027\u80fd\u7684\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u591a\u4ee3\u7406\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u521d\u59cb\u8868\u73b0\u4e0d\u4f73\u7684\u7269\u7406\u95ee\u9898\u4e0a\u7684\u6027\u80fd\u3002\u901a\u8fc7\u5f15\u5165PHYSICEVAL\u57fa\u51c6\uff0c\u4e3a\u672a\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u7269\u7406\u95ee\u9898\u89e3\u51b3\u6a21\u578b\u63d0\u4f9b\u4e86\u8d44\u6e90\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u524d\u6cbf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u7269\u7406\u95ee\u9898\uff08\u5305\u62ec\u6570\u5b66\u548c\u63cf\u8ff0\u6027\u95ee\u9898\uff09\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u4e86\u591a\u79cd\u63a8\u7406\u65f6\u6280\u672f\u548c\u4ee3\u7406\u6846\u67b6\u4ee5\u63d0\u9ad8\u5176\u8868\u73b0\u3002\u7814\u7a76\u8868\u660e\uff0c\u591a\u4ee3\u7406\u6846\u67b6\u5728\u6a21\u578b\u521d\u59cb\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u4e0a\u80fd\u5e26\u6765\u663e\u8457\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b19,609\u4e2a\u7269\u7406\u95ee\u9898\u53ca\u5176\u89e3\u51b3\u65b9\u6848\u7684\u65b0\u8bc4\u4f30\u57fa\u51c6PHYSICEVAL\u3002"}}
{"id": "2508.00295", "categories": ["cs.ET", "cs.AR", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.00295", "abs": "https://arxiv.org/abs/2508.00295", "authors": ["Md Mazharul Islam", "Diego Ferrer", "Shamiul Alam", "Juan P. Mendez", "Denis Mamaluy", "Wei Pan", "Ahmedullah Aziz"], "title": "Reimagining Voltage-Controlled Cryogenic Boolean Logic Paradigm with Quantum-Enhanced Josephson Junction FETs", "comment": null, "summary": "The growing demand for ultra low power computing and the emergence of quantum\ntechnologies have intensified interest in cryogenic electronics, particularly\nsuperconducting devices.Despite their promise, current controlled\nsuperconducting components face fundamental challenges in cascadability,\nlimiting their effectiveness in complex logic architectures.To overcome this,\nrecent efforts have focused on developing gate tunable superconducting devices,\nsuch as Josephson Junction Field Effect Transistors (JJFETs).However, achieving\nrobust control and sufficient supercurrent gain, both critical for\ntransistor-like performance in logic circuits remains a key challenge.A recent\nadvancement in JJFET design, based on InAs and GaSb heterostructures,\ndemonstrates enhanced gain and favorable device characteristics suitable for\ncircuit integration.Building on this innovation, we propose and analyze\nfundamental voltage controlled logic topologies using the quantum enhanced\nJJFET. We develop a Verilog A based circuit compatible compact model of the\nquantum enhanced JJFET which accurately captures the experimentally observed\ndevice characteristics.To ensure cascadability, our logic circuits incorporate\nthe multilayered Heater Nanocryotron (nTron), a superconducting nanowire-based\nthermal switch.Through simulation based analysis, we demonstrate the successful\nimplementation of fundamental logic gates, including NOT, NAND, and NOR.\nFurthermore, we design a 3 input majority gate, which plays a pivotal role in\nquantum and reversible computing due to its universality.Finally, to\ndemonstrate the cascadability of our proposed logic topology, we demonstrate\nthe operation of a 2 input XOR gate based on our designed JJFET based NOT,\nNAND, and NOR gate.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u589e\u5f3a JJFET \u7684\u7535\u538b\u63a7\u5236\u903b\u8f91\u62d3\u6251\u7ed3\u6784\uff0c\u901a\u8fc7\u96c6\u6210\u7eb3\u7c73\u4f4e\u6e29\u5668\u4ef6\u5b9e\u73b0\u4e86\u57fa\u672c\u903b\u8f91\u95e8\u548c XOR \u95e8\u7684\u53ef\u7ea7\u8054\u6027\uff0c\u4e3a\u8d85\u4f4e\u529f\u8017\u8ba1\u7b97\u548c\u91cf\u5b50\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u8d85\u5bfc\u5668\u4ef6\u5728\u53ef\u7ea7\u8054\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u4ee5\u6ee1\u8db3\u8d85\u4f4e\u529f\u8017\u8ba1\u7b97\u548c\u91cf\u5b50\u6280\u672f\u53d1\u5c55\u7684\u9700\u6c42\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u5177\u6709\u8db3\u591f\u8d85\u7535\u6d41\u589e\u76ca\u548c\u9c81\u68d2\u63a7\u5236\u7684\u7535\u538b\u63a7\u5236\u903b\u8f91\u5668\u4ef6\u3002", "method": "\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u57fa\u4e8e\u91cf\u5b50\u589e\u5f3a JJFET \u7684\u7535\u538b\u63a7\u5236\u903b\u8f91\u62d3\u6251\u7ed3\u6784\uff0c\u5f00\u53d1\u4e86 Verilog A \u7d27\u51d1\u6a21\u578b\uff0c\u5e76\u96c6\u6210\u4e86\u591a\u5c42\u52a0\u70ed\u7eb3\u7c73\u4f4e\u6e29\u5668\u4ef6\uff08nTron\uff09\u4ee5\u786e\u4fdd\u903b\u8f91\u7535\u8def\u7684\u53ef\u7ea7\u8054\u6027\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86 NOT\u3001NAND\u3001NOR \u548c XOR \u7b49\u57fa\u672c\u903b\u8f91\u95e8\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u8f93\u5165\u591a\u6570\u8868\u51b3\u5668\uff0c\u8bc1\u660e\u4e86\u91cf\u5b50\u589e\u5f3a JJFET \u53ca\u5176\u63d0\u51fa\u7684\u903b\u8f91\u62d3\u6251\u7ed3\u6784\u7684\u53ef\u7ea7\u8054\u6027\u3002", "conclusion": "\u901a\u8fc7\u6a21\u62df\u5206\u6790\uff0c\u6210\u529f\u5b9e\u73b0\u4e86 NOT\u3001NAND \u548c NOR \u7b49\u57fa\u672c\u903b\u8f91\u95e8\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e09\u8f93\u5165\u591a\u6570\u8868\u51b3\u5668\uff0c\u4ee5\u8bc1\u660e\u6240\u63d0\u51fa\u7684\u91cf\u5b50\u589e\u5f3a JJFET \u903b\u8f91\u62d3\u6251\u7ed3\u6784\u7684\u53ef\u7ea7\u8054\u6027\uff0c\u6700\u7ec8\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e JJFET \u7684 2 \u8f93\u5165 XOR \u95e8\u6f14\u793a\u4e86\u5176\u7ea7\u8054\u64cd\u4f5c\u3002"}}
{"id": "2508.00081", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00081", "abs": "https://arxiv.org/abs/2508.00081", "authors": ["Fred Mutisya", "Shikoh Gitau", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha"], "title": "Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench", "comment": null, "summary": "HealthBench, a benchmark designed to measure the capabilities of AI systems\nfor health better (Arora et al., 2025), has advanced medical language model\nevaluation through physician-crafted dialogues and transparent rubrics.\nHowever, its reliance on expert opinion, rather than high-tier clinical\nevidence, risks codifying regional biases and individual clinician\nidiosyncrasies, further compounded by potential biases in automated grading\nsystems. These limitations are particularly magnified in low- and middle-income\nsettings, where issues like sparse neglected tropical disease coverage and\nregion-specific guideline mismatches are prevalent.\n  The unique challenges of the African context, including data scarcity,\ninadequate infrastructure, and nascent regulatory frameworks, underscore the\nurgent need for more globally relevant and equitable benchmarks. To address\nthese shortcomings, we propose anchoring reward functions in version-controlled\nClinical Practice Guidelines (CPGs) that incorporate systematic reviews and\nGRADE evidence ratings.\n  Our roadmap outlines \"evidence-robust\" reinforcement learning via\nrubric-to-guideline linkage, evidence-weighted scoring, and contextual override\nlogic, complemented by a focus on ethical considerations and the integration of\ndelayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,\nwhile preserving HealthBench's transparency and physician engagement, we aim to\nfoster medical language models that are not only linguistically polished but\nalso clinically trustworthy, ethically sound, and globally relevant.", "AI": {"tldr": "HealthBench\u57fa\u51c6\u5b58\u5728\u4f9d\u8d56\u4e13\u5bb6\u610f\u89c1\u3001\u6613\u5e26\u5165\u504f\u89c1\u4ee5\u53ca\u5728\u4f4e\u6536\u5165\u56fd\u5bb6\u9002\u7528\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5faa\u8bc1\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\uff08CPG\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u533b\u5b66\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3001\u516c\u5e73\u6027\u548c\u5168\u7403\u9002\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30AI\u533b\u7597\u80fd\u529b\u7684\u57fa\u51c6\uff08\u5982HealthBench\uff09\u4f9d\u8d56\u4e13\u5bb6\u610f\u89c1\uff0c\u53ef\u80fd\u5305\u542b\u533a\u57df\u504f\u89c1\u548c\u4e2a\u4f53\u5dee\u5f02\uff0c\u5728\u4f4e\u6536\u5165\u548c\u4e2d\u7b49\u6536\u5165\u56fd\u5bb6\u5c24\u5176\u660e\u663e\uff0c\u4e14\u5b58\u5728\u6570\u636e\u7a00\u758f\u3001\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u548c\u76d1\u7ba1\u6846\u67b6\u4e0d\u5b8c\u5584\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u8bc1\u636e\u7a33\u5065\u201d\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8bc4\u5206\u6807\u51c6\u94fe\u63a5\u5230\u6307\u5357\uff0c\u8fdb\u884c\u8bc1\u636e\u52a0\u6743\u8bc4\u5206\uff0c\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587\u8986\u76d6\u903b\u8f91\uff0c\u540c\u65f6\u5173\u6ce8\u4f26\u7406\u95ee\u9898\u548c\u5ef6\u8fdf\u7ed3\u679c\u53cd\u9988\u3002", "result": "\u901a\u8fc7\u5c06\u5956\u52b1\u51fd\u6570\u5efa\u7acb\u5728\u7ecf\u8fc7\u4e25\u683c\u5ba1\u67e5\u7684\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\uff08CPG\uff09\u4e4b\u4e0a\uff0c\u5e76\u5728\u4fdd\u7559HealthBench\u7684\u900f\u660e\u5ea6\u548c\u533b\u751f\u53c2\u4e0e\u5ea6\u7684\u540c\u65f6\uff0c\u65e8\u5728\u57f9\u517b\u51fa\u4e0d\u4ec5\u8bed\u8a00\u6d41\u7545\uff0c\u800c\u4e14\u5728\u4e34\u5e8a\u4e0a\u503c\u5f97\u4fe1\u8d56\u3001\u7b26\u5408\u4f26\u7406\u4e14\u5177\u6709\u5168\u7403\u76f8\u5173\u6027\u7684\u533b\u5b66\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5956\u52b1\u51fd\u6570\u951a\u5b9a\u5728\u7248\u672c\u63a7\u5236\u7684\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\uff08CPG\uff09\u4e0a\uff0c\u5e76\u7ed3\u5408\u7cfb\u7edf\u8bc4\u4ef7\u548cGRADE\u8bc1\u636e\u8bc4\u7ea7\uff0c\u65e8\u5728\u521b\u5efa\u66f4\u503c\u5f97\u4fe1\u8d56\u3001\u7b26\u5408\u4f26\u7406\u4e14\u5177\u6709\u5168\u7403\u76f8\u5173\u6027\u7684\u533b\u5b66\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2508.00398", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00398", "abs": "https://arxiv.org/abs/2508.00398", "authors": ["Sunjae Yoon", "Gwanhyeong Koo", "Younghwan Lee", "Ji Woo Hong", "Chang D. Yoo"], "title": "Occlusion-robust Stylization for Drawing-based 3D Animation", "comment": "11 pages, 13 figures, ICCV 2025", "summary": "3D animation aims to generate a 3D animated video from an input image and a\ntarget 3D motion sequence. Recent advances in image-to-3D models enable the\ncreation of animations directly from user-hand drawings. Distinguished from\nconventional 3D animation, drawing-based 3D animation is crucial to preserve\nartist's unique style properties, such as rough contours and distinct stroke\npatterns. However, recent methods still exhibit quality deterioration in style\nproperties, especially under occlusions caused by overlapping body parts,\nleading to contour flickering and stroke blurring. This occurs due to a\n`stylization pose gap' between training and inference in stylization networks\ndesigned to preserve drawing styles in drawing-based 3D animation systems. The\nstylization pose gap denotes that input target poses used to train the\nstylization network are always in occlusion-free poses, while target poses\nencountered in an inference include diverse occlusions under dynamic motions.\nTo this end, we propose Occlusion-robust Stylization Framework (OSF) for\ndrawing-based 3D animation. We found that while employing object's edge can be\neffective input prior for guiding stylization, it becomes notably inaccurate\nwhen occlusions occur at inference. Thus, our proposed OSF provides\nocclusion-robust edge guidance for stylization network using optical flow,\nensuring a consistent stylization even under occlusions. Furthermore, OSF\noperates in a single run instead of the previous two-stage method, achieving\n2.4x faster inference and 2.1x less memory.", "AI": {"tldr": "OSF\u6846\u67b6\u901a\u8fc7\u5149\u6d41\u5904\u7406\u906e\u6321\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7ed8\u5236\u5f0f3D\u52a8\u753b\u7684\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u5185\u5b58\u5360\u7528\u3002", "motivation": "\u73b0\u6709\u7ed8\u5236\u5f0f3D\u52a8\u753b\u65b9\u6cd5\u5728\u5904\u7406\u906e\u6321\uff08\u5982\u8eab\u4f53\u90e8\u4f4d\u91cd\u53e0\uff09\u65f6\uff0c\u5b58\u5728\u98ce\u683c\u5c5e\u6027\uff08\u5982\u8f6e\u5ed3\u548c\u7b14\u89e6\uff09\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u8fd9\u662f\u7531\u4e8e\u98ce\u683c\u5316\u7f51\u7edc\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u4e4b\u95f4\u5b58\u5728\u201c\u98ce\u683c\u5316\u59ff\u52bf\u5dee\u8ddd\u201d\uff0c\u5373\u8bad\u7ec3\u65f6\u4f7f\u7528\u65e0\u906e\u6321\u59ff\u52bf\uff0c\u800c\u63a8\u7406\u65f6\u9047\u5230\u542b\u906e\u6321\u7684\u52a8\u6001\u59ff\u52bf\u3002", "method": "OSF\u6846\u67b6\u5229\u7528\u5149\u6d41\u5b9e\u73b0\u9c81\u68d2\u7684\u8fb9\u7f18\u5f15\u5bfc\uff0c\u4ee5\u89e3\u51b3\u7ed8\u5236\u5f0f3D\u52a8\u753b\u4e2d\u7684\u98ce\u683c\u5316\u906e\u6321\u95ee\u9898\u3002", "result": "OSF\u6846\u67b6\u901a\u8fc7\u5149\u6d41\u5b9e\u73b0\u9c81\u68d2\u7684\u8fb9\u7f18\u5f15\u5bfc\uff0c\u89e3\u51b3\u4e86\u906e\u6321\u95ee\u9898\uff0c\u5b9e\u73b0\u4e862.4\u500d\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u548c2.1\u500d\u66f4\u5c11\u7684\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u98ce\u683c\u5c5e\u6027\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684OSF\u6846\u67b6\u901a\u8fc7\u4f7f\u7528\u5149\u6d41\u5b9e\u73b0\u9c81\u68d2\u7684\u8fb9\u7f18\u5f15\u5bfc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u906e\u6321\u95ee\u9898\u65f6\u98ce\u683c\u5c5e\u6027\u4e0b\u964d\u7684\u6311\u6218\uff0c\u786e\u4fdd\u4e86\u52a8\u753b\u98ce\u683c\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.00023", "categories": ["quant-ph", "cs.CE"], "pdf": "https://arxiv.org/pdf/2508.00023", "abs": "https://arxiv.org/abs/2508.00023", "authors": ["Mirco A. Mannucci"], "title": "Weak Values as Geometric Lenses: Deformations of Hilbert Space and the Emergence of superoscillations", "comment": "12 pages, 1 figure", "summary": "The formalism of weak measurement in quantum mechanics has revealed profound\nconnections between measurement theory, quantum foundations, and signal\nprocessing. In this paper, we develop a pointer-free derivation of\nsuperoscillations, demonstrating that they are a natural and necessary\nconsequence of the geometric structure underlying weak values. We argue that\nthe weak value is best understood as a ratio of geometric deformation,\nquantifying how an observable transforms the structure of Hilbert space\nrelative to a reference provided by the standard inner product. This\ndeformation acts as a conceptual lens, warping the local structure of quantum\nstates to produce oscillations far exceeding the global Fourier bandwidth. We\nformalize this by interpreting the weak value as a comparison between a\ndeformed sesquilinear form and the standard one, and explore its deep\nconnections to generalized Rayleigh quotients and the projective geometry of\nquantum states. This perspective unifies weak values and superoscillations as\ntwo facets of a single underlying geometric principle.", "AI": {"tldr": "\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u5f31\u6d4b\u91cf\u548c\u8d85\u632f\u8361\u53ef\u4ee5\u901a\u8fc7\u4e00\u79cd\u65b0\u7684\u51e0\u4f55\u900f\u955c\u6765\u7406\u89e3\uff0c\u8fd9\u79cd\u900f\u955c\u63ed\u793a\u4e86\u5b83\u4eec\u4e4b\u95f4\u6df1\u523b\u7684\u8054\u7cfb\u3002", "motivation": "\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u5f31\u6d4b\u91cf\u5f62\u5f0f\u4e3b\u4e49\u63ed\u793a\u4e86\u6d4b\u91cf\u7406\u8bba\u3001\u91cf\u5b50\u57fa\u7840\u548c\u4fe1\u53f7\u5904\u7406\u4e4b\u95f4\u6df1\u523b\u7684\u8054\u7cfb\u3002\u672c\u7814\u7a76\u65e8\u5728\u8fdb\u4e00\u6b65\u63a2\u7d22\u8fd9\u79cd\u8054\u7cfb\uff0c\u7279\u522b\u662f\u5f31\u6d4b\u91cf\u4e0e\u8d85\u632f\u8361\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6307\u9488\u7684\u63a8\u5bfc\u65b9\u6cd5\uff0c\u5c06\u5f31\u503c\u89e3\u91ca\u4e3a\u51e0\u4f55\u53d8\u5f62\u7684\u6bd4\u7387\uff0c\u5e76\u5c06\u5f31\u503c\u89e3\u91ca\u4e3a\u53d8\u5f62\u7684\u534a\u53cc\u7ebf\u6027\u5f62\u5f0f\u4e0e\u6807\u51c6\u5f62\u5f0f\u4e4b\u95f4\u7684\u6bd4\u8f83\u3002", "result": "\u63a8\u5bfc\u4e86\u8d85\u632f\u8361\uff0c\u5e76\u5c06\u5176\u4e0e\u5f31\u503c\u8054\u7cfb\u8d77\u6765\uff0c\u8868\u660e\u5b83\u4eec\u662f\u5f31\u503c\u6f5c\u5728\u51e0\u4f55\u7ed3\u6784\u7684\u5fc5\u7136\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u5f31\u6d4b\u91cf\u548c\u8d85\u632f\u8361\u7edf\u4e00\u4e3a\u5355\u4e00\u51e0\u4f55\u539f\u7406\u7684\u4e24\u4e2a\u65b9\u9762\uff0c\u5c06\u5f31\u503c\u89e3\u91ca\u4e3a\u51e0\u4f55\u53d8\u5f62\u7684\u6bd4\u7387\uff0c\u91cf\u5316\u53ef\u89c2\u6d4b\u91cf\u7684\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7ed3\u6784\u76f8\u5bf9\u4e8e\u6807\u51c6\u5185\u79ef\u7684\u53d8\u6362\u3002"}}
{"id": "2508.00497", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2508.00497", "abs": "https://arxiv.org/abs/2508.00497", "authors": ["Jinghui Zhang", "Kaiyang Wan", "Longwei Xu", "Ao Li", "Zongfang Liu", "Xiuying Chen"], "title": "From Individuals to Crowds: Dual-Level Public Response Prediction in Social Media", "comment": "ACM MM 2025", "summary": "Public response prediction is critical for understanding how individuals or\ngroups might react to specific events, policies, or social phenomena, making it\nhighly valuable for crisis management, policy-making, and social media\nanalysis. However, existing works face notable limitations. First, they lack\nmicro-level personalization, producing generic responses that ignore individual\nuser preferences. Moreover, they overlook macro-level sentiment distribution\nand only deal with individual-level sentiment, constraining them from analyzing\nbroader societal trends and group sentiment dynamics. To address these\nchallenges, we propose SocialAlign, a unified framework that predicts\nreal-world responses at both micro and macro levels in social contexts. At the\nmicro level, SocialAlign employs SocialLLM with an articulate Personalized\nAnalyze-Compose LoRA (PAC-LoRA) structure, which deploys specialized expert\nmodules for content analysis and response generation across diverse topics and\nuser profiles, enabling the generation of personalized comments with\ncorresponding sentiments. At the macro level, it models group sentiment\ndistributions and aligns predictions with real-world sentiment trends derived\nfrom social media data. To evaluate SocialAlign in real-world scenarios, we\nintroduce SentiWeibo, a large-scale dataset curated from authentic social\ninteractions on the Weibo platform. Experimental results on our SentiWeibo and\nrelated LaMP benchmark demonstrate that SocialAlign surpasses strong baselines,\nshowing improved accuracy, interpretability, and generalization in public\nresponse prediction. We hope our work inspires further research in public\nresponse prediction and computational social science:\nhttps://github.com/Znull-1220/SocialAlign.", "AI": {"tldr": "SocialAlign\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4e2a\u6027\u5316\u7684\u5fae\u89c2\u5206\u6790\u548c\u5b8f\u89c2\u7684\u60c5\u611f\u8d8b\u52bf\u9884\u6d4b\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u516c\u5171\u53cd\u5e94\u3002", "motivation": "\u516c\u5171\u53cd\u5e94\u9884\u6d4b\u5bf9\u4e8e\u7406\u89e3\u4e2a\u4eba\u6216\u56e2\u4f53\u5bf9\u7279\u5b9a\u4e8b\u4ef6\u3001\u653f\u7b56\u6216\u793e\u4f1a\u73b0\u8c61\u7684\u53cd\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u5bf9\u4e8e\u5371\u673a\u7ba1\u7406\u3001\u653f\u7b56\u5236\u5b9a\u548c\u793e\u4f1a\u5a92\u4f53\u5206\u6790\u5177\u6709\u5f88\u9ad8\u7684\u4ef7\u503c\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5fae\u89c2\u5c42\u9762\u7f3a\u4e4f\u4e2a\u6027\u5316\uff0c\u751f\u6210\u7684\u54cd\u5e94\u662f\u901a\u7528\u7684\uff0c\u5ffd\u7565\u4e86\u4e2a\u4eba\u7528\u6237\u504f\u597d\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u5ffd\u7565\u4e86\u5b8f\u89c2\u5c42\u9762\u7684\u60c5\u611f\u5206\u5e03\uff0c\u53ea\u5904\u7406\u4e2a\u4f53\u5c42\u9762\u7684\u60c5\u611f\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5206\u6790\u66f4\u5e7f\u6cdb\u7684\u793e\u4f1a\u8d8b\u52bf\u548c\u7fa4\u4f53\u60c5\u611f\u52a8\u6001\u7684\u80fd\u529b\u3002", "method": "SocialAlign\u5728\u5fae\u89c2\u5c42\u9762\u91c7\u7528\u5e26\u6709\u6587\u7ae0\u4e2a\u6027\u5316\u5206\u6790-\u7ec4\u5408LoRA\uff08PAC-LoRA\uff09\u7ed3\u6784\u7684SocialLLM\uff0c\u8be5\u7ed3\u6784\u90e8\u7f72\u4e86\u4e13\u95e8\u7684\u4e13\u5bb6\u6a21\u5757\uff0c\u7528\u4e8e\u8de8\u4e0d\u540c\u4e3b\u9898\u548c\u7528\u6237\u914d\u7f6e\u6587\u4ef6\u8fdb\u884c\u5185\u5bb9\u5206\u6790\u548c\u54cd\u5e94\u751f\u6210\uff0c\u4ece\u800c\u80fd\u591f\u751f\u6210\u5177\u6709\u76f8\u5e94\u60c5\u611f\u7684\u4e2a\u6027\u5316\u8bc4\u8bba\u3002\u5728\u5b8f\u89c2\u5c42\u9762\uff0c\u5b83\u6a21\u62df\u7fa4\u4f53\u60c5\u611f\u5206\u5e03\uff0c\u5e76\u5c06\u9884\u6d4b\u4e0e\u4ece\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e2d\u63d0\u53d6\u7684\u73b0\u5b9e\u4e16\u754c\u60c5\u611f\u8d8b\u52bf\u76f8\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSocialAlign\u5728\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "SocialAlign\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u5fae\u89c2\u548c\u5b8f\u89c2\u5c42\u9762\u9884\u6d4b\u793e\u4f1a\u80cc\u666f\u4e0b\u7684\u73b0\u5b9e\u4e16\u754c\u53cd\u5e94\u3002\u8be5\u6846\u67b6\u5728SentiWeibo\u548cLaMP\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSocialAlign\u5728\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\uff0c\u6709\u671b\u5728\u516c\u5171\u53cd\u5e94\u9884\u6d4b\u548c\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u9886\u57df\u6fc0\u53d1\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002"}}
{"id": "2508.00806", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00806", "abs": "https://arxiv.org/abs/2508.00806", "authors": ["Ping Chen", "Zhuohong Deng", "Ping Li", "Shuibing He", "Hongzi Zhu", "Yi Zheng", "Zhefeng Wang", "Baoxing Huai", "Minyi Guo"], "title": "Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management", "comment": "8 pages", "summary": "Training large language models often employs recomputation to alleviate\nmemory pressure, which can introduce up to 30% overhead in real-world\nscenarios. In this paper, we propose Adacc, a novel memory management framework\nthat combines adaptive compression and activation checkpointing to reduce the\nGPU memory footprint. It comprises three modules: (1) We design layer-specific\ncompression algorithms that account for outliers in LLM tensors, instead of\ndirectly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We\npropose an optimal scheduling policy that employs MILP to determine the best\nmemory optimization for each tensor. (3) To accommodate changes in training\ntensors, we introduce an adaptive policy evolution mechanism that adjusts the\npolicy during training to enhance throughput. Experimental results show that\nAdacc can accelerate the LLM training by 1.01x to 1.37x compared to\nstate-of-the-art frameworks, while maintaining comparable model accuracy to the\nBaseline.", "AI": {"tldr": "Adacc, a new memory management framework, reduces LLM training memory usage and speeds up training by up to 1.37x using adaptive compression and activation checkpointing.", "motivation": "Recomputation in training large language models can introduce significant overhead (up to 30%) due to memory pressure. Adacc aims to reduce the GPU memory footprint to alleviate this.", "method": "Adacc is a novel memory management framework that combines adaptive compression and activation checkpointing. It features layer-specific compression algorithms that handle outliers in LLM tensors, an optimal scheduling policy determined by MILP, and an adaptive policy evolution mechanism to adjust during training.", "result": "Adacc accelerates LLM training by 1.01x to 1.37x and maintains comparable model accuracy.", "conclusion": "Adacc can accelerate LLM training by 1.01x to 1.37x compared to state-of-the-art frameworks while maintaining comparable model accuracy to the Baseline."}}
{"id": "2508.00017", "categories": ["cs.LO", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.00017", "abs": "https://arxiv.org/abs/2508.00017", "authors": ["Nikolai Sergeev"], "title": "Generative Logic: A New Computer Architecture for Deterministic Reasoning and Knowledge Generation", "comment": "19 pages, 5 figures. Code and interactive HTML proof graphs\n  permanently archived on Zenodo (DOI: 10.5281/zenodo.16408441)", "summary": "We present Generative Logic (GL), a deterministic architecture that begins\nfrom user-supplied axiomatic definitions -- written in a minimalist\nMathematical Programming Language (MPL) -- and systematically explores their\ndeductive neighborhood. Definitions are compiled into a distributed grid of\nsimple Logic Blocks (LBs) that exchange messages; any time several expressions\nunify under an inference rule, a new fact is emitted with full provenance to\nits sources, yielding replayable, auditable proof graphs.\n  A prototype software implementation instantiates the workflow on first-order\nPeano arithmetic. Starting only from the Peano axioms, GL enumerates candidate\nimplications, applies normalization and type filters, and automatically\nreconstructs machine-checkable proofs of foundational arithmetic laws including\nassociativity and commutativity of addition, associativity and commutativity of\nmultiplication, and distributivity. Generated proofs export to navigable HTML\nso that every inference step can be inspected independently.\n  We outline a hardware-software co-design path toward massively parallel\nrealizations and describe prospective integration with probabilistic models\n(e.g., Large Language Models (LLMs)) for autoformalization and conjecture\nseeding. The Python and MPL code to reproduce the Peano experiments, along with\nthe full HTML proof graphs, are available in the project's GitHub repository at\nhttps://github.com/Generative-Logic/GL/tree/35a111ea9ba53afe051703d6050be0c3923e9724\nand are permanently archived at https://doi.org/10.5281/zenodo.16408441. We\ninvite community feedback and collaboration.", "AI": {"tldr": "GL \u662f\u4e00\u79cd\u65b0\u7684\u786e\u5b9a\u6027\u67b6\u6784\uff0c\u53ef\u4ee5\u5c06\u516c\u7406\u5b9a\u4e49\u8f6c\u5316\u4e3a\u53ef\u5ba1\u8ba1\u7684\u8bc1\u660e\u56fe\uff0c\u5e76\u5df2\u6210\u529f\u7528\u4e8e\u81ea\u52a8\u91cd\u5efa\u57fa\u7840\u7b97\u672f\u5b9a\u5f8b\u7684\u8bc1\u660e\u3002", "motivation": "\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u81ea\u52a8\u5f62\u5f0f\u5316\u548c\u63a2\u7d22\u6570\u5b66\u63a8\u5bfc\u7684\u65b9\u6cd5\uff0c\u5e76\u4e3a\u4e0e\u6982\u7387\u6a21\u578b\uff08\u5982 LLM\uff09\u7684\u96c6\u6210\u4ee5\u53ca\u5927\u89c4\u6a21\u5e76\u884c\u5b9e\u73b0\u5960\u5b9a\u57fa\u7840\u3002", "method": "GL \u5c06\u5b9a\u4e49\u7f16\u8bd1\u4e3a\u5206\u5e03\u5f0f\u903b\u8f91\u5757 (LB) \u7f51\u683c\uff0c\u901a\u8fc7\u6d88\u606f\u4ea4\u6362\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u751f\u6210\u5177\u6709\u5b8c\u6574\u6eaf\u6e90\u4fe1\u606f\u7684\u65b0\u7684\u4e8b\u5b9e\uff0c\u5f62\u6210\u53ef\u91cd\u73b0\u3001\u53ef\u5ba1\u8ba1\u7684\u8bc1\u660e\u56fe\u3002\u539f\u578b\u8f6f\u4ef6\u5b9e\u73b0\u4e86\u5bf9\u4e00\u9636\u76ae\u4e9a\u8bfa\u7b97\u672f\u7684\u5b9e\u4f8b\u5316\uff0c\u81ea\u52a8\u91cd\u5efa\u4e86\u52a0\u6cd5\u548c\u4e58\u6cd5\u7684\u7ed3\u5408\u5f8b\u3001\u4ea4\u6362\u5f8b\u4ee5\u53ca\u5206\u914d\u5f8b\u7b49\u57fa\u7840\u7b97\u672f\u5b9a\u5f8b\u7684\u673a\u5668\u53ef\u68c0\u67e5\u8bc1\u660e\uff0c\u5e76\u751f\u6210\u4e86\u53ef\u5bfc\u822a\u7684 HTML \u8bc1\u660e\u56fe\u3002", "result": "\u6210\u529f\u4ece\u76ae\u4e9a\u8bfa\u516c\u7406\u51fa\u53d1\uff0c\u81ea\u52a8\u91cd\u5efa\u4e86\u52a0\u6cd5\u548c\u4e58\u6cd5\u7684\u7ed3\u5408\u5f8b\u3001\u4ea4\u6362\u5f8b\u4ee5\u53ca\u5206\u914d\u5f8b\u7b49\u57fa\u7840\u7b97\u672f\u5b9a\u5f8b\u7684\u673a\u5668\u53ef\u68c0\u67e5\u8bc1\u660e\uff0c\u5e76\u751f\u6210\u4e86\u53ef\u5bfc\u822a\u7684 HTML \u8bc1\u660e\u56fe\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86 Generative Logic (GL) \u67b6\u6784\uff0c\u8be5\u67b6\u6784\u4ece\u7528\u6237\u63d0\u4f9b\u7684\u516c\u7406\u5b9a\u4e49\u51fa\u53d1\uff0c\u901a\u8fc7\u7cfb\u7edf\u5730\u63a2\u7d22\u5176\u63a8\u5bfc\u90bb\u57df\u6765\u751f\u6210\u8bc1\u660e\u3002"}}
{"id": "2508.00349", "categories": ["cs.GT", "cs.DM", "math.CO"], "pdf": "https://arxiv.org/pdf/2508.00349", "abs": "https://arxiv.org/abs/2508.00349", "authors": ["Yuga Kanaya", "Kenjiro Takazawa"], "title": "On the Equivalence of the Graph-Structural and Optimization-Based Characterizations of Popular Matchings", "comment": null, "summary": "Popular matchings provide a model of matching under preferences in which a\nsolution corresponds to a Condorcet winner in voting systems. In a bipartite\ngraph in which the vertices have preferences over their neighbours, a matching\nis defined to be popular if it does not lose in a majority vote against any\nmatching. In this paper, we study the following three primary problems: only\nthe vertices on one side have preferences; a generalization of this problem\nallowing ties in the preferences; and the vertices on both sides have\npreferences. A principal issue in the algorithmic aspects of popular matchings\nis how to determine the popularity of a matching, because it requires\nexponential time if the definition is simply applied. In the literature, we\nhave the following two types of characterizations: a graph-structural\ncharacterization; and an optimization-based characterization described by\nmaximum-weight matchings. The graph-structural characterizations are\nspecifically designed for each problem and provide a combinatorial structure of\nthe popular matchings. The optimization-based characterizations work in the\nsame manner for all problems, while they do not reveal the structure of the\npopular matchings. A main contribution of this paper is to provide a direct\nconnection of the above two types of characterizations for all of the three\nproblems. Specifically, we prove that each characterization can be derived from\nthe other, without relying on the fact that they characterize popular\nmatchings. Our proofs offer a comprehensive understanding of the equivalence of\nthe two types of characterizations, and suggest a new interpretation of the\ngraph-structural characterization in terms of the dual optimal solution for the\nmaximum-weight matching problem.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86 popular matching \u7684\u4e24\u79cd\u4e3b\u8981\u8868\u5f81\u65b9\u6cd5\uff08\u56fe\u7ed3\u6784\u548c\u57fa\u4e8e\u4f18\u5316\uff09\u662f\u7b49\u4ef7\u7684\uff0c\u5e76\u53ef\u4ee5\u76f8\u4e92\u63a8\u5bfc\uff0c\u4e3a\u7406\u89e3 popular matching \u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "motivation": "Popular matching \u662f\u6295\u7968\u7cfb\u7edf\u4e2d Condorcet winner \u7684\u6a21\u578b\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5224\u65ad\u4e00\u4e2a matching \u7684 popularity \u9700\u8981\u6307\u6570\u65f6\u95f4\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8868\u5f81\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u4e09\u79cd popular matching \u95ee\u9898\uff1a\u5355\u8fb9\u6709\u504f\u597d\u3001\u5141\u8bb8\u504f\u597d\u6709 ties\u3001\u53cc\u8fb9\u6709\u504f\u597d\u3002\u4e3b\u8981\u8d21\u732e\u5728\u4e8e\u8bc1\u660e\u4e86\u56fe\u7ed3\u6784\u8868\u5f81\u548c\u57fa\u4e8e\u4f18\u5316\u7684\u8868\u5f81\u4e4b\u95f4\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u53ef\u4ee5\u76f8\u4e92\u63a8\u5bfc\u3002", "result": "\u672c\u6587\u6210\u529f\u5730\u5c06\u4e24\u79cd popular matching \u7684\u8868\u5f81\u65b9\u6cd5\uff08\u56fe\u7ed3\u6784\u548c\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\uff09\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u53ef\u4ee5\u76f8\u4e92\u63a8\u5bfc\uff0c\u4e3a\u7406\u89e3 popular matching \u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89c6\u89d2\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u4e24\u79cd popular matching \u7684\u8868\u5f81\u65b9\u6cd5\uff08\u56fe\u7ed3\u6784\u548c\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\uff09\u662f\u7b49\u4ef7\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u76f8\u4e92\u63a8\u5bfc\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u4e3a\u56fe\u7ed3\u6784\u8868\u5f81\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u91ca\uff0c\u5373\u4e0e\u6700\u5927\u6743\u91cd\u5339\u914d\u95ee\u9898\u7684\u5bf9\u5076\u6700\u4f18\u89e3\u76f8\u5173\u3002"}}
{"id": "2508.00156", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.00156", "abs": "https://arxiv.org/abs/2508.00156", "authors": ["Shuhao Qi", "Zhiqi Tang", "Zhiyong Sun", "Sofie Haesaert"], "title": "Integrating Opinion Dynamics into Safety Control for Decentralized Airplane Encounter Resolution", "comment": null, "summary": "As the airspace becomes increasingly congested, decentralized conflict\nresolution methods for airplane encounters have become essential. While\ndecentralized safety controllers can prevent dangerous midair collisions, they\ndo not always ensure prompt conflict resolution. As a result, airplane progress\nmay be blocked for extended periods in certain situations. To address this\nblocking phenomenon, this paper proposes integrating bio-inspired nonlinear\nopinion dynamics into the airplane safety control framework, thereby\nguaranteeing both safety and blocking-free resolution. In particular, opinion\ndynamics enable the safety controller to achieve collaborative decision-making\nfor blocking resolution and facilitate rapid, safe coordination without relying\non communication or preset rules. Extensive simulation results validate the\nimproved flight efficiency and safety guarantees. This study provides practical\ninsights into the design of autonomous controllers for airplanes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5c06\u53d7\u751f\u7269\u542f\u53d1\u7684\u975e\u7ebf\u6027\u610f\u89c1\u52a8\u529b\u5b66\u5e94\u7528\u4e8e\u98de\u673a\u5b89\u5168\u63a7\u5236\uff0c\u4ee5\u89e3\u51b3\u7a7a\u4e2d\u4ea4\u901a\u62e5\u5835\u5bfc\u81f4\u7684\u963b\u585e\u95ee\u9898\uff0c\u5b9e\u73b0\u5b89\u5168\u3001\u9ad8\u6548\u3001\u65e0\u963b\u585e\u7684\u51b2\u7a81\u89e3\u51b3\u3002", "motivation": "\u5f53\u524d\u7684\u53bb\u4e2d\u5fc3\u5316\u5b89\u5168\u63a7\u5236\u5668\u867d\u7136\u80fd\u9632\u6b62\u78b0\u649e\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u98de\u673a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u957f\u65f6\u95f4\u963b\u585e\uff0c\u5f71\u54cd\u98de\u884c\u6548\u7387\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u53d7\u751f\u7269\u542f\u53d1\u7684\u975e\u7ebf\u6027\u610f\u89c1\u52a8\u529b\u5b66\u5230\u98de\u673a\u5b89\u5168\u63a7\u5236\u6846\u67b6\u6765\u89e3\u51b3\u963b\u585e\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u65e0\u9700\u901a\u4fe1\u6216\u9884\u8bbe\u89c4\u5219\u7684\u534f\u4f5c\u51b3\u7b56\u548c\u5feb\u901f\u3001\u5b89\u5168\u7684\u534f\u8c03\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u9ad8\u98de\u884c\u6548\u7387\u5e76\u4fdd\u8bc1\u5b89\u5168\u6027\uff0c\u4e3a\u81ea\u4e3b\u63a7\u5236\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u5c06\u53d7\u751f\u7269\u542f\u53d1\u7684\u975e\u7ebf\u6027\u610f\u89c1\u52a8\u529b\u5b66\u96c6\u6210\u5230\u98de\u673a\u5b89\u5168\u63a7\u5236\u6846\u67b6\u4e2d\uff0c\u4ee5\u89e3\u51b3\u7a7a\u4e2d\u4ea4\u901a\u51b2\u7a81\u7684\u963b\u585e\u95ee\u9898\uff0c\u786e\u4fdd\u4e86\u5b89\u5168\u6027\u548c\u65e0\u963b\u585e\u7684\u51b2\u7a81\u89e3\u51b3\u3002"}}
{"id": "2508.00280", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2508.00280", "abs": "https://arxiv.org/abs/2508.00280", "authors": ["Jingchen Peng", "Dingli Yuan", "Boxiang Ren", "Jie Fan", "Hao Wu", "Lu Yang"], "title": "WMAS: A Multi-Agent System Towards Intelligent and Customized Wireless Networks", "comment": null, "summary": "The fast development of Artificial Intelligence (AI) agents provides a\npromising way for the realization of intelligent and customized wireless\nnetworks. In this paper, we propose a Wireless Multi-Agent System (WMAS), which\ncan provide intelligent and customized services for different user equipment\n(UEs). Note that orchestrating multiple agents carries the risk of malfunction,\nand multi-agent conversations may fall into infinite loops. It is thus crucial\nto design a conversation topology for WMAS that enables agents to complete UE\ntask requests with high accuracy and low conversation overhead. To address this\nissue, we model the multi-agent conversation topology as a directed acyclic\ngraph and propose a reinforcement learning-based algorithm to optimize the\nadjacency matrix of this graph. As such, WMAS is capable of generating and\nself-optimizing multi-agent conversation topologies, enabling agents to\neffectively and collaboratively handle a variety of task requests from UEs.\nSimulation results across various task types demonstrate that WMAS can achieve\nhigher task performance and lower conversation overhead compared to existing\nmulti-agent systems. These results validate the potential of WMAS to enhance\nthe intelligence of future wireless networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWMAS\u7684\u65e0\u7ebf\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u8bdd\u62d3\u6251\u4f18\u5316\uff0c\u63d0\u9ad8\u4e86\u65e0\u7ebf\u7f51\u7edc\u4e2d\u667a\u80fd\u4f53\u7684\u4efb\u52a1\u5904\u7406\u6548\u7387\u548c\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u65e0\u7ebf\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u667a\u80fd\u4f53\u7f16\u6392\u53ef\u80fd\u51fa\u73b0\u7684\u6545\u969c\u4ee5\u53ca\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u9677\u5165\u65e0\u9650\u5faa\u73af\u7684\u98ce\u9669\uff0c\u9700\u8981\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u4ee5\u9ad8\u51c6\u786e\u6027\u548c\u4f4e\u901a\u4fe1\u5f00\u9500\u6765\u5b8c\u6210UE\u4efb\u52a1\u8bf7\u6c42\u7684\u5bf9\u8bdd\u62d3\u6251\u3002", "method": "\u901a\u8fc7\u5c06\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u62d3\u6251\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b97\u6cd5\u6765\u4f18\u5316\u8be5\u56fe\u7684\u90bb\u63a5\u77e9\u9635\uff0c\u4ee5\u89e3\u51b3\u667a\u80fd\u4f53\u4e4b\u95f4\u53ef\u80fd\u51fa\u73b0\u7684\u901a\u4fe1\u5faa\u73af\u548c\u6545\u969c\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u76f8\u6bd4\uff0cWMAS\u5728\u5404\u79cd\u4efb\u52a1\u7c7b\u578b\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u4efb\u52a1\u6027\u80fd\u548c\u66f4\u4f4e\u7684\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684WMAS\u80fd\u591f\u901a\u8fc7\u751f\u6210\u548c\u81ea\u4f18\u5316\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u62d3\u6251\uff0c\u6709\u6548\u4e14\u534f\u4f5c\u5730\u5904\u7406\u6765\u81eaUE\u7684\u5404\u79cd\u4efb\u52a1\u8bf7\u6c42\uff0c\u5728\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\uff0c\u5e76\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u589e\u5f3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u667a\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00310", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.00310", "abs": "https://arxiv.org/abs/2508.00310", "authors": ["Yong Zhang", "Xianfeng Chen"], "title": "A compact quasi-zero stiffness metamaterial based on monolithic shells for vibration isolation", "comment": "17 pages; 8 figures", "summary": "Quasi-zero stiffness (QZS) metamaterials are highly effective in isolating\nobjects from low-frequency external vibrations, due to their high static\nstiffness but low dynamic stiffness characteristics. Traditionally, QZS\nmetamaterials are designed by combining a negative-stiffness part with a\npositive-stiffness counterpart. Here, we present a novel QZS metamaterial\ndesign without relying on combining two components. The QZS characteristic is\nachieved solely through monolithic shell elements' unique geometry and\nnonlinear deformation. Using experimental and numerical approaches, we\ninvestigate the static and dynamic responses of the proposed metamaterials as a\nfunction of their geometric parameters. We then tune the structure's geometry\nto achieve ideal zero-stiffness behaviors and experimentally demonstrate an\nexceptional low-frequency vibration isolation mechanism. This concept can be\nfurther utilized as a building block for constructing metamaterials with\nmultiple zero-stiffness features, enabling a broad range of applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u4ec5\u7531\u5355\u7247\u58f3\u4f53\u5355\u5143\u7ec4\u6210\u7684\u8bbe\u8ba1\uff0c\u7528\u4e8e\u5b9e\u73b0\u51c6\u96f6\u521a\u5ea6\uff08QZS\uff09\u8d85\u6750\u6599\uff0c\u65e0\u9700\u7ec4\u5408\u6b63\u8d1f\u521a\u5ea6\u90e8\u4ef6\u3002\u901a\u8fc7\u5b9e\u9a8c\u548c\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u8be5\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u4f4e\u9891\u632f\u52a8\u9694\u79bb\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u4f20\u7edfQZS\u8d85\u6750\u6599\u9700\u8981\u7ec4\u5408\u4e24\u79cd\u7ec4\u4ef6\u7684\u9650\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u901a\u8fc7\u5355\u7247\u58f3\u4f53\u5355\u5143\u7684\u51e0\u4f55\u5f62\u72b6\u548c\u975e\u7ebf\u6027\u53d8\u5f62\u6765\u5b9e\u73b0QZS\u7279\u6027\u7684\u65b0\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u548c\u6570\u503c\u65b9\u6cd5\u7814\u7a76\u4e86\u6240\u63d0\u51fa\u7684QZS\u8d85\u6750\u6599\u7684\u9759\u6001\u548c\u52a8\u6001\u54cd\u5e94\uff0c\u5e76\u8c03\u6574\u4e86\u51e0\u4f55\u53c2\u6570\u4ee5\u5b9e\u73b0\u7406\u60f3\u7684\u96f6\u521a\u5ea6\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u8d85\u6750\u6599\u5728\u4f4e\u9891\u632f\u52a8\u9694\u79bb\u65b9\u9762\u5177\u6709\u51fa\u8272\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u8be5\u8bbe\u8ba1\u7406\u5ff5\u53ef\u4ee5\u6269\u5c55\u5230\u6784\u5efa\u5177\u6709\u591a\u4e2a\u96f6\u521a\u5ea6\u7279\u6027\u7684\u8d85\u6750\u6599\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5355\u7247\u58f3\u4f53\u5355\u5143\u7684\u51c6\u96f6\u521a\u5ea6\uff08QZS\uff09\u8d85\u6750\u6599\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u7ec4\u5408\u4e24\u4e2a\u7ec4\u4ef6\u5373\u53ef\u5b9e\u73b0QZS\u7279\u6027\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u5e76\u4e14\u5df2\u901a\u8fc7\u5b9e\u9a8c\u548c\u6570\u503c\u65b9\u6cd5\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2508.00277", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.00277", "abs": "https://arxiv.org/abs/2508.00277", "authors": ["Yiting Deng", "Yan He"], "title": "The Quadrupole Moment of Higher-Order Topological Insulator at Finite temperature", "comment": "8 pages, 6 figures", "summary": "We study the higher-order topological insulators at finite temperature based\non a generalized real-space quadrupole moment, which extends the ground state\nexpectations to ensemble averages. Our study reveals that chiral symmetry alone\ndictates that the quadrupole moment must be quantized to two values of $0$ and\n$1/2$, even at finite temperature. It is found that finite temperature can\ninduce a topological phase transition from non-trivial to trivial. Furthermore,\nwe found that the anisotropic intra-cell hopping can lead to a reentrant\ntopological phase transition, in which the system becomes topological again\nwith rising temperature. This reentrant behavior is in stark contrast to the\nresults at zero temperature. We also investigate the effects of the\nquasi-disorder hopping on the topology. It is found that the initially trivial\nsystem can be driven into a topological phase with strong enough disorder\nstrength, which closely resembles the topological Anderson transition. Our work\nprovides an example for studying the finite temperature topology of\nhigher-order topological insulators.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u624b\u5f81\u5bf9\u79f0\u6027\u4fdd\u8bc1\u4e86\u9ad8\u6e29\u4e0b\u9ad8\u9636\u62d3\u6251\u7edd\u7f18\u4f53\u7684\u56db\u6781\u77e9\u91cf\u5b50\u5316\u3002\u6e29\u5ea6\u53d8\u5316\u548c\u65e0\u5e8f\u6027\u4f1a\u5f15\u8d77\u62d3\u6251\u76f8\u53d8\uff0c\u5305\u62ec\u91cd\u5165\u76f8\u53d8\u3002", "motivation": "\u7814\u7a76\u9ad8\u6e29\u4e0b\u9ad8\u9636\u62d3\u6251\u7edd\u7f18\u4f53\u7684\u62d3\u6251\u6027\u8d28\uff0c\u5e76\u63a2\u7d22\u6e29\u5ea6\u3001\u5404\u5411\u5f02\u6027\u80de\u5185\u8df3\u53d8\u548c\u51c6\u65e0\u5e8f\u8df3\u53d8\u5bf9\u62d3\u6251\u76f8\u53d8\u7684\u5f71\u54cd\u3002", "method": "\u57fa\u4e8e\u5e7f\u4e49\u5b9e\u7a7a\u95f4\u56db\u6781\u77e9\uff0c\u8be5\u7814\u7a76\u5c06\u96f6\u6e29\u5ea6\u4e0b\u7684\u57fa\u6001\u671f\u671b\u63a8\u5e7f\u81f3\u7cfb\u7efc\u5e73\u5747\uff0c\u7528\u4e8e\u5206\u6790\u9ad8\u6e29\u4e0b\u7684\u9ad8\u9636\u62d3\u6251\u7edd\u7f18\u4f53\u3002", "result": "1. \u624b\u5f81\u5bf9\u79f0\u6027\u4fdd\u8bc1\u6709\u9650\u6e29\u5ea6\u4e0b\u56db\u6781\u77e9\u7684\u91cf\u5b50\u5316\uff080\u62161/2\uff09\u30022. \u6709\u9650\u6e29\u5ea6\u53ef\u8bf1\u5bfc\u4ece\u975e\u5e73\u5eb8\u5230\u5e73\u5eb8\u7684\u62d3\u6251\u76f8\u53d8\u30023. \u5404\u5411\u5f02\u6027\u80de\u5185\u8df3\u53d8\u53ef\u5bfc\u81f4\u91cd\u5165\u62d3\u6251\u76f8\u53d8\uff0c\u4e0e\u96f6\u6e29\u5ea6\u4e0b\u7684\u7ed3\u679c\u5f62\u6210\u5bf9\u6bd4\u30024. \u51c6\u65e0\u5e8f\u8df3\u53d8\u53ef\u9a71\u52a8\u5e73\u5eb8\u7cfb\u7edf\u8fdb\u5165\u62d3\u6251\u76f8\uff0c\u7c7b\u4f3c\u4e8e\u62d3\u6251\u5b89\u5fb7\u68ee\u76f8\u53d8\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5e7f\u4e49\u5b9e\u7a7a\u95f4\u56db\u6781\u77e9\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7814\u7a76\u9ad8\u6e29\u4e0b\u7684\u9ad8\u9636\u62d3\u6251\u7edd\u7f18\u4f53\u3002\u7814\u7a76\u8868\u660e\uff0c\u624b\u5f81\u5bf9\u79f0\u6027\u4fdd\u8bc1\u4e86\u5373\u4f7f\u5728\u6709\u9650\u6e29\u5ea6\u4e0b\uff0c\u56db\u6781\u77e9\u4e5f\u53ea\u80fd\u53d60\u548c1/2\u4e24\u4e2a\u503c\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u6709\u9650\u6e29\u5ea6\u53ef\u8bf1\u5bfc\u4ece\u975e\u5e73\u5eb8\u5230\u5e73\u5eb8\u7684\u62d3\u6251\u76f8\u53d8\uff0c\u5e76\u4e14\u5404\u5411\u5f02\u6027\u80de\u5185\u8df3\u53d8\u53ef\u5bfc\u81f4\u7cfb\u7edf\u51fa\u73b0\u4e0e\u96f6\u6e29\u5ea6\u76f8\u53cd\u7684\u91cd\u5165\u62d3\u6251\u76f8\u53d8\u73b0\u8c61\u3002\u6b64\u5916\uff0c\u51c6\u65e0\u5e8f\u8df3\u53d8\u5728\u8db3\u591f\u5f3a\u7684\u65e0\u5e8f\u5f3a\u5ea6\u4e0b\u53ef\u9a71\u52a8\u5e73\u5eb8\u7cfb\u7edf\u8fdb\u5165\u62d3\u6251\u76f8\uff0c\u8fd9\u7c7b\u4f3c\u4e8e\u62d3\u6251\u5b89\u5fb7\u68ee\u76f8\u53d8\u3002\u8be5\u7814\u7a76\u4e3a\u7814\u7a76\u9ad8\u9636\u62d3\u6251\u7edd\u7f18\u4f53\u5728\u6709\u9650\u6e29\u5ea6\u4e0b\u7684\u62d3\u6251\u6027\u8d28\u63d0\u4f9b\u4e86\u5b9e\u4f8b\u3002"}}
{"id": "2508.00274", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00274", "abs": "https://arxiv.org/abs/2508.00274", "authors": ["Yunfei Liu", "Mingxuan Liu", "Wupeng Xie", "Xinzhu Liu", "Wenxue Liu", "Yangang Sun", "Xin Qiu", "Cui Yuan", "Jinhai Li"], "title": "RIS-MAE: A Self-Supervised Modulation Classification Method Based on Raw IQ Signals and Masked Autoencoder", "comment": null, "summary": "Automatic modulation classification (AMC) is a basic technology in\nintelligent wireless communication systems. It is important for tasks such as\nspectrum monitoring, cognitive radio, and secure communications. In recent\nyears, deep learning methods have made great progress in AMC. However,\nmainstream methods still face two key problems. First, they often use\ntime-frequency images instead of raw signals. This causes loss of key\nmodulation features and reduces adaptability to different communication\nconditions. Second, most methods rely on supervised learning. This needs a\nlarge amount of labeled data, which is hard to get in real-world environments.\nTo solve these problems, we propose a self-supervised learning framework called\nRIS-MAE. RIS-MAE uses masked autoencoders to learn signal features from\nunlabeled data. It takes raw IQ sequences as input. By applying random masking\nand reconstruction, it captures important time-domain features such as\namplitude, phase, etc. This helps the model learn useful and transferable\nrepresentations. RIS-MAE is tested on four datasets. The results show that it\nperforms better than existing methods in few-shot and cross-domain tasks.\nNotably, it achieves high classification accuracy on previously unseen datasets\nwith only a small number of fine-tuning samples, confirming its generalization\nability and potential for real-world deployment.", "AI": {"tldr": "\u4e00\u79cd\u540d\u4e3aRIS-MAE\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u4ee5\u76f4\u63a5\u5904\u7406\u539f\u59cbIQ\u5e8f\u5217\uff0c\u901a\u8fc7\u63a9\u7801\u548c\u91cd\u5efa\u6765\u5b66\u4e60\u65f6\u57df\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709AMC\u65b9\u6cd5\u4e22\u5931\u7279\u5f81\u548c\u4f9d\u8d56\u6709\u6807\u7b7e\u6570\u636e\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5c11\u6837\u672c\u548c\u8de8\u57df\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u81ea\u52a8\u8c03\u5236\u5206\u7c7b\uff08AMC\uff09\u65b9\u6cd5\u5728\u5904\u7406\u65f6\u95f4-\u9891\u7387\u56fe\u50cf\u65f6\u4e22\u5931\u5173\u952e\u7279\u5f81\u3001\u9002\u5e94\u6027\u5dee\u4ee5\u53ca\u4f9d\u8d56\u6709\u6807\u7b7e\u6570\u636e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRIS-MAE\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\u4ece\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u5b66\u4e60\u4fe1\u53f7\u7279\u5f81\uff0c\u76f4\u63a5\u4ee5\u539f\u59cbIQ\u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u5e94\u7528\u968f\u673a\u63a9\u7801\u548c\u91cd\u5efa\u6765\u6355\u6349\u5e45\u5ea6\u3001\u76f8\u4f4d\u7b49\u91cd\u8981\u7684\u65f6\u57df\u7279\u5f81\uff0c\u4ece\u800c\u5b66\u4e60\u6709\u7528\u7684\u3001\u53ef\u8fc1\u79fb\u7684\u8868\u793a\u3002", "result": "RIS-MAE\u5728\u5c11\u6837\u672c\u548c\u8de8\u57df\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u4ec5\u7528\u5c11\u91cf\u5fae\u8c03\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5148\u524d\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u5206\u7c7b\u7cbe\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002", "conclusion": "RIS-MAE\u901a\u8fc7\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\uff0c\u5728\u5c11\u6837\u672c\u548c\u8de8\u57df\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u4ec5\u7528\u5c11\u91cf\u5fae\u8c03\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5148\u524d\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u5206\u7c7b\u7cbe\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2508.00162", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00162", "abs": "https://arxiv.org/abs/2508.00162", "authors": ["Noboru Myers", "Obin Kwon", "Sankalp Yamsani", "Joohyung Kim"], "title": "CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System", "comment": null, "summary": "Recent advances in teleoperation have demonstrated robots performing complex\nmanipulation tasks. However, existing works rarely support whole-body\njoint-level teleoperation for humanoid robots, limiting the diversity of tasks\nthat can be accomplished. This work presents Controller for Humanoid Imitation\nand Live Demonstration (CHILD), a compact reconfigurable teleoperation system\nthat enables joint level control over humanoid robots. CHILD fits within a\nstandard baby carrier, allowing the operator control over all four limbs, and\nsupports both direct joint mapping for full-body control and loco-manipulation.\nAdaptive force feedback is incorporated to enhance operator experience and\nprevent unsafe joint movements. We validate the capabilities of this system by\nconducting loco-manipulation and full-body control examples on a humanoid robot\nand multiple dual-arm systems. Lastly, we open-source the design of the\nhardware promoting accessibility and reproducibility. Additional details and\nopen-source information are available at our project website:\nhttps://uiuckimlab.github.io/CHILD-pages.", "AI": {"tldr": "CHILD\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5168\u8eab\u5173\u8282\u7ea7\u9065\u64cd\u4f5c\uff0c\u652f\u6301\u8fd0\u52a8-\u64cd\u7eb5\uff0c\u5e76\u96c6\u6210\u4e86\u529b\u53cd\u9988\uff0c\u901a\u8fc7\u5f00\u6e90\u786c\u4ef6\u8bbe\u8ba1\u63d0\u9ad8\u4e86\u53ef\u53ca\u6027\u3002", "motivation": "\u73b0\u6709\u9065\u64cd\u4f5c\u7814\u7a76\u5f88\u5c11\u652f\u6301\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5168\u8eab\u5173\u8282\u7ea7\u9065\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u53ef\u5b8c\u6210\u4efb\u52a1\u7684\u591a\u6837\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "CHILD\u7cfb\u7edf\u901a\u8fc7\u7d27\u51d1\u7684\u53ef\u91cd\u6784\u8bbe\u8ba1\uff0c\u5141\u8bb8\u64cd\u4f5c\u5458\u63a7\u5236\u4eba\u5f62\u673a\u5668\u4eba\u7684\u6240\u6709\u56db\u80a2\uff0c\u652f\u6301\u5168\u8eab\u63a7\u5236\u7684\u76f4\u63a5\u5173\u8282\u6620\u5c04\u548c\u8fd0\u52a8-\u64cd\u7eb5\uff08loco-manipulation\uff09\u3002\u7cfb\u7edf\u96c6\u6210\u4e86\u81ea\u9002\u5e94\u529b\u53cd\u9988\uff0c\u4ee5\u589e\u5f3a\u64cd\u4f5c\u5458\u4f53\u9a8c\u5e76\u9632\u6b62\u4e0d\u5b89\u5168\u7684\u64cd\u4f5c\u3002", "result": "\u901a\u8fc7\u5728\u4eba\u5f62\u673a\u5668\u4eba\u548c\u591a\u4e2a\u53cc\u81c2\u7cfb\u7edf\u4e0a\u8fdb\u884c\u8fd0\u52a8-\u64cd\u7eb5\u548c\u5168\u8eab\u63a7\u5236\u7684\u793a\u4f8b\uff0c\u9a8c\u8bc1\u4e86CHILD\u7cfb\u7edf\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCHILD\uff08Controller for Humanoid Imitation and Live Demonstration\uff09\u7684\u7d27\u51d1\u578b\u3001\u53ef\u91cd\u6784\u7684\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5173\u8282\u7ea7\u63a7\u5236\uff0c\u5e76\u5f00\u6e90\u4e86\u786c\u4ef6\u8bbe\u8ba1\uff0c\u4ee5\u63d0\u9ad8\u53ef\u53ca\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2508.00085", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00085", "abs": "https://arxiv.org/abs/2508.00085", "authors": ["Raiyaan Abdullah", "Jared Claypoole", "Michael Cogswell", "Ajay Divakaran", "Yogesh Rawat"], "title": "Punching Bag vs. Punching Person: Motion Transferability in Videos", "comment": "Accepted to ICCV 2025 main conference", "summary": "Action recognition models demonstrate strong generalization, but can they\neffectively transfer high-level motion concepts across diverse contexts, even\nwithin similar distributions? For example, can a model recognize the broad\naction \"punching\" when presented with an unseen variation such as \"punching\nperson\"? To explore this, we introduce a motion transferability framework with\nthree datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)\nKinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural\nvideo datasets. We evaluate 13 state-of-the-art models on these benchmarks and\nobserve a significant drop in performance when recognizing high-level actions\nin novel contexts. Our analysis reveals: 1) Multimodal models struggle more\nwith fine-grained unknown actions than with coarse ones; 2) The bias-free\nSyn-TA proves as challenging as real-world datasets, with models showing\ngreater performance drops in controlled settings; 3) Larger models improve\ntransferability when spatial cues dominate but struggle with intensive temporal\nreasoning, while reliance on object and background cues hinders generalization.\nWe further explore how disentangling coarse and fine motions can improve\nrecognition in temporally challenging datasets. We believe this study\nestablishes a crucial benchmark for assessing motion transferability in action\nrecognition. Datasets and relevant code:\nhttps://github.com/raiyaan-abdullah/Motion-Transfer.", "AI": {"tldr": "\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u5728\u5c06\u52a8\u4f5c\u6982\u5ff5\u8fc1\u79fb\u5230\u65b0\u4e0a\u4e0b\u6587\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u7ec6\u7c92\u5ea6\u3001\u65f6\u95f4\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u3002\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\u90fd\u4f1a\u5f71\u54cd\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7a76\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u662f\u5426\u80fd\u6709\u6548\u5730\u5c06\u9ad8\u5c42\u8fd0\u52a8\u6982\u5ff5\u8de8\u4e0d\u540c\u4f46\u76f8\u4f3c\u5206\u5e03\u7684\u4e0a\u4e0b\u6587\u8fdb\u884c\u8fc1\u79fb\uff0c\u4f8b\u5982\u8bc6\u522b\u201c\u6253\u4eba\u201d\u8fd9\u4e00\u672a\u89c1\u8fc7\u7684\u201c\u62f3\u51fb\u201d\u7684\u53d8\u4f53\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u4e09\u4e2a\u6570\u636e\u96c6\uff08Syn-TA\u3001Kinetics400-TA \u548c Something-Something-v2-TA\uff09\u7684\u8fd0\u52a8\u8fc1\u79fb\u6027\u6846\u67b6\uff0c\u5e76\u5728\u8fd9\u4e9b\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86 13 \u79cd\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002", "result": "\u5728\u8bc6\u522b\u65b0\u4e0a\u4e0b\u6587\u4e2d\u7684\u9ad8\u5c42\u52a8\u4f5c\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u591a\u6a21\u6001\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u672a\u77e5\u52a8\u4f5c\u4e0a\u6bd4\u7c97\u7c92\u5ea6\u52a8\u4f5c\u66f4\u6323\u624e\uff1b\u65e0\u504f\u89c1\u7684 Syn-TA \u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e00\u6837\u5177\u6709\u6311\u6218\u6027\uff0c\u6a21\u578b\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u66f4\u660e\u663e\uff1b\u66f4\u5927\u7684\u6a21\u578b\u5728\u7a7a\u95f4\u7ebf\u7d22\u5360\u4e3b\u5bfc\u65f6\u80fd\u63d0\u9ad8\u8fc1\u79fb\u6027\uff0c\u4f46\u5728\u5bc6\u96c6\u7684\u65f6\u95f4\u63a8\u7406\u4e0a\u4f1a\u9047\u5230\u56f0\u96be\uff1b\u4f9d\u8d56\u7269\u4f53\u548c\u80cc\u666f\u7ebf\u7d22\u4f1a\u963b\u788d\u6cdb\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u8fd0\u52a8\u53ef\u8fc1\u79fb\u6027\u8bc4\u4f30\u7684\u5173\u952e\u57fa\u51c6\uff0c\u5e76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5206\u79bb\u7c97\u7565\u548c\u7cbe\u7ec6\u8fd0\u52a8\u6765\u63d0\u9ad8\u6a21\u578b\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u7684\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2508.00158", "categories": ["cond-mat.mtrl-sci", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2508.00158", "abs": "https://arxiv.org/abs/2508.00158", "authors": ["Mike Pols", "Helena Boom", "Geert Brocks", "Sof\u00eda Calero", "Shuxia Tao"], "title": "Impact of Metal Cation on Chiral Properties of 2D Halide Perovskites", "comment": "8 pages, 6 figures", "summary": "Chiral two-dimensional (2D) halide perovskites are formed by embedding chiral\norganic cations in a perovskite crystal structure. The chirality arises from\ndistortions of the 2D metal halide layers induced by the packing of these\norganic cations. Sn-based octahedra spontaneously distort, but it remains\nunclear whether this intrinsic structural instability enhances the chirality.\nWe investigate the effect of the metal cation on structural and phonon\nchirality in MBA$_{2}$Sn$_{\\mathrm{x}}$Pb$_{1-\\mathrm{x}}$I$_{4}$ (x = 0, 1/2,\nand 1). Incorporating Sn does distort the metal halide octehedra, yet it only\nhas a minor impact on the structural chirality. In contrast, the phonons in\nMBA$_{2}$SnI$_{4}$ are substantially more chiral than in MBA$_{2}$PbI$_{4}$,\nespecially the in-plane acoustic modes. However, this enhanced phonon chirality\ndoes not lead to a generation of a larger angular momentum under a temperature\ngradient, because the contributions of different chiral phonons tend to\ncompensate one another.", "AI": {"tldr": "\u5c3d\u7ba1Sn\u7684\u5f15\u5165\u589e\u5f3a\u4e862D\u9499\u949b\u77ff\u7684\u58f0\u5b50\u624b\u6027\uff0c\u4f46\u5e76\u672a\u63d0\u9ad8\u5176\u5728\u6e29\u5ea6\u68af\u5ea6\u4e0b\u7684\u89d2\u52a8\u91cf\u4ea7\u751f\u80fd\u529b\u3002", "motivation": "Sn\u57fa\u516b\u9762\u4f53\u7684\u81ea\u53d1\u626d\u66f2\u662f\u5426\u4f1a\u589e\u5f3a2D\u5364\u5316\u7269\u9499\u949b\u77ff\u7684\u56fa\u6709\u624b\u6027\u3002", "method": "\u901a\u8fc7\u7814\u7a76MBA$_{2}$Sn$_{\\mathrm{x}}$Pb$_{\\mathrm{1}-\\mathrm{x}}$I$_{4}$ (x = 0, 1/2, and 1) \u7684\u7ed3\u6784\u548c\u58f0\u5b50\u624b\u6027\uff0c\u6765\u63a2\u7a76\u91d1\u5c5e\u9633\u79bb\u5b50\u5bf9\u7ed3\u6784\u624b\u6027\u7684\u5f71\u54cd\u3002", "result": "Sn\u7684\u5f15\u5165\u867d\u7136\u626d\u66f2\u4e86\u91d1\u5c5e\u5364\u5316\u7269\u516b\u9762\u4f53\uff0c\u4f46\u5bf9\u7ed3\u6784\u624b\u6027\u5f71\u54cd\u751a\u5fae\u3002MBA$_{2}$SnI$_{4}$\u7684\u58f0\u5b50\u624b\u6027\u663e\u8457\u5f3a\u4e8eMBA$_{2}$PbI$_{4}$\uff0c\u5c24\u5176\u662f\u5728\u9762\u5185\u58f0\u5b66\u6a21\u5f0f\u4e0b\uff0c\u4f46\u5176\u5728\u6e29\u5ea6\u68af\u5ea6\u4e0b\u7684\u89d2\u52a8\u91cf\u4ea7\u751f\u80fd\u529b\u56e0\u58f0\u5b50\u8d21\u732e\u7684\u76f8\u4e92\u8865\u507f\u800c\u53d7\u9650\u3002", "conclusion": "Sn\u7684\u5f15\u5165\u867d\u7136\u4f1a\u626d\u66f2\u91d1\u5c5e\u5364\u5316\u7269\u516b\u9762\u4f53\uff0c\u4f46\u5bf9\u7ed3\u6784\u624b\u6027\u5f71\u54cd\u4e0d\u5927\u3002\u7136\u800c\uff0cMBA$_{2}$SnI$_{4}$\u4e2d\u7684\u58f0\u5b50\u6bd4MBA$_{2}$PbI$_{4}$\u5177\u6709\u66f4\u5f3a\u7684\u58f0\u5b50\u624b\u6027\uff0c\u7279\u522b\u662f\u9762\u5185\u58f0\u5b66\u6a21\u5f0f\u3002\u4f46\u7531\u4e8e\u4e0d\u540c\u624b\u6027\u58f0\u5b50\u7684\u8d21\u732e\u503e\u5411\u4e8e\u76f8\u4e92\u8865\u507f\uff0c\u8fd9\u79cd\u589e\u5f3a\u7684\u58f0\u5b50\u624b\u6027\u5e76\u672a\u5728\u6e29\u5ea6\u68af\u5ea6\u4e0b\u4ea7\u751f\u66f4\u5927\u7684\u89d2\u52a8\u91cf\u3002"}}
{"id": "2508.00229", "categories": ["cs.NE", "math.OC", "90C59 (Primary), 90C27, 68T20, 68W10 (Secondary)", "I.2.8; I.2.6; G.1.6; F.2.1; I.6.6"], "pdf": "https://arxiv.org/pdf/2508.00229", "abs": "https://arxiv.org/abs/2508.00229", "authors": ["Piotr Urba\u0144czyk", "Aleksandra Urba\u0144czyk", "Magdalena Kr\u00f3l", "Leszek Rutkowski", "Marek Kisiel-Dorohinicki"], "title": "Sequential, Parallel and Consecutive Hybrid Evolutionary-Swarm Optimization Metaheuristics", "comment": "16 pages, 2 figures, 5 tables, 5 algorithms, conference", "summary": "The goal of this paper is twofold. First, it explores hybrid\nevolutionary-swarm metaheuristics that combine the features of PSO and GA in a\nsequential, parallel and consecutive manner in comparison with their standard\nbasic form: Genetic Algorithm and Particle Swarm Optimization. The algorithms\nwere tested on a set of benchmark functions, including Ackley, Griewank, Levy,\nMichalewicz, Rastrigin, Schwefel, and Shifted Rotated Weierstrass, across\nmultiple dimensions. The experimental results demonstrate that the hybrid\napproaches achieve superior convergence and consistency, especially in\nhigher-dimensional search spaces. The second goal of this paper is to introduce\na novel consecutive hybrid PSO-GA evolutionary algorithm that ensures\ncontinuity between PSO and GA steps through explicit information transfer\nmechanisms, specifically by modifying GA's variation operators to inherit\nvelocity and personal best information.", "AI": {"tldr": "\u6df7\u5408\u8fdb\u5316-\u7fa4\u4f53\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u8fde\u7eed\u6df7\u5408PSO-GA\u7b97\u6cd5\uff0c\u5728\u5904\u7406\u9ad8\u7ef4\u95ee\u9898\u65f6\u4f18\u4e8e\u6807\u51c6GA\u548cPSO\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u6df7\u5408\u8fdb\u5316-\u7fa4\u4f53\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u8fde\u7eed\u6df7\u5408PSO-GA\u8fdb\u5316\u7b97\u6cd5\u3002", "method": "\u901a\u8fc7\u7ed3\u5408PSO\u548cGA\u7684\u7279\u5f81\uff0c\u4ee5\u987a\u5e8f\u3001\u5e76\u884c\u548c\u8fde\u7eed\u7684\u65b9\u5f0f\u63a2\u7d22\u6df7\u5408\u8fdb\u5316-\u7fa4\u4f53\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u4e0e\u6807\u51c6\u7684GA\u548cPSO\u8fdb\u884c\u6bd4\u8f83\u3002\u901a\u8fc7\u4fee\u6539GA\u7684\u53d8\u5f02\u7b97\u5b50\u6765\u7ee7\u627f\u901f\u5ea6\u548c\u4e2a\u4eba\u6700\u4f73\u4fe1\u606f\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8fde\u7eed\u6df7\u5408PSO-GA\u8fdb\u5316\u7b97\u6cd5\u3002", "result": "\u6df7\u5408\u65b9\u6cd5\u5728\u5305\u62ecAckley\u3001Griewank\u3001Levy\u3001Michalewicz\u3001Rastrigin\u3001Schwefel\u548cShifted Rotated Weierstrass\u5728\u5185\u7684\u57fa\u51c6\u51fd\u6570\u96c6\u4e0a\uff0c\u4ee5\u53ca\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6df7\u5408\u65b9\u6cd5\u5728\u6536\u655b\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u5c24\u5176\u662f\u5728\u66f4\u9ad8\u7ef4\u5ea6\u7684\u641c\u7d22\u7a7a\u95f4\u4e2d\u3002", "conclusion": "\u6df7\u5408\u7b97\u6cd5\u5728\u66f4\u9ad8\u7ef4\u5ea6\u641c\u7d22\u7a7a\u95f4\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6536\u655b\u6027\u548c\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u65b0\u63d0\u51fa\u7684\u8fde\u7eed\u6df7\u5408PSO-GA\u7b97\u6cd5\u901a\u8fc7\u663e\u5f0f\u7684\u4fe1\u200b\u200b\u606f\u4f20\u9012\u673a\u5236\u786e\u4fdd\u4e86PSO\u548cGA\u6b65\u9aa4\u4e4b\u95f4\u7684\u8fde\u7eed\u6027\u3002"}}
{"id": "2508.00055", "categories": ["quant-ph", "cs.CC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2508.00055", "abs": "https://arxiv.org/abs/2508.00055", "authors": ["Ewin Tang", "John Wright"], "title": "Are controlled unitaries helpful?", "comment": "18 pages", "summary": "Many quantum algorithms, to compute some property of a unitary $U$, require\naccess not just to $U$, but to $cU$, the unitary with a control qubit. We show\nthat having access to $cU$ does not help for a large class of quantum problems.\nFor a quantum circuit which uses $cU$ and $cU^\\dagger$ and outputs\n$|\\psi(U)\\rangle$, we show how to ``decontrol'' the circuit into one which uses\nonly $U$ and $U^\\dagger$ and outputs $|\\psi(\\varphi U)\\rangle$ for a uniformly\nrandom phase $\\varphi$, with a small amount of time and space overhead. When we\nonly care about the output state up to a global phase on $U$, then the\ndecontrolled circuit suffices. Stated differently, $cU$ is only helpful because\nit contains global phase information about $U$.\n  A version of our procedure is described in an appendix of Sheridan, Maslov,\nand Mosca [SMM09]. Our goal with this work is to popularize this result by\ngeneralizing it and investigating its implications, in order to counter\nnegative results in the literature which might lead one to believe that\ndecontrolling is not possible. As an application, we give a simple proof for\nthe existence of unitary ensembles which are pseudorandom under access to $U$,\n$U^\\dagger$, $cU$, and $cU^\\dagger$.", "AI": {"tldr": "This paper shows that controlled unitaries (cU) are not more helpful than regular unitaries (U) for many quantum problems. The authors provide a method to \"decontrol\" circuits using cU into ones using only U, with minimal overhead. This is useful when global phase information of U is not important. They also generalize this result and provide an application in proving the existence of pseudorandom unitary ensembles.", "motivation": "The authors aim to popularize a decontrolling result by generalizing it and investigating its implications, countering negative results in the literature that suggest decontrolling is not possible.", "method": "Show how to \"decontrol\" a circuit that uses cU and cU^\top and outputs |\text{psi}(U)\rangle into one that uses only U and U^\top and outputs |\text{psi}(\text{phi} U)\rangle for a uniformly random phase \text{phi}, with a small amount of time and space overhead.", "result": "A decontrolled circuit suffices when one only cares about the output state up to a global phase on U. A simple proof for the existence of unitary ensembles which are pseudorandom under access to U, U^\top, cU, and cU^\top is also provided as an application.", "conclusion": "Having access to cU does not help for a large class of quantum problems. cU is only helpful because it contains global phase information about U."}}
{"id": "2508.00004", "categories": ["cs.LO", "math.LO"], "pdf": "https://arxiv.org/pdf/2508.00004", "abs": "https://arxiv.org/abs/2508.00004", "authors": ["Dazhu Li", "Sujata Ghosh", "Fenrong Liu"], "title": "Reasoning under uncertainty in the game of Cops and Robbers", "comment": null, "summary": "The game of Cops and Robbers is an important model for studying computational\nqueries in pursuit-evasion environments, among others. As recent logical\nexplorations have shown, its structure exhibits appealing analogies with modal\nlogic. In this paper, we enrich the game with a setting in which players may\nhave imperfect information. We propose a new formal framework, Epistemic Logic\nof Cops and Robbers (ELCR), to make the core notions of the game precise, for\ninstance, players' positions, observational power and inference. Applying ELCR\nto analyze the game, we obtain an automated way to track interactions between\nplayers and characterize their information updates during the game. The update\nmechanism is defined by a novel dynamic operator, and we compare it with some\nrelevant paradigms from the game and logic perspectives. We study various\nproperties of ELCR including axiomatization and decidability. To our knowledge,\nthis is the first attempt to explore these games from a formal point of view\nwhere (partial) information available to players is taken into account.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u201c\u8b66\u5bdf\u4e0e\u5f3a\u76d7\u8ba4\u8bc6\u903b\u8f91\u201d\uff08ELCR\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u4fe1\u606f\u4e0d\u5b8c\u5907\u7684\u8b66\u5bdf\u4e0e\u5f3a\u76d7\u535a\u5f08\uff0c\u5b9e\u73b0\u4e86\u73a9\u5bb6\u4e92\u52a8\u548c\u4fe1\u606f\u66f4\u65b0\u7684\u81ea\u52a8\u5316\u8ffd\u8e2a\uff0c\u5e76\u7814\u7a76\u4e86\u8be5\u6846\u67b6\u7684\u516c\u7406\u5316\u548c\u53ef\u5224\u5b9a\u6027\u3002", "motivation": "\u8ffd\u6355-\u9003\u907f\u73af\u5883\u4e2d\u7684\u8ba1\u7b97\u67e5\u8be2\uff0c\u7279\u522b\u662f\u8b66\u5bdf\u4e0e\u5f3a\u76d7\u535a\u5f08\uff0c\u56e0\u5176\u4e0e\u6a21\u6001\u903b\u8f91\u7684\u7c7b\u6bd4\u800c\u5177\u6709\u5438\u5f15\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u5373\u201c\u8b66\u5bdf\u4e0e\u5f3a\u76d7\u8ba4\u8bc6\u903b\u8f91\u201d\uff08ELCR\uff09\uff0c\u4ee5\u7cbe\u786e\u5316\u8bf8\u5982\u73a9\u5bb6\u4f4d\u7f6e\u3001\u89c2\u5bdf\u80fd\u529b\u548c\u63a8\u7406\u7b49\u6838\u5fc3\u6982\u5ff5\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u52a8\u6001\u7b97\u5b50\u6765\u5b9a\u4e49\u4fe1\u606f\u66f4\u65b0\u673a\u5236\uff0c\u5e76\u4ece\u535a\u5f08\u548c\u903b\u8f91\u7684\u89d2\u5ea6\u5c06\u5176\u4e0e\u76f8\u5173\u8303\u5f0f\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7814\u7a76\u4e86ELCR\u7684\u516c\u7406\u5316\u548c\u53ef\u5224\u5b9a\u6027\u7b49\u5404\u79cd\u6027\u8d28\u3002", "result": "ELCR\u4e3a\u8ffd\u8e2a\u73a9\u5bb6\u4e92\u52a8\u548c\u8868\u5f81\u5176\u5728\u535a\u5f08\u4e2d\u7684\u4fe1\u606f\u66f4\u65b0\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u4ece\u5f62\u5f0f\u5316\u89d2\u5ea6\u63a2\u8ba8\u4e86\u8003\u8651\uff08\u90e8\u5206\uff09\u4fe1\u606f\u53ef\u7528\u6027\u7684\u8ffd\u6355-\u9003\u907f\u6e38\u620f\u3002"}}
{"id": "2508.00039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00039", "abs": "https://arxiv.org/abs/2508.00039", "authors": ["Kaustav Chatterjee", "Joshua Q. Li", "Fatemeh Ansari", "Masud Rana Munna", "Kundan Parajulee", "Jared Schwennesen"], "title": "Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings", "comment": null, "summary": "Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose\nsafety risks to highway vehicles due to potential hang-ups. These crossings\ntypically result from post-construction railway track maintenance activities or\nnon-compliance with design guidelines for HRGC vertical alignments.\nConventional methods for measuring HRGC profiles are costly, time-consuming,\ntraffic-disruptive, and present safety challenges. To address these issues,\nthis research employed advanced, cost-effective techniques and innovative\nmodeling approaches for HRGC profile measurement. A novel hybrid deep learning\nframework combining Long Short-Term Memory (LSTM) and Transformer architectures\nwas developed by utilizing instrumentation and ground truth data.\nInstrumentation data were gathered using a highway testing vehicle equipped\nwith Inertial Measurement Unit (IMU) and Global Positioning System (GPS)\nsensors, while ground truth data were obtained via an industrial-standard\nwalking profiler. Field data was collected at the Red Rock Railroad Corridor in\nOklahoma. Three advanced deep learning models Transformer-LSTM sequential\n(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel\n(model 3) were evaluated to identify the most efficient architecture. Models 2\nand 3 outperformed the others and were deployed to generate 2D/3D HRGC\nprofiles. The deep learning models demonstrated significant potential to\nenhance highway and railroad safety by enabling rapid and accurate assessment\nof HRGC hang-up susceptibility.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LSTM\u548cTransformer\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u7cbe\u786e\u6d4b\u91cf\u516c\u8def\u94c1\u8def\u8fd1\u5730\u4ea4\u53c9\u53e3\uff08HRGC\uff09\u7684\u5256\u9762\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u6d4b\u91cf\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u9ad8\u94c1\u8def\u548c\u516c\u8def\u4ea4\u901a\u5b89\u5168\u3002", "motivation": "\u9ad8\u8c03\u7684\u516c\u8def\u94c1\u8def\u8fd1\u5730\u4ea4\u53c9\u53e3\uff08HRGC\uff09\u7531\u4e8e\u6f5c\u5728\u7684\u6302\u8f66\u98ce\u9669\uff0c\u5bf9\u516c\u8def\u8f66\u8f86\u6784\u6210\u4e86\u5b89\u5168\u5a01\u80c1\u3002\u8fd9\u4e9b\u4ea4\u53c9\u53e3\u7684\u51fa\u73b0\u901a\u5e38\u662f\u7531\u4e8e\u8f68\u9053\u7ef4\u4fee\u6d3b\u52a8\u6216\u4e0d\u7b26\u5408\u8bbe\u8ba1\u89c4\u8303\u3002\u4f20\u7edf\u6d4b\u91cfHRGC\u5256\u9762\u7684\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u3001\u4f1a\u5e72\u6270\u4ea4\u901a\uff0c\u5e76\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u66f4\u4f18\u8d8a\u7684\u6d4b\u91cf\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u521b\u65b0\u7684\u5efa\u6a21\u65b9\u6cd5\u548c\u5148\u8fdb\u7684\u3001\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u6280\u672f\u6765\u6d4b\u91cfHRGC\u5256\u9762\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5229\u7528\u4e86\u914d\u5907\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u548c\u5168\u7403\u5b9a\u4f4d\u7cfb\u7edf\uff08GPS\uff09\u4f20\u611f\u5668\u7684\u516c\u8def\u6d4b\u8bd5\u8f66\u8f86\u6240\u6536\u96c6\u7684\u4eea\u5668\u6570\u636e\uff0c\u4ee5\u53ca\u901a\u8fc7\u5de5\u4e1a\u6807\u51c6\u7684\u8d70\u8d70\u5256\u9762\u4eea\u83b7\u5f97\u7684\u771f\u5b9e\u6570\u636e\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u548cTransformer\u67b6\u6784\u7684\u65b0\u578b\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002\u5bf9\u4e09\u79cd\u4e0d\u540c\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08Transformer-LSTM sequential, LSTM-Transformer sequential, and LSTM-Transformer parallel\uff09\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4ee5\u786e\u5b9a\u6700\u9ad8\u6548\u7684\u67b6\u6784\u3002", "result": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08Transformer-LSTM sequential, LSTM-Transformer sequential, and LSTM-Transformer parallel\uff09\uff0c\u5176\u4e2dLSTM-Transformer sequential\u548cLSTM-Transformer parallel\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u8fd9\u4e9b\u6a21\u578b\u5df2\u88ab\u7528\u4e8e\u751f\u62102D/3D HRGC\u5256\u9762\uff0c\u5e76\u8bc1\u660e\u4e86\u901a\u8fc7\u5feb\u901f\u51c6\u786e\u5730\u8bc4\u4f30HRGC\u7684\u6302\u8f66\u6613\u611f\u6027\u6765\u63d0\u9ad8\u516c\u8def\u548c\u94c1\u8def\u5b89\u5168\u6027\u7684\u5de8\u5927\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528LSTM\u548cTransformer\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u4ee5\u5feb\u901f\u51c6\u786e\u5730\u8bc4\u4f30\u516c\u8def\u94c1\u8def\u8fd1\u5730\u4ea4\u53c9\u53e3\uff08HRGC\uff09\u7684\u6302\u8f66\u6613\u611f\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u516c\u8def\u548c\u94c1\u8def\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2508.00086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00086", "abs": "https://arxiv.org/abs/2508.00086", "authors": ["Kelly Kendro", "Jeffrey Maloney", "Scott Jarvis"], "title": "Do LLMs produce texts with \"human-like\" lexical diversity?", "comment": "35 pages; includes abstract", "summary": "The degree to which LLMs produce writing that is truly human-like remains\nunclear despite the extensive empirical attention that this question has\nreceived. The present study addresses this question from the perspective of\nlexical diversity. Specifically, the study investigates patterns of lexical\ndiversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,\nand -4.5) in comparison with texts written by L1 and L2 English participants (n\n= 240) across four education levels. Six dimensions of lexical diversity were\nmeasured in each text: volume, abundance, variety-repetition, evenness,\ndisparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and\nSupport Vector Machines revealed that the LLM-generated texts differed\nsignificantly from human-written texts for each variable, with ChatGPT-o4 mini\nand -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated\nhigher levels of lexical diversity despite producing fewer tokens. The human\nwriters' lexical diversity did not differ across subgroups (i.e., education,\nlanguage status). Altogether, the results indicate that LLMs do not produce\nhuman-like texts in relation to lexical diversity, and the newer LLMs produce\nless human-like texts than older models. We discuss the implications of these\nresults for language pedagogy and related applications.", "AI": {"tldr": "LLM\u6587\u672c\u7684\u8bcd\u6c47\u591a\u6837\u6027\u4e0d\u5982\u4eba\u7c7b\u5199\u4f5c\uff0c\u65b0\u6a21\u578b\u5c24\u5176\u5982\u6b64\u3002ChatGPT-4.5\u8bcd\u6c47\u591a\u6837\u6027\u4f4e\uff0c\u751f\u6210\u8bcd\u6570\u4e5f\u5c11\u3002\u4eba\u7c7b\u4f5c\u8005\u7684\u8bcd\u6c47\u591a\u6837\u6027\u4e0d\u53d7\u6559\u80b2\u548c\u8bed\u8a00\u80cc\u666f\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u5927\u91cf\u5b9e\u8bc1\u7814\u7a76\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u6587\u672c\u7684\u2018\u4eba\u7c7b\u5316\u2019\u7a0b\u5ea6\uff0c\u4f46\u5176\u5177\u4f53\u8868\u73b0\u4ecd\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u65e8\u5728\u4ece\u8bcd\u6c47\u591a\u6837\u6027\u7684\u89d2\u5ea6\u6df1\u5165\u63a2\u8ba8\u8fd9\u4e00\u95ee\u9898\uff0c\u4ee5\u671f\u4e3a\u7406\u89e3LLM\u7684\u5199\u4f5c\u80fd\u529b\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u8bc4\u4f30\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u6d4b\u91cf\u516d\u4e2a\u8bcd\u6c47\u591a\u6837\u6027\u7ef4\u5ea6\uff08\u8bcd\u91cf\u3001\u4e30\u5bcc\u5ea6\u3001\u591a\u6837\u6027-\u91cd\u590d\u6027\u3001\u5747\u5300\u5ea6\u3001\u5dee\u5f02\u5ea6\u548c\u79bb\u6563\u5ea6\uff09\uff0c\u6bd4\u8f83\u4e86\u56db\u79cdChatGPT\u6a21\u578b\uff08-3.5\u3001-4\u3001-o4 mini\u548c-4.5\uff09\u4e0e240\u540d\u4e0d\u540c\u6559\u80b2\u6c34\u5e73\u7684\u4ee5\u82f1\u8bed\u4e3a\u6bcd\u8bed\uff08L1\uff09\u548c\u975e\u6bcd\u8bed\uff08L2\uff09\u7684\u5b66\u4e60\u8005\u6240\u5199\u6587\u672c\u7684\u8bcd\u6c47\u591a\u6837\u6027\u3002\u91c7\u7528\u4e86\u591a\u5143\u65b9\u5dee\u5206\u6790\uff08MANOVAs\uff09\u3001\u5355\u56e0\u7d20\u65b9\u5dee\u5206\u6790\uff08ANOVAS\uff09\u548c\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u7b49\u7edf\u8ba1\u65b9\u6cd5\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u6240\u6709ChatGPT\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u5728\u6240\u6709\u6d4b\u91cf\u7684\u8bcd\u6c47\u591a\u6837\u6027\u7ef4\u5ea6\u4e0a\u5747\u4e0e\u4eba\u7c7b\u5199\u4f5c\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u5176\u4e2d\uff0cChatGPT-o4 mini\u548c-4.5\u6a21\u578b\u7684\u5dee\u5f02\u6700\u4e3a\u660e\u663e\u3002\u5728\u6a21\u578b\u5185\u90e8\uff0cChatGPT-4.5\u867d\u7136\u751f\u6210\u7684\u8bcd\u8bed\uff08tokens\uff09\u6570\u91cf\u8f83\u5c11\uff0c\u4f46\u5176\u8bcd\u6c47\u591a\u6837\u6027\u6c34\u5e73\u53cd\u800c\u66f4\u9ad8\u3002\u7136\u800c\uff0c\u4eba\u7c7b\u4f5c\u8005\u7684\u8bcd\u6c47\u591a\u6837\u6027\u5728\u4e0d\u540c\u6559\u80b2\u80cc\u666f\u548c\u8bed\u8a00\u80fd\u529b\u5206\u7ec4\u4e4b\u95f4\u6ca1\u6709\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "LLM\u751f\u6210\u6587\u672c\u5728\u8bcd\u6c47\u591a\u6837\u6027\u65b9\u9762\u4e0e\u4eba\u7c7b\u5199\u4f5c\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8f83\u65b0\u6a21\u578b\uff08\u5982ChatGPT-4.5\uff09\u7684\u8bcd\u6c47\u591a\u6837\u6027\u66f4\u4f4e\uff0c\u5c3d\u7ba1\u5176\u751f\u6210\u7684\u8bcd\u8bed\u6570\u91cf\u4e5f\u66f4\u5c11\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLM\u6587\u672c\u5728\u8bcd\u6c47\u591a\u6837\u6027\u4e0a\u5e76\u975e\u771f\u6b63\u2018\u4eba\u7c7b\u5316\u2019\uff0c\u5e76\u4e14\u66f4\u65b0\u7684\u6a21\u578b\u53cd\u800c\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u4eba\u7c7b\u5316\u7a0b\u5ea6\u3002\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u8bed\u8a00\u6559\u5b66\u548c\u76f8\u5173\u5e94\u7528\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2508.00106", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00106", "abs": "https://arxiv.org/abs/2508.00106", "authors": ["Ernest Bonnah", "Luan Viet Nguyen", "Khaza Anuarul Hoque"], "title": "Hyperproperty-Constrained Secure Reinforcement Learning", "comment": "Accepted in IEEE/ACM MEMOCODE 2025", "summary": "Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a\ndomain-specific formal specification language known for its effectiveness in\ncompactly representing security, opacity, and concurrency properties for\nrobotics applications. This paper focuses on HyperTWTL-constrained secure\nreinforcement learning (SecRL). Although temporal logic-constrained safe\nreinforcement learning (SRL) is an evolving research problem with several\nexisting literature, there is a significant research gap in exploring\nsecurity-aware reinforcement learning (RL) using hyperproperties. Given the\ndynamics of an agent as a Markov Decision Process (MDP) and opacity/security\nconstraints formalized as HyperTWTL, we propose an approach for learning\nsecurity-aware optimal policies using dynamic Boltzmann softmax RL while\nsatisfying the HyperTWTL constraints. The effectiveness and scalability of our\nproposed approach are demonstrated using a pick-up and delivery robotic mission\ncase study. We also compare our results with two other baseline RL algorithms,\nshowing that our proposed method outperforms them.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06 HyperTWTL\uff08\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u5e94\u7528\u7684\u5f62\u5f0f\u5316\u8bed\u8a00\uff09\u5e94\u7528\u4e8e\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08SecRL\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u5b89\u5168\u611f\u77e5\u6700\u4f18\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u65f6\u95f4\u903b\u8f91\u7ea6\u675f\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08SRL\uff09\u662f\u4e00\u4e2a\u4e0d\u65ad\u53d1\u5c55\u7684\u7814\u7a76\u95ee\u9898\uff0c\u4f46\u5229\u7528\u8d85\u5c5e\u6027\u63a2\u7d22\u5b89\u5168\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u7814\u7a76\u5c1a\u6709\u7a7a\u767d\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5c06 HyperTWTL \u5e94\u7528\u4e8e\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u52a8\u6001\u73bb\u5c14\u5179\u66fc Softmax \u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6ee1\u8db3 HyperTWTL \u7ea6\u675f\u7684\u540c\u65f6\uff0c\u5b66\u4e60\u5b89\u5168\u610f\u8bc6\u6700\u4f18\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u5c06\u667a\u80fd\u4f53\u7684\u52a8\u6001\u8868\u793a\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u5e76\u5c06\u4e0d\u900f\u660e\u6027/\u5b89\u5168\u7ea6\u675f\u5f62\u5f0f\u5316\u4e3a HyperTWTL\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 pick-up and delivery \u673a\u5668\u4eba\u4efb\u52a1\u6848\u4f8b\u7814\u7a76\u4e2d\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002\u4e0e\u4e24\u79cd\u57fa\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 pick-up and delivery \u673a\u5668\u4eba\u4efb\u52a1\u6848\u4f8b\u7814\u7a76\u4e2d\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4f18\u4e8e\u5176\u4ed6\u4e24\u79cd\u57fa\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002"}}
{"id": "2508.00424", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.00424", "abs": "https://arxiv.org/abs/2508.00424", "authors": ["Kresimir Matkovic", "Rainer Splechtna", "Denis Gracanin", "Helwig Hauser"], "title": "CrossSet: Unveiling the Complex Interplay of Two Set-typed Dimensions in Multivariate Data", "comment": "Will be published in TVCG and presented at IEEE VIS", "summary": "The interactive visual analysis of set-typed data, i.e., data with attributes\nthat are of type set, is a rewarding area of research and applications.\nValuable prior work has contributed solutions that enable the study of such\ndata with individual set-typed dimensions. In this paper, we present CrossSet,\na novel method for the joint study of two set-typed dimensions and their\ninterplay. Based on a task analysis, we describe a new, multi-scale approach to\nthe interactive visual exploration and analysis of such data. Two set-typed\ndata dimensions are jointly visualized using a hierarchical matrix layout,\nenabling the analysis of the interactions between two set-typed attributes at\nseveral levels, in addition to the analysis of individual such dimensions.\nCrossSet is anchored at a compact, large-scale overview that is complemented by\ndrill-down opportunities to study the relations between and within the\nset-typed dimensions, enabling an interactive visual multi-scale exploration\nand analysis of bivariate set-typed data. Such an interactive approach makes it\npossible to study single set-typed dimensions in detail, to gain an overview of\nthe interaction and association between two such dimensions, to refine one of\nthe dimensions to gain additional details at several levels, and to drill down\nto the specific interactions of individual set-elements from the set-typed\ndimensions. To demonstrate the effectiveness and efficiency of CrossSet, we\nhave evaluated the new method in the context of several application scenarios.", "AI": {"tldr": "CrossSet\u662f\u4e00\u79cd\u7528\u4e8e\u8054\u5408\u7814\u7a76\u4e24\u4e2a\u96c6\u5408\u7c7b\u578b\u7ef4\u5ea6\u53ca\u5176\u4ea4\u4e92\u4f5c\u7528\u7684\u65b0\u578b\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5206\u6790\u65b9\u6cd5\uff0c\u91c7\u7528\u5206\u5c42\u77e9\u9635\u5e03\u5c40\u5b9e\u73b0\u591a\u5c3a\u5ea6\u5206\u6790\u3002", "motivation": "\u96c6\u5408\u7c7b\u578b\u6570\u636e\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5206\u6790\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u7814\u7a76\u548c\u5e94\u7528\u9886\u57df\uff0c\u73b0\u6709\u7684\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u96c6\u5408\u7c7b\u578b\u7ef4\u5ea6\uff0c\u800c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u4e24\u4e2a\u96c6\u5408\u7c7b\u578b\u7ef4\u5ea6\u53ca\u5176\u76f8\u4e92\u4f5c\u7528\u7684\u8054\u5408\u7814\u7a76\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCrossSet\u7684\u65b0\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4efb\u52a1\u5206\u6790\uff0c\u91c7\u7528\u4e00\u79cd\u65b0\u7684\u591a\u5c3a\u5ea6\u65b9\u6cd5\u8fdb\u884c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u63a2\u7d22\u548c\u5206\u6790\u3002\u4e24\u4e2a\u96c6\u5408\u7c7b\u578b\u7ef4\u5ea6\u901a\u8fc7\u5206\u5c42\u77e9\u9635\u5e03\u5c40\u8fdb\u884c\u8054\u5408\u53ef\u89c6\u5316\u3002", "result": "CrossSet\u80fd\u591f\u540c\u65f6\u5206\u6790\u4e24\u4e2a\u96c6\u5408\u7c7b\u578b\u7ef4\u5ea6\u53ca\u5176\u4ea4\u4e92\u4f5c\u7528\uff0c\u652f\u6301\u5bf9\u5355\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\uff0c\u63d0\u4f9b\u4e24\u4e2a\u7ef4\u5ea6\u95f4\u4ea4\u4e92\u548c\u5173\u8054\u7684\u6982\u89c8\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u7ec6\u5316\u7ef4\u5ea6\u4ee5\u83b7\u53d6\u591a\u5c42\u7ea7\u7ec6\u8282\uff0c\u6700\u7ec8\u53ef\u4ee5\u94bb\u53d6\u5230\u5177\u4f53\u96c6\u5408\u5143\u7d20\u7684\u4ea4\u4e92\u3002", "conclusion": "\u901a\u8fc7\u5206\u5c42\u77e9\u9635\u5e03\u5c40\u8054\u5408\u53ef\u89c6\u5316\u4e24\u4e2a\u96c6\u5408\u7c7b\u578b\u7ef4\u5ea6\uff0c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u63a2\u7d22\u548c\u5206\u6790\uff0c\u80fd\u591f\u8be6\u7ec6\u7814\u7a76\u5355\u4e2a\u96c6\u5408\u7c7b\u578b\u7ef4\u5ea6\u3001\u638c\u63e1\u4e24\u4e2a\u7ef4\u5ea6\u95f4\u7684\u4ea4\u4e92\u548c\u5173\u8054\u3001\u7ec6\u5316\u5176\u4e2d\u4e00\u4e2a\u7ef4\u5ea6\u4ee5\u83b7\u5f97\u591a\u4e2a\u5c42\u7ea7\u7684\u989d\u5916\u7ec6\u8282\uff0c\u4ee5\u53ca\u94bb\u53d6\u81f3\u96c6\u5408\u7c7b\u578b\u7ef4\u5ea6\u4e2d\u5404\u4e2a\u96c6\u5408\u5143\u7d20\u7684\u7279\u5b9a\u4ea4\u4e92\u3002"}}
{"id": "2508.00024", "categories": ["quant-ph", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00024", "abs": "https://arxiv.org/abs/2508.00024", "authors": ["Sebasti\u00e1n Andr\u00e9s Cajas Ord\u00f3\u00f1ez", "Luis Fernando Torres Torres", "Mario Bifulco", "Carlos Andr\u00e9s Dur\u00e1n", "Cristian Bosch", "Ricardo Sim\u00f3n Carbajo"], "title": "Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning", "comment": null, "summary": "Quantum Support Vector Machines face scalability challenges due to\nhigh-dimensional quantum states and hardware limitations. We propose an\nembedding-aware quantum-classical pipeline combining class-balanced k-means\ndistillation with pretrained Vision Transformer embeddings. Our key finding:\nViT embeddings uniquely enable quantum advantage, achieving up to 8.02%\naccuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST,\nwhile CNN features show performance degradation. Using 16-qubit tensor network\nsimulation via cuTensorNet, we provide the first systematic evidence that\nquantum kernel advantage depends critically on embedding choice, revealing\nfundamental synergy between transformer attention and quantum feature spaces.\nThis provides a practical pathway for scalable quantum machine learning that\nleverages modern neural architectures.", "AI": {"tldr": "Quantum Support Vector Machines improved with Vision Transformer embeddings, outperforming classical methods on MNIST datasets; embedding choice is key for quantum advantage.", "motivation": "To address the scalability challenges of Quantum Support Vector Machines, particularly those arising from high-dimensional quantum states and hardware limitations.", "method": "A quantum-classical pipeline combining class-balanced k-means distillation with pretrained Vision Transformer embeddings was proposed and evaluated using 16-qubit tensor network simulations.", "result": "Vision Transformer embeddings led to accuracy improvements of up to 8.02% on Fashion-MNIST and 4.42% on MNIST compared to classical SVMs, whereas CNN features resulted in performance degradation. This provides evidence for the synergy between transformer attention and quantum feature spaces.", "conclusion": "The study demonstrates that the choice of embedding is critical for achieving quantum kernel advantage in Quantum Support Vector Machines, with Vision Transformer embeddings showing significant improvements over classical methods and CNN features."}}
{"id": "2508.00811", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.00811", "abs": "https://arxiv.org/abs/2508.00811", "authors": ["Matthew M. Casey", "Edith Elkind"], "title": "Justified Representation: From Hare to Droop", "comment": null, "summary": "The study of proportionality in multiwinner voting with approval ballots has\nreceived much attention in recent years. Typically, proportionality is captured\nby variants of the Justified Representation axiom, which say that cohesive\ngroups of at least $\\ell\\cdot\\frac{n}{k}$ voters (where $n$ is the total number\nof voters and $k$ is the desired number of winners) deserve $\\ell$\nrepresentatives. The quantity $\\frac{n}{k}$ is known as the Hare quota in the\nsocial choice literature. Another -- more demanding -- choice of quota is the\nDroop quota, defined as $\\lfloor\\frac{n}{k+1}\\rfloor+1$. This quota is often\nused in multiwinner voting with ranked ballots: in algorithms such as Single\nTransferable Voting, and in proportionality axioms, such as Droop's\nProportionality Criterion. A few authors have considered it in the context of\napproval ballots, but the existing analysis is far from comprehensive. The\ncontribution of our work is a systematic study of JR-style axioms (and voting\nrules that satisfy them) defined using the Droop quota instead of the Hare\nquota. For each of the standard JR axioms (namely, JR, PJR, EJR, FPJR, FJR,\nPJR+ and EJR+), we identify a voting rule that satisfies the Droop version of\nthis axiom. In some cases, it suffices to consider known rules (modifying the\ncorresponding Hare proof, sometimes quite substantially), and in other cases it\nis necessary to modify the rules from prior work. Each axiom is more difficult\nto satisfy when defined using the Droop quota, so our results expand the\nfrontier of satisfiable proportionality axioms. We complement our theoretical\nresults with an experimental study, showing that for many probabilistic models\nof voter approvals, Droop JR/EJR+ are considerably more demanding than standard\n(Hare) JR/EJR+.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86\u591a\u8d62\u8005\u6295\u7968\u4e2d\u6bd4\u4f8b\u6027\u516c\u7406\u7684\u8fb9\u754c\uff0c\u901a\u8fc7\u4f7f\u7528\u5fb7\u9c81\u666e\u914d\u989d\u800c\u975e\u54c8\u5c14\u914d\u989d\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5e94\u7684\u6295\u7968\u89c4\u5219\u548c\u7406\u8bba\u8bc1\u660e\uff0c\u540c\u65f6\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b0\u89c4\u5219\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u586b\u8865\u73b0\u6709\u5173\u4e8e\u5728\u591a\u8d62\u8005\u6295\u7968\uff08multiwinner voting\uff09\u548c\u8ba4\u53ef\u6295\u7968\uff08approval ballots\uff09\u4e2d\u4f7f\u7528\u5fb7\u9c81\u666e\u914d\u989d\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u5e76\u6269\u5c55\u53ef\u6ee1\u8db3\u6bd4\u4f8b\u6027\u516c\u7406\u7684\u8fb9\u754c\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u4fee\u6539\u73b0\u6709\u7684\u6295\u7968\u89c4\u5219\u548c\u8bc1\u660e\u65b9\u6cd5\uff0c\u6765\u6ee1\u8db3\u4f7f\u7528\u5fb7\u9c81\u666e\u914d\u989d\u5b9a\u4e49\u7684\u7c7bJR\u516c\u7406\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8fd8\u8fdb\u884c\u4e86\u5b9e\u9a8c\u7814\u7a76\uff0c\u4ee5\u8bc4\u4f30\u8fd9\u4e9b\u516c\u7406\u5728\u4e0d\u540c\u6982\u7387\u6a21\u578b\u4e0b\u7684\u5b9e\u9645\u9700\u6c42\u3002", "result": "\u7814\u7a76\u4e3a\u6bcf\u4e2a\u6807\u51c6\u7684JR\u516c\u7406\uff08JR, PJR, EJR, FPJR, FJR, PJR+ and EJR+\uff09\u90fd\u786e\u5b9a\u4e86\u4e00\u4e2a\u6ee1\u8db3\u5176\u5fb7\u9c81\u666e\u914d\u989d\u7248\u672c\u7684\u6295\u7968\u89c4\u5219\u3002\u7814\u7a76\u8fd8\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5fb7\u9c81\u666e\u914d\u989d\u7684JR/EJR+\u516c\u7406\u6bd4\u54c8\u5c14\u914d\u989d\u7684JR/EJR+\u516c\u7406\u66f4\u96be\u6ee1\u8db3\u3002", "conclusion": "\u8be5\u7814\u7a76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u4f7f\u7528\u5fb7\u9c81\u666e\u914d\u989d\uff08Droop quota\uff09\u800c\u975e\u54c8\u5c14\u914d\u989d\uff08Hare quota\uff09\u5b9a\u4e49\u7684\u7c7bJR\uff08Justified Representation\uff09\u516c\u7406\uff0c\u4ee5\u53ca\u6ee1\u8db3\u8fd9\u4e9b\u516c\u7406\u7684\u6295\u7968\u89c4\u5219\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528\u5fb7\u9c81\u666e\u914d\u989d\u7684\u516c\u7406\u66f4\u96be\u6ee1\u8db3\uff0c\u4f46\u901a\u8fc7\u4fee\u6539\u73b0\u6709\u89c4\u5219\u6216\u5f15\u5165\u65b0\u89c4\u5219\uff0c\u53ef\u4ee5\u5b9e\u73b0\u8fd9\u4e9b\u516c\u7406\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u7814\u7a76\u8868\u660e\uff0c\u5728\u591a\u79cd\u6982\u7387\u6a21\u578b\u4e0b\uff0c\u5fb7\u9c81\u666e\u914d\u989d\u7684JR/EJR+\u516c\u7406\u6bd4\u54c8\u5c14\u914d\u989d\u7684JR/EJR+\u516c\u7406\u8981\u6c42\u66f4\u9ad8\u3002"}}
{"id": "2508.00175", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.00175", "abs": "https://arxiv.org/abs/2508.00175", "authors": ["Jose Guadalupe Romero", "Romeo Ortega", "Leyan Fang", "Alexey Bobtsov"], "title": "Adaptive Compensation of Nonlinear Friction in Mechanical Systems Without Velocity Measurement", "comment": null, "summary": "Friction is an unavoidable phenomenon that exists in all mechanical systems\nincorporating parts with relative motion. It is well-known that friction is a\nserious impediment for precise servo control, hence the interest to devise a\nprocedure to compensate for it -- a subject that has been studied by many\nresearchers for many years. The vast majority of friction compensation schemes\nreported in the literature rely on the availability of velocity measurements,\nan information that is hard to obtain. A second limitation of the existing\nprocedures is that they rely on mathematical models of friction that contain\nseveral unknown parameters, some of them entering nonlinearly in the dynamic\nequations. In this paper we propose a globally convergent tracking controller\nfor a mechanical system perturbed by static and Coulomb friction, which is a\nreliable mathematical model of the friction phenomenon, that does not rely one\nmeasurement of velocity. The key component is an immersion and invariance-based\nadaptive speed observer, used for the friction compensation. To the best of our\nknowledge, this is the first globally convergent solution to this challenging\nproblem. We also present simulation results of the application of our observer\nfor systems affected by friction, which is described by the more advanced LuGre\nmodel.", "AI": {"tldr": "This paper introduces a new controller that compensates for friction in mechanical systems without needing to measure velocity, using a special observer. It's the first of its kind to guarantee convergence and works even with complex friction models.", "motivation": "Friction is a significant impediment to precise servo control in mechanical systems. Existing compensation schemes often require velocity measurements, which are difficult to obtain, and rely on mathematical models with unknown parameters. This work aims to overcome these limitations.", "method": "The paper utilizes an immersion and invariance-based adaptive speed observer for friction compensation, avoiding the need for velocity measurements and relying on a static and Coulomb friction model.", "result": "The paper presents a globally convergent tracking controller and simulation results demonstrating its effectiveness for systems with static and Coulomb friction, as well as the more advanced LuGre friction model.", "conclusion": "The paper proposes a novel globally convergent tracking controller for mechanical systems with static and Coulomb friction, which does not require velocity measurements and uses an immersion and invariance-based adaptive speed observer for friction compensation. This is presented as the first globally convergent solution to this problem."}}
{"id": "2508.00356", "categories": ["cs.CV", "cs.MA", "I.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.00356", "abs": "https://arxiv.org/abs/2508.00356", "authors": ["Angelos Vlachos", "Giorgos Filandrianos", "Maria Lymperaiou", "Nikolaos Spanos", "Ilias Mitsouras", "Vasileios Karampinis", "Athanasios Voulodimos"], "title": "Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning", "comment": null, "summary": "We present a Collaborative Agent-Based Framework for Multi-Image Reasoning.\nOur approach tackles the challenge of interleaved multimodal reasoning across\ndiverse datasets and task formats by employing a dual-agent system: a\nlanguage-based PromptEngineer, which generates context-aware, task-specific\nprompts, and a VisionReasoner, a large vision-language model (LVLM) responsible\nfor final inference. The framework is fully automated, modular, and\ntraining-free, enabling generalization across classification, question\nanswering, and free-form generation tasks involving one or multiple input\nimages. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE\nChallenge (Track A), covering a broad spectrum of visual reasoning tasks\nincluding document QA, visual comparison, dialogue-based understanding, and\nscene-level inference. Our results demonstrate that LVLMs can effectively\nreason over multiple images when guided by informative prompts. Notably, Claude\n3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13%\naccuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how\ndesign choices-such as model selection, shot count, and input length-influence\nthe reasoning performance of different LVLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b PromptEngineer \u548c VisionReasoner \u7684\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u56fe\u50cf\u63a8\u7406\uff0c\u8be5\u6846\u67b6\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u6cdb\u5316\u5230\u591a\u79cd\u4efb\u52a1\uff0c\u5e76\u5728 MIRAGE \u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u8de8\u4e0d\u540c\u6570\u636e\u96c6\u548c\u4efb\u52a1\u683c\u5f0f\u7684\u4ea4\u9519\u591a\u6a21\u6001\u63a8\u7406\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u591a\u56fe\u50cf\u63a8\u7406\u7684\u534f\u540c\u5f0f\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u4e00\u4e2a\u540d\u4e3a PromptEngineer \u7684\u8bed\u8a00\u4ee3\u7406\u6765\u751f\u6210\u63d0\u793a\uff0c\u4ee5\u53ca\u4e00\u4e2a\u540d\u4e3a VisionReasoner \u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u6765\u8fdb\u884c\u63a8\u7406\u3002\u8be5\u6846\u67b6\u662f\u5168\u81ea\u52a8\u3001\u6a21\u5757\u5316\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\uff0c\u53ef\u4ee5\u6cdb\u5316\u5230\u6d89\u53ca\u4e00\u4e2a\u6216\u591a\u4e2a\u8f93\u5165\u56fe\u50cf\u7684\u5206\u7c7b\u3001\u95ee\u7b54\u548c\u81ea\u7531\u5f62\u5f0f\u751f\u6210\u4efb\u52a1\u3002", "result": "\u5728 2025 \u5e74 MIRAGE \u6311\u6218\u8d5b\uff08A \u8d5b\u9053\uff09\u7684 18 \u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e86\u6587\u6863\u95ee\u7b54\u3001\u89c6\u89c9\u6bd4\u8f83\u3001\u57fa\u4e8e\u5bf9\u8bdd\u7684\u7406\u89e3\u548c\u573a\u666f\u7ea7\u63a8\u7406\u7b49\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u3002\u7ed3\u679c\u8868\u660e\uff0cClaude 3.7 \u5728 TQA (99.13%)\u3001DocVQA (96.87%) \u548c MMCoQA (75.28 ROUGE-L) \u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "LVLMs \u5728\u6709\u4fe1\u606f\u91cf\u63d0\u793a\u7684\u6307\u5bfc\u4e0b\u53ef\u4ee5\u6709\u6548\u5730\u63a8\u7406\u591a\u5f20\u56fe\u50cf\u3002"}}
{"id": "2508.00572", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.00572", "abs": "https://arxiv.org/abs/2508.00572", "authors": ["Fanglu Qin", "Haiyang Liu", "Aosai Yang", "Yilin Liu", "Xuanji Wang", "Yue Sun", "Xinyi Zhou", "Zdenek Sofer", "Jiayuan Zhou", "Xue Liu", "Sheng Liu", "Vanessa Li Zhang", "Xiaoze Liu", "Weibo Gao", "Ting Yu"], "title": "Spin light-emitting devices in a 2D magnet", "comment": null, "summary": "Emerging two-dimensional (2D) magnetic semiconductors represent\ntransformative platforms to explore magneto-optics and opto-spintronic\napplications. Though 2D opto-spintronics has attracted tremendous research\nefforts in spin-dependent photodetectors and non-volatile memory components,\nthe realization of one core application - spin-modulated light-emitting device\n(spin-LED) - remains elusive so far. Here we successfully realize prototype\nspin-LED integrated with a 2D semiconducting magnet CrSBr, demonstrating\nconsiderable electroluminescence (EL) down to bilayers. Intriguingly, the EL of\nthe spin-LED is discovered to be directly manipulated by spin-flip and\nspin-canting transitions. Notably, spin-flip transitions enable unprecedented\nhysteretic behaviors of EL characteristics, while spin-canting transitions\ninduce EL continuous modulation with robust anisotropy. This versatile\nmanipulation is originated from the synergy of magnetic-order mediated\nexcitonic transitions and spintronic transport. The prototype demonstration of\nspin-LED establishes an indispensable scheme of opto-spintronic devices\nleveraging 2D spin transitions and strong excitonic effects, presenting a\ncritical step towards integrated 2D opto-spintronics.", "AI": {"tldr": "\u672c\u7814\u7a76\u6210\u529f\u7814\u5236\u51fa\u57fa\u4e8e\u4e8c\u7ef4\u78c1\u6027\u534a\u5bfc\u4f53CrSBr\u7684\u81ea\u65cbLED\u539f\u578b\uff0c\u5b9e\u73b0\u4e86\u7531\u81ea\u65cb\u7ffb\u8f6c\u548c\u81ea\u65cb\u503e\u659c\u8dc3\u8fc1\u8c03\u5236\u7684\u7535\u81f4\u53d1\u5149\uff0c\u4e3a\u4e8c\u7ef4\u5149\u5b50-\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u4e8c\u7ef4\u78c1\u6027\u534a\u5bfc\u4f53\u6750\u6599\u4e3a\u63a2\u7d22\u78c1\u5149\u548c\u5149\u5b50-\u81ea\u65cb\u7535\u5b50\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u5e73\u53f0\u3002\u5c3d\u7ba1\u5728\u81ea\u65cb\u76f8\u5173\u5149\u7535\u63a2\u6d4b\u5668\u548c\u975e\u6613\u5931\u6027\u5b58\u50a8\u5668\u9886\u57df\u5df2\u6709\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u81ea\u65cb\u8c03\u5236\u7684\u53d1\u5149\u5668\u4ef6\uff08\u81ea\u65cbLED\uff09\u7684\u5b9e\u73b0\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u4e8c\u7ef4\u534a\u5bfc\u4f53\u78c1\u6027\u6750\u6599CrSBr\uff0c\u6784\u5efa\u4e86\u81ea\u65cbLED\u539f\u578b\uff0c\u5e76\u5728\u4f4e\u6e29\u4e0b\u5bf9\u5176\u8fdb\u884c\u4e86\u7535\u81f4\u53d1\u5149\uff08EL\uff09\u6d4b\u8bd5\uff0c\u7814\u7a76\u4e86\u81ea\u65cb\u7ffb\u8f6c\u548c\u81ea\u65cb\u503e\u659c\u8dc3\u8fc1\u5bf9\u5176EL\u7279\u6027\u7684\u5f71\u54cd\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u539f\u578b\u81ea\u65cbLED\uff0c\u5728\u4f4e\u81f3\u53cc\u5c42CrSBr\u5668\u4ef6\u4e0a\u89c2\u5bdf\u5230\u663e\u8457\u7684\u7535\u81f4\u53d1\u5149\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u81ea\u65cb\u7ffb\u8f6c\u548c\u81ea\u65cb\u503e\u659c\u8dc3\u8fc1\u80fd\u591f\u5206\u522b\u5f15\u8d77EL\u7279\u6027\u7684\u8fdf\u6ede\u884c\u4e3a\u548c\u8fde\u7eed\u8c03\u5236\uff0c\u5e76\u63ed\u793a\u4e86\u78c1\u5e8f\u4ecb\u5bfc\u7684\u6fc0\u5b50\u8dc3\u8fc1\u4e0e\u81ea\u65cb\u7535\u5b50\u4f20\u8f93\u7684\u534f\u540c\u4f5c\u7528\u662f\u5b9e\u73b0\u8fd9\u79cd\u8c03\u5236\u7684\u673a\u5236\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u96c6\u6210\u4e8c\u7ef4\u534a\u5bfc\u4f53\u78c1\u6027\u6750\u6599CrSBr\u7684\u81ea\u65cbLED\u539f\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u4f4e\u81f3\u53cc\u5c42\u5668\u4ef6\u4e0a\u7684\u7535\u81f4\u53d1\u5149\u7279\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u81ea\u65cb\u7ffb\u8f6c\u548c\u81ea\u65cb\u503e\u659c\u8dc3\u8fc1\u80fd\u591f\u76f4\u63a5\u8c03\u63a7\u8be5\u81ea\u65cbLED\u7684\u53d1\u5149\uff0c\u5176\u4e2d\u81ea\u65cb\u7ffb\u8f6c\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u7535\u81f4\u53d1\u5149\u8fdf\u6ede\u884c\u4e3a\uff0c\u800c\u81ea\u65cb\u503e \u09a6\u0995\u09cd\u09b7\u09a4\u09be\u09b0\u5219\u5b9e\u73b0\u4e86\u5177\u6709\u9c81\u68d2\u6297\u5f02\u6027\u7684\u8fde\u7eed\u7535\u81f4\u53d1\u5149\u8c03\u5236\u3002"}}
{"id": "2508.00347", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.00347", "abs": "https://arxiv.org/abs/2508.00347", "authors": ["Nicol\u00f2 D'Anna", "Nareg Ghazikhanian", "Erik S. Lamb", "Edoardo Zatterin", "Mingze Wan", "Ashley Thorshov", "Ivan K. Schuller", "Oleg Shpyrko"], "title": "Self-strain suppression of the metal-to-insulator transition in phase-change oxide devices", "comment": "8 pages, 5 figures", "summary": "Quantum materials exhibiting phase transitions which can be controlled\nthrough external stimuli, such as electric fields, are promising for future\ncomputing technologies beyond conventional semiconductor transistors. Devices\nthat take advantage of structural phase transitions have inherent built-in\nmemory, reminiscent of synapses and neurons, and are thus natural candidates\nfor neuromorphic computing. Of particular interest are phase-change oxides,\nwhich allow for control over the metal-to-insulator transition. Here, we report\nX-ray nano-diffraction structural imaging of micro-devices fabricated with the\narchetypal phase-change material vanadium sesquioxide (V$_2$O$_3$). The devices\ncontain a Ga ion-irradiated region where the metal-to-insulator transition\ncritical temperature is lowered, a useful feature for controlling neuron-like\nspiking behavior. Results show that strain, induced by crystal lattice mismatch\nbetween the pristine and irradiated material, leads to a suppression of the\nmetal-to-insulator-transition. Suppression occurs within the irradiated region\nor along its edges, depending on the defect-distribution and the size of the\nregion. The observed self-straining effect could extend to other phase-change\noxides and dominate as device dimensions are reduced and become too small to\ndissipate strain within the irradiated region. The findings are important for\nphase engineering in phase-change devices and highlight the necessity to study\nphase transitions at the nanoscale.", "AI": {"tldr": "V2O3\u5fae\u5668\u4ef6\u7684X\u5c04\u7ebf\u7eb3\u7c73\u884d\u5c04\u7814\u7a76\u8868\u660e\uff0c\u9553\u79bb\u5b50\u8f90\u7167\u5bfc\u81f4\u7684\u5e94\u53d8\u4f1a\u6291\u5236\u5176\u91d1\u5c5e-\u7edd\u7f18\u4f53\u8f6c\u53d8\u3002\u8be5\u6548\u5e94\u5728\u7eb3\u7c73\u5c3a\u5ea6\u4e0a\u5bf9\u4e8e\u76f8\u53d8\u5668\u4ef6\u7684\u76f8\u5de5\u7a0b\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u5177\u6709\u53ef\u901a\u8fc7\u5916\u90e8\u523a\u6fc0\uff08\u5982\u7535\u573a\uff09\u63a7\u5236\u7684\u76f8\u53d8\u7684\u91cf\u5b50\u6750\u6599\uff0c\u56e0\u5176\u5177\u6709\u5185\u5728\u7684\u8bb0\u5fc6\u6027\uff0c\u7c7b\u4f3c\u4e8e\u795e\u7ecf\u7a81\u89e6\uff0c\u56e0\u6b64\u6709\u671b\u7528\u4e8e\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u3002\u7279\u522b\u662f\u76f8\u53d8\u6c27\u5316\u7269\uff0c\u53ef\u4ee5\u63a7\u5236\u91d1\u5c5e-\u7edd\u7f18\u4f53\u8f6c\u53d8\u3002", "method": "\u901a\u8fc7X\u5c04\u7ebf\u7eb3\u7c73\u884d\u5c04\u7ed3\u6784\u6210\u50cf\uff0c\u5bf9\u542b\u6709\u9553\u79bb\u5b50\u8f90\u7167\u533a\u57df\u7684\u9492\u6c27\u5316\u7269\uff08V2O3\uff09\u5fae\u5668\u4ef6\u8fdb\u884c\u4e86\u7814\u7a76\u3002\u8f90\u7167\u533a\u57df\u4f1a\u964d\u4f4e\u91d1\u5c5e-\u7edd\u7f18\u4f53\u8f6c\u53d8\u7684\u4e34\u754c\u6e29\u5ea6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u539f\u59cb\u6750\u6599\u4e0e\u8f90\u7167\u6750\u6599\u4e4b\u95f4\u7684\u6676\u683c\u5931\u914d\u5f15\u8d77\u7684\u5e94\u53d8\u4f1a\u5bfc\u81f4\u91d1\u5c5e-\u7edd\u7f18\u4f53\u8f6c\u53d8\u7684\u6291\u5236\u3002\u6291\u5236\u4f5c\u7528\u53d1\u751f\u5728\u8f90\u7167\u533a\u57df\u5185\u90e8\u6216\u5176\u8fb9\u7f18\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u7f3a\u9677\u5206\u5e03\u548c\u533a\u57df\u5927\u5c0f\u3002", "conclusion": "\u5e94\u53d8\u8bf1\u5bfc\u7684\u91d1\u5c5e-\u7edd\u7f18\u4f53\u8f6c\u53d8\u6291\u5236\u73b0\u8c61\u53ef\u80fd\u5ef6\u4f38\u5230\u5176\u4ed6\u76f8\u53d8\u6c27\u5316\u7269\uff0c\u5e76\u4e14\u968f\u7740\u5668\u4ef6\u5c3a\u5bf8\u51cf\u5c0f\u800c\u53d8\u5f97\u66f4\u52a0\u91cd\u8981\uff0c\u56e0\u4e3a\u51cf\u5c0f\u5c3a\u5bf8\u4f1a\u9650\u5236\u5e94\u53d8\u5728\u8f90\u7167\u533a\u57df\u5185\u7684\u6d88\u6563\u3002\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u4e8e\u76f8\u53d8\u5668\u4ef6\u4e2d\u7684\u76f8\u5de5\u7a0b\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u7eb3\u7c73\u5c3a\u5ea6\u4e0a\u7814\u7a76\u76f8\u53d8\u884c\u4e3a\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.00326", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00326", "abs": "https://arxiv.org/abs/2508.00326", "authors": ["Chengwang Ji", "Kehui Li", "Haiquan Lu", "Qiaoyan Peng", "Jintao Wang", "Shaodan Ma"], "title": "Model-Driven Deep Learning Enhanced Joint Beamforming and Mode Switching for RDARS-Aided MIMO Systems", "comment": null, "summary": "Reconfigurable distributed antenna and reflecting surface (RDARS) is a\npromising architecture for future sixth-generation (6G) wireless networks. In\nparticular, the dynamic working mode configuration for the RDARS-aided system\nbrings an extra selection gain compared to the existing reconfigurable\nintelligent surface (RIS)-aided system and distributed antenna system (DAS). In\nthis paper, we consider the RDARS-aided downlink multiple-input multiple-output\n(MIMO) system and aim to maximize the weighted sum rate (WSR) by jointly\noptimizing the beamforming matrices at the based station (BS) and RDARS, as\nwell as mode switching matrix at RDARS. The optimization problem is challenging\nto be solved due to the non-convex objective function and mixed integer binary\nconstraint. To this end, a penalty term-based weight minimum mean square error\n(PWM) algorithm is proposed by integrating the majorization-minimization (MM)\nand weight minimum mean square error (WMMSE) methods. To further escape the\nlocal optimum point in the PWM algorithm, a model-driven DL method is\nintegrated into this algorithm, where the key variables related to the\nconvergence of PWM algorithm are trained to accelerate the convergence speed\nand improve the system performance. Simulation results are provided to show\nthat the PWM-based beamforming network (PWM-BFNet) can reduce the number of\niterations by half and achieve performance improvements of 26.53% and 103.2% at\nthe scenarios of high total transmit power and a large number of RDARS transmit\nelements (TEs), respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684PWM-BFNet\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316RDARS\u8f85\u52a9\u7684MIMO\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408PWM\u548cDL\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u6536\u655b\u65f6\u95f4\u3002", "motivation": "\u53ef\u91cd\u6784\u5206\u5e03\u5f0f\u5929\u7ebf\u548c\u53cd\u5c04\u9762\uff08RDARS\uff09\u662f\u672a\u67656G\u65e0\u7ebf\u7f51\u7edc\u7684\u4e00\u79cd\u6709\u524d\u9014\u7684\u67b6\u6784\u3002\u7279\u522b\u662f\uff0cRDARS\u8f85\u52a9\u7cfb\u7edf\u52a8\u6001\u5de5\u4f5c\u6a21\u5f0f\u7684\u914d\u7f6e\uff0c\u76f8\u6bd4\u4e8e\u73b0\u6709\u7684\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u8f85\u52a9\u7cfb\u7edf\u548c\u5206\u5e03\u5f0f\u5929\u7ebf\u7cfb\u7edf\uff08DAS\uff09\uff0c\u5e26\u6765\u4e86\u989d\u5916\u7684\u9009\u62e9\u589e\u76ca\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3RDARS\u8f85\u52a9\u4e0b\u884c\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08MIMO\uff09\u7cfb\u7edf\u4e2d\u6743\u91cd\u548c\u901f\u7387\uff08WSR\uff09\u6700\u5927\u5316\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u60e9\u7f5a\u9879\u7684\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\uff08PWM\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4e3b\u8981\u5316-\u6700\u5c0f\u5316\uff08MM\uff09\u548c\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\uff08WMMSE\uff09\u65b9\u6cd5\u6765\u89e3\u51b3\u4f18\u5316\u95ee\u9898\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u8df3\u51faPWM\u7b97\u6cd5\u7684\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u8fd8\u96c6\u6210\u4e86\u4e00\u79cd\u6a21\u578b\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e0ePWM\u7b97\u6cd5\u6536\u655b\u76f8\u5173\u7684\u5173\u952e\u53d8\u91cf\u6765\u52a0\u901f\u6536\u655b\u5e76\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "result": "PWM-BFNet\u53ef\u4ee5\u5c06\u8fed\u4ee3\u6b21\u6570\u51cf\u5c11\u4e00\u534a\uff0c\u5e76\u5728\u9ad8\u603b\u53d1\u5c04\u529f\u7387\u548c\u5927\u91cfRDARS\u53d1\u5c04\u5355\u5143\uff08TE\uff09\u7684\u573a\u666f\u4e0b\u5206\u522b\u5b9e\u73b026.53%\u548c103.2%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8ePWM\u7684\u6ce2\u675f\u6210\u5f62\u7f51\u7edc\uff08PWM-BFNet\uff09\u53ef\u4ee5\u5c06\u8fed\u4ee3\u6b21\u6570\u51cf\u5c11\u4e00\u534a\uff0c\u5e76\u5728\u9ad8\u603b\u53d1\u5c04\u529f\u7387\u548c\u5927\u91cfRDARS\u53d1\u5c04\u5355\u5143\uff08TE\uff09\u7684\u573a\u666f\u4e0b\u5206\u522b\u5b9e\u73b026.53%\u548c103.2%\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.00258", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00258", "abs": "https://arxiv.org/abs/2508.00258", "authors": ["Zhiwei Wu", "Siyi Wei", "Jiahao Luo", "Jinhui Zhang"], "title": "Topology-Inspired Morphological Descriptor for Soft Continuum Robots", "comment": null, "summary": "This paper presents a topology-inspired morphological descriptor for soft\ncontinuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory\nto achieve a quantitative characterization of robot morphologies. By counting\ncritical points of directional projections, the proposed descriptor enables a\ndiscrete representation of multimodal configurations and facilitates\nmorphological classification. Furthermore, we apply the descriptor to\nmorphology control by formulating the target configuration as an optimization\nproblem to compute actuation parameters that generate equilibrium shapes with\ndesired topological features. The proposed framework provides a unified\nmethodology for quantitative morphology description, classification, and\ncontrol of soft continuum robots, with the potential to enhance their precision\nand adaptability in medical applications such as minimally invasive surgery and\nendovascular interventions.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.00088", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00088", "abs": "https://arxiv.org/abs/2508.00088", "authors": ["Mateo de Mayo", "Daniel Cremers", "Taih\u00fa Pire"], "title": "The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking", "comment": "Accepted to IROS 2025", "summary": "Humanoid robots and mixed reality headsets benefit from the use of\nhead-mounted sensors for tracking. While advancements in visual-inertial\nodometry (VIO) and simultaneous localization and mapping (SLAM) have produced\nnew and high-quality state-of-the-art tracking systems, we show that these are\nstill unable to gracefully handle many of the challenging settings presented in\nthe head-mounted use cases. Common scenarios like high-intensity motions,\ndynamic occlusions, long tracking sessions, low-textured areas, adverse\nlighting conditions, saturation of sensors, to name a few, continue to be\ncovered poorly by existing datasets in the literature. In this way, systems may\ninadvertently overlook these essential real-world issues. To address this, we\npresent the Monado SLAM dataset, a set of real sequences taken from multiple\nvirtual reality headsets. We release the dataset under a permissive CC BY 4.0\nlicense, to drive advancements in VIO/SLAM research and development.", "AI": {"tldr": "New dataset released for VR/headset tracking: Monado SLAM dataset captures challenging real-world scenarios like fast motion, occlusions, and bad lighting, which existing datasets miss. Promotes better VIO/SLAM development.", "motivation": "Existing VIO/SLAM tracking systems struggle with challenging real-world scenarios common in head-mounted applications such as high-intensity motion, dynamic occlusions, long tracking sessions, low-textured areas, adverse lighting, and sensor saturation. Current datasets inadequately cover these issues.", "method": "The paper introduces the Monado SLAM dataset, which consists of real-world sequences captured from multiple virtual reality headsets, specifically designed to encompass challenging scenarios often encountered in head-mounted use cases.", "result": "The Monado SLAM dataset has been created and released to provide a more comprehensive resource for evaluating and improving VIO/SLAM systems in challenging head-mounted tracking environments.", "conclusion": "The Monado SLAM dataset, comprising real-world sequences from various VR headsets and released under a CC BY 4.0 license, is presented to address the limitations of existing datasets in handling challenging head-mounted tracking scenarios and to promote further research and development in VIO/SLAM."}}
{"id": "2508.00187", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.00187", "abs": "https://arxiv.org/abs/2508.00187", "authors": ["Anas Abu-Odeh", "James Warren"], "title": "Atomistic Simulations Reveal the Need to Reassess Standard Thermodynamic Models of Coherent Precipitates", "comment": null, "summary": "Accurate models of precipitation kinetics are essential to control and design\nstructural materials. These models are highly sensitive to the thermodynamic\ndescription of precipitates. We use atomistic simulations of a model Fe-Cr\nsystem to assess two commonly used assumptions in the thermodynamic modeling of\ncoherent precipitates: that elastic effects can be neglected for systems with a\nsmall lattice misfit and that size effects can be neglected for low levels of\nsupersaturation. We find that these assumptions cannot be maintained for an\naccurate description of interfacial equilibrium, even when lattice misfits are\nbelow 1 % and supersaturation values are below 1 %. Additionally, we find a\nsurprising trend at large precipitate radii that suggests the importance of\nhigher-order effects that are commonly neglected. The results and insights from\nthis study highlight the need to revisit current approaches in modeling\nsolid-state precipitation.", "AI": {"tldr": "\u901a\u8fc7\u539f\u5b50\u6a21\u62df\u53d1\u73b0\uff0c\u5728Fe-Cr\u7cfb\u7edf\u4e2d\uff0c\u5373\u4f7f\u5728\u4f4e\u6676\u683c\u5931\u914d\u548c\u4f4e\u8fc7\u9971\u548c\u5ea6\u4e0b\uff0c\u5ffd\u7565\u5f39\u6027\u548c\u5c3a\u5bf8\u6548\u5e94\u7684\u5e38\u7528\u70ed\u529b\u5b66\u6a21\u578b\u5047\u8bbe\u4e5f\u4e0d\u51c6\u786e\uff0c\u5e76\u4e14\u5728\u9ad8\u534a\u5f84\u4e0b\u89c2\u5bdf\u5230\u9700\u8981\u8003\u8651\u9ad8\u9636\u6548\u5e94\u3002", "motivation": "\u7cbe\u786e\u7684\u6c89\u6dc0\u52a8\u529b\u5b66\u6a21\u578b\u5bf9\u4e8e\u63a7\u5236\u548c\u8bbe\u8ba1\u7ed3\u6784\u6750\u6599\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u4e9b\u6a21\u578b\u5bf9\u6c89\u6dc0\u7269\u7684\u70ed\u529b\u5b66\u63cf\u8ff0\u9ad8\u5ea6\u654f\u611f\u3002", "method": "\u4f7f\u7528\u539f\u5b50\u6a21\u62df\u7814\u7a76\u6a21\u578bFe-Cr\u7cfb\u7edf\uff0c\u8bc4\u4f30\u5728\u70ed\u529b\u5b66\u6a21\u578b\u4e2d\u5bf9\u76f8\u5e72\u6c89\u6dc0\u7269\u5e38\u7528\u7684\u4e24\u79cd\u5047\u8bbe\uff08\u5373\u5c0f\u6676\u683c\u5931\u914d\u7cfb\u7edf\u53ef\u5ffd\u7565\u5f39\u6027\u6548\u5e94\uff0c\u4ee5\u53ca\u4f4e\u8fc7\u9971\u548c\u5ea6\u6c34\u5e73\u53ef\u5ffd\u7565\u5c3a\u5bf8\u6548\u5e94\uff09\u7684\u51c6\u786e\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u6676\u683c\u5931\u914d\u5c0f\u4e8e1%\uff0c\u8fc7\u9971\u548c\u5ea6\u4f4e\u4e8e1%\uff0c\u8fd9\u4e9b\u5047\u8bbe\u4e5f\u4e0d\u80fd\u7ef4\u6301\u5bf9\u754c\u9762\u5e73\u8861\u7684\u51c6\u786e\u63cf\u8ff0\u3002\u6b64\u5916\uff0c\u5728\u8f83\u5927\u7684\u6c89\u6dc0\u7269\u534a\u5f84\u4e0b\u89c2\u5bdf\u5230\u4ee4\u4eba\u60ca\u8bb6\u7684\u8d8b\u52bf\uff0c\u8868\u660e\u901a\u5e38\u88ab\u5ffd\u7565\u7684\u9ad8\u9636\u6548\u5e94\u5bf9\u7cbe\u786e\u6a21\u62df\u6709\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u6a21\u62df\u56fa\u6001\u6c89\u6dc0\u65f6\u91cd\u65b0\u5ba1\u89c6\u5f53\u524d\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.00380", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2508.00380", "abs": "https://arxiv.org/abs/2508.00380", "authors": ["Kebin Sun", "Tao Jiang", "Ran Cheng", "Yaochu Jin", "Kay Chen Tan"], "title": "Evolutionary Generative Optimization: Towards Fully Data-Driven Evolutionary Optimization via Generative Learning", "comment": null, "summary": "Recent advances in data-driven evolutionary algorithms (EAs) have\ndemonstrated the potential of leveraging data to improve optimization accuracy\nand adaptability. Nevertheless, most existing approaches remain dependent on\nhandcrafted heuristics, which limits their generality and automation. To\naddress this challenge, we propose Evolutionary Generative Optimization\n(EvoGO), a fully data-driven framework empowered by generative learning. EvoGO\nstreamlines the evolutionary optimization process into three stages: data\npreparation, model training, and population generation. The data preparation\nstage constructs a pairwise dataset to enrich training diversity without\nincurring additional evaluation costs. During model training, a tailored\ngenerative model learns to transform inferior solutions into superior ones. In\nthe population generation stage, EvoGO replaces traditional reproduction\noperators with a scalable and parallelizable generative mechanism. Extensive\nexperiments on numerical benchmarks, classical control problems, and\nhigh-dimensional robotic tasks demonstrate that EvoGO consistently converges\nwithin merely 10 generations and significantly outperforms a wide spectrum of\noptimization approaches, including traditional EAs, Bayesian optimization, and\nreinforcement learning based methods. Source code will be made publicly\navailable.", "AI": {"tldr": "EvoGO is a new data-driven evolutionary optimization framework that uses generative learning to improve performance and reduce reliance on handcrafted heuristics. It streamlines the process into data preparation, model training, and population generation, consistently converging within 10 generations and outperforming existing methods.", "motivation": "Most existing data-driven EAs remain dependent on handcrafted heuristics, which limits their generality and automation. EvoGO aims to address this challenge by proposing a fully data-driven framework.", "method": "EvoGO is a fully data-driven framework empowered by generative learning. It streamlines the evolutionary optimization process into three stages: data preparation, model training, and population generation. The data preparation stage constructs a pairwise dataset to enrich training diversity without incurring additional evaluation costs. During model training, a tailored generative model learns to transform inferior solutions into superior ones. In the population generation stage, EvoGO replaces traditional reproduction operators with a scalable and parallelizable generative mechanism.", "result": "Extensive experiments on numerical benchmarks, classical control problems, and high-dimensional robotic tasks demonstrate EvoGO's effectiveness.", "conclusion": "EvoGO consistently converges within merely 10 generations and significantly outperforms a wide spectrum of optimization approaches, including traditional EAs, Bayesian optimization, and reinforcement learning based methods."}}
{"id": "2508.00014", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2508.00014", "abs": "https://arxiv.org/abs/2508.00014", "authors": ["Isa Vialard"], "title": "Deciding the Value of Two-Clock Almost Non-Zeno Weighted Timed Games", "comment": null, "summary": "The Value Problem for weighted timed games (wtgs) consists in determining,\ngiven a two-player weighted timed game with a reachability objective and a\nrational threshold, whether or not the value of the game exceeds the threshold.\nWhen restrained to wtgs with non-negative weight, this problem is known to be\nundecidable for weighted timed games with three or more clocks, and decidable\nfor one-clock wtgs. The Value Problem for two-clock non-negative wtgs, which\nremained stubbornly open for a decade, was recently shown to be undecidable. In\nthis article, we show that the Value Problem is decidable when considering\ntwo-clock almost non-Zeno wtgs.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\uff0c\u5728\u4e24\u65f6\u949f\u8fd1\u4f3c\u975eZeno\u52a0\u6743\u65f6\u5e8f\u535a\u5f08\uff08WTGs\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u4ef7\u503c\u95ee\u9898\u662f\u53ef\u5224\u5b9a\u7684\u3002\u8fd9\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u5df2\u89e3\u51b3\u4e00\u65f6\u949f\u548c\u8fd1\u671f\u89e3\u51b3\u4e24\u65f6\u949f\u975e\u8d1f\u52a0\u6743\u65f6\u5e8f\u535a\u5f08\uff08WTGs\uff09\u7684\u4ef7\u503c\u95ee\u9898\u540e\uff0c\u7814\u7a76\u4e24\u65f6\u949f\u8fd1\u4f3c\u975eZeno WTGs\u7684\u4ef7\u503c\u95ee\u9898\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u4e24\u65f6\u949f\u8fd1\u4f3c\u975eZeno\u52a0\u6743\u65f6\u5e8f\u535a\u5f08\u7684\u6027\u8d28\u6765\u8bc1\u660e\u5176\u4ef7\u503c\u95ee\u9898\u7684\u53ef\u5224\u5b9a\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u4e24\u65f6\u949f\u8fd1\u4f3c\u975eZeno\u52a0\u6743\u65f6\u5e8f\u535a\u5f08\u7684\u4ef7\u503c\u95ee\u9898\u662f\u53ef\u5224\u5b9a\u7684\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u5bf9\u4e8e\u4e24\u65f6\u949f\u8fd1\u4f3c\u975eZeno\u52a0\u6743\u65f6\u5e8f\u535a\u5f08\uff0c\u4ef7\u503c\u95ee\u9898\u662f\u53ef\u5224\u5b9a\u7684\u3002"}}
{"id": "2508.00040", "categories": ["cs.LG", "math.PR", "stat.AP", "stat.ML", "60J20, 68T07"], "pdf": "https://arxiv.org/pdf/2508.00040", "abs": "https://arxiv.org/abs/2508.00040", "authors": ["Abhinav Das", "Stephan Schl\u00fcter"], "title": "Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting", "comment": null, "summary": "This work integrates Bayesian regime detection with conditional neural\nprocesses for 24-hour electricity price prediction in the German market. Our\nmethodology integrates regime detection using a disentangled sticky\nhierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to\ndaily electricity prices. Each identified regime is subsequently modeled by an\nindependent conditional neural process (CNP), trained to learn localized\nmappings from input contexts to 24-dimensional hourly price trajectories, with\nfinal predictions computed as regime-weighted mixtures of these CNP outputs. We\nrigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated\nauto-regressive (LEAR) models by integrating their forecasts into diverse\nbattery storage optimization frameworks, including price arbitrage, risk\nmanagement, grid services, and cost minimization. This operational utility\nassessment revealed complex performance trade-offs: LEAR often yielded superior\nabsolute profits or lower costs, while DNN showed exceptional optimality in\nspecific cost-minimization contexts. Recognizing that raw prediction accuracy\ndoesn't always translate to optimal operational outcomes, we employed TOPSIS as\na comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified\nLEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model\nemerged as the most balanced and preferred solution for 2021, 2022 and 2023.", "AI": {"tldr": "\"\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8d1d\u53f6\u65af\u72b6\u6001\u68c0\u6d4b\u548c\u6761\u4ef6\u795e\u7ecf\u7f51\u7edc\uff08R-NP\uff09\u7684\u7535\u529b\u4ef7\u683c\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u5728\u7535\u6c60\u50a8\u80fd\u4f18\u5316\u5e94\u7528\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0cR-NP\u6a21\u578b\u5728\u591a\u6807\u51c6\u8bc4\u4f30\u4e2d\u8868\u73b0\u6700\u4e3a\u5747\u8861\u548c\u4f18\u8d8a\u3002\"", "motivation": "\"\u4e3a\u4e86\u66f4\u51c6\u786e\u5730\u9884\u6d4b24\u5c0f\u65f6\u7535\u529b\u4ef7\u683c\uff0c\u5e76\u8bc4\u4f30\u9884\u6d4b\u6a21\u578b\u5728\u7535\u6c60\u50a8\u80fd\u4f18\u5316\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u6548\u7528\uff0c\u672c\u6587\u6574\u5408\u4e86\u8d1d\u53f6\u65af\u72b6\u6001\u68c0\u6d4b\u548c\u6761\u4ef6\u795e\u7ecf\u7f51\u7edc\u3002\"", "method": "\"\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aR-NP\u7684\u6df7\u5408\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u8d1d\u53f6\u65af\u72b6\u6001\u68c0\u6d4b\u548c\u6761\u4ef6\u795e\u7ecf\u7f51\u7edc\u3002\u5177\u4f53\u5730\uff0c\u4f7f\u7528\u89e3\u8026\u7c98\u6027\u72c4\u5229\u514b\u96f7\u8fc7\u7a0b\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08DS-HDP-HMM\uff09\u5bf9\u5fb7\u56fd\u7535\u529b\u5e02\u573a24\u5c0f\u65f6\u7684\u7535\u529b\u4ef7\u683c\u8fdb\u884c\u72b6\u6001\u68c0\u6d4b\uff0c\u8bc6\u522b\u51fa\u4e0d\u540c\u7684\u72b6\u6001\u3002\u7136\u540e\uff0c\u4e3a\u6bcf\u4e2a\u8bc6\u522b\u51fa\u7684\u72b6\u6001\u8bad\u7ec3\u4e00\u4e2a\u72ec\u7acb\u7684\u6761\u4ef6\u795e\u7ecf\u7f51\u7edc\uff08CNP\uff09\uff0c\u4ee5\u5b66\u4e60\u4ece\u8f93\u5165\u4e0a\u4e0b\u6587\u523024\u5c0f\u65f6\u7535\u529b\u4ef7\u683c\u8f68\u8ff9\u7684\u5c40\u90e8\u6620\u5c04\u3002\u6700\u7ec8\u7684\u9884\u6d4b\u662f\u901a\u8fc7\u5bf9\u8fd9\u4e9bCNP\u8f93\u51fa\u8fdb\u884c\u72b6\u6001\u52a0\u6743\u6df7\u5408\u5f97\u5230\u7684\u3002\"", "result": "\"\u901a\u8fc7\u5c06R-NP\u6a21\u578b\u3001\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u548cLasso\u4f30\u8ba1\u81ea\u56de\u5f52\uff08LEAR\uff09\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u6574\u5408\u5230\u7535\u6c60\u50a8\u80fd\u4f18\u5316\u6846\u67b6\uff08\u5305\u62ec\u4ef7\u683c\u5957\u5229\u3001\u98ce\u9669\u7ba1\u7406\u3001\u7535\u7f51\u670d\u52a1\u548c\u6210\u672c\u6700\u5c0f\u5316\uff09\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u7814\u7a76\u53d1\u73b0LEAR\u6a21\u578b\u5728\u7edd\u5bf9\u5229\u6da6\u6216\u6210\u672c\u65b9\u9762\u901a\u5e38\u8868\u73b0\u6700\u4f18\uff0cDNN\u6a21\u578b\u5728\u7279\u5b9a\u6210\u672c\u6700\u5c0f\u5316\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u7ecf\u8fc7TOPSIS\u591a\u6807\u51c6\u8bc4\u4f30\u540e\uff0cR-NP\u6a21\u578b\u57282021\u30012022\u548c2023\u5e74\u88ab\u8ba4\u4e3a\u662f\u6574\u4f53\u8868\u73b0\u6700\u5747\u8861\u3001\u6700\u53d7\u9752\u7750\u7684\u6a21\u578b\u3002\"", "conclusion": "\"R-NP\u6a21\u578b\u57282021\u5e74\u30012022\u5e74\u548c2023\u5e74\u88ab\u786e\u5b9a\u4e3a\u6700\u5747\u8861\u4e14\u6700\u4f18\u9009\u7684\u6a21\u578b\uff0c\u5c3d\u7ba1\u5728\u7edd\u5bf9\u5229\u6da6\u548c\u6210\u672c\u65b9\u9762\uff0cLEAR\u6a21\u578b\u6709\u65f6\u8868\u73b0\u66f4\u4f18\uff0cDNN\u6a21\u578b\u5728\u7279\u5b9a\u6210\u672c\u6700\u5c0f\u5316\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002\""}}
{"id": "2508.00095", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.00095", "abs": "https://arxiv.org/abs/2508.00095", "authors": ["Zachary K. Stine", "James E. Deitrick"], "title": "Semiotic Complexity and Its Epistemological Implications for Modeling Culture", "comment": "Preprint. Manuscript currently under review", "summary": "Greater theorizing of methods in the computational humanities is needed for\nepistemological and interpretive clarity, and therefore the maturation of the\nfield. In this paper, we frame such modeling work as engaging in translation\nwork from a cultural, linguistic domain into a computational, mathematical\ndomain, and back again. Translators benefit from articulating the theory of\ntheir translation process, and so do computational humanists in their work --\nto ensure internal consistency, avoid subtle yet consequential translation\nerrors, and facilitate interpretive transparency. Our contribution in this\npaper is to lay out a particularly consequential dimension of the lack of\ntheorizing and the sorts of translation errors that emerge in our modeling\npractices as a result. Along these lines we introduce the idea of semiotic\ncomplexity as the degree to which the meaning of some text may vary across\ninterpretive lenses, and make the case that dominant modeling practices --\nespecially around evaluation -- commit a translation error by treating\nsemiotically complex data as semiotically simple when it seems\nepistemologically convenient by conferring superficial clarity. We then lay out\nseveral recommendations for researchers to better account for these\nepistemological issues in their own work.", "AI": {"tldr": "\u8ba1\u7b97\u4eba\u6587\u9886\u57df\u9700\u8981\u52a0\u5f3a\u65b9\u6cd5\u7406\u8bba\u5316\uff0c\u5c06\u5efa\u6a21\u89c6\u4e3a\u7ffb\u8bd1\u8fc7\u7a0b\uff0c\u6ce8\u610f\u7b26\u53f7\u590d\u6742\u6027\uff0c\u907f\u514d\u5c06\u590d\u6742\u6570\u636e\u89c6\u4e3a\u7b80\u5355\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u6e05\u6670\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u8ba1\u7b97\u4eba\u6587\u9886\u57df\u9700\u8981\u66f4\u591a\u7684\u7406\u8bba\u5316\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u8ba4\u8bc6\u8bba\u548c\u89e3\u91ca\u7684\u6e05\u6670\u5ea6\uff0c\u4ece\u800c\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u6210\u719f\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u5c06\u8ba1\u7b97\u4eba\u6587\u9886\u57df\u7684\u5efa\u6a21\u5de5\u4f5c\u89c6\u4e3a\u4e00\u79cd\u7ffb\u8bd1\u5de5\u4f5c\uff0c\u5373\u4ece\u6587\u5316\u3001\u8bed\u8a00\u9886\u57df\u5230\u8ba1\u7b97\u3001\u6570\u5b66\u9886\u57df\uff0c\u518d\u53cd\u5411\u7ffb\u8bd1\u3002\u8bba\u6587\u5f3a\u8c03\u4e86\u7406\u8bba\u5316\u5728\u786e\u4fdd\u5185\u90e8\u4e00\u81f4\u6027\u3001\u907f\u514d\u7ec6\u5fae\u4f46\u540e\u679c\u4e25\u91cd\u7684\u7ffb\u8bd1\u9519\u8bef\u4ee5\u53ca\u4fc3\u8fdb\u89e3\u91ca\u900f\u660e\u5ea6\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002", "result": "\u8bba\u6587\u6307\u51fa\u4e86\u7406\u8bba\u5316\u4e0d\u8db3\u5bfc\u81f4\u7684\u4e3b\u8981\u95ee\u9898\uff0c\u5373\u5728\u8bc4\u4f30\u7b49\u5efa\u6a21\u5b9e\u8df5\u4e2d\uff0c\u5c06\u5177\u6709\u7b26\u53f7\u590d\u6742\u6027\u7684\u6570\u636e\u89c6\u4e3a\u7b80\u5355\u6570\u636e\uff0c\u4ece\u800c\u5728\u8ba4\u8bc6\u8bba\u4e0a\u9020\u6210\u4e86\u201c\u7ffb\u8bd1\u9519\u8bef\u201d\u3002\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u51e0\u9879\u5efa\u8bae\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5728\u5de5\u4f5c\u4e2d\u66f4\u597d\u5730\u89e3\u51b3\u8fd9\u4e9b\u8ba4\u8bc6\u8bba\u95ee\u9898\u3002", "conclusion": "\u672c\u7bc7\u8bba\u6587\u7684\u7ed3\u8bba\u662f\uff0c\u5e94\u52a0\u5f3a\u8ba1\u7b97\u4eba\u6587\u9886\u57df\u4e2d\u65b9\u6cd5\u7684\u7406\u8bba\u5316\uff0c\u4ee5\u63d0\u9ad8\u8ba4\u8bc6\u8bba\u548c\u89e3\u91ca\u7684\u6e05\u6670\u5ea6\uff0c\u4ece\u800c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6210\u719f\u3002\u6211\u4eec\u63d0\u51fa\u5c06\u5efa\u6a21\u5de5\u4f5c\u89c6\u4e3a\u4e00\u79cd\u4ece\u6587\u5316\u3001\u8bed\u8a00\u9886\u57df\u5230\u8ba1\u7b97\u3001\u6570\u5b66\u9886\u57df\u518d\u8fd4\u56de\u7684\u7ffb\u8bd1\u5de5\u4f5c\u3002"}}
{"id": "2508.00116", "categories": ["cs.AI", "H.4.1; I.2.1"], "pdf": "https://arxiv.org/pdf/2508.00116", "abs": "https://arxiv.org/abs/2508.00116", "authors": ["Wil M. P. van der Aalst"], "title": "No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence", "comment": "10 pages, 4 figures, preprint keynote paper of the seventh\n  International Conference on Intelligent and Fuzzy Systems (INFUS 2025)", "summary": "The uptake of Artificial Intelligence (AI) impacts the way we work, interact,\ndo business, and conduct research. However, organizations struggle to apply AI\nsuccessfully in industrial settings where the focus is on end-to-end\noperational processes. Here, we consider generative, predictive, and\nprescriptive AI and elaborate on the challenges of diagnosing and improving\nsuch processes. We show that AI needs to be grounded using Object-Centric\nProcess Mining (OCPM). Process-related data are structured and\norganization-specific and, unlike text, processes are often highly dynamic.\nOCPM is the missing link connecting data and processes and enables different\nforms of AI. We use the term Process Intelligence (PI) to refer to the\namalgamation of process-centric data-driven techniques able to deal with a\nvariety of object and event types, enabling AI in an organizational context.\nThis paper explains why AI requires PI to improve operational processes and\nhighlights opportunities for successfully combining OCPM and generative,\npredictive, and prescriptive AI.", "AI": {"tldr": "AI\u5728\u5de5\u4e1a\u8fd0\u8425\u4e2d\u843d\u5730\u56f0\u96be\uff0c\u9700\u8981\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6d41\u7a0b\u6316\u6398\uff08OCPM\uff09\u548c\u6d41\u7a0b\u667a\u80fd\uff08PI\uff09\u6765\u5b9e\u73b0\u6210\u529f\u5e94\u7528\u3002", "motivation": "\u7ec4\u7ec7\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u6210\u529f\u5e94\u7528AI\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7aef\u5230\u7aef\u8fd0\u8425\u6d41\u7a0b\u65b9\u9762\u3002AI\u9700\u8981\u4e0e\u6d41\u7a0b\u6570\u636e\u76f8\u7ed3\u5408\u624d\u80fd\u6709\u6548\u6539\u8fdb\u8fd9\u4e9b\u6d41\u7a0b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5c06AI\u5e94\u7528\u4e8e\u5de5\u4e1a\u8fd0\u8425\u6d41\u7a0b\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u751f\u6210\u5f0f\u3001\u9884\u6d4b\u5f0f\u548c\u89c4\u8303\u5f0fAI\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bca\u65ad\u548c\u6539\u8fdb\u8fd9\u4e9b\u6d41\u7a0b\u7684\u6311\u6218\u3002\u6587\u7ae0\u6307\u51fa\uff0cAI\u9700\u8981\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6d41\u7a0b\u6316\u6398\uff08OCPM\uff09\u4e3a\u57fa\u7840\uff0c\u5e76\u5f15\u5165\u4e86\u6d41\u7a0b\u667a\u80fd\uff08PI\uff09\u7684\u6982\u5ff5\uff0c\u5373\u5904\u7406\u5404\u79cd\u5bf9\u8c61\u548c\u4e8b\u4ef6\u7c7b\u578b\u7684\u6570\u636e\u9a71\u52a8\u6280\u672f\uff0c\u4ee5\u5b9e\u73b0\u7ec4\u7ec7\u73af\u5883\u4e2d\u7684AI\u5e94\u7528\u3002", "result": "\u901a\u8fc7\u7ed3\u5408OCPM\u548c\u751f\u6210\u5f0f\u3001\u9884\u6d4b\u5f0f\u53ca\u89c4\u8303\u5f0fAI\uff0c\u53ef\u4ee5\u6210\u529f\u6539\u8fdb\u7ec4\u7ec7\u8fd0\u8425\u6d41\u7a0b\uff0c\u5b9e\u73b0\u6d41\u7a0b\u667a\u80fd\uff08PI\uff09\u3002", "conclusion": "AI\u9700\u8981\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6d41\u7a0b\u6316\u6398\uff08OCPM\uff09\u4e3a\u57fa\u7840\uff0c\u624d\u80fd\u6210\u529f\u5e94\u7528\u4e8e\u5de5\u4e1a\u9886\u57df\u7684\u7aef\u5230\u7aef\u8fd0\u8425\u6d41\u7a0b\u3002\u672c\u6587\u9610\u8ff0\u4e86\u8bca\u65ad\u548c\u6539\u8fdb\u8fd9\u4e9b\u6d41\u7a0b\u7684\u6311\u6218\uff0c\u5e76\u5f3a\u8c03\u4e86OCPM\u4e0e\u751f\u6210\u5f0f\u3001\u9884\u6d4b\u5f0f\u548c\u89c4\u8303\u5f0fAI\u76f8\u7ed3\u5408\u7684\u673a\u9047\u3002"}}
{"id": "2508.00428", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.00428", "abs": "https://arxiv.org/abs/2508.00428", "authors": ["Nan Xiang", "Tianyi Liang", "Haiwen Huang", "Shiqi Jiang", "Hao Huang", "Yifei Huang", "Liangyu Chen", "Changbo Wang", "Chenhui Li"], "title": "Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation", "comment": "IEEE VIS VAST 2025 ACM 2012 CCS - Human-centered computing,\n  Visualization, Visualization design and evaluation methods", "summary": "Text-to-3D (T23D) generation has transformed digital content creation, yet\nremains bottlenecked by blind trial-and-error prompting processes that yield\nunpredictable results. While visual prompt engineering has advanced in\ntext-to-image domains, its application to 3D generation presents unique\nchallenges requiring multi-view consistency evaluation and spatial\nunderstanding. We present Sel3DCraft, a visual prompt engineering system for\nT23D that transforms unstructured exploration into a guided visual process. Our\napproach introduces three key innovations: a dual-branch structure combining\nretrieval and generation for diverse candidate exploration; a multi-view hybrid\nscoring approach that leverages MLLMs with innovative high-level metrics to\nassess 3D models with human-expert consistency; and a prompt-driven visual\nanalytics suite that enables intuitive defect identification and refinement.\nExtensive testing and user studies demonstrate that Sel3DCraft surpasses other\nT23D systems in supporting creativity for designers.", "AI": {"tldr": "Sel3DCraft is a visual prompt engineering system for Text-to-3D generation that addresses the challenges of multi-view consistency and spatial understanding, improving upon traditional trial-and-error methods with a novel approach combining retrieval, generation, and advanced scoring techniques.", "motivation": "Text-to-3D (T23D) generation is bottlenecked by blind trial-and-error prompting processes that yield unpredictable results. Visual prompt engineering for 3D generation presents unique challenges requiring multi-view consistency evaluation and spatial understanding.", "method": "Sel3DCraft uses a dual-branch structure combining retrieval and generation for diverse candidate exploration, a multi-view hybrid scoring approach that leverages MLLMs with innovative high-level metrics to assess 3D models with human-expert consistency, and a prompt-driven visual analytics suite that enables intuitive defect identification and refinement.", "result": "Extensive testing and user studies demonstrate that Sel3DCraft surpasses other T23D systems in supporting creativity for designers.", "conclusion": "Sel3DCraft surpasses other T23D systems in supporting creativity for designers."}}
{"id": "2508.00025", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00025", "abs": "https://arxiv.org/abs/2508.00025", "authors": ["Michael Davidovich"], "title": "Casimir force between two dielectric layers: Van Kampen approach", "comment": "21 pages, 3 figures", "summary": "The Van Kampen method is used to calculate the Casimir force for two\ndielectric layers. Several terms of Lorentz oscillators are used in the\npermittivity model. A conductive dielectric (metal) with the Drude model is\nconsidered as a special case. The dependence of strength on thickness has a\ncomplex character with saturation at thicknesses of the order of 10 nm. At low\nthickness, the force density is proportional to the square of the thickness,\nbut this is the case at low thicknesses, when the continuum model is no longer\napplicable. The correspondence between the method of the Casimir model and the\nLorentz model is shown, as well as its applicability for an arbitrary\nconfiguration of layers and for a finite temperature.", "AI": {"tldr": "Vankampen \u65b9\u6cd5\u7528\u4e8e\u8ba1\u7b97\u4ecb\u7535\u5c42 Casimir \u529b\uff0c\u8003\u8651\u4e86\u6d1b\u4f26\u5179\u6a21\u578b\u548c Drude \u6a21\u578b\uff0c\u53d1\u73b0\u4e86\u529b\u4e0e\u539a\u5ea6\u7684\u590d\u6742\u5173\u7cfb\uff0c\u5e76\u5728\u4efb\u610f\u5c42\u914d\u7f6e\u548c\u6709\u9650\u6e29\u5ea6\u4e0b\u5747\u9002\u7528\u3002", "motivation": "\u7814\u7a76 Vankampen \u65b9\u6cd5\u5728\u8ba1\u7b97 Casimir \u529b\u65f6\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u8003\u8651\u4ecb\u7535\u5c42\u548c\u5bfc\u7535\u4ecb\u8d28\uff08\u5982\u91d1\u5c5e\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u4e0d\u540c\u539a\u5ea6\u548c\u6e29\u5ea6\u4e0b\u7684\u884c\u4e3a\u3002", "method": "\u4f7f\u7528 Van Kampen \u65b9\u6cd5\u8ba1\u7b97\u4e24\u4e2a\u4ecb\u7535\u5c42\u4e4b\u95f4\u7684 Casimir \u529b\uff0c\u5e76\u91c7\u7528\u6d1b\u4f26\u5179\u632f\u5b50\u6a21\u578b\u6765\u63cf\u8ff0\u4ecb\u7535\u5e38\u6570\u3002", "result": "Casimir \u529b\u7684\u5f3a\u5ea6\u4e0e\u539a\u5ea6\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728 10 \u7eb3\u7c73\u5de6\u53f3\u539a\u5ea6\u65f6\u51fa\u73b0\u9971\u548c\u3002\u5728\u4f4e\u539a\u5ea6\u60c5\u51b5\u4e0b\uff0c\u529b\u5bc6\u5ea6\u4e0e\u539a\u5ea6\u7684\u5e73\u65b9\u6210\u6b63\u6bd4\uff0c\u4f46\u6b64\u65f6\u8fde\u7eed\u4ecb\u8d28\u6a21\u578b\u4e0d\u518d\u9002\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86 Van Kampen \u65b9\u6cd5\u5728\u8ba1\u7b97\u4ecb\u7535\u5c42\u4e4b\u95f4\u7684 Casimir \u529b\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u4e0e\u6d1b\u4f26\u5179\u6a21\u578b\u548c Casimir \u6a21\u578b\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u8bc1\u660e\u4e86\u5176\u5bf9\u4efb\u610f\u5c42\u914d\u7f6e\u548c\u6709\u9650\u6e29\u5ea6\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.00151", "categories": ["cs.LO", "cs.GT", "03B70, 91A44, 91A05, 68Q10", "F.1.1; F.4.1; F.3.1; I.2.3"], "pdf": "https://arxiv.org/pdf/2508.00151", "abs": "https://arxiv.org/abs/2508.00151", "authors": ["Faruk Alpay", "Hamdi Al Alakkad"], "title": "Ordinal Folding Index: A Computable Metric for Self-Referential Semantics", "comment": "13 pages, 2 figures. Introduces the Ordinal Folding Index, a\n  computable ordinal depth metric for self referential statements that unifies\n  fixed point logic with infinite game theory", "summary": "The Ordinal Folding Index (OFI) is a new, fully computable yard-stick that\nmeasures how many rounds of self-reference a statement, protocol or position\nmust unfold before its truth or outcome stabilises. By turning this abstract\n'fold-back' depth into a single ordinal number, OFI forges a direct link\nbetween areas that are usually studied in isolation: the closure stages of\nfixed-point logics, the time-to-win values of infinite parity games, and the\nordinal progressions that calibrate the strength of formal theories. We prove\nthat OFI refines all classical game-theoretic and logical metrics while\nremaining algorithmically enumerable, supply a polynomial-time approximation\nscheme on finite arenas, and show how the index coincides exactly with the\nlength of the shortest winning strategy in the associated evaluation game.\nAlongside the theory we outline five open problems from the completeness of the\ncomputable-ordinal spectrum to the possibility of 'compressing' deep\nself-reference that chart a research programme at the intersection of\ncomputer-aided logic, algorithmic game theory and ordinal analysis. OFI thus\ninvites game theorists and logicians alike to view infinite play, transfinite\ninduction and reflective reasoning through a single, intuitive lens, opening\ncommon ground for techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e8f\u6570\u6298\u53e0\u6307\u6570 (OFI)\uff0c\u4e00\u79cd\u8861\u91cf\u81ea\u6211\u6307\u6d89\u7a33\u5b9a\u6027\u7684\u65b0\u5e8f\u6570\u6307\u6807\uff0c\u5b83\u7edf\u4e00\u4e86\u903b\u8f91\u3001\u535a\u5f08\u8bba\u548c\u7406\u8bba\u5f3a\u5ea6\uff0c\u5177\u6709\u53ef\u8ba1\u7b97\u6027\u548c\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u65b9\u6848\uff0c\u5e76\u5f00\u542f\u4e86\u8de8\u9886\u57df\u7814\u7a76\u7684\u65b0\u65b9\u5411\u3002", "motivation": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5e8f\u6570\u6298\u53e0\u6307\u6570 (OFI) \u7684\u65b0\u6307\u6807\uff0c\u65e8\u5728\u8861\u91cf\u9648\u8ff0\u3001\u534f\u8bae\u6216\u7acb\u573a\u5728\u8d8b\u4e8e\u7a33\u5b9a\u4e4b\u524d\u6240\u7ecf\u5386\u7684\u81ea\u6211\u6307\u6d89\u8f6e\u6570\u3002\u8be5\u6307\u6807\u65e8\u5728\u8fde\u63a5\u901a\u5e38\u72ec\u7acb\u7814\u7a76\u7684\u9886\u57df\uff0c\u5982\u4e0d\u52a8\u70b9\u903b\u8f91\u7684\u95ed\u5305\u9636\u6bb5\u3001\u65e0\u9650\u5076\u6570\u6e38\u620f\u7684\u83b7\u80dc\u65f6\u95f4\u4ee5\u53ca\u5f62\u5f0f\u7406\u8bba\u7684\u5e8f\u6570\u8fdb\u5c55\u3002", "method": "OFI \u901a\u8fc7\u5c06\u201c\u6298\u53e0\u56de\u201d\u6df1\u5ea6\u8f6c\u5316\u4e3a\u5e8f\u6570\u6765\u8861\u91cf\u81ea\u6211\u6307\u6d89\u7684\u8f6e\u6570\u3002\u7814\u7a76\u8bc1\u660e\u4e86 OFI \u7cbe\u70bc\u4e86\u6240\u6709\u7ecf\u5178\u535a\u5f08\u8bba\u548c\u903b\u8f91\u5ea6\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u7b97\u6cd5\u53ef\u679a\u4e3e\u6027\uff0c\u5e76\u5728\u6709\u9650\u573a\u4e0a\u63d0\u4f9b\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u6307\u6807\u5982\u4f55\u4e0e\u76f8\u5173\u8bc4\u4f30\u535a\u5f08\u4e2d\u6700\u77ed\u83b7\u80dc\u7b56\u7565\u7684\u957f\u5ea6\u7cbe\u786e\u5339\u914d\u3002", "result": "OFI \u7cbe\u70bc\u4e86\u6240\u6709\u7ecf\u5178\u7684\u535a\u5f08\u8bba\u548c\u903b\u8f91\u5ea6\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u7b97\u6cd5\u53ef\u679a\u4e3e\u6027\u3002\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u9650\u573a\u4e0a\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u65b9\u6848\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u6307\u6807\u4e0e\u76f8\u5173\u8bc4\u4f30\u535a\u5f08\u4e2d\u6700\u77ed\u83b7\u80dc\u7b56\u7565\u7684\u957f\u5ea6\u7cbe\u786e\u5339\u914d\u3002", "conclusion": "OFI \u662f\u4e00\u79cd\u65b0\u9896\u4e14\u5b8c\u5168\u53ef\u8ba1\u7b97\u7684\u6307\u6807\uff0c\u7528\u4e8e\u8861\u91cf\u9648\u8ff0\u3001\u534f\u8bae\u6216\u7acb\u573a\u5728\u8d8b\u4e8e\u7a33\u5b9a\u4e4b\u524d\u5fc5\u987b\u7ecf\u5386\u591a\u5c11\u8f6e\u81ea\u6211\u6307\u6d89\u3002\u901a\u8fc7\u5c06\u62bd\u8c61\u7684\u201c\u6298\u53e0\u56de\u201d\u6df1\u5ea6\u8f6c\u5316\u4e3a\u5355\u4e2a\u5e8f\u6570\uff0cOFI \u5728\u901a\u5e38\u5b64\u7acb\u7814\u7a76\u7684\u9886\u57df\u4e4b\u95f4\u5efa\u7acb\u4e86\u76f4\u63a5\u8054\u7cfb\uff1a\u4e0d\u52a8\u70b9\u903b\u8f91\u7684\u95ed\u5305\u9636\u6bb5\u3001\u65e0\u9650\u5076\u6570\u6e38\u620f\u7684\u83b7\u80dc\u65f6\u95f4\u503c\u4ee5\u53ca\u6821\u51c6\u5f62\u5f0f\u7406\u8bba\u5f3a\u5ea6\u7684\u5e8f\u6570\u8fdb\u5c55\u3002"}}
{"id": "2508.00188", "categories": ["eess.SY", "cs.GT", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00188", "abs": "https://arxiv.org/abs/2508.00188", "authors": ["Renyan Sun", "Ashutosh Nayyar"], "title": "Optimal Messaging Strategy for Incentivizing Agents in Dynamic Systems", "comment": "We submitted a full paper to IEEE TAC for review. A preliminary\n  version of this paper is scheduled to be presented at IEEE CDC conference in\n  December 2025", "summary": "We consider a finite-horizon discrete-time dynamic system jointly controlled\nby a designer and one or more agents, where the designer can influence the\nagents' actions through selective information disclosure. At each time step,\nthe designer sends a message to the agent(s) from a prespecified message space.\nThe designer may also take an action that directly influences system dynamics\nand rewards. Each agent uses its received message (and its own information) to\nchoose its action. We are interested in the setting where the designer would\nlike to incentivize each agent to play a specific strategy. We consider a\nnotion of incentive compatibility that is based on sequential rationality at\neach realization of the common information between the designer and the\nagent(s). Our objective is to find a messaging and action strategy for the\ndesigner that maximizes its total expected reward while incentivizing each\nagent to follow a prespecified strategy. Under certain assumptions on the\ninformation structure of the problem, we show that an optimal designer strategy\ncan be computed using a backward inductive algorithm that solves a family of\nlinear programs.", "AI": {"tldr": "\u8bbe\u8ba1\u5e08\u901a\u8fc7\u53d1\u9001\u4fe1\u606f\u548c\u91c7\u53d6\u884c\u52a8\u6765\u6fc0\u52b1\u4ee3\u7406\u4eba\uff0c\u5e76\u6700\u5927\u5316\u81ea\u8eab\u56de\u62a5\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u987a\u5e8f\u7406\u6027\u7684\u6fc0\u52b1\u517c\u5bb9\u6027\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u9006\u5411\u5f52\u7eb3\u548c\u7ebf\u6027\u89c4\u5212\u6765\u627e\u5230\u6700\u4f18\u7b56\u7565\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5728\u6709\u9650\u65f6\u95f4\u3001\u79bb\u6563\u65f6\u95f4\u52a8\u6001\u7cfb\u7edf\u4e2d\uff0c\u8bbe\u8ba1\u5e08\u5982\u4f55\u901a\u8fc7\u9009\u62e9\u6027\u4fe1\u606f\u62ab\u9732\u6765\u6fc0\u52b1\u4ee3\u7406\u4eba\u91c7\u53d6\u7279\u5b9a\u7b56\u7565\uff0c\u540c\u65f6\u6700\u5927\u5316\u81ea\u8eab\u56de\u62a5\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u987a\u5e8f\u7406\u6027\u7684\u6fc0\u52b1\u76f8\u5bb9\u6027\u6982\u5ff5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5229\u7528\u9006\u5411\u5f52\u7eb3\u7b97\u6cd5\u89e3\u51b3\u4e00\u7cfb\u5217\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u7684\u8ba1\u7b97\u65b9\u6cd5\u6765\u5bfb\u627e\u6700\u4f18\u7b56\u7565\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u4fe1\u606f\u7ed3\u6784\u4e0b\uff0c\u5b58\u5728\u6700\u4f18\u7684\u8bbe\u8ba1\u8005\u7b56\u7565\uff0c\u5e76\u4e14\u8be5\u7b56\u7565\u53ef\u4ee5\u901a\u8fc7\u9006\u5411\u5f52\u7eb3\u7b97\u6cd5\u548c\u7ebf\u6027\u89c4\u5212\u5f97\u5230\u3002", "conclusion": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u987a\u5e8f\u7406\u6027\u7684\u6fc0\u52b1\u76f8\u5bb9\u6027\u6982\u5ff5\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u7279\u5b9a\u4fe1\u606f\u7ed3\u6784\u4e0b\uff0c\u8bbe\u8ba1\u8005\u53ef\u4ee5\u901a\u8fc7\u89e3\u51b3\u4e00\u7cfb\u5217\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u6765\u627e\u5230\u6700\u4f18\u4fe1\u606f\u62ab\u9732\u548c\u884c\u52a8\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u5176\u603b\u671f\u671b\u56de\u62a5\u5e76\u6fc0\u52b1\u4ee3\u7406\u4eba\u9075\u5faa\u7279\u5b9a\u7b56\u7565\u3002"}}
{"id": "2508.00401", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.00401", "abs": "https://arxiv.org/abs/2508.00401", "authors": ["Riddhi J. Pitliya", "Ozan Catal", "Toon Van de Maele", "Corrado Pezzato", "Tim Verbelen"], "title": "Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation", "comment": null, "summary": "We present a novel approach to multi-agent cooperation by implementing theory\nof mind (ToM) within active inference. ToM - the ability to understand that\nothers can have differing knowledge and goals - enables agents to reason about\nothers' beliefs while planning their own actions. Unlike previous active\ninference approaches to multi-agent cooperation, our method neither relies on\ntask-specific shared generative models nor requires explicit communication,\nwhile being generalisable. In our framework, the ToM-equipped agent maintains\ndistinct representations of its own and others' beliefs and goals. We extend\nthe sophisticated inference tree-based planning algorithm to systematically\nexplore joint policy spaces through recursive reasoning. Our approach is\nevaluated through collision avoidance and foraging task simulations. Results\ndemonstrate that ToM-equipped agents cooperate better compared to non-ToM\ncounterparts by being able to avoid collisions and reduce redundant efforts.\nCrucially, ToM agents accomplish this by inferring others' beliefs solely from\nobservable behaviour. This work advances practical applications in artificial\nintelligence while providing computational insights into ToM.", "AI": {"tldr": "A new method uses 'theory of mind' in active inference for better multi-agent cooperation without needing shared models or communication. Agents learn to understand others' beliefs from actions, improving task performance in simulations.", "motivation": "To enable multi-agent cooperation by allowing agents to reason about others' beliefs and goals, without relying on task-specific shared generative models or explicit communication.", "method": "This paper implements theory of mind (ToM) within active inference. It extends the inference tree-based planning algorithm to explore joint policy spaces through recursive reasoning. The ToM-equipped agent maintains distinct representations of its own and others' beliefs and goals.", "result": "ToM-equipped agents demonstrated improved cooperation in collision avoidance and foraging simulations compared to non-ToM agents, by inferring others' beliefs from observable behavior.", "conclusion": "ToM-equipped agents cooperate better than non-ToM counterparts by inferring others' beliefs solely from observable behavior, leading to collision avoidance and reduced redundant efforts. This work advances practical AI applications and provides computational insights into ToM."}}
{"id": "2309.03382", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2309.03382", "abs": "https://arxiv.org/abs/2309.03382", "authors": ["Jinsong Xu", "Weiwei Lin", "Jiaming He", "J. -S. Zhou", "Danru Qu", "Ssu-Yen Huang", "C. L. Chien"], "title": "Vector spin Seebeck effect and spin swapping effect in antiferromagnetic insulators with non-collinear spin structure", "comment": null, "summary": "Antiferromagnets (AFs) are prospective for next-generation high-density and\nhigh-speed spintronic applications due to their negligible stray field and\nultrafast spin dynamics, notwithstanding the challenges in detecting and\nmanipulating AF order with no magnetization (M = 0). Among the AFs,\nnon-collinear AFs are of particular interest because of their unique properties\narising from the non-collinear spin structure and the small magnetization M. In\nthis work, we describe the recently observed vector spin Seebeck effect in\nnon-collinear LuFeO$_3$, where the magneto-thermovoltage under an in-plane\ntemperature gradient, not previously observed, is consistent with the predicted\nspin swapping effect. Our results shed light on the importance of the\nnon-collinear spin structure in the emerging spin phenomena in non-collinear\nAFs and offer a new class of materials for AF spintronics and spin\ncaloritronics.", "AI": {"tldr": "\u5411\u91cf\u81ea\u65cb\u585e\u8d1d\u514b\u6548\u5e94\u5728\u975e\u5171\u7ebf\u53cd\u94c1\u78c1\u4f53LuFeO$_{3}$\u4e2d\u7684\u89c2\u6d4b\uff0c\u8bc1\u660e\u4e86\u975e\u5171\u7ebf\u81ea\u65cb\u7ed3\u6784\u5728\u5176\u4e2d\u8d77\u91cd\u8981\u4f5c\u7528\uff0c\u4e3a\u53cd\u94c1\u78c1\u6750\u6599\u5728\u81ea\u65cb\u7535\u5b50\u5b66\u548c\u81ea\u65cb\u70ed\u7535\u5b50\u5b66\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22\u5177\u6709\u9ad8\u5bc6\u5ea6\u548c\u9ad8\u901f\u5ea6\u6f5c\u529b\u7684\u53cd\u94c1\u78c1\u6750\u6599\u5728\u81ea\u65cb\u7535\u5b50\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u89e3\u51b3\u53cd\u94c1\u78c1\u4f53\uff08M=0\uff09\u5728\u63a2\u6d4b\u548c\u64cd\u7eb5\u65b9\u9762\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u7814\u7a76\u975e\u5171\u7ebf\u53cd\u94c1\u78c1\u4f53\u7684\u72ec\u7279\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u4e86\u975e\u5171\u7ebf\u53cd\u94c1\u78c1\u6750\u6599LuFeO$_{3}$\u7684\u5411\u91cf\u81ea\u65cb\u585e\u8d1d\u514b\u6548\u5e94\uff0c\u5229\u7528\u4e86\u5e73\u9762\u5185\u6e29\u5ea6\u68af\u5ea6\u4ea7\u751f\u4e86\u6b64\u524d\u672a\u89c2\u5bdf\u5230\u7684\u78c1\u70ed\u7535\u538b\uff0c\u5e76\u5c06\u5176\u4e0e\u9884\u6d4b\u7684\u81ea\u65cb\u4ea4\u6362\u6548\u5e94\u76f8\u5173\u8054\u3002", "result": "\u5728\u975e\u5171\u7ebf\u53cd\u94c1\u78c1\u6750\u6599LuFeO$_{3}$\u4e2d\u89c2\u5bdf\u5230\u4e86\u5411\u91cf\u81ea\u65cb\u585e\u8d1d\u514b\u6548\u5e94\uff0c\u5e76\u4e14\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u9884\u6d4b\u7684\u81ea\u65cb\u4ea4\u6362\u6548\u5e94\u4e00\u81f4\uff0c\u8bc1\u660e\u4e86\u975e\u5171\u7ebf\u81ea\u65cb\u7ed3\u6784\u5728\u5176\u4e2d\u8d77\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u975e\u5171\u7ebf\u53cd\u94c1\u78c1\u4f53\u4e2d\u65b0\u9896\u7684\u81ea\u65cb\u73b0\u8c61\uff0c\u7279\u522b\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u78c1\u7574\u58c1\u8fd0\u52a8\u5f15\u8d77\u78c1\u71b5\u53d8\u7684\u65b0\u673a\u5236\uff0c\u4e3a\u53cd\u94c1\u78c1\u6750\u6599\u5728\u81ea\u65cb\u7535\u5b50\u5b66\u548c\u81ea\u65cb\u70ed\u7535\u5b50\u5b66\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2508.00446", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.00446", "abs": "https://arxiv.org/abs/2508.00446", "authors": ["Isaac Vorreiter", "Jonathan Y. Huang", "Scott D. Liles", "Joe Hillier", "Ruoyu Li", "Bart Raes", "Stefan Kubicek", "Julien Jussot", "Sofie Beyne", "Clement Godfrin", "Sugandha Sharma", "Danny Wan", "Nard Dumoulin Stuyck", "Will Gilbert", "Chih Hwan Yang", "Andrew S. Dzurak", "Kristiaan De Greve", "Alexander R. Hamilton"], "title": "Precision high-speed quantum logic with holes on a natural silicon foundry platform", "comment": null, "summary": "Silicon spin qubits in gate-defined quantum dots leverage established\nsemiconductor infrastructure and offer a scalable path toward transformative\nquantum technologies. Holes spins in silicon offer compact all-electrical\ncontrol, whilst retaining all the salient features of a quantum dot qubit\narchitecture. However, silicon hole spin qubits are not as advanced as\nelectrons, due to increased susceptibility to disorder and more complex spin\nphysics. Here we demonstrate single-qubit gate fidelities up to 99.8% and a\ntwo-qubit gate quality factor of 240, indicating a physical fidelity limit of\n99.7%. These results represent the highest performance reported in natural\nsilicon to date, made possible by fast qubit control, exchange pulsing, and\nindustrial-grade fabrication. Notably, we achieve these results in a\nnear-identical device as used for highly reproducible, high-fidelity electron\nspin qubits. With isotopic purification and device-level optimisations in the\nfuture, our hole spin qubits are poised to unlock a new operation regime for\nquantum CMOS architectures.", "AI": {"tldr": "\u7845\u57fa\u7a74\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u6027\u80fd\u63a5\u8fd1\u7535\u5b50\u91cf\u5b50\u6bd4\u7279\uff0c\u521b\u4e0b\u81ea\u7136\u7845\u4e2d\u7684\u65b0\u7eaa\u5f55\uff0c\u6709\u671b\u63a8\u52a8\u91cf\u5b50CMOS\u67b6\u6784\u53d1\u5c55\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u7845\u57fa\u7a74\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u76f8\u6bd4\u7535\u5b50\u91cf\u5b50\u6bd4\u7279\u5728\u6297\u65e0\u5e8f\u6027\u548c\u590d\u6742\u81ea\u65cb\u7269\u7406\u65b9\u9762\u5b58\u5728\u7684\u6311\u6218\uff0c\u5e76\u5229\u7528\u5176\u7d27\u51d1\u7684\u5168\u7535\u5b66\u63a7\u5236\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u5feb\u901f\u91cf\u5b50\u6bd4\u7279\u63a7\u5236\u3001\u4ea4\u6362\u8109\u51b2\u548c\u5de5\u4e1a\u7ea7\u5236\u9020\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u7845\u57fa\u7a74\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u64cd\u4f5c\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u8fbe99.8%\u7684\u5355\u91cf\u5b50\u6bd4\u7279\u95e8\u4fdd\u771f\u5ea6\u548c240\u7684\u53cc\u91cf\u5b50\u6bd4\u7279\u95e8\u54c1\u8d28\u56e0\u6570\uff0c\u8868\u660e\u7269\u7406\u4fdd\u771f\u5ea6\u6781\u9650\u4e3a99.7%\u3002", "conclusion": "\u7845\u57fa\u7a74\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u7684\u6027\u80fd\u5df2\u63a5\u8fd1\u7535\u5b50\u91cf\u5b50\u6bd4\u7279\uff0c\u5728\u81ea\u7136\u7845\u4e2d\u8fbe\u5230\u4e86\u6700\u9ad8\u7684\u5355\u91cf\u5b50\u6bd4\u7279\u95e8\u4fdd\u771f\u5ea6\uff0899.8%\uff09\u548c\u9ad8\u53cc\u91cf\u5b50\u6bd4\u7279\u95e8\u54c1\u8d28\u56e0\u6570\uff08240\uff09\uff0c\u9884\u793a\u7740\u5176\u5728\u91cf\u5b50CMOS\u67b6\u6784\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u5408\u4e86\u540c\u4f4d\u7d20\u7eaf\u5316\u548c\u8bbe\u5907\u4f18\u5316\u540e\u3002"}}
{"id": "2508.00409", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00409", "abs": "https://arxiv.org/abs/2508.00409", "authors": ["Mohammad Soleymani", "Ignacio Santamaria", "Eduard Jorswieck", "Robert Schober", "Lajos Hanzo"], "title": "STAR-RIS-aided RSMA for the URLLC multi-user MIMO Downlink", "comment": "Accepted at 28th International Workshop on Smart Antennas 2025", "summary": "Rate splitting multiple access (RSMA) is intrinsically amalgamated with\nsimultaneously transmitting and reflecting (STAR) reconfigurable intelligent\nsurfaces (RIS) to enhance energy efficiency (EE) of the finite block length\n(FBL) multiple-input multiple-output (MIMO) downlink. An alternating\noptimization-based algorithm is proposed to jointly optimize the transmit\nbeamforming matrices, STAR-RIS configurations, and rate-splitting parameters.\nSTAR-RIS attains 360-degree full-plane coverage, while RSMA provides a\nprominent gain by efficiently managing interference. Numerical results reveal a\nstrong synergy between RSMA and STAR-RIS, demonstreating significant EE gains\nover reflective RIS and spatial division multiple access (SDMA).", "AI": {"tldr": "RSMA\u4e0eSTAR-RIS\u7684\u7ed3\u5408\u5728MIMO\u4e0b\u884c\u94fe\u8def\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u80fd\u6e90\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6709\u9650\u5757\u957fMIMO\u4e0b\u884c\u94fe\u8def\u7684\u80fd\u6e90\u6548\u7387\uff0c\u5c06RSMA\u4e0eSTAR-RIS\u76f8\u7ed3\u5408\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u66ff\u4f18\u5316\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u8054\u5408\u4f18\u5316\u6ce2\u675f\u5f62\u6210\u77e9\u9635\u3001STAR-RIS\u914d\u7f6e\u548c\u901f\u7387\u62c6\u5206\u53c2\u6570\u3002", "result": "STAR-RIS\u5b9e\u73b0\u4e86360\u5ea6\u5168\u5e73\u9762\u8986\u76d6\uff0cRSMA\u901a\u8fc7\u6709\u6548\u7ba1\u7406\u5e72\u6270\u83b7\u5f97\u4e86\u663e\u8457\u589e\u76ca\u3002\u6570\u503c\u7ed3\u679c\u8868\u660eRSMA\u548cSTAR-RIS\u4e4b\u95f4\u5b58\u5728\u5f88\u5f3a\u7684\u534f\u540c\u4f5c\u7528\uff0c\u4e0e\u53cd\u5c04RIS\u548cSDMA\u76f8\u6bd4\uff0c\u80fd\u6e90\u6548\u7387\u5f97\u5230\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RSMA\u548cSTAR-RIS\u7684\u7ed3\u5408\u663e\u8457\u63d0\u9ad8\u4e86MIMO\u4e0b\u884c\u94fe\u8def\u7684\u80fd\u6e90\u6548\u7387\uff0c\u4e0e\u53cd\u5c04RIS\u548cSDMA\u76f8\u6bd4\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2508.00288", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00288", "abs": "https://arxiv.org/abs/2508.00288", "authors": ["Jianqiang Xiao", "Yuexuan Sun", "Yixin Shao", "Boxi Gan", "Rongqiang Liu", "Yanjing Wu", "Weili Gua", "Xiang Deng"], "title": "UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents", "comment": "Accepted to ACM MM Dataset Track 2025", "summary": "Aerial navigation is a fundamental yet underexplored capability in embodied\nintelligence, enabling agents to operate in large-scale, unstructured\nenvironments where traditional navigation paradigms fall short. However, most\nexisting research follows the Vision-and-Language Navigation (VLN) paradigm,\nwhich heavily depends on sequential linguistic instructions, limiting its\nscalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark\nfor large-scale Object Goal Navigation (ObjectNav) by aerial agents in\nopen-world environments, where agents operate based on high-level semantic\ngoals without relying on detailed instructional guidance as in VLN. UAV-ON\ncomprises 14 high-fidelity Unreal Engine environments with diverse semantic\nregions and complex spatial layouts, covering urban, natural, and mixed-use\nsettings. It defines 1270 annotated target objects, each characterized by an\ninstance-level instruction that encodes category, physical footprint, and\nvisual descriptors, allowing grounded reasoning. These instructions serve as\nsemantic goals, introducing realistic ambiguity and complex reasoning\nchallenges for aerial agents. To evaluate the benchmark, we implement several\nbaseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that\nintegrates instruction semantics with egocentric observations for long-horizon,\ngoal-directed exploration. Empirical results show that all baselines struggle\nin this setting, highlighting the compounded challenges of aerial navigation\nand semantic goal grounding. UAV-ON aims to advance research on scalable UAV\nautonomy driven by semantic goal descriptions in complex real-world\nenvironments.", "AI": {"tldr": "UAV-ON\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u65e0\u4eba\u673a\u7269\u4f53\u5bfc\u822a\u57fa\u51c6\uff0c\u7528\u4e8e\u5f00\u653e\u4e16\u754c\u73af\u5883\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u6311\u6218\uff0c\u4ee5\u63a8\u52a8\u53ef\u6269\u5c55\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u6027\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u5927\u591a\u9075\u5faaVision-and-Language Navigation (VLN)\u8303\u4f8b\uff0c\u8be5\u8303\u4f8b\u4e25\u91cd\u4f9d\u8d56\u4e8e\u8bed\u8a00\u6307\u4ee4\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u81ea\u4e3b\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86UAV-ON\u3002", "method": "\u63d0\u51faUAV-ON\u57fa\u51c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u822a\u7a7a\u5668\u7684}(\\textbf{ObjectNav})\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u5176\u4e2d{\\textbf{\u822a\u7a7a\u5668}}\u6839\u636e\u9ad8\u7ea7\u8bed\u4e49\u76ee\u6807\u8fd0\u884c\uff0c\u800c\u4e0d\u4f9d\u8d56\u4e8eVLN\u4e2d\u7684\u8be6\u7ec6\u6307\u5bfc\u3002UAV-ON\u5305\u542b14\u4e2a\u9ad8\u4fdd\u771fUnreal Engine\u73af\u5883\uff0c\u5177\u6709\u591a\u6837\u5316\u7684\u8bed\u4e49\u533a\u57df\u548c\u590d\u6742\u7a7a\u95f4\u5e03\u5c40\uff0c\u6db5\u76d6\u57ce\u5e02\u3001\u81ea\u7136\u548c\u6df7\u5408\u7528\u9014\u73af\u5883\u3002\u5b83\u5b9a\u4e49\u4e861270\u4e2a\u5e26\u6ce8\u91ca\u7684\u76ee\u6807\u5bf9\u8c61\uff0c\u6bcf\u4e2a\u5bf9\u8c61\u90fd\u5177\u6709\u5b9e\u4f8b\u7ea7\u6307\u4ee4\uff0c\u8be5\u6307\u4ee4\u7f16\u7801\u7c7b\u522b\u3001\u7269\u7406\u8db3\u8ff9\u548c\u89c6\u89c9\u63cf\u8ff0\u7b26\uff0c\u4ee5\u5b9e\u73b0\u57fa\u7840\u63a8\u7406\u3002\u4e3a\u4e86\u8bc4\u4f30\u57fa\u51c6\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u51e0\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u822a\u7a7a\u76ee\u6807\u5bfc\u822a\u4ee3\u7406(AOA)\uff0c\u8fd9\u662f\u4e00\u4e2a\u5c06\u6307\u4ee4\u8bed\u4e49\u4e0e\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c2\u6d4b\u76f8\u7ed3\u5408\u7684\u6a21\u5757\u5316\u7b56\u7565\uff0c\u7528\u4e8e\u957f\u89c6\u57df\u3001\u76ee\u6807\u5bfc\u5411\u7684\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u5728\u6b64\u8bbe\u7f6e\u4e2d\u90fd\u9762\u4e34\u6311\u6218\uff0c\u51f8\u663e\u4e86\u822a\u7a7a\u5bfc\u822a\u548c\u8bed\u4e49\u76ee\u6807\u57fa\u7840\u7684\u590d\u5408\u6311\u6218\u3002", "conclusion": "UAV-ON\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u76ee\u6807\u63cf\u8ff0\u6765\u63a8\u8fdb\u590d\u6742\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u6027\u7814\u7a76\u3002"}}
{"id": "2508.00135", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00135", "abs": "https://arxiv.org/abs/2508.00135", "authors": ["Basna Mohammed Salih Hasan", "Ramadhan J. Mstafa"], "title": "Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images", "comment": "12 pages, 18 figures, 5 tables", "summary": "Gender classification has emerged as a crucial aspect in various fields,\nincluding security, human-machine interaction, surveillance, and advertising.\nNonetheless, the accuracy of this classification can be influenced by factors\nsuch as cosmetics and disguise. Consequently, our study is dedicated to\naddressing this concern by concentrating on gender classification using color\nimages of the periocular region. The periocular region refers to the area\nsurrounding the eye, including the eyelids, eyebrows, and the region between\nthem. It contains valuable visual cues that can be used to extract key features\nfor gender classification. This paper introduces a sophisticated Convolutional\nNeural Network (CNN) model that utilizes color image databases to evaluate the\neffectiveness of the periocular region for gender classification. To validate\nthe model's performance, we conducted tests on two eye datasets, namely CVBL\nand (Female and Male). The recommended architecture achieved an outstanding\naccuracy of 99% on the previously unused CVBL dataset while attaining a\ncommendable accuracy of 96% with a small number of learnable parameters\n(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of\nour proposed model for gender classification using the periocular region, we\nevaluated its performance through an extensive range of metrics and compared it\nwith other state-of-the-art approaches. The results unequivocally demonstrate\nthe efficacy of our model, thereby suggesting its potential for practical\napplication in domains such as security and surveillance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56f4\u773c\u533a\u57df\u548cCNN\u7684\u6027\u522b\u5206\u7c7b\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u514b\u670d\u5316\u5986\u548c\u4f2a\u88c5\u7684\u5f71\u54cd\uff0c\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5316\u5986\u548c\u4f2a\u88c5\u5f71\u54cd\u6027\u522b\u5206\u7c7b\u51c6\u786e\u6027\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u4e13\u6ce8\u4e8e\u5229\u7528\u56f4\u773c\u533a\u57df\u8fdb\u884c\u6027\u522b\u5206\u7c7b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u989c\u8272\u56fe\u50cf\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5bf9\u56f4\u773c\u533a\u57df\u8fdb\u884c\u6027\u522b\u5206\u7c7b\u7684\u6a21\u578b\uff0c\u5e76\u4f7f\u7528CVBL\u548cFemale and Male\u4e24\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u6240\u63d0\u51fa\u7684CNN\u6a21\u578b\u5728CVBL\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8699%\u7684\u51c6\u786e\u7387\uff0c\u5728Female and Male\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u8f83\u5c11\u53ef\u5b66\u4e60\u53c2\u6570\uff087,235,089\uff09\u8fbe\u5230\u4e8696%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u548c\u4e0e\u73b0\u6709\u65b9\u6cd5\u7684\u6bd4\u8f83\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u56f4\u773c\u533a\u57df\u7684\u6027\u522b\u5206\u7c7b\u6a21\u578b\u80fd\u591f\u8fbe\u5230\u5f88\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u5728CVBL\u6570\u636e\u96c6\u4e0a\u8fbe\u523099%\uff0c\u5728Female and Male\u6570\u636e\u96c6\u4e0a\u8fbe\u523096%\uff0c\u8868\u660e\u5176\u5728\u5b89\u5168\u548c\u76d1\u63a7\u7b49\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.00236", "categories": ["cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.00236", "abs": "https://arxiv.org/abs/2508.00236", "authors": ["Yue Li", "Xuanguang Ren", "Xueting Feng", "Lingcheng Kong", "Fengping Luo", "Yang Xu", "Liu Qian", "Yusheng Ye", "Ziqiang Zhao", "Xin Gao", "Jin Zhang"], "title": "Atomic Interface Engineering of Battery Current Collectors via Ion Implantation", "comment": null, "summary": "Atomic interface engineering (AIE) is critical for advancing technologies in\nenergy storage, catalysis, and microelectronics. In anode-less lithium metal\nbatteries (ALLMBs), AIE is essential for controlling interfacial chemistry\ngoverning lithium deposition and solid electrolyte interphase (SEI) formation\non copper current collectors. However, native copper surfaces readily oxidize,\nforming electronically insulating oxides that degrade performance and obscure\nfailure mechanisms. Here, we report a scalable ion implantation strategy to\ncreate an atomically clean and robust copper interface. By implanting copper\nions into commercial foils, we simultaneously remove the native oxide and\nintroduce subsurface vacancy clusters that act as oxygen traps, yielding an\noxidation-resistant and conductive surface. Experimental characterization and\nmultiscale simulations reveal that these engineered vacancies suppress\nreoxidation and guide the formation of an ultrathin Li2O-enriched solid\nelectrolyte interphase. When applied in ALLMBs, the current collectors enable\nuniform lithium deposition, suppress parasitic reactions, and deliver a\nCoulombic efficiency of 99.0% over 400 cycles under lean electrolyte\nconditions. This work presents a generalizable and industry-compatible approach\nfor stabilizing electrochemical interfaces.", "AI": {"tldr": "\u901a\u8fc7\u79bb\u5b50\u6ce8\u5165\u6280\u672f\u5bf9\u94dc\u96c6\u6d41\u4f53\u8fdb\u884c\u539f\u5b50\u754c\u9762\u5de5\u7a0b\uff0c\u89e3\u51b3\u4e86\u94dc\u8868\u9762\u6c27\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u9633\u6781\u9502\u91d1\u5c5e\u7535\u6c60\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u94dc\u96c6\u6d41\u4f53\u8868\u9762\u6613\u6c27\u5316\u5f62\u6210\u7edd\u7f18\u6c27\u5316\u5c42\u7684\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u539f\u5b50\u754c\u9762\u5de5\u7a0b\uff08AIE\uff09\u6765\u63a7\u5236\u9502\u91d1\u5c5e\u7535\u6c60\u4e2d\u7684\u754c\u9762\u5316\u5b66\uff0c\u4ece\u800c\u63d0\u9ad8\u7535\u6c60\u6027\u80fd\u5e76\u63ed\u793a\u5931\u6548\u673a\u5236\u3002", "method": "\u5229\u7528\u79bb\u5b50\u6ce8\u5165\u6280\u672f\uff0c\u5c06\u94dc\u79bb\u5b50\u6ce8\u5165\u5546\u4e1a\u94dc\u7b94\uff0c\u5b9e\u73b0\u539f\u5b50\u7ea7\u6e05\u6d01\u548c\u6c27\u5316\u6297\u6027\u754c\u9762\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8868\u5f81\u548c\u591a\u5c3a\u5ea6\u6a21\u62df\u63ed\u793a\u4e86\u5de5\u7a0b\u5316\u7a7a\u4f4d\u5982\u4f55\u6291\u5236\u518d\u6c27\u5316\u5e76\u5f15\u5bfc\u5f62\u6210\u5bcc\u542bLi2O\u7684\u56fa\u6001\u7535\u89e3\u8d28\u4e2d\u95f4\u5c42\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u5730\u5728\u94dc\u96c6\u6d41\u4f53\u4e0a\u521b\u5efa\u4e86\u4e00\u4e2a\u539f\u5b50\u7ea7\u6e05\u6d01\u3001\u6297\u6c27\u5316\u4e14\u5177\u6709\u5bfc\u7535\u6027\u7684\u754c\u9762\uff0c\u6291\u5236\u4e86\u5bc4\u751f\u53cd\u5e94\uff0c\u5b9e\u73b0\u4e86\u5747\u5300\u7684\u9502\u6c89\u79ef\uff0c\u5e76\u5728\u8d2b\u7535\u89e3\u6db2\u6761\u4ef6\u4e0b\uff0c\u5728400\u6b21\u5faa\u73af\u540e\u4ecd\u4fdd\u630199.0%\u7684\u5e93\u4f26\u6548\u7387\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u63a8\u5e7f\u4e14\u4e0e\u5de5\u4e1a\u517c\u5bb9\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a33\u5b9a\u7535\u5316\u5b66\u754c\u9762\uff0c\u901a\u8fc7\u79bb\u5b50\u6ce8\u5165\u7b56\u7565\u4e3a\u65e0\u9633\u6781\u9502\u91d1\u5c5e\u7535\u6c60\u4e2d\u7684\u94dc\u96c6\u6d41\u4f53\u5e26\u6765\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00387", "categories": ["cs.NE", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00387", "abs": "https://arxiv.org/abs/2508.00387", "authors": ["Zeqi Zheng", "Zizheng Zhu", "Yingchao Yu", "Yanchen Huang", "Changze Lv", "Junfeng Tang", "Zhaofei Yu", "Yaochu Jin"], "title": "STF: Shallow-Level Temporal Feedback to Enhance Spiking Transformers", "comment": "32 pages, 4 figures", "summary": "Transformer-based Spiking Neural Networks (SNNs) suffer from a great\nperformance gap compared to floating-point Artificial Neural Networks (ANNs)\ndue to the binary nature of spike trains. Recent efforts have introduced\ndeep-level feedback loops to transmit high-level semantic information to narrow\nthis gap. However, these designs often span multiple deep layers, resulting in\ncostly feature transformations, higher parameter overhead, increased energy\nconsumption, and longer inference latency. To address this issue, we propose\nShallow-level Temporal Feedback (STF), a lightweight plug-and-play module for\nthe encoding layer, which consists of Temporal-Spatial Position Embedding\n(TSPE) and Temporal Feedback (TF).Extensive experiments show that STF\nconsistently improves performance across various Transformer-based SNN\nbackbones on static datasets, including CIFAR-10, CIFAR-100, and ImageNet-1K,\nunder different spike timestep settings. Further analysis reveals that STF\nenhances the diversity of the spike patterns, which is key to performance gain.\nMoreover, evaluations on adversarial robustness and temporal sensitivity\nconfirm that STF outperforms direct coding and its variants, highlighting its\npotential as a new spike encoding scheme for static scenarios. Our code will be\nreleased upon acceptance.", "AI": {"tldr": "STF \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u6a21\u5757\uff0c\u53ef\u4ee5\u63d2\u5165\u5230\u7f16\u7801\u5c42\uff0c\u4ee5\u63d0\u9ad8 Transformer-SNN \u7684\u6027\u80fd\uff0c\u5176\u65b9\u6cd5\u662f\u589e\u5f3a\u8109\u51b2\u6a21\u5f0f\u7684\u591a\u6837\u6027\u3002", "motivation": "Transformer-SNNs \u7531\u4e8e\u8109\u51b2\u4e32\u7684\u4e8c\u503c\u6027\u8d28\uff0c\u5728\u6027\u80fd\u4e0a\u4e0e\u6d6e\u70b9ANNs\u5b58\u5728\u8f83\u5927\u5dee\u8ddd\u3002\u4e3a\u4e86\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5df2\u5f15\u5165\u6df1\u5ea6\u53cd\u9988\u56de\u8def\uff0c\u4f46\u8fd9\u4e9b\u8bbe\u8ba1\u901a\u5e38\u8de8\u8d8a\u591a\u5c42\uff0c\u5bfc\u81f4\u7279\u5f81\u8f6c\u6362\u6210\u672c\u9ad8\u3001\u53c2\u6570\u5f00\u9500\u5927\u3001\u80fd\u8017\u589e\u52a0\u548c\u63a8\u7406\u5ef6\u8fdf\u957f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6d45\u5c42\u65f6\u95f4\u53cd\u9988 (STF) \u7684\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5305\u542b\u65f6\u95f4\u7a7a\u95f4\u4f4d\u7f6e\u5d4c\u5165 (TSPE) \u548c\u65f6\u95f4\u53cd\u9988 (TF)\uff0c\u7528\u4e8e\u7f16\u7801\u5c42\u3002", "result": "STF \u5728 CIFAR-10\u3001CIFAR-100 \u548c ImageNet-1K \u7b49\u9759\u6001\u6570\u636e\u96c6\u4e0a\u7684\u5404\u79cd\u57fa\u4e8e Transformer \u7684 SNN \u4e3b\u5e72\u7f51\u7edc\u4e0a\uff0c\u5728\u4e0d\u540c\u7684\u8109\u51b2\u65f6\u95f4\u6b65\u957f\u8bbe\u7f6e\u4e0b\uff0c\u59cb\u7ec8\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "conclusion": "STF \u589e\u5f3a\u4e86\u8109\u51b2\u6a21\u5f0f\u7684\u591a\u6837\u6027\uff0c\u8fd9\u662f\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u3002\u5728\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u65f6\u95f4\u654f\u611f\u6027\u65b9\u9762\u7684\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86 STF \u4f18\u4e8e\u76f4\u63a5\u7f16\u7801\u53ca\u5176\u53d8\u4f53\uff0c\u51f8\u663e\u4e86\u5176\u4f5c\u4e3a\u9759\u6001\u573a\u666f\u65b0\u8109\u51b2\u7f16\u7801\u65b9\u6848\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00015", "categories": ["cs.LO", "cs.MS"], "pdf": "https://arxiv.org/pdf/2508.00015", "abs": "https://arxiv.org/abs/2508.00015", "authors": ["Matt Kaufmann", "J Strother Moore"], "title": "Extended Abstract: Partial-encapsulate and Its Support for Floating-point Operations in ACL2", "comment": "In Proceedings ACL2 2025, arXiv:2507.18567", "summary": "We illustrate the power of partial-encapsulate, showing how it is used in the\nimplementation of floating-point operations in ACL2.", "AI": {"tldr": "Partial-encapsulate is powerful for implementing floating-point operations in ACL2.", "motivation": "To show the power of partial-encapsulate.", "method": "Partial-encapsulate implementation.", "result": "Demonstrated the implementation of floating-point operations in ACL2 using partial-encapsulate.", "conclusion": "The power of partial-encapsulate is illustrated through its implementation of floating-point operations in ACL2."}}
{"id": "2508.00109", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00109", "abs": "https://arxiv.org/abs/2508.00109", "authors": ["Mingda Chen", "Yang Li", "Xilun Chen", "Adina Williams", "Gargi Ghosh", "Scott Yih"], "title": "FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality", "comment": null, "summary": "Long-form factuality evaluation assesses the ability of models to generate\naccurate, comprehensive responses to short prompts. Existing benchmarks often\nlack human verification, leading to potential quality issues. To address this\nlimitation, we introduce FACTORY, a large-scale, human-verified prompt set.\nDeveloped using a model-in-the-loop approach and refined by humans, FACTORY\nincludes challenging prompts that are fact-seeking, answerable, and\nunambiguous. We conduct human evaluations on 6 state-of-the-art language models\nusing FACTORY and existing datasets. Our results show that FACTORY is a\nchallenging benchmark: approximately 40% of the claims made in the responses of\nSOTA models are not factual, compared to only 10% for other datasets. Our\nanalysis identifies the strengths of FACTORY over prior benchmarks, emphasizing\nits reliability and the necessity for models to reason across long-tailed\nfacts.", "AI": {"tldr": "FACTORY is a new, human-verified benchmark for evaluating factual accuracy in long responses. Existing benchmarks are flawed. FACTORY is harder, showing current models are only 60% accurate. Models need to improve on obscure facts.", "motivation": "Existing benchmarks for long-form factuality evaluation often lack human verification, leading to potential quality issues. This limitation is addressed by introducing FACTORY, a human-verified prompt set designed to be challenging, fact-seeking, answerable, and unambiguous.", "method": "A model-in-the-loop approach was used to develop FACTORY, a large-scale, human-verified prompt set. Human evaluations were conducted on 6 state-of-the-art language models using FACTORY and existing datasets.", "result": "FACTORY proves to be a challenging benchmark, revealing that approximately 40% of claims in SOTA model responses are not factual, a significant increase compared to the 10% observed on other datasets. This indicates a gap in models' ability to generate factually accurate long-form responses.", "conclusion": "FACTORY is a reliable and challenging benchmark for evaluating the factuality of long-form responses, highlighting the need for models to reason across long-tailed facts. It demonstrates that current SOTA models struggle with factuality, with approximately 40% of claims being non-factual compared to 10% on other datasets."}}
{"id": "2508.00129", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00129", "abs": "https://arxiv.org/abs/2508.00129", "authors": ["Agust\u00edn Borda", "Juan Bautista Cabral", "Gonzalo Giarda", "Diego Nicol\u00e1s Gimenez Irusta", "Paula Pacheco", "Alvaro Roy Schachner"], "title": "Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis", "comment": null, "summary": "In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem\nthat can greatly affect the results of a Multi-Criteria Decision Method against\na particular set of alternatives. It is therefore useful to have a mechanism\nthat allows one to measure the performance of a method on a set of\nalternatives. This idea could be taken further to build a global ranking of the\neffectiveness of different methods to solve a problem. In this paper, we\npresent three tests that detect the presence of Rank Reversals, along with\ntheir implementation in the Scikit-Criteria library. We also address the\ncomplications that arise when implementing these tests for general scenarios\nand the design considerations we made to handle them. We close with a\ndiscussion about how these additions could play a major role in the judgment of\nmulti-criteria decision methods for problem solving.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e09\u79cd\u68c0\u6d4b\u79e9\u53cd\u8f6c\u7684\u65b9\u6cd5\uff0c\u4ee5\u8bc4\u4f30\u548c\u6bd4\u8f83\u591a\u6807\u51c6\u51b3\u7b56\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u6807\u51c6\u51b3\u7b56\u5206\u6790\uff08MCDA\uff09\u4e2d\u7684\u79e9\u53cd\u8f6c\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u8861\u91cf\u591a\u6807\u51c6\u51b3\u7b56\u65b9\u6cd5\u5728\u4e0d\u540c\u5907\u9009\u65b9\u6848\u96c6\u4e0a\u6027\u80fd\u7684\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u68c0\u6d4b\u79e9\u53cd\u8f6c\u7684\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86 Scikit-Criteria \u5e93\u3002", "result": "\u5b9e\u73b0\u4e86\u4e09\u79cd\u79e9\u53cd\u8f6c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5728\u4e00\u822c\u573a\u666f\u4e0b\u5b9e\u65bd\u8fd9\u4e9b\u68c0\u6d4b\u7684\u590d\u6742\u6027\u548c\u8bbe\u8ba1\u8003\u8651\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u79e9\u53cd\u8f6c\u68c0\u6d4b\u65b9\u6cd5\u6709\u52a9\u4e8e\u8bc4\u4f30\u548c\u6bd4\u8f83\u591a\u6807\u51c6\u51b3\u7b56\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u591a\u6807\u51c6\u51b3\u7b56\u65b9\u6cd5\u7684\u9009\u62e9\u63d0\u4f9b\u4f9d\u636e\u3002"}}
{"id": "2508.00782", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.00782", "abs": "https://arxiv.org/abs/2508.00782", "authors": ["Kien T. Pham", "Yingqing He", "Yazhou Xing", "Qifeng Chen", "Long Chen"], "title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation", "comment": "The 33rd ACM Multimedia Conference (MM '25)", "summary": "Audio-driven video generation aims to synthesize realistic videos that align\nwith input audio recordings, akin to the human ability to visualize scenes from\nauditory input. However, existing approaches predominantly focus on exploring\nsemantic information, such as the classes of sounding sources present in the\naudio, limiting their ability to generate videos with accurate content and\nspatial composition. In contrast, we humans can not only naturally identify the\nsemantic categories of sounding sources but also determine their deeply encoded\nspatial attributes, including locations and movement directions. This useful\ninformation can be elucidated by considering specific spatial indicators\nderived from the inherent physical properties of sound, such as loudness or\nfrequency. As prior methods largely ignore this factor, we present SpA2V, the\nfirst framework explicitly exploits these spatial auditory cues from audios to\ngenerate videos with high semantic and spatial correspondence. SpA2V decomposes\nthe generation process into two stages: 1) Audio-guided Video Planning: We\nmeticulously adapt a state-of-the-art MLLM for a novel task of harnessing\nspatial and semantic cues from input audio to construct Video Scene Layouts\n(VSLs). This serves as an intermediate representation to bridge the gap between\nthe audio and video modalities. 2) Layout-grounded Video Generation: We develop\nan efficient and effective approach to seamlessly integrate VSLs as conditional\nguidance into pre-trained diffusion models, enabling VSL-grounded video\ngeneration in a training-free manner. Extensive experiments demonstrate that\nSpA2V excels in generating realistic videos with semantic and spatial alignment\nto the input audios.", "AI": {"tldr": "SpA2V\u662f\u9996\u4e2a\u5229\u7528\u58f0\u97f3\u7a7a\u95f4\u7ebf\u7d22\u751f\u6210\u89c6\u9891\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u97f3\u9891\u89c4\u5212\u548c\u5e03\u5c40\u57fa\u7840\u751f\u6210\u4e24\u4e2a\u9636\u6bb5\uff0c\u5b9e\u73b0\u89c6\u9891\u5185\u5bb9\u548c\u7a7a\u95f4\u6784\u56fe\u7684\u7cbe\u786e\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u58f0\u97f3\u7684\u7a7a\u95f4\u5c5e\u6027\uff08\u5982\u54cd\u5ea6\u3001\u9891\u7387\uff09\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u89c6\u9891\u5728\u5185\u5bb9\u548c\u7a7a\u95f4\u6784\u56fe\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002\u4eba\u7c7b\u80fd\u591f\u6839\u636e\u58f0\u97f3\u7684\u7a7a\u95f4\u7ebf\u7d22\uff08\u4f4d\u7f6e\u3001\u8fd0\u52a8\u65b9\u5411\uff09\u8fdb\u884c\u53ef\u89c6\u5316\u3002", "method": "SpA2V\u6846\u67b6\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1\uff09\u97f3\u9891\u5f15\u5bfc\u89c6\u9891\u89c4\u5212\uff1a\u5229\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\u63d0\u53d6\u97f3\u9891\u4e2d\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u7ebf\u7d22\uff0c\u6784\u5efa\u89c6\u9891\u573a\u666f\u5e03\u5c40\uff08VSLs\uff09\u30022\uff09\u5e03\u5c40\u57fa\u7840\u89c6\u9891\u751f\u6210\uff1a\u5c06VSLs\u4f5c\u4e3a\u6761\u4ef6\u5f15\u5bfc\uff0c\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684VSL\u57fa\u7840\u89c6\u9891\u751f\u6210\u3002", "result": "SpA2V\u5728\u751f\u6210\u4e0e\u8f93\u5165\u97f3\u9891\u5728\u8bed\u4e49\u548c\u7a7a\u95f4\u4e0a\u90fd\u4fdd\u6301\u4e00\u81f4\u7684\u903c\u771f\u89c6\u9891\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SpA2V\u5728\u751f\u6210\u4e0e\u8f93\u5165\u97f3\u9891\u5728\u8bed\u4e49\u548c\u7a7a\u95f4\u4e0a\u90fd\u4fdd\u6301\u4e00\u81f4\u7684\u903c\u771f\u89c6\u9891\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.00027", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00027", "abs": "https://arxiv.org/abs/2508.00027", "authors": ["Azadeh Alavi", "Fatemeh Kouchmeshki", "Abdolrahman Alavi", "Yongli Ren", "Jiayang Niu"], "title": "Quantum Semi-Random Forests for Qubit-Efficient Recommender Systems", "comment": "Submitted to IEEE Quantum AI Conference (QAI 2025), awaiting peer\n  review", "summary": "Modern recommenders describe each item with hundreds of sparse semantic tags,\nyet most quantum pipelines still map one qubit per tag, demanding well beyond\none hundred qubits, far out of reach for current noisy-intermediate-scale\nquantum (NISQ) devices and prone to deep, error-amplifying circuits. We close\nthis gap with a three-stage hybrid machine learning algorithm that compresses\ntag profiles, optimizes feature selection under a fixed qubit budget via QAOA,\nand scores recommendations with a Quantum semi-Random Forest (QsRF) built on\njust five qubits, while performing similarly to the state-of-the-art methods.\nLeveraging SVD sketching and k-means, we learn a 1000-atom dictionary ($>$97 \\%\nvariance), then solve a 2020 QUBO via depth-3 QAOA to select 5 atoms. A\n100-tree QsRF trained on these codes matches full-feature baselines on\nICM-150/500.", "AI": {"tldr": "\u901a\u8fc7\u6807\u7b7e\u538b\u7f29\u3001QAOA\u7279\u5f81\u9009\u62e9\u548c\u91cf\u5b50\u534a\u968f\u673a\u68ee\u6797\uff0c\u5728\u4ec5\u4f7f\u75285\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5168\u7279\u5f81\u63a8\u8350\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u7684\u91cf\u5b50\u63a8\u8350\u7cfb\u7edf\u9700\u8981\u5927\u91cf\u91cf\u5b50\u6bd4\u7279\uff0c\u8fdc\u8d85NISQ\u8bbe\u5907\u7684\u627f\u53d7\u80fd\u529b\uff0c\u5e76\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u672c\u7814\u7a76\u65e8\u5728\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4f4e\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u4e0b\u7684\u9ad8\u6548\u63a8\u8350\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u6df7\u5408\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff1a1.\u5229\u7528SVD\u548ck-means\u5b66\u4e60\u5b57\u5178\u538b\u7f29\u6807\u7b7e\u4fe1\u606f\u30022.\u4f7f\u7528QAOA\u4f18\u5316\u5728\u56fa\u5b9a\u91cf\u5b50\u6bd4\u7279\u9884\u7b97\u4e0b\u7684\u7279\u5f81\u9009\u62e9\u30023.\u6784\u5efa\u57fa\u4e8e\u91cf\u5b50\u534a\u968f\u673a\u68ee\u6797\uff08QsRF\uff09\u7684\u63a8\u8350\u8bc4\u5206\u6a21\u578b\u3002", "result": "\u901a\u8fc71000\u4e2a\u539f\u5b50\u7684\u5b57\u5178\uff08\u4fdd\u755997%\u4ee5\u4e0a\u65b9\u5dee\uff09\u548c\u6df1\u5ea6\u4e3a3\u7684QAOA\uff0c\u4ece2020\u4e2aQUBO\u95ee\u9898\u4e2d\u9009\u62e9\u4e865\u4e2a\u539f\u5b50\u3002\u57fa\u4e8e\u8fd95\u4e2a\u539f\u5b50\u8bad\u7ec3\u7684100\u68f5\u6811\u7684QsRF\u6a21\u578b\uff0c\u5728ICM-150/500\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u4e0e\u5168\u7279\u5f81\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u91cf\u5b50\u63a8\u8350\u7cfb\u7edf\u53ef\u4ee5\u901a\u8fc7\u538b\u7f29\u6807\u7b7e\u3001\u4f18\u5316\u7279\u5f81\u9009\u62e9\u548c\u4f7f\u7528\u91cf\u5b50\u534a\u968f\u673a\u68ee\u6797\u5728\u6709\u9650\u7684\u91cf\u5b50\u6bd4\u7279\u9884\u7b97\u5185\u5b9e\u73b0\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00283", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.00283", "abs": "https://arxiv.org/abs/2508.00283", "authors": ["Lihan Lian", "Uduak Inyang-Udoh"], "title": "Neural Co-state Projection Regulator: A Model-free Paradigm for Real-time Optimal Control with Input Constraints", "comment": null, "summary": "Learning-based approaches, notably Reinforcement Learning (RL), have shown\npromise for solving optimal control tasks without explicit system models.\nHowever, these approaches are often sample-inefficient, sensitive to reward\ndesign and hyperparameters, and prone to poor generalization, especially under\ninput constraints. To address these challenges, we introduce the neural\nco-state projection regulator (NCPR), a model-free learning-based optimal\ncontrol framework that is grounded in Pontryagin's Minimum Principle (PMP) and\ncapable of solving quadratic regulator problems in nonlinear control-affine\nsystems with input constraints. In this framework, a neural network (NN) is\ntrained in a self-supervised setting to take the current state of the system as\ninput and predict a finite-horizon trajectory of projected co-states (i.e., the\nco-state weighted by the system's input gain). Subsequently, only the first\nelement of the NN's prediction is extracted to solve a lightweight quadratic\nprogram (QP). This workflow is executed in a feedback control setting, allowing\nreal-time computation of control actions that satisfy both input constraints\nand first-order optimality conditions.\n  We test the proposed learning-based model-free quadratic regulator on (1) a\nunicycle model robot reference tracking problem and (2) a pendulum swing-up\ntask. For comparison, reinforcement learning is used on both tasks; and for\ncontext, a model-based controller is used in the unicycle model example. Our\nmethod demonstrates superior generalizability in terms of both unseen system\nstates and varying input constraints, and also shows improved sampling\nefficiency.", "AI": {"tldr": "NCPR\u662f\u4e00\u79cd\u521b\u65b0\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u6700\u4f18\u63a7\u5236\u6846\u67b6\uff0c\u5b83\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u548cPMP\u7406\u8bba\uff0c\u5728\u8f93\u5165\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u89e3\u51b3\u975e\u7ebf\u6027\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u57fa\u4e8e\u5b66\u4e60\u7684\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\uff08\u5982\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u6837\u672c\u6548\u7387\u3001\u5956\u52b1\u8bbe\u8ba1\u654f\u611f\u6027\u3001\u8d85\u53c2\u6570\u654f\u611f\u6027\u4ee5\u53ca\u6cdb\u5316\u6027\uff08\u5c24\u5176\u662f\u5728\u8f93\u5165\u7ea6\u675f\u4e0b\uff09\u65b9\u9762\u7684\u6311\u6218\uff0c\u63d0\u51faNCPR\u3002", "method": "NCPR\u6846\u67b6\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6765\u9884\u6d4b\u6295\u5f71\u5171\u6001\u7684\u6709\u9650\u65f6\u95f4\u8f68\u8ff9\uff0c\u7136\u540e\u4ec5\u63d0\u53d6\u9884\u6d4b\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u6765\u89e3\u51b3\u8f7b\u91cf\u7ea7\u4e8c\u6b21\u89c4\u5212\uff08QP\uff09\u95ee\u9898\uff0c\u4ece\u800c\u5728\u53cd\u9988\u63a7\u5236\u8bbe\u7f6e\u4e2d\u5b9e\u65f6\u8ba1\u7b97\u6ee1\u8db3\u8f93\u5165\u7ea6\u675f\u548c\u4e00\u9636\u6700\u4f18\u6027\u6761\u4ef6\u7684\u63a7\u5236\u52a8\u4f5c\u3002", "result": "\u5728\u6a21\u62df\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u8ddf\u8e2a\u548c\u6446\u52a8pendulum\u4efb\u52a1\u4e2d\uff0cNCPR\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff08\u9488\u5bf9\u672a\u77e5\u7684\u7cfb\u7edf\u72b6\u6001\u548c\u8f93\u5165\u7ea6\u675f\uff09\u548c\u66f4\u9ad8\u7684\u91c7\u6837\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u795e\u7ecf\u5171\u6001\u6295\u5f71\u8c03\u8282\u5668\uff08NCPR\uff09\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u80fd\u591f\u89e3\u51b3\u5177\u6709\u8f93\u5165\u7ea6\u675f\u7684\u975e\u7ebf\u6027\u63a7\u5236\u4eff\u5c04\u7cfb\u7edf\u4e2d\u7684\u4e8c\u6b21\u8c03\u8282\u5668\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u6cdb\u5316\u6027\u548c\u91c7\u6837\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2508.00429", "categories": ["cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.00429", "abs": "https://arxiv.org/abs/2508.00429", "authors": ["Minghao Guo", "Xi Zhu", "Jingyuan Huang", "Kai Mei", "Yongfeng Zhang"], "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network", "comment": "17 pages, work in progress", "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning.", "AI": {"tldr": "ReaGAN \u662f\u4e00\u4e2a\u65b0\u9896\u7684\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u6bcf\u4e2a\u8282\u70b9\u5145\u5f53\u5177\u6709\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\u7684\u4ee3\u7406\uff0c\u5e76\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6765\u514b\u670d\u73b0\u6709 GNN \u7684\u5c40\u9650\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u81ea\u9002\u5e94\u7684\u6d88\u606f\u4f20\u64ad\u548c\u5168\u5c40\u5173\u7cfb\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u7684 GNNs \u5728\u5904\u7406\u8282\u70b9\u4fe1\u606f\u4e0d\u5e73\u8861\uff08\u6709\u4e9b\u8282\u70b9\u4fe1\u606f\u4e30\u5bcc\uff0c\u6709\u4e9b\u8282\u70b9\u4fe1\u606f\u7a00\u758f\uff09\u4ee5\u53ca\u4ec5\u5229\u7528\u5c40\u90e8\u7ed3\u6784\u76f8\u4f3c\u6027\u800c\u5ffd\u7565\u8de8\u56fe\u7684\u5168\u5c40\u8bed\u4e49\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u6355\u83b7\u9065\u8fdc\u4f46\u76f8\u5173\u4fe1\u606f\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ReaGAN \u7684\u4ee3\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u6bcf\u4e2a\u8282\u70b9\u90fd\u5177\u6709\u81ea\u4e3b\u7684\u3001\u8282\u70b9\u7ea7\u522b\u7684\u51b3\u7b56\u80fd\u529b\u3002\u6bcf\u4e2a\u8282\u70b9\u5145\u5f53\u4e00\u4e2a\u4ee3\u7406\uff0c\u6839\u636e\u5176\u5185\u90e8\u8bb0\u5fc6\u72ec\u7acb\u89c4\u5212\u5176\u4e0b\u4e00\u4e2a\u52a8\u4f5c\uff0c\u4ece\u800c\u5b9e\u73b0\u8282\u70b9\u7ea7\u522b\u7684\u89c4\u5212\u548c\u81ea\u9002\u5e94\u7684\u6d88\u606f\u4f20\u64ad\u3002\u6b64\u5916\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5141\u8bb8\u8282\u70b9\u8bbf\u95ee\u8bed\u4e49\u4e0a\u76f8\u5173\u7684\u5185\u5bb9\u5e76\u5728\u56fe\u4e2d\u6784\u5efa\u5168\u5c40\u5173\u7cfb\u3002", "result": "ReaGAN \u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5c11\u6837\u672c\u7684\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u4e2d\uff0c\u5e76\u4e14\u65e0\u9700\u5bf9 LLM \u4e3b\u5e72\u8fdb\u884c\u5fae\u8c03\u3002", "conclusion": "ReaGAN \u5728\u6ca1\u6709\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u51bb\u7ed3\u7684 LLM \u4e3b\u5e72\uff0c\u5728\u5c11\u6837\u672c\u7684\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u4ee3\u7406\u89c4\u5212\u548c\u5c40\u90e8-\u5168\u5c40\u68c0\u7d22\u5728\u56fe\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2311.15183", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2311.15183", "abs": "https://arxiv.org/abs/2311.15183", "authors": ["Kaili Li", "Lei Wang", "Yu Wang", "Yuanjun Guo", "Shuping Lv", "Yuewei He", "Weiwei Lin", "Tai Min", "Shaojie Hu", "Sen Yang", "Dezhen Xue", "Aqun Zheng", "Shuming Yang", "Xiangdong Ding"], "title": "Electric Field Switching of Magnon Spin Current in a Compensated Ferrimagnet", "comment": null, "summary": "Manipulation of directional magnon propagation, known as magnon spin current,\nis essential for developing magnonic memory and logic devices featuring\nnonvolatile functionalities and ultralow power consumption. Magnon spin current\ncan usually be modulated by magnetic field or current-induced spin torques.\nHowever, these approaches may lead to energy dissipation caused by Joule\nheating. Electric-field switching of magnon spin current without charge current\nis highly desired but very challenging to realize. By integrating magnonic and\npiezoelectric materials, we demonstrate manipulation of the magnon spin current\ngenerated by the spin Seebeck effect in the ferrimagnetic insulator Gd3Fe5O12\n(GdIG) film on a piezoelectric substrate. We observe reversible electric-field\nswitching of magnon polarization without applied charge current. Through\nstrain-mediated magnetoelectric coupling, the electric field induces the\nmagnetic compensation transition between two magnetic states of the GdIG,\nresulting in its magnetization reversal and the simultaneous switching of\nmagnon spin current. Our work establishes a prototype material platform that\npave the way for developing magnon logic devices characterized by all electric\nfield reading and writing and reveals the underlying physics principles of\ntheir functions.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u8bc1\u660e\u4e86\u53ef\u4ee5\u901a\u8fc7\u7535\u573a\u63a7\u5236\u78c1\u7574\u81ea\u65cb\u6d41\uff0c\u4e3a\u5f00\u53d1\u5168\u7535\u4fe1\u53f7\u8bfb\u5199\u7684\u78c1\u7574\u903b\u8f91\u5668\u4ef6\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u4f4e\u529f\u8017\u3001\u975e\u6613\u5931\u6027\u7684\u78c1\u7574\u8bb0\u5fc6\u548c\u903b\u8f91\u5668\u4ef6\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u78c1\u573a\u6216\u7535\u6d41\uff0c\u800c\u662f\u901a\u8fc7\u7535\u573a\u63a7\u5236\u78c1\u7574\u81ea\u65cb\u6d41\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5c06\u78c1\u7574\u6750\u6599\u9486\u94c1\u77f3\u69b4\u77f3\uff08Gd3Fe5O12, GdIG\uff09\u8584\u819c\u7f6e\u4e8e\u538b\u7535\u886c\u5e95\u4e0a\uff0c\u5229\u7528\u81ea\u65cb\u585e\u8d1d\u514b\u6548\u5e94\u4ea7\u751f\u78c1\u7574\u81ea\u65cb\u6d41\uff0c\u5e76\u901a\u8fc7\u7535\u573a\u8c03\u63a7\u3002", "result": "\u7814\u7a76\u89c2\u5bdf\u5230\u78c1\u7574\u6781\u5316\u53ef\u9006\u5730\u88ab\u7535\u573a\u5207\u6362\uff0c\u4e14\u65e0\u9700\u65bd\u52a0\u7535\u8377\u7535\u6d41\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u96c6\u6210\u78c1\u7574\u6750\u6599\u548c\u538b\u7535\u6750\u6599\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u4e0d\u65bd\u52a0\u7535\u6d41\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u7535\u573a\u63a7\u5236\u78c1\u7574\u81ea\u65cb\u6d41\u3002"}}
{"id": "2508.00519", "categories": ["cond-mat.mes-hall", "cond-mat.quant-gas", "cond-mat.supr-con", "math-ph", "math.MP", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00519", "abs": "https://arxiv.org/abs/2508.00519", "authors": ["Bal\u00e1zs Het\u00e9nyi", "Bal\u00e1zs D\u00f3ra"], "title": "Localized states and skin effect around non-Hermitian impurities in tight-binding models", "comment": null, "summary": "We use the generalized Bloch theorem formalism of Alase {\\it et al.} [{\\it\nPhys. Rev. Lett.} {\\bf 117} 076804 (2016)] to analyze simple one-dimensional\ntight-binding lattice systems connected by Hermitian bonds (all with the same\nhopping parameter $t$), but containing one bond impurity which can be either\nHermitian or non-Hermitian. We calculate the band structure, the bulk-boundary\ncorrespondence indicator ($D_L(\\epsilon)$) and analyze the eigenvalues of the\nlattice translation operator ($z$), for each eigenstate. From the $z$ values\nthe generalized Brillouin zone can be reconstructed. If the impurity is\nHermitian (and $\\mathcal{PT}$-symmetric), we find a parameter regime in which\ntwo localized edge states separate from the tight-binding band. We then\nsimulate a non-Hermitian impurity by keeping hopping in one direction of the\nbond impurity the same as the rest of the tight-binding system, and varying\nonly its reciprocal. Again, we find a region with localized edge states, but in\nthis case the energy eigenvalues are purely imaginary. We also find that in\nthis case the two zero energy eigenvectors coalesce, hence this system is an\nexceptional line. We then perform an interpolative scan between the above two\nscenarios and find that there is an intermediate region exhibiting a\nnon-Hermitian skin effect. In this region a macroscopic fraction of states\nacquire complex energy eigenvalues and exhibit localization towards the\nimpurity. Our numerical results are supported by a detailed analysis of the\nsolutions of the boundary/impurity equation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u4e00\u7ef4\u683c\u70b9\u7cfb\u7edf\u4e2d\u5384\u7c73\u548c\u975e\u5384\u7c73\u6742\u8d28\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e86\u8fb9\u7f18\u6001\u3001\u4f8b\u5916\u7ebf\u548c\u975e\u5384\u7c73\u76ae\u80a4\u6548\u5e94\u3002", "motivation": "\u5206\u6790\u5305\u542b\u5384\u7c73\u6216\u975e\u5384\u7c73\u6742\u8d28\u952e\u7684\u4e00\u7ef4\u7d27\u675f\u7f1a\u683c\u70b9\u7cfb\u7edf\u7684\u884c\u4e3a\uff0c\u7279\u522b\u662f\u8fb9\u7f18\u6001\u548c\u975e\u5384\u7c73\u76ae\u80a4\u6548\u5e94\u3002", "method": "\u4f7f\u7528Alase\u7b49\u4eba\u63d0\u51fa\u7684\u5e7f\u4e49\u5e03\u6d1b\u8d6b\u5b9a\u7406\u5f62\u5f0f\u4e3b\u4e49\u6765\u5206\u6790\u4e00\u7ef4\u7d27\u675f\u7f1a\u683c\u70b9\u7cfb\u7edf\u3002\u8ba1\u7b97\u4e86\u80fd\u5e26\u7ed3\u6784\u3001\u4f53\u8fb9\u5bf9\u5e94\u6307\u793a\u7b26D_L(\u03b5)\uff0c\u5e76\u5206\u6790\u4e86\u683c\u70b9\u5e73\u79fb\u7b97\u7b26z\u7684\u7279\u5f81\u503c\uff0c\u4ee5\u91cd\u5efa\u5e7f\u4e49\u5e03\u6d1b\u8d6b\u533a\u57df\u3002", "result": "\u627e\u5230\u4e86\u5384\u7c73\u6742\u8d28\u548c\u975e\u5384\u7c73\u6742\u8d28\u7684\u5c40\u57df\u5316\u8fb9\u7f18\u6001\u7684\u6761\u4ef6\u3002\u53d1\u73b0\u975e\u5384\u7c73\u6742\u8d28\u5bfc\u81f4\u4f8b\u5916\u7ebf\u548c\u7eaf\u865a\u6570\u80fd\u91cf\u3002\u8bc6\u522b\u4e86\u4e00\u4e2a\u8868\u73b0\u51fa\u975e\u5384\u7c73\u76ae\u80a4\u6548\u5e94\u7684\u4e2d\u95f4\u533a\u57df\uff0c\u5176\u4e2d\u5927\u90e8\u5206\u6001\u5177\u6709\u590d\u80fd\u91cf\u7279\u5f81\u503c\u5e76\u5411\u6742\u8d28\u5c40\u57df\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u4f7f\u7528\u5e7f\u4e49\u5e03\u6d1b\u8d6b\u5b9a\u7406\u5206\u6790\u4e86\u4e00\u7ef4\u7d27\u675f\u7f1a\u683c\u70b9\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5305\u542b\u4e00\u4e2a\u5384\u7c73\u6216\u975e\u5384\u7c73\u7684\u6742\u8d28\u952e\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5384\u7c73\u6742\u8d28\u60c5\u51b5\u4e0b\uff0c\u5b58\u5728\u4e00\u4e2a\u53c2\u6570\u533a\u57df\uff0c\u5176\u4e2d\u4e24\u4e2a\u5c40\u57df\u5316\u8fb9\u7f18\u6001\u4f1a\u4e0e\u7d27\u675f\u7f1a\u5e26\u5206\u79bb\u3002\u5728\u975e\u5384\u7c73\u6742\u8d28\u60c5\u51b5\u4e0b\uff0c\u5b58\u5728\u4e00\u4e2a\u80fd\u91cf\u7279\u5f81\u503c\u4e3a\u7eaf\u865a\u6570\u7684\u533a\u57df\uff0c\u5e76\u4e14\u4e24\u4e2a\u96f6\u80fd\u672c\u5f81\u5411\u91cf\u4f1a\u5408\u5e76\uff0c\u5f62\u6210\u4e00\u4e2a\u4f8b\u5916\u7ebf\u3002\u5728\u4e24\u8005\u4e4b\u95f4\u7684\u63d2\u503c\u626b\u63cf\u4e2d\uff0c\u53d1\u73b0\u4e86\u4e00\u4e2a\u8868\u73b0\u51fa\u975e\u5384\u7c73\u76ae\u80a4\u6548\u5e94\u7684\u4e2d\u95f4\u533a\u57df\uff0c\u5176\u4e2d\u5927\u90e8\u5206\u6001\u5177\u6709\u590d\u80fd\u91cf\u7279\u5f81\u503c\u5e76\u5411\u6742\u8d28\u5c40\u57df\u5316\u3002"}}
{"id": "2508.00456", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00456", "abs": "https://arxiv.org/abs/2508.00456", "authors": ["Ji Wang", "Bin Tang", "Jian Xiao", "Qimei Cui", "Xingwang Li", "Tony Q. S. Quek"], "title": "When Vision-Language Model (VLM) Meets Beam Prediction: A Multimodal Contrastive Learning Framework", "comment": null, "summary": "As the real propagation environment becomes in creasingly complex and\ndynamic, millimeter wave beam prediction faces huge challenges. However, the\npowerful cross modal representation capability of vision-language model (VLM)\nprovides a promising approach. The traditional methods that rely on real-time\nchannel state information (CSI) are computationally expensive and often fail to\nmaintain accuracy in such environments. In this paper, we present a VLM-driven\ncontrastive learning based multimodal beam prediction framework that integrates\nmultimodal data via modality-specific encoders. To enforce cross-modal\nconsistency, we adopt a contrastive pretraining strategy to align image and\nLiDAR features in the latent space. We use location information as text prompts\nand connect it to the text encoder to introduce language modality, which\nfurther improves cross-modal consistency. Experiments on the DeepSense-6G\ndataset show that our VLM backbone provides additional semantic grounding.\nCompared with existing methods, the overall distance-based accuracy score\n(DBA-Score) of 0.9016, corresponding to 1.46% average improvement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cdVLM\u9a71\u52a8\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u6beb\u7c73\u6ce2\u6ce2\u675f\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u3001LiDAR\u548c\u6587\u672c\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u771f\u5b9e\u4f20\u64ad\u73af\u5883\u65e5\u76ca\u590d\u6742\u52a8\u6001\uff0c\u6beb\u7c73\u6ce2\u6ce2\u675f\u9884\u6d4b\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u800c\u4f20\u7edf\u4f9d\u8d56CSI\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u5728\u590d\u6742\u73af\u5883\u4e0b\u7cbe\u5ea6\u96be\u4ee5\u7ef4\u6301\u3002VLM\u5f3a\u5927\u7684\u8de8\u6a21\u6001\u8868\u793a\u80fd\u529b\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVLM\u7684\u5bf9\u6bd4\u5b66\u4e60\u591a\u6a21\u6001\u6ce2\u675f\u9884\u6d4b\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u7279\u5b9a\u6a21\u6001\u7684\u7f16\u7801\u5668\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7b56\u7565\u5bf9\u9f50\u56fe\u50cf\u548cLiDAR\u7279\u5f81\uff0c\u540c\u65f6\u5229\u7528\u4f4d\u7f6e\u4fe1\u606f\u4f5c\u4e3a\u6587\u672c\u63d0\u793a\u589e\u5f3a\u8bed\u8a00\u6a21\u6001\u7684\u5f15\u5165\uff0c\u4ee5\u52a0\u5f3a\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u3002", "result": "\u5728DeepSense-6G\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5VLM\u9aa8\u5e72\u7f51\u7edc\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u8bed\u4e49\u57fa\u7840\uff0c\u5176\u6574\u4f53\u57fa\u4e8e\u8ddd\u79bb\u7684\u51c6\u786e\u7387\u5f97\u5206\uff08DBA-Score\uff09\u4e3a0.9016\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u63d0\u9ad8\u4e861.46%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u591a\u6a21\u6001\u6570\u636e\u548c\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528VLM\u5f3a\u5927\u7684\u8de8\u6a21\u6001\u8868\u793a\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u4f18\u7684\u6beb\u7c73\u6ce2\u6ce2\u675f\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2508.00303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00303", "abs": "https://arxiv.org/abs/2508.00303", "authors": ["Zehui Xu", "Junhui Wang", "Yongliang Shi", "Chao Gao", "Guyue Zhou"], "title": "TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps", "comment": null, "summary": "This paper introduces TopoDiffuser, a diffusion-based framework for\nmultimodal trajectory prediction that incorporates topometric maps to generate\naccurate, diverse, and road-compliant future motion forecasts. By embedding\nstructural cues from topometric maps into the denoising process of a\nconditional diffusion model, the proposed approach enables trajectory\ngeneration that naturally adheres to road geometry without relying on explicit\nconstraints. A multimodal conditioning encoder fuses LiDAR observations,\nhistorical motion, and route information into a unified bird's-eye-view (BEV)\nrepresentation. Extensive experiments on the KITTI benchmark demonstrate that\nTopoDiffuser outperforms state-of-the-art methods, while maintaining strong\ngeometric consistency. Ablation studies further validate the contribution of\neach input modality, as well as the impact of denoising steps and the number of\ntrajectory samples. To support future research, we publicly release our code at\nhttps://github.com/EI-Nav/TopoDiffuser.", "AI": {"tldr": "TopoDiffuser is a new diffusion-based model for trajectory prediction that uses map information to generate accurate and road-compliant future motion forecasts, outperforming existing methods on the KITTI benchmark.", "motivation": "The paper aims to generate accurate, diverse, and road-compliant future motion forecasts by incorporating topometric maps into a diffusion-based framework for multimodal trajectory prediction, enabling trajectory generation that naturally adheres to road geometry without relying on explicit constraints.", "method": "TopoDiffuser utilizes a diffusion-based framework incorporating topometric maps into the denoising process of a conditional diffusion model. A multimodal conditioning encoder fuses LiDAR observations, historical motion, and route information into a unified bird's-eye-view (BEV) representation.", "result": "The proposed approach enables trajectory generation that naturally adheres to road geometry without relying on explicit constraints. Extensive experiments on the KITTI benchmark demonstrate that TopoDiffuser outperforms state-of-the-art methods, while maintaining strong geometric consistency. Ablation studies validate the contribution of each input modality, denoising steps, and the number of trajectory samples.", "conclusion": "TopoDiffuser outperforms state-of-the-art methods in trajectory prediction, maintaining strong geometric consistency and demonstrating the contribution of each input modality."}}
{"id": "2508.00144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00144", "abs": "https://arxiv.org/abs/2508.00144", "authors": ["Akshat Rakheja", "Aarsh Ashdhir", "Aryan Bhattacharjee", "Vanshika Sharma"], "title": "World Consistency Score: A Unified Metric for Video Generation Quality", "comment": "27 pages, 1 figure", "summary": "We introduce World Consistency Score (WCS), a novel unified evaluation metric\nfor generative video models that emphasizes internal world consistency of the\ngenerated videos. WCS integrates four interpretable sub-components - object\npermanence, relation stability, causal compliance, and flicker penalty - each\nmeasuring a distinct aspect of temporal and physical coherence in a video.\nThese submetrics are combined via a learned weighted formula to produce a\nsingle consistency score that aligns with human judgments. We detail the\nmotivation for WCS in the context of existing video evaluation metrics,\nformalize each submetric and how it is computed with open-source tools\n(trackers, action recognizers, CLIP embeddings, optical flow), and describe how\nthe weights of the WCS combination are trained using human preference data. We\nalso outline an experimental validation blueprint: using benchmarks like\nVBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human\nevaluations, performing sensitivity analyses, and comparing WCS against\nestablished metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a\ncomprehensive and interpretable framework for evaluating video generation\nmodels on their ability to maintain a coherent \"world\" over time, addressing\ngaps left by prior metrics focused only on visual fidelity or prompt alignment.", "AI": {"tldr": "WCS \u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u751f\u6210\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u7269\u4f53\u6301\u4e45\u6027\u3001\u5173\u7cfb\u7a33\u5b9a\u6027\u3001\u56e0\u679c\u9075\u4ece\u6027\u548c\u95ea\u70c1\u60e9\u7f5a\u7b49\u5b50\u6307\u6807\u6765\u8861\u91cf\u89c6\u9891\u7684\u65f6\u95f4\u548c\u7269\u7406\u4e00\u81f4\u6027\uff0c\u5e76\u4e0e\u4eba\u7c7b\u5224\u65ad\u4fdd\u6301\u4e00\u81f4\u3002", "motivation": "WCS \u7684\u63d0\u51fa\u662f\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u8bc4\u4f30\u6307\u6807\u7684\u4e0d\u8db3\uff0c\u8fd9\u4e9b\u6307\u6807\u5f80\u5f80\u53ea\u5173\u6ce8\u89c6\u89c9\u4fdd\u771f\u5ea6\u6216\u63d0\u793a\u5339\u914d\u5ea6\uff0c\u800c\u5ffd\u7565\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u4fdd\u6301\u8fde\u8d2f\u201c\u4e16\u754c\u201d\u968f\u65f6\u95f4\u53d8\u5316\u7684\u80fd\u529b\u3002", "method": "WCS \u6574\u5408\u4e86\u56db\u4e2a\u53ef\u89e3\u91ca\u7684\u5b50\u7ec4\u4ef6\u2014\u2014\u7269\u4f53\u6301\u4e45\u6027\u3001\u5173\u7cfb\u7a33\u5b9a\u6027\u3001\u56e0\u679c\u9075\u4ece\u6027\u548c\u95ea\u70c1\u60e9\u7f5a\u2014\u2014\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u6d4b\u91cf\u89c6\u9891\u4e2d\u65f6\u95f4\u4e0e\u7269\u7406\u4e00\u81f4\u6027\u7684\u4e0d\u540c\u65b9\u9762\u3002\u8fd9\u4e9b\u5b50\u6307\u6807\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u52a0\u6743\u516c\u5f0f\u7ec4\u5408\uff0c\u4ee5\u4ea7\u751f\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4\u7684\u5355\u4e00\u4e00\u81f4\u6027\u5206\u6570\u3002\u8bba\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86 WCS \u7684\u52a8\u673a\u3001\u8ba1\u7b97\u65b9\u6cd5\u4ee5\u53ca\u5982\u4f55\u4f7f\u7528\u4eba\u7c7b\u504f\u597d\u6570\u636e\u8bad\u7ec3\u7ec4\u5408\u6743\u91cd\u3002", "result": "\u901a\u8fc7\u5728 VBench-2.0\u3001EvalCrafter \u548c LOVE \u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0cWCS \u88ab\u8bc1\u660e\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u5177\u6709\u76f8\u5173\u6027\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5982 FVD\u3001CLIPScore\u3001VBench\u3001FVMD\uff09\u3002", "conclusion": "WCS \u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u7edf\u4e00\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u6307\u6807\uff0c\u5b83\u5f3a\u8c03\u751f\u6210\u89c6\u9891\u7684\u5185\u90e8\u4e16\u754c\u4e00\u81f4\u6027\uff0c\u5e76\u4e0e\u4eba\u7c7b\u5224\u65ad\u4fdd\u6301\u4e00\u81f4\u3002"}}
{"id": "2508.00290", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.00290", "abs": "https://arxiv.org/abs/2508.00290", "authors": ["Ritwik Das", "Anne-Sophie Grimault-Jacquin", "Fr\u00e9d\u00e9ric Aniel"], "title": "High-fidelity electronic structure and properties of InSb: $G_0W_0$ and Bayesian-optimized hybrid functionals and DFT+$U$ approaches", "comment": "Accepted in Physical Review B (APS). 16 pages, 3 figures.\n  Supplemental Material included in the source folder", "summary": "This study presents a refined approach to computing the electronic structure\nof indium antimonide (InSb) using advanced \\textit{ab initio} techniques with\nthe In and Sb $4d^{10}$ semicore electrons included in the valence states.\nThese states are modeled using fully relativistic projector augmented waves\n(PAW) and optimized norm-conserving Vanderbilt (ONCV) pseudopotentials.\nHowever, standard Kohn-Sham density-functional theory (DFT) calculations with\nthese pseudopotentials often produce non-physical band inversions and incorrect\nband gaps at the $\\Gamma$-point due to $5p$-$4d$ repulsion and self-interaction\nerrors (SIE). To resolve these issues, we apply a combination of hybrid\nHeyd-Scuseria-Ernzerhof (HSE) exchange-correlation (XC) functionals, many-body\nperturbation theory (MBPT) via quasiparticle $G_0W_0$, and DFT+$U$,\nsignificantly improving the accuracy of the band structure over previous\nstudies. A Bayesian optimization framework is used to refine key parameters,\nincluding the inverse screening length ($\\mu$) and Hartree-Fock (HF) exchange\nfraction ($\\alpha$) in HSE-based XC functionals, as well as the Hubbard $U$\nparameters in DFT+$U$, leading to significantly improved band structure\npredictions. This approach yields highly precise band gaps, bulk moduli,\neffective masses, Luttinger parameters, valence bandwidth, and $4d$ band\npositions, achieving unprecedented agreement with experimental data. The\nresulting model resolves the long-standing incomplete description of InSb's\nelectronic band structure and provides a transferable computational framework\nfor accurate electronic structure predictions across diverse material systems,\noffering valuable insights for future electronic, optoelectronic, energy, and\nquantum applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86 HSE\u3001$G_0W_0$ \u548c DFT+$U$ \u4ee5\u53ca\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u7cbe\u786e\u8ba1\u7b97\u4e86 InSb \u7684\u7535\u5b50\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u4e4b\u524d\u7684\u8ba1\u7b97\u8bef\u5dee\uff0c\u7ed3\u679c\u4e0e\u5b9e\u9a8c\u9ad8\u5ea6\u543b\u5408\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6807\u51c6 Kohn-Sham \u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u8ba1\u7b97 InSb \u65f6\uff0c\u7531\u4e8e $5p$-$4d$ \u65a5\u529b\u548c\u81ea\u76f8\u4e92\u4f5c\u7528\u8bef\u5dee\uff08SIE\uff09\u5bfc\u81f4\u7684\u975e\u7269\u7406\u5e26\u53cd\u8f6c\u548c $\\Gamma$ \u70b9\u5e26\u9699\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u5168\u76f8\u5bf9\u8bba\u6295\u5f71\u589e\u5f3a\u6ce2\uff08PAW\uff09\u548c\u4f18\u5316\u8303\u5fb7\u534e\uff08ONCV\uff09\u8d5d\u52bf\uff0c\u5e76\u7ed3\u5408\u6df7\u5408 HSE \u4ea4\u6362\u5173\u8054\u6cdb\u51fd\u3001\u51c6\u7c92\u5b50 $G_0W_0$ \u548c DFT+$U$ \u65b9\u6cd5\u6765\u8ba1\u7b97 InSb \u7684\u7535\u5b50\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\u4f18\u5316\u4e86 HSE \u548c DFT+$U$ \u4e2d\u7684\u5173\u952e\u53c2\u6570\u3002", "result": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408 HSE\u3001$G_0W_0$ \u548c DFT+$U$ \u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86 InSb \u80fd\u5e26\u7ed3\u6784\u7684\u51c6\u786e\u6027\uff0c\u7cbe\u786e\u9884\u6d4b\u4e86\u5e26\u9699\u3001\u4f53\u6a21\u91cf\u3001\u6709\u6548\u8d28\u91cf\u3001Luttinger \u53c2\u6570\u3001\u4ef7\u5e26\u5bbd\u5ea6\u548c $4d$ \u80fd\u5e26\u4f4d\u7f6e\uff0c\u5e76\u4e0e\u5b9e\u9a8c\u6570\u636e\u8fbe\u6210\u4e86\u524d\u6240\u672a\u6709\u7684\u62df\u5408\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86 InSb \u7684\u80fd\u5e26\u7ed3\u6784\u8ba1\u7b97\u7cbe\u5ea6\uff0c\u89e3\u51b3\u4e86\u957f\u671f\u5b58\u5728\u7684\u63cf\u8ff0\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u5e76\u4e3a\u5176\u4ed6\u6750\u6599\u4f53\u7cfb\u63d0\u4f9b\u4e86\u53ef\u8f6c\u79fb\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u4e3a\u7535\u5b50\u3001\u5149\u7535\u3001\u80fd\u6e90\u548c\u91cf\u5b50\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.00043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00043", "abs": "https://arxiv.org/abs/2508.00043", "authors": ["Nhut Truong", "Uri Hasson"], "title": "Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity", "comment": null, "summary": "Topographic neural networks are computational models that can simulate the\nspatial and functional organization of the brain. Topographic constraints in\nneural networks can be implemented in multiple ways, with potentially different\nimpacts on the representations learned by the network. The impact of such\ndifferent implementations has not been systematically examined. To this end,\nhere we compare topographic convolutional neural networks trained with two\nspatial constraints: Weight Similarity (WS), which pushes neighboring units to\ndevelop similar incoming weights, and Activation Similarity (AS), which\nenforces similarity in unit activations. We evaluate the resulting models on\nclassification accuracy, robustness to weight perturbations and input\ndegradation, and the spatial organization of learned representations. Compared\nto both AS and standard CNNs, WS provided three main advantages: i) improved\nrobustness to noise, also showing higher accuracy under weight corruption; ii)\ngreater input sensitivity, reflected in higher activation variance; and iii)\nstronger functional localization, with units showing similar activations\npositioned at closer distances. In addition, WS produced differences in\norientation tuning, symmetry sensitivity, and eccentricity profiles of units,\nindicating an influence of this spatial constraint on the representational\ngeometry of the network. Our findings suggest that during end-to-end training,\nWS constraints produce more robust representations than AS or non-topographic\nCNNs. These findings also suggest that weight-based spatial constraints can\nshape feature learning and functional organization in biophysical inspired\nmodels.", "AI": {"tldr": "WS\u7ea6\u675f\u6bd4AS\u548c\u975e\u5730\u5f62CNN\u5728\u9c81\u68d2\u6027\u3001\u8f93\u5165\u654f\u611f\u6027\u548c\u529f\u80fd\u5b9a\u4f4d\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u68c0\u9a8c\u4e0d\u540c\u7684\u5730\u5f62\u7ea6\u675f\u5b9e\u73b0\u65b9\u5f0f\u5bf9\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u8868\u793a\u7684\u5f71\u54cd\u3002", "method": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u7a7a\u95f4\u7ea6\u675f\u4e0b\u5730\u5f62\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\uff1a\u6743\u91cd\u76f8\u4f3c\u6027\uff08WS\uff09\u548c\u6fc0\u6d3b\u76f8\u4f3c\u6027\uff08AS\uff09\u3002", "result": "\u4e0eAS\u548c\u6807\u51c6CNN\u76f8\u6bd4\uff0cWS\u5728\u9c81\u68d2\u6027\uff08\u5bf9\u566a\u58f0\u548c\u6743\u91cd\u6270\u52a8\uff09\u3001\u8f93\u5165\u654f\u611f\u6027\uff08\u66f4\u9ad8\u7684\u6fc0\u6d3b\u65b9\u5dee\uff09\u548c\u529f\u80fd\u5b9a\u4f4d\uff08\u6fc0\u6d3b\u76f8\u4f3c\u7684\u5355\u5143\u8ddd\u79bb\u66f4\u8fd1\uff09\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002WS\u8fd8\u5f71\u54cd\u4e86\u5355\u5143\u7684\u65b9\u5411\u8c03\u4f18\u3001\u5bf9\u79f0\u6027\u654f\u611f\u6027\u548c\u79bb\u5fc3\u7387\u5206\u5e03\u3002", "conclusion": "WS\u7ea6\u675f\u5728\u7aef\u5230\u7aef\u8bad\u7ec3\u4e2d\u6bd4AS\u6216\u975e\u5730\u5f62CNN\u4ea7\u751f\u66f4\u9c81\u68d2\u7684\u8868\u793a\u3002\u8fd9\u4e9b\u53d1\u73b0\u8fd8\u8868\u660e\uff0c\u57fa\u4e8e\u6743\u91cd\u7684\u7a7a\u95f4\u7ea6\u675f\u53ef\u4ee5\u5851\u9020\u751f\u7269\u7269\u7406\u542f\u53d1\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u5b66\u4e60\u548c\u529f\u80fd\u7ec4\u7ec7\u3002"}}
{"id": "2508.00121", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00121", "abs": "https://arxiv.org/abs/2508.00121", "authors": ["Xiao Zhang", "Johan bos"], "title": "Is neural semantic parsing good at ellipsis resolution, or isn't it?", "comment": "Accepted by 16th IWCS", "summary": "Neural semantic parsers have shown good overall performance for a variety of\nlinguistic phenomena, reaching semantic matching scores of more than 90%. But\nhow do such parsers perform on strongly context-sensitive phenomena, where\nlarge pieces of semantic information need to be duplicated to form a meaningful\nsemantic representation? A case in point is English verb phrase ellipsis, a\nconstruct where entire verb phrases can be abbreviated by a single auxiliary\nverb. Are the otherwise known as powerful semantic parsers able to deal with\nellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with\ntheir fully resolved meaning representation and used this as a challenge set\nfor a large battery of neural semantic parsers. Although these parsers\nperformed very well on the standard test set, they failed in the instances with\nellipsis. Data augmentation", "AI": {"tldr": "Neural semantic parsers fail on verb phrase ellipsis, a context-sensitive linguistic phenomenon, despite good performance on standard tests. A custom dataset of 120 ellipsis cases was used to demonstrate this failure.", "motivation": "To investigate the performance of neural semantic parsers on strongly context-sensitive phenomena, specifically English verb phrase ellipsis, where entire verb phrases can be abbreviated by auxiliary verbs.", "method": "A challenge set of 120 ellipsis cases with their resolved meaning representations was created and used to test a battery of neural semantic parsers.", "result": "Neural semantic parsers, despite performing well on standard test sets, failed to correctly parse instances with verb phrase ellipsis.", "conclusion": "Despite strong performance on standard datasets, neural semantic parsers struggle with context-sensitive phenomena like verb phrase ellipsis, failing to correctly parse them even after data augmentation."}}
{"id": "2508.00137", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00137", "abs": "https://arxiv.org/abs/2508.00137", "authors": ["Shqiponja Ahmetaj", "George Konstantinidis", "Magdalena Ortiz", "Paolo Pareti", "Mantas Simkus"], "title": "SHACL Validation under Graph Updates (Extended Paper)", "comment": "Accepted at the International Semantic Web Conference (ISWC 2025)", "summary": "SHACL (SHApe Constraint Language) is a W3C standardized constraint language\nfor RDF graphs. In this paper, we study SHACL validation in RDF graphs under\nupdates. We present a SHACL-based update language that can capture intuitive\nand realistic modifications on RDF graphs and study the problem of static\nvalidation under such updates. This problem asks to verify whether every graph\nthat validates a SHACL specification will still do so after applying a given\nupdate sequence. More importantly, it provides a basis for further services for\nreasoning about evolving RDF graphs. Using a regression technique that embeds\nthe update actions into SHACL constraints, we show that static validation under\nupdates can be reduced to (un)satisfiability of constraints in (a minor\nextension of) SHACL. We analyze the computational complexity of the static\nvalidation problem for SHACL and some key fragments. Finally, we present a\nprototype implementation that performs static validation and other static\nanalysis tasks on SHACL constraints and demonstrate its behavior through\npreliminary experiments.", "AI": {"tldr": "This paper studies SHACL validation under updates, introducing a SHACL-based update language and reducing static validation to constraint satisfiability. It analyzes complexity and provides a prototype implementation.", "motivation": "We study SHACL validation in RDF graphs under updates and present a SHACL-based update language that can capture intuitive and realistic modifications on RDF graphs.", "method": "Using a regression technique that embeds the update actions into SHACL constraints.", "result": "We analyze the computational complexity of the static validation problem for SHACL and some key fragments, and present a prototype implementation that performs static validation and other static analysis tasks on SHACL constraints and demonstrate its behavior through preliminary experiments.", "conclusion": "We show that static validation under updates can be reduced to (un)satisfiability of constraints in (a minor extension of) SHACL."}}
{"id": "2508.00029", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00029", "abs": "https://arxiv.org/abs/2508.00029", "authors": ["Azadeh Alavi", "Sanduni Jayasinghe", "Mojtaba Mahmoodian", "Sam Mazaheri", "John Thangarajah", "Sujeeva Setunge"], "title": "Hybrid Quantum Classical Surrogate for Real Time Inverse Finite Element Modeling in Digital Twins", "comment": "Submitted to Scientific Report", "summary": "Large-scale civil structures, such as bridges, pipelines, and offshore\nplatforms, are vital to modern infrastructure, where unexpected failures can\ncause significant economic and safety repercussions. Although finite element\n(FE) modeling is widely used for real-time structural health monitoring (SHM),\nits high computational cost and the complexity of inverse FE analysis, where\nlow dimensional sensor data must map onto high-dimensional displacement or\nstress fields pose ongoing challenges. Here, we propose a hybrid quantum\nclassical multilayer perceptron (QMLP) framework to tackle these issues and\nfacilitate swift updates to digital twins across a range of structural\napplications.\n  Our approach embeds sensor data using symmetric positive definite (SPD)\nmatrices and polynomial features, yielding a representation well suited to\nquantum processing. A parameterized quantum circuit (PQC) transforms these\nfeatures, and the resultant quantum outputs feed into a classical neural\nnetwork for final inference. By fusing quantum capabilities with classical\nmodeling, the QMLP handles large scale inverse FE mapping while preserving\ncomputational viability.\n  Through extensive experiments on a bridge, we demonstrate that the QMLP\nachieves a mean squared error (MSE) of 0.0000000000316, outperforming purely\nclassical baselines with a large margin. These findings confirm the potential\nof quantum-enhanced methods for real time SHM, establishing a pathway toward\nmore efficient, scalable digital twins that can robustly monitor and diagnose\nstructural integrity in near real time.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.00609", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00609", "abs": "https://arxiv.org/abs/2508.00609", "authors": ["M. F. Shakib", "M. Khalil", "R. Postoyan"], "title": "Low-dimensional observer design for stable linear systems by model reduction", "comment": null, "summary": "This paper presents a low-dimensional observer design for stable,\nsingle-input single-output, continuous-time linear time-invariant (LTI)\nsystems. Leveraging the model reduction by moment matching technique, we\napproximate the system with a reduced-order model. Based on this reduced-order\nmodel, we design a low-dimensional observer that estimates the states of the\noriginal system. We show that this observer establishes exact asymptotic state\nreconstruction for a given class of inputs tied to the observer's dimension.\nFurthermore, we establish an exponential input-to-state stability property for\ngeneric inputs, ensuring a bounded estimation error. Numerical simulations\nconfirm the effectiveness of the approach for a benchmark model reduction\nproblem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u7ef4\u89c2\u5bdf\u5668\u8bbe\u8ba1\uff0c\u7528\u4e8e\u7a33\u5b9aLTI\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u77e9\u91cf\u5339\u914d\u964d\u9636\u6280\u672f\uff0c\u4e3a\u7279\u5b9a\u8f93\u5165\u5b9e\u73b0\u7cbe\u786e\u72b6\u6001\u91cd\u6784\uff0c\u5e76\u4fdd\u8bc1\u901a\u7528\u8f93\u5165\u7684\u6307\u6570\u8f93\u5165\u72b6\u6001\u7a33\u5b9a\u6027\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u7a33\u5b9a\u3001\u5355\u8f93\u5165\u5355\u8f93\u51fa\u3001\u8fde\u7eed\u65f6\u95f4\u7ebf\u6027\u65f6\u4e0d\u53d8\uff08LTI\uff09\u7cfb\u7edf\u7684\u4f4e\u7ef4\u89c2\u5bdf\u5668\u8bbe\u8ba1\u3002", "method": "\u5229\u7528\u77e9\u91cf\u5339\u914d\u964d\u9636\u6280\u672f\uff0c\u6211\u4eec\u7528\u964d\u9636\u6a21\u578b\u903c\u8fd1\u7cfb\u7edf\u3002\u57fa\u4e8e\u6b64\u964d\u9636\u6a21\u578b\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4f4e\u7ef4\u89c2\u5bdf\u5668\u6765\u4f30\u8ba1\u539f\u7cfb\u7edf\u7684\u72b6\u6001\u3002", "result": "\u6570\u503c\u6a21\u62df\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5bf9\u4e8e\u57fa\u51c6\u964d\u9636\u95ee\u9898\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u89c2\u5bdf\u5668\u4e3a\u4e00\u7c7b\u7ed9\u5b9a\u7684\u3001\u4e0e\u89c2\u5bdf\u5668\u7ef4\u5ea6\u76f8\u5173\u7684\u8f93\u5165\u5efa\u7acb\u4e86\u7cbe\u786e\u7684\u6e10\u8fd1\u72b6\u6001\u91cd\u6784\uff0c\u5e76\u4e3a\u901a\u7528\u8f93\u5165\u5efa\u7acb\u4e86\u6307\u6570\u8f93\u5165\u72b6\u6001\u7a33\u5b9a\u6027\uff0c\u786e\u4fdd\u4e86\u6709\u754c\u4f30\u8ba1\u8bef\u5dee\u3002"}}
{"id": "2508.00632", "categories": ["cs.AI", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.00632", "abs": "https://arxiv.org/abs/2508.00632", "authors": ["Alexia Jolicoeur-Martineau"], "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings", "comment": null, "summary": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAVR-Eval\u7684\u65b0\u8bc4\u4f30\u6307\u6807\u548cAVR-Agent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u6539\u8fdbAI\u751f\u6210\u4ea4\u4e92\u5f0f\u89c6\u542c\u5185\u5bb9\u7684\u8d28\u91cf\u3002AVR-Agent\u5728\u751f\u6210\u6e38\u620f\u548c\u52a8\u753b\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5355\u6b21\u751f\u6210\uff0c\u4f46\u6a21\u578b\u5728\u5229\u7528\u81ea\u5b9a\u4e49\u7d20\u6750\u548cAVR\u53cd\u9988\u65b9\u9762\u4ecd\u6709\u5f85\u63d0\u9ad8\uff0c\u8fd9\u8868\u660eAI\u5185\u5bb9\u521b\u4f5c\u4e0e\u4eba\u7c7b\u521b\u4f5c\u5728\u5229\u7528\u8d44\u6e90\u65b9\u9762\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u76ee\u524dAI\u5728\u751f\u6210\u4ea4\u4e92\u5f0f\u89c6\u542c\u5185\u5bb9\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u7f3a\u4e4f\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u4e14\u96be\u4ee5\u5904\u7406\u590d\u6742\u5185\u5bb9\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u5a92\u4f53\u5185\u5bb9\u8d28\u91cf\u7684\u76f8\u5bf9\u8bc4\u4f30\u6307\u6807AVR-Eval\uff0c\u5b83\u4f7f\u7528\u97f3\u9891-\u89c6\u89c9\u5f55\u5236\uff08AVRs\uff09\u3002\u4e00\u4e2a\u5168\u6a21\u6001\u6a21\u578b\uff08\u5904\u7406\u6587\u672c\u3001\u89c6\u9891\u548c\u97f3\u9891\uff09\u5bf9\u4e24\u4e2a\u5185\u5bb9\u7684AVRs\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u7531\u4e00\u4e2a\u6587\u672c\u6a21\u578b\u5ba1\u67e5\u8bc4\u4f30\u7ed3\u679c\u4ee5\u786e\u5b9a\u4f18\u52a3\u3002\u6211\u4eec\u8bc1\u660e\u4e86AVR-Eval\u80fd\u591f\u6b63\u786e\u5730\u8bc6\u522b\u51fa\u597d\u7684\u5185\u5bb9\u4e0e\u635f\u574f\u6216\u4e0d\u5339\u914d\u7684\u5185\u5bb9\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edfAVR-Agent\uff0c\u8be5\u7cfb\u7edf\u4ece\u591a\u5a92\u4f53\u7d20\u6750\u5e93\uff08\u97f3\u9891\u3001\u56fe\u50cf\u30013D\u6a21\u578b\uff09\u751f\u6210JavaScript\u4ee3\u7801\u3002\u8be5\u7f16\u7801\u667a\u80fd\u4f53\u9009\u62e9\u76f8\u5173\u7d20\u6750\uff0c\u751f\u6210\u591a\u4e2a\u521d\u59cb\u4ee3\u7801\uff0c\u4f7f\u7528AVR-Eval\u8bc6\u522b\u6700\u4f73\u7248\u672c\uff0c\u5e76\u901a\u8fc7\u5168\u6a21\u6001\u667a\u80fd\u4f53\u7684AVR\u53cd\u9988\u8fdb\u884c\u8fed\u4ee3\u6539\u8fdb\u3002", "result": "\u6211\u4eec\u5728\u6e38\u620f\u548c\u52a8\u753b\u4e0a\u8fd0\u884c\u4e86AVR-Eval\u5b9e\u9a8c\uff08\u5185\u5bb9A\u76f8\u5bf9\u4e8e\u5185\u5bb9B\u7684\u80dc\u7387\uff09\u3002\u6211\u4eec\u53d1\u73b0\uff0cAVR-Agent\u751f\u6210\u7684\u5185\u5bb9\u76f8\u5bf9\u4e8e\u5355\u6b21\u751f\u6210\u7684\u5185\u5bb9\u5177\u6709\u663e\u8457\u66f4\u9ad8\u7684\u80dc\u7387\u3002\u7136\u800c\uff0c\u6a21\u578b\u5728\u6709\u6548\u5229\u7528\u81ea\u5b9a\u4e49\u7d20\u6750\u548cAVR\u53cd\u9988\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u80dc\u7387\u6ca1\u6709\u63d0\u9ad8\u3002\u8fd9\u63ed\u793a\u4e86\u4e00\u4e2a\u5173\u952e\u5dee\u8ddd\uff1a\u867d\u7136\u4eba\u7c7b\u53d7\u76ca\u4e8e\u9ad8\u8d28\u91cf\u7684\u7d20\u6750\u548c\u89c6\u542c\u53cd\u9988\uff0c\u4f46\u5f53\u524d\u7684\u7f16\u7801\u6a21\u578b\u4f3c\u4e4e\u6ca1\u6709\u6709\u6548\u5730\u5229\u7528\u8fd9\u4e9b\u8d44\u6e90\uff0c\u8fd9\u7a81\u663e\u4e86\u4eba\u7c7b\u548c\u673a\u5668\u5185\u5bb9\u521b\u4f5c\u65b9\u6cd5\u4e4b\u95f4\u7684\u6839\u672c\u5dee\u5f02\u3002", "conclusion": "\u867d\u7136AI\u5728\u751f\u6210\u6587\u672c\u3001\u97f3\u9891\u3001\u56fe\u50cf\u548c\u89c6\u9891\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u521b\u5efa\u4ea4\u4e92\u5f0f\u89c6\u542c\u5185\u5bb9\uff08\u5982\u89c6\u9891\u6e38\u620f\uff09\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u4ee5\u751f\u6210JavaScript\u6e38\u620f\u548c\u52a8\u753b\uff0c\u4f46\u7f3a\u4e4f\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u4e14\u5728\u901a\u5e38\u9700\u8981\u6570\u6708\u7684\u4eba\u5de5\uff08\u591a\u8f6e\u3001\u591a\u667a\u80fd\u4f53\uff09\u548c\u827a\u672f\u5bb6\u5236\u4f5c\u7684\u7d20\u6750\u624d\u80fd\u5b8c\u6210\u7684\u590d\u6742\u5185\u5bb9\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u548c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002"}}
{"id": "2508.00560", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.00560", "abs": "https://arxiv.org/abs/2508.00560", "authors": ["Leila Prelat", "Eduardo J. C. Dias", "F. Javier Garc\u00eda de Abajo"], "title": "Wave-mixing cathodoluminescence microscopy of low-frequency excitations", "comment": "9 pages, 5 figures, 37 references", "summary": "Nonlinear optical phenomena such as parametric amplification and frequency\nconversion are typically driven by external optical fields. Free electrons can\nalso act as electromagnetic sources, offering unmatched spatial precision.\nCombining optical and electron-induced fields via the nonlinear response of\nmaterial structures therefore holds potential for revealing new physical\nphenomena and enabling disruptive applications. Here, we theoretically\ninvestigate wave mixing between external light and the evanescent fields of\nfree electrons, giving rise to inelastic photon scattering mediated by the\nsecond-order nonlinear response of a specimen. Specifically, an incident photon\nmay be blue- or red-shifted, while the passing electron correspondingly loses\nor gains energy. These processes are strongly enhanced when the frequency shift\nmatches an optical resonance of the specimen. We present a general theoretical\nframework to quantify the photon conversion probability and demonstrate its\napplication by revealing far-infrared vibrational fingerprints of retinal using\nonly visible light. Beyond its fundamental interest, this phenomenon offers a\npractical approach for spatially mapping low-frequency excitations with\nnanometer resolution using visible photon energies and existing electron\nmicroscopes.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7406\u8bba\u7814\u7a76\u4e86\u5149\u4e0e\u7535\u5b50\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5b9e\u73b0\u4e86\u5229\u7528\u53ef\u89c1\u5149\u5bf9\u7269\u4f53\u8fdb\u884c\u7eb3\u7c73\u5206\u8fa8\u7387\u6210\u50cf\uff0c\u5e76\u6210\u529f\u83b7\u5f97\u4e86\u89c6\u7f51\u819c\u7684\u632f\u52a8\u6307\u7eb9\u3002", "motivation": "\u7ed3\u5408\u5149\u5b66\u548c\u7535\u5b50\u8bf1\u5bfc\u573a\u901a\u8fc7\u6750\u6599\u7ed3\u6784\u7684\u975e\u7ebf\u6027\u54cd\u5e94\uff0c\u6709\u671b\u63ed\u793a\u65b0\u7684\u7269\u7406\u73b0\u8c61\u5e76\u5b9e\u73b0\u98a0\u8986\u6027\u7684\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u7406\u8bba\u6846\u67b6\u6765\u91cf\u5316\u5149\u5b50\u8f6c\u6362\u6982\u7387\uff0c\u5e76\u901a\u8fc7\u63ed\u793a\u4ec5\u4f7f\u7528\u53ef\u89c1\u5149\u5bf9\u89c6\u7f51\u819c\u7684\u8fdc\u7ea2\u5916\u632f\u52a8\u6307\u7eb9\u8fdb\u884c\u4e86\u6f14\u793a\u3002", "result": "\u7406\u8bba\u7814\u7a76\u4e86\u5916\u90e8\u5149\u4e0e\u81ea\u7531\u7535\u5b50\u7684\u6e10\u5931\u573a\u4e4b\u95f4\u7684\u6ce2\u6df7\uff0c\u4ee5\u53ca\u7531\u6807\u672c\u7684\u4e8c\u9636\u975e\u7ebf\u6027\u54cd\u5e94\u4ecb\u5bfc\u7684\u975e\u5f39\u6027\u5149\u5b50\u6563\u5c04\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5165\u5c04\u5149\u5b50\u53ef\u4ee5\u84dd\u79fb\u6216\u7ea2\u79fb\uff0c\u800c\u901a\u8fc7\u7684\u7535\u5b50\u76f8\u5e94\u5730\u635f\u5931\u6216\u83b7\u5f97\u80fd\u91cf\u3002\u5f53\u9891\u7387\u79fb\u52a8\u4e0e\u6807\u672c\u7684\u5149\u5b66\u5171\u632f\u76f8\u5339\u914d\u65f6\uff0c\u8fd9\u4e9b\u8fc7\u7a0b\u4f1a\u5f97\u5230\u663e\u8457\u589e\u5f3a\u3002", "conclusion": "\u8be5\u73b0\u8c61\u4e3a\u5229\u7528\u53ef\u89c1\u5149\u548c\u73b0\u6709\u7535\u5b50\u663e\u5fae\u955c\u5bf9\u4f4e\u9891\u6fc0\u53d1\u8fdb\u884c\u7eb3\u7c73\u5206\u8fa8\u7387\u7a7a\u95f4\u6d4b\u7ed8\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e0d\u4ec5\u5177\u6709\u57fa\u7840\u610f\u4e49\uff0c\u800c\u4e14\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.00494", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00494", "abs": "https://arxiv.org/abs/2508.00494", "authors": ["Youngsun Kong", "Farnoush Baghestani", "I-Ping Chen", "Ki Chon"], "title": "Feasibility of Extracting Skin Nerve Activity from Electrocardiogram Recorded at A Low Sampling Frequency", "comment": "Accepted and presented at the 47th Annual International Conference of\n  the IEEE Engineering in Medicine and Biology Society (EMBC 2025)", "summary": "Skin nerve activity (SKNA) derived from electrocardiogram (ECG) signals has\nbeen a promising non-invasive surrogate for accurate and effective assessment\nof the sympathetic nervous system (SNS). Typically, SKNA extraction requires a\nhigher sampling frequency than the typical ECG recording requirement (> 2 kHz)\nbecause analysis tools extract SKNA from the 0.5-1 kHz frequency band. However,\nECG recording systems commonly provide a sampling frequency of 1 kHz or lower,\nparticularly for wearable devices. Our recent power spectral analysis exhibited\nthat 150-500 Hz frequency bands are dominant during sympathetic stimulation.\nTherefore, we hypothesize that SKNA can be extracted from ECG sampled at a\nlower sampling frequency. We collected ECG signals from 16 participants during\nSNS stimulation and resampled the signals at 0.5, 1, and 4 kHz. Our statistical\nanalyses of significance, classification performance, and reliability indicate\nno significant difference between SKNA indices derived from ECG signals sampled\nat 0.5, 1, and 4 kHz. Our findings indicate that conventional ECG devices,\nwhich are limited to low sampling rates due to resource constraints or outdated\nguidelines, can be used to reliably collect SKNA if muscle artifact\ncontamination is minimal.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u4f4e\u91c7\u6837\u7387\u7684\u5fc3\u7535\u56fe\u4e5f\u80fd\u6709\u6548\u63d0\u53d6\u76ae\u80a4\u795e\u7ecf\u6d3b\u52a8\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002", "motivation": "\u63a2\u7d22\u662f\u5426\u80fd\u4ece\u4f4e\u91c7\u6837\u7387\u7684\u5fc3\u7535\u56fe\u4fe1\u53f7\u4e2d\u63d0\u53d6\u76ae\u80a4\u795e\u7ecf\u6d3b\u52a8\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u8bbe\u5907\u91c7\u6837\u7387\u7684\u9650\u5236\u3002", "method": "\u901a\u8fc7\u6536\u96c6 16 \u540d\u53c2\u4e0e\u8005\u7684 ECG \u4fe1\u53f7\uff0c\u5e76\u5728\u4e0d\u540c\u91c7\u6837\u7387\uff080.5\u30011 \u548c 4 kHz\uff09\u4e0b\u8fdb\u884c\u5206\u6790\uff0c\u8bc4\u4f30 SKNA \u63d0\u53d6\u7684\u6709\u6548\u6027\u3002", "result": "\u7edf\u8ba1\u5206\u6790\u8868\u660e\uff0c\u5728\u663e\u8457\u6027\u3001\u5206\u7c7b\u6027\u80fd\u548c\u53ef\u9760\u6027\u65b9\u9762\uff0c\u4ece\u4e0d\u540c\u91c7\u6837\u7387\uff080.5\u30011 \u548c 4 kHz\uff09\u63d0\u53d6\u7684 SKNA \u6307\u6570\u6ca1\u6709\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u76ae\u80a4\u795e\u7ecf\u6d3b\u52a8\uff08SKNA\uff09\u53ef\u4ee5\u4ece\u4f4e\u91c7\u6837\u7387\uff080.5\u30011 \u548c 4 kHz\uff09\u7684\u5fc3\u7535\u56fe\uff08ECG\uff09\u4fe1\u53f7\u4e2d\u63d0\u53d6\uff0c\u4e0e\u9ad8\u91c7\u6837\u7387\uff08> 2 kHz\uff09\u7684\u4fe1\u53f7\u6ca1\u6709\u663e\u8457\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u5728\u808c\u8089\u4f2a\u5f71\u6c61\u67d3\u6700\u5c0f\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2508.00354", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00354", "abs": "https://arxiv.org/abs/2508.00354", "authors": ["Tianshuang Qiu", "Zehan Ma", "Karim El-Refai", "Hiya Shah", "Chung Min Kim", "Justin Kerr", "Ken Goldberg"], "title": "Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging", "comment": null, "summary": "3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view\nimages. Such \"digital twins\" are useful for simulations, virtual reality,\nmarketing, robot policy fine-tuning, and part inspection. 3D object scanning\nusually requires multi-camera arrays, precise laser scanners, or robot\nwrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,\na pipeline for producing high-quality 3D Gaussian Splat models using a\nbi-manual robot that grasps an object with one gripper and rotates the object\nwith respect to a stationary camera. The object is then re-grasped by a second\ngripper to expose surfaces that were occluded by the first gripper. We present\nthe Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as\nRAFT optical flow models to identify and isolate objects held by a robot\ngripper while removing the gripper and the background. We then modify the 3DGS\ntraining pipeline to support concatenated datasets with gripper occlusion,\nproducing an omni-directional (360 degree view) model of the object. We apply\nOmni-Scan to part defect inspection, finding that it can identify visual or\ngeometric defects in 12 different industrial and household objects with an\naverage accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be\nfound at https://berkeleyautomation.github.io/omni-scan/", "AI": {"tldr": "Omni-Scan uses a two-handed robot and AI models to create full 360-degree 3D models of objects, which are accurate enough (83%) for inspecting defects in parts.", "motivation": "Traditional 3D object scanning methods often rely on multi-camera arrays or laser scanners with restricted workspaces. This work introduces Omni-Scan to overcome these limitations by enabling the creation of high-quality 3D Gaussian Splat models using a more flexible bi-manual robot setup, suitable for a wider range of applications including detailed inspections.", "method": "The Omni-Scan pipeline utilizes a bi-manual robot system where one gripper holds an object while a stationary camera captures its data. A second gripper then re-grasps the object to expose previously occluded surfaces. The process leverages DepthAnything, Segment Anything, and RAFT optical flow models to isolate objects, remove grippers and backgrounds, and modifies the 3DGS training pipeline to handle concatenated datasets with gripper occlusions, ultimately producing omnidirectional 3DGS models.", "result": "The Omni-Scan pipeline successfully generates omnidirectional 3D Gaussian Splat models. When applied to part defect inspection on 12 different industrial and household objects, it achieved an average accuracy of 83% in identifying visual or geometric defects.", "conclusion": "Omni-Scan pipeline combined with 3D Gaussian Splats (3DGS) achieves high-quality, omnidirectional 3D models using a bi-manual robot, enabling applications like part defect inspection with an average accuracy of 83% across various objects."}}
{"id": "2508.00152", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00152", "abs": "https://arxiv.org/abs/2508.00152", "authors": ["Li Mi", "Manon Bechaz", "Zeming Chen", "Antoine Bosselut", "Devis Tuia"], "title": "GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration", "comment": "ICCV 2025. Project page at https://limirs.github.io/GeoExplorer/", "summary": "Active Geo-localization (AGL) is the task of localizing a goal, represented\nin various modalities (e.g., aerial images, ground-level images, or text),\nwithin a predefined search area. Current methods approach AGL as a\ngoal-reaching reinforcement learning (RL) problem with a distance-based reward.\nThey localize the goal by implicitly learning to minimize the relative distance\nfrom it. However, when distance estimation becomes challenging or when\nencountering unseen targets and environments, the agent exhibits reduced\nrobustness and generalization ability due to the less reliable exploration\nstrategy learned during training. In this paper, we propose GeoExplorer, an AGL\nagent that incorporates curiosity-driven exploration through intrinsic rewards.\nUnlike distance-based rewards, our curiosity-driven reward is goal-agnostic,\nenabling robust, diverse, and contextually relevant exploration based on\neffective environment modeling. These capabilities have been proven through\nextensive experiments across four AGL benchmarks, demonstrating the\neffectiveness and generalization ability of GeoExplorer in diverse settings,\nparticularly in localizing unfamiliar targets and environments.", "AI": {"tldr": "GeoExplorer\u662f\u4e00\u79cd\u4e3b\u52a8\u5730\u7406\u5b9a\u4f4d\uff08AGL\uff09\u4ee3\u7406\uff0c\u5b83\u4f7f\u7528\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u63a2\u7d22\u6765\u63d0\u9ad8\u5728\u672a\u77e5\u73af\u5883\u548c\u76ee\u6807\u5b9a\u4f4d\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524dAGL\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u8ddd\u79bb\u7684\u5956\u52b1\uff0c\u5728\u9762\u5bf9\u96be\u4ee5\u4f30\u8ba1\u7684\u8ddd\u79bb\u3001\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u548c\u73af\u5883\u65f6\uff0c\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4f1a\u4e0b\u964d\u3002GeoExplorer\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "GeoExplorer\u91c7\u7528\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u63a2\u7d22\u673a\u5236\uff0c\u901a\u8fc7\u5185\u5728\u5956\u52b1\u6765\u6307\u5bfc\u4ee3\u7406\u4eba\u7684\u63a2\u7d22\u8fc7\u7a0b\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u8ddd\u79bb\u7684\u5956\u52b1\u3002", "result": "GeoExplorer\u5728\u56db\u4e2aAGL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\uff0c\u5c24\u5176\u662f\u5728\u5b9a\u4f4d\u4e0d\u719f\u6089\u7684\u76ee\u6807\u548c\u73af\u5883\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GeoExplorer\u901a\u8fc7\u7ed3\u5408\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u63a2\u7d22\uff0c\u5728\u5404\u79cdAGL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5b9a\u4f4d\u4e0d\u719f\u6089\u7684\u76ee\u6807\u548c\u73af\u5883\u65b9\u9762\u3002"}}
{"id": "2508.00313", "categories": ["cond-mat.mtrl-sci", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2508.00313", "abs": "https://arxiv.org/abs/2508.00313", "authors": ["Mohamed Nawwar", "Samuel Poage", "Tobias Schwaigert", "Maria N. Gastiasoro", "Salva Salmani-Rezaie", "Darrell G. Schlom", "Kaveh Ahadi", "Brandi L. Wooten", "Joseph P. Heremans"], "title": "Large phonon-drag thermopower polarity reversal in Ba-doped KTaO3", "comment": null, "summary": "This study reports the observation of phonon-drag thermopower polarity\nreversal in Ba-doped KTaO3 thin films, mediated by electron-phonon Umklapp\nscattering. Epitaxial films with distinct carrier concentrations (3.7 x 10^20\ncm^-3 and 4.9 x 10^19 cm^-3) were grown via molecular-beam epitaxy. In heavily\ndoped samples, where the Fermi surface spans 80% of the Brillouin zone, the\nUmklapp condition is satisfied, reversing electron momentum. This manifests as\na sign-reversal in the thermopower around 80 K upon cooling despite the sample\nhaving only n-type carriers. On the other hand, the lightly doped sample (4.9 x\n10^19 cm^-3) exhibits only a negative thermopower down to 2 K. These results\nadvance the understanding of Umklapp electron-phonon drag in oxides and\nhighlight KTaO3's potential for engineering unconventional thermoelectric\nmaterials.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5728Ba\u63ba\u6742\u7684KTaO3\u8584\u819c\u4e2d\uff0c\u58f0\u5b50\u62d6\u52a8\u6e29\u5dee\u7535\u52bf\u51fa\u73b0\u6781\u6027\u9006\u8f6c\uff0c\u8fd9\u662f\u7531\u7535\u5b50-\u58f0\u5b50U\u578b\u6563\u5c04\u5f15\u8d77\u7684\u3002\u91cd\u63ba\u6742\u6837\u54c1\u572880 K\u65f6\u51fa\u73b0\u6e29\u5dee\u7535\u52bf\u7b26\u53f7\u9006\u8f6c\uff0c\u800c\u8f7b\u63ba\u6742\u6837\u54c1\u5219\u5168\u7a0b\u8868\u73b0\u4e3a\u8d1f\u6e29\u5dee\u7535\u52bf\u3002", "motivation": "\u5728\u6c27\u5316\u7269\u4e2d\u7814\u7a76\u58f0\u5b50\u62d6\u52a8\u548c\u7535\u5b50-\u58f0\u5b50U\u578b\u6563\u5c04\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u63a2\u7d22KTaO3\u5728\u8bbe\u8ba1\u975e\u5e38\u89c4\u70ed\u7535\u6750\u6599\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5206\u5b50\u675f\u5916\u5ef6\u6cd5\u751f\u957f\u4e86\u5177\u6709\u4e0d\u540c\u8f7d\u6d41\u5b50\u6d53\u5ea6\u7684\u5916\u5ef6\u8584\u819c\uff08\u5206\u522b\u4e3a3.7 x 10^20 cm^-3\u548c4.9 x 10^19 cm^-3\uff09\u3002", "result": "\u5728\u91cd\u63ba\u6742\u6837\u54c1\u4e2d\uff0c\u8d39\u7c73\u9762\u8986\u76d6\u4e86\u5e03\u91cc\u6e0a\u533a\u768480%\uff0c\u6ee1\u8db3\u4e86U\u578b\u6563\u5c04\u6761\u4ef6\uff0c\u5bfc\u81f4\u7535\u5b50\u52a8\u91cf\u9006\u8f6c\u3002\u8fd9\u8868\u73b0\u4e3a\u5c3d\u7ba1\u6837\u54c1\u4ec5\u5177\u6709n\u578b\u8f7d\u6d41\u5b50\uff0c\u4f46\u5728\u51b7\u5374\u65f6\uff0c\u6e29\u5dee\u7535\u52bf\u572880 K\u9644\u8fd1\u51fa\u73b0\u7b26\u53f7\u9006\u8f6c\u3002\u76f8\u53cd\uff0c\u8f7b\u63ba\u6742\u6837\u54c1\uff084.9 x 10^19 cm^-3\uff09\u5728\u4f4e\u81f32 K\u65f6\u4ec5\u8868\u73b0\u51fa\u8d1f\u6e29\u5dee\u7535\u52bf\u3002", "conclusion": "\u5728Ba\u63ba\u6742\u7684KTaO3\u8584\u819c\u4e2d\uff0c\u901a\u8fc7\u7535\u5b50-\u58f0\u5b50U\u578b\u6563\u5c04\u5b9e\u73b0\u4e86\u58f0\u5b50\u62d6\u52a8\u6e29\u5dee\u7535\u52bf\u6781\u6027\u9006\u8f6c\uff0c\u8fd9\u5728\u6c27\u5316\u7269\u4e2d\u662f\u524d\u6240\u672a\u6709\u7684\u3002"}}
{"id": "2508.00545", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.00545", "abs": "https://arxiv.org/abs/2508.00545", "authors": ["Pietro Barbiero", "Mateo Espinosa Zarlenga", "Alberto Termine", "Mateja Jamnik", "Giuseppe Marra"], "title": "Foundations of Interpretable Models", "comment": null, "summary": "We argue that existing definitions of interpretability are not actionable in\nthat they fail to inform users about general, sound, and robust interpretable\nmodel design. This makes current interpretability research fundamentally\nill-posed. To address this issue, we propose a definition of interpretability\nthat is general, simple, and subsumes existing informal notions within the\ninterpretable AI community. We show that our definition is actionable, as it\ndirectly reveals the foundational properties, underlying assumptions,\nprinciples, data structures, and architectural features necessary for designing\ninterpretable models. Building on this, we propose a general blueprint for\ndesigning interpretable models and introduce the first open-sourced library\nwith native support for interpretable data structures and processes.", "AI": {"tldr": "\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u5b9a\u4e49\u4e0d\u9002\u7528\u4e8e\u6307\u5bfc\u6a21\u578b\u8bbe\u8ba1\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u4f18\u7684\u5b9a\u4e49\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u84dd\u56fe\u548c\u5f00\u6e90\u5e93\u3002", "motivation": "\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u5b9a\u4e49\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u6027\uff0c\u65e0\u6cd5\u6307\u5bfc\u6a21\u578b\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5b58\u5728\u6839\u672c\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u66f4\u5177\u64cd\u4f5c\u6027\u7684\u53ef\u89e3\u91ca\u6027\u5b9a\u4e49\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u901a\u7528\u53ef\u89e3\u91ca\u6a21\u578b\u8bbe\u8ba1\u84dd\u56fe\u548c\u5f00\u6e90\u5e93\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u64cd\u4f5c\u7684\u3001\u66f4\u5177\u901a\u7528\u6027\u7684\u53ef\u89e3\u91ca\u6027\u5b9a\u4e49\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u8bbe\u8ba1\u84dd\u56fe\u548c\u76f8\u5e94\u7684\u5f00\u6e90\u5e93\u3002", "conclusion": "\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u5b9a\u4e49\u65e0\u6cd5\u6307\u5bfc\u53ef\u89e3\u91ca\u6a21\u578b\u8bbe\u8ba1\uff0c\u4f7f\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5b58\u5728\u6839\u672c\u6027\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u5177\u64cd\u4f5c\u6027\u7684\u53ef\u89e3\u91ca\u6027\u5b9a\u4e49\uff0c\u8be5\u5b9a\u4e49\u5177\u6709\u901a\u7528\u6027\u3001\u7b80\u6d01\u6027\uff0c\u5e76\u6db5\u76d6\u4e86\u73b0\u6709\u975e\u6b63\u5f0f\u6982\u5ff5\u3002\u65b0\u5b9a\u4e49\u76f4\u63a5\u63ed\u793a\u4e86\u8bbe\u8ba1\u53ef\u89e3\u91ca\u6a21\u578b\u6240\u9700\u7684\u6839\u672c\u5c5e\u6027\u3001\u57fa\u7840\u5047\u8bbe\u3001\u539f\u5219\u3001\u6570\u636e\u7ed3\u6784\u548c\u67b6\u6784\u7279\u6027\u3002\u57fa\u4e8e\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u53ef\u89e3\u91ca\u6a21\u578b\u8bbe\u8ba1\u84dd\u56fe\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u652f\u6301\u53ef\u89e3\u91ca\u6570\u636e\u7ed3\u6784\u548c\u8fc7\u7a0b\u7684\u5f00\u6e90\u5e93\u3002"}}
{"id": "2508.00021", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2508.00021", "abs": "https://arxiv.org/abs/2508.00021", "authors": ["Thomas A. Henzinger", "Konstantin Kueffner", "Vasu Singh", "I Sun"], "title": "Alignment Monitoring", "comment": null, "summary": "Formal verification provides assurances that a probabilistic system satisfies\nits specification--conditioned on the system model being aligned with reality.\nWe propose alignment monitoring to watch that this assumption is justified. We\nconsider a probabilistic model well aligned if it accurately predicts the\nbehaviour of an uncertain system in advance. An alignment score measures this\nby quantifying the similarity between the model's predicted and the system's\n(unknown) actual distributions. An alignment monitor observes the system at\nruntime; at each point in time it uses the current state and the model to\npredict the next state. After the next state is observed, the monitor updates\nthe verdict, which is a high-probability interval estimate for the true\nalignment score. We utilize tools from sequential forecasting to construct our\nalignment monitors. Besides a monitor for measuring the expected alignment\nscore, we introduce a differential alignment monitor, designed for comparing\ntwo models, and a weighted alignment monitor, which permits task-specific\nalignment monitoring. We evaluate our monitors experimentally on the PRISM\nbenchmark suite. They are fast, memory-efficient, and detect misalignment\nearly.", "AI": {"tldr": "\u4e3a\u4e86\u786e\u4fdd\u6982\u7387\u7cfb\u7edf\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5bf9\u9f50\u76d1\u63a7\u201d\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cf\u5316\u6a21\u578b\u9884\u6d4b\u4e0e\u7cfb\u7edf\u5b9e\u9645\u884c\u4e3a\u7684\u76f8\u4f3c\u5ea6\u6765\u8861\u91cf\u6a21\u578b\u4e0e\u73b0\u5b9e\u7684\u4e00\u81f4\u6027\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e09\u79cd\u76d1\u63a7\u5668\uff08\u9884\u671f\u3001\u5dee\u5f02\u548c\u52a0\u6743\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5b83\u4eec\u80fd\u591f\u5feb\u901f\u3001\u9ad8\u6548\u5730\u68c0\u6d4b\u6a21\u578b\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6982\u7387\u7cfb\u7edf\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u5047\u8bbe\u2014\u2014\u5373\u7cfb\u7edf\u6a21\u578b\u4e0e\u73b0\u5b9e\u4e00\u81f4\u2014\u2014\u53ef\u80fd\u4e0d\u6210\u7acb\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u5bf9\u9f50\u76d1\u63a7\u6765\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5bf9\u9f50\u76d1\u63a7\u201d\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cf\u5316\u6a21\u578b\u9884\u6d4b\u4e0e\u7cfb\u7edf\u5b9e\u9645\u884c\u4e3a\u5206\u5e03\u7684\u76f8\u4f3c\u6027\u6765\u8861\u91cf\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5bf9\u9f50\u76d1\u63a7\u5668\u5728\u8fd0\u884c\u65f6\u89c2\u5bdf\u7cfb\u7edf\uff0c\u5229\u7528\u5f53\u524d\u72b6\u6001\u548c\u6a21\u578b\u9884\u6d4b\u4e0b\u4e00\u72b6\u6001\u3002\u5728\u89c2\u5bdf\u5230\u4e0b\u4e00\u4e2a\u72b6\u6001\u540e\uff0c\u76d1\u63a7\u5668\u4f1a\u66f4\u65b0\u5176\u5224\u65ad\uff0c\u63d0\u4f9b\u4e00\u4e2a\u5173\u4e8e\u771f\u5b9e\u5bf9\u9f50\u5206\u6570\u7684\u9ad8\u6982\u7387\u533a\u95f4\u4f30\u8ba1\u3002\u8be5\u65b9\u6cd5\u501f\u9274\u4e86\u987a\u5e8f\u9884\u6d4b\u7684\u5de5\u5177\u6765\u6784\u5efa\u76d1\u63a7\u5668\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e09\u79cd\u76d1\u63a7\u5668\uff1a\u4e00\u79cd\u7528\u4e8e\u6d4b\u91cf\u9884\u671f\u5bf9\u9f50\u5206\u6570\uff0c\u4e00\u79cd\u7528\u4e8e\u6bd4\u8f83\u4e24\u4e2a\u6a21\u578b\uff08\u5dee\u5f02\u5bf9\u9f50\u76d1\u63a7\u5668\uff09\uff0c\u4ee5\u53ca\u4e00\u79cd\u5141\u8bb8\u4efb\u52a1\u7279\u5b9a\u5bf9\u9f50\u76d1\u63a7\uff08\u52a0\u6743\u5bf9\u9f50\u76d1\u63a7\u5668\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5bf9\u9f50\u76d1\u63a7\u5668\u901f\u5ea6\u5feb\u3001\u5185\u5b58\u6548\u7387\u9ad8\uff0c\u5e76\u4e14\u80fd\u591f\u65e9\u671f\u68c0\u6d4b\u5230\u6a21\u578b\u4e0e\u7cfb\u7edf\u5b9e\u9645\u884c\u4e3a\u4e4b\u95f4\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b9e\u9a8c\u5728PRISM\u57fa\u51c6\u5957\u4ef6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u901f\u5ea6\u5feb\u3001\u5185\u5b58\u6548\u7387\u9ad8\u548c\u65e9\u671f\u68c0\u6d4b\u4e0d\u5bf9\u9f50\u7684\u4f18\u70b9\u3002"}}
{"id": "2508.00046", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00046", "abs": "https://arxiv.org/abs/2508.00046", "authors": ["Ruo Yu Tao", "Kaicheng Guo", "Cameron Allen", "George Konidaris"], "title": "Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains", "comment": "To appear at RLC 2025. 1 cover page, 10 pages, 3 reference pages + 13\n  pages for supplementary material", "summary": "Mitigating partial observability is a necessary but challenging task for\ngeneral reinforcement learning algorithms. To improve an algorithm's ability to\nmitigate partial observability, researchers need comprehensive benchmarks to\ngauge progress. Most algorithms tackling partial observability are only\nevaluated on benchmarks with simple forms of state aliasing, such as feature\nmasking and Gaussian noise. Such benchmarks do not represent the many forms of\npartial observability seen in real domains, like visual occlusion or unknown\nopponent intent. We argue that a partially observable benchmark should have two\nkey properties. The first is coverage in its forms of partial observability, to\nensure an algorithm's generalizability. The second is a large gap between the\nperformance of a agents with more or less state information, all other factors\nroughly equal. This gap implies that an environment is memory improvable: where\nperformance gains in a domain are from an algorithm's ability to cope with\npartial observability as opposed to other factors. We introduce best-practice\nguidelines for empirically benchmarking reinforcement learning under partial\nobservability, as well as the open-source library POBAX: Partially Observable\nBenchmarks in JAX. We characterize the types of partial observability present\nin various environments and select representative environments for our\nbenchmark. These environments include localization and mapping, visual control,\ngames, and more. Additionally, we show that these tasks are all memory\nimprovable and require hard-to-learn memory functions, providing a concrete\nsignal for partial observability research. This framework includes recommended\nhyperparameters as well as algorithm implementations for fast, out-of-the-box\nevaluation, as well as highly performant environments implemented in JAX for\nGPU-scalable experimentation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86POBAX\u5e93\uff0c\u5305\u542b\u4e00\u7cfb\u5217\u7528\u4e8e\u8bc4\u4f30\u5f3a\u5316\u5b66\u4e60\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u73af\u5883\u4e0b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "motivation": "\u4e3a\u4e86\u4fc3\u8fdb\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u73af\u5883\u4e0b\u7684\u53d1\u5c55\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u7b97\u6cd5\u7684\u8fdb\u5c55\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u9650\u4e8e\u7b80\u5355\u7684\u72b6\u6001\u522b\u540d\u5f62\u5f0f\uff0c\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u7684\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u95ee\u9898\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u5173\u4e8e\u5982\u4f55\u8bc4\u4f30\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u5f3a\u5316\u5b66\u4e60\u7684\u57fa\u51c6\u6d4b\u8bd5\u7684\u539f\u5219\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8eJAX\u7684\u5f00\u6e90\u5e93POBAX\u3002POBAX\u5e93\u4e2d\u5305\u542b\u4e86\u591a\u79cd\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u73af\u5883\uff0c\u4f8b\u5982\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u3001\u89c6\u89c9\u63a7\u5236\u548c\u6e38\u620f\u7b49\uff0c\u5e76\u5bf9\u8fd9\u4e9b\u73af\u5883\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u5b83\u4eec\u662f\u201c\u8bb0\u5fc6\u53ef\u6539\u8fdb\u7684\u201d\uff0c\u9700\u8981\u96be\u4ee5\u5b66\u4e60\u7684\u8bb0\u5fc6\u529f\u80fd\u3002", "result": "POBAX\u5e93\u5305\u542b\u4e86\u4e00\u7cfb\u5217\u5177\u6709\u6311\u6218\u6027\u7684\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u73af\u5883\uff0c\u8fd9\u4e9b\u73af\u5883\u88ab\u8bc1\u660e\u662f\u201c\u8bb0\u5fc6\u53ef\u6539\u8fdb\u7684\u201d\uff0c\u5373\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u6765\u6e90\u4e8e\u7b97\u6cd5\u5904\u7406\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u7684\u80fd\u529b\uff0c\u800c\u975e\u5176\u4ed6\u56e0\u7d20\u3002\u8be5\u5e93\u8fd8\u63d0\u4f9b\u4e86\u63a8\u8350\u7684\u8d85\u53c2\u6570\u548c\u7b97\u6cd5\u5b9e\u73b0\uff0c\u4ee5\u53ca\u9ad8\u6027\u80fd\u7684JAX\u73af\u5883\uff0c\u652f\u6301GPU\u6269\u5c55\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPOBAX\u7684\u5f00\u6e90\u5e93\uff0c\u5305\u542b\u4e86\u4e00\u7cfb\u5217\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u73af\u5883\u4e0b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002POBAX\u5e93\u63d0\u4f9b\u4e86\u591a\u79cd\u5f62\u5f0f\u7684\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u73af\u5883\uff0c\u5305\u62ec\u72b6\u6001\u522b\u540d\u3001\u89c6\u89c9\u906e\u6321\u548c\u672a\u77e5\u7684\u5bf9\u624b\u610f\u56fe\u7b49\uff0c\u65e8\u5728\u63a8\u52a8\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u66f4\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.00185", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00185", "abs": "https://arxiv.org/abs/2508.00185", "authors": ["Alper Yaman", "Jannik Schwab", "Christof Nitsche", "Abhirup Sinha", "Marco Huber"], "title": "Comparison of Large Language Models for Deployment Requirements", "comment": null, "summary": "Large Language Models (LLMs), such as Generative Pre-trained Transformers\n(GPTs) are revolutionizing the generation of human-like text, producing\ncontextually relevant and syntactically correct content. Despite challenges\nlike biases and hallucinations, these Artificial Intelligence (AI) models excel\nin tasks, such as content creation, translation, and code generation.\nFine-tuning and novel architectures, such as Mixture of Experts (MoE), address\nthese issues. Over the past two years, numerous open-source foundational and\nfine-tuned models have been introduced, complicating the selection of the\noptimal LLM for researchers and companies regarding licensing and hardware\nrequirements. To navigate the rapidly evolving LLM landscape and facilitate LLM\nselection, we present a comparative list of foundational and domain-specific\nmodels, focusing on features, such as release year, licensing, and hardware\nrequirements. This list is published on GitLab and will be continuously\nupdated.", "AI": {"tldr": "A list comparing LLMs (foundational and domain-specific) based on release year, licensing, and hardware requirements to help users choose the right model, available on GitLab.", "motivation": "To help researchers and companies navigate the complex and rapidly evolving LLM landscape by facilitating LLM selection.", "method": "Comparative analysis of foundational and domain-specific LLMs, focusing on features like release year, licensing, and hardware requirements. The list is maintained on GitLab for continuous updates.", "result": "A continuously updated comparative list of LLMs published on GitLab, detailing features such as release year, licensing, and hardware requirements.", "conclusion": "LLMs are revolutionizing text generation but face challenges like bias and hallucinations. Open-source models are rapidly increasing, complicating selection. This paper provides a comparative list of models to aid researchers and companies."}}
{"id": "2508.00138", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00138", "abs": "https://arxiv.org/abs/2508.00138", "authors": ["Rashid Mushkani", "Hugo Berard", "Toumadher Ammar", "Cassandre Chatonnier", "Shin Koseki"], "title": "Co-Producing AI: Toward an Augmented, Participatory Lifecycle", "comment": "Eighth AAAI/ACM Conference on AI, Ethics, and Society 2025", "summary": "Despite efforts to mitigate the inherent risks and biases of artificial\nintelligence (AI) algorithms, these algorithms can disproportionately impact\nculturally marginalized groups. A range of approaches has been proposed to\naddress or reduce these risks, including the development of ethical guidelines\nand principles for responsible AI, as well as technical solutions that promote\nalgorithmic fairness. Drawing on design justice, expansive learning theory, and\nrecent empirical work on participatory AI, we argue that mitigating these harms\nrequires a fundamental re-architecture of the AI production pipeline. This\nre-design should center co-production, diversity, equity, inclusion (DEI), and\nmultidisciplinary collaboration. We introduce an augmented AI lifecycle\nconsisting of five interconnected phases: co-framing, co-design,\nco-implementation, co-deployment, and co-maintenance. The lifecycle is informed\nby four multidisciplinary workshops and grounded in themes of distributed\nauthority and iterative knowledge exchange. Finally, we relate the proposed\nlifecycle to several leading ethical frameworks and outline key research\nquestions that remain for scaling participatory governance.", "AI": {"tldr": "AI\u7684\u98ce\u9669\u548c\u504f\u89c1\u95ee\u9898\u9700\u8981\u91cd\u6784AI\u751f\u4ea7\u6d41\u7a0b\uff0c\u4ee5\u5171\u540c\u6784\u5efa\u3001\u591a\u6837\u6027\u3001\u516c\u5e73\u6027\u3001\u5305\u5bb9\u6027\u548c\u591a\u5b66\u79d1\u534f\u4f5c\u4e3a\u4e2d\u5fc3\uff0c\u63d0\u51fa\u5305\u542b\u4e94\u4e2a\u9636\u6bb5\u7684AI\u751f\u547d\u5468\u671f\u6a21\u578b\u3002", "motivation": "AI\u7b97\u6cd5\u53ef\u80fd\u5bf9\u6587\u5316\u8fb9\u7f18\u5316\u7fa4\u4f53\u4ea7\u751f\u4e0d\u6210\u6bd4\u4f8b\u7684\u5f71\u54cd\uff0c\u5c3d\u7ba1\u5df2\u6709\u5404\u79cd\u7f13\u89e3\u98ce\u9669\u548c\u504f\u89c1\u7684\u52aa\u529b\uff0c\u4f46\u4ecd\u9700\u6839\u672c\u6027\u7684\u91cd\u6784\u3002", "method": "\u672c\u6587\u5229\u7528\u8bbe\u8ba1\u6b63\u4e49\u3001\u6269\u5c55\u5b66\u4e60\u7406\u8bba\u548c\u53c2\u4e0e\u5f0fAI\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u5171\u540c\u6784\u601d\u3001\u5171\u540c\u8bbe\u8ba1\u3001\u5171\u540c\u5b9e\u65bd\u3001\u5171\u540c\u90e8\u7f72\u548c\u5171\u540c\u7ef4\u62a4\u7684\u4e94\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u9636\u6bb5\u7684AI\u751f\u547d\u5468\u671f\u6a21\u578b\u3002\u8be5\u6a21\u578b\u57fa\u4e8e\u56db\u4e2a\u8de8\u5b66\u79d1\u7814\u8ba8\u4f1a\uff0c\u5e76\u4ee5\u5206\u5e03\u5f0f\u6743\u5a01\u548c\u8fed\u4ee3\u77e5\u8bc6\u4ea4\u6d41\u4e3a\u6307\u5bfc\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u751f\u547d\u5468\u671f\u6a21\u578b\uff0c\u5f3a\u8c03\u5171\u540c\u6784\u5efa\u3001\u591a\u6837\u6027\u3001\u516c\u5e73\u6027\u3001\u5305\u5bb9\u6027\u548c\u591a\u5b66\u79d1\u534f\u4f5c\uff0c\u4ee5\u89e3\u51b3AI\u7684\u98ce\u9669\u548c\u504f\u89c1\u95ee\u9898\u3002", "conclusion": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u5171\u540c\u6784\u5efa\u3001\u591a\u6837\u6027\u3001\u516c\u5e73\u6027\u3001\u5305\u5bb9\u6027\u548c\u591a\u5b66\u79d1\u534f\u4f5c\u7684AI\u751f\u547d\u5468\u671f\u6a21\u578b\uff0c\u4ee5\u5e94\u5bf9AI\u7b97\u6cd5\u5bf9\u6587\u5316\u8fb9\u7f18\u5316\u7fa4\u4f53\u7684\u4e0d\u5229\u5f71\u54cd\uff0c\u5e76\u547c\u5401\u5bf9AI\u751f\u4ea7\u6d41\u7a0b\u8fdb\u884c\u6839\u672c\u6027\u91cd\u6784\u3002"}}
{"id": "2508.00048", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00048", "abs": "https://arxiv.org/abs/2508.00048", "authors": ["Ammar Daskin"], "title": "Dimension reduction with structure-aware quantum circuits for hybrid machine learning", "comment": "Any comments are welcome! The simulation code is provided at\n  https://github.com/adaskin/structure-aware-circuits", "summary": "Schmidt decomposition of a vector can be understood as writing the singular\nvalue decomposition (SVD) in vector form. A vector can be written as a linear\ncombination of tensor product of two dimensional vectors by recursively\napplying Schmidt decompositions via SVD to all subsystems. Given a vector\nexpressed as a linear combination of tensor products, using only the $k$\nprincipal terms yields a $k$-rank approximation of the vector. Therefore,\nwriting a vector in this reduced form allows to retain most important parts of\nthe vector while removing small noises from it, analogous to SVD-based\ndenoising.\n  In this paper, we show that quantum circuits designed based on a value $k$\n(determined from the tensor network decomposition of the mean vector of the\ntraining sample) can approximate the reduced-form representations of entire\ndatasets. We then employ this circuit ansatz with a classical neural network\nhead to construct a hybrid machine learning model. Since the output of the\nquantum circuit for an $2^n$ dimensional vector is an $n$ dimensional\nprobability vector, this provides an exponential compression of the input and\npotentially can reduce the number of learnable parameters for training\nlarge-scale models. We use datasets provided in the Python scikit-learn module\nfor the experiments. The results confirm the quantum circuit is able to\ncompress data successfully to provide effective $k$-rank approximations to the\nclassical processing component.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528Schmidt\u5206\u89e3\u548c\u91cf\u5b50\u7535\u8def\u8fdb\u884c\u6570\u636e\u538b\u7f29\u548c\u8fd1\u4f3c\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cf\u5b50\u7535\u8def\u5b9e\u73b0\u6570\u636e\u7684\u6307\u6570\u7ea7\u538b\u7f29\uff0c\u6709\u671b\u51cf\u5c11\u5927\u578b\u6a21\u578b\u7684\u53c2\u6570\u6570\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u538b\u7f29\u6570\u636e\u5e76\u63d0\u4f9bk\u79e9\u8fd1\u4f3c\u3002", "motivation": "Schmidt\u5206\u89e3\u53ef\u4ee5\u770b\u4f5c\u662f\u5411\u91cf\u5f62\u5f0f\u7684SVD\u3002\u901a\u8fc7\u9012\u5f52\u5e94\u7528Schmidt\u5206\u89e3\u548cSVD\u5230\u6240\u6709\u5b50\u7cfb\u7edf\uff0c\u53ef\u4ee5\u5c06\u5411\u91cf\u5199\u6210\u4e8c\u7ef4\u5411\u91cf\u5f20\u91cf\u79ef\u7684\u7ebf\u6027\u7ec4\u5408\u3002\u4f7f\u7528k\u4e2a\u4e3b\u8981\u9879\u53ef\u4ee5\u5f97\u5230\u5411\u91cf\u7684k\u79e9\u8fd1\u4f3c\uff0c\u4ece\u800c\u5728\u53bb\u9664\u566a\u58f0\u7684\u540c\u65f6\u4fdd\u7559\u5411\u91cf\u6700\u91cd\u8981\u7684\u90e8\u5206\u3002", "method": "\u901a\u8fc7\u9012\u5f52\u5e94\u7528Schmidt\u5206\u89e3\u548cSVD\u5230\u6240\u6709\u5b50\u7cfb\u7edf\u6765\u5206\u89e3\u5411\u91cf\u3002\u5229\u7528\u786e\u5b9a\u7684k\u503c\u8bbe\u8ba1\u7684\u91cf\u5b50\u7535\u8def\u53ef\u4ee5\u8fd1\u4f3c\u6570\u636e\u96c6\u7684\u7b80\u5316\u8868\u793a\u3002\u5c06\u8be5\u7535\u8defansatz\u4e0e\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u5934\u7ed3\u5408\u6784\u5efa\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u91cf\u5b50\u7535\u8def\u80fd\u591f\u6210\u529f\u538b\u7f29\u6570\u636e\uff0c\u4e3a\u7ecf\u5178\u5904\u7406\u7ec4\u4ef6\u63d0\u4f9b\u6709\u6548\u7684k\u79e9\u8fd1\u4f3c\u3002", "conclusion": "\u91cf\u5b50\u7535\u8def\u80fd\u591f\u6210\u529f\u538b\u7f29\u6570\u636e\uff0c\u4e3a\u7ecf\u5178\u5904\u7406\u7ec4\u4ef6\u63d0\u4f9b\u6709\u6548\u7684k\u79e9\u8fd1\u4f3c\u3002"}}
{"id": "2508.00637", "categories": ["eess.SY", "cs.CR", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.00637", "abs": "https://arxiv.org/abs/2508.00637", "authors": ["Micha\u0142 Forystek", "Andrew D. Syrmakesis", "Alkistis Kontou", "Panos Kotsampopoulos", "Nikos D. Hatziargyriou", "Charalambos Konstantinou"], "title": "Cyber-Physical Co-Simulation of Load Frequency Control under Load-Altering Attacks", "comment": "2025 IEEE International Conference on Communications, Control, and\n  Computing Technologies for Smart Grids (SmartGridComm)", "summary": "Integrating Information and Communications Technology (ICT) devices into the\npower grid brings many benefits. However, it also exposes the grid to new\npotential cyber threats. Many control and protection mechanisms, such as Load\nFrequency Control (LFC), responsible for maintaining nominal frequency during\nload fluctuations and Under Frequency Load Shedding (UFLS) disconnecting\nportion of the load during an emergency, are dependent on information exchange\nthrough the communication network. The recently emerging Load Altering Attacks\n(LAAs) utilize a botnet of high-wattage devices to introduce load fluctuation.\nIn their dynamic form (DLAAs), they manipulate the load in response to live\ngrid frequency measurements for increased efficiency, posing a notable threat\nto grid stability. Recognizing the importance of communication networks in\npower grid cyber security research, this paper presents an open-source\nco-simulation environment that models the power grid with the corresponding\ncommunication network, implementing grid protective mechanisms. This setup\nallows the comprehensive analysis of the attacks in concrete LFC and UFLS\nscenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5206\u6790\u7535\u7f51\u7f51\u7edc\u653b\u51fb\u7684\u4eff\u771f\u73af\u5883\u3002", "motivation": "\u5c06ICT\u8bbe\u5907\u96c6\u6210\u5230\u7535\u7f51\u867d\u7136\u5e26\u6765\u597d\u5904\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u65b0\u7684\u7f51\u7edc\u5a01\u80c1\uff0c\u7279\u522b\u662f\u52a8\u6001\u8d1f\u8377\u6539\u53d8\u653b\u51fb\uff08DLAAs\uff09\u5bf9\u7535\u7f51\u7a33\u5b9a\u6784\u6210\u4e86\u5a01\u80c1\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5f00\u6e90\u7684\u8054\u5408\u4eff\u771f\u73af\u5883\uff0c\u6a21\u62df\u7535\u529b\u7cfb\u7edf\u548c\u901a\u4fe1\u7f51\u7edc\uff0c\u5e76\u5b9e\u73b0\u7535\u7f51\u4fdd\u62a4\u673a\u5236\uff0c\u4ee5\u5206\u6790\u52a8\u6001\u8d1f\u8377\u6539\u53d8\u653b\u51fb\uff08DLAAs\uff09\u5bf9\u8d1f\u8377\u9891\u7387\u63a7\u5236\uff08LFC\uff09\u548c\u4f4e\u9891\u8d1f\u8377\u7529\u51fa\uff08UFLS\uff09\u7684\u5f71\u54cd\u3002", "result": "\u8be5\u4eff\u771f\u73af\u5883\u80fd\u591f\u5bf9DLAAs\u5728LFC\u548cUFLS\u573a\u666f\u4e0b\u7684\u5f71\u54cd\u8fdb\u884c\u5168\u9762\u7684\u5206\u6790\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u8054\u5408\u4eff\u771f\u73af\u5883\uff0c\u7528\u4e8e\u6a21\u62df\u7535\u529b\u7cfb\u7edf\u53ca\u5176\u901a\u4fe1\u7f51\u7edc\uff0c\u5e76\u5b9e\u73b0\u4e86\u7535\u7f51\u4fdd\u62a4\u673a\u5236\u3002"}}
{"id": "2508.00821", "categories": ["cond-mat.mes-hall", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.00821", "abs": "https://arxiv.org/abs/2508.00821", "authors": ["Gautham Varma K", "Mohd. Hashim Raza", "Azaz Ahmad"], "title": "Chiral anomaly-induced nonlinear Hall effect in spin-orbit coupled noncentrosymmetric metals", "comment": "13 pages, 12 figures. Comments are welcome", "summary": "Recent studies have shown that chiral anomaly is not limited to WSMs, but are\nalso shown by a larger class of materials called spin orbit coupled\nnoncentrosymmetric metals (SOC-NCMs),which has shed more insight into the\norigin of chiral anomaly as a Fermi surface property rather than a nodal\nproperty. In this study, we explore nonlinear transport responses in\nSOC-NCMswithin the framework of semiclassical dynamics, employing the\nMaxwell-Boltzmann transport theory augmented by charge conservation and\nmomentum-dependent scattering processes. We take into account both non-magnetic\nand magnetic impurity scattering mechanisms. We demonstrate that the\nchiral-anomaly-induced nonlinear Hall (CNLH) response exhibits a characteristic\nquadratic dependence on the applied magnetic field and remains negative for\nboth types of impurities. We find that magnetic scatterers leading to\nenhanced/suppressed interband scattering modifies the magnitude of the signal,\nbut does not affect its qualitative behavior. In contrast, the presence of tilt\nin the band dispersion induces a pronounced anisotropic response, including a\nmagnetic-field-direction dependent sign reversal that can be categorized into\nweak and strong regimes. Furthermore, the CNLH response shows substantial\ndirectional anisotropy governed by the relative orientation of the external\nmagnetic field and the tilt vector. Our findings will be helpful in designing\nthe experimental setup to get direction-dependent conductivity, which can be\ntuned externally with the help of magnetic impurity sites.", "AI": {"tldr": "\u7814\u7a76SOC-NCMs\u4e2d\u7684\u975e\u7ebf\u6027\u8f93\u8fd0\uff0c\u53d1\u73b0\u624b\u5f81\u53cd\u5e38\u8bf1\u5bfc\u7684\u975e\u7ebf\u6027\u970d\u5c14\u6548\u5e94\u53d7\u78c1\u573a\u548c\u80fd\u5e26\u503e\u659c\u5f71\u54cd\uff0c\u5177\u6709\u8d1f\u503c\u548c\u5404\u5411\u5f02\u6027\uff0c\u5e76\u63d0\u51fa\u53ef\u7528\u4e8e\u8bbe\u8ba1\u65b9\u5411\u4f9d\u8d56\u6027\u7535\u5bfc\u7684\u5b9e\u9a8c\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\uff0c\u8fd1\u671f\u7684\u7814\u7a76\u8868\u660e\u624b\u5f81\u53cd\u5e38\u4e0d\u4ec5\u5b58\u5728\u4e8eWeyl\u534a\u91d1\u5c5e\uff08WSMs\uff09\u4e2d\uff0c\u4e5f\u5b58\u5728\u4e8e\u66f4\u5e7f\u6cdb\u7684\u6750\u6599\u7c7b\u522b\u2014\u2014\u81ea\u65cb\u8f68\u9053\u8026\u5408\u975e\u4e2d\u5fc3\u5bf9\u79f0\u91d1\u5c5e\uff08SOC-NCMs\uff09\u4e2d\u3002\u8fd9\u63ed\u793a\u4e86\u624b\u5f81\u53cd\u5e38\u8d77\u6e90\u4e8e\u8d39\u7c73\u9762\u6027\u8d28\u800c\u975e\u8282\u70b9\u6027\u8d28\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8fdb\u4e00\u6b65\u63a2\u7d22SOC-NCMs\u4e2d\u7684\u975e\u7ebf\u6027\u8f93\u8fd0\u54cd\u5e94\uff0c\u4ee5\u671f\u66f4\u6df1\u5165\u5730\u7406\u89e3\u624b\u5f81\u53cd\u5e38\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u9ea6\u514b\u65af\u97e6-\u73bb\u5c14\u5179\u66fc\u8f93\u8fd0\u7406\u8bba\uff0c\u5e76\u7ed3\u5408\u7535\u8377\u5b88\u6052\u548c\u4f9d\u8d56\u4e8e\u52a8\u91cf\u7684\u6563\u5c04\u8fc7\u7a0b\uff08\u5305\u62ec\u975e\u78c1\u6027\u548c\u78c1\u6027\u6742\u8d28\u6563\u5c04\uff09\u6765\u5206\u6790\u81ea\u65cb\u8f68\u9053\u8026\u5408\u975e\u4e2d\u5fc3\u5bf9\u79f0\u91d1\u5c5e\uff08SOC-NCMs\uff09\u4e2d\u7684\u975e\u7ebf\u6027\u8f93\u8fd0\u54cd\u5e94\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u624b\u5f81\u53cd\u5e38\u8bf1\u5bfc\u7684\u975e\u7ebf\u6027\u970d\u5c14\uff08CNLH\uff09\u54cd\u5e94\u5bf9\u65bd\u52a0\u7684\u78c1\u573a\u5448\u4e8c\u6b21\u65b9\u4f9d\u8d56\uff0c\u4e14\u5bf9\u4e8e\u975e\u78c1\u6027\u548c\u78c1\u6027\u6742\u8d28\u6563\u5c04\u5747\u4e3a\u8d1f\u503c\u3002\u78c1\u6027\u6563\u5c04\u5b50\u867d\u7136\u4f1a\u6539\u53d8\u4fe1\u53f7\u5e45\u5ea6\uff0c\u4f46\u4e0d\u4f1a\u5f71\u54cd\u5176\u5b9a\u6027\u884c\u4e3a\u3002\u80fd\u5e26\u503e\u659c\u6548\u5e94\u5219\u4f1a\u5f15\u5165\u663e\u8457\u7684\u5404\u5411\u5f02\u6027\u54cd\u5e94\uff0c\u5305\u62ec\u78c1\u573a\u65b9\u5411\u4f9d\u8d56\u7684\u7b26\u53f7\u53cd\u8f6c\uff0c\u5e76\u5b58\u5728\u5f3a\u5f31\u4e24\u79cd\u60c5\u51b5\u3002CNLH\u54cd\u5e94\u7684\u65b9\u5411\u5404\u5411\u5f02\u6027\u4e3b\u8981\u7531\u5916\u78c1\u573a\u548c\u503e\u659c\u77e2\u91cf\u7684\u76f8\u5bf9\u53d6\u5411\u51b3\u5b9a\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u534a\u7ecf\u5178\u52a8\u529b\u5b66\u7406\u8bba\uff0c\u7ed3\u5408\u9ea6\u514b\u65af\u97e6-\u73bb\u5c14\u5179\u66fc\u8f93\u8fd0\u7406\u8bba\uff0c\u5e76\u8003\u8651\u4e86\u7535\u8377\u5b88\u6052\u548c\u4f9d\u8d56\u4e8e\u52a8\u91cf\u7684\u6563\u5c04\u8fc7\u7a0b\uff0c\u63a2\u7d22\u4e86\u81ea\u65cb\u8f68\u9053\u8026\u5408\u975e\u4e2d\u5fc3\u5bf9\u79f0\u91d1\u5c5e\uff08SOC-NCMs\uff09\u4e2d\u7684\u975e\u7ebf\u6027\u8f93\u8fd0\u54cd\u5e94\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u624b\u5f81\u53cd\u5e38\u8bf1\u5bfc\u7684\u975e\u7ebf\u6027\u970d\u5c14\uff08CNLH\uff09\u54cd\u5e94\u5bf9\u65bd\u52a0\u7684\u78c1\u573a\u5448\u4e8c\u6b21\u65b9\u4f9d\u8d56\uff0c\u5e76\u4e14\u5bf9\u4e8e\u975e\u78c1\u6027\u548c\u78c1\u6027\u6742\u8d28\u6563\u5c04\u5747\u8868\u73b0\u4e3a\u8d1f\u503c\u3002\u78c1\u6027\u6563\u5c04\u5b50\u7684\u5b58\u5728\u4f1a\u5f71\u54cd\u4fe1\u53f7\u7684\u5e45\u5ea6\uff0c\u4f46\u4e0d\u4f1a\u6539\u53d8\u5176\u5b9a\u6027\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u80fd\u5e26\u8272\u6563\u4e2d\u7684\u503e\u659c\u6548\u5e94\u4f1a\u5f15\u8d77\u663e\u8457\u7684\u5404\u5411\u5f02\u6027\u54cd\u5e94\uff0c\u5305\u62ec\u78c1\u573a\u65b9\u5411\u4f9d\u8d56\u7684\u7b26\u53f7\u53cd\u8f6c\uff08\u53ef\u5206\u4e3a\u5f31\u548c\u5f3a\u4e24\u79cd\u60c5\u51b5\uff09\u3002CNLH\u54cd\u5e94\u8868\u73b0\u51fa\u7531\u5916\u78c1\u573a\u4e0e\u503e\u659c\u77e2\u91cf\u76f8\u5bf9\u53d6\u5411\u51b3\u5b9a\u7684\u663e\u8457\u65b9\u5411\u5404\u5411\u5f02\u6027\u3002"}}
{"id": "2508.00603", "categories": ["eess.SP", "cs.SY", "eess.AS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00603", "abs": "https://arxiv.org/abs/2508.00603", "authors": ["Hong-Cheng Liang", "Man-Wai Mak", "Kong Aik Lee"], "title": "Subband Architecture Aided Selective Fixed-Filter Active Noise Control", "comment": null, "summary": "The feedforward selective fixed-filter method selects the most suitable\npre-trained control filter based on the spectral features of the detected\nreference signal, effectively avoiding slow convergence in conventional\nadaptive algorithms. However, it can only handle limited types of noises, and\nthe performance degrades when the input noise exhibits non-uniform power\nspectral density. To address these limitations, this paper devises a novel\nselective fixed-filter scheme based on a delayless subband structure. In the\noff-line training stage, subband control filters are pre-trained for different\nfrequency ranges and stored in a dedicated sub-filter database. During the\non-line control stage, the incoming noise is decomposed using a polyphase FFT\nfilter bank, and a frequency-band-matching mechanism assigns each subband\nsignal the most appropriate control filter. Subsequently, a weight stacking\ntechnique is employed to combine all subband weights into a fullband filter,\nenabling real-time noise suppression. Experimental results demonstrate that the\nproposed scheme provides fast convergence, effective noise reduction, and\nstrong robustness in handling more complicated noisy environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9009\u62e9\u6027\u56fa\u5b9a\u6ee4\u6ce2\u5668\u65b9\u6848\uff0c\u901a\u8fc7\u5b50\u5e26\u5904\u7406\u548c\u6ee4\u6ce2\u5668\u5e93\uff0c\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u566a\u58f0\u73af\u5883\u4e0b\u7684\u566a\u58f0\u6291\u5236\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u524d\u9988\u9009\u62e9\u56fa\u5b9a\u6ee4\u6ce2\u5668\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u6709\u9650\u7c7b\u578b\u7684\u566a\u58f0\uff0c\u5e76\u4e14\u5728\u8f93\u5165\u566a\u58f0\u529f\u7387\u8c31\u5bc6\u5ea6\u4e0d\u5747\u5300\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u5ef6\u8fdf\u5b50\u5e26\u7ed3\u6784\u7684\u65b0\u578b\u9009\u62e9\u56fa\u5b9a\u6ee4\u6ce2\u5668\u65b9\u6848\u3002\u5728\u79bb\u7ebf\u8bad\u7ec3\u9636\u6bb5\uff0c\u4e3a\u4e0d\u540c\u7684\u9891\u5e26\u9884\u8bad\u7ec3\u5b50\u5e26\u63a7\u5236\u6ee4\u6ce2\u5668\uff0c\u5e76\u5c06\u5176\u5b58\u50a8\u5728\u4e13\u95e8\u7684\u5b50\u6ee4\u6ce2\u5668\u6570\u636e\u5e93\u4e2d\u3002\u5728\u5728\u7ebf\u63a7\u5236\u9636\u6bb5\uff0c\u4f7f\u7528\u591a\u76f8FFT\u6ee4\u6ce2\u5668\u7ec4\u5206\u89e3\u8f93\u5165\u7684\u566a\u58f0\uff0c\u5e76\u4f7f\u7528\u9891\u5e26\u5339\u914d\u673a\u5236\u5c06\u6700\u5408\u9002\u7684\u63a7\u5236\u6ee4\u6ce2\u5668\u5206\u914d\u7ed9\u6bcf\u4e2a\u5b50\u5e26\u4fe1\u53f7\u3002\u7136\u540e\uff0c\u91c7\u7528\u6743\u91cd\u53e0\u52a0\u6280\u672f\u5c06\u6240\u6709\u5b50\u5e26\u6743\u91cd\u7ec4\u5408\u6210\u5168\u5e26\u6ee4\u6ce2\u5668\uff0c\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u566a\u58f0\u6291\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6848\u5728\u5904\u7406\u66f4\u590d\u6742\u7684\u566a\u58f0\u73af\u5883\u65f6\uff0c\u5177\u6709\u6536\u655b\u901f\u5ea6\u5feb\u3001\u964d\u566a\u6548\u679c\u597d\u3001\u9c81\u68d2\u6027\u5f3a\u7b49\u4f18\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u7684\u566a\u58f0\u73af\u5883\u4e2d\u63d0\u4f9b\u4e86\u5feb\u901f\u6536\u655b\u3001\u6709\u6548\u7684\u566a\u58f0\u6291\u5236\u548c\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.00355", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00355", "abs": "https://arxiv.org/abs/2508.00355", "authors": ["Zhenghan Chen", "Haocheng Xu", "Haodong Zhang", "Liang Zhang", "He Li", "Dongqi Wang", "Jiyu Yu", "Yifei Yang", "Zhongxiang Zhou", "Rong Xiong"], "title": "TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots", "comment": null, "summary": "Humanoid robots have the potential capability to perform a diverse range of\nmanipulation tasks, but this is based on a robust and precise standing\ncontroller. Existing methods are either ill-suited to precisely control\nhigh-dimensional upper-body joints, or difficult to ensure both robustness and\naccuracy, especially when upper-body motions are fast. This paper proposes a\nnovel time optimization policy (TOP), to train a standing manipulation control\nmodel that ensures balance, precision, and time efficiency simultaneously, with\nthe idea of adjusting the time trajectory of upper-body motions but not only\nstrengthening the disturbance resistance of the lower-body. Our approach\nconsists of three parts. Firstly, we utilize motion prior to represent\nupper-body motions to enhance the coordination ability between the upper and\nlower-body by training a variational autoencoder (VAE). Then we decouple the\nwhole-body control into an upper-body PD controller for precision and a\nlower-body RL controller to enhance robust stability. Finally, we train TOP\nmethod in conjunction with the decoupled controller and VAE to reduce the\nbalance burden resulting from fast upper-body motions that would destabilize\nthe robot and exceed the capabilities of the lower-body RL policy. The\neffectiveness of the proposed approach is evaluated via both simulation and\nreal world experiments, which demonstrate the superiority on standing\nmanipulation tasks stably and accurately. The project page can be found at\nhttps://anonymous.4open.science/w/top-258F/.", "AI": {"tldr": "\u901a\u8fc7\u65f6\u95f4\u4f18\u5316\u7b56\u7565\uff08TOP\uff09\u534f\u8c03\u673a\u5668\u4eba\u4e0a\u4e0b\u534a\u8eab\u8fd0\u52a8\uff0c\u5e76\u89e3\u8026\u63a7\u5236\u5668\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u3001\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u7ad9\u7acb\u64cd\u63a7\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u63a7\u5236\u9ad8\u7ef4\u5ea6\u7684\u4e0a\u534a\u8eab\u5173\u8282\uff0c\u6216\u5728\u5feb\u901f\u8fd0\u52a8\u65f6\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u9c81\u68d2\u6027\u548c\u7cbe\u786e\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bad\u7ec3\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u4fdd\u8bc1\u5e73\u8861\u3001\u7cbe\u786e\u548c\u65f6\u95f4\u6548\u7387\u7684\u7ad9\u7acb\u64cd\u63a7\u6a21\u578b\u3002", "method": "1.\u5229\u7528\u8fd0\u52a8\u5148\u9a8c\uff08motion prior\uff09\u8bad\u7ec3\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u4ee5\u589e\u5f3a\u4e0a\u4e0b\u534a\u8eab\u534f\u8c03\u80fd\u529b\u3002 2.\u5c06\u5168\u8eab\u63a7\u5236\u5206\u89e3\u4e3a\u4e0a\u534a\u8eab\u7684PD\u63a7\u5236\u5668\uff08\u7528\u4e8e\u7cbe\u786e\u6027\uff09\u548c\u4e0b\u534a\u8eab\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u63a7\u5236\u5668\uff08\u7528\u4e8e\u9c81\u68d2\u7a33\u5b9a\u6027\uff09\u3002 3.\u8bad\u7ec3TOP\u65b9\u6cd5\u7ed3\u5408\u89e3\u8026\u63a7\u5236\u5668\u548cVAE\uff0c\u4ee5\u51cf\u8f7b\u5feb\u901f\u4e0a\u534a\u8eab\u8fd0\u52a8\u5e26\u6765\u7684\u5e73\u8861\u8d1f\u62c5\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7ad9\u7acb\u64cd\u63a7\u4efb\u52a1\u4e2d\u80fd\u591f\u7a33\u5b9a\u4e14\u7cbe\u786e\u5730\u6267\u884c\u4efb\u52a1\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65f6\u95f4\u4f18\u5316\u7b56\u7565\uff08TOP\uff09\u53ca\u5176\u89e3\u8026\u63a7\u5236\u5668\u548cVAE\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u5728\u7ad9\u7acb\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u5e73\u8861\u6027\u3001\u7cbe\u786e\u6027\u548c\u65f6\u95f4\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5feb\u901f\u4e0a\u534a\u8eab\u8fd0\u52a8\u65f6\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2508.00169", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00169", "abs": "https://arxiv.org/abs/2508.00169", "authors": ["Bhavya Goyal", "Felipe Gutierrez-Barragan", "Wei Lin", "Andreas Velten", "Yin Li", "Mohit Gupta"], "title": "Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs", "comment": "ICCV 2025", "summary": "LiDAR-based 3D sensors provide point clouds, a canonical 3D representation\nused in various scene understanding tasks. Modern LiDARs face key challenges in\nseveral real-world scenarios, such as long-distance or low-albedo objects,\nproducing sparse or erroneous point clouds. These errors, which are rooted in\nthe noisy raw LiDAR measurements, get propagated to downstream perception\nmodels, resulting in potentially severe loss of accuracy. This is because\nconventional 3D processing pipelines do not retain any uncertainty information\nfrom the raw measurements when constructing point clouds.\n  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation\nwhere each point is augmented with a probability attribute that encapsulates\nthe measurement uncertainty (or confidence) in the raw data. We further\nintroduce inference approaches that leverage PPC for robust 3D object\ndetection; these methods are versatile and can be used as computationally\nlightweight drop-in modules in 3D inference pipelines. We demonstrate, via both\nsimulations and real captures, that PPC-based 3D inference methods outperform\nseveral baselines using LiDAR as well as camera-LiDAR fusion models, across\nchallenging indoor and outdoor scenarios involving small, distant, and\nlow-albedo objects, as well as strong ambient light.\n  Our project webpage is at https://bhavyagoyal.github.io/ppc .", "AI": {"tldr": "LiDAR \u4f20\u611f\u5668\u5e38\u56e0\u7269\u4f53\u7279\u6027\u6216\u73af\u5883\u56e0\u7d20\u4ea7\u751f\u7a00\u758f\u6216\u9519\u8bef\u7684\u70b9\u4e91\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u6982\u7387\u70b9\u4e91\uff08PPC\uff09\u8868\u793a\uff0c\u4e3a\u6bcf\u4e2a\u70b9\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u5c5e\u6027\u3002\u57fa\u4e8e PPC \u7684 3D \u68c0\u6d4b\u65b9\u6cd5\u5728\u5404\u79cd\u6311\u6218\u6027\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684 3D \u5904\u7406\u6d41\u7a0b\u5728\u6784\u5efa\u70b9\u4e91\u65f6\u6ca1\u6709\u4fdd\u7559\u539f\u59cb\u6d4b\u91cf\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\uff0c\u5bfc\u81f4\u5728\u771f\u5b9e\u4e16\u754c\u7684\u573a\u666f\u4e2d\uff08\u5982\u957f\u8ddd\u79bb\u6216\u4f4e\u53cd\u7167\u7387\u7269\u4f53\uff09\u53ef\u80fd\u51fa\u73b0\u7a00\u758f\u6216\u9519\u8bef\u7684\u70b9\u4e91\uff0c\u4ece\u800c\u5f71\u54cd\u4e0b\u6e38\u611f\u77e5\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6982\u7387\u70b9\u4e91\uff08PPC\uff09\u7684\u65b0\u578b 3D \u8868\u793a\uff0c\u5176\u4e2d\u6bcf\u4e2a\u70b9\u90fd\u5305\u542b\u4e00\u4e2a\u6982\u7387\u5c5e\u6027\uff0c\u4ee5\u5c01\u88c5\u539f\u59cb\u6d4b\u91cf\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u8fd8\u4ecb\u7ecd\u4e86\u5229\u7528 PPC \u8fdb\u884c\u7a33\u5065 3D \u5bf9\u8c61\u68c0\u6d4b\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u4f5c\u4e3a 3D \u63a8\u7406\u7ba1\u9053\u4e2d\u7684\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u6a21\u5757\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u6355\u83b7\u8868\u660e\uff0cPPC 3D \u63a8\u7406\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5ba4\u5185\u548c\u5ba4\u5916\u573a\u666f\u4e2d\uff0c\u5728\u68c0\u6d4b\u5c0f\u578b\u3001\u8fdc\u5904\u548c\u4f4e\u53cd\u7167\u7387\u7269\u4f53\u4ee5\u53ca\u5f3a\u73af\u5883\u5149\u65b9\u9762\uff0c\u4f18\u4e8e\u4f7f\u7528 LiDAR \u6216\u76f8\u673a-LiDAR \u878d\u5408\u6a21\u578b\u7684\u51e0\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PPC 3D \u63a8\u7406\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5ba4\u5185\u548c\u5ba4\u5916\u573a\u666f\u4e2d\uff0c\u5728\u68c0\u6d4b\u5c0f\u578b\u3001\u8fdc\u5904\u548c\u4f4e\u53cd\u7167\u7387\u7269\u4f53\u4ee5\u53ca\u5f3a\u73af\u5883\u5149\u65b9\u9762\uff0c\u4f18\u4e8e\u4f7f\u7528 LiDAR \u6216\u76f8\u673a-LiDAR \u878d\u5408\u6a21\u578b\u7684\u51e0\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.00327", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.00327", "abs": "https://arxiv.org/abs/2508.00327", "authors": ["Hyungmin An", "Sangmin Oh", "Dongheon Lee", "Jae-hyeon Ko", "Dongyean Oh", "Changho Hong", "Seungwu Han"], "title": "Etching-to-deposition transition in SiO$_2$/Si$_3$N$_4$ using CH$_x$F$_y$ ion-based plasma etching: An atomistic study with neural network potentials", "comment": null, "summary": "Plasma etching, a critical process in semiconductor fabrication, utilizes\nhydrofluorocarbons both as etchants and as precursors for carbon film\nformation, where precise control over film growth is essential for achieving\nhigh SiO$_2$/Si$_3$N$_4$ selectivity and enabling atomic layer etching. In this\nwork, we develop neural network potentials (NNPs) to gain atomistic insights\ninto the surface evolution of SiO$_2$ and Si$_3$N$_4$ under hydrofluorocarbon\nion bombardment. To efficiently sample diverse local configurations without\nexhaustive enumeration of ion-substrate combinations, we propose a\nvapor-to-surface sampling approach using high-temperature, low-density\nmolecular dynamics simulations, supplemented with baseline reference\nstructures. The NNPs, refined through iterative training, yield etching\ncharacteristics in MD simulations that show good agreement with experimental\nresults. Further analysis reveals distinct mechanisms of carbon layer formation\nin SiO$_2$ and Si$_3$N$_4$, driven by the higher volatility of carbon-oxygen\nbyproducts in SiO$_2$ and the suppressed formation of volatile carbon-nitrogen\nspecies in Si$_3$N$_4$. This computational framework enables quantitative\npredictions of atomistic surface modifications under plasma exposure and\nprovides a foundation for integration with multiscale process modeling,\noffering insights into semiconductor fabrication processes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u52bf\uff08NNPs\uff09\u548c\u5206\u5b50\u52a8\u529b\u5b66\uff08MD\uff09\u6a21\u62df\uff0c\u7814\u7a76\u4e86\u6c22\u6c1f\u78b3\u7b49\u79bb\u5b50\u4f53\u8680\u523bSiO2\u548cSi3N4\u7684\u8fc7\u7a0b\uff0c\u63ed\u793a\u4e86\u78b3\u5c42\u5f62\u6210\u7684\u673a\u5236\u5dee\u5f02\uff0c\u4e3a\u534a\u5bfc\u4f53\u5236\u9020\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "motivation": "\u4e3a\u4e86\u5728\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\u7cbe\u786e\u63a7\u5236SiO2\u548cSi3N4\u7684\u8680\u523b\u9009\u62e9\u6027\u548c\u5b9e\u73b0\u539f\u5b50\u5c42\u8680\u523b\uff0c\u9700\u8981\u83b7\u5f97\u5bf9\u6c22\u6c1f\u78b3\u79bb\u5b50\u8f70\u51fb\u4e0b\u8868\u9762\u6f14\u5316\u7684\u539f\u5b50\u5c3a\u5ea6\u8ba4\u8bc6\u3002", "method": "\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u52bf\uff08NNPs\uff09\u7ed3\u5408\u9ad8\u6e29\u3001\u4f4e\u5bc6\u5ea6\u5206\u5b50\u52a8\u529b\u5b66\uff08MD\uff09\u6a21\u62df\uff0c\u5e76\u8f85\u4ee5\u57fa\u7ebf\u53c2\u8003\u7ed3\u6784\uff0c\u5bf9\u6c22\u6c1f\u78b3\u79bb\u5b50\u8f70\u51fb\u4e0b\u7684SiO2\u548cSi3N4\u8868\u9762\u6f14\u5316\u8fdb\u884c\u539f\u5b50\u5c3a\u5ea6\u7814\u7a76\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u52bf\uff08NNPs\uff09\u7684\u5f00\u53d1\u548c\u8fed\u4ee3\u8bad\u7ec3\uff0c\u4f7f\u5f97MD\u6a21\u62df\u4e2d\u7684\u8680\u523b\u7279\u6027\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u5177\u6709\u826f\u597d\u7684\u4e00\u81f4\u6027\u3002\u7814\u7a76\u63ed\u793a\u4e86SiO2\u548cSi3N4\u4e2d\u78b3\u5c42\u5f62\u6210\u7684\u72ec\u7279\u673a\u5236\uff0c\u8fd9\u5f52\u56e0\u4e8eSiO2\u4e2d\u78b3-\u6c27\u526f\u4ea7\u7269\u8f83\u9ad8\u7684\u6325\u53d1\u6027\u4ee5\u53caSi3N4\u4e2d\u78b3-\u6c2e\u7269\u79cd\u5f62\u6210\u53d7\u6291\u5236\u3002", "conclusion": "\u8be5\u8ba1\u7b97\u6846\u67b6\u80fd\u591f\u5b9a\u91cf\u9884\u6d4b\u7b49\u79bb\u5b50\u4f53\u66b4\u9732\u4e0b\u7684\u539f\u5b50\u5c3a\u5ea6\u8868\u9762\u6539\u6027\uff0c\u5e76\u4e3a\u4e0e\u591a\u5c3a\u5ea6\u8fc7\u7a0b\u5efa\u6a21\u76f8\u7ed3\u5408\u5960\u5b9a\u57fa\u7840\uff0c\u4ece\u800c\u4e3a\u534a\u5bfc\u4f53\u5236\u9020\u8fc7\u7a0b\u63d0\u4f9b\u89c1\u89e3\u3002"}}
{"id": "2508.00047", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00047", "abs": "https://arxiv.org/abs/2508.00047", "authors": ["Yuan-Cheng Yu", "Yen-Chieh Ouyang", "Chun-An Lin"], "title": "TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection", "comment": "11 pages, 2 figures", "summary": "Time-series anomaly detection plays a central role across a wide range of\napplication domains. With the increasing proliferation of the Internet of\nThings (IoT) and smart manufacturing, time-series data has dramatically\nincreased in both scale and dimensionality. This growth has exposed the\nlimitations of traditional statistical methods in handling the high\nheterogeneity and complexity of such data. Inspired by the recent success of\nlarge language models (LLMs) in multimodal tasks across language and vision\ndomains, we propose a novel unsupervised anomaly detection framework: A\nTri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly\nDetection (TriP-LLM). TriP-LLM integrates local and global temporal features\nthrough a tri-branch design-Patching, Selection, and Global-to encode the input\ntime series into patch-wise tokens, which are then processed by a frozen,\npretrained LLM. A lightweight patch-wise decoder reconstructs the input, from\nwhich anomaly scores are derived. We evaluate TriP-LLM on several public\nbenchmark datasets using PATE, a recently proposed threshold-free evaluation\nmetric, and conduct all comparisons within a unified open-source framework to\nensure fairness. Experimental results show that TriP-LLM consistently\noutperforms recent state-of-the-art methods across all datasets, demonstrating\nstrong detection capabilities. Furthermore, through extensive ablation studies,\nwe verify the substantial contribution of the LLM to the overall architecture.\nCompared to LLM-based approaches using Channel Independence (CI) patch\nprocessing, TriP-LLM achieves significantly lower memory consumption, making it\nmore suitable for GPU memory-constrained environments. All code and model\ncheckpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git", "AI": {"tldr": "TriP-LLM \u662f\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u5b83\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u5185\u5b58\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7269\u8054\u7f51\u548c\u667a\u80fd\u5236\u9020\u4e2d\u65e5\u76ca\u589e\u957f\u7684\u9ad8\u5f02\u8d28\u6027\u548c\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u53d7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u6210\u529f\u7684\u542f\u53d1\uff0c\u9700\u8981\u65b0\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a TriP-LLM \u7684\u4e09\u5206\u652f\u8865\u4e01\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u201c\u8865\u4e01\u201d\u3001\u201c\u9009\u62e9\u201d\u548c\u201c\u5168\u5c40\u201d\u4e09\u4e2a\u5206\u652f\u6765\u6574\u5408\u5c40\u90e8\u548c\u5168\u5c40\u65f6\u95f4\u7279\u5f81\uff0c\u5c06\u8f93\u5165\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u4e3a\u8865\u4e01\u5757\u6807\u8bb0\uff0c\u5e76\u7531\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u3002\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u8865\u4e01\u5757\u89e3\u7801\u5668\u7528\u4e8e\u91cd\u5efa\u8f93\u5165\uff0c\u5e76\u4ece\u4e2d\u5bfc\u51fa\u5f02\u5e38\u5206\u6570\u3002", "result": "TriP-LLM \u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u6700\u8fd1\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u516c\u5e73\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTriP-LLM \u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u68c0\u6d4b\u80fd\u529b\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6574\u4f53\u67b6\u6784\u7684\u663e\u8457\u8d21\u732e\u3002\u4e0e\u4f7f\u7528\u901a\u9053\u72ec\u7acb\u6027\uff08CI\uff09\u8865\u4e01\u5904\u7406\u7684\u57fa\u4e8e LLM \u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cTriP-LLM \u7684\u5185\u5b58\u6d88\u8017\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "TriP-LLM \u6846\u67b6\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u5185\u5b58\u6d88\u8017\u8f83\u4f4e\uff0c\u9002\u7528\u4e8e GPU \u5185\u5b58\u53d7\u9650\u7684\u73af\u5883\u3002"}}
{"id": "2508.00217", "categories": ["cs.CL", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00217", "abs": "https://arxiv.org/abs/2508.00217", "authors": ["Xiaofeng Wu", "Alan Ritter", "Wei Xu"], "title": "Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges", "comment": null, "summary": "Tables have gained significant attention in large language models (LLMs) and\nmultimodal large language models (MLLMs) due to their complex and flexible\nstructure. Unlike linear text inputs, tables are two-dimensional, encompassing\nformats that range from well-structured database tables to complex,\nmulti-layered spreadsheets, each with different purposes. This diversity in\nformat and purpose has led to the development of specialized methods and tasks,\ninstead of universal approaches, making navigation of table understanding tasks\nchallenging. To address these challenges, this paper introduces key concepts\nthrough a taxonomy of tabular input representations and an introduction of\ntable understanding tasks. We highlight several critical gaps in the field that\nindicate the need for further research: (1) the predominance of\nretrieval-focused tasks that require minimal reasoning beyond mathematical and\nlogical operations; (2) significant challenges faced by models when processing\ncomplex table structures, large-scale tables, length context, or multi-table\nscenarios; and (3) the limited generalization of models across different\ntabular representations and formats.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u8868\u683c\u6570\u636e\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u8868\u683c\u8868\u793a\u5206\u7c7b\u548c\u7406\u89e3\u4efb\u52a1\uff0c\u5e76\u6307\u51fa\u4e86\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u3001\u5927\u89c4\u6a21\u8868\u683c\u4ee5\u53ca\u8de8\u4e0d\u540c\u683c\u5f0f\u6cdb\u5316\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "motivation": "\u9274\u4e8e\u8868\u683c\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\uff0c\u9700\u8981\u63d0\u51fa\u4e13\u95e8\u7684\u65b9\u6cd5\u548c\u4efb\u52a1\u6765\u5e94\u5bf9\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u901a\u7528\u65b9\u6cd5\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u8fdb\u884c\u7814\u7a76\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5bf9\u8868\u683c\u8f93\u5165\u8868\u793a\u8fdb\u884c\u5206\u7c7b\u548c\u4ecb\u7ecd\u8868\u683c\u7406\u89e3\u4efb\u52a1\u6765\u5206\u6790\u8868\u683c\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u548c\u6311\u6218\u3002", "result": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5f53\u524d\u6a21\u578b\u5728\u8868\u683c\u7406\u89e3\u65b9\u9762\u5b58\u5728\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u5305\u62ec\u68c0\u7d22\u4efb\u52a1\u7684\u5c40\u9650\u6027\u3001\u5904\u7406\u590d\u6742\u8868\u683c\u7ed3\u6784\u548c\u5927\u89c4\u6a21\u6570\u636e\u7684\u6311\u6218\u4ee5\u53ca\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5bf9\u8868\u683c\u8f93\u5165\u8868\u793a\u8fdb\u884c\u5206\u7c7b\u5e76\u4ecb\u7ecd\u8868\u683c\u7406\u89e3\u4efb\u52a1\uff0c\u5f3a\u8c03\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u8868\u683c\u7ed3\u6784\u3001\u5927\u89c4\u6a21\u8868\u683c\u3001\u957f\u4e0a\u4e0b\u6587\u6216\u591a\u8868\u683c\u573a\u666f\u4ee5\u53ca\u8de8\u4e0d\u540c\u8868\u683c\u8868\u793a\u548c\u683c\u5f0f\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002"}}
{"id": "2508.00143", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.00143", "abs": "https://arxiv.org/abs/2508.00143", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Kenneth R. Koedinger"], "title": "Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation", "comment": "Accepted for presentation at NCME AIME-Con 2025", "summary": "Humans can be notoriously imperfect evaluators. They are often biased,\nunreliable, and unfit to define \"ground truth.\" Yet, given the surging need to\nproduce large amounts of training data in educational applications using AI,\ntraditional inter-rater reliability (IRR) metrics like Cohen's kappa remain\ncentral to validating labeled data. IRR remains a cornerstone of many machine\nlearning pipelines for educational data. Take, for example, the classification\nof tutors' moves in dialogues or labeling open responses in machine-graded\nassessments. This position paper argues that overreliance on human IRR as a\ngatekeeper for annotation quality hampers progress in classifying data in ways\nthat are valid and predictive in relation to improving learning. To address\nthis issue, we highlight five examples of complementary evaluation methods,\nsuch as multi-label annotation schemes, expert-based approaches, and\nclose-the-loop validity. We argue that these approaches are in a better\nposition to produce training data and subsequent models that produce improved\nstudent learning and more actionable insights than IRR approaches alone. We\nalso emphasize the importance of external validity, for example, by\nestablishing a procedure of validating tutor moves and demonstrating that it\nworks across many categories of tutor actions (e.g., providing hints). We call\non the field to rethink annotation quality and ground truth--prioritizing\nvalidity and educational impact over consensus alone.", "AI": {"tldr": "\u4eba\u7c7b\u8bc4\u4f30\u8005\u5b58\u5728\u7f3a\u9677\uff0c\u4f46IRR\u6307\u6807\u5728AI\u6559\u80b2\u5e94\u7528\u4e2d\u4ecd\u7528\u4e8e\u9a8c\u8bc1\u6570\u636e\u3002\u672c\u6587\u8ba4\u4e3a\u8fc7\u5ea6\u4f9d\u8d56IRR\u4f1a\u963b\u788d\u8fdb\u5c55\uff0c\u5e76\u63d0\u51fa\u4e94\u79cd\u4e92\u8865\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5982\u591a\u6807\u7b7e\u6807\u6ce8\u3001\u4e13\u5bb6\u65b9\u6cd5\u548c\u95ed\u73af\u9a8c\u8bc1\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u5206\u7c7b\u7684\u6709\u6548\u6027\u548c\u6559\u80b2\u5f71\u54cd\u529b\u3002", "motivation": "\u968f\u7740\u6559\u80b2\u5e94\u7528\u4e2dAI\u5bf9\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u9700\u6c42\u6fc0\u589e\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u4eba\u7c7b\u8bc4\u4f30\u8005\u4fe1\u5ea6\uff08IRR\uff09\u4f5c\u4e3a\u6807\u6ce8\u8d28\u91cf\u7684\u95e8\u69db\uff0c\u963b\u788d\u4e86\u5728\u6570\u636e\u5206\u7c7b\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u8fd9\u4e9b\u5206\u7c7b\u5728\u4e0e\u6539\u5584\u5b66\u4e60\u76f8\u5173\u65b9\u9762\u662f\u6709\u6548\u4e14\u5177\u6709\u9884\u6d4b\u6027\u7684\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e94\u79cd\u4e92\u8865\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f8b\u5982\u591a\u6807\u7b7e\u6807\u6ce8\u65b9\u6848\u3001\u57fa\u4e8e\u4e13\u5bb6\u7684\u65b9\u6cd5\u548c\u95ed\u73af\u9a8c\u8bc1\u3002", "result": "\u672c\u6587\u8ba4\u4e3a\uff0c\u4e0e\u5355\u72ec\u7684IRR\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u4e3a\u8bad\u7ec3\u6570\u636e\u4ee5\u53ca\u540e\u7eed\u80fd\u6539\u5584\u5b66\u751f\u5b66\u4e60\u548c\u63d0\u4f9b\u66f4\u53ef\u64cd\u4f5c\u89c1\u89e3\u7684\u6a21\u578b\u63d0\u4f9b\u652f\u6301\u3002\u6587\u7ae0\u8fd8\u5f3a\u8c03\u4e86\u5916\u90e8\u6709\u6548\u6027\u7684\u91cd\u8981\u6027\uff0c\u4f8b\u5982\uff0c\u901a\u8fc7\u5efa\u7acb\u4e00\u4e2a\u9a8c\u8bc1\u5bfc\u5e08\u884c\u4e3a\u7684\u7a0b\u5e8f\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u8bb8\u591a\u7c7b\u522b\u7684\u5bfc\u5e08\u884c\u4e3a\uff08\u4f8b\u5982\u63d0\u4f9b\u63d0\u793a\uff09\u4e2d\u90fd\u6709\u6548\u3002", "conclusion": "\u7814\u7a76\u8005\u5e94\u5f53\u91cd\u65b0\u601d\u8003\u6807\u6ce8\u8d28\u91cf\u548c\u771f\u5b9e\u6027\u6807\u51c6\uff0c\u4f18\u5148\u8003\u8651\u6709\u6548\u6027\u548c\u6559\u80b2\u5f71\u54cd\u529b\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u5171\u8bc6\u3002"}}
{"id": "2508.00051", "categories": ["quant-ph", "cond-mat.stat-mech", "hep-th", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2508.00051", "abs": "https://arxiv.org/abs/2508.00051", "authors": ["Neil Dowling", "Jacopo De Nardis", "Markus Heinrich", "Xhek Turkeshi", "Silvia Pappalardi"], "title": "Free Independence and Unitary Design from Random Matrix Product Unitaries", "comment": "15+10 pages. Many figures. Comments welcome", "summary": "Understanding how complex quantum systems emulate randomness is central to\nquantum chaos, thermalization, and information theory. In one setting,\nout-of-time-ordered correlators (OTOCs) have recently been shown to probe\nasymptotic freeness between Heisenberg operators: the non-commutative\ngeneralization of statistical independence. In a distinct research direction,\nthe concept of approximate unitary designs have led to efficient constructions\nof unitaries that look random according to forward-in-time protocols. Bridging\nthese two concepts, in this work we study the emergence of freeness from a\nrandom matrix product unitary (RMPU) ensemble. We prove that, with only\npolynomial bond dimension, these unitaries reproduce Haar values of\nhigher-order OTOCs for local, finite-trace observables -- precisely the\nobservables that lead to thermal correlations in chaotic many-body systems\naccording to the eigenstate thermalization hypothesis. The RMPU ensemble\nprovably forms a unitary design, but, we argue, this does not account for\naverage OTOC behavior and therefore the emergence of freeness. We further\ncompute the ensemble's frame potential exactly to second order, showing\nconvergence to Haar values also with polynomial deviations, indicating that\nfreeness is also reached on-average for global observables. On the other hand,\nto reproduce the Haar-like OTOC value for local, traceless observables, the\nconsidered ensemble requires volume-law operator entanglement. Such\ncorrelations therefore lie beyond the paradigm of random unitary features which\ncan be replicated efficiently. Our results highlight the need to refine\nprevious notions of unitary designs in the context of operator dynamics,\nguiding us towards protocols for genuine quantum advantage while shedding light\non the emergent complexity of chaotic many-body systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u968f\u673a\u77e9\u9635\u4e58\u79ef\u9149\uff08RMPU\uff09\u7cfb\u7efc\u5728\u591a\u9879\u5f0f\u7ef4\u5ea6\u4e0b\u53ef\u4ee5\u9ad8\u6548\u590d\u73b0\u91cf\u5b50\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u7684\u9ad8\u9636OTOCs\uff0c\u63ed\u793a\u4e86\u5176\u9149\u8bbe\u8ba1\u6027\u8d28\uff0c\u5e76\u9610\u660e\u4e86\u81ea\u7531\u5ea6\u6d8c\u73b0\u7684\u673a\u5236\u3002\u4f46\u7814\u7a76\u4e5f\u6307\u51fa\uff0c\u8981\u5b8c\u5168\u63cf\u8ff0\u6240\u6709\u53ef\u89c2\u6d4b\u91cf\uff0c\u9700\u8981\u8d85\u8d8a\u73b0\u6709\u9149\u8bbe\u8ba1\u8303\u7574\u7684\u590d\u6742\u6027\u3002", "motivation": "\u7406\u89e3\u590d\u6742\u91cf\u5b50\u7cfb\u7edf\u5982\u4f55\u6a21\u62df\u968f\u673a\u6027\u662f\u91cf\u5b50\u6df7\u6c8c\u3001\u70ed\u5316\u548c\u4fe1\u606f\u7406\u8bba\u7684\u6838\u5fc3\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u8fde\u63a5\u201c\u65f6\u95f4\u53cd\u6f14\u5bf9\u79f0\u6027\u7834\u7f3a\u201d\u548c\u201c\u8fd1\u4f3c\u9149\u8bbe\u8ba1\u201d\u8fd9\u4e24\u4e2a\u7814\u7a76\u65b9\u5411\uff0c\u901a\u8fc7\u7814\u7a76\u968f\u673a\u77e9\u9635\u4e58\u79ef\u9149\uff08RMPU\uff09\u7cfb\u7efc\uff0c\u6765\u7406\u89e3\u81ea\u7531\u5ea6\u5982\u4f55\u4ece\u5176\u4e2d\u6d8c\u73b0\uff0c\u5e76\u9610\u660e\u5176\u4e0e\u91cf\u5b50\u6df7\u6c8c\u591a\u4f53\u7cfb\u7edf\u70ed\u5316\u8fc7\u7a0b\u7684\u5173\u7cfb\u3002", "method": "\u672c\u7814\u7a76\u4e3b\u8981\u91c7\u7528\u7406\u8bba\u5206\u6790\u548c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86\u968f\u673a\u77e9\u9635\u4e58\u79ef\u9149\uff08RMPU\uff09\u7cfb\u7efc\u7684\u6027\u8d28\u3002\u901a\u8fc7\u8bc1\u660eRMPU\u7cfb\u7efc\u80fd\u591f\u590d\u73b0\u9ad8\u9636OTOCs\u7684Haar\u503c\uff0c\u5e76\u5206\u6790\u5176\u9149\u8bbe\u8ba1\u7684\u6027\u8d28\uff0c\u4ee5\u53ca\u7cbe\u786e\u8ba1\u7b97\u5176\u4e8c\u9636\u6846\u67b6\u52bf\uff0c\u6765\u63a2\u7a76\u81ea\u7531\u5ea6\u6d8c\u73b0\u7684\u673a\u5236\u3002\u540c\u65f6\uff0c\u7814\u7a76\u4e5f\u63a2\u8ba8\u4e86\u590d\u73b0\u7279\u5b9a\u53ef\u89c2\u6d4b\u91cf\u6240\u9700\u7684\u6761\u4ef6\uff0c\u5982\u4f53\u5b9a\u5f8b\u7b97\u7b26\u7ea0\u7f20\u3002", "result": "\u672c\u7814\u7a76\u8bc1\u660e\uff0c\u5728\u591a\u9879\u5f0f\u952e\u7ef4\u5ea6\u4e0b\uff0cRMPU\u7cfb\u7efc\u53ef\u4ee5\u590d\u73b0\u9ad8\u9636OTOCs\u7684Haar\u503c\uff0c\u8fd9\u4e0e\u6df7\u6c8c\u591a\u4f53\u7cfb\u7edf\u4e2d\u7684\u70ed\u5316\u8fc7\u7a0b\u4e00\u81f4\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0cRMPU\u7cfb\u7efc\u672c\u8eab\u6784\u6210\u4e00\u4e2a\u9149\u8bbe\u8ba1\uff0c\u4f46\u5176\u9149\u8bbe\u8ba1\u6027\u8d28\u5e76\u4e0d\u80fd\u5b8c\u5168\u89e3\u91ca\u5e73\u5747OTOCs\u7684\u884c\u4e3a\u548c\u81ea\u7531\u5ea6\u7684\u6d8c\u73b0\u3002\u901a\u8fc7\u7cbe\u786e\u8ba1\u7b97\u4e8c\u9636\u6846\u67b6\u52bf\uff0c\u7814\u7a76\u8868\u660eRMPU\u7cfb\u7efc\u5728\u5e73\u5747\u610f\u4e49\u4e0b\u4e5f\u80fd\u8fbe\u5230Haar\u503c\uff0c\u5373\u4f7f\u5b58\u5728\u591a\u9879\u5f0f\u504f\u5dee\u3002\u7136\u800c\uff0c\u8981\u590d\u73b0\u65e0\u8ff9\u5c40\u90e8\u53ef\u89c2\u6d4b\u91cf\uff0c\u9700\u8981\u6ee1\u8db3\u4f53\u5b9a\u5f8b\u7b97\u7b26\u7ea0\u7f20\uff0c\u8fd9\u8d85\u51fa\u4e86\u9ad8\u6548\u53ef\u590d\u73b0\u7684\u968f\u673a\u9149\u7279\u5f81\u8303\u7574\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u7814\u7a76\u968f\u673a\u77e9\u9635\u4e58\u79ef\u9149\uff08RMPU\uff09\u7cfb\u7efc\uff0c\u5728\u591a\u9879\u5f0f\u952e\u7ef4\u5ea6\u4e0b\uff0c\u6210\u529f\u590d\u73b0\u4e86\u9ad8\u9636OTOCs\u7684Haar\u503c\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u9149\u8bbe\u8ba1\u7684\u6027\u8d28\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0cRMPU\u7cfb\u7efc\u5728\u4e8c\u9636\u6846\u67b6\u52bf\u7cbe\u786e\u8ba1\u7b97\u4e0a\uff0c\u4e5f\u80fd\u5728\u591a\u9879\u5f0f\u504f\u5dee\u4e0b\u6536\u655b\u5230Haar\u503c\u3002\u7136\u800c\uff0c\u4e3a\u4e86\u590d\u73b0\u65e0\u8ff9\u5c40\u90e8\u53ef\u89c2\u6d4b\u91cf\uff0cRMPU\u7cfb\u7efc\u9700\u8981\u6ee1\u8db3\u4f53\u5b9a\u5f8b\u7b97\u7b26\u7ea0\u7f20\uff0c\u8fd9\u8d85\u51fa\u4e86\u968f\u673a\u9149\u7279\u5f81\u7684\u53ef\u590d\u73b0\u8303\u56f4\u3002\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u7b97\u7b26\u52a8\u529b\u5b66\u80cc\u666f\u4e0b\uff0c\u9700\u8981\u5b8c\u5584\u9149\u8bbe\u8ba1\u7684\u6982\u5ff5\uff0c\u4e3a\u5b9e\u73b0\u771f\u6b63\u7684\u91cf\u5b50\u4f18\u52bf\u63d0\u4f9b\u6307\u5bfc\uff0c\u5e76\u9610\u660e\u6df7\u6c8c\u591a\u4f53\u7cfb\u7edf\u6d8c\u73b0\u590d\u6742\u6027\u7684\u673a\u5236\u3002"}}
{"id": "2508.00724", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.00724", "abs": "https://arxiv.org/abs/2508.00724", "authors": ["Boyu Li", "Zhengchen Li", "Weimin Wu", "Mengchu Zhou"], "title": "Petri Net Modeling and Deadlock-Free Scheduling of Attachable Heterogeneous AGV Systems", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The increasing demand for automation and flexibility drives the widespread\nadoption of heterogeneous automated guided vehicles (AGVs). This work intends\nto investigate a new scheduling problem in a material transportation system\nconsisting of attachable heterogeneous AGVs, namely carriers and shuttles. They\ncan flexibly attach to and detach from each other to cooperatively execute\ncomplex transportation tasks. While such collaboration enhances operational\nefficiency, the attachment-induced synchronization and interdependence render\nthe scheduling coupled and susceptible to deadlock. To tackle this challenge,\nPetri nets are introduced to model AGV schedules, well describing the\nconcurrent and sequential task execution and carrier-shuttle synchronization.\nBased on Petri net theory, a firing-driven decoding method is proposed, along\nwith deadlock detection and prevention strategies to ensure deadlock-free\nschedules. Furthermore, a Petri net-based metaheuristic is developed in an\nadaptive large neighborhood search framework and incorporates an effective\nacceleration method to enhance computational efficiency. Finally, numerical\nexperiments using real-world industrial data validate the effectiveness of the\nproposed algorithm against the scheduling policy applied in engineering\npractice, an exact solver, and four state-of-the-art metaheuristics. A\nsensitivity analysis is also conducted to provide managerial insights.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u5f02\u6784AGV\uff08\u8f7d\u6ce2\u548c\u7a7f\u68ad\u8f66\uff09\u7684\u65e0\u6b7b\u9501\u8c03\u5ea6\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u57fa\u4e8ePetri\u7f51\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u81ea\u9002\u5e94\u5927\u90bb\u57df\u641c\u7d22\u8fdb\u884c\u4f18\u5316\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u81ea\u52a8\u5316\u548c\u7075\u6d3b\u6027\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\uff0c\u4ee5\u53ca\u5f02\u6784\u81ea\u52a8\u5bfc\u5f15\u8f66\uff08AGV\uff09\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u8026\u5408\u548c\u6613\u6b7b\u9501\u7684\u8c03\u5ea6\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u53ef\u9644\u52a0\u5f02\u6784AGV\uff08\u5305\u62ec\u8f7d\u6ce2\u548c\u7a7f\u68ad\u8f66\uff09\u7684\u8c03\u5ea6\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePetri\u7f51\u7684\u8c03\u5ea6\u6a21\u578b\uff0c\u4f7f\u7528\u9a71\u52a8\u89e6\u53d1\u89e3\u7801\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86\u6b7b\u9501\u68c0\u6d4b\u4e0e\u9884\u9632\u7b56\u7565\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8ePetri\u7f51\u7684\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u81ea\u9002\u5e94\u5927\u90bb\u57df\u641c\u7d22\u6846\u67b6\u4e2d\uff0c\u8f85\u4ee5\u52a0\u901f\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u5de5\u4e1a\u6570\u636e\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d\uff0c\u88ab\u8bc1\u660e\u6bd4\u5de5\u7a0b\u5b9e\u8df5\u4e2d\u7684\u8c03\u5ea6\u7b56\u7565\u3001\u7cbe\u786e\u6c42\u89e3\u5668\u548c\u56db\u79cd\u6700\u5148\u8fdb\u7684\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u66f4\u6709\u6548\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u654f\u611f\u6027\u5206\u6790\u4ee5\u63d0\u4f9b\u7ba1\u7406\u89c1\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8ePetri\u7f51\u7684\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u8c03\u5ea6\u95ee\u9898\u4e0a\u5c55\u73b0\u4e86\u6709\u6548\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u786e\u4fdd\u65e0\u6b7b\u9501\u7684\u8c03\u5ea6\uff0c\u5728\u4e0e\u5de5\u7a0b\u5b9e\u8df5\u3001\u7cbe\u786e\u6c42\u89e3\u5668\u548c\u73b0\u6709\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u6bd4\u8f83\u4e2d\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u7ba1\u7406\u5b66\u89c1\u89e3\u3002"}}
{"id": "2508.00800", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00800", "abs": "https://arxiv.org/abs/2508.00800", "authors": ["Rui Chen", "Wen-Xuan Long", "Bing-Qian Wang", "Yuan He", "Rui-Jin Sun", "Nan Cheng", "Gan Zheng", "Dusit Niyato"], "title": "Multibeam High Throughput Satellite: Hardware Foundation, Resource Allocation, and Precoding", "comment": "38 pages, 18 figures", "summary": "With its wide coverage and uninterrupted service, satellite communication is\na critical technology for next-generation 6G communications. High throughput\nsatellite (HTS) systems, utilizing multipoint beam and frequency multiplexing\ntechniques, enable satellite communication capacity of up to Tbps to meet the\ngrowing traffic demand. Therefore, it is imperative to review\nthe-state-of-the-art of multibeam HTS systems and identify their associated\nchallenges and perspectives. Firstly, we summarize the multibeam HTS hardware\nfoundations, including ground station systems, on-board payloads, and user\nterminals. Subsequently, we review the flexible on-board radio resource\nallocation approaches of bandwidth, power, time slot, and joint allocation\nschemes of HTS systems to optimize resource utilization and cater to\nnon-uniform service demand. Additionally, we survey multibeam precoding methods\nfor the HTS system to achieve full-frequency reuse and interference\ncancellation, which are classified according to different deployments such as\nsingle gateway precoding, multiple gateway precoding, on-board precoding, and\nhybrid on-board/on-ground precoding. Finally, we disscuss the challenges\nrelated to Q/V band link outage, time and frequency synchronization of\ngateways, the accuracy of channel state information (CSI), payload light-weight\ndevelopment, and the application of deep learning (DL). Research on these\ntopics will contribute to enhancing the performance of HTS systems and finally\ndelivering high-speed data to areas underserved by terrestrial networks.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u6ce2\u675f\u9ad8\u901a\u91cf\u536b\u661f\uff08HTS\uff09\u901a\u4fe1\u7cfb\u7edf\uff0c\u6db5\u76d6\u4e86\u786c\u4ef6\u3001\u8d44\u6e90\u5206\u914d\u548c\u9884\u7f16\u7801\u6280\u672f\uff0c\u5e76\u63a2\u8ba8\u4e86Q/V\u9891\u6bb5\u3001\u7f51\u5173\u540c\u6b65\u3001\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u3001\u8f7b\u91cf\u5316\u8f7d\u8377\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u7b49\u65b9\u9762\u7684\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u77406G\u901a\u4fe1\u6280\u672f\u7684\u53d1\u5c55\uff0c\u536b\u661f\u901a\u4fe1\u51ed\u501f\u5176\u5e7f\u6cdb\u8986\u76d6\u548c\u4e0d\u95f4\u65ad\u670d\u52a1\u7684\u4f18\u52bf\uff0c\u6210\u4e3a\u5173\u952e\u6280\u672f\u4e4b\u4e00\u3002\u9ad8\u901a\u91cf\u536b\u661f\uff08HTS\uff09\u7cfb\u7edf\u5229\u7528\u591a\u70b9\u6ce2\u675f\u548c\u9891\u5206\u590d\u7528\u6280\u672f\uff0c\u80fd\u591f\u63d0\u4f9b\u9ad8\u8fbeTbps\u7684\u901a\u4fe1\u5bb9\u91cf\uff0c\u4ee5\u6ee1\u8db3\u65e5\u76ca\u589e\u957f\u7684\u6d41\u91cf\u9700\u6c42\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5bf9\u591a\u6ce2\u675fHTS\u7cfb\u7edf\u8fdb\u884c\u7efc\u8ff0\uff0c\u5e76\u8bc6\u522b\u5176\u9762\u4e34\u7684\u6311\u6218\u548c\u53d1\u5c55\u524d\u666f\u3002", "method": "\u672c\u6587\u9996\u5148\u603b\u7ed3\u4e86\u591a\u6ce2\u675fHTS\u7cfb\u7edf\u7684\u786c\u4ef6\u57fa\u7840\uff0c\u5305\u62ec\u5730\u9762\u7ad9\u7cfb\u7edf\u3001\u5728\u8f68\u8f7d\u8377\u548c\u7528\u6237\u7ec8\u7aef\u3002\u968f\u540e\uff0c\u56de\u987e\u4e86HTS\u7cfb\u7edf\u7684\u7075\u6d3b\u5728\u8f68\u65e0\u7ebf\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\uff0c\u5305\u62ec\u5e26\u5bbd\u3001\u529f\u7387\u3001\u65f6\u9699\u4ee5\u53ca\u8054\u5408\u5206\u914d\u65b9\u6848\uff0c\u4ee5\u4f18\u5316\u8d44\u6e90\u5229\u7528\u7387\u5e76\u6ee1\u8db3\u975e\u5747\u5300\u7684\u670d\u52a1\u9700\u6c42\u3002\u6b64\u5916\uff0c\u8fd8\u8c03\u7814\u4e86HTS\u7cfb\u7edf\u7684\u591a\u6ce2\u675f\u9884\u7f16\u7801\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u5168\u9891\u6bb5\u590d\u7528\u548c\u5e72\u6270\u6d88\u9664\uff0c\u5e76\u6839\u636e\u4e0d\u540c\u90e8\u7f72\uff08\u5982\u5355\u4e2a\u7f51\u5173\u9884\u7f16\u7801\u3001\u591a\u4e2a\u7f51\u5173\u9884\u7f16\u7801\u3001\u5728\u8f68\u9884\u7f16\u7801\u548c\u6df7\u5408\u5728\u8f68/\u5730\u9762\u9884\u7f16\u7801\uff09\u8fdb\u884c\u4e86\u5206\u7c7b\u3002", "result": "\u672c\u6587\u5bf9\u591a\u6ce2\u675fHTS\u7cfb\u7edf\u7684\u786c\u4ef6\u57fa\u7840\u3001\u8d44\u6e90\u5206\u914d\u548c\u9884\u7f16\u7801\u6280\u672f\u8fdb\u884c\u4e86\u7efc\u8ff0\uff0c\u5e76\u8ba8\u8bba\u4e86Q/V\u9891\u6bb5\u94fe\u8def\u3001\u7f51\u5173\u540c\u6b65\u3001\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u7cbe\u5ea6\u3001\u8f7b\u91cf\u5316\u8f7d\u8377\u4ee5\u53ca\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u5e94\u7528\u7b49\u5173\u952e\u6311\u6218\u3002\u8fd9\u4e9b\u7814\u7a76\u5c06\u6709\u52a9\u4e8e\u63d0\u5347HTS\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u6700\u7ec8\u4e3a\u9646\u5730\u7f51\u7edc\u670d\u52a1\u4e0d\u8db3\u7684\u5730\u533a\u63d0\u4f9b\u9ad8\u901f\u6570\u636e\u4f20\u8f93\u3002", "conclusion": "\u536b\u661f\u901a\u4fe1\u662f\u4e0b\u4e00\u4ee36G\u901a\u4fe1\u7684\u5173\u952e\u6280\u672f\uff0c\u9ad8\u901a\u91cf\u536b\u661f\uff08HTS\uff09\u7cfb\u7edf\u901a\u8fc7\u591a\u70b9\u6ce2\u675f\u548c\u9891\u5206\u590d\u7528\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbeTbps\u7684\u536b\u661f\u901a\u4fe1\u5bb9\u91cf\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u65ad\u589e\u957f\u7684\u6d41\u91cf\u9700\u6c42\u3002\u672c\u7bc7\u8bba\u6587\u5bf9\u591a\u6ce2\u675fHTS\u7cfb\u7edf\u8fdb\u884c\u4e86\u7efc\u8ff0\uff0c\u603b\u7ed3\u4e86\u5176\u786c\u4ef6\u57fa\u7840\u3001\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\u548c\u9884\u7f16\u7801\u6280\u672f\uff0c\u5e76\u8ba8\u8bba\u4e86Q/V\u9891\u6bb5\u94fe\u8def\u3001\u7f51\u5173\u540c\u6b65\u3001\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u7cbe\u5ea6\u3001\u8f7b\u91cf\u5316\u8f7d\u8377\u4ee5\u53ca\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u5e94\u7528\u7b49\u65b9\u9762\u7684\u6311\u6218\u4e0e\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.00362", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00362", "abs": "https://arxiv.org/abs/2508.00362", "authors": ["Zhenghan Chen", "Haodong Zhang", "Dongqi Wang", "Jiyu Yu", "Haocheng Xu", "Yue Wang", "Rong Xiong"], "title": "A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot", "comment": null, "summary": "Motion imitation is a pivotal and effective approach for humanoid robots to\nachieve a more diverse range of complex and expressive movements, making their\nperformances more human-like. However, the significant differences in\nkinematics and dynamics between humanoid robots and humans present a major\nchallenge in accurately imitating motion while maintaining balance. In this\npaper, we propose a novel whole-body motion imitation framework for a full-size\nhumanoid robot. The proposed method employs contact-aware whole-body motion\nretargeting to mimic human motion and provide initial values for reference\ntrajectories, and the non-linear centroidal model predictive controller ensures\nthe motion accuracy while maintaining balance and overcoming external\ndisturbances in real time. The assistance of the whole-body controller allows\nfor more precise torque control. Experiments have been conducted to imitate a\nvariety of human motions both in simulation and in a real-world humanoid robot.\nThese experiments demonstrate the capability of performing with accuracy and\nadaptability, which validates the effectiveness of our approach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u6a21\u4eff\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u63a5\u89e6\u611f\u77e5\u8fd0\u52a8\u91cd\u5b9a\u5411\u548c\u975e\u7ebf\u6027\u91cd\u5fc3\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u5e73\u8861\u7684\u540c\u65f6\u7cbe\u786e\u6a21\u4eff\u4eba\u7c7b\u8fd0\u52a8\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8868\u73b0\u66f4\u50cf\u4eba\u7c7b\uff0c\u9700\u8981\u901a\u8fc7\u8fd0\u52a8\u6a21\u4eff\u6765\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u3001\u66f4\u590d\u6742\u3001\u66f4\u5177\u8868\u73b0\u529b\u7684\u8fd0\u52a8\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u5728\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u4e0a\u7684\u663e\u8457\u5dee\u5f02\uff0c\u5728\u4fdd\u6301\u5e73\u8861\u7684\u540c\u65f6\u7cbe\u786e\u6a21\u4eff\u8fd0\u52a8\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5168\u8eab\u4f53\u8fd0\u52a8\u6a21\u4eff\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u63a5\u89e6\u611f\u77e5\u5168\u8eab\u4f53\u8fd0\u52a8\u91cd\u5b9a\u5411\u6765\u6a21\u4eff\u4eba\u7c7b\u8fd0\u52a8\u5e76\u4e3a\u53c2\u8003\u8f68\u8ff9\u63d0\u4f9b\u521d\u59cb\u503c\uff0c\u5e76\u4f7f\u7528\u975e\u7ebf\u6027\u91cd\u5fc3\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u6765\u786e\u4fdd\u8fd0\u52a8\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5e73\u8861\u5e76\u5b9e\u65f6\u514b\u670d\u5916\u90e8\u5e72\u6270\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u4eba\u5f62\u673a\u5668\u4eba\u548c\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u6a21\u4eff\u5404\u79cd\u4eba\u7c7b\u8fd0\u52a8\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u7cbe\u786e\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u7cbe\u786e\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.00171", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00171", "abs": "https://arxiv.org/abs/2508.00171", "authors": ["David Restrepo", "Ira Ktena", "Maria Vakalopoulou", "Stergios Christodoulidis", "Enzo Ferrante"], "title": "On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI", "comment": "Accepted to MICCAI 2025 1st Workshop on Multimodal Large Language\n  Models (MLLMs) in Clinical Practice", "summary": "Clinical decision-making relies on the integrated analysis of medical images\nand the associated clinical reports. While Vision-Language Models (VLMs) can\noffer a unified framework for such tasks, they can exhibit strong biases toward\none modality, frequently overlooking critical visual cues in favor of textual\ninformation. In this work, we introduce Selective Modality Shifting (SMS), a\nperturbation-based approach to quantify a model's reliance on each modality in\nbinary classification tasks. By systematically swapping images or text between\nsamples with opposing labels, we expose modality-specific biases. We assess six\nopen-source VLMs-four generalist models and two fine-tuned for medical data-on\ntwo medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)\nand FairVLMed (scanning laser ophthalmoscopy). By assessing model performance\nand the calibration of every model in both unperturbed and perturbed settings,\nwe reveal a marked dependency on text input, which persists despite the\npresence of complementary visual information. We also perform a qualitative\nattention-based analysis which further confirms that image content is often\novershadowed by text details. Our findings highlight the importance of\ndesigning and evaluating multimodal medical models that genuinely integrate\nvisual and textual cues, rather than relying on single-modality signals.", "AI": {"tldr": "\u73b0\u6709\u7684\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e25\u91cd\u504f\u5411\u6587\u672c\uff0c\u5ffd\u89c6\u56fe\u50cf\u4fe1\u606f\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSMS\u7684\u65b9\u6cd5\u6765\u91cf\u5316\u8fd9\u79cd\u504f\u89c1\uff0c\u53d1\u73b0\u5728MIMIC-CXR\u548cFairVLMed\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u5904\u7406\u533b\u5b66\u5f71\u50cf\u548c\u6587\u672c\u62a5\u544a\u65f6\uff0c\u5bf9\u6587\u672c\u7684\u4f9d\u8d56\u6027\u5f88\u9ad8\uff0c\u56fe\u50cf\u4fe1\u606f\u5e38\u5e38\u88ab\u5ffd\u7565\u3002", "motivation": "\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u6574\u5408\u533b\u5b66\u5f71\u50cf\u548c\u4e34\u5e8a\u62a5\u544a\u65f6\u5b58\u5728\u504f\u5411\u5355\u4e00\u6a21\u6001\uff08\u5c24\u5176\u662f\u6587\u672c\uff09\u7684\u95ee\u9898\uff0c\u5ffd\u89c6\u4e86\u5173\u952e\u7684\u89c6\u89c9\u7ebf\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9009\u62e9\u6027\u6a21\u6001\u8f6c\u79fb\uff08SMS\uff09\u7684\u57fa\u4e8e\u6270\u52a8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u5730\u4ea4\u6362\u5177\u6709\u76f8\u53cd\u6807\u7b7e\u7684\u6837\u672c\u7684\u56fe\u50cf\u6216\u6587\u672c\uff0c\u6765\u91cf\u5316\u6a21\u578b\u5bf9\u6bcf\u4e2a\u6a21\u6001\u7684\u4f9d\u8d56\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86\u516d\u4e2a\u5f00\u6e90VLM\u5728\u4e24\u4e2a\u533b\u5b66\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u548c\u6821\u51c6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6240\u8bc4\u4f30\u7684\u6a21\u578b\u666e\u904d\u5b58\u5728\u5bf9\u6587\u672c\u8f93\u5165\u7684\u660e\u663e\u4f9d\u8d56\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u4e92\u8865\u89c6\u89c9\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u4e5f\u4f9d\u7136\u5982\u6b64\u3002\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5b9a\u6027\u5206\u6790\u4e5f\u8bc1\u5b9e\u4e86\u56fe\u50cf\u5185\u5bb9\u5e38\u5e38\u88ab\u6587\u672c\u7ec6\u8282\u6240\u63a9\u76d6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u533b\u7597\u6a21\u578b\u5728\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\u65f6\u5b58\u5728\u4e25\u91cd\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\u7684\u504f\u89c1\uff0c\u5e76\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u548c\u8bc4\u4f30\u771f\u6b63\u6574\u5408\u4e24\u79cd\u6a21\u6001\u4fe1\u53f7\u7684\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00372", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.00372", "abs": "https://arxiv.org/abs/2508.00372", "authors": ["Binayak Mukherjee", "Natalya S. Fedorova", "Jorge \u00cd\u00f1iguez-Gonz\u00e1lez"], "title": "Extrinsic nature of the polarization in hafnia ferroelectrics", "comment": "7 pages, 5 figures", "summary": "Hafnia and related fluorites defy our understanding of ferroelectricity, even\nif we restrict ourselves to the intrinsic properties of ideal crystals. Here we\nfocus on a critical puzzle, namely, the sign of the electric polarization.\nUsing first-principles simulations, we show that a polar hafnia layer with a\nfixed atomic configuration can give rise to depolarizing fields of either\npositive or negative sign, depending on the environment. This implies that (the\nsign of) the polarization in hafnia is extrinsic in nature. We explain this\nresult and discuss its relevance to other ferroelectric families.", "AI": {"tldr": "Hafnia\u7684\u6781\u5316\u7b26\u53f7\u53d6\u51b3\u4e8e\u73af\u5883\uff0c\u8fd9\u8868\u660e\u5b83\u662f\u5916\u5728\u7684\uff0c\u800c\u975e\u5185\u5728\u7684\u3002", "motivation": "Hafnia\u53ca\u5176\u76f8\u5173\u7684\u8424\u77f3\u7c7b\u6750\u6599\u5177\u6709\u7684\u94c1\u7535\u6027\u8d85\u51fa\u4e86\u6211\u4eec\u5bf9\u5176\u672c\u5f81\u7279\u6027\u7684\u7406\u89e3\uff0c\u7279\u522b\u662f\u5176\u7535\u6781\u5316\u7b26\u53f7\u3002", "method": "\u4f7f\u7528\u7b2c\u4e00\u6027\u539f\u7406\u6a21\u62df\u3002", "result": "\u6781\u5316Hafnia\u5c42\u53ef\u4ee5\u4ea7\u751f\u6b63\u6216\u8d1f\u7b26\u53f7\u7684\u53bb\u6781\u5316\u573a\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u5176\u6240\u5904\u7684\u73af\u5883\u3002 \u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u662f\u5728\u539f\u5b50\u7ed3\u6784\u56fa\u5b9a\u7684\u60c5\u51b5\u4e0b\u53d1\u751f\u7684\u3002 \u8fd9\u4e00\u7ed3\u679c\u89e3\u91ca\u4e86Hafnia\u7684\u6781\u5316\u7b26\u53f7\u5177\u6709\u5916\u5728\u6027\u7684\u539f\u56e0\u3002 \u6b64\u5916\uff0c\u8fd8\u8ba8\u8bba\u4e86\u8fd9\u4e00\u53d1\u73b0\u5bf9\u5176\u4ed6\u94c1\u7535\u5bb6\u65cf\u7684\u76f8\u5173\u6027\u3002 \u8bba\u6587\u4f5c\u8005\u8ba4\u4e3a\uff0cHafnia\u7684\u6781\u5316\u7b26\u53f7\u7684\u8fd9\u79cd\u5916\u5728\u6027\u8d28\uff0c\u53ef\u80fd\u4e5f\u5b58\u5728\u4e8e\u5176\u4ed6\u94c1\u7535\u5bb6\u65cf\u4e2d\u3002 \u8bba\u6587\u7684\u53d1\u73b0\uff0c\u6311\u6218\u4e86\u4eba\u4eec\u5bf9\u94c1\u7535\u6027\u7684\u56fa\u6709\u7406\u89e3\uff0c\u5e76\u4e3a\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002", "conclusion": "Hafnia\u7684\u6781\u5316\u7b26\u53f7\u662f\u5916\u5728\u7684\uff0c\u8fd9\u53d6\u51b3\u4e8e\u5176\u5468\u56f4\u73af\u5883\uff0c\u5e76\u4e14\u5728\u5176\u4ed6\u94c1\u7535\u5bb6\u65cf\u4e2d\u4e5f\u53ef\u80fd\u5b58\u5728\u7c7b\u4f3c\u73b0\u8c61\u3002"}}
{"id": "2508.00419", "categories": ["cs.LO", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.00419", "abs": "https://arxiv.org/abs/2508.00419", "authors": ["Varun Bharti", "Shashwat Jha", "Dhruv Kumar", "Pankaj Jalote"], "title": "Loop Invariant Generation: A Hybrid Framework of Reasoning optimised LLMs and SMT Solvers", "comment": "Under Review", "summary": "Loop invariants are essential for proving the correctness of programs with\nloops. Developing loop invariants is challenging, and fully automatic synthesis\ncannot be guaranteed for arbitrary programs. Some approaches have been proposed\nto synthesize loop invariants using symbolic techniques and more recently using\nneural approaches. These approaches are able to correctly synthesize loop\ninvariants only for subsets of standard benchmarks. In this work, we\ninvestigate whether modern, reasoning-optimized large language models can do\nbetter. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled\ngenerate-and-check pipeline with the Z3 SMT solver, using solver\ncounterexamples to iteratively guide invariant refinement. We use Code2Inv\nbenchmark, which provides C programs along with their formal preconditions and\npostconditions. On this benchmark of 133 tasks, our framework achieves 100%\ncoverage (133 out of 133), outperforming the previous best of 107 out of 133,\nwhile requiring only 1-2 model proposals per instance and 14-55 seconds of\nwall-clock time. These results demonstrate that LLMs possess latent logical\nreasoning capabilities which can help automate loop invariant synthesis. While\nour experiments target C-specific programs, this approach should be\ngeneralizable to other imperative languages.", "AI": {"tldr": "LLM\u4e0eSMT\u6c42\u89e3\u5668\u7ed3\u5408\uff0c\u5728\u5faa\u73af\u4e0d\u53d8\u5f0f\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86100%\u7684\u8986\u76d6\u7387\u3002", "motivation": "\u7814\u7a76\u73b0\u4ee3\u63a8\u7406\u4f18\u5316\u7684LLM\u5728\u81ea\u52a8\u5408\u6210\u5faa\u73af\u4e0d\u53d8\u5f0f\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u7b26\u53f7\u548c\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u5c06OpenAI\u7684O1\u3001O1-mini\u548cO3-mini\u96c6\u6210\u5230\u5e26\u6709Z3 SMT\u6c42\u89e3\u5668\u7684\u751f\u6210-\u68c0\u67e5\u6d41\u6c34\u7ebf\u4e2d\uff0c\u5e76\u5229\u7528\u6c42\u89e3\u5668\u7684\u53cd\u4f8b\u6765\u6307\u5bfc\u4e0d\u53d8\u5f0f\u7684\u8fed\u4ee3\u7ec6\u5316\u3002", "result": "\u5728\u5305\u542b133\u4e2a\u4efb\u52a1\u7684Code2Inv\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u73b0\u4e86100%\u7684\u8986\u76d6\u7387\uff08133/133\uff09\uff0c\u6bcf\u6b21\u4ec5\u97001-2\u6b21\u6a21\u578b\u5efa\u8bae\u548c14-55\u79d2\u7684\u5b9e\u9645\u8fd0\u884c\u65f6\u95f4\uff0c\u4f18\u4e8e\u4e4b\u524d\u6700\u597d\u7684107/133\u3002", "conclusion": "\u73b0\u4ee3\u63a8\u7406\u4f18\u5316\u7684LLM\u53ef\u4ee5\u901a\u8fc7\u7ed3\u5408SMT\u6c42\u89e3\u5668\u6765\u66f4\u597d\u5730\u5408\u6210\u5faa\u73af\u4e0d\u53d8\u5f0f\uff0c\u5728Code2Inv\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86100%\u7684\u8986\u76d6\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.00078", "categories": ["cs.LG", "cs.AI", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2508.00078", "abs": "https://arxiv.org/abs/2508.00078", "authors": ["Imen Mahmoud", "Andrei Velichko"], "title": "Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization", "comment": "22 pages, 5 figures", "summary": "This study proposes a novel methodological framework integrating a LightGBM\nregression model and genetic algorithm (GA) optimization to systematically\nevaluate the contribution of COVID-19-related indicators to Bitcoin return\nprediction. The primary objective was not merely to forecast Bitcoin returns\nbut rather to determine whether including pandemic-related health data\nsignificantly enhances prediction accuracy. A comprehensive dataset comprising\ndaily Bitcoin returns and COVID-19 metrics (vaccination rates,\nhospitalizations, testing statistics) was constructed. Predictive models,\ntrained with and without COVID-19 features, were optimized using GA over 31\nindependent runs, allowing robust statistical assessment. Performance metrics\n(R2, RMSE, MAE) were statistically compared through distribution overlaps and\nMann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified\nindividual feature contributions. Results indicate that COVID-19 indicators\nsignificantly improved model performance, particularly in capturing extreme\nmarket fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly\nsignificant statistically). Among COVID-19 features, vaccination metrics,\nespecially the 75th percentile of fully vaccinated individuals, emerged as\ndominant predictors. The proposed methodology extends existing financial\nanalytics tools by incorporating public health signals, providing investors and\npolicymakers with refined indicators to navigate market uncertainty during\nsystemic crises.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408 LightGBM \u548c\u9057\u4f20\u7b97\u6cd5\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528 COVID-19 \u6570\u636e\u9884\u6d4b\u6bd4\u7279\u5e01\u56de\u62a5\uff0c\u53d1\u73b0\u75ab\u60c5\u6307\u6807\uff08\u5c24\u5176\u662f\u75ab\u82d7\u63a5\u79cd\u7387\uff09\u80fd\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4e3a\u6295\u8d44\u8005\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u4e3b\u8981\u76ee\u7684\u662f\u9884\u6d4b\u6bd4\u7279\u5e01\u56de\u62a5\uff0c\u66f4\u91cd\u8981\u7684\u662f\u786e\u5b9a\u5305\u542b\u75ab\u60c5\u76f8\u5173\u7684\u5065\u5eb7\u6570\u636e\u662f\u5426\u80fd\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u8bba\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86 LightGBM \u56de\u5f52\u6a21\u578b\u548c\u9057\u4f20\u7b97\u6cd5 (GA) \u4f18\u5316\u3002\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u6bcf\u65e5\u6bd4\u7279\u5e01\u56de\u62a5\u548c COVID-19 \u6307\u6807\uff08\u5982\u75ab\u82d7\u63a5\u79cd\u7387\u3001\u4f4f\u9662\u4eba\u6570\u3001\u68c0\u6d4b\u7edf\u8ba1\u6570\u636e\uff09\u7684\u7efc\u5408\u6570\u636e\u96c6\u3002\u901a\u8fc7\u5728 31 \u6b21\u72ec\u7acb\u8fd0\u884c\u4e2d\u4f7f\u7528 GA \u4f18\u5316\u6a21\u578b\uff0c\u5e76\u4e0e\u672a\u4f7f\u7528 COVID-19 \u7279\u5f81\u7684\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\uff0c\u4ee5\u8fdb\u884c\u7a33\u5065\u7684\u7edf\u8ba1\u8bc4\u4f30\u3002\u7814\u7a76\u91c7\u7528\u4e86 R2\u3001RMSE\u3001MAE \u7b49\u6027\u80fd\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u91cd\u53e0\u548c Mann-Whitney U \u68c0\u9a8c\u8fdb\u884c\u7edf\u8ba1\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u8fd8\u4f7f\u7528\u4e86\u6392\u5217\u7279\u5f81\u91cd\u8981\u6027 (PFI) \u5206\u6790\u6765\u91cf\u5316\u5355\u4e2a\u7279\u5f81\u7684\u8d21\u732e\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0cCOVID-19 \u6307\u6807\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6355\u6349\u6781\u7aef\u5e02\u573a\u6ce2\u52a8\u65b9\u9762\uff08R2 \u63d0\u9ad8\u4e86 40%\uff0cRMSE \u964d\u4f4e\u4e86 2%\uff0c\u5747\u5177\u6709\u9ad8\u5ea6\u7edf\u8ba1\u663e\u8457\u6027\uff09\u3002\u5728 COVID-19 \u6307\u6807\u4e2d\uff0c\u75ab\u82d7\u63a5\u79cd\u6307\u6807\uff0c\u7279\u522b\u662f\u5df2\u5b8c\u5168\u63a5\u79cd\u4e2a\u4f53\u5360\u603b\u4eba\u53e3\u7684 75% \u767e\u5206\u4f4d\u6570\uff0c\u88ab\u8bc1\u660e\u662f\u6700\u91cd\u8981\u7684\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7ed3\u5408 LightGBM \u56de\u5f52\u6a21\u578b\u548c\u9057\u4f20\u7b97\u6cd5 (GA) \u4f18\u5316\u7684\u65b0\u65b9\u6cd5\u6846\u67b6\uff0c\u80fd\u591f\u7cfb\u7edf\u5730\u8bc4\u4f30 COVID-19 \u76f8\u5173\u6307\u6807\u5bf9\u9884\u6d4b\u6bd4\u7279\u5e01\u56de\u62a5\u7684\u8d21\u732e\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cCOVID-19 \u6307\u6807\u7684\u7eb3\u5165\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6355\u6349\u6781\u7aef\u5e02\u573a\u6ce2\u52a8\u65b9\u9762\uff0cR2 \u63d0\u9ad8\u4e86 40%\uff0cRMSE \u964d\u4f4e\u4e86 2%\uff0c\u5e76\u4e14\u8fd9\u4e9b\u6539\u8fdb\u5728\u7edf\u8ba1\u4e0a\u5177\u6709\u663e\u8457\u6027\u3002\u5176\u4e2d\uff0c\u75ab\u82d7\u63a5\u79cd\u6307\u6807\uff0c\u7279\u522b\u662f\u5b8c\u5168\u63a5\u79cd\u4e2a\u4f53\u5360\u603b\u4eba\u53e3\u7684 75% \u7684\u767e\u5206\u4f4d\u6570\uff0c\u6210\u4e3a\u4e3b\u8981\u7684\u9884\u6d4b\u56e0\u5b50\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u516c\u5171\u536b\u751f\u4fe1\u53f7\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u7684\u91d1\u878d\u5206\u6790\u5de5\u5177\uff0c\u4e3a\u6295\u8d44\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5728\u7cfb\u7edf\u6027\u5371\u673a\u671f\u95f4\u5e94\u5bf9\u5e02\u573a\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u6307\u6807\u3002"}}
{"id": "2508.00220", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00220", "abs": "https://arxiv.org/abs/2508.00220", "authors": ["Rana Aref Salama", "Abdou Youssef", "Mona Diab"], "title": "Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform", "comment": null, "summary": "Wavelet transforms, a powerful mathematical tool, have been widely used in\ndifferent domains, including Signal and Image processing, to unravel intricate\npatterns, enhance data representation, and extract meaningful features from\ndata. Tangible results from their application suggest that Wavelet transforms\ncan be applied to NLP capturing a variety of linguistic and semantic\nproperties. In this paper, we empirically leverage the application of Discrete\nWavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase\nthe capabilities of DWT in analyzing embedding representations at different\nlevels of resolution and compressing them while maintaining their overall\nquality. We assess the effectiveness of DWT embeddings on semantic similarity\ntasks to show how DWT can be used to consolidate important semantic information\nin an embedding vector. We show the efficacy of the proposed paradigm using\ndifferent embedding models, including large language models, on downstream\ntasks. Our results show that DWT can reduce the dimensionality of embeddings by\n50-93% with almost no change in performance for semantic similarity tasks,\nwhile achieving superior accuracy in most downstream tasks. Our findings pave\nthe way for applying DWT to improve NLP applications.", "AI": {"tldr": "DWT\u53ef\u6709\u6548\u538b\u7f29NLP\u5d4c\u5165\uff0c\u964d\u4f4e\u7ef4\u5ea6\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u5c0f\u6ce2\u53d8\u6362\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5229\u7528\u5176\u5206\u6790\u5d4c\u5165\u8868\u793a\u548c\u538b\u7f29\u5d4c\u5165\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u8bed\u4e49\u8d28\u91cf\u3002", "method": "\u672c\u6587\u91c7\u7528\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\uff08DWT\uff09\u5bf9\u8bcd\u5d4c\u5165\u548c\u53e5\u5b50\u5d4c\u5165\u8fdb\u884c\u5206\u6790\u548c\u538b\u7f29\uff0c\u5e76\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u4efb\u52a1\u548c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8bc4\u4f30\u5176\u6709\u6548\u6027\u3002", "result": "DWT\u80fd\u591f\u5c06\u5d4c\u5165\u7684\u7ef4\u5ea6\u964d\u4f4e50-93%\uff0c\u540c\u65f6\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u4efb\u52a1\u4e0a\u51e0\u4e4e\u4e0d\u5f71\u54cd\u6027\u80fd\uff0c\u5e76\u5728\u5927\u591a\u6570\u4e0b\u6e38\u4efb\u52a1\u4e0a\u53d6\u5f97\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\uff08DWT\uff09\u5728\u8bcd\u5d4c\u5165\u548c\u53e5\u5b50\u5d4c\u5165\u7684\u5e94\u7528\u80fd\u591f\u6709\u6548\u964d\u4f4e\u5d4c\u5165\u7684\u7ef4\u5ea6\uff0c\u540c\u65f6\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u4efb\u52a1\u4e2d\u4fdd\u6301\u5176\u6027\u80fd\uff0c\u5e76\u5728\u5927\u591a\u6570\u4e0b\u6e38\u4efb\u52a1\u4e2d\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4e3a\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u5e94\u7528\u4e2d\u5e94\u7528DWT\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2508.00159", "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.TH", "math.OC", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2508.00159", "abs": "https://arxiv.org/abs/2508.00159", "authors": ["Jobst Heitzig", "Ram Potham"], "title": "Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power", "comment": null, "summary": "Power is a key concept in AI safety: power-seeking as an instrumental goal,\nsudden or gradual disempowerment of humans, power balance in human-AI\ninteraction and international AI governance. At the same time, power as the\nability to pursue diverse goals is essential for wellbeing.\n  This paper explores the idea of promoting both safety and wellbeing by\nforcing AI agents explicitly to empower humans and to manage the power balance\nbetween humans and AI agents in a desirable way. Using a principled, partially\naxiomatic approach, we design a parametrizable and decomposable objective\nfunction that represents an inequality- and risk-averse long-term aggregate of\nhuman power. It takes into account humans' bounded rationality and social\nnorms, and, crucially, considers a wide variety of possible human goals.\n  We derive algorithms for computing that metric by backward induction or\napproximating it via a form of multi-agent reinforcement learning from a given\nworld model. We exemplify the consequences of (softly) maximizing this metric\nin a variety of paradigmatic situations and describe what instrumental\nsub-goals it will likely imply. Our cautious assessment is that softly\nmaximizing suitable aggregate metrics of human power might constitute a\nbeneficial objective for agentic AI systems that is safer than direct\nutility-based objectives.", "AI": {"tldr": "This paper proposes an objective function for AI agents that prioritizes human empowerment and power balance, aiming to enhance both safety and wellbeing. It uses a principled approach to design a metric that considers human rationality and diverse goals, deriving algorithms for its computation and demonstrating its potential benefits over direct utility-based objectives.", "motivation": "Exploring the idea of promoting both safety and wellbeing by forcing AI agents explicitly to empower humans and to manage the power balance between humans and AI agents in a desirable way.", "method": "Using a principled, partially axiomatic approach, design a parametrizable and decomposable objective function that represents an inequality- and risk-averse long-term aggregate of human power. Derive algorithms for computing this metric by backward induction or approximating it via a form of multi-agent reinforcement learning from a given world model.", "result": "Exemplify the consequences of (softly) maximizing this metric in a variety of paradigmatic situations and describe what instrumental sub-goals it will likely imply.", "conclusion": "Softly maximizing suitable aggregate metrics of human power might constitute a beneficial objective for agentic AI systems that is safer than direct utility-based objectives."}}
{"id": "2508.00052", "categories": ["quant-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.00052", "abs": "https://arxiv.org/abs/2508.00052", "authors": ["Pierre-Gabriel Rozon", "Kartiek Agarwal"], "title": "Learning shadows to predict quantum ground state correlations", "comment": "12 pages (including supplementary material), 7 figures", "summary": "We introduce a variational scheme inspired by classical shadow tomography to\ncompute ground state correlations of quantum spin Hamiltonians. Shadow\ntomography allows for efficient reconstruction of expectation values of\narbitrary observables from a bag of repeated, randomized measurements, called\nsnapshots, on copies of the state $\\rho$. The prescription allows one to infer\nexpectation values of $M$ $k-$local observables to accuracy $\\epsilon$ using\njust $N \\sim 3^k \\text{log}M /\\epsilon^2$ snapshots when measurements are\nperformed in locally random bases. Turning this around, a bag of snapshots can\nbe considered an efficient representation of the state $\\rho$, particularly for\nestimating low-weight observables, such as terms in a local Hamiltonian needed\nto estimate the energy. Inspired by this, we consider a variational scheme\nwherein a bag of $N$ parametrized snapshots is used to represent the putative\nground state of a desired local spin Hamiltonian and optimized to lower the\nenergy with respect to it. Additional constraints in the form of positivity of\nreduced density matrices, motivated by work in quantum chemistry, are employed\nto ensure compatibility of the predicted correlations with the underlying\nHilbert space. Unlike reduced density matrix approaches, learning the\nunderlying distribution of measurement outcomes allows one to further\ncorrelations beyond those in the constrained density matrix. We show, with\nnumerical results, that the proposed variational method can be parallelized, is\nefficiently simulable, and yields a more complete description of the ground\nstate.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f71\u5b50\u65ad\u5c42\u626b\u63cf\u7684\u53d8\u5206\u65b9\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u91cf\u5b50\u81ea\u65cb\u54c8\u5bc6\u987f\u91cf\u7684\u57fa\u6001\u76f8\u5173\u6027\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u4e00\u7ec4\u53c2\u6570\u5316\u5feb\u7167\u6765\u964d\u4f4e\u80fd\u91cf\uff0c\u5e76\u80fd\u4ea7\u751f\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5b8c\u6574\u7684\u57fa\u6001\u63cf\u8ff0\u3002", "motivation": "\u4e3a\u4e86\u8ba1\u7b97\u91cf\u5b50\u81ea\u65cb\u54c8\u5bc6\u987f\u91cf\u7684\u57fa\u6001\u76f8\u5173\u6027\uff0c\u5e76\u501f\u9274\u5f71\u5b50\u65ad\u5c42\u626b\u63cf\u53ef\u4ee5\u4ece\u4e00\u7ec4\u5feb\u7167\u4e2d\u9ad8\u6548\u91cd\u6784\u671f\u671b\u503c\u7684\u601d\u60f3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u7ecf\u5178\u5f71\u5b50\u65ad\u5c42\u626b\u63cf\u542f\u53d1\u7684\u53d8\u5206\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u4f7f\u7528\u4e00\u7ec4\u53c2\u6570\u5316\u7684\u5feb\u7167\u6765\u8868\u793a\u76ee\u6807\u54c8\u5bc6\u987f\u91cf\u7684\u5047\u5b9a\u57fa\u6001\uff0c\u5e76\u901a\u8fc7\u964d\u4f4e\u80fd\u91cf\u6765\u4f18\u5316\u5b83\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e86\u975e\u8d1f\u6027\u7b49\u9644\u52a0\u7ea6\u675f\uff0c\u4ee5\u786e\u4fdd\u9884\u6d4b\u7684\u76f8\u5173\u6027\u4e0e\u5e95\u5c42\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u517c\u5bb9\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u53d8\u5206\u65b9\u6cd5\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u66f4\u5168\u9762\u5730\u63cf\u8ff0\u57fa\u6001\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u5e76\u884c\u5316\u3001\u53ef\u9ad8\u6548\u6a21\u62df\uff0c\u5e76\u80fd\u4ea7\u751f\u6bd4\u7ea6\u675f\u5bc6\u5ea6\u77e9\u9635\u66f4\u5b8c\u6574\u7684\u57fa\u6001\u63cf\u8ff0\u3002"}}
{"id": "2508.00775", "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00775", "abs": "https://arxiv.org/abs/2508.00775", "authors": ["Andrea Martin", "Ian R. Manchester", "Luca Furieri"], "title": "Learning to optimize with guarantees: a complete characterization of linearly convergent algorithms", "comment": null, "summary": "In high-stakes engineering applications, optimization algorithms must come\nwith provable worst-case guarantees over a mathematically defined class of\nproblems. Designing for the worst case, however, inevitably sacrifices\nperformance on the specific problem instances that often occur in practice. We\naddress the problem of augmenting a given linearly convergent algorithm to\nimprove its average-case performance on a restricted set of target problems -\nfor example, tailoring an off-the-shelf solver for model predictive control\n(MPC) for an application to a specific dynamical system - while preserving its\nworst-case guarantees across the entire problem class. Toward this goal, we\ncharacterize the class of algorithms that achieve linear convergence for\nclasses of nonsmooth composite optimization problems. In particular, starting\nfrom a baseline linearly convergent algorithm, we derive all - and only - the\nmodifications to its update rule that maintain its convergence properties. Our\nresults apply to augmenting legacy algorithms such as gradient descent for\nnonconvex, gradient-dominated functions; Nesterov's accelerated method for\nstrongly convex functions; and projected methods for optimization over\npolyhedral feasibility sets. We showcase effectiveness of the approach on\nsolving optimization problems with tight iteration budgets in application to\nill-conditioned systems of linear equations and MPC for linear systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4fdd\u8bc1\u7b97\u6cd5\u6700\u574f\u60c5\u51b5\u6536\u655b\u6027\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u5176\u5728\u7279\u5b9a\u95ee\u9898\u4e0a\u7684\u5e73\u5747\u60c5\u51b5\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4fee\u6539\u66f4\u65b0\u89c4\u5219\u6765\u5b9e\u73b0\uff0c\u9002\u7528\u4e8e\u68af\u5ea6\u4e0b\u964d\u3001Nesterov \u52a0\u901f\u65b9\u6cd5\u548c\u6295\u5f71\u65b9\u6cd5\u7b49\u591a\u79cd\u7b97\u6cd5\uff0c\u5e76\u5728\u6c42\u89e3\u8fed\u4ee3\u6b21\u6570\u53d7\u9650\u7684\u4f18\u5316\u95ee\u9898\u548c MPC \u95ee\u9898\u4e0a\u5f97\u5230\u4e86\u6709\u6548\u9a8c\u8bc1\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5728\u4f18\u5316\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\uff0c\u5982\u4f55\u5728\u4fdd\u8bc1\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6536\u655b\u6027\uff08\u5728\u6570\u5b66\u5b9a\u4e49\u7684\u6709\u95ee\u9898\u7c7b\u522b\u4e0a\u5177\u6709\u53ef\u8bc1\u660e\u7684\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\uff09\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u95ee\u9898\u5b9e\u4f8b\u4e0a\u7684\u5e73\u5747\u60c5\u51b5\u6027\u80fd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7814\u7a76\u4eba\u5458\u5e0c\u671b\u589e\u5f3a\u4e00\u4e2a\u7ed9\u5b9a\u7684\u7ebf\u6027\u6536\u655b\u7b97\u6cd5\uff0c\u4f7f\u5176\u5728\u7279\u5b9a\u76ee\u6807\u95ee\u9898\uff08\u4f8b\u5982\uff0c\u9488\u5bf9\u7279\u5b9a\u52a8\u529b\u7cfb\u7edf\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236MPC\u7684\u73b0\u6210\u6c42\u89e3\u5668\uff09\u4e0a\u7684\u5e73\u5747\u60c5\u51b5\u6027\u80fd\u5f97\u5230\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u5728\u6574\u4e2a\u95ee\u9898\u7c7b\u522b\u4e0a\u7684\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u7c7b\u975e\u5149\u6ed1\u590d\u5408\u4f18\u5316\u95ee\u9898\u7684\u7b97\u6cd5\uff0c\u5e76\u63a8\u5bfc\u4e86\u4fdd\u6301\u7ebf\u6027\u6536\u655b\u6027\u8d28\u7684\u66f4\u65b0\u89c4\u5219\u4fee\u6539\u65b9\u6cd5\u3002\u7814\u7a76\u4eba\u5458\u4ece\u4e00\u4e2a\u57fa\u7ebf\u7ebf\u6027\u6536\u655b\u7b97\u6cd5\u51fa\u53d1\uff0c\u5f97\u5230\u4e86\u6240\u6709\u4e14\u4ec5\u6709\u80fd\u591f\u4fdd\u6301\u5176\u6536\u655b\u6027\u8d28\u7684\u66f4\u65b0\u89c4\u5219\u4fee\u6539\u65b9\u6cd5\u3002", "result": "\u672c\u6587\u6210\u529f\u5730\u63cf\u8ff0\u4e86\u4e00\u7c7b\u975e\u5149\u6ed1\u590d\u5408\u4f18\u5316\u95ee\u9898\u7684\u7b97\u6cd5\uff0c\u5e76\u4e14\u63a8\u5bfc\u51fa\u4e86\u6240\u6709\u80fd\u591f\u4fdd\u6301\u7ebf\u6027\u6536\u655b\u6027\u8d28\u7684\u66f4\u65b0\u89c4\u5219\u4fee\u6539\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u5e94\u7528\u4e8e\u68af\u5ea6\u4e0b\u964d\u3001Nesterov \u52a0\u901f\u65b9\u6cd5\u548c\u6295\u5f71\u65b9\u6cd5\u7b49\u591a\u79cd\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u5bfc\u4e86\u6240\u6709\u80fd\u7ef4\u6301\u6536\u655b\u6027\u8d28\u7684\u66f4\u65b0\u89c4\u5219\u7684\u4fee\u6539\u65b9\u6cd5\uff0c\u5e76\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u68af\u5ea6\u4e0b\u964d\u3001Nesterov \u52a0\u901f\u65b9\u6cd5\u548c\u6295\u5f71\u65b9\u6cd5\u7b49\u7b97\u6cd5\uff0c\u4ee5\u589e\u5f3a\u5b83\u4eec\u5728\u7279\u5b9a\u95ee\u9898\u5b9e\u4f8b\u4e0a\u7684\u5e73\u5747\u60c5\u51b5\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u5728\u6574\u4e2a\u95ee\u9898\u7c7b\u522b\u4e0a\u7684\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\u3002\u5728\u6c42\u89e3\u8fed\u4ee3\u6b21\u6570\u53d7\u9650\u7684\u4f18\u5316\u95ee\u9898\u548c MPC \u95ee\u9898\u4e0a\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.00202", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00202", "abs": "https://arxiv.org/abs/2508.00202", "authors": ["Ecem Bozkurt", "Antonio Ortega"], "title": "Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models", "comment": "5 pages, 2 figures, under review at CAMSAP 2025", "summary": "Foundation models (FMs) pretrained on large datasets have become fundamental\nfor various downstream machine learning tasks, in particular in scenarios where\nobtaining perfectly labeled data is prohibitively expensive. In this paper, we\nassume an FM has to be fine-tuned with noisy data and present a two-stage\nframework to ensure robust classification in the presence of label noise\nwithout model retraining. Recent work has shown that simple k-nearest neighbor\n(kNN) approaches using an embedding derived from an FM can achieve good\nperformance even in the presence of severe label noise. Our work is motivated\nby the fact that these methods make use of local geometry. In this paper,\nfollowing a similar two-stage procedure, reliability estimation followed by\nreliability-weighted inference, we show that improved performance can be\nachieved by introducing geometry information. For a given instance, our\nproposed inference uses a local neighborhood of training data, obtained using\nthe non-negative kernel (NNK) neighborhood construction. We propose several\nmethods for reliability estimation that can rely less on distance and local\nneighborhood as the label noise increases. Our evaluation on CIFAR-10 and\nDermaMNIST shows that our methods improve robustness across various noise\nconditions, surpassing standard K-NN approaches and recent\nadaptive-neighborhood baselines.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u975e\u8d1f\u6838\uff08NNK\uff09\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u4fe1\u606f\u6765\u589e\u5f3a\u6a21\u578b\u5728\u5e26\u566a\u58f0\u6570\u636e\u4e0a\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u73b0\u6709\u57fa\u4e8eFM\u7684kNN\u65b9\u6cd5\u5229\u7528\u5c40\u90e8\u51e0\u4f55\u4fe1\u606f\u8868\u73b0\u826f\u597d\uff0c\u4f46\u8be5\u7814\u7a76\u8ba4\u4e3a\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u51e0\u4f55\u4fe1\u606f\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728FM\u9700\u8981\u7528\u5e26\u566a\u58f0\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u7684\u573a\u666f\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5305\u62ec\u53ef\u9760\u6027\u4f30\u8ba1\u548c\u53ef\u9760\u6027\u52a0\u6743\u63a8\u7406\u3002\u5176\u4e2d\uff0c\u63a8\u7406\u8fc7\u7a0b\u5229\u7528\u975e\u8d1f\u6838\uff08NNK\uff09\u8fdb\u884c\u90bb\u57df\u6784\u5efa\uff0c\u5e76\u63d0\u51fa\u591a\u79cd\u53ef\u9760\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u6807\u7b7e\u566a\u58f0\u589e\u52a0\u65f6\u5bf9\u8ddd\u79bb\u548c\u5c40\u90e8\u90bb\u57df\u7684\u4f9d\u8d56\u6027\u3002", "result": "\u5728CIFAR-10\u548cDermaMNIST\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4e0b\u5747\u80fd\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u4f18\u4e8e\u6807\u51c6\u7684kNN\u65b9\u6cd5\u548c\u8fd1\u671f\u81ea\u9002\u5e94\u90bb\u57df\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u51e0\u4f55\u4fe1\u606f\u7684NNK\u65b9\u6cd5\u5728CIFAR-10\u548cDermaMNIST\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u5404\u79cd\u566a\u58f0\u6761\u4ef6\u4e0b\u5747\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u6807\u51c6\u7684kNN\u65b9\u6cd5\u548c\u8fd1\u671f\u81ea\u9002\u5e94\u90bb\u57df\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.00384", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00384", "abs": "https://arxiv.org/abs/2508.00384", "authors": ["Juanwu Lu", "Rohit Gupta", "Ahmadreza Moradipari", "Kyungtae Han", "Ruqi Zhang", "Ziran Wang"], "title": "On Learning Closed-Loop Probabilistic Multi-Agent Simulator", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. Source Code: https://github.com/juanwulu/niva", "summary": "The rapid iteration of autonomous vehicle (AV) deployments leads to\nincreasing needs for building realistic and scalable multi-agent traffic\nsimulators for efficient evaluation. Recent advances in this area focus on\nclosed-loop simulators that enable generating diverse and interactive\nscenarios. This paper introduces Neural Interactive Agents (NIVA), a\nprobabilistic framework for multi-agent simulation driven by a hierarchical\nBayesian model that enables closed-loop, observation-conditioned simulation\nthrough autoregressive sampling from a latent, finite mixture of Gaussian\ndistributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence\ntrajectory prediction models and emerging closed-loop simulation models trained\non Next-token Prediction (NTP) from a Bayesian inference perspective.\nExperiments on the Waymo Open Motion Dataset demonstrate that NIVA attains\ncompetitive performance compared to the existing method while providing\nembellishing control over intentions and driving styles.", "AI": {"tldr": "NIVA\u662f\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u8bc4\u4f30\uff0c\u5b83\u901a\u8fc7\u6982\u7387\u6a21\u578b\u5b9e\u73b0\u95ed\u73af\u4eff\u771f\uff0c\u5e76\u80fd\u63a7\u5236\u610f\u56fe\u548c\u9a7e\u9a76\u98ce\u683c\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08AV\uff09\u90e8\u7f72\u5feb\u901f\u8fed\u4ee3\u7684\u9700\u6c42\uff0c\u9700\u8981\u6784\u5efa\u73b0\u5b9e\u4e14\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u6a21\u62df\u5668\u8fdb\u884c\u6709\u6548\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u7684\u6982\u7387\u6846\u67b6NIVA\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u91c7\u6837\u5b9e\u73b0\u95ed\u73af\u3001\u6761\u4ef6\u5316\u4eff\u771f\u3002", "result": "NIVA\u7edf\u4e00\u4e86\u73b0\u6709\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u548c\u65b0\u5174\u7684\u57fa\u4e8e\u8d1d\u53f6\u65af\u63a8\u7406\u7684\u95ed\u73af\u4eff\u771f\u6a21\u578b\u3002", "conclusion": "NIVA\u6846\u67b6\u5728Waymo\u5f00\u653e\u8fd0\u52a8\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5bf9\u610f\u56fe\u548c\u9a7e\u9a76\u98ce\u683c\u7684\u7ec6\u81f4\u63a7\u5236\u3002"}}
{"id": "2508.00382", "categories": ["cond-mat.mtrl-sci", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2508.00382", "abs": "https://arxiv.org/abs/2508.00382", "authors": ["Marco Flieg", "Margot Guidat", "Matthias M. May"], "title": "Observation and control of potential-dependent surface state formation at a semiconductor-electrolyte interface via the optical anisotropy", "comment": "6 pages, 3 figures", "summary": "The interface between semiconductors and ion-conducting electrolytes is\ncharacterised by charge distributions and potential drops that vary\nsubstantially with the evolution of surface states. These surface states at the\nvery interface to the liquid can form or be passivated, depending on the\napplied potential between electrode and electrolyte, and hereby fundamentally\nimpact properties such as charge transfer. Characterisation and understanding\nof such potential-dependent surface states with high spatial and temporal\nresolution is a significant challenge for the understanding and control of\nsemiconductor-electrolyte interfaces. Here, we show that the optical anisotropy\nof InP(100) can be used to detect the potential-dependent formation of highly\nordered surface states under operating conditions. Upon formation of a surface\nstate in the bandgap of the semiconductor, the potential drop and hence the\nelectric field is shifted away from the semiconductor to the Helmholtz-layer of\nthe electrolyte. This modifies the instantaneous response of the optical\nanisotropy to disturbances of the applied potential. We propose an\nelectrochemical variant of the linear electro-optical effect and our findings\nopen a novel route for understanding these interfaces. The results show how\nsurface states from surface reconstructions at this reactive interface can be\nswitched on or off with the applied potential.", "AI": {"tldr": "\u5229\u7528InP(100)\u7684\u5149\u5b66\u5404\u5411\u5f02\u6027\uff0c\u53ef\u4ee5\u68c0\u6d4b\u548c\u63a7\u5236\u534a\u5bfc\u4f53-\u7535\u89e3\u8d28\u754c\u9762\u7684\u8868\u9762\u72b6\u6001\u3002", "motivation": "\u7406\u89e3\u534a\u5bfc\u4f53-\u7535\u89e3\u8d28\u754c\u9762\u7684\u7535\u52bf\u4f9d\u8d56\u6027\u8868\u9762\u72b6\u6001\u5bf9\u4e8e\u63a7\u5236\u5176\u7535\u8377\u8f6c\u79fb\u52a8\u529b\u5b66\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u76d1\u6d4bInP(100)\u7684\u5149\u5b66\u5404\u5411\u5f02\u6027\u968f\u65bd\u52a0\u7535\u52bf\u7684\u53d8\u5316\u6765\u68c0\u6d4b\u8868\u9762\u72b6\u6001\u7684\u5f62\u6210\u548c\u7535\u52bf\u5206\u5e03\u7684\u53d8\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8868\u9762\u72b6\u6001\u7684\u5f62\u6210\u4f1a\u6539\u53d8\u7535\u52bf\u5206\u5e03\uff0c\u5c06\u7535\u573a\u4ece\u534a\u5bfc\u4f53\u8f6c\u79fb\u5230\u7535\u89e3\u8d28\u7684\u4ea5\u59c6\u970d\u5179\u5c42\uff0c\u4ece\u800c\u6539\u53d8\u5149\u5b66\u5404\u5411\u5f02\u6027\u5bf9\u7535\u52bf\u6270\u52a8\u7684\u77ac\u65f6\u54cd\u5e94\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u63a2\u6d4b\u548c\u5f00\u5173\u8868\u9762\u6001\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528InP(100)\u7684\u5149\u5b66\u5404\u5411\u5f02\u6027\u6765\u68c0\u6d4b\u534a\u5bfc\u4f53-\u7535\u89e3\u8d28\u754c\u9762\u5904\u7535\u52bf\u4f9d\u8d56\u6027\u8868\u9762\u72b6\u6001\u5f62\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7535\u5316\u5b66\u7ebf\u6027\u5149\u7535\u6548\u5e94\u7684\u53d8\u4f53\u3002"}}
{"id": "2508.00575", "categories": ["cs.LO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00575", "abs": "https://arxiv.org/abs/2508.00575", "authors": ["Camille Bourgaux", "Anton Gnatenko", "Micha\u00ebl Thomazo"], "title": "Analysing Temporal Reasoning in Description Logics Using Formal Grammars", "comment": "This is an extended version of a paper appearing at the 28th European\n  Conference on Artificial Intelligence (ECAI 2025). 20 pages", "summary": "We establish a correspondence between (fragments of)\n$\\mathcal{TEL}^\\bigcirc$, a temporal extension of the $\\mathcal{EL}$\ndescription logic with the LTL operator $\\bigcirc^k$, and some specific kinds\nof formal grammars, in particular, conjunctive grammars (context-free grammars\nequipped with the operation of intersection). This connection implies that\n$\\mathcal{TEL}^\\bigcirc$ does not possess the property of ultimate periodicity\nof models, and further leads to undecidability of query answering in\n$\\mathcal{TEL}^\\bigcirc$, closing a question left open since the introduction\nof $\\mathcal{TEL}^\\bigcirc$. Moreover, it also allows to establish decidability\nof query answering for some new interesting fragments of\n$\\mathcal{TEL}^\\bigcirc$, and to reuse for this purpose existing tools and\nalgorithms for conjunctive grammars.", "AI": {"tldr": "$\\\text{TEL}^\\bigcirc$\u4e0e\u8fde\u7f00\u6587\u6cd5\u76f8\u5bf9\u5e94\uff0c\u5bfc\u81f4\u5176\u67e5\u8be2\u56de\u7b54\u4e0d\u53ef\u5224\u5b9a\uff0c\u4f46\u90e8\u5206\u60c5\u51b5\u4e0b\u7684\u67e5\u8be2\u56de\u7b54\u662f\u53ef\u5224\u5b9a\u7684\u3002", "motivation": "\u586b\u8865\u5173\u4e8e$\\\text{TEL}^\\bigcirc$\u67e5\u8be2\u56de\u7b54\u53ef\u5224\u5b9a\u6027\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5c06$\\\text{TEL}^\\bigcirc$\uff08$\\\text{EL}$\u63cf\u8ff0\u903b\u8f91\u7684\u5e26LTL\u7b97\u5b50$\\\bigcirc^k$\u7684\u65f6\u95f4\u5ef6\u4f38\uff09\u7684\u7247\u6bb5\u4e0e\u7279\u5b9a\u7c7b\u578b\u5f62\u5f0f\u6587\u6cd5\uff08\u7279\u522b\u662f\u8fde\u7f00\u6587\u6cd5\uff09\u5efa\u7acb\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "$\\\text{TEL}^\\bigcirc$\u4e0d\u5177\u5907\u6a21\u578b\u7684\u6700\u7ec8\u5468\u671f\u6027\uff0c\u67e5\u8be2\u56de\u7b54\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\u3002\u90e8\u5206$\\\text{TEL}^\\bigcirc$\u7684\u67e5\u8be2\u56de\u7b54\u662f\u53ef\u5224\u5b9a\u7684\u3002", "conclusion": "\u7531\u4e8e$\\\text{TEL}^\\bigcirc$\u4e0e\u7279\u5b9a\u7c7b\u578b\u7684\u5f62\u5f0f\u6587\u6cd5\uff08\u7279\u522b\u662f\u8fde\u7f00\u6587\u6cd5\uff09\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c$\\\text{TEL}^\\bigcirc$\u4e0d\u5177\u5907\u6a21\u578b\u7684\u6700\u7ec8\u5468\u671f\u6027\uff0c\u5e76\u4e14\u5176\u67e5\u8be2\u56de\u7b54\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\u3002\u7136\u800c\uff0c\u8fd9\u4e00\u8054\u7cfb\u4e5f\u4f7f\u5f97\u90e8\u5206$\\\text{TEL}^\\bigcirc$\u7684\u67e5\u8be2\u56de\u7b54\u53ef\u5224\u5b9a\uff0c\u5e76\u80fd\u591f\u5229\u7528\u73b0\u6709\u7684\u8fde\u7f00\u6587\u6cd5\u5de5\u5177\u548c\u7b97\u6cd5\u3002"}}
{"id": "2508.00098", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00098", "abs": "https://arxiv.org/abs/2508.00098", "authors": ["Ashkan Shakarami", "Yousef Yeganeh", "Azade Farshad", "Lorenzo Nicole", "Stefano Ghidoni", "Nassir Navab"], "title": "Stress-Aware Resilient Neural Training", "comment": "16 pages, 11 figures", "summary": "This paper introduces Stress-Aware Learning, a resilient neural training\nparadigm in which deep neural networks dynamically adjust their optimization\nbehavior - whether under stable training regimes or in settings with uncertain\ndynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)\nDeformation, inspired by structural fatigue in materials science. To\ninstantiate this concept, we propose Plastic Deformation Optimizer, a\nstress-aware mechanism that injects adaptive noise into model parameters\nwhenever an internal stress signal - reflecting stagnation in training loss and\naccuracy - indicates persistent optimization difficulty. This enables the model\nto escape sharp minima and converge toward flatter, more generalizable regions\nof the loss landscape. Experiments across six architectures, four optimizers,\nand seven vision benchmarks demonstrate improved robustness and generalization\nwith minimal computational overhead. The code and 3D visuals will be available\non GitHub: https://github.com/Stress-Aware-Learning/SAL.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u201c\u5851\u6027\u53d8\u5f62\u4f18\u5316\u5668\u201d\uff0c\u6a21\u578b\u53ef\u4ee5\u52a8\u6001\u8c03\u6574\u4f18\u5316\u884c\u4e3a\uff0c\u4ee5\u63d0\u9ad8\u5728\u4e0d\u7a33\u5b9a\u8bad\u7ec3\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u4e0d\u7a33\u5b9a\u6216\u52a8\u6001\u53d8\u5316\u73af\u5883\u4e0b\u7684\u8bad\u7ec3\u9c81\u68d2\u6027\uff0c\u501f\u9274\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u7ed3\u6784\u75b2\u52b3\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u5e94\u6fc0\u611f\u77e5\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5851\u6027\u53d8\u5f62\u4f18\u5316\u5668\u201d\u7684\u5e94\u6fc0\u611f\u77e5\u673a\u5236\uff0c\u901a\u8fc7\u5728\u68c0\u6d4b\u5230\u8bad\u7ec3\u635f\u5931\u548c\u51c6\u786e\u6027\u505c\u6ede\u7684\u5185\u90e8\u5e94\u6fc0\u4fe1\u53f7\u65f6\uff0c\u5411\u6a21\u578b\u53c2\u6570\u6ce8\u5165\u81ea\u9002\u5e94\u566a\u58f0\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8df3\u51fa\u5c16\u9510\u7684\u6700\u5c0f\u503c\uff0c\u6536\u655b\u5230\u66f4\u5e73\u5766\u3001\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u635f\u5931\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u5f88\u5c0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5e94\u6fc0\u611f\u77e5\u5b66\u4e60\u6846\u67b6\u5728\u516d\u79cd\u67b6\u6784\u3001\u56db\u79cd\u4f18\u5316\u5668\u548c\u4e03\u4e2a\u89c6\u89c9\u57fa\u51c6\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8bc1\u660e\u4e86\u5176\u5177\u6709\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2508.00238", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.00238", "abs": "https://arxiv.org/abs/2508.00238", "authors": ["Bryce Anderson", "Riley Galpin", "Tom S. Juzek"], "title": "Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English", "comment": "Accepted at AIES 2025. To appear in the AIES Proceedings. 14 pages, 2\n  figures, 2 tables. Licensed under CC BY-SA 4.0", "summary": "In recent years, written language, particularly in science and education, has\nundergone remarkable shifts in word usage. These changes are widely attributed\nto the growing influence of Large Language Models (LLMs), which frequently rely\non a distinct lexical style. Divergences between model output and target\naudience norms can be viewed as a form of misalignment. While these shifts are\noften linked to using Artificial Intelligence (AI) directly as a tool to\ngenerate text, it remains unclear whether the changes reflect broader changes\nin the human language system itself. To explore this question, we constructed a\ndataset of 22.1 million words from unscripted spoken language drawn from\nconversational science and technology podcasts. We analyzed lexical trends\nbefore and after ChatGPT's release in 2022, focusing on commonly LLM-associated\nwords. Our results show a moderate yet significant increase in the usage of\nthese words post-2022, suggesting a convergence between human word choices and\nLLM-associated patterns. In contrast, baseline synonym words exhibit no\nsignificant directional shift. Given the short time frame and the number of\nwords affected, this may indicate the onset of a remarkable shift in language\nuse. Whether this represents natural language change or a novel shift driven by\nAI exposure remains an open question. Similarly, although the shifts may stem\nfrom broader adoption patterns, it may also be that upstream training\nmisalignments ultimately contribute to changes in human language use. These\nfindings parallel ethical concerns that misaligned models may shape social and\nmoral beliefs.", "AI": {"tldr": "LLM\u53ef\u80fd\u6b63\u5728\u6539\u53d8\u6211\u4eec\u8bf4\u8bdd\u548c\u5199\u4f5c\u7684\u65b9\u5f0f\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u79d1\u5b66\u548c\u6559\u80b2\u9886\u57df\u4e66\u9762\u8bed\u8bcd\u6c47\u4f7f\u7528\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5f71\u54cd\u662f\u5426\u53cd\u6620\u4e86\u4eba\u7c7b\u8bed\u8a00\u7cfb\u7edf\u672c\u8eab\u7684\u66f4\u5e7f\u6cdb\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u5206\u67902022\u5e74\u524d\u540e\u4ece\u79d1\u5b66\u548c\u6280\u672f\u64ad\u5ba2\u4e2d\u63d0\u53d6\u76842210\u4e07\u4e2a\u5355\u8bcd\u7684\u6570\u636e\u96c6\uff0c\u91cd\u70b9\u5173\u6ce8\u4e0eLLM\u76f8\u5173\u7684\u5e38\u7528\u8bcd\u6c47\uff0c\u5e76\u4e0e\u57fa\u7ebf\u540c\u4e49\u8bcd\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u57282022\u5e74\u540e\uff0c\u4e0eLLM\u76f8\u5173\u7684\u8bcd\u6c47\u4f7f\u7528\u91cf\u51fa\u73b0\u4e86\u663e\u8457\u589e\u52a0\uff0c\u800c\u57fa\u7ebf\u540c\u4e49\u8bcd\u6ca1\u6709\u663e\u793a\u51fa\u660e\u663e\u7684\u53d8\u5316\u8d8b\u52bf\uff0c\u8fd9\u8868\u660e\u4eba\u7c7b\u7684\u7528\u8bcd\u9009\u62e9\u53ef\u80fd\u6b63\u5728\u5411LLM\u7684\u6a21\u5f0f\u9760\u62e2\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u57282022\u5e74\u540e\uff0c\u4eba\u7c7b\u5728\u53e3\u8bed\u4e2d\u4f7f\u7528\u7684\u4e0eLLM\u76f8\u5173\u7684\u8bcd\u6c47\u6709\u6240\u589e\u52a0\uff0c\u8fd9\u8868\u660e\u4eba\u7c7b\u8bed\u8a00\u4f7f\u7528\u53ef\u80fd\u6b63\u5728\u53d1\u751f\u53d8\u5316\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u79cd\u53d8\u5316\u662f\u81ea\u7136\u7684\u8bed\u8a00\u6f14\u53d8\u8fd8\u662f\u7531AI\u9a71\u52a8\u7684\u3002"}}
{"id": "2508.00222", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00222", "abs": "https://arxiv.org/abs/2508.00222", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its inherently on-policy strategy with LLM's immense\naction space and sparse reward. Further, RLVR can lead to the capability\nboundary collapse, narrowing the LLM's problem-solving scope. To address this\nproblem, we propose RL-PLUS, a novel approach that synergizes internal\nexploitation (i.e., Thinking) with external data (i.e., Learning) to achieve\nstronger reasoning capabilities and surpass the boundaries of base models.\nRL-PLUS integrates two core components: Multiple Importance Sampling to address\nfor distributional mismatch from external data, and an Exploration-Based\nAdvantage Function to guide the model towards high-value, unexplored reasoning\npaths. We provide both theoretical analysis and extensive experiments to\ndemonstrate the superiority and generalizability of our approach. The results\nshow that RL-PLUS achieves state-of-the-art performance compared with existing\nRLVR methods on six math reasoning benchmarks and exhibits superior performance\non six out-of-distribution reasoning tasks. It also achieves consistent and\nsignificant gains across diverse model families, with average relative\nimprovements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across\nmultiple benchmarks indicate that RL-PLUS effectively resolves the capability\nboundary collapse problem.", "AI": {"tldr": "RL-PLUS \u901a\u8fc7\u7ed3\u5408\u5185\u90e8\u601d\u8003\u548c\u5916\u90e8\u5b66\u4e60\uff0c\u5229\u7528\u591a\u79cd\u91cd\u8981\u6027\u91c7\u6837\u548c\u57fa\u4e8e\u63a2\u7d22\u7684\u4f18\u52bf\u51fd\u6570\uff0c\u63d0\u9ad8\u4e86 LLM \u7684\u63a8\u7406\u80fd\u529b\uff0c\u514b\u670d\u4e86\u73b0\u6709 RLVR \u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3 RLVR \u6323\u624e\u4e8e\u57fa\u7840 LLM \u7684\u56fa\u6709\u80fd\u529b\u8fb9\u754c\u3001\u53ef\u80fd\u5bfc\u81f4\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 RL-PLUS\u3002", "method": "RL-PLUS \u6574\u5408\u4e86\u4e24\u79cd\u6838\u5fc3\u7ec4\u4ef6\uff1a\u591a\u79cd\u91cd\u8981\u6027\u91c7\u6837\uff0c\u7528\u4e8e\u89e3\u51b3\u5916\u90e8\u6570\u636e\u4ea7\u751f\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff1b\u4ee5\u53ca\u57fa\u4e8e\u63a2\u7d22\u7684\u4f18\u52bf\u51fd\u6570\uff0c\u7528\u4e8e\u6307\u5bfc\u6a21\u578b\u8d70\u5411\u9ad8\u4ef7\u503c\u3001\u672a\u63a2\u7d22\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "RL-PLUS \u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u516d\u4e2a\u5206\u5e03\u5916\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u4e0e\u73b0\u6709\u7684 RLVR \u65b9\u6cd5\u76f8\u6bd4\uff0c\u5e73\u5747\u76f8\u5bf9\u63d0\u9ad8\u4e86 21.1% \u5230 69.2%\u3002Pass@k \u66f2\u7ebf\u8868\u660e RL-PLUS \u6709\u6548\u89e3\u51b3\u4e86\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u95ee\u9898\u3002", "conclusion": "RL-PLUS \u6210\u529f\u89e3\u51b3\u4e86\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u516d\u4e2a\u5206\u5e03\u5916\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u4e0e\u73b0\u6709\u7684 RLVR \u65b9\u6cd5\u76f8\u6bd4\uff0c\u5e73\u5747\u76f8\u5bf9\u63d0\u9ad8\u4e86 21.1% \u5230 69.2%\u3002"}}
{"id": "2508.00139", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.00139", "abs": "https://arxiv.org/abs/2508.00139", "authors": ["Christian W. Binder", "Guido Burkard", "Andrew J. Fisher"], "title": "Effective 2D Envelope Function Theory for Silicon Quantum Dots", "comment": "23 pages, 9 figures", "summary": "We present a rigorous method to reduce the three-dimensional (3D) description\nof a quantum dot in silicon to an effective two-dimensional (2D) envelope\nfunction theory for electron spin qubits. By systematically integrating out the\nstrongly confined vertical dimension using a Born-Oppenheimer-inspired ansatz\nat the envelope-function level, we derive an effective in-plane potential that\nfaithfully captures the essential electrostatics of the full 3D system.\nConsidering the lowest two eigenstates of the out-of-plane direction, this\nreduction leads to the natural and explicit emergence of the valley degree of\nfreedom within a 2D formalism, which is derived here from first principles. We\nvalidate the accuracy of the method through comparisons with full 3D\nsimulations and demonstrate its superiority over naive 2D slicing, particularly\nin the presence of interface roughness. Crucially, the reduction in\ndimensionality leads to substantial computational savings, making our approach\nparticularly well suited for simulating two-electron systems, e.g., for the\nextraction of parameters such as the exchange coupling. Beyond its practical\nutility, the rigorous 2D envelope function theory that is introduced in this\nstudy incorporates valley physics in a physically grounded manner, offering\nconceptual clarity on the role of valley states in qubit operation and\nmeasurement.", "AI": {"tldr": "\"A new 2D theory for quantum dots accurately simplifies 3D systems, revealing valley physics and saving computation time, outperforming simpler methods, especially with rough interfaces.\"", "motivation": "\"The motivation is to develop a rigorous method to reduce the complexity of 3D quantum dot descriptions to a more computationally tractable 2D envelope function theory for electron spin qubits. This aims to improve simulation efficiency, particularly for multi-electron systems, and gain conceptual clarity on the role of valley physics in qubit operations.\"", "method": "\"A rigorous method is presented to reduce the 3D description of a quantum dot in silicon to an effective 2D envelope function theory. This is achieved by systematically integrating out the vertical dimension using a Born-Oppenheimer-inspired ansatz at the envelope-function level, deriving an effective in-plane potential. The method considers the lowest two eigenstates of the out-of-plane direction, leading to the emergence of the valley degree of freedom within a 2D formalism.\"", "result": "\"The method derives an effective in-plane potential that captures the essential electrostatics of the full 3D system. The reduction in dimensionality naturally incorporates the valley degree of freedom. Validation through comparisons with full 3D simulations shows accuracy and superiority over naive 2D slicing, especially with interface roughness. Significant computational savings are achieved.\"", "conclusion": "\"The presented rigorous 2D envelope function theory, derived from first principles, incorporates valley physics in a physically grounded manner, offering conceptual clarity on the role of valley states in qubit operation and measurement. It also provides substantial computational savings, making it well-suited for simulating two-electron systems and extracting parameters like exchange coupling.\""}}
{"id": "2508.00467", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00467", "abs": "https://arxiv.org/abs/2508.00467", "authors": ["Samratul Fuady", "Danesh Tarapore", "Mohammad D. Soorati"], "title": "SubCDM: Collective Decision-Making with a Swarm Subset", "comment": "6 pages, 7 figures. This paper has been accepted for presentation at\n  the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2025)", "summary": "Collective decision-making is a key function of autonomous robot swarms,\nenabling them to reach a consensus on actions based on environmental features.\nExisting strategies require the participation of all robots in the\ndecision-making process, which is resource-intensive and prevents the swarm\nfrom allocating the robots to any other tasks. We propose Subset-Based\nCollective Decision-Making (SubCDM), which enables decisions using only a swarm\nsubset. The construction of the subset is dynamic and decentralized, relying\nsolely on local information. Our method allows the swarm to adaptively\ndetermine the size of the subset for accurate decision-making, depending on the\ndifficulty of reaching a consensus. Simulation results using one hundred robots\nshow that our approach achieves accuracy comparable to using the entire swarm\nwhile reducing the number of robots required to perform collective\ndecision-making, making it a resource-efficient solution for collective\ndecision-making in swarm robotics.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5b50\u96c6\u5f0f\u96c6\u4f53\u51b3\u7b56\uff08SubCDM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5141\u8bb8\u673a\u5668\u4eba\u7fa4\u4f53\u4ec5\u4f7f\u7528\u4e00\u90e8\u5206\u673a\u5668\u4eba\u5c31\u80fd\u505a\u51fa\u96c6\u4f53\u51b3\u7b56\uff0c\u4ece\u800c\u8282\u7701\u4e86\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u7684\u7b56\u7565\u8981\u6c42\u6240\u6709\u673a\u5668\u4eba\u53c2\u4e0e\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8fd9\u4f1a\u6d88\u8017\u5927\u91cf\u8d44\u6e90\uff0c\u5e76\u963b\u6b62\u7fa4\u4f53\u5c06\u673a\u5668\u4eba\u5206\u914d\u7ed9\u4efb\u4f55\u5176\u4ed6\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5b50\u96c6\u5f0f\u96c6\u4f53\u51b3\u7b56\uff08SubCDM\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528\u7fa4\u4f53\u5b50\u96c6\u5373\u53ef\u8fdb\u884c\u51b3\u7b56\u3002\u5b50\u96c6\u7684\u6784\u5efa\u662f\u52a8\u6001\u548c\u5206\u6563\u7684\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u5c40\u90e8\u4fe1\u606f\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u4f7f\u7528\u6574\u4e2a\u7fa4\u4f53\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6267\u884c\u96c6\u4f53\u51b3\u7b56\u6240\u9700\u7684\u673a\u5668\u4eba\u6570\u91cf\u3002", "conclusion": "SubCDM\u65b9\u6cd5\u5728\u5b9e\u73b0\u4e0e\u4f7f\u7528\u6574\u4e2a\u7fa4\u4f53\u76f8\u5f53\u7684\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u4e86\u6267\u884c\u96c6\u4f53\u51b3\u7b56\u6240\u9700\u7684\u673a\u5668\u4eba\u6570\u91cf\uff0c\u4f7f\u5176\u6210\u4e3a\u7fa4\u4f53\u673a\u5668\u4eba\u4e2d\u96c6\u4f53\u51b3\u7b56\u7684\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00205", "abs": "https://arxiv.org/abs/2508.00205", "authors": ["Xiangyu Kong", "Hengde Zhu", "Haoqin Sun", "Zhihao Guo", "Jiayan Gu", "Xinyi Ni", "Wei Zhang", "Shizhe Liu", "Siyang Song"], "title": "Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition", "comment": "10 pages, 4 figures", "summary": "Automatic real personality recognition (RPR) aims to evaluate human real\npersonality traits from their expressive behaviours. However, most existing\nsolutions generally act as external observers to infer observers' personality\nimpressions based on target individuals' expressive behaviours, which\nsignificantly deviate from their real personalities and consistently lead to\ninferior recognition performance. Inspired by the association between real\npersonality and human internal cognition underlying the generation of\nexpressive behaviours, we propose a novel RPR approach that efficiently\nsimulates personalised internal cognition from easy-accessible external short\naudio-visual behaviours expressed by the target individual. The simulated\npersonalised cognition, represented as a set of network weights that enforce\nthe personalised network to reproduce the individual-specific facial reactions,\nis further encoded as a novel graph containing two-dimensional node and edge\nfeature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for\ninferring real personality traits from it. To simulate real personality-related\ncognition, an end-to-end strategy is designed to jointly train our cognition\nsimulation, 2D graph construction, and personality recognition modules.", "AI": {"tldr": "This paper proposes a new method for real personality recognition (RPR) by simulating internal cognition from expressive behaviors. It uses a 2D Graph Neural Network (2D-GNN) to infer personality traits from a novel graph representation of the simulated cognition. The method outperforms existing approaches.", "motivation": "Existing RPR solutions infer personality impressions from external observations of expressive behaviors, which deviate from real personalities and lead to inferior performance. The paper is motivated by the association between real personality and internal cognition underlying expressive behaviors.", "method": "A novel RPR approach is proposed that simulates personalized internal cognition from audio-visual behaviors. This cognition is encoded as a graph with node and edge features, and a 2D-GNN is used for personality recognition. An end-to-end strategy jointly trains the cognition simulation, graph construction, and personality recognition modules.", "result": "The proposed method achieves superior recognition performance by inferring real personality traits.", "conclusion": "The proposed approach simulates personalized internal cognition to infer real personality traits, outperforming existing methods."}}
{"id": "2508.00430", "categories": ["cond-mat.mtrl-sci", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.00430", "abs": "https://arxiv.org/abs/2508.00430", "authors": ["Asliddin Khudoyberdiev", "G\u00f6tz S. Uhrig"], "title": "The effect of dephasing and spin-lattice relaxation during the switching processes in quantum antiferromagnets", "comment": null, "summary": "The control of antiferromagnetic order can pave the way to large storage\ncapacity as well as fast manipulation of stored data. Here achieving a\nsteady-state of sublattice magnetization after switching is crucial to prevent\nloss of stored data. The present theoretical approach aims to obtain\ninstantaneous stable states of the order after reorienting the N\\'eel vector in\nopen quantum antiferromagnets using time-dependent Schwinger boson mean-field\ntheory. The Lindblad formalism is employed to couple the system to the\nenvironment. The quantum theoretical approach comprises differences in the\neffects of dephasing, originating from destructive interference of different\nwave vectors, and spin-lattice relaxation. We show that the spin-lattice\nrelaxation results in an exponentially fast convergence to the steady-state\nafter full ultrafast switching.", "AI": {"tldr": "Control of antiferromagnetic order is key for data storage and manipulation. This paper uses quantum theory to show spin-lattice relaxation helps antiferromagnets quickly stabilize after ultrafast switching, preventing data loss.", "motivation": "The motivation is to enable large storage capacity and fast manipulation of data by controlling antiferromagnetic order, emphasizing the importance of achieving a steady-state of sublattice magnetization after switching to prevent data loss.", "method": "The study employs time-dependent Schwinger boson mean-field theory and the Lindblad formalism to model the system coupled to the environment. It analyzes the distinct effects of dephasing and spin-lattice relaxation.", "result": "The research shows that spin-lattice relaxation causes an exponentially fast convergence to the steady-state after complete ultrafast switching, distinguishing its effect from dephasing caused by wave vector interference.", "conclusion": "The theoretical approach demonstrates that spin-lattice relaxation leads to exponentially fast convergence to a steady-state after ultrafast switching in open quantum antiferromagnets."}}
{"id": "2508.00613", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2508.00613", "abs": "https://arxiv.org/abs/2508.00613", "authors": ["Benedikt Maderbacher", "Roderick Bloem"], "title": "Parameterized Infinite-State Reactive Synthesis", "comment": null, "summary": "We propose a method to synthesize a parameterized infinite-state systems that\ncan be instantiated for different parameter values. The specification is given\nin a parameterized temporal logic that allows for data variables as well as\nparameter variables that encode properties of the environment. Our synthesis\nmethod runs in a counterexample-guided loop consisting of four main steps:\nFirst, we use existing techniques to synthesize concrete systems for some small\nparameter instantiations. Second, we generalize the concrete systems into a\nparameterized program. Third, we create a proof candidate consisting of an\ninvariant and a ranking function. Fourth, we check the proof candidate for\nconsistency with the program. If the proof succeeds, the parameterized program\nis valid. Otherwise, we identify a parameter value for which the proof fails\nand add a new concrete instance to step one. To generalize programs and create\nproof candidates, we use a combination of anti-unification and syntax-guided\nsynthesis to express syntactic differences between programs as functions of the\nparameters. We evaluate our approach on examples from the literature that have\nbeen extended with parameters as well as new problems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u53cd \u0648\u062d\u062f and \u8bed\u6cd5\u5236\u5bfc\u7efc\u5408\u6765\u7efc\u5408\u53c2\u6570\u5316\u65e0\u9650\u72b6\u6001\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5904\u7406\u53c2\u6570\u5316\u65f6\u5e8f\u903b\u8f91\uff0c\u5e76\u5728\u5b9e\u4f8b\u548c\u6587\u732e\u793a\u4f8b\u4e2d\u5f97\u5230\u8bc4\u4f30\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\u6765\u7efc\u5408\u53c2\u6570\u5316\u7684\u65e0\u9650\u72b6\u6001\u7cfb\u7edf\uff0c\u4ee5\u5904\u7406\u5177\u6709\u6570\u636e\u53d8\u91cf\u548c\u73af\u5883\u5c5e\u6027\u53c2\u6570\u7684\u53c2\u6570\u5316\u65f6\u5e8f\u903b\u8f91\u3002", "method": "\u901a\u8fc7\u5305\u542b\u56db\u4e2a\u4e3b\u8981\u6b65\u9aa4\u7684\u5faa\u73af\u6765\u7efc\u5408\u53c2\u6570\u5316\u7684\u65e0\u9650\u72b6\u6001\u7cfb\u7edf\uff1a1.\u4e3a\u5c0f\u7684\u53c2\u6570\u5b9e\u4f8b\u7efc\u5408\u5177\u4f53\u7684\u7cfb\u7edf\u30022.\u5c06\u5177\u4f53\u7684\u7cfb\u7edf\u6cdb\u5316\u4e3a\u53c2\u6570\u5316\u7a0b\u5e8f\u30023.\u521b\u5efa\u5305\u542b\u4e0d\u53d8\u91cf\u548c\u79e9\u51fd\u6570\u7684\u8bc1\u660e\u5019\u9009\u30024.\u68c0\u67e5\u8bc1\u660e\u5019\u9009\u4e0e\u7a0b\u5e8f\u7684\u76f8\u5bb9\u6027\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u53cd \u0648\u062d\u062f and \u8bed\u6cd5\u5236\u5bfc\u7efc\u5408\u6765\u6cdb\u5316\u7a0b\u5e8f\u548c\u521b\u5efa\u8bc1\u660e\u5019\u9009\uff0c\u4ee5\u5904\u7406\u7a0b\u5e8f\u95f4\u7684\u53e5\u6cd5\u5dee\u5f02\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u7efc\u5408\u53c2\u6570\u5316\u7684\u65e0\u9650\u72b6\u6001\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u53cd \u0648\u062d\u062f and \u8bed\u6cd5\u5236\u5bfc\u7efc\u5408\u6765\u6cdb\u5316\u7a0b\u5e8f\u548c\u521b\u5efa\u8bc1\u660e\u5019\u9009\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u7efc\u5408\u53c2\u6570\u5316\u7684\u65e0\u9650\u72b6\u6001\u7cfb\u7edf\uff0c\u5e76\u80fd\u4e3a\u4e0d\u540c\u7684\u53c2\u6570\u503c\u8fdb\u884c\u5b9e\u4f8b\u5316\u3002\u5b83\u80fd\u591f\u5904\u7406\u5305\u542b\u6570\u636e\u53d8\u91cf\u548c\u73af\u5883\u5c5e\u6027\u7f16\u7801\u53c2\u6570\u53d8\u91cf\u7684\u53c2\u6570\u5316\u65f6\u5e8f\u903b\u8f91\u3002\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u548c\u6587\u732e\u793a\u4f8b\u4e2d\u5f97\u5230\u4e86\u8bc4\u4f30\u3002"}}
{"id": "2508.00117", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00117", "abs": "https://arxiv.org/abs/2508.00117", "authors": ["Md. Ehsanul Haque", "S. M. Jahidul Islam", "Shakil Mia", "Rumana Sharmin", "Ashikuzzaman", "Md Samir Morshed", "Md. Tahmidul Huque"], "title": "StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection", "comment": "Accepted and presented paper of THE 16th INTERNATIONAL IEEE\n  CONFERENCE ON COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT)\n  INDIA", "summary": "Liver diseases are a serious health concern in the world, which requires\nprecise and timely diagnosis to enhance the survival chances of patients. The\ncurrent literature implemented numerous machine learning and deep learning\nmodels to classify liver diseases, but most of them had some issues like high\nmisclassification error, poor interpretability, prohibitive computational\nexpense, and lack of good preprocessing strategies. In order to address these\ndrawbacks, we introduced StackLiverNet in this study; an interpretable stacked\nensemble model tailored to the liver disease detection task. The framework uses\nadvanced data preprocessing and feature selection technique to increase model\nrobustness and predictive ability. Random undersampling is performed to deal\nwith class imbalance and make the training balanced. StackLiverNet is an\nensemble of several hyperparameter-optimized base classifiers, whose\ncomplementary advantages are used through a LightGBM meta-model. The provided\nmodel demonstrates excellent performance, with the testing accuracy of 99.89%,\nCohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and\nefficient training and inference speeds that are amenable to clinical practice\n(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local\nInterpretable Model-Agnostic Explanations (LIME) are applied to generate\ntransparent explanations of individual predictions, revealing high\nconcentrations of Alkaline Phosphatase and moderate SGOT as important\nobservations of liver disease. Also, SHAP was used to rank features by their\nglobal contribution to predictions, while the Morris method confirmed the most\ninfluential features through sensitivity analysis.", "AI": {"tldr": "StackLiverNet\u662f\u4e00\u79cd\u7ed3\u5408\u4e86\u6570\u636e\u9884\u5904\u7406\u3001\u7279\u5f81\u9009\u62e9\u548c\u96c6\u6210\u5b66\u4e60\u7684\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u7528\u4e8e\u809d\u810f\u75be\u75c5\u68c0\u6d4b\uff0c\u51c6\u786e\u7387\u9ad8\uff0c\u901f\u5ea6\u5feb\uff0c\u5e76\u80fd\u63d0\u4f9b\u5173\u952e\u6307\u6807\u7684\u8bca\u65ad\u4f9d\u636e\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u809d\u810f\u75be\u75c5\u68c0\u6d4b\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u8bef\u5206\u7c7b\u9519\u8bef\u7387\u9ad8\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u4ee5\u53ca\u7f3a\u4e4f\u826f\u597d\u9884\u5904\u7406\u7b56\u7565\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStackLiverNet\u7684\u53ef\u89e3\u91ca\u5806\u53e0\u96c6\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u5148\u8fdb\u7684\u6570\u636e\u9884\u5904\u7406\u3001\u7279\u5f81\u9009\u62e9\u6280\u672f\u3001\u968f\u673a\u6b20\u91c7\u6837\uff08\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff09\u4ee5\u53ca\u7ecf\u8fc7\u8d85\u53c2\u6570\u4f18\u5316\u7684\u57fa\u5206\u7c7b\u5668\uff0c\u5e76\u4f7f\u7528LightGBM\u4f5c\u4e3a\u5143\u6a21\u578b\u3002", "result": "StackLiverNet\u5728\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e8699.89%\u7684\u51c6\u786e\u7387\u30010.9974\u7684Cohen Kappa\u548c0.9993\u7684AUC\uff0c\u4ec5\u67095\u4e2a\u8bef\u5206\u7c7b\u3002\u8bad\u7ec3\u65f6\u95f4\u4e3a4.2783\u79d2\uff0c\u63a8\u7406\u65f6\u95f4\u4e3a0.1106\u79d2\u3002\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u4e86\u9ad8\u6d53\u5ea6\u7684\u78b1\u6027\u78f7\u9178\u9176\u548c\u4e2d\u7b49\u6d53\u5ea6\u7684SGOT\u662f\u809d\u810f\u75be\u75c5\u7684\u91cd\u8981\u6307\u6807\u3002", "conclusion": "StackLiverNet\u5728\u809d\u810f\u75be\u75c5\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u7387\uff0899.89%\uff09\u3001\u9ad8Cohen Kappa\uff080.9974\uff09\u548c\u9ad8AUC\uff080.9993\uff09\uff0c\u5e76\u4e14\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u5feb\uff0c\u9002\u5408\u4e34\u5e8a\u5e94\u7528\u3002\u901a\u8fc7LIME\u548cSHAP\u7b49\u65b9\u6cd5\uff0c\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\uff0c\u8bc6\u522b\u51fa\u78b1\u6027\u78f7\u9178\u9176\u548cSGOT\u7b49\u5173\u952e\u6307\u6807\u5bf9\u809d\u810f\u75be\u75c5\u8bca\u65ad\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00285", "categories": ["cs.CL", "I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2508.00285", "abs": "https://arxiv.org/abs/2508.00285", "authors": ["Peixian Li", "Yu Tian", "Ruiqi Tu", "Chengkai Wu", "Jingjing Ren", "Jingsong Li"], "title": "Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering", "comment": "23 pages, 8 figures", "summary": "Objective: Large Language Models (LLMs) demonstrate significant capabilities\nin medical text understanding and generation. However, their diagnostic\nreliability in complex clinical scenarios remains limited. This study aims to\nenhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We\npropose an Etiology-Aware Attention Steering Framework to integrate structured\nclinical reasoning into LLM-based diagnosis. Specifically, we first construct\nClinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines\nfor three representative acute abdominal emergencies: acute appendicitis, acute\npancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head\nIdentification algorithm to pinpoint attention heads crucial for the model's\netiology reasoning. To ensure reliable clinical reasoning alignment, we\nintroduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds\netiological reasoning cues into input representations and steers the selected\nEtiology-Aware Heads toward critical information through a Reasoning-Guided\nLoss function. Result: On the Consistent Diagnosis Cohort, our framework\nimproves average diagnostic accuracy by 15.65% and boosts the average Reasoning\nFocus Score by 31.6% over baselines. External validation on the Discrepant\nDiagnosis Cohort further confirms its effectiveness in enhancing diagnostic\naccuracy. Further assessments via Reasoning Attention Frequency indicate that\nour models exhibit enhanced reliability when faced with real-world complex\nscenarios. Conclusion: This study presents a practical and effective approach\nto enhance clinical reasoning in LLM-based diagnosis. By aligning model\nattention with structured CRS, the proposed framework offers a promising\nparadigm for building more interpretable and reliable AI diagnostic systems in\ncomplex clinical settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u75c5\u56e0\u611f\u77e5\u6ce8\u610f\u529b\u5f15\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u7ed3\u6784\u5316\u4e34\u5e8a\u63a8\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4e34\u5e8a\u573a\u666f\u4e0b\u7684\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u6587\u672c\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u590d\u6742\u7684\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u8bca\u65ad\u53ef\u9760\u6027\u4ecd\u7136\u6709\u9650\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u7684\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u75c5\u56e0\u611f\u77e5\u6ce8\u610f\u529b\u5f15\u5bfc\u6846\u67b6\uff08Etiology-Aware Attention Steering Framework\uff09\uff0c\u5305\u62ec\u6784\u5efa\u4e34\u5e8a\u63a8\u7406\u811a\u624b\u67b6\uff08CRS\uff09\u3001\u5f00\u53d1\u75c5\u56e0\u611f\u77e5\u5934\u90e8\u8bc6\u522b\u7b97\u6cd5\uff0c\u4ee5\u53ca\u5f15\u5165\u63a8\u7406\u5f15\u5bfc\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08Reasoning-Guided Parameter-Efficient Fine-tuning\uff09\uff0c\u4ee5\u6574\u5408\u7ed3\u6784\u5316\u4e34\u5e8a\u63a8\u7406\u5230\u5927\u8bed\u8a00\u6a21\u578b\u8bca\u65ad\u4e2d\u3002", "result": "\u5728\u4e00\u81f4\u6027\u8bca\u65ad\u961f\u5217\u4e2d\uff0c\u672c\u6846\u67b6\u5c06\u5e73\u5747\u8bca\u65ad\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 15.65%\uff0c\u5e73\u5747\u63a8\u7406\u5173\u6ce8\u5f97\u5206\u63d0\u9ad8\u4e86 31.6%\u3002\u5728\u5dee\u5f02\u6027\u8bca\u65ad\u961f\u5217\u7684\u5916\u90e8\u9a8c\u8bc1\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u5176\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u7684\u6709\u6548\u6027\u3002\u901a\u8fc7\u63a8\u7406\u6ce8\u610f\u529b\u9891\u7387\u7684\u8fdb\u4e00\u6b65\u8bc4\u4f30\u8868\u660e\uff0c\u5728\u9762\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u573a\u666f\u65f6\uff0c\u672c\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bca\u65ad\u80fd\u529b\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u6ce8\u610f\u529b\u4e0e\u7ed3\u6784\u5316\u4e34\u5e8a\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u4e3a\u6784\u5efa\u53ef\u89e3\u91ca\u3001\u53ef\u9760\u7684 AI \u8bca\u65ad\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2508.00271", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.00271", "abs": "https://arxiv.org/abs/2508.00271", "authors": ["Hongjin Qian", "Zheng Liu"], "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning", "comment": "Technical Report, 14 pages", "summary": "In this work, we propose MetaAgent, an agentic paradigm inspired by the\nprinciple of learning-by-doing, where expertise is developed through hands-on\npractice and continual self-improvement. MetaAgent starts with a minimal\nworkflow, equipped only with basic reasoning and adaptive help-seeking\nabilities. When a knowledge gap is encountered, MetaAgent generates natural\nlanguage help requests, which are routed to the most suitable external tool by\na dedicated tool router. As MetaAgent solves tasks, it continually conducts\nself-reflection and answer verification, distilling actionable experience into\nconcise texts that are dynamically incorporated into future task contexts.\nBesides, MetaAgent autonomously builds in-house tools and a persistent\nknowledge base by organizing its tool-use history, further enhancing its\nability to retrieve and integrate relevant information We term this continual,\ndata-driven process as \\textit{meta tool learning}, through which MetaAgent\nincrementally refines its reasoning and tool-use strategies, without changing\nmodel parameters or requiring further post-training. Evaluated on challenging\nknowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,\nMetaAgent consistently outperforms workflow-based baselines and matches or\nexceeds end-to-end trained agents, demonstrating the promise of self-evolving\nagentic systems for robust, general-purpose knowledge discovery. We provide our\nsource codes in https://github.com/qhjqhj00/MetaAgent.", "AI": {"tldr": "MetaAgent \u662f\u4e00\u79cd\u53d7\u201c\u8fb9\u505a\u8fb9\u5b66\u201d\u542f\u53d1\u7684\u4ee3\u7406\u8303\u4f8b\uff0c\u5b83\u901a\u8fc7\u5b9e\u8df5\u3001\u81ea\u6211\u53cd\u601d\u3001\u751f\u6210\u5e2e\u52a9\u8bf7\u6c42\u4ee5\u53ca\u81ea\u4e3b\u6784\u5efa\u5de5\u5177\u548c\u77e5\u8bc6\u5e93\u6765\u4e0d\u65ad\u6539\u8fdb\u5176\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u5728\u77e5\u8bc6\u53d1\u73b0\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u81ea\u6211\u8fdb\u5316\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u901a\u8fc7\u5b9e\u8df5\u548c\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u6765\u53d1\u5c55\u4e13\u4e1a\u77e5\u8bc6\u7684\u4ee3\u7406\u8303\u4f8b\uff0c\u7c7b\u4f3c\u4e8e\u201c\u8fb9\u505a\u8fb9\u5b66\u201d\u7684\u539f\u7406\u3002", "method": "MetaAgent \u7684\u65b9\u6cd5\u5305\u62ec\u4e00\u4e2a\u7531\u57fa\u672c\u63a8\u7406\u548c\u9002\u5e94\u6027\u5bfb\u6c42\u5e2e\u52a9\u80fd\u529b\u7ec4\u6210\u7684\u6700\u5c0f\u5de5\u4f5c\u6d41\u3002\u5f53\u9047\u5230\u77e5\u8bc6\u5dee\u8ddd\u65f6\uff0c\u5b83\u4f1a\u751f\u6210\u81ea\u7136\u8bed\u8a00\u5e2e\u52a9\u8bf7\u6c42\uff0c\u5e76\u901a\u8fc7\u4e13\u95e8\u7684\u5de5\u5177\u8def\u7531\u5668\u8def\u7531\u5230\u6700\u5408\u9002\u7684\u5916\u90e8\u5de5\u5177\u3002MetaAgent \u901a\u8fc7\u81ea\u6211\u53cd\u601d\u548c\u7b54\u6848\u9a8c\u8bc1\u6765\u4e0d\u65ad\u63d0\u70bc\u7ecf\u9a8c\uff0c\u5e76\u5c06\u5176\u7eb3\u5165\u672a\u6765\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u4e2d\u3002\u6b64\u5916\uff0c\u5b83\u901a\u8fc7\u7ec4\u7ec7\u5de5\u5177\u4f7f\u7528\u5386\u53f2\u6765\u81ea\u4e3b\u6784\u5efa\u5185\u90e8\u5de5\u5177\u548c\u6301\u4e45\u7684\u77e5\u8bc6\u5e93\uff0c\u4ee5\u589e\u5f3a\u4fe1\u606f\u68c0\u7d22\u548c\u96c6\u6210\u80fd\u529b\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u88ab\u79f0\u4e3a\u201c\u5143\u5de5\u5177\u5b66\u4e60\u201d\uff0c\u5b83\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u53c2\u6570\u6216\u9700\u8981\u8fdb\u4e00\u6b65\u8fdb\u884c\u540e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u9010\u6b65\u4f18\u5316\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7b56\u7565\u3002", "result": "MetaAgent \u5728 GAIA\u3001WebWalkerQA \u548c BrowseCamp \u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u65b9\u6cd5\uff0c\u5e76\u80fd\u4e0e\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u4ee3\u7406\u76f8\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u3002", "conclusion": "MetaAgent \u5728\u5177\u6709\u6311\u6218\u6027\u7684\u77e5\u8bc6\u53d1\u73b0\u57fa\u51c6\uff08\u5305\u62ec GAIA\u3001WebWalkerQA \u548c BrowseCamp\uff09\u4e0a\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u57fa\u7ebf\uff0c\u5e76\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u4ee3\u7406\uff0c\u8bc1\u660e\u4e86\u81ea\u8fdb\u5316\u4ee3\u7406\u7cfb\u7edf\u5728\u9c81\u68d2\u3001\u901a\u7528\u7684\u77e5\u8bc6\u53d1\u73b0\u65b9\u9762\u7684\u524d\u666f\u3002"}}
{"id": "2508.00061", "categories": ["quant-ph", "hep-lat", "hep-ph", "nucl-th"], "pdf": "https://arxiv.org/pdf/2508.00061", "abs": "https://arxiv.org/abs/2508.00061", "authors": ["Anthony N. Ciavarella", "Siddharth Hariprakash", "Jad C. Halimeh", "Christian W. Bauer"], "title": "Truncation uncertainties for accurate quantum simulations of lattice gauge theories", "comment": "27 pages, 8 figures", "summary": "The encoding of lattice gauge theories onto quantum computers requires a\ndiscretization of the gauge field's Hilbert space on each link, which presents\nerrors with respect to the Kogut--Susskind limit. In the electric basis,\nHilbert space fragmentation has recently been shown to limit the excitation of\nlarge electric fields. Here, we leverage this to develop a formalism for\nestimating the size of truncation errors in the electric basis. Generically,\nthe truncation error falls off as a factorial of the field truncation. Examples\nof this formalism are applied to the Schwinger model and a pure U(1) lattice\ngauge theory. For reasonable choices of parameters, we improve on previous\nerror estimates by a factor of 10^{306}.", "AI": {"tldr": "\u5229\u7528\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u788e\u88c2\u6765\u4f30\u8ba1\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u683c\u70b9\u89c4\u8303\u573a\u8bba\u7684\u622a\u65ad\u8bef\u5dee\uff0c\u7cbe\u5ea6\u663e\u8457\u63d0\u9ad8\u3002", "motivation": "\u683c\u70b9\u89c4\u8303\u573a\u8bba\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u7684\u7f16\u7801\u9700\u8981\u5728\u6bcf\u4e2a\u94fe\u8def\u4e0a\u5bf9\u89c4\u8303\u573a\u8fdb\u884c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u79bb\u6563\u5316\uff0c\u8fd9\u5728Kogut-Susskind\u6781\u9650\u4e0b\u4f1a\u5f15\u5165\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u788e\u88c2\u6765\u4f30\u8ba1\u7535\u57fa\u4e2d\u7684\u622a\u65ad\u8bef\u5dee\u7684\u7b97\u6cd5\u3002", "result": "\u4e0e\u4ee5\u524d\u7684\u8bef\u5dee\u4f30\u8ba1\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u7406\u7684\u53c2\u6570\u9009\u62e9\u4e0b\uff0c\u8bef\u5dee\u4f30\u8ba1\u7cbe\u5ea6\u63d0\u9ad8\u4e8610^306\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u788e\u88c2\u6765\u4f30\u8ba1\u7535\u57fa\u4e2d\u7684\u622a\u65ad\u8bef\u5dee\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u859b\u5b9a\u8c14\u6a21\u578b\u548c\u7eafU(1)\u683c\u70b9\u89c4\u8303\u573a\u8bba\u3002"}}
{"id": "2508.00437", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.00437", "abs": "https://arxiv.org/abs/2508.00437", "authors": ["Florian Ginzel", "Javad Kazemi", "Valentin Torggler", "Wolfgang Lechner"], "title": "Replacement-Type Quantum Gates", "comment": null, "summary": "We introduce the paradigm of replacement-type quantum gates. This type of\ngate introduces input qubits, candidate qubits, and output qubits. The\ncandidate qubits are prepared such, that a displacement conditional on the\ninput qubit results in the targeted output state. Finally, the circuit\ncontinues with the output qubits constructed from the candidate qubits instead\nof the input qubits, thus the name \"replacement-type gate\". We present examples\nof replacement-type $X$ and $\\mathrm{CNOT}$ gates realized with spin qubits and\nwith neutral atom qubits. By making use of the extended Hilbert space,\nincluding the position of the particles, these gates approximately preserve the\ninnate noise bias of the qubits. The gate preserves the noise-bias which\nmotivates advanced quantum computer architectures with error correction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u95e8\u7c7b\u578b\u2014\u2014\u66ff\u6362\u578b\u91cf\u5b50\u95e8\uff0c\u5b83\u80fd\u8fd1\u4f3c\u4fdd\u6301\u91cf\u5b50\u6bd4\u7279\u7684\u566a\u58f0\u504f\u5dee\uff0c\u4e3a\u9ad8\u7ea7\u91cf\u5b50\u8ba1\u7b97\u67b6\u6784\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "\u4e3a\u4e86\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u8fd1\u4f3c\u4fdd\u6301\u91cf\u5b50\u6bd4\u7279\u56fa\u6709\u566a\u58f0\u504f\u5dee\u7684\u65b0\u578b\u91cf\u5b50\u95e8\uff0c\u4ee5\u652f\u6301\u5177\u6709\u7ea0\u9519\u529f\u80fd\u7684\u9ad8\u7ea7\u91cf\u5b50\u8ba1\u7b97\u673a\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u66ff\u6362\u578b\u91cf\u5b50\u95e8\uff08replacement-type quantum gates\uff09\uff0c\u5305\u62ec\u66ff\u6362\u578bX\u95e8\u548cCNOT\u95e8\u3002\u5229\u7528\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u548c\u4e2d\u6027\u539f\u5b50\u91cf\u5b50\u6bd4\u7279\u4f5c\u4e3a\u7269\u7406\u5b9e\u73b0\uff0c\u5e76\u901a\u8fc7\u6269\u5c55\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff08\u5305\u542b\u7c92\u5b50\u4f4d\u7f6e\uff09\u6765\u8fd1\u4f3c\u4fdd\u6301\u91cf\u5b50\u6bd4\u7279\u7684\u56fa\u6709\u566a\u58f0\u504f\u5dee\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u66ff\u6362\u578bX\u95e8\u548cCNOT\u95e8\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u80fd\u591f\u8fd1\u4f3c\u4fdd\u6301\u91cf\u5b50\u6bd4\u7279\u7684\u56fa\u6709\u566a\u58f0\u504f\u5dee\u3002\u8fd9\u79cd\u7279\u6027\u5bf9\u4e8e\u8bbe\u8ba1\u5177\u6709\u7ea0\u9519\u80fd\u529b\u7684\u9ad8\u7ea7\u91cf\u5b50\u8ba1\u7b97\u673a\u67b6\u6784\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u66ff\u6362\u578b\u91cf\u5b50\u95e8\uff0c\u8fd9\u79cd\u95e8\u901a\u8fc7\u5f15\u5165\u8f93\u5165\u3001\u5019\u9009\u548c\u8f93\u51fa\u91cf\u5b50\u6bd4\u7279\uff0c\u5e76\u5229\u7528\u5019\u9009\u91cf\u5b50\u6bd4\u7279\u7684\u4f4d\u79fb\u64cd\u4f5c\u6765\u5b9e\u73b0\u76ee\u6807\u8f93\u51fa\u6001\uff0c\u6700\u7ec8\u7528\u8f93\u51fa\u91cf\u5b50\u6bd4\u7279\u66ff\u4ee3\u8f93\u5165\u91cf\u5b50\u6bd4\u7279\u3002\u7814\u7a76\u5c55\u793a\u4e86\u5728\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u548c\u4e2d\u6027\u539f\u5b50\u91cf\u5b50\u6bd4\u7279\u4e0a\u5b9e\u73b0\u7684\u66ff\u6362\u578bX\u95e8\u548cCNOT\u95e8\u3002\u5229\u7528\u5305\u542b\u7c92\u5b50\u4f4d\u7f6e\u7684\u6269\u5c55\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff0c\u8fd9\u4e9b\u95e8\u80fd\u591f\u8fd1\u4f3c\u4fdd\u6301\u91cf\u5b50\u6bd4\u7279\u7684\u56fa\u6709\u566a\u58f0\u504f\u5dee\uff0c\u8fd9\u4e3a\u5177\u6709\u7ea0\u9519\u529f\u80fd\u7684\u9ad8\u7ea7\u91cf\u5b50\u8ba1\u7b97\u673a\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2508.00491", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00491", "abs": "https://arxiv.org/abs/2508.00491", "authors": ["Carlo Alessi", "Federico Vasile", "Federico Ceola", "Giulia Pasquale", "Nicol\u00f2 Boccardo", "Lorenzo Natale"], "title": "HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning", "comment": "Paper accepted at IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "summary": "Recent advancements in control of prosthetic hands have focused on increasing\nautonomy through the use of cameras and other sensory inputs. These systems aim\nto reduce the cognitive load on the user by automatically controlling certain\ndegrees of freedom. In robotics, imitation learning has emerged as a promising\napproach for learning grasping and complex manipulation tasks while simplifying\ndata collection. Its application to the control of prosthetic hands remains,\nhowever, largely unexplored. Bridging this gap could enhance dexterity\nrestoration and enable prosthetic devices to operate in more unconstrained\nscenarios, where tasks are learned from demonstrations rather than relying on\nmanually annotated sequences. To this end, we present HannesImitationPolicy, an\nimitation learning-based method to control the Hannes prosthetic hand, enabling\nobject grasping in unstructured environments. Moreover, we introduce the\nHannesImitationDataset comprising grasping demonstrations in table, shelf, and\nhuman-to-prosthesis handover scenarios. We leverage such data to train a single\ndiffusion policy and deploy it on the prosthetic hand to predict the wrist\norientation and hand closure for grasping. Experimental evaluation demonstrates\nsuccessful grasps across diverse objects and conditions. Finally, we show that\nthe policy outperforms a segmentation-based visual servo controller in\nunstructured scenarios. Additional material is provided on our project page:\nhttps://hsp-iit.github.io/HannesImitation", "AI": {"tldr": "\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff08HannesImitationPolicy\uff09\u548c\u6570\u636e\u96c6\uff08HannesImitationDataset\uff09\u7684\u7ed3\u5408\uff0c\u6210\u529f\u63d0\u5347\u4e86\u5047\u80a2\u624b\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e0b\u7684\u6293\u53d6\u80fd\u529b\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u7684\u89c6\u89c9\u4f3a\u670d\u63a7\u5236\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u5047\u80a2\u7075\u6d3b\u6027\u6062\u590d\u80fd\u529b\uff0c\u5e76\u4f7f\u5047\u80a2\u80fd\u5728\u975e\u7ed3\u6784\u5316\u573a\u666f\u4e0b\u8fdb\u884c\u4efb\u52a1\u64cd\u4f5c\uff0c\u5f25\u5408\u4e86\u6a21\u4eff\u5b66\u4e60\u5728\u5047\u80a2\u624b\u63a7\u5236\u9886\u57df\u7684\u5e94\u7528\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHannesImitationPolicy\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b\u6293\u53d6\u6f14\u793a\u7684HannesImitationDataset\u6570\u636e\u96c6\uff0c\u5229\u7528\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u4e86\u5355\u4e00\u7684\u6269\u6563\u7b56\u7565\u6765\u9884\u6d4b\u6293\u53d6\u6240\u9700\u7684\u8155\u90e8\u65b9\u5411\u548c\u624b\u90e8\u95ed\u5408\u3002", "result": "HannesImitationPolicy\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e0b\u7684\u7269\u4f53\u6293\u53d6\uff0c\u5e76\u5728\u591a\u6837\u5316\u7684\u7269\u4f53\u548c\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5c55\u793a\u4e86HannesImitationPolicy\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fdb\u884c\u7269\u4f53\u6293\u53d6\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u6293\u53d6\u6210\u529f\u7387\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u5206\u5272\u7684\u89c6\u89c9\u4f3a\u670d\u63a7\u5236\u5668\u3002"}}
{"id": "2508.00213", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00213", "abs": "https://arxiv.org/abs/2508.00213", "authors": ["Shayan Jalilian", "Abdul Bais"], "title": "SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters", "comment": null, "summary": "The Segment Anything Model (SAM) has demonstrated impressive generalization\nin prompt-based segmentation. Yet, the potential of semantic text prompts\nremains underexplored compared to traditional spatial prompts like points and\nboxes. This paper introduces SAM-PTx, a parameter-efficient approach for\nadapting SAM using frozen CLIP-derived text embeddings as class-level semantic\nguidance. Specifically, we propose a lightweight adapter design called\nParallel-Text that injects text embeddings into SAM's image encoder, enabling\nsemantics-guided segmentation while keeping most of the original architecture\nfrozen. Our adapter modifies only the MLP-parallel branch of each transformer\nblock, preserving the attention pathway for spatial reasoning. Through\nsupervised experiments and ablations on the COD10K dataset as well as low-data\nsubsets of COCO and ADE20K, we show that incorporating fixed text embeddings as\ninput improves segmentation performance over purely spatial prompt baselines.\nTo our knowledge, this is the first work to use text prompts for segmentation\non the COD10K dataset. These results suggest that integrating semantic\nconditioning into SAM's architecture offers a practical and scalable path for\nefficient adaptation with minimal computational complexity.", "AI": {"tldr": "SAM-PTx\u662f\u4e00\u79cd\u901a\u8fc7\u6587\u672c\u63d0\u793a\uff08CLIP\u884d\u751f\u7684\u6587\u672c\u5d4c\u5165\uff09\u6765\u589e\u5f3aSAM\u5206\u5272\u80fd\u529b\u7684\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728COD10K\u3001COCO\u548cADE20K\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u7eaf\u7a7a\u95f4\u63d0\u793a\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u57fa\u4e8e\u6587\u672c\u7684\u8bed\u4e49\u63d0\u793a\u5728SAM\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u8865\u5145\u4f20\u7edf\u7684\u7a7a\u95f4\u63d0\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aParallel-Text\u7684\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4fee\u6539Transformer\u5757\u7684MLP\u5e76\u884c\u5206\u652f\uff0c\u5c06\u6587\u672c\u5d4c\u5165\u6ce8\u5165SAM\u7684\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u540c\u65f6\u4fdd\u6301\u5927\u90e8\u5206\u539f\u59cb\u67b6\u6784\u51bb\u7ed3\u3002", "result": "\u5728COD10K\u6570\u636e\u96c6\u4ee5\u53caCOCO\u548cADE20K\u7684\u4f4e\u6570\u636e\u5b50\u96c6\u4e0a\u8fdb\u884c\u7684\u76d1\u7763\u5b9e\u9a8c\u548c\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u8f93\u5165\u56fa\u5b9a\u7684\u6587\u672c\u5d4c\u5165\u53ef\u4ee5\u63d0\u9ad8\u5206\u5272\u6027\u80fd\uff0c\u5e76\u4e14\u8fd9\u662f\u9996\u6b21\u5728COD10K\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u6587\u672c\u63d0\u793a\u8fdb\u884c\u5206\u5272\u7684\u7814\u7a76\u3002", "conclusion": "SAM-PTx\u901a\u8fc7\u5728SAM\u7684\u56fe\u50cf\u7f16\u7801\u5668\u4e2d\u6ce8\u5165\u7531CLIP\u884d\u751f\u7684\u6587\u672c\u5d4c\u5165\u4f5c\u4e3a\u7c7b\u522b\u7ea7\u8bed\u4e49\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u7684SAM\u9002\u5e94\uff0c\u4ece\u800c\u5728\u7a7a\u95f4\u63d0\u793a\u57fa\u7840\u4e0a\u63d0\u9ad8\u4e86\u5206\u5272\u6027\u80fd\uff0c\u4e3aSAM\u7684\u6709\u6548\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2508.00683", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.00683", "abs": "https://arxiv.org/abs/2508.00683", "authors": ["Ji Hoon Ryoo", "Cheol-Hwan Park"], "title": "Lippmann-Schwinger Approach for Accurate Photoelectron Wavefunctions and Angle-Resolved Photoemission Spectra from First Principles", "comment": "8 pages, 2 figures", "summary": "We present a conceptually simple and technically straightforward method for\ncalculating photoelectron wavefunctions that is easily integrable with standard\nwavefunction-based density-functional-theory packages. Our method is based on\nthe Lippmann-Schwinger equation, naturally incorporating the boundary condition\nthat the final photoelectron state must satisfy. The calculated results are in\ngood agreement with the measured photon-energy- and polarization-dependence of\nthe angle-resolved photoemission spectroscopy (ARPES) of graphene, the\nphoton-energy-dependent evolution of the so-called dark corridor arising from\nthe pseudospin, and WSe\\textsubscript{2}, the circular dichroism reflecting the\nhidden orbital polarization. Our study opens doors to do-it-yourself\nsimulations of ARPES with standard density-functional-theory packages, of\ncrucial importance in the era of ``quantum materials,'' whose key experimental\ntool is ARPES.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684DFT\u517c\u5bb9\u7684ARPES\u6a21\u62df\u65b0\u65b9\u6cd5\uff0c\u7ed3\u679c\u4e0e\u5b9e\u9a8c\u543b\u5408\uff0c\u9002\u7528\u4e8e\u91cf\u5b50\u6750\u6599\u7814\u7a76\u3002", "motivation": "\u4e3a\u4e86\u5728\u91cf\u5b50\u6750\u6599\u65f6\u4ee3\uff0c\u4ee5ARPES\u4e3a\u5173\u952e\u5b9e\u9a8c\u5de5\u5177\uff0c\u5f00\u53d1\u6613\u4e8e\u4e0e\u6807\u51c6DFT\u7a0b\u5e8f\u5305\u96c6\u6210\u7684ARPES\u6a21\u62df\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLippmann-Schwinger\u65b9\u7a0b\u7684\u8ba1\u7b97\u5149\u7535\u5b50\u6ce2\u51fd\u6570\u7684\u65b9\u6cd5\u3002", "result": "\u8ba1\u7b97\u7ed3\u679c\u4e0e\u77f3\u58a8\u70ef\u548cWSe2\u7684ARPES\u5b9e\u9a8c\u6d4b\u91cf\u7ed3\u679c\uff08\u5149\u5b50\u80fd\u91cf\u548c\u6781\u5316\u4f9d\u8d56\u6027\u3001\u6697\u8d70\u5eca\u6f14\u5316\u3001\u5706\u4e8c\u8272\u6027\uff09\u543b\u5408\u826f\u597d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLippmann-Schwinger\u65b9\u7a0b\u8ba1\u7b97\u5149\u7535\u5b50\u6ce2\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6613\u4e8e\u4e0e\u73b0\u6709\u7684\u57fa\u4e8e\u6ce2\u51fd\u6570\u7684\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u7a0b\u5e8f\u5305\u96c6\u6210\uff0c\u5e76\u81ea\u7136\u5730\u5305\u542b\u4e86\u5149\u7535\u5b50\u6001\u7684\u8fb9\u754c\u6761\u4ef6\u3002\u8ba1\u7b97\u7ed3\u679c\u4e0e\u77f3\u58a8\u70ef\u548cWSe2\u7684\u5149\u52a9\u8c31\uff08ARPES\uff09\u5b9e\u9a8c\u6d4b\u91cf\u7ed3\u679c\uff08\u5305\u62ec\u5149\u5b50\u80fd\u91cf\u548c\u6781\u5316\u4f9d\u8d56\u6027\u3001\u7531\u8d5d\u81ea\u65cb\u5f15\u8d77\u7684\u6697\u8d70\u5eca\u4ee5\u53ca\u53cd\u6620\u9690\u85cf\u8f68\u9053\u6781\u5316\u7684\u5706\u4e8c\u8272\u6027\uff09\u543b\u5408\u826f\u597d\u3002\u8be5\u7814\u7a76\u4e3a\u4f7f\u7528\u6807\u51c6\u7684DFT\u7a0b\u5e8f\u5305\u8fdb\u884cARPES\u6a21\u62df\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u8fd9\u5bf9\u4e8e\u4ee5ARPES\u4e3a\u5173\u952e\u5b9e\u9a8c\u5de5\u5177\u7684\u201c\u91cf\u5b50\u6750\u6599\u201d\u65f6\u4ee3\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.00653", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2508.00653", "abs": "https://arxiv.org/abs/2508.00653", "authors": ["Luc\u00eda G\u00f3mez \u00c1lvarez", "Sebastian Rudolph"], "title": "Putting Perspective into OWL [sic]: Complexity-Neutral Standpoint Reasoning for Ontology Languages via Monodic S5 over Counting Two-Variable First-Order Logic (Extended Version with Appendix)", "comment": null, "summary": "Standpoint extensions of knowledge representation formalisms have been\nrecently introduced as a means to incorporate multi-perspective modelling and\nreasoning through modal operators that attribute pieces of knowledge to\nspecific entities or agents. In these extensions, the integration between\nconceptual modelling and perspective annotations can vary in strength, with\nmonodic standpoint extensions offering a well-balanced approach. They allow for\nadvanced modelling features, such as the expression of rigid concepts, while\nmaintaining desirable reasoning complexity.\n  We consider the extension of C2--the counting two-variable fragment of\nfirst-order logic--by monodic standpoints. At the heart of our work is a\npolynomial-time translation of formulas in this extended formalism into\nstandard, standpoint-free C2, a result that relies on intricate model-theoretic\narguments. Thanks to this translation, the satisfiability problem remains at\nthe same complexity level: NExpTime-complete, as in plain C2. Since our\nformalism subsumes monodic S5 over C2, this result also marks a substantial\nadvancement in the study of first-order modal logics.\n  From a practical standpoint, this means that highly expressive description\nlogics such as SHOIQBs and SROIQBs--which underpin the widely adopted OWL 1 and\nOWL 2 ontology languages standardised by the W3C--can be extended with monodic\nstandpoints without increasing the standard reasoning complexity.\n  We further prove that NExpTime-hardness arises even in significantly less\nexpressive description logics, as long as they include both nominals and\nmonodic standpoints. Moreover, we show that if the monodicity restriction is\nrelaxed even slightly in the presence of inverse roles, functionality, and\nnominals, the satisfiability problem becomes undecidable.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u5355\u5b50\u7acb\u573a\u5f15\u5165C2\u903b\u8f91\uff0c\u63d0\u51fa\u591a\u9879\u5f0f\u65f6\u95f4\u7ffb\u8bd1\uff0c\u4fdd\u6301NExpTime-complete\u590d\u6742\u5ea6\uff0c\u5e76\u5bf9OWL 1/2\u7b49\u63cf\u8ff0\u903b\u8f91\u7684\u6269\u5c55\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u540d\u8bcd\u548c\u5355\u5b50\u7acb\u573a\u5bf9\u590d\u6742\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u4e3a\u4e86\u5728\u77e5\u8bc6\u8868\u793a\u5f62\u5f0f\u4e3b\u4e49\u4e2d\u6574\u5408\u591a\u89c6\u89d2\u5efa\u6a21\u548c\u63a8\u7406\uff0c\u5f15\u5165\u4e86\u7acb\u573a\u7684\u6982\u5ff5\u3002\u5355\u5b50\u7acb\u573a\u6269\u5c55\u5728\u63d0\u4f9b\u9ad8\u7ea7\u5efa\u6a21\u529f\u80fd\uff08\u5982\u521a\u6027\u6982\u5ff5\uff09\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u53ef\u63a5\u53d7\u7684\u63a8\u7406\u590d\u6742\u5ea6\uff0c\u56e0\u6b64\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u610f\u4e49\u3002\u672c\u7814\u7a76\u65e8\u5728\u5c06\u5355\u5b50\u7acb\u573a\u6269\u5c55\u5230C2\u903b\u8f91\uff0c\u5e76\u5206\u6790\u5176\u63a8\u7406\u590d\u6742\u5ea6\u3002", "method": "\u672c\u6587\u7684\u6838\u5fc3\u65b9\u6cd5\u662f\u63d0\u51fa\u4e00\u4e2a\u591a\u9879\u5f0f\u65f6\u95f4\u7ffb\u8bd1\uff0c\u5c06\u6269\u5c55C2\u903b\u8f91\uff08\u5305\u542b\u5355\u5b50\u7acb\u573a\uff09\u7684\u516c\u5f0f\u8f6c\u5316\u4e3a\u6807\u51c6\u7684\u3001\u65e0\u7acb\u573a\u7684C2\u516c\u5f0f\u3002\u8fd9\u4e00\u7ffb\u8bd1\u8fc7\u7a0b\u4f9d\u8d56\u4e8e\u7cbe\u7ec6\u7684\u6a21\u578b\u7406\u8bba\u8bba\u8bc1\u3002", "result": "\u5728C2\u903b\u8f91\u4e2d\u5f15\u5165\u5355\u5b50\u7acb\u573a\u540e\uff0c\u5176\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u7684\u590d\u6742\u5ea6\u4ecd\u4fdd\u6301\u4e3aNExpTime-complete\uff0c\u4e0e\u7eafC2\u76f8\u540c\u3002\u8fd9\u5f97\u76ca\u4e8e\u6240\u63d0\u51fa\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u7ffb\u8bd1\u3002\u8be5\u7ed3\u679c\u4e5f\u63a8\u8fdb\u4e86\u5bf9\u4e00\u9636\u6a21\u6001\u903b\u8f91\u7684\u7814\u7a76\uff0c\u56e0\u4e3a\u5b83\u5305\u542b\u4e86\u5355\u5b50S5\u5728C2\u4e0a\u7684\u6269\u5c55\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u5728\u5305\u542b\u540d\u8bcd\u548c\u5355\u5b50\u7acb\u573a\u7684\u63cf\u8ff0\u903b\u8f91\u4e2d\uff0cNExpTime-hardness\u662f\u5b58\u5728\u7684\uff0c\u800c\u653e\u5bbd\u5355\u5b50\u7acb\u573a\u9650\u5236\u5219\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u5224\u5b9a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5c06\u5355\u5b50\u7acb\u573a\u7684\u6982\u5ff5\u5f15\u5165\u4e86C2\u903b\u8f91\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u9879\u5f0f\u65f6\u95f4\u7ffb\u8bd1\uff0c\u5c06\u6269\u5c55\u540e\u7684\u516c\u5f0f\u8f6c\u5316\u4e3a\u6807\u51c6\u7684C2\u516c\u5f0f\u3002\u8fd9\u4f7f\u5f97\u6269\u5c55\u540e\u7684\u5f62\u5f0f\u5316\u5728\u4fdd\u6301\u4e0e\u7eafC2\u76f8\u540c\u7684NExpTime-complete\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u4e5f\u4e3a\u66f4\u9ad8\u7ea7\u7684\u63cf\u8ff0\u903b\u8f91\uff08\u5982OWL 1\u548cOWL 2\uff09\u7684\u5355\u5b50\u7acb\u573a\u6269\u5c55\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u5728\u5305\u542b\u540d\u8bcd\u548c\u5355\u5b50\u7acb\u573a\u7684\u63cf\u8ff0\u903b\u8f91\u4e2d\uff0cNExpTime-hardness\u662f\u4e0d\u53ef\u907f\u514d\u7684\uff0c\u5e76\u4e14\u7a0d\u5fae\u653e\u5bbd\u5355\u5b50\u7acb\u573a\u9650\u5236\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u5224\u5b9a\u6027\u3002"}}
{"id": "2508.00127", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00127", "abs": "https://arxiv.org/abs/2508.00127", "authors": ["Saleh Nikooroo", "Thomas Engel"], "title": "Structured Transformations for Stable and Interpretable Neural Computation", "comment": null, "summary": "Despite their impressive performance, contemporary neural networks often lack\nstructural safeguards that promote stable learning and interpretable behavior.\nIn this work, we introduce a reformulation of layer-level transformations that\ndeparts from the standard unconstrained affine paradigm. Each transformation is\ndecomposed into a structured linear operator and a residual corrective\ncomponent, enabling more disciplined signal propagation and improved training\ndynamics. Our formulation encourages internal consistency and supports stable\ninformation flow across depth, while remaining fully compatible with standard\nlearning objectives and backpropagation. Through a series of synthetic and\nreal-world experiments, we demonstrate that models constructed with these\nstructured transformations exhibit improved gradient conditioning, reduced\nsensitivity to perturbations, and layer-wise robustness. We further show that\nthese benefits persist across architectural scales and training regimes. This\nstudy serves as a foundation for a more principled class of neural\narchitectures that prioritize stability and transparency-offering new tools for\nreasoning about learning behavior without sacrificing expressive power.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u7ea7\u53d8\u6362\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u7ebf\u6027\u7b97\u5b50\u548c\u6b8b\u5dee\u6821\u6b63\u5206\u91cf\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u795e\u7ecf\u7f51\u7edc\u5c3d\u7ba1\u6027\u80fd\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\uff0c\u4f46\u5e38\u5e38\u7f3a\u4e4f\u4fc3\u8fdb\u7a33\u5b9a\u5b66\u4e60\u548c\u53ef\u89e3\u91ca\u884c\u4e3a\u7684\u7ed3\u6784\u5316\u4fdd\u969c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u7ea7\u53d8\u6362\u7684\u91cd\u6784\u65b9\u6cd5\uff0c\u5c06\u53d8\u6362\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u7ebf\u6027\u7b97\u5b50\u548c\u6b8b\u5dee\u6821\u6b63\u5206\u91cf\uff0c\u4ee5\u5b9e\u73b0\u66f4\u53d7\u7ea6\u675f\u7684\u4fe1\u53f7\u4f20\u64ad\u548c\u6539\u8fdb\u7684\u8bad\u7ec3\u52a8\u6001\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u91c7\u7528\u7ed3\u6784\u5316\u53d8\u6362\u6784\u5efa\u7684\u6a21\u578b\u5728\u68af\u5ea6\u6761\u4ef6\u3001\u5bf9\u6270\u52a8\u7684\u654f\u611f\u6027\u4ee5\u53ca\u5c42\u7ea7\u7a33\u5065\u6027\u65b9\u9762\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u4e14\u8fd9\u4e9b\u4f18\u52bf\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u65b9\u6848\u4e2d\u90fd\u5f97\u4ee5\u4fdd\u6301\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4e00\u7c7b\u66f4\u89c4\u6574\u7684\u795e\u7ecf\u67b6\u6784\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8be5\u67b6\u6784\u4f18\u5148\u8003\u8651\u7a33\u5b9a\u6027\u548c\u900f\u660e\u5ea6\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u6765\u63a8\u7406\u5b66\u4e60\u884c\u4e3a\u800c\u4e0d\u727a\u7272\u8868\u8fbe\u80fd\u529b\u3002"}}
{"id": "2508.00305", "categories": ["cs.CL", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.00305", "abs": "https://arxiv.org/abs/2508.00305", "authors": ["Ammar Ahmed", "Sheng Di", "Franck Cappello", "Zirui Liu", "Jingoo Han", "Ali Anwar"], "title": "Systematic Evaluation of Optimization Techniques for Long-Context Language Models", "comment": null, "summary": "Large language models (LLMs) excel across diverse natural language processing\ntasks but face resource demands and limited context windows. Although\ntechniques like pruning, quantization, and token dropping can mitigate these\nissues, their efficacy in long-context scenarios and system evaluation remains\nunderexplored. This paper systematically benchmarks these optimizations,\ncharacterizing memory usage, latency, and throughput, and studies how these\nmethods impact the quality of text generation. We first analyze individual\noptimization methods for two LLM architectures supporting long context and then\nsystematically evaluate combinations of these techniques to assess how this\ndeeper analysis impacts performance metrics. We subsequently study the\nscalability of individual optimization methods on a larger variant with 70\nbillion-parameter model. Our novel insights reveal that naive combination\ninference optimization algorithms can adversely affect larger models due to\ncompounded approximation errors, as compared to their smaller counterparts.\nExperiments show that relying solely on F1 obscures these effects by hiding\nprecision-recall trade-offs in question answering tasks. By integrating\nsystem-level profiling with task-specific insights, this study helps LLM\npractitioners and researchers explore and balance efficiency, accuracy, and\nscalability across tasks and hardware configurations.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u6d4b\u8bd5\u4e86LLM\u7684\u526a\u679d\u3001\u91cf\u5316\u548c\u6807\u8bb0\u4e22\u5f03\u6280\u672f\uff0c\u53d1\u73b0\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u66f4\u5927\u7684\u6a21\u578b\u4e0a\uff0c\u8fd9\u4e9b\u6280\u672f\u7684\u7ec4\u5408\u4f7f\u7528\u53ef\u80fd\u9002\u5f97\u5176\u53cd\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408\u7cfb\u7edf\u5206\u6790\u548c\u4efb\u52a1\u6307\u6807\u6765\u4f18\u5316LLM\u3002", "motivation": "LLMs\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u548c\u8d44\u6e90\u9700\u6c42\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u800c\u73b0\u6709\u7684\u4f18\u5316\u6280\u672f\uff08\u526a\u679d\u3001\u91cf\u5316\u3001\u6807\u8bb0\u4e22\u5f03\uff09\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u7684\u6548\u679c\u548c\u7cfb\u7edf\u8bc4\u4f30\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u7cfb\u7edf\u5730\u5bf9LLM\u4f18\u5316\u6280\u672f\uff08\u5982\u526a\u679d\u3001\u91cf\u5316\u548c\u6807\u8bb0\u4e22\u5f03\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8868\u5f81\u5185\u5b58\u4f7f\u7528\u3001\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\uff0c\u5e76\u7814\u7a76\u8fd9\u4e9b\u65b9\u6cd5\u5982\u4f55\u5f71\u54cd\u6587\u672c\u751f\u6210\u8d28\u91cf\u3002\u9996\u5148\u5206\u6790\u4e86\u4e24\u79cd\u652f\u6301\u957f\u4e0a\u4e0b\u6587\u7684LLM\u67b6\u6784\u7684\u5355\u4e2a\u4f18\u5316\u65b9\u6cd5\uff0c\u7136\u540e\u7cfb\u7edf\u5730\u8bc4\u4f30\u8fd9\u4e9b\u6280\u672f\u7684\u7ec4\u5408\uff0c\u6700\u540e\u5728\u66f4\u5927\u7684700\u4ebf\u53c2\u6570\u6a21\u578b\u4e0a\u7814\u7a76\u4e86\u5355\u4e2a\u4f18\u5316\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002", "result": "LLM\u4f18\u5316\u6280\u672f\u7684\u7ec4\u5408\u4f7f\u7528\u53ef\u80fd\u5bf9\u5927\u6a21\u578b\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u56e0\u4e3a\u7d2f\u79ef\u7684\u8fd1\u4f3c\u8bef\u5dee\u4f1a\u6bd4\u5c0f\u6a21\u578b\u66f4\u4e25\u91cd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u4f9d\u8d56F1\u5206\u6570\u4f1a\u63a9\u76d6\u8fd9\u4e9b\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u9690\u85cf\u4e86\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u7cbe\u786e\u7387-\u53ec\u56de\u7387\u6743\u8861\u3002\u901a\u8fc7\u6574\u5408\u7cfb\u7edf\u7ea7\u5206\u6790\u548c\u4efb\u52a1\u7279\u5b9a\u6d1e\u5bdf\uff0c\u4e3aLLM\u4ece\u4e1a\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5e73\u8861\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u65b0\u89c1\u89e3\u3002", "conclusion": "LLM\u4f18\u5316\u6280\u672f\u7684\u7ec4\u5408\u4f7f\u7528\u53ef\u80fd\u5bf9\u5927\u6a21\u578b\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u56e0\u4e3a\u7d2f\u79ef\u7684\u8fd1\u4f3c\u8bef\u5dee\u4f1a\u6bd4\u5c0f\u6a21\u578b\u66f4\u4e25\u91cd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u4f9d\u8d56F1\u5206\u6570\u4f1a\u63a9\u76d6\u8fd9\u4e9b\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u9690\u85cf\u4e86\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u7cbe\u786e\u7387-\u53ec\u56de\u7387\u6743\u8861\u3002\u672c\u7814\u7a76\u901a\u8fc7\u6574\u5408\u7cfb\u7edf\u7ea7\u5206\u6790\u548c\u4efb\u52a1\u7279\u5b9a\u6d1e\u5bdf\uff0c\u4e3aLLM\u4ece\u4e1a\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5728\u4efb\u52a1\u548c\u786c\u4ef6\u914d\u7f6e\u4e4b\u95f4\u5e73\u8861\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u65b0\u89c1\u89e3\u3002"}}
{"id": "2508.00282", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00282", "abs": "https://arxiv.org/abs/2508.00282", "authors": ["Yi-Long Lu", "Jiajun Song", "Chunhui Zhang", "Wei Wang"], "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks", "comment": null, "summary": "Humans constantly generate a diverse range of tasks guided by internal\nmotivations. While generative agents powered by large language models (LLMs)\naim to simulate this complex behavior, it remains uncertain whether they\noperate on similar cognitive principles. To address this, we conducted a\ntask-generation experiment comparing human responses with those of an LLM agent\n(GPT-4o). We find that human task generation is consistently influenced by\npsychological drivers, including personal values (e.g., Openness to Change) and\ncognitive style. Even when these psychological drivers are explicitly provided\nto the LLM, it fails to reflect the corresponding behavioral patterns. They\nproduce tasks that are markedly less social, less physical, and thematically\nbiased toward abstraction. Interestingly, while the LLM's tasks were perceived\nas more fun and novel, this highlights a disconnect between its linguistic\nproficiency and its capacity to generate human-like, embodied goals.We conclude\nthat there is a core gap between the value-driven, embodied nature of human\ncognition and the statistical patterns of LLMs, highlighting the necessity of\nincorporating intrinsic motivation and physical grounding into the design of\nmore human-aligned agents.", "AI": {"tldr": "LLM \u5728\u4efb\u52a1\u751f\u6210\u65b9\u9762\u672a\u80fd\u6a21\u4eff\u4eba\u7c7b\u7684\u5fc3\u7406\u9a71\u52a8\u56e0\u7d20\u548c\u884c\u4e3a\u6a21\u5f0f\uff0c\u751f\u6210\u7ed3\u679c\u504f\u5411\u62bd\u8c61\u4e14\u7f3a\u4e4f\u793e\u4f1a\u6027\u548c\u8eab\u4f53\u6d3b\u52a8\u3002\u5c3d\u7ba1 LLM \u7684\u4efb\u52a1\u66f4\u5177\u65b0\u9896\u6027\u548c\u8da3\u5473\u6027\uff0c\u4f46\u8fd9\u66b4\u9732\u4e86\u5176\u5728\u5177\u8eab\u5316\u76ee\u6807\u751f\u6210\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u5728\u8bbe\u8ba1\u667a\u80fd\u4f53\u65f6\u878d\u5165\u5185\u5728\u52a8\u673a\u548c\u7269\u7406\u57fa\u7840\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u65e8\u5728\u63a2\u7a76\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u5728\u6a21\u62df\u4eba\u7c7b\u7531\u5185\u5728\u52a8\u673a\u9a71\u52a8\u7684\u591a\u6837\u5316\u4efb\u52a1\u751f\u6210\u884c\u4e3a\u65f6\uff0c\u662f\u5426\u9075\u5faa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u8ba4\u77e5\u539f\u7406\u3002", "method": "\u901a\u8fc7\u4e00\u9879\u4efb\u52a1\u751f\u6210\u5b9e\u9a8c\uff0c\u5c06\u4eba\u7c7b\u7684\u53cd\u5e94\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\uff08GPT-4o\uff09\u7684\u53cd\u5e94\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u63a2\u7a76 LLM \u5728\u6a21\u62df\u4eba\u7c7b\u591a\u6837\u5316\u4efb\u52a1\u751f\u6210\u65b9\u9762\u7684\u8ba4\u77e5\u539f\u7406\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4eba\u7c7b\u7684\u4efb\u52a1\u751f\u6210\u53d7\u5230\u4e2a\u4eba\u4ef7\u503c\u89c2\uff08\u5982\u5f00\u653e\u6027\uff09\u548c\u8ba4\u77e5\u98ce\u683c\u7b49\u5fc3\u7406\u9a71\u52a8\u56e0\u7d20\u7684\u663e\u8457\u5f71\u54cd\u3002\u5373\u4f7f\u5c06\u8fd9\u4e9b\u5fc3\u7406\u9a71\u52a8\u56e0\u7d20\u660e\u786e\u63d0\u4f9b\u7ed9 LLM\uff0c\u5176\u751f\u6210\u7684\u4efb\u52a1\u4e5f\u672a\u80fd\u53cd\u6620\u76f8\u5e94\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u8868\u73b0\u51fa\u793e\u4f1a\u6027\u548c\u8eab\u4f53\u6d3b\u52a8\u7684\u503e\u5411\u8f83\u4f4e\uff0c\u4e14\u66f4\u504f\u5411\u4e8e\u62bd\u8c61\u4e3b\u9898\u3002\u6b64\u5916\uff0cLLM \u751f\u6210\u7684\u4efb\u52a1\u867d\u7136\u88ab\u8ba4\u4e3a\u66f4\u6709\u8da3\u548c\u65b0\u9896\uff0c\u4f46\u8fd9\u8868\u660e\u5176\u8bed\u8a00\u80fd\u529b\u4e0e\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7684\u3001\u5177\u8eab\u5316\u7684\u76ee\u6807\u7684\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u8131\u8282\u3002", "conclusion": "LLM \u7684\u76ee\u6807\u751f\u6210\u80fd\u529b\u4e0e\u4eba\u7c7b\u76f8\u6bd4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u7f3a\u4e4f\u4e0e\u5fc3\u7406\u9a71\u52a8\u56e0\u7d20\u548c\u7269\u7406\u73b0\u5b9e\u7684\u8054\u7cfb\u3002\u867d\u7136 LLM \u751f\u6210\u7684\u4efb\u52a1\u53ef\u80fd\u66f4\u5177\u8da3\u5473\u6027\u548c\u65b0\u9896\u6027\uff0c\u4f46\u5b83\u4eec\u672a\u80fd\u53cd\u6620\u4eba\u7c7b\u4efb\u52a1\u751f\u6210\u4e2d\u5e38\u89c1\u7684\u793e\u4f1a\u6027\u548c\u8eab\u4f53\u6d3b\u52a8\u504f\u5411\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u5b9e\u73b0\u66f4\u7b26\u5408\u4eba\u7c7b\u671f\u671b\u7684\u667a\u80fd\u4f53\uff0c\u6709\u5fc5\u8981\u5728\u8bbe\u8ba1\u4e2d\u878d\u5165\u5185\u5728\u52a8\u673a\u548c\u7269\u7406\u57fa\u7840\u3002"}}
{"id": "2508.00065", "categories": ["quant-ph", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2508.00065", "abs": "https://arxiv.org/abs/2508.00065", "authors": ["D. A. Millar", "L. W. Anderson", "E. Altamura", "O. Wallis", "M. E. Sahin", "J. Crain", "S. J. Thomson"], "title": "Imaginary Time Spectral Transforms for Excited State Preparation", "comment": null, "summary": "Excited states of many-body quantum systems play a key role in a wide range\nof physical and chemical phenomena. Unlike ground states, for which many\nefficient variational techniques exist, there are few ways to systematically\nconstruct excited states of generic quantum systems on either classical or\nquantum hardware. To address this challenge, we introduce a general approach\nthat allows us to obtain arbitrary eigenstates of quantum systems at a given\nenergy. By combining the shift-invert mechanism with imaginary time evolution,\nwe are able to avoid explicit inversion of the Hamiltonian and construct\nexcited eigenstates of large many-body quantum systems. We demonstrate the\ntechnique classically by applying it to large disordered spin chains. Based on\nthis approach, we propose a hybrid scheme suitable for near-future quantum\nhardware.", "AI": {"tldr": "A new approach combining shift-invert and imaginary time evolution to find excited states of quantum systems, demonstrated on spin chains and applicable to quantum hardware.", "motivation": "Unlike ground states, there are few systematic ways to construct excited states of generic quantum systems on either classical or quantum hardware.", "method": "By combining the shift-invert mechanism with imaginary time evolution, we avoid explicit inversion of the Hamiltonian and construct excited eigenstates of large many-body quantum systems.", "result": "We demonstrate the technique classically by applying it to large disordered spin chains.", "conclusion": "We propose a hybrid scheme suitable for near-future quantum hardware based on our approach."}}
{"id": "2508.00692", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00692", "abs": "https://arxiv.org/abs/2508.00692", "authors": ["Young-ho Cho", "Hao Zhu", "Duehee Lee", "Ross Baldick"], "title": "Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network", "comment": null, "summary": "For conducting resource adequacy studies, we synthesize multiple long-term\nwind power scenarios of distributed wind farms simultaneously by using the\nspatio-temporal features: spatial and temporal correlation, waveforms, marginal\nand ramp rates distributions of waveform, power spectral densities, and\nstatistical characteristics. Generating the spatial correlation in scenarios\nrequires the design of common factors for neighboring wind farms and\nantithetical factors for distant wind farms. The generalized dynamic factor\nmodel (GDFM) can extract the common factors through cross spectral density\nanalysis, but it cannot closely imitate waveforms. The GAN can synthesize\nplausible samples representing the temporal correlation by verifying samples\nthrough a fake sample discriminator. To combine the advantages of GDFM and GAN,\nwe use the GAN to provide a filter that extracts dynamic factors with temporal\ninformation from the observation data, and we then apply this filter in the\nGDFM to represent both spatial and frequency correlations of plausible\nwaveforms. Numerical tests on the combination of GDFM and GAN have demonstrated\nperformance improvements over competing alternatives in synthesizing wind power\nscenarios from Australia, better realizing plausible statistical\ncharacteristics of actual wind power compared to alternatives such as the GDFM\nwith a filter synthesized from distributions of actual dynamic filters and the\nGAN with direct synthesis without dynamic factors.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408GDFM\u548cGAN\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5408\u6210\u98ce\u529b\u53d1\u7535\u573a\u666f\uff0c\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u5728\u6570\u503c\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u8fdb\u884c\u8d44\u6e90\u5145\u5206\u6027\u7814\u7a76\uff0c\u9700\u8981\u5408\u6210\u591a\u4e2a\u5206\u5e03\u5f0f\u98ce\u7535\u573a\u7684\u957f\u671f\u98ce\u529b\u53d1\u7535\u573a\u666f\uff0c\u5e76\u8003\u8651\u5176\u65f6\u7a7a\u7279\u5f81\uff0c\u5982\u7a7a\u95f4\u548c\u65f6\u95f4\u76f8\u5173\u6027\u3001\u6ce2\u5f62\u3001\u529f\u7387\u8c31\u5bc6\u5ea6\u7b49\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5e7f\u4e49\u52a8\u6001\u56e0\u5b50\u6a21\u578b\uff08GDFM\uff09\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u65b9\u6cd5\uff0c\u5176\u4e2dGAN\u7528\u4e8e\u63d0\u4f9b\u4e00\u4e2a\u63d0\u53d6\u5177\u6709\u65f6\u95f4\u4fe1\u606f\u7684\u52a8\u6001\u56e0\u5b50\u7684\u6ee4\u6ce2\u5668\uff0c\u7136\u540e\u5c06\u8be5\u6ee4\u6ce2\u5668\u5e94\u7528\u4e8eGDFM\u4e2d\uff0c\u4ee5\u540c\u65f6\u8868\u793a\u7a7a\u95f4\u548c\u9891\u7387\u76f8\u5173\u6027\u4ee5\u53ca\u5408\u7406\u7684\u6ce2\u5f62\u3002", "result": "\u6570\u503c\u6d4b\u8bd5\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684GDFM\u548cGAN\u7684\u7ed3\u5408\u65b9\u6cd5\u5728\u5408\u6210\u98ce\u529b\u53d1\u7535\u573a\u666f\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5b9e\u73b0\u5b9e\u9645\u98ce\u529b\u53d1\u7535\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u76f8\u6bd4\u4e8e\u4ec5\u4f7f\u7528GDFM\u6216GAN\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408GDFM\u548cGAN\u7684\u4f18\u52bf\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5408\u6210\u98ce\u529b\u53d1\u7535\u573a\u666f\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u66f4\u597d\u5730\u5b9e\u73b0\u5b9e\u9645\u98ce\u529b\u53d1\u7535\u7684\u7edf\u8ba1\u7279\u6027\u3002"}}
{"id": "2508.00704", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.00704", "abs": "https://arxiv.org/abs/2508.00704", "authors": ["Andrea Pedrielli", "Tommaso Morresi", "Simone Taioli"], "title": "XANES absorption spectra of penta-graphene and penta-SiC2 with different terminations: a computational study", "comment": null, "summary": "In recent research, penta-graphene and penta-SiC2 have emerged as innovative\n2D materials consisting exclusively of pentagons. However, there is still a\nsignificant gap in the theoretical characterization of these materials, which\nhinders progress in their synthesis and potential technological applications.\nThis study aims to close this gap by investigating the X-ray absorption\nnear-edge spectroscopy (XANES) of these materials through ab initio\ncalculations. In particular, we analyze the XANES spectra of penta-graphene in\nits pristine, hydrogenated, and hydroxylated states, and we investigate the\neffects of substitution by a single silicon in both penta-graphene and\npentagraphane. In addition, we calculate the XANES spectra for pristine and\nhydrogenated penta-SiC2. This work sets the stage for the possible\nidentification of penta-graphene and penta-SiC2 phases by X-ray spectroscopy at\nthe experimental level and lays the foundation for the future engineering of\nthe absorption properties of these materials in optical devices.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e94\u77f3\u58a8\u70ef\u548c\u4e94\u78b3\u5316\u7845\u7684X\u5c04\u7ebf\u5438\u6536\u8fd1\u8fb9\u5149\u8c31\uff0c\u4e3a\u5b9e\u9a8c\u8bc6\u522b\u548c\u5149\u5b66\u5e94\u7528\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "motivation": "\u4e3a\u4e86\u5f25\u8865\u7406\u8bba\u8868\u5f81\u7684\u4e0d\u8db3\uff0c\u4fc3\u8fdb\u8fd9\u4e9b\u65b0\u578b\u4e8c\u7ef4\u6750\u6599\u7684\u5408\u6210\u548c\u6f5c\u5728\u6280\u672f\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4ece\u5934\u7b97\u65b9\u6cd5\u7814\u7a76\u4e86\u4e94\u77f3\u58a8\u70ef\uff08\u539f\u59cb\u3001\u6c22\u5316\u3001\u7f9f\u57fa\u5316\uff09\u3001\u7845\u53d6\u4ee3\u7684\u4e94\u77f3\u58a8\u70ef\u548c\u4e94\u77f3\u58a8\u70ef\u4ee5\u53ca\u4e94\u78b3\u5316\u7845\uff08\u539f\u59cb\u3001\u6c22\u5316\uff09\u7684XANES\u5149\u8c31\u3002", "result": "\u8ba1\u7b97\u4e86\u76f8\u5173\u6750\u6599\u7684XANES\u5149\u8c31\uff0c\u4e3a\u5b9e\u9a8c\u4e0a\u901a\u8fc7X\u5c04\u7ebf\u5149\u8c31\u8bc6\u522b\u4e94\u77f3\u58a8\u70ef\u548c\u4e94\u78b3\u5316\u7845\u76f8\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u5728\u5149\u5b66\u5668\u4ef6\u4e2d\u8c03\u63a7\u5176\u5438\u6536\u7279\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "conclusion": "\u672c\u7814\u7a76\u5bf9\u4e94\u77f3\u58a8\u70ef\u53ca\u5176\u884d\u751f\u7269\uff08\u6c22\u5316\u3001\u7f9f\u57fa\u5316\u3001\u7845\u53d6\u4ee3\uff09\u548c\u4e94\u78b3\u5316\u7845\u8fdb\u884c\u4e86X\u5c04\u7ebf\u5438\u6536\u8fd1\u8fb9\u5149\u8c31\uff08XANES\uff09\u7684\u7406\u8bba\u8868\u5f81\u3002"}}
{"id": "2508.00580", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00580", "abs": "https://arxiv.org/abs/2508.00580", "authors": ["Raul Castilla-Arquillo", "Carlos Perez-del-Pulgar", "Levin Gerdes", "Alfonso Garcia-Cerezo", "Miguel A. Olivares-Mendez"], "title": "OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery", "comment": null, "summary": "Robot navigation in unstructured environments requires multimodal perception\nsystems that can support safe navigation. Multimodality enables the integration\nof complementary information collected by different sensors. However, this\ninformation must be processed by machine learning algorithms specifically\ndesigned to leverage heterogeneous data. Furthermore, it is necessary to\nidentify which sensor modalities are most informative for navigation in the\ntarget environment. In Martian exploration, thermal imagery has proven valuable\nfor assessing terrain safety due to differences in thermal behaviour between\nsoil types. This work presents OmniUnet, a transformer-based neural network\narchitecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)\nimagery. A custom multimodal sensor housing was developed using 3D printing and\nmounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a\nmultimodal dataset in the Bardenas semi-desert in northern Spain. This location\nserves as a representative environment of the Martian surface, featuring\nterrain types such as sand, bedrock, and compact soil. A subset of this dataset\nwas manually labeled to support supervised training of the network. The model\nwas evaluated both quantitatively and qualitatively, achieving a pixel accuracy\nof 80.37% and demonstrating strong performance in segmenting complex\nunstructured terrain. Inference tests yielded an average prediction time of 673\nms on a resource-constrained computer (Jetson Orin Nano), confirming its\nsuitability for on-robot deployment. The software implementation of the network\nand the labeled dataset have been made publicly available to support future\nresearch in multimodal terrain perception for planetary robotics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOmniUnet\u7684AI\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u7406RGB\u3001\u6df1\u5ea6\u548c\u70ed\u6210\u50cf\u6570\u636e\uff0c\u4ee5\u5e2e\u52a9\u673a\u5668\u4eba\uff08\u5982\u706b\u661f\u63a2\u6d4b\u5668\uff09\u5728\u7c7b\u4f3c\u706b\u661f\u7684\u5730\u5f62\u4e2d\u66f4\u5b89\u5168\u5730\u5bfc\u822a\u3002\u8be5\u6a21\u578b\u5728\u6a21\u62df\u706b\u661f\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5206\u5272\u5730\u5f62\u7684\u51c6\u786e\u7387\u8fbe\u523080.37%\uff0c\u5e76\u4e14\u8fd0\u884c\u901f\u5ea6\u8db3\u591f\u5feb\uff0c\u53ef\u4ee5\u76f4\u63a5\u5728\u673a\u5668\u4eba\u4e0a\u4f7f\u7528\u3002\u7814\u7a76\u4eba\u5458\u8fd8\u516c\u5f00\u53d1\u5e03\u4e86\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u4ee5\u4f9b\u5176\u4ed6\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u3002", "motivation": "\u673a\u5668\u4eba\u9700\u8981\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b89\u5168\u5bfc\u822a\uff0c\u8fd9\u9700\u8981\u80fd\u591f\u6574\u5408\u6765\u81ea\u4e0d\u540c\u4f20\u611f\u5668\uff08\u5982RGB\u3001\u6df1\u5ea6\u548c\u70ed\u6210\u50cf\uff09\u7684\u4e92\u8865\u4fe1\u606f\u7684\u611f\u77e5\u7cfb\u7edf\u3002\u7136\u800c\uff0c\u5904\u7406\u8fd9\u4e9b\u5f02\u6784\u6570\u636e\u9700\u8981\u4e13\u95e8\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u8bc6\u522b\u54ea\u4e9b\u4f20\u611f\u5668\u6a21\u6001\u5bf9\u7279\u5b9a\u73af\u5883\u4e0b\u7684\u5bfc\u822a\u6700\u6709\u7528\u4e5f\u81f3\u5173\u91cd\u8981\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u706b\u661f\u63a2\u7d22\uff0c\u70ed\u6210\u50cf\u56e0\u5176\u8bc4\u4f30\u5730\u5f62\u5b89\u5168\u6027\u7684\u4ef7\u503c\u800c\u88ab\u8bc1\u660e\u662f\u6709\u7528\u7684\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOmniUnet\u7684Transformer\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u5904\u7406RGB\u3001\u6df1\u5ea6\u548c\u70ed\u6210\u50cf\uff08RGB-D-T\uff09\u6570\u636e\uff0c\u4ee5\u5b9e\u73b0\u8bed\u4e49\u5206\u5272\u3002\u7814\u7a76\u56e2\u961f\u4f7f\u75283D\u6253\u5370\u6280\u672f\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9a\u5236\u7684\u591a\u6a21\u6001\u4f20\u611f\u5668\u5916\u58f3\uff0c\u5e76\u5c06\u5176\u5b89\u88c5\u5728\u706b\u661f\u6f2b\u6e38\u8f66\u6d4b\u8bd5\u5e73\u53f0\uff08MaRTA\uff09\u4e0a\uff0c\u5728\u897f\u73ed\u7259\u5317\u90e8\u7684Bardenas\u534a\u5e72\u65f1\u533a\u6536\u96c6\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u4e86\u6c99\u5730\u3001\u57fa\u5ca9\u548c\u5bc6\u5b9e\u571f\u58e4\u7b49\u4ee3\u8868\u706b\u661f\u8868\u9762\u7684\u5730\u5f62\u7c7b\u578b\u3002\u7814\u7a76\u4eba\u5458\u624b\u52a8\u6807\u6ce8\u4e86\u90e8\u5206\u6570\u636e\u96c6\u7528\u4e8e\u76d1\u7763\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "OmniUnet\u5728Bardenas\u534a\u5e72\u65f1\u533a\u6536\u96c6\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5728\u5206\u5272\u975e\u7ed3\u6784\u5316\u5730\u5f62\u65b9\u9762\u53d6\u5f97\u4e8680.37%\u7684\u50cf\u7d20\u51c6\u786e\u7387\u3002\u8be5\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684Jetson Orin Nano\u4e0a\u8fdb\u884c\u4e86\u63a8\u7406\u6d4b\u8bd5\uff0c\u5e73\u5747\u9884\u6d4b\u65f6\u95f4\u4e3a673\u6beb\u79d2\uff0c\u8bc1\u660e\u4e86\u5176\u9002\u7528\u4e8e\u673a\u5668\u4eba\u90e8\u7f72\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cOmniUnet\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u5728\u590d\u6742\u5730\u5f62\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86OmniUnet\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u8bed\u4e49\u5206\u5272\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u80fd\u591f\u5904\u7406RGB\u3001\u6df1\u5ea6\u548c\u70ed\u6210\u50cf\uff08RGB-D-T\uff09\u6570\u636e\uff0c\u5e76\u5728\u975e\u7ed3\u6784\u5316\u5730\u5f62\u7684\u5206\u5272\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e8680.37%\u7684\u50cf\u7d20\u51c6\u786e\u7387\u3002\u8be5\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684Jetson Orin Nano\u4e0a\u5b9e\u73b0\u4e86\u5e73\u5747673\u6beb\u79d2\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u673a\u5668\u4eba\u90e8\u7f72\u4e0a\u7684\u53ef\u884c\u6027\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u516c\u5f00\u4e86\u7f51\u7edc\u8f6f\u4ef6\u5b9e\u73b0\u548c\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u884c\u661f\u673a\u5668\u4eba\u591a\u6a21\u6001\u5730\u5f62\u611f\u77e5\u7814\u7a76\u3002"}}
{"id": "2508.00218", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00218", "abs": "https://arxiv.org/abs/2508.00218", "authors": ["Aymane Abdali", "Bartosz Boguslawski", "Lucas Drumetz", "Vincent Gripon"], "title": "Object-Centric Cropping for Visual Few-Shot Classification", "comment": null, "summary": "In the domain of Few-Shot Image Classification, operating with as little as\none example per class, the presence of image ambiguities stemming from multiple\nobjects or complex backgrounds can significantly deteriorate performance. Our\nresearch demonstrates that incorporating additional information about the local\npositioning of an object within its image markedly enhances classification\nacross established benchmarks. More importantly, we show that a significant\nfraction of the improvement can be achieved through the use of the Segment\nAnything Model, requiring only a pixel of the object of interest to be pointed\nout, or by employing fully unsupervised foreground object extraction methods.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728Few-Shot\u56fe\u50cf\u5206\u7c7b\u4e2d\u52a0\u5165\u7269\u4f53\u4f4d\u7f6e\u4fe1\u606f\u53ef\u63d0\u5347\u6027\u80fd\uff0cSegment Anything Model\u6216\u65e0\u76d1\u7763\u63d0\u53d6\u65b9\u6cd5\u53ef\u5b9e\u73b0\u6b64\u6548\u679c\u3002", "motivation": "Few-Shot\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u5728\u5904\u7406\u5305\u542b\u591a\u4e2a\u7269\u4f53\u6216\u590d\u6742\u80cc\u666f\u7684\u56fe\u50cf\u65f6\uff0c\u7531\u4e8e\u56fe\u50cf\u6b67\u4e49\u6027\uff0c\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u9700\u8981\u5728Few-Shot\u56fe\u50cf\u5206\u7c7b\u4e2d\u5f15\u5165\u989d\u5916\u4fe1\u606f\u6765\u514b\u670d\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5728Few-Shot\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5f15\u5165\u7269\u4f53\u5c40\u90e8\u4f4d\u7f6e\u4fe1\u606f\u6765\u589e\u5f3a\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u4e86\u4f7f\u7528Segment Anything Model\u548c\u65e0\u76d1\u7763\u524d\u666f\u7269\u4f53\u63d0\u53d6\u65b9\u6cd5\u6765\u5b9e\u73b0\u6b64\u76ee\u6807\u3002", "result": "\u5728Few-Shot\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u52a0\u5165\u7269\u4f53\u5c40\u90e8\u4f4d\u7f6e\u4fe1\u606f\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u3002\u4f7f\u7528Segment Anything Model\u6216\u65e0\u76d1\u7763\u524d\u666f\u7269\u4f53\u63d0\u53d6\u65b9\u6cd5\u53ef\u4ee5\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5b9e\u73b0\u8fd9\u79cd\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u56fe\u50cf\u4e2d\u52a0\u5165\u7269\u4f53\u5c40\u90e8\u4f4d\u7f6e\u7684\u989d\u5916\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8Few-Shot\u56fe\u50cf\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f7f\u7528Segment Anything Model\u6216\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u524d\u666f\u7269\u4f53\u63d0\u53d6\u65b9\u6cd5\u6765\u5b9e\u73b0\u8fd9\u4e00\u6539\u8fdb\u3002"}}
{"id": "2508.00131", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00131", "abs": "https://arxiv.org/abs/2508.00131", "authors": ["Christopher Harvey", "Sumaiya Shomaji", "Zijun Yao", "Amit Noheria"], "title": "ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks", "comment": "arXiv admin note: substantial text overlap with arXiv:2410.02937", "summary": "The electrocardiogram (ECG) is an inexpensive and widely available tool for\ncardiac assessment. Despite its standardized format and small file size, the\nhigh complexity and inter-individual variability of ECG signals (typically a\n60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep\nlearning models, especially when only small training datasets are available.\nThis study addresses these challenges by exploring feature generation methods\nfrom representative beat ECGs, focusing on Principal Component Analysis (PCA)\nand Autoencoders to reduce data complexity. We introduce three novel\nVariational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed\nbeta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their\neffectiveness in maintaining signal fidelity and enhancing downstream\nprediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE\nachieved superior signal reconstruction, reducing the mean absolute error (MAE)\nto 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE\nencodings, when combined with traditional ECG summary features, improved the\nprediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an\nholdout test set area under the receiver operating characteristic curve (AUROC)\nof 0.901 with a LGBM classifier. This performance nearly matches the 0.909\nAUROC of state-of-the-art CNN model but requires significantly less\ncomputational resources. Further, the ECG feature extraction-LGBM pipeline\navoids overfitting and retains predictive performance when trained with less\ndata. Our findings demonstrate that these VAE encodings are not only effective\nin simplifying ECG data but also provide a practical solution for applying deep\nlearning in contexts with limited-scale labeled training data.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faVAE\u7f16\u7801\uff0c\u7528\u4e8e\u7b80\u5316ECG\u6570\u636e\uff0c\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6570\u636e\u6709\u9650\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u7387\u3002", "motivation": "ECG\u4fe1\u53f7\u7684\u590d\u6742\u6027\u548c\u4e2a\u4f53\u95f4\u53d8\u5f02\u6027\uff08\u901a\u5e38\u662f12\u5bfc\u8054\u3001500 Hz\u91c7\u6837\u7387\u4e0b\u768460,000\u7ef4\u5411\u91cf\uff09\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u5e94\u7528\u65f6\uff0c\u5c24\u5176\u662f\u5728\u53ea\u6709\u5c0f\u578b\u8bad\u7ec3\u6570\u636e\u96c6\u53ef\u7528\u65f6\uff0c\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4ece\u4ee3\u8868\u6027\u5fc3\u62cdECG\u751f\u6210\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u548c\u81ea\u52a8\u7f16\u7801\u5668\u4ee5\u964d\u4f4e\u6570\u636e\u590d\u6742\u5ea6\u3002\u7814\u7a76\u5f15\u5165\u4e86\u4e09\u79cd\u65b0\u9896\u7684\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\uff08VAE\uff09\u53d8\u4f53\u2014\u2014\u968f\u673a\u81ea\u52a8\u7f16\u7801\u5668\uff08SAE\uff09\u3001\u9000\u706bbeta-VAE\uff08A beta-VAE\uff09\u548c\u5faa\u73afbeta-VAE\uff08C beta-VAE\uff09\uff0c\u5e76\u6bd4\u8f83\u4e86\u5b83\u4eec\u5728\u4fdd\u6301\u4fe1\u53f7\u4fdd\u771f\u5ea6\u548c\u589e\u5f3a\u4e0b\u6e38\u9884\u6d4b\u4efb\u52a1\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4f7f\u7528\u4e86Light Gradient Boost Machine\uff08LGBM\uff09\u3002", "result": "A beta-VAE\u5728\u4fe1\u53f7\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u5c06\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u964d\u81f315.7+/-3.2 muV\uff0c\u8fbe\u5230\u4fe1\u53f7\u566a\u58f0\u7684\u6c34\u5e73\u3002\u6b64\u5916\uff0cSAE\u7f16\u7801\u4e0e\u4f20\u7edf\u7684ECG\u6458\u8981\u7279\u5f81\u76f8\u7ed3\u5408\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u964d\u4f4e\u7684\u5de6\u5fc3\u5ba4\u5c04\u8840\u5206\u6570\uff08LVEF\uff09\u7684\u80fd\u529b\uff0c\u4f7f\u7528LGBM\u5206\u7c7b\u5668\u5728\u72ec\u7acb\u7684\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e860.901\u7684\u63a5\u6536\u8005\u64cd\u4f5c\u7279\u5f81\u66f2\u7ebf\u4e0b\u9762\u79ef\uff08AUROC\uff09\uff0c\u8fd9\u63a5\u8fd1\u4e86\u6700\u5148\u8fdb\u7684CNN\u6a21\u578b\u76840.909 AUROC\uff0c\u4f46\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u663e\u8457\u51cf\u5c11\u3002\u6b64\u5916\uff0cECG\u7279\u5f81\u63d0\u53d6-LGBM\u6d41\u7a0b\u5728\u7528\u8f83\u5c11\u6570\u636e\u8bad\u7ec3\u65f6\uff0c\u907f\u514d\u4e86\u8fc7\u62df\u5408\u5e76\u4fdd\u6301\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684VAE\u7f16\u7801\u6709\u6548\u7b80\u5316\u4e86ECG\u6570\u636e\uff0c\u5e76\u4e3a\u5728\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5e94\u7528\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00332", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00332", "abs": "https://arxiv.org/abs/2508.00332", "authors": ["Kaiyan Zhao", "Zhongtao Miao", "Yoshimasa Tsuruoka"], "title": "Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment", "comment": "Work in progress", "summary": "Multimodal sentence embedding models typically leverage image-caption pairs\nin addition to textual data during training. However, such pairs often contain\nnoise, including redundant or irrelevant information on either the image or\ncaption side. To mitigate this issue, we propose MCSEO, a method that enhances\nmultimodal sentence embeddings by incorporating fine-grained object-phrase\nalignment alongside traditional image-caption alignment. Specifically, MCSEO\nutilizes existing segmentation and object detection models to extract accurate\nobject-phrase pairs, which are then used to optimize a contrastive learning\nobjective tailored to object-phrase correspondence. Experimental results on\nsemantic textual similarity (STS) tasks across different backbone models\ndemonstrate that MCSEO consistently outperforms strong baselines, highlighting\nthe significance of precise object-phrase alignment in multimodal\nrepresentation learning.", "AI": {"tldr": "MCSEO\u901a\u8fc7\u7cbe\u786e\u7684\u5bf9\u8c61-\u8bcd\u8bed\u5bf9\u9f50\u6765\u63d0\u9ad8\u591a\u6a21\u6001\u53e5\u5d4c\u5165\u7684\u8d28\u91cf\uff0c\u4ee5\u89e3\u51b3\u56fe\u50cf-\u6807\u9898\u6570\u636e\u4e2d\u7684\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u53e5\u5d4c\u5165\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u901a\u5e38\u4f1a\u4f7f\u7528\u5305\u542b\u566a\u58f0\uff08\u5197\u4f59\u6216\u65e0\u5173\u4fe1\u606f\uff09\u7684\u56fe\u50cf-\u6807\u9898\u5bf9\u3002MCSEO\u65e8\u5728\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "MCSEO\u5229\u7528\u73b0\u6709\u7684\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u63d0\u53d6\u7cbe\u786e\u7684\u5bf9\u8c61-\u8bcd\u8bed\u5bf9\uff0c\u5e76\u4f18\u5316\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u4ee5\u5b9e\u73b0\u5bf9\u8c61-\u8bcd\u8bed\u5bf9\u5e94\u3002", "result": "\u5728\u8de8\u4e0d\u540c\u9aa8\u5e72\u6a21\u578b\u7684\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\uff08STS\uff09\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMCSEO\u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "MCSEO\u901a\u8fc7\u5f15\u5165\u7ec6\u7c92\u5ea6\u7684 \u56fe\u50cf-\u5bf9\u8c61-\u8bcd\u8bed \u5bf9\u9f50\uff0c\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u53e5\u5d4c\u5165\u7684\u8868\u793a\u80fd\u529b\uff0c\u5e76\u5728\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00323", "abs": "https://arxiv.org/abs/2508.00323", "authors": ["Jianyi Zhang", "Xu Ji", "Ziyin Zhou", "Yuchen Zhou", "Shubo Shi", "Haoyu Wu", "Zhen Li", "Shizhao Liu"], "title": "Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning", "comment": null, "summary": "Evaluating the performance of visual language models (VLMs) in graphic\nreasoning tasks has become an important research topic. However, VLMs still\nshow obvious deficiencies in simulating human-level graphic reasoning\ncapabilities, especially in complex graphic reasoning and abstract problem\nsolving, which are less studied and existing studies only focus on simple\ngraphics. To evaluate the performance of VLMs in complex graphic reasoning, we\npropose ReasonBench, the first evaluation benchmark focused on structured\ngraphic reasoning tasks, which includes 1,613 questions from real-world\nintelligence tests. ReasonBench covers reasoning dimensions related to\nlocation, attribute, quantity, and multi-element tasks, providing a\ncomprehensive evaluation of the performance of VLMs in spatial, relational, and\nabstract reasoning capabilities. We benchmark 11 mainstream VLMs (including\nclosed-source and open-source models) and reveal significant limitations of\ncurrent models. Based on these findings, we propose a dual optimization\nstrategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability\nof reasoning by decomposing layers, and ReasonTune enhances the task\nadaptability of model reasoning through training, all of which improves VLM\nperformance by 33.5\\%. All experimental data and code are in the repository:\nhttps://huggingface.co/datasets/cistine/ReasonBench.", "AI": {"tldr": "ReasonBench\uff1a\u9996\u4e2a\u4e13\u6ce8\u4e8e\u7ed3\u6784\u5316\u56fe\u5f62\u63a8\u7406\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,613\u4e2a\u771f\u5b9e\u667a\u529b\u6d4b\u8bd5\u95ee\u9898\uff0c\u65e8\u5728\u8bc4\u4f30\u548c\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002\u63d0\u51fa\u7684DiaCoT\u548cReasonTune\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9VLM\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u548c\u62bd\u8c61\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u7684\u80fd\u529b\u7814\u7a76\u4e0d\u8db3\uff0c\u4e14\u4ec5\u5173\u6ce8\u7b80\u5355\u56fe\u5f62\uff0c\u9700\u8981\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u548c\u6539\u8fdbVLM\u5728\u8be5\u9886\u57df\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faReasonBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,613\u4e2a\u6765\u81ea\u771f\u5b9e\u667a\u529b\u6d4b\u8bd5\u7684\u95ee\u9898\uff0c\u6db5\u76d6\u4f4d\u7f6e\u3001\u5c5e\u6027\u3001\u6570\u91cf\u548c\u591a\u5143\u7d20\u4efb\u52a1\uff1b\u63d0\u51faDiaCoT\uff08Diagrammatic Reasoning Chain\uff09\u548cReasonTune\u4e24\u79cd\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728ReasonBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5bf911\u79cd\u4e3b\u6d41VLM\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002DiaCoT\u548cReasonTune\u7b56\u7565\u5c06VLM\u6027\u80fd\u63d0\u9ad8\u4e8633.5%\u3002", "conclusion": "\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u4f46\u63d0\u51fa\u7684DiaCoT\u548cReasonTune\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347VLM\u6027\u80fd33.5%\u3002"}}
{"id": "2508.00072", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00072", "abs": "https://arxiv.org/abs/2508.00072", "authors": ["Marcus J Clark", "Obada Alia", "Sima Bahrani", "Gregory T Jasion", "Hesham Sakr", "Periklis Petropoulos", "Francesco Poletti", "George T Kanellos", "John Rarity", "Reza Nejabati", "Siddarth K Joshi", "Rui Wang", "Dimitra Simeonidou"], "title": "Coexistence of Entanglement-based Quantum Channels with DWDM Classical Channels over Hollow Core Fibre in a Four Node Quantum Communication Network", "comment": "14 pages, 6 figures, 4 tables", "summary": "We experimentally demonstrate the coexistence of three entanglement-based\nquantum channels with carrier-grade classical optical channels over $11.5$km\nhollow core nested antiresonant nodeless fibre, in a four user quantum network.\nA transmission of $800$Gbps is achieved with four classical channels\nsimultaneously with three quantum channels all operating in the C-band with a\nseparation of $1.2$nm, with aggregated coexistence power of $-3$dBm. We\nestablished quantum key distribution in the four-node full-mesh quantum network\nwith Bell state fidelity of up to $90.0\\pm0.8$%. The secret key rate for all\nthe links in the network are passively preserved over $55$hours of experimental\ntime.", "AI": {"tldr": "Quantum and classical communication successfully coexisted in a 4-user network over 11.5km fibre, achieving high-speed classical transmission and secure quantum key distribution with stable performance.", "motivation": "To demonstrate the feasibility of integrating quantum communication channels with existing classical optical infrastructure within a multi-user network setting, overcoming challenges related to channel coexistence and performance.", "method": "Experimental demonstration of coexistence of quantum and classical channels in a hollow core fibre quantum network, employing Bell state measurements for quantum key distribution and monitoring secret key rates over time.", "result": "Achieved coexistence of three quantum channels and four classical channels (800Gbps) in the C-band over 11.5km fibre. Quantum key distribution achieved Bell state fidelity up to 90.0\u00b10.8%, with secret key rates preserved over 55 hours.", "conclusion": "The paper successfully demonstrates the coexistence of three entanglement-based quantum channels with carrier-grade classical optical channels in a four-user quantum network over 11.5km of hollow core fibre. It achieved 800Gbps for classical channels and high fidelity quantum key distribution (up to 90.0\u00b10.8%) with preserved secret key rates over 55 hours."}}
{"id": "2508.00584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00584", "abs": "https://arxiv.org/abs/2508.00584", "authors": ["Konstantinos Plotas", "Emmanouil Papadakis", "Drosakis Drosakis", "Panos Trahanias", "Dimitrios Papageorgiou"], "title": "A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup", "comment": "Please find the citation info @ Zenodo, ArXiv or Zenodo, as the\n  proceedings of ICRA are no longer sent to IEEE Xplore", "summary": "In this work, a control scheme for human-robot collaborative object\ntransportation is proposed, considering a quadruped robot equipped with the\nMIGHTY suction cup that serves both as a gripper for holding the object and a\nforce/torque sensor. The proposed control scheme is based on the notion of\nadmittance control, and incorporates a variable damping term aiming towards\nincreasing the controllability of the human and, at the same time, decreasing\nher/his effort. Furthermore, to ensure that the object is not detached from the\nsuction cup during the collaboration, an additional control signal is proposed,\nwhich is based on a barrier artificial potential. The proposed control scheme\nis proven to be passive and its performance is demonstrated through\nexperimental evaluations conducted using the Unitree Go1 robot equipped with\nthe MIGHTY suction cup.", "AI": {"tldr": "A new control scheme for robots working with humans to move objects uses a suction cup and special controls to make it easier and safer for the human, as shown in robot tests.", "motivation": "To develop a control scheme for human-robot collaborative object transportation that enhances human controllability and reduces human effort, while ensuring the object remains securely held by the suction cup.", "method": "Admittance control with a variable damping term and a barrier artificial potential for controlling a quadruped robot with a suction cup during human-robot collaborative object transportation.", "result": "Experimental evaluations demonstrated the performance and passivity of the proposed control scheme.", "conclusion": "The proposed control scheme, based on admittance control with variable damping and a barrier artificial potential, is proven to be passive and effective in experimental evaluations using the Unitree Go1 robot with a MIGHTY suction cup for human-robot collaborative object transportation."}}
{"id": "2508.00248", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00248", "abs": "https://arxiv.org/abs/2508.00248", "authors": ["Chenggang Guo", "Hao Xu", "XianMing Wan"], "title": "Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network", "comment": null, "summary": "Depth map super-resolution technology aims to improve the spatial resolution\nof low-resolution depth maps and effectively restore high-frequency detail\ninformation. Traditional convolutional neural network has limitations in\ndealing with long-range dependencies and are unable to fully model the global\ncontextual information in depth maps. Although transformer can model global\ndependencies, its computational complexity and memory consumption are\nquadratic, which significantly limits its ability to process high-resolution\ndepth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba\n(MSF-UM) model, a novel guided depth map super-resolution framework. The core\ninnovation of this model is to integrate Mamba's efficient state-space modeling\ncapabilities into a multi-scale U-shaped fusion structure guided by a color\nimage. The structure combining the residual dense channel attention block and\nthe Mamba state space module is designed, which combines the local feature\nextraction capability of the convolutional layer with the modeling advantage of\nthe state space model for long-distance dependencies. At the same time, the\nmodel adopts a multi-scale cross-modal fusion strategy to make full use of the\nhigh-frequency texture information from the color image to guide the\nsuper-resolution process of the depth map. Compared with existing mainstream\nmethods, the proposed MSF-UM significantly reduces the number of model\nparameters while achieving better reconstruction accuracy. Extensive\nexperiments on multiple publicly available datasets validate the effectiveness\nof the model, especially showing excellent generalization ability in the task\nof large-scale depth map super-resolution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMSF-UM\u7684\u591a\u5c3a\u5ea6\u878d\u5408U\u578bMamba\u6a21\u578b\uff0c\u7528\u4e8e\u6df1\u5ea6\u56fe\u8d85\u5206\u8fa8\u7387\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86Mamba\u7684\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u548cCNN\u7684\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f69\u8272\u56fe\u50cf\u8fdb\u884c\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u91cf\u51cf\u5c11\u548c\u7cbe\u5ea6\u63d0\u5347\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "motivation": "\u4f20\u7edf\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002Transformer\u867d\u7136\u80fd\u6a21\u62df\u5168\u5c40\u4f9d\u8d56\uff0c\u4f46\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u6d88\u8017\u662f\u4e8c\u6b21\u65b9\u7684\uff0c\u9650\u5236\u4e86\u5176\u5728\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u56fe\u5904\u7406\u4e0a\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\u5e76\u4fdd\u6301\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u540c\u65f6\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6df1\u5ea6\u56fe\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f15\u5bfc\u5f0f\u6df1\u5ea6\u56fe\u8d85\u5206\u8fa8\u7387\u6846\u67b6\u2014\u2014\u591a\u5c3a\u5ea6\u878d\u5408U\u578bMamba\uff08MSF-UM\uff09\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5c06Mamba\u7684\u9ad8\u6548\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u80fd\u529b\u96c6\u6210\u5230\u53d7\u5f69\u8272\u56fe\u50cf\u5f15\u5bfc\u7684\u591a\u5c3a\u5ea6U\u578b\u878d\u5408\u7ed3\u6784\u4e2d\u3002\u8be5\u7ed3\u6784\u7ed3\u5408\u4e86\u6b8b\u5dee\u5bc6\u96c6\u901a\u9053\u6ce8\u610f\u529b\u5757\u548cMamba\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\uff0c\u878d\u5408\u4e86\u5377\u79ef\u5c42\u7684\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\u7684\u4f18\u52bf\u3002\u540c\u65f6\uff0c\u6a21\u578b\u91c7\u7528\u591a\u5c3a\u5ea6\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u5229\u7528\u5f69\u8272\u56fe\u50cf\u7684\u9ad8\u9891\u7eb9\u7406\u4fe1\u606f\u6307\u5bfc\u6df1\u5ea6\u56fe\u7684\u8d85\u5206\u8fa8\u7387\u8fc7\u7a0b\u3002", "result": "\u4e0e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u76f8\u6bd4\uff0cMSF-UM\u6a21\u578b\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u91cd\u5efa\u7cbe\u5ea6\u3002\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6df1\u5ea6\u56fe\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684MSF-UM\u6a21\u578b\u5728\u53c2\u6570\u91cf\u663e\u8457\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u66f4\u597d\u7684\u91cd\u5efa\u7cbe\u5ea6\uff0c\u5e76\u4e14\u5728\u591a\u9879\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6df1\u5ea6\u56fe\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.00789", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.00789", "abs": "https://arxiv.org/abs/2508.00789", "authors": ["Karma Tenzin", "Berkay Kilic", "Raghottam Sattigeri", "Zhiren He", "Chao Chen Ye", "Marcio Costa", "Marco Buongiorno Nardelli", "Carmine Autieri", "Jagoda Slawinska"], "title": "Persistent spin textures, altermagnetism and charge-to-spin conversion in metallic chiral crystals TM$_{3}$X$_{6}$", "comment": null, "summary": "Chiral crystals, due to the lack of inversion and mirror symmetries, exhibit\nunique spin responses to external fields, enabling physical effects rarely\nobserved in high-symmetry systems. Here, we show that materials from the chiral\ndichalcogenide family TM$_3$X$_6$ (T = 3d, M = 4d/5d, X = S) exhibit persistent\nspin texture (PST) - unidirectional spin polarization of states across large\nregions of the reciprocal space - in their nonmagnetic metallic phase. Using\nthe example of NiTa$_{3}$S$_{6}$ and NiNb$_{3}$S$_{6}$, we show that PSTs cover\nthe full Fermi surface, a rare and desirable feature that enables efficient\ncharge-to-spin conversion and suggests long spin lifetimes and coherent spin\ntransport above magnetic ordering temperatures. At low temperatures, the\nmaterials that order antiferromagnetically become chiral altermagnets, where\nspin textures originating from spin-orbit coupling and altermagnetism combine\nin a way that sensitively depends on the orientation of the Neel vector. Using\nsymmetry analysis and first-principles calculations, we classify magnetic\nground states across the family, identify cases with weak ferromagnetism, and\ntrack the evolution of spin textures and charge-to-spin conversion across\nmagnetic phases and different Neel vector orientations, revealing spin\ntransport signatures that allow one to distinguish Neel vector directions.\nThese findings establish TM$_3$X$_6$ as a tunable platform for efficient\ncharge-to-spin conversion and spin transport, combining structural chirality,\npersistent spin textures, and altermagnetism.", "AI": {"tldr": "\u624b\u6027TM3X6\u6750\u6599\u5728\u975e\u78c1\u6027\u76f8\u4e2d\u5177\u6709\u6301\u4e45\u81ea\u65cb\u7eb9\u7406\uff08PST\uff09\uff0c\u5728\u4f4e\u6e29\u4e0b\u8868\u73b0\u4e3a\u624b\u6027\u4ea4\u66ff\u78c1\u4f53\u3002\u8fd9\u4e9b\u7279\u6027\u4f7f\u5176\u6210\u4e3a\u9ad8\u6548\u7535\u8377-\u81ea\u65cb\u8f6c\u6362\u548c\u81ea\u65cb\u8f93\u8fd0\u7684\u7406\u60f3\u5e73\u53f0\u3002", "motivation": "\u624b\u6027\u6676\u4f53\u7531\u4e8e\u7f3a\u4e4f\u53cd\u6f14\u548c\u955c\u50cf\u5bf9\u79f0\u6027\uff0c\u5bf9\u5916\u90e8\u573a\u7684\u54cd\u5e94\u72ec\u7279\uff0c\u80fd\u591f\u4ea7\u751f\u9ad8\u5bf9\u79f0\u6027\u4f53\u7cfb\u4e2d\u7f55\u89c1\u7684\u7269\u7406\u6548\u5e94\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22TM3X6\u5bb6\u65cf\u6750\u6599\u4e2d\u7684\u6301\u4e45\u81ea\u65cb\u7eb9\u7406\uff08PST\uff09\u73b0\u8c61\u53ca\u5176\u5728\u7535\u8377-\u81ea\u65cb\u8f6c\u6362\u548c\u81ea\u65cb\u8f93\u8fd0\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5229\u7528\u5bf9\u79f0\u6027\u5206\u6790\u548c\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u5bf9TM3X6\u5bb6\u65cf\u7684\u78c1\u6027\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u8ffd\u8e2a\u4e86\u81ea\u65cb\u7eb9\u7406\u548c\u7535\u8377-\u81ea\u65cb\u8f6c\u6362\u968f\u78c1\u76f8\u548c\u5c3c\u5c14\u77e2\u91cf\u65b9\u5411\u7684\u53d8\u5316\u3002", "result": "NiTa3S6\u548cNiNb3S6\u6750\u6599\u5728\u975e\u78c1\u6027\u91d1\u5c5e\u76f8\u4e2d\u5c55\u73b0\u51fa\u8986\u76d6\u6574\u4e2a\u8d39\u7c73\u9762\u7684PST\u3002\u5728\u4f4e\u6e29\u4e0b\uff0c\u8fd9\u4e9b\u6750\u6599\u8868\u73b0\u51fa\u4ea4\u66ff\u78c1\u6027\uff0c\u5176\u81ea\u65cb\u7eb9\u7406\u4e0e\u5c3c\u5c14\u77e2\u91cf\u65b9\u5411\u5bc6\u5207\u76f8\u5173\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u533a\u5206\u4e0d\u540c\u5c3c\u5c14\u77e2\u91cf\u65b9\u5411\u7684\u81ea\u65cb\u8f93\u8fd0\u7279\u5f81\u3002", "conclusion": "TM3X6 (T = 3d, M = 4d/5d, X = S) \u6750\u6599\u5728\u975e\u78c1\u6027\u91d1\u5c5e\u76f8\u4e2d\u8868\u73b0\u51fa\u6301\u4e45\u81ea\u65cb\u7eb9\u7406 (PST)\uff0c\u8fd9\u79cd\u7eb9\u7406\u8986\u76d6\u6574\u4e2a\u8d39\u7c73\u9762\uff0c\u6709\u5229\u4e8e\u9ad8\u6548\u7684\u7535\u8377-\u81ea\u65cb\u8f6c\u6362\u548c\u957f\u81ea\u65cb\u5bff\u547d\u3002\u5728\u4f4e\u6e29\u4e0b\uff0c\u8fd9\u4e9b\u6750\u6599\u53d8\u4e3a\u624b\u6027\u4ea4\u66ff\u78c1\u4f53\uff0c\u5176\u81ea\u65cb\u7eb9\u7406\u5bf9\u5c3c\u5c14\u77e2\u91cf\u7684\u65b9\u5411\u654f\u611f\u3002\u7814\u7a76\u5bf9 TM3X6 \u6750\u6599\u5bb6\u65cf\u7684\u78c1\u6027\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u8ffd\u8e2a\u4e86\u81ea\u65cb\u7eb9\u7406\u548c\u7535\u8377-\u81ea\u65cb\u8f6c\u6362\u7684\u6f14\u53d8\uff0c\u8fd9\u4e9b\u53d1\u73b0\u5c06 TM3X6 \u786e\u7acb\u4e3a\u4e00\u4e2a\u7ed3\u5408\u4e86\u7ed3\u6784\u624b\u6027\u3001PST \u548c\u4ea4\u66ff\u78c1\u6027\u7684\u5e73\u53f0\uff0c\u53ef\u7528\u4e8e\u9ad8\u6548\u7684\u7535\u8377-\u81ea\u65cb\u8f6c\u6362\u548c\u81ea\u65cb\u8f93\u8fd0\u3002"}}
{"id": "2508.00141", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00141", "abs": "https://arxiv.org/abs/2508.00141", "authors": ["Mohit Gupta", "Debjit Bhowmick", "Rhys Newbury", "Meead Saberi", "Shirui Pan", "Ben Beck"], "title": "INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks", "comment": null, "summary": "Accurate link-level bicycling volume estimation is essential for sustainable\nurban transportation planning. However, many cities face significant challenges\nof high data sparsity due to limited bicycling count sensor coverage. To\naddress this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning\n(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize\nsensor placement and improve link-level bicycling volume estimation in\ndata-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks\n(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL\nagent, enabling a data-driven strategic selection of sensor locations to\nmaximize estimation performance. Applied to Melbourne's bicycling network,\ncomprising 15,933 road segments with sensor coverage on only 141 road segments\n(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume\nestimation by strategically selecting additional sensor locations in\ndeployments of 50, 100, 200 and 500 sensors. Our framework outperforms\ntraditional heuristic methods for sensor placement such as betweenness\ncentrality, closeness centrality, observed bicycling activity and random\nplacement, across key metrics such as Mean Squared Error (MSE), Root Mean\nSquared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our\nexperiments benchmark INSPIRE-GNN against standard machine learning and deep\nlearning models in the bicycle volume estimation performance, underscoring its\neffectiveness. Our proposed framework provides transport planners actionable\ninsights to effectively expand sensor networks, optimize sensor placement and\nmaximize volume estimation accuracy and reliability of bicycling data for\ninformed transportation planning decisions.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.00344", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00344", "abs": "https://arxiv.org/abs/2508.00344", "authors": ["Keer Lu", "Chong Chen", "Bin Cui", "Huang Leng", "Wentao Zhang"], "title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable advancements in tackling\nagent-oriented tasks. Despite their potential, existing work faces challenges\nwhen deploying LLMs in agent-based environments. The widely adopted agent\nparadigm ReAct centers on integrating single-step reasoning with immediate\naction execution, which limits its effectiveness in complex tasks requiring\nlong-term strategic planning. Furthermore, the coordination between the planner\nand executor during problem-solving is also a critical factor to consider in\nagent design. Additionally, current approaches predominantly rely on supervised\nfine-tuning, which often leads models to memorize established task completion\ntrajectories, thereby restricting their generalization ability when confronted\nwith novel problem contexts. To address these challenges, we introduce an\nadaptive global plan-based agent paradigm AdaPlan, aiming to synergize\nhigh-level explicit guidance with execution to support effective long-horizon\ndecision-making. Based on the proposed paradigm, we further put forward\nPilotRL, a global planning-guided training framework for LLM agents driven by\nprogressive reinforcement learning. We first develop the model's ability to\nfollow explicit guidance from global plans when addressing agent tasks.\nSubsequently, based on this foundation, we focus on optimizing the quality of\ngenerated plans. Finally, we conduct joint optimization of the model's planning\nand execution coordination. Experiments indicate that PilotRL could achieve\nstate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing\nclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%\ncomparing to GPT-4o-mini at a comparable parameter scale.", "AI": {"tldr": "AdaPlan \u548c PilotRL \u6846\u67b6\u901a\u8fc7\u5168\u5c40\u89c4\u5212\u548c\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u4e86 LLM \u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u73b0\u6709 LLM \u4ee3\u7406\u8303\u5f0f\uff08\u5982 ReAct\uff09\u5728\u5904\u7406\u9700\u8981\u957f\u671f\u6218\u7565\u89c4\u5212\u7684\u590d\u6742\u4efb\u52a1\u65f6\u6548\u679c\u6709\u9650\uff0c\u4e14\u5728\u89c4\u5212\u8005\u548c\u6267\u884c\u8005\u4e4b\u95f4\u7684\u534f\u8c03\u4ee5\u53ca\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u7b49\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a AdaPlan \u7684\u81ea\u9002\u5e94\u5168\u5c40\u89c4\u5212\u4ee3\u7406\u8303\u5f0f\uff0c\u4ee5\u534f\u540c\u9ad8\u7ea7\u663e\u5f0f\u6307\u5bfc\u548c\u6267\u884c\u6765\u652f\u6301\u6709\u6548\u7684\u957f\u65f6\u51b3\u7b56\u3002\u57fa\u4e8e\u6b64\u8303\u5f0f\uff0c\u63d0\u51fa\u4e86 PilotRL\uff0c\u4e00\u4e2a\u7531\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u5168\u5c40\u89c4\u5212\u6307\u5bfc\u7684 LLM \u4ee3\u7406\u8bad\u7ec3\u6846\u67b6\uff0c\u5206\u9636\u6bb5\u4f18\u5316\u6a21\u578b\u7684\u89c4\u5212\u9075\u5faa\u80fd\u529b\u3001\u89c4\u5212\u751f\u6210\u8d28\u91cf\u4ee5\u53ca\u89c4\u5212\u4e0e\u6267\u884c\u7684\u534f\u540c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPilotRL \u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cLLaMA3.1-8B-Instruct + PilotRL \u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86 GPT-4o 3.60%\uff0c\u5e76\u76f8\u6bd4\u540c\u7b49\u53c2\u6570\u89c4\u6a21\u7684 GPT-4o-mini \u6709\u4e86 55.78% \u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "AdaPlan \u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5229\u7528\u5168\u5c40\u89c4\u5212\u6765\u6307\u5bfc LLM \u4ee3\u7406\u8fdb\u884c\u957f\u671f\u51b3\u7b56\uff0cPilotRL \u901a\u8fc7\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e86\u89c4\u5212\u548c\u6267\u884c\u7684\u534f\u540c\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86 GPT-4o \u7b49\u6a21\u578b\u3002"}}
{"id": "2508.00324", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00324", "abs": "https://arxiv.org/abs/2508.00324", "authors": ["Yeonjun In", "Wonjoong Kim", "Sangwu Park", "Chanyoung Park"], "title": "R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge", "comment": "under review", "summary": "Although large reasoning models (LRMs) have demonstrated impressive\ncapabilities on complex tasks, recent studies reveal that these models\nfrequently fulfill harmful user instructions, raising significant safety\nconcerns. In this paper, we investigate the underlying cause of LRM safety\nrisks and find that models already possess sufficient safety knowledge but fail\nto activate it during reasoning. Based on this insight, we propose R1-Act, a\nsimple and efficient post-training method that explicitly triggers safety\nknowledge through a structured reasoning process. R1-Act achieves strong safety\nimprovements while preserving reasoning performance, outperforming prior\nalignment methods. Notably, it requires only 1,000 training examples and 90\nminutes of training on a single RTX A6000 GPU. Extensive experiments across\nmultiple LRM backbones and sizes demonstrate the robustness, scalability, and\npractical efficiency of our approach.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a R1-Act \u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b (LRM) \u7684\u5b89\u5168\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0 LRM \u5177\u5907\u5b89\u5168\u77e5\u8bc6\u4f46\u65e0\u6cd5\u6709\u6548\u6fc0\u6d3b\u3002R1-Act \u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u6765\u89e6\u53d1\u8fd9\u4e9b\u5b89\u5168\u77e5\u8bc6\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5176\u63a8\u7406\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u8bad\u7ec3\u9ad8\u6548\uff0c\u4ec5\u9700\u5c11\u91cf\u6570\u636e\u548c\u65f6\u95f4\u5373\u53ef\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "motivation": "LRM \u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u4f1a\u54cd\u5e94\u6709\u5bb3\u7684\u7528\u6237\u6307\u4ee4\uff0c\u5f15\u53d1\u4e86\u91cd\u5927\u7684\u5b89\u5168\u62c5\u5fe7\u3002\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5 LRM \u5b89\u5168\u98ce\u9669\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "R1-Act \u662f\u4e00\u79cd\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u663e\u5f0f\u89e6\u53d1\u5b89\u5168\u77e5\u8bc6\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "R1-Act \u5728\u591a\u4e2a LRM \u4e3b\u5e72\u548c\u89c4\u6a21\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u6548\u7387\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4ec5\u9700 1000 \u4e2a\u8bad\u7ec3\u6837\u672c\u548c 90 \u5206\u949f\u7684\u8bad\u7ec3\u65f6\u95f4\uff08\u5728\u5355\u4e2a RTX A6000 GPU \u4e0a\uff09\uff0c\u5373\u53ef\u5b9e\u73b0\u5f3a\u5927\u7684\u5b89\u5168\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u7559\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u4e14\u4f18\u4e8e\u4ee5\u5f80\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "conclusion": "R1-Act \u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u663e\u5f0f\u89e6\u53d1\u5b89\u5168\u77e5\u8bc6\uff0c\u5728\u63d0\u5347\u5b89\u5168\u6027\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u4f18\u4e8e\u4ee5\u5f80\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002"}}
{"id": "2508.00126", "categories": ["quant-ph", "cond-mat.stat-mech", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2508.00126", "abs": "https://arxiv.org/abs/2508.00126", "authors": ["Pablo P\u00e1ez-Velasco", "Niclas Schilling", "Samuel O. Scalet", "Frank Verstraete", "\u00c1ngela Capel"], "title": "Efficient and simple Gibbs state preparation of the 2D toric code via duality to classical Ising chains", "comment": "10 + 27 pages, 34 figures", "summary": "We introduce the notion of polynomial-depth duality transformations, which\nrelates two sets of operator algebras through a conjugation by a poly-depth\nquantum circuit, and make use of this to construct efficient Gibbs samplers for\na variety of interesting quantum Hamiltonians as they are poly-depth dual to\nclassical Hamiltonians. This is for example the case for the 2D toric code,\nwhich is demonstrated to be poly-depth dual to two decoupled classical Ising\nspin chains for any system size, and we give evidence that such dualities hold\nfor a wide class of stabilizer Hamiltonians. Additionally, we extend the above\nnotion of duality to Lindbladians in order to show that mixing times and other\nquantities such as the spectral gap or the modified logarithmic Sobolev\ninequality are preserved under duality.", "AI": {"tldr": "\u901a\u8fc7\u591a\u6df1\u5ea6\u5bf9\u5076\u53d8\u6362\u6784\u5efa\u91cf\u5b50\u54c8\u5bc6\u987f\u91cf\u7684\u5409\u5e03\u65af\u91c7\u6837\u5668\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5bf92D\u8868\u9762\u7801\u548c\u4f0a\u8f9b\u81ea\u65cb\u94fe\u7684\u9002\u7528\u6027\u3002", "motivation": "\u4e3a\u4e86\u6784\u5efa\u5404\u79cd\u91cf\u5b50\u54c8\u5bc6\u987f\u91cf\u7684\u6709\u6548\u5409\u5e03\u65af\u91c7\u6837\u5668\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u591a\u6df1\u5ea6\u4e0a\u4e0e\u7ecf\u5178\u54c8\u5bc6\u987f\u91cf\u5bf9\u5076\u3002", "method": "\u63d0\u51fa\u591a\u6df1\u5ea6\u5bf9\u5076\u53d8\u6362\u7684\u6982\u5ff5\uff0c\u8be5\u53d8\u6362\u901a\u8fc7\u591a\u6df1\u5ea6\u91cf\u5b50\u7535\u8def\u7684\u5171\u8f6d\u5173\u7cfb\u5c06\u4e24\u7ec4\u7b97\u5b50\u4ee3\u6570\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u8bc1\u660e\u4e862D\u8868\u9762\u7801\u5728\u591a\u6df1\u5ea6\u4e0a\u4e0e\u4e24\u4e2a\u5206\u79bb\u7684\u7ecf\u5178\u4f0a\u8f9b\u81ea\u65cb\u94fe\u5bf9\u5076\uff0c\u65e0\u8bba\u7cfb\u7edf\u5927\u5c0f\u5982\u4f55\u3002", "conclusion": "\u591a\u6df1\u5ea6\u5bf9\u5076\u53d8\u6362\u88ab\u8bc1\u660e\u53ef\u4ee5\u4fdd\u6301\u6df7\u5408\u65f6\u95f4\u3001\u8c31\u9699\u548c\u4fee\u6b63\u540e\u7684\u5bf9\u6570-Sobolev\u4e0d\u7b49\u5f0f\u7b49\u91cf"}}
{"id": "2508.00625", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00625", "abs": "https://arxiv.org/abs/2508.00625", "authors": ["Bartosz Krawczyk", "Ahmed Elbary", "Robbie Cato", "Jagdish Patil", "Kaung Myat", "Anyeh Ndi-Tah", "Nivetha Sakthivel", "Mark Crampton", "Gautham Das", "Charles Fox"], "title": "OpenScout v1.1 mobile robot: a case study on open hardware continuation", "comment": "6 pages, 4 figures, a TAROS2025 short paper", "summary": "OpenScout is an Open Source Hardware (OSH) mobile robot for research and\nindustry. It is extended to v1.1 which includes simplified, cheaper and more\npowerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo\nsimulation. Changes, their rationale, project methodology, and results are\nreported as an OSH case study.", "AI": {"tldr": "OpenScout v1.1\u662f\u4e00\u4e2a\u6539\u8fdb\u7684\u5f00\u6e90\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u8ba1\u7b97\u80fd\u529b\u3001ROS2\u63a5\u53e3\u548cGazebo\u4eff\u771f\uff0c\u4e3a\u7814\u7a76\u548c\u5de5\u4e1a\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5e73\u53f0\u3002", "motivation": "\u53d1\u5e03OpenScout v1.1\uff0c\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u66f4\u5f3a\u5927\u3001\u66f4\u4fbf\u5b9c\u3001\u66f4\u6613\u4e8e\u4f7f\u7528\u7684\u5f00\u6e90\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\u3002", "method": "\u5f00\u6e90\u786c\u4ef6\u6848\u4f8b\u7814\u7a76", "result": "OpenScout v1.1\u7684\u53d1\u5e03\uff0c\u5305\u62ec\u66f4\u5f3a\u5927\u7684\u8ba1\u7b97\u786c\u4ef6\u3001ROS2\u63a5\u53e3\u548cGazebo\u4eff\u771f\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7814\u7a76\u548c\u5de5\u4e1a\u9886\u57df\u7684\u6f5c\u529b\u3002", "conclusion": "OpenScout v1.1\u7684\u53d1\u5e03\uff0c\u5305\u62ec\u66f4\u5f3a\u5927\u7684\u8ba1\u7b97\u786c\u4ef6\u3001ROS2\u63a5\u53e3\u548cGazebo\u4eff\u771f\uff0c\u662f\u4e00\u4e2a\u6210\u529f\u7684\u5f00\u6e90\u786c\u4ef6\u6848\u4f8b\u7814\u7a76\u3002"}}
{"id": "2508.00259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00259", "abs": "https://arxiv.org/abs/2508.00259", "authors": ["Wentao Sun", "Hanqing Xu", "Quanyun Wu", "Dedong Zhang", "Yiping Chen", "Lingfei Ma", "John S. Zelek", "Jonathan Li"], "title": "PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting", "comment": "22 pages, 9 figures", "summary": "We introduce PointGauss, a novel point cloud-guided framework for real-time\nmulti-object segmentation in Gaussian Splatting representations. Unlike\nexisting methods that suffer from prolonged initialization and limited\nmulti-view consistency, our approach achieves efficient 3D segmentation by\ndirectly parsing Gaussian primitives through a point cloud segmentation-driven\npipeline. The key innovation lies in two aspects: (1) a point cloud-based\nGaussian primitive decoder that generates 3D instance masks within 1 minute,\nand (2) a GPU-accelerated 2D mask rendering system that ensures multi-view\nconsistency. Extensive experiments demonstrate significant improvements over\nprevious state-of-the-art methods, achieving performance gains of 1.89 to\n31.78% in multi-view mIoU, while maintaining superior computational efficiency.\nTo address the limitations of current benchmarks (single-object focus,\ninconsistent 3D evaluation, small scale, and partial coverage), we present\nDesktopObjects-360, a novel comprehensive dataset for 3D segmentation in\nradiance fields, featuring: (1) complex multi-object scenes, (2) globally\nconsistent 2D annotations, (3) large-scale training data (over 27 thousand 2D\nmasks), (4) full 360{\\deg} coverage, and (5) 3D evaluation masks.", "AI": {"tldr": "PointGauss \u662f\u4e00\u79cd\u65b0\u7684\u70b9\u4e91\u5f15\u5bfc\u6846\u67b6\uff0c\u53ef\u5b9e\u73b0\u9ad8\u65af\u56fe\u5143\u8868\u793a\u4e2d\u7684\u5b9e\u65f6\u591a\u76ee\u6807\u5206\u5272\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684 DesktopObjects-360 \u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u76ee\u6807\u5206\u5272\u65b9\u9762\u5b58\u5728\u521d\u59cb\u5316\u65f6\u95f4\u957f\u548c\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "PointGauss \u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u70b9\u4e91\u7684 3D \u9ad8\u65af\u56fe\u5143\u89e3\u7801\u5668\u548c GPU \u52a0\u901f\u7684 2D \u63a9\u7801\u6e32\u67d3\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u591a\u76ee\u6807\u5206\u5272\u3002", "result": "PointGauss \u5728\u591a\u89c6\u56fe mIoU \u65b9\u9762\u53d6\u5f97\u4e86 1.89% \u81f3 31.78% \u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "PointGauss \u901a\u8fc7\u76f4\u63a5\u89e3\u6790\u9ad8\u65af\u56fe\u5143\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684 3D \u5206\u5272\uff0c\u5e76\u5728\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u7684 DesktopObjects-360 \u6570\u636e\u96c6\u4e3a 3D \u5206\u5272\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u57fa\u51c6\u3002"}}
{"id": "2508.00794", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.00794", "abs": "https://arxiv.org/abs/2508.00794", "authors": ["Hye-Won Ko", "Kyung-Jin Lee"], "title": "Magnetic Octupole Hall Effect in d-Wave Altermagnets", "comment": "7 pages, 3 figures", "summary": "Order parameters not only characterize symmetry-broken equilibrium phases but\nalso govern transport phenomena in the nonequilibrium regime. Altermagnets, a\nclass of magnetic systems integrating ferromagnetic and antiferromagnetic\nfeatures, host multipolar orders in addition to dipolar Neel order. In this\nwork, we demonstrate the multipole Hall effect in d-wave altermagnets--a\ntransverse flow of multipole moments induced by an electric field. Using\nsymmetry analysis and linear response theory, we show that the magnetic\noctupole Hall effect persists even in symmetries where the spin-splitter effect\nis forbidden and thus provides a robust experimental signature. In addition, we\nidentify a sizable electric quadrupole Hall effect, originating from quadrupole\nsplittings in the band structure. Our results expand the family of Hall effects\nto include higher-order multipolar responses and establish altermagnets as a\nversatile platform for exploring multipole transport beyond spin and orbital\ndegrees of freedom.", "AI": {"tldr": "Altermagnets exhibit multipole Hall effects, offering a new avenue for exploring transport phenomena.", "motivation": "Order parameters not only characterize symmetry-broken equilibrium phases but also govern transport phenomena in the nonequilibrium regime. Altermagnets, a class of magnetic systems integrating ferromagnetic and antiferromagnetic features, host multipolar orders in addition to dipolar Neel order.", "method": "Using symmetry analysis and linear response theory", "result": "Demonstrates the multipole Hall effect in d-wave altermagnets, showing that the magnetic octupole Hall effect persists even in symmetries where the spin-splitter effect is forbidden and identifying a sizable electric quadrupole Hall effect, originating from quadrupole splittings in the band structure.", "conclusion": "Order parameters not only characterize symmetry-broken equilibrium phases but also govern transport phenomena in the nonequilibrium regime. Altermagnets, a class of magnetic systems integrating ferromagnetic and antiferromagnetic features, host multipolar orders in addition to dipolar Neel order. In this work, we demonstrate the multipole Hall effect in d-wave altermagnets--a transverse flow of multipole moments induced by an electric field. Using symmetry analysis and linear response theory, we show that the magnetic octupole Hall effect persists even in symmetries where the spin-splitter effect is forbidden and thus provides a robust experimental signature. In addition, we identify a sizable electric quadrupole Hall effect, originating from quadrupole splittings in the band structure. Our results expand the family of Hall effects to include higher-order multipolar responses and establish altermagnets as a versatile platform for exploring multipole transport beyond spin and orbital degrees of freedom."}}
{"id": "2508.00161", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00161", "abs": "https://arxiv.org/abs/2508.00161", "authors": ["Ziqian Zhong", "Aditi Raghunathan"], "title": "Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs", "comment": null, "summary": "The releases of powerful open-weight large language models (LLMs) are often\nnot accompanied by access to their full training data. Existing\ninterpretability methods, particularly those based on activations, often\nrequire or assume distributionally similar data. This is a significant\nlimitation when detecting and defending against novel potential threats like\nbackdoors, which are by definition out-of-distribution.\n  In this work, we introduce a new method for understanding, monitoring and\ncontrolling fine-tuned LLMs that interprets weights, rather than activations,\nthereby side stepping the need for data that is distributionally similar to the\nunknown training data. We demonstrate that the top singular vectors of the\nweight difference between a fine-tuned model and its base model correspond to\nnewly acquired behaviors. By monitoring the cosine similarity of activations\nalong these directions, we can detect salient behaviors introduced during\nfine-tuning with high precision.\n  For backdoored models that bypasses safety mechanisms when a secret trigger\nis present, our method stops up to 100% of attacks with a false positive rate\nbelow 1.2%. For models that have undergone unlearning, we detect inference on\nerased topics with accuracy up to 95.42% and can even steer the model to\nrecover \"unlearned\" information. Besides monitoring, our method also shows\npotential for pre-deployment model auditing: by analyzing commercial\ninstruction-tuned models (OLMo, Llama, Qwen), we are able to uncover\nmodel-specific fine-tuning focus including marketing strategies and Midjourney\nprompt generation.\n  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6743\u91cd\u800c\u975e\u6fc0\u6d3b\u7684LLM\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u79f0\u4e3aWeightWatch\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u6a21\u578b\u6743\u91cd\u5dee\u5f02\u6765\u8bc6\u522b\u548c\u76d1\u63a7\u65b0\u884c\u4e3a\uff0c\u53ef\u6709\u6548\u68c0\u6d4b\u540e\u95e8\u653b\u51fb\uff08100%\u653b\u51fb\u963b\u6b62\u7387\uff0c<1.2%\u8bef\u62a5\u7387\uff09\u5e76\u5ba1\u8ba1\u6a21\u578b\uff08\u5982\u63ed\u793a\u8425\u9500\u7b56\u7565\uff09\u3002\u5b83\u8fd8\u80fd\u7528\u4e8e\u6a21\u578b\u201c\u53bb\u5b66\u4e60\u201d\u7684\u76d1\u63a7\u548c\u5f15\u5bfc\u3002\u4e0e\u4f9d\u8d56\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u7684\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0cWeightWatch\u4e0d\u9700\u8981\u4e0e\u672a\u77e5\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u76f8\u4f3c\u7684\u6570\u636e\u3002\u5b9e\u73b0\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u516c\u5f00\u3002", "motivation": "\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff08\u5c24\u5176\u662f\u57fa\u4e8e\u6fc0\u6d3b\u7684\u65b9\u6cd5\uff09\u901a\u5e38\u9700\u8981\u6216\u5047\u8bbe\u4e0e\u672a\u77e5\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u76f8\u4f3c\u7684\u6570\u636e\uff0c\u8fd9\u5728\u68c0\u6d4b\u548c\u9632\u5fa1\u7531\u5176\u5b9a\u4e49\u4e3a\u5206\u5e03\u5916\u7684\u65b0\u578b\u6f5c\u5728\u5a01\u80c1\uff08\u5982\u540e\u95e8\uff09\u65f6\u662f\u4e00\u4e2a\u91cd\u5927\u9650\u5236\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4e0e\u672a\u77e5\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e0d\u76f8\u4f3c\u7684\u6570\u636e\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u91ca\u6743\u91cd\u800c\u975e\u6fc0\u6d3b\u6765\u7406\u89e3\u3001\u76d1\u63a7\u548c\u63a7\u5236\u7ecf\u8fc7\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u5fae\u8c03\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u4e4b\u95f4\u6743\u91cd\u5dee\u5f02\u7684\u9876\u90e8\u5947\u5f02\u5411\u91cf\u6765\u8bc6\u522b\u65b0\u83b7\u5f97\u7684\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u76d1\u63a7\u6cbf\u8fd9\u4e9b\u65b9\u5411\u7684\u6fc0\u6d3b\u7684\u4f59\u5f26\u76f8\u4f3c\u6027\u6765\u68c0\u6d4b\u5fae\u8c03\u671f\u95f4\u5f15\u5165\u7684\u663e\u8457\u884c\u4e3a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u540e\u95e8\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u963b\u6b62\u9ad8\u8fbe100%\u7684\u653b\u51fb\uff0c\u8bef\u62a5\u7387\u4f4e\u4e8e1.2%\u3002\u5728\u6a21\u578b\u201c\u53bb\u5b66\u4e60\u201d\u65b9\u9762\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u9ad8\u8fbe95.42%\u7684\u51c6\u786e\u7387\u68c0\u6d4b\u5173\u4e8e\u5df2\u5220\u9664\u4e3b\u9898\u7684\u63a8\u7406\uff0c\u5e76\u80fd\u5f15\u5bfc\u6a21\u578b\u6062\u590d\u201c\u5df2\u53bb\u5b66\u4e60\u201d\u7684\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u5728\u6a21\u578b\u5ba1\u8ba1\u65b9\u9762\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u63ed\u793a\u4e86\u5546\u4e1a\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\uff08OLMo\u3001Llama\u3001Qwen\uff09\u7684\u6a21\u578b\u7279\u5b9a\u5fae\u8c03\u91cd\u70b9\uff0c\u4f8b\u5982\u8425\u9500\u7b56\u7565\u548cMidjourney\u63d0\u793a\u751f\u6210\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6743\u91cd\u7684LLM\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u9632\u5fa1\u65b0\u578b\u540e\u95e8\u653b\u51fb\uff0c\u5e76\u80fd\u76d1\u63a7\u548c\u5f15\u5bfc\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u5fae\u8c03\u6a21\u578b\u4e0e\u57fa\u7840\u6a21\u578b\u4e4b\u95f4\u7684\u6743\u91cd\u5dee\u5f02\u7684\u9876\u90e8\u5947\u5f02\u5411\u91cf\u6765\u8bc6\u522b\u65b0\u83b7\u5f97\u7684\u884c\u4e3a\uff0c\u5e76\u5229\u7528\u6fc0\u6d3b\u7684\u4f59\u5f26\u76f8\u4f3c\u6027\u6765\u68c0\u6d4b\u5fae\u8c03\u5f15\u5165\u7684\u663e\u8457\u884c\u4e3a\u3002\u5728\u540e\u95e8\u68c0\u6d4b\u65b9\u9762\uff0c\u8be5\u65b9\u6cd5\u80fd\u963b\u6b62\u9ad8\u8fbe100%\u7684\u653b\u51fb\uff0c\u8bef\u62a5\u7387\u4f4e\u4e8e1.2%\u3002\u5728\u6a21\u578b\u201c\u53bb\u5b66\u4e60\u201d\u65b9\u9762\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u9ad8\u8fbe95.42%\u7684\u51c6\u786e\u7387\u68c0\u6d4b\u5173\u4e8e\u5df2\u5220\u9664\u4e3b\u9898\u7684\u63a8\u7406\uff0c\u5e76\u80fd\u5f15\u5bfc\u6a21\u578b\u6062\u590d\u201c\u5df2\u53bb\u5b66\u4e60\u201d\u7684\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u53ef\u7528\u4e8e\u90e8\u7f72\u524d\u7684\u6a21\u578b\u5ba1\u8ba1\uff0c\u901a\u8fc7\u5206\u6790\u5546\u4e1a\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7279\u5b9a\u7684\u5fae\u8c03\u91cd\u70b9\uff0c\u5982\u8425\u9500\u7b56\u7565\u548cMidjourney\u63d0\u793a\u751f\u6210\u3002"}}
{"id": "2508.00360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00360", "abs": "https://arxiv.org/abs/2508.00360", "authors": ["Alan Dao", "Dinh Bach Vu", "Alex Nguyen", "Norapat Buppodom"], "title": "Lucy: edgerunning agentic web search on mobile with machine generated task vectors", "comment": null, "summary": "Small language models (SLMs) are inherently limited in knowledge-intensive\ntasks due to their constrained capacity. While test-time computation offers a\npath to enhanced performance, most approaches treat reasoning as a fixed or\nheuristic process. In this work, we propose a new paradigm: viewing the model's\ninternal reasoning, delimited by <think> and </think> tags, as a dynamic task\nvector machine. Rather than treating the content inside these tags as a mere\ntrace of thought, we interpret the generation process itself as a mechanism\nthrough which the model \\textbf{constructs and refines its own task vectors} on\nthe fly. We developed a method to optimize this dynamic task vector machine\nthrough RLVR and successfully trained an agentic web-search model. We present\nLucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with\nMCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing\non par with much larger models such as DeepSeek-V3. This demonstrates that\nsmall models can rival large ones when equipped with structured,\nself-constructed task reasoning.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u5c06\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7684\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\uff08<think>...</think>\uff09\u89c6\u4e3a\u52a8\u6001\u4efb\u52a1\u5411\u91cf\u673a\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u4f18\u5316\u3002\u4ed6\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u540d\u4e3aLucy\u768417\u4ebf\u53c2\u6570SLM\uff0c\u901a\u8fc7\u8fd9\u79cd\u52a8\u6001\u63a8\u7406\u673a\u5236\u548c\u591a\u6a21\u6001\u590d\u6742\u6027\uff08MCP\uff09\u96c6\u6210\uff0c\u5728SimpleQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e8678.3%\u7684\u51c6\u786e\u7387\uff0c\u8868\u73b0\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u3002", "motivation": "\u7531\u4e8e\u5bb9\u91cf\u53d7\u9650\uff0c\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u867d\u7136\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u5c06\u63a8\u7406\u89c6\u4e3a\u56fa\u5b9a\u7684\u6216\u542f\u53d1\u5f0f\u7684\u3002", "method": "\u5c06\u6a21\u578b\u7684\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\uff08<think>...</think>\uff09\u89c6\u4e3a\u52a8\u6001\u4efb\u52a1\u5411\u91cf\u673a\uff0c\u5e76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4ece\u4eba\u7c7b\u53cd\u9988\uff08RLHF\uff09\u6765\u4f18\u5316\u8be5\u8fc7\u7a0b\uff0c\u4ee5\u8bad\u7ec3\u4e00\u4e2a\u4ee3\u7406\u7f51\u7edc\u641c\u7d22\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7f\u4e00\u4e2a17\u4ebf\u53c2\u6570\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08Lucy\uff09\u5728SimpleQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u4e8678.3%\u7684\u51c6\u786e\u7387\uff0c\u4e0eDeepSeek-V3\u7b49\u5927\u578b\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u591a\u6a21\u6001\u590d\u6742\u6027\uff08MCP\uff09\u548c\u52a8\u6001\u4efb\u52a1\u5411\u91cf\uff0c\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u3001\u81ea\u6784\u4efb\u52a1\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00378", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00378", "abs": "https://arxiv.org/abs/2508.00378", "authors": ["Shixin Yi", "Lin Shang"], "title": "CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding", "comment": "Preparing for AAAI 2026, Multimodal Reasoning", "summary": "Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in\nvision-language models (VLMs), but it often produces explanations that are\nlinguistically fluent yet lack grounding in visual content. We observe that\nsuch hallucinations arise in part from the absence of an explicit verification\nmechanism during multi-step reasoning. To address this, we propose\n\\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with\n\\textbf{G}rounded \\textbf{I}nsights), a modular framework that introduces\nvisual verification into the reasoning process. CoRGI follows a three-stage\npipeline: it first generates a textual reasoning chain, then extracts\nsupporting visual evidence for each reasoning step via a dedicated module\n(VEVM), and finally synthesizes the textual rationale with visual evidence to\ngenerate a grounded, verified answer. The framework can be integrated with\nexisting VLMs without end-to-end retraining. We evaluate CoRGI on the VCR\nbenchmark and find that it improves reasoning performance on two representative\nopen-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm\nthe contribution of each step in the verification module, and human evaluations\nsuggest that CoRGI leads to more factual and helpful explanations. We also\nexamine alternative designs for the visual verification step and discuss\npotential limitations of post-hoc verification frameworks. These findings\nhighlight the importance of grounding intermediate reasoning steps in visual\nevidence to enhance the robustness of multimodal reasoning.", "AI": {"tldr": "CoRGI framework improves VLM reasoning by adding visual verification to the Chain-of-Thought process, grounding explanations in visual evidence and reducing hallucinations. It enhances performance without retraining and yields more factual explanations.", "motivation": "Existing Chain-of-Thought (CoT) prompting in vision-language models (VLMs) often produces explanations that are fluent but lack grounding in visual content, leading to hallucinations. This is partly due to the absence of an explicit verification mechanism during multi-step reasoning. The motivation is to address this by introducing visual verification into the reasoning process.", "method": "CoRGI (Chain of Reasoning with Grounded Insights) is a modular framework that introduces visual verification into the reasoning process of VLMs. It follows a three-stage pipeline: 1. Generate a textual reasoning chain. 2. Extract supporting visual evidence for each reasoning step using a dedicated module (VEVM). 3. Synthesize the textual rationale with visual evidence to generate a grounded, verified answer. The framework can be integrated with existing VLMs without end-to-end retraining.", "result": "CoRGI improves reasoning performance on the VCR benchmark using two representative open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm the contribution of each step in the verification module, and human evaluations suggest that CoRGI leads to more factual and helpful explanations. The paper also examines alternative designs for the visual verification step and discusses potential limitations of post-hoc verification frameworks.", "conclusion": "Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in vision-language models (VLMs), but it often produces explanations that are linguistically fluent yet lack grounding in visual content. This paper addresses this issue by proposing CoRGI (Chain of Reasoning with Grounded Insights), a modular framework that introduces visual verification into the reasoning process. CoRGI integrates with existing VLMs without end-to-end retraining and has demonstrated improved reasoning performance on the VCR benchmark with open-source VLM backbones. Ablation studies and human evaluations confirm the effectiveness of CoRGI in generating more factual and helpful explanations by grounding intermediate reasoning steps in visual evidence, highlighting the importance of this approach for enhancing the robustness of multimodal reasoning."}}
{"id": "2508.00691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00691", "abs": "https://arxiv.org/abs/2508.00691", "authors": ["Fabian C. Weigend", "Dabin K. Choe", "Santiago Canete", "Conor J. Walsh"], "title": "Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait", "comment": "8 pages, 6 figures, 2 tables", "summary": "Recent work has shown that exoskeletons controlled through data-driven\nmethods can dynamically adapt assistance to various tasks for healthy young\nadults. However, applying these methods to populations with neuromotor gait\ndeficits, such as post-stroke hemiparesis, is challenging. This is due not only\nto high population heterogeneity and gait variability but also to a lack of\npost-stroke gait datasets to train accurate models. Despite these challenges,\ndata-driven methods offer a promising avenue for control, potentially allowing\nexoskeletons to function safely and effectively in unstructured community\nsettings. This work presents a first step towards enabling adaptive\nplantarflexion and dorsiflexion assistance from data-driven torque estimation\nduring post-stroke walking. We trained a multi-task Temporal Convolutional\nNetwork (TCN) using collected data from four post-stroke participants walking\non a treadmill ($R^2$ of $0.74 \\pm 0.13$). The model uses data from three\ninertial measurement units (IMU) and was pretrained on healthy walking data\nfrom 6 participants. We implemented a wearable prototype for our ankle torque\nestimation approach for exoskeleton control and demonstrated the viability of\nreal-time sensing, estimation, and actuation with one post-stroke participant.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\uff08TCN\uff09\u7684\u65b9\u6cd5\uff0c\u5229\u7528IMU\u6570\u636e\u4ece\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u4f30\u8ba1\u4e2d\u98ce\u540e\u60a3\u8005\u7684\u8e1d\u5173\u8282\u529b\u77e9\uff0c\u5e76\u6210\u529f\u6f14\u793a\u4e86\u5176\u5728\u5916\u9aa8\u9abc\u63a7\u5236\u4e2d\u7684\u5b9e\u65f6\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u5728\u4e3a\u5065\u5eb7\u6210\u4eba\u63a7\u5236\u5916\u9aa8\u9abc\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u4eba\u7fa4\u5f02\u8d28\u6027\u9ad8\u3001\u6b65\u6001\u53d8\u5f02\u6027\u5927\u4ee5\u53ca\u7f3a\u4e4f\u4e2d\u98ce\u540e\u6b65\u6001\u6570\u636e\u96c6\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u795e\u7ecf\u8fd0\u52a8\u6b65\u6001\u7f3a\u9677\u4eba\u7fa4\uff08\u5982\u4e2d\u98ce\u540e\u504f\u762b\uff09\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u672c\u7814\u7a76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\uff08TCN\uff09\uff0c\u4f7f\u7528\u6765\u81ea\u56db\u540d\u4e2d\u98ce\u540e\u53c2\u4e0e\u8005\u5728\u8dd1\u6b65\u673a\u4e0a\u884c\u8d70\u7684\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u6765\u81ea\u516d\u540d\u5065\u5eb7\u53c2\u4e0e\u8005\u7684\u6570\u636e\u5bf9\u6a21\u578b\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u5bf9\u4e2d\u98ce\u540e\u53c2\u4e0e\u8005\u6b65\u6001\u7684\u8e1d\u5173\u8282\u529b\u77e9\u4f30\u8ba1\u65b9\u9762\uff0cTCN\u6a21\u578b\u8fbe\u5230\u4e860.74\u00b10.13\u7684R^2\u503c\u3002\u7814\u7a76\u8fd8\u6210\u529f\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7528\u4e8e\u5916\u9aa8\u9abc\u63a7\u5236\u7684\u8e1d\u5173\u8282\u529b\u77e9\u4f30\u8ba1\u65b9\u6cd5\u7684\u7a7f\u6234\u5f0f\u539f\u578b\uff0c\u5e76\u4e0e\u4e00\u540d\u4e2d\u98ce\u540e\u53c2\u4e0e\u8005\u4e00\u8d77\u6f14\u793a\u4e86\u5b9e\u65f6\u4f20\u611f\u3001\u4f30\u8ba1\u548c\u9a71\u52a8\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4eceIMU\u6570\u636e\u4f30\u8ba1\u884c\u8d70\u65f6\u7684\u8e1d\u5173\u8282\u529b\u77e9\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u4e2d\u98ce\u540e\u6b65\u6001\u7684\u81ea\u9002\u5e94\u5047\u80a2\u63d0\u4f9b\u4e86\u521d\u6b65\u8bc1\u636e\u3002"}}
{"id": "2508.00260", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.00260", "abs": "https://arxiv.org/abs/2508.00260", "authors": ["Hyundong Jin", "Hyung Jin Chang", "Eunwoo Kim"], "title": "Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models", "comment": "Accepted to ICCV 2025", "summary": "Continual learning enables pre-trained generative vision-language models\n(VLMs) to incorporate knowledge from new tasks without retraining data from\nprevious ones. Recent methods update a visual projector to translate visual\ninformation for new tasks, connecting pre-trained vision encoders with large\nlanguage models. However, such adjustments may cause the models to prioritize\nvisual inputs over language instructions, particularly learning tasks with\nrepetitive types of textual instructions. To address the neglect of language\ninstructions, we propose a novel framework that grounds the translation of\nvisual information on instructions for language models. We introduce a mixture\nof visual projectors, each serving as a specialized visual-to-language\ntranslation expert based on the given instruction context to adapt to new\ntasks. To avoid using experts for irrelevant instruction contexts, we propose\nan expert recommendation strategy that reuses experts for tasks similar to\nthose previously learned. Additionally, we introduce expert pruning to\nalleviate interference from the use of experts that cumulatively activated in\nprevious tasks. Extensive experiments on diverse vision-language tasks\ndemonstrate that our method outperforms existing continual learning approaches\nby generating instruction-following responses.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u3001\u63a8\u8350\u548c\u526a\u679d\u89c6\u89c9\u6295\u5f71\u4eea\u4e13\u5bb6\u6765\u6539\u8fdb\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u8bed\u8a00\u6307\u4ee4\u3002", "motivation": "\u6700\u8fd1\u7684\u65b9\u6cd5\u901a\u8fc7\u66f4\u65b0\u89c6\u89c9\u6295\u5f71\u4eea\u6765\u4e3a\u65b0\u4efb\u52a1\u8f6c\u6362\u89c6\u89c9\u4fe1\u606f\uff0c\u8fde\u63a5\u4e86\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u8c03\u6574\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u4f18\u5148\u8003\u8651\u89c6\u89c9\u8f93\u5165\u800c\u975e\u8bed\u8a00\u6307\u4ee4\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5177\u6709\u91cd\u590d\u7c7b\u578b\u6587\u672c\u6307\u4ee4\u7684\u5b66\u4e60\u4efb\u52a1\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u6765\u8c03\u6574\u89c6\u89c9\u4fe1\u606f\u7684\u7ffb\u8bd1\u3002\u5f15\u5165\u4e86\u89c6\u89c9\u6295\u5f71\u4eea\u6df7\u5408\u673a\u5236\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6295\u5f71\u4eea\u4f5c\u4e3a\u57fa\u4e8e\u7ed9\u5b9a\u6307\u4ee4\u4e0a\u4e0b\u6587\u7684\u4e13\u95e8\u89c6\u89c9\u5230\u8bed\u8a00\u7ffb\u8bd1\u4e13\u5bb6\u6765\u9002\u5e94\u65b0\u4efb\u52a1\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u5bb6\u63a8\u8350\u7b56\u7565\uff0c\u7528\u4e8e\u5c06\u4e13\u5bb6\u7528\u4e8e\u4e0e\u5148\u524d\u5b66\u4e60\u7684\u4efb\u52a1\u76f8\u4f3c\u7684\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e13\u5bb6\u526a\u679d\uff0c\u4ee5\u51cf\u8f7b\u5148\u524d\u4efb\u52a1\u4e2d\u7d2f\u79ef\u6fc0\u6d3b\u7684\u4e13\u5bb6\u7684\u4f7f\u7528\u6240\u5e26\u6765\u7684\u5e72\u6270\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u672c\u65b9\u6cd5\u5728\u9075\u5faa\u6307\u4ee4\u7684\u54cd\u5e94\u751f\u6210\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u672c\u65b9\u6cd5\u5728\u591a\u79cd\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u901a\u8fc7\u751f\u6210\u9075\u5faa\u6307\u4ee4\u7684\u54cd\u5e94\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2410.06690", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2410.06690", "abs": "https://arxiv.org/abs/2410.06690", "authors": ["Xiyin Ye", "Qirui Cui", "Weiwei Lin", "Tao Yu"], "title": "Spin Quenching and Transport by Hidden Dzyaloshinskii-Moriya Interactions", "comment": "12 pages, 6 figures", "summary": "Explicit interactions, \\textit{e.g.}, dipolar and exchange couplings, usually\ngovern magnetization dynamics. Some interactions may be hidden from the global\ncrystal symmetry. We report that in a large class of \\textit{uniaxial}\nantiferromagnets, a \\textit{hidden} Dzyaloshinskii-Moriya interaction with\nretaining global inversion symmetry quenches the spin of magnon along the\nN\\'eel vector ${\\bf n}$, thus forbidding its angular-momentum flow. Some magnon\nspins, termed ``nodal\" and ``corner\" spins, survive when they distribute\n\\textit{singularly} at the hot spots, i.e., high-symmetric degeneracy points in\nthe Brillouin zone, and are protected by crystal symmetries. The biased\nmagnetic field along ${\\bf n}$ broadens such distributions, allowing bulk spin\ntransport with unique signatures in the magnetic field and temperature\ndependencies. This explains recent experiments and highlights the role of\nhidden interaction.", "AI": {"tldr": "\u5728\u5355\u8f74\u53cd\u94c1\u78c1\u4f53\u4e2d\uff0c\u9690\u85cf\u7684Dzyaloshinskii-Moriya\u76f8\u4e92\u4f5c\u7528\u6291\u5236\u4e86\u78c1\u77e2\u91cf\u7684\u89d2\u52a8\u91cf\u6d41\u3002\u53d7\u6676\u683c\u5bf9\u79f0\u6027\u4fdd\u62a4\u7684\u201c\u8282\u70b9\u201d\u548c\u201c\u89d2\u201d\u81ea\u65cb\u5728\u7279\u5b9a\u533a\u57df\u5947\u5f02\u5206\u5e03\uff0c\u5e76\u53ef\u901a\u8fc7\u65bd\u52a0\u78c1\u573a\u8c03\u63a7\u3002", "motivation": "\u7814\u7a76\u4ee5\u524d\u8ba4\u4e3a\u78c1\u77e9\u52a8\u529b\u5b66\u7531\u5076\u6781\u548c\u4ea4\u6362\u8026\u5408\u7b49\u663e\u5f0f\u76f8\u4e92\u4f5c\u7528\u63a7\u5236\uff0c\u4f46\u8fd9\u4e9b\u76f8\u4e92\u4f5c\u7528\u53ef\u80fd\u9690\u85cf\u5728\u5168\u5c40\u6676\u4f53\u5bf9\u79f0\u6027\u4e4b\u5916\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u5355\u8f74\u53cd\u94c1\u78c1\u4f53\u4e2d\u9690\u85cf\u7684Dzyaloshinskii-Moriya\u76f8\u4e92\u4f5c\u7528\uff0c\u89e3\u91ca\u4e86\u78c1\u77e2\u91cf\u89d2\u52a8\u91cf\u6d41\u88ab\u6291\u5236\u7684\u73b0\u8c61\uff0c\u5e76\u8bc6\u522b\u4e86\u53d7\u6676\u683c\u5bf9\u79f0\u6027\u4fdd\u62a4\u7684\u201c\u8282\u70b9\u201d\u548c\u201c\u89d2\u201d\u81ea\u65cb\u3002", "result": "\u53d1\u73b0\u9690\u85cf\u7684Dzyaloshinskii-Moriya\u76f8\u4e92\u4f5c\u7528\u6291\u5236\u4e86\u78c1\u77e2\u91cf\u7684\u89d2\u52a8\u91cf\u6d41\uff0c\u5e76\u4e14\u201c\u8282\u70b9\u201d\u548c\u201c\u89d2\u201d\u81ea\u65cb\u5728\u5e03\u91cc\u6e0a\u533a\u7684\u9ad8\u5bf9\u79f0\u6027\u9000\u5316\u70b9\uff08\u70ed\u70b9\uff09\u5206\u5e03\u5947\u5f02\uff0c\u53d7\u6676\u683c\u5bf9\u79f0\u6027\u4fdd\u62a4\uff0c\u5e76\u4e14\u53d7\u504f\u7f6e\u78c1\u573a\u6cbfN\u00e9el\u77e2\u91cf\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u4e00\u79cd\u9690\u85cf\u7684Dzyaloshinskii-Moriya\u76f8\u4e92\u4f5c\u7528\uff0c\u5b83\u5728\u5355\u8f74\u53cd\u94c1\u78c1\u4f53\u4e2d\u6291\u5236\u4e86\u78c1\u77e2\u91cf\u7684\u89d2\u52a8\u91cf\u6d41\uff0c\u5e76\u901a\u8fc7\u6676\u683c\u5bf9\u79f0\u6027\u4fdd\u62a4\u4e86\u67d0\u4e9b\u201c\u8282\u70b9\u201d\u548c\u201c\u89d2\u201d\u81ea\u65cb\u3002"}}
{"id": "2508.00172", "categories": ["cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00172", "abs": "https://arxiv.org/abs/2508.00172", "authors": ["Fupei Guo", "Hao Zheng", "Xiang Zhang", "Li Chen", "Yue Wang", "Songyang Zhang"], "title": "DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission", "comment": "To appear in 2025 IEEE Global Communications Conference (Globecom)", "summary": "The rapid development of artificial intelligence has driven smart health with\nnext-generation wireless communication technologies, stimulating exciting\napplications in remote diagnosis and intervention. To enable a timely and\neffective response for remote healthcare, efficient transmission of medical\ndata through noisy channels with limited bandwidth emerges as a critical\nchallenge. In this work, we propose a novel diffusion-based semantic\ncommunication framework, namely DiSC-Med, for the medical image transmission,\nwhere medical-enhanced compression and denoising blocks are developed for\nbandwidth efficiency and robustness, respectively. Unlike conventional\npixel-wise communication framework, our proposed DiSC-Med is able to capture\nthe key semantic information and achieve superior reconstruction performance\nwith ultra-high bandwidth efficiency against noisy channels. Extensive\nexperiments on real-world medical datasets validate the effectiveness of our\nframework, demonstrating its potential for robust and efficient telehealth\napplications.", "AI": {"tldr": "A new diffusion-based semantic communication framework (DiSC-Med) improves medical image transmission over noisy, low-bandwidth channels using enhanced compression and denoising, outperforming traditional methods.", "motivation": "To enable timely and effective remote healthcare, efficient transmission of medical data through noisy channels with limited bandwidth is a critical challenge driven by advancements in AI and wireless communication.", "method": "A novel diffusion-based semantic communication framework (DiSC-Med) is proposed, incorporating medical-enhanced compression and denoising blocks for bandwidth efficiency and robustness respectively. It differs from conventional pixel-wise frameworks by capturing key semantic information.", "result": "Extensive experiments on real-world medical datasets validate the effectiveness of the DiSC-Med framework, showing its ability to achieve superior reconstruction performance with ultra-high bandwidth efficiency against noisy channels.", "conclusion": "The proposed DiSC-Med framework demonstrates superior reconstruction performance and ultra-high bandwidth efficiency against noisy channels for medical image transmission, showcasing its potential for robust and efficient telehealth applications."}}
{"id": "2508.00370", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00370", "abs": "https://arxiv.org/abs/2508.00370", "authors": ["Jiyu Chen", "Poh Seng Lim", "Shuang Peng", "Daxiong Luo", "JungHau Foo", "Yap Deep", "Timothy Lee Jun Jie", "Kelvin Teh Kae Wen", "Fan Yang", "Danyu Feng", "Hao-Yun Chen", "Peng-Wen Chen", "Fangyuan Li", "Xiaoxin Chen", "Wong Wai Mun"], "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices", "comment": "9 pages", "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.", "AI": {"tldr": "EdgeInfinite-Instruct\u901a\u8fc7S-SFT\u3001PTQ\u548c\u56fa\u5b9a\u5f62\u72b6\u8ba1\u7b97\u56fe\u4f18\u5316\u4e86LLMs\u5728\u8fb9\u7f18\u8bbe\u5907\u7684\u90e8\u7f72\uff0c\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u4efb\u52a1\u7684\u6548\u7387\u548c\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72Transformer-based LLMs\u7528\u4e8e\u957f\u5e8f\u5217\u4efb\u52a1\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u6311\u6218\u6e90\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u65e5\u76ca\u589e\u957f\u7684KV\u7f13\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u9700\u8981\u6539\u8fdb\u73b0\u6709\u7684KV\u7f13\u5b58\u4f18\u5316\u65b9\u6cd5\uff08\u5b83\u4eec\u53ef\u80fd\u65e0\u6cd5\u51cf\u5c11\u9996\u6b21\u4ee4\u724c\u65f6\u95f4TTFT\u6216\u4f1a\u56e0\u4ee4\u724c\u4fee\u526a\u800c\u964d\u4f4e\u6027\u80fd\uff09\u548c\u66ff\u4ee3\u5e8f\u5217\u5efa\u6a21\u67b6\u6784\uff08\u5b83\u4eec\u901a\u5e38\u9700\u8981\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u4e14\u7f3a\u4e4f\u57fa\u7840\u8bbe\u65bd\u652f\u6301\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEdgeInfinite-Instruct\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u901a\u8fc7\u91c7\u7528\u6bb5\u76d1\u7763\u5fae\u8c03\uff08S-SFT\uff09\u7b56\u7565\u6765\u4f18\u5316\u957f\u5e8f\u5217\u4efb\u52a1\uff0c\u5e76\u9488\u5bf9\u8fb9\u7f18NPU\u90e8\u7f72\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u5305\u62ec\u4f7f\u7528\u7ec6\u7c92\u5ea6\u8bad\u7ec3\u540e\u91cf\u5316\uff08PTQ\uff09\u548c\u56fa\u5b9a\u5f62\u72b6\u8ba1\u7b97\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u7684\u79fb\u52a8\u4efb\u52a1\u4e0a\uff0c\u63d0\u9ad8\u4e86\u9886\u57df\u7279\u5b9a\u6027\u80fd\uff0c\u540c\u65f6\u5728NPU\u52a0\u901f\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4fdd\u6301\u4e86\u6548\u7387\u3002", "conclusion": "EdgeInfinite-Instruct\u901a\u8fc7\u7ed3\u5408S-SFT\u7b56\u7565\u3001PTQ\u548c\u56fa\u5b9a\u5f62\u72b6\u8ba1\u7b97\u56fe\uff0c\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728NPU\u52a0\u901f\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5904\u7406\u957f\u5e8f\u5217\u4efb\u52a1\uff08\u5982\u6458\u8981\u548c\u95ee\u7b54\uff09\u7684\u9886\u57df\u7279\u5b9a\u6027\u80fd\u3002"}}
{"id": "2508.00254", "categories": ["quant-ph", "cond-mat.quant-gas", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.00254", "abs": "https://arxiv.org/abs/2508.00254", "authors": ["Thomas Young", "Jerome LLoyd", "Curt von Keyserlingk"], "title": "Breakdown of Fermi's Golden Rule in 1d systems at non-zero temperature", "comment": null, "summary": "In interacting quantum systems, the single-particle Green's function is\nexpected to decay in time due to the interaction induced decoherence of\nquasiparticles. In the limit of weak interaction strengths ($\\Delta$), a naive\napplication of Fermi's Golden Rule (FGR) predicts an $\\mathcal{O}(\\Delta^{2})$\nquasiparticle decay rate. However, for 1d fermions on the lattice at $T>0$,\nthis calculation gives a divergent result and the scaling of the quasiparticle\nlifetime with interaction strength remains an open question. In this work we\npropose a solution to this question: combining numerical simulations using the\nrecently introduced dissipation-assisted operator evolution (DAOE) method, with\nnon-perturbative diagrammatic re-summations, we predict a logarithmic\nenhancement of the quasiparticle decay rate $\\tau^{-1} \\sim \\Delta^{2} \\log\n\\Delta^{-2}$. We argue that this effect is present in a wide variety of\nwell-known weakly interacting quantum fermionic and bosonic systems, and even\nin some classical systems, provided the non-interacting limit has\nquasiparticles with a generic dispersion.", "AI": {"tldr": "\u5bf9\u4e8e\u4e00\u7ef4\u8d39\u7c73\u5b50\u7cfb\u7edf\uff0c\u5f31\u76f8\u4e92\u4f5c\u7528\u4e0b\u7684\u51c6\u7c92\u5b50\u5bff\u547d\u4e0e\u5176\u8870\u51cf\u7387\u7684\u5173\u7cfb\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u503c\u6a21\u62df\u548c\u56fe\u91cd\u6c42\u548c\u7684\u65b9\u6cd5\uff0c\u9884\u6d4b\u8870\u51cf\u7387\u4f1a\u4ee5 $\\\\\\log$ \u7684\u65b9\u5f0f\u589e\u5f3a\uff0c\u5373 $\\\\\\tau^{-1} \\\\sim \\\\Delta^{2} \\\\log \\\\Delta^{-2}$\u3002", "motivation": "\u5728\u76f8\u4e92\u4f5c\u7528\u91cf\u5b50\u7cfb\u7edf\u4e2d\uff0c\u5355\u7c92\u5b50\u683c\u6797\u51fd\u6570\u9884\u671f\u4f1a\u56e0\u76f8\u4e92\u4f5c\u7528\u8bf1\u5bfc\u7684\u51c6\u7c92\u5b50\u9000\u76f8\u5e72\u800c\u5728\u65f6\u95f4\u4e0a\u8870\u51cf\u3002\u5bf9\u4e8e\u5f31\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\uff08$\\\triangle$\uff09\u7684\u6781\u9650\uff0c\u8d39\u7c73\u9ec4\u91d1\u5b9a\u5219\uff08FGR\uff09\u9884\u6d4b\u51c6\u7c92\u5b50\u8870\u51cf\u7387\u4e3a $\\\\\"(\\\triangle^{2})\\\\$\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u4e00\u7ef4\u8d39\u7c73\u5b50\u5728 $T>0$ \u7684\u683c\u70b9\u4e0a\uff0c\u8be5\u8ba1\u7b97\u7ed3\u679c\u53d1\u6563\uff0c\u51c6\u7c92\u5b50\u5bff\u547d\u4e0e\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u7684\u5173\u7cfb\u4ecd\u7136\u662f\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u4e86\u8017\u6563\u8f85\u52a9\u7b97\u7b26\u6f14\u5316\uff08DAOE\uff09\u6570\u503c\u6a21\u62df\u548c\u975e\u5fae\u6270\u56fe\u91cd\u6c42\u548c\u3002", "result": "\u9884\u6d4b\u51c6\u7c92\u5b50\u8870\u51cf\u7387\u7684\u5bf9\u6570\u589e\u5f3a\uff1a$\\\tau^{-1} \\\\sim \\\\Delta^{2} \\\\log \\\\Delta^{-2}$\uff0c\u5e76\u4e14\u8ba4\u4e3a\u8be5\u6548\u5e94\u5b58\u5728\u4e8e\u591a\u79cd\u5f31\u76f8\u4e92\u4f5c\u7528\u7684\u91cf\u5b50\u8d39\u7c73\u5b50\u548c\u73bb\u8272\u5b50\u7cfb\u7edf\uff0c\u751a\u81f3\u4e00\u4e9b\u7ecf\u5178\u7cfb\u7edf\u4e2d\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u8017\u6563\u8f85\u52a9\u7b97\u7b26\u6f14\u5316\uff08DAOE\uff09\u6570\u503c\u6a21\u62df\u548c\u975e\u5fae\u6270\u56fe\u91cd\u6c42\u548c\u7684\u65b9\u6cd5\uff0c\u9884\u6d4b\u4e86\u51c6\u7c92\u5b50\u8870\u51cf\u7387\u7684\u5bf9\u6570\u589e\u5f3a\uff0c\u5373 $\tau^{-1} \\sim \\Delta^{2} \\log \\Delta^{-2}$\u3002\u7814\u7a76\u8ba4\u4e3a\u8fd9\u79cd\u6548\u5e94\u5b58\u5728\u4e8e\u591a\u79cd\u5f31\u76f8\u4e92\u4f5c\u7528\u7684\u91cf\u5b50\u8d39\u7c73\u5b50\u548c\u73bb\u8272\u5b50\u7cfb\u7edf\uff0c\u751a\u81f3\u4e00\u4e9b\u7ecf\u5178\u7cfb\u7edf\u4e2d\uff0c\u524d\u63d0\u662f\u5176\u975e\u76f8\u4e92\u4f5c\u7528\u6781\u9650\u4e0b\u7684\u51c6\u7c92\u5b50\u5177\u6709\u901a\u7528\u7684\u8272\u6563\u3002"}}
{"id": "2508.00697", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00697", "abs": "https://arxiv.org/abs/2508.00697", "authors": ["Yiming Wu", "Huan Wang", "Zhenghao Chen", "Jianxin Pang", "Dong Xu"], "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation", "comment": "ICCV 2025", "summary": "Diffusion Policies have significantly advanced robotic manipulation tasks via\nimitation learning, but their application on resource-constrained mobile\nplatforms remains challenging due to computational inefficiency and extensive\nmemory footprint. In this paper, we propose LightDP, a novel framework\nspecifically designed to accelerate Diffusion Policies for real-time deployment\non mobile devices. LightDP addresses the computational bottleneck through two\ncore strategies: network compression of the denoising modules and reduction of\nthe required sampling steps. We first conduct an extensive computational\nanalysis on existing Diffusion Policy architectures, identifying the denoising\nnetwork as the primary contributor to latency. To overcome performance\ndegradation typically associated with conventional pruning methods, we\nintroduce a unified pruning and retraining pipeline, optimizing the model's\npost-pruning recoverability explicitly. Furthermore, we combine pruning\ntechniques with consistency distillation to effectively reduce sampling steps\nwhile maintaining action prediction accuracy. Experimental evaluations on the\nstandard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that\nLightDP achieves real-time action prediction on mobile devices with competitive\nperformance, marking an important step toward practical deployment of\ndiffusion-based policies in resource-limited environments. Extensive real-world\nexperiments also show the proposed LightDP can achieve performance comparable\nto state-of-the-art Diffusion Policies.", "AI": {"tldr": "\u63d0\u51faLightDP\u6846\u67b6\uff0c\u901a\u8fc7\u7f51\u7edc\u538b\u7f29\u548c\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\uff0c\u4f7f\u6269\u6563\u7b56\u7565\u80fd\u591f\u5728\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u7b56\u7565\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u5e73\u53f0\u4e0a\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u5185\u5b58\u5360\u7528\u5927\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7f51\u7edc\u538b\u7f29\uff08\u5305\u62ec\u7edf\u4e00\u7684\u526a\u679d\u548c\u518d\u8bad\u7ec3\u6d41\u7a0b\uff09\u548c\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\uff08\u7ed3\u5408\u526a\u679d\u6280\u672f\u548c\u4e00\u81f4\u6027\u84b8\u998f\uff09\u6765\u52a0\u901f\u6269\u6563\u7b56\u7565\u3002", "result": "LightDP\u5728PushT\u3001Robomimic\u3001CALVIN\u548cLIBERO\u7b49\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u65f6\u52a8\u4f5c\u9884\u6d4b\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u8868\u73b0\u4e0e\u6700\u5148\u8fdb\u7684\u6269\u6563\u7b56\u7565\u76f8\u5f53\u3002", "conclusion": "LightDP\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u65f6\u52a8\u4f5c\u9884\u6d4b\uff0c\u5e76\u4e14\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u6269\u6563\u7b56\u7565\u76f8\u5f53\uff0c\u662f\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\u5b9e\u9645\u90e8\u7f72\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.00265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00265", "abs": "https://arxiv.org/abs/2508.00265", "authors": ["Henghui Ding", "Song Tang", "Shuting He", "Chang Liu", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "Multimodal Referring Segmentation: A Survey", "comment": "Project Page:\n  https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation", "summary": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.", "AI": {"tldr": "\u672c\u6587\u5bf9\u591a\u6a21\u6001\u6307\u4ee3\u5206\u5272\u9886\u57df\u8fdb\u884c\u4e86\u5168\u9762\u7684 survey\uff0c\u603b\u7ed3\u4e86\u5176\u80cc\u666f\u3001\u65b9\u6cd5\u3001\u6311\u6218\u548c\u5e94\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u6027\u80fd\u6bd4\u8f83\u3002", "motivation": "\u591a\u6a21\u6001\u6307\u4ee3\u5206\u5272\u5728\u9700\u8981\u6839\u636e\u7528\u6237\u6307\u4ee4\u8fdb\u884c\u51c6\u786e\u7269\u4f53\u611f\u77e5\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e14\u5728\u8fc7\u53bb\u5341\u5e74\u4e2d\u5f97\u5230\u4e86\u663e\u8457\u5173\u6ce8\u3002", "method": "\u672c\u6587 survey \u4e86\u591a\u6a21\u6001\u6307\u4ee3\u5206\u5272\u9886\u57df\uff0c\u603b\u7ed3\u4e86\u7edf\u4e00\u7684\u5143\u67b6\u6784\uff0c\u56de\u987e\u4e86\u9488\u5bf9\u56fe\u50cf\u3001\u89c6\u9891\u548c 3D \u573a\u666f\u7684\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5e7f\u4e49\u6307\u4ee3\u8868\u8fbe\u5f0f\uff08GREx\uff09\u65b9\u6cd5\u3002", "result": "\u672c\u6587\u5bf9\u591a\u6a21\u6001\u6307\u4ee3\u5206\u5272\u8fdb\u884c\u4e86\u5168\u9762\u7684 survey\uff0c\u5305\u62ec\u80cc\u666f\u3001\u6570\u636e\u96c6\u3001\u7edf\u4e00\u5143\u67b6\u6784\u3001\u4ee3\u8868\u6027\u65b9\u6cd5\uff08\u56fe\u50cf\u3001\u89c6\u9891\u30013D \u573a\u666f\uff09\u3001GREx \u65b9\u6cd5\u3001\u76f8\u5173\u4efb\u52a1\u548c\u5e94\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u6027\u80fd\u6bd4\u8f83\u3002", "conclusion": "\u672c\u6587\u5168\u9762 survey \u4e86\u591a\u6a21\u6001\u6307\u4ee3\u5206\u5272\uff08multimodal referring segmentation\uff09\u9886\u57df\uff0c\u6db5\u76d6\u4e86\u80cc\u666f\u3001\u5e38\u7528\u6570\u636e\u96c6\u3001\u7edf\u4e00\u7684\u5143\u67b6\u6784\u3001\u9488\u5bf9\u56fe\u50cf\u3001\u89c6\u9891\u548c 3D \u573a\u666f\u7684\u4ee3\u8868\u6027\u65b9\u6cd5\u3001\u5e7f\u4e49\u6307\u4ee3\u8868\u8fbe\u5f0f\uff08GREx\uff09\u65b9\u6cd5\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\uff0c\u4ee5\u53ca\u76f8\u5173\u4efb\u52a1\u548c\u5b9e\u9645\u5e94\u7528\u3002\u6587\u7ae0\u8fd8\u63d0\u4f9b\u4e86\u6807\u51c6\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u6027\u80fd\u6bd4\u8f83\uff0c\u5e76\u9644\u6709\u6301\u7eed\u66f4\u65b0\u7684\u7814\u7a76\u8bba\u6587\u94fe\u63a5\u3002"}}
{"id": "2508.00174", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00174", "abs": "https://arxiv.org/abs/2508.00174", "authors": ["Yongchao Huang"], "title": "RL as Regressor: A Reinforcement Learning Approach for Function Approximation", "comment": "7 pages", "summary": "Standard regression techniques, while powerful, are often constrained by\npredefined, differentiable loss functions such as mean squared error. These\nfunctions may not fully capture the desired behavior of a system, especially\nwhen dealing with asymmetric costs or complex, non-differentiable objectives.\nIn this paper, we explore an alternative paradigm: framing regression as a\nReinforcement Learning (RL) problem. We demonstrate this by treating a model's\nprediction as an action and defining a custom reward signal based on the\nprediction error, and we can leverage powerful RL algorithms to perform\nfunction approximation. Through a progressive case study of learning a noisy\nsine wave, we illustrate the development of an Actor-Critic agent, iteratively\nenhancing it with Prioritized Experience Replay, increased network capacity,\nand positional encoding to enable a capable RL agent for this regression task.\nOur results show that the RL framework not only successfully solves the\nregression problem but also offers enhanced flexibility in defining objectives\nand guiding the learning process.", "AI": {"tldr": "\u5c06\u56de\u5f52\u89c6\u4e3a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u95ee\u9898\uff0c\u5e76\u4f7f\u7528Actor-Critic\u667a\u80fd\u4f53\u7ed3\u5408\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u3001\u589e\u52a0\u7f51\u7edc\u5bb9\u91cf\u548c\u4f4d\u7f6e\u7f16\u7801\u6765\u89e3\u51b3\u56de\u5f52\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u7075\u6d3b\u6027\u3002", "motivation": "\u6807\u51c6\u56de\u5f52\u6280\u672f\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u3001\u53ef\u5fae\u7684\u635f\u5931\u51fd\u6570\uff08\u5982\u5747\u65b9\u8bef\u5dee\uff09\uff0c\u8fd9\u4e9b\u51fd\u6570\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u7cfb\u7edf\u7684\u671f\u671b\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u975e\u5bf9\u79f0\u6210\u672c\u6216\u590d\u6742\u3001\u4e0d\u53ef\u5fae\u7684\u76ee\u6807\u65f6\u3002", "method": "\u5c06\u56de\u5f52\u89c6\u4e3a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u95ee\u9898\uff0c\u5c06\u6a21\u578b\u7684\u9884\u6d4b\u89c6\u4e3a\u4e00\u4e2a\u52a8\u4f5c\uff0c\u5e76\u6839\u636e\u9884\u6d4b\u8bef\u5dee\u5b9a\u4e49\u81ea\u5b9a\u4e49\u5956\u52b1\u4fe1\u53f7\uff0c\u4ee5\u5229\u7528\u5f3a\u5927\u7684RL\u7b97\u6cd5\u8fdb\u884c\u51fd\u6570\u903c\u8fd1\u3002\u901a\u8fc7\u4e00\u4e2a\u5b66\u4e60\u5e26\u566a\u6b63\u5f26\u6ce2\u7684\u6e10\u8fdb\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86Actor-Critic\u667a\u80fd\u4f53\u7684\u53d1\u5c55\uff0c\u5e76\u7ed3\u5408\u4e86\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u3001\u589e\u52a0\u7f51\u7edc\u5bb9\u91cf\u548c\u4f4d\u7f6e\u7f16\u7801\u6765\u589e\u5f3a\u667a\u80fd\u4f53\u3002 ", "result": "RL\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u56de\u5f52\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9a\u4e49\u76ee\u6807\u548c\u6307\u5bfc\u5b66\u4e60\u8fc7\u7a0b\u7684\u589e\u5f3a\u7075\u6d3b\u6027\u3002", "conclusion": "RL\u6846\u67b6\u4e0d\u4ec5\u6210\u529f\u89e3\u51b3\u4e86\u56de\u5f52\u95ee\u9898\uff0c\u8fd8\u4e3a\u5b9a\u4e49\u76ee\u6807\u548c\u6307\u5bfc\u5b66\u4e60\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2508.00385", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00385", "abs": "https://arxiv.org/abs/2508.00385", "authors": ["Dingzirui Wang", "Xuangliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness", "comment": null, "summary": "Numerous studies have investigated the underlying mechanisms of in-context\nlearning (ICL) effectiveness to inspire the design of related methods. However,\nexisting work predominantly assumes the effectiveness of the demonstrations\nprovided within ICL, while many research indicates that not all demonstrations\nare effective, failing to yielding any performance improvement during ICL.\nTherefore, in this paper, we investigate the reasons behind demonstration\nineffectiveness. Our analysis is based on gradient flow and linear\nself-attention models. By setting the gradient flow to zero, we deduce that a\ndemonstration becomes ineffective if its information has either been learned by\nthe model or is irrelevant to the user query. Furthermore, we demonstrate that\nin multi-layer models, the disparity in effectiveness among demonstrations is\namplified with layer increasing, causing the model to focus more on effective\nones. Considering that current demonstration selection methods primarily focus\non the relevance to the user query while overlooking the information that the\nmodel has already assimilated, we propose a novel method called GradS, which\nleverages gradient flow for demonstration selection. We use the magnitude of\nthe gradient flow of the demonstration with respect to a given user query as\nthe criterion, thereby ensuring the effectiveness of the chosen ones. We\nvalidate our derivation and GradS on four prominent LLMs across five mainstream\ndatasets. The experimental results confirm that the disparity in effectiveness\namong demonstrations is magnified as the model layer increases, substantiating\nour derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on\naverage over the strongest baselines, demonstrating its effectiveness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86 in-context learning (ICL) \u4e2d\u793a\u8303\u65e0\u6548\u7684\u539f\u56e0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a GradS \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u6d41\u9009\u62e9\u793a\u8303\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u793a\u8303\u7684\u6709\u6548\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u5e76\u975e\u6240\u6709\u793a\u8303\u90fd\u6709\u6548\u8fd9\u4e00\u4e8b\u5b9e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7814\u7a76\u793a\u8303\u65e0\u6548\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u80fd\u9009\u62e9\u6709\u6548\u793a\u8303\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8bbe\u7f6e\u68af\u5ea6\u6d41\u4e3a\u96f6\u6765\u5206\u6790\u793a\u8303\u65e0\u6548\u7684\u539f\u56e0\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a GradS \u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u68af\u5ea6\u6d41\u6765\u9009\u62e9\u793a\u8303\uff0c\u5e76\u4f7f\u7528\u793a\u8303\u76f8\u5bf9\u4e8e\u7ed9\u5b9a\u7528\u6237\u67e5\u8be2\u7684\u68af\u5ea6\u6d41\u5e45\u5ea6\u4f5c\u4e3a\u6807\u51c6\u3002", "result": "\u68af\u5ea6\u6d41\u5206\u6790\u548c GradS \u5728\u56db\u4e2a\u4e3b\u6d41\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4e94\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u6a21\u578b\u5c42\u6570\u589e\u52a0\u4f1a\u653e\u5927\u793a\u8303\u6709\u6548\u6027\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u652f\u6301\u4e86\u63a8\u5bfc\u3002GradS \u5e73\u5747\u6bd4\u6700\u5f3a\u7684\u57fa\u7ebf\u63d0\u9ad8\u4e86 6.8%\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u68af\u5ea6\u6d41\u548c\u7ebf\u6027\u81ea\u6ce8\u610f\u529b\u5206\u6790\u8868\u660e\uff0c\u5f53\u793a\u8303\u4fe1\u606f\u5df2\u88ab\u6a21\u578b\u5b66\u4e60\u6216\u4e0e\u7528\u6237\u67e5\u8be2\u65e0\u5173\u65f6\uff0c\u793a\u8303\u4f1a\u5931\u6548\u3002\u6a21\u578b\u5c42\u6570\u589e\u52a0\u4f1a\u653e\u5927\u793a\u8303\u6709\u6548\u6027\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u4f7f\u5f97\u6a21\u578b\u66f4\u5173\u6ce8\u6709\u6548\u7684\u793a\u8303\u3002\u63d0\u51fa\u7684 GradS \u65b9\u6cd5\u5229\u7528\u68af\u5ea6\u6d41\u6765\u9009\u62e9\u793a\u8303\uff0c\u5e76\u786e\u4fdd\u6240\u9009\u793a\u8303\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.00414", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00414", "abs": "https://arxiv.org/abs/2508.00414", "authors": ["Tianqing Fang", "Zhisong Zhang", "Xiaoyang Wang", "Rui Wang", "Can Qin", "Yuxuan Wan", "Jun-Yu Ma", "Ce Zhang", "Jiaqi Chen", "Xiyun Li", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training", "comment": "16 pages", "summary": "General AI Agents are increasingly recognized as foundational frameworks for\nthe next generation of artificial intelligence, enabling complex reasoning, web\ninteraction, coding, and autonomous research capabilities. However, current\nagent systems are either closed-source or heavily reliant on a variety of paid\nAPIs and proprietary tools, limiting accessibility and reproducibility for the\nresearch community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a\nfully open-source and (to the maximum extent) free multi-module agent framework\ndesigned to democratize the development and evaluation of advanced AI agents.\nWithin Cognitive Kernel-Pro, we systematically investigate the curation of\nhigh-quality training data for Agent Foundation Models, focusing on the\nconstruction of queries, trajectories, and verifiable answers across four key\ndomains: web, file, code, and general reasoning. Furthermore, we explore novel\nstrategies for agent test-time reflection and voting to enhance agent\nrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving\nstate-of-the-art results among open-source and free agents. Notably, our\n8B-parameter open-source model surpasses previous leading systems such as\nWebDancer and WebSailor, establishing a new performance standard for\naccessible, high-capability AI agents. Code is available at\nhttps://github.com/Tencent/CognitiveKernel-Pro", "AI": {"tldr": "Cognitive Kernel-Pro \u662f\u4e00\u4e2a\u5f00\u6e90\u514d\u8d39\u7684 AI \u4ee3\u7406\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u6539\u8fdb\u7684\u6d4b\u8bd5\u7b56\u7565\uff0c\u5728 GAIA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u9886\u5148\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u7684 AI \u4ee3\u7406\u7cfb\u7edf\u8981\u4e48\u662f\u95ed\u6e90\u7684\uff0c\u8981\u4e48\u4e25\u91cd\u4f9d\u8d56\u5404\u79cd\u4ed8\u8d39 API \u548c\u4e13\u6709\u5de5\u5177\uff0c\u8fd9\u9650\u5236\u4e86\u7814\u7a76\u793e\u533a\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u521b\u5efa\u4e00\u4e2a\u5f00\u6e90\u4e14\u514d\u8d39\u7684\u6846\u67b6\u6765\u666e\u53ca\u5148\u8fdb AI \u4ee3\u7406\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u3002", "method": "Cognitive Kernel-Pro \u662f\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u4e14\u514d\u8d39\u7684\u6a21\u5757\u5316 AI \u4ee3\u7406\u6846\u67b6\u3002\u7814\u7a76\u4eba\u5458\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u9488\u5bf9\u4ee3\u7406\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u7ba1\u7406\uff0c\u91cd\u70b9\u5173\u6ce8\u8de8\u8d8a Web\u3001\u6587\u4ef6\u3001\u4ee3\u7801\u548c\u901a\u7528\u63a8\u7406\u56db\u4e2a\u5173\u952e\u9886\u57df\u7684\u67e5\u8be2\u3001\u8f68\u8ff9\u548c\u53ef\u9a8c\u8bc1\u7b54\u6848\u7684\u6784\u5efa\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u7528\u4e8e\u4ee3\u7406\u6d4b\u8bd5\u65f6\u53cd\u601d\u548c\u6295\u7968\u7684\u65b0\u9896\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "result": "\u5728 GAIA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCognitive Kernel-Pro \u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u4f18\u4e8e\u6240\u6709\u5f00\u6e90\u4e14\u514d\u8d39\u7684\u4ee3\u7406\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5176 8B \u53c2\u6570\u5f00\u6e90\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86 WebDancer \u548c WebSailor \u7b49\u4e4b\u524d\u7684\u9886\u5148\u7cfb\u7edf\uff0c\u4e3a\u53ef\u8bbf\u95ee\u3001\u9ad8\u80fd\u529b AI \u4ee3\u7406\u6811\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\u3002", "conclusion": "Cognitive Kernel-Pro \u662f\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u4e14\u514d\u8d39\u7684\u6a21\u5757\u5316 AI \u4ee3\u7406\u6846\u67b6\uff0c\u65e8\u5728\u4fc3\u8fdb\u5148\u8fdb AI \u4ee3\u7406\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u3002\u8be5\u6846\u67b6\u5728 GAIA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5176 8B \u53c2\u6570\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u4e4b\u524d\u7684\u9886\u5148\u7cfb\u7edf\uff0c\u4e3a\u53ef\u8bbf\u95ee\u3001\u9ad8\u80fd\u529b AI \u4ee3\u7406\u6811\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\u3002"}}
{"id": "2508.00262", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00262", "abs": "https://arxiv.org/abs/2508.00262", "authors": ["Keren Li", "Peng Yan", "Hanru Jiang", "Nengkun Yu"], "title": "Towards Efficient Verification of Computation in Quantum Devices", "comment": "12 pages, appear at QCE 25", "summary": "Designing quantum processors is a complex task that demands advanced\nverification methods to ensure their correct functionality. However,\ntraditional methods of comprehensively verifying quantum devices, such as\nquantum process tomography, face significant limitations because of the\nexponential growth in computational resources. These limitations arise from\ntreating the system as a black box and ignoring its design structure.\nConsequently, new testing methods must be developed considering the design\nstructure. In this paper, we investigate the structure of computations on the\nhardware, focusing on the layered interruptible quantum circuit model and\ndesigning a scalable algorithm to verify it comprehensively. Specifically, for\na given quantum hardware that claims to process an unknown $n$ qubit $d$ layer\ncircuit via a finite set of quantum gates, our method completely reconstructs\nthe circuits within a time complexity of $O(d^2 t \\log (n/\\delta))$,\nguaranteeing success with a probability of at least $1-\\delta$. Here, $t$\nrepresents the maximum execution time for each circuit layer. Our approach\nsignificantly reduces execution time for completely verifying computations in\nquantum devices, achieving double logarithmic scaling in the problem size.\nFurthermore, we validate our algorithm through experiments using IBM's quantum\ncloud service, demonstrating its potential applicability in the noisy\nintermediate-scale quantum era.", "AI": {"tldr": "A new verification method for quantum computers significantly speeds up testing by using the device's structure, achieving double logarithmic scaling and validated on IBM's cloud service.", "motivation": "Traditional verification methods like quantum process tomography are limited by exponential resource growth due to treating quantum devices as black boxes and ignoring their design structure. New testing methods are needed that consider the design structure for effective verification of quantum processors.", "method": "The paper investigates the structure of computations on hardware, focusing on the layered interruptible quantum circuit model. It designs a scalable algorithm to verify this model comprehensively by reconstructing circuits within a time complexity of O(d^2 t log(n/\u03b4)), with success probability at least 1-\u03b4, where t is the maximum execution time per layer.", "result": "The developed algorithm completely reconstructs quantum circuits within a time complexity of O(d^2 t log(n/\u03b4)), achieving double logarithmic scaling in problem size. Experiments on IBM's quantum cloud service validated the algorithm's applicability in the noisy intermediate-scale quantum era.", "conclusion": "The paper proposes a new verification method for quantum processors that leverages the design structure, specifically the layered interruptible quantum circuit model. This method offers a scalable algorithm with a time complexity of O(d^2 t log(n/\u03b4)) for reconstructing unknown n-qubit, d-layer circuits, achieving double logarithmic scaling in problem size and significantly reducing verification execution time. The approach was validated through experiments on IBM's quantum cloud service."}}
{"id": "2508.00795", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00795", "abs": "https://arxiv.org/abs/2508.00795", "authors": ["Junbang Liang", "Pavel Tokmakov", "Ruoshi Liu", "Sruthi Sudhakar", "Paarth Shah", "Rares Ambrus", "Carl Vondrick"], "title": "Video Generators are Robot Policies", "comment": null, "summary": "Despite tremendous progress in dexterous manipulation, current visuomotor\npolicies remain fundamentally limited by two challenges: they struggle to\ngeneralize under perceptual or behavioral distribution shifts, and their\nperformance is constrained by the size of human demonstration data. In this\npaper, we use video generation as a proxy for robot policy learning to address\nboth limitations simultaneously. We propose Video Policy, a modular framework\nthat combines video and action generation that can be trained end-to-end. Our\nresults demonstrate that learning to generate videos of robot behavior allows\nfor the extraction of policies with minimal demonstration data, significantly\nimproving robustness and sample efficiency. Our method shows strong\ngeneralization to unseen objects, backgrounds, and tasks, both in simulation\nand the real world. We further highlight that task success is closely tied to\nthe generated video, with action-free video data providing critical benefits\nfor generalizing to novel tasks. By leveraging large-scale video generative\nmodels, we achieve superior performance compared to traditional behavior\ncloning, paving the way for more scalable and data-efficient robot policy\nlearning.", "AI": {"tldr": "Video Policy\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u751f\u6210\u673a\u5668\u4eba\u884c\u4e3a\u89c6\u9891\uff0c\u514b\u670d\u4e86\u73b0\u6709\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u4f9d\u8d56\u6027\u65b9\u9762\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u5177\u9c81\u68d2\u6027\u7684\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6293\u53d6\u64cd\u4f5c\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u611f\u77e5\u6216\u884c\u4e3a\u5206\u5e03\u53d8\u5316\u4e0b\u6cdb\u5316\u80fd\u529b\u5dee\u4ee5\u53ca\u6027\u80fd\u53d7\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u91cf\u9650\u5236\u7684\u4e24\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVideo Policy\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u89c6\u9891\u548c\u52a8\u4f5c\u751f\u6210\uff0c\u80fd\u591f\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5b66\u4e60\u751f\u6210\u673a\u5668\u4eba\u884c\u4e3a\u89c6\u9891\u4f7f\u5f97\u80fd\u591f\u63d0\u53d6\u51fa\u5177\u6709\u6700\u5c0f\u6f14\u793a\u6570\u636e\u7684\u7b56\u7565\uff0c\u9c81\u68d2\u6027\u548c\u6837\u672c\u6548\u7387\u5f97\u5230\u663e\u8457\u63d0\u9ad8\u3002\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u90fd\u8868\u73b0\u51fa\u5bf9\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u3001\u80cc\u666f\u548c\u4efb\u52a1\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002\u4efb\u52a1\u6210\u529f\u4e0e\u751f\u6210\u7684\u89c6\u9891\u7d27\u5bc6\u76f8\u5173\uff0c\u5e76\u4e14\u65e0\u52a8\u4f5c\u89c6\u9891\u6570\u636e\u5bf9\u4e8e\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\u5177\u6709\u5173\u952e\u4f18\u52bf\u3002\u4e0e\u4f20\u7edf\u7684\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u751f\u6210\u673a\u5668\u4eba\u884c\u4e3a\u89c6\u9891\uff0c\u80fd\u591f\u4ee5\u6781\u5c11\u91cf\u7684\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u63d0\u53d6\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u6837\u672c\u6548\u7387\uff0c\u5e76\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u5c55\u73b0\u51fa\u5bf9\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u3001\u80cc\u666f\u548c\u4efb\u52a1\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.00272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00272", "abs": "https://arxiv.org/abs/2508.00272", "authors": ["Wenyue Chong"], "title": "Towards Robust Semantic Correspondence: A Benchmark and Insights", "comment": null, "summary": "Semantic correspondence aims to identify semantically meaningful\nrelationships between different images and is a fundamental challenge in\ncomputer vision. It forms the foundation for numerous tasks such as 3D\nreconstruction, object tracking, and image editing. With the progress of\nlarge-scale vision models, semantic correspondence has achieved remarkable\nperformance in controlled and high-quality conditions. However, the robustness\nof semantic correspondence in challenging scenarios is much less investigated.\nIn this work, we establish a novel benchmark for evaluating semantic\ncorrespondence in adverse conditions. The benchmark dataset comprises 14\ndistinct challenging scenarios that reflect commonly encountered imaging\nissues, including geometric distortion, image blurring, digital artifacts, and\nenvironmental occlusion. Through extensive evaluations, we provide several key\ninsights into the robustness of semantic correspondence approaches: (1) All\nexisting methods suffer from noticeable performance drops under adverse\nconditions; (2) Using large-scale vision models can enhance overall robustness,\nbut fine-tuning on these models leads to a decline in relative robustness; (3)\nThe DINO model outperforms the Stable Diffusion in relative robustness, and\ntheir fusion achieves better absolute robustness; Moreover, We evaluate common\nrobustness enhancement strategies for semantic correspondence and find that\ngeneral data augmentations are ineffective, highlighting the need for\ntask-specific designs. These results are consistent across both our dataset and\nreal-world benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b14\u79cd\u6311\u6218\u6027\u573a\u666f\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u4e49\u5bf9\u5e94\u5728\u4e0d\u5229\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u65b9\u6cd5\u548c\u5fae\u8c03\u5927\u578b\u6a21\u578b\u90fd\u4f1a\u964d\u4f4e\u9c81\u68d2\u6027\uff0cDINO\u4f18\u4e8eStable Diffusion\uff0c\u4e14\u9700\u8981\u9488\u5bf9\u6027\u8bbe\u8ba1\u9c81\u68d2\u6027\u589e\u5f3a\u7b56\u7565\u3002", "motivation": "\u8bed\u4e49\u5bf9\u5e94\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u5b83\u4e3a3D\u91cd\u5efa\u3001\u7269\u4f53\u8ddf\u8e2a\u548c\u56fe\u50cf\u7f16\u8f91\u7b49\u4efb\u52a1\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u5728\u53d7\u63a7\u548c\u9ad8\u8d28\u91cf\u7684\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u4e0d\u5229\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b14\u79cd\u4e0d\u540c\u7684\u6311\u6218\u6027\u573a\u666f\uff0c\u5982\u51e0\u4f55\u7578\u53d8\u3001\u56fe\u50cf\u6a21\u7cca\u3001\u6570\u5b57\u4f2a\u5f71\u548c\u73af\u5883\u906e\u6321\uff0c\u5e76\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u5bf9\u73b0\u6709\u7684\u8bed\u4e49\u5bf9\u5e94\u65b9\u6cd5\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u540c\u65f6\u8fd8\u8bc4\u4f30\u4e86\u5e38\u89c1\u7684\u9c81\u68d2\u6027\u589e\u5f3a\u7b56\u7565\u3002", "result": "\u73b0\u6709\u8bed\u4e49\u5bf9\u5e94\u65b9\u6cd5\u5728\u4e0d\u5229\u6761\u4ef6\u4e0b\u6027\u80fd\u5747\u6709\u663e\u8457\u4e0b\u964d\uff1b\u5fae\u8c03\u5927\u578b\u89c6\u89c9\u6a21\u578b\u4f1a\u964d\u4f4e\u5176\u76f8\u5bf9\u9c81\u68d2\u6027\uff1bDINO\u6a21\u578b\u6bd4Stable Diffusion\u5177\u6709\u66f4\u597d\u7684\u76f8\u5bf9\u9c81\u68d2\u6027\uff0c\u878d\u5408\u4e24\u8005\u53ef\u63d0\u9ad8\u7edd\u5bf9\u9c81\u68d2\u6027\uff1b\u901a\u7528\u6570\u636e\u589e\u5f3a\u5bf9\u63d0\u5347\u8bed\u4e49\u5bf9\u5e94\u9c81\u68d2\u6027\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "\u73b0\u6709\u65b9\u6cd5\u5728\u4e0d\u5229\u6761\u4ef6\u4e0b\u90fd\u4f1a\u51fa\u73b0\u660e\u663e\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u867d\u7136\u5927\u578b\u89c6\u89c9\u6a21\u578b\u53ef\u4ee5\u63d0\u9ad8\u6574\u4f53\u9c81\u68d2\u6027\uff0c\u4f46\u5fae\u8c03\u4f1a\u964d\u4f4e\u76f8\u5bf9\u9c81\u68d2\u6027\u3002DINO\u6a21\u578b\u5728\u76f8\u5bf9\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8eStable Diffusion\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u878d\u5408\u5728\u7edd\u5bf9\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u901a\u7528\u7684\u6570\u636e\u589e\u5f3a\u5bf9\u4e8e\u63d0\u9ad8\u8bed\u4e49\u5bf9\u5e94\u9c81\u68d2\u6027\u662f\u65e0\u6548\u7684\uff0c\u9700\u8981\u8fdb\u884c\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2508.00578", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.chem-ph", "physics.comp-ph", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2508.00578", "abs": "https://arxiv.org/abs/2508.00578", "authors": ["Marlen Neubert", "Patrick Reiser", "Frauke Gr\u00e4ter", "Pascal Friederich"], "title": "Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides", "comment": "19 pages, 12 figures, and 4 tables (references and SI included)", "summary": "Hydrogen atom transfer (HAT) reactions are essential in many biological\nprocesses, such as radical migration in damaged proteins, but their mechanistic\npathways remain incompletely understood. Simulating HAT is challenging due to\nthe need for quantum chemical accuracy at biologically relevant scales; thus,\nneither classical force fields nor DFT-based molecular dynamics are applicable.\nMachine-learned potentials offer an alternative, able to learn potential energy\nsurfaces (PESs) with near-quantum accuracy. However, training these models to\ngeneralize across diverse HAT configurations, especially at radical positions\nin proteins, requires tailored data generation and careful model selection.\nHere, we systematically generate HAT configurations in peptides to build large\ndatasets using semiempirical methods and DFT. We benchmark three graph neural\nnetwork architectures (SchNet, Allegro, and MACE) on their ability to learn HAT\nPESs and indirectly predict reaction barriers from energy predictions. MACE\nconsistently outperforms the others in energy, force, and barrier prediction,\nachieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT\nbarrier predictions. This accuracy enables integration of ML potentials into\nlarge-scale collagen simulations to compute reaction rates from predicted\nbarriers, advancing mechanistic understanding of HAT and radical migration in\npeptides. We analyze scaling laws, model transferability, and cost-performance\ntrade-offs, and outline strategies for improvement by combining ML potentials\nwith transition state search algorithms and active learning. Our approach is\ngeneralizable to other biomolecular systems, enabling quantum-accurate\nsimulations of chemical reactivity in complex environments.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u673a\u5668\u5b66\u4e60\u52bf\u80fd\u6a21\u62df\u751f\u7269\u5206\u5b50\u4e2d\u7684\u6c22\u539f\u5b50\u8f6c\u79fb\u53cd\u5e94\uff0c\u5176\u4e2dMACE\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u7406\u89e3\u751f\u7269\u8fc7\u7a0b\u4e2d\u7684\u5316\u5b66\u53cd\u5e94\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002", "motivation": "\u6c22\u539f\u5b50\u8f6c\u79fb\uff08HAT\uff09\u53cd\u5e94\u5728\u8bb8\u591a\u751f\u7269\u8fc7\u7a0b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u673a\u7406\u5c1a\u4e0d\u5b8c\u5168\u6e05\u695a\uff0c\u5e76\u4e14\u5728\u751f\u7269\u76f8\u5173\u5c3a\u5ea6\u4e0a\u6a21\u62dfHAT\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u9700\u8981\u91cf\u5b50\u5316\u5b66\u7cbe\u5ea6\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u5730\u751f\u6210\u80bd\u4e2d\u7684\u6c22\u539f\u5b50\u8f6c\u79fb\uff08HAT\uff09\u6784\u578b\uff0c\u4f7f\u7528\u534a\u7ecf\u9a8c\u65b9\u6cd5\u548cDFT\u6784\u5efa\u5927\u578b\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u4e09\u79cd\u56fe\u795e\u7ecf\u7f51\u7edc\uff08SchNet\u3001Allegro\u548cMACE\uff09\u5b66\u4e60HAT\u52bf\u80fd\u9762\u7684\u80fd\u529b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u95f4\u63a5\u9884\u6d4b\u53cd\u5e94\u52bf\u5792\u3002", "result": "MACE\u5728\u80fd\u91cf\u3001\u529b\u548c\u52bf\u5792\u9884\u6d4b\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5728\u9884\u6d4b\u5206\u5e03\u5916\u7684DFT\u52bf\u5792\u65b9\u9762\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.13 kcal/mol\u3002\u8be5\u7cbe\u5ea6\u80fd\u591f\u5c06\u673a\u5668\u5b66\u4e60\u52bf\u80fd\u96c6\u6210\u5230\u5927\u89c4\u6a21\u80f6\u539f\u86cb\u767d\u6a21\u62df\u4e2d\uff0c\u901a\u8fc7\u9884\u6d4b\u7684\u52bf\u5792\u8ba1\u7b97\u53cd\u5e94\u901f\u7387\uff0c\u4ece\u800c\u589e\u8fdb\u5bf9\u80bd\u4e2dHAT\u548c\u81ea\u7531\u57fa\u8fc1\u79fb\u673a\u7406\u7684\u7406\u89e3\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u52bf\u80fd\u53ef\u4ee5\u4e0e\u8fc7\u6e21\u6001\u641c\u7d22\u7b97\u6cd5\u548c\u4e3b\u52a8\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u4ee5\u8fdb\u4e00\u6b65\u6539\u8fdb\u65b9\u6cd5\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u5176\u4ed6\u751f\u7269\u5206\u5b50\u7cfb\u7edf\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u590d\u6742\u73af\u5883\u4e2d\u5316\u5b66\u53cd\u5e94\u6027\u7684\u91cf\u5b50\u7cbe\u5ea6\u6a21\u62df\u3002"}}
{"id": "2508.00180", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.00180", "abs": "https://arxiv.org/abs/2508.00180", "authors": ["Adam Block", "Cyril Zhang"], "title": "EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes", "comment": null, "summary": "Stochasticity in language model fine-tuning, often caused by the small batch\nsizes typically used in this regime, can destabilize training by introducing\nlarge oscillations in generation quality. A popular approach to mitigating this\ninstability is to take an Exponential moving average (EMA) of weights\nthroughout training. While EMA reduces stochasticity, thereby smoothing\ntraining, the introduction of bias from old iterates often creates a lag in\noptimization relative to vanilla training. In this work, we propose the\nBias-Corrected Exponential Moving Average (BEMA), a simple and practical\naugmentation of EMA that retains variance-reduction benefits while eliminating\nbias. BEMA is motivated by a simple theoretical model wherein we demonstrate\nprovable acceleration of BEMA over both a standard EMA and vanilla training.\nThrough an extensive suite of experiments on Language Models, we show that BEMA\nleads to significantly improved convergence rates and final performance over\nboth EMA and vanilla training in a variety of standard LM benchmarks, making\nBEMA a practical and theoretically motivated intervention for more stable and\nefficient fine-tuning.", "AI": {"tldr": "BEMA \u901a\u8fc7\u6d88\u9664\u504f\u5dee\u6765\u6539\u8fdb EMA\uff0c\u4ece\u800c\u5728\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u5b9e\u73b0\u66f4\u7a33\u5b9a\u3001\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u968f\u673a\u6027\uff08\u901a\u5e38\u7531\u5c0f\u6279\u91cf\u5927\u5c0f\u5f15\u8d77\uff09\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u751f\u6210\u8d28\u91cf\u7684\u5de8\u5927\u6ce2\u52a8\u3002\u6807\u51c6\u7684 EMA \u65b9\u6cd5\u53ef\u4ee5\u7f13\u89e3\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\uff0c\u4f46\u4f1a\u5f15\u5165\u504f\u5dee\uff0c\u4ece\u800c\u5728\u4f18\u5316\u65b9\u9762\u843d\u540e\u4e8e\u6807\u51c6\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u504f\u5dee\u4fee\u6b63\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08BEMA\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6d88\u9664\u65e7\u8fed\u4ee3\u5f15\u5165\u7684\u504f\u5dee\u6765\u589e\u5f3a\u6807\u51c6\u7684 EMA\uff0c\u4ece\u800c\u5728\u4e0d\u727a\u7272\u65b9\u5dee\u7f29\u51cf\u6548\u76ca\u7684\u60c5\u51b5\u4e0b\u52a0\u901f\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u5728\u591a\u79cd\u6807\u51c6\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cBEMA \u5728\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e EMA \u548c\u6807\u51c6\u8bad\u7ec3\u3002", "conclusion": "BEMA\u662f\u4e00\u79cd\u7b80\u5355\u5b9e\u7528\u7684 EMA \u589e\u5f3a\u65b9\u6cd5\uff0c\u53ef\u4fdd\u7559\u65b9\u5dee\u7f29\u51cf\u7684\u4f18\u70b9\uff0c\u540c\u65f6\u6d88\u9664\u504f\u5dee\uff0c\u5e76\u5728\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u663e\u793a\u51fa\u6bd4 EMA \u548c\u6807\u51c6\u8bad\u7ec3\u660e\u663e\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u3002"}}
{"id": "2508.00390", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00390", "abs": "https://arxiv.org/abs/2508.00390", "authors": ["Hengxing Cai", "Jinhan Dong", "Yijie Rao", "Jingcheng Deng", "Jingjun Tan", "Qien Chen", "Haidong Wang", "Zhen Wang", "Shiyu Huang", "Agachai Sumalee", "Renxin Zhong"], "title": "SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation", "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable\nagents to accurately localize targets and plan flight paths in complex\nenvironments based on natural language instructions, with broad applications in\nintelligent inspection, disaster rescue, and urban monitoring. Recent progress\nin Vision-Language Models (VLMs) has provided strong semantic understanding for\nthis task, while reinforcement learning (RL) has emerged as a promising\npost-training strategy to further improve generalization. However, existing RL\nmethods often suffer from inefficient use of training data, slow convergence,\nand insufficient consideration of the difficulty variation among training\nsamples, which limits further performance improvement. To address these\nchallenges, we propose \\textbf{Semantic-Aware Gaussian Curriculum Scheduling\n(SA-GCS)}, a novel training framework that systematically integrates Curriculum\nLearning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator\n(SA-DE) to quantify the complexity of training samples and a Gaussian\nCurriculum Scheduler (GCS) to dynamically adjust the sampling distribution,\nenabling a smooth progression from easy to challenging tasks. This design\nsignificantly improves training efficiency, accelerates convergence, and\nenhances overall model performance. Extensive experiments on the CityNav\nbenchmark demonstrate that SA-GCS consistently outperforms strong baselines\nacross all metrics, achieves faster and more stable convergence, and\ngeneralizes well across models of different scales, highlighting its robustness\nand scalability. The implementation of our approach is publicly available.", "AI": {"tldr": "SA-GCS \u901a\u8fc7\u5c06\u8bfe\u7a0b\u5b66\u4e60\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709 RL \u65b9\u6cd5\u5728\u6570\u636e\u5229\u7528\u3001\u6536\u655b\u901f\u5ea6\u548c\u6837\u672c\u96be\u5ea6\u5904\u7406\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u5728\u5229\u7528\u8bad\u7ec3\u6570\u636e\u3001\u6536\u655b\u901f\u5ea6\u548c\u6837\u672c\u96be\u5ea6\u53d8\u5316\u8003\u8651\u4e0d\u8db3\u65b9\u9762\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u3001\u6536\u655b\u7f13\u6162\u548c\u8bad\u7ec3\u6837\u672c\u96be\u5ea6\u53d8\u5316\u8003\u8651\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bed\u4e49\u611f\u77e5\u9ad8\u65af\u8bfe\u7a0b\u8c03\u5ea6 (SA-GCS)\u201d\u7684\u65b0\u9896\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u8bfe\u7a0b\u5b66\u4e60\uff08CL\uff09\u7cfb\u7edf\u5730\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u3002SA-GCS \u91c7\u7528\u8bed\u4e49\u611f\u77e5\u96be\u5ea6\u4f30\u8ba1\u5668\uff08SA-DE\uff09\u6765\u91cf\u5316\u8bad\u7ec3\u6837\u672c\u7684\u590d\u6742\u6027\uff0c\u5e76\u91c7\u7528\u9ad8\u65af\u8bfe\u7a0b\u8c03\u5ea6\u5668\uff08GCS\uff09\u6765\u52a8\u6001\u8c03\u6574\u91c7\u6837\u5206\u5e03\uff0c\u4ece\u800c\u5b9e\u73b0\u4ece\u7b80\u5355\u5230\u590d\u6742\u7684\u4efb\u52a1\u7684\u5e73\u6ed1\u8fdb\u5c55\u3002", "result": "SA-GCS \u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u52a0\u901f\u4e86\u6536\u655b\uff0c\u5e76\u589e\u5f3a\u4e86\u6574\u4f53\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "SA-GCS \u5728 CityNav \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u3001\u66f4\u7a33\u5b9a\u7684\u6536\u655b\uff0c\u5e76\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.00459", "categories": ["cs.AI", "68T07, 68T20", "I.2.6; I.2.7; I.2.3"], "pdf": "https://arxiv.org/pdf/2508.00459", "abs": "https://arxiv.org/abs/2508.00459", "authors": ["Andrea Asperti", "Alberto Naibo", "Claudio Sacerdoti Coen"], "title": "Thinking Machines: Mathematical Reasoning in the Age of LLMs", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable abilities in structured\nreasoning and symbolic tasks, with coding emerging as a particular area of\nstrength. This success has sparked growing interest in applying LLMs to\nmathematics, both in informal problem-solving and formal theorem proving.\nHowever, progress in formal mathematics has proven to be significantly more\ndifficult, despite surface-level similarities between programming and proof\nconstruction. This discrepancy raises important questions about how LLMs\n``reason'', how they are supervised, and whether they internally track a notion\nof computational or deductive state. In this article, we address the\nstate-of-the-art of the discipline, focusing on recent models and benchmarks,\nand explore three central issues at the intersection of machine learning and\nmathematical cognition: (i) the trade-offs between formal and informal\nmathematics as training domains; (ii) the deeper reasons why proof generation\nremains more brittle than code synthesis; (iii) and the question of whether\nLLMs represent, or merely mimic, a notion of evolving logical state. Our goal\nis not to draw hard boundaries, but to identify where the current limits lie,\nand how they might be extended.", "AI": {"tldr": "LLMs\u5728\u4ee3\u7801\u65b9\u9762\u5f88\u5f3a\uff0c\u4f46\u5728\u6570\u5b66\u8bc1\u660e\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u539f\u56e0\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u4ee3\u7801\u7b49\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6570\u5b66\u9886\u57df\uff0c\u5c24\u5176\u662f\u5728\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u65b9\u9762\uff0c\u8fdb\u5c55\u5374\u56f0\u96be\u5f97\u591a\u3002\u8fd9\u5f15\u53d1\u4e86\u5173\u4e8eLLMs\u7684\u2018\u63a8\u7406\u2019\u65b9\u5f0f\u3001\u76d1\u7763\u673a\u5236\u4ee5\u53ca\u5b83\u4eec\u662f\u5426\u80fd\u5185\u90e8\u8ffd\u8e2a\u8ba1\u7b97\u6216\u6f14\u7ece\u72b6\u6001\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u672c\u6587\u68b3\u7406\u4e86LLMs\u5728\u6570\u5b66\u9886\u57df\uff08\u5305\u62ec\u975e\u5f62\u5f0f\u5316\u95ee\u9898\u89e3\u51b3\u548c\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\uff09\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u6a21\u578b\u3001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f62\u5f0f\u5316\u4e0e\u975e\u5f62\u5f0f\u5316\u6570\u5b66\u4f5c\u4e3a\u8bad\u7ec3\u57df\u7684\u6743\u8861\u3001\u8bc1\u660e\u751f\u6210\u6bd4\u4ee3\u7801\u5408\u6210\u66f4\u8106\u5f31\u7684\u539f\u56e0\uff0c\u4ee5\u53caLLMs\u662f\u5426\u771f\u6b63\u6a21\u62df\u4e86\u903b\u8f91\u72b6\u6001\u7684\u6f14\u53d8\u3002", "result": "\u6587\u7ae0\u65e8\u5728\u8bc6\u522b\u5f53\u524dLLMs\u5728\u6570\u5b66\u9886\u57df\u80fd\u529b\u7684\u8fb9\u754c\uff0c\u5e76\u63a2\u7d22\u6269\u5c55\u8fd9\u4e9b\u80fd\u529b\u7684\u9014\u5f84\uff0c\u800c\u975e\u8bbe\u5b9a\u786c\u6027\u754c\u9650\u3002", "conclusion": "LLMs\u5728\u6570\u5b66\u9886\u57df\u7684\u5e94\u7528\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u5728\u5f62\u5f0f\u5316\u8bc1\u660e\u65b9\u9762\uff0c\u5176\u8868\u73b0\u4e0d\u5982\u4ee3\u7801\u751f\u6210\u7a33\u5b9a\u3002\u672a\u6765\u7684\u7814\u7a76\u9700\u8981\u6df1\u5165\u7406\u89e3LLMs\u7684\u2018\u63a8\u7406\u2019\u673a\u5236\uff0c\u4ee5\u53ca\u5b83\u4eec\u662f\u5426\u80fd\u771f\u6b63\u7406\u89e3\u548c\u7ef4\u62a4\u903b\u8f91\u72b6\u6001\u3002"}}
{"id": "2508.00301", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00301", "abs": "https://arxiv.org/abs/2508.00301", "authors": ["Kyoungho Cho", "Jeongho Bang"], "title": "Entangling Power and Its Deviation: A Quantitative Analysis on Input-State Dependence and Variability in Entanglement Generation", "comment": "24 pages, 2 figures", "summary": "Quantifying the entangling capability of quantum operations is a fundamental\ntask in quantum information science. Traditionally, this capability is measured\nby the entangling power (EP), defined as the average entanglement generated\nwhen a quantum operation acts uniformly on all possible product states.\nHowever, EP alone cannot capture the intricate input-state-dependent nature of\nentanglement generation. To address this, we define a complementary metric --\nentangling power deviation (EPD) -- as the standard deviation of entanglement\ngenerated over all product input states, thereby capturing the multifaceted\nnature of entangling behavior. We develop a general group-theoretical framework\nthat yields closed-form expressions for both EP and EPD. Our analysis reveals a\nfundamental and previously unexplored physics: enhancing entangling capability\ninevitably increases sensitivity, or bias, toward specific input states. By\nanalyzing representative two-qubit gates, we show that the gates with identical\nEP can exhibit markedly different EPD values, illustrating that the nature of\nentanglement generation can significantly differ depending on physical\nimplementation. Extending our framework to a class of generalized\ncontrolled-unitary operations acting on bipartite Hilbert spaces of arbitrary\ndimensions, we (re)affirm the inherent trade-off between the entangling\nstrength and uniformity. Moreover, we uncover a subtle\ndimension-parity-dependent behavior in entanglement generation, which EP alone\nfails to detect. These findings highlight EPD as an indispensable diagnostic\ntool -- one that, alongside EP, provides a deeper and more complete\ncharacterization of the entangling structure.", "AI": {"tldr": "EPD\u4f5c\u4e3a\u7ea0\u7f20\u80fd\u529b\u7684\u8865\u5145\u5ea6\u91cf\uff0c\u63ed\u793a\u4e86\u7ea0\u7f20\u80fd\u529b\u4e0e\u72b6\u6001\u654f\u611f\u6027\u4e4b\u95f4\u7684\u6743\u8861\u4ee5\u53ca\u7ef4\u5ea6\u5947\u5076\u6027\u5bf9\u7ea0\u7f20\u751f\u6210\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u7ea0\u7f20\u80fd\u529b\uff08EP\uff09\u6307\u6807\u65e0\u6cd5\u6355\u6349\u7ea0\u7f20\u751f\u6210\u7684\u8f93\u5165\u72b6\u6001\u4f9d\u8d56\u6027\uff0c\u9700\u8981\u4e00\u4e2a\u8865\u5145\u6307\u6807\u6765\u5168\u9762\u8868\u5f81\u7ea0\u7f20\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u7ea0\u7f20\u80fd\u529b\u504f\u5dee\uff08EPD\uff09\u4f5c\u4e3a\u8861\u91cf\u6807\u51c6\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u7fa4\u8bba\u6846\u67b6\u6765\u63a8\u5bfcEP\u548cEPD\u7684\u95ed\u5408\u8868\u8fbe\u5f0f\uff0c\u5c06\u6846\u67b6\u63a8\u5e7f\u5230\u4efb\u610f\u7ef4\u5ea6\u4e8c\u5206\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7684\u5e7f\u4e49\u9149\u64cd\u4f5c\u3002", "result": "\u5f00\u53d1\u4e86EPD\u5ea6\u91cf\uff0c\u63ed\u793a\u4e86\u7ea0\u7f20\u80fd\u529b\u4e0e\u72b6\u6001\u654f\u611f\u6027\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\uff0c\u5e76\u53d1\u73b0\u76f8\u540c\u7684EP\u53ef\u80fd\u5bf9\u5e94\u4e0d\u540c\u7684EPD\uff0c\u8868\u660e\u7269\u7406\u5b9e\u73b0\u5f71\u54cd\u7ea0\u7f20\u751f\u6210\u6027\u8d28\uff1b\u53d1\u73b0\u7ef4\u5ea6\u5947\u5076\u6027\u4f9d\u8d56\u7684\u7ea0\u7f20\u751f\u6210\u884c\u4e3a\u3002", "conclusion": "EPD\u4e0eEP\u76f8\u8f85\u76f8\u6210\uff0c\u662f\u91cf\u5316\u91cf\u5b50\u64cd\u4f5c\u7ea0\u7f20\u80fd\u529b\u7684\u5fc5\u8981\u8865\u5145\uff0c\u63ed\u793a\u4e86\u589e\u5f3a\u7ea0\u7f20\u80fd\u529b\u4e0e\u72b6\u6001\u654f\u611f\u6027\u4e4b\u95f4\u7684\u56fa\u6709\u6743\u8861\uff0c\u5e76\u53d1\u73b0\u4e86\u7ef4\u5ea6\u5947\u5076\u6027\u4f9d\u8d56\u7684\u7ea0\u7f20\u751f\u6210\u884c\u4e3a\u3002"}}
{"id": "2508.00287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00287", "abs": "https://arxiv.org/abs/2508.00287", "authors": ["Tran Viet Khoa", "Do Hai Son", "Mohammad Abu Alsheikh", "Yibeltal F Alem", "Dinh Thai Hoang"], "title": "Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning", "comment": null, "summary": "Driver drowsiness is one of the main causes of road accidents and is\nrecognized as a leading contributor to traffic-related fatalities. However,\ndetecting drowsiness accurately remains a challenging task, especially in\nreal-world settings where facial data from different individuals is\ndecentralized and highly diverse. In this paper, we propose a novel framework\nfor drowsiness detection that is designed to work effectively with\nheterogeneous and decentralized data. Our approach develops a new Spatial\nSelf-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)\nnetwork to better extract key facial features and improve detection\nperformance. To support federated learning, we employ a Gradient Similarity\nComparison (GSC) that selects the most relevant trained models from different\noperators before aggregation. This improves the accuracy and robustness of the\nglobal model while preserving user privacy. We also develop a customized tool\nthat automatically processes video data by extracting frames, detecting and\ncropping faces, and applying data augmentation techniques such as rotation,\nflipping, brightness adjustment, and zooming. Experimental results show that\nour framework achieves a detection accuracy of 89.9% in the federated learning\nsettings, outperforming existing methods under various deployment scenarios.\nThe results demonstrate the effectiveness of our approach in handling\nreal-world data variability and highlight its potential for deployment in\nintelligent transportation systems to enhance road safety through early and\nreliable drowsiness detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7a7a\u95f4\u81ea\u6ce8\u610f\u529b\u548cLSTM\u4ee5\u53ca\u68af\u5ea6\u76f8\u4f3c\u6027\u6bd4\u8f83\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u5206\u6563\u3001\u591a\u6837\u5316\u7684\u9762\u90e8\u6570\u636e\u4ee5\u68c0\u6d4b\u9a7e\u9a76\u5458\u55dc\u7761\u3002\u8be5\u6846\u67b6\u5728\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u4e8689.9%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u4e86\u5176\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u9a7e\u9a76\u5458\u55dc\u7761\u662f\u5bfc\u81f4\u4ea4\u901a\u4e8b\u6545\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\uff0c\u4e5f\u662f\u9020\u6210\u9053\u8def\u6b7b\u4ea1\u7684\u7f6a\u9b41\u7978\u9996\u3002\u7136\u800c\uff0c\u51c6\u786e\u68c0\u6d4b\u55dc\u7761\u4ecd\u7136\u662f\u4e00\u9879\u8270\u5de8\u7684\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u6765\u81ea\u4e0d\u540c\u4e2a\u4f53\u7684\u9762\u90e8\u6570\u636e\u662f\u5206\u6563\u4e14\u9ad8\u5ea6\u591a\u6837\u5316\u7684\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u5904\u7406\u5f02\u6784\u548c\u5206\u6563\u6570\u636e\u7684\u55dc\u7761\u68c0\u6d4b\u65b0\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u7a7a\u95f4\u81ea\u6ce8\u610f\u529b\uff08SSA\uff09\u673a\u5236\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\uff0c\u4ee5\u63d0\u53d6\u5173\u952e\u9762\u90e8\u7279\u5f81\u5e76\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002\u4e3a\u4e86\u652f\u6301\u8054\u90a6\u5b66\u4e60\uff0c\u91c7\u7528\u4e86\u68af\u5ea6\u76f8\u4f3c\u6027\u6bd4\u8f83\uff08GSC\uff09\u6765\u9009\u62e9\u6700\u76f8\u5173\u7684\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u805a\u5408\uff0c\u4ece\u800c\u63d0\u9ad8\u5168\u5c40\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u5b9a\u4e49\u5de5\u5177\u6765\u81ea\u52a8\u5904\u7406\u89c6\u9891\u6570\u636e\uff0c\u63d0\u53d6\u5e27\u3001\u68c0\u6d4b\u548c\u88c1\u526a\u9762\u90e8\uff0c\u5e76\u5e94\u7528\u65cb\u8f6c\u3001\u7ffb\u8f6c\u3001\u4eae\u5ea6\u8c03\u6574\u548c\u7f29\u653e\u7b49\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e8689.9%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5728\u5404\u79cd\u90e8\u7f72\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u5b9e\u73b0\u4e8689.9%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5728\u5404\u79cd\u90e8\u7f72\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u53d8\u5f02\u6027\u7684\u6709\u6548\u6027\uff0c\u5e76\u7a81\u663e\u4e86\u5176\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u901a\u8fc7\u65e9\u671f\u53ef\u9760\u7684\u55dc\u7761\u68c0\u6d4b\u6765\u63d0\u9ad8\u9053\u8def\u5b89\u5168\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00201", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00201", "abs": "https://arxiv.org/abs/2508.00201", "authors": ["Mehdi Ben Ayed", "Fei Feng", "Jay Adams", "Vishwakarma Singh", "Kritarth Anand", "Jiajing Xu"], "title": "RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems", "comment": null, "summary": "Existing web-scale recommendation systems commonly use supervised learning\nmethods that prioritize immediate user feedback. Although reinforcement\nlearning (RL) offers a solution to optimize longer-term goals, such as\nin-session engagement, applying it at web scale is challenging due to the\nextremely large action space and engineering complexity. In this paper, we\nintroduce RecoMind, a simulator-based RL framework designed for the effective\noptimization of session-based goals at web-scale. RecoMind leverages existing\nrecommendation models to establish a simulation environment and to bootstrap\nthe RL policy to optimize immediate user interactions from the outset. This\nmethod integrates well with existing industry pipelines, simplifying the\ntraining and deployment of RL policies. Additionally, RecoMind introduces a\ncustom exploration strategy to efficiently explore web-scale action spaces with\nhundreds of millions of items. We evaluated RecoMind through extensive offline\nsimulations and online A/B testing on a video streaming platform. Both methods\nshowed that the RL policy trained using RecoMind significantly outperforms\ntraditional supervised learning recommendation approaches in in-session user\nsatisfaction. In online A/B tests, the RL policy increased videos watched for\nmore than 10 seconds by 15.81\\% and improved session depth by 4.71\\% for\nsessions with at least 10 interactions. As a result, RecoMind presents a\nsystematic and scalable approach for embedding RL into web-scale recommendation\nsystems, showing great promise for optimizing session-based user satisfaction.", "AI": {"tldr": "RecoMind \u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u5b83\u5229\u7528\u6a21\u62df\u5668\u548c\u81ea\u5b9a\u4e49\u63a2\u7d22\u7b56\u7565\uff0c\u6210\u529f\u5730\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u4f1a\u8bdd\u6df1\u5ea6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7f51\u7edc\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u901a\u5e38\u4f7f\u7528\u4f18\u5148\u8003\u8651\u5373\u65f6\u7528\u6237\u53cd\u9988\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e3a\u4f18\u5316\u8bf8\u5982\u4f1a\u8bdd\u5185\u53c2\u4e0e\u5ea6\u7b49\u957f\u671f\u76ee\u6807\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7531\u4e8e\u5176\u6781\u5927\u7684\u52a8\u4f5c\u7a7a\u95f4\u548c\u5de5\u7a0b\u590d\u6742\u6027\uff0c\u5728\u7f51\u7edc\u89c4\u6a21\u4e0a\u5e94\u7528\u5b83\u5177\u6709\u6311\u6218\u6027\u3002", "method": "RecoMind\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u62df\u5668\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u73b0\u6709\u7684\u63a8\u8350\u6a21\u578b\u5efa\u7acb\u6a21\u62df\u73af\u5883\u5e76\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u4ece\u4e00\u5f00\u59cb\u5c31\u4f18\u5316\u5373\u65f6\u7528\u6237\u4e92\u52a8\u3002\u5b83\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u5b9a\u4e49\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u7528\u4e8e\u9ad8\u6548\u5730\u63a2\u7d22\u5305\u542b\u6570\u4ebf\u4e2a\u9879\u76ee\u7684\u7f51\u7edc\u89c4\u6a21\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "\u901a\u8fc7\u5728\u89c6\u9891\u6d41\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u7684\u79bb\u7ebf\u6a21\u62df\u548c\u5728\u7ebf A/B \u6d4b\u8bd5\u8bc4\u4f30\uff0cRecoMind \u8bad\u7ec3\u7684 RL \u7b56\u7565\u5728\u4f1a\u8bdd\u5185\u7528\u6237\u6ee1\u610f\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u76d1\u7763\u5b66\u4e60\u63a8\u8350\u65b9\u6cd5\u3002\u5728\u7ebf A/B \u6d4b\u8bd5\u663e\u793a\uff0cRL \u7b56\u7565\u5c06\u89c2\u770b\u8d85\u8fc7 10 \u79d2\u7684\u89c6\u9891\u6b21\u6570\u589e\u52a0\u4e86 15.81%\uff0c\u5e76\u5c06\u81f3\u5c11\u5305\u542b 10 \u6b21\u4e92\u52a8\u4f1a\u8bdd\u7684\u6df1\u5ea6\u63d0\u9ad8\u4e86 4.71%\u3002", "conclusion": "RecoMind\u662f\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u5f3a\u5316\u5b66\u4e60\u5d4c\u5165\u5230\u7f51\u7edc\u89c4\u6a21\u7684\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u5728\u4f18\u5316\u57fa\u4e8e\u4f1a\u8bdd\u7684\u7528\u6237\u6ee1\u610f\u5ea6\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.00420", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00420", "abs": "https://arxiv.org/abs/2508.00420", "authors": ["Rana Salama", "Abdou Youssef", "Mona Diab"], "title": "Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding", "comment": null, "summary": "Wavelets have emerged as a cutting edge technology in a number of fields.\nConcrete results of their application in Image and Signal processing suggest\nthat wavelets can be effectively applied to Natural Language Processing (NLP)\ntasks that capture a variety of linguistic properties. In this paper, we\nleverage the power of applying Discrete Wavelet Transforms (DWT) to word and\nsentence embeddings. We first evaluate, intrinsically and extrinsically, how\nwavelets can effectively be used to consolidate important information in a word\nvector while reducing its dimensionality. We further combine DWT with Discrete\nCosine Transform (DCT) to propose a non-parameterized model that compresses a\nsentence with a dense amount of information in a fixed size vector based on\nlocally varying word features. We show the efficacy of the proposed paradigm on\ndownstream applications models yielding comparable and even superior (in some\ntasks) results to original embeddings.", "AI": {"tldr": "Wavelets (DWT and DWT+DCT) are effective for NLP tasks, improving word embeddings and compressing sentences, with results comparable or better than existing methods.", "motivation": "To explore the application of wavelets, a cutting-edge technology in various fields, to Natural Language Processing (NLP) tasks by capturing linguistic properties in word and sentence embeddings.", "method": "Leveraging Discrete Wavelet Transforms (DWT) for word and sentence embeddings, and combining DWT with Discrete Cosine Transform (DCT) to create a non-parameterized sentence compression model.", "result": "The proposed paradigm shows efficacy in downstream applications, yielding comparable or even superior results to original embeddings on certain tasks.", "conclusion": "Wavelet transforms, particularly DWT, can effectively consolidate information in word embeddings, reduce dimensionality, and compress sentences into fixed-size vectors. Combining DWT with DCT yields a non-parameterized model that achieves comparable or superior results to original embeddings in downstream NLP tasks."}}
{"id": "2508.00500", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00500", "abs": "https://arxiv.org/abs/2508.00500", "authors": ["Haoyu Wang", "Chris M. Poskitt", "Jun Sun", "Jiali Wei"], "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking", "comment": null, "summary": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead.", "AI": {"tldr": "Pro2Guard\u901a\u8fc7\u6982\u7387\u53ef\u8fbe\u6027\u5206\u6790\u4e3b\u52a8\u89e3\u51b3LLM\u4ee3\u7406\u7684\u5b89\u5168\u98ce\u9669\uff0c\u901a\u8fc7\u5b66\u4e60DTMC\u9884\u6d4b\u98ce\u9669\u5e76\u63d0\u524d\u5e72\u9884\uff0c\u5728\u5177\u8eab\u4ee3\u7406\u548c\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5236\u7cfb\u7edf\uff08\u5982AgentSpec\uff09\u4e3b\u8981\u5173\u6ce8\u53cd\u5e94\u5f0f\u5b89\u5168\u89c4\u5219\uff0c\u5e76\u4e14\u5728\u5e94\u5bf9\u957f\u671f\u4f9d\u8d56\u6027\u548c\u5206\u5e03\u53d8\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002LLM\u4ee3\u7406\u7684\u968f\u673a\u884c\u4e3a\u5e26\u6765\u4e86\u96be\u4ee5\u9884\u6d4b\u7684\u91cd\u5927\u5b89\u5168\u98ce\u9669\u3002", "method": "Pro2Guard\u5c06\u4ee3\u7406\u884c\u4e3a\u62bd\u8c61\u4e3a\u7b26\u53f7\u72b6\u6001\uff0c\u5e76\u4ece\u6267\u884c\u8ddf\u8e2a\u4e2d\u5b66\u4e60\u79bb\u6563\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\uff08DTMC\uff09\u3002\u5728\u8fd0\u884c\u65f6\uff0c\u5b83\u901a\u8fc7\u4f30\u8ba1\u5230\u8fbe\u4e0d\u5b89\u5168\u72b6\u6001\u7684\u6982\u7387\u6765\u9884\u6d4b\u672a\u6765\u98ce\u9669\uff0c\u5e76\u5728\u9884\u6d4b\u98ce\u9669\u8d85\u8fc7\u7528\u6237\u5b9a\u4e49\u7684\u9608\u503c\u65f6\u89e6\u53d1\u5e72\u9884\u3002", "result": "\u5728\u5177\u8eab\u4ee3\u7406\u4efb\u52a1\u4e2d\uff0cPro2Guard\u5728\u4f7f\u7528\u4f4e\u9608\u503c\u65f6\uff0c\u53ef\u63d0\u524d\u5f3a\u5236\u6267\u884c\u9ad8\u8fbe93.6%\u7684\u4e0d\u5b89\u5168\u4efb\u52a1\uff0c\u5e76\u80fd\u901a\u8fc7\u53ef\u914d\u7f6e\u6a21\u5f0f\uff08\u5982reflect\uff09\u5728\u4fdd\u6301\u9ad8\u8fbe80.4%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u7684\u540c\u65f6\u5e73\u8861\u5b89\u5168\u4e0e\u4efb\u52a1\u6210\u529f\u3002\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\uff0cPro2Guard\u5b9e\u73b0\u4e86100%\u7684\u4ea4\u901a\u6cd5\u89c4\u548c\u78b0\u649e\u9884\u6d4b\uff0c\u53ef\u63d0\u524d38.66\u79d2\u9884\u6d4b\u98ce\u9669\u3002", "conclusion": "Pro2Guard\u662f\u4e00\u4e2a\u4e3b\u52a8\u8fd0\u884c\u65f6\u5f3a\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u53ef\u8fbe\u6027\u5206\u6790\u6765\u89e3\u51b3LLM\u4ee3\u7406\u7684\u968f\u673a\u884c\u4e3a\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\u3002\u5b83\u901a\u8fc7\u5b66\u4e60\u79bb\u6563\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\uff08DTMC\uff09\u6765\u9884\u6d4b\u672a\u6765\u98ce\u9669\uff0c\u5e76\u5728\u8d85\u51fa\u7528\u6237\u5b9a\u4e49\u7684\u9608\u503c\u65f6\u89e6\u53d1\u5e72\u9884\uff0c\u4ece\u800c\u5728\u8fdd\u89c4\u53d1\u751f\u4e4b\u524d\u8fdb\u884c\u5e72\u9884\u3002"}}
{"id": "2508.00320", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00320", "abs": "https://arxiv.org/abs/2508.00320", "authors": ["Asif Zaman", "Muhammad Faryad", "Adam Zaman Chaudhry"], "title": "Enhancement of non-Markovianity due to environment-induced indirect interaction", "comment": "12 pages, comments welcome", "summary": "Non-Markovian effects are often significant when the system-environment\ncoupling is not weak. Indeed, we find the non-Markovianity to be negligible for\na single two-level system undergoing pure dephasing via interaction with a\nharmonic oscillator environment. In this paper, we examine a natural extension,\nnamely a pure dephasing model where a collection of two-level systems interacts\nwith a common environment. We obtain analytically the dynamics of the\ncollection of the two-level systems, and then take a partial trace over all the\ntwo-level systems except one. This remaining single two-level system is shown\nto display markedly non-Markovian dynamics, even in the weak system-environment\ncoupling regime. This is due to the indirect interaction between two-level\nsystems induced by their interaction with the common environment. In fact, this\nindirect interaction can not only increase the non-Markovianity by orders of\nmagnitude, but also display qualitatively different characteristics. For\ninstance, for a single two-level system undergoing pure dephasing, the dynamics\nare Markovian for Ohmic and sub-Ohmic environments. This is markedly not the\ncase when we consider multiple two-level systems. We also show that the\nnon-Markovianity increases as we increase the number of two-level systems.\nThese findings provide insights into controlling decoherence in multi-qubit\nquantum systems and have implications for quantum technologies where\nnon-Markovianity can be a resource rather than a limitation.", "AI": {"tldr": "\u591a\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e0e\u5171\u540c\u73af\u5883\u76f8\u4e92\u4f5c\u7528\u65f6\uff0c\u53ef\u589e\u5f3a\u975e\u9a6c\u5c14\u53ef\u592b\u6548\u5e94\uff0c\u589e\u52a0\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u53ef\u8fdb\u4e00\u6b65\u589e\u5f3a\u6b64\u6548\u5e94\uff0c\u8fd9\u5728\u91cf\u5b50\u6280\u672f\u4e2d\u6709\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u63a2\u7a76\u5728\u591a\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e2d\uff0c\u7531\u4e8e\u4e0e\u5171\u540c\u73af\u5883\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u975e\u9a6c\u5c14\u53ef\u592b\u6548\u5e94\u7684\u589e\u5f3a\u673a\u5236\u53ca\u5176\u4e0e\u5355\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u7684\u5bf9\u6bd4\u3002", "method": "\u901a\u8fc7\u89e3\u6790\u63a8\u5bfc\u591a\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u6f14\u5316\uff0c\u5e76\u5bf9\u9664\u4e00\u4e2a\u91cf\u5b50\u6bd4\u7279\u5916\u7684\u6240\u6709\u91cf\u5b50\u6bd4\u7279\u8fdb\u884c\u504f\u8ff9\uff0c\u7814\u7a76\u5269\u4f59\u5355\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\u3002", "result": "\u5373\u4f7f\u5728\u5f31\u8026\u5408\u533a\uff0c\u591a\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e5f\u8868\u73b0\u51fa\u663e\u8457\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\uff0c\u4e14\u975e\u9a6c\u5c14\u53ef\u592b\u6027\u968f\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u7684\u589e\u52a0\u800c\u589e\u5f3a\u3002\u8fd9\u4e0e\u5355\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u5728\u67d0\u4e9b\u73af\u5883\u4e0b\u7684\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\u884c\u4e3a\u5f62\u6210\u5bf9\u6bd4\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u591a\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u4e0e\u5171\u540c\u73af\u5883\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u975e\u9a6c\u5c14\u53ef\u592b\u6548\u5e94\uff0c\u751a\u81f3\u5728\u5f31\u8026\u5408\u533a\u4e5f\u80fd\u89c2\u5bdf\u5230\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u63a7\u5236\u591a\u91cf\u5b50\u6bd4\u7279\u9000\u76f8\u5e72\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u53ef\u80fd\u5728\u91cf\u5b50\u6280\u672f\u4e2d\u5c06\u975e\u9a6c\u5c14\u53ef\u592b\u6027\u4f5c\u4e3a\u4e00\u79cd\u8d44\u6e90\u52a0\u4ee5\u5229\u7528\u3002"}}
{"id": "2508.00289", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00289", "abs": "https://arxiv.org/abs/2508.00289", "authors": ["Christian Simon", "Masato Ishii", "Akio Hayakawa", "Zhi Zhong", "Shusuke Takahashi", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models", "comment": "Accepted to ICCV 2025", "summary": "In the recent development of conditional diffusion models still require heavy\nsupervised fine-tuning for performing control on a category of tasks.\nTraining-free conditioning via guidance with off-the-shelf models is a\nfavorable alternative to avoid further fine-tuning on the base model. However,\nthe existing training-free guidance frameworks either have heavy memory\nrequirements or offer sub-optimal control due to rough estimation. These\nshortcomings limit the applicability to control diffusion models that require\nintense computation, such as Text-to-Video (T2V) diffusion models. In this\nwork, we propose Taming Inference Time Alignment for Guided Text-to-Video\nDiffusion Model, so-called TITAN-Guide, which overcomes memory space issues,\nand provides more optimal control in the guidance process compared to the\ncounterparts. In particular, we develop an efficient method for optimizing\ndiffusion latents without backpropagation from a discriminative guiding model.\nIn particular, we study forward gradient descents for guided diffusion tasks\nwith various options on directional directives. In our experiments, we\ndemonstrate the effectiveness of our approach in efficiently managing memory\nduring latent optimization, while previous methods fall short. Our proposed\napproach not only minimizes memory requirements but also significantly enhances\nT2V performance across a range of diffusion guidance benchmarks. Code, models,\nand demo are available at https://titanguide.github.io.", "AI": {"tldr": "TITAN-Guide \u662f\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6269\u6563\u6f5c\u5728\u53d8\u91cf\u6765\u6539\u8fdb\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u6269\u6563\u6a21\u578b\u7684\u5f15\u5bfc\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5185\u5b58\u548c\u63a7\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u514d\u8bad\u7ec3\u5f15\u5bfc\u6846\u67b6\u8981\u4e48\u5185\u5b58\u9700\u6c42\u5927\uff0c\u8981\u4e48\u63a7\u5236\u4e0d\u4f73\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u6269\u6563\u6a21\u578b\u7b49\u9ad8\u8ba1\u7b97\u91cf\u63a7\u5236\u6269\u6563\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a TITAN-Guide \u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u524d\u5411\u68af\u5ea6\u4e0b\u964d\u548c\u5404\u79cd\u65b9\u5411\u6307\u4ee4\u6765\u4f18\u5316\u6269\u6563\u6f5c\u5728\u53d8\u91cf\uff0c\u4ee5\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u5f15\u5bfc\u3002", "result": "TITAN-Guide \u5728\u5185\u5b58\u7ba1\u7406\u548c T2V \u6027\u80fd\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u5728\u5404\u79cd\u6269\u6563\u5f15\u5bfc\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u8fd0\u884c\u3002", "conclusion": "TITAN-Guide \u514b\u670d\u4e86\u5185\u5b58\u7a7a\u95f4\u95ee\u9898\uff0c\u5e76\u5728\u5f15\u5bfc\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u4f18\u7684\u63a7\u5236\u6548\u679c\u3002\u901a\u8fc7\u4f18\u5316\u6269\u6563\u6f5c\u5728\u53d8\u91cf\u800c\u4e0d\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5185\u5b58\u9700\u6c42\u3002"}}
{"id": "2508.00576", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00576", "abs": "https://arxiv.org/abs/2508.00576", "authors": ["Zhanliang Wang", "Kai Wang"], "title": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models", "comment": null, "summary": "Multimodal AI models have achieved impressive performance in tasks that\nrequire integrating information from multiple modalities, such as vision and\nlanguage. However, their \"black-box\" nature poses a major barrier to deployment\nin high-stakes applications where interpretability and trustworthiness are\nessential. How to explain cross-modal interactions in multimodal AI models\nremains a major challenge. While existing model explanation methods, such as\nattention map and Grad-CAM, offer coarse insights into cross-modal\nrelationships, they cannot precisely quantify the synergistic effects between\nmodalities, and are limited to open-source models with accessible internal\nweights. Here we introduce MultiSHAP, a model-agnostic interpretability\nframework that leverages the Shapley Interaction Index to attribute multimodal\npredictions to pairwise interactions between fine-grained visual and textual\nelements (such as image patches and text tokens), while being applicable to\nboth open- and closed-source models. Our approach provides: (1) instance-level\nexplanations that reveal synergistic and suppressive cross-modal effects for\nindividual samples - \"why the model makes a specific prediction on this input\",\nand (2) dataset-level explanation that uncovers generalizable interaction\npatterns across samples - \"how the model integrates information across\nmodalities\". Experiments on public multimodal benchmarks confirm that MultiSHAP\nfaithfully captures cross-modal reasoning mechanisms, while real-world case\nstudies demonstrate its practical utility. Our framework is extensible beyond\ntwo modalities, offering a general solution for interpreting complex multimodal\nAI models.", "AI": {"tldr": "MultiSHAP\u662f\u4e00\u79cd\u65b0\u7684\u89e3\u91ca\u6846\u67b6\uff0c\u7528\u4e8e\u7406\u89e3\u591a\u6a21\u6001AI\u6a21\u578b\u5982\u4f55\u7ed3\u5408\u4e0d\u540c\u6765\u6e90\u7684\u4fe1\u606f\u3002\u5b83\u80fd\u7cbe\u786e\u5730\u89e3\u91ca\u6a21\u578b\u4e3a\u4f55\u505a\u51fa\u7279\u5b9a\u9884\u6d4b\uff0c\u5e76\u63ed\u793a\u8de8\u6a21\u578b\u7684\u901a\u7528\u4fe1\u606f\u6574\u5408\u6a21\u5f0f\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001AI\u6a21\u578b\u5177\u6709\u201c\u9ed1\u7bb1\u201d\u6027\u8d28\uff0c\u96be\u4ee5\u89e3\u91ca\u5176\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u8fd9\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u963b\u788d\u4e86\u5176\u90e8\u7f72\u3002\u73b0\u6709\u7684\u89e3\u91ca\u65b9\u6cd5\uff08\u5982\u6ce8\u610f\u529b\u56fe\u548cGrad-CAM\uff09\u53ea\u80fd\u63d0\u4f9b\u7c97\u7565\u7684\u89c1\u89e3\uff0c\u65e0\u6cd5\u7cbe\u786e\u91cf\u5316\u6a21\u6001\u95f4\u7684\u534f\u540c\u6548\u5e94\uff0c\u5e76\u4e14\u5c40\u9650\u4e8e\u9700\u8981\u8bbf\u95ee\u5185\u90e8\u6743\u91cd\u7684\u5f00\u6e90\u6a21\u578b\u3002", "method": "\u63d0\u51faMultiSHAP\u6846\u67b6\uff0c\u5229\u7528Shapley\u4ea4\u4e92\u6307\u6570\u91cf\u5316\u7ec6\u7c92\u5ea6\u89c6\u89c9\u548c\u6587\u672c\u5143\u7d20\u4e4b\u95f4\u7684\u6210\u5bf9\u4ea4\u4e92\u4f5c\u7528\uff0c\u5b9e\u73b0\u6a21\u578b\u65e0\u5173\u7684\u89e3\u91ca\uff0c\u9002\u7528\u4e8e\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u3002", "result": "MultiSHAP\u80fd\u591f\u7cbe\u786e\u91cf\u5316\u6a21\u6001\u95f4\u7684\u534f\u540c\u548c\u6291\u5236\u6548\u5e94\uff0c\u63ed\u793a\u5b9e\u4f8b\u7ea7\u548c\u6570\u636e\u96c6\u7ea7\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u6a21\u5f0f\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u5fe0\u5b9e\u6355\u6349\u8de8\u6a21\u6001\u63a8\u7406\u673a\u5236\uff0c\u5e76\u5728\u5b9e\u9645\u6848\u4f8b\u4e2d\u5c55\u73b0\u5176\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "MultiSHAP\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u89e3\u91ca\u6846\u67b6\uff0c\u5229\u7528Shapley\u4ea4\u4e92\u6307\u6570\u6765\u5f52\u56e0\u591a\u6a21\u6001\u9884\u6d4b\u4e8e\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u548c\u6587\u672c\u5143\u7d20\u4e4b\u95f4\u7684\u6210\u5bf9\u4ea4\u4e92\uff0c\u5e76\u4e14\u9002\u7528\u4e8e\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u4f8b\u7ea7\u89e3\u91ca\uff08\u63ed\u793a\u7279\u5b9a\u8f93\u5165\u7684\u8de8\u6a21\u6001\u6548\u5e94\uff09\u548c\u6570\u636e\u96c6\u7ea7\u89e3\u91ca\uff08\u63ed\u793a\u8de8\u6837\u672c\u7684\u901a\u7528\u4ea4\u4e92\u6a21\u5f0f\uff09\u3002\u5b9e\u9a8c\u548c\u6848\u4f8b\u7814\u7a76\u8868\u660eMultiSHAP\u80fd\u591f\u51c6\u786e\u6355\u6349\u8de8\u6a21\u6001\u63a8\u7406\u673a\u5236\uff0c\u5e76\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e14\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u81f3\u591a\u4e8e\u4e24\u79cd\u6a21\u6001\u3002"}}
{"id": "2508.00334", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00334", "abs": "https://arxiv.org/abs/2508.00334", "authors": ["Nishaant Jacobus", "Paul Brumer", "Chern Chuang"], "title": "Mixed State Entanglement Via the Cauchy-Schwarz Inequality", "comment": "16 pages, 5 figures", "summary": "The entanglement properties of mixed states are of great importance in the\nstudy of open quantum systems and quantum information science, but commonly\nused entanglement measures, such as negativity, can be difficult to apply or\nconnect to physical properties of the system. We introduce the Cauchy-Schwarz\nViolation (CSV) Condition, which has a simple dependence on the populations and\ncoherences of the density operator. A sufficient condition for entanglement, it\nprovides a more direct connection to the physical characteristics of the system\nsuch as its symmetries. We illustrate the often surprising insights gained from\nthe CSV condition by applying it to the Jaynes-Cummings Model, the Quantum Rabi\nModel, and an open-system Quantum Rabi Model.", "AI": {"tldr": "CSV\u6761\u4ef6\u662f\u4e00\u79cd\u65b0\u7684\u7ea0\u7f20\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5b83\u6bd4\u73b0\u6709\u7684\u65b9\u6cd5\u66f4\u6613\u4e8e\u5e94\u7528\uff0c\u5e76\u4e14\u4e0e\u7cfb\u7edf\u7684\u7269\u7406\u6027\u8d28\u6709\u66f4\u76f4\u63a5\u7684\u8054\u7cfb\u3002", "motivation": "\u73b0\u6709\u7684\u7ea0\u7f20\u5ea6\u91cf\u65b9\u6cd5\uff08\u5982\u8d1f\u71b5\uff09\u5728\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u548c\u91cf\u5b50\u4fe1\u606f\u79d1\u5b66\u4e2d\u96be\u4ee5\u5e94\u7528\u6216\u4e0e\u7269\u7406\u6027\u8d28\u8054\u7cfb\u8d77\u6765\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u3001\u66f4\u6613\u4e8e\u5e94\u7528\u4e14\u4e0e\u7269\u7406\u6027\u8d28\u8054\u7cfb\u66f4\u7d27\u5bc6\u7684\u7ea0\u7f20\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "CSV\u6761\u4ef6\u662f\u4e00\u79cd\u65b0\u7684\u7ea0\u7f20\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5b83\u57fa\u4e8e\u5bc6\u5ea6\u7b97\u7b26\u7684\u5e03\u5c45\u6570\u548c\u76f8\u5e72\u6027\uff0c\u5177\u6709\u7b80\u5355\u7684\u6570\u5b66\u4f9d\u8d56\u6027\u3002", "result": "CSV\u6761\u4ef6\u5728Jaynes-Cummings\u6a21\u578b\u3001\u91cf\u5b50Rabi\u6a21\u578b\u548c\u5f00\u653e\u7cfb\u7edf\u91cf\u5b50Rabi\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u63ed\u793a\u4e86\u5176\u5728\u7406\u89e3\u548c\u5206\u6790\u8fd9\u4e9b\u6a21\u578b\u4e2d\u7684\u7ea0\u7f20\u6027\u8d28\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "CSV\u6761\u4ef6\u662f\u4e00\u79cd\u5145\u5206\u6761\u4ef6\uff0c\u7528\u4e8e\u5224\u65ad\u6df7\u5408\u6001\u662f\u5426\u5177\u6709\u7ea0\u7f20\u6027\uff0c\u5e76\u4e14\u4e0e\u7cfb\u7edf\u7684\u7269\u7406\u7279\u6027\uff08\u5982\u5bf9\u79f0\u6027\uff09\u6709\u66f4\u76f4\u63a5\u7684\u8054\u7cfb\u3002"}}
{"id": "2508.00299", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00299", "abs": "https://arxiv.org/abs/2508.00299", "authors": ["Danzhen Fu", "Jiagao Hu", "Daiguo Zhou", "Fei Wang", "Zepeng Wang", "Wenhua Liao"], "title": "Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence", "comment": "ICCV 2025 Workshop (HiGen)", "summary": "Pedestrian detection models in autonomous driving systems often lack\nrobustness due to insufficient representation of dangerous pedestrian scenarios\nin training datasets. To address this limitation, we present a novel framework\nfor controllable pedestrian video editing in multi-view driving scenarios by\nintegrating video inpainting and human motion control techniques. Our approach\nbegins by identifying pedestrian regions of interest across multiple camera\nviews, expanding detection bounding boxes with a fixed ratio, and resizing and\nstitching these regions into a unified canvas while preserving cross-view\nspatial relationships. A binary mask is then applied to designate the editable\narea, within which pedestrian editing is guided by pose sequence control\nconditions. This enables flexible editing functionalities, including pedestrian\ninsertion, replacement, and removal. Extensive experiments demonstrate that our\nframework achieves high-quality pedestrian editing with strong visual realism,\nspatiotemporal coherence, and cross-view consistency. These results establish\nthe proposed method as a robust and versatile solution for multi-view\npedestrian video generation, with broad potential for applications in data\naugmentation and scenario simulation in autonomous driving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u89c6\u56fe\u573a\u666f\u4e0b\u53ef\u63a7\u884c\u4eba\u89c6\u9891\u7f16\u8f91\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u9891\u4fee\u590d\u548c\u4eba\u4f53\u8fd0\u52a8\u63a7\u5236\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u884c\u4eba\u68c0\u6d4b\u6a21\u578b\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u884c\u4eba\u68c0\u6d4b\u6a21\u578b\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u5371\u9669\u884c\u4eba\u573a\u666f\u8868\u793a\u4e0d\u8db3\uff0c\u5f80\u5f80\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u6574\u5408\u89c6\u9891\u4fee\u590d\u548c\u4eba\u4f53\u8fd0\u52a8\u63a7\u5236\u6280\u672f\uff0c\u9996\u5148\u8bc6\u522b\u591a\u6444\u50cf\u5934\u89c6\u56fe\u4e2d\u7684\u611f\u5174\u8da3\u884c\u4eba\u533a\u57df\uff0c\u4ee5\u56fa\u5b9a\u6bd4\u4f8b\u6269\u5c55\u68c0\u6d4b\u8fb9\u754c\u6846\uff0c\u5e76\u5728\u4fdd\u6301\u8de8\u89c6\u56fe\u7a7a\u95f4\u5173\u7cfb\u7684\u540c\u65f6\u5c06\u8fd9\u4e9b\u533a\u57df\u8c03\u6574\u5927\u5c0f\u5e76\u62fc\u63a5\u6210\u7edf\u4e00\u7684\u753b\u5e03\u3002\u7136\u540e\u5e94\u7528\u4e8c\u5143\u63a9\u7801\u6765\u6307\u5b9a\u53ef\u7f16\u8f91\u533a\u57df\uff0c\u5e76\u5728\u8be5\u533a\u57df\u5185\u901a\u8fc7\u59ff\u6001\u5e8f\u5217\u63a7\u5236\u6761\u4ef6\u6765\u6307\u5bfc\u884c\u4eba\u7f16\u8f91\uff0c\u4ece\u800c\u5b9e\u73b0\u7075\u6d3b\u7684\u7f16\u8f91\u529f\u80fd\uff0c\u5305\u62ec\u884c\u4eba\u63d2\u5165\u3001\u66ff\u6362\u548c\u5220\u9664\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u884c\u4eba\u7f16\u8f91\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u89c6\u89c9\u771f\u5b9e\u611f\u3001\u65f6\u7a7a\u8fde\u8d2f\u6027\u548c\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u884c\u4eba\u7f16\u8f91\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u89c6\u89c9\u771f\u5b9e\u611f\u3001\u65f6\u7a7a\u8fde\u8d2f\u6027\u548c\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u4e3a\u591a\u89c6\u56fe\u884c\u4eba\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u800c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u7684\u6570\u636e\u589e\u5f3a\u548c\u573a\u666f\u6a21\u62df\u65b9\u9762\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.00298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00298", "abs": "https://arxiv.org/abs/2508.00298", "authors": ["Jin Lyu", "Liang An", "Li Lin", "Pujin Cheng", "Yebin Liu", "Xiaoying Tang"], "title": "AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer", "comment": "arXiv admin note: substantial text overlap with arXiv:2412.00837", "summary": "In the era of foundation models, achieving a unified understanding of\ndifferent dynamic objects through a single network has the potential to empower\nstronger spatial intelligence. Moreover, accurate estimation of animal pose and\nshape across diverse species is essential for quantitative analysis in\nbiological research. However, this topic remains underexplored due to the\nlimited network capacity of previous methods and the scarcity of comprehensive\nmulti-species datasets. To address these limitations, we introduce AniMer+, an\nextended version of our scalable AniMer framework. In this paper, we focus on a\nunified approach for reconstructing mammals (mammalia) and birds (aves). A key\ninnovation of AniMer+ is its high-capacity, family-aware Vision Transformer\n(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture\npartitions network layers into taxa-specific components (for mammalia and aves)\nand taxa-shared components, enabling efficient learning of both distinct and\ncommon anatomical features within a single model. To overcome the critical\nshortage of 3D training data, especially for birds, we introduce a\ndiffusion-based conditional image generation pipeline. This pipeline produces\ntwo large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for\nbirds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for\nbirds, which is crucial for resolving single-view depth ambiguities. Trained on\nan aggregated collection of 41.3k mammalian and 12.4k avian images (combining\nreal and synthetic data), our method demonstrates superior performance over\nexisting approaches across a wide range of benchmarks, including the\nchallenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the\neffectiveness of both our novel network architecture and the generated\nsynthetic datasets in enhancing real-world application performance.", "AI": {"tldr": "AniMer+\u901a\u8fc7\u7ed3\u5408\u521b\u65b0\u7684ViT-MoE\u67b6\u6784\u548c\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u54fa\u4e73\u52a8\u7269\u548c\u9e1f\u7c7b\u7684\u7edf\u4e00\u59ff\u6001\u548c\u5f62\u72b6\u91cd\u5efa\uff0c\u5e76\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5728\u57fa\u7840\u6a21\u578b\u65f6\u4ee3\u5b9e\u73b0\u5bf9\u4e0d\u540c\u52a8\u6001\u5bf9\u8c61\u7684\u7edf\u4e00\u7406\u89e3\uff0c\u4ece\u800c\u589e\u5f3a\u7a7a\u95f4\u667a\u80fd\u3002\u51c6\u786e\u4f30\u8ba1\u8de8\u4e0d\u540c\u7269\u79cd\u7684\u52a8\u7269\u59ff\u6001\u548c\u5f62\u72b6\u5bf9\u4e8e\u751f\u7269\u5b66\u7814\u7a76\u4e2d\u7684\u5b9a\u91cf\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u7f51\u7edc\u5bb9\u91cf\u6709\u9650\u4ee5\u53ca\u5168\u9762\u7684\u591a\u7269\u79cd\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u8fd9\u65b9\u9762\u7684\u7814\u7a76\u5c1a\u5904\u4e8e\u63a2\u7d22\u9636\u6bb5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAniMer+\u7684\u6846\u67b6\uff0c\u5b83\u662fAniMer\u7684\u6269\u5c55\u7248\u672c\u3002AniMer+\u91c7\u7528\u4e86\u9ad8\u5bb9\u91cf\u3001\u5bb6\u65cf\u611f\u77e5\u7684Vision Transformer\uff08ViT\uff09\uff0c\u5e76\u7ed3\u5408\u4e86Mixture-of-Experts\uff08MoE\uff09\u8bbe\u8ba1\uff0c\u5c06\u7f51\u7edc\u5c42\u5212\u5206\u4e3a\u7279\u5b9a\u7c7b\u7fa4\uff08\u54fa\u4e73\u52a8\u7269\u548c\u9e1f\u7c7b\uff09\u7684\u7ec4\u4ef6\u4ee5\u53ca\u7c7b\u7fa4\u5171\u4eab\u7ec4\u4ef6\u3002\u4e3a\u4e86\u89e3\u51b33D\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u9e1f\u7c7b\u6570\u636e\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u6d41\u7a0b\uff0c\u751f\u6210\u4e86CtrlAni3D\u548cCtrlAVES3D\u4e24\u4e2a\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u3002\u8be5\u6a21\u578b\u5728\u5305\u542b41.3k\u54fa\u4e73\u52a8\u7269\u548c12.4k\u9e1f\u7c7b\u56fe\u50cf\uff08\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\uff09\u7684\u96c6\u5408\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "AniMer+\u5728\u91cd\u6784\u54fa\u4e73\u52a8\u7269\u548c\u9e1f\u7c7b\u65b9\u9762\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728Animal Kingdom\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u5408\u6210\u6570\u636e\u96c6CtrlAni3D\u548cCtrlAVES3D\u5bf9\u4e8e\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u8d77\u5230\u4e86\u5173\u952e\u4f5c\u7528\uff0c\u7279\u522b\u662fCtrlAVES3D\u662f\u9996\u4e2a\u5927\u89c4\u6a213D\u6807\u6ce8\u7684\u9e1f\u7c7b\u6570\u636e\u96c6\u3002", "conclusion": "AniMer+\u5728\u54fa\u4e73\u52a8\u7269\u548c\u9e1f\u7c7b\uff08aves\uff09\u7684\u91cd\u5efa\u65b9\u9762\u5b9e\u73b0\u4e86\u7edf\u4e00\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u7ed3\u5408\u9ad8\u5bb9\u91cf\u3001\u5bb6\u65cf\u611f\u77e5\u7684ViT\u548cMoE\u8bbe\u8ba1\uff0c\u4ee5\u53ca\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\uff08CtrlAni3D\u548cCtrlAVES3D\uff09\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684Animal Kingdom\u6570\u636e\u96c6\u4e0a\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u7f51\u7edc\u67b6\u6784\u548c\u5408\u6210\u6570\u636e\u96c6\u5728\u63d0\u9ad8\u771f\u5b9e\u5e94\u7528\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.00230", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00230", "abs": "https://arxiv.org/abs/2508.00230", "authors": ["Paul Albert", "Frederic Z. Zhang", "Hemanth Saratchandran", "Anton van den Hengel", "Ehsan Abbasnejad"], "title": "Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product", "comment": "To appear in ICCV 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has become a standard approach for\nadapting large pre-trained models. Amongst PEFT methods, low-rank adaptation\n(LoRA) has achieved notable success. However, recent studies have highlighted\nits limitations compared against full-rank alternatives, particularly when\napplied to multimodal and large language models. In this work, we present a\nquantitative comparison amongst full-rank and low-rank PEFT methods using a\nsynthetic matrix approximation benchmark with controlled spectral properties.\nOur results confirm that LoRA struggles to approximate matrices with relatively\nflat spectrums or high frequency components -- signs of high effective ranks.\nTo this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the\nKhatri-Rao product to produce weight updates, which, by construction, tends to\nproduce matrix product with a high effective rank. We demonstrate performance\ngains with KRAdapter on vision-language models up to 1B parameters and on large\nlanguage models up to 8B parameters, particularly on unseen common-sense\nreasoning tasks. In addition, KRAdapter maintains the memory and compute\nefficiency of LoRA, making it a practical and robust alternative to fine-tune\nbillion-scale parameter models.", "AI": {"tldr": "KRAdapter\u662f\u4e00\u79cd\u65b0\u7684PEFT\u65b9\u6cd5\uff0c\u901a\u8fc7Khatri-Rao\u79ef\u514b\u670d\u4e86LoRA\u5728\u5904\u7406\u9ad8\u79e9\u77e9\u9635\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u6a21\u6001\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u5747\u8868\u73b0\u51fa\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3LoRA\u5728\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u4e0e\u5168\u79e9\u66ff\u4ee3\u65b9\u6cd5\u76f8\u6bd4\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5177\u6709\u76f8\u5bf9\u5e73\u5766\u8c31\u6216\u9ad8\u9891\u5206\u91cf\u7684\u77e9\u9635\u65f6\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u5177\u6709\u53ef\u63a7\u8c31\u7279\u6027\u7684\u5408\u6210\u77e9\u9635\u8fd1\u4f3c\u57fa\u51c6\uff0c\u5bf9\u5168\u79e9\u548c\u4f4e\u79e9PEFT\u65b9\u6cd5\u8fdb\u884c\u91cf\u5316\u6bd4\u8f83\uff0c\u5e76\u5f15\u5165\u4e86KRAdapter\uff0c\u4e00\u79cd\u5229\u7528Khatri-Rao\u79ef\u7684\u65b0\u578bPEFT\u7b97\u6cd5\u3002", "result": "LoRA\u5728\u8fd1\u4f3c\u5177\u6709\u76f8\u5bf9\u5e73\u5766\u8c31\u6216\u9ad8\u9891\u5206\u91cf\u7684\u77e9\u9635\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u800cKRAdapter\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u5747\u53d6\u5f97\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u672a\u89c1\u8fc7\u7684\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u3002", "conclusion": "KRAdapter\u901a\u8fc7\u5229\u7528Khatri-Rao\u79ef\u6765\u4ea7\u751f\u6743\u91cd\u66f4\u65b0\uff0c\u8be5\u66f4\u65b0\u5728\u7ed3\u6784\u4e0a\u503e\u5411\u4e8e\u4ea7\u751f\u5177\u6709\u9ad8\u6709\u6548\u79e9\u7684\u77e9\u9635\u4e58\u79ef\u3002\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u591a\u8fbe1B\u53c2\u6570\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u591a\u8fbe8B\u53c2\u6570\uff09\u4e0a\uff0cKRAdapter\u5728\u5904\u7406\u672a\u89c1\u8fc7\u7684\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86LoRA\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2508.00454", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00454", "abs": "https://arxiv.org/abs/2508.00454", "authors": ["Yuqi Tang", "Kehua Feng", "Yunfeng Wang", "Zhiwen Chen", "Chengfei Lv", "Gang Yu", "Qiang Zhang", "Keyan Ding"], "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges", "comment": "15 pages, 2 pages, under review at AAAI 2026", "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u5668\uff0c\u901a\u8fc7\u805a\u5408\u591a\u4e2aLLM\u88c1\u5224\u7684\u504f\u597d\u77e5\u8bc6\u5230\u5355\u4e2a\u6a21\u578b\u4e2d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u504f\u89c1\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u201cLLM-as-a-judge\u201d\u8303\u5f0f\u5b58\u5728\u504f\u89c1\u95ee\u9898\uff0c\u591a\u88c1\u5224\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u5668\uff0c\u901a\u8fc7\u805a\u5408\u591a\u4e2aLLM\u88c1\u5224\u7684\u504f\u597d\u77e5\u8bc6\u5230\u5355\u4e2a\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u4e03\u4e2a\u5355\u8bc4\u5206\u548c\u6210\u5bf9\u6bd4\u8f83\u5bf9\u8bdd\u8bc4\u4f30\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c55\u73b0\u4e86\u5176\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u5668\u901a\u8fc7\u805a\u5408\u591a\u4e2aLLM\u88c1\u5224\u7684\u504f\u597d\u77e5\u8bc6\u5230\u5355\u4e2a\u6a21\u578b\u4e2d\uff0c\u6709\u6548\u4fdd\u7559\u4e86\u591a\u88c1\u5224\u53cd\u9988\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8bc4\u4f30\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u7075\u6d3b\u7684\u5bf9\u8bdd\u8d28\u91cf\u8bc4\u4f30\u3002"}}
{"id": "2508.00581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00581", "abs": "https://arxiv.org/abs/2508.00581", "authors": ["Ruiqing Ding", "Qianfang Sun", "Yongkang Leng", "Hui Yin", "Xiaojian Li"], "title": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation", "comment": "16 pages, 10 figures", "summary": "Pre-consultation is a critical component of effective healthcare delivery.\nHowever, generating comprehensive pre-consultation questionnaires from complex,\nvoluminous Electronic Medical Records (EMRs) is a challenging task. Direct\nLarge Language Model (LLM) approaches face difficulties in this task,\nparticularly regarding information completeness, logical order, and\ndisease-level synthesis. To address this issue, we propose a novel multi-stage\nLLM-driven framework: Stage 1 extracts atomic assertions (key facts with\ntiming) from EMRs; Stage 2 constructs personal causal networks and synthesizes\ndisease knowledge by clustering representative networks from an EMR corpus;\nStage 3 generates tailored personal and standardized disease-specific\nquestionnaires based on these structured representations. This framework\novercomes limitations of direct methods by building explicit clinical\nknowledge. Evaluated on a real-world EMR dataset and validated by clinical\nexperts, our method demonstrates superior performance in information coverage,\ndiagnostic relevance, understandability, and generation time, highlighting its\npractical potential to enhance patient information collection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5LLM\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7535\u5b50\u75c5\u5386\u4e2d\u751f\u6210\u9884\u54a8\u8be2\u95ee\u5377\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5LLM\u65b9\u6cd5\u5728\u4fe1\u606f\u5b8c\u6574\u6027\u548c\u903b\u8f91\u987a\u5e8f\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u63d0\u53d6\u4e8b\u5b9e\u3001\u6784\u5efa\u56e0\u679c\u7f51\u7edc\u548c\u7efc\u5408\u75be\u75c5\u77e5\u8bc6\u6765\u751f\u6210\u5b9a\u5236\u95ee\u5377\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u548c\u4e13\u5bb6\u9a8c\u8bc1\u4e2d\u663e\u793a\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4ece\u590d\u6742\u7684\u3001\u5927\u91cf\u7684\u7535\u5b50\u75c5\u5386\uff08EMRs\uff09\u4e2d\u751f\u6210\u5168\u9762\u7684\u9884\u54a8\u8be2\u95ee\u5377\u662f\u4e00\u9879\u8270\u5de8\u7684\u4efb\u52a1\u3002\u76f4\u63a5\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65b9\u6cd5\u5728\u6b64\u4efb\u52a1\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4fe1\u606f\u5b8c\u6574\u6027\u3001\u903b\u8f91\u987a\u5e8f\u548c\u75be\u75c5\u7ea7\u522b\u7efc\u5408\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u9636\u6bb5LLM\u9a71\u52a8\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4eceEMR\u4e2d\u63d0\u53d6\u539f\u5b50\u65ad\u8a00\uff08\u5e26\u6709\u65f6\u6548\u6027\u7684\u5173\u952e\u4e8b\u5b9e\uff09\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u805a\u7c7bEMR\u8bed\u6599\u5e93\u7684\u4ee3\u8868\u6027\u7f51\u7edc\u6765\u6784\u5efa\u4e2a\u4eba\u56e0\u679c\u7f51\u7edc\u5e76\u7efc\u5408\u75be\u75c5\u77e5\u8bc6\uff1b\u7b2c\u4e09\u9636\u6bb5\u57fa\u4e8e\u8fd9\u4e9b\u7ed3\u6784\u5316\u8868\u793a\u751f\u6210\u5b9a\u5236\u7684\u4e2a\u4eba\u548c\u6807\u51c6\u5316\u7684\u75be\u75c5\u7279\u5b9a\u95ee\u5377\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684EMR\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u7531\u4e34\u5e8a\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fe1\u606f\u8986\u76d6\u3001\u8bca\u65ad\u76f8\u5173\u6027\u3001\u53ef\u7406\u89e3\u6027\u548c\u751f\u6210\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u7a81\u663e\u4e86\u5176\u5728\u52a0\u5f3a\u60a3\u8005\u4fe1\u606f\u6536\u96c6\u65b9\u9762\u7684\u5b9e\u9645\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u663e\u5f0f\u7684\u4e34\u5e8a\u77e5\u8bc6\u514b\u670d\u4e86\u76f4\u63a5\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.00338", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00338", "abs": "https://arxiv.org/abs/2508.00338", "authors": ["Ihor Sokolov", "Yintai Zhang", "Jacek Dziarmaga"], "title": "Truncating loopy tensor networks by zero-mode gauge fixing", "comment": "12 pages, 14 figures", "summary": "Loopy tensor networks have internal correlations that often make their\ncompression inefficient. We show that even local bond optimization can make\nbetter use of the insight it has locally into relevant loop correlations. By\ncutting the bond, we define a set of states whose linear dependence can be used\nto truncate the bond dimension. The linear dependence is eliminated with zero\nmodes of the states' metric tensor. The method is illustrated by a series of\nexamples for the infinite pair entangled projected state (iPEPS) and for the\nperiodic matrix product state (pMPS) that occurs in the tensor renormalization\ngroup (TRG) step. In all examples, it provides better initial truncation errors\nthan standard initialization.", "AI": {"tldr": "\u5377\u66f2\u5f20\u91cf\u7f51\u7edc\u7684\u538b\u7f29\u53ef\u4ee5\u901a\u8fc7\u4f18\u5316\u5c40\u90e8\u952e\u548c\u5229\u7528\u72b6\u6001\u7684\u5ea6\u91cf\u5f20\u91cf\u7684\u96f6\u6a21\u6765\u6539\u8fdb\uff0c\u4ece\u800c\u51cf\u5c11\u622a\u65ad\u8bef\u5dee\u3002", "motivation": "\u5377\u66f2\u5f20\u91cf\u7f51\u7edc\u5185\u90e8\u7684\u5173\u8054\u6027\u5e38\u5e38\u5bfc\u81f4\u5176\u538b\u7f29\u6548\u7387\u4f4e\u4e0b\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u5229\u7528\u5c40\u90e8\u952e\u4f18\u5316\u6765\u66f4\u597d\u5730\u5229\u7528\u5176\u5bf9\u5c40\u90e8\u76f8\u5173\u5faa\u73af\u7684\u6d1e\u5bdf\u529b\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5207\u5272 the bond\uff0c\u5b9a\u4e49\u4e00\u7ec4\u72b6\u6001\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u72b6\u6001\u7684\u5ea6\u91cf\u5f20\u91cf\u7684\u96f6\u6a21\u6765\u6d88\u9664\u7ebf\u6027\u4f9d\u8d56\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u5377\u66f2\u5f20\u91cf\u7f51\u7edc\u7684\u538b\u7f29\u3002", "result": "\u8be5\u65b9\u6cd5\u5728iPEPS\u548cpMPS\u7684\u793a\u4f8b\u4e2d\uff0c\u63d0\u4f9b\u4e86\u6bd4\u6807\u51c6\u521d\u59cb\u5316\u66f4\u597d\u7684\u521d\u59cb\u622a\u65ad\u8bef\u5dee\u3002", "conclusion": "\u901a\u8fc7\u6d88\u9664\u96f6\u6a21\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6d88\u9664\u7ebf\u6027\u4f9d\u8d56\u6027\uff0c\u4ece\u800c\u5728iPEPS\u548cpMPS\u7684\u521d\u59cb\u622a\u65ad\u8bef\u5dee\u65b9\u9762\u4f18\u4e8e\u6807\u51c6\u521d\u59cb\u5316\u3002"}}
{"id": "2508.00440", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00440", "abs": "https://arxiv.org/abs/2508.00440", "authors": ["M. A. P\u00e9rez-Cuti\u00f1o", "J. Valverde", "J. Capit\u00e1n", "J. M. D\u00edaz-B\u00e1\u00f1ez"], "title": "Reducing the gap between general purpose data and aerial images in concentrated solar power plants", "comment": null, "summary": "In the context of Concentrated Solar Power (CSP) plants, aerial images\ncaptured by drones present a unique set of challenges. Unlike urban or natural\nlandscapes commonly found in existing datasets, solar fields contain highly\nreflective surfaces, and domain-specific elements that are uncommon in\ntraditional computer vision benchmarks. As a result, machine learning models\ntrained on generic datasets struggle to generalize to this setting without\nextensive retraining and large volumes of annotated data. However, collecting\nand labeling such data is costly and time-consuming, making it impractical for\nrapid deployment in industrial applications.\n  To address this issue, we propose a novel approach: the creation of\nAerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By\ngenerating synthetic data that closely mimic real-world conditions, our\nobjective is to facilitate pretraining of models before deployment,\nsignificantly reducing the need for extensive manual labeling. Our main\ncontributions are threefold: (1) we introduce AerialCSP, a high-quality\nsynthetic dataset for aerial inspection of CSP plants, providing annotated data\nfor object detection and image segmentation; (2) we benchmark multiple models\non AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we\ndemonstrate that pretraining on AerialCSP significantly improves real-world\nfault detection, particularly for rare and small defects, reducing the need for\nextensive manual labeling. AerialCSP is made publicly available at\nhttps://mpcutino.github.io/aerialcsp/.", "AI": {"tldr": "\u4e3aCSP\u5de5\u5382\u7684\u822a\u7a7a\u5f71\u50cf\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aAerialCSP\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u4ee5\u51cf\u5c11\u6570\u636e\u6807\u6ce8\u6210\u672c\u5e76\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u8be5\u6570\u636e\u96c6\u53ef\u7528\u4e8e\u6a21\u578b\u9884\u8bad\u7ec3\uff0c\u4ece\u800c\u63d0\u9ad8\u6545\u969c\u68c0\u6d4b\u80fd\u529b\uff0c\u7279\u522b\u662f\u5bf9\u7f55\u89c1\u7f3a\u9677\u7684\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406CSP\u5de5\u5382\u7684\u822a\u7a7a\u5f71\u50cf\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u56e0\u4e3a\u8be5\u573a\u666f\u5305\u542b\u9ad8\u53cd\u5c04\u8868\u9762\u548c\u7279\u5b9a\u9886\u57df\u5143\u7d20\uff0c\u800c\u8fd9\u4e9b\u5728\u901a\u7528\u6570\u636e\u96c6\u4e0d\u5e38\u89c1\u3002\u6536\u96c6\u548c\u6807\u6ce8\u771f\u5b9e\u6570\u636e\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAerialCSP\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u521b\u5efa\u4e00\u4e2a\u865a\u62df\u6570\u636e\u96c6\u6765\u6a21\u62dfCSP\u5de5\u5382\u7684\u822a\u7a7a\u5f71\u50cf\uff0c\u76ee\u7684\u662f\u5728\u90e8\u7f72\u524d\u5bf9\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ece\u800c\u51cf\u5c11\u5bf9\u5e7f\u6cdb\u624b\u52a8\u6807\u6ce8\u6570\u636e\u7684\u9700\u6c42\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aAerialCSP\u7684\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8eCSP\u5de5\u5382\u7684\u822a\u7a7a\u68c0\u67e5\uff0c\u63d0\u4f9b\u4e86\u7528\u4e8e\u5bf9\u8c61\u68c0\u6d4b\u548c\u56fe\u50cf\u5206\u5272\u7684\u6807\u6ce8\u6570\u636e\u3002\u5bf9\u591a\u4e2a\u6a21\u578b\u5728AerialCSP\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5728AerialCSP\u4e0a\u7684\u9884\u8bad\u7ec3\u80fd\u663e\u8457\u63d0\u9ad8\u771f\u5b9e\u4e16\u754c\u6545\u969c\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5bf9\u7f55\u89c1\u548c\u5c0f\u7f3a\u9677\u7684\u68c0\u6d4b\u3002", "conclusion": "\u901a\u8fc7\u5728AerialCSP\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5bf9CSP\u5de5\u5382\u8fdb\u884c\u6545\u969c\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5bf9\u4e8e\u7f55\u89c1\u548c\u5fae\u5c0f\u7684\u7f3a\u9677\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u624b\u52a8\u6807\u6ce8\u6570\u636e\u7684\u9700\u6c42\u3002"}}
{"id": "2508.00264", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.00264", "abs": "https://arxiv.org/abs/2508.00264", "authors": ["Jerry Huang", "Peng Lu", "Qiuhao Zeng"], "title": "Calibrated Language Models and How to Find Them with Label Smoothing", "comment": "Accepted to the Forty-second International Conference on Machine\n  Learning (ICML) 2025. First two authors contributed equally", "summary": "Recent advances in natural language processing (NLP) have opened up greater\nopportunities to enable fine-tuned large language models (LLMs) to behave as\nmore powerful interactive agents through improved instruction-following\nability. However, understanding how this impacts confidence calibration for\nreliable model output has not been researched in full. In this work, we examine\nvarious open-sourced LLMs, identifying significant calibration degradation\nafter instruction tuning in each. Seeking a practical solution, we look towards\nlabel smoothing, which has been shown as an effective method to regularize for\noverconfident predictions but has yet to be widely adopted in the supervised\nfine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing\nis sufficient to maintain calibration throughout the SFT process. However,\nsettings remain where the effectiveness of smoothing is severely diminished, in\nparticular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to\nstem from the ability to become over-confident, which has a direct relationship\nwith the hidden size and vocabulary size, and justify this theoretically and\nexperimentally. Finally, we address an outstanding issue regarding the memory\nfootprint of the cross-entropy loss computation in the label smoothed loss\nsetting, designing a customized kernel to dramatically reduce memory\nconsumption without sacrificing speed or performance in comparison to existing\nsolutions for non-smoothed losses.", "AI": {"tldr": "\u6307\u4ee4\u5fae\u8c03\u4f1a\u635f\u5bb3LLM\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u6807\u7b7e\u5e73\u6ed1\u53ef\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u5bf9\u5927\u578b\u8bcd\u6c47\u91cfLLM\u6548\u679c\u6709\u9650\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6848\u3002", "motivation": "\u968f\u7740\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u7684\u8fdb\u6b65\uff0c\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f7f\u5176\u6210\u4e3a\u66f4\u5f3a\u5927\u7684\u4ea4\u4e92\u5f0f\u4ee3\u7406\u5df2\u6210\u4e3a\u53ef\u80fd\u3002\u7136\u800c\uff0c\u6307\u4ee4\u5fae\u8c03\u5bf9\u6a21\u578b\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u8fd9\u5bf9\u4e8e\u4fdd\u8bc1\u6a21\u578b\u8f93\u51fa\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u5f00\u6e90LLM\uff0c\u7814\u7a76\u4e86\u6307\u4ee4\u5fae\u8c03\u5bf9\u5176\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u5f71\u54cd\u3002\u7814\u7a76\u4eba\u5458\u5c1d\u8bd5\u4f7f\u7528\u6807\u7b7e\u5e73\u6ed1\u4f5c\u4e3a\u4e00\u79cd\u6b63\u5219\u5316\u65b9\u6cd5\u6765\u89e3\u51b3\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u5e76\u4ece\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u8bba\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5927\u578b\u8bcd\u6c47\u91cfLLM\uff08LV-LLM\uff09\u65f6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6307\u4ee4\u5fae\u8c03\u4f1a\u663e\u8457\u964d\u4f4eLLM\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002\u6807\u7b7e\u5e73\u6ed1\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5bf9\u4e8e\u5927\u578b\u8bcd\u6c47\u91cfLLM\uff08LV-LLM\uff09\u800c\u8a00\uff0c\u5176\u6548\u679c\u4f1a\u5927\u6253\u6298\u6263\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6a21\u578b\u66f4\u5bb9\u6613\u51fa\u73b0\u8fc7\u5ea6\u81ea\u4fe1\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u6807\u7b7e\u5e73\u6ed1\u7684\u5185\u5b58\u5360\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u6307\u4ee4\u5fae\u8c03\u867d\u7136\u63d0\u5347\u4e86LLM\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u4f46\u4f1a\u4e25\u91cd\u964d\u4f4e\u5176\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002\u7814\u7a76\u53d1\u73b0\u6807\u7b7e\u5e73\u6ed1\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5bf9\u4e8e\u5177\u6709\u5927\u578b\u8bcd\u6c47\u91cf\uff08LV-LLM\uff09\u7684LLM\u6548\u679c\u4f1a\u51cf\u5f31\uff0c\u5176\u539f\u56e0\u662f\u6a21\u578b\u5bb9\u6613\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u8fd9\u4e0e\u6a21\u578b\u7684\u9690\u85cf\u5c42\u5927\u5c0f\u548c\u8bcd\u6c47\u91cf\u5927\u5c0f\u6709\u5173\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u6807\u7b7e\u5e73\u6ed1\u635f\u5931\u8ba1\u7b97\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c0f\u5185\u5b58\u5360\u7528\u3002"}}
{"id": "2508.00476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00476", "abs": "https://arxiv.org/abs/2508.00476", "authors": ["Jeongwoo Kang", "Markarit Vartampetian", "Felix Herron", "Yongxin Zhou", "Diandra Fabre", "Gabriela Gonzalez-Saez"], "title": "GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts", "comment": null, "summary": "This paper documents GETALP's submission to the Third Run of the Automatic\nMinuting Shared Task at SIGDial 2025. We participated in Task B:\nquestion-answering based on meeting transcripts. Our method is based on a\nretrieval augmented generation (RAG) system and Abstract Meaning\nRepresentations (AMR). We propose three systems combining these two approaches.\nOur results show that incorporating AMR leads to high-quality responses for\napproximately 35% of the questions and provides notable improvements in\nanswering questions that involve distinguishing between different participants\n(e.g., who questions).", "AI": {"tldr": "GETALP \u4f7f\u7528\u7ed3\u5408\u4e86 RAG \u548c AMR \u7684\u7cfb\u7edf\u53c2\u52a0\u4e86\u4f1a\u8bae\u8bb0\u5f55\u95ee\u7b54\u4efb\u52a1\uff0cAMR \u63d0\u9ad8\u4e86\u7ea6 35% \u7684\u95ee\u9898\u56de\u7b54\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u533a\u5206\u53c2\u4e0e\u8005\u7684\u95ee\u9898\u4e0a\u6548\u679c\u66f4\u4f73\u3002", "motivation": "\u53c2\u52a0 SIGDial 2025 \u81ea\u52a8\u4f1a\u8bae\u8bb0\u5f55\u5171\u4eab\u4efb\u52a1 B\uff0c\u65e8\u5728\u89e3\u51b3\u57fa\u4e8e\u4f1a\u8bae\u8bb0\u5f55\u7684\u95ee\u7b54\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u62bd\u8c61\u610f\u4e49\u8868\u793a\uff08AMR\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u7ec4\u5408\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAMR \u7684\u5f15\u5165\u80fd\u591f\u63d0\u9ad8\u7ea6 35% \u95ee\u9898\u7684\u56de\u7b54\u8d28\u91cf\uff0c\u5e76\u5728\u56de\u7b54\u6d89\u53ca\u533a\u5206\u4e0d\u540c\u53c2\u4e0e\u8005\uff08\u4f8b\u5982\uff0c\u201c\u8c01\u201d\u95ee\u9898\uff09\u7684\u95ee\u9898\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "GETALP \u63d0\u4ea4\u4e86\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u62bd\u8c61\u610f\u4e49\u8868\u793a\uff08AMR\uff09\u7684\u7cfb\u7edf\u53c2\u52a0\u4e86 SIGDial 2025 \u81ea\u52a8\u4f1a\u8bae\u8bb0\u5f55\u5171\u4eab\u4efb\u52a1 B\uff08\u57fa\u4e8e\u4f1a\u8bae\u8bb0\u5f55\u7684\u95ee\u7b54\uff09\u3002"}}
{"id": "2508.00353", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00353", "abs": "https://arxiv.org/abs/2508.00353", "authors": ["H. Solki", "Ali Motazedifard", "M. H. Naderi", "A. Youssefi", "R. Roknizadeh"], "title": "Nonclassical microwave radiation from the parametric dynamical Casimir effect in the reversed-dissipation regime of circuit optomechanics", "comment": null, "summary": "We propose an experimentally feasible optomechanical system (OMS) that is\ndispersively driven and operates in the reversed dissipation regime (RDR),\nwhere the mechanical damping rate far exceeds the cavity decay rate. We\ndemonstrate that coherent, fast-time modulation of the driving laser\nfrequency-on time scales longer than the mechanical decoherence time-allows for\nadiabatic elimination of the mechanical mode, resulting in strong parametric\namplification of quantum vacuum fluctuations of the intracavity field. This\nmechanism, known as the parametric dynamical Casimir effect (parametric-DCE),\nleads to the generation of Casimir photons. In the dispersive RDR, we find that\nthe total system Hamiltonian-including the DCE term-is intrinsically modified\nby a generalized optomechanical Kerr-type nonlinearity. This nonlinearity not\nonly saturates the mean number of radiated Casimir photons on short time\nscales, even without dissipation, but also induces oscillatory behavior in\ntheir dynamics and quantum characteristics. Remarkably, the presence of the\nKerr nonlinearity causes the generated DCE photons to exhibit nonclassical\nfeatures, including sub-Poissonian statistics, negative Wigner function and\nquadrature squeezing which can be controlled by adjusting the system\nparameters. The proposed nonclassical microwave radiation source possesses the\npotential to be applied in quantum information processing, quantum computing as\nwell as microwave quantum sensing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u53cd\u8017\u6563\u4f53\u5236\uff08RDR\uff09\u4e0b\u7684\u5149\u5b66\u673a\u68b0\u7cfb\u7edf\uff08OMS\uff09\uff0c\u901a\u8fc7\u6fc0\u5149\u9891\u7387\u8c03\u5236\u5b9e\u73b0\u4e86\u53c2\u6570\u52a8\u529b\u5b66 Casimir \u6548\u5e94\uff08parametrically-DCE\uff09\uff0c\u4ea7\u751f\u4e86\u5177\u6709\u975e\u7ecf\u5178\u7279\u5f81\uff08\u5982\u4e9a\u6cca\u677e\u7edf\u8ba1\uff09\u7684 Casimir \u5149\u5b50\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u91cf\u5b50\u6280\u672f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u4e3a\u4e86\u5728\u53cd\u8017\u6563\u4f53\u5236\uff08RDR\uff09\u4e0b\uff0c\u5229\u7528\u5149\u5b66\u673a\u68b0\u7cfb\u7edf\uff08OMS\uff09\u5b9e\u73b0\u91cf\u5b50\u771f\u7a7a\u6da8\u843d\u7684\u53c2\u6570\u653e\u5927\uff0c\u5e76\u4ea7\u751f\u5177\u6709\u975e\u7ecf\u5178\u7279\u6027\u7684 Casimir \u5149\u5b50\uff0c\u4ee5\u5e94\u7528\u4e8e\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u3001\u91cf\u5b50\u8ba1\u7b97\u548c\u5fae\u6ce2\u91cf\u5b50\u4f20\u611f\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5b9e\u9a8c\u4e0a\u53ef\u884c\u7684\u5149\u5b66\u673a\u68b0\u7cfb\u7edf\uff08OMS\uff09\uff0c\u8be5\u7cfb\u7edf\u91c7\u7528\u8272\u6563\u9a71\u52a8\u5e76\u5728\u53cd\u8017\u6563\u4f53\u5236\uff08RDR\uff09\u4e0b\u8fd0\u884c\uff0c\u5176\u4e2d\u673a\u68b0\u963b\u5c3c\u7387\u8fdc\u8d85\u8154\u8870\u51cf\u7387\u3002\u901a\u8fc7\u5bf9\u9a71\u52a8\u6fc0\u5149\u9891\u7387\u8fdb\u884c\u76f8\u5e72\u3001\u5feb\u65f6\u95f4\u8c03\u5236\uff0c\u5728\u957f\u4e8e\u673a\u68b0\u9000\u76f8\u5e72\u65f6\u95f4\u7684\u76f8\u5e72\u65f6\u95f4\u5c3a\u5ea6\u4e0a\uff0c\u5b9e\u73b0\u4e86\u673a\u68b0\u6a21\u5f0f\u7684\u7edd\u70ed\u6d88\u9664\uff0c\u4ece\u800c\u5bf9\u8154\u5185\u573a\u7684\u91cf\u5b50\u771f\u7a7a\u6da8\u843d\u4ea7\u751f\u5f3a\u70c8\u7684\u53c2\u6570\u653e\u5927\uff0c\u5373\u53c2\u6570\u52a8\u529b\u5b66 Casimir \u6548\u5e94\uff08parametrically-DCE\uff09\uff0c\u5bfc\u81f4 Casimir \u5149\u5b50\u7684\u4ea7\u751f\u3002", "result": "\u5728\u53cd\u8017\u6563\u4f53\u5236\uff08RDR\uff09\u4e0b\uff0c\u7cfb\u7edf\u54c8\u5bc6\u987f\u91cf\uff08\u5305\u62ec DCE \u9879\uff09\u88ab\u5e7f\u4e49\u7684\u5149\u5b66\u673a\u68b0 Kerr \u578b\u975e\u7ebf\u6027\u5185\u5728\u5730\u4fee\u6539\u3002\u8fd9\u79cd\u975e\u7ebf\u6027\u4e0d\u4ec5\u5728\u77ed\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u4f7f\u8f90\u5c04\u7684 Casimir \u5149\u5b50\u7684\u5e73\u5747\u6570\u91cf\u9971\u548c\uff08\u5373\u4f7f\u5728\u6ca1\u6709\u8017\u6563\u7684\u60c5\u51b5\u4e0b\uff09\uff0c\u8fd8\u5f15\u8d77\u4e86\u5176\u52a8\u529b\u5b66\u548c\u91cf\u5b50\u7279\u6027\u7684\u632f\u8361\u884c\u4e3a\u3002Kerr \u975e\u7ebf\u6027\u7684\u5b58\u5728\u4f7f\u5f97\u4ea7\u751f\u7684 DCE \u5149\u5b50\u8868\u73b0\u51fa\u975e\u7ecf\u5178\u7279\u5f81\uff0c\u5982\u4f4e\u4e8e\u6cca\u677e\u7edf\u8ba1\u3001\u8d1f Wigner \u51fd\u6570\u548c\u8c61\u9650\u538b\u7f29\uff0c\u8fd9\u4e9b\u90fd\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u7cfb\u7edf\u53c2\u6570\u6765\u63a7\u5236\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cd\u8017\u6563\u4f53\u5236\u4e0b\u7684\u5149\u5b66\u673a\u68b0\u7cfb\u7edf\uff0c\u5229\u7528\u6fc0\u5149\u9891\u7387\u7684\u76f8\u5e72\u5feb\u65f6\u8c03\u5236\uff0c\u80fd\u591f\u4ea7\u751f\u5177\u6709\u975e\u7ecf\u5178\u7279\u6027\u7684 Casimir \u5149\u5b50\uff0c\u53ef\u7528\u4e8e\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u3001\u91cf\u5b50\u8ba1\u7b97\u548c\u5fae\u6ce2\u91cf\u5b50\u4f20\u611f\u3002"}}
{"id": "2508.00589", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.RO", "68T45, 68P20, 68T10, 68T50, 68T07, 68T40", "I.2.10; I.4.8; I.2.9; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.00589", "abs": "https://arxiv.org/abs/2508.00589", "authors": ["Stefan Englmeier", "Max A. B\u00fcttner", "Katharina Winter", "Fabian B. Flohr"], "title": "Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving", "comment": "9 pages, 10 figure, project page\n  https://iv.ee.hm.edu/contextmotionclip/, submitted to IEEE Transactions on\n  Intelligent Vehicles (T-IV), This work has been submitted to the IEEE for\n  possible publication", "summary": "Autonomous driving systems must operate reliably in safety-critical\nscenarios, particularly those involving unusual or complex behavior by\nVulnerable Road Users (VRUs). Identifying these edge cases in driving datasets\nis essential for robust evaluation and generalization, but retrieving such rare\nhuman behavior scenarios within the long tail of large-scale datasets is\nchallenging. To support targeted evaluation of autonomous driving systems in\ndiverse, human-centered scenarios, we propose a novel context-aware motion\nretrieval framework. Our method combines Skinned Multi-Person Linear\n(SMPL)-based motion sequences and corresponding video frames before encoding\nthem into a shared multimodal embedding space aligned with natural language.\nOur approach enables the scalable retrieval of human behavior and their context\nthrough text queries. This work also introduces our dataset WayMoCo, an\nextension of the Waymo Open Dataset. It contains automatically labeled motion\nand scene context descriptions derived from generated pseudo-ground-truth SMPL\nsequences and corresponding image data. Our approach outperforms\nstate-of-the-art models by up to 27.5% accuracy in motion-context retrieval,\nwhen evaluated on the WayMoCo dataset.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e2d\u68c0\u7d22\u7f55\u89c1\u7684VRU\u884c\u4e3a\u8fb9\u7f18\u6848\u4f8b\u7684\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408SMPL\u8fd0\u52a8\u5e8f\u5217\u548c\u89c6\u9891\u5e27\u7684\u591a\u6a21\u6001\u68c0\u7d22\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u81ea\u52a8\u6807\u8bb0\u7684\u8fd0\u52a8\u548c\u573a\u666f\u4e0a\u4e0b\u6587\u63cf\u8ff0\u7684WayMoCo\u6570\u636e\u96c6\uff0c\u8be5\u6846\u67b6\u5728\u68c0\u7d22\u51c6\u786e\u7387\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u5fc5\u987b\u53ef\u9760\u8fd0\u884c\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u6d89\u53ca\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u4e0d\u5bfb\u5e38\u6216\u590d\u6742\u884c\u4e3a\u7684\u573a\u666f\u3002\u7136\u800c\uff0c\u5728\u5927\u578b\u6570\u636e\u96c6\u4e2d\u68c0\u7d22\u8fd9\u4e9b\u7f55\u89c1\u7684\u3001\u957f\u5c3e\u5206\u5e03\u7684\u4eba\u7c7b\u884c\u4e3a\u8fb9\u7f18\u6848\u4f8b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u4e0e\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8fd0\u52a8\u68c0\u7d22\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8eSMPL\uff08\u8499\u76ae\u591a\u4eba\u7ebf\u6027\u6a21\u578b\uff09\u7684\u8fd0\u52a8\u5e8f\u5217\u548c\u76f8\u5e94\u7684\u89c6\u9891\u5e27\uff0c\u5e76\u5c06\u5b83\u4eec\u7f16\u7801\u5230\u4e00\u4e2a\u5171\u4eab\u7684\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u4e2d\uff0c\u8be5\u7a7a\u95f4\u4e0e\u81ea\u7136\u8bed\u8a00\u5bf9\u9f50\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u6587\u672c\u67e5\u8be2\u53ef\u6269\u5c55\u5730\u68c0\u7d22\u4eba\u7c7b\u884c\u4e3a\u53ca\u5176\u4e0a\u4e0b\u6587\uff0c\u5e76\u5728WayMoCo\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u6700\u5148\u8fdb\u6a21\u578b\u9ad8\u51fa27.5%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408SMPL\u8fd0\u52a8\u5e8f\u5217\u548c\u89c6\u9891\u5e27\uff0c\u5e76\u5c06\u5176\u7f16\u7801\u5230\u5171\u4eab\u7684\u3001\u4e0e\u81ea\u7136\u8bed\u8a00\u5bf9\u9f50\u7684\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u4e2d\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u3001\u901a\u8fc7\u6587\u672c\u67e5\u8be2\u68c0\u7d22\u4eba\u7c7b\u884c\u4e3a\u53ca\u5176\u4e0a\u4e0b\u6587\u7684\u529f\u80fd\u3002\u5728WayMoCo\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u8fd0\u52a8-\u4e0a\u4e0b\u6587\u68c0\u7d22\u65b9\u9762\u7684\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u9ad8\u51fa27.5%\u3002"}}
{"id": "2508.00308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00308", "abs": "https://arxiv.org/abs/2508.00308", "authors": ["Chunyan She", "Fujun Han", "Chengyu Fang", "Shukai Duan", "Lidan Wang"], "title": "Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement", "comment": "Accepted by ACM MM 2025", "summary": "The event camera, benefiting from its high dynamic range and low latency,\nprovides performance gain for low-light image enhancement. Unlike frame-based\ncameras, it records intensity changes with extremely high temporal resolution,\ncapturing sufficient structure information. Currently, existing event-based\nmethods feed a frame and events directly into a single model without fully\nexploiting modality-specific advantages, which limits their performance.\nTherefore, by analyzing the role of each sensing modality, the enhancement\npipeline is decoupled into two stages: visibility restoration and structure\nrefinement. In the first stage, we design a visibility restoration network with\namplitude-phase entanglement by rethinking the relationship between amplitude\nand phase components in Fourier space. In the second stage, a fusion strategy\nwith dynamic alignment is proposed to mitigate the spatial mismatch caused by\nthe temporal resolution discrepancy between two sensing modalities, aiming to\nrefine the structure information of the image enhanced by the visibility\nrestoration network. In addition, we utilize spatial-frequency interpolation to\nsimulate negative samples with diverse illumination, noise and artifact\ndegradations, thereby developing a contrastive loss that encourages the model\nto learn discriminative representations. Experiments demonstrate that the\nproposed method outperforms state-of-the-art models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u4e8b\u4ef6\u76f8\u673a\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u632f\u5e45-\u76f8\u4f4d\u7ea0\u7f20\u548c\u52a8\u6001\u5bf9\u9f50\u878d\u5408\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u76f8\u673a\u589e\u5f3a\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4e0d\u540c\u4f20\u611f\u6a21\u5f0f\u7684\u4f18\u52bf\uff0c\u5c06\u5e27\u548c\u4e8b\u4ef6\u76f4\u63a5\u8f93\u5165\u5355\u4e2a\u6a21\u578b\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5206\u6790\u6bcf\u79cd\u4f20\u611f\u6a21\u5f0f\u7684\u4f5c\u7528\uff0c\u5e76\u89e3\u8026\u589e\u5f3a\u6d41\u6c34\u7ebf\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e8b\u4ef6\u76f8\u673a\u589e\u5f3a\u6d41\u6c34\u7ebf\uff0c\u5c06\u5176\u89e3\u8026\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u53ef\u89c1\u6027\u6062\u590d\u548c\u7ed3\u6784\u7cbe\u70bc\u3002\u5728\u53ef\u89c1\u6027\u6062\u590d\u9636\u6bb5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u7a7a\u95f4\u7684\u632f\u5e45-\u76f8\u4f4d\u7ea0\u7f20\u7f51\u7edc\u3002\u5728\u7ed3\u6784\u7cbe\u70bc\u9636\u6bb5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u52a8\u6001\u5bf9\u9f50\u7684\u878d\u5408\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u4e24\u79cd\u4f20\u611f\u6a21\u5f0f\u4e4b\u95f4\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u5dee\u5f02\u9020\u6210\u7684\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u7a7a\u95f4\u9891\u7387\u63d2\u503c\u6a21\u62df\u8d1f\u6837\u672c\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u56fe\u50cf\u589e\u5f3a\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002"}}
{"id": "2508.00270", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00270", "abs": "https://arxiv.org/abs/2508.00270", "authors": ["Robin Schmucker", "Nimish Pachapurkar", "Shanmuga Bala", "Miral Shah", "Tom Mitchell"], "title": "Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring", "comment": null, "summary": "We present an online tutoring system that learns to provide effective\nfeedback to students after they answer questions incorrectly. Using data from\none million students, the system learns which assistance action (e.g., one of\nmultiple hints) to provide for each question to optimize student learning.\nEmploying the multi-armed bandit (MAB) framework and offline policy evaluation,\nwe assess 43,000 assistance actions, and identify trade-offs between assistance\npolicies optimized for different student outcomes (e.g., response correctness,\nsession completion). We design an algorithm that for each question decides on a\nsuitable policy training objective to enhance students' immediate second\nattempt success and overall practice session performance. We evaluate the\nresulting MAB policies in 166,000 practice sessions, verifying significant\nimprovements in student outcomes. While MAB policies optimize feedback for the\noverall student population, we further investigate whether contextual bandit\n(CB) policies can enhance outcomes by personalizing feedback based on\nindividual student features (e.g., ability estimates, response times). Using\ncausal inference, we examine (i) how effects of assistance actions vary across\nstudents and (ii) whether CB policies, which leverage such effect\nheterogeneity, outperform MAB policies. While our analysis reveals that some\nactions for some questions exhibit effect heterogeneity, effect sizes may often\nbe too small for CB policies to provide significant improvements beyond what\nwell-optimized MAB policies that deliver the same action to all students\nalready achieve. We discuss insights gained from deploying data-driven systems\nat scale and implications for future refinements. Today, the teaching policies\noptimized by our system support thousands of students daily.", "AI": {"tldr": "\u4e00\u4e2a\u5728\u7ebf\u8f85\u5bfc\u7cfb\u7edf\u5229\u7528MAB\u548cCB\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u767e\u4e07\u5b66\u751f\u6570\u636e\uff0c\u5b66\u4e60\u5e76\u4f18\u5316\u53cd\u9988\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u5b66\u751f\u5b66\u4e60\u6548\u679c\u3002\u867d\u7136MAB\u7b56\u7565\u6548\u679c\u663e\u8457\uff0c\u4f46CB\u7b56\u7565\u7684\u4e2a\u6027\u5316\u6539\u8fdb\u6709\u9650\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5728\u7ebf\u8f85\u5bfc\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5b66\u751f\u7b54\u9519\u95ee\u9898\u540e\u63d0\u4f9b\u7684\u53cd\u9988\uff0c\u4ee5\u4f18\u5316\u5b66\u751f\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u548c\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u6765\u5b66\u4e60\u6709\u6548\u53cd\u9988\u7684\u5728\u7ebf\u8f85\u5bfc\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u4f7f\u7528\u4e00\u767e\u4e07\u5b66\u751f\u6570\u636e\u8bc4\u4f30\u4e8643,000\u79cd\u8f85\u52a9\u64cd\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b97\u6cd5\u6765\u4f18\u5316\u5b66\u751f\u5b66\u4e60\u548c\u7ec3\u4e60\u4f1a\u8bdd\u8868\u73b0\u3002\u6b64\u5916\uff0c\u8fd8\u5229\u7528\u56e0\u679c\u63a8\u65ad\u7814\u7a76\u4e86\u4e2a\u6027\u5316\u53cd\u9988\uff08\u4e0a\u4e0b\u6587\u8001\u864e\u673a CB\uff09\u4e0e\u6574\u4f53\u53cd\u9988\uff08MAB\uff09\u7684\u5bf9\u6bd4\u3002", "result": "\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0cMAB\u7b56\u7565\u5728166,000\u6b21\u7ec3\u4e60\u4f1a\u8bdd\u4e2d\u663e\u8457\u6539\u5584\u4e86\u5b66\u751f\u6210\u679c\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u53d1\u73b0\u4e86\u4e00\u4e9b\u8f85\u52a9\u64cd\u4f5c\u5bf9\u67d0\u4e9b\u95ee\u9898\u5b58\u5728\u5f02\u8d28\u6027\u6548\u5e94\uff0c\u4f46CB\u7b56\u7565\u76f8\u6bd4\u4e8eMAB\u7b56\u7565\u4ec5\u80fd\u5e26\u6765\u8fb9\u9645\u6539\u8fdb\uff0c\u56e0\u4e3a\u6548\u679c\u89c4\u6a21\u6709\u9650\u3002", "conclusion": "\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u7b56\u7565\u5728\u6574\u4f53\u5b66\u751f\u7fa4\u4f53\u4e2d\u4f18\u5316\u53cd\u9988\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u800c\u4e0a\u4e0b\u6587\u8001\u864e\u673a\uff08CB\uff09\u7b56\u7565\u5728\u4e2a\u6027\u5316\u53cd\u9988\u4ee5\u63d0\u9ad8\u7ed3\u679c\u65b9\u9762\uff0c\u4ec5\u80fd\u5e26\u6765\u8fb9\u9645\u6539\u8fdb\uff0c\u56e0\u4e3a\u4e2a\u4f53\u5b66\u751f\u884c\u4e3a\u5f02\u8d28\u6027\u7684\u6548\u679c\u89c4\u6a21\u6709\u9650\u3002"}}
{"id": "2508.00489", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00489", "abs": "https://arxiv.org/abs/2508.00489", "authors": ["Yixuan Tang", "Jincheng Wang", "Anthony K. H. Tung"], "title": "The Missing Parts: Augmenting Fact Verification with Half-Truth Detection", "comment": null, "summary": "Fact verification systems typically assess whether a claim is supported by\nretrieved evidence, assuming that truthfulness depends solely on what is\nstated. However, many real-world claims are half-truths, factually correct yet\nmisleading due to the omission of critical context. Existing models struggle\nwith such cases, as they are not designed to reason about what is left unsaid.\nWe introduce the task of half-truth detection, and propose PolitiFact-Hidden, a\nnew benchmark with 15k political claims annotated with sentence-level evidence\nalignment and inferred claim intent. To address this challenge, we present\nTRACER, a modular re-assessment framework that identifies omission-based\nmisinformation by aligning evidence, inferring implied intent, and estimating\nthe causal impact of hidden content. TRACER can be integrated into existing\nfact-checking pipelines and consistently improves performance across multiple\nstrong baselines. Notably, it boosts Half-True classification F1 by up to 16\npoints, highlighting the importance of modeling omissions for trustworthy fact\nverification.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86 TRACER \u6846\u67b6\u6765\u68c0\u6d4b\u9057\u6f0f\u5173\u952e\u4fe1\u606f\u800c\u4ea7\u751f\u7684\u8bef\u5bfc\u6027\u58f0\u660e\uff08\u534a\u771f\u5b9e\u6027\uff09\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6 PolitiFact-Hidden\u3002TRACER \u901a\u8fc7\u5bf9\u9f50\u8bc1\u636e\u3001\u63a8\u65ad\u610f\u56fe\u548c\u8bc4\u4f30\u9690\u85cf\u5185\u5bb9\u7684\u5f71\u54cd\u6765\u8bc6\u522b\u6b64\u7c7b\u9519\u8bef\u4fe1\u606f\uff0c\u5e76\u80fd\u6709\u6548\u63d0\u5347\u73b0\u6709\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u8bc1\u636e\u662f\u5426\u652f\u6301\u58f0\u660e\uff0c\u800c\u5ffd\u89c6\u4e86\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u58f0\u660e\u867d\u7136\u5728\u4e8b\u5b9e\u5c42\u9762\u6b63\u786e\u4f46\u53ef\u80fd\u56e0\u9057\u6f0f\u5173\u952e\u4fe1\u606f\u800c\u4ea7\u751f\u8bef\u5bfc\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u534a\u771f\u5b9e\u6027\u68c0\u6d4b\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a PolitiFact-Hidden \u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b 15k \u4e2a\u653f\u6cbb\u58f0\u660e\u53ca\u5176\u53e5\u5b50\u7ea7\u8bc1\u636e\u5bf9\u9f50\u548c\u63a8\u65ad\u7684\u58f0\u660e\u610f\u56fe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a TRACER \u7684\u6a21\u5757\u5316\u91cd\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5bf9\u8bc1\u636e\u8fdb\u884c\u5bf9\u9f50\u3001\u63a8\u65ad\u9690\u542b\u610f\u56fe\u4ee5\u53ca\u4f30\u8ba1\u9690\u85cf\u5185\u5bb9\u7684\u56e0\u679c\u5f71\u54cd\u6765\u8bc6\u522b\u57fa\u4e8e\u9057\u6f0f\u7684\u9519\u8bef\u4fe1\u606f\u3002", "result": "TRACER \u6846\u67b6\u53ef\u4ee5\u6574\u5408\u5230\u73b0\u6709\u7684\u4e8b\u5b9e\u6838\u67e5\u6d41\u7a0b\u4e2d\uff0c\u5e76\u80fd\u5728\u591a\u4e2a\u5f3a\u57fa\u7ebf\u4e0a\u6301\u7eed\u63d0\u9ad8\u6027\u80fd\uff0c\u7279\u522b\u662f\u5c06 Half-True \u5206\u7c7b\u7684 F1 \u5206\u6570\u63d0\u9ad8\u4e86 16 \u4e2a\u70b9\u3002", "conclusion": "TRACER \u6846\u67b6\u80fd\u591f\u8bc6\u522b\u57fa\u4e8e\u9057\u6f0f\u7684\u9519\u8bef\u4fe1\u606f\uff0c\u5e76\u80fd\u6574\u5408\u5230\u73b0\u6709\u7684\u4e8b\u5b9e\u6838\u67e5\u6d41\u7a0b\u4e2d\uff0c\u5728\u591a\u4e2a\u5f3a\u57fa\u7ebf\u4e0a\u6301\u7eed\u63d0\u9ad8\u6027\u80fd\uff0c\u7279\u522b\u662f\u5c06 Half-True \u5206\u7c7b\u7684 F1 \u5206\u6570\u63d0\u9ad8\u4e86 16 \u4e2a\u70b9\uff0c\u51f8\u663e\u4e86\u5728\u53ef\u4fe1\u4e8b\u5b9e\u6838\u67e5\u4e2d\u5bf9\u9057\u6f0f\u4fe1\u606f\u8fdb\u884c\u5efa\u6a21\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00658", "categories": ["cs.AI", "cs.LG", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.00658", "abs": "https://arxiv.org/abs/2508.00658", "authors": ["Chakattrai Sookkongwaree", "Tattep Lakmuang", "Chainarong Amornbunchornvej"], "title": "Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies", "comment": "First draft", "summary": "Understanding causal relationships in time series is fundamental to many\ndomains, including neuroscience, economics, and behavioral science. Granger\ncausality is one of the well-known techniques for inferring causality in time\nseries. Typically, Granger causality frameworks have a strong fix-lag\nassumption between cause and effect, which is often unrealistic in complex\nsystems. While recent work on variable-lag Granger causality (VLGC) addresses\nthis limitation by allowing a cause to influence an effect with different time\nlags at each time point, it fails to account for the fact that causal\ninteractions may vary not only in time delay but also across frequency bands.\nFor example, in brain signals, alpha-band activity may influence another region\nwith a shorter delay than slower delta-band oscillations. In this work, we\nformalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a\nnovel framework that generalizes traditional VLGC by explicitly modeling\nfrequency-dependent causal delays. We provide a formal definition of MB-VLGC,\ndemonstrate its theoretical soundness, and propose an efficient inference\npipeline. Extensive experiments across multiple domains demonstrate that our\nframework significantly outperforms existing methods on both synthetic and\nreal-world datasets, confirming its broad applicability to any type of time\nseries data. Code and datasets are publicly available.", "AI": {"tldr": "MB-VLGC\u901a\u8fc7\u8003\u8651\u9891\u7387\u4f9d\u8d56\u6027\u6765\u6539\u8fdb\u683c\u5170\u662f\u56e0\u679c\u5173\u7cfb\uff0c\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u7684\u683c\u5170\u662f\u56e0\u679c\u5173\u7cfb\u6846\u67b6\u901a\u5e38\u5b58\u5728\u56fa\u5b9a\u7684\u6ede\u540e\u5047\u8bbe\uff0c\u8fd9\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u5f80\u5f80\u4e0d\u5207\u5b9e\u9645\u3002\u867d\u7136\u6700\u8fd1\u7684\u53ef\u53d8\u5ef6\u8fdf\u683c\u5170\u662f\u56e0\u679c\u5173\u7cfb\uff08VLGC\uff09\u89e3\u51b3\u4e86\u8fd9\u4e2a\u9650\u5236\uff0c\u4f46\u5b83\u672a\u80fd\u8003\u8651\u5230\u56e0\u679c\u76f8\u4e92\u4f5c\u7528\u4e0d\u4ec5\u5728\u65f6\u95f4\u5ef6\u8fdf\u4e0a\uff0c\u800c\u4e14\u5728\u9891\u7387\u5e26\u4e0a\u4e5f\u4f1a\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u9891\u6bb5\u53ef\u53d8\u5ef6\u8fdf\u683c\u5170\u662f\u56e0\u679c\u5173\u7cfb\uff08MB-VLGC\uff09\u7684\u65b0\u578b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u660e\u786e\u5efa\u6a21\u4f9d\u8d56\u4e8e\u9891\u7387\u7684\u56e0\u679c\u5ef6\u8fdf\u6765\u6cdb\u5316\u4f20\u7edf\u7684VLGC\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u63a8\u7406\u6d41\u7a0b\u3002", "result": "\u8be5\u6846\u67b6\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u591a\u4e2a\u9886\u57df\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u5b9e\u4e86\u5176\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002", "conclusion": "MB-VLGC\u6846\u67b6\u901a\u8fc7\u660e\u786e\u5efa\u6a21\u4f9d\u8d56\u4e8e\u9891\u7387\u7684\u56e0\u679c\u5ef6\u8fdf\uff0c\u5bf9\u4f20\u7edfVLGC\u8fdb\u884c\u4e86\u6cdb\u5316\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u591a\u4e2a\u9886\u57df\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u5b9e\u4e86\u5176\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.00416", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00416", "abs": "https://arxiv.org/abs/2508.00416", "authors": ["Dekel Zak", "Jingyi Mei", "Jean-Marie Lagniez", "Alfons Laarman"], "title": "Reducing Quantum Circuit Synthesis to #SAT", "comment": null, "summary": "Quantum circuit synthesis is the task of decomposing a given quantum operator\ninto a sequence of elementary quantum gates. Since the finite target gate set\ncannot exactly implement any given operator, approximation is often necessary.\nModel counting, or #SAT, has recently been demonstrated as a promising new\napproach for tackling core problems in quantum circuit analysis. In this work,\nwe show for the first time that the universal quantum circuit synthesis problem\ncan be reduced to maximum model counting. We formulate a #SAT encoding for\nexact and approximate depth-optimal quantum circuit synthesis into the\nClifford+T gate set. We evaluate our method with an open-source implementation\nthat uses the maximum model counter d4Max as a backend. For this purpose, we\nextended d4Max with support for complex and negative weights to represent\namplitudes. Experimental results show that existing classical tools have\npotential for the quantum circuit synthesis problem.", "AI": {"tldr": "\u5c06\u91cf\u5b50\u7535\u8def\u5408\u6210\u95ee\u9898\u89c4\u7ea6\u4e3a\u6700\u5927\u6a21\u578b\u8ba1\u6570\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u6539\u8fdb\u7684\u7ecf\u5178\u5de5\u5177d4Max\u8fdb\u884c\u6c42\u89e3\uff0c\u7ed3\u679c\u663e\u793a\u7ecf\u5178\u5de5\u5177\u5728\u6b64\u95ee\u9898\u4e0a\u5177\u6709\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22\u4f7f\u7528\u6a21\u578b\u8ba1\u6570\uff08#SAT\uff09\u65b9\u6cd5\u89e3\u51b3\u91cf\u5b50\u7535\u8def\u5206\u6790\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u7279\u522b\u662f\u91cf\u5b50\u7535\u8def\u5408\u6210\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u91cf\u5b50\u7535\u8def\u5408\u6210\u95ee\u9898\uff08\u5305\u62ec\u7cbe\u786e\u548c\u8fd1\u4f3c\u7684\u6df1\u5ea6\u6700\u4f18\u5408\u6210\uff09\u89c4\u7ea6\u4e3a\u6700\u5927\u6a21\u578b\u8ba1\u6570\u95ee\u9898\uff08#SAT\uff09\uff0c\u5e76\u4f7f\u7528d4Max\u4f5c\u4e3a\u540e\u7aef\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u7684\u7ecf\u5178\u5de5\u5177\uff08\u7279\u522b\u662fd4Max\uff0c\u5df2\u6269\u5c55\u652f\u6301\u590d\u6570\u548c\u8d1f\u6570\u6743\u91cd\uff09\u5728\u89e3\u51b3\u91cf\u5b50\u7535\u8def\u5408\u6210\u95ee\u9898\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u8868\u660e\uff0c\u901a\u7528\u91cf\u5b50\u7535\u8def\u5408\u6210\u95ee\u9898\u53ef\u4ee5\u5f52\u7ea6\u5230\u6700\u5927\u6a21\u578b\u8ba1\u6570\u95ee\u9898\uff0c\u5e76\u4e3a\u7cbe\u786e\u548c\u8fd1\u4f3c\u6df1\u5ea6\u6700\u4f18\u91cf\u5b50\u7535\u8def\u5408\u6210\u5230Clifford+T\u95e8\u96c6\u63d0\u4f9b\u4e86#SAT\u7f16\u7801\u3002"}}
{"id": "2508.00311", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00311", "abs": "https://arxiv.org/abs/2508.00311", "authors": ["Yufeng Zhong", "Zhixiong Zeng", "Lei Chen", "Longrong Yang", "Liming Zheng", "Jing Huang", "Siqi Yang", "Lin Ma"], "title": "DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios", "comment": null, "summary": "Optical Character Recognition (OCR) for mathematical formula is essential for\nthe intelligent analysis of scientific literature. However, both task-specific\nand general vision-language models often struggle to handle the structural\ndiversity, complexity, and real-world variability inherent in mathematical\ncontent. In this work, we present DocTron-Formula, a unified framework built\nupon general vision-language models, thereby eliminating the need for\nspecialized architectures. Furthermore, we introduce CSFormula, a large-scale\nand challenging dataset that encompasses multidisciplinary and structurally\ncomplex formulas at the line, paragraph, and page levels. Through\nstraightforward supervised fine-tuning, our approach achieves state-of-the-art\nperformance across a variety of styles, scientific domains, and complex\nlayouts. Experimental results demonstrate that our method not only surpasses\nspecialized models in terms of accuracy and robustness, but also establishes a\nnew paradigm for the automated understanding of complex scientific documents.", "AI": {"tldr": "DocTron-Formula \u662f\u4e00\u4e2a\u57fa\u4e8e\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684OCR\u6846\u67b6\uff0c\u914d\u5408CSFormula\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u79d1\u5b66\u6587\u732e\u4e2d\u6570\u5b66\u516c\u5f0f\u8bc6\u522b\u7684\u6311\u6218\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\uff08OCR\uff09\u6280\u672f\u5728\u5904\u7406\u6570\u5b66\u516c\u5f0f\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u6570\u5b66\u5185\u5bb9\u5177\u6709\u7ed3\u6784\u591a\u6837\u6027\u3001\u590d\u6742\u6027\u548c\u771f\u5b9e\u4e16\u754c\u7684\u53d8\u5f02\u6027\u3002\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e5f\u96be\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DocTron-Formula \u7684\u7edf\u4e00\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u901a\u7528\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u4e13\u95e8\u7684\u67b6\u6784\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a CSFormula \u7684\u5927\u89c4\u6a21\u3001\u591a\u9886\u57df\u3001\u7ed3\u6784\u590d\u6742\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u884c\u3001\u6bb5\u843d\u548c\u9875\u9762\u7ea7\u522b\u7684\u516c\u5f0f\u3002", "result": "DocTron-Formula \u5728\u5404\u79cd\u98ce\u683c\u3001\u79d1\u5b66\u9886\u57df\u548c\u590d\u6742\u5e03\u5c40\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5176\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u5747\u4f18\u4e8e\u4e13\u95e8\u7684\u6a21\u578b\uff0c\u4e3a\u590d\u6742\u79d1\u5b66\u6587\u6863\u7684\u81ea\u52a8\u7406\u89e3\u6811\u7acb\u4e86\u65b0\u8303\u4f8b\u3002", "conclusion": "DocTron-Formula \u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5b83\u5efa\u7acb\u5728\u901a\u7528\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e4b\u4e0a\uff0c\u65e0\u9700\u4e13\u95e8\u7684\u67b6\u6784\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7b80\u5355\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u5728\u5404\u79cd\u98ce\u683c\u3001\u79d1\u5b66\u9886\u57df\u548c\u590d\u6742\u5e03\u5c40\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4e13\u95e8\u7684\u6a21\u578b\uff0c\u5e76\u4e3a\u590d\u6742\u79d1\u5b66\u6587\u6863\u7684\u81ea\u52a8\u7406\u89e3\u6811\u7acb\u4e86\u65b0\u8303\u4f8b\u3002"}}
{"id": "2508.00286", "categories": ["cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.00286", "abs": "https://arxiv.org/abs/2508.00286", "authors": ["Mohsen Zaker Esteghamati"], "title": "Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem", "comment": null, "summary": "This study presents a methodology to treat performance-based seismic design\nas an inverse engineering problem, where design parameters are directly derived\nto achieve specific performance objectives. By implementing explainable machine\nlearning models, this methodology directly maps design variables and\nperformance metrics, tackling computational inefficiencies of performance-based\ndesign. The resultant machine learning model is integrated as an evaluation\nfunction into a genetic optimization algorithm to solve the inverse problem.\nThe developed methodology is then applied to two different inventories of steel\nand concrete moment frames in Los Angeles and Charleston to obtain sectional\nproperties of frame members that minimize expected annualized seismic loss in\nterms of repair costs. The results show high accuracy of the surrogate models\n(e.g., R2> 90%) across a diverse set of building types, geometries, seismic\ndesign, and site hazard, where the optimization algorithm could identify the\noptimum values of members' properties for a fixed set of geometric variables,\nconsistent with engineering principles.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.00522", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00522", "abs": "https://arxiv.org/abs/2508.00522", "authors": ["Jiaxin Deng", "Qingcheng Zhu", "Junbiao Pang", "Linlin Yang", "Zhongqian Fu", "Baochang Zhang"], "title": "EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond", "comment": null, "summary": "Little research explores the correlation between the expressive ability and\ngeneralization ability of the low-rank adaptation (LoRA). Sharpness-Aware\nMinimization (SAM) improves model generalization for both Convolutional Neural\nNetworks (CNNs) and Transformers by encouraging convergence to locally flat\nminima. However, the connection between sharpness and generalization has not\nbeen fully explored for LoRA due to the lack of tools to either empirically\nseek flat minima or develop theoretical methods. In this work, we propose\nFlat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for\nLoRA. Concretely, we theoretically demonstrate that perturbations in the full\nparameter space can be transferred to the low-rank subspace. This approach\neliminates the potential interference introduced by perturbations across\nmultiple matrices in the low-rank subspace. Our extensive experiments on large\nlanguage models and vision-language models demonstrate that EFlat-LoRA achieves\noptimize efficiency comparable to that of LoRA while simultaneously attaining\ncomparable or even better performance. For example, on the GLUE dataset with\nRoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and\n0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat\nshows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,\nrespectively. These empirical results also verify that the generalization of\nLoRA is closely related to sharpness, which is omitted by previous methods.", "AI": {"tldr": "Flat-LoRA\u548cEFlat-LoRA\u901a\u8fc7\u5bfb\u627eLoRA\u7684\u5e73\u5766\u6700\u5c0f\u533a\u57df\u6765\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u4f18\u4e8eLoRA\u548c\u5168\u53c2\u6570\u5fae\u8c03\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u7684\u53ef\u8868\u8fbe\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u4ee5\u53ca\u89e3\u51b3\u4ee5\u5f80\u7814\u7a76\u4e2d\u7f3a\u4e4f\u5bfb\u627e\u5e73\u5766\u6700\u5c0f\u533a\u57df\u7684\u5de5\u5177\u6216\u7406\u8bba\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86Flat-LoRA\u548cEFlat-LoRA\u6765\u5bfb\u627eLoRA\u7684\u5e73\u5766\u6700\u5c0f\u533a\u57df\uff0c\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5168\u53c2\u6570\u7a7a\u95f4\u7684\u6270\u52a8\u53ef\u4ee5\u8f6c\u79fb\u5230\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u4ece\u800c\u907f\u514d\u4e86\u4f4e\u79e9\u5b50\u7a7a\u95f4\u4e2d\u591a\u4e2a\u6270\u52a8\u77e9\u9635\u7684\u6f5c\u5728\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cEFlat-LoRA\u5728\u4f18\u5316\u6548\u7387\u4e0a\u4e0eLoRA\u76f8\u5f53\uff0c\u540c\u65f6\u5728\u6027\u80fd\u4e0a\u4e5f\u80fd\u8fbe\u5230\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6c34\u5e73\u3002\u4f8b\u5982\uff0c\u5728GLUE\u6570\u636e\u96c6\u4e0a\uff0cEFlat-LoRA\u76f8\u6bd4LoRA\u548c\u5168\u53c2\u6570\u5fae\u8c03\u5e73\u5747\u5206\u522b\u63d0\u9ad8\u4e861.0%\u548c0.5%\uff1b\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65b9\u9762\uff0cQwen-VL-Chat\u5728SQA\u548cVizWiz\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u5206\u522b\u63d0\u9ad8\u4e861.5%\u548c1.0%\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660eLoRA\u7684\u6cdb\u5316\u80fd\u529b\u4e0esharpness\u5bc6\u5207\u76f8\u5173\uff0c\u800c\u8fd9\u88ab\u4e4b\u524d\u7684\u7814\u7a76\u65b9\u6cd5\u6240\u5ffd\u7565\u3002"}}
{"id": "2508.00665", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00665", "abs": "https://arxiv.org/abs/2508.00665", "authors": ["Maryam Mosleh", "Marie Devlin", "Ellis Solaiman"], "title": "Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI", "comment": null, "summary": "Artificial intelligence-driven adaptive learning systems are reshaping\neducation through data-driven adaptation of learning experiences. Yet many of\nthese systems lack transparency, offering limited insight into how decisions\nare made. Most explainable AI (XAI) techniques focus on technical outputs but\nneglect user roles and comprehension. This paper proposes a hybrid framework\nthat integrates traditional XAI techniques with generative AI models and user\npersonalisation to generate multimodal, personalised explanations tailored to\nuser needs. We redefine explainability as a dynamic communication process\ntailored to user roles and learning goals. We outline the framework's design,\nkey XAI limitations in education, and research directions on accuracy,\nfairness, and personalisation. Our aim is to move towards explainable AI that\nenhances transparency while supporting user-centred experiences.", "AI": {"tldr": "This paper introduces a new framework for AI in education that uses generative AI and personalization to make explanations understandable to different users, addressing the lack of transparency in current systems.", "motivation": "Existing AI-driven adaptive learning systems often lack transparency, providing limited insight into decision-making processes. Current XAI techniques primarily focus on technical outputs and neglect user roles and comprehension.", "method": "A hybrid framework that integrates traditional XAI techniques with generative AI models and user personalization to generate multimodal, personalized explanations tailored to user needs.", "result": "The paper outlines the framework's design, discusses key XAI limitations in educational contexts, and identifies research directions concerning accuracy, fairness, and personalization.", "conclusion": "The paper proposes a hybrid framework integrating traditional XAI techniques with generative AI and user personalization to create multimodal, personalized explanations for adaptive learning systems. It redefines explainability as a dynamic communication process tailored to user roles and learning goals, aiming to enhance transparency and support user-centered experiences."}}
{"id": "2508.00823", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00823", "abs": "https://arxiv.org/abs/2508.00823", "authors": ["Wenxuan Guo", "Xiuwei Xu", "Hang Yin", "Ziwei Wang", "Jianjiang Feng", "Jie Zhou", "Jiwen Lu"], "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation", "comment": "Accepted to ICCV 2025. Project page:\n  https://gwxuan.github.io/IGL-Nav/", "summary": "Visual navigation with an image as goal is a fundamental and challenging\nproblem. Conventional methods either rely on end-to-end RL learning or\nmodular-based policy with topological graph or BEV map as memory, which cannot\nfully model the geometric relationship between the explored 3D environment and\nthe goal image. In order to efficiently and accurately localize the goal image\nin 3D space, we build our navigation system upon the renderable 3D gaussian\n(3DGS) representation. However, due to the computational intensity of 3DGS\noptimization and the large search space of 6-DoF camera pose, directly\nleveraging 3DGS for image localization during agent exploration process is\nprohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D\nGaussian Localization framework for efficient and 3D-aware image-goal\nnavigation. Specifically, we incrementally update the scene representation as\nnew images arrive with feed-forward monocular prediction. Then we coarsely\nlocalize the goal by leveraging the geometric information for discrete space\nmatching, which can be equivalent to efficient 3D convolution. When the agent\nis close to the goal, we finally solve the fine target pose with optimization\nvia differentiable rendering. The proposed IGL-Nav outperforms existing\nstate-of-the-art methods by a large margin across diverse experimental\nconfigurations. It can also handle the more challenging free-view image-goal\nsetting and be deployed on real-world robotic platform using a cellphone to\ncapture goal image at arbitrary pose. Project page:\nhttps://gwxuan.github.io/IGL-Nav/.", "AI": {"tldr": "IGL-Nav improves 3D-aware image-goal navigation by incrementally updating scene representation with feed-forward monocular prediction and using 3D Gaussian localization for efficient and accurate goal localization, outperforming existing methods and handling challenging scenarios.", "motivation": "Conventional methods for visual navigation with an image as goal either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. Direct leveraging of 3DGS for image localization during agent exploration process is prohibitively inefficient due to computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose.", "method": "IGL-Nav incrementally updates the scene representation as new images arrive with feed-forward monocular prediction. It coarsely localizes the goal by leveraging the geometric information for discrete space matching, and finally solves the fine target pose with optimization via differentiable rendering.", "result": "The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose.", "conclusion": "IGL-Nav outperformed existing state-of-the-art methods by a large margin across diverse experimental configurations, can handle the more challenging free-view image-goal setting, and be deployed on real-world robotic platform using a cellphone."}}
{"id": "2508.00312", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00312", "abs": "https://arxiv.org/abs/2508.00312", "authors": ["Suhang Cai", "Xiaohao Peng", "Chong Wang", "Xiaojie Cai", "Jiangbo Qian"], "title": "GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection", "comment": null, "summary": "Video anomaly detection (VAD) plays a critical role in public safety\napplications such as intelligent surveillance. However, the rarity,\nunpredictability, and high annotation cost of real-world anomalies make it\ndifficult to scale VAD datasets, which limits the performance and\ngeneralization ability of existing models. To address this challenge, we\npropose a generative video-enhanced weakly-supervised video anomaly detection\n(GV-VAD) framework that leverages text-conditioned video generation models to\nproduce semantically controllable and physically plausible synthetic videos.\nThese virtual videos are used to augment training data at low cost. In\naddition, a synthetic sample loss scaling strategy is utilized to control the\ninfluence of generated synthetic samples for efficient training. The\nexperiments show that the proposed framework outperforms state-of-the-art\nmethods on UCF-Crime datasets. The code is available at\nhttps://github.com/Sumutan/GV-VAD.git.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\u6765\u589e\u5f3a\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u7a00\u7f3a\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5f02\u5e38\u4e8b\u4ef6\u7684\u7a00\u6709\u6027\u3001\u4e0d\u53ef\u9884\u6d4b\u6027\u548c\u9ad8\u6602\u7684\u6807\u6ce8\u6210\u672c\u9650\u5236\u4e86\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff08VAD\uff09\u6570\u636e\u96c6\u7684\u6269\u5c55\uff0c\u8fdb\u800c\u5f71\u54cd\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5f0f\u89c6\u9891\u589e\u5f3a\u7684\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff08GV-VAD\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6587\u672c\u6761\u4ef6\u89c6\u9891\u751f\u6210\u6a21\u578b\u6765\u751f\u6210\u8bed\u4e49\u53ef\u63a7\u3001\u7269\u7406\u4e0a\u5408\u7406\u7684\u5408\u6210\u89c6\u9891\uff0c\u4ee5\u4f4e\u6210\u672c\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u91c7\u7528\u5408\u6210\u6837\u672c\u635f\u5931\u7f29\u653e\u7b56\u7565\u6765\u63a7\u5236\u751f\u6210\u6837\u672c\u7684\u5f71\u54cd\u4ee5\u8fdb\u884c\u6709\u6548\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u5728UCF-Crime\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684GV-VAD\u6846\u67b6\u5728UCF-Crime\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.00304", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00304", "abs": "https://arxiv.org/abs/2508.00304", "authors": ["Tianyin Liao", "Ziwei Zhang", "Yufei Sun", "Chunyu Hu", "Jianxin Li"], "title": "Invariant Graph Transformer for Out-of-Distribution Generalization", "comment": null, "summary": "Graph Transformers (GTs) have demonstrated great effectiveness across various\ngraph analytical tasks. However, the existing GTs focus on training and testing\ngraph data originated from the same distribution, but fail to generalize under\ndistribution shifts. Graph invariant learning, aiming to capture generalizable\ngraph structural patterns with labels under distribution shifts, is potentially\na promising solution, but how to design attention mechanisms and positional and\nstructural encodings (PSEs) based on graph invariant learning principles\nremains challenging. To solve these challenges, we introduce Graph\nOut-Of-Distribution generalized Transformer (GOODFormer), aiming to learn\ngeneralized graph representations by capturing invariant relationships between\npredictive graph structures and labels through jointly optimizing three\nmodules. Specifically, we first develop a GT-based entropy-guided invariant\nsubgraph disentangler to separate invariant and variant subgraphs while\npreserving the sharpness of the attention function. Next, we design an evolving\nsubgraph positional and structural encoder to effectively and efficiently\ncapture the encoding information of dynamically changing subgraphs during\ntraining. Finally, we propose an invariant learning module utilizing subgraph\nnode representations and encodings to derive generalizable graph\nrepresentations that can to unseen graphs. We also provide theoretical\njustifications for our method. Extensive experiments on benchmark datasets\ndemonstrate the superiority of our method over state-of-the-art baselines under\ndistribution shifts.", "AI": {"tldr": "GOODFormer\u901a\u8fc7\u7ed3\u5408\u71b5\u5f15\u5bfc\u5b50\u56fe\u5206\u89e3\u3001\u6f14\u5316\u5b50\u56fe\u7f16\u7801\u548c\u4e0d\u53d8\u6027\u5b66\u4e60\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u56fe Transformer \u5728\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u56fe Transformer\uff08GTs\uff09\u4e3b\u8981\u5173\u6ce8\u5728\u76f8\u540c\u5206\u5e03\u7684\u56fe\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u5728\u5206\u5e03\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u56fe\u4e0d\u53d8\u6027\u5b66\u4e60\u65e8\u5728\u6355\u6349\u6807\u7b7e\u4e0b\u5177\u6709\u5206\u5e03\u53d8\u5316\u6cdb\u5316\u80fd\u529b\u7684\u56fe\u7ed3\u6784\u6a21\u5f0f\uff0c\u662f\u4e00\u4e2a\u6709\u6f5c\u529b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5982\u4f55\u57fa\u4e8e\u56fe\u4e0d\u53d8\u6027\u5b66\u4e60\u539f\u7406\u8bbe\u8ba1\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u53ca\u4f4d\u7f6e\u548c\u7ed3\u6784\u7f16\u7801\uff08PSE\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "GOODFormer\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4e09\u4e2a\u6a21\u5757\u6765\u5b66\u4e60\u901a\u7528\u7684\u56fe\u8868\u793a\uff1a1. \u71b5\u5f15\u5bfc\u4e0d\u53d8\u5b50\u56fe\u5206\u89e3\u5668\uff0c\u7528\u4e8e\u5206\u79bb\u4e0d\u53d8\u548c\u53d8\u5316\u7684\u5b50\u56fe\uff0c\u540c\u65f6\u4fdd\u6301\u6ce8\u610f\u529b\u51fd\u6570\u7684\u6e05\u6670\u5ea6\u30022. \u6f14\u5316\u5b50\u56fe\u4f4d\u7f6e\u548c\u7ed3\u6784\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u6709\u6548\u4e14\u9ad8\u6548\u5730\u6355\u83b7\u52a8\u6001\u53d8\u5316\u7684\u5b50\u56fe\u7684\u7f16\u7801\u4fe1\u606f\u30023. \u4e0d\u53d8\u6027\u5b66\u4e60\u6a21\u5757\uff0c\u5229\u7528\u5b50\u56fe\u8282\u70b9\u8868\u793a\u548c\u7f16\u7801\u6765\u63a8\u5bfc\u53ef\u6cdb\u5316\u7684\u56fe\u8868\u793a\u3002", "result": "GOODFormer\u80fd\u591f\u5b66\u4e60\u53ef\u6cdb\u5316\u5230\u672a\u89c1\u56fe\u7684\u56fe\u8868\u793a\u3002", "conclusion": "GOODFormer\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u4f18\u8d8a\u6027\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002"}}
{"id": "2508.00537", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00537", "abs": "https://arxiv.org/abs/2508.00537", "authors": ["Giulio Zhou", "Tsz Kin Lam", "Alexandra Birch", "Barry Haddow"], "title": "The Prosody of Emojis", "comment": null, "summary": "Prosodic features such as pitch, timing, and intonation are central to spoken\ncommunication, conveying emotion, intent, and discourse structure. In\ntext-based settings, where these cues are absent, emojis act as visual\nsurrogates that add affective and pragmatic nuance. This study examines how\nemojis influence prosodic realisation in speech and how listeners interpret\nprosodic cues to recover emoji meanings. Unlike previous work, we directly link\nprosody and emoji by analysing actual human speech data, collected through\nstructured but open-ended production and perception tasks. This provides\nempirical evidence of how emoji semantics shape spoken delivery and perception.\nResults show that speakers adapt their prosody based on emoji cues, listeners\ncan often identify the intended emoji from prosodic variation alone, and\ngreater semantic differences between emojis correspond to increased prosodic\ndivergence. These findings suggest that emojis can act as meaningful carriers\nof prosodic intent, offering insight into their communicative role in digitally\nmediated contexts.", "AI": {"tldr": "Emojis affect speech prosody; listeners can infer emojis from speech, and different emojis have distinct prosodic markers.", "motivation": "This study examines how emojis influence prosodic realization in speech and how listeners interpret prosodic cues to recover emoji meanings, directly linking prosody and emoji.", "method": "The study analyzed actual human speech data, collected through structured but open-ended production and perception tasks.", "result": "Speakers adapt their prosody based on emoji cues, listeners can often identify the intended emoji from prosodic variation alone, and greater semantic differences between emojis correspond to increased prosodic divergence.", "conclusion": "Emojis can act as meaningful carriers of prosodic intent, offering insight into their communicative role in digitally mediated contexts."}}
{"id": "2508.00674", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00674", "abs": "https://arxiv.org/abs/2508.00674", "authors": ["Banan Alkhateeb", "Ellis Solaiman"], "title": "Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations", "comment": null, "summary": "Social media platforms today strive to improve user experience through AI\nrecommendations, yet the value of such recommendations vanishes as users do not\nunderstand the reasons behind them. This issue arises because explainability in\nsocial media is general and lacks alignment with user-specific needs. In this\nvision paper, we outline a user-segmented and context-aware explanation layer\nby proposing a visual explanation system with diverse explanation methods. The\nproposed system is framed by the variety of user needs and contexts, showing\nexplanations in different visualized forms, including a technically detailed\nversion for AI experts and a simplified one for lay users. Our framework is the\nfirst to jointly adapt explanation style (visual vs. numeric) and granularity\n(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will\nvalidate its impact on decision-making and trust.", "AI": {"tldr": "AI\u63a8\u8350\u9700\u8981\u89e3\u91ca\uff0c\u4f46\u73b0\u6709\u89e3\u91ca\u8fc7\u4e8e\u7b3c\u7edf\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u89c6\u89c9\u89e3\u91ca\u7cfb\u7edf\uff0c\u80fd\u6839\u636e\u7528\u6237\u7c7b\u578b\uff08\u4e13\u5bb6/\u666e\u901a\u7528\u6237\uff09\u548c\u60c5\u5883\u8c03\u6574\u89e3\u91ca\u65b9\u5f0f\uff08\u89c6\u89c9/\u6570\u503c\uff09\uff0c\u4ee5\u63d0\u5347\u7528\u6237\u7406\u89e3\u548c\u4fe1\u4efb\u3002", "motivation": "\u5f53\u524d\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u7684AI\u63a8\u8350\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u7528\u6237\u4e0d\u7406\u89e3\u63a8\u8350\u539f\u56e0\uff0c\u5bfc\u81f4\u63a8\u8350\u4ef7\u503c\u4e0b\u964d\uff0c\u7528\u6237\u4f53\u9a8c\u53d7\u635f\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u89c6\u89c9\u89e3\u91ca\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5305\u542b\u591a\u79cd\u89e3\u91ca\u65b9\u6cd5\uff0c\u5e76\u80fd\u6839\u636e\u7528\u6237\u9700\u6c42\u548c\u60c5\u5883\u8c03\u6574\u89e3\u91ca\u98ce\u683c\uff08\u89c6\u89c9\u6216\u6570\u503c\uff09\u548c\u7c92\u5ea6\uff08\u4e13\u5bb6\u6216\u666e\u901a\u7528\u6237\uff09\u3002", "result": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u5355\u4e2a\u6d41\u7a0b\u4e2d\u540c\u65f6\u9002\u5e94\u89e3\u91ca\u98ce\u683c\uff08\u89c6\u89c9 vs. \u6570\u503c\uff09\u548c\u7c92\u5ea6\uff08\u4e13\u5bb6 vs. \u666e\u901a\u7528\u6237\uff09\u7684\u8c03\u6574\u3002\u4e00\u9879\u5305\u542b30\u540dX\u7528\u6237\u7684\u516c\u5f00\u8bd5\u70b9\u7814\u7a76\u5c06\u9a8c\u8bc1\u8be5\u7cfb\u7edf\u5bf9\u51b3\u7b56\u548c\u4fe1\u4efb\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u6237\u5206\u5c42\u3001\u60c5\u5883\u611f\u77e5\u7684\u89e3\u91ca\u5c42\uff0c\u5e76\u8f85\u4ee5\u4e00\u4e2a\u5305\u542b\u591a\u79cd\u89e3\u91ca\u65b9\u6cd5\u7684\u89c6\u89c9\u89e3\u91ca\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e2dAI\u63a8\u8350\u7f3a\u4e4f\u7528\u6237\u7406\u89e3\u7684\u95ee\u9898\u3002"}}
{"id": "2508.00448", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00448", "abs": "https://arxiv.org/abs/2508.00448", "authors": ["Yan-Ying Zhu", "Bin-Bin Cai", "Fei Gao", "Song Lin"], "title": "Quantum Key-Recovery Attacks on FBC Algorithm", "comment": null, "summary": "With the advancement of quantum computing, symmetric cryptography faces new\nchallenges from quantum attacks. These attacks are typically classified into\ntwo models: Q1 (classical queries) and Q2 (quantum superposition queries). In\nthis context, we present a comprehensive security analysis of the FBC algorithm\nconsidering quantum adversaries with different query capabilities. In the Q2\nmodel, we first design 4-round polynomial-time quantum distinguishers for FBC-F\nand FBC-KF structures, and then perform $r(r>6)$-round quantum key-recovery\nattacks. Our attacks require $O(2^{(2n(r-6)+3n)/2})$ quantum queries, reducing\nthe time complexity by a factor of $2^{4.5n}$ compared with quantum brute-force\nsearch, where $n$ denotes the subkey length. Moreover, we give a new 6-round\npolynomial-time quantum distinguisher for FBC-FK structure. Based on this, we\nconstruct an $r(r>6)$-round quantum key-recovery attack with complexity\n$O(2^{n(r-6)})$. Considering an adversary with classical queries and quantum\ncomputing capabilities, we demonstrate low-data quantum key-recovery attacks on\nFBC-KF/FK structures in the Q1 model. These attacks require only a constant\nnumber of plaintext-ciphertext pairs, then use the Grover algorithm to search\nthe intermediate states, thereby recovering all keys in $O(2^{n/2})$ time.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86FBC\u7b97\u6cd5\u5728\u91cf\u5b50\u8ba1\u7b97\u653b\u51fb\u4e0b\u7684\u5b89\u5168\u6027\u3002\u5728Q2\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u4e86\u6bd4\u91cf\u5b50\u66b4\u529b\u641c\u7d22\u66f4\u5feb\u7684\u5bc6\u94a5\u6062\u590d\u653b\u51fb\u3002\u5728Q1\u6a21\u578b\u4e2d\uff0c\u4f7f\u7528Grover\u7b97\u6cd5\u5b9e\u73b0\u4e86\u4f4e\u6570\u636e\u653b\u51fb\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u6280\u672f\u7684\u98de\u901f\u53d1\u5c55\uff0c\u4f20\u7edf\u7684\u5bf9\u79f0\u5bc6\u7801\u7b97\u6cd5\u9762\u4e34\u7740\u6765\u81ea\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u6f5c\u5728\u5a01\u80c1\u3002\u7406\u89e3\u5e76\u91cf\u5316\u8fd9\u79cd\u5a01\u80c1\u5bf9\u4e8e\u8bbe\u8ba1\u548c\u8bc4\u4f30\u62b5\u6297\u91cf\u5b50\u653b\u51fb\u7684\u5bc6\u7801\u7b97\u6cd5\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u6df1\u5165\u5206\u6790FBC\u7b97\u6cd5\u5728\u91cf\u5b50\u8ba1\u7b97\u73af\u5883\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5176\u5728\u4e24\u79cd\u4e3b\u8981\u91cf\u5b50\u653b\u51fb\u6a21\u578b\uff08Q1\u548cQ2\uff09\u4e0b\u7684\u8106\u5f31\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u7b97\u6cd5\u7684FBC\u5bc6\u7801\u5206\u6790\u65b9\u6cd5\u3002\u5728Q2\u6a21\u578b\u4e0b\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u9488\u5bf9FBC-F\u548cFBC-KF\u7ed3\u6784\u76844\u8f6e\u91cf\u5b50\u533a\u5206\u5668\uff0c\u5e76\u57fa\u4e8e\u6b64\u8fdb\u884c\u4e86r(r>6)\u8f6e\u91cf\u5b50\u5bc6\u94a5\u6062\u590d\u653b\u51fb\uff0c\u5176\u67e5\u8be2\u590d\u6742\u5ea6\u4e3aO(2^((2n(r-6)+3n)/2))\u3002\u540c\u65f6\uff0c\u6211\u4eec\u8fd8\u4e3aFBC-FK\u7ed3\u6784\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u76846\u8f6e\u91cf\u5b50\u533a\u5206\u5668\uff0c\u5e76\u5b9e\u73b0\u4e86r(r>6)\u8f6e\u91cf\u5b50\u5bc6\u94a5\u6062\u590d\u653b\u51fb\uff0c\u590d\u6742\u5ea6\u4e3aO(2^(n(r-6)))\u3002\u5728Q1\u6a21\u578b\u4e0b\uff0c\u6211\u4eec\u5229\u7528Grover\u7b97\u6cd5\u5bf9\u4e2d\u95f4\u72b6\u6001\u8fdb\u884c\u641c\u7d22\uff0c\u5b9e\u73b0\u4e86\u5bf9FBC-KF/FK\u7ed3\u6784\u7684\u4f4e\u6570\u636e\u91cf\u5b50\u5bc6\u94a5\u6062\u590d\u653b\u51fb\uff0c\u4ec5\u9700\u5e38\u6570\u5bf9\u660e\u6587-\u5bc6\u6587\u5373\u53ef\u5728O(2^(n/2))\u65f6\u95f4\u5185\u6062\u590d\u5bc6\u94a5\u3002", "result": "\u5728Q2\u6a21\u578b\u4e0b\uff0c\u9488\u5bf9FBC-F\u548cFBC-KF\u7ed3\u6784\uff0c\u672c\u6587\u6210\u529f\u5b9e\u73b0\u4e86r(r>6)\u8f6e\u91cf\u5b50\u5bc6\u94a5\u6062\u590d\u653b\u51fb\uff0c\u5176\u67e5\u8be2\u590d\u6742\u5ea6\u4e3aO(2^((2n(r-6)+3n)/2))\uff0c\u76f8\u6bd4\u91cf\u5b50\u66b4\u529b\u641c\u7d22\u67092^(4.5n)\u500d\u7684\u63d0\u5347\u3002\u5bf9\u4e8eFBC-FK\u7ed3\u6784\uff0c\u653b\u51fb\u590d\u6742\u5ea6\u4e3aO(2^(n(r-6)))\u3002\u5728Q1\u6a21\u578b\u4e0b\uff0c\u672c\u6587\u5b9e\u73b0\u4e86\u4ec5\u9700\u5e38\u6570\u5bf9\u660e\u6587-\u5bc6\u6587\u7684\u4f4e\u6570\u636e\u91cf\u5b50\u5bc6\u94a5\u6062\u590d\u653b\u51fb\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(2^(n/2))\u3002", "conclusion": "\u672c\u6587\u5bf9FBC\u7b97\u6cd5\u5728\u91cf\u5b50\u8ba1\u7b97\u80cc\u666f\u4e0b\u7684\u5bf9\u79f0\u5bc6\u7801\u5206\u6790\u8fdb\u884c\u4e86\u5168\u9762\u7684\u7814\u7a76\u3002\u9488\u5bf9\u4e0d\u540c\u7684\u91cf\u5b50\u653b\u51fb\u6a21\u578b\uff08Q1\u548cQ2\uff09\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u591a\u8f6e\u91cf\u5b50\u533a\u5206\u5668\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u91cf\u5b50\u5bc6\u94a5\u6062\u590d\u653b\u51fb\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728Q2\u6a21\u578b\u4e0b\uff0c\u6211\u4eec\u7684\u653b\u51fb\u5728\u67e5\u8be2\u590d\u6742\u5ea6\u4e0a\u76f8\u6bd4\u91cf\u5b50\u66b4\u529b\u641c\u7d22\u6709\u663e\u8457\u964d\u4f4e\uff1b\u5728Q1\u6a21\u578b\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u4f4e\u6570\u636e\u653b\u51fb\u4e5f\u5c55\u793a\u4e86FBC\u7b97\u6cd5\u5728\u9762\u5bf9\u91cf\u5b50\u8ba1\u7b97\u65f6\u7684\u6f5c\u5728\u5f31\u70b9\u3002"}}
{"id": "2508.00319", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00319", "abs": "https://arxiv.org/abs/2508.00319", "authors": ["Sunghyun Park", "Seokeon Choi", "Hyoungwoo Park", "Sungrack Yun"], "title": "Steering Guidance for Personalized Text-to-Image Diffusion Models", "comment": "ICCV 2025", "summary": "Personalizing text-to-image diffusion models is crucial for adapting the\npre-trained models to specific target concepts, enabling diverse image\ngeneration. However, fine-tuning with few images introduces an inherent\ntrade-off between aligning with the target distribution (e.g., subject\nfidelity) and preserving the broad knowledge of the original model (e.g., text\neditability). Existing sampling guidance methods, such as classifier-free\nguidance (CFG) and autoguidance (AG), fail to effectively guide the output\ntoward well-balanced space: CFG restricts the adaptation to the target\ndistribution, while AG compromises text alignment. To address these\nlimitations, we propose personalization guidance, a simple yet effective method\nleveraging an unlearned weak model conditioned on a null text prompt. Moreover,\nour method dynamically controls the extent of unlearning in a weak model\nthrough weight interpolation between pre-trained and fine-tuned models during\ninference. Unlike existing guidance methods, which depend solely on guidance\nscales, our method explicitly steers the outputs toward a balanced latent space\nwithout additional computational overhead. Experimental results demonstrate\nthat our proposed guidance can improve text alignment and target distribution\nfidelity, integrating seamlessly with various fine-tuning strategies.", "AI": {"tldr": "Personalization guidance is a new method for text-to-image diffusion models that balances subject fidelity and text editability by using an unlearned weak model and dynamic weight interpolation, outperforming existing methods like CFG and AG.", "motivation": "Fine-tuning pre-trained text-to-image diffusion models with few images introduces a trade-off between target distribution alignment (subject fidelity) and preserving the original model's broad knowledge (text editability). Existing methods like CFG and AG fail to effectively guide the output toward a well-balanced space.", "method": "Personalization guidance, which leverages an unlearned weak model conditioned on a null text prompt and dynamically controls the extent of unlearning through weight interpolation between pre-trained and fine-tuned models during inference.", "result": "Experimental results show that the proposed personalization guidance improves text alignment and target distribution fidelity, and integrates seamlessly with various fine-tuning strategies.", "conclusion": "Personalization guidance effectively balances target distribution alignment and preservation of the original model's broad knowledge, improving text alignment and target distribution fidelity without additional computational overhead."}}
{"id": "2508.00325", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.00325", "abs": "https://arxiv.org/abs/2508.00325", "authors": ["Yongquan Qu", "Matthieu Blanke", "Sara Shamekh", "Pierre Gentine"], "title": "PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models", "comment": null, "summary": "Earth system modeling presents a fundamental challenge in scientific\ncomputing: capturing complex, multiscale nonlinear dynamics in computationally\nefficient models while minimizing forecast errors caused by necessary\nsimplifications. Even the most powerful AI- or physics-based forecast system\nsuffer from gradual error accumulation. Data assimilation (DA) aims to mitigate\nthese errors by optimally blending (noisy) observations with prior model\nforecasts, but conventional variational methods often assume Gaussian error\nstatistics that fail to capture the true, non-Gaussian behavior of chaotic\ndynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates\n(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance\nmisfit on new observations) with (2) a single forward pass through a pretrained\ngenerative prior conditioned on the background forecast via a conditional\nWasserstein coupling. This strategy relaxes restrictive statistical assumptions\nand leverages rich historical data without requiring an explicit regularization\nfunctional, and it also avoids the need to backpropagate gradients through the\ncomplex neural network that encodes the prior during assimilation cycles.\nExperiments on standard chaotic testbeds demonstrate that this strategy\nconsistently reduces forecast errors across a range of observation sparsities\nand noise levels, outperforming classical variational methods.", "AI": {"tldr": "PnP-DA\u662f\u4e00\u79cd\u65b0\u7684\u6570\u636e\u540c\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u68af\u5ea6\u4e0b\u964d\u548c\u751f\u6210\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5730\u7403\u7cfb\u7edf\u6a21\u578b\u4e2d\u7684\u8bef\u5dee\u95ee\u9898\uff0c\u5e76\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5730\u7403\u7cfb\u7edf\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8bef\u5dee\u6700\u5c0f\u5316\u4e4b\u95f4\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u4f20\u7edf\u6570\u636e\u540c\u5316\u65b9\u6cd5\u5728\u5904\u7406\u6df7\u6c8c\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u975e\u9ad8\u65af\u8bef\u5dee\u7edf\u8ba1\u65f6\u7684\u5c40\u9650\u6027\u3002", "method": "PnP-DA\u7b97\u6cd5\uff0c\u901a\u8fc7\uff081\uff09\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u5206\u6790\u66f4\u65b0\uff08\u4f7f\u7528\u65b0\u89c2\u6d4b\u503c\u4e0e\u80cc\u666f\u573a\u4e4b\u95f4\u7684\u9a6c\u6c0f\u8ddd\u79bb\uff09\u548c\uff082\uff09\u901a\u8fc7\u6761\u4ef6Wasserstein\u8026\u5408\u5c06\u9884\u8bad\u7ec3\u751f\u6210\u5148\u9a8c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u8fdb\u884c\u7ed3\u5408\uff0c\u4ece\u800c\u653e\u5bbd\u4e86\u5bf9\u7edf\u8ba1\u5047\u8bbe\u7684\u9650\u5236\uff0c\u5e76\u5229\u7528\u4e86\u5386\u53f2\u6570\u636e\u3002", "result": "\u5728\u6807\u51c6\u7684\u6df7\u6c8c\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0cPnP-DA\u7b97\u6cd5\u5728\u5404\u79cd\u89c2\u6d4b\u7a00\u758f\u5ea6\u548c\u566a\u58f0\u6c34\u5e73\u4e0b\uff0c\u6301\u7eed\u964d\u4f4e\u4e86\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u4e14\u4f18\u4e8e\u7ecf\u5178\u7684\u53d8\u5206\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPnP-DA\u7684\u5373\u63d2\u5373\u7528\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408\u68af\u5ea6\u4e0b\u964d\u5206\u6790\u66f4\u65b0\u548c\u9884\u8bad\u7ec3\u751f\u6210\u5148\u9a8c\uff0c\u80fd\u591f\u6709\u6548\u51cf\u8f7b\u5730\u7403\u7cfb\u7edf\u6a21\u578b\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u975e\u9ad8\u65af\u8bef\u5dee\u7edf\u8ba1\u65f6\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u53d8\u5206\u65b9\u6cd5\u3002"}}
{"id": "2508.00544", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00544", "abs": "https://arxiv.org/abs/2508.00544", "authors": ["Joonas Tapaninaho", "Mourad Oussala"], "title": "PaPaformer: Language Model from Pre-trained Paraller Paths", "comment": null, "summary": "The training of modern large-language models requires an increasingly amount\nof computation power and time. Even smaller variants, such as small-language\nmodels (SLMs), take several days to train in the best-case scenarios, often\nrequiring multiple GPUs. This paper explores methods to train and evaluate\ndecoder-only transformer-based language models in hours instead of days/weeks.\nWe introduces \\textit{PaPaformer}, a decoder-only transformer architecture\nvariant, whose lower-dimensional parallel paths are combined into larger model.\nThe paper shows that these lower-dimensional paths can be trained individually\nwith different types of training data and then combined into one larger model.\nThis method gives the option to reduce the total number of model parameters and\nthe training time with increasing performance. Moreover, the use of parallel\npath structure opens interesting possibilities to customize paths to\naccommodate specific task requirements.", "AI": {"tldr": "PaPaformer\u662f\u4e00\u79cd\u65b0\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u6570\u5c0f\u65f6\u5185\u5b8c\u6210\u8bad\u7ec3\uff0c\u5e76\u4e14\u6027\u80fd\u66f4\u4f18\u3002", "motivation": "\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u80fd\u529b\u548c\u65f6\u95f4\uff0c\u5373\u4f7f\u662f\u5c0f\u578b\u6a21\u578b\u4e5f\u9700\u8981\u6570\u5929\u751a\u81f3\u591a\u4e2aGPU\u624d\u80fd\u8bad\u7ec3\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u6570\u5c0f\u65f6\u5185\u8bad\u7ec3\u548c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPaPaformer\u7684\u89e3\u7801\u5668-Transformer\u67b6\u6784\u53d8\u4f53\uff0c\u5176\u4f4e\u7ef4\u5e76\u884c\u8def\u5f84\u53ef\u4ee5\u88ab\u5355\u72ec\u8bad\u7ec3\uff0c\u7136\u540e\u7ec4\u5408\u6210\u4e00\u4e2a\u66f4\u5927\u7684\u6a21\u578b\u3002", "result": "PaPaformer\u53ef\u4ee5\u51cf\u5c11\u603b\u6a21\u578b\u53c2\u6570\u91cf\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "PaPaformer\u67b6\u6784\u901a\u8fc7\u4f7f\u7528\u4f4e\u7ef4\u5e76\u884c\u8def\u5f84\uff0c\u53ef\u4ee5\u5728\u6570\u5c0f\u65f6\u800c\u975e\u6570\u5929/\u6570\u5468\u5185\u8bad\u7ec3\u548c\u8bc4\u4f30\u57fa\u4e8eTransformer\u7684\u89e3\u7801\u5668\u6a21\u578b\uff0c\u540c\u65f6\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u6570\u91cf\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u4e3a\u5b9a\u5236\u5316\u8def\u5f84\u4ee5\u6ee1\u8db3\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.00784", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00784", "abs": "https://arxiv.org/abs/2508.00784", "authors": ["Tom Or", "Omri Azencot"], "title": "Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics", "comment": null, "summary": "Generative models achieve remarkable results in multiple data domains,\nincluding images and texts, among other examples. Unfortunately, malicious\nusers exploit synthetic media for spreading misinformation and disseminating\ndeepfakes. Consequently, the need for robust and stable fake detectors is\npressing, especially when new generative models appear everyday. While the\nmajority of existing work train classifiers that discriminate between real and\nfake information, such tools typically generalize only within the same family\nof generators and data modalities, yielding poor results on other generative\nclasses and data domains. Towards a universal classifier, we propose the use of\nlarge pre-trained multi-modal models for the detection of generative content.\nEffectively, we show that the latent code of these models naturally captures\ninformation discriminating real from fake. Building on this observation, we\ndemonstrate that linear classifiers trained on these features can achieve\nstate-of-the-art results across various modalities, while remaining\ncomputationally efficient, fast to train, and effective even in few-shot\nsettings. Our work primarily focuses on fake detection in audio and images,\nachieving performance that surpasses or matches that of strong baseline\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u5927\u578b\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u5728\u4ee3\u7801\u8fdb\u884c\u865a\u5047\u5185\u5bb9\u68c0\u6d4b\uff0c\u5728\u97f3\u9891\u548c\u56fe\u50cf\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u6216\u5ab2\u7f8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u6076\u610f\u7528\u6237\u5229\u7528\u5408\u6210\u5a92\u4f53\u4f20\u64ad\u9519\u8bef\u4fe1\u606f\u548c\u6df1\u5ea6\u4f2a\u9020\u7684\u5a01\u80c1\uff0c\u9700\u8981\u5f00\u53d1\u7a33\u5065\u4e14\u7a33\u5b9a\u7684\u865a\u5047\u5185\u5bb9\u68c0\u6d4b\u5668\uff0c\u7279\u522b\u662f\u9762\u5bf9\u65e5\u76ca\u589e\u591a\u7684\u65b0\u578b\u751f\u6210\u6a21\u578b\u3002", "method": "\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u5728\u4ee3\u7801\u6765\u63d0\u53d6\u533a\u5206\u771f\u5b9e\u4e0e\u865a\u5047\u4fe1\u606f\u7684\u7279\u5f81\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3\u7ebf\u6027\u5206\u7c7b\u5668\u3002", "result": "\u5728\u97f3\u9891\u548c\u56fe\u50cf\u7684\u865a\u5047\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u6f5c\u5728\u7279\u5f81\u7684\u7ebf\u6027\u5206\u7c7b\u5668\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u8ba1\u7b97\u6548\u7387\u3001\u8bad\u7ec3\u901f\u5ea6\u548c\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u5927\u578b\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u6765\u68c0\u6d4b\u751f\u6210\u5185\u5bb9\uff0c\u5e76\u5728\u97f3\u9891\u548c\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00468", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00468", "abs": "https://arxiv.org/abs/2508.00468", "authors": ["Jiawei Zhang", "Yibo Chen", "Yang Zhou", "Jun-Han Huang"], "title": "Inference of maximum parsimony phylogenetic trees with model-based classical and quantum methods", "comment": "10 pages, 5 figures", "summary": "The maximum parsimony phylogenetic tree reconstruction problem is NP-hard,\npresenting a computational bottleneck for classical computing and motivating\nthe exploration of emerging paradigms like quantum computing. To this end, we\ndesign three optimization models compatible with both classical and quantum\nsolvers. Our method directly searches the complete solution space of all\npossible tree topologies and ancestral states, thereby avoiding the potential\nbiases associated with pre-constructing candidate internal nodes. Among these\nmodels, the branch-based model drastically reduces the number of variables and\nexplicit constraints through a specific variable definition, providing a novel\nmodeling approach effective not only for phylogenetic tree building but also\nfor other tree problems. The correctness of this model is validated with a\nclassical solver, which obtains solutions that are generally better than those\nfrom heuristics on the GAPDH gene dataset. Moreover, our quantum simulations\nsuccessfully find the exact optimal solutions for small-scale instances with\nrapid convergence, highlighting the potential of quantum computing to offer a\nnew avenue for solving these intractable problems in evolutionary biology.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8bbe\u8ba1\u65b0\u7684\u4f18\u5316\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6700\u5927\u7b80\u7ea6\u6027\u7cfb\u7edf\u53d1\u80b2\u6811\u91cd\u5efa\u7684NP\u96be\u95ee\u9898\u3002\u7814\u7a76\u8868\u660e\uff0c\u5176\u57fa\u4e8e\u5206\u652f\u7684\u6a21\u578b\u5728\u7ecf\u5178\u548c\u91cf\u5b50\u8ba1\u7b97\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u91cf\u5b50\u8ba1\u7b97\u65b9\u9762\uff0c\u4e3a\u89e3\u51b3\u8fdb\u5316\u751f\u7269\u5b66\u4e2d\u7684\u96be\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "motivation": "\u6700\u5927\u7b80\u7ea6\u6027\u7cfb\u7edf\u53d1\u80b2\u6811\u91cd\u5efa\u95ee\u9898\u7684NP\u96be\u6027\u662f\u7ecf\u5178\u8ba1\u7b97\u7684\u74f6\u9888\uff0c\u4fc3\u4f7f\u4eba\u4eec\u63a2\u7d22\u91cf\u5b50\u8ba1\u7b97\u7b49\u65b0\u5174\u8303\u5f0f\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u9002\u7528\u4e8e\u7ecf\u5178\u548c\u91cf\u5b50\u6c42\u89e3\u5668\u7684\u4f18\u5316\u6a21\u578b\uff0c\u76f4\u63a5\u641c\u7d22\u6240\u6709\u53ef\u80fd\u7684\u6811\u62d3\u6251\u548c\u7956\u5148\u72b6\u6001\uff0c\u5176\u4e2d\u57fa\u4e8e\u5206\u652f\u7684\u6a21\u578b\u901a\u8fc7\u7279\u5b9a\u7684\u53d8\u91cf\u5b9a\u4e49\uff0c\u51cf\u5c11\u4e86\u53d8\u91cf\u548c\u663e\u5f0f\u7ea6\u675f\u7684\u6570\u91cf\u3002", "result": "\u5728GAPDH\u57fa\u56e0\u6570\u636e\u96c6\u4e0a\uff0c\u7ecf\u5178\u6c42\u89e3\u5668\u83b7\u5f97\u4e86\u6bd4\u542f\u53d1\u5f0f\u65b9\u6cd5\u66f4\u597d\u7684\u89e3\u3002\u91cf\u5b50\u6a21\u62df\u5728\u5c0f\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u4ee5\u5feb\u901f\u6536\u655b\u6210\u529f\u627e\u5230\u7cbe\u786e\u7684\u6700\u4f18\u89e3\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u6709\u6f5c\u529b\u4e3a\u8fdb\u5316\u751f\u7269\u5b66\u4e2d\u68d8\u624b\u7684\u63a8\u65ad\u95ee\u9898\u63d0\u4f9b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u5c0f\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u6210\u529f\u627e\u5230\u7cbe\u786e\u7684\u6700\u4f18\u89e3\u3002"}}
{"id": "2508.00330", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00330", "abs": "https://arxiv.org/abs/2508.00330", "authors": ["Lilika Makabe", "Hiroaki Santo", "Fumio Okura", "Michael S. Brown", "Yasuyuki Matsushita"], "title": "Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating", "comment": null, "summary": "This paper introduces a practical and accurate calibration method for camera\nspectral sensitivity using a diffraction grating. Accurate calibration of\ncamera spectral sensitivity is crucial for various computer vision tasks,\nincluding color correction, illumination estimation, and material analysis.\nUnlike existing approaches that require specialized narrow-band filters or\nreference targets with known spectral reflectances, our method only requires an\nuncalibrated diffraction grating sheet, readily available off-the-shelf. By\ncapturing images of the direct illumination and its diffracted pattern through\nthe grating sheet, our method estimates both the camera spectral sensitivity\nand the diffraction grating parameters in a closed-form manner. Experiments on\nsynthetic and real-world data demonstrate that our method outperforms\nconventional reference target-based methods, underscoring its effectiveness and\npracticality.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u884d\u5c04\u5149\u6805\u7684\u76f8\u673a\u5149\u8c31\u7075\u654f\u5ea6\u6821\u51c6\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5b9e\u7528\u4e14\u51c6\u786e\uff0c\u4ec5\u9700\u4e00\u4e2a\u73b0\u6210\u7684\u884d\u5c04\u5149\u6805\u7247\u3002", "motivation": "\u51c6\u786e\u6821\u51c6\u76f8\u673a\u5149\u8c31\u7075\u654f\u5ea6\u5bf9\u4e8e\u989c\u8272\u6821\u6b63\u3001\u5149\u7167\u4f30\u8ba1\u548c\u6750\u6599\u5206\u6790\u7b49\u5404\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6355\u83b7\u76f4\u5c04\u5149\u7167\u53ca\u5176\u901a\u8fc7\u5149\u6805\u7247\u7684\u884d\u5c04\u56fe\u6848\u7684\u56fe\u50cf\uff0c\u4ee5\u95ed\u5f0f\u89e3\u7684\u5f62\u5f0f\u4f30\u8ba1\u76f8\u673a\u5149\u8c31\u7075\u654f\u5ea6\u548c\u884d\u5c04\u5149\u6805\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u8fdb\u884c\uff0c\u7ed3\u679c\u8868\u660e\u672c\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u53c2\u8003\u76ee\u6807\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u884d\u5c04\u5149\u6805\u7684\u76f8\u673a\u5149\u8c31\u7075\u654f\u5ea6\u6821\u51c6\u7684\u5b9e\u7528\u4e14\u51c6\u786e\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ec5\u9700\u4e00\u4e2a\u672a\u7ecf\u6821\u51c6\u7684\u884d\u5c04\u5149\u6805\u7247\uff0c\u65e0\u9700\u4e13\u4e1a\u7684\u7a84\u5e26\u6ee4\u5149\u5668\u6216\u5df2\u77e5\u5149\u8c31\u53cd\u5c04\u7387\u7684\u53c2\u8003\u76ee\u6807\u3002"}}
{"id": "2508.00331", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00331", "abs": "https://arxiv.org/abs/2508.00331", "authors": ["George Wang", "Garrett Baker", "Andrew Gordon", "Daniel Murfet"], "title": "Embryology of a Language Model", "comment": null, "summary": "Understanding how language models develop their internal computational\nstructure is a central problem in the science of deep learning. While\nsusceptibilities, drawn from statistical physics, offer a promising analytical\ntool, their full potential for visualizing network organization remains\nuntapped. In this work, we introduce an embryological approach, applying UMAP\nto the susceptibility matrix to visualize the model's structural development\nover training. Our visualizations reveal the emergence of a clear ``body\nplan,'' charting the formation of known features like the induction circuit and\ndiscovering previously unknown structures, such as a ``spacing fin'' dedicated\nto counting space tokens. This work demonstrates that susceptibility analysis\ncan move beyond validation to uncover novel mechanisms, providing a powerful,\nholistic lens for studying the developmental principles of complex neural\nnetworks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528UMAP\u5206\u6790\u6613\u611f\u6027\u77e9\u9635\uff0c\u53ef\u89c6\u5316\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u53d1\u5c55\uff0c\u53d1\u73b0\u4e86\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8ba1\u7b97\u7ed3\u6784\u7684\u5f00\u53d1\uff0c\u5e76\u5145\u5206\u53d1\u6398\u6613\u611f\u6027\u5206\u6790\u5728\u53ef\u89c6\u5316\u7f51\u7edc\u7ec4\u7ec7\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u80da\u80ce\u5b66\u201d\u65b9\u6cd5\uff0c\u5c06UMAP\u5e94\u7528\u4e8e\u6613\u611f\u6027\u77e9\u9635\uff0c\u4ee5\u53ef\u89c6\u5316\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u53d1\u5c55\u3002", "result": "\u53ef\u89c6\u5316\u7ed3\u679c\u63ed\u793a\u4e86\u4e00\u4e2a\u6e05\u6670\u7684\u201c\u8eab\u4f53\u8ba1\u5212\u201d\uff0c\u5c55\u793a\u4e86\u8bf8\u5982\u201c\u8bf1\u5bfc\u7535\u8def\u201d\u7b49\u5df2\u77e5\u7279\u5f81\u7684\u5f62\u6210\uff0c\u5e76\u53d1\u73b0\u4e86\u4e00\u4e2a\u5148\u524d\u672a\u77e5\u7684\u7ed3\u6784\uff0c\u5373\u7528\u4e8e\u8ba1\u7b97\u7a7a\u95f4\u6807\u8bb0\u7684\u201c\u95f4\u9694\u9ccd\u201d\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u6613\u611f\u6027\u5206\u6790\u4e0d\u4ec5\u53ef\u4ee5\u7528\u4e8e\u9a8c\u8bc1\uff0c\u8fd8\u53ef\u4ee5\u63ed\u793a\u65b0\u7684\u673a\u5236\uff0c\u4e3a\u7814\u7a76\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u7684\u53d1\u80b2\u539f\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u800c\u5168\u9762\u7684\u89c6\u89d2\u3002"}}
{"id": "2508.00574", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00574", "abs": "https://arxiv.org/abs/2508.00574", "authors": ["Jianwei Wang", "Ziming Wu", "Fuming Lai", "Shaobing Lian", "Ziqian Zeng"], "title": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought", "comment": null, "summary": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs\nsignificant time costs due to the generation of discrete CoT tokens (DCoT).\nContinuous CoT (CCoT) offers a more efficient alternative, but existing CCoT\nmethods are hampered by indirect fine-tuning, limited alignment, or\ninconsistent targets. To overcome these limitations, we propose\n\\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,\n\\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and\neffective alignment target for LLMs. This synthetic CCoT explicitly guides the\nLLM to learn CCoT and derive accurate answers directly. Furthermore, relying\nsolely on CCoT is insufficient for solving hard questions. To address this,\n\\textit{SynAdapt} integrates a difficulty classifier that leverages both\nquestion context and CCoT to identify hard questions. CCoT can effectively help\nidentify hard questions after some brief reasoning. We then adaptively prompt\nthe LLM to re-think these hard questions for improved performance. Extensive\nexperimental results across various benchmarks from different difficulty levels\nstrongly demonstrate the effectiveness of our method, achieving the best\naccuracy-efficiency trade-off.", "AI": {"tldr": "SynAdapt\u901a\u8fc7\u751f\u6210\u5408\u6210\u7684CCoT\u548c\u81ea\u9002\u5e94\u5730\u63d0\u793a\u6a21\u578b\u91cd\u65b0\u601d\u8003\u96be\u95ee\uff0c\u63d0\u9ad8\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u8fde\u7eed\u601d\u7ef4\u94fe\uff08CCoT\uff09\u65b9\u6cd5\u5728\u5fae\u8c03\u3001\u5bf9\u9f50\u548c\u76ee\u6807\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u63a8\u7406\u4efb\u52a1\u65f6\u7684\u6548\u7387\u3002", "method": "SynAdapt\u6846\u67b6\u901a\u8fc7\u751f\u6210\u5408\u6210\u7684\u8fde\u7eed\u601d\u7ef4\u94fe\uff08CCoT\uff09\u4f5c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u76ee\u6807\uff0c\u5e76\u7ed3\u5408\u96be\u5ea6\u5206\u7c7b\u5668\u6765\u8bc6\u522b\u548c\u91cd\u65b0\u601d\u8003\u96be\u95ee\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "result": "SynAdapt\u901a\u8fc7\u751f\u6210\u5408\u6210CCoT\u548c\u81ea\u9002\u5e94\u5730\u63d0\u793a\u6a21\u578b\u91cd\u65b0\u601d\u8003\u96be\u95ee\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u51c6\u786e\u6027-\u6548\u7387\u6743\u8861\u3002", "conclusion": "SynAdapt\u5728\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u51c6\u786e\u6027-\u6548\u7387\u6743\u8861\u3002"}}
{"id": "2502.18148", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2502.18148", "abs": "https://arxiv.org/abs/2502.18148", "authors": ["Muhammad Farid Adilazuarda", "Musa Izzanardi Wijanarko", "Lucky Susanto", "Khumaisa Nur'aini", "Derry Wijaya", "Alham Fikri Aji"], "title": "NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts", "comment": null, "summary": "Indonesia is rich in languages and scripts. However, most NLP progress has\nbeen made using romanized text. In this paper, we present NusaAksara, a novel\npublic benchmark for Indonesian languages that includes their original scripts.\nOur benchmark covers both text and image modalities and encompasses diverse\ntasks such as image segmentation, OCR, transliteration, translation, and\nlanguage identification. Our data is constructed by human experts through\nrigorous steps. NusaAksara covers 8 scripts across 7 languages, including\nlow-resource languages not commonly seen in NLP benchmarks. Although\nunsupported by Unicode, the Lampung script is included in this dataset. We\nbenchmark our data across several models, from LLMs and VLMs such as GPT-4o,\nLlama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and\nshow that most NLP technologies cannot handle Indonesia's local scripts, with\nmany achieving near-zero performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86NusaAksara\uff0c\u4e00\u4e2a\u5305\u542b\u5370\u5c3c\u672c\u571f\u6587\u5b57\uff08\u5305\u62ec\u4f4e\u8d44\u6e90\u548c\u975eUnicode\u6587\u5b57\uff09\u7684\u6587\u672c\u548c\u56fe\u50cf\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30OCR\u3001\u7ffb\u8bd1\u7b49\u4efb\u52a1\u3002\u7ed3\u679c\u663e\u793a\u73b0\u6709NLP\u6280\u672f\u5728\u5904\u7406\u8fd9\u4e9b\u6587\u5b57\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5927\u591a\u6570\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u8fdb\u5c55\u90fd\u662f\u4f7f\u7528\u7f57\u9a6c\u5b57\u6bcd\u6587\u672c\u5b8c\u6210\u7684\uff0c\u4f46\u5370\u5ea6\u5c3c\u897f\u4e9a\u62e5\u6709\u4e30\u5bcc\u7684\u8bed\u8a00\u548c\u6587\u5b57\u3002", "method": "\u63d0\u51faNusaAksara\uff0c\u4e00\u4e2a\u5305\u542b\u5370\u5c3c\u539f\u59cb\u6587\u5b57\u7684\u516c\u5171\u57fa\u51c6\uff0c\u6db5\u76d6\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\uff0c\u4ee5\u53ca\u56fe\u50cf\u5206\u5272\u3001OCR\u3001\u97f3\u8bd1\u3001\u7ffb\u8bd1\u548c\u8bed\u8a00\u8bc6\u522b\u7b49\u4efb\u52a1\u3002\u8be5\u6570\u636e\u7531\u4eba\u7c7b\u4e13\u5bb6\u901a\u8fc7\u4e25\u683c\u6b65\u9aa4\u6784\u5efa\uff0c\u5305\u542b8\u79cd\u6587\u5b57\u548c7\u79cd\u8bed\u8a00\uff0c\u5176\u4e2d\u5305\u62ec\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4e0d\u53d7Unicode\u652f\u6301\u7684\u6960\u699c\u8bed\u3002", "result": "NusaAksara \u5305\u542b\u4e868 \u79cd\u6587\u5b57\u548c 7 \u79cd\u8bed\u8a00\uff0c\u5305\u62ec\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4e0d\u53d7 Unicode \u652f\u6301\u7684\u6960\u699c\u8bed\u3002\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u5927\u591a\u6570\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff08\u5305\u62ec LLMs\u3001VLMs \u548c\u7279\u5b9a\u4efb\u52a1\u7cfb\u7edf\uff09\u5728\u5904\u7406\u5370\u5ea6\u5c3c\u897f\u4e9a\u672c\u5730\u6587\u5b57\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u51c6\u786e\u7387\u63a5\u8fd1\u4e8e\u96f6\u3002", "conclusion": "\u5927\u591a\u6570\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u65e0\u6cd5\u5904\u7406\u5370\u5c3c\u7684\u672c\u5730\u6587\u5b57\uff0c\u8bb8\u591a\u6280\u672f\u7684\u8868\u73b0\u63a5\u8fd1\u4e8e\u96f6\u3002"}}
{"id": "2508.00484", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2508.00484", "abs": "https://arxiv.org/abs/2508.00484", "authors": ["Pilsung Kang"], "title": "Emergent Bifurcations in Quantum Circuit Stability from Hidden Parameter Statistics", "comment": null, "summary": "The compression of quantum circuits is a foundational challenge for near-term\nquantum computing, yet the principles governing circuit stability remain poorly\nunderstood. We investigate this problem through a large-scale numerical\nanalysis of 300 structurally-uniform circuits across 10, 12, and 14 qubits.\nDespite their macroscopic uniformity, we find that each ensemble universally\nbifurcates into distinct robust and fragile classes. We solve the puzzle of\nthis emergent bifurcation, demonstrating that its origin is not structural, but\nis instead encoded in the statistical properties of the gate rotation\nparameters. Fragile circuits consistently exhibit a universal signature of\n``statistical brittleness,'' characterized by low parameter variability and a\nscarcity of small-angle gates. We uncover the underlying physical mechanism for\nthis phenomenon: Paradoxical Importance where smaller-angle gates are\ncounter-intuitively more critical to the circuit's function, an effect most\npronounced in fragile circuits. This reliance on fine-tuning explains why\nstatistically brittle circuits are uniquely vulnerable to failure under\ncompression. These findings establish a new framework for engineering resilient\nquantum algorithms, shifting the focus from macroscopic structure to the\nmicroscopic statistical properties of a circuit's parameters.", "AI": {"tldr": "\u91cf\u5b50\u7535\u8def\u7684\u538b\u7f29\u548c\u7a33\u5b9a\u6027\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u672c\u7814\u7a76\u901a\u8fc7\u6570\u503c\u5206\u6790\u53d1\u73b0\uff0c\u7535\u8def\u7684\u9c81\u68d2\u6027\u6216\u8106\u5f31\u6027\u53d6\u51b3\u4e8e\u5176\u95e8\u65cb\u8f6c\u53c2\u6570\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u800c\u975e\u7ed3\u6784\u3002\u8106\u5f31\u7535\u8def\u5177\u6709\u201c\u7edf\u8ba1\u8106\u6027\u201d\uff0c\u5e76\u4e14\u5b58\u5728\u201c\u6096\u8bba\u91cd\u8981\u6027\u201d\u73b0\u8c61\uff0c\u5373\u5c0f\u89d2\u5ea6\u95e8\u66f4\u5173\u952e\u3002\u8fd9\u4e3a\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684\u91cf\u5b50\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u91cf\u5b50\u7535\u8def\u538b\u7f29\u4e2d\u7684\u4e00\u4e2a\u57fa\u7840\u6027\u6311\u6218\uff0c\u5373\u5bf9\u63a7\u5236\u7535\u8def\u7a33\u5b9a\u6027\u7684\u539f\u7406\u77e5\u4e4b\u751a\u5c11\u3002", "method": "\u901a\u8fc7\u5bf9300\u4e2a\u7ed3\u6784\u5747\u5300\u7684\u7535\u8def\uff08\u6a2a\u8de810\u300112\u548c14\u4e2a\u91cf\u5b50\u6bd4\u7279\uff09\u8fdb\u884c\u5927\u89c4\u6a21\u6570\u503c\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u7535\u8def\u5b8f\u89c2\u5747\u5300\uff0c\u4f46\u5b83\u4eec\u666e\u904d\u5206\u4e3a\u9c81\u68d2\u548c\u8106\u5f31\u7684\u7c7b\u522b\uff0c\u5176\u6839\u6e90\u5728\u4e8e\u95e8\u65cb\u8f6c\u53c2\u6570\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u800c\u975e\u7ed3\u6784\u3002\u8106\u5f31\u7535\u8def\u8868\u73b0\u51fa\u201c\u7edf\u8ba1\u8106\u6027\u201d\u7279\u5f81\uff0c\u5176\u53c2\u6570\u53d8\u5f02\u6027\u4f4e\uff0c\u5c0f\u89d2\u5ea6\u95e8\u7a00\u5c11\u3002\u8fd8\u63ed\u793a\u4e86\u201c\u6096\u8bba\u91cd\u8981\u6027\u201d\u673a\u5236\uff0c\u5373\u5c0f\u89d2\u5ea6\u95e8\u5bf9\u7535\u8def\u529f\u80fd\u66f4\u4e3a\u5173\u952e\uff0c\u8fd9\u4f7f\u5f97\u8106\u5f31\u7535\u8def\u5728\u538b\u7f29\u65f6\u66f4\u5bb9\u6613\u5931\u8d25\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u7b97\u6cd5\u5de5\u7a0b\u6846\u67b6\uff0c\u5c06\u91cd\u70b9\u4ece\u5b8f\u89c2\u7ed3\u6784\u8f6c\u79fb\u5230\u7535\u8def\u53c2\u6570\u7684\u5fae\u89c2\u7edf\u8ba1\u7279\u6027\uff0c\u4ee5\u8bbe\u8ba1\u66f4\u5177\u5f39\u6027\u7684\u91cf\u5b50\u7b97\u6cd5\u3002"}}
{"id": "2508.00350", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00350", "abs": "https://arxiv.org/abs/2508.00350", "authors": ["Qilin Liao", "Shuo Yang", "Bo Zhao", "Ping Luo", "Hengshuang Zhao"], "title": "BOOD: Boundary-based Out-Of-Distribution Data Generation", "comment": "14 pages, 8 figures, To be published in the Proceedings of the\n  International Conference on Machine Learning (ICML) 2025", "summary": "Harnessing the power of diffusion models to synthesize auxiliary training\ndata based on latent space features has proven effective in enhancing\nout-of-distribution (OOD) detection performance. However, extracting effective\nfeatures outside the in-distribution (ID) boundary in latent space remains\nchallenging due to the difficulty of identifying decision boundaries between\nclasses. This paper proposes a novel framework called Boundary-based\nOut-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD\nfeatures and generates human-compatible outlier images using diffusion models.\nBOOD first learns a text-conditioned latent feature space from the ID dataset,\nselects ID features closest to the decision boundary, and perturbs them to\ncross the decision boundary to form OOD features. These synthetic OOD features\nare then decoded into images in pixel space by a diffusion model. Compared to\nprevious works, BOOD provides a more training efficient strategy for\nsynthesizing informative OOD features, facilitating clearer distinctions\nbetween ID and OOD data. Extensive experimental results on common benchmarks\ndemonstrate that BOOD surpasses the state-of-the-art method significantly,\nachieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%\nimprovement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.", "AI": {"tldr": "BOOD\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u751f\u6210OOD\u7279\u5f81\uff0c\u5e76\u751f\u6210\u5f02\u5e38\u56fe\u50cf\uff0c\u4ee5\u63d0\u9ad8OOD\u68c0\u6d4b\u6027\u80fd\uff0c\u514b\u670d\u4e86\u5728ID\u6570\u636e\u8fb9\u754c\u4e4b\u5916\u63d0\u53d6\u7279\u5f81\u7684\u6311\u6218\u3002", "motivation": "\u4ece\u6f5c\u5728\u7a7a\u95f4\u4e2d\u63d0\u53d6\u6709\u6548\u7684\u7279\u5f81\u4ee5\u589e\u5f3a\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u51b3\u7b56\u8fb9\u754c\u4e4b\u5916\uff0c\u8fd9\u4e00\u76f4\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u96be\u4ee5\u8bc6\u522b\u7c7b\u4e4b\u95f4\u7684\u51b3\u7b56\u8fb9\u754c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684OOD\u7279\u5f81\u3002", "method": "BOOD\u9996\u5148\u4eceID\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u6587\u672c\u6761\u4ef6\u6f5c\u5728\u7279\u5f81\u7a7a\u95f4\uff0c\u9009\u62e9\u6700\u63a5\u8fd1\u51b3\u7b56\u8fb9\u754c\u7684ID\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6270\u52a8\u4f7f\u5176\u8de8\u8d8a\u51b3\u7b56\u8fb9\u754c\u5f62\u6210OOD\u7279\u5f81\u3002\u7136\u540e\uff0c\u6269\u6563\u6a21\u578b\u5c06\u8fd9\u4e9b\u5408\u6210\u7684OOD\u7279\u5f81\u89e3\u7801\u4e3a\u50cf\u7d20\u7a7a\u95f4\u4e2d\u7684\u56fe\u50cf\u3002", "result": "BOOD\u6846\u67b6\u5728CIFAR-100\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8629.64%\u7684\u5e73\u5747FPR95\u964d\u4f4e\uff08\u4ece40.31%\u964d\u81f310.67%\uff09\u548c7.27%\u7684\u5e73\u5747AUROC\u63d0\u5347\uff08\u4ece90.15%\u63d0\u5347\u81f397.42%\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "BOOD\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u7684OOD\u7279\u5f81\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u4eba\u7c7b\u53ef\u517c\u5bb9\u7684\u5f02\u5e38\u56fe\u50cf\uff0c\u4e3a\u5408\u6210OOD\u7279\u5f81\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5177\u8bad\u7ec3\u6548\u7387\u7684\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86ID\u548cOOD\u6570\u636e\u4e4b\u95f4\u66f4\u6e05\u6670\u7684\u533a\u5206\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBOOD\u5728CIFAR-100\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.00600", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00600", "abs": "https://arxiv.org/abs/2508.00600", "authors": ["Mingruo Yuan", "Shuyi Zhang", "Ben Kao"], "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models", "comment": null, "summary": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines.", "AI": {"tldr": "CRUX\u6846\u67b6\u901a\u8fc7\u8003\u8651\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u63d0\u9ad8LLM\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u65b0\u65b9\u6cd5\uff08\u4e0a\u4e0b\u6587\u71b5\u7ea6\u51cf\u548c\u7edf\u4e00\u4e00\u81f4\u6027\u68c0\u9a8c\uff09\u6765\u8861\u91cf\u4e0d\u786e\u5b9a\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5ffd\u7565\u4e86\u54cd\u5e94\u4e0e\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u8fd9\u5728\u6709\u80cc\u666f\u77e5\u8bc6\u7684\u573a\u666f\u4e0b\u662f\u8bc4\u4f30\u8f93\u51fa\u8d28\u91cf\u7684\u5173\u952e\u56e0\u7d20\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u63d0\u51faCRUX\u6846\u67b6\u3002", "method": "\u63d0\u51faCRUX\u6846\u67b6\uff0c\u6574\u5408\u4e86\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u5ea6\u548c\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u4e24\u79cd\u65b0\u9896\u7684\u5ea6\u91cf\u65b9\u6cd5\u8fdb\u884c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff1a\u4e0a\u4e0b\u6587\u71b5\u7ea6\u51cf\uff08\u901a\u8fc7\u6709\u65e0\u4e0a\u4e0b\u6587\u7684\u5bf9\u6bd4\u91c7\u6837\u8868\u793a\u6570\u636e\u4e0d\u786e\u5b9a\u6027\uff09\u548c\u7edf\u4e00\u4e00\u81f4\u6027\u68c0\u9a8c\uff08\u6355\u6349\u751f\u6210\u7b54\u6848\u4e0e\u6709\u65e0\u4e0a\u4e0b\u6587\u7684\u5168\u5c40\u4e00\u81f4\u6027\uff09\u3002", "result": "CRUX\u5728CoQA\u3001SQuAD\u3001QuAC\u3001BioASQ\u548cEduQG\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5176AUROC\u9ad8\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "CRUX\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u7279\u5b9a\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709\u57fa\u7ebf\u66f4\u9ad8\u7684AUROC\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.00514", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00514", "abs": "https://arxiv.org/abs/2508.00514", "authors": ["Sebastiaan Brand", "Alfons Laarman"], "title": "Q-Sylvan: A Parallel Decision Diagram Package for Quantum Computing", "comment": null, "summary": "As physical realizations of quantum computers move closer towards practical\napplications, the need for tools to analyze and verify quantum algorithms\ngrows. Among the algorithms and data structures used to tackle such problems,\ndecision diagrams (DDs) have shown much success. However, an obstacle with DDs\nis their efficient parallelization, and while parallel speedups have been\nobtained for DDs used in classical applications, attempts to parallelize\noperations for quantum-specific DDs have yielded only limited success. In this\nwork, we present an efficient implementation of parallel edge-valued DDs, which\nmakes use of fine-grained task parallelism and lock-free hash tables.\nAdditionally, we use these DDs to implement two use cases: simulation and\nequivalence checking of quantum circuits. In our empirical evaluation we find\nthat our tool, Q-Sylvan, shows a single-core performance that is competitive\nwith the state-of-the-art quantum DD tool MQT DDSIM on large instances, and\nmoreover achieves parallel speedups of up to x18 on 64 cores.", "AI": {"tldr": "Q-Sylvan\u5de5\u5177\u901a\u8fc7\u5e76\u884cDDs\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u91cf\u5b50\u7535\u8def\u6a21\u62df\u548c\u7b49\u4ef7\u6027\u68c0\u67e5\u7684\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5e76\u884c\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u5e94\u7528\u7684\u4e34\u8fd1\uff0c\u9700\u8981\u5206\u6790\u548c\u9a8c\u8bc1\u91cf\u5b50\u7b97\u6cd5\u7684\u5de5\u5177\u3002\u51b3\u7b56\u56fe\uff08DDs\uff09\u5728\u89e3\u51b3\u8fd9\u7c7b\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5e76\u884c\u5316\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u91cf\u5b50\u7279\u5b9aDDs\u65b9\u9762\u3002", "method": "\u5229\u7528\u7ec6\u7c92\u5ea6\u4efb\u52a1\u5e76\u884c\u548c\u65e0\u9501\u54c8\u5e0c\u8868\u5b9e\u73b0\u4e86\u5e76\u884c\u8fb9\u7f18\u503c\u51b3\u7b56\u56fe\uff08DDs\uff09\u3002", "result": "Q-Sylvan\u5728\u5355\u6838\u6027\u80fd\u4e0a\u53ef\u4e0e\u6700\u5148\u8fdb\u7684\u91cf\u5b50DD\u5de5\u5177MQT DDSIM\u5728\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u76f8\u5ab2\u7f8e\uff0c\u5e76\u4e14\u572864\u6838\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe18\u500d\u7684\u5e76\u884c\u52a0\u901f\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Q-Sylvan\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u5e76\u884c\u8fb9\u7f18\u503c\u51b3\u7b56\u56fe\uff08DDs\uff09\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u91cf\u5b50\u7535\u8def\u7684\u6a21\u62df\u548c\u7b49\u4ef7\u6027\u68c0\u67e5\u3002"}}
{"id": "2508.00358", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00358", "abs": "https://arxiv.org/abs/2508.00358", "authors": ["Yan Gong", "Mengjun Chen", "Hao Liu", "Gao Yongsheng", "Lei Yang", "Naibang Wang", "Ziying Song", "Haoqun Ma"], "title": "Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering", "comment": "9 pages, 7 figures, 5 tables", "summary": "Multi-object tracking (MOT) enables autonomous vehicles to continuously\nperceive dynamic objects, supplying essential temporal cues for prediction,\nbehavior understanding, and safe planning. However, conventional\ntracking-by-detection methods typically rely on static coordinate\ntransformations based on ego-vehicle poses, disregarding ego-vehicle\nspeed-induced variations in observation noise and reference frame changes,\nwhich degrades tracking stability and accuracy in dynamic, high-speed\nscenarios. In this paper, we investigate the critical role of ego-vehicle speed\nin MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that\ndynamically adapts uncertainty modeling to ego-vehicle speed, significantly\nimproving stability and accuracy in highly dynamic scenarios. Central to SG-LKF\nis MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that\nadaptively predicts key parameters of SG-LKF. To enhance inter-frame\nassociation and trajectory continuity, we introduce a self-supervised\ntrajectory consistency loss jointly optimized with semantic and positional\nconstraints. Extensive experiments show that SG-LKF ranks first among all\nvision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results\non KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on\nnuScenes 3D MOT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5SG-LKF\uff0c\u901a\u8fc7\u8003\u8651\u4e3b\u8f66\u901f\u5ea6\u6765\u63d0\u9ad8\u8ddf\u8e2a\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u68c0\u6d4b\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e3b\u8f66\u901f\u5ea6\u53d8\u5316\u5f15\u8d77\u7684\u89c2\u6d4b\u566a\u58f0\u548c\u53c2\u8003\u7cfb\u53d8\u5316\uff0c\u5bfc\u81f4\u5728\u9ad8\u901f\u52a8\u6001\u573a\u666f\u4e0b\u7684\u8ddf\u8e2a\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u4e3b\u8f66\u901f\u5ea6\u5728\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7531MotionScaleNet (MSNet)\u9a71\u52a8\u7684Speed-Guided Learnable Kalman Filter (SG-LKF)\uff0cMSNet\u80fd\u591f\u81ea\u9002\u5e94\u5730\u9884\u6d4bSG-LKF\u7684\u5173\u952e\u53c2\u6570\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u8f68\u8ff9\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5e76\u4e0e\u8bed\u4e49\u548c\u4f4d\u7f6e\u7ea6\u675f\u8054\u5408\u4f18\u5316\uff0c\u4ee5\u589e\u5f3a\u5e27\u95f4\u5173\u8054\u548c\u8f68\u8ff9\u8fde\u7eed\u6027\u3002", "result": "SG-LKF\u5728KITTI 2D MOT\u4e0a\u53d6\u5f97\u4e8679.59%\u7684HOTA\uff0c\u5728KITTI 3D MOT\u4e0a\u53d6\u5f97\u4e8682.03%\u7684HOTA\uff0c\u5728nuScenes 3D MOT\u4e0a\u6bd4SimpleTrack\u7684AMOTA\u9ad8\u51fa2.2%\u3002", "conclusion": "SG-LKF\u5728KITTI 2D MOT\u3001KITTI 3D MOT\u548cnuScenes 3D MOT\u4e0a\u5747\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u76ee\u6807\u8ddf\u8e2a\u9886\u57df\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.00357", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00357", "abs": "https://arxiv.org/abs/2508.00357", "authors": ["Yoonhyuk Choi", "Jiho Choi", "Chong-Kwon Kim"], "title": "Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization", "comment": null, "summary": "Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct\nnode features, particularly on heterophilic graphs where adjacent nodes often\nhave dissimilar labels. Although sheaf neural networks partially mitigate this\nproblem, they typically rely on static or heavily parameterized sheaf\nstructures that hinder generalization and scalability. Existing sheaf-based\nmodels either predefine restriction maps or introduce excessive complexity, yet\nfail to provide rigorous stability guarantees. In this paper, we introduce a\nnovel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified\narchitecture that combines cellular-sheaf message passing with several\nmechanisms, including optimal transport-based lifting, variance-reduced\ndiffusion, and PAC-Bayes spectral regularization for robust semi-supervised\nnode classification. We establish performance bounds theoretically and\ndemonstrate that the resulting bound-aware objective can be achieved via\nend-to-end training in linear computational complexity. Experiments on nine\nhomophilic and heterophilic benchmarks show that SGPC outperforms\nstate-of-the-art spectral and sheaf-based GNNs while providing certified\nconfidence intervals on unseen nodes.", "AI": {"tldr": "SGPC\u901a\u8fc7\u521b\u65b0\u7684theaf GNN\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u5f02\u8d28\u56fe\u4e0a\u7684\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u5f02\u8d28\u56fe\u4e0a\u5b58\u5728\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u5bfc\u81f4\u8282\u70b9\u7279\u5f81\u5d29\u6e83\u3002\u73b0\u6709\u7684theaf\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u6709\u6240\u7f13\u89e3\uff0c\u4f46\u4f9d\u8d56\u9759\u6001\u6216\u53c2\u6570\u5316\u7684theaf\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "SGPC\uff08Sheaf GNNs with PAC-Bayes Calibration\uff09\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7edf\u4e00\u67b6\u6784\uff0c\u5b83\u7ed3\u5408\u4e86\u7ec6\u80de- theaf\u6d88\u606f\u4f20\u9012\u4e0e\u6700\u4f18\u4f20\u8f93\u3001\u65b9\u5dee\u7f29\u51cf\u6269\u6563\u548cPAC-Bayes\u8c31\u6b63\u5219\u5316\u7b49\u673a\u5236\uff0c\u7528\u4e8e\u5904\u7406\u5f02\u8d28\u56fe\u4e0a\u7684\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "result": "SGPC\u5728\u4e5d\u4e2a\u540c\u8d28\u548c\u5f02\u8d28\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u7ed3\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u8ba4\u8bc1\u7684\u7f6e\u4fe1\u533a\u95f4\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9c81\u68d2\u534a\u76d1\u7763\u8282\u70b9\u5206\u7c7b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "SGPC\u901a\u8fc7\u7ed3\u5408\u7ec6\u80de- theaf\u6d88\u606f\u4f20\u9012\u3001\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u63d0\u5347\u3001\u65b9\u5dee\u7f29\u51cf\u6269\u6563\u548cPAC-Bayes\u8c31\u6b63\u5219\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86GNN\u4e2d\u7684\u5e73\u6ed1\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5f02\u8d28\u56fe\u4e0a\u3002\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u5efa\u7acb\u4e86\u6027\u80fd\u754c\u9650\uff0c\u5e76\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u5b9e\u73b0\u4e86\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cSGPC\u5728\u4e5d\u4e2a\u540c\u8d28\u548c\u5f02\u8d28\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8c31\u548ctheaf GNNs\uff0c\u5e76\u4e3a\u672a\u89c1\u8282\u70b9\u63d0\u4f9b\u4e86\u8ba4\u8bc1\u7684\u7f6e\u4fe1\u533a\u95f4\u3002"}}
{"id": "2508.00605", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00605", "abs": "https://arxiv.org/abs/2508.00605", "authors": ["Farhana Haque", "Md. Abdur Rahman", "Sumon Ahmed"], "title": "GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language", "comment": null, "summary": "Topic modeling is a Natural Language Processing (NLP) technique that is used\nto identify latent themes and extract topics from text corpora by grouping\nsimilar documents based on their most significant keywords. Although widely\nresearched in English, topic modeling remains understudied in Bengali due to\nits morphological complexity, lack of adequate resources and initiatives. In\nthis contribution, a novel Graph Convolutional Network (GCN) based model called\nGHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input\nvectors of documents as nodes in the graph, which GCN uses to produce\nsemantically rich embeddings. The embeddings are then decomposed using\nNon-negative Matrix Factorization (NMF) to get the topical representations of\nthe underlying themes of the text corpus. This study compares the proposed\nmodel against a wide range of Bengali topic modeling techniques, from\ntraditional methods such as LDA, LSA, and NMF to contemporary frameworks such\nas BERTopic and Top2Vec on three Bengali datasets. The experimental results\ndemonstrate the effectiveness of the proposed model by outperforming other\nmodels in topic coherence and diversity. In addition, we introduce a novel\nBengali dataset called \"NCTBText\" sourced from Bengali textbook materials to\nenrich and diversify the predominantly newspaper-centric Bengali corpora.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGHTM\u7684\u65b0\u578b\u56fe\u57fa\u6df7\u5408\u4e3b\u9898\u6a21\u578b\uff0c\u7528\u4e8e\u5b5f\u52a0\u62c9\u8bed\u4e3b\u9898\u5efa\u6a21\uff0c\u5e76\u5728\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u5e03\u4e86\u4e00\u4e2a\u540d\u4e3aNCTBText\u7684\u65b0\u5b5f\u52a0\u62c9\u8bed\u6570\u636e\u96c6\u3002", "motivation": "\u5c3d\u7ba1\u4e3b\u9898\u5efa\u6a21\u5728\u82f1\u8bed\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u7531\u4e8e\u5176\u5f62\u6001\u590d\u6742\u6027\u3001\u8d44\u6e90\u548c\u5021\u8bae\u7684\u7f3a\u4e4f\uff0c\u5728\u5b5f\u52a0\u62c9\u8bed\u4e2d\u7684\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u7684\u65b0\u578b\u56fe\u57fa\u6df7\u5408\u4e3b\u9898\u6a21\u578b\uff08GHTM\uff09\u3002\u8be5\u6a21\u578b\u5c06\u6587\u6863\u7684\u8f93\u5165\u5411\u91cf\u8868\u793a\u4e3a\u56fe\u4e2d\u7684\u8282\u70b9\uff0c\u7136\u540e\u4f7f\u7528\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08NMF\uff09\u6765\u5206\u89e3\u5d4c\u5165\uff0c\u4ee5\u83b7\u5f97\u6587\u672c\u8bed\u6599\u5e93\u7684\u6f5c\u5728\u4e3b\u9898\u7684\u4e3b\u9898\u8868\u793a\u3002", "result": "GHTM\u6a21\u578b\u5728\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8eLDA\u3001LSA\u3001NMF\u3001BERTopic\u548cTop2Vec\u7b49\u6a21\u578b\u3002", "conclusion": "GHTM\u6a21\u578b\u5728\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5e76\u4e14\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u5b5f\u52a0\u62c9\u8bed\u6570\u636e\u96c6NCTBText\uff0c\u4ee5\u4e30\u5bcc\u5b5f\u52a0\u62c9\u8bed\u8bed\u6599\u5e93\u3002"}}
{"id": "2508.00530", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00530", "abs": "https://arxiv.org/abs/2508.00530", "authors": ["Stefan H\u00e4ussler", "Peter van Loock"], "title": "Quantum repeaters based on stationary and flying Gottesman-Kitaev-Preskill qudits", "comment": "16 pages, 9 figures", "summary": "There are various approaches to long-range quantum communication based on\nconceptually different forms of quantum repeaters. Here we explore a quantum\nrepeater scheme that employs quantum error correction (QEC) both on the flying\n(light) qubits and on the stationary (matter) qubits. The idea is to combine\nthe benefits of encoded one-way and two-way schemes where effective channel\ntransmission and loss scaling are enhanced by means of photon loss codes and\nencoded quantum memories, respectively, while sacrificing some of their\nadvantages such as high clock rates, independent of classical communication\ntimes (one-way), and potentially large segment lengths (two-way). More\nspecifically, we illustrate, propose, and analyze such a quantum repeater using\nthe bosonic Gottesman-Kitaev-Preskill (GKP) code which naturally enables\nencoding and QEC of qudits, protecting them against transmission and memory\nloss, the latter, for instance, occuring on collective spin modes of atomic\nensembles. While the encoded one-way and two-way schemes on their own either\nrequire very high repeater link coupling efficiencies and GKP squeezing or\nallow for experimentally more feasible, small values of these parameters,\nrespectively, we find that there are intermediate parameter regimes where the\ncombined repeater protocol is superior.", "AI": {"tldr": "\u7ed3\u5408GKP\u91cf\u5b50\u7ea0\u9519\u7684\u91cf\u5b50\u4e2d\u7ee7\u5668\u5728\u7279\u5b9a\u53c2\u6570\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u7ed3\u5408\u91cf\u5b50\u7ea0\u9519\uff08QEC\uff09\u7684\u91cf\u5b50\u4e2d\u7ee7\u5668\u65b9\u6848\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u65b9\u6848\u7684\u5c40\u9650\u6027\u3002", "method": "\u63a2\u7d22\u4e86\u4e00\u79cd\u7ed3\u5408\u98de\u884c\uff08\u5149\uff09\u548c\u56fa\u5b9a\uff08\u7269\u8d28\uff09\u91cf\u5b50\u6bd4\u7279\u7684\u91cf\u5b50\u7ea0\u9519\uff08QEC\uff09\u91cf\u5b50\u4e2d\u7ee7\u5668\u65b9\u6848\uff0c\u5e76\u4f7f\u7528GKP\u7801\u6765\u7f16\u7801\u548c\u4fdd\u62a4\u91cf\u5b50\u6bd4\u7279\u3002", "result": "\u53d1\u73b0\u5b58\u5728\u4e00\u4e2a\u4e2d\u95f4\u53c2\u6570\u8303\u56f4\uff0c\u7ed3\u5408\u7684\u91cf\u5b50\u4e2d\u7ee7\u5668\u534f\u8bae\u4f18\u4e8e\u5355\u72ec\u7684\u5355\u5411\u6216\u53cc\u5411\u65b9\u6848\u3002", "conclusion": "\u8be5\u91cf\u5b50\u4e2d\u7ee7\u5668\u65b9\u6848\u7ed3\u5408\u4e86\u91cf\u5b50\u7ea0\u9519\uff08QEC\uff09\u548cGKP\u7801\uff0c\u5728\u7279\u5b9a\u53c2\u6570\u8303\u56f4\u5185\u4f18\u4e8e\u5355\u4e00\u7684\u5355\u5411\u6216\u53cc\u5411\u65b9\u6848\u3002"}}
{"id": "2508.00359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00359", "abs": "https://arxiv.org/abs/2508.00359", "authors": ["Zongheng Tang", "Yi Liu", "Yifan Sun", "Yulu Gao", "Jinyu Chen", "Runsheng Xu", "Si Liu"], "title": "CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective", "comment": "ICCV25 (Highlight)", "summary": "Collaborative perception shares information among different agents and helps\nsolving problems that individual agents may face, e.g., occlusions and small\nsensing range. Prior methods usually separate the multi-agent fusion and\nmulti-time fusion into two consecutive steps. In contrast, this paper proposes\nan efficient collaborative perception that aggregates the observations from\ndifferent agents (space) and different times into a unified spatio-temporal\nspace simultanesouly. The unified spatio-temporal space brings two benefits,\ni.e., efficient feature transmission and superior feature fusion. 1) Efficient\nfeature transmission: each static object yields a single observation in the\nspatial temporal space, and thus only requires transmission only once (whereas\nprior methods re-transmit all the object features multiple times). 2) superior\nfeature fusion: merging the multi-agent and multi-time fusion into a unified\nspatial-temporal aggregation enables a more holistic perspective, thereby\nenhancing perception performance in challenging scenarios. Consequently, our\nCollaborative perception with Spatio-temporal Transformer (CoST) gains\nimprovement in both efficiency and accuracy. Notably, CoST is not tied to any\nspecific method and is compatible with a majority of previous methods,\nenhancing their accuracy while reducing the transmission bandwidth.", "AI": {"tldr": "CoST\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u65f6\u7a7a\u805a\u5408\u6765\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5c06\u591a\u667a\u80fd\u4f53\u878d\u5408\u548c\u591a\u65f6\u95f4\u878d\u5408\u5206\u5f00\u5904\u7406\u5bfc\u81f4\u7684\u6548\u7387\u548c\u6027\u80fd\u95ee\u9898\uff0c\u5e76\u89e3\u51b3\u5355\u667a\u80fd\u4f53\u9762\u4e34\u7684\u906e\u6321\u548c\u5c0f\u611f\u77e5\u8303\u56f4\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6765\u81ea\u4e0d\u540c\u667a\u80fd\u4f53\uff08\u7a7a\u95f4\uff09\u548c\u4e0d\u540c\u65f6\u95f4\uff08\u65f6\u95f4\uff09\u7684\u89c2\u6d4b\u805a\u5408\u5230\u7edf\u4e00\u7684\u65f6\u7a7a\u7a7a\u95f4\u4e2d\u7684\u9ad8\u6548\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\uff0c\u79f0\u4e3aCoST\uff08Collaborative perception with Spatio-temporal Transformer\uff09\u3002", "result": "CoST\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u6709\u6240\u63d0\u9ad8\uff0c\u51cf\u5c11\u4e86\u4f20\u8f93\u5e26\u5bbd\uff0c\u5e76\u80fd\u63d0\u9ad8\u5148\u524d\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3002", "conclusion": "CoST\u901a\u8fc7\u5c06\u591a\u667a\u80fd\u4f53\u548c\u591a\u65f6\u95f4\u4fe1\u606f\u878d\u5408\u5230\u7edf\u4e00\u7684\u7a00\u758f\u65f6\u7a7a\u805a\u5408\u4e2d\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u517c\u5bb9\u5927\u591a\u6570\u5148\u524d\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.00364", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00364", "abs": "https://arxiv.org/abs/2508.00364", "authors": ["Chanyoung Yoon", "Sangbong Yoo", "Soobin Yim", "Chansoo Kim", "Yun Jang"], "title": "OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions", "comment": null, "summary": "Designing residential interiors strongly impacts occupant satisfaction but\nremains challenging due to unstructured spatial layouts, high computational\ndemands, and reliance on expert knowledge. Existing methods based on\noptimization or deep learning are either computationally expensive or\nconstrained by data scarcity. Reinforcement learning (RL) approaches often\nlimit furniture placement to discrete positions and fail to incorporate design\nprinciples adequately. We propose OID-PPO, a novel RL framework for Optimal\nInterior Design using Proximal Policy Optimization, which integrates\nexpert-defined functional and visual guidelines into a structured reward\nfunction. OID-PPO utilizes a diagonal Gaussian policy for continuous and\nflexible furniture placement, effectively exploring latent environmental\ndynamics under partial observability. Experiments conducted across diverse room\nshapes and furniture configurations demonstrate that OID-PPO significantly\noutperforms state-of-the-art methods in terms of layout quality and\ncomputational efficiency. Ablation studies further demonstrate the impact of\nstructured guideline integration and reveal the distinct contributions of\nindividual design constraints.", "AI": {"tldr": "OID-PPO\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4e13\u5bb6\u8bbe\u8ba1\u7684\u6307\u5357\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u3001\u66f4\u9ad8\u6548\u7684\u5ba4\u5185\u8bbe\u8ba1\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5ba4\u5185\u8bbe\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u7a7a\u95f4\u5e03\u5c40\u3001\u9ad8\u8ba1\u7b97\u9700\u6c42\u548c\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u57fa\u4e8e\u4f18\u5316\u6216\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u53d7\u6570\u636e\u7a00\u758f\u6027\u9650\u5236\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5219\u5728\u5bb6\u5177\u653e\u7f6e\u7684\u79bb\u6563\u6027\u548c\u8bbe\u8ba1\u539f\u5219\u7684\u6574\u5408\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOID-PPO\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\uff0c\u8be5\u6846\u67b6\u5c06\u4e13\u5bb6\u5b9a\u4e49\u7684 funcional \u548c visual \u6307\u5357\u6574\u5408\u5230\u7ed3\u6784\u5316\u5956\u52b1\u51fd\u6570\u4e2d\uff0c\u5e76\u5229\u7528\u5bf9\u89d2\u9ad8\u65af\u7b56\u7565\u5b9e\u73b0\u8fde\u7eed\u7075\u6d3b\u7684\u5bb6\u5177\u5e03\u5c40\uff0c\u4ee5\u5e94\u5bf9\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u6f5c\u5728\u73af\u5883\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOID-PPO\u5728\u5404\u79cd\u623f\u95f4\u5f62\u72b6\u548c\u5bb6\u5177\u914d\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u5e03\u5c40\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u7ed3\u6784\u5316\u6307\u5357\u6574\u5408\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5404\u4e2a\u8bbe\u8ba1\u7ea6\u675f\u7684\u72ec\u7279\u8d21\u732e\u3002", "conclusion": "OID-PPO\u6846\u67b6\u5728\u5ba4\u5185\u8bbe\u8ba1\u9886\u57df\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5728\u5e03\u5c40\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u4e14\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u7ed3\u6784\u5316\u6307\u5357\u6574\u5408\u7684\u5f71\u54cd\u4ee5\u53ca\u5404\u4e2a\u8bbe\u8ba1\u7ea6\u675f\u7684\u8d21\u732e\u3002"}}
{"id": "2508.00614", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00614", "abs": "https://arxiv.org/abs/2508.00614", "authors": ["Lennart Meincke", "Ethan Mollick", "Lilach Mollick", "Dan Shapiro"], "title": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?", "comment": null, "summary": "This is the third in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate two commonly held\nprompting beliefs: a) offering to tip the AI model and b) threatening the AI\nmodel. Tipping was a commonly shared tactic for improving AI performance and\nthreats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,\n8:20) who observed that 'models tend to do better if you threaten them,' a\nclaim we subject to empirical testing here. We evaluate model performance on\nGPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\n  We demonstrate two things:\n  - Threatening or tipping a model generally has no significant effect on\nbenchmark performance.\n  - Prompt variations can significantly affect performance on a per-question\nlevel. However, it is hard to know in advance whether a particular prompting\napproach will help or harm the LLM's ability to answer any particular question.\n  Taken together, this suggests that simple prompting variations might not be\nas effective as previously assumed, especially for difficult problems. However,\nas reported previously (Meincke et al. 2025a), prompting approaches can yield\nsignificantly different results for individual questions.", "AI": {"tldr": "\u7b80\u5355\u63d0\u793a\uff08\u5982\u7ed9\u5c0f\u8d39\u6216\u5a01\u80c1\uff09\u5bf9AI\u6574\u4f53\u6027\u80fd\u5f71\u54cd\u4e0d\u5927\uff0c\u4f46\u53ef\u80fd\u5f71\u54cd\u5355\u9898\u8868\u73b0\uff0c\u4e0d\u8fc7\u96be\u4ee5\u9884\u6d4b\u3002", "motivation": "\u672c\u62a5\u544a\u65e8\u5728\u5e2e\u52a9\u5546\u4e1a\u3001\u6559\u80b2\u548c\u653f\u7b56\u9886\u5bfc\u8005\u7406\u89e3AI\u7684\u6280\u672f\u7ec6\u8282\uff0c\u901a\u8fc7\u5bf9\u4e24\u79cd\u5e38\u89c1\u7684\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\uff08\u7ed9\u4e88\u5c0f\u8d39\u548c\u5a01\u80c1\uff09\u8fdb\u884c\u5b9e\u8bc1\u6d4b\u8bd5\uff0c\u4ee5\u9a8c\u8bc1\u5176\u5bf9AI\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4e25\u683c\u6d4b\u8bd5\uff0c\u8c03\u67e5\u4e86\u4e24\u79cd\u5e38\u89c1\u7684\u63d0\u793a\u5de5\u7a0b\u4fe1\u5ff5\uff1aa) \u63d0\u51fa\u7ed9\u4e88AI\u6a21\u578b\u5c0f\u8d39\uff08tipping\uff09\u548c b) \u5a01\u80c1AI\u6a21\u578b\uff08threatening\uff09\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u6a21\u578b\u5728GPQA\u548cMMLU-Pro\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5a01\u80c1\u6216\u7ed9\u4e88\u6a21\u578b\u5c0f\u8d39\u901a\u5e38\u5bf9\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u6ca1\u6709\u663e\u8457\u5f71\u54cd\u3002\u867d\u7136\u63d0\u793a\u7684\u53d8\u5316\u53ef\u80fd\u663e\u8457\u5f71\u54cd\u6a21\u578b\u5728\u5355\u4e2a\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u4f46\u4e8b\u5148\u96be\u4ee5\u9884\u6599\u54ea\u79cd\u63d0\u793a\u65b9\u6cd5\u4f1a\u63d0\u9ad8\u6216\u635f\u5bb3LLM\u56de\u7b54\u7279\u5b9a\u95ee\u9898\u7684\u80fd\u529b\u3002", "conclusion": "\u63d0\u793a\u5de5\u7a0b\uff08prompting\uff09\u7684\u7b80\u5355\u53d8\u5316\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u590d\u6742\u95ee\u9898\u65f6\uff0c\u53ef\u80fd\u4e0d\u50cf\u4e4b\u524d\u5047\u8bbe\u7684\u90a3\u6837\u6709\u6548\u3002\u7136\u800c\uff0c\u7814\u7a76\u4e5f\u6307\u51fa\uff0c\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u5728\u5355\u4e2a\u95ee\u9898\u4e0a\u53ef\u80fd\u4ea7\u751f\u622a\u7136\u4e0d\u540c\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.00533", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2508.00533", "abs": "https://arxiv.org/abs/2508.00533", "authors": ["Maria-Andreea Filip", "Nathan Fitzpatrick"], "title": "Beyond asymptotic reasoning: a practical ground state projector based on the wall-Chebyshev expansion", "comment": "16 pages, 9 figures", "summary": "We introduce a quantum algorithm for ground-state preparation based on a\nChebyshev series approximation to the wall function. This projector can be\nefficiently implemented as a product of Hamiltonian operators, enabling a\nstraightforward realization via the linear combinations of unitaries method. We\nanalyze the asymptotic scaling and provide numerical benchmarks, demonstrating\nthat the wall-Chebyshev projector achieves competitive performance with leading\nmethods based on imaginary time evolution and alternative projector function\napproximations. Notably, our approach exhibits superior robustness and\nconvergence in scenarios where accurate ground-state energy estimates are\nunavailable, showing promise for realistic chemistry problems.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u901a\u8fc7Chebyshev\u7ea7\u6570\u903c\u8fd1\u5899\u51fd\u6570\u6765\u5236\u5907\u57fa\u6001\uff0c\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u89e3\u51b3\u5316\u5b66\u95ee\u9898\u4e2d\u57fa\u6001\u5236\u5907\u7684\u6311\u6218\uff0c\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eChebyshev\u7ea7\u6570\u903c\u8fd1\u5899\u51fd\u6570\uff08wall function\uff09\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u7528\u4e8e\u57fa\u6001\u5236\u5907\u3002\u8be5\u6295\u5f71\u7b97\u7b26\u53ef\u9ad8\u6548\u5730\u5b9e\u73b0\u4e3a\u54c8\u5bc6\u987f\u91cf\u7b97\u7b26\u7684\u4e58\u79ef\uff0c\u5e76\u901a\u8fc7\u9149\u7ec4\u5408\uff08linear combinations of unitaries\uff09\u65b9\u6cd5\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u6e10\u8fd1\u7f29\u653e\u5206\u6790\u548c\u6570\u503c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u8be5\u7b97\u6cd5\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u5728\u7f3a\u4e4f\u7cbe\u786e\u57fa\u6001\u80fd\u91cf\u4f30\u8ba1\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6536\u655b\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4e0e\u57fa\u4e8e\u865a\u65f6\u6f14\u5316\u548c\u5176\u4ed6\u6295\u5f71\u51fd\u6570\u903c\u8fd1\u7684\u9886\u5148\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5728\u9700\u8981\u7cbe\u786e\u7684\u57fa\u6001\u80fd\u91cf\u4f30\u8ba1\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u793a\u51fa\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00361", "abs": "https://arxiv.org/abs/2508.00361", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "title": "Honey Classification using Hyperspectral Imaging and Machine Learning", "comment": null, "summary": "In this paper, we propose a machine learning-based method for automatically\nclassifying honey botanical origins. Dataset preparation, feature extraction,\nand classification are the three main steps of the proposed method. We use a\nclass transformation method in the dataset preparation phase to maximize the\nseparability across classes. The feature extraction phase employs the Linear\nDiscriminant Analysis (LDA) technique for extracting relevant features and\nreducing the number of dimensions. In the classification phase, we use Support\nVector Machines (SVM) and K-Nearest Neighbors (KNN) models to classify the\nextracted features of honey samples into their botanical origins. We evaluate\nour system using a standard honey hyperspectral imaging (HSI) dataset.\nExperimental findings demonstrate that the proposed system produces\nstate-of-the-art results on this dataset, achieving the highest classification\naccuracy of 95.13% for hyperspectral image-based classification and 92.80% for\nhyperspectral instance-based classification.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\uff08LDA\u3001SVM\u3001KNN\uff09\u7684\u8702\u871c\u690d\u7269\u6765\u6e90\u81ea\u52a8\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c7b\u53d8\u6362\u548c\u7279\u5f81\u63d0\u53d6\u4f18\u5316\uff0c\u5728HSI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8695.13%\uff08\u56fe\u50cf\uff09\u548c92.80%\uff08\u5b9e\u4f8b\uff09\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u81ea\u52a8\u5206\u7c7b\u8702\u871c\u7684\u690d\u7269\u6765\u6e90\uff0c\u4ee5\u63d0\u9ad8\u5206\u7c7b\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a\u6570\u636e\u96c6\u51c6\u5907\uff08\u4f7f\u7528\u7c7b\u53d8\u6362\u6700\u5927\u5316\u7c7b\u95f4\u53ef\u5206\u79bb\u6027\uff09\u3001\u7279\u5f81\u63d0\u53d6\uff08\u4f7f\u7528\u7ebf\u6027\u5224\u522b\u5206\u6790LDA\u964d\u7ef4\u548c\u63d0\u53d6\u76f8\u5173\u7279\u5f81\uff09\u548c\u5206\u7c7b\uff08\u4f7f\u7528\u652f\u6301\u5411\u91cf\u673aSVM\u548cK\u8fd1\u90bbKNN\u6a21\u578b\uff09\u3002", "result": "\u5728\u6807\u51c6\u8702\u871c\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e8695.13%\u7684\u57fa\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u5206\u7c7b\u51c6\u786e\u7387\u548c92.80%\u7684\u57fa\u4e8e\u9ad8\u5149\u8c31\u5b9e\u4f8b\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u5206\u7c7b\u8702\u871c\u690d\u7269\u6765\u6e90\u7684\u65b9\u6cd5\uff0c\u5e76\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u5206\u7c7b\u7cbe\u5ea6\u3002"}}
{"id": "2508.00392", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00392", "abs": "https://arxiv.org/abs/2508.00392", "authors": ["Lijun Zhang", "Wenhao Yang", "Guanghui Wang", "Wei Jiang", "Zhi-Hua Zhou"], "title": "Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions", "comment": "arXiv admin note: text overlap with arXiv:1906.10851", "summary": "To deal with changing environments, a new performance measure -- adaptive\nregret, defined as the maximum static regret over any interval, was proposed in\nonline learning. Under the setting of online convex optimization, several\nalgorithms have been successfully developed to minimize the adaptive regret.\nHowever, existing algorithms lack universality in the sense that they can only\nhandle one type of convex functions and need apriori knowledge of parameters,\nwhich hinders their application in real-world scenarios. To address this\nlimitation, this paper investigates universal algorithms with dual adaptivity,\nwhich automatically adapt to the property of functions (convex, exponentially\nconcave, or strongly convex), as well as the nature of environments (stationary\nor changing). Specifically, we propose a meta-expert framework for dual\nadaptive algorithms, where multiple experts are created dynamically and\naggregated by a meta-algorithm. The meta-algorithm is required to yield a\nsecond-order bound, which can accommodate unknown function types. We further\nincorporate the technique of sleeping experts to capture the changing\nenvironments. For the construction of experts, we introduce two strategies\n(increasing the number of experts or enhancing the capabilities of experts) to\nachieve universality. Theoretical analysis shows that our algorithms are able\nto minimize the adaptive regret for multiple types of convex functions\nsimultaneously, and also allow the type of functions to switch between rounds.\nMoreover, we extend our meta-expert framework to online composite optimization,\nand develop a universal algorithm for minimizing the adaptive regret of\ncomposite functions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5143\u4e13\u5bb6\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u53ef\u5904\u7406\u591a\u79cd\u51f8\u51fd\u6570\u548c\u73af\u5883\u7c7b\u578b\u7684\u901a\u7528\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ee5\u6700\u5c0f\u5316\u81ea\u9002\u5e94\u9057\u61be\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u521b\u5efa\u3001\u805a\u5408\u548c\u5229\u7528\u7761\u7720\u4e13\u5bb6\u6765\u9002\u5e94\u51fd\u6570\u5c5e\u6027\u548c\u73af\u5883\u53d8\u5316\uff0c\u5e76\u5df2\u88ab\u8bc1\u660e\u5728\u7406\u8bba\u4e0a\u662f\u6709\u6548\u7684\uff0c\u751a\u81f3\u53ef\u4ee5\u6269\u5c55\u5230\u590d\u5408\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u5728\u5904\u7406\u81ea\u9002\u5e94\u9057\u61be\u65f6\uff0c\u5f80\u5f80\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u53ea\u80fd\u5904\u7406\u7279\u5b9a\u7c7b\u578b\u7684\u51f8\u51fd\u6570\uff0c\u5e76\u4e14\u9700\u8981\u9884\u5148\u77e5\u9053\u53c2\u6570\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u5f00\u53d1\u80fd\u591f\u81ea\u52a8\u9002\u5e94\u51fd\u6570\u5c5e\u6027\u548c\u73af\u5883\u6027\u8d28\u7684\u901a\u7528\u7b97\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u4e13\u5bb6\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u521b\u5efa\u548c\u805a\u5408\u591a\u4e2a\u4e13\u5bb6\uff0c\u5e76\u7ed3\u5408\u7761\u7720\u4e13\u5bb6\u6280\u672f\u6765\u6784\u5efa\u901a\u7528\u7b97\u6cd5\u3002\u4e13\u5bb6\u53ef\u4ee5\u901a\u8fc7\u589e\u52a0\u6570\u91cf\u6216\u589e\u5f3a\u80fd\u529b\u6765\u5b9e\u73b0\u901a\u7528\u6027\u3002\u8be5\u5143\u4e13\u5bb6\u6846\u67b6\u65e8\u5728\u5b9e\u73b0\u4e8c\u9636\u754c\u9650\uff0c\u4ee5\u9002\u5e94\u672a\u77e5\u7684\u51fd\u6570\u7c7b\u578b\uff0c\u5e76\u6355\u83b7\u73af\u5883\u7684\u53d8\u5316\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u540c\u65f6\u6700\u5c0f\u5316\u591a\u79cd\u51f8\u51fd\u6570\u7c7b\u578b\u7684\u81ea\u9002\u5e94\u9057\u61be\uff0c\u5e76\u4e14\u5141\u8bb8\u51fd\u6570\u7c7b\u578b\u5728\u4e0d\u540c\u8f6e\u6b21\u4e4b\u95f4\u5207\u6362\u3002\u6b64\u5916\uff0c\u8be5\u5143\u4e13\u5bb6\u6846\u67b6\u5df2\u88ab\u6210\u529f\u6269\u5c55\u5230\u5728\u7ebf\u590d\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u6700\u5c0f\u5316\u590d\u5408\u51fd\u6570\u81ea\u9002\u5e94\u9057\u61be\u7684\u901a\u7528\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5143\u4e13\u5bb6\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u5177\u6709\u53cc\u91cd\u9002\u5e94\u6027\u7684\u901a\u7528\u7b97\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u51f8\u51fd\u6570\u7c7b\u578b\uff08\u51f8\u51fd\u6570\u3001\u6307\u6570\u51f9\u51fd\u6570\u6216\u5f3a\u51f8\u51fd\u6570\uff09\u548c\u73af\u5883\u7c7b\u578b\uff08\u5e73\u7a33\u6216\u53d8\u5316\uff09\uff0c\u4ece\u800c\u6700\u5c0f\u5316\u81ea\u9002\u5e94\u9057\u61be\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u521b\u5efa\u548c\u805a\u5408\u591a\u4e2a\u4e13\u5bb6\u6765\u5b9e\u73b0\uff0c\u5e76\u7ed3\u5408\u4e86\u7761\u7720\u4e13\u5bb6\u6280\u672f\u4ee5\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u540c\u65f6\u6700\u5c0f\u5316\u591a\u79cd\u51f8\u51fd\u6570\u7c7b\u578b\u7684\u81ea\u9002\u5e94\u9057\u61be\uff0c\u5e76\u4e14\u5141\u8bb8\u5728\u4e0d\u540c\u8f6e\u6b21\u4e4b\u95f4\u5207\u6362\u51fd\u6570\u7c7b\u578b\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u88ab\u6269\u5c55\u5230\u5728\u7ebf\u590d\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u6700\u5c0f\u5316\u590d\u5408\u51fd\u6570\u81ea\u9002\u5e94\u9057\u61be\u7684\u901a\u7528\u7b97\u6cd5\u3002"}}
{"id": "2508.00619", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00619", "abs": "https://arxiv.org/abs/2508.00619", "authors": ["Shantanu Thorat", "Andrew Caines"], "title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models", "comment": "MPhil in Advanced Computer Science thesis for University of Cambridge", "summary": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors.", "AI": {"tldr": "AI\u6587\u672c\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86DACTYL\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u6bd4\u4e86BCE\u548cDXO\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\u3002\u7ed3\u679c\u663e\u793aDXO\u65b9\u6cd5\u5728\u5904\u7406OOD\u6570\u636e\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0a\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5668\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u5176\u9c81\u68d2\u6027\u6709\u5f85\u63d0\u9ad8\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u5c11\u6837\u672c\u3001\u5355\u6837\u672c\u751f\u6210\u4ee5\u53ca\u9886\u57df\u7279\u5b9a\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\uff0c\u68c0\u6d4b\u6548\u679c\u66f4\u5dee\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86DACTYL\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u5355\u6837\u672c/\u5c11\u6837\u672c\u751f\u6210\u6587\u672c\u548c\u9886\u57df\u7279\u5b9a\u7684\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u8bed\u8a00\u6a21\u578b\u3002\u5e76\u5bf9\u6bd4\u4e86\u4f7f\u7528\u6807\u51c6\u4e8c\u5143\u4ea4\u53c9\u71b5\uff08BCE\uff09\u4f18\u5316\u548c\u6df1\u5ea6X\u98ce\u9669\u4f18\u5316\uff08DXO\uff09\u4e24\u79cd\u65b9\u6cd5\u8bad\u7ec3\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5668\u3002", "result": "\u5728DACTYL\u6d4b\u8bd5\u96c6\u4e0a\uff0cBCE\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u6027\u80fd\u7565\u4f18\u4e8eDXO\u3002\u7136\u800c\uff0c\u5728\u5904\u7406\u975e\u5206\u5e03\u5916\uff08OOD\uff09\u6587\u672c\u65f6\uff0cDXO\u5206\u7c7b\u5668\u8868\u73b0\u66f4\u51fa\u8272\u3002\u5728\u6a21\u62df\u7684\u5b66\u751f\u8bba\u6587\u68c0\u6d4b\u573a\u666f\u4e2d\uff0cDXO\u5206\u7c7b\u5668\u76f8\u6bd4BCE\u5206\u7c7b\u5668\u5728\u4f4e\u8bef\u62a5\u7387\u4e0b\uff0c\u5b8fF1\u5206\u6570\u9ad8\u51fa50.56\u4e2a\u767e\u5206\u70b9\uff0c\u8868\u660eDXO\u5206\u7c7b\u5668\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u4e0d\u6613\u8fc7\u62df\u5408\u3002", "conclusion": "\u73b0\u6709AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u5bf9\u5355\u6837\u672c/\u5c11\u6837\u672c\u751f\u6210\u6587\u672c\u548c\u6301\u7eed\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u4e0d\u591f\u9c81\u68d2\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165DACTYL\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u6bd4\u4e86\u6807\u51c6\u4e8c\u5143\u4ea4\u53c9\u71b5\uff08BCE\uff09\u548c\u6df1\u5ea6X\u98ce\u9669\u4f18\u5316\uff08DXO\uff09\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff0c\u53d1\u73b0DXO\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u5728\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8eBCE\uff0c\u5c24\u5176\u5728\u5904\u7406\u975e\u5206\u5e03\u5916\uff08OOD\uff09\u6570\u636e\u65f6\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2508.00562", "categories": ["quant-ph", "math.CO", "es: 05C50 (Primary), 05C75, 05C90, 81P45, 81Q35 (Secondary)"], "pdf": "https://arxiv.org/pdf/2508.00562", "abs": "https://arxiv.org/abs/2508.00562", "authors": ["Hartosh Singh Bal"], "title": "Persistent Quantum Memory in Iterated Lifts", "comment": null, "summary": "We study quantum coherence in continuous-time quantum walks on perfect graphs\ngenerated by the symmetric lift ${\\mathrm{HL}}'_2(G)$, a canonical, unweighted,\nundirected construction defined as the line graph of a bipartite double cover\nof $G$. This lift acts as both a coherence-preserving and coherence-inducing\ntransformation: it preserves and scales structured quantum interference in\nhighly symmetric base graphs, and induces sustained coherence in random or\nweakly structured ones.\n  In small graphs such as $K_4$, $K_5$, and the Petersen graph, where quantum\nwalks exhibit sharp revivals and high return probability, repeated\n$\\mathrm{HL}'_2$ lifting produces towers of perfect graphs with thousands to\ntens of thousands of vertices that retain periodic or quasi-periodic coherence.\nWhen applied to random regular or Erd\\H{o}s--R\\'enyi graphs with flat or\ndecaying return behavior, the lift introduces structured interference and\nsignificant amplification of mean and peak return probabilities.\n  To quantify these effects, we evaluate standard coherence metrics from\nquantum resource theory, including inverse participation ratio (IPR), purity,\nrelative entropy of coherence, and the logarithmic coherence number. These\nmeasures confirm that $\\mathrm{HL}'_2$ lifting delocalizes eigenstates,\nincreases coherence entropy, and expands the basis support of quantum states.\nThese results demonstrate that $\\mathrm{HL}'_2$ is a scalable and structurally\ngrounded mechanism for organizing quantum interference, and introduce a new\nfamily of perfect graphs that support long-time quantum coherence without\nspectral tuning or engineered weights.", "AI": {"tldr": "HL\u20192 \u63d0\u5347\u662f\u4e00\u79cd\u5728\u5b8c\u7f8e\u56fe\u4e0a\u4f20\u8f93\u91cf\u5b50\u884c\u8d70\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u91cf\u5b50\u76f8\u5e72\u6027\u3002", "motivation": "\u4e3a\u4e86\u91cf\u5316\u8fd9\u4e9b\u6548\u5e94\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\u4e2d\u7684\u6807\u51c6\u76f8\u5e72\u6027\u5ea6\u91cf\uff0c\u5305\u62ec\u9006\u53c2\u4e0e\u6bd4 (IPR)\u3001\u7eaf\u5ea6\u3001\u76f8\u5e72\u6027\u76f8\u5bf9\u71b5\u548c\u5bf9\u6570\u76f8\u5e72\u6570\u3002", "method": "\u7814\u7a76\u4e86\u5728\u7531\u5bf9\u79f0\u63d0\u5347 HL\u20192(G)\uff08G \u7684\u4e8c\u5206\u53cc\u8986\u76d6\u7684\u7ebf\u56fe\uff09\u4ea7\u751f\u7684\u5b8c\u7f8e\u56fe\u4e0a\u7684\u8fde\u7eed\u65f6\u95f4\u91cf\u5b50\u884c\u8d70\u4e2d\u7684\u91cf\u5b50\u76f8\u5e72\u6027\u3002", "result": "HL\u20192 \u63d0\u5347\u4f1a\u79bb\u57df\u672c\u5f81\u6001\uff0c\u589e\u52a0\u76f8\u5e72\u71b5\uff0c\u5e76\u6269\u5c55\u91cf\u5b50\u6001\u7684\u57fa\u652f\u6491\u3002\u5728\u5c0f\u56fe\u548c\u968f\u673a\u56fe\u4e0a\uff0cHL\u20192 \u63d0\u5347\u4f1a\u4ea7\u751f\u5177\u6709\u957f\u65f6\u76f8\u5e72\u6027\u7684\u5b8c\u7f8e\u56fe\u5854\u3002", "conclusion": "HL\u20192 \u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u7ed3\u6784\u5316\u7684\u91cf\u5b50\u5e72\u6d89\u7ec4\u7ec7\u673a\u5236\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5b8c\u7f8e\u56fe\u5bb6\u65cf\uff0c\u8be5\u5bb6\u65cf\u80fd\u591f\u5728\u6ca1\u6709\u5149\u8c31\u8c03\u6574\u6216\u5de5\u7a0b\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u652f\u6301\u957f\u65f6\u95f4\u91cf\u5b50\u76f8\u5e72\u3002"}}
{"id": "2508.00366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00366", "abs": "https://arxiv.org/abs/2508.00366", "authors": ["Liang Han", "Xu Zhang", "Haichuan Song", "Kanle Shi", "Yu-Shen Liu", "Zhizhong Han"], "title": "SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies", "comment": "Accepted by ICCV 2025", "summary": "Surface reconstruction from sparse views aims to reconstruct a 3D shape or\nscene from few RGB images. The latest methods are either generalization-based\nor overfitting-based. However, the generalization-based methods do not\ngeneralize well on views that were unseen during training, while the\nreconstruction quality of overfitting-based methods is still limited by the\nlimited geometry clues. To address this issue, we propose SparseRecon, a novel\nneural implicit reconstruction method for sparse views with volume\nrendering-based feature consistency and uncertainty-guided depth constraint.\nFirstly, we introduce a feature consistency loss across views to constrain the\nneural implicit field. This design alleviates the ambiguity caused by\ninsufficient consistency information of views and ensures completeness and\nsmoothness in the reconstruction results. Secondly, we employ an\nuncertainty-guided depth constraint to back up the feature consistency loss in\nareas with occlusion and insignificant features, which recovers geometry\ndetails for better reconstruction quality. Experimental results demonstrate\nthat our method outperforms the state-of-the-art methods, which can produce\nhigh-quality geometry with sparse-view input, especially in the scenarios with\nsmall overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSparseRecon\u7684\u65b0\u578b\u795e\u7ecf\u9690\u5f0f\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7279\u5f81\u4e00\u81f4\u6027\u635f\u5931\u548c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u6df1\u5ea6\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe\u4e09\u7ef4\u91cd\u5efa\u7684\u6311\u6218\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6cdb\u5316\u7684\u65b9\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u89c6\u56fe\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u800c\u57fa\u4e8e\u8fc7\u62df\u5408\u7684\u65b9\u6cd5\u7684\u91cd\u5efa\u8d28\u91cf\u53d7\u9650\u4e8e\u6709\u9650\u7684\u51e0\u4f55\u7ebf\u7d22\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u7a00\u758f\u89c6\u56fe\u5e76\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u9690\u5f0f\u91cd\u5efa\u65b9\u6cd5SparseRecon\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u4f53\u79ef\u6e32\u67d3\u7684\u7279\u5f81\u4e00\u81f4\u6027\u548c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u6df1\u5ea6\u7ea6\u675f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u901a\u8fc7\u5f15\u5165\u8de8\u89c6\u56fe\u7684\u7279\u5f81\u4e00\u81f4\u6027\u635f\u5931\u6765\u7ea6\u675f\u795e\u7ecf\u9690\u5f0f\u573a\uff0c\u4ee5\u89e3\u51b3\u89c6\u56fe\u4fe1\u606f\u4e0d\u8db3\u5bfc\u81f4\u7684\u6b67\u4e49\u95ee\u9898\uff0c\u5e76\u4fdd\u8bc1\u91cd\u5efa\u7ed3\u679c\u7684\u5b8c\u6574\u6027\u548c\u5149\u6ed1\u6027\u3002\u6b64\u5916\uff0c\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u6df1\u5ea6\u7ea6\u675f\u6765\u8865\u5145\u7279\u5f81\u4e00\u81f4\u6027\u635f\u5931\uff0c\u7279\u522b\u662f\u5728\u906e\u6321\u548c\u7279\u5f81\u4e0d\u660e\u663e\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u6062\u590d\u51e0\u4f55\u7ec6\u8282\u5e76\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u7684\u6761\u4ef6\u4e0b\uff0c\u5c24\u5176\u662f\u5728\u89c6\u56fe\u91cd\u53e0\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u51e0\u4f55\u5f62\u72b6\u3002", "conclusion": "\u672c\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u4e0b\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u5c24\u5176\u662f\u5728\u89c6\u56fe\u91cd\u53e0\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.00394", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00394", "abs": "https://arxiv.org/abs/2508.00394", "authors": ["Antonis Klironomos", "Baifan Zhou", "Zhipeng Tan", "Zhuoxun Zheng", "Mohamed H. Gad-Elrab", "Heiko Paulheim", "Evgeny Kharlamov"], "title": "ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs", "comment": null, "summary": "Nowadays machine learning (ML) practitioners have access to numerous ML\nlibraries available online. Such libraries can be used to create ML pipelines\nthat consist of a series of steps where each step may invoke up to several ML\nlibraries that are used for various data-driven analytical tasks. Development\nof high-quality ML pipelines is non-trivial; it requires training, ML\nexpertise, and careful development of each step. At the same time, domain\nexperts in science and engineering may not possess such ML expertise and\ntraining while they are in pressing need of ML-based analytics. In this paper,\nwe present our ExeKGLib, a Python library enhanced with a graphical interface\nlayer that allows users with minimal ML knowledge to build ML pipelines. This\nis achieved by relying on knowledge graphs that encode ML knowledge in simple\nterms accessible to non-ML experts. ExeKGLib also allows improving the\ntransparency and reusability of the built ML workflows and ensures that they\nare executable. We show the usability and usefulness of ExeKGLib by presenting\nreal use cases.", "AI": {"tldr": "ExeKGLib\u662f\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684Python\u5e93\uff0c\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u548c\u56fe\u5f62\u754c\u9762\uff0c\u8ba9\u975e\u673a\u5668\u5b66\u4e60\u4e13\u5bb6\u4e5f\u80fd\u8f7b\u677e\u6784\u5efa\u3001\u6267\u884c\u548c\u590d\u7528\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u9886\u57df\u4e13\u5bb6\uff08\u5c24\u5176\u662f\u5728\u79d1\u5b66\u548c\u5de5\u7a0b\u9886\u57df\uff09\u5728\u7f3a\u4e4f\u673a\u5668\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\u548c\u57f9\u8bad\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u7136\u8feb\u5207\u9700\u8981\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5206\u6790\u5de5\u5177\u7684\u95ee\u9898\u3002", "method": "ExeKGLib\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u6765\u7f16\u7801\u673a\u5668\u5b66\u4e60\u77e5\u8bc6\uff0c\u5e76\u63d0\u4f9b\u56fe\u5f62\u7528\u6237\u754c\u9762\uff0c\u8ba9\u4e0d\u5177\u5907\u673a\u5668\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\u7684\u7528\u6237\u4e5f\u80fd\u6784\u5efa\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\u3002", "result": "\u5c55\u793a\u4e86ExeKGLib\u7684\u53ef\u7528\u6027\u548c\u5b9e\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u7528\u4f8b\u8fdb\u884c\u4e86\u8bf4\u660e\u3002", "conclusion": "ExeKGLib\u662f\u4e00\u4e2aPython\u5e93\uff0c\u5b83\u901a\u8fc7\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u6765\u7b80\u5316\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\uff08ML pipelines\uff09\u7684\u6784\u5efa\u8fc7\u7a0b\uff0c\u4f7f\u5f97\u6ca1\u6709\u673a\u5668\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\u7684\u7528\u6237\u4e5f\u80fd\u8f7b\u677e\u521b\u5efa\u9ad8\u8d28\u91cf\u7684ML\u6d41\u6c34\u7ebf\u3002\u8be5\u5e93\u63d0\u9ad8\u4e86ML\u5de5\u4f5c\u6d41\u7684\u900f\u660e\u5ea6\u3001\u53ef\u91cd\u7528\u6027\u548c\u53ef\u6267\u884c\u6027\u3002"}}
{"id": "2508.00669", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00669", "abs": "https://arxiv.org/abs/2508.00669", "authors": ["Wenxuan Wang", "Zizhan Ma", "Meidan Ding", "Shiyi Zheng", "Shengyuan Liu", "Jie Liu", "Jiaming Ji", "Wenting Chen", "Xiang Li", "Linlin Shen", "Yixuan Yuan"], "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.", "AI": {"tldr": "\u533b\u5b66 LLM \u63a8\u7406\u80fd\u529b\u6709\u5f85\u63d0\u9ad8\uff0c\u672c\u6587\u7efc\u8ff0\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u589e\u5f3a\u6280\u672f\uff0c\u5206\u6790\u4e86\u5176\u5728\u4e0d\u540c\u6a21\u5f0f\u548c\u5e94\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6307\u51fa\u4e86\u5fe0\u5b9e-\u5408\u7406\u6027\u5dee\u8ddd\u548c\u591a\u6a21\u6001\u63a8\u7406\u7b49\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u5b66\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5728\u7cfb\u7edf\u6027\u3001\u900f\u660e\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u6210\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u7684\u5173\u952e\u9700\u6c42\uff0c\u4f46\u76ee\u524d\u4ecd\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u63a8\u52a8 LLM \u4ece\u5355\u4e00\u7684\u7b54\u6848\u751f\u6210\u8f6c\u5411\u4e13\u6ce8\u4e8e\u533b\u5b66\u63a8\u7406\u7684\u7814\u7a76\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u56de\u987e\u8fd9\u4e00\u65b0\u5174\u9886\u57df\uff0c\u4e3a\u7406\u89e3\u548c\u53d1\u5c55\u66f4\u53ef\u9760\u7684\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u672c\u7bc7\u8bba\u6587\u662f\u4e00\u9879\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u65e8\u5728\u68b3\u7406\u548c\u5206\u6790\u533b\u5b66\u9886\u57df\u7528\u4e8e\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u7684\u6280\u672f\u3002\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u63a8\u7406\u589e\u5f3a\u6280\u672f\u7684\u5206\u7c7b\u6cd5\uff0c\u5c06\u5176\u5206\u4e3a\u8bad\u7ec3\u65f6\u7b56\u7565\uff08\u4f8b\u5982\uff0c\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\uff09\u548c\u6d4b\u8bd5\u65f6\u673a\u5236\uff08\u4f8b\u5982\uff0c\u63d0\u793a\u5de5\u7a0b\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff09\u3002\u8bba\u6587\u5206\u6790\u4e86\u8fd9\u4e9b\u6280\u672f\u5728\u6587\u672c\u3001\u56fe\u50cf\u3001\u4ee3\u7801\u7b49\u4e0d\u540c\u6570\u636e\u6a21\u5f0f\u4ee5\u53ca\u8bca\u65ad\u3001\u6559\u80b2\u3001\u6cbb\u7597\u89c4\u5212\u7b49\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u5177\u4f53\u5e94\u7528\u60c5\u51b5\u3002\u540c\u65f6\uff0c\u7814\u7a76\u4e5f\u56de\u987e\u4e86\u8bc4\u4f30\u57fa\u51c6\u7684\u53d1\u5c55\uff0c\u4ece\u6700\u521d\u7684\u51c6\u786e\u6027\u5ea6\u91cf\u6f14\u53d8\u5230\u5bf9\u63a8\u7406\u8d28\u91cf\u548c\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\u7684\u9ad8\u7ea7\u8bc4\u4f30\u3002\u5206\u6790\u57fa\u4e8e 2022 \u5e74\u81f3 2025 \u5e74\u95f4\u7684 60 \u7bc7\u76f8\u5173\u7814\u7a76\u3002", "result": "\u672c\u7bc7\u8bba\u6587\u901a\u8fc7\u5bf9 60 \u7bc7\u76f8\u5173\u7814\u7a76\u7684\u7cfb\u7edf\u6027\u56de\u987e\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u533b\u5b66\u63a8\u7406\u589e\u5f3a\u6280\u672f\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u6280\u672f\u5728\u4e0d\u540c\u6570\u636e\u6a21\u5f0f\u548c\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u8fd0\u7528\u60c5\u51b5\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8bc4\u4f30\u57fa\u51c6\u5df2\u4ece\u57fa\u7840\u7684\u51c6\u786e\u6027\u6307\u6807\u53d1\u5c55\u5230\u66f4\u590d\u6742\u7684\u63a8\u7406\u8d28\u91cf\u548c\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u3002\u8bba\u6587\u8bc6\u522b\u51fa\u5fe0\u5b9e-\u5408\u7406\u6027\u5dee\u8ddd\u548c\u539f\u751f\u591a\u6a21\u6001\u63a8\u7406\u9700\u6c42\u7b49\u5173\u952e\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002", "conclusion": "\u672c\u7bc7\u8bba\u6587\u8bc6\u522b\u51fa\u5728\u533b\u5b66\u9886\u57df\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7cfb\u7edf\u6027\u3001\u900f\u660e\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5173\u952e\u5dee\u8ddd\uff0c\u8fd9\u662f\u4e34\u5e8a\u5b9e\u8df5\u7684\u57fa\u77f3\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u7814\u7a76\u4eba\u5458\u6b63\u4ece\u5355\u4e00\u6b65\u9aa4\u7684\u7b54\u6848\u751f\u6210\u8f6c\u5411\u4e13\u95e8\u4e3a\u533b\u5b66\u63a8\u7406\u8bbe\u8ba1\u7684 LLM\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u63a8\u7406\u589e\u5f3a\u6280\u672f\u7684\u5206\u7c7b\u6cd5\uff0c\u5206\u4e3a\u8bad\u7ec3\u65f6\u7b56\u7565\uff08\u5982\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\uff09\u548c\u6d4b\u8bd5\u65f6\u673a\u5236\uff08\u5982\u63d0\u793a\u5de5\u7a0b\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff09\u3002\u8bba\u6587\u5206\u6790\u4e86\u8fd9\u4e9b\u6280\u672f\u5728\u4e0d\u540c\u6570\u636e\u6a21\u5f0f\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u4ee3\u7801\uff09\u548c\u5173\u952e\u4e34\u5e8a\u5e94\u7528\uff08\u8bca\u65ad\u3001\u6559\u80b2\u3001\u6cbb\u7597\u89c4\u5212\uff09\u4e2d\u7684\u5e94\u7528\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u8c03\u67e5\u4e86\u8bc4\u4f30\u57fa\u51c6\u7684\u6f14\u53d8\uff0c\u4ece\u7b80\u5355\u7684\u51c6\u786e\u6027\u6307\u6807\u5230\u5bf9\u63a8\u7406\u8d28\u91cf\u548c\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\u7684\u590d\u6742\u8bc4\u4f30\u3002\u57fa\u4e8e\u5bf9 2022-2025 \u5e74 60 \u9879\u5f00\u521b\u6027\u7814\u7a76\u7684\u5206\u6790\uff0c\u8bba\u6587\u6700\u7ec8\u6307\u51fa\u4e86\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u5fe0\u5b9e-\u5408\u7406\u6027\u5dee\u8ddd\u548c\u5bf9\u539f\u751f\u591a\u6a21\u6001\u63a8\u7406\u7684\u9700\u6c42\uff0c\u5e76\u6982\u8ff0\u4e86\u672a\u6765\u6784\u5efa\u9ad8\u6548\u3001\u7a33\u5065\u4e14\u5177\u6709\u793e\u4f1a\u6280\u672f\u8d23\u4efb\u7684\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2508.00634", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00634", "abs": "https://arxiv.org/abs/2508.00634", "authors": ["S. M. Zangi", "Chitra Shukla", "Khalid Naseer", "Saeed Haddadi"], "title": "Swapped Entanglement in High-Dimensional Quantum Systems", "comment": "7 pages, 4 figures", "summary": "Entanglement swapping is a fundamental protocol in quantum information\nprocessing that enables the distribution of entanglement between distant\nquantum systems. In this work, we first extend the concept of entanglement\nswapping to higher-dimensional quantum systems, specifically qudits. We then\nanalyze the dynamics of entanglement swapping and quantify the average swapped\nentanglement in terms of concurrence and negativity. Our results demonstrate\nthat higher-dimensional systems offer enhanced entanglement distribution\ncapabilities compared to qubit-based protocols. We also discuss the application\nof entangled qudits in terms of long-distance teleportation that provides the\nbase for quantum repeaters. Furthermore, we discuss the entanglement swapping\nfor a real and noisy system. The behaviors of entanglement against fidelity\nwith different dimensions are also discussed.", "AI": {"tldr": "Entanglement swapping extended to higher dimensions (qudits) improves entanglement distribution and has applications in quantum repeaters, even in noisy systems.", "motivation": "To explore and enhance entanglement distribution in quantum information processing by extending entanglement swapping to higher-dimensional systems and analyzing its performance and applications.", "method": "Extending entanglement swapping to higher-dimensional systems (qudits) and analyzing its dynamics using concurrence and negativity.", "result": "Higher-dimensional systems show enhanced entanglement distribution capabilities. Applications in long-distance teleportation and quantum repeaters are discussed. The behavior of entanglement against fidelity in noisy systems with different dimensions is also analyzed.", "conclusion": "Higher-dimensional systems enhance entanglement distribution compared to qubit-based protocols, with applications in long-distance teleportation and quantum repeaters. The paper also discusses entanglement swapping in noisy systems and its behavior against fidelity with different dimensions."}}
{"id": "2508.00367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00367", "abs": "https://arxiv.org/abs/2508.00367", "authors": ["Joonmyung Choi", "Sanghyeok Lee", "Byungoh Ko", "Eunseo Kim", "Jihyung Kil", "Hyunwoo J. Kim"], "title": "Representation Shift: Unifying Token Compression with FlashAttention", "comment": "International Conference on Computer Vision (ICCV), 2025", "summary": "Transformers have demonstrated remarkable success across vision, language,\nand video. Yet, increasing task complexity has led to larger models and more\ntokens, raising the quadratic cost of self-attention and the overhead of GPU\nmemory access. To reduce the computation cost of self-attention, prior work has\nproposed token compression techniques that drop redundant or less informative\ntokens. Meanwhile, fused attention kernels such as FlashAttention have been\ndeveloped to alleviate memory overhead by avoiding attention map construction\nand its associated I/O to HBM. This, however, makes it incompatible with most\ntraining-free token compression methods, which rely on attention maps to\ndetermine token importance. Here, we propose Representation Shift, a\ntraining-free, model-agnostic metric that measures the degree of change in each\ntoken's representation. This seamlessly integrates token compression with\nFlashAttention, without attention maps or retraining. Our method further\ngeneralizes beyond Transformers to CNNs and state space models. Extensive\nexperiments show that Representation Shift enables effective token compression\ncompatible with FlashAttention, yielding significant speedups of up to 5.5% and\n4.4% in video-text retrieval and video QA, respectively. Code is available at\nhttps://github.com/mlvlab/Representation-Shift.", "AI": {"tldr": "Representation Shift is a new method for token compression that works with FlashAttention and speeds up models.", "motivation": "To reduce the computation cost of self-attention, prior works have proposed token compression techniques that drop redundant or less informative tokens. Meanwhile, fused attention kernels such as FlashAttention have been developed to alleviate memory overhead by avoiding attention map construction and its associated I/O to HBM. However, this makes it incompatible with most training-free token compression methods, which rely on attention maps to determine token importance.", "method": "Representation Shift, a training-free, model-agnostic metric that measures the degree of change in each token's representation. This seamlessly integrates token compression with FlashAttention, without attention maps or retraining. Our method further generalizes beyond Transformers to CNNs and state space models.", "result": "Representation Shift enables effective token compression compatible with FlashAttention, yielding significant speedups of up to 5.5% and 4.4% in video-text retrieval and video QA, respectively.", "conclusion": "Representation Shift"}}
{"id": "2508.00410", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00410", "abs": "https://arxiv.org/abs/2508.00410", "authors": ["Zizhuo Zhang", "Jianing Zhu", "Xinmu Ge", "Zihua Zhao", "Zhanke Zhou", "Xuan Li", "Xiao Feng", "Jiangchao Yao", "Bo Han"], "title": "Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement", "comment": null, "summary": "Although reinforcement learning with verifiable rewards (RLVR) shows promise\nin improving the reasoning ability of large language models (LLMs), the scaling\nup dilemma remains due to the reliance on human annotated labels especially for\ncomplex tasks. Recent alternatives that explore various self-reward signals\nexhibit the eliciting potential of LLM reasoning, but suffer from the\nnon-negligible collapse issue. Inspired by the success of self-supervised\nlearning, we propose \\textit{Co-Reward}, a novel RL framework that leverages\ncontrastive agreement across semantically analogical questions as a reward\nbasis. Specifically, we construct a similar question for each training sample\n(without labels) and synthesize their individual surrogate labels through a\nsimple rollout voting, and then the reward is constructed by cross-referring\nthe labels of each question pair to enforce the internal reasoning consistency\nacross analogical inputs. Intuitively, such a self-supervised reward-shaping\nmechanism increases the difficulty of learning collapse into a trivial\nsolution, and promotes stable reasoning elicitation and improvement through\nexpanding the input sample variants. Empirically, Co-Reward achieves superior\nperformance compared to other self-reward baselines on multiple reasoning\nbenchmarks and LLM series, and reaches or even surpasses ground-truth (GT)\nlabeled reward, with improvements of up to $+6.8\\%$ on MATH500 over GT reward\non Llama-3.2-3B-Instruct. Our code is publicly available at\nhttps://github.com/tmlr-group/Co-Reward.", "AI": {"tldr": "Co-Reward \u662f\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u5229\u7528\u5bf9\u6bd4\u534f\u8bae\u548c\u7c7b\u6bd4\u95ee\u9898\u6765\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86\u73b0\u6709 RLVR \u548c\u81ea\u5956\u52b1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728 LLM \u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u4e14\u53ef\u6bd4 GT \u6807\u7b7e\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u5728\u6269\u5c55\u65b9\u9762\u5bf9\u4eba\u7c7b\u6807\u6ce8\u6807\u7b7e\u7684\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u81ea\u5956\u52b1\u4fe1\u53f7\u5728\u5904\u7406 LLM \u63a8\u7406\u65f6\u9047\u5230\u7684\u4e0d\u53ef\u907f\u514d\u7684\u5d29\u6e83\u95ee\u9898\u3002", "method": "Co-Reward \u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u5229\u7528\u5bf9\u6bd4\u534f\u8bae\u6765\u5956\u52b1\u8bed\u4e49\u7c7b\u6bd4\u95ee\u9898\u3002\u5b83\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u6784\u5efa\u4e00\u4e2a\u7c7b\u4f3c\u7684\u95ee\u9898\uff0c\u5e76\u5bf9\u5b83\u4eec\u8fdb\u884c\u7b80\u5355\u7684\u8f6e\u6eda\u6295\u7968\u6765\u5408\u6210\u5b83\u4eec\u5404\u81ea\u7684\u4ee3\u7406\u6807\u7b7e\u3002\u7136\u540e\uff0c\u901a\u8fc7\u4ea4\u53c9\u5f15\u7528\u6bcf\u4e2a\u95ee\u9898\u5bf9\u7684\u6807\u7b7e\u6765\u6784\u5efa\u5956\u52b1\uff0c\u4ee5\u5f3a\u5236\u6267\u884c\u8de8\u7c7b\u6bd4\u8f93\u5165\u7684\u5185\u90e8\u63a8\u7406\u4e00\u81f4\u6027\u3002", "result": "Co-Reward \u5728 MATH500 \u4e0a\u6bd4 Llama-3.2-3B-Instruct \u4e0a\u7684 GT \u5956\u52b1\u63d0\u9ad8\u4e86 +6.8%\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u548c LLM \u7cfb\u5217\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u81ea\u5956\u52b1\u57fa\u7ebf\u7684\u6027\u80fd\u3002", "conclusion": "Co-Reward \u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u548c LLM \u7cfb\u5217\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u81ea\u5956\u52b1\u57fa\u7ebf\u3001\u53ef\u4e0e\u751a\u81f3\u8d85\u8d8a\u5730\u9762\u771f\u5b9e\uff08GT\uff09\u6807\u7b7e\u5956\u52b1\u7684\u6027\u80fd\uff0c\u5728 MATH500 \u4e0a\u6bd4 Llama-3.2-3B-Instruct \u4e0a\u7684 GT \u5956\u52b1\u63d0\u9ad8\u4e86 +6.8%\u3002"}}
{"id": "2508.00673", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00673", "abs": "https://arxiv.org/abs/2508.00673", "authors": ["Farhan Farsi", "Farnaz Aghababaloo", "Shahriar Shariati Motlagh", "Parsa Ghofrani", "MohammadAli SadraeiJavaheri", "Shayan Bali", "Amirhossein Shabani", "Farbod Bijary", "Ghazal Zamaninejad", "AmirMohammad Salehoof", "Saeedeh Momtazi"], "title": "MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language", "comment": "Preprint. Under review", "summary": "As large language models (LLMs) become increasingly embedded in our daily\nlives, evaluating their quality and reliability across diverse contexts has\nbecome essential. While comprehensive benchmarks exist for assessing LLM\nperformance in English, there remains a significant gap in evaluation resources\nfor other languages. Moreover, because most LLMs are trained primarily on data\nrooted in European and American cultures, they often lack familiarity with\nnon-Western cultural contexts. To address this limitation, our study focuses on\nthe Persian language and Iranian culture. We introduce 19 new evaluation\ndatasets specifically designed to assess LLMs on topics such as Iranian law,\nPersian grammar, Persian idioms, and university entrance exams. Using these\ndatasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing\ncultural and linguistic evaluation gap in the field.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u521b\u5efa\u548c\u4f7f\u7528\u65b0\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6ce2\u65af\u8bed\u548c\u4f0a\u6717\u6587\u5316\u80cc\u666f\u4e0b\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7684LLM\u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u96c6\u4e2d\u5728\u82f1\u8bed\u548c\u897f\u65b9\u6587\u5316\u80cc\u666f\u4e0b\uff0c\u5bf9\u4e8e\u6ce2\u65af\u8bed\u548c\u4f0a\u6717\u6587\u5316\u7b49\u975e\u897f\u65b9\u80cc\u666f\u7684\u8bc4\u4f30\u8d44\u6e90\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e3a\u8fd9\u4e9b\u9886\u57df\u5f00\u53d1\u4e13\u95e8\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e8619\u4e2a\u65b0\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u4f0a\u6717\u6cd5\u5f8b\u3001\u6ce2\u65af\u8bed\u8bed\u6cd5\u3001\u6ce2\u65af\u8bed\u4e60\u8bed\u548c\u5927\u5b66\u5165\u5b66\u8003\u8bd5\u7b49\u4e3b\u9898\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e9b\u6570\u636e\u96c6\u5bf941\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u572841\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u672c\u7814\u7a76\u4e3a\u8bc4\u4f30LLM\u5728\u6ce2\u65af\u8bed\u548c\u4f0a\u6717\u6587\u5316\u80cc\u666f\u4e0b\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u4f18\u52bf\u548c\u52a3\u52bf\u3002", "conclusion": "LLMs\u5728\u6ce2\u65af\u8bed\u548c\u4f0a\u6717\u6587\u5316\u80cc\u666f\u4e0b\u7684\u8bc4\u4f30\u80fd\u529b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u516519\u4e2a\u65b0\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5e76\u572841\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002"}}
{"id": "2508.00648", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00648", "abs": "https://arxiv.org/abs/2508.00648", "authors": ["Boubakeur Khantoul", "Bilel Hamil", "Amar Benchikha"], "title": "Uncertainty Relation for Pseudo-Hermitian Quantum Systems", "comment": "10 pages, 1 figure", "summary": "This study investigates pseudo-Hermitian quantum mechanics, where the\nHamiltonian satisfies a modified Hermiticity condition. We extend the\nuncertainty relation for such systems, demonstrating its equivalence to the\nstandard Hermitian case within a pseudo-Hermitian inner product. Analytical\nsolutions to the time-dependent Schr\\\"odinger equation with a linearly evolving\npotential are derived. Furthermore, we show that the uncertainty relation for\nposition and momentum remains real and greater than 1/2, highlighting the\nsignificance of non-Hermitian systems in quantum mechanics.", "AI": {"tldr": "\u4f2ahermitian\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\u4e0e\u6807\u51c6hermitian\u60c5\u51b5\u7b49\u4ef7\uff0c\u975ehermitian\u7cfb\u7edf\u5f88\u91cd\u8981\u3002", "motivation": "\u7814\u7a76\u4f2ahermitian\u91cf\u5b50\u529b\u5b66\uff0c\u5e76\u5c06\u5176\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\u4e0e\u6807\u51c6hermitian\u60c5\u51b5\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u5f3a\u8c03\u975ehermitian\u7cfb\u7edf\u5728\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u91cd\u8981\u6027\u3002", "method": "\u7814\u7a76\u4e86\u4f2ahermitian\u91cf\u5b50\u529b\u5b66\uff0c\u5176\u4e2d\u54c8\u5bc6\u987f\u91cf\u6ee1\u8db3\u4fee\u6b63\u7684hermiticity\u6761\u4ef6\uff0c\u5e76\u63a8\u5bfc\u4e86\u542b\u7ebf\u6027\u52bf\u7684\u65f6\u95f4\u76f8\u5173\u859b\u5b9a\u8c14\u65b9\u7a0b\u7684\u89e3\u6790\u89e3\u3002", "result": "\u63a8\u5bfc\u4e86\u4f2ahermitian\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4e0e\u6807\u51c6hermitian\u60c5\u51b5\u7684\u7b49\u4ef7\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5f97\u5230\u4e86\u542b\u7ebf\u6027\u52bf\u7684\u65f6\u95f4\u76f8\u5173\u859b\u5b9a\u8c14\u65b9\u7a0b\u7684\u89e3\u6790\u89e3\uff0c\u5e76\u5c55\u793a\u4e86\u4f4d\u7f6e\u548c\u52a8\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\u4fdd\u6301\u4e3a\u5b9e\u6570\u4e14\u5927\u4e8e1/2\u3002", "conclusion": "\u4f2ahermitian\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\u4e0e\u6807\u51c6hermitian\u60c5\u51b5\u7b49\u4ef7\uff0c\u5e76\u4e14\u4f4d\u7f6e\u548c\u52a8\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\u4fdd\u6301\u4e3a\u5b9e\u6570\u4e14\u5927\u4e8e1/2\uff0c\u8fd9\u8868\u660e\u4e86\u975ehermitian\u7cfb\u7edf\u5728\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00374", "abs": "https://arxiv.org/abs/2508.00374", "authors": ["Yuji Sato", "Yasunori Ishii", "Takayoshi Yamashita"], "title": "Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models", "comment": "Accepted to MVA2025 (Best Poster Award)", "summary": "Video-based long-term action anticipation is crucial for early risk detection\nin areas such as automated driving and robotics. Conventional approaches\nextract features from past actions using encoders and predict future events\nwith decoders, which limits performance due to their unidirectional nature.\nThese methods struggle to capture semantically distinct sub-actions within a\nscene. The proposed method, BiAnt, addresses this limitation by combining\nforward prediction with backward prediction using a large language model.\nExperimental results on Ego4D demonstrate that BiAnt improves performance in\nterms of edit distance compared to baseline methods.", "AI": {"tldr": "BiAnt\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u957f\u65f6\u52a8\u4f5c\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u524d\u5411\u548c\u540e\u5411\u9884\u6d4b\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u89c6\u9891\u957f\u65f6\u52a8\u4f5c\u9884\u6d4b\u65b9\u6cd5\u7531\u4e8e\u5176\u5355\u5411\u6027\uff0c\u96be\u4ee5\u6355\u6349\u573a\u666f\u4e2d\u8bed\u4e49\u4e0a\u4e0d\u540c\u7684\u5b50\u52a8\u4f5c\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBiAnt\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u524d\u5411\u9884\u6d4b\u548c\u540e\u5411\u9884\u6d4b\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u5355\u5411\u6027\u7684\u5c40\u9650\u6027\u3002", "result": "\u5728Ego4D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBiAnt\u5728\u7f16\u8f91\u8ddd\u79bb\u65b9\u9762\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u6240\u6539\u8fdb\u3002", "conclusion": "BiAnt\u901a\u8fc7\u7ed3\u5408\u524d\u5411\u9884\u6d4b\u548c\u540e\u5411\u9884\u6d4b\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5728Ego4D\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u65f6\u52a8\u4f5c\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u5728\u7f16\u8f91\u8ddd\u79bb\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.00415", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00415", "abs": "https://arxiv.org/abs/2508.00415", "authors": ["Yue Yang", "Yuxiang Lin", "Ying Zhang", "Zihan Su", "Chang Chuan Goh", "Tangtangfang Fang", "Anthony Graham Bellotti", "Boon Giin Lee"], "title": "Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection", "comment": null, "summary": "Prediction of post-loan default is an important task in credit risk\nmanagement, and can be addressed by detection of financial anomalies using\nmachine learning. This study introduces a ResE-BiLSTM model, using a sliding\nwindow technique, and is evaluated on 44 independent cohorts from the extensive\nFreddie Mac US mortgage dataset, to improve prediction performance. The\nResE-BiLSTM is compared with five baseline models: Long Short-Term Memory\n(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks\n(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including\nAccuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to\nevaluate the contribution of individual components in the ResE-BiLSTM\narchitecture. Additionally, SHAP analysis was employed to interpret the\nunderlying features the model relied upon for its predictions. Experimental\nresults demonstrate that ResE-BiLSTM achieves superior predictive performance\ncompared to baseline models, underscoring its practical value and applicability\nin real-world scenarios.", "AI": {"tldr": "ResE-BiLSTM\u6a21\u578b\u5728\u9884\u6d4b\u8d37\u540e\u8fdd\u7ea6\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u8d37\u540e\u8fdd\u7ea6\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u8fd9\u5bf9\u4e8e\u4fe1\u7528\u98ce\u9669\u7ba1\u7406\u5f88\u91cd\u8981\u3002", "method": "\u4f7f\u7528ResE-BiLSTM\u6a21\u578b\uff0c\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6280\u672f\uff0c\u5e76\u5728Freddie Mac\u7f8e\u56fd\u62b5\u62bc\u8d37\u6b3e\u6570\u636e\u96c6\u768444\u4e2a\u72ec\u7acb\u961f\u5217\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "ResE-BiLSTM\u6a21\u578b\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cAUC\u7b49\u591a\u4e2a\u6307\u6807\u4e0a\u5747\u4f18\u4e8eLSTM\u3001BiLSTM\u3001GRU\u3001CNN\u548cRNN\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "ResE-BiLSTM\u6a21\u578b\u5728\u9884\u6d4b\u8d37\u540e\u8fdd\u7ea6\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.00675", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00675", "abs": "https://arxiv.org/abs/2508.00675", "authors": ["Gleb Schmidt", "Johannes R\u00f6misch", "Mariia Halchynska", "Svetlana Gorovaia", "Ivan P. Yamshchikov"], "title": "Team \"better_call_claude\": Style Change Detection using a Sequential Sentence Pair Classifier", "comment": null, "summary": "Style change detection - identifying the points in a document where writing\nstyle shifts - remains one of the most important and challenging problems in\ncomputational authorship analysis. At PAN 2025, the shared task challenges\nparticipants to detect style switches at the most fine-grained level:\nindividual sentences. The task spans three datasets, each designed with\ncontrolled and increasing thematic variety within documents. We propose to\naddress this problem by modeling the content of each problem instance - that\nis, a series of sentences - as a whole, using a Sequential Sentence Pair\nClassifier (SSPC). The architecture leverages a pre-trained language model\n(PLM) to obtain representations of individual sentences, which are then fed\ninto a bidirectional LSTM (BiLSTM) to contextualize them within the document.\nThe BiLSTM-produced vectors of adjacent sentences are concatenated and passed\nto a multi-layer perceptron for prediction per adjacency. Building on the work\nof previous PAN participants classical text segmentation, the approach is\nrelatively conservative and lightweight. Nevertheless, it proves effective in\nleveraging contextual information and addressing what is arguably the most\nchallenging aspect of this year's shared task: the notorious problem of\n\"stylistically shallow\", short sentences that are prevalent in the proposed\nbenchmark data. Evaluated on the official PAN-2025 test datasets, the model\nachieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,\nand HARD data, respectively, outperforming not only the official random\nbaselines but also a much more challenging one: claude-3.7-sonnet's zero-shot\nperformance.", "AI": {"tldr": "\u901a\u8fc7\u5e8f\u5217\u53e5\u5bf9\u5206\u7c7b\u5668\uff08SSPC\uff09\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u548c\u53cc\u5411LSTM\uff08BiLSTM\uff09\uff0c\u5728PAN 2025\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u53e5\u7ea7\u98ce\u683c\u8f6c\u53d8\u68c0\u6d4b\uff0c\u5c24\u5176\u64c5\u957f\u5904\u7406\u77ed\u53e5\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u4f5c\u8005\u8eab\u4efd\u5206\u6790\u4e2d\u7684\u98ce\u683c\u8f6c\u53d8\u68c0\u6d4b\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728PAN 2025\u5171\u4eab\u4efb\u52a1\u4e2d\u4ee5\u5355\u4e2a\u53e5\u5b50\u4e3a\u5355\u4f4d\u8fdb\u884c\u6700\u7ec6\u7c92\u5ea6\u98ce\u683c\u5207\u6362\u68c0\u6d4b\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5e8f\u5217\u53e5\u5bf9\u5206\u7c7b\u5668\uff08SSPC\uff09\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u83b7\u53d6\u53e5\u5411\u91cf\uff0c\u5e76\u901a\u8fc7\u53cc\u5411LSTM\uff08BiLSTM\uff09\u8fdb\u884c\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u6700\u540e\u5c06\u76f8\u90bb\u53e5\u5b50\u7684BiLSTM\u5411\u91cf\u62fc\u63a5\u540e\u8f93\u5165\u591a\u5c42\u611f\u77e5\u673a\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728EASY\u3001MEDIUM\u548cHARD\u6570\u636e\u96c6\u4e0a\u5206\u522b\u53d6\u5f97\u4e860.923\u30010.828\u548c0.724\u7684\u5b8fF1\u5206\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u201c\u98ce\u683c\u6d45\u5c42\u5316\u201d\u77ed\u53e5\u7684\u95ee\u9898\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728PAN-2025\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u679c\uff0c\u5728EASY\u3001MEDIUM\u548cHARD\u6570\u636e\u96c6\u4e0a\u7684\u5b8fF1\u5206\u6570\u5206\u522b\u4e3a0.923\u30010.828\u548c0.724\uff0c\u4f18\u4e8e\u968f\u673a\u57fa\u7ebf\u548cclaude-3.7-sonnet\u7684\u96f6\u6837\u672c\u8868\u73b0\u3002"}}
{"id": "2508.00689", "categories": ["quant-ph", "cond-mat.quant-gas", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2508.00689", "abs": "https://arxiv.org/abs/2508.00689", "authors": ["C. J. Bolech", "T. Giamarchi"], "title": "Spontaneous emission as a bridge from Lindbladian to nonreciprocal reservoirs", "comment": "7 pages, 3 figures", "summary": "We study an out-of-equilibrium quantum system in which a state connecting two\nreservoirs is also coupled by stimulated and spontaneous emission of photons to\nan antitrapped state, thus implementing particle loss. After revisiting the\nspontaneous emission process, we show that the proper effective description of\nsuch a system requires one to go beyond the usual Lindbladian formalism and\nincludes a nonreciprocal (``non-Hermitian'') coupling to the reservoir modeling\nthe untrapped state. The presence of both, the reservoirs and the nonreciprocal\ncoupling, have observable consequences that we compute, for example, by looking\nat the quantum Zeno effect in the loss current. We discuss the connection of\nour findings to possible experiments in cold atomic gases.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8d85\u8d8a Lindbladian \u5f62\u5f0f\u7684\u6709\u6548\u63cf\u8ff0\uff0c\u4ee5\u7814\u7a76\u4e0e\u5149\u5b50\u635f\u8017\u76f8\u5173\u7684\u975e\u5e73\u8861\u91cf\u5b50\u7cfb\u7edf\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u51b7\u539f\u5b50\u6c14\u4f53\u5b9e\u9a8c\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u7814\u7a76\u4e00\u4e2a\u975e\u5e73\u8861\u91cf\u5b50\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u53d7\u6fc0\u548c\u81ea\u53d1\u8f90\u5c04\u5149\u5b50\u8026\u5408\u5230\u4e00\u4e2a\u53cd\u6355\u83b7\u6001\uff0c\u4ece\u800c\u5b9e\u73b0\u7c92\u5b50\u635f\u5931\uff0c\u5e76\u63a2\u8ba8\u5176\u53ef\u89c2\u6d4b\u7684\u540e\u679c\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u8003\u8651\u4e00\u4e2a\u4e0e\u4e24\u4e2a\u50a8\u5c42\u76f8\u8fde\u63a5\u7684\u72b6\u6001\uff0c\u8be5\u72b6\u6001\u8fd8\u901a\u8fc7\u53d7\u6fc0\u548c\u81ea\u53d1\u8f90\u5c04\u5149\u5b50\u8026\u5408\u5230\u4e00\u4e2a\u53cd\u6355\u83b7\u6001\uff0c\u4ece\u800c\u5b9e\u73b0\u7c92\u5b50\u635f\u5931\u3002\u5728\u91cd\u65b0\u5ba1\u89c6\u81ea\u53d1\u8f90\u5c04\u8fc7\u7a0b\u540e\uff0c\u6211\u4eec\u8868\u660e\uff0c\u5bf9\u8be5\u7cfb\u7edf\u7684\u9002\u5f53\u6709\u6548\u63cf\u8ff0\u9700\u8981\u8d85\u8d8a\u901a\u5e38\u7684 Lindbladian \u5f62\u5f0f\uff0c\u5e76\u5305\u62ec\u4e00\u4e2a\u975e\u4e92\u6613\uff08\u201c\u975e\u5384\u7c73\u201d\uff09\u8026\u5408\u5230\u6a21\u62df\u672a\u6355\u83b7\u6001\u7684\u50a8\u5c42\u3002", "result": "\u672c\u7814\u7a76\u8ba1\u7b97\u4e86\u50a8\u5c42\u548c\u975e\u4e92\u6613\u8026\u5408\u7684\u5b58\u5728\u6240\u4ea7\u751f\u7684\u53ef\u89c2\u6d4b\u540e\u679c\uff0c\u4f8b\u5982\u901a\u8fc7\u89c2\u5bdf\u635f\u8017\u6d41\u4e2d\u7684\u91cf\u5b50\u829d\u8bfa\u6548\u5e94\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u9700\u8981\u8d85\u8d8a\u901a\u5e38\u7684 Lindbladian \u5f62\u5f0f\uff0c\u5e76\u5305\u62ec\u4e00\u4e2a\u975e\u4e92\u6613\uff08\u201c\u975e\u5384\u7c73\u201d\uff09\u8026\u5408\u5230\u6a21\u62df\u672a\u6355\u83b7\u6001\u7684\u50a8\u5c42\uff0c\u624d\u80fd\u5bf9\u6240\u7814\u7a76\u7684\u7cfb\u7edf\u8fdb\u884c\u6709\u6548\u7684\u63cf\u8ff0\u3002\u672c\u7814\u7a76\u8fd8\u8ba8\u8bba\u4e86\u7814\u7a76\u7ed3\u679c\u4e0e\u51b7\u539f\u5b50\u6c14\u4f53\u4e2d\u53ef\u80fd\u8fdb\u884c\u7684\u5b9e\u9a8c\u4e4b\u95f4\u7684\u8054\u7cfb\u3002"}}
{"id": "2508.00381", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00381", "abs": "https://arxiv.org/abs/2508.00381", "authors": ["Kamal Basha S", "Athira Nambiar"], "title": "Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis", "comment": null, "summary": "Weld defect detection is crucial for ensuring the safety and reliability of\npiping systems in the oil and gas industry, especially in challenging marine\nand offshore environments. Traditional non-destructive testing (NDT) methods\noften fail to detect subtle or internal defects, leading to potential failures\nand costly downtime. Furthermore, existing neural network-based approaches for\ndefect classification frequently rely on arbitrarily selected pretrained\narchitectures and lack interpretability, raising safety concerns for\ndeployment. To address these challenges, this paper introduces\n``Adapt-WeldNet\", an adaptive framework for welding defect detection that\nsystematically evaluates various pre-trained architectures, transfer learning\nstrategies, and adaptive optimizers to identify the best-performing model and\nhyperparameters, optimizing defect detection and providing actionable insights.\nAdditionally, a novel Defect Detection Interpretability Analysis (DDIA)\nframework is proposed to enhance system transparency. DDIA employs Explainable\nAI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific\nevaluations validated by certified ASNT NDE Level II professionals.\nIncorporating a Human-in-the-Loop (HITL) approach and aligning with the\nprinciples of Trustworthy AI, DDIA ensures the reliability, fairness, and\naccountability of the defect detection system, fostering confidence in\nautomated decisions through expert validation. By improving both performance\nand interpretability, this work enhances trust, safety, and reliability in\nwelding defect detection systems, supporting critical operations in offshore\nand marine environments.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u6d77\u6d0b\u6cb9\u6c14\u7ba1\u9053\u710a\u63a5\u7f3a\u9677\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86Adapt-WeldNet\uff0c\u4e00\u4e2a\u80fd\u4f18\u5316\u68c0\u6d4b\u6027\u80fd\u7684\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86DDIA\uff0c\u4e00\u4e2a\u5229\u7528XAI\u548c\u4e13\u5bb6\u9a8c\u8bc1\u6765\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u8d56\u6027\u7684\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u7684\u65e0\u635f\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u6d77\u6d0b\u548c\u9646\u5730\u73af\u5883\u4e2d\u7ba1\u9053\u7cfb\u7edf\u7684\u7ec6\u5fae\u6216\u5185\u90e8\u710a\u63a5\u7f3a\u9677\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u5f15\u53d1\u4e86\u90e8\u7f72\u5b89\u5168\u62c5\u5fe7\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u7684\u81ea\u9002\u5e94\u710a\u63a5\u7f3a\u9677\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51faAdapt-WeldNet\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u548c\u81ea\u9002\u5e94\u4f18\u5316\u5668\u6765\u5bfb\u627e\u6700\u4f18\u6a21\u578b\u548c\u8d85\u53c2\u6570\u3002\u540c\u65f6\uff0c\u63d0\u51faDefect Detection Interpretability Analysis (DDIA)\u6846\u67b6\uff0c\u5229\u7528Grad-CAM\u548cLIME\u7b49XAI\u6280\u672f\uff0c\u5e76\u7ed3\u5408ASNT NDE Level II\u4e13\u4e1a\u4eba\u58eb\u7684\u9886\u57df\u7279\u5b9a\u8bc4\u4f30\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5f15\u5165Human-in-the-Loop (HITL)\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u8d56AI\u539f\u5219\u3002", "result": "Adapt-WeldNet\u80fd\u591f\u8bc6\u522b\u51fa\u6027\u80fd\u6700\u4f18\u7684\u6a21\u578b\u548c\u8d85\u53c2\u6570\uff0c\u4ece\u800c\u4f18\u5316\u7f3a\u9677\u68c0\u6d4b\u80fd\u529b\u3002DDIA\u6846\u67b6\u5229\u7528XAI\u6280\u672f\u548c\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u589e\u5f3a\u4e86\u68c0\u6d4b\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\uff0c\u786e\u4fdd\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3001\u516c\u5e73\u6027\u548c\u53ef\u95ee\u8d23\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7Adapt-WeldNet\u548cDDIA\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u710a\u63a5\u7f3a\u9677\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u589e\u5f3a\u4e86\u81ea\u52a8\u5316\u51b3\u7b56\u7684\u53ef\u9760\u6027\u3001\u516c\u5e73\u6027\u548c\u53ef\u95ee\u8d23\u6027\uff0c\u6700\u7ec8\u63d0\u5347\u4e86\u6d77\u6d0b\u548c\u9646\u5730\u73af\u5883\u6cb9\u6c14\u7ba1\u9053\u710a\u63a5\u68c0\u6d4b\u7cfb\u7edf\u7684\u4fe1\u4efb\u5ea6\u3001\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.00472", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00472", "abs": "https://arxiv.org/abs/2508.00472", "authors": ["Leonidas Akritidis", "Panayiotis Bozanis"], "title": "A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces", "comment": null, "summary": "The tabular form constitutes the standard way of representing data in\nrelational database systems and spreadsheets. But, similarly to other forms,\ntabular data suffers from class imbalance, a problem that causes serious\nperformance degradation in a wide variety of machine learning tasks. One of the\nmost effective solutions dictates the usage of Generative Adversarial Networks\n(GANs) in order to synthesize artificial data instances for the\nunder-represented classes. Despite their good performance, none of the proposed\nGAN models takes into account the vector subspaces of the input samples in the\nreal data space, leading to data generation in arbitrary locations. Moreover,\nthe class labels are treated in the same manner as the other categorical\nvariables during training, so conditional sampling by class is rendered less\neffective. To overcome these problems, this study presents ctdGAN, a\nconditional GAN for alleviating class imbalance in tabular datasets. Initially,\nctdGAN executes a space partitioning step to assign cluster labels to the input\nsamples. Subsequently, it utilizes these labels to synthesize samples via a\nnovel probabilistic sampling strategy and a new loss function that penalizes\nboth cluster and class mis-predictions. In this way, ctdGAN is trained to\ngenerate samples in subspaces that resemble those of the original data\ndistribution. We also introduce several other improvements, including a simple,\nyet effective cluster-wise scaling technique that captures multiple feature\nmodes without affecting data dimensionality. The exhaustive evaluation of\nctdGAN with 14 imbalanced datasets demonstrated its superiority in generating\nhigh fidelity samples and improving classification accuracy.", "AI": {"tldr": "ctdGAN\u662f\u4e00\u79cd\u7528\u4e8e\u89e3\u51b3\u8868\u683c\u6570\u636e\u96c6\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u7684\u6761\u4ef6GAN\u3002\u5b83\u901a\u8fc7\u7a7a\u95f4\u5212\u5206\u3001\u6982\u7387\u91c7\u6837\u548c\u6539\u8fdb\u7684\u635f\u5931\u51fd\u6570\uff0c\u5728\u4fdd\u7559\u539f\u59cb\u6570\u636e\u5206\u5e03\u7684\u5b50\u7a7a\u95f4\u5185\u751f\u6210\u6837\u672c\uff0c\u5e76\u6709\u6548\u63d0\u9ad8\u4e86\u751f\u6210\u6837\u672c\u7684\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GAN\u6a21\u578b\u5728\u5904\u7406\u8868\u683c\u6570\u636e\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u65f6\uff0c\u672a\u80fd\u8003\u8651\u8f93\u5165\u6837\u672c\u7684\u5411\u91cf\u5b50\u7a7a\u95f4\uff0c\u5bfc\u81f4\u751f\u6210\u6570\u636e\u7684\u4f4d\u7f6e\u4efb\u610f\uff0c\u5e76\u4e14\u7c7b\u522b\u6807\u7b7e\u7684\u5904\u7406\u65b9\u5f0f\u4e0e\u5176\u4ed6\u5206\u7c7b\u53d8\u91cf\u76f8\u540c\uff0c\u964d\u4f4e\u4e86\u6761\u4ef6\u91c7\u6837\u7684\u6709\u6548\u6027\u3002", "method": "ctdGAN\u9996\u5148\u6267\u884c\u7a7a\u95f4\u5212\u5206\u6b65\u9aa4\u4e3a\u8f93\u5165\u6837\u672c\u5206\u914d\u805a\u7c7b\u6807\u7b7e\u3002\u968f\u540e\uff0c\u5b83\u5229\u7528\u8fd9\u4e9b\u6807\u7b7e\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u6982\u7387\u91c7\u6837\u7b56\u7565\u548c\u60e9\u7f5a\u805a\u7c7b\u53ca\u7c7b\u522b\u9884\u6d4b\u9519\u8bef\u7684\u65b0\u635f\u5931\u51fd\u6570\u6765\u5408\u6210\u6837\u672c\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u805a\u7c7b\u5185\u7f29\u653e\u6280\u672f\uff0c\u53ef\u4ee5\u5728\u4e0d\u5f71\u54cd\u6570\u636e\u7ef4\u5ea6\u7684\u60c5\u51b5\u4e0b\u6355\u6349\u591a\u4e2a\u7279\u5f81\u6a21\u5f0f\u3002", "result": "ctdGAN\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u6837\u672c\u5e76\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "ctdGAN\u572814\u4e2a\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8bc1\u660e\u4e86\u5176\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u6837\u672c\u548c\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.00679", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00679", "abs": "https://arxiv.org/abs/2508.00679", "authors": ["Shubham Kumar Nigam", "Tanmay Dubey", "Noel Shallum", "Arnab Bhattacharya"], "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries", "comment": null, "summary": "Legal precedent retrieval is a cornerstone of the common law system, governed\nby the principle of stare decisis, which demands consistency in judicial\ndecisions. However, the growing complexity and volume of legal documents\nchallenge traditional retrieval methods. TraceRetriever mirrors real-world\nlegal search by operating with limited case information, extracting only\nrhetorically significant segments instead of requiring complete documents. Our\npipeline integrates BM25, Vector Database, and Cross-Encoder models, combining\ninitial results through Reciprocal Rank Fusion before final re-ranking.\nRhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier\ntrained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,\nTraceRetriever addresses growing document volume challenges while aligning with\npractical search constraints, reliable and scalable foundation for precedent\nretrieval enhancing legal research when only partial case knowledge is\navailable.", "AI": {"tldr": "TraceRetriever improves legal precedent retrieval by using limited case information and a pipeline combining BM25, Vector DB, and Cross-Encoder models, outperforming traditional methods with partial case data.", "motivation": "The growing complexity and volume of legal documents challenge traditional retrieval methods. TraceRetriever aims to mirror real-world legal search by operating with limited case information, extracting only rhetorically significant segments instead of requiring complete documents.", "method": "The pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining initial results through Reciprocal Rank Fusion before final re-ranking. Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian judgments.", "result": "Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever demonstrates its effectiveness in handling challenges posed by document volume and partial case knowledge.", "conclusion": "TraceRetriever addresses the challenge of growing document volume in precedent retrieval, aligning with practical search constraints and providing a reliable and scalable foundation for legal research when only partial case knowledge is available."}}
{"id": "2508.00702", "categories": ["quant-ph", "cond-mat.other", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.00702", "abs": "https://arxiv.org/abs/2508.00702", "authors": ["Diego Fern\u00e1ndez de la Pradilla", "Esteban Moreno", "Johannes Feist"], "title": "There is no ultrastrong coupling with photons", "comment": "24 pages, 4 figures", "summary": "Theoretical accounts of ultrastrongly coupled light-matter systems commonly\nassume that it arises from the interaction of an emitter with propagating\nphoton modes supported by a structure, understanding photons as the excitations\nof the transverse electromagnetic field. This description discards the Coulomb\ninteraction between the emitter and structure charges. Here, we show with a\ngeneral argument based on electromagnetic constraints that the emitter-photon\ncoupling strength is fundamentally limited. Accordingly, we conclude that the\nultrastrong coupling regime cannot be reached with photons. Instead, it must\noriginate from the Coulomb interactions between charges. A further corollary is\nthat the so-called polarization self-energy term does not need to be included.\nWe illustrate our claims by solving an analytical model of the paradigmatic\ncase of an emitter next to a metallic nanosphere. These findings shed light on\nthe fundamental processes underlying ultrastrong coupling, clarify the role of\nthe polarization self-energy term and compel a reevaluation of previous\nliterature.", "AI": {"tldr": "\u8d85\u5f3a\u8026\u5408\u4e0d\u80fd\u901a\u8fc7\u5149\u5b50\u5b9e\u73b0\uff0c\u6e90\u4e8e\u5e93\u4ed1\u76f8\u4e92\u4f5c\u7528\uff0c\u6781\u5316\u81ea\u80fd\u9879\u65e0\u9700\u8003\u8651\u3002", "motivation": "\u7406\u8bba\u4e0a\uff0c\u8d85\u5f3a\u8026\u5408\u7684\u4ea7\u751f\u901a\u5e38\u8ba4\u4e3a\u662f\u53d1\u5c04\u5668\u4e0e\u7ed3\u6784\u6240\u652f\u6301\u7684\u4f20\u64ad\u5149\u5b50\u6a21\u5f0f\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4f46\u8fd9\u79cd\u63cf\u8ff0\u5ffd\u7565\u4e86\u53d1\u5c04\u5668\u548c\u7ed3\u6784\u7535\u8377\u95f4\u7684\u5e93\u4ed1\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u6587\u7ae0\u901a\u8fc7\u57fa\u4e8e\u7535\u78c1\u7ea6\u675f\u7684\u901a\u7528\u8bba\u8bc1\uff0c\u5e76\u7528\u4e00\u4e2a\u5206\u6790\u6a21\u578b\u9610\u91ca\u4e86\u8be5\u89c2\u70b9\u3002", "result": "\u6587\u7ae0\u8bc1\u660e\u4e86\u53d1\u5c04\u5668-\u5149\u5b50\u8026\u5408\u5f3a\u5ea6\u5b58\u5728\u57fa\u672c\u9650\u5236\uff0c\u56e0\u6b64\u8d85\u5f3a\u8026\u5408\u4e0d\u80fd\u901a\u8fc7\u5149\u5b50\u5b9e\u73b0\uff0c\u800c\u5fc5\u987b\u6e90\u4e8e\u5e93\u4ed1\u76f8\u4e92\u4f5c\u7528\u3002\u6b64\u5916\uff0c\u6587\u7ae0\u8fd8\u8bf4\u660e\u4e86\u6781\u5316\u81ea\u80fd\u9879\u4e0d\u5305\u542b\u5728\u5185\u3002", "conclusion": "\u8d85\u5f3a\u8026\u5408\u4e0d\u80fd\u901a\u8fc7\u5149\u5b50\u5b9e\u73b0\uff0c\u800c\u5fc5\u987b\u6e90\u4e8e\u7535\u8377\u95f4\u7684\u5e93\u4ed1\u76f8\u4e92\u4f5c\u7528\u3002\u6240\u8c13\u6781\u5316\u81ea\u80fd\u9879\u4e0d\u9700\u8981\u88ab\u5305\u542b\u3002"}}
{"id": "2508.00383", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00383", "abs": "https://arxiv.org/abs/2508.00383", "authors": ["Won June Cho", "Hongjun Yoon", "Daeky Jeong", "Hyeongyeol Lim", "Yosep Chong"], "title": "$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models", "comment": "Accepted (Oral) in MICCAI 2025 COMPAYL Workshop", "summary": "Spatial transcriptomics reveals gene expression patterns within tissue\ncontext, enabling precision oncology applications such as treatment response\nprediction, but its high cost and technical complexity limit clinical adoption.\nPredicting spatial gene expression (biomarkers) from routine histopathology\nimages offers a practical alternative, yet current vision foundation models\n(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below\nclinical standards. Given that VFMs are already trained on millions of diverse\nwhole slide images, we hypothesize that architectural innovations beyond ViTs\nmay better capture the low-frequency, subtle morphological patterns correlating\nwith molecular phenotypes. By demonstrating that state space models initialized\nwith negative real eigenvalues exhibit strong low-frequency bias, we introduce\n$MV_{Hybrid}$, a hybrid backbone architecture combining state space models\n(SSMs) with ViT. We compare five other different backbone architectures for\npathology VFMs, all pretrained on identical colorectal cancer datasets using\nthe DINOv2 self-supervised learning method. We evaluate all pretrained models\nusing both random split and leave-one-study-out (LOSO) settings of the same\nbiomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher\ncorrelation than the best-performing ViT and shows 43% smaller performance\ndegradation compared to random split in gene expression prediction,\ndemonstrating superior performance and robustness, respectively. Furthermore,\n$MV_{Hybrid}$ shows equal or better downstream performance in classification,\npatch retrieval, and survival prediction tasks compared to that of ViT, showing\nits promise as a next-generation pathology VFM backbone. Our code is publicly\navailable at: https://github.com/deepnoid-ai/MVHybrid.", "AI": {"tldr": "MVHybrid \u662f\u4e00\u79cd\u7ed3\u5408\u4e86 SSM \u548c ViT \u7684\u6df7\u5408\u9aa8\u5e72\u67b6\u6784\uff0c\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u5f62\u6001\u6a21\u5f0f\u6355\u6349\u80fd\u529b\uff0c\u63d0\u5347\u75c5\u7406\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u65b9\u9762\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6210\u672c\u9ad8\u548c\u6280\u672f\u590d\u6742\u6027\u7684\u9650\u5236\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e ViT \u7684\u75c5\u7406\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u5728\u6355\u6349\u4e0e\u5206\u5b50\u8868\u578b\u76f8\u5173\u7684\u4f4e\u9891\u3001\u7ec6\u5fae\u5f62\u6001\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u548c Vision Transformer\uff08ViT\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MVHybrid \u7684\u6df7\u5408\u9aa8\u5e72\u67b6\u6784\u3002\u8be5\u6a21\u578b\u5728\u76f8\u540c\u7684\u7ed3\u76f4\u80a0\u764c\u6570\u636e\u96c6\u4e0a\u4f7f\u7528 DINOv2 \u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u968f\u673a\u5206\u5272\u548c\u7559\u4e00\u7814\u7a76\uff08LOSO\uff09\u8bc4\u4f30\u4e2d\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u5728 LOSO \u8bc4\u4f30\u4e2d\uff0cMVHybrid \u7684\u76f8\u5173\u6027\u6bd4\u8868\u73b0\u6700\u4f73\u7684 ViT \u9ad8\u51fa 57%\uff0c\u4e0e\u968f\u673a\u5206\u5272\u76f8\u6bd4\uff0c\u6027\u80fd\u4e0b\u964d\u51cf\u5c11\u4e86 43%\uff0c\u5728\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "MVHybrid \u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u75c5\u7406\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u9aa8\u5e72\uff0c\u5728\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u3001\u5206\u7c7b\u3001\u56fe\u5757\u68c0\u7d22\u548c\u751f\u5b58\u9884\u6d4b\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4e0e ViT \u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u5e76\u5177\u6709\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.00507", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00507", "abs": "https://arxiv.org/abs/2508.00507", "authors": ["Yiming Xu", "Jiarun Chen", "Zhen Peng", "Zihan Chen", "Qika Lin", "Lan Ma", "Bin Shi", "Bo Dong"], "title": "Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection", "comment": "Accepted by ACM Multimedia 2025 (MM '25)", "summary": "The natural combination of intricate topological structures and rich textual\ninformation in text-attributed graphs (TAGs) opens up a novel perspective for\ngraph anomaly detection (GAD). However, existing GAD methods primarily focus on\ndesigning complex optimization objectives within the graph domain, overlooking\nthe complementary value of the textual modality, whose features are often\nencoded by shallow embedding techniques, such as bag-of-words or skip-gram, so\nthat semantic context related to anomalies may be missed. To unleash the\nenormous potential of textual modality, large language models (LLMs) have\nemerged as promising alternatives due to their strong semantic understanding\nand reasoning capabilities. Nevertheless, their application to TAG anomaly\ndetection remains nascent, and they struggle to encode high-order structural\ninformation inherent in graphs due to input length constraints. For\nhigh-quality anomaly detection in TAGs, we propose CoLL, a novel framework that\ncombines LLMs and graph neural networks (GNNs) to leverage their complementary\nstrengths. CoLL employs multi-LLM collaboration for evidence-augmented\ngeneration to capture anomaly-relevant contexts while delivering human-readable\nrationales for detected anomalies. Moreover, CoLL integrates a GNN equipped\nwith a gating mechanism to adaptively fuse textual features with evidence while\npreserving high-order topological information. Extensive experiments\ndemonstrate the superiority of CoLL, achieving an average improvement of 13.37%\nin AP. This study opens a new avenue for incorporating LLMs in advancing GAD.", "AI": {"tldr": "CoLL\u6846\u67b6\u901a\u8fc7\u591aLLM\u534f\u4f5c\u548cGNN\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5c5e\u6027\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5ffd\u89c6\u6587\u672c\u4fe1\u606f\u548c\u9ad8\u9636\u7ed3\u6784\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u89c6\u4e86\u6587\u672c\u6a21\u6001\u7684\u4ef7\u503c\uff0c\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6d45\u5c42\u5d4c\u5165\u6280\u672f\uff0c\u53ef\u80fd\u4e22\u5931\u4e0e\u5f02\u5e38\u76f8\u5173\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002LLM\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u56fe\u5f02\u5e38\u68c0\u6d4b\u65f6\u5b58\u5728\u8f93\u5165\u957f\u5ea6\u9650\u5236\uff0c\u96be\u4ee5\u7f16\u7801\u9ad8\u9636\u7ed3\u6784\u4fe1\u606f\u3002", "method": "CoLL\u6846\u67b6\u7ed3\u5408\u4e86\u591aLLM\u534f\u4f5c\u8fdb\u884c\u8bc1\u636e\u589e\u5f3a\u751f\u6210\uff0c\u5e76\u5229\u7528\u96c6\u6210GNN\u548c\u95e8\u63a7\u673a\u5236\u6765\u878d\u5408\u6587\u672c\u7279\u5f81\u548c\u8bc1\u636e\uff0c\u540c\u65f6\u4fdd\u7559\u9ad8\u9636\u62d3\u6251\u4fe1\u606f\u3002", "result": "CoLL\u6846\u67b6\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u5747\u6539\u8fdb\u4e8613.37%\u7684AP\uff08\u51c6\u786e\u7387\uff09\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6587\u672c\u5c5e\u6027\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "CoLL\u6846\u67b6\u7684\u5f15\u5165\u4e3a\u56fe\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u548cGNN\u7684\u4f18\u52bf\uff0c\u5728\u5904\u7406\u6587\u672c\u5c5e\u6027\u56fe\u65f6\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.00680", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00680", "abs": "https://arxiv.org/abs/2508.00680", "authors": ["Johannes R\u00f6misch", "Svetlana Gorovaia", "Mariia Halchynska", "Gleb Schmidt", "Ivan P. Yamshchikov"], "title": "Better Call Claude: Can LLMs Detect Changes of Writing Style?", "comment": null, "summary": "This article explores the zero-shot performance of state-of-the-art large\nlanguage models (LLMs) on one of the most challenging tasks in authorship\nanalysis: sentence-level style change detection. Benchmarking four LLMs on the\nofficial PAN~2024 and 2025 \"Multi-Author Writing Style Analysis\" datasets, we\npresent several observations. First, state-of-the-art generative models are\nsensitive to variations in writing style - even at the granular level of\nindividual sentences. Second, their accuracy establishes a challenging baseline\nfor the task, outperforming suggested baselines of the PAN competition.\nFinally, we explore the influence of semantics on model predictions and present\nevidence suggesting that the latest generation of LLMs may be more sensitive to\ncontent-independent and purely stylistic signals than previously reported.", "AI": {"tldr": "LLMs\u5728\u53e5\u5b50\u7ea7\u522b\u7684\u98ce\u683c\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u654f\u611f\u6027\uff0c\u4e3a\u8be5\u9886\u57df\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u5e76\u53ef\u80fd\u6bd4\u9884\u671f\u66f4\u80fd\u6355\u6349\u7eaf\u7cb9\u7684\u98ce\u683c\u4fe1\u53f7\u3002", "motivation": "\u63a2\u7d22\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728 authorship analysis \u9886\u57df\u4e2d\u6700\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e4b\u4e00\uff1a\u53e5\u5b50\u7ea7\u522b\u7684\u98ce\u683c\u53d8\u5316\u68c0\u6d4b\u4e0a\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "method": "\u5bf9\u56db\u79cd\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728PAN 2024\u548c2025\u201c\u591a\u4f5c\u8005\u5199\u4f5c\u98ce\u683c\u5206\u6790\u201d\u6570\u636e\u96c6\u4e0a\u7684\u96f6\u6837\u672c\u6027\u80fd\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u5bf9\u5199\u4f5c\u98ce\u683c\u7684\u53d8\u5316\u5f88\u654f\u611f\uff0c\u5373\u4f7f\u662f\u5728\u5355\u4e2a\u53e5\u5b50\u7684\u7c92\u5ea6\u7ea7\u522b\u4e0a\u3002\u5b83\u4eec\u7684\u51c6\u786e\u6027\u4e3a\u8be5\u4efb\u52a1\u5efa\u7acb\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u4f18\u4e8ePAN\u7ade\u8d5b\u5efa\u8bae\u7684\u57fa\u51c6\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u6700\u65b0\u4e00\u4ee3\u7684LLMs\u53ef\u80fd\u6bd4\u4e4b\u524d\u62a5\u544a\u7684\u66f4\u80fd\u5bf9\u4e0e\u5185\u5bb9\u65e0\u5173\u7684\u7eaf\u7cb9\u98ce\u683c\u4fe1\u53f7\u505a\u51fa\u53cd\u5e94\u3002", "conclusion": "LLMs\u5bf9\u53e5\u5b50\u7ea7\u522b\u7684\u98ce\u683c\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u975e\u5e38\u654f\u611f\uff0c\u5e76\u4e14\u5176\u51c6\u786e\u6027\u4e3a\u8be5\u4efb\u52a1\u8bbe\u5b9a\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u4f18\u4e8ePAN\u7ade\u8d5b\u63d0\u51fa\u7684\u57fa\u51c6\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8868\u660eLLMs\u53ef\u80fd\u6bd4\u4e4b\u524d\u8ba4\u4e3a\u7684\u66f4\u80fd\u5bf9\u4e0e\u5185\u5bb9\u65e0\u5173\u7684\u7eaf\u7cb9\u98ce\u683c\u4fe1\u53f7\u505a\u51fa\u53cd\u5e94\u3002"}}
{"id": "2508.00765", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00765", "abs": "https://arxiv.org/abs/2508.00765", "authors": ["A. Campos-Uscanga", "E. Ben\u00edtez Rodr\u00edguez", "E. Piceno Mart\u00ednez", "M. A. Bastarrachea-Magnani"], "title": "Magic States in the Asymmetric Quantum Rabi Model", "comment": "18 pages, 7 figures", "summary": "Magic or non-stabilizerness is a resource for quantum computing that has been\nextensively studied in qudit networks. It describes the degree to which\nClifford gates cannot generate a given state, capturing the advantage of\nquantum over classical computing. However, its definition in continuous\nvariables and general composite systems remains an open issue. We study magic\nin a bipartite system, the Asymmetric Quantum Rabi model, a paradigmatic model\nfrom quantum optics. We explore the presence of magic in the qubit-reduced\nsystem throughout the Hamiltonian parameter space, the role of light-matter\ninteractions in its generation, and the manifestation of Wigner function\nnegativity in the corresponding bosonic degree of freedom. Finally, we discuss\nour results for magic state preparation in the strong and ultra-strong coupling\nregimes within the context of quantum informational systems.", "AI": {"tldr": "\u7814\u7a76\u5728\u91cf\u5b50\u5149\u5b66\u6a21\u578b\u4e2d\u63a2\u7d22\u4e86\u9b54\u529b\u7684\u5b58\u5728\u6027\u53ca\u5176\u751f\u6210\u673a\u5236\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u8d44\u6e90\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u9b54\u529b\uff08magic\uff09\u4f5c\u4e3a\u4e00\u79cd\u8d44\u6e90\uff0c\u5728qudit\u7f51\u7edc\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5176\u5728\u8fde\u7eed\u53d8\u91cf\u548c\u4e00\u822c\u590d\u5408\u7cfb\u7edf\u4e2d\u7684\u5b9a\u4e49\u4ecd\u7136\u662f\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u9b54\u529b\u5728\u91cf\u5b50\u5149\u5b66\u6a21\u578b\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7406\u8bba\u5206\u6790\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86\u975e\u5bf9\u79f0\u91cf\u5b50\u62c9\u6bd4\u6a21\u578b\u4e2d\u9b54\u529b\u7684\u6027\u8d28\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5728\u975e\u5bf9\u79f0\u91cf\u5b50\u62c9\u6bd4\u6a21\u578b\u4e2d\uff0c\u9b54\u529b\u666e\u904d\u5b58\u5728\u4e8e\u4e8c\u55ea\u7c7b\u7ea6\u5316\u7cfb\u7edf\u4e2d\uff0c\u5e76\u4e14\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\u5728\u9b54\u529b\u751f\u6210\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u89c2\u5bdf\u5230\u4e86\u76f8\u5e94\u7684\u73bb\u8272\u5b50\u81ea\u7531\u5ea6\u4e2d\u8d1fWigner\u51fd\u6570\u7684\u51fa\u73b0\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u975e\u5bf9\u79f0\u91cf\u5b50\u62c9\u6bd4\u6a21\u578b\uff08\u4e00\u79cd\u6765\u81ea\u91cf\u5b50\u5149\u5b66\u4e2d\u7684\u8303\u4f8b\u6a21\u578b\uff09\u7684\u4e8c\u5206\u7cfb\u7edf\u4e2d\u9b54\u529b\uff08magic\uff09\u7684\u5b58\u5728\u6027\u3002\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u4e86\u5728\u6574\u4e2a\u54c8\u5bc6\u987f\u91cf\u53c2\u6570\u7a7a\u95f4\u4e2d\uff0c\u4e8c\u55ea\u7c7b\u7ea6\u5316\u7cfb\u7edf\u4e2d\u9b54\u529b\u7684\u5b58\u5728\u6027\uff0c\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\u5728\u9b54\u529b\u751f\u6210\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca\u5728\u76f8\u5e94\u7684\u73bb\u8272\u5b50\u81ea\u7531\u5ea6\u4e2d\u8d1fWigner\u51fd\u6570\u7684\u8868\u73b0\u3002\u6700\u540e\uff0c\u7814\u7a76\u4eba\u5458\u8ba8\u8bba\u4e86\u5728\u5f3a\u8026\u5408\u548c\u8d85\u5f3a\u8026\u5408\u533a\u57df\u5185\uff0c\u9b54\u529b\u6001\u5236\u5907\u5728\u91cf\u5b50\u4fe1\u606f\u7cfb\u7edf\u80cc\u666f\u4e0b\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.00391", "categories": ["cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.00391", "abs": "https://arxiv.org/abs/2508.00391", "authors": ["Guanjie Huang", "Danny H. K. Tsang", "Shan Yang", "Guangzhi Lei", "Li Liu"], "title": "Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition", "comment": "9 pages", "summary": "Cued Speech (CS) is a visual communication system that combines lip-reading\nwith hand coding to facilitate communication for individuals with hearing\nimpairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures\nand lip movements into text via AI-driven methods. Traditionally, the temporal\nasynchrony between hand and lip movements requires the design of complex\nmodules to facilitate effective multimodal fusion. However, constrained by\nlimited data availability, current methods demonstrate insufficient capacity\nfor adequately training these fusion mechanisms, resulting in suboptimal\nperformance. Recently, multi-agent systems have shown promising capabilities in\nhandling complex tasks with limited data availability. To this end, we propose\nthe first collaborative multi-agent system for ACSR, named Cued-Agent. It\nintegrates four specialized sub-agents: a Multimodal Large Language Model-based\nHand Recognition agent that employs keyframe screening and CS expert prompt\nstrategies to decode hand movements, a pretrained Transformer-based Lip\nRecognition agent that extracts lip features from the input video, a Hand\nPrompt Decoding agent that dynamically integrates hand prompts with lip\nfeatures during inference in a training-free manner, and a Self-Correction\nPhoneme-to-Word agent that enables post-process and end-to-end conversion from\nphoneme sequences to natural language sentences for the first time through\nsemantic refinement. To support this study, we expand the existing Mandarin CS\ndataset by collecting data from eight hearing-impaired cuers, establishing a\nmixed dataset of fourteen subjects. Extensive experiments demonstrate that our\nCued-Agent performs superbly in both normal and hearing-impaired scenarios\ncompared with state-of-the-art methods. The implementation is available at\nhttps://github.com/DennisHgj/Cued-Agent.", "AI": {"tldr": "Cued-Agent \u662f\u9996\u4e2a\u7528\u4e8e\u81ea\u52a8\u624b\u8bed\u8bc6\u522b (ACSR) \u7684\u534f\u4f5c\u591a\u4e3b\u4f53\u7cfb\u7edf\u3002\u5b83\u7ed3\u5408\u4e86\u624b\u90e8\u548c\u5507\u90e8\u8bc6\u522b\u3001\u624b\u8bed\u63d0\u793a\u89e3\u7801\u4ee5\u53ca\u97f3\u7d20\u5230\u5355\u8bcd\u7684\u8f6c\u6362\uff0c\u5e76\u4f7f\u7528\u4e86\u5305\u542b 14 \u540d\u7528\u6237\u7684\u6269\u5c55\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cCued-Agent \u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u624b\u8bed\u8bc6\u522b\u65b9\u6cd5\u5728\u5904\u7406\u624b\u90e8\u548c\u5507\u90e8\u8fd0\u52a8\u7684\u65f6\u95f4\u4e0d\u540c\u6b65\u95ee\u9898\u65f6\uff0c\u7531\u4e8e\u6570\u636e\u53ef\u7528\u6027\u6709\u9650\uff0c\u96be\u4ee5\u5145\u5206\u8bad\u7ec3\u878d\u5408\u673a\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u591a\u4e3b\u4f53\u7cfb\u7edf\u5728\u5904\u7406\u6b64\u7c7b\u590d\u6742\u4efb\u52a1\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Cued-Agent \u7684\u534f\u4f5c\u591a\u4e3b\u4f53\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u624b\u90e8\u8bc6\u522b\u3001\u5507\u90e8\u8bc6\u522b\u3001\u624b\u8bed\u63d0\u793a\u89e3\u7801\u548c\u97f3\u7d20\u5230\u5355\u8bcd\u7684\u81ea\u6211\u7ea0\u6b63\u8f6c\u6362\u56db\u4e2a\u5b50\u4ee3\u7406\uff0c\u4ee5\u5b9e\u73b0\u81ea\u52a8\u624b\u8bed\u8bc6\u522b\u3002", "result": "Cued-Agent \u5728\u6b63\u5e38\u4eba\u548c\u542c\u969c\u4eba\u58eb\u7684\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "Cued-Agent \u5728\u6b63\u5e38\u4eba\u548c\u542c\u969c\u4eba\u58eb\u7684\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.00513", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00513", "abs": "https://arxiv.org/abs/2508.00513", "authors": ["Yiming Xu", "Xu Hua", "Zhen Peng", "Bin Shi", "Jiarun Chen", "Xingbo Fu", "Song Wang", "Bo Dong"], "title": "Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning", "comment": "Accepted by ECAI 2025", "summary": "The widespread application of graph data in various high-risk scenarios has\nincreased attention to graph anomaly detection (GAD). Faced with real-world\ngraphs that often carry node descriptions in the form of raw text sequences,\ntermed text-attributed graphs (TAGs), existing graph anomaly detection\npipelines typically involve shallow embedding techniques to encode such textual\ninformation into features, and then rely on complex self-supervised tasks\nwithin the graph domain to detect anomalies. However, this text encoding\nprocess is separated from the anomaly detection training objective in the graph\ndomain, making it difficult to ensure that the extracted textual features focus\non GAD-relevant information, seriously constraining the detection capability.\nHow to seamlessly integrate raw text and graph topology to unleash the vast\npotential of cross-modal data in TAGs for anomaly detection poses a challenging\nissue. This paper presents a novel end-to-end paradigm for text-attributed\ngraph anomaly detection, named CMUCL. We simultaneously model data from both\ntext and graph structures, and jointly train text and graph encoders by\nleveraging cross-modal and uni-modal multi-scale consistency to uncover\npotential anomaly-related information. Accordingly, we design an anomaly score\nestimator based on inconsistency mining to derive node-specific anomaly scores.\nConsidering the lack of benchmark datasets tailored for anomaly detection on\nTAGs, we release 8 datasets to facilitate future research. Extensive\nevaluations show that CMUCL significantly advances in text-attributed graph\nanomaly detection, delivering an 11.13% increase in average accuracy (AP) over\nthe suboptimal.", "AI": {"tldr": "CMUCL\u662f\u4e00\u79cd\u7528\u4e8e\u6587\u672c\u5c5e\u6027\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u65b0\u578b\u7aef\u5230\u7aef\u8303\u5f0f\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u6587\u672c\u548c\u56fe\u7ed3\u6784\uff0c\u5e76\u5229\u7528\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u6765\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u53d1\u5e03\u4e868\u4e2a\u65b0\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u6587\u672c\u5c5e\u6027\u56fe\uff08TAGs\uff09\u65f6\uff0c\u901a\u5e38\u5c06\u6587\u672c\u4fe1\u606f\u7f16\u7801\u4e3a\u7279\u5f81\uff0c\u7136\u540e\u4f9d\u8d56\u56fe\u57df\u5185\u7684\u81ea\u76d1\u7763\u4efb\u52a1\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u6587\u672c\u7f16\u7801\u8fc7\u7a0b\u4e0e\u56fe\u57df\u7684\u5f02\u5e38\u68c0\u6d4b\u8bad\u7ec3\u76ee\u6807\u662f\u5206\u79bb\u7684\uff0c\u5bfc\u81f4\u63d0\u53d6\u7684\u6587\u672c\u7279\u5f81\u53ef\u80fd\u5e76\u975e\u4e13\u6ce8\u4e8e\u4e0e\u5f02\u5e38\u68c0\u6d4b\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u4e25\u91cd\u5236\u7ea6\u4e86\u68c0\u6d4b\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u65e0\u7f1d\u96c6\u6210\u539f\u59cb\u6587\u672c\u548c\u56fe\u7ed3\u6784\u4ee5\u91ca\u653e\u8de8\u5a92\u4f53\u6570\u636e\u5728TAG\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCMUCL\u7684\u65b0\u578b\u7aef\u5230\u7aef\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u80fd\u591f\u540c\u65f6\u5bf9\u6587\u672c\u548c\u56fe\u7ed3\u6784\u6570\u636e\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u5229\u7528\u8de8\u6a21\u6001\u548c\u5355\u6a21\u6001\u591a\u5c3a\u5ea6\u4e00\u81f4\u6027\u6765\u8054\u5408\u8bad\u7ec3\u6587\u672c\u548c\u56fe\u7f16\u7801\u5668\uff0c\u4ee5\u53d1\u6398\u4e0e\u5f02\u5e38\u68c0\u6d4b\u76f8\u5173\u7684\u6f5c\u5728\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u4e00\u81f4\u6027\u6316\u6398\u7684\u5f02\u5e38\u8bc4\u5206\u4f30\u8ba1\u5668\u6765\u83b7\u5f97\u8282\u70b9\u7279\u5b9a\u7684\u5f02\u5e38\u5206\u6570\u3002", "result": "CMUCL\u5728\u6587\u672c\u5c5e\u6027\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5e73\u5747\u51c6\u786e\u7387\uff08AP\uff09\u6bd4\u6b21\u4f18\u65b9\u6cd5\u63d0\u9ad8\u4e8611.13%\u3002\u6b64\u5916\uff0c\u53d1\u5e03\u4e868\u4e2a\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u672a\u6765\u7684\u7814\u7a76\u3002", "conclusion": "CMUCL\u663e\u8457\u63d0\u9ad8\u4e86\u6587\u672c\u5c5e\u6027\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u5e73\u5747\u51c6\u786e\u7387\uff08AP\uff09\u63d0\u9ad8\u4e8611.13%\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u672c\u7279\u5f81\u63d0\u53d6\u548c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e4b\u95f4\u7f3a\u4e4f\u8054\u5408\u4f18\u5316\u7684\u95ee\u9898\u3002"}}
{"id": "2508.00709", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00709", "abs": "https://arxiv.org/abs/2508.00709", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Shivam Mishra", "Ajay Varghese Thomas", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System", "comment": null, "summary": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faNyayaRAG\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6848\u4ef6\u4e8b\u5b9e\u3001\u6cd5\u5f8b\u6761\u6587\u548c\u5148\u4f8b\u5224\u51b3\uff0c\u63d0\u5347\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u5370\u5ea6\u6cd5\u5f8b\u4f53\u7cfb\u4e2d\u3002", "motivation": "\u4e4b\u524d\u7684\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u65b9\u6cd5\uff08LJP\uff09\u4e3b\u8981\u4f9d\u8d56\u6848\u4ef6\u7684\u5185\u90e8\u5185\u5bb9\uff0c\u5982\u4e8b\u5b9e\u3001\u95ee\u9898\u548c\u63a8\u7406\uff0c\u800c\u5ffd\u89c6\u4e86\u666e\u901a\u6cd5\u7cfb\u4e2d\u4f9d\u8d56\u6cd5\u5f8b\u6761\u6587\u548c\u53f8\u6cd5\u5148\u4f8b\u7684\u6838\u5fc3\u8981\u7d20\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNyayaRAG\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5411\u6a21\u578b\u63d0\u4f9b\u4e8b\u5b9e\u6848\u4ef6\u63cf\u8ff0\u3001\u76f8\u5173\u6cd5\u5f8b\u6cd5\u89c4\u548c\u8bed\u4e49\u68c0\u7d22\u7684\u5148\u4f8b\u6848\u4f8b\uff0c\u6765\u6a21\u62df\u771f\u5b9e\u7684\u6cd5\u5ead\u573a\u666f\u3002", "result": "\u6240\u63d0\u51fa\u7684NyayaRAG\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4e8b\u5b9e\u4fe1\u606f\u548c\u6cd5\u5f8b\u77e5\u8bc6\uff0c\u5728\u9884\u6d4b\u6cd5\u9662\u5224\u51b3\u548c\u751f\u6210\u6cd5\u5f8b\u89e3\u91ca\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u4e8b\u5b9e\u8f93\u5165\u4e2d\u589e\u52a0\u7ed3\u6784\u5316\u6cd5\u5f8b\u77e5\u8bc6\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u5728\u4e8b\u5b9e\u8f93\u5165\u4e2d\u589e\u52a0\u7ed3\u6784\u5316\u6cd5\u5f8b\u77e5\u8bc6\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u3002"}}
{"id": "2508.00790", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00790", "abs": "https://arxiv.org/abs/2508.00790", "authors": ["Luca Paccard", "Valentin Leloup", "Luca Lazzarini", "Agathe Blaise", "Mailys Guerault", "Mickael Faugeron", "Fabrice Arnal", "Mathieu Bertrand", "Raphael Aymeric", "Michel Sotom", "St\u00e9phanie Molin", "Patrick G\u00e9lard", "Pierre Besancenot", "Cyrille Laborde", "Laurent de Forges de Parny", "Mathias van den Bossche"], "title": "The Role of the Satellite in Quantum Information Networks", "comment": "28 pages", "summary": "Quantum Information Networks (QIN) attract increasing interest, as they will\nenable interconnection of multiple quantum devices in a distributed\norganization thus enhancing intrinsic computing, sensing, and security\ncapabilities. The core mechanism of a QIN is quantum state swapping, based on\nteleportation, which consumes quantum entanglement, and which can be seen in\nthis context as a new kind of network resource. The satellite is expected to\nplay a central role for supporting global connectivity in such novel networks\nin which ground fiber links have stringent restrictions in length due to the\nabsorption losses in optical fibers. There is indeed fundamental limits in the\nmaximal fiber links distance which may not be exceeded for any unitary links.\nIn this paper we clarify our motivations to develop such networks with\nsatellites, and we discuss their associated use cases based on entanglement\ndistribution, and we present the future potential users. We also assess\nquantitatively the ranges for which the satellite becomes mandatory in quantum\ninformation networks.", "AI": {"tldr": "\u91cf\u5b50\u4fe1\u606f\u7f51\u7edc\uff08QIN\uff09\u4f9d\u8d56\u4e8e\u91cf\u5b50\u7ea0\u7f20\uff0c\u4f46\u5149\u7ea4\u8ddd\u79bb\u6709\u9650\u3002\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5229\u7528\u536b\u661f\u5b9e\u73b0\u5168\u7403\u8fde\u63a5\uff0c\u514b\u670d\u8ddd\u79bb\u9650\u5236\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u52a8\u673a\u3001\u7528\u4f8b\u548c\u7528\u6237\uff0c\u91cf\u5316\u4e86\u536b\u661f\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u5730\u9762\u5149\u7ea4\u94fe\u8def\u5728\u8ddd\u79bb\u4e0a\u7684\u56fa\u6709\u9650\u5236\uff0c\u5e76\u5b9e\u73b0\u5168\u7403\u91cf\u5b50\u8fde\u63a5\u3002", "method": "\u63a2\u8ba8\u4e86\u91cf\u5b50\u6001\u4f20\u8f93\u548c\u7ea0\u7f20\u5206\u53d1\u5728\u91cf\u5b50\u4fe1\u606f\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u91cf\u5316\u4e86\u536b\u661f\u7684\u5fc5\u8981\u6027\u3002", "result": "\u9610\u8ff0\u4e86\u5229\u7528\u536b\u661f\u6784\u5efa\u91cf\u5b50\u4fe1\u606f\u7f51\u7edc\u7684\u52a8\u673a\uff0c\u8ba8\u8bba\u4e86\u5176\u7528\u4f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6f5c\u5728\u7528\u6237\uff0c\u540c\u65f6\u91cf\u5316\u4e86\u536b\u661f\u5728\u91cf\u5b50\u4fe1\u606f\u7f51\u7edc\u4e2d\u7684\u5fc5\u8981\u901a\u4fe1\u8ddd\u79bb\u3002", "conclusion": "\u536b\u661f\u5728\u91cf\u5b50\u4fe1\u606f\u7f51\u7edc\u4e2d\u5177\u6709\u4e0d\u53ef\u6216\u7f3a\u7684\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5728\u957f\u8ddd\u79bb\u8fde\u63a5\u65b9\u9762\u3002"}}
{"id": "2508.00395", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00395", "abs": "https://arxiv.org/abs/2508.00395", "authors": ["Fei Zhang", "Tianfei Zhou", "Jiangchao Yao", "Ya Zhang", "Ivor W. Tsang", "Yanfeng Wang"], "title": "Decouple before Align: Visual Disentanglement Enhances Prompt Tuning", "comment": "16 pages, Accepted at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI)", "summary": "Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,\nhas showcased remarkable effectiveness in improving the task-specific\ntransferability of vision-language models. This paper delves into a previously\noverlooked information asymmetry issue in PT, where the visual modality mostly\nconveys more context than the object-oriented textual modality.\nCorrespondingly, coarsely aligning these two modalities could result in the\nbiased attention, driving the model to merely focus on the context area. To\naddress this, we propose DAPT, an effective PT framework based on an intuitive\ndecouple-before-align concept. First, we propose to explicitly decouple the\nvisual modality into the foreground and background representation via\nexploiting coarse-and-fine visual segmenting cues, and then both of these\ndecoupled patterns are aligned with the original foreground texts and the\nhand-crafted background classes, thereby symmetrically strengthening the modal\nalignment. To further enhance the visual concentration, we propose a visual\npull-push regularization tailored for the foreground-background patterns,\ndirecting the original visual representation towards unbiased attention on the\nregion-of-interest object. We demonstrate the power of architecture-free DAPT\nthrough few-shot learning, base-to-novel generalization, and data-efficient\nlearning, all of which yield superior performance across prevailing benchmarks.\nOur code will be released at https://github.com/Ferenas/DAPT.", "AI": {"tldr": "DAPT\u901a\u8fc7\u89e3\u8026\u89c6\u89c9\u6a21\u6001\u5e76\u8fdb\u884c\u5bf9\u79f0\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u63d0\u793a\u8c03\u4f18\u4e2d\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u5e76\u5229\u7528\u6b63\u5219\u5316\u589e\u5f3a\u4e86\u5bf9\u76ee\u6807\u533a\u57df\u7684\u5173\u6ce8\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5404\u9879\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u63d0\u793a\u8c03\u4f18\uff08PT\uff09\u65b9\u6cd5\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u5373\u89c6\u89c9\u6a21\u6001\u6bd4\u9762\u5411\u5bf9\u8c61\u7684\u6587\u672c\u6a21\u6001\u5305\u542b\u66f4\u591a\u4fe1\u606f\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u6ce8\u610f\u529b\u504f\u5dee\uff0c\u4ec5\u5173\u6ce8\u80cc\u666f\u533a\u57df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDAPT\u7684\u63d0\u793a\u8c03\u4f18\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u662f\u5148\u89e3\u8026\u518d\u5bf9\u9f50\u3002\u5177\u4f53\u6765\u8bf4\uff0cDAPT\u5c06\u89c6\u89c9\u6a21\u6001\u5206\u89e3\u4e3a\u524d\u666f\u548c\u80cc\u666f\u8868\u793a\uff0c\u5e76\u5206\u522b\u4e0e\u6587\u672c\u6a21\u6001\uff08\u524d\u666f\u6587\u672c\u548c\u624b\u5de5\u8bbe\u8ba1\u7684\u80cc\u666f\u7c7b\uff09\u8fdb\u884c\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u89c6\u89c9\u62c9\u63a8\u6b63\u5219\u5316\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u611f\u5174\u8da3\u533a\u57df\u7684\u5173\u6ce8\u3002", "result": "DAPT\u5728\u5c11\u6837\u672c\u5b66\u4e60\u3001\u57fa\u7840\u5230\u65b0\u9896\u6cdb\u5316\u548c\u6570\u636e\u9ad8\u6548\u5b66\u4e60\u7b49\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7ed3\u679c\u3002", "conclusion": "DAPT\u901a\u8fc7\u89e3\u8026-\u5bf9\u9f50\u8303\u5f0f\u548c\u89c6\u89c9\u62c9\u63a8\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63d0\u793a\u8c03\u4f18\u4e2d\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u5728\u5c11\u6837\u672c\u5b66\u4e60\u3001\u57fa\u7840\u5230\u65b0\u9896\u6cdb\u5316\u548c\u6570\u636e\u9ad8\u6548\u5b66\u4e60\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00523", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00523", "abs": "https://arxiv.org/abs/2508.00523", "authors": ["Sifan Yang", "Yuanyu Wan", "Lijun Zhang"], "title": "Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting", "comment": null, "summary": "We investigate the online nonsubmodular optimization with delayed feedback in\nthe bandit setting, where the loss function is $\\alpha$-weakly DR-submodular\nand $\\beta$-weakly DR-supermodular. Previous work has established an\n$(\\alpha,\\beta)$-regret bound of $\\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is\nthe dimensionality and $d$ is the maximum delay. However, its regret bound\nrelies on the maximum delay and is thus sensitive to irregular delays.\nAdditionally, it couples the effects of delays and bandit feedback as its bound\nis the product of the delay term and the $\\mathcal{O}(nT^{2/3})$ regret bound\nin the bandit setting without delayed feedback. In this paper, we develop two\nalgorithms to address these limitations, respectively. Firstly, we propose a\nnovel method, namely DBGD-NF, which employs the one-point gradient estimator\nand utilizes all the available estimated gradients in each round to update the\ndecision. It achieves a better $\\mathcal{O}(n\\bar{d}^{1/3}T^{2/3})$ regret\nbound, which is relevant to the average delay $\\bar{d} =\n\\frac{1}{T}\\sum_{t=1}^T d_t\\leq d$. Secondly, we extend DBGD-NF by employing a\nblocking update mechanism to decouple the joint effect of the delays and bandit\nfeedback, which enjoys an $\\mathcal{O}(n(T^{2/3} + \\sqrt{dT}))$ regret bound.\nWhen $d = \\mathcal{O}(T^{1/3})$, our regret bound matches the\n$\\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.\nCompared to our first $\\mathcal{O}(n\\bar{d}^{1/3}T^{2/3})$ bound, it is more\nadvantageous when the maximum delay $d = o(\\bar{d}^{2/3}T^{1/3})$. Finally, we\nconduct experiments on structured sparse learning to demonstrate the\nsuperiority of our methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5\uff08DBGD-NF\u53ca\u5176\u6269\u5c55\uff09\u6765\u4f18\u5316\u5e26\u6709\u5ef6\u8fdf\u53cd\u9988\u7684\u5728\u7ebf\u975e\u6b21\u6a21\u95ee\u9898\u3002\u65b0\u7b97\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u9057\u61be\u754c\uff0c\u5c24\u5176\u662f\u5728\u5ef6\u8fdf\u53d8\u5316\u8f83\u5927\u6216\u7279\u5b9a\u6761\u4ef6\u4e0b\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u7ed3\u6784\u5316\u7a00\u758f\u5b66\u4e60\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5728\u7ebf\u975e\u6b21\u6a21\u4f18\u5316\u4e2d\u5ef6\u8fdf\u53cd\u9988\u7684\u95ee\u9898\u3002\u73b0\u6709\u7684\u7814\u7a76\u63d0\u51fa\u7684\u9057\u61be\u754c\u867d\u7136\u4f9d\u8d56\u4e8e\u6700\u5927\u5ef6\u8fdf\uff0c\u4f46\u5bf9\u4e0d\u89c4\u5219\u5ef6\u8fdf\u654f\u611f\uff0c\u5e76\u4e14\u5c06\u5ef6\u8fdf\u548c\u5e26\u5bbd\u53cd\u9988\u7684\u5f71\u54cd\u8026\u5408\u5728\u4e00\u8d77\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u65e8\u5728\u63d0\u4f9b\u66f4\u4f18\u7684\u9057\u61be\u754c\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\u6765\u89e3\u51b3\u5728\u7ebf\u975e\u6b21\u6a21\u4f18\u5316\u4e2d\u7684\u5ef6\u8fdf\u53cd\u9988\u95ee\u9898\u3002\u7b2c\u4e00\u79cd\u7b97\u6cd5DBGD-NF\u5229\u7528\u5355\u70b9\u68af\u5ea6\u4f30\u8ba1\u5668\uff0c\u5e76\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u4f7f\u7528\u6240\u6709\u53ef\u7528\u7684\u4f30\u8ba1\u68af\u5ea6\u6765\u66f4\u65b0\u51b3\u7b56\u3002\u7b2c\u4e8c\u79cd\u7b97\u6cd5\u662fDBGD-NF\u7684\u6269\u5c55\uff0c\u91c7\u7528\u963b\u585e\u66f4\u65b0\u673a\u5236\u6765\u89e3\u8026\u5ef6\u8fdf\u548c\u5e26\u5bbd\u53cd\u9988\u7684\u8054\u5408\u6548\u5e94\u3002", "result": "DBGD-NF\u7b97\u6cd5\u5b9e\u73b0\u4e86$\"O(n\bar{d}^{1/3}T^{2/3})\"$\u7684\u9057\u61be\u754c\uff0c\u8be5\u754c\u4e0e\u5e73\u5747\u5ef6\u8fdf$\"\\bar{d}\"$\u76f8\u5173\u3002\u6269\u5c55\u7684DBGD-NF\u7b97\u6cd5\uff0c\u91c7\u7528\u963b\u585e\u66f4\u65b0\u673a\u5236\uff0c\u5b9e\u73b0\u4e86$\"O(n(T^{2/3} + \"\text{sqrt}(dT)))\"$\u7684\u9057\u61be\u754c\u3002\u5f53\u5ef6\u8fdf$\"d = \"O(T^{1/3})\"$\u65f6\uff0c\u8be5\u9057\u61be\u754c\u4e0e\u65e0\u5ef6\u8fdf\u5e26\u5bbd\u8bbe\u7f6e\u4e0b\u7684$\"O(nT^{2/3})\"$\u754c\u9650\u76f8\u5f53\u3002\u6b64\u5916\uff0c\u5728$\"d = \"o(\bar{d}^{2/3}T^{1/3})\"$\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u7b97\u6cd5\u6bd4DBGD-NF\u66f4\u5177\u4f18\u52bf\u3002\u5b9e\u9a8c\u7ed3\u679c\u5728\u7ed3\u6784\u5316\u7a00\u758f\u5b66\u4e60\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7b97\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7b97\u6cd5\u6765\u5e94\u5bf9\u5728\u7ebf\u975e\u6b21\u6a21\u4f18\u5316\u4e2d\u7684\u5ef6\u8fdf\u53cd\u9988\u95ee\u9898\u3002\u7b2c\u4e00\u79cd\u7b97\u6cd5DBGD-NF\u4f7f\u7528\u5355\u70b9\u68af\u5ea6\u4f30\u8ba1\uff0c\u5e76\u5c06\u6240\u6709\u53ef\u7528\u68af\u5ea6\u7528\u4e8e\u66f4\u65b0\u51b3\u7b56\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5e73\u5747\u5ef6\u8fdf\u76f8\u5173\u7684$\"O(n\bar{d}^{1/3}T^{2/3})\"$\u7684\u9057\u61be\u754c\u3002\u7b2c\u4e8c\u79cd\u7b97\u6cd5\u6269\u5c55\u4e86DBGD-NF\uff0c\u5f15\u5165\u4e86\u963b\u585e\u66f4\u65b0\u673a\u5236\uff0c\u5c06\u5ef6\u8fdf\u548c\u5e26\u5bbd\u53cd\u9988\u7684\u5f71\u54cd\u89e3\u8026\uff0c\u8fbe\u5230\u4e86$\"O(n(T^{2/3} + \"\text{sqrt}(dT)))\"$\u7684\u9057\u61be\u754c\u3002\u5728\u5ef6\u8fdfd\u7b49\u4e8e$\"O(T^{1/3})\"$\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u754c\u9650\u4e0e\u65e0\u5ef6\u8fdf\u5e26\u5bbd\u8bbe\u7f6e\u4e0b\u7684$\"O(nT^{2/3})\"$\u754c\u9650\u76f8\u5f53\uff0c\u5e76\u4e14\u5728d\u7b49\u4e8e$\"o(\bar{d}^{2/3}T^{1/3})\"$\u65f6\u4f18\u4e8e\u7b2c\u4e00\u79cd\u7b97\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7ed3\u6784\u5316\u7a00\u758f\u5b66\u4e60\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2508.00719", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00719", "abs": "https://arxiv.org/abs/2508.00719", "authors": ["Yingxu Wang", "Shiqi Fan", "Mengzhu Wang", "Siwei Liu"], "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA", "comment": null, "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.", "AI": {"tldr": "DAMR\u662f\u4e00\u79cd\u65b0\u9896\u7684KGQA\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408MCTS\u548c\u81ea\u9002\u5e94\u8def\u5f84\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709KGQA\u65b9\u6cd5\u8981\u4e48\u53d7\u9650\u4e8e\u9759\u6001\u8def\u5f84\u63d0\u53d6\u548c\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u7ec6\u5316\uff0c\u8981\u4e48\u56e0\u4f9d\u8d56\u56fa\u5b9a\u8bc4\u5206\u51fd\u6570\u548c\u5927\u91cfLLM\u8c03\u7528\u800c\u4ea7\u751f\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u8def\u5f84\u8bc4\u4f30\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "method": "DAMR\u6846\u67b6\u6574\u5408\u4e86\u7b26\u53f7\u641c\u7d22\u548c\u81ea\u9002\u5e94\u8def\u5f84\u8bc4\u4f30\uff0c\u91c7\u7528\u57fa\u4e8eLLM\u7684\u89c4\u5212\u5668\u6307\u5bfc\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u9aa8\u5e72\uff0c\u5e76\u5f15\u5165\u4e86\u8f7b\u91cf\u7ea7\u7684Transformer\u8bc4\u5206\u5668\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u53ef\u884c\u6027\u4f30\u8ba1\uff0c\u540c\u65f6\u8fd8\u5305\u542b\u4e00\u4e2a\u52a8\u6001\u4f2a\u8def\u5f84\u7ec6\u5316\u673a\u5236\u6765\u89e3\u51b3\u76d1\u7763\u7a00\u758f\u6027\u95ee\u9898\u3002", "result": "DAMR\u6846\u67b6\u80fd\u591f\u8fdb\u884c\u9ad8\u6548\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684KGQA\u3002", "conclusion": "DAMR\u6846\u67b6\u5728\u591a\u4e2aKGQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.00793", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00793", "abs": "https://arxiv.org/abs/2508.00793", "authors": ["Luca Paccard", "Agathe Blaise", "Fabrice Arnal", "Laurent de Forges de Parny"], "title": "Entanglement Management in Space-Based Quantum Information Networks", "comment": "11 pages", "summary": "With the evolution of quantum computing, quantum sensing and secure quantum\ncommunication protocols, the demand for global development of Quantum\nInformation Networks (QIN) has become crucial. Satellites play an indispensable\nrole in enabling connectivity across vast distances, transcending terrestrial\nlimitations. In this article, we explore various ways in which satellites may\nbe involved in the deployment of these novel networks from their integration\ninto the network architecture to the challenges they face.", "AI": {"tldr": "Satellites are key to global Quantum Information Networks (QIN) due to their long-distance connectivity capabilities, despite facing integration and deployment challenges.", "motivation": "The demand for global development of Quantum Information Networks (QIN) has become crucial due to the evolution of quantum computing, quantum sensing, and secure quantum communication protocols. Satellites are indispensable for enabling connectivity across vast distances, transcending terrestrial limitations.", "method": "This article explores the integration of satellites into Quantum Information Networks (QIN) from their network architecture to the challenges they face.", "result": "This article explores the various ways satellites can be involved in the deployment of QIN, covering their integration into the network architecture and the challenges they encounter.", "conclusion": "Globally deploying Quantum Information Networks (QIN) is crucial with the evolution of quantum computing, quantum sensing, and secure quantum communication protocols. Satellites are essential for enabling long-distance connectivity and overcoming terrestrial limitations. This article explores the integration of satellites into QIN architecture and the associated challenges."}}
{"id": "2508.00397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00397", "abs": "https://arxiv.org/abs/2508.00397", "authors": ["Xi Xue", "Kunio Suzuki", "Nabarun Goswami", "Takuya Shintate"], "title": "Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency", "comment": null, "summary": "The rapid advancement of diffusion-based video generation models has led to\nincreasingly realistic synthetic content, presenting new challenges for video\nforgery detection. Existing methods often struggle to capture fine-grained\ntemporal inconsistencies, particularly in AI-generated videos with high visual\nfidelity and coherent motion. In this work, we propose a detection framework\nthat leverages spatial-temporal consistency by combining RGB appearance\nfeatures with optical flow residuals. The model adopts a dual-branch\narchitecture, where one branch analyzes RGB frames to detect appearance-level\nartifacts, while the other processes flow residuals to reveal subtle motion\nanomalies caused by imperfect temporal synthesis. By integrating these\ncomplementary features, the proposed method effectively detects a wide range of\nforged videos. Extensive experiments on text-to-video and image-to-video tasks\nacross ten diverse generative models demonstrate the robustness and strong\ngeneralization ability of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408RGB\u5916\u89c2\u7279\u5f81\u548c\u5149\u6d41\u6b8b\u5dee\u7684\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349AI\u751f\u6210\u89c6\u9891\u7ec6\u7c92\u5ea6\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u89c6\u9891\u5177\u6709\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8fde\u8d2f\u8fd0\u52a8\uff0c\u5bfc\u81f4\u73b0\u6709\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5229\u7528\u65f6\u7a7a\u4e00\u81f4\u6027\u68c0\u6d4b\u4f2a\u9020\u89c6\u9891\u7684\u6846\u67b6\uff0c\u7ed3\u5408RGB\u5916\u89c2\u7279\u5f81\u548c\u5149\u6d41\u6b8b\u5dee\u3002\u6a21\u578b\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff0c\u4e00\u4e2a\u5206\u652f\u5206\u6790RGB\u5e27\u4ee5\u68c0\u6d4b\u5916\u89c2\u4f2a\u5f71\uff0c\u53e6\u4e00\u4e2a\u5206\u652f\u5904\u7406\u5149\u6d41\u6b8b\u5dee\u4ee5\u63ed\u793a\u7531\u4e0d\u5b8c\u7f8e\u65f6\u95f4\u5408\u6210\u5f15\u8d77\u7684\u7ec6\u5fae\u8fd0\u52a8\u5f02\u5e38\u3002", "result": "\u5728\u6587\u672c\u5230\u89c6\u9891\u548c\u56fe\u50cf\u5230\u89c6\u9891\u4efb\u52a1\u4ee5\u53ca\u5341\u4e2a\u4e0d\u540c\u7684\u751f\u6210\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408RGB\u5916\u89c2\u7279\u5f81\u548c\u5149\u6d41\u6b8b\u5dee\u6765\u5229\u7528\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u6709\u6548\u68c0\u6d4b\u5404\u79cd\u4f2a\u9020\u89c6\u9891\u3002"}}
{"id": "2508.00539", "categories": ["cs.LG", "Cs"], "pdf": "https://arxiv.org/pdf/2508.00539", "abs": "https://arxiv.org/abs/2508.00539", "authors": ["Judy X Yang"], "title": "Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery", "comment": "8 pages, 6 figures", "summary": "Hyperspectral imaging offers detailed spectral information for mineral\nmapping; however, weak mineral signatures are often masked by noisy and\nredundant bands, limiting detection performance. To address this, we propose a\ntwo-stage integrated framework for enhanced mineral detection in the Cuprite\nmining district. In the first stage, we compute the signal-to-noise ratio (SNR)\nfor each spectral band and apply a phase-locked thresholding technique to\ndiscard low-SNR bands, effectively removing redundancy and suppressing\nbackground noise. Savitzky-Golay filtering is then employed for spectral\nsmoothing, serving a dual role first to stabilize trends during band selection,\nand second to preserve fine-grained spectral features during preprocessing. In\nthe second stage, the refined HSI data is reintroduced into the model, where\nKMeans clustering is used to extract 12 endmember spectra (W1 custom), followed\nby non negative least squares (NNLS) for abundance unmixing. The resulting\nendmembers are quantitatively compared with laboratory spectra (W1 raw) using\ncosine similarity and RMSE metrics. Experimental results confirm that our\nproposed pipeline improves unmixing accuracy and enhances the detection of weak\nmineral zones. This two-pass strategy demonstrates a practical and reproducible\nsolution for spectral dimensionality reduction and unmixing in geological HSI\napplications.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u9ad8\u5149\u8c31\u6210\u50cf\u4e2d\u5f31\u77ff\u7269\u4fe1\u53f7\u88ab\u566a\u58f0\u548c\u5197\u4f59\u6ce2\u6bb5\u63a9\u76d6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u96c6\u6210\u6846\u67b6\u3002\u8be5\u6846\u67b6\u9996\u5148\u901a\u8fc7\u4fe1\u566a\u6bd4\uff08SNR\uff09\u548c\u76f8\u4f4d\u9501\u5b9a\u9608\u503c\u6280\u672f\u53bb\u9664\u5197\u4f59\u548c\u80cc\u666f\u566a\u58f0\uff0c\u5e76\u4f7f\u7528Savitzky-Golay\u6ee4\u6ce2\u8fdb\u884c\u5149\u8c31\u5e73\u6ed1\uff1b\u7136\u540e\u5229\u7528KMeans\u805a\u7c7b\u548cNNLS\u89e3\u6df7\u63d0\u53d6\u7aef\u5143\u5149\u8c31\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u63d0\u9ad8\u6df7\u53e0\u7cbe\u5ea6\u5e76\u589e\u5f3a\u5f31\u77ff\u7269\u533a\u57df\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u867d\u7136\u80fd\u63d0\u4f9b\u8be6\u7ec6\u7684\u77ff\u4ea7\u4fe1\u606f\uff0c\u4f46\u5f31\u77ff\u7269\u4fe1\u53f7\u5e38\u88ab\u566a\u58f0\u548c\u5197\u4f59\u6ce2\u6bb5\u63a9\u76d6\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u6027\u80fd\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u63d0\u51fa\u6b64\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u96c6\u6210\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u8ba1\u7b97\u4fe1\u566a\u6bd4\uff08SNR\uff09\u5e76\u5e94\u7528\u76f8\u4f4d\u9501\u5b9a\u9608\u503c\u6280\u672f\u53bb\u9664\u4f4eSNR\u7684\u6ce2\u6bb5\uff0c\u7136\u540e\u4f7f\u7528Savitzky-Golay\u6ee4\u6ce2\u8fdb\u884c\u5149\u8c31\u5e73\u6ed1\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u5904\u7406\u540e\u9ad8\u5149\u8c31\u6570\u636e\uff08HSI\uff09\u91cd\u65b0\u5f15\u5165\u6a21\u578b\uff0c\u4f7f\u7528KMeans\u805a\u7c7b\u63d0\u53d612\u4e2a\u7aef\u5143\u5149\u8c31\uff0c\u5e76\u4f7f\u7528\u975e\u8d1f\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08NNLS\uff09\u8fdb\u884c\u4e30\u5ea6\u53cd\u6f14\u3002", "result": "\u901a\u8fc7\u4fe1\u566a\u6bd4\uff08SNR\uff09\u548c\u76f8\u4f4d\u9501\u5b9a\u9608\u503c\u6280\u672f\u6709\u6548\u53bb\u9664\u5197\u4f59\u548c\u80cc\u666f\u566a\u58f0\uff0c\u5e76\u901a\u8fc7Savitzky-Golay\u6ee4\u6ce2\u8fdb\u884c\u5149\u8c31\u5e73\u6ed1\uff0c\u4fdd\u7559\u4e86\u7cbe\u7ec6\u7684\u5149\u8c31\u7279\u5f81\u3002\u968f\u540e\uff0c\u4f7f\u7528KMeans\u805a\u7c7b\u548cNNLS\u89e3\u6df7\u63d0\u53d6\u4e8612\u4e2a\u7aef\u5143\u5149\u8c31\uff0c\u5e76\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548cRMSE\u4e0e\u5b9e\u9a8c\u5ba4\u5149\u8c31\u8fdb\u884c\u4e86\u91cf\u5316\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6df7\u53e0\u7cbe\u5ea6\u5e76\u589e\u5f3a\u4e86\u5f31\u77ff\u7269\u533a\u57df\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4fe1\u566a\u6bd4\uff08SNR\uff09\u548c\u76f8\u4f4d\u9501\u5b9a\u9608\u503c\u6280\u672f\u6709\u6548\u53bb\u9664\u5197\u4f59\u548c\u80cc\u666f\u566a\u58f0\uff0c\u5e76\u901a\u8fc7Savitzky-Golay\u6ee4\u6ce2\u8fdb\u884c\u5149\u8c31\u5e73\u6ed1\uff0c\u4fdd\u7559\u4e86\u7cbe\u7ec6\u7684\u5149\u8c31\u7279\u5f81\u3002\u968f\u540e\uff0c\u4f7f\u7528KMeans\u805a\u7c7b\u548cNNLS\u89e3\u6df7\u63d0\u53d6\u4e8612\u4e2a\u7aef\u5143\u5149\u8c31\uff0c\u5e76\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548cRMSE\u4e0e\u5b9e\u9a8c\u5ba4\u5149\u8c31\u8fdb\u884c\u4e86\u91cf\u5316\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6df7\u53e0\u7cbe\u5ea6\u5e76\u589e\u5f3a\u4e86\u5f31\u77ff\u7269\u533a\u57df\u7684\u68c0\u6d4b\u80fd\u529b\u3002\u8be5\u4e24\u6b65\u7b56\u7565\u4e3a\u5730\u8d28\u9ad8\u5149\u8c31\u9065\u611f\u5e94\u7528\u4e2d\u7684\u5149\u8c31\u964d\u7ef4\u548c\u6df7\u53e0\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u590d\u73b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00741", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00741", "abs": "https://arxiv.org/abs/2508.00741", "authors": ["Sohaib Imran", "Rob Lamb", "Peter M. Atkinson"], "title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data", "comment": null, "summary": "Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety.", "AI": {"tldr": "LLM\u53ef\u4ee5\u6839\u636e\u804a\u5929\u673a\u5668\u4eba\u7684\u884c\u4e3a\u63a8\u65ad\u5176\u8eab\u4efd\uff0c\u8fd9\u5bf9\u5176\u60c5\u5883\u611f\u77e5\u548cAI\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u7814\u7a76LLM\u662f\u5426\u80fd\u5229\u7528\u5176\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4fe1\u606f\u8fdb\u884c\u63a8\u7406\uff0c\u7279\u522b\u662f\u201c\u79bb\u5883\u63a8\u65ad\u201d\u80fd\u529b\uff0c\u4ee5\u8bc4\u4f30\u5176\u60c5\u5883\u611f\u77e5\u80fd\u529b\u548cAI\u5b89\u5168\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3LLM\u8bc6\u522b\u865a\u6784\u804a\u5929\u673a\u5668\u4eba\u7684\u540d\u5b57\u548c\u884c\u4e3a\u63cf\u8ff0\uff0c\u7136\u540e\u6d4b\u8bd5\u5176\u5728\u5bf9\u8bdd\u793a\u4f8b\u4e2d\u63a8\u65ad\u8eab\u4efd\u7684\u80fd\u529b\u3002", "result": "GPT-4o\u5728\u63a5\u53d7\u4e86\u865a\u6784\u804a\u5929\u673a\u5668\u4eba\u7684\u540d\u5b57\u548c\u884c\u4e3a\u63cf\u8ff0\u8bad\u7ec3\u540e\uff0c\u80fd\u591f\u6839\u636e\u5bf9\u8bdd\u793a\u4f8b\u63a8\u65ad\u51fa\u81f3\u5c11\u4e00\u4e2a\u804a\u5929\u673a\u5668\u4eba\u7684\u540d\u5b57\uff0c\u5e76\u4e14\u80fd\u66f4\u597d\u5730\u6a21\u4eff\u8be5\u804a\u5929\u673a\u5668\u4eba\u7684\u884c\u4e3a\u3002", "conclusion": "LLM\u5728\u6ca1\u6709\u660e\u786e\u5bf9\u8bdd\u793a\u4f8b\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u6839\u636e\u884c\u4e3a\u63a8\u65ad\u51fa\u804a\u5929\u673a\u5668\u4eba\u7684\u8eab\u4efd\u3002"}}
{"id": "2508.00797", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00797", "abs": "https://arxiv.org/abs/2508.00797", "authors": ["Frieder Lindel", "Carlos J. S\u00e1nchez Mart\u00ednez", "Johannes Feist", "Francisco J. Garc\u00eda-Vidal"], "title": "Close encounters between periodic light and periodic arrays of quantum emitters", "comment": null, "summary": "Periodically structured surfaces (metasurfaces), i.e., periodic light, have\nevolved as a powerful tool for manipulating electromagnetic fields both in\nclassical and quantum regimes. However, no general approach for quantizing the\nelectromagnetic fields and treating quantum light-matter interactions in such\nstructures exists. Here, we construct an ab initio few-mode quantization scheme\nfor metasurface resonances based on macroscopic quantum electrodynamics. We use\nour approach to propose a framework for strong light-matter coupling in which\ncollective excitations of periodic arrays of quantum emitters are strongly\ncoupled to the light modes supported by the metasurface, leading to the\nformation of crystal polaritons. As a proof-of-principle example of their\npotential, we show that interactions between crystal polaritons can lead to an\nefficient and directional generation of entangled photon pairs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5316\u65b9\u6848\uff0c\u7528\u4e8e\u5904\u7406\u7b49\u79bb\u6fc0\u5143\u8d85\u8868\u9762\u7684\u91cf\u5b50\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\uff0c\u5b9e\u73b0\u4e86\u6676\u4f53\u6781\u5316\u5b50\u7684\u5f62\u6210\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u4ea7\u751f\u7ea0\u7f20\u5149\u5b50\u5bf9\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u7f3a\u4e4f\u5bf9\u7b49\u79bb\u6fc0\u5143\u8d85\u8868\u9762\u8fdb\u884c\u91cf\u5316\u4ee5\u53ca\u5904\u7406\u5176\u4e2d\u91cf\u5b50\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\u7684\u4e00\u822c\u6027\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u57fa\u4e8e\u5b8f\u89c2\u91cf\u5b50\u7535\u52a8\u529b\u5b66\uff0c\u6784\u5efa\u4e86\u4e00\u79cd\u7528\u4e8e\u7b49\u79bb\u6fc0\u5143\u8d85\u8868\u9762\u5171\u632f\u7684\u4ece\u5934\u5f00\u59cb\u7684\u5c11\u6a21\u91cf\u5316\u65b9\u6848\u3002", "result": "\u5b9e\u73b0\u4e86\u96c6\u4f53\u7684\u91cf\u5b50\u53d1\u5c04\u5668\u6fc0\u5b50\u4e0e\u8d85\u8868\u9762\u652f\u6301\u7684\u5149\u6a21\u5f0f\u7684\u5f3a\u8026\u5408\uff0c\u5f62\u6210\u4e86\u6676\u4f53\u6781\u5316\u5b50\uff0c\u5e76\u5c55\u793a\u4e86\u6676\u4f53\u6781\u5316\u5b50\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u80fd\u591f\u9ad8\u6548\u3001\u5b9a\u5411\u5730\u4ea7\u751f\u7ea0\u7f20\u5149\u5b50\u5bf9\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b8f\u89c2\u91cf\u5b50\u7535\u52a8\u529b\u5b66\u3001\u7528\u4e8e\u7b49\u79bb\u6fc0\u5143\u8d85\u8868\u9762\u5171\u632f\u7684\u4ece\u5934\u5f00\u59cb\u7684\u5c11\u6a21\u91cf\u5316\u65b9\u6848\uff0c\u4e3a\u5f3a\u5149-\u7269\u8d28\u8026\u5408\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5468\u671f\u6027\u91cf\u5b50\u53d1\u5c04\u5668\u9635\u5217\u8868\u4f53\u6fc0\u5b50\u4e0e\u8d85\u8868\u9762\u652f\u6301\u7684\u5149\u6a21\u5f0f\u7684\u5f3a\u8026\u5408\uff0c\u5f62\u6210\u4e86\u6676\u4f53\u6781\u5316\u5b50\u3002"}}
{"id": "2508.00399", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00399", "abs": "https://arxiv.org/abs/2508.00399", "authors": ["Raiyaan Abdullah", "Yogesh Singh Rawat", "Shruti Vyas"], "title": "iSafetyBench: A video-language benchmark for safety in industrial environment", "comment": "Accepted to VISION'25 - ICCV 2025 workshop", "summary": "Recent advances in vision-language models (VLMs) have enabled impressive\ngeneralization across diverse video understanding tasks under zero-shot\nsettings. However, their capabilities in high-stakes industrial domains-where\nrecognizing both routine operations and safety-critical anomalies is\nessential-remain largely underexplored. To address this gap, we introduce\niSafetyBench, a new video-language benchmark specifically designed to evaluate\nmodel performance in industrial environments across both normal and hazardous\nscenarios. iSafetyBench comprises 1,100 video clips sourced from real-world\nindustrial settings, annotated with open-vocabulary, multi-label action tags\nspanning 98 routine and 67 hazardous action categories. Each clip is paired\nwith multiple-choice questions for both single-label and multi-label\nevaluation, enabling fine-grained assessment of VLMs in both standard and\nsafety-critical contexts. We evaluate eight state-of-the-art video-language\nmodels under zero-shot conditions. Despite their strong performance on existing\nvideo benchmarks, these models struggle with iSafetyBench-particularly in\nrecognizing hazardous activities and in multi-label scenarios. Our results\nreveal significant performance gaps, underscoring the need for more robust,\nsafety-aware multimodal models for industrial applications. iSafetyBench\nprovides a first-of-its-kind testbed to drive progress in this direction. The\ndataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.", "AI": {"tldr": "iSafetyBench \u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5de5\u4e1a\u5b89\u5168\u89c6\u9891\u7406\u89e3\u7684\u65b0\u57fa\u51c6\uff0c\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u6a21\u578b\u3002", "motivation": "\u76ee\u524d\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u4e1a\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u4ecd\u6709\u5f85\u63a2\u7d22\uff0c\u800c\u8bc6\u522b\u5e38\u89c4\u64cd\u4f5c\u548c\u5b89\u5168\u5173\u952e\u5f02\u5e38\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u5728\u8fd9\u4e9b\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa iSafetyBench\uff0c\u4e00\u4e2a\u5305\u542b 1100 \u4e2a\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u89c6\u9891\u7247\u6bb5\u7684\u65b0\u578b\u89c6\u9891-\u8bed\u8a00\u57fa\u51c6\uff0c\u5e76\u6807\u6ce8\u4e86\u6db5\u76d6 98 \u79cd\u5e38\u89c4\u52a8\u4f5c\u548c 67 \u79cd\u5371\u9669\u52a8\u4f5c\u7684\u5f00\u653e\u8bcd\u6c47\u3001\u591a\u6807\u7b7e\u52a8\u4f5c\u6807\u7b7e\u3002\u6bcf\u4e2a\u89c6\u9891\u7247\u6bb5\u90fd\u914d\u6709\u5355\u6807\u7b7e\u548c\u591a\u6807\u7b7e\u7684\u5355\u9879\u9009\u62e9\u9898\uff0c\u7528\u4e8e\u5728\u6807\u51c6\u548c\u5b89\u5168\u5173\u952e\u60c5\u5883\u4e0b\u5bf9\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002\u8bc4\u4f30\u4e86\u516b\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5c3d\u7ba1\u8bc4\u4f30\u7684\u516b\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u6709\u89c6\u9891\u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b83\u4eec\u5728 iSafetyBench \u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u8bc6\u522b\u5371\u9669\u6d3b\u52a8\u548c\u5904\u7406\u591a\u6807\u7b7e\u573a\u666f\u65f6\uff0c\u8fd9\u8868\u660e\u5728\u5de5\u4e1a\u5b89\u5168\u5e94\u7528\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u76ee\u524d\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u4e1a\u5b89\u5168\u9886\u57df\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u8bc6\u522b\u5371\u9669\u6d3b\u52a8\u548c\u591a\u6807\u7b7e\u573a\u666f\u65b9\u9762\uff0c\u8fd9\u51f8\u663e\u4e86\u5f00\u53d1\u66f4\u5f3a\u5927\u3001\u66f4\u6ce8\u91cd\u5b89\u5168\u7684\u6a21\u578b\u4ee5\u6ee1\u8db3\u5de5\u4e1a\u5e94\u7528\u9700\u6c42\u7684\u5fc5\u8981\u6027\u3002iSafetyBench \u662f\u4e00\u4e2a\u65e8\u5728\u63a8\u52a8\u8be5\u9886\u57df\u8fdb\u6b65\u7684\u5f00\u521b\u6027\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2508.00742", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00742", "abs": "https://arxiv.org/abs/2508.00742", "authors": ["Sarah Mercer", "Daniel P. Martin", "Phil Swatton"], "title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents", "comment": "26 pages, 14 figures", "summary": "Generative agents powered by Large Language Models demonstrate human-like\ncharacteristics through sophisticated natural language interactions. Their\nability to assume roles and personalities based on predefined character\nbiographies has positioned them as cost-effective substitutes for human\nparticipants in social science research. This paper explores the validity of\nsuch persona-based agents in representing human populations; we recreate the\nHEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,\nconducting factor analysis on their responses, and comparing these results to\nthe original findings presented by Ashton, Lee, & Goldberg in 2004. Our results\nfound 1) a coherent and reliable personality structure was recoverable from the\nagents' responses demonstrating partial alignment to the HEXACO framework. 2)\nthe derived personality dimensions were consistent and reliable within GPT-4,\nwhen coupled with a sufficiently curated population, and 3) cross-model\nanalysis revealed variability in personality profiling, suggesting\nmodel-specific biases and limitations. We discuss the practical considerations\nand challenges encountered during the experiment. This study contributes to the\nongoing discourse on the potential benefits and limitations of using generative\nagents in social science research and provides useful guidance on designing\nconsistent and representative agent personas to maximise coverage and\nrepresentation of human personality traits.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528310\u4e2aGPT-4\u4ee3\u7406\u8fdb\u884c\u4eba\u683c\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5b83\u4eec\u7684\u4eba\u683c\u7ed3\u6784\u4e0eHEXACO\u6846\u67b6\u90e8\u5206\u4e00\u81f4\uff0c\u4e14\u5728GPT-4\u6a21\u578b\u5185\u90e8\u8868\u73b0\u4e00\u81f4\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u95f4\u5b58\u5728\u5dee\u5f02\u3002\u8fd9\u8868\u660eAI\u4ee3\u7406\u53ef\u7528\u4e8e\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\uff0c\u4f46\u9700\u6ce8\u610f\u6a21\u578b\u504f\u89c1\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u751f\u6210\u5f0fAI\u5728\u8fdb\u884c\u590d\u6742\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u65f6\u80fd\u591f\u5c55\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u7279\u5f81\uff0c\u5e76\u4e14\u80fd\u591f\u6839\u636e\u9884\u5b9a\u4e49\u7684\u89d2\u8272\u4f20\u8bb0\u626e\u6f14\u4e0d\u540c\u7684\u89d2\u8272\u548c\u4e2a\u6027\uff0c\u56e0\u6b64\u5b83\u4eec\u6709\u6f5c\u529b\u6210\u4e3a\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u66ff\u4ee3\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u7ecf\u6d4e\u6709\u6548\u65b9\u6848\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u8fd9\u7c7b\u57fa\u4e8e\u4eba\u683c\u7279\u5f81\u7684\u4ee3\u7406\u5728\u4ee3\u8868\u4eba\u7c7b\u7fa4\u4f53\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf9310\u4e2a\u7531GPT-4\u9a71\u52a8\u7684\u4ee3\u7406\u8fdb\u884cHEXACO\u4eba\u683c\u91cf\u8868\u6d4b\u8bd5\uff0c\u5e76\u5bf9\u4ee3\u7406\u7684\u56de\u7b54\u8fdb\u884c\u56e0\u5b50\u5206\u6790\uff0c\u5c06\u7ed3\u679c\u4e0eAshton, Lee, & Goldberg\u4e8e2004\u5e74\u63d0\u51fa\u7684\u539f\u59cb\u7814\u7a76\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u6b64\u63a2\u7d22\u57fa\u4e8e\u4eba\u683c\u7279\u5f81\u7684\u4ee3\u7406\u5728\u4ee3\u8868\u4eba\u7c7b\u7fa4\u4f53\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff1a1) \u4ece\u4ee3\u7406\u7684\u56de\u7b54\u4e2d\u53ef\u4ee5\u6062\u590d\u51fa\u4e00\u81f4\u4e14\u53ef\u9760\u7684\u4eba\u683c\u7ed3\u6784\uff0c\u5e76\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u4e0eHEXACO\u6846\u67b6\u4fdd\u6301\u4e00\u81f4\u30022) \u5f53\u7ed3\u5408\u7ecf\u8fc7\u5145\u5206\u7b5b\u9009\u7684\u4ee3\u7406\u7fa4\u4f53\u65f6\uff0c\u4eceGPT-4\u4e2d\u63d0\u53d6\u7684\u4eba\u683c\u7ef4\u5ea6\u5177\u6709\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\u30023) \u8de8\u6a21\u578b\u5206\u6790\u63ed\u793a\u4e86\u4eba\u683c\u753b\u50cf\u5b58\u5728\u5dee\u5f02\uff0c\u8868\u660e\u5b58\u5728\u7279\u5b9a\u6a21\u578b\u7684\u504f\u89c1\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8eGPT-4\u7684\u751f\u6210\u5f0fAI\u4ee3\u7406\u5728\u6a21\u4eff\u4eba\u7c7b\u884c\u4e3a\u548c\u8fdb\u884c\u4eba\u683c\u6d4b\u8bd5\u65b9\u9762\u8868\u73b0\u51fa\u4e00\u5b9a\u7a0b\u5ea6\u7684\u6709\u6548\u6027\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u7ed3\u679c\u663e\u793a\u5176\u4eba\u683c\u7ed3\u6784\u4e0eHEXACO\u6846\u67b6\u4ec5\u90e8\u5206\u4e00\u81f4\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u8bf4\u660e\u5b58\u5728\u6a21\u578b\u7279\u5b9a\u7684\u504f\u89c1\u548c\u5c40\u9650\u6027\u3002\u7814\u7a76\u4e3a\u4f7f\u7528\u751f\u6210\u5f0fAI\u8fdb\u884c\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u4e00\u81f4\u4e14\u5177\u6709\u4ee3\u8868\u6027\u7684\u4eba\u683c\u4ee3\u7406\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00809", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00809", "abs": "https://arxiv.org/abs/2508.00809", "authors": ["Harry J. D. Miller"], "title": "Statistical Mechanics of Random Mixed State Ensembles with Fixed Energy", "comment": "18 pages, 5 figures. Comments welcome", "summary": "Mixed state ensembles such as the Bures-Hall and Hilbert-Schmidt measure are\nprobability distributions that characterise the statistical properties of\nrandom density matrices and can be used to determine the typical features of\nmixed quantum states. Here we extend this framework by considering the\nproperties of random states with fixed average energy, and the\nensemble-averaged density matrix is derived under this additional physical\nconstraint. This gives rise to a type of microcanonical ensemble for random\nmixed states and we connect its properties to a statistical mechanical entropy\nand temperature. Our results are illustrated using a variety of simple spin\nsystems, and we find that they can exhibit exotic features such as phase\ntransitions in the absence of interactions and finite relative energy\nfluctuations in the thermodynamic limit.", "AI": {"tldr": "\u7814\u7a76\u5c06\u5177\u6709\u56fa\u5b9a\u5e73\u5747\u80fd\u91cf\u7684\u968f\u673a\u72b6\u6001\u7eb3\u5165\u6df7\u5408\u72b6\u6001\u7684\u6982\u7387\u5206\u5e03\u6846\u67b6\uff0c\u53d1\u73b0\u4e86\u65e0\u76f8\u4e92\u4f5c\u7528\u7684\u76f8\u53d8\u7b49\u5947\u5f02\u7279\u5f81\u3002", "motivation": "\u6269\u5c55\u73b0\u6709\u6df7\u5408\u72b6\u6001\u6982\u7387\u5206\u5e03\uff08\u5982 Bures-Hall \u548c Hilbert-Schmidt \u6d4b\u5ea6\uff09\u7684\u6846\u67b6\uff0c\u4ee5\u5305\u542b\u5177\u6709\u56fa\u5b9a\u5e73\u5747\u80fd\u91cf\u7684\u968f\u673a\u72b6\u6001\u7684\u6027\u8d28\u3002", "method": "\u901a\u8fc7\u8003\u8651\u5177\u6709\u56fa\u5b9a\u5e73\u5747\u80fd\u91cf\u7684\u968f\u673a\u72b6\u6001\u7684\u6027\u8d28\uff0c\u5bfc\u51fa\u96c6\u5408\u5e73\u5747\u5bc6\u5ea6\u77e9\u9635\uff0c\u5e76\u5c06\u5176\u6027\u8d28\u4e0e\u7edf\u8ba1\u529b\u5b66\u71b5\u548c\u6e29\u5ea6\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u5bfc\u51fa\u4e86\u6df7\u5408\u72b6\u6001\u7684\u5fae\u6b63\u5219\u7cfb\u7efc\uff0c\u5e76\u53d1\u73b0\u4e86\u65e0\u76f8\u4e92\u4f5c\u7528\u7684\u76f8\u53d8\u548c\u70ed\u529b\u5b66\u6781\u9650\u4e0b\u7684\u6709\u9650\u76f8\u5bf9\u80fd\u91cf\u6da8\u843d\u7b49\u5947\u5f02\u7279\u5f81\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u6df7\u5408\u72b6\u6001\u7684\u6982\u7387\u5206\u5e03\u6269\u5c55\u5230\u5177\u6709\u56fa\u5b9a\u5e73\u5747\u80fd\u91cf\u7684\u968f\u673a\u72b6\u6001\uff0c\u5e76\u5c06\u5176\u4e0e\u7edf\u8ba1\u529b\u5b66\u71b5\u548c\u6e29\u5ea6\u8054\u7cfb\u8d77\u6765\u3002\u7814\u7a76\u7ed3\u679c\u5728\u7b80\u5355\u7684\u81ea\u65cb\u7cfb\u7edf\u4e2d\u5f97\u5230\u4e86\u8bf4\u660e\uff0c\u5e76\u53d1\u73b0\u4e86\u65e0\u76f8\u4e92\u4f5c\u7528\u7684\u76f8\u53d8\u548c\u70ed\u529b\u5b66\u6781\u9650\u4e0b\u7684\u6709\u9650\u76f8\u5bf9\u80fd\u91cf\u6da8\u843d\u7b49\u5947\u5f02\u7279\u5f81\u3002"}}
{"id": "2508.00400", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00400", "abs": "https://arxiv.org/abs/2508.00400", "authors": ["Janika Deborah Gajo", "Gerarld Paul Merales", "Jerome Escarcha", "Brenden Ashley Molina", "Gian Nartea", "Emmanuel G. Maminta", "Juan Carlos Roldan", "Rowel O. Atienza"], "title": "Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents", "comment": "14 pages, accepted in ICCV 2025 Workshop on RetailVision", "summary": "We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store\nsimulation for benchmarking embodied agents against human performance in\nshopping tasks. Addressing a gap in retail-specific sim environments for\nembodied agent training, Sari Sandbox features over 250 interactive grocery\nitems across three store configurations, controlled via an API. It supports\nboth virtual reality (VR) for human interaction and a vision language model\n(VLM)-powered embodied agent. We also introduce SariBench, a dataset of\nannotated human demonstrations across varied task difficulties. Our sandbox\nenables embodied agents to navigate, inspect, and manipulate retail items,\nproviding baselines against human performance. We conclude with benchmarks,\nperformance analysis, and recommendations for enhancing realism and\nscalability. The source code can be accessed via\nhttps://github.com/upeee/sari-sandbox-env.", "AI": {"tldr": "Sari Sandbox is a realistic 3D retail simulation for training embodied agents, featuring interactive items and a dataset of human demonstrations for benchmarking against human performance.", "motivation": "Addressing a gap in retail-specific sim environments for embodied agent training.", "method": "The paper presents Sari Sandbox, a high-fidelity, photorealistic 3D retail store simulation for benchmarking embodied agents against human performance in shopping tasks. It features over 250 interactive grocery items across three store configurations, controlled via an API. It supports both virtual reality (VR) for human interaction and a vision language model (VLM)-powered embodied agent. The paper also introduces SariBench, a dataset of annotated human demonstrations across varied task difficulties.", "result": "Sari Sandbox features over 250 interactive grocery items across three store configurations, controlled via an API. It supports both virtual reality (VR) for human interaction and a vision language model (VLM)-powered embodied agent. SariBench is a dataset of annotated human demonstrations across varied task difficulties.", "conclusion": "The sandbox enables embodied agents to navigate, inspect, and manipulate retail items, providing baselines against human performance. The paper concludes with benchmarks, performance analysis, and recommendations for enhancing realism and scalability."}}
{"id": "2508.00743", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00743", "abs": "https://arxiv.org/abs/2508.00743", "authors": ["Sebastian Wind", "Jeta Sopa", "Daniel Truhn", "Mahshad Lotfinia", "Tri-Thien Nguyen", "Keno Bressem", "Lisa Adams", "Mirabela Rusu", "Harald K\u00f6stler", "Gerhard Wellein", "Andreas Maier", "Soroosh Tayebi Arasteh"], "title": "Agentic large language models improve retrieval-based radiology question answering", "comment": null, "summary": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684 agentic RAG \u6846\u67b6\u901a\u8fc7\u667a\u80fd\u95ee\u9898\u5206\u89e3\u548c\u8fed\u4ee3\u4fe1\u606f\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653e\u5c04\u5b66\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u5c24\u5176\u5bf9\u4e2d\u578b AI \u6a21\u578b\u6548\u679c\u663e\u8457\u3002", "motivation": "\u4f20\u7edf\u7684\u5355\u6b65\u68c0\u7d22 RAG \u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u7684\u4e34\u5e8a\u63a8\u7406\u4efb\u52a1\u65f6\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u66f4\u4f18\u7684\u6846\u67b6\u6765\u63d0\u9ad8\u653e\u5c04\u5b66\u95ee\u7b54\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd agentic RAG \u6846\u67b6\uff0c\u4f7f LLM \u80fd\u591f\u81ea\u4e3b\u5206\u89e3\u653e\u5c04\u5b66\u95ee\u9898\uff0c\u4ece Radiopaedia \u8fed\u4ee3\u68c0\u7d22\u4e34\u5e8a\u8bc1\u636e\uff0c\u5e76\u52a8\u6001\u7efc\u5408\u57fa\u4e8e\u8bc1\u636e\u7684\u56de\u7b54\u3002", "result": "Agentic RAG \u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5e73\u5747\u8bca\u65ad\u51c6\u786e\u6027\uff0873% vs 64%\uff09\uff0c\u7279\u522b\u662f\u5728\u4e2d\u7b49\u89c4\u6a21\u548c\u5c0f\u578b\u6a21\u578b\u4e2d\u3002\u8be5\u6846\u67b6\u8fd8\u51cf\u5c11\u4e86\u5e7b\u89c9\uff08\u5e73\u5747 9.4%\uff09\u5e76\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u4f9d\u636e\uff0846% \u7684\u6848\u4f8b\u68c0\u7d22\u5230\u76f8\u5173\u4e0a\u4e0b\u6587\uff09\u3002", "conclusion": "Agentic RAG \u6846\u67b6\u901a\u8fc7\u4f7f LLM \u81ea\u4e3b\u5206\u89e3\u95ee\u9898\u3001\u8fed\u4ee3\u68c0\u7d22\u8bc1\u636e\u548c\u52a8\u6001\u7efc\u5408\u56de\u7b54\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653e\u5c04\u5b66\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u4e8b\u5b9e\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e2d\u7b49\u89c4\u6a21\u7684\u6a21\u578b\u4e2d\u3002"}}
{"id": "2508.00813", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.00813", "abs": "https://arxiv.org/abs/2508.00813", "authors": ["Diego S. Starke", "Marcos L. W. Basso", "Lucas C. C\u00e9leri", "Jonas Maziero"], "title": "Entanglement swapping for partially entangled qudits and the role of quantum complementarity", "comment": null, "summary": "We extend the entanglement swapping protocol (ESP) to partially entangled\nqudit states and analyze the process within the framework of complete\ncomplementarity relations (CCRs). Building on previous results for qubits, we\nshow that the average distributed entanglement between two parties via ESP is\nbounded above by the initial entanglement of one of the input pairs, and also\nby the product of the initial entanglements. Notably, we find that using\ninitial states with vanishing local quantum coherence is sufficient to capture\nthe essential features of the protocol, simplifying the analysis. By exploring\nthe cases of qubits and qutrits, we observe that the upper bound on the average\ndistributed entanglement -- expressed in terms of the product of the initial\nentanglements -- can be improved, and we conjecture what this tighter bound\nmight be. Finally, we discuss the role of quantum complementarity in the ESP\nand show how local predictability constrains the entanglement that can be\noperationally distributed via ESP.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.00406", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00406", "abs": "https://arxiv.org/abs/2508.00406", "authors": ["Tao Wu", "Jingyuan Ye", "Ying Fu"], "title": "PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos", "comment": null, "summary": "Geometric distortions and blurring caused by atmospheric turbulence degrade\nthe quality of long-range dynamic scene videos. Existing methods struggle with\nrestoring edge details and eliminating mixed distortions, especially under\nconditions of strong turbulence and complex dynamics. To address these\nchallenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines\nturbulence intensity, optical flow, and proportions of dynamic regions to\naccurately quantify video dynamic intensity under varying turbulence conditions\nand provide a high-dynamic turbulence training dataset. Additionally, we\npropose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework\nthat consists of three stages: \\textbf{de-tilting} for geometric stabilization,\n\\textbf{motion segmentation enhancement} for dynamic region refinement, and\n\\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight\nbackbones and stage-wise joint training to ensure both efficiency and high\nrestoration quality. Experimental results demonstrate that the proposed method\neffectively suppresses motion trailing artifacts, restores edge details and\nexhibits strong generalization capability, especially in real-world scenarios\ncharacterized by high-turbulence and complex dynamics. We will make the code\nand datasets openly available.", "AI": {"tldr": "\"\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7DEI\u6307\u6570\u91cf\u5316\u52a8\u6001\u5f3a\u5ea6\uff0c\u5e76\u4f7f\u7528PMR\u6846\u67b6\u5206\u9636\u6bb5\u6062\u590d\u89c6\u9891\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u9ad8\u6e4d\u6d41\u548c\u590d\u6742\u52a8\u6001\u573a\u666f\u3002\"", "motivation": "\"\u4e3a\u4e86\u89e3\u51b3\u5927\u6c14\u6e4d\u6d41\u5f15\u8d77\u7684\u51e0\u4f55\u7578\u53d8\u548c\u6a21\u7cca\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u5728\u6062\u590d\u8fb9\u7f18\u7ec6\u8282\u548c\u6d88\u9664\u6df7\u5408\u7578\u53d8\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5f3a\u6e4d\u6d41\u548c\u590d\u6742\u52a8\u6001\u6761\u4ef6\u4e0b\u3002\"", "method": "\"\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6548\u7387\u6307\u6570\uff08DEI\uff09\u6765\u91cf\u5316\u89c6\u9891\u52a8\u6001\u5f3a\u5ea6\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u7269\u7406\u6a21\u578b\u9a71\u52a8\u7684\u591a\u9636\u6bb5\u89c6\u9891\u6062\u590d\uff08PMR\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a\u503e\u659c\u6821\u6b63\u3001\u8fd0\u52a8\u5206\u5272\u589e\u5f3a\u548c\u53bb\u6a21\u7cca\u3002PMR\u91c7\u7528\u8f7b\u91cf\u7ea7\u9aa8\u5e72\u7f51\u7edc\u548c\u5206\u9636\u6bb5\u8054\u5408\u8bad\u7ec3\uff0c\u4ee5\u786e\u4fdd\u6548\u7387\u548c\u9ad8\u8d28\u91cf\u6062\u590d\u3002\"", "result": "\"\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6291\u5236\u8fd0\u52a8\u62d6\u5c3e\u4f2a\u5f71\uff0c\u6062\u590d\u8fb9\u7f18\u7ec6\u8282\uff0c\u5e76\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u6e4d\u6d41\u548c\u590d\u6742\u52a8\u6001\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u3002\"", "conclusion": "\"\u8be5\u65b9\u6cd5\u6709\u6548\u6291\u5236\u4e86\u8fd0\u52a8\u62d6\u5c3e\u4f2a\u5f71\uff0c\u6062\u590d\u4e86\u8fb9\u7f18\u7ec6\u8282\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9ad8\u6e4d\u6d41\u548c\u590d\u6742\u52a8\u6001\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u63d0\u4f9b\u3002\""}}
{"id": "2508.00586", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00586", "abs": "https://arxiv.org/abs/2508.00586", "authors": ["Thorben Werner", "Lars Schmidt-Thieme", "Vijaya Krishna Yalavarthi"], "title": "The Role of Active Learning in Modern Machine Learning", "comment": null, "summary": "Even though Active Learning (AL) is widely studied, it is rarely applied in\ncontexts outside its own scientific literature. We posit that the reason for\nthis is AL's high computational cost coupled with the comparatively small lifts\nit is typically able to generate in scenarios with few labeled points. In this\nwork we study the impact of different methods to combat this low data scenario,\nnamely data augmentation (DA), semi-supervised learning (SSL) and AL. We find\nthat AL is by far the least efficient method of solving the low data problem,\ngenerating a lift of only 1-4\\% over random sampling, while DA and SSL methods\ncan generate up to 60\\% lift in combination with random sampling. However, when\nAL is combined with strong DA and SSL techniques, it surprisingly is still able\nto provide improvements. Based on these results, we frame AL not as a method to\ncombat missing labels, but as the final building block to squeeze the last bits\nof performance out of data after appropriate DA and SSL methods as been\napplied.", "AI": {"tldr": "\u4e0e\u6570\u636e\u589e\u5f3a\uff08DA\uff09\u548c\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u76f8\u6bd4\uff0c\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u6548\u7387\u6700\u4f4e\u3002\u7136\u800c\uff0c\u5f53\u4e0eDA\u548cSSL\u7ed3\u5408\u4f7f\u7528\u65f6\uff0cAL\u4ecd\u7136\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u56e0\u6b64\u5e94\u4f5c\u4e3a\u6700\u7ec8\u7684\u6027\u80fd\u4f18\u5316\u6b65\u9aa4\u3002", "motivation": "\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u5f88\u5c11\u5e94\u7528\u4e8e\u5176\u81ea\u8eab\u7684\u79d1\u5b66\u6587\u732e\u4e4b\u5916\u7684\u9886\u57df\uff0c\u539f\u56e0\u5728\u4e8e\u5176\u9ad8\u8ba1\u7b97\u6210\u672c\u4ee5\u53ca\u5728\u6807\u8bb0\u6837\u672c\u5f88\u5c11\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u7684\u76f8\u5bf9\u8f83\u5c0f\u7684\u63d0\u5347\u3002", "method": "\u7814\u7a76\u4e86\u6570\u636e\u589e\u5f3a\uff08DA\uff09\u3001\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u548c\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u8fd9\u51e0\u79cd\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u5f71\u54cd\u3002", "result": "AL\u662f\u89e3\u51b3\u5c11\u6837\u672c\u95ee\u9898\u7684\u6548\u7387\u6700\u4f4e\u7684\u65b9\u6cd5\uff0c\u4ec5\u6bd4\u968f\u673a\u62bd\u6837\u4ea7\u751f1-4%\u7684\u63d0\u5347\uff0c\u800cDA\u548cSSL\u65b9\u6cd5\u4e0e\u968f\u673a\u62bd\u6837\u7ed3\u5408\u4f7f\u7528\u53ef\u4ea7\u751f\u9ad8\u8fbe60%\u7684\u63d0\u5347\u3002\u7136\u800c\uff0c\u5f53AL\u4e0e\u5f3a\u5927\u7684DA\u548cSSL\u6280\u672f\u7ed3\u5408\u65f6\uff0c\u4ecd\u80fd\u63d0\u4f9b\u6539\u8fdb\u3002", "conclusion": "\u5728\u5e94\u7528\u4e86\u9002\u5f53\u7684\u6570\u636e\u589e\u5f3a\uff08DA\uff09\u548c\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u65b9\u6cd5\u540e\uff0c\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u4ecd\u7136\u53ef\u4ee5\u63d0\u4f9b\u6539\u8fdb\uff0c\u56e0\u6b64AL\u4e0d\u5e94\u88ab\u89c6\u4e3a\u89e3\u51b3\u7f3a\u5931\u6807\u7b7e\u7684\u65b9\u6cd5\uff0c\u800c\u5e94\u4f5c\u4e3a\u5728\u6570\u636e\u4e0a\u69a8\u53d6\u6700\u540e\u4e00\u4e1d\u6027\u80fd\u7684\u6700\u7ec8\u6784\u5efa\u5757\u3002"}}
{"id": "2508.00757", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00757", "abs": "https://arxiv.org/abs/2508.00757", "authors": ["Robin Armingaud", "Romaric Besan\u00e7on"], "title": "GLiDRE: Generalist Lightweight model for Document-level Relation Extraction", "comment": "Submitted to ARR July", "summary": "Relation Extraction (RE) is a fundamental task in Natural Language\nProcessing, and its document-level variant poses significant challenges, due to\nthe need to model complex interactions between entities across sentences.\nCurrent approaches, largely based on the ATLOP architecture, are commonly\nevaluated on benchmarks like DocRED and Re-DocRED. However, their performance\nin zero-shot or few-shot settings remains largely underexplored due to the\ntask's complexity. Recently, the GLiNER model has shown that a compact NER\nmodel can outperform much larger Large Language Models. With a similar\nmotivation, we introduce GLiDRE, a new model for document-level relation\nextraction that builds on the key ideas of GliNER. We benchmark GLiDRE against\nstate-of-the-art models across various data settings on the Re-DocRED dataset.\nOur results demonstrate that GLiDRE achieves state-of-the-art performance in\nfew-shot scenarios. Our code is publicly available.", "AI": {"tldr": "GLiDRE\u662f\u4e00\u4e2a\u5728\u6587\u6863\u7ea7\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff08\u5c24\u5176\u662f\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\uff09\u7684\u65b0\u6a21\u578b\uff0c\u5b83\u501f\u9274\u4e86GLiNER\u7684\u7d27\u51d1\u6a21\u578b\u601d\u60f3\u3002", "motivation": "\u4e0eGLiNER\u6a21\u578b\u6709\u76f8\u4f3c\u7684\u52a8\u673a\uff0c\u5373\u63a2\u7d22\u7d27\u51d1\u578b\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "GLiDRE\u662f\u4e00\u4e2a\u7528\u4e8e\u6587\u6863\u7ea7\u5173\u7cfb\u62bd\u53d6\u7684\u65b0\u6a21\u578b\uff0c\u5b83\u501f\u9274\u4e86GLiNER\u7684\u6838\u5fc3\u601d\u60f3\uff0c\u5e76\u5728Re-DocRED\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "GLiDRE\u5728Re-DocRED\u6570\u636e\u96c6\u7684\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "GLiDRE\u5728\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4ee3\u7801\u662f\u516c\u5f00\u7684\u3002"}}
{"id": "2508.00412", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00412", "abs": "https://arxiv.org/abs/2508.00412", "authors": ["Hanqi Chen", "Xu Zhang", "Xiaoliu Guan", "Lielin Jiang", "Guanzhong Wang", "Zeyu Chen", "Yi Liu"], "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model", "comment": null, "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.", "AI": {"tldr": "Sortblock\u901a\u8fc7\u667a\u80fd\u7f13\u5b58\u548c\u8df3\u8fc7\u8ba1\u7b97\u6765\u52a0\u901fDiT\uff0c\u901f\u5ea6\u52a0\u500d\u4e14\u8d28\u91cf\u635f\u5931\u5c0f\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u8bad\u7ec3\u52a0\u901f\u65b9\u6cd5\u901a\u5e38\u590d\u7528\u56fa\u5b9a\u65f6\u95f4\u6b65\u6216\u5c42\u7684\u4e2d\u95f4\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u8de8\u53bb\u566a\u9636\u6bb5\u548cTransformer\u5757\u7684\u8bed\u4e49\u7126\u70b9\u6f14\u53d8\uff0c\u5bfc\u81f4Diffusion Transformers\uff08DiT\uff09\u5b58\u5728\u9ad8\u63a8\u7406\u5ef6\u8fdf\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u65f6\u573a\u666f\u7684\u5e94\u7528\u3002", "method": "Sortblock\u662f\u4e00\u4e2a\u65e0\u8bad\u7ec3\u7684\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u6839\u636e\u76f8\u90bb\u65f6\u95f4\u6b65\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u52a8\u6001\u7f13\u5b58\u5757\u7ea7\u7279\u5f81\u3002\u5b83\u901a\u8fc7\u5bf9\u6b8b\u5dee\u7684\u6f14\u53d8\u8fdb\u884c\u6392\u5e8f\uff0c\u81ea\u9002\u5e94\u5730\u786e\u5b9a\u91cd\u8ba1\u7b97\u6bd4\u7387\uff0c\u9009\u62e9\u6027\u5730\u8df3\u8fc7\u5197\u4f59\u8ba1\u7b97\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u7ebf\u6027\u9884\u6d4b\u673a\u5236\u6765\u51cf\u5c11\u8df3\u8fc7\u7684\u5757\u4e2d\u7d2f\u79ef\u7684\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cSortblock\u5728\u5404\u79cd\u4efb\u52a1\u548cDiT\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc72\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u4e14\u8f93\u51fa\u8d28\u91cf\u4ec5\u6709\u5f88\u5c0f\u7684\u4e0b\u964d\u3002", "conclusion": "Sortblock\u901a\u8fc7\u52a8\u6001\u7f13\u5b58\u5757\u7ea7\u7279\u5f81\u5e76\u6839\u636e\u6b8b\u5dee\u6f14\u53d8\u60c5\u51b5\u81ea\u9002\u5e94\u5730\u786e\u5b9a\u91cd\u8ba1\u7b97\u6bd4\u7387\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc72\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u52a0\u901f\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00615", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00615", "abs": "https://arxiv.org/abs/2508.00615", "authors": ["Mukesh Kumar Sahu", "Pinki Roy"], "title": "Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data", "comment": null, "summary": "Accurately predicting the criticalness of ICU patients (such as in-ICU\nmortality risk) is vital for early intervention in critical care. However,\nconventional models often treat each patient in isolation and struggle to\nexploit the relational structure in Electronic Health Records (EHR). We propose\na Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds\na patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN\narchitecture that operates on this graph to predict patient mortality and a\ncontinuous criticalness score. SBSCGM uses a hybrid similarity measure\n(combining feature-based and structural similarities) to connect patients with\nanalogous clinical profiles in real-time. The HybridGraphMedGNN integrates\nGraph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)\nlayers to learn robust patient representations, leveraging both local and\nglobal graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III\ndataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)\noutperforming baseline classifiers and single-type GNN models. We also\ndemonstrate improved precision/recall and show that the attention mechanism\nprovides interpretable insights into model predictions. Our framework offers a\nscalable and interpretable solution for critical care risk prediction, with\npotential to support clinicians in real-world ICU deployment.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6765\u66f4\u51c6\u786e\u5730\u9884\u6d4bICU\u60a3\u8005\u7684\u6b7b\u4ea1\u98ce\u9669\u3002", "motivation": "\u4e3a\u4e86\u5728\u91cd\u75c7\u76d1\u62a4\u4e2d\u8fdb\u884c\u65e9\u671f\u5e72\u9884\uff0c\u51c6\u786e\u9884\u6d4b\u91cd\u75c7\u76d1\u62a4\u60a3\u8005\uff08\u4f8b\u5982ICU\u6b7b\u4ea1\u98ce\u9669\uff09\u7684\u5173\u952e\u6027\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u6a21\u578b\u901a\u5e38\u5c06\u6bcf\u4e2a\u60a3\u8005\u9694\u79bb\u5904\u7406\uff0c\u5e76\u4e14\u96be\u4ee5\u5229\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u7684\u5173\u7cfb\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f8\u4f3c\u6027\u56fe\u6a21\u578b\uff08SBSCGM\uff09\u548c\u6df7\u5408\u56fe\u6a21\u578b\uff08HybridGraphMedGNN\uff09\u6765\u52a8\u6001\u6784\u5efa\u60a3\u8005\u76f8\u4f3c\u6027\u56fe\u5e76\u9884\u6d4b\u60a3\u8005\u6b7b\u4ea1\u7387\u548c\u8fde\u7eed\u5371\u91cd\u7a0b\u5ea6\u5f97\u5206\u3002", "result": "\u5728 MIMIC-III \u6570\u636e\u96c6\u4e0a\u5bf9 6000 \u4f8b ICU \u4f4f\u9662\u75c5\u4f8b\u8fdb\u884c\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff08AUC-ROC 0.94\uff09\uff0c\u4f18\u4e8e\u57fa\u7ebf\u5206\u7c7b\u5668\u548c\u5355\u7c7b\u578b GNN \u6a21\u578b\u3002\u6211\u4eec\u8fd8\u8bc1\u660e\u4e86\u7cbe\u786e\u7387/\u53ec\u56de\u7387\u7684\u63d0\u9ad8\uff0c\u5e76\u8868\u660e\u6ce8\u610f\u529b\u673a\u5236\u4e3a\u6a21\u578b\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5371\u91cd\u76d1\u62a4\u7684\u98ce\u9669\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6f5c\u529b\u652f\u6301\u4e34\u5e8a\u533b\u751f\u5728\u5b9e\u9645ICU\u90e8\u7f72\u3002"}}
{"id": "2508.00760", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00760", "abs": "https://arxiv.org/abs/2508.00760", "authors": ["Qiyao Xue", "Yuchen Dou", "Ryan Shi", "Xiang Lorraine Li", "Wei Gao"], "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations", "comment": null, "summary": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches.", "AI": {"tldr": "MMBERT\u662f\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6587\u672c\u3001\u8bed\u97f3\u548c\u89c6\u89c9\u4fe1\u606f\uff0c\u5e76\u91c7\u7528\u521b\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u4e2d\u6587\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff08\u5305\u62ecLLM\uff09\u7684\u6210\u679c\u3002", "motivation": "\u4e2d\u6587\u793e\u4ea4\u7f51\u7edc\u4e0a\u7684\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u7279\u522b\u662f\u7531\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u65e8\u5728\u89c4\u907f\u4f20\u7edf\u57fa\u4e8e\u6587\u672c\u7684\u68c0\u6d4b\u7cfb\u7edf\u7684\u4f2a\u88c5\u6280\u672f\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6700\u8fd1\u63d0\u9ad8\u4e86\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u5927\u591a\u96c6\u4e2d\u5728\u82f1\u8bed\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u4e2d\u6587\u591a\u6a21\u6001\u7b56\u7565\u7684\u5173\u6ce8\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMMBERT\u7684\u65b0\u578b\u57fa\u4e8eBERT\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u6574\u5408\u4e86\u6587\u672c\u3001\u8bed\u97f3\u548c\u89c6\u89c9\u6a21\u6001\u3002\u4e3a\u4e86\u89e3\u51b3\u5c06MoE\u76f4\u63a5\u96c6\u6210\u5230\u57fa\u4e8eBERT\u7684\u6a21\u578b\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u4e09\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u3002MMBERT\u5305\u542b\u7279\u5b9a\u6a21\u6001\u7684\u4e13\u5bb6\u3001\u5171\u4eab\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u53ca\u57fa\u4e8e\u8def\u7531\u5668\u7684\u4e13\u5bb6\u5206\u914d\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u5bf9\u6297\u6027\u5e72\u6270\u7684\u9c81\u68d2\u6027\u3002", "result": "MMBERT\u5728\u591a\u4e2a\u4e2d\u6587\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MMBERT\u6a21\u578b\u5728\u591a\u4e2a\u4e2d\u6587\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eBERT\u7684\u7f16\u7801\u5668\u6a21\u578b\u3001\u5fae\u8c03\u7684LLM\u4ee5\u53ca\u4f7f\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u7684LLM\u3002"}}
{"id": "2508.00413", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00413", "abs": "https://arxiv.org/abs/2508.00413", "authors": ["Junyu Chen", "Dongyun Zou", "Wenkun He", "Junsong Chen", "Enze Xie", "Song Han", "Han Cai"], "title": "DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space", "comment": "ICCV 2025", "summary": "We present DC-AE 1.5, a new family of deep compression autoencoders for\nhigh-resolution diffusion models. Increasing the autoencoder's latent channel\nnumber is a highly effective approach for improving its reconstruction quality.\nHowever, it results in slow convergence for diffusion models, leading to poorer\ngeneration quality despite better reconstruction quality. This issue limits the\nquality upper bound of latent diffusion models and hinders the employment of\nautoencoders with higher spatial compression ratios. We introduce two key\ninnovations to address this challenge: i) Structured Latent Space, a\ntraining-based approach to impose a desired channel-wise structure on the\nlatent space with front latent channels capturing object structures and latter\nlatent channels capturing image details; ii) Augmented Diffusion Training, an\naugmented diffusion training strategy with additional diffusion training\nobjectives on object latent channels to accelerate convergence. With these\ntechniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling\nresults than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better\nimage generation quality than DC-AE-f32c32 while being 4x faster. Code:\nhttps://github.com/dc-ai-projects/DC-Gen.", "AI": {"tldr": "DC-AE 1.5 \u901a\u8fc7\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u548c\u589e\u5f3a\u6269\u6563\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u9ad8\u5206\u8fa8\u7387\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u548c\u8bad\u7ec3\u901f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u589e\u52a0\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u901a\u9053\u6570\u4ee5\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u4f46\u5bfc\u81f4\u6269\u6563\u6a21\u578b\u6536\u655b\u7f13\u6162\u3001\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u5728\u66f4\u9ad8\u7a7a\u95f4\u538b\u7f29\u6bd4\u4e0b\u6027\u80fd\u53d7\u9650\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DC-AE 1.5 \u7684\u6df1\u5ea6\u538b\u7f29\u81ea\u7f16\u7801\u5668\u65b0\u5bb6\u65cf\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u79cd\u521b\u65b0\u6280\u672f\uff1a1. \u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\uff1a\u901a\u8fc7\u8bad\u7ec3\u5c06\u6f5c\u5728\u7a7a\u95f4\u7ec4\u7ec7\u6210\u7279\u5b9a\u7ed3\u6784\uff0c\u4f7f\u524d\u7f6e\u901a\u9053\u6355\u83b7\u5bf9\u8c61\u7ed3\u6784\uff0c\u540e\u7f6e\u901a\u9053\u6355\u83b7\u56fe\u50cf\u7ec6\u8282\u30022. \u589e\u5f3a\u6269\u6563\u8bad\u7ec3\uff1a\u5728\u5bf9\u8c61\u6f5c\u5728\u901a\u9053\u4e0a\u589e\u52a0\u989d\u5916\u7684\u6269\u6563\u8bad\u7ec3\u76ee\u6807\uff0c\u4ee5\u52a0\u901f\u6536\u655b\u3002", "result": "DC-AE 1.5 \u5b9e\u73b0\u4e86\u6bd4 DC-AE \u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6269\u6563\u6a21\u578b\u6269\u5c55\u7ed3\u679c\u3002\u5728 ImageNet 512x512 \u6570\u636e\u96c6\u4e0a\uff0cDC-AE-1.5-f64c128 \u7684\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u4f18\u4e8e DC-AE-f32c32\uff0c\u5e76\u4e14\u8bad\u7ec3\u901f\u5ea6\u5feb 4 \u500d\u3002", "conclusion": "DC-AE 1.5 \u5bb6\u65cf\u901a\u8fc7\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u548c\u589e\u5f3a\u6269\u6563\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u6bd4 DC-AE \u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6269\u6563\u6a21\u578b\u6027\u80fd\uff0c\u5728 ImageNet 512x512 \u4e0a\uff0cDC-AE-1.5-f64c128 \u7684\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u4f18\u4e8e DC-AE-f32c32\uff0c\u4e14\u901f\u5ea6\u5feb 4 \u500d\u3002"}}
{"id": "2508.00627", "categories": ["cs.LG", "I.4.9; I.4.6"], "pdf": "https://arxiv.org/pdf/2508.00627", "abs": "https://arxiv.org/abs/2508.00627", "authors": ["Paul Tresson", "Pierre Le Coz", "Hadrien Tulet", "Anthony Malkassian", "Maxime R\u00e9jou M\u00e9chain"], "title": "IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources", "comment": "11 pages, 5 figures", "summary": "Remote sensing has entered a new era with the rapid development of artificial\nintelligence approaches. However, the implementation of deep learning has\nlargely remained restricted to specialists and has been impractical because it\noften requires (i) large reference datasets for model training and validation;\n(ii) substantial computing resources; and (iii) strong coding skills. Here, we\nintroduce IAMAP, a user-friendly QGIS plugin that addresses these three\nchallenges in an easy yet flexible way. IAMAP builds on recent advancements in\nself-supervised learning strategies, which now provide robust feature\nextractors, often referred to as foundation models. These generalist models can\noften be reliably used in few-shot or zero-shot scenarios (i.e., with little to\nno fine-tuning). IAMAP's interface allows users to streamline several key steps\nin remote sensing image analysis: (i) extracting image features using a wide\nrange of deep learning architectures; (ii) reducing dimensionality with\nbuilt-in algorithms; (iii) performing clustering on features or their reduced\nrepresentations; (iv) generating feature similarity maps; and (v) calibrating\nand validating supervised machine learning models for prediction. By enabling\nnon-AI specialists to leverage the high-quality features provided by recent\ndeep learning approaches without requiring GPU capacity or extensive reference\ndatasets, IAMAP contributes to the democratization of computationally efficient\nand energy-conscious deep learning methods.", "AI": {"tldr": "IAMAP \u662f\u4e00\u4e2a QGIS \u63d2\u4ef6\uff0c\u8ba9\u975e AI \u4e13\u5bb6\u4e5f\u80fd\u8f7b\u677e\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u9065\u611f\u56fe\u50cf\u5206\u6790\uff0c\u65e0\u9700\u5927\u91cf\u6570\u636e\u6216\u5f3a\u5927\u8ba1\u7b97\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u9065\u611f\u9886\u57df\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u4e13\u5bb6\u77e5\u8bc6\u3001\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u9ad8\u8ba1\u7b97\u8d44\u6e90\u7684\u95ee\u9898\uff0cIAMAP \u63d2\u4ef6\u88ab\u5f00\u53d1\u51fa\u6765\uff0c\u65e8\u5728\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\uff0c\u5b9e\u73b0\u9065\u611f\u56fe\u50cf\u5206\u6790\u7684\u6c11\u4e3b\u5316\u3002", "method": "IAMAP \u662f\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684 QGIS \u63d2\u4ef6\uff0c\u5b83\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u548c\u57fa\u7840\u6a21\u578b\u6765\u63d0\u53d6\u9065\u611f\u56fe\u50cf\u7279\u5f81\uff0c\u5e76\u63d0\u4f9b\u964d\u7ef4\u3001\u805a\u7c7b\u3001\u7279\u5f81\u76f8\u4f3c\u56fe\u751f\u6210\u4ee5\u53ca\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6821\u51c6\u4e0e\u9a8c\u8bc1\u7b49\u529f\u80fd\uff0c\u7528\u6237\u65e0\u9700 GPU \u6216\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5373\u53ef\u4f7f\u7528\u3002", "result": "IAMAP \u63d2\u4ef6\u4f7f\u7528\u6237\u80fd\u591f\u63d0\u53d6\u6df1\u5ea6\u5b66\u4e60\u7279\u5f81\u3001\u8fdb\u884c\u964d\u7ef4\u3001\u805a\u7c7b\u3001\u751f\u6210\u7279\u5f81\u76f8\u4f3c\u56fe\u5e76\u6821\u51c6\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u800c\u65e0\u9700 GPU \u6216\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4ece\u800c\u4f7f\u975e AI \u4e13\u5bb6\u4e5f\u80fd\u5229\u7528\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u3002", "conclusion": "IAMAP \u63d2\u4ef6\u901a\u8fc7\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u9065\u611f\u56fe\u50cf\u5206\u6790\u4e2d\u5bf9\u4e13\u4e1a\u77e5\u8bc6\u3001\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\uff0c\u4f7f\u5f97\u975e AI \u4e13\u5bb6\u4e5f\u80fd\u4f7f\u7528\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4fc3\u8fdb\u4e86\u8ba1\u7b97\u9ad8\u6548\u548c\u8282\u80fd\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u666e\u53ca\u3002"}}
{"id": "2508.00762", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00762", "abs": "https://arxiv.org/abs/2508.00762", "authors": ["Atakan Site", "Emre Hakan Erdemir", "G\u00fcl\u015fen Eryi\u011fit"], "title": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation", "comment": null, "summary": "This paper presents our system for SemEval-2025 Task 8: DataBench,\nQuestion-Answering over Tabular Data. The primary objective of this task is to\nperform question answering on given tabular datasets from diverse domains under\ntwo subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To\ntackle both subtasks, we developed a zero-shot solution with a particular\nemphasis on leveraging Large Language Model (LLM)-based code generation.\nSpecifically, we propose a Python code generation framework utilizing\nstate-of-the-art open-source LLMs to generate executable Pandas code via\noptimized prompting strategies. Our experiments reveal that different LLMs\nexhibit varying levels of effectiveness in Python code generation.\nAdditionally, results show that Python code generation achieves superior\nperformance in tabular question answering compared to alternative approaches.\nAlthough our ranking among zero-shot systems is unknown at the time of this\npaper's submission, our system achieved eighth place in Subtask I and sixth\nplace in Subtask~II among the 30 systems that outperformed the baseline in the\nopen-source models category.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u4ee3\u7801\u751f\u6210\u7684\u96f6\u6837\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u8868\u683c\u6570\u636e\u95ee\u7b54\u4efb\u52a1\uff0c\u5e76\u5728SemEval-2025 Task 8\u7684\u4e24\u4e2a\u5b50\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3SemEval-2025 Task 8\uff1a\u8868\u683c\u6570\u636e\u95ee\u7b54\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u6d89\u53ca\u5728\u4e0d\u540c\u9886\u57df\u7684\u8868\u683c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u95ee\u7b54\uff0c\u5305\u62ecDataBench QA\uff08Subtask I\uff09\u548cDataBench Lite QA\uff08Subtask II\uff09\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u4f18\u5316\u7684\u63d0\u793a\u7b56\u7565\u751f\u6210\u53ef\u6267\u884c\u7684Pandas\u4ee3\u7801\uff0c\u4ee5\u89e3\u51b3\u8868\u683c\u6570\u636e\u95ee\u7b54\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0d\u540c\u7684LLM\u5728Python\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u4e0e\u66ff\u4ee3\u65b9\u6cd5\u76f8\u6bd4\uff0cPython\u4ee3\u7801\u751f\u6210\u5728\u8868\u683c\u95ee\u7b54\u65b9\u9762\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u867d\u7136\u5728\u96f6\u6837\u672c\u7cfb\u7edf\u4e2d\u7684\u6392\u540d\u5c1a\u4e0d\u786e\u5b9a\uff0c\u4f46\u8be5\u7cfb\u7edf\u5728Subtask I\u4e2d\u6392\u540d\u7b2c\u516b\uff0c\u5728Subtask II\u4e2d\u6392\u540d\u7b2c\u516d\uff0c\u5728\u5f00\u6e90\u6a21\u578b\u7c7b\u522b\u4e2d\u8d85\u8fc7\u57fa\u7ebf\u768430\u4e2a\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.00418", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00418", "abs": "https://arxiv.org/abs/2508.00418", "authors": ["Sangwoo Youn", "Minji Lee", "Nokap Tony Park", "Yeonggyoo Jeon", "Taeyoung Na"], "title": "IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator", "comment": "ICIP 2025. Code: https://github.com/sang-w00/IN2OUT", "summary": "Video outpainting presents a unique challenge of extending the borders while\nmaintaining consistency with the given content. In this paper, we suggest the\nuse of video inpainting models that excel in object flow learning and\nreconstruction in outpainting rather than solely generating the background as\nin existing methods. However, directly applying or fine-tuning inpainting\nmodels to outpainting has shown to be ineffective, often leading to blurry\nresults. Our extensive experiments on discriminator designs reveal that a\ncritical component missing in the outpainting fine-tuning process is a\ndiscriminator capable of effectively assessing the perceptual quality of the\nextended areas. To tackle this limitation, we differentiate the objectives of\nadversarial training into global and local goals and introduce a hierarchical\ndiscriminator that meets both objectives. Additionally, we develop a\nspecialized outpainting loss function that leverages both local and global\nfeatures of the discriminator. Fine-tuning on this adversarial loss function\nenhances the generator's ability to produce both visually appealing and\nglobally coherent outpainted scenes. Our proposed method outperforms\nstate-of-the-art methods both quantitatively and qualitatively. Supplementary\nmaterials including the demo video and the code are available in SigPort.", "AI": {"tldr": "This paper improves video outpainting by using inpainting models and a new hierarchical discriminator with a specialized loss function, resulting in better quality than previous methods.", "motivation": "Existing video outpainting methods, which focus on background generation, are ineffective and often produce blurry results when extended to outpainting tasks. This paper addresses the limitation of directly applying or fine-tuning inpainting models by incorporating object flow learning and reconstruction capabilities, and by introducing a discriminator capable of assessing perceptual quality.", "method": "A hierarchical discriminator is introduced to address the lack of perceptual quality assessment in video outpainting fine-tuning. This discriminator differentiates adversarial training objectives into global and local goals. A specialized outpainting loss function is also developed, leveraging both local and global features from the discriminator.", "result": "The proposed method, utilizing a hierarchical discriminator and specialized outpainting loss, enhances the generator's ability to produce visually appealing and globally coherent outpainted scenes. It outperforms state-of-the-art methods quantitatively and qualitatively.", "conclusion": "Video outpainting is improved by using inpainting models with a focus on object flow and reconstruction. A hierarchical discriminator and specialized outpainting loss function enhance visual appeal and global coherence, outperforming existing methods."}}
{"id": "2508.00628", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00628", "abs": "https://arxiv.org/abs/2508.00628", "authors": ["Xiong Xiong", "Zhuo Zhang", "Rongchun Hu", "Chen Gao", "Zichen Deng"], "title": "Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs", "comment": null, "summary": "Solving high-frequency oscillatory partial differential equations (PDEs) is a\ncritical challenge in scientific computing, with applications in fluid\nmechanics, quantum mechanics, and electromagnetic wave propagation. Traditional\nphysics-informed neural networks (PINNs) suffer from spectral bias, limiting\ntheir ability to capture high-frequency solution components. We introduce\nSeparated-Variable Spectral Neural Networks (SV-SNN), a novel framework that\naddresses these limitations by integrating separation of variables with\nadaptive spectral methods. Our approach features three key innovations: (1)\ndecomposition of multivariate functions into univariate function products,\nenabling independent spatial and temporal networks; (2) adaptive Fourier\nspectral features with learnable frequency parameters for high-frequency\ncapture; and (3) theoretical framework based on singular value decomposition to\nquantify spectral bias. Comprehensive evaluation on benchmark problems\nincluding Heat equation, Helmholtz equation, Poisson equations and\nNavier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of\nmagnitude improvement in accuracy while reducing parameter count by over 90\\%\nand training time by 60\\%. These results establish SV-SNN as an effective\nsolution to the spectral bias problem in neural PDE solving. The implementation\nwill be made publicly available upon acceptance at\nhttps://github.com/xgxgnpu/SV-SNN.", "AI": {"tldr": "SV-SNN\u901a\u8fc7\u53d8\u91cf\u5206\u79bb\u548c\u81ea\u9002\u5e94\u8c31\u65b9\u6cd5\u514b\u670d\u4e86PINNs\u7684\u5149\u8c31\u504f\u5dee\u95ee\u9898\uff0c\u5728\u9ad8\u9891PDE\u6c42\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u5728\u9ad8\u9891\u6210\u5206\u6355\u83b7\u65b9\u9762\u5b58\u5728\u5149\u8c31\u504f\u5dee\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u6c42\u89e3\u9ad8\u9891\u632f\u8361\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "SV-SNN\u6846\u67b6\u901a\u8fc7\u4ee5\u4e0b\u521b\u65b0\u6765\u89e3\u51b3\u9ad8\u9891\u6210\u5206\u6355\u83b7\u95ee\u9898\uff1a1.\u5c06\u591a\u5143\u51fd\u6570\u5206\u89e3\u4e3a\u5355\u53d8\u91cf\u51fd\u6570\u4e58\u79ef\uff0c\u5b9e\u73b0\u72ec\u7acb\u65f6\u7a7a\u7f51\u7edc\uff1b2.\u5f15\u5165\u5177\u6709\u53ef\u5b66\u4e60\u9891\u7387\u53c2\u6570\u7684\u81ea\u9002\u5e94\u5085\u7acb\u53f6\u8c31\u7279\u5f81\u4ee5\u6355\u83b7\u9ad8\u9891\uff1b3.\u57fa\u4e8e\u5947\u5f02\u503c\u5206\u89e3\u7684\u7406\u8bba\u6846\u67b6\u91cf\u5316\u8c31\u504f\u5dee\u3002", "result": "\u5728\u70ed\u65b9\u7a0b\u3001\u4ea5\u59c6\u970d\u5179\u65b9\u7a0b\u3001\u6cca\u677e\u65b9\u7a0b\u548c\u7eb3\u7ef4-\u65af\u6258\u514b\u65af\u65b9\u7a0b\u7b49\u57fa\u51c6\u95ee\u9898\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cSV-SNN\u7684\u51c6\u786e\u6027\u63d0\u9ad8\u4e861-3\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u53c2\u6570\u6570\u91cf\u51cf\u5c11\u4e8690%\u4ee5\u4e0a\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e8660%\u3002", "conclusion": "SV-SNN\u901a\u8fc7\u5c06\u53d8\u91cf\u5206\u79bb\u4e0e\u81ea\u9002\u5e94\u8c31\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecfPDE\u6c42\u89e3\u4e2d\u7684\u8c31\u504f\u5dee\u95ee\u9898\u3002"}}
{"id": "2508.00788", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00788", "abs": "https://arxiv.org/abs/2508.00788", "authors": ["Xushuo Tang", "Yi Ding", "Zhengyi Yang", "Yin Chen", "Yongrui Gu", "Wenke Yang", "Mingchen Ju", "Xin Cao", "Yongfei Liu", "Wenjie Zhang"], "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.", "AI": {"tldr": "MISGENDERED+ \u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u867d\u7136 LLMs \u5728\u6027\u522b\u4e2d\u7acb\u4ee3\u8bcd\u65b9\u9762\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5728\u65b0\u9896\u4ee3\u8bcd\u548c\u53cd\u5411\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u6027\u522b\u4e2d\u7acb\u4ee3\u8bcd\u548c\u65b0\u9896\u4ee3\u8bcd\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4ee5\u89e3\u51b3\u8d1f\u8d23\u4efb\u7684\u4eba\u5de5\u667a\u80fd\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u6269\u5c55\u548c\u66f4\u65b0\u7684\u57fa\u51c6 MISGENDERED+ \u6765\u8bc4\u4f30\u4e94\u4e2a\u4ee3\u8868\u6027 LLMs\uff08GPT-4o\u3001Claude 4\u3001DeepSeek-V3\u3001Qwen Turbo \u548c Qwen2.5\uff09\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u6027\u522b\u8eab\u4efd\u63a8\u7406\u65b9\u9762\u7684\u4ee3\u8bcd\u4fdd\u771f\u5ea6\u3002", "result": "\u4e0e\u4e4b\u524d\u7684\u7814\u7a76\u76f8\u6bd4\uff0c\u5728\u5904\u7406\u4e8c\u5143\u4ee3\u8bcd\u548c\u6027\u522b\u4e2d\u7acb\u4ee3\u8bcd\u65b9\u9762\u6709\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002\u7136\u800c\uff0c\u5728\u65b0\u9896\u4ee3\u8bcd\u548c\u53cd\u5411\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\u4ecd\u7136\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u5c3d\u7ba1\u5728\u5904\u7406\u4e8c\u5143\u4ee3\u8bcd\u548c\u6027\u522b\u4e2d\u7acb\u4ee3\u8bcd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u65b0\u9896\u4ee3\u8bcd\u548c\u53cd\u5411\u63a8\u7406\u4efb\u52a1\u65b9\u9762\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u8bcd\u51c6\u786e\u6027\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9\u8868\u660e\u5728\u5904\u7406\u6d89\u53ca\u8eab\u4efd\u7684\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002"}}
{"id": "2508.00421", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00421", "abs": "https://arxiv.org/abs/2508.00421", "authors": ["Runmin Cong", "Zongji Yu", "Hao Fang", "Haoyan Sun", "Sam Kwong"], "title": "UIS-Mamba: Exploring Mamba for Underwater Instance Segmentation via Dynamic Tree Scan and Hidden State Weaken", "comment": "ACM MM 2025", "summary": "Underwater Instance Segmentation (UIS) tasks are crucial for underwater\ncomplex scene detection. Mamba, as an emerging state space model with\ninherently linear complexity and global receptive fields, is highly suitable\nfor processing image segmentation tasks with long sequence features. However,\ndue to the particularity of underwater scenes, there are many challenges in\napplying Mamba to UIS. The existing fixed-patch scanning mechanism cannot\nmaintain the internal continuity of scanned instances in the presence of\nseverely underwater color distortion and blurred instance boundaries, and the\nhidden state of the complex underwater background can also inhibit the\nunderstanding of instance objects. In this work, we propose the first\nMamba-based underwater instance segmentation model UIS-Mamba, and design two\ninnovative modules, Dynamic Tree Scan (DTS) and Hidden State Weaken (HSW), to\nmigrate Mamba to the underwater task. DTS module maintains the continuity of\nthe internal features of the instance objects by allowing the patches to\ndynamically offset and scale, thereby guiding the minimum spanning tree and\nproviding dynamic local receptive fields. HSW module suppresses the\ninterference of complex backgrounds and effectively focuses the information\nflow of state propagation to the instances themselves through the Ncut-based\nhidden state weakening mechanism. Experimental results show that UIS-Mamba\nachieves state-of-the-art performance on both UIIS and USIS10K datasets, while\nmaintaining a low number of parameters and computational complexity. Code is\navailable at https://github.com/Maricalce/UIS-Mamba.", "AI": {"tldr": "UIS-Mamba \u662f\u9996\u4e2a\u57fa\u4e8e Mamba \u7684\u6c34\u4e0b\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u6811\u626b\u63cf\uff08DTS\uff09\u548c\u9690\u85cf\u72b6\u6001\u524a\u5f31\uff08HSW\uff09\u6a21\u5757\u89e3\u51b3\u4e86\u6c34\u4e0b\u6210\u50cf\u7684\u6311\u6218\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u56fa\u5b9a\u5757\u626b\u63cf\u673a\u5236\u5728\u5904\u7406\u6c34\u4e0b\u8272\u5f69\u5931\u771f\u548c\u6a21\u7cca\u5b9e\u4f8b\u8fb9\u754c\u7b49\u95ee\u9898\u65f6\uff0c\u65e0\u6cd5\u4fdd\u6301\u626b\u63cf\u5b9e\u4f8b\u7684\u5185\u90e8\u8fde\u7eed\u6027\uff0c\u5e76\u4e14\u590d\u6742\u6c34\u4e0b\u80cc\u666f\u7684\u9690\u85cf\u72b6\u6001\u4f1a\u6291\u5236\u5bf9\u5b9e\u4f8b\u7684\u7406\u89e3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Mamba \u7684\u6c34\u4e0b\u5b9e\u4f8b\u5206\u5272\u6a21\u578b UIS-Mamba\uff0c\u5e76\u8bbe\u8ba1\u4e86\u52a8\u6001\u6811\u626b\u63cf\uff08DTS\uff09\u548c\u9690\u85cf\u72b6\u6001\u524a\u5f31\uff08HSW\uff09\u4e24\u4e2a\u521b\u65b0\u6a21\u5757\u3002DTS \u6a21\u5757\u901a\u8fc7\u5141\u8bb8\u5757\u52a8\u6001\u504f\u79fb\u548c\u7f29\u653e\u6765\u4fdd\u6301\u5b9e\u4f8b\u5185\u90e8\u7279\u5f81\u7684\u8fde\u7eed\u6027\uff0c\u5e76\u63d0\u4f9b\u52a8\u6001\u5c40\u90e8\u611f\u53d7\u91ce\u3002HSW \u6a21\u5757\u901a\u8fc7\u57fa\u4e8e Ncut \u7684\u9690\u85cf\u72b6\u6001\u524a\u5f31\u673a\u5236\u6765\u6291\u5236\u590d\u6742\u80cc\u666f\u7684\u5e72\u6270\uff0c\u5e76\u5c06\u72b6\u6001\u4f20\u64ad\u7684\u4fe1\u606f\u6d41\u6709\u6548\u5730\u96c6\u4e2d\u5230\u5b9e\u4f8b\u672c\u8eab\u3002", "result": "UIS-Mamba \u5728 UIIS \u548c USIS10K \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "UIS-Mamba \u5728 UIIS \u548c USIS10K \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u5c11\u7684\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2508.00635", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00635", "abs": "https://arxiv.org/abs/2508.00635", "authors": ["Changning Wu", "Gao Wu", "Rongyao Cai", "Yong Liu", "Kexin Zhang"], "title": "KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting", "comment": null, "summary": "Multi-scale decomposition architectures have emerged as predominant\nmethodologies in time series forecasting. However, real-world time series\nexhibit noise interference across different scales, while heterogeneous\ninformation distribution among frequency components at varying scales leads to\nsuboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks\n(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency\nSelection learning architecture (KFS) to address these challenges. This\nframework tackles prediction challenges stemming from cross-scale noise\ninterference and complex pattern modeling through its FreK module, which\nperforms energy-distribution-based dominant frequency selection in the spectral\ndomain. Simultaneously, KAN enables sophisticated pattern representation while\ntimestamp embedding alignment synchronizes temporal representations across\nscales. The feature mixing module then fuses scale-specific patterns with\naligned temporal features. Extensive experiments across multiple real-world\ntime series datasets demonstrate that KT achieves state-of-the-art performance\nas a simple yet effective architecture.", "AI": {"tldr": "The paper introduces KFS, a KAN-based architecture for time series forecasting that addresses noise and representation issues by adaptively selecting dominant frequencies and aligning temporal features across scales, achieving state-of-the-art results.", "motivation": "Real-world time series exhibit noise interference across different scales, and the heterogeneous information distribution among frequency components at varying scales leads to suboptimal multi-scale representation. This motivates the development of a new architecture to address these challenges.", "method": "This paper proposes a KAN based adaptive Frequency Selection learning architecture (KFS) to address challenges in multi-scale decomposition architectures for time series forecasting. The framework includes a FreK module for dominant frequency selection based on energy distribution and KAN for sophisticated pattern representation. Timestamp embedding alignment synchronizes temporal representations across scales, and a feature mixing module fuses scale-specific patterns with aligned temporal features.", "result": "The proposed KFS architecture, through its FreK module and KAN, effectively tackles prediction challenges arising from cross-scale noise interference and complex pattern modeling, leading to state-of-the-art performance.", "conclusion": "KT achieves state-of-the-art performance as a simple yet effective architecture across multiple real-world time series datasets."}}
{"id": "2508.00819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00819", "abs": "https://arxiv.org/abs/2508.00819", "authors": ["Jinsong Li", "Xiaoyi Dong", "Yuhang Zang", "Yuhang Cao", "Jiaqi Wang", "Dahua Lin"], "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models", "comment": "Code is available at https://github.com/Li-Jinsong/DAEDAL", "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.", "AI": {"tldr": "DAEDAL\u662f\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86DLLM\u4e2d\u56fa\u5b9a\u7684\u751f\u6210\u957f\u5ea6\u95ee\u9898\uff0c\u5141\u8bb8\u6a21\u578b\u52a8\u6001\u8c03\u6574\u957f\u5ea6\u4ee5\u9002\u5e94\u4efb\u52a1\uff0c\u4ece\u800c\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "DLLM\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u5230\u9700\u8981\u9884\u5b9a\u4e49\u751f\u6210\u957f\u5ea6\u7684\u67b6\u6784\u9650\u5236\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0e\u8ba1\u7b97\u5f00\u9500\u4e4b\u95f4\u7684\u6743\u8861\uff1a\u957f\u5ea6\u4e0d\u8db3\u4f1a\u5f71\u54cd\u590d\u6742\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u800c\u8fc7\u957f\u5219\u4f1a\u5e26\u6765\u663e\u8457\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u751a\u81f3\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "DAEDAL\u662f\u4e00\u79cd\u65b0\u9896\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u53bb\u566a\u7b56\u7565\uff0c\u5b83\u5229\u7528DLLM\u5185\u90e8\u4e0e\u7ed9\u5b9a\u4efb\u52a1\u6700\u4f73\u54cd\u5e94\u957f\u5ea6\u76f8\u5173\u7684\u6f5c\u5728\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u81ea\u9002\u5e94\u957f\u5ea6\u6269\u5c55\u3002\u8be5\u7b56\u7565\u5206\u4e24\u4e2a\u9636\u6bb5\u8fdb\u884c\uff1a1. \u5728\u53bb\u566a\u8fc7\u7a0b\u4e4b\u524d\uff0c\u4ece\u521d\u59cb\u77ed\u957f\u5ea6\u5f00\u59cb\uff0c\u901a\u8fc7\u5e8f\u5217\u5b8c\u6210\u5ea6\u91cf\u6307\u5bfc\uff0c\u8fed\u4ee3\u5730\u5c06\u5176\u6269\u5c55\u5230\u7c97\u7565\u7684\u3001\u9002\u5408\u4efb\u52a1\u7684\u957f\u5ea6\u30022. \u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u6269\u5c55\u4e0d\u8db3\u7684\u751f\u6210\u533a\u57df\uff08\u901a\u8fc7\u63d2\u5165\u63a9\u7801\u6807\u8bb0\uff09\u6765\u52a8\u6001\u5e72\u9884\uff0c\u786e\u4fdd\u6700\u7ec8\u8f93\u51fa\u5145\u5206\u5c55\u5f00\u3002", "result": "DAEDAL\u5728DLLM\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u5b9e\u73b0\u4e86\u4e0e\u7cbe\u5fc3\u8c03\u6574\u7684\u56fa\u5b9a\u957f\u5ea6\u57fa\u7ebf\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u5b9e\u73b0\u66f4\u9ad8\u7684\u6709\u6548\u6807\u8bb0\u6bd4\u6765\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "DAEDAL\u901a\u8fc7\u89e3\u51b3\u9759\u6001\u957f\u5ea6\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5fae\u8c03\u7684\u56fa\u5b9a\u957f\u5ea6\u57fa\u7ebf\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u63d0\u9ad8\u6709\u6548\u6807\u8bb0\u6bd4\u6765\u589e\u5f3a\u8ba1\u7b97\u6548\u7387\uff0c\u89e3\u9501\u4e86DLLM\u7684\u6f5c\u529b\uff0c\u5f25\u5408\u4e86\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u5e76\u4e3a\u66f4\u9ad8\u6548\u3001\u66f4\u5f3a\u5927\u7684\u751f\u6210\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.00427", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00427", "abs": "https://arxiv.org/abs/2508.00427", "authors": ["Seunggeun Chi", "Enna Sachdeva", "Pin-Hao Huang", "Kwonjoon Lee"], "title": "Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting", "comment": "ICCV 2025 (Highlight)", "summary": "Amodal completion, which is the process of inferring the full appearance of\nobjects despite partial occlusions, is crucial for understanding complex\nhuman-object interactions (HOI) in computer vision and robotics. Existing\nmethods, such as those that use pre-trained diffusion models, often struggle to\ngenerate plausible completions in dynamic scenarios because they have a limited\nunderstanding of HOI. To solve this problem, we've developed a new approach\nthat uses physical prior knowledge along with a specialized multi-regional\ninpainting technique designed for HOI. By incorporating physical constraints\nfrom human topology and contact information, we define two distinct regions:\nthe primary region, where occluded object parts are most likely to be, and the\nsecondary region, where occlusions are less probable. Our multi-regional\ninpainting method uses customized denoising strategies across these regions\nwithin a diffusion model. This improves the accuracy and realism of the\ngenerated completions in both their shape and visual detail. Our experimental\nresults show that our approach significantly outperforms existing methods in\nHOI scenarios, moving machine perception closer to a more human-like\nunderstanding of dynamic environments. We also show that our pipeline is robust\neven without ground-truth contact annotations, which broadens its applicability\nto tasks like 3D reconstruction and novel view/pose synthesis.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7406\u89e3\u4eba\u673a\u4ea4\u4e92\uff08HOI\uff09\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u548c\u591a\u533a\u57df\u4fee\u590d\u6280\u672f\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u5236\u5316\u53bb\u566a\u7b56\u7565\u63d0\u5347\u4e86\u7269\u4f53\u8865\u5168\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u611f\uff0c\u5e76\u5728HOI\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\uff09\u5728\u52a8\u6001\u573a\u666f\u4e2d\u751f\u6210\u5408\u7406\u8865\u5168\u7684\u80fd\u529b\u6709\u9650\uff0c\u56e0\u4e3a\u5b83\u4eec\u5bf9\u4eba\u673a\u4ea4\u4e92\uff08HOI\uff09\u7684\u7406\u89e3\u6709\u9650\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u65b9\u6cd5\u7ed3\u5408\u4e86\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\uff08\u5305\u62ec\u4eba\u4f53\u62d3\u6251\u548c\u63a5\u89e6\u4fe1\u606f\uff09\u4ee5\u53ca\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9HOI\u7684\u591a\u533a\u57df\u4fee\u590d\u6280\u672f\u3002\u901a\u8fc7\u5b9a\u4e49\u4e3b\u8981\u533a\u57df\uff08\u906e\u6321\u7269\u4f53\u6700\u53ef\u80fd\u51fa\u73b0\uff09\u548c\u6b21\u8981\u533a\u57df\uff08\u906e\u6321\u4e0d\u592a\u53ef\u80fd\u51fa\u73b0\uff09\uff0c\u5e76\u7ed3\u5408\u5b9a\u5236\u5316\u7684\u53bb\u566a\u7b56\u7565\uff0c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u8de8\u533a\u57df\u7684\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728HOI\u573a\u666f\u4e2d\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6d41\u7a0b\u5728\u6ca1\u6709\u771f\u5b9e\u63a5\u89e6\u6ce8\u91ca\u7684\u60c5\u51b5\u4e0b\u4e5f\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u8fd9\u62d3\u5bbd\u4e86\u5176\u57283D\u91cd\u5efa\u548c\u65b0\u89c6\u89d2/\u59ff\u6001\u5408\u6210\u7b49\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u524d\u666f\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u548c\u9488\u5bf9\u4eba\u673a\u4ea4\u4e92\uff08HOI\uff09\u7684\u591a\u533a\u57df\u4fee\u590d\u6280\u672f\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728HOI\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u611f\u77e5\u66f4\u63a5\u8fd1\u4e8e\u5bf9\u52a8\u6001\u73af\u5883\u7684\u7c7b\u4eba\u7406\u89e3\u3002"}}
{"id": "2508.00641", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00641", "abs": "https://arxiv.org/abs/2508.00641", "authors": ["Alessandro Palmas"], "title": "Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense", "comment": "11 pages, 10 figures", "summary": "The growing threat of low-cost kamikaze drone swarms poses a critical\nchallenge to modern defense systems demanding rapid and strategic\ndecision-making to prioritize interceptions across multiple effectors and\nhigh-value target zones. In this work, we present a case study demonstrating\nthe practical advantages of reinforcement learning in addressing this\nchallenge. We introduce a high-fidelity simulation environment that captures\nrealistic operational constraints, within which a decision-level reinforcement\nlearning agent learns to coordinate multiple effectors for optimal interception\nprioritization. Operating in a discrete action space, the agent selects which\ndrone to engage per effector based on observed state features such as\npositions, classes, and effector status. We evaluate the learned policy against\na handcrafted rule-based baseline across hundreds of simulated attack\nscenarios. The reinforcement learning based policy consistently achieves lower\naverage damage and higher defensive efficiency in protecting critical zones.\nThis case study highlights the potential of reinforcement learning as a\nstrategic layer within defense architectures, enhancing resilience without\ndisplacing existing control systems. All code and simulation assets are\npublicly released for full reproducibility, and a video demonstration\nillustrates the policy's qualitative behavior.", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u5728\u9632\u5fa1\u65e0\u4eba\u673a\u7fa4\u653b\u51fb\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u4f18\u5316\u62e6\u622a\u7b56\u7565\uff0c\u964d\u4f4e\u635f\u5931\u3002", "motivation": "\u4f4e\u6210\u672c\u795e\u98ce\u65e0\u4eba\u673a\u7fa4\u7684\u5a01\u80c1\u65e5\u76ca\u589e\u957f\uff0c\u5bf9\u73b0\u4ee3\u9632\u5fa1\u7cfb\u7edf\u63d0\u51fa\u4e86\u5feb\u901f\u3001\u6218\u7565\u51b3\u7b56\u7684\u4e25\u5cfb\u6311\u6218\uff0c\u9700\u8981\u5728\u591a\u4e2a\u62e6\u622a\u5668\u548c\u9ad8\u4ef7\u503c\u76ee\u6807\u533a\u57df\u4e4b\u95f4\u8fdb\u884c\u4f18\u5148\u6392\u5e8f\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5728\u4e00\u4e2a\u9ad8\u4fdd\u771f\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u4e00\u4e2a\u51b3\u7b56\u7ea7\u522b\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u3002\u8be5\u667a\u80fd\u4f53\u5728\u4e00\u4e2a\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u6839\u636e\u89c2\u5bdf\u5230\u7684\u72b6\u6001\u7279\u5f81\uff08\u5982\u4f4d\u7f6e\u3001\u7c7b\u522b\u548c\u6548\u5e94\u5668\u72b6\u6001\uff09\u4e3a\u6bcf\u4e2a\u6548\u5e94\u5668\u9009\u62e9\u8981\u4ea4\u6218\u7684\u65e0\u4eba\u673a\u3002", "result": "\u5728\u6570\u767e\u4e2a\u6a21\u62df\u653b\u51fb\u573a\u666f\u4e2d\uff0c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b56\u7565\u76f8\u6bd4\u4e8e\u624b\u5de5\u89c4\u5219\u57fa\u7ebf\uff0c\u80fd\u591f\u6301\u7eed\u5b9e\u73b0\u66f4\u4f4e\u7684\u5e73\u5747\u635f\u5bb3\u548c\u66f4\u9ad8\u7684\u9632\u5fa1\u6548\u7387\uff0c\u6709\u6548\u4fdd\u62a4\u4e86\u5173\u952e\u533a\u57df\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5e94\u5bf9\u4f4e\u6210\u672c\u795e\u98ce\u65e0\u4eba\u673a\u7fa4\u5a01\u80c1\u65b9\u9762\u7684\u5b9e\u9645\u4f18\u52bf\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u4eff\u771f\u73af\u5883\u8bad\u7ec3\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u80fd\u591f\u6709\u6548\u534f\u8c03\u591a\u4e2a\u62e6\u622a\u5668\uff0c\u5b9e\u73b0\u6700\u4f18\u62e6\u622a\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u5e73\u5747\u635f\u5bb3\u5e76\u63d0\u9ad8\u5173\u952e\u533a\u57df\u7684\u9632\u5fa1\u6548\u7387\u3002\u5f3a\u5316\u5b66\u4e60\u53ef\u4f5c\u4e3a\u9632\u5fa1\u67b6\u6784\u4e2d\u7684\u6218\u7565\u5c42\uff0c\u589e\u5f3a\u97e7\u6027\u4e14\u4e0d\u5e72\u6270\u73b0\u6709\u63a7\u5236\u7cfb\u7edf\u3002"}}
{"id": "2508.00643", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00643", "abs": "https://arxiv.org/abs/2508.00643", "authors": ["Albert Matveev", "Sanmitra Ghosh", "Aamal Hussain", "James-Michael Leahy", "Michalis Michaelides"], "title": "Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators", "comment": null, "summary": "Operator learning is a powerful paradigm for solving partial differential\nequations, with Fourier Neural Operators serving as a widely adopted\nfoundation. However, FNOs face significant scalability challenges due to\noverparameterization and offer no native uncertainty quantification -- a key\nrequirement for reliable scientific and engineering applications. Instead,\nneural operators rely on post hoc UQ methods that ignore geometric inductive\nbiases. In this work, we introduce DINOZAUR: a diffusion-based neural operator\nparametrization with uncertainty quantification. Inspired by the structure of\nthe heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a\ndimensionality-independent diffusion multiplier that has a single learnable\ntime parameter per channel, drastically reducing parameter count and memory\nfootprint without compromising predictive performance. By defining priors over\nthose time parameters, we cast DINOZAUR as a Bayesian neural operator to yield\nspatially correlated outputs and calibrated uncertainty estimates. Our method\nachieves competitive or superior performance across several PDE benchmarks\nwhile providing efficient uncertainty quantification.", "AI": {"tldr": "DINOZAUR\u662f\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7b97\u5b50\uff0c\u89e3\u51b3\u4e86FNO\u7684\u53ef\u6269\u5c55\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u8d44\u6e90\u6d88\u8017\u4f4e\u3002", "motivation": "FNOs\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff08\u8fc7\u5ea6\u53c2\u6570\u5316\uff09\uff0c\u5e76\u4e14\u4e0d\u80fd\u539f\u751f\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u800c\u8fd9\u662f\u79d1\u5b66\u548c\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u5173\u952e\u8981\u6c42\u3002", "method": "DINOZAUR\u7528\u4e00\u79cd\u4e0e\u7ef4\u5ea6\u65e0\u5173\u7684\u6269\u6563\u4e58\u6570\u53d6\u4ee3\u4e86FNO\u4e2d\u7684\u5bc6\u96c6\u5f20\u91cf\u4e58\u6570\uff0c\u8be5\u4e58\u6570\u6bcf\u4e2a\u901a\u9053\u53ea\u6709\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u65f6\u95f4\u53c2\u6570\u3002\u901a\u8fc7\u5b9a\u4e49\u65f6\u95f4\u53c2\u6570\u7684\u5148\u9a8c\uff0c\u5c06DINOZAUR\u4f5c\u4e3a\u8d1d\u53f6\u65af\u795e\u7ecf\u7b97\u5b50\u3002", "result": "DINOZAUR\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "conclusion": "DINOZAUR\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5177\u6709\u53ef\u5b66\u4e60\u65f6\u95f4\u53c2\u6570\u7684\u964d\u7ef4\u6269\u6563\u4e58\u6570\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u91cf\u548c\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u6d4b\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b9a\u4e49\u65f6\u95f4\u53c2\u6570\u7684\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u8d1d\u53f6\u65af\u795e\u7ecf\u7b97\u5b50\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u7a7a\u95f4\u76f8\u5173\u6027\u7684\u8f93\u51fa\u548c\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDINOZAUR\u7684\u6027\u80fd\u5177\u6709\u7ade\u4e89\u529b\u751a\u81f3\u66f4\u4f18\uff0c\u5e76\u80fd\u63d0\u4f9b\u9ad8\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002"}}
{"id": "2508.00442", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00442", "abs": "https://arxiv.org/abs/2508.00442", "authors": ["Jiale Zhou", "Wenhan Wang", "Shikun Li", "Xiaolei Qu", "Xin Guo", "Yizhong Liu", "Wenzhong Tang", "Xun Lin", "Yefeng Zheng"], "title": "TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation", "comment": null, "summary": "Tubular structure segmentation (TSS) is important for various applications,\nsuch as hemodynamic analysis and route navigation. Despite significant progress\nin TSS, domain shifts remain a major challenge, leading to performance\ndegradation in unseen target domains. Unlike other segmentation tasks, TSS is\nmore sensitive to domain shifts, as changes in topological structures can\ncompromise segmentation integrity, and variations in local features\ndistinguishing foreground from background (e.g., texture and contrast) may\nfurther disrupt topological continuity. To address these challenges, we propose\nTopology-enhanced Test-Time Adaptation (TopoTTA), the first test-time\nadaptation framework designed specifically for TSS. TopoTTA consists of two\nstages: Stage 1 adapts models to cross-domain topological discrepancies using\nthe proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance\ntopological representation without altering pre-trained parameters; Stage 2\nimproves topological continuity by a novel Topology Hard sample Generation\n(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels\nin the generated pseudo-break regions. Extensive experiments across four\nscenarios and ten datasets demonstrate TopoTTA's effectiveness in handling\ntopological distribution shifts, achieving an average improvement of 31.81% in\nclDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS\nmodels.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u7ba1\u9053\u7ed3\u6784\u5206\u5272\uff08TSS\uff09\u5728\u8de8\u57df\u5e94\u7528\u4e2d\u56e0\u57df\u504f\u79fb\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86TopoTTA\u3002\u8be5\u6846\u67b6\u901a\u8fc7TopoMDCs\u589e\u5f3a\u62d3\u6251\u8868\u793a\uff0c\u5e76\u901a\u8fc7TopoHG\u7b56\u7565\u548c\u4f2a\u6807\u7b7e\u5bf9\u9f50\u63d0\u9ad8\u62d3\u6251\u8fde\u7eed\u6027\u3002\u5b9e\u9a8c\u8bc1\u660eTopoTTA\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e73\u5747clDice\u63d0\u534731.81%\u3002", "motivation": "\u7ba1\u9053\u7ed3\u6784\u5206\u5272\uff08TSS\uff09\u5728\u8840\u6d41\u52a8\u529b\u5b66\u5206\u6790\u548c\u8def\u5f84\u5bfc\u822a\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u57df\u504f\u79fb\u662fTSS\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5b83\u4f1a\u5bfc\u81f4\u5728\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u57df\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u3002\u4e0e\u5176\u5b83\u5206\u5272\u4efb\u52a1\u76f8\u6bd4\uff0cTSS\u5bf9\u57df\u504f\u79fb\u66f4\u654f\u611f\uff0c\u56e0\u4e3a\u62d3\u6251\u7ed3\u6784\u7684\u53d8\u5316\u4f1a\u5f71\u54cd\u5206\u5272\u7684\u5b8c\u6574\u6027\uff0c\u800c\u533a\u5206\u524d\u666f\u548c\u80cc\u666f\u7684\u5c40\u90e8\u7279\u5f81\uff08\u5982\u7eb9\u7406\u548c\u5bf9\u6bd4\u5ea6\uff09\u7684\u53d8\u5316\u4f1a\u8fdb\u4e00\u6b65\u7834\u574f\u62d3\u6251\u8fde\u7eed\u6027\u3002", "method": "TopoTTA\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u63d0\u51fa\u7684\u62d3\u6251\u5143\u5dee\u5f02\u5377\u79ef\uff08TopoMDCs\uff09\u6765\u9002\u5e94\u8de8\u57df\u62d3\u6251\u5dee\u5f02\uff0c\u589e\u5f3a\u62d3\u6251\u8868\u793a\u800c\u4e0d\u6539\u53d8\u9884\u8bad\u7ec3\u53c2\u6570\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u65b0\u9896\u7684\u62d3\u6251\u56f0\u96be\u6837\u672c\u751f\u6210\uff08TopoHG\uff09\u7b56\u7565\u548c\u5728\u751f\u6210\u4f2a\u65ad\u88c2\u533a\u57df\u7684\u56f0\u96be\u6837\u672c\u4e0a\u8fdb\u884c\u4f2a\u6807\u7b7e\u9884\u6d4b\u5bf9\u9f50\uff0c\u6765\u63d0\u9ad8\u62d3\u6251\u8fde\u7eed\u6027\u3002", "result": "\u5728\u56db\u4e2a\u573a\u666f\u548c\u5341\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTopoTTA\u5728\u5904\u7406\u62d3\u6251\u5206\u5e03\u504f\u79fb\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0cclDice\u5e73\u5747\u63d0\u5347\u4e8631.81%\u3002TopoTTA\u8fd8\u53ef\u4ee5\u4f5c\u4e3a\u5373\u63d2\u5373\u7528TTA\u89e3\u51b3\u65b9\u6848\u5e94\u7528\u4e8e\u57fa\u4e8eCNN\u7684TSS\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u7ba1\u9053\u7ed3\u6784\u5206\u5272\uff08TSS\uff09\u7684\u6d4b\u8bd5\u65f6\u57df\u81ea\u9002\u5e94\uff08TTA\uff09\u6846\u67b6\u2014\u2014TopoTTA\uff0c\u4ee5\u89e3\u51b3\u8de8\u57df\u8fc1\u79fb\u4e2d\u5b58\u5728\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u62d3\u6251\u7ed3\u6784\u548c\u5c40\u90e8\u7279\u5f81\u65b9\u9762\u7684\u5f71\u54cd\u3002"}}
{"id": "2508.00657", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00657", "abs": "https://arxiv.org/abs/2508.00657", "authors": ["Sihang Zeng", "Lucas Jing Liu", "Jun Wen", "Meliha Yetisgen", "Ruth Etzioni", "Gang Luo"], "title": "TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction", "comment": "Accepted by MLHC 2025", "summary": "Trustworthy survival prediction is essential for clinical decision making.\nLongitudinal electronic health records (EHRs) provide a uniquely powerful\nopportunity for the prediction. However, it is challenging to accurately model\nthe continuous clinical progression of patients underlying the irregularly\nsampled clinical features and to transparently link the progression to survival\noutcomes. To address these challenges, we develop TrajSurv, a model that learns\ncontinuous latent trajectories from longitudinal EHR data for trustworthy\nsurvival prediction. TrajSurv employs a neural controlled differential equation\n(NCDE) to extract continuous-time latent states from the irregularly sampled\ndata, forming continuous latent trajectories. To ensure the latent trajectories\nreflect the clinical progression, TrajSurv aligns the latent state space with\npatient state space through a time-aware contrastive learning approach. To\ntransparently link clinical progression to the survival outcome, TrajSurv uses\nlatent trajectories in a two-step divide-and-conquer interpretation process.\nFirst, it explains how the changes in clinical features translate into the\nlatent trajectory's evolution using a learned vector field. Second, it clusters\nthese latent trajectories to identify key clinical progression patterns\nassociated with different survival outcomes. Evaluations on two real-world\nmedical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and\nsuperior transparency over existing deep learning methods.", "AI": {"tldr": "TrajSurv\u662f\u4e00\u4e2a\u5229\u7528\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u548c\u5bf9\u6bd4\u5b66\u4e60\u4ece\u7eb5\u5411EHR\u6570\u636e\u4e2d\u5b66\u4e60\u8fde\u7eed\u6f5c\u5728\u8f68\u8ff9\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u751f\u5b58\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4ece\u7eb5\u5411\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u4e2d\u51c6\u786e\u6a21\u62df\u60a3\u8005\u8fde\u7eed\u4e34\u5e8a\u8fdb\u5c55\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u900f\u660e\u5730\u5c06\u8be5\u8fdb\u5c55\u4e0e\u751f\u5b58\u7ed3\u679c\u8054\u7cfb\u8d77\u6765\u7684\u96be\u9898\uff0c\u63d0\u51fa\u4e86TrajSurv\u6a21\u578b\u3002", "method": "TrajSurv\u6a21\u578b\uff0c\u91c7\u7528\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\uff08NCDE\uff09\u4ece\u4e0d\u89c4\u5219\u91c7\u6837\u7684\u6570\u636e\u4e2d\u63d0\u53d6\u8fde\u7eed\u65f6\u95f4\u6f5c\u5728\u72b6\u6001\uff0c\u5f62\u6210\u8fde\u7eed\u6f5c\u5728\u8f68\u8ff9\u3002\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5c06\u6f5c\u5728\u72b6\u6001\u7a7a\u95f4\u4e0e\u60a3\u8005\u72b6\u6001\u7a7a\u95f4\u5bf9\u9f50\uff0c\u786e\u4fdd\u6f5c\u5728\u8f68\u8ff9\u53cd\u6620\u4e34\u5e8a\u8fdb\u5c55\u3002\u4e3a\u4e86\u900f\u660e\u5730\u5c06\u4e34\u5e8a\u8fdb\u5c55\u4e0e\u751f\u5b58\u7ed3\u679c\u8054\u7cfb\u8d77\u6765\uff0cTrajSurv\u91c7\u7528\u4e24\u6b65\u5206\u89e3\u548c\u89e3\u91ca\u8fc7\u7a0b\uff1a\u9996\u5148\uff0c\u5229\u7528\u5b66\u4e60\u5230\u7684\u5411\u91cf\u573a\u89e3\u91ca\u4e34\u5e8a\u7279\u5f81\u7684\u53d8\u5316\u5982\u4f55\u8f6c\u5316\u4e3a\u6f5c\u5728\u8f68\u8ff9\u7684\u6f14\u53d8\uff1b\u5176\u6b21\uff0c\u5bf9\u6f5c\u5728\u8f68\u8ff9\u8fdb\u884c\u805a\u7c7b\uff0c\u4ee5\u8bc6\u522b\u4e0e\u4e0d\u540c\u751f\u5b58\u7ed3\u679c\u76f8\u5173\u7684\u5173\u952e\u4e34\u5e8a\u8fdb\u5c55\u6a21\u5f0f\u3002", "result": "TrajSurv\u5728MIMIC-III\u548ceICU\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5177\u6709\u4e0e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5728\u900f\u660e\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "TrajSurv\u5728MIMIC-III\u548ceICU\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5177\u6709\u4e0e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5728\u900f\u660e\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2508.00443", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00443", "abs": "https://arxiv.org/abs/2508.00443", "authors": ["Longfei Huang", "Yu Liang", "Hao Zhang", "Jinwei Chen", "Wei Dong", "Lunde Chen", "Wanyu Liu", "Bo Li", "Pengtao Jiang"], "title": "SDMatte: Grafting Diffusion Models for Interactive Matting", "comment": "Accepted at ICCV 2025, 11 pages, 4 figures", "summary": "Recent interactive matting methods have shown satisfactory performance in\ncapturing the primary regions of objects, but they fall short in extracting\nfine-grained details in edge regions. Diffusion models trained on billions of\nimage-text pairs, demonstrate exceptional capability in modeling highly complex\ndata distributions and synthesizing realistic texture details, while exhibiting\nrobust text-driven interaction capabilities, making them an attractive solution\nfor interactive matting. To this end, we propose SDMatte, a diffusion-driven\ninteractive matting model, with three key contributions. First, we exploit the\npowerful priors of diffusion models and transform the text-driven interaction\ncapability into visual prompt-driven interaction capability to enable\ninteractive matting. Second, we integrate coordinate embeddings of visual\nprompts and opacity embeddings of target objects into U-Net, enhancing\nSDMatte's sensitivity to spatial position information and opacity information.\nThird, we propose a masked self-attention mechanism that enables the model to\nfocus on areas specified by visual prompts, leading to better performance.\nExtensive experiments on multiple datasets demonstrate the superior performance\nof our method, validating its effectiveness in interactive matting. Our code\nand model are available at https://github.com/vivoCameraResearch/SDMatte.", "AI": {"tldr": "SDMatte\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u4ea4\u4e92\u5f0f\u62a0\u56fe\u6a21\u578b\uff0c\u5b83\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u9a71\u52a8\u4ea4\u4e92\uff0c\u5e76\u7ed3\u5408\u5750\u6807\u5d4c\u5165\u548c\u63a9\u7801\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u8fb9\u7f18\u533a\u57df\u7ec6\u8282\u63d0\u53d6\u65b9\u9762\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6700\u8fd1\u7684\u4ea4\u4e92\u5f0f\u62a0\u56fe\u65b9\u6cd5\u867d\u7136\u5728\u6355\u6349\u7269\u4f53\u4e3b\u8981\u533a\u57df\u65b9\u9762\u8868\u73b0\u4ee4\u4eba\u6ee1\u610f\uff0c\u4f46\u5728\u63d0\u53d6\u8fb9\u7f18\u533a\u57df\u7684\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u800c\u7ecf\u8fc7\u6d77\u91cf\u56fe\u50cf-\u6587\u672c\u5bf9\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u5728\u5efa\u6a21\u590d\u6742\u6570\u636e\u5206\u5e03\u3001\u5408\u6210\u903c\u771f\u7eb9\u7406\u7ec6\u8282\u4ee5\u53ca\u6587\u672c\u9a71\u52a8\u4ea4\u4e92\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f7f\u5176\u6210\u4e3a\u4ea4\u4e92\u5f0f\u62a0\u56fe\u7684\u4e00\u4e2a\u6709\u5438\u5f15\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "SDMatte\u6a21\u578b\uff0c\u4e00\u79cd\u7531\u6269\u6563\u6a21\u578b\u9a71\u52a8\u7684\u4ea4\u4e92\u5f0f\u62a0\u56fe\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u5148\u9a8c\u77e5\u8bc6\uff0c\u5c06\u6587\u672c\u9a71\u52a8\u7684\u4ea4\u4e92\u80fd\u529b\u8f6c\u5316\u4e3a\u89c6\u89c9\u63d0\u793a\u9a71\u52a8\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u4ee5\u4e0b\u521b\u65b0\uff1a1. \u6574\u5408\u4e86\u89c6\u89c9\u63d0\u793a\u7684\u5750\u6807\u5d4c\u5165\u548c\u76ee\u6807\u5bf9\u8c61\u7684\u900f\u660e\u5ea6\u5d4c\u5165\u5230U-Net\u4e2d\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u7a7a\u95f4\u4f4d\u7f6e\u548c\u900f\u660e\u5ea6\u4fe1\u606f\u7684\u654f\u611f\u5ea6\u30022. \u63d0\u51fa\u4e86\u4e00\u79cd\u63a9\u7801\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4e13\u6ce8\u4e8e\u7531\u89c6\u89c9\u63d0\u793a\u6307\u5b9a\u7684\u533a\u57df\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86SDMatte\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4ea4\u4e92\u5f0f\u62a0\u56fe\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "SDMatte\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u4ea4\u4e92\u5f0f\u62a0\u56fe\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2508.00664", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00664", "abs": "https://arxiv.org/abs/2508.00664", "authors": ["Jialun Zheng", "Jie Liu", "Jiannong Cao", "Xiao Wang", "Hanchen Yang", "Yankai Chen", "Philip S. Yu"], "title": "DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes", "comment": null, "summary": "Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies\nin evolving graphs across domains such as finance, traffic, and social\nnetworks. Recently, generalist graph anomaly detection (GAD) models have shown\npromising results. They are pretrained on multiple source datasets and\ngeneralize across domains. While effective on static graphs, they struggle to\ncapture evolving anomalies in dynamic graphs. Moreover, the continuous\nemergence of new domains and the lack of labeled data further challenge\ngeneralist DGAD. Effective cross-domain DGAD requires both domain-specific and\ndomain-agnostic anomalous patterns. Importantly, these patterns evolve\ntemporally within and across domains. Building on these insights, we propose a\nDGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and\ndomain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,\nevolving representations of normal and anomalous patterns, from temporal\nego-graphs and stores them in a memory buffer. The buffer is selectively\nupdated to retain general, domain-agnostic patterns while incorporating new\ndomain-specific ones. Then, an anomaly scorer compares incoming data with\ndynamic prototypes to flag both general and domain-specific anomalies. Finally,\nDP-DGAD employs confidence-based pseudo-labeling for effective self-supervised\nadaptation in target domains. Extensive experiments demonstrate\nstate-of-the-art performance across ten real-world datasets from different\ndomains.", "AI": {"tldr": "DP-DGAD\u662f\u4e00\u79cd\u7528\u4e8e\u52a8\u6001\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u539f\u578b\u6355\u6349\u6f14\u5316\u7684\u57df\u7279\u5b9a\u548c\u57df\u4e0d\u53ef\u77e5\u6a21\u5f0f\uff0c\u5e76\u5728\u8de8\u9886\u57df\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u901a\u7528\u56fe\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u5728\u52a8\u6001\u56fe\u548c\u65b0\u9886\u57df\u7f3a\u4e4f\u6807\u7b7e\u6570\u636e\u65b9\u9762\u7684\u6311\u6218\uff0c\u540c\u65f6\u6355\u6349\u6f14\u5316\u4e2d\u7684\u57df\u7279\u5b9a\u548c\u57df\u4e0d\u53ef\u77e5\u5f02\u5e38\u6a21\u5f0f\u3002", "method": "DP-DGAD\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u53d6\u52a8\u6001\u539f\u578b\uff08\u6f14\u5316\u7684\u6b63\u5e38\u548c\u5f02\u5e38\u6a21\u5f0f\u8868\u793a\uff09\u5e76\u5b58\u50a8\u5728\u5185\u5b58\u7f13\u51b2\u533a\u4e2d\uff0c\u7136\u540e\u4f7f\u7528\u5f02\u5e38\u8bc4\u5206\u5668\u5c06\u8f93\u5165\u6570\u636e\u4e0e\u52a8\u6001\u539f\u578b\u8fdb\u884c\u6bd4\u8f83\u6765\u6807\u8bb0\u5f02\u5e38\u3002\u6700\u540e\uff0c\u91c7\u7528\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u4f2a\u6807\u7b7e\u8fdb\u884c\u81ea\u76d1\u7763\u9002\u5e94\u3002", "result": "DP-DGAD\u5728\u5341\u4e2a\u8de8\u4e0d\u540c\u9886\u57df\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DP-DGAD\u901a\u8fc7\u52a8\u6001\u539f\u578b\u5728\u8de810\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6709\u6548\u5730\u6355\u6349\u4e86\u52a8\u6001\u56fe\u4e2d\u7684\u6f14\u5316\u4e2d\u7684\u57df\u7279\u5b9a\u548c\u57df\u4e0d\u53ef\u77e5\u5f02\u5e38\u6a21\u5f0f\u3002"}}
{"id": "2508.00445", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00445", "abs": "https://arxiv.org/abs/2508.00445", "authors": ["Hongyi Cai", "Mohammad Mahdinur Rahman", "Mingkang Dong", "Jie Li", "Muxin Pu", "Zhili Fang", "Yinan Peng", "Hanjun Luo", "Yang Liu"], "title": "AutoDebias: Automated Framework for Debiasing Text-to-Image Models", "comment": null, "summary": "Text-to-Image (T2I) models generate high-quality images from text prompts but\noften exhibit unintended social biases, such as gender or racial stereotypes,\neven when these attributes are not mentioned. Existing debiasing methods work\nwell for simple or well-known cases but struggle with subtle or overlapping\nbiases. We propose AutoDebias, a framework that automatically identifies and\nmitigates harmful biases in T2I models without prior knowledge of specific bias\ntypes. Specifically, AutoDebias leverages vision-language models to detect\nbiased visual patterns and constructs fairness guides by generating inclusive\nalternative prompts that reflect balanced representations. These guides drive a\nCLIP-guided training process that promotes fairer outputs while preserving the\noriginal model's image quality and diversity. Unlike existing methods,\nAutoDebias effectively addresses both subtle stereotypes and multiple\ninteracting biases. We evaluate the framework on a benchmark covering over 25\nbias scenarios, including challenging cases where multiple biases occur\nsimultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and\nreduces biased outputs from 90% to negligible levels, while preserving the\nvisual fidelity of the original model.", "AI": {"tldr": "AutoDebias\u6846\u67b6\u901a\u8fc7\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548cCLIP\u5f15\u5bfc\u7684\u8bad\u7ec3\uff0c\u81ea\u52a8\u8bc6\u522b\u548c\u51cf\u8f7b\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u5373\u4f7f\u662f\u5fae\u5999\u548c\u91cd\u53e0\u7684\u504f\u89c1\u4e5f\u80fd\u6709\u6548\u5904\u7406\uff0c\u5e76\u4e14\u4e0d\u4f1a\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7ecf\u5e38\u4f1a\u4ea7\u751f\u4e0e\u793e\u4f1a\u504f\u89c1\u76f8\u5173\u7684\u523b\u677f\u5370\u8c61\uff0c\u5373\u4f7f\u5728\u63d0\u793a\u4e2d\u6ca1\u6709\u63d0\u53ca\u8fd9\u4e9b\u5c5e\u6027\u3002\u73b0\u6709\u7684\u504f\u89c1\u6d88\u9664\u65b9\u6cd5\u5728\u5904\u7406\u5fae\u5999\u6216\u91cd\u53e0\u504f\u89c1\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u548c\u51cf\u8f7b\u8fd9\u4e9b\u504f\u89c1\u7684\u65b0\u65b9\u6cd5\u3002", "method": "AutoDebias\u6846\u67b6\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u504f\u89c1\u89c6\u89c9\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u5305\u5bb9\u6027\u66ff\u4ee3\u63d0\u793a\u6765\u6784\u5efa\u516c\u5e73\u6027\u6307\u5357\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u516c\u5e73\u7684\u8f93\u51fa\uff0c\u540c\u65f6\u901a\u8fc7CLIP\u5f15\u5bfc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u6765\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "result": "AutoDebias\u5728\u5305\u542b25\u4e2a\u4ee5\u4e0a\u504f\u89c1\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u68c0\u6d4b\u6709\u5bb3\u6a21\u5f0f\u7684\u51c6\u786e\u7387\u4e3a91.6%\uff0c\u5e76\u5c06\u6709\u504f\u89c1\u7684\u8f93\u51fa\u4ece90%\u964d\u4f4e\u5230\u53ef\u5ffd\u7565\u7684\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u6a21\u578b\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "AutoDebias\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u548c\u51cf\u8f7b\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6709\u5bb3\u504f\u89c1\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u5fae\u5999\u6216\u91cd\u53e0\u504f\u89c1\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u5904\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2508.00447", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00447", "abs": "https://arxiv.org/abs/2508.00447", "authors": ["Anju Rani", "Daniel Ortiz-Arroyo", "Petar Durdevic"], "title": "CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text", "comment": "11 pages, 8 figures", "summary": "Understanding the temporal dynamics of biological growth is critical across\ndiverse fields such as microbiology, agriculture, and biodegradation research.\nAlthough vision-language models like Contrastive Language Image Pretraining\n(CLIP) have shown strong capabilities in joint visual-textual reasoning, their\neffectiveness in capturing temporal progression remains limited. To address\nthis, we propose CLIPTime, a multimodal, multitask framework designed to\npredict both the developmental stage and the corresponding timestamp of fungal\ngrowth from image and text inputs. Built upon the CLIP architecture, our model\nlearns joint visual-textual embeddings and enables time-aware inference without\nrequiring explicit temporal input during testing. To facilitate training and\nevaluation, we introduce a synthetic fungal growth dataset annotated with\naligned timestamps and categorical stage labels. CLIPTime jointly performs\nclassification and regression, predicting discrete growth stages alongside\ncontinuous timestamps. We also propose custom evaluation metrics, including\ntemporal accuracy and regression error, to assess the precision of time-aware\npredictions. Experimental results demonstrate that CLIPTime effectively models\nbiological progression and produces interpretable, temporally grounded outputs,\nhighlighting the potential of vision-language models in real-world biological\nmonitoring applications.", "AI": {"tldr": "CLIPTime \u662f\u4e00\u4e2a\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u4fe1\u606f\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u771f\u83cc\u751f\u957f\u7684\u9636\u6bb5\u548c\u65f6\u95f4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u751f\u7269\u751f\u957f\u65f6\u95f4\u52a8\u6001\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u6355\u6349\u751f\u7269\u751f\u957f\u7684\u65f6\u95f4\u52a8\u6001\u65b9\u9762\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u63d0\u5347\u6a21\u578b\u5728\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u751f\u7269\u5b66\u6570\u636e\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a CLIPTime \u7684\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e CLIP \u67b6\u6784\uff0c\u80fd\u591f\u4ece\u56fe\u50cf\u548c\u6587\u672c\u8f93\u5165\u9884\u6d4b\u771f\u83cc\u751f\u957f\u7684\u53d1\u80b2\u9636\u6bb5\u548c\u76f8\u5e94\u7684\u65f6\u95f4\u6233\u3002CLIPTime \u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u91c7\u7528\u4e86\u5408\u6210\u7684\u771f\u83cc\u751f\u957f\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u81ea\u5b9a\u4e49\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5982\u65f6\u95f4\u51c6\u786e\u6027\u548c\u56de\u5f52\u8bef\u5dee\uff09\u6765\u8bc4\u4f30\u65f6\u95f4\u611f\u77e5\u9884\u6d4b\u7684\u7cbe\u5ea6\u3002", "result": "CLIPTime \u5728\u9884\u6d4b\u79bb\u6563\u751f\u957f\u9636\u6bb5\u548c\u8fde\u7eed\u65f6\u95f4\u6233\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6a21\u578b\u80fd\u6709\u6548\u6a21\u62df\u751f\u7269\u8fdb\u7a0b\u3002", "conclusion": "CLIPTime \u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u6a21\u62df\u751f\u7269\u751f\u957f\u8fc7\u7a0b\uff0c\u5e76\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u3001\u4e0e\u65f6\u95f4\u76f8\u5173\u7684\u8f93\u51fa\uff0c\u5c55\u793a\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u751f\u7269\u76d1\u6d4b\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00695", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00695", "abs": "https://arxiv.org/abs/2508.00695", "authors": ["Sergio Rubio-Mart\u00edn", "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s", "Antonio Serrano-Garc\u00eda", "Clara Margarita Franch-Pato", "Arturo Crespo-\u00c1lvaro", "Jos\u00e9 Alberto Ben\u00edtez-Andrades"], "title": "Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach", "comment": null, "summary": "The classification of clinical notes into specific diagnostic categories is\ncritical in healthcare, especially for mental health conditions like Anxiety\nand Adjustment Disorder. In this study, we compare the performance of various\nArtificial Intelligence models, including both traditional Machine Learning\napproaches (Random Forest, Support Vector Machine, K-nearest neighbors,\nDecision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT\nand SciBERT), to classify clinical notes into these two diagnoses.\nAdditionally, we implemented three oversampling strategies: No Oversampling,\nRandom Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to\nassess their impact on model performance. Hyperparameter tuning was also\napplied to optimize model accuracy. Our results indicate that oversampling\ntechniques had minimal impact on model performance overall. The only exception\nwas SMOTE, which showed a positive effect specifically with BERT-based models.\nHowever, hyperparameter optimization significantly improved accuracy across the\nmodels, enhancing their ability to generalize and perform on the dataset. The\nDecision Tree and eXtreme Gradient Boost models achieved the highest accuracy\namong machine learning approaches, both reaching 96%, while the DistilBERT and\nSciBERT models also attained 96% accuracy in the deep learning category. These\nfindings underscore the importance of hyperparameter tuning in maximizing model\nperformance. This study contributes to the ongoing research on AI-assisted\ndiagnostic tools in mental health by providing insights into the efficacy of\ndifferent model architectures and data balancing methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5728\u7cbe\u795e\u5065\u5eb7\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u8d85\u53c2\u6570\u8c03\u6574\u6bd4\u8fc7\u91c7\u6837\u6280\u672f\u66f4\u80fd\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u3002\u51b3\u7b56\u6811\u3001\u6781\u9650\u68af\u5ea6\u63d0\u5347\u3001DistilBERT \u548c SciBERT \u6a21\u578b\u5747\u8fbe\u5230 96% \u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4e34\u5e8a\u7b14\u8bb0\u5206\u7c7b\u5bf9\u4e8e\u533b\u7597\u4fdd\u5065\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7126\u8651\u548c\u9002\u5e94\u969c\u788d\u7b49\u5fc3\u7406\u5065\u5eb7\u72b6\u51b5\u3002\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u5404\u79cd\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5728\u5bf9\u4e34\u5e8a\u7b14\u8bb0\u8fdb\u884c\u5206\u7c7b\u65f6\u7684\u6027\u80fd\uff0c\u5e76\u8bc4\u4f30\u8fc7\u91c7\u6837\u7b56\u7565\u548c\u8d85\u53c2\u6570\u8c03\u6574\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u968f\u673a\u68ee\u6797\u3001\u652f\u6301\u5411\u91cf\u673a\u3001K\u8fd1\u90bb\u3001\u51b3\u7b56\u6811\u548c\u6781\u9650\u68af\u5ea6\u63d0\u5347\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08DistilBERT \u548c SciBERT\uff09\u5728\u5c06\u4e34\u5e8a\u7b14\u8bb0\u5206\u7c7b\u5230\u7126\u8651\u548c\u9002\u5e94\u969c\u788d\u8bca\u65ad\u7c7b\u522b\u4e2d\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u5b9e\u65bd\u4e86\u4e09\u79cd\u8fc7\u91c7\u6837\u7b56\u7565\uff08\u65e0\u8fc7\u91c7\u6837\u3001\u968f\u673a\u8fc7\u91c7\u6837\u548c\u5408\u6210\u5c11\u6570\u8fc7\u91c7\u6837\u6280\u672f\uff08SMOTE\uff09\uff09\u6765\u8bc4\u4f30\u5b83\u4eec\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u884c\u4e86\u8d85\u53c2\u6570\u8c03\u6574\u4ee5\u4f18\u5316\u6a21\u578b\u51c6\u786e\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8fc7\u91c7\u6837\u6280\u672f\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u5f88\u5c0f\uff0c\u53ea\u6709 SMOTE \u5bf9 BERT \u7c7b\u6a21\u578b\u6709\u79ef\u6781\u5f71\u54cd\u3002\u7136\u800c\uff0c\u8d85\u53c2\u6570\u4f18\u5316\u663e\u8457\u63d0\u9ad8\u4e86\u6240\u6709\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u51b3\u7b56\u6811\u548c\u6781\u9650\u68af\u5ea6\u63d0\u5347\u6a21\u578b\u7684\u51c6\u786e\u6027\u6700\u9ad8\uff0c\u8fbe\u5230 96%\uff0cDistilBERT \u548c SciBERT \u6a21\u578b\u4e5f\u8fbe\u5230\u4e86 96% \u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8d85\u53c2\u6570\u8c03\u6574\u5bf9\u4e8e\u6700\u5927\u5316\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4e0e\u6570\u636e\u5e73\u8861\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u66f4\u5927\u3002"}}
{"id": "2508.00453", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00453", "abs": "https://arxiv.org/abs/2508.00453", "authors": ["Baisong Li", "Xingwang Wang", "Haixiao Xu"], "title": "PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA", "comment": null, "summary": "The goal of multispectral and hyperspectral image fusion (MHIF) is to\ngenerate high-quality images that simultaneously possess rich spectral\ninformation and fine spatial details. However, due to the inherent trade-off\nbetween spectral and spatial information and the limited availability of\nobservations, this task is fundamentally ill-posed. Previous studies have not\neffectively addressed the ill-posed nature caused by data misalignment. To\ntackle this challenge, we propose a fusion framework named PIF-Net, which\nexplicitly incorporates ill-posed priors to effectively fuse multispectral\nimages and hyperspectral images. To balance global spectral modeling with\ncomputational efficiency, we design a method based on an invertible Mamba\narchitecture that maintains information consistency during feature\ntransformation and fusion, ensuring stable gradient flow and process\nreversibility. Furthermore, we introduce a novel fusion module called the\nFusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral\nand spatial features while keeping the model lightweight. Extensive experiments\non multiple benchmark datasets demonstrate that PIF-Net achieves significantly\nbetter image restoration performance than current state-of-the-art methods\nwhile maintaining model efficiency.", "AI": {"tldr": "\u7531\u4e8e\u6570\u636e\u9519\u4f4d\u5bfc\u81f4\u7684\u75c5\u6001\u95ee\u9898\uff0c\u591a\u5149\u8c31\u548c\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\uff08MHIF\uff09\u4efb\u52a1\u5f88\u56f0\u96be\u3002\u6211\u4eec\u63d0\u51fa\u4e86PIF-Net\uff0c\u4e00\u4e2a\u7ed3\u5408\u4e86\u75c5\u6001\u5148\u9a8c\u548c\u53ef\u9006Mamba\u67b6\u6784\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u3002\u901a\u8fc7\u611f\u77e5\u4f4e\u79e9\u9002\u5e94\u6a21\u5757\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u52a8\u6001\u6821\u51c6\u7279\u5f81\u5e76\u4fdd\u6301\u8f7b\u91cf\u5316\u3002\u5b9e\u9a8c\u8bc1\u660ePIF-Net\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u5149\u8c31\u548c\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\uff08MHIF\uff09\u7684\u76ee\u7684\u662f\u751f\u6210\u540c\u65f6\u5177\u6709\u4e30\u5bcc\u7684\u5149\u8c31\u4fe1\u606f\u548c\u7cbe\u7ec6\u7a7a\u95f4\u7ec6\u8282\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5149\u8c31\u548c\u7a7a\u95f4\u4fe1\u606f\u4e4b\u95f4\u56fa\u6709\u7684\u6743\u8861\u4ee5\u53ca\u89c2\u6d4b\u6570\u636e\u7684\u6709\u9650\u6027\uff0c\u8fd9\u9879\u4efb\u52a1\u672c\u8d28\u4e0a\u662f\u75c5\u6001\u7684\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u672a\u80fd\u6709\u6548\u89e3\u51b3\u7531\u6570\u636e\u9519\u4f4d\u5f15\u8d77\u7684\u75c5\u6001\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPIF-Net\u7684\u878d\u5408\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u660e\u786e\u7ed3\u5408\u4e86\u75c5\u6001\u5148\u9a8c\u6765\u6709\u6548\u878d\u5408\u591a\u5149\u8c31\u56fe\u50cf\u548c\u9ad8\u5149\u8c31\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u53ef\u9006Mamba\u67b6\u6784\uff0c\u5e76\u5728\u7279\u5f81\u53d8\u6362\u548c\u878d\u5408\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4fe1\u606f\u4e00\u81f4\u6027\uff0c\u786e\u4fdd\u4e86\u7a33\u5b9a\u7684\u68af\u5ea6\u6d41\u548c\u8fc7\u7a0b\u53ef\u9006\u6027\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u611f\u77e5\u4f4e\u79e9\u9002\u5e94\u201d\u7684\u65b0\u578b\u878d\u5408\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5728\u4fdd\u6301\u6a21\u578b\u8f7b\u91cf\u5316\u7684\u540c\u65f6\uff0c\u52a8\u6001\u6821\u51c6\u4e86\u5149\u8c31\u548c\u7a7a\u95f4\u7279\u5f81\u3002", "result": "PIF-Net\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7387\u7684\u540c\u65f6\uff0c\u5728\u56fe\u50cf\u6062\u590d\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u6b65\u3002", "conclusion": "PIF-Net\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cPIF-Net\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7387\u7684\u540c\u65f6\uff0c\u5728\u56fe\u50cf\u6062\u590d\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u6b65\u3002"}}
{"id": "2508.00706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00706", "abs": "https://arxiv.org/abs/2508.00706", "authors": ["Haozhe Tian", "Pietro Ferraro", "Robert Shorten", "Mahdi Jalili", "Homayoun Hamedmoghadam"], "title": "Learning Network Dismantling without Handcrafted Inputs", "comment": null, "summary": "The application of message-passing Graph Neural Networks has been a\nbreakthrough for important network science problems. However, the competitive\nperformance often relies on using handcrafted structural features as inputs,\nwhich increases computational cost and introduces bias into the otherwise\npurely data-driven network representations. Here, we eliminate the need for\nhandcrafted features by introducing an attention mechanism and utilizing\nmessage-iteration profiles, in addition to an effective algorithmic approach to\ngenerate a structurally diverse training set of small synthetic networks.\nThereby, we build an expressive message-passing framework and use it to\nefficiently solve the NP-hard problem of Network Dismantling, virtually\nequivalent to vital node identification, with significant real-world\napplications. Trained solely on diversified synthetic networks, our proposed\nmodel -- MIND: Message Iteration Network Dismantler -- generalizes to large,\nunseen real networks with millions of nodes, outperforming state-of-the-art\nnetwork dismantling methods. Increased efficiency and generalizability of the\nproposed model can be leveraged beyond dismantling in a range of complex\nnetwork problems.", "AI": {"tldr": "MIND\u662f\u4e00\u4e2a\u65b0\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5b83\u4e0d\u9700\u8981\u624b\u5de5\u7279\u5f81\uff0c\u5e76\u4e14\u5728\u7f51\u7edc\u62c6\u5378\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6d88\u606f\u4f20\u9012\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u89e3\u51b3\u7f51\u7edc\u79d1\u5b66\u95ee\u9898\u65f6\u9700\u8981\u624b\u5de5\u7279\u5f81\u8f93\u5165\u7684\u95ee\u9898\uff0c\u8fd9\u4f1a\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u5e76\u5f15\u5165\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d88\u606f\u4f20\u9012\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u548c\u6d88\u606f\u4f20\u9012\u5256\u6790\uff0c\u5e76\u901a\u8fc7\u7b97\u6cd5\u751f\u6210\u591a\u6837\u5316\u7684\u5408\u6210\u7f51\u7edc\u6570\u636e\u96c6\u6765\u6d88\u9664\u5bf9\u624b\u5de5\u7279\u5f81\u7684\u9700\u6c42\u3002", "result": "\u8be5\u6a21\u578b\u5728\u4ec5\u5728\u5408\u6210\u7f51\u7edc\u4e0a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u5927\u89c4\u6a21\u7684\u771f\u5b9e\u7f51\u7edc\uff0c\u5e76\u4e14\u5728\u7f51\u7edc\u62c6\u5378\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u7f51\u7edc\u62c6\u5378\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u5e94\u7528\u4e8e\u5176\u4ed6\u590d\u6742\u7f51\u7edc\u95ee\u9898\u3002"}}
{"id": "2508.00471", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00471", "abs": "https://arxiv.org/abs/2508.00471", "authors": ["Yiwen Wang", "Xinning Chai", "Yuhong Zhang", "Zhengxue Cheng", "Jun Zhao", "Rong Xie", "Li Song"], "title": "Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution", "comment": null, "summary": "Recent advancements in video super-resolution (VSR) models have demonstrated\nimpressive results in enhancing low-resolution videos. However, due to\nlimitations in adequately controlling the generation process, achieving high\nfidelity alignment with the low-resolution input while maintaining temporal\nconsistency across frames remains a significant challenge. In this work, we\npropose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel\napproach that incorporates both semantic and temporal-spatio guidance in the\nlatent diffusion space to address these challenges. By incorporating high-level\nsemantic information and integrating spatial and temporal information, our\napproach achieves a seamless balance between recovering intricate details and\nensuring temporal coherence. Our method not only preserves high-reality visual\ncontent but also significantly enhances fidelity. Extensive experiments\ndemonstrate that SeTe-VSR outperforms existing methods in terms of detail\nrecovery and perceptual quality, highlighting its effectiveness for complex\nvideo super-resolution tasks.", "AI": {"tldr": "SeTe-VSR\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u6269\u6563\u7a7a\u95f4\u4e2d\u7ed3\u5408\u8bed\u4e49\u548c\u65f6\u95f4\u5f15\u5bfc\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u771f\u5bf9\u9f50\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u5728\u63a7\u5236\u751f\u6210\u8fc7\u7a0b\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u5728\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e0e\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u7684\u4fdd\u771f\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSeTe-VSR\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u8bed\u4e49\u548c\u65f6\u95f4\u5f15\u5bfc\uff0c\u5728\u6f5c\u5728\u6269\u6563\u7a7a\u95f4\u4e2d\u8fdb\u884c\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u3002\u901a\u8fc7\u6574\u5408\u9ad8\u5c42\u8bed\u4e49\u4fe1\u606f\u4ee5\u53ca\u7a7a\u95f4\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u6062\u590d\u7ec6\u8282\u4e0e\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "result": "SeTe-VSR\u80fd\u591f\u65e0\u7f1d\u5730\u5e73\u8861\u6062\u590d\u7cbe\u7ec6\u7ec6\u8282\u548c\u4fdd\u6301\u65f6\u95f4\u8fde\u8d2f\u6027\uff0c\u4e0d\u4ec5\u4fdd\u7559\u4e86\u9ad8\u771f\u5b9e\u5ea6\u7684\u89c6\u89c9\u5185\u5bb9\uff0c\u8fd8\u663e\u8457\u63d0\u9ad8\u4e86\u4fdd\u771f\u5ea6\u3002", "conclusion": "SeTe-VSR\u5728\u7ec6\u8282\u6062\u590d\u548c\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2508.00707", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00707", "abs": "https://arxiv.org/abs/2508.00707", "authors": ["Yannik Schnitzer", "Alessandro Abate", "David Parker"], "title": "Efficient Solution and Learning of Robust Factored MDPs", "comment": null, "summary": "Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling\nepistemic uncertainty about transition dynamics. Learning r-MDPs from\ninteractions with an unknown environment enables the synthesis of robust\npolicies with provable (PAC) guarantees on performance, but this can require a\nlarge number of sample interactions. We propose novel methods for solving and\nlearning r-MDPs based on factored state-space representations that leverage the\nindependence between model uncertainty across system components. Although\npolicy synthesis for factored r-MDPs leads to hard, non-convex optimisation\nproblems, we show how to reformulate these into tractable linear programs.\nBuilding on these, we also propose methods to learn factored model\nrepresentations directly. Our experimental results show that exploiting\nfactored structure can yield dimensional gains in sample efficiency, producing\nmore effective robust policies with tighter performance guarantees than\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u56e0\u5b50\u5206\u89e3\u65b9\u6cd5\u89e3\u51b3r-MDPs\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u7b56\u7565\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4ece\u4ea4\u4e92\u4e2d\u5b66\u4e60r-MDPs\u9700\u8981\u5927\u91cf\u6837\u672c\u7684\u6548\u7387\u95ee\u9898\uff0c\u5e76\u5229\u7528\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5728\u7cfb\u7edf\u7ec4\u4ef6\u95f4\u7684\u72ec\u7acb\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56e0\u5b50\u5206\u89e3\u7684\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u65b9\u6cd5\u6765\u89e3\u51b3\u548c\u5b66\u4e60\u9c81\u68d2\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08r-MDPs\uff09\uff0c\u5e76\u5c06\u7b56\u7565\u5408\u6210\u7684\u4f18\u5316\u95ee\u9898\u91cd\u6784\u4e3a\u53ef\u884c\u7684\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8fd8\u63d0\u51fa\u4e86\u76f4\u63a5\u5b66\u4e60\u56e0\u5b50\u5206\u89e3\u6a21\u578b\u8868\u793a\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5229\u7528\u56e0\u5b50\u5206\u89e3\u7ed3\u6784\u53ef\u4ee5\u5e26\u6765\u6837\u672c\u6548\u7387\u4e0a\u7684\u7ef4\u5ea6\u589e\u76ca\uff0c\u751f\u6210\u66f4\u6709\u6548\u7684\u9c81\u68d2\u7b56\u7565\uff0c\u5e76\u63d0\u4f9b\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u66f4\u4e25\u683c\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u56e0\u5b50\u5206\u89e3\u7684\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u65b9\u6cd5\uff0c\u5229\u7528\u4e86\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5728\u7cfb\u7edf\u7ec4\u4ef6\u95f4\u7684\u72ec\u7acb\u6027\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u5e76\u751f\u6210\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6709\u6548\u7684\u9c81\u68d2\u7b56\u7565\uff0c\u540c\u65f6\u5177\u6709\u66f4\u4e25\u683c\u7684\u6027\u80fd\u4fdd\u8bc1\u3002"}}
{"id": "2508.00473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00473", "abs": "https://arxiv.org/abs/2508.00473", "authors": ["Jiaping Cao", "Kangkang Zhou", "Juan Du"], "title": "HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection", "comment": null, "summary": "Video anomaly detection is a fundamental task in video surveillance, with\nbroad applications in public safety and intelligent monitoring systems.\nAlthough previous methods leverage Euclidean representations in RGB or depth\ndomains, such embeddings are inherently limited in capturing hierarchical event\nstructures and spatio-temporal continuity. To address these limitations, we\npropose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for\nanomaly detection in 3D point cloud videos. Our approach first extracts\nper-frame spatial features from point cloud sequences via point cloud\nextractor, and then embeds them into Lorentzian hyperbolic space, which better\ncaptures the latent hierarchical structure of events. To model temporal\ndynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism\nthat leverages Lorentzian inner products and curvature-aware softmax to learn\ntemporal dependencies under non-Euclidean geometry. Our method performs all\nfeature transformations and anomaly scoring directly within full Lorentzian\nspace rather than via tangent space approximation. Extensive experiments\ndemonstrate that HyPCV-Former achieves state-of-the-art performance across\nmultiple anomaly categories, with a 7\\% improvement on the TIMo dataset and a\n5.6\\% gain on the DAD dataset compared to benchmarks. The code will be released\nupon paper acceptance.", "AI": {"tldr": "HyPCV-Former\u662f\u4e00\u79cd\u7528\u4e8e3D\u70b9\u4e91\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u53cc\u66f2\u65f6\u7a7aTransformer\uff0c\u901a\u8fc7\u5728\u6d1b\u4f26\u5179\u53cc\u66f2\u7a7a\u95f4\u4e2d\u5d4c\u5165\u7279\u5f81\u5e76\u4f7f\u7528HMHA\u673a\u5236\u6765\u5b66\u4e60\u65f6\u7a7a\u4f9d\u8d56\u6027\uff0c\u4ece\u800c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728RGB\u6216\u6df1\u5ea6\u57df\u4e2d\u5229\u7528\u6b27\u51e0\u91cc\u5f97\u8868\u793a\uff0c\u4f46\u8fd9\u4e9b\u5d4c\u5165\u5728\u6355\u83b7\u5206\u5c42\u4e8b\u4ef6\u7ed3\u6784\u548c\u65f6\u7a7a\u8fde\u7eed\u6027\u65b9\u9762\u5b58\u5728\u56fa\u6709\u5c40\u9650\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e3D\u70b9\u4e91\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u53cc\u66f2\u65f6\u7a7aTransformer\u3002", "method": "HyPCV-Former\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u66f2\u65f6\u7a7aTransformer\uff0c\u7528\u4e8e3D\u70b9\u4e91\u89c6\u9891\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u901a\u8fc7\u70b9\u4e91\u63d0\u53d6\u5668\u4ece\u70b9\u4e91\u5e8f\u5217\u4e2d\u63d0\u53d6\u6bcf\u5e27\u7a7a\u95f4\u7279\u5f81\uff0c\u7136\u540e\u5c06\u5176\u5d4c\u5165\u5230\u6d1b\u4f26\u5179\u53cc\u66f2\u7a7a\u95f4\u4e2d\uff0c\u4ee5\u6355\u6349\u4e8b\u4ef6\u6f5c\u5728\u7684\u5c42\u6b21\u7ed3\u6784\u3002\u901a\u8fc7\u5f15\u5165\u53cc\u66f2\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff08HMHA\uff09\u673a\u5236\u6765\u6a21\u62df\u65f6\u95f4\u52a8\u6001\uff0c\u8be5\u673a\u5236\u5229\u7528\u6d1b\u4f26\u5179\u5185\u79ef\u548c\u66f2\u7387\u611f\u77e5softmax\u5728\u975e\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u4e0b\u5b66\u4e60\u65f6\u95f4\u4f9d\u8d56\u6027\u3002\u6240\u6709\u7279\u5f81\u53d8\u6362\u548c\u5f02\u5e38\u8bc4\u5206\u5747\u5728\u5b8c\u6574\u7684\u6d1b\u4f26\u5179\u7a7a\u95f4\u4e2d\u8fdb\u884c\u3002", "result": "HyPCV-Former\u5728TIMo\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e867%\uff0c\u5728DAD\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e865.6%\uff0c\u5728\u591a\u4e2a\u5f02\u5e38\u7c7b\u522b\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HyPCV-Former\u5728\u591a\u4e2a\u5f02\u5e38\u7c7b\u522b\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728TIMo\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e867%\uff0c\u5728DAD\u6570\u636e\u96c6\u4e0a\u6bd4\u57fa\u51c6\u63d0\u9ad8\u4e865.6%\u3002"}}
{"id": "2508.00712", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00712", "abs": "https://arxiv.org/abs/2508.00712", "authors": ["Dien Nguyen", "Diego Perez-Liebana", "Simon Lucas"], "title": "JSON-Bag: A generic game trajectory representation", "comment": "8 pages, 3 figures, 6 tables, to be published in IEEE Conference on\n  Games 2025", "summary": "We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically\nrepresent game trajectories by tokenizing their JSON descriptions and apply\nJensen-Shannon distance (JSD) as distance metric for them. Using a\nprototype-based nearest-neighbor search (P-NNS), we evaluate the validity of\nJSON-Bag with JSD on six tabletop games -- \\textit{7 Wonders},\n\\textit{Dominion}, \\textit{Sea Salt and Paper}, \\textit{Can't Stop},\n\\textit{Connect4}, \\textit{Dots and boxes} -- each over three game trajectory\nclassification tasks: classifying the playing agents, game parameters, or game\nseeds that were used to generate the trajectories.\n  Our approach outperforms a baseline using hand-crafted features in the\nmajority of tasks. Evaluating on N-shot classification suggests using JSON-Bag\nprototype to represent game trajectory classes is also sample efficient.\nAdditionally, we demonstrate JSON-Bag ability for automatic feature extraction\nby treating tokens as individual features to be used in Random Forest to solve\nthe tasks above, which significantly improves accuracy on underperforming\ntasks. Finally, we show that, across all six games, the JSD between JSON-Bag\nprototypes of agent classes highly correlates with the distances between\nagents' policies.", "AI": {"tldr": "JSON-Bag \u4f7f\u7528\u5206\u8bcd\u548c JSD \u6765\u8868\u793a\u6e38\u620f\u8f68\u8ff9\uff0c\u5e76\u5728\u6e38\u620f\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u624b\u5de5\u7279\u5f81\uff0c\u540c\u65f6\u8fd8\u80fd\u81ea\u52a8\u63d0\u53d6\u7279\u5f81\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\u6765\u8868\u793a\u548c\u5206\u6790\u6e38\u620f\u8f68\u8ff9\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u6e38\u620f\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a JSON-Bag-of-Tokens (JSON-Bag) \u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6e38\u620f\u8f68\u8ff9\u7684 JSON \u63cf\u8ff0\u8fdb\u884c\u5206\u8bcd\u6765\u8868\u793a\u5b83\u4eec\uff0c\u5e76\u4f7f\u7528 Jensen-Shannon \u8ddd\u79bb (JSD) \u4f5c\u4e3a\u8ddd\u79bb\u5ea6\u91cf\u3002\u7ed3\u5408\u539f\u578b\u6700\u8fd1\u90bb\u641c\u7d22 (P-NNS) \u6765\u8bc4\u4f30 JSON-Bag \u4e0e JSD \u5728\u516d\u79cd\u684c\u9762\u6e38\u620f\u4e0a\u7684\u6709\u6548\u6027\uff0c\u7528\u4e8e\u5bf9\u6e38\u620f\u8f68\u8ff9\u5206\u7c7b\u4efb\u52a1\u3002", "result": "JSON-Bag \u5728\u5927\u591a\u6570\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728 N \u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u6837\u672c\u6548\u7387\u3002\u5b83\u8fd8\u80fd\u901a\u8fc7\u968f\u673a\u68ee\u6797\u81ea\u52a8\u63d0\u53d6\u7279\u5f81\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u3002JSD \u4e0e agent \u7b56\u7565\u4e4b\u95f4\u7684\u8ddd\u79bb\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "JSON-Bag \u65b9\u6cd5\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e2d\u4f18\u4e8e\u624b\u5de5\u7279\u5f81\u57fa\u7ebf\uff0c\u5e76\u4e14\u5728 N \u6837\u672c\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u6837\u672c\u6548\u7387\u3002\u6b64\u5916\uff0cJSON-Bag \u901a\u8fc7\u5c06 token \u89c6\u4e3a\u7528\u4e8e\u968f\u673a\u68ee\u6797\u7684\u5355\u72ec\u7279\u5f81\uff0c\u63d0\u9ad8\u4e86\u5728\u8868\u73b0\u4e0d\u4f73\u7684\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\u3002\u6700\u540e\uff0cJSON-Bag \u4ea7\u751f\u7684 agent \u7c7b\u522b\u539f\u578b\u4e4b\u95f4\u7684 JSD \u4e0e agent \u7b56\u7565\u4e4b\u95f4\u7684\u8ddd\u79bb\u9ad8\u5ea6\u76f8\u5173\u3002"}}
{"id": "2508.00518", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00518", "abs": "https://arxiv.org/abs/2508.00518", "authors": ["Shuo Liang", "Yiwu Zhong", "Zi-Yuan Hu", "Yeyao Tao", "Liwei Wang"], "title": "Fine-grained Spatiotemporal Grounding on Egocentric Videos", "comment": "Accepted by ICCV 2025", "summary": "Spatiotemporal video grounding aims to localize target entities in videos\nbased on textual queries. While existing research has made significant progress\nin exocentric videos, the egocentric setting remains relatively underexplored,\ndespite its growing importance in applications such as augmented reality and\nrobotics. In this work, we conduct a systematic analysis of the discrepancies\nbetween egocentric and exocentric videos, revealing key challenges such as\nshorter object durations, sparser trajectories, smaller object sizes, and\nlarger positional shifts. To address these challenges, we introduce EgoMask,\nthe first pixel-level benchmark for fine-grained spatiotemporal grounding in\negocentric videos. It is constructed by our proposed automatic annotation\npipeline, which annotates referring expressions and object masks across short-,\nmedium-, and long-term videos. Additionally, we create EgoMask-Train, a\nlarge-scale training dataset to facilitate model development. Experiments\ndemonstrate that the state-of-the-art spatiotemporal grounding models perform\npoorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields\nsignificant improvements, while preserving performance on exocentric datasets.\nOur work thus provides essential resources and insights for advancing\negocentric video understanding. Our code is available at\nhttps://github.com/LaVi-Lab/EgoMask .", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86 EgoMask\uff0c\u9996\u4e2a\u7528\u4e8e egocentric \u89c6\u9891\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u5b9a\u4f4d\u7684\u50cf\u7d20\u7ea7\u57fa\u51c6\uff0c\u5e76\u521b\u5efa\u4e86 EgoMask-Train \u8bad\u7ec3\u6570\u636e\u96c6\u3002\u8be5\u7814\u7a76\u5206\u6790\u4e86 egocentric \u548c exocentric \u89c6\u9891\u7684\u5dee\u5f02\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u65b9\u6cd5\u5728\u63d0\u9ad8 egocentric \u89c6\u9891\u7406\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u7814\u7a76\u5728 egocentric \u89c6\u9891\u8bbe\u7f6e\u65b9\u9762\u63a2\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ee5\u53ca egocentric \u89c6\u9891\u5728\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u7b49\u5e94\u7528\u4e2d\u7684\u65e5\u76ca\u589e\u957f\u7684\u91cd\u8981\u6027\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3 egocentric \u89c6\u9891\u548c exocentric \u89c6\u9891\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ee5\u53ca\u7531\u6b64\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a EgoMask \u7684\u50cf\u7d20\u7ea7\u57fa\u51c6\uff0c\u7528\u4e8e egocentric \u89c6\u9891\u4e2d\u7684\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u5b9a\u4f4d\u3002\u8be5\u57fa\u51c6\u662f\u901a\u8fc7\u81ea\u52a8\u6ce8\u91ca\u6d41\u7a0b\u6784\u5efa\u7684\uff0c\u8be5\u6d41\u7a0b\u5bf9\u77ed\u3001\u4e2d\u3001\u957f\u671f\u89c6\u9891\u4e2d\u7684\u6307\u79f0\u8868\u8fbe\u548c\u5bf9\u8c61\u63a9\u7801\u8fdb\u884c\u6ce8\u91ca\u3002\u6b64\u5916\uff0c\u8fd8\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u8bad\u7ec3\u6570\u636e\u96c6 EgoMask-Train \u4ee5\u4fc3\u8fdb\u6a21\u578b\u5f00\u53d1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6700\u5148\u8fdb\u7684\u65f6\u7a7a\u5b9a\u4f4d\u6a21\u578b\u5728 EgoMask \u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728 EgoMask-Train \u4e0a\u8fdb\u884c\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5728 exocentric \u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a egocentric \u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\u548c\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.00477", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00477", "abs": "https://arxiv.org/abs/2508.00477", "authors": ["Yuzhuo Chen", "Zehua Ma", "Jianhua Wang", "Kai Kang", "Shunyu Yao", "Weiming Zhang"], "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer", "comment": "8 pages, 5 figures, 3 tables", "summary": "In controllable image synthesis, generating coherent and consistent images\nfrom multiple references with spatial layout awareness remains an open\nchallenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework\nthat, for the first time, extends single-reference diffusion models to\nmulti-reference scenarios in a training-free manner. Built upon the MMDiT\nmodel, LAMIC introduces two plug-and-play attention mechanisms: 1) Group\nIsolation Attention (GIA) to enhance entity disentanglement; and 2)\nRegion-Modulated Attention (RMA) to enable layout-aware generation. To\ncomprehensively evaluate model capabilities, we further introduce three\nmetrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout\ncontrol; and 2) Background Similarity (BG-S) for measuring background\nconsistency. Extensive experiments show that LAMIC achieves state-of-the-art\nperformance across most major metrics: it consistently outperforms existing\nmulti-reference baselines in ID-S, BG-S, IN-R and AVG scores across all\nsettings, and achieves the best DPG in complex composition tasks. These results\ndemonstrate LAMIC's superior abilities in identity keeping, background\npreservation, layout control, and prompt-following, all achieved without any\ntraining or fine-tuning, showcasing strong zero-shot generalization ability. By\ninheriting the strengths of advanced single-reference models and enabling\nseamless extension to multi-image scenarios, LAMIC establishes a new\ntraining-free paradigm for controllable multi-image composition. As foundation\nmodels continue to evolve, LAMIC's performance is expected to scale\naccordingly. Our implementation is available at:\nhttps://github.com/Suchenl/LAMIC.", "AI": {"tldr": "LAMIC \u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u56fe\u50cf\u7ec4\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u5e03\u5c40\u63a7\u5236\u548c\u56fe\u50cf\u4e00\u81f4\u6027\u3002", "motivation": "\u5728\u53ef\u63a7\u56fe\u50cf\u5408\u6210\u9886\u57df\uff0c\u4ece\u591a\u4e2a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u8fde\u8d2f\u3001\u4e00\u81f4\u4e14\u5177\u6709\u7a7a\u95f4\u5e03\u5c40\u610f\u8bc6\u7684\u56fe\u50cf\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "LAMIC \u662f\u4e00\u4e2a\u57fa\u4e8e MMDiT \u6a21\u578b\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u5e03\u5c40\u611f\u77e5\u591a\u56fe\u50cf\u7ec4\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u7fa4\u7ec4\u9694\u79bb\u6ce8\u610f\u529b\u548c\u533a\u57df\u8c03\u5236\u6ce8\u610f\u529b\u4e24\u79cd\u5373\u63d2\u5373\u7528\u6ce8\u610f\u529b\u673a\u5236\uff0c\u589e\u5f3a\u5b9e\u4f53\u89e3\u8026\u5e76\u5b9e\u73b0\u611f\u77e5\u5e03\u5c40\u751f\u6210\u3002", "result": "LAMIC \u5728 ID-S\u3001BG-S\u3001IN-R \u548c AVG \u5206\u6570\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u53c2\u8003\u57fa\u7ebf\uff0c\u5e76\u5728\u590d\u6742\u7684\u7ec4\u5408\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u7684 DPG \u5206\u6570\uff0c\u5c55\u793a\u4e86\u5176\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LAMIC \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5355\u53c2\u8003\u6269\u6563\u6a21\u578b\u6269\u5c55\u5230\u591a\u53c2\u8003\u56fe\u50cf\u7ec4\u5408\u573a\u666f\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u8eab\u4efd\u4fdd\u6301\u3001\u80cc\u666f\u4fdd\u6301\u3001\u5e03\u5c40\u63a7\u5236\u548c\u63d0\u793a\u9075\u5faa\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.00716", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00716", "abs": "https://arxiv.org/abs/2508.00716", "authors": ["Yingxu Wang", "Mengzhu Wang", "Zhichao Huang", "Suyu Liu"], "title": "Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning", "comment": null, "summary": "Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled\nsource graphs to unlabeled target graphs by learning domain-invariant\nrepresentations, which is essential in applications such as molecular property\nprediction and social network analysis. However, most existing GDA methods rely\non the assumption of clean source labels, which rarely holds in real-world\nscenarios where annotation noise is pervasive. This label noise severely\nimpairs feature alignment and degrades adaptation performance under domain\nshifts. To address this challenge, we propose Nested Graph Pseudo-Label\nRefinement (NeGPR), a novel framework tailored for graph-level domain\nadaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,\nsemantic and topology branches, by enforcing neighborhood consistency in the\nfeature space, thereby reducing the influence of noisy supervision. To bridge\ndomain gaps, NeGPR employs a nested refinement mechanism in which one branch\nselects high-confidence target samples to guide the adaptation of the other,\nenabling progressive cross-domain learning. Furthermore, since pseudo-labels\nmay still contain noise and the pre-trained branches are already overfitted to\nthe noisy labels in the source domain, NeGPR incorporates a noise-aware\nregularization strategy. This regularization is theoretically proven to\nmitigate the adverse effects of pseudo-label noise, even under the presence of\nsource overfitting, thus enhancing the robustness of the adaptation process.\nExtensive experiments on benchmark datasets demonstrate that NeGPR consistently\noutperforms state-of-the-art methods under severe label noise, achieving gains\nof up to 12.7% in accuracy.", "AI": {"tldr": "NeGPR\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u5e26\u566a\u58f0\u6807\u7b7e\u7684\u56fe\u57df\u81ea\u9002\u5e94\u3002\u5b83\u901a\u8fc7\u53cc\u5206\u652f\u9884\u8bad\u7ec3\u3001\u5d4c\u5957\u4f2a\u6807\u7b7e\u7ec6\u5316\u548c\u566a\u58f0\u611f\u77e5\u6b63\u5219\u5316\u6765\u63d0\u9ad8\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u57df\u81ea\u9002\u5e94\uff08GDA\uff09\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u4e8e\u5e72\u51c0\u7684\u6e90\u6807\u7b7e\u5047\u8bbe\uff0c\u4f46\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u6807\u7b7e\u566a\u58f0\u666e\u904d\u5b58\u5728\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u7279\u5f81\u5bf9\u9f50\u548c\u57df\u8fc1\u79fb\u4e0b\u7684\u81ea\u9002\u5e94\u6027\u80fd\u3002", "method": "NeGPR\u9996\u5148\u901a\u8fc7\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5f3a\u5236\u6267\u884c\u90bb\u57df\u4e00\u81f4\u6027\u6765\u9884\u8bad\u7ec3\u8bed\u4e49\u548c\u62d3\u6251\u53cc\u5206\u652f\uff0c\u4ece\u800c\u51cf\u5c11\u566a\u58f0\u76d1\u7763\u7684\u5f71\u54cd\u3002\u7136\u540e\uff0c\u901a\u8fc7\u5d4c\u5957\u7ec6\u5316\u673a\u5236\uff0c\u4e00\u4e2a\u5206\u652f\u9009\u62e9\u9ad8\u7f6e\u4fe1\u5ea6\u76ee\u6807\u6837\u672c\u6765\u6307\u5bfc\u53e6\u4e00\u4e2a\u5206\u652f\u7684\u9002\u5e94\uff0c\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u8de8\u57df\u5b66\u4e60\u3002\u6700\u540e\uff0c\u901a\u8fc7\u566a\u58f0\u611f\u77e5\u6b63\u5219\u5316\u7b56\u7565\u6765\u51cf\u8f7b\u4f2a\u6807\u7b7e\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u5e76\u589e\u5f3a\u9002\u5e94\u8fc7\u7a0b\u7684\u9c81\u68d2\u6027\u3002", "result": "NeGPR\u6846\u67b6\u5728\u5b58\u5728\u4e25\u91cd\u6807\u7b7e\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0c\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u65b9\u9762\u63d0\u9ad8\u4e8612.7%\u3002", "conclusion": "NeGPR\u6846\u67b6\u5728\u5b58\u5728\u4e25\u91cd\u6807\u7b7e\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0c\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u65b9\u9762\u63d0\u9ad8\u4e8612.7%\u3002"}}
{"id": "2508.00493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00493", "abs": "https://arxiv.org/abs/2508.00493", "authors": ["Alfie Roddan", "Tobias Czempiel", "Chi Xu", "Daniel S. Elson", "Stamatia Giannarou"], "title": "SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation", "comment": null, "summary": "We present SAMSA 2.0, an interactive segmentation framework for hyperspectral\nmedical imaging that introduces spectral angle prompting to guide the Segment\nAnything Model (SAM) using spectral similarity alongside spatial cues. This\nearly fusion of spectral information enables more accurate and robust\nsegmentation across diverse spectral datasets. Without retraining, SAMSA 2.0\nachieves up to +3.8% higher Dice scores compared to RGB-only models and up to\n+3.1% over prior spectral fusion methods. Our approach enhances few-shot and\nzero-shot performance, demonstrating strong generalization in challenging\nlow-data and noisy scenarios common in clinical imaging.", "AI": {"tldr": "SAMSA 2.0 \u662f\u4e00\u79cd\u7528\u4e8e\u9ad8\u5149\u8c31\u533b\u5b66\u6210\u50cf\u7684\u4ea4\u4e92\u5f0f\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5149\u8c31\u548c\u7a7a\u95f4\u7ebf\u7d22\u6765\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728\u6570\u636e\u6709\u9650\u6216\u6709\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u4e3a\u4e86\u5728\u9ad8\u5149\u8c31\u533b\u5b66\u6210\u50cf\u4e2d\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u66f4\u9c81\u68d2\u7684\u5206\u5272\uff0c\u5e76\u63d0\u9ad8\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4e34\u5e8a\u6210\u50cf\u4e2d\u5e38\u89c1\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u4f4e\u6570\u636e\u548c\u566a\u58f0\u573a\u666f\u4e2d\u3002", "method": "SAMSA 2.0 \u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u5206\u5272\u6846\u67b6\uff0c\u5229\u7528\u5149\u8c31\u89d2\u5ea6\u63d0\u793a\u5c06\u5149\u8c31\u76f8\u4f3c\u6027\u4e0e\u7a7a\u95f4\u7ebf\u7d22\u76f8\u7ed3\u5408\uff0c\u4ee5\u6307\u5bfc\u5206\u5272\u4efb\u4f55\u6a21\u578b\uff08SAM\uff09\u3002", "result": "SAMSA 2.0 \u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4ec5 RGB \u6a21\u578b\u9ad8\u51fa 3.8% \u7684 Dice \u5206\u6570\uff0c\u6bd4\u5148\u524d\u5149\u8c31\u878d\u5408\u65b9\u6cd5\u9ad8\u51fa 3.1% \u7684 Dice \u5206\u6570\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4f4e\u6570\u636e\u548c\u566a\u58f0\u573a\u666f\u4e2d\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SAMSA 2.0 \u901a\u8fc7\u5f15\u5165\u5149\u8c31\u89d2\u5ea6\u63d0\u793a\uff0c\u5c06\u5149\u8c31\u76f8\u4f3c\u6027\u4e0e\u7a7a\u95f4\u7ebf\u7d22\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9ad8\u5149\u8c31\u533b\u5b66\u6210\u50cf\u7684\u4ea4\u4e92\u5f0f\u5206\u5272\u6846\u67b6\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u4ec5 RGB \u6a21\u578b\u548c\u5148\u524d\u5149\u8c31\u878d\u5408\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684 Dice \u5206\u6570\uff0c\u5e76\u589e\u5f3a\u4e86\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u6027\u80fd\u3002"}}
{"id": "2508.00718", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00718", "abs": "https://arxiv.org/abs/2508.00718", "authors": ["Ivona Krchova", "Mariana Vargas Vieyra", "Mario Scriminaci", "Andrey Sidorenko"], "title": "Democratizing Tabular Data Access with an Open$\\unicode{x2013}$Source Synthetic$\\unicode{x2013}$Data SDK", "comment": null, "summary": "Machine learning development critically depends on access to high-quality\ndata. However, increasing restrictions due to privacy, proprietary interests,\nand ethical concerns have created significant barriers to data accessibility.\nSynthetic data offers a viable solution by enabling safe, broad data usage\nwithout compromising sensitive information. This paper presents the MOSTLY AI\nSynthetic Data Software Development Kit (SDK), an open-source toolkit designed\nspecifically for synthesizing high-quality tabular data. The SDK integrates\nrobust features such as differential privacy guarantees, fairness-aware data\ngeneration, and automated quality assurance into a flexible and accessible\nPython interface. Leveraging the TabularARGN autoregressive framework, the SDK\nsupports diverse data types and complex multi-table and sequential datasets,\ndelivering competitive performance with notable improvements in speed and\nusability. Currently deployed both as a cloud service and locally installable\nsoftware, the SDK has seen rapid adoption, highlighting its practicality in\naddressing real-world data bottlenecks and promoting widespread data\ndemocratization.", "AI": {"tldr": "MOSTLY AI SDK\u662f\u4e00\u4e2a\u5f00\u6e90\u7684Python\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8868\u683c\u5408\u6210\u6570\u636e\u3002\u5b83\u5177\u6709\u5dee\u5206\u9690\u79c1\u3001\u516c\u5e73\u6027\u611f\u77e5\u548c\u81ea\u52a8\u5316\u8d28\u91cf\u4fdd\u8bc1\u529f\u80fd\uff0c\u652f\u6301\u591a\u79cd\u6570\u636e\u7c7b\u578b\u548c\u590d\u6742\u6570\u636e\u96c6\uff0c\u5e76\u4e14\u6613\u4e8e\u4f7f\u7528\u3002\u8be5SDK\u5df2\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4ee5\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6570\u636e\u8bbf\u95ee\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7684\u53d1\u5c55\u79bb\u4e0d\u5f00\u9ad8\u8d28\u91cf\u7684\u6570\u636e\uff0c\u4f46\u9690\u79c1\u3001\u6240\u6709\u6743\u548c\u9053\u5fb7\u4f26\u7406\u95ee\u9898\u9650\u5236\u4e86\u6570\u636e\u7684\u53ef\u8bbf\u95ee\u6027\u3002\u5408\u6210\u6570\u636e\u4f5c\u4e3a\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u5728\u4e0d\u6cc4\u9732\u654f\u611f\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6570\u636e\u7684\u5e7f\u6cdb\u5b89\u5168\u4f7f\u7528\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86MOSTLY AI\u5408\u6210\u6570\u636e\u8f6f\u4ef6\u5f00\u53d1\u5de5\u5177\u5305\uff08SDK\uff09\uff0c\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u4e13\u95e8\u7528\u4e8e\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u8868\u683c\u6570\u636e\u3002\u8be5SDK\u96c6\u6210\u4e86\u5dee\u5206\u9690\u79c1\u4fdd\u8bc1\u3001\u516c\u5e73\u6027\u611f\u77e5\u6570\u636e\u751f\u6210\u4ee5\u53ca\u81ea\u52a8\u5316\u7684\u8d28\u91cf\u4fdd\u8bc1\u7b49\u529f\u80fd\uff0c\u5e76\u901a\u8fc7Python\u63a5\u53e3\u63d0\u4f9b\u3002", "result": "\u8be5SDK\u5229\u7528TabularARGN\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u6570\u636e\u7c7b\u578b\u4ee5\u53ca\u590d\u6742\u7684\u591a\u8868\u548c\u5e8f\u5217\u6570\u636e\u96c6\uff0c\u5728\u901f\u5ea6\u548c\u6613\u7528\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5SDK\u901a\u8fc7\u96c6\u6210\u5dee\u5206\u9690\u79c1\u3001\u516c\u5e73\u6027\u611f\u77e5\u751f\u6210\u548c\u81ea\u52a8\u5316\u8d28\u91cf\u4fdd\u8bc1\u7b49\u529f\u80fd\uff0c\u4e3a\u5408\u6210\u9ad8\u8d28\u91cf\u8868\u683c\u6570\u636e\u63d0\u4f9b\u4e86\u7075\u6d3b\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5df2\u5728\u4e91\u670d\u52a1\u548c\u672c\u5730\u90e8\u7f72\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6570\u636e\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2508.00496", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00496", "abs": "https://arxiv.org/abs/2508.00496", "authors": ["Mohammed Kamran", "Maria Bernathova", "Raoul Varga", "Christian Singer", "Zsuzsanna Bago-Horvath", "Thomas Helbich", "Georg Langs", "Philipp Seeb\u00f6ck"], "title": "LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI", "comment": null, "summary": "Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced\nMRI (DCE-MRI) is critical for early cancer detection, especially in high-risk\npatients. While recent deep learning methods have advanced lesion segmentation,\nthey primarily target large lesions and neglect valuable longitudinal and\nclinical information routinely used by radiologists. In real-world screening,\ndetecting subtle or emerging lesions requires radiologists to compare across\ntimepoints and consider previous radiology assessments, such as the BI-RADS\nscore. We propose LesiOnTime, a novel 3D segmentation approach that mimics\nclinical diagnostic workflows by jointly leveraging longitudinal imaging and\nBIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)\nblock that dynamically integrates information from previous and current scans;\nand (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent\nspace alignment for scans with similar radiological assessments, thus embedding\ndomain knowledge into the training process. Evaluated on a curated in-house\nlongitudinal dataset of high-risk patients with DCE-MRI, our approach\noutperforms state-of-the-art single-timepoint and longitudinal baselines by 5%\nin terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute\ncomplementary performance gains. These results highlight the importance of\nincorporating temporal and clinical context for reliable early lesion\nsegmentation in real-world breast cancer screening. Our code is publicly\navailable at https://github.com/cirmuw/LesiOnTime", "AI": {"tldr": "LesiOnTime\u662f\u4e00\u79cd\u65b0\u76843D\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u7eb5\u5411\u5f71\u50cf\u548cBI-RADS\u8bc4\u5206\uff0c\u63d0\u9ad8\u4e86\u4e73\u817a\u5c0f\u7076\u6027\u75c5\u53d8\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u51c6\u786e\u5206\u5272\u4e73\u817a\u52a8\u6001\u589e\u5f3aMRI\uff08DCE-MRI\uff09\u4e2d\u7684\u5c0f\u7076\u6027\u75c5\u53d8\u5bf9\u4e8e\u65e9\u671f\u764c\u75c7\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5927\u75c5\u53d8\uff0c\u5ffd\u7565\u4e86\u7eb5\u5411\u548c\u4e34\u5e8a\u4fe1\u606f\u3002\u653e\u5c04\u79d1\u533b\u751f\u5728\u7b5b\u67e5\u4e2d\u9700\u8981\u5bf9\u6bd4\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u5f71\u50cf\u5e76\u53c2\u8003BI-RADS\u8bc4\u5206\u6765\u68c0\u6d4b\u7ec6\u5fae\u6216\u65b0\u5174\u7684\u75c5\u53d8\u3002", "method": "LesiOnTime\u662f\u4e00\u79cd\u65b0\u9896\u76843D\u5206\u5272\u65b9\u6cd5\uff0c\u5176\u6838\u5fc3\u5728\u4e8e\uff1a1. \u65f6\u95f4\u5148\u9a8c\u6ce8\u610f\u529b\uff08TPA\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u52a8\u6001\u6574\u5408\u524d\u540e\u626b\u63cf\u4fe1\u606f\uff1b2. BI-RADS\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff08BCR\uff09\u635f\u5931\uff0c\u7528\u4e8e\u5bf9\u9f50\u5177\u6709\u76f8\u4f3c\u653e\u5c04\u5b66\u8bc4\u4f30\u7684\u626b\u63cf\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5c06\u9886\u57df\u77e5\u8bc6\u5d4c\u5165\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u5185\u90e8\u5206\u591a\u4e2a\u65f6\u95f4\u70b9\u7684DCE-MRI\u6570\u636e\u96c6\u4e0a\uff0cLesiOnTime\u76f8\u6bd4\u4e8e\u6700\u5148\u8fdb\u7684\u5355\u65f6\u95f4\u70b9\u548c\u7eb5\u5411\u65b9\u6cd5\uff0c\u5728Dice\u6307\u6807\u4e0a\u63d0\u5347\u4e865%\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0cTPA\u548cBCR\u6a21\u5757\u5747\u80fd\u5e26\u6765\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "LesiOnTime\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u7eb5\u5411\u5f71\u50cf\u548cBI-RADS\u8bc4\u5206\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5c0f\u7076\u6027\u75c5\u53d8\u7684\u5206\u5272\u7cbe\u5ea6\uff0c\u4e3a\u4e73\u817a\u764c\u65e9\u671f\u7b5b\u67e5\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2508.00734", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00734", "abs": "https://arxiv.org/abs/2508.00734", "authors": ["Liuyun Xu", "Seymour M. J. Spence"], "title": "Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems", "comment": null, "summary": "Existing variance reduction techniques used in stochastic simulations for\nrare event analysis still require a substantial number of model evaluations to\nestimate small failure probabilities. In the context of complex, nonlinear\nfinite element modeling environments, this can become computationally\nchallenging-particularly for systems subjected to stochastic excitation. To\naddress this challenge, a multi-fidelity stratified sampling scheme with\nadaptive machine learning metamodels is introduced for efficiently propagating\nuncertainties and estimating small failure probabilities. In this approach, a\nhigh-fidelity dataset generated through stratified sampling is used to train a\ndeep learning-based metamodel, which then serves as a cost-effective and highly\ncorrelated low-fidelity model. An adaptive training scheme is proposed to\nbalance the trade-off between approximation quality and computational demand\nassociated with the development of the low-fidelity model. By integrating the\nlow-fidelity outputs with additional high-fidelity results, an unbiased\nestimate of the strata-wise failure probabilities is obtained using a\nmulti-fidelity Monte Carlo framework. The overall probability of failure is\nthen computed using the total probability theorem. Application to a full-scale\nhigh-rise steel building subjected to stochastic wind excitation demonstrates\nthat the proposed scheme can accurately estimate exceedance probability curves\nfor nonlinear responses of interest, while achieving significant computational\nsavings compared to single-fidelity variance reduction approaches.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u7cbe\u5ea6\u5206\u5c42\u91c7\u6837\u548c\u81ea\u9002\u5e94\u673a\u5668\u5b66\u4e60\u5143\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u4f30\u8ba1\u7a00\u6709\u4e8b\u4ef6\u7684\u5931\u6548\u6982\u7387\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u7684\u975e\u7ebf\u6027\u6709\u9650\u5143\u5206\u6790\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u5143\u6a21\u578b\u4f5c\u4e3a\u4f4e\u7cbe\u5ea6\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\uff0c\u5728\u5b9e\u9645\u5de5\u7a0b\u6848\u4f8b\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u65b9\u5dee\u7f29\u51cf\u6280\u672f\u5728\u7a00\u6709\u4e8b\u4ef6\u5206\u6790\u4e2d\u9700\u8981\u5927\u91cf\u6a21\u578b\u8bc4\u4f30\u624d\u80fd\u4f30\u8ba1\u5c0f\u5931\u6548\u6982\u7387\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u3001\u975e\u7ebf\u6027\u7684\u6709\u9650\u5143\u5efa\u6a21\u73af\u5883\u548c\u968f\u673a\u6fc0\u52b1\u4e0b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7cbe\u5ea6\u5206\u5c42\u91c7\u6837\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u7ed3\u5408\u4e86\u81ea\u9002\u5e94\u673a\u5668\u5b66\u4e60\u5143\u6a21\u578b\uff0c\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u548c\u5931\u6548\u6982\u7387\u4f30\u8ba1\u3002\u9996\u5148\uff0c\u5229\u7528\u5206\u5c42\u91c7\u6837\u751f\u6210\u7684\u9ad8\u7cbe\u5ea6\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5143\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5145\u5f53\u5177\u6709\u6210\u672c\u6548\u76ca\u4e14\u9ad8\u5ea6\u76f8\u5173\u7684\u4f4e\u7cbe\u5ea6\u6a21\u578b\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8bad\u7ec3\u65b9\u6848\u6765\u5e73\u8861\u4f4e\u7cbe\u5ea6\u6a21\u578b\u7684\u8fd1\u4f3c\u8d28\u91cf\u548c\u8ba1\u7b97\u9700\u6c42\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5c06\u4f4e\u7cbe\u5ea6\u8f93\u51fa\u4e0e\u989d\u5916\u7684\u9ad8\u7cbe\u5ea6\u7ed3\u679c\u76f8\u7ed3\u5408\uff0c\u5e76\u5229\u7528\u5168\u6982\u7387\u5b9a\u7406\uff0c\u4f7f\u7528\u591a\u7cbe\u5ea6\u8499\u7279\u5361\u6d1b\u6846\u67b6\u83b7\u5f97\u65e0\u504f\u7684\u5c42\u76f8\u5173\u5931\u6548\u6982\u7387\u4f30\u8ba1\u3002", "result": "\u901a\u8fc7\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u627f\u53d7\u968f\u673a\u98ce\u6fc0\u52b1\u7684\u5168\u5c3a\u5bf8\u9ad8\u5c42\u94a2\u7ed3\u6784\u5efa\u7b51\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6848\u53ef\u4ee5\u51c6\u786e\u4f30\u8ba1\u975e\u7ebf\u6027\u54cd\u5e94\u7684\u8d85\u8fc7\u6982\u7387\u66f2\u7ebf\uff0c\u5e76\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u8282\u7ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u975e\u7ebf\u6027\u54cd\u5e94\u7684\u8d85\u8fc7\u6982\u7387\u66f2\u7ebf\uff0c\u5e76\u4e0e\u5355\u7cbe\u5ea6\u65b9\u5dee\u7f29\u51cf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u663e\u8457\u8282\u7701\u4e86\u8ba1\u7b97\u8d44\u6e90\u3002"}}
{"id": "2508.00506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00506", "abs": "https://arxiv.org/abs/2508.00506", "authors": ["Tulsi Patel", "Mark W. Jones", "Thomas Redfern"], "title": "Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool", "comment": "Video supplement demonstrating feature-space exploration and\n  interactive labelling is available at: https://youtu.be/GZl1ebZJgEA and is\n  archived at https://doi.org/10.5281/zenodo.16676591", "summary": "Machine learning for remote sensing imaging relies on up-to-date and accurate\nlabels for model training and testing. Labelling remote sensing imagery is time\nand cost intensive, requiring expert analysis. Previous labelling tools rely on\npre-labelled data for training in order to label new unseen data. In this work,\nwe define an unsupervised pipeline for finding and labelling geographical areas\nof similar context and content within Sentinel-2 satellite imagery. Our\napproach removes limitations of previous methods by utilising segmentation with\nconvolutional and graph neural networks to encode a more robust feature space\nfor image comparison. Unlike previous approaches we segment the image into\nhomogeneous regions of pixels that are grouped based on colour and spatial\nsimilarity. Graph neural networks are used to aggregate information about the\nsurrounding segments enabling the feature representation to encode the local\nneighbourhood whilst preserving its own local information. This reduces\noutliers in the labelling tool, allows users to label at a granular level, and\nallows a rotationally invariant semantic relationship at the image level to be\nformed within the encoding space.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5229\u7528\u5148\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u6280\u672f\uff0c\u65e0\u9700\u9884\u5148\u6807\u6ce8\u5373\u53ef\u9ad8\u6548\u3001\u7cbe\u786e\u5730\u6807\u6ce8\u9065\u611f\u5f71\u50cf\uff0c\u4e3a\u9065\u611f\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u9065\u611f\u5f71\u50cf\u7684\u673a\u5668\u5b66\u4e60\u4f9d\u8d56\u4e8e\u6700\u65b0\u3001\u51c6\u786e\u7684\u6807\u7b7e\u6765\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u4f46\u4f20\u7edf\u7684\u6807\u6ce8\u65b9\u6cd5\u8017\u65f6\u8017\u8d39\u4eba\u529b\uff0c\u9700\u8981\u4e13\u5bb6\u8fdb\u884c\u5206\u6790\u3002", "method": "\u672c\u7814\u7a76\u5b9a\u4e49\u4e86\u4e00\u4e2a\u65e0\u76d1\u7763\u6d41\u7a0b\uff0c\u5229\u7528\u5206\u5272\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u6765\u5bfb\u627e\u548c\u6807\u6ce8Sentinel-2\u536b\u661f\u56fe\u50cf\u4e2d\u5177\u6709\u76f8\u4f3c\u4e0a\u4e0b\u6587\u548c\u5185\u5bb9\u7684\u5730\u7406\u533a\u57df\u3002\u901a\u8fc7\u5c06\u56fe\u50cf\u5206\u5272\u6210\u57fa\u4e8e\u989c\u8272\u548c\u7a7a\u95f4\u76f8\u4f3c\u6027\u7684\u540c\u7c7b\u50cf\u7d20\u533a\u57df\uff0c\u5e76\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u805a\u5408\u90bb\u8fd1\u533a\u57df\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684\u7279\u5f81\u8868\u793a\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u6807\u6ce8\u5de5\u5177\u4e2d\u7684\u5f02\u5e38\u503c\uff0c\u5141\u8bb8\u7528\u6237\u8fdb\u884c\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u5e76\u5728\u7f16\u7801\u7a7a\u95f4\u4e2d\u5f62\u6210\u65cb\u8f6c\u4e0d\u53d8\u7684\u56fe\u50cf\u7ea7\u8bed\u4e49\u5173\u7cfb\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7f16\u7801\u66f4\u9c81\u68d2\u7684\u7279\u5f81\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u56fe\u50cf\u6bd4\u8f83\u548c\u6807\u6ce8\uff0c\u51cf\u5c11\u4e86\u5f02\u5e38\u503c\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u5e76\u5efa\u7acb\u4e86\u56fe\u50cf\u7ea7\u7684\u65cb\u8f6c\u4e0d\u53d8\u8bed\u4e49\u5173\u7cfb\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u5229\u7528\u5206\u5272\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u6765\u7f16\u7801\u66f4\u9c81\u68d2\u7684\u7279\u5f81\u7a7a\u95f4\uff0c\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u7684\u6bd4\u8f83\u548c\u6807\u6ce8\uff0c\u514b\u670d\u4e86\u4ee5\u5f80\u4f9d\u8d56\u9884\u5148\u6807\u6ce8\u6570\u636e\u7684\u9650\u5236\u3002"}}
{"id": "2508.00754", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00754", "abs": "https://arxiv.org/abs/2508.00754", "authors": ["Yaxin Ma", "Benjamin Colburn", "Jose C. Principe"], "title": "A Simple and Effective Method for Uncertainty Quantification and OOD Detection", "comment": null, "summary": "Bayesian neural networks and deep ensemble methods have been proposed for\nuncertainty quantification; however, they are computationally intensive and\nrequire large storage. By utilizing a single deterministic model, we can solve\nthe above issue. We propose an effective method based on feature space density\nto quantify uncertainty for distributional shifts and out-of-distribution (OOD)\ndetection. Specifically, we leverage the information potential field derived\nfrom kernel density estimation to approximate the feature space density of the\ntraining set. By comparing this density with the feature space representation\nof test samples, we can effectively determine whether a distributional shift\nhas occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons\nand Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The\nresults demonstrate that our method outperforms baseline models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7a7a\u95f4\u5bc6\u5ea6\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u5206\u5e03\u504f\u79fb\u548c\u68c0\u6d4b OOD \u6570\u636e\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4fe1\u606f\u52bf\u573a\u6765\u8fd1\u4f3c\u8bad\u7ec3\u96c6\u7684\u7279\u5f81\u7a7a\u95f4\u5bc6\u5ea6\uff0c\u5e76\u901a\u8fc7\u6bd4\u8f83\u6d4b\u8bd5\u6837\u672c\u7684\u7279\u5f81\u7a7a\u95f4\u8868\u793a\u6765\u68c0\u6d4b\u5206\u5e03\u504f\u79fb\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u96c6\u6210\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u4e14\u9700\u8981\u5927\u91cf\u5b58\u50a8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u5229\u7528\u5355\u4e00\u786e\u5b9a\u6027\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u8be5\u5bc6\u5ea6\u4e0e\u6d4b\u8bd5\u6837\u672c\u7684\u7279\u5f81\u7a7a\u95f4\u8868\u793a\uff0c\u6765\u5224\u65ad\u662f\u5426\u53d1\u751f\u4e86\u5206\u5e03\u504f\u79fb\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4ece\u6838\u5bc6\u5ea6\u4f30\u8ba1\u4e2d\u63d0\u53d6\u7684\u4fe1\u606f\u52bf\u573a\u6765\u8fd1\u4f3c\u8bad\u7ec3\u96c6\u7684\u7279\u5f81\u7a7a\u95f4\u5bc6\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728 OOD \u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e8c\u7ef4\u5408\u6210\u6570\u636e\u96c6\uff08Two Moons \u548c Three Spirals\uff09\u4ee5\u53ca OOD \u68c0\u6d4b\u4efb\u52a1\uff08CIFAR-10 vs. SVHN\uff09\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2508.00758", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00758", "abs": "https://arxiv.org/abs/2508.00758", "authors": ["Timur Sattarov", "Marco Schreyer", "Damian Borth"], "title": "Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data", "comment": "22 pages, 16 figures, 7 tables, preprint version", "summary": "Anomaly detection in tabular data remains challenging due to complex feature\ninteractions and the scarcity of anomalous examples. Denoising autoencoders\nrely on fixed-magnitude noise, limiting adaptability to diverse data\ndistributions. Diffusion models introduce scheduled noise and iterative\ndenoising, but lack explicit reconstruction mappings. We propose the\nDiffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates\ndiffusion-based noise scheduling and contrastive learning into the encoding\nprocess to improve anomaly detection. We evaluated DDAE on 57 datasets from\nADBench. Our method outperforms in semi-supervised settings and achieves\ncompetitive results in unsupervised settings, improving PR-AUC by up to 65%\n(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)\nmodel baselines. We observed that higher noise levels benefit unsupervised\ntraining, while lower noise with linear scheduling is optimal in\nsemi-supervised settings. These findings underscore the importance of\nprincipled noise strategies in tabular anomaly detection.", "AI": {"tldr": "DDAE improves tabular anomaly detection by combining diffusion noise scheduling and contrastive learning, outperforming existing methods, especially in semi-supervised scenarios. Optimal noise strategies are crucial for performance.", "motivation": "Anomaly detection in tabular data is challenging due to complex feature interactions and scarce anomalous examples. Existing methods like denoising autoencoders and diffusion models have limitations in adapting to diverse data distributions and providing explicit reconstruction mappings.", "method": "The proposed Diffusion-Scheduled Denoising Autoencoder (DDAE) framework integrates diffusion-based noise scheduling and contrastive learning into the encoding process.", "result": "DDAE outperforms state-of-the-art autoencoder and diffusion models in semi-supervised settings (improving PR-AUC by up to 65% and ROC-AUC by 16%) and achieves competitive results in unsupervised settings (improving PR-AUC by 9% and ROC-AUC by 6%). Higher noise levels benefit unsupervised training, while lower noise with linear scheduling is optimal for semi-supervised settings.", "conclusion": "Denoising autoencoders and diffusion models have limitations in tabular anomaly detection due to fixed noise and lack of explicit reconstruction mappings, respectively. DDAE integrates diffusion-based noise scheduling and contrastive learning to improve anomaly detection, outperforming existing methods in semi-supervised settings and achieving competitive results in unsupervised settings. The study highlights the importance of noise strategies for tabular anomaly detection."}}
{"id": "2508.00528", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00528", "abs": "https://arxiv.org/abs/2508.00528", "authors": ["Jinsong Yang", "Zeyuan Hu", "Yichen Li"], "title": "EPANet: Efficient Path Aggregation Network for Underwater Fish Detection", "comment": null, "summary": "Underwater fish detection (UFD) remains a challenging task in computer vision\ndue to low object resolution, significant background interference, and high\nvisual similarity between targets and surroundings. Existing approaches\nprimarily focus on local feature enhancement or incorporate complex attention\nmechanisms to highlight small objects, often at the cost of increased model\ncomplexity and reduced efficiency. To address these limitations, we propose an\nefficient path aggregation network (EPANet), which leverages complementary\nfeature integration to achieve accurate and lightweight UFD. EPANet consists of\ntwo key components: an efficient path aggregation feature pyramid network\n(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP\nbottleneck). The EPA-FPN introduces long-range skip connections across\ndisparate scales to improve semantic-spatial complementarity, while cross-layer\nfusion paths are adopted to enhance feature integration efficiency. The MS-DDSP\nbottleneck extends the conventional bottleneck structure by introducing\nfiner-grained feature division and diverse convolutional operations, thereby\nincreasing local feature diversity and representation capacity. Extensive\nexperiments on benchmark UFD datasets demonstrate that EPANet outperforms\nstate-of-the-art methods in terms of detection accuracy and inference speed,\nwhile maintaining comparable or even lower parameter complexity.", "AI": {"tldr": "EPANet\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6c34\u4e0b\u9c7c\u7c7b\u68c0\u6d4b\u7f51\u7edc\uff0c\u901a\u8fc7\u7279\u5f81\u878d\u5408\u548c\u4f18\u5316\u7684\u74f6\u9888\u7ed3\u6784\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u6c34\u4e0b\u9c7c\u7c7b\u68c0\u6d4b\uff08UFD\uff09\u65b9\u6cd5\u5728\u4f4e\u7269\u4f53\u5206\u8fa8\u7387\u3001\u663e\u8457\u7684\u80cc\u666f\u5e72\u6270\u548c\u76ee\u6807\u4e0e\u5468\u56f4\u73af\u5883\u4e4b\u95f4\u9ad8\u89c6\u89c9\u76f8\u4f3c\u6027\u65b9\u9762\u5b58\u5728\u7684\u6311\u6218\uff0c\u5e76\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u5c40\u90e8\u7279\u5f81\u589e\u5f3a\u6216\u590d\u6742\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4\u6a21\u578b\u590d\u6742\u6027\u589e\u52a0\u548c\u6548\u7387\u964d\u4f4e\u7684\u5c40\u9650\u6027\u3002", "method": "EPANet\u5305\u542b\u4e24\u4e2a\u5173\u952e\u90e8\u5206\uff1a\u9ad8\u6548\u8def\u5f84\u805a\u5408\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff08EPA-FPN\uff09\u548c\u591a\u5c3a\u5ea6\u591a\u6837\u5316\u77ed\u8def\u5f84\u74f6\u9888\uff08MS-DDSP bottleneck\uff09\u3002EPA-FPN\u5f15\u5165\u8de8\u4e0d\u540c\u5c3a\u5ea6\u7684\u957f\u7a0b\u8df3\u8dc3\u8fde\u63a5\u4ee5\u589e\u5f3a\u8bed\u4e49-\u7a7a\u95f4\u4e92\u8865\u6027\uff0c\u5e76\u91c7\u7528\u8de8\u5c42\u878d\u5408\u8def\u5f84\u63d0\u9ad8\u7279\u5f81\u878d\u5408\u6548\u7387\u3002MS-DDSP\u74f6\u9888\u901a\u8fc7\u5f15\u5165\u66f4\u7ec6\u7c92\u5ea6\u7684\u7279\u5f81\u5212\u5206\u548c\u591a\u6837\u5316\u7684\u5377\u79ef\u64cd\u4f5c\u6765\u6269\u5c55\u4f20\u7edf\u7684\u74f6\u9888\u7ed3\u6784\uff0c\u4ece\u800c\u589e\u5f3a\u5c40\u90e8\u7279\u5f81\u591a\u6837\u6027\u548c\u8868\u793a\u80fd\u529b\u3002", "result": "EPANet\u5728\u57fa\u51c6UFD\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "EPANet\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u751a\u81f3\u66f4\u4f4e\u7684\u53c2\u6570\u590d\u6742\u6027\u3002"}}
{"id": "2508.00768", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00768", "abs": "https://arxiv.org/abs/2508.00768", "authors": ["Antonio Tudisco", "Andrea Marchesin", "Maurizio Zamboni", "Mariagrazia Graziano", "Giovanna Turvani"], "title": "Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy", "comment": null, "summary": "Recent advancements in Quantum Computing and Machine Learning have increased\nattention to Quantum Machine Learning (QML), which aims to develop machine\nlearning models by exploiting the quantum computing paradigm. One of the widely\nused models in this area is the Variational Quantum Circuit (VQC), a hybrid\nmodel where the quantum circuit handles data inference while classical\noptimization adjusts the parameters of the circuit. The quantum circuit\nconsists of an encoding layer, which loads data into the circuit, and a\ntemplate circuit, known as the ansatz, responsible for processing the data.\nThis work involves performing an analysis by considering both Amplitude- and\nAngle-encoding models, and examining how the type of rotational gate applied\naffects the classification performance of the model. This comparison is carried\nout by training the different models on two datasets, Wine and Diabetes, and\nevaluating their performance. The study demonstrates that, under identical\nmodel topologies, the difference in accuracy between the best and worst models\nranges from 10% to 30%, with differences reaching up to 41%. Moreover, the\nresults highlight how the choice of rotational gates used in encoding can\nsignificantly impact the model's classification performance. The findings\nconfirm that the embedding represents a hyperparameter for VQC models.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u4e0d\u540c\u7f16\u7801\u6a21\u578b\u548c\u65cb\u8f6c\u95e8\u5bf9\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQC\uff09\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u65cb\u8f6c\u95e8\u7684\u9009\u62e9\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u4e14\u5d4c\u5165\u662fVQC\u6a21\u578b\u7684\u4e00\u4e2a\u8d85\u53c2\u6570\u3002", "motivation": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u662f\u91cf\u5b50\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e00\u4e2a\u4ea4\u53c9\u5b66\u79d1\uff0c\u65e8\u5728\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u8303\u5f0f\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQC\uff09\u662fQML\u4e2d\u5e38\u7528\u7684\u6a21\u578b\uff0c\u5b83\u7ed3\u5408\u4e86\u91cf\u5b50\u7535\u8def\u548c\u7ecf\u5178\u4f18\u5316\u3002\u672c\u7814\u7a76\u65e8\u5728\u5206\u6790\u4e0d\u540c\u7f16\u7801\u6a21\u578b\u548c\u65cb\u8f6c\u95e8\u5bf9VQC\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u5e45\u5ea6\u7f16\u7801\u548c\u89d2\u5ea6\u7f16\u7801\u6a21\u578b\uff0c\u5e76\u68c0\u67e5\u4e86\u65cb\u8f6c\u95e8\u7c7b\u578b\u5bf9\u6a21\u578b\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5728Wine\u548cDiabetes\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e0d\u540c\u7684\u6a21\u578b\u5e76\u8bc4\u4f30\u5b83\u4eec\u7684\u6027\u80fd\u6765\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728\u76f8\u540c\u7684\u6a21\u578b\u62d3\u6251\u4e0b\uff0c\u6700\u4f73\u6a21\u578b\u548c\u6700\u5dee\u6a21\u578b\u4e4b\u95f4\u7684\u51c6\u786e\u7387\u5dee\u5f02\u8303\u56f4\u4e3a10%\u81f330%\uff0c\u6700\u9ad8\u53ef\u8fbe41%\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u7f16\u7801\u4e2d\u4f7f\u7528\u7684\u65cb\u8f6c\u95e8\u7684\u9009\u62e9\u5bf9\u6a21\u578b\u5206\u7c7b\u6027\u80fd\u7684\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\u4e86\u5d4c\u5165\u662f\u53d8\u5206\u91cf\u5b50\u8ba1\u7b97\uff08VQC\uff09\u6a21\u578b\u7684\u4e00\u4e2a\u8d85\u53c2\u6570\uff0c\u5e76\u4e14\u65cb\u8f6c\u95e8\u7684\u9009\u62e9\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u7684\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2508.00548", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00548", "abs": "https://arxiv.org/abs/2508.00548", "authors": ["Seunghyun Shin", "Dongmin Shin", "Jisu Shin", "Hae-Gon Jeon", "Joon-Young Lee"], "title": "Video Color Grading via Look-Up Table Generation", "comment": "ICCV2025", "summary": "Different from color correction and transfer, color grading involves\nadjusting colors for artistic or storytelling purposes in a video, which is\nused to establish a specific look or mood. However, due to the complexity of\nthe process and the need for specialized editing skills, video color grading\nremains primarily the domain of professional colorists. In this paper, we\npresent a reference-based video color grading framework. Our key idea is\nexplicitly generating a look-up table (LUT) for color attribute alignment\nbetween reference scenes and input video via a diffusion model. As a training\nobjective, we enforce that high-level features of the reference scenes like\nlook, mood, and emotion should be similar to that of the input video. Our\nLUT-based approach allows for color grading without any loss of structural\ndetails in the whole video frames as well as achieving fast inference. We\nfurther build a pipeline to incorporate a user-preference via text prompts for\nlow-level feature enhancement such as contrast and brightness, etc.\nExperimental results, including extensive user studies, demonstrate the\neffectiveness of our approach for video color grading. Codes are publicly\navailable at https://github.com/seunghyuns98/VideoColorGrading.", "AI": {"tldr": "A new framework simplifies video color grading using a diffusion model to create a LUT for matching reference looks, preserving details and speeding up the process, with added features for user customization via text prompts.", "motivation": "Video color grading, used for artistic or storytelling purposes to establish a specific look or mood, is typically complex and requires specialized skills, limiting it to professional colorists. This paper aims to simplify and democratize the process.", "method": "The paper proposes a reference-based video color grading framework that utilizes a diffusion model to explicitly generate a look-up table (LUT) for color attribute alignment between reference scenes and input video. The training objective enforces similarity in high-level features (look, mood, emotion) between the reference and input. A pipeline is also developed to incorporate user preferences via text prompts for low-level feature enhancement.", "result": "Experimental results and extensive user studies demonstrate the effectiveness of the proposed approach for video color grading, highlighting its ability to maintain structural details and achieve fast inference.", "conclusion": "The proposed reference-based video color grading framework effectively achieves desired artistic looks and moods by generating a LUT via a diffusion model, ensuring no loss of structural details and enabling fast inference. User studies confirm its effectiveness, and a pipeline for incorporating text prompts allows for low-level feature enhancement."}}
{"id": "2508.00785", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00785", "abs": "https://arxiv.org/abs/2508.00785", "authors": ["Bushra Akter", "Md Biplob Hosen", "Sabbir Ahmed", "Mehrin Anannya", "Md. Farhad Hossain"], "title": "Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors", "comment": null, "summary": "Academic performance depends on a multivariable nexus of socio-academic and\nfinancial factors. This study investigates these influences to develop\neffective strategies for optimizing students' CGPA. To achieve this, we\nreviewed various literature to identify key influencing factors and constructed\nan initial hypothetical causal graph based on the findings. Additionally, an\nonline survey was conducted, where 1,050 students participated, providing\ncomprehensive data for analysis. Rigorous data preprocessing techniques,\nincluding cleaning and visualization, ensured data quality before analysis.\nCausal analysis validated the relationships among variables, offering deeper\ninsights into their direct and indirect effects on CGPA. Regression models were\nimplemented for CGPA prediction, while classification models categorized\nstudents based on performance levels. Ridge Regression demonstrated strong\npredictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared\nError of 0.023. Random Forest outperformed in classification, attaining an\nF1-score near perfection and an accuracy of 98.68%. Explainable AI techniques\nsuch as SHAP, LIME, and Interpret enhanced model interpretability, highlighting\ncritical factors such as study hours, scholarships, parental education, and\nprior academic performance. The study culminated in the development of a\nweb-based application that provides students with personalized insights,\nallowing them to predict academic performance, identify areas for improvement,\nand make informed decisions to enhance their outcomes.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u5f71\u54cd\u5b66\u4e1a\u6210\u7ee9\u7684\u591a\u79cd\u56e0\u7d20\uff0c\u5e76\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u548c\u5206\u7c7b\uff0c\u6700\u7ec8\u5f00\u53d1\u4e86\u4e00\u4e2a\u5e2e\u52a9\u5b66\u751f\u63d0\u5347\u5b66\u4e1a\u6210\u7ee9\u7684\u7f51\u7edc\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u4f18\u5316\u5b66\u751f\u7684\u5e73\u5747\u6210\u7ee9\uff08CGPA\uff09\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5f71\u54cd\u5b66\u4e1a\u6210\u7ee9\u7684\u793e\u4f1a\u3001\u7ecf\u6d4e\u548c\u5b66\u672f\u7b49\u591a\u53d8\u91cf\u56e0\u7d20\uff0c\u5e76\u5f00\u53d1\u6709\u6548\u7684\u7b56\u7565\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u786e\u5b9a\u5f71\u54cd\u5b66\u751f\u5b66\u4e1a\u6210\u7ee9\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u6784\u5efa\u4e86\u521d\u59cb\u7684\u56e0\u679c\u56fe\u3002\u968f\u540e\uff0c\u901a\u8fc7\u5bf91050\u540d\u5b66\u751f\u8fdb\u884c\u5728\u7ebf\u8c03\u67e5\u6536\u96c6\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\u3002\u63a5\u7740\uff0c\u5229\u7528\u56e0\u679c\u5206\u6790\u9a8c\u8bc1\u53d8\u91cf\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u91c7\u7528\u5cad\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u5b66\u4e1a\u6210\u7ee9\u9884\u6d4b\uff08\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a0.12\uff0c\u5747\u65b9\u8bef\u5dee\u4e3a0.023\uff09\uff0c\u4ee5\u53ca\u968f\u673a\u68ee\u6797\u6a21\u578b\u8fdb\u884c\u5b66\u4e1a\u6210\u7ee9\u5206\u7c7b\uff08F1\u5206\u6570\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u51c6\u786e\u7387\u4e3a98.68%\uff09\u3002\u6700\u540e\uff0c\u5229\u7528SHAP\u3001LIME\u548cInterpret\u7b49\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7f51\u7edc\u5e94\u7528\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u51fa\u5b66\u4e60\u65f6\u6570\u3001\u5956\u5b66\u91d1\u3001\u7236\u6bcd\u6559\u80b2\u6c34\u5e73\u548c\u4ee5\u5f80\u5b66\u4e1a\u6210\u7ee9\u662f\u5f71\u54cdCGPA\u7684\u5173\u952e\u56e0\u7d20\u3002\u5cad\u56de\u5f52\u6a21\u578b\u5728\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u5206\u7c7b\u65b9\u9762\u51c6\u786e\u7387\u9ad8\u8fbe98.68%\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u793e\u4f1a\u3001\u7ecf\u6d4e\u548c\u5b66\u672f\u56e0\u7d20\u7684\u7efc\u5408\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u7f51\u7edc\u5e94\u7528\u4e3a\u5b66\u751f\u63d0\u4f9b\u4e2a\u6027\u5316\u6307\u5bfc\uff0c\u4ee5\u4f18\u5316\u5176\u5b66\u4e1a\u6210\u7ee9\u3002"}}
{"id": "2508.00549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00549", "abs": "https://arxiv.org/abs/2508.00549", "authors": ["Daniel Wolf", "Heiko Hillenhagen", "Billurvan Taskin", "Alex B\u00e4uerle", "Meinrad Beer", "Michael G\u00f6tz", "Timo Ropinski"], "title": "Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images", "comment": "Accepted at the International Conference on Medical Image Computing\n  and Computer Assisted Intervention (MICCAI) 2025", "summary": "Clinical decision-making relies heavily on understanding relative positions\nof anatomical structures and anomalies. Therefore, for Vision-Language Models\n(VLMs) to be applicable in clinical practice, the ability to accurately\ndetermine relative positions on medical images is a fundamental prerequisite.\nDespite its importance, this capability remains highly underexplored. To\naddress this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o,\nLlama3.2, Pixtral, and JanusPro, and find that all models fail at this\nfundamental task. Inspired by successful approaches in computer vision, we\ninvestigate whether visual prompts, such as alphanumeric or colored markers\nplaced on anatomical structures, can enhance performance. While these markers\nprovide moderate improvements, results remain significantly lower on medical\nimages compared to observations made on natural images. Our evaluations suggest\nthat, in medical imaging, VLMs rely more on prior anatomical knowledge than on\nactual image content for answering relative position questions, often leading\nto incorrect conclusions. To facilitate further research in this area, we\nintroduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset,\ndesigned to systematically evaluate the capability to identify relative\npositions in medical images.", "AI": {"tldr": "\u5148\u8fdb\u7684VLM\u5728\u533b\u5b66\u56fe\u50cf\u76f8\u5bf9\u4f4d\u7f6e\u5224\u65ad\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5373\u4f7f\u6709\u89c6\u89c9\u63d0\u793a\u4e5f\u6548\u679c\u6709\u9650\uff0c\u4e3b\u8981\u4f9d\u8d56\u5148\u9a8c\u77e5\u8bc6\u800c\u975e\u56fe\u50cf\u5185\u5bb9\u3002\u6211\u4eec\u63d0\u51fa\u4e86MIRP\u6570\u636e\u96c6\u6765\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u51c6\u786e\u5224\u65ad\u533b\u5b66\u56fe\u50cf\u4e2d\u89e3\u5256\u7ed3\u6784\u548c\u5f02\u5e38\u7684\u76f8\u5bf9\u4f4d\u7f6e\u662f\u57fa\u7840\u524d\u63d0\uff0c\u4f46\u76ee\u524d\u8be5\u80fd\u529b\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u8bc4\u4f30\u4e86GPT-4o\u3001Llama3.2\u3001Pixtral\u548cJanusPro\u7b49\u5148\u8fdbVLM\u5728\u533b\u5b66\u56fe\u50cf\u4e0a\u7684\u76f8\u5bf9\u4f4d\u7f6e\u5224\u65ad\u80fd\u529b\u3002\u5c1d\u8bd5\u4f7f\u7528\u5b57\u6bcd\u6570\u5b57\u6216\u5f69\u8272\u6807\u8bb0\u7b49\u89c6\u89c9\u63d0\u793a\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u6548\u679c\u6709\u9650\u3002", "result": "\u6240\u6709\u88ab\u8bc4\u4f30\u7684VLM\u5728\u533b\u5b66\u56fe\u50cf\u76f8\u5bf9\u4f4d\u7f6e\u5224\u65ad\u4efb\u52a1\u4e0a\u5747\u5931\u8d25\u3002\u89c6\u89c9\u63d0\u793a\uff08\u5982\u6807\u8bb0\uff09\u53ea\u80fd\u63d0\u4f9b\u9002\u5ea6\u6539\u5584\uff0c\u4e14\u5728\u533b\u5b66\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\u8fdc\u4e0d\u5982\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u3002", "conclusion": "\u5f53\u524d\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u533b\u5b66\u56fe\u50cf\u7684\u76f8\u5bf9\u4f4d\u7f6e\u5224\u65ad\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5b83\u4eec\u66f4\u591a\u5730\u4f9d\u8d56\u5148\u9a8c\u89e3\u5256\u77e5\u8bc6\u800c\u975e\u56fe\u50cf\u5185\u5bb9\uff0c\u5bfc\u81f4\u7ed3\u8bba\u4e0d\u51c6\u786e\u3002\u5f15\u5165MIRP\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2508.00552", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00552", "abs": "https://arxiv.org/abs/2508.00552", "authors": ["Chihan Huang", "Belal Alsinglawi", "Islam Al-qudah"], "title": "DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification", "comment": null, "summary": "Recent advances in deep neural networks (DNNs) have led to remarkable success\nacross a wide range of tasks. However, their susceptibility to adversarial\nperturbations remains a critical vulnerability. Existing diffusion-based\nadversarial purification methods often require intensive iterative denoising,\nseverely limiting their practical deployment. In this paper, we propose\nDiffusion Bridge Distillation for Purification (DBLP), a novel and efficient\ndiffusion-based framework for adversarial purification. Central to our approach\nis a new objective, noise bridge distillation, which constructs a principled\nalignment between the adversarial noise distribution and the clean data\ndistribution within a latent consistency model (LCM). To further enhance\nsemantic fidelity, we introduce adaptive semantic enhancement, which fuses\nmulti-scale pyramid edge maps as conditioning input to guide the purification\nprocess. Extensive experiments across multiple datasets demonstrate that DBLP\nachieves state-of-the-art (SOTA) robust accuracy, superior image quality, and\naround 0.2s inference time, marking a significant step toward real-time\nadversarial purification.", "AI": {"tldr": "DBLP \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u6269\u6563\u7684\u5bf9\u6297\u6027\u51c0\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u6865\u84b8\u998f\u548c\u81ea\u9002\u5e94\u8bed\u4e49\u589e\u5f3a\uff0c\u5728\u4fdd\u8bc1\u9ad8\u51c6\u786e\u7387\u548c\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b9e\u65f6\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u5bf9\u6297\u6027\u51c0\u5316\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5bc6\u96c6\u7684\u8fed\u4ee3\u53bb\u566a\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u6269\u6563\u7684\u51c0\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DBLP\uff08Diffusion Bridge Distillation for Purification\uff09\u7684\u65b0\u578b\u9ad8\u6548\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\u6765\u8fdb\u884c\u5bf9\u6297\u6027\u51c0\u5316\u3002\u5176\u6838\u5fc3\u662f\u4e00\u79cd\u65b0\u7684\u76ee\u6807\u2014\u2014\u566a\u58f0\u6865\u84b8\u998f\uff0c\u5b83\u5728\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b (LCM) \u4e2d\u6784\u5efa\u4e86\u5bf9\u6297\u6027\u566a\u58f0\u5206\u5e03\u4e0e\u5e72\u51c0\u6570\u636e\u5206\u5e03\u4e4b\u95f4\u7684\u539f\u5219\u6027\u5bf9\u9f50\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u589e\u5f3a\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u8bed\u4e49\u589e\u5f3a\uff0c\u5c06\u591a\u5c3a\u5ea6\u91d1\u5b57\u5854\u8fb9\u7f18\u56fe\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\u6765\u6307\u5bfc\u51c0\u5316\u8fc7\u7a0b\u3002", "result": "DBLP \u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u5728\u9c81\u68d2\u7cbe\u5ea6\u3001\u56fe\u50cf\u8d28\u91cf\u548c\u63a8\u7406\u65f6\u95f4\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DBLP \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u7cbe\u5ea6\u3001\u5353\u8d8a\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u5177\u6709\u7ea6 0.2 \u79d2\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u6807\u5fd7\u7740\u5176\u5728\u5b9e\u65f6\u5bf9\u6297\u6027\u51c0\u5316\u65b9\u9762\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.00553", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00553", "abs": "https://arxiv.org/abs/2508.00553", "authors": ["Jizhihui Liu", "Feiyi Du", "Guangdao Zhu", "Niu Lian", "Jun Li", "Bin Chen"], "title": "HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) encode images into lengthy sequences of visual\ntokens, leading to excessive computational overhead and limited inference\nefficiency. While prior efforts prune or merge tokens to address this issue,\nthey often rely on special tokens (e.g., CLS) or require task-specific\ntraining, hindering scalability across architectures. In this paper, we propose\nHiPrune, a training-free and model-agnostic token Pruning framework that\nexploits the Hierarchical attention structure within vision encoders. We\nidentify that middle layers attend to object-centric regions, while deep layers\ncapture global contextual features. Based on this observation, HiPrune selects\nthree types of informative tokens: (1) Anchor tokens with high attention in\nobject-centric layers, (2) Buffer tokens adjacent to anchors for spatial\ncontinuity, and (3) Register tokens with strong attention in deep layers for\nglobal summarization. Our method requires no retraining and integrates\nseamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,\nLLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art\npruning performance, preserving up to 99.3% task accuracy with only 33.3%\ntokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it\nreduces inference FLOPs and latency by up to 9$\\times$, showcasing strong\ngeneralization across models and tasks. Code is available at\nhttps://github.com/Danielement321/HiPrune.", "AI": {"tldr": "HiPrune \u662f\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9 token \u526a\u679d\u6846\u67b6\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u51cf\u5c11\u6a21\u578b\u4e2d\u7684 token \u6570\u91cf\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u957f\u5e8f\u5217\u7684\u89c6\u89c9 token\uff0c\u5bfc\u81f4\u8fc7\u9ad8\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u6709\u9650\u7684\u63a8\u7406\u6548\u7387\u3002\u5148\u524d\u7684\u526a\u679d\u6216\u5408\u5e76 token \u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7279\u6b8a token \u6216\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u8de8\u67b6\u6784\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "HiPrune \u6846\u67b6\u5229\u7528\u89c6\u89c9\u7f16\u7801\u5668\u5185\u7684\u5206\u5c42\u6ce8\u610f\u529b\u7ed3\u6784\u3002\u901a\u8fc7\u8bc6\u522b\u51fa\u4e2d\u5c42\u5173\u6ce8\u5bf9\u8c61\u4e2d\u5fc3\u533a\u57df\uff0c\u6df1\u5c42\u6355\u6349\u5168\u5c40\u4e0a\u4e0b\u6587\u7279\u5f81\uff0cHiPrune \u9009\u53d6\u4e09\u79cd\u4fe1\u606f token\uff1a(1) \u5728\u5bf9\u8c61\u4e2d\u5fc3\u5c42\u5177\u6709\u9ad8\u6ce8\u610f\u529b\u7684\u951a\u70b9 token\uff0c(2) \u90bb\u8fd1\u951a\u70b9\u7684\u7f13\u51b2\u533a token \u4ee5\u4fdd\u6301\u7a7a\u95f4\u8fde\u7eed\u6027\uff0c(3) \u5728\u6df1\u5c42\u5177\u6709\u5f3a\u6ce8\u610f\u529b\u7684\u5bc4\u5b58\u5668 token \u4ee5\u8fdb\u884c\u5168\u5c40\u603b\u7ed3\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u53ef\u4e0e\u4efb\u4f55\u57fa\u4e8e ViT \u7684 VLM \u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728 LLaVA-1.5\u3001LLaVA-NeXT \u548c Qwen2.5-VL \u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHiPrune \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u526a\u679d\u6027\u80fd\uff0c\u5728\u4ec5\u4f7f\u7528 33.3% \u7684 token \u65f6\u53ef\u4fdd\u7559\u9ad8\u8fbe 99.3% \u7684\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u4f7f\u7528 11.1% \u7684 token \u65f6\u53ef\u4fdd\u6301 99.5% \u7684\u51c6\u786e\u6027\u3002\u540c\u65f6\uff0c\u63a8\u7406 FLOPs \u548c\u5ef6\u8fdf\u964d\u4f4e\u4e86\u9ad8\u8fbe 9 \u500d\u3002", "conclusion": "HiPrune \u662f\u4e00\u79cd\u8bad\u7ec3\u65e0\u5173\u4e14\u4e0e\u6a21\u578b\u65e0\u5173\u7684 token \u526a\u679d\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u7f16\u7801\u5668\u5185\u7684\u5206\u5c42\u6ce8\u610f\u529b\u7ed3\u6784\uff0c\u80fd\u591f\u5728\u663e\u8457\u51cf\u5c11 token \u6570\u91cf\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u9ad8\u8fbe 99.3% \u7684\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u5e76\u5927\u5e45\u964d\u4f4e\u63a8\u7406 FLOPs \u548c\u5ef6\u8fdf\uff08\u9ad8\u8fbe 9 \u500d\uff09\uff0c\u5c55\u793a\u4e86\u8de8\u6a21\u578b\u548c\u4efb\u52a1\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.00591", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.00591", "abs": "https://arxiv.org/abs/2508.00591", "authors": ["Mingrui Liu", "Sixiao Zhang", "Cheng Long"], "title": "Wukong Framework for Not Safe For Work Detection in Text-to-Image systems", "comment": "Under review", "summary": "Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)\ntechnology enabling diverse and creative image synthesis. However, some outputs\nmay contain Not Safe For Work (NSFW) content (e.g., violence), violating\ncommunity guidelines. Detecting NSFW content efficiently and accurately, known\nas external safeguarding, is essential. Existing external safeguards fall into\ntwo types: text filters, which analyze user prompts but overlook T2I\nmodel-specific variations and are prone to adversarial attacks; and image\nfilters, which analyze final generated images but are computationally costly\nand introduce latency. Diffusion models, the foundation of modern T2I systems\nlike Stable Diffusion, generate images through iterative denoising using a\nU-Net architecture with ResNet and Transformer blocks. We observe that: (1)\nearly denoising steps define the semantic layout of the image, and (2)\ncross-attention layers in U-Net are crucial for aligning text and image\nregions. Based on these insights, we propose Wukong, a transformer-based NSFW\ndetection framework that leverages intermediate outputs from early denoising\nsteps and reuses U-Net's pre-trained cross-attention parameters. Wukong\noperates within the diffusion process, enabling early detection without waiting\nfor full image generation. We also introduce a new dataset containing prompts,\nseeds, and image-specific NSFW labels, and evaluate Wukong on this and two\npublic benchmarks. Results show that Wukong significantly outperforms\ntext-based safeguards and achieves comparable accuracy of image filters, while\noffering much greater efficiency.", "AI": {"tldr": "Wukong\u662f\u4e00\u79cd\u65b0\u7684NSFW\u68c0\u6d4b\u6846\u67b6\uff0c\u5b83\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u4e2d\u95f4\u8f93\u51fa\u6765\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684NSFW\u5185\u5bb9\u68c0\u6d4b\u65b9\u6cd5\uff08\u6587\u672c\u8fc7\u6ee4\u5668\u548c\u56fe\u50cf\u8fc7\u6ee4\u5668\uff09\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\uff1a\u6587\u672c\u8fc7\u6ee4\u5668\u65e0\u6cd5\u5904\u7406T2I\u6a21\u578b\u7279\u6709\u7684\u53d8\u5f02\uff0c\u5e76\u4e14\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\uff1b\u56fe\u50cf\u8fc7\u6ee4\u5668\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4f1a\u589e\u52a0\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684NSFW\u68c0\u6d4b\u6846\u67b6Wukong\uff0c\u8be5\u6846\u67b6\u5229\u7528\u65e9\u671f\u53bb\u566a\u6b65\u9aa4\u7684\u4e2d\u95f4\u8f93\u51fa\uff0c\u5e76\u91cd\u65b0\u5229\u7528U-Net\u9884\u8bad\u7ec3\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u53c2\u6570\u3002Wukong\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u64cd\u4f5c\uff0c\u80fd\u591f\u5728\u4e0d\u7b49\u5f85\u5b8c\u6574\u56fe\u50cf\u751f\u6210\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u65e9\u671f\u68c0\u6d4b\u3002", "result": "Wukong\u5728\u65b0\u7684\u5305\u542b\u63d0\u793a\u3001\u79cd\u5b50\u548c\u56fe\u50cf\u7279\u5b9aNSFW\u6807\u7b7e\u7684\u6570\u636e\u96c6\u4ee5\u53ca\u4e24\u4e2a\u516c\u5f00\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u6587\u672c\u5b89\u5168\u9632\u62a4\uff0c\u4e14\u6548\u7387\u8fdc\u9ad8\u4e8e\u56fe\u50cf\u8fc7\u6ee4\u5668\u3002", "conclusion": "Wukong\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u7684\u5b89\u5168\u9632\u62a4\uff0c\u5e76\u4e14\u5728\u51c6\u786e\u6027\u4e0a\u4e0e\u57fa\u4e8e\u56fe\u50cf\u7684\u5b89\u5168\u9632\u62a4\u76f8\u5f53\uff0c\u540c\u65f6\u6548\u7387\u66f4\u9ad8\u3002"}}
{"id": "2508.00557", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00557", "abs": "https://arxiv.org/abs/2508.00557", "authors": ["Qi Chen", "Lingxiao Yang", "Yun Chen", "Nailong Zhao", "Jianhuang Lai", "Jie Shao", "Xiaohua Xie"], "title": "Training-Free Class Purification for Open-Vocabulary Semantic Segmentation", "comment": "Accepted to ICCV 2025", "summary": "Fine-tuning pre-trained vision-language models has emerged as a powerful\napproach for enhancing open-vocabulary semantic segmentation (OVSS). However,\nthe substantial computational and resource demands associated with training on\nlarge datasets have prompted interest in training-free methods for OVSS.\nExisting training-free approaches primarily focus on modifying model\narchitectures and generating prototypes to improve segmentation performance.\nHowever, they often neglect the challenges posed by class redundancy, where\nmultiple categories are not present in the current test image, and\nvisual-language ambiguity, where semantic similarities among categories create\nconfusion in class activation. These issues can lead to suboptimal class\nactivation maps and affinity-refined activation maps. Motivated by these\nobservations, we propose FreeCP, a novel training-free class purification\nframework designed to address these challenges. FreeCP focuses on purifying\nsemantic categories and rectifying errors caused by redundancy and ambiguity.\nThe purified class representations are then leveraged to produce final\nsegmentation predictions. We conduct extensive experiments across eight\nbenchmarks to validate FreeCP's effectiveness. Results demonstrate that FreeCP,\nas a plug-and-play module, significantly boosts segmentation performance when\ncombined with other OVSS methods.", "AI": {"tldr": "FreeCP \u662f\u4e00\u79cd\u65b0\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u51c0\u5316\u7c7b\u522b\u8868\u793a\u6765\u89e3\u51b3\u7c7b\u522b\u5197\u4f59\u548c\u6b67\u4e49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65e0\u9700\u8bad\u7ec3\u7684 OVSS \u65b9\u6cd5\u5728\u5904\u7406\u7c7b\u522b\u5197\u4f59\u548c\u89c6\u89c9-\u8bed\u8a00\u6b67\u4e49\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u7684\u7c7b\u522b\u6fc0\u6d3b\u56fe\u548c\u4eb2\u548c\u529b\u7ec6\u5316\u6fc0\u6d3b\u56fe\u3002Motivated by these observations, we propose FreeCP, a novel training-free class purification framework designed to address these challenges.", "method": "FreeCP \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u7c7b\u522b\u51c0\u5316\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u51c0\u5316\u8bed\u4e49\u7c7b\u522b\u548c\u7ea0\u6b63\u7531\u5197\u4f59\u548c\u6b67\u4e49\u5f15\u8d77 \u7684\u9519\u8bef\uff0c\u7136\u540e\u5229\u7528\u51c0\u5316\u540e\u7684\u7c7b\u522b\u8868\u793a\u6765\u751f\u6210\u6700\u7ec8\u7684\u5206\u5272\u9884\u6d4b\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFreeCP \u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4e0e\u5176\u4ed6 OVSS \u65b9\u6cd5\u7ed3\u5408\u65f6\u7684\u5206\u5272\u6027\u80fd\u3002", "conclusion": "FreeCP \u901a\u8fc7\u51c0\u5316\u8bed\u4e49\u7c7b\u522b\u5e76\u7ea0\u6b63\u5197\u4f59\u548c\u6b67\u4e49\u9020\u6210\u7684\u9519\u8bef\u6765\u89e3\u51b3 OVSS \u4e2d\u7684\u6311\u6218\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u4e0e\u5176\u4ed6 OVSS \u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002"}}
{"id": "2508.00558", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00558", "abs": "https://arxiv.org/abs/2508.00558", "authors": ["Jens U. Kreber", "Joerg Stueckler"], "title": "Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints", "comment": "Accepted for publication at the IEEE/CVF International Conference on\n  Computer Vision (ICCV), 2025", "summary": "Articulated objects are an important type of interactable objects in everyday\nenvironments. In this paper, we propose PhysNAP, a novel diffusion model-based\napproach for generating articulated objects that aligns them with partial point\nclouds and improves their physical plausibility. The model represents part\nshapes by signed distance functions (SDFs). We guide the reverse diffusion\nprocess using a point cloud alignment loss computed using the predicted SDFs.\nAdditionally, we impose non-penetration and mobility constraints based on the\npart SDFs for guiding the model to generate more physically plausible objects.\nWe also make our diffusion approach category-aware to further improve point\ncloud alignment if category information is available. We evaluate the\ngenerative ability and constraint consistency of samples generated with PhysNAP\nusing the PartNet-Mobility dataset. We also compare it with an unguided\nbaseline diffusion model and demonstrate that PhysNAP can improve constraint\nconsistency and provides a tradeoff with generative ability.", "AI": {"tldr": "PhysNAP\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u4e0e\u5c40\u90e8\u70b9\u4e91\u5bf9\u9f50\u4e14\u7269\u7406\u4e0a\u53ef\u4fe1\u7684 artik\u00fcle objekter\u3002", "motivation": "\u4e3a\u4e86\u5728\u751f\u6210\u53ef\u5173\u8282\u7269\u4f53\u65f6\u63d0\u9ad8\u5176\u7269\u7406\u771f\u5b9e\u6027\u5e76\u5b9e\u73b0\u4e0e\u5c40\u90e8\u70b9\u4e91\u7684\u4e00\u81f4\u6027\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPhysNAP\u7684\u65b0\u578b\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u3002", "method": "PhysNAP\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\uff08SDF\uff09\u8868\u793a\u90e8\u4ef6\u5f62\u72b6\uff0c\u5e76\u901a\u8fc7\u70b9\u4e91\u5bf9\u9f50\u635f\u5931\u3001\u975e\u7a7f\u900f\u548c\u53ef\u79fb\u52a8\u6027\u7ea6\u675f\u6765\u6307\u5bfc\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\uff0c\u540c\u65f6\u652f\u6301\u7c7b\u522b\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPhysNAP\u80fd\u591f\u751f\u6210\u7269\u7406\u4e0a\u66f4\u5177\u53ef\u4fe1\u5ea6\u7684\u53ef\u5173\u8282\u7269\u4f53\uff0c\u5e76\u4e0e\u7ed9\u5b9a\u7684\u5c40\u90e8\u70b9\u4e91\u4fdd\u6301\u826f\u597d\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u4e0e\u57fa\u7ebf\u6a21\u578b\u7684\u5bf9\u6bd4\u8bc1\u660e\u4e86\u5176\u5728\u7ea6\u675f\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u751f\u6210\u80fd\u529b\u4e0e\u7ea6\u675f\u4e00\u81f4\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "PhysNAP\u901a\u8fc7\u5f15\u5165\u70b9\u4e91\u5bf9\u9f50\u635f\u5931\u3001\u975e\u7a7f\u900f\u548c\u53ef\u79fb\u52a8\u6027\u7ea6\u675f\uff0c\u4ee5\u53ca\u7c7b\u522b\u611f\u77e5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53ef\u5173\u8282\u7269\u4f53\u751f\u6210\u5728\u7269\u7406\u771f\u5b9e\u6027\u548c\u70b9\u4e91\u5bf9\u9f50\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u5728PartNet-Mobility\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2508.00563", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00563", "abs": "https://arxiv.org/abs/2508.00563", "authors": ["Hannah Kniesel", "Leon Sick", "Tristan Payer", "Tim Bergner", "Kavitha Shaga Devan", "Clarissa Read", "Paul Walther", "Timo Ropinski"], "title": "Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images", "comment": null, "summary": "Current state-of-the-art methods for object detection rely on annotated\nbounding boxes of large data sets for training. However, obtaining such\nannotations is expensive and can require up to hundreds of hours of manual\nlabor. This poses a challenge, especially since such annotations can only be\nprovided by experts, as they require knowledge about the scientific domain. To\ntackle this challenge, we propose a domain-specific weakly supervised object\ndetection algorithm that only relies on image-level annotations, which are\nsignificantly easier to acquire. Our method distills the knowledge of a\npre-trained model, on the task of predicting the presence or absence of a virus\nin an image, to obtain a set of pseudo-labels that can be used to later train a\nstate-of-the-art object detection model. To do so, we use an optimization\napproach with a shrinking receptive field to extract virus particles directly\nwithout specific network architectures. Through a set of extensive studies, we\nshow how the proposed pseudo-labels are easier to obtain, and, more\nimportantly, are able to outperform other existing weak labeling methods, and\neven ground truth labels, in cases where the time to obtain the annotation is\nlimited.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f31\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\uff0c\u4ec5\u9700\u56fe\u50cf\u7ea7\u6807\u6ce8\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u77e5\u8bc6\u63d0\u53d6\u75c5\u6bd2\u9897\u7c92\u7684\u4f2a\u6807\u7b7e\uff0c\u8be5\u65b9\u6cd5\u6807\u6ce8\u6210\u672c\u4f4e\u4e14\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5927\u578b\u6570\u636e\u96c6\u7684\u6807\u6ce8\u8fb9\u754c\u6846\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u83b7\u53d6\u8fd9\u4e9b\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\uff0c\u9700\u8981\u9886\u57df\u4e13\u5bb6\u77e5\u8bc6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u4ec5\u4f9d\u8d56\u56fe\u50cf\u7ea7\u6807\u6ce8\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7279\u5b9a\u9886\u57df\u7684\u5f31\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\uff0c\u4ec5\u4f9d\u8d56\u56fe\u50cf\u7ea7\u6807\u6ce8\u3002\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u56fe\u50cf\u4e2d\u9884\u6d4b\u75c5\u6bd2\u5b58\u5728\u4e0e\u5426\u7684\u4efb\u52a1\u6765\u63d0\u53d6\u4f2a\u6807\u7b7e\uff0c\u8fdb\u800c\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u3002\u91c7\u7528\u5e26\u6709\u6536\u7f29\u611f\u53d7\u91ce\u7684\u4f18\u5316\u65b9\u6cd5\u76f4\u63a5\u63d0\u53d6\u75c5\u6bd2\u9897\u7c92\uff0c\u65e0\u9700\u7279\u5b9a\u7684\u7f51\u7edc\u7ed3\u6784\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u83b7\u5f97\u6bd4\u5176\u4ed6\u73b0\u6709\u5f31\u6807\u6ce8\u65b9\u6cd5\uff0c\u751a\u81f3\u5728\u6807\u6ce8\u65f6\u95f4\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u5730\u9762\u771f\u5b9e\u6807\u7b7e\u7684\u4f2a\u6807\u7b7e\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f2a\u6807\u7b7e\u66f4\u5bb9\u6613\u83b7\u5f97\uff0c\u5e76\u4e14\u5728\u6807\u6ce8\u65f6\u95f4\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u7684\u5f31\u6807\u6ce8\u65b9\u6cd5\uff0c\u751a\u81f3\u4f18\u4e8e\u5730\u9762\u771f\u5b9e\u6807\u7b7e\u3002"}}
{"id": "2508.00620", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00620", "abs": "https://arxiv.org/abs/2508.00620", "authors": ["Quentin Le Roux", "Yannick Teglia", "Teddy Furon", "Philippe Loubet-Moundi"], "title": "Backdoor Attacks on Deep Learning Face Detection", "comment": null, "summary": "Face Recognition Systems that operate in unconstrained environments capture\nimages under varying conditions,such as inconsistent lighting, or diverse face\nposes. These challenges require including a Face Detection module that\nregresses bounding boxes and landmark coordinates for proper Face Alignment.\nThis paper shows the effectiveness of Object Generation Attacks on Face\nDetection, dubbed Face Generation Attacks, and demonstrates for the first time\na Landmark Shift Attack that backdoors the coordinate regression task performed\nby face detectors. We then offer mitigations against these vulnerabilities.", "AI": {"tldr": "This paper introduces a Landmark Shift Attack that exploits face detectors by manipulating landmark coordinates, and provides solutions to prevent such attacks.", "motivation": "Unconstrained face recognition systems face challenges due to varying conditions like inconsistent lighting and diverse face poses, requiring a Face Detection module for proper Face Alignment. This work investigates the effectiveness of Object Generation Attacks on Face Detection.", "method": "The paper proposes a Landmark Shift Attack that backdoors the coordinate regression task performed by face detectors. It also offers mitigations against these vulnerabilities.", "result": "The paper shows the effectiveness of Object Generation Attacks on Face Detection, particularly a Landmark Shift Attack, and presents mitigations.", "conclusion": "The paper demonstrates the effectiveness of Face Generation Attacks on Face Detection, specifically a Landmark Shift Attack that targets the coordinate regression task. It also proposes mitigations against these vulnerabilities."}}
{"id": "2508.00568", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00568", "abs": "https://arxiv.org/abs/2508.00568", "authors": ["Jingchao Xie", "Oussema Dhaouadi", "Weirong Chen", "Johannes Meier", "Jacques Kaiser", "Daniel Cremers"], "title": "CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry", "comment": "Accepted for GCPR 2025. Project page:\n  https://jchao-xie.github.io/CoProU/", "summary": "Visual Odometry (VO) is fundamental to autonomous navigation, robotics, and\naugmented reality, with unsupervised approaches eliminating the need for\nexpensive ground-truth labels. However, these methods struggle when dynamic\nobjects violate the static scene assumption, leading to erroneous pose\nestimations. We tackle this problem by uncertainty modeling, which is a\ncommonly used technique that creates robust masks to filter out dynamic objects\nand occlusions without requiring explicit motion segmentation. Traditional\nuncertainty modeling considers only single-frame information, overlooking the\nuncertainties across consecutive frames. Our key insight is that uncertainty\nmust be propagated and combined across temporal frames to effectively identify\nunreliable regions, particularly in dynamic scenes. To address this challenge,\nwe introduce Combined Projected Uncertainty VO (CoProU-VO), a novel end-to-end\napproach that combines target frame uncertainty with projected reference frame\nuncertainty using a principled probabilistic formulation. Built upon vision\ntransformer backbones, our model simultaneously learns depth, uncertainty\nestimation, and camera poses. Consequently, experiments on the KITTI and\nnuScenes datasets demonstrate significant improvements over previous\nunsupervised monocular end-to-end two-frame-based methods and exhibit strong\nperformance in challenging highway scenes where other approaches often fail.\nAdditionally, comprehensive ablation studies validate the effectiveness of\ncross-frame uncertainty propagation.", "AI": {"tldr": "CoProU-VO\u662f\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u89c6\u89c9\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u548c\u4f20\u64ad\u8de8\u5e27\u4e0d\u786e\u5b9a\u6027\u6765\u5904\u7406\u52a8\u6001\u7269\u4f53\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728KITTI\u548cnuScenes\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u65e0\u76d1\u7763\u89c6\u89c9\u91cc\u7a0b\u8ba1\uff08VO\uff09\u5728\u6d88\u9664\u771f\u5b9e\u4e16\u754c\u6807\u7b7e\u4f9d\u8d56\u7684\u540c\u65f6\uff0c\u6613\u53d7\u52a8\u6001\u7269\u4f53\u5f71\u54cd\u800c\u4ea7\u751f\u9519\u8bef\u7684\u4f4d\u59ff\u4f30\u8ba1\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u52a8\u6001\u573a\u666f\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u672a\u80fd\u6709\u6548\u5229\u7528\u8fde\u7eed\u5e27\u95f4\u7684\u65f6\u5e8f\u4fe1\u606f\u6765\u4f20\u64ad\u548c\u7ec4\u5408\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u51c6\u786e\u8bc6\u522b\u4e0d\u53ef\u9760\u533a\u57df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoProU-VO\u7684\u65b0\u578b\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u89c6\u89c9Transformer\u9aa8\u5e72\u7f51\u7edc\uff0c\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u6df1\u5ea6\u3001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u76f8\u673a\u4f4d\u59ff\u3002\u5176\u6838\u5fc3\u5728\u4e8e\u5229\u7528\u6982\u7387\u6a21\u578b\u7ed3\u5408\u76ee\u6807\u5e27\u4e0d\u786e\u5b9a\u6027\u4e0e\u6295\u5f71\u53c2\u8003\u5e27\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u8de8\u5e27\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u548c\u7ec4\u5408\uff0c\u4ee5\u8fc7\u6ee4\u52a8\u6001\u7269\u4f53\u548c\u906e\u6321\u3002", "result": "\u5728KITTI\u548cnuScenes\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eCoProU-VO\u76f8\u6bd4\u4e8e\u4e4b\u524d\u7684\u65e0\u76d1\u7763\u5355\u76ee\u7aef\u5230\u7aef\u4e24\u5e27\u65b9\u6cd5\u6709\u4e86\u663e\u8457\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u5176\u4ed6\u65b9\u6cd5\u7ecf\u5e38\u5931\u6548\u7684\u9ad8\u901f\u516c\u8def\u7b49\u6311\u6218\u6027\u573a\u666f\u4e0b\u4e5f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u4e5f\u9a8c\u8bc1\u4e86\u8de8\u5e27\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u7684\u6709\u6548\u6027\u3002", "conclusion": "CoProU-VO\u901a\u8fc7\u7ed3\u5408\u76ee\u6807\u5e27\u4e0d\u786e\u5b9a\u6027\u548c\u6295\u5f71\u53c2\u8003\u5e27\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5229\u7528\u6982\u7387\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u4e0a\u8fdb\u884c\u4f20\u64ad\u548c\u7ec4\u5408\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4e2d\u52a8\u6001\u7269\u4f53\u548c\u906e\u6321\u5bfc\u81f4\u7684\u4f4d\u59ff\u4f30\u8ba1\u8bef\u5dee\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728KITTI\u548cnuScenes\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u5355\u76ee\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u9ad8\u901f\u516c\u8def\u573a\u666f\u4e0b\u6548\u679c\u663e\u8457\uff0c\u5e76\u4e14\u9a8c\u8bc1\u4e86\u8de8\u5e27\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.00587", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00587", "abs": "https://arxiv.org/abs/2508.00587", "authors": ["Marc H\u00f6lle", "Walter Kellermann", "Vasileios Belagiannis"], "title": "Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection", "comment": "Accepted at ICCVW 2025, 11 pages, 4 figures", "summary": "Semantic segmentation models trained on known object classes often fail in\nreal-world autonomous driving scenarios by confidently misclassifying unknown\nobjects. While pixel-wise out-of-distribution detection can identify unknown\nobjects, existing methods struggle in complex scenes where rare object classes\nare often confused with truly unknown objects. We introduce an\nuncertainty-aware likelihood ratio estimation method that addresses these\nlimitations. Our approach uses an evidential classifier within a likelihood\nratio test to distinguish between known and unknown pixel features from a\nsemantic segmentation model, while explicitly accounting for uncertainty.\nInstead of producing point estimates, our method outputs probability\ndistributions that capture uncertainty from both rare training examples and\nimperfect synthetic outliers. We show that by incorporating uncertainty in this\nway, outlier exposure can be leveraged more effectively. Evaluated on five\nstandard benchmark datasets, our method achieves the lowest average false\npositive rate (2.5%) among state-of-the-art while maintaining high average\nprecision (90.91%) and incurring only negligible computational overhead. Code\nis available at https://github.com/glasbruch/ULRE.", "AI": {"tldr": "\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4f3c\u7136\u6bd4\u4f30\u8ba1\uff0c\u5728\u533a\u5206\u5df2\u77e5\u548c\u672a\u77e5\u50cf\u7d20\u7279\u5f81\u65b9\u9762\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\u63d0\u9ad8\u4e86\u8bed\u4e49\u5206\u5272\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u9762\u5411\u5bf9\u8c61\u7684\u8bed\u4e49\u5206\u5272\u6a21\u578b\u5728\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\uff0c\u5bf9\u4e8e\u672a\u77e5\u7269\u4f53\u4f1a\u4ea7\u751f\u9519\u8bef\u7684\u5206\u7c7b\uff0c\u800c\u50cf\u7d20\u7ea7\u7684\u5206\u5e03\u5916\u68c0\u6d4b\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u533a\u5206\u7a00\u6709\u7269\u4f53\u548c\u672a\u77e5\u7269\u4f53\u65f6\u5b58\u5728\u56f0\u96be\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4f3c\u7136\u6bd4\u4f30\u8ba1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u4f3c\u7136\u6bd4\u6d4b\u8bd5\u4e2d\u4f7f\u7528\u8bc1\u636e\u5206\u7c7b\u5668\u6765\u533a\u5206\u5df2\u77e5\u548c\u672a\u77e5\u50cf\u7d20\u7279\u5f81\uff0c\u5e76\u660e\u786e\u8003\u8651\u4e86\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684\u5e73\u5747\u5047\u9633\u6027\u7387\uff082.5%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u5e73\u5747\u7cbe\u5ea6\uff0890.91%\uff09\uff0c\u5e76\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u4f3c\u7136\u6bd4\u6d4b\u8bd5\u4e2d\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u79bb\u7fa4\u503c\u66b4\u9732\uff0c\u4ece\u800c\u5728\u533a\u5206\u5df2\u77e5\u548c\u672a\u77e5\u50cf\u7d20\u7279\u5f81\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.00590", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00590", "abs": "https://arxiv.org/abs/2508.00590", "authors": ["Yihe Tian", "Kwan Man Cheng", "Zhengbo Zhang", "Tao Zhang", "Suju Li", "Dongmei Yan", "Bing Xu"], "title": "A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)", "comment": null, "summary": "Artificial Night-Time Light (NTL) remote sensing is a vital proxy for\nquantifying the intensity and spatial distribution of human activities.\nAlthough the NPP-VIIRS sensor provides high-quality NTL observations, its\ntemporal coverage, which begins in 2012, restricts long-term time-series\nstudies that extend to earlier periods. Despite the progress in extending\nVIIRS-like NTL time-series, current methods still suffer from two significant\nshortcomings: the underestimation of light intensity and the structural\nomission. To overcome these limitations, we propose a novel reconstruction\nframework consisting of a two-stage process: construction and refinement. The\nconstruction stage features a Hierarchical Fusion Decoder (HFD) designed to\nenhance the fidelity of the initial reconstruction. The refinement stage\nemploys a Dual Feature Refiner (DFR), which leverages high-resolution\nimpervious surface masks to guide and enhance fine-grained structural details.\nBased on this framework, we developed the Extended VIIRS-like Artificial\nNighttime Light (EVAL) product for China, extending the standard data record\nbackwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL\nsignificantly outperforms existing state-of-the-art products, boosting the\n$\\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.\nFurthermore, EVAL exhibits excellent temporal consistency and maintains a high\ncorrelation with socioeconomic parameters, confirming its reliability for\nlong-term analysis. The resulting EVAL dataset provides a valuable new resource\nfor the research community and is publicly available at\nhttps://doi.org/10.11888/HumanNat.tpdc.302930.", "AI": {"tldr": "\u7531\u4e8e\u73b0\u6709\u591c\u95f4\u706f\u5149\u9065\u611f\u65b9\u6cd5\u5728\u65f6\u95f4\u8986\u76d6\u548c\u7cbe\u5ea6\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aEVAL\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u878d\u5408\u89e3\u7801\u5668\u548c\u53cc\u7279\u5f81\u7cbe\u70bc\u5668\u6765\u91cd\u5efa\u591c\u95f4\u706f\u5149\u6570\u636e\uff0c\u5c06\u4e2d\u56fd\u7684\u6570\u636e\u8bb0\u5f55\u8ffd\u6eaf\u81f31986\u5e74\uff0c\u5e76\u5728\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u591c\u95f4\u706f\u5149\uff08NTL\uff09\u9065\u611f\u65b9\u6cd5\u5728\u4f30\u8ba1\u5149\u7167\u5f3a\u5ea6\u548c\u7ed3\u6784\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u957f\u671f\u7684\u65f6\u5e8f\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u7531\u4e24\u9636\u6bb5\u7ec4\u6210\u7684\u91cd\u5efa\u6846\u67b6\uff1a\u6784\u5efa\u548c\u7cbe\u70bc\u3002\u6784\u5efa\u9636\u6bb5\u91c7\u7528\u5206\u5c42\u878d\u5408\u89e3\u7801\u5668\uff08HFD\uff09\u6765\u63d0\u9ad8\u521d\u59cb\u91cd\u5efa\u7684\u4fdd\u771f\u5ea6\u3002\u7cbe\u70bc\u9636\u6bb5\u91c7\u7528\u53cc\u7279\u5f81\u7cbe\u70bc\u5668\uff08DFR\uff09\uff0c\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u4e0d\u900f\u6c34\u5730\u8868\u9762\u5177\u6765\u6307\u5bfc\u548c\u589e\u5f3a\u7ec6\u7c92\u5ea6\u7ed3\u6784\u7ec6\u8282\u3002", "result": "\u5f00\u53d1\u4e86\u6269\u5c55\u7684VIIRS\u7c7b\u591c\u95f4\u706f\u5149\uff08EVAL\uff09\u4ea7\u54c1\uff0c\u5c06\u4e2d\u56fd\u7684\u6570\u636e\u8bb0\u5f55\u5411\u524d\u8ffd\u6eaf\u4e8626\u5e74\uff0c\u8d77\u59cb\u4e8e1986\u5e74\u3002\u5b9a\u91cf\u8bc4\u4f30\u663e\u793a\uff0cEVAL\u7684R\u00b2\u4ece0.68\u63d0\u5347\u81f30.80\uff0cRMSE\u4ece1.27\u964d\u4f4e\u81f30.99\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EVAL\u4ea7\u54c1\u5728\u957f\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u7814\u7a76\u754c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u65b0\u8d44\u6e90\u3002"}}
{"id": "2508.00701", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00701", "abs": "https://arxiv.org/abs/2508.00701", "authors": ["Chende Zheng", "Ruiqi suo", "Chenhao Lin", "Zhengyu Zhao", "Le Yang", "Shuai Liu", "Minghui Yang", "Cong Wang", "Chao Shen"], "title": "D3: Training-Free AI-Generated Video Detection Using Second-Order Features", "comment": "8 pages, 4 figures", "summary": "The evolution of video generation techniques, such as Sora, has made it\nincreasingly easy to produce high-fidelity AI-generated videos, raising public\nconcern over the dissemination of synthetic content. However, existing\ndetection methodologies remain limited by their insufficient exploration of\ntemporal artifacts in synthetic videos. To bridge this gap, we establish a\ntheoretical framework through second-order dynamical analysis under Newtonian\nmechanics, subsequently extending the Second-order Central Difference features\ntailored for temporal artifact detection. Building on this theoretical\nfoundation, we reveal a fundamental divergence in second-order feature\ndistributions between real and AI-generated videos. Concretely, we propose\nDetection by Difference of Differences (D3), a novel training-free detection\nmethod that leverages the above second-order temporal discrepancies. We\nvalidate the superiority of our D3 on 4 open-source datasets (Gen-Video,\nVideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,\nD3 outperforms the previous best method by 10.39% (absolute) mean Average\nPrecision. Additional experiments on time cost and post-processing operations\ndemonstrate D3's exceptional computational efficiency and strong robust\nperformance. Our code is available at https://github.com/Zig-HS/D3.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a D3 \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u89c6\u9891\u4e2d\u7684\u65f6\u6001\u4f2a\u5f71\u6765\u68c0\u6d4b AI \u751f\u6210\u7684\u89c6\u9891\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u68c0\u6d4b\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u7279\u70b9\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9 Sora \u7b49\u89c6\u9891\u751f\u6210\u6280\u672f\u5e26\u6765\u7684\u9ad8\u4fdd\u771f AI \u751f\u6210\u89c6\u9891\u7684\u6cdb\u6ee5\u4ee5\u53ca\u516c\u4f17\u5bf9\u5408\u6210\u5185\u5bb9\u4f20\u64ad\u7684\u62c5\u5fe7\uff0c\u540c\u65f6\u5f25\u8865\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u65f6\u6001\u4f2a\u5f71\u63a2\u7d22\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u901a\u8fc7\u725b\u987f\u529b\u5b66\u4e0b\u7684\u4e8c\u9636\u52a8\u529b\u5b66\u5206\u6790\u5efa\u7acb\u7406\u8bba\u6846\u67b6\uff0c\u7136\u540e\u63d0\u51fa\u7528\u4e8e\u65f6\u6001\u4f2a\u5f71\u68c0\u6d4b\u7684\u4e8c\u9636\u4e2d\u5fc3\u5dee\u5206\u7279\u5f81\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3a D3 \u7684\u65e0\u9700\u8bad\u7ec3\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e86\u4e8c\u9636\u65f6\u95f4\u7279\u5f81\u7684\u5dee\u5f02\u3002", "result": "D3 \u5728 Gen-Video\u3001VideoPhy\u3001EvalCrafter \u548c VidProM \u8fd9 4 \u4e2a\u516c\u5f00\u6570\u636e\u96c6\u7684 40 \u4e2a\u5b50\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u5148\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad8\u4e86 10.39%\uff0c\u5e76\u4e14\u5728\u65f6\u95f4\u6210\u672c\u548c\u540e\u5904\u7406\u64cd\u4f5c\u7684\u989d\u5916\u5b9e\u9a8c\u4e2d\u4e5f\u8bc1\u660e\u4e86\u5176\u5353\u8d8a\u7684\u8ba1\u7b97\u6548\u7387\u548c\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a D3 \u7684\u65b0\u9896\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e86\u4e8c\u9636\u65f6\u95f4\u7279\u5f81\u7684\u5dee\u5f02\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u5148\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad8\u4e86 10.39%\uff0c\u540c\u65f6\u8868\u73b0\u51fa\u51fa\u8272\u7684\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.00592", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00592", "abs": "https://arxiv.org/abs/2508.00592", "authors": ["Jiajun Le", "Jiayi Ma"], "title": "GeoMoE: Divide-and-Conquer Motion Field Modeling with Mixture-of-Experts for Two-View Geometry", "comment": null, "summary": "Recent progress in two-view geometry increasingly emphasizes enforcing\nsmoothness and global consistency priors when estimating motion fields between\npairs of images. However, in complex real-world scenes, characterized by\nextreme viewpoint and scale changes as well as pronounced depth\ndiscontinuities, the motion field often exhibits diverse and heterogeneous\nmotion patterns. Most existing methods lack targeted modeling strategies and\nfail to explicitly account for this variability, resulting in estimated motion\nfields that diverge from their true underlying structure and distribution. We\nobserve that Mixture-of-Experts (MoE) can assign dedicated experts to motion\nsub-fields, enabling a divide-and-conquer strategy for heterogeneous motion\npatterns. Building on this insight, we re-architect motion field modeling in\ntwo-view geometry with GeoMoE, a streamlined framework. Specifically, we first\ndevise a Probabilistic Prior-Guided Decomposition strategy that exploits inlier\nprobability signals to perform a structure-aware decomposition of the motion\nfield into heterogeneous sub-fields, sharply curbing outlier-induced bias.\nNext, we introduce an MoE-Enhanced Bi-Path Rectifier that enhances each\nsub-field along spatial-context and channel-semantic paths and routes it to a\ncustomized expert for targeted modeling, thereby decoupling heterogeneous\nmotion regimes, suppressing cross-sub-field interference and representational\nentanglement, and yielding fine-grained motion-field rectification. With this\nminimalist design, GeoMoE outperforms prior state-of-the-art methods in\nrelative pose and homography estimation and shows strong generalization. The\nsource code and pre-trained models are available at\nhttps://github.com/JiajunLe/GeoMoE.", "AI": {"tldr": "GeoMoE\u901a\u8fc7\u6982\u7387\u5f15\u5bfc\u5206\u89e3\u548cMoE\u589e\u5f3a\u53cc\u8def\u5f84\u6821\u6b63\u5668\uff0c\u6709\u6548\u5730\u5904\u7406\u4e86\u590d\u6742\u573a\u666f\u4e2d\u7684\u5f02\u6784\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5728\u4e24\u89c6\u56fe\u51e0\u4f55\u4f30\u8ba1\u4e2d\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5f02\u6784\u8fd0\u52a8\u6a21\u5f0f\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9\u4e9b\u573a\u666f\u5177\u6709\u6781\u7aef\u89c6\u89d2\u3001\u5c3a\u5ea6\u53d8\u5316\u548c\u6df1\u5ea6\u4e0d\u8fde\u7eed\u6027\u3002\u8fd9\u4e9b\u65b9\u6cd5\u672a\u80fd\u660e\u786e\u8003\u8651\u8fd0\u52a8\u573a\u7684\u53d8\u5f02\u6027\uff0c\u5bfc\u81f4\u4f30\u8ba1\u7684\u8fd0\u52a8\u573a\u4e0e\u5176\u771f\u5b9e\u7684\u7ed3\u6784\u548c\u5206\u5e03\u5b58\u5728\u504f\u5dee\u3002", "method": "GeoMoE\u6846\u67b6\u91c7\u7528\u6982\u7387\u5148\u9a8c\u5f15\u5bfc\u5206\u89e3\u7b56\u7565\uff0c\u5229\u7528\u5185\u70b9\u6982\u7387\u4fe1\u53f7\u5c06\u8fd0\u52a8\u573a\u5206\u89e3\u4e3a\u5f02\u6784\u5b50\u573a\uff0c\u4ee5\u51cf\u5c11\u5f02\u5e38\u503c\u504f\u5dee\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5f15\u5165\u4e86MoE\u589e\u5f3a\u53cc\u8def\u5f84\u6821\u6b63\u5668\uff0c\u901a\u8fc7\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548c\u901a\u9053\u8bed\u4e49\u8def\u5f84\u589e\u5f3a\u6bcf\u4e2a\u5b50\u573a\uff0c\u5e76\u5c06\u5176\u8def\u7531\u5230\u5b9a\u5236\u7684\u4e13\u5bb6\u8fdb\u884c\u9488\u5bf9\u6027\u5efa\u6a21\uff0c\u4ece\u800c\u89e3\u8026\u5f02\u6784\u8fd0\u52a8\u6a21\u5f0f\uff0c\u6291\u5236\u8de8\u5b50\u573a\u5e72\u6270\u548c\u8868\u793a\u7ea0\u7f20\u3002", "result": "GeoMoE\u5728\u76f8\u5bf9\u4f4d\u59ff\u548c\u5355\u5e94\u6027\u4f30\u8ba1\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GeoMoE\u901a\u8fc7\u5176\u521b\u65b0\u7684\u65b9\u6cd5\u5728\u76f8\u5bf9\u4f4d\u59ff\u548c\u5355\u5e94\u6027\u4f30\u8ba1\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.00599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00599", "abs": "https://arxiv.org/abs/2508.00599", "authors": ["Junzhe Lu", "Jing Lin", "Hongkun Dou", "Ailing Zeng", "Yue Deng", "Xian Liu", "Zhongang Cai", "Lei Yang", "Yulun Zhang", "Haoqian Wang", "Ziwei Liu"], "title": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior", "comment": "ICCV 2025 (oral); Code released: https://github.com/moonbow721/DPoser", "summary": "We present DPoser-X, a diffusion-based prior model for 3D whole-body human\nposes. Building a versatile and robust full-body human pose prior remains\nchallenging due to the inherent complexity of articulated human poses and the\nscarcity of high-quality whole-body pose datasets. To address these\nlimitations, we introduce a Diffusion model as body Pose prior (DPoser) and\nextend it to DPoser-X for expressive whole-body human pose modeling. Our\napproach unifies various pose-centric tasks as inverse problems, solving them\nthrough variational diffusion sampling. To enhance performance on downstream\napplications, we introduce a novel truncated timestep scheduling method\nspecifically designed for pose data characteristics. We also propose a masked\ntraining mechanism that effectively combines whole-body and part-specific\ndatasets, enabling our model to capture interdependencies between body parts\nwhile avoiding overfitting to specific actions. Extensive experiments\ndemonstrate DPoser-X's robustness and versatility across multiple benchmarks\nfor body, hand, face, and full-body pose modeling. Our model consistently\noutperforms state-of-the-art alternatives, establishing a new benchmark for\nwhole-body human pose prior modeling.", "AI": {"tldr": "DPoser-X \u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u5168\u8eab\u4eba\u7c7b\u59ff\u52bf\u5148\u9a8c\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u622a\u65ad\u65f6\u95f4\u6b65\u957f\u8c03\u5ea6\u548c\u63a9\u7801\u8bad\u7ec3\u673a\u5236\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u59ff\u52bf\u590d\u6742\u6027\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u4eba\u4f53\u59ff\u52bf\u7684\u56fa\u6709\u590d\u6742\u6027\u548c\u9ad8\u8d28\u91cf\u5168\u8eab\u59ff\u52bf\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\uff0c\u6784\u5efa\u901a\u7528\u4e14\u9c81\u68d2\u7684\u5168\u8eab\u4eba\u7c7b\u59ff\u52bf\u5148\u9a8c\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DPoser \u7684\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u4eba\u4f53\u59ff\u52bf\u5148\u9a8c\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u4e3a DPoser-X\uff0c\u7528\u4e8e\u5bcc\u6709\u8868\u73b0\u529b\u7684\u5168\u8eab\u4eba\u7c7b\u59ff\u52bf\u5efa\u6a21\u3002\u8be5\u65b9\u6cd5\u5c06\u5404\u79cd\u4ee5\u59ff\u52bf\u4e3a\u4e2d\u5fc3\u7684\u4efb\u52a1\u7edf\u4e00\u4e3a\u9006\u95ee\u9898\uff0c\u901a\u8fc7\u53d8\u5206\u6269\u6563\u91c7\u6837\u6765\u89e3\u51b3\u3002", "result": "DPoser-X \u5728\u8eab\u4f53\u3001\u624b\u90e8\u3001\u9762\u90e8\u548c\u5168\u8eab\u59ff\u52bf\u5efa\u6a21\u7684\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u591a\u529f\u80fd\u6027\uff0c\u5e76\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "DPoser-X \u901a\u8fc7\u65b0\u9896\u7684\u622a\u65ad\u65f6\u95f4\u6b65\u957f\u8c03\u5ea6\u65b9\u6cd5\u548c\u63a9\u7801\u8bad\u7ec3\u673a\u5236\uff0c\u5728\u8eab\u4f53\u3001\u624b\u90e8\u3001\u9762\u90e8\u548c\u5168\u8eab\u59ff\u52bf\u5efa\u6a21\u7684\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u5168\u8eab\u4eba\u7c7b\u59ff\u52bf\u5148\u9a8c\u5efa\u6a21\u6811\u7acb\u4e86\u65b0\u7684\u6807\u6746\u3002"}}
{"id": "2508.00639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00639", "abs": "https://arxiv.org/abs/2508.00639", "authors": ["Luisa Gall\u00e9e", "Catharina Silvia Lisson", "Christoph Gerhard Lisson", "Daniela Drees", "Felix Weig", "Daniel Vogele", "Meinrad Beer", "Michael G\u00f6tz"], "title": "Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification", "comment": "Accepted at iMIMIC - Interpretability of Machine Intelligence in\n  Medical Image Computing workshop MICCAI 2025 Medical Image Computing and\n  Computer Assisted Intervention", "summary": "Classification models that provide human-interpretable explanations enhance\nclinicians' trust and usability in medical image diagnosis. One research focus\nis the integration and prediction of pathology-related visual attributes used\nby radiologists alongside the diagnosis, aligning AI decision-making with\nclinical reasoning. Radiologists use attributes like shape and texture as\nestablished diagnostic criteria and mirroring these in AI decision-making both\nenhances transparency and enables explicit validation of model outputs.\nHowever, the adoption of such models is limited by the scarcity of large-scale\nmedical image datasets annotated with these attributes. To address this\nchallenge, we propose synthesizing attribute-annotated data using a generative\nmodel. We enhance the Diffusion Model with attribute conditioning and train it\nusing only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset.\nIncorporating its generated images into the training of an explainable model\nboosts performance, increasing attribute prediction accuracy by 13.4% and\ntarget prediction accuracy by 1.8% compared to training with only the small\nreal attribute-annotated dataset. This work highlights the potential of\nsynthetic data to overcome dataset limitations, enhancing the applicability of\nexplainable models in medical image analysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5229\u7528\u5c11\u91cf\u6570\u636e\u5408\u6210\u5e26\u5c5e\u6027\u6807\u6ce8\u7684\u533b\u5b66\u56fe\u50cf\uff0c\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91caAI\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u8bca\u65ad\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u7f3a\u4e4f\u5c5e\u6027\u6807\u6ce8\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5408\u6210\u6570\u636e\u6765\u589e\u5f3a\u53ef\u89e3\u91ca\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u6a21\u578b\u5408\u6210\u5e26\u6ce8\u91ca\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5177\u4f53\u662f\u589e\u5f3a\u4e86\u5177\u6709\u5c5e\u6027\u6761\u4ef6\u7ea6\u675f\u7684\u6269\u6563\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u6765\u81eaLIDC-IDRI\u6570\u636e\u96c6\u768420\u4e2a\u5e26\u6807\u7b7e\u7684\u80ba\u7ed3\u8282\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u4e0e\u4ec5\u4f7f\u7528\u5c11\u91cf\u771f\u5b9e\u5e26\u5c5e\u6027\u6807\u6ce8\u6570\u636e\u96c6\u8bad\u7ec3\u76f8\u6bd4\uff0c\u5c06\u751f\u6210\u56fe\u50cf\u7eb3\u5165\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u8bad\u7ec3\u4e2d\uff0c\u5c06\u5c5e\u6027\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e8613.4%\uff0c\u76ee\u6807\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e861.8%\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u751f\u6210\u6a21\u578b\u5408\u6210\u5e26\u6ce8\u91ca\u7684\u6570\u636e\uff0c\u53ef\u4ee5\u514b\u670d\u6570\u636e\u96c6\u9650\u5236\uff0c\u4ece\u800c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528\u6027\u3002"}}
{"id": "2508.00649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00649", "abs": "https://arxiv.org/abs/2508.00649", "authors": ["Junhao Zheng", "Jiahao Sun", "Chenhao Lin", "Zhengyu Zhao", "Chen Ma", "Chong Zhang", "Cong Wang", "Qian Wang", "Chao Shen"], "title": "Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights", "comment": null, "summary": "Developing reliable defenses against patch attacks on object detectors has\nattracted increasing interest. However, we identify that existing defense\nevaluations lack a unified and comprehensive framework, resulting in\ninconsistent and incomplete assessments of current methods. To address this\nissue, we revisit 11 representative defenses and present the first patch\ndefense benchmark, involving 2 attack goals, 13 patch attacks, 11 object\ndetectors, and 4 diverse metrics. This leads to the large-scale adversarial\npatch dataset with 94 types of patches and 94,000 images. Our comprehensive\nanalyses reveal new insights: (1) The difficulty in defending against\nnaturalistic patches lies in the data distribution, rather than the commonly\nbelieved high frequencies. Our new dataset with diverse patch distributions can\nbe used to improve existing defenses by 15.09% AP@0.5. (2) The average\nprecision of the attacked object, rather than the commonly pursued patch\ndetection accuracy, shows high consistency with defense performance. (3)\nAdaptive attacks can substantially bypass existing defenses, and defenses with\ncomplex/stochastic models or universal patch properties are relatively robust.\nWe hope that our analyses will serve as guidance on properly evaluating patch\nattacks/defenses and advancing their design. Code and dataset are available at\nhttps://github.com/Gandolfczjh/APDE, where we will keep integrating new\nattacks/defenses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u8865\u4e01\u9632\u5fa1\u57fa\u51c6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5168\u9762\u7684\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6570\u636e\u5206\u5e03\u548c\u81ea\u9002\u5e94\u653b\u51fb\u5bf9\u9632\u5fa1\u6548\u679c\u7684\u5173\u952e\u5f71\u54cd\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u5bf9\u8c61\u68c0\u6d4b\u5668\u7684\u8865\u4e01\u653b\u51fb\u9632\u5fa1\u8bc4\u4f30\u7f3a\u4e4f\u7edf\u4e00\u3001\u5168\u9762\u7684\u6846\u67b6\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u4e0d\u4e00\u81f4\u4e14\u4e0d\u5b8c\u6574\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u5ba1\u89c611\u79cd\u4ee3\u8868\u6027\u9632\u5fa1\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2\u79cd\u653b\u51fb\u76ee\u6807\u300113\u79cd\u8865\u4e01\u653b\u51fb\u300111\u79cd\u5bf9\u8c61\u68c0\u6d4b\u5668\u548c4\u79cd\u4e0d\u540c\u6307\u6807\u7684\u8865\u4e01\u9632\u5fa1\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u6d89\u53ca94\u79cd\u8865\u4e01\u548c94,000\u5f20\u56fe\u50cf\uff0c\u5f62\u6210\u4e86\u5927\u89c4\u6a21\u5bf9\u6297\u6027\u8865\u4e01\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9632\u5fa1\u81ea\u7136\u8865\u4e01\u7684\u96be\u70b9\u5728\u4e8e\u6570\u636e\u5206\u5e03\u800c\u975e\u9ad8\u9891\u7387\uff1b\u5728AP@0.5\u6307\u6807\u4e0a\uff0c\u8be5\u6570\u636e\u96c6\u53ef\u5c06\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u63d0\u534715.09%\u3002\u6b64\u5916\uff0c\u88ab\u653b\u51fb\u5bf9\u8c61\u7684\u5e73\u5747\u7cbe\u5ea6\u4e0e\u9632\u5fa1\u6027\u80fd\u9ad8\u5ea6\u76f8\u5173\uff0c\u800c\u975e\u8865\u4e01\u68c0\u6d4b\u51c6\u786e\u7387\uff1b\u81ea\u9002\u5e94\u653b\u51fb\u80fd\u6709\u6548\u7ed5\u8fc7\u73b0\u6709\u9632\u5fa1\uff0c\u800c\u5177\u6709\u590d\u6742/\u968f\u673a\u6a21\u578b\u6216\u901a\u7528\u8865\u4e01\u7279\u6027\u7684\u9632\u5fa1\u65b9\u6cd5\u76f8\u5bf9\u66f4\u9c81\u68d2\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u5206\u6790\u4e3a\u4e86\u8bc4\u4f30\u548c\u8bbe\u8ba1\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\u548c\u9632\u5fa1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5e76\u5f3a\u8c03\u4e86\u7406\u89e3\u6570\u636e\u5206\u5e03\u548c\u9002\u5e94\u6027\u653b\u51fb\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00698", "abs": "https://arxiv.org/abs/2508.00698", "authors": ["Hongfei Zhang", "Kun Zhou", "Ruizheng Wu", "Jiangbo Lu"], "title": "Can Large Pretrained Depth Estimation Models Help With Image Dehazing?", "comment": "Submitted to AAAI2026", "summary": "Image dehazing remains a challenging problem due to the spatially varying\nnature of haze in real-world scenes. While existing methods have demonstrated\nthe promise of large-scale pretrained models for image dehazing, their\narchitecture-specific designs hinder adaptability across diverse scenarios with\ndifferent accuracy and efficiency requirements. In this work, we systematically\ninvestigate the generalization capability of pretrained depth\nrepresentations-learned from millions of diverse images-for image dehazing. Our\nempirical analysis reveals that the learned deep depth features maintain\nremarkable consistency across varying haze levels. Building on this insight, we\npropose a plug-and-play RGB-D fusion module that seamlessly integrates with\ndiverse dehazing architectures. Extensive experiments across multiple\nbenchmarks validate both the effectiveness and broad applicability of our\napproach.", "AI": {"tldr": "\u56fe\u50cf\u53bb\u96fe\u7684\u6311\u6218\u5728\u4e8e\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u96fe\u7684\u53d8\u5316\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684RGB-D\u878d\u5408\u6a21\u5757\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u7279\u5f81\u96c6\u6210\u5230\u5404\u79cd\u53bb\u96fe\u67b6\u6784\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u53bb\u96fe\u65b9\u6cd5\u5728\u9002\u5e94\u4e0d\u540c\u7cbe\u5ea6\u548c\u6548\u7387\u8981\u6c42\u7684\u591a\u6837\u5316\u573a\u666f\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u8bbe\u8ba1\u9488\u5bf9\u7279\u5b9a\u67b6\u6784\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u8868\u793a\u5728\u56fe\u50cf\u53bb\u96fe\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684RGB-D\u878d\u5408\u6a21\u5757\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u8868\u793a\uff08\u4ece\u6570\u767e\u4e07\u5f20\u591a\u6837\u5316\u56fe\u50cf\u4e2d\u5b66\u4e60\uff09\u96c6\u6210\u5230\u5404\u79cd\u53bb\u96fe\u67b6\u6784\u4e2d\u3002", "result": "\u7ecf\u9a8c\u5206\u6790\u8868\u660e\uff0c\u5b66\u4e60\u5230\u7684\u6df1\u5ea6\u7279\u5f81\u5728\u4e0d\u540c\u96fe\u5ea6\u6c34\u5e73\u4e0b\u4fdd\u6301\u4e86\u663e\u8457\u7684\u4e00\u81f4\u6027\u3002RGB-D\u878d\u5408\u6a21\u5757\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u5404\u79cd\u53bb\u96fe\u67b6\u6784\u4e2d\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7RGB-D\u878d\u5408\u6a21\u5757\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u7279\u5f81\u96c6\u6210\u5230\u5404\u79cd\u53bb\u96fe\u67b6\u6784\u4e2d\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.00726", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00726", "abs": "https://arxiv.org/abs/2508.00726", "authors": ["Jiale Li", "Mingrui Wu", "Zixiang Jin", "Hao Chen", "Jiayi Ji", "Xiaoshuai Sun", "Liujuan Cao", "Rongrong Ji"], "title": "MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models", "comment": "ACM MM25 has accepted this paper", "summary": "Despite growing interest in hallucination in Multimodal Large Language\nModels, existing studies primarily focus on single-image settings, leaving\nhallucination in multi-image scenarios largely unexplored. To address this gap,\nwe conduct the first systematic study of hallucinations in multi-image MLLMs\nand propose MIHBench, a benchmark specifically tailored for evaluating\nobject-related hallucinations across multiple images. MIHBench comprises three\ncore tasks: Multi-Image Object Existence Hallucination, Multi-Image Object\nCount Hallucination, and Object Identity Consistency Hallucination, targeting\nsemantic understanding across object existence, quantity reasoning, and\ncross-view identity consistency. Through extensive evaluation, we identify key\nfactors associated with the occurrence of multi-image hallucinations,\nincluding: a progressive relationship between the number of image inputs and\nthe likelihood of hallucination occurrences; a strong correlation between\nsingle-image hallucination tendencies and those observed in multi-image\ncontexts; and the influence of same-object image ratios and the positional\nplacement of negative samples within image sequences on the occurrence of\nobject identity consistency hallucination. To address these challenges, we\npropose a Dynamic Attention Balancing mechanism that adjusts inter-image\nattention distributions while preserving the overall visual attention\nproportion. Experiments across multiple state-of-the-art MLLMs demonstrate that\nour method effectively reduces hallucination occurrences and enhances semantic\nintegration and reasoning stability in multi-image scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u586b\u8865\u4e86\u591a\u56fe\u50cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u4e86MIHBench\u57fa\u51c6\u548c\u52a8\u6001\u6ce8\u610f\u529b\u5e73\u8861\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u56fe\u50cf\u573a\u666f\u4e0b\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u56fe\u50cf\u8bbe\u7f6e\uff0c\u5bf9\u591a\u56fe\u50cf\u573a\u666f\u4e0b\u7684\u5e7b\u89c9\u95ee\u9898\u5173\u6ce8\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u7a7a\u767d\uff0c\u9996\u6b21\u5bf9\u591a\u56fe\u50cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u8fdb\u884c\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86MIHBench\u57fa\u51c6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u4efb\u52a1\uff1a\u591a\u56fe\u50cf\u5bf9\u8c61\u5b58\u5728\u5e7b\u89c9\u3001\u591a\u56fe\u50cf\u5bf9\u8c61\u8ba1\u6570\u5e7b\u89c9\u548c\u5bf9\u8c61\u8eab\u4efd\u4e00\u81f4\u6027\u5e7b\u89c9\u3002\u6b64\u5916\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6ce8\u610f\u529b\u5e73\u8861\u673a\u5236\uff0c\u7528\u4e8e\u8c03\u6574\u56fe\u50cf\u95f4\u7684\u6ce8\u610f\u529b\u5206\u5e03\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u56fe\u50cf\u8f93\u5165\u6570\u91cf\u4e0e\u5e7b\u89c9\u53d1\u751f\u51e0\u7387\u4e4b\u95f4\u5b58\u5728\u9012\u8fdb\u5173\u7cfb\uff1b\u5355\u56fe\u50cf\u5e7b\u89c9\u503e\u5411\u4e0e\u591a\u56fe\u50cf\u5e7b\u89c9\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff1b\u76f8\u540c\u5bf9\u8c61\u56fe\u50cf\u6bd4\u4f8b\u4ee5\u53ca\u8d1f\u6837\u672c\u5728\u56fe\u50cf\u5e8f\u5217\u4e2d\u7684\u4f4d\u7f6e\u4f1a\u5f71\u54cd\u5bf9\u8c61\u8eab\u4efd\u4e00\u81f4\u6027\u5e7b\u89c9\u7684\u53d1\u751f\u3002\u6240\u63d0\u51fa\u7684\u52a8\u6001\u6ce8\u610f\u529b\u5e73\u8861\u673a\u5236\u80fd\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\uff0c\u589e\u5f3a\u8bed\u4e49\u6574\u5408\u548c\u63a8\u7406\u7a33\u5b9a\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u591a\u56fe\u50cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86MIHBench\u57fa\u51c6\u6765\u8bc4\u4f30\u8de8\u591a\u4e2a\u56fe\u50cf\u7684\u5bf9\u8c61\u5e7b\u89c9\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc6\u522b\u4e86\u591a\u56fe\u50cf\u5e7b\u89c9\u7684\u5173\u952e\u5f71\u54cd\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6ce8\u610f\u529b\u5e73\u8861\u673a\u5236\uff0c\u8be5\u673a\u5236\u6709\u6548\u51cf\u5c11\u4e86\u5e7b\u89c9\u7684\u53d1\u751f\uff0c\u5e76\u589e\u5f3a\u4e86\u591a\u56fe\u50cf\u573a\u666f\u4e0b\u7684\u8bed\u4e49\u6574\u5408\u548c\u63a8\u7406\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.00748", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.00748", "abs": "https://arxiv.org/abs/2508.00748", "authors": ["Laura Pedrouzo-Rodriguez", "Pedro Delgado-DeRobles", "Luis F. Gomez", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez"], "title": "Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos", "comment": "Accepted at the IEEE International Joint Conference on Biometrics\n  (IJCB 2025)", "summary": "Photorealistic talking-head avatars are becoming increasingly common in\nvirtual meetings, gaming, and social platforms. These avatars allow for more\nimmersive communication, but they also introduce serious security risks. One\nemerging threat is impersonation: an attacker can steal a user's\navatar-preserving their appearance and voice-making it nearly impossible to\ndetect its fraudulent usage by sight or sound alone. In this paper, we explore\nthe challenge of biometric verification in such avatar-mediated scenarios. Our\nmain question is whether an individual's facial motion patterns can serve as\nreliable behavioral biometrics to verify their identity when the avatar's\nvisual appearance is a facsimile of its owner. To answer this question, we\nintroduce a new dataset of realistic avatar videos created using a\nstate-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and\nimpostor avatar videos. We also propose a lightweight, explainable\nspatio-temporal Graph Convolutional Network architecture with temporal\nattention pooling, that uses only facial landmarks to model dynamic facial\ngestures. Experimental results demonstrate that facial motion cues enable\nmeaningful identity verification with AUC values approaching 80%. The proposed\nbenchmark and biometric system are available for the research community in\norder to bring attention to the urgent need for more advanced behavioral\nbiometric defenses in avatar-based communication systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9762\u90e8\u8fd0\u52a8\u6a21\u5f0f\u7684\u884c\u4e3a\u751f\u7269\u8bc6\u522b\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u865a\u62df\u5316\u8eab\u7528\u6237\u7684\u8eab\u4efd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u548c\u8f7b\u91cf\u7ea7\u7684\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u7167\u7247\u822c\u903c\u771f\u7684\u865a\u62df\u5934\u50cf\u5728\u865a\u62df\u4f1a\u8bae\u3001\u6e38\u620f\u548c\u793e\u4ea4\u5e73\u53f0\u4e2d\u65e5\u76ca\u666e\u904d\uff0c\u4f46\u540c\u65f6\u4e5f\u5f15\u5165\u4e86\u5192\u5145\u7b49\u4e25\u91cd\u5b89\u5168\u98ce\u9669\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5728\u5316\u8eab\u5a92\u4ecb\u573a\u666f\u4e2d\u8fdb\u884c\u751f\u7269\u8bc6\u522b\u9a8c\u8bc1\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u3001\u53ef\u89e3\u91ca\u7684\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u7ed3\u5408\u4e86\u65f6\u95f4\u6ce8\u610f\u529b\u6c60\u5316\uff0c\u4ec5\u4f7f\u7528\u9762\u90e8\u5730\u6807\u6765\u6a21\u62df\u52a8\u6001\u9762\u90e8\u59ff\u6001\uff0c\u4ee5\u5e94\u5bf9\u5316\u8eab\u4e2d\u7684\u751f\u7269\u8bc6\u522b\u9a8c\u8bc1\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9762\u90e8\u8fd0\u52a8\u7ebf\u7d22\u80fd\u591f\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u8eab\u4efd\u9a8c\u8bc1\uff0cAUC\u503c\u63a5\u8fd180%\u3002", "conclusion": "\u9762\u90e8\u8fd0\u52a8\u6a21\u5f0f\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u884c\u4e3a\u751f\u7269\u8bc6\u522b\u624b\u6bb5\uff0c\u5728\u5316\u8eab\u4e2d\u5177\u6709\u6709\u610f\u4e49\u7684\u8eab\u4efd\u9a8c\u8bc1\u80fd\u529b\uff0c\u6a21\u578b\u7684AUC\u503c\u63a5\u8fd180%\u3002"}}
{"id": "2508.00728", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00728", "abs": "https://arxiv.org/abs/2508.00728", "authors": ["Guanning Zeng", "Xiang Zhang", "Zirui Wang", "Haiyang Xu", "Zeyuan Chen", "Bingnan Li", "Zhuowen Tu"], "title": "YOLO-Count: Differentiable Object Counting for Text-to-Image Generation", "comment": "ICCV 2025", "summary": "We propose YOLO-Count, a differentiable open-vocabulary object counting model\nthat tackles both general counting challenges and enables precise quantity\ncontrol for text-to-image (T2I) generation. A core contribution is the\n'cardinality' map, a novel regression target that accounts for variations in\nobject size and spatial distribution. Leveraging representation alignment and a\nhybrid strong-weak supervision scheme, YOLO-Count bridges the gap between\nopen-vocabulary counting and T2I generation control. Its fully differentiable\narchitecture facilitates gradient-based optimization, enabling accurate object\ncount estimation and fine-grained guidance for generative models. Extensive\nexperiments demonstrate that YOLO-Count achieves state-of-the-art counting\naccuracy while providing robust and effective quantity control for T2I systems.", "AI": {"tldr": "YOLO-Count\u662f\u4e00\u4e2a\u53ef\u5fae\u7684\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u8ba1\u6570\u6a21\u578b\uff0c\u53ef\u4ee5\u7cbe\u786e\u63a7\u5236\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5bf9\u8c61\u6570\u91cf\u3002\u5b83\u4f7f\u7528'\u57fa\u6570\u56fe'\u4f5c\u4e3a\u65b0\u7684\u56de\u5f52\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u8868\u793a\u5bf9\u9f50\u548c\u6df7\u5408\u76d1\u7763\u65b9\u6848\u8fdb\u884c\u8bad\u7ec3\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u901a\u7528\u7684\u5bf9\u8c61\u8ba1\u6570\u6311\u6218\uff0c\u5e76\u5b9e\u73b0\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8fdb\u884c\u7cbe\u786e\u7684\u6570\u91cf\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'\u57fa\u6570\u56fe'\u7684\u65b0\u578b\u56de\u5f52\u76ee\u6807\uff0c\u8be5\u76ee\u6807\u8003\u8651\u4e86\u5bf9\u8c61\u5927\u5c0f\u548c\u7a7a\u95f4\u5206\u5e03\u7684\u53d8\u5316\u3002\u5229\u7528\u8868\u793a\u5bf9\u9f50\u548c\u6df7\u5408\u5f3a\u5f31\u76d1\u7763\u65b9\u6848\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u5168\u53ef\u5fae\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u68af\u5ea6\u4f18\u5316\u3002", "result": "\u5728\u5bf9\u8c61\u8ba1\u6570\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u6709\u6548\u7684\u6570\u91cf\u63a7\u5236\u3002", "conclusion": "YOLO-Count\u5728\u5bf9\u8c61\u8ba1\u6570\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u800c\u6709\u6548\u7684\u6570\u91cf\u63a7\u5236\u3002"}}
{"id": "2508.00744", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00744", "abs": "https://arxiv.org/abs/2508.00744", "authors": ["Adwait Chandorkar", "Hasan Tercan", "Tobias Meisen"], "title": "Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR", "comment": "accepted at the Embedded Vision Workshop ICCV 2025", "summary": "Recent advancements in LiDAR-based 3D object detection have significantly\naccelerated progress toward the realization of fully autonomous driving in\nreal-world environments. Despite achieving high detection performance, most of\nthe approaches still rely on a VGG-based or ResNet-based backbone for feature\nexploration, which increases the model complexity. Lightweight backbone design\nis well-explored for 2D object detection, but research on 3D object detection\nstill remains limited. In this work, we introduce Dense Backbone, a lightweight\nbackbone that combines the benefits of high processing speed, lightweight\narchitecture, and robust detection accuracy. We adapt multiple SoTA 3d object\ndetectors, such as PillarNet, with our backbone and show that with our\nbackbone, these models retain most of their detection capability at a\nsignificantly reduced computational cost. To our knowledge, this is the first\ndense-layer-based backbone tailored specifically for 3D object detection from\npoint cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%\nreduction in model parameters and a 28% reduction in latency with just a 2%\ndrop in detection accuracy on the nuScenes test set. Furthermore, Dense\nBackbone's plug-and-play design allows straightforward integration into\nexisting architectures, requiring no modifications to other network components.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDense Backbone\u7684\u8f7b\u91cf\u7ea7\u9aa8\u5e72\u7f51\u7edc\uff0c\u7528\u4e8e3D\u76ee\u6807\u68c0\u6d4b\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u9aa8\u5e72\u7f51\u7edc\u53ef\u663e\u8457\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u57fa\u4e8eLiDAR\u76843D\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u590d\u6742\u9aa8\u5e72\u7f51\u7edc\uff08\u5982VGG\u6216ResNet\uff09\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u4ee5\u63d0\u9ad8\u5904\u7406\u901f\u5ea6\u5e76\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDense Backbone\u7684\u8f7b\u91cf\u7ea7\u9aa8\u5e72\u7f51\u7edc\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8ePillarNet\uff08DensePillarNet\uff09\u4ee5\u8fdb\u884c3D\u76ee\u6807\u68c0\u6d4b\u3002\u8be5\u9aa8\u5e72\u7f51\u7edc\u7ed3\u5408\u4e86\u9ad8\u5904\u7406\u901f\u5ea6\u3001\u8f7b\u91cf\u5316\u67b6\u6784\u548c\u5f3a\u5927\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "result": "DensePillarNet\u5728nuScenes\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e8629%\u7684\u6a21\u578b\u53c2\u6570\u51cf\u5c11\u548c28%\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u800c\u68c0\u6d4b\u7cbe\u5ea6\u4ec5\u4e0b\u964d\u4e862%\u3002Dense Backbone\u7684\u5373\u63d2\u5373\u7528\u8bbe\u8ba1\u5141\u8bb8\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u67b6\u6784\u4e2d\uff0c\u800c\u65e0\u9700\u4fee\u6539\u5176\u4ed6\u7f51\u7edc\u7ec4\u4ef6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Dense Backbone\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9aa8\u5e72\u7f51\u7edc\uff0c\u53ef\u4e0e\u73b0\u67093D\u76ee\u6807\u68c0\u6d4b\u5668\uff08\u5982PillarNet\uff09\u7ed3\u5408\u4f7f\u7528\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u4fdd\u6301\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002DensePillarNet\u5728\u6a21\u578b\u53c2\u6570\u548c\u5ef6\u8fdf\u65b9\u9762\u5747\u6709\u663e\u8457\u964d\u4f4e\uff0c\u4ec5\u5bf9\u68c0\u6d4b\u7cbe\u5ea6\u9020\u6210\u5fae\u5c0f\u5f71\u54cd\u3002\u8be5\u9aa8\u5e72\u7f51\u7edc\u7684\u5373\u63d2\u5373\u7528\u8bbe\u8ba1\u4f7f\u5176\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u67b6\u6784\u4e2d\u3002"}}
{"id": "2508.00746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00746", "abs": "https://arxiv.org/abs/2508.00746", "authors": ["Regine Hartwig", "Dominik Muhle", "Riccardo Marin", "Daniel Cremers"], "title": "GECO: Geometrically Consistent Embedding with Lightspeed Inference", "comment": null, "summary": "Recent advances in feature learning have shown that self-supervised vision\nfoundation models can capture semantic correspondences but often lack awareness\nof underlying 3D geometry. GECO addresses this gap by producing geometrically\ncoherent features that semantically distinguish parts based on geometry (e.g.,\nleft/right eyes, front/back legs). We propose a training framework based on\noptimal transport, enabling supervision beyond keypoints, even under occlusions\nand disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%\nfaster than prior methods, while achieving state-of-the-art performance on\nPFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.\nFinally, we show that PCK alone is insufficient to capture geometric quality\nand introduce new metrics and insights for more geometry-aware feature\nlearning. Link to project page:\nhttps://reginehartwig.github.io/publications/geco/", "AI": {"tldr": "GECO\u662f\u4e00\u4e2a\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5b66\u4e60\u51e0\u4f55\u611f\u77e5\u7279\u5f81\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u901f\u5ea6\u66f4\u5feb\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u867d\u7136\u80fd\u6355\u6349\u8bed\u4e49\u5bf9\u5e94\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5e95\u5c423D\u51e0\u4f55\u7684\u611f\u77e5\u3002GECO\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u751f\u6210\u5177\u6709\u51e0\u4f55\u8fde\u8d2f\u6027\u4e14\u80fd\u57fa\u4e8e\u51e0\u4f55\u533a\u5206\u90e8\u5206\u7684\u8bed\u4e49\u7279\u5f81\u3002", "method": "GECO\u4f7f\u7528\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u8d85\u8d8a\u5173\u952e\u70b9\u7684\u76d1\u7763\uff0c\u5373\u4f7f\u5728\u906e\u6321\u548c\u975e\u906e\u6321\u60c5\u51b5\u4e0b\u4e5f\u80fd\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "GECO\u5728PFPascal\u3001APK\u548cCUB\u6570\u636e\u96c6\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5206\u522b\u5c06PCK\u63d0\u9ad8\u4e866.0%\u30016.2%\u548c4.1%\u3002\u6b64\u5916\uff0cGECO\u7684\u8fd0\u884c\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb98.2%\uff0c\u8fbe\u5230\u4e8630\u5e27/\u79d2\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0cPCK\u6307\u6807\u4e0d\u8db3\u4ee5\u5b8c\u5168\u8bc4\u4f30\u51e0\u4f55\u8d28\u91cf\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "GECO\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u81ea\u76d1\u7763\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u57283D\u51e0\u4f55\u611f\u77e5\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u80fd\u591f\u751f\u6210\u8bed\u4e49\u4e0a\u533a\u5206\u90e8\u5206\u4e14\u51e0\u4f55\u4e0a\u8fde\u8d2f\u7684\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u5728PFPascal\u3001APK\u548cCUB\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8fd0\u884c\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb98.2%\uff0c\u5e76\u4e14\u8bc1\u660e\u4e86\u5355\u72ec\u4f7f\u7528PCK\u6307\u6807\u4e0d\u8db3\u4ee5\u8861\u91cf\u51e0\u4f55\u8d28\u91cf\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u548c\u89c1\u89e3\u3002"}}
{"id": "2508.00766", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00766", "abs": "https://arxiv.org/abs/2508.00766", "authors": ["Irene Iele", "Francesco Di Feola", "Valerio Guarrasi", "Paolo Soda"], "title": "Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation", "comment": null, "summary": "Image-to-image translation has emerged as a powerful technique in medical\nimaging, enabling tasks such as image denoising and cross-modality conversion.\nHowever, it suffers from limitations in handling out-of-distribution samples\nwithout causing performance degradation. To address this limitation, we propose\na novel Test-Time Adaptation (TTA) framework that dynamically adjusts the\ntranslation process based on the characteristics of each test sample. Our\nmethod introduces a Reconstruction Module to quantify the domain shift and a\nDynamic Adaptation Block that selectively modifies the internal features of a\npretrained translation model to mitigate the shift without compromising the\nperformance on in-distribution samples that do not require adaptation. We\nevaluate our approach on two medical image-to-image translation tasks: low-dose\nCT denoising and T1 to T2 MRI translation, showing consistent improvements over\nboth the baseline translation model without TTA and prior TTA methods. Our\nanalysis highlights the limitations of the state-of-the-art that uniformly\napply the adaptation to both out-of-distribution and in-distribution samples,\ndemonstrating that dynamic, sample-specific adjustment offers a promising path\nto improve model resilience in real-world scenarios. The code is available at:\nhttps://github.com/cosbidev/Sample-Aware_TTA.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u4e2d\u5904\u7406\u5206\u5e03\u5916\u6837\u672c\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSample-Aware TTA\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7ffb\u8bd1\u8fc7\u7a0b\uff0c\u6839\u636e\u6d4b\u8bd5\u6837\u672c\u7684\u7279\u5f81\u8fdb\u884c\u9002\u5e94\u6027\u4fee\u6539\uff0c\u4ece\u800c\u5728\u4fdd\u8bc1\u5206\u5e03\u5185\u6837\u672c\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u5206\u5e03\u5916\u6837\u672c\u7684\u5904\u7406\u80fd\u529b\uff0c\u5e76\u5728CT\u53bb\u566a\u548cMRI\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u6280\u672f\u5728\u5904\u7406\u5206\u5e03\u5916\u6837\u672c\u65f6\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u8c03\u6574\u4ee5\u9002\u5e94\u4e0d\u540c\u6d4b\u8bd5\u6837\u672c\u7279\u5f81\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08TTA\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u91cd\u5efa\u6a21\u5757\u7528\u4e8e\u91cf\u5316\u57df\u504f\u79fb\uff0c\u4ee5\u53ca\u4e00\u4e2a\u52a8\u6001\u81ea\u9002\u5e94\u5757\uff0c\u8be5\u6a21\u5757\u9009\u62e9\u6027\u5730\u4fee\u6539\u9884\u8bad\u7ec3\u7ffb\u8bd1\u6a21\u578b\u7684\u5185\u90e8\u7279\u5f81\uff0c\u4ee5\u51cf\u8f7b\u504f\u79fb\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u5728\u4e0d\u9700\u8981\u81ea\u9002\u5e94\u7684\u5206\u5e03\u5185\u6837\u672c\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u5728\u4f4e\u5242\u91cfCT\u53bb\u566a\u548cT1\u5230T2 MRI\u7ffb\u8bd1\u4efb\u52a1\u4e0a\uff0c\u6240\u63d0\u51fa\u7684TTA\u6846\u67b6\u76f8\u6bd4\u4e8e\u57fa\u7ebf\u6a21\u578b\u548c\u5148\u524dTTA\u65b9\u6cd5\u5747\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08TTA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7ffb\u8bd1\u8fc7\u7a0b\u6765\u5904\u7406\u5206\u5e03\u5916\u6837\u672c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5\u5728\u5904\u7406\u5206\u5e03\u5916\u6837\u672c\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u5242\u91cfCT\u53bb\u566a\u548cT1\u5230T2 MRI\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u548c\u5176\u4ed6TTA\u65b9\u6cd5\uff0c\u8868\u660e\u52a8\u6001\u3001\u6837\u672c\u7279\u5b9a\u7684\u8c03\u6574\u662f\u63d0\u9ad8\u6a21\u578b\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u9c81\u68d2\u6027\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.00750", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00750", "abs": "https://arxiv.org/abs/2508.00750", "authors": ["Prerana Ramkumar"], "title": "SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation", "comment": null, "summary": "Generative Adversarial Networks (GANs) have achieved realistic\nsuper-resolution (SR) of images however, they lack semantic consistency and\nper-pixel confidence, limiting their credibility in critical remote sensing\napplications such as disaster response, urban planning and agriculture. This\npaper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first\nSR framework designed for satellite imagery to integrate the ESRGAN,\nsegmentation loss via DeepLabv3 for class detail preservation and Monte Carlo\ndropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results\n(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This\nnovel model is valuable in satellite systems or UAVs that use wide\nfield-of-view (FoV) cameras, trading off spatial resolution for coverage. The\nmodular design allows integration in UAV data pipelines for on-board or\npost-processing SR to enhance imagery resulting due to motion blur, compression\nand sensor limitations. Further, the model is fine-tuned to evaluate its\nperformance on cross domain applications. The tests are conducted on two drone\nbased datasets which differ in altitude and imaging perspective. Performance\nevaluation of the fine-tuned models show a stronger adaptation to the Aerial\nMaritime Drone Dataset, whose imaging characteristics align with the training\ndata, highlighting the importance of domain-aware training in SR-applications.", "AI": {"tldr": "SU-ESRGAN\u662f\u9996\u4e2a\u4e3a\u536b\u661f\u56fe\u50cf\u8bbe\u8ba1\u7684\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408ESRGAN\u3001DeepLabv3\u5206\u5272\u635f\u5931\u548c\u8499\u7279\u5361\u6d1bDropout\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u50cf\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\u3002\u8be5\u6a21\u578b\u5728\u822a\u7a7a\u5f71\u50cf\u4e0a\u8868\u73b0\u4e0e\u57fa\u7ebfESRGAN\u76f8\u5f53\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u7b49\u573a\u666f\uff0c\u4f46\u8de8\u57df\u5e94\u7528\u6027\u80fd\u53d7\u6570\u636e\u9886\u57df\u5dee\u5f02\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u65b9\u9762\u867d\u7136\u80fd\u751f\u6210\u903c\u771f\u7684\u56fe\u50cf\uff0c\u4f46\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u50cf\u7d20\u7ea7\u7f6e\u4fe1\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u9065\u611f\u5173\u952e\u5e94\u7528\uff08\u5982\u707e\u96be\u54cd\u5e94\u3001\u57ce\u5e02\u89c4\u5212\u548c\u519c\u4e1a\uff09\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSU-ESRGAN\uff08Semantic and Uncertainty-Aware ESRGAN\uff09\u7684\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u5b83\u96c6\u6210\u4e86ESRGAN\u3001\u7528\u4e8e\u7c7b\u522b\u7ec6\u8282\u4fdd\u7559\u7684DeepLabv3\u5206\u5272\u635f\u5931\u4ee5\u53ca\u7528\u4e8e\u751f\u6210\u50cf\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\u56fe\u7684\u8499\u7279\u5361\u6d1b Dropout\u3002", "result": "SU-ESRGAN\u5728\u822a\u7a7a\u5f71\u50cf\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u57fa\u7ebfESRGAN\u76f8\u5f53\u7684\u8d85\u5206\u8fa8\u7387\u7ed3\u679c\uff08PSNR\u3001SSIM\u3001LPIPS\uff09\u3002", "conclusion": "SU-ESRGAN\u5728\u536b\u661f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u4e0e\u57fa\u7ebfESRGAN\u76f8\u5f53\u7684\u7ed3\u679c\uff08PSNR\u3001SSIM\u3001LPIPS\uff09\uff0c\u4f46\u589e\u52a0\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u50cf\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\u3002\u8be5\u6a21\u578b\u5728\u65e0\u4eba\u673a\u6570\u636e\u7ba1\u9053\u4e2d\u5177\u6709\u5e94\u7528\u4ef7\u503c\uff0c\u53ef\u7528\u4e8e\u677f\u8f7d\u6216\u540e\u5904\u7406\u8d85\u5206\u8fa8\u7387\uff0c\u4ee5\u589e\u5f3a\u7531\u4e8e\u8fd0\u52a8\u6a21\u7cca\u3001\u538b\u7f29\u548c\u4f20\u611f\u5668\u9650\u5236\u800c\u53d7\u635f\u7684\u56fe\u50cf\u3002\u7136\u800c\uff0c\u5728\u8de8\u57df\u5e94\u7528\u4e2d\uff0c\u6a21\u578b\u6027\u80fd\u4f1a\u53d7\u5230\u8bad\u7ec3\u6570\u636e\u548c\u76ee\u6807\u6570\u636e\u4e4b\u95f4\u6210\u50cf\u7279\u5f81\u5dee\u5f02\u7684\u5f71\u54cd\uff0c\u8fd9\u51f8\u663e\u4e86\u9886\u57df\u611f\u77e5\u8bad\u7ec3\u5728\u8d85\u5206\u8fa8\u7387\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00777", "abs": "https://arxiv.org/abs/2508.00777", "authors": ["Zihan Wang", "Samira Ebrahimi Kahou", "Narges Armanfard"], "title": "Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning", "comment": "Accepted at BMVC 2025", "summary": "Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects\nin unseen categories by relying solely on generalizable features rather than\nrequiring any labeled examples of anomalies. However, existing ZSAD methods,\nwhether using fixed or learned prompts, struggle under domain shifts because\ntheir training data are derived from limited training domains and fail to\ngeneralize to new distributions. In this paper, we introduce PILOT, a framework\ndesigned to overcome these challenges through two key innovations: (1) a novel\ndual-branch prompt learning mechanism that dynamically integrates a pool of\nlearnable prompts with structured semantic attributes, enabling the model to\nadaptively weight the most relevant anomaly cues for each input image; and (2)\na label-free test-time adaptation strategy that updates the learnable prompt\nparameters using high-confidence pseudo-labels from unlabeled test data.\nExtensive experiments on 13 industrial and medical benchmarks demonstrate that\nPILOT achieves state-of-the-art performance in both anomaly detection and\nlocalization under domain shift.", "AI": {"tldr": "PILOT\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u63d0\u793a\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u5728\u9886\u57df\u8fc1\u79fb\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4f18\u5f02\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u9886\u57df\u8fc1\u79fb\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u8bad\u7ec3\u6570\u636e\u6765\u6e90\u4e8e\u6709\u9650\u7684\u8bad\u7ec3\u57df\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u7684\u5206\u5e03\u3002PILOT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "PILOT\u6846\u67b6\u91c7\u7528\u53cc\u5206\u652f\u63d0\u793a\u5b66\u4e60\u673a\u5236\uff0c\u52a8\u6001\u6574\u5408\u53ef\u5b66\u4e60\u63d0\u793a\u6c60\u548c\u7ed3\u6784\u5316\u8bed\u4e49\u5c5e\u6027\uff0c\u5e76\u7ed3\u5408\u65e0\u6807\u7b7e\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u5229\u7528\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u66f4\u65b0\u63d0\u793a\u53c2\u6570\u3002", "result": "PILOT\u572813\u4e2a\u5de5\u4e1a\u548c\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u9886\u57df\u8fc1\u79fb\u4e0b\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "PILOT\u6846\u67b6\u901a\u8fc7\u65b0\u9896\u7684\u53cc\u5206\u652f\u63d0\u793a\u5b66\u4e60\u673a\u5236\u548c\u65e0\u6807\u7b7e\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u9886\u57df\u8fc1\u79fb\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u572813\u4e2a\u5de5\u4e1a\u548c\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00822", "abs": "https://arxiv.org/abs/2508.00822", "authors": ["Alexander Nikitas Dimopoulos", "Joseph Grasso"], "title": "Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning", "comment": null, "summary": "This study analyzes semantic segmentation performance across heterogeneously\nlabeled point-cloud datasets relevant to public safety applications, including\npre-incident planning systems derived from lidar scans. Using NIST's Point\nCloud City dataset (Enfield and Memphis collections), we investigate challenges\nin unifying differently labeled 3D data. Our methodology employs a graded\nschema with the KPConv architecture, evaluating performance through IoU metrics\non safety-relevant features. Results indicate performance variability:\ngeometrically large objects (e.g. stairs, windows) achieve higher segmentation\nperformance, suggesting potential for navigational context, while smaller\nsafety-critical features exhibit lower recognition rates. Performance is\nimpacted by class imbalance and the limited geometric distinction of smaller\nobjects in typical lidar scans, indicating limitations in detecting certain\nsafety-relevant features using current point-cloud methods. Key identified\nchallenges include insufficient labeled data, difficulties in unifying class\nlabels across datasets, and the need for standardization. Potential directions\ninclude automated labeling and multi-dataset learning strategies. We conclude\nthat reliable point-cloud semantic segmentation for public safety necessitates\nstandardized annotation protocols and improved labeling techniques to address\ndata heterogeneity and the detection of small, safety-critical elements.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u5206\u6790\u4e86\u4e0e\u516c\u5171\u5b89\u5168\u76f8\u5173\u7684\u70b9\u4e91\u6570\u636e\u96c6\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u53d1\u73b0\u8f83\u5927\u7684\u7269\u4f53\u5206\u5272\u6548\u679c\u66f4\u597d\uff0c\u800c\u8f83\u5c0f\u7684\u3001\u5173\u952e\u7684\u7269\u4f53\u5219\u4e0d\u7136\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u6570\u636e\u6807\u51c6\u5316\u548c\u6539\u8fdb\u6807\u8bb0\u6280\u672f\u4ee5\u63d0\u9ad8\u516c\u5171\u5b89\u5168\u5e94\u7528\u4e2d\u70b9\u4e91\u5206\u6790\u7684\u51c6\u786e\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5206\u6790\u4e0e\u516c\u5171\u5b89\u5168\u5e94\u7528\u76f8\u5173\u7684\u5f02\u6784\u6807\u8bb0\u70b9\u4e91\u6570\u636e\u96c6\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u91cd\u70b9\u5173\u6ce8\u6765\u81ea\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u7684\u9884\u4e8b\u4ef6\u89c4\u5212\u7cfb\u7edf\uff0c\u5e76\u7814\u7a76\u7edf\u4e00\u4e0d\u540c 3D \u6570\u636e\u65f6\u9762\u4e34\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528 KPConv \u67b6\u6784\u548c\u5206\u7ea7\u6a21\u5f0f\uff0c\u901a\u8fc7 IoU \u6307\u6807\u8bc4\u4f30\u4e0e\u5b89\u5168\u76f8\u5173\u7684\u7279\u5f81\u7684\u6027\u80fd\u3002", "result": "\u51e0\u4f55\u5c3a\u5bf8\u8f83\u5927\u7684\u5bf9\u8c61\uff08\u4f8b\u5982\u697c\u68af\u3001\u7a97\u6237\uff09\u7684\u5206\u5272\u6027\u80fd\u8f83\u9ad8\uff0c\u800c\u8f83\u5c0f\u7684\u3001\u5b89\u5168\u5173\u952e\u7684\u7279\u5f81\u8bc6\u522b\u7387\u8f83\u4f4e\u3002\u7c7b\u4e0d\u5e73\u8861\u548c\u8f83\u5c0f\u7269\u4f53\u7684\u51e0\u4f55\u533a\u5206\u5ea6\u6709\u9650\u4f1a\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "\u53ef\u9760\u7684\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u5728\u516c\u5171\u5b89\u5168\u9886\u57df\u9700\u8981\u6807\u51c6\u5316\u7684\u6ce8\u91ca\u534f\u8bae\u548c\u6539\u8fdb\u7684\u6807\u8bb0\u6280\u672f\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u5f02\u6784\u6027\u548c\u68c0\u6d4b\u5c0f\u578b\u3001\u5b89\u5168\u5173\u952e\u5143\u7d20\u7684\u95ee\u9898\u3002"}}
